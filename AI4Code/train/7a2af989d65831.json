{"cell_type":{"f97dbb85":"code","681124fb":"code","54692680":"code","1ff75485":"code","073df23d":"code","692a1009":"code","534b829a":"code","195a7d95":"code","df83c264":"code","77126f3e":"code","c806bdf4":"code","0c5cb3f3":"code","d733e61b":"code","cb754a08":"code","ed726bbc":"code","917cb432":"code","c705531c":"code","4c702867":"code","a892cd91":"code","2843714f":"code","259b8add":"code","f6df2fc5":"code","3093939e":"code","a5df2f91":"code","d0891224":"code","fb6b6631":"code","85727d9c":"code","8fc2f227":"markdown","60dd5b79":"markdown","d1bbccbb":"markdown","40ce2f88":"markdown","8b2d3930":"markdown","c6f66ea3":"markdown","6863ba86":"markdown","ca3ca0f9":"markdown","ebc59f71":"markdown","1b8f782e":"markdown","9fd60429":"markdown","f7ced4dc":"markdown","8b209fa0":"markdown","ca4186ed":"markdown"},"source":{"f97dbb85":"from kaggle_environments import make #Create the RPS environment.\nenv = make(\"rps\", configuration = {\"episodeSteps\":1000})","681124fb":"%%writefile copy_opponent.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\ndef copy_opponent(observation, configuration):\n    if observation.step > 0:\n        return observation.lastOpponentAction\n    else:\n        return random.randrange(0, configuration.signs)","54692680":"%%writefile reactionary.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\nlast_react_action = None\n\n\ndef reactionary(observation, configuration):\n    global last_react_action\n    if observation.step == 0:\n        last_react_action = random.randrange(0, configuration.signs)\n    elif get_score(last_react_action, observation.lastOpponentAction) <= 1:\n        last_react_action = (observation.lastOpponentAction + 1) % configuration.signs\n\n    return last_react_action","1ff75485":"%%writefile counter_reactionary.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\nlast_counter_action = None\n\n\ndef counter_reactionary(observation, configuration):\n    global last_counter_action\n    if observation.step == 0:\n        last_counter_action = random.randrange(0, configuration.signs)\n    elif get_score(last_counter_action, observation.lastOpponentAction) == 1:\n        last_counter_action = (last_counter_action + 2) % configuration.signs\n    else:\n        last_counter_action = (observation.lastOpponentAction + 1) % configuration.signs\n\n    return last_counter_action","073df23d":"%%writefile markov_agent.py\n\nimport numpy as np\nimport collections\n\ndef markov_agent(observation, configuration):\n    k = 2\n    global table, action_seq\n    if observation.step % 250 == 0: # refresh table every 250 steps\n        action_seq, table = [], collections.defaultdict(lambda: [1, 1, 1])    \n    if len(action_seq) <= 2 * k + 1:\n        action = int(np.random.randint(3))\n        if observation.step > 0:\n            action_seq.extend([observation.lastOpponentAction, action])\n        else:\n            action_seq.append(action)\n        return action\n    # update table\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    table[key][observation.lastOpponentAction] += 1\n    # update action seq\n    action_seq[:-2] = action_seq[2:]\n    action_seq[-2] = observation.lastOpponentAction\n    # predict opponent next move\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    if observation.step < 500:\n        next_opponent_action_pred = np.argmax(table[key])\n    else:\n        scores = np.array(table[key])\n        next_opponent_action_pred = np.random.choice(3, p=scores\/scores.sum()) # add stochasticity for second part of the game\n    # make an action\n    action = (next_opponent_action_pred + 1) % 3\n    # if high probability to lose -> let's surprise our opponent with sudden change of our strategy\n    if observation.step > 900:\n        action = next_opponent_action_pred\n    action_seq[-1] = action\n    return int(action)","692a1009":"%%writefile memory_patterns.py\n\nimport random\n\ndef find_pattern(memory_patterns, memory, memory_length):\n    \"\"\" find appropriate pattern in memory \"\"\"\n    for pattern in memory_patterns:\n        actions_matched = 0\n        for i in range(memory_length):\n            if pattern[\"actions\"][i] == memory[i]:\n                actions_matched += 1\n            else:\n                break\n        # if memory fits this pattern\n        if actions_matched == memory_length:\n            return pattern\n    # appropriate pattern not found\n    return None\n\ndef get_step_result_for_my_agent(my_agent_action, opp_action):\n    \"\"\" \n        get result of the step for my_agent\n        1, 0 and -1 representing win, tie and lost results of the game respectively\n    \"\"\"\n    if my_agent_action == opp_action:\n        return 0\n    elif (my_agent_action == (opp_action + 1)) or (my_agent_action == 0 and opp_action == 2):\n        return 1\n    else:\n        return -1\n\n\n# maximum steps in the pattern\nsteps_max = 3\n# minimum steps in the pattern\nsteps_min = 3\n# maximum amount of steps until reassessment of effectiveness of current memory patterns\nmax_steps_until_memory_reassessment = random.randint(80, 120)\n\n# current memory of the agent\ncurrent_memory = []\n# list of 1, 0 and -1 representing win, tie and lost results of the game respectively\n# length is max_steps_until_memory_reassessment\nresults = []\n# current best sum of results\nbest_sum_of_results = 0\n# how many times each action was performed by opponent\nopponent_actions_count = [0, 0, 0]\n# memory length of patterns in first group\n# steps_max is multiplied by 2 to consider both my_agent's and opponent's actions\ngroup_memory_length = steps_max * 2\n# list of groups of memory patterns\ngroups_of_memory_patterns = []\nfor i in range(steps_max, steps_min - 1, -1):\n    groups_of_memory_patterns.append({\n        # how many steps in a row are in the pattern\n        \"memory_length\": group_memory_length,\n        # list of memory patterns\n        \"memory_patterns\": []\n    })\n    group_memory_length -= 2\n    \n\ndef my_agent(obs, conf):\n    \"\"\" your ad here \"\"\"\n    global results\n    global best_sum_of_results\n    # action of my_agent\n    my_action = None\n    \n    # if it's not first step, add opponent's last action to agent's current memory\n    # and reassess effectiveness of current memory patterns\n    if obs[\"step\"] > 0:\n        # count opponent's actions\n        opponent_actions_count[obs[\"lastOpponentAction\"]] += 1\n        # add opponent's last step to current_memory\n        current_memory.append(obs[\"lastOpponentAction\"])\n        # previous step won or lost\n        results.append(get_step_result_for_my_agent(current_memory[-2], current_memory[-1]))\n        \n        # if there is enough steps added to results for memery reassessment\n        if len(results) == max_steps_until_memory_reassessment:\n            results_sum = sum(results)\n            # if effectiveness of current memory patterns has decreased significantly\n            if results_sum < (best_sum_of_results * 0.5):\n                # flush all current memory patterns\n                best_sum_of_results = 0\n                results = []\n                for group in groups_of_memory_patterns:\n                    group[\"memory_patterns\"] = []\n            else:\n                # if effectiveness of current memory patterns has increased\n                if results_sum > best_sum_of_results:\n                    best_sum_of_results = results_sum\n                del results[:1]\n    \n    # search for my_action in memory patterns\n    for group in groups_of_memory_patterns:\n        # if length of current memory is bigger than necessary for a new memory pattern\n        if len(current_memory) > group[\"memory_length\"]:\n            # get momory of the previous step\n            previous_step_memory = current_memory[:group[\"memory_length\"]]\n            previous_pattern = find_pattern(group[\"memory_patterns\"], previous_step_memory, group[\"memory_length\"])\n            if previous_pattern == None:\n                previous_pattern = {\n                    \"actions\": previous_step_memory.copy(),\n                    \"opp_next_actions\": [\n                        {\"action\": 0, \"amount\": 0, \"response\": 1},\n                        {\"action\": 1, \"amount\": 0, \"response\": 2},\n                        {\"action\": 2, \"amount\": 0, \"response\": 0}\n                    ]\n                }\n                group[\"memory_patterns\"].append(previous_pattern)\n            # if such pattern already exists\n            for action in previous_pattern[\"opp_next_actions\"]:\n                if action[\"action\"] == obs[\"lastOpponentAction\"]:\n                    action[\"amount\"] += 1\n            # delete first two elements in current memory (actions of the oldest step in current memory)\n            del current_memory[:2]\n            \n            # if action was not yet found\n            if my_action == None:\n                pattern = find_pattern(group[\"memory_patterns\"], current_memory, group[\"memory_length\"])\n                # if appropriate pattern is found\n                if pattern != None:\n                    my_action_amount = 0\n                    for action in pattern[\"opp_next_actions\"]:\n                        # if this opponent's action occurred more times than currently chosen action\n                        # or, if it occured the same amount of times and this one is choosen randomly among them\n                        if (action[\"amount\"] > my_action_amount or\n                                (action[\"amount\"] == my_action_amount and random.random() > 0.5)):\n                            my_action_amount = action[\"amount\"]\n                            my_action = action[\"response\"]\n    \n    # if no action was found\n    if my_action == None:\n        # choose action randomly\n        my_action = random.randint(0, 2)\n    \n    current_memory.append(my_action)\n    return my_action","534b829a":"%%writefile multi_armed_bandit.py\n\n\nimport pandas as pd\nimport numpy as np\nimport json\n\n\n# base class for all agents, random agent\nclass agent():\n    def initial_step(self):\n        return np.random.randint(3)\n    \n    def history_step(self, history):\n        return np.random.randint(3)\n    \n    def step(self, history):\n        if len(history) == 0:\n            return int(self.initial_step())\n        else:\n            return int(self.history_step(history))\n    \n# agent that returns (previousCompetitorStep + shift) % 3\nclass mirror_shift(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def history_step(self, history):\n        return (history[-1]['competitorStep'] + self.shift) % 3\n    \n    \n# agent that returns (previousPlayerStep + shift) % 3\nclass self_shift(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def history_step(self, history):\n        return (history[-1]['step'] + self.shift) % 3    \n\n\n# agent that beats the most popular step of competitor\nclass popular_beater(agent):\n    def history_step(self, history):\n        counts = np.bincount([x['competitorStep'] for x in history])\n        return (int(np.argmax(counts)) + 1) % 3\n\n    \n# agent that beats the agent that beats the most popular step of competitor\nclass anti_popular_beater(agent):\n    def history_step(self, history):\n        counts = np.bincount([x['step'] for x in history])\n        return (int(np.argmax(counts)) + 2) % 3\n    \n    \n# simple transition matrix: previous step -> next step\nclass transition_matrix(agent):\n    def __init__(self, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type = 'step' \n        else:\n            self.step_type = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        \n    def history_step(self, history):\n        matrix = np.zeros((3,3)) + self.init_value\n        for i in range(len(history) - 1):\n            matrix = (matrix - self.init_value) \/ self.decay + self.init_value\n            matrix[int(history[i][self.step_type]), int(history[i+1][self.step_type])] += 1\n\n        if  self.deterministic:\n            step = np.argmax(matrix[int(history[-1][self.step_type])])\n        else:\n            step = np.random.choice([0,1,2], p = matrix[int(history[-1][self.step_type])]\/matrix[int(history[-1][self.step_type])].sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n    \n\n# similar to the transition matrix but rely on both previous steps\nclass transition_tensor(agent):\n    \n    def __init__(self, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type1 = 'step' \n            self.step_type2 = 'competitorStep'\n        else:\n            self.step_type2 = 'step' \n            self.step_type1 = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        \n    def history_step(self, history):\n        matrix = np.zeros((3,3, 3)) + 0.1\n        for i in range(len(history) - 1):\n            matrix = (matrix - self.init_value) \/ self.decay + self.init_value\n            matrix[int(history[i][self.step_type1]), int(history[i][self.step_type2]), int(history[i+1][self.step_type1])] += 1\n\n        if  self.deterministic:\n            step = np.argmax(matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])])\n        else:\n            step = np.random.choice([0,1,2], p = matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])]\/matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])].sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n\n        \n# looks for the same pattern in history and returns the best answer to the most possible counter strategy\nclass pattern_matching(agent):\n    def __init__(self, steps = 3, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type = 'step' \n        else:\n            self.step_type = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        self.steps = steps\n        \n    def history_step(self, history):\n        if len(history) < self.steps + 1:\n            return self.initial_step()\n        \n        next_step_count = np.zeros(3) + self.init_value\n        pattern = [history[i][self.step_type] for i in range(- self.steps, 0)]\n        \n        for i in range(len(history) - self.steps):\n            next_step_count = (next_step_count - self.init_value)\/self.decay + self.init_value\n            current_pattern = [history[j][self.step_type] for j in range(i, i + self.steps)]\n            if np.sum([pattern[j] == current_pattern[j] for j in range(self.steps)]) == self.steps:\n                next_step_count[history[i + self.steps][self.step_type]] += 1\n        \n        if next_step_count.max() == self.init_value:\n            return self.initial_step()\n        \n        if  self.deterministic:\n            step = np.argmax(next_step_count)\n        else:\n            step = np.random.choice([0,1,2], p = next_step_count\/next_step_count.sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n        \n# if we add all agents the algorithm will spend more that 1 second on turn and will be invalidated\n# right now the agens are non optimal and the same computeations are repeated a lot of times\n# the approach can be optimised to run much faster\nagents = {\n    'mirror_0': mirror_shift(0),\n    'mirror_1': mirror_shift(1),  \n    'mirror_2': mirror_shift(2),\n    'self_0': self_shift(0),\n    'self_1': self_shift(1),  \n    'self_2': self_shift(2),\n    'popular_beater': popular_beater(),\n    'anti_popular_beater': anti_popular_beater(),\n    'random_transitison_matrix': transition_matrix(False, False),\n    'determenistic_transitison_matrix': transition_matrix(True, False),\n    'random_self_trans_matrix': transition_matrix(False, True),\n    'determenistic_self_trans_matrix': transition_matrix(True, True),\n    'random_transitison_tensor': transition_tensor(False, False),\n    'determenistic_transitison_tensor': transition_tensor(True, False),\n    'random_self_trans_tensor': transition_tensor(False, True),\n    'determenistic_self_trans_tensor': transition_tensor(True, True),\n    \n    'random_transitison_matrix_decay': transition_matrix(False, False, decay = 1.05),\n    'random_self_trans_matrix_decay': transition_matrix(False, True, decay = 1.05),\n    'random_transitison_tensor_decay': transition_tensor(False, False, decay = 1.05),\n    'random_self_trans_tensor_decay': transition_tensor(False, True, decay = 1.05),\n    \n    'determenistic_transitison_matrix_decay': transition_matrix(True, False, decay = 1.05),\n    'determenistic_self_trans_matrix_decay': transition_matrix(True, True, decay = 1.05),\n    'determenistic_transitison_tensor_decay': transition_tensor(True, False, decay = 1.05),\n    'determenistic_self_trans_tensor_decay': transition_tensor(True, True, decay = 1.05),\n    \n#     'random_transitison_matrix_decay2': transition_matrix(False, False, decay = 1.001),\n#     'random_self_trans_matrix_decay2': transition_matrix(False, True, decay = 1.001),\n#     'random_transitison_tensor_decay2': transition_tensor(False, False, decay = 1.001),\n#     'random_self_trans_tensor_decay2': transition_tensor(False, True, decay = 1.001),\n    \n#     'determenistic_transitison_matrix_decay2': transition_matrix(True, False, decay = 1.001),\n#     'determenistic_self_trans_matrix_decay2': transition_matrix(True, True, decay = 1.001),\n#     'determenistic_transitison_tensor_decay2': transition_tensor(True, False, decay = 1.001),\n#     'determenistic_self_trans_tensor_decay2': transition_tensor(True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_1': pattern_matching(1, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_1': pattern_matching(1, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_1': pattern_matching(1, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_1': pattern_matching(1, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_2': pattern_matching(2, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_2': pattern_matching(2, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_2': pattern_matching(2, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_2': pattern_matching(2, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_3': pattern_matching(3, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_3': pattern_matching(3, False, True, decay = 1.001),\n    'determenistic_pattern_matching_decay_3': pattern_matching(3, True, False, decay = 1.001),\n    'determenistic_self_pattern_matching_decay_3': pattern_matching(3, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_4': pattern_matching(4, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_4': pattern_matching(4, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_4': pattern_matching(4, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_4': pattern_matching(4, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_5': pattern_matching(5, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_5': pattern_matching(5, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_5': pattern_matching(5, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_5': pattern_matching(5, True, True, decay = 1.001),\n    \n#     'random_pattern_matching_decay_6': pattern_matching(6, False, False, decay = 1.001),\n#     'random_self_pattern_matching_decay_6': pattern_matching(6, False, True, decay = 1.001),\n#     'determenistic_pattern_matching_decay_6': pattern_matching(6, True, False, decay = 1.001),\n#     'determenistic_self_pattern_matching_decay_6': pattern_matching(6, True, True, decay = 1.001),\n}\n\nhistory = []\nbandit_state = {k:[1,1] for k in agents.keys()}\n    \ndef multi_armed_bandit_agent (observation, configuration):\n    \n    # bandits' params\n    step_size = 3 # how much we increase a and b \n    decay_rate = 1.05 # how much do we decay old historical data\n    \n    global history, bandit_state\n    \n    def log_step(step = None, history = None, agent = None, competitorStep = None, file = 'history.csv'):\n        if step is None:\n            step = np.random.randint(3)\n        if history is None:\n            history = []\n        history.append({'step': step, 'competitorStep': competitorStep, 'agent': agent})\n        if file is not None:\n            pd.DataFrame(history).to_csv(file, index = False)\n        return step\n    \n    def update_competitor_step(history, competitorStep):\n        history[-1]['competitorStep'] = int(competitorStep)\n        return history\n    \n    # load history\n    if observation.step == 0:\n        pass\n    else:\n        history = update_competitor_step(history, observation.lastOpponentAction)\n        \n        # updating bandit_state using the result of the previous step\n        # we can update all states even those that were not used\n        for name, agent in agents.items():\n            agent_step = agent.step(history[:-1])\n            bandit_state[name][1] = (bandit_state[name][1] - 1) \/ decay_rate + 1\n            bandit_state[name][0] = (bandit_state[name][0] - 1) \/ decay_rate + 1\n            \n            if (history[-1]['competitorStep'] - agent_step) % 3 == 1:\n                bandit_state[name][1] += step_size\n            elif (history[-1]['competitorStep'] - agent_step) % 3 == 2:\n                bandit_state[name][0] += step_size\n            else:\n                bandit_state[name][0] += step_size\/2\n                bandit_state[name][1] += step_size\/2\n            \n    # we can use it for analysis later\n#     with open('bandit.json', 'w') as outfile:\n#         json.dump(bandit_state, outfile)\n#     \n    \n    # generate random number from Beta distribution for each agent and select the most lucky one\n    best_proba = -1\n    best_agent = None\n    for k in bandit_state.keys():\n        proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\n        if proba > best_proba:\n            best_proba = proba\n            best_agent = k\n        \n    step = agents[best_agent].step(history)\n    \n    return log_step(step, history, best_agent)","195a7d95":"%%writefile opponent_transition_matrix.py\n\nimport numpy as np\nimport pandas as pd\nimport random\n\nT = np.zeros((3, 3))\nP = np.zeros((3, 3))\n\n# a1 is the action of the opponent 1 step ago\n# a2 is the action of the opponent 2 steps ago\na1, a2 = None, None\n\ndef transition_agent(observation, configuration):\n    global T, P, a1, a2\n    if observation.step > 1:\n        a1 = observation.lastOpponentAction\n        T[a2, a1] += 1\n        P = np.divide(T, np.maximum(1, T.sum(axis=1)).reshape(-1, 1))\n        a2 = a1\n        if np.sum(P[a1, :]) == 1:\n            return int((np.random.choice(\n                [0, 1, 2],\n                p=P[a1, :]\n            ) + 1) % 3)\n        else:\n            return int(np.random.randint(3))\n    else:\n        if observation.step == 1:\n            a2 = observation.lastOpponentAction\n        return int(np.random.randint(3))","df83c264":"%%writefile decision_tree_classifier.py\n\nimport numpy as np\nimport collections\nfrom sklearn.tree import DecisionTreeClassifier\n\ndef construct_local_features(rollouts):\n    features = np.array([[step % k for step in rollouts['steps']] for k in (2, 3, 5)])\n    features = np.append(features, rollouts['steps'])\n    features = np.append(features, rollouts['actions'])\n    features = np.append(features, rollouts['opp-actions'])\n    return features\n\ndef construct_global_features(rollouts):\n    features = []\n    for key in ['actions', 'opp-actions']:\n        for i in range(3):\n            actions_count = np.mean([r == i for r in rollouts[key]])\n            features.append(actions_count)\n    \n    return np.array(features)\n\ndef construct_features(short_stat_rollouts, long_stat_rollouts):\n    lf = construct_local_features(short_stat_rollouts)\n    gf = construct_global_features(long_stat_rollouts)\n    features = np.concatenate([lf, gf])\n    return features\n\ndef predict_opponent_move(train_data, test_sample):\n    classifier = DecisionTreeClassifier(random_state=42)\n    classifier.fit(train_data['x'], train_data['y'])\n    return classifier.predict(test_sample)\n\ndef update_rollouts_hist(rollouts_hist, last_move, opp_last_action):\n    rollouts_hist['steps'].append(last_move['step'])\n    rollouts_hist['actions'].append(last_move['action'])\n    rollouts_hist['opp-actions'].append(opp_last_action)\n    return rollouts_hist\n\ndef warmup_strategy(observation, configuration):\n    global rollouts_hist, last_move\n    action = int(np.random.randint(3))\n    if observation.step == 0:\n        last_move = {'step': 0, 'action': action}\n        rollouts_hist = {'steps': [], 'actions': [], 'opp-actions': []}\n    else:\n        rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n        last_move = {'step': observation.step, 'action': action}\n    return int(action)\n\ndef init_training_data(rollouts_hist, k):\n    for i in range(len(rollouts_hist['steps']) - k + 1):\n        short_stat_rollouts = {key: rollouts_hist[key][i:i+k] for key in rollouts_hist}\n        long_stat_rollouts = {key: rollouts_hist[key][:i+k] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, long_stat_rollouts)        \n        data['x'].append(features)\n    test_sample = data['x'][-1].reshape(1, -1)\n    data['x'] = data['x'][:-1]\n    data['y'] = rollouts_hist['opp-actions'][k:]\n    return data, test_sample\n\ndef agent(observation, configuration):\n    # hyperparameters\n    k = 5\n    min_samples = 25\n    global rollouts_hist, last_move, data, test_sample\n    if observation.step == 0:\n        data = {'x': [], 'y': []}\n    # if not enough data -> randomize\n    if observation.step <= min_samples + k:\n        return warmup_strategy(observation, configuration)\n    # update statistics\n    rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n    # update training data\n    if len(data['x']) == 0:\n        data, test_sample = init_training_data(rollouts_hist, k)\n    else:        \n        short_stat_rollouts = {key: rollouts_hist[key][-k:] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, rollouts_hist)\n        data['x'].append(test_sample[0])\n        data['y'] = rollouts_hist['opp-actions'][k:]\n        test_sample = features.reshape(1, -1)\n        \n    # predict opponents move and choose an action\n    next_opp_action_pred = predict_opponent_move(data, test_sample)\n    action = int((next_opp_action_pred + 1) % 3)\n    last_move = {'step': observation.step, 'action': action}\n    return action","77126f3e":"%%writefile statistical_prediction.py\n\nimport random\nimport pydash\nfrom collections import Counter\n\n# Create a small amount of starting history\nhistory = {\n    \"guess\":      [0,1,2],\n    \"prediction\": [0,1,2],\n    \"expected\":   [0,1,2],\n    \"action\":     [0,1,2],\n    \"opponent\":   [0,1],\n}\ndef statistical_prediction_agent(observation, configuration):    \n    global history\n    actions         = list(range(configuration.signs))  # [0,1,2]\n    last_action     = history['action'][-1]\n    opponent_action = observation.lastOpponentAction if observation.step > 0 else 2\n    \n    history['opponent'].append(opponent_action)\n\n    # Make weighted random guess based on the complete move history, weighted towards relative moves based on our last action \n    move_frequency       = Counter(history['opponent'])\n    response_frequency   = Counter(zip(history['action'], history['opponent'])) \n    move_weights         = [ move_frequency.get(n,1) + response_frequency.get((last_action,n),1) for n in range(configuration.signs) ] \n    guess                = random.choices( population=actions, weights=move_weights, k=1 )[0]\n    \n    # Compare our guess to how our opponent actually played\n    guess_frequency      = Counter(zip(history['guess'], history['opponent']))\n    guess_weights        = [ guess_frequency.get((guess,n),1) for n in range(configuration.signs) ]\n    prediction           = random.choices( population=actions, weights=guess_weights, k=1 )[0]\n\n    # Repeat, but based on how many times our prediction was correct\n    prediction_frequency = Counter(zip(history['prediction'], history['opponent']))\n    prediction_weights   = [ prediction_frequency.get((prediction,n),1) for n in range(configuration.signs) ]\n    expected             = random.choices( population=actions, weights=prediction_weights, k=1 )[0]\n\n    # Play the +1 counter move\n    action = (expected + 1) % configuration.signs\n    \n    # Persist state\n    history['guess'].append(guess)\n    history['prediction'].append(prediction)\n    history['expected'].append(expected)\n    history['action'].append(action)\n\n    # Print debug information\n    print('opponent_action                = ', opponent_action)\n    print('move_weights,       guess      = ', move_weights, guess)\n    print('guess_weights,      prediction = ', guess_weights, prediction)\n    print('prediction_weights, expected   = ', prediction_weights, expected)\n    print('action                         = ', action)\n    print()\n    \n    return action","c806bdf4":"%%writefile iocaine.py\n\nimport random\n\n\ndef recall(age, hist):\n    \"\"\"Looking at the last 'age' points in 'hist', finds the\n    last point with the longest similarity to the current point,\n    returning 0 if none found.\"\"\"\n    end, length = 0, 0\n    for past in range(1, min(age + 1, len(hist) - 1)):\n        if length >= len(hist) - past: break\n        for i in range(-1 - length, 0):\n            if hist[i - past] != hist[i]: break\n        else:\n            for length in range(length + 1, len(hist) - past):\n                if hist[-past - length - 1] != hist[-length - 1]: break\n            else: length += 1\n            end = len(hist) - past\n    return end\n\ndef beat(i):\n    return (i + 1) % 3\ndef loseto(i):\n    return (i - 1) % 3\n\nclass Stats:\n    \"\"\"Maintains three running counts and returns the highest count based\n         on any given time horizon and threshold.\"\"\"\n    def __init__(self):\n        self.sum = [[0, 0, 0]]\n    def add(self, move, score):\n        self.sum[-1][move] += score\n    def advance(self):\n        self.sum.append(self.sum[-1])\n    def max(self, age, default, score):\n        if age >= len(self.sum): diff = self.sum[-1]\n        else: diff = [self.sum[-1][i] - self.sum[-1 - age][i] for i in range(3)]\n        m = max(diff)\n        if m > score: return diff.index(m), m\n        return default, score\n\nclass Predictor:\n    \"\"\"The basic iocaine second- and triple-guesser.    Maintains stats on the\n         past benefits of trusting or second- or triple-guessing a given strategy,\n         and returns the prediction of that strategy (or the second- or triple-\n         guess) if past stats are deviating from zero farther than the supplied\n         \"best\" guess so far.\"\"\"\n    def __init__(self):\n        self.stats = Stats()\n        self.lastguess = -1\n    def addguess(self, lastmove, guess):\n        if lastmove != -1:\n            diff = (lastmove - self.prediction) % 3\n            self.stats.add(beat(diff), 1)\n            self.stats.add(loseto(diff), -1)\n            self.stats.advance()\n        self.prediction = guess\n    def bestguess(self, age, best):\n        bestdiff = self.stats.max(age, (best[0] - self.prediction) % 3, best[1])\n        return (bestdiff[0] + self.prediction) % 3, bestdiff[1]\n\nages = [1000, 100, 10, 5, 2, 1]\n\nclass Iocaine:\n\n    def __init__(self):\n        \"\"\"Build second-guessers for 50 strategies: 36 history-based strategies,\n             12 simple frequency-based strategies, the constant-move strategy, and\n             the basic random-number-generator strategy.    Also build 6 meta second\n             guessers to evaluate 6 different time horizons on which to score\n             the 50 strategies' second-guesses.\"\"\"\n        self.predictors = []\n        self.predict_history = self.predictor((len(ages), 2, 3))\n        self.predict_frequency = self.predictor((len(ages), 2))\n        self.predict_fixed = self.predictor()\n        self.predict_random = self.predictor()\n        self.predict_meta = [Predictor() for a in range(len(ages))]\n        self.stats = [Stats() for i in range(2)]\n        self.histories = [[], [], []]\n\n    def predictor(self, dims=None):\n        \"\"\"Returns a nested array of predictor objects, of the given dimensions.\"\"\"\n        if dims: return [self.predictor(dims[1:]) for i in range(dims[0])]\n        self.predictors.append(Predictor())\n        return self.predictors[-1]\n\n    def move(self, them):\n        \"\"\"The main iocaine \"move\" function.\"\"\"\n\n        # histories[0] stores our moves (last one already previously decided);\n        # histories[1] stores their moves (last one just now being supplied to us);\n        # histories[2] stores pairs of our and their last moves.\n        # stats[0] and stats[1] are running counters our recent moves and theirs.\n        if them != -1:\n            self.histories[1].append(them)\n            self.histories[2].append((self.histories[0][-1], them))\n            for watch in range(2):\n                self.stats[watch].add(self.histories[watch][-1], 1)\n\n        # Execute the basic RNG strategy and the fixed-move strategy.\n        rand = random.randrange(3)\n        self.predict_random.addguess(them, rand)\n        self.predict_fixed.addguess(them, 0)\n\n        # Execute the history and frequency stratgies.\n        for a, age in enumerate(ages):\n            # For each time window, there are three ways to recall a similar time:\n            # (0) by history of my moves; (1) their moves; or (2) pairs of moves.\n            # Set \"best\" to these three timeframes (zero if no matching time).\n            best = [recall(age, hist) for hist in self.histories]\n            for mimic in range(2):\n                # For each similar historical moment, there are two ways to anticipate\n                # the future: by mimicing what their move was; or mimicing what my\n                # move was.    If there were no similar moments, just move randomly.\n                for watch, when in enumerate(best):\n                    if not when: move = rand\n                    else: move = self.histories[mimic][when]\n                    self.predict_history[a][mimic][watch].addguess(them, move)\n                # Also we can anticipate the future by expecting it to be the same\n                # as the most frequent past (either counting their moves or my moves).\n                mostfreq, score = self.stats[mimic].max(age, rand, -1)\n                self.predict_frequency[a][mimic].addguess(them, mostfreq)\n\n        # All the predictors have been updated, but we have not yet scored them\n        # and chosen a winner for this round.    There are several timeframes\n        # on which we can score second-guessing, and we don't know timeframe will\n        # do best.    So score all 50 predictors on all 6 timeframes, and record\n        # the best 6 predictions in meta predictors, one for each timeframe.\n        for meta, age in enumerate(ages):\n            best = (-1, -1)\n            for predictor in self.predictors:\n                best = predictor.bestguess(age, best)\n            self.predict_meta[meta].addguess(them, best[0])\n\n        # Finally choose the best meta prediction from the final six, scoring\n        # these against each other on the whole-game timeframe. \n        best = (-1, -1)\n        for meta in range(len(ages)):\n            best = self.predict_meta[meta].bestguess(len(self.histories[0]) , best) \n\n        # We've picked a next move.    Record our move in histories[0] for next time.\n        self.histories[0].append(best[0])\n\n        # And return it.\n        return best[0]\n\niocaine = None\n\ndef iocaine_agent(observation, configuration):\n    global iocaine\n    if observation.step == 0:\n        iocaine = Iocaine()\n        act = iocaine.move(-1)\n    else:\n        act = iocaine.move(observation.lastOpponentAction)\n        \n    return act","0c5cb3f3":"%%writefile greenberg.py\n\n# greenberg roshambo bot, winner of 2nd annual roshambo programming competition\n# http:\/\/webdocs.cs.ualberta.ca\/~darse\/rsbpc.html\n\n# original source by Andrzej Nagorko\n# http:\/\/www.mathpuzzle.com\/greenberg.c\n\n# Python translation by Travis Erdman\n# https:\/\/github.com\/erdman\/roshambo\n\nimport random\nfrom operator import itemgetter\n# from itertools import izip\nizip   = zip   # BUGFIX: izip   is python2\nxrange = range # BUGFIX: xrange is python2\n\nrps_to_text  = ('rock','paper','scissors')\nrps_to_num   = {'rock':0, 'paper':1, 'scissors':2}\n\ndef player(my_moves, opp_moves):\n    wins_with    = (1,2,0)  # superior\n    best_without = (2,0,1)  # inferior\n\n    lengths = (10, 20, 30, 40, 49, 0)\n    p_random = random.choice([0,1,2])  #called 'guess' in iocaine\n\n    TRIALS = 1000\n    score_table =((0,-1,1),(1,0,-1),(-1,1,0))\n    T = len(opp_moves)  #so T is number of trials completed\n\n    def min_index(values):\n        return min(enumerate(values), key=itemgetter(1))[0]\n\n    def max_index(values):\n        return max(enumerate(values), key=itemgetter(1))[0]\n\n    def find_best_prediction(l):  # l = len\n        bs = -TRIALS\n        bp = 0\n        if player.p_random_score > bs:\n            bs = player.p_random_score\n            bp = p_random\n        for i in xrange(3):\n            for j in xrange(24):\n                for k in xrange(4):\n                    new_bs = player.p_full_score[T%50][j][k][i] - (player.p_full_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (player.p_full[j][k] + i) % 3\n                for k in xrange(2):\n                    new_bs = player.r_full_score[T%50][j][k][i] - (player.r_full_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (player.r_full[j][k] + i) % 3\n            for j in xrange(2):\n                for k in xrange(2):\n                    new_bs = player.p_freq_score[T%50][j][k][i] - (player.p_freq_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (player.p_freq[j][k] + i) % 3\n                    new_bs = player.r_freq_score[T%50][j][k][i] - (player.r_freq_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (player.r_freq[j][k] + i) % 3\n        return bp\n\n\n    if not my_moves:\n        player.opp_history = [0]  #pad to match up with 1-based move indexing in original\n        player.my_history = [0]\n        player.gear = [[0] for _ in xrange(24)]\n        # init()\n        player.p_random_score = 0\n        player.p_full_score = [[[[0 for i in xrange(3)] for k in xrange(4)] for j in xrange(24)] for l in xrange(50)]\n        player.r_full_score = [[[[0 for i in xrange(3)] for k in xrange(2)] for j in xrange(24)] for l in xrange(50)]\n        player.p_freq_score = [[[[0 for i in xrange(3)] for k in xrange(2)] for j in xrange(2)] for l in xrange(50)]\n        player.r_freq_score = [[[[0 for i in xrange(3)] for k in xrange(2)] for j in xrange(2)] for l in xrange(50)]\n        player.s_len = [0] * 6\n\n        player.p_full = [[0,0,0,0] for _ in xrange(24)]\n        player.r_full = [[0,0] for _ in xrange(24)]\n    else:\n        player.my_history.append(rps_to_num[my_moves[-1]])\n        player.opp_history.append(rps_to_num[opp_moves[-1]])\n        # update_scores()\n        player.p_random_score += score_table[p_random][player.opp_history[-1]]\n        player.p_full_score[T%50] = [[[player.p_full_score[(T+49)%50][j][k][i] + score_table[(player.p_full[j][k] + i) % 3][player.opp_history[-1]] for i in xrange(3)] for k in xrange(4)] for j in xrange(24)]\n        player.r_full_score[T%50] = [[[player.r_full_score[(T+49)%50][j][k][i] + score_table[(player.r_full[j][k] + i) % 3][player.opp_history[-1]] for i in xrange(3)] for k in xrange(2)] for j in xrange(24)]\n        player.p_freq_score[T%50] = [[[player.p_freq_score[(T+49)%50][j][k][i] + score_table[(player.p_freq[j][k] + i) % 3][player.opp_history[-1]] for i in xrange(3)] for k in xrange(2)] for j in xrange(2)]\n        player.r_freq_score[T%50] = [[[player.r_freq_score[(T+49)%50][j][k][i] + score_table[(player.r_freq[j][k] + i) % 3][player.opp_history[-1]] for i in xrange(3)] for k in xrange(2)] for j in xrange(2)]\n        player.s_len = [s + score_table[p][player.opp_history[-1]] for s,p in izip(player.s_len,player.p_len)]\n\n\n    # update_history_hash()\n    if not my_moves:\n        player.my_history_hash = [[0],[0],[0],[0]]\n        player.opp_history_hash = [[0],[0],[0],[0]]\n    else:\n        player.my_history_hash[0].append(player.my_history[-1])\n        player.opp_history_hash[0].append(player.opp_history[-1])\n        for i in xrange(1,4):\n            player.my_history_hash[i].append(player.my_history_hash[i-1][-1] * 3 + player.my_history[-1])\n            player.opp_history_hash[i].append(player.opp_history_hash[i-1][-1] * 3 + player.opp_history[-1])\n\n\n    #make_predictions()\n\n    for i in xrange(24):\n        player.gear[i].append((3 + player.opp_history[-1] - player.p_full[i][2]) % 3)\n        if T > 1:\n            player.gear[i][T] += 3 * player.gear[i][T-1]\n        player.gear[i][T] %= 9 # clearly there are 9 different gears, but original code only allocated 3 gear_freq's\n                               # code apparently worked, but got lucky with undefined behavior\n                               # I fixed by allocating gear_freq with length = 9\n    if not my_moves:\n        player.freq = [[0,0,0],[0,0,0]]\n        value = [[0,0,0],[0,0,0]]\n    else:\n        player.freq[0][player.my_history[-1]] += 1\n        player.freq[1][player.opp_history[-1]] += 1\n        value = [[(1000 * (player.freq[i][2] - player.freq[i][1])) \/ float(T),\n                  (1000 * (player.freq[i][0] - player.freq[i][2])) \/ float(T),\n                  (1000 * (player.freq[i][1] - player.freq[i][0])) \/ float(T)] for i in xrange(2)]\n    player.p_freq = [[wins_with[max_index(player.freq[i])], wins_with[max_index(value[i])]] for i in xrange(2)]\n    player.r_freq = [[best_without[min_index(player.freq[i])], best_without[min_index(value[i])]] for i in xrange(2)]\n\n    f = [[[[0,0,0] for k in xrange(4)] for j in xrange(2)] for i in xrange(3)]\n    t = [[[0,0,0,0] for j in xrange(2)] for i in xrange(3)]\n\n    m_len = [[0 for _ in xrange(T)] for i in xrange(3)]\n\n    for i in xrange(T-1,0,-1):\n        m_len[0][i] = 4\n        for j in xrange(4):\n            if player.my_history_hash[j][i] != player.my_history_hash[j][T]:\n                m_len[0][i] = j\n                break\n        for j in xrange(4):\n            if player.opp_history_hash[j][i] != player.opp_history_hash[j][T]:\n                m_len[1][i] = j\n                break\n        for j in xrange(4):\n            if player.my_history_hash[j][i] != player.my_history_hash[j][T] or player.opp_history_hash[j][i] != player.opp_history_hash[j][T]:\n                m_len[2][i] = j\n                break\n\n    for i in xrange(T-1,0,-1):\n        for j in xrange(3):\n            for k in xrange(m_len[j][i]):\n                f[j][0][k][player.my_history[i+1]] += 1\n                f[j][1][k][player.opp_history[i+1]] += 1\n                t[j][0][k] += 1\n                t[j][1][k] += 1\n\n                if t[j][0][k] == 1:\n                    player.p_full[j*8 + 0*4 + k][0] = wins_with[player.my_history[i+1]]\n                if t[j][1][k] == 1:\n                    player.p_full[j*8 + 1*4 + k][0] = wins_with[player.opp_history[i+1]]\n                if t[j][0][k] == 3:\n                    player.p_full[j*8 + 0*4 + k][1] = wins_with[max_index(f[j][0][k])]\n                    player.r_full[j*8 + 0*4 + k][0] = best_without[min_index(f[j][0][k])]\n                if t[j][1][k] == 3:\n                    player.p_full[j*8 + 1*4 + k][1] = wins_with[max_index(f[j][1][k])]\n                    player.r_full[j*8 + 1*4 + k][0] = best_without[min_index(f[j][1][k])]\n\n    for j in xrange(3):\n        for k in xrange(4):\n            player.p_full[j*8 + 0*4 + k][2] = wins_with[max_index(f[j][0][k])]\n            player.r_full[j*8 + 0*4 + k][1] = best_without[min_index(f[j][0][k])]\n\n            player.p_full[j*8 + 1*4 + k][2] = wins_with[max_index(f[j][1][k])]\n            player.r_full[j*8 + 1*4 + k][1] = best_without[min_index(f[j][1][k])]\n\n    for j in xrange(24):\n        gear_freq = [0] * 9 # was [0,0,0] because original code incorrectly only allocated array length 3\n\n        for i in xrange(T-1,0,-1):\n            if player.gear[j][i] == player.gear[j][T]:\n                gear_freq[player.gear[j][i+1]] += 1\n\n        #original source allocated to 9 positions of gear_freq array, but only allocated first three\n        #also, only looked at first 3 to find the max_index\n        #unclear whether to seek max index over all 9 gear_freq's or just first 3 (as original code)\n        player.p_full[j][3] = (player.p_full[j][1] + max_index(gear_freq)) % 3\n\n    # end make_predictions()\n\n    player.p_len = [find_best_prediction(l) for l in lengths]\n\n    return rps_to_text[player.p_len[max_index(player.s_len)]]\n\n\n\n# observation   =  {'step': 1, 'lastOpponentAction': 1}\n# configuration =  {'episodeSteps': 10, 'agentTimeout': 60, 'actTimeout': 1, 'runTimeout': 1200, 'isProduction': False, 'signs': 3}\nmy_moves    = []\nopp_moves   = []\ndef kaggle_agent(observation, configuration):    \n    global my_moves\n    global opp_moves\n    if observation.step > 0:\n        opp_move = rps_to_text[ observation.lastOpponentAction ]\n        opp_moves.append( opp_move )\n        \n    action_text = player(my_moves, opp_moves)\n    action      = rps_to_num[action_text]\n\n    my_moves.append(action_text)\n    return int(action)","d733e61b":"%%writefile preCoded_hist.py\n\nimport random\n\nmoves = [0, 1, 2]\ndna_encode = {\n    '11': '1', '10': '2', '12': '3',\n    '01': '4', '02': '5', '00': '6',\n    '22': '7', '21': '8', '20': '9' }\n\ndef beat_move(x):\n    return (x + 1) % 3\n\ndef agent (observation, configuration):\n    global opp_history, action, dna\n    if observation.step == 0:\n        opp_history = ''\n        dna = ''\n        action = random.choice([0, 1, 2])\n    else:\n        opp_history += str(observation.lastOpponentAction)\n        dna += dna_encode[str(observation.lastOpponentAction) + str(action)]\n\n        for length in (100, 90, 80, 70, 60, 50, 40, 30, 20, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1):\n            # Search for the last longest chain\n            x = dna[:-1].rfind (dna[-length:])\n            if x >= 0:\n                # If found: Pick what will be the next move and play against it\n                next_move = opp_history[x + length]\n                action = beat_move(int(next_move))\n                break\n    \n    return action","cb754a08":"%%writefile xgboost.py\n\nimport random\nfrom pandas import DataFrame\nfrom xgboost import XGBClassifier\n\nnumTurnsPredictors = 5 #number of previous turns to use as predictors\nminTrainSetRows = 10 #only start predicting moves after we have enough data\nmyLastMove = None\nmySecondLastMove = None\nopponentLastMove = None\nnumDummies = 2 #how many dummy vars we need to represent a move\npredictors = DataFrame(columns=[str(x) for x in range(numTurnsPredictors * 2 * numDummies)])\npredictors = predictors.astype(\"int\")\nopponentsMoves = []\nroundHistory = [] #moves made by both players in each round\nclf = XGBClassifier(n_estimators=10)\n\ndef randomMove():\n    return random.randint(0,2)\n\n#converts my and opponents moves into dummy variables i.e. [1,2] into [0,1,1,0]\ndef convertToDummies(moves):\n    newMoves = []\n    dummies = [[0,0], [0,1], [1,0]]\n\n    for move in moves:\n        newMoves.extend(dummies[move])\n\n    return newMoves\n\ndef updateRoundHistory(myMove, opponentMove):\n    global roundHistory\n    roundHistory.append(convertToDummies([myMove, opponentMove]))\n\ndef flattenData(data):\n    return sum(data, [])\n\ndef updateFeatures(rounds):\n    global predictors\n    flattenedRounds = flattenData(rounds)\n    predictors.loc[len(predictors)] = flattenedRounds\n\ndef fitAndPredict(clf, x, y, newX):\n    df = DataFrame.from_records([newX], columns=[str(i) for i in range(numTurnsPredictors * 2 * numDummies)])\n    clf.fit(x, y)\n    return int(clf.predict(df)[0])\n\ndef makeMove(observation, configuration):\n    global myLastMove\n    global mySecondLastMove\n    global opponentLastMove\n    global predictors\n    global opponentsMoves\n    global roundHistory\n\n    if observation.step == 0:\n        myLastMove = randomMove()\n        return myLastMove\n\n    if observation.step == 1:\n        updateRoundHistory(myLastMove, observation.lastOpponentAction)\n        myLastMove = randomMove()\n        return myLastMove\n\n    else:\n        updateRoundHistory(myLastMove, observation.lastOpponentAction)\n        opponentsMoves.append(observation.lastOpponentAction)\n\n        if observation.step > numTurnsPredictors:\n            updateFeatures(roundHistory[-numTurnsPredictors - 1: -1])\n\n        if len(predictors) > minTrainSetRows:\n            predictX = flattenData(roundHistory[-numTurnsPredictors:]) #data to predict next move\n            predictedMove = fitAndPredict(clf, predictors,\n                                opponentsMoves[(numTurnsPredictors-1):], predictX)\n            myLastMove = (predictedMove + 1) % 3\n            return myLastMove\n        else:\n            myLastMove = randomMove()\n            return myLastMove","ed726bbc":"%%writefile not_losing.py\nimport random\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\n\nUSE_BACK = 10\nPRED_USE_STEP_THRES = 200\nPRED_USE_SCORE_THRES = 0.9\nmy_actions = []\nop_actions = []\nsolutions   = []\n\n## ============== LIGHT GBM PREDICTION ============== ## \ndef predict(my_actions, op_actions):\n    size = len(my_actions)\n    \n    d = dict()\n    for u in range(USE_BACK):\n        d[f\"OP_{u}\"] = op_actions[u: size - (USE_BACK - u)]\n        d[f\"MY_{u}\"] = my_actions[u: size - (USE_BACK - u)]\n    \n    X_train = pd.DataFrame(d)\n    y_train = op_actions[USE_BACK: size]\n    y_train = pd.DataFrame(y_train, columns=[\"y\"])\n    \n    n = dict()\n    for u in range(USE_BACK):\n        n[f\"OP_{u}\"] = [op_actions[size - (USE_BACK - u)]]\n        n[f\"MY_{u}\"] = [my_actions[size - (USE_BACK - u)]]\n    \n    X_test = pd.DataFrame(n)\n\n    classifier = lgb.LGBMClassifier(\n        random_state=0, \n        n_estimators=10, \n    )\n    \n    classifier.fit(X_train, y_train)\n    return classifier.predict_proba(X_test).tolist()[0], int(classifier.predict(X_test)[0])\n\n## ============== RANDOM(NASH EQUILIBRIUM) ============== ##\ndef randomize():\n    return int(random.randint(0, 2))\n\n## ============== PREDICT ONLY CONFIRM, OTHER RANDOM ============== ##\ndef predict_only_when_confirmation(observation, configuration):\n    global my_actions\n    global op_actions\n    \n    if observation.step != 0:\n        op_actions.append(observation.lastOpponentAction)\n    \n    if observation.step > PRED_USE_STEP_THRES:\n        pred_proba, pred = predict(my_actions, op_actions)      \n        if max(pred_proba) > PRED_USE_SCORE_THRES:\n            my_action = pred\n            my_action = (my_action + 1) % 3\n        \n        else:\n            my_action = randomize()\n        \n    else:    \n        my_action = randomize()\n    \n    my_actions.append(my_action)\n    \n    return my_action","917cb432":"%%writefile simple_method.py\nimport random\n\nrecs = []\n\ndef my_agent(obs, conf):\n    global recs\n    \n    count=len(recs)\n    \n    if count==0:\n        hand = random.randint(0, 2)\n        \n    else:\n        recs.append(obs[\"lastOpponentAction\"])\n\n        hand_count = [recs.count(0), recs.count(1), recs.count(2)]\n        hand_ratio = [hand_count[0], hand_count[0]+hand_count[1], count] \/ count\n\n        hand_rand = random.random()\n        for i,ratio in enumerate(hand_ratio):\n            if hand_rand <= ratio:\n                hand = i\n                break\n    \n    return (hand+1)%3","c705531c":"%%writefile geometry.py\n\nimport operator\nimport numpy as np\nimport cmath\nfrom typing import List\nfrom collections import namedtuple\nimport traceback\nimport sys\n\n\nbasis = np.array(\n    [1, cmath.exp(2j * cmath.pi * 1 \/ 3), cmath.exp(2j * cmath.pi * 2 \/ 3)]\n)\n\n\nHistMatchResult = namedtuple(\"HistMatchResult\", \"idx length\")\n\n\ndef find_all_longest(seq, max_len=None) -> List[HistMatchResult]:\n    \"\"\"\n    Find all indices where end of `seq` matches some past.\n    \"\"\"\n    result = []\n\n    i_search_start = len(seq) - 2\n\n    while i_search_start > 0:\n        i_sub = -1\n        i_search = i_search_start\n        length = 0\n\n        while i_search >= 0 and seq[i_sub] == seq[i_search]:\n            length += 1\n            i_sub -= 1\n            i_search -= 1\n\n            if max_len is not None and length > max_len:\n                break\n\n        if length > 0:\n            result.append(HistMatchResult(i_search_start + 1, length))\n\n        i_search_start -= 1\n\n    result = sorted(result, key=operator.attrgetter(\"length\"), reverse=True)\n\n    return result\n\n\ndef probs_to_complex(p):\n    return p @ basis\n\n\ndef _fix_probs(probs):\n    \"\"\"\n    Put probs back into triangle. Sometimes this happens due to rounding errors or if you\n    use complex numbers which are outside the triangle.\n    \"\"\"\n    if min(probs) < 0:\n        probs -= min(probs)\n\n    probs \/= sum(probs)\n\n    return probs\n\n\ndef complex_to_probs(z):\n    probs = (2 * (z * basis.conjugate()).real + 1) \/ 3\n    probs = _fix_probs(probs)\n    return probs\n\n\ndef z_from_action(action):\n    return basis[action]\n\n\ndef sample_from_z(z):\n    probs = complex_to_probs(z)\n    return np.random.choice(3, p=probs)\n\n\ndef bound(z):\n    return probs_to_complex(complex_to_probs(z))\n\n\ndef norm(z):\n    return bound(z \/ abs(z))\n\n\nclass Pred:\n    def __init__(self, *, alpha):\n        self.offset = 0\n        self.alpha = alpha\n        self.last_feat = None\n\n    def train(self, target):\n        if self.last_feat is not None:\n            offset = target * self.last_feat.conjugate()   # fixed\n\n            self.offset = (1 - self.alpha) * self.offset + self.alpha * offset\n\n    def predict(self, feat):\n        \"\"\"\n        feat is an arbitrary feature with a probability on 0,1,2\n        anything which could be useful anchor to start with some kind of sensible direction\n        \"\"\"\n        feat = norm(feat)\n\n        # offset = mean(target - feat)\n        # so here we see something like: result = feat + mean(target - feat)\n        # which seems natural and accounts for the correlation between target and feat\n        # all RPSContest bots do no more than that as their first step, just in a different way\n        \n        result = feat * self.offset\n\n        self.last_feat = feat\n\n        return result\n    \n    \nclass BaseAgent:\n    def __init__(self):\n        self.my_hist = []\n        self.opp_hist = []\n        self.my_opp_hist = []\n        self.outcome_hist = []\n        self.step = None\n\n    def __call__(self, obs, conf):\n        try:\n            if obs.step == 0:\n                action = np.random.choice(3)\n                self.my_hist.append(action)\n                return action\n\n            self.step = obs.step\n\n            opp = int(obs.lastOpponentAction)\n            my = self.my_hist[-1]\n\n            self.my_opp_hist.append((my, opp))\n            self.opp_hist.append(opp)\n\n            outcome = {0: 0, 1: 1, 2: -1}[(my - opp) % 3]\n            self.outcome_hist.append(outcome)\n\n            action = self.action()\n\n            self.my_hist.append(action)\n\n            return action\n        except Exception:\n            traceback.print_exc(file=sys.stderr)\n            raise\n\n    def action(self):\n        pass\n\n\nclass Agent(BaseAgent):\n    def __init__(self, alpha=0.01):\n        super().__init__()\n\n        self.predictor = Pred(alpha=alpha)\n\n    def action(self):\n        self.train()\n\n        pred = self.preds()\n\n        return_action = sample_from_z(pred)\n\n        return return_action\n\n    def train(self):\n        last_beat_opp = z_from_action((self.opp_hist[-1] + 1) % 3)\n        self.predictor.train(last_beat_opp)\n\n    def preds(self):\n        hist_match = find_all_longest(self.my_opp_hist, max_len=20)\n\n        if not hist_match:\n             return 0\n\n        feat = z_from_action(self.opp_hist[hist_match[0].idx])\n\n        pred = self.predictor.predict(feat)\n\n        return pred\n    \n    \nagent = Agent()\n\n\ndef call_agent(obs, conf):\n    return agent(obs, conf)","4c702867":"%%writefile \"anti_geo.py\"\n\nimport operator\nimport numpy as np\nimport cmath\nfrom collections import namedtuple\n\nbasis = np.array([1, cmath.exp(2j * cmath.pi * 1 \/ 3), cmath.exp(2j * cmath.pi * 2 \/ 3)])\nHistMatchResult = namedtuple(\"HistMatchResult\", \"idx length\")\n\ndef find_all_longest(seq, max_len=None): \n        result = []\n        i_search_start = len(seq) - 2\n        while i_search_start > 0:\n            i_sub = -1\n            i_search = i_search_start\n            length = 0\n            while i_search >= 0 and seq[i_sub] == seq[i_search]:\n                length += 1\n                i_sub -= 1\n                i_search -= 1\n                if max_len is not None and length > max_len: break\n            if length > 0: result.append(HistMatchResult(i_search_start + 1, length))\n            i_search_start -= 1\n\n        return sorted(result, key=operator.attrgetter(\"length\"), reverse=True)\n\ndef complex_to_probs(z):\n        probs = (2 * (z * basis.conjugate()).real + 1) \/ 3\n        if min(probs) < 0: probs -= min(probs)\n        return probs \/ sum(probs)\n\nopp_hist = []\nmy_opp_hist = []\noffset = 0\nlast_feat = None\n\ndef agent(obs, conf):\n    global action, opp_hist, my_opp_hist, offset, last_feat\n\n    if obs.step == 0:\n        action = np.random.choice(3)\n    else:\n        my_opp_hist.append((obs.lastOpponentAction, action))\n        opp_hist.append(action)\n\n        if last_feat is not None:\n            this_offset = (basis[(opp_hist[-1] + 1) % 3]) * last_feat.conjugate()\n            offset = (1 - .01) * offset + .01 * this_offset\n\n        hist_match = find_all_longest(my_opp_hist, 20)\n        if not hist_match:\n            pred = 0\n        else:\n            feat = basis[opp_hist[hist_match[0].idx]]\n            last_feat = complex_to_probs(feat \/ abs(feat)) @ basis\n            pred = last_feat * offset * cmath.exp(2j * cmath.pi * 1\/9)\n\n        probs = complex_to_probs(pred)\n        if probs[np.argmax(probs)] > .334:\n            action = (int(np.argmax(probs))+1)%3\n        else:\n            action = (np.random.choice(3, p=probs)+1)%3\n\n    return action","a892cd91":"%%writefile \"anti_otm.py\"\n\nimport numpy as np\nimport pandas as pd\nimport random\n\nT = np.zeros((3, 3))\nP = np.zeros((3, 3))\n\na1, a2 = None, None\nlast_action = None # track my action.\n\n\n###########################################\n# Original agent with modifications marked ->\n###########################################\n\ndef anti_transition_agent(observation, configuration):\n    global T, P, a1, a2, last_action\n    if observation.step > 1:\n        a1 = last_action   # on me only; take mirrored view on game\n        T[a2, a1] += 1\n        P = np.divide(T, np.maximum(1, T.sum(axis=1)).reshape(-1, 1))\n        a2 = a1\n        if np.sum(P[a1, :]) == 1:\n            probs = P[a1,:]\n            \n            probs += 0.63 * np.roll(probs, 1)    # This is the magic addition of phase\n            \n            result = (int(probs.argmax()) + 1) % 3   # Changed to argmax instead of stochastic\n        else:\n            result = int(np.random.randint(3))\n    else:\n        if observation.step == 1:\n            a2 = last_action    # on me only\n        result = int(np.random.randint(3))\n        \n    result = (result + 1) % 3  # beat what he would have done\n        \n    last_action = result\n        \n    return result","2843714f":"%%writefile new_mlb.py\n##### .\/memory_patterns.py #####\n\n\nimport random\nimport numpy as np \nimport pandas as pd\nfrom typing import List, Dict, Tuple, Any\nfrom operator import itemgetter\nfrom collections import defaultdict\nimport torch\nfrom torch import nn, optim\n\nfrom kaggle_environments import evaluate, make, utils\nfrom kaggle_environments.envs.rps.utils import get_score\nfrom kaggle_environments.envs.rps.agents import *\n\ndef batch(iterable, n=1):\n    l = len(iterable)\n    for ndx in range(0, l, n):\n        yield iterable[ndx:min(ndx + n, l)]\n\n\nclass MemoryPatterns:\n    def __init__(self, min_memory=2, max_memory=20, threshold=0.5, warmup=5, verbose=True):\n        self.min_memory = min_memory\n        self.max_memory = max_memory\n        self.threshold  = threshold\n        self.warmup     = warmup\n        self.verbose    = verbose\n        self.history = {\n            \"step\":      [],\n            \"reward\":    [],\n            \"opponent\":  [],\n            \"pattern\":   [],\n            \"action\":    [],\n            # \"rotn_self\": [],\n            # \"rotn_opp\":  [],\n        }\n        pass\n    \n    def __call__(self, obs, conf):\n        return self.agent(obs, conf)\n\n    \n    # obs  {'remainingOverageTime': 60, 'step': 1, 'reward': 0, 'lastOpponentAction': 0}\n    # conf {'episodeSteps': 1000, 'actTimeout': 1, 'runTimeout': 1200, 'signs': 3, 'tieRewardThreshold': 20, 'agentTimeout': 60}\n    def agent(self, obs, conf):\n        # pass\n        # pass\n        self.obs  = obs\n        self.conf = conf\n        self.update_state(obs, conf)\n        if obs.step < self.warmup:\n            expected = self.random_action(obs, conf)\n        else:\n            for keys in [ (\"opponent\", \"action\"), (\"opponent\",) ]:\n                # history  = self.generate_history([\"opponent\", \"action\"])  # \"action\" must be last\n                history  = self.generate_history([\"opponent\"])  \n                memories = self.build_memory(history) \n                patterns = self.find_patterns(history, memories)\n                if len(patterns): break\n            score, expected, pattern = self.find_best_pattern(patterns)\n            self.history['pattern'].append(pattern)    \n            if self.verbose:\n                pass\n                pass\n                pass\n                pass\n                pass\n                pass\n                pass\n\n        action = (expected + 1) % conf.signs\n        self.history['action'].append(action)\n        \n        if self.verbose:\n            pass\n        return int(action) \n    \n    \n    def random_action(self, obs, conf) -> int:\n        return random.randint(0, conf.signs-1)\n\n    def sequential_action(self, obs, conf) -> int:\n        return (obs.step + 1) % conf.signs\n\n    \n    def update_state(self, obs, conf):\n        self.history['step'].append( obs.step )\n        self.history['reward'].append( obs.reward )\n        if obs.step != 0:\n            self.history['opponent'].append( obs.lastOpponentAction )\n            # rotn_self = (self.history['opponent'][-1] - self.history['opponent'][-2]) % conf.signs \n            # rotn_opp  = (self.history['opponent'][-1] - self.history['action'][-1]))  % conf.signs\n            # self.history['rotn_self'].append( rotn_self )\n            # self.history['rotn_opp'].append( rotn_opp )\n        \n        \n    def generate_history(self, keys: List[str]) -> List[Tuple[int]]:\n        # Reverse order to correctly match up arrays\n        history = list(zip(*[ reversed(self.history[key]) for key in keys ]))\n        history = list(reversed(history))\n        return history\n    \n    \n    def build_memory(self, history: List[Tuple[int]]) -> List[ Dict[Tuple[int], List[int]] ]:\n        output    = [ dict() ] * self.min_memory\n        expecteds = self.generate_history([\"opponent\"])\n        for batch_size in range(self.min_memory, self.max_memory+1):\n            if batch_size >= len(history): break  # ignore batch sizes larger than history\n            output_batch    = defaultdict(lambda: [0,0,0])\n            history_batches  = list(batch(history, batch_size+1))\n            expected_batches = list(batch(expecteds, batch_size+1))\n            for n, (pattern, expected_batch) in enumerate(zip(history_batches, expected_batches)):\n                previous_pattern = tuple(pattern[:-1])\n                expected         = (expected_batch[-1][-1] or 0) % self.conf.signs  # assume \"action\" is always last \n                output_batch[ previous_pattern ][ expected ] += 1\n            output.append( dict(output_batch) )\n        return output\n\n    \n    def find_patterns(self, history: List[Tuple[int]], memories: List[ Dict[Tuple[int], List[int]] ]) -> List[Tuple[float, int, Tuple[int]]]:\n        patterns = []\n        for n in range(1, self.max_memory+1):\n            if n >= len(history): break\n                \n            pattern = tuple(history[-n:])\n            if pattern in memories[n]:\n                score    = np.std(memories[n][pattern])\n                expected = np.argmax(memories[n][pattern])\n                patterns.append( (score, expected, pattern) )\n        patterns = sorted(patterns, key=itemgetter(0), reverse=True)\n        return patterns\n    \n    \n    def find_best_pattern(self, patterns: List[Tuple[float, int, Tuple[int]]] ) -> Tuple[float, int, Tuple[int]]:\n        patterns       = sorted(patterns, key=itemgetter(0), reverse=True)\n        pattern_scores = self.get_pattern_scores()\n        for (score, expected, pattern) in patterns:\n            break\n            # if pattern in pattern_scores:\n            #     if pattern_scores[pattern] > self.threshold:\n            #         break\n            #     else:\n            #         expected += 1\n            #         break\n            # else:\n            #     break\n        else:\n            score    = 0.0\n            expected = self.random_action(self.obs, self.conf)\n            pattern  = tuple()\n        return score, expected, pattern\n    \n    \n    def get_pattern_scores(self):\n        pattern_rewards = defaultdict(list)\n        for reward, pattern in self.generate_history([\"reward\", \"pattern\"]):\n            pattern_rewards[pattern].append( reward )\n        pattern_scores = { pattern: np.mean(rewards) for patten, rewards in pattern_rewards.items() }\n        return pattern_scores\n                    \n            \n            \ninstance = MemoryPatterns()\ndef memory_patterns(obs, conf):\n    return instance(obs, conf)\n\n\n\n##### .\/reactionary.py #####\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\n\nlast_react_action = None\n\n\ndef reactionary(observation, configuration):\n    global last_react_action\n    if observation.step == 0:\n        last_react_action = random.randrange(0, configuration.signs)\n    elif get_score(last_react_action, observation.lastOpponentAction) <= 1:\n        last_react_action = (observation.lastOpponentAction + 1) % configuration.signs\n\n    return last_react_action\n\n\n##### .\/decision_tree_3.py #####\n\n\nimport time\nimport os\nimport random\nimport numpy as np\nfrom typing import List, Dict\nfrom sklearn.tree import DecisionTreeClassifier\n\ndef random_agent(observation, configuration):\n    return random.randint(0, configuration.signs-1)\n\ndef rock_agent(observation, configuration):\n    return 0\n\ndef paper_agent(observation, configuration):\n    return 1\n\ndef scissors_agent(observation, configuration):\n    return 2\n\ndef sequential_agent(observation, configuration):\n    return observation.step % configuration.signs\n\n\n\ndef get_winstats(decision_tree_history_2) -> Dict[str,int]:\n    total = len(decision_tree_history_2['action'])\n    wins = 0\n    draw = 0\n    loss = 0 \n    for n in range(total):\n        if   decision_tree_history_2['action'][n] == decision_tree_history_2['opponent'][n] + 1: wins +=  1\n        elif decision_tree_history_2['action'][n] == decision_tree_history_2['opponent'][n]:     draw +=  1\n        elif decision_tree_history_2['action'][n] == decision_tree_history_2['opponent'][n] - 1: loss +=  1\n    return { \"wins\": wins, \"draw\": draw, \"loss\": loss }\n\ndef get_winrate(decision_tree_history_2):\n    winstats = get_winstats(decision_tree_history_2)\n    winrate  = winstats['wins'] \/ (winstats['wins'] + winstats['loss']) if (winstats['wins'] + winstats['loss']) else 0\n    return winrate\n    \n    \n# Initialize starting decision_tree_history_2\ndecision_tree_history_2 = {\n    \"step\":        [],\n    \"prediction1\": [],\n    \"prediction2\": [],\n    \"expected\":    [],\n    \"action\":      [],\n    \"opponent\":    [],\n}\n\n# NOTE: adding statistics causes the DecisionTree to make random moves \ndef get_statistics(values) -> List[float]:\n    values = np.array(values)\n    return [\n        np.count_nonzero(values == n) \/ len(values)\n        if len(values) else 0.0\n        for n in [0,1,2]\n    ]\n\n\n# observation   =  {'step': 1, 'lastOpponentAction': 1}\n# configuration =  {'episodeSteps': 10, 'agentTimeout': 60, 'actTimeout': 1, 'runTimeout': 1200, 'isProduction': False, 'signs': 3}\ndef decision_tree_agent_3(observation, configuration, window=5, stages=2, random_freq=0.66, warmup_period=10, max_samples=1000):    \n    global decision_tree_history_2\n    warmup_period   = warmup_period  # if os.environ.get('KAGGLE_KERNEL_RUN_TYPE','') != 'Interactive' else 0\n    models          = [ None ] + [ DecisionTreeClassifier() ] * stages\n    \n    time_start      = time.perf_counter()\n    actions         = list(range(configuration.signs))  # [0,1,2]\n    \n    step            = observation.step\n    last_action     = decision_tree_history_2['action'][-1]          if len(decision_tree_history_2['action']) else 2\n    opponent_action = observation.lastOpponentAction if observation.step > 0   else 2\n        \n    if observation.step > 0:\n        decision_tree_history_2['opponent'].append(opponent_action)\n        \n    winrate  = get_winrate(decision_tree_history_2)\n    winstats = get_winstats(decision_tree_history_2)\n    \n    # Set default values     \n    prediction1 = random.randint(0,2)\n    prediction2 = random.randint(0,2)\n    prediction3 = random.randint(0,2)\n    expected    = random.randint(0,2)\n\n    # We need at least some turns of decision_tree_history_2 for DecisionTreeClassifier to work\n    if observation.step >= window:\n        # First we try to predict the opponents next move based on move decision_tree_history_2\n        # TODO: create windowed decision_tree_history_2\n        try:\n            n_start = max(1, len(decision_tree_history_2['opponent']) - window - max_samples) \n            # pass\n            if stages >= 1:\n                X = np.stack([\n                    np.array([\n                        # get_statistics(decision_tree_history_2['action'][:n+window]),\n                        # get_statistics(decision_tree_history_2['opponent'][:n-1+window]),\n                        decision_tree_history_2['action'][n:n+window], \n                        decision_tree_history_2['opponent'][n:n+window]\n                    ]).flatten()\n                    for n in range(n_start,len(decision_tree_history_2['opponent'])-window-warmup_period) \n                ])\n                Y = np.array([\n                    decision_tree_history_2['opponent'][n+window]\n                    for n in range(n_start,len(decision_tree_history_2['opponent'])-window-warmup_period) \n                ])  \n                Z = np.array([\n                    # get_statistics(decision_tree_history_2['action']),\n                    # get_statistics(decision_tree_history_2['opponent']),\n                    decision_tree_history_2['action'][-window+1:] + [ last_action ], \n                    decision_tree_history_2['opponent'][-window:] \n                ]).flatten().reshape(1, -1)\n\n                models[1].fit(X, Y)\n                expected = prediction1 = models[1].predict(Z)[0]\n\n            if stages >= 2:\n                # Now retrain including prediction decision_tree_history_2\n                X = np.stack([\n                    np.array([\n                        # get_statistics(decision_tree_history_2['action'][:n+window]),\n                        # get_statistics(decision_tree_history_2['prediction1'][:n+window]),\n                        # get_statistics(decision_tree_history_2['opponent'][:n-1+window]),\n                        decision_tree_history_2['action'][n:n+window], \n                        decision_tree_history_2['prediction1'][n:n+window],\n                        decision_tree_history_2['opponent'][n:n+window],\n                    ]).flatten()\n                    for n in range(n_start,len(decision_tree_history_2['opponent'])-window-warmup_period) \n                ])\n                Y = np.array([\n                    decision_tree_history_2['opponent'][n+window]\n                    for n in range(n_start,len(decision_tree_history_2['opponent'])-window-warmup_period) \n                ])  \n                Z = np.array([\n                    # get_statistics(decision_tree_history_2['action']),\n                    # get_statistics(decision_tree_history_2['prediction1']),\n                    # get_statistics(decision_tree_history_2['opponent']),\n                    decision_tree_history_2['action'][-window+1:]      + [ last_action ], \n                    decision_tree_history_2['prediction1'][-window+1:] + [ prediction1 ],\n                    decision_tree_history_2['opponent'][-window:] \n                ]).flatten().reshape(1, -1)\n\n                models[2].fit(X, Y)\n                expected = prediction2 = models[2].predict(Z)[0]\n\n            if stages >= 3:\n                # Now retrain including prediction decision_tree_history_2\n                X = np.stack([\n                    np.array([\n                        # get_statistics(decision_tree_history_2['action'][:n+window]),\n                        # get_statistics(decision_tree_history_2['prediction1'][:n+window]),\n                        # get_statistics(decision_tree_history_2['prediction2'][:n+window]),\n                        # get_statistics(decision_tree_history_2['opponent'][:n-1+window]),\n                        decision_tree_history_2['action'][n:n+window], \n                        decision_tree_history_2['prediction1'][n:n+window],\n                        decision_tree_history_2['prediction2'][n:n+window],\n                        decision_tree_history_2['opponent'][n:n+window],\n                    ]).flatten()\n                    for n in range(n_start,len(decision_tree_history_2['opponent'])-window-warmup_period) \n                ])\n                Y = np.array([\n                    decision_tree_history_2['opponent'][n+window]\n                    for n in range(n_start,len(decision_tree_history_2['opponent'])-window-warmup_period) \n                ])  \n                Z = np.array([\n                    # get_statistics(decision_tree_history_2['action']),\n                    # get_statistics(decision_tree_history_2['prediction1']),\n                    # get_statistics(decision_tree_history_2['prediction2']),\n                    # get_statistics(decision_tree_history_2['opponent']),\n                    decision_tree_history_2['action'][-window+1:]      + [ last_action ], \n                    decision_tree_history_2['prediction1'][-window+1:] + [ prediction1 ],\n                    decision_tree_history_2['prediction2'][-window+1:] + [ prediction2 ],\n                    decision_tree_history_2['opponent'][-window:] \n                ]).flatten().reshape(1, -1)\n\n                models[3].fit(X, Y)\n                expected = prediction3 = models[3].predict(Z)[0]\n        \n        except Exception as exception:\n            pass\n                    \n    # During the warmup period, play random to get a feel for the opponent \n    if (observation.step <= max(warmup_period,window)):\n        actor  = 'warmup'\n        action = random_agent(observation, configuration)    \n    \n    # Play a purely random move occasionally, which will hopefully distort any opponent statistics\n    elif (random.random() <= random_freq):\n        actor  = 'random'\n        action = random_agent(observation, configuration)\n        \n    # But mostly use DecisionTreeClassifier to predict the next move\n    else:\n        actor  = 'DecisionTree'\n        action = (expected + 1) % configuration.signs\n    \n    # Persist state\n    decision_tree_history_2['step'].append(step)\n    decision_tree_history_2['prediction1'].append(prediction1)\n    decision_tree_history_2['prediction2'].append(prediction2)\n    decision_tree_history_2['expected'].append(expected)\n    decision_tree_history_2['action'].append(action)\n    if observation.step == 0:  # keep arrays equal length\n        decision_tree_history_2['opponent'].append(random.randint(0, 2))\n\n\n    # Print debug information\n    time_taken = time.perf_counter() - time_start\n    # pass    \n    pass    \n    return int(action)\n\n\n\n##### .\/testing_please_ignore.py #####\n\ncode_ignore = compile(\n    \"\"\"\nfrom collections import defaultdict\nimport operator\nimport random\nif input == \"\":\n    score  = {'RR': 0, 'PP': 0, 'SS': 0, \\\n              'PR': 1, 'RS': 1, 'SP': 1, \\\n              'RP': -1, 'SR': -1, 'PS': -1,}\n    cscore = {'RR': 'r', 'PP': 'r', 'SS': 'r', \\\n              'PR': 'b', 'RS': 'b', 'SP': 'b', \\\n              'RP': 'c', 'SR': 'c', 'PS': 'c',}\n    beat = {'P': 'S', 'S': 'R', 'R': 'P'}\n    cede = {'P': 'R', 'S': 'P', 'R': 'S'}\n    rps = ['R', 'P', 'S']\n    wlt = {1: 0, -1: 1, 0: 2}\n\n    def counter_prob(probs):\n        weighted_list = []\n        for h in rps:\n            weighted = 0\n            for p in probs.keys():\n                points = score[h + p]\n                prob = probs[p]\n                weighted += points * prob\n            weighted_list.append((h, weighted))\n\n        return max(weighted_list, key=operator.itemgetter(1))[0]\n\n    played_probs = defaultdict(lambda: 1)\n    dna_probs = [\n        defaultdict(lambda: defaultdict(lambda: 1)) for i in range(18)\n    ]\n\n    wlt_probs = [defaultdict(lambda: 1) for i in range(9)]\n\n    answers = [{'c': 1, 'b': 1, 'r': 1} for i in range(12)]\n\n    patterndict = [defaultdict(str) for i in range(6)]\n\n    consec_strat_usage = [[0] * 6, [0] * 6,\n                          [0] * 6]  #consecutive strategy usage\n    consec_strat_candy = [[], [], []]  #consecutive strategy candidates\n\n    output = random.choice(rps)\n    histories = [\"\", \"\", \"\"]\n    dna = [\"\" for i in range(12)]\n\n    sc = 0\n    strats = [[] for i in range(3)]\nelse:\n    prev_sc = sc\n\n    sc = score[output + input]\n    for j in range(3):\n        prev_strats = strats[j][:]\n        for i, c in enumerate(consec_strat_candy[j]):\n            if c == input:\n                consec_strat_usage[j][i] += 1\n            else:\n                consec_strat_usage[j][i] = 0\n        m = max(consec_strat_usage[j])\n        strats[j] = [\n            i for i, c in enumerate(consec_strat_candy[j])\n            if consec_strat_usage[j][i] == m\n        ]\n\n        for s1 in prev_strats:\n            for s2 in strats[j]:\n                wlt_probs[j * 3 + wlt[prev_sc]][chr(s1) + chr(s2)] += 1\n\n        if dna[2 * j + 0] and dna[2 * j + 1]:\n            answers[2 * j + 0][cscore[input + dna[2 * j + 0]]] += 1\n            answers[2 * j + 1][cscore[input + dna[2 * j + 1]]] += 1\n        if dna[2 * j + 6] and dna[2 * j + 7]:\n            answers[2 * j + 6][cscore[input + dna[2 * j + 6]]] += 1\n            answers[2 * j + 7][cscore[input + dna[2 * j + 7]]] += 1\n\n        for length in range(min(10, len(histories[j])), 0, -2):\n            pattern = patterndict[2 * j][histories[j][-length:]]\n            if pattern:\n                for length2 in range(min(10, len(pattern)), 0, -2):\n                    patterndict[2 * j +\n                                1][pattern[-length2:]] += output + input\n            patterndict[2 * j][histories[j][-length:]] += output + input\n    played_probs[input] += 1\n    dna_probs[0][dna[0]][input] += 1\n    dna_probs[1][dna[1]][input] += 1\n    dna_probs[2][dna[1] + dna[0]][input] += 1\n    dna_probs[9][dna[6]][input] += 1\n    dna_probs[10][dna[6]][input] += 1\n    dna_probs[11][dna[7] + dna[6]][input] += 1\n\n    histories[0] += output + input\n    histories[1] += input\n    histories[2] += output\n\n    dna = [\"\" for i in range(12)]\n    for j in range(3):\n        for length in range(min(10, len(histories[j])), 0, -2):\n            pattern = patterndict[2 * j][histories[j][-length:]]\n            if pattern != \"\":\n                dna[2 * j + 1] = pattern[-2]\n                dna[2 * j + 0] = pattern[-1]\n                for length2 in range(min(10, len(pattern)), 0, -2):\n                    pattern2 = patterndict[2 * j + 1][pattern[-length2:]]\n                    if pattern2 != \"\":\n                        dna[2 * j + 7] = pattern2[-2]\n                        dna[2 * j + 6] = pattern2[-1]\n                        break\n                break\n\n    probs = {}\n    for hand in rps:\n        probs[hand] = played_probs[hand]\n\n    for j in range(3):\n        if dna[j * 2] and dna[j * 2 + 1]:\n            for hand in rps:\n                probs[hand] *= dna_probs[j*3+0][dna[j*2+0]][hand] * \\\n                               dna_probs[j*3+1][dna[j*2+1]][hand] * \\\n                      dna_probs[j*3+2][dna[j*2+1]+dna[j*2+0]][hand]\n                probs[hand] *= answers[j*2+0][cscore[hand+dna[j*2+0]]] * \\\n                               answers[j*2+1][cscore[hand+dna[j*2+1]]]\n            consec_strat_candy[j] = [dna[j*2+0], beat[dna[j*2+0]], cede[dna[j*2+0]],\\\n                                     dna[j*2+1], beat[dna[j*2+1]], cede[dna[j*2+1]]]\n            strats_for_hand = {'R': [], 'P': [], 'S': []}\n            for i, c in enumerate(consec_strat_candy[j]):\n                strats_for_hand[c].append(i)\n            pr = wlt_probs[wlt[sc] + 3 * j]\n            for hand in rps:\n                for s1 in strats[j]:\n                    for s2 in strats_for_hand[hand]:\n                        probs[hand] *= pr[chr(s1) + chr(s2)]\n        else:\n            consec_strat_candy[j] = []\n    for j in range(3):\n        if dna[j * 2 + 6] and dna[j * 2 + 7]:\n            for hand in rps:\n                probs[hand] *= dna_probs[j*3+9][dna[j*2+6]][hand] * \\\n                               dna_probs[j*3+10][dna[j*2+7]][hand] * \\\n                      dna_probs[j*3+11][dna[j*2+7]+dna[j*2+6]][hand]\n                probs[hand] *= answers[j*2+6][cscore[hand+dna[j*2+6]]] * \\\n                               answers[j*2+7][cscore[hand+dna[j*2+7]]]\n\n    output = counter_prob(probs)\n\"\"\", '<string>', 'exec')\ngg_ignore = {}\n\n\ndef testing_please_ignore(observation, configuration):\n    global gg_ignore\n    global code_ignore\n    inp = ''\n    try:\n        inp = 'RPS'[observation.lastOpponentAction]\n    except:\n        pass\n    gg_ignore['input'] = inp\n    exec(code_ignore, gg_ignore)\n    return {'R': 0, 'P': 1, 'S': 2}[gg_ignore['output']]\n\n\n\n##### .\/pi.py #####\n\nimport re\n\nPI = \"3.14159 26535 89793 23846 26433 83279 50288 41971 69399 37510 58209 74944 59230 78164 06286 20899 86280 34825 34211 70679 82148 08651 32823 06647 09384 46095 50582 23172 53594 08128 48111 74502 84102 70193 85211 05559 64462 29489 54930 38196 44288 10975 66593 34461 28475 64823 37867 83165 27120 19091 45648 56692 34603 48610 45432 66482 13393 60726 02491 41273 72458 70066 06315 58817 48815 20920 96282 92540 91715 36436 78925 90360 01133 05305 48820 46652 13841 46951 94151 16094 33057 27036 57595 91953 09218 61173 81932 61179 31051 18548 07446 23799 62749 56735 18857 52724 89122 79381 83011 94912 98336 73362 44065 66430 86021 39494 63952 24737 19070 21798 60943 70277 05392 17176 29317 67523 84674 81846 76694 05132 00056 81271 45263 56082 77857 71342 75778 96091 73637 17872 14684 40901 22495 34301 46549 58537 10507 92279 68925 89235 42019 95611 21290 21960 86403 44181 59813 62977 47713 09960 51870 72113 49999 99837 29780 49951 05973 17328 16096 31859 50244 59455 34690 83026 42522 30825 33446 85035 26193 11881 71010 00313 78387 52886 58753 32083 81420 61717 76691 47303 59825 34904 28755 46873 11595 62863 88235 37875 93751 95778 18577 80532 17122 68066 13001 92787 66111 95909 21642 01989 38095 25720 10654 85863 27886 59361 53381 82796 82303 01952 03530 18529 68995 77362 25994 13891 24972 17752 83479 13151 55748 57242 45415 06959 50829 53311 68617 27855 88907 50983 81754 63746 49393 19255 06040 09277 01671 13900 98488 24012 85836 16035 63707 66010 47101 81942 95559 61989 46767 83744 94482 55379 77472 68471 04047 53464 62080 46684 25906 94912 93313 67702 89891 52104 75216 20569 66024 05803 81501 93511 25338 24300 35587 64024 74964 73263 91419 92726 04269 92279 67823 54781 63600 93417 21641 21992 45863 15030 28618 29745 55706 74983 85054 94588 58692 69956 90927 21079 75093 02955 32116 53449 87202 75596 02364 80665 49911 98818 34797 75356 63698 07426 54252 78625 51818 41757 46728 90977 77279 38000 81647 06001 61452 49192 17321 72147 72350 14144 19735 68548 16136 11573 52552 13347 57418 49468 43852 33239 07394 14333 45477 62416 86251 89835 69485 56209 92192 22184 27255 02542 56887 67179 04946 01653 46680 49886 27232 79178 60857 84383 82796 79766 81454 10095 38837 86360 95068 00642 25125 20511 73929 84896 08412 84886 26945 60424 19652 85022 21066 11863 06744 27862 20391 94945 04712 37137 86960 95636 43719 17287 46776 46575 73962 41389 08658 32645 99581 33904 78027 59009 94657 64078 95126 94683 98352 59570 98258 22620 52248 94077 26719 47826 84826 01476 99090 26401 36394 43745 53050 68203 49625 24517 49399 65143 14298 09190 65925 09372 21696 46151 57098 58387 41059 78859 59772 97549 89301 61753 92846 81382 68683 86894 27741 55991 85592 52459 53959 43104 99725 24680 84598 72736 44695 84865 38367 36222 62609 91246 08051 24388 43904 51244 13654 97627 80797 71569 14359 97700 12961 60894 41694 86855 58484 06353 42207 22258 28488 64815 84560 28506 01684 27394 52267 46767 88952 52138 52254 99546 66727 82398 64565 96116 35488 62305 77456 49803 55936 34568 17432 41125 15076 06947 94510 96596 09402 52288 79710 89314 56691 36867 22874 89405 60101 50330 86179 28680 92087 47609 17824 93858 90097 14909 67598 52613 65549 78189 31297 84821 68299 89487 22658 80485 75640 14270 47755 51323 79641 45152 37462 34364 54285 84447 95265 86782 10511 41354 73573 95231 13427 16610 21359 69536 23144 29524 84937 18711 01457 65403 59027 99344 03742 00731 05785 39062 19838 74478 08478 48968 33214 45713 86875 19435 06430 21845 31910 48481 00537 06146 80674 91927 81911 97939 95206 14196 63428 75444 06437 45123 71819 21799 98391 01591 95618 14675 14269 12397 48940 90718 64942 31961 56794 52080 95146 55022 52316 03881 93014 20937 62137 85595 66389 37787 08303 90697 92077 34672 21825 62599 66150 14215 03068 03844 77345 49202 60541 46659 25201 49744 28507 32518 66600 21324 34088 19071 04863 31734 64965 14539 05796 26856 10055 08106 65879 69981 63574 73638 40525 71459 10289 70641 40110 97120 62804 39039 75951 56771 57700 42033 78699 36007 23055 87631 76359 42187 31251 47120 53292 81918 26186 12586 73215 79198 41484 88291 64470 60957 52706 95722 09175 67116 72291 09816 90915 28017 35067 12748 58322 28718 35209 35396 57251 21083 57915 13698 82091 44421 00675 10334 67110 31412 67111 36990 86585 16398 31501 97016 51511 68517 14376 57618 35155 65088 49099 89859 98238 73455 28331 63550 76479 18535 89322 61854 89632 13293 30898 57064 20467 52590 70915 48141 65498 59461 63718 02709 81994 30992 44889 57571 28289 05923 23326 09729 97120 84433 57326 54893 82391 19325 97463 66730 58360 41428 13883 03203 82490 37589 85243 74417 02913 27656 18093 77344 40307 07469 21120 19130 20330 38019 76211 01100 44929 32151 60842 44485 96376 69838 95228 68478 31235 52658 21314 49576 85726 24334 41893 03968 64262 43410 77322 69780 28073 18915 44110 10446 82325 27162 01052 65227 21116 60396 66557 30925 47110 55785 37634 66820 65310 98965 26918 62056 47693 12570 58635 66201 85581 00729 36065 98764 86117 91045 33488 50346 11365 76867 53249 44166 80396 26579 78771 85560 84552 96541 26654 08530 61434 44318 58676 97514 56614 06800 70023 78776 59134 40171 27494 70420 56223 05389 94561 31407 11270 00407 85473 32699 39081 45466 46458 80797 27082 66830 63432 85878 56983 05235 80893 30657 57406 79545 71637 75254 20211 49557 61581 40025 01262 28594 13021 64715 50979 25923 09907 96547 37612 55176 56751 35751 78296 66454 77917 45011 29961 48903 04639 94713 29621 07340 43751 89573 59614 58901 93897 13111 79042 97828 56475 03203 19869 15140 28708 08599 04801 09412 14722 13179 47647 77262 24142 54854 54033 21571 85306 14228 81375 85043 06332 17518 29798 66223 71721 59160 77166 92547 48738 98665 49494 50114 65406 28433 66393 79003 97692 65672 14638 53067 36096 57120 91807 63832 71664 16274 88880 07869 25602 90228 47210 40317 21186 08204 19000 42296 61711 96377 92133 75751 14959 50156 60496 31862 94726 54736 42523 08177 03675 15906 73502 35072 83540 56704 03867 43513 62222 47715 89150 49530 98444 89333 09634 08780 76932 59939 78054 19341 44737 74418 42631 29860 80998 88687 41326 04721 56951 62396 58645 73021 63159 81931 95167 35381 29741 67729 47867 24229 24654 36680 09806 76928 23828 06899 64004 82435 40370 14163 14965 89794 09243 23789 69070 69779 42236 25082 21688 95738 37986 23001 59377 64716 51228 93578 60158 81617 55782 97352 33446 04281 51262 72037 34314 65319 77774 16031 99066 55418 76397 92933 44195 21541 34189 94854 44734 56738 31624 99341 91318 14809 27777 10386 38773 43177 20754 56545 32207 77092 12019 05166 09628 04909 26360 19759 88281 61332 31666 36528 61932 66863 36062 73567 63035 44776 28035 04507 77235 54710 58595 48702 79081 43562 40145 17180 62464 36267 94561 27531 81340 78330 33625 42327 83944 97538 24372 05835 31147 71199 26063 81334 67768 79695 97030 98339 13077 10987 04085 91337 46414 42822 77263 46594 70474 58784 77872 01927 71528 07317 67907 70715 72134 44730 60570 07334 92436 93113 83504 93163 12840 42512 19256 51798 06941 13528 01314 70130 47816 43788 51852 90928 54520 11658 39341 96562 13491 43415 95625 86586 55705 52690 49652 09858 03385 07224 26482 93972 85847 83163 05777 75606 88876 44624 82468 57926 03953 52773 48030 48029 00587 60758 25104 74709 16439 61362 67604 49256 27420 42083 20856 61190 62545 43372 13153 59584 50687 72460 29016 18766 79524 06163 42522 57719 54291 62991 93064 55377 99140 37340 43287 52628 88963 99587 94757 29174 64263 57455 25407 90914 51357 11136 94109 11939 32519 10760 20825 20261 87985 31887 70584 29725 91677 81314 96990 09019 21169 71737 27847 68472 68608 49003 37702 42429 16513 00500 51683 23364 35038 95170 29893 92233 45172 20138 12806 96501 17844 08745 19601 21228 59937 16231 30171 14448 46409 03890 64495 44400 61986 90754 85160 26327 50529 83491 87407 86680 88183 38510 22833 45085 04860 82503 93021 33219 71551 84306 35455 00766 82829 49304 13776 55279 39751 75461 39539 84683 39363 83047 46119 96653 85815 38420 56853 38621 86725 23340 28308 71123 28278 92125 07712 62946 32295 63989 89893 58211 67456 27010 21835 64622 01349 67151 88190 97303 81198 00497 34072 39610 36854 06643 19395 09790 19069 96395 52453 00545 05806 85501 95673 02292 19139 33918 56803 44903 98205 95510 02263 53536 19204 19947 45538 59381 02343 95544 95977 83779 02374 21617 27111 72364 34354 39478 22181 85286 24085 14006 66044 33258 88569 86705 43154 70696 57474 58550 33232 33421 07301 54594 05165 53790 68662 73337 99585 11562 57843 22988 27372 31989 87571 41595 78111 96358 33005 94087 30681 21602 87649 62867 44604 77464 91599 50549 73742 56269 01049 03778 19868 35938 14657 41268 04925 64879 85561 45372 34786 73303 90468 83834 36346 55379 49864 19270 56387 29317 48723 32083 76011 23029 91136 79386 27089 43879 93620 16295 15413 37142 48928 30722 01269 01475 46684 76535 76164 77379 46752 00490 75715 55278 19653 62132 39264 06160 13635 81559 07422 02020 31872 77605 27721 90055 61484 25551 87925 30343 51398 44253 22341 57623 36106 42506 39049 75008 65627 10953 59194 65897 51413 10348 22769 30624 74353 63256 91607 81547 81811 52843 66795 70611 08615 33150 44521 27473 92454 49454 23682 88606 13408 41486 37767 00961 20715 12491 40430 27253 86076 48236 34143 34623 51897 57664 52164 13767 96903 14950 19108 57598 44239 19862 91642 19399 49072 36234 64684 41173 94032 65918 40443 78051 33389 45257 42399 50829 65912 28508 55582 15725 03107 12570 12668 30240 29295 25220 11872 67675 62204 15420 51618 41634 84756 51699 98116 14101 00299 60783 86909 29160 30288 40026 91041 40792 88621 50784 24516 70908 70006 99282 12066 04183 71806 53556 72525 32567 53286 12910 42487 76182 58297 65157 95984 70356 22262 93486 00341 58722 98053 49896 50226 29174 87882 02734 20922 22453 39856 26476 69149 05562 84250 39127 57710 28402 79980 66365 82548 89264 88025 45661 01729 67026 64076 55904 29099 45681 50652 65305 37182 94127 03369 31378 51786 09040 70866 71149 65583 43434 76933 85781 71138 64558 73678 12301 45876 87126 60348 91390 95620 09939 36103 10291 61615 28813 84379 09904 23174 73363 94804 57593 14931 40529 76347 57481 19356 70911 01377 51721 00803 15590 24853 09066 92037 67192 20332 29094 33467 68514 22144 77379 39375 17034 43661 99104 03375 11173 54719 18550 46449 02636 55128 16228 82446 25759 16333 03910 72253 83742 18214 08835 08657 39177 15096 82887 47826 56995 99574 49066 17583 44137 52239 70968 34080 05355 98491 75417 38188 39994 46974 86762 65516 58276 58483 58845 31427 75687 90029 09517 02835 29716 34456 21296 40435 23117 60066 51012 41200 65975 58512 76178 58382 92041 97484 42360 80071 93045 76189 32349 22927 96501 98751 87212 72675 07981 25547 09589 04556 35792 12210 33346 69749 92356 30254 94780 24901 14195 21238 28153 09114 07907 38602 51522 74299 58180 72471 62591 66854 51333 12394 80494 70791 19153 26734 30282 44186 04142 63639 54800 04480 02670 49624 82017 92896 47669 75831 83271 31425 17029 69234 88962 76684 40323 26092 75249 60357 99646 92565 04936 81836 09003 23809 29345 95889 70695 36534 94060 34021 66544 37558 90045 63288 22505 45255 64056 44824 65151 87547 11962 18443 96582 53375 43885 69094 11303 15095 26179 37800 29741 20766 51479 39425 90298 96959 46995 56576 12186 56196 73378 62362 56125 21632 08628 69222 10327 48892 18654 36480 22967 80705 76561 51446 32046 92790 68212 07388 37781 42335 62823 60896 32080 68222 46801 22482 61177 18589 63814 09183 90367 36722 20888 32151 37556 00372 79839 40041 52970 02878 30766 70944 47456 01345 56417 25437 09069 79396 12257 14298 94671 54357 84687 88614 44581 23145 93571 98492 25284 71605 04922 12424 70141 21478 05734 55105 00801 90869 96033 02763 47870 81081 75450 11930 71412 23390 86639 38339 52942 57869 05076 43100 63835 19834 38934 15961 31854 34754 64955 69781 03829 30971 64651 43840 70070 73604 11237 35998 43452 25161 05070 27056 23526 60127 64848 30840 76118 30130 52793 20542 74628 65403 60367 45328 65105 70658 74882 25698 15793 67897 66974 22057 50596 83440 86973 50201 41020 67235 85020 07245 22563 26513 41055 92401 90274 21624 84391 40359 98953 53945 90944 07046 91209 14093 87001 26456 00162 37428 80210 92764 57931 06579 22955 24988 72758 46101 26483 69998 92256 95968 81592 05600 10165 52563 7567\"\nPI = re.sub('[^1-9]', '', PI)\n\n# observation   =  {'step': 1, 'lastOpponentAction': 1}\n# configuration =  {'episodeSteps': 10, 'agentTimeout': 60, 'actTimeout': 1, 'runTimeout': 1200, 'isProduction': False, 'signs': 3}\ndef pi_agent(observation, configuration):    \n    action = int(PI[observation.step]) % configuration.signs\n    return int(action)\n\n\n\n##### .\/statistical.py #####\n\n\nimport random\nimport pydash\nfrom collections import Counter\n\n# Create a small amount of starting statistical_history\nstatistical_history = {\n    \"guess\":      [0,1,2],\n    \"prediction\": [0,1,2],\n    \"expected\":   [0,1,2],\n    \"action\":     [1,2,0],\n    \"opponent\":   [0,1],\n    \"rotn\":       [0,1],\n}\n# observation   =  {'step': 1, 'lastOpponentAction': 1}\n# configuration =  {'episodeSteps': 1000, 'agentTimeout': 60, 'actTimeout': 1, 'runTimeout': 1200, 'isProduction': False, 'signs': 3}\ndef statistical_prediction_agent(observation, configuration):    \n    global statistical_history\n    actions          = list(range(configuration.signs))  # [0,1,2]\n    last_action      = statistical_history['action'][-1]\n    prev_opp_action  = statistical_history['opponent'][-1]\n    opponent_action  = observation.lastOpponentAction if observation.step > 0 else 2\n    rotn             = (opponent_action - prev_opp_action) % configuration.signs\n\n    statistical_history['opponent'].append(opponent_action)\n    statistical_history['rotn'].append(rotn)\n    \n    # Make weighted random guess based on the complete move statistical_history, weighted towards relative moves based on our last action \n    move_frequency   = Counter(statistical_history['rotn'])\n    action_frequency = Counter(zip(statistical_history['action'], statistical_history['rotn'])) \n    move_weights     = [   move_frequency.get(n, 1) \n                         + action_frequency.get((last_action,n), 1) \n                         for n in range(configuration.signs) ] \n    guess            = random.choices( population=actions, weights=move_weights, k=1 )[0]\n    \n    # Compare our guess to how our opponent actually played\n    guess_frequency  = Counter(zip(statistical_history['guess'], statistical_history['rotn']))\n    guess_weights    = [ guess_frequency.get((guess,n), 1) \n                         for n in range(configuration.signs) ]\n    prediction       = random.choices( population=actions, weights=guess_weights, k=1 )[0]\n\n    # Repeat, but based on how many times our prediction was correct\n    pred_frequency   = Counter(zip(statistical_history['prediction'], statistical_history['rotn']))\n    pred_weights     = [ pred_frequency.get((prediction,n), 1) \n                         for n in range(configuration.signs) ]\n    expected         = random.choices( population=actions, weights=pred_weights, k=1 )[0]\n\n    \n    # Slowly decay to 50% pure randomness as the match progresses\n    pure_random_chance = observation.step \/ (configuration.episodeSteps * 2)\n    if random.random() < pure_random_chance:\n        action = random.randint(0, configuration.signs-1)\n        is_pure_random_chance = True\n    else:\n        # Play the +1 counter move\n        # action = (expected + 1) % configuration.signs                  # without rotn\n        action = (opponent_action + expected + 1) % configuration.signs  # using   rotn\n        is_pure_random_chance = False\n    \n    # Persist state\n    statistical_history['guess'].append(guess)\n    statistical_history['prediction'].append(prediction)\n    statistical_history['expected'].append(expected)\n    statistical_history['action'].append(action)\n\n    # Print debug information\n    pass\n    pass\n    pass\n    pass\n    pass\n    pass\n    pass\n    pass\n    \n    return action\n\n\n\n##### .\/dllu1.py #####\n\ncode_dllu1 = compile(\n    \"\"\"\n# see also www.dllu.net\/rps\n# remember, rpsdllu1_agentner.py is extremely useful for offline testing, \n# here's a screenshot: http:\/\/i.imgur.com\/DcO9M.png\nimport random\nnumPre = 30\nnumMeta = 6\nif not input:\n    limit = 8\n    beat={'R':'P','P':'S','S':'R'}\n    moves=['','','','']\n    pScore=[[5]*numPre,[5]*numPre,[5]*numPre,[5]*numPre,[5]*numPre,[5]*numPre]\n    centrifuge={'RP':0,'PS':1,'SR':2,'PR':3,'SP':4,'RS':5,'RR':6,'PP':7,'SS':8}\n    centripete={'R':0,'P':1,'S':2}\n    soma = [0,0,0,0,0,0,0,0,0];\n    rps = [1,1,1];\n    a=\"RPS\"\n    best = [0,0,0];\n    length=0\n    p=[random.choice(\"RPS\")]*numPre\n    m=[random.choice(\"RPS\")]*numMeta\n    mScore=[5,2,5,2,4,2]\nelse:\n    for i in range(numPre):\n        pp = p[i]\n        bpp = beat[pp]\n        bbpp = beat[bpp]\n        pScore[0][i]=0.9*pScore[0][i]+((input==pp)-(input==bbpp))*3\n        pScore[1][i]=0.9*pScore[1][i]+((output==pp)-(output==bbpp))*3\n        pScore[2][i]=0.87*pScore[2][i]+(input==pp)*3.3-(input==bpp)*1.2-(input==bbpp)*2.3\n        pScore[3][i]=0.87*pScore[3][i]+(output==pp)*3.3-(output==bpp)*1.2-(output==bbpp)*2.3\n        pScore[4][i]=(pScore[4][i]+(input==pp)*3)*(1-(input==bbpp))\n        pScore[5][i]=(pScore[5][i]+(output==pp)*3)*(1-(output==bbpp))\n    for i in range(numMeta):\n        mScore[i]=0.96*(mScore[i]+(input==m[i])-(input==beat[beat[m[i]]]))\n    soma[centrifuge[input+output]] +=1;\n    rps[centripete[input]] +=1;\n    moves[0]+=str(centrifuge[input+output])\n    moves[1]+=input\n    moves[2]+=output\n    length+=1\n    for y in range(3):\n        j=min([length,limit])\n        while j>=1 and not moves[y][length-j:length] in moves[y][0:length-1]:\n            j-=1\n        i = moves[y].rfind(moves[y][length-j:length],0,length-1)\n        p[0+2*y] = moves[1][j+i] \n        p[1+2*y] = beat[moves[2][j+i]]\n    j=min([length,limit])\n    while j>=2 and not moves[0][length-j:length-1] in moves[0][0:length-2]:\n        j-=1\n    i = moves[0].rfind(moves[0][length-j:length-1],0,length-2)\n    if j+i>=length:\n        p[6] = p[7] = random.choice(\"RPS\")\n    else:\n        p[6] = moves[1][j+i] \n        p[7] = beat[moves[2][j+i]]\n        \n    best[0] = soma[centrifuge[output+'R']]*rps[0]\/rps[centripete[output]]\n    best[1] = soma[centrifuge[output+'P']]*rps[1]\/rps[centripete[output]]\n    best[2] = soma[centrifuge[output+'S']]*rps[2]\/rps[centripete[output]]\n    p[8] = p[9] = a[best.index(max(best))]\n    \n    for i in range(10,numPre):\n        p[i]=beat[beat[p[i-10]]]\n        \n    for i in range(0,numMeta,2):\n        m[i]=       p[pScore[i  ].index(max(pScore[i  ]))]\n        m[i+1]=beat[p[pScore[i+1].index(max(pScore[i+1]))]]\noutput = beat[m[mScore.index(max(mScore))]]\nif max(mScore)<0.07 or random.randint(3,40)>length:\n    output=beat[random.choice(\"RPS\")]\n\"\"\", '<string>', 'exec')\ngg_dllu1 = {}\n\n\ndef dllu1_agent(observation, configuration):\n    global gg_dllu1\n    global code_dllu1\n    inp = ''\n    try:\n        inp = 'RPS'[observation.lastOpponentAction]\n    except:\n        pass\n    gg_dllu1['input'] = inp\n    exec(code_dllu1, gg_dllu1)\n    return {'R': 0, 'P': 1, 'S': 2}[gg_dllu1['output']]\n\n\n\n##### .\/greenberg.py #####\n\n\n# greenberg roshambo bot, winner of 2nd annual roshambo programming competition\n# http:\/\/webdocs.cs.ualberta.ca\/~darse\/rsbpc.html\n\n# original source by Andrzej Nagorko\n# http:\/\/www.mathpuzzle.com\/greenberg.c\n\n# Python translation by Travis Erdman\n# https:\/\/github.com\/erdman\/roshambo\n\nimport random\nfrom operator import itemgetter\n# from itertools import izip\nizip   = zip   # BUGFIX: izip   is python2\nxrange = range # BUGFIX: xrange is python2\n\nrps_to_text  = ('rock','paper','scissors')\nrps_to_num   = {'rock':0, 'paper':1, 'scissors':2}\n\ndef player(my_moves, opp_moves):\n    wins_with    = (1,2,0)  # superior\n    best_without = (2,0,1)  # inferior\n\n    lengths = (10, 20, 30, 40, 49, 0)\n    p_random = random.choice([0,1,2])  #called 'guess' in iocaine\n\n    TRIALS = 1000\n    score_table =((0,-1,1),(1,0,-1),(-1,1,0))\n    T = len(opp_moves)  #so T is number of trials completed\n\n    def min_index(values):\n        return min(enumerate(values), key=itemgetter(1))[0]\n\n    def max_index(values):\n        return max(enumerate(values), key=itemgetter(1))[0]\n\n    def find_best_prediction(l):  # l = len\n        bs = -TRIALS\n        bp = 0\n        if player.p_random_score > bs:\n            bs = player.p_random_score\n            bp = p_random\n        for i in xrange(3):\n            for j in xrange(24):\n                for k in xrange(4):\n                    new_bs = player.p_full_score[T%50][j][k][i] - (player.p_full_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (player.p_full[j][k] + i) % 3\n                for k in xrange(2):\n                    new_bs = player.r_full_score[T%50][j][k][i] - (player.r_full_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (player.r_full[j][k] + i) % 3\n            for j in xrange(2):\n                for k in xrange(2):\n                    new_bs = player.p_freq_score[T%50][j][k][i] - (player.p_freq_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (player.p_freq[j][k] + i) % 3\n                    new_bs = player.r_freq_score[T%50][j][k][i] - (player.r_freq_score[(50+T-l)%50][j][k][i] if l else 0)\n                    if new_bs > bs:\n                        bs = new_bs\n                        bp = (player.r_freq[j][k] + i) % 3\n        return bp\n\n\n    if not my_moves:\n        player.opp_history = [0]  #pad to match up with 1-based move indexing in original\n        player.my_history = [0]\n        player.gear = [[0] for _ in xrange(24)]\n        # init()\n        player.p_random_score = 0\n        player.p_full_score = [[[[0 for i in xrange(3)] for k in xrange(4)] for j in xrange(24)] for l in xrange(50)]\n        player.r_full_score = [[[[0 for i in xrange(3)] for k in xrange(2)] for j in xrange(24)] for l in xrange(50)]\n        player.p_freq_score = [[[[0 for i in xrange(3)] for k in xrange(2)] for j in xrange(2)] for l in xrange(50)]\n        player.r_freq_score = [[[[0 for i in xrange(3)] for k in xrange(2)] for j in xrange(2)] for l in xrange(50)]\n        player.s_len = [0] * 6\n\n        player.p_full = [[0,0,0,0] for _ in xrange(24)]\n        player.r_full = [[0,0] for _ in xrange(24)]\n    else:\n        player.my_history.append(rps_to_num[my_moves[-1]])\n        player.opp_history.append(rps_to_num[opp_moves[-1]])\n        # update_scores()\n        player.p_random_score += score_table[p_random][player.opp_history[-1]]\n        player.p_full_score[T%50] = [[[player.p_full_score[(T+49)%50][j][k][i] + score_table[(player.p_full[j][k] + i) % 3][player.opp_history[-1]] for i in xrange(3)] for k in xrange(4)] for j in xrange(24)]\n        player.r_full_score[T%50] = [[[player.r_full_score[(T+49)%50][j][k][i] + score_table[(player.r_full[j][k] + i) % 3][player.opp_history[-1]] for i in xrange(3)] for k in xrange(2)] for j in xrange(24)]\n        player.p_freq_score[T%50] = [[[player.p_freq_score[(T+49)%50][j][k][i] + score_table[(player.p_freq[j][k] + i) % 3][player.opp_history[-1]] for i in xrange(3)] for k in xrange(2)] for j in xrange(2)]\n        player.r_freq_score[T%50] = [[[player.r_freq_score[(T+49)%50][j][k][i] + score_table[(player.r_freq[j][k] + i) % 3][player.opp_history[-1]] for i in xrange(3)] for k in xrange(2)] for j in xrange(2)]\n        player.s_len = [s + score_table[p][player.opp_history[-1]] for s,p in izip(player.s_len,player.p_len)]\n\n\n    # update_history_hash()\n    if not my_moves:\n        player.my_history_hash = [[0],[0],[0],[0]]\n        player.opp_history_hash = [[0],[0],[0],[0]]\n    else:\n        player.my_history_hash[0].append(player.my_history[-1])\n        player.opp_history_hash[0].append(player.opp_history[-1])\n        for i in xrange(1,4):\n            player.my_history_hash[i].append(player.my_history_hash[i-1][-1] * 3 + player.my_history[-1])\n            player.opp_history_hash[i].append(player.opp_history_hash[i-1][-1] * 3 + player.opp_history[-1])\n\n\n    #make_predictions()\n\n    for i in xrange(24):\n        player.gear[i].append((3 + player.opp_history[-1] - player.p_full[i][2]) % 3)\n        if T > 1:\n            player.gear[i][T] += 3 * player.gear[i][T-1]\n        player.gear[i][T] %= 9 # clearly there are 9 different gears, but original code only allocated 3 gear_freq's\n                               # code apparently worked, but got lucky with undefined behavior\n                               # I fixed by allocating gear_freq with length = 9\n    if not my_moves:\n        player.freq = [[0,0,0],[0,0,0]]\n        value = [[0,0,0],[0,0,0]]\n    else:\n        player.freq[0][player.my_history[-1]] += 1\n        player.freq[1][player.opp_history[-1]] += 1\n        value = [[(1000 * (player.freq[i][2] - player.freq[i][1])) \/ float(T),\n                  (1000 * (player.freq[i][0] - player.freq[i][2])) \/ float(T),\n                  (1000 * (player.freq[i][1] - player.freq[i][0])) \/ float(T)] for i in xrange(2)]\n    player.p_freq = [[wins_with[max_index(player.freq[i])], wins_with[max_index(value[i])]] for i in xrange(2)]\n    player.r_freq = [[best_without[min_index(player.freq[i])], best_without[min_index(value[i])]] for i in xrange(2)]\n\n    f = [[[[0,0,0] for k in xrange(4)] for j in xrange(2)] for i in xrange(3)]\n    t = [[[0,0,0,0] for j in xrange(2)] for i in xrange(3)]\n\n    m_len = [[0 for _ in xrange(T)] for i in xrange(3)]\n\n    for i in xrange(T-1,0,-1):\n        m_len[0][i] = 4\n        for j in xrange(4):\n            if player.my_history_hash[j][i] != player.my_history_hash[j][T]:\n                m_len[0][i] = j\n                break\n        for j in xrange(4):\n            if player.opp_history_hash[j][i] != player.opp_history_hash[j][T]:\n                m_len[1][i] = j\n                break\n        for j in xrange(4):\n            if player.my_history_hash[j][i] != player.my_history_hash[j][T] or player.opp_history_hash[j][i] != player.opp_history_hash[j][T]:\n                m_len[2][i] = j\n                break\n\n    for i in xrange(T-1,0,-1):\n        for j in xrange(3):\n            for k in xrange(m_len[j][i]):\n                f[j][0][k][player.my_history[i+1]] += 1\n                f[j][1][k][player.opp_history[i+1]] += 1\n                t[j][0][k] += 1\n                t[j][1][k] += 1\n\n                if t[j][0][k] == 1:\n                    player.p_full[j*8 + 0*4 + k][0] = wins_with[player.my_history[i+1]]\n                if t[j][1][k] == 1:\n                    player.p_full[j*8 + 1*4 + k][0] = wins_with[player.opp_history[i+1]]\n                if t[j][0][k] == 3:\n                    player.p_full[j*8 + 0*4 + k][1] = wins_with[max_index(f[j][0][k])]\n                    player.r_full[j*8 + 0*4 + k][0] = best_without[min_index(f[j][0][k])]\n                if t[j][1][k] == 3:\n                    player.p_full[j*8 + 1*4 + k][1] = wins_with[max_index(f[j][1][k])]\n                    player.r_full[j*8 + 1*4 + k][0] = best_without[min_index(f[j][1][k])]\n\n    for j in xrange(3):\n        for k in xrange(4):\n            player.p_full[j*8 + 0*4 + k][2] = wins_with[max_index(f[j][0][k])]\n            player.r_full[j*8 + 0*4 + k][1] = best_without[min_index(f[j][0][k])]\n\n            player.p_full[j*8 + 1*4 + k][2] = wins_with[max_index(f[j][1][k])]\n            player.r_full[j*8 + 1*4 + k][1] = best_without[min_index(f[j][1][k])]\n\n    for j in xrange(24):\n        gear_freq = [0] * 9 # was [0,0,0] because original code incorrectly only allocated array length 3\n\n        for i in xrange(T-1,0,-1):\n            if player.gear[j][i] == player.gear[j][T]:\n                gear_freq[player.gear[j][i+1]] += 1\n\n        #original source allocated to 9 positions of gear_freq array, but only allocated first three\n        #also, only looked at first 3 to find the max_index\n        #unclear whether to seek max index over all 9 gear_freq's or just first 3 (as original code)\n        player.p_full[j][3] = (player.p_full[j][1] + max_index(gear_freq)) % 3\n\n    # end make_predictions()\n\n    player.p_len = [find_best_prediction(l) for l in lengths]\n\n    return rps_to_text[player.p_len[max_index(player.s_len)]]\n\n\n\n# observation   =  {'step': 1, 'lastOpponentAction': 1}\n# configuration =  {'episodeSteps': 10, 'agentTimeout': 60, 'actTimeout': 1, 'runTimeout': 1200, 'isProduction': False, 'signs': 3}\nmy_moves    = []\nopp_moves   = []\ndef greenberg_agent(observation, configuration):    \n    global my_moves\n    global opp_moves\n    if observation.step > 0:\n        opp_move = rps_to_text[ observation.lastOpponentAction ]\n        opp_moves.append( opp_move )\n        \n    action_text = player(my_moves, opp_moves)\n    action      = rps_to_num[action_text]\n\n    my_moves.append(action_text)\n    return int(action)\n\n\n\n##### .\/decision_tree_1.py #####\n\n\nimport time\nimport os\nimport random\nimport numpy as np\nfrom typing import List, Dict\nfrom sklearn.tree import DecisionTreeClassifier\n\ndef random_agent(observation, configuration):\n    return random.randint(0, configuration.signs-1)\n\ndef rock_agent(observation, configuration):\n    return 0\n\ndef paper_agent(observation, configuration):\n    return 1\n\ndef scissors_agent(observation, configuration):\n    return 2\n\ndef sequential_agent(observation, configuration):\n    return observation.step % configuration.signs\n\n\n\ndef get_winstats(decision_tree_history_1) -> Dict[str,int]:\n    total = len(decision_tree_history_1['action'])\n    wins = 0\n    draw = 0\n    loss = 0 \n    for n in range(total):\n        if   decision_tree_history_1['action'][n] == decision_tree_history_1['opponent'][n] + 1: wins +=  1\n        elif decision_tree_history_1['action'][n] == decision_tree_history_1['opponent'][n]:     draw +=  1\n        elif decision_tree_history_1['action'][n] == decision_tree_history_1['opponent'][n] - 1: loss +=  1\n    return { \"wins\": wins, \"draw\": draw, \"loss\": loss }\n\ndef get_winrate(decision_tree_history_1):\n    winstats = get_winstats(decision_tree_history_1)\n    winrate  = winstats['wins'] \/ (winstats['wins'] + winstats['loss']) if (winstats['wins'] + winstats['loss']) else 0\n    return winrate\n    \n    \n# Initialize starting decision_tree_history_1\ndecision_tree_history_1 = {\n    \"step\":        [],\n    \"prediction1\": [],\n    \"prediction2\": [],\n    \"expected\":    [],\n    \"action\":      [],\n    \"opponent\":    [],\n}\n\n# NOTE: adding statistics causes the DecisionTree to make random moves \ndef get_statistics(values) -> List[float]:\n    values = np.array(values)\n    return [\n        np.count_nonzero(values == n) \/ len(values)\n        if len(values) else 0.0\n        for n in [0,1,2]\n    ]\n\n\n# observation   =  {'step': 1, 'lastOpponentAction': 1}\n# configuration =  {'episodeSteps': 10, 'agentTimeout': 60, 'actTimeout': 1, 'runTimeout': 1200, 'isProduction': False, 'signs': 3}\ndef decision_tree_agent_1(observation, configuration, window=5, stages=2, random_freq=0.66, warmup_period=10, max_samples=1000):    \n    global decision_tree_history_1\n    warmup_period   = warmup_period  # if os.environ.get('KAGGLE_KERNEL_RUN_TYPE','') != 'Interactive' else 0\n    models          = [ None ] + [ DecisionTreeClassifier() ] * stages\n    \n    time_start      = time.perf_counter()\n    actions         = list(range(configuration.signs))  # [0,1,2]\n    \n    step            = observation.step\n    last_action     = decision_tree_history_1['action'][-1]          if len(decision_tree_history_1['action']) else 2\n    opponent_action = observation.lastOpponentAction if observation.step > 0   else 2\n        \n    if observation.step > 0:\n        decision_tree_history_1['opponent'].append(opponent_action)\n        \n    winrate  = get_winrate(decision_tree_history_1)\n    winstats = get_winstats(decision_tree_history_1)\n    \n    # Set default values     \n    prediction1 = random.randint(0,2)\n    prediction2 = random.randint(0,2)\n    prediction3 = random.randint(0,2)\n    expected    = random.randint(0,2)\n\n    # We need at least some turns of decision_tree_history_1 for DecisionTreeClassifier to work\n    if observation.step >= window:\n        # First we try to predict the opponents next move based on move decision_tree_history_1\n        # TODO: create windowed decision_tree_history_1\n        try:\n            n_start = max(1, len(decision_tree_history_1['opponent']) - window - max_samples) \n            # pass\n            if stages >= 1:\n                X = np.stack([\n                    np.array([\n                        # get_statistics(decision_tree_history_1['action'][:n+window]),\n                        # get_statistics(decision_tree_history_1['opponent'][:n-1+window]),\n                        decision_tree_history_1['action'][n:n+window], \n                        decision_tree_history_1['opponent'][n:n+window]\n                    ]).flatten()\n                    for n in range(n_start,len(decision_tree_history_1['opponent'])-window-warmup_period) \n                ])\n                Y = np.array([\n                    decision_tree_history_1['opponent'][n+window]\n                    for n in range(n_start,len(decision_tree_history_1['opponent'])-window-warmup_period) \n                ])  \n                Z = np.array([\n                    # get_statistics(decision_tree_history_1['action']),\n                    # get_statistics(decision_tree_history_1['opponent']),\n                    decision_tree_history_1['action'][-window+1:] + [ last_action ], \n                    decision_tree_history_1['opponent'][-window:] \n                ]).flatten().reshape(1, -1)\n\n                models[1].fit(X, Y)\n                expected = prediction1 = models[1].predict(Z)[0]\n\n            if stages >= 2:\n                # Now retrain including prediction decision_tree_history_1\n                X = np.stack([\n                    np.array([\n                        # get_statistics(decision_tree_history_1['action'][:n+window]),\n                        # get_statistics(decision_tree_history_1['prediction1'][:n+window]),\n                        # get_statistics(decision_tree_history_1['opponent'][:n-1+window]),\n                        decision_tree_history_1['action'][n:n+window], \n                        decision_tree_history_1['prediction1'][n:n+window],\n                        decision_tree_history_1['opponent'][n:n+window],\n                    ]).flatten()\n                    for n in range(n_start,len(decision_tree_history_1['opponent'])-window-warmup_period) \n                ])\n                Y = np.array([\n                    decision_tree_history_1['opponent'][n+window]\n                    for n in range(n_start,len(decision_tree_history_1['opponent'])-window-warmup_period) \n                ])  \n                Z = np.array([\n                    # get_statistics(decision_tree_history_1['action']),\n                    # get_statistics(decision_tree_history_1['prediction1']),\n                    # get_statistics(decision_tree_history_1['opponent']),\n                    decision_tree_history_1['action'][-window+1:]      + [ last_action ], \n                    decision_tree_history_1['prediction1'][-window+1:] + [ prediction1 ],\n                    decision_tree_history_1['opponent'][-window:] \n                ]).flatten().reshape(1, -1)\n\n                models[2].fit(X, Y)\n                expected = prediction2 = models[2].predict(Z)[0]\n\n            if stages >= 3:\n                # Now retrain including prediction decision_tree_history_1\n                X = np.stack([\n                    np.array([\n                        # get_statistics(decision_tree_history_1['action'][:n+window]),\n                        # get_statistics(decision_tree_history_1['prediction1'][:n+window]),\n                        # get_statistics(decision_tree_history_1['prediction2'][:n+window]),\n                        # get_statistics(decision_tree_history_1['opponent'][:n-1+window]),\n                        decision_tree_history_1['action'][n:n+window], \n                        decision_tree_history_1['prediction1'][n:n+window],\n                        decision_tree_history_1['prediction2'][n:n+window],\n                        decision_tree_history_1['opponent'][n:n+window],\n                    ]).flatten()\n                    for n in range(n_start,len(decision_tree_history_1['opponent'])-window-warmup_period) \n                ])\n                Y = np.array([\n                    decision_tree_history_1['opponent'][n+window]\n                    for n in range(n_start,len(decision_tree_history_1['opponent'])-window-warmup_period) \n                ])  \n                Z = np.array([\n                    # get_statistics(decision_tree_history_1['action']),\n                    # get_statistics(decision_tree_history_1['prediction1']),\n                    # get_statistics(decision_tree_history_1['prediction2']),\n                    # get_statistics(decision_tree_history_1['opponent']),\n                    decision_tree_history_1['action'][-window+1:]      + [ last_action ], \n                    decision_tree_history_1['prediction1'][-window+1:] + [ prediction1 ],\n                    decision_tree_history_1['prediction2'][-window+1:] + [ prediction2 ],\n                    decision_tree_history_1['opponent'][-window:] \n                ]).flatten().reshape(1, -1)\n\n                models[3].fit(X, Y)\n                expected = prediction3 = models[3].predict(Z)[0]\n        \n        except Exception as exception:\n            pass\n                    \n    # During the warmup period, play random to get a feel for the opponent \n    if (observation.step <= max(warmup_period,window)):\n        actor  = 'warmup'\n        action = random_agent(observation, configuration)    \n    \n    # Play a purely random move occasionally, which will hopefully distort any opponent statistics\n    elif (random.random() <= random_freq):\n        actor  = 'random'\n        action = random_agent(observation, configuration)\n        \n    # But mostly use DecisionTreeClassifier to predict the next move\n    else:\n        actor  = 'DecisionTree'\n        action = (expected + 1) % configuration.signs\n    \n    # Persist state\n    decision_tree_history_1['step'].append(step)\n    decision_tree_history_1['prediction1'].append(prediction1)\n    decision_tree_history_1['prediction2'].append(prediction2)\n    decision_tree_history_1['expected'].append(expected)\n    decision_tree_history_1['action'].append(action)\n    if observation.step == 0:  # keep arrays equal length\n        decision_tree_history_1['opponent'].append(random.randint(0, 2))\n\n\n    # Print debug information\n    time_taken = time.perf_counter() - time_start\n    # pass    \n    pass    \n    return int(action)\n\n\n\n##### .\/IOU2.py #####\n\nimport random\n\nclass Strategy:\n  def __init__(self):\n    # 2 different self.lengths of history, 3 kinds of history, both, mine, yours\n    # 3 different self.limit self.length of reverse learning\n    # 6 kinds of strategy based on Iocaine Powder\n    self.num_predictor = 27\n\n\n    self.len_rfind = [20]\n    self.limit = [10,20,60]\n    self.beat = { \"R\":\"P\" , \"P\":\"S\", \"S\":\"R\"}\n    self.not_lose = { \"R\":\"PPR\" , \"P\":\"SSP\" , \"S\":\"RRS\" } #50-50 chance\n    self.my_his   =\"\"\n    self.your_his =\"\"\n    self.both_his =\"\"\n    self.list_predictor = [\"\"]*self.num_predictor\n    self.length = 0\n    self.temp1 = { \"PP\":\"1\" , \"PR\":\"2\" , \"PS\":\"3\",\n              \"RP\":\"4\" , \"RR\":\"5\", \"RS\":\"6\",\n              \"SP\":\"7\" , \"SR\":\"8\", \"SS\":\"9\"}\n    self.temp2 = { \"1\":\"PP\",\"2\":\"PR\",\"3\":\"PS\",\n                \"4\":\"RP\",\"5\":\"RR\",\"6\":\"RS\",\n                \"7\":\"SP\",\"8\":\"SR\",\"9\":\"SS\"} \n    self.who_win = { \"PP\": 0, \"PR\":1 , \"PS\":-1,\n                \"RP\": -1,\"RR\":0, \"RS\":1,\n                \"SP\": 1, \"SR\":-1, \"SS\":0}\n    self.score_predictor = [0]*self.num_predictor\n    self.output = random.choice(\"RPS\")\n    self.predictors = [self.output]*self.num_predictor\n\n\n  def prepare_next_move(self, prev_input):\n    input = prev_input\n\n    #update self.predictors\n    #\"\"\"\n    if len(self.list_predictor[0])<5:\n        front =0\n    else:\n        front =1\n    for i in range (self.num_predictor):\n        if self.predictors[i]==input:\n            result =\"1\"\n        else:\n            result =\"0\"\n        self.list_predictor[i] = self.list_predictor[i][front:5]+result #only 5 rounds before\n    #history matching 1-6\n    self.my_his += self.output\n    self.your_his += input\n    self.both_his += self.temp1[input+self.output]\n    self.length +=1\n    for i in range(1):\n        len_size = min(self.length,self.len_rfind[i])\n        j=len_size\n        #self.both_his\n        while j>=1 and not self.both_his[self.length-j:self.length] in self.both_his[0:self.length-1]:\n            j-=1\n        if j>=1:\n            k = self.both_his.rfind(self.both_his[self.length-j:self.length],0,self.length-1)\n            self.predictors[0+6*i] = self.your_his[j+k]\n            self.predictors[1+6*i] = self.beat[self.my_his[j+k]]\n        else:\n            self.predictors[0+6*i] = random.choice(\"RPS\")\n            self.predictors[1+6*i] = random.choice(\"RPS\")\n        j=len_size\n        #self.your_his\n        while j>=1 and not self.your_his[self.length-j:self.length] in self.your_his[0:self.length-1]:\n            j-=1\n        if j>=1:\n            k = self.your_his.rfind(self.your_his[self.length-j:self.length],0,self.length-1)\n            self.predictors[2+6*i] = self.your_his[j+k]\n            self.predictors[3+6*i] = self.beat[self.my_his[j+k]]\n        else:\n            self.predictors[2+6*i] = random.choice(\"RPS\")\n            self.predictors[3+6*i] = random.choice(\"RPS\")\n        j=len_size\n        #self.my_his\n        while j>=1 and not self.my_his[self.length-j:self.length] in self.my_his[0:self.length-1]:\n            j-=1\n        if j>=1:\n            k = self.my_his.rfind(self.my_his[self.length-j:self.length],0,self.length-1)\n            self.predictors[4+6*i] = self.your_his[j+k]\n            self.predictors[5+6*i] = self.beat[self.my_his[j+k]]\n        else:\n            self.predictors[4+6*i] = random.choice(\"RPS\")\n            self.predictors[5+6*i] = random.choice(\"RPS\")\n\n    for i in range(3):\n        temp =\"\"\n        search = self.temp1[(self.output+input)] #last round\n        for start in range(2, min(self.limit[i],self.length) ):\n            if search == self.both_his[self.length-start]:\n                temp+=self.both_his[self.length-start+1]\n        if(temp==\"\"):\n            self.predictors[6+i] = random.choice(\"RPS\")\n        else:\n            collectR = {\"P\":0,\"R\":0,\"S\":0} #take win\/lose from opponent into account\n            for sdf in temp:\n                next_move = self.temp2[sdf]\n                if(self.who_win[next_move]==-1):\n                    collectR[self.temp2[sdf][1]]+=3\n                elif(self.who_win[next_move]==0):\n                    collectR[self.temp2[sdf][1]]+=1\n                elif(self.who_win[next_move]==1):\n                    collectR[self.beat[self.temp2[sdf][0]]]+=1\n            max1 = -1\n            p1 =\"\"\n            for key in collectR:\n                if(collectR[key]>max1):\n                    max1 = collectR[key]\n                    p1 += key\n            self.predictors[6+i] = random.choice(p1)\n    \n    #rotate 9-27:\n    for i in range(9,27):\n        self.predictors[i] = self.beat[self.beat[self.predictors[i-9]]]\n        \n    #choose a predictor\n    len_his = len(self.list_predictor[0])\n    for i in range(self.num_predictor):\n        sum = 0\n        for j in range(len_his):\n            if self.list_predictor[i][j]==\"1\":\n                sum+=(j+1)*(j+1)\n            else:\n                sum-=(j+1)*(j+1)\n        self.score_predictor[i] = sum\n    max_score = max(self.score_predictor)\n    #min_score = min(self.score_predictor)\n    #c_temp = {\"R\":0,\"P\":0,\"S\":0}\n    #for i in range (self.num_predictor):\n        #if self.score_predictor[i]==max_score:\n        #    c_temp[self.predictors[i]] +=1\n        #if self.score_predictor[i]==min_score:\n        #    c_temp[self.predictors[i]] -=1\n    if max_score>0:\n        predict = self.predictors[self.score_predictor.index(max_score)]\n    else:\n        predict = random.choice(self.your_his)\n    self.output = random.choice(self.not_lose[predict])\n    return self.output\n\n\nglobal GLOBAL_STRATEGY\nGLOBAL_STRATEGY = Strategy()\n\n\ndef iou2_agent(observation, configuration):\n  global GLOBAL_STRATEGY\n\n  # Action mapping\n  to_char = [\"R\", \"P\", \"S\"]\n  from_char = {\"R\": 0, \"P\": 1, \"S\": 2}\n\n  if observation.step > 0:\n    GLOBAL_STRATEGY.prepare_next_move(to_char[observation.lastOpponentAction])\n  action = from_char[GLOBAL_STRATEGY.output]\n  return action\n\n\n\n##### .\/memory_patterns_v20.py #####\n\n# start executing cells from here to rewrite submission.py\n\nimport random\n\ndef evaluate_pattern_efficiency(previous_step_result):\n    \"\"\" \n        evaluate efficiency of the pattern and, if pattern is inefficient,\n        remove it from agent's memory\n    \"\"\"\n    pattern_group_index = previous_action[\"pattern_group_index\"]\n    pattern_index = previous_action[\"pattern_index\"]\n    pattern = groups_of_memory_patterns[pattern_group_index][\"memory_patterns\"][pattern_index]\n    pattern[\"reward\"] += previous_step_result\n    # if pattern is inefficient\n    if pattern[\"reward\"] <= EFFICIENCY_THRESHOLD:\n        # remove pattern from agent's memory\n        del groups_of_memory_patterns[pattern_group_index][\"memory_patterns\"][pattern_index]\n    \ndef find_action(group, group_index):\n    \"\"\" if possible, find my_action in this group of memory patterns \"\"\"\n    if len(current_memory) > group[\"memory_length\"]:\n        this_step_memory = current_memory[-group[\"memory_length\"]:]\n        memory_pattern, pattern_index = find_pattern(group[\"memory_patterns\"], this_step_memory, group[\"memory_length\"])\n        if memory_pattern != None:\n            my_action_amount = 0\n            for action in memory_pattern[\"opp_next_actions\"]:\n                # if this opponent's action occurred more times than currently chosen action\n                # or, if it occured the same amount of times and this one is choosen randomly among them\n                if (action[\"amount\"] > my_action_amount or\n                        (action[\"amount\"] == my_action_amount and random.random() > 0.5)):\n                    my_action_amount = action[\"amount\"]\n                    my_action = action[\"response\"]\n            return my_action, pattern_index\n    return None, None\n\ndef find_pattern(memory_patterns, memory, memory_length):\n    \"\"\" find appropriate pattern and its index in memory \"\"\"\n    for i in range(len(memory_patterns)):\n        actions_matched = 0\n        for j in range(memory_length):\n            if memory_patterns[i][\"actions\"][j] == memory[j]:\n                actions_matched += 1\n            else:\n                break\n        # if memory fits this pattern\n        if actions_matched == memory_length:\n            return memory_patterns[i], i\n    # appropriate pattern not found\n    return None, None\n\ndef get_step_result_for_memory_patterns_v20(memory_patterns_v20_action, opp_action):\n    \"\"\" \n        get result of the step for memory_patterns_v20\n        1, 0 and -1 representing win, tie and lost results of the game respectively\n        reward will be taken from observation in the next release of kaggle environments\n    \"\"\"\n    if memory_patterns_v20_action == opp_action:\n        return 0\n    elif (memory_patterns_v20_action == (opp_action + 1)) or (memory_patterns_v20_action == 0 and opp_action == 2):\n        return 1\n    else:\n        return -1\n    \ndef update_current_memory(obs, my_action):\n    \"\"\" add memory_patterns_v20's current step to current_memory \"\"\"\n    # if there's too many actions in the current_memory\n    if len(current_memory) > current_memory_max_length:\n        # delete first two elements in current memory\n        # (actions of the oldest step in current memory)\n        del current_memory[:2]\n    # add agent's last action to agent's current memory\n    current_memory.append(my_action)\n    \ndef update_memory_pattern(obs, group):\n    \"\"\" if possible, update or add some memory pattern in this group \"\"\"\n    # if length of current memory is suitable for this group of memory patterns\n    if len(current_memory) > group[\"memory_length\"]:\n        # get memory of the previous step\n        # considering that last step actions of both agents are already present in current_memory\n        previous_step_memory = current_memory[-group[\"memory_length\"] - 2 : -2]\n        previous_pattern, pattern_index = find_pattern(group[\"memory_patterns\"], previous_step_memory, group[\"memory_length\"])\n        if previous_pattern == None:\n            previous_pattern = {\n                # list of actions of both players\n                \"actions\": previous_step_memory.copy(),\n                # total reward earned by using this pattern\n                \"reward\": 0,\n                # list of observed opponent's actions after each occurrence of this pattern\n                \"opp_next_actions\": [\n                    # action that was made by opponent,\n                    # amount of times that action occurred,\n                    # what should be the response of memory_patterns_v20\n                    {\"action\": 0, \"amount\": 0, \"response\": 1},\n                    {\"action\": 1, \"amount\": 0, \"response\": 2},\n                    {\"action\": 2, \"amount\": 0, \"response\": 0}\n                ]\n            }\n            group[\"memory_patterns\"].append(previous_pattern)\n        # update previous_pattern\n        for action in previous_pattern[\"opp_next_actions\"]:\n            if action[\"action\"] == obs[\"lastOpponentAction\"]:\n                action[\"amount\"] += 1\n    \n# \"%%writefile -a submission.py\" will append the code below to submission.py,\n# it WILL NOT rewrite submission.py\n\n# maximum steps in a memory pattern\nSTEPS_MAX = 5\n# minimum steps in a memory pattern\nSTEPS_MIN = 3\n# lowest efficiency threshold of a memory pattern before being removed from agent's memory\nEFFICIENCY_THRESHOLD = -3\n# amount of steps between forced random actions\nFORCED_RANDOM_ACTION_INTERVAL = random.randint(STEPS_MIN, STEPS_MAX)\n\n# current memory of the agent\ncurrent_memory = []\n# previous action of memory_patterns_v20\nprevious_action = {\n    \"action\": None,\n    # action was taken from pattern\n    \"action_from_pattern\": False,\n    \"pattern_group_index\": None,\n    \"pattern_index\": None\n}\n# amount of steps remained until next forced random action\nsteps_to_random = FORCED_RANDOM_ACTION_INTERVAL\n# maximum length of current_memory\ncurrent_memory_max_length = STEPS_MAX * 2\n# current reward of memory_patterns_v20\n# will be taken from observation in the next release of kaggle environments\nreward = 0\n# memory length of patterns in first group\n# STEPS_MAX is multiplied by 2 to consider both memory_patterns_v20's and opponent's actions\ngroup_memory_length = current_memory_max_length\n# list of groups of memory patterns\ngroups_of_memory_patterns = []\nfor i in range(STEPS_MAX, STEPS_MIN - 1, -1):\n    groups_of_memory_patterns.append({\n        # how many steps in a row are in the pattern\n        \"memory_length\": group_memory_length,\n        # list of memory patterns\n        \"memory_patterns\": []\n    })\n    group_memory_length -= 2\n    \n# \"%%writefile -a submission.py\" will append the code below to submission.py,\n# it WILL NOT rewrite submission.py\n\ndef memory_patterns_v20(obs, conf):\n    \"\"\" your ad here \"\"\"\n    # action of memory_patterns_v20\n    my_action = None\n    \n    # forced random action\n    global steps_to_random\n    steps_to_random -= 1\n    if steps_to_random <= 0:\n        steps_to_random = FORCED_RANDOM_ACTION_INTERVAL\n        # choose action randomly\n        my_action = random.randint(0, 2)\n        # save action's data\n        previous_action[\"action\"] = my_action\n        previous_action[\"action_from_pattern\"] = False\n        previous_action[\"pattern_group_index\"] = None\n        previous_action[\"pattern_index\"] = None\n    \n    # if it's not first step\n    if obs[\"step\"] > 0:\n        # add opponent's last step to current_memory\n        current_memory.append(obs[\"lastOpponentAction\"])\n        # previous step won or lost\n        previous_step_result = get_step_result_for_memory_patterns_v20(current_memory[-2], current_memory[-1])\n        global reward\n        reward += previous_step_result\n        # if previous action of memory_patterns_v20 was taken from pattern\n        if previous_action[\"action_from_pattern\"]:\n            evaluate_pattern_efficiency(previous_step_result)\n    \n    for i in range(len(groups_of_memory_patterns)):\n        # if possible, update or add some memory pattern in this group\n        update_memory_pattern(obs, groups_of_memory_patterns[i])\n        # if action was not yet found\n        if my_action == None:\n            my_action, pattern_index = find_action(groups_of_memory_patterns[i], i)\n            if my_action != None:\n                # save action's data\n                previous_action[\"action\"] = my_action\n                previous_action[\"action_from_pattern\"] = True\n                previous_action[\"pattern_group_index\"] = i\n                previous_action[\"pattern_index\"] = pattern_index\n    \n    # if no action was found\n    if my_action == None:\n        # choose action randomly\n        my_action = random.randint(0, 2)\n        # save action's data\n        previous_action[\"action\"] = my_action\n        previous_action[\"action_from_pattern\"] = False\n        previous_action[\"pattern_group_index\"] = None\n        previous_action[\"pattern_index\"] = None\n    \n    # add memory_patterns_v20's current step to current_memory\n    update_current_memory(obs, my_action)\n    return my_action\n\n\n\n##### .\/anti_pi.py #####\n\nimport re\n\nPI = \"3.14159 26535 89793 23846 26433 83279 50288 41971 69399 37510 58209 74944 59230 78164 06286 20899 86280 34825 34211 70679 82148 08651 32823 06647 09384 46095 50582 23172 53594 08128 48111 74502 84102 70193 85211 05559 64462 29489 54930 38196 44288 10975 66593 34461 28475 64823 37867 83165 27120 19091 45648 56692 34603 48610 45432 66482 13393 60726 02491 41273 72458 70066 06315 58817 48815 20920 96282 92540 91715 36436 78925 90360 01133 05305 48820 46652 13841 46951 94151 16094 33057 27036 57595 91953 09218 61173 81932 61179 31051 18548 07446 23799 62749 56735 18857 52724 89122 79381 83011 94912 98336 73362 44065 66430 86021 39494 63952 24737 19070 21798 60943 70277 05392 17176 29317 67523 84674 81846 76694 05132 00056 81271 45263 56082 77857 71342 75778 96091 73637 17872 14684 40901 22495 34301 46549 58537 10507 92279 68925 89235 42019 95611 21290 21960 86403 44181 59813 62977 47713 09960 51870 72113 49999 99837 29780 49951 05973 17328 16096 31859 50244 59455 34690 83026 42522 30825 33446 85035 26193 11881 71010 00313 78387 52886 58753 32083 81420 61717 76691 47303 59825 34904 28755 46873 11595 62863 88235 37875 93751 95778 18577 80532 17122 68066 13001 92787 66111 95909 21642 01989 38095 25720 10654 85863 27886 59361 53381 82796 82303 01952 03530 18529 68995 77362 25994 13891 24972 17752 83479 13151 55748 57242 45415 06959 50829 53311 68617 27855 88907 50983 81754 63746 49393 19255 06040 09277 01671 13900 98488 24012 85836 16035 63707 66010 47101 81942 95559 61989 46767 83744 94482 55379 77472 68471 04047 53464 62080 46684 25906 94912 93313 67702 89891 52104 75216 20569 66024 05803 81501 93511 25338 24300 35587 64024 74964 73263 91419 92726 04269 92279 67823 54781 63600 93417 21641 21992 45863 15030 28618 29745 55706 74983 85054 94588 58692 69956 90927 21079 75093 02955 32116 53449 87202 75596 02364 80665 49911 98818 34797 75356 63698 07426 54252 78625 51818 41757 46728 90977 77279 38000 81647 06001 61452 49192 17321 72147 72350 14144 19735 68548 16136 11573 52552 13347 57418 49468 43852 33239 07394 14333 45477 62416 86251 89835 69485 56209 92192 22184 27255 02542 56887 67179 04946 01653 46680 49886 27232 79178 60857 84383 82796 79766 81454 10095 38837 86360 95068 00642 25125 20511 73929 84896 08412 84886 26945 60424 19652 85022 21066 11863 06744 27862 20391 94945 04712 37137 86960 95636 43719 17287 46776 46575 73962 41389 08658 32645 99581 33904 78027 59009 94657 64078 95126 94683 98352 59570 98258 22620 52248 94077 26719 47826 84826 01476 99090 26401 36394 43745 53050 68203 49625 24517 49399 65143 14298 09190 65925 09372 21696 46151 57098 58387 41059 78859 59772 97549 89301 61753 92846 81382 68683 86894 27741 55991 85592 52459 53959 43104 99725 24680 84598 72736 44695 84865 38367 36222 62609 91246 08051 24388 43904 51244 13654 97627 80797 71569 14359 97700 12961 60894 41694 86855 58484 06353 42207 22258 28488 64815 84560 28506 01684 27394 52267 46767 88952 52138 52254 99546 66727 82398 64565 96116 35488 62305 77456 49803 55936 34568 17432 41125 15076 06947 94510 96596 09402 52288 79710 89314 56691 36867 22874 89405 60101 50330 86179 28680 92087 47609 17824 93858 90097 14909 67598 52613 65549 78189 31297 84821 68299 89487 22658 80485 75640 14270 47755 51323 79641 45152 37462 34364 54285 84447 95265 86782 10511 41354 73573 95231 13427 16610 21359 69536 23144 29524 84937 18711 01457 65403 59027 99344 03742 00731 05785 39062 19838 74478 08478 48968 33214 45713 86875 19435 06430 21845 31910 48481 00537 06146 80674 91927 81911 97939 95206 14196 63428 75444 06437 45123 71819 21799 98391 01591 95618 14675 14269 12397 48940 90718 64942 31961 56794 52080 95146 55022 52316 03881 93014 20937 62137 85595 66389 37787 08303 90697 92077 34672 21825 62599 66150 14215 03068 03844 77345 49202 60541 46659 25201 49744 28507 32518 66600 21324 34088 19071 04863 31734 64965 14539 05796 26856 10055 08106 65879 69981 63574 73638 40525 71459 10289 70641 40110 97120 62804 39039 75951 56771 57700 42033 78699 36007 23055 87631 76359 42187 31251 47120 53292 81918 26186 12586 73215 79198 41484 88291 64470 60957 52706 95722 09175 67116 72291 09816 90915 28017 35067 12748 58322 28718 35209 35396 57251 21083 57915 13698 82091 44421 00675 10334 67110 31412 67111 36990 86585 16398 31501 97016 51511 68517 14376 57618 35155 65088 49099 89859 98238 73455 28331 63550 76479 18535 89322 61854 89632 13293 30898 57064 20467 52590 70915 48141 65498 59461 63718 02709 81994 30992 44889 57571 28289 05923 23326 09729 97120 84433 57326 54893 82391 19325 97463 66730 58360 41428 13883 03203 82490 37589 85243 74417 02913 27656 18093 77344 40307 07469 21120 19130 20330 38019 76211 01100 44929 32151 60842 44485 96376 69838 95228 68478 31235 52658 21314 49576 85726 24334 41893 03968 64262 43410 77322 69780 28073 18915 44110 10446 82325 27162 01052 65227 21116 60396 66557 30925 47110 55785 37634 66820 65310 98965 26918 62056 47693 12570 58635 66201 85581 00729 36065 98764 86117 91045 33488 50346 11365 76867 53249 44166 80396 26579 78771 85560 84552 96541 26654 08530 61434 44318 58676 97514 56614 06800 70023 78776 59134 40171 27494 70420 56223 05389 94561 31407 11270 00407 85473 32699 39081 45466 46458 80797 27082 66830 63432 85878 56983 05235 80893 30657 57406 79545 71637 75254 20211 49557 61581 40025 01262 28594 13021 64715 50979 25923 09907 96547 37612 55176 56751 35751 78296 66454 77917 45011 29961 48903 04639 94713 29621 07340 43751 89573 59614 58901 93897 13111 79042 97828 56475 03203 19869 15140 28708 08599 04801 09412 14722 13179 47647 77262 24142 54854 54033 21571 85306 14228 81375 85043 06332 17518 29798 66223 71721 59160 77166 92547 48738 98665 49494 50114 65406 28433 66393 79003 97692 65672 14638 53067 36096 57120 91807 63832 71664 16274 88880 07869 25602 90228 47210 40317 21186 08204 19000 42296 61711 96377 92133 75751 14959 50156 60496 31862 94726 54736 42523 08177 03675 15906 73502 35072 83540 56704 03867 43513 62222 47715 89150 49530 98444 89333 09634 08780 76932 59939 78054 19341 44737 74418 42631 29860 80998 88687 41326 04721 56951 62396 58645 73021 63159 81931 95167 35381 29741 67729 47867 24229 24654 36680 09806 76928 23828 06899 64004 82435 40370 14163 14965 89794 09243 23789 69070 69779 42236 25082 21688 95738 37986 23001 59377 64716 51228 93578 60158 81617 55782 97352 33446 04281 51262 72037 34314 65319 77774 16031 99066 55418 76397 92933 44195 21541 34189 94854 44734 56738 31624 99341 91318 14809 27777 10386 38773 43177 20754 56545 32207 77092 12019 05166 09628 04909 26360 19759 88281 61332 31666 36528 61932 66863 36062 73567 63035 44776 28035 04507 77235 54710 58595 48702 79081 43562 40145 17180 62464 36267 94561 27531 81340 78330 33625 42327 83944 97538 24372 05835 31147 71199 26063 81334 67768 79695 97030 98339 13077 10987 04085 91337 46414 42822 77263 46594 70474 58784 77872 01927 71528 07317 67907 70715 72134 44730 60570 07334 92436 93113 83504 93163 12840 42512 19256 51798 06941 13528 01314 70130 47816 43788 51852 90928 54520 11658 39341 96562 13491 43415 95625 86586 55705 52690 49652 09858 03385 07224 26482 93972 85847 83163 05777 75606 88876 44624 82468 57926 03953 52773 48030 48029 00587 60758 25104 74709 16439 61362 67604 49256 27420 42083 20856 61190 62545 43372 13153 59584 50687 72460 29016 18766 79524 06163 42522 57719 54291 62991 93064 55377 99140 37340 43287 52628 88963 99587 94757 29174 64263 57455 25407 90914 51357 11136 94109 11939 32519 10760 20825 20261 87985 31887 70584 29725 91677 81314 96990 09019 21169 71737 27847 68472 68608 49003 37702 42429 16513 00500 51683 23364 35038 95170 29893 92233 45172 20138 12806 96501 17844 08745 19601 21228 59937 16231 30171 14448 46409 03890 64495 44400 61986 90754 85160 26327 50529 83491 87407 86680 88183 38510 22833 45085 04860 82503 93021 33219 71551 84306 35455 00766 82829 49304 13776 55279 39751 75461 39539 84683 39363 83047 46119 96653 85815 38420 56853 38621 86725 23340 28308 71123 28278 92125 07712 62946 32295 63989 89893 58211 67456 27010 21835 64622 01349 67151 88190 97303 81198 00497 34072 39610 36854 06643 19395 09790 19069 96395 52453 00545 05806 85501 95673 02292 19139 33918 56803 44903 98205 95510 02263 53536 19204 19947 45538 59381 02343 95544 95977 83779 02374 21617 27111 72364 34354 39478 22181 85286 24085 14006 66044 33258 88569 86705 43154 70696 57474 58550 33232 33421 07301 54594 05165 53790 68662 73337 99585 11562 57843 22988 27372 31989 87571 41595 78111 96358 33005 94087 30681 21602 87649 62867 44604 77464 91599 50549 73742 56269 01049 03778 19868 35938 14657 41268 04925 64879 85561 45372 34786 73303 90468 83834 36346 55379 49864 19270 56387 29317 48723 32083 76011 23029 91136 79386 27089 43879 93620 16295 15413 37142 48928 30722 01269 01475 46684 76535 76164 77379 46752 00490 75715 55278 19653 62132 39264 06160 13635 81559 07422 02020 31872 77605 27721 90055 61484 25551 87925 30343 51398 44253 22341 57623 36106 42506 39049 75008 65627 10953 59194 65897 51413 10348 22769 30624 74353 63256 91607 81547 81811 52843 66795 70611 08615 33150 44521 27473 92454 49454 23682 88606 13408 41486 37767 00961 20715 12491 40430 27253 86076 48236 34143 34623 51897 57664 52164 13767 96903 14950 19108 57598 44239 19862 91642 19399 49072 36234 64684 41173 94032 65918 40443 78051 33389 45257 42399 50829 65912 28508 55582 15725 03107 12570 12668 30240 29295 25220 11872 67675 62204 15420 51618 41634 84756 51699 98116 14101 00299 60783 86909 29160 30288 40026 91041 40792 88621 50784 24516 70908 70006 99282 12066 04183 71806 53556 72525 32567 53286 12910 42487 76182 58297 65157 95984 70356 22262 93486 00341 58722 98053 49896 50226 29174 87882 02734 20922 22453 39856 26476 69149 05562 84250 39127 57710 28402 79980 66365 82548 89264 88025 45661 01729 67026 64076 55904 29099 45681 50652 65305 37182 94127 03369 31378 51786 09040 70866 71149 65583 43434 76933 85781 71138 64558 73678 12301 45876 87126 60348 91390 95620 09939 36103 10291 61615 28813 84379 09904 23174 73363 94804 57593 14931 40529 76347 57481 19356 70911 01377 51721 00803 15590 24853 09066 92037 67192 20332 29094 33467 68514 22144 77379 39375 17034 43661 99104 03375 11173 54719 18550 46449 02636 55128 16228 82446 25759 16333 03910 72253 83742 18214 08835 08657 39177 15096 82887 47826 56995 99574 49066 17583 44137 52239 70968 34080 05355 98491 75417 38188 39994 46974 86762 65516 58276 58483 58845 31427 75687 90029 09517 02835 29716 34456 21296 40435 23117 60066 51012 41200 65975 58512 76178 58382 92041 97484 42360 80071 93045 76189 32349 22927 96501 98751 87212 72675 07981 25547 09589 04556 35792 12210 33346 69749 92356 30254 94780 24901 14195 21238 28153 09114 07907 38602 51522 74299 58180 72471 62591 66854 51333 12394 80494 70791 19153 26734 30282 44186 04142 63639 54800 04480 02670 49624 82017 92896 47669 75831 83271 31425 17029 69234 88962 76684 40323 26092 75249 60357 99646 92565 04936 81836 09003 23809 29345 95889 70695 36534 94060 34021 66544 37558 90045 63288 22505 45255 64056 44824 65151 87547 11962 18443 96582 53375 43885 69094 11303 15095 26179 37800 29741 20766 51479 39425 90298 96959 46995 56576 12186 56196 73378 62362 56125 21632 08628 69222 10327 48892 18654 36480 22967 80705 76561 51446 32046 92790 68212 07388 37781 42335 62823 60896 32080 68222 46801 22482 61177 18589 63814 09183 90367 36722 20888 32151 37556 00372 79839 40041 52970 02878 30766 70944 47456 01345 56417 25437 09069 79396 12257 14298 94671 54357 84687 88614 44581 23145 93571 98492 25284 71605 04922 12424 70141 21478 05734 55105 00801 90869 96033 02763 47870 81081 75450 11930 71412 23390 86639 38339 52942 57869 05076 43100 63835 19834 38934 15961 31854 34754 64955 69781 03829 30971 64651 43840 70070 73604 11237 35998 43452 25161 05070 27056 23526 60127 64848 30840 76118 30130 52793 20542 74628 65403 60367 45328 65105 70658 74882 25698 15793 67897 66974 22057 50596 83440 86973 50201 41020 67235 85020 07245 22563 26513 41055 92401 90274 21624 84391 40359 98953 53945 90944 07046 91209 14093 87001 26456 00162 37428 80210 92764 57931 06579 22955 24988 72758 46101 26483 69998 92256 95968 81592 05600 10165 52563 7567\"\nPI = re.sub('[^1-9]', '', PI)\n\n# observation   =  {'step': 1, 'lastOpponentAction': 1}\n# configuration =  {'episodeSteps': 10, 'agentTimeout': 60, 'actTimeout': 1, 'runTimeout': 1200, 'isProduction': False, 'signs': 3}\ndef anti_pi_agent(observation, configuration):    \n    action = (int(PI[observation.step]) + 1) % configuration.signs\n    return int(action)\n\n\n\n##### .\/naive_bayes.py #####\n\nfrom collections import defaultdict\nfrom itertools import chain, combinations\nimport random\nimport sys\nfrom typing import *\n\nimport numpy as np\nfrom pydash import flatten\n\n\nclass RPSNaiveBayes():\n    def __init__(self, max_memory=20, verbose=True):\n        self.max_memory = max_memory\n        self.verbose    = verbose\n        self.history = {\n            \"opponent\": [],\n            \"rotn\":     [],\n            \"expected\": [],\n            \"action\":   [],\n        }\n        # self.root_keys = ['action','opponent','rotn','expected']\n        self.root_keys = ['action','opponent']\n        self.keys = [\n            \",\".join(combo)\n            for n in range(1,len(self.root_keys)+1)        \n            for combo in combinations(self.root_keys, n)\n        ]\n        # self.keys = ['action', 'opponent', 'rotn', 'action,opponent', 'action,rotn', 'opponent,rotn', 'action,opponent,rotn']\n        self.memory = {\n            key: defaultdict(lambda: np.array([0,0,0]))\n            for key in self.keys\n        }\n        \n    def __call__(self, obs, conf):\n        return self.agent(obs, conf)\n\n\n    # obs  {'remainingOverageTime': 60, 'step': 1, 'reward': 0, 'lastOpponentAction': 0}\n    # conf {'episodeSteps': 10, 'actTimeout': 1, 'runTimeout': 1200, 'signs': 3, 'tieRewardThreshold': 20, 'agentTimeout': 60}\n    def agent(self, obs, conf):\n        # pass\n        self.update_state(obs, conf)\n\n        views          = self.get_current_views()\n        log_likelihood = self.get_log_likelihood(views)\n        probability    = self.get_probability(log_likelihood)\n\n        expected = random.choices( population=[0,1,2], weights=probability, k=1 )[0]\n        action   = int(expected + 1) % conf.signs\n        self.history['expected'].insert(0, expected)\n        self.history['action'].insert(0, action)\n\n        if self.verbose:\n            pass\n\n        return int(action)\n\n\n    def update_state(self, obs, conf):\n        if obs.step > 0:\n            rotn = obs.lastOpponentAction - self.history['action'][0] \n\n            self.history['opponent'].insert(0, obs.lastOpponentAction % conf.signs)\n            self.history['rotn'].insert(0, rotn)\n\n        for keys in self.memory.keys():\n            memories = self.get_new_memories(keys)\n            for value, path in memories:\n                self.memory[keys][path][value] += 1\n\n\n    def get_key_min_length(self, keys: str) -> int:\n        min_length = min([ len(self.history[key]) for key in keys.split(',') ])\n        return min_length\n\n\n    def get_new_memories(self, keys: Union[str,List[str]]) -> List[Tuple[Tuple,int]]:\n        min_length = self.get_key_min_length(keys)\n        min_length = min(min_length, self.max_memory)\n        memories   = []\n        for n in range(1,min_length):\n            value = self.history[\"opponent\"][0]\n            paths = []\n            for key in keys.split(','):\n                path = self.history[key][1:n]\n                if len(path): paths.append(path)\n            paths = tuple(flatten(paths))\n            if len(paths):\n                memories.append( (value, paths) )\n        return memories\n\n\n    def get_current_views(self) -> Dict[str, List[Tuple[int]]]:\n        views = {\n            keys: [\n                tuple(flatten([value, paths]))\n                for (value, paths) in self.get_new_memories(keys)\n            ]\n            for keys in self.memory.keys()\n        }\n        return views\n\n\n    def get_log_likelihood(self, views: List[Tuple]) -> np.ndarray:\n        log_likelihoods = np.array([.0,.0,.0])\n        for keys in self.memory.keys():\n            count = np.sum( np.array(list(self.memory[keys].values())).shape )\n            for path in views[keys]:\n                try:\n                    n_unique = 3 ** len(path)\n                    freqs = self.memory[keys][path] * n_unique    \n                    probs = (freqs + 1) \/ ( count + n_unique )    # Laplacian Smoothing\n                    log_likelihood = [\n                        np.log(probs[a]) - np.log(probs[b] + probs[c])\n                        if (probs[b] + probs[c]) > 0 else 0.0\n                        for a, b, c in [ (0,1,2), (1,2,0), (2,0,1) ]\n                    ]\n                    log_likelihood = [ n if not np.isnan(n) else 0.0 for n in log_likelihood ]\n                    log_likelihoods += np.array(log_likelihood)\n                except ZeroDivisionError: pass\n\n        return log_likelihoods\n\n    \n    def get_probability(self, log_likelihood: np.ndarray) -> np.ndarray:\n        probability = np.exp(log_likelihood)\n        probability[ probability == np.inf ] = sys.maxsize \/ len(probability) \/ 2\n        probability = probability \/ np.sum(probability)\n        return probability\n        \n            \n    \n    \nnaive_bayes_instance = RPSNaiveBayes()\ndef naive_bayes(obs, conf):\n    return naive_bayes_instance.agent(obs, conf)\n\n\n\n##### .\/anti_anti_pi.py #####\n\nimport re\n\nPI = \"3.14159 26535 89793 23846 26433 83279 50288 41971 69399 37510 58209 74944 59230 78164 06286 20899 86280 34825 34211 70679 82148 08651 32823 06647 09384 46095 50582 23172 53594 08128 48111 74502 84102 70193 85211 05559 64462 29489 54930 38196 44288 10975 66593 34461 28475 64823 37867 83165 27120 19091 45648 56692 34603 48610 45432 66482 13393 60726 02491 41273 72458 70066 06315 58817 48815 20920 96282 92540 91715 36436 78925 90360 01133 05305 48820 46652 13841 46951 94151 16094 33057 27036 57595 91953 09218 61173 81932 61179 31051 18548 07446 23799 62749 56735 18857 52724 89122 79381 83011 94912 98336 73362 44065 66430 86021 39494 63952 24737 19070 21798 60943 70277 05392 17176 29317 67523 84674 81846 76694 05132 00056 81271 45263 56082 77857 71342 75778 96091 73637 17872 14684 40901 22495 34301 46549 58537 10507 92279 68925 89235 42019 95611 21290 21960 86403 44181 59813 62977 47713 09960 51870 72113 49999 99837 29780 49951 05973 17328 16096 31859 50244 59455 34690 83026 42522 30825 33446 85035 26193 11881 71010 00313 78387 52886 58753 32083 81420 61717 76691 47303 59825 34904 28755 46873 11595 62863 88235 37875 93751 95778 18577 80532 17122 68066 13001 92787 66111 95909 21642 01989 38095 25720 10654 85863 27886 59361 53381 82796 82303 01952 03530 18529 68995 77362 25994 13891 24972 17752 83479 13151 55748 57242 45415 06959 50829 53311 68617 27855 88907 50983 81754 63746 49393 19255 06040 09277 01671 13900 98488 24012 85836 16035 63707 66010 47101 81942 95559 61989 46767 83744 94482 55379 77472 68471 04047 53464 62080 46684 25906 94912 93313 67702 89891 52104 75216 20569 66024 05803 81501 93511 25338 24300 35587 64024 74964 73263 91419 92726 04269 92279 67823 54781 63600 93417 21641 21992 45863 15030 28618 29745 55706 74983 85054 94588 58692 69956 90927 21079 75093 02955 32116 53449 87202 75596 02364 80665 49911 98818 34797 75356 63698 07426 54252 78625 51818 41757 46728 90977 77279 38000 81647 06001 61452 49192 17321 72147 72350 14144 19735 68548 16136 11573 52552 13347 57418 49468 43852 33239 07394 14333 45477 62416 86251 89835 69485 56209 92192 22184 27255 02542 56887 67179 04946 01653 46680 49886 27232 79178 60857 84383 82796 79766 81454 10095 38837 86360 95068 00642 25125 20511 73929 84896 08412 84886 26945 60424 19652 85022 21066 11863 06744 27862 20391 94945 04712 37137 86960 95636 43719 17287 46776 46575 73962 41389 08658 32645 99581 33904 78027 59009 94657 64078 95126 94683 98352 59570 98258 22620 52248 94077 26719 47826 84826 01476 99090 26401 36394 43745 53050 68203 49625 24517 49399 65143 14298 09190 65925 09372 21696 46151 57098 58387 41059 78859 59772 97549 89301 61753 92846 81382 68683 86894 27741 55991 85592 52459 53959 43104 99725 24680 84598 72736 44695 84865 38367 36222 62609 91246 08051 24388 43904 51244 13654 97627 80797 71569 14359 97700 12961 60894 41694 86855 58484 06353 42207 22258 28488 64815 84560 28506 01684 27394 52267 46767 88952 52138 52254 99546 66727 82398 64565 96116 35488 62305 77456 49803 55936 34568 17432 41125 15076 06947 94510 96596 09402 52288 79710 89314 56691 36867 22874 89405 60101 50330 86179 28680 92087 47609 17824 93858 90097 14909 67598 52613 65549 78189 31297 84821 68299 89487 22658 80485 75640 14270 47755 51323 79641 45152 37462 34364 54285 84447 95265 86782 10511 41354 73573 95231 13427 16610 21359 69536 23144 29524 84937 18711 01457 65403 59027 99344 03742 00731 05785 39062 19838 74478 08478 48968 33214 45713 86875 19435 06430 21845 31910 48481 00537 06146 80674 91927 81911 97939 95206 14196 63428 75444 06437 45123 71819 21799 98391 01591 95618 14675 14269 12397 48940 90718 64942 31961 56794 52080 95146 55022 52316 03881 93014 20937 62137 85595 66389 37787 08303 90697 92077 34672 21825 62599 66150 14215 03068 03844 77345 49202 60541 46659 25201 49744 28507 32518 66600 21324 34088 19071 04863 31734 64965 14539 05796 26856 10055 08106 65879 69981 63574 73638 40525 71459 10289 70641 40110 97120 62804 39039 75951 56771 57700 42033 78699 36007 23055 87631 76359 42187 31251 47120 53292 81918 26186 12586 73215 79198 41484 88291 64470 60957 52706 95722 09175 67116 72291 09816 90915 28017 35067 12748 58322 28718 35209 35396 57251 21083 57915 13698 82091 44421 00675 10334 67110 31412 67111 36990 86585 16398 31501 97016 51511 68517 14376 57618 35155 65088 49099 89859 98238 73455 28331 63550 76479 18535 89322 61854 89632 13293 30898 57064 20467 52590 70915 48141 65498 59461 63718 02709 81994 30992 44889 57571 28289 05923 23326 09729 97120 84433 57326 54893 82391 19325 97463 66730 58360 41428 13883 03203 82490 37589 85243 74417 02913 27656 18093 77344 40307 07469 21120 19130 20330 38019 76211 01100 44929 32151 60842 44485 96376 69838 95228 68478 31235 52658 21314 49576 85726 24334 41893 03968 64262 43410 77322 69780 28073 18915 44110 10446 82325 27162 01052 65227 21116 60396 66557 30925 47110 55785 37634 66820 65310 98965 26918 62056 47693 12570 58635 66201 85581 00729 36065 98764 86117 91045 33488 50346 11365 76867 53249 44166 80396 26579 78771 85560 84552 96541 26654 08530 61434 44318 58676 97514 56614 06800 70023 78776 59134 40171 27494 70420 56223 05389 94561 31407 11270 00407 85473 32699 39081 45466 46458 80797 27082 66830 63432 85878 56983 05235 80893 30657 57406 79545 71637 75254 20211 49557 61581 40025 01262 28594 13021 64715 50979 25923 09907 96547 37612 55176 56751 35751 78296 66454 77917 45011 29961 48903 04639 94713 29621 07340 43751 89573 59614 58901 93897 13111 79042 97828 56475 03203 19869 15140 28708 08599 04801 09412 14722 13179 47647 77262 24142 54854 54033 21571 85306 14228 81375 85043 06332 17518 29798 66223 71721 59160 77166 92547 48738 98665 49494 50114 65406 28433 66393 79003 97692 65672 14638 53067 36096 57120 91807 63832 71664 16274 88880 07869 25602 90228 47210 40317 21186 08204 19000 42296 61711 96377 92133 75751 14959 50156 60496 31862 94726 54736 42523 08177 03675 15906 73502 35072 83540 56704 03867 43513 62222 47715 89150 49530 98444 89333 09634 08780 76932 59939 78054 19341 44737 74418 42631 29860 80998 88687 41326 04721 56951 62396 58645 73021 63159 81931 95167 35381 29741 67729 47867 24229 24654 36680 09806 76928 23828 06899 64004 82435 40370 14163 14965 89794 09243 23789 69070 69779 42236 25082 21688 95738 37986 23001 59377 64716 51228 93578 60158 81617 55782 97352 33446 04281 51262 72037 34314 65319 77774 16031 99066 55418 76397 92933 44195 21541 34189 94854 44734 56738 31624 99341 91318 14809 27777 10386 38773 43177 20754 56545 32207 77092 12019 05166 09628 04909 26360 19759 88281 61332 31666 36528 61932 66863 36062 73567 63035 44776 28035 04507 77235 54710 58595 48702 79081 43562 40145 17180 62464 36267 94561 27531 81340 78330 33625 42327 83944 97538 24372 05835 31147 71199 26063 81334 67768 79695 97030 98339 13077 10987 04085 91337 46414 42822 77263 46594 70474 58784 77872 01927 71528 07317 67907 70715 72134 44730 60570 07334 92436 93113 83504 93163 12840 42512 19256 51798 06941 13528 01314 70130 47816 43788 51852 90928 54520 11658 39341 96562 13491 43415 95625 86586 55705 52690 49652 09858 03385 07224 26482 93972 85847 83163 05777 75606 88876 44624 82468 57926 03953 52773 48030 48029 00587 60758 25104 74709 16439 61362 67604 49256 27420 42083 20856 61190 62545 43372 13153 59584 50687 72460 29016 18766 79524 06163 42522 57719 54291 62991 93064 55377 99140 37340 43287 52628 88963 99587 94757 29174 64263 57455 25407 90914 51357 11136 94109 11939 32519 10760 20825 20261 87985 31887 70584 29725 91677 81314 96990 09019 21169 71737 27847 68472 68608 49003 37702 42429 16513 00500 51683 23364 35038 95170 29893 92233 45172 20138 12806 96501 17844 08745 19601 21228 59937 16231 30171 14448 46409 03890 64495 44400 61986 90754 85160 26327 50529 83491 87407 86680 88183 38510 22833 45085 04860 82503 93021 33219 71551 84306 35455 00766 82829 49304 13776 55279 39751 75461 39539 84683 39363 83047 46119 96653 85815 38420 56853 38621 86725 23340 28308 71123 28278 92125 07712 62946 32295 63989 89893 58211 67456 27010 21835 64622 01349 67151 88190 97303 81198 00497 34072 39610 36854 06643 19395 09790 19069 96395 52453 00545 05806 85501 95673 02292 19139 33918 56803 44903 98205 95510 02263 53536 19204 19947 45538 59381 02343 95544 95977 83779 02374 21617 27111 72364 34354 39478 22181 85286 24085 14006 66044 33258 88569 86705 43154 70696 57474 58550 33232 33421 07301 54594 05165 53790 68662 73337 99585 11562 57843 22988 27372 31989 87571 41595 78111 96358 33005 94087 30681 21602 87649 62867 44604 77464 91599 50549 73742 56269 01049 03778 19868 35938 14657 41268 04925 64879 85561 45372 34786 73303 90468 83834 36346 55379 49864 19270 56387 29317 48723 32083 76011 23029 91136 79386 27089 43879 93620 16295 15413 37142 48928 30722 01269 01475 46684 76535 76164 77379 46752 00490 75715 55278 19653 62132 39264 06160 13635 81559 07422 02020 31872 77605 27721 90055 61484 25551 87925 30343 51398 44253 22341 57623 36106 42506 39049 75008 65627 10953 59194 65897 51413 10348 22769 30624 74353 63256 91607 81547 81811 52843 66795 70611 08615 33150 44521 27473 92454 49454 23682 88606 13408 41486 37767 00961 20715 12491 40430 27253 86076 48236 34143 34623 51897 57664 52164 13767 96903 14950 19108 57598 44239 19862 91642 19399 49072 36234 64684 41173 94032 65918 40443 78051 33389 45257 42399 50829 65912 28508 55582 15725 03107 12570 12668 30240 29295 25220 11872 67675 62204 15420 51618 41634 84756 51699 98116 14101 00299 60783 86909 29160 30288 40026 91041 40792 88621 50784 24516 70908 70006 99282 12066 04183 71806 53556 72525 32567 53286 12910 42487 76182 58297 65157 95984 70356 22262 93486 00341 58722 98053 49896 50226 29174 87882 02734 20922 22453 39856 26476 69149 05562 84250 39127 57710 28402 79980 66365 82548 89264 88025 45661 01729 67026 64076 55904 29099 45681 50652 65305 37182 94127 03369 31378 51786 09040 70866 71149 65583 43434 76933 85781 71138 64558 73678 12301 45876 87126 60348 91390 95620 09939 36103 10291 61615 28813 84379 09904 23174 73363 94804 57593 14931 40529 76347 57481 19356 70911 01377 51721 00803 15590 24853 09066 92037 67192 20332 29094 33467 68514 22144 77379 39375 17034 43661 99104 03375 11173 54719 18550 46449 02636 55128 16228 82446 25759 16333 03910 72253 83742 18214 08835 08657 39177 15096 82887 47826 56995 99574 49066 17583 44137 52239 70968 34080 05355 98491 75417 38188 39994 46974 86762 65516 58276 58483 58845 31427 75687 90029 09517 02835 29716 34456 21296 40435 23117 60066 51012 41200 65975 58512 76178 58382 92041 97484 42360 80071 93045 76189 32349 22927 96501 98751 87212 72675 07981 25547 09589 04556 35792 12210 33346 69749 92356 30254 94780 24901 14195 21238 28153 09114 07907 38602 51522 74299 58180 72471 62591 66854 51333 12394 80494 70791 19153 26734 30282 44186 04142 63639 54800 04480 02670 49624 82017 92896 47669 75831 83271 31425 17029 69234 88962 76684 40323 26092 75249 60357 99646 92565 04936 81836 09003 23809 29345 95889 70695 36534 94060 34021 66544 37558 90045 63288 22505 45255 64056 44824 65151 87547 11962 18443 96582 53375 43885 69094 11303 15095 26179 37800 29741 20766 51479 39425 90298 96959 46995 56576 12186 56196 73378 62362 56125 21632 08628 69222 10327 48892 18654 36480 22967 80705 76561 51446 32046 92790 68212 07388 37781 42335 62823 60896 32080 68222 46801 22482 61177 18589 63814 09183 90367 36722 20888 32151 37556 00372 79839 40041 52970 02878 30766 70944 47456 01345 56417 25437 09069 79396 12257 14298 94671 54357 84687 88614 44581 23145 93571 98492 25284 71605 04922 12424 70141 21478 05734 55105 00801 90869 96033 02763 47870 81081 75450 11930 71412 23390 86639 38339 52942 57869 05076 43100 63835 19834 38934 15961 31854 34754 64955 69781 03829 30971 64651 43840 70070 73604 11237 35998 43452 25161 05070 27056 23526 60127 64848 30840 76118 30130 52793 20542 74628 65403 60367 45328 65105 70658 74882 25698 15793 67897 66974 22057 50596 83440 86973 50201 41020 67235 85020 07245 22563 26513 41055 92401 90274 21624 84391 40359 98953 53945 90944 07046 91209 14093 87001 26456 00162 37428 80210 92764 57931 06579 22955 24988 72758 46101 26483 69998 92256 95968 81592 05600 10165 52563 7567\"\nPI = re.sub('[^1-9]', '', PI)\n\n# observation   =  {'step': 1, 'lastOpponentAction': 1}\n# configuration =  {'episodeSteps': 10, 'agentTimeout': 60, 'actTimeout': 1, 'runTimeout': 1200, 'isProduction': False, 'signs': 3}\ndef anti_anti_pi_agent(observation, configuration):    \n    action = (int(PI[observation.step]) + 2) % configuration.signs\n    return int(action)\n\n\n\n##### .\/anti_rotn.py #####\n\n\nimport random\n\nrotn_history    = []\nrotn_stats = [ 0, 0, 0 ]\n\n# observation   =  {'step': 1, 'lastOpponentAction': 1}\n# configuration =  {'episodeSteps': 10, 'agentTimeout': 60, 'actTimeout': 1, 'runTimeout': 1200, 'isProduction': False, 'signs': 3}\ndef anti_rotn(observation, configuration, warmup=25):    \n    global rotn_history\n    global rotn_stats\n    \n    if observation.step > 0:\n        rotn_history.append( observation.lastOpponentAction )\n\n    if len(rotn_history) >= 2:\n        rotn = (rotn_history[-1] - rotn_history[-2]) % configuration.signs\n        rotn_stats[ rotn ] += 1\n        \n    if observation.step < warmup:\n        action = random.randint(0, configuration.signs-1)\n    else:\n        ev = list({\n            0: rotn_stats[2] - rotn_stats[1], \n            1: rotn_stats[0] - rotn_stats[2], \n            2: rotn_stats[1] - rotn_stats[0], \n        }.values())\n        offset = ev.index(max(ev))\n        action = (offset + observation.lastOpponentAction) % configuration.signs\n        pass\n        \n    return int(action)\n\n\n\n##### .\/geometry.py #####\n\nimport operator\nimport numpy as np\nimport cmath\nfrom typing import List\nfrom collections import namedtuple\nimport traceback\nimport sys\n\n\nbasis = np.array(\n    [1, cmath.exp(2j * cmath.pi * 1 \/ 3), cmath.exp(2j * cmath.pi * 2 \/ 3)]\n)\n\n\nHistMatchResult = namedtuple(\"HistMatchResult\", \"idx length\")\n\n\ndef find_all_longest(seq, max_len=None) -> List[HistMatchResult]:\n    \"\"\"\n    Find all indices where end of `seq` matches some past.\n    \"\"\"\n    result = []\n\n    i_search_start = len(seq) - 2\n\n    while i_search_start > 0:\n        i_sub = -1\n        i_search = i_search_start\n        length = 0\n\n        while i_search >= 0 and seq[i_sub] == seq[i_search]:\n            length += 1\n            i_sub -= 1\n            i_search -= 1\n\n            if max_len is not None and length > max_len:\n                break\n\n        if length > 0:\n            result.append(HistMatchResult(i_search_start + 1, length))\n\n        i_search_start -= 1\n\n    result = sorted(result, key=operator.attrgetter(\"length\"), reverse=True)\n\n    return result\n\n\ndef probs_to_complex(p):\n    return p @ basis\n\n\ndef _fix_probs(probs):\n    \"\"\"\n    Put probs back into triangle. Sometimes this happens due to rounding errors or if you\n    use complex numbers which are outside the triangle.\n    \"\"\"\n    if min(probs) < 0:\n        probs -= min(probs)\n\n    probs \/= sum(probs)\n\n    return probs\n\n\ndef complex_to_probs(z):\n    probs = (2 * (z * basis.conjugate()).real + 1) \/ 3\n    probs = _fix_probs(probs)\n    return probs\n\n\ndef z_from_action(action):\n    return basis[action]\n\n\ndef sample_from_z(z):\n    probs = complex_to_probs(z)\n    return np.random.choice(3, p=probs)\n\n\ndef bound(z):\n    return probs_to_complex(complex_to_probs(z))\n\n\ndef norm(z):\n    return bound(z \/ abs(z))\n\n\nclass Pred:\n    def __init__(self, *, alpha):\n        self.offset = 0\n        self.alpha = alpha\n        self.last_feat = None\n\n    def train(self, target):\n        if self.last_feat is not None:\n            offset = target * self.last_feat.conjugate()   # fixed\n\n            self.offset = (1 - self.alpha) * self.offset + self.alpha * offset\n\n    def predict(self, feat):\n        \"\"\"\n        feat is an arbitrary feature with a probability on 0,1,2\n        anything which could be useful anchor to start with some kind of sensible direction\n        \"\"\"\n        feat = norm(feat)\n\n        # offset = mean(target - feat)\n        # so here we see something like: result = feat + mean(target - feat)\n        # which seems natural and accounts for the correlation between target and feat\n        # all RPSContest bots do no more than that as their first step, just in a different way\n        \n        result = feat * self.offset\n\n        self.last_feat = feat\n\n        return result\n    \n    \nclass BaseAgent:\n    def __init__(self):\n        self.my_hist = []\n        self.opp_hist = []\n        self.my_opp_hist = []\n        self.outcome_hist = []\n        self.step = None\n\n    def __call__(self, obs, conf):\n        try:\n            if obs.step == 0:\n                action = np.random.choice(3)\n                self.my_hist.append(action)\n                return action\n\n            self.step = obs.step\n\n            opp = int(obs.lastOpponentAction)\n            my = self.my_hist[-1]\n\n            self.my_opp_hist.append((my, opp))\n            self.opp_hist.append(opp)\n\n            outcome = {0: 0, 1: 1, 2: -1}[(my - opp) % 3]\n            self.outcome_hist.append(outcome)\n\n            action = self.action()\n\n            self.my_hist.append(action)\n\n            return action\n        except Exception:\n            traceback.print_exc(file=sys.stderr)\n            raise\n\n    def action(self):\n        pass\n\n\nclass Agent(BaseAgent):\n    def __init__(self, alpha=0.01):\n        super().__init__()\n\n        self.predictor = Pred(alpha=alpha)\n\n    def action(self):\n        self.train()\n\n        pred = self.preds()\n\n        return_action = sample_from_z(pred)\n\n        return return_action\n\n    def train(self):\n        last_beat_opp = z_from_action((self.opp_hist[-1] + 1) % 3)\n        self.predictor.train(last_beat_opp)\n\n    def preds(self):\n        hist_match = find_all_longest(self.my_opp_hist, max_len=20)\n\n        if not hist_match:\n             return 0\n\n        feat = z_from_action(self.opp_hist[hist_match[0].idx])\n\n        pred = self.predictor.predict(feat)\n\n        return pred\n    \n    \nagent = Agent()\n\n\ndef geometry_agent(obs, conf):\n    return agent(obs, conf)\n\n\n\n##### .\/iocaine.py #####\n\n\nimport random\n\n\ndef recall(age, hist):\n    \"\"\"Looking at the last 'age' points in 'hist', finds the\n    last point with the longest similarity to the current point,\n    returning 0 if none found.\"\"\"\n    end, length = 0, 0\n    for past in range(1, min(age + 1, len(hist) - 1)):\n        if length >= len(hist) - past: break\n        for i in range(-1 - length, 0):\n            if hist[i - past] != hist[i]: break\n        else:\n            for length in range(length + 1, len(hist) - past):\n                if hist[-past - length - 1] != hist[-length - 1]: break\n            else: length += 1\n            end = len(hist) - past\n    return end\n\ndef beat(i):\n    return (i + 1) % 3\ndef loseto(i):\n    return (i - 1) % 3\n\nclass Stats:\n    \"\"\"Maintains three running counts and returns the highest count based\n         on any given time horizon and threshold.\"\"\"\n    def __init__(self):\n        self.sum = [[0, 0, 0]]\n    def add(self, move, score):\n        self.sum[-1][move] += score\n    def advance(self):\n        self.sum.append(self.sum[-1])\n    def max(self, age, default, score):\n        if age >= len(self.sum): diff = self.sum[-1]\n        else: diff = [self.sum[-1][i] - self.sum[-1 - age][i] for i in range(3)]\n        m = max(diff)\n        if m > score: return diff.index(m), m\n        return default, score\n\nclass Predictor:\n    \"\"\"The basic iocaine second- and triple-guesser.    Maintains stats on the\n         past benefits of trusting or second- or triple-guessing a given strategy,\n         and returns the prediction of that strategy (or the second- or triple-\n         guess) if past stats are deviating from zero farther than the supplied\n         \"best\" guess so far.\"\"\"\n    def __init__(self):\n        self.stats = Stats()\n        self.lastguess = -1\n    def addguess(self, lastmove, guess):\n        if lastmove != -1:\n            diff = (lastmove - self.prediction) % 3\n            self.stats.add(beat(diff), 1)\n            self.stats.add(loseto(diff), -1)\n            self.stats.advance()\n        self.prediction = guess\n    def bestguess(self, age, best):\n        bestdiff = self.stats.max(age, (best[0] - self.prediction) % 3, best[1])\n        return (bestdiff[0] + self.prediction) % 3, bestdiff[1]\n\nages = [1000, 100, 10, 5, 2, 1]\n\nclass Iocaine:\n\n    def __init__(self):\n        \"\"\"Build second-guessers for 50 strategies: 36 history-based strategies,\n             12 simple frequency-based strategies, the constant-move strategy, and\n             the basic random-number-generator strategy.    Also build 6 meta second\n             guessers to evaluate 6 different time horizons on which to score\n             the 50 strategies' second-guesses.\"\"\"\n        self.predictors = []\n        self.predict_history = self.predictor((len(ages), 2, 3))\n        self.predict_frequency = self.predictor((len(ages), 2))\n        self.predict_fixed = self.predictor()\n        self.predict_random = self.predictor()\n        self.predict_meta = [Predictor() for a in range(len(ages))]\n        self.stats = [Stats() for i in range(2)]\n        self.histories = [[], [], []]\n\n    def predictor(self, dims=None):\n        \"\"\"Returns a nested array of predictor objects, of the given dimensions.\"\"\"\n        if dims: return [self.predictor(dims[1:]) for i in range(dims[0])]\n        self.predictors.append(Predictor())\n        return self.predictors[-1]\n\n    def move(self, them):\n        \"\"\"The main iocaine \"move\" function.\"\"\"\n\n        # histories[0] stores our moves (last one already previously decided);\n        # histories[1] stores their moves (last one just now being supplied to us);\n        # histories[2] stores pairs of our and their last moves.\n        # stats[0] and stats[1] are running counters our recent moves and theirs.\n        if them != -1:\n            self.histories[1].append(them)\n            self.histories[2].append((self.histories[0][-1], them))\n            for watch in range(2):\n                self.stats[watch].add(self.histories[watch][-1], 1)\n\n        # Execute the basic RNG strategy and the fixed-move strategy.\n        rand = random.randrange(3)\n        self.predict_random.addguess(them, rand)\n        self.predict_fixed.addguess(them, 0)\n\n        # Execute the history and frequency stratgies.\n        for a, age in enumerate(ages):\n            # For each time window, there are three ways to recall a similar time:\n            # (0) by history of my moves; (1) their moves; or (2) pairs of moves.\n            # Set \"best\" to these three timeframes (zero if no matching time).\n            best = [recall(age, hist) for hist in self.histories]\n            for mimic in range(2):\n                # For each similar historical moment, there are two ways to anticipate\n                # the future: by mimicing what their move was; or mimicing what my\n                # move was.    If there were no similar moments, just move randomly.\n                for watch, when in enumerate(best):\n                    if not when: move = rand\n                    else: move = self.histories[mimic][when]\n                    self.predict_history[a][mimic][watch].addguess(them, move)\n                # Also we can anticipate the future by expecting it to be the same\n                # as the most frequent past (either counting their moves or my moves).\n                mostfreq, score = self.stats[mimic].max(age, rand, -1)\n                self.predict_frequency[a][mimic].addguess(them, mostfreq)\n\n        # All the predictors have been updated, but we have not yet scored them\n        # and chosen a winner for this round.    There are several timeframes\n        # on which we can score second-guessing, and we don't know timeframe will\n        # do best.    So score all 50 predictors on all 6 timeframes, and record\n        # the best 6 predictions in meta predictors, one for each timeframe.\n        for meta, age in enumerate(ages):\n            best = (-1, -1)\n            for predictor in self.predictors:\n                best = predictor.bestguess(age, best)\n            self.predict_meta[meta].addguess(them, best[0])\n\n        # Finally choose the best meta prediction from the final six, scoring\n        # these against each other on the whole-game timeframe. \n        best = (-1, -1)\n        for meta in range(len(ages)):\n            best = self.predict_meta[meta].bestguess(len(self.histories[0]) , best) \n\n        # We've picked a next move.    Record our move in histories[0] for next time.\n        self.histories[0].append(best[0])\n\n        # And return it.\n        return best[0]\n\niocaine = None\n\ndef iocaine_agent(observation, configuration):\n    global iocaine\n    if observation.step == 0:\n        iocaine = Iocaine()\n        act = iocaine.move(-1)\n    else:\n        act = iocaine.move(observation.lastOpponentAction)\n        \n    return act\n\n\n\n##### .\/decision_tree_2.py #####\n\n\nimport time\nimport os\nimport random\nimport numpy as np\nfrom typing import List, Dict\nfrom sklearn.tree import DecisionTreeClassifier\n\ndef random_agent(observation, configuration):\n    return random.randint(0, configuration.signs-1)\n\ndef rock_agent(observation, configuration):\n    return 0\n\ndef paper_agent(observation, configuration):\n    return 1\n\ndef scissors_agent(observation, configuration):\n    return 2\n\ndef sequential_agent(observation, configuration):\n    return observation.step % configuration.signs\n\n\n\ndef get_winstats(decision_tree_history_2) -> Dict[str,int]:\n    total = len(decision_tree_history_2['action'])\n    wins = 0\n    draw = 0\n    loss = 0 \n    for n in range(total):\n        if   decision_tree_history_2['action'][n] == decision_tree_history_2['opponent'][n] + 1: wins +=  1\n        elif decision_tree_history_2['action'][n] == decision_tree_history_2['opponent'][n]:     draw +=  1\n        elif decision_tree_history_2['action'][n] == decision_tree_history_2['opponent'][n] - 1: loss +=  1\n    return { \"wins\": wins, \"draw\": draw, \"loss\": loss }\n\ndef get_winrate(decision_tree_history_2):\n    winstats = get_winstats(decision_tree_history_2)\n    winrate  = winstats['wins'] \/ (winstats['wins'] + winstats['loss']) if (winstats['wins'] + winstats['loss']) else 0\n    return winrate\n    \n    \n# Initialize starting decision_tree_history_2\ndecision_tree_history_2 = {\n    \"step\":        [],\n    \"prediction1\": [],\n    \"prediction2\": [],\n    \"expected\":    [],\n    \"action\":      [],\n    \"opponent\":    [],\n}\n\n# NOTE: adding statistics causes the DecisionTree to make random moves \ndef get_statistics(values) -> List[float]:\n    values = np.array(values)\n    return [\n        np.count_nonzero(values == n) \/ len(values)\n        if len(values) else 0.0\n        for n in [0,1,2]\n    ]\n\n\n# observation   =  {'step': 1, 'lastOpponentAction': 1}\n# configuration =  {'episodeSteps': 10, 'agentTimeout': 60, 'actTimeout': 1, 'runTimeout': 1200, 'isProduction': False, 'signs': 3}\ndef decision_tree_agent_2(observation, configuration, window=5, stages=2, random_freq=0.66, warmup_period=10, max_samples=1000):    \n    global decision_tree_history_2\n    warmup_period   = warmup_period  # if os.environ.get('KAGGLE_KERNEL_RUN_TYPE','') != 'Interactive' else 0\n    models          = [ None ] + [ DecisionTreeClassifier() ] * stages\n    \n    time_start      = time.perf_counter()\n    actions         = list(range(configuration.signs))  # [0,1,2]\n    \n    step            = observation.step\n    last_action     = decision_tree_history_2['action'][-1]          if len(decision_tree_history_2['action']) else 2\n    opponent_action = observation.lastOpponentAction if observation.step > 0   else 2\n        \n    if observation.step > 0:\n        decision_tree_history_2['opponent'].append(opponent_action)\n        \n    winrate  = get_winrate(decision_tree_history_2)\n    winstats = get_winstats(decision_tree_history_2)\n    \n    # Set default values     \n    prediction1 = random.randint(0,2)\n    prediction2 = random.randint(0,2)\n    prediction3 = random.randint(0,2)\n    expected    = random.randint(0,2)\n\n    # We need at least some turns of decision_tree_history_2 for DecisionTreeClassifier to work\n    if observation.step >= window:\n        # First we try to predict the opponents next move based on move decision_tree_history_2\n        # TODO: create windowed decision_tree_history_2\n        try:\n            n_start = max(1, len(decision_tree_history_2['opponent']) - window - max_samples) \n            # pass\n            if stages >= 1:\n                X = np.stack([\n                    np.array([\n                        # get_statistics(decision_tree_history_2['action'][:n+window]),\n                        # get_statistics(decision_tree_history_2['opponent'][:n-1+window]),\n                        decision_tree_history_2['action'][n:n+window], \n                        decision_tree_history_2['opponent'][n:n+window]\n                    ]).flatten()\n                    for n in range(n_start,len(decision_tree_history_2['opponent'])-window-warmup_period) \n                ])\n                Y = np.array([\n                    decision_tree_history_2['opponent'][n+window]\n                    for n in range(n_start,len(decision_tree_history_2['opponent'])-window-warmup_period) \n                ])  \n                Z = np.array([\n                    # get_statistics(decision_tree_history_2['action']),\n                    # get_statistics(decision_tree_history_2['opponent']),\n                    decision_tree_history_2['action'][-window+1:] + [ last_action ], \n                    decision_tree_history_2['opponent'][-window:] \n                ]).flatten().reshape(1, -1)\n\n                models[1].fit(X, Y)\n                expected = prediction1 = models[1].predict(Z)[0]\n\n            if stages >= 2:\n                # Now retrain including prediction decision_tree_history_2\n                X = np.stack([\n                    np.array([\n                        # get_statistics(decision_tree_history_2['action'][:n+window]),\n                        # get_statistics(decision_tree_history_2['prediction1'][:n+window]),\n                        # get_statistics(decision_tree_history_2['opponent'][:n-1+window]),\n                        decision_tree_history_2['action'][n:n+window], \n                        decision_tree_history_2['prediction1'][n:n+window],\n                        decision_tree_history_2['opponent'][n:n+window],\n                    ]).flatten()\n                    for n in range(n_start,len(decision_tree_history_2['opponent'])-window-warmup_period) \n                ])\n                Y = np.array([\n                    decision_tree_history_2['opponent'][n+window]\n                    for n in range(n_start,len(decision_tree_history_2['opponent'])-window-warmup_period) \n                ])  \n                Z = np.array([\n                    # get_statistics(decision_tree_history_2['action']),\n                    # get_statistics(decision_tree_history_2['prediction1']),\n                    # get_statistics(decision_tree_history_2['opponent']),\n                    decision_tree_history_2['action'][-window+1:]      + [ last_action ], \n                    decision_tree_history_2['prediction1'][-window+1:] + [ prediction1 ],\n                    decision_tree_history_2['opponent'][-window:] \n                ]).flatten().reshape(1, -1)\n\n                models[2].fit(X, Y)\n                expected = prediction2 = models[2].predict(Z)[0]\n\n            if stages >= 3:\n                # Now retrain including prediction decision_tree_history_2\n                X = np.stack([\n                    np.array([\n                        # get_statistics(decision_tree_history_2['action'][:n+window]),\n                        # get_statistics(decision_tree_history_2['prediction1'][:n+window]),\n                        # get_statistics(decision_tree_history_2['prediction2'][:n+window]),\n                        # get_statistics(decision_tree_history_2['opponent'][:n-1+window]),\n                        decision_tree_history_2['action'][n:n+window], \n                        decision_tree_history_2['prediction1'][n:n+window],\n                        decision_tree_history_2['prediction2'][n:n+window],\n                        decision_tree_history_2['opponent'][n:n+window],\n                    ]).flatten()\n                    for n in range(n_start,len(decision_tree_history_2['opponent'])-window-warmup_period) \n                ])\n                Y = np.array([\n                    decision_tree_history_2['opponent'][n+window]\n                    for n in range(n_start,len(decision_tree_history_2['opponent'])-window-warmup_period) \n                ])  \n                Z = np.array([\n                    # get_statistics(decision_tree_history_2['action']),\n                    # get_statistics(decision_tree_history_2['prediction1']),\n                    # get_statistics(decision_tree_history_2['prediction2']),\n                    # get_statistics(decision_tree_history_2['opponent']),\n                    decision_tree_history_2['action'][-window+1:]      + [ last_action ], \n                    decision_tree_history_2['prediction1'][-window+1:] + [ prediction1 ],\n                    decision_tree_history_2['prediction2'][-window+1:] + [ prediction2 ],\n                    decision_tree_history_2['opponent'][-window:] \n                ]).flatten().reshape(1, -1)\n\n                models[3].fit(X, Y)\n                expected = prediction3 = models[3].predict(Z)[0]\n        \n        except Exception as exception:\n            pass\n                    \n    # During the warmup period, play random to get a feel for the opponent \n    if (observation.step <= max(warmup_period,window)):\n        actor  = 'warmup'\n        action = random_agent(observation, configuration)    \n    \n    # Play a purely random move occasionally, which will hopefully distort any opponent statistics\n    elif (random.random() <= random_freq):\n        actor  = 'random'\n        action = random_agent(observation, configuration)\n        \n    # But mostly use DecisionTreeClassifier to predict the next move\n    else:\n        actor  = 'DecisionTree'\n        action = (expected + 1) % configuration.signs\n    \n    # Persist state\n    decision_tree_history_2['step'].append(step)\n    decision_tree_history_2['prediction1'].append(prediction1)\n    decision_tree_history_2['prediction2'].append(prediction2)\n    decision_tree_history_2['expected'].append(expected)\n    decision_tree_history_2['action'].append(action)\n    if observation.step == 0:  # keep arrays equal length\n        decision_tree_history_2['opponent'].append(random.randint(0, 2))\n\n\n    # Print debug information\n    time_taken = time.perf_counter() - time_start\n    # pass    \n    pass    \n    return int(action)\n\n\n\n##### .\/centrifugal_bumblepuppy_v4.py #####\n\ncode_bumblepuppy = compile(\n    \"\"\"\n#                         WoofWoofWoof\n#                     Woof            Woof\n#                Woof                      Woof\n#              Woof                          Woof\n#             Woof  Centrifugal Bumble-puppy  Woof\n#              Woof                          Woof\n#                Woof                      Woof\n#                     Woof            Woof\n#                         WoofWoofWoof\n\nimport random\n\nnumber_of_predictors = 60 #yes, this really has 60 predictors.\nnumber_of_metapredictors = 4 #actually, I lied! This has 240 predictors.\n\n\nif not input:\n\tlimits = [50,20,6]\n\tbeat={'R':'P','P':'S','S':'R'}\n\turmoves=\"\"\n\tmymoves=\"\"\n\tDNAmoves=\"\"\n\toutputs=[random.choice(\"RPS\")]*number_of_metapredictors\n\tpredictorscore1=[3]*number_of_predictors\n\tpredictorscore2=[3]*number_of_predictors\n\tpredictorscore3=[3]*number_of_predictors\n\tpredictorscore4=[3]*number_of_predictors\n\tnuclease={'RP':'a','PS':'b','SR':'c','PR':'d','SP':'e','RS':'f','RR':'g','PP':'h','SS':'i'}\n\tlength=0\n\tpredictors=[random.choice(\"RPS\")]*number_of_predictors\n\tmetapredictors=[random.choice(\"RPS\")]*number_of_metapredictors\n\tmetapredictorscore=[3]*number_of_metapredictors\nelse:\n\n\tfor i in range(number_of_predictors):\n\t\t#metapredictor 1\n\t\tpredictorscore1[i]*=0.8\n\t\tpredictorscore1[i]+=(input==predictors[i])*3\n\t\tpredictorscore1[i]-=(input==beat[beat[predictors[i]]])*3\n\t\t#metapredictor 2: beat metapredictor 1 (probably contains a bug)\n\t\tpredictorscore2[i]*=0.8\n\t\tpredictorscore2[i]+=(output==predictors[i])*3\n\t\tpredictorscore2[i]-=(output==beat[beat[predictors[i]]])*3\n\t\t#metapredictor 3\n\t\tpredictorscore3[i]+=(input==predictors[i])*3\n\t\tif input==beat[beat[predictors[i]]]:\n\t\t\tpredictorscore3[i]=0\n\t\t#metapredictor 4: beat metapredictor 3 (probably contains a bug)\n\t\tpredictorscore4[i]+=(output==predictors[i])*3\n\t\tif output==beat[beat[predictors[i]]]:\n\t\t\tpredictorscore4[i]=0\n\t\t\t\n\tfor i in range(number_of_metapredictors):\n\t\tmetapredictorscore[i]*=0.96\n\t\tmetapredictorscore[i]+=(input==metapredictors[i])*3\n\t\tmetapredictorscore[i]-=(input==beat[beat[metapredictors[i]]])*3\n\t\t\n\t\n\t#Predictors 1-18: History matching\n\turmoves+=input\t\t\n\tmymoves+=output\n\tDNAmoves+=nuclease[input+output]\n\tlength+=1\n\t\n\tfor z in range(3):\n\t\tlimit = min([length,limits[z]])\n\t\tj=limit\n\t\twhile j>=1 and not DNAmoves[length-j:length] in DNAmoves[0:length-1]:\n\t\t\tj-=1\n\t\tif j>=1:\n\t\t\ti = DNAmoves.rfind(DNAmoves[length-j:length],0,length-1) \n\t\t\tpredictors[0+6*z] = urmoves[j+i] \n\t\t\tpredictors[1+6*z] = beat[mymoves[j+i]] \n\t\tj=limit\t\t\t\n\t\twhile j>=1 and not urmoves[length-j:length] in urmoves[0:length-1]:\n\t\t\tj-=1\n\t\tif j>=1:\n\t\t\ti = urmoves.rfind(urmoves[length-j:length],0,length-1) \n\t\t\tpredictors[2+6*z] = urmoves[j+i] \n\t\t\tpredictors[3+6*z] = beat[mymoves[j+i]] \n\t\tj=limit\n\t\twhile j>=1 and not mymoves[length-j:length] in mymoves[0:length-1]:\n\t\t\tj-=1\n\t\tif j>=1:\n\t\t\ti = mymoves.rfind(mymoves[length-j:length],0,length-1) \n\t\t\tpredictors[4+6*z] = urmoves[j+i] \n\t\t\tpredictors[5+6*z] = beat[mymoves[j+i]]\n\t#Predictor 19,20: RNA Polymerase\t\t\n\tL=len(mymoves)\n\ti=DNAmoves.rfind(DNAmoves[L-j:L-1],0,L-2)\n\twhile i==-1:\n\t\tj-=1\n\t\ti=DNAmoves.rfind(DNAmoves[L-j:L-1],0,L-2)\n\t\tif j<2:\n\t\t\tbreak\n\tif i==-1 or j+i>=L:\n\t\tpredictors[18]=predictors[19]=random.choice(\"RPS\")\n\telse:\n\t\tpredictors[18]=beat[mymoves[j+i]]\n\t\tpredictors[19]=urmoves[j+i]\n\n\t#Predictors 21-60: rotations of Predictors 1:20\n\tfor i in range(20,60):\n\t\tpredictors[i]=beat[beat[predictors[i-20]]] #Trying to second guess me?\n\t\n\tmetapredictors[0]=predictors[predictorscore1.index(max(predictorscore1))]\n\tmetapredictors[1]=beat[predictors[predictorscore2.index(max(predictorscore2))]]\n\tmetapredictors[2]=predictors[predictorscore3.index(max(predictorscore3))]\n\tmetapredictors[3]=beat[predictors[predictorscore4.index(max(predictorscore4))]]\n\t\n\t#compare predictors\noutput = beat[metapredictors[metapredictorscore.index(max(metapredictorscore))]]\nif max(metapredictorscore)<0:\n\toutput = beat[random.choice(urmoves)]\n\"\"\", '<string>', 'exec')\ngg_bumblepuppy = {}\n\n\ndef centrifugal_bumblepuppy(observation, configuration):\n    global gg_bumblepuppy\n    global code_bumblepuppy\n    inp = ''\n    try:\n        inp = 'RPS'[observation.lastOpponentAction]\n    except:\n        pass\n    gg_bumblepuppy['input'] = inp\n    exec(code_bumblepuppy, gg_bumblepuppy)\n    return {'R': 0, 'P': 1, 'S': 2}[gg_bumblepuppy['output']]\n\n\n\n\n\n##### .\/CNN.py ####\n\nbs = 6 # batch size  \n\nopponent_actions = []\nagent_actions = []\nactions = []\nbatch_x = []\nbatch_y = []\n\nclass RPS(nn.Module):\n    \"\"\"\n    Class that predict logits of action probabilities given game history.\n        Inputs: game history [bs, 2, 10].\n        Outputs: logits of action probabilities [bs, 3].\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.conv_layers = nn.Sequential(\n            nn.Conv1d(2, 4, 3, 1, 1, bias=False),\n            nn.ReLU(),\n            nn.AvgPool1d(2),\n            nn.Conv1d(4, 8, 3, 1, 1, bias=False),\n            nn.ReLU(),\n            nn.AvgPool1d(2),\n            nn.Conv1d(8, 16, 2, 1, 1, bias=False),\n            nn.ReLU(),\n            nn.AvgPool1d(2)\n        )\n        self.head = nn.Sequential(\n            nn.Linear(16, 6),\n            nn.ReLU(),\n            nn.Linear(6, 3)\n        )\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = torch.flatten(x, 1)\n        x = self.head(x)\n        return x\n\ndef soft_cross_entropy(target, prediciton):\n    log_probs = nn.functional.log_softmax(prediciton, dim=1)\n    sce = -(target * log_probs).sum() \/ target.shape[0]\n    return sce\n\ndef train_step(model, data, optimizer):\n    model.train()\n    torch.set_grad_enabled(True)\n\n    X = data['X'].view(-1, 2, 10)\n    y = data['y'].view(-1, 3)\n    prd = model(X)\n    loss = soft_cross_entropy(y, prd)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nmodel = RPS()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\ndef CNN_agent(observation, configuration):\n    \n    global actions, agent_actions, opponent_actions\n    global model, optimizer\n    global batch_x, batch_y\n    global bs\n    \n    # first step\n    if observation.step == 0:\n        hand = np.random.randint(2)\n        actions.append(hand)\n        return hand\n    \n    # first warm up rounds\n    if 0 < observation.step < 12:\n        opponent_actions.append(observation.lastOpponentAction)\n        agent_actions.append(actions[-1])\n        hand = np.random.randint(2)\n        actions.append(hand)\n        return hand\n    \n    # start to train CNN\n    elif observation.step >= 12:\n        opponent_actions.append(observation.lastOpponentAction)\n        agent_actions.append(actions[-1])\n        \n        wining_action = (opponent_actions[-1] + 1) % 3 \n        fair_action = opponent_actions[-1]\n        lose_action = (opponent_actions[-1] - 1) % 3 \n\n        # soft labels for target    \n        y = [0, 0, 0]\n        y[wining_action] = 0.7\n        y[fair_action] = 0.2\n        y[lose_action] = 0.1 \n        \n        # add data for history\n        batch_x.append([opponent_actions[-2:-12:-1],\n                        agent_actions[-2:-12:-1]])\n        batch_y.append(y)\n        \n        # data for single CNN update \n        data = {'X': torch.Tensor([opponent_actions[-2:-12:-1],\n                                   agent_actions[-2:-12:-1]]),\n                'y': torch.Tensor(y)} \n        \n        # evaluate single training step\n        train_step(model, data, optimizer)\n        \n        # evaluate mini-batch training steps\n        if observation.step % 10 == 0:\n            k = 1 if observation.step < 100 else 3\n            for _ in range(k):\n                idxs = np.random.choice(list(range(len(batch_y))), bs)\n                data = {'X': torch.Tensor(np.array(batch_x)[idxs]),\n                        'y': torch.Tensor(np.array(batch_y)[idxs])}\n                train_step(model, data, optimizer)\n        \n        # data for current action prediction\n        X_prd = torch.Tensor([opponent_actions[-1:-11:-1],\n                              agent_actions[-1:-11:-1]]).view(1, 2, -1)\n        \n        # predict logits\n        probs = model(X_prd).view(3)\n        # calculate probabilities\n        probs = nn.functional.softmax(probs, dim=0).detach().cpu().numpy()\n        \n        # choose action\n        hand = np.random.choice([0, 1, 2], p=probs)\n        actions.append(hand)\n        \n        return int(hand)\n\n\n\n\n#### .\/anti_otm.py ####\nT = np.zeros((3, 3))\nP = np.zeros((3, 3))\n\na1, a2 = None, None\nlast_action = None # track my action.\n\n\n###########################################\n# Original agent with modifications marked ->\n###########################################\n\ndef anti_transition_agent(observation, configuration):\n    global T, P, a1, a2, last_action\n    if observation.step > 1:\n        a1 = last_action   # on me only; take mirrored view on game\n        T[a2, a1] += 1\n        P = np.divide(T, np.maximum(1, T.sum(axis=1)).reshape(-1, 1))\n        a2 = a1\n        if np.sum(P[a1, :]) == 1:\n            probs = P[a1,:]\n            \n            probs += 0.63 * np.roll(probs, 1)    # This is the magic addition of phase\n            \n            result = (int(probs.argmax()) + 1) % 3   # Changed to argmax instead of stochastic\n        else:\n            result = int(np.random.randint(3))\n    else:\n        if observation.step == 1:\n            a2 = last_action    # on me only\n        result = int(np.random.randint(3))\n        \n    result = (result + 1) % 3  # beat what he would have done\n        \n    last_action = result\n        \n    return result\n\n\n\n##### .\/multi_armed_bandit.py #####\n\n\nfrom collections import defaultdict\nimport numpy as np\nimport time\nfrom operator import itemgetter\n\nmlb_opponent = []\nmlb_expected = defaultdict(list)\nmlb_agents   = {\n#     'random':               (lambda obs, conf: random_agent(obs, conf)),\n#     'pi':                   (lambda obs, conf: pi_agent(obs, conf)),\n    'anti_pi':              (lambda obs, conf: anti_pi_agent(obs, conf)),\n#     'anti_anti_pi':         (lambda obs, conf: anti_anti_pi_agent(obs, conf)),\n#     'reactionary':          (lambda obs, conf: reactionary(obs, conf)),\n#     'anti_rotn':            (lambda obs, conf: anti_rotn(obs, conf, warmup=1)),\n    \n    'iou2':                  (lambda obs, conf: iou2_agent(obs, conf)),\n    'geometry':              (lambda obs, conf: geometry_agent(obs, conf)),\n    'memory_patterns_v20':   (lambda obs, conf: memory_patterns_v20(obs, conf)),\n    'testing_please_ignore': (lambda obs, conf: testing_please_ignore(obs, conf)),\n    'bumblepuppy':           (lambda obs, conf: centrifugal_bumblepuppy(obs, conf)), \n    'dllu1_agent':           (lambda obs, conf: dllu1_agent(obs, conf)), \n    \n    'memory_patterns':       (lambda obs, conf: memory_patterns(obs, conf)),\n    'naive_bayes':           (lambda obs, conf: naive_bayes(obs, conf)),\n    'iocaine':               (lambda obs, conf: iocaine_agent(obs, conf)),\n    'greenberg':             (lambda obs, conf: greenberg_agent(obs, conf)),\n    'statistical':           (lambda obs, conf: statistical_prediction_agent(obs, conf)),\n    'statistical_expected':  (lambda obs, conf: statistical_history['expected'][-1] + 1),       \n    'decision_tree_1':       (lambda obs, conf: decision_tree_agent_1(obs, conf, stages=1, window=4)),\n    'decision_tree_2':       (lambda obs, conf: decision_tree_agent_2(obs, conf, stages=2, window=6)),\n    'decision_tree_3':       (lambda obs, conf: decision_tree_agent_3(obs, conf, stages=3, window=10)),\n    'CNN':                   (lambda obs, conf: CNN_agent(obs, conf)),\n    'anti_otm':              (lambda obs, conf: anti_transition_agent(obs, conf)),\n}\n\n# observation   = {'step': 1, 'lastOpponentAction': 1}\n# configuration = {'episodeSteps': 10, 'agentTimeout': 60, 'actTimeout': 1, 'runTimeout': 1200, 'isProduction': False, 'signs': 3}\ndef multi_armed_bandit_agent(observation, configuration, warmup=1, step_reward=3, decay_rate=0.95 ):\n    global mlb_expected\n    global mlb_opponent\n    global mlb_agents\n    time_start = time.perf_counter()\n    if os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Localhost') == 'Interactive':\n        warmup = 1\n\n    \n    if observation.step != 0: \n        mlb_opponent += [ observation.lastOpponentAction ]\n    # else:\n    #     mlb_opponent += [ random_agent(observation, configuration) ]\n    \n    \n    # Implement Multi Armed Bandit Logic\n    win_loss_scores = defaultdict(lambda: [0.0, 0.0])\n    for name, values in list(mlb_expected.items()):\n        for n in range(min(len(values), len(mlb_opponent))):\n            win_loss_scores[name][1] = (win_loss_scores[name][1] - 1) * decay_rate + 1\n            win_loss_scores[name][0] = (win_loss_scores[name][0] - 1) * decay_rate + 1\n            \n            # win | expect rock, play paper -> opponent plays rock\n            if   mlb_expected[name][n] == (mlb_opponent[n] + 0) % configuration.signs:                \n                win_loss_scores[name][0] += step_reward \n                \n            # draw | expect rock, play paper -> opponent plays paper\n            elif mlb_expected[name][n] == (mlb_opponent[n] + 1) % configuration.signs:  \n                win_loss_scores[name][0] += step_reward \n                win_loss_scores[name][1] += step_reward \n                \n            # win | expect rock, play paper -> opponent plays scissors\n            elif mlb_expected[name][n] == (mlb_opponent[n] + 2) % configuration.signs:\n                win_loss_scores[name][1] += step_reward \n      \n\n    # Update predictions for next turn\n    for name, agent_fn in list(mlb_agents.items()):\n        try:\n            agent_action        = agent_fn(observation, configuration)\n            agent_expected      = (agent_action - 1) % configuration.signs\n            mlb_expected[name] += [ agent_expected ]\n        except Exception as exception:\n            print('Exception:', name, agent_fn, exception)\n    \n    \n    # Pick the Best Agent\n    beta_scores = {\n        name: np.random.beta(win_loss_scores[name][0], win_loss_scores[name][1])\n        for name in win_loss_scores.keys()\n    }\n\n    if observation.step == 0:\n        # Always play scissors first move\n        # At Auction       - https:\/\/www.artsy.net\/article\/artsy-editorial-christies-sothebys-played-rock-paper-scissors-20-million-consignment\n        # EDA best by test - https:\/\/www.kaggle.com\/jamesmcguigan\/rps-episode-archive-dataset-eda\n        agent_name = 'scissors'\n        expected = 1  \n    elif observation.step < warmup:\n        agent_name = 'random'\n        expected   = random_agent(observation, configuration)       \n    else:\n        agent_name = sorted(beta_scores.items(), key=itemgetter(1), reverse=True)[0][0]\n        expected   = mlb_expected[agent_name][-1]\n    \n    action = (expected + 1) % configuration.signs\n        \n                \n    time_taken = time.perf_counter() - time_start\n    print(f'opponent        = ', mlb_opponent)\n    print(f'expected        = ', dict(mlb_expected),    '\\n')\n    print(f'win_loss_scores = ', dict(win_loss_scores), '\\n')\n    print(f'beta_scores     = ', dict(beta_scores),     '\\n')\n    print(f'action          =  {action} | agent = {agent_name} | step = {observation.step} | {time_taken:.3f}s')\n    print('-'*20, '\\n')\n    \n    return int(action)\n\n\n\n\n","259b8add":"%%writefile \"CNN.py\"\nimport numpy as np\n\nimport torch\nfrom torch import nn, optim\n\nfrom kaggle_environments import evaluate, make, utils\nfrom kaggle_environments.envs.rps.utils import get_score\nfrom kaggle_environments.envs.rps.agents import *\n\nclass RPS(nn.Module):\n    \"\"\"\n    Class that predict logits of action probabilities given game history.\n        Inputs: game history [bs, 2, 10].\n        Outputs: logits of action probabilities [bs, 3].\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.conv_layers = nn.Sequential(\n            nn.Conv1d(2, 4, 3, 1, 1, bias=False),\n            nn.ReLU(),\n            nn.AvgPool1d(2),\n            nn.Conv1d(4, 8, 3, 1, 1, bias=False),\n            nn.ReLU(),\n            nn.AvgPool1d(2),\n            nn.Conv1d(8, 16, 2, 1, 1, bias=False),\n            nn.ReLU(),\n            nn.AvgPool1d(2)\n        )\n        self.head = nn.Sequential(\n            nn.Linear(16, 6),\n            nn.ReLU(),\n            nn.Linear(6, 3)\n        )\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = torch.flatten(x, 1)\n        x = self.head(x)\n        return x\n\ndef soft_cross_entropy(target, prediciton):\n    log_probs = nn.functional.log_softmax(prediciton, dim=1)\n    sce = -(target * log_probs).sum() \/ target.shape[0]\n    return sce\n\ndef train_step(model, data, optimizer):\n    model.train()\n    torch.set_grad_enabled(True)\n\n    X = data['X'].view(-1, 2, 10)\n    y = data['y'].view(-1, 3)\n    prd = model(X)\n    loss = soft_cross_entropy(y, prd)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nbs = 6 # batch size  \n\nopponent_actions = []\nagent_actions = []\nactions = []\nbatch_x = []\nbatch_y = []\n\n\n\nmodel = RPS()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\ndef agent(observation, configuration):\n    \n    global actions, agent_actions, opponent_actions\n    global model, optimizer\n    global batch_x, batch_y\n    global bs\n    \n    # first step\n    if observation.step == 0:\n        hand = np.random.randint(2)\n        actions.append(hand)\n        return hand\n    \n    # first warm up rounds\n    if 0 < observation.step < 12:\n        opponent_actions.append(observation.lastOpponentAction)\n        agent_actions.append(actions[-1])\n        hand = np.random.randint(2)\n        actions.append(hand)\n        return hand\n    \n    # start to train CNN\n    elif observation.step >= 12:\n        opponent_actions.append(observation.lastOpponentAction)\n        agent_actions.append(actions[-1])\n        \n        wining_action = (opponent_actions[-1] + 1) % 3 \n        fair_action = opponent_actions[-1]\n        lose_action = (opponent_actions[-1] - 1) % 3 \n\n        # soft labels for target    \n        y = [0, 0, 0]\n        y[wining_action] = 0.7\n        y[fair_action] = 0.2\n        y[lose_action] = 0.1 \n        \n        # add data for history\n        batch_x.append([opponent_actions[-2:-12:-1],\n                        agent_actions[-2:-12:-1]])\n        batch_y.append(y)\n        \n        # data for single CNN update \n        data = {'X': torch.Tensor([opponent_actions[-2:-12:-1],\n                                   agent_actions[-2:-12:-1]]),\n                'y': torch.Tensor(y)} \n        \n        # evaluate single training step\n        train_step(model, data, optimizer)\n        \n        # evaluate mini-batch training steps\n        if observation.step % 10 == 0:\n            k = 1 if observation.step < 100 else 3\n            for _ in range(k):\n                idxs = np.random.choice(list(range(len(batch_y))), bs)\n                data = {'X': torch.Tensor(np.array(batch_x)[idxs]),\n                        'y': torch.Tensor(np.array(batch_y)[idxs])}\n                train_step(model, data, optimizer)\n        \n        # data for current action prediction\n        X_prd = torch.Tensor([opponent_actions[-1:-11:-1],\n                              agent_actions[-1:-11:-1]]).view(1, 2, -1)\n        \n        # predict logits\n        probs = model(X_prd).view(3)\n        # calculate probabilities\n        probs = nn.functional.softmax(probs, dim=0).detach().cpu().numpy()\n        \n        # choose action\n        hand = np.random.choice([0, 1, 2], p=probs)\n        actions.append(hand)\n        \n        return int(hand)\n","f6df2fc5":"import numpy as np\nimport random\n\nlr = 0.9\ngamma = 0.96\nepsilon = 0.05 #Start with lots of random moves. We'll change this later on.\nbeat = {0:1, 1:2, 2:0}\ndelta = 0.2   # This is to control the amount of noise. Check chooseAction() for details\n\n# Look at last 5 moves for both of us, so total 10 moves (5 + 5)\n# The total number of states in the Q-Table is 3**(2 * numPast), so this QTable is GIGANTIC!!!! There's no way to fill all values of the table without pre-training.\n# Small Q-Tables don't work. So we're stuck with this for now. \n# Go even bigger, and it doesn't work again. Seems like 5 is the sweet spot.\nnumPast = 5\n\n# Map the game situation to the state number\ngameToState = {}\n\n# Just setting qTable to 0 so that python sees it as a global variable. createQTable will initialize the QTable\nqTable = 0\n\n# Initializes the QTable with 0s with n states and m actions\ndef createQTable(states, actions):\n    global qTable\n    qTable = np.zeros((actions, states))\n    qTable = np.matrix(qTable)\n\n# Creates a dict to map each game scenario (ex - \"0112\") to state number.\ndef createGameToStateMap(numPast):\n    global gameToState\n    count = 0\n    while count < 3**(2*numPast):\n        key = str(np.base_repr(count, base=3))\n        key = key.zfill(2*numPast)\n        gameToState[key] = count\n        count += 1\n\n\n# Chooses action based on QTable. Exploitation vs Exploration is included here.\ndef chooseAction(currentState):\n    global epsilon, qTable, delta\n    if np.random.uniform() < epsilon:  # 1-epsilon % of the time, we want to play random. Else play normally. \n        \n        # The below line of code is one of the most important additions to get Q-Learning to work. \n        tempQ = qTable + (delta * np.random.randn(qTable.shape[0], qTable.shape[1])) #Add noise to the q table. \n        #Noise will be taken from a normal distribution, and the amount of noise is scaled by delta\n        \n        \n        stateAction = tempQ[:, currentState]\n        if np.max(stateAction) == np.min(stateAction):  #Best action has same q-value as weakest action, then pick random.\n            action = random.choice([0, 1, 2])\n        else:\n            action = np.argmax(stateAction)\n            action = int(action)\n    \n    else:\n        action = random.choice([0, 1, 2])\n    return action\n\n\n# Update the QTable based on the rewards received\ndef learn(currentState, action, reward, nextState):\n    global qTable, lr, gamma\n    qPredict = qTable[action, currentState]\n\n    qTarget = reward + gamma * qTable[:, nextState].max()\n\n    qTable[action, currentState] += lr * (qTarget - qPredict)\n\n    \n# To calculate reward\ndef winLose(myMove, oppMove):\n    if oppMove == beat[myMove]:\n        return -1\n    elif oppMove == myMove:\n        return 0\n    elif myMove == beat[oppMove]:\n        return 1\n\n# Convert history array into game string. Used for finding which state in the q-table we are in.\ndef generateGameString(myHist, oppHist, numPast):\n    gameString = \"\"\n    for i in range(numPast):\n        gameString += str(myHist[len(myHist) - 1 - i])\n        gameString += str(oppHist[len(oppHist) - 1 - i])\n    return gameString\n        \nmyHist = []\noppHist = []\nmove = 0\nnextState = 0\ncurrentState = 0\n\ncreateQTable(3**(2*numPast), 3)\ncreateGameToStateMap(numPast)\n\ndef agent(observation, step):\n    global myHist, oppHist, numPast, gameToState, move, nextState, currentState, epsilon\n    game_num = observation.step\n    if game_num == 0:\n        epsilon = 0.05\n        myHist = []\n        oppHist = []\n        nextState = 0\n        currentState = 0\n        move = random.choice([0, 1, 2])\n        return move\n    lastOppMove = observation.lastOpponentAction\n\n    if game_num < 5:\n        myHist.append(move)\n        oppHist.append(lastOppMove)\n        move = random.choice([0, 1, 2])\n        return move\n\n    if game_num < 6: # First time we have just enough history to make a move using the Q-Table, but not enough to train it first.\n        myHist.append(move)\n        oppHist.append(lastOppMove)\n        \n        nextState = gameToState[generateGameString(myHist, oppHist, numPast)]\n        move = chooseAction(nextState)\n        return move\n\n    # Here, we first update the Q-Table based on what the bot played last time. Then we make the next move. \n    currentState = nextState\n    \n    myHist.append(move)\n    oppHist.append(lastOppMove)\n\n    nextState = gameToState[generateGameString(myHist, oppHist, numPast)]\n    reward = winLose(move, lastOppMove)\n    \n    learn(currentState, move, reward, nextState)\n    move = chooseAction(nextState)\n    \n    # This is another important aspect for the bot. Since every opponent is practicaly a new opponent, we have to learn the specific quirks first.\n    # For this, I initially start playing randomly 90% of the time, and over the course of 150 moves, slowly reduce it to only 10% random.\n    if epsilon < 0.9:\n        epsilon += (0.8 \/ 150)\n    return move","3093939e":"# Define all enemies\nagents = ['statistical', 'copy_opponent.py', 'reactionary.py', 'preCoded_hist.py', 'counter_reactionary.py', 'markov_agent', 'memory_patterns.py', \n         'iocaine.py', 'greenberg.py', 'xgboost.py', 'multi_armed_bandit.py', 'opponent_transition_matrix.py', 'decision_tree_classifier.py', 'statistical_prediction.py', \n          'not_losing.py', 'simple_method.py', 'geometry.py', 'anti_geo.py', 'anti_otm.py', 'new_mlb.py', 'CNN.py']","a5df2f91":"# Keep Track of Score performance\nscoresOverTime = {}\nfor opp in agents:\n    scoresOverTime[opp] = []\n\nscoresOverTime","d0891224":"epochs = 25\n\nfor i in range(epochs):\n    print()\n    print(\"EPOCH  \" + str(i + 1) + \" ---------------------------------\")\n    for opp in agents:\n        print(\"Now Playing Agent \" + opp, end='')\n        gamePlayed = env.run([agent, opp])\n        print(\"   Score -> \" + str(gamePlayed[len(gamePlayed) - 1][0]['reward']))\n        scoresOverTime[opp].append(gamePlayed[len(gamePlayed) - 1][0]['reward'])\nprint(\"Finished Training\")","fb6b6631":"qTable","85727d9c":"np.save('qtable', qTable)","8fc2f227":"A small thing to keep in mind: the above code block cannot be a separate .py file, as we DON'T want to reset the Q-Table every time. After we are done training, we can convert it to a .py file for submission.","60dd5b79":"This is the more comprehensive Multi-Layered Bandit by JamesMcGuigan. https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-multi-stage-decision-tree.\nI have made some modifications to make it stronger (just including some other top public agents). This is infact close to my top agent on the leaderboard.","d1bbccbb":"Save the Q-Table for use later on. When you submit the Q-Learning bot for the leaderboard, copy the above q-learning code. Then inside of `createQTable()`, read in the .npy file instead of initializing it with 0s. ","40ce2f88":"There are a few insights I learned from getting the Q-learning algorithm to work.\n1. Looking at 5 moves of history (per player) is close to the most optimum history length for Q-Learning.\n2. Add noise to the Q-Table to make it robust.\n\nThe second point is extremely crucial in getting the agent to work. The reason is that a Q-Table is static and doesn't change much from move to move. Thus, any medium-strong agent can quickly learn and exploit the q-table. However, if you add noise to the q-table, this can cause the agent to pick the second best (or 3rd best in the extreme) move, which reduces the predictability of the agent.\n\nAs all things in life, the amount of noise has to be just right. Too less and your agent can be exploited. Too much and your agent can't use its knowledge, and becomes a random agent.\n\nThe code is commented providing as many details as I can","8b2d3930":"# Intro\nIn this notebook, I explored the idea of making a Q-Learning bot do well. I was surprised by how more common reinforcement learning models in other tasks (Q-learning, Deep Q-learning, etc) weren't working very well in the RPS competition, as I had mentioned in this discussion sometime back: https:\/\/www.kaggle.com\/c\/rock-paper-scissors\/discussion\/201837\n\nThen I worked hard and I think I got it to work.\nI developed a crude way of training the Q-Learning model, and after that the agent has an average score of ~850-900, which is not bad. It's probably good to include it in an ensemble, but wasn't able to test the idea out extensively.\n\nI noticed that this bot is extremely flexible and really does learn (if you train it right). After geometry bot was released, my old Q-Learning bot (and others) were in bit of a mess. I just added it in the agents dict, and now the Q-Learning bot is about neutral (maybe slightly positive) against Geometry Bot. \n\nI also highly recommend to use this agent in an ensemble, as relatively static Q-Tables in general can be vulnerable to exploitation. \n\nI'll add comments explaining each part of the code.","c6f66ea3":"# Q-Learning Code","6863ba86":"And that's it! With just this much code, you can decently train the Q-Learning bot, and get a score of about ~900. Now since this is a single agent (and a relatively static bot), it's score can vary, so including it in an ensemble will probably give better results.\n\n\n# Potential Improvements\n\n1. When looking at my agent's logs, I saw that often it lost around 10 points within the first 100 moves (even though the bot is practicaly random at this point). I think it has to do with the fact that I am using `random.randint`. Maybe using numpy or a combination of random number generators could make it betteer.\n2. Sometimes, the Q-Table would take a lead of like 30-40 points after just 500 games, but then the opponent would figure out the table and end up winning \/ drawing. I didn't do it here, but it could easily be solved by playing random after a strong lead. Avoid the Q-Table from being exploited.\n3. I could get the training to work very well with Tony Robinson's RPS logs. If you can train it using that, you can even get the Q-Table to learn from hidden agents like the one by Stas SL!\n\n\nI hope this showed some modifications that I found interesting and helped the Q-Learning algo to work. I didn't expect such a gigantic Q-Table to be feasible, but with training it just is.","ca3ca0f9":"The crux of the training here, is that the Q-Learning agent learns from experience. For a strong agent, it needs good experience. For this, I pretty much picked all of the top public agents shared on Kaggle. I don't have the links for the opponents, but most are compiled in https:\/\/www.kaggle.com\/ihelon\/rock-paper-scissors-agents-comparison","ebc59f71":"Let's just see the first few values of the Q-Table...","1b8f782e":"This is the part where we train. \nEpochs is the number of times the Q-Learning bot plays against all agents. I just set it at 25 for now. Too less and the bot doesn't learn enough, too much and it overfits to the agents in the sample.","9fd60429":"So now let's just define all of the agents in separate .py files.","f7ced4dc":"# Training\n\nPre-training the Q-Table is what got my agent to work. Since the table has about 200,000 values, it needs pre-training to understand the \"general structure\" of an agent. ","8b209fa0":"# Enemy Agents","ca4186ed":"Based on the evaluation, I noticed that the Q-Learning bot is relatively neutral with Geometry Bot! About half the time (little more) it wins, and the other half it loses. Overall, with my experiments I saw that this bot is extremely flexible. Any new game changer bot is released to the public. Just add it in the agents list, and the Q-Table will do a decent job against it. "}}