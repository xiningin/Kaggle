{"cell_type":{"6d5d4e39":"code","d9059f5f":"code","621cc4a4":"code","d2e4cd55":"code","1e90fed0":"code","a47defed":"code","59fab3b3":"code","20ffd0ef":"code","af8ae7ee":"code","9e276c55":"code","162cc20d":"code","8bb93d8b":"code","f81f23ba":"code","9292af74":"code","9a5f4c1b":"code","343a6266":"code","8cb006a2":"code","b975d5b6":"code","4a908dee":"code","118e033d":"code","90c4ed59":"code","8af71070":"markdown","17d1d734":"markdown","f8ec0387":"markdown","412cf86d":"markdown","53bcf4be":"markdown","7f45000d":"markdown","f63684f6":"markdown","5fec2fc6":"markdown","949e13b4":"markdown","2efb2d4b":"markdown","9be21e1e":"markdown","dc95f2c9":"markdown","592cd1af":"markdown"},"source":{"6d5d4e39":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d9059f5f":"train_data = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/test.csv')","621cc4a4":"print('The number of categorical target :', len(train_data['Cover_Type'].unique()))","d2e4cd55":"train_data.head()","1e90fed0":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ntrain_data['Count'] = 1\ntrain_data_cover_type = train_data.groupby('Cover_Type').sum()\nsns.set()\nsns.barplot(x = train_data_cover_type.index, y = train_data_cover_type.Count)\nplt.title('The relation between Cover_Type and Count')","a47defed":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set()\nfig, axes = plt.subplots(1, 7, figsize=(50, 10))\nfig.suptitle('The features : Elevation', fontsize=30)\nfor i in range(7):\n    Cover_type_cate = train_data[train_data['Cover_Type'] == (i+1)]\n    sns.histplot(x =  Cover_type_cate.Elevation, ax = axes[i])\n    axes[i].set_title(f'Cover_Type : {i+1}', fontsize=30)\n    axes[i].set_xlabel('Elevation', fontsize=30)\n    axes[i].set_ylabel('Count', fontsize=30)\n","59fab3b3":"sns.set()\nfig, axes = plt.subplots(1, 7, figsize=(40, 10))\nfig.suptitle('The features : Aspect', fontsize=30)\nfor i in range(7):\n    Cover_type_cate = train_data[train_data['Cover_Type'] == (i+1)]\n    sns.histplot(x =  Cover_type_cate.Aspect, ax = axes[i])\n    axes[i].set_title(f'Cover_Type : {i+1}', fontsize=30)\n    axes[i].set_xlabel('Aspect', fontsize=30)\n    axes[i].set_ylabel('Count', fontsize=30)","20ffd0ef":"sns.set()\nfig, axes = plt.subplots(1, 7, figsize=(40, 10))\nfig.suptitle('The features : Slope', fontsize=30)\nfor i in range(7):\n    Cover_type_cate = train_data[train_data['Cover_Type'] == (i+1)]\n    sns.histplot(x =  Cover_type_cate.Slope, ax = axes[i])\n    axes[i].set_title(f'Cover_Type : {i+1}', fontsize=30)\n    axes[i].set_xlabel('Slope', fontsize=30)\n    axes[i].set_ylabel('Count', fontsize=30)","af8ae7ee":"sns.set()\nfig, axes = plt.subplots(1, 7, figsize=(40, 10))\nfig.suptitle('The features : Horizontal_Distance_To_Hydrology', fontsize=30)\nfor i in range(7):\n    Cover_type_cate = train_data[train_data['Cover_Type'] == (i+1)]\n    sns.histplot(x =  Cover_type_cate.Horizontal_Distance_To_Hydrology, ax = axes[i])\n    axes[i].set_title(f'Cover_Type : {i+1}', fontsize=30)\n    axes[i].set_xlabel('Horizontal_Distance_To_Hydrology', fontsize=15)\n    axes[i].set_ylabel('Count', fontsize=30)","9e276c55":"\nsns.set()\nfig, axes = plt.subplots(1, 7, figsize=(40, 10))\nfig.suptitle('The features : Vertical_Distance_To_Hydrology ', fontsize=30)\nfor i in range(7):\n    Cover_type_cate = train_data[train_data['Cover_Type'] == (i+1)]\n    sns.histplot(x =  Cover_type_cate.Vertical_Distance_To_Hydrology, ax = axes[i])\n    axes[i].set_title(f'Cover_Type : {i+1}', fontsize=30)\n    axes[i].set_xlabel('Vertical_Distance_To_Hydrology', fontsize=15)\n    axes[i].set_ylabel('Count', fontsize=30)","162cc20d":"sns.set()\nfig, axes = plt.subplots(1, 7, figsize=(40, 10))\nfig.suptitle('The features : Horizontal_Distance_To_Roadways ', fontsize=30)\nfor i in range(7):\n    Cover_type_cate = train_data[train_data['Cover_Type'] == (i+1)]\n    sns.histplot(x =  Cover_type_cate.Horizontal_Distance_To_Roadways, ax = axes[i])\n    axes[i].set_title(f'Cover_Type : {i+1}', fontsize=30)\n    axes[i].set_xlabel('Horizontal_Distance_To_Roadways', fontsize=15)\n    axes[i].set_ylabel('Count', fontsize=30)","8bb93d8b":"train_data = train_data.drop(columns = ['Count', 'Id'])\ntest_data = test_data.drop(columns = ['Id'])","f81f23ba":"from sklearn.preprocessing import MinMaxScaler\ndata = train_data.drop(columns = 'Cover_Type')\ntarget = train_data['Cover_Type']\n\nScale = MinMaxScaler()\ndata = Scale.fit_transform(data)\ntest_data = Scale.transform(test_data)\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import plot_model, to_categorical\nx_train, x_test, y_train, y_test = train_test_split(data, target, train_size = 0.8, random_state = 10)\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)","9292af74":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.utils import plot_model, to_categorical\nmodel = Sequential()\nmodel.add(Dense(128, activation = 'relu', input_shape = (x_train.shape[1],)))\nmodel.add(Dense(32, activation = 'relu'))\nmodel.add(Dense(8, activation = 'softmax'))\nmodel.compile(optimizer= 'adam', \n              loss = 'categorical_crossentropy', \n              metrics = ['accuracy'])\n\nhistory = model.fit(x_train, y_train, epochs = 30, validation_split = 0.2, batch_size = 512, verbose = 0)","9a5f4c1b":"df_DL = pd.DataFrame(history.history)\ndf_DL.head()","343a6266":"plt.plot(df_DL.index, df_DL['loss'], label = 'loss')\nplt.plot(df_DL.index, df_DL['val_loss'], label = 'Val_loss')\nplt.xlabel( 'Epochs')\nplt.ylabel('Binary_crossentropy')\nplt.title('DL loss function')\nplt.legend()","8cb006a2":"y_pred = model.predict(x_test)","b975d5b6":"y_test = np.argmax(y_test, axis = 1)\ny_pred = np.argmax(y_pred, axis = 1)","4a908dee":"from sklearn.metrics import confusion_matrix\nplt.figure(figsize = (15, 15))\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True)","118e033d":"submission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')\nsubmission['Cover_Type'] = np.argmax(model.predict(test_data), axis = 1)\nsubmission.to_csv('submission.csv', index=False)","90c4ed59":"submission['Cover_Type'].unique()","8af71070":"## 2-5. Horizontal_Distance_To_Hydrology Feature","17d1d734":"## 2-4. Slope Feature","f8ec0387":"# 2. EDA","412cf86d":"## 2-7. Horizontal_Distance_To_Roadways Feature","53bcf4be":"## 3-2. Comparsion between Reality and Prediction -> Confusion Matrix","7f45000d":"# 3. Deep Learning Model","f63684f6":"## 2-2. Elevation features","5fec2fc6":"# 4. Submission","949e13b4":"## 2-3. Aspect Feature","2efb2d4b":"## 3-1. Model's Training Process","9be21e1e":"## 2-6. Vertical_Distance_To_Hydrology Feature","dc95f2c9":"## 2-1. The count of each Cover_Type","592cd1af":"# 1. Read The Data"}}