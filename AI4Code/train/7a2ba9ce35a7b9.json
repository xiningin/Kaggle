{"cell_type":{"2f607d36":"code","20638781":"code","51129e1e":"code","3feb64ea":"code","7b39de0a":"code","b1f4078a":"code","8588391d":"code","1ced278a":"code","6a21b9e1":"code","30b4c8d1":"code","83ddbe06":"code","beeed01a":"code","ef8b50c8":"code","de13b272":"code","25e54599":"code","195e8754":"code","4dc993a5":"code","ade1bfc2":"markdown"},"source":{"2f607d36":"# importing libraries\nimport os\nimport numpy as np \nimport pandas as pd\nimport scipy.io\nimport warnings\nimport tensorflow as tf\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import optimizers\nfrom scipy import signal\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\n\nwarnings.filterwarnings('ignore')\nos.listdir('\/kaggle\/input\/BloodPressureDataset')","20638781":"# Creating a config file\/dictionary some variables.\n\nconfig = {\n    'seed': 77,\n    'sample_size': 125,\n    'train_size': 3000000, # train on 3 million data points\n    'train_params':{\n        'batch_size':128,\n        'epochs': 10,\n        'learn_rate': 1e-3,\n    }\n}","51129e1e":"# Initializing TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# Instantiate distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    \nauto = tf.data.experimental.AUTOTUNE\nreplicas = tpu_strategy.num_replicas_in_sync\nBatch_size = config['train_params']['batch_size'] * replicas\nprint(f'Number of Replicas: {replicas}')","3feb64ea":"def SetSeed(seed=None):\n    \"\"\"Set seed for reproducibility.\"\"\"\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    \nSetSeed(seed=config['seed'])\n    ","7b39de0a":"# Getting dataset from GCS\n# since TPUs stream data from GCS\n#gcs_dataset_path = KaggleDatasets().get_gcs_path('BloodPressureDataset')\n#train_files = tf.io.gfile.glob(os.path.join(gcs_dataset_path, '*.mat'))\n#train_files  ","b1f4078a":"# Loading a sample .mat file to understand the data dimensions\ntest_sample = scipy.io.loadmat(f'..\/input\/BloodPressureDataset\/part_{1}.mat')['p']\nprint(f'test_sample Data type: {type(test_sample)}')\nprint(f'test_sample shape\/dimensions: {test_sample.shape}')","8588391d":"print(f\"Total Samples: {len(test_sample[0])}\")\nprint(f\"Number of readings in each sample(column): {len(test_sample[0][0])}\")\nprint(f\"Number of samples in each reading(ECG): {len(test_sample[0][0][2])}\")\n\ntemp_mat = test_sample[0, 999]\ntemp_length = temp_mat.shape[1]\nsample_size = config['sample_size']\n\n\nprint(temp_length)\nprint((int)(temp_length\/sample_size))","1ced278a":"# Extracting signal data\nppg = []\nbp = []\necg = []\n\nfor i in range(1000):\n    temp_mat = test_sample[0, i]\n    temp_length = temp_mat.shape[1]\n    for j in range((int)(temp_length\/sample_size)):\n        temp_ppg = temp_mat[0, j*sample_size:(j+1)*sample_size]\n        temp_ecg = temp_mat[2, j*sample_size:(j+1)*sample_size]\n        temp_bp = temp_mat[1, j*sample_size:(j+1)*sample_size]\n        ppg.append(temp_ppg)\n        ecg.append(temp_ecg)\n        bp.append(temp_bp)","6a21b9e1":"# Reshaping the ecg, ppg and bp signal data into column vectors\nppg, ecg, bp = np.array(ppg).reshape(-1,1), np.array(ecg).reshape(-1,1), np.array(bp).reshape(-1,1)\nprint(f'PPG_shape: {ppg.shape}\\n ECG_shape: {ecg.shape}\\n BP_shape: {bp.shape}')","30b4c8d1":"##plotting sample ppg, ecg and bp signals\n##using a sample size of 125\nfig, ax = plt.subplots(3,1, figsize=(9,12), sharex=True)\n\nax[0].set_title('PPG graph', fontsize=16)\nax[0].set_ylabel('Signal Value')\nax[0].plot(ppg[:125])\n\nax[1].set_title('ECG graph', fontsize=16)\nax[1].set_ylabel('Signal Value')\nax[1].plot(ecg[:125])\n\nax[2].set_title('Blood Pressure (BP) graph', fontsize=16)\nax[2].set_ylabel('Signal Value')\nax[2].set_xlabel('Sample size')\nax[2].plot(bp[:125])","83ddbe06":"# creating train and test sets\nX_train, X_test, y_train, y_test = train_test_split(ppg, bp, test_size=0.30)","beeed01a":"# Baseline Sequential Model\ndef Model(input_dim, num_class):\n    model = Sequential()\n\n    model.add(layers.Dense(1024, input_dim = input_dim, activation='relu'))\n    model.add(layers.Dropout(0.5))\n\n    model.add(layers.Dense(512, 'relu')) \n    model.add(layers.Dropout(0.5))\n\n    model.add(layers.Dense(64, 'relu'))    \n    model.add(layers.Dropout(0.25))\n    model.add(layers.Dense(num_class, 'linear'))\n    \n    \n    return model\n\nInput_dim = X_train.shape[1]\nClasses = 1\n\n# Creating the model on TPU\nwith tpu_strategy.scope():\n    model = Model(input_dim=Input_dim, num_class=Classes)\n    model.compile(loss=tf.keras.losses.MeanAbsoluteError(),\n                  optimizer=optimizers.SGD(lr=config['train_params']['learn_rate']),\n                  metrics=[tf.keras.metrics.RootMeanSquaredError()]\n                 )\nmodel.summary()","ef8b50c8":"# Training the model\nhistory = model.fit(X_train[:config['train_size']], # using the first 3 million rows.\n                    y_train[:config['train_size']].squeeze(),\n                    epochs=config['train_params']['epochs'],\n                    batch_size=Batch_size,\n                    verbose = 1\n                   )","de13b272":"#Predicting on the test set using the nn(neural network) model\nnn_predictions = model.predict(X_test[:config['train_size']])\n\nrmse = tf.keras.metrics.RootMeanSquaredError()\nrmse.update_state(y_test[:config['train_size']], nn_predictions)\nprint(f'Model RMSE: {rmse.result().numpy()}')","25e54599":"plt.title('Train loss against Root_Mean_Squared_error')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.plot(history.history['loss'])\nplt.plot(history.history['root_mean_squared_error'])\nplt.legend(['Loss', 'Root_Mean_Squared_error'])","195e8754":"# Visualize predicted BP and the True BP\nplt.title(\"===True BP values Vs Predicted BP values===\")\nplt.xlabel('Number of samples taken')\nplt.ylabel('BP values')\nplt.plot(y_test[:100]) #only plotting 100 samples\nplt.plot(nn_predictions[:100])\nplt.legend(['True_BP', 'Predicted_BP'])","4dc993a5":"# Visualizing the model\n#tf.keras.utils.plot_model(model)","ade1bfc2":"# Cuff_Less Blood Pressure Estimation with TPUs\n\n**Overview:**\n\n#### In this notebook I'll use TPUs(Tensor Processing Units) to train a deep learning model for Blood Pressure Estimation.\n\nThis notebook builds on top of my [Blood Pressure Analysis Notebook](https:\/\/www.kaggle.com\/stephenmugisha\/bloodpressure-analysis). Visit that analysis notebook to get an understanding of the data.\n\n<strong style=\"color:red\">**Kindly UPVOTE**<\/strong> if you find these notebooks helpful.\n"}}