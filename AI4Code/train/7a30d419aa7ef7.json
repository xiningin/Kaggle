{"cell_type":{"9b1873c4":"code","e5df3564":"code","53ac48d8":"code","a97bdf08":"code","85bf3dee":"code","ce4d61b7":"code","7611e9ff":"code","12d81a60":"code","ea8fec67":"code","66ebc00a":"code","c6f459b4":"code","027bdd38":"code","f500ce74":"code","88a89e70":"code","bc9fb801":"code","5195faed":"code","7cf918cc":"code","cccbcfce":"code","b7de8d70":"code","c67a97f5":"code","62ac87c0":"code","51771e18":"code","a75393ef":"code","7ae66049":"code","f54855d5":"code","5ff28ad5":"code","69014acd":"code","f9bfad86":"code","23a7a9f5":"code","e86acf2c":"code","a35694bc":"code","247c55c5":"code","56c6951f":"code","d615fefc":"code","58ff654e":"code","f41f4160":"code","f976e585":"code","0a735710":"code","d9801f4b":"code","934adf75":"code","b3266829":"code","0c7a9215":"code","98825200":"code","a61b9f73":"code","ffb86eda":"code","3ebecb5d":"code","adda6aee":"code","71a80af0":"code","c2c19277":"code","85beaa78":"code","f9095b2a":"markdown","3c51998c":"markdown","c48aef5b":"markdown","93864395":"markdown","ea536fd6":"markdown","29ebcb1e":"markdown","2df2d95e":"markdown","852b093c":"markdown","6abca09a":"markdown","01e4ec69":"markdown","b47de3f3":"markdown","54100671":"markdown","f357b51c":"markdown","110e1138":"markdown","f1193413":"markdown","a3650568":"markdown","177863b3":"markdown","483bec65":"markdown","0be3ed36":"markdown"},"source":{"9b1873c4":"import pandas as pd #Analysis \nimport matplotlib.pyplot as plt #Visulization\nimport seaborn as sns #Visulization\nimport numpy as np #Analysis \nfrom scipy.stats import norm #Analysis \nfrom sklearn.preprocessing import StandardScaler #Analysis \nfrom scipy import stats #Analysis \nimport warnings \nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport gc\nimport spacy\nimport os\nimport string\ncolor = sns.color_palette()\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport lightgbm as lgb\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999","e5df3564":"from bs4 import BeautifulSoup\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers","53ac48d8":"df_train = pd.read_csv('..\/input\/kuc-hackathon-winter-2018\/drugsComTrain_raw.csv')\ndf_test = pd.read_csv('..\/input\/kuc-hackathon-winter-2018\/drugsComTest_raw.csv')\n","a97bdf08":"# as both the dataset contains same columns we can combine them for better analysis\ndf_train.describe()","85bf3dee":"df_train.dtypes","ce4d61b7":"df_train.isnull().any()","7611e9ff":"df_train = df_train.dropna(how=\"any\", axis=0)\ndf_train.isnull().any()","12d81a60":"sns.set(font_scale = 1.2, style = 'darkgrid')\nplt.rcParams['figure.figsize'] = [15, 8]\n\nrating = dict(df_train.loc[df_train.rating == 1, \"drugName\"].value_counts())\ndrugname = list(rating.keys())\ndrug_rating = list(rating.values())\n\nsns_rating = sns.barplot(x = drugname[0:20], y = drug_rating[0:20], palette = 'winter')\n\nsns_rating.set_title('Top 20 drugs with 1\/10 rating')\nsns_rating.set_ylabel(\"Number of Ratings\")\nsns_rating.set_xlabel(\"Drug Names\")\nplt.setp(sns_rating.get_xticklabels(), rotation=90);","ea8fec67":"size = [68005, 46901, 36708, 25046, 12547, 10723, 8462, 6671]\ncolors = ['pink', 'cyan', 'maroon',  'magenta', 'orange', 'navy', 'lightgreen', 'yellow']\nlabels = \"10\", \"1\", \"9\", \"8\", \"7\", \"5\", \"6\", \"4\"\n\nmy_circle = plt.Circle((0, 0), 0.7, color = 'white')\n\nplt.rcParams['figure.figsize'] = (10, 10)\nplt.pie(size, colors = colors, labels = labels, autopct = '%.2f%%')\nplt.axis('off')\nplt.title('Pie Chart Representation of Ratings', fontsize = 25)\np = plt.gcf()\nplt.gca().add_artist(my_circle)\nplt.legend()\nplt.show()","66ebc00a":"from wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\n\nstop_words = set(STOPWORDS)\n\n\nwordcloud = WordCloud(background_color = 'lightblue', stopwords = stop_words, width = 1200, height = 800).generate(str(df_train['review']))\n\nplt.rcParams['figure.figsize'] = (15, 15)\nplt.title('WORD CLOUD OF REVIEWS', fontsize = 25)\nprint(wordcloud)\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()","c6f459b4":"df_train['date'] = pd.to_datetime(df_train['date'], errors = 'coerce')\n\n# now extracting year from date\ndf_train['Year'] = df_train['date'].dt.year\n\n# extracting the month from the date\ndf_train['month'] = df_train['date'].dt.month\n\n# extracting the days from the date\ndf_train['day'] = df_train['date'].dt.day","027bdd38":"plt.rcParams['figure.figsize'] = (19, 8)\nsns.countplot(df_train['Year'], palette ='colorblind')\nplt.title('The No. of Reviews each year', fontsize = 30)\nplt.xlabel('Year', fontsize = 15)\nplt.ylabel('Count of Reviews', fontsize = 15)\nplt.show()","f500ce74":"plt.rcParams['figure.figsize'] = (19, 8)\nsns.countplot(df_train['month'], palette ='tab10')\nplt.title('The No. of Reviews each Month', fontsize = 30)\nplt.xlabel('Months', fontsize = 15)\nplt.ylabel('Ratings', fontsize = 15)\nplt.show()","88a89e70":"df_all = pd.concat([df_train,df_test])","bc9fb801":"from collections import defaultdict\ndf_all_6_10 = df_all[df_all[\"rating\"]>5]\ndf_all_1_5 = df_all[df_all[\"rating\"]<6]","5195faed":"from wordcloud import WordCloud, STOPWORDS\n# Obtain Ngrams from the text, degree here n_gram=1\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart \ndef horizontal_bar_chart(df, color):\n    # specify the data into the plotly function \n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n# Generate Bi-grams\n\n# to map item with a default key \nfreq_dict = defaultdict(int)\n# Create a dictionary of frequency of words in review text with rating 1-5\nfor sent in df_all_1_5[\"review\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\n# Sort the list wrt frequency\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n# We create two columns for the word and its count \nfd_sorted.columns = [\"word\", \"wordcount\"]\n# Plot the Biagram\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n# Create a dictionary of frequency of words in review text with rating 6-10\nfreq_dict = defaultdict(int)\nfor sent in df_all_6_10[\"review\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent biagrams of rating 1 to 5\", \n                                          \"Frequent biagrams of rating 6 to 10\"])\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig['layout'].update(height=1200, width=1000, paper_bgcolor='rgb(233,233,233)', title=\"Ngram - Bigram Count Plots\")\npy.iplot(fig, filename='word-plots')","7cf918cc":"# Generate Tri-grams\n\n# to map item with a default key \nfreq_dict = defaultdict(int)\n# Create a dictionary of frequency of words in review text with rating 1-5\nfor sent in df_all_1_5[\"review\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\n# Sort the list wrt frequency\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n# We create two columns for the word and its count \nfd_sorted.columns = [\"word\", \"wordcount\"]\n# Plot the Triagram\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'lightblue')\n# Create a dictionary of frequency of words in review text with rating 6-10\nfreq_dict = defaultdict(int)\nfor sent in df_all_6_10[\"review\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(50), 'lightblue')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent Triagrams of rating 1 to 5\", \n                                          \"Frequent Triagrams of rating 6 to 10\"])\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig['layout'].update(height=1200, width=1000, paper_bgcolor='rgb(233,233,233)', title=\"Ngram - Triagram Count Plots\")\npy.iplot(fig, filename='word-plots')","cccbcfce":"# Generate Quad-grams ie N = 4\n\n# to map item with a default key \nfreq_dict = defaultdict(int)\n# Create a dictionary of frequency of words in review text with rating 1-5\nfor sent in df_all_1_5[\"review\"]:\n    for word in generate_ngrams(sent,4):\n        freq_dict[word] += 1\n# Sort the list wrt frequency\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n# We create two columns for the word and its count \nfd_sorted.columns = [\"word\", \"wordcount\"]\n# Plot the Quad-gram\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'lightpink')\n# Create a dictionary of frequency of words in review text with rating 6-10\nfreq_dict = defaultdict(int)\nfor sent in df_all_6_10[\"review\"]:\n    for word in generate_ngrams(sent,4):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(50), 'lightpink')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent Quad-grams of rating 1 to 5\", \n                                          \"Frequent Quad-grams of rating 6 to 10\"])\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig['layout'].update(height=1200, width=1000, paper_bgcolor='rgb(233,233,233)', title=\"Ngram - Quad-gram Count Plots\")\npy.iplot(fig, filename='word-plots')","b7de8d70":"df_train = df_train.dropna(axis=0)\ndf_test = df_test.dropna(axis=0)","c67a97f5":"df_all = pd.concat([df_train,df_test]).reset_index()\ndel df_all['index']\npercent = (df_all.isnull().sum()).sort_values(ascending=False)\npercent.plot(kind=\"bar\", figsize = (14,6), fontsize = 10, color='green')\nplt.xlabel(\"Columns\", fontsize = 20)\nplt.ylabel(\"\", fontsize = 20)\nplt.title(\"Total Missing Value \", fontsize = 20)","62ac87c0":"def POS_Tag():\n    nlp=spacy.load('en_core_web_sm')\n    text=df_all['review'][0][1:-1]\n    print(text)\n    for token in nlp(text):\n        print(token.text,'=>',token.dep_,'=>',token.head.text)\n    spacy.displacy.render(nlp(text),jupyter=True)\nPOS_Tag()","51771e18":"all_list = set(df_all.index)\nspan_list = []\nfor i,j in enumerate(df_all['condition']):\n    if '<\/span>' in j:\n        span_list.append(i)","a75393ef":"new_idx = all_list.difference(set(span_list))\ndf_all = df_all.iloc[list(new_idx)].reset_index()\ndel df_all['index']","7ae66049":"from bs4 import BeautifulSoup\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer","f54855d5":"stops = set(stopwords.words('english'))\n#stops\nnot_stop = [\"aren't\",\"couldn't\",\"didn't\",\"doesn't\",\"don't\",\"hadn't\",\"hasn't\",\"haven't\",\"isn't\",\"mightn't\",\"mustn't\",\"needn't\",\"no\",\"nor\",\"not\",\"shan't\",\"shouldn't\",\"wasn't\",\"weren't\",\"wouldn't\"]\nfor i in not_stop:\n    stops.remove(i)","5ff28ad5":"stemmer = SnowballStemmer('english')\n\ndef review_to_words(raw_review):\n    # 1. Delete HTML \n    review_text = BeautifulSoup(raw_review, 'html.parser').get_text()\n    # 2. Make a space\n    letters_only = re.sub('[^a-zA-Z]', ' ', review_text)\n    # 3. lower letters\n    words = letters_only.lower().split()\n    # 5. Stopwords \n    meaningful_words = [w for w in words if not w in stops]\n    # 6. Stemming\n    stemming_words = [stemmer.stem(w) for w in meaningful_words]\n    # 7. space join words\n    return( ' '.join(stemming_words))","69014acd":"%time df_all['review_clean'] = df_all['review'].apply(review_to_words)","f9bfad86":"# Making a rating system of our own, encoding the rating, and creating a opinion column \n\ndf_all['opinion'] = df_all[\"rating\"].apply(lambda x: 1 if x > 5 else 0)","23a7a9f5":"df_train, df_test = train_test_split(df_all, test_size=0.33, random_state=42) \ndf_train.head()","e86acf2c":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import Pipeline\n\nvectorizer = CountVectorizer(analyzer = 'word', \n                             tokenizer = None,\n                             preprocessor = None, \n                             stop_words = None, \n                             min_df = 2, \n                             ngram_range=(4, 4),\n                             max_features = 20000\n                            )\nvectorizer","a35694bc":"pipeline = Pipeline([\n    ('vect', vectorizer),\n])\npipeline","247c55c5":"%time train_data_features = pipeline.fit_transform(df_train['review_clean'])\n%time test_data_features = pipeline.fit_transform(df_test['review_clean'])","56c6951f":"from tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Bidirectional, LSTM, BatchNormalization, Dropout\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences","d615fefc":"import numpy as np\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport random\n\n# 1. Dataset\ny_train = df_train['opinion']\ny_test = df_test['opinion']\nsolution = y_test.copy()\n\n# 2. Model Structure\nmodel = keras.models.Sequential()\n\nmodel.add(keras.layers.Dense(200, input_shape=(20000,)))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Activation('relu'))\nmodel.add(keras.layers.Dropout(0.5))\n\nmodel.add(keras.layers.Dense(300))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Activation('relu'))\nmodel.add(keras.layers.Dropout(0.5))\n\nmodel.add(keras.layers.Dense(100, activation='relu'))\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))\n\n# 3. Model compile\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","58ff654e":"model.summary()","f41f4160":"# 4. Train model\nhist = model.fit(train_data_features, y_train, epochs=10, batch_size=64)\n\n# 5. Traing process\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nfig, loss_ax = plt.subplots()\n\nacc_ax = loss_ax.twinx()\n\nloss_ax.set_ylim([0.0, 1.0])\nacc_ax.set_ylim([0.0, 1.0])\n\nloss_ax.plot(hist.history['loss'], 'y', label='train loss')\nacc_ax.plot(hist.history['acc'], 'b', label='train acc')\n\nloss_ax.set_xlabel('epoch')\nloss_ax.set_ylabel('loss')\nacc_ax.set_ylabel('accuray')\n\nloss_ax.legend(loc='upper left')\nacc_ax.legend(loc='lower left')\n\nplt.show()\n\n# 6. Evaluation\nloss_and_metrics = model.evaluate(test_data_features, y_test, batch_size=32)\nprint('loss_and_metrics : ' + str(loss_and_metrics))\n","f976e585":"from sklearn.metrics import confusion_matrix\nsub_preds_deep = model.predict(test_data_features,batch_size=32)","0a735710":"for i in range(len(sub_preds_deep)): \n    if(sub_preds_deep[i] < 0.5): \n        sub_preds_deep[i] = 0\n    else: sub_preds_deep[i] = 1\nsub_preds_deep","d9801f4b":"soln = df_test['opinion']\nconfusion_matrix(y_pred=sub_preds_deep, y_true=soln)","934adf75":"from sklearn.metrics import classification_report\nprint(classification_report(y_pred=sub_preds_deep, y_true=soln, labels=[0, 1]))","b3266829":"len_train = df_train.shape[0]\ndf_all = pd.concat([df_train,df_test])\ndel df_train, df_test;\ngc.collect()","0c7a9215":"df_all['date'] = pd.to_datetime(df_all['date'])\ndf_all['day'] = df_all['date'].dt.day\ndf_all['year'] = df_all['date'].dt.year\ndf_all['month'] = df_all['date'].dt.month","98825200":"from textblob import TextBlob\nfrom tqdm import tqdm\nreviews = df_all['review_clean']\n\nPredict_Opinion = []\nfor review in tqdm(reviews):\n    blob = TextBlob(review)\n    Predict_Opinion += [blob.sentiment.polarity]\ndf_all[\"Predict_Opinion\"] = Predict_Opinion\ndf_all.head()","a61b9f73":"reviews = df_all['review']\n\nPredict_Opinion = []\nfor review in tqdm(reviews):\n    blob = TextBlob(review)\n    Predict_Opinion += [blob.sentiment.polarity]\ndf_all[\"Predict_Opinion2\"] = Predict_Opinion","ffb86eda":"#Sentence length (counting the number of newline marks)\ndf_all['count_sent']=df_all[\"review\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n\n#Word count in each comment:(number of words)\ndf_all['count_word']=df_all[\"review_clean\"].apply(lambda x: len(str(x).split()))\n\n#Unique word count(unique number of words)\ndf_all['count_unique_word']=df_all[\"review_clean\"].apply(lambda x: len(set(str(x).split())))\n\n#Letter count(review length)\ndf_all['count_letters']=df_all[\"review_clean\"].apply(lambda x: len(str(x)))\n\n#punctuation count(Special Characters)\ndf_all[\"count_punctuations\"] = df_all[\"review\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n#upper case words count(Number of words that are all uppercase)\ndf_all[\"count_words_upper\"] = df_all[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n#title case words count(Number of words with the first letter capitalized)\ndf_all[\"count_words_title\"] = df_all[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n#Number of stopwords(number of stopwords)\ndf_all[\"count_stopwords\"] = df_all[\"review\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stops]))\n\n#Average length of the words(average word length)\ndf_all[\"mean_word_len\"] = df_all[\"review_clean\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","3ebecb5d":"df_all['season'] = df_all[\"month\"].apply(lambda x: 1 if ((x>2) & (x<6)) else(2 if (x>5) & (x<9) else (3 if (x>8) & (x<12) else 4)))","adda6aee":"df_train = df_all[:len_train]\ndf_test = df_all[len_train:]","71a80af0":"from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\nfrom sklearn.model_selection import KFold\nfrom lightgbm import LGBMClassifier\n\n#folds = KFold(n_splits=5, shuffle=True, random_state=546789)\ntarget = df_train['opinion']\nfeats = ['usefulCount','day','year','month','Predict_Opinion','Predict_Opinion2', 'count_sent','count_word', 'count_unique_word', 'count_letters', 'count_punctuations','count_words_upper', 'count_words_title', 'count_stopwords', 'mean_word_len', 'season']\n\nsub_preds = np.zeros(df_test.shape[0])\n\ntrn_x, val_x, trn_y, val_y = train_test_split(df_train[feats], target, test_size=0.2, random_state=42) \nfeature_importance_df = pd.DataFrame() \n    \nclf = LGBMClassifier(\n        n_estimators=10000,\n        learning_rate=0.10,\n        num_leaves=30,\n        subsample=.9,\n        max_depth=7,\n        reg_alpha=.1,\n        reg_lambda=.1,\n        min_split_gain=.01,\n        min_child_weight=2,\n        silent=-1,\n        verbose=-1,\n        )\n        \nclf.fit(trn_x, trn_y, \n        eval_set= [(trn_x, trn_y), (val_x, val_y)], \n        verbose=100, early_stopping_rounds=100  #30\n    )\n\nsub_preds = clf.predict(df_test[feats])\n        \nfold_importance_df = pd.DataFrame()\nfold_importance_df[\"feature\"] = feats\nfold_importance_df[\"importance\"] = clf.feature_importances_\nfeature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)","c2c19277":"confusion_matrix(y_pred=sub_preds, y_true=solution)","85beaa78":"from sklearn.metrics import classification_report\nprint(classification_report(y_pred=sub_preds, y_true=solution, labels=[0, 1]))","f9095b2a":"## 2.1.2 Part-Of-Speech(POS) Tagging\nThe package 'Spacy' contains a module named en_core_web_sm which contains a trained pipeline as well as wordnet that reads the words previous to a particular word and determines the Part of Speech it belongs to.\n![image.png](attachment:99127936-c771-4c9f-bb89-2385ab32c027.png)","3c51998c":"## 2.2 Condition Preprocessing","c48aef5b":"* ### We visualise the Ngrams, and eliminate words that are so commonly used that they carry very little useful information which are STOPWORDS\n* ### Ngrams are a set of co-occurring words within a given window\n* ### Create Bi-grams, and visualise the window data","93864395":"#### Creating stop word set for data cleaning, these words are key parts of emotional analysis, so we will remove them from stopwords.","ea536fd6":"#### Delete all the `<\/span>` tag from condition column. ","29ebcb1e":"# 3. Model Creation","2df2d95e":"# 0. Import Data and Analyse ","852b093c":"## 2.1.1 Missing Values Removal","6abca09a":"![img.png](attachment:98184f94-dc32-4a00-9933-64f76e994933.png)","01e4ec69":"### To improve the low accuracy, we will use machine learning. First of all, this is the opinion analysis model using only usefulCount.","b47de3f3":"## LightGBM model","54100671":"# 2. Data Preprocessing","f357b51c":"## 3.1. Deep Learning Model Using N-gram","110e1138":"#### Create Countvectoriser to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text","f1193413":"## 2.3 Review Preprocessing","a3650568":"## Explanations for variables in the dataset.\n\n#### * drugName (categorical): name of drug\n#### * condition (categorical): name of condition\n#### * review (text): patient review\n#### * rating (numerical): 10 star patient rating\n#### * date (date): date of review entry\n#### * usefulCount (numerical): number of users who found review useful\n\n#### The structure of the data is that a patient with a unique ID purchases a drug that meets his condition and writes a review and rating for the drug he\/she purchased on the date. Afterwards, if the others read that review and find it helpful, they will click usefulCount, which will add 1 for the variable.","177863b3":"### Stemming using SnowballStemmer","483bec65":"# 1. Data Visualisation","0be3ed36":"#### Creating new index, after deleting the span list"}}