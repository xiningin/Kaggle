{"cell_type":{"d6576777":"code","4b0f676c":"code","9ad7df78":"code","f5c70330":"code","b1febe86":"code","bb257752":"code","7236b85d":"code","f95f461b":"code","84c4c10c":"code","ac0019ba":"code","7e27d879":"code","037f1032":"code","938955b0":"code","8f498cdd":"code","cbd17807":"code","f74d31f4":"code","273ac13b":"code","48fac5d8":"code","6d34e41d":"code","4bbef1e2":"code","ce9560a6":"code","3f4bb0f2":"code","bd141fc1":"code","608edd99":"code","5636b1ba":"code","cb3cc71a":"code","2a51d8c4":"code","266f1813":"code","a1637da7":"code","891a1d3c":"code","f0d5ef37":"code","71685e23":"code","7de6126e":"code","8a1cba61":"code","052da797":"code","1bdd0149":"code","c68186eb":"code","2c8338e1":"code","00394b79":"code","b0103bed":"code","28396929":"code","fcfd5018":"code","cb59d3a6":"code","fa8db12d":"code","a4d654cd":"code","0277961b":"code","5a830d53":"code","362c3797":"code","5cae0241":"code","976401f8":"code","33c475b4":"code","3574e279":"code","0918f191":"code","f79b1c06":"code","4f5c5666":"code","c8647398":"markdown","ae9d1e3d":"markdown","ffa4eafd":"markdown","7fd53824":"markdown","3c31a737":"markdown","8b7689eb":"markdown","e9becc83":"markdown","fe336e68":"markdown","460c4e8a":"markdown"},"source":{"d6576777":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4b0f676c":"df = pd.read_csv('\/kaggle\/input\/thoracicsurgery\/Thoracic-Surgery.csv', index_col = 'id')","9ad7df78":"df.head()","f5c70330":"df.info()","b1febe86":"df[['PRE7', 'PRE8', 'PRE9', 'PRE10', 'PRE11', 'PRE17', 'PRE19', 'PRE25', 'PRE30', 'PRE32', 'Risk1Yr']] = (df[['PRE7', 'PRE8', 'PRE9', 'PRE10', 'PRE11', 'PRE17', 'PRE19', 'PRE25', 'PRE30', 'PRE32', 'Risk1Yr']] == 'T').astype(int)","bb257752":"df.head()","7236b85d":"df.shape","f95f461b":"df['DGN'] = df['DGN'].str[-1:].astype(int)\ndf['PRE6'] = df['PRE6'].str[-1:].astype(int)\ndf['PRE14'] = df['PRE14'].str[-1:].astype(int)","84c4c10c":"df.columns","ac0019ba":"col = ['Daignosis','Forced_Capacity','Forced_Expiration','Zubrod_scale','Pain',' Haemoptysis','Dyspnoea',\n       'Cough','Weakness','Size_of_tumor','diabetes','MI_6months','PAD','Smoker','Asthmatic','Age','Risk_1y']\ndf.columns = col","7e27d879":"df.head()","037f1032":"# Train test split \nfrom sklearn.model_selection import train_test_split \nX = df.drop('Risk_1y', axis=1) \ny = df.Risk_1y \nX_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 3) ","938955b0":"X_train.head() ","8f498cdd":"y_test[:10] ","cbd17807":"from sklearn.linear_model import LogisticRegression \nmodel_lr = LogisticRegression() \nmodel_lr.fit(X_train, y_train) \nmodel_lr.score(x_test, y_test) ","f74d31f4":"y_pred = model_lr.predict(x_test)  ","273ac13b":"from sklearn.metrics import accuracy_score \nprint(accuracy_score(y_test, y_pred)) ","48fac5d8":"# from sklearn.metrics import precision_score\n# print(precision_score(y_true, y_pred))\n\n# The precision is the ratio tp \/ (tp + fp)\n# y_true = [0, 1, 2, 0, 1, 2]\n# y_pred = [0, 2, 1, 0, 0, 1]","6d34e41d":"from sklearn.metrics import confusion_matrix \nc_matrix = confusion_matrix(y_test, y_pred) \nc_matrix","4bbef1e2":"import seaborn as sns\nsns.heatmap(c_matrix, annot=True, cmap = 'Blues')\nplt.show()","ce9560a6":"from sklearn.neighbors import KNeighborsClassifier \nmodel_knc = KNeighborsClassifier(n_neighbors=5)  \nmodel_knc.fit(X_train, y_train)  \ny_knn_pred = model_knc.predict(x_test)","3f4bb0f2":"accuracy_score(y_test, y_knn_pred) ","bd141fc1":"model_knc.predict_proba(x_test) ","608edd99":"model_knc.score(x_test, y_test) ","5636b1ba":"c_matrix = confusion_matrix(y_test, y_knn_pred) \nprint(c_matrix)\n\nsns.heatmap(c_matrix, annot=True, cmap = 'Blues') \nplt.show() ","cb3cc71a":"# For KNN = 1 \nmodel_knc_1 = KNeighborsClassifier(n_neighbors = 1) \nmodel_knc_1.fit(X_train, y_train) \ny_knn_pred_1 = model_knc_1.predict(x_test) \naccuracy_score(y_test, y_knn_pred_1) ","2a51d8c4":"c_matrix = confusion_matrix(y_test, y_knn_pred_1) \nprint(c_matrix)\n\nsns.heatmap(c_matrix, annot=True, cmap = 'Blues') \nplt.show() ","266f1813":"# for KNN = 3 \nmodel_knc_3 = KNeighborsClassifier(n_neighbors = 3) \nmodel_knc_3.fit(X_train, y_train) \ny_knn_pred_3 = model_knc_3.predict(x_test) \naccuracy_score(y_test, y_knn_pred_3)  ","a1637da7":"c_matrix = confusion_matrix(y_test, y_knn_pred_3) \nprint(c_matrix)\n\nsns.heatmap(c_matrix, annot=True, cmap = 'Blues') \nplt.show() ","891a1d3c":"# for KNN = 5 \nmodel_knc_5 = KNeighborsClassifier(n_neighbors = 5) \nmodel_knc_5.fit(X_train, y_train) \ny_knn_pred_5 = model_knc_5.predict(x_test) \naccuracy_score(y_test, y_knn_pred_5)  ","f0d5ef37":"c_matrix = confusion_matrix(y_test, y_knn_pred_5) \nprint(c_matrix)\n\nsns.heatmap(c_matrix, annot=True, cmap = 'Blues') \nplt.show() ","71685e23":"# for KNN = 7 \nmodel_knc_7 = KNeighborsClassifier(n_neighbors = 7) \nmodel_knc_7.fit(X_train, y_train) \ny_knn_pred_7 = model_knc_7.predict(x_test) \naccuracy_score(y_test, y_knn_pred_7)  ","7de6126e":"c_matrix = confusion_matrix(y_test, y_knn_pred_7) \nprint(c_matrix)\n\nsns.heatmap(c_matrix, annot=True, cmap = 'Blues') \nplt.show() ","8a1cba61":"# for KNN = 9 \nmodel_knc_9 = KNeighborsClassifier(n_neighbors = 9) \nmodel_knc_9.fit(X_train, y_train) \ny_knn_pred_9 = model_knc_9.predict(x_test) \naccuracy_score(y_test, y_knn_pred_9) ","052da797":"c_matrix = confusion_matrix(y_test, y_knn_pred_9) \nprint(c_matrix)\n\nsns.heatmap(c_matrix, annot=True, cmap = 'Blues') \nplt.show() ","1bdd0149":"# Evaluation of KNN models on bases of Accuracy. \ndict_ms = {'N_Neighbors':[1,3,5,7,9], 'Accuracy':[accuracy_score(y_test, y_knn_pred_1) * 100, \n                                                  accuracy_score(y_test, y_knn_pred_3) * 100, \n                                                  accuracy_score(y_test, y_knn_pred) * 100,\n                                                  accuracy_score(y_test, y_knn_pred_7) * 100,\n                                                  accuracy_score(y_test, y_knn_pred_9) * 100]} \nmodel_selection = pd.DataFrame(dict_ms)  \nmodel_selection ","c68186eb":"x = df.drop('Risk_1y', axis=1) \nx_norm = (x-np.min(x))\/(np.max(x)-np.min(x))   \nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.3) ","2c8338e1":"# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\ngrid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\nlogreg=LogisticRegression()\nlogreg_cv=GridSearchCV(logreg,grid,cv=10)\nlogreg_cv.fit(x_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\nprint(\"accuracy :\",logreg_cv.best_score_) ","00394b79":"logreg2=LogisticRegression(C=0.001,penalty=\"l2\")\nlogreg2.fit(x_train,y_train)\nprint(\"score\",logreg2.score(x_test,y_test)) ","b0103bed":"c_matrix = confusion_matrix(y_test, y_pred) \nprint(c_matrix)\n\nsns.heatmap(c_matrix, annot=True, cmap = 'Blues') \nplt.show() ","28396929":"df.info()  ","fcfd5018":"category_col = ['Daignosis','Zubrod_scale','Pain', ' Haemoptysis', 'Dyspnoea', 'Cough','Weakness',\n               'Size_of_tumor', 'diabetes', 'MI_6months', 'PAD', 'Smoker', 'Asthmatic', 'Risk_1y'] \nfor col in category_col:\n     df[col]=df[col].astype('category')","cb59d3a6":"df.info() ","fa8db12d":"X_dtrees = df.drop('Risk_1y', axis=1) \ny_dtrees = df['Risk_1y'] ","a4d654cd":"from sklearn.model_selection import train_test_split  \nX_train, x_test, y_train, y_test = train_test_split(X, y.values.ravel(), test_size = 0.3, \n                                                    random_state = 3) \nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nclf = DecisionTreeClassifier()\n\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n\n#Predict the response for test dataset\ny_pred = clf.predict(x_test)  ","0277961b":"from sklearn.metrics import accuracy_score \naccuracy_score(y_test, y_pred) ","5a830d53":"c_matrix = confusion_matrix(y_test, y_pred) \nprint(c_matrix)\n\nsns.heatmap(c_matrix, annot=True, cmap = 'Blues') \nplt.show() ","362c3797":"#Import Random Forest Model\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Create a Gaussian Classifier\nclf=RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\n\ny_pred=clf.predict(x_test)  \n\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred)) ","5cae0241":"c_matrix = confusion_matrix(y_test, y_pred) \nprint(c_matrix)\n\nsns.heatmap(c_matrix, annot=True, cmap = 'Blues') \nplt.show() ","976401f8":"from sklearn.feature_selection import RFE\nlogReg = LogisticRegression()\nrfe_selector = RFE(logReg, 10) \nrfe_selector.fit(X_train, y_train)","33c475b4":"rfe_selector.support_","3574e279":"rfe_selector.ranking_","0918f191":"X_train.columns","f79b1c06":"top_features = ['Daignosis', 'Forced_Expiration', 'Zubrod_scale','Pain', 'Dyspnoea', 'Cough', 'Size_of_tumor', \n                'diabetes', 'MI_6months', 'Smoker']\nX_selected = X_train[top_features] \nx_test_selected = x_test[top_features]","4f5c5666":"logreg3 = LogisticRegression()\nlogreg3.fit(X_selected,y_train)\nprint(\"score\",logreg3.score(x_test_selected, y_test)) ","c8647398":"#### KNN","ae9d1e3d":"### Converting Object dat types variables to integer data type","ffa4eafd":"__Logistic Regression with GridSearchCV__","7fd53824":"#### Random Forests","3c31a737":"### Model Building","8b7689eb":"# Reading the dataset","e9becc83":"#### Decision Trees","fe336e68":"#### Feature Selection","460c4e8a":"#### Logistic Regression"}}