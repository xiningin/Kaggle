{"cell_type":{"2c6c3772":"code","860a721d":"code","5e19b535":"code","74104c2c":"code","7c247ecc":"code","3ac5685d":"code","647e332d":"code","2592d903":"code","f3a90e31":"code","a100c61b":"code","b5d9967c":"code","5a209d25":"code","2be2f14a":"code","9bd95223":"code","af19c807":"code","d0f71a29":"code","e87bfe57":"code","a691e973":"code","5e1a3f2a":"code","b20d9506":"code","7094874a":"code","05a79b57":"code","40e52f3d":"code","d420c71b":"code","865fe20c":"code","08b14449":"code","b23e88ef":"code","509c4c41":"code","50ddfd7e":"code","7b598c5b":"code","fc872c39":"code","2b950ca6":"code","98dbb877":"code","1db0fd4c":"code","3f3de2b1":"code","1153664f":"code","dd3fba68":"code","f876b0bd":"code","c973130c":"code","5186984e":"code","ccd708df":"code","da3fafd0":"code","38bd543f":"code","b5317ad0":"code","ce3b0289":"code","d245adcf":"code","964ec1ae":"code","3807a62b":"code","bbd8916c":"code","dd6e8886":"code","ebdbe05a":"code","3adb56e8":"code","5bef55ea":"code","829feda1":"code","639d3873":"code","004b4932":"code","ef8bd857":"code","c25315ed":"code","092d4e5f":"code","de3279b3":"code","85019ad6":"code","fe1014ff":"code","c19c9f65":"code","869f0649":"code","59739e1f":"code","37e974ed":"markdown","81e93ad2":"markdown","5f0e7b17":"markdown","fdbc7ace":"markdown","23d6c1df":"markdown","3314dffa":"markdown","79e7fe71":"markdown","ac7c38a1":"markdown","80db7f7f":"markdown","0127ad4f":"markdown","1b6c13ce":"markdown","83b0ff73":"markdown","60b3f3e5":"markdown","92042c71":"markdown","3b1d70fa":"markdown","61322c88":"markdown","d19beb72":"markdown","43afdc6d":"markdown","a8bcaedc":"markdown","34a6bf3e":"markdown","b1545a32":"markdown","8ad331a0":"markdown","e47f2651":"markdown","973acd88":"markdown","2ef2e947":"markdown","7515876f":"markdown","f843cf8a":"markdown","fdde0aac":"markdown","c5ab218d":"markdown","c08c25fb":"markdown","1101a13b":"markdown","2103f534":"markdown","ea287c8a":"markdown"},"source":{"2c6c3772":"!pip install googletrans\n!pip install pytrends","860a721d":"#import require modules\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\n%matplotlib inline\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport gc\nimport pickle\nfrom sys import getsizeof\nfrom googletrans import Translator\nfrom pytrends.request import TrendReq","5e19b535":"#variables setting\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\nori_data_path = '\/kaggle\/input\/competitive-data-science-predict-future-sales\/'\nmodel_path = '\/'\nprocessed_data_path = '\/'\ntables_path = '\/'\nitems_IP_path = '..\/input\/predict-future-sales-ip-groups\/items_IPs_v1.csv'\nitems_IP_trend_path = '..\/input\/predict-future-sales-ip-groups\/items_IP_trend.csv'","74104c2c":"#import datasets\ndf = pd.read_csv(ori_data_path+'sales_train.csv')\ndf_test = pd.read_csv(ori_data_path+'test.csv')\nsample_submission = pd.read_csv(ori_data_path+'sample_submission.csv')\nitem_categories = pd.read_csv(ori_data_path+'item_categories.csv')\nitems = pd.read_csv(ori_data_path+'items.csv')\nshops = pd.read_csv(ori_data_path+'shops.csv')","7c247ecc":"df[['date_block_num','shop_id']] = df[['date_block_num','shop_id']].astype('int8')\ndf[['item_cnt_day','item_id']] = df[['item_cnt_day','item_id']].astype('int16')\ndf['item_price'] = df['item_price'].astype('float32')","3ac5685d":"shop_test = df_test['shop_id'].nunique()\nitem_test = df_test['item_id'].nunique()\ntotal_product_test = df_test[['shop_id','item_id']].drop_duplicates()\nshop_train = df['shop_id'].nunique()\nitem_train = df['item_id'].nunique()\ntotal_product_train = df[['shop_id','item_id']].drop_duplicates()\n\nprint('We need to predict products from {} shops and {} kinds of products (total shop-item {} pairs)'.format(shop_test,item_test,total_product_test.shape[0]))\nprint('given that historical daily sale number from {} shops and {} kinds of products (total shop-item {} pairs)'.format(shop_train,item_train,total_product_train.shape[0]))","647e332d":"#100,000 pairs have no sale recored\ncommon_pairs = total_product_test.merge(total_product_train,how='inner',on=['shop_id','item_id']).shape[0]\ntest_only_pairs = total_product_test.shape[0] - common_pairs\nprint(f'{common_pairs} pairs of products are shared by historical and to be predicted.')\nprint(f'{test_only_pairs} pairs are new pairs, so there is no corresponding historical data for prediciton.')","2592d903":"#363 items are new items that never seen before\ntotal_items_test = total_product_test['item_id'].drop_duplicates()\ntotal_items_train = total_product_train['item_id'].drop_duplicates()\ntotal_items_join = pd.merge(total_items_test,total_items_train,how='outer',on=['item_id'],indicator=True)\ntotal_items_join = total_items_join.replace({'both':'hisory and target','left_only':'target only','right_only':'hisory only'})\ntotal_items_join['_merge'].value_counts()","f3a90e31":"del shop_test\ndel item_test\ndel total_product_test\ndel shop_train\ndel item_train\ndel total_product_train\ndel common_pairs\ndel test_only_pairs\ndel total_items_test\ndel total_items_train\ndel total_items_join\ngc.collect()","a100c61b":"shop_item_pair_num_month = df\nshop_item_pair_num_month['month'] = shop_item_pair_num_month['date'].apply(lambda d: d.split('.')[2] + '-' + d.split('.')[1])\nshop_item_pair_num_month = df.groupby(['month','date_block_num'])['item_cnt_day'].sum()\nshop_item_pair_num_month.name = 'monthly_sale_cnt'\nshop_item_pair_num_month = shop_item_pair_num_month.reset_index()\nax = shop_item_pair_num_month.plot(x='month',y='monthly_sale_cnt',x_compat=True,figsize=(15,4),marker='o')\n\nplt.xticks(shop_item_pair_num_month.date_block_num, shop_item_pair_num_month.month, rotation='vertical')\n\nplt.xlabel('Year-Month',fontsize=16)\nplt.ylabel('Monthly Sale Count (ea)',fontsize=16)\nplt.title('Monthly Sale Count Trend',fontsize=20)","b5d9967c":"all_shops = np.sort(df['shop_id'].unique())\nall_items = np.sort(df['item_id'].unique())\nall_pairs = pd.DataFrame(index=all_shops,columns=all_items)\nall_pairs = all_pairs.fillna(-1).stack()\nall_pairs.index.names = ['shop_id','item_id']\nall_pairs = all_pairs.reset_index()\nall_pairs = all_pairs.drop(0,axis=1)\n\nall_existed_pair_month = df.pivot_table(values='item_cnt_day', columns='date_block_num', index=['shop_id','item_id'],aggfunc='sum')\n\nall_pairs = pd.merge(all_pairs,all_existed_pair_month,on=['shop_id','item_id'],how='left')\n\nplt.figure(figsize=(16,12))\n\nsns.heatmap(all_pairs.groupby('shop_id').sum().drop('item_id',axis=1).replace(0,np.nan),vmax=5000,cmap='plasma',linewidths=1,linecolor='black')\nplt.xticks(shop_item_pair_num_month.date_block_num, shop_item_pair_num_month.month, rotation='vertical')\n\nplt.xlabel('Year-Month',fontsize=16)\nplt.ylabel('Shops',fontsize=16)\nplt.title('Shops Monthly Sale',fontsize=20)","5a209d25":"item_trend = df.pivot_table(index='date',columns='item_id',values='item_cnt_day',aggfunc='mean').fillna(0)\n#original date format is dd.mm.yyyy\nitem_trend.index = item_trend.index.map(lambda d:pd.to_datetime(d.split('.')[2]+'.'+d.split('.')[1]+'.'+d.split('.')[0]))\n\n#set multiple plots\nfig = plt.figure(figsize=(15,6))\nax1 = fig.add_subplot(311)\nax1.set(ylabel='Sale Count (ea)')\nax1.set_title('best seller shopping bags(20949)')\n\nax2 = fig.add_subplot(312)\nax2.set(ylabel='Sale Count (ea)')\nax2.set_title('Battery(22088)')\n\nax3 = fig.add_subplot(313)\nax3.set(ylabel='Sale Count (ea)')\nax3.set_title('PlayStation Network wallet (5823)')\nplt.tight_layout()\n\n\n#best seller shopping bags\nitem_1 = item_trend[20949].sort_index()\nitem_1 = pd.DataFrame({'day':item_1,\n                         '7days_avg':item_1.rolling(window=7).mean(),\n                         '30days_avg':item_1.rolling(window=30).mean()})\n#Battery\nitem_2 = item_trend[22088].sort_index()\nitem_2 = pd.DataFrame({'day':item_2,\n                          '7days_avg':item_2.rolling(window=7).mean(),\n                          '30days_avg':item_2.rolling(window=30).mean()})\n#PlayStation Network wallet\nitem_3 = item_trend[5823].sort_index()\nitem_3 = pd.DataFrame({'day':item_3,\n                            '7days_avg':item_3.rolling(window=7).mean(),\n                            '30days_avg':item_3.rolling(window=30).mean()})\n\n\nsns.lineplot(data=item_1, palette='dark', linewidth=3, ax=ax1)\nsns.lineplot(data=item_2, palette='dark', linewidth=3, ax=ax2)\nsns.lineplot(data=item_3, palette='dark', linewidth=3, ax=ax3)\n","2be2f14a":"#set multiple plots\nfig = plt.figure(figsize=(15,6))\nax1 = fig.add_subplot(311)\nax1.set(ylabel='Sale Count (ea)')\nax1.set_title('Movie \"Kingsman: The Secret Service\" DVD (4084)')\n\nax2 = fig.add_subplot(312)\nax2.set(ylabel='Sale Count (ea)')\nax2.set_title('Grand Theft Auto V PS3 (3732)')\n\nax3 = fig.add_subplot(313)\nax3.set(ylabel='Sale Count (ea)')\nax3.set_title('annual exhibition Ticket \"Igromir 2014\"(9241)')\n\nplt.tight_layout()\n\n\n#Movie 'Kingsman' DVD , release day Jun 2015\nitem_1 = item_trend[4084].sort_index()\nitem_1 = pd.DataFrame({'day':item_1,\n                         '7days_avg':item_1.rolling(window=7).mean(),\n                         '30days_avg':item_1.rolling(window=30).mean()})\n#Grand Theft Auto V PS3 (highest volumne at beginning), Sep 2013  Release on PS3\nitem_2 = item_trend[3732].sort_index()\nitem_2 = pd.DataFrame({'day':item_2,\n                          '7days_avg':item_2.rolling(window=7).mean(),\n                          '30days_avg':item_2.rolling(window=30).mean()})\n#annual exhibition Ticket \"Igromir 2014\"  (October 3, 2014), cut off after event started\nitem_3 = item_trend[9241].sort_index()\nitem_3 = pd.DataFrame({'day':item_3,\n                            '7days_avg':item_3.rolling(window=7).mean(),\n                            '30days_avg':item_3.rolling(window=30).mean()})\n\n\nsns.lineplot(data=item_1, palette='dark', linewidth=3, ax=ax1)\nsns.lineplot(data=item_2, palette='dark', linewidth=3, ax=ax2)\nsns.lineplot(data=item_3, palette='dark', linewidth=3, ax=ax3)\n","9bd95223":"del shop_item_pair_num_month\ndel ax\ndel all_pairs\ndel all_existed_pair_month\ndel all_items\ndel all_shops\ndel item_trend\ndel item_1\ndel item_2\ndel item_3\ndel fig\ndel ax1\ndel ax2\ndel ax3\ngc.collect()","af19c807":"%%time\nfor m in df['date_block_num'].unique():\n    shops_list_m = np.sort(df[df['date_block_num']==m]['shop_id'].unique())\n    items_list_m = np.sort(df[df['date_block_num']==m]['item_id'].unique())\n    all_pairs_m = pd.DataFrame(index=shops_list_m,columns=items_list_m).fillna(m).stack()\n    all_pairs_m.index.names = ['shop_id','item_id']\n    all_pairs_m.name = 'date_block_num'\n    \n    assert len(shops_list_m) * len(items_list_m) == len(all_pairs_m)\n    \n    if m == 0:\n        dataset = all_pairs_m\n    else:\n        dataset = pd.concat([dataset,all_pairs_m],axis=0)\n        \n    del shops_list_m\n    del items_list_m\n    del all_pairs_m\n    gc.collect()\n    \ndataset = dataset.reset_index()\ndataset = dataset.sort_values(by=['shop_id','item_id','date_block_num']).reset_index(drop=True)","d0f71a29":"cnt_month = df.groupby(['shop_id','item_id','date_block_num']).sum()['item_cnt_day']\ncnt_month.name = 'item_cnt_month'\n\ndataset = dataset.merge(cnt_month,on=['shop_id','item_id','date_block_num'],how='left')\ndataset['item_cnt_month'] = dataset['item_cnt_month'].fillna(0).clip(0,20)\n\ndel cnt_month\ngc.collect()","e87bfe57":"price_pivot_month = df.pivot_table(values='item_price', columns='date_block_num', index=['shop_id','item_id'])\n\nprice_pivot_month = price_pivot_month.fillna(method='ffill',axis=1)\nprice_pivot_month = price_pivot_month.fillna(0)\n\n#transform from wide to long format\nprice_pivot_month = price_pivot_month.stack()\nprice_pivot_month.name = 'avg_price'\nprice_pivot_month = price_pivot_month.reset_index()\n\n#clip price to 0~50000 to prevent outliers\nprice_pivot_month['avg_price'] = price_pivot_month['avg_price'].clip(0,50000)\n\n#merge back to main table\ndataset = dataset.merge(price_pivot_month,on=['shop_id','item_id','date_block_num'],how='left').fillna(0)\ndataset['revenue']=dataset['avg_price']*dataset['item_cnt_month']\n\ndel price_pivot_month\ngc.collect()","a691e973":"df_test['date_block_num'] = 34\ndataset = pd.concat([dataset,df_test.drop('ID',axis=1)],axis=0)\ndataset = dataset.sort_values(by=['date_block_num','shop_id','item_id']).reset_index(drop=True)\n\nassert dataset.isnull().sum().max() == len(df_test)\n\ndataset = dataset.fillna(0) #only for test data","5e1a3f2a":"#compress DataFrame for lower memory usage\nfloat32_cols = ['revenue','avg_price']\nint16_cols =['item_id']\nint8_cols = ['date_block_num','shop_id','item_cnt_month']\n\ndataset[int8_cols] = dataset[int8_cols].astype('int8')\ndataset[int16_cols] = dataset[int16_cols].astype('int16')\ndataset[float32_cols] = dataset[float32_cols].astype('float32')","b20d9506":"def add_lag_v3(dataset,lookup_set,lag_months,lag_features_list,on_key,fillna_value,dtypes):\n    \n    \"\"\"\n    Function to create one of multiple lagged features\n    \n    Parameters: \n    ------------\n    dataset : (pd.DataFrame) target dataset \n    \n    look_set :(pd.DataFrame)  source of features. Need to be groupped by desired key such as shop_id, item_category_id, or \n        other customized group. create_mean_encoding_table() is created for this. 'None' means using the target dataset. \n    \n    lag_months : (list) list of lagged features, ex. [1,2,3] means adding 3 columns with lagged 1,2 and 3\n    \n    lag_features_list : (list) list of targeted features' name, should be existing in the lookup_set\n    \n    on_key : (list) list of keys used to merge the dataset and lookup_set\n    \n    fillna_value : (list) how to fill missing for each of lagged features in the lag_features_list.\n    \n    dtypes : (list) dtypes of generated lagged featrures. Note 'fillna_value' and 'dtypese' are addtional properties of \n        'lag_features_list', so it is necessery to be in the same order and length. \n    ------------\n    \"\"\"\n    if lookup_set is None:\n        lookup_set = dataset[on_key + lag_features_list].copy()\n    else:    \n        dataset = pd.merge(dataset,lookup_set,how='left',on=on_key)\n    \n    #breakpoint()\n    \n    \n    # create lagged features for each month\n    for lag in lag_months:\n        dataset_alignment = dataset[on_key + lag_features_list].copy() #small subset for alignment to reduce memory usage\n        dataset_lag = lookup_set[on_key + lag_features_list]\n        dataset_lag['date_block_num'] = dataset_lag['date_block_num'] + lag\n        dataset_alignment = pd.merge(dataset_alignment,dataset_lag,how='left',on=on_key,suffixes=['','_lag_'+str(lag)+'m'])\n        dataset_alignment = dataset_alignment.drop(on_key + lag_features_list,axis=1)\n        #breakpoint()\n        dataset = pd.concat([dataset,dataset_alignment],axis=1,join='outer')\n        del dataset_lag\n        del dataset_alignment\n        gc.collect()\n    \n    # dtypes setting\n    for i,feature in enumerate(lag_features_list):\n        new_columns = dataset.columns.str.startswith(feature + '_lag_')\n        dataset.loc[:,new_columns] = dataset.loc[:,new_columns].fillna(fillna_value[i]).astype(dtypes[i])\n        \n    return dataset","7094874a":"def add_first_last_sale(dataset,source_set,keys,prefix):\n    \"\"\"\n    Search the month of first sale and last sale, only read historical data, \n    ex. for month 8, only month 0~7 will be used to judge.\n    Then, it creates both relative and absolut values. \n    ex. at month 8, the absolute last month is 7, and then the relative will be -1\n    \n    Parameters: \n    ------------\n    dataset : (pd.DataFrame) target dataset \n    \n    source_set :(pd.DataFrame) source dataframe to judge first\/last\n    \n    keys : (list) level of group. ex. ['shop_id','item_id'] means judging by shop-item pair \n    \n    prefix : (str) prefix of created columns\n    ------------\n    \"\"\"    \n    for m in range(1,35): #1~34m\n    \n        #only use historical data to judge (0 to m-1)\n        tmp = source_set[source_set['date_block_num']<m].drop_duplicates(keys+['date_block_num']).groupby(keys).agg({'date_block_num':[np.min,np.max]})\n        tmp.columns = [prefix + '_first_sold', prefix + '_last_sold']\n        tmp = tmp.reset_index()\n        tmp['date_block_num'] = m \n        \n        if m == 1:  \n            first_last_sold = tmp\n        else:\n            first_last_sold = pd.concat([first_last_sold,tmp],axis=0)\n            \n        del tmp\n        gc.collect()\n    #breakpoint()\n        \n    dataset = dataset.merge(first_last_sold,on=keys+['date_block_num'],how='left')\n        \n    dataset[prefix + '_first_sold_offset'] = dataset[prefix + '_first_sold'] - dataset['date_block_num']\n    dataset[prefix + '_last_sold_offset'] = dataset[prefix + '_last_sold'] - dataset['date_block_num']\n    \n    columns = [prefix + '_first_sold', prefix + '_last_sold',prefix + '_first_sold_offset', prefix + '_last_sold_offset']\n    dataset[columns] = dataset[columns].fillna(0)\n    dataset[columns] = dataset[columns].astype('int8')\n    \n    return dataset","05a79b57":"def price_diff(row,columnA,columnB):\n        #compare columnA price with columnB ((A-B)\/B) in percentage\n        if row[columnB] == 0:\n            increase_percent = 0\n        else:    \n            increase_percent = round((row[columnA]-row[columnB])\/row[columnB]*100)\n        return increase_percent","40e52f3d":"def create_mean_encoding_table(dataset:pd.DataFrame, encode_item:list, prefix:str):\n    #breakpoint()\n    encoding_table = dataset[encode_item +['date_block_num','item_cnt_month','revenue','avg_price']] #remove 0 to reduce bias\n    encoding_table.loc[:,['item_cnt_month','revenue','avg_price']] = encoding_table.loc[:,['item_cnt_month','revenue','avg_price']].replace(0,np.nan)\n    encoding_table = encoding_table.groupby(encode_item + ['date_block_num']).agg({'item_cnt_month':[np.mean],'revenue':[np.mean],'avg_price':[np.mean]})\n    encoding_table = encoding_table.fillna(0)\n    encoding_table.columns = [prefix+'_avg_cnt_month', prefix+'_avg_revenue_month',prefix+'_avg_price_month']\n    encoding_table = encoding_table.reset_index()\n    \n    encoding_table[['date_block_num',prefix+'_avg_cnt_month']] = encoding_table[['date_block_num',prefix+'_avg_cnt_month']].astype('float16')\n    encoding_table[encode_item] = encoding_table[encode_item].astype('int16')\n    encoding_table[[prefix+'_avg_revenue_month',prefix+'_avg_price_month']] = encoding_table[[prefix+'_avg_revenue_month',prefix+'_avg_price_month']].astype('float32')\n    \n    return encoding_table","d420c71b":"def first_sale_cnt(dataset,source_set,keys,prefix,forward_range=6):\n    \"\"\"\n    function to calculate avg. first sale count\n    \n    Parameters: \n    ------------\n    dataset : (pd.DataFrame) target dataset \n    \n    source_set :(pd.DataFrame) source dataframe to judge first\/last and avg. sale count\n    \n    keys : (list) level of group. ex. ['shop_id','item_id'] means judging by shop-item pair \n    \n    prefix : (str) prefix of created columns\n    \n    forward_range : (int) include how many months data to calculate. It may have large variage by judging only one month, \n        because their would be only few items are been sold for the fisrt time in the last month\n    ------------\n    \"\"\"    \n    #filter unnecessary data (not first sale, extra columns)\n    source_set = source_set[keys+['date_block_num','item_first_sold_offset','item_cnt_month']]\n    source_set = source_set[source_set['item_first_sold_offset']==0]\n    source_set = source_set.drop('item_first_sold_offset',axis=1)\n    source_set = source_set[source_set['item_cnt_month']>0] #extra zeros are padded to simulate test set\n    \n    for m in range(1,35): #1~34m      \n        \n        #take previous 6 months data to prevent large variation\n        tmp = source_set[source_set['date_block_num'].between(m-1-6,m-1)].groupby(keys).agg({'item_cnt_month':[np.mean]})\n        tmp.columns = [prefix + '_avg_first_sold_cnt_month']\n        tmp = tmp.reset_index()\n        tmp['date_block_num'] = m #(m-1-6 to m-1)data is for month m\n        \n        if m == 1:  \n            avg_first_sold_cnt = tmp\n        else:\n            avg_first_sold_cnt = pd.concat([avg_first_sold_cnt,tmp],axis=0)\n            \n        del tmp\n        #breakpoint()\n        \n    dataset = dataset.merge(avg_first_sold_cnt,on=keys+['date_block_num'],how='left')\n        \n    columns = [prefix + '_avg_first_sold_cnt_month']\n    dataset[columns] = dataset[columns].fillna(0)\n    dataset[columns] = dataset[columns].astype('float16')\n    \n    return dataset","865fe20c":"def get_google_trends(IPs, file_path, reference):\n    \"\"\"\n    function to access Google Trends to fetch search popularity trend for each keywords\n    \n    Parameters: \n    ------------\n    IPs : (list) list of keywords need to be searched\n    \n    file_path :(str) path to save\/load Google Trend data since API has a usage limit\n    \n    reference : (str) a reference keyword to make Google Trend data can be compared across different keywords \n    ------------\n    \"\"\"    \n    \n    pytrend = TrendReq(hl='ru', tz=360)\n    \n    try:\n        IP_trend = pd.read_csv(file_path)\n        existed_columns = IP_trend.columns\n    except FileNotFoundError:\n        IP_trend = None\n        existed_columns = []\n    #breakpoint()  \n    for IP in IPs:\n        if IP in existed_columns:\n            continue\n        \n        #every trend is normalized to itself, so a general reference will make it comparable with others\n        if IP == reference:\n            keywords = [reference]\n        else:\n            keywords = [IP,reference]\n        pytrend.build_payload(kw_list=keywords,cat=0,timeframe='2013-01-01 2015-10-31',geo='RU',gprop='')\n\n        trend_df = pytrend.interest_over_time()\n        trend_df = trend_df.drop(reference,axis=1)\n        if len(trend_df) == 0:\n            #no key word trends found\n            if IP_trend is not None:\n                IP_trend[IP] = 0\n        else:    \n            trend_df = trend_df.reset_index()\n\n            trend_df['year'] = trend_df['date'].dt.year\n            trend_df['month'] = trend_df['date'].dt.month\n\n            trend_df = trend_df.drop(['date','isPartial'],axis=1)\n            trend_df = trend_df.groupby(['year','month']).max().reset_index()\n\n            if IP_trend is None:\n                IP_trend = trend_df\n            else:       \n                IP_trend = IP_trend.merge(trend_df,on=['year','month'],how='left')\n\n# cannot override the file system on kaggle\n#     IP_trend.to_csv(file_path, index=False)\n    return IP_trend","08b14449":"%%time\nlag_months = [1,2,3,6,12]\nlag_features_list = ['item_cnt_month']\non_key = ['shop_id','item_id','date_block_num']\nfillna = [0]\ndtypes = ['int8']\n\ndataset = add_lag_v3(dataset,None,lag_months,lag_features_list,on_key,fillna,dtypes)","b23e88ef":"%%time\nlag_months = [1]\nlag_features_list = ['avg_price']\non_key = ['shop_id','item_id','date_block_num']\nfillna = [0]\ndtypes = ['float32']\n\ndataset = add_lag_v3(dataset,None,lag_months,lag_features_list,on_key,fillna,dtypes)","509c4c41":"#add lag first sale month and last sale mont\ndataset = add_first_last_sale(dataset,df,['shop_id','item_id'],'pair')\ndataset = dataset.drop(['pair_first_sold','pair_last_sold'],axis=1)","50ddfd7e":"%%time\nitem_month_sale = create_mean_encoding_table(dataset, ['item_id'], 'item')\n\nlag_months = [1]\nlag_features_list = ['item_avg_cnt_month', 'item_avg_revenue_month', 'item_avg_price_month']\non_key = ['item_id','date_block_num']\nfillna = [0,0,0]\ndtypes = ['float16','float32','float32']\n\ndataset = add_lag_v3(dataset,item_month_sale,lag_months,lag_features_list,on_key,fillna,dtypes)\n\ndataset = dataset.drop(['item_avg_cnt_month','item_avg_revenue_month', 'item_avg_price_month'],axis=1)\ndel item_month_sale\ngc.collect()","7b598c5b":"%%time\n#add lag first sale month and last sale mont\ndataset = add_first_last_sale(dataset,df,['item_id'],'item')\ndataset = dataset.drop(['item_first_sold','item_last_sold'],axis=1)","fc872c39":"#merge item_category id into main dataset\ndataset = pd.merge(dataset,items[['item_id','item_category_id']],how='left',on=['item_id'])\ndataset['item_category_id'] = dataset['item_category_id'].astype('int16')","2b950ca6":"#google API has daily usage limit, try to read file first\ntry:\n    item_categories_plus = pd.read_csv(tables_path + 'item_categories_plus.csv')\nexcept FileNotFoundError:\n    item_categories_plus = item_categories.copy()\n    #translation\n    trans = Translator()\n    item_categories_plus['item_category_name_en'] = item_categories_plus['item_category_name'].apply(lambda name: trans.translate(name,dest='en').text)\n    item_categories_plus\n    item_categories_plus.to_csv(tables_path + 'item_categories_plus.csv')\n    \n#roughly grouping first\nitem_categories_plus['main_category'] = item_categories_plus['item_category_name_en'].apply(lambda name: name.split('-')[0].strip())\nitem_categories_plus['sub_category'] = item_categories_plus['item_category_name_en'].apply(\n    lambda name: (name.split('-')[1].strip()) if len(name.split('-'))>1 else (name.split('-')[0].strip()))\n\n#manually correct grouping\nitem_categories_plus.loc[24,'main_category'] = 'Games' #translation failed\nitem_categories_plus.loc[71,'main_category'] = 'Platic Bags and Gift' #plastic bag outlier -> stand alone group\nitem_categories_plus.loc[78,'main_category'] = 'Program' #programs -> program\nitem_categories_plus.loc[32,'main_category'] = 'Payment cards' #Payment card (Movies, Music, Games) -> Payment cards, and (Movies, Music, Games)\nitem_categories_plus.loc[32,'sub_category'] = '(Movies, Music, Games)' #Payment card  -> Payment cards\nitem_categories_plus.loc[36,'main_category'] = 'Payment cards'\nitem_categories_plus.loc[37:41,'main_category'] = 'Movies' #correct Movie, Movies, Cinema two groups\n\n#label encoding\nmain_category_encoder = LabelEncoder()\nitem_categories_plus['main_category_enc'] = main_category_encoder.fit_transform(item_categories_plus['main_category']).astype('int8')\nsub_category_encoder = LabelEncoder()\nitem_categories_plus['sub_category_enc'] = sub_category_encoder.fit_transform(item_categories_plus['sub_category']).astype('int8')\n\ndataset = pd.merge(dataset,item_categories_plus[['item_category_id','main_category_enc','sub_category_enc']],how='left',on=['item_category_id'])\n\ndel item_categories_plus\ngc.collect()","98dbb877":"# %%time\n# cate_month_sale = create_mean_encoding_table(dataset, ['item_category_id'], 'cate')\n\n# lag_months = [1]\n# lag_features_list = ['cate_avg_cnt_month', 'cate_avg_revenue_month', 'cate_avg_price_month']\n# on_key = ['item_category_id','date_block_num']\n# fillna = [0,0,0]\n# dtypes = ['float16','float32','float32']\n\n# dataset = add_lag_v3(dataset,cate_month_sale,lag_months,lag_features_list,on_key,fillna,dtypes)\n# dataset = dataset.drop(['cate_avg_cnt_month', 'cate_avg_revenue_month','cate_avg_price_month'],axis=1)\n\n# del cate_month_sale\n# gc.collect()","1db0fd4c":"%%time\nmain_cate_month_sale = create_mean_encoding_table(dataset, ['main_category_enc'], 'main_cate')\n\nlag_months = [1]\nlag_features_list = ['main_cate_avg_cnt_month', 'main_cate_avg_revenue_month', 'main_cate_avg_price_month']\non_key = ['main_category_enc','date_block_num']\nfillna = [0,0,0]\ndtypes = ['float16','float32','float32']\n\ndataset = add_lag_v3(dataset,main_cate_month_sale,lag_months,lag_features_list,on_key,fillna,dtypes)\ndataset = dataset.drop(['main_cate_avg_cnt_month','main_cate_avg_revenue_month', 'main_cate_avg_price_month'],axis=1)\n\ndel main_cate_month_sale\ngc.collect()","3f3de2b1":"%%time\nsub_cate_month_sale = create_mean_encoding_table(dataset, ['sub_category_enc'], 'sub_cate')\n\nlag_months = [1]\nlag_features_list = ['sub_cate_avg_cnt_month', 'sub_cate_avg_revenue_month', 'sub_cate_avg_price_month']\non_key = ['sub_category_enc','date_block_num']\nfillna = [0,0,0]\ndtypes = ['float16','float32','float32']\n\ndataset = add_lag_v3(dataset,sub_cate_month_sale,lag_months,lag_features_list,on_key,fillna,dtypes)\ndataset = dataset.drop(['sub_cate_avg_cnt_month','sub_cate_avg_revenue_month', 'sub_cate_avg_price_month'],axis=1)\n\ndel sub_cate_month_sale\ngc.collect()","1153664f":"%%time\n#add first sold information for categories\ndataset = first_sale_cnt(dataset,dataset,['item_category_id'],'cate')\ndataset = first_sale_cnt(dataset,dataset,['main_category_enc'],'main_cate')\ndataset = first_sale_cnt(dataset,dataset,['sub_category_enc'],'sub_cate')","dd3fba68":"#parse year\/month information from 'date'\ndate_features = df.drop_duplicates(['date_block_num'])[['date','date_block_num']]\ndate_features['year'] = date_features['date'].apply(lambda d: d.split('.')[2])\ndate_features['month'] = date_features['date'].apply(lambda d: d.split('.')[1])\ndate_features = date_features.drop('date',axis=1).reset_index(drop=True)\n\n#fill year\/month information for test data\ntest_date = [34,2015,11]\ndate_features.loc[34] = test_date\n\n#merge into main dataset\ndataset = pd.merge(dataset,date_features,how='left',on=['date_block_num'])\n\n#compress and delete temporary dataset to release memory\ndataset[['month','year']] = dataset[['month','year']].astype('int16')\n\n#keep date_features for later usage","f876b0bd":"dataset['shop_id'] = dataset['shop_id'].replace({0:57,1:58,10:11})\ndf['shop_id'] = df['shop_id'].replace({0:57,1:58,10:11})","c973130c":"try:\n    shops_plus = pd.read_csv(tables_path + 'shops_plus.csv')\nexcept FileNotFoundError:\n    shops_plus = shops.copy()\n    trans = Translator()\n    shops_plus['shop_name_en'] = shops_plus['shop_name'].apply(lambda name: trans.translate(name,dest='en').text)\n    shops_plus.to_csv(tables_path + 'shops_plus.csv')\n\n#extract city\nshops_plus['shop_city'] = shops_plus['shop_name_en'].apply(lambda shop: shop.split(' ')[0].strip())\n\n#manually correct\nshops_plus.loc[0:1,'shop_city'] = 'Yakutsk' #correct name\nshops_plus.loc[12,'shop_city'] = 'Online_emergency' #online emergency shop\nshops_plus.loc[20,'shop_city'] = 'Sale' #seperate from Moscow\nshops_plus.loc[42:43,'shop_city'] = 'St.Petersburg' #correct name\n\n#encode\nshop_enc = LabelEncoder()\nshops_plus['shop_city_enc'] = shop_enc.fit_transform(shops_plus['shop_city'])\nshops_plus['shop_city_enc'] = shops_plus['shop_city_enc'].astype('int8')\n\n#merge city index into main table\ndataset = dataset.merge(shops_plus[['shop_id','shop_city_enc']],how='left',on=['shop_id'])\ndel shops_plus\ngc.collect()","5186984e":"#add lag first sale month and last sale mont\ndataset = add_first_last_sale(dataset,df,['shop_id'],'shop')\ndataset = dataset.drop(['shop_first_sold','shop_last_sold'],axis=1)","ccd708df":"%%time\nshop_month_sale = create_mean_encoding_table(dataset, ['shop_id'], 'shop')\n\nlag_months = [1,2,3,6,12]\nlag_features_list = ['shop_avg_cnt_month', 'shop_avg_revenue_month', 'shop_avg_price_month']\non_key = ['shop_id','date_block_num']\nfillna = [0,0,0]\ndtypes = ['float16','float32','float32']\n\ndataset = add_lag_v3(dataset,shop_month_sale,lag_months,lag_features_list,on_key,fillna,dtypes)\ndataset = dataset.drop(['shop_avg_cnt_month', 'shop_avg_revenue_month','shop_avg_price_month'],axis=1)\n\ndel shop_month_sale\ngc.collect()","da3fafd0":"%%time\nshop_city_month_sale = create_mean_encoding_table(dataset, ['shop_city_enc'], 'shop_city')\n\nlag_months = [1]\nlag_features_list = ['shop_city_avg_cnt_month', 'shop_city_avg_revenue_month', 'shop_city_avg_price_month']\non_key = ['shop_city_enc','date_block_num']\nfillna = [0,0,0]\ndtypes = ['float16','float32','float32']\n\ndataset = add_lag_v3(dataset,shop_city_month_sale,lag_months,lag_features_list,on_key,fillna,dtypes)\ndataset = dataset.drop(['shop_city_avg_cnt_month','shop_city_avg_revenue_month','shop_city_avg_price_month'],axis=1)\n\ndel shop_city_month_sale\ngc.collect()","38bd543f":"# %%time\n# shop_cate_month_sale = create_mean_encoding_table(dataset, ['shop_id','item_category_id'], 'shop_cate')\n\n# lag_months = [1,2,3,6,12]\n# lag_features_list = ['shop_cate_avg_cnt_month', 'shop_cate_avg_revenue_month', 'shop_cate_avg_price_month']\n# on_key = ['shop_id','item_category_id','date_block_num']\n# fillna = [0,0,0]\n# dtypes = ['float16','float32','float32']\n\n# dataset = add_lag_v3(dataset,shop_cate_month_sale,lag_months,lag_features_list,on_key,fillna,dtypes)\n# dataset = dataset.drop(['shop_cate_avg_revenue_month','shop_cate_avg_revenue_month','shop_cate_avg_price_month'],axis=1)\n\n# del shop_cate_month_sale\n# gc.collect()","b5317ad0":"%%time\nshop_main_cate_month_sale = create_mean_encoding_table(dataset, ['shop_id','main_category_enc'], 'shop_main_cate')\n\nlag_months = [1]\nlag_features_list = ['shop_main_cate_avg_cnt_month', 'shop_main_cate_avg_revenue_month', 'shop_main_cate_avg_price_month']\non_key = ['shop_id','main_category_enc','date_block_num']\nfillna = [0,0,0]\ndtypes = ['float16','float32','float32']\n\ndataset = add_lag_v3(dataset,shop_main_cate_month_sale,lag_months,lag_features_list,on_key,fillna,dtypes)\ndataset = dataset.drop(['shop_main_cate_avg_cnt_month','shop_main_cate_avg_revenue_month','shop_main_cate_avg_price_month'],axis=1)\n\ndel shop_main_cate_month_sale\ngc.collect()","ce3b0289":"%%time\nshop_sub_cate_month_sale = create_mean_encoding_table(dataset, ['shop_id','sub_category_enc'], 'shop_sub_cate')\n\nlag_months = [1,2,3,6,12]\nlag_features_list = ['shop_sub_cate_avg_cnt_month', 'shop_sub_cate_avg_revenue_month', 'shop_sub_cate_avg_price_month']\non_key = ['shop_id','sub_category_enc','date_block_num']\nfillna = [0,0,0]\ndtypes = ['float16','float32','float32']\n\ndataset = add_lag_v3(dataset,shop_sub_cate_month_sale,lag_months,lag_features_list,on_key,fillna,dtypes)\ndataset = dataset.drop(['shop_sub_cate_avg_cnt_month','shop_sub_cate_avg_revenue_month','shop_sub_cate_avg_price_month'],axis=1)\n\ndel shop_sub_cate_month_sale\ngc.collect()","d245adcf":"%%time\n#extra table item_id - IPs (names or titles or certain series of games, movies, characters)\nitem_IP = pd.read_csv(items_IP_path) #manually groupped table\nIP_label = LabelEncoder()\nitem_IP['IP_encode'] = IP_label.fit_transform(item_IP['Search_Key_Word'])\ndataset = dataset.merge(item_IP[['item_id','IP_encode']],on='item_id',how='left')\ndataset['IP_encode'] = dataset['IP_encode'].astype('int16')\n\n#google trends API to get trends, (will not override existed data)\nIPs = list(item_IP['Search_Key_Word'].unique())\nIPs.remove('Others')\nIP_trend = get_google_trends(IPs, items_IP_trend_path,'Total War') #collect google trends info by IP keywords\n\n#reshape to long format from wide to be able to merge\ndate_features[['year','month']] = date_features[['year','month']].astype('int64')\nIP_trend = IP_trend.merge(date_features,on=['year','month'],how='left')\nIP_trend = IP_trend.drop(['month','year'],axis=1)\nIP_trend = IP_trend.set_index('date_block_num').stack().reset_index()\nIP_trend.columns = ['date_block_num','Search_Key_Word','google_trends']\n\n#lookup IP_encode (this way allow csv to save old key words and their trends)\nIP_trend = IP_trend.merge(item_IP[['Search_Key_Word','IP_encode']].drop_duplicates(),on='Search_Key_Word',how='left')\nIP_trend = IP_trend.dropna()\nIP_trend = IP_trend.sort_values(by=['IP_encode','date_block_num'])","964ec1ae":"%%time\nlag_months = [1,2,3,6,12]\nlag_features_list = ['google_trends']\non_key = ['IP_encode','date_block_num']\nfillna = [0]\ndtypes = ['int8']\n\ndataset = add_lag_v3(dataset,IP_trend,lag_months,lag_features_list,on_key,fillna,dtypes)\ndataset = dataset.drop(['Search_Key_Word','google_trends'],axis=1)\ndel IP_trend\ngc.collect()","3807a62b":"%%time\nIP_month_sale = create_mean_encoding_table(dataset, ['IP_encode'], 'IP')\n\nlag_months = [1]\nlag_features_list = ['IP_avg_cnt_month', 'IP_avg_revenue_month', 'IP_avg_price_month']\non_key = ['IP_encode','date_block_num']\nfillna = [0,0,0]\ndtypes = ['float16','float32','float32']\n\ndataset = add_lag_v3(dataset,IP_month_sale,lag_months,lag_features_list,on_key,fillna,dtypes)\ndataset = dataset.drop(['IP_avg_cnt_month', 'IP_avg_revenue_month','IP_avg_price_month'],axis=1)\ndel IP_month_sale\ngc.collect()","bbd8916c":"del IPs\ndel item_IP\ndel date_features\n\ndel lag_months\ndel lag_features_list\ndel on_key\ndel fillna\ndel dtypes\n\ndel IP_label\ndel LabelEncoder\n\ndel df\ndel df_test\ndel sample_submission\ndel items\ndel shops\ngc.collect()","dd6e8886":"assert dataset.isnull().sum().sum() == 0\nassert np.isfinite(dataset).sum().sum() == dataset.shape[0] * dataset.shape[1]\n\ndataset = dataset.drop(['avg_price','revenue'],axis=1)\n\ndataset.to_pickle(processed_data_path + 'dataset.pkl')\n\n# del dataset\n# gc.collect()","ebdbe05a":"dataset = pd.read_pickle(processed_data_path + 'dataset.pkl')","3adb56e8":"dataset.columns","5bef55ea":"#feature selection\nselected = [\n    'item_cnt_month', #must (target)\n    'date_block_num', #must (to split training\/validation\/testing, will be omitted later)\n    \n    #pairs-related\n    'item_cnt_month_lag_1m','item_cnt_month_lag_2m','item_cnt_month_lag_3m',\n    'item_cnt_month_lag_6m','item_cnt_month_lag_12m','pair_first_sold_offset',\n    'avg_price_lag_1m',\n    \n    #items-related\n    'item_avg_cnt_month_lag_1m','item_first_sold_offset','item_avg_price_month_lag_1m',\n    \n    #item categories-related\n    'main_cate_avg_cnt_month_lag_1m','main_cate_avg_revenue_month_lag_1m',\n    'sub_cate_avg_revenue_month_lag_1m','sub_cate_avg_cnt_month_lag_1m',    \n    'cate_avg_first_sold_cnt_month','main_cate_avg_first_sold_cnt_month','sub_cate_avg_first_sold_cnt_month',\n    'main_cate_avg_price_month_lag_1m','sub_cate_avg_price_month_lag_1m',\n    \n    #date\n    'month',\n    \n    #shops-related\n    'shop_avg_cnt_month_lag_1m','shop_avg_cnt_month_lag_2m','shop_avg_cnt_month_lag_3m',\n    'shop_avg_cnt_month_lag_6m','shop_avg_cnt_month_lag_12m','shop_city_avg_cnt_month_lag_1m',\n    'shop_avg_price_month_lag_1m','shop_first_sold_offset','shop_last_sold_offset',\n    'shop_main_cate_avg_cnt_month_lag_1m','shop_sub_cate_avg_cnt_month_lag_1m','shop_sub_cate_avg_cnt_month_lag_2m',\n    'shop_sub_cate_avg_cnt_month_lag_3m','shop_sub_cate_avg_cnt_month_lag_6m','shop_sub_cate_avg_cnt_month_lag_12m',\n    \n    #IPs and google trends\n    'IP_avg_cnt_month_lag_1m','IP_avg_revenue_month_lag_1m',\n    'google_trends_lag_1m','google_trends_lag_2m','google_trends_lag_3m',\n    'google_trends_lag_6m','google_trends_lag_12m',\n]\ndataset = dataset[selected]","829feda1":"#exclude first year from training because incomplete lagged features, and take month 33 as validation , monthe 34 as testing\ntrain = dataset[dataset['date_block_num'].between(12,32)]\nvalidate = dataset[dataset['date_block_num']==33]\ntest = dataset[dataset['date_block_num']==34]\n\ndel dataset\ngc.collect()","639d3873":"#check the monthes\nprint(train['date_block_num'].unique())\nprint(validate['date_block_num'].unique())\nprint(test['date_block_num'].unique())","004b4932":"#X,y split\nX_train = train.drop(['item_cnt_month','date_block_num'],axis=1)\ny_train = train['item_cnt_month']\n\nX_validate = validate.drop(['item_cnt_month','date_block_num'],axis=1)\ny_validate = validate['item_cnt_month']\n\nX_test = test.drop(['item_cnt_month','date_block_num'],axis=1)\n\ndel train\ndel test\ndel validate\ngc.collect()","ef8bd857":"#features for model training\nX_train.columns","c25315ed":"#final check of shape and missing\nprint(f'training shape: X {X_train.shape}, y {y_train.shape}')\nprint(f'validation shape: X {X_validate.shape}, y {y_validate.shape}')\nprint(f'testing shape: X {X_test.shape}')\n\nprint(f'training missing: X {X_train.isnull().sum().sum()}, y {y_train.isnull().sum().sum()}')\nprint(f'validation missing: X {X_validate.isnull().sum().sum()}, y {y_validate.isnull().sum().sum()}')\nprint(f'testing missing: X {X_test.isnull().sum().sum()}')","092d4e5f":"#able to save multiple model hyperparameters and select one by comment out \/ uncomment\ndef create_model():\n    \n    model = xgb.XGBRegressor(eta=0.01, max_depth=10,n_estimators=2000,\n                             colsample_bytree=0.5,subsample=0.8,\n                             gamma=2, reg_alpha=0, reg_lambda=2, min_child_weight=300,\n                             tree_method='gpu_hist', gpu_id=0, max_bin=2048,\n                             n_jobs=-1)\n    return model","de3279b3":"%%time\nmodel = create_model()\nmodel.fit(X_train,y_train,eval_set=[(X_train,y_train),(X_validate,y_validate)],eval_metric='rmse',early_stopping_rounds=10)","85019ad6":"with open( model_path + 'model.pkl', 'wb') as f:\n    pickle.dump(model, f)","fe1014ff":"fig = plt.figure(figsize=(8,24))\nax1 = fig.add_subplot(311)\nxgb.plot_importance(model,ax=ax1)\n\nax2 = fig.add_subplot(312)\nxgb.plot_importance(model,ax=ax2,importance_type='gain',title='Gain')\n\nax3 = fig.add_subplot(313)\nxgb.plot_importance(model,ax=ax3,importance_type='cover',title='Cover')","c19c9f65":"pred_val = model.predict(X_validate)\nresidue = y_validate-pred_val\n\nplt.figure(figsize=(6,4))\ng = sns.distplot(residue,bins=35,kde=False)\ng.set(yscale='log')\ng.set(xlabel='residue', ylabel='frequency')","869f0649":"pred_test = model.predict(X_test)\noutput = pd.DataFrame({'item_cnt_month':pred_test})\noutput.index.name = 'ID'","59739e1f":"output = output.clip(0,20)\noutput.to_csv('.\/submission.csv')","37e974ed":"# 2.Exploratory Data Analyses","81e93ad2":"### Validation Residue","5f0e7b17":"### Merge Test Data","fdbc7ace":"# 1.Setup","23d6c1df":"## 4.6 Intellectual Property (IP) and Google Search Trends API\nThe iead of this part is that if some Intellectual Properties (IPs) like The Witcher, Minecraft, Spider Man, Star Wars etc. are popular right now, not just the games or movies themselves, their related merchandise such as key chains or mugs will have a higher sell than others.\n\nMoreover, the model heavily relies on historical sale data, so for new prodcuts, especially those with a very high sale count, are hard to predict. Google Search Trends provides a local keywords search popularity with respect to time, and thus we can utilize the popularity information before the start of selling. For example, a new movie or game or game console is about to release to market, more people getting interesting in, more of them are to search for information.\n\n- Use Pytrends API to get Google Trends infomation\n- Add IP\/Google Trends avg. sale count \/ revenue \/ price lagged features","3314dffa":"## 4.1 Shop-item Pairs\n- Add pair sale count lagged features\n- Add pair price lagged features\n- Add pair fisrt\/last time being sold, both absolute monthes and relative monthes (only the data before each month can be used)","79e7fe71":"### Item Daily Sale Trend\nSome items are stable and with continous sales almost everyday such as shopping bags, batteries or PlayStaion Network wallets. Based on previous sale records, it will be relatively easier to predict.","ac7c38a1":"### Shop Monthly Sale\nThe white grids in the heatmap mean there are no sale records for the shops. It may be closed or haven't started for bussiness or only open for bussiness at special time or events. The heatmap combining with shop names, we can found some shops could be the same.\n- 0 (! Yakutsk Ordzhonikidze, 56 Franc) -> 57 (Yakutsk Ordzhonikidze, 56)\n- 1 (! Yakutsk TC \"Central\" Franc) -> 58 (Yakutsk TC \"Central\")\n- 10 (Zhukovsky Street. Chkalov 39m?) -> 11 (Zhukovsky Street. Chkalov 39m)\n\nMoreover, shop 9 and 20 are only open few times a year. I guess it could be special kind of shops, like itinerant or annual sale.","80db7f7f":"### Load Dataset and Setting\nThe structure allows the pre-processing procedure separated with the training procedure.","0127ad4f":"https:\/\/trends.google.com\/trends\/explore?date=2013-01-01%202015-10-31&geo=RU&q=KINGSMAN,Grand%20Theft%20Auto,%D0%98%D0%B3%D1%80%D0%BE%D0%9C%D0%B8%D1%80&hl=en-US","1b6c13ce":"## Outline\n\n1. Setup\n2. Exploratory Data Analyses\n3. Data Preprocess and Base Creation\n4. Feature Engineering\n    - 4.1 Shop-item Pairs (Lag Features)\n    - 4.2 Items (Lag Features)\n    - 4.3 Item Categories (Main\/Sub-Categories Extraction, Lag Features)\n    - 4.4 Date (Month\/Year Extraction)\n    - 4.5 Shops (Shop CityExtraction, Lag Features, Shop-Item Categories Combination)\n    - 4.6 Intellectual Property(IP) and Google Search Trends API\n5. XGBoost Training\n6. Evaluation and Submission","83b0ff73":"### Re-usable Functions","60b3f3e5":"## 4.5 Shops\n- Merge shops\n- Add City of shops, but shome of them are special seperate from city, \n    like no.9 is itinerant, no.12 is online emergency,no.20 is only open once a year, and no.55 is online shop\n- Add first\/last month the shops have sale record (absolute and relative)\n- Add Shop\/City avg. sale count \/ revenue \/ price lagged features\n- Add Shop-Item Category avg. sale count \/ revenue \/ price lagged features","92042c71":"## 4.2 Items\n- Add item avg. sale count \/ revenue \/ price lagged features\n- Add item fisrt\/last time being sold, both absolute monthes and relative monthes","3b1d70fa":"### Item Daily Sale Trend (conti.)\nHowever, some items, ex. movie, anticipated games, console, tickets, etc., tend to have large number of sale in short term, and sometimes even at the month of release. It gives extra difficulty to predict, because there is no historical sale as a referece. We could use item categories and shops information to assist the predictions, but the gap still huge. My iead is to utilize the external Google Trends (search engine keyword popularity) information to assist the mission. As the figures below, the sudden sale in the dataset could be reflected on the Google Trends, but since we can't leak the future, so the nearest data I can take is the month before the target month.","61322c88":"## 4.4 Date\n- Extract Year\/Month information from date","d19beb72":"Thanks for the inspiring works and translation.\n- https:\/\/www.kaggle.com\/dlarionov\/feature-engineering-xgboost\n- https:\/\/www.kaggle.com\/kyakovlev\/1st-place-solution-part-1-hands-on-data\n- https:\/\/www.kaggle.com\/szhou42\/predict-future-sales-top-11-solution\n- https:\/\/www.kaggle.com\/c\/competitive-data-science-predict-future-sales\/discussion\/54949","43afdc6d":"### Merge Monthly Sale Count \/ Avg. Price \/ Montly Revenue\nThe test set have monthly sale clipped between 0 and 20, so apply same method to the training set. The sale count distribution is skewed, and around 95% of them are either zero or only one sold in that month.","a8bcaedc":"## 4.3 Item Categories\n- Add item_category_id to main dataset\n- Translate item_category_name and create main- and sub-categories. Original names are in format 'MainGroup-SubGroup'\n- Add 3 categories (item_category , main-, sub-category) avg. sale count \/ revenue \/ price lagged features","34a6bf3e":"![GTA.png](attachment:GTA.png)","b1545a32":"# 5. XGBoost Training\nExtreme Gradient Boost (XGBoost) is a powerful optimized decision tree-based grident boost algorithm, and it is possible to accelerate by GPU. Grident boost uses large numbers of relative small and weak decision trees to model the residues for each iteration. In each iteration, it gives a 'direction', then update the predictions and residues until stop criteria are reached.\n\n- Load Dataset and Setting\n- Model Hyperparameter Tuning and Training","8ad331a0":"### Monthly Trend\nWe can observe that for every December, the number of items sold is obviously higher than rest of the year, and overall is trending down.","e47f2651":"# 4. Feature Engineering","973acd88":"# Google Trends for Sale Prediction - XGBoost","2ef2e947":"# 3. Data Preprocess and Base Creation\nIn this section, the base is created. As mentioned, the test set consists of all 214,200 combinations of 42 shops and 5,100 items. Thus, to mimic the test set, the base also extends to all combinations. For examples, in month 0 ('date_block_num'=0), the sale records of 63,224 unique shop-item pairs from 45 shops and 8,115 items. However, 45 shops and 8,115 itmes can create 365,175 pairs, so the rest about 300,000 combinations will be filled by zeros.","7515876f":"### Importance","f843cf8a":"### Ouput Submission","fdde0aac":"![Google_trend_example.png](attachment:Google_trend_example.png)","c5ab218d":"### Model Hyperparameter Tuning and Training\nXGBoost or Gradient boost has risk of being overfitted, even validation could suffer, because tuning hyperparameters or doing feature engineering to reach lower validation loss is a part of model selection. Thus regularization (gamma, reg_lambda, min_child_weight) is applied.\n\nGPU is used to accelerate the training, tree method will switch from default 'approx' to 'gpu_hist', but as far as my experience, not much difference on this set. Then, lower learning rate (eta) is effective to build a better model, but it is extremely time-consuming if not accelerated by GPU.","c08c25fb":"### Finshing\nclear temporary variables, check if missing\/infinite existed, save dataset for latter usage","1101a13b":"# 6. Evaluation and Submission","2103f534":"In this kernel, intellectual property (ex. Grand Theft Auto, Assisan's Creed, Batman, etc.) information is extracted from the item names. The main idea is that if the game or movie is popular, then the related merchandise such as toys, mugs or key chains should be popular as well. Moreover, since the dataset is real-world data, the weeks before anticipated items being sold, related keywords will be frequently searched (Google Trends). The external data from Google Trends is open and publicly accessible. Please upvote if this work is informative to you. Thanks!","ea287c8a":"### Know Your Enemy\nIn training set, we have daily sale data from 60 shops and 21,807 kind of items (363 are new), forming 424,124 shop-itme paris. It means not all items have been sold in all shops. Here comes my first assumption. Since only when the pairs were sold at the given dates the sale data will be included, it is hard to sad the missings (zero sale count) belong to 'not introduced the that market' or 'introduced but not be sold'. Howere, we have to predict 214,400 pairs from 42 shops and 5100 kinds of items in the next month. It probabily includes a lot of zeros, and the model should be capable to predict them as well."}}