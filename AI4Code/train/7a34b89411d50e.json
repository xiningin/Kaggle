{"cell_type":{"b25ffca8":"code","63516dc9":"code","92c7edcb":"code","3df63865":"code","2d28d916":"code","85d7a2c2":"code","962f41a8":"code","ef353362":"code","5279db8a":"code","997f8bf8":"code","49ae1661":"code","fb9ac691":"code","3ee21c2b":"code","8f9eaae4":"code","cdd87243":"code","c17d92e8":"code","14e1e10a":"markdown","7291d704":"markdown","87ff1aa0":"markdown","723c71ba":"markdown","c09e5107":"markdown","ea79c57c":"markdown","82d5ff0f":"markdown","8b0ee910":"markdown","3cd6eed3":"markdown","cd2d6104":"markdown","a13515ee":"markdown"},"source":{"b25ffca8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# IMPORTING LIBRARIES\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, RandomForestRegressor\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.model_selection import train_test_split,RandomizedSearchCV\n\n# Input data files are available in the read-only \"..\/input\/\" directory\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","63516dc9":"# IMPORTING DATA\nbank_data = pd.read_csv('..\/input\/cap-4611-spring-21-assignment-1\/train.csv')\nsample_data = pd.read_csv('..\/input\/cap-4611-spring-21-assignment-1\/sample_submission.csv')\ntest_data = pd.read_csv('..\/input\/cap-4611-spring-21-assignment-1\/test.csv')\nbank_data.head()","92c7edcb":"#GET DATA INFO\nbank_data.info()","3df63865":"# Checking Nan presence\n[print(col) for col in bank_data if bank_data[col].isna().sum() > 0]","2d28d916":"# Computing the descriptive statistics of our numrerical features\n\nbank_data.describe()","85d7a2c2":"bank_data.duplicated().sum()","962f41a8":"# The classes are heavily skewed we need to solve this issue later.\nprint(bank_data['Bankrupt'].value_counts())\nprint('-'* 30)\nprint('Financially stable: ', round(bank_data['Bankrupt'].value_counts()[0]\/len(bank_data) * 100,2), '% of the dataset')\nprint('Financially unstable: ', round(bank_data['Bankrupt'].value_counts()[1]\/len(bank_data) * 100,2), '% of the dataset')","ef353362":"# Checking labels distributions\n\nfor column in bank_data:\n    plt.figure()\n    sns.boxplot(x=bank_data[column])","5279db8a":"# Lets shuffle the data before creating the subsamples\ndf = bank_data.sample(frac=1)\n\n# amount of fraud classes 220 rows\nunstable_df = df.loc[df['Bankrupt'] == 1]\nstable_df = df.loc[df['Bankrupt'] == 0][:220]\n\nnormal_distributed_df = pd.concat([unstable_df, stable_df])\n\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n\nnew_df.head()","997f8bf8":"# Checking labels distributions\nsns.set_context(\"paper\")\n\nplt.figure(figsize = (10,5))\nsns.countplot(new_df['Bankrupt'])\nplt.title('Class Distributions \\n (0: Fin. Stable || 1: Fin. Unstable)', fontsize=14)\nplt.show()","49ae1661":"#DROPPING VALUES\nX = bank_data.drop(['Bankrupt'],axis = 1).values\ny = bank_data['Bankrupt'].values\ny = y.reshape(-1, 1)\n\n# Transforming the dataset\nX_train,X_val,y_train,y_val = train_test_split(X,y,\n                                               test_size = 0.33,\n                                               random_state=42)","fb9ac691":"#DECISION TREE\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import roc_auc_score\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(X_train, y_train)\ny_pred = decisiontree.predict(X_val)\n\n# Classification Report\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(\"Decision Tree Accuracy:\")\nprint(classification_report(y_val, y_pred))\n\nprint(\"ROC AUC SCORE:\", roc_auc_score(y_val,y_pred))","3ee21c2b":"# RANDOM FOREST\ntest_data = test_data.values\nrandomForest = RandomForestClassifier(n_estimators=200, min_samples_split=20, min_samples_leaf=12, max_depth=40, max_features='sqrt',  bootstrap=False)\nrandomForest.fit(X_train, y_train)\ny_pred = randomForest.predict_proba(X_val)[:,1]\n\n# Classification Report\nacc_randomforest = round(accuracy_score(y_val, y_pred.round()) * 100, 2)\nprint(\"Random Forest Accuracy:\")\nprint(classification_report(y_val, y_pred.round()))\n\nprint(\"ROC AUC SCORE:\", roc_auc_score(y_val,y_pred))","8f9eaae4":"#GENERATING SCORE OVERVIEW OF ALL ALGORITHM PERFORMANCES\nmodels = pd.DataFrame({\n    'Model': ['Decision Tree', 'Random Forest'],\n    'Score': [acc_decisiontree, acc_randomforest]})\nmodels.sort_values(by='Score', ascending=True)","cdd87243":"#TAKING A LOOK AT THE SAMPLE DATA\nsample_data","c17d92e8":"#MAKING CSV FILE USING PROBA ALGORITHM\n\ny_pred = randomForest.predict_proba(test_data)[:,1]\noutput = pd.DataFrame({'id': np.arange(0,3409), 'Bankrupt': y_pred})\noutput = output.set_index('id')\noutput['Bankrupt'] = y_pred\noutput.to_csv(\"submission.csv\")\noutput","14e1e10a":"I can see that most of the data is financially stable, however, I would like to take a look over at the distribution of the dataset","7291d704":"Outliers removal\nI have decided not to remove highly skewed outliers in order to increase performance in the algorithm.","87ff1aa0":"MODELING\nI will start modeling the predictions and splitting the data set to get the x values for train and tes and y values for train and test.","723c71ba":"Despite the fact that we already know that there are no missing values it is important to computationally check that this is true, to avoid errors and time wasted in the following steps of the project. A further consideration needs to be done on the possibility to have duplicates in our data. Duplicates are identical observations that can create redundancy in our data and need to be dropped.","c09e5107":"Now that I have an idea of our data, I need to obtain more information possible on them. The first action that I want to understand is the nature of the data, namely if the data are numerical or categorical and if we have missing information among them. I can start taking a look a the data using data.info() which was provided in on of the first classes for this semester.","ea79c57c":"The resulting panel is strongly informative for us, and it shows how:\n\nThe dataset is composed of a combination of 6819 observations per each of our 96 features. Also, I can tell that all features are numerical and they are either Integer or a float value. Also, I can conlcude that there are no missing values among the data. considering that all of the features are numeric. I can easily calculate their descriptive statistics. ","82d5ff0f":"Now, let's look at the distribution of these features for companies that are close to bankruptcy by just taking a look over at the shape distribution","8b0ee910":"Comparing the results with the one obtained using undersampling, we can see how the difference is between using random Forest and Decision Tree Algorithm. I can conclude that Random Forest would be the best option for me","3cd6eed3":"Considering the project task, and now that we have a general overview of our data, we need focus our attention on the labels: which are the financially stable and unstable companies? I can use the following piece of code","cd2d6104":"Since, I can see that Proba Algorithm provides me with a desired output and it provides with a close approach to what I am looking for. I have enough evidence to conclude that RandomForest is the best Machine Learning Algorithm to attack this Bankrupcy prediction","a13515ee":"Looking at the plot above we can clearly see how our labels are strongly unbalanced, and this is a the main obstacle that we need to solve to obtain good performances."}}