{"cell_type":{"622f1e81":"code","9a07d097":"code","d7c28054":"code","e265c642":"code","0bae3156":"code","76ca924e":"code","cbad75f6":"code","d19e5df9":"code","a7010360":"code","f8e69509":"code","d8eb8c81":"code","1a6b49f5":"code","9c6da139":"code","cf5af2ee":"markdown","9647629a":"markdown","028e2738":"markdown","cd783ba3":"markdown"},"source":{"622f1e81":"import tensorflow as tf \nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","9a07d097":"learning_rate = 0.001\nn_epochs = 100\nbatch_size = 50","d7c28054":"data_dir=\"..\/input\"\n\ndef unpickle(file):\n    '''Load byte data from file'''\n    with open(file, 'rb') as f:\n        data = pickle.load(f,encoding='latin1')\n    return data\n\n\ndef load_cifar10_data():\n    '''\n    Return train_data, train_labels, test_data, test_labels\n    The shape of data returned would be as it is in the data-set N X 3072\n\n    We don't particularly need the metadata - the mapping of label numbers to real labels\n    '''\n    train_data = None\n    train_labels = []\n\n    for i in range(1, 6):\n        data_dic = unpickle(data_dir + \"\/data_batch_\"+str(i))\n        if i == 1:\n            train_data = data_dic['data']\n        else:\n            train_data = np.append(train_data, data_dic['data'])\n        train_labels += data_dic['labels']\n\n    test_data_dic = unpickle(data_dir + \"\/test_batch\")\n    test_data = test_data_dic['data']\n    test_labels = test_data_dic['labels']\n    names=unpickle(data_dir+'\/batches.meta')\n    \n    return train_data, np.array(train_labels), test_data, np.array(test_labels), names['label_names']\n","e265c642":"train_data,train_labels,test_data,test_labels,names=load_cifar10_data()\ntrain_data.shape","0bae3156":"IMG_IDX=13\ntrain_data_vis=train_data.reshape((-1,3072))\ntrain_data_vis=train_data_vis[IMG_IDX].reshape(3,32,32).transpose([1, 2, 0])\nplt.imshow(train_data_vis,interpolation='sinc')\nprint (names[train_labels[IMG_IDX]])\n\n","76ca924e":"features=tf.placeholder(tf.float32,shape=(None,3*1024)) # will store the train-set images in each of its rows\nlabels=tf.placeholder(tf.float32,shape=(None))\n\ninput_layer=tf.reshape(features,[-1,32,32,3]) # -1 is to derive the shape along that axis automatically\n\n# Convolutional Layer #1\nkernel_1 = tf.Variable(tf.random_normal([5,5,3,64]))\nbiases_1 = tf.Variable(tf.random_normal([64]))\nconv1_ = tf.nn.conv2d(\n    input_layer,\n    kernel_1,\n    [1,1,1,1],\n    padding=\"VALID\")\n\nconv1=tf.nn.relu(tf.nn.bias_add(conv1_,biases_1))\n\n# Pooling Layer #1\npool1 = tf.nn.max_pool(conv1, [1,2,2,1], [1,1,1,1],\"VALID\")","cbad75f6":"pool1.shape","d19e5df9":"# Convolutional Layer #2 and Pooling Layer #2\nkernel_2 = tf.Variable(tf.random_normal([5,5,64,128]))\nbiases_2=tf.Variable(tf.random_normal([128]))\nconv2_ = tf.nn.conv2d(\n    pool1,\n    kernel_2,\n    [1,1,1,1],\n    padding=\"VALID\")\nconv2=tf.nn.relu(tf.nn.bias_add(conv2_,biases_2))\npool2 = tf.nn.max_pool(conv2, [1,2,2,1], [1,2,2,1],\"VALID\")\npool2_flat=tf.reshape(pool2,[-1,11*11*128])","a7010360":"pool2.shape","f8e69509":"'''\n# Convolutional Layer #3 and Pooling Layer #3\nkernel_3 = tf.Variable(tf.random_normal([3,3,128,32]))\nbiases_3 = tf.Variable(tf.random_normal([32]))\nconv3_ = tf.nn.conv2d(\n    pool2,\n    kernel_3,\n    [1,1,1,1],\n    padding=\"VALID\")\nconv3=tf.nn.relu(tf.nn.bias_add(conv3_,biases_3))\npool3 = tf.nn.max_pool(conv3, [1,2,2,1], [1,2,2,1],\"VALID\")\n'''","d8eb8c81":"pool3.shape","1a6b49f5":"#pool3_flat = tf.reshape(pool3, [-1, 4 * 4 * 32])\n\ndense1 = tf.layers.dense(inputs=pool2_flat, units=3072)\ndense2 = tf.layers.dense(inputs=dense1, units=1012)\ndense3 = tf.layers.dense(inputs=dense2, units=200)\n\n# we have only 10 classes to classify\nlogits = tf.layers.dense(inputs=dense3, units=10)\n\n# finding the class wiht max probability\npredictions = tf.argmax(input=logits, axis=1)\nonehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\nloss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)\ntrain_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n\n\naccu = tf.metrics.accuracy(labels=labels, predictions=predictions)\ninit_g = tf.global_variables_initializer()\ninit_l = tf.local_variables_initializer()","9c6da139":"sess=tf.Session()\nsess.run(init_g)\nsess.run(init_l)\n\n#############################  The real training stage begins here ##############################################\ntrain_data = train_data.reshape(-1,3072)\nrand_index = np.arange(len(train_data))\nfor j in range(n_epochs):\n    np.random.shuffle(rand_index)\n    train_data=train_data[rand_index]\n    train_labels=train_labels[rand_index]\n    train_data_split=np.split(train_data,batch_size)\n    train_labels_split=np.split(train_labels,batch_size)\n    n_batches = len(train_data_split)\n    for i in range(n_batches):\n        batch_x = train_data_split[i]\n        batch_y = train_labels_split[i]\n        sess.run(train_step, feed_dict={features: batch_x, labels: batch_y})\n        # finding E_out at the end of each epoch\n    acc_val=sess.run(accu,feed_dict={features: test_data.reshape((-1,3072)), labels: test_labels})\n    print (\"epoch:\",j,\":\\t\\t Test Accuracy:% 0.4f\" %(acc_val[1]))","cf5af2ee":"## Viewing Some Samples","9647629a":"## Training the graph","028e2738":"## Building the model now:","cd783ba3":"# CNN trained on CIFAR - 10 dataset\n**More detailed notebook can be found in my GitHub repo [here](https:\/\/github.com\/suyashdamle\/deep_learning_projects\/blob\/master\/CIFAR\/CIFAR_CNN.ipynb)**"}}