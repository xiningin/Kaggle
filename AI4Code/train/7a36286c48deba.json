{"cell_type":{"84c52c51":"code","06fc3afe":"code","52cfad72":"code","1ab44fc2":"code","f23dd0da":"code","f983ff1f":"code","b6c7a116":"code","6c7ae1c2":"code","ad1be970":"code","5aa3ccad":"code","058ea770":"code","d81f0e70":"code","b22c124b":"code","bb201eff":"code","2d87b5aa":"code","36a6066a":"code","f5094bfd":"code","10c400c5":"code","16710be8":"code","34038991":"code","2a8d9cd3":"code","21d1d169":"code","89e94f33":"code","70b51fd2":"code","d0aa648d":"code","e36f8d8d":"code","c2f4e786":"code","e40416e8":"code","882c0c11":"markdown"},"source":{"84c52c51":"import pandas as pd \nfrom collections import Counter\nimport string\nimport scipy as np\nfrom tqdm import tqdm\nfrom keras.preprocessing import sequence\nfrom sklearn.model_selection import train_test_split\nfrom bs4 import BeautifulSoup\nfrom random import randint\nfrom datetime import datetime\nimport re\nimport time\nimport warnings \nwarnings.filterwarnings('ignore')\n\nimport tensorflow as tf","06fc3afe":"%matplotlib notebook\nimport matplotlib.pyplot as plt\n\ndef plt_dynamic(x, vy, ty, ax, colors=['b'], save=False):\n    ax.clear()\n    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n    ax.plot(x, ty, 'r', label=\"Train Loss\")\n    plt.ylim([0.49,0.5])\n    plt.legend()\n    plt.grid()\n    plt.pause(1)\n    fig.canvas.draw()\n    if save:\n        plt.save('plot.png')","52cfad72":"import os\nos.listdir('..\/input')","1ab44fc2":"data = pd.read_csv('..\/input\/Reviews.csv').dropna()\nprint(data.columns)","f23dd0da":"data = data[['Text', 'Summary', 'Score']]\ndata.drop_duplicates(inplace=True)\ndata.fillna('', inplace=True)\ndata = data[data['Score'] != 3] \ndata['Score'] = [1 if item>3 else 0 for item in data['Score'].values]","f983ff1f":"data['Score'].unique()","b6c7a116":"def preprocess(x):\n    # x = BeautifulSoup(x, 'lxml').get_text()\n    x = re.sub('<[^>]*>', '', x)\n    for punc in string.punctuation:\n        if punc != \"\\'\":\n            x = x.replace(punc, f' {punc} ')\n    return ' '.join(x.split()).lower()\n\ndata['Text'] = [preprocess(item) for item in tqdm(data['Text'].values, total=len(data['Text']))]\ndata['Summary'] = [preprocess(item) for item in tqdm(data['Summary'].values, total=len(data['Summary']))]","6c7ae1c2":"X_data = [i+' '+j for i,j in zip(list(data['Summary'].values), list(data['Text'].values))]\nY_data = list(data['Score'].values)","ad1be970":"corpus = dict(Counter(' '.join(X_data).split()))\nprint('Number of unique tokens:', len(corpus))","5aa3ccad":"min_word_count = np.percentile(list(corpus.values()), 90)\nprint('Minimum frequency of words:', min_word_count)","058ea770":"words = list(corpus.keys())\nfor w in words:\n    if corpus[w] < min_word_count:\n        del corpus[w]\n\nprint('Number of unique tokens after deleting less frequent tokens:', len(corpus))","d81f0e70":"seq_len = [len(item.split()) for item in X_data]\n\nsuitable_seq_len = int(np.percentile(seq_len, 90))\nprint('Suitable sequence length:', suitable_seq_len)","b22c124b":"# Creating the word ids\nword_ids = {\n    item: index+2 for index, item in enumerate(corpus.keys())\n}","bb201eff":"X_data_int = []; Y_data_new = []\nfor item, y in zip(X_data, Y_data):\n    temp = [word_ids.get(word, 1) for word in item.split()]\n    if temp:\n        X_data_int.append(temp)\n        Y_data_new.append(y)","2d87b5aa":"X_data_int = sequence.pad_sequences(X_data_int, maxlen=suitable_seq_len)\nprint('Sample X_data with word ids:', X_data_int[0])\nprint('Sample X_data with proper words:', X_data[0])","36a6066a":"def one_hot_maker(x):\n    with tf.Session() as sess:\n         return sess.run(tf.one_hot(x, depth=len(np.unique(x))))\n    \nY_data_new = one_hot_maker(Y_data_new)","f5094bfd":"X_train, X_test, y_train, y_test = train_test_split(X_data_int, Y_data_new, test_size=0.027, random_state=101)\nprint(len(X_train), len(X_test))","10c400c5":"# defining the configuration\n\nconfig = {\n    'rnn_size': 128,\n    'rnn_layer': 1,\n    'sequence_length': suitable_seq_len,\n    'word_embedding_size': 300,\n    'vocab_size': len(corpus)+2,\n    'learning_rate': 3e-4,\n    'batch_size': 128,\n    'epoch': 10,\n    'num_classes': len(y_train[0]),\n    'dropout_lstm': .5,\n    'dropout_dense': .5,\n    'dense_unit_size': 100,\n    'l2_reg_param': .01\n}\n\ndata_x = tf.placeholder(name='data_x', dtype=tf.int64, shape=[None, config['sequence_length']])\ntarget = tf.placeholder(name='target', dtype=tf.float32, shape=[None, config['num_classes']])","16710be8":"config","34038991":"def get_embedding():\n    with tf.variable_scope('get_embedding', reuse=tf.AUTO_REUSE):\n        word_embedding = tf.get_variable('word_embedding', [config['vocab_size'], config['word_embedding_size']])\n        return word_embedding","2a8d9cd3":"def get_lstm_cell():\n    lstm_single_layer = tf.contrib.rnn.LSTMCell(config['rnn_size'], name='LSTM_CELLS', state_is_tuple=True)\n    dropout = tf.contrib.rnn.DropoutWrapper(lstm_single_layer, output_keep_prob=config['dropout_lstm'])\n    return dropout","21d1d169":"def create_rnn():\n    cell = tf.contrib.rnn.MultiRNNCell([get_lstm_cell() for _ in range(config['rnn_layer'])], state_is_tuple=True)\n    return cell","89e94f33":"def get_weights():\n    weights = tf.keras.initializers.he_normal(seed=None)\n    biases = tf.zeros_initializer()\n    return weights, biases\n\n# defining model \n\ndef model():\n    # getting the embedding\n    word_embedding = get_embedding()\n    embedded_words = tf.nn.embedding_lookup(word_embedding, data_x)\n    \n    # creating the RNN layer\n    CELL = create_rnn()\n    with tf.variable_scope('model', reuse=tf.AUTO_REUSE):\n        OUTPUT, FINAL_STATE = tf.nn.dynamic_rnn(CELL, embedded_words, dtype=tf.float32)\n    OUTPUT = tf.transpose(OUTPUT, [1,0,2])\n    OUTPUT = tf.gather(OUTPUT, OUTPUT.get_shape()[0]-1)\n    \n    # defining weight for dense layers\n    \n    # Dense layer 1\n    dense = tf.contrib.layers.fully_connected(OUTPUT, num_outputs=config['dense_unit_size'],\n                                             activation_fn=tf.nn.leaky_relu, weights_initializer=get_weights()[0], \n                                              biases_initializer=get_weights()[1])\n    dense = tf.contrib.layers.dropout(dense, keep_prob=config['dropout_dense'])\n    \n    # Dense layer 2\n    dense = tf.contrib.layers.fully_connected(dense, num_outputs=config['dense_unit_size'],\n                                             activation_fn=tf.nn.leaky_relu, weights_initializer=get_weights()[0], \n                                              biases_initializer=get_weights()[1])\n    dense = tf.contrib.layers.dropout(dense, keep_prob=config['dropout_dense'])\n    \n    # adding a batch normalization layer\n    # I know the paper claims that we should add batch normalization before activation function of the previous layer, but [https:\/\/goo.gl\/7CP9hs] this link claims otherwise.\n    dense = tf.layers.batch_normalization(dense)\n    \n    # Dense layer 3\n    dense = tf.contrib.layers.fully_connected(dense, num_outputs=config['dense_unit_size'],\n                                             activation_fn=tf.nn.leaky_relu, weights_initializer=get_weights()[0], \n                                              biases_initializer=get_weights()[1])\n    dense = tf.contrib.layers.dropout(dense, keep_prob=config['dropout_dense'])\n    \n    # Last softmax layer\n    predictions = tf.contrib.layers.fully_connected(dense, num_outputs=config['num_classes'],\n                                                   activation_fn=tf.nn.softmax, \n                                                    weights_initializer=tf.truncated_normal_initializer(mean=0., stddev=.1), \n                                                  biases_initializer=get_weights()[1])\n    return predictions","70b51fd2":"pred = model()\nprint(pred)\nprint(pred.get_shape())","d0aa648d":"tf.trainable_variables()","e36f8d8d":"with tf.variable_scope('setting_training', reuse=tf.AUTO_REUSE):\n    y_hats = model()\n    \n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=target, logits=y_hats)\\\n                         + tf.add_n([config['l2_reg_param']*tf.nn.l2_loss(V) for V in tf.trainable_variables()]))\n    \n    optimizer = tf.train.AdamOptimizer(learning_rate=config['learning_rate'])\n    \n    train = optimizer.minimize(cost)\n    \n    mistakes = tf.not_equal(tf.argmax(target, 1), tf.argmax(y_hats, 1))\n    error = tf.reduce_mean(tf.cast(mistakes, tf.float32))","c2f4e786":"no_of_tr_batches = int(len(X_train)\/config['batch_size'])\nno_of_ts_batches = int(len(X_test)\/config['batch_size'])\ntrain_loss = []\ntest_loss = []","e40416e8":"fig, ax = plt.subplots(1,1)\nplt_dynamic([1,2], [1,2], [1,3], ax)\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\nfor i in range(config['epoch']):\n    print('epoch:',i+1)\n    time.sleep(1)\n    cc = 0\n    # for training data\n    for j in range(no_of_tr_batches):\n        inp = X_train[j*config['batch_size']:(j+1)*config['batch_size']]\n        out = y_train[j*config['batch_size']:(j+1)*config['batch_size']]\n        _, c = sess.run([train, cost], {data_x: inp, target: out})\n        cc += c\n        print(f'{j+1} \/ {no_of_tr_batches}', end='\\r')\n    train_loss.append(cc\/no_of_tr_batches)\n    print('')\n    # for validation data\n    cc = 0\n    for j in range(no_of_ts_batches):\n        inp = X_test[j*config['batch_size']:(j+1)*config['batch_size']]\n        out = y_test[j*config['batch_size']:(j+1)*config['batch_size']]\n        c = sess.run(cost, {data_x: inp, target: out})\n        cc += c\n        print(f'{j+1} \/ {no_of_ts_batches}', end='\\r')\n    test_loss.append(cc\/no_of_ts_batches)\n    print('')\n#     if i%5 == 0:\n    print('Train loss:', train_loss[-1])\n    print('Validation loss:', test_loss[-1])\n    print('Validation error:', sess.run(error, {data_x: X_test, target: y_test}))\n    print('='*40)\n    plt_dynamic(range(i+1), test_loss, train_loss, ax)\n#     else:\n#         plt_dynamic(range(i+1), test_loss, train_loss, ax)","882c0c11":"## CONSTRUCTION AND TRAINING"}}