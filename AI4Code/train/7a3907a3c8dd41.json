{"cell_type":{"fad89c39":"code","fba58148":"code","73ff931f":"code","a611e7a7":"code","72e84431":"code","9e9d9725":"code","b5656524":"code","95517254":"code","a1e29ab5":"code","c69ff1a3":"code","eca69872":"code","9e234c4c":"code","0ba22ce8":"code","458fa7c0":"code","86ef85d3":"code","289d885d":"code","82b28200":"code","8e367d81":"code","029605a4":"code","b23b3888":"code","ce31246d":"code","863d25b8":"code","259626fd":"code","60a8b24e":"code","d833577a":"code","f13c7ac8":"code","c5b536e7":"code","5b2b32b8":"code","69d265c7":"code","950a80c7":"code","5060f3e3":"code","f858b292":"code","9599c028":"code","034a4f07":"code","faac4b76":"code","9c216d8f":"code","425da5b2":"code","fa196dbf":"code","ad78de02":"code","3336c541":"code","90468330":"code","5c020bd7":"code","0ee9c65e":"code","d3c9398d":"code","1bc27644":"code","8bdc22b4":"code","bab90ccc":"code","ada8512b":"markdown","41c9b2d9":"markdown"},"source":{"fad89c39":"#Getting some helper functions and classes\nfrom __future__ import absolute_import, division, print_function\n\nimport csv\nimport os\nimport sys\n\ncsv.field_size_limit(2147483647) # Increase CSV reader's field limit incase we have long text.\n\n\nclass InputExample(object):\n    \"\"\"A single training\/test example for simple sequence classification.\"\"\"\n\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label\n\n\nclass DataProcessor(object):\n    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n\n    def get_train_examples(self, data_dir):\n        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n        raise NotImplementedError()\n\n    def get_dev_examples(self, data_dir):\n        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n        raise NotImplementedError()\n\n    def get_labels(self):\n        \"\"\"Gets the list of labels for this data set.\"\"\"\n        raise NotImplementedError()\n\n    @classmethod\n    def _read_tsv(cls, input_file, quotechar=None):\n        \"\"\"Reads a tab separated value file.\"\"\"\n        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n            lines = []\n            for line in reader:\n                if sys.version_info[0] == 2:\n                    line = list(unicode(cell, 'utf-8') for cell in line)\n                lines.append(line)\n            return lines\n\n\nclass BinaryClassificationProcessor(DataProcessor):\n    \"\"\"Processor for binary classification dataset.\"\"\"\n\n    def get_train_examples(self, data_dir):\n        \"\"\"See base class.\"\"\"\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n\n    def get_dev_examples(self, data_dir):\n        \"\"\"See base class.\"\"\"\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n\n    def get_labels(self):\n        \"\"\"See base class.\"\"\"\n        return [\"0\", \"1\"]\n\n    def _create_examples(self, lines, set_type):\n        \"\"\"Creates examples for the training and dev sets.\"\"\"\n        examples = []\n        for (i, line) in enumerate(lines):\n            guid = \"%s-%s\" % (set_type, i)\n            text_a = line[3]\n            label = line[1]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n        return examples","fba58148":"#Getting some helper functions and classes\nclass InputFeatures(object):\n    \"\"\"A single set of features of data.\"\"\"\n\n    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_id = label_id\n\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n\n    # This is a simple heuristic which will always truncate the longer sequence\n    # one token at a time. This makes more sense than truncating an equal percent\n    # of tokens from each, since if one sequence is very short then each token\n    # that's truncated likely contains more information than a longer sequence.\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()\n\n\ndef convert_example_to_feature(example_row):\n    # return example_row\n    example, label_map, max_seq_length, tokenizer, output_mode = example_row\n\n    tokens_a = tokenizer.tokenize(example.text_a)\n\n    tokens_b = None\n    if example.text_b:\n        tokens_b = tokenizer.tokenize(example.text_b)\n        # Modifies `tokens_a` and `tokens_b` in place so that the total\n        # length is less than the specified length.\n        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n    else:\n        # Account for [CLS] and [SEP] with \"- 2\"\n        if len(tokens_a) > max_seq_length - 2:\n            tokens_a = tokens_a[:(max_seq_length - 2)]\n\n    tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n    segment_ids = [0] * len(tokens)\n\n    if tokens_b:\n        tokens += tokens_b + [\"[SEP]\"]\n        segment_ids += [1] * (len(tokens_b) + 1)\n\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n    # tokens are attended to.\n    input_mask = [1] * len(input_ids)\n\n    # Zero-pad up to the sequence length.\n    padding = [0] * (max_seq_length - len(input_ids))\n    input_ids += padding\n    input_mask += padding\n    segment_ids += padding\n\n    assert len(input_ids) == max_seq_length\n    assert len(input_mask) == max_seq_length\n    assert len(segment_ids) == max_seq_length\n\n    if output_mode == \"classification\":\n        label_id = label_map[example.label]\n    elif output_mode == \"regression\":\n        label_id = float(example.label)\n    else:\n        raise KeyError(output_mode)\n\n    return InputFeatures(input_ids=input_ids,\n                         input_mask=input_mask,\n                         segment_ids=segment_ids,\n                         label_id=label_id)","73ff931f":"#Getting bert\n!pip install pytorch-pretrained-bert","a611e7a7":"#Getting the relevant imports\nimport torch\nimport pickle\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\nfrom torch.nn import CrossEntropyLoss, MSELoss\n\nfrom tqdm import tqdm_notebook, trange\nimport os\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification\nfrom pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n\nfrom multiprocessing import Pool, cpu_count\nimport pandas as pd\nimport numpy as np\nimport tarfile\nimport shutil\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","72e84431":"#Readin the train file\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntrain_df.head()","9e9d9725":"#Reading the test file\ntest_df = pd.read_csv('..\/input\/test.csv')\ntest_df.head()","b5656524":"#Getting the training data bert ready\ntrain_df_bert = pd.DataFrame({\n    'id':range(len(train_df)),\n    'label':train_df['sentiment'],\n    'alpha':['a']*train_df.shape[0],\n    'text': train_df['review'].replace(r'\\n', ' ', regex=True)\n})\n\ntrain_df_bert.head()","95517254":"#Getting the testing data bert ready\ntest_df_bert = pd.DataFrame({\n    'id':range(len(test_df)),\n    'label':'1',\n    'alpha':['a']*test_df.shape[0],\n    'text': test_df['review'].replace(r'\\n', ' ', regex=True)\n})\n\ntest_df_bert.head()","a1e29ab5":"#Getting the relevant data files\ntrain_df_bert.to_csv('train.tsv', sep='\\t', index=False, header=False)\ntest_df_bert.to_csv('test.tsv', sep='\\t', index=False, header=False)\nprint('Training and testing files are generated')","c69ff1a3":"#Instantiating model parameters and directories\nDATA_DIR = ''\nBERT_MODEL = 'bert-base-cased'\nTASK_NAME = 'imdb'\nOUTPUT_DIR = f'outputs\/{TASK_NAME}\/'\nREPORTS_DIR = f'reports\/{TASK_NAME}_evaluation_report\/'\nCACHE_DIR = 'cache\/'\nMAX_SEQ_LENGTH = 256\n\nTRAIN_BATCH_SIZE = 24\nEVAL_BATCH_SIZE = 32\nLEARNING_RATE = 2e-5\nNUM_TRAIN_EPOCHS = 10\nRANDOM_SEED = 42\nGRADIENT_ACCUMULATION_STEPS = 1\nWARMUP_PROPORTION = 0.1\nOUTPUT_MODE = 'classification'\n\nCONFIG_NAME = \"config.json\"\nWEIGHTS_NAME = \"pytorch_model.bin\"\noutput_mode = OUTPUT_MODE\ncache_dir = CACHE_DIR","eca69872":"#Making the required directories\nif os.path.exists(REPORTS_DIR) and os.listdir(REPORTS_DIR):\n        REPORTS_DIR += f'\/report_{len(os.listdir(REPORTS_DIR))}'\n        os.makedirs(REPORTS_DIR)\nif not os.path.exists(REPORTS_DIR):\n    os.makedirs(REPORTS_DIR)\n    REPORTS_DIR += f'\/report_{len(os.listdir(REPORTS_DIR))}'\n    os.makedirs(REPORTS_DIR)","9e234c4c":"#Making the required directories\nif os.path.exists(OUTPUT_DIR) and os.listdir(OUTPUT_DIR):\n        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(OUTPUT_DIR))\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","0ba22ce8":"#Using a processor to read the data from training file\nprocessor = BinaryClassificationProcessor()\ntrain_examples = processor.get_train_examples(DATA_DIR)\ntrain_examples_len = len(train_examples)\nprint(len(train_examples))","458fa7c0":"#Using the processor to get the labels\nlabel_list = processor.get_labels() # [0, 1] for binary classification\nnum_labels = len(label_list)\nprint(label_list)\nprint(len(label_list))","86ef85d3":"#Calculating the number of optimization steps\nnum_train_optimization_steps = int(\n    train_examples_len \/ TRAIN_BATCH_SIZE \/ GRADIENT_ACCUMULATION_STEPS) * NUM_TRAIN_EPOCHS\nprint(num_train_optimization_steps)","289d885d":"# Load pre-trained model tokenizer (vocabulary)\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\nprint(tokenizer)","82b28200":"#getting the label map\nlabel_map = {label: i for i, label in enumerate(label_list)}\ntrain_examples_for_processing = [(example, label_map, MAX_SEQ_LENGTH, tokenizer, OUTPUT_MODE) for example in train_examples]","8e367d81":"#Converting the input data in the required format\nprocess_count = cpu_count() - 1\nif __name__ ==  '__main__':\n    print(f'Preparing to convert {train_examples_len} examples..')\n    print(f'Spawning {process_count} processes..')\n    with Pool(process_count) as p:\n        train_features = list(tqdm_notebook(p.imap(convert_example_to_feature, train_examples_for_processing), total=train_examples_len))","029605a4":"#Dumping the converted features into a file\nwith open(DATA_DIR + \"train_features.pkl\", \"wb\") as f:\n    pickle.dump(train_features, f)","b23b3888":"#Loading the pretrained model\nmodel = BertForSequenceClassification.from_pretrained(BERT_MODEL, cache_dir=CACHE_DIR, num_labels=num_labels)","ce31246d":"#Moving the model to GPU\nmodel.to(device)","863d25b8":"#Instantiating the parameters of optimizer\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]","259626fd":"#Instantiating the optimizer\noptimizer = BertAdam(optimizer_grouped_parameters,\n                     lr=LEARNING_RATE,\n                     warmup=WARMUP_PROPORTION,\n                     t_total=num_train_optimization_steps)","60a8b24e":"#Initializing some relevant variables\nglobal_step = 0\nnb_tr_steps = 0\ntr_loss = 0","d833577a":"#Converting the input features to appropriate tensors\nall_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\nall_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\nall_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n\nif OUTPUT_MODE == \"classification\":\n    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\nelif OUTPUT_MODE == \"regression\":\n    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.float)","f13c7ac8":"#Dataloader and data instantiated\ntrain_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=TRAIN_BATCH_SIZE)","c5b536e7":"#Training starts\nmodel.train()\nfor _ in trange(int(NUM_TRAIN_EPOCHS), desc=\"Epoch\"):\n    tr_loss = 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    for step, batch in enumerate(tqdm_notebook(train_dataloader, desc=\"Iteration\")):\n        batch = tuple(t.to(device) for t in batch)\n        input_ids, input_mask, segment_ids, label_ids = batch\n\n        logits = model(input_ids, segment_ids, input_mask, labels=None)\n\n        if OUTPUT_MODE == \"classification\":\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n        elif OUTPUT_MODE == \"regression\":\n            loss_fct = MSELoss()\n            loss = loss_fct(logits.view(-1), label_ids.view(-1))\n\n        if GRADIENT_ACCUMULATION_STEPS > 1:\n            loss = loss \/ GRADIENT_ACCUMULATION_STEPS\n\n        loss.backward()\n        print(\"\\r%f\" % loss, end='')\n        \n        tr_loss += loss.item()\n        nb_tr_examples += input_ids.size(0)\n        nb_tr_steps += 1\n        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n            global_step += 1","5b2b32b8":"#Saving the model\nmodel_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n\n# If we save using the predefined names, we can load using `from_pretrained`\noutput_model_file = os.path.join(OUTPUT_DIR, WEIGHTS_NAME)\noutput_config_file = os.path.join(OUTPUT_DIR, CONFIG_NAME)\n\ntorch.save(model_to_save.state_dict(), output_model_file)\nmodel_to_save.config.to_json_file(output_config_file)\ntokenizer.save_vocabulary(OUTPUT_DIR)","69d265c7":"#Compressing the model and exporting it to the cache folder for later access for evaluation\nos.chdir('outputs\/imdb\/')\nwith tarfile.open('imdb.tar.gz', \"w:gz\") as tar:\n        tar.add('config.json', arcname=os.path.basename('config.json'))\n        tar.add('pytorch_model.bin', arcname=os.path.basename('pytorch_model.bin'))\nshutil.copy('imdb.tar.gz','..\/..\/'+CACHE_DIR)\nos.chdir('..\/..\/')","950a80c7":"#Ensuring the relevant imports\nimport torch\nimport numpy as np\nimport pickle\n\nfrom sklearn.metrics import matthews_corrcoef, confusion_matrix\n\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n                              TensorDataset)\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn import CrossEntropyLoss, MSELoss\n\nfrom multiprocessing import Pool, cpu_count\n\nfrom tqdm import tqdm_notebook, trange\nimport os\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification\nfrom pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","5060f3e3":"# The input data dir. Should contain the .tsv files (or other data files) for the task.\nDATA_DIR = \"\"\n\n# Bert pre-trained model selected in the list: bert-base-uncased, \n# bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased,\n# bert-base-multilingual-cased, bert-base-chinese.\nBERT_MODEL = 'imdb.tar.gz'\n\n# The name of the task to train.I'm going to name this 'yelp'.\nTASK_NAME = 'imdb'\n\n# The output directory where the fine-tuned model and checkpoints will be written.\nOUTPUT_DIR = f'outputs\/{TASK_NAME}\/'\n\n# The directory where the evaluation reports will be written to.\nREPORTS_DIR = f'reports\/{TASK_NAME}_evaluation_reports\/'\n\n# This is where BERT will look for pre-trained models to load parameters from.\nCACHE_DIR = 'cache\/'\n\n# The maximum total input sequence length after WordPiece tokenization.\n# Sequences longer than this will be truncated, and sequences shorter than this will be padded.\nMAX_SEQ_LENGTH = 256\n\nTRAIN_BATCH_SIZE = 24\nEVAL_BATCH_SIZE = 8\nLEARNING_RATE = 2e-5\nNUM_TRAIN_EPOCHS = 10\nRANDOM_SEED = 42\nGRADIENT_ACCUMULATION_STEPS = 1\nWARMUP_PROPORTION = 0.1\nOUTPUT_MODE = 'classification'\n\nCONFIG_NAME = \"config.json\"\nWEIGHTS_NAME = \"pytorch_model.bin\"","f858b292":"#Ensuring that directories exist\nif os.path.exists(REPORTS_DIR) and os.listdir(REPORTS_DIR):\n        REPORTS_DIR += f'\/report_{len(os.listdir(REPORTS_DIR))}'\n        os.makedirs(REPORTS_DIR)\nif not os.path.exists(REPORTS_DIR):\n    os.makedirs(REPORTS_DIR)\n    REPORTS_DIR += f'\/report_{len(os.listdir(REPORTS_DIR))}'\n    os.makedirs(REPORTS_DIR)","9599c028":"#Some evalutation funcitons which we can use to measure the overall accuracy of the model if we have true labels\ndef get_eval_report(task_name, labels, preds):\n    mcc = matthews_corrcoef(labels, preds)\n    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n    return {\n        \"task\": task_name,\n        \"mcc\": mcc,\n        \"tp\": tp,\n        \"tn\": tn,\n        \"fp\": fp,\n        \"fn\": fn\n    }\n\ndef compute_metrics(task_name, labels, preds):\n    assert len(preds) == len(labels)\n    return get_eval_report(task_name, labels, preds)","034a4f07":"#Using the tokenizer used in training to generate tokens\ntokenizer = BertTokenizer.from_pretrained(OUTPUT_DIR + 'vocab.txt', do_lower_case=False)","faac4b76":"#Loading the data in the correct format\nprocessor = BinaryClassificationProcessor()\neval_examples = processor.get_dev_examples(DATA_DIR)\nlabel_list = processor.get_labels() # [0, 1] for binary classification\nnum_labels = len(label_list)\neval_examples_len = len(eval_examples)","9c216d8f":"#Loading the label map of the test data\nlabel_map = {label: i for i, label in enumerate(label_list)}\neval_examples_for_processing = [(example, label_map, MAX_SEQ_LENGTH, tokenizer, OUTPUT_MODE) for example in eval_examples]","425da5b2":"#Concverting the test examples in the appropriate format\nprocess_count = cpu_count() - 1\nif __name__ ==  '__main__':\n    print(f'Preparing to convert {eval_examples_len} examples..')\n    print(f'Spawning {process_count} processes..')\n    with Pool(process_count) as p:\n        eval_features = list(tqdm_notebook(p.imap(convert_example_to_feature, eval_examples_for_processing), total=eval_examples_len))","fa196dbf":"#Converting all input data to tensor for Pytorch Purposes\nall_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\nall_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\nall_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)","ad78de02":"if OUTPUT_MODE == \"classification\":\n    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\nelif OUTPUT_MODE == \"regression\":\n    all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.float)","3336c541":"#Setting up the dataloaders\neval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n\n# Run prediction for full data\neval_sampler = SequentialSampler(eval_data)\neval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=EVAL_BATCH_SIZE)","90468330":"#Loading the pre-trained models\nmodel_pretrained = BertForSequenceClassification.from_pretrained(CACHE_DIR + BERT_MODEL, cache_dir=CACHE_DIR, num_labels=len(label_list))","5c020bd7":"#Porting the pre-trained model to GPU\nmodel_pretrained.to(device)","0ee9c65e":"#Runnning the inference\nmodel.eval()\neval_loss = 0\nnb_eval_steps = 0\npreds = []\n\nfor input_ids, input_mask, segment_ids, label_ids in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n    input_ids = input_ids.to(device)\n    input_mask = input_mask.to(device)\n    segment_ids = segment_ids.to(device)\n    label_ids = label_ids.to(device)\n\n    with torch.no_grad():\n        logits = model(input_ids, segment_ids, input_mask, labels=None)\n\n    # create eval loss and other metric required by the task\n    if OUTPUT_MODE == \"classification\":\n        loss_fct = CrossEntropyLoss()\n        tmp_eval_loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n    elif OUTPUT_MODE == \"regression\":\n        loss_fct = MSELoss()\n        tmp_eval_loss = loss_fct(logits.view(-1), label_ids.view(-1))\n\n    eval_loss += tmp_eval_loss.mean().item()\n    nb_eval_steps += 1\n    if len(preds) == 0:\n        preds.append(logits.detach().cpu().numpy())\n    else:\n        preds[0] = np.append(\n            preds[0], logits.detach().cpu().numpy(), axis=0)\n\neval_loss = eval_loss \/ nb_eval_steps\npreds = preds[0]","d3c9398d":"#Analyzing the resulting logits and calculating probabilities\nprint(preds)\nprint(len(preds))\nfrom scipy.special import softmax\npred_probs = softmax(preds, axis=1)\nprint(pred_probs)\nprint(len(pred_probs))","1bc27644":"#Converting probabilities to absolute classes\npredictions = []\nfor pred in preds:\n    if pred[0] > pred[1]:\n        predictions.append(0)\n    else:\n        predictions.append(1)\nprint(len(predictions))\nprint(predictions)","8bdc22b4":"#Storing the results in a CSV\nfinal_test_df = pd.read_csv('..\/input\/test.csv')\nfinal_test_df['sentiment'] = predictions\nfinal_test_df = final_test_df.drop(\"review\", axis=1)\nfinal_test_df.head()\nfinal_test_df.to_csv('final_result.csv', index = None, header=True)","bab90ccc":"#Checking whether the CSV is successfully created or not\nfinal_test_df.head()","ada8512b":"For this task of Binary Text Classification, I decided to try out BERT as it is believed to be a current state of the art in NLP tasks by a lot of experts.\nThe exact model used by me was bert-case-based and the learning rate used was 2e-5. I wasn't able to experiment much given the shortage of time and the unstability of Kaggle Kernels but these values also helped me achieve decent score of 0.88, that too with just 1 epoch. A Colab kernel running this code for 5 epochs runs as I am typing this and I plan to submit the resulting file thus generated as a 'late submission' to analyze the improvement in accuracy with the number of epochs.\nDue to lack of time and the unstability of the Kaggle kernels, I wasn't able to fine-tune and clean-up my code and thus there might some information missing. Irrespective of all of this, I was able to generate the output file which contains the resulting predictions for 15.0 k test samples.\n\n","41c9b2d9":"Starting evaluation"}}