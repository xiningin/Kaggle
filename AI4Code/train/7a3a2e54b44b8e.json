{"cell_type":{"57f6d1c2":"code","103eff04":"code","5e00831b":"code","77aa1f95":"code","a5fa2f55":"code","e271edb3":"code","d33d49e4":"code","d71b52b3":"code","e3c62b94":"code","b5c7e659":"code","59365ac9":"code","c2bd52ae":"code","2b89afb6":"code","2daf38a4":"code","f9b79dd9":"code","ca782cd4":"code","a4dd9301":"code","9e2bc2cb":"code","e4482f80":"markdown","a112ff36":"markdown","4318a2bb":"markdown","596b32ee":"markdown","77f67aa5":"markdown","3670189d":"markdown","ba6e572c":"markdown","dc94cfd0":"markdown"},"source":{"57f6d1c2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","103eff04":"# load the necessary packages\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis","5e00831b":"# load the train & test datasets\ninput_path = r'..\/input\/mobile-price-classification'\ndf_train, df_test = pd.read_csv(os.path.join(input_path,'train.csv')), pd.read_csv(os.path.join(input_path,'test.csv'))\ndf_train.head()","77aa1f95":"df_train.describe()","a5fa2f55":"df_test.describe()","e271edb3":"print(\"There are %d and %d entries where sc_w and px_height equal to 0, respectively in the train dataset\"\n      % (df_train[df_train.sc_w == 0].shape[0], df_train[df_train.px_height == 0].shape[0]))\n\nprint(\"There are %d and %d entries where sc_w and px_height equal to 0, respectively in the test dataset\"\n      % (df_test[df_test.sc_w == 0].shape[0], df_test[df_test.px_height == 0].shape[0]))","d33d49e4":"# Keeping the rows where both px_height & sc_w are greater than 0\ndf_test = df_test[(df_test.px_height!=0) & (df_test.sc_w!=0)]\ndf_train = df_train[(df_train.px_height!=0) & (df_train.sc_w!=0)]\ndf_train.info()","d71b52b3":"sns.heatmap(df_train.corr(), vmin=-1, vmax=1)","e3c62b94":"\n# feature modification\ndf_train['tot_res'] = np.sqrt(df_train.px_height * df_train.px_width)\ndf_train['dim'] = np.sqrt(df_train.sc_h * df_train.sc_w)\ndf_train['cam'] = np.sqrt(df_train.pc * df_train.fc)\ndf_train['network'] = (df_train.four_g*2 + df_train.three_g)\/2\n\ndf_train = df_train.drop(['four_g','three_g','pc','fc','px_height', 'px_width', 'sc_h', 'sc_w'], axis = 1)\n\n# separate the design matrix & the label \nX, Y = df_train.drop(['price_range'], axis=1), df_train[\"price_range\"]","b5c7e659":"sns.pairplot(df_train[['battery_power', \"tot_res\", \"dim\",\n                       \"cam\", \"network\", \"price_range\"]], hue = \"price_range\")","59365ac9":"sns.jointplot(x = df_train['network'], y = df_train[\"price_range\"], kind = \"kde\")","c2bd52ae":"sns.jointplot(x = df_train['battery_power'], y = df_train[\"price_range\"], kind = \"kde\")","2b89afb6":"sns.jointplot(x = df_train['ram'], y = df_train[\"price_range\"], kind = \"kde\")","2daf38a4":"sns.jointplot(x = df_train['wifi'], y = df_train[\"price_range\"], kind = \"kde\")","f9b79dd9":"acc_scores = pd.DataFrame(columns = [\"test_size\",\"log_res\",\"svm\",\"nb\"])\n\n# define the classifiers\n## a. logistic reg\nlr_model = linear_model.LogisticRegression(penalty='l1', solver='saga',\n                                            max_iter=400, random_state=0)\n    \n## b. support vector machine\nsvm_model = SVC(random_state = 0)\n    \n## c. naive bayes\nnb_model = GaussianNB()\n\n\ntest_size = [0.1,0.15,0.2,0.25,0.3]\nfor x in test_size:\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = x, random_state = 0)\n        \n    # train svm and naive bayes, and predict accuracy score on test set\n    acc_svm = svm_model.fit(X_train, Y_train).score(X_test, Y_test)\n    acc_nb = nb_model.fit(X_train, Y_train).score(X_test, Y_test)\n\n    ## scale the features to run logistic regression model\n    scaler = StandardScaler()\n\n    scaler.fit(X_train)\n    X_train = scaler.transform(X_train)\n    \n    scaler.fit(X_test)\n    X_test = scaler.transform(X_test)\n\n    acc_log = lr_model.fit(X_train, Y_train).score(X_test, Y_test)\n\n    acc_scores = acc_scores.append({\"test_size\": x,\n                                    \"log_res\": acc_log,\n                                    \"svm\": acc_svm,\n                                    \"nb\": acc_nb\n                                   }, ignore_index = True)\n    \nacc_scores","ca782cd4":"acc_rstate = pd.DataFrame(columns = [\"random_state\",\"log_res\",\"svm\",\"nb\"])\n\nr_state = [0,42,82,112,200]\n\nfor x in r_state:\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15, \n                                                        random_state = x)\n    \n    # define the classifiers\n    ## a. logistic reg\n    lr_model = linear_model.LogisticRegression(penalty='l1', solver='saga',\n                                               max_iter=400, random_state=x)\n    \n    ## b. support vector machine\n    svm_model = SVC(random_state = x)\n    \n    ## c. naive bayes\n    nb_model = GaussianNB()\n    \n    # train svm and naive bayes, and predict accuracy score on test set\n    acc_svm = svm_model.fit(X_train, Y_train).score(X_test, Y_test)\n    acc_nb = nb_model.fit(X_train, Y_train).score(X_test, Y_test)\n\n    ## scale the features to run logistic regression model\n    scaler = StandardScaler()\n\n    scaler.fit(X_train)\n    X_train = scaler.transform(X_train)\n    \n    scaler.fit(X_test)\n    X_test = scaler.transform(X_test)\n\n    acc_log = lr_model.fit(X_train, Y_train).score(X_test, Y_test)\n\n    acc_rstate = acc_rstate.append({\"random_state\": str(x),\n                                    \"log_res\": acc_log,\n                                    \"svm\": acc_svm,\n                                    \"nb\": acc_nb\n                                   }, ignore_index = True)\n    \nacc_rstate","a4dd9301":"for x in [82, 200]:\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15, \n                                                        random_state = x)\n    lr_model = linear_model.LogisticRegression(penalty='l1', solver='saga',\n                                               max_iter=400, random_state=x)\n    \n    ## scale the features to run logistic regression model\n    scaler = StandardScaler()\n\n    scaler.fit(X_train)\n    X_train = scaler.transform(X_train)\n    \n    scaler.fit(X_test)\n    X_test = scaler.transform(X_test)\n    \n    print(\"Confusion matrix with random state = %d\" %x)\n    print(confusion_matrix(Y_test, lr_model.fit(X_train, Y_train).predict(X_test)))","9e2bc2cb":"X_new = df_test.iloc[:,1:]\n\n# feature modification\nX_new['tot_res'] = np.sqrt(X_new.px_height * X_new.px_width)\nX_new['dim'] = np.sqrt(X_new.sc_h * X_new.sc_w)\nX_new['cam'] = np.sqrt(X_new.pc * X_new.fc)\nX_new['network'] = (X_new.four_g * 2 + X_new.three_g)\/2\n\nX_new = X_new.drop(['four_g','three_g','pc','fc','px_height', 'px_width', 'sc_h', 'sc_w'], axis = 1)\n\nfinal_res = pd.DataFrame()\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15, \n                                                        random_state = 82)\n\nsample_id = df_test['id'].values\n\n# define the logistic regression classifier\nlr_model = linear_model.LogisticRegression(penalty='l1', solver='saga',\n                                            max_iter=400, random_state=82)\n    \n# scale the features to run logistic regression model\nscaler = StandardScaler()\n\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\n\nscaler.fit(X_new)\nX_new = scaler.transform(X_new)\n\n# predict the target using logistic regression classifier\npred = lr_model.fit(X_train, Y_train).predict(X_new)    \n\nfinal_res['id'] = sample_id\nfinal_res[\"predicted_price_range\"] = pred\n\nfinal_res","e4482f80":"From the summary of accuracy scores, logistic regression seems to be the model of choice and a test size in the range of 15% - 25% deems ideal. However, the svm accuracy scores remain more or less consistent against the test-train splits. To probe the stability of the models further, we vary the *random state* and compare the change in accuracy scores due to the variation.   ","a112ff36":"### Data Visualization & Preprocessing","4318a2bb":"It is a problematic to see that the accuracy score varies significantly while we vary the random state and it raises question about the robustness of the models. Let us review the confusion matrices produced by logistic regression classifier with the random state set to 82 and 200","596b32ee":"## Training & Classification\nWe will use three different classifiers, (a) logistic regression, (b) support vector machine, and (c) naive bayes, and select a model based on *accuracy & robustness*. The training dataset is split into a train and a test datasets. We will split the data at different proportions and compare the model accuracy score on each of the cases.","77f67aa5":"### Objective:\nClassify a mobile into 4 price ranges based on features & specification of the gadget\n\n### Data description:  \nThe dataset has 20 features-\n* battery_power: battery capacity in mAh  \n* blue: has bluetooth or not   \n* clock_speed: processor speed   \n* dual_sim: has dual sim or not   \n* fc: front camera mega pixel   \n* four_g: 4G or not  \n* int_memory: internal memory in GB   \n* m_dep: depth in cm   \n* mobile_wt: weight   \n* n_cores: core processors   \n* pc: primary cam resolution   \n* px_height: height in pixel  \n* px_width: width in pixel   \n* ram: RAM   \n* sc_h: screen height   \n* sc_w: width  \n* talk_time: max talk time in single charge  \n* three_g: 3G or not  \n* touch_screen: touch screen or not   \n* wifi: has wifi or not   \n","3670189d":"The model with random state = 200 works poorly, especially in classifying mobiles in the price range of 1 and 2. Let us apply the logistic regression classifier to the unknown test dataset and predict the price range","ba6e572c":"Note: Both px_height & sc_w have minimum values of 0, which doesn't make sense. Hence we check the number of entries having that issue, and remove those ","dc94cfd0":"Note: Feature pairs of (px_height, px_width), (sc_h, sc_w), and (pc, fc) are highly correlated. Therefore we multiply the individual features to form the new features, tot_res (px_height * px_width), dim (sc_h * sc_w), and cam (pc * fc). Next, we notice that the pair (three_g, four_g) show correlation as well. We replace the pair by the feature, network = (2 * four_g + three_g). Its value equals 1.5 for a phone that is both 4G and 3G compatible, while only 4G or 3G produces a value of 1 and 0.5, respectively.      "}}