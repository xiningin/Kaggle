{"cell_type":{"07a3c326":"code","8228cbd2":"code","3abc5f18":"code","5749826e":"code","c7ed6a07":"code","e157ea5a":"code","e0a1eacf":"code","c6542892":"code","03b0e441":"code","af6495b7":"code","6f24de99":"code","64b14c47":"code","3ef40241":"code","e041e4e5":"code","aeee53df":"code","7d61f39b":"code","b3a9b5f4":"code","431c3d3f":"code","149e6ee3":"code","de1a6b89":"code","0a0ccd67":"code","761d49f0":"code","6c433cd5":"code","3c3aec9d":"code","c3341bfa":"code","46a44365":"code","d197b7a0":"code","c5a2e9ba":"code","c8048941":"code","9e93ef1b":"code","49ed481d":"code","094eb037":"code","12fdb5b0":"code","ca665f3a":"code","b1360f69":"code","c62e1be8":"code","724661e8":"code","8942d188":"code","7e53cff6":"code","49d384ba":"code","41411c60":"code","6dd294d2":"code","1a931e0c":"code","a182acb9":"code","77dd2211":"code","4a4eb4e9":"code","c7557521":"code","a1c64dd6":"code","02084ba0":"code","8d2093a7":"code","2ac73adb":"code","2b657926":"code","42d4b04d":"code","20960b3d":"code","bd7ffda1":"code","fc071ca6":"code","1f0ac74a":"code","125a0d69":"code","8c987794":"code","61b17b3c":"code","993ee4b8":"code","e1531345":"code","03c63f91":"code","8db8cdb7":"code","f4ab20d4":"code","8ebef281":"code","8f0a957e":"code","b7b73aa1":"code","82ba23c5":"code","7a2c0ae7":"code","bc1473ce":"code","8292d4de":"code","45987362":"code","ac3d397c":"code","db006ee5":"code","126374e8":"code","d082fecf":"code","cd2e33ac":"code","c6790b85":"code","544ef9e6":"code","ea6b9317":"code","963bde50":"code","f8b37fdc":"code","854bed6f":"code","9542f98f":"code","32553d38":"code","97664c88":"code","614cf27d":"code","024374ea":"code","107f511d":"code","a38d0edd":"code","5ffcef74":"code","cd9aef3b":"code","76431242":"code","5781e29e":"code","b326ca01":"code","57b1a7aa":"code","93dd0879":"code","3d094a3c":"code","90482ab5":"code","21807295":"code","0884fd31":"code","c82163a5":"code","8623fdbe":"code","39c7b096":"code","d9869996":"code","376e7ce3":"code","ec5acca9":"code","e305e26c":"code","b02fcd2b":"code","e075f54a":"code","7b7af891":"code","2e79e82b":"code","e57421a6":"code","080f2d27":"code","c5c96d6b":"code","be5570e4":"code","ccfbd811":"code","a8827cfb":"code","b6e8b595":"code","f6c871b5":"code","51608120":"code","190a1f45":"code","0af7f661":"code","32041062":"code","1e4998d3":"code","44e7de6f":"code","1018a502":"code","50c3b8ae":"code","d7709a4a":"code","19454428":"code","3f2cf5eb":"code","8e042866":"code","f1f36e47":"code","27a1979a":"code","31a7e681":"code","1d3e11d4":"code","9e337d22":"code","220d1cfc":"code","82ee6df6":"code","9d76dbb1":"code","ccf1b8c5":"code","cf2d4203":"code","d2278e23":"code","601101c1":"code","4dec693d":"code","89f2bc3c":"code","30f413a5":"code","ec74483e":"code","ea9a30d0":"code","22ddf4f1":"code","71cd36dd":"code","c7f9df6c":"code","7369fc85":"code","3f5708e8":"code","5730efdb":"code","dd9c33ab":"code","21a5c23e":"code","4cbefef9":"code","5ecfcd58":"code","213f52fe":"code","d8555351":"code","ed3b1016":"code","df4c5a6a":"code","2626aa81":"code","97901afd":"code","d1a36302":"code","e225958c":"code","51b89c6b":"code","ae35c9b9":"code","bbdb496f":"markdown","7294f712":"markdown","fd89e439":"markdown","45313cc0":"markdown","1028fb0e":"markdown","9204fb6f":"markdown","5a06f178":"markdown","7c6f84b7":"markdown","f6ac130f":"markdown","5ae55b98":"markdown","bd3d03ad":"markdown","c9a4c80b":"markdown","07f13bf3":"markdown","1b43dd98":"markdown","81563b43":"markdown","9586fe4b":"markdown","874cf692":"markdown","70747c0d":"markdown","a48c40c9":"markdown","5b794ff0":"markdown","caa65d7d":"markdown","cb3d40a6":"markdown","5a0e53a0":"markdown","46cfe4d7":"markdown","6d55e5d1":"markdown","01415cbe":"markdown","ee1484c6":"markdown","f2f08f9e":"markdown","4cd735d0":"markdown","2ec15403":"markdown","04ad4e89":"markdown","b66baa47":"markdown","ef8ec754":"markdown","336a7a30":"markdown","fec31892":"markdown","5a6e33f0":"markdown","88b20f61":"markdown","9c0af3fd":"markdown","cbba5204":"markdown","8b9c68f5":"markdown","ed3fcef0":"markdown","8fa8f002":"markdown","d9e9a770":"markdown","a73959f5":"markdown","bdf6eaad":"markdown","7ecdc72b":"markdown","7bed6713":"markdown","2efc826c":"markdown","b8b8fa1c":"markdown","db08a079":"markdown","ece80453":"markdown","ce27485a":"markdown","3ce72d31":"markdown","e2021962":"markdown","fd979a66":"markdown","27f227c3":"markdown","9877b157":"markdown","a2a0ed89":"markdown","46878d3d":"markdown","0d035011":"markdown","5935d4e6":"markdown","5fc61e1e":"markdown","8308d6af":"markdown","3917adaa":"markdown","2731d45c":"markdown","8c1b3d0f":"markdown","d3a7d623":"markdown","484032b5":"markdown","a920336b":"markdown","c1b818a9":"markdown","05085abd":"markdown","7efab77a":"markdown","4fee7965":"markdown","98754033":"markdown","ccd958f0":"markdown","1b3a3d4c":"markdown","1342efee":"markdown","ec29077d":"markdown","f2842653":"markdown","e376faa3":"markdown","93b68462":"markdown","d7fb340c":"markdown","5dc15008":"markdown","8ec552cf":"markdown","f3397643":"markdown","30d172e2":"markdown","287eb29a":"markdown","3b17b535":"markdown","1cce192b":"markdown","c99910d6":"markdown","855f9448":"markdown","22350065":"markdown","881322e7":"markdown","6abec905":"markdown","fbfd7731":"markdown","cded4c0b":"markdown","96d5dc0e":"markdown","cc2778be":"markdown","6f433c53":"markdown","d20b872b":"markdown","00a4ff57":"markdown","099c696d":"markdown","b36e3df8":"markdown","55e4160d":"markdown","8f72591b":"markdown","2145a41e":"markdown","b0b5237d":"markdown","d949a3e2":"markdown","175a6f32":"markdown","6f65f055":"markdown","2595bcb0":"markdown","65845464":"markdown","2e617ae6":"markdown","03636f13":"markdown","502e0519":"markdown","9355e26e":"markdown","689659d9":"markdown","490e48d1":"markdown","abe2211b":"markdown","4db8e76c":"markdown","a1609b10":"markdown","6399b806":"markdown"},"source":{"07a3c326":"!pip install pandas-profiling CurrencyConverter hyperopt xgboost textfeatures","8228cbd2":"# import common packages\nimport re\nimport sys\nimport itertools\nimport datetime\nfrom tqdm.notebook import tqdm\nimport pandas_profiling\nfrom currency_converter import CurrencyConverter\nfrom datetime import datetime\n\n# import visualization packages\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# import packages to work with the numeric, tabular data\nimport numpy as np \nimport pandas as pd \n\n# import ML packages\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, PolynomialFeatures\nfrom sklearn.feature_selection import f_regression, mutual_info_regression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV, cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor, BaggingRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor, StackingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, make_scorer\n\nfrom catboost import CatBoostRegressor\nimport xgboost as xgb\n\nfrom hyperopt import tpe, hp, fmin, STATUS_OK,Trials\nfrom hyperopt.pyll.base import scope\n\nimport nltk\nimport textfeatures\nfrom nltk.stem import PorterStemmer\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","3abc5f18":"print('Python:', sys.version.split('\\n')[0])\nprint('Numpy:', np.__version__)","5749826e":"# let's fix the package versions so that the experiments are reproducible:\n!pip freeze > requirements.txt","c7ed6a07":"# fix the RANDOM_SEED value so that the experiments are reproducible:\nRANDOM_SEED = 42","e157ea5a":"def mape(\n    y_true: np.ndarray, \n    y_pred: np.ndarray\n):\n    \"\"\"\n    Calculate the mean absolute percentage error (MAPE).\n    \n    The mean absolute percentage error (MAPE) is a measure of how accurate a forecast system is. \n    It measures this accuracy as a percentage, and can be calculated as the average absolute \n    percent error for each time period minus actual values divided by actual values.\n    \"\"\"\n    return np.mean(np.abs((y_pred-y_true)\/y_true))\n\n\ndef print_df_differences(\n    dfs_names: list, \n    dfs: list\n):\n    \"\"\"\n    \u0421ompare datasets in pairs and print unique columns for each dataset and the type of the common \n    columns, if different.\n    \n    This function takes a list of dataframe names (dfs_names) and the dataframes themselves (dfs) \n    and, by comparing them against each other, display a list of columns that are present in one dataset \n    and absent in the other. It also prints the type of columns the two datasets have in common, if they \n    are different.\n    \"\"\"\n    for df in itertools.permutations(zip(dfs_names, dfs)):\n        print(f\"\\nColumns that are present in -{df[0][0]}- dataset and missing in -{df[1][0]}- dataset:\")\n        print(set(df[0][1].columns).difference(df[1][1].columns))\n        for col in set(df[0][1].columns).intersection(df[1][1].columns):\n            if df[0][1][col].dtypes != df[1][1][col].dtypes:\n                print(f\"The common {col} column: {df[0][1][col].dtypes} in -{df[0][0]}- dataset and {df[1][1][col].dtypes} in -{df[1][0]}- dataset\")\n                \ndef convert_engineDisplacement_to_float(\n    row: str\n):\n    \"\"\"\n    Convert the engineDisplacement column values to floats.\n    \"\"\"\n    extracted_value = re.findall('\\d\\.\\d', str(row))\n    if extracted_value:\n        return float(extracted_value[0])\n    return None\n\ndef convert_owners_to_float(\n    value: str\n):\n    \"\"\"\n    Convert the owners column values to floats.\n    \"\"\"\n    if isinstance(value, str):\n        return float(value.replace('\\xa0', ' ').split()[0])\n    return value\n\ndef convert_vehicleTransmission_to_categ(\n    value: str\n):\n    \"\"\"\n    Convert the vehicleTransmission column values to 2 categories.\n    \"\"\"\n    if isinstance(value, str):\n        if value in ['MECHANICAL', '\u043c\u0435\u0445\u0430\u043d\u0438\u0447\u0435\u0441\u043a\u0430\u044f']:\n            return 'mechanical'\n        else:\n            return 'automatic'\n    return value\n\ndef convert_vehicle_licence_to_categ(\n    value: str\n):\n    \"\"\"\n    Convert the vehicle_licence column values to 2 categories.\n    \"\"\"\n    if isinstance(value, str):\n        if value in ['\u041e\u0440\u0438\u0433\u0438\u043d\u0430\u043b', 'ORIGINAL']:\n            return 'original'\n        else:\n            return 'duplicate'\n    return value\n\ndef convert_enginePower_to_float(\n    value: str\n):\n    \"\"\"\n    Convert the enginePower column values to floats.\n    \"\"\"\n    if isinstance(value, str):\n        if value == 'undefined N12':\n            return None\n        else:\n            return float(value.replace(' N12', ''))\n    return value\n\ndef fill_model_name_with_name(\n    all_model_names: list, \n    name: str\n):\n    \"\"\"\n    Fill the missing values of the model_name feature with the data from name column.\n    \"\"\"\n    name = name.lower()\n    if isinstance(name, str):\n        if \"RS Q8\".lower() in name:\n            return 'Q8'\n        elif \"QX55\".lower() in name:\n            return \"QX55\"\n        try:\n            value = name.split()\n        except:\n            return None\n        if value[1] in all_model_names:\n            return value[1]\n        try:\n            joined_value = \"_\".join([value[1], value[2]])\n        except IndexError:\n            return None\n        if joined_value in all_model_names:\n            return f\"{value[1]}_{value[2]}\"\n        elif joined_value.replace('_', ' ') in all_model_names:\n            return f\"{value[1]} {value[2]}\"\n    return None\n\ndef fill_steering_wheel(\n    brand: str, \n    model: str, \n    right_wheel_dict: dict\n):\n    \"\"\"\n    Fill the missing values of the steering_wheel feature with the data about right wheels brand's models.\n    \"\"\"\n    try:\n        if model in right_wheel_dict[brand]:\n            return 'right'\n        else:\n            return 'left'\n    except:\n        return 'left'\n    \ndef fill_driving_gear(\n    brand: str, \n    model_name: str, \n    df: pd.DataFrame\n):\n    \"\"\"\n    Fill the missing values of the driving_gear feature with the most popular driving gear in the brand's models.\n    \"\"\"\n    sliced_df = df[(df.brand == brand) & (df.model_name == model_name)]\n    try:\n        return sliced_df.driving_gear.values[0]\n    except:\n        # the most popular driving_gear in the combined dataset: combined_df.driving_gear.value_counts().index[0]\n        return '\u043f\u043e\u043b\u043d\u044b\u0439'     \n    \ndef vis_num_feature(\n    data: pd.DataFrame, \n    column: str,\n    target_column: str,\n    query_for_slicing: str\n):\n    \"\"\"\n    Show the EDA plots for numerical data.\n    \"\"\"\n    plt.style.use('seaborn-paper')\n    fig, ax = plt.subplots(2, 3, figsize=(15, 9))\n    data[column].plot.hist(ax=ax[0][0])\n    ax[0][0].set_title(column)\n    sns.boxplot(data=data, y=column, ax=ax[0][1], orient='v')\n    sns.scatterplot(data=data.query(query_for_slicing), x=column, y=target_column, ax=ax[0][2])\n    np.log2(data[column] + 1).plot.hist(ax=ax[1][0])\n    ax[1][0].set_title(f'log2 transformed {column}')\n    sns.boxplot(y=np.log2(data[column]), ax=ax[1][1], orient='v')\n    plt.show()\n    \ndef calculate_stat_outliers(\n    data_initial: pd.DataFrame,\n    column: str,\n    log: bool = False\n):\n    \"\"\"\n    Calculate the outliers for the numerical features using IQR and q1\/q3 values.\n    \"\"\"\n    data = data_initial.copy()\n    if log:\n        data[column] = np.log2(data[column] + 1)\n    q1 = data[column].quantile(0.25)\n    q3 = data[column].quantile(0.75)\n    IQR = q3 - q1\n    mask25 = q1 - IQR * 1.5                   \n    mask75 = q3 + IQR * 1.5\n\n    values = {}\n    values['borders'] = mask25, mask75\n    values['# outliers'] = data[(data[column] < mask25)].shape[0], data[data[column] > mask75].shape[0]\n\n    return pd.DataFrame.from_dict(data=values, orient='index', columns=['left', 'right'])\n\ndef show_boxplot(\n    data: pd.DataFrame, \n    column: str, \n    target_column: str\n):\n    \"\"\"\n    Show the boxplot for the numerical feature.\n    \"\"\"\n    fig, ax = plt.subplots(figsize = (14, 4))\n    sns.boxplot(x=column, y=target_column, \n                data=data.loc[data.loc[:, column].isin(data.loc[:, column].value_counts().index)],\n                ax=ax)\n    plt.xticks(rotation=45)\n    ax.set_title('Boxplot for ' + column)\n    plt.show()\n    \ndef extract_autodealer(\n    value: str\n):\n    \"\"\"\n    Extract the information from the Description column whether auto dealer published the ad.\n    \"\"\"\n    if isinstance(value, str):\n        auto_dealer = '\u0444\u0438\u043b\u0438\u0430\u043b_\u0442\u0435\u0441\u0442-\u0434\u0440\u0430\u0439\u0432_\u043c\u0435\u043d\u0435\u0434\u0436\u0435\u0440_\u043a\u043b\u0438\u0435\u043d\u0442_\u043c\u0435\u0433\u0430\u043c\u043e\u043b\u043b_\u0430\u0432\u0442\u043e\u043a\u0440\u0435\u0434\u0438\u0442_\u0430\u043e_\u043e\u0444\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0439 \u0434\u0438\u043b\u0435\u0440_\u0432\u0430\u0448 \u0432\u044b\u0431\u043e\u0440'.split('_')\n        for each in auto_dealer:\n            if each in value:\n                return 1\n        if len(value) > 500:\n            return 0\n        return 0\n    return None","e0a1eacf":"VERSION    = 4\nDIR_TRAIN  = '..\/input\/parsing-all-moscow-auto-ru-09-09-2020\/' # connected an external dataset to the laptop\nDIR_TRAIN_PARSED  = '..\/input\/final-car-price-prediction-df-parsed-sep-2021\/' # connected a parsed dataset from the page `auto.ru`\nDIR_TEST   = '..\/input\/sf-dst-car-price-prediction\/'\nVAL_SIZE= 0.20   # 20%\ncols_to_remove = []","c6542892":"!ls '..\/input'","03b0e441":"train = pd.read_csv(DIR_TRAIN + 'all_auto_ru_09_09_2020.csv') \ntest = pd.read_csv(DIR_TEST + 'test.csv')\nsample_submission = pd.read_csv(DIR_TEST + 'sample_submission.csv')","af6495b7":"train.head(2)","6f24de99":"test.head(2)","64b14c47":"train.info()","3ef40241":"test.info()","e041e4e5":"set(test.columns).difference(train.columns)","aeee53df":"train_parsed = pd.read_csv(DIR_TRAIN_PARSED + 'train_df_full_part1.csv')\ntrain_parsed.head(2)","7d61f39b":"train_parsed.info()","b3a9b5f4":"print_df_differences(dfs_names=['train', 'train_parsed', 'test'], dfs=[train, train_parsed, test])","431c3d3f":"test.vendor.unique()","149e6ee3":"vendor_dict = {k:v for v,k in test.groupby(['vendor', 'brand']).name.count().index}\nprint(vendor_dict)\ntrain_parsed['vendor'] = train_parsed.brand.map(vendor_dict)\ntrain_parsed.vendor.unique()","de1a6b89":"test.model_info[0]","0a0ccd67":"test.drop(labels='model_info', axis=1, inplace=True)\ntrain_parsed.drop(labels='model_info', axis=1, inplace=True)","761d49f0":"test['price'] = 0.0","6c433cd5":"print_df_differences(dfs_names=['train_parsed', 'test'], dfs=[train_parsed, test])","3c3aec9d":"train.brand.unique()","c3341bfa":"train = train[train.brand.isin(test.brand.unique())]","46a44365":"print_df_differences(dfs_names=['train', 'test'], dfs=[train, test])","d197b7a0":"train.rename(columns={'model': 'model_name', '\u041a\u043e\u043c\u043f\u043b\u0435\u043a\u0442\u0430\u0446\u0438\u044f': 'complectation_dict'}, inplace=True)","c5a2e9ba":"train.engineDisplacement.unique()","c8048941":"train.engineDisplacement = train.name.apply(convert_engineDisplacement_to_float)","9e93ef1b":"train.engineDisplacement.unique()","49ed481d":"train['vendor'] = train.brand.map(vendor_dict)","094eb037":"train['parsing_unixtime'] = int(datetime.strptime('09\/09\/2020', '%d\/%m\/%Y').strftime(\"%s\"))","12fdb5b0":"# let's add the column showing whether we have train or test set to have the opportunity to combine\/split them easily\ntrain['train'] = 1\ntrain_parsed['train'] = 1 \ntest['train'] = 0\n\n# this column should be added to the the train datasets because it's present in the test one and is used for the submission \ntrain['sell_id'] = 0  \ntrain_parsed['sell_id'] = 0 \n\n# create a column showing whether the data is new or old\ntrain['new_data'] = 0\ntrain_parsed['new_data'] = 1 \ntest['new_data'] = 0","ca665f3a":"combined_df = pd.concat([test, train, train_parsed.drop(labels=['views', 'date_added', 'region'], axis=1)], join='inner', ignore_index=True)\ncombined_df.head()","b1360f69":"combined_df[combined_df['\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b'].isna()].productionDate.unique()","c62e1be8":"combined_df.columns","724661e8":"# renaming\ncombined_df.rename(columns={'\u0412\u043b\u0430\u0434\u0435\u043b\u044c\u0446\u044b': 'owners', '\u0412\u043b\u0430\u0434\u0435\u043d\u0438\u0435': 'ownership', '\u041f\u0422\u0421': 'vehicle_licence',\n       '\u041f\u0440\u0438\u0432\u043e\u0434': 'driving_gear', '\u0420\u0443\u043b\u044c': 'steering_wheel', '\u0421\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435': 'condition', '\u0422\u0430\u043c\u043e\u0436\u043d\u044f': 'customs'}, inplace=True)","8942d188":"# parsing_date\ncombined_df['parsing_date'] = pd.to_datetime(combined_df.parsing_unixtime, unit='s')\ncombined_df['parsing_date'] = pd.to_datetime(combined_df.parsing_date.dt.floor('d'))","7e53cff6":"combined_df.price = combined_df.apply(lambda row: row.price if row.parsing_date.year == 2020 else row.price * 0.86, axis=1)\n# this percentage was set to 30% in the beginning, but then I checked that it gave too big difference between the MAPE values on the train and test datasets. Therefore, I have practically found this coefficient.","49d384ba":"# I also tried to calculate the mean difference in prices for different brands and apply this inflation coefficient for the price changes. It didn't play out, therefore it was removed.\n# brands = combined_df.brand.unique()\n# for brand in brands:\n#     brand_mean_price_2020 = combined_df[(combined_df.parsing_date.dt.year == 2020) & (combined_df.brand == brand)].price.mean()\n#     brand_mean_price_2021 = combined_df[(combined_df.parsing_date.dt.year == 2021) & (combined_df.brand == brand)].price.mean()\n#     inflation = (brand_mean_price_2021 - brand_mean_price_2020) \/ brand_mean_price_2020\n#     print(f\"Inflation level for the {brand} is {inflation}.\")\n#     if inflation > 0:\n#         combined_df.loc[(combined_df.parsing_date.dt.year == 2021) & (combined_df.brand == brand), 'price'] = combined_df[(combined_df.parsing_date.dt.year == 2021) & (combined_df.brand == brand)].price.apply(\n#             lambda x: int(x \/ (1 + inflation))\n#         )\n# combined_df.loc[214193, 'price']","41411c60":"# pandas_profiling.ProfileReport(combined_df)","6dd294d2":"combined_df.bodyType = combined_df.bodyType.apply(lambda x: x.lower().split()[0].strip() if isinstance(x, str) else x)","1a931e0c":"combined_df.bodyType.value_counts(normalize=True)","a182acb9":"combined_df.color.value_counts()","77dd2211":"# create a dict to convert the missing colors:\ncolor_dict = {'040001': '\u0447\u0451\u0440\u043d\u044b\u0439', 'FAFBFB': '\u0431\u0435\u043b\u044b\u0439', '97948F': '\u0441\u0435\u0440\u044b\u0439', 'CACECB': '\u0441\u0435\u0440\u0435\u0431\u0440\u0438\u0441\u0442\u044b\u0439', '0000CC': '\u0441\u0438\u043d\u0438\u0439', '200204': '\u043a\u043e\u0440\u0438\u0447\u043d\u0435\u0432\u044b\u0439',\n              'EE1D19': '\u043a\u0440\u0430\u0441\u043d\u044b\u0439',  '007F00': '\u0437\u0435\u043b\u0451\u043d\u044b\u0439', 'C49648': '\u0431\u0435\u0436\u0435\u0432\u044b\u0439', '22A0F8': '\u0433\u043e\u043b\u0443\u0431\u043e\u0439', '660099': '\u043f\u0443\u0440\u043f\u0443\u0440\u043d\u044b\u0439', 'DEA522': '\u0437\u043e\u043b\u043e\u0442\u0438\u0441\u0442\u044b\u0439', \n              '4A2197': '\u0444\u0438\u043e\u043b\u0435\u0442\u043e\u0432\u044b\u0439', 'FFD600': '\u0436\u0451\u043b\u0442\u044b\u0439', 'FF8649': '\u043e\u0440\u0430\u043d\u0436\u0435\u0432\u044b\u0439', 'FFC0CB': '\u0440\u043e\u0437\u043e\u0432\u044b\u0439'}\n","4a4eb4e9":"combined_df.color.replace(to_replace=color_dict, inplace=True)\ncombined_df.color.value_counts(normalize=True)","c7557521":"combined_df.engineDisplacement.unique()","a1c64dd6":"combined_df.engineDisplacement = combined_df.engineDisplacement.apply(lambda x: x.replace(\" LTR\", \"0.0 LTR\") if x == \" LTR\" else x)\ncombined_df.engineDisplacement = combined_df.engineDisplacement.apply(lambda x: float(x.replace(\"LTR\", \"\")) if isinstance(x, str) else x)","02084ba0":"combined_df.engineDisplacement.unique()","8d2093a7":"print(combined_df.enginePower.unique())\ncombined_df.enginePower = combined_df.enginePower.apply(convert_enginePower_to_float)\ncombined_df.enginePower.unique()","2ac73adb":"print(combined_df.owners.unique())\ncombined_df.owners = combined_df.owners.apply(convert_owners_to_float)\ncombined_df.owners.unique()","2b657926":"print(combined_df.customs.unique())\ncombined_df.customs = combined_df.customs.apply(lambda x: 1 if x == '\u0420\u0430\u0441\u0442\u0430\u043c\u043e\u0436\u0435\u043d' or x == True else 0)\nprint(combined_df.customs.unique())","42d4b04d":"combined_df.vehicleTransmission.unique()","20960b3d":"combined_df.vehicleTransmission = combined_df.vehicleTransmission.apply(convert_vehicleTransmission_to_categ)\ncombined_df.vehicleTransmission.unique()","bd7ffda1":"print(combined_df.vehicle_licence.unique())\ncombined_df.vehicle_licence = combined_df.vehicle_licence.apply(convert_vehicle_licence_to_categ)\ncombined_df.vehicle_licence.unique()","fc071ca6":"print(combined_df.steering_wheel.unique())\ncombined_df.steering_wheel = combined_df.steering_wheel.str.replace('\u041b\u0435\u0432\u044b\u0439', 'left').replace('\u041f\u0440\u0430\u0432\u044b\u0439', 'right').str.lower()\ncombined_df.steering_wheel.unique()","1f0ac74a":"cols_to_remove.extend(['ownership', 'description', 'parsing_unixtime', 'vehicleConfiguration'])","125a0d69":"print(sum(combined_df.duplicated()))\ncombined_df.shape","8c987794":"combined_df.drop_duplicates(inplace=True)\ncombined_df.shape","61b17b3c":"combined_df.isna().sum(axis=0) * 100 \/ combined_df.shape[0] ","993ee4b8":"# missing values for the train dataset\nsns.heatmap(combined_df[combined_df.train == 1].isna(), cbar=False);","e1531345":"# missing values for the test dataset\nsns.heatmap(combined_df[combined_df.train == 0].isna(), cbar=False);","03c63f91":"combined_df[combined_df.train == 1].price.isna().sum(), combined_df[combined_df.train == 0].price.isna().sum()","8db8cdb7":"combined_df.dropna(subset=['price'], inplace=True)","f4ab20d4":"combined_df.info()","8ebef281":"sns.heatmap(combined_df[combined_df.train == 1].isna(), cbar=False);","8f0a957e":"combined_df.condition.value_counts()","b7b73aa1":"combined_df.condition = combined_df.condition.apply(lambda x: 1 if x == '\u041d\u0435 \u0442\u0440\u0435\u0431\u0443\u0435\u0442 \u0440\u0435\u043c\u043e\u043d\u0442\u0430' else 0)","82ba23c5":"combined_df[combined_df.model_name.isna()].parsing_date.unique()","7a2c0ae7":"# save all available in the dataset model_names\navailable_model_names = list(combined_df.model_name.str.lower().unique())","bc1473ce":"combined_df.loc[combined_df.model_name.isna(), 'model_name'] = combined_df[combined_df.model_name.isna()].name.apply(\n     lambda x: fill_model_name_with_name(available_model_names, x))\ncombined_df.model_name = combined_df.model_name.str.lower().str.replace('_', ' ')","8292d4de":"combined_df.model_name.isna().sum()","45987362":"cols_to_remove.append('name')","ac3d397c":"combined_df[combined_df.vehicleTransmission.isna()].vehicleConfiguration","db006ee5":"combined_df[combined_df.engineDisplacement.isna()].fuelType.value_counts()","126374e8":"combined_df.loc[(combined_df.engineDisplacement.isna()) & (combined_df.fuelType == '\u044d\u043b\u0435\u043a\u0442\u0440\u043e'), 'engineDisplacement'] = 0.0","d082fecf":"combined_df.loc[(combined_df.owners.isna()) & (combined_df.mileage == 0.0), 'owners'] = 0","cd2e33ac":"print(combined_df[combined_df.owners.isnull()].groupby('productionDate').modelDate.count())\ncombined_df[combined_df.owners.isnull()].groupby('productionDate').median()","c6790b85":"combined_df.loc[combined_df.owners.isnull(), ['mileage', 'owners']] = 0","544ef9e6":"combined_df.vehicle_licence.value_counts()","ea6b9317":"combined_df[combined_df.vehicle_licence.isnull()].groupby('productionDate')[['mileage', 'owners']].median()","963bde50":"combined_df.loc[(combined_df.vehicle_licence.isna()) & (combined_df.mileage == 0.0), 'vehicle_licence'] = 'original'\ncombined_df.loc[(combined_df.vehicle_licence.isna()) & (combined_df.mileage != 0.0), 'vehicle_licence'] = 'duplicate'","f8b37fdc":"combined_df.steering_wheel.value_counts()","854bed6f":"right_wheel_models = combined_df[combined_df.steering_wheel == 'right'].groupby(['brand', 'model_name']).bodyType.count() \nright_wheel_models = right_wheel_models.reset_index().sort_values('bodyType', ascending=False)\nright_wheel_dict = right_wheel_models[right_wheel_models.bodyType > 1].groupby('brand').agg({'model_name': lambda x: x.tolist()}).to_dict()['model_name']","9542f98f":"combined_df.loc[combined_df.steering_wheel.isna(), 'steering_wheel'] = combined_df[combined_df.steering_wheel.isna()].apply(\n    lambda row: fill_steering_wheel(row.brand, row.model_name, right_wheel_dict), axis=1)","32553d38":"driving_gear_df = combined_df.groupby(['brand', 'model_name', 'driving_gear']).bodyType.count().reset_index().drop_duplicates(['brand', 'model_name']).drop(columns=['bodyType'])","97664c88":"combined_df.loc[combined_df.driving_gear.isna(), 'driving_gear'] = combined_df[combined_df.driving_gear.isna()].apply(\n    lambda row: fill_driving_gear(row.brand, row.model_name, driving_gear_df), axis=1)","614cf27d":"cols_to_remove.append('complectation_dict')","024374ea":"combined_df.numberOfDoors.value_counts()","107f511d":"combined_df[combined_df.numberOfDoors == 0]","a38d0edd":"combined_df[combined_df.train == 1].dropna(inplace=True)","5ffcef74":"combined_df.isna().sum(axis=0)","cd9aef3b":"# for col in ['enginePower', 'numberOfDoors', 'productionDate', 'owners', 'modelDate']:\n#     combined_df[col] = combined_df[col].astype(int)","76431242":"combined_df.nunique(dropna=False)","5781e29e":"combined_df.info()","b326ca01":"cat_cols = ['bodyType', 'brand', 'color', 'fuelType', 'model_name', 'name', 'driving_gear', 'owners', 'numberOfDoors']\nnum_cols = ['engineDisplacement', 'enginePower', 'mileage', 'modelDate', 'productionDate']\nbin_cols = ['condition', 'customs', 'steering_wheel', 'vehicleTransmission', 'vendor', 'vehicle_licence']\nhelp_cols = ['train', 'sell_id', 'parsing_date']\ntarget_cols = ['price']\n\nall_cols = cat_cols + num_cols + bin_cols + help_cols + target_cols\nlen(all_cols)","57b1a7aa":"sns.pairplot(combined_df[num_cols]);","93dd0879":"combined_df[num_cols].describe()","3d094a3c":"for each in num_cols:\n    display(vis_num_feature(combined_df, each, 'price', 'train == 1'))\n    display(calculate_stat_outliers(combined_df, each, log=True))\n    print('\\n' + '-' * 10 + '\\n')","90482ab5":"plt.figure(figsize=(15, 8));\nsns.heatmap(combined_df[combined_df.train == 1][num_cols + ['price']].corr(), vmin=-1, vmax=1, annot=True, cmap='vlag');","21807295":"cols_to_remove.extend(['modelDate', 'engineDisplacement'])","0884fd31":"combined_df.query('train == 1').enginePower.describe(), combined_df.query('train == 0').enginePower.describe()","c82163a5":"combined_df[(combined_df.train == 1) & (combined_df.enginePower > combined_df.query('train == 0').enginePower.max())]","8623fdbe":"combined_df.drop(\n    combined_df[(combined_df.train == 1) & (combined_df.enginePower > combined_df.query('train == 0').enginePower.max())].index, inplace=True\n)","39c7b096":"combined_df.query('train == 1').mileage.describe(), combined_df.query('train == 0').mileage.describe()","d9869996":"plt.figure(figsize=(15, 6))\nfor year in combined_df.productionDate.value_counts().index[:5]:\n    combined_df[combined_df.productionDate == year].mileage.hist(bins=50, alpha=0.5)\nplt.xlim(0, 400000)\nplt.ylim(0, 4000)","376e7ce3":"plt.figure(figsize=(15, 6))\nsns.scatterplot(data=combined_df[combined_df['train'] == 1], x='mileage', y=\"price\")","ec5acca9":"combined_df.query('train == 1').productionDate.describe(), combined_df.query('train == 0').productionDate.describe()","e305e26c":"combined_df[combined_df.productionDate < 1960]","b02fcd2b":"plt.figure(figsize=(25, 6))\nsns.scatterplot(data=combined_df[combined_df['train'] == 1], x='productionDate', y=\"price\")","e075f54a":"# combined_df.drop(labels=['modelDate', 'engineDisplacement'], axis=1, inplace=True)","7b7af891":"for col in ['modelDate', 'engineDisplacement']:\n    num_cols.remove(col)","2e79e82b":"for col in bin_cols + cat_cols:\n    if col not in ['model_name', 'name']: # these columns have too many categories, the plots don't show any useful information for them. Create new features!\n        fig, ax = plt.subplots(figsize=(15, 4), ncols=2, nrows=1)\n        ax[0].set_title(f'TRAIN: # observations in {col} column.', fontdict={'fontsize': 14})\n        combined_df[combined_df.train == 1][col].value_counts(normalize=True).plot(kind='bar', ax=ax[0])\n        ax[1].set_title(f'TEST: # observations in {col} column.', fontdict={'fontsize': 14})\n        combined_df[combined_df.train == 0][col].value_counts(normalize=True).plot(kind='bar', ax=ax[1])\n        # to visualize the boxplots for the price that has a lot of outliers we'll use 90% quantile for the price\n        show_boxplot(data=combined_df[combined_df.price <= combined_df.price.quantile(0.9)], column=col, target_column='price')\n        print('_' * 150)\n        print('\\n')\n        plt.show()","e57421a6":"cols_to_remove.extend(['condition', 'customs'])","080f2d27":"combined_df.query('train == 1').price.hist();\nplt.title('The target variable distribution', fontdict={'fontsize': 14});\nplt.xlabel('price, RUB * 10^7');","c5c96d6b":"np.log2(combined_df.query('train == 1').price).hist();\nplt.title('The log2 target variable distribution', fontdict={'fontsize': 14});","be5570e4":"combined_df['price_log2'] = np.log2(combined_df.price + 1)\n# combined_df['price_log2'].replace([np.inf, -np.inf], 0, inplace=True)","ccfbd811":"combined_df.parsing_date.unique()","a8827cfb":"combined_df.parsing_date = combined_df.parsing_date.dt.strftime(\"%Y-%m-%d\")\ncombined_df.parsing_date.unique()","b6e8b595":"c = CurrencyConverter(fallback_on_missing_rate=True)\n\nconverter_dict = {}\nfor each in combined_df.parsing_date.unique():\n    # no data from September 2021\n    year, month, day = [int(value) for value in each.split('-')]\n    try:\n        converter_dict[each] = c.convert(1, 'RUB', 'EUR', date=datetime(year, month, day))\n    except:\n        print(each)\n        \nconverter_dict['2021-09-25'] = 0.01172\nconverter_dict['2021-09-26'] = 0.01174\nconverter_dict['2021-09-27'] = 0.01177\nconverter_dict['2021-09-28'] = 0.01180\nconverter_dict['2021-09-29'] = 0.01181\nconverter_dict['2021-09-30'] = 0.01186\nconverter_dict['2021-10-01'] = 0.01180","f6c871b5":"combined_df['price_EURO'] = combined_df.apply(lambda x: x.price * converter_dict[x.parsing_date] if x.price != 0 else x.price, axis=1)","51608120":"combined_df.description.fillna('', inplace=True)","190a1f45":"def avg_word_length(value):\n    words = value.split()  \n    if words:\n        return (sum(len(word) for word in words)\/len(words))\n    else:\n        return 0","0af7f661":"# new numerical columns\ncombined_df['mileage_per_year'] = combined_df.productionDate \/ combined_df.mileage\ncombined_df['mileage_per_year'].replace([np.inf, -np.inf], 0, inplace=True)\ncombined_df['age_year'] = 2021 - combined_df.productionDate\ncombined_df['age_year'].replace([np.inf, -np.inf], 0, inplace=True)\ncombined_df['time_bw_model_product'] = combined_df.productionDate - combined_df.modelDate\ncombined_df.loc[combined_df.time_bw_model_product < 0, 'time_bw_model_product'] = 0\ntextfeatures.word_count(combined_df, 'description', 'descr_words_count')\ntextfeatures.char_count(combined_df, 'description', 'descr_len')\ntextfeatures.stopwords_count(combined_df, 'description', 'descr_stop_words_count')\ncombined_df['descr_avg_words_len'] = combined_df.description.apply(avg_word_length)\n\nnum_cols_added = ['mileage_per_year', 'age_year', 'time_bw_model_product', 'descr_words_count', 'descr_len', 'descr_avg_words_len', 'descr_stop_words_count']","32041062":"# new binary columns\ncombined_df['rarity'] = combined_df.productionDate.apply(lambda x: 1 if x < 1960 else 0)\ncombined_df['older_3y'] = combined_df.productionDate.apply(lambda x: 1 if x < 2021 - 3 else 0)\ncombined_df['older_5y'] = combined_df.productionDate.apply(lambda x: 1 if x < 2021 - 5 else 0)\n# there columns were removed because they didn't improve the metrics\n# mean_enginePower_per_model = combined_df.groupby('model_name').enginePower.mean().sort_values(ascending=False)\n# sport_models = mean_enginePower_per_model[mean_enginePower_per_model>2000].index.values # this value 2000 was found after the analysis of the hist\n# combined_df['sport'] = combined_df.model_name.apply(lambda x: 1 if x in sport_models else 0)\n# combined_df['dealer'] = combined_df.description.apply(extract_autodealer)\ncombined_df['top2_bodyType'] = combined_df.bodyType.apply(lambda x: 1 if x in ['\u0432\u043d\u0435\u0434\u043e\u0440\u043e\u0436\u043d\u0438\u043a', '\u0441\u0435\u0434\u0430\u043d'] else 0)\ncombined_df['rare_bodyType'] = combined_df.bodyType.apply(lambda x: 1 if x in ['\u043c\u0438\u043a\u0440\u043e\u0432\u044d\u043d', '\u0441\u0435\u0434\u0430\u043d-\u0445\u0430\u0440\u0434\u0442\u043e\u043f', '\u043b\u0438\u043c\u0443\u0437\u0438\u043d', '\u0442\u0430\u0440\u0433\u0430', '\u0444\u0430\u0441\u0442\u0431\u0435\u043a'] else 0)\ncombined_df['top5_colors'] = combined_df.color.apply(lambda x: 1 if x in ['\u0447\u0435\u0440\u043d\u044b\u0439', '\u0431\u0435\u043b\u044b\u0439', '\u0441\u0435\u0440\u044b\u0439', '\u0441\u0438\u043d\u0438\u0439', '\u0441\u0435\u0440\u0435\u0431\u0440\u0438\u0441\u0442\u044b\u0439'] else 0)\ncombined_df['rare_colors'] = combined_df.color.apply(lambda x: 1 if x in ['\u0444\u0438\u043e\u043b\u0435\u0442\u043e\u0432\u044b\u0439', '\u043f\u0443\u0440\u043f\u0443\u0440\u043d\u044b\u0439', '\u0437\u043e\u043b\u043e\u0442\u0438\u0441\u0442\u044b\u0439', '\u043e\u0440\u0430\u043d\u0436\u0435\u0432\u044b\u0439', '\u0436\u0451\u043b\u0442\u044b\u0439', '\u0440\u043e\u0437\u043e\u0432\u044b\u0439'] else 0)\ncombined_df['top2_door_numb'] = combined_df.numberOfDoors.apply(lambda x: 1 if x in [4, 5] else 0)\ncombined_df['longer_500_words'] = combined_df['descr_words_count'].apply(lambda x: 1 if x > 500 else 0)\n\n\nbin_cols_added = ['rarity', 'older_3y', 'older_5y', 'top2_bodyType', 'rare_bodyType', 'top5_colors', 'rare_colors', 'top2_door_numb', 'longer_500_words']","1e4998d3":"# from nltk.corpus import stopwords\n# stop_words = set(stopwords.words(\"russian\"))\n\n# def preprocess_description(x):\n#     X_proccess = []\n#     stemmer = PorterStemmer()\n#     x = x.lower()\n#     x = nltk.word_tokenize(x)\n#     x = [word for word in x if word.isalnum() and not word.isnumeric()]\n#     x = [stemmer.stem(w) for w in x]\n#     x = [word for word in x if not word in stop_words]\n#     X_proccess.append(' '.join(x))\n#     return X_proccess\n\n# combined_df['description_preproc'] = combined_df.description.apply(preprocess_description)","44e7de6f":"# combined_df['description_preproc'] = combined_df.description.apply(preprocess_description)","1018a502":"# from sklearn.feature_extraction.text import TfidfVectorizer\n\n# vectorizer = TfidfVectorizer()\n# X_train_vec = vectorizer.fit_transform(combined_df[combined_df.train == 1].description_preproc.values)","50c3b8ae":"# X_train_vec.shape","d7709a4a":"combined_df.dropna(subset=['enginePower', 'time_bw_model_product'], inplace=True)","19454428":"# after removing some observations it's better to reset the index\ncombined_df.reset_index(drop=True, inplace=True)","3f2cf5eb":"# the addition of these features haven't shown the improvements in the metrics. Therefore, it was decided to remove them.\n# pf = PolynomialFeatures(2, include_bias=False)\n# poly_data = pf.fit_transform(combined_df[num_cols+num_cols_added])[:, len(num_cols+num_cols_added):]\n# poly_cols = pf.get_feature_names()[len(num_cols+num_cols_added):]\n# poly_df = pd.DataFrame(poly_data, columns=poly_cols)\n# combined_df = combined_df.join(poly_df, how='left')","8e042866":"for each in num_cols_added:\n    display(vis_num_feature(combined_df, each, 'price', 'train == 1'))\n    display(calculate_stat_outliers(combined_df, each, log=True))\n    print('\\n' + '-' * 10 + '\\n')","f1f36e47":"combined_df['age_year_log2'] = np.log2(combined_df.age_year+1)\n# combined_df['age_year_log2'].replace([np.inf, -np.inf], 0, inplace=True)\ncombined_df['time_bw_model_product_log2'] = np.log2(combined_df.time_bw_model_product+1)\n# combined_df['descr_words_count_log2'] = np.log2(combined_df.descr_words_count+1)\n\ncombined_df['mileage_per_year_log2'] = np.log2(combined_df.mileage_per_year+1)\n# combined_df['mileage_per_year_log2'].replace([np.inf, -np.inf], 0, inplace=True)\ncombined_df['descr_words_count_log2'] = np.log2(combined_df.descr_words_count+1)\ncombined_df['descr_len_log2'] = np.log2(combined_df.descr_len+1)\ncombined_df['descr_avg_words_len_log2'] = np.log2(combined_df.descr_avg_words_len+1)\ncombined_df['descr_stop_words_count_log2'] = np.log2(combined_df.descr_stop_words_count+1)","27a1979a":"num_cols_added.remove('age_year')\nnum_cols_added.remove('mileage_per_year')\nnum_cols_added.remove('time_bw_model_product')\nnum_cols_added.remove('descr_words_count')\nnum_cols_added.remove('descr_len')\nnum_cols_added.remove('descr_avg_words_len')\nnum_cols_added.remove('descr_stop_words_count')","31a7e681":"plt.figure(figsize=(15, 8));\nsns.heatmap(combined_df[combined_df.train == 1][num_cols_added + ['time_bw_model_product_log2',\n    'age_year_log2', 'mileage_per_year_log2', 'descr_words_count_log2', \n#     'descr_len_log2', \n    'descr_avg_words_len_log2', 'descr_stop_words_count_log2',\n    'price']].corr(), vmin=-1, vmax=1, annot=True, cmap='vlag');","1d3e11d4":"num_cols_added.extend(['time_bw_model_product_log2', 'age_year_log2', 'mileage_per_year_log2', 'descr_words_count_log2', \n                       'descr_avg_words_len_log2', 'descr_stop_words_count_log2',])","9e337d22":"for col in bin_cols_added:\n    fig, ax = plt.subplots(figsize=(15, 4), ncols=2, nrows=1)\n    ax[0].set_title(f'TRAIN: # observations in {col} column.', fontdict={'fontsize': 14})\n    combined_df[combined_df.train == 1][col].value_counts(normalize=True).plot(kind='bar', ax=ax[0])\n    ax[1].set_title(f'TEST: # observations in {col} column.', fontdict={'fontsize': 14})\n    combined_df[combined_df.train == 0][col].value_counts(normalize=True).plot(kind='bar', ax=ax[1])\n    # to visualize the boxplots for the price that has a lot of outliers we'll use 90% quantile for the price\n    show_boxplot(data=combined_df[combined_df.price <= combined_df.price.quantile(0.9)], column=col, target_column='price')\n    print('_' * 150)\n    print('\\n')\n    plt.show()","220d1cfc":"combined_df.drop(cols_to_remove, axis=1, inplace=True)","82ee6df6":"num_cols.extend(num_cols_added)\nbin_cols.extend(bin_cols_added)","9d76dbb1":"num_cols, bin_cols, cat_cols","ccf1b8c5":"num_cols.remove('age_year_log2')","cf2d4203":"#let's log2-transform all other left numerical columns - in this case we get the highest correlation between added values\ncombined_df['enginePower_log2'] = np.log2(combined_df.enginePower+1)\ncombined_df['enginePower_log2'].replace([np.inf, -np.inf], 0, inplace=True)\ncombined_df['mileage_log2'] = np.log2(combined_df.mileage+1)\ncombined_df['mileage_log2'].replace([np.inf, -np.inf], 0, inplace=True)\n# combined_df['productionDate_log2'] = np.log2(combined_df.productionDate)\n# combined_df['productionDate_log2'].replace([np.inf, -np.inf], 0, inplace=True)","d2278e23":"num_cols","601101c1":"num_cols.remove('enginePower')\nnum_cols.remove('productionDate')","4dec693d":"combined_df.drop(['mileage_log2', 'enginePower', 'age_year_log2', 'productionDate'], axis=1, inplace=True)","89f2bc3c":"plt.figure(figsize=(15, 8));\nsns.heatmap(combined_df[combined_df.train == 1][num_cols + ['price']].corr(), vmin=-1, vmax=1, annot=True, cmap='vlag');","30f413a5":"combined_df.dropna(inplace=True)","ec74483e":"imp_num = pd.Series(f_regression(combined_df[combined_df.train == 1][num_cols], combined_df[combined_df.train == 1]['price'])[0], index = num_cols)\nimp_num.sort_values(inplace = True)\nimp_num.plot(kind = 'barh')\nplt.xlabel('f-statistic value');","ea9a30d0":"# labels encoding for all categorical and binary columns\n# tried to use OneHotEncoding. The results of the MAPE metrics was not improved. Decided to use the labels encoding for all categorical and binary columns\nfor colum in ['steering_wheel', 'vehicleTransmission', 'vendor', 'vehicle_licence']:\n    combined_df[colum] = combined_df[colum].astype('category').cat.codes\n\ncols_to_encode = list(set(combined_df.columns) & set(cat_cols))\nfor colum in cols_to_encode:\n    combined_df[colum] = combined_df[colum].astype('category').cat.codes","22ddf4f1":"# let's look at the importance of the categorical and binary columns\nimp_cat = pd.Series(\n    mutual_info_regression(\n        combined_df[combined_df.train == 1][list(set(combined_df.columns) & set(cat_cols+bin_cols))], \n        combined_df[combined_df.train == 1]['price'], \n        discrete_features=True), index=list(set(combined_df.columns) & set(cat_cols+bin_cols))\n)\nimp_cat.sort_values(inplace=True)\nimp_cat.plot(kind='barh', title='The importance of the categorical and binary columns.')\nplt.show()","71cd36dd":"X = combined_df.query('train == 1').drop(['price', 'train', 'parsing_date', 'price_log2', 'price_EURO', 'time_bw_model_product_log2', 'rare_bodyType', 'rarity'\n                                         ], axis=1)\nX_sub = combined_df.query('train == 0').drop(['price', 'train', 'parsing_date', 'price_log2', 'price_EURO', 'time_bw_model_product_log2', 'rare_bodyType', 'rarity'\n                                             ], axis=1)\ny = combined_df.query('train == 1').price","c7f9df6c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=VAL_SIZE, shuffle=True, random_state=RANDOM_SEED)","7369fc85":"# tmp_train = X_train.copy()\n# tmp_train['price'] = y_train","3f5708e8":"# predict = X_test['mileage'].map(tmp_train.groupby('mileage')['price'].median())\n# print(f\"The accuracy of the naive model using MAPE metrics is : {(mape(y_test, predict.values))*100:0.2f}%.\")","5730efdb":"# cat_boost = CatBoostRegressor(iterations = 5000,\n#                           random_seed = RANDOM_SEED,\n#                           eval_metric='MAPE',\n#                           custom_metric=['R2', 'MAE'],\n#                           silent=True,\n#                          )\n\n# skf = KFold(n_splits=4, random_state=RANDOM_SEED)\n# cat_boost_mape_values = []\n\n# for train_index, test_index in skf.split(X, y):\n#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n#     X_train, X_test = X[X.index.isin(train_index)], X[X.index.isin(test_index)]\n#     y_train, y_test = y[y.index.isin(train_index)], y[y.index.isin(test_index)]\n    \n#     # For training, fit() is used\n#     cat_boost.fit(X_train, y_train,\n#          #cat_features=cat_features_ids,\n#          eval_set=(X_test, y_test),\n#          verbose_eval=0,\n#          use_best_model=True,\n#          #plot=True\n#          )\n\n#     # For MAPE metric (or any other), we need the predictions of the model\n#     y_pred = cat_boost.predict(X_test)\n\n# #     print(mean_squared_error(y_test, y_pred))\n# #     print(r2_score(y_test, y_pred))\n#     cat_boost_mape_value = mape(y_test, y_pred)\n#     cat_boost_mape_values.append(cat_boost_mape_value)\n#     print(cat_boost_mape_value)\n\n# print(f\"The MAPE mertic for the default CatBoost model using 4-fold CV is: {(np.mean(cat_boost_mape_values) * 100):0.2f}%.\")","dd9c33ab":"# cat_boost_log = CatBoostRegressor(iterations = 5000,\n#                           random_seed = RANDOM_SEED,\n#                           eval_metric='MAPE',\n#                           custom_metric=['R2', 'MAE'],\n#                           silent=True,\n#                          )\n\n# skf = KFold(n_splits=4, random_state=RANDOM_SEED)\n# cat_boost_log_mape_values = []\n\n# for train_index, test_index in skf.split(X, y):\n#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n#     X_train, X_test = X[X.index.isin(train_index)], X[X.index.isin(test_index)]\n#     y_train, y_test = y[y.index.isin(train_index)], y[y.index.isin(test_index)]\n    \n#     # For training, fit() is used\n#     cat_boost_log.fit(X_train, np.log(y_train),\n#          #cat_features=cat_features_ids,\n#          eval_set=(X_test, np.log(y_test)),\n#          verbose_eval=0,\n#          use_best_model=True,\n#          #plot=True\n#          )\n\n#     # For MAPE metric (or any other), we need the predictions of the model\n#     y_pred = np.exp(cat_boost_log.predict(X_test))\n\n# #     print(mean_squared_error(y_test, y_pred))\n# #     print(r2_score(y_test, y_pred))\n#     cat_boost_log_mape_value = mape(y_test, y_pred)\n#     cat_boost_log_mape_values.append(cat_boost_log_mape_value)\n#     print(cat_boost_log_mape_value)\n\n# print(f\"The MAPE mertic for the default CatBoost model using 4-fold CV is : {(np.mean(cat_boost_log_mape_values) * 100):0.2f}%.\")","21a5c23e":"# without log-transformation of the target variable\n# rf = RandomForestRegressor(random_state=RANDOM_SEED, n_jobs=-1, verbose=1)\n# rf.fit(X_train, y_train)\n# predict_rf = rf.predict(X_test)\n\n# print(f\"The MAPE mertics of the Random Forest model using MAPE metrics: {(mape(y_test, predict_rf) * 100):0.2f}%.\")\n\n# # with log-transformation of the target variable\n# rf_log = RandomForestRegressor(random_state=RANDOM_SEED, n_jobs=-1, verbose=1)\n# rf_log.fit(X_train, np.log(y_train))\n# predict_rf_log = np.exp(rf_log.predict(X_test))\n\n# print(f\"The MAPE mertic for the Random Forest model is : {(mape(y_test, predict_rf_log) * 100):0.2f}%.\")","4cbefef9":"# model = RandomForestRegressor(random_state=RANDOM_SEED, n_jobs=-1, verbose=1)\n\n# skf = KFold(n_splits=4, random_state=RANDOM_SEED)\n# mape_values = []\n\n# for train_index, test_index in skf.split(X, y):\n#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n#     X_train, X_test = X[X.index.isin(train_index)], X[X.index.isin(test_index)]\n#     y_train, y_test = y[y.index.isin(train_index)], y[y.index.isin(test_index)]\n    \n#     # For training, fit() is used\n#     model.fit(X_train, np.log(y_train))\n\n#     # For MAPE metric (or any other), we need the predictions of the model\n#     y_pred = np.exp(model.predict(X_test))\n\n# #     print(mean_squared_error(y_test, y_pred))\n# #     print(r2_score(y_test, y_pred))\n#     mape_value = mape(y_test, y_pred)\n#     mape_values.append(mape_value)\n#     print(mape_value)\n\n# print(f\"The MAPE mertics for the default Random Forest model using 4-fold CV is : {(np.mean(mape_values) * 100):0.2f}%.\")","5ecfcd58":"# hp.uniform('n_estimators',100,500),\n# hp.choice(\"n_estimators\", [int(x) for x in np.linspace(200, 1000, num = 17)])\n\n# def objective(params):\n#     model=RandomForestRegressor(\n#         n_estimators=int(params['n_estimators']),\n#         max_depth=int(params['max_depth']),\n#         min_samples_leaf=int(params['min_samples_leaf']),\n#         min_samples_split=int(params['min_samples_split']),\n#         bootstrap=params['bootstrap'],\n#         max_features=params['max_features'],\n#         random_state=RANDOM_SEED,\n#         n_jobs=-1\n#     )\n#     model.fit(X_train, np.log(y_train))\n#     pred=model.predict(X_test)\n#     score=mape(y_test,np.exp(pred))\n#     return score\n\n# def optimize(trial):\n#     params={\n#         'n_estimators': hp.uniform('n_estimators',100,500),\n#         'max_features': hp.choice(\"max_features\", ['auto', 'sqrt']),\n#         'max_depth': hp.uniform('max_depth',5,15),\n#         'min_samples_split': hp.uniform('min_samples_split',2,10),\n#         'min_samples_leaf': hp.uniform('min_samples_leaf',1,5),\n#         'bootstrap': hp.choice(\"bootstrap\", [True, False])\n#     }\n#     best=fmin(fn=objective, space=params, algo=tpe.suggest, trials=trial, max_evals=100, rstate=np.random.RandomState(RANDOM_SEED))\n#     return best\n\n# trial=Trials()\n# best=optimize(trial)\n# print(best)","213f52fe":"# # with log-transformation of the target variable\n# rf_opt_log = RandomForestRegressor(random_state=RANDOM_SEED, \n#                                    n_jobs=-1, \n#                                    verbose=1, \n#                                    n_estimators = 250, \n#                                    min_samples_split = 8, \n#                                    min_samples_leaf = 3, \n#                                    max_features = 'auto', \n#                                    max_depth = 15, \n#                                    bootstrap = True)\n\n# rf_opt_mape_values = []\n# skf = KFold(n_splits=4, random_state=RANDOM_SEED)\n\n# for train_index, test_index in skf.split(X, y):\n#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n#     X_train, X_test = X[X.index.isin(train_index)], X[X.index.isin(test_index)]\n#     y_train, y_test = y[y.index.isin(train_index)], y[y.index.isin(test_index)]\n    \n#     # For training, fit() is used\n#     rf_opt_log.fit(X_train, np.log(y_train))\n\n#     # For MAPE metric (or any other), we need the predictions of the model\n#     y_pred = np.exp(rf_opt_log.predict(X_test))\n\n# #     print(mean_squared_error(y_test, y_pred))\n# #     print(r2_score(y_test, y_pred))\n#     rf_opt_mape_value = mape(y_test, y_pred)\n#     rf_opt_mape_values.append(rf_opt_mape_value)\n#     print(rf_opt_mape_value)\n\n# print(f\"The MAPE mertic for the optimized Random Forest model using 4-fold CV is : {(np.mean(rf_opt_mape_values) * 100):0.2f}%.\")","d8555351":"# # in the RFR there is an option to visualize the features' importance for the model \n# plt.rcParams['figure.figsize'] = (10,10)\n# feat_importances = pd.Series(rf_opt_log.feature_importances_, index=X.columns)\n# feat_importances.nlargest(15).plot(kind='barh')","ed3b1016":"# with log-transformation of the target variable\netr_log = ExtraTreesRegressor(random_state=RANDOM_SEED, n_jobs=-1, verbose=1)\n\nskf = KFold(n_splits=4, random_state=RANDOM_SEED)\netr_log_mape_values = []\n\nfor train_index, test_index in skf.split(X, y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X[X.index.isin(train_index)], X[X.index.isin(test_index)]\n    y_train, y_test = y[y.index.isin(train_index)], y[y.index.isin(test_index)]\n    \n    # For training, fit() is used\n    etr_log.fit(X_train, np.log(y_train))\n\n    # For MAPE metric (or any other), we need the predictions of the model\n    y_pred = np.exp(etr_log.predict(X_test))\n\n#     print(mean_squared_error(y_test, y_pred))\n#     print(r2_score(y_test, y_pred))\n    etr_log_mape_value = mape(y_test, y_pred)\n    etr_log_mape_values.append(etr_log_mape_value)\n    print(etr_log_mape_value)\n\nprint(f\"The MAPE mertic for the default ExtraTreesRegressor model using 4-fold CV is: {(np.mean(etr_log_mape_values) * 100):0.2f}%.\")","df4c5a6a":"# hp.uniform('n_estimators',100,500),\n# hp.choice(\"n_estimators\", [int(x) for x in np.linspace(200, 1000, num = 17)])\n\n# def objective(params):\n#     model=ExtraTreesRegressor(\n#         n_estimators=int(params['n_estimators']),\n#         max_depth=int(params['max_depth']),\n#         min_samples_leaf=int(params['min_samples_leaf']),\n#         min_samples_split=int(params['min_samples_split']),\n#         bootstrap=params['bootstrap'],\n#         max_features=params['max_features'],\n#         random_state=RANDOM_SEED,\n#         n_jobs=-1\n#     )\n#     model.fit(X_train, np.log(y_train))\n#     pred=model.predict(X_test)\n#     score=mape(y_test,np.exp(pred))\n#     return score\n\n# def optimize(trial):\n#     params={\n#         'n_estimators': hp.uniform('n_estimators',100,500),\n#         'max_features': hp.choice(\"max_features\", ['auto', 'sqrt']),\n#         'max_depth': hp.uniform('max_depth',5,15),\n#         'min_samples_split': hp.uniform('min_samples_split',2,10),\n#         'min_samples_leaf': hp.uniform('min_samples_leaf',1,5),\n#         'bootstrap': hp.choice(\"bootstrap\", [True, False])\n#     }\n#     best=fmin(fn=objective, space=params, algo=tpe.suggest, trials=trial, max_evals=100, rstate=np.random.RandomState(RANDOM_SEED))\n#     return best\n\n# trial=Trials()\n# best=optimize(trial)","2626aa81":"# # with log-transformation of the target variable and the cross validation\n# etr_log_opt = ExtraTreesRegressor(random_state=RANDOM_SEED, \n#                                    n_jobs=-1, \n#                                    verbose=1, \n#                                    n_estimators = 430, \n#                                    min_samples_split = 5, \n#                                    min_samples_leaf = 3, \n#                                    max_features = 'auto', \n#                                    max_depth = 17, \n#                                    bootstrap = False)\n\n# skf = KFold(n_splits=4, random_state=RANDOM_SEED)\n# etr_log_opt_mape_values = []\n\n# for train_index, test_index in skf.split(X, y):\n#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n#     X_train, X_test = X[X.index.isin(train_index)], X[X.index.isin(test_index)]\n#     y_train, y_test = y[y.index.isin(train_index)], y[y.index.isin(test_index)]\n    \n#     # For training, fit() is used\n#     etr_log_opt.fit(X_train, np.log(y_train))\n\n#     # For MAPE metric (or any other), we need the predictions of the model\n#     y_pred = np.exp(etr_log_opt.predict(X_test))\n\n# #     print(mean_squared_error(y_test, y_pred))\n# #     print(r2_score(y_test, y_pred))\n#     etr_log_opt_mape_value = mape(y_test, y_pred)\n#     etr_log_opt_mape_values.append(etr_log_opt_mape_value)\n#     print(etr_log_opt_mape_value)\n\n# print(f\"The MAPE mertic for the optimized ExtraTreesRegressor model using 4-fold CV is: {(np.mean(etr_log_opt_mape_values) * 100):0.2f}%.\")","97901afd":"# in the ETR there is an option to visualize the features' importance for the model \n# plt.rcParams['figure.figsize'] = (10,10)\n# feat_importances = pd.Series(etr_log_opt.feature_importances_, index=X.columns)\n# feat_importances.nlargest(15).plot(kind='barh')","d1a36302":"# xgb_log = xgb.XGBRegressor(\n#     objective='reg:squarederror', \n#     colsample_bytree=0.5,               \n#     learning_rate=0.05, \n#     max_depth=12, \n#     alpha=1,                   \n#     n_estimators=1000,\n#     random_state=RANDOM_SEED,\n#     verbose=1, \n#     n_jobs=-1,\n# )\n\n# skf = KFold(n_splits=4, random_state=RANDOM_SEED)\n# xgb_log_mape_values = []\n\n# for train_index, test_index in skf.split(X, y):\n#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n#     X_train, X_test = X[X.index.isin(train_index)], X[X.index.isin(test_index)]\n#     y_train, y_test = y[y.index.isin(train_index)], y[y.index.isin(test_index)]\n    \n#     # For training, fit() is used\n#     xgb_log.fit(X_train, np.log(y_train))\n\n#     # For MAPE metric (or any other), we need the predictions of the model\n#     y_pred = np.exp(xgb_log.predict(X_test))\n\n# #     print(mean_squared_error(y_test, y_pred))\n# #     print(r2_score(y_test, y_pred))\n#     xgb_log_mape_value = mape(y_test, y_pred)\n#     xgb_log_mape_values.append(xgb_log_mape_value)\n#     print(xgb_log_mape_value)\n\n# print(f\"The MAPE mertic for the XGBRegressor model using 4-fold CV: {(np.mean(xgb_log_mape_values) * 100):0.2f}%.\")","e225958c":"# xgb.plot_importance(xgb_log)\n# plt.show()","51b89c6b":"# estimators = [\n#     ('etr', ExtraTreesRegressor(random_state=RANDOM_SEED, n_jobs=-1, verbose=1)),\n#     ('xgb', xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.5, learning_rate=0.05, max_depth=12, alpha=1, n_jobs=-1, n_estimators=1000, random_state=RANDOM_SEED))\n# ]\n\n# sr_log = StackingRegressor(\n#     estimators=estimators,\n#     final_estimator=LinearRegression()\n# )\n\n# # For training, fit() is used\n# sr_log.fit(X_train, np.log(y_train))\n\n# # For MAPE metric (or any other), we need the predictions of the model\n# y_pred = np.exp(sr_log.predict(X_test))\n\n# print(f\"The MAPE mertic for the default StackingRegressor model: {(mape(y_test, y_pred) * 100):0.2f}%.\")","ae35c9b9":"VERSION=13\npredict_submission = np.exp(etr_log.predict(X_sub))\nsample_submission['price'] = predict_submission\nsample_submission.to_csv(f'submission_final.csv', index=False)\nsample_submission.head(10)","bbdb496f":"### - `model_name` column","7294f712":"# 3.3. Remove duplicates","fd89e439":"# 4.0. Machine learning","45313cc0":"The most import column is meleage that is quite expected.","1028fb0e":"# 2. Data upload and preliminary analysis","9204fb6f":"## SUMMARY (there are no balanced features):\n\n-  `condition` and `customs` columns: comparing the relative frequencies of unique values in the training and test datasets, only one value for each of the above columns is present in the test set.  This means that it makes sense to remove these two columns from the analysis. TODO: check later!\n\n## Heavy unbalanced features:\n- `steeting_wheel` column: the relative frequencies of unique values for this feature are similar in the train and test sets. \u201cLeft\u201d occurs about 90-95% of the time;\n- `vehicleTransmission` column: the relative frequencies of the unique values for this feature are similar in train and test sets. \"Automatic\" is found in about 80-85% of cases. Prices for automatic cars are also higher than for mechanical cars;\n- `vehicle_licence` column: here we observe unbalanced distribution, with ~85-90% \"original\" and ~10-15% \"duplicate\" values in both datasets. The distribution of prices for vehicles with an 'original' vehicle licence is higher;\n- `bodyType` column: in the train and test datasets,the two most common values are \u2018\u0432\u043d\u0435\u0434\u043e\u0440\u043e\u0436\u043d\u0438\u043a\u2019 and \u2018\u0441\u0435\u0434\u0430\u043d\u2019 (~30-50% for each). But if we look at the price distribution between different body types we see for each type quite significant differences for each type. Based on this information, we can create a new binary feature `top2_bodyType`;\n- `brand` column: here we can see the distribution of the 12 brands represented in the dataset and it looks similar for train and test datasets. The prices vary significantly but there are some trends: the iqr for \u2018Mercedes\u2019 is the largest > \u2018bmv\u2019 > \u2018audi\u2019 etc.;\n- `color` column: the distribution of this feature is similar for the train and test datasets, but we can create `top5 colours`: \u0447\u0435\u0440\u043d\u044b\u0439, \u0431\u0435\u043b\u044b\u0439, \u0441\u0435\u0440\u044b\u0439, \u0441\u0438\u043d\u0438\u0439, \u0441\u0435\u0440\u0435\u0431\u0440\u0438\u0441\u0442\u044b\u0439 - based on their frequencies and similar price distribution > let\u2019s create a new binary column;\n- `fuelType` column: similar distributions for the train and test sets. The most popular is \u2018\u0431\u0435\u043d\u0437\u0438\u043d\u2019 (~80%) but \u2018\u0434\u0438\u0437\u0435\u043b\u044c\u2019 has higher prices;\n- `numberOfDoors` column: cars with 4-5 doors dominate in both train and test datasets. Let's create a `top2_door_num`. The cheapest autos with 3 doors;\n- `model` and `model_name` columns: we haven\u2019t shown them in the graphs because there are too many categories.\n\n## Relatively balanced features:\n- `vendor` column: here we see ~60% \u2018european\u2019 and ~40% \u2018japanese\u2019 autos in both datasets. The price distribution is also similar for both vendors;\n- `driving_gear` column: almost the same distribution of \u2018\u043f\u0435\u0440\u0435\u0434\u043d\u0435-\u2018 and \u2018\u043f\u043e\u043b\u043d\u043e\u043f\u0440\u0438\u0432\u043e\u0434\u044b\u0445\u2019 in the train and test datasets. But prices are much higher for the \u2018\u043f\u043e\u043b\u043d\u043e\u043f\u0440\u0438\u0432\u043e\u0434\u044b\u0445\u2019 autos;\n- `owners` column: there are no observations in test dataset without owners (no new autos?). There is a linear correlation between the number of owners and price. For cars with 0 owners it's worth to create a new binary column: `new`.","5a06f178":"Let's fill these values with 'original' for new autos. And 'duplicate' for the autos with any # owners.","7c6f84b7":"Analyzing the data presented above, it can be seen that all three datasets differ both in the presence of the various features and in the types of data in the common featutes. For further work, all data should be converted to the same form.\n\n## 1) Comparison of train_parsed and test datasets:\nOnly two columns are missing in the train_parsed df compared to the test df:\n- `vendor`\n- `model_info`\n\n### - `vendor`\n\nSo let's check all the unique vendor values that are present in the test df.","f6ac130f":"### - `parsing_unixtime`\n\nIt's known that all data in train dataset was parsed on 09_09_2020. Let's convert it in the unixtime.","5ae55b98":"# # Model 1: A naive ML model\n![](https:\/\/i.pinimg.com\/originals\/d9\/e2\/69\/d9e269ac226808cafc6bd0b1a98c6403.jpg)  \n\nThis model will predict the average price by mileage. Other models can be compared with this naive one.","bd3d03ad":"# # Model 6 : StackingRegressor\n![](https:\/\/www.treehugger.com\/thmb\/MPyNjODcEJqfxa9B_PO4r-B41D4=\/887x665\/smart\/filters:no_upscale()\/__opt__aboutcom__coeus__resources__content_migration__mnn__images__2018__08__Stacking-rocks-beach-cairns-ca8fcb6c081d40409bf960865c2315b5.jpg)\n\nHere it's intesting to try to stack the models that have demostrated the best results: ExtraTreeRegressor and XGBRegressor with the LinearRegression meta model:\n\nIn this case the stacking of the model can also improve the metrics value and prevent the overfitting.","c9a4c80b":"The output: \n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100\/100 [21:49<00:00, 13.09s\/trial, best loss: 0.07103610618565381]\n{'bootstrap': 0, 'max_depth': 14.934436737745148, 'max_features': 0, 'min_samples_leaf': 2.884056440272862, 'min_samples_split': 7.701088338517911, 'n_estimators': 250.21497714272903}","07f13bf3":"### Since the data is merged into one dataset, one can start looking at the features individually and prepare them for the following model buildings.","1b43dd98":"Looking at the distribution we can see that `modelDate` and `productionDate` columns are highly correlated. We need to remove one of these columns in the end. In all cases we don't see a normal distribution for the values, and the distributions are either right- or left-skewed.","81563b43":"### for both train and test datasets:\n- delete 2 columns: `ownership` and `complectation_dict`;\n\n### for train dataset:\n- `price` target column: has some missing values that can only be deleted and can\u2019t be populated;\n- `model_name` and `name` columns can be merged as they seem to contain the same information;\n- `vehicleConfiguration` column: duplicates information from `vehicleTransmission`,\u00a0`engineDisplacement`,\u00a0`bodyType`. If there are missing values in these three columns, > try to fill them in and finally delete this column;\n- `mileage` column is correlated with the `modelDate` column;\n- `owners` column\n- all column containing information on `vehicle_licence`, `driving_gear`, `steering_wheel`, `conditions` and `customs` are missing for some number of the same observations;  ","9586fe4b":"Remove all left missing values in the train set.","874cf692":"### More detailed analysis of numerical features:\n\n### - `enginePower`\n\nLet's look at the distribution of the enginePower in the train and test datasets separately.","70747c0d":"The output:\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100\/100 [15:34<00:00,  9.35s\/trial, best loss: 0.07921314865515763]\n{'bootstrap': 1, 'max_depth': 14.611090900165628, 'max_features': 0, 'min_samples_leaf': 2.872679970184805, 'min_samples_split': 4.514311914203175, 'n_estimators': 430.8962602244797}","a48c40c9":"## Concatenate all dataframes","5b794ff0":"# Submission","caa65d7d":"Let's evaluate the importance of numerical values by performing univariate feature selection. Univariate feature selection works by selecting the best features based on univariate statistical tests.","cb3d40a6":"# # Model 2 : CatBoost\n![](https:\/\/pbs.twimg.com\/media\/DP-jUCyXcAArRTo.png:large)   \n\n\nAlmost all attributes in our data are categorical. Especially for work with such data was created very convenient library CatBoost from Yandex. [https:\/\/catboost.ai](http:\/\/).     \nRight now **CatBoost is one of the best libraries for tabular data!\n\n#### Useful videos about CatBoost (in Russian):\n* [CatBoost report](https:\/\/youtu.be\/9ZrfErvm97M)\n* [Fresh Tutorial from the CatBoost team (practical part)](https:\/\/youtu.be\/wQt4kgAOgV0) ","5a0e53a0":"# 3. Data preprocessing and EDA","46cfe4d7":"### - `price` column","6d55e5d1":"We can see that the highest mileage value presents in both train and test datasets. Therefore, we'll leave the data how they are without changes.\nPreviously, we observed that in the mileage distribution we observed a separate column in the hist with 0 values. Let's try to look at the distribution of mileage for different productionDates or modelDates:","01415cbe":"The top5 most important features are: `model_name`, `brand`, `color`, `owners` and `bodyType`.","ee1484c6":"The MAPE metric in the notebook is: 14.78%, on the leaderboard: 10.75%. ","f2f08f9e":"This column contains the model name which may be useful in further analysis. But this information is already included in the test dataset as well as in the train_parsed dataset as a `model_name` column. Therefore, we can delete the `model_info` column.","4cd735d0":"Add functions to help with the analysis.","2ec15403":"### - `productionDate`","04ad4e89":"They are old autos and could be that they didn't have doors. One observation is in test dataset and another one is in train. Let's leave them.","b66baa47":"### numerical columns\n- `mileage_per_year`: using the `productionDate` and `mileage` columns to extract the information how many km the auto drove per year;\n- `age_years`: how old is the auto in years;\n- `time_bw_model_product`: the difference in years between the release of the model and her production;\n- `descr_words_count`: the count of words in the description of the auto;\n\n### binary columns\n- `rarity`: whether the auto was produced before 1940;\n- `older_3y`: whether the auto is older than 3 years;\n- `older_5y`: whether the auto is older than 5 years;\n- `sport`: weather the auto is a sport car; #was removed from the analysis\n- `dealer`: whether the auto dealer published an ad; #was removed from the analysis\n- `top2_bodyType`: whether the auto has \u2018\u0432\u043d\u0435\u0434\u043e\u0440\u043e\u0436\u043d\u0438\u043a\u2019 and \u2018\u0441\u0435\u0434\u0430\u043d\u2019 body type;\n- `rate_bodyType` : whether the auto has the rare body type: '\u043c\u0438\u043a\u0440\u043e\u0432\u044d\u043d', '\u0441\u0435\u0434\u0430\u043d-\u0445\u0430\u0440\u0434\u0442\u043e\u043f', '\u043b\u0438\u043c\u0443\u0437\u0438\u043d', '\u0442\u0430\u0440\u0433\u0430', '\u0444\u0430\u0441\u0442\u0431\u0435\u043a';\n- `top5_colors`:  whether the auto has top5 colors: \u0447\u0435\u0440\u043d\u044b\u0439, \u0431\u0435\u043b\u044b\u0439, \u0441\u0435\u0440\u044b\u0439, \u0441\u0438\u043d\u0438\u0439, \u0441\u0435\u0440\u0435\u0431\u0440\u0438\u0441\u0442\u044b\u0439;\n- `rare_colors`: whether the auto has a rare color: '\u0444\u0438\u043e\u043b\u0435\u0442\u043e\u0432\u044b\u0439', '\u043f\u0443\u0440\u043f\u0443\u0440\u043d\u044b\u0439', '\u0437\u043e\u043b\u043e\u0442\u0438\u0441\u0442\u044b\u0439', '\u043e\u0440\u0430\u043d\u0436\u0435\u0432\u044b\u0439', '\u0436\u0451\u043b\u0442\u044b\u0439', '\u0440\u043e\u0437\u043e\u0432\u044b\u0439';\n- `top2_door_num`:  whether the auto has 4 or 5 doors;","ef8ec754":"### Log Traget","336a7a30":"### - `vendor`","fec31892":"There are quite significant drops in the price after 3 years and 5 years. Let's create new binary columns for these observations later.","5a6e33f0":"The statistics is similar for this feature in the train and test datasets. We have some old autos. Let's look how many.","88b20f61":"### - `driving_gear` and `steering_wheel` columns\n\ncan be filled with the most popular value between the same brand ","9c0af3fd":"# 3.5. Detailed EDA of all features, outliers' detection\n\nLet's look at the number of unique values per each column and on their data types that can help us to split the columns into categories.","cbba5204":"### - `engineDisplacement`","8b9c68f5":"### - `enginePower`, `owners`, `customs`","ed3fcef0":"### - `bodyType`","8fa8f002":"The MAPE metrics in the notebook: 16.74%, on the leaderboard: 13.059%. ","d9e9a770":"The different types of the same columns in two datasets can be explained by the fact that there are still missing values in our train_parsed dataset. Once they are processed (replaced\/removed), the same data types will be set for these columns.","a73959f5":"The MAPE metrics in the notebook: 16.48%, on the leaderboard the best submission: 9.66%. ","bdf6eaad":"In this case we see that the distribution is more normal. Therefore, it makes sense to create a more informative feature such as how many km per year an auto drove.","7ecdc72b":"### - `vehicleConfiguration` column\n\nduplicates information from vehicleTransmission,\u00a0engineDisplacement,\u00a0bodyType. If there are some missing values in these three columns > try to fill them and finally delete this column;","7bed6713":"# Summary:\n![](https:\/\/rgchannel.edu.sg\/wp-content\/uploads\/2018\/04\/OGMT8M0-1024x694.jpg)\n\n\n","2efc826c":"### - `mileage`","b8b8fa1c":"All data with missing model_name column is in the newly parsed dataset. Let's try to fill the na using the information from another `name` column.","db08a079":"It's shown that the min value is similar for the train and test datasets. Let's leave it like this then. But the max value is much higher in the train dataset. Let's look at these observations which enginePower value is higher in comparison to the test dataset.","ece80453":"## Construct final lists of all columns","ce27485a":"Now it looks like the normal distribution. Let's add this column to the dataset:","3ce72d31":"Let's build a new RFR model with the optimized hyperparameters.","e2021962":"We can observe a big descrease in price with the increase of the mileage. Maybe to use it to generate a new feature later.","fd979a66":"Here we may observe strongly correlated features:\n- `modelDate` and `productionDate` (Pearson correlation coefficient = 0.98, direct correlation) > looking at the correlation with the target variable it is worth keeping the `productionDate` (higher coefficient) and removing the `modelDate` feature;\n- `enginePower` and `engineDisplacement` (Pearson correlation coefficient = 0.80, direct correlation) > these two features also demostrate high correlation. Similar to the previous decision, let's leave the feature that has higher correlation with the target variable: `enginePower` and remove the `engineDisplacement` feature.\n- `productionDate` and `mileage` (Pearson correlation coefficient = -0.81, indirect correlation) > these two features also show a high indirect correlation. Both features are high correlated with the target variable, let's analyze them both in more details and decide whether to remove the `mileage` feature from the dataset.","27f227c3":"Let's now work with the merged dataset. Changes that could be usefully added:\n- rename column names written in Russian to English names;\n- convert the `parsing_unixtime` column to a `parsing_date` column which will be easier for us to comprehend;\n- check the dynamics of prices between different parsing dates (we only had information about the date the car was added to the site in our parsed dataframe. So using the parsing date, the exact difference in prices can't be calculate. But we can roughly estimate it using the difference between 2020 and 2021). For this let's use the Autostat information from this image.  \n![autostat](https:\/\/www.autostat.ru\/application\/includes\/blocks\/big_photo\/images\/cache\/000\/094\/996\/aba1c0d9-670-0.jpg)  \nApproximately, car prices (blue line) have increased by about 30% compared to prices in 2020. Consequently, we need to reduce prices by 30% for the newly decomposed data. We'll use this information as a first assumption but after that we'll check it practically using our model to try to find the best coefficient.","9877b157":"The distribution has heavy right tail, therefore it makes sense to try to log-transform it.","a2a0ed89":"The MAPE mertic is lower than for the default model - 17.53%. LB result: 13.60814%. The hyperparameters tuning hasn't shown the improvements in the model metrics.","46878d3d":"# 1. Install and import all necessary libraries, supplementary functions, setup","0d035011":"Let's convert this variable to the binary values and all values that contained \"\u041d\u0435 \u0442\u0440\u0435\u0431\u0443\u0435\u0442 \u0440\u0435\u043c\u043e\u043d\u0442\u0430\" values will be 1 and the observations that haven't had any information 0.","5935d4e6":"The accuracy of the naive model using MAPE metric is: 62.95%.","5fc61e1e":"Let's summarize our observations:\n\n## Summary\n\n* The dataset consists of 30 features and 214196 observations;\n* The percentage of missing values is 8.2% (will be done later);\n* There are no duplicates in the dataset;\n* Before any preprocessing there are 17 categorical columns, 8 numerical, 1 datetime and 1 boolean column. 3 columns have an unsupported data type (will be checked later);\n* There is a strong correlation between some columns: `productionDate` and `modelDate`, `vendor` and `brand`.\n\n## Features to be preprocessed\n\n- `bodyType` column: some values are written with a capital letter and some with a small letter, e.g. \u201c\u0441\u0435\u0434\u0430\u043d\u201d and \u201cC\u0435\u0434\u0430\u043d\u201d > convert to lower case and delete the information about the number of doors as it is in a separate column;\n- `color` column: some colours are written in words and some as the hex values, e.g. \u2018FAFBFB\u2019  > convert to the same form, e.g. words;\n- `complectation_dict` column: the percentage of missing values is really high (~25%), but for existing values > extract unique values and create a separate bool column for each of them; # later was decided not to do it and just drop this column;\n- `desctiption` column > try to extract some new information (number of words, length of description) and finally delete the column;\n- `engineDisplacement` column: some values contain 'LTR', some do not > remove 'LTR' and convert column from categorical to floats;\n- `enginePower`, `owners` and `customs` columns should be converted to the appropriate type;\n- strongly correlated columns: `productionDate` and `modelDate` > leave only one of them;\n- `model_name` and `name` columns can be merged as they seem to contain the same information;\n- `numberOfDoors` column: check the observations that have 0 doors.\n- `vehicleConfiguration` column: duplicates information from vehicleTransmission, engineDisplacement, bodyType. If there are missing values in these three columns > try to fill them and finally delete this column;\n- `vehicleTransmission`, `vehicle_licence`, `steering_wheel` columns: contain values in English and Russian > translate all values into English;\n- strongly correlated columns `vendor` and `brand` > keep only one of them;\n- `ownership` column: contains information about number of years and months as strings > convert to column # month int + also has too many missing values > delete column;\n- `price` target column: has several missing values that can only be deleted but cannot be filled;\n\n### Columns to delete:\n- `parsing_unixtime`;\n- `vehicleConfiguration`;\n- `description`;\n- `ownership`;\n\n# 3.2. Data cleaning","8308d6af":"On several models we see that the results are better with the log-transformed target variable. Let's use it in all other models only log-transformed.","3917adaa":"### - `numberOfDoors` column","2731d45c":"## Final decision about the model:\n\nThe best MAPE metrics on our data on the leaderboard were shown by tree-based (ExtraTreeRegressor: 9.66%) and booster (XGBoostRegressor: 10.75%) models. Stacking of these two models using a LinearRegression meta model showed also quite a high result on the submission - 10.80%. \nAs a result, a default ExtraTreeRegressor model was decided to use as a final model for the prediction. (The hyperparameters optimization hasn't gained any improvements.)","8c1b3d0f":"### - binary columns","d3a7d623":"## 2) Comparison of train and test datasets:\n\nFirst, we can check the brands that are present in the train dataset and exclude all brands that are not included in the test dataset. We have also not parsed this data from the web page.","484032b5":"Let's look further at the production_date of these autos, customs, mileage and so on.","a920336b":"## Summary:\n\n","c1b818a9":"We haven't parsed this information from the website but we can recover it manually using the car brand using dictionary mapping.","05085abd":"### polynomial features (were removed from the analysis)","7efab77a":"Finally, each auto has only two options of the transmission: mechanical or automatic. Let's convert the values into these two categories.","4fee7965":"### - `owners` column\n\nFor the columns that have missing `owners` values and `mileage` = 0 we need to fill the `owners` column with '0' because these autos are new.","98754033":"Let's check for autos with which fuel type we observe the missing values in the engineDisplacement column:","ccd958f0":"### - `priceCurrency`\n\nThis column is present in test and in train_parsed sets but it contains only the 'RUB' information therefore we could drop it in all dataframes.\n\n#### All other columns that are absent in the train dataset can't be restored. Therefore, they will be droped during the inner concatenation.","1b3a3d4c":"### - columns to delete","1342efee":"### - `vehicleTransmission`, `vehicle_licence`, `steering_wheel` columns","ec29077d":"## Work on this project was carried out in several steps:\n\n1. **Data enrichment** (parsing from `auto.ru`):\n    - collection of additional relevant data for the training dataset containing **130201** observations;\n2. **EDA**:\n    - transforming the training and test datasets into a unified form and merging them;\n    - cleaning and unification of the data;\n    - handling of missing values;\n    - handling of duplicates;\n    - analysis of relationship between features (numerical, categorical, binary) and between the features and the target;\n    - outlier analysis;\n3. **Feature engineering**:\n    - 4 numerical and 8 binary columns were added to the data and used for the final submission of the project to improve the MAPE metric. Additional columns (3 columns and polynomial features) were added during the development phase, but they did not improve the model, so they were removed for the final submission;\n4. **ML**:\n    - all binary and categorical data are encoded in the final model as labels as the hot-encoding method did not yield any improvements of the MAPE metric;\n    - standartization was not used as it yielded no improvements;\n    - 5 different ML models were tested: CatBoost, RandomForest, ExtraTrees, XGBoost and Stacking. For the RandomForest and ExtraTrees models the hyperparameters tuning was performed using hyperopt library but it didn't improve the result.\n\n## Results:\nThe best MAPE metrics in the leaderboard were shown by the tree-based (ExtraTreeRegressor: 9.66%) and the booster (XGBoostRegressor: 10.75%) models. The stacking of these two models using a LinearRegression meta model showed also quite a high result on the submission - 10.80%. As a result, I decided to use the default ExtraTreeRegressor model as a final prediction model.\n\nThe proposed solution is ranked **9th** out of 202th on the leaderboard (**MAPE = 9.66%**) (08.10.2021).\n\n## What could be improved:\n- additional feature engineering, such as the feature extraction from text (NLP), parsing of new data, etc., could help to improve the MAPE metric;\n- other methods to optimize the hyperparameters of the model;\n- to run lazypredict library and try to use other high ranked ML models;\n- analyse the results of the ML model work and try to understand why there is a difference between the MAPE metric results in the notebook and on the submission.","f2842653":"We have 41 rare autos in the dataset. We can add a separate column later: whether the auto old is or not.","e376faa3":"So the test dataset contains more columns and the train dataset is relatively small. Therefore, it's important to parse the up-to-date training dataset for this project separately from `auto.ru`. You can find more information about parsing in [a separate notebook](https:\/\/github.com\/EugeniaVoytik\/Car_price_prediction\/blob\/main\/%5BSF-DST%20Car%20Price%20Prediction%5D%20Data%20parsing.ipynb) on the GitHub.\n\n## A brief summary of what has been done in the parsing phase:\n1. In the first step, for all twelve car brands present in the test dataset all url links available on the `auto.ru` page were collected and saved to a file.\n2. Second, for all available links I extracted all information that is present in the test dataset, except for two columns: model_info and vendor, whose information was not informative. I also included several additional columns that could potentially improve the model, such as `views`, `date_added`, `region`, `price`. \n\nThe result dataset contains **34 features** and **130201** observations. \n\n## Import parsed data into a separate dataframe.","93b68462":"# - Analysis of binary and categorical columns","d7fb340c":"# # Model 3 : Random Forest\n![](https:\/\/i.guim.co.uk\/img\/media\/d143e03bccd1150ef52b8b6abd7f3e46885ea1b3\/0_182_5472_3283\/master\/5472.jpg?width=1200&quality=85&auto=format&fit=max&s=d5a74a011c3fef1ad9c1c962721d221d)","5dc15008":"As we can already see, two datasets contain different information (columns). Let's extract the columns that present only in the test set and missing in the train set. ","8ec552cf":"The MAPE metric on Kaggle is 11.863%. And that's a really big difference with the result that we get - 6.42%. There could be several explanations, e.g. overfitting.\nLet's try to use the k-fold cross-validation.","f3397643":"## Split the columns into groups based on their types","30d172e2":"Here it's clear that there is a lot of wrong information in this column instead of the correct values that one would want to retrieve. But for a train set this information can be extracted from the `name` column.","287eb29a":"There are only 3 observations here. They could be outliers therefore we could allow us to delete them.","3b17b535":"Log transformation helps in the case of `enginePower` feature, let's do it later. We see a lot of outliers for our numerical features. To make a final decision what to do with them, it would be good to analyze them in more details.","1cce192b":"### - `description` column\n\nTo improve the results of the model it's worth to work with the `description` column. ","c99910d6":"# 3.4. Analysis and filling of missing values","855f9448":"It shows that the missing values in the `price` column present only in the train test. Let's remove them.","22350065":"The list of top5 important features for the model:\n- owners\n- age_year\n- mileage\n- enginePower_log2\n- driving_gear","881322e7":"<img src=\"https:\/\/whatcar.vn\/media\/2018\/09\/car-lot-940x470.jpg\" \/>\n\n# Project: Predicting the price of a car based on its characteristics\n\n#### Project done by Eugenia Voytik, October 2021\n\nLet's imagine that there is a company that sells used cars in Moscow. The main task of the company and its managers is to find bargains as quickly as possible (to put it simply, to buy below the market and sell above the market price).\n\nThe company's management needs to create a model that will predict the value of the car based on its characteristics.\n\nIf this model works well, you will be able to quickly identify bargains (when the price desired by the seller is lower than the predicted market price). This will greatly speed up your managers' work and increase your company's profits.\n\nProblem: Historically, the company has not collected data from the beginning. There is only a small dataset with a short period of sales history, which is clearly insufficient to train the model. We will use it for the test, the rest will have to be collected independently.\n\n### Terms and Conditions:\n\n- It is allowed to use external data, but its source must be public and accessible for all contestants (e.g. `auto.ru`);\n- It is allowed to use any ML algorithms and libraries (except DL);\n- Model quality metric: MAPE - Mean Absolute Percentage Error.","6abec905":"It's clear that the log2-transformed columns have closer to the normal distribution and have higher positive or negative correlation with the target variable. Let's leave these columns log2-transformed.","fbfd7731":"# # Model 5 : XGBoostRegressor\n![](https:\/\/cdn3.vectorstock.com\/i\/1000x1000\/74\/02\/coffee-boost-energy-vector-19917402.jpg)","cded4c0b":"It looks like we have absolutely new autos here, therefore we have 0 in customs and missing owners and mileage values. Let's fill them in with 0.","96d5dc0e":"## Current Setup","cc2778be":"'descr_words_count_log2' and 'descr_len_log2' new features are highly correlated. It's worth to delete the 'descr_len_log2' feature because it has lower correlation with the target variable.","6f433c53":"Now the MAPE metric is 17.37% and on the leaderboard is 11.31%.  \nThe results on the default model are already quite nice, but let's try to optimize the hyperparameters. As a practice, we'll use the `hyperopt` advanced hyperparameter optimization method that helps to obtain the best parameters for a given model.","d20b872b":"As expected it is in almost all cases for electrical autos. Let's fill these values with 0.0.","00a4ff57":"This MAPE metric is 14.19% and on the leaderboard is 12.82%. ","099c696d":"# - Target feature ('price') analysis\n\nLet's look at the distribution of the `price` column.","b36e3df8":"Let's try with the basic RF model with default hyperparameters.","55e4160d":"### - `color`","8f72591b":"# - Analysis of numerical columns: distribution, correlation, outliers","2145a41e":"Based on the plots we could conclude that all variable can be useful for the model.","b0b5237d":"We can also try to convert the 'RUB' into \"EURO\" based on the conversion course on the parsing_date (because we don't know when the ads were added to the website).","d949a3e2":"# 3.6. Feature engineering","175a6f32":"# 3.1. Combining train and test datasets\n\nTo start data-preprocessing and EDA, I first had to combine our \"old\" and \"up-to-date\" train datasets with the test one. To do this it's worth to check the columns that are present or absent in the datasets and compare the column types.","6f65f055":"# # Model 4 : ExtraTreesRegressor\n![](https:\/\/miro.medium.com\/max\/640\/0*4VpGqWJUJnmD2mm0.jpg)","2595bcb0":"This MAPE metric is 17.88% and on the leaderboard is 13.21%. ","65845464":"### - `engineDisplacement`","2e617ae6":"We can try to extract some information from the existing columns in the train dataset to fill in the missing values of the same columns that are present in the test.\n\n### - `model_name`, - `complectation_dict`\nThe `model_name` and `complectation_dict` columns in the test set are the same as `model` and `\u041a\u043e\u043c\u043f\u043b\u0435\u043a\u0442\u0430\u0446\u0438\u044f` columns in the train set. Let's rename them.","03636f13":"In the train dataset there are more vendors that present in the test set and in our parsed dataset.","502e0519":"We can see that the range of the values of numerical columns is too different, we need to **normalize** them before the ML step. As well the observations about heavy right or left tails are confirmed with the statistics data, therefore it makes sense to try to use the log values. For this we need to check how the distribution of the log valued numerical columns will look like as well as to check the outliers.","9355e26e":"The MAPE metric in the notebook is: 15.39%, on the leaderboard: %10.80. ","689659d9":"### - `condition` column","490e48d1":"### - `model_info`","abe2211b":"It's quite interesting that XBR has different pattern in the feature importances. Here top5 features are:\n- descr_words_count\n- mileage\n- mileage_per_year\n- enginePower_log2\n- model_name","4db8e76c":"Four columns are missing in the test but present in the the parsed dataset:\n\n- `date_added `\n- `region` \n- `price` \n- `views`\n\n### - `price`\n\nIn the test dataset we don't have a price column because we need to predict it. Let's create it for now and fill with 0s.","a1609b10":"## Analysis\n\n### - numerical","6399b806":"### Train Split"}}