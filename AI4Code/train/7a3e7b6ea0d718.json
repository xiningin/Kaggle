{"cell_type":{"c0222ece":"code","fb783249":"code","fea8dce4":"code","14e9c385":"code","52da8ecd":"code","0b6b052a":"code","e089bd86":"code","0bc18de4":"code","ff9478d0":"code","0455f2d8":"code","0e0afd80":"code","f093cc17":"code","0df4bda4":"code","7e04f5fd":"code","56516e19":"code","b8774aa3":"code","5d51dc41":"code","68c89e58":"code","5d3fdc44":"code","c47b5efe":"code","62688b3e":"code","c4da1768":"code","1945546d":"code","f1d69a94":"code","206fd5d0":"code","a71456cc":"code","6c9f2317":"code","4292c915":"code","dba5e4b8":"code","d83ff0f8":"code","d4ee8500":"code","31ca4b34":"code","477c7ad2":"code","444d0a13":"code","21d2242f":"code","3f12e038":"code","d0060ee7":"code","824bdf62":"code","cb9b3c1e":"code","2bff04cf":"code","0208433b":"code","120f6485":"code","7e63712a":"code","120e43f1":"code","e0e24146":"code","fef1dca9":"code","352858cc":"code","21d4a4ee":"code","42c0475a":"code","9f54a5bf":"code","544afa84":"code","960b501b":"code","9a6fc67b":"code","c44bf77c":"code","d68597ab":"code","b328fb72":"code","152f2fc4":"code","418ff2ea":"code","4307b257":"code","e9f9822b":"code","191e2a41":"code","21841a4f":"code","d4899ebc":"code","cf5a6d3c":"markdown","5bef7560":"markdown","2d095fd8":"markdown","d1c4a139":"markdown","55e2357c":"markdown","6b960044":"markdown","97c3fd2a":"markdown","e1ced228":"markdown","52161303":"markdown","bc6211c1":"markdown","8bb9ce32":"markdown","9d16c550":"markdown","3fd6621b":"markdown"},"source":{"c0222ece":"import pandas as pd\nimport numpy as np\nfrom scipy.stats import shapiro\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.cross_validation import cross_val_score","fb783249":"#Importing the data DC_Properties to a pandas dataframe\ndc_prop=pd.read_csv('..\/input\/DC_Properties.csv', low_memory = False)\n# dc_prop.head()","fea8dce4":"#Gives the number of null values in each column\nnull_columns=dc_prop.columns[dc_prop.isnull().any()]\ndc_prop[null_columns].isnull().sum()","14e9c385":"#Getting the unknown data\nunknown_data=dc_prop[dc_prop[\"PRICE\"].isnull()]\n# unknown_data.head()","52da8ecd":"#Removing the unknown data from the original data\nfinal_dc_prop=pd.concat([dc_prop, unknown_data, unknown_data]).drop_duplicates(keep=False)\n# final_dc_prop.head()","0b6b052a":"null_columns=final_dc_prop.columns[final_dc_prop.isnull().any()]\nfinal_dc_prop[null_columns].isnull().sum()\n\n#So there are no null values of price in the final_dc_prop data","e089bd86":"sns.distplot(final_dc_prop[\"PRICE\"])\nplt.show()","0bc18de4":"sns.boxplot(final_dc_prop[\"PRICE\"])\nplt.show()","ff9478d0":"final_dc_prop[\"PRICE1\"] = final_dc_prop[\"PRICE\"]**(1.\/3)\nfinal_dc_prop = final_dc_prop[np.abs(final_dc_prop[\"PRICE1\"]-final_dc_prop[\"PRICE1\"].mean()) <= (3*final_dc_prop[\"PRICE1\"].std())]\nfinal_dc_prop.drop(columns = \"PRICE\", inplace = True)","0455f2d8":"sns.distplot(final_dc_prop[\"PRICE1\"])\nplt.show()","0e0afd80":"### Missing values in the data\nnull_columns = final_dc_prop.columns[final_dc_prop.isnull().any()]\nn = len(final_dc_prop.index)\nnull = final_dc_prop[null_columns].isnull().sum()*100\/n\n# null\nrem = []\nfor i in range(0,len(null)):\n    if null[i] > 30:\n        rem.append(null.index[i])\n# rem\n     \n#Dropping columns with missing values more than 30%","f093cc17":"final_dc_prop.drop(columns = rem, inplace = True)","0df4bda4":"final_dc_prop['AC']=final_dc_prop['AC'].astype('category')\nfinal_dc_prop['AC'].replace('0',np.nan, inplace = True)","7e04f5fd":"final_dc_prop['AYB'].describe()","56516e19":"final_dc_prop['QUADRANT'].value_counts()","b8774aa3":"final_dc_prop['AYB'].fillna((final_dc_prop['AYB'].mean()), inplace=True)\nfinal_dc_prop['QUADRANT'].fillna(\"NW\", inplace=True)","5d51dc41":"final_dc_prop[\"HEAT\"].value_counts()\nfinal_dc_prop[\"HEAT\"] = final_dc_prop[\"HEAT\"].replace(to_replace = \"No Data\", value = np.nan)\n","68c89e58":"final_dc_prop[\"ZIPCODE\"] = final_dc_prop[\"ZIPCODE\"].astype('category')","5d3fdc44":"#CREATING DUMMY VARIABLES FOR CATEGORICAL DATA IN  DATA\ndummy = pd.get_dummies(final_dc_prop.loc[:,[\"HEAT\",'AC',\"QUALIFIED\",\"SOURCE\",\"ASSESSMENT_NBHD\",\"ASSESSMENT_SUBNBHD\",\"WARD\",\"QUADRANT\", \"ZIPCODE\"]])\n","c47b5efe":"#Dropping Categories which we have replaced by dummy variables in  data\nfinal_dc_prop.drop(['Unnamed: 0','HEAT','QUALIFIED','SOURCE',\"ASSESSMENT_SUBNBHD\",'ASSESSMENT_NBHD','WARD','QUADRANT','AC','X','Y',\"LATITUDE\",\"LONGITUDE\",'SQUARE','ZIPCODE'],axis=1,inplace=True)\n# final_dc_prop.drop(['Unnamed: 0','HEAT','QUALIFIED','SOURCE','ASSESSMENT_NBHD','WARD','QUADRANT','AC'],axis=1,inplace=True)\n","62688b3e":"final_dc_prop = final_dc_prop.join(dummy)","c4da1768":"#Removing all NA values from data\nfinal_dc_prop.dropna(inplace=True)","1945546d":"#Extracting year from Saledate and storing it as a column sale_date in thedataset\ndate=final_dc_prop['SALEDATE']\ndate=date.tolist()\ntype(date)\ndate[1]\nnew_date=[]\nfor i in range(0,len(date)):\n    g=str(date[i]).split('-')\n    new_date.append(g[0])\nnew_date\nnew_date = list(map(int, new_date))\nnew_date\nsale_date = list(map(lambda x: 2018-x, new_date))\n# sale_date","f1d69a94":"se=pd.Series(sale_date)\nfinal_dc_prop['saledate']=se.values\n# train_new.head()","206fd5d0":"#AYB is the earliest time the main portion of the building was build, so we subtract it from the \n#year of the saledate in order to know that after how many years the building was sold after its\n#main portion was build\nayb=final_dc_prop['AYB']\nayb=ayb.tolist()\nayb_saledate=np.subtract(new_date,ayb)\nayb_saledate=ayb_saledate.tolist()\n\n#EYB is the year an improvement was built more recent than actual year built, so we subtract it from the \n#year of the saledate in order to know that after how many years the building was sold after its\n#first improvement was built.\neyb=final_dc_prop['EYB']\neyb=eyb.tolist()\neyb_saledate=np.subtract(new_date,eyb)\neyb_saledate=eyb_saledate.tolist()\n#eyb_saledate","a71456cc":"#Adding ayb_saledate and eyb_saledate which consists of the number of years after which the building \n#was sold after its main portion was build and the number of years after which the building was sold\n#after its first improvement was built respectively\nse=pd.Series(ayb_saledate)\nfinal_dc_prop['ayb_saledate']=se.values\n#train_new.head()\nse=pd.Series(eyb_saledate)\nfinal_dc_prop['eyb_saledate']=se.values\n# train_new.head()","6c9f2317":"#dropping some redundant columns from dataset\nfinal_dc_prop.drop(['GIS_LAST_MOD_DTTM','SALEDATE','AYB','EYB'],axis=1,inplace=True)","4292c915":"#Dividing the data final_dc_prop into trainining and test\nmsk = np.random.rand(len(final_dc_prop)) < 0.8\ntrain_new = final_dc_prop[msk]\ntest_new = final_dc_prop[~msk]","dba5e4b8":"#Separating the data into response and regressors.\nX=train_new.drop(['PRICE1'],axis=1)\nY=train_new['PRICE1']\nX=X.reset_index();\n#X.head()\nY=Y.reset_index();\n#Y.head()\nY=Y.drop('index',axis=1);\n#Y.head()\nX=X.drop('index',axis=1);\n#list(X.columns.values)","d83ff0f8":"train_new.head()","d4ee8500":"#Fitting the regular linear regression model to the training data set\nimport statsmodels.api as sm\n\nX_train_sm = X\nX_train_sm = sm.add_constant(X_train_sm)\n\nlm_sm = sm.OLS(Y,X_train_sm.astype(float)).fit()\n\n# lm_sm.params","31ca4b34":"print(lm_sm.summary())","477c7ad2":"#Separating the data into response and regressors.\nX1=test_new.drop(['PRICE1'],axis=1)\nY1=test_new['PRICE1']\nX1=X1.reset_index();\n#X.head()\nY1=Y1.reset_index();\n#Y.head()\nY1=Y1.drop('index',axis=1);\n#Y.head()\nX1=X1.drop('index',axis=1);\n#list(X.columns.values)","444d0a13":"X1_sm = X1\nX1_sm = sm.add_constant(X1_sm)\ny_pred = lm_sm.predict(X1_sm)\n# y_pred","21d2242f":"res = Y1[\"PRICE1\"] - y_pred","3f12e038":"from sklearn.metrics import mean_squared_error, r2_score\nmse = mean_squared_error(Y1[\"PRICE1\"], y_pred)\nr_squared = r2_score(y_pred,Y1[\"PRICE1\"])\nprint('Mean_Squared_Error :' ,mse)\nprint('r_square_value :',r_squared)","d0060ee7":"##Ridge Regression\nfrom sklearn.linear_model import RidgeCV\nclf = RidgeCV(alphas = np.arange(0.001,2,0.01),store_cv_values=True, cv=None)\nmodel = clf.fit(X_train_sm, Y)\nclf.score(X_train_sm,Y, sample_weight=None)","824bdf62":"model.alpha_","cb9b3c1e":"clf.score(X1_sm,Y1,sample_weight=None)\n","2bff04cf":"y_pr = clf.predict(X1_sm)\ny_pr = y_pr.tolist()","0208433b":"from itertools import chain\ny_pred = list(chain(*y_pr))\ny_pred = np.array(y_pred)","120f6485":"y_pred = pd.Series(y_pred)\nres1 = Y1[\"PRICE1\"] - y_pred","7e63712a":"from sklearn.metrics import mean_squared_error, r2_score\nmse = mean_squared_error(Y1[\"PRICE1\"], y_pred)\nr_squared = r2_score(y_pred,Y1[\"PRICE1\"])\nprint('Mean_Squared_Error :' ,mse)\nprint('r_square_value :',r_squared)","120e43f1":"sns.residplot(res1,y_pred)","e0e24146":"rf = RandomForestRegressor(n_estimators = 300, max_features=5)","fef1dca9":"rf.fit(X,np.array(Y))","352858cc":"preds = rf.predict(X1)","21d4a4ee":"mse = mean_squared_error(Y1[\"PRICE1\"], preds)\nr_squared = r2_score(preds,Y1[\"PRICE1\"])\nprint('Mean_Squared_Error :' ,mse)\nprint('r_square_value :',r_squared)","42c0475a":"xg_reg = xgb.XGBRegressor(objective ='reg:linear',max_depth = 5, min_child_weight = 1, colsample_bytree = 0.8)","9f54a5bf":"X.head()","544afa84":"xg_reg.fit(X,Y)\npreds = xg_reg.predict(X1)","960b501b":"mse = mean_squared_error(Y1[\"PRICE1\"], preds)\nr_squared = r2_score(preds,Y1[\"PRICE1\"])\nprint('Mean_Squared_Error :' ,mse)\nprint('r_square_value :',r_squared)","9a6fc67b":"#Parameter tuning in Random Forest using Grid Search \n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_features': [3, 4, 5,],\n    'n_estimators': [50, 100, 200, 300]\n}\n# Create a based model\nrf = RandomForestRegressor()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3)","c44bf77c":"# Fit the grid search to the data\ngrid_search.fit(X,Y)\ngrid_search.best_params_","d68597ab":"# Hence we get the optimal parameters as: max features as 4 and n estimators as 200 \nrf = RandomForestRegressor(n_estimators = 200, max_features=4)","b328fb72":"rf.fit(X,np.array(Y))","152f2fc4":"preds = rf.predict(X1)","418ff2ea":"mse = mean_squared_error(Y1[\"PRICE1\"], preds)\nr_squared = r2_score(preds,Y1[\"PRICE1\"])\nprint('Mean_Squared_Error :' ,mse)\nprint('r_square_value :',r_squared)","4307b257":"#The grid search gave us similar results to our original parameters. Indicating that we need to expand \n#the range of parameters in the grid and apply ranges for more parameters. ","e9f9822b":"def xgboostcv(\n              learning_rate,\n              colsample_bytree,\n              silent=True,\n              nthread=-1):\n    score= cross_val_score(xgb.XGBRegressor(objective='reg:linear',\n                                             learning_rate=learning_rate,\n                                             colsample_bytree=colsample_bytree),\n                           X,\n                           Y,\n                           cv=5).mean()\n    score=np.array(score)\n    return score","191e2a41":"xgboostBO = BayesianOptimization(xgboostcv,\n                                 {\n                                  'learning_rate': (0.01, 0.3),\n                                  'colsample_bytree' :(0.5, 0.99)\n                                  })","21841a4f":"xgboostBO.maximize(init_points=3, n_iter=10)","d4899ebc":"xgb_bayesopt.res['max']['max_params']","cf5a6d3c":"The target variable is highly skewed and has a outliers, thus transforming it and removing the outliers ","5bef7560":"## Tuning XGBoost Parameters using Bayesian Optimization","2d095fd8":"validating on the test data","d1c4a139":"**Distribution of the target variable**","55e2357c":"Spliting into training and test datasets","6b960044":"since the fit on the training data was not much accurate, we go for regularisation technique","97c3fd2a":"## Random Forest","e1ced228":"Categorical variables ","52161303":"## Tuning Random Forest Parameters using Grid Search","bc6211c1":"## XGBoost","8bb9ce32":"Droping the columns with missing values more than 30%","9d16c550":"## Ridge Regression","3fd6621b":"Fitting regression "}}