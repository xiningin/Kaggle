{"cell_type":{"48e93ef4":"code","68ff8c71":"code","a769b705":"code","1a2bb176":"code","2254877a":"code","cb8286c5":"code","e8ba325f":"code","0ceb1fd3":"code","e28e514c":"code","20d4372d":"code","9805700e":"code","d0f41cf7":"code","51afa04d":"code","f04f3749":"code","a8a16703":"code","79914d35":"code","737763d0":"code","6f2008df":"code","d96d80b5":"code","d9b0d554":"code","872fc6b4":"code","0a9fc281":"code","d29fc105":"code","93e4316f":"code","582e8529":"code","77e08d9f":"code","aba503c3":"code","d77c0db0":"markdown","db67259b":"markdown","9e2d1b1f":"markdown","b786fb1b":"markdown","627740a3":"markdown","21bb6c86":"markdown","a1ad9e7a":"markdown","d3bb9682":"markdown","05fb3cc1":"markdown","77625f51":"markdown","779e69da":"markdown","66ebf5e0":"markdown","84e09271":"markdown","d3b72935":"markdown","0ad20bc4":"markdown","0b2dd14f":"markdown","414bd8a4":"markdown","d69a9486":"markdown","cd287272":"markdown","6da9edec":"markdown","4d81b62c":"markdown","8e52f799":"markdown","582d31b1":"markdown","df061c0b":"markdown"},"source":{"48e93ef4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","68ff8c71":"dataset_train = pd.read_csv('..\/input\/teslastock\/teslastock\/tesla train.csv')","a769b705":"dataset_train.head()","1a2bb176":"dataset_train.info()","2254877a":"dataset_train.describe()","cb8286c5":"training_set = dataset_train.iloc[:, 1:2].values","e8ba325f":"training_set","0ceb1fd3":"training_set.shape","e28e514c":"from sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler()\ntraining_set_scaled = sc.fit_transform(training_set)","20d4372d":"X_train = []\ny_train = []\nfor i in range(70, 1259):\n    X_train.append(training_set_scaled[i-70:i,0])\n    y_train.append(training_set_scaled[i,0])\n#converting lists into numpy array for smooth procedures\nX_train, y_train = np.array(X_train), np.array(y_train)","9805700e":"X_train.shape","d0f41cf7":"y_train.shape","51afa04d":"X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))","f04f3749":"X_train.shape","a8a16703":"from tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout","79914d35":"regressor = Sequential()","737763d0":"regressor.add(LSTM(units=55, return_sequences=True, input_shape=(X_train.shape[1],1)))\nregressor.add(Dropout(0.2))","6f2008df":"regressor.add(LSTM(units=55, return_sequences=True))\nregressor.add(Dropout(0.2))","d96d80b5":"regressor.add(LSTM(units=55))\nregressor.add(Dropout(0.2))","d9b0d554":"regressor.add(Dense(1))","872fc6b4":"regressor.compile(optimizer='adam', loss='mean_squared_error')","0a9fc281":"regressor.fit(X_train, y_train, epochs=100, batch_size=32)","d29fc105":"dataset_test = pd.read_csv('..\/input\/teslastock\/teslastock\/tesla test.csv')\nreal_stock_price = dataset_test.iloc[:, 1:2].values","93e4316f":"dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis=0)\ninputs = dataset_total[len(dataset_total)-len(dataset_test)-70:].values\ninputs = inputs.reshape(-1,1)\ninputs = sc.transform(inputs)\n\nX_test = []\nfor i in range(70,157):\n    X_test.append(inputs[i-70:i, 0])\nX_test = np.array(X_test)\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1],1))\npredicted_stock_price = regressor.predict(X_test)\npredicted_stock_price = sc.inverse_transform(predicted_stock_price)","582e8529":"predicted_stock_price","77e08d9f":"predicted_stock_price.shape","aba503c3":"plt.plot(real_stock_price, color='red', label='Real Tesla Stock Price')\nplt.plot(predicted_stock_price, color='blue', label='Predicted Tesla Stock Price')\nplt.title(\"Tesla Stock Price Prediction\")\nplt.xlabel('time(days)')\nplt.ylabel('Tesla stock Price')\nplt.legend()","d77c0db0":"### Adding the second LSTM layer and some Dropout regularization\n* after first lstm layer we dont need to specify input_shape","db67259b":"## Feature Scaling\n*  there are mainly two ways to feature scaling\n   1. Standardisation  \n   2. Normalization \n* in our case we gonna use normalization and specifically minmax scaler and the idea behind that is since we gonna create RNN and in this network we use sigmoid activation function in output layer.so it's great to values lies between 0 to 1 and minmax scaler exactly doing same","9e2d1b1f":"## Adding the output layer\n* units: 1 since we predict only one value","b786fb1b":"## Visualizing the Result","627740a3":"# Part 1 - Data Preprocessing","21bb6c86":"### Checking for Null values","a1ad9e7a":"## Imporing the training set","d3bb9682":"## Creating a data structure with 70 timesteps and 1 output\n* we'll take 70 timestamps, first of for whats timestemp? since we gonna predict stock price(Open) so timestamp of 70 is saying that when you predict stock price at any given day at that time consider last 70 day's stock price to make prediction.\n* then we create 2 numpy array X_train and y_train where X_train is containing stock price with last 70 days stock price values and y_train containing single day stock price at given day.","05fb3cc1":"# Part 3 - Making the predictions and visualising the results","77625f51":"## Compiling the RNN","779e69da":"### Getting the predicted stock price of 2021\n### 3 Key Points\n1. we trained our model to be able to predict the stock price at time t+1 based on the 70 previous stock prices and therefore to predict each stock price of each financial day of 2021 we will need 70 previous stock prices of the 70 previous financial days, before the actual day.\n2. in order to get day of 2021 the 70 previous stock prices of the 70 previous day, so we will need both the train set and test set because we will have some of the 70 days that will be from the testing set because they will be form november 2021 and december 2021 and we will also have some stock prices of the test set because some of them will come from 2021, and therefore first thing we need to do now is now some concatenation of training set and test set to be able to get these 70 previous inputs for each day of 2021.\n3. we will have to concatenate both but we do not change the actual test values. so we have to concatenate orininal dataset.(we have to feed our model with scaling values but make prediction on actual values so we have to keep them)","66ebf5e0":"## Importing the libraries","84e09271":"### Getting the real stock price of 2021 which we gonna predict","d3b72935":"* optimizer: **adam** is always a safe choice or you can use **RMSprop** which is also a good choice for RNN\n* loss: since we have a regression problem we gonna use **mean_squared_error**","0ad20bc4":"### Checking first 5 records of training set","0b2dd14f":"### we are going to train our model on tesla stock price of year 2016-2020 and test\/predict on 2021(till 7th may)\n###    data source : [tesla](https:\/\/in.finance.yahoo.com\/quote\/TSLA?p=TSLA&.tsrc=fin-srch), Let's get started","414bd8a4":"* units : number of neurons we choose 55\n* return_sequences: choose **True** because we are gonna add another lstm layer after this one\n* input_shape: it will take 2d array(timestemps and indicator(no of predictor) and the value would be **(X_train.shape[1], 1)**\n* Dropout Layer : we give it to 0.2 means 20%, that means drop the 20 percent(11 neurons) of neurons which are least involving.\n* **NOTE**: Dropout is used to Prevent **Overfitting**","d69a9486":"## Reshaping\n* we have to convert into rnn standard format X_train into 3D tensor with shape(batch_size, timestemps, output_size)\n* let's see each perameters values for our model\n1. batch_size: No of Observation(X_train.shape[0] which is 1189)\n2. timestems: 70 as we initialized above(X_train.shape[1])\n3. input_dim: 1 since we gonna predict only one varibale google stock price","cd287272":"### Adding a first LSTM layers and some Dropout Regularization","6da9edec":"#### Now we only select second column of the dataset_train which is **Open** and convert into numpy array using values method","4d81b62c":"### Adding the third LSTM layer and some Dropout Regularization\n* no need to add return sequences in the last lstm layer","8e52f799":"### Initializing the RNN","582d31b1":"## Fitting the RNN to the Training Set","df061c0b":"# Part 2 - Building the RNN"}}