{"cell_type":{"3a108b15":"code","686c02be":"code","ccf7c3e9":"code","4452640b":"code","59f031e5":"code","28aeea6e":"code","f3138d47":"code","b607cde4":"code","d51fc4c5":"code","f77002ce":"code","915769bb":"code","2a935097":"code","bab369fe":"code","75bd34c6":"code","83b87599":"code","2ef315ed":"code","11c07bf3":"code","0e9ef388":"code","0e977a12":"code","63f1d7b9":"code","f64c3774":"code","4f0a3634":"code","f0b6e2ea":"code","e5d4a24e":"code","6acbfc98":"code","0b50b319":"code","99904d41":"code","1a7b7339":"code","36388882":"code","c9370c48":"code","7c2bf646":"code","3c16ec07":"code","deceab92":"code","475d56d6":"code","6a87ab4b":"code","c1dc8f9c":"code","af04ac43":"code","2c1c9706":"code","a573e78d":"code","fcdc6eaf":"code","0c5abec9":"code","f4cb0238":"code","78cb9b13":"code","1990a1f8":"code","c7e4358a":"code","2f7e2145":"code","e87a973b":"code","f78ed233":"code","b171592f":"code","9adc82aa":"code","ed9e0812":"code","e4d32b62":"code","352d559a":"code","6d4496a0":"code","cc3b00f8":"code","5617f2a9":"code","1c7cb91a":"code","1e411be9":"code","9107ac06":"code","0045d929":"code","def331db":"code","449173fb":"code","c90ce295":"code","cb632172":"code","bd71a860":"code","3a2c0dbb":"code","995d605d":"code","47ec708e":"code","69992bdb":"code","c4c83c4b":"code","2f284098":"code","f48e9d72":"code","6a4dcbdb":"code","85af2d5a":"code","17941eca":"code","8c8bf4e1":"code","a31093ae":"code","80baeef0":"code","2f8dc984":"code","a837bf0b":"code","45eb7f99":"code","b264279c":"code","79ac0bb4":"code","c42dd33f":"code","b8184f9a":"code","8b80f9f3":"code","97b80e6e":"code","e5e9dc8e":"code","9bd85cd3":"code","fd40bbb9":"code","eede9707":"code","285cd2c5":"code","9800e37c":"code","007ef6b1":"code","268bc86e":"code","852d6d83":"code","1eeeee04":"code","c3863dad":"code","9269cbcf":"code","fb145ffa":"code","9628e5cf":"code","d2392e53":"code","e379ca34":"code","7c59342c":"code","e339f5e2":"code","ccbf150a":"code","baf1107d":"code","c8c6779d":"code","b12175a0":"code","9a0b2152":"code","3d431163":"code","1adbb3b1":"code","c42d9d02":"code","c2f49de7":"code","3e663966":"code","74f7c40b":"code","1d26445f":"code","b0b29f68":"code","e1eed5d6":"code","b9baf2bb":"code","cd9886f1":"code","8e525478":"code","92918074":"code","072c6cf5":"code","522803f0":"markdown","f5f95229":"markdown","fbfa1b76":"markdown","a791efa5":"markdown","e0cd9c0d":"markdown","a3c16e6a":"markdown","bb50f83f":"markdown","dee9200b":"markdown","0ff6bf81":"markdown","00bf5bb2":"markdown","e65f8630":"markdown","78957b7f":"markdown","3a78140e":"markdown","69b22fba":"markdown","c53ae938":"markdown","ffddaa27":"markdown","b65f3cb3":"markdown","ee2562a1":"markdown","c4d39df5":"markdown","410cc24c":"markdown","dae9a018":"markdown","60b895fd":"markdown","a6ef8123":"markdown","6a68da9f":"markdown","c4df6561":"markdown","5be3beed":"markdown","7fa33305":"markdown","64aaacee":"markdown","42d4f055":"markdown","85833076":"markdown"},"source":{"3a108b15":"#Python libraries\n#Classic,data manipulation \nimport numpy as np\nimport pandas as pd\n# Plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option(\"display.max_rows\",None)\nfrom sklearn import preprocessing \nfrom sklearn.metrics import precision_score, recall_score, precision_recall_curve,f1_score, fbeta_score,accuracy_score\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import StackingClassifier\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler \nfrom sklearn.metrics import fbeta_score, make_scorer\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import plot_roc_curve\nfrom xgboost import XGBClassifier\nfrom xgboost import XGBRegressor\n# Dataprep\n# Modeling \nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import cross_val_score \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nfrom imblearn.combine import SMOTEENN #resampling\nfrom sklearn import datasets, linear_model, metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n#Import pipeline to allow you to do multiple steps at once\nfrom imblearn.pipeline import Pipeline, make_pipeline\n","686c02be":"df= pd.read_csv('..\/input\/diabetes-health-indicators-dataset\/diabetes_012_health_indicators_BRFSS2015.csv')\n","ccf7c3e9":"df.info()","4452640b":"df.shape","59f031e5":"df.sample(100)","28aeea6e":"df.columns","f3138d47":"# to rename the some columns important of dataset\ndf.rename(columns={'Diabetes_012': 'Diabetes_Type'}, inplace=True)\ndf.head()","b607cde4":"df.isna().sum() # No missing value","d51fc4c5":"df.BMI.unique()","f77002ce":"df.GenHlth.unique()","915769bb":"df.MentHlth.unique()","2a935097":"df.PhysHlth.unique()","bab369fe":"df.Age.unique()","75bd34c6":"df.Education.unique()","83b87599":"df.Income.unique()","2ef315ed":"df.Diabetes_Type.unique()","11c07bf3":"Diabetes=df['Diabetes_Type']\nDiabetes.value_counts()","0e9ef388":"df['Diabetes_Type'].replace({2.0: 1.0},inplace = True)","0e977a12":"Diabetes=df['Diabetes_Type']\nDiabetes.value_counts()","63f1d7b9":"corr = df.corr()\nfig, ax = plt.subplots(figsize=(25,15)) \nsns.heatmap(corr,annot=True, cmap = \"Blues\", linewidth = 0.30)\nplt.title(\"Correlation matrix of features\")\nplt.show()","f64c3774":"df['Diabetes_Type'] = df['Diabetes_Type'].astype('int')","4f0a3634":"df['Diabetes']=df['Diabetes_Type']","f0b6e2ea":"df['Diabetes'] = df['Diabetes_Type'].map({0:'No Diabetes', 1:'Diabetes'})","e5d4a24e":"Diabetes=df['Diabetes_Type']\nDiabetes.value_counts()","6acbfc98":"diabetes_bp = df.groupby(['Diabetes_Type', 'HighBP']).size().reset_index(name = 'Count')\nprint(diabetes_bp)","0b50b319":"df.columns\n","99904d41":"df['GH']=df['GenHlth']","1a7b7339":"df['GH'] = df['GH'].map({1:5, 2:4 ,3:3 ,4:2 , 5:1})","36388882":"df.head()","c9370c48":"df.isna().sum()","7c2bf646":"smaller_df=df.loc[:,['Diabetes_Type', 'HighBP', 'HighChol', 'BMI', 'HeartDiseaseorAttack','PhysActivity', 'GenHlth','MentHlth','DiffWalk', 'Age']]\nsmaller_df1=df.loc[:,['Diabetes_Type', 'HighBP', 'HighChol', 'BMI', 'HeartDiseaseorAttack','PhysActivity', 'GenHlth','MentHlth','DiffWalk', 'Age']]                  \n                \n","3c16ec07":"df_train, df_test = train_test_split(smaller_df, test_size=0.20, random_state=0)\ndf_train, df_val = train_test_split(df_train, test_size=0.20, random_state=0)\n\nx,y = df_train.drop(['Diabetes_Type'],axis=1),df_train['Diabetes_Type']\n","deceab92":"# The figure above display the correlation between the features and the target, for this i choose only these features\nx_train1,y_train = df_train.drop(['Diabetes_Type'],axis=1),df_train['Diabetes_Type']\nx_val1,y_val = df_val.drop(['Diabetes_Type'],axis=1),df_val['Diabetes_Type']\nx_test1,y_test= df_test.drop(['Diabetes_Type'],axis=1),df_test['Diabetes_Type']","475d56d6":"print(x_train1.shape)\nprint(x_val1.shape)\nprint(x_test1.shape)","6a87ab4b":"#For smaller df1\nscaler = MinMaxScaler()\nscaler.fit(x_train1)\n\nx_train = scaler.transform(x_train1)\nx_test= scaler.transform(x_test1)\nx_val=scaler.transform(x_val1)","c1dc8f9c":"# create a dict to store the scores of each model\nmodels_evalutions = {'Model':[],\n                     'Accuracy':[],\n                     'Precision':[],\n                     'Recall':[], \n                     'F1 score':[]}","af04ac43":"#model_names = [\"knn_final\", \"lr_final\",\"Dt_final\",\"rf_final\"]\n#model_vars = [eval(n) for n in model_names]\n#model_list = list(zip(model_names, model_vars))","2c1c9706":"x0_train=x_train.copy()\nx0_val=x_val.copy()","a573e78d":"# Using KNN (smaller df1) train on training set, and Test on testing set \nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(x0_train, y_train)\nprint(\"The score for kNN without cross val and without using smote is\")\nprint(\"Training set: {:6.2f}%\".format(100*knn.score(x0_train, y_train)))\nprint(\"Validation set: {:6.2f}%\".format(100*knn.score(x0_val, y_val)))\nprint(\"Test set: {:6.2f}%\".format(100*knn.score(x_test, y_test)))","fcdc6eaf":"#test the baseline model for smaller df\n#prediction\nval_pred=knn.predict(x0_val)\n#Accuracy\nconfusion_hard = confusion_matrix( y_val, val_pred)\naccuracy = accuracy_score(y_val , val_pred)\nprecision = precision_score(y_val , val_pred)\nrecall = recall_score(y_val , val_pred)\nf1 = f1_score(y_val,val_pred) \nprint('\\nKNN Accuracy for validation set=: {0:.4f}, \\nprecision: {1:.4f}, \\nrecall: {2:.4f},\\\n\\nF1: {3:.4f}'.format(accuracy, precision, recall, f1))","0c5abec9":"cm = confusion_matrix(y_val, val_pred)\nclass_label = [\"No_Diabetes\", \"Diabetes\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\",  cmap = \"Blues\" )\nplt.title('Confusion matrix for knn befoes balancing two classes and using cross validation', fontsize = 20); # title with fontsize 20","f4cb0238":"kf = KFold(n_splits=5, random_state=42, shuffle=True)","78cb9b13":"knn = KNeighborsClassifier(n_neighbors=5)\naccuracy_score1=[]\nf1_score1 = []\npercision_score1 = []\nrecall_score1 = []\n\n# enumerate the splits and summarize the distributions\nfor train_ix, test_ix in kf.split(x, y):\n    # select rows\n    train_x, test_X = x.iloc[train_ix], x.iloc[test_ix]\n    train_y, test_y = y.iloc[train_ix], y.iloc[test_ix]\n    #print(train_X.shape, train_y.shape)\n    #print(test_X.shape, test_y.shape)\n    oversample = SMOTE(random_state = 0)\n    train_x, train_y = oversample.fit_resample(train_x, train_y)\n    #scores= cross_val_score(knn, test_X, test_y, cv=5, scoring='accuracy') \n    knn.fit(train_x, train_y)\n    y_pred =knn.predict(test_X)\n    #score = f1_score(test_y,y_pred)\n    accuracy_score1.append(metrics.accuracy_score(test_y, y_pred))\n    percision_score1.append(metrics.precision_score(test_y, y_pred))\n    recall_score1.append(metrics.recall_score(test_y, y_pred))\n    f1_score1.append(metrics.f1_score(test_y, y_pred))\n\n\n    \nprint(\"kNN accuracy score: \\t\")\nprint(sum(accuracy_score1) \/ len(accuracy_score1))\nprint(\"----------------\")    \nprint(\"kNN score: \\t\")\nprint(sum(f1_score1) \/ len(f1_score1))\nprint(\"----------------\")\nconf_mat3 = confusion_matrix(test_y, y_pred)\nprint(\"kNN confusion matrix: \\n\",conf_mat3)\nprint(\"----------------\")\nprint(\"KNN precision score\")\nprint(sum(percision_score1) \/ len(percision_score1))\nprint(\"----------------\")\nprint(\"KNN recall_score\")\nprint(sum(recall_score1) \/ len(recall_score1))\nprint(\"----------------\")\n#print score for all evaluations\n","1990a1f8":"models_evalutions['Model'].append(\"KNN after balance our target's labels\")\nmodels_evalutions['Accuracy'].append(accuracy_score(test_y, y_pred))\nmodels_evalutions['Precision'].append(precision_score(test_y, y_pred))\nmodels_evalutions['Recall'].append(recall_score(test_y, y_pred))\nmodels_evalutions['F1 score'].append(f1_score(test_y, y_pred))","c7e4358a":"knn_final = knn.n_neighbors","2f7e2145":"cm =confusion_matrix(test_y, y_pred)\nclass_label = [\"No-Diabetes\", \"Diabetes\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\", cmap='Blues')\nplt.title('Confusion matrix for knn with cross validation and Smote', fontsize = 20); # title with fontsize 20","e87a973b":"x2_train=x_train.copy()\nx2_val=x_val.copy()","f78ed233":"#Before balance classes\nlr=LogisticRegression()\nprams ={\"penalty\": [ 'l1', 'l2'],\n       \"C\": [0.5 , 0.7,0.8 , 1 , 2.0 , 3.0]}\n\nlr_cv= GridSearchCV(lr , param_grid=prams, n_jobs=-1 ,cv=10)\nlr_cv.fit(x2_train , y_train )\n\nprint(\"Best params: \", lr_cv.best_params_)\nprint(\"Best estimator: \" ,lr_cv.best_estimator_)\nprint(\"Best score: \", lr_cv.best_score_)\n\nprint(\"Training Score before balance the labels:\",lr_cv.score(x2_train, y_train))\nprint(\"Validation Score before balance the labels:\",lr_cv.score(x2_val, y_val))\n\ny_pred = lr_cv.predict(x2_val)\nprint(\"\\nLogistic Regression Accuracy=\",accuracy_score(y_val, y_pred))\nprint(\"Logistic Regression F1 score=\",f1_score(y_val, y_pred))","b171592f":"lr_cv.best_score_","9adc82aa":"lr_final = lr_cv.best_estimator_\nlr_final","ed9e0812":"lr_final.score(x2_train , y_train)","e4d32b62":"lr_final.score(x_val , y_val)","352d559a":"#experiment2-1 with random over sampling\nlg1 = LogisticRegression(C=1,penalty=\"l2\")\n\n# randomly oversample positive samples\nROS = RandomOverSampler(random_state=42)\n\nX_tr_rs, y_tr_rs = ROS.fit_resample(x2_train, y_train)\n\nlg1.fit(X_tr_rs, y_tr_rs)\nprint(\"Training Score after balance the labels (RandomOverSampler):\",lg1.score(X_tr_rs, y_tr_rs))\nprint(\"Validation Score after balance the labels (RandomOverSampler)\",lg1.score(x_val, y_val))\n#model_eval(model3,X_test_std,y_test)","6d4496a0":"#experiment2-2 whith random under sampling\nlg2 = LogisticRegression(C=1,penalty=\"l2\")\n\nRUS = RandomUnderSampler(random_state=42)\n\nX_tr_us, y_tr_us = RUS.fit_resample(x2_train, y_train)\n\nlg2.fit(X_tr_us, y_tr_us)\nprint(\"Training Score after balance the labels (RandomUnderSampler)\",lg2.score(X_tr_us, y_tr_us))\nprint(\"Validation Score after balance the labels (RandomUnderSampler):\",lg2.score(x2_val, y_val))\n#model_eval(lg2,X_test_std,y_test)","cc3b00f8":"#experiment2-3 whith balanced weighted classes sampling\nlg3 = LogisticRegression(C=1,penalty=\"l2\",class_weight='balanced')\n\nlg3.fit(x2_train, y_train)\n#y_pred=lg3.predict(x2_val)\nprint(\"Training Score after Balanced class weights Logistic Regression\",lg3.score(x2_train, y_train))\nprint(\"Validation Score after Balanced class weights Logistic Regression:\",lg3.score(x2_val, y_val))\n","5617f2a9":"#experiment2-4 whith Smote\nlg = LogisticRegression(C=1,penalty=\"l2\")\n\nSMT = SMOTE(random_state=42)\n\nX_tr_smt, y_tr_smt = SMT.fit_resample(x2_train, y_train)\n\nlg.fit(X_tr_smt, y_tr_smt)\ny_pred=lg.predict(x2_val)\n\nprint(\"Training Score after balance the labels (Smote):\",lg.score(X_tr_smt, y_tr_smt))\nprint(\"Validation Score after balance the labels (Smote):\",lg.score(x2_val, y_val))\n#model_eval(model3,X_test_std,y_test)","1c7cb91a":"# classification report for logisitic\nprint(classification_report(y_val, y_pred, digits=3, zero_division = 1))\nacc_nb = accuracy_score(y_val, y_pred)\nrecall_nb = recall_score(y_val, y_pred, average=\"binary\")\nprint(\"ACCURACY:\",accuracy_score(y_val, y_pred))\nprint(\"RECALL:\",recall_score(y_val, y_pred, average=\"binary\"))","1e411be9":"models_evalutions['Model'].append(\"LogisticRegression with Smote\")\nmodels_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))\nmodels_evalutions['Precision'].append(precision_score(y_val, y_pred))\nmodels_evalutions['Recall'].append(recall_score(y_val, y_pred))\nmodels_evalutions['F1 score'].append(f1_score(y_val, y_pred))","9107ac06":"# plotting confusion mtrix\ncm = confusion_matrix(y_val, y_pred)\nclass_label = [\"No-Diabetes\", \"Diabetes\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\", cmap='Blues')\nplt.title('Confusion matrix for LogisticRegression ', fontsize = 20); # title with fontsize 20","0045d929":"x3_train=x_train.copy()\nx3_val=x_val.copy()","def331db":"# finding the best parameters for the decision tree\nparam_grid = {'criterion' :['gini', 'entropy'],'max_depth': [4, 6, 10, 12]}\n\ntree_clas = DecisionTreeClassifier(random_state=42)\ngrid_search = GridSearchCV(estimator=tree_clas, param_grid=param_grid, cv=5, verbose=True, scoring = 'f1')\ngrid_search.fit(x3_train, y_train)\n\nprint(grid_search.best_estimator_)","449173fb":"Dt_final = grid_search.best_estimator_\nDt_final","c90ce295":"# trying with entropy, since it didn't show in the previose step\ntree = DecisionTreeClassifier(criterion='entropy',\n                                     max_depth=10,\n                                     max_features='auto',\n                                     random_state=42)\n\ntree.fit(x3_train,y_train)\n\nprint(\"Training Score In Decision Tree Classification:\",tree.score(x3_train, y_train))\nprint(\"Validation Score In Decision Tree: Classification\",tree.score(x2_val, y_val))\ny_pred = tree.predict(x3_val)\n\nprint(\"DT Accuracy=\",accuracy_score(y_val, y_pred))\nprint(\"DT F1 score=\",f1_score(y_val, y_pred))","cb632172":"# classification report for Decision Tree \nprint(classification_report(y_val, y_pred, digits=3, zero_division = 1))\nacc_nb = accuracy_score(y_val, y_pred)\nrecall_nb = recall_score(y_val, y_pred, average=\"binary\")\nprint(\"ACCURACY:\",accuracy_score(y_val, y_pred))\nprint(\"RECALL:\",recall_score(y_val, y_pred, average=\"binary\"))","bd71a860":"models_evalutions['Model'].append(\"Decision Tree Classification\")\nmodels_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))\nmodels_evalutions['Precision'].append(precision_score(y_val, y_pred))\nmodels_evalutions['Recall'].append(recall_score(y_val, y_pred))\nmodels_evalutions['F1 score'].append(f1_score(y_val, y_pred))","3a2c0dbb":"# plotting confusion mtrix\ncm = confusion_matrix(y_val, y_pred)\nclass_label = [\"No-Diabetes\", \"Diabetes\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\", cmap='Blues')\nplt.title('Confusion matrix for Decision Tree classification', fontsize = 20); # title with fontsize 20","995d605d":"x5_train=x_train.copy()\nx5_val=x_val.copy()","47ec708e":"params = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [4, 6, 10, 12],\n    'random_state': [13]\n}","69992bdb":"#kf = KFold(n_splits=5, random_state=42, shuffle=False)\nexample_params = {\n        'n_estimators': 50,\n        'max_depth':4 ,\n        'random_state': 13\n    }\n    ","c4c83c4b":"imba_pipeline = make_pipeline(SMOTE(random_state=42),RandomForestClassifier(n_estimators=5, random_state=13))\ncross_val_score(imba_pipeline, x5_train, y_train, scoring='f1', cv=kf)","2f284098":"new_params = {'randomforestclassifier__' + key: params[key] for key in params}\ngrid_imba = GridSearchCV(imba_pipeline, param_grid=new_params, cv=kf, scoring='f1',return_train_score=True)\ngrid_imba.fit(x5_train, y_train)","f48e9d72":"print(\"Training Score In Random Forest Classification:\",grid_imba.score(x5_train, y_train))\nprint(\"Validation Score In Random Forest Classification:\",grid_imba.score(x5_val, y_val))","6a4dcbdb":"rf_best=grid_imba.best_params_\nrf_best","85af2d5a":"# Random Forest with best hyperparameter\nrf_best = RandomForestClassifier(n_estimators=100,\n                                 max_depth=12,\n                                 random_state=13)\nrf_best.fit(x5_train, y_train)\ny_pred = rf_best.predict(x5_val)\n","17941eca":"print(\"Training Score In Random Forest Classification with best parameters:\",rf_best.score(x5_train, y_train))\nprint(\"Validation Score In Random Forest Classification:with best parameters\",rf_best.score(x5_val, y_val))","8c8bf4e1":"# classification report for Random forest \nprint(classification_report(y_val, y_pred, digits=3, zero_division = 1))\nacc_nb = accuracy_score(y_val, y_pred)\nrecall_nb = recall_score(y_val, y_pred, average=\"binary\")\nprint(\"ACCURACY:\",accuracy_score(y_val, y_pred))\nprint(\"RECALL:\",recall_score(y_val, y_pred, average=\"binary\"))","a31093ae":"models_evalutions['Model'].append(\"RandomForestClassifier_best parameters\")\nmodels_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))\nmodels_evalutions['Precision'].append(precision_score(y_val, y_pred))\nmodels_evalutions['Recall'].append(recall_score(y_val, y_pred))\nmodels_evalutions['F1 score'].append(f1_score(y_val, y_pred))","80baeef0":"# plotting confusion mtrix\ncm = confusion_matrix(y_val, y_pred)\nclass_label = [\"No-Diabetes\", \"Diabetes\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\", cmap='Blues');\nplt.title('Confusion matrix for Random Forest classification with best parameters', fontsize = 20); # title with fontsize 20","2f8dc984":"x6_train=x_train.copy()\nx6_val=x_val.copy()","a837bf0b":"model_names = [\"rf_best\",\"Dt_final\"]\n\nmodel_vars = [eval(n) for n in model_names]\nmodel_list = list(zip(model_names, model_vars))","45eb7f99":"model_names","b264279c":"model_list","79ac0bb4":"for model_name in model_names:\n    curr_model = eval(model_name)\n    print(f'{model_name} score: {curr_model.score(x_val, y_val)}')","c42dd33f":"# create voting classifier\nvoting_classifer = VotingClassifier(estimators=model_list,voting='hard', n_jobs=-1)\nvoting_classifer.fit(x6_train, y_train)","b8184f9a":"# get accuracy (model to beat: RF with 0.8136 accuracy)\ny_pred = voting_classifer.predict(x_val)","8b80f9f3":"print(\"Training Score In Hard Voting and select the best model scores(DT& RF))\",voting_classifer.score(x6_train, y_train))\nprint(\"Training Score In HRD Voting and select the best model scores(DT& RF))\",voting_classifer.score(x6_val, y_val))","97b80e6e":"# classification report for Voting \nprint(classification_report(y_val, y_pred, digits=3, zero_division = 1))\nacc_nb = accuracy_score(y_val, y_pred)\nrecall_nb = recall_score(y_val, y_pred, average=\"binary\")\nprint(\"ACCURACY:\",accuracy_score(y_val, y_pred))\nprint(\"RECALL:\",recall_score(y_val, y_pred, average=\"binary\"))","e5e9dc8e":"models_evalutions['Model'].append(\"VotingClassifier-Hard\")\nmodels_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))\nmodels_evalutions['Recall'].append(recall_score(y_val, y_pred))\nmodels_evalutions['Precision'].append(precision_score(y_val, y_pred))\nmodels_evalutions['F1 score'].append(f1_score(y_val, y_pred))","9bd85cd3":"# plotting confusion mtrix\ncm = confusion_matrix(y_val, y_pred)\nclass_label = [\"No-Diabetes\", \"Diabetes\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\", cmap='Blues')\nplt.title('Confusion matrix for Voting', fontsize = 20); # title with fontsize 20","fd40bbb9":"x7_train=x_train.copy()\nx7_val=x_val.copy()","eede9707":"# create voting classifier\nvoting_classifer = VotingClassifier(estimators=model_list,voting='soft', n_jobs=-1)\nvoting_classifer.fit(x7_train, y_train)","285cd2c5":"print(\"Training Score In Average Voting and select the best model scores(DT& RF))\",voting_classifer.score(x7_train, y_train))\nprint(\"Training Score In Avaerage Voting and select the best model scores(DT& RF))\",voting_classifer.score(x7_val, y_val))","9800e37c":"# Get accuracy (model to beat: RF with 0.8136 accuracy)\ny_pred = voting_classifer.predict(x7_val)","007ef6b1":"# classification report for voting avarage\nprint(classification_report(y_val, y_pred, digits=3, zero_division = 1))\nacc_nb = accuracy_score(y_val, y_pred)\nrecall_nb = recall_score(y_val, y_pred, average=\"binary\")\nprint(\"ACCURACY:\",accuracy_score(y_val, y_pred))\nprint(\"RECALL:\",recall_score(y_val, y_pred, average=\"binary\"))","268bc86e":"models_evalutions['Model'].append(\"VotingClassifier-Average Voting\")\nmodels_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))\nmodels_evalutions['Recall'].append(recall_score(y_val, y_pred))\nmodels_evalutions['Precision'].append(precision_score(y_val, y_pred))\nmodels_evalutions['F1 score'].append(f1_score(y_val, y_pred))","852d6d83":"# plotting confusion mtrix\ncm = confusion_matrix(y_val, y_pred)\nclass_label = [\"No-Diabetes\", \"Diabetes\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\", cmap='Blues')\nplt.title('Confusion matrix for Avarege Voting', fontsize = 20); # title with fontsize 20","1eeeee04":"x8_train=x_train.copy()\nx8_val=x_val.copy()","c3863dad":"# create voting classifier\nweights = [1.5,3.8]\nvoting_model = VotingClassifier(estimators=model_list, voting='soft', weights = weights, n_jobs=-1)\nvoting_model.fit(x8_train, y_train)","9269cbcf":"# Get accuracy (model to beat: RF with 0.8136 accuracy)\ny_pred = voting_model.predict(x8_val)","fb145ffa":"print(\"Training Score In Weighted Voting and select the best model scores(DT& RF))\",voting_model.score(x7_train, y_train))\nprint(\"Training Score In Weighted Voting and select the best model scores(DT& RF))\",voting_model.score(x7_val, y_val))","9628e5cf":"# classification report for Naive Bayes\nprint(classification_report(y_val, y_pred, digits=3, zero_division = 1))\nacc_nb = accuracy_score(y_val, y_pred)\nrecall_nb = recall_score(y_val, y_pred, average=\"binary\")\nprint(\"ACCURACY:\",accuracy_score(y_val, y_pred))\nprint(\"RECALL:\",recall_score(y_val, y_pred, average=\"binary\"))","d2392e53":"models_evalutions['Model'].append(\"VotingClassifier-Weighted Voting\")\nmodels_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))\nmodels_evalutions['Recall'].append(recall_score(y_val, y_pred))\nmodels_evalutions['Precision'].append(precision_score(y_val, y_pred))\nmodels_evalutions['F1 score'].append(f1_score(y_val, y_pred))","e379ca34":"# plotting confusion mtrix\ncm = confusion_matrix(y_val, y_pred)\nclass_label = [\"No-Diabetes\", \"Diabetes\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\", cmap='Blues')\nplt.title('Confusion matrix for Wieghted Voting', fontsize = 20); # title with fontsize 20","7c59342c":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import BaggingClassifier\ntree_clas = DecisionTreeClassifier(random_state=42)","e339f5e2":"bag_model = BaggingClassifier(\n    base_estimator = DecisionTreeClassifier(),\n    n_estimators = 100,\n    max_samples = 0.8,\n    oob_score=True,\n    random_state = 0\n)","ccbf150a":"bag_model.fit(x_train, y_train)","baf1107d":"bag_model.oob_score_","c8c6779d":"bag_model.score(x_val,y_val)","b12175a0":"y_pred = tree.predict(x_val)\n\nprint(\"DT Accuracy=\",accuracy_score(y_val, y_pred))\nprint(\"DT F1 score=\",f1_score(y_val, y_pred))","9a0b2152":"# classification report Baggin\nprint(classification_report(y_val, y_pred, digits=3, zero_division = 1))\nacc_nb = accuracy_score(y_val, y_pred)\nrecall_nb = recall_score(y_val, y_pred, average=\"binary\")\nprint(\"ACCURACY:\",accuracy_score(y_val, y_pred))\nprint(\"RECALL:\",recall_score(y_val, y_pred, average=\"binary\"))","3d431163":"models_evalutions['Model'].append(\" BaggingClassifier\")\nmodels_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))\nmodels_evalutions['Recall'].append(recall_score(y_val, y_pred))\nmodels_evalutions['Precision'].append(precision_score(y_val, y_pred))\nmodels_evalutions['F1 score'].append(f1_score(y_val, y_pred))","1adbb3b1":"# plotting confusion mtrix\ncm = confusion_matrix(y_val, y_pred)\nclass_label = [\"No-Diabetes\", \"Diabetes\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\", cmap='Blues')\nplt.title('Confusion matrix for BaggingClassifier', fontsize = 20); # title with fontsize 20","c42d9d02":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier","c2f49de7":"nb = GaussianNB()\nnb.fit(x_train,y_train)\nprint(\"Naive Bayes val score: \",nb.score(x_val,y_val))\nprint(\"Naive Bayes Train score: \",nb.score(x_train,y_train))\ny_pred_nb = nb.predict(x_val)","3e663966":"y_pred=nb.predict(x_val)","74f7c40b":"# classification report for Naive Bayes\nprint(classification_report(y_val, y_pred, digits=3, zero_division = 1))\nacc_nb = accuracy_score(y_val, y_pred)\nrecall_nb = recall_score(y_val, y_pred, average=\"binary\")\nprint(\"ACCURACY:\",accuracy_score(y_val, y_pred))\nprint(\"RECALL:\",recall_score(y_val, y_pred, average=\"binary\"))","1d26445f":"models_evalutions['Model'].append(\"Naive-B\")\nmodels_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))\nmodels_evalutions['Precision'].append(precision_score(y_val, y_pred))\nmodels_evalutions['Recall'].append(recall_score(y_val, y_pred))\nmodels_evalutions['F1 score'].append(f1_score(y_val, y_pred))","b0b29f68":"# plotting confusion mtrix\ncm = confusion_matrix(y_val, y_pred)\nclass_label = [\"No-Diabetes\", \"Diabetes\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\", cmap='Blues')\nplt.title('Confusion matrix forGaussianNB', fontsize = 20); # title with fontsize 20","e1eed5d6":"result= pd.DataFrame.from_dict(models_evalutions)\nresult","b9baf2bb":"cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nresult.plot.bar(x='Model',y=['Accuracy','Precision','Precision','Recall','F1 score'], cmap='Blues_r', figsize=(10,10))\nplt.title(\"Comparison of Scores of the Models\")\nplt.ylabel(\"Models\")\nplt.xlabel(\"ML Algorithms's scores in Diabetes Dataset\")\nplt.show()","cd9886f1":"import warnings\nimport pickle\nfrom sklearn.pipeline import make_pipeline\nwarnings.filterwarnings(\"ignore\")","8e525478":"lg = LogisticRegression()\nlg.fit(x_train, y_train)","92918074":"data = {\"model\":lg, \"scaler\": scaler}\nwith open('saved_steps.pkl', 'wb') as file:\n    pickle.dump(data, file)","072c6cf5":"x_train","522803f0":"# Expreiment 7: Bagging","f5f95229":"13-level age categorys:(_AGEG5YR see codebook)\n> 1 = 18-24 \n\n> 9 = 60-64 \n\n> 13 = 80 or older \n","fbfa1b76":"## Scalling to give us fair distrubtion btw features","a791efa5":"# Import Libraries","e0cd9c0d":"# Deplyment ","a3c16e6a":"The Class Imbalance classification divided into three:\n\n> 1. Before model training: Resampling strategies (oversampling, undersampling)\n\n> 2. During model training: Training with adjusted class weights\n\n> 3. After model training: Adjusting the decision threshold (F1 optimization strategy)","bb50f83f":"the best result was for Smote ","dee9200b":"## Experiment 3: Decision Tree Classification","0ff6bf81":"## Expreiment 2: Logistic Regression Model with smote","00bf5bb2":"Mental health scale:\n\n> scale 1-30 days .","e65f8630":"# Expreiment 8 : Naive Bayes","78957b7f":"## Expreiment 5: Random Forest Classification ","3a78140e":"# Build models","69b22fba":"## Experiment 6-1: Ensembling with Voting","c53ae938":"Now thinking about your physical health, for how many days during the past 30 days \nwas your physical health not good?\n\n> scale 1-30 days ","ffddaa27":"# Cleaning Dataset","b65f3cb3":"Income scale 1-8:\n> 1 = less than $10,000 \n\n> 5 = less than $35,000 \n\n> 8 = $75,000 or more\n\n","ee2562a1":"Becouse of our target's labels imbalance we will use Smote to balance them with cross validation.","c4d39df5":"# Visualze the correlation ","410cc24c":"## Experiment 1-1: K-nearest Neighbors Classification without using cross vaidation","dae9a018":"## Experiment 6-2: Ensembling with Average Voting","60b895fd":"Education level (EDUCA see codebook) scale 1-6:\n> 1 = Never attended school or only kindergarten \n\n> 2 = Grades 1 through 8 (Elementary)\n\n> 3 = Grades 9 throug 11 (Some high school)\n\n> 4 = Grade 12 or GED (High school graduate)\n\n> 5 = College 1 year to 3 years (Some college or technical school) \n\n> 6 = College 4 years or more (College graduate)","a6ef8123":"## Experiment 6-3: Ensembling with Weighted Voting","6a68da9f":"# preprocessing","c4df6561":"## Experiment 1-2: K-nearest Neighbors Classification with cross validation & Smote","5be3beed":"# Explore Dataset","7fa33305":"Would you say that in general your health is:scale 1-5:\n> 1 = excellent \n\n> 2 = very good \n\n> 3 = good \n\n> 4 = fair \n\n> 5 = poor\n","64aaacee":"# Load Dataset ","42d4f055":"## Baseline Model","85833076":"## Experiment 1: Split data to training,validation and test set"}}