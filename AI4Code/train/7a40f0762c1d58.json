{"cell_type":{"76b959f1":"code","85d2e0ea":"code","3abda353":"code","d879ba88":"code","049979a2":"code","86cb1eab":"code","d3147e84":"code","bc826a91":"code","2a181afb":"code","bc30e464":"code","b6b57ecf":"code","518ea0ed":"code","c7d2860c":"code","d09f64f6":"code","d8872127":"code","779f6135":"code","d18be8d7":"code","521ccac5":"code","e547e568":"code","31dda9b7":"code","943d81a7":"code","4676baa5":"code","5127356c":"code","9534e6ca":"code","eb931562":"code","31673cc5":"code","928da5c2":"code","6952bdbe":"code","de9d09bc":"code","28aee4ad":"code","2172358f":"code","5da2240f":"code","a00418d0":"code","f4370ee2":"code","3f4c0ac0":"code","a1a53db7":"code","57518a41":"code","375320d0":"code","26bae6a6":"code","9d510d62":"code","432daece":"code","6516f67a":"code","360691b4":"code","fdeb24f6":"code","17dfcabb":"code","8b922f42":"code","977d631c":"code","f9fb80f5":"code","c5881fc2":"code","490a17ce":"markdown","f4bb9ec4":"markdown","ed3301c7":"markdown","9c7c80da":"markdown","409451d4":"markdown","87365451":"markdown","1ab8fa09":"markdown","9020acc2":"markdown","05f966e2":"markdown","c33c57e3":"markdown","a79f10b2":"markdown","120c1b4d":"markdown","db5408d7":"markdown","ec4888fa":"markdown","5bed4b20":"markdown","5f328414":"markdown","2ac6c7e3":"markdown"},"source":{"76b959f1":"# IMPORTING LIBRARIES\n\n# Main Libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as stats\nfrom scipy.stats import norm\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n\n# Pre-processing Libraries\n\nfrom sklearn.utils import class_weight\nfrom imblearn.over_sampling import SMOTE\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom prettytable import PrettyTable\n\n# Machine Learning Libraries\n\nimport sklearn\nfrom sklearn import tree\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import KFold, cross_validate\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score\nfrom sklearn.metrics import recall_score, f1_score, roc_auc_score\n\n\n# Defining working directory\n\nwork_dir = '..\/input\/heart-failure-clinical-data\/'","85d2e0ea":"# IMPORTING DATA\n\nhf_data = pd.read_csv(work_dir + 'heart_failure_clinical_records_dataset.csv')\nhf_data.head()","3abda353":"hf_data.info()","d879ba88":"# Finding duplicates \n\nhf_data.duplicated().sum()","049979a2":"# Finding missing values(Nan)\n\n[print(col) for col in hf_data if hf_data[col].isna().sum() > 0]","86cb1eab":"# Transforming categorical values into strings\n\nhf_data['anaemia'] = hf_data['anaemia'].apply(str)\nhf_data['diabetes'] = hf_data['diabetes'].apply(str)\nhf_data['high_blood_pressure'] = hf_data['high_blood_pressure'].apply(str)\nhf_data['smoking'] = hf_data['smoking'].apply(str)\nhf_data['sex'] = hf_data['sex'].apply(str)\nhf_data['DEATH_EVENT'] = hf_data['DEATH_EVENT'].apply(str)","d3147e84":"# Let's look at the descriptive statistics\n\nhf_data.describe()","bc826a91":"# Checking labels distributions\n\nsns.set_theme(context = 'paper')\n\nplt.figure(figsize = (10,5))\nsns.countplot(hf_data['DEATH_EVENT'])\nplt.title('Class Distributions \\n (0: Survived || 1: Passed )', fontsize=14)\nplt.show()","2a181afb":"# Let's plot the numerical faetures (Histograms & Scatterplots)\n\nplt.figure(figsize = (20,15))\nsns.pairplot(hf_data)\nplt.show()","bc30e464":"# EDA & VISUALIZATIONS\n\n# Correlation Heatmap\n\nf, ax = plt.subplots(figsize=(15, 15))\nmat = hf_data.corr('pearson')\nmask = np.triu(np.ones_like(mat, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0, annot = True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","b6b57ecf":"# Plotting features with an interesting correlation\n\nf, axes = plt.subplots(ncols=4, figsize=(24,6))\n\nsns.boxplot(x='DEATH_EVENT', y=\"age\", data=hf_data, hue = 'sex',ax=axes[0])\naxes[0].set_title('Age vs Death Event', fontsize = 14)\n\nsns.boxplot(x='DEATH_EVENT', y=\"creatinine_phosphokinase\", data=hf_data, hue = 'sex', ax=axes[1]) \naxes[1].set_title('Creatinine Phosphokinase vs Death Event', fontsize = 14)\n\n\nsns.boxplot(x='DEATH_EVENT', y=\"ejection_fraction\", data=hf_data, hue = 'sex', ax=axes[2])\naxes[2].set_title('Ejection Fraction vs Death Event', fontsize = 14)\n\n\nsns.boxplot(x='DEATH_EVENT', y=\"platelets\", data=hf_data, hue = 'sex',ax=axes[3])  \naxes[3].set_title('Platelets vs Death Event', fontsize = 14) \n\nplt.show()","518ea0ed":"# Plotting more features with an interesting correlation\n\nf, axes = plt.subplots(ncols=3, figsize=(24,6))\n\nsns.boxplot(x='DEATH_EVENT', y=\"serum_creatinine\", data=hf_data,hue = 'sex', ax=axes[0])\naxes[0].set_title('Serum Creatinine vs Death Event', fontsize = 14)\n\nsns.boxplot(x='DEATH_EVENT', y=\"serum_sodium\", data=hf_data,hue = 'sex', ax=axes[1]) \naxes[1].set_title('Serum Sodium vs Death Event', fontsize = 14)\n\n\nsns.boxplot(x='DEATH_EVENT', y=\"time\", data=hf_data,hue = 'sex', ax=axes[2])\naxes[2].set_title('Time vs Death Event', fontsize = 14)\n \nplt.show()","c7d2860c":"# Plotting the feature distributions our numeric features\n\nf, ax = plt.subplots(1,4, figsize=(24, 6))\n\nsns.distplot(hf_data['age'],fit=norm, color='#FB8861', ax = ax[0])\nax[0].set_title('Age \\n Normal dist.', fontsize=14)\n\nsns.distplot(hf_data['creatinine_phosphokinase'], fit=norm, color='#56F9BB',ax=ax[1])\nax[1].set_title('Creatinine Phosphokinase \\n Non normal dist.', fontsize=14)\n\nsns.distplot(hf_data['ejection_fraction'], fit=norm, color='#C5B3F9', ax = ax[2])\nax[2].set_title('Ejection Fraction\\n Non normal dist.', fontsize=14)\n\nsns.distplot(hf_data['platelets'], fit=norm, color='#C5B3F9',ax = ax[3])\nax[3].set_title(' Platelets \\n Non normal dist.', fontsize=14)\n\nplt.show()","d09f64f6":"# Plotting the feature distributions our numeric features\n\nf, ax = plt.subplots(1,3, figsize=(24, 6))\n\nsns.distplot(hf_data['serum_creatinine'],fit=norm, color='#FB8861', ax = ax[0])\nax[0].set_title('serum Creatinine \\n Non normal dist.', fontsize=14)\n\nsns.distplot(hf_data['serum_sodium'], fit=norm, color='#56F9BB',ax=ax[1])\nax[1].set_title('Serum Sodium \\n Normal dist.', fontsize=14)\n\nsns.distplot(hf_data['time'], fit=norm, color='#C5B3F9', ax = ax[2])\nax[2].set_title('Time \\n Non normal dist.', fontsize=14)\n\nplt.show()","d8872127":"# Outliers removal function\n\ndef outliers_removal(feature,feature_name,dataset):\n    \n    # Identify 25th & 75th quartiles\n\n    q25, q75 = np.percentile(feature, 25), np.percentile(feature, 75)\n    print('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\n    feat_iqr = q75 - q25\n    print('iqr: {}'.format(feat_iqr))\n    \n    feat_cut_off = feat_iqr * 1.5\n    feat_lower, feat_upper = q25 - feat_cut_off, q75 + feat_cut_off\n    print('Cut Off: {}'.format(feat_cut_off))\n    print(feature_name +' Lower: {}'.format(feat_lower))\n    print(feature_name +' Upper: {}'.format(feat_upper))\n    \n    outliers = [x for x in feature if x < feat_lower or x > feat_upper]\n    print(feature_name + ' outliers for close to bankruptcy cases: {}'.format(len(outliers)))\n    #print(feature_name + ' outliers:{}'.format(outliers))\n\n    dataset = dataset.drop(dataset[(dataset[feature_name] > feat_upper) | (dataset[feature_name] < feat_lower)].index)\n    print('-' * 65)\n    \n    return dataset\n\nhf_data = outliers_removal(hf_data['age'],'age', hf_data)\nhf_data = outliers_removal(hf_data['creatinine_phosphokinase'],'creatinine_phosphokinase', hf_data)\nhf_data = outliers_removal(hf_data['ejection_fraction'],'ejection_fraction', hf_data)\nhf_data = outliers_removal(hf_data['platelets'],'platelets', hf_data)\nhf_data = outliers_removal(hf_data['serum_creatinine'],'serum_creatinine', hf_data)\nhf_data = outliers_removal(hf_data['serum_sodium'],'serum_sodium', hf_data)\nhf_data = outliers_removal(hf_data['time'],'time', hf_data)","779f6135":"# Plotting boxplots of numerical features\n\nf, axes = plt.subplots(ncols=4, figsize=(24,6))\n\nsns.boxplot(x='DEATH_EVENT', y=\"age\", data=hf_data, hue = 'sex',ax=axes[0])\naxes[0].set_title('Age vs Death Event', fontsize = 14)\n\nsns.boxplot(x='DEATH_EVENT', y=\"creatinine_phosphokinase\", data=hf_data, hue = 'sex', ax=axes[1]) \naxes[1].set_title('Creatinine Phosphokinase vs Death Event', fontsize = 14)\n\n\nsns.boxplot(x='DEATH_EVENT', y=\"ejection_fraction\", data=hf_data, hue = 'sex', ax=axes[2])\naxes[2].set_title('Ejection Fraction vs Death Event', fontsize = 14)\n\n\nsns.boxplot(x='DEATH_EVENT', y=\"platelets\", data=hf_data, hue = 'sex',ax=axes[3])  \naxes[3].set_title('Platelets vs Death Event', fontsize = 14) \n\nplt.show()","d18be8d7":"# Plotting boxplots of numerical features\n\nf, axes = plt.subplots(ncols=3, figsize=(24,6))\n\nsns.boxplot(x='DEATH_EVENT', y=\"serum_creatinine\", data=hf_data,hue = 'sex', ax=axes[0])\naxes[0].set_title('Serum Creatinine vs Death Event', fontsize = 14)\n\nsns.boxplot(x='DEATH_EVENT', y=\"serum_sodium\", data=hf_data,hue = 'sex', ax=axes[1]) \naxes[1].set_title('Serum Sodium vs Death Event', fontsize = 14)\n\n\nsns.boxplot(x='DEATH_EVENT', y=\"time\", data=hf_data,hue = 'sex', ax=axes[2])\naxes[2].set_title('Time vs Death Event', fontsize = 14)\n \nplt.show()","521ccac5":"# Plotting the feature distributions our numeric features\n\nf, ax = plt.subplots(1,4, figsize=(24, 6))\n\nsns.distplot(hf_data['age'],fit=norm, color='#FB8861', ax = ax[0])\nax[0].set_title('Age \\n Non normal dist.', fontsize=14)\n\nsns.distplot(hf_data['creatinine_phosphokinase'], fit=norm, color='#56F9BB',ax=ax[1])\nax[1].set_title('Creatinine Phosphokinase \\n Non normal dist.', fontsize=14)\n\nsns.distplot(hf_data['ejection_fraction'], fit=norm, color='#C5B3F9', ax = ax[2])\nax[2].set_title('Ejection Fraction\\n Non normal dist.', fontsize=14)\n\nsns.distplot(hf_data['platelets'], fit=norm, color='#C5B3F9',ax = ax[3])\nax[3].set_title(' Platelets \\n Non normal dist.', fontsize=14)\n\nplt.show()","e547e568":"# Plotting the feature distributions our numeric features\n\nf, ax = plt.subplots(1,3, figsize=(24, 6))\n\nsns.distplot(hf_data['serum_creatinine'],fit=norm, color='#FB8861', ax = ax[0])\nax[0].set_title('serum Creatinine \\n Non normal dist.', fontsize=14)\n\nsns.distplot(hf_data['serum_sodium'], fit=norm, color='#56F9BB',ax=ax[1])\nax[1].set_title('Serum Sodium \\n Non normal dist.', fontsize=14)\n\nsns.distplot(hf_data['time'], fit=norm, color='#C5B3F9', ax = ax[2])\nax[2].set_title('Time \\n Non normal dist.', fontsize=14)\n\nplt.show()","31dda9b7":"# Checking Normality\n\ndef check_normality(data, name):\n    shap_t,shap_p = stats.shapiro(data)\n    print(name + ' parameters:')\n    print()\n    print(\"Skewness: %f\" % abs(data).skew())\n    print(\"Kurtosis: %f\" % abs(data).kurt())\n    print(\"Shapiro Test: %f\" % shap_t)\n    print(\"Shapiro p_value: %f\" % shap_p)\n    \n    if shap_p > 0.05:\n        print('The distribution is normal')\n    else:\n        print('The distribution is not normal')\n    \ncheck_normality(hf_data['age'],'Age')\nprint('-------------------------')\ncheck_normality(hf_data['creatinine_phosphokinase'],'Creatinine Phosphokinase')\nprint('-------------------------')\ncheck_normality(hf_data['ejection_fraction'],'Ejection Fraction')\nprint('-------------------------')\ncheck_normality(hf_data['platelets'],'Platelets')\nprint('-------------------------')\ncheck_normality(hf_data['serum_creatinine'],'Serum Creatinine')\nprint('-------------------------')\ncheck_normality(hf_data['serum_sodium'],'Serum Sodium')\nprint('-------------------------')\ncheck_normality(hf_data['time'],'Time')","943d81a7":"# Train data - labels separation\n\nlabels = hf_data['DEATH_EVENT']\ntrain = hf_data.drop(['DEATH_EVENT'], axis = 1)","4676baa5":"# Dealing with categorical data\n\ntrain_dummy = pd.get_dummies(train)\n\n# Splitting the data into Train & Test sets\n\nXtrain,X_test,ytrain,y_test = train_test_split(train_dummy,labels,\n                                               test_size = 0.1,\n                                               stratify = labels,\n                                               shuffle = True)\n\nX_train,X_val,y_train,y_val = train_test_split(Xtrain,ytrain,\n                                               test_size = 0.1,\n                                               stratify = ytrain,\n                                               shuffle = True)\n\n# This is how our train data preprocessed look like\n\nX_train.head()","5127356c":"# A line of code that help us finding the names of all the metrics available in sklearn library\n\nsklearn.metrics.SCORERS.keys()","9534e6ca":"# 10FOLD - LOGISTIC REGRESSION (Baseline)\n\nl_reg = LogisticRegression(class_weight = 'balanced',\n                           random_state = 42)\nlog_model = l_reg.fit(X_train,y_train)\n \n\nscores_log = cross_validate(log_model, X_train, y_train, cv=10,\n                        scoring=('accuracy','precision_weighted','recall_weighted','f1_weighted', 'roc_auc'),\n                        return_train_score=True)\n\n# Creating a DataFrame of results\n\ncv_scores_log = pd.DataFrame(scores_log, columns = scores_log.keys()).mean()\ncv_scores_avg = pd.DataFrame(columns = scores_log.keys())\ncv_scores_avg = cv_scores_avg.append(cv_scores_log, ignore_index = True)\n\n\n\n\n# 10FOLD - RANDOM FOREST CLASSIFIER (Baseline)\n\nrfc = RandomForestClassifier(class_weight = 'balanced',\n                             random_state = 42)\nrfc_model = rfc.fit(X_train,y_train)\n \n\nscores_rfc = cross_validate(rfc_model, X_train, y_train, cv=10,\n                        scoring=('accuracy','precision_weighted','recall_weighted','f1_weighted', 'roc_auc'),\n                        return_train_score=True)\n\ncv_scores_rfc = pd.DataFrame(scores_rfc, columns = scores_rfc.keys()).mean()\ncv_scores_avg = cv_scores_avg.append(cv_scores_rfc, ignore_index = True)\n\ncv_scores_avg","eb931562":"# Transforming the dataset\n\noversample = SMOTE()\nX_smote, y_smote = oversample.fit_resample(train_dummy, labels)\ncounter = Counter(y_smote)\nprint(counter)\n\n# Splitting the data\n\nX_train_sm,X_val_sm,y_train_sm,y_val_sm = train_test_split(X_smote,y_smote,\n                                               test_size = 0.1,\n                                               stratify = y_smote,\n                                               shuffle = True)","31673cc5":"# 10FOLD - LOGISTIC REGRESSION (Baseline Smote)\n\nl_reg = LogisticRegression(random_state = 42)\nlog_model_sm = l_reg.fit(X_train_sm,y_train_sm)\n \n\nscores_log_sm = cross_validate(log_model_sm, X_train_sm, y_train_sm, cv=10,\n                        scoring=('accuracy','precision_weighted','recall_weighted','f1_weighted', 'roc_auc'),\n                        return_train_score=True)","928da5c2":"# 10FOLD - RANDOM FOREST CLASSIFIER (Baseline Smote)\n\nrfc = RandomForestClassifier(random_state = 42)\nrfc_model_sm = rfc.fit(X_train_sm,y_train_sm)\n \n\nscores_rfc_sm = cross_validate(rfc_model_sm, X_train_sm, y_train_sm, cv=10,\n                        scoring=('accuracy','precision_weighted','recall_weighted','f1_weighted', 'roc_auc'),\n                        return_train_score=True)","6952bdbe":"cv_scores_log_sm = pd.DataFrame(scores_log_sm, columns = scores_log_sm.keys()).mean()\ncv_scores_rfc_sm = pd.DataFrame(scores_rfc_sm, columns = scores_rfc_sm.keys()).mean()\ncv_scores_avg = cv_scores_avg.append(cv_scores_log_sm, ignore_index = True)\ncv_scores_avg = cv_scores_avg.append(cv_scores_rfc_sm, ignore_index = True)\ncv_scores_avg['Classifiers'] = ['LOG','RFC','LOG_sm','RFC_sm']\ncv_scores_avg","de9d09bc":"f, ax = plt.subplots(1,5, figsize = (25,5))\n\nsns.barplot(cv_scores_avg['Classifiers'], cv_scores_avg['test_accuracy'], ax = ax[0])\nax[0].set_title('Accuracy Scores', fontsize = 13)\nsns.barplot(cv_scores_avg['Classifiers'], cv_scores_avg['test_precision_weighted'], ax = ax[1])\nax[1].set_title('Precision Scores', fontsize = 13)\nsns.barplot(cv_scores_avg['Classifiers'], cv_scores_avg['test_recall_weighted'], ax = ax[2])\nax[2].set_title('Recall Scores', fontsize = 13)\nsns.barplot(cv_scores_avg['Classifiers'], cv_scores_avg['test_f1_weighted'], ax = ax[3])\nax[3].set_title('F1_Scores Scores', fontsize = 13)\nsns.barplot(cv_scores_avg['Classifiers'], cv_scores_avg['test_roc_auc'], ax = ax[4])\nax[4].set_title('Auc-Roc Scores', fontsize = 13)\n\nplt.show()","28aee4ad":"f, ax = plt.subplots(1,2, figsize = (20,5))\n\nsns.barplot(cv_scores_avg['Classifiers'], cv_scores_avg['fit_time'], ax = ax[0])\nax[0].set_title('Training Time', fontsize = 13)\nax[0].set_ylabel('Time (ms)')\nsns.barplot(cv_scores_avg['Classifiers'], cv_scores_avg['score_time'], ax = ax[1])\nax[1].set_title('Predictions Time', fontsize = 13)\nax[1].set_ylabel('Time (ms)')\nplt.show()","2172358f":"# Which are the default parameters of Logistic Regression ?\n\nlog_params = log_model.get_params()\nlog_params ","5da2240f":"# Logistic Regression Optimization\n\nlog_parameters = dict(C = [0.5,1,1.5],\n                      penalty = ['l2', 'l1','elasticnet'],\n                      class_weight = ['Balanced', None],\n                      solver = ['liblinear','lbfgs','newton-cg'])\n\n\nlog_RGS = RandomizedSearchCV(log_model, log_parameters, random_state=42)\nsearch = log_RGS.fit(X_train, y_train)\nopt_params = search.best_params_\nopt_params","a00418d0":"# LOGISTIC REGRESSION WITH OPTIMIZED HYPERPARAMETERS\n\nlog_opt = LogisticRegression(**opt_params)\nlog_model_opt = log_opt.fit(X_train,y_train)\n\nscores_log_opt = cross_validate(log_model_opt, X_train, y_train, cv=10,\n                        scoring=('accuracy','precision_weighted','recall_weighted','f1_weighted', 'roc_auc'),\n                        return_train_score=True)\n\ncv_scores_log_opt = pd.DataFrame(scores_log_opt, columns = scores_log_sm.keys()).mean()\ncv_scores_avg = cv_scores_avg.append(cv_scores_log_opt, ignore_index = True)\n\ncv_scores_avg['Classifiers'][4] = 'LOG_opt'\ncv_scores_avg","f4370ee2":"# Which are the default parameters of Random Forest Classifier ?\n\nrfc_params = rfc_model_sm.get_params()\nrfc_params ","3f4c0ac0":"# Random Forest Classifier Optimization\n\nrfc_parameters = dict(criterion = ['gini', 'entropy'],\n                      ccp_alpha = [0.0,0.1,0.5],\n                      bootstrap = [True,False])\n\n\nrfc_RGS = RandomizedSearchCV(rfc_model_sm, rfc_parameters, random_state=42)\nsearch_rfc = rfc_RGS.fit(X_train_sm, y_train_sm)\nopt_params_rfc = search_rfc.best_params_\nopt_params_rfc","a1a53db7":"# RANDOM FOREST CLASSIFIER WITH OPTIMIZED HYPERPARAMETERS\n\nrfc_opt = RandomForestClassifier(**opt_params_rfc)\nrfc_model_opt = rfc_opt.fit(X_train_sm,y_train_sm)\n\nscores_rfc_opt = cross_validate(rfc_model_opt, X_train_sm, y_train_sm, cv=10,\n                        scoring=('accuracy','precision_weighted','recall_weighted','f1_weighted', 'roc_auc'),\n                        return_train_score=True)\n\nrfc_pred_opt = rfc_model_opt.predict(X_test)\ncv_scores_rfc_opt = pd.DataFrame(scores_rfc_opt, columns = scores_rfc_sm.keys()).mean()\ncv_scores_avg = cv_scores_avg.append(cv_scores_rfc_opt, ignore_index = True)","57518a41":"cv_scores_avg['Classifiers'][5] = 'RFC_opt'\ncv_scores_avg","375320d0":"# Plotting the first tree of our optimized forest \n\nplt.figure(figsize = (20,10))\ntree.plot_tree(rfc_opt.estimators_[0], feature_names=X_train.columns, filled=True, fontsize=7)\nplt.show()","26bae6a6":"feat_importance = pd.Series(rfc_model_opt.feature_importances_, index=X_train_sm.columns)\n\nplt.figure(figsize = (15,6))\nsns.barplot(feat_importance.nlargest(20),feat_importance.nlargest(20).index)\nplt.title(\"Random Forest features' importance\", fontsize = 13)\nplt.xlabel('Feature Importance')\nplt.show()","9d510d62":"# Plotting Classifiers Performances (all metrics)\n\nf, ax = plt.subplots(1,5, figsize = (25,5))\n\nsns.barplot(cv_scores_avg['Classifiers'], cv_scores_avg['test_accuracy'], ax = ax[0])\nax[0].set_title('Accuracy Scores', fontsize = 13)\nsns.barplot(cv_scores_avg['Classifiers'], cv_scores_avg['test_precision_weighted'], ax = ax[1])\nax[1].set_title('Precision Scores', fontsize = 13)\nsns.barplot(cv_scores_avg['Classifiers'], cv_scores_avg['test_recall_weighted'], ax = ax[2])\nax[2].set_title('Recall Scores', fontsize = 13)\nsns.barplot(cv_scores_avg['Classifiers'], cv_scores_avg['test_f1_weighted'], ax = ax[3])\nax[3].set_title('F1_Scores Scores', fontsize = 13)\nsns.barplot(cv_scores_avg['Classifiers'], cv_scores_avg['test_roc_auc'], ax = ax[4])\nax[4].set_title('Auc-Roc Scores', fontsize = 13)\n\nplt.show()","432daece":"# CATBOOST CLASSIFIER (Oversampled data)\n\ncat = CatBoostClassifier(eval_metric = 'F1')\n\ncat_model_sm = cat.fit(X_train_sm,y_train_sm,\n                     eval_set = (X_val_sm,y_val_sm),\n                     use_best_model=True,\n                     verbose = 0,\n                     plot=True)","6516f67a":"# Preforming a Random Grid Search to find the best combination of parameters\n\ngrid = {'iterations': [500,1000],\n        'learning_rate': [0.01, 0.03, 0.05],\n        'depth': [2, 6, 10],\n        'l2_leaf_reg': [1, 3, 5, 9]}\n\nfinal_model = CatBoostClassifier()\nrandomized_search_result = final_model.randomized_search(grid,\n                                                   X = X_train_sm,\n                                                   y= y_train_sm,\n                                                   verbose = False,\n                                                   plot=False)\n\nbest_params = randomized_search_result['params']\nbest_params['loss_function'] = 'Logloss'\nbest_params['eval_metric'] = 'F1'","360691b4":"best_params","fdeb24f6":"from catboost import cv, Pool\n\ncv_dataset = Pool(data = X_train_sm,\n                  label = y_train_sm)\n\nparams = randomized_search_result['params']\n                  \n# params = {\"iterations\": 1000,\n#           \"learning_rate\": 0.03,\n#           'eval_metric': 'F1',\n#           \"depth\": 2,\n#           'l2_leaf_reg': 1,\n#           \"loss_function\": \"Logloss\",\n#           \"verbose\": False}\n\nscores = cv(cv_dataset,\n            params,\n            fold_count=10,\n            plot= False)\n\nscores","17dfcabb":"# Plotting F1_Score and Log Loss\n\nf, ax = plt.subplots(2,1, figsize = (15,8))\nplt.subplots_adjust(left=None, bottom=None, right=None, top=1.3, wspace=None, hspace=None)\n\nsns.lineplot(scores['iterations'],scores['test-F1-mean'], ci = 'sd', ax = ax[0])\nax[0].set_title('Catboost 10K Cross Val F1_SCore', fontsize = 14)\nax[0].fill_between(scores['iterations'], (scores['test-F1-mean'] + scores['test-F1-std']),\n                  (scores['test-F1-mean'] - scores['test-F1-std']), color='b', alpha=.2)\n                \nsns.lineplot(scores['iterations'],scores['test-Logloss-mean'], ax = ax[1])\nax[1].fill_between(scores['iterations'], (scores['test-Logloss-mean'] + scores['test-Logloss-std']),\n                  (scores['test-Logloss-mean'] - scores['test-Logloss-std']), color='b', alpha=.2)\nax[1].set_title('Catboost 10K Cross Val Log Loss', fontsize = 14)\nplt.show()","8b922f42":"cat_opt = CatBoostClassifier(**params)\n\ncat_model_opt = cat_opt.fit(X_train_sm,y_train_sm,\n                     eval_set = (X_val_sm,y_val_sm),\n                     use_best_model=True,\n                     verbose = 0,\n                     plot=True)","977d631c":"# Features' importance of our model\n\nfeat_imp = cat_model_opt.get_feature_importance(prettified=True)\n\n# Plotting top 20 features' importance\n\nplt.figure(figsize = (15,6))\nsns.barplot(feat_imp['Importances'],feat_imp['Feature Id'], orient = 'h')\nplt.show()","f9fb80f5":"# Testing\n\nrfc_pred_opt = rfc_model_opt.predict(X_test)\ncat_pred_opt = cat_model_opt.predict(X_test)\nlog_pred_opt = log_model_opt.predict(X_test)\n\n# Plotting the confusion matrix of the results\n\nconf_mx0 = confusion_matrix(y_test,log_pred_opt)\nconf_mx1 = confusion_matrix(y_test,rfc_pred_opt)\nconf_mx2 = confusion_matrix(y_test,cat_pred_opt)\n\nheat_cm0 = pd.DataFrame(conf_mx0, columns=np.unique(y_test), index = np.unique(y_test))\nheat_cm0.index.name = 'Actual'\nheat_cm0.columns.name = 'Predicted'\n\nheat_cm1 = pd.DataFrame(conf_mx1, columns=np.unique(y_test), index = np.unique(y_test))\nheat_cm1.index.name = 'Actual'\nheat_cm1.columns.name = 'Predicted'\n\nheat_cm2 = pd.DataFrame(conf_mx2, columns=np.unique(y_test), index = np.unique(y_test))\nheat_cm2.index.name = 'Actual'\nheat_cm2.columns.name = 'Predicted'\n\nf, ax = plt.subplots(1, 3, figsize=(12,8))\nf.subplots_adjust(left=None, bottom=None, right= 2, top=None, wspace=None, hspace= None)\n\nsns.heatmap(heat_cm0, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[0])\nax[0].set_title('Logistic Regression', fontsize = 15)\nsns.heatmap(heat_cm1, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[1])\nax[1].set_title('Random Forest Classifier', fontsize = 15)\nsns.heatmap(heat_cm2, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[2])\nax[2].set_title('Catboot Classifier', fontsize = 15)\n\nplt.show()","c5881fc2":"# Pretty table to sum up all the results\n\nmy_table = PrettyTable(['Algorithm (opt)','Overall Accuracy','Precision','Recall','F1_Score','Roc-Auc'])\n\nmy_table.add_row(['Logistic Regression',\n                 accuracy_score(y_test, log_pred_opt).round(4),\n                 precision_score(y_test, log_pred_opt, average=\"binary\", pos_label=\"1\").round(4),\n                 recall_score(y_test, log_pred_opt, average=\"binary\", pos_label=\"1\").round(4),\n                 f1_score(y_test,log_pred_opt, average='weighted').round(4),\n                 roc_auc_score(y_test,log_pred_opt, average='weighted').round(4)])\n\nmy_table.add_row(['Random Forest Classifier',\n                 accuracy_score(y_test, rfc_pred_opt).round(4),\n                 precision_score(y_test, rfc_pred_opt, average=\"binary\", pos_label=\"1\").round(4),\n                 recall_score(y_test, rfc_pred_opt, average=\"binary\", pos_label=\"1\").round(4),\n                 f1_score(y_test,rfc_pred_opt, average='weighted').round(4),\n                 roc_auc_score(y_test,rfc_pred_opt, average='weighted').round(4)])\n\nmy_table.add_row(['CatBoost Classifier',\n                 accuracy_score(y_test, cat_pred_opt).round(4),\n                 precision_score(y_test, cat_pred_opt, average=\"binary\", pos_label=\"1\").round(4),\n                 recall_score(y_test, cat_pred_opt, average=\"binary\", pos_label=\"1\").round(4),\n                 f1_score(y_test,cat_pred_opt, average='weighted').round(4),\n                 roc_auc_score(y_test,cat_pred_opt, average='weighted').round(4)])\n\n\n\nprint(my_table)","490a17ce":"**SMOTE**\n\nNow, before hyperparameters optimizations, I want to see if using SMOTE (an upsampling technique that generates data similar to the one present in the minority class until when the two, or more, classes will be paired forming a balanced dataset) we can incrase the baseline performance. ","f4bb9ec4":"Looking at the plot it is clear how we are facing an imbalanced dataset with a 2:1 ratio in favour of survived patient (0). ","ed3301c7":"## Heart Failure Prediction \ud83e\ude7a\n\n\nThe aim of this project is to use the fournished data to predict which patients could die becuase of heart failure. Let's start! \n\nP.s: If you like this notebook don't forget to **UPVOTE**!","9c7c80da":"Despite having non normal distribution yet, we can see that now the distributions are definitely closer to normality. In order to be sure of their normality we can use the statistical Shapiro test:  ","409451d4":"Now let's see the meaning of our categorical values:\n\n1. Boolean features\n        * Sex - Gender of patient Male = 1, Female =0\n        * Diabetes - 0 = No, 1 = Yes\n        * Anaemia - 0 = No, 1 = Yes\n        * High_blood_pressure - 0 = No, 1 = Yes\n        * Smoking - 0 = No, 1 = Yes\n        * DEATH_EVENT - 0 = No, 1 = Yes","87365451":"Exploratory Data Analysis\n\nThe EDA is a crucial process that helps us to better understand our data through graphical representations and that fournish us the opportunity to gain insight about them. We can use different types of visualizations to optimize the process. The main steps usually are:\n\n- Investigation of distributions\n- Class balancing (in classification tasks)\n- Outlier detection \n- Investigation of possible correlations","1ab8fa09":"<img src =\"http:\/\/25.media.tumblr.com\/tumblr_m9kolagxR81qfvx4yo1_400.gif\">","9020acc2":"As you can see, despite some features are close to normality, they cannot be consider as such. Anyway, let's see what performances we can obtain using these basilcally raw data.","05f966e2":"Now that we have our two optimized models, let's see if we can increase the perfomance using a different classifier: **CATBOOST**","c33c57e3":"In this part of the notebook, I will try to use several algorithms to see which is the most efficient and which are their strengths and weaknesses. In order to submit the results to the two opened tasks for this competition I deciced to use three different models:\n\n- *Logistic Regression*\n- *Random Forest Classifier*\n- *Catboost Classifier*\n\nLet's start modifying data to make them usable by these classifiers (Cabtoost is the only one who can automatically deal with outliers and categorical features, but the other two can't). In order to make this happen, we need to preprocess the categorical features manually using dummy coding. ","a79f10b2":"**TESTING**\n\nNow that we have different optimized model, we can see how they perform on the test data to make our final considerations!","120c1b4d":"The correlation map confirms what found with the pairplot. No meaningful correlations are present among our data.","db5408d7":"Let's take a look at our data:","ec4888fa":"# Modeling","5bed4b20":"Now that our data are ready to be used, let's start to model. The first part will be focused on obtaining a **baseline value** for each model used with its default parameters. In order to have robust results I decided to apply 10-Fold Cross validation, meaning that we'll fit and evaluate our model on 10 different folds of train and validation data:","5f328414":"Now we have our baselines values for normal and Smote dataset. From this first analysis, we can see how the Random Forest Classifier with upsampled (smote) data seems to be the most promising. The cost of this efficiency is paid in terms of time, if we look at the second plot we can see how Random Forest Classifier is 2 times slower than the Logistic Regression.  Now,in this context this is negligible, but time is an important variable to consider when we train deep models. Let's try to optimize them!\n\n**RANDOMIZED GRID SEARCH OPTIMIZATION:**","2ac6c7e3":"The pairplot show us the histograms of each numerical variable and the scatterplots representig their correlations. Looking at it we can see how our data do not seem to be normally distributed and that no clear positive or negative correlations are visible. Let's investigate a little more!"}}