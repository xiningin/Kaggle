{"cell_type":{"7b397348":"code","51ff9284":"code","3a98dbf6":"code","f19fe216":"code","b72263f7":"code","de43266f":"code","08565d98":"code","de0e0478":"code","dc7df395":"code","7df99286":"code","ca58a481":"code","4ff80e44":"code","e253e366":"code","91e1e2e7":"code","770100e1":"code","3d9877b8":"code","4fab0df6":"code","fc773106":"markdown","f029241f":"markdown","36d427eb":"markdown","d4de1309":"markdown","b3ee82dc":"markdown","5a06361a":"markdown","dea784ff":"markdown","7d35cf39":"markdown","b0b801a2":"markdown","58181da3":"markdown","d5cf59dd":"markdown","396eb4b0":"markdown","93bae0b4":"markdown"},"source":{"7b397348":"import numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\n\n# Read in data files\nBASE_DIR = '..\/input\/nfl-health-and-safety-helmet-assignment'\n\n# Labels and sample submission\nlabels = pd.read_csv(f'{BASE_DIR}\/train_labels.csv')\nss = pd.read_csv(f'{BASE_DIR}\/sample_submission.csv')\n\n# Player tracking data\ntr_tracking = pd.read_csv(f'{BASE_DIR}\/train_player_tracking.csv')\nte_tracking = pd.read_csv(f'{BASE_DIR}\/test_player_tracking.csv')\n\n# Baseline helmet detection labels\ntr_helmets = pd.read_csv(f'{BASE_DIR}\/train_baseline_helmets.csv')\nte_helmets = pd.read_csv(f'{BASE_DIR}\/test_baseline_helmets.csv')\n\n# Extra image labels\nimg_labels = pd.read_csv(f'{BASE_DIR}\/image_labels.csv')","51ff9284":"def check_submission(sub):\n    \"\"\"\n    Checks that the submission meets all the requirements.\n\n    1. No more than 22 Boxes per frame.\n    2. Only one label prediction per video\/frame\n    3. No duplicate boxes per frame.\n\n    Returns:\n        True -> Passed the tests\n        False -> Failed the test\n    \"\"\"\n    # Maximum of 22 boxes per frame.\n    max_box_per_frame = sub.groupby([\"video_frame\"])[\"label\"].count().max()\n    if max_box_per_frame > 22:\n        print(\"Has more than 22 boxes in a single frame\")\n        return False\n    # Only one label allowed per frame.\n    has_duplicate_labels = sub[[\"video_frame\", \"label\"]].duplicated().any()\n    if has_duplicate_labels:\n        print(\"Has duplicate labels\")\n        return False\n    # Check for unique boxes\n    has_duplicate_boxes = (\n        sub[[\"video_frame\", \"left\", \"width\", \"top\", \"height\"]].duplicated().any()\n    )\n    if has_duplicate_boxes:\n        print(\"Has duplicate boxes\")\n        return False\n    return True\n","3a98dbf6":"# The sample submission meets these requirements.\ncheck_submission(ss)","f19fe216":"from sklearn.metrics import accuracy_score\nimport pandas as pd\nimport numpy as np\n\n\nclass NFLAssignmentScorer:\n    def __init__(\n        self,\n        labels_df: pd.DataFrame = None,\n        labels_csv=\"train_labels.csv\",\n        check_constraints=True,\n        weight_col=\"isDefinitiveImpact\",\n        impact_weight=1000,\n        iou_threshold=0.35,\n        remove_sideline=True,\n    ):\n        \"\"\"\n        Helper class for grading submissions in the\n        2021 Kaggle Competition for helmet assignment.\n        Version 1.0\n        https:\/\/www.kaggle.com\/robikscube\/nfl-helmet-assignment-getting-started-guide\n\n        Use:\n        ```\n        scorer = NFLAssignmentScorer(labels)\n        scorer.score(submission_df)\n\n        or\n\n        scorer = NFLAssignmentScorer(labels_csv='labels.csv')\n        scorer.score(submission_df)\n        ```\n\n        Args:\n            labels_df (pd.DataFrame, optional):\n                Dataframe containing theground truth label boxes.\n            labels_csv (str, optional): CSV of the ground truth label.\n            check_constraints (bool, optional): Tell the scorer if it\n                should check the submission file to meet the competition\n                constraints. Defaults to True.\n            weight_col (str, optional):\n                Column in the labels DataFrame used to applying the scoring\n                weight.\n            impact_weight (int, optional):\n                The weight applied to impacts in the scoring metrics.\n                Defaults to 1000.\n            iou_threshold (float, optional):\n                The minimum IoU allowed to correctly pair a ground truth box\n                with a label. Defaults to 0.35.\n            remove_sideline (bool, optional):\n                Remove slideline players from the labels DataFrame\n                before scoring.\n        \"\"\"\n        if labels_df is None:\n            # Read label from CSV\n            if labels_csv is None:\n                raise Exception(\"labels_df or labels_csv must be provided\")\n            else:\n                self.labels_df = pd.read_csv(labels_csv)\n        else:\n            self.labels_df = labels_df.copy()\n        if remove_sideline:\n            self.labels_df = (\n                self.labels_df.query(\"isSidelinePlayer == False\")\n                .reset_index(drop=True)\n                .copy()\n            )\n        self.impact_weight = impact_weight\n        self.check_constraints = check_constraints\n        self.weight_col = weight_col\n        self.iou_threshold = iou_threshold\n\n    def check_submission(self, sub):\n        \"\"\"\n        Checks that the submission meets all the requirements.\n\n        1. No more than 22 Boxes per frame.\n        2. Only one label prediction per video\/frame\n        3. No duplicate boxes per frame.\n\n        Args:\n            sub : submission dataframe.\n\n        Returns:\n            True -> Passed the tests\n            False -> Failed the test\n        \"\"\"\n        # Maximum of 22 boxes per frame.\n        max_box_per_frame = sub.groupby([\"video_frame\"])[\"label\"].count().max()\n        if max_box_per_frame > 22:\n            print(\"Has more than 22 boxes in a single frame\")\n            return False\n        # Only one label allowed per frame.\n        has_duplicate_labels = sub[[\"video_frame\", \"label\"]].duplicated().any()\n        if has_duplicate_labels:\n            print(\"Has duplicate labels\")\n            return False\n        # Check for unique boxes\n        has_duplicate_boxes = (\n            sub[[\"video_frame\", \"left\", \"width\", \"top\", \"height\"]].duplicated().any()\n        )\n        if has_duplicate_boxes:\n            print(\"Has duplicate boxes\")\n            return False\n        return True\n\n    def add_xy(self, df):\n        \"\"\"\n        Adds `x1`, `x2`, `y1`, and `y2` columns necessary for computing IoU.\n\n        Note - for pixel math, 0,0 is the top-left corner so box orientation\n        defined as right and down (height)\n        \"\"\"\n\n        df[\"x1\"] = df[\"left\"]\n        df[\"x2\"] = df[\"left\"] + df[\"width\"]\n        df[\"y1\"] = df[\"top\"]\n        df[\"y2\"] = df[\"top\"] + df[\"height\"]\n        return df\n\n    def merge_sub_labels(self, sub, labels, weight_col=\"isDefinitiveImpact\"):\n        \"\"\"\n        Perform an outer join between submission and label.\n        Creates a `sub_label` dataframe which stores the matched label for each submission box.\n        Ground truth values are given the `_gt` suffix, submission values are given `_sub` suffix.\n        \"\"\"\n        sub = sub.copy()\n        labels = labels.copy()\n\n        sub = self.add_xy(sub)\n        labels = self.add_xy(labels)\n\n        base_columns = [\n            \"label\",\n            \"video_frame\",\n            \"x1\",\n            \"x2\",\n            \"y1\",\n            \"y2\",\n            \"left\",\n            \"width\",\n            \"top\",\n            \"height\",\n        ]\n\n        sub_labels = sub[base_columns].merge(\n            labels[base_columns + [weight_col]],\n            on=[\"video_frame\"],\n            how=\"right\",\n            suffixes=(\"_sub\", \"_gt\"),\n        )\n        return sub_labels\n\n    def get_iou_df(self, df):\n        \"\"\"\n        This function computes the IOU of submission (sub)\n        bounding boxes against the ground truth boxes (gt).\n        \"\"\"\n        df = df.copy()\n\n        # 1. get the coordinate of inters\n        df[\"ixmin\"] = df[[\"x1_sub\", \"x1_gt\"]].max(axis=1)\n        df[\"ixmax\"] = df[[\"x2_sub\", \"x2_gt\"]].min(axis=1)\n        df[\"iymin\"] = df[[\"y1_sub\", \"y1_gt\"]].max(axis=1)\n        df[\"iymax\"] = df[[\"y2_sub\", \"y2_gt\"]].min(axis=1)\n\n        df[\"iw\"] = np.maximum(df[\"ixmax\"] - df[\"ixmin\"] + 1, 0.0)\n        df[\"ih\"] = np.maximum(df[\"iymax\"] - df[\"iymin\"] + 1, 0.0)\n\n        # 2. calculate the area of inters\n        df[\"inters\"] = df[\"iw\"] * df[\"ih\"]\n\n        # 3. calculate the area of union\n        df[\"uni\"] = (\n            (df[\"x2_sub\"] - df[\"x1_sub\"] + 1) * (df[\"y2_sub\"] - df[\"y1_sub\"] + 1)\n            + (df[\"x2_gt\"] - df[\"x1_gt\"] + 1) * (df[\"y2_gt\"] - df[\"y1_gt\"] + 1)\n            - df[\"inters\"]\n        )\n        # print(uni)\n        # 4. calculate the overlaps between pred_box and gt_box\n        df[\"iou\"] = df[\"inters\"] \/ df[\"uni\"]\n\n        return df.drop(\n            [\"ixmin\", \"ixmax\", \"iymin\", \"iymax\", \"iw\", \"ih\", \"inters\", \"uni\"], axis=1\n        )\n\n    def filter_to_top_label_match(self, sub_labels):\n        \"\"\"\n        Ensures ground truth boxes are only linked to the box\n        in the submission file with the highest IoU.\n        \"\"\"\n        return (\n            sub_labels.sort_values(\"iou\", ascending=False)\n            .groupby([\"video_frame\", \"label_gt\"])\n            .first()\n            .reset_index()\n        )\n\n    def add_isCorrect_col(self, sub_labels):\n        \"\"\"\n        Adds True\/False column if the ground truth label\n        and submission label are identical\n        \"\"\"\n        sub_labels[\"isCorrect\"] = (\n            sub_labels[\"label_gt\"] == sub_labels[\"label_sub\"]\n        ) & (sub_labels[\"iou\"] >= self.iou_threshold)\n        return sub_labels\n\n    def calculate_metric_weighted(\n        self, sub_labels, weight_col=\"isDefinitiveImpact\", weight=1000\n    ):\n        \"\"\"\n        Calculates weighted accuracy score metric.\n        \"\"\"\n        sub_labels[\"weight\"] = sub_labels.apply(\n            lambda x: weight if x[weight_col] else 1, axis=1\n        )\n        y_pred = sub_labels[\"isCorrect\"].values\n        y_true = np.ones_like(y_pred)\n        weight = sub_labels[\"weight\"]\n        return accuracy_score(y_true, y_pred, sample_weight=weight)\n\n    def score(self, sub, labels_df=None, drop_extra_cols=True):\n        \"\"\"\n        Scores the submission file against the labels.\n\n        Returns the evaluation metric score for the helmet\n        assignment kaggle competition.\n\n        If `check_constraints` is set to True, will return -999 if the\n            submission fails one of the submission constraints.\n        \"\"\"\n        if labels_df is None:\n            labels_df = self.labels_df.copy()\n\n        if self.check_constraints:\n            if not self.check_submission(sub):\n                return -999\n        sub_labels = self.merge_sub_labels(sub, labels_df, self.weight_col)\n        sub_labels = self.get_iou_df(sub_labels).copy()\n        sub_labels = self.filter_to_top_label_match(sub_labels).copy()\n        sub_labels = self.add_isCorrect_col(sub_labels)\n        score = self.calculate_metric_weighted(\n            sub_labels, self.weight_col, self.impact_weight\n        )\n        # Keep `sub_labels for review`\n        if drop_extra_cols:\n            drop_cols = [\n                \"x1_sub\",\n                \"x2_sub\",\n                \"y1_sub\",\n                \"y2_sub\",\n                \"x1_gt\",\n                \"x2_gt\",\n                \"y1_gt\",\n                \"y2_gt\",\n            ]\n            sub_labels = sub_labels.drop(drop_cols, axis=1)\n        self.sub_labels = sub_labels\n        return score\n","b72263f7":"SUB_COLUMNS = ss.columns # Expected submission columns\nscorer = NFLAssignmentScorer(labels)\n\n# Score the sample submission\nss_score = scorer.score(ss)\nprint(f'Sample submission scores: {ss_score:0.4f}')\n\n# Score a submission with only impacts\nperfect_impacts = labels.query('isDefinitiveImpact == True and isSidelinePlayer == False')\nimp_score = scorer.score(perfect_impacts)\nprint(f'A submission with perfect predictions only for impacts scores: {imp_score:0.4f}')\n\n# Score a submission with only non-impacts\nperfect_nonimpacts = labels.query('isDefinitiveImpact == False and isSidelinePlayer == False')\nnonimp_score = scorer.score(perfect_nonimpacts)\nprint(f'A submission with perfect predictions only for non-impacts scores: {nonimp_score:0.4f}')\n\n# Score a perfect submission\nperfect_train = labels.query('isSidelinePlayer == False')[SUB_COLUMNS].copy()\nperfect_score = scorer.score(perfect_train)\nprint(f'A perfrect training submission scores: {perfect_score:0.4f}')","de43266f":"scorer.sub_labels.head()","08565d98":"import os\nimport cv2\nimport subprocess\nfrom IPython.display import Video, display\nimport pandas as pd\n\n\ndef video_with_baseline_boxes(\n    video_path: str, baseline_boxes: pd.DataFrame, gt_labels: pd.DataFrame, verbose=True\n) -> str:\n    \"\"\"\n    Annotates a video with both the baseline model boxes and ground truth boxes.\n    Baseline model prediction confidence is also displayed.\n    \"\"\"\n    VIDEO_CODEC = \"MP4V\"\n    HELMET_COLOR = (0, 0, 0)  # Black\n    BASELINE_COLOR = (255, 255, 255)  # White\n    IMPACT_COLOR = (0, 0, 255)  # Red\n    video_name = os.path.basename(video_path).replace(\".mp4\", \"\")\n    if verbose:\n        print(f\"Running for {video_name}\")\n    baseline_boxes = baseline_boxes.copy()\n    gt_labels = gt_labels.copy()\n\n    baseline_boxes[\"video\"] = (\n        baseline_boxes[\"video_frame\"].str.split(\"_\").str[:3].str.join(\"_\")\n    )\n    gt_labels[\"video\"] = gt_labels[\"video_frame\"].str.split(\"_\").str[:3].str.join(\"_\")\n    baseline_boxes[\"frame\"] = (\n        baseline_boxes[\"video_frame\"].str.split(\"_\").str[-1].astype(\"int\")\n    )\n    gt_labels[\"frame\"] = gt_labels[\"video_frame\"].str.split(\"_\").str[-1].astype(\"int\")\n\n    vidcap = cv2.VideoCapture(video_path)\n    fps = vidcap.get(cv2.CAP_PROP_FPS)\n    width = int(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    output_path = \"labeled_\" + video_name + \".mp4\"\n    tmp_output_path = \"tmp_\" + output_path\n    output_video = cv2.VideoWriter(\n        tmp_output_path, cv2.VideoWriter_fourcc(*VIDEO_CODEC), fps, (width, height)\n    )\n    frame = 0\n    while True:\n        it_worked, img = vidcap.read()\n        if not it_worked:\n            break\n        # We need to add 1 to the frame count to match the label frame index\n        # that starts at 1\n        frame += 1\n\n        # Let's add a frame index to the video so we can track where we are\n        img_name = f\"{video_name}_frame{frame}\"\n        cv2.putText(\n            img,\n            img_name,\n            (0, 50),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            1.0,\n            HELMET_COLOR,\n            thickness=2,\n        )\n\n        # Now, add the boxes\n        boxes = baseline_boxes.query(\"video == @video_name and frame == @frame\")\n        if len(boxes) == 0:\n            print(\"Boxes incorrect\")\n            return\n        for box in boxes.itertuples(index=False):\n            cv2.rectangle(\n                img,\n                (box.left, box.top),\n                (box.left + box.width, box.top + box.height),\n                BASELINE_COLOR,\n                thickness=1,\n            )\n            cv2.putText(\n                img,\n                f\"{box.conf:0.2}\",\n                (box.left, max(0, box.top - 5)),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                0.5,\n                BASELINE_COLOR,\n                thickness=1,\n            )\n\n        boxes = gt_labels.query(\"video == @video_name and frame == @frame\")\n        if len(boxes) == 0:\n            print(\"Boxes incorrect\")\n            return\n        for box in boxes.itertuples(index=False):\n            # Filter for definitive head impacts and turn labels red\n            if box.isDefinitiveImpact == True:\n                color, thickness = IMPACT_COLOR, 3\n            else:\n                color, thickness = HELMET_COLOR, 1\n            cv2.rectangle(\n                img,\n                (box.left, box.top),\n                (box.left + box.width, box.top + box.height),\n                color,\n                thickness=thickness,\n            )\n            cv2.putText(\n                img,\n                box.label,\n                (box.left + 1, max(0, box.top - 20)),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                0.5,\n                color,\n                thickness=1,\n            )\n\n        output_video.write(img)\n    output_video.release()\n    # Not all browsers support the codec, we will re-load the file at tmp_output_path\n    # and convert to a codec that is more broadly readable using ffmpeg\n    if os.path.exists(output_path):\n        os.remove(output_path)\n    subprocess.run(\n        [\n            \"ffmpeg\",\n            \"-i\",\n            tmp_output_path,\n            \"-crf\",\n            \"18\",\n            \"-preset\",\n            \"veryfast\",\n            \"-vcodec\",\n            \"libx264\",\n            output_path,\n        ]\n    )\n    os.remove(tmp_output_path)\n\n    return output_path\n","de0e0478":"example_video = '..\/input\/nfl-health-and-safety-helmet-assignment\/train\/57584_000336_Sideline.mp4'\noutput_video = video_with_baseline_boxes(example_video,\n                          tr_helmets, labels)\n\nfrac = 0.65 # scaling factor for display\ndisplay(Video(data=output_video,\n              embed=True,\n              height=int(720*frac),\n              width=int(1280*frac))\n       )","dc7df395":"def add_track_features(tracks, fps=59.94, snap_frame=10):\n    \"\"\"\n    Add column features helpful for syncing with video data.\n    \"\"\"\n    tracks = tracks.copy()\n    tracks[\"game_play\"] = (\n        tracks[\"gameKey\"].astype(\"str\")\n        + \"_\"\n        + tracks[\"playID\"].astype(\"str\").str.zfill(6)\n    )\n    tracks[\"time\"] = pd.to_datetime(tracks[\"time\"])\n    snap_dict = (\n        tracks.query('event == \"ball_snap\"')\n        .groupby(\"game_play\")[\"time\"]\n        .first()\n        .to_dict()\n    )\n    tracks[\"snap\"] = tracks[\"game_play\"].map(snap_dict)\n    tracks[\"isSnap\"] = tracks[\"snap\"] == tracks[\"time\"]\n    tracks[\"team\"] = tracks[\"player\"].str[0].replace(\"H\", \"Home\").replace(\"V\", \"Away\")\n    tracks[\"snap_offset\"] = (tracks[\"time\"] - tracks[\"snap\"]).astype(\n        \"timedelta64[ms]\"\n    ) \/ 1_000\n    # Estimated video frame\n    tracks[\"est_frame\"] = (\n        ((tracks[\"snap_offset\"] * fps) + snap_frame).round().astype(\"int\")\n    )\n    return tracks\n\n\ntr_tracking = add_track_features(tr_tracking)\nte_tracking = add_track_features(te_tracking)\n","7df99286":"import matplotlib.patches as patches\nimport matplotlib.pylab as plt\n\ndef create_football_field(\n    linenumbers=True,\n    endzones=True,\n    highlight_line=False,\n    highlight_line_number=50,\n    highlighted_name=\"Line of Scrimmage\",\n    fifty_is_los=False,\n    figsize=(12, 6.33),\n    field_color=\"lightgreen\",\n    ez_color='forestgreen',\n    ax=None,\n):\n    \"\"\"\n    Function that plots the football field for viewing plays.\n    Allows for showing or hiding endzones.\n    \"\"\"\n    rect = patches.Rectangle(\n        (0, 0),\n        120,\n        53.3,\n        linewidth=0.1,\n        edgecolor=\"r\",\n        facecolor=field_color,\n        zorder=0,\n    )\n\n    if ax is None:\n        fig, ax = plt.subplots(1, figsize=figsize)\n    ax.add_patch(rect)\n\n    plt.plot([10, 10, 10, 20, 20, 30, 30, 40, 40, 50, 50, 60, 60, 70, 70, 80,\n              80, 90, 90, 100, 100, 110, 110, 120, 0, 0, 120, 120],\n             [0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3,\n              53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 53.3, 0, 0, 53.3],\n             color='black')\n\n    if fifty_is_los:\n        ax.plot([60, 60], [0, 53.3], color=\"gold\")\n        ax.text(62, 50, \"<- Player Yardline at Snap\", color=\"gold\")\n    # Endzones\n    if endzones:\n        ez1 = patches.Rectangle(\n            (0, 0),\n            10,\n            53.3,\n            linewidth=0.1,\n            edgecolor=\"black\",\n            facecolor=ez_color,\n            alpha=0.6,\n            zorder=0,\n        )\n        ez2 = patches.Rectangle(\n            (110, 0),\n            120,\n            53.3,\n            linewidth=0.1,\n            edgecolor=\"black\",\n            facecolor=ez_color,\n            alpha=0.6,\n            zorder=0,\n        )\n        ax.add_patch(ez1)\n        ax.add_patch(ez2)\n    ax.axis(\"off\")\n    if linenumbers:\n        for x in range(20, 110, 10):\n            numb = x\n            if x > 50:\n                numb = 120 - x\n            ax.text(\n                x,\n                5,\n                str(numb - 10),\n                horizontalalignment=\"center\",\n                fontsize=20,  # fontname='Arial',\n                color=\"black\",\n            )\n            ax.text(\n                x - 0.95,\n                53.3 - 5,\n                str(numb - 10),\n                horizontalalignment=\"center\",\n                fontsize=20,  # fontname='Arial',\n                color=\"black\",\n                rotation=180,\n            )\n    if endzones:\n        hash_range = range(11, 110)\n    else:\n        hash_range = range(1, 120)\n\n    for x in hash_range:\n        ax.plot([x, x], [0.4, 0.7], color=\"black\")\n        ax.plot([x, x], [53.0, 52.5], color=\"black\")\n        ax.plot([x, x], [22.91, 23.57], color=\"black\")\n        ax.plot([x, x], [29.73, 30.39], color=\"black\")\n\n    if highlight_line:\n        hl = highlight_line_number + 10\n        ax.plot([hl, hl], [0, 53.3], color=\"yellow\")\n        ax.text(hl + 2, 50, \"<- {}\".format(highlighted_name), color=\"yellow\")\n\n    border = patches.Rectangle(\n        (-5, -5),\n        120 + 10,\n        53.3 + 10,\n        linewidth=0.1,\n        edgecolor=\"orange\",\n        facecolor=\"white\",\n        alpha=0,\n        zorder=0,\n    )\n    ax.add_patch(border)\n    ax.set_xlim((0, 120))\n    ax.set_ylim((0, 53.3))\n    return ax","ca58a481":"game_play = \"57584_000336\"\nexample_tracks = tr_tracking.query(\"game_play == @game_play and isSnap == True\")\nax = create_football_field()\nfor team, d in example_tracks.groupby(\"team\"):\n    ax.scatter(d[\"x\"], d[\"y\"], label=team, s=65, lw=1, edgecolors=\"black\", zorder=5)\nax.legend().remove()\nax.set_title(f\"Tracking data for {game_play}: at snap\", fontsize=15)\nplt.show()\n","4ff80e44":"cap = cv2.VideoCapture(\n    \"..\/input\/nfl-health-and-safety-helmet-assignment\/train\/57584_000336_Endzone.mp4\"\n)\ncap.get(10)\n_, ez_snap_img = cap.read()\n\ncap = cv2.VideoCapture(\n    \"..\/input\/nfl-health-and-safety-helmet-assignment\/train\/57584_000336_Sideline.mp4\"\n)\ncap.get(10)\n_, sl_snap_img = cap.read()\n\nfig, axs = plt.subplots(2, 1, figsize=(15, 15))\n\naxs[0].imshow(cv2.cvtColor(ez_snap_img, cv2.COLOR_BGR2RGB))\naxs[0].axis(\"off\")\naxs[0].set_title(f\"57584_000336_Endzone.mp4 at snap\", fontsize=20)\naxs[1].imshow(cv2.cvtColor(sl_snap_img, cv2.COLOR_BGR2RGB))\naxs[1].set_title(f\"57584_000336_Sideline.mp4 at snap\", fontsize=20)\naxs[1].axis(\"off\")\nplt.tight_layout()\n","e253e366":"import plotly.express as px\nimport plotly.graph_objects as go\nimport plotly\n\n\ndef add_plotly_field(fig):\n    # Reference https:\/\/www.kaggle.com\/ammarnassanalhajali\/nfl-big-data-bowl-2021-animating-players\n    fig.update_traces(marker_size=20)\n    \n    fig.update_layout(paper_bgcolor='#29a500', plot_bgcolor='#29a500', font_color='white',\n        width = 800,\n        height = 600,\n        title = \"\",\n        \n        xaxis = dict(\n        nticks = 10,\n        title = \"\",\n        visible=False\n        ),\n        \n        yaxis = dict(\n        scaleanchor = \"x\",\n        title = \"Temp\",\n        visible=False\n        ),\n        showlegend= True,\n  \n        annotations=[\n       dict(\n            x=-5,\n            y=26.65,\n            xref=\"x\",\n            yref=\"y\",\n            text=\"ENDZONE\",\n            font=dict(size=16,color=\"#e9ece7\"),\n            align='center',\n            showarrow=False,\n            yanchor='middle',\n            textangle=-90\n        ),\n        dict(\n            x=105,\n            y=26.65,\n            xref=\"x\",\n            yref=\"y\",\n            text=\"ENDZONE\",\n            font=dict(size=16,color=\"#e9ece7\"),\n            align='center',\n            showarrow=False,\n            yanchor='middle',\n            textangle=90\n        )]  \n        ,\n        legend=dict(\n        traceorder=\"normal\",\n        font=dict(family=\"sans-serif\",size=12),\n        title = \"\",\n        orientation=\"h\",\n        yanchor=\"bottom\",\n        y=1.00,\n        xanchor=\"center\",\n        x=0.5\n        ),\n    )\n    ####################################################\n        \n    fig.add_shape(type=\"rect\", x0=-10, x1=0,  y0=0, y1=53.3,line=dict(color=\"#c8ddc0\",width=3),fillcolor=\"#217b00\" ,layer=\"below\")\n    fig.add_shape(type=\"rect\", x0=100, x1=110, y0=0, y1=53.3,line=dict(color=\"#c8ddc0\",width=3),fillcolor=\"#217b00\" ,layer=\"below\")\n    for x in range(0, 100, 10):\n        fig.add_shape(type=\"rect\", x0=x,   x1=x+10, y0=0, y1=53.3,line=dict(color=\"#c8ddc0\",width=3),fillcolor=\"#29a500\" ,layer=\"below\")\n    for x in range(0, 100, 1):\n        fig.add_shape(type=\"line\",x0=x, y0=1, x1=x, y1=2,line=dict(color=\"#c8ddc0\",width=2),layer=\"below\")\n    for x in range(0, 100, 1):\n        fig.add_shape(type=\"line\",x0=x, y0=51.3, x1=x, y1=52.3,line=dict(color=\"#c8ddc0\",width=2),layer=\"below\")\n    \n    for x in range(0, 100, 1):\n        fig.add_shape(type=\"line\",x0=x, y0=20.0, x1=x, y1=21,line=dict(color=\"#c8ddc0\",width=2),layer=\"below\")\n    for x in range(0, 100, 1):\n        fig.add_shape(type=\"line\",x0=x, y0=32.3, x1=x, y1=33.3,line=dict(color=\"#c8ddc0\",width=2),layer=\"below\")\n    \n    \n    fig.add_trace(go.Scatter(\n    x=[2,10,20,30,40,50,60,70,80,90,98], y=[5,5,5,5,5,5,5,5,5,5,5],\n    text=[\"G\",\"1 0\",\"2 0\",\"3 0\",\"4 0\",\"5 0\",\"4 0\",\"3 0\",\"2 0\",\"1 0\",\"G\"],\n    mode=\"text\",\n    textfont=dict(size=20,family=\"Arail\"),\n    showlegend=False,\n    ))\n    \n    fig.add_trace(go.Scatter(\n    x=[2,10,20,30,40,50,60,70,80,90,98], y=[48.3,48.3,48.3,48.3,48.3,48.3,48.3,48.3,48.3,48.3,48.3],\n    text=[\"G\",\"1 0\",\"2 0\",\"3 0\",\"4 0\",\"5 0\",\"4 0\",\"3 0\",\"2 0\",\"1 0\",\"G\"],\n    mode=\"text\",\n    textfont=dict(size=20,family=\"Arail\"),\n    showlegend=False,\n    ))\n    \n    return fig\n","91e1e2e7":"tr_tracking[\"track_time_count\"] = (\n    tr_tracking.sort_values(\"time\")\n    .groupby(\"game_play\")[\"time\"]\n    .rank(method=\"dense\")\n    .astype(\"int\")\n)\n\nfig = px.scatter(\n    tr_tracking.query(\"game_play == @game_play\"),\n    x=\"x\",\n    y=\"y\",\n    range_x=[-10, 110],\n    range_y=[-10, 53.3],\n    hover_data=[\"player\", \"s\", \"a\", \"dir\"],\n    color=\"team\",\n    animation_frame=\"track_time_count\",\n    text=\"player\",\n    title=f\"Animation of NGS data for game_play {game_play}\",\n)\n\nfig.update_traces(textfont_size=10)\nfig = add_plotly_field(fig)\nfig.show()\n","770100e1":"def random_label_submission(helmets, tracks):\n    \"\"\"\n    Creates a baseline submission with randomly assigned helmets\n    based on the top 22 most confident baseline helmet boxes for\n    a frame.\n    \"\"\"\n    # Take up to 22 helmets per frame based on confidence:\n    helm_22 = (\n        helmets.sort_values(\"conf\", ascending=False)\n        .groupby(\"video_frame\")\n        .head(22)\n        .sort_values(\"video_frame\")\n        .reset_index(drop=True)\n        .copy()\n    )\n    # Identify player label choices for each game_play\n    game_play_choices = tracks.groupby([\"game_play\"])[\"player\"].unique().to_dict()\n    # Loop through frames and randomly assign boxes\n    ds = []\n    helm_22[\"label\"] = np.nan\n    for video_frame, data in helm_22.groupby(\"video_frame\"):\n        game_play = video_frame[:12]\n        choices = game_play_choices[game_play]\n        np.random.shuffle(choices)\n        data[\"label\"] = choices[: len(data)]\n        ds.append(data)\n    submission = pd.concat(ds)\n    return submission\n","3d9877b8":"train_submission = random_label_submission(tr_helmets, tr_tracking)\nscorer = NFLAssignmentScorer(labels)\nbaseline_score = scorer.score(train_submission)\nprint(f\"The score of random labels on the training set is {baseline_score:0.4f}\")\n","4fab0df6":"te_tracking = add_track_features(te_tracking)\nrandom_submission = random_label_submission(te_helmets, te_tracking)\n# Check to make sure it meets the submission requirements.\nassert check_submission(random_submission)\nrandom_submission[ss.columns].to_csv(\"submission.csv\", index=False)\n","fc773106":"## Example Animation of NGS Data\n\nBelow is an animation of an example play using plotly. Thanks to this notebook author for creating the awesome plotly football field:\n\nhttps:\/\/www.kaggle.com\/ammarnassanalhajali\/nfl-big-data-bowl-2021-animating-players","f029241f":"After scoring, the `sub_labels` dataframe within the `NFLAssignmentScorer` object can be used to evaluate results including the `iou` between predictions and ground truth boxes and `isCorrect` for correct labels. Ground truth fields have the suffix `_gt` while submission fields have the suffix `_sub`.","36d427eb":"# What data are provided?\n\nFor the full data details, please review the [data description page](https:\/\/www.kaggle.com\/c\/nfl-health-and-safety-helmet-assignment\/data).\n\n*Note: This is a code competition. When you submit, your model will be rerun on a set of 15 unseen plays located in a holdout test set. The publicly provided test videos are simply a set of mock plays (copied from the training set) which are not used in scoring.*\n\n- `\/train\/` and `\/test\/` folders contain the video mp4 files to be labeled.\n- `train_labels.csv` - This file is only available for the training dataset and provides the ground truth for the 120 training videos.\n- `train_player_tracking.csv` and `test_player_tracking.csv` contain the tracking data for all players on the field during the play.\n- `train_baseline_helmets.csv` and `test_baseline_helmets.csv` contain imperfect baseline predictions for helmet boxes. The model used to create these files was trained only on the additional images found in the images folder. If you so choose, you may train your own helmet detection model and ignore these files.\n\nExtra Images:\n- The `\/images\/` folder and `image_labels.csv` contains helmet boxes for random frames in videos. These images were used to train the model that produced the `*_baseline_helmets.csv` files. You may choose to use these to train your own helmet detection model.","d4de1309":"The provided `NFLImpactScorer` class can be used to assist in scoring your local predictions. Note that this is not the exact code used in the kaggle scoring system, but results should be nearly identical.","b3ee82dc":"# How does scoring work?\n\nThe scoring used for this competition is `Weighted Accuracy`, where detected helmets involved in an impact are weighted 1000x more than non-impact helmets. This scoring is designed specifically because ultimately the NFL will be using this algorithm to assign helmet impacts to players.\n\nTo reduce gamification of this metric additional restrictions are placed on submissions. They are:\n1. Submission boxes must have at least a 0.35 [Intersection over Union (IoU)](https:\/\/en.wikipedia.org\/wiki\/Jaccard_index) with the ground truth helmet box.\n2. Each ground truth helmet box will only be paired with one helmet box per frame in the submitted solution.  For each ground truth box, the submitted box with the highest IoU will be considered for scoring.\n3. No more than 22 helmet predictions per video frame (the maximum number of players participating on field at a time). In some situations, sideline players can be seen in the video footage. Sideline players' helmets are not scored in the grading algorithm and can be ignored. Sideline players will have the helmet labels \"H00\" and \"V00\". Sideline players should not be included in the submission to avoid exceeding the 22-box and unique label constraints.\n4. A player's helmet label must only be predicted once per video frame, i.e. no duplicated labels per frame.\n5. All submitted helmet boxes must be unique per video frame, i.e. no identical (left, right, height and width) boxes per frame.\n\n\nThe `check_submission` function can be used to check if your submission meets the above requirements.","5a06361a":"# What next?\nThere are infinite ways of approaching this problem. Some ideas that can be explored are:\n- Using optimization methods to match helmet boxes to NGS data based on relative distances.\n- Tracking of helmets throughout the duration of a play and assigning labels to these tracks.\n- Imputing boxes for partially occluded helmets based on the surrounding frames.\n- Using computer vision techniques to identify player jersey numbers and pair with helmets.\n- Identifying key points on the field (line numbers, hash marks) as reference points.\n\nWe are excited to see what solutions the kaggle community comes up with!","dea784ff":"For context, below are images from the sideline and endzone view of the above play at the moment of snap.","7d35cf39":"# A Baseline Submission\n\nThe following code shows how to make a baseline submission. In this submission we've randomly assigned labels to the top 22 baseline helmet boxes for each frame.\n\nFirst we will run this code on the training data and calculate the expected score. Next we create this submission on the test set for submission.","b0b801a2":"# NFL Helmet Assignment Competition\n\n<p align=\"center\">\n  <img src=\"https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/NFL%20player%20safety%20analytics\/assingment_example.gif.gif\" alt=\"animated\" \/>\n<\/p>\n\nWelcome to the 2021 NFL Health & Safety, Helmet Assignment competition! The competition is part of a collaborative effort between National Football League (NFL) and Amazon Web Services (AWS) to assist in the development of the best sports injury surveillance and mitigation program in the world.\n\nIf you participated in the 2020 NFL Impact Detection competition this dataset may look familiar to you. While last year's competition was focused on detecting helmet impacts from video footage - this competition challenges competitors to correctly assign each helmet to its associated player. To solve this problem, Kagglers will need to leverage both video footage and NFL's Next Gen Stats (NGS) tracking data.","58181da3":"# What is the goal of this competition?\n\nSimply put, we are trying to assign the correct player \"label\" on helmets in NFL game footage. Labels consist of a value H (for home team) and V (for visiting team) followed by the player's jersey number. Player labels are provided within the NGS tracking data for each play. A perfect submission would correctly identify the helmet box for every helmet in every frame of video- and assign that helmet the correct player label.","d5cf59dd":"## NGS Tracking Data\nThe use of the NGS tracking data will be important for correctly labeling videos. Some things to note are:\n- NGS data is sampled at a rate of 10Hz, while videos are sampled at roughly 59.94Hz.\n- NGS data and videos can be approximately synced by linking the NGS data where `event == \"ball_snap\"` to the 10th frame of the video (approximately syncronized to the ball snap in the video).\n- The NGS data and the orientation of the video cameras are not consistent. Your solution must account for matching the orientation of the video angle relative to the NGS data.\n\nThe provided `add_track_features` function may be helpful when attempting to synchronize the NGS data with the video footage.","396eb4b0":"## Video and Baseline Boxes\nAs noted above, the provided baseline boxes are imperfect but allow you to quickly tackle the problem without having to address the helmet detection. The below video shows an example of these baseline predictions alongside the true helmet labels.","93bae0b4":"Below is a plot of the NGS data for an example play at the moment the ball is snapped. NGS also includes the speed (`s`), acceleration (`a`), orientation (`o`) and direction (`dir`) for each player. More details can be found in the [data description page](https:\/\/www.kaggle.com\/c\/nfl-health-and-safety-helmet-assignment\/data)."}}