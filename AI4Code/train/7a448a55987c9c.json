{"cell_type":{"a9a4d190":"code","36464cd0":"code","f8d6a377":"code","2d4ea192":"code","d7843053":"code","548d1469":"code","05488c04":"code","27f910cf":"code","a62f7487":"code","71ad73a6":"code","c2040d96":"code","c7c83908":"code","eb94b8b2":"code","93a2a021":"code","23f99838":"code","91f56838":"code","1148ff42":"code","27c82072":"code","cbc44808":"code","ca25cacf":"code","d2afa0a8":"code","64dd328d":"code","2ed5d1ee":"code","46cc51d1":"code","6be80070":"code","4ecfd394":"code","0971d57c":"code","cca34814":"code","c29a4a7b":"code","bee2063d":"code","9ab74340":"code","bf7a0bd6":"code","ea735c27":"code","0ba45d58":"code","79ff5c6e":"code","358df098":"code","5fb7388b":"code","981a4723":"code","fc2711d1":"code","6b6445e9":"code","bd07dea0":"code","57c3c1fb":"code","4920e423":"code","b30cef33":"code","34bbd566":"code","eeaef014":"code","f97efe25":"code","612858ac":"code","5b92c81d":"code","b26e37e5":"code","59e0a45d":"code","e4da7171":"markdown","8cc3326a":"markdown","5f1f4659":"markdown","5b020b1c":"markdown","e55164c5":"markdown","6d185da4":"markdown","1e0a33c0":"markdown","e519f721":"markdown","2922a610":"markdown","f9e29ce6":"markdown","2788d12d":"markdown"},"source":{"a9a4d190":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","36464cd0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom subprocess import check_output\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nfrom pandas.plotting import lag_plot\nfrom pandas import datetime\n\nfrom sklearn.metrics import mean_squared_error\n\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller, kpss, acf\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom pylab import rcParams","f8d6a377":"# print(os.listdir(\"..\/input\"))\n# print(check_output([\"ls\", \"..\/input\/Data\"]).decode(\"utf8\"))","2d4ea192":"df = pd.read_csv(\"\/kaggle\/input\/price-volume-data-for-all-us-stocks-etfs\/Stocks\/tsla.us.txt\")\ndf.head()","d7843053":"df.shape","548d1469":"df.describe()","05488c04":"df[['Close']].plot()\nplt.title('Tesla')\nplt.show()","27f910cf":"# Comulative Return\ndr = df.cumsum()\ndr.plot()\nplt.title('Tesla Cumulative Returns')","a62f7487":"plt.figure(figsize=(10,10))\nlag_plot(df['Open'], lag=5)\nplt.title('Tesla Autocorrelation plot')","71ad73a6":"# Converting 'Date' to datetime object\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['Year'] = df['Date'].dt.year\ndf['Month'] = df['Date'].dt.month\n","c2040d96":"# Line plot\nfig, ax = plt.subplots(figsize=(15, 6))\nsns.lineplot(df['Date'], df['Open'] )\n\n# Formatting\nax.set_title('Open Price', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\nax.set_xlabel('Year', fontsize = 16, fontdict=dict(weight='bold'))\nax.set_ylabel('Price', fontsize = 16, fontdict=dict(weight='bold'))\nplt.tick_params(axis='y', which='major', labelsize=16)\nplt.tick_params(axis='x', which='major', labelsize=16)","c7c83908":"df.isna().sum()","eb94b8b2":"# Plot Daily Volume Lineplot\nfig, ax = plt.subplots(figsize=(15, 6))\nsns.lineplot(df['Date'], df['Volume'] )\n\nax.set_title('Daily Volume', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\nax.set_xlabel('Year', fontsize = 16, fontdict=dict(weight='bold'))\nax.set_ylabel('Price', fontsize = 16, fontdict=dict(weight='bold'))\nplt.tick_params(axis='y', which='major', labelsize=16)\nplt.tick_params(axis='x', which='major', labelsize=16)\n","93a2a021":"# Aggregating the Time Series to a monthly scaled index\ny = df[['Date','Volume']].copy()\ny.set_index('Date', inplace=True)\ny.index = pd.to_datetime(y.index)\ny = y.resample('1M').mean()\ny['Date'] = y.index\n\n# Plot the Monthly Volume Lineplot\nfig, ax = plt.subplots(figsize=(15, 6))\nsns.lineplot(y['Date'], y['Volume'] )\n\nax.set_title('Monthly Volume', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\nax.set_xlabel('Year', fontsize = 16, fontdict=dict(weight='bold'))\nplt.tick_params(axis='y', which='major', labelsize=16)\nplt.tick_params(axis='x', which='major', labelsize=16)","23f99838":"df['Year'].unique()","91f56838":"variable = 'Open'\nfig, ax = plt.subplots(figsize=(15, 6))\n\nsns.lineplot(df['Month'], df[variable], hue = df['Year'])\nax.set_title('Seasonal plot of Price', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\nax.set_xlabel('Month', fontsize = 16, fontdict=dict(weight='bold'))\nax.set_ylabel('Price', fontsize = 16, fontdict=dict(weight='bold'))\nax.legend(labels = [str(2010+i) for i in range(11)], bbox_to_anchor=(1.1, 1.05))\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\n\nsns.boxplot(df['Year'], df[variable], ax=ax[0])\nax[0].set_title('Year-wise Box Plot\\n(The Trend)', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\nax[0].set_xlabel('Year', fontsize = 16, fontdict=dict(weight='bold'))\nax[0].set_ylabel('Price', fontsize = 16, fontdict=dict(weight='bold'))\n\nsns.boxplot(df['Month'], df[variable], ax=ax[1])\nax[1].set_title('Month-wise Box Plot\\n(The Seasonality)', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\nax[1].set_xlabel('Month', fontsize = 16, fontdict=dict(weight='bold'))\nax[1].set_ylabel('Price', fontsize = 16, fontdict=dict(weight='bold'))\n\nfig.autofmt_xdate()","1148ff42":"# Aggregating the Time Series to a monthly scaled index\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf,month_plot,quarter_plot\n\ny = df[['Date','Open','Close']].copy()\ny.set_index('Date', inplace=True)\ny.index = pd.to_datetime(y.index)\ny = y.resample('1M').mean()\n\n# The magic\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 10))\n\nmonth_plot(y['Open'], ax=ax[0]);\nax[0].set_ylabel('Open', fontsize = 16, fontdict=dict(weight='bold'))\n\nmonth_plot(y['Close'], ax=ax[1]);\nax[1].set_ylabel('Closing_Price', fontsize = 16, fontdict=dict(weight='bold'))","27c82072":"# Aggregating the Time Series to a monthly scaled index\ny = df[['Date','Open']].copy()\ny.set_index('Date', inplace=True)\ny.index = pd.to_datetime(y.index)\ny = y.resample('1M').mean()\n\n# Setting rcparams\nrcParams['figure.figsize'] = 15, 12\nrcParams['axes.labelsize'] = 20\nrcParams['ytick.labelsize'] = 16\nrcParams['xtick.labelsize'] = 16\n\n# Using statistical tools of statsmodel library\ndecomposition = sm.tsa.seasonal_decompose(y, model='multiplicative', freq = 12)\ndecomp = decomposition.plot()\ndecomp.suptitle('Open decomposition', fontsize=22)\n","cbc44808":"fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(15, 6))\nautocorr = acf(df['Open'], nlags=60, fft=False)\nprint(autocorr)\n\nplot_acf(df['Open'].tolist(), lags=60, ax=ax[0], fft=False);\nplot_pacf(df['Open'].tolist(), lags=60, ax=ax[1]);\n","ca25cacf":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping","d2afa0a8":"\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0,1))\nscaled_data = scaler.fit_transform(df['Close'].values.reshape(-1,1))","64dd328d":"df1 = df.reset_index()['Close']","2ed5d1ee":"df1.shape","46cc51d1":"import matplotlib.pyplot as plt\nplt.plot(df1)","6be80070":"#Scaling our data to pass into LSTM as it is sensitive to scaling\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0,1))\ndf1 = scaler.fit_transform(np.array(df1).reshape(-1,1))","4ecfd394":"df1.shape","0971d57c":"#separating traing and testing data\ntraining_size = int(len(df1)*0.70)\ntest_size = len(df1) - training_size\ntrain_data,test_data = df1[0:training_size,:],df1[training_size:len(df1),:1]","cca34814":"training_size,test_size","c29a4a7b":"#convert an arrays of value into dataset matrix\ndef create_dataset(dataset,timestep=1):\n    dataX,dataY = [],[]\n    for i in range(len(dataset)-timestep-1):\n        a = dataset[i:(i+timestep),0]\n        dataX.append(a)\n        dataY.append(dataset[i+timestep,0])\n    return np.array(dataX),np.array(dataY)","bee2063d":"#reshape our dataset into 100 time steps\ntime_step=100\nx_train,y_train = create_dataset(train_data,time_step)\nx_test,y_test = create_dataset(test_data,time_step)","9ab74340":"x_train.shape,y_train.shape","bf7a0bd6":"x_test.shape,y_test.shape","ea735c27":"#As LSTM model expects 3D input so we would reshape our input data\nx_train = x_train.reshape(x_train.shape[0],x_train.shape[1],1)\nx_test = x_test.reshape(x_test.shape[0],x_test.shape[1],1)","0ba45d58":"def LSTM_model():\n    \n    model = Sequential()\n    \n    model.add(LSTM(units = 50, return_sequences = True, input_shape = (x_train.shape[1],1)))\n    model.add(Dropout(0.2))\n\n    model.add(LSTM(units = 50, return_sequences = True))\n    model.add(Dropout(0.2))\n\n    model.add(LSTM(units = 50))\n    model.add(Dropout(0.2))\n    \n    model.add(Dense(units=1))\n    \n    return model","79ff5c6e":"model = LSTM_model()\nmodel.summary()\nmodel.compile(optimizer='adam', \n              loss='mean_squared_error')","358df098":"# Define callbacks\n\n# Save weights only for best model\ncheckpointer = ModelCheckpoint(filepath = 'weights_best.hdf5', \n                               verbose = 2, \n                               save_best_only = True)\n\nmodel.fit(x_train, \n          y_train, \n          epochs=25, \n          batch_size = 32,\n          callbacks = [checkpointer])","5fb7388b":"import tensorflow as tf","981a4723":"train_predict = model.predict(x_train)\ntest_predict = model.predict(x_test)","fc2711d1":"train_predict = scaler.inverse_transform(train_predict)\ntest_predict = scaler.inverse_transform(test_predict)","6b6445e9":"import math\nfrom sklearn.metrics import mean_squared_error\nmath.sqrt(mean_squared_error(y_train,train_predict))\n","bd07dea0":"math.sqrt(mean_squared_error(y_test,test_predict))","57c3c1fb":"### Plotting \n# shift train predictions for plotting\nlook_back=100\ntrainPredictPlot = np.empty_like(df1)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict\n# shift test predictions for plotting\ntestPredictPlot = np.empty_like(df1)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(train_predict)+(look_back*2)+1:len(df1)-1, :] = test_predict\n# plot baseline and predictions\nplt.plot(scaler.inverse_transform(df1))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()","4920e423":"len(test_data)","b30cef33":"x_input=test_data[458:].reshape(1,-1)\nx_input.shape","34bbd566":"temp_input=list(x_input)\ntemp_input=temp_input[0].tolist()\n","eeaef014":"temp_input","f97efe25":"from numpy import array\n\nlst_output=[]\nn_steps=100\ni=0\nwhile(i<30):\n    \n    if(len(temp_input)>100):\n        #print(temp_input)\n        x_input=np.array(temp_input[1:])\n        print(\"{} day input {}\".format(i,x_input))\n        x_input=x_input.reshape(1,-1)\n        x_input = x_input.reshape((1, n_steps, 1))\n        #print(x_input)\n        yhat = model.predict(x_input, verbose=0)\n        print(\"{} day output {}\".format(i,yhat))\n        temp_input.extend(yhat[0].tolist())\n        temp_input=temp_input[1:]\n        #print(temp_input)\n        lst_output.extend(yhat.tolist())\n        i=i+1\n    else:\n        x_input = x_input.reshape((1, n_steps,1))\n        yhat = model.predict(x_input, verbose=0)\n        print(yhat[0])\n        temp_input.extend(yhat[0].tolist())\n        print(len(temp_input))\n        lst_output.extend(yhat.tolist())\n        i=i+1\n    \n\nprint(lst_output)","612858ac":"day_new=np.arange(1,101)\nday_pred=np.arange(101,131)","5b92c81d":"len(df1)","b26e37e5":"plt.plot(day_new,scaler.inverse_transform(df1[1758:]))\nplt.plot(day_pred,scaler.inverse_transform(lst_output))","59e0a45d":"df3=df1.tolist()\ndf3.extend(lst_output)\nplt.plot(df3[1200:])","e4da7171":"There are no missing values","8cc3326a":"# Importing Libraries","5f1f4659":"# LSTM","5b020b1c":"## 1. Scaling","e55164c5":"## 2. Splitting data","6d185da4":"# Preprocessing","1e0a33c0":"# Tesla Stock Market Analysis","e519f721":"## 3. Build LSTM Model","2922a610":"For autocorrelation, the y-axis is the value for the correlation between a value and its lag. The lag is on the x-axis. The zero-lag has a correlation of 1 because it correlates with itself perfectly.\nThe autocorrelation plot shows that most of the spikes are not statistically significant. This indicates that the returns are not highly correlated, as shown here.","f9e29ce6":"## 4. Training","2788d12d":"This graph performs a groupby function to see more clearly the months of seasonality."}}