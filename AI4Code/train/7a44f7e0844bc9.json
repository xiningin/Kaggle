{"cell_type":{"3a5f41a3":"code","2c4427ea":"code","2fa0e10a":"code","571bdc10":"code","4772ebcd":"code","96bf69b1":"code","a2e8308a":"code","b212c273":"code","93bb443b":"code","e0ee21b6":"code","7dbd4492":"code","9a292239":"code","2c869633":"code","edc9a747":"code","7e3d5a8b":"code","2c1e5db6":"code","f232212f":"code","8d3338eb":"code","f2e0a0f6":"code","33cde8e8":"code","714a8117":"code","daa09499":"code","e3452713":"code","4b3f5c7a":"code","ffadef5b":"code","fb8a4fcd":"code","f66e4e9f":"code","b2f619fc":"code","c9fc1e5f":"code","9f4af1e1":"code","0119db0b":"code","34f78010":"code","2f73b6c1":"code","14390026":"code","70ffa9f9":"code","86d6b205":"code","27256dd2":"code","1f89e9bc":"code","8ce22a4d":"code","ca31a9a3":"code","c2b5454b":"code","9fbb1890":"code","5bba2cb6":"code","3af10071":"code","e5186095":"code","419a9ba1":"code","ad88407f":"code","b20a06ed":"code","096ad8c6":"code","34de19a9":"code","ff2774eb":"markdown","49de4e20":"markdown","3488afcb":"markdown"},"source":{"3a5f41a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2c4427ea":"from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom fbprophet import Prophet\nfrom statsmodels.tsa.arima_model import ARIMA\nimport matplotlib.pyplot as plt\nimport datetime\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","2fa0e10a":"!ls -l \/kaggle\/input\/\n!pwd","571bdc10":"train = pd.read_csv('\/kaggle\/input\/av-timeseries-jetrail-traffic-forecasting\/Train.csv',parse_dates=['Datetime'],dayfirst=True,infer_datetime_format=True)\ntest = pd.read_csv('\/kaggle\/input\/av-timeseries-jetrail-traffic-forecasting\/Test.csv',parse_dates=['Datetime'],dayfirst=True,infer_datetime_format=True)\nsubmission = pd.read_csv('\/kaggle\/input\/av-timeseries-jetrail-traffic-forecasting\/sample_submission.csv',dayfirst=True)","4772ebcd":"#train[]","96bf69b1":"train.info()","a2e8308a":"display(train.head(), train.tail())","b212c273":"display(train.iloc[200])","93bb443b":"display(test.head(), test.tail())","e0ee21b6":"display(submission.head(), submission.tail())","7dbd4492":"display(train.describe().T, test.describe().T, submission.describe().T)","9a292239":"train.shape, test.shape, submission.shape","2c869633":"train.index, test.index, submission.index","edc9a747":"train.info(), test.info(), submission.info()","7e3d5a8b":"train['Datetime'].duplicated().sum(), test['Datetime'].duplicated().sum()","2c1e5db6":"train.isna().sum()","f232212f":"train['Count'].value_counts()","8d3338eb":"train.drop(columns='ID').plot()","f2e0a0f6":"data = train.copy()","33cde8e8":"data.drop(columns=['ID','Datetime']).plot()","714a8117":"model = Prophet()","daa09499":"rn_train = train.drop(columns='ID').copy()\nrn_train.columns = ['ds','y']\nrn_train.head()","e3452713":"rn_test = test.drop(columns='ID').copy()\nrn_test.columns = ['ds']\nrn_test.head()","4b3f5c7a":"model.fit(rn_train)","ffadef5b":"print(model.t_scale, model.y_scale)\n#print(model.history_dates.to_csv('a2.csv'))\n#print(model.history.to_csv('b2.csv'))","fb8a4fcd":"print(1065\/365)\nprint(365*3)\nprint(365*2*24)\nprint(365*2*24+32*24,'<--')\nprint('*'*10)\nprint(30*7*24)\nprint(30*7*24+(4-2+1)*24,'<--')","f66e4e9f":"train.shape, test.shape, submission.shape","b2f619fc":"model.component_modes","c9fc1e5f":"### Create future dates of 365 days\nfuture_dates=model.make_future_dataframe(periods=3312, freq='H')","9f4af1e1":"print(5112\/24)\nprint(7*30*24+48+24)","0119db0b":"rn_train.isna().sum()","34f78010":"#new_future_dt = \npd.date_range('2014-01-01', periods=10) + pd.Timedelta(days=1)","2f73b6c1":"future_dates=model.make_future_dataframe(periods=3312, freq='H')\ndisplay('train:',rn_train.head(), rn_train.tail(),\n'new future:',future_dates.head(), future_dates.tail(),\n'test:',test.head(), test.tail())","14390026":"future_dates = rn_test.copy()\nforecast = model.predict(future_dates)\nmodel.plot(forecast)\n","70ffa9f9":"model.plot_components(forecast)","86d6b205":"fcst = forecast[['ds','yhat']]#.tail(3961)\ndisplay(forecast)\nprint(fcst)","27256dd2":"forecast.ds.min(), forecast.ds.max()","1f89e9bc":"submission['Count'] = forecast['yhat']\n#submission.to_csv('submission_fb_v1.csv',index=False) #wrong dates\n#383.540 Rnk#1066\n#submission.to_csv('submission_fb_v2.csv',index=False) #corrected dates\n#228.317 Rnk#604","8ce22a4d":"data = train.copy()\ndata['Source'] = 'Train'\ndisplay(data.head(),data.tail())\n\ndata = pd.concat([data,test])\ndata['Source'] = data['Source'].fillna('Test')\n\ndata[\"Datetime\"] = pd.to_datetime(data[\"Datetime\"], dayfirst=True)\ndata=data.sort_values([\"Datetime\"])\ndata[\"Quarter\"] = data[\"Datetime\"].dt.quarter\ndata[\"Year\"] = data[\"Datetime\"].dt.year\ndata[\"Month\"] = data[\"Datetime\"].dt.month\ndata[\"Day\"] = data[\"Datetime\"].dt.day\ndata[\"DayofWeek\"] = data[\"Datetime\"].dt.dayofweek\ndata[\"WeekofYear\"] = data[\"Datetime\"].dt.weekofyear\ndata[\"Hour\"] = data[\"Datetime\"].dt.hour\ndata[\"Minute\"] = data[\"Datetime\"].dt.minute\ndata[['Datetime','Day','DayofWeek','WeekofYear','Month','Quarter','Year','Hour','Minute']]\ndisplay(data.head(),data.tail())","ca31a9a3":"#train['Minute'].notna().sum()","c2b5454b":"#train[train['Minute']==0]['Minute'].sum()\n#display(data[['Minute','Quarter','Year','Month','Day','DayofWeek','WeekofYear','Hour']].value_counts())\n\n#print(train.Quarter.value_counts().sort_index())\ntrain = data[data['Source']=='Train']\ntest = data[data['Source']=='Test']\ndisplay(train.Minute.value_counts().sort_index(),\ntrain.Quarter.value_counts().sort_index(),\ntrain.Year.value_counts().sort_index(),     \ntrain.Month.value_counts().sort_index(),\ntrain.Day.value_counts().sort_index(),\ntrain.DayofWeek.value_counts().sort_index(),\ntrain.WeekofYear.value_counts().sort_index(),\ntrain.Hour.value_counts().sort_index()      \n     )\n\n","9fbb1890":"train = data[data['Source']=='Train']\ntest = data[data['Source']=='Test']\n\nmodel = LinearRegression()\ntrain_X = train.drop(columns = ['ID','Datetime','Count','Minute','Source'])\ntrain_y = train['Count']\ndisplay(train_X.head(),train_y.head())\n\nmodel.fit(train_X,train_y)\nprint('Score:',model.score(train_X,train_y))\nprint(mean_squared_error(train_y,model.predict(train_X),squared=False))\n","5bba2cb6":"ss = StandardScaler()\ncols = train_X.columns\nprint(cols)\nss_train_X = pd.DataFrame(ss.fit_transform(train_X[cols]),columns=cols)\ndisplay(ss_train_X)\n\nmodel.fit(ss_train_X,train_y)\nprint('Score:',model.score(ss_train_X,train_y))\nprint('RMSE:',mean_squared_error(train_y,model.predict(ss_train_X),squared=False))","3af10071":"ms = MinMaxScaler()\ncols = train_X.columns\nprint(cols)\nms_train_X = pd.DataFrame(ss.fit_transform(train_X[cols]),columns=cols)\ndisplay(ms_train_X)\n\nmodel.fit(ms_train_X,train_y)\nprint('Score:',model.score(ms_train_X,train_y))\nprint('RMSE:',mean_squared_error(train_y,model.predict(ms_train_X),squared=False))","e5186095":"rs = RobustScaler()\ncols = train_X.columns\nprint(cols)\nrs_train_X = pd.DataFrame(ss.fit_transform(train_X[cols]),columns=cols)\ndisplay(rs_train_X)\n\nmodel.fit(rs_train_X,train_y)\nprint('Score:',model.score(rs_train_X,train_y))\nprint('RMSE:',mean_squared_error(train_y,model.predict(rs_train_X),squared=False))","419a9ba1":"\n#ohe = OneHotEncoder()\n#ohe_train_X = ohe.fit_transform(train_X).toarray()\n#display(ohe_train_X)\ncols = train_X.columns\nprint(cols)\ngd_train_X = pd.get_dummies(train_X[cols].astype('O'))\ndisplay(gd_train_X.shape,gd_train_X)\n\nmodel.fit(gd_train_X,train_y)\nprint('Score:',model.score(gd_train_X,train_y))\nprint('RMSE:',mean_squared_error(train_y,model.predict(gd_train_X),squared=False))","ad88407f":"train_X = train[['Year','Month','Day','Hour']] #.drop(columns = ['ID','Datetime','Count','Minute','Source'])\ntrain_y = train[['Count']]\ncols = train_X.columns\nprint(cols)\ngd_train_X = pd.get_dummies(train_X[cols].astype('O'))\ndisplay(gd_train_X.shape,gd_train_X)\n\nmodel.fit(gd_train_X,train_y)\nprint('Score:',model.score(gd_train_X,train_y))\nprint('RMSE:',mean_squared_error(train_y,model.predict(gd_train_X),squared=False))","b20a06ed":"data_X = data.drop(columns=['ID','Count','Datetime','Minute'])\ncols = data_X.columns\nprint(cols)\ngddt_data_X = pd.get_dummies(data[cols].astype('O'), drop_first=True)\n#display(gddt_data_X.shape,gddt_data_X.columns)\n\ngddt_train_X = gddt_data_X[gddt_data_X['Source_Train']==1]\ngddt_test_X = gddt_data_X[gddt_data_X['Source_Train']==0]\n\ngddt_train_X =  gddt_train_X.drop(columns = ['Source_Train'])\ndisplay(gddt_train_X.shape,gddt_train_X)\n\n\nmodel.fit(gddt_train_X,train_y)\nprint('Score:',model.score(gddt_train_X,train_y))\nprint('RMSE:',mean_squared_error(train_y,model.predict(gddt_train_X),squared=False))\n\n#test_X = test.drop(columns = ['ID','Count','Datetime','Minute','Source'])\n#cols = train_X.columns\n#print(cols)\n#gddt_test_X = pd.get_dummies(test_X[cols].astype('O'), drop_first=True)\nprint('Test\\n','*'*15)\ngddt_test_X =  gddt_test_X.drop(columns = ['Source_Train'])\ndisplay(gddt_test_X.shape,gddt_test_X)\nprint(gddt_test_X.columns)\npred_test_y = model.predict(gddt_test_X)\n\nsubmission['Count'] = pred_test_y\n#submission.to_csv('submission_lr_gddt_v1.csv', index=False) # wrong dates\n#317.592 Rnk#958\n#submission.to_csv('submission_lr_gddt_v2.csv', index=False) # corrected dates","096ad8c6":"\n\n#Regression model\ndef Regression(train,test): #data):\n    print('Regresssion Starting...')\n    \n    ## Split the Date into Month, Quarter & Year ##\n    ###############################################\n    data = train.copy()\n    \n    data['Source'] = 'Train'\n    data = pd.concat([data,test])\n    data['Source'] = data['Source'].fillna('Test')\n\n    data[\"Datetime\"] = pd.to_datetime(data[\"Datetime\"], dayfirst=True)\n    data=data.sort_values([\"Datetime\"])\n    data[\"Quarter\"] = data[\"Datetime\"].dt.quarter\n    data[\"Year\"] = data[\"Datetime\"].dt.year\n    data[\"Month\"] = data[\"Datetime\"].dt.month\n    data[\"Day\"] = data[\"Datetime\"].dt.day\n    data[\"DayofWeek\"] = data[\"Datetime\"].dt.dayofweek\n    data[\"WeekofYear\"] = data[\"Datetime\"].dt.weekofyear\n    data[\"Hour\"] = data[\"Datetime\"].dt.hour\n    data[\"Minute\"] = data[\"Datetime\"].dt.minute\n    data[['Datetime','Day','DayofWeek','WeekofYear','Month','Quarter','Year','Hour','Minute']]\n    data.rename(columns={'Count': 'Volume'}, inplace=True)\n    ## data.to_csv('EDA_data1.csv',index=False)\n    \n    Train = data[data['Source']=='Train']\n    Test = data[data['Source']=='Test']\n    display(Train.columns)\n    \n    ## Split the Data into Train(year<2019) & Test(year=2019) ##\n    ############################################################    \n    #Train=data[(data.Year>=2015)&(data.Year<2019)] #modify date according to your dataset; Train : 2017-2018\n    #Test=data[(data.Year==2019)]  #modify date according to your dataset; Test : 2019\n    \n    ## compute seasoned_index by year ##\n    Train[\"SI_Y\"]=Train[\"Volume\"]\/Train.groupby(\"Year\")[\"Volume\"].transform(np.mean)\n    \n    ## compute seasoned_index by month ##\n    Train[\"F_SI\"]=Train.groupby(\"Month\")[\"SI_Y\"].transform(np.mean)\n    \n    ## compute seasoned_index by Trend ##\n    Train[\"D_Seasonalised_trend\"] = Train[\"Volume\"]\/Train[\"F_SI\"]    \n    \n    ## compute level_index_1 using Q1 with preceding.Q4 ##\n    ##Train[\"Level_index1\"]=np.mean(Train[(Train.Year==2018)&(Train.Quarter==1)][\"D_Seasonalised_trend\"])\/np.mean(Train[(Train.Year==2017)&(Train.Quarter==4)][\"D_Seasonalised_trend\"])\n    Train[\"Level_index1\"]=np.mean(Train[(Train.Year==2014)&(Train.Quarter==1)][\"D_Seasonalised_trend\"])\/np.mean(Train[(Train.Year==2013)&(Train.Quarter==4)][\"D_Seasonalised_trend\"])\n    \n    ## compute level_index_2 using last Q4-Q3 with preceding.Q3-Q2 ##\n    ##numer1=np.mean(Train[(Train.Year==2018)&(Train.Quarter==3)][\"D_Seasonalised_trend\"])\/np.mean(Train[(Train.Year==2018)&(Train.Quarter==2)][\"D_Seasonalised_trend\"])\n    ##numer2=np.mean(Train[(Train.Year==2018)&(Train.Quarter==4)][\"D_Seasonalised_trend\"])\/np.mean(Train[(Train.Year==2018)&(Train.Quarter==3)][\"D_Seasonalised_trend\"])\n    numer1=np.mean(Train[(Train.Year==2013)&(Train.Quarter==3)][\"D_Seasonalised_trend\"])\/np.mean(Train[(Train.Year==2013)&(Train.Quarter==2)][\"D_Seasonalised_trend\"])\n    numer2=np.mean(Train[(Train.Year==2013)&(Train.Quarter==4)][\"D_Seasonalised_trend\"])\/np.mean(Train[(Train.Year==2013)&(Train.Quarter==3)][\"D_Seasonalised_trend\"])\n    \n    Train[\"Level_index2\"]=np.mean([numer1,numer2])\n    Train=Train.sort_values([\"Datetime\"])\n    Train.index=range(len(Train))\n    #Train[\"ID\"]=range(1,(len(Train)+1))\n    \n    Train[\"Deleveled_series\"]=np.where(Train.Year==2013, Train[\"D_Seasonalised_trend\"]*Train[\"Level_index1\"],Train[\"D_Seasonalised_trend\"])\n    \n    lm = LinearRegression()\n    #X = np.array(Train[[\"ID\", \"Variable_1\"]]) # In case of no extra variable in the dataset, remove the extra variable name from the list, then append the line with \".reshape(-1, 1)\"\n    X = np.array(Train[[\"ID\"]])\n    Y = np.array(Train[\"Deleveled_series\"]).reshape(-1,1)\n    #Y = np.array(Train[\"Volume\"]).reshape(-1,1)\n    display(Train.head(3))\n    #plt.plot(Train['ID'],Train['Volume'],\"r--\")\n    #plt.show()\n    model = lm.fit(X,Y)\n    y_train_pred = lm.predict(X)\n    y_train_pred = y_train_pred*Train.iloc[0][\"Level_index2\"]*np.array(Train.iloc[0:len(y_train_pred)][\"F_SI\"]).reshape(-1,1)\n    y_train_act = Train[['Volume']]\n    acc_scr =  np.mean(np.abs(lm.score(X,Y)))\n    rmse_scr = np.mean(np.abs(mean_squared_error(np.array(y_train_act),np.array(y_train_pred),squared=False)))\n    mape_scr = np.mean(np.abs((np.array(y_train_act) - np.array(y_train_pred)) \/ np.array(y_train_act))) * 100\n    print('Coeff:',lm.coef_)\n    print('Intercept:',lm.intercept_)\n    print('MAPE:',mape_scr)\n    print('RMSE:',rmse_scr)\n    print('ACC:',acc_scr)\n    # return(data)\n    # zzz\n    ### print(max(Train[\"ID\"]))\n    ### print(len(Test))\n    ### print(range(len(Test)))\n    ### Test[\"ID\"]=range(len(Test))\n    ### #Test[\"ID\"]=Test[\"ID\"]+max(Train[\"ID\"])\n    ### Test[\"ID\"]=Test[\"ID\"]+(max(Train[\"ID\"])+1)\n    display(Test.head(3))\n    #X_test=np.array(Test[[\"ID\", \"Variable_1\"]]) # In case of no extra variable in the dataset, remove the extra variable name from the list, then append the line with \".reshape(-1, 1)\"\n    X_test=np.array(Test[[\"ID\"]])\n    Y_test=model.predict(X_test)\n    \n    Pred1 = Y_test*Train.iloc[0][\"Level_index2\"]*np.array(Train.iloc[0:len(Y_test)][\"F_SI\"]).reshape(-1,1)\n    ### Pred1 = Y_test*np.array(Train.iloc[0:len(Y_test)][\"F_SI\"]).reshape(-1,1)\n    ### #Test[\"Predictions\"]=Pred1\n    ### Test[\"D_Seasonalised_trend\"]=Pred1\n    Test[\"Volume\"]=Pred1\n    \n    submission['Count'] = Pred1\n    #submission.to_csv('submission_lr_SI_TR_LI2_v1.csv', index=False)\n    #293.390 Rnk#933\n\n    display(Test.head(3))\n    out = pd.concat([Train, Test], axis = 0)\n    plt.figure(figsize=(18,9))\n    plt.scatter(out['ID'],out['Volume'],color=\"red\")\n    plt.plot(Train['ID'],Train['Volume'],\"b-\")\n    plt.plot(out['ID'],out['D_Seasonalised_trend'],\"g-\")\n    plt.show()   \n    #return(Test['Predictions'])\n    print('Regresssion Ending...')\n    return(Test['Volume'])\nRegression(train,test)","34de19a9":"#Arima model\ndef Arima(train,test): \n    train.rename(columns={'Count': 'Volume'}, inplace=True)\n    test.rename(columns={'Count': 'Volume'}, inplace=True)\n    train = train.drop(columns='ID')\n    test = test.drop(columns='ID')\n    print(train.info())\n    train['Datetime'] = pd.to_datetime(train['Datetime'],format=\"%y-%m-%d %H:%M\") \n    test['Datetime'] = pd.to_datetime(test['Datetime'],format=\"%y-%m-%d %H:%M\") \n    \n    #train = train.astype('float32')\n    #test = test.astype('float32')\n    display(train,test)\n    X = train['Volume'].values\n    #size = np.sum(data['Date']<='12\/31\/2018')\n    #size = np.sum(train['Datetime'])\n    #train, test = X[0:size], X[size:len(X)]\n    history = [x for x in train['Volume'].values]\n    predictions = list()  \n    display('len(history)=',len(history))    \n    display('len(test)=',len(test)) \n    display('range(len(test))=',range(len(test)))\n    #test = test[:,1].values\n    display('test.head=',test.head())\n    display('test.shape=',test.shape)\n    for t in range(len(test)):\n        #print('t=',t)\n        model = ARIMA(history, order=(1,1,0))\n        model_fit = model.fit(disp=0)\n        output = model_fit.forecast()\n        yhat = output[0]\n        predictions.append(yhat)\n        #obs = test[t]\n        #history.append(obs)\n    display('predict=',predictions)\n    return predictions  \n#Arima(train,test)","ff2774eb":"# Linear Model","49de4e20":"# FBProphet","3488afcb":"# ARIMA model"}}