{"cell_type":{"8fb7e661":"code","4e595506":"code","60fa91bb":"code","f015cd9b":"code","6328a7d4":"code","e466dca2":"code","708c3cc5":"code","5913c74d":"code","1d87af2f":"code","5bc0e97e":"code","33901d4d":"code","d45f138c":"code","1973a03c":"code","2e3de6a5":"code","d1754b35":"code","d5438419":"markdown"},"source":{"8fb7e661":"import os\nimport gc\nimport sys\nimport cv2\nimport glob\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n!pip install accelerate\nfrom accelerate import Accelerator\n\nfrom functools import partial\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torchvision\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\n\nimport albumentations as A \nfrom albumentations.pytorch.transforms import ToTensorV2\n\nfrom colorama import Fore, Back, Style\nr_ = Fore.RED\nb_ = Fore.BLUE\nc_ = Fore.CYAN\ng_ = Fore.GREEN\ny_ = Fore.YELLOW\nm_ = Fore.MAGENTA\nsr_ = Style.RESET_ALL","4e595506":"config = {'lr':1e-3,\n          'wd':1e-2,\n          'bs':256,\n          'img_size':128,\n          'epochs':100,\n          'seed':1000}\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])\n\ntrain_paths = np.random.choice(glob.glob('..\/input\/imagenetmini-1000\/imagenet-mini\/train\/**\/*.JPEG'),10000)\nvalid_paths = np.random.choice(glob.glob('..\/input\/imagenetmini-1000\/imagenet-mini\/val\/**\/*.JPEG'),1000)","60fa91bb":"def get_train_transforms():\n    return A.Compose(\n        [\n            A.Resize(config['img_size'],config['img_size'],always_apply=True),\n            A.Normalize(),\n            ToTensorV2(p=1.0)\n        ])","f015cd9b":"class ImageNetDataset(Dataset):\n    def __init__(self,paths,augmentations):\n        self.paths = paths\n        self.augmentations = augmentations\n    \n    def __getitem__(self,idx):\n        path = self.paths[idx]\n        image = cv2.imread(path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']\n        \n        return image\n    \n    def __len__(self):\n        return len(self.paths)","6328a7d4":"test_dataset = ImageNetDataset(valid_paths,augmentations=get_train_transforms())\ntest_dl = DataLoader(test_dataset,batch_size=16,shuffle=False,num_workers=4)\n\ndataiter = iter(test_dl)\nsample = dataiter.next()\n\nimg = torchvision.utils.make_grid(sample).permute(1,2,0).numpy()\nplt.figure(figsize=(15,15))\nplt.imshow(img);","e466dca2":"class VQ(nn.Module):\n    \n    def __init__(self,num_embeddings=512,embedding_dim=64,commitment_cost=0.25):\n        super().__init__()\n\n        self.num_embeddings = num_embeddings\n        self.embedding_dim = embedding_dim\n        self.commitment_cost = commitment_cost\n        \n        self.embeddings = nn.Embedding(self.num_embeddings,self.embedding_dim)\n        self.embeddings.weight.data.uniform_(-1\/self.num_embeddings,1\/self.num_embeddings)\n    \n    def forward(self,inputs):\n        inputs = inputs.permute(0,2,3,1).contiguous()\n        input_shape = inputs.shape\n        \n        flat_inputs = inputs.view(-1,self.embedding_dim)\n        \n        distances = torch.cdist(flat_inputs,self.embeddings.weight)\n        encoding_index = torch.argmin(distances,dim=1) \n        \n        quantized = torch.index_select(self.embeddings.weight,0,encoding_index).view(input_shape)\n        \n        e_latent_loss = F.mse_loss(quantized.detach(),inputs)\n        q_latent_loss = F.mse_loss(quantized,inputs.detach())\n        c_loss = q_latent_loss + self.commitment_cost * e_latent_loss\n        \n        quantized = inputs + (quantized - inputs).detach()\n        \n        quantized = quantized.permute(0,3,1,2).contiguous()\n        return c_loss, quantized","708c3cc5":"class ResudialBlock(nn.Module):\n    def __init__(self,in_channels,out_channels,hidden_channels):\n        super(ResudialBlock,self).__init__()\n        self.resblock = nn.Sequential(nn.ReLU(inplace=True),\n                                       nn.Conv2d(in_channels,hidden_channels,kernel_size=3,stride=1,padding=1,bias=False),\n                                       nn.ReLU(inplace=True),\n                                       nn.Conv2d(hidden_channels,out_channels,kernel_size=1,stride=1,bias=False))\n    def forward(self,x):\n        return x + self.resblock(x)\n\nclass ResudialStack(nn.Module):\n    def __init__(self,in_channels,out_channels,hidden_channels,num_res_layers):\n        super(ResudialStack,self).__init__()\n        self.num_res_layers = num_res_layers\n        self.layers = nn.ModuleList([ResudialBlock(in_channels,out_channels,hidden_channels) for _ in range(num_res_layers)])\n    \n    def forward(self,x):\n        for i in range(self.num_res_layers):\n            x = self.layers[i](x)\n        return F.relu(x)","5913c74d":"class Model(nn.Module):\n\n    def __init__(self,num_embeddings=512,embedding_dim=64,commitment_cost=0.25):\n        super().__init__()\n\n        self.embedding_dim = embedding_dim\n        self.num_embeddings = num_embeddings\n        self.commitment_cost = commitment_cost\n\n        #encode\n        self.conv1 = nn.Conv2d(3,64,kernel_size=4,stride=2,padding=1)\n        self.conv2 = nn.Conv2d(64,128,kernel_size=4,stride=2,padding=1)\n        self.conv3 = nn.Conv2d(128,128,kernel_size=3,stride=1,padding=1)\n        self.resblock1 = ResudialStack(128,128,64,3)\n        \n        #vq \n        self.vq_conv = nn.Conv2d(128,self.embedding_dim,kernel_size=1,stride=1)\n        self.vq = VQ(self.num_embeddings,self.embedding_dim,self.commitment_cost)\n        \n        #decode\n        self.conv4 = nn.Conv2d(self.embedding_dim,64,kernel_size=3,stride=1,padding=1)\n        self.resblock2 = ResudialStack(64,64,32,3)\n        self.conv5 = nn.ConvTranspose2d(64,32,kernel_size=4,stride=2,padding=1)\n        self.conv6 = nn.ConvTranspose2d(32,3,kernel_size=4,stride=2,padding=1)\n\n\n    def encode(self,x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = self.conv3(x)\n        x = self.resblock1(x)\n        return x\n        \n    def decode(self,quantized):\n        x = self.conv4(quantized)\n        x = self.resblock2(x)\n        x = F.relu(self.conv5(x))\n        x = self.conv6(x)\n        return x\n\n    def forward(self,inputs):\n        x = self.encode(inputs)\n        c_loss,quantized =  self.vq(self.vq_conv(x))\n        outputs = self.decode(quantized)\n        rec_loss = F.mse_loss(outputs,inputs)\n        loss = rec_loss + c_loss\n        return loss,outputs,rec_loss","1d87af2f":"def run():\n        \n    def evaluate(model,valid_loader):\n        model.eval()\n        valid_loss = 0\n        rec_loss = 0\n        with torch.no_grad():\n            for i, inputs in enumerate(valid_loader):\n                loss,_,loss2 = model(inputs)\n                valid_loss += loss.item()\n                rec_loss += loss2.item()\n\n        valid_loss \/= len(valid_loader)\n        rec_loss \/= len(valid_loader)\n        return valid_loss,rec_loss\n        \n    def train_and_evaluate_loop(train_loader,valid_loader,model,optimizer,\n                                epoch,best_loss,lr_scheduler=None):\n        train_loss = 0\n        for i, inputs in enumerate(train_loader):\n            optimizer.zero_grad()\n            model.train()\n            loss,_,_ = model(inputs)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            \n            if lr_scheduler:\n                lr_scheduler.step()\n        \n        train_loss \/= len(train_loader)\n        valid_loss,rec_loss = evaluate(model,valid_loader) \n        \n        print(f\"Epoch:{epoch} |Train Loss:{train_loss}|Valid Loss:{valid_loss}|Rec Loss:{rec_loss}\")\n        \n        if rec_loss <= best_loss:\n            print(f\"{g_}Loss Decreased from {best_loss} to {rec_loss}{sr_}\")\n\n            best_loss = rec_loss\n            torch.save(model.state_dict(),'.\/imagenet_vq_vae_model.bin')\n                    \n        return best_loss\n        \n    accelerator = Accelerator()\n    print(f\"{accelerator.device} is used\")\n\n    model = Model()\n    \n    ## train\n    train_dataset = ImageNetDataset(train_paths,augmentations=get_train_transforms())\n    train_dl = DataLoader(train_dataset,batch_size=config['bs'],shuffle=True,num_workers=4)\n        \n    #valid\n    valid_dataset = ImageNetDataset(valid_paths,augmentations=get_train_transforms())\n    valid_dl = DataLoader(valid_dataset,batch_size=config['bs'],shuffle=False,num_workers=4)\n    \n    \n    optimizer = optim.Adam(model.parameters(),lr=config['lr'],amsgrad=False)\n    lr_scheduler = None\n\n    model,train_dl,valid_dl,optimizer,lr_scheduler = accelerator.prepare(model,train_dl,valid_dl,optimizer,lr_scheduler)\n\n    best_loss = 9999999\n    start_time = time.time()\n    for epoch in range(config[\"epochs\"]):\n        print(f\"Epoch Started:{epoch}\")\n        best_loss = train_and_evaluate_loop(train_dl,valid_dl,model,optimizer,epoch,best_loss,lr_scheduler)\n        \n        end_time = time.time()\n        print(f\"{m_}Time taken by epoch {epoch} is {end_time-start_time:.2f}s{sr_}\")\n        start_time = end_time\n        \n    return best_loss","5bc0e97e":"run()","33901d4d":"def generate_and_save_images(model,test_sample,figsize=(20,15)):    \n    f, axarr = plt.subplots(1,2,figsize=figsize)\n    img= torchvision.utils.make_grid(test_sample).permute(1,2,0).numpy()\n    axarr[0].imshow(img)\n    plt.title(\"Orignal\")\n    \n    _,outputs,_ = model(test_sample)\n    predictions = outputs.detach().cpu()\n    img = torchvision.utils.make_grid(predictions).permute(1,2,0).numpy()\n    \n    plt.savefig('image.png')\n    axarr[1].imshow(img)\n    plt.title(\"Reconstruction\")\n","d45f138c":"model = Model()\nmodel.load_state_dict(torch.load('.\/imagenet_vq_vae_model.bin'))\nmodel.eval()\n\ntest_dataset = ImageNetDataset(valid_paths,augmentations=get_train_transforms())\ntest_dl = DataLoader(test_dataset,batch_size=64,shuffle=False,num_workers=4)\n\ndataiter = iter(test_dl)\nsample = dataiter.next()","1973a03c":"generate_and_save_images(model,sample)","2e3de6a5":"test_dataset = ImageNetDataset(valid_paths,augmentations=get_train_transforms())\ntest_dl = DataLoader(test_dataset,batch_size=4,shuffle=False,num_workers=4)\n\ndataiter = iter(test_dl)\nsample = dataiter.next()\ngenerate_and_save_images(model,sample,figsize=(15,10))","d1754b35":"test_dataset = ImageNetDataset(valid_paths,augmentations=get_train_transforms())\ntest_dl = DataLoader(test_dataset,batch_size=1,shuffle=False,num_workers=4)\n\ndataiter = iter(test_dl)\nsample = dataiter.next()\ngenerate_and_save_images(model,sample,figsize=(7,7))","d5438419":"# Training VQ-VAE On Pytorch"}}