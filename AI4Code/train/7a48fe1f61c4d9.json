{"cell_type":{"201a980c":"code","5e4d4d55":"code","18f57a81":"code","cbffbc60":"code","80b73a08":"code","da10bd8f":"code","0faea6ff":"code","f5b43414":"code","f49c162e":"code","c0b06286":"code","269ab71a":"code","83b9e797":"code","59ffbb9c":"markdown","fdad008a":"markdown","bed95a31":"markdown","f4428f3d":"markdown","ca509e6d":"markdown"},"source":{"201a980c":"import os\nimport random\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport torch\nimport torch.nn as nn\n\nimport matplotlib.pyplot as plt","5e4d4d55":"# run this if using kaggle notebooks\n!cp -r ..\/input\/lux-ai-2021\/* .\n# if working locally, download the `simple\/lux` folder from here https:\/\/github.com\/Lux-AI-Challenge\/Lux-Design-2021\/tree\/master\/kits\/python\n# and we recommend following instructions in there for local development with python bots\n\n# for kaggle-environments\nfrom lux.game import Game\nfrom lux.game_map import Cell, RESOURCE_TYPES, Position\nfrom lux.constants import Constants\nfrom lux.game_constants import GAME_CONSTANTS\nfrom lux import annotate\nimport math\nimport sys\nfrom kaggle_environments import make","18f57a81":"class ResourceFeatures:\n    def __init__(self, game_state):\n        self.game_state=game_state\n        \n    def get_resource_features(self, cell):\n        resource_feats=np.zeros(3)\n        \n        resource=cell.resource\n        resource_amount=0\n        resource_type=0\n        if cell.has_resource():\n            resource_amount=resource.amount\n\n        #0-no resource, 1-wood, 2-coal, 3-uranium\n        if cell.has_resource() and RESOURCE_TYPES.WOOD == resource.type:\n            resource_feats[0]=(resource_amount - 521.49)\/219.398\n        elif cell.has_resource() and RESOURCE_TYPES.COAL == resource.type:\n            resource_feats[1]=( resource_amount  - 388.595)\/21.614\n        elif cell.has_resource() and RESOURCE_TYPES.URANIUM == resource.type:\n            resource_feats[2]=(resource_amount-324.63)\/14.55\n        return resource_feats\n    \n    def get_grid_resources(self, game_state):\n        cell_feats=[]\n        self.game_state=game_state\n        game_map = self.game_state.map\n        (width, height) = (game_map.width, game_map.height)\n        resource_feats=np.zeros( (width, height, 3) )\n        for i in range(width):\n            for j in range(height):\n                cell = game_map.get_cell(i, j)    \n                r_feats=self.get_resource_features(cell)\n                resource_feats[i][j]=r_feats\n        return resource_feats\n    \nclass CityFeatures:\n    def __init__(self, game_state):\n        self.game_state=game_state\n    def get_adjacent_friendly_tile_counts(self, x, y, fuel, citytiles):\n        num_tiles=0\n        for citytile in citytiles:\n            (adjx, adjy) = (citytile.pos.x, citytile.pos.y)\n            if x==adjx and y==adjy:\n                continue\n            if num_tiles==4:\n                break\n            if x==1+adjx and y==adjy:\n                num_tiles+=1\n            elif x==adjx and y==1+adjy:\n                num_tiles+=1\n            elif x == adjx-1 and y==adjy:\n                num_tiles+=1\n            elif x==adjx and y==adjy-1:\n                num_tiles+=1\n        return num_tiles\n        \n    def get_city_features(self, game_state):\n        self.game_state=game_state\n        \n        turn=self.game_state.turn\n        team=self.game_state.id\n        players=self.game_state.players\n        game_map = self.game_state.map\n        (width, height) = (game_map.width, game_map.height)\n        \n        city_feats=np.zeros((width, height, 6))\n        for player in players:\n            cities=player.cities\n            for city_id, city in cities.items():\n                fuel=city.fuel\n                fuel_per_citytile=fuel\/len(city.citytiles)\n                city_team=city.team\n                \n                opp_team=1\n                if city_team==team:\n                    opp_team=0\n                    \n                for city_tile in city.citytiles:\n                    cooldown=city_tile.cooldown\n                    (x, y) = (city_tile.pos.x, city_tile.pos.y)\n                    num_tiles=self.get_adjacent_friendly_tile_counts(x, y, fuel, city.citytiles)\n                    fuel_required_to_survive = (23 - 5*num_tiles) * 10\n\n                    city_feats[x][y][0] = opp_team\n                    city_feats[x][y][1] = 1-opp_team\n                    city_feats[x][y][2] = cooldown\n                    city_feats[x][y][3] = min(1, fuel_per_citytile\/230) #clipping range [0, 1]\n                    city_feats[x][y][4] = fuel_required_to_survive\/230 #always be [0, 1]\n                    city_feats[x][y][5] = min(0, (fuel_per_citytile - fuel_required_to_survive)\/230)#Clips [-1, 0]\n        return city_feats\n\nclass UnitFeatures:\n    def __init__(self, game_state):\n        self.game_state=game_state\n    def get_unit_feats(self, game_state):\n        self.game_state=game_state\n        turn=self.game_state.turn\n        team=self.game_state.id\n        players=self.game_state.players\n        game_map = self.game_state.map\n        (width, height) = (game_map.width, game_map.height)\n        \n        unit_feats=np.zeros((width, height, 9))\n        for player in players:\n            units=player.units\n            for unit in units:\n                (x, y) = (unit.pos.x, unit.pos.y)\n                opp_team = 0 if (unit.team==team) else 1\n                cooldown=unit.cooldown\n                unit_type=unit.type\n                max_cargo=100\n                if unit_type == 1:\n                    max_cargo=2000\n                cargo_wood=unit.cargo.wood\/max_cargo\n                cargo_coal=unit.cargo.coal\/max_cargo\n                cargo_uranium=unit.cargo.uranium\/max_cargo\n                cargo_space_left=unit.get_cargo_space_left()\/max_cargo\n\n                unit_feats[x][y][0]=opp_team\n                unit_feats[x][y][1]=1-opp_team\n                unit_feats[x][y][2]=cooldown\n\n                unit_feats[x][y][3]=cargo_wood\n                unit_feats[x][y][4]=cargo_coal\n                unit_feats[x][y][5]=cargo_uranium\n                unit_feats[x][y][6]=cargo_space_left\n\n                #unit_type = 0 if cart\n                #unit type=1 if worker\n                \n                unit_feats[x][y][7]=1-unit_type #1 if worker\n                unit_feats[x][y][8]=unit_type\n                \n        return unit_feats","cbffbc60":"class GameFeatures:\n    def __init__(self, game_state):\n        self.game_state=game_state\n        self.resource_features=ResourceFeatures(game_state)\n        self.unit_features=UnitFeatures(game_state)\n        self.city_features=CityFeatures(game_state)\n        \n    def get_map_features(self, game_state):\n        resource_feats=self.resource_features.get_grid_resources(game_state)\n        city_feats=self.city_features.get_city_features(game_state)\n        unit_feats=self.unit_features.get_unit_feats(game_state)\n        feats=np.concatenate([resource_feats, city_feats, unit_feats ], axis=2)\n        return feats\n    \n    def get_private_features(self, feats, x, y):\n        (W, H) = (feats.shape[0], feats.shape[1])\n        w=4; h=4\n        \n        (x1, x2) = (x-w, x+w)\n        (y1, y2) = (y-h, y+h)\n\n        subx1=max(0, x1); subx2=min(W, x2)\n        suby1=max(0, y1); suby2=min(H, y2)\n\n        pad_left=0 if x1>=0 else -x1\n        pad_right=0 if x2<W else x2-W+1\n        pad_bottom=0 if y1>=0 else -y1\n        pad_top=0 if y2<H else y2-H+1\n\n        sub_feats=feats[subx1: 1+subx2, suby1: 1+suby2]\n        sub_feats=np.pad(sub_feats, [(pad_left, pad_right), (pad_top, pad_bottom), (0, 0)],  constant_values=[(0, 0)])\n        sub_feats=torch.tensor(sub_feats, dtype=torch.float32)\n        sub_feats=sub_feats.transpose(0, 2)\n        return sub_feats","80b73a08":"def get_reward(player):\n    num_tiles=0\n    num_units=len(player.units)\n    \n    worker=player.units[0]\n    (worker_x, worker_y)=(worker.pos.x, worker.pos.y)    \n    \n    city_survival=0.0\n    worker_survival = min(0, (worker.cargo.wood-40)\/40) # range [-1, 0]\n    worker_survival = 1 if worker_survival>=0 else worker_survival #worker survival range [-1, +1]\n    \n    \n    for city_id, city in player.cities.items():\n        fuel=city.fuel\n        num_tiles+=len(city.citytiles)\n        city_survival = min( 0, (fuel-230)\/23) #[-1, 0]\n        city_survival = 10 if city_survival>=0 else city_survival \n        \n        for citytile in city.citytiles:    \n            (city_x, city_y)=(citytile.pos.x, citytile.pos.y)\n            if (city_x == worker_x) and (city_y == worker_y) and city_survival>0:\n                worker_survival=1\n                break\n                \n    total_reward=(5 * num_units) + (20*num_tiles)\n    total_reward += (worker_survival + city_survival)\n    \n    return (num_tiles, num_units, total_reward)","da10bd8f":"class ConvModule(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ConvModule, self).__init__()\n        self.conv1=nn.Conv2d(in_channels, 2*in_channels, 3, stride=1, padding=1)\n        self.relu1=nn.ReLU6()\n        self.bn1=nn.BatchNorm2d(2*in_channels)\n        self.dropout1=nn.Dropout2d(0.1)\n        \n        \n        self.conv2=nn.Conv2d(2*in_channels, out_channels, 3, stride=1, padding=1)\n        self.relu2=nn.ReLU6()\n        self.bn2=nn.BatchNorm2d(out_channels)\n        self.dropout2=nn.Dropout2d(0.1)\n    def forward(self, x):\n        x=self.conv1(x)\n        x=self.relu1(x)\n        x=self.bn1(x)\n        x=self.dropout1(x)\n        \n        x=self.conv2(x)\n        x=self.relu2(x)\n        x=self.bn2(x)\n        x=self.dropout2(x)\n        return x\n    \nclass Backbone(nn.Module):\n    def __init__(self):\n        super(Backbone, self).__init__()\n        self.conv1=ConvModule(18, 64)\n        self.conv2=ConvModule(64, 64)\n        self.conv3=ConvModule(64, 64)\n        self.conv4=ConvModule(64, 64)\n        self.conv5=ConvModule(64, 64)\n        \n        self.avg_pool=nn.AvgPool2d(9)\n    def forward(self, x):\n        batch_size=x.shape[0]\n        \n        x=self.conv1(x)\n        x=self.conv2(x)\n        x=self.conv3(x)\n        x=self.conv4(x)\n        x=self.conv5(x)\n        \n        x=self.avg_pool(x)\n        \n        x=x.view(batch_size, -1)\n        return x\n    \n    \nclass WorkerPolicy(nn.Module):\n    def __init__(self):\n        super(WorkerPolicy, self).__init__()\n        self.backbone=Backbone()\n        \n        #Actions: move(N, S, E, W) or no action\n        # 0-N, 1-S, 2-E, 3-W\n        # 4-No Action\n        self.actor=nn.Sequential(\n            nn.BatchNorm1d(64),\n            nn.Linear(64, 64),\n            nn.ReLU6(),\n            nn.Dropout(0.1),\n            \n            nn.BatchNorm1d(64),\n            nn.Linear(64, 5)\n        )\n        self.critic=nn.Sequential(\n            nn.BatchNorm1d(64),\n            nn.Linear(64, 64),\n            nn.ReLU6(),\n            nn.Dropout(0.1),\n            \n            nn.BatchNorm1d(64),\n            nn.Linear(64, 1)\n        )\n        \n    def forward(self, x):\n        x=self.backbone(x)\n        actions=self.actor(x).softmax(dim=-1)\n        values=self.critic(x).view(-1)\n        return (actions, values)","0faea6ff":"class GameDataset(torch.utils.data.Dataset):\n    def __init__(self, mpfeatures, nxt_mpfeatures, returns, actions):\n        self.mpfeatures=mpfeatures\n        self.nxt_mpfeatures=nxt_mpfeatures\n        self.returns=returns\n        self.actions=actions\n        \n    def __getitem__(self, idx):\n        X=self.mpfeatures[idx]\n        X_next=self.nxt_mpfeatures[idx]\n        R=self.returns[idx]\n        action=self.actions[idx]\n        \n        X=torch.tensor(X, dtype=torch.float32)\n        X_next=torch.tensor(X_next, dtype=torch.float32)\n        R=torch.tensor(R, dtype=torch.float32)\n        action=torch.tensor(action, dtype=torch.long)\n        return (X, X_next, R, action)\n    \n    def __len__(self):\n        return len(self.mpfeatures)","f5b43414":"class PlayGame:\n    def __init__(self, worker_policy, evaluate=False):\n        self.env=make(\"lux_ai_2021\",\n                      configuration={\"seed\": random.randint(0, 100000000), \n                                     \"loglevel\": 0,\n                                     \"annotations\": False,\n                                     \"width\": 12,\n                                     \"height\": 12\n                                    },\n                      debug=True)\n        self.max_turns=0\n        self.evaluate=evaluate\n        self.worker_policy=worker_policy\n        self.game_state=None\n        self.game_features=None\n        self.states=[]\n        self.rewards=[]\n        self.worker_actions=[]\n    \n    def update_experience(self, feats, reward, worker_action):\n        self.states.append(feats)\n        self.worker_actions.append(worker_action)\n        self.rewards.append(reward)\n        \n    def get_worker_actions(self, game_features, game_state):\n        actions=[]\n        team=game_state.id\n        feats=game_features.get_map_features(game_state)\n        player=game_state.players[team]\n        units=player.units\n        \n        for unit in units:\n            if unit.can_act():\n                (x, y)=(unit.pos.x, unit.pos.y)\n                unit_feats=game_features.get_private_features(feats, x, y)\n                self.worker_policy.eval()\n                with torch.no_grad():\n                    worker_actions, values=self.worker_policy(unit_feats.unsqueeze(0))\n                    if (worker_actions is None):\n                        continue\n                    if self.evaluate:\n                        worker_action=np.argmax( worker_actions.view(-1).numpy() )\n                    else:\n                        worker_action=np.random.choice(5, p=worker_actions.view(-1).numpy())\n                    (num_tiles, num_units, total_reward)=get_reward(player)\n                    self.update_experience(unit_feats, total_reward, worker_action)\n                    \n                    if num_tiles > 0 and num_units>0:\n                        self.max_turns=game_state.turn\n                        \n                    if worker_action == 0:\n                        actions.append( unit.move('n') )\n                    elif worker_action==1:\n                        actions.append( unit.move('s') )\n                    elif worker_action==2:\n                        actions.append( unit.move('e') )\n                    elif worker_action==3:\n                        actions.append( unit.move('w') )\n                    elif worker_action==4:\n                        pass\n                        #Make No Move --> to ensure the resources are collected.\n                        #actions.append( unit.build_city() )\n        return actions\n    \n    \n    def agent(self, observation, configuration):\n        ### Do not edit ###\n        if observation[\"step\"] == 0:\n            self.game_state = Game()\n            self.game_state._initialize(observation[\"updates\"])\n            self.game_state._update(observation[\"updates\"][2:])\n            self.game_state.id = observation.player\n            self.game_features=GameFeatures(self.game_state)\n        else:\n            self.game_state._update(observation[\"updates\"])\n\n        ### AI Code goes down here! ### \n        actions=self.get_worker_actions(self.game_features, self.game_state)\n        # add debug statements like so!\n        return actions\n    \n    def play(self):\n        self.env.run([ self.agent,  \"simple_agent\"])\n        #if self.evaluate:\n            #self.env.render(mode=\"ipython\", width=700, height=700)","f49c162e":"def optimize_policy(optimizer, worker_policy, states, next_states, returns, worker_actions):\n    dataset=GameDataset(states, next_states, returns, worker_actions)\n    dataloader=torch.utils.data.DataLoader(dataset,\n                                           batch_size=256,\n                                           shuffle=True,\n                                           drop_last=False\n                                          )\n    worker_policy.train()\n    for (X, X_next, R, action) in dataloader:\n        if R.shape[0] < 100:\n            continue\n        optimizer.zero_grad()\n        policy, state_values = worker_policy(X)\n        _, nxt_state_values = worker_policy(X_next)\n\n        critic_reward = (R + 0.9*0.9 * nxt_state_values).detach() - state_values\n        actor_loss = -torch.log( torch.gather(policy, 1, action.unsqueeze(-1)) ).view(-1)\n        actor_loss = actor_loss * critic_reward.detach()\n        actor_loss = actor_loss.mean()\n\n        critic_loss = critic_reward ** 2\n        critic_loss = torch.sqrt(critic_loss.mean())\n\n        loss = (actor_loss + critic_loss)\/2\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(worker_policy.parameters(), 1)\n        optimizer.step()\n        \n        return (loss.item(), actor_loss.item(), critic_loss.item())","c0b06286":"eval_every=100\ntrain_every=100\nnum_eval_games=100\nnum_games=5000\n\nall_states=[]; all_actions=[]; allR=[]; all_next_states=[]\neval_mean_turns=[];\ntrain_turns=[]\n\n\nworker_policy=WorkerPolicy()\noptimizer=torch.optim.Adam(worker_policy.parameters(), lr=1e-4)\n\nfor i in range(num_games):\n    game=PlayGame(worker_policy)\n    game.play()\n    (states, actions, rewards) = (game.states, game.worker_actions, game.rewards)\n    train_turns.append(game.max_turns)\n    R=[]\n    for k in range(len(rewards)):\n        if k== len(rewards)-1:\n            R.append(rewards[k])\n            continue\n\n        r1=rewards[k]\n        r2=rewards[k+1]\n\n        R.append(r1+(0.9*r2))\n\n    all_states+=states[:-2]\n    all_actions+=actions[:-2]\n    allR+=R[:-2]\n    all_next_states+=states[2:]\n\n    if (i+1)%train_every == 0:\n        print(\"Training at game no. \", i+1)\n        (loss, actor_loss, critic_loss) = optimize_policy(optimizer, worker_policy,\n                                                          all_states, all_next_states, allR, all_actions)\n        torch.save(worker_policy, \"worker_policy_{}\".format(i+1))\n        all_states=[]; all_actions=[]; allR=[]; all_next_states=[]\n        print(\"Loss:{:.3f} | Actor Loss:{:.3f} | Critic Loss:{:.3f}\".format(loss, actor_loss, critic_loss))\n        \n    if (i+1)%eval_every == 0:\n        turns=[]\n        print(\"Evaluating at game no.\", i+1)\n        for j in range(num_eval_games):\n            game=PlayGame(worker_policy, evaluate=True)\n            game.play()\n            turns.append(game.max_turns)\n        eval_mean_turns.append(np.mean(turns))\n        print(\"Training mean number of survival turns:\", np.mean(train_turns))\n        print(\"Evaluation Mean number of turns of survival:\", np.mean(turns))","269ab71a":"plt.figure(figsize=(17, 6))\nplt.title(\"Evaluation Turns\")\nplt.plot(eval_mean_turns)\nplt.show()","83b9e797":"plt.figure(figsize=(17, 6))\nplt.title(\"Training Turns\")\nplt.plot(train_turns)\nplt.show()","59ffbb9c":"# Feature extraction","fdad008a":"# Dataloader","bed95a31":"# simulation","f4428f3d":"# Model","ca509e6d":"# Reward function"}}