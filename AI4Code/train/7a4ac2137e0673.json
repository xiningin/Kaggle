{"cell_type":{"3087cb83":"code","dd8ed1a3":"code","43d9a48b":"code","c82c4669":"code","4109d8b9":"code","f981cabd":"code","d8a2e458":"code","59582fc7":"code","87169145":"code","f115a7d5":"code","b3639567":"code","d495786f":"code","13e58b9d":"code","5cb1d680":"code","444da633":"code","8f1b0615":"code","59ef8e2b":"markdown","5d80c7a8":"markdown","95d8eb5a":"markdown","e6f72bf7":"markdown","f4e8034b":"markdown","8dc2bb15":"markdown","d931c4fa":"markdown"},"source":{"3087cb83":"# import libraries \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O\nimport matplotlib.pyplot as plt # Import matplotlib for data visualisation\nimport seaborn as sns\nimport h5py #It lets you store huge amounts of numerical data, and easily manipulate that data from NumPy\nimport random \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","dd8ed1a3":"filename = '\/kaggle\/input\/train_happy.h5'\nf = h5py.File(filename, 'r')\n\nfor key in f.keys():\n    print(key) #Names of the groups in HDF5 file.","43d9a48b":"happy_training = h5py.File('\/kaggle\/input\/train_happy.h5', \"r\")\nhappy_testing  = h5py.File('\/kaggle\/input\/test_happy.h5', \"r\")","c82c4669":"X_train = np.array(happy_training[\"train_set_x\"][:]) \ny_train = np.array(happy_training[\"train_set_y\"][:]) \n\nX_test = np.array(happy_testing[\"test_set_x\"][:])\ny_test = np.array(happy_testing[\"test_set_y\"][:]) \n\n#print(X_train, y_train, X_test, y_test)\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","4109d8b9":"i = random.randint(1,600) # select any random index from 1 to 600\nplt.imshow( X_train[i] )\nprint(y_train[i])","f981cabd":"W_grid = 3\nL_grid = 3\n\nfig, axes = plt.subplots(L_grid, W_grid, figsize = (9,9))\n\naxes = axes.ravel() # flaten the 15 x 15 matrix into 225 array\n\nn_training = len(X_train) # get the length of the training dataset\n\nfor i in np.arange(0, W_grid * L_grid): # create evenly spaces variables \n\n    index = np.random.randint(0, n_training)\n    axes[i].imshow( X_train[index])\n    axes[i].set_title(y_train[index], fontsize = 25)\n    axes[i].axis('off')\n\nplt.subplots_adjust(hspace=0.4)\n","d8a2e458":"# Let's normalize dataset\nX_train = X_train\/255\nX_test = X_test\/255","59582fc7":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\nfrom keras.optimizers import Adam\nfrom keras.callbacks import TensorBoard","87169145":"cnn_model = Sequential() #Specifying the input shape\n\ncnn_model.add(Conv2D(64, 6, 6, input_shape = (64,64,3), activation='relu'))\ncnn_model.add(MaxPooling2D(pool_size = (2, 2)))\n\ncnn_model.add(Dropout(0.2))\n\ncnn_model.add(Conv2D(64, 5, 5, activation='relu'))\ncnn_model.add(MaxPooling2D(pool_size = (2, 2)))\n\ncnn_model.add(Flatten())\ncnn_model.add(Dense(output_dim = 128, activation = 'relu'))\ncnn_model.add(Dense(output_dim = 1, activation = 'sigmoid'))","f115a7d5":"cnn_model.compile(loss ='binary_crossentropy', optimizer=Adam(lr=0.001),metrics =['accuracy'])","b3639567":"epochs = 9\n\nhistory = cnn_model.fit(X_train,\n                        y_train,\n                        batch_size = 60,\n                        nb_epoch = epochs,\n                        verbose = 1)","d495786f":"evaluation = cnn_model.evaluate(X_test, y_test)\nprint('Test Accuracy : {:.3f}'.format(evaluation[1]))","13e58b9d":"# get the predictions for the test data\npredicted_classes = cnn_model.predict_classes(X_test)\n","5cb1d680":"print(\"predicted_classes shape : \", predicted_classes.shape)\nprint(\"y_test shape : \", y_test.shape)","444da633":"L = 5\nW = 5\nfig, axes = plt.subplots(L, W, figsize = (12,12))\naxes = axes.ravel() # \n\nfor i in np.arange(0, L * W):  \n    axes[i].imshow(X_test[i])\n    axes[i].set_title(\"Prediction Class = {}\\n True Class = {}\".format(predicted_classes[i], y_test[i]))\n    axes[i].axis('off')\n\nplt.subplots_adjust(wspace=0.5)","8f1b0615":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test.T, predicted_classes))","59ef8e2b":"# STEP #4: TRAINING THE MODEL","5d80c7a8":"# STEP #5: EVALUATING THE MODEL","95d8eb5a":"# STEP #3: VISUALIZATION OF THE DATASET  ","e6f72bf7":"# STEP #1: PROBLEM STATEMENT AND BUSINESS CASE","f4e8034b":"# CASE STUDY: SMILING FACES DETECTOR","8dc2bb15":"* The dataset contains a series of images that can be used to solve the Happy House problem!\n* We need to build an artificial neural network that can detect smiling faces.\n* Only smiling people will be allowed to enter the house!\n* The train set has 600 examples. The test set has 150 examples.\n* Data Source: https:\/\/www.kaggle.com\/iarunava\/happy-house-dataset","d931c4fa":"# STEP #2: IMPORTING DATA"}}