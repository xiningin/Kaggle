{"cell_type":{"e6f35e06":"code","6f3294f8":"code","f2f60aab":"code","04ff47ba":"code","039a08f1":"code","03e2b2fb":"code","24648d7b":"code","62b4e140":"code","67beb936":"code","6c7c6854":"markdown","12c5a6ca":"markdown","a710d4f0":"markdown","2de23978":"markdown","94981ca1":"markdown","c566aecf":"markdown","51401f3d":"markdown","a2326de6":"markdown","366b63db":"markdown","abbde898":"markdown","c39e5b48":"markdown","e60810f0":"markdown","a2d94a9d":"markdown","43980558":"markdown","cf6a775d":"markdown","8d28bdc3":"markdown","33224c02":"markdown"},"source":{"e6f35e06":"# base libs\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport math\n# model libs\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings(\"ignore\")","6f3294f8":"# dataframe information\ndef data_info(df):\n    \n    null_series = df.isnull().sum()\n    print(f\"######### MISSING VALUES:\")\n    print(f\"{null_series[null_series > 0]}\\n\")\n    \n    print(f\"######### LABEL BALANCE:\")\n    print(f\"{pd.value_counts(df['label'])}\\n\")\n          \n    print(f\"######### DESCRIBE:\")\n    print(f\"{df.describe()}\\n\")\n    \n    for column in df:\n        print('######### COLUMN NAME:', column)\n        print('TYPE:', df[column].dtypes)\n        print('LEN:', len(df[column]))\n        print('NUNIQUE:', df[column].nunique())\n        print('NaN:', df[column].isnull().values.sum())\n        print('')\n        \n    print(msno.bar(df, figsize=(16, 4)))       \n\n\n# prepare neural network input\ndef df_to_nnetwork(df, normalization=True, verbose=False):\n    \n    # dataframe to neural network input format\n    dataset = df.values\n    nb_features = len(chosen_feat)\n\n    X = dataset[:,1:nb_features].astype(float)\n    Y = dataset[:,0]\n    if verbose:\n        print(f\"######### RAW DATA STATISTICS:\")\n        print(f\"{stats.describe(X)}\\n\")\n          \n    # data normalization\n    if normalization:\n        X = (X - X.min()) \/ (X.max() - X.min())\n        if verbose:\n            print(f\"######### MEAN NORMALIZED DATA STATISTICS:\")\n            print(f\"{stats.describe(X)}\\n\")\n\n    # encode class values as integers\n    encoder = LabelEncoder()\n    encoder.fit(Y)\n    encoded_Y = encoder.transform(Y)\n\n    # convert integers to dummy variables (i.e. one hot encoded)\n    Y = to_categorical(encoded_Y)\n    if verbose:\n        print(f\"######### NEURAL NETWORK INPUT FORMAT:\")\n        print(f\"Data Shape: {X.shape}\")\n        print(f\"Labels Shape: {Y.shape}\")\n        \n    return X, Y\n\n\n# model build    \ndef build_model(X, Y, nb_classes, verbose=False):\n    \n    # definitions\n    neurons = 16\n    input_dim = len(X[0])\n    depth = 3\n\n    # define baseline model\n    def baseline_model():\n\n        # create model\n        model = Sequential()\n        for i in range(depth):\n            model.add(Dense(neurons, input_dim=input_dim, activation='relu'))\n        model.add(Dense(nb_classes, activation='softmax'))\n        # Compile model\n        model.compile(loss='categorical_crossentropy', \n                      optimizer='adam', \n                      metrics=['accuracy'])\n\n        return model\n        \n    model = baseline_model() \n    if verbose:\n        print(model.summary())\n    \n    return model  \n\n\n# quantitative model metrics\ndef result_metrics(data, labels, model, classes):\n\n    y_pred = model.predict(data)\n    y_pred = (y_pred > 0.5)\n    print('######### ACCURACY: %.2f%%' % (accuracy_score(labels, y_pred)*100))\n\n    cm = confusion_matrix(labels.argmax(axis=1), \n                          y_pred.argmax(axis=1))\n    print('\\n######### CONFUSION MATRIX:\\n\\n',cm)\n\n    print(f\"\\n######### METRICS LABELS: {classes}\\n\\n\", classification_report(labels, y_pred))\n    print(\"\")\n    \n\n# learning curves visualization    \ndef model_plots(history):\n    \n    print(\"\\n######### LEARNING CURVES:\")      \n    plt.figure(figsize=(18, 6))\n    \n    # summarize history for accuracy\n    plt.subplot(1, 2, 1)\n    plt.plot(history['accuracy'])\n    plt.title(f\"model accuracy\")\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='best')\n\n    # summarize history for loss\n    plt.subplot(1, 2, 2)\n    plt.plot(history['loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper right')\n\n    plt.show()\n","f2f60aab":"# definitions\nlabel = 'SARS-Cov-2 exam result'\n\n# load data from excel\npath = '\/kaggle\/input\/covid19\/'\ndf = pd.read_excel(path+\"dataset.xlsx\")\n\nprint(f\"######### ALL DATASET FEATURES:\")\nfeat_pool = []\nfor column in df:\n    if column != label:\n        feat_pool.append(column)\n#print(feat_pool)  \n\n# shuffle the rows reseting indexs\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# show dataset balance\nprint(f\"\\n######### LABELS BALANCING:\")\ndf[label].value_counts().plot.barh()","04ff47ba":"# data preprocess config\ndrop_nan = True\n\n# feature list\nchosen_feat = [df['SARS-Cov-2 exam result'], \n               df['Patient addmited to regular ward (1=yes, 0=no)'],\n               df['Patient addmited to semi-intensive unit (1=yes, 0=no)'],\n               df['Patient addmited to intensive care unit (1=yes, 0=no)'],\n               #df['Patient age quantile'],\n               df['Leukocytes'], \n               df['Platelets'], \n               #df['Eosinophils'],\n               df['Monocytes'],\n               #df['Neutrophils'],\n               #df['Hematocrit'],\n               #df['Hemoglobin']\n              ]\nkeys=['label', \n      'regular',\n      'semi_intensive',\n      'intensive',\n      #'age_quantile',\n      'Leukocytes', \n      'Platelets', \n      #'Eosinophils',\n      'Monocytes',\n      #'Neutrophils',\n      #'Hematocrit',\n      #'Hemoglobin'\n     ]\n\n# create a reduced dataframe with keys features \ndf = pd.concat(chosen_feat, axis=1, keys=keys)\n\n# drop NaN rows \nif drop_nan:\n    df = df.dropna(axis=0).reset_index(drop=True) \n\n# ajust data for just positive diagnostic    \ndf = df[df[\"label\"]==\"positive\"].reset_index(drop=True)  \n\n# redefining labels\ndf['label'] = df['label'].map(lambda x: None)\nfor i in range(len(df)):\n    if df['regular'].loc[i] == 0 and df['semi_intensive'].loc[i] == 0 and df['intensive'].loc[i] == 0:\n        df['label'].loc[i] = 'home'\n        continue\n    if df['regular'].loc[i] == 1:\n        df['label'].loc[i] = 'regular'\n        continue\n    if df['semi_intensive'].loc[i] == 1:\n        df['label'].loc[i] = 'semi_intensive_or_intensive'\n        continue\n    if df['intensive'].loc[i] == 1:\n        df['label'].loc[i] = 'semi_intensive_or_intensive'\n\n# drop some columns        \ndf = df.drop(['regular', 'semi_intensive', 'intensive'], axis=1)        \n\n# shuffle data\ndf = df.sample(n=len(df), random_state=42).reset_index(drop=True)\n\n# general information about the data\ndata_info(df) ","039a08f1":"# plot correlations between best features\nfeats_set = keys[4:]\nprint(\"######### FEATURES SET CORRELATIONS:\", feats_set)\nsns.pairplot(df, hue='label')","03e2b2fb":"# heat map\nprint(\"######### FEATURES HEAT MAP:\")\nplt.figure(figsize=(10,8))\nmyBasicCorr = df[feats_set].corr('spearman')\nsns.heatmap(myBasicCorr, annot = True)","24648d7b":"# prepare neural network input\ndata, labels = df_to_nnetwork(df, normalization=True, verbose=True)","62b4e140":"# train paramters\nclasses = {0:'home', 1:'regular', 2:'semi_intensive_or_intensive'} \nnb_classes = len(classes)\nbatch_size = 2\nepochs = 300\n\n# build the deep learning model\nmodel = build_model(data, labels, nb_classes, verbose=False)  \n\n# run!!\nHistory = model.fit(data, \n                    labels, \n                    batch_size=batch_size, \n                    epochs=epochs)","67beb936":"# visualization of learning curves\nmodel_plots(History.history)\n\n# quantitative metrics\nprint(f\"######### FEATURES: {keys[4:]}\\n\")\nresult_metrics(data, labels, model, classes)","6c7c6854":"# Best Result:\n### Accuracy: 57.83%, Features: ['Leukocytes', 'Platelets', 'Monocytes']","12c5a6ca":"# Result Metrics","a710d4f0":"<font size=\"4\">4- At first, we are dealing with a 4 classes problem: (\"regular ward\", \"semi-intensive unit\", \"intensive care unit\", \"none of then\"). The last one \"none of then\" i gently called \"home\". As we have a very limited amount of realiable data, I strongely recommend to reduce the complexity of the task to get more reliable predictions in the real world. My new 3 classes are: (\"regular\", \"semi_intensive_or_intensive\", \"home\"). I just joined the semi_intensive and intensive classes in one.<\/font>","2de23978":"# Preprocessing Data","94981ca1":"<font size=\"4\">3- Replacing NaNs with mean values doesnt works either!! \nThis makes the data very biased because\nthe numbers of NaN values in this dataset is very high.<\/font>\n","c566aecf":"# Important notes:","51401f3d":"# Challenge Task:\n<font size=\"4\">TASK 2\n\u2022 Predict admission to general ward, semi-intensive unit or intensive care unit among confirmed COVID-19 cases.\nBased on the results of laboratory tests commonly collected among confirmed COVID-19 cases during a visit to the emergency room, would it be possible to predict which patients will need to be admitted to a general ward, semi-intensive unit or intensive care unit?<\/font>","a2326de6":"<font size=\"4\">5- This deep learning model will predicted way long better if we add more data. <\/font>","366b63db":"# Parser to Deep Learning Model Input","abbde898":"# Features Data Correlations","c39e5b48":"<font size=\"4\">1- My approach is a simple Neural Network model that can be easily extended to a more robust version but we are dealing with a very incomplete dataset so more robust neural network models will not help here anyway.<\/font>","e60810f0":"# Model Build and Run ","a2d94a9d":"# Load Data","43980558":"### Accuracy: 57.83%, Features: ['Leukocytes', 'Platelets', 'Monocytes']\n### Accuracy: 56.63%, Features: ['age_quantile', 'Leukocytes', 'Platelets', 'Monocytes']\n### Accuracy: 55.42%, Features: ['Leukocytes', 'Platelets', 'Eosinophils']\n### Accuracy: 54.22%, Features: ['age_quantile', 'Leukocytes', 'Platelets', 'Eosinophils', 'Monocytes']\n### Accuracy: 50.67%, Features: ['age_quantile', 'Leukocytes', 'Platelets', 'Eosinophils', 'Monocytes', 'Neutrophils', 'Hematocrit', 'Hemoglobin']\n### Accuracy: 49.40%, Features: ['Leukocytes', 'Platelets']\n### Accuracy: 40.96%, Features: ['age_quantile', 'Leukocytes', 'Platelets']","cf6a775d":"# Notebook Methods","8d28bdc3":"<font size=\"4\">2- We should be very carefull with unbalanced data (much more negatives than positives) because\nthis alone can cause a heavy data overfitting (causing dangerous misleading interpretrations) especially \nusing neural networks in healthcare. On task2 this is even worse because we are dealing only with the \"positive\" subset. So I preprocessed the data using only non NaN rows. This reduces the amount of data and overall accuracy but the results are much more resilient for real world prediction.<\/font>\n","33224c02":"# Results:"}}