{"cell_type":{"bd5f23eb":"code","dd57ed71":"code","542a2e85":"code","7f2b0173":"code","b4db3d78":"code","42cfb588":"code","ba929bc4":"code","40debbcb":"code","30efc67f":"code","3f40c8be":"code","c31e36ce":"code","1a1bd7f3":"code","da970801":"code","969e0970":"code","20273d7e":"code","4c9c09a6":"code","f9f81a36":"code","e47b7c87":"code","e8abf9e1":"code","b0f37183":"code","b6033e0e":"code","dcc430ec":"code","56adf9ec":"code","76e7bdea":"code","f191ac66":"code","2b143b3c":"markdown","d25407bf":"markdown","4ab8cd75":"markdown","ab24245a":"markdown","eec523d9":"markdown","c774bc72":"markdown","4d0eb9a5":"markdown","34011921":"markdown","1a4c4415":"markdown"},"source":{"bd5f23eb":"#Importing Libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn import linear_model, decomposition, datasets\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler","dd57ed71":"from sklearn.metrics import r2_score,classification_report,f1_score,matthews_corrcoef,recall_score,plot_roc_curve\nfrom sklearn.metrics import accuracy_score, confusion_matrix,precision_score,mean_squared_error,mean_absolute_error\n%matplotlib inline\nimport seaborn as sns","542a2e85":"#Loading the dataset\ndata = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\n\n#Print the first 5 rows of the dataframe.\ndata.head()","7f2b0173":"X = data.drop('DEATH_EVENT',axis=1)\ny = data['DEATH_EVENT']","b4db3d78":"X.shape","42cfb588":"## Train test Split\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1)","ba929bc4":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score","40debbcb":"log_reg = LogisticRegression()","30efc67f":"log_reg.fit(X_train,y_train)","3f40c8be":"logr_pred = log_reg.predict(X_test)","c31e36ce":"accuracyLR = accuracy_score(y_test,logr_pred)\naccuracyLR","1a1bd7f3":"log_reg = LogisticRegression()\ngrid = {\"penalty\" : [\"l1\", \"l2\"],\"C\" : np.arange(0,100,1)}\nlog_reg_cv = GridSearchCV(log_reg, grid, cv=3)\nlog_reg_cv.fit(X_train,y_train)","da970801":"print(\"Tuned hyperparameter n_estimators: {}\".format(log_reg_cv.best_params_)) \nprint(\"Best score: {}\".format(log_reg_cv.best_score_))\nprint(\"Best Estimator: {}\".format(log_reg_cv.best_estimator_))","969e0970":"results_NB = pd.DataFrame(log_reg_cv.cv_results_['params'])\nresults_NB['test_score'] = log_reg_cv.cv_results_['mean_test_score']\nresults_NB","20273d7e":"#Performance Comparison for Logistics Regression\nimport matplotlib.pyplot as plt\nfor i in ['l1', 'l2']:\n    temp = results_NB[results_NB['penalty'] == i]\n    temp_average = temp.groupby('C').agg({'test_score': 'mean'})\n    plt.plot(temp_average, marker = '.', label = i)\n    \n    \nplt.legend()\nplt.xlabel('C')\nplt.ylabel(\"Mean CV Score\")\nplt.title(\"Logistic Regression Performance Comparison\")\nplt.show()","4c9c09a6":"model_LR = log_reg_cv.best_estimator_\nmodel_LR.fit(X_train,y_train)\npredictions_LR =  model_LR.predict(X_test)\nprint('\\n')\nprint('Accuracy: ', accuracy_score(y_test,predictions_LR))\nprint('f1-score:', f1_score(y_test, predictions_LR))\nprint('Precision score: ', precision_score(y_test,predictions_LR))\nprint('Recall score: ', recall_score(y_test,predictions_LR))\nprint('MCC: ',matthews_corrcoef(y_test,predictions_LR) )\nprint('Mean Squared Error:', mean_squared_error(y_test, predictions_LR) ** 0.5)\nprint('Mean Absolute Error:', mean_absolute_error(y_test, predictions_LR) ** 0.5)\nprint('\\n')\nprint(classification_report(y_test, predictions_LR))\nprint('\\n')\nax= plt.subplot()\nsns.heatmap(confusion_matrix(y_test, predictions_LR), annot=True, ax = ax, fmt='g')\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')","f9f81a36":"\nfrom sklearn.metrics import roc_curve, auc","e47b7c87":"clf = LogisticRegression()\nmodel=log_reg.fit(X_train,y_train)\npred_val = log_reg.predict(X_test)\n\n### Compute ROC curve and ROC area for predictions on validation set\nfpr, tpr, _ = roc_curve(y_test, pred_val)\nroc_auc = auc(fpr, tpr)\n\n### Plot\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","e8abf9e1":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\nprint(\"accuracy: \", accuracy_score(y_test, predictions))","b0f37183":"grid = {\"n_estimators\" : np.arange(0,200,2)}\nrf = RandomForestClassifier()\nrf_random = GridSearchCV(rf, grid, cv=3)\nrf_random.fit(X_train,y_train)","b6033e0e":"print(rf_random.best_params_)\nprint(rf_random.best_estimator_)","dcc430ec":"results_NB = pd.DataFrame(rf_random.cv_results_['params'])\nresults_NB['test_score'] = rf_random.cv_results_['mean_test_score']\nresults_NB","56adf9ec":"#NB Performance Comparison \nplt.plot(results_NB['n_estimators'], results_NB['test_score'], marker = '.') \nplt.xlabel('n_estimators')\nplt.ylabel(\"Mean CV Score\")\nplt.title(\"NB Performance Comparison\")\nplt.show()","76e7bdea":"model_RF = rf_random.best_estimator_\nmodel_RF.fit(X_train,y_train)\npredictions_RF =  model_RF.predict(X_test)\nprint('\\n')\nprint('Accuracy: ', accuracy_score(y_test,predictions_RF))\nprint('f1-score:', f1_score(y_test, predictions_RF))\nprint('Precision score: ', precision_score(y_test,predictions_RF))\nprint('Recall score: ', recall_score(y_test,predictions_RF))\nprint('MCC: ',matthews_corrcoef(y_test,predictions_RF) )\nprint('Mean Squared Error:', mean_squared_error(y_test, predictions_RF) ** 0.5)\nprint('Mean Absolute Error:', mean_absolute_error(y_test, predictions_RF) ** 0.5)\nprint('\\n')\nprint(classification_report(y_test, predictions_RF))\nprint('\\n')\nax= plt.subplot()\nsns.heatmap(confusion_matrix(y_test, predictions_RF), annot=True, ax = ax, fmt='g')\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\nax.set_title('Confusion Matrix')","f191ac66":"\n### Fit a sklearn classifier on train dataset and output probabilities\nclf = RandomForestClassifier()\nmodel=clf.fit(X_train, y_train)\npred_val = clf.predict(X_test)\n\n\n\n### Compute ROC curve and ROC area for predictions on validation set\nfpr, tpr, _ = roc_curve(y_test, pred_val)\nroc_auc = auc(fpr, tpr)\n\n### Plot\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='black',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='green', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","2b143b3c":"Here DEATH EVENT is Dependent Feauture","d25407bf":"Area under curve is 0.81,Performance of Model is Good","4ab8cd75":"# Hyperparameter Tuning of Random Forest","ab24245a":"Got Accuracy of 82 percent after hyper parameter tuning","eec523d9":"PY SAGAR -- Compared Logistic Regression and Random Forest -(With Confusion Matrix, HyperParameter Tuning,ROC Curve)","c774bc72":"# Random Forest","4d0eb9a5":"Got Accuracy of 86.6 % Accuracy in Logistics Regression","34011921":"# Hyper parameter tuning of Logistics Regression","1a4c4415":"Area under curve is 0.87,Means model is good"}}