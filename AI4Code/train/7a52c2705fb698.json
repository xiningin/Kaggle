{"cell_type":{"b47b67ac":"code","f9fdb3dd":"code","b6aa00be":"code","2dd8c623":"code","b8129fcc":"code","c7d0c839":"code","7a142eec":"code","85f8ca11":"code","bbe22d77":"code","0fd97e4e":"code","7757eec9":"code","b473d553":"code","366a6233":"code","f805d1ef":"code","2cf32d5b":"code","d4620827":"code","2fc3289d":"code","fd7ff36f":"code","1dedfcfc":"code","f3d56812":"code","05969416":"code","903358a2":"code","bbc1ac2e":"code","822a8504":"code","d0be9953":"code","5f40113a":"code","11662e3e":"code","040525fb":"code","11ede5d3":"code","2fd14c9b":"code","72403e52":"markdown","e59130c0":"markdown","3fc89c67":"markdown","cbbc65ba":"markdown","ad6ac899":"markdown","f9f64ace":"markdown","77244aa4":"markdown","470ec767":"markdown","363c90ea":"markdown"},"source":{"b47b67ac":"import pandas as pd \nimport os\nfrom catboost import CatBoostClassifier, Pool\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nimport shap\nshap.initjs()\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f9fdb3dd":"df = pd.read_csv(\"\/kaggle\/input\/air-marshal-misconduct\/FederalAirMarshalMisconduct.csv\",parse_dates=[\"Date Case Opened\"])\ndisplay(df)","b6aa00be":"df.isna().sum()","2dd8c623":"print(\"% duplicate rows\", (100*(df.shape[0] - df.drop_duplicates().shape[0])\/df.shape[0]))\n## note: dropping Final Disposition results in more duplicates","b8129fcc":"df[\"Date Case Opened\"].describe(datetime_is_numeric=True)","c7d0c839":"for c in df.columns:\n    print(c)\n    print(\"# unique values\/cardinality\",df[c].nunique())\n    print(df[c].value_counts())","7a142eec":"df[\"Field Office \"] = df[\"Field Office \"].str.upper()\ndf[\"Field Office \"] = df[\"Field Office \"].fillna(\"\") # fill missing values\nprint(\"Field Office unique values after casing\",df[\"Field Office \"].nunique())\ndf['Allegation '] = df['Allegation '].str.lower()\nprint(\"Allegation unique values after casing\",df[\"Allegation \"].nunique())","85f8ca11":"df[\"year\"] = df[\"Date Case Opened\"].dt.year\ndf[\"month\"] = df[\"Date Case Opened\"].dt.month\ndf[\"week\"] = df[\"Date Case Opened\"].dt.isocalendar().week.astype(int) # replaces dt.week","bbe22d77":"target_freq_background = df[\"target\"].value_counts(normalize=True).round(4)\ntarget_freq_background","0fd97e4e":"df.groupby(\"year\")[\"target\"].value_counts(normalize=True)\n##TODO: Join and divide by background target frequency, to see change in target freq, given feature\/time. \n## apply this to other features for EDA","7757eec9":"df","b473d553":"## code from: https:\/\/stackoverflow.com\/questions\/29836836\/how-do-i-filter-a-pandas-dataframe-based-on-value-counts    \ndf = df.loc[df['target'].map(df['target'].value_counts()) > 20]\n\n## drop original datecol and the \"raw\" outcomes col\ndf.drop([\"Date Case Opened\",\"Final Disposition\"],axis=1,inplace=True)\nprint(df.shape)","366a6233":"print(\"% duplicate rows\", (100*(df.shape[0] - df.drop_duplicates().shape[0])\/df.shape[0]))\nprint(\"# rows left if we drop duplicates:\",df.drop_duplicates().shape[0])\nprint(\"missing vals \\n\",df.isna().sum())\n\n\n# df = df.drop_duplicates()","f805d1ef":"X = df.drop(\"target\",axis=1)\ny= df[\"target\"]","2cf32d5b":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42) # the \"stop word\" of ML code ;)","d4620827":"## Identufy categorical columns in features:  (We coyuld also use textCols, but then we lose interpretability with current catbosot)\n## good practice for cases with lots of cols: \nCAT_COLS = list(X.select_dtypes(include=\"O\").columns)\nCAT_COLS","2fc3289d":"model = CatBoostClassifier(\n#         iterations=600,\n        loss_function='MultiClass',\n        eval_metric='AUC',         \n#         task_type=\"GPU\",\n        verbose=False)\n\n# we could improve model with a validation split or CV to avoid overfitting\nmodel.fit(\n        X_train, y_train, plot=True,\n        cat_features=CAT_COLS,       \n#     cat_features=['Field Office '],text_features= ['Allegation '] # catboost doesn't yet support feature importance for text it's own text features. We can do without here\n);","fd7ff36f":"y_pred = model.predict(X_test)\nprint(classification_report(y_true=y_test, y_pred=y_pred))","1dedfcfc":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(Pool(X, y,\n#                                          cat_features=['Field Office '],text_features= ['Allegation ']\n                                         cat_features=CAT_COLS\n                                        ))","f3d56812":"# feature importance plot overall\nshap.summary_plot(shap_values, X)","05969416":"list(y.unique())\n## hopefully shap picked classes in order the yappeared in data? ","903358a2":"## class \"7\" - retirement?\nshap.summary_plot(shap_values[6], X)","bbc1ac2e":"## class \"3\" - resignation?\nshap.summary_plot(shap_values[3], X)","822a8504":"df2 = df[[\"target\",\"year\"]].copy()\ndf2[\"is_target\"] = (df2[\"target\"]==\"suspension\").astype(int)\nprint(\"total suspension by year:\")\ndf2.groupby(\"year\").sum().plot();","d0be9953":"print(\"fraction of complaints handled as suspension by year:\");\ndf2.groupby(\"year\").mean().plot()","5f40113a":"## class \"5\" -removal ?\nshap.summary_plot(shap_values[5], X)","11662e3e":"## class \"2\" -  letter of counsel\nshap.summary_plot(shap_values[2], X)","040525fb":"## class \"0\" - verbal counsel\nshap.summary_plot(shap_values[0], X)","11ede5d3":"## class \"1\" - suspension ?\nshap.summary_plot(shap_values[1], X)","2fd14c9b":"## class \"6\" - no further action?\nshap.summary_plot(shap_values[6], X)","72403e52":"* Dropping \"genuine\" duplicates isn't ideal as we lose EDA info and recurring complaints that may be easy to classify.","e59130c0":"* Some simple feature\/target analysis\n\n* TODO: Join and divide by background target frequency, to see change in target freq, given feature\/time. \n* apply this to other features for EDA","3fc89c67":"* `Field Office` : a (very few) misspellings (lAX vs LAX). We'll clean by uppercasing all. \n    * This column could be attached to external data about the airports - how busy they are for example, how many international flights, specific carriers, hubs etc\n    * We will clean the short input texts and target columns by uppercasing\n* `target` = `Final Disposition` cafter cleaning (e.g. merging of Suspension 1\/3\/X days together, and text lowercasing). \n\n","cbbc65ba":"## Feature Engineering\n* Clean text\n* Add basic calendar features\n    * Much more can be done - adding external data, checking holidays, time-series\/history of office (nvm individual in question) etc'","ad6ac899":"* We see that resignations\/class 3 are much less likely to occur in later years!\n* Lets plot the total amount of resignations by year ","f9f64ace":"## Features importance\n* Use SHAP for now - score is importance of each feature, given all other features in model. \n* SHAP doesn't seem to include anything \"easy\" for getting the class names","77244aa4":"We can also get a shap plot of features importance per class (i.e a \"binary\" view):\n","470ec767":"## Model & Target\n* Drop rarest target outcomes - keep those with at least 20 outcomes cases\n* Simple catboost model as a baseline + Shap for explanations\n    * Use pool and define categorical columns for catboost","363c90ea":"#### EDA\n* Analyze input data & output\/`target` frequencies\n* Drop \"Final Disposition\" column (leak).\n* Keep subset of target column (due to cardinality) \n* We could also strip the whitespace from column names if handling further"}}