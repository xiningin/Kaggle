{"cell_type":{"71a328d6":"code","6e13046f":"code","e60d9a53":"code","d01dcb18":"code","db343280":"code","0b18e0d1":"code","d335d39a":"code","760147af":"code","4b74eb46":"code","64ef5c70":"code","0da0ad05":"code","a7d3611a":"code","68c266d3":"code","97c43169":"code","e6304b18":"code","d475e32b":"code","5a28092c":"code","47ffa076":"code","f4bc328f":"code","059796dc":"code","95dcc14d":"code","1deb5196":"markdown","b3afc63c":"markdown","ac8b8a81":"markdown","b8dfe833":"markdown","43cb7250":"markdown","604dbb1e":"markdown","50054041":"markdown","b416b524":"markdown","1b0ca87f":"markdown","951568a2":"markdown"},"source":{"71a328d6":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport keras\nfrom keras.models import Sequential, load_model\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.utils import np_utils\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\nfrom kerastuner.tuners import RandomSearch # HyperParameter Tunining\nfrom keras.optimizers import Adam # Optimizer used in the NN\nfrom keras.callbacks import EarlyStopping # Early Stopping Callback in the NN\n\nimport itertools\n\nimport pandas as pd\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 100)\npd.set_option('display.width', 1000)\n\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\nimport matplotlib\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\nprint('Shape of the train data: ',train.shape)\nprint('Shape of the test data: ',test.shape)","6e13046f":"# Save test data with ID for results\ntest_id = test\n\n# Drop ID\n#train.drop(['Id'], axis=1, inplace=True)\n#test.drop(['Id'], axis=1, inplace=True)\n\n# Log is used to respond to skewness towards large values \n#train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\ny = train['SalePrice'].reset_index(drop=True)\n\n# Put test & train features to one df\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test\nfeatures = pd.concat([train_features, test_features]).reset_index(drop=True)\n\n# Removing features that are not very useful:\n# - more then 48% of NaN at start : 'PoolQC','MiscFeature','Alley','Fence','FireplaceQu'\nfeatures = features.drop(['PoolQC','MiscFeature','Alley','Fence','FireplaceQu'], axis=1)\n# - more then 95% of the same data in 'Utilities', 'Street'\nfeatures = features.drop(['Utilities', 'Street'], axis=1)\n\n# Since these column are actually a category , using a numerical number will lead the model to assume\n# that it is numerical , so we convert to string .\nfeatures['MSSubClass'] = features['MSSubClass'].apply(str)\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)\n\n## Filling these columns With most suitable value for these columns \nfeatures['Functional'] = features['Functional'].fillna('Typ') \nfeatures['Electrical'] = features['Electrical'].fillna(\"SBrkr\") \nfeatures['KitchenQual'] = features['KitchenQual'].fillna(\"TA\") \n\n## Filling these with MODE, i.e. , the most frequent value in these columns .\nfeatures['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0]) \nfeatures['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\nfeatures['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n\n### Missing data in GarageYrBit most probably means missing Garage, so replace NaN with zero . \nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    features[col] = features[col].fillna(0)\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    features[col] = features[col].fillna('None')\n\n### Same with basement\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    features[col] = features[col].fillna('None')\n\n# This code will filll the missing values with the mode \n# (The frequently category appearing) By each MSsubclass:\n# Idea is that similar MSSubClasses will have similar MSZoning\nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\n# Fill the remaining columns as None\nobjects = []\nfor i in features.columns:\n    if features[i].dtype == object:\n        objects.append(i)\nfeatures.update(features[objects].fillna('None'))\n\n# For missing values in numerical cols , we fillNa with 0\n# We are still filling up missing values \nfeatures['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics.append(i)\nfeatures.update(features[numerics].fillna(0))\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics2 = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics2.append(i)\nskew_features = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nfor i in skew_index:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))","e60d9a53":"# Adding new features. Sums of categiries.\nfeatures['YrBltAndRemod']=features['YearBuilt']+features['YearRemodAdd']\nfeatures['TotalSF']=features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\nfeatures['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                                 features['1stFlrSF'] + features['2ndFlrSF'])\nfeatures['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\nfeatures['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                              features['EnclosedPorch'] + features['ScreenPorch'] +\n                              features['WoodDeckSF'])\n\n## If PoolArea = 0 , Then HasPool = 0 too, ...\nfeatures['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\n#  Get_dummies converts Categorical data to numerical  # \nfinal_features = pd.get_dummies(features).reset_index(drop=True)\n\nprint(final_features.shape)\n# final_features","d01dcb18":"# Outliers with IsolationForest\nfrom sklearn.ensemble import IsolationForest\n\nclf = IsolationForest(max_samples = 50, random_state = 42) # many samples to decrease outliers quantity\nclf.fit(final_features)\ny_noano = clf.predict(final_features)\ny_noano = pd.DataFrame(y_noano, columns = ['Top'])\ny_noano[y_noano['Top'] == 1].index.values\n# final_features.iloc[y_noano[y_noano['Top'] != 1].index.values] # table with Outliers only\n# final_features.iloc[y_noano[y_noano['Top'] == 1].index.values] # table without Outliers\n\noutliers = y_noano.iloc[y_noano[y_noano['Top'] != 1].index.values]\noutliers = list(outliers.index.values)\n\n# Outliers manualy finded: 30, 88, 462, 631, 1322\noutliers_manual  =  [30, 462, 631, 1322]\noutliers.extend(outliers_manual)\noutliers.sort()\n\nnew_outliers=[]\nfor x in outliers:\n    if x <= 1460:  # clean train data only\n        new_outliers.append(x)","db343280":"##########################################################\n#  Now, again train and test are spilt back seperately,  #\n#  as now all data processing is done.                   #\n#  Y is taget and its length is used to split            #\n##########################################################\n\nX = final_features.iloc[:len(y), :]\nX_sub = final_features.iloc[len(y):, :]\nX.shape, y.shape, X_sub.shape","0b18e0d1":"X_result = X_sub #.reset_index()\n#X_result.rename(columns={'index': 'Id'}, inplace=True)\nX_result","d335d39a":"# Removing outliers.\n# Can be seen by plotting them in a graph.\nX = X.drop(X.index[new_outliers])\ny = y.drop(y.index[new_outliers])\n\nprint('Deleted outliers: ',new_outliers)","760147af":"#X = X.join(y) # add back 'SalePrice' to train","4b74eb46":"scale = StandardScaler()\nX = scale.fit_transform(X)","64ef5c70":"def build_model(hp):\n    model = Sequential()\n    for i in range(hp.Int('layers', 2, 10)):\n        model.add(Dense(units=hp.Int('units_' + str(i),\n                                            min_value=32,\n                                            max_value=512,\n                                            step=32),\n                               activation='relu'))\n    model.add(Dense(1))\n    model.compile(\n        optimizer=Adam(\n            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n        loss='mse',\n        metrics=['mse'])\n    return model\n\ntuner = RandomSearch(\n    build_model,\n    objective='val_mse',\n    max_trials=10,\n    executions_per_trial=3,\n    directory='model_dir',\n    project_name='House_Price_Prediction')\ntuner.search_space_summary()\n\n# tuner.search(X[1100:],y[1100:],batch_size=128,epochs=200,validation_data=validation_data=(X[:1100],y[:1100]))\n# model = tuner.get_best_models(1)[0]\n\n# After implementing this and tuning further we get the below model that I have implemented separately.Won't be running this here.\n\ndef create_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(320, input_dim=X.shape[1], activation='relu'))\n    model.add(Dense(384, activation='relu'))\n    model.add(Dense(352, activation='relu'))\n    model.add(Dense(448, activation='relu'))\n    model.add(Dense(160, activation='relu'))\n    model.add(Dense(160, activation='relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(1))\n    # Compile model\n    model.compile(optimizer=Adam(learning_rate=0.0001), loss = 'mse')\n    return model","0da0ad05":"model = create_model()\nmodel.summary()","a7d3611a":"# We would be using early stopping callback and would use 1\/10th \n# of the training data as validation to estimate the optimum number of epochs that would prevent overfitting\n\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\nhistory = model.fit(x=X,y=y,\n          validation_split=0.1,\n          batch_size=128,epochs=1000, callbacks=[early_stop])\nlosses = pd.DataFrame(model.history.history)\n","68c266d3":"losses.plot()\n\nmodel = create_model() # Resetting the model.","97c43169":"# Training the model with full training data and optimum number of epochs!!\n\nhistory = model.fit(x=X,y=y,\n          batch_size=128,epochs=104)\nlosses = pd.DataFrame(model.history.history)","e6304b18":"losses.plot()","d475e32b":"model.evaluate(X,y)","5a28092c":"test","47ffa076":"X_result","f4bc328f":"X_test = scale.transform(X_result)\nresult = model.predict(X_test)\nresult = pd.DataFrame(result,columns=['SalePrice'])\nresult.head()\nresult['Id'] = test['Id']\nresult = result[['Id','SalePrice']]\nresult.head()","059796dc":"result.to_csv(\"submission.csv\", index=False)","95dcc14d":"\"\"\"\nsample_submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\n\nimport matplotlib.pyplot as plt\nfig = plt.figure(figsize=(20,20),linewidth=2)\n\nax1 = fig.add_subplot(211)\nax1.plot(result[['SalePrice']])\n\nax2 = fig.add_subplot(211)\nax2.plot(sample_submission[['SalePrice']])\n\nplt.legend([\"my\",\"sample\"], fontsize=15)\n           \nplt.xlabel('ID', fontsize=15);\nplt.show()\n\"\"\"","1deb5196":"## <center> Data processing:","b3afc63c":"## <center> Feature Engineering:","ac8b8a81":"## <center> Import lib & load data","b8dfe833":"# <center> Prediction & Evaluation","43cb7250":"## <center> Outliers:","604dbb1e":"## Main goal:\nThe main goal is to make a model that allows us to make the most accurate price predictions.\nThe path to this goal lies through a deep understanding of the data to be processed.\nWe need to get away from working with a black box.The better we understand each of the 79 variables, the better our model will perform.\n\nPreviously we made a model on this data with **SCORE 0.119** using **RIDGE**, **LASSO**, **elastic net**, **SVR**, **lightgbm**, **gbr**, **xgboost** and **blending models**. Now we want to test the theory that **Keras** and **TensorFlow** can improve the result.\n \nNotebooks for this data with different models:\n\n1. 20 rows of code version with low score * - \n   [House Sales (Forest)](https:\/\/www.kaggle.com\/dmitrikurochkin\/house-sales-forest\/)  - SCORE 0,222\n \n2. My best version at this moment - \n   [House Sales (Blending Models)](https:\/\/www.kaggle.com\/dmitrikurochkin\/house-sales-blending-models\/) - SCORE 0,119 (top 6% at Kaggle)\n\n3. Currient version - \n   [House Sales (Keras)](https:\/\/www.kaggle.com\/dmitrikurochkin\/house-sales-keras\/)  - SCORE 0,150\n\n\n       * sometimes we need any result in 5 min, then we can spend a week to made it better :-)","50054041":"## Brief description of the data set:\n\nThe Ames Housing dataset was compiled by Dean De Cock for use\nin data science education. It's an incredible alternative for data scientists\nlooking for a modernized and expanded version of the often cited\nBoston Housing dataset.\n\nData has:\n- 81 explanatory variables describing (almost) every aspect\nof residential homes;\n- 38 continuous features and 43 categorical features;\n- 2919 observations;\n- target Variable is the SalePrice.\n- variables can be grouped.\n\nA detailed description of the variables is in the data source:\nhttps:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/","b416b524":"# Conclusions and plans:\n\n1. Currient score of this model at kaggle.com is 0.15 and it's good, but not enough, because our last model with blending has score 0,119\n2. We need to try TensorFlow and PyTorch to improve the result. In general, even this model is a working tool without significant flaws, but it can be better\n","1b0ca87f":"# <center> A Neural Network Model for House Sales","951568a2":"# <center> MODELLING\n## <center> We would use Random Algorithm from keras for hyper-parameter tuning of the model."}}