{"cell_type":{"8842ca2e":"code","8faf8680":"code","8836c5f2":"code","9d374a4c":"code","6fca4615":"code","62939abb":"code","dde7b37f":"code","65dd6b9b":"code","b447f6e8":"code","131f7661":"code","fdd11c86":"code","b90a1ea3":"code","65ce2cfa":"code","e75a7745":"code","80b0d146":"code","eeb62490":"code","2ec5bc1b":"code","9f4d0cdc":"code","2966eca6":"code","41d24ca9":"code","9ca146f6":"code","b519789c":"code","052ef70c":"markdown","ee2af314":"markdown","7ec155af":"markdown","40a3e1b0":"markdown","6080bd83":"markdown"},"source":{"8842ca2e":"import numpy as np\nimport pandas as pd\nimport os\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report, roc_curve\nfrom sklearn.model_selection import learning_curve, cross_val_score, GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import RobustScaler,StandardScaler,MinMaxScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')","8faf8680":"image_dir = Path('..\/input\/a-large-scale-fish-dataset\/Fish_Dataset\/Fish_Dataset')\n\n# Get filepaths and labels\nfilepaths = list(image_dir.glob(r'**\/*.png'))\nlabels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n\nfilepaths = pd.Series(filepaths, name='Filepaths').astype(str)\nlabels = pd.Series(labels, name='Labels')\n\n# Concatenate filepaths and labels\nimage_df = pd.concat([filepaths, labels], axis=1)\n\n# Drop GT images\nimage_df = image_df[image_df['Labels'].apply(lambda x: x[-2:] != 'GT')]\n\n# Shuffle the DataFrame and reset index\nimage_df = image_df.sample(frac=1).reset_index(drop = True)\n\n# Show the result\nimage_df.head()","8836c5f2":"#Displaying a subsample of the dataset\nfig, axes = plt.subplots(nrows=3, ncols=5, figsize=(15,10), subplot_kw={'xticks':[], 'yticks':[]})\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(image_df.Filepaths[i]))\n    ax.set_title(image_df.Labels[i])\n    \nplt.show()","9d374a4c":"!pip install dataprep\nfrom dataprep.eda import create_report","6fca4615":"create_report(image_df)","62939abb":"import tensorflow as tf","dde7b37f":"# Separate in train and test data\ntrain_df, test_df = train_test_split(image_df, train_size=0.85, shuffle=True, random_state=1)","65dd6b9b":"train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n    validation_split=0.15\n)\n\ntest_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n)","b447f6e8":"train_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepaths',\n    y_col='Labels',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=64,\n    shuffle=True,\n    seed=42,\n    subset='training'\n)","131f7661":"val_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepaths',\n    y_col='Labels',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=64,\n    shuffle=True,\n    seed=42,\n    subset='validation'\n)","fdd11c86":"test_images = test_generator.flow_from_dataframe(\n    dataframe=test_df,\n    x_col='Filepaths',\n    y_col='Labels',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=64,\n    shuffle=False\n)","b90a1ea3":"from keras.models import Sequential, Model\nfrom keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D","65ce2cfa":"input_shape = (224, 224, 3)\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=input_shape ),\n    tf.keras.layers.MaxPool2D(pool_size = (2,2)),\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPool2D(pool_size = (2,2)),\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n    tf.keras.layers.MaxPool2D(pool_size = (2,2)),\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n    tf.keras.layers.MaxPool2D(pool_size = (2,2)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(9, activation='softmax')\n])\n\nmodel.summary()","e75a7745":"from tensorflow.keras.optimizers import Adam","80b0d146":"model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=[\"accuracy\"])\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=4)\n\nhistory = model.fit(train_images, validation_data=val_images, epochs=6, callbacks=callback)","eeb62490":"model.save('1rst-model.h5')","2ec5bc1b":"from tensorflow.keras.preprocessing.image import img_to_array, load_img\nimg_path=test_df['Filepaths'][5221]\n# Define a new Model, Input= image \n# Output= intermediate representations for all layers in the  \n# previous model after the first.\nsuccessive_outputs = [layer.output for layer in model.layers[1:]]\n#visualization_model = Model(img_input, successive_outputs)\nvisualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n#Load the input image\nimg = load_img(img_path, target_size=(224, 224))\n# Convert ht image to Array of dimension (150,150,3)\nx   = img_to_array(img)                           \nx   = x.reshape((1,) + x.shape)\n# Rescale by 1\/255\nx \/= 255.0\n# Let's run input image through our vislauization network\n# to obtain all intermediate representations for the image.\nsuccessive_feature_maps = visualization_model.predict(x)\n# Retrieve are the names of the layers, so can have them as part of our plot\nlayer_names = [layer.name for layer in model.layers]\nfor layer_name, feature_map in zip(layer_names, successive_feature_maps):\n  print(feature_map.shape)\n  if len(feature_map.shape) == 4:\n    \n    # Plot Feature maps for the conv \/ maxpool layers, not the fully-connected layers\n   \n    n_features = feature_map.shape[-1]  # number of features in the feature map\n    size       = feature_map.shape[ 1]  # feature map shape (1, size, size, n_features)\n    \n    # We will tile our images in this matrix\n    display_grid = np.zeros((size, size * n_features))\n    \n    # Postprocess the feature to be visually palatable\n    for i in range(n_features):\n      x  = feature_map[0, :, :, i]\n      x -= x.mean()\n      x \/= x.std ()\n      x *=  64\n      x += 128\n      x  = np.clip(x, 0, 255).astype('uint8')\n      # Tile each filter into a horizontal grid\n      display_grid[:, i * size : (i + 1) * size] = x\n# Display the grid\n    scale = 20. \/ n_features\n    plt.figure( figsize=(scale * n_features, scale) )\n    plt.title ( layer_name )\n    plt.grid  ( False )\n    plt.imshow( display_grid, aspect='auto', cmap='viridis' )","9f4d0cdc":"accuracy = history.history['accuracy']\nval_accuracy  = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']","2966eca6":"plt.figure(figsize=(15,10))\n\nplt.subplot(2, 2, 1)\nplt.plot(accuracy, label = \"Training accuracy\")\nplt.plot(val_accuracy, label=\"Validation accuracy\")\nplt.legend()\nplt.title(\"Training vs validation accuracy\")\n\n\nplt.subplot(2,2,2)\nplt.plot(loss, label = \"Training loss\")\nplt.plot(val_loss, label=\"Validation loss\")\nplt.legend()\nplt.title(\"Training vs validation loss\")\n\nplt.show()","41d24ca9":"pred = model.predict(test_images)\npred = np.argmax(pred, axis=1)\n\nlabels = train_images.class_indices\nlabels = dict((v,k) for k, v in labels.items())\n\nprint(labels)\nprint(pred)","9ca146f6":"y_pred = [labels[k] for k in pred]\nprint(classification_report(test_df.Labels, y_pred))\nprint('--------------------------------')\nprint(confusion_matrix(test_df.Labels, y_pred))\nprint('--------------------------------')","b519789c":"model.evaluate(test_images)[1]","052ef70c":"# Visualize the  filters","ee2af314":"## Performances","7ec155af":"# Modelling","40a3e1b0":"<h1><center>\ud83d\udc1fFish Classification\ud83d\udd0e<\/center><\/h1>\n<h3><center>\ud83d\udc20(Prediction at the end)\ud83d\udd2e<\/center><\/h3>\n<center><img src= \"https:\/\/i.pinimg.com\/originals\/38\/e8\/f6\/38e8f6e7abe5a3267752754694c7a5ec.jpg\" alt =\"Fish\" style='width: 600px;'><\/center>\n\n<h3>A Large-Scale Dataset for Segmentation and Classification<\/h3>\n<p>\n\nAuthors: O. Ulucan, D. Karakaya, M. Turkan\nDepartment of Electrical and Electronics Engineering, Izmir University of Economics, Izmir, Turkey\nCorresponding author: M. Turkan\nContact Information: mehmet.turkan@ieu.edu.tr\n\nPaper : A Large-Scale Dataset for Fish Segmentation and Classification\nGeneral Introduction\n\nThis dataset contains 9 different seafood types collected from a supermarket in Izmir, Turkey\nfor a university-industry collaboration project at Izmir University of Economics, and this work\nwas published in ASYU 2020.\nThe dataset includes gilt head bream, red sea bream, sea bass, red mullet, horse mackerel,\nblack sea sprat, striped red mullet, trout, shrimp image samples.\n\n<\/p>\n\n<h3>Purpose of the work<\/h3>\n<p>\n    \nThis dataset was collected in order to carry out segmentation, feature extraction, and classification tasks\nand compare the common segmentation, feature extraction, and classification algorithms (Semantic Segmentation, Convolutional Neural Networks, Bag of Features).\nAll of the experiment results prove the usability of our dataset for purposes mentioned above.\n\nData Gathering Equipment and Data Augmentation\n\nImages were collected via 2 different cameras, Kodak Easyshare Z650 and Samsung ST60.\nTherefore, the resolution of the images are 2832 x 2128, 1024 x 768, respectively.\n\nBefore the segmentation, feature extraction, and classification process, the dataset was resized to 590 x 445\nby preserving the aspect ratio. After resizing the images, all labels in the dataset were augmented (by flipping and rotating).\n\nAt the end of the augmentation process, the number of total images for each class became 2000; 1000 for the RGB fish images\nand 1000 for their pair-wise ground truth labels.\n<\/p>\n\n<h3>Description of the dataset<\/h3>\n<p>\n\nThe dataset contains 9 different seafood types. For each class, there are 1000 augmented images and their pair-wise augmented ground truths.\nEach class can be found in the \"Fish_Dataset\" file with their ground truth labels. All images for each class are ordered from \"00000.png\" to \"01000.png\".\n\nFor example, if you want to access the ground truth images of the shrimp in the dataset, the order should be followed is \"Fish->Shrimp->Shrimp GT\".\n    \n<\/p>","6080bd83":"## Creating the model"}}