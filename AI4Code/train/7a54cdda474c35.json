{"cell_type":{"dc85b2bc":"code","6a1269ae":"code","162d5819":"code","1b30d5e4":"code","69929d66":"code","44bd5cb2":"code","5b84aa2a":"code","64888f43":"code","f0bf48ba":"code","2ecd5b57":"code","21a53609":"code","2c135bc1":"code","ed63d592":"code","fb58df1c":"code","abe0d740":"code","a3010145":"code","7140dcc2":"code","5c7ac209":"code","72bfe36c":"code","5b09f68b":"code","351c4c58":"code","4ca375b6":"code","bc35ab06":"code","c5c7e06d":"code","e0fa00a7":"code","c8bc85c0":"code","3936c992":"code","b548d0e6":"code","48825de9":"code","ff6c649f":"code","17ef9dd1":"code","b4069f4c":"code","067ca29e":"code","968493c9":"code","5f995e59":"code","bc58b45a":"code","ea29875e":"code","57d7e7a5":"code","8a2f51ca":"code","bb21247c":"code","2e171d3a":"code","d8e775c5":"code","9354730b":"code","9801b5c0":"code","f8bf57e1":"code","79327674":"code","6657de2a":"code","89e207ff":"code","e6a8e090":"markdown","47b4f72c":"markdown","5cc05102":"markdown","deb047a5":"markdown","2272b8ff":"markdown","0eceb973":"markdown","fa696bbf":"markdown","9920b651":"markdown","062abf59":"markdown","1caf932d":"markdown","f08682c1":"markdown","d2d9bbdc":"markdown","a6235dbb":"markdown","9b34c7bc":"markdown","cef25bf5":"markdown","228ab20b":"markdown","b8ff10cf":"markdown","42cffd9d":"markdown","1691d0da":"markdown","2f4cd90a":"markdown","0bf79eb0":"markdown","c12770e8":"markdown","5b033872":"markdown","76e62e02":"markdown","594849b6":"markdown","58c0b1f5":"markdown","d8bbeda1":"markdown","bc562f25":"markdown","2f40693e":"markdown","caef8c12":"markdown","2f725e2c":"markdown"},"source":{"dc85b2bc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns #Data Viz\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom scipy import stats\nfrom datetime import datetime\nfrom scipy.stats import skew  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nimport plotly.graph_objs as go\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport scipy.stats as stats\nimport sklearn.linear_model as linear_model\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6a1269ae":"#Importing the data\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","162d5819":"#Checking the structure of train and test datasets\nprint('Train shape: ', train.shape)\nprint('Test Shape: ', test.shape)\n\n#Types of fields present in the test and train\nprint('Train: \\n', train.dtypes.value_counts())\nprint('Test: \\n', test.dtypes.value_counts())","1b30d5e4":"#merging test and train\nmerged_df = pd.concat([train, test], axis = 0, sort = True)\nprint('Merged Shape: \\n', merged_df.shape)\nprint('Merged : \\n', merged_df.dtypes.value_counts())\n","69929d66":"#Extracting quantitative and qualitative fields\nnumeric_merged = merged_df.select_dtypes(include = ['int64','float64'])\nobj_merged = merged_df.select_dtypes(include = ['object'])\n\n#Shapes\nprint('Shape of Numeric: \\n', numeric_merged.shape)\nprint('Shape of Object type: \\n', numeric_merged.shape)","44bd5cb2":"#Histograms for the numeric fields. We will create a 19X2 area for the 38 histograms.\nfig = plt.figure()\nfor i, variable in enumerate (numeric_merged.columns):\n    ax = fig.add_subplot(19, 2, i+1)\n    numeric_merged[variable].hist(bins = 50, ax = ax, color = 'blue', alpha = 0.5, figsize = (50,150))\n    ax.set_title (variable, fontsize = 35)\n    ax.tick_params(axis = 'both', which = 'major',labelsize = 30)\n    ax.tick_params(axis = 'both', which = 'minor',labelsize = 30)\n    ax.set_xlabel('')\n    \nplt.show()\n","5b84aa2a":"#Converting some numeric fields to object (categorical) fields\nmerged_df.loc[:, ['MSSubClass', 'OverallQual', 'OverallCond']] = merged_df.loc[:, ['MSSubClass', 'OverallQual', 'OverallCond']].astype('object')\n\n#new shape of merged_df\nprint('New Shape of merged data: \\n', merged_df.shape)\n\n#Datatypes in merged_df post conversion to object\nprint('Merged structure : \\n', merged_df.dtypes.value_counts())","64888f43":"#Generation & Visualisation of Correlation Matrix for train\nc_mat = train.corr()\nf, ax = plt.subplots(figsize=(15, 12))\nsns.heatmap(c_mat, vmax=.8, square=True);","f0bf48ba":"#Finding the 10 highest correlated fields to 'SalePrice'\nn = 10 #Number of fields\ncols = c_mat.nlargest(n, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","2ecd5b57":"#Scatter plot of GrLivArea v SalePrice\ntrain.plot.scatter(x='GrLivArea', y='SalePrice', title = 'GrLivArea v SalePrice');","21a53609":"#Dropping GrLivArea > 4000\ntrain.drop(train[train.GrLivArea>4000].index, inplace = True)\ntrain.reset_index(drop = True, inplace = True)\n\ndisplay(train.shape)","2c135bc1":"#Scatter plot of GarageCars v SalePrice\ntrain.plot.scatter(x='GarageCars', y='SalePrice', title = 'GarageCars v SalePrice');","ed63d592":"#Scatter plot of YearBuilt v SalePrice\ntrain.plot.scatter(x='YearBuilt', y='SalePrice', title = 'YearBuilt v SalePrice');\n\n#Drop observations where YearBulit is less than 1893 sq.ft\ntrain.drop(train[train.YearBuilt<1900].index, inplace = True)\ntrain.reset_index(drop = True, inplace = True)","fb58df1c":"#Scatter plot of YearBuilt v SalePrice\ntrain.plot.scatter(x='YearBuilt', y='SalePrice', title = 'YearBuilt v SalePrice');","abe0d740":"#Scatter plot of OverallQual v SalePrice\ntrain.plot.scatter(x='OverallQual', y='SalePrice', title = 'OverallQual v SalePrice');","a3010145":"#Scatter plot of FullBath v SalePrice\ntrain.plot.scatter(x='FullBath', y='SalePrice', title = 'FullBath v SalePrice');\n\n#Scatter plot of TotalBsmtSF v SalePrice\ntrain.plot.scatter(x='TotalBsmtSF', y='SalePrice', title = 'TotalBsmtSF v SalePrice');","7140dcc2":"#Removing obs with 'TotalBsmtSF' > 3000\ntrain.drop(train[train.TotalBsmtSF>3000].index, inplace = True)\ntrain.reset_index(drop = True, inplace = True)","5c7ac209":"#Scatter plot of TotalBsmtSF v SalePrice after outlier removal\ntrain.plot.scatter(x='TotalBsmtSF', y='SalePrice', title = 'TotalBsmtSF v SalePrice');","72bfe36c":"#Extracting 'SalePrice' and dropping from feature set\ny_train = train.SalePrice\ntrain.drop('SalePrice', axis = 1, inplace = True)\ntrain.shape\n\n#merging with test\nmerged_df = pd.concat([train, test], axis = 0)\nmerged_df.shape\n","5b09f68b":"#Converting some numeric fields to object (categorical) fields\nmerged_df.loc[:, ['MSSubClass', 'OverallQual', 'OverallCond']] = merged_df.loc[:, ['MSSubClass', 'OverallQual', 'OverallCond']].astype('object')\n\n#new shape of merged_df\nprint('New Shape of merged data: \\n', merged_df.shape)\n\n#Datatypes in merged_df post conversion to object\nprint('Merged structure : \\n', merged_df.dtypes.value_counts())","351c4c58":"#Columns with missing information\nmissing_col = merged_df.columns[merged_df.isnull().any()].values\nprint(missing_col)","4ca375b6":"missing_col = len(merged_df) - merged_df.loc[:, np.sum(merged_df.isnull())>0].count()\nmissing_col","bc35ab06":"'''We can see that for 'MiscFeature', NaN means absence of the feature as per the description'''\nmerged_df['MiscFeature'].head(10)","c5c7e06d":"'''We can see that for 'Fence', NaN means absence of the feature as per the description'''\nmerged_df['Fence'].head(10)","e0fa00a7":"#Replacing the NaN in the above 14 fields fith \"None\"\nreplace_none = merged_df.loc[:, ['PoolQC','Fence','MiscFeature','GarageCond','GarageQual','GarageFinish','GarageType','Alley','FireplaceQu','BsmtFinType2','BsmtFinType1','BsmtExposure','BsmtCond','BsmtQual']]\n\nfor i in replace_none.columns:\n    merged_df[i].fillna('None', inplace = True)\n    \n#After removal of the 'NaN'\nmissing_col = len(merged_df) - merged_df.loc[:, np.sum(merged_df.isnull())>0].count()\nmissing_col","c8bc85c0":"#Replacing non-numeric missing by corresponding mode\nreplace_mode = merged_df.loc[:, ['MSZoning', 'Electrical', 'Utilities', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'KitchenQual', 'Functional', 'SaleType']]\n\nfor i in replace_mode.columns:\n    merged_df[i].fillna(merged_df[i].mode()[0], inplace = True)\n    \n#After removal of the 'NaN'\nmissing_col = len(merged_df) - merged_df.loc[:, np.sum(merged_df.isnull())>0].count()\nmissing_col","3936c992":"#Replacing numeric missing by corresponding Median\nreplace_mode = merged_df.loc[:, ['LotFrontage','MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'GarageYrBlt', 'GarageCars', 'GarageArea']]\n\nfor i in replace_mode.columns:\n    merged_df[i].fillna(merged_df[i].median(), inplace = True)\n    \n#After removal of the 'NaN'\nmissing_col = len(merged_df) - merged_df.loc[:, np.sum(merged_df.isnull())>0].count()\nmissing_col","b548d0e6":"#Checking Skewness of 'SalePrice'\nprint('Skewness of SalePrice is: ', y_train.skew())\n\n#Checking normality of 'SalePrice'\nsns.distplot(y_train, fit = norm)\nfigure = plt.figure()","48825de9":"#Taking log of 'SalePrice'\ny_train = np.log1p(y_train)\n\n#Re-checking normality after transformation\nsns.distplot(y_train, fit = norm)\nfigure = plt.figure()","ff6c649f":"#Checking skewness of numeric features\nnumeric_skew = pd.DataFrame(data = merged_df.select_dtypes(include = ['int64', 'float64']).skew(), columns = ['Skewness'])\nnumeric_skew_sort = numeric_skew.sort_values(ascending = False, by = 'Skewness')\nnumeric_skew_sort","17ef9dd1":"#Extract all numeric fields from merged_df\nmerged_df_num = merged_df.select_dtypes(include = ['int64', 'float64'])\n\n#Transform the numeric fields with Skewness > 0.5\nmerged_df_skew = np.log1p(merged_df_num[merged_df_num.skew()[merged_df_num.skew() > 0.5].index])\n\n#Pick up non-skewed features\nmerged_df_noskew = merged_df_num[merged_df_num.skew()[merged_df_num.skew() < 0.5].index]\n\n#Merging Skewed and non-skewed population\nmerged_df_num_final = pd.concat([merged_df_skew, merged_df_noskew], axis = 1)\n\n#Merge with the main numeric dataframe\nmerged_df_num.update(merged_df_num_final)","b4069f4c":"#Using StandardScaler to scale the numeric features\nstandard_scaler = StandardScaler()\nmerged_df_num_scaled = standard_scaler.fit_transform(merged_df_num)\nmerged_df_num_scaled = pd.DataFrame(data = merged_df_num_scaled, columns = merged_df_num.columns, index = merged_df_num.index)","067ca29e":"#Extracting the categoricals\nmerged_df_cat = merged_df.select_dtypes(include = ['object']).astype('category')\nmerged_df_cat.shape\n\n#Using LabelEncoder for the categorucal features\nmerged_df_cat_encoded = merged_df_cat.apply(LabelEncoder().fit_transform)\nmerged_df_cat_encoded.shape\n\n#OneHotEncoding using Pandas\nmerged_df_one_hot = merged_df_cat.select_dtypes(include = ['category'])\nmerged_df_one_hot = pd.get_dummies(merged_df_one_hot, drop_first = True)\nmerged_df_one_hot.shape\n\n#Merging label encoded and one hot encoded features together\nmerged_df_cat_encoded_final = pd.concat([merged_df_one_hot, merged_df_cat_encoded], axis = 1)\n\n#Joining the processed numeric and categorical fields\nmerged_df_final_processed =pd.concat([merged_df_num_scaled, merged_df_cat_encoded_final], axis = 1)\nmerged_df_final_processed.shape","968493c9":"#Isolating Train and Test features\ntrain_final = merged_df_final_processed.iloc[0:1438, :]\ntest_final = merged_df_final_processed.iloc[1438:, :]\n\n#Target field\ny_train = y_train\n\nX = train_final\ny = y_train","5f995e59":"#Setting Random State\nstate = 40\n\n#KFolds Cross Validation\nkfolds = KFold(n_splits=10, shuffle=True, random_state=state)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)","bc58b45a":"alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]","ea29875e":"ridge = RidgeCV(alphas=alphas_alt, cv=kfolds)\nlasso = LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds)\nelasticnet = ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio)                            \nsvr = SVR(C= 20, epsilon= 0.008, gamma=0.0003)","57d7e7a5":"gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =42)                             ","8a2f51ca":"lightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       )","bb21247c":"xgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)","2e171d3a":"stack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","d8e775c5":"'''ab = AdaBoostRegressor(random_state = state)\nrf = RandomForestRegressor(n_jobs = -1, random_state = state)\nknn = KNeighborsRegressor(n_jobs= -1)\ndt = DecisionTreeRegressor(random_state = state)'''","9354730b":"score = cv_rmse(ridge)\nscore = cv_rmse(lasso)\nprint(\"LASSO: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(elasticnet)\nprint(\"elastic net: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(svr)\nprint(\"SVR: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(gbr)\nprint(\"gbr: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(xgboost)\nprint(\"xgboost: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\n'''Excluding as RMSE was quite high'''\n\n'''score = cv_rmse(ab)\nprint(\"adaboost: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(rf)\nprint(\"Random Forest: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(knn)\nprint(\"KNN: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(dt)\nprint(\"Decision Tree: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )'''","9801b5c0":"#Fitting the data\n\nprint('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(y))\n\nprint('elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y)\n\nprint('Lasso')\nlasso_model_full_data = lasso.fit(X, y)\n\nprint('Ridge')\nridge_model_full_data = ridge.fit(X, y)\n\nprint('Svr')\nsvr_model_full_data = svr.fit(X, y)\n\nprint('GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)\n\nprint('xgboost')\nxgb_model_full_data = xgboost.fit(X, y)\n\nprint('lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)","f8bf57e1":"def blend_models_predict(X):\n    return ((0.1 * elastic_model_full_data.predict(X)) + \\\n            (0.05 * lasso_model_full_data.predict(X)) + \\\n            (0.1 * ridge_model_full_data.predict(X)) + \\\n            (0.1 * svr_model_full_data.predict(X)) + \\\n            (0.1 * gbr_model_full_data.predict(X)) + \\\n            (0.15 * xgb_model_full_data.predict(X)) + \\\n            (0.1 * lgb_model_full_data.predict(X)) + \\\n            (0.3 * stack_gen_model.predict(np.array(X))))","79327674":"print('RMSLE score on train data:')\nprint(rmsle(y, blend_models_predict(X)))","6657de2a":"#Creating Submission\nprint('Predict submission')\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(blend_models_predict(test_final)))","89e207ff":"submission.head(5)\nsubmission.to_csv('submission.csv',index=False)","e6a8e090":"Finding the columns with missing information.","47b4f72c":"<h2>Importing Data<\/h2>","5cc05102":"<h4>Non-Numeric Missiong Fields where 'NaN'does not have any relevant meaning<\/h4>\n\nFor the non-numeric fields, missing would be replaced by Mode. Of the above, the following will be replaced by Mode of the observations.\n* MSZoning\n* Utilities\n* Exterior1st\n* Exterior2nd\n* MasVnrType\n* KitchenQual\n* Functional\n* SaleType","deb047a5":"From the graphs above and the description of the fields, we can safely say that the fields - 'MSSubClass', 'OverallQual' and 'OverallCond'. We can convert them to 'object' data type for categorical field handling.","2272b8ff":"Now we can separate the train and test features","0eceb973":"<h2>Model Fitting<\/h2>","fa696bbf":"<h2>Importing Libraries<\/h2>\nFirst step is to import the libraries. Using the regular ones for the usual tasks and Seaborn for the data visualisation. Also including some libraries for the preprocessing and feature scaling.","9920b651":"Checking correlation between the different features and 'SalePrice' in Train dataset using heatmaps.","062abf59":"<h2>Encoding the Categorical Vars<\/h2>\n\nThe categorical features need to be encoded before fitting the model. We will use the LabelEncoder from sklearn for the encoding.","1caf932d":"\nFrom the heatmap, we can say the following:\n1. 'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'\n1. 'GarageCars' and 'GarageArea' are also pretty strongly correlated to 'SalePrice'. However, we can choose to keep only one of these as they are quite related. I will choose 'GarageCars'.\n1. 'TotalBsmtSF' and '1stFloorSF' are strongly correlated to 'SalePrice'. I am taking '1stFloorSF' as it should in most cases be equal to 'TotalBsmtSF'.\n\nAlso, we can find out the 10 fields with the heighest correlation factors to 'SalePrice'","f08682c1":"Next, we have the following.\n* FullBath\n* TotalBsmtSF","d2d9bbdc":"We can see that 'TotalBsmtSF' has 3 outlier cases where the value is >3000. We will remove these.","a6235dbb":"As a next step, we need to check the linearity of the above mentioned fields with 'SalePrice'.","9b34c7bc":"Next, we will look at the different fields and add change things as and when required. To simplify the process and to make sure we apply the same changes to both test and train datasets, we will now merge the two together.","cef25bf5":"We will apply transformation to all the fields with skewness > 0.5 in the merged dataframe.","228ab20b":"We can start with the features which have the highest number of missing values.\n\n*MiscFeature*\n\nThe description says:\n'Miscellaneous feature not covered in other categories\n\t\t\n       Elev\tElevator\n       Gar2\t2nd Garage (if not described in garage section)\n       Othr\tOther\n       Shed\tShed (over 100 SF)\n       TenC\tTennis Court\n       NA\tNone'\n","b8ff10cf":"The numeric features must be scaled before training the model. We will use Standard Scalar for the scaling bits.","42cffd9d":"As we can see from the above sampling, few of the fields, even though they are numeric and quantative, are actually categorical. For example, for the feature 'OverallQual', the description says:\n\n'OverallQual: Rates the overall material and finish of the house\n\n       10\tVery Excellent\n       9\tExcellent\n       8\tVery Good\n       7\tGood\n       6\tAbove Average\n       5\tAverage\n       4\tBelow Average\n       3\tFair\n       2\tPoor\n       1\tVery Poor'\n       \nWe can take a look at the histograms of the numeric features to identify the ones that can be taken as categorical fields.","1691d0da":"<h2>Checking distributions and applying transformations<\/h2>\nIts evident that **'SalePrice'** needs to be predicted based on the feature variables. 'SalePrice' needs to be analysed for normality before using multivariate analysis on it. First step is to check the skewness of 'SalePrice'. As a rule of thumb, if skewness is <-1 or >1, data is highly skewed. Uniform data would usually have skewness between -0.5 and 0.5.","2f4cd90a":"<h2>Analysing the Data using EDA techniques<\/h2>\nNext step would be to extract the quantitative and qualitative features.\n* Quantitative : include int64 and float64\n* Qualitative : include object","0bf79eb0":"Next up is 'YearBuilt'.There dont seem to be any outliers as such.","c12770e8":"For the remaining, we can fill them by the Median.","5b033872":"As we can see, 'SalePrice' is not normally distributed. Taking log of 'SalePrice' and then checking for normality. After applying the transformation, 'SalePrice' is much more normally distributed.","76e62e02":"<h4>Non-numeric missing fields where 'NaN' actually means 'Not Present' as per description<\/h4>\n\nLooking at the description and following the above process, the following fields were identified where \"NaN\" actually means that the corresponding feature is missing.\n1. MiscFeature\n2. Fence\n3. PoolQC          \n4. GarageCond\n5. GarageQual\n6. GarageFinish\n7. GarageType\n8. FireplaceQu\n9. BsmtFinType2\n10. BsmtFinType1\n11. BsmtExposure\n12. BsmtCond\n13. BsmtQual\n14. Alley","594849b6":"Next up is 'OverallQual'.There dont seem to be any outliers as such.","58c0b1f5":"*Fence*\n\nThe description says:\n'Fence quality\n\t\t\n       GdPrv\tGood Privacy\n       MnPrv\tMinimum Privacy\n       GdWo\tGood Wood\n       MnWw\tMinimum Wood\/Wire\n       NA\tNo Fence'","d8bbeda1":"<h2> Handling Missing Values<\/h2>\nWe will first drop 'SalePrice' from the training dataset and then find out the missing fields after merging the train and test datasets.","bc562f25":"From above, we can gather that the following fields are most strongly correlated to 'SalePrice':\n* GrLivArea\n* GarageCars\n* YearBuilt\n* OverallQual\n* FullBath\n* TotalBsmtSF\n\nWe can ignore 'GarageArea' because it should be prportional to the number of cars the garage can hold - 'GarageCars'. Also, '1stFlrSF' can be ignored as it should in most cases be similar to the 'TotalBsmtSF'.","2f40693e":"Next we can check the skewness of the other numeric features.","caef8c12":"We can see that 'GrLivArea' and 'SalePrice' are linear. There are 4 outliers in the data - the ones with 'GrLivArea' >4000. As a next step we will remove them.","2f725e2c":"Next up is 'GarageCars'.There dont seem to be any outliers as such."}}