{"cell_type":{"dc0070a5":"code","896565c7":"code","b987e083":"code","47a00fd5":"code","5651ae1c":"code","b7c413c4":"code","0084960c":"code","b6f2a910":"code","66b1a1df":"code","dcfbdcd7":"code","40f0e489":"code","21236fea":"code","5fb86b73":"code","e277970b":"code","69ff36fb":"code","eb9af4d7":"code","9eda2d6a":"code","3ee51a30":"code","d162dc98":"markdown","35f340ff":"markdown","9875ef24":"markdown","61341005":"markdown","bfbe8f69":"markdown","64271a9d":"markdown","277dc1ed":"markdown"},"source":{"dc0070a5":"import pandas as pd\nimport numpy as np\nfrom itertools import product, combinations\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\nrand_state = 719","896565c7":"data_path = '\/kaggle\/input\/learn-together\/'\ndef reload(x):\n    return pd.read_csv(data_path + x, index_col = 'Id')\n\ntrain = reload('train.csv')\nn_train = len(train)\ntest = reload('test.csv')\nn_test = len(test)\n\nindex_test = test.index.copy()\ny_train = train.Cover_Type.copy()\nall_data = train.iloc[:,train.columns != 'Cover_Type'].append(test)\nall_data['train'] = [1]*n_train + [0]*n_test\n\ndel train\ndel test","b987e083":"numerical = ['Elevation', 'Horizontal_Distance_To_Hydrology',\n             'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n             'Horizontal_Distance_To_Fire_Points',\n             'Aspect', 'Slope', \n             'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\n\ncategorical = ['Soil_Type{}'.format(i) for i in range(1,41)] + ['Wilderness_Area{}'.format(i) for i in range(1,5)]","47a00fd5":"questionable_0 = ['Hillshade_9am', 'Hillshade_3pm']\n\ncorr_cols = {'Hillshade_9am': ['Hillshade_3pm', 'Aspect', 'Slope', 'Soil_Type10', 'Wilderness_Area1',\n                   'Wilderness_Area4', 'Vertical_Distance_To_Hydrology'],\n             'Hillshade_3pm': ['Hillshade_9am', 'Hillshade_Noon', 'Slope', 'Aspect']\n            }","5651ae1c":"rfr = RandomForestRegressor(n_estimators = 100, random_state = rand_state, verbose = 1, n_jobs = -1)\n\n# for col in questionable_0: \n#     print('='*20)\n#     scores = cross_val_score(rfr,\n#                              all_data_non0[corr_cols[col]], \n#                              all_data_non0[col],\n#                              n_jobs = -1)\n#     print(col + ': {0:.4} (+\/- {1:.4}) ## [{2}]'.format(scores.mean(), scores.std()*2, ', '.join(map(str, np.round(scores,4)))))\n\n# ====================\n# Hillshade_9am: 1.0 (+\/- 0.00056) ## [0.9995, 0.9993, 0.9988]\n# ====================\n# Hillshade_Noon: 1.0 (+\/- 0.00076) ## [0.9995, 0.9995, 0.9987] # Hillshade_Noon is removed\n# ====================\n# Hillshade_3pm: 1.0 (+\/- 0.0029) ## [0.9981, 0.9971, 0.9947]\n# ====================\n# Slope: 0.9985 (+\/- 0.001297) ## [0.9979, 0.9994, 0.9982]     # corr_cols['Slope'] with 12 highest corr columns has 0.85 accuracy\n\n## NEAR PERFECT SCORES FOR ALL => no need further feature engineering for questionable_0 predictions","b7c413c4":"for col in questionable_0:\n    print('='*20)\n    print(col)\n    all_data_0 = all_data[all_data[col] == 0].copy()\n    all_data_non0 = all_data[all_data[col] != 0].copy()\n    rfr.fit(all_data_non0[corr_cols[col]], all_data_non0[col])\n    pred = rfr.predict(all_data_0[corr_cols[col]])\n    pred_col = 'predicted_{}'.format(col)\n    \n    all_data[pred_col] = all_data[col].copy()\n    all_data.loc[all_data_0.index, pred_col] = pred\n\nfor col in questionable_0:\n    all_data['predicted_{}'.format(col)] = all_data['predicted_{}'.format(col)].apply(int)","0084960c":"def aspect_slope(df):\n    df['AspectSin'] = np.sin(np.radians(df.Aspect))\n    df['AspectCos'] = np.cos(np.radians(df.Aspect))\n    df['AspectSin_Slope'] = df.AspectSin * df.Slope\n    df['AspectCos_Slope'] = df.AspectCos * df.Slope\n    df['AspectSin_Slope_Abs'] = np.abs(df.AspectSin_Slope)\n    df['AspectCos_Slope_Abs'] = np.abs(df.AspectCos_Slope)\n    df['Hillshade_Mean'] = df[['Hillshade_9am',\n                              'Hillshade_Noon',\n                              'Hillshade_3pm']].apply(np.mean, axis = 1)\n    return df","b6f2a910":"def distances(df):\n    horizontal = ['Horizontal_Distance_To_Fire_Points', \n                  'Horizontal_Distance_To_Roadways',\n                  'Horizontal_Distance_To_Hydrology']\n    \n    df['Euclidean_to_Hydrology'] = np.sqrt(df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2)\n    df['EuclidHydro_Slope'] = df.Euclidean_to_Hydrology * df.Slope\n    df['Elevation_VDH_sum'] = df.Elevation + df.Vertical_Distance_To_Hydrology\n    df['Elevation_VDH_diff'] = df.Elevation - df.Vertical_Distance_To_Hydrology\n    df['Elevation_2'] = df.Elevation**2\n    df['Elevation_3'] = df.Elevation**3\n    df['Elevation_log1p'] = np.log1p(df.Elevation) # credit: https:\/\/www.kaggle.com\/evimarp\/top-6-roosevelt-national-forest-competition\/notebook\n    \n    for col1, col2 in combinations(zip(horizontal, ['HDFP', 'HDR', 'HDH']), 2):\n        df['{0}_{1}_diff'.format(col1[1], col2[1])] = df[col1[0]] - df[col2[0]]\n        df['{0}_{1}_sum'.format(col1[1], col2[1])] = df[col1[0]] + df[col2[0]]\n    \n    df['Horizontal_sum'] = df[horizontal].sum(axis = 1)\n    return df","66b1a1df":"def OHE_to_cat(df, colname, data_range): # data_range = [min_index, max_index+1]\n    df[colname] = sum([i * df[colname + '{}'.format(i)] for i in range(data_range[0], data_range[1])])\n    return df","dcfbdcd7":"soils = [\n    [7, 15, 8, 14, 16, 17,\n     19, 20, 21, 23], #unknow and complex \n    [3, 4, 5, 10, 11, 13],   # rubbly\n    [6, 12],    # stony\n    [2, 9, 18, 26],      # very stony\n    [1, 24, 25, 27, 28, 29, 30,\n     31, 32, 33, 34, 36, 37, 38, \n     39, 40, 22, 35], # extremely stony and bouldery\n]\nsoil_dict = {}\nfor index, soil_group in enumerate(soils):\n    for soil in soil_group:\n        soil_dict[soil] = index\n\ndef rocky(df):\n    df['Rocky'] = sum(i * df['Soil_Type' + str(i)] for i in range(1,41))\n    df['Rocky'] = df['Rocky'].map(soil_dict)\n    return df","40f0e489":"all_data = aspect_slope(all_data)\nall_data = distances(all_data)\nall_data = OHE_to_cat(all_data, 'Wilderness_Area', [1,5])\nall_data = OHE_to_cat(all_data, 'Soil_Type', [1,41])\nall_data = rocky(all_data)\nall_data.drop(['Soil_Type7', 'Soil_Type15', 'train'] + questionable_0, axis = 1, inplace = True)","21236fea":"X_train = all_data.iloc[:n_train,:].copy()\nX_test = all_data.iloc[n_train:, :].copy()\ndel all_data","5fb86b73":"# NO NEED TO CHOOSE IMPORTANT COLS AS ADABOOST USES TREE STUMP\n# clf_rf.fit(X_train, y_train)\n# predict = clf_rf.predict(X_test)\n\n# output = pd.DataFrame({'Id': test.index,\n#                       'Cover_Type': predict})\n# output.to_csv('Submission.csv', index=False)\n\n# importances = pd.DataFrame({'Features': X_train.columns, \n#                                 'Importances': clf_rf.feature_importances_})\n# fig = plt.figure(figsize=(10,16))\n# sns.barplot(x='Importances', y='Features', data=importances.sort_values(by=['Importances']\n#                                                                        , axis = 'index'\n#                                                                        , ascending = False)\n#            , orient=\"h\")\n# plt.xticks(rotation='vertical')\n# plt.show()\n\n# important_cols = importances[importances.Importances >= 0.003].Features","e277970b":"# base_est = [DecisionTreeClassifier(max_depth = i) for i in [1,3,5,7,10]]\n# params = {'base_estimator': base_est\n#           , 'n_estimators': np.logspace(1.5,3, 6).astype(int)\n#           , 'learning_rate': [0.01, 0.03, 0.1, 0.3, 0.6, 1]\n#          }\n# adac = AdaBoostClassifier()\n# grid = GridSearchCV(estimator = adac\n#                   , param_grid = params\n#                   , n_jobs = -1\n#                   , cv = 3\n#                   , scoring = 'accuracy'\n#                   , verbose = 2\n#                   )\n\n# grid.fit(X_train, y_train)\n\n# print('Best hyper-parameters found:')\n# print(grid.best_params_)\n# print('\\nFitting time:')\n# print(grid.refit_time_)\n# print('\\Best score:')\n# print(grid.best_score_)\n\n####### OUTPUT##########\n# # Best hyper-parameters found:\n# # {'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n# #                        max_features=None, max_leaf_nodes=None,\n# #                        min_impurity_decrease=0.0, min_impurity_split=None,\n# #                        min_samples_leaf=1, min_samples_split=2,\n# #                        min_weight_fraction_leaf=0.0, presort=False,\n# #                        random_state=None, splitter='best'), 'learning_rate': 1, 'n_estimators': 1000}\n\n# # Fitting time:\n# # 431.2910895347595\n# # \\Best score:\n# # 0.7846560846560846","69ff36fb":"base_est = [DecisionTreeClassifier(max_depth = i) for i in [10,13,16,19]]\nparams = {'base_estimator': base_est\n          , 'n_estimators': np.logspace(2,3.2, 5).astype(int) #array([ 100,  199,  398,  794, 1584])\n          , 'learning_rate': [0.01, 0.03, 0.1, 0.3, 1]\n         }\nadac = AdaBoostClassifier()\ngrid = GridSearchCV(estimator = adac\n                  , param_grid = params\n                  , n_jobs = -1\n                  , cv = 3\n                  , scoring = 'accuracy'\n                  , verbose = 2\n                  )\n\ngrid.fit(X_train, y_train)\n\nprint('Best hyper-parameters found:')\nprint(grid.best_params_)\nprint('\\nFitting time:')\nprint(grid.refit_time_)\nprint('\\Best score:')\nprint(grid.best_score_)\n\npredict = grid.predict(X_test)","eb9af4d7":"results = pd.DataFrame(grid.cv_results_)\nresults.sort_values(by=['rank_test_score'], inplace=True)\nresults.head(10)","9eda2d6a":"print(results.head(10))","3ee51a30":"# adac = AdaBoostClassifier(n_estimators = 163\n#                , learning_rate = 0.003\n#                )\n# adac.fit(X_train, y_train) \n# predict = adac.predict(X_test)\noutput = pd.DataFrame({'Id': index_test\n                       ,'Cover_Type': predict\n                      })\noutput.to_csv('Submission.csv', index=False)","d162dc98":"## Summary and Output","35f340ff":"### Distances & Elevation","9875ef24":"### Rockiness","61341005":"# Load Data","bfbe8f69":"# Other Features\n### Aspect, Slope & Shadow","64271a9d":"### Categorical","277dc1ed":"# Impute \"Fake\" 0s"}}