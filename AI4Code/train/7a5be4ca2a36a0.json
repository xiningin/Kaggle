{"cell_type":{"c7200ff3":"code","ec18a8ee":"code","737b404b":"code","a25c525f":"code","f691775b":"code","8a819e59":"code","1aea0394":"code","52fbf628":"code","35f6364b":"code","e49f048a":"code","78a2c07d":"markdown","4d12cff1":"markdown","229fac6b":"markdown","ca0b782b":"markdown","7939e7d6":"markdown","efa6e997":"markdown","b6b83a15":"markdown","4bc84654":"markdown"},"source":{"c7200ff3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec18a8ee":"import pandas as pd\nimport numpy as np\nfrom numpy.random import RandomState\nimport matplotlib.pyplot as plt\nimport pickle\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n#CLASSIFIERS FOR TRAINING\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression","737b404b":"data = pd.read_csv(\"..\/input\/fetal-health-classification\/fetal_health.csv\") \nprint(\"\u2666 LOOK AND CHECK DATA:\")\nprint(data.head())\nprint()\nprint(\"\u2666 DATASET LENGHT = \", len(data))","a25c525f":"cols = data.columns\nprint(cols)","f691775b":"mean = data.mean(axis=0)\nprint(mean)\n\nmedian = data.median(axis=0)\nprint(median)\n\nmode = data.mode(axis=0)\nprint(mode)\n\nstd = data.std(axis=0)\nprint(std)\n\nprint(data.corrwith(data[\"fetal_health\"]))","8a819e59":"#STEP-1\nscaler = MinMaxScaler(feature_range=(0, 1))\n#STEP-2\ntemp = data[\"fetal_health\"]\n#STEP-3\nnorm_data = scaler.fit_transform(data)\n#STEP-4\ndata = pd.DataFrame(data=norm_data, columns=cols)\n#STEP-5\ndata[\"fetal_health\"] = temp\n#STEP-6\nprint(data.head())","1aea0394":"\nrng = RandomState()\n\n#STEP-2\ntrain = data.sample(frac=0.7, random_state=rng)\nval = data.loc[~data.index.isin(train.index)]\n\n#STEP-3\ntrain.reset_index(drop=True, inplace=True)\nval.reset_index(drop=True, inplace=True)\n\n#STEP-4\nprint(\"\u2666 TRAIN SET:\")\nprint(train.head())\nprint()\nprint(\"\u2666 VALIDATION SET:\")\nprint(val.head())","52fbf628":"#STEP-1\nx_columns = cols[:-1]\ny_column = cols[-1]\n\n#STEP-2\nx_raw_train = train[x_columns]\ny_raw_train = train[y_column]\n\n#STEP-3\nX_train = x_raw_train.copy()\nY_train = y_raw_train.copy()\n\n#STEP-4\nprint(\"\u2666 X_TRAIN: \")\nprint(X_train.head())\nprint()\nprint(\"\u2666 Y_TRAIN: \")\nprint(Y_train.head())\nprint()\n\n#STEP-5\nx_raw_val = val[x_columns]\ny_raw_val = val[y_column]\n\n#STEP-6\nX_val = x_raw_val.copy()\nY_val = y_raw_val.copy()\n\n#STEP-7\nprint(\"\u2666 X_VAL: \")\nprint(X_val.head())\nprint()\nprint(\"\u2666 Y_VAL: \")\nprint(Y_val.head())","35f6364b":"#STEP-1\nall_classifers = [\n    KNeighborsClassifier(3),\n    SVC(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression()\n]\n\n#STEP-2\nall_acc = {}\n\n#STEP-3\nfor classifier in all_classifers:\n    #STEP-4\n    model = classifier\n    model.fit(X_train, Y_train)\n    #STEP-5\n    model_pred = model.predict(X_val)\n    model_acc = accuracy_score(Y_val, model_pred)\n    #STEP-6\n    classfier_name = classifier.__class__.__name__\n    #STEP-7\n    all_acc[classfier_name] = model_acc\n    #STEP-8\n    filename = classfier_name+'_model.pickle'\n    pickle.dump(model, open(filename, 'wb'))  \n    #STEP-9\n    loaded_model = pickle.load(open(filename, 'rb'))\n    result = loaded_model.score(X_val, Y_val)     \n    #STEP-10\n    print(\"\u2666 {:<30} = {:<12} {:>25} = {:>12}\".format(classfier_name, model_acc, 'loaded pickle model', result))","e49f048a":"#STEP-1\nall_acc = dict(sorted(all_acc.items(), key=lambda item: item[1], reverse=True))\n\n#STEP-2\nkeys = all_acc.keys()\nvalues = all_acc.values()\n\n#STEP-3\nplt.figure(figsize=(10,5))\nplt.title('ACCURCY OF CLASSIFIERS')\nplt.xlabel('classifiers')\nplt.ylabel('accuracy')\nplt.bar(keys, values, color=\"g\")\n\n#STEP-4\nplt.xticks(rotation=90)\n\n#STEP-5\nplt.grid()\nplt.show()","78a2c07d":"We are going to plot and visualize the output from the selected classifiers to determine which is the most appropriate","4d12cff1":"CONCLUSION\n\nFrom our plot, we can say that the GradientBoostingClassifier is the most appropriate classifier for this dataset.\n\nThank you for following through","229fac6b":"Next, we are going to use classifiers from the sklearn library on our dataset\n\nThen we will train, validate, save and load models","ca0b782b":"SPLIT DATASET INTO TRAIN AND TEST DATA\n\nSo, here i will divide the dataset into two parts (for model training and validation); 70%:30% respectively","7939e7d6":"There are 22 columns here (21 columns are our input data and the last one column will be used as prediction column). Also, it has 2126 rows, it is 2126 measurements extracted from cardiotocograms and classified by expert obstetricians into 3 categories:\n\n* Normal\n* Suspect\n* Pathological","efa6e997":"RENAMING VARIABLES\n\nI will create the X and Y variables for input and target values respectively for better comprehension","b6b83a15":"NORMALIZE DATA\n\nIt is necessary to normalize data before making some classification tasks. Here i will normalize it in a range from 0 to 1.","4bc84654":"ANALYZE DATA\n\nOne of the most important things is to understand data which you work with. Here we will use some well-known methods for easier understanding of our data.\n\nFor all dataset columns we will find some statistical information, like: Mean, Median, Mode, Standard Deviation and Correlation using Pandas functions."}}