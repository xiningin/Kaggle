{"cell_type":{"672683be":"code","6c2eae84":"code","3f275442":"code","18b62560":"code","aa88fdb4":"code","69b9cabe":"code","facf6c0d":"code","aacb4890":"code","c47c2523":"code","20f3401c":"code","2deaa371":"code","55c4f407":"code","7b046edc":"code","62761671":"code","db4842a0":"code","9d3532ba":"code","17a904dc":"code","720acd4e":"code","c367b285":"code","177782f7":"code","9cb339f5":"code","3d145209":"code","f1f2b6e5":"code","b0b63dfa":"markdown","6a37e673":"markdown","b0079abe":"markdown","9a05db54":"markdown","e831da81":"markdown","be221da5":"markdown"},"source":{"672683be":"# installing requirements\n!pip install transformers","6c2eae84":"# so the imports\nfrom tqdm.auto import tqdm\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch import optim\nfrom torch.utils import data as torchdata\nfrom sklearn import metrics as ms\nfrom sklearn import model_selection as md\nfrom matplotlib import pyplot as plt\n\nimport torch\nimport random\nimport os\n\nimport argparse as ap\nimport pandas as pd\nimport numpy as np\nimport transformers as tr","3f275442":"def seed_everything(seed = 42):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nseed_everything()","18b62560":"import re\nimport emoji\nimport unicodedata\n\ndef process_tweet(s, save_text_formatting=False, keep_emoji=False, keep_usernames=False):\n        # NOTE: will sometimes need to use Windows encoding here, depending on how CSV is generated.\n        # All results saved in UTF-8\n        # TODO: Try to get input data in UTF-8 and don't let it touch windows (Excel). That loses emoji, among other things\n\n        # Clean up the text before tokenizing.\n        # Why is this necessary?\n        # Unsupervised training (and tokenization) is usually on clean, unformatted text.\n        # Supervised training\/classification may be on tweets -- with non-ASCII, hashtags, emoji, URLs.\n        # Not obvious what to do. Two options:\n        # A. Rewrite formatting to something in ASCII, then finetune.\n        # B. Remove all formatting, keep only the text.\n        \n            \n        EMOJI_DESCRIPTION_SCRUB = re.compile(r':(\\S+?):')\n        HASHTAG_BEFORE = re.compile(r'#(\\S+)')\n        BAD_HASHTAG_LOGIC = re.compile(r'(\\S+)!!')\n        FIND_MENTIONS = re.compile(r'@(\\S+)')\n        LEADING_NAMES = re.compile(r'^\\s*((?:@\\S+\\s*)+)')\n        TAIL_NAMES = re.compile(r'\\s*((?:@\\S+\\s*)+)$')\n        \n        def remove_accents(input_str):\n            nfkd_form = unicodedata.normalize('NFKD', input_str)\n            return u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n        \n        if save_text_formatting:\n            s = re.sub(r'https\\S+', r'xxxx', str(s))\n        else:\n            s = re.sub(r'https\\S+', r' ', str(s))\n            s = re.sub(r'x{3,5}', r' ', str(s))\n\n        # Try to rewrite all non-ASCII if known printable equivalent\n        s = re.sub(r'\\\\n', ' ', s)\n        s = re.sub(r'\\s', ' ', s)\n        s = re.sub(r'<br>', ' ', s)\n        s = re.sub(r'&amp;', '&', s)\n        s = re.sub(r'&#039;', \"'\", s)\n        s = re.sub(r'&gt;', '>', s)\n        s = re.sub(r'&lt;', '<', s)\n        s = re.sub(r'\\'', \"'\", s)\n\n        # Rewrite emoji as words? Need to import a function for that.\n        # If no formatting, just get the raw words -- else save formatting so model can \"learn\" emoji\n        # TODO: Debug to show differences?\n        if save_text_formatting:\n            s = emoji.demojize(s)\n        elif keep_emoji:\n            s = emoji.demojize(s)\n            # Transliterating directly is ineffective w\/o emoji training. Try to shorten & fix\n            s = s.replace('face_with', '')\n            s = s.replace('face_', '')\n            s = s.replace('_face', '')\n            # remove emjoi formatting (: and _)\n            # TODO: A\/B test -- better to put emoji in parens, or just print to screen?\n            #s = re.sub(EMOJI_DESCRIPTION_SCRUB, r' (\\1) ', s)\n            s = re.sub(EMOJI_DESCRIPTION_SCRUB, r' \\1 ', s)\n            # TODO -- better to replace '_' within the emoji only...\n            s = s.replace('(_', '(')\n            s = s.replace('_', ' ')\n\n        # Remove all non-printable and non-ASCII characters, including unparsed emoji\n        s = re.sub(r\"\\\\x[0-9a-z]{2,3,4}\", \"\", s)\n        # NOTE: We can't use \"remove accents\" as long as foreign text and emoji gets parsed as characters. Better to delete it.\n        # Replace accents with non-accented English letter, if possible.\n        # WARNING: Will try to parse corrupted text... (as aAAAa_A)\n        s = remove_accents(s)\n        # Rewrite or remove hashtag symbols -- important text, but not included in ASCII unsupervised set\n        if save_text_formatting:\n            s = re.sub(HASHTAG_BEFORE, r'\\1!!', s)\n        else:\n            s = re.sub(HASHTAG_BEFORE, r'\\1', s)\n            # bad logic in case ^^ done already\n            s = re.sub(BAD_HASHTAG_LOGIC, r'\\1', s)\n        # Keep user names -- or delete them if not saving formatting.\n        # NOTE: This is not an obvious choice -- we could also treat mentions vs replies differently. Or we could sub xxx for user name\n        # The question is, does name in the @mention matter for category prediction? For emotion, it should not, most likely.\n        if save_text_formatting:\n            # TODO -- should we keep but anonymize mentions? Same as we rewrite URLs.\n            pass\n        else:\n            # If removing formatting, either remove all mentions, or just the @ sign.\n            if keep_usernames:\n                # quick cleanup extra spaces\n                s = ' '.join(s.split())\n\n                # If keep usernames, *still* remove leading and trailing names in @ mentions (or tail mentions)\n                # Why? These are not part of the text -- and should not change sentiment\n                s = re.sub(LEADING_NAMES, r' ', s)\n                s = re.sub(TAIL_NAMES, r' ', s)\n\n                # Keep remaining mentions, as in \"what I like about @nvidia drivers\"\n                s = re.sub(FIND_MENTIONS, r'\\1', s)\n            else:\n                s = re.sub(FIND_MENTIONS, r' ', s)\n        #s = re.sub(re.compile(r'@(\\S+)'), r'@', s)\n        # Just in case -- remove any non-ASCII and unprintable characters, apart from whitespace\n        s = \"\".join(x for x in s if (x.isspace() or (31 < ord(x) < 127)))\n        # Final cleanup -- remove extra spaces created by rewrite.\n        s = ' '.join(s.split())\n        return s","aa88fdb4":"cfg = ap.Namespace()\ncfg.train_file = \"..\/input\/nlp-getting-started\/train.csv\"\ncfg.test_file = \"..\/input\/nlp-getting-started\/test.csv\"\ncfg.x_column = \"text\"\ncfg.y_column = \"target\"\n\ncfg.device = \"cuda:0\"\n\ncfg.model_name = 'roberta-base'\ncfg.num_classes = 1\ncfg.max_length = 128\ncfg.batch_size = 32\ncfg.lr = 2e-5\ncfg.weight_decay = 5e-5\ncfg.n_epochs = 3\ncfg.warmup_ratio = 0.1\ncfg.max_grad_norm = 1.0\n\ndevice = torch.device(cfg.device)","69b9cabe":"class DataframeDataset(torchdata.Dataset):\n    def __init__(self, dataframe: pd.DataFrame, x_column: str, y_column: str, tokenizer: tr.PreTrainedTokenizer, max_length: int):\n        super(DataframeDataset, self).__init__()\n        self.X = dataframe.loc[:, [x_column]].values\n        if y_column is not None:\n            self.Y = dataframe.loc[:, [y_column]].values\n        else:\n            self.Y = None\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        x = process_tweet(self.X[idx])\n\n        x = self.tokenizer.encode_plus(x)['input_ids']\n        x_tensor = np.full((self.max_length), fill_value=self.tokenizer.pad_token_id) # i prefer padding all the texts to the same length\n        attn_mask = np.zeros_like(x_tensor) # we'd like not to attend to padding zeros, so we need attention mask which tells where the padding and where the tokens\n        for i, tok in enumerate(x):\n            x_tensor[i] = tok\n            attn_mask[i] = 1\n            \n        if self.Y is None:\n            return x_tensor, attn_mask\n        else:\n            y = self.Y[idx]\n            return x_tensor, attn_mask, y.astype(np.float32)","facf6c0d":"tokenizer = tr.RobertaTokenizer.from_pretrained(cfg.model_name)\nmodel = tr.RobertaForSequenceClassification.from_pretrained(\n    cfg.model_name,\n    num_labels=cfg.num_classes,\n    #hidden_act='gelu_new' ## fixing for consistency with google implementation\n)","aacb4890":"train_df = pd.read_csv(cfg.train_file)\ntrain, valid = md.train_test_split(train_df, test_size=0.2, random_state=42)\n\ntest = pd.read_csv(cfg.test_file)","c47c2523":"train_dataset = DataframeDataset(train, cfg.x_column, cfg.y_column, tokenizer, cfg.max_length)\nvalid_dataset = DataframeDataset(valid, cfg.x_column, cfg.y_column, tokenizer, cfg.max_length)\ntest_dataset  = DataframeDataset(test,  cfg.x_column, None,         tokenizer, cfg.max_length)","20f3401c":"train_dl = torchdata.DataLoader(train_dataset, batch_size=cfg.batch_size, num_workers=8, sampler=torchdata.RandomSampler(train_dataset))\nvalid_dl = torchdata.DataLoader(valid_dataset, batch_size=cfg.batch_size, num_workers=8, sampler=torchdata.SequentialSampler(valid_dataset))","2deaa371":"total_steps = len(train_dl) * cfg.n_epochs\nn_warmup_steps = int(total_steps * cfg.warmup_ratio) # it is required to use learning rate warmup","55c4f407":"def model_step(model, x, attn_mask, y, compute_metrics=False, return_true_pred=False):\n    x = x.to(device)\n    attn_mask = attn_mask.to(device)\n    y = y.to(device)\n    \n    logits, = model(x, attention_mask=attn_mask) # for some reason transformers models are returning one-element tuples\n    loss = F.binary_cross_entropy_with_logits(logits, y)\n    result_dict = {\n        'loss': loss\n    }\n    if compute_metrics:\n        y_pred = torch.sigmoid(logits).detach().cpu().numpy() > 0.5 # here and below we \n        y_true = y.detach().cpu().numpy()\n        f1 = ms.f1_score(y_true, y_pred)\n        accuracy = ms.accuracy_score(y_true, y_pred)\n        result_dict['f1'] = f1\n        result_dict['accuracy'] = accuracy\n        \n    if return_true_pred:\n        y_pred = np.squeeze(torch.sigmoid(logits).detach().cpu().numpy() > 0.5)\n        y_true = np.squeeze(y.detach().cpu().numpy())\n        result_dict['data'] = (y_true, y_pred)\n        \n    return result_dict","7b046edc":"def evaluate(model, test_dataset):\n    test_dl = torchdata.DataLoader(test_dataset, batch_size=1, num_workers=1, sampler=torchdata.SequentialSampler(test_dataset))\n    predicts = []\n    model.eval()\n    with torch.no_grad():\n        for x, attn in tqdm(test_dl):\n            x = x.to(device)\n            attn = attn.to(device)\n            logits, = model(x, attention_mask=attn)\n            pred = torch.sigmoid(logits) > 0.5\n            pred = pred.cpu().squeeze().item()\n            predicts.append(pred)\n            \n    return np.array(predicts)","62761671":"model = model.to(device)\noptimizer = tr.optimization.AdamW(params=model.parameters(), lr=cfg.lr)\nscheduler = tr.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=n_warmup_steps, num_training_steps=total_steps)","db4842a0":"global_train_loss = []\nglobal_val_acc    = []\nglobal_val_f1     = []\nglobal_val_loss   = []\nfor i in range(cfg.n_epochs):\n    model.train()\n    train_loss = []\n    for x, attn, y in tqdm(train_dl, unit='batch', desc=f\"Train {i}\"):\n        result = model_step(model, x, attn, y)\n        loss = result['loss']\n        loss.backward()\n            \n        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n        optimizer.step()\n        scheduler.step()\n        train_loss.append(loss.item())\n        optimizer.zero_grad()\n            \n    global_train_loss += train_loss\n    \n    model.eval()\n    valid_loss  = []\n    valid_f1    = []\n    valid_acc   = []\n    with torch.no_grad():\n        for x, attn, y in tqdm(valid_dl, unit='batch', desc=f\"Valid {i}\"):\n            result = model_step(model, x, attn, y, compute_metrics=True)\n            valid_loss.append(result['loss'].item())\n            valid_f1.append(result['f1'])\n            valid_acc.append(result['accuracy'])\n            \n    global_val_acc  += valid_acc\n    global_val_f1   += valid_f1\n    global_val_loss += valid_loss\n\n    print(f\"Epoch {i}: Train Loss = {np.mean(train_loss):.6f}, Val Loss = {np.mean(valid_loss):.6f}, F1 = {np.mean(valid_f1):.4f}, Accuracy = {np.mean(valid_acc):.4f}\")","9d3532ba":"plt.plot(global_train_loss)","17a904dc":"plt.plot(global_val_f1)","720acd4e":"plt.plot(global_val_loss)","c367b285":"true, pred = [], []\nfor x, attn, y in tqdm(valid_dl):\n    with torch.no_grad():\n        y_true, y_pred = model_step(model, x, attn, y, return_true_pred=True)['data']\n        true += list(y_true)\n        pred += list(y_pred)\n        \ntrue = np.vstack(true)\npred = np.vstack(pred)\nf1  = ms.f1_score(y_true=true, y_pred=pred)\nacc = ms.accuracy_score(y_true=true, y_pred=pred)","177782f7":"print(f\"F1 - {f1:.4f}\")\nprint(f\"Acc - {acc:.4f}\")","9cb339f5":"predicts = evaluate(model, test_dataset)","3d145209":"submission = pd.DataFrame({\n    'id': test['id'],\n    'target': predicts.astype(np.int32)\n})","f1f2b6e5":"submission.to_csv(\".\/submission.csv\", index=False, encoding='utf-8')","b0b63dfa":"As the data in this competition consist of tweets, we need extensive preprocessing for them. This code snippet is from [nvidia\/sentiment-discovery](https:\/\/github.com\/NVIDIA\/sentiment-discovery) and it just makes everything better.","6a37e673":"### Let's train","b0079abe":"### Reproduceability First\nand possibly random seed optimization))","9a05db54":"### Loading model and tokenizer","e831da81":"### Reading the data and performing train-dev split","be221da5":"# RoBERTa-based approach, a comprehensive example\n In this notebook i'll try to present some of my approaches to near-SOTA text classification via fine-tuning RoBERTa model.\nAlso, it may be considered as an example of using huggingface\/transformers library."}}