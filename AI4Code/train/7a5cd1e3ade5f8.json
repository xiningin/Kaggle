{"cell_type":{"6c4f4ee4":"code","ac5d8bfe":"code","785dce64":"code","1cba8063":"code","ab4e17e0":"code","a0b1343b":"code","a64bd1c6":"code","d2b319cf":"code","29448dd5":"code","91ecd77f":"code","f00ab05b":"code","89d97a68":"code","0a30660a":"code","4941f45f":"code","eb37b183":"code","03caafc0":"code","5aa082cd":"code","223b3ad2":"code","46fd1d86":"code","2adb6a1d":"code","92460b34":"markdown","ee442477":"markdown","726f602e":"markdown","996b6a84":"markdown","5d0291a0":"markdown","600243eb":"markdown","c6b7aa55":"markdown","40e590c2":"markdown","4fd1ddf0":"markdown","544f9bb9":"markdown","95518773":"markdown","0a04482b":"markdown","69fbf667":"markdown","03d6d96a":"markdown","b01c27ad":"markdown","d5a4b663":"markdown","07f294c0":"markdown","e219bc6c":"markdown","53ce1bed":"markdown","0d04d572":"markdown","385204d7":"markdown"},"source":{"6c4f4ee4":"# import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ac5d8bfe":"# Firstly, import data\ndata = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")","785dce64":"# First look to data\ndata.head()","1cba8063":"data.info()","ab4e17e0":"# Adjust x and y for making normalization\ny = data.sex.values # make it np array\nx_data = data.drop([\"sex\"], axis = 1) # everything, exludes sex, is on the x\n\n# Normalization \nx = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data)).values ","a0b1343b":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state = 42)\nx_train.shape","a64bd1c6":"x_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","d2b319cf":"best_accuracy = [] # in order to compare all results, I will create an empty list that will be filled after all tests.\n\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))\n\nbest_accuracy.append(lr.score(x_test.T,y_test.T))","29448dd5":"x_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","91ecd77f":"from sklearn.neighbors import KNeighborsClassifier\nscore_list = [] # to keep all scores for plot\nbest_score = 0\nbest_k = 0\nfor each in range(1,15): # I will try for 15 values of k\n    knn = KNeighborsClassifier(n_neighbors = each)\n    knn.fit(x_train,y_train) # train the model\n    score_list.append(knn.score(x_test,y_test))\n    if (knn.score(x_test,y_test) > best_score): # if you find a value that bigger than before, keep it!\n       best_score = knn.score(x_test,y_test)\n       best_k = each\n    \nplt.plot(range(1,15), score_list) # x_axis=range(1,15), y_axis=score_list\nplt.xlabel(\"k values\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n\nprint(\"The best accuracy we got is \", best_score)\nprint(\"Best accuracy's k value is \", best_k)\n\nbest_accuracy.append(best_score)\n","f00ab05b":"from sklearn.svm import SVC\nsvm = SVC(random_state = 42)\nsvm.fit(x_train,y_train)\nprint(\"Accuracy of SVM: \",svm.score(x_test,y_test))\n\nbest_accuracy.append(svm.score(x_test,y_test))","89d97a68":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\nprint(\"Accuracy of NB: \", nb.score(x_test,y_test))\n\nbest_accuracy.append(nb.score(x_test,y_test))","0a30660a":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nprint(\"Accuracy of Decision Tree: \", dt.score(x_test,y_test))\n\nbest_accuracy.append(dt.score(x_test,y_test))","4941f45f":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100,random_state=1) \n#n_estimator = we determined of how many trees will be on our forest\nrf.fit(x_train,y_train)\nprint(\"Accuracy of Random Forest: \", rf.score(x_test,y_test)) # It gives us a bit better results than decision tree.\n\nbest_accuracy.append(rf.score(x_test,y_test))\n","eb37b183":"best_accuracy","03caafc0":"# Bar Plot with Seaborn\nsv_ml = [\"Logistic Regression\", \"KNN\", \"SVM\",\"Naive Bayes\", \"Decision Tree\", \"Random Forest\"]\n\nplt.figure(figsize=(15,10))\nsns.barplot(x = sv_ml, y = best_accuracy)\nplt.xticks(rotation= 30)\nplt.xlabel('Accuracy')\nplt.ylabel('Supervised Learning Types')\nplt.title('Supervised Learning Types v Accuracy')\nplt.show()","5aa082cd":"# Pie Chart with Seaborn\ncolors = ['red','green','blue','cyan','purple','yellow']\nlabels = sv_ml\nexplode = [0,0,0,0,0,0]\nsizes = best_accuracy\n\n# visual\nplt.figure(figsize = (7,7))\nplt.pie(sizes, labels=labels, explode=explode, colors=colors, autopct='%1.1f%%')\nplt.title('Comparison of Accuracies',color = 'brown',fontsize = 15)\nplt.show()","223b3ad2":"# I think, Bar Plot with Plotly will be much useful than Seaborn's Bar Plot, because values are too close to read with Seaborn.\nimport plotly.graph_objs as go\nimport plotly.io as pio\n# Create trace\ntrace1 = go.Bar(\n                x = sv_ml,\n                y = best_accuracy,\n                name = \"Accuracy Plot\",\n                marker = dict(color = 'rgba(10, 100, 255, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5))\n)\ndata = [trace1]\nlayout = go.Layout(barmode = \"group\")\nfig = go.Figure(data = data, layout = layout)\npio.show(fig)","46fd1d86":"# go has been defined above\ntrace1 = go.Scatter(\n                    x = sv_ml,\n                    y = best_accuracy,\n                    mode = \"lines\",\n                    name = \"Accuracy\",\n                    marker = dict(color = 'rgba(255, 0, 0, 0.7)')\n)\n# trace2 for finding the top points easily with attention getting colur\ntrace2 =go.Scatter( \n                    x = sv_ml,\n                    y = best_accuracy,\n                    mode = \"markers\",\n                    name = \"Highlight Point\",\n                    marker = dict(color = 'rgba(0, 255, 155, 1)')\n)\n\ndata = [trace1,trace2]\nlayout = dict(title = 'Accuracies of Supervised Learning Types',\n              xaxis= dict(title= 'Accuracy',ticklen= 5,zeroline= False),\n              yaxis= dict(title= 'Types',ticklen= 5,zeroline= False)\n             )\nfig = dict(data = data, layout = layout)\npio.show(fig)","2adb6a1d":"# It's just a bonus, not for an analysis, just because it is one of the my favorite plots :)\n# Word Cloud\nfrom wordcloud import WordCloud\n\nsv_ml2 = [\"Logistic Regression\", \"KNN\", \"SVM\",\"Naive Bayes\", \"Decision Tree\", \"Random Forest\"]\nlist_label = [\"Type\",\"Accuracy\"]\nlist_col = [sv_ml2,best_accuracy]\nzipped = list(zip(list_label,list_col))\ndata_dict = dict(zipped) \ndf = pd.DataFrame(data_dict)\n\ncloud = df.Type\n\nplt.subplots(figsize=(10,10))\nwordcloud = WordCloud(\n                          background_color='white',\n                          width=512,\n                          height=384\n                         ).generate(\" \".join(cloud))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.savefig('graph.png')\n\nplt.show()\n","92460b34":"<a id=\"7\"><\/a> <br>\n# Comparison Methods","ee442477":"Random Forest is like decision tree. You can think that we are creating a forest using trees.","726f602e":"<a id=\"3\"><\/a> <br>\n# Support Vector Machine(SVM)","996b6a84":"Our learning tests have been done. As you can see, I collected all the accuracy result's on a list. Firstly, let's check the list and then, we will decide which supervised learning type gave the best result.","5d0291a0":"It did not satisfy me, but it is early to say something. Let's continue with our another learning method: ","600243eb":"<a id=\"4\"><\/a> <br>\n# Naive Bayes Classification","c6b7aa55":"The best way for comparation is to visualize this value, obviously. Let's make some plots before make a decision.","40e590c2":"<a id=\"2\"><\/a> <br>\n# K-Nearest Neighbour(KNN)","4fd1ddf0":"I will use **sklearn** for implementation. As a conclusion, I will decide which way is the best for Supervised Learning.","544f9bb9":"***Train Test Split***\n\nNow, I will make a split the data for decision of how much of them will be train and how much of them will be test. \nI will split 20% of the data for test, 80% of the data for train.","95518773":"<a id=\"5\"><\/a> <br>\n# Decision Tree Classification","0a04482b":"<a id=\"6\"><\/a> <br>\n# Random Forest","69fbf667":"<a id=\"1\"><\/a> <br>\n# Logistic Regression\n\nBefore I countinue, I have to make a trick for using my values. As it can be seen above, shape of x_train (242, 13), but this is wrong for linear regression. Our data has to be array[number of the feature, number of the sample], so I need to make transposition them in order to be able to use.","03d6d96a":"# INTRODUCTION\n   Humanity has to struggle with diseasters since humanity exists. Eventhough some disaster has been popular from time to time, crucial illnesses like heart diseases became much more popular in our days. \nIn this kernel, what I will make is to analyze a data about how genders are effected from 'Heart Disease'. \nAlso, I will use Supervised Machine Learning techniques to predict datas in order to detect whether the gender is male, or female according to data.\n\nContent:\n1. [Logistic Regression](#1)\n1. [K-Nearest Neighbour(KNN)](#2)\n1. [Support Vector Machine(SVM)](#3)\n1. [Naive Bayes Classification](#4)\n1. [Decision Tree Classification](#5)\n1. [Random Forest Classification](#6)\n1. [Comparison using Plots](#7)\n1. [Conclusion](#8)","b01c27ad":"Transposition will be wrong for KNN, so make their shapes' how they were before.","d5a4b663":"Actually, if the data includes sexes as string(means as Male, Female), we had to make them int. The data gave us 1 for males, 0 for females, it can be seen on above, as well, so, there is no need to change something and we can keep doing our analysis.","07f294c0":"I like interactive plots, I want to 2 more plot with Plotly, as addition to Seaborn.","e219bc6c":"I will use Seaborn and Plotly libraries. I tried to choose the best plots in order to see the differences easily.","53ce1bed":"1 for males, 0 for females according to data. ","0d04d572":"<a id=\"8\"><\/a> <br>\n# Conclusion\nAfter making bunch of visualization, it can be said KNN Algorithm is the best way to prediction for our data. Eventhough the results are a bit less than what I expected, KNN has became much more better than others. \n\nYou can see that **sklearn** provides us too much convenience, and I tried to show their implementatin as much as I can. Please vote and comment if you enjoyed my kernel, thanks for reading. ","385204d7":"***Normalization***\n\nWe made normalization process in order to observe all datas clearly, which means we may encounter with some problems when we see the data on charts because we have discrete values such as 233, 2.5, 130 etc.\nHow normalization helps us to see datas better is based on that normalization process takes the all values between 0-1, so we can see the vales easily."}}