{"cell_type":{"777b0a30":"code","de4633fd":"code","87cd255b":"code","6f8cdf4a":"code","112e4a5c":"code","11514fd8":"code","71484332":"code","59c1e79f":"markdown","e9fd864d":"markdown","1b552658":"markdown","f0ddf009":"markdown","9c9c3856":"markdown"},"source":{"777b0a30":"import matplotlib.pyplot as plt\r\nimport torch\r\nimport torch.nn as nn\r\nimport torchvision.transforms as transforms\r\nimport torchvision.datasets as datasets\r\nfrom torchvision.utils import make_grid\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport IPython.display as display\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom tqdm import tqdm\r\nimport copy","de4633fd":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n\r\n# hyperparameter settings #######################\r\n# latent = number of latent dimensions\r\n# nE = number of epochs\r\n# bs = batch size\r\n# cap = capacity of network\r\n# lr = learning rate of optimizer\r\n# wd = weight decay\r\n# vb = variational beta\r\nruns = [\r\n    # {\"latent\":2, \"nE\":100, \"bs\":128, \"cap\":64, \"lr\":1e-3, \"wd\":1e-5, \"vb\":1} # batch 1\r\n    # ,{\"latent\":6, \"nE\":100, \"bs\":128, \"cap\":64, \"lr\":1e-3, \"wd\":1e-5, \"vb\":1} # batch 1\r\n    # ,{\"latent\":10, \"nE\":100, \"bs\":128, \"cap\":64, \"lr\":1e-3, \"wd\":1e-5, \"vb\":1} # batch 1\r\n    # ,{\"latent\":20, \"nE\":100, \"bs\":128, \"cap\":64, \"lr\":1e-3, \"wd\":1e-5, \"vb\":1} # batch 1\r\n    {\"latent\":20, \"nE\":100, \"bs\":64, \"cap\":64, \"lr\":1e-3, \"wd\":1e-5, \"vb\":1}\r\n    ,{\"latent\":20, \"nE\":200, \"bs\":64, \"cap\":64, \"lr\":1e-3, \"wd\":1e-5, \"vb\":1}\r\n    ,{\"latent\":20, \"nE\":100, \"bs\":64, \"cap\":64, \"lr\":1e-3, \"wd\":1e-5, \"vb\":0.5}\r\n    ,{\"latent\":20, \"nE\":200, \"bs\":64, \"cap\":64, \"lr\":1e-3, \"wd\":1e-5, \"vb\":0.5}\r\n        ]\r\n\r\ntrain_dir='..\/input\/covid19-chest-ct-image-augmentation-gan-dataset\/COVID-19\/COVID-19\/train'\r\n\r\ntrain_dataset = datasets.ImageFolder(\r\n    train_dir,\r\n    transforms.Compose([transforms.Resize( (256,256)) ,\r\n        transforms.ToTensor(),\r\n    ]))","87cd255b":"\r\ndef show_image(image_tensor, num_images=25, size=(1, 28, 28)):\r\n    '''\r\n    Function for visualizing images: Given a tensor of images, number of images, and\r\n    size per image, plots and prints the images in an uniform grid.\r\n    '''\r\n    image_tensor = (image_tensor + 1) \/ 2\r\n    image_unflat = image_tensor.detach().cpu()\r\n    image_grid = make_grid(image_unflat[:num_images], nrow=5)\r\n    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\r\n    plt.show()\r\n\r\ndef plot_loss(training_loss, settings):\r\n    fig, ax = plt.subplots()\r\n    ax.plot(range(len(training_loss)), training_loss)\r\n\r\n    ax.set(xlabel='epochs', ylabel='BCE loss')\r\n    ax.set_title(f'Loss for {settings}', y=1.1)\r\n    ax.grid()\r\n\r\n    fig.savefig(\"test.png\")\r\n    plt.show()","6f8cdf4a":"class Encoder(nn.Module):\r\n    '''encoder for VAE, goes from image conv net to linear latent layer'''\r\n    def __init__(self, capacity, latent_dims):\r\n        super(Encoder, self).__init__()\r\n        c = capacity\r\n        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = c, kernel_size = 4, stride = 2, padding = 1)\r\n        self.conv2 = nn.Conv2d(in_channels = c, out_channels = c*2, kernel_size = 4, stride = 2, padding = 1)\r\n        self.conv3 = nn.Conv2d(in_channels = c*2, out_channels = c*2*2, kernel_size = 4, stride = 2, padding = 1)\r\n        self.conv4 = nn.Conv2d(in_channels = c*2*2, out_channels = c*2*2*2, kernel_size = 4, stride = 2, padding = 1)\r\n        self.conv5 = nn.Conv2d(in_channels = c*2*2*2, out_channels = c*2*2*2*2, kernel_size = 4, stride = 2, padding = 1)\r\n        self.conv6 = nn.Conv2d(in_channels = c*2*2*2*2, out_channels = c*2*2*2*2*2, kernel_size = 4, stride = 2, padding = 1)\r\n        self.fc_mu = nn.Linear(in_features = 32768, out_features = latent_dims)\r\n        self.fc_logvar = nn.Linear(in_features = 32768, out_features = latent_dims)\r\n\r\n    def forward(self, x):\r\n        x = F.relu(self.conv1(x))\r\n        x = F.relu(self.conv2(x))\r\n        x = F.relu(self.conv3(x))\r\n        x = F.relu(self.conv4(x))\r\n        x = F.relu(self.conv5(x))\r\n        x = F.relu(self.conv6(x))\r\n        x = x.view(x.size(0), -1) # flatten feature maps to feature vectors for linear layers\r\n        x_mu = self.fc_mu(x)\r\n        x_logvar = self.fc_logvar(x)\r\n        return x_mu, x_logvar\r\n\r\nclass Decoder(nn.Module):\r\n    '''decoder for VAE, goes from linear latent layer to deconv layers to reconstruct image'''\r\n    def __init__(self, capacity, latent_dims):\r\n        super(Decoder, self).__init__()\r\n        c = capacity\r\n        self.fc = nn.Linear(in_features = latent_dims, out_features = 32768)\r\n        self.conv1 = nn.ConvTranspose2d(in_channels = c, out_channels = 3, kernel_size=4, stride=2, padding=1)\r\n        self.conv2 = nn.ConvTranspose2d(out_channels = c, in_channels = c*2, kernel_size = 4, stride = 2, padding = 1)\r\n        self.conv3 = nn.ConvTranspose2d(out_channels = c*2, in_channels = c*2*2, kernel_size = 4, stride = 2, padding = 1)\r\n        self.conv4 = nn.ConvTranspose2d(out_channels = c*2*2, in_channels = c*2*2*2, kernel_size = 4, stride = 2, padding = 1)\r\n        self.conv5 = nn.ConvTranspose2d(out_channels = c*2*2*2, in_channels = c*2*2*2*2, kernel_size = 4, stride = 2, padding = 1)\r\n        self.conv6 = nn.ConvTranspose2d(out_channels = c*2*2*2*2, in_channels = c*2*2*2*2*2, kernel_size = 4, stride = 2, padding = 1)\r\n\r\n    def forward(self, x):\r\n        x = self.fc(x)\r\n        x = x.view(x.size(0), 2048, 4, 4) # unflatten feature vectors to feature maps for conv layers\r\n        x = F.relu(self.conv6(x))\r\n        x = F.relu(self.conv5(x))\r\n        x = F.relu(self.conv4(x))\r\n        x = F.relu(self.conv3(x))\r\n        x = F.relu(self.conv2(x))\r\n        x = torch.sigmoid(self.conv1(x)) # using BCE (Binary Crossentropy) as reconstruction loss, so output is sigmoid\r\n        return x\r\n\r\nclass VAE(nn.Module):\r\n    '''VAE architecture for encoder -> sample from latent -> decode latent sample'''\r\n    def __init__(self, capacity, latent_dims):\r\n        super(VAE, self).__init__()\r\n        self.encoder = Encoder(capacity, latent_dims)\r\n        self.decoder = Decoder(capacity, latent_dims)\r\n\r\n    def forward(self, x):\r\n        latent_mu, latent_logvar = self.encoder(x)\r\n        latent = self.latent_sample(latent_mu, latent_logvar) # sample an image from latent distribution\r\n        x_recon = self.decoder(latent)\r\n        return x_recon, latent_mu, latent_logvar\r\n\r\n    def latent_sample(self, mu, logvar):\r\n        if self.training:\r\n            # the reparamterization trick\r\n            std = logvar.mul(0.5).exp_()\r\n            eps = torch.empty_like(std).normal_()\r\n            return eps.mul(std).add_(mu)\r\n        else:\r\n            return mu\r\n\r\ndef vae_loss(recon_x, x, mu, logvar, variational_beta): # pass variational beta\r\n    recon_loss = F.binary_cross_entropy(recon_x.view(-1, 65536), x.view(-1, 65536), reduction = 'sum')\r\n\r\n    kldivergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\r\n\r\n    return recon_loss + variational_beta * kldivergence","112e4a5c":"def setup_model(capacity, latent_dims):\r\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n    print(device)\r\n\r\n    vae = VAE(capacity = capacity, latent_dims = latent_dims).to(device)\r\n    print(vae)\r\n\r\n    return vae","11514fd8":"def train(vae, train_loader, n_epochs, learning_rate, weight_decay, variational_beta):\r\n\r\n    optimizer = torch.optim.Adam(params=vae.parameters(), lr=learning_rate, weight_decay=weight_decay)\r\n\r\n\r\n    # set to training mode\r\n    vae.train()\r\n\r\n    train_loss = []\r\n    best_model_wts = None\r\n    bmw_epoch = 0\r\n\r\n\r\n    print('Training ...')\r\n    for epoch in tqdm(range(n_epochs)):\r\n        \r\n        num_batches = 0\r\n        avg_loss = 0\r\n        best_loss = 0\r\n        \r\n        image_batch_recon = None\r\n        \r\n        for image_batch, _ in train_loader:\r\n            \r\n            image_batch = image_batch.to(device)\r\n\r\n            # vae reconstruction\r\n            image_batch_recon, latent_mu, latent_logvar = vae(image_batch)\r\n            \r\n            # reconstruction error\r\n            loss = vae_loss(image_batch_recon, image_batch, latent_mu, latent_logvar, variational_beta)\r\n            \r\n            # backpropagation\r\n            optimizer.zero_grad()\r\n            loss.backward()\r\n            \r\n            # one step of the optmizer (using the gradients from backpropagation)\r\n            optimizer.step()\r\n\r\n            avg_loss += loss.item()\r\n            num_batches += 1\r\n            \r\n        if epoch % (n_epochs\/\/10) == 0 and epoch != 0:\r\n            show_image(image_batch_recon)\r\n        avg_loss \/= num_batches\r\n        train_loss.append(avg_loss)\r\n\r\n        ## best weight\r\n        if epoch == 0:\r\n            best_loss = avg_loss\r\n            best_model_wts = copy.deepcopy(vae.state_dict())\r\n        if avg_loss < best_loss:\r\n            best_model_wts = copy.deepcopy(vae.state_dict())\r\n            best_loss = avg_loss\r\n            bmw_epoch = epoch + 1\r\n    \r\n    print('Epoch [%d \/ %d] average reconstruction error: %f' % (epoch+1, n_epochs, sum(train_loss)\/len(train_loss)))\r\n    return vae, best_model_wts, bmw_epoch, train_loss","71484332":"for k, settings in enumerate(runs):\r\n\r\n    print(f\"starting run ... {k}\/{len(runs)}\")\r\n    print(settings)\r\n\r\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=settings[\"bs\"], shuffle=True, num_workers=0)\r\n\r\n    vae = setup_model(settings[\"cap\"], settings[\"latent\"])\r\n\r\n    vae, best_model_wts, bmw_epoch, train_loss = train(vae, train_loader, settings[\"nE\"], settings[\"lr\"], settings[\"wd\"], settings[\"vb\"])\r\n    last_modeL_wts = vae.state_dict()\r\n\r\n    plot_loss(train_loss, settings)\r\n\r\n    print(\"-------------------------------------------------------------------\")\r\n    print(\"saving weights for model\")\r\n\r\n    torch.save(vae.state_dict(), f'n_{settings[\"nE\"]}.ld_{settings[\"latent\"]}.lr_{settings[\"lr\"]}.vb_{settings[\"vb\"]}.last_model.wts')\r\n    vae.load_state_dict(best_model_wts)\r\n    torch.save(vae.state_dict(), f'n_{bmw_epoch}.ld_{settings[\"latent\"]}.lr_{settings[\"lr\"]}.vb_{settings[\"vb\"]}.lowest_loss_model.wts')\r\n\r\n    print(\"-------------------------------------------------------------------\")\r\n    print()","59c1e79f":"## setting up functions for training","e9fd864d":"# Implenetation of a Variational Autoencoder\r\n\r\nA simple implementation of a VAE on the following dataset:\r\n* https:\/\/www.kaggle.com\/mloey1\/covid19-chest-ct-image-augmentation-gan-dataset \r\n\r\nPurpose is to examine and compare results to the generated data from the following paper, where they used a CGAN to improve classification results:\r\n* https:\/\/link.springer.com\/article\/10.1007%2Fs00521-020-05437-x ","1b552658":"## Hyperparameter settings and data loading","f0ddf009":"## training model","9c9c3856":"## Setting up VAE architecture"}}