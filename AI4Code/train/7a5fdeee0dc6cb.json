{"cell_type":{"9665c18a":"code","c5f3d1de":"code","69d5ed8a":"code","224b2d65":"code","001eb486":"code","e1cc6a91":"code","f3b321a5":"code","8d0e74fd":"code","7bc6e66b":"code","8a962469":"code","286545bf":"code","69e82a9f":"code","5f291122":"code","ab6a86b0":"code","45d1a20b":"code","c6c05ebb":"code","b86b44cc":"code","3623954a":"code","1850f751":"code","e8481c2a":"markdown"},"source":{"9665c18a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c5f3d1de":"import numpy as np\nindiana_pines = np.load('\/kaggle\/input\/indian-pines-hyperspectral-dataset\/indianpinearray.npy')\nground_truth = np.load('\/kaggle\/input\/indian-pines-hyperspectral-dataset\/IPgt.npy')","69d5ed8a":"import spectral as sp\nview = sp.imshow(indiana_pines, (145, 145, 199))","224b2d65":"import matplotlib.pyplot as plt\nplt.imshow(ground_truth)\nprint(np.unique(ground_truth))","001eb486":"flattened_image = np.reshape(indiana_pines,(-1,indiana_pines.shape[-1]))\nflattened_ground = np.reshape(ground_truth,(-1,))","e1cc6a91":"from sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import TruncatedSVD\n\npipeline = Pipeline([('scaling', StandardScaler()), ('pca', PCA(n_components=50))])\npca_image = pipeline.fit_transform(flattened_image)\nsvdpipeline = Pipeline([('scaling', StandardScaler()), ('svd', TruncatedSVD(n_components=50))])\nsvd_image = svdpipeline.fit_transform(flattened_image)","f3b321a5":"norm_flattened = StandardScaler().fit_transform(flattened_image)","8d0e74fd":"def flatten(X):\n    if len(X.shape) <= 2:\n        X = X\n    else:\n        X = np.reshape(X,(-1,X.shape[-1]))\n    return X","7bc6e66b":"def standard(X):\n    from sklearn.preprocessing import StandardScaler\n    return StandardScaler().fit_transform(flatten(X))","8a962469":"def unflatten(X,size=145):\n    return np.reshape(X,(size,-1,X.shape[-1]))","286545bf":"class VarianceScorer:\n    def __init__(self,X_perfect):\n        self._original_variance = np.var(standard(X_perfect),axis = 0)\n        self._original = flatten(X_perfect)\n    \n    def score(self,X):\n        current_variance = np.var(flatten(X),axis = 0)\n        return (sum(current_variance) \/ sum(self._original_variance))","69e82a9f":"def neighbour_score(X,mode=\"prepend\"):\n    d1 = np.linalg.norm(np.diff(X,axis = 0,prepend = 0),axis = -1)\n    d2 = np.linalg.norm(np.flip(np.diff(np.flip(X,axis=0),axis = 0,prepend = 0),axis=0),axis=-1)\n    d3 = np.linalg.norm(np.diff(X,axis = 1,prepend = 0),axis=-1)\n    d4 = np.linalg.norm(np.flip(np.diff(np.flip(X,axis=1),axis = 1,prepend = 0),axis=1),axis=-1)\n    return np.dstack([d1,d2,d3,d4])","5f291122":"class NeighbourhoodScorer:\n    def __init__(self,X_perfect):\n        self._original_neighbourhood = self._neighbour_score(X_perfect)\n        self._original = flatten(X_perfect)\n        \n    def _neighbour_score(self,X,mode=\"wrap\"):\n        if mode == \"prepend\":\n            # Results in ~0.99699-0.99853 for num_components~50-75\n            d1 = np.linalg.norm(np.diff(X,axis = 0,prepend = 0),axis = -1)\n            d2 = np.linalg.norm(np.flip(np.diff(np.flip(X,axis=0),axis = 0,prepend = 0),axis=0),axis=-1)\n            d3 = np.linalg.norm(np.diff(X,axis = 1,prepend = 0),axis=-1)\n            d4 = np.linalg.norm(np.flip(np.diff(np.flip(X,axis=1),axis = 1,prepend = 0),axis=1),axis=-1)\n        elif mode == \"wrap\":\n            # Results in ~0.99695-0.99852 for num_component~50-75\n            d1 = np.linalg.norm(X - np.roll(X,shift=1,axis=0),axis = -1)\n            d2 = np.linalg.norm(X - np.roll(X,shift=-1,axis=0),axis = -1)\n            d3 = np.linalg.norm(X - np.roll(X,shift=1,axis=1),axis = -1)\n            d4 = np.linalg.norm(X - np.roll(X,shift=-1,axis=1),axis = -1)\n        return np.dstack([d1,d2,d3,d4])\n    \n    def score(self,X):\n        current_mat = self._neighbour_score(X)\n        return 1\/(1+np.linalg.norm(current_mat - self._original_neighbourhood)\/(current_mat.shape[0]*current_mat.shape[1]))\n    \n    def score1(self,X):\n        current_mat = self._neighbour_score(X)\n        scale = np.mean(current_mat,axis=(0,1)) \/ np.mean(self._original_neighbourhood,axis=(0,1))\n        bias = np.mean(self._original_neighbourhood - (current_mat\/scale))\n        return 1\/(1+np.linalg.norm((current_mat\/scale) + bias - self._original_neighbourhood)\/(current_mat.shape[0]*current_mat.shape[1]))\n            ","ab6a86b0":"Scorer = VarianceScorer(indiana_pines)\nScorer.score(pca_image)","45d1a20b":"Scorer = VarianceScorer(indiana_pines)\nScorer.score(svd_image)","c6c05ebb":"%timeit Scorer.score(svd_image)","b86b44cc":"Scorer1 = NeighbourhoodScorer(unflatten(standard(indiana_pines),145))\nScorer1.score1(unflatten(pca_image,145))","3623954a":"Scorer1 = NeighbourhoodScorer(unflatten(standard(indiana_pines),145))\nproper_svd = unflatten(svd_image,145)\nScorer1.score1(unflatten(svd_image,145))","1850f751":"%timeit Scorer1.score1(proper_svd)","e8481c2a":"Observations:\n - Variance Scorer drops much more steeply with drop in num_components compared to Neighbourhood Scorer.\n - Neighbourhood Scorer in wrap mode gives lower values than in prepend\n - Both give very high scores for PCA at num_components = 75\n - Variance Scorer rates PCA > SVD while Neighbourhood Scorer is opposite but difference is not significant ~ 10^-6 . Actual - SVD > PCA so Neighbourhood Scorer seems more accurate\n - Time:\n     - Variance Scorer - 4.26 ms \u00b1 93.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n     - Neighbourhood Scorer - 24.2 ms \u00b1 364 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n     - Neighbourhood Scorer is ~5 times slower as Neighbourhood scorer takes 4 axis differences to be more accurate having higher computation\n - In Neighbourhood Scorer score ~ 2ms faster than score1 on avg. but gives PCA > SVD"}}