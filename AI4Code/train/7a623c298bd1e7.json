{"cell_type":{"140e535c":"code","39bbc99b":"code","3f567d4a":"code","4892edd6":"code","89dd41fa":"code","84645294":"code","205286c6":"code","d4b6558f":"code","5c55203a":"code","8ecf44c3":"code","6843d497":"code","2e88fbac":"code","834b8821":"code","6a9463b4":"code","8f5f0502":"code","6bdcb8f2":"code","064bcdfa":"code","d2bcf68f":"code","e382ec52":"markdown","d91788af":"markdown","7c4bcf8b":"markdown","9b5be721":"markdown","1d257e2d":"markdown","97f4f1f1":"markdown","ea7132b8":"markdown","2c6c7b80":"markdown"},"source":{"140e535c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport scipy.io\nimport warnings\nimport os\nprint(os.listdir(\"..\/input\"))\nwarnings.filterwarnings('ignore')\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","39bbc99b":"print(\"Shape of Training data = \", train.shape)\nprint(\"Sahpe of Test Data = \", test.shape)","3f567d4a":"PC = train.iloc[:, 0:1]\ntrain = train.iloc[:, 1:]\nprint(\"trainX = \", train.shape, \"trainY = \", PC.shape)","4892edd6":"variance = np.var(train, axis = 0)>1000\ntrain = train.loc[:, variance]\ntest = test.loc[:, variance]\nprint(\"Shape of Training data = \", train.shape)\nprint(\"Sahpe of Test Data = \", test.shape)","89dd41fa":"from sklearn.model_selection import train_test_split\ntrain_X, val_X, train_Y, val_Y = train_test_split(train, PC, test_size = 0.2, \n                                                 random_state = 0)\nprint(\"train_X Shape = \", train_X.shape, \"train_Y Shape = \", train_Y.shape)\nprint(\"val_X Shape = \", val_X.shape, \"val_Y Shape = \", val_Y.shape)","84645294":"from sklearn.decomposition import PCA\npca = PCA(n_components=0.8)\npca.fit(train_X)\ntrain_X = pca.transform(train_X)\nval_X = pca.transform(val_X)\ntest = pca.transform(test)\nprint(\"train_X Shape = \", train_X.shape)\nprint(\"val_X Shape = \", val_X.shape)\nprint(\"test Shape = \", test.shape)","205286c6":"def normalize(sigma, mean, X):\n    X = (X - mean)\/sigma\n    return X\n    \nsigma = np.std(train_X, axis = 0)\nmean = np.mean(train_X, axis = 0)\n\ntrain_X = normalize(sigma, mean, train_X)\nval_X = normalize(sigma, mean, val_X)\ntest = normalize(sigma, mean, test)","d4b6558f":"print(train_X.shape)","5c55203a":"train.isna().any().any()","8ecf44c3":"l1Dist = np.array([[3],\n                  [1],\n                  [2]])\ny = np.array([[1],\n             [2],\n             [3],\n             [4]])\nind = np.argsort(l1Dist, axis = 0)\nprint(ind)\nprint(y[ind[:2,0]].ravel())","6843d497":"class KNN:\n    def __init__(self, k = 5):\n        self.k = k\n    #X.shape = [1, n]\n    def l1(self, X):\n        return np.sum(np.abs(X - self.X_train), axis = 1)\n    #X.shape = [number of examples, feature] = [m, n]\n    #y.shape = [number of examples, 1]\n    def fit(self, X, y):\n        self.X_train = X\n        self.Y_train = y.reshape(y.shape[0], 1)\n        \n    def predictHelper(self, l1Dist):\n        l1Dist = l1Dist.reshape(l1Dist.shape[0], 1)\n        l1Arg = np.argsort(l1Dist, axis = 0)[:self.k,0]\n        counts = np.bincount(self.Y_train[l1Arg].ravel())\n        return np.argmax(counts)\n    \n    def predict(self, X_test):\n        y_test = np.zeros((X_test.shape[0], 1))\n        for i in range(X_test.shape[0]):\n            l1Dist = self.l1(X_test[i:i+1])\n            y_test[i] = self.predictHelper(l1Dist)\n        \n        return y_test.ravel().astype(np.int)","2e88fbac":"model = KNN(5)\nmodel.fit(train_X, train_Y.values)\ny_test = model.predict(val_X)","834b8821":"boolMap =  (y_test == val_Y.iloc[:,0].values)","6a9463b4":"acc = np.sum(boolMap)\/y_test.shape[0]\nprint(acc)","8f5f0502":"sub = pd.DataFrame()\nsub['ImageId'] = np.arange(1, test.shape[0]+1)\nsub['Label'] = model.predict(test)\nsub.to_csv(\"outKNN.csv\", index = False, header = True)\nsub.head(n = 10)","6bdcb8f2":"from sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=10)\nneigh.fit(train_X, train_Y)\nprint(\"Training Error = \", neigh.score(train_X, train_Y))\nprint(\"Validation Error = \", neigh.score(val_X, val_Y))","064bcdfa":"(val_Y.values>0).sum()","d2bcf68f":"sub = pd.DataFrame()\nsub['ImageId'] = np.arange(1, test.shape[0]+1)\nsub['Label'] = neigh.predict(test)\nsub.to_csv(\"outKNN1.csv\", index = False, header = True)\nsub.head(n = 10)","e382ec52":"### Remove Columns with Less Variance","d91788af":"### Train using sklearn KNN Classifier","7c4bcf8b":"### Dimensionality Reduction Using PCA\n> We will choose top n components which can represent 80% variance of data","9b5be721":"#### Separate Features and Prediction","1d257e2d":"### Custome k nearest neighbour classifier","97f4f1f1":"### Normalize Data","ea7132b8":"### Import Libraries and read data","2c6c7b80":"#### Split Data in Training and Validation Set\n- Training Data: 80%\n- Validation Data: 20%"}}