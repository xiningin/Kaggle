{"cell_type":{"ad629276":"code","8e2086e4":"code","ee21f362":"code","9d644b82":"code","117ea511":"code","45af0902":"code","50a4c6c1":"code","8bc8edb5":"code","e13fedba":"code","093da3ad":"code","4e376019":"code","39904e05":"code","76bf32b1":"code","63a63468":"code","15856578":"code","4d29954f":"code","83f08f3e":"code","f122d826":"code","00becf7d":"code","dbdf55f8":"code","e9fdc8f2":"code","9a399b0d":"code","c5f61459":"code","ec70c3c7":"code","351da517":"code","66b6873e":"code","f8253dc8":"code","fcf851eb":"code","55d84cd2":"code","eb8dfc00":"code","81272f44":"code","be405e86":"code","5005b439":"code","b337d67e":"code","38dbd808":"code","0ee564a0":"code","deb652b6":"code","cd7f1e31":"code","1ea4a605":"code","c6a2c3c7":"code","f405ead3":"code","aa7e268c":"code","b20124dd":"code","d57de352":"code","10c0f23e":"code","b6938af9":"code","0a70f70f":"code","8477594b":"code","8457e045":"code","bd8969ef":"code","b56141c5":"code","27d01f5e":"code","d643c128":"code","b3578e9b":"code","78c1a9c3":"code","be6b59e4":"markdown","afd27f15":"markdown","a5163e61":"markdown","dd9333c5":"markdown","7848e264":"markdown","ec9c6894":"markdown","9bfad2de":"markdown","4fb50bc4":"markdown","4aae1058":"markdown","c2009fc5":"markdown","6355fc97":"markdown","79895464":"markdown","40069307":"markdown","d0c098b9":"markdown","a5ceba58":"markdown","63d61060":"markdown","0a459586":"markdown","a286953e":"markdown","8ab03518":"markdown","e388aa03":"markdown","8d9b1528":"markdown","255d7a7d":"markdown","8d5e6ebd":"markdown","34759f5a":"markdown","b1494ebf":"markdown","8813914f":"markdown"},"source":{"ad629276":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8e2086e4":"mercedesData = pd.read_csv(\"\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/merc.csv\")","ee21f362":"mercedesData.head()","9d644b82":"mercedesData.describe()","117ea511":"mercedesData.info()","45af0902":"mercedesData.dtypes","50a4c6c1":"mercedesData.isnull().sum()","8bc8edb5":"categorical_datas = mercedesData.select_dtypes(include=[\"object\"])","e13fedba":"categorical_datas.head()","093da3ad":"categorical_datas[\"transmission\"].value_counts()","4e376019":"categorical_datas.fuelType.value_counts()","39904e05":"categorical_datas[\"model\"].value_counts()","76bf32b1":"mercedesData = mercedesData[mercedesData.year >= 1990]","63a63468":"import matplotlib.pyplot as plt\nimport seaborn as sns","15856578":"numericalDatas = mercedesData.select_dtypes(exclude=[\"object\"])","4d29954f":"plt.figure(figsize=(10,15))\nsns.pairplot(numericalDatas)\nplt.show()","83f08f3e":"plt.figure(figsize=(15,10))\nplt.title(\"Relationship Between The Features of The Mercedes Data\")\nsns.heatmap(mercedesData.corr(),annot=True,fmt=\".5f\",linewidths=1,linecolor=\"gray\")\nplt.show()","f122d826":"pricevsYear = mercedesData.loc[:,[\"price\",\"year\"]]","00becf7d":"pricevsYear=pricevsYear.groupby([\"year\"]).mean().price","dbdf55f8":"plt.figure(figsize=(15,10))\nsns.barplot(x=pricevsYear.index,y=pricevsYear.values,palette=sns.cubehelix_palette())\nplt.xticks(rotation=45,fontsize=20)\nplt.yticks(fontsize=20)\nplt.ylabel(\"Price\",fontsize=20)\nplt.xlabel(\"Year\",fontsize=20)\nplt.title(\"Price vs Year\",fontsize=30)\nplt.show()","e9fdc8f2":"mercedesData.engineSize.mean()","9a399b0d":"under2 = mercedesData[mercedesData.engineSize < 2].price.mean()\nover2 = mercedesData[mercedesData.engineSize > 2].price.mean()\n\nd = {\n    \"under2\" : under2,\n    \"over2\" : over2\n}\n\ndata = pd.Series(d)\n\n\nsns.barplot(x = data.index,y=data.values)\nplt.title(\"Car's Prices According to Kinds of Engine Sizes\")\nplt.show()","c5f61459":"zeroTo2 = mercedesData[mercedesData.engineSize < 2.0].price.mean()\ntoTwo4 = mercedesData[(mercedesData.engineSize > 2) & (mercedesData.engineSize < 4)].price.mean()\nfourTo6 = mercedesData[(mercedesData.engineSize > 4) & (mercedesData.engineSize < 6)].price.mean()","ec70c3c7":"d1 = {\n    \"0-2\" : zeroTo2,\n    \"2-4\" : toTwo4,\n    \"4-6\" : fourTo6\n}","351da517":"data1 = pd.Series(d1)\n\n\nsns.barplot(x = data1.index,y=data1.values)\nplt.title(\"Car's Milages According to Kinds of Engine Sizes\")\nplt.show()","66b6873e":"mercedesData.mileage.describe()","f8253dc8":"yearvsPrice = mercedesData.groupby([\"year\"]).price.mean()\n\nplt.figure(figsize=(15,10))\nplt.xticks(rotation=45,fontsize=10)\nplt.ylabel(\"Prices\")\nplt.xlabel(\"Years\")\nsns.barplot(x= yearvsPrice.index,y=yearvsPrice.values)\nplt.show()","fcf851eb":"from sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n","55d84cd2":"clone = mercedesData.copy()","eb8dfc00":"# numFeatures = clone.select_dtypes(exclude=[\"object\"])\n# catFeatures = clone.select_dtypes(include=[\"object\"])\n\n\n# preprocessor = make_column_transformer(\n#     (StandardScaler(), numFeatures),\n#     (OneHotEncoder(), catFeatures),\n# )\n","81272f44":"df = pd.get_dummies(data=clone,columns=[\"model\",\"transmission\",\"fuelType\"])\ndf.head()","be405e86":"from sklearn.model_selection import train_test_split","5005b439":"# X = df.drop(\"price\",axis=1)\n# y = df[\"price\"]","b337d67e":"df.head()","38dbd808":"df_train, df_test = train_test_split(df, \n                                     train_size = 0.7, \n                                     test_size = 0.3, \n                                     random_state = 100)","0ee564a0":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error as MSE \nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import mean_absolute_error","deb652b6":"# rescale the features\nscaler = MinMaxScaler()\n\nnumCols = [col for col in df.columns if df[col].dtype in [\"int64\",\"float64\"] ]\n\n# scaling training data\n\ndf_train[numCols] = scaler.fit_transform(df_train[numCols])\n\ndf_train.head()\n","cd7f1e31":"df_test[numCols] = scaler.fit_transform(df_test[numCols])\ndf_test.head()","1ea4a605":"y_train = df_train.pop(\"price\")\nX_train = df_train","c6a2c3c7":"y_test = df_test.pop(\"price\")\nX_test = df_test","f405ead3":"linReg = LinearRegression()\nlinReg.fit(X_train,y_train)\n\nrfe = RFE(linReg, n_features_to_select=10)             \nrfe = rfe.fit(X_train, y_train)\n\n# y_pred = linReg.predict(X_test)\ny_pred = rfe.predict(X_test)\n\n\n# print(\"Model score : {}\".format(np.sqrt(MSE(y_test,y_pred))))\nprint(\"Model score : {}\".format(mean_absolute_error(y_test,y_pred)))\n","aa7e268c":"from sklearn.model_selection import cross_val_score","b20124dd":"lm = LinearRegression()\nscores = cross_val_score(lm, X_train, y_train, scoring='r2', cv=5)\nscores","d57de352":"from sklearn.ensemble import RandomForestRegressor","10c0f23e":"# Separate target from predictors\ny = df.price\nX = df.drop(['price'], axis=1)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)","b6938af9":"ranFor = RandomForestRegressor(n_estimators=100, random_state=0)\nranFor.fit(X_train,y_train)\npreds = ranFor.predict(X_valid)\n# score = np.sqrt(MSE(y_valid,preds))\nscore = mean_absolute_error(y_valid,preds)\nprint('mean_absolute_error:', score)","0a70f70f":"from xgboost import XGBRegressor","8477594b":"my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\nmy_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)\nmy_model.score(X_valid,y_valid)","8457e045":"predictions_1 = my_model.predict(X_valid)\n\nprint(\"MSE : {}\".format(MSE(y_valid,predictions_1)))","bd8969ef":"from sklearn.metrics import mean_absolute_error\nmae_1 = mean_absolute_error(predictions_1,y_valid)\nprint(\"Mean Absolute Error:\" , mae_1)\n","b56141c5":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\nfrom tensorflow import keras","27d01f5e":"input_shape = [X_train.shape[1]]\n\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_shape=input_shape),\n    layers.Dense(1024,activation=\"relu\"),\n    layers.Dropout(0.5),\n    layers.BatchNormalization(),\n    layers.Dense(1024,activation=\"relu\"),\n     layers.Dropout(0.5),\n    layers.BatchNormalization(),\n    layers.Dense(1024,activation=\"relu\"),\n     layers.Dropout(0.5),\n    layers.BatchNormalization(),\n    layers.Dense(1),\n])\n\nmodel.compile(\n    optimizer='sgd',\n    loss='mae',\n    metrics=['mae'],\n)\n\n# model.compile(\n#     optimizer='adam',\n#     loss='mse'\n# )\n\nearly_stopping = callbacks.EarlyStopping(\n    patience=20,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\n\n\nhistory = model.fit(\n    X_train,y_train,\n    validation_data=(X_valid,y_valid),\n    batch_size=512,\n    epochs = 600,\n    callbacks=[early_stopping],\n    verbose=0\n)\n\n","d643c128":"history_df = pd.DataFrame(history.history)\nhistory_df.head()","b3578e9b":"history_df.loc[:,[\"loss\",\"val_loss\"]].plot()\nplt.show()","78c1a9c3":"history_df.loc[:,\"mae\"].max()","be6b59e4":"**I have removed old cars from our dataset to prevent occur outliers**","afd27f15":"## Model - 4 -> Neural Network","a5163e61":"## Mileage vs Engine-Sizes","dd9333c5":"Heatmap is one of the most important tools to see relationship between the features of the data","7848e264":"## Split Data","ec9c6894":"The method of **describe** gives us detailed information about numerical features. As well, you can detect outliers values here. For instance, the minimum year of the cars is 1970 and the maximum year of the cars is 2020. So the price difference between those cars very large, it may be caused to the wrong prediction so we should remove old cars.","9bfad2de":"## Model-1 -> Linear Regression","4fb50bc4":"I will scale train and test data to get better result ","4aae1058":"The dataset below consists of categorical features. If we want to obtain recognizable predict results, we should use categorical data to train our model, too.","c2009fc5":"### Cross Validation","6355fc97":"### Engine Size vs Price","79895464":"# Exploring Data Analysis (EDA)","40069307":"### Year vs Price","d0c098b9":"## Model-3 -> XGBoost","a5ceba58":"To do this, I will convert categorical data to numerical data with dummies. Then I will concat dummy variables df with the main df","63d61060":"### divide into our datas as X_train, y_train, X_test, y_test","0a459586":"- I will develop neural network model with keras.\n- ","a286953e":"### scaling training data","8ab03518":"# Data Preparation","e388aa03":"## Overview of The Dataset","8d9b1528":"This dataset has no non-recorded data. We can jump into develop the model after data-analysis without handle missing values","255d7a7d":"# Developing Models","8d5e6ebd":"### scale test data","34759f5a":"## Visualize The Dataset","b1494ebf":"## Model-2 -> Random Forest","8813914f":"### Price vs Year"}}