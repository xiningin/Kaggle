{"cell_type":{"7f58d7e8":"code","adfe0c3e":"code","8c20ca02":"code","00b30563":"code","fb15daac":"code","734b8106":"code","60d1e337":"code","51d0a306":"code","83b754bc":"code","f61cb145":"code","a0a1faa2":"code","985dbb0e":"code","0a346644":"code","2972961f":"code","957daf5b":"code","35920158":"code","a8fd9d14":"code","acbf777a":"code","0080cf89":"code","df1fd249":"markdown","180d3ee6":"markdown","6952140e":"markdown","2f5f00d1":"markdown","6e640ff2":"markdown","055de178":"markdown","af4804a5":"markdown","1608c7dc":"markdown"},"source":{"7f58d7e8":"import re\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\n# Porter stemmer\nfrom nltk.stem.porter import PorterStemmer\nporter = PorterStemmer()\n# Snowball stemmer\nfrom nltk.stem import SnowballStemmer\nsnowball = SnowballStemmer('english')\n# Wordnet lemmatizer\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\nimport warnings\nwarnings.filterwarnings('ignore')","adfe0c3e":"tweets_data = \"..\/input\/disaster-tweets\/tweets.csv\"","8c20ca02":"# Read data\ntweets = pd.read_csv(tweets_data)\ntweets.head()","00b30563":"def preprocessor(text):\n    text = re.sub('<[^>]*>', '', text)\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n        ' '.join(emoticons).replace('-', '')\n    return text","fb15daac":"# apply the preprocess function to all tweets\ntweets['text'] = tweets['text'].apply(preprocessor)","734b8106":"X = tweets['text']\ny = tweets['target']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","60d1e337":"bag_of_words_vectorizer = CountVectorizer(min_df=5)\nbow_vectors = bag_of_words_vectorizer.fit_transform(tweets['text'])","51d0a306":"#bag_of_words_vectorizer.vocabulary_","83b754bc":"# For bag of words\npca = PCA(n_components=2)\nx_pca = pca.fit_transform(bow_vectors.todense())\nplt.figure(figsize=(8,6))\nplt.scatter(x_pca[:,0],x_pca[:,1],c=tweets['target'],cmap='rainbow')\nplt.xlabel('First principal component')\nplt.ylabel('Second Principal Component')","f61cb145":"TFIDF_vectorizer = TfidfVectorizer(min_df=5)\ntfidf_vectors = TFIDF_vectorizer.fit_transform(tweets['text'])","a0a1faa2":"#TFIDF_vectorizer.get_feature_names()","985dbb0e":"# For TFIDF vectors\npca_tfidf = PCA(n_components=2)\nx_pca = pca_tfidf.fit_transform(tfidf_vectors.todense())\nplt.figure(figsize=(8,6))\nplt.scatter(x_pca[:,0],x_pca[:,1],c=tweets['target'],cmap='rainbow')\nplt.xlabel('First principal component')\nplt.ylabel('Second Principal Component')","0a346644":"def tokenizer(text):\n    return text.split()\n\ndef tokenizer_porter(text):\n    return [porter.stem(word) for word in text.split()]\n\ndef tokenizer_snowball(text):\n    return [snowball.stem(word) for word in text.split()]\n\ndef tokenizer_wordnet_lemmatizer(text):\n    return [lemmatizer.lemmatize(word) for word in text.split()]\n\nTFIDF_vectorizer = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)","2972961f":"param_grid = [\n    {\n        'vect__ngram_range': [(1, 2)],\n        'vect__stop_words': [stop, None],\n        'vect__tokenizer': [tokenizer, tokenizer_porter, tokenizer_snowball, tokenizer_wordnet_lemmatizer],\n        'clf__penalty': ['l1', 'l2'],\n        'clf__C': [1.0, 10.0, 100.0]        \n    },\n    {\n        'vect__ngram_range': [(1, 2)],\n        'vect__stop_words': [stop, None],\n        'vect__tokenizer': [tokenizer, tokenizer_porter, tokenizer_snowball, tokenizer_wordnet_lemmatizer],\n        'vect__use_idf': [False],\n        'vect__norm': [None],\n        'clf__penalty': ['l1', 'l2'],\n        'clf__C': [1.0, 10.0, 100.0]\n    }\n]\n\nlr_tfidf = Pipeline([('vect', TFIDF_vectorizer),\n                     ('clf', LogisticRegression(random_state=0))])\n\ngs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy',\n                           cv=5, verbose=1, n_jobs=-1)","957daf5b":"gs_lr_tfidf.fit(X_train, y_train)","35920158":"# Get the best parameters\ngs_lr_tfidf.best_params_","a8fd9d14":"# Get the best score\ngs_lr_tfidf.best_score_","acbf777a":"# Determine the score of the best model on the test set (We use here TFIDF vectorizer + LogisticRegression)\nclf = gs_lr_tfidf.best_estimator_\nclf.score(X_test, y_test)","0080cf89":"print(tweets['text'][0])\nprint(clf.predict([preprocessor(tweets['text'][0])]))\nprint('True target: ', tweets['target'][0])","df1fd249":"# Clean up Data ","180d3ee6":"## Count Vectorizer","6952140e":"## TF-IDF Vectorizer","2f5f00d1":"# Vectorization","6e640ff2":"# Pipeline (TFIDF + LR)","055de178":"# Test the Pipeline","af4804a5":"# Explore textual data","1608c7dc":"# Import Libraries"}}