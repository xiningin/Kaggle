{"cell_type":{"c4792db7":"code","7193611d":"code","51ab895d":"code","aef451ad":"code","fbf28994":"code","62827884":"code","cdeb5101":"code","7ea81b07":"code","f6d4e56e":"code","4a20501d":"code","0d89e69d":"code","6cc974e7":"code","069fa60b":"code","cd2aeabb":"code","54a4fd19":"code","7ab0f51d":"code","ded712ef":"code","0961f4c6":"code","7f8070da":"code","b17840fd":"code","8aeba4b5":"code","0ea12666":"code","bed1e974":"code","fa22f23c":"code","fa711d45":"code","5080abf9":"code","37034f0f":"code","34228e39":"code","f7c3c1cd":"code","4b160851":"code","68e541a7":"code","b6d6dc6b":"code","37ff110f":"code","405b0687":"code","816b80f5":"code","14e3203e":"code","d51b5c28":"code","f11b76c6":"code","76a1ffc6":"code","cfe10fa4":"code","56af404c":"code","d9d5b85f":"code","ae729ecd":"code","b9f108e2":"code","9f45d54b":"code","d765d0c3":"code","775769ef":"code","0b38966d":"code","d139ed6d":"code","530c47f5":"code","3b89ea3e":"code","4c975c4f":"code","e1f5d6ab":"code","a8f50bb4":"code","a2f37977":"code","7146cfdb":"code","c5b45cfb":"code","b786acd0":"code","15d0aa47":"code","d534855c":"code","dcfc1d17":"code","0a3903b2":"code","2f24a267":"code","eff67c43":"code","97230e74":"code","1c364eb5":"code","145e2d9d":"code","20501bed":"code","93d12c76":"code","c145a72a":"code","4b1525f9":"code","6a98c107":"code","3cb77136":"code","fd827bfa":"code","60544521":"code","99c420a9":"code","b93fbd56":"code","6a31b42a":"code","efe81e36":"code","681a3bc2":"code","02dd307a":"code","4df37e8c":"code","d6d7e57c":"code","edb15582":"markdown","2287327a":"markdown","abf88864":"markdown","495affa8":"markdown","dd32957c":"markdown","0b6e2269":"markdown","d5ba4105":"markdown","74ef35e9":"markdown","45507c0e":"markdown","33fa73e1":"markdown","943c9916":"markdown","620683ea":"markdown","7779850e":"markdown","299264c1":"markdown","5cb644bf":"markdown","3b7f9d6e":"markdown","e3c53fbe":"markdown","c71fc2b9":"markdown","e05f001c":"markdown","b21a46e3":"markdown","061348d0":"markdown","0a964f14":"markdown","4b20f200":"markdown","4c30b4b8":"markdown","69ac4526":"markdown","0c420219":"markdown","f965980c":"markdown","88260cd3":"markdown","cdad2adc":"markdown","4e0d21b8":"markdown","680aa394":"markdown","bdfeab56":"markdown","e5035f12":"markdown","027e71ae":"markdown","40003422":"markdown","59ca67ee":"markdown","21464f62":"markdown","a630b969":"markdown","00085cb4":"markdown","942a225a":"markdown","99724714":"markdown","30e58029":"markdown","58cc4edf":"markdown","e0ecccdf":"markdown","ecf165bc":"markdown","bab7538e":"markdown","cfcb41b0":"markdown","f69caa5c":"markdown","ab05ca7f":"markdown","63b94532":"markdown","a54a5d8b":"markdown","5aade1b3":"markdown","43596a96":"markdown","6d58d1fb":"markdown","d92f3a94":"markdown","aaa25927":"markdown","c59fdef3":"markdown","b7dba452":"markdown","99cbe660":"markdown","18c80122":"markdown","9be0d6cc":"markdown","b19a8840":"markdown","07163873":"markdown","db489f2f":"markdown","c5041eab":"markdown","ce7c4580":"markdown","197e6c67":"markdown","7c4251a8":"markdown","2b42e408":"markdown","ecbee19e":"markdown","94f97671":"markdown","de1f0427":"markdown","5f0d06f4":"markdown","ae4e45b6":"markdown"},"source":{"c4792db7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7193611d":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score,confusion_matrix","51ab895d":"train=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ngender_sub=pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")","aef451ad":"train.shape,test.shape,gender_sub.shape","fbf28994":"print(\"Columns of gender_sub:\",gender_sub.columns)\nprint(\"\\nColumns of test:\",test.columns)","62827884":"df_test=pd.concat([test,gender_sub[\"Survived\"]],axis=1)","cdeb5101":"train.shape,df_test.shape","7ea81b07":"df=pd.concat([train,df_test],axis=0,sort=False)","f6d4e56e":"df.head()","4a20501d":"df.shape","0d89e69d":"#Age Distribution \nfig=plt.figure(figsize=(10,7))\nsns.histplot(\n    df[\"Age\"],\n    bins=10,\n    color=\"magenta\"\n)\nplt.title(\"Age Distribution\")\nplt.show()","6cc974e7":"fig=plt.figure(figsize=(10,7))\nsns.countplot(\n    data=df,\n    x=df[\"Sex\"],\n    hue=df[\"Survived\"]\n)\nplt.title(\"Gender Survival\")\nplt.show()","069fa60b":"plt.pie(\n    df[\"Embarked\"].value_counts(),\n    radius=1.7,\n    autopct=\"%0.2f%%\",\n    labels=df[\"Embarked\"].dropna().unique(),\n    shadow=True,\n)\nplt.show()","cd2aeabb":"sns.catplot(\n    x=\"Parch\",\n    data=df,\n    kind=\"count\",\n    row=\"Sex\",\n    col=\"Pclass\"\n)\nplt.show()","54a4fd19":"sns.catplot(\n    x=\"Survived\",\n    y=\"Fare\",\n    data=df,\n    kind=\"box\",\n    hue=\"Sex\",\n    dodge=True,\n    col=\"Embarked\",\n)\nplt.show()","7ab0f51d":"fig=plt.figure(figsize=(10,7))\ncorr=df.corr()\nsns.heatmap(corr,annot=True)\nplt.show()","ded712ef":"train.info()","0961f4c6":"test.info()","7f8070da":"train.describe(include=\"all\")","b17840fd":"test.describe(include=\"all\")","8aeba4b5":"def check_null(df):\n    for col in df:\n        null_count=df[col].isnull().sum()\n        total=df.shape[0]\n        \n        percent_null=null_count\/total*100\n        \n        if null_count!=0:\n            print(col,\" contains: {:.2f}% null values\".format(percent_null))     ","0ea12666":"print(\"Null Values of Training Data:\")\ncheck_null(train)\nprint(\"-----------------------------\")\nprint(\"Null Values of Testing Data:\")\ncheck_null(test)","bed1e974":"train=train.drop(\"Cabin\",axis=1)\ntest=test.drop(\"Cabin\",axis=1)","fa22f23c":"fig=plt.figure(figsize=(10,6))\nsns.kdeplot(\n    df[\"Age\"].dropna(),\n         )\nplt.title(\"Age Distribution in original Data\")\nplt.show()","fa711d45":"df[\"Age\"].describe()","5080abf9":"train[\"Age\"].fillna(\n    df[\"Age\"].median(),\n    inplace=True\n)\ntest[\"Age\"].fillna(\n    df[\"Age\"].median(),\n    inplace=True\n)","37034f0f":"df[\"Embarked\"].dtype","34228e39":"df[\"Embarked\"].value_counts()","f7c3c1cd":"train[\"Embarked\"].fillna(\n    df[\"Embarked\"].mode()[0],\n    inplace=True\n)","4b160851":"train.isnull().sum()","68e541a7":"fig=plt.figure(figsize=(10,6))\nsns.kdeplot(\n    df[\"Fare\"].dropna(),\n)\nplt.show()","b6d6dc6b":"df[\"Fare\"].describe()","37ff110f":"test[\"Fare\"].fillna(\n    df[\"Fare\"].median(),\n    inplace=True\n)","405b0687":"test.isnull().sum()","816b80f5":"train[\"Embarked\"].replace(\n    {\n        \"S\":\"Southampton\",\n        \"Q\":\"Queenstown\",\n        \"C\":\"Cherbourg\"\n    },\n    inplace=True\n)\ntest[\"Embarked\"].replace(\n    {\n        \"S\":\"Southampton\",\n        \"Q\":\"Queenstown\",\n        \"C\":\"Cherbourg\"\n    },\n    inplace=True\n)","14e3203e":"train.head(3)","d51b5c28":"test.head(3)","f11b76c6":"cat_col=[]\nnum_col=[]\nfor col in test:\n    if test[col].dtype==\"O\":\n        cat_col.append(col)\n    else:\n        num_col.append(col)","76a1ffc6":"cat_col","cfe10fa4":"num_col","56af404c":"train_cat=pd.DataFrame(\n    data=train,\n    columns=cat_col\n)\ntest_cat=pd.DataFrame(\n    data=test,\n    columns=cat_col\n)","d9d5b85f":"train_num=pd.DataFrame(\n    data=train,\n    columns=num_col\n)\ntest_num=pd.DataFrame(\n    data=test,\n    columns=num_col\n)","ae729ecd":"train_cat.head(3)","b9f108e2":"test_cat.head(3)","9f45d54b":"train_cat[\"Ticket\"].nunique()","d765d0c3":"test_cat[\"Ticket\"].nunique()","775769ef":"train_cat.drop(\"Ticket\",\n              axis=1,\n              inplace=True\n              )\n\ntest_cat.drop(\"Ticket\",\n              axis=1,\n              inplace=True\n             )","0b38966d":"train_cat.columns","d139ed6d":"train_cat.columns","530c47f5":"titles=df[\"Name\"].str.split(\",\",expand=True)[1]\ntitles=titles.str.split(\".\",expand=True)[0]","3b89ea3e":"titles.value_counts()","4c975c4f":"def get_title(df):\n    \n    title_list=[' Mr', ' Mrs', ' Miss']\n    \n    titles=df[\"Name\"].str.split(\",\",expand=True)[1]\n    titles=titles.str.split(\".\",expand=True)[0]\n    \n    for title in titles.unique():\n        if title not in title_list:\n            titles.replace(\n                {\n            title:\"Others\"\n                },\n                inplace=True\n            )\n    return titles       \n    ","e1f5d6ab":"train_title=get_title(train)\ntest_title=get_title(test)\ntrain_cat[\"Title\"]=train_title\ntest_cat[\"Title\"]=test_title","a8f50bb4":"train_cat.drop(\"Name\",axis=1,inplace=True)\ntest_cat.drop(\"Name\",axis=1,inplace=True)","a2f37977":"train_cat.columns","7146cfdb":"test_cat.columns","c5b45cfb":"train_cat=pd.get_dummies(\n    train_cat,\n    drop_first=True\n)\ntest_cat=pd.get_dummies(\n    test_cat,\n    drop_first=True\n)","b786acd0":"train_cat.head()","15d0aa47":"test_cat.head()","d534855c":"train_final=pd.concat([train_num,train_cat],\n                     axis=1)\ntest_final=pd.concat([test_num,test_cat],\n                    axis=1)","dcfc1d17":"train_final.head()","0a3903b2":"test_final.head()","2f24a267":"train_final.shape","eff67c43":"test_final.shape","97230e74":"train_final=pd.concat([train_final,train[\"Survived\"]],\n                     axis=1)","1c364eb5":"train_final.shape","145e2d9d":"train_final.head(3)","20501bed":"train_final.drop(\"PassengerId\",axis=1,inplace=True)\ntest_final.drop(\"PassengerId\",axis=1,inplace=True)","93d12c76":"X=train_final.drop(\"Survived\",\n                  axis=1)\ny=train_final[\"Survived\"]","c145a72a":"scaler=StandardScaler()\nX=scaler.fit_transform(X)\ntest_final=scaler.transform(test_final)","4b1525f9":"model=LogisticRegression()","6a98c107":"params={\n    \"C\":[0.001,0.01,0.1,1,10,100],\n    \"max_iter\":[10,100,1000]\n}\ngrid=GridSearchCV(model,param_grid=params,cv=10)","3cb77136":"grid.fit(X,y)\n","fd827bfa":"grid.best_params_","60544521":"logreg=LogisticRegression(**grid.best_params_)","99c420a9":"logreg.fit(X,y)","b93fbd56":"y_test=gender_sub[\"Survived\"]\ny_pred=logreg.predict(test_final)","6a31b42a":"cm=confusion_matrix(y_test,y_pred)\nsns.heatmap(\n    cm,\n    annot=True\n)\nplt.show()","efe81e36":"accuracy_score(y_test,y_pred)","681a3bc2":"predict=pd.DataFrame(\n    data=y_pred,\n    columns=[\"Survived\"]\n)","02dd307a":"output=pd.concat([gender_sub[\"PassengerId\"],predict],axis=1)","4df37e8c":"output.head()","d6d7e57c":"output.to_csv(\"submission.csv\",index=False)","edb15582":"The funtion below check if the data contains any null values","2287327a":"The Embarked have object datatype, and contains three unique values.","abf88864":"Make dataframes for non-object columns for both test and train data","495affa8":"Droppend Ticket in both train and test dataframe","dd32957c":"lets visualize the confusion_matrix","0b6e2269":"The below kdeplot of Fare shows that the curve is left skewed and hence for this type of distribution the mean will be shifted more towards left. ","d5ba4105":"First I will check about the Ticket features","74ef35e9":"A model prediction is pretty high which is a good fitting model","45507c0e":"Check the unique titles of passengers.","33fa73e1":"import necessary models required for Notebook","943c9916":"The Embarked column is clearly a categorical column and for categorical feature I will impute the train missing value with the mode of Embarked **original** dataframe(df)","620683ea":"Load Data ","7779850e":"We have Embarked missing value in train data so lets try to impute the value.","299264c1":"Final categorical dataframe","5cb644bf":"We get a good model generalization ","3b7f9d6e":"Since we already have PassengerId in test we will concatenate only Survived column","e3c53fbe":"The best parameter we got are C:0.01 & max_iter:10","c71fc2b9":"Comming to Name its also Unique however we can see that titles like Mr,Mrs,Miss etc...are being addressed in thier name so lets try to extract thier titles form the name.","e05f001c":"Our datasets now does not contains any null values, however the Embarked columns contains names that does not makes any sense, so for better understanding lets change to their original(full) names","b21a46e3":"Now we got the desired shape","061348d0":"Now we can drop Name column in both the dataframes","0a964f14":"Lets Seperate data into dependent and independent variables","4b20f200":"Dont get surprised with same feature numbers because we have excluded our target variable for transfroming our independent varibles earlier","4c30b4b8":"In Both the dataset, we can see that Cabin contains more than 77% missing data, so lets drop the Cabin column and impute missing data for other columns","69ac4526":"We have filled in the missing data for trian data.","0c420219":"for that lets make use of y_test and y_pred for the evaluation","f965980c":"**Data Preparation**: We have explore and visualized the data so lets prepare our train and test data","88260cd3":"Now we have an equal number of features so lets concatenate them to get original data","cdad2adc":"Make dataframes with object columns for both test and train data","4e0d21b8":"Since we know that both the Sex and Embarked contains 2 and 3 labels respectively however we are intrested in Name and Ticket features.","680aa394":"We now have obtained all the categorical variables so lets perform one_hot encoding to our data","bdfeab56":"We have filled in the missing data for test data.","e5035f12":"I have to transfrom the independent varaibles and hence for that I have iterated over the columns of test data since it is free of dependent variable, I could have iterated over any of train or original df but later part I will have to drop Survived column so its multi-task.","027e71ae":"**Age Distribution:** The ship contains passengers who are just few months old to more than 80 years of age. However most of the passengers are in the range of late teens to early thirties.","40003422":"Heatmap shows correlation between the numerial features","59ca67ee":"**EDA(Exploratory Data Analysis)**: Lets combine all the data to get the proper insights of data. gender_sub contains the target variable \"Survived\" for test data so lets concatenate them.","21464f62":"The pie-plot shows that 69.93% embarked from Southampton, 9.41% from Queenstown and 20.66% from Cherbourg","a630b969":"The Fare description shows that the mean of 33.2 and median of 14.45 so there is a vast difference so for this lets fill the fare using median.","00085cb4":"Displaying the shapes of all files.\n\nThe train contains 12 features and test with 11 features because test does not contain target variable.","942a225a":"Countplot shows that maximum of the passengers didnt had any number of parents\/Children, in all class of both gender","99724714":"Both the data contains missing data for Age column, so lets fill the age null values","30e58029":"The Ticket are unique for a person or a group so it wont be of any use so lets drop Ticket column","58cc4edf":"Both the dataframes will contain features with object features","e0ecccdf":"The test data still contains missing values in Fare column so lets try to plot a kde plot of original Dataframe(df) Fare.","ecf165bc":"Lets fit our train data For hypertuning","bab7538e":"Our data contains 1309 entries and 12 columns","cfcb41b0":"Lets make a submission.csv file for an output","f69caa5c":"As expected we got a mean of 29.88 and median of 28 so less difference, However for this model let me use medain value i.e 28 for imputing missing age values in both the dataset.","ab05ca7f":"Displaying the information and Description of train and test data","63b94532":"The box plot shows passengers embarked from Southampton and Cherbourg paid higher Fare than ones embarked from Queenstown, which maybe because they booked in a first class or else they might have to travel longer distance from the destination. And moreover females embarked form Cherbourg have survived more.","a54a5d8b":"For imputing Age in both the data lets visualize and consider the age distribution of our original data so it will give us the proper values needed to impute in those missing fields","5aade1b3":"Since now all our categorical features have numerical datatypes so lets combine them with our numerical dataframes","43596a96":"From the unique value counts for all titles its noticed that Mr,Miss and Mrs have the highest number and rest with countable numbers","6d58d1fb":"The countplot shows that Female survied far more than Males, in the ship sink","d92f3a94":"Lets clean the dataframes with object columns","aaa25927":"**Model Building:** I have used logisticRegression for this model","c59fdef3":"**Preprocessing:**\nI have used StandardScaler to scale our data. \nI will scale a values in a way that the mean is 0 and unit variance","b7dba452":"So what i had done is I will transfrom into categorical variable in such a way that the dataset contains categorical vairable of Mr, Mrs, Miss and Others ","99cbe660":"We have PassenderId on both the dataframes which we can drop becase it is a unique values given to individual passengers, and it wont hamper our model performance","18c80122":"store columns of datatype object in cat_col and int and float in num_col","9be0d6cc":"I have given a choice values for diffrent parameters, GridSearch will search over the best parameters and return us with best parameters.","b19a8840":"Lets perform Hypertuing for LogisticRegression to obtain the best parameters that will well fit our model","07163873":"You can directly your independent varaible as train_final however I have built a dataframe for my convinience","db489f2f":"Lets Evaluate our model uising the metrics from sci-kit learn","c5041eab":"Display the first 3 entiries of data","ce7c4580":"Our data does not contains any null values however its not yet ready to be fed into our Model, and contains mixed datatypes so lets try to segregate them and transform them into machine understandable form","197e6c67":"Now lets obtain a traning dataframe by combining train target variable \"Survived\" with the train_final dataframe.","7c4251a8":"Create model with the best parameters obtained above","2b42e408":"Splitting data into X & y variable","ecbee19e":"Display the shape","94f97671":"The plot shows a normal distribution of age, so the difference between the mean and median would be very mininal so we can choose either of the two values to impute the missing values. ","de1f0427":"# Problem Approach:\n\n**Data Loading:** \nThis notebook have three csv files namely **train.csv** for training our model, **test.csv** for model testing without the target variable in our case(Survived-which are one-hot encoded 1-Survived and 0-Deceased). The target variable of test.csv are however stored in **gender_submission.csv** which helps evaluate our model.\n\n**EDA(Exploratory Data Analysis):**\nThis part of Notebook will help us get the detailed insights of Data. To get proper instights of data its important that we scan through all the data despite doing seperately for train & test data.\nSo I have concatenated the gender_submission.csv Survived column with the test data, which is further concatenated with train.csv file to get the original data.\n\n**Data Cleaning:**\nAfter exploring the data, I checked if the data contains any null values and imputed the null values with desired values. I performed one_hot encoding in our categorical variables and converted the data in numerical forms which can be understood well by our ML algorithm.\n\n**Preprocessing**:\nOnce the data is prepared in the numerical form I have scaled the independent variable using StandardScaler from scikit-learn.\n\n**Model Building:**\nFor this particular problem I have used LogisticRegression for classification. LogisticRegression have many parameters which when tuned at right values gives us better generalization, thus for this purpose I have used GridSearchCV for hypertuning. Which then the model is fitted onto our model with the best parameters obtained while hypertuning.\n\n**Model Evaluation:**\nThe model is tested on our test data which has been preprocessed earlier. And confusion matrix & accuracy score metrics from scikit-learn for final evaluation.","5f0d06f4":"Create column Title in both the train and test dataset","ae4e45b6":"Lets see the accuracy score of the model"}}