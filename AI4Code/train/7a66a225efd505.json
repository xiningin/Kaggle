{"cell_type":{"74733ffa":"code","60512369":"code","cf99b601":"code","abdb8dc5":"code","182b39f4":"code","cdff7f8e":"code","56fa4b90":"code","559a24eb":"code","e1f5f725":"code","f0019c18":"code","094b971b":"code","3c34483b":"code","5094585c":"code","4b1d71a0":"code","c3b2e0ce":"code","933f8604":"code","7ca686eb":"code","1ba13dbb":"code","88764838":"code","b33f2ef9":"code","9802aef9":"code","31d8d749":"code","dfa4198e":"markdown","3d0e44c2":"markdown","130d3426":"markdown","23c48644":"markdown"},"source":{"74733ffa":"!apt install aptitude -y\n!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n!pip install mecab-python3==0.996.6rc2\n\n!pip install unidic-lite\n\n!pip install neologdn","60512369":"import re\nimport gc\nimport os\nimport json\nimport pickle\nimport random\n\nimport pandas as pd\nimport numpy as np\n\nimport tqdm\nfrom matplotlib import pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nfrom torch.nn import functional as F\n\nimport MeCab\nimport neologdn\nfrom transformers import *\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GroupKFold\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndevice = torch.device('cuda')","cf99b601":"!ls \/kaggle\/input\/aio-my-prediction","abdb8dc5":"class config:\n    MODEL_DIR = \"\/kaggle\/input\/aio-my-prediction\"\n    MODELS = [\n        \"model_2.bin\",  # LB BEST\n        \"model_2_very_hard_best_prev.bin\",\n        #\"model_2_dev1_best.bin\",\n        \"model_2_dev2_best.bin\",  # dev1,2 avg BEST\n        #\"model_2_dev2_best_sigmoid.bin\",\n        \"model_2_dev1_train.bin\",\n        \"model_2_dev2_train.bin\",\n    ]\n    DATA_DIR1 = \"\/kaggle\/input\/aio-make-token-ids-selection\"\n    DATA_DIR2 = \"\/kaggle\/input\/aio-make-tokenid-with\"\n    SEED = 0\n    MODEL_TYPE = \"cl-tohoku\/bert-base-japanese-whole-word-masking\"\n    TOKENIZER = BertJapaneseTokenizer.from_pretrained(MODEL_TYPE)\n    MAX_LEN = 512","182b39f4":"with open(f\"{config.DATA_DIR1}\/dev1.pkl\", \"rb\") as f:\n    dev1_1 = pickle.load(f)\nwith open(f\"{config.DATA_DIR1}\/dev2.pkl\", \"rb\") as f:\n    dev2_1 = pickle.load(f)\nwith open(f\"{config.DATA_DIR2}\/dev1.pkl\", \"rb\") as f:\n    dev1_2 = pickle.load(f)\nwith open(f\"{config.DATA_DIR2}\/dev2.pkl\", \"rb\") as f:\n    dev2_2 = pickle.load(f)\ndf_dev1_questions = pd.read_json(f'{config.DATA_DIR1}\/dev1_questions.json', orient='records', lines=True)\ndf_dev2_questions = pd.read_json(f'{config.DATA_DIR1}\/dev2_questions.json', orient='records', lines=True)","cdff7f8e":"class BertForAIO(nn.Module):\n    def __init__(self, activate=\"softmax\"):\n        super(BertForAIO, self).__init__()\n\n        bert_conf = BertConfig(config.MODEL_TYPE)\n        bert_conf.output_hidden_states = True\n        bert_conf.vocab_size = config.TOKENIZER.vocab_size\n\n        self.n_use_layer = 1\n        self.dropout_sample = 5\n        self.activate = activate\n\n        self.bert = AutoModel.from_pretrained(config.MODEL_TYPE, config=bert_conf)\n        \n        self.dropout = nn.Dropout(0.2)\n        n_weights = bert_conf.num_hidden_layers + 1\n        weights_init = torch.zeros(n_weights).float()\n        weights_init.data[:-1] = -3\n        self.layer_weights = torch.nn.Parameter(weights_init)\n        \n        self.dense1 = nn.Linear(bert_conf.hidden_size*self.n_use_layer, bert_conf.hidden_size*self.n_use_layer)\n        self.dense2 = nn.Linear(bert_conf.hidden_size*self.n_use_layer, bert_conf.hidden_size*self.n_use_layer)\n        \n        self.dropouts = nn.ModuleList([nn.Dropout(0.2) for _ in range(self.dropout_sample)])\n    \n        self.fc = nn.Linear(bert_conf.hidden_size*self.n_use_layer, 1)\n\n\n    def forward(self, ids, mask, token_type_ids):\n        n_choice = ids.shape[1]\n        \n        ids = ids.view(-1, ids.size(-1))\n        mask = mask.view(-1, mask.size(-1))\n        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n\n        _, _, h = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n\n        cat_output = torch.stack([self.dropout(layer[:, 0, :]) for layer in h], dim=2)\n        if self.activate == \"softmax\":\n            cat_output = (torch.softmax(self.layer_weights, dim=0) * cat_output).sum(-1)\n        elif self.activate == \"sigmoid\":\n            cat_output = (torch.sigmoid(self.layer_weights) * cat_output).sum(-1)\n\n        cat_output = self.dense1(cat_output)\n        cat_output = nn.ReLU()(cat_output)\n        cat_output = self.dense2(cat_output)\n        cat_output = nn.ReLU()(cat_output)\n\n        logits = sum([self.fc(dropout(cat_output)) for dropout in self.dropouts])\/self.dropout_sample\n\n        logits = logits.view(-1, n_choice)\n\n        return logits","56fa4b90":"models = []\nfor _m in config.MODELS:\n    print(_m)\n    if \"sigmoid\" in _m:\n        print(\"  to sigmoid\")\n        model = BertForAIO(\"sigmoid\")\n    else:\n        model = BertForAIO()\n    model.load_state_dict(torch.load(f\"{config.MODEL_DIR}\/{_m}\", map_location=torch.device('cpu')))\n    model.to(device)\n    model.eval()\n    models.append(model)\n    del model\n    gc.collect()\n\nprint(models[0].activate, models[1].activate, models[2].activate, models[3].activate, models[4].activate)\nprint(\"*** model load complete ***\")","559a24eb":"preds_dfs = []\nfor idx in tqdm.tqdm_notebook(range(len(dev1_1))):\n    row = df_dev1_questions.iloc[idx]\n    ans_idx = row[\"answer_candidates\"].index(row[\"answer_entity\"])\n    preds = []\n    \n    d = dev1_1[idx]\n    input_ids = torch.tensor(d[\"input_ids\"]).unsqueeze(0).to(device)\n    attention_mask = torch.tensor(d[\"attention_mask\"]).unsqueeze(0).to(device)\n    token_type_ids = torch.tensor(d[\"token_type_ids\"]).unsqueeze(0).to(device)\n    \n    for model in models[:2]:\n        with torch.no_grad():\n            pred = model(input_ids, attention_mask, token_type_ids)\n        pred = pred.cpu().detach()[0]\n        preds.append(pred)\n\n    d = dev1_2[idx]\n    input_ids = torch.tensor(d[\"input_ids\"]).unsqueeze(0).to(device)\n    attention_mask = torch.tensor(d[\"attention_mask\"]).unsqueeze(0).to(device)\n    token_type_ids = torch.tensor(d[\"token_type_ids\"]).unsqueeze(0).to(device)\n    \n    for model in models[2:]:\n        with torch.no_grad():\n            pred = model(input_ids, attention_mask, token_type_ids)\n        pred = pred.cpu().detach()[0]\n        preds.append(pred)\n\n    # grep\n    preds = torch.stack(preds).T.tolist()\n    preds_df = pd.DataFrame(preds, columns=config.MODELS)\n    preds_df[\"target\"] = np.eye(20)[ans_idx].tolist()\n    preds_df[\"qid\"] = row[\"qid\"]\n    \n    preds_dfs.append(preds_df)\n\ndev1_preds_df = pd.concat(preds_dfs).reset_index(drop=True)\n\ndev1_preds_df.to_csv(\"dev1_preds.csv\", index=None)\ndev1_preds_df.head()","e1f5f725":"acc_lst = []\nfor qid, df in dev1_preds_df.groupby(\"qid\"):\n    y_true = df[\"target\"].values.argmax()\n    \n    _acc = []\n    for m_name in config.MODELS:\n        y_pred = df[m_name].values.argmax()\n        _acc.append(y_true==y_pred)\n    acc_lst.append(_acc)\n    \npd.DataFrame(zip(config.MODELS, np.array(acc_lst).mean(0)), columns=[\"model\", \"accuracy\"])","f0019c18":"X = dev1_preds_df.drop([\"target\", \"qid\"], axis=1).values\ny = dev1_preds_df[\"target\"].values\ngroups = dev1_preds_df[\"qid\"].values\n\noof = np.zeros(len(y))\ngroup_kfold = GroupKFold(n_splits=5)\nfor fold, (fit_i, val_i) in enumerate(group_kfold.split(X, y, groups)):\n    \n    x_fit = X[fit_i, :]\n    y_fit = y[fit_i]\n    x_val = X[val_i, :]\n    y_val = y[val_i]\n\n    lr_model = LogisticRegression()\n    lr_model.fit(x_fit, y_fit)\n    y_pred = lr_model.predict_proba(x_val)\n    \n    oof[val_i] = y_pred[:, 1]\n    \naccs = []\nfor idx in range(len(oof)\/\/20):\n    h, t = int(idx*20), int((idx+1)*20)\n    y_pres = oof[h:t].argmax()\n    y_true = y[h:t].argmax()\n    accs.append(y_true==y_pres)\n    \nsum(accs)\/len(accs)","094b971b":"preds_dfs = []\nfor idx in tqdm.tqdm_notebook(range(len(dev2_1))):\n    row = df_dev2_questions.iloc[idx]\n    ans_idx = row[\"answer_candidates\"].index(row[\"answer_entity\"])\n    preds = []\n    \n    d = dev2_1[idx]\n    input_ids = torch.tensor(d[\"input_ids\"]).unsqueeze(0).to(device)\n    attention_mask = torch.tensor(d[\"attention_mask\"]).unsqueeze(0).to(device)\n    token_type_ids = torch.tensor(d[\"token_type_ids\"]).unsqueeze(0).to(device)\n\n    for model in models[:2]:\n        with torch.no_grad():\n            pred = model(input_ids, attention_mask, token_type_ids)\n        pred = pred.cpu().detach()[0]\n        preds.append(pred)\n\n    d = dev2_2[idx]\n    input_ids = torch.tensor(d[\"input_ids\"]).unsqueeze(0).to(device)\n    attention_mask = torch.tensor(d[\"attention_mask\"]).unsqueeze(0).to(device)\n    token_type_ids = torch.tensor(d[\"token_type_ids\"]).unsqueeze(0).to(device)\n\n    for model in models[2:]:\n        with torch.no_grad():\n            pred = model(input_ids, attention_mask, token_type_ids)\n        pred = pred.cpu().detach()[0]\n        preds.append(pred)\n        \n    # grep\n    preds = torch.stack(preds).T.tolist()\n    preds_df = pd.DataFrame(preds, columns=config.MODELS)\n    preds_df[\"target\"] = np.eye(20)[ans_idx].tolist()\n    preds_df[\"qid\"] = row[\"qid\"]\n    \n    preds_dfs.append(preds_df)\n    \ndev2_preds_df = pd.concat(preds_dfs).reset_index(drop=True)\n\ndev2_preds_df.to_csv(\"dev2_preds.csv\", index=None)\ndev2_preds_df.head()","3c34483b":"acc_lst = []\nfor qid, df in dev2_preds_df.groupby(\"qid\"):\n    y_true = df[\"target\"].values.argmax()\n    \n    _acc = []\n    for m_name in config.MODELS:\n        y_pred = df[m_name].values.argmax()\n        _acc.append(y_true==y_pred)\n    acc_lst.append(_acc)\n    \npd.DataFrame(zip(config.MODELS, np.array(acc_lst).mean(0)), columns=[\"model\", \"accuracy\"])","5094585c":"X = dev2_preds_df.drop([\"target\", \"qid\"], axis=1).values\ny = dev2_preds_df[\"target\"].values\ngroups = dev2_preds_df[\"qid\"].values\n\noof = np.zeros(len(y))\ngroup_kfold = GroupKFold(n_splits=5)\nfor fold, (fit_i, val_i) in enumerate(group_kfold.split(X, y, groups)):\n    \n    x_fit = X[fit_i, :]\n    y_fit = y[fit_i]\n    x_val = X[val_i, :]\n    y_val = y[val_i]\n\n    lr_model = LogisticRegression()\n    lr_model.fit(x_fit, y_fit)\n    y_pred = lr_model.predict_proba(x_val)\n    \n    oof[val_i] = y_pred[:, 1]\n    \naccs = []\nfor idx in range(len(oof)\/\/20):\n    h, t = int(idx*20), int((idx+1)*20)\n    y_pres = oof[h:t].argmax()\n    y_true = y[h:t].argmax()\n    accs.append(y_true==y_pres)\n    \nsum(accs)\/len(accs)","4b1d71a0":"all_dev = pd.concat([dev1_preds_df, dev2_preds_df], axis=0).reset_index(drop=True)\nall_dev.shape","c3b2e0ce":"X = all_dev.drop([\"target\", \"qid\"], axis=1).values\ny = all_dev[\"target\"].values\ngroups = all_dev[\"qid\"].values\n\nlr_models = []\noof = np.zeros(len(y))\ngroup_kfold = GroupKFold(n_splits=5)\nfor fold, (fit_i, val_i) in enumerate(group_kfold.split(X, y, groups)):\n    \n    x_fit = X[fit_i, :]\n    y_fit = y[fit_i]\n    x_val = X[val_i, :]\n    y_val = y[val_i]\n\n    lr_model = LogisticRegression()\n    lr_model.fit(x_fit, y_fit)\n    y_pred = lr_model.predict_proba(x_val)\n    \n    oof[val_i] = y_pred[:, 1]\n    lr_models.append(lr_model)\n    \naccs = []\nfor idx in range(len(oof)\/\/20):\n    h, t = int(idx*20), int((idx+1)*20)\n    y_pres = oof[h:t].argmax()\n    y_true = y[h:t].argmax()\n    accs.append(y_true==y_pres)\n    \nsum(accs)\/len(accs)","933f8604":"with open(\"stacking_lr_model.pkl\", \"wb\") as f:\n    pickle.dump(lr_models, f)","7ca686eb":"without_dev1_train_col = [\n        \"model_2.bin\", # LB best\n        \"model_2_very_hard_best_prev.bin\",\n        \"model_2_dev2_best.bin\",  # dev1, dev2 AVG best\n        \"model_2_dev2_train.bin\",\n]\nwithout_dev2_train_col = [\n        \"model_2.bin\", # LB best\n        \"model_2_very_hard_best_prev.bin\",\n        \"model_2_dev2_best.bin\", # dev1, dev2 AVG best\n        \"model_2_dev1_train.bin\",\n]","1ba13dbb":"X = dev1_preds_df[without_dev1_train_col].values\ny = dev1_preds_df[\"target\"].values\ngroups = dev1_preds_df[\"qid\"].values\n\nlr_models_without_dev1_train = []\noof = np.zeros(len(y))\ngroup_kfold = GroupKFold(n_splits=5)\nfor fold, (fit_i, val_i) in enumerate(group_kfold.split(X, y, groups)):\n    \n    x_fit = X[fit_i, :]\n    y_fit = y[fit_i]\n    x_val = X[val_i, :]\n    y_val = y[val_i]\n\n    lr_model = LogisticRegression()\n    lr_model.fit(x_fit, y_fit)\n    y_pred = lr_model.predict_proba(x_val)\n    \n    oof[val_i] = y_pred[:, 1]\n    lr_models_without_dev1_train.append(lr_model)\n    \naccs = []\nfor idx in range(len(oof)\/\/20):\n    h, t = int(idx*20), int((idx+1)*20)\n    y_pres = oof[h:t].argmax()\n    y_true = y[h:t].argmax()\n    accs.append(y_true==y_pres)\n    \nsum(accs)\/len(accs)","88764838":"with open(\"stacking_lr_model_without_dev1_train.pkl\", \"wb\") as f:\n    pickle.dump(lr_models_without_dev1_train, f)","b33f2ef9":"X = dev2_preds_df[without_dev2_train_col].values\ny = dev2_preds_df[\"target\"].values\ngroups = dev2_preds_df[\"qid\"].values\n\nlr_models_without_dev2_train = []\noof = np.zeros(len(y))\ngroup_kfold = GroupKFold(n_splits=5)\nfor fold, (fit_i, val_i) in enumerate(group_kfold.split(X, y, groups)):\n    \n    x_fit = X[fit_i, :]\n    y_fit = y[fit_i]\n    x_val = X[val_i, :]\n    y_val = y[val_i]\n\n    lr_model = LogisticRegression()\n    lr_model.fit(x_fit, y_fit)\n    y_pred = lr_model.predict_proba(x_val)\n    \n    oof[val_i] = y_pred[:, 1]\n    lr_models_without_dev2_train.append(lr_model)\n    \naccs = []\nfor idx in range(len(oof)\/\/20):\n    h, t = int(idx*20), int((idx+1)*20)\n    y_pres = oof[h:t].argmax()\n    y_true = y[h:t].argmax()\n    accs.append(y_true==y_pres)\n    \nsum(accs)\/len(accs)","9802aef9":"with open(\"stacking_lr_model_without_dev2_train.pkl\", \"wb\") as f:\n    pickle.dump(lr_models_without_dev2_train, f)","31d8d749":"!ls","dfa4198e":"# Dev1","3d0e44c2":"## for train dev","130d3426":"# Stacking","23c48644":"# Dev2"}}