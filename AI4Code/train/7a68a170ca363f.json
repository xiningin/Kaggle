{"cell_type":{"bef6d259":"code","6991f07e":"code","383f1a09":"code","32fb9052":"code","1577e10b":"code","51d9f6fc":"code","e8873d89":"code","16052f2e":"code","d298cfd5":"code","3d050617":"code","ff601027":"code","5b2eb0c2":"code","dfa8ba12":"code","6641bee8":"code","8ceaa1d0":"code","3558165a":"code","1fa00a61":"code","10aa86d9":"code","0f09819b":"code","bb9340bc":"code","b1431520":"code","aaccfc6d":"code","df4b690e":"code","f7ba28de":"code","283a6514":"markdown","df0d0359":"markdown","7f5d4bcd":"markdown","648093bd":"markdown","d8d704a4":"markdown","53155b98":"markdown","8b5c968e":"markdown","ca8105a7":"markdown","dd885880":"markdown","c74e3930":"markdown","538612fa":"markdown","73f8019d":"markdown","3040b2e3":"markdown","8ffb6d02":"markdown","a91a3e44":"markdown","3751af04":"markdown","27915814":"markdown"},"source":{"bef6d259":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# plotly\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# import warnings\nimport warnings\n# filter warnings\nwarnings.filterwarnings('ignore')\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n# Any results you write to the current directory are saved as output.\nimport os\nprint(os.listdir(\"..\/input\"))","6991f07e":"test = pd.read_csv('..\/input\/test.csv')  # importing data in csv format with pandas library\ntrain = pd.read_csv('..\/input\/train.csv')\ntrain.head()       ","383f1a09":"train.info()","32fb9052":"### Splitting Data & Normalization ###\n\nx = train.drop(['label'], axis=1).values\/255 # make input values numpy array, then normalize by dividing with 255.\ny = train.label\nx_test = test.values\/255\n\n","1577e10b":"### Let's check the first 9 handwritten images ###\nfor i in range(9):   \n    \n    plt.subplot(3,3,i+1)\n    plt.imshow(x[i].reshape(28,28), cmap='gray')\n    plt.axis('off') \n","51d9f6fc":"### Value Counts of Digits & Countplot ###\nplt.figure(figsize=(15,5))\nsns.countplot(y, palette='icefire')\nplt.title('Counts of Each Class')\nplt.xlabel('Classes')\nplt.ylabel('Counts')\ny.value_counts()","e8873d89":"### Reshape ###\nx = x.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)","16052f2e":"from keras.utils.np_utils import to_categorical\ny = to_categorical(y, num_classes=10)\ny.shape","d298cfd5":"### Train Test Split ###\nfrom sklearn.model_selection import train_test_split\nX_train, x_validation, y_train, y_validation = train_test_split(x, y, test_size = 0.1, random_state = 42)","3d050617":"### Importing Neural Network Libraries ###\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D","ff601027":"### Convolution Part ###\n\n# Assign the framework as \"model\"\nmodel = Sequential()\n\n# I choose the kernel(filter) as (5,5) matrix with 32 feature maps.\n# ReLU activation function is a better choice than tanh to avoid vanishing gradient problem unless if you use \"batch normalization\".\n# Like in feed-forward neural networks, we need to indicate the input shape at the beginning.\nmodel.add(Conv2D(32, (5,5), padding ='Same', activation ='relu', input_shape =(28,28,1)))\n\n# We use \"max pooling\" method as (2,2) matrix to keep more information after having a smaller matrix in convolution operation\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\n# dropout helps avoding overfitting problem. Generally 0.2 - 0.3 are the best choices\nmodel.add(Dropout(0.25))\n\n# Second convolution layer has 32 feature maps again, but our kernel is an (3,3) matrix now, since we have reduced parameters.\nmodel.add(Conv2D(32, (3,3), padding ='Same', activation ='relu'))\n\n# Max pooling again.\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n\n# Dropout before fully connected layer\nmodel.add(Dropout(0.25))","5b2eb0c2":"### Fully Connected Layer ###\n\n# Fully connected layer is where the classification will be done, so we flatten it firstly since we have multiple dimensions.\nmodel.add(Flatten())\n# 256 nodes in first layer\nmodel.add(Dense(256, activation = \"relu\"))\n# Dropout to avoid overfitting\nmodel.add(Dropout(0.25))\n# Output layer with 10 different labels.\nmodel.add(Dense(10, activation = \"softmax\"))\n\n# Compile & Fit Model\n\n# Since we have multiple classes, we need to use categorical cross entropy. I used adam as an optimizer.\nmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n\n# Fit the model with 392 batch size which is almost half of the pixels, with verbose = 2 showing only a line while in progress and 5 epochs.\n# Model parameters are recorded in \"history\" for further uses.\nhistory = model.fit(X_train, y_train, batch_size=392, verbose = 2, validation_data=(x_validation, y_validation), epochs = 5)\n","dfa8ba12":"# Demonstrating Predicted Classes\n\npredicted_classes = pd.DataFrame(model.predict(test))\nplt.figure(figsize=(10,10)) # Setting the figure size\nfor i in range(16):\n    plt.subplot(4,4,i+1)\n    plt.imshow(x_test[i].reshape(28,28), cmap='gray')\n    plt.title(predicted_classes.iloc[i].idxmax(axis=1)) # \"idmax\" of pandas library gives us the column name(which are our outputs) of the maximum value in a row\n    plt.axis('off') # don't show the axis\n    plt.axis('off')","6641bee8":"loss = go.Scatter(y= history.history['val_loss'], x=np.arange(0,5), mode = \"lines+markers\", name='Test Loss') \naccuracy = go.Scatter(y= history.history['val_acc'], x=np.arange(0,5), mode = \"lines+markers\", name='Test Accuracy') \nlayout = dict(title = 'Test Loss & Accuracy Visualization',\n              xaxis= dict(title= 'Epochs',ticklen= 5,zeroline= True),\n              yaxis= dict(title= 'Loss & Accuracy',ticklen= 5,zeroline= True))\ndata = [loss, accuracy]\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","8ceaa1d0":"# Importing confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\n# Since we don't have the labels for \"test\" data like in real life, we will only create a confusion matrix of validation values.\n\n# Predict test values.\ny_predicted = model.predict(x_validation)\n\n# Find the column indices of maximum values which corresponds to predicted digits.\n# An alternative method to do this, as it's done in subplots above, to convert the matrix to dataframe first, then find maximum column indices with \"idxmax\".\ny_predicted = np.argmax(y_predicted, axis = 1) \ny_true = np.argmax(y_validation, axis = 1) \n\n# Create the confusion matrix.\nconfusion__matrix = confusion_matrix(y_true, y_predicted) \n\n# Plot it!\nplt.figure(figsize=(10,10))\nsns.heatmap(confusion__matrix, annot=True, linewidths=0.2, cmap=\"Paired\",linecolor=\"black\",  fmt= '.1f')\nplt.xlabel(\"Predicted Labels\", fontsize=15)\nplt.ylabel(\"True Labels\", fontsize=15)\nplt.title(\"Confusion Matrix\", color = 'red', fontsize = 20)\nplt.show()","3558165a":"cnn= pd.DataFrame(history.history).iloc[4]\ncnn_accuracies = pd.DataFrame(cnn).T\n\ncnn_accuracies.drop(['loss', 'val_loss'],inplace=True,axis=1)\n\nann_accuracies = {'acc':[0.8913], 'val_acc': [0.9075]}\nann_accuracies = pd.DataFrame(ann_accuracies)\n\ndata = pd.concat([ann_accuracies,cnn_accuracies], ignore_index=True)\ndata['methods']= ['ANN','CNN']\ndata","1fa00a61":"### Comparing Accuracies via pandas.DataFrame.barplot ###\ndata.plot(x='methods', y=['val_acc','acc'], kind='bar', figsize=(10,7))\nplt.xticks(rotation = 0)\nplt.show()\n\n#sns.barplot(data=dataa, x='methods', y='acc', color='yellow' alpha=1)\n#sns.barplot(data=dataa, x='methods', y='val_acc', color='red', alpha = 05)","10aa86d9":"### Comparing Accuracies via Bar Charts of Plotly ###\n\nbar1 = go.Bar(\n                x = data.methods,\n                y = data.acc,\n                name = 'Train'\n                )\n\nbar2 = go.Bar(\n                x = data.methods,\n                y = data.val_acc,\n                name = 'Validation'\n                )\n\ndata2 = [bar1, bar2]\nlayout = go.Layout(barmode = 'group')\nfig = go.Figure(data = data2, layout = layout)\niplot(fig)","0f09819b":"from sklearn.metrics import precision_recall_curve\nclasses = y.shape[1]\n\nprecision = dict()\nrecall = dict()\ny_predict = model.predict(x_validation)\nfor i in range(classes):\n    precision[i], recall[i], _ = precision_recall_curve(y_validation[:, i], y_predict[:, i])\n\n","bb9340bc":"colors = ['red', 'blue', 'green', 'black', 'cyan', 'purple', 'pink', 'navy', 'yellow', 'brown']\nlines = []\nlabels = []\nplt.figure(figsize=(10,10))\n\nfor i in range(classes):\n    plt.plot(recall[i], precision[i], color=colors[i])\n    labels.append('Precision-recall for class {0}'.format(i+1))\n    \nplt.ylim([0.0, 1.03])\nplt.xlim([0.0, 1.03])\nplt.xlabel('Recall',fontsize=15)\nplt.ylabel('Precision', fontsize = 15)\nplt.title('Recall vs Precision',fontsize = 20)\nplt.legend(labels, loc=(.3, .3), prop={'size':12})\nplt.show()","b1431520":"colors = ['red', 'blue', 'green', 'black', 'cyan', 'purple', 'pink', 'navy', 'yellow', 'brown']\nlines = []\nlabels = []\n\nplt.figure(figsize=(10,30))\n\nfor i in range(classes):\n    plt.subplot(5,2,i+1)\n    labels.append('Precision-recall for class {}'.format(i+1))\n    plt.plot(recall[i], precision[i], color=colors[i], label=labels[i])\n    plt.legend(loc=(.1, .3), prop={'size':12})\n    plt.title('Class {}'.format(i+1),fontsize = 15)\n    plt.xlabel('Recall',fontsize=10)\n    plt.ylabel('Precision', fontsize = 10)\n\n    \nplt.show()\n","aaccfc6d":"from sklearn.metrics import f1_score\nprint('F1 Score: {}'.format(f1_score(y_true, y_predicted, average='macro')))","df4b690e":"y_predicted = y_predicted.T\ny_true = y_true.T\n\nfrom sklearn.preprocessing import label_binarize\ny_true_roc = label_binarize(y_true,classes=[0,1,2,3,4,5,6,7,8,9])\ny_pred_roc= label_binarize(y_predicted, classes=[0,1,2,3,4,5,6,7,8,9])\n\nfpr = {} # false positive rate\ntpr = {} #  true positive rate\nroc_auc = {}\nfrom sklearn.metrics import roc_curve, auc\nfor i in range(y_true_roc.shape[1]):\n    fpr[i], tpr[i], _ = roc_curve(y_pred_roc[:, i], y_true_roc[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])","f7ba28de":"colors = ['red', 'blue', 'green', 'black', 'cyan', 'purple', 'pink', 'navy', 'yellow', 'brown']\nlines = []\nlabels = []\n\nplt.figure(figsize=(17,40))\nfor i in range(y_true_roc.shape[1]):\n    plt.subplot(5,2,i+1)\n    labels.append('ROC curve for class {} & Area = {:f}'.format(i+1, roc_auc[i])) \n    plt.plot(fpr[i], tpr[i], color = colors[i],label=labels[i])\n    plt.legend(loc=(.1, .3), prop={'size':15})\n    plt.ylim([0.0, 1.03])\n    plt.xlim([0.0, 1.03])\n    plt.xlabel('False Positive Rate', fontsize=15)\n    plt.ylabel('True Positive Rate', fontsize =15)\n    plt.title('ROC curves & AUC scores'.format(i+1), fontsize=15)\nplt.show()","283a6514":"* Below, CNN accuracies and ANN accuracies of my previous kernel will be concatenated in \"data\" dataframe in order that we can bar charts grouped side by side in a plot.","df0d0359":" # **2. PREPROCESSING**\n* Unlike feed-forward neural networks, CNN needs images in 3D format. So it's required to reshape the images.\n*  -1 : sample size;\n*  28, 28 : pixels of the images\n*  1 indicates that there is one color channel which is \"grayscale\". If it'd be 3, then we would have 3 channels as \"Red\", \"Green\" and \"Blue\".","7f5d4bcd":"* Since the preceeding plots cannot be observed clearly, let's see all of them in subplots.","648093bd":"- To find ROC curve in multiclass problems, we need binarized labels.","d8d704a4":"# Test Loss & Accuracy Visualization\n* The fit() method of Keras model returns a \"**history**\" object. The history.history attribute is a dictionary recording training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values.\n* In below figure, the visualization of change in validation loss and validation accuracy is shown. We can learn from this graph how many epochs are enough for our model where it started not to show a significant decreasing in loss after a certain epoch. Since the model parameters are assigned as random, it will change in every run of code.\n* You can also see the change in the loss and the accuracy interactively by holding mouse on scatter points","53155b98":"* Since we have 10 different labels, our labels must be encoded one-hot.\n* If you decide to use some model selection methods like \"Grid Search\", \"Cross Validation\" etc. then it's ok for you to apply only label encoding due to the difference of return types of predict method.","8b5c968e":" # **4. PLOTTING & MODEL EVALUATION**\n* Since we don't have any labels on our real life test data, we can't find the accuracy. Then we take a look at our first 16 predictions.","ca8105a7":"# Precision - Recall Curve & F1 Score\n* Now it's time to evaluate our model. We use \"Precision-Recall\" metric for evaluating classifier models. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).\n    * Precision is defined as the number of true positives(TP) over the number of true positives(TP) plus the number of false positives(FP). = TP \/ (TP + FP)\n    * Recall is defined as the number of true positives(TP) over the number of true positives(TP) plus the number of false negatives(FN).          = TP \/ (TP + FN)\n    * The relationship between recall and precision can be observed in the stairstep area of the plot which will be done below.\n    * You can think of positive(P) and negatives(N) as an evaluation between just 2 categories in \"confusion matrix\" we've plotted in heatmap above. But since we have 10 different labels in our model, we will use one-vs-all method by plotting \"precision-recall\" graph.","dd885880":"* You can see the comparison of ANN and CNN accuracies below through pandas plot method.\n* If you prefer to see them in each one bars, you can delete the comment sign #, and see clearly by setting the appropriate colors and opacity(alpha).","c74e3930":" # **3. CREATING MODEL**","538612fa":"**I'd be glad if this kernel could help you, thanks in advance.**\n# **END**\n\n\n\n\n\n\n\n","73f8019d":"# Confusion Matrix\n\n* A confusion matrix gives us the number of correct and incorrect predictions of a classification model compared to the actual outcomes. Size of a confusion matrix is NxN, where N is the number of classes. Performance of such models is commonly evaluated using the data in the matrix.","3040b2e3":"* Plotly is my all time favoite visualization library.\n* Thanks to its interactive feature, you can observe even the smallest decimals corresponding to the points in graphs just by holding your cursor on it.","8ffb6d02":"# **INTRODUCTION**\n* In this kernel, I'll show you my  classification of 10 digits from 0 to 9 with Convolutional Neural Networks.\n* After modeling and results obtained, there will be some plots in which we can observe changes in loss and accuracies of the model which are returned by \"history\" of Keras library.\n* Also there will be some bar plots with \"pandas\" and \"plotly\" comparing the \"Feed-forward Neural Network\" model which is my preceeding kernel with the same dataset and CNN model.\n* Finally, our classifier will be evaluated with **Precision-Recall metric and F1 score** and plotted as well as **ROC curve and AUC score**. Usually, one of those metrics is enough but I want to demonstrate both in my kernel. \n\nLet's start by importing libraries & data.","a91a3e44":"* Eventually, to find a specific score of precision-recall for all labels, we need to calculate F1 score. \n* The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n* Since we have multiple labels, we need to determine a method in \"average\" parameter in which I prefered using \"macro\" which calculate metrics for each label, and find their unweighted mean.\n* Notice that, both arrays including \"true\" and \"predicted\" labels must consist of only label values to be able to evaluate F1 score, NOT in one-hot encoded forms. But if you use 'samples' in average parameter, then the classes must be in one-hot encoded form.","3751af04":"# ROC Curve & AUC Score\n* ROC curves typically feature true positive rate(**TPR**) on the y axis, and false positive rate(**FPR**) on the x axis\n* Ideal point of a ROC curve is on top left where TP rate is 1 and FP rate is 0.\n* AUC score equals the area under the ROC curve. When it equals 1, then the classification is done without any errors.","27915814":" # **1. DATA OVERVIEW**\n* The data I used which is imported as 'train' has 785 columns and 42,000 rows and 'test.csv' file which will be used for testing has 28000 samples. \n* Rows are the number of images of the digits we have in our data.\n* As for the columns, the first column is 'label' column, which contains the given answers, here are some label-encoded digits between 0 - 9 corresponding every images, like in any supervised learning.\n* The rest of the columns are the pixels of our 2D images, whose size is 28x28. Since we use a 2D matrix, it's already converted to 2D by multiplying the pixels with each other.\n\nLet me show you some of the images below:"}}