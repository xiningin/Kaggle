{"cell_type":{"ffc99ac5":"code","2b6ec978":"code","30e2c120":"code","273add53":"code","0785b5f7":"code","f17bdd84":"code","0a763cbf":"code","ca3c17ab":"code","e482e44a":"code","f5efdc96":"code","1e89e321":"code","4e7eea13":"code","243c0ba4":"code","1fa98872":"code","5d026fb5":"code","ce257eff":"code","50668ddf":"code","b1b63e5e":"code","47124eb1":"code","bb340222":"code","81440075":"code","7b3fa8d0":"code","3a040358":"code","723ed52e":"code","159dcbe1":"code","ce17883b":"code","79e59f28":"code","b3244a35":"code","190ff12b":"code","7391d7c5":"code","db2d9ed8":"code","2aa0c74d":"markdown","544d3c82":"markdown","d2f971a4":"markdown","397eca43":"markdown","a7417ae0":"markdown","4592f97c":"markdown","2a7de349":"markdown","72a7bb36":"markdown","5c10ab5b":"markdown","575a68b2":"markdown","14638330":"markdown","2ed4d48c":"markdown","c2cb2d7a":"markdown","4e20c140":"markdown","258790ab":"markdown","e4f99263":"markdown","3405b361":"markdown","bcb6caa6":"markdown","62173024":"markdown","b8fb1fae":"markdown","0cc734f9":"markdown","27b80d97":"markdown"},"source":{"ffc99ac5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2b6ec978":"from PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nfrom keras import preprocessing\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D,Dropout,Dense,Flatten,Conv2DTranspose,BatchNormalization,LeakyReLU,Reshape\nimport tensorflow as tf\n","30e2c120":"import os\ncwd = os.getcwd()\nos.chdir(cwd)\nprint(os.listdir(\"..\/input\"))","273add53":"path_celeb = []\ntrain_path_celeb = \"\/kaggle\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba\/\"\nfor path in os.listdir(train_path_celeb):\n    if '.jpg' in path:\n        path_celeb.append(os.path.join(train_path_celeb, path))","0785b5f7":"new_path=path_celeb[0:50000]\n","f17bdd84":"len(new_path)","0a763cbf":"crop = (30, 55, 150, 175) #croping size for the image so that only the face at centre is obtained\nimages = [np.array((Image.open(path).crop(crop)).resize((64,64))) for path in new_path]\n\nfor i in range(len(images)):\n    images[i] = ((images[i] - images[i].min())\/(255 - images[i].min()))\n    #images[i] = images[i]*2-1  #uncomment this if activation is tanh for generator last layer\n    \nimages = np.array(images) ","ca3c17ab":"train_data=images","e482e44a":"print(path_celeb)","f5efdc96":"len(path_celeb)","1e89e321":"print(train_data.shape)","4e7eea13":"plt.figure(figsize=(10,10))\nfig,ax=plt.subplots(2,5)\nfig.suptitle(\"Real Images\")\nidx=800\n\nfor i in range(2):\n    for j in range(5):\n            ax[i,j].imshow(train_data[idx].reshape(64,64,3))\n            #ax[i,j].set_title(\"Real Image\")\n            \n            idx+=600\n            \nplt.tight_layout()\nplt.show()","243c0ba4":"X_train = train_data","1fa98872":"noise_shape = 100","5d026fb5":"generator=Sequential()\ngenerator.add(Dense(4*4*512,input_shape=[noise_shape]))\ngenerator.add(Reshape([4,4,512]))\ngenerator.add(Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"))\ngenerator.add(LeakyReLU(alpha=0.2))\ngenerator.add(BatchNormalization())\ngenerator.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"))\ngenerator.add(LeakyReLU(alpha=0.2))\ngenerator.add(BatchNormalization())\ngenerator.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\"))\ngenerator.add(LeakyReLU(alpha=0.2))\ngenerator.add(BatchNormalization())\ngenerator.add(Conv2DTranspose(3, kernel_size=4, strides=2, padding=\"same\",\n                                 activation='sigmoid'))","ce257eff":"generator.summary()","50668ddf":"discriminator=Sequential()\ndiscriminator.add(Conv2D(32, kernel_size=4, strides=2, padding=\"same\",input_shape=[64,64, 3]))\ndiscriminator.add(Conv2D(64, kernel_size=4, strides=2, padding=\"same\"))\ndiscriminator.add(LeakyReLU(0.2))\ndiscriminator.add(BatchNormalization())\ndiscriminator.add(Conv2D(128, kernel_size=4, strides=2, padding=\"same\"))\ndiscriminator.add(LeakyReLU(0.2))\ndiscriminator.add(BatchNormalization())\ndiscriminator.add(Conv2D(256, kernel_size=4, strides=2, padding=\"same\"))\ndiscriminator.add(LeakyReLU(0.2))\ndiscriminator.add(Flatten())\ndiscriminator.add(Dropout(0.5))\ndiscriminator.add(Dense(1,activation='sigmoid'))","b1b63e5e":"discriminator.summary()","47124eb1":"GAN =Sequential([generator,discriminator])","bb340222":"discriminator.compile(optimizer='adam',loss='binary_crossentropy')\ndiscriminator.trainable = False","81440075":"GAN.compile(optimizer='adam',loss='binary_crossentropy')","7b3fa8d0":"GAN.layers","3a040358":"GAN.summary()","723ed52e":"epochs = 300  #set epoch according to your training dataset size,i had chosen 50k images hence epochs are high as 300...\nbatch_size = 128","159dcbe1":"D_loss=[] #list to collect loss for the discriminator model\nG_loss=[] #list to collect loss for generator model","ce17883b":"with tf.device('\/gpu:0'):\n for epoch in range(epochs):\n    print(f\"Currently on Epoch {epoch+1}\")\n    \n    # For every batch in the dataset\n    for i in range(X_train.shape[0]\/\/batch_size):\n        \n        if (i)%100 == 0:\n            print(f\"\\tCurrently on batch number {i} of {len(X_train)\/\/batch_size}\")\n            \n        noise=np.random.uniform(-1,1,size=[batch_size,noise_shape])\n        \n        gen_image = generator.predict_on_batch(noise)\n        \n        train_dataset = X_train[i*batch_size:(i+1)*batch_size]\n        #train on real image\n        train_label=np.ones(shape=(batch_size,1))\n        discriminator.trainable = True\n        d_loss1 = discriminator.train_on_batch(train_dataset,train_label)\n        \n        #train on fake image\n        train_label=np.zeros(shape=(batch_size,1))\n        d_loss2 = discriminator.train_on_batch(gen_image,train_label)\n        \n        \n        noise=np.random.uniform(-1,1,size=[batch_size,noise_shape])\n        train_label=np.ones(shape=(batch_size,1))\n        discriminator.trainable = False\n        #train the generator\n        g_loss = GAN.train_on_batch(noise, train_label)\n        D_loss.append(d_loss1+d_loss2)\n        G_loss.append(g_loss)\n        \n         \n    if epoch % 5 == 0:\n        samples = 10\n        x_fake = generator.predict(np.random.normal(loc=0, scale=1, size=(samples,100)))\n\n        for k in range(samples):\n            plt.subplot(2, 5, k+1)\n            plt.imshow(x_fake[k].reshape(64,64,3))\n            plt.xticks([])\n            plt.yticks([])\n\n        \n        plt.tight_layout()\n        plt.show()\n    print('Epoch: %d,  Loss: D_real = %.3f, D_fake = %.3f,  G = %.3f' %   (epoch+1, d_loss1, d_loss2, g_loss))        \nprint('Training is complete')","79e59f28":"noise=np.random.uniform(-1,1,size=[500,noise_shape])","b3244a35":"im=generator.predict(noise)","190ff12b":"for i in range(5):\n plt.figure(figsize=(7,7))   \n for k in range(20):\n            noise=np.random.uniform(-1,1,size=[100,noise_shape])\n            im=generator.predict(noise) \n            plt.subplot(5, 4, k+1)\n            plt.imshow(im[k].reshape(64,64,3))\n            plt.xticks([])\n            plt.yticks([])\n \n plt.tight_layout()\n plt.show()","7391d7c5":"plt.figure(figsize=(10,10))\nplt.plot(G_loss,color='red',label='Generator_loss')\nplt.plot(D_loss,color='blue',label='Discriminator_loss')\nplt.legend()\nplt.xlabel('total batches')\nplt.ylabel('loss')\nplt.title('Model loss per batch')\nplt.show()","db2d9ed8":"import pickle\nPkl_Filename = \"DCGAN.pkl\"  \n\nwith open(Pkl_Filename, 'wb') as file:  \n    pickle.dump(GAN, file)","2aa0c74d":"# **Loss Curve**","544d3c82":"# **Comment if you have any queries or find anything wrong with the code**","d2f971a4":"# **Upvote if you like the kernel**","397eca43":"# **LOSS FUNCTION of simple GAN**\n\nIn the paper that introduced GANs(link below in the reference), the generator tries to minimize the following function while the discriminator tries to maximize it:\n\n![image.png](attachment:image.png)\n\nIn this function:\n\nD(x) is the discriminator's estimate of the probability that real data instance x is real.\n\nEx is the expected value over all real data instances.\n\nG(z) is the generator's output when given noise z.\n\nD(G(z)) is the discriminator's estimate of the probability that a fake instance is real.\n\nEz is the expected value over all random inputs to the generator (in effect, the expected value over all generated fake instances G(z)).\n\nThe generator can't directly affect the log(D(x)) term in the function, so, for the generator, minimizing the loss is equivalent to minimizing log(1 - D(G(z))).","a7417ae0":"approx training time for 300 epochs was somthing little more than 2 hours.","4592f97c":"the dataset have more than 202k images of which only 50k are being selected for the training purpose","2a7de349":"# **Generator**","72a7bb36":"# **Reference**\n\n1. https:\/\/medium.com\/coloredfeather\/generating-human-faces-using-adversarial-network-960863bc1deb blog post\n2. https:\/\/arxiv.org\/abs\/1511.06434  paper on DCGANS published in 2016\n","5c10ab5b":"![download%20%282%29.png](attachment:download%20%282%29.png)","575a68b2":"# **Thanks**","14638330":"# **Discriminator**","2ed4d48c":"# **Deep Convolution Adversarial Networks**","c2cb2d7a":"**Note**  -- the image pixels are normalized dividing each pixel by 255 and then modified to bring within (-1,1) range by multiplying with 2 and substracting 1 since the last layer activation of generator is tanh whose range limits (-1,1)","4e20c140":"the last layer for generator  have sigmoid activation in this version since I was trying between sigmoid and tanh to see which outputs better results. I found sigmoid activated model having better clarity in this case, however I have tried tanh activated generator too so if anyone want to refer to that can look at version 6\/7 of this kernel where I use tanh. In case of using tanh the input pixels should be normalised between (-1,1) unlike sigmoid where it has been normalised between (0,1) .\n","258790ab":"# **Loadind and Preprocessing data**","e4f99263":"# **Architecture of the Model**\n\nThe core to the DCGAN architecture uses a standard CNN architecture on the discriminative model. For the generator, convolutions are replaced with upconvolutions, so the representation at each layer of the generator is actually successively larger, as it mapes from a low-dimensional latent vector onto a high-dimensional image.\n\n\n\nUse batch normalization in both the generator and the discriminator.\n\nRemove fully connected hidden layers for deeper architectures.\n\nUse ReLU activation in generator for all layers except for the output, which uses Tanh.\n\nUse LeakyReLU activation in the discriminator for all layers.","3405b361":"# **Training**","bcb6caa6":"# **Output Visualization**","62173024":"![dcgan.png](attachment:dcgan.png)","b8fb1fae":"# **DCGAN (combined model)**","0cc734f9":"Generative adversarial networks (GANs) are algorithmic architectures that use two neural networks, pitting one against the other (thus the \u201cadversarial\u201d) in order to generate new, synthetic instances of data that can pass for real data. They are used widely in image generation, video generation and voice generation.\n\nGANs are made up of two components -\n\n1. **Generator - generates new data instances**\n\n2. **Discriminator - tries to distinguish the generated or fake data from the real dataset.**\n\nDiscriminative algorithms try to classify input data; that is, given the features of an instance of data, they predict a label or category to which that data belongs. So discriminative algorithms map features to labels. They are concerned solely with that correlation.One the other way,loosely speaking,generative algorithms do the opposite. Instead of predicting a label given certain features, they attempt to predict features given a certain label.\n\nWhile training they both start together from scratch and the generator learn to shape the random distribition through the training epochs.","27b80d97":"# **Data Visualization**"}}