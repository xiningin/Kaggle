{"cell_type":{"60344e1d":"code","ab2154d7":"code","37e8e3d3":"code","bd82f512":"code","1a00b0d4":"code","f30c80e4":"code","7599f2b1":"code","57aec8f4":"code","060765ba":"code","cb0e4ae9":"code","811173b5":"code","dc0d4535":"code","08af45ee":"code","84df24df":"code","7033bd65":"code","dc8b3552":"code","bc2421a8":"code","334c9bee":"code","c3c242c3":"code","359dc9c2":"code","c697e358":"code","c2bf06e0":"code","eed52ff6":"code","6f61a928":"code","962620d2":"code","a299d587":"code","7c86c272":"code","28dff337":"code","bcff99a0":"code","633d73e1":"code","502c7ebd":"markdown","59b93c0c":"markdown","532c837d":"markdown","e8c8a096":"markdown","b0072cdd":"markdown"},"source":{"60344e1d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport xgboost as xg\nfrom collections import Counter\n!pip install kneed\n# kneed is not installed in kaggle. uncomment the above line.\nfrom kneed import KneeLocator\nimport matplotlib.pyplot as plt","ab2154d7":"# Reading train dataset in the environment.\ndataset_pd = pd.read_csv(\"\/kaggle\/input\/otto-group-product-classification-challenge\/train.csv\", index_col = 0)\nprint(dataset_pd.shape)\n# Reading test dataset in the environment.\ndataset_pd2 = pd.read_csv(\"\/kaggle\/input\/otto-group-product-classification-challenge\/test.csv\", index_col = 0)\nprint(dataset_pd2.shape)","37e8e3d3":"# Creating a predictor matrix (removing the response variable column)\ndataset_train = dataset_pd.values\nX = dataset_train[:,0:93] # Predictors\ny = dataset_train[:,93] # Response \n\n# XGBoost do not take a categorical variable as input. We can use LabelEncoder to assign labels to categorical variables.\nlabel_encoder = LabelEncoder()\nlabel_encoder = label_encoder.fit(y)\nlabel_encoder_y = label_encoder.transform(y)","bd82f512":"# Train and test split of the data\nX_train, X_test, y_train, y_test = train_test_split(X, label_encoder_y, test_size = 0.33, random_state = 7)","1a00b0d4":"# Running a XGBoost with default settings.\nmodel = xg.XGBClassifier()\nmodel.fit(X_train, y_train)","f30c80e4":"# Check the accuracy of the model on train and test dataset.\naccuracy_train = accuracy_score(y_train, model.predict(X_train))\nprint(\"Accuracy on train dataset %.2f%%\" % (accuracy_train * 100))\n\naccuracy_test = accuracy_score(y_test, model.predict(X_test))\nprint(\"Accuracy on test dataset %.2f%%\" % (accuracy_test * 100))","7599f2b1":"# Creating a confusion matrix \nprint(confusion_matrix(y_test, model.predict(X_test)))\nprint(classification_report(y_test, model.predict(X_test)))","57aec8f4":"# Running a XGBoost with less column sample.\nmodel = xg.XGBClassifier(colsample_bytree = 0.5)\nmodel.fit(X_train, y_train)\n# Check the accuracy of the model on train and test dataset.\naccuracy_train = accuracy_score(y_train, model.predict(X_train))\nprint(\"Accuracy on train dataset %.2f%%\" % (accuracy_train * 100))\n\naccuracy_test = accuracy_score(y_test, model.predict(X_test))\nprint(\"Accuracy on test dataset %.2f%%\" % (accuracy_test * 100))","060765ba":"Counter(y_test)","cb0e4ae9":"# Storing the feature importance matrix\nfeature_imp = pd.DataFrame(model.feature_importances_, \n                           index = dataset_pd.drop('target', axis = 1).columns, columns = ['imp'])\nfeature_imp.sort_values(by = 'imp', ascending = False, inplace = True)","811173b5":"# Calculating accuracy considering different threshold for feature importance.\nnum = []\nscore = []\nfor thresh in model.feature_importances_:\n    selection = SelectFromModel(model, threshold = thresh, prefit = True)\n    Select_X_train = selection.transform(X_train)\n    selection_model = xg.XGBClassifier()\n    selection_model.fit(Select_X_train, y_train)\n    Select_X_test = selection.transform(X_test)\n    y_pred = selection_model.predict(Select_X_test)\n    num.append(Select_X_train.shape[1])\n    score.append(accuracy_score(y_test, y_pred)* 100)\n    print(\"Thresh: %.3f, n = %d, Accuracy: %.2f%%\" % (thresh, Select_X_train.shape[1], accuracy_score(y_test, y_pred)* 100))","dc0d4535":"# Storing the accuracy table for different threshold and then plotting it.\naccuracy_table = pd.DataFrame({'params' : num, 'accuracy' : score})\naccuracy_table.sort_values(by = 'accuracy', ascending = False, inplace = True)\nplt.plot(range(93), accuracy_table['accuracy'])\nplt.show()","08af45ee":"# Storing the accuracy table for different threshold and then plotting it.\naccuracy_table.sort_values(by = 'params', inplace = True)\nplt.plot(range(93), accuracy_table['accuracy'])\nplt.show()","84df24df":"# We can find the elbow using KneeLocator.\nkl = KneeLocator(range(1, 94), accuracy_table['accuracy'], curve=\"concave\", direction=\"increasing\")\nkl.elbow","7033bd65":"# We can select 25 top variables and then fit the model again.\nfeature_imp[:25].index","dc8b3552":"# Selecting the top 25 variables.\ndata_top25 = dataset_pd[feature_imp[:25].index]\nX_top25 = data_top25.values","bc2421a8":"# Train and test split of the data\nX_train, X_test, y_train, y_test = train_test_split(X_top25, label_encoder_y, test_size = 0.33, random_state = 7)","334c9bee":"# Running a XGBoost with default settings with only top 25 variables.\nmodel = xg.XGBClassifier()\nmodel.fit(X_train, y_train)\n\n# Check the accuracy of the model on train and test dataset.\naccuracy_train = accuracy_score(y_train, model.predict(X_train))\nprint(\"Accuracy on train dataset %.2f%%\" % (accuracy_train * 100))\n\naccuracy_test = accuracy_score(y_test, model.predict(X_test))\nprint(\"Accuracy on test dataset %.2f%%\" % (accuracy_test * 100))","c3c242c3":"# Grid Search for number of trees\nmodel = xg.XGBClassifier(n_thread = -1)\nn_estimators = range(100, 500, 50)\n#max_depth = [2,4,6,8]\nparam_grid = dict(n_estimators = n_estimators)\nkfold = StratifiedKFold(n_splits = 8, shuffle = True, random_state = 7)\ngrid_search = GridSearchCV(model, param_grid, scoring = \"neg_log_loss\", n_jobs = -1, cv = kfold, verbose = 3)\nresult = grid_search.fit(X_top25, label_encoder_y)","359dc9c2":"print(\"Best paramter is %s \" % result.best_params_)","c697e358":"# Mean score for all the paramters tested\npd.DataFrame({\"params\": result.cv_results_['params'], \"mean_score\": result.cv_results_['mean_test_score'],\n             \"std_score\": result.cv_results_['std_test_score']})","c2bf06e0":"plt.errorbar(n_estimators, result.cv_results_['mean_test_score'], yerr = result.cv_results_['std_test_score'])\nplt.xlabel(\"n_estimators\")\nplt.ylabel(\"Log Loss\")\nplt.show()","eed52ff6":"# Grid Search for learning rate\nmodel = xg.XGBClassifier()\nlearning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\nparam_grid = dict(learning_rate = learning_rate, n_estimators = [150])\nkfold = StratifiedKFold(n_splits = 8, shuffle = True, random_state = 7)\ngrid_search = GridSearchCV(model, param_grid, scoring = \"neg_log_loss\", n_jobs = -1, cv = kfold, verbose = 1)\nresult = grid_search.fit(X, label_encoder_y)","6f61a928":"print(\"Best paramter is %s \" % result.best_params_)\nprint(\"Best score is %f\" % result.best_score_)","962620d2":"pd.DataFrame({\"params\": result.cv_results_['params'], \"mean_score\": result.cv_results_['mean_test_score'],\n             \"std_score\": result.cv_results_['std_test_score']})","a299d587":"plt.errorbar(learning_rate, result.cv_results_['mean_test_score'], yerr = result.cv_results_['std_test_score'])\nplt.xlabel(\"Learning_rate\")\nplt.ylabel(\"Log Loss\")\nplt.show()","7c86c272":"# Creating a predictor matrix (removing the response variable column)\ndataset_train = dataset_pd.values\nX = dataset_train[:,0:93] # Predictors\ny = dataset_train[:,93] # Response \n\n# XGBoost do not take a categorical variable as input. We can use LabelEncoder to assign labels to categorical variables.\nlabel_encoder = LabelEncoder()\nlabel_encoder = label_encoder.fit(y)\nlabel_encoder_y = label_encoder.transform(y)\n\n# Train and test split of the data\nX_train, X_test, y_train, y_test = train_test_split(X, label_encoder_y, test_size = 0.33, random_state = 7)\n\n# Running a XGBoost with less column sample.\nmodel = xg.XGBClassifier(n_estimators = 150, learning_rate = 0.2)\nmodel.fit(X_train, y_train)\n# Check the accuracy of the model on train and test dataset.\naccuracy_train = accuracy_score(y_train, model.predict(X_train))\nprint(\"Accuracy on train dataset %.2f%%\" % (accuracy_train * 100))\n\naccuracy_test = accuracy_score(y_test, model.predict(X_test))\nprint(\"Accuracy on test dataset %.2f%%\" % (accuracy_test * 100))","28dff337":"# Train and test split of the data\nX_train, X_test, y_train, y_test = train_test_split(X_top25, label_encoder_y, test_size = 0.33, random_state = 7)\n\n# Running a XGBoost with less column sample.\nmodel = xg.XGBClassifier(n_estimators = 150, learning_rate = 0.2)\nmodel.fit(X_train, y_train)\n# Check the accuracy of the model on train and test dataset.\naccuracy_train = accuracy_score(y_train, model.predict(X_train))\nprint(\"Accuracy on train dataset %.2f%%\" % (accuracy_train * 100))\n\naccuracy_test = accuracy_score(y_test, model.predict(X_test))\nprint(\"Accuracy on test dataset %.2f%%\" % (accuracy_test * 100))","bcff99a0":"# Train and test split of the data\nX_train, X_test, y_train, y_test = train_test_split(X_top25, label_encoder_y, test_size = 0.33, random_state = 7)\n\n# Running a XGBoost with less column sample.\nmodel = xg.XGBClassifier(n_estimators = 150, learning_rate = 0.2, colsample_bytree = 0.7)\nmodel.fit(X_train, y_train)\n# Check the accuracy of the model on train and test dataset.\naccuracy_train = accuracy_score(y_train, model.predict(X_train))\nprint(\"Accuracy on train dataset %.2f%%\" % (accuracy_train * 100))\n\naccuracy_test = accuracy_score(y_test, model.predict(X_test))\nprint(\"Accuracy on test dataset %.2f%%\" % (accuracy_test * 100))","633d73e1":"dataset_test = dataset_pd2.values\n# Selecting the top 25 variables.\ndata_top25_test = dataset_pd2[feature_imp[:25].index]\ndataset_test = data_top25_test.values\nprediction_sub = model.predict(dataset_test)\n\n#dataset_pd2[\"prediction\"] = prediction_sub\nX_sub = np.array(prediction_sub).reshape(-1,1)\nonehot_encoder = OneHotEncoder(sparse = False)\nsubmission_file = onehot_encoder.fit_transform(X_sub)\n\nsubmission_file_df = pd.DataFrame(submission_file, \n                                  columns = ['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6',\n                                            'Class_7','Class_8','Class_9'], index = dataset_pd2.index)\n\nsubmission_file_df.to_csv(\"submission_otto_ver2.csv\")","502c7ebd":"### Hyperparameter Tuning\n\nNumber of trees and size of the tree.","59b93c0c":"### XGBoost Model with default settings","532c837d":"### Studying the feature importance and selecting the top variables.","e8c8a096":"Now we have optimized 2 paramters, now we will try to build a XGBoost using these new hyperparamters.\n\nFirst we will try with all the variables and then with top 25 variables.","b0072cdd":"### Loading the dataset and data preprocessing"}}