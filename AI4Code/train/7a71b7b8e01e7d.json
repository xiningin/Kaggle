{"cell_type":{"9a830928":"code","5c4a5456":"code","a78259fe":"code","7a9781cf":"code","5e6c2fe1":"code","4a36a41b":"code","e3585ce7":"code","6213ef7b":"code","d974b83f":"code","8b20a46e":"code","e782a1cf":"code","a626afb0":"code","c465eb63":"code","47cf06b2":"code","ade199db":"code","49d991e8":"code","e219d8e6":"code","113e2619":"code","2204a847":"code","d0b75d56":"code","35dd0848":"code","62b8626e":"code","412d791b":"code","12552970":"code","7604952c":"code","af301f31":"markdown","ee2289ec":"markdown","9c2e06d2":"markdown","16aa565b":"markdown","95109637":"markdown","593f3b7a":"markdown","a716a1c3":"markdown","36c88ced":"markdown","8f57bc23":"markdown","5c3f5238":"markdown","e11ab68f":"markdown","f15f344e":"markdown","9e1f7699":"markdown","de5e0d01":"markdown","90976251":"markdown","26981c7d":"markdown","e1d6466f":"markdown","a6dbb32e":"markdown"},"source":{"9a830928":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, KBinsDiscretizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neural_network import MLPClassifier\nimport xgboost as xgb","5c4a5456":"import warnings\nwarnings.filterwarnings(\"ignore\")","a78259fe":"import matplotlib.pyplot as plt\n%matplotlib inline","7a9781cf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5e6c2fe1":"data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n# data = pd.read_csv('train.csv')","4a36a41b":"data.head()","e3585ce7":"data.info()","6213ef7b":"data.describe()","d974b83f":"plt.figure(figsize = (15,10))\ngrid = '22'\n\nplt.subplot(grid+'1')\nsns.countplot(data.Survived)\n\nplt.subplot(grid+'2')\nsns.countplot(data.Pclass)\n\nplt.subplot(grid+'3')\nsns.countplot(data.SibSp)\n\nplt.subplot(grid+'4')\nsns.countplot(data.Parch)","8b20a46e":"plt.figure(figsize = (20,5))\n\n\nplt.subplot(121)\nsns.distplot(data.Age.dropna(), hist = False, kde_kws = {'shade':True, 'alpha':0.3})\n\nplt.subplot(122)\nsns.boxplot(data.Age.dropna())\n","e782a1cf":"plt.figure(figsize = (8,5))\n\nsns.countplot(data.Embarked.dropna())","a626afb0":"plt.figure(figsize = (20,15))\n\ngrid = '33'\n\nplt.subplot(grid+'1')\nsns.barplot(x = 'Embarked', y = 'Survived', data = data)\n\nplt.subplot(grid+'2')\nsns.barplot(y = 'Survived', x = 'Pclass', data = data)\n\nplt.subplot(grid+'3')\nsns.barplot(y = 'Survived', x = 'Fare', data = pd.concat([data.Survived, pd.cut(data.Fare, 3, labels = 'low medium high'.split())], axis = 1))\n\nplt.subplot(grid+'4')\nsns.barplot(x = 'Embarked', y = 'Fare', data = data)\n\nplt.subplot(grid+'5')\nsns.barplot(y = 'Fare', x = 'Survived', data = data)\n\nplt.subplot(grid+'6')\nsns.barplot(y = 'Fare', x = 'Pclass', data = data)\n\nplt.tight_layout()","c465eb63":"plt.figure(figsize = (8,7))\nsns.heatmap(data.groupby(['Embarked','Pclass']).size().unstack(0).apply(lambda x: x\/x.sum()), annot = True, cbar = False)","47cf06b2":"# Evaluate thresholds\ndef test_thresh(lower = 0.1, upper = 0.95, jump = 0.01):\n    \n    accs = {}\n    for i in np.arange(lower, upper, jump):\n        accs[i] = accuracy_score(test[1], predict(test[0], model, transformers, thresh = i, prep = False)['Survived'])\n    \n    best_thresh = np.round(sorted(accs.items(), key = lambda x: x[1], reverse = True)[0][0], 2)\n    \n    train_acc = accuracy_score(train[1], predict(train[0], model, transformers, thresh = best_thresh, prep = False)['Survived'])\n    print(f'Train score: {train_acc}')\n    \n    print('Top 5 thresholds (test score):')\n    print(sorted(accs.items(), key = lambda x: (1-abs(0.5-x[0]))*x[1], reverse = True)[:5])\n    \n    \n    return best_thresh","ade199db":"# All data wrangling is performed here\n\ndef process_data(data, transformers = None):\n    data = data.copy()\n    data.set_index('PassengerId', inplace = True)\n        \n    # Extract title of person (Mr, Mrs, etc) and see if age changes between those groups significantly\n\n    data['title'] = data.Name.str.split(',').str[1].str.strip().str.split().str[0]\n    \n    low_freq_titles = data.title.value_counts()[lambda x: x < 10].index\n    \n    data['title'] = data['title'].apply(lambda x: 'Misc' if x in low_freq_titles else x)\n    \n    age_by_title = data.groupby('title').Age.median()\n\n    data['Age'] = data.apply(lambda x: age_by_title[x.title] if pd.isna(x.Age) else x.Age, axis = 1)\n\n    data.loc[pd.isna(data.Embarked), 'Embarked'] = 'C'    \n    \n    data['fam_size'] = data.Parch + data.SibSp\n    \n    data = data.fillna(method = 'ffill')\n    \n    vars_for_bins = ['Age', 'Fare']\n    vars_for_dummies = ['Sex', 'Embarked']\n    \n    if transformers is not None:\n        bins = transformers[0]\n        dummy = transformers[1]\n        \n        binned_age = bins.transform(np.array(data[vars_for_bins]))\n        dummies = dummy.transform(data[vars_for_dummies])\n        \n \n    else:\n        bins = KBinsDiscretizer()\n        dummy = OneHotEncoder(sparse = False, drop = 'first')\n        \n        binned_age = bins.fit_transform(np.array(data[vars_for_bins]))\n        dummies = dummy.fit_transform(data[vars_for_dummies])\n\n    dummies = pd.DataFrame(dummies, columns = dummy.get_feature_names())\n\n    binned_age = pd.DataFrame(binned_age.toarray(), columns = ['Age_'+str(i) for i in bins.bin_edges_[0]][:-1] + ['Fare_'+str(i) for i in bins.bin_edges_[1]][:-1])\n    \n    data.drop(['Cabin', 'Ticket', 'Name', 'Fare', 'Parch', 'SibSp', 'Age', 'Pclass', 'Sex', 'title', 'Embarked'], inplace = True, axis = 1, errors = 'ignore')\n    \n    data = pd.concat([data.reset_index(), dummies, binned_age], axis = 1).set_index('PassengerId')\n    \n    if transformers is None:\n        return data, bins, dummy\n    else:\n        return data","49d991e8":"# fit model\ndef fit_model(train_data, scale = True):\n    preped, bins, dummy = process_data(train_data)\n    \n    scaler = StandardScaler()\n    \n    X = preped.drop('Survived', axis = 1)\n    \n    if scale: \n        X = pd.DataFrame(scaler.fit_transform(X), columns = X.columns, index = X.index)\n    else:\n        scaler = None\n        \n    y = preped['Survived']\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n    print('Training Neural Net')\n    best_nn = RandomizedSearchCV(MLPClassifier(), \n                         verbose = 0, \n                         n_iter = 30,\n                         param_distributions = {'hidden_layer_sizes':[(50,50,),(100,100,100,),(300,300,),(50,50,50,50,),(50,50,50,50,50,),(200,200,200,)],\n                                                        'activation':['relu','tanh'],\n                                                        'solver':['adam','lbfgs'],\n                                                        'learning_rate':['constant','adaptive'],\n                                                        'learning_rate_init': np.logspace(np.log10(0.00001), np.log(1), 20),\n                                                        'max_iter':[50,100,200,500],\n                                                        'random_state':[0]})\n    \n    best_nn.fit(X_train, y_train)\n    best_nn = best_nn.best_estimator_\n\n    print('Training XGB')\n    \n    best_xgb = RandomizedSearchCV(xgb.XGBClassifier(), n_iter = 30,\n                     param_distributions = {'n_estimators':np.arange(50,550,50),\n                                   'booster':['gbtree', 'gblinear','dart'],\n                                   'max_depth':np.arange(1,50),\n                                   'learning_rate': np.logspace(np.log10(0.00001), np.log(1), 20),\n                                   'reg_alpha':np.logspace(np.log10(0.00001), np.log(1), 20)})\n    best_xgb.fit(X_train, y_train)\n    best_xgb = best_xgb.best_estimator_\n    \n    \n    print('Training Logistic Regression')\n\n    best_log = RandomizedSearchCV(LogisticRegression(), \n                         error_score=0.0, \n                         n_iter = 30,\n                         param_distributions = {'penalty':['l1', 'l2', 'elasticnet','none'], \n                                       'dual':[True,False],\n                                       'C': np.logspace(np.log10(0.00001), np.log(1), 20), \n                                       'fit_intercept':[True, False], \n                                       'random_state':[0], \n                                       'max_iter':[30,50,100,200,500,1000],\n                                       'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']})\n    best_log.fit(X_train, y_train)\n    best_log = best_log.best_estimator_\n\n    print('Training Random Forest')\n\n    best_forest = RandomizedSearchCV(RandomForestClassifier(), n_iter = 30, param_distributions = {'bootstrap': [True, False],\n                                                     'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n                                                     'max_features': ['auto', 'sqrt'],\n                                                     'min_samples_leaf': [1, 2, 4],\n                                                     'min_samples_split': [2, 5, 10],\n                                                     'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]})\n    best_forest.fit(X_train, y_train)\n    best_forest = best_forest.best_estimator_\n\n    best_estimators = [('Neural Net', best_nn), ('XGB', best_xgb), ('Logistic Reg', best_log), ('Random Forest', best_forest)]\n    \n    \n    \n    ensemble_model = VotingClassifier(best_estimators, voting = 'hard')\n\n#         model = GridSearchCV(DecisionTreeClassifier(), cv = 5, param_grid = {'max_depth':[1,2,3,5,7,10], \n#                                                       'criterion':['entropy','gini'], \n#                                                       'random_state':[0], \n#                                                       'min_samples_leaf':[10,25,50,70,100,150,200]})\n\n    ensemble_model.fit(X_train, y_train)\n    \n    print(f'Train Acc Score: {accuracy_score(y_train, ensemble_model.predict(X_train))}')\n    print(f'Test Acc Score: {accuracy_score(y_test, ensemble_model.predict(X_test))}')\n    \n    return ensemble_model, (bins, dummy, scaler), preped, (X_train, y_train), (X_test, y_test)","e219d8e6":"# make predictions\n\ndef predict(test_df, model, transformers, thresh = 0.5, prep = True):\n    if prep:\n        temp_df = process_data(test_df, transformers)\n        idx = temp_df.index\n        if transformers[2] is not None:\n            temp_df = pd.DataFrame(transformers[2].transform(temp_df), columns = temp_df.columns)\n            \n    else:\n        temp_df = pd.DataFrame(test_df, columns = preped.drop('Survived',axis = 1).columns).copy()\n        idx = temp_df.index\n    \n    \n    if thresh != 0.5:\n        predictions = (model.predict_proba(temp_df)[:,1] > thresh) * 1\n        return pd.DataFrame(columns = ['PassengerId','Survived'], data = [*zip(idx, predictions)])\n    \n    else:\n        return pd.DataFrame(columns = ['PassengerId','Survived'], data = [*zip(idx, model.predict(temp_df))])","113e2619":"model, transformers, preped, train, test = fit_model(data, scale = True)","2204a847":"# best_thresh = test_thresh()","d0b75d56":"test_set = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n# test_set = pd.read_csv('test.csv')\npredictions = predict(test_set, model, transformers)\npredictions.head()","35dd0848":"process_data(test_set, transformers).describe()","62b8626e":"process_data(data.drop('Survived', axis = 1))[0].describe(include = 'all')","412d791b":"model.estimators_","12552970":"predictions.to_csv('submission.csv', index = False)","7604952c":"test_labels = pd.read_csv('\/kaggle\/input\/titanic-solutions-for-selfscoring\/pub')\naccuracy_score(test_labels['survived'], predictions['Survived'])","af301f31":"## Univariate analysis  ","ee2289ec":"A lot of people from Port C survived relative to the other two ports. Also, in average, people from 1st class had higher probability of surviving than other two classes. \n\nIt seems that in average, people from Port C, payed more and people that payed more, had much more probability of surviving. Then, maybe we could discard the **Embarked** variable because we may extract the same information from **Pclass** or from **Fare**. These two variables seem to contain similar information and Fare has a little bit of more noice. ","9c2e06d2":"## Threshold selection","16aa565b":"## Fit model","95109637":"### Submission","593f3b7a":"\n#### Let's make sure both train and test set have same number of columns afters passed to the data wrangling function","a716a1c3":"## Helper Functions: Data Wrangling, Thresholds, Fit Models, Predictions","36c88ced":"We can see more peopled died than survived. Regardless, this is not a big unbalance, no need to fix the proportions of the data.\n\nAlso, we have a big third class in the sample and there was a lot of people traveling by themselves or with one or two companions. Big families were not common.","8f57bc23":"## Introduction\n\nTitanic is a well-studied use case for machine learning. The objective of this notebook is to show my approach to try to inspire other people on how to tackle it. If you want to try out any other approach based on mine, feel free to fork and your feedback is super welcome! \n\nThe steps I followed were the following:\n1. EDA: some basic univariate and multivariate analysis to look for obvious patterns or relation between variables that required some attention or that would give me a head start on tackling the problem. \n    1. On this section, I wrote my ideas\/insights between the visualizations of what I saw in the graphs. Also, your comments are super welcomed. \n2. Coding some helper functions: after EDA I wrote some helper functions so preping, training and predicting would be easy to do a bunch of times. \n    1. Please do look into the **fit_model** function, maybe you can tune better any of those models.\n3. After having created the helper functions, it is quite straight-forward to model and test different approaches.\n\n\n#### Something cool about this kernel: at the end there's a section for self-scoring with the test set for the actual competition so you don't have to wast your 10 attempts on little changes you may have applied to the model.","5c3f5238":"## Prediction & Submission","e11ab68f":"## EDA","f15f344e":"## Multivariate analysis  ","9e1f7699":"Only useful when not using Voting Classifier","de5e0d01":"What are the estimators used by the Voting Classifier?","90976251":"### Self-scoring section\nNote: this wasn't used for any tuning or training purposes. Only for scoring before submitting.\n\nFrom here on, the code will only work on local machine if you have the needed files","26981c7d":"We saw that Port C had something special. With this heatmap, we can appreciate that Port C had the biggest proportion of 1st class passengers from all ports. This explains the high rate of survival. Now, we know that **Embarked**, **Fare** and **Pclass** may have similar information. We can remove one of them, probably **Pclass** as said before and see how it goes. ","e1d6466f":"!kaggle competitions submit -c titanic -f submission.csv -m \"Here goes nothing?\"","a6dbb32e":"### Import libraries"}}