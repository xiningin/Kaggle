{"cell_type":{"37b277e0":"code","a1a8a9b5":"code","19f470b8":"code","15c55c3d":"code","1448743c":"code","88cb352f":"code","a780f670":"code","a4715936":"code","2f019104":"code","8501cc54":"code","7b6dd9e2":"code","17db480c":"markdown","84f139d3":"markdown","f3ab6b1d":"markdown","1631a5e9":"markdown","92c61636":"markdown","a63a454f":"markdown","b6342901":"markdown","238b4673":"markdown","52465a44":"markdown","beccb89c":"markdown"},"source":{"37b277e0":"import pickle\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split","a1a8a9b5":"# download embeddings\nimport os\nif not os.path.isfile('.\/glove.twitter.27B.zip'):\n    !wget 'https:\/\/nlp.stanford.edu\/data\/glove.twitter.27B.zip'\n    !unzip '.\/glove.twitter.27B.zip'","19f470b8":"from tensorflow.keras import Sequential, layers, losses, metrics, callbacks, optimizers\nimport tensorflow as tf\nimport numpy as np\nimport tensorflow_addons as tfa\n\n\nclass RNN:\n    \"\"\"\n    Class used to easily initialize RNN model with some hyper-parameters and options\n    already chosen\n    \"\"\"\n\n    def __init__(self, vocab_size=None, embedding_matrix=None, number_of_rnn_layers=2, rnn_cells=128,\n                 number_of_dense_layers=1, dense_cells=16, output_size=2, dropout_rate=0.5, lr=1e-4,\n                 num_classes=2, checkpoint_path='\/saved_models\/checkpoint'):\n\n        self.model = Sequential()\n        self.callbacks = [callbacks.EarlyStopping(monitor='val_loss', patience=5),\n                          callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                    save_best_only=True,\n                                                    save_weights_only=True,\n                                                    verbose=1)]\n        self.optimizer = optimizers.RMSprop(learning_rate=lr)\n\n        self.loss = losses.CategoricalCrossentropy(from_logits=False)\n        self.metrics = [tfa.metrics.F1Score(average='weighted', num_classes=num_classes),\n                        metrics.CategoricalAccuracy()]\n\n        # add embedding layer if pretrained embeddings are loaded\n        if vocab_size is not None and embedding_matrix is not None:\n            self.model.add(layers.Embedding(input_dim=vocab_size,\n                                            output_dim=200,\n                                            weights=[embedding_matrix],\n                                            trainable=False,\n                                            mask_zero=True))\n        for _ in range(number_of_rnn_layers):\n            self.model.add(layers.GRU(units=rnn_cells,\n                                      activation='relu',\n                                      return_sequences=True if _ != number_of_rnn_layers - 1\n                                                               and number_of_dense_layers > 1 else False))\n            if dropout_rate:\n                self.model.add(layers.Dropout(rate=dropout_rate))\n\n        dense_cells_current = dense_cells\n        for _ in range(number_of_dense_layers):\n            self.model.add(layers.Dense(units=int(dense_cells_current),\n                                        activation='relu'))\n            dense_cells_current \/= 2\n            if dropout_rate:\n                self.model.add(layers.Dropout(rate=dropout_rate))\n\n        self.model.add(layers.Dense(units=output_size,\n                                    activation='softmax'))\n\n    def compile(self):\n        self.model.compile(optimizer=self.optimizer,\n                           loss=self.loss,\n                           metrics=self.metrics)\n\n    def fit(self, x_train, y_train, batch_size=256, epochs=10, validation_split=0.2, class_weights=None):\n        return self.model.fit(x=x_train, y=y_train,\n                              batch_size=batch_size,\n                              epochs=epochs,\n                              callbacks=self.callbacks,\n                              validation_split=validation_split,\n                              class_weight=class_weights)\n\n    def evaluate(self, x, y):\n        return self.model.evaluate(x, y)\n\n    def predict(self, x):\n        return self.model.predict(x)\n\n    def save(self, path: str, weights_only=False):\n        if weights_only:\n            self.model.save_weights(path)\n        else:\n            self.model.save(path)\n\n","15c55c3d":"import string\nfrom re import sub\n\nfrom nltk import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\n\nclass Preprocessing:\n    \"\"\"\n    Methods:\n        remove_usernames(string): returns input string without usernames (words starting with '@')\n        remove_punctuation(string): returns input string without punctuation\n        remove_links(string): returns input string without links (words starting with 'http' or 'https')\n        tokenize(string): returns tokens of input string, using Keras Tokenizer\n        preprocessing_pipe(string, boolean): returns input with applying all of preprocessing steps\n\n    \"\"\"\n\n    def __init__(self, vocab_size=10000, remove_stopwords_flag=True, lemmatize_flag=True):\n        self.stop_words = set(stopwords.words('english'))\n        self.lemmatizer = WordNetLemmatizer()\n        self.vocab_size = vocab_size\n        self.remove_stopwords_flag = remove_stopwords_flag\n        self.lemmatize_flag = lemmatize_flag\n\n    @staticmethod\n    def remove_usernames(raw):\n        return sub(r'@[^\\s]*', '', raw)\n\n    @staticmethod\n    def remove_punctuation(raw):\n        return raw.translate(str.maketrans('', '', string.punctuation))\n\n    @staticmethod\n    def remove_links(raw):\n        return sub(r'https?:\/\/\\S+', '', raw)\n\n    def remove_stopwords(self, raw):\n        return \" \".join([word for word in raw.split() if word not in self.stop_words])\n\n    def lemmatize(self, raw):\n        return \" \".join([self.lemmatizer.lemmatize(str.lower(token)) for token in raw.split()])\n\n    def preprocessing_pipeline(self, raw):\n        result = self.remove_punctuation(self.remove_usernames(self.remove_links(raw)))\n        if self.remove_stopwords_flag:\n            result = self.remove_stopwords(result)\n        if self.lemmatize_flag:\n            result = self.lemmatize(result)\n        return result","1448743c":"from sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\n\nclass Postprocessing:\n    def __init__(self):\n        self.tokenizer = None\n        self.label_encoder = None\n        self.word_index = None\n        self.vocab_size = None\n\n    def init_tokenizer(self, data, vocab_size=None):\n        self.tokenizer = Tokenizer(num_words=vocab_size)\n        self.tokenizer.fit_on_texts(data)\n        self.word_index = self.tokenizer.word_index\n        if vocab_size is not None:\n            self.vocab_size = vocab_size\n        else:\n            self.vocab_size = len(self.tokenizer.word_index) + 1\n\n    def tokenize_and_pad(self, data, max_len=32):\n        return pad_sequences(self.tokenizer.texts_to_sequences(data), maxlen=max_len)\n\n    def init_label_encoder(self, labels):\n        self.label_encoder = LabelEncoder()\n        self.label_encoder.fit(labels.to_list())\n\n    def encode_labels(self, labels):\n        return self.label_encoder.transform(labels.to_list())\n\n    def get_vocab(self):\n        return self.tokenizer.word_index\n","88cb352f":"class PredictionPipeline:\n    \"\"\"\n\n    \"\"\"\n\n    def __init__(self, preprocessor, postprocessor, model):\n        self.preprocessor = preprocessor\n        self.model = model\n        self.postprocessor = postprocessor\n\n    def get_prediction(self, text):\n        data = self.preprocessor.preprocessing_pipeline(text)\n        x = self.postprocessor.tokenize_and_pad([data])\n        return self.model.predict(x)\n","a780f670":"import numpy as np\nfrom tqdm import tqdm\n\n\nclass Embedding:\n    def __init__(self, embedding_file_path=None):\n        self.embeddings_index = dict()\n        if embedding_file_path is None:\n            embedding_file_path = '.\/glove.twitter.27B.200d.txt'\n        with open(embedding_file_path, 'r', encoding='utf-8') as f:\n            for line in tqdm(f.readlines()):\n                values = line.split()\n                word = values[0]\n                vector = np.asarray(values[1:], dtype='float32')\n                self.embeddings_index[word] = vector\n\n    def make_embedding_matrix(self, vocab, embedding_dim=None):\n        if embedding_dim is None:\n            embedding_dim = 200\n        hits, misses = 0, 0\n        embedding_matrix = np.zeros((len(vocab) + 1, embedding_dim))\n        for word, i in vocab.items():\n            token_vector = self.embeddings_index.get(word, None)\n            if token_vector is not None:\n                embedding_matrix[i] = token_vector\n                hits += 1\n            else:\n                misses += 1\n\n        print(\"Converted %d words (%d misses)\" % (hits, misses))\n        return embedding_matrix\n","a4715936":"import pandas as pd\nimport numpy as np\n\n\ndef load_sentiment_dataset(path, encoding=\"ISO-8859-1\", names=None, drop_extras=True):\n    if names is None:\n        names = [\"label\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n    df = pd.read_csv(path, encoding=encoding, names=names)\n    if drop_extras:\n        df.drop([\"id\", \"date\", \"flag\", \"user\"], axis=1, inplace=True)\n    return df\n\n\ndef load_emotions_dataset(path, encoding=\"ISO-8859-1\", names=None, drop_extras=True):\n    if names is None:\n        names = [\"id\", \"label\", \"text\"]\n    df = pd.read_csv(path, encoding=encoding, names=names)\n    if drop_extras:\n        df.drop([\"id\"], axis=1, inplace=True)\n    return df\n\n\ndef calc_class_weights(labels):\n    label_sum = np.sum(labels, axis=0)\n    total_sum = np.sum(label_sum)\n    no_of_labels = len(label_sum)\n    weights = {}\n    for i in range(len(label_sum)):\n        weights.update({i: (1 \/ label_sum[i]) * (total_sum \/ no_of_labels)})\n    return weights\n","2f019104":"if not os.path.exists('.\/saved_models'):\n    !mkdir '.\/saved_models'","8501cc54":"pd.options.mode.chained_assignment = None\n\nprint(\"Loading data...\")\ndf = load_sentiment_dataset(\"..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv\")\n\nprint(\"Preprocessing...\")\nprep = Preprocessing(remove_stopwords_flag=False, lemmatize_flag=False)\ndf['text'] = df['text'].apply(lambda x: prep.preprocessing_pipeline(x))\nx_train, x_test, y_train, y_test = train_test_split(df.text, df.label, test_size=0.2)\n\nprint(\"Postprocessing...\")\npostp = Postprocessing()\npostp.init_tokenizer(x_train)\nx_train = postp.tokenize_and_pad(x_train)\nx_test = postp.tokenize_and_pad(x_test)\n\npostp.init_label_encoder(y_train)\ny_train = tf.one_hot(postp.encode_labels(y_train), 2)\ny_test = tf.one_hot(postp.encode_labels(y_test), 2)\n\nprint(\"Embedding...\")\nemb = Embedding()\nembedding_matrix = emb.make_embedding_matrix(vocab=postp.get_vocab())\n\nrnn = RNN(vocab_size=postp.vocab_size,\n                embedding_matrix=embedding_matrix,\n                number_of_rnn_layers=1,\n                rnn_cels=32,\n                dense_cells=16,\n                number_of_dense_layers=2,\n                dropout_rate=0.4,\n                lr=3e-4,)\n\nrnn.compile()\nhistory = rnn.fit(x_train, y_train, batch_size=128, epochs=7)\nrnn.evaluate(x_test, y_test)\n\nrnn.save('.\/saved_models\/sentiment_model', weights_only=False)\n\npredictions = np.argmax(rnn.predict(x_test), axis=1)\ny_true = np.argmax(y_test, axis=1)\ncf = confusion_matrix(y_true, predictions)\nacc = accuracy_score(y_true, predictions)\n\nprint(\"Confusion matrix: \", cf)\nprint(\"Acc: {}\".format(acc))\n\npp = PredictionPipeline(preprocessor=prep, postprocessor=postp, model=rnn)\nprint(pp.get_prediction(\"This is very nice Tweet and I'm happy!\"))\nprint(pp.get_prediction(\"I hate myself\"))\nprint(pp.get_prediction(\"Life is good!!!\"))\nprint(\"Done!\")\n","7b6dd9e2":"pd.options.mode.chained_assignment = None\n\nprint(\"Loading data...\")\ndf = load_emotions_dataset(\"..\/input\/emotion-detection-from-text\/tweet_emotions.csv\")\n\nprint(\"Preprocessing...\")\nprep = Preprocessing(remove_stopwords_flag=False, lemmatize_flag=False)\ndf['text'] = df['text'].apply(lambda x: prep.preprocessing_pipeline(x))\n\nlabel_dim = 6\ndf = df[df['label'].isin(df['label'].value_counts().index[:label_dim])]\ndf['label'] = df['label'].astype('category')\n\nx_train, x_test, y_train, y_test = train_test_split(df.text, df.label, test_size=0.1)\n\nprint(\"Postprocessing...\")\npostp = Postprocessing()\npostp.init_tokenizer(x_train)\nx_train = postp.tokenize_and_pad(x_train)\nx_test = postp.tokenize_and_pad(x_test)\n\npostp.init_label_encoder(y_train)\ny_train = tf.one_hot(postp.encode_labels(y_train), label_dim)\ny_test = tf.one_hot(postp.encode_labels(y_test), label_dim)\nclass_weights = calc_class_weights(y_train)\n\nprint(\"Embedding...\")\nemb = Embedding()\nembedding_matrix = emb.make_embedding_matrix(vocab=postp.get_vocab())\n\nrnn = RNN(vocab_size=postp.vocab_size,\n                embedding_matrix=embedding_matrix,\n                output_size=label_dim,\n                dense_cells=256,\n                number_of_dense_layers=2,\n                dropout_rate=0.35,\n                lr=1e-4,\n                num_classes=label_dim)\n\nrnn.compile()\n\nhistory = rnn.fit(x_train, y_train, batch_size=32, epochs=45, class_weights=class_weights)\nrnn.evaluate(x_test, y_test)\n\nrnn.save('.\/saved_models\/emotions', weights_only=False)\n\npp = PredictionPipeline(preprocessor=prep, postprocessor=postp, model=rnn)\nprint(pp.get_prediction(\"This is very nice Tweet and I'm happy!\"))\nprint(pp.get_prediction(\"I hate myself\"))\nprint(pp.get_prediction(\"Life is good!!!\"))\nprint(\"Done!\")","17db480c":"## Emotions model training","84f139d3":"# Model class","f3ab6b1d":"## Postprocessing class","1631a5e9":"# Other classes and methods","92c61636":"## Prediction pipeline","a63a454f":"## Utils class","b6342901":"## Sentiment model training","238b4673":"## Embedding class","52465a44":"## Preprocessing class","beccb89c":"# Training\nRun Model class and Other classes and methods before running Training section!"}}