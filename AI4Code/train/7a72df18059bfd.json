{"cell_type":{"5691dc5e":"code","59bc9b3d":"code","07bc6d13":"code","b97add1b":"code","1fe224e2":"code","d6e59eaa":"code","49b756fc":"code","50d28cfb":"code","06d7d93d":"code","437e395b":"code","ed271549":"markdown","758be709":"markdown","2edbc84e":"markdown","d5ebf2fd":"markdown","4a9ecf1f":"markdown","794076a8":"markdown","90f4d4eb":"markdown","3656db8d":"markdown","3dfc71cf":"markdown","eea9866a":"markdown"},"source":{"5691dc5e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","59bc9b3d":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\n\ndef score_dataset(X_train, X_valid, y_train, y_valid, nbEstimators):\n    model = RandomForestRegressor(n_estimators=nbEstimators, criterion='mse', random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    print(str(nbEstimators))\n    return mean_squared_error(y_valid, preds)","07bc6d13":"help(mean_squared_error)","b97add1b":"from sklearn.model_selection import train_test_split\n\n# Read the data\nX_full = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv', index_col='Id')\nX_test_full = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X_full.SalePrice\nX_full.drop(['SalePrice'], axis=1, inplace=True)\n\n# To keep things simple, we'll use only numerical predictors\nX = X_full.select_dtypes(exclude=['object'])\nX_test = X_test_full.select_dtypes(exclude=['object'])\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)","1fe224e2":"from sklearn.impute import SimpleImputer\n\n# Fill in the lines below: imputation\n\nmy_imputer = SimpleImputer()\n\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\nimputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n\n# Fill in the lines below: imputation removed column names; put them back\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns","d6e59eaa":"nb_estimators = [1,10,50,100,150,200,250,300]\n\nfor nb in nb_estimators:\n    print(\"Nb estimators %d, score = %d\" % (nb,score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid, nb)))","49b756fc":"from numpy import *\nfrom pylab import *\n\nnb_estimators2 = concatenate((arange(1,20,1),arange(20,251,10)),axis = 0)\nerror_results = [score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid, nb) for nb in nb_estimators2]\n\nprint(error_results)\n\nfigure(1)\nsubplot(2,1,1)\nplot(nb_estimators2,error_results)\nylabel('Final training score')\n\nsubplot(2,1,2)\nsemilogy(nb_estimators2,error_results)\nxlabel('Number of estimators')\nylabel('Final training score')\n","50d28cfb":"help(min)","06d7d93d":"best_index = error_results.index(min(error_results))\nbest_estimators = nb_estimators2[best_index]\n\nprint(best_estimators)\nprint(error_results[best_index])","437e395b":"imputed_X_Full_Train = pd.DataFrame(my_imputer.fit_transform(X))\nimputed_X_Full_Test = pd.DataFrame(my_imputer.transform(X_test))\n\n# Model and fit\nmodel = RandomForestRegressor(n_estimators=best_estimators, criterion='mse', random_state=0)\nmodel.fit(imputed_X_Full_Train, y)\n\n# Fill in the line below: get test predictions\npreds_test = model.predict(imputed_X_Full_Test)\n\n# Save predictions in format used for competition scoring\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds_test})\noutput.to_csv('SubmissionRSE.csv', index=False)","ed271549":"I try now to bring a longer but maybe more accurate approach by better scanning the estimators number.","758be709":"There is only to build our final model and create a submission file.","2edbc84e":"After running all previous codes, we notice that 250 estimators is the best, however, from 50 estimators, the results is not very different. If we time is a factor, we can use 150. If not, just pick the one with the smallest result, which is 250 in this case","d5ebf2fd":"We also use the function proposed by the exercise on \"missing values\" intermediate ML course. However, the error is using mean squared error. We will use it to find an optimized number of estimators. \n\nNOTE : one variable was added to the function \"score_dataset\" so we can change the number of estimators.","4a9ecf1f":"The logarithmic plot is used to be able to see small variations.\n\nWith the curve we find that the optimized number of estimators is 250. But to save time, we can also use 20, which should still give a good result.\n\nLet us find the minimum from the previous calculation.","794076a8":"Hello,\n\nHere is a try I did on my own. I just finished the exercises on missing values from the intermediate ML course. I have a bit of a scientific \/ signal processing so I wanted to try other error calculation method and see the impact on the final result.\n\nIt seems that using mean squared error improves slightly the results however not as much as I expected. Is it because the values used are very different therefore we the algorithm only optimize using the big values?\n\nThank you in advance.","90f4d4eb":"We also use the imputation method since it was the best at the exercize \"missing values\".","3656db8d":"Now let us setup everything the same way it was done in the exercises so we have datas to test and do a submission later.","3dfc71cf":"The goal is to try an idea I got : using regression tree with a criterion. MAE is good but puts the same wait whether datas are close or far from each other. To take that into account, we should take mean squared error MSE.\n\n> model = RandomForestRegressor(n_estimators=100, criterion='mae', random_state=0)\n\nbecomes : \n\n> model = RandomForestRegressor(n_estimators=100, criterion='mse', random_state=0)\n\nRoot mean square (rms) also exists","eea9866a":"We have now all datas setup, we will try to find a good model for our data."}}