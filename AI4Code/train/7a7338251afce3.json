{"cell_type":{"dfd3cd86":"code","484165f4":"code","f20a9c9f":"code","732a3fb4":"code","487ead17":"code","dcf3417c":"code","1a09a275":"code","b153d5a3":"code","1f0e5493":"code","2d8b1bd6":"code","1d0e58c8":"code","1dab0b08":"code","fd4931ec":"code","f64a0c1b":"code","abdf61f6":"code","c2f9f06f":"code","63d90b01":"code","4104ab12":"code","c3152f72":"code","f02fbaed":"code","1bc191e8":"code","7d60c7b3":"code","89d82c7e":"code","1b94a504":"code","1e3f3c8b":"code","e1615048":"code","e58004cc":"code","f5810a29":"code","c0401fab":"code","78770804":"code","60375b9a":"code","d4a78401":"code","0181e5ef":"code","60a0f0fa":"markdown"},"source":{"dfd3cd86":"#@author: Abhishek Kumar Gauraw\n# 1.1 Data manipulation modules\nimport pandas as pd        # R-like data manipulation\nimport numpy as np         # n-dimensional arrays\n\n# 1.2 For pltting\nimport matplotlib.pyplot as plt      # For base plotting\n# Seaborn is a library for making statistical graphics\n# in Python. It is built on top of matplotlib and \n#  numpy and pandas data structures.\nimport seaborn as sns                # Easier plotting\n\n# 1.3 Misc\nimport os\n\n","484165f4":"############## Data Loading and Exploration ##################\n# 2.1 Read data file\ndata_test = pd.read_csv(\"..\/input\/test.csv\")\ndata_train = pd.read_csv(\"..\/input\/train.csv\")\n\n\n# 2.2 Explore data\nprint(\"Dimension of Test DataSet :\" ) \ndata_test.shape                         # dim()\n","f20a9c9f":"print(\"Dimension of Training DataSet :\")\ndata_train.shape                        # dim()\n","732a3fb4":"print(\"Columns of Test DataSet :\")              \ndata_test.columns\n","487ead17":"print(\"Columns of Training DataSet :\")\ndata_train.columns\n","dcf3417c":"print(\"Gllimpse\/Summary of Test DataSet :\")\ndata_test.describe()                      # summary()\n","1a09a275":"print(\"Gllimpse\/Summmary of Training DataSet :\")\ndata_train.describe()                     # summary()\n","b153d5a3":"print(\"First five records of Test DataSet :\")\ndata_test.head()                          # Top 5 records of test data\n","1f0e5493":"print(\"First five records of Training DataSet :\")\ndata_train.head()                          # Top 5 records of training data\n","2d8b1bd6":"# Identifying and Removing columns not useful in analysis\nmissing_col_train = data_train.isnull().sum().sort_values(0, ascending = False)\nprint(\"Missing or Null Value column: \")\nmissing_col_train.head()","1d0e58c8":"# Looking at this columns rez_esc,v18q1 and v2al have many null\/missing values\n## Hence we can remove these 3 columns to avoid any issue in modelling further\n\ndata_train.drop([\"rez_esc\",\n              \"v18q1\",\n              \"v2a1\",\n    ], axis=1, inplace=True)\n\ndel(missing_col_train)\n\nprint(\"Removed 3 columns\")\n","1dab0b08":"## Remove these 3 columns from test data to avoid any issue in modelling further\n\ndata_test.drop([\"rez_esc\",\n              \"v18q1\",\n              \"v2a1\",\n    ], axis=1, inplace=True)\n\nprint(\"Removed 3 columns from Test data\")","fd4931ec":"print(\"New Dimension of Training DataSet :\")\ndata_train.shape  ","f64a0c1b":"#We can finish off with the meaneduc and SQBmeaned label by imputing them with the median of the columns.\nmedian_meaneduc = data_train['meaneduc'].median()\nmedian_SQBmeaned = data_train['SQBmeaned'].median()\ndata_train['meaneduc'] = data_train['meaneduc'].fillna(median_meaneduc)\ndata_train['SQBmeaned'] = data_train['SQBmeaned'].fillna(median_SQBmeaned)\n\nmedian_meaneduc_test = data_test['meaneduc'].median()\nmedian_SQBmeaned_test = data_test['SQBmeaned'].median()\ndata_test['meaneduc'] = data_test['meaneduc'].fillna(median_meaneduc_test)\ndata_test['SQBmeaned'] = data_test['SQBmeaned'].fillna(median_SQBmeaned_test)","abdf61f6":"data_train.loc[data_train['Target'].isin([1]),'target_des'] = \"Extereme Poverty\"         \ndata_train.loc[data_train['Target'].isin([2]),'target_des'] = \"Vulnerable\"         \ndata_train.loc[data_train['Target'].isin([3]),'target_des'] = \"Moderate Poverty\"         \ndata_train.loc[data_train['Target'].isin([4]),'target_des'] = \"NonVulnerable\"         \n\ndata_train['Target'].value_counts()","c2f9f06f":"#Target - the target is an ordinal variable indicating groups of income levels. \n## 1 = extreme poverty \n## 2 = moderate poverty \n## 3 = vulnerable households \n## 4 = non vulnerable households\n\n# Count Plot for Group of different Income levels\nincome_lvl_plot = sns.countplot(\"target_des\", data = data_train)\nincome_lvl_plot.set_title(\"Group of Income Levels at Cost Rica\")\nincome_lvl_plot.set_xticklabels(income_lvl_plot.get_xticklabels(), rotation=45)\n","63d90b01":"# Violin Plot to determine gender wise distribution along with poverty level \ngenderwise_total = data_train[[\"r4h3\", \"r4m3\"]].groupby(data_train[\"target_des\"]).sum()\nprint(genderwise_total)\ngender_plot = (sns.violinplot(data=genderwise_total,\n               split=True,         # If hue variable has two levels, draw half of a violin for each level.\n               inner=\"quartile\"    #  Options: \u201cbox\u201d, \u201cquartile\u201d, \u201cpoint\u201d, \u201cstick\u201d, None \n               )\n        .set_xticklabels(['Male','Female'])    \n)\n","4104ab12":"male_plot = (sns.violinplot( y=data_train[\"target_des\"], x=data_train[\"r4h3\"] )\n           .set(xlabel='Male', ylabel='Poverty level')  \n           )\n\n\n","c3152f72":"female_plot = (sns.violinplot( y=data_train[\"target_des\"], x=data_train[\"r4m3\"] )\n            .set(xlabel='Female', ylabel='Poverty level')\n            )\n\ndel(genderwise_total)","f02fbaed":"\n# Education level of people from Costa-Rica\nEdu_level_total = data_train[[\"instlevel1\", \"instlevel2\", \"instlevel3\",\"instlevel4\", \"instlevel5\", \"instlevel6\",\"instlevel7\", \"instlevel8\", \"instlevel9\"]].groupby(data_train[\"target_des\"]).sum()\n\nprint(Edu_level_total)\nlabels = ['No level of education', 'Incomplete primary', 'Complete primary', 'Incomplete academic secondary level','Complete academic secondary level','Incomplete technical secondary level','Complete technical secondary level','Undergraduate and higher education','Postgraduate higher education']\nsns.set_style(\"whitegrid\")\n\nedu_lvl_plot = (\n   sns.violinplot(data=Edu_level_total,\n               split=True,         \n               inner=\"quartile\"     \n               )\n   .set(xlabel='Education Level', ylabel='Poverty level')\n        \n)\n\nedu_lvl_plot = (\n   sns.violinplot(data=Edu_level_total,\n               split=True,         \n               inner=\"quartile\"     \n               )\n    .set_xticklabels(labels,rotation=20)\n)\n\ndel(Edu_level_total)","1bc191e8":"#Check for households where The household population has unequal target distribution\nall_equal = data_train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\nnot_equal = all_equal[all_equal != True]\nprint(len(not_equal))","7d60c7b3":"#correcting the unqual households\nfor household in not_equal.index:\n    true_target = int(data_train[(data_train['idhogar'] == household) & (data_train['parentesco1'] == 1.0)]['Target'])\n    data_train.loc[data_train['idhogar'] == household, 'Target'] = true_target","89d82c7e":"data_train.fillna(-1, inplace = True)\ndata_test.fillna(-1, inplace = True)","1b94a504":"data_train['dependency'] = np.sqrt(data_train['SQBdependency'])\ndata_test['dependency'] = np.sqrt(data_test['SQBdependency'])","1e3f3c8b":"def mapping(data):\n    if data == 'yes':\n        return 1\n    elif data == 'no':\n        return 0\n    else:\n        return data\ndata_train['dependency'] = data_train['dependency'].apply(mapping).astype(float)\ndata_train['edjefa'] = data_train['edjefa'].apply(mapping).astype(float)\ndata_train['edjefe'] = data_train['edjefe'].apply(mapping).astype(float)\n\ndata_test['dependency'] = data_test['dependency'].apply(mapping).astype(float)\ndata_test['edjefa'] = data_test['edjefa'].apply(mapping).astype(float)\ndata_test['edjefe'] = data_test['edjefe'].apply(mapping).astype(float)","e1615048":"#converting into percentages\ndata_train['males_above_12'] = data_train['r4h2']\/data_train['r4h3']\ndata_train['person_above_12'] = data_train['r4t2']\/data_train['r4t3']\ndata_train['size_to_person_ratio'] = data_train['tamhog']\/data_train['tamviv']\n\ndata_test['males_above_12'] = data_test['r4h2']\/data_test['r4h3']\ndata_test['person_above_12'] = data_test['r4t2']\/data_test['r4t3']\ndata_test['size_to_person_ratio'] = data_test['tamhog']\/data_test['tamviv']","e58004cc":"data_train['males-above_12'] = data_train['males_above_12'].fillna(0)\ndata_test['males-above_12'] = data_test['males_above_12'].fillna(0)\n\ndata_train = data_train.fillna(0)\ndata_test = data_test.fillna(0)","f5810a29":"# Assigning ID to sub before dropping\nsubmission = data_test[['Id']]\n#dropping other useless columns\ncols = ['Id','idhogar','SQBescolari','SQBage','SQBhogar_total','SQBedjefe','SQBhogar_nin','SQBovercrowding','SQBdependency','SQBmeaned','agesq']\ndata_train.drop(cols, axis = 1, inplace = True)\ndata_test.drop(cols, axis = 1, inplace = True)","c0401fab":"# Feature Engineering\n#creating the matrics of features\ny = data_train.Target.values\ndata_train.drop('Target', axis =1, inplace = True)\ndata_train.drop('target_des', axis =1, inplace = True)","78770804":"X = data_train.iloc[:,:].values\nX_test = data_test.iloc[:,:].values","60375b9a":"from sklearn.ensemble import RandomForestClassifier as RFC\nclassifier = RFC(n_estimators =25 , random_state = 0)\nclassifier.fit(X,y)","d4a78401":"predict_result = classifier.predict(X_test).astype(int)","0181e5ef":"sub = pd.DataFrame({\n    \"Id\" : submission['Id'],\n    \"Target\" : predict_result\n})\nsub.to_csv('sample_submission.csv', index =False, encoding = 'utf-8')\nsub.head()","60a0f0fa":"# Costa-Rica Household Poverty Prediction"}}