{"cell_type":{"36e6937c":"code","2ea66419":"code","7f90d4d3":"code","c13cab9b":"code","400ea9ef":"code","73f76e5e":"code","12400796":"code","1a937c6b":"code","78289bad":"code","02832d43":"code","dabf3058":"code","74274182":"code","9d7bbe6b":"code","2776993d":"code","0985478c":"code","ac437400":"code","145238d9":"code","e36ecc37":"code","e55793d6":"code","5b661b06":"code","0e498734":"code","6dfb5efb":"code","65046bf4":"markdown","f5ef6f6e":"markdown","12747608":"markdown","6fd68ca0":"markdown","7fbc8675":"markdown"},"source":{"36e6937c":"import numpy as np              # linear algebra\nimport pandas as pd             # data processing, CSV file I\/O (e.g. pd.read_csv)\n                                \nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns           # data visualization\n                                \nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\n\nimport optuna\nimport lightgbm as lgb\nfrom sklearn.metrics import auc, roc_curve","2ea66419":"sample_submission = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/sample_submission.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv\")","7f90d4d3":"sample_submission.shape","c13cab9b":"train.shape","400ea9ef":"train.info()","73f76e5e":"test.shape","12400796":"test.info()","1a937c6b":"data = pd.concat([train, test], sort = False)\ndata.shape","78289bad":"data.info()","02832d43":"data.head()","dabf3058":"null_cols = [col for col in data.iloc[: , : -1].columns if data[col].isnull().sum() != 0]\nnull_cols","74274182":"del data","9d7bbe6b":"float_cols = [col for col in train.iloc[: , 1 : -1].columns if train[col].dtype == \"float64\"]\nlen(float_cols)","2776993d":"CHUNMEIHONG = '#f1939c'\nQIUBOLAN = '#8abcd1'\nXIANGYABAI = '#fffef8'\nZHENZHUHUI = '#e4dfd7'\n\nfig, axes = plt.subplots(20, 5, figsize = (16, 48))\naxes = axes.flatten()\n\ndef features_distribution(axes):\n    for idx, ax in enumerate(axes):\n        sns.kdeplot(\n            data = train[float_cols + ['target']],\n            ax = ax,\n            hue = 'target',\n            fill = True,\n            x = f'f{idx}',\n            palette = [f'{CHUNMEIHONG}', f'{QIUBOLAN}'],\n            legend = idx == 0,\n            alpha = .5,\n            linewidth = 2.5,\n        )\n        \n        ax.grid(\n            color = XIANGYABAI,\n            linestyle = \":\",\n            linewidth = 1.25,\n            alpha = 0.3,\n        )\n        ax.set_facecolor(ZHENZHUHUI)\n        #ax.set_xticks([])\n        #ax.set_yticks([])\n        ax.spines['left'].set_visible(False)\n        ax.spines['top'].set_visible(False)\n        ax.yaxis.tick_right()\n        ax.yaxis.set_label_position(\"left\")\n        ax.set_title(\n            f'f{idx}',\n            loc = 'right',\n            weight = 'bold',\n            fontsize = 10,\n        )\n        #ax.set_xticks([])\n        #ax.set_yticks([])\n        ax.set_xlabel('')\n        #ax.set_ylabel('')\n        if idx % 5 != 0:\n            ax.set_ylabel('')\n\nfeatures_distribution(axes)\n\nfig.supxlabel('Probability', ha = 'center', fontweight = 'bold', fontsize = 16, y = -0.005,)\nfig.supylabel('Density', ha = 'center', fontweight = 'bold', fontsize = 16, x = -0.005,)\nfig.suptitle('Features Distribution', ha = 'center', fontweight = 'heavy', fontsize = 20, y = 1,)\nfig.tight_layout()","0985478c":"df_train = train.copy()\ndf_test = test.copy()\n\npeaks = ['f0','f2','f4','f9','f12','f16','f19','f20','f23','f24','f27',\n    'f28','f30','f31','f32','f33','f35','f39','f42','f44','f46','f48',\n    'f49','f51','f52','f53','f56','f58','f59','f60','f61','f62','f63',\n    'f64','f68','f69','f72','f73','f75','f76','f78','f79','f81','f83',\n    'f84','f87','f88','f89','f90','f92','f93','f94','f95','f98','f99']\n\nno_peaks = [feats for feats in df_test.columns if feats not in peaks]\n\ndf_train['median_peaks'] = df_train[peaks].median(axis = 1)\ndf_train['median_no_peaks'] = df_train[no_peaks].median(axis = 1)\ndf_test['median_peaks'] = df_test[peaks].median(axis = 1)\ndf_test['median_no_peaks'] = df_test[no_peaks].median(axis = 1)\n\ndf_train['mean_peaks'] = df_train[peaks].mean(axis = 1)\ndf_train['mean_no_peaks'] = df_train[no_peaks].mean(axis = 1)\ndf_test['mean_peaks'] = df_test[peaks].mean(axis = 1)\ndf_test['mean_no_peaks'] = df_test[no_peaks].mean(axis = 1)\n\ndf_train['std_peaks'] = df_train[peaks].std(axis = 1)\ndf_train['std_no_peaks'] = df_train[no_peaks].std(axis = 1)\ndf_test['std_peaks'] = df_test[peaks].std(axis = 1)\ndf_test['std_no_peaks'] = df_test[no_peaks].std(axis = 1)\n\ndf_train['sum_peaks'] = df_train[peaks].sum(axis = 1)\ndf_train['sum_no_peaks'] = df_train[no_peaks].sum(axis = 1)\ndf_test['sum_peaks'] = df_test[peaks].sum(axis = 1)\ndf_test['sum_no_peaks'] = df_test[no_peaks].sum(axis = 1)\n\ndf_train['min_peaks'] = df_train[peaks].min(axis = 1)\ndf_train['min_no_peaks'] = df_train[no_peaks].min(axis = 1)\ndf_test['min_peaks'] = df_test[peaks].min(axis = 1)\ndf_test['min_no_peaks'] = df_test[no_peaks].min(axis = 1)\n\ndf_train['max_peaks'] = df_train[peaks].max(axis = 1)\ndf_train['max_no_peaks'] = df_train[no_peaks].max(axis = 1)\ndf_test['max_peaks'] = df_test[peaks].max(axis = 1)\ndf_test['max_no_peaks'] = df_test[no_peaks].max(axis = 1)\n\ndf_train['skew_peaks'] = df_train[peaks].skew(axis = 1)\ndf_train['skew_no_peaks'] = df_train[no_peaks].skew(axis = 1)\ndf_test['skew_peaks'] = df_test[peaks].skew(axis = 1)\ndf_test['skew_no_peaks'] = df_test[no_peaks].skew(axis = 1)","ac437400":"scaler = StandardScaler()\n\nfloat_columns = [feats for feats in df_train.select_dtypes('float')]\n\ndf_train[float_columns] = scaler.fit_transform(df_train[float_columns])\ndf_train = df_train.drop('id', axis = 1)\ndf_test = pd.DataFrame(scaler.transform(df_test[float_columns]), columns = df_test[float_columns].columns)","145238d9":"X = df_train.copy()\ny = X.pop('target')\nX_test = df_test.copy()\n\ndel train, test\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.3,\n                                                      random_state =0, stratify = y)","e36ecc37":"lgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_valid, y_valid, reference = lgb_train)\n    \n\ndef objective(trial):\n    params = {\n        'metric': 'auc',\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'feature_pre_filter': False,\n        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.02),\n        'max_bin': trial.suggest_int('max_bin', 64, 255),\n        'num_leaves': trial.suggest_int('num_leaves', 8, 32),\n        'device': 'gpu',\n    }\n    \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_eval = lgb.Dataset(X_valid, y_valid, reference = lgb_train)\n    \n    model = lgb.train(params,\n                      lgb_train,\n                      valid_sets = [lgb_train, lgb_eval],\n                      verbose_eval = 10,\n                      num_boost_round = 1000,\n                      early_stopping_rounds = 10)\n    \n    y_pred_valid = model.predict(X_valid, num_iteration = model.best_iteration)\n    \n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_valid, y_pred_valid)\n    score = auc(false_positive_rate, true_positive_rate)\n    return score\n\nstudy = optuna.create_study(direction = 'maximize', sampler = optuna.samplers.RandomSampler(seed = 0))\nstudy.optimize(objective, n_trials = 30)","e55793d6":"study.best_params","5b661b06":"params = {\n    'metric': 'auc',\n    'lambda_l1': study.best_params['lambda_l1'],\n    'lambda_l2': study.best_params['lambda_l2'],\n    'feature_fraction': study.best_params['feature_fraction'],\n    'bagging_fraction': study.best_params['bagging_fraction'],\n    'bagging_freq': study.best_params['bagging_freq'],\n    'min_child_samples': study.best_params['min_child_samples'],\n    'feature_pre_filter': False,\n    'learning_rate': study.best_params['learning_rate'],\n    'num_leaves': study.best_params['num_leaves'],\n    'max_bin': study.best_params['max_bin'],\n    'device': 'gpu',\n}\n\n\nlgb_train = lgb.Dataset(X_train, y_train,)\nlgb_eval = lgb.Dataset(X_valid, y_valid, reference = lgb_train,)\n\nmodel = lgb.train(params,\n                  lgb_train,\n                  valid_sets = [lgb_train, lgb_eval],\n                  verbose_eval = 10,\n                  num_boost_round = 1000,\n                  early_stopping_rounds = 10)\n\n\ny_pred = model.predict(X_test, num_iteration = model.best_iteration)","0e498734":"sub = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/sample_submission.csv\")\nsub['target'] = y_pred\nsub.to_csv('submission_1st_trial.csv', index = False)","6dfb5efb":"sub.head()","65046bf4":"## Data Concatenation","f5ef6f6e":"# Data Visualization\n\n---\n\n## Features Distribution","12747608":"## Feature Engineering","6fd68ca0":"# Data Preparation\n\n---\n\n## Data Extraction","7fbc8675":"## Null Check"}}