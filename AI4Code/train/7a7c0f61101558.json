{"cell_type":{"46c79d65":"code","a7a4b583":"code","52805985":"code","e86fe10a":"code","6020f04f":"code","8f9dbfcf":"code","3729c1e4":"code","fbf57320":"code","36b8be74":"code","af9ef3d1":"code","afe5cb75":"code","ceb04f29":"code","b262e5db":"code","10562552":"code","fa02fbda":"markdown","857437d1":"markdown","7bf0288d":"markdown","3a965bdf":"markdown","bcce3a7a":"markdown","30228085":"markdown","70a53e1c":"markdown"},"source":{"46c79d65":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport gc\nimport os\nimport pickle\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom keras.utils.np_utils import to_categorical\nfrom keras.optimizers import RMSprop\nfrom keras.models import Sequential\nfrom keras import preprocessing\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler","a7a4b583":"cats_paths = []\ncats_path = '\/kaggle\/input\/cat-and-dog\/training_set\/training_set\/cats'\nfor path in os.listdir(cats_path):\n    if '.jpg' in path:\n        cats_paths.append(os.path.join(cats_path, path))\n\ndogs_paths = []\ndogs_path = '\/kaggle\/input\/cat-and-dog\/training_set\/training_set\/dogs'\nfor path in os.listdir(dogs_path):\n    if '.jpg' in path:\n        dogs_paths.append(os.path.join(dogs_path, path))\n\nlen(cats_paths)","52805985":"# Load data\ndata = np.zeros((8000, 150, 150, 3), dtype='float32')\nfor i in range(8000):\n    if i < 4000:\n        path = dogs_paths[i]\n        img = preprocessing.image.load_img(path, target_size=(150, 150))\n        data[i] = preprocessing.image.img_to_array(img)\n    else:\n        path = cats_paths[i - 4000]\n        img = preprocessing.image.load_img(path, target_size=(150, 150))\n        data[i] = preprocessing.image.img_to_array(img)","e86fe10a":"# Normalisation\ndata = data\/255","6020f04f":"# Target vector\nY = np.concatenate((np.zeros(4000), np.ones(4000)))\nY = to_categorical(Y, num_classes = 2)\nprint(Y)","8f9dbfcf":"X_train, X_cv, Y_train, Y_cv = train_test_split(data, Y, test_size = 0.1, random_state=1234)","3729c1e4":"plt.imshow(X_train[0])\nprint(np.argmax(Y_train[0]))","fbf57320":"datagen = ImageDataGenerator(rotation_range = 10,\n                             zoom_range = 0.2,\n                             width_shift_range = 0.2,\n                             height_shift_range = 0.2,\n                             shear_range = 0.2,\n                             horizontal_flip = True,\n                             vertical_flip = False)","36b8be74":"model = Sequential()\n\n\n# 2x couches de convolution + Max pooling layer\nmodel.add(Conv2D(filters = 32, kernel_size = 3, activation = 'relu', input_shape = (150,150,3)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 32, kernel_size = 3, activation = 'relu'))\nmodel.add(BatchNormalization())\n\n# model.add(Conv2D(filters = 32, kernel_size = 3, strides = 2, activation ='relu'))\nmodel.add(MaxPool2D((2, 2)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\n# 2x couches de convolution + Max pooling layer\nmodel.add(Conv2D(filters = 64, kernel_size = 3, activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 64, kernel_size = 3, activation = 'relu'))\nmodel.add(BatchNormalization())\n\n# model.add(Conv2D(filters = 64, kernel_size = 3, strides = 2, activation ='relu'))\nmodel.add(MaxPool2D((2, 2)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\n# 2x couches de convolution + Max pooling layer\nmodel.add(Conv2D(filters = 128, kernel_size = 3, activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 128, kernel_size = 3, activation = 'relu'))\nmodel.add(BatchNormalization())\n\n# model.add(Conv2D(filters = 64, kernel_size = 3, strides = 2, activation ='relu'))\nmodel.add(MaxPool2D((2, 2)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\n\n# Couche 2D --> 1D + Fully-connected + Dropout + Fully-connected\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation = \"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\nmodel.add(Dense(2, activation = \"softmax\"))\n\n# R\u00e9sum\u00e9 de notre mod\u00e8le\nmodel.summary()","af9ef3d1":"# Compile the model\nmodel.compile(optimizer = \"adam\" , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n# Set a learning rate annealer\nannealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)","afe5cb75":"# Fit the model\nhistory = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=64), validation_data = (X_cv,Y_cv),\n                              epochs = 50,\n                              steps_per_epoch = X_train.shape[0]\/\/64,\n                              verbose = True,\n                              callbacks=[annealer])\n\nwith open('model.pickle', 'wb') as file:\n    pickle.dump(model, file)","ceb04f29":"# Check model performance\nprint('Validation accuracy = ' + str(round(max(history.history['val_accuracy']), 4)))\n\nplt.figure(1, figsize=(10,5))\nplt.plot(history.history['loss'], color='blue', label='training loss')\nplt.plot(history.history['val_loss'], color='red', label='validation loss')\nplt.legend()\nplt.grid()\n\nplt.figure(2, figsize=(10,5))\nplt.plot(history.history['accuracy'], color='blue', label='training accuracy')\nplt.plot(history.history['val_accuracy'], color='red', label='validation accuracy')\nplt.legend()\nplt.grid()","b262e5db":"# On arrondit \u00e0 l'entier le plus proche\npredictions = np.argmax(model.predict(X_cv), axis=1)\n\nconfusion = confusion_matrix(np.argmax(Y_cv, axis=1), predictions)\nplt.figure(1, figsize=(10,7))\nsns.heatmap(confusion, annot=True, cmap='PuBu')\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\n","10562552":"# D\u00e9finition d'une dataframe avec les predictions et vrais labels\ndf_predictions = pd.DataFrame({'prediction' : predictions,\n                               'actual': np.argmax(Y_cv, axis=1)})\n\n# On ajoute la colonne 'correct_pred' qui nous dit si la pr\u00e9diction est bonne\ndf_predictions['correct_pred'] = np.where(df_predictions.prediction == df_predictions.actual, 1, 0)\n\n# on cr\u00e9e un array de tous les index avec erreur sur la validation\nerror_index = df_predictions.loc[df_predictions.correct_pred == 0, :].index\n\n# Maintenant on peut plot les chiffres o\u00f9 on s'est tromp\u00e9\nplt.figure(1, figsize=(15,15))\nfor i in range(16):\n    plt.subplot(4, 4, i+1)\n    plt.imshow(X_cv[error_index[i],:,:,:])\n    plt.title('actual : ' + str(df_predictions.loc[error_index[i]].actual) + ' - predicted : ' + str(df_predictions.loc[error_index[i]].prediction))","fa02fbda":"### Plot des erreurs","857437d1":"* 32C3-P2 - 2x[64C3-P2] - 100 : 0.785\n* 32C3-P2 - 2x[64C3-P2] - 100 | **adam** : 0.806\n* 2x[32C5] - 32C5S2 - D(0.2) - 2x[64C5] - 64C5S2 - D(0.2) - 1024 | **adam + DA(0.2)** [double C5 layer + learnable polling layer] : 0.8238\n* 2x[32C3] - P2 - D(0.2) - 2x[64C3] - P2 - D(0.2) - 1024 | **adam + DA(0.2)** [MaxPooling + filter 3x3] : 0.8587\n* 2x[32C3] - P2 - D(0.2) - 2x[64C3] - P2 - D(0.2) - 2x[128C3] - P2 - D(0.2) - 1024 | **adam + DA(0.2)** [3x double_conv+pool] : 0.9","7bf0288d":"### D\u00e9finition du mod\u00e8le","3a965bdf":"### Confusion matrix analysis","bcce3a7a":"### S\u00e9paration train \/ cv \/ test\n* 80% train\n* 10% cross validation\n* 10% test","30228085":"![garageISEP.png](attachment:garageISEP.png)","70a53e1c":"### Data augmentation"}}