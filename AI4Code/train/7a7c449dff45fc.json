{"cell_type":{"07d4f03f":"code","1ba4b9a3":"code","3e6f2d0b":"code","c9873cef":"code","d42aa705":"code","c429d697":"code","5e8a6ac7":"code","4ed43c8a":"code","eac550a3":"code","52383b30":"code","0b3aa85f":"code","0044beea":"code","ba52a145":"code","e3c5fc37":"code","d8fb58ba":"code","8f09cf47":"code","eb72f5bf":"code","a7e701d0":"code","7e74cad2":"code","d129ef09":"code","edafeb9d":"code","e977b01b":"code","6d6eaa69":"code","7b457833":"code","624cf00f":"code","59e35247":"code","06bb9792":"code","39386454":"code","33fb9bd5":"code","bf771c0e":"code","fb04eb70":"code","56c429af":"code","d44988b3":"code","ee32449b":"code","3b858732":"code","642be83b":"code","41bbdb7f":"code","f86335e1":"code","7280d93e":"code","cb05e7d4":"code","7c35c87b":"code","33c1c0cb":"code","4393d010":"code","27e6d6f1":"code","b3e1b3cd":"code","407ea478":"code","167f3cb5":"markdown","a2b6ec0a":"markdown","c7cc6333":"markdown","287ba602":"markdown","70c30829":"markdown","5d177275":"markdown","1cdcc13a":"markdown","5d6b2a78":"markdown","61cd192b":"markdown","07319f50":"markdown","2752e546":"markdown","618c6734":"markdown","347c17df":"markdown","68f58de4":"markdown","be543e74":"markdown","8a639f9c":"markdown","a5017ad7":"markdown","924ef23e":"markdown","e8b12ef5":"markdown","63964af9":"markdown","c619c991":"markdown","cd752a0b":"markdown","ca85b29b":"markdown","d1f4aaf9":"markdown"},"source":{"07d4f03f":"import numpy as np           # used for advanced mathematical opertion.\nimport pandas as pd          # used for analysing and handling data.\n\n# for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# for ignoring warning\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","1ba4b9a3":"df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf.head()","3e6f2d0b":"df.shape           # shape of dataset (i.e... 5110 records with 12 features columns)","c9873cef":"df.info()          # Summary of a DataFrame","d42aa705":"df.describe()       # Statistical data of the numerical features of a DataFrame","c429d697":"df.describe(include='O').T","5e8a6ac7":"df.isna().sum()","4ed43c8a":"plt.figure(figsize=(18,6))\ng = sns.barplot(x=df.columns, y=df.isna().sum(), palette='Pastel2')\nplt.xticks(rotation=90)\nplt.title('Missing Values', size=16, color = '#025955')\ng.set(ylim=(0,250))\nfor p in g.patches:\n    g.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.4, p.get_height()+20),ha='center', va='bottom',\n               color= 'black')\n    \nplt.show()\nplt.tight_layout()","eac550a3":"from sklearn.impute import KNNImputer","52383b30":"imputer = KNNImputer(n_neighbors = 5)\ndf['bmi'] = imputer.fit_transform(df[['bmi']])","0b3aa85f":"df.isnull().sum()","0044beea":"df.head()","ba52a145":"plt.figure(figsize=(18,8))\nplt.subplot(1,2,1)\nvc = df['stroke'].value_counts()\ng = sns.barplot(x=vc.index,y=vc, palette='Pastel2')\nfor p in g.patches:\n    g.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.4, p.get_height()+20),ha='center', va='bottom',\n               color= 'black')\nplt.title('Count of Stroke')\nplt.subplot(1,2,2)\ncolors = ['#CFD6E4', '#EFCFE3', '#E4F0CF', '#F3CFB6', '#B9DCCC']\ndf['stroke'].value_counts().plot(kind='pie', explode=[0.1,0], autopct='%.2f%%', colors=colors)\nplt.title('Distribution of Stoke')\nplt.show()","e3c5fc37":"col_num=[]\nfor x in df.columns:\n    if (df[x].dtype=='int64') | (df[x].dtype == 'float64'):\n        col_num.append(x)\ncol_num","d8fb58ba":"cols = df[['age','hypertension', 'heart_disease', 'avg_glucose_level', 'bmi']]\ncols.head()","8f09cf47":"plt.figure(figsize=(20,30), facecolor = 'white')\nplot = 1\n\nfor column in cols:\n    if plot <= 15:\n        ax = plt.subplot(5,3,plot)\n        sns.kdeplot(cols[column])\n        plt.xlabel(column,fontsize=20)\n    \n    plot = plot + 1\nplt.tight_layout()","eb72f5bf":"plt.figure(figsize=(30,30))\n\ncol= ['gender','ever_married','work_type','Residence_type','smoking_status']\ni = 1\nfor a in col:\n    \n    plt.subplot(3,2,i)\n    g = sns.countplot(x=a,data=df,palette='Pastel2')\n    for p in g.patches:\n        g.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.2, p.get_height()+20),ha='center', va='bottom',\n                   color= 'black')\n        plt.xlabel(a,fontsize=30)\n    i = i+1\n    \nplt.tight_layout()","a7e701d0":"plt.figure(figsize=(30,30))\n\ncol= ['gender','ever_married','work_type','Residence_type','smoking_status']\ni = 1\nfor a in col:\n    \n    plt.subplot(3,2,i)\n    g = sns.countplot(x=a,hue='stroke',data=df,palette='Pastel2')\n    for p in g.patches:\n        g.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.2, p.get_height()+20),ha='center', va='bottom',\n                   color= 'black')\n        plt.xlabel(a,fontsize=30)\n        \n    i = i+1\n    \nplt.tight_layout()","7e74cad2":"plt.figure(figsize=(24,10))\n\ncols= ['hypertension', 'heart_disease']\ni = 1\nfor a in cols:\n    \n    plt.subplot(2,1,i)\n    g = sns.countplot(x=a,hue='stroke',data=df,palette='Pastel2')\n    for p in g.patches:\n        g.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.08, p.get_height()+20),ha='center', va='bottom',\n                   color= 'black')\n        plt.xlabel(a,fontsize=20)\n        \n    i = i+1\n    \n#plt.tight_layout()","d129ef09":"gender = pd.get_dummies(df[['gender']] , drop_first=True)\nmarried = pd.get_dummies(df[['ever_married']] , drop_first=True)\nwork = pd.get_dummies(df[['work_type']] , drop_first=True)\nresidence = pd.get_dummies(df[['Residence_type']] , drop_first=True)\nsmoking = pd.get_dummies(df[['smoking_status']] , drop_first=True)","edafeb9d":"categorical_features = []\nfor x in df.columns:\n    if df[x].dtype=='object':\n        categorical_features.append(x)\ncategorical_features","e977b01b":"# concatinating columns\ndata = pd.concat([df,gender,married,work,residence,smoking] , axis =1 )\ndata.head()","6d6eaa69":"data = data.drop(columns = categorical_features , axis = 1)\ndata = data.drop(columns = 'id' , axis = 1)","7b457833":"data.head()\n","624cf00f":"corr = data.corr()['stroke'].sort_values(ascending=False).to_frame()\nplt.figure(figsize=(2,8))\nsns.heatmap(corr, cmap='Blues', cbar=False , annot=True)\nplt.show()","59e35247":"x = data.drop('stroke' , axis = 1)\ny = data['stroke']","06bb9792":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx = scaler.fit_transform(x)","39386454":"print(x.shape)\nprint(y.shape)","33fb9bd5":"from sklearn.model_selection import train_test_split\n\n#Split the data into test and train\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33 , random_state = 42) ","bf771c0e":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","fb04eb70":"from sklearn.linear_model import LogisticRegression\nlg = LogisticRegression()\nlg.fit(x_train,y_train)\ny_pred = lg.predict(x_test)","56c429af":"lg.score(x_test,y_test)","d44988b3":"from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\n\nplt.figure(figsize=(6, 6))\nax = plt.subplot()\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm, annot=True, ax = ax, fmt = 'g' ,cmap=plt.cm.Blues)\nax.set_xlabel('Predicted label')\nax.set_ylabel('Actual label')\nplt.show()\n\nprint(classification_report(y_test,y_pred))\nprint(\"Accuracy of The Model :\",accuracy_score(y_test,y_pred)*100)","ee32449b":"from imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state=42)\n\nx_sm , y_sm = sm.fit_resample(x,y)","3b858732":"from sklearn.model_selection import train_test_split\n\n#Split the data into test and train\nx_train, x_test, y_train, y_test = train_test_split(x_sm, y_sm, test_size=0.33 , random_state = 42)  ","642be83b":"from sklearn.linear_model import LogisticRegression\nlg = LogisticRegression()\nlg.fit(x_train,y_train)\ny_pred = lg.predict(x_test)","41bbdb7f":"lg.score(x_test,y_test)","f86335e1":"from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\n\nplt.figure(figsize=(6, 6))\nax = plt.subplot()\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm, annot=True, ax = ax, fmt = 'g' ,cmap=plt.cm.Blues)\nax.set_xlabel('Predicted label')\nax.set_ylabel('Actual label')\nplt.show()\n\nprint(classification_report(y_test,y_pred))\nprint(\"Accuracy of The Model :\",accuracy_score(y_test,y_pred)*100)","7280d93e":"from sklearn.svm import SVC\nmodel = SVC()\nmodel.fit(x_train,y_train)\ny_pred = model.predict(x_test)","cb05e7d4":"model.score(x_test,y_test)","7c35c87b":"from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\n\nplt.figure(figsize=(6, 6))\nax = plt.subplot()\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm, annot=True, ax = ax, fmt = 'g' ,cmap=plt.cm.Blues)\nax.set_xlabel('Predicted label')\nax.set_ylabel('Actual label')\nplt.show()\n\nprint(classification_report(y_test,y_pred))\nprint(\"Accuracy of The Model :\",accuracy_score(y_test,y_pred)*100)","33c1c0cb":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'C' : [0.1 , 1 , 10 , 100 , 1000],\n    'gamma' : [1 , 0.1 , 0.01 , 0.001 , 0.0001],\n    'kernel' : ['rbf']\n}\n\ngrid = GridSearchCV(SVC() , param_grid , refit = True , verbose = 3)\n\ngrid.fit(x_train , y_train)","4393d010":"grid.best_params_","27e6d6f1":"from sklearn.svm import SVC\nmodel = SVC(C=10 , gamma = 1 , kernel = 'rbf')\nmodel.fit(x_train,y_train)\ny_pred = model.predict(x_test)","b3e1b3cd":"model.score(x_test,y_test)","407ea478":"from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\n\nplt.figure(figsize=(6, 6))\nax = plt.subplot()\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm, annot=True, ax = ax, fmt = 'g' ,cmap=plt.cm.Blues)\nax.set_xlabel('Predicted label')\nax.set_ylabel('Actual label')\nplt.show()\n\nprint(classification_report(y_test,y_pred))\nprint(\"Accuracy of The Model :\",accuracy_score(y_test,y_pred)*100)","167f3cb5":"## Correlation","a2b6ec0a":"# Visualizing countplot for :\n    gender\n    ever_married\n    work_type\n    Residence_type\n    smoking_status\n\n      w.r.t stroke","c7cc6333":"# About Target Feature ' Stroke '\n    As we can see from above Countplot and Pie-plot that this feature column is absoultely im-balanced.\n    We cannot give this columns directly to model building which will leads to mis-classification.\n    So in order to make it balanced we need to use some sampling technique","287ba602":"## Checking Missing Values using \" Bar Plot \"","70c30829":"# Pre-processing\n## Handling Categorical Data\n## Coverting categorical features into numerical using Dummy","5d177275":"# Importing Libraries","1cdcc13a":"# Best parameter we get for SVC\n    C : 10\n    gamma : 1\n    kernel : rbf\n## Applying SVC with best parameter we get from GridSearchCV","5d6b2a78":"# EDA (Exploratory Data Analysis)\n## Checking for Null values","61cd192b":"# Visualizing countplot for :\n    gender\n    ever_married\n    work_type\n    Residence_type\n    smoking_status\n\n      w.r.t their unique categories present in it.","07319f50":"# GridSearchCV\n## Using GridSearchCV to find best parameter for SVC (Tuning SVC)","2752e546":"# Handling missing value using KNNImputer","618c6734":"# Suport Vector Classifier","347c17df":"# SMOTE\n    Synthetic Minority Oversampling Technique) is one of the most commonly used oversampling methods to solve the imbalance problem. It aims to balance class distribution by randomly increasing minority class examples by replicating them.","68f58de4":"# Logistic Regression using SMOTE","be543e74":"# Scaling Data\n## Using StandardScaler to scale the train and test sets into scaled versions.\n    Scaling is used to ensure uniformity across the dataset ( means... not to get bais on certain features )\n    We need to fit as well as transform for training part\n    And for testing part we only need to transform the data","8a639f9c":"# About Stroke :\n    A stroke occurs when the blood supply to part of your brain is interrupted or reduced, preventing brain tissue from getting oxygen and nutrients. Brain cells begin to die in minutes. A stroke is a medical emergency, and prompt treatment is crucial.\n    \n    According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.","a5017ad7":"# Visualization\n## Visualizing the distribution of Stroke","924ef23e":"# Logistic Regression","e8b12ef5":"# Building Model","63964af9":"## Train Test Split","c619c991":"## Checking whether data is Normal Distribution or not","cd752a0b":"# Visualizing countplot for :\n    hypertension\n    heart_disease\n\n      w.r.t stroke","ca85b29b":"# Accuracy of The Model : 94.61%","d1f4aaf9":"## Dropping unnecessary features which will not help while building the model"}}