{"cell_type":{"16f14d6d":"code","7c2bb818":"code","a26b1f0a":"code","5adbdeb3":"code","e3733db0":"code","394049e4":"code","fc46a2c2":"code","2ad28b7f":"code","80b53a52":"code","73597d79":"code","df72d82d":"code","082f59ef":"code","5b552f2a":"code","2994fe58":"code","e50f0c01":"code","0debed40":"code","e311535a":"code","c878063f":"code","c75c9d78":"code","0939e34a":"code","e676b7d6":"code","81065936":"code","13125d6d":"code","8a17c889":"code","0bd773a9":"code","c5970bd2":"code","c3cdd10e":"code","b4df9b13":"code","d81ff5bd":"code","f5e7e9a3":"code","eda5eaba":"code","75e89e1e":"code","e4178302":"code","27531d7e":"code","f9d258cc":"code","5e693dfe":"code","2777313e":"code","acc79797":"code","16667047":"code","5eaa7fe6":"code","41e53d47":"code","f539708b":"code","76fc8be1":"code","09a6b089":"code","698aca31":"code","d4451873":"code","7d1498bb":"code","4699bafd":"code","e80b5ac1":"code","b0ad1a51":"code","2791a59f":"code","9c5a581b":"code","6217556b":"code","ccce3f42":"code","5fb4af0d":"code","2f7a454c":"code","9cdfa819":"code","d3fe12a2":"code","868da8f6":"code","2462805c":"code","e4ed674c":"code","ca065a9c":"code","751684b9":"code","b84e8a6a":"code","fb575a54":"code","88298aaa":"code","e47b8b0c":"markdown","fabda53b":"markdown","fc1902ee":"markdown","f46bb833":"markdown","8e0e1b74":"markdown","b3759eb9":"markdown","8b49af62":"markdown","1d27f33f":"markdown","365562f2":"markdown","9993b807":"markdown","d3e28733":"markdown","515caa76":"markdown","fe709c35":"markdown","9b76f404":"markdown","9f264d0f":"markdown","4519f2c5":"markdown","7155ee4f":"markdown","4e16c744":"markdown","b7250e94":"markdown","af3deab9":"markdown","0b52e917":"markdown","4da22359":"markdown","2971c7f7":"markdown","18839003":"markdown","01e24028":"markdown","acc4757d":"markdown","51048648":"markdown","1d6d8ecd":"markdown","137e0b3e":"markdown","3ec7b700":"markdown","7896f2a9":"markdown","401d8967":"markdown","8af36082":"markdown","5b0e78d2":"markdown","09f25ad5":"markdown","03ca7b11":"markdown","945e7b3b":"markdown","9adfde03":"markdown","b061f338":"markdown","dec9c59c":"markdown","86e8ceba":"markdown","c71ffdcd":"markdown","b614591d":"markdown","d16764b6":"markdown","b35ece5b":"markdown","95f7e5ee":"markdown","9676469d":"markdown","a9f52e44":"markdown","a0001f0f":"markdown","ea523d4b":"markdown","ff995693":"markdown","0427143a":"markdown","ba65bf49":"markdown","7fd810ba":"markdown","0be132b0":"markdown","841ffaa3":"markdown","edc775c6":"markdown","28fbe205":"markdown","53ebdea3":"markdown","f5146bfe":"markdown","37301ce3":"markdown","843907ae":"markdown","ade828ca":"markdown","b5cabfd8":"markdown","13ba4532":"markdown","cbb1594c":"markdown","2f8cc9cc":"markdown","50f30d4a":"markdown"},"source":{"16f14d6d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom geopy.geocoders import Nominatim\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","7c2bb818":"df = pd.read_csv('..\/input\/zomato.csv',low_memory=False)","a26b1f0a":"df.info()","5adbdeb3":"print('Features \\t # unique values\\n')\nfor col in list(df):\n    print(f'{col}:\\t{df[col].nunique()}')","e3733db0":"df.head(2)","394049e4":"df.rename({'approx_cost(for two people)': 'approx_cost_2_people',\n           'listed_in(type)':'listed_in_type',\n           'listed_in(city)':'listed_in_city'\n          }, axis=1, inplace=True)","fc46a2c2":"# approx_cost constains some values of format '1,000' wich could not be directly convert to int\n# we need to have this format '1000' in order to do the convertion\n# We will use the lambda function below to transform '1,000' to '1000' and then to int\nreplace_coma = lambda x: int(x.replace(',', '')) if type(x) == np.str and x != np.nan else x \ndf.votes = df.votes.astype('int')\ndf['approx_cost_2_people'] = df['approx_cost_2_people'].apply(replace_coma)\ndf = df.drop(['url', 'phone'], axis=1)","2ad28b7f":"df.rate.dtype, df.rate[0]","80b53a52":"df.rate.unique()","73597d79":"(df.rate =='NEW').sum(), (df.rate =='-').sum()","df72d82d":"df = df.loc[df.rate !='NEW']\ndf = df.loc[df.rate !='-'].reset_index(drop=True)","082f59ef":"print(f'The new shape of the date is {df.shape}')","5b552f2a":"new_format = lambda x: x.replace('\/5', '') if type(x) == np.str else x\ndf.rate = df.rate.apply(new_format).str.strip().astype('float')\ndf.rate.head()","2994fe58":"def label_encode(df):\n    for col in df.columns[~df.columns.isin(['rate', 'approx_cost_2_people', 'votes'])]:\n        df[col] = df[col].factorize()[0]\n    return df","e50f0c01":"df_encoded = label_encode(df.copy())\ndf_encoded.head()","0debed40":"target = df_encoded.rate.fillna(df_encoded.rate.mean()) # Filling nan values in target by mean","e311535a":"plt.figure(figsize=(10,4))\nsns.distplot(target)\nplt.title('Target distribution')","c878063f":"corr = df_encoded.corr(method='kendall') # kendall since some of our features are ordinal.\ndf_encoded = df_encoded.drop(['rate'], axis=1).fillna(-1) #filling nan values by -1","c75c9d78":"plt.figure(figsize=(10,8))\nsns.heatmap(corr, annot=True)","0939e34a":"from sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import train_test_split","e676b7d6":"def rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))","81065936":"# Helper function for scaling data to range [0, 1]\n# Linear models are very sensitve to outliers, so let's scaled the data.\nminmax = lambda x: (x - x.min())\/(x.max() - x.min())","13125d6d":"x_train, x_test, y_train, y_test = train_test_split(minmax(df_encoded),target, random_state=2)","8a17c889":"lr = LinearRegression(n_jobs=-1)\nsvr = SVR()\nrf = RandomForestRegressor(random_state=44, n_jobs=-1)\nmodels = [lr, svr, rf]\nfor model in models:\n    model.fit(x_train, y_train)\n    pred_model = model.predict(x_test)\n    print(f'The RMSE of {model.__class__.__name__} is {rmse(y_test, pred_model)}')","0bd773a9":"def plot_importances(model, cols):\n    plt.figure(figsize=(12,6))\n    f_imp = pd.Series(model.feature_importances_, index=cols).sort_values(ascending=True)\n    f_imp.plot(kind='barh')","c5970bd2":"plot_importances(rf, list(x_train))","c3cdd10e":"preds_rf = rf.predict(x_test)\npd.Series(preds_rf).plot(kind='hist', label='predictions')\ny_test.reset_index(drop=True).plot(kind='hist', label='true values')\nplt.legend()","b4df9b13":"from sklearn.tree import export_graphviz\nfrom IPython.display import Image","d81ff5bd":"def convert_dot_to_png(model, max_depth=3, feature_names=list(x_train)):\n    export_graphviz(model.estimators_[0], out_file='tree.dot', max_depth=max_depth, feature_names=feature_names, rounded=True)\n    !dot -Tpng tree.dot -o tree.png","f5e7e9a3":"convert_dot_to_png(rf)\nImage('tree.png')","eda5eaba":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer","75e89e1e":"rmse_scoring = make_scorer(rmse, greater_is_better=False)","e4178302":"param_grid = {'n_estimators':[20, 50, 100], 'max_features': [None, 'sqrt', 0.5]}\ngrid_search = GridSearchCV(estimator= rf, param_grid=param_grid, cv=3, n_jobs=-1, scoring=rmse_scoring)\ngrid_search.fit(x_train, y_train)\nNone","27531d7e":"grid_search_pred = grid_search.predict(x_test)\nscore_grid_search = rmse(y_test, grid_search_pred)\nbest_estimator = grid_search.best_estimator_\nprint(f'The best estimator is:\\n {best_estimator} \\n and the it\\'s score is {score_grid_search}')","f9d258cc":"N_ITER=10\ndef run_n_iter(estimator, train, target,test, N_ITER=N_ITER):\n    pred_n_iter = np.zeros((y_test.shape[0],), dtype='float')\n    for i in range(N_ITER):\n        estimator.set_params(random_state= i, )\n        estimator.fit(train, target)\n        pred_n_iter += estimator.predict(test) \/ N_ITER\n    return pred_n_iter\n\npred_n_iter = run_n_iter(best_estimator, x_train, y_train, x_test)\nprint(f'The RMSE of {N_ITER} iterations is {rmse(y_test, pred_n_iter)}')","5e693dfe":"# Defining a helper function for count encoding\ndef count_encoding(df, cat_cols):\n    for col in cat_cols:\n        count = df[col].value_counts()\n        new_colname = col + '_count'\n        df[new_colname] = df[col].map(count)\n    return df","2777313e":"cat_cols = x_train.columns[~x_train.columns.isin(['votes', 'approx_cost_2_people'])]","acc79797":"df_ce = count_encoding(df_encoded.copy(), cat_cols)\nx_train_ce, x_test_ce = df_ce.iloc[x_train.index], df_ce.iloc[x_test.index]","16667047":"new_features = [col for col in list(x_train_ce) if 'count' in col]\nx_train_ce.loc[:, new_features].head()","5eaa7fe6":"pred_n_iter_2 = run_n_iter(best_estimator, x_train_ce, y_train, x_test_ce)\nprint(f'The RMSE of with engineered features is {rmse(y_test, pred_n_iter_2)}')","41e53d47":"# Defining a helper function for One-hot encoding\n# For computational reasons, we will limit the one-hot encoding to \n# features having unique values less or equal 100.\ndef ohe(df, max_nunique_vals=100, drop_encoded_feature=True):\n    for col in list(df):\n        if df[col].nunique() <= max_nunique_vals:\n            dummies = pd.get_dummies(df[col].astype('category'), prefix=col)\n            df = pd.concat([df, dummies], axis=1)\n            if drop_encoded_feature:\n                df.drop(col, axis=1, inplace=True)\n    return df","f539708b":"df_ohe = ohe(df_encoded.copy())\nx_train_ohe, x_test_ohe = df_ohe.iloc[x_train.index], df_ohe.iloc[x_test.index]","76fc8be1":"x_train_ohe.iloc[:, 8:].head()","09a6b089":"best_estimator.set_params(random_state=5, n_jobs=-1) # we add n_jobs to speed up the computation.\nbest_estimator.fit(x_train_ohe, y_train)\npred_n_iter_3 = best_estimator.predict(x_test_ohe)\nprint(f'The RMSE of with engineered features is {rmse(y_test, pred_n_iter_3)}')","698aca31":"def get_lat_lon(df):\n    # modified code from https:\/\/www.kaggle.com\/shahules\/zomato-complete-eda-and-lstm-model\n    locations=pd.DataFrame({\"Name\":df['location'].unique()})\n    locations['Name']=locations['Name'].apply(lambda x: \"Bangalore \" + str(x))\n    lat=[]\n    lon=[]\n    geolocator=Nominatim(user_agent=\"app\")\n    for location in locations['Name']:\n        location = geolocator.geocode(location)\n        if location is None:\n            lat.append(np.nan)\n            lon.append(np.nan)\n        else:    \n            lat.append(location.latitude)\n            lon.append(location.longitude)\n    locations['lat']=lat\n    locations['lon']=lon\n    return locations","d4451873":"locations = get_lat_lon(df)\n# Merging the coordiantes data to the original one\ndf_coord = df_encoded.copy()\nunique_locations = df_coord.location.unique()\ndf_coord['lat'] = df_coord.location.replace(unique_locations, locations.lat)\ndf_coord['lon'] = df_coord.location.replace(unique_locations, locations.lon)\ndf_coord.iloc[:, -5:].head()","7d1498bb":"df_coord = df_coord.fillna(-1)\nx_train_coord, x_test_coord = df_coord.iloc[x_train.index], df_coord.iloc[x_test.index]","4699bafd":"best_estimator.fit(x_train_coord, y_train)\npred_n_iter_4 = best_estimator.predict(x_test_coord)\nprint(f'The RMSE of with coordinates features is {rmse(y_test, pred_n_iter_4)}')","e80b5ac1":"# Intalling the package\n# If not install in your kaggle docker, uncomment to install it.\n# !pip install haversine\nfrom haversine import haversine","b0ad1a51":"# default unit of distance is in km\nlal_bagh_coordinates= (12.9453, 77.5901)\ndf_coord['distance_to_lalbagh_km'] = [haversine((lat, lon), lal_bagh_coordinates)\n                                    for (lat, lon) in df_coord[['lat','lon']].values]\ndf_coord.iloc[:, -5:].head() ","2791a59f":"x_train_dist, x_test_dist = df_coord.iloc[x_train.index,:], df_coord.iloc[x_test.index,:]","9c5a581b":"best_estimator.fit(x_train_dist, y_train)\npred_n_iter_5 = best_estimator.predict(x_test_dist)\nprint(f'The RMSE of with coordinates features and distance to Lal Bagh is {rmse(y_test, pred_n_iter_5)}')","6217556b":"df_coord['nb_review'] = [len(val) for val in df['reviews_list']]\nx_train_dist_coord_review, x_test_dist_coord_review = df_coord.iloc[x_train.index,:], df_coord.iloc[x_test.index,:]\ndf_coord.iloc[:, -5:].head()","ccce3f42":"best_estimator.fit(x_train_dist_coord_review, y_train)\npred_n_iter_6 = best_estimator.predict(x_test_dist_coord_review)\nprint(f'The RMSE coordinates features, distance to Lal Bagh and # of reviews is {rmse(y_test, pred_n_iter_6)}')","5fb4af0d":"import lightgbm as lgb\nimport catboost as cat\nimport xgboost as xgb","2f7a454c":"def run_models(models, x_train, y_train, x_test, y_test):\n    preds = np.zeros((x_test.shape[0], 3), dtype='float')\n    for i, model in enumerate(models):\n        model.fit(x_train, y_train)\n        tmp = model.predict(x_test)\n        print(f'The RMSE of {model.__class__.__name__} is {rmse(y_test, tmp)}')\n        preds[:, i] = tmp\n    return preds","9cdfa819":"clf_lgb = lgb.LGBMRegressor(random_state=97)\nclf_cat = cat.CatBoostRegressor(random_state=2019, verbose=0)\nclf_xgb = xgb.XGBRegressor(random_state=500)\nmodels = [clf_lgb, clf_cat, clf_xgb]\npreds_models = run_models(models, x_train, y_train, x_test, y_test)","d3fe12a2":"preds_models_2 = run_models(models, x_train_dist, y_train, x_test_dist, y_test)","868da8f6":"from scipy.stats import ks_2samp","2462805c":"GBM_models = ['LGB', 'CatBoost', 'XGBoost']\nfor i in range(2):\n    print('The p-value between {0} and {1} is {2}'.format(GBM_models[0], GBM_models[i+1], ks_2samp(preds_models_2[:, 0],\n                                                                                                   preds_models_2[:, i + 1])[1]))","e4ed674c":"print('The p-value between {0} and {1} is {2}'.format('Random forest regressor', GBM_models[0], \n                                                      ks_2samp(preds_rf,preds_models_2[:, 0])[1]))","ca065a9c":"blend_1 = 0.95*preds_rf + 0.05*preds_models_2[:, 0]\nblend_2 = 1.05*preds_rf - 0.05*preds_models_2[:, 0]\nprint(f'The p-value of the blended predictions (1) between RF and LGBM is {rmse(y_test, blend_1)}')\nprint(f'The p-value of the blended predictions (2) between RF and LGBM is {rmse(y_test, blend_2)}')","751684b9":"from sklearn.model_selection import KFold","b84e8a6a":"def cv(model, train, target, n_splits=5):\n    oof = np.zeros((train.shape[0],), dtype='float')\n    kf = KFold(n_splits=n_splits, shuffle=True)\n    scores = pd.Series(np.zeros((n_splits,)))\n    for i, (tr_idx, te_idx) in enumerate(kf.split(train, target)):\n        x_tr, x_te = train.loc[tr_idx], train.loc[te_idx]\n        y_tr, y_te = target[tr_idx], target[te_idx]\n        model.fit(x_tr, y_tr)\n        tmp = model.predict(x_te)\n        oof[te_idx] = tmp\n        scores[i] = rmse(y_te, tmp)\n        print('Fold {} score {}'.format(i, scores[i]))\n    return oof, scores","fb575a54":"# The cross-validation is done on the original data + engineered features (count, coordinates, distance)\noof, scores = cv(best_estimator, df_coord.drop('nb_review', axis=1), target)","88298aaa":"print('Mean score {}, STD {}'.format(scores.mean(),scores.std()))","e47b8b0c":"In this part we will add some features to the data and see how they will impact the score.","fabda53b":"From the result above, the one-hot encoding decreasedthe score! So it's not a good solution here.","fc1902ee":"## Converting votes and approx_cost to numeric","f46bb833":"## Renaming some columns","8e0e1b74":"## plotting the first tree","b3759eb9":"The randomness give us an imporve of **0.0013**! Could we improve the score significantly ? Let's get seriouse with the data!","8b49af62":"## Getting important features","1d27f33f":"The current type of our target is **'O'** i.e object. From the unique values, we can note the value **NEW** which is not a rate. We can also not the format of rates are not the same **number\/5** and **number \/5** (example 4.1\/5 and 3.4 \/5, note the space between 3.4 and \/5). Let's drop instances with rates ='NEW' and '-' which number of instances are respectively 2208 and 69 then we will transform the rate to the format **number**, and type **float**.","365562f2":"Up to now we were using only some part of the data (x_train, y_train) for training and other part for testing (x_test, y_test). By doing this, our models are missing information contained on the test set. To overcome this, the common way is doing cross-validation.","9993b807":"From the results above, light gradient boosting machine method outperformed Catboost and XGBoost! However the score obtained is lower than the one of random forest. Let's run the models with engineered features.","d3e28733":"# Loading packages","515caa76":"The goal of this kernel is to predict the rate of the restaurants in Bangalore. We tried different models and different feature engineering. The best model we obtained is the random forest regressor which outperfomed boosting models (big surprise!). A cross-validation showed that the best estimator performance is stable. Please note that, the code and results of this kernel could be improved!. Indeed, other models (like neural nets) could be tested and a deep hyperparameters tuning could be done. Don't hesitate to fork and try new ideas. ","fe709c35":"## One-hot encoding","9b76f404":"# Final model","9f264d0f":"Now, let's add coordinates data of restaurants to our original data and see if they will be informative.","4519f2c5":"# Converting data to numeric","7155ee4f":"From the figure above, the most important feature is **votes**, then **cuisines** and **name**. Remember when we on the heatmap votes were also the most correlated to our target! Book_table had the most negatively correlation with the feature, however it seems not to be an important feature for random forest. The third most important feature is **name**, my interpretation of the feature of being important is based on the reputation of the restaurant. Suppose you to go to the restaurant X, and you ask someone if it's a god or bad restaurant. Surely the answer will influence whether you go to X or change for another one. One could then say that our data contain famous restaurants and bad ones.","4e16c744":"From the scores above, the random forest regressor outperfomed the linear models. We will keep it as baseline model.","b7250e94":"The data contain 51.717 instances and 17 features from which only one is of type int. One can note, there are missing values in some features (the number of instances is different to 51.717, for example the feature **rate**.\nLet's now check the number of unique values of each feature. In case we have a feature with unique value, it will not be informative for any model.","af3deab9":"# Loading data","0b52e917":"The one-hot encoding is another common feature engineering technic for categorical data. In this process, the we will have a new feature for each feature value such that we have 1 if the is an occurence of the value and 0 otherwise. For example let say we have the attribute A = \\['Yes', 'No'\\] then we will have with the one-hot encoding the 2 new features A_Yes = \\[1, 0\\] and A_No = \\[0, 1\\].","4da22359":"From the heatmap, we can see that some variables are positively correlated and some other negatively. The highest correlation **0.63** is between **name** and **address** and the lowest **-0.44** between **book_table** and **approx_cost_2_people**. Highly correlated features maybe imply redondant information. So we might drop one the them. The correlations that will interest us at this stage are between our target **rate** and the other features. The more a feature is correlated to the target (positively or negatively) the more it could help predicting the target. And from the plot, one can see that the 2 top positively correlated features to the target are **dish_liked**, **vote**. Which means the more a dish is liked and a restaurant receive higher votes the more the rate increase. One can also note from the plot that only one feature **book_table** is negatively correlated to the target. Which means that restaurant in which booking table is needed have low rate.","2971c7f7":"Blending is a technic of combining the predictions of different estimators. Perfect blending would be with estimators having approximately the same score but different distribution.","18839003":"## Tuning hyperparameters","01e24028":"Since the randomness doesn't improve significantly and time consuming for the random forest regressor, we will then run the best estimator with one seed.","acc4757d":"Correlation between features.","51048648":"Let's see how linear models (linear regression and svm regressor) and an ensemble one (random forest regressor) will perform.","1d6d8ecd":"## Plotting true rates VS predicted","137e0b3e":"From the figure above, we can see that the distribution of the rates is not uniform! We have restaurants we high rates while other have low.","3ec7b700":"## Adding # of reviews to the data","7896f2a9":" Let's ckeck if the distribution of our GBM estimators are different. We will use the [**Kolmogorov-Smirnov**](https:\/\/en.wikipedia.org\/wiki\/Kolmogorov%E2%80%93Smirnov_test) statistic (K-test) which is a two-sided test for the null hypothesis that 2 independent samples are drawn from the same continuous distribution. If the K-S statistic is small or the p-value is high, then we cannot reject the hypothesis that the distributions of the two samples are the same.","401d8967":"The goal of this kernel is to predict the rate of each restaurant. To achieve this, we the clean the data, do feature engineering (count encoding, one-hot encoding, coordinates, distances, # reviews), and test different models (linear, svm and boosting). I will not go in detail of some part of the code, if you are interested please let me know in comments and upvote if find it interesting :). Note also that, I skipped some EDA part that could be find in other kernels.","8af36082":"If you like the work done in this kernel, please upvote!","5b0e78d2":"The K-test between the random forest and LGBM is lower than those obtained with GBM models. Let's blend the 2 predictions and see the score even it would not be less than the one obtained our baseline model.","09f25ad5":"Using the distance our score increased a little bit. Let's try now boosting models. For these models, we will use the original data first then plus some engineered features one: count encoded features, coordinates and the distance.","03ca7b11":"## Count encoding","945e7b3b":"Let's check if the new features will increase the score!","9adfde03":"Let's add the number of reviews to the data. This feature could be interesting as the number of reviews could give an idea on the quality of the restaurant. Restaurants with high reviews could ether mean that it is of good quality so that people would like to recommand it or bad quality to aware people of going there.","b061f338":"The 3 previous scores decreased but still lower thant our baseline model.","dec9c59c":"In this part we will tune some hyperparameters of our baseline model and see if it could imporve the score.","86e8ceba":"The best results of the grid search on the number of estimators and the max features is for **n_estimators=100** and **max_features=None**, with a score of 0.11 hence an improve of 0.0126.","c71ffdcd":"Here we will plot the first tree of the random forest regressor, with a depth=3.","b614591d":"Scikit-learn does't have the RSME metric, so let's create it.","d16764b6":"## Blending","b35ece5b":"Seems that we are adding noise to the data (our score decreased!).","95f7e5ee":"# Data cleansing","9676469d":"The coordiantes features have no influence on score! Let's add the distance from the restaurants to the most place to visit in Bangalor (Lal Bagh) I found on internet. We will use the [**harversine**](https:\/\/en.wikipedia.org\/wiki\/Haversine_formula) distance.","a9f52e44":"Let's first look at the top 2 instances of the data.","a0001f0f":"Some of our features are categorical, the count encoding is the process of replacing feature values by their frequencies. It is a common feature engineering technic for categorical data.","ea523d4b":"In this part, we will first split the data into training and test sets. Then we will run a random forest regressor to as baseline model to see how it could perform on this data. The performance of our models will be evaluated by computing the root [**mean squared error**](https:\/\/en.wikipedia.org\/wiki\/Mean_squared_error) (RMSE). This metric represent somehow the standard deviation the errors beetwen the true values and the predicted one. The smaller the RSME the better.","ff995693":"# Conclusion","0427143a":"The p-value of the K-test of the 3 obtained predictions are very low, then we can conclude that the predictions are from the same distribution, hence a blend will not be very effective. Let's now compute the K-test between the predictions of our baseline model and the LGB one (best score of GBM models).","ba65bf49":"The mean score of the cross-validation is quite similar to the one obtained with our baseline model and the standard deviation is low. Hence, the performance of the random forest regressor on this data set and engineered feature is **stable**.","7fd810ba":"## Could randomness improve our score ?","0be132b0":"## Spliting data into train and test","841ffaa3":"## Coordinates data","edc775c6":"Let's run the best model found above with different random seed and see if it could help us improve the score.","28fbe205":"## Boosting models","53ebdea3":"# Information on data","f5146bfe":"A scoring differ from a metric by indicating whether the metric should be minimize or maximize. As the RMSE metric, Scikit-learn doesn't have RMSE scoring. Let's define it.","37301ce3":"# Upvote :)","843907ae":"Here we will label encode each feature for example 'yes' and 'no' will be converted to 0 and 1.","ade828ca":"First, let's convert features **votes** and **approx_cost_2_people** to int and drop the features **url** and **phone**, I don't think if they would help any model to accurately predict the rate. Do you have any idea how they could ?  ","b5cabfd8":"# Feature engineering","13ba4532":"# Modeling","cbb1594c":"The new features seem to not help the model to accurately predict the target. This means that counting the number of occurence of target values is not informative for the model. ","2f8cc9cc":"# Exploratory data analysis (EDA)","50f30d4a":"As can be seen, all the features have non-unique values. However it doesn't mean that there are all informative.\nNote that the number of unique values for features reviews_list and the adress are different from the number of instances in the data. Does it mean that some restaurant are in the same adress receive the same reviews ?"}}