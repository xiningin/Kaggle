{"cell_type":{"b315dd04":"code","7260a318":"code","ccd6ff65":"code","f11a4c96":"code","0474807f":"code","ca0d9fec":"code","0fe3ebb5":"code","3e9bbdb5":"code","5afe0a6e":"code","03ea5441":"code","19cef242":"code","2446b077":"code","1d093a5a":"code","a115e6af":"code","452e2fa1":"code","c62e0934":"code","9fe3d543":"code","98ce6a7b":"code","8091102a":"code","8d49642d":"code","bb6513ab":"code","5f2efe7f":"code","9c3b1fe4":"code","495a2f40":"code","8e387776":"code","4bcee969":"code","e6b3d162":"code","e402fe74":"code","e22a1414":"code","c4b1c796":"code","831c5285":"code","247f2350":"code","a08c1cfb":"code","6305fb62":"code","239d416a":"markdown","d79854d5":"markdown","0d33bd8a":"markdown","16a55674":"markdown","559d16ee":"markdown","88fa2fa7":"markdown","6b26490d":"markdown","5a067bea":"markdown","3f44d9d1":"markdown","73082568":"markdown","cabc26f6":"markdown","1d5b718e":"markdown","dd6a5675":"markdown","7ffdf4fb":"markdown","d9611c3f":"markdown","1050432f":"markdown","6709b22f":"markdown","8c8c61d7":"markdown","88432c78":"markdown","f1ebe879":"markdown","6f0ef573":"markdown","69ad9aaa":"markdown","20825fc6":"markdown","3d7821a7":"markdown","3bad939f":"markdown","bba6b15a":"markdown"},"source":{"b315dd04":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7260a318":"import pandas as pd\nimport numpy as np\nfrom scipy.stats import mode\nfrom sklearn.svm import SVC\nfrom sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.ensemble import AdaBoostClassifier","ccd6ff65":"titanic=pd.read_csv(\"\/kaggle\/input\/train.csv\")\ndf=titanic.copy()\ndf.head()","f11a4c96":"df.isnull().sum()","0474807f":"test=pd.read_csv(\"\/kaggle\/input\/test.csv\")\ntest_df=test.copy()\ntest_df.isnull().sum()","ca0d9fec":"df[\"Initial\"]=df[\"Name\"].str.extract('([A-Za-z]+)\\.')\nprint(df[\"Initial\"].unique())\ndf[\"Initial\"].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],\n                      ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)","0fe3ebb5":"df.groupby(\"Initial\")[\"Age\"].mean()","3e9bbdb5":"df.loc[(df.Age.isnull())&(df.Initial=='Master'),\"Age\"]=5\ndf.loc[(df.Age.isnull())&(df.Initial=='Miss'),\"Age\"]=22\ndf.loc[(df.Age.isnull())&(df.Initial=='Mr'),\"Age\"]=33\ndf.loc[(df.Age.isnull())&(df.Initial=='Mrs'),\"Age\"]=36\ndf.loc[(df.Age.isnull())&(df.Initial=='Other'),\"Age\"]=46","5afe0a6e":"print(df.Embarked.mode())\ndf.Embarked.fillna(\"S\",inplace=True)","03ea5441":"df[\"Family_size\"]=df[\"SibSp\"]+df[\"Parch\"]\ndf[\"Alone\"]=0\ndf.loc[df.Family_size==0,\"Alone\"]=1","19cef242":"df['Age_band']=0\ndf.loc[df['Age']<=16,'Age_band']=0\ndf.loc[(df['Age']>16)&(df['Age']<=32),'Age_band']=1\ndf.loc[(df['Age']>32)&(df['Age']<=48),'Age_band']=2\ndf.loc[(df['Age']>48)&(df['Age']<=64),'Age_band']=3\ndf.loc[df['Age']>64,'Age_band']=4","2446b077":"df['Fare_Range']=pd.qcut(df['Fare'],4)\ndf.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","1d093a5a":"df['Fare_cat']=0\ndf.loc[df['Fare']<=7.91,'Fare_cat']=0\ndf.loc[(df['Fare']>7.91)&(df['Fare']<=14.454),'Fare_cat']=1\ndf.loc[(df['Fare']>14.454)&(df['Fare']<=31),'Fare_cat']=2\ndf.loc[(df['Fare']>31)&(df['Fare']<=513),'Fare_cat']=3","a115e6af":"df['Sex'].replace(['male','female'],[0,1],inplace=True)\ndf['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\ndf['Initial'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4],inplace=True)","452e2fa1":"df.drop(['Name','Age','Ticket','Fare','Cabin','Fare_Range','PassengerId'],axis=1,inplace=True)","c62e0934":"df.describe()","9fe3d543":"df.isnull().sum()","98ce6a7b":"test_df[\"Initial\"]=test_df[\"Name\"].str.extract('([A-Za-z]+)\\.')\nprint(test_df[\"Initial\"].unique())\ntest_df[\"Initial\"].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Dona'],\n                      ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)","8091102a":"test_df.loc[(test_df.Age.isnull())&(test_df.Initial=='Master'),\"Age\"]=5\ntest_df.loc[(test_df.Age.isnull())&(test_df.Initial=='Miss'),\"Age\"]=22\ntest_df.loc[(test_df.Age.isnull())&(test_df.Initial=='Mr'),\"Age\"]=33\ntest_df.loc[(test_df.Age.isnull())&(test_df.Initial=='Mrs'),\"Age\"]=36\ntest_df.loc[(test_df.Age.isnull())&(test_df.Initial=='Other'),\"Age\"]=46\ntest_df.Embarked.fillna(\"S\",inplace=True)","8d49642d":"test_df[\"Family_size\"]=test_df[\"SibSp\"]+test_df[\"Parch\"]\ntest_df[\"Alone\"]=0\ntest_df.loc[df.Family_size==0,\"Alone\"]=1","bb6513ab":"test_df['Age_band']=0\ntest_df.loc[test_df['Age']<=16,'Age_band']=0\ntest_df.loc[(test_df['Age']>16)&(test_df['Age']<=32),'Age_band']=1\ntest_df.loc[(test_df['Age']>32)&(test_df['Age']<=48),'Age_band']=2\ntest_df.loc[(test_df['Age']>48)&(test_df['Age']<=64),'Age_band']=3\ntest_df.loc[test_df['Age']>64,'Age_band']=4","5f2efe7f":"test_df[test_df.Fare.isnull()]","9c3b1fe4":"test_df.loc[(test_df.Pclass==3) & (test_df.Embarked==\"S\"),\"Fare\"].mean()\ntest_df.Fare.fillna(14,inplace=True)","495a2f40":"test_df['Fare_cat']=0\ntest_df.loc[test_df['Fare']<=7.91,'Fare_cat']=0\ntest_df.loc[(test_df['Fare']>7.91)&(test_df['Fare']<=14.454),'Fare_cat']=1\ntest_df.loc[(test_df['Fare']>14.454)&(test_df['Fare']<=31),'Fare_cat']=2\ntest_df.loc[(test_df['Fare']>31)&(test_df['Fare']<=513),'Fare_cat']=3","8e387776":"test_df['Sex'].replace(['male','female'],[0,1],inplace=True)\ntest_df['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\ntest_df['Initial'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4],inplace=True)","4bcee969":"test_df.drop(['Name','Age','Ticket','Fare','Cabin','PassengerId'],axis=1,inplace=True)","e6b3d162":"test_df.describe(include=\"all\")","e402fe74":"test_df.isnull().sum()","e22a1414":"ytrain=df[\"Survived\"]\ndel df[\"Survived\"]","c4b1c796":"xtrain=df.values\nxtest=test_df.values","831c5285":"xtrain.shape,ytrain.shape,xtest.shape","247f2350":"n_estimators=list(range(100,1100,100))\nlearn_rate=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\nhyper={'n_estimators':n_estimators,'learning_rate':learn_rate}\ngd=GridSearchCV(estimator=AdaBoostClassifier(),param_grid=hyper,verbose=True)\ngd.fit(xtrain,ytrain)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","a08c1cfb":"clf=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.05)\nclf.fit(xtrain,ytrain)\nprint(clf.score(xtrain,ytrain))\nypred=clf.predict(xtest)","6305fb62":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": ypred\n    })\nsubmission.to_csv('submission.csv', index=False)","239d416a":"#### We then convert the continous features Age, Fare into categorical features by dividing the data in bands(ranges).","d79854d5":"## Predicting\/Modelling","0d33bd8a":"### The Data after cleaning","16a55674":"#### We fill the ages NULLs in the same way as training data.","559d16ee":"#### We read in the train and test files using pandas. Then we view their heads and find out which features contain NULLs.","88fa2fa7":"#### We handle the NULL in Fare by filling it with the mean fare of class 3 people who embarked from port S.","6b26490d":"#### We extract the Initials of the names i.e the titles of the people. We see there are some spelling mistakes like Mlle,Mme,Ms which shoukd be Miss. Then we replace these spelling mistakes and some titles to get a more succinct list of titles","5a067bea":"#### First we will import all the dependencies and modules we need.","3f44d9d1":"#### We apply a Grid Search on AdaBoostClassifier to get the optimum parameters","73082568":"#### We use the optimum features to train our Classifier and then predict on it.","cabc26f6":"#### We then convert the string data into numerical data.","1d5b718e":"# Test data Cleaning\/Processing","dd6a5675":"#### Finally we drop the extra\/rendundant features","7ffdf4fb":"# Train Data Cleaning\/Processing","d9611c3f":"#### We create the output(ytrain) using the Survived column. We use the rest of the columns of our dataframe as our training features(xtrain, xtest).","1050432f":"#### We then convert the string data into numerical data.","6709b22f":"#### We create new features Family_size, Alone similarly\/","8c8c61d7":"### The Data after cleaning","88432c78":"#### Finally we drop the extra\/rendundant features","f1ebe879":"#### We finally Create the Submission File.","6f0ef573":"#### We then convert the continous features Age, Fare into categorical features by dividing the data in bands(ranges).","69ad9aaa":"#### We extract the Initials of the names i.e the titles of the people. We see there are some spelling mistakes like Mlle,Mme,Ms which shoukd be Miss. Then we replace these spelling mistakes and some titles to get a more succinct list of titles","20825fc6":"#### We fill NULLs in Embarked with S which is the mode of the Embarked feature.","3d7821a7":"#### We create 2 new features:\n1. Family_size, which is the sum of SibSp(no. of siblings) and Parch(no. of parents)\n2. Alone, which has value 1 if the passenger was travelling alone(Family_size=0) and 0 otherwise","3bad939f":"#### We find out the mean ages of each title and then fill the NULLs in age with mean age of that title.","bba6b15a":"## Titanic Voyage\n#### This is my attempt at the classic Titanic challenge. Any and all improvements\/suggestions are welcome!"}}