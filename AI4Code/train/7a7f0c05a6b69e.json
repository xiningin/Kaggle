{"cell_type":{"9eb412ce":"code","6358c25f":"code","ea1e81ac":"code","e1508691":"code","13fafeb9":"code","57aa3b71":"code","ce383e8e":"code","e55b907f":"code","65a1c521":"code","995167c9":"code","ddad4304":"code","8173bd82":"code","4d705b03":"code","947a09a6":"code","ae995168":"code","ec222b41":"code","c0c249b4":"code","c0456088":"code","40afc041":"code","a3596971":"code","24bc2f80":"code","ce82f5d3":"code","8f2dea81":"markdown","c0a534d9":"markdown","6e7fedff":"markdown","ae4345eb":"markdown","efa167d0":"markdown","7c3b27ca":"markdown","abcb264a":"markdown","eea146ad":"markdown"},"source":{"9eb412ce":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \n\nfrom sklearn.utils import shuffle\n\nimport re\nimport nltk\nnltk.download('stopwords')\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\n\nimport keras \nfrom keras.models import Sequential, Model \nfrom keras import layers\nfrom keras.layers import Dense, Dropout, Input, Embedding","6358c25f":"dataset_cols = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\ndataset = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', header=None, encoding='ISO-8859-1', names=dataset_cols)","ea1e81ac":"dataset.shape","e1508691":"dataset.head()","13fafeb9":"word_bank = []\n\ndef preprocess(text):\n    review = re.sub('[^a-zA-Z]',' ',text) \n    review = review.lower()\n    review = review.split()\n    ps = LancasterStemmer()\n    all_stopwords = stopwords.words('english')\n    all_stopwords.remove('not')\n    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n    return ' '.join(review)","57aa3b71":"df = shuffle(dataset,random_state=42)\ndf = df[1:700000]","ce383e8e":"df['target'].value_counts()","e55b907f":"df['text'] = df['text'].apply(lambda x: preprocess(x))","65a1c521":"y = df['target']\nle = LabelEncoder()\ny = le.fit_transform(y)","995167c9":"X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size = 0.15, random_state = 0)","ddad4304":"tfidf = TfidfVectorizer(max_features = 600)\nX_train_tf = tfidf.fit_transform(X_train).toarray() \nX_test_tf = tfidf.transform(X_test).toarray()","8173bd82":"X_train_tf.shape, X_test_tf.shape, y_train.shape, y_test.shape","4d705b03":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ny_pred = gnb.fit(X_train_tf, y_train).predict(X_test_tf)\nprint(\"Number of mislabeled points out of a total %d points : %d\"% (X_test.shape[0], (y_test != y_pred).sum()))","947a09a6":"print(\"Accuracy:\\n\", accuracy_score(y_test, y_pred))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))","ae995168":"lr = LogisticRegression(random_state = 0)\nlr.fit(X_train_tf, y_train) \ny_pred_lr = lr.predict(X_test_tf)\nprint(\"Accuracy:\\n\", accuracy_score(y_test, y_pred_lr))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lr))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_lr))","ec222b41":"from lightgbm import LGBMClassifier\nlgbm = LGBMClassifier()\nlgbm.fit(X_train_tf, y_train)\ny_pred_lgb = lgbm.predict(X_test_tf)\ny_pred_lgb[y_pred_lgb > 0.5] = 1\ny_pred_lgb[y_pred_lgb <= 0.5] = 0\nprint(\"Accuracy:\\n\", accuracy_score(y_test, y_pred_lgb))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lgb))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_lgb))","c0c249b4":"dc = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\ndc.fit(X_train_tf, y_train)\ny_pred_dc = dc.predict(X_test_tf)\nprint(\"Accuracy:\\n\", accuracy_score(y_test, y_pred_dc))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_dc))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_dc))","c0456088":"rf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nrf.fit(X_train_tf, y_train)\ny_pred_rf = rf.predict(X_test_tf)\nprint(\"Accuracy:\\n\", accuracy_score(y_test, y_pred_rf))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))","40afc041":"from keras.models import Sequential\nfrom keras.layers import Dense\nmodel = Sequential()\nmodel.add(Dense(256, activation='relu', input_dim=600))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(12, input_dim=8, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","a3596971":"model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","24bc2f80":"history = model.fit(X_train_tf, y_train, epochs=20, batch_size=32,validation_data=(X_test_tf,y_test))\nloss, accuracy = model.evaluate(X_train_tf, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test_tf, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))","ce82f5d3":"y_pred_new = model.predict(X_test_tf)\ny_pred_nn = np.where(y_pred_new>0.5,1,0)\nprint(\"Accuracy:\\n\", accuracy_score(y_test, y_pred_nn))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_nn))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_nn))","8f2dea81":"# **Logistic Regression**","c0a534d9":"# **Decision Tree**","6e7fedff":"# **Random Forest**","ae4345eb":"# **Classifier NN**","efa167d0":"# **Data Pre-Processing**","7c3b27ca":"# **Light GBM**","abcb264a":"# **Naive Bayes**","eea146ad":"# **Conclusion**\n\n1. Naive Bayes - 0.7026285714285714\n2. Logistic Regression - 0.7362285714285715\n3. Light GBM - 0.6871047619047619\n4. Decision Tree - 0.6845333333333333\n5. Random Forest - 0.7249428571428571\n6. Classifier NN - 0.7284095238095238 {Can be improved by tuning)"}}