{"cell_type":{"57c6e996":"code","0eabb8c1":"code","59ccff8c":"code","a7f79897":"code","839443c2":"code","7d2a25e0":"code","857a456c":"code","b68c3dd2":"code","69586118":"code","8af6702a":"code","e89ec1b6":"code","bc42d173":"code","1ff46c1b":"code","4d0a9990":"code","57152f0f":"code","ae164031":"code","0800cc66":"markdown","4bf61854":"markdown","16d011d6":"markdown","443e0cf2":"markdown","22fad02f":"markdown"},"source":{"57c6e996":"import warnings\nwarnings.filterwarnings('ignore')","0eabb8c1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","59ccff8c":"diamond_data = sns.load_dataset('diamonds')\ndiamond_data.head()","a7f79897":"diamond_data.info()","839443c2":"from sklearn.preprocessing import StandardScaler\n\nfor col_name in diamond_data.select_dtypes(['int64','float64']):\n  if col_name != 'price':\n    diamond_data[col_name] = StandardScaler().fit_transform(diamond_data[[col_name]])","7d2a25e0":"from sklearn.preprocessing import LabelEncoder\n\nfor col_name in diamond_data.select_dtypes('category'):\n  diamond_data[col_name] = LabelEncoder().fit_transform(diamond_data[col_name])","857a456c":"diamond_data.head()","b68c3dd2":"X = diamond_data.drop('price',axis=1)\nY = diamond_data.price","69586118":"from sklearn.model_selection import KFold,cross_val_score\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nLR = LinearRegression()\nDTR = DecisionTreeRegressor()\n# RFR = RandomForestRegressor() Commenting Random forest as it consumes ample ammount of time\nKNR = KNeighborsRegressor()\n\nkf = KFold(n_splits=10,shuffle=True,random_state=10)\n\nprint(f\" Linear Regression : {np.sqrt(-cross_val_score(LR,X,Y,cv=kf,scoring='neg_mean_squared_error').mean())}\")\nprint(f\" Decision Tree : {np.sqrt(-cross_val_score(DTR,X,Y,cv=kf,scoring='neg_mean_squared_error').mean())}\")\n# print(f\" Random Forest : {np.sqrt(-cross_val_score(RFR,X,Y,cv=kf,scoring='neg_mean_squared_error').mean())}\")\nprint(f\" KNeighbors : {np.sqrt(-cross_val_score(KNR,X,Y,cv=kf,scoring='neg_mean_squared_error').mean())}\")","8af6702a":"iris_data = sns.load_dataset('iris')\niris_data.head()","e89ec1b6":"iris_data.info()","bc42d173":"iris_data.species.value_counts()","1ff46c1b":"sns.scatterplot(x='sepal_length',y='sepal_width',hue='species',style='species',data=iris_data);","4d0a9990":"sns.scatterplot(x='petal_length',y='petal_width',hue='species',style='species',data=iris_data);","57152f0f":"X = iris_data.loc[:,['petal_length','petal_width']]\nY = iris_data.species","ae164031":"from sklearn.model_selection import StratifiedKFold,cross_val_score\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nLR_model = LogisticRegression()\nDTC = DecisionTreeClassifier()\nRFC = RandomForestClassifier()\nKNC = KNeighborsClassifier()\nNB = GaussianNB()\n\nskf = StratifiedKFold(n_splits=10,shuffle=True,random_state=10)\n\nprint(f'Logistic Regression Classifier Accuracy is {round((cross_val_score(LR_model,X,Y,cv=skf,scoring=\"accuracy\").mean())*100,2)}')\nprint(f'Decision Tree Classifier Accuracy is {round((cross_val_score(DTC,X,Y,cv=skf,scoring=\"accuracy\").mean())*100,2)}')\nprint(f'Random Forest Classifier Accuracy is {round((cross_val_score(RFC,X,Y,cv=skf,scoring=\"accuracy\").mean())*100,2)}')\nprint(f'KNeighbors Classifier Accuracy is {round((cross_val_score(KNC,X,Y,cv=skf,scoring=\"accuracy\").mean())*100,2)}')\nprint(f'Naive Bayes Classifier Accuracy is {round((cross_val_score(NB,X,Y,cv=skf,scoring=\"accuracy\").mean())*100,2)}')","0800cc66":"# Cross Val Score\n\nIn cross-validation, we run our modeling process on different subsets of the data to get multiple measures of model quality.\n\nFor example, we could begin by dividing the data into 5 pieces, each 20% of the full dataset. In this case, we say that we have broken the data into 5 \"folds\".\n\n\n\n![](https:\/\/drive.google.com\/uc?export=view&id=1pxfmwyYKRNxHcw29qoR0dXCoZaFTEcLC)\n\n\n**`Then, we run one experiment for each fold:`**\n\n1. In Experiment 1, we use the first fold as a validation (or holdout) set and everything else as training data. This gives us a measure of model quality based on a 20% holdout set.\n<br><br>\n2. In Experiment 2, we hold out data from the second fold (and use everything except the second fold for training the model). The holdout set is then used to get a second estimate of model quality.\n<br><br>\n3. We repeat this process, using every fold once as the holdout set. Putting this together, 100% of the data is used as holdout at some point, and we end up with a measure of model quality that is based on all of the rows in the dataset (even if we don't use all rows simultaneously).\n\n\nWhen should you use cross-validation?\n--\n\nCross-validation gives a more accurate measure of model quality, which is especially important if you are making a lot of modeling decisions. However, **`it can take longer to run`**, because it estimates multiple models (one for each fold).","4bf61854":"From above example we would go with Logistic Regression.","16d011d6":"# K-fold for Regression\n\nK-fold defines number of fold and is used for shuffling data. It should be used for regression problems.","443e0cf2":"# Stratified k-fold for classification\n\nStratified k-fold defines number of fold and is used for shuffling data and maintaining proportion of label in each fold. It should be used for classification problems.","22fad02f":"From above example we would go with Decission Tree as it took less time compared to Random Forest and has almost half the RMSE than Linear Regression."}}