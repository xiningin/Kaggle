{"cell_type":{"60e22da6":"code","d5e00b56":"code","7c1f8fa3":"code","47f51ef0":"code","bd8d82d6":"code","11cb3c7d":"code","90d114fe":"code","2a39a3d8":"markdown","4dc4f57a":"markdown"},"source":{"60e22da6":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.impute import SimpleImputer\n\n# Read the data\ndef find_sample(random_state, return_all):\n    X = pd.read_csv('..\/input\/train.csv', index_col='Id')\n    X_test_full = pd.read_csv('..\/input\/test.csv', index_col='Id')\n\n    # Remove rows with missing target, separate target from predictors\n    X.dropna(axis=0, subset=['SalePrice'], inplace=True)\n    y = X.SalePrice              \n    X.drop(['SalePrice'], axis=1, inplace=True)\n\n    # Break off validation set from training data\n    X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                    random_state=random_state, shuffle=True)\n\n    # \"Cardinality\" means the number of unique values in a column\n    # Select categorical columns with relatively low cardinality (convenient but arbitrary)\n    low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                            X_train_full[cname].dtype == \"object\"]\n\n    # Select numeric columns\n    numeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n\n\n    # Keep selected columns only\n    my_cols = low_cardinality_cols + numeric_cols\n    X_train = X_train_full[my_cols].copy()\n    X_valid = X_valid_full[my_cols].copy()\n    X_test = X_test_full[my_cols].copy()\n    \n   \n\n\n    # One-hot encode the data (to shorten the code, we use pandas)\n    X_train = pd.get_dummies(X_train)\n    X_valid = pd.get_dummies(X_valid)\n    X_test = pd.get_dummies(X_test)\n    X_train, X_valid = X_train.align(X_valid, join='left', axis=1)\n    X_train, X_test = X_train.align(X_test, join='left', axis=1)\n    \n    \n    #Removing outliers\n    X_train = X_train[(X_train.GrLivArea<4000) ]\n    y_train = y_train[y_train.index.isin(X_train.index)]\n    \n    X_valid = X_valid[(X_valid.GrLivArea<4000) ]\n    y_valid = y_valid[y_valid.index.isin(X_valid.index)]\n    \n    #missing \n    missing_column = X_test.isnull().all()[lambda x:x].index.tolist()\n    X_test[missing_column] = X_test[missing_column].apply(lambda x: x.fillna(-1))\n    X_train[missing_column] = X_train[missing_column].apply(lambda x: x.fillna(-1))\n    X_valid[missing_column] = X_valid[missing_column].apply(lambda x: x.fillna(-1))\n    X_train = X_train.apply(lambda x: x.fillna(x.mean()))\n    X_valid = X_valid.apply(lambda x: x.fillna(x.mean()))\n    \n    \n    \n    current_model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, gamma=0, importance_type='gain',\n       learning_rate=0.07, max_delta_step=0, max_depth=2,\n       min_child_weight=1, missing=None, n_estimators=1500, n_jobs=-1,\n       nthread=-1, objective='reg:linear', predictor='gpu_predictor',\n       random_state=0, reg_alpha=0, reg_lambda=2, scale_pos_weight=1,\n       seed=None, silent=True, subsample=1)\n    current_model.fit(X_train, y_train)\n    pred =current_model.predict(X_valid)\n    print(\"Mean absolute error \", mean_absolute_error(pred, y_valid))\n    \n    if return_all:\n        return mean_absolute_error(pred, y_valid), current_model, X_train, X_valid, y_train, y_valid, X_test\n    else:\n        return mean_absolute_error(pred, y_valid)\n    ","d5e00b56":"#import warnings\n#warnings.simplefilter(action='ignore', category=FutureWarning)\n#better = dict()\n#found = False\n#used = []\n#while not found: \n#    randrange = random.sample(range(0,10000), 100) \n#    randrange = [i for i in randrange if i not in used]\n#    for i in randrange:\n#        used.append(i)\n#        result = find_sample(i, False)\n#        if result < 12200:\n#            print(\"Quite good result MAE : {}\".format(result))\n#            better[i] = result\n#            if result < 12000:\n#                better[i] = result\n#                found = True\n#                break\n            ","7c1f8fa3":"_,final_model, X_train, X_valid, y_train, y_valid,X_test =  find_sample(3076, True)","47f51ef0":"X_comb = X_train.append(X_valid)\ny_comb = y_train.append(y_valid)\n\nfinal_model.fit(X_comb, y_comb)","bd8d82d6":"predict_test = final_model.predict(X_test)","11cb3c7d":"output = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': predict_test})\noutput.to_csv('submission.csv', index=False)","90d114fe":"!head submission.csv","2a39a3d8":"# Keep going\n\nContinue to learn about **[data leakage](https:\/\/www.kaggle.com\/alexisbcook\/data-leakage)**.  This is an important issue for a data scientist to understand, and it has the potential to ruin your models in subtle and dangerous ways!","4dc4f57a":"---\n**[Intermediate Machine Learning Micro-Course Home Page](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)**\n\n"}}