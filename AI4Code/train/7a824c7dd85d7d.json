{"cell_type":{"45964900":"code","bb116b50":"code","5fc098b4":"code","8620f37c":"code","b3b461df":"code","f7145bbb":"code","d990e7d0":"code","bbb4f93f":"code","4e26daf7":"code","cbabdb16":"code","d4fac3e2":"code","e7cdbd32":"code","0c8290b0":"code","c1cf2657":"code","0a604ad8":"code","f0cdf213":"code","872740c9":"code","ffb8f1e9":"code","22f5cc4c":"code","b2508d96":"code","ed49a531":"code","37f9a86a":"code","2f0c9b14":"code","3d8cfcdc":"code","4ab5cb1a":"code","4858873d":"code","a43fd27e":"code","c1731c5e":"code","f05522c3":"code","011a5b57":"code","2475a81d":"code","eb146a0b":"code","ae2aeec8":"code","ae66e862":"code","94bd31e1":"code","5de4c28b":"code","1e033cca":"code","2ec3103a":"code","2f537446":"code","11cdcc9e":"code","fe440b8a":"code","ba9e9da2":"code","2df79d8c":"code","a7564b3d":"code","1facb898":"code","51b829bb":"code","37000e26":"code","6a2eb872":"code","b3b57292":"code","863257ee":"code","9d468b66":"code","539fd076":"code","b52d588a":"code","a8d18d8b":"code","acd38995":"code","999e1117":"code","fecea2fb":"code","60c3958c":"code","993d40a7":"code","90662868":"code","c29f0948":"code","cbcb6b29":"code","89863eec":"code","19ce8bda":"code","54047b5f":"code","d8c04d4d":"code","7af32dd4":"code","e87aff2b":"code","af090b65":"code","4c1a20e1":"code","4714940c":"code","659285b6":"code","298b435c":"code","16fa2ef8":"code","4a6b6a72":"code","743f4c5a":"code","ebc82413":"code","7c90b248":"code","074bd99b":"code","88e62004":"code","34d2c061":"code","8d932c07":"code","1c6220bd":"code","c6e24017":"code","2f350e21":"code","b036dd2c":"code","374f1591":"code","7252657a":"code","c2499617":"code","12290eab":"code","083d6659":"code","fec15066":"code","91b63302":"code","8159583e":"code","1b1e4dee":"code","2787ce79":"code","4ccfd3d2":"code","1beff063":"code","c4e895f2":"code","bb5ffb07":"code","6af17cc4":"code","d9e197d6":"code","c5e8d044":"code","63b87082":"code","ebdeb19c":"code","27685659":"code","cea03a8b":"code","f5847156":"code","bb7808d2":"code","ea1bf2a5":"code","efb9e863":"code","83e0e08b":"code","4e8228b8":"code","b1c941a6":"code","2109271f":"code","bf545f8e":"code","a871eb07":"code","f5cd4773":"code","811d4a1d":"code","b271273e":"code","25fd64dc":"code","2af9b4cf":"code","2b44d63c":"code","e30d568e":"code","305cb71d":"code","fec3cdaa":"code","734c0eb4":"code","2d7a4d70":"code","4cfd18c8":"code","409cfa36":"code","8c018204":"code","51401747":"code","a3102639":"code","732ee819":"code","abed8e39":"code","f67eaff2":"markdown","fd37994c":"markdown","7e100201":"markdown","83d611ba":"markdown","daaa5fb7":"markdown","bece2eca":"markdown","7209a017":"markdown","ea1d1a9b":"markdown","52984ec6":"markdown","0d4ff400":"markdown","ebbfa54d":"markdown","0457dafb":"markdown","897af8ab":"markdown","aea767e1":"markdown","ea862d6f":"markdown","94e4e198":"markdown","940f0dd8":"markdown","3c91cdf6":"markdown","bd7e4ac4":"markdown","89f6e3fa":"markdown","5a0fa91a":"markdown","b53d95f5":"markdown","3e457f38":"markdown","0bb22176":"markdown","598c4211":"markdown","44bf4c9d":"markdown","f276bf35":"markdown","3e0e9a69":"markdown","1b555429":"markdown","cfd543c9":"markdown","01931acd":"markdown","8347817b":"markdown","81725b36":"markdown","5f818464":"markdown","0ee7efae":"markdown","c57a79b6":"markdown","b0920b7d":"markdown","6698fb6a":"markdown","3cea363a":"markdown","62234627":"markdown","ac113d2f":"markdown","300958a3":"markdown","3642832e":"markdown","bd2db355":"markdown","6ed03a98":"markdown","9522c545":"markdown","24848a3a":"markdown","5a4d5b8c":"markdown","c95f0950":"markdown","1cc318de":"markdown","639817d5":"markdown","4b46dcdf":"markdown","6f3e04d1":"markdown","4b2df647":"markdown","5d154da9":"markdown","b9c913c6":"markdown","4c97dedd":"markdown","d7be31c3":"markdown","b45fb14f":"markdown","d96b86e1":"markdown","a69a4c50":"markdown","914fdfb1":"markdown","20bc9839":"markdown","8c9012f1":"markdown","2bb1c44a":"markdown","ac5542bd":"markdown","2c379a47":"markdown","4813b2eb":"markdown","0d50cadd":"markdown","e30928cb":"markdown","86ad2858":"markdown","eabc63d8":"markdown","50fada28":"markdown","f66ddfb2":"markdown","e22d0841":"markdown","3a315d95":"markdown","010ab630":"markdown","b4601ffc":"markdown","4f007e56":"markdown","23fa5f25":"markdown","6dbf7bbf":"markdown","01c138b0":"markdown","acdd1430":"markdown","294641f0":"markdown","b88bb0f3":"markdown","7be560b3":"markdown","abd26857":"markdown","0c460eab":"markdown","263a1a42":"markdown","6a3e4884":"markdown","49574da2":"markdown","7d4a16da":"markdown","78d16445":"markdown","8c103dfd":"markdown","572399b7":"markdown","0694da11":"markdown","63b3b9c2":"markdown","b4ad7a44":"markdown","89f4337a":"markdown","d68a0507":"markdown","85a90ed4":"markdown","dd61466e":"markdown","f7c9813c":"markdown","3b684d42":"markdown","e252cc1f":"markdown","32f5ef77":"markdown","9573fb4b":"markdown","7de210c2":"markdown","3bafae69":"markdown","9ca10439":"markdown","6fea464b":"markdown","586af65a":"markdown","300129f1":"markdown","76524253":"markdown","3a257eef":"markdown","363168df":"markdown","397149c6":"markdown"},"source":{"45964900":"# General Data Science Modules\nimport numpy as np\nimport pandas as pd\n\n# Plotting Modules\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Basical Modules\nimport math, os, time\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n\n# Machine Learning Classifiers\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier, Perceptron, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, \\\n                             GradientBoostingClassifier, StackingClassifier, VotingClassifier, BaggingClassifier\n\n# Feature and Model Selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import RFE, RFECV","bb116b50":"#df_train = pd.read_csv('train.csv')\n#df_test = pd.read_csv('test.csv')\n# For Kaggle, uncomment:\ndf_train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\ndf = pd.concat([df_train, df_test])\n\n# Here, I'll be saving the test set for the submition\noriginal_test = df_test.copy()","5fc098b4":"df.info()","8620f37c":"df.head(10)","b3b461df":"df.describe()","f7145bbb":"df.PassengerId.unique()","d990e7d0":"df.Survived.unique()","bbb4f93f":"df.Pclass.unique()","4e26daf7":"df.Name.unique()","cbabdb16":"df.Sex.unique()","d4fac3e2":"df.SibSp.unique()","e7cdbd32":"df.Parch.unique()","0c8290b0":"df.Fare.unique()","c1cf2657":"df.Ticket.unique()","0a604ad8":"df.Embarked.unique()","f0cdf213":"df.Cabin.unique()","872740c9":"sns.set_style('white')\nsns.set_palette('coolwarm')\nsns.set(font_scale=1.1)","ffb8f1e9":"# Here, I add 'SibSp' and 'Parch' to the categorical_values because there're only few values \n# Thus I can group them to get some insight\ncategorical_values = ['Pclass', 'Sex', 'Embarked', 'SibSp', 'Parch']\nnumerical_values = ['Age', 'SibSp', 'Parch', 'Fare']\nother_valeus = ['PassengerId', 'Name', 'Ticket', 'Cabin']","22f5cc4c":"for c in categorical_values:\n    # Creating a figure\n    plt.figure(figsize=(7,5))\n    \n    # Getting the total amount of each category\n    vc = df[c].value_counts()\n    vc.index = vc.index.astype(str)\n    vc = vc.sort_index()\n    \n    # Getting the survivors count for each category\n    gc = df.query('Survived == 1').groupby(c)['Survived'].count()\n    gc.index = gc.index.astype(str)\n    gc = gc.sort_index()\n    \n    # Getting the percentages\n    percs = gc \/ vc\n    percs.replace(np.nan, 0, inplace=True)\n    \n    # Plottin the total and survived amounts\n    total_bar = plt.bar(vc.index, vc, label='total')\n    survived_bar = plt.bar(gc.index, gc, color='orange', label='survived')\n    \n    # Plotting the percentages as text on top of each bar\n    for bar, perc in zip(total_bar, percs):\n        plt.text(bar.get_x() + bar.get_width()\/2 - 0.12, bar.get_height() + 20, f'{perc:.2f}%',\n                fontsize='large')\n    \n    # Adding title and legend\n    plt.title(c)\n    plt.legend()\n    y_min, y_max = plt.ylim()\n    plt.ylim((y_min, y_max+30)) # Here, I just add some extra space for the percentages labels\n    plt.show() # Showing each image at the end of the for loop","b2508d96":"sns.set_style('whitegrid')\nfor n in numerical_values:\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n    h = sns.histplot(x=n, data=df, ax=ax1, kde=True)\n    b = sns.boxplot(x=n, data=df, ax=ax2)\n    plt.show()","ed49a531":"sns.set_style('white')\n# Creating the figure\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plotting the first axis (total vs survivors by age)\ndf['Age'].plot.hist(label='total', ax=ax1, bins=15)\ndf.query('Survived == 1')['Age'].plot.hist(label='survived', ax=ax1, color='orange', bins=15)\n\n# Plotting the second axis (male vs female by age)\ndf['Age'].plot.hist(color='g', label='male', ax=ax2, bins=15)\ndf.query('Sex == \"female\"')['Age'].plot.hist(color='pink', ax=ax2, range=(0,80), label='female', bins=15)\n\n# Adding labels, titles and legends\nax1.set_xlabel('Age in year')\nax2.set_xlabel('Age in year')\nax1.set_title('Total vs Survivors by Age')\nax2.set_title('Male vs Female by Age')\nax2.set_ylabel('') # removing an auto added ylabel by pandas\nax1.legend()\nax2.legend()\n\n# Showing the figure\nplt.show()","37f9a86a":"plt.figure(figsize=(9,9))\nsns.set_context('paper', font_scale=1.2)\ncorr = df.drop('PassengerId', axis='columns').corr()\nsns.heatmap(corr, annot=True, cmap='coolwarm', linewidths=1,linecolor='black', fmt='.2f',\n            annot_kws={'fontsize':'large', 'fontweight':'semibold'},\n            cbar_kws={'extend':'both'}, square=True)\nplt.title('Correlation between numerical values');","2f0c9b14":"df['Surname'] = df['Name'].str.split(',').str[0]\ndf[['Survived', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Surname']].sample(10)","3d8cfcdc":"def def_rare(title):\n    \"\"\"\n    This function defines if a title is rare or not on the data. \n    If the title appers less than 10 times, it's considered a rare title, and,\n    thus, is renamed to 'rare'. \n    Otherwise, we keep it the same.\n    \"\"\"\n    global vc\n    if vc[title] < 10:\n        return 'rare'\n    return title.strip()\n\ntitle = df['Name'].str.split(',').str[1].str.split('.').str[0]\nvc = title.value_counts()\ndf['Title'] = title.apply(def_rare)\nprint(f'Unique values for title: {df.Title.unique()}')\ndf[['Survived', 'Name', 'Sex', 'Age', 'Surname', 'Title']].sample(10)","4ab5cb1a":"df['AgeMissing'] = df['Age'].isna().astype(int)\ndf[['Survived', 'Name', 'Sex', 'Age', 'AgeMissing']].sample(10)","4858873d":"df['IsChild'] = (df['Age'] <= 15).astype(int)\ndf[['Survived', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'IsChild']].sample(10)","a43fd27e":"df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\ndf[['Survived', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'FamilySize']].head()","c1731c5e":"df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\ndf[['Survived', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'FamilySize', 'IsAlone']].head()","f05522c3":"df.Ticket.unique()","011a5b57":"df['NumericalTicket'] = df['Ticket'].str.isdigit().astype(int)\ndf[['Survived', 'Name', 'Ticket', 'NumericalTicket']]","2475a81d":"def def_group_id(group):\n    if group.name == 'GroupId':\n        def_group_id.group_id += 1\n        return def_group_id.group_id\n\ndef_group_id.group_id = 0\ndf['GroupId'] = 0\ndf['GroupId'] = df[['Ticket', 'GroupId']].groupby('Ticket').transform(def_group_id)['GroupId']\ndf[['Survived', 'Name', 'Ticket', 'FamilySize', 'Surname', 'GroupId']].sort_values(by='GroupId').head(10)","eb146a0b":"def def_group_size(group):\n    return len(group)\n\ndf['GroupSize'] = 0\ndf['GroupSize'] = df[['Ticket', 'GroupSize']].groupby('Ticket').transform(def_group_size)['GroupSize']\n(df[['Survived', 'Name', 'Ticket', 'FamilySize', 'Surname', 'GroupId', 'GroupSize']]\n                                                 .sort_values(by='GroupId').head(10))","ae2aeec8":"def def_group_type(group):\n    surnames = group.Surname.unique()\n    \n    if len(group) == 1:\n        return 'Alone'\n    elif len(surnames) == 1:\n        return 'Family'\n    elif len(group) == len(surnames):\n        return 'NonFamily'\n    else:\n        return 'Mixed'\n\nticket_to_type = df[['Ticket', 'GroupSize', 'Surname']].groupby('Ticket').apply(def_group_type)\ndf = df.set_index('Ticket')\ndf['GroupType'] = ticket_to_type\ndf = df.reset_index()\n(df[['Survived', 'Name', 'Surname', 'FamilySize', 'Ticket', 'Cabin', 'GroupId', 'GroupSize', 'GroupType']]\n                                                                         .sort_values(by='GroupId').head(10))","ae66e862":"vars_of_interest = ['Name', 'Surname', 'Cabin', 'Ticket', 'GroupId', 'SibSp', 'Parch', 'FamilySize', 'GroupSize', 'GroupType']\ndf.query('FamilySize != GroupSize and GroupType == \"Family\"')[vars_of_interest].sort_values(by='GroupId')","94bd31e1":"group_id_to_family_size = {\n    36  : 2, # Carrau\n    121 : 3, # Minahan\n    588 : 2, # Risien\n    574 : 3, # Newell\n    765 : 2, # Watt\n    777 : 3, # Ware\n    316 : 3, # Ware\n    340 : 4, # Renouf\n    760 : 4, # Jefferys and Denbury\n    339 : 3, # Hirvonen\n    904 : 3, # Lindqvist\n    176 : 4, # Christy\n    190 : 4, # Jacobsohn\n    320 : 6, # Hocking\n    321 : 6, # Hocking\n    322 : 6, # Richards\n    332 : 4, # Gustafsson\n    333 : 4, # Gustafsson\n    334 : 4, # Backstrom\n    418 : 4, # Vander\n    419 : 4, # Vander\n    565 : 3, # Klasen\n    566 : 3, # Klasen\n    596 : 3, # Bourke\n    597 : 3, # Bourke\n    437 : 3, # Strom\n    461 : 3, # Persson\n    87  : 4, # Hays\n    780 : 4, # Davidson\n    117 : 3, # Frauenthal\n    828 : 3, # Frauenthal\n    26  : 3, # Crosby\n    929 : 3, # Crosby\n}\n\ngroup_id_to_group_size = {\n    777 : 3, # Ware\n    316 : 3, # Ware\n    176 : 4, # Christy\n    190 : 4, # Jacobsohn\n    320 : 6, # Hocking\n    321 : 6, # Hocking\n    322 : 6, # Richards\n    332 : 4, # Gustafsson\n    333 : 4, # Gustafsson\n    334 : 4, # Backstrom\n    418 : 4, # Vander\n    419 : 4, # Vander\n    565 : 3, # Klasen\n    566 : 3, # Klasen\n    596 : 3, # Bourke\n    597 : 3, # Bourke\n    437 : 3, # Strom\n    461 : 3, # Persson\n    117 : 3, # Frauenthal\n    828 : 3, # Frauenthal\n    26  : 3, # Crosby\n    929 : 3, # Crosby\n}\n\ngroup_id_to_group_type = {\n    316 : 'Family', # Ware's bought two different tickets\n    904 : 'Family', # Lindqvist is accompained by the Hirvonen's\n    320 : 'Family', # Hocking and Richards are the same family\n    321 : 'Family', # Hocking and Richards are the same family\n    322 : 'Family', # Hocking and Richards are the same family\n    332 : 'Family', # Backstrom and Gustafsson are the same family\n    333 : 'Family', # Backstrom and Gustafsson are the same family\n    334 : 'Family', # Backstrom and Gustafsson are the same family\n    565 : 'Family', # Klasen's bought two different tickets\n    566 : 'Family', # Klasen's bought two different tickets\n    596 : 'Family', # Bourke's bought two different tickets \n    437 : 'Family', # Strom and Persson are the same family\n    461 : 'Family', # Strom and Persson are the same family\n    117 : 'Family', # Frauenthal's bought two different tickets\n    828 : 'Family', # Frauenthal's bought two different tickets\n    26  : 'Family', # Crosby's bought two different tickets\n    929 : 'Family', # Crosby's bought two different tickets\n}\n\n\ngroup_id_to_is_alone = {\n    36 : 0, # Carrau\n    320 : 0, # Hocking \n    321 : 0, # Hocking\n    322 : 0, # Richard\n    765 : 0, # Watt\n    777 : 0, # Ware\n    316 : 0, # Ware\n    588 : 0, # Risien\n}\n\ngroup_id_to_cabin = {\n    828 : 'D40', # One of the Frauenthal's is in cabin D40, so the others might be at least under the D lobby\n}\n\npassenger_id_to_family_size = {\n    1230 : 1, # Denbury is traveling with Jefferys\n    521  : 1, # Perreault is traveling with Hays\n    1282 : 1, # Payne is traveling with Hays\n}\n\ndf = df.set_index('GroupId') # First we set the GroupId to be the index\n# Now we iterate over the dictionaries that have the GroupId as keys\nfor group_id, family_size in group_id_to_family_size.items():\n    df.loc[group_id, 'FamilySize'] = family_size\nfor group_id, group_size in group_id_to_group_size.items():\n    df.loc[group_id, 'GroupSize'] = group_size\nfor group_id, group_type in group_id_to_group_type.items():\n    df.loc[group_id, 'GroupType'] = group_type\nfor group_id, is_alone in group_id_to_is_alone.items():\n    df.loc[group_id, 'IsAlone'] = is_alone\nfor group_id, cabin in group_id_to_cabin.items():\n    df.loc[group_id, 'Cabin'] = cabin\n# Now we reset the index   \ndf = df.reset_index()\ndf = df.set_index('PassengerId')\n# And iterate over the passenger_id\nfor passenger_id, family_size in passenger_id_to_family_size.items():\n    df.loc[passenger_id, 'FamilySize'] = family_size\ndf = df.reset_index()\n\n# Now, let's show the modified dataframe entried.\nmodified_entried = set()\nmodified_entried.union(set(group_id_to_family_size.keys()))\nmodified_entried = modified_entried.union(set(group_id_to_group_size.keys()))\nmodified_entried = modified_entried.union(set(group_id_to_group_type.keys()))\nmodified_entried = modified_entried.union(set(group_id_to_is_alone.keys()))\nmodified_entried = modified_entried.union(set(group_id_to_cabin.keys()))\n\ndf.query(f'GroupId in {list(modified_entried)}')[vars_of_interest].sort_values(by='GroupId')","5de4c28b":"df.query(f'FamilySize > 1 and (GroupType == \"NonFamily\" or GroupType == \"Alone\")')[vars_of_interest].sort_values(by='GroupId')","1e033cca":"group_id_to_family_size = {\n    780 : 5, # Hays \n    230 : 3, # Thomas\n    234 : 3, # Thomas\n    270 : 4, # Elias\n    271 : 4, # Elias\n    286 : 4, # Elias\n    291 : 4, # Elias\n    302 : 2, # Giles\n    303 : 2, # Giles\n    330 : 2, # Wiklund\n    331 : 2, # Wiklund\n    335 : 11, # Andersson\n    453 : 11, # Andersson\n    460 : 11, # Andersson\n    468 : 11, # Andersson\n    554 : 11, # Andersson\n    364 : 5, # Kink\n    365 : 5, # Kink\n    366 : 5, # Kink\n    435 : 2, # Braund\n    721 : 2, # Braund\n    546 : 4, # Hansen's bouhgt three different tickets\n    547 : 4, # Hansen's bought three different tickets\n    548 : 4, # Hansen's bought three different tickets\n    556 : 4, # Jensen's bought four different tickets\n    557 : 4, # Jensen's bought four different tickets\n    558 : 4, # Jensen's bought four different tickets\n    559 : 4, # Jensen's bought four different tickets\n    610 : 2, # Kiernan\n    612 : 2, # Kiernan\n    675 : 2, # Jussila\n    676 : 2, # Jussila\n    695 : 2, # Larsson\n    696 : 2, # Larsson\n    681 : 2, # Hagland\n    682 : 2, # Hagland\n}\n\ngroup_id_to_group_size = {\n    74  : 3, # Francatelli knows Mr and Mrs Morgan\n    799 : 3, # Morgan\n    77  : 3, # Lamson\n    78  : 3, # Lamson\n    97  : 3, # Frolicher\n    98  : 3, # Frolicher\n    230 : 3, # Thomas\n    234 : 3, # Thomas\n    270 : 4, # Elias\n    271 : 4, # Elias\n    286 : 4, # Elias\n    291 : 4, # Elias\n    302 : 2, # Giles\n    303 : 2, # Giles\n    330 : 2, # Wiklund\n    331 : 2, # Wiklund\n    335 : 11, # Andersson\n    453 : 11, # Andersson\n    460 : 11, # Andersson\n    468 : 11, # Andersson\n    554 : 11, # Andersson\n    364 : 5, # Kink\n    365 : 5, # Kink\n    366 : 5, # Kink\n    435 : 2, # Braund\n    721 : 2, # Braund\n    546 : 4, # Hansen's bouhgt three different tickets\n    547 : 4, # Hansen's bought three different tickets\n    548 : 4, # Hansen's bought three different tickets\n    556 : 4, # Jensen's bought four different tickets\n    557 : 4, # Jensen's bought four different tickets\n    558 : 4, # Jensen's bought four different tickets\n    559 : 4, # Jensen's bought four different tickets\n    610 : 2, # Kiernan\n    612 : 2, # Kiernan\n    675 : 2, # Jussila\n    676 : 2, # Jussila\n    678 : 2, # Olsen\n    742 : 2, # Olsen\n    695 : 2, # Larsson\n    696 : 2, # Larsson\n    715 : 4, # Davies\n    716 : 4, # Davies\n    863 : 2, # Duran y More\n    864 : 2, # Duran y More\n    912 : 2, # Ilmakangas\n    913 : 2, # Ilmakangas\n    681 : 2, # Hagland\n    682 : 2, # Hagland\n}\n\ngroup_id_to_group_type = {\n    39  : 'Family', # Bowerman\n    74  : 'Mixed', # Francatelli and Morgan\n    799 : 'Mixed', # Francatelli and Morgan\n    76  : 'Mixed', # Hays and Potter\n    77  : 'Family', # Lamson's bought two different tickets\n    78  : 'Family', # Lamson's bought two different tickets\n    92  : 'Family', # Mock\n    93  : 'Mixed',  # Andrews and Longley\n    98  : 'Family', # Frolicher\n    148 : 'Family', # Parrish\n    188 : 'Mixed', # Walton \n    230 : 'Family', # Thomas\n    270 : 'Family', # Elias\n    271 : 'Family', # Elias\n    286 : 'Family', # Elias\n    291 : 'Family', # Elias\n    302 : 'Family', # Giles\n    303 : 'Family', # Giles\n    330 : 'Family', # Wiklund's bought two different tickets\n    331 : 'Family', # Wiklund's bought two different tickets\n    335 : 'Family', # Andersson\n    453 : 'Family', # Andersson\n    468 : 'Family', # Andersson\n    554 : 'Family', # Andersson\n    364 : 'Family', # Kink\n    365 : 'Family', # Kink\n    435 : 'Family', # Braund\n    721 : 'Family', # Braund\n    546 : 'Family', # Hansen's bouhgt three different tickets\n    548 : 'Family', # Hansen's bought three different tickets\n    556 : 'Family', # Jensen\n    557 : 'Family', # Jensen\n    558 : 'Family', # Jensen\n    559 : 'Family', # Jensen\n    610 : 'Family', # Kiernan\n    612 : 'Family', # Kiernan\n    628 : 'Family', # Eustis\n    675 : 'Family', # Jussila\n    676 : 'Family', # Jussila\n    678 : 'Family', # Olsen\n    742 : 'Family', # Olsen\n    695 : 'Family', # Larsson\n    696 : 'Family', # Larsson\n    716 : 'Mixed', # Davies\n    863 : 'Family', # Duran y More\n    864 : 'Family', # Duran y More\n    912 : 'Family', # Ilmakangas\n    913 : 'Family', # Ilmakangas\n    681 : 'Family', # Hagland\n    682 : 'Family', # Hagland\n}\n\ngroup_id_to_surname = {\n    97  : 'Frolicher',\n    366 : 'Kink'\n}\n\ngroup_id_to_is_alone = {\n    230 : 0, # Thomas\n    270 : 0, # Elias\n    271 : 0, # Elias\n    286 : 0, # Elias\n    291 : 0, # Elias\n    453 : 0, # Andersson\n    554 : 0, # Andersson\n    435 : 0, # Braund\n    721 : 0, # Braund\n    546 : 0, # Hansen\n    548 : 0, # Hansen\n    556 : 0, # Jensen\n    557 : 0, # Jensen\n    558 : 0, # Jensen\n    559 : 0, # Jensen\n    610 : 0, # Kiernan\n    612 : 0, # Kiernan\n    675 : 0, # Jussila\n    676 : 0, # Jussila\n    695 : 0, # Larsson\n    696 : 0, # Larsson\n    716 : 0, # Davies\n    681 : 0, # Hagland\n    682 : 0, # Hagland\n}\n\npassenger_id_to_family_size = {\n    311 : 5, # Hays\n    821 : 5, # Hays\n    1200 : 5, # Hays\n}\n\npassenger_id_to_cabin = {\n    1216 : 'B', # Everyone in the group was on a B cabin\n}\n\ndf = df.set_index('GroupId') # First we set the GroupId to be the index\n# Now we iterate over the dictionaries that have the GroupId as keys\nfor group_id, family_size in group_id_to_family_size.items():\n    df.loc[group_id, 'FamilySize'] = family_size\nfor group_id, group_size in group_id_to_group_size.items():\n    df.loc[group_id, 'GroupSize'] = group_size\nfor group_id, group_type in group_id_to_group_type.items():\n    df.loc[group_id, 'GroupType'] = group_type\nfor group_id, surname in group_id_to_surname.items():\n    df.loc[group_id, 'Surname'] = surname\nfor group_id, is_alone in group_id_to_is_alone.items():\n    df.loc[group_id, 'IsAlone'] = is_alone\n# Now we reset the index   \ndf = df.reset_index()\ndf = df.set_index('PassengerId')\n# And iterate over the passenger_id\nfor passenger_id, family_size in passenger_id_to_family_size.items():\n    df.loc[passenger_id, 'FamilySize'] = family_size\ndf = df.reset_index()\n\n# Now, let's show the modified dataframe entried.\nmodified_entried = set()\nmodified_entried.union(set(group_id_to_family_size.keys()))\nmodified_entried = modified_entried.union(set(group_id_to_group_size.keys()))\nmodified_entried = modified_entried.union(set(group_id_to_group_type.keys()))\nmodified_entried = modified_entried.union(set(group_id_to_is_alone.keys()))\nmodified_entried = modified_entried.union(set(group_id_to_cabin.keys()))\n\ndf.query(f'GroupId in {list(modified_entried)}')[vars_of_interest].sort_values(by='GroupId')","2ec3103a":"groups = [72, 223, 225, 454, 583, 814]\nsurnames = []\ntickets = []\nvars_of_interest = ['Survived', 'Name', 'Surname', 'Age', 'Cabin', 'Ticket', 'GroupId', 'SibSp', 'Parch', 'FamilySize', 'GroupSize', 'GroupType', 'Embarked']\ndf.query(f'GroupId in {groups} or Surname in {surnames} or Ticket in {tickets}')[vars_of_interest].sort_values(by='GroupId')","2f537446":"vars_of_interest = ['Name', 'Surname', 'Cabin', 'Ticket', 'GroupId', 'SibSp', 'Parch', 'FamilySize', 'GroupSize', 'GroupType', 'Embarked', 'IsAlone']\ndf.query(f'IsAlone == 0 and GroupType == \"Alone\"')[vars_of_interest].sort_values(by='GroupId')","11cdcc9e":"df.query(f'IsAlone == 1 and GroupType == \"Family\"')[vars_of_interest]","fe440b8a":"same_groups = [\n    [316, 777], # Ware's\n    [339, 904], # Lindqvist and Hirvonen\n    [320, 321, 322], # Hocking's and Richards'\n    [332, 333, 334], # Backstrom's and Gustafsson's\n    [565, 566], # Klasen's \n    [596, 597], # Bourke's\n    [437, 461], # Strom's and Persson's\n    [117, 828], # Frauenthal's\n    [26, 929], # Crosby's\n    [74, 799], # Francatelli's and Morgan's\n    [77, 78], # Lamson's\n    [97, 98], # Frolicher's\n    [230, 234], # Thomas's\n    [270, 271, 286, 291], # Elias's\n    [302, 303], # Giles'\n    [330, 331], # Wiklund\n    [335, 453, 460, 554], # Andersson's\n    [364, 365, 366], # Kink's\n    [435, 721], # Braund's\n    [546, 547, 548], # Hansen's\n    [556, 557, 558, 559], # Jensen's\n    [610, 612], # Kiernan's\n    [675, 676], # Jussila's\n    [678, 742], # Olsen's\n    [695, 696], # Larsson's\n    [715, 716], # Davies'\n    [863, 864], # Duran y More's\n    [912, 913], # Ilmakangas'\n    [681, 682], # Hagland's\n]\n\ndef reassign_group(groups):\n    new_group = groups[0]\n    for group in groups:\n        rows = list(df[df['GroupId'] == group].index)\n        df.loc[rows, 'GroupId'] = new_group \n\nfor groups in same_groups:\n    reassign_group(groups)\n\n# Here we see the changes in the GroupId on the Richards', Hocking's, Backstrom's, and Gustafsson's\ndf.query('GroupId in [320, 321, 322, 332, 333, 334]')[vars_of_interest].sort_values(by='GroupId')","ba9e9da2":"# We group by the id, because we saw many people knew each other but didn't buy the same ticket.\ngroups = df.groupby('GroupId')\n\ngroup_to_survivors = {}\ngroup_to_deceased = {}\nfor group_id, group in groups:\n    num_survivors = group.query(f'Survived == 1')['Survived'].count()\n    num_deceased = group.query(f'Survived == 0')['Survived'].count()\n    group_to_survivors[group_id] = num_survivors\n    group_to_deceased[group_id] = num_deceased\n\ndf['GroupNumDeceased'] = df['GroupId'].map(group_to_deceased)\ndf['GroupNumSurvivors'] = df['GroupId'].map(group_to_survivors)\n    \nvars_of_interest = ['Survived', 'Name', 'Surname', 'FamilySize', 'GroupSize', 'GroupId', 'GroupNumSurvivors', 'GroupNumDeceased']\ndf[vars_of_interest].sample(20)","2df79d8c":"vars_of_interest = ['Name', 'Ticket', 'Fare', 'GroupId', 'FamilySize', 'GroupSize']\ndf.loc[[138, 876], vars_of_interest]","a7564b3d":"# Let's average the values\ndf.loc[[138, 876], 'Fare'] = df.loc[[138, 876], 'Fare'].mean()\ndf.loc[[138, 876], vars_of_interest]","1facb898":"def def_split_fare(group):\n    if len(group.unique()) != 1:\n        print(f'ERROR, this group have more than one ticket value!')\n        print(group)\n    return group.iloc[0] \/ len(group)\n\ndf['SplitFare'] = 0\ndf['SplitFare'] = df[['Fare', 'Ticket']].groupby('Ticket').transform(def_split_fare)['Fare']\nvars_of_interest += ['SplitFare']\ndf[vars_of_interest].sort_values(by='Ticket').head(10)","51b829bb":"df.Cabin.dropna().unique()","37000e26":"df['Cabin'].fillna('N', inplace=True)","6a2eb872":"def cabin_letter(s):\n    if type(s) == float and math.isnan(s):\n        return np.nan\n    return s[0]\n\ndf['CabinLetter'] = df.Cabin.apply(cabin_letter)\ndf[['Survived', 'Name', 'Sex', 'Age', 'Cabin', 'CabinLetter']].head(7)","b3b57292":"def more_than_one_cabin(s):\n    if type(s) == float and math.isnan(s):\n        return np.nan\n    return int(len(s.split()) > 1)\n\ndf['MoreCabins'] = df.Cabin.apply(more_than_one_cabin)\ndf[['Survived', 'Name', 'Cabin', 'MoreCabins']].sample(10)","863257ee":"# How many people in more than one cabin are there?\ndf.MoreCabins.sum()","9d468b66":"categorical_values = ['Title', 'AgeMissing', 'CabinLetter', 'MoreCabins', 'IsChild', 'IsAlone', 'FamilySize', 'GroupSize', 'NumericalTicket',\n                    'GroupType', 'GroupNumSurvivors', 'GroupNumDeceased']\nfor c in categorical_values:\n    # Creating a figure\n    plt.figure(figsize=(9,5))\n    \n    # Getting the total amount of each category\n    vc = df[c].value_counts()\n    vc.index = vc.index.astype(str)\n    vc = vc.sort_values(ascending=False)\n    \n    # Getting the survivors count for each category\n    gc = df.query('Survived == 1').groupby(c)['Survived'].count()\n    gc.index = gc.index.astype(str)\n    gc = gc.reindex(vc.index)\n    \n    # Getting the percentages\n    percs = gc \/ vc\n    percs.replace(np.nan, 0, inplace=True)\n    percs = percs.reindex(vc.index)\n    \n    # Plottin the total and survived amounts\n    total_bar = plt.bar(vc.index, vc, label='total')\n    survived_bar = plt.bar(gc.index, gc, color='orange', label='survived')\n    \n    # Plotting the percentages as text on top of each bar\n    for bar, perc in zip(total_bar, percs):\n        plt.text(bar.get_x() + bar.get_width()\/2 - 0.12, bar.get_height() + 20, f'{perc:.2f}%',\n                fontsize='large')\n    \n    # Adding title and legend\n    plt.title(c)\n    plt.legend(loc='upper right')\n    y_min, y_max = plt.ylim()\n    plt.ylim((y_min, y_max+30)) # Here, I just add some extra space for the percentages labels\n    plt.show() # Showing each image at the end of the for loop","539fd076":"plt_list = [['A', 'B', 'C'], ['D', 'E', 'F']]\nfig, axs = plt.subplots(2, 3, figsize=(15, 10))\nfor i in range(2):\n     for j in range(3):\n            vc = df[df['CabinLetter'] == plt_list[i][j]]['Embarked'].value_counts()\n            axs[i][j].pie(vc \/ vc.sum(), labels=vc.index, autopct='%1.1f%%', shadow=True)\n            axs[i][j].set_title(f'CabinLetter = {plt_list[i][j]} ({vc.sum()} total travelers)')","b52d588a":"plt_list = [['A', 'B', 'C'], ['D', 'E', 'F']]\nfig, axs = plt.subplots(2, 3, figsize=(15, 10))\nfor i in range(2):\n     for j in range(3):\n            vc = df[df['CabinLetter'] == plt_list[i][j]]['Pclass'].value_counts()\n            axs[i][j].pie(vc \/ vc.sum(), labels=vc.index, normalize=True, autopct='%1.1f%%', shadow=True)\n            axs[i][j].set_title(f'CabinLetter = {plt_list[i][j]} ({vc.sum()} total travelers)')","a8d18d8b":"fig, axs = plt.subplots(1, 3, figsize=(15, 7))\nfor i, (city, group) in enumerate(df.groupby('Embarked')):\n    vc = group['Pclass'].value_counts()\n    axs[i].pie(vc \/ vc.sum(), labels=vc.index, normalize=True, autopct='%1.1f%%', shadow=True)\n    axs[i].set_title(f'City = {city} ({len(group)} total travelers)')","acd38995":"sns.set_style('whitegrid')\nsns.set(font_scale=1.5)\nsns.set_palette('deep')\nsns.catplot(x='GroupSize', data=df, hue='Survived', col='GroupType', col_wrap=2, kind='count')","999e1117":"sns.catplot(data=df, x='IsChild', kind='count', col='GroupType', col_wrap=2);","fecea2fb":"sns.countplot(data=df, x='GroupType', hue='Sex');","60c3958c":"sns.catplot(x='MoreCabins', data=df.query(f\"MoreCabins==1\"), hue='Pclass', kind='count');","993d40a7":"vars_of_interest = ['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'FamilySize', 'GroupSize', 'GroupType', 'Ticket', 'Cabin', 'CabinLetter', 'MoreCabins']\ndf.query(f\"MoreCabins==1 and Pclass==3\")[vars_of_interest]","90662868":"df.loc[(df['MoreCabins'] == 1) & (df['Pclass'] == 3), 'Cabin'] = df.loc[(df['MoreCabins'] == 1) & (df['Pclass'] == 3), 'Cabin'].apply(lambda x : x[2:])\ndf.loc[(df['MoreCabins'] == 1) & (df['Pclass'] == 3), 'CabinLetter'] = df.loc[(df['MoreCabins'] == 1) & (df['Pclass'] == 3), 'Cabin'].apply(lambda x : x[0])\ni = df.loc[(df['MoreCabins'] == 1) & (df['Pclass'] == 3), 'MoreCabins'].index \ndf.loc[(df['MoreCabins'] == 1) & (df['Pclass'] == 3), 'MoreCabins'] = 0\ndf.loc[i, vars_of_interest]","c29f0948":"def is_small(s):\n   return (s > 1) & (s < 5) \ndf['SmallGroup'] = df['GroupSize'].apply(is_small).astype(int) \ndf['SmallFamily'] = df['FamilySize'].apply(is_small).astype(int)\ndf[['Survived', 'Name', 'FamilySize', 'SmallFamily', 'GroupSize', 'SmallGroup']]","cbcb6b29":"df.Title.unique()","89863eec":"male = ['Mr', 'Master']\nfemale = ['Mrs', 'Miss']\n\ninconsistencies = []\nfor index, row in df.iterrows():\n    if row['Title'] in male and row['Sex'] == 'female':\n        inconsistencies.append(index)\n    elif row['Title'] in female and row['Sex'] == 'male':\n        inconsistencies.append(index)\nprint(f'Numer of Sex\/Title inconsistencies: {len(inconsistencies)}')","19ce8bda":"inconsistencies = []\nfor group_id, group in df.groupby('GroupId'):\n    if len(group['Embarked'].unique()) != 1:\n        inconsistencies.append(group_id)\n        \nvars_of_interest = ['Survived', 'Pclass', 'Name', 'Sex', 'GroupSize', 'GroupType', 'GroupId', 'Ticket', 'Cabin', 'Embarked']\ndf.query(f'GroupId in {inconsistencies}')[vars_of_interest].sort_values(by='GroupId')","54047b5f":"df.loc[[270, 269], 'Embarked'] = 'C'\ndf.query(f'GroupId in {inconsistencies}')[vars_of_interest].sort_values(by='GroupId')","d8c04d4d":"inconsistencies = []\nfor index, row in df.iterrows():\n     if row['Age'] <= 21 and row['Parch'] > 2:\n            inconsistencies.append(index)\n            \nvars_of_interest = ['Survived', 'Pclass', 'Name', 'Age', 'Sex', 'Parch', 'SibSp', 'FamilySize', 'GroupSize', 'GroupType', 'GroupId', 'Ticket', 'Cabin', 'Embarked']\ndf.loc[inconsistencies, vars_of_interest]","7af32dd4":"df.query(f'GroupId == 925')[vars_of_interest]","e87aff2b":"surnames = ['Johnston', 'Harknett']\ndf.query(f'Surname in {surnames}')[vars_of_interest]","af090b65":"groups = [924, 925, 926]\ndf.set_index('GroupId', inplace=True)\ndf.loc[groups, 'GroupSize'] = 10\ndf.loc[groups, 'GroupType'] = 'Family'\ndf.loc[groups, 'IsAlone'] = 0\ndf.loc[groups, 'FamilySize'] = df.loc[groups, 'SibSp'] + df.loc[groups, 'Parch']\ndf.reset_index(inplace=True)\n\nchildren = [86, 147, 436, 1058]\ndf.loc[children, 'SibSp'] = 3\ndf.loc[children, 'Parch'] = 1\ndf.loc[736, 'Parch'] = 4\n\nindex = df.query(f'GroupId in {groups}').index\ndf.loc[index, 'GroupId'] = 924\n\ndf.query(f'GroupId in {groups}')[vars_of_interest].sort_values(by='Ticket')","4c1a20e1":"def get_null_perc(s):\n    return s.isna().sum() \/ len(s) * 100\ndef get_null_amount(s):\n    return s.isna().sum()\n\ndf.agg([get_null_amount, get_null_perc]).T.sort_values(by='get_null_perc', ascending=False)","4714940c":"vars_of_interest = ['Survived', 'Name', 'Sex', 'Age', 'FamilySize', 'GroupSize', 'Ticket', 'SplitFare', 'Cabin', 'Embarked']\ndf[df['Embarked'].isna()][vars_of_interest]","659285b6":"sns.set_style('whitegrid')\nsns.set(font_scale=1.2)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6), sharey=True)\nsns.boxplot(x='Embarked', y='SplitFare', hue='Sex', data=df.query('CabinLetter == \"B\"'), ax=ax1);\nsns.violinplot(x='Embarked', y='SplitFare', hue='Sex', data=df.query('CabinLetter == \"B\"'), ax=ax2);\nax1.set_ylim([0, 75]);","298b435c":"df.fillna({'Embarked' : 'C'}, inplace=True)\ndf.query('SplitFare == 40.0')[vars_of_interest]","16fa2ef8":"vars_of_interest += ['Fare']\ndf[df['Fare'].isna()][vars_of_interest]","4a6b6a72":"plt.figure(figsize=(7,6))\nsns.boxplot(x='Pclass', y='SplitFare', hue='Sex', data=df)","743f4c5a":"plt.figure(figsize=(7,6))\nsns.histplot(x='SplitFare', data=df.query('Pclass == 3'), bins=25)","ebc82413":"sns.violinplot(x='Pclass', y='SplitFare', hue='Sex', data=df.query('Pclass == 3'))","7c90b248":"df.query('Pclass == 3 and Sex == \"male\"')['SplitFare'].describe()","074bd99b":"median = df.query('Pclass == 3 and Sex == \"male\"')['SplitFare'].median()\n# As the groupsize is 1, we can fill with the same value\ndf.fillna({'Fare' : median}, inplace=True) \ndf.fillna({'SplitFare' : median}, inplace=True) \ndf[df['PassengerId'] == 1044][vars_of_interest]","88e62004":"sns.set_style('white')\nfig, ax = plt.subplots(1, 1, figsize=(10, 7))\nax.set_ylim(0, 150)\nax.set_title('Original Age Distribution')\nsns.histplot(x='Age', data=df, ax=ax, bins=40, kde=True);","34d2c061":"sns.set_context('paper', font_scale=1.3)\nsns.set_style('whitegrid')\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\nsns.boxplot(x='Sex', y='Age', data=df, ax=ax1)\nsns.boxplot(x='Pclass', y='Age', data=df, ax=ax2)\n\nfig, ax = plt.subplots(1, 4, figsize=(15, 5))\nsns.boxplot(x='GroupSize', y='Age', data=df, ax=ax[0])\nsns.boxplot(x='FamilySize', y='Age', data=df, ax=ax[1])\nsns.boxplot(x='SibSp', y='Age', data=df, ax=ax[2])\nsns.boxplot(x='Parch', y='Age', data=df, ax=ax[3])\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\nsns.boxplot(x='Title', y='Age', data=df, ax=ax[0])\nsns.boxplot(x='Embarked', y='Age', data=df, ax=ax[1])\nsns.boxplot(x='CabinLetter', y='Age', data=df, ax=ax[2])","8d932c07":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\nsns.histplot(x='Age', data=df, ax=ax1, bins=60, kde=True);\ndf['Age'] = df.groupby(['Pclass', 'Title', 'SibSp', 'Parch', 'Sex', 'Embarked'])['Age'].transform(lambda x : x.fillna(x.median()))\ndf['Age'] = df.groupby(['Pclass', 'Title', 'SibSp', 'Parch'])['Age'].transform(lambda x : x.fillna(x.mean()))\ndf['Age'] = df.groupby(['Pclass', 'Title'])['Age'].transform(lambda x : x.fillna(x.mean()))\ndf['Age'] = df.groupby(['Pclass'])['Age'].transform(lambda x : x.fillna(x.mean()))\nsns.histplot(x='Age', data=df, ax=ax2, bins=60, kde=True);\nax1.set_title('Before'); ax2.set_title('After');","1c6220bd":"# Checking if every age was filled out\nvars_of_interest = ['Survived', 'Name', 'Sex', 'Age', 'FamilySize', 'GroupSize', 'Title', 'Pclass', 'SibSp', 'Parch']\ndf[df['Age'].isna()][vars_of_interest]","c6e24017":"df['IsChild'] = (df['Age'] <= 15).astype(int)","2f350e21":"df['Sex'] = df['Sex'].map({'male' : 0, 'female' : 1})","b036dd2c":"df.columns","374f1591":"df = pd.get_dummies(df, columns=['Pclass', 'Embarked', 'CabinLetter', 'GroupType', 'Title'])\ndf.head()","7252657a":"df.columns","c2499617":"numerical_features = ['Age', 'SibSp', 'Parch', 'Fare', 'FamilySize', 'GroupSize', 'GroupNumDeceased',\n                     'GroupNumSurvivors', 'SplitFare']\n\nscaler = StandardScaler()\ndf[numerical_features] = scaler.fit_transform(df[numerical_features])\nvars_of_interest = ['Survived', 'Name'] + numerical_features\ndf[vars_of_interest].head()","12290eab":"feature_list = set(df.columns) - set(['GroupId', 'Ticket', 'Cabin', 'Survived', 'Name', 'PassengerId', 'Surname', 'Fare']) \\\n                               - set(['GroupNumSurvivors', 'GroupNumDeceased'])\n\ntarget = 'Survived'\nprint(f'''Inicialy we have a total of {len(feature_list)} features.\\n\\nThey are:\\n{sorted(feature_list)}\n \\nAnd our target variable is: \\'{target}\\'''')","083d6659":"df_train = df[~df['Survived'].isna()]\ndf_test = df[df['Survived'].isna()]","fec15066":"models = [\n    LogisticRegression(),\n    RidgeClassifier(alpha=0.1), \n    Perceptron(),\n    SGDClassifier(alpha=0.01),\n    DecisionTreeClassifier(),\n    ExtraTreeClassifier(),\n    RandomForestClassifier(),\n    ExtraTreesClassifier(),\n    LinearSVC(),\n    SVC(),\n    KNeighborsClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    XGBClassifier(booster='gbtree', use_label_encoder=False, eval_metric='error'),\n    GradientBoostingClassifier(),\n    AdaBoostClassifier(DecisionTreeClassifier())\n]","91b63302":"def get_model_name(model):\n    \"\"\"\n    This function returns the model name\n    \"\"\"\n    return str(type(model)).split('.')[-1].replace(\"\\'>\", '')\n\ndef train_model_CV(model, X, y, aggs=[np.mean], model_params=None, cv_splits=5, random_state=42, n_jobs=-1, verbose=0):\n    \"\"\"\n    This method will perform a stratified K-fold cross validation on a model on a data set.\n    Then, it validates the model using accuracy, f1 score and roc_auc.\n    These scores will be aggregated using the functions passed by aggs and these aggregations will be returned in a dictionary.\n    Also, if the model has feature_importances, they will be returned.\n    \n    : param model : the sklearn classifier object\n    : param X : the pandas dataframe containg our training features.\n    : param y : the pandas series containg our training target class.\n    : param model_params : (Optional) a dictionary containing extra parameters for the model.\n    : param cv_splits : (Optional) integer number of cross-validation splits.\n    : param random_state : (Optional) the random seed.\n    : param n_jobs : (Optional) number of threads to be used during the traning.\n    : param verbose : (Optional) a number used for debuging propouses. The higher this number is, the more information will be printed.\n                                <=0 - no debug\n                                >0 - print the model name\n                                >1 - print the model parameters\n                                >2 - print the current fold number\n                                >3 - print the scores arrays\n    \"\"\"\n    \n    # Setting the random_state and n_jobs parameters of the model\n    if model_params == None:\n        model_params = dict()\n    if 'random_state' in model.get_params():\n        model_params['random_state'] = random_state\n    if 'seed' in model.get_params():\n        model_params['seed'] = random_state\n    if 'n_jobs' in model.get_params():\n        model_params['n_jobs'] = n_jobs\n        \n    model.set_params(**model_params)\n    \n    if verbose > 0 and verbose < 2:\n        print(f'Training model {get_model_name(model)}...')\n    elif verbose > 1:\n        print(f'Training model {get_model_name(model)} with parameters:\\n{model.get_params()}')\n    if verbose > 2:\n        print()\n    \n    # Creating a Stratified K-Fold object to split the data for the Cross-Validation\n    kfold_obj = StratifiedKFold(n_splits=cv_splits, random_state=random_state, shuffle=True)\n    \n    # Creting empty arrays for the metrics we'll compute\n    train_accuracies = []\n    test_accuracies = []\n    train_f1_scores = []\n    test_f1_scores = []\n    train_roc_scores = []\n    test_roc_scores = []\n    \n    feature_importances = []\n    mean_feature_importances = None\n    \n    # Performing the Cross Validation\n    for i, (idx_train, idx_test) in enumerate(kfold_obj.split(X, y)):\n        if verbose > 2:\n            print(f'Performing {i} kfold of {cv_splits}...')\n            \n        X_train = X.iloc[idx_train]\n        X_test = X.iloc[idx_test]\n        y_train = y.iloc[idx_train]\n        y_test = y.iloc[idx_test]\n        \n        # Fitting the model to the training data\n        model.fit(X_train, y_train) \n        \n        # Getting the prediction values\n        train_pred = model.predict(X_train)\n        test_pred = model.predict(X_test)\n        \n        # Computating the scores\n        train_accuracies.append(accuracy_score(y_train, train_pred))\n        test_accuracies.append(accuracy_score(y_test, test_pred))\n        train_f1_scores.append(f1_score(y_train, train_pred))\n        test_f1_scores.append(f1_score(y_test, test_pred))\n        train_roc_scores.append(roc_auc_score(y_train, train_pred))\n        test_roc_scores.append(roc_auc_score(y_test, test_pred))\n        \n        # Computing the feature importance if the model has it\n        if hasattr(model, 'feature_importances_'):\n            feature_importances.append(model.feature_importances_)\n    \n    if verbose > 3:\n        print()\n        print(f'train_accuracies = {train_accuracies}')\n        print(f'test_accuracies = {test_accuracies}')\n        print(f'train_f1_scores = {train_f1_scores}')\n        print(f'test_f1_scores = {test_f1_scores}')\n        print(f'train_roc_scores = {train_roc_scores}')\n        print(f'test_roc_scores = {test_roc_scores}')\n        \n    # Now let's compute the aggregation functions\n    aggregations = dict()\n    scores = [train_accuracies, test_accuracies, train_f1_scores, test_f1_scores, train_roc_scores, test_roc_scores]\n    score_names = ['train_accuracies', 'test_accuracies', 'train_f1_scores', 'test_f1_scores', 'train_roc_scores', 'test_roc_scores']\n    for name, score in zip(score_names, scores):\n        for func in aggs:\n            aggregations[f'{name}_{func.__name__}'] = func(score)\n    \n    # Computing the feature importances mean\n    if hasattr(model, 'feature_importances_'):\n        mean_feature_importances = np.mean(feature_importances, axis=0)\n    \n    # Returning the aggergations and the feature importances\n    return aggregations, mean_feature_importances","8159583e":"X = df_train[feature_list]\ny = df_train[target]\n\naggregations = []\nfeature_importances = []\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfor model in models:\n    aggs, fi = train_model_CV(model, X, y, verbose=1)\n    model_name = get_model_name(model)\n    aggregations.append((model_name, aggs))\n    feature_importances.append((model_name, fi))\n    \nindex = list(map(lambda x : x[0], aggregations)) \nresults = pd.DataFrame(list(map(lambda x : x[1], aggregations)), index=index)","1b1e4dee":"results.sort_values(by='test_accuracies_mean', ascending=False)","2787ce79":"def show_importances(importances, features_names, model_name):\n    # Sorting the importances \n    importances_dict = dict(sorted(zip(features_names, importances), key=lambda x : x[1], reverse=True))\n    \n    with sns.plotting_context('notebook', font_scale=1.4):\n        with sns.axes_style('whitegrid'):\n            fig, ax = plt.subplots(1, 1, figsize=(20, 5))\n            g = sns.barplot(x=list(importances_dict.keys()), y=list(importances_dict.values()), ax=ax, color='b')\n            ax.set_title(f'Feature Importances for {model_name}', fontdict=dict(fontsize=20))\n            g.set_xticklabels(list(importances_dict.keys()), rotation=90)\n\nfor name, fi in feature_importances:\n    if type(fi) != type(None) and name != 'AdaBoostClassifier':\n        show_importances(fi, list(feature_list), name)","4ccfd3d2":"def get_features_rank_for_model(model, X, y):\n    rfe_obj = RFE(model, n_features_to_select=1)\n    rfe_obj.fit(X, y)\n    return rfe_obj.ranking_\n    \ndef get_features_rank(X, y, skip_models=None, verbose=0):\n    if skip_models == None:\n        skip_models = []\n    \n    features_df = pd.DataFrame(index=list(feature_list))\n    features_df['Overall rank'] = 0\n    for model in models:\n        model_name = get_model_name(model)\n        \n        # Some models can't be used with RFE, so we need to skip them\n        if model_name in skip_models:\n            if verbose > 0:\n                print(f'Skipping for model {model_name}...')\n            continue\n        \n        # Getting the ranks for each model\n        if verbose > 0:\n            print(f'Performing RFE for model {model_name}...')\n        features_rank = get_features_rank_for_model(model, X, y)\n        features_df[f'Rank {model_name}'] = features_rank\n        \n    # Getting the overall ranks\n    features_df['Overall rank'] = features_df.sum(axis=1)\n    features_df['Overall rank'] = features_df['Overall rank'].rank().astype(int)\n    \n    return features_df\n\nskip_models = ['SVC', 'KNeighborsClassifier', 'GaussianNB', 'AdaBoostClassifier']\nfeatures_df = get_features_rank(X, y, skip_models=skip_models, verbose=1)","1beff063":"features_df = features_df.sort_values('Overall rank', ascending=True)\nfeatures_df","c4e895f2":"features_df.loc['Sex']","bb5ffb07":"features_df.loc[['Age', 'AgeMissing', 'IsChild']].T","6af17cc4":"features_df.loc[['Pclass_1', 'Pclass_2', 'Pclass_3']].T","d9e197d6":"features_df.loc[['CabinLetter_A', 'CabinLetter_B', 'CabinLetter_C', 'CabinLetter_D', 'CabinLetter_E',\n                 'CabinLetter_F', 'CabinLetter_G', 'CabinLetter_T', 'CabinLetter_N']].T","c5e8d044":"features_df.loc[['Title_Mr', 'Title_Mrs', 'Title_Master', 'Title_Miss', 'Title_rare']].T","63b87082":"features_df.loc[['SibSp', 'Parch', 'FamilySize', 'GroupSize', 'SmallGroup', 'SmallFamily']].T","ebdeb19c":"features_df.loc[['GroupType_Mixed', 'GroupType_Family', 'GroupType_NonFamily', 'GroupType_Alone']].T","27685659":"# Let's fix some features that we saw are very good in the majority of the situations\n# For now, let's fix these three features. We saw they are indeed the best\nfixed_features = ['Sex', 'SplitFare', 'CabinLetter_N']\n\nage_features = ['Age', 'AgeMissing', 'IsChild']\ncabin_letter_features = ['CabinLetter_A', 'CabinLetter_B', 'CabinLetter_C', 'CabinLetter_D', \n                         'CabinLetter_E', 'CabinLetter_F', 'CabinLetter_G'] # I'll leave CabinLetter_T out\nclass_features = ['Pclass_1', 'Pclass_2', 'Pclass_3']\ntitle_features = ['Title_Mr', 'Title_Mrs', 'Title_Master', 'Title_Miss'] # I'll leave Title_rare out\ngroup_type_features = ['GroupType_Alone', 'GroupType_Family', 'GroupType_Mixed', 'GroupType_NonFamily']\nsmall_features = ['SmallFamily', 'SmallGroup']\nsize_features = ['FamilySize', 'GroupSize']\nembarked_features = ['Embarked_C', 'Embarked_Q', 'Embarked_S']\nfamily_features = ['SibSp', 'Parch']\n\nbad_features = ['MoreCabins', 'IsAlone', 'NumericalTicket']","cea03a8b":"# Let's also divide our models into Linear and Tree based models, because we saw that there are many differences between them\nlinear_models = [\n    LogisticRegression(),\n    RidgeClassifier(alpha=0.1), \n    Perceptron(),\n    SGDClassifier(alpha=0.01),\n    LinearSVC(),\n    LinearDiscriminantAnalysis(),\n]\ntree_models = [\n    DecisionTreeClassifier(),\n    ExtraTreeClassifier(),\n    RandomForestClassifier(),\n    ExtraTreesClassifier(),\n    XGBClassifier(booster='gbtree', use_label_encoder=False, eval_metric='error'),\n    GradientBoostingClassifier(),\n    AdaBoostClassifier(DecisionTreeClassifier())\n]\nother_models = [\n    SVC(),\n    KNeighborsClassifier(),\n    GaussianNB(),\n]","f5847156":"# Now let's create some functions to help us evaluate each set\ndef overall_performance(models, X, y, cv_splits=5, random_state=42, n_jobs=-1, verbose=0):\n    aggs = []\n    \n    for model in models:\n        model_name = get_model_name(model)\n        agg, fi = train_model_CV(model, X, y, cv_splits=cv_splits, random_state=random_state, n_jobs=n_jobs, verbose=verbose)\n        aggs.append((model_name, agg))\n        \n    index = list(map(lambda x : x[0], aggs)) \n    results = pd.DataFrame(list(map(lambda x : x[1], aggs)), index=index)\n    return results\n\ndef test_feature_set(feature_set, X, y, cv_splits=5, random_state=42, n_jobs=-1, verbose=0):\n    X_feat = X[feature_set]\n    \n    linear_df = overall_performance(linear_models, X_feat, y, cv_splits=cv_splits,\n                                    random_state=random_state, n_jobs=n_jobs, verbose=verbose)\n    tree_df = overall_performance(tree_models, X_feat, y, cv_splits=cv_splits,\n                                    random_state=random_state, n_jobs=n_jobs, verbose=verbose)\n    other_df = overall_performance(other_models, X_feat, y, cv_splits=cv_splits,\n                                    random_state=random_state, n_jobs=n_jobs, verbose=verbose)\n    \n    return linear_df, tree_df, other_df\n\ndef test_all_feature_sets(feature_sets, X, y, score_col='test_accuracies_mean',\n                          cv_splits=5, random_state=42, n_jobs=-1, verbose=0):\n    \n    if verbose > 0:\n        print('Testing for the base features...')\n    l_base, t_base, o_base = test_feature_set(feature_sets[0][1], X, y, cv_splits=cv_splits,\n                                    random_state=random_state, n_jobs=n_jobs, verbose=verbose-1)\n    l_df = pd.DataFrame({'Base Score' : l_base[score_col]}, index=l_base.index)\n    t_df = pd.DataFrame({'Base Score' : t_base[score_col]}, index=t_base.index)\n    o_df = pd.DataFrame({'Base Score' : o_base[score_col]}, index=o_base.index)\n    \n    for set_name, feature_set in feature_sets[1:]:\n        if verbose > 0:\n            print(f'Testing for set \"{set_name}\"...')\n        l, t, o = test_feature_set(feature_set, X, y, cv_splits=cv_splits,\n                                    random_state=random_state, n_jobs=n_jobs, verbose=verbose-1)\n        l_df[f'Score {set_name}'] = l[score_col]\n        t_df[f'Score {set_name}'] = t[score_col]\n        o_df[f'Score {set_name}'] = o[score_col]\n        \n    return l_df, t_df, o_df","bb7808d2":"other_features = [age_features, cabin_letter_features, class_features, title_features, group_type_features, small_features,\n                 size_features, embarked_features, family_features, bad_features]\nset_names = ['Ages', 'CabinLetters', 'Classes', 'Titles', 'GroupTypes', 'Small', 'Sizes', 'Embarkeds', 'Family', 'Bad']","ea1bf2a5":"# First let's make feature sets using the fixed set and each other set\nall_feature_sets = [('Fixed', fixed_features)]\n\nfor fs, sn in zip(other_features, set_names):\n    new_set = fixed_features + fs\n    all_feature_sets.append((f'Fixed and {sn}', new_set))","efb9e863":"l_df, t_df, o_df = test_all_feature_sets(all_feature_sets, X, y, verbose=1)","83e0e08b":"l_df","4e8228b8":"t_df","b1c941a6":"o_df","2109271f":"my_feature_sets = [\n    title_features + class_features + embarked_features + size_features + ['Age'],\n    title_features + class_features + embarked_features + small_features + ['Age'],\n    ['Title_Mr', 'Title_Master', 'Title_rare'] + class_features + embarked_features + size_features + ['Age'],\n    ['Title_Mr', 'Title_Master', 'Title_rare'] + class_features + embarked_features + small_features + ['Age'],\n    title_features + ['Pclass_3'] + embarked_features + size_features + ['Age'],\n    title_features + ['Pclass_3'] + embarked_features + small_features + ['Age'],\n    ['Title_Mr', 'Title_Master', 'Title_rare'] + ['Pclass_3'] + embarked_features + size_features + ['Age'],\n    ['Title_Mr', 'Title_Master', 'Title_rare'] + ['Pclass_3'] + embarked_features + small_features + ['Age'],\n    title_features + class_features + size_features + ['Age'],\n    title_features + class_features + small_features + ['Age'],\n    ['Title_Mr', 'Title_Master', 'Title_rare'] + class_features + size_features + ['Age'],\n    ['Title_Mr', 'Title_Master', 'Title_rare'] + class_features + small_features + ['Age'],\n    title_features + ['Pclass_3'] + size_features + ['Age'],\n    title_features + ['Pclass_3'] + small_features + ['Age'],\n    ['Title_Mr', 'Title_Master', 'Title_rare'] + ['Pclass_3'] + size_features + ['Age'],\n    ['Title_Mr', 'Title_Master', 'Title_rare'] + ['Pclass_3'] + small_features + ['Age'],\n    title_features + class_features + embarked_features + size_features,\n    title_features + class_features + embarked_features + small_features,\n    ['Title_Mr', 'Title_Master', 'Title_rare'] + class_features + embarked_features + size_features,\n    ['Title_Mr', 'Title_Master', 'Title_rare'] + class_features + embarked_features + small_features,\n    title_features + ['Pclass_3'] + embarked_features + size_features,\n    title_features + ['Pclass_3'] + embarked_features + small_features,\n    ['Title_Mr', 'Title_Master', 'Title_rare'] + ['Pclass_3'] + embarked_features + size_features,\n    ['Title_Mr', 'Title_Master', 'Title_rare'] + ['Pclass_3'] + embarked_features + small_features,\n    title_features + class_features + size_features,\n    title_features + class_features + small_features,\n    ['Title_Mr', 'Title_Master', 'Title_rare'] + class_features + size_features,\n    ['Title_Mr', 'Title_Master', 'Title_rare'] + class_features + small_features,\n    title_features + ['Pclass_3'] + size_features,\n    title_features + ['Pclass_3'] + small_features,\n    ['Title_Mr', 'Title_Master', 'Title_rare'] + ['Pclass_3'] + size_features,\n    ['Title_Mr', 'Title_Master', 'Title_rare'] + ['Pclass_3'] + small_features,\n] \n\nnames = range(32)\nnames = list(map(str, names))\n\nmfs = [('Fixed', fixed_features)]\n\nfor fs, sn in zip(my_feature_sets, names):\n    new_set = fixed_features + fs\n    mfs.append((f'Fixed and {sn}', new_set))\n                   \nl_df, t_df, o_dft = test_all_feature_sets(mfs, X, y, verbose=1)","bf545f8e":"t_df.sort_values('Base Score', ascending=False)","a871eb07":"t_df.sum().sort_values(ascending=False)","f5cd4773":"with_age      = [0] + [1] * 16 + [0] * 16\nwith_embarked = [0] + [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0] * 2\nfull_class    = [0] + [1, 1, 1, 1, 0, 0, 0, 0] * 4 \nfull_title    = [0] + [1, 1, 0, 0] * 8\nfull_size     = [0] + [1, 0] * 16\nwith_age = np.array(list(map(bool, with_age)))\nwith_embarked = np.array(list(map(bool, with_embarked)))\nfull_class = np.array(list(map(bool, full_class)))\nfull_title = np.array(list(map(bool, full_title)))\nfull_size = np.array(list(map(bool, full_size)))","811d4a1d":"score_with_age = t_df.loc[:, with_age].sum().sum()\nscore_without_age = t_df.loc[:, ~with_age].sum().sum()\nscore_with_embarked = t_df.loc[:, with_embarked].sum().sum()\nscore_without_embarked = t_df.loc[:, ~with_embarked].drop('Base Score', axis='columns').sum().sum()\nscore_full_class = t_df.loc[:, full_class].sum().sum()\nscore_not_full_class = t_df.loc[:, ~full_class].drop('Base Score', axis='columns').sum().sum()\nscore_full_title = t_df.loc[:, full_title].sum().sum()\nscore_not_full_title = t_df.loc[:, ~full_title].drop('Base Score', axis='columns').sum().sum()\nscore_full_size = t_df.loc[:, full_size].sum().sum()\nscore_not_full_size = t_df.loc[:, ~full_size].drop('Base Score', axis='columns').sum().sum()\n\nprint(f'Score with age is {score_with_age:.3f} and {score_without_age:.3f} without it.')\nprint(f'Score with embarked is {score_with_embarked:.3f} and {score_without_embarked:.3f} without it.')\nprint(f'Score with full_class is {score_full_class:.3f} and {score_not_full_class:.3f} without it.')\nprint(f'Score with full_title is {score_full_title:.3f} and {score_not_full_title:.3f} without it.')\nprint(f'Score with full_size is {score_full_size:.3f} and {score_not_full_size:.3f} without it.')","b271273e":"my_feature_sets[26], my_feature_sets[30], my_feature_sets[27], my_feature_sets[31]","25fd64dc":"features_set1, features_set2, features_set3, features_set4 = my_feature_sets[26], my_feature_sets[30], my_feature_sets[27], my_feature_sets[31]\nfeatures_set = [features_set1, features_set2, features_set3, features_set4]\nfeatures_set = list(map(lambda x : fixed_features + x, features_set))\nfeatures_set += [list(feature_list)] # Let's also add the full set of features","2af9b4cf":"def tune_model(model, search_params, X, y, cv_splits=5, score='accuracy', randomized=False, \n               n_iter=100, n_jobs=-1, random_state=1, verbose=0):\n    \"\"\"\n    This function trains a model and tune its hyperparameters according to the parameters passed to it.\n    : param model : The classifier object, assumed to have sklearn wrappers.\n    : param search_params : A dictionary contaning the hyperparameters to search over.\n    : param X : The pandas dataframe training set of features\n    : param y : The pandas series traning set of labels\n    : param score : The score optimized by the parameters search \n    : param randomized : If True, it will use RandomSearchCV. Otherwise, it will use GridSearchCV\n    : param n_iter : Only used if randomized is True. It's passed to RandomSearchCV\n    : param n_jobs : The number of threads to use in the search\n    : param random_state : The seed used for random algorithms (StratifiedKFold and RandomSearchCV).\n    : param verbose : If 0, no information is printed.\n                      If >0, information about the best classifier found is printed.\n    : return s_obj : The search object (RandomizedSearchCV or GridSearchCV instance) after fitting the data\n    \"\"\"\n    kfold = StratifiedKFold(n_splits=cv_splits, random_state=random_state)\n    \n    if randomized:\n        s_obj = RandomizedSearchCV(model, search_params, cv=kfold, scoring=score, n_iter=n_iter,\n                                   n_jobs=n_jobs, random_state=random_state, verbose=verbose-1)\n    else:\n        s_obj = GridSearchCV(model, search_params, cv=kfold, scoring=score, n_jobs=n_jobs, verbose=verbose-1)\n        \n    if verbose > 0:\n        print(f'Model: {get_model_name(model)}')\n        \n    inicial_time = time.time()\n    s_obj.fit(X, y)\n    end_time = time.time() \n    total_time = end_time - inicial_time\n    \n    if verbose > 0:\n        print(f'Best score: {s_obj.best_score_:.4f}')\n        print(f'Best parameters set: {s_obj.best_params_}')\n        print(f'Time execution: {total_time:.2f} seconds')\n    return s_obj # Returning the obj we can get all of its attributes\n\ndef tune_models(models, list_search_params, X, y, cv_splits=5, score='accuracy', randomized=None, \n               n_iter=100, n_jobs=-1, random_state=1, verbose=0):\n    \"\"\"\n    This function tunes the hyperparameters of many models search over the parameters.\n    : param models : A list of classifier objects, assumed to have sklearn wrappers.\n    : param list_search_params : A list of dictionaries contaning the hyperparameters to search over.\n    : param X : The pandas dataframe training set of features\n    : param y : The pandas series traning set of labels\n    : param score : The score optimized by the parameters search \n    : param randomized : A boolean list. If the i-th position is True, it will use RandomSearchCV on the i-th model.\n                         Otherwise, it will use GridSearchCV\n    : param n_iter : Only used if randomized is True. It's passed to RandomSearchCV\n    : param n_jobs : The number of threads to use in the search\n    : param random_state : The seed used for random algorithms (StratifiedKFold and RandomSearchCV).\n    : param verbose : If 0, no information is printed.\n                      If >0, information about the best classifier found is printed.\n    : return s_objs : The list of search objects (RandomSearchCV or GridSearchCV instance) after fitting the data\n    \"\"\"\n    s_objs = []\n    i = 1\n    n = len(models)\n    start_time = time.time()\n    if randomized == None:\n        print('All not randomized')\n        randomized = [False for i in range(n)]\n    for model, search_params in zip(models, list_search_params):\n        if verbose > 0:\n            print(f' *** Model {i} of {n} ***', end=' ')\n            if randomized[i-1]:\n                print('(Randomized)')\n            else:\n                print()\n        i += 1\n             \n        s_obj = tune_model(model, search_params, X, y, cv_splits, score, randomized, \n                           n_iter, n_jobs, random_state, verbose)\n        s_objs.append(s_obj)\n        \n        if verbose > 0:\n            print()\n        \n    end_time = time.time()\n    if verbose > 0:\n        print(f'Total time spent: {end_time-start_time:.2f} seconds.')\n    \n    return s_objs","2b44d63c":"# Defining some parameters that will be the same for every model\nrandom_state = 42\nn_jobs = -1\n\n# Defining the lists we'll use on the tuning function\nmodels = []\nlist_search_params = []\nrandomized = []\n\n# *** Logistic Regression Classifier ***\nlr = LogisticRegression()\n\nlr_param = {'random_state' : [random_state], \n            'penalty' : ['l1', 'l2', 'elasticnet'], \n            'C' : np.geomspace(1e-4, 1e4, 9)\n}\n\nmodels.append(lr)\nlist_search_params.append(lr_param)\nrandomized.append(False)\n\n# *** Ridge Classifier ***\nrc = RidgeClassifier()\n\nrc_param = {'random_state' : [random_state], \n            'alpha' : np.geomspace(1e-4, 1e4, 9)\n}\n\nmodels.append(rc)\nlist_search_params.append(rc_param)\nrandomized.append(False)\n\n# *** Stochastic Gradient Descent ***\nsgd = SGDClassifier()\n\nsgd_param = {'random_state' : [random_state],\n             'n_jobs' : [n_jobs],\n             'loss' : ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n             'penalty' : ['l1', 'l2', 'elasticnet'],\n             'alpha' : np.geomspace(1e-4, 1e2, 7),\n             'epsilon' : np.geomspace(1e-4, 1e2, 7),\n             'learning_rate' : ['constant', 'optimal', 'invscaling', 'adaptive'],\n             'eta0' : np.geomspace(1e-4, 1e2, 7),\n             'max_iter' : [10000]\n}  \n\nmodels.append(sgd)\nlist_search_params.append(sgd_param)\nrandomized.append(True)\n\n# *** Decision Tree Classifier ***\ndt = DecisionTreeClassifier()\n\ndt_param = {'random_state' : [random_state],\n             'criterion' : ['gini', 'entropy'],\n             'splitter' : ['best', 'random'],\n             'max_depth' : [None, 3, 5, 7, 10],\n             'min_samples_leaf' : np.linspace(1, 10, 5).astype(int), \n             'min_samples_split' : np.linspace(2, 10, 5).astype(int), \n             'max_features': [0.25, 0.50, 0.75, 1.00]\n} \n\nmodels.append(dt)\nlist_search_params.append(dt_param)\nrandomized.append(False)\n\n\n# *** ExtraTree Classifier ***\n# (highly-randomized version of a Decision Tree)\net = ExtraTreeClassifier()\n\net_param = {'random_state' : [random_state],\n            'criterion' : ['gini', 'entropy'],\n            'splitter' : ['best', 'random'],\n            'max_depth' : [None, 3, 5, 7, 10],\n            'min_samples_leaf' : np.linspace(1, 10, 5).astype(int), \n            'min_samples_split' : np.linspace(2, 10, 5).astype(int), \n            'max_features': [0.25, 0.50, 0.75, 1.00]\n}\n\nmodels.append(et)\nlist_search_params.append(et_param)\nrandomized.append(True)\n\n# *** Random Forest Classifier ***\nrf = RandomForestClassifier()\n\nrf_param = {'random_state' : [random_state],\n            'n_jobs' : [n_jobs],\n            'criterion' : ['gini', 'entropy'],\n            'max_depth' : [None, 3, 5, 7, 10],\n            'min_samples_leaf' : np.linspace(1, 10, 5).astype(int), \n            'min_samples_split' : np.linspace(2, 10, 5).astype(int), \n            'max_features': [0.25, 0.50, 0.75, 1.00],\n            'n_estimators': np.geomspace(10, 1000, 7).astype(int)\n}\n\nmodels.append(rf)\nlist_search_params.append(rf_param)\nrandomized.append(True)\n\n# *** SVC Classifier ***\nsvc = SVC()\n\nsvc_param = {'random_state' : [random_state],\n             'C' : [0.25, 0.5, 1.0, 2.0, 4.0],\n             'gamma' : ['auto'],\n             'kernel' : ['rbf', 'linear', 'poly', 'sigmoid'],\n             'shrinking' : [True, False]\n}\n\nmodels.append(svc)\nlist_search_params.append(svc_param)\nrandomized.append(False)\n\n# *** K-Nearest Neighbors Classifier ***\n\nknn = KNeighborsClassifier()\n\nknn_param = {'n_jobs' : [n_jobs],\n             'n_neighbors' : np.linspace(3, 7, 5).astype(int),\n             'weights' : ['uniform', 'distance'],\n             'metric' : ['euclidean', 'manhattan', 'chebyshev', 'minkowski', 'wminkowski', 'seuclidean', 'mahalanobis']\n}\n\nmodels.append(knn)\nlist_search_params.append(knn_param)\nrandomized.append(False)\n\n# *** Gradient Boosting Classifier ***\ngb = GradientBoostingClassifier()\n\ngb_param = {'random_state' : [random_state], \n             'loss' : ['deviance', 'exponential'],\n             'learning_rate': [0.01, 0.05, 0.1, 0.25],\n             'n_estimators' : [100, 250, 500],\n             'max_depth': [None, 3, 6, 9],\n             'min_samples_leaf': [1, 5],\n             'max_features': [0.25, 0.50, 0.75, 1.0]  \n}\nmodels.append(gb)\nlist_search_params.append(gb_param)\nrandomized.append(False)\n\n# *** Extra Trees Classifier ***\nets = ExtraTreesClassifier()\n\nets_param = {'random_state' : [random_state],\n             'n_jobs' : [n_jobs],\n             'n_estimators' : [100, 500, 1000],\n             'criterion' : ['gini', 'entropy'],\n             'max_depth' : [None, 3, 5, 7, 10],\n             'min_samples_leaf' : np.linspace(1, 10, 5).astype(int), \n             'min_samples_split' : np.linspace(2, 10, 5).astype(int), \n             'max_features': [0.25, 0.50, 0.75, 1.00]\n}\nmodels.append(ets)\nlist_search_params.append(ets_param)\nrandomized.append(True)\n\n# *** XGBooster ***\nxgb = XGBClassifier()\n\nxgb_param = {'seed' : [random_state],\n             'booster' : ['gbtree'],\n             'eval_metric' : ['mae'],\n             'use_label_encoder' : [False],\n             'learning_rate' : [0.05, 0.1, 0.3], \n             'max_depth' : [None, 3, 6, 10],\n             'subsample' : [0.2, 0.5, 1.0], \n             'colsample_bytree' : [0.1, 0.2, 0.5],\n             'reg_lambda' : [1.0, 1.5, 2.0, 2.5],\n             'reg_alpha' : [0.0, 0.5, 0.7]}\n\nmodels.append(xgb)\nlist_search_params.append(xgb_param)\nrandomized.append(False)\n\n\n# *** AdaBoost on a Decision Tree Classifier ***\nada = AdaBoostClassifier(DecisionTreeClassifier())\n\nada_param = {'random_state' : [random_state],\n             'base_estimator__criterion' : ['gini'],\n             'base_estimator__max_depth' : [5],\n             'base_estimator__min_samples_leaf' : [10], \n             'base_estimator__min_samples_split' : [2],\n             'base_estimator__max_features': [0.25, 0.50, 0.75, 1.00], \n             'base_estimator__splitter' : ['best'], \n             'algorithm' : ['SAMME', 'SAMME.R'], \n             'n_estimators' : [10, 25, 50, 100], \n             'learning_rate' : [0.01, 0.1, 0.5, 1.0, 1.5]\n}\n\nmodels.append(ada)\nlist_search_params.append(ada_param)\nrandomized.append(False)","e30d568e":"# Here we choose the traning set we'll tune our models\nX = df_train[features_set[4]]\ny = df_train[target]\n# Try using verbose=2 if it's taking too much time. You can follow the progress of each model\n#s_objs = tune_models(models, list_search_params, X, y, randomized=randomized, n_iter=100, verbose=1)\n# I'm commenting because Kaggle takes A LOT OF TIME to run this (so I hardcoded the best estimators)","305cb71d":"#best_estimators = list(map(lambda x : x.best_estimator_, s_objs))\n#best_params = list(map(lambda x : x.best_params_, s_objs))\n#best_scores = list(map(lambda x : x.best_score_, s_objs))\n#best_estimators","fec3cdaa":"best_estimators = [\n LogisticRegression(random_state=42),\n    \n RidgeClassifier(alpha=10.0, random_state=42),\n    \n SGDClassifier(alpha=0.001, epsilon=0.01, eta0=1.0, loss='log', max_iter=10000,\n               n_jobs=-1, random_state=42),\n    \n DecisionTreeClassifier(max_depth=5, max_features=1.0, min_samples_split=10,\n                        random_state=42),\n    \n ExtraTreeClassifier(max_depth=5, max_features=1.0, min_samples_split=10,\n                     random_state=42, splitter='best'),\n    \n RandomForestClassifier(criterion='entropy', max_depth=10, max_features=0.75,\n                        min_samples_leaf=3, n_estimators=464, n_jobs=-1,\n                        random_state=42),\n    \n SVC(C=2.0, gamma='auto', random_state=42),\n    \n KNeighborsClassifier(metric='euclidean', n_jobs=-1, n_neighbors=7),\n    \n GradientBoostingClassifier(learning_rate=0.05, max_features=1.0,\n                            min_samples_leaf=5, n_estimators=250,\n                            random_state=42),\n    \n ExtraTreesClassifier(criterion='entropy', max_depth=5, max_features=0.25,\n                      min_samples_leaf=7, min_samples_split=8, n_estimators=1000,\n                      n_jobs=-1, random_state=42),\n    \n XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n               colsample_bynode=1, colsample_bytree=0.2, eval_metric='mae',\n               gamma=0, gpu_id=-1, importance_type='gain',\n               interaction_constraints='', learning_rate=0.3, max_delta_step=0,\n               max_depth=6, min_child_weight=1, \n               monotone_constraints='()', n_estimators=100, n_jobs=8,\n               num_parallel_tree=1, random_state=42, reg_alpha=0.5,\n               reg_lambda=1.5, scale_pos_weight=1, seed=42, subsample=1.0,\n               tree_method='exact', use_label_encoder=False,\n               validate_parameters=1, verbosity=None),\n    \n AdaBoostClassifier(algorithm='SAMME',\n                    base_estimator=DecisionTreeClassifier(max_depth=5,\n                                                          max_features=0.5,\n                                                          min_samples_leaf=10),\n                    learning_rate=0.01, n_estimators=100, random_state=42)]\n\nfor estimator in best_estimators:\n    estimator.fit(X, y)","734c0eb4":"# Selecting the feature set\nfinal_feature_set = features_set[4]\n# Selecting the estimator\nfinal_estimator = best_estimators[8]\nprint(f'Using a {get_model_name(final_estimator)} as the final estimator.')","2d7a4d70":"X_test = df_test[final_feature_set]\nprediction = final_estimator.predict(X_test).astype(int)","4cfd18c8":"original_test['Prediction'] = prediction\noriginal_test","409cfa36":"output = pd.DataFrame(index=original_test.index)\noutput['PassengerId'] = original_test['PassengerId']\noutput['Survived'] = prediction\noutput.head(7) # Here I'll show the head of the dataframe we're going to submit","8c018204":"# Total positive predictions:\noutput.Survived.sum()","51401747":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nsns.scatterplot(data=df_train, x='Age', y='Fare', hue='Survived', ax=ax1)\nsns.scatterplot(data=original_test, x='Age', y='Fare', hue='Prediction', ax=ax2);","a3102639":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nax1.set_yticks([0, 1])\nax1.set_yticklabels(['male', 'female'])\nsns.scatterplot(data=df_train, x='Age', y='Sex', hue='Survived', ax=ax1)\nsns.scatterplot(data=original_test, x='Age', y='Sex', hue='Prediction', ax=ax2);","732ee819":"output.to_csv('titanic_predictions.csv', index=False)","abed8e39":"original_test","f67eaff2":"First, I'll fill the NaN Cabins with the letter 'N'.","fd37994c":"Let's define the functions we're going to use to tune the hyperparameters.","7e100201":"By looking at the data, we see that a few thing might be the case for these rows:\n- We've got a lower value for the family size because some people might be from the same family but not exactly were counted by the columns SibSp and Parch. They would be cousins, uncles or something like that;\n- We have a higher value for the family size because some families didn't buy the same ticket;\n- We have people that don't have the same surname as the parents or spouse;\n- Or it can be some other strange case.","83d611ba":"## Fare\nNow let's look at the NaN Fare value in the test data.","daaa5fb7":"# First Steps\nLet's first of all import our libraries and read the data sets.\n\nThem, let's visualize them and get some of the main info about them.","bece2eca":"Looking at the data, we see two main cases:\n- People that are from the same family, but broght two different tickets (most of them are consecutive);\n- Groups that are Family, but were considered Non-Family because many people have a different single name.","7209a017":"We can visualy see the correlation between the data, with stronger colors representing higher correlations.","ea1d1a9b":"## Embarked and Group","52984ec6":"## CabinLetter versus Embarked","0d4ff400":"Where we see that they are two women that were in the cabin B28, which costed 40 and is the same ticket. Both of them survived. It's strange to see that we have missing values for people that survived. \n\nWe can try to find any pattern that indicate which city these two ladies came from.","ebbfa54d":"### Cabin Letter","0457dafb":"# Visualizing our data\n\nJust printing unique values or try to use `pandas` commands like `value_counts` or even `pivot_table` is not enjoy to really get insightful information about the data. So we're going to plot several graphs in order to better understand the trend of the data. Then, we'll be able to tell, based on the features that we already have, if a person is going or not to survive. \n\nThis process also helps to get more ideias for the creation process of new features (so called Feature Engineering).","897af8ab":"## Categorial Values","aea767e1":"# Exploratory Data Analysis\n\nNow that we've created lots of features, we can plot more graphs to try to visualize correlations between these new features and the target features (`Survived`).","ea862d6f":"# Feature Selection and Basic Machine Learning Base Models\n\nSo finally we can start doing some Machine Learning. \n\nWe created lots of potencial features and saw that many of them have a high correlation with your target variable (`Survived`) while some others don't seem to help. Now it's time to filter only the ones that will truly help our models to learn the discard those that won't. \n\nTo do so, we're going to perform what's called Feature Selection: selecting only the best features. \n\nSo let's begin.","94e4e198":"Finally, we need to redo the `IsChild` calculations.","940f0dd8":"### Child?","3c91cdf6":"## Recursive Feature Elimination (RFE)\nLet's now try a different way of feature selection. RFE uses some kind of metric to give each feature a grade and remove the one with the lowest grade at each step. Using RFE, we can visualize the ranks of each feature.","bd7e4ac4":"### Title\n\nThere are many titles that have very few examples, so we're going to group them under the same category.","89f6e3fa":"## CabinLetter verus Pclass","5a0fa91a":"### Alone?","b53d95f5":"### Small Group and Family","3e457f38":"This is a Python notebook for the Titanic Competition on [Kaggle](https:\/\/www.kaggle.com\/c\/titanic).\n\nIs was produced by a team from the [BeeData](https:\/\/www.linkedin.com\/company\/beedata-usp\/mycompany\/) student group of the University of S\u00e3o Paulo (Brazil).\n\nTitanic is our first competition and we based our work on some other notebooks available online.\nHere is a list of them:\n\n- [RM-W-Titanic-vF1](https:\/\/rpmarchildon.com\/wp-content\/uploads\/2018\/06\/RM-W-Titanic-vF1.html) by Ryan P. Marchildon.\n- [A Statistical Analysis & ML workflow of Titanic](https:\/\/www.kaggle.com\/masumrumi\/a-statistical-analysis-ml-workflow-of-titanic) by Masum Rumi\n\nWe hope you like =)","0bb22176":"It seems like we have a kid with `Parch = 3`. Let's look at the whole group to we if we find anything:","598c4211":"Also, we need to split again our data set into the training set and test set. The test set is the one we need to predict the target variable.","44bf4c9d":"I've already took a closer look at the data and solved all the cases one by one. As I was creating a huge section of the notebook, I'll just fix them quickly without much explanation.","f276bf35":"We have a family of five member of which four are kids.\n\nDoing some research with the family, we see that Mr. William, Mr. Edward, Miss. Robina and Miss. Doolina are siblings and Mrs. Margaret is their mother. Also, I found that indeed a sister of her (called Eliza Johnston) was on board and also a friend (Alice Harknett). Let's correct the values for this family.","3e0e9a69":"### More than one cabin\n\nThere are just very few examples of people that reserved more than one cabins. Thus, I think it's better to just have a binary value instead of a 'NumCabins' feature. ","1b555429":"## MoreCabin verus Pclass","cfd543c9":"So we're left with just two options: 'S' or 'C'. But we can look at the SplitFare value. \n\nWe can see that it's more probable that they came from city 'C', because the SplitValues are more centred around 40.0.","01931acd":"#### Group ID","8347817b":"### Surname","81725b36":"We see that much the `Family` and `Mixed` has more survivors and deceased in sizes of 2 and 3 for family and 3, 4, and 5 for mixed. And families of size 4 have amost an equal amount of survivors and deceased. So this shows that, indeed, there's a correlation between these group sizes and surviving or not.\n\nIt could also be the case that families will tend to have more child and women than people traveling alone. Let's check if that's the case.","5f818464":"Now we can proceed. I'll show you that this was the only group with different fares:","0ee7efae":"Here we see that, even though `Pclass_3` has an overall rank of `4`, it not all models agree that this is a good feature. But they agree that `Pclass_1` is a bad feature. And they also agree, with the exception of the perceptron, that `Pclass_2` is not a good feature.","c57a79b6":"Here we can have a great view of how each model perform without any parameter tuning. \n\nWe already can see that many models are overfitting the data. That's the case of almost all tree-based models. In particular we can see that AdaBoost and ExtraTrees have mean accuracy of 99% in the training set, but just about 80% in the test set.","b0920b7d":"## Models Performance on Full Feature Set\nNow let's apply these functions and get a first view of our models in action when applied to the full feature set.","6698fb6a":"Fortunately, almost all sets are better than the base set. As I tried many different version for the same kind of feature, let's try to identify what option is the best for each feature.","3cea363a":"- Here we see that `Title_Mr` is quoted as one of the best features by many algorithms with some exceptions from both linear and tree based models (Ridge, Perceptron, LinearSVC, LDA and ExtraTree), but this feature seems to be one of the bests (indeed many algorithm cite this as *the* best feature);\n- On the other hand, `Title_Mrs` doesn't seem to be a good feature (maybe because `Mr` already give the information we want. We can try don't RFE again without `Mr`);\n- We also see that `Title_Miss` doesn't have much impact;\n- And that `Title_Master` and `Title_rare` seem to be good features. But at the same time, we see that they are very good on some models but very bad or others. `Master` got very good ranks on linear models, but very bad on tree models. A similar thing happens with `rare` (and `rare` doesn't have much examples, so it might be a good idea to drop that feature).\n","62234627":"Somethings were very excepted while others are very interesting to realize.\n\n- First the graphs for `GroupNumSurvived` and `GroupNumDeceased` are very obvious\n\n- The only feature that doesn't seen to have much impact is the `NumericalTicket`column.\n\n- In the `Title` we see the same trend for girls and childs. The same thing happens in the `IsChild` column. The `rare` Title doesn't\nhave a higher change of surviving, which is very interesting (I was excepting it to be high).\n\n- One interesting thing is that indeed the `AgeMissing` column seens to have some correlation with the rates of surviving, but that's\nfar from what I personally excepted (I was excepting something like we visualized in the `CabinLetter`).\n\n- Talking about the `CabinLetter`, we can see that people with NaN in the cabin entry had a much lower change of surviving than the \npeople that we knew the cabins, specially cabins in the lobbies B, D and E (we'll visualize more about that in the future).\n\n- Now talking about families and groups. In the `IsAlone` column, we see that people traveling alone had a lower survival rate.\nBut when the divide the families and groups by size, we see that very large families and groups also have higher changes of dying. \nIndeed the higher rates are in families and groups of size 2 to 4.\n                                                                        \n- Finally, we see that the type of group really has some influence. (Or it could be just that the `Mixed` group tend to have \nabout 3 members)\n\nLet's try to compare different features to see have more insights.","ac113d2f":"# Feature Engineering \n\nNow let's create some new features that might be useful to us.\n\nThere are a lot that we can create and think. Not everything might be useful, so we need to be careful. \n\nRight now, our stragety is creating the maximum amount of features we can. Later we'll try to select only those that seen be lead to the best results.\n\nAfter reading some notebooks of other people, we came with these features for each original column:\n- `Name`:\n    - Surname\n    - Title\n- `Age`:\n    - Is Child?\n    - Is Age Missing?\n- `SibSp` and `Parch`:\n    - Family Size\n    - Is Alone?\n- `Ticket`:\n    - Group features: \n        - Group Id\n        - Group Size\n        - Group Type\n        - Group Num Survivors\n        - Group Num Deceased\n    - Is Numerical Ticket?\n- `Fare`:\n    - Split Fare (Fare divided by Group Size)\n- `Cabin`:\n    - Cabin Letter\n    - Multiple Cabins\n    \nThe columns `Pclass` and `Embarked` doesn't seen to offer much more than theirselves in their original format.","300958a3":"### Split Fare\nWhen a ticket was bought by more than one person, the original fare is multipled by the number of people that bought the ticket. Therefore, it's fair to create a new column `SplitFare` using the fare divided by the number of buyers.","3642832e":"Here we see that we have 3 groups:\n- GroupId 63 have a couple with same ticket, but one came from S and the other from C. That probably is a mistake, we need to do some research to find where did they come from;\n- GroupId is a family with two tickets. It was one of the families we put in the same group. So it could be right, but we also need to do some research;\n- GroupId is another family with the same ticket but different cities, that also is probably a mistake.\n\nLooking at the [encyclopedia-titanica](https:\/\/www.encyclopedia-titanica.org\/), we see that indeed Miss. Augusta came from S, that's a mistake.\n\nWe also see that indeed Mr. Issac Gerald embarked in C and that he is a sibling of Dr. Henry. So C is not a mistake.\n\nFinally, Miss. Amelia Bissette embarked in C with the other members of her gropus. She was the personal maid of Ella Holmes. So S was a mistake as well.","bd2db355":"## Age\n\nWe also need to decide that to do with the NaN ages. First, let's print the age distribution to see how it is before the changes we'll do. We want to fill the NaN values and keep the shape of the graph as similar as possible.","6ed03a98":"## Getting a final set of features\nSo after all this jorney to find a very good set of feature, we have four sets of features that we can try to use in order to get the best score we can. Let's set them as variables:","9522c545":"Now, let's count the number of survivors and deceased. ","24848a3a":"### Numerical Ticket\n\nA thing we can see in the ticket column is the fact that some tickets just have numbers while others have numbers and letters. We can use this as a feature as well.","5a4d5b8c":"# Find inconsistencies\n\nBefore cleaning the data, let's try to find and adjust some inconsistencies in the data. ","c95f0950":"This one will be harder, since we don't have the Cabin value to help us. Let's try to see the division of the prices by class for the city 'S'.","1cc318de":"The cell below has the list of the best estimators before traning. If you want to run every cell and don't the searches, make sure to comment the cell above and just use the cell below:","639817d5":"### Group features \n\nLet's group the tickets and measure some metrics upon these groups. Based on the surname, we can classify each ticket as a person that is traveling alone, a family, a group of friends, or a mixed between family and friends.\n\nThis can be useful because, for instance, if a whole family in the training set survived and there's another family member in the test set, odds are this person will also survive.","4b46dcdf":"Both dataframes have many null values in the *Cabin* column and some on the *Age* column.\nThe *train* df has two null values in the Embarked column and the *test* df has a null value in the fare column (it might be worth to take a look later).","6f3e04d1":"Let's begin with the first one:","4b2df647":"We can now check how they performed in the test set according to each metric (their positions don't have much if we change the sorting metric).","5d154da9":"Let's visualize the Age against many categorial variables.","b9c913c6":"Here we can see that in many cases the base score is actually better than adding more features.\n\nBetter:\n- Classes (try just class 3), Title (try each title)\n\nBetter in some case, worst in others:\n- Small, Sizes, Family\n\nAlmost the same:\n- Embarked\n\nWorst:\n- Grouptype, Bad, Ages (try just Age), CabinLetter (try each letter), ","4c97dedd":"Here we got amazing scores for the tree models. Linear and other models were also good, but tree models are defined getting the best results, so we're going to just keep analysing them (because we saw that if we find a good feature set for tree models, the other models will also benefit from it.\n\nCommeting the best results:\n- GradientBossting seem to be the best model we have so far. It got almost 85% in many sets;\n- XGB also is very good, getting the best score so far (84.96%);\n\nLet's try to check what of these sets are the best by summing the columns:","d7be31c3":"Let's now look at the third inconsistencies:\n- IsAlone = False but GroupType = Alone","b45fb14f":"## Standartization and Normalization","d96b86e1":"Before doing that, though, I found that there's one group that have different fares even buying the same ticket. So let's treat that case before proceeding:","a69a4c50":"### Testing my final opinions\nLet's know try some feature sets that I think are going to be good after doing all this analysis.","914fdfb1":"# Validations and Hypterparamether Tuning\n\nNow that we've selected the best feature set we could, let's start tuning the hyperparameter for each model and try to extract their best before moving on for the ensambles (like stacking and voting ensambles).","20bc9839":"We can see that the third inconsistencies are the same as those I said I don't know what to do.","8c9012f1":"The distribuition of the age after filling the NaN values. The two graphs must look as similar as possible.","2bb1c44a":"Let's go column by column:","ac5542bd":"Looking at `GroupType` features, we see that none of them were very good, nether on linear nor on tree models. We can try to drop them and see if the scores increase.","2c379a47":"Here we see that the third class tend to pay around 7.5.","4813b2eb":"## Embarked\n Let's take care of these few missing values in embarked and fare first.","0d50cadd":"Here we can clearly see that all tickets cost about 26.5, but larger groups have higher fare because this value is the orginal value multiplied by the number of people that bought the same ticket.","e30928cb":"### Family Size","86ad2858":"Finally, let's look at the last inconsistency:\n- IsAlone = True but GroupType = Family","eabc63d8":"Now let's try to look just at third class.","50fada28":"# Making the predicting the preparing for submition\n\nIt was being a lot jorney since here, but we're finally ready to submit our predictions. We just need to select the estimator and the features set that we consider the best to submit, make the predictions and save them into a `.csv` file.","f66ddfb2":"And let's define all the base models we're going to test:","e22d0841":"For the cabin letters, we have many cases. Let's start with the easy one:\n- `CabinLetter_N` is one of the best features and we already were expecting that because we saw in the EDA section that there was a huge difference between having a cabin assigned to a row or not;\n\n- `CabinLetter_T` is quoted as best than letters A and F, but that's because some linear classifiers think this is a good, but we know that we actually should have already dropped this feature because it's completely useless;\n\n- `CabinLetter_A` and `CabinLetter_F` as quoted as the worst features in our dataset. Going back to the EAD section, we see that these cabins don't have many examples (they have about 20 instances) so that could be the reason why they are not good;\n\n- `CabinLetter_C`, on the other hand, has about 100 entries but it's also listed as a bad feature. I think it could be because everyone on that desk is from the first class, but I don't know;\n\n- `CabinLetter_D` and `CabinLetter_E` are listed as good features. That could be because there are many classes on those desks and because they have higher chances of surviving. \n\n- Finally, `CabinLetter_B` and `CabinLetter_G` are listed as median features.\n\nI'll definely keep `CabinLetter_N` and we can test keeping letters `B`, `D`, `E` and maybe `G`.","3a315d95":"#### Num Group Survivors and Deceased\n\nNow we're going to calculate the number of survivors of a group. We saw that using the ticket we can find many people that were traveling together. So we can count the number of survivors of the groups. That's a good feature because if we have 5 people in a group and four of them survived and the fifth one we don't know, the odds are higher that this person will also survive. \n\nHowever, this number could be misleading. Suppose we have two groups, both with four people:\n- One of the groups have three people that we know died in the accident and the four we need to predict;\n- The other groups have four people that we need to predict.\n\nIf we simply count the number of people we know survived, both groups will have a value of zero! Therefore, it's important to have not just the number of survivors metric, but also the number of deceased. Then, group 1 will have a deceased number of 3 while group 2 will have a number of 0. ","010ab630":"Some conclusions for this part are:\n- The only cabins with people from Q are C, E and F;\n- Decks (or lobbies) A, B and C just have people from the first class (Pclass = 1);\n- More than 90% of people who embarked from Q are from third class (Pclass = 3);\n- The other two cities have more passengers and their class distribution is more similar.\n- Deck F doesn't have any passengers from class 1.\n- B, D and E have the higher survival rates. So that means the it might not be necessarly due to the class or city, since\nthese decks have people from all classes and cities (if it was very correlated with the class, we would except that the \nhigher survival rates would be in A, B and C, since these only have first class passengers);","b4601ffc":"## New Features\n\nMotiveted by the EAD process, we can create new features that will reinforce the trends we saw in the data.\n\nThe features I'll create are:\n- SmallGroup (2 up to 4 members)\n- SmallFamily (2 up to 4 members)","4f007e56":"## Embarked versus Pclass","23fa5f25":"Here we see that indeed using all the classes or not doesn't affect much and it's better not using `Age` and `Embarked`.","6dbf7bbf":"We see:\n- `Age` indeed an horrible feature (that could the way the filled the values or something like that, I really don't know);  \n- `Embarked` is also not good;\n- Using or not all the classes doesn't metter much;\n- Same thing for the size;\n- Using only `Mr`, `Master` and `rare` is better.\n\nLet's check the best sets:","01c138b0":"# Cleaning data\n\nLet's now see what we'll do about the null data we have in the data sets.","acdd1430":"## Feature Importances\n\nAnd let's examine the feature importances that each model gave to each feature and start checking which features we'll keep or not.","294641f0":"Indeed, as we suspected, the `Family` and `Mixed` types have much more Child and Women rates than the other types. This could explain the trends we saw.","b88bb0f3":"We see that there's no difference in `Sex` and `Embarked`. \n\nThe `Title` have many differences, specially in the `Master` and `rare` categories. \n\nOne more interesting thing is the `Pclass`. We see that the higher the class, the higher the median age. \n\nAnother good variable to see is the `Parch`. That's because `Parch` equal to `1` or `2` are mainly kids and greater than that will never be a child, therefore it can be also used. The same thing happens with the `SibSp`, adults will not have more than one `Spouse` and kids tend to have a few siblings.\n\n`FamilySize` and `GroupSize` don't seem to help a lot, so let's use `Title`, `Pclass`, `Parch` and `SibSp` and just use `Sex` to untie.","7be560b3":"There are many interesting things we visualize here.\n\n- The `Pclass_1` feature is considered one of the worst features. That's very interesting, because we saw there's a high correlaction between the classes and the survival hate;\n- The `Pclass_2` is also considered a very bed feature. Perhaps the Pclass have some dependence with other features or the most important class is the third. We see that it's market as the third best feature;\n- `IsChild` is also market as a bad feature, probably because the `Age` is enough;\n- `MoreCabins` is another irrelevant feature;\n- `AgeMissing` and `NumericalTicket` are also not important (we also saw this in the EDA section);\n- One very interesting thing is that `Title_rare` was market as a good feature, that seems a bit strange;\n- Another very strange thing is the fact that `Age` is market as a bed feature;\n\nLet's try to understand better checking only some feature at time.","abd26857":"Here we see that surprisingly there are about 7 people from class 3 with more than one cabin. Let's try to look at these entried:","0c460eab":"# Titanic Competition on Kaggle","263a1a42":"Now let's define some functions that will be useful to evalute our models.","6a3e4884":"# Pre-Processing the Data\n\nThe last step before finally going into the Machine Learning models is pre-processing the data.\n\nThe steps we need to do are:\n- change the `Sex` column from `string` to `integer`;\n- do One-Hot Encoding in the categorial columns (like `Pclass` and `Embarked`);\n- scale the numerical features to help the models (using *MinMax* or *Standartization*).","49574da2":"`Sex`, with the exception of Decision Tree, is considered a very good feature for all models. So indeed, it will be a feature that we want to keep.","7d4a16da":"## Title and Sex ","78d16445":"First, there are some exceptions we found previously. So let's group those exceptions together:","8c103dfd":"## Sex \nWe need to transform the Sex column into a numerical column. \n\nWe'll use:\n- 0 for male\n- 1 for female","572399b7":"##### Inconsistencies\n\nWe still need to fix some group types and look closer the data in some cases. \n\nThis process is very boring, so I'll try to be lean.\n\nThe types of inconsistencies we can find (or at least the ones I tought) are:\n- GroupSize \u2260 FamilySize where GroupType = Family \n- FamilySize > 1 but GroupType \u2260 Family\n- IsAlone = False but GroupType = Alone\n- IsAlone = True but GroupType = Family","0694da11":"## One-hot encoding\nFinally, we need to change the non-binary categorical values into binary values using one-hot encoding.","63b3b9c2":"We see that the `Age` is considered a good feature for many of the Tree-Based models, but a really bad feature for all linear classifiers that makes the overall rank to be very bad, but we don't have to really remove this feature. \n\nAnother thing we see is the fact that `AgeMissing` and `IsChild` are not considered good by any model, so we probably can remove them. This is probably because `Age` have all the information we need.","b4ad7a44":"### Testing each subset of similar features","89f4337a":"We see that, actually, we should have just G or E as they cabin letters, so we'll fix that.","d68a0507":"Here we see that, indeed, their tickets are sequential.","85a90ed4":"Let's take a look at our original test set with the new column `Prediction` to understand the decisions of our model.","dd61466e":"Let's now look at the second inconsistency:\n- FamilySize > 1 but GroupType \u2260 Family","f7c9813c":"Visualizing the predicitons versus the trend in the traning set.","3b684d42":"## GroupType and GroupSize","e252cc1f":"Here we see again that some features are very good on tree models while not that good on linear models. \n\nWe can see that many of these features (`SibSp`, `Parch`, `FamilySize`, `GroupSize` and `SmallGroup`)  have a better rank on tree models. So even if they are not listed as very good features, we might want to keep some of them because the tree-based models got the better results. \n\n- When comparing the family features, we see that `SibSp` and `SmallFamily` got the better ranks, but `SmallFamily` wasn't good on tree-based models, while `FamilySize` was (so we might want to stay with `FamilySize`). \n- When comparing group features, we see that `GroupSize` was better in the majority of the situation, so we might want to keep it instead of `SmallGroup`.","32f5ef77":"With this analysis, we see that the sex and class are very important to decide if a person will survive or not.","9573fb4b":"So there are about 20% of missing ages and about 78% of missing cabins. While there is 2 embarked missing in the training set and one fare missing in the test set.","7de210c2":"There are still some inconsistencies that I did know how to resolve:","3bafae69":"### Age Missing?\nThere are many NaN values in the Age column. In the feature, I'll be look for the best way to find these NaN values, but I don't want to lose the NaNs, beucase they can be meaningful. For instance, there could be a higher probabilty of not surving if we today don't know the age of that person. ","9ca10439":"## Testing Different Feature Sets\nSo now let's try many different feature sets using the information we got from the previous tests.","6fea464b":"### Settings ","586af65a":"#### Group Type","300129f1":"Now let's define the models and hyperparameters we're going to search over.","76524253":"We don't have any cases. So we're ready to proceed.","3a257eef":"Now every numberial feature has mean equal 0 and standart deviation equal to 1. That helps almost every non-tree based model.","363168df":"We also see that it's not a normal distribution, so let's take the median instead of the mean.","397149c6":"## Age and Parch\nWe except that passengers who are kids shouldn't have a `Parch` greater than 2."}}