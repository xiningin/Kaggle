{"cell_type":{"9d0c79ea":"code","f0fad51e":"code","987085c6":"code","de076a0b":"code","940666f1":"code","315a88bd":"code","0f19aac0":"code","ea6863b3":"code","3f998b85":"code","1e49c9d0":"code","f871ca03":"code","d28875b3":"code","3405f2b3":"code","2cec4b90":"code","822887b5":"code","09f34ca1":"code","ce93504b":"code","4b6bf5f0":"code","18c02e69":"code","3200e615":"code","78b9c854":"code","9c0397e9":"code","b59fe172":"code","36fac9a7":"code","dc80bd6f":"code","c58ed028":"code","70ca6a5f":"code","e54652ed":"code","791b74df":"code","e262291b":"code","ed7d234d":"code","65d2dd01":"code","f3d820e0":"code","08d466af":"code","7dd7d0ea":"code","214101a3":"code","bec2c337":"code","899889ab":"code","440e6094":"code","1e25568a":"code","752284b0":"code","add45362":"code","a7d3d661":"code","c641a833":"code","8e14ba72":"code","c52d13fa":"code","96b1ded3":"code","80766743":"code","24705b06":"code","a178493e":"code","4d35de87":"code","8aa25179":"code","30a271ee":"code","d1bc3e52":"markdown","39654391":"markdown","374e832c":"markdown","7fd4b440":"markdown","d51e573b":"markdown","a5a773e5":"markdown","8da14a17":"markdown","42824866":"markdown","91199061":"markdown","5e8aad89":"markdown","59cbfa5c":"markdown","bf8d081b":"markdown","2d5beebb":"markdown","5d678948":"markdown","0849f687":"markdown","0630b4b2":"markdown","15131c47":"markdown","409f9081":"markdown","439cd2a1":"markdown","bffee00e":"markdown","2b255880":"markdown","f9a3390d":"markdown"},"source":{"9d0c79ea":"import os\nimport numpy as np\nfrom os import path\nfrom urllib.request import urlretrieve\nimport sklearn\nfrom sklearn.manifold import TSNE\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nimport unidecode\nEPSILON = 1e-15","f0fad51e":"nltk.download('punkt')","987085c6":"class PretrainedEmbeddings():\n    def __init__(self, language, embeddings):\n        self.vec_file = None\n        if language == 'en':\n            if embeddings == 'glove':\n                self.vec_file = 'glove_100k.en.vec'\n            elif embeddings == 'ft':\n                self.vec_file = 'ft_300k.en.vec'\n        elif language == 'fr':\n            if embeddings == 'glove':\n                print('No GloVe french embeddings!')\n                return None\n            elif embeddings == 'ft':\n                self.vec_file = 'ft_50k.fr.vec'\n        self.language = language\n        self.url = \"https:\/\/github.com\/ECE-Deep-Learning\/courses_labs\/releases\/download\/0.1\/\" + self.vec_file\n        self.file_location = os.path.join('..\/input\/', self.vec_file)\n        self.embeddings_index = None\n        self.embeddings_index_inversed = None\n        self.embeddings_vectors = None\n        self.voc_size = None\n        self.dim = None\n    \n    @staticmethod\n    def _normalize(array):\n        return array \/ np.linalg.norm(array, axis=-1, keepdims=True)\n        \n    def download(self):\n        if not path.exists(self.file_location):\n            print('Downloading from %s to %s...' % (self.url, self.file_location))\n            urlretrieve(self.url, self.file_location)\n            print('Downloaded embeddings')        \n            \n    # Note that you can choose to normalize directly the embeddings \n    # to make the cosine similarity computation easier afterward\n    def load(self, normalize=False):\n        self.embeddings_index, self.embeddings_index_inversed = {}, {}\n        self.embeddings_vectors = []\n        file = open(self.file_location, encoding='utf-8')\n        header = next(file)\n        self.voc_size, self.dim = [int(token) for token in header.split()]\n        print('Vocabulary size: {0}\\nEmbeddings dimension: {1}'.format(self.voc_size, self.dim))\n        print('Loading embeddings in memory...')\n        for idx, line in enumerate(file):\n            values = line.split()\n            word = values[0]\n            vector = np.asarray(values[1:], dtype='float32')\n            self.embeddings_index[word] = idx\n            self.embeddings_index_inversed[idx] = word\n            self.embeddings_vectors.append(vector)\n        self.embeddings_vectors = np.asarray(self.embeddings_vectors)\n        print('Embeddings loaded')\n        if normalize:\n            self.embeddings_vectors = self._normalize(self.embeddings_vectors)\n            print('Embeddings normalized')\n        file.close()\n        \n    # Return an embedding vector associated to a given word\n    # For this you are supposed to used the objects defined in the load function\n    # Be sure to handle the case where the received word is not found in the embeddings' vocabulary\n    def word_to_vec(self, word):\n        # TODO:\n        if word in self.embeddings_index:\n            return self.embeddings_vectors[self.embeddings_index[word]]\n        else:\n            return None\n        #else:\n            #print(\"Error : word is not found in the embeddings' vocabulary\")\n    \n    #\u00a0Return the closest word associated to a given embedding vector\n    # The vector passed as argument might not be in self.embeddings_vectors\n    #\u00a0In other terms, you have to compute every cosine similarity between the vec argument\n    # and the embeddings found in self.embeddings_vectors. Then determine the embedding in \n    # self.embeddings_vectors with the highest similarity and return its associated string word\n    def vec_to_word(self, vec, n=1):\n        # TODO:\n        #First Technique # seem to have the same result than the second one.\n        #w2 = self.embeddings_vectors\n        #resultat = []\n        #cosine = np.dot(w2, vec.T) \/ (np.linalg.norm(w2) * np.linalg.norm(vec))\n        #topResultats = cosine.argsort()[::-1][:n]\n        #for vec in topResultats:\n        #    resultat.append(self.embeddings_index_inversed[vec])\n        #return resultat\n    \n        cos = np.dot(self.embeddings_vectors, np.transpose(vec))\/(np.linalg.norm(self.embeddings_vectors)*np.linalg.norm(vec))\n        words = [self.embeddings_index_inversed[item] for item in reversed(np.argsort(cos)[-n:])]\n        if n==1: return words[0]\n        else: return words\n\n    # Return the n top similar words from a given string input\n    # The similarities are based on the cosine similarities between the embeddings vectors\n    #\u00a0Note that the string could be a full sentence composed of several words\n    # Split the sentence, map the words that can be found in self.embeddings_vectors to vectors and\n    # average them. Then return the top (default: top=10) words associated to the top embeddings \n    # in self.embeddings_vectors that have the highest cosine similarity with the previously computed average\n    def most_similar(self, query, top=10):\n        # TODO:\n        vecs = []\n        for word in query.split():\n            vecs.append(self.word_to_vec(word))\n        return self.vec_to_word(np.mean(vecs, axis=0), top)\n    \n    def project_and_visualize(self, sample=1000):\n        embeddings_tsne = TSNE(perplexity=30).fit_transform(self.embeddings_vectors[:sample])\n        plt.figure(figsize=(40, 40))\n        axis = plt.gca()\n        np.set_printoptions(suppress=True)\n        plt.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], marker=\".\", s=1)\n        for idx in range(sample):\n            plt.annotate(\n                self.embeddings_index_inversed[idx],\n                xy=(embeddings_tsne[idx, 0], embeddings_tsne[idx, 1]),\n                xytext=(0, 0), textcoords='offset points'\n            )","de076a0b":"pretrained_embeddings = PretrainedEmbeddings(language='en', embeddings='glove')\n#pretrained_embeddings.download()\npretrained_embeddings.load(normalize=True)\nvec = pretrained_embeddings.word_to_vec(\"annual\")\npretrained_embeddings.vec_to_word(vec)","940666f1":"pretrained_embeddings.project_and_visualize()","315a88bd":"pretrained_embeddings.most_similar('french city')","0f19aac0":"class LanguageModel():\n    def __init__(self):\n        self.corpus_path = None\n        self.corpus = None\n    \n    def load_data(self, corpus_path):\n        self.corpus_path = os.path.join('..\/input\/', corpus_path)\n        file = open(self.corpus_path, encoding=\"utf-8\")\n        self.corpus = unidecode.unidecode(file.read().lower().replace(\"\\n\", \" \"))\n        print('Corpus length: {0} characters'.format(len(self.corpus)))\n        file.close()\n    \n    def get_contiguous_sample(self, size):\n        index = np.random.randint(1, len(self.corpus) - size)\n        return self.corpus[index:index+size]","ea6863b3":"sample_size = 500\n\nlanguage_model = LanguageModel()\nlanguage_model.load_data('rousseau.txt')\nprint('Sample of {0} characters:\\n{1}'.format(\n    sample_size, language_model.get_contiguous_sample(sample_size)\n))","3f998b85":"from keras.models import Sequential\nfrom keras.layers import LSTM, Dense\nfrom keras.optimizers import Adam\n\nclass CharLanguageModel(LanguageModel):\n    def __init__(self):\n        super(LanguageModel, self).__init__()\n        self.char_index = None\n        self.char_index_inversed = None\n        self.vocabulary_size = None\n        self.max_length_sequence = None\n        self.X = None\n        self.y = None\n        self.model = None\n    \n    def extract_vocabulary(self):\n        chars = sorted(set(self.corpus))\n        self.char_index = dict((c, i) for i, c in enumerate(chars))\n        self.char_index_inversed = dict((i, c) for i, c in enumerate(chars))\n        self.vocabulary_size = len(self.char_index)\n        print('Vocabulary size: {0}'.format(self.vocabulary_size))\n        \n    def plot_vocabulary_distribution(self):\n        counter = Counter(self.corpus)\n        chars, counts = zip(*counter.most_common())\n        indices = np.arange(len(counts))\n        plt.figure(figsize=(16, 5))\n        plt.bar(indices, counts, 0.8)\n        plt.xticks(indices, chars)\n        \n    \"\"\"\n    Convert X and y into one-hot encoded matrices\n    \n    Importante note: if the sequence length if smaller than max_length_sequence, \n    we pad the input with zeros vectors at the beginning of the one-hot encoded matrix\n    \"\"\"\n    def _one_hot_encoding(self, X, y):\n        X_one_hot = np.zeros(\n            (len(X), self.max_length_sequence, self.vocabulary_size), \n            dtype=np.float32\n        )\n        y_one_hot = np.zeros(\n            (len(X), self.vocabulary_size), \n            dtype=np.float32\n        )\n        # Leave above code as it is, change X_one_hot and y_one_hot below\n        # TODO:\n        for i in range(len(X)):\n            for j in range(len(X[i])):\n                X_one_hot[i][self.max_length_sequence - len(X[i]) + j][self.char_index.get(X[i][j])] = 1\n        \n        if y is not None:\n            for i in range(len(y)):\n                y_one_hot[i][self.char_index.get(y[i])] = 1\n        return X_one_hot, y_one_hot \n    \n    \"\"\"\n    The matrices X and y are created in this method\n    It consists of sampling sentences in the corpus as training vectors with the next character as target\n    \"\"\"\n    def build_dataset(self, \n                      max_length_sequence=40, min_length_sentence=5, max_length_sentence=200, \n                      step=3):\n        self.X, self.y = [], []\n        \n        sentences = sent_tokenize(self.corpus)\n        sentences = filter(\n            lambda x: len(x) >= min_length_sentence and len(x) <= max_length_sentence, \n            sentences\n        )\n        for sentence in sentences:\n            for i in range(0, max(len(sentence) - max_length_sequence, 1), step):\n                last_index = min(i+max_length_sequence, i+len(sentence)-1)\n                self.X.append(sentence[i:last_index])\n                self.y.append(sentence[last_index])\n\n        self.max_length_sequence = max_length_sequence\n        self.X, self.y = sklearn.utils.shuffle(self.X, self.y)\n        print('Number of training sequences: {0}'.format(len(self.X)))\n        self.X, self.y = self._one_hot_encoding(self.X, self.y)\n        print('X shape: {0}\\ny shape: {1}'.format(self.X.shape, self.y.shape))\n    \n    \"\"\"\n    Define, compile, and fit a Keras model on (self.X, self.y)\n    It should be composed of :\n        - one recurrent LSTM layer projecting into hidden_size dimensions\n        - one Dense layer with a softmax activation projecting into vocabulary_size dimensions\n    \"\"\"\n    def train(self, hidden_size=128, batch_size=128, epochs=10):\n        # TODO:\n        self.model = Sequential()\n        self.model.add(LSTM(units = hidden_size))\n        self.model.add(Dense(units = self.vocabulary_size, activation=\"softmax\"))\n        self.model.compile(optimizer=Adam(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n        self.model.fit(x=self.X, y=self.y, batch_size=batch_size, epochs=epochs,verbose=1)\n\n    \n    \"\"\"\n    Return the prediction of our model, meaning the next token given an input sequence\n    \n    If preprocessed is specified as True, we consider X as an array of strings and we will transform\n    it to a one-hot encoded matrix\n    Importante note: if the sequence length if smaller than max_length_sequence, \n    we pad the input with zeros vectors at the beginning of the one-hot encoded matrix\n    \n    If preprocessed is specified as False, we apply the model predict on X as it is\n    \"\"\"\n    def predict(self, X, verbose=1, preprocessed=True):\n        if not preprocessed:\n            X_one_hot = np.zeros(\n                (len(X), self.max_length_sequence, self.vocabulary_size), dtype=np.float32\n            )\n            # Leave above code as it is, change X_one_hot below\n            # TODO:\n            X_one_hot, _ = self._one_hot_encoding(X, None)\n        else:\n            X_one_hot = X\n        return self.model.predict(X_one_hot, verbose=verbose)\n    \n    # Perplexity metric used to appreciate the performance of our model\n    def perplexity(self, y_true, y_pred):\n        likelihoods = np.sum(y_pred * y_true, axis=1)\n        return 2 ** -np.mean(np.log2(likelihoods + EPSILON))\n    \n    \"\"\"\n    Sample the next character according to the predictions.\n    \n    Use a lower temperature to force the model to output more\n    confident predictions: more peaky distribution.\n    \"\"\"\n    def _sample_next_char(self, preds, temperature=1.0):\n        preds = np.asarray(preds).astype('float64')\n        preds = np.log(preds + EPSILON) \/ temperature\n        exp_preds = np.exp(preds)\n        preds = exp_preds \/ np.sum(exp_preds + EPSILON)\n        probs = np.random.multinomial(1, preds, size=1)\n        return np.argmax(probs)\n    \n    def generate_text(self, seed_string, length=300, temperature=1.0):\n        if self.model is None:\n            print('The language model has not been trained yet!')\n            return None\n        generated = seed_string\n        prefix = seed_string\n        for i in range(length):\n            predictions = np.ravel(self.predict([prefix], verbose=0, preprocessed=False))\n            next_index = self._sample_next_char(predictions, temperature)\n            next_char = self.char_index_inversed[next_index]\n            generated += next_char\n            prefix = prefix[1:] + next_char\n        return generated","1e49c9d0":"language_model = CharLanguageModel()\nlanguage_model.load_data('rousseau.txt')","f871ca03":"language_model.extract_vocabulary()\nlanguage_model.char_index","d28875b3":"language_model.plot_vocabulary_distribution()","3405f2b3":"language_model.build_dataset()","2cec4b90":"epochs = 5\nlanguage_model.train(epochs=epochs)\nif language_model.model is not None:\n    print('Perplexity after {0} epochs: {1}'.format(\n        epochs, language_model.perplexity(language_model.y, language_model.model.predict(language_model.X))\n    ))","822887b5":"language_model.generate_text(\"l'etat n'est pas au-dessus de la loi\", temperature=0.25)","09f34ca1":"language_model.generate_text(\"la republique\", temperature=0.25)","ce93504b":"from spacy.lang.fr import French\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense\nfrom keras.optimizers import Adam\n\nclass WordLanguageModel(LanguageModel):\n    def __init__(self):\n        super(LanguageModel, self).__init__()\n        self.pretrained_embeddings = PretrainedEmbeddings(language='fr', embeddings='ft')\n        self.pretrained_embeddings.download()\n        self.pretrained_embeddings.load()\n        self.parser = None\n        self.word_index = None\n        self.word_index_inversed = None\n        self.vocabulary_size = None\n        self.max_length_sequence = None\n        self.tokens = None\n        self.X = None\n        self.y = None\n        self.model = None\n        \n    def extract_vocabulary(self, max_vocabulary=1500000):\n        self.parser = French(max_length=max_vocabulary)\n        self.tokens = [token.orth_ for token in self.parser(self.corpus) if token.is_alpha]\n        unique_tokens = set(self.tokens)\n        self.word_index = dict((w, i) for i, w in enumerate(unique_tokens))\n        self.word_index_inversed = dict((i, w) for i, w in enumerate(unique_tokens))\n        self.vocabulary_size = len(self.word_index)\n        print('Vocabulary size: {0}'.format(self.vocabulary_size))\n        \n    \"\"\"\n    Convert X and y into embedded matrices\n    Hint: use the self.pretrained_embeddings.word_to_vec method for each token found\n    \n    Importante notes: \n    - if the sequence length if smaller than max_length_sequence, \n    we pad the input with zeros vectors at the beginning of the embedded matrix\n    - if a word is not found in self.pretrained_embeddings.word_to_vec then word should be\n    mapped to a vector of zeros instead\n    \"\"\"\n    def _token_embedding(self, X, y):\n        X_embedding = np.zeros(\n            (len(X), self.max_length_sequence, self.pretrained_embeddings.dim), \n            dtype=np.float32\n        )       \n        y_one_hot = np.zeros(\n            (len(X), self.vocabulary_size), \n            dtype=np.float32\n        )\n        # Leave above code as it is, change X_embedding and y_one_hot below\n        # TODO:\n        for i in range(len(X)):\n            for j in range(len(X[i])):\n                word = X[i][j]\n                if word is not None:\n                    if self.pretrained_embeddings.word_to_vec(word) is not None:\n                        X_embedding[i][j] = self.pretrained_embeddings.word_to_vec(word)\n                    \n        if y is not None:\n            for i in range(len(y)):\n                y_one_hot[i][self.word_index.get(y[i])] = 1\n        \n        return X_embedding, y_one_hot\n        \n    def build_dataset(self, max_length_sequence=40, step=3):\n        self.X, self.y = [], []\n        for i in range(0, len(self.tokens) - max_length_sequence, step):\n            self.X.append(self.tokens[i:i+max_length_sequence])\n            self.y.append(self.tokens[i+max_length_sequence])\n        self.max_length_sequence = max_length_sequence\n        self.X, self.y = sklearn.utils.shuffle(self.X, self.y)\n        print('Number of training sequences: {0}'.format(len(self.X)))\n        self.X, self.y = self._token_embedding(self.X, self.y)\n        print('X shape: {0}\\ny shape: {1}'.format(self.X.shape, self.y.shape))\n        \n    \"\"\"\n    Define, compile, and fit a Keras model on (self.X, self.y)\n    It should be composed of :\n        - one or many recurrent LSTM layers projecting into hidden_size dimensions\n        - one Dense layer with a relu activation projecting into hidden_size dimensions\n        - one Dense layer with a softmax activation projecting into vocabulary_size dimensions\n    \"\"\"\n    def train(self, hidden_size=128, batch_size=128, epochs=10):\n        # TODO:\n        self.model = Sequential()\n        self.model.add(LSTM(units = hidden_size, return_sequences=True))\n        self.model.add(LSTM(units = hidden_size))\n        self.model.add(Dense(units = hidden_size, activation=\"relu\"))\n        self.model.add(Dense(units = self.vocabulary_size, activation=\"softmax\"))\n        self.model.compile(optimizer=Adam(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n        self.model.fit(x=self.X,y=self.y, batch_size=batch_size, epochs=epochs,verbose=1)\n        \n    \"\"\"\n    Return the prediction of our model, meaning the next token given an input sequence\n    \n    If preprocessed is specified as True, we consider X as an array of strings and we will transform\n    it to an embedded matrix using self.pretrained_embeddings.word_to_vec\n    Importante note: if the sequence length if smaller than max_length_sequence, \n    we pad the input with zeros vectors at the beginning of the embedded matrix\n    \n    If preprocessed is specified as False, we apply the model predict on X as it is\n    \"\"\"\n    def predict(self, X, verbose=1, preprocessed=True):\n        if not preprocessed:\n            X_embedding = np.zeros(\n                (len(X), self.max_length_sequence, self.pretrained_embeddings.dim), \n                dtype=np.float32\n            )\n            # Leave above code as it is, change X_embedding\n            # TODO:\n            X_embedding, _ = self._token_embedding(X, None)\n        else:\n            X_embedding = X\n        return self.model.predict(X_embedding, verbose=verbose)\n    \n    \"\"\"\n    Sample the next word according to the predictions.\n    \n    Use a lower temperature to force the model to output more\n    confident predictions: more peaky distribution.\n    \"\"\"\n    def _sample_next_word(self, preds, temperature=1.0):\n        preds = np.asarray(preds).astype('float64')\n        preds = np.log(preds + EPSILON) \/ temperature\n        exp_preds = np.exp(preds)\n        preds = exp_preds \/ np.sum(exp_preds + EPSILON)\n        probs = np.random.multinomial(1, preds, size=1)\n        return np.argmax(probs)\n    \n    def generate_text(self, seed_string, length=50, temperature=1.0):\n        if self.model is None:\n            print('The language model has not been trained yet!')\n            return None\n        seed_tokens = [token.orth_ for token in self.parser(seed_string) if token.is_alpha]\n        prefix = seed_tokens\n        generated = seed_tokens\n        for i in range(length):\n            predictions = np.ravel(self.predict([prefix], verbose=0, preprocessed=False))\n            next_index = self._sample_next_word(predictions)\n            next_word = self.word_index_inversed[next_index]\n            generated += [next_word]\n            prefix = prefix[1:] + [next_word]\n        return \" \".join(generated)","4b6bf5f0":"language_model = WordLanguageModel()\nlanguage_model.load_data('rousseau.txt')","18c02e69":"language_model.extract_vocabulary()","3200e615":"language_model.build_dataset()","78b9c854":"epochs = 5\nlanguage_model.train(epochs=epochs)","9c0397e9":"language_model.generate_text(\"un \u00e9tat ne saurait r\u00e9ussir \u00e0\", temperature=0.5)","b59fe172":"language_model.generate_text(\"la r\u00e9publique ne doit pas\", temperature=1)","36fac9a7":"from sklearn.datasets import fetch_20newsgroups\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences","dc80bd6f":"newsgroups_train = fetch_20newsgroups(subset='train')\nnewsgroups_test = fetch_20newsgroups(subset='test')","c58ed028":"print(\"sklearn object type : {}\".format(type(newsgroups_train)))\nprint(\"sklearn object keys :\")\nfor k in newsgroups_train:\n    print(k)","70ca6a5f":"print(\"Classes to predict : {}\".format(os.linesep.join(newsgroups_train['target_names'])))\nprint()\nprint(\"Integer mapped-classes to predict :\")\nprint(newsgroups_train['target'])","e54652ed":"class_int_str = dict(\n    zip(range(len(newsgroups_train['target_names'])), newsgroups_train['target_names'])\n)","791b74df":"class_int_str","e262291b":"print(\"Example of document in dataset:\", os.linesep)\nsample_idx = np.random.randint(len(newsgroups_train[\"data\"]))\nprint(newsgroups_train[\"data\"][sample_idx])\nsample_idx_class = class_int_str[newsgroups_train[\"target\"][sample_idx]]\nprint(\"Example class to predict : {}\".format(sample_idx_class))","ed7d234d":"MAX_NB_WORDS = 20000  # number of different integers mapping our vocabulary\n\n# get the raw text data\ntexts_train = newsgroups_train[\"data\"]\ntexts_test = newsgroups_test[\"data\"]\n\n# finally, vectorize the text samples into a 2D integer tensor of shape (nb_sequences, sequence_length)\n# TODO:\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS)\ntokenizer.fit_on_texts(texts_train)\nsequences_train = tokenizer.texts_to_sequences(texts_train)\nsequences_test = tokenizer.texts_to_sequences(texts_test)\n\nif tokenizer is not None:\n    word_index = tokenizer.word_index\n    print('Found %s unique tokens.' % len(word_index))","65d2dd01":"print(\"First raw text example: \", os.linesep, texts_train[0])","f3d820e0":"if sequences_train is not None:\n    print(\"First text conversion to token_ids: \", os.linesep, sequences_train[0])\n    print(\"First text number of token_ids: {}\".format(len(sequences_train[0])))","08d466af":"if tokenizer is not None:\n    word_to_index = tokenizer.word_index.items()\n    index_to_word = dict((i, w) for w, i in word_to_index)","7dd7d0ea":"if sequences_train is not None:\n    print(\"Original sentence retrieved :\", os.linesep)\n    print(\" \".join([index_to_word[i] for i in sequences_train[0]]))","214101a3":"MAX_SEQUENCE_LENGTH = 200\n\n# pad 1-D sequences with 0s\n# use the pad_sequences method on your sequences\n# TODO:\nx_train = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\nx_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\nif x_train is not None and x_test is not None:\n    print('Shape of data tensor:', x_train.shape)\n    print('Shape of data test tensor:', x_test.shape)","bec2c337":"if x_train is not None:\n    print(\"Example of tensor after padding\/truncating : \", os.linesep, x_train[0])","899889ab":"y_train = newsgroups_train[\"target\"]\ny_test = newsgroups_test[\"target\"]\n\n# One-hot encode integer-mapped classes\ny_train_onehot = to_categorical(np.asarray(y_train))\nprint('Shape of train target tensor:', y_train_onehot.shape)","440e6094":"from keras.models import Sequential\nfrom keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout\nfrom keras.optimizers import Adam\n\nEMBEDDING_DIM = 50\nN_CLASSES = y_train_onehot.shape[1]\n\n# TODO:\nmodel = Sequential()\nmodel.add(Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\nmodel.add(GlobalAveragePooling1D())\nmodel.add(Dense(units = N_CLASSES,activation=\"softmax\"))\nmodel.compile(optimizer=Adam(),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n#model.compile(optimizer=Adam(),loss=\"categorical_crossentropy\",metrics=['Accuracy', 'Loss'])","1e25568a":"model.summary() if model is not None else None","752284b0":"if model is not None and x_train is not None:\n    model.fit(x_train, y_train_onehot, validation_split=0.1,\n              epochs=150, batch_size=128)","add45362":"if model is not None and x_test is not None:\n    print(\"test accuracy:\", np.mean(model.predict(x_test).argmax(axis=-1) == y_test))","a7d3d661":"\"\"\"\nGet an input tensor and replace the word->integer mapping with pretrained embeddings\nBe sure that the word is existing (not a 0 padding) and is in the embeddings' vocabulary\n\"\"\"\n\ndef preprocess_with_pretrained_embeddings(X, language, embeddings):\n    pretrained_embeddings = PretrainedEmbeddings(language=language, embeddings=embeddings)\n    pretrained_embeddings.download()\n    pretrained_embeddings.load()\n    X_embedding = np.zeros((X.shape[0], X.shape[1], pretrained_embeddings.dim))\n    # TODO:\n    for i in range(len(X)):\n        for j in range(len(X[i])):\n            word = index_to_word[X[i][j]] if X[i][j] != 0 else None\n            if word is not None:\n                if pretrained_embeddings.word_to_vec(word) is not None:\n                    X_embedding[i][j] = pretrained_embeddings.word_to_vec(word)\n\n    return X_embedding","c641a833":"x_train","8e14ba72":"if x_train is not None and x_test is not None:\n    x_train_embedding = preprocess_with_pretrained_embeddings(x_train, language='en', embeddings='glove')\n    x_test_embedding = preprocess_with_pretrained_embeddings(x_test, language='en', embeddings='glove')\n    print('Embedded training matrix shape:', x_train_embedding.shape)\n    print('Embedded test matrix shape:', x_test_embedding.shape)","c52d13fa":"from keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Dropout\nfrom keras.optimizers import Adam\n\nN_CLASSES = y_train_onehot.shape[1]\nDIM_1 = x_train_embedding.shape[1]\nDIM_2 = x_train_embedding.shape[2]\n\n# TODO:\nmodel = Sequential()\nmodel.add(GlobalAveragePooling1D(input_shape=(DIM_1, DIM_2)))\nmodel.add(Dense(units = N_CLASSES, activation=\"softmax\"))\nmodel.compile(optimizer=Adam(),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])","96b1ded3":"model.summary() if model is not None else None","80766743":"if model is not None and x_train_embedding is not None:\n    model.fit(x_train_embedding, y_train_onehot, validation_split=0.1, \n              epochs=200, batch_size=128)","24705b06":"if model is not None and x_test_embedding is not None:\n    print(\"test accuracy:\", np.mean(model.predict(x_test_embedding).argmax(axis=-1) == y_test))","a178493e":"from keras.models import Sequential\nfrom keras.layers import Embedding, MaxPooling1D, LSTM, Dense\nfrom keras.optimizers import Adam\n\nEMBEDDING_DIM = 50\nN_CLASSES = y_train_onehot.shape[1]\npooling_size = 5\nhidden_size = 64\n\n# TODO:\nmodel = Sequential()\nmodel.add(Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\nmodel.add(MaxPooling1D(pooling_size))\nmodel.add(LSTM(units = hidden_size))\nmodel.add(Dense(units = N_CLASSES, activation=\"softmax\"))\nmodel.compile(optimizer=Adam(),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])","4d35de87":"model.summary() if model is not None else None","8aa25179":"if model is not None and x_train is not None:\n    model.fit(x_train, y_train_onehot, validation_split=0.1, \n              epochs=25, batch_size=128)","30a271ee":"if model is not None and x_test is not None:\n    print(\"test accuracy:\", np.mean(model.predict(x_test).argmax(axis=-1) == y_test))","d1bc3e52":"### III - d) Recurrent classification model","39654391":"## Conclusion du TP4\nWe can see that our simple classification model perform very well whereas our simple classification model with pre-trained embeddings learn well but not as well as the first one.\nRecurrent classification model give good result with a final accuracy of 64% on validation. However we see that we overfit on the training set because we reach 99.9%.","374e832c":"Let's truncate and pad all the sequences to $1000$ symbols to build the training set.\n\nUse a padding function: https:\/\/keras.io\/preprocessing\/sequence\/#pad_sequences\n   - Function actually also truncates sequences longer than max length\n   - Default mode is to remove first elems of sequences longer than max length or pad with $0$s the beginning of sequences shorter than max length","7fd4b440":"Use either Sequential or Functional Keras API:\n\n- GlobalAveragePooling1D() Layer: average the vector representation of all words in each sequence\n- Dense Layer(): end with a dense layer with softmax to output 20 classes","d51e573b":"The following computes a very simple model:\n\n<img src=\"..\/images\/supervised_text_classification.png\" style=\"width: 600px;\" \/>\n\nUse either Sequential or Functional Keras API:\n\n- Embedding() Layer: build an embedding layer mapping each word to a vector representation\n- GlobalAveragePooling1D() Layer: average the vector representation of all words in each sequence\n- Dense Layer(): end with a dense layer with softmax to output 20 classes","a5a773e5":"###\u00a0II - a) Character-based language modelling","8da14a17":"### III - Supervised text classification","42824866":"###\u00a0II - Language modelling","91199061":"#### Sampling random text from the model\n\nFirst part of language modelling will be about predicting the next character of a finite sequence of characters of size $k$.\n\nRecursively generate one character at a time:\n\nYour model outputs the probability distribution $p_{\\theta}(c_{n} | c_{n-1}, \\ldots, c_{n-k})$\n\nUsing this probability distribution, a predicted character $c_{n}$ will be sampled. The temperature parameter makes it possible to remove additional entropy (bias) into the parameterized multinoulli distribution of the output of the model.\n\nThen use your prediction $c_{n}$ to compute $c_{n+1}$. Your model outputs:\n$p_{\\theta}(c_{n+1} | $<span style=\"color:red\">$c_{n}$<\/span>$, \\ldots, c_{n-k+1})$","5e8aad89":"###\u00a0II - b) Word-based language modelling","59cbfa5c":"#### Measuring per-character perplexity\n\nTo measure the quality of a language model we usually use the perplexity.\n(https:\/\/en.wikipedia.org\/wiki\/Perplexity)\n\nHere is how it is defined:\n\n$$perplexity_\\theta = 2^{-\\frac{1}{n} \\sum_{i=1}^{n} log_2 (p_\\theta(c_i)^T\\cdot y_i)}$$\n$p_\\theta(c_i)$ is your predicted column vector of probabilities over the possible next characters for the $i^{th}$ sequence.\n$y_i$ is the one-hot encoding vector of the answer: the next character of the $i^{th}$ sequence.\n\nYou just compute the average negative loglikelihood like you have done previously, only using a log2 logarithm. Then just perform a base $2$ exponentiation of the quantity just computed.","bf8d081b":"Now we are going to enrich the previous model with recurrent LSTM layers:\n\n<img src=\"..\/images\/rnn.png\" style=\"width: 600px;\" \/>\n\nUse either Sequential or Functional Keras API:\n\n- Embedding() Layer: build an embedding layer mapping each word to a vector representation\n- MaxPooling1D() Layer: add a MaxPooling1D layer in your sequence to reduce the dimension\n- LSTM() Layer: add a LSTM layer to extract information from the reduced sequence\n- Dense Layer(): end with a dense layer with softmax to output 20 classes\n\nN.B. you can either use the pretrained embeddings or recompute them with a Embedding layer here","2d5beebb":"### III - a) Load, handle, and preprocess the data","5d678948":"- Each text has been converted to a list of token_ids\n- Each token_id represents $1$ of MAX_NB_WORDS most frequent words in train dataset","0849f687":"The tokenizer object stores a mapping (vocabulary) from word strings to token ids that can be inverted to reconstruct the original message:","0630b4b2":"###\u00a0I - Loading and visualizing pre-trained embeddings","15131c47":"#### Preprocessing text for the (supervised) CBOW model","409f9081":"### III - c) Simple classification model with pre-trained embeddings","439cd2a1":"To measure smilariy between vectors, we often use the cosine similarity.\n(https:\/\/en.wikipedia.org\/wiki\/Cosine_similarity)\n$ cosine\\_similarity(w_1, w_2) = \\Large \\frac{\\langle w_1, w_2 \\rangle}{||w_1|| \\cdot ||w_2||} = \\frac{w_1^T \\cdot w_2}{||w_1|| \\cdot ||w_2||}$\n\n$w_1$ and $w_2$ are $2$ word vector embeddings.\n\nIt is a measure of how aligned $w_1$ and $w_2$ are.","bffee00e":"We will implement a simple classification model in Keras. \nAlso we will have to perform preprocessing on raw text.\n\nThe following cells use Keras to preprocess text.\n\n- Use a tokenizer: https:\/\/keras.io\/preprocessing\/text\/#tokenizer\n   - This converts the texts into sequences of integers representing the MAX_NB_WORDS most frequent words\n   \n- The following methods from the Tokenizer object should be useful:\n   - tokenizer.fit_on_texts(corpus)\n   - tokenizer.texts_to_sequences(corpus)","2b255880":"#### 20 Newsgroups Dataset\n\nThe 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups\n\nIn this third part, we will:\n- Train a simple network to learn embeddings on a classification task\n- Use pre-trained embeddings like GloVe or FastText and see the difference\n- Train a recurrent neural network to handle the text structure\n\nHowever keep in mind:\n- We are here to learn deep learning methods for NLP tasks, but simple sparse TF-IDF bigrams features without any embedding or Logistic Regression are often competitive in small to medium datasets for text classification.","f9a3390d":"### III - b) Simple classification model"}}