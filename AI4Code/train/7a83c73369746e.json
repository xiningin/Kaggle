{"cell_type":{"818c9ab4":"code","f2e3f177":"code","2193004b":"code","a00aab78":"code","d2815ccd":"code","33b1a193":"code","147dae5b":"code","f72602b8":"code","1418460a":"code","803ef34a":"code","5a48b397":"code","d10c4de5":"code","2b722914":"code","c4c41cbb":"code","b1deda00":"code","7f11f571":"code","77727d49":"code","45140512":"code","f18e8895":"code","a1e3ea42":"code","4c022d5c":"code","94cde71c":"code","58883dfa":"code","078e8daa":"code","e02e768a":"code","3960e7ae":"code","dbf4ff40":"code","42b1d6ec":"code","5d20ba61":"code","c0435797":"code","144810b6":"code","469c0932":"code","07d23095":"code","5cfd6dd6":"code","477e68bc":"code","9073a7d0":"code","51145e94":"code","1477c06b":"code","10f46952":"code","4cb78e2e":"code","8ad00c7b":"code","f86cea6f":"code","8d5ea62b":"code","394b098b":"code","64ecce4d":"code","90f79276":"code","f186c5db":"code","6ccfcd57":"code","40c0b33b":"code","10489e31":"code","6da10721":"code","70195f8b":"code","7d473cb7":"code","efb8b9a4":"code","38dad92c":"code","6d781250":"code","465e243b":"code","f23323c7":"code","da1a091d":"markdown","e519e8a0":"markdown","97220475":"markdown","da861e7f":"markdown","0ee03190":"markdown","c63e7c6c":"markdown","204dbcc2":"markdown","f3455bfb":"markdown","131cf328":"markdown","5c1660fe":"markdown","31e61ab4":"markdown","ecefe14f":"markdown","0c6b8511":"markdown","732fb2f7":"markdown","13b32e15":"markdown","e17b59c4":"markdown"},"source":{"818c9ab4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f2e3f177":"train_data = pd.read_csv('\/kaggle\/input\/massp-health-insurance-prediction\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/massp-health-insurance-prediction\/test.csv')","2193004b":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","a00aab78":"train_data = reduce_mem_usage(train_data, verbose = True)","d2815ccd":"from sklearn.feature_selection import mutual_info_regression\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","33b1a193":"def plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n","147dae5b":"from pandas_profiling import ProfileReport\nprofile_train = ProfileReport(train_data,explorative=True)\nprofile_train","f72602b8":"import seaborn as sns\ntarget = train_data.Response\nsns.displot(data = target)","1418460a":"from sklearn.metrics import roc_auc_score\n#roc_auc_score(target, predictions)","803ef34a":"numerical = train_data.select_dtypes(include='number')\nnumerical = numerical.drop(columns=['id'])\nnumerical_target = numerical['Response']\nnumerical_cols = numerical.drop(columns=['Response'])","5a48b397":"numerical['Vehicle_Damage'] = train_data['Vehicle_Damage'].map({'Yes': 1, 'No': 0})","d10c4de5":"numerical","2b722914":"print(numerical.describe())","c4c41cbb":"\ndef plot_numeric_var(col):\n    figure(num=None, figsize=(10, 4), dpi=80)\n    ax1 = sns.kdeplot(numerical.loc[numerical['Response'] == 1, col] , label = 'Customer is interested', linewidth=5.0)\n    sns.kdeplot(numerical.loc[numerical['Response'] == 0, col] , label = 'Customer is not interested', linewidth=5.0)\n\n    ax1.tick_params(axis='both', which='major', labelsize=15)\n    ax1.set_ylabel('Density', size=20)\n    ax1.set_xlabel(col, size=18)\n\n    leg = plt.legend()\n    leg_texts = leg.get_texts()\n    plt.setp(leg_texts, fontsize='x-large')\n    plt.grid()\n    plt.show()","b1deda00":"plot_numeric_var('Age')","7f11f571":"plot_numeric_var('Driving_License')","77727d49":"plot_numeric_var('Region_Code')","45140512":"plot_numeric_var('Previously_Insured')","f18e8895":"plot_numeric_var('Policy_Sales_Channel')","a1e3ea42":"plot_numeric_var('Vintage')","4c022d5c":"plot_numeric_var('Annual_Premium')","94cde71c":"plot_numeric_var('Vehicle_Damage')","58883dfa":"sns.distplot(a = numerical['Previously_Insured'])","078e8daa":"sns.distplot(a = numerical['Policy_Sales_Channel'])","e02e768a":"scale = StandardScaler()\nnumerical_cols = numerical_cols.drop(columns=['Age','Region_Code'])\nnumerical_scaled= scale.fit_transform(numerical[numerical_cols.columns])\n# Create principal components\npca = PCA()\nnumerical_pca = pca.fit_transform(numerical_scaled)\n\n# Convert to dataframe\ncomponent_names = [f\"PC{i+1}\" for i in range(numerical_pca.shape[1])]\nnumerical_pca = pd.DataFrame(numerical_pca, columns=numerical_cols.columns)\n\nnumerical_pca.head()","3960e7ae":"loadings = pd.DataFrame(\n    pca.components_.T,  # transpose the matrix of loadings\n    columns=component_names,  # so the columns are the principal components\n    index=numerical_pca.columns,  # and the rows are the original features\n)\nloadings","dbf4ff40":"mi_scores = make_mi_scores(numerical_pca, numerical_target, discrete_features=False)\nmi_scores","42b1d6ec":"categorical = train_data.select_dtypes(exclude='number')\ncategorical","5d20ba61":"features = train_data\nfeatures = features.drop(columns=['id','Driving_License','Response','Policy_Sales_Channel','Region_Code'])\nfeatures","c0435797":"train_data['Vehicle_Damage'] = train_data['Vehicle_Damage'].map({'Yes': 1, 'No': 0})\ntest_data['Vehicle_Damage'] = test_data['Vehicle_Damage'].map({'Yes': 1, 'No': 0})","144810b6":"train_data['Gender'] = train_data['Gender'].map({'Male': 1, 'Female': 0})\ntest_data['Gender'] = test_data['Gender'].map({'Male': 1, 'Female': 0})","469c0932":"categorical_features = features.select_dtypes(exclude='number')\ncategorical_features = categorical_features.columns\nnumerical_features = features.select_dtypes(include='number')\nnumerical_features = numerical_features.columns","07d23095":"X = train_data[features.columns]","5cfd6dd6":"from xgboost import XGBRegressor\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n#my_model = XGBClassifier(n_estimators = 2000,\n                           # max_depth = 6,\n                            #learning_rate = 0.05,\n                            #subsample = 0.8,\n                           # colsample_bytree = 0.4,\n                           # missing = -1,\n                           #tree_method = 'exact'\n                           #)\nmy_model = XGBClassifier(n_estimators=1000, learning_rate=0.05)\n#my_model = RandomForestClassifier(max_depth=2, random_state=0)","477e68bc":"\ncategorical_cols = categorical_features\nnumerical_cols = numerical_features\n# Preprocessing for numerical data\n#numerical_transformer = Pipeline(steps = [('')])\n# Preprocessing for categorical data","9073a7d0":"col = 'Vehicle_Age'\nX[col+ \"_encoding\"] = LabelEncoder().fit_transform(X[col])\ntest_data[col+ \"_encoding\"] = LabelEncoder().fit_transform(test_data[col])","51145e94":"X = X.drop(columns = ['Vehicle_Age'])\ntest_data = test_data.drop(columns = ['Vehicle_Age'])","1477c06b":"\nover = SMOTE()\nsteps = [('o', over)]\npipeline = Pipeline(steps=steps)","10f46952":"num_cols= X.columns\n\nscale = MinMaxScaler()\n#X = preprocessor.fit_transform(X)\nX_scaled, target = pipeline.fit_resample(X, target)\nX_scaled[num_cols] = scale.fit_transform(X_scaled[num_cols])\n","4cb78e2e":"X","8ad00c7b":"X_scaled","f86cea6f":"X_scaled.shape","8d5ea62b":"#s\u1eed d\u1ee5ng PCA ph\u00e2n t\u00edch\n# Create principal components\npca = PCA()\nX_pca = pca.fit_transform(X_scaled)\n\n# Convert to dataframe\ncomponent_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\nX_pca = pd.DataFrame(X_pca, columns=component_names)\n\nX_pca.head()","394b098b":"loadings = pd.DataFrame(\n    pca.components_.T,  # transpose the matrix of loadings\n    columns=component_names,  # so the columns are the principal components\n    index=X.columns,  # and the rows are the original features\n)\nloadings","64ecce4d":"# Look at explained variance\nplot_variance(pca);","90f79276":"mi_scores = make_mi_scores(X_pca, target, discrete_features=False)\nmi_scores","f186c5db":"sns.distplot(a = target)","6ccfcd57":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X_scaled, target)","40c0b33b":"from sklearn.model_selection import cross_val_score\n\n# Multiply by -1 since sklearn calculates *negative* MAE\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\nscores = cross_val_score(my_model, X_scaled, target,\n                              cv=cv,\n                              scoring='roc_auc')\n\nprint(\"AUC Score:\\n\", np.mean(scores))","10489e31":"my_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)","6da10721":"#pipeline.fit(OH_X_train, y_train, \n     \n             #model__eval_set=[(OH_X_valid, y_valid)], \n            #model__verbose=False)\n","70195f8b":"test_data","7d473cb7":"features = X\nfeatures","efb8b9a4":"test_data[features.columns]","38dad92c":"test_data[num_cols] = scale.fit_transform(test_data[num_cols])\npredicted_X_test = my_model.predict_proba(test_data[features.columns])","6d781250":"test_data","465e243b":"test  = pd.read_csv('\/kaggle\/input\/massp-health-insurance-prediction\/test.csv')\nmy_submission = pd.DataFrame({'id': test.id, 'Response': predicted_X_test[:,1]})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('.\/submission.csv', index=False)","f23323c7":"my_submission.head(10)","da1a091d":"# T\u1ed5ng qu\u00e1t:\n- \n# 1. Gi\u1edbi thi\u1ec7u Data:\n  - 1.1 Th\u00f4ng tin t\u1ed5ng qu\u00e1t d\u1eef li\u1ec7u\n  - 1.2 Th\u00f4ng tin t\u1ed5ng qu\u00e1t v\u1ec1 Target\n  - 1.3 Th\u00f4ng tin v\u1ec1 Metrics\n  \n# 2. EDA:\n  - 2.1 D\u1eef li\u1ec7u Numerical\n  - 2.2 D\u1eef l\u1ec7u Categorical\n  - 2.3 T\u1ed5ng k\u1ebft\n  \n# 3. Feature Engineering:\n  - 3.1 X\u1eed l\u00fd d\u1eef li\u1ec7u\n  \n# 4. X\u00e2y d\u1ef1ng Model:\n  - 4.1 Ch\u1ecdn Model\n  - 4.2 Evaluation\n  - 4.3 Optimization","e519e8a0":"- B\u1ecf tr\u01b0\u1eddng, v\u00ec kh\u00f4ng c\u00f3 d\u1eef li\u1ec7u v\u1ec1 ng\u01b0\u1eddi kh\u00f4ng c\u00f3 License s\u1ebd \u1ea3nh h\u01b0\u1edfng ntn t\u1edbi Target","97220475":"- **2.1 D\u1eef li\u1ec7u Numerical**","da861e7f":"# 1. Gi\u1edbi thi\u1ec7u Data:\n - **1.1 T\u1ed5ng qu\u00e1t d\u1eef li\u1ec7u**","0ee03190":"# 4. X\u00e2y d\u1ef1ng Model","c63e7c6c":"**- 2.2 D\u1eef li\u1ec7u Categorical**","204dbcc2":"- Ph\u00e2n ph\u1ed1i t\u1eadp trung v\u00e0o Age 20-30. Right skewed\n- Customer is not interested nhi\u1ec1u nh\u1ea5t v\u00e0o \u0111\u1ed9 tu\u1ed5i ph\u00e2n ph\u1ed1i 20-30\n- L\u01b0\u1ee3ng tu\u1ed5i interested cao nh\u1ea5t l\u00e0 v\u00e0o 40-50","f3455bfb":"- **1.3 Th\u00f4ng tin Metric**:\n    - V\u1edbi b\u00e0i to\u00e1n classification , target ch\u00eanh l\u1ec7ch gi\u1eefa 2 l\u1edbp, th\u01b0\u1eddng s\u1eed d\u1ee5ng AUC (Area Under The Curve) score \u0111o \u0111\u1ea1c \u0111\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a m\u00f4 h\u00ecnh. AUC (Area Under The Curve) l\u00e0 di\u1ec7n t\u00edch d\u01b0\u1edbi \u0111\u01b0\u1eddng cong ROC.","131cf328":"import h2o\nfrom h2o.automl import H2OAutoML\nh2o.init()\ny = \"Response\"\ndf = h2o.import_file('\/kaggle\/input\/massp-health-insurance-prediction\/train.csv')\nX = df.columns\nX.remove(y)\nX.remove('id')\nX.remove('Driving_License')\nX.remove('Vintage')\nX.remove('Vehicle_Age')\nX.remove('Annual_Premium')\n","5c1660fe":"- D\u1eef li\u1ec7u imbalanced\n- Kh\u00e1ch m\u00e0 ch\u01b0a bao h insured th\u00ec kh\u1ea3 n\u0103ng cao s\u1ebd h\u1ee9ng th\u00fa","31e61ab4":"- **1.2 Th\u00f4ng tin Metric**","ecefe14f":"# 2. Explanatory Data Analysis","0c6b8511":"- D\u1eef li\u1ec7u c\u00f3 imbalanced l\u1edbn\n- S\u1eed d\u1ee5ng SMOTE \u0111\u1ec3 x\u1eed l\u00fd imbalanced","732fb2f7":"- Size 47Mb, kha kh\u00e1\n- 6 bi\u1ebfn Numerical\n- 4 bi\u1ebfn Boolean, \u0111\u1ed5i th\u00e0nh Numerical sau n\u00e0y\n- 2 bi\u1ebfn Categorical\n- D\u1eef li\u1ec7u kh\u00e1 s\u1ea1ch, kh\u00f4ng c\u1ea7n x\u1eed l\u00fd missing value\n- \u00cdt features, c\u1ea7n ph\u00e2n t\u00edch v\u00e0 t\u1ed1i \u01b0u s\u00e2u","13b32e15":"- Ph\u00e2n ph\u1ed1i chu\u1ea9n v\u00e0 \u0111\u1ec1u\n- Kh\u00f4ng c\u00f3 s\u1ef1 kh\u00e1c bi\u1ec7t qu\u00e1","e17b59c4":"fig, axes = plt.subplots(2,2 , figsize=(18, 10))\n  \nfig.suptitle('Some Numerical Data vs Price')\n  \nsns.kdeplot(ax=axes[0, 1], data= numerical, x='Policy_Sales_Channel', y='Response')\nsns.kdeplot(ax=axes[1, 0], data= numerical, x='Vehicle_Damage', y='Response')"}}