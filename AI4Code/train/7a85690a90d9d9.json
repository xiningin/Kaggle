{"cell_type":{"e02b3fdc":"code","706c89b7":"code","9c57705c":"code","8d13992c":"code","6fb44561":"code","7a6c257d":"code","e3632769":"code","21c0ef84":"code","5b7adaf7":"code","7e49cecb":"code","10b58d7a":"code","a2c648ab":"code","8ff63728":"code","0ab7d62d":"code","2e7f5758":"code","881e97c0":"code","9584300d":"code","69a7bbc8":"code","5f836c7a":"code","05814850":"code","4d57d50e":"code","97f8de49":"code","f110de49":"code","6da5bb6e":"code","6b891217":"markdown","a520941b":"markdown","14e9da00":"markdown","9208fa74":"markdown","57882881":"markdown","7dbd8ea1":"markdown","b3291d77":"markdown","81c2b637":"markdown","e85951c1":"markdown","a37f11ea":"markdown","8034503d":"markdown","d7cd5f1a":"markdown","03a7cd54":"markdown","9fb0efe7":"markdown","1c646aa4":"markdown","5aba237d":"markdown","bab1f8f1":"markdown","e86092c0":"markdown"},"source":{"e02b3fdc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#libraries for visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncolor = sns.color_palette()\n\n%matplotlib inline\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\n\n\nsns.set_style(\"white\")\nimport scipy.stats as stats\n\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n\n\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n\n\n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\n","706c89b7":"trainset=pd.read_csv('..\/input\/train.csv')\ntrainset.shape\n","9c57705c":"trainset.info()","8d13992c":"trainset.head()","6fb44561":"testset=pd.read_csv('..\/input\/test.csv')\ntestset.shape\n\n","7a6c257d":"testset.head()","e3632769":"trainset.describe()\n","21c0ef84":"testset.describe()","5b7adaf7":"trainset_cat = trainset.select_dtypes(include=['object']).copy()\n\ntrainset_cat.head()\n\nlista=list(trainset_cat)\nprint(lista)\n\n\ntestset_cat = testset.select_dtypes(include=['object']).copy()\n\ntestset_cat.head()\n\nlista_test=list(testset_cat)\nprint(lista_test)\n\n#trainset_onehot =trainset.copy()\n#trainset_onehot_f = pd.get_dummies(trainset_onehot, columns=['Neighborhood'], prefix = ['Neighborhood'])\n\n#print(trainset_onehot_f.head())","7e49cecb":"for name in lista:\n trainset[name]=trainset_cat[name].astype('category').cat.codes\n\n\nfor nametest in lista_test:\n testset[nametest]=testset_cat[nametest].astype('category').cat.codes","10b58d7a":"trainset.head()","a2c648ab":"print(trainset['SalePrice'].describe())\nplt.figure(figsize=(9, 8))\nsns.distplot(trainset['SalePrice'], color='c', bins=100, hist_kws={'alpha': 0.4});","8ff63728":"trainset_c=trainset[trainset.SalePrice<500000]\n","0ab7d62d":"print(trainset_c['SalePrice'].describe())\nplt.figure(figsize=(9, 8))\nsns.distplot(trainset_c['SalePrice'], color='c', bins=100, hist_kws={'alpha': 0.4});","2e7f5758":"labels = []\nvalues = []\nfor col in trainset_c.columns:\n    if col not in [\"Id\", \"SalePrice\"] and trainset_c[col].dtype!='object':\n        labels.append(col)\n        values.append(np.corrcoef(trainset_c[col].values, trainset_c[\"SalePrice\"].values)[0,1])\ncorr_df = pd.DataFrame({'columns_labels':labels, 'corr_values':values})\ncorr_df = corr_df.sort_values(by='corr_values')\n \ncorr_df = corr_df[(corr_df['corr_values']>0.25) | (corr_df['corr_values']<-0.25)]\nind = np.arange(corr_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(10,6))\nrects = ax.barh(ind, np.array(corr_df.corr_values.values), color='gold')\nax.set_yticks(ind)\nax.set_yticklabels(corr_df.columns_labels.values, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation coefficient of the variables\")\nplt.show()","881e97c0":"missingdata = trainset_c.isnull().sum(axis=0).reset_index()\nmissingdata.columns = ['column_name', 'missing_count']\nmissingdata = missingdata.ix[missingdata['missing_count']>0]\nmissingdata = missingdata.sort_values(by='missing_count')\n\nind = np.arange(missingdata.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,18))\nrects = ax.barh(ind, missingdata.missing_count.values, color='blue')\nax.set_yticks(ind)\nax.set_yticklabels(missingdata.column_name.values, rotation='horizontal')\nax.set_xlabel(\"Count of missing values\")\nax.set_title(\"Number of missing values in each column\")\nplt.show()","9584300d":"temp_df = trainset_c[corr_df.columns_labels.tolist()]\ncorrmat = temp_df.corr(method='pearson')\nf, ax = plt.subplots(figsize=(12, 12))\n\n# Draw the heatmap using seaborn\nsns.heatmap(corrmat, vmax=1., square=True, cmap=\"YlOrRd\")\nplt.title(\"Correlation Matrix\", fontsize=15)\nplt.show()","69a7bbc8":"trainset_c['OverallQual'].loc[trainset_c['OverallQual']>7] = 7\nplt.figure(figsize=(12,8))\nsns.violinplot(x='OverallQual', y='SalePrice', data=trainset_c)\nplt.xlabel('Overall Quality', fontsize=12)\nplt.ylabel('SalePrice', fontsize=12)\nplt.show()","5f836c7a":"col = \"GrLivArea\"\nulimit = np.percentile(trainset_c[col].values, 99.5)\nllimit = np.percentile(trainset_c[col].values, 0.5)\ntrainset_c[col].loc[trainset_c[col]>ulimit] = ulimit\ntrainset_c[col].loc[trainset_c[col]<llimit] = llimit\n\nplt.figure(figsize=(12,12))\nsns.jointplot(x=trainset_c[col].values, y=trainset_c.SalePrice.values, height=10, color=color[4])\nplt.ylabel('SalePrice', fontsize=12)\nplt.xlabel('GrLivArea', fontsize=12)\nplt.title(\"GrLivArea Vs SalePrice\", fontsize=15)\nplt.show()","05814850":"trainset_c.fillna( method ='ffill', inplace = True)\n","4d57d50e":"\ntestset.fillna( method ='ffill', inplace = True)\n\n","97f8de49":"trainset_y = trainset_c.SalePrice\nx_col = ['OverallQual','GrLivArea','GarageCars','TotalBsmtSF','FullBath','YearBuilt','YearRemodAdd','TotRmsAbvGrd','Fireplaces','Foundation','BsmtFinSF1','OpenPorchSF','WoodDeckSF','GarageCond','2ndFlrSF','HalfBath','LotArea','LotShape','GarageFinish','HeatingQC','BsmtQual','KitchenQual','ExterQual']\n\ntrainset_x = trainset_c[x_col]\n\n\n\nmy_model = GradientBoostingRegressor()\nmy_model.fit(trainset_x, trainset_y)\n","f110de49":"testset_X = testset[x_col]\ntestset_X.head()\n# Use the model to make predictions\npredicted_prices = my_model.predict(testset_X)\n# We will look at the predicted prices to ensure we have something sensible.\nprint(predicted_prices)","6da5bb6e":"my_submission = pd.DataFrame({'Id': testset.Id, 'SalePrice': predicted_prices})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","6b891217":"Some variables are high correlated among themselves. GarageCars and GarageArea, 1stFlrSF and TotalBsmfSF, Fireplaces and Fireplacequ have high correlation. We should think about removing one of each of these groups before applying Machine Learning Techniques. ","a520941b":"Target is  right skewed and some outliers are above 500000. We delete these outliers to get a normal distribution for my target variable. ","14e9da00":"**Machine Learning Tecniques**\n\n Gradient Boosting Regressor using as predictors the variables with the highest correlations with the Target.  We don't consider, for example, GarageArea because (as previously demonstrated) the correlation with GarageCars is suggesting that they contain the same information. \nGradient Boosting Regressor is a tree-based model and it doesn't benefit from scaling each feature or from using the one-hot encoding. These techniques should be used in non tree based models. \n        \n","9208fa74":"Transformation of all the categorical variables in numerical variables with one easy loop.","57882881":"Let's analyse the relationship between some variables having high correlation with our target variable. ","7dbd8ea1":"Creating graph showing the highly correlated variable with the target\n","b3291d77":"Train the model ","81c2b637":"The datasets have some categorical variables, we are transorming them in numerical in order to use ML on them. Ideally, if we use a non based tree model, we should transform them using one-hot encoding in order to have a binary output for each category. To use one-hot encoding we should first merge trainset and testset, apply one-hot encoding and then split them again. We are leaving this method to future updates of this kernel. ","e85951c1":"******House prices: Advanced Regression Techniques** \n\nAuthor Notebook: PG\n\nDescription: \n79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, in this competition challenges we need to predict the final price of each home.\nThis is an example of supervised dataset with numerical target.\nThe target variable in the testset is the variable named \"SalePrice\"","a37f11ea":"**Missing Values Analysis**","8034503d":"**EXPLORATORY DATA ANALYSIS:**\n\n1) Defining Training set and Test set and exploring shape and initial data. Descriptive Statistics on all the variables. \n\n2) Creating hystograms, other graphs and correlation matrix to understand how the variables are correlated. \n\n3) Analysing missing data. \n\n4) Applying machine learning techniques.\n","d7cd5f1a":"Histogram on the target variable \"SalePrice\"","03a7cd54":"Test the model and create prediction, a good ideas is to use crossvalidation to improve our prediction. We will leave this method to future updates of this kernel.","9fb0efe7":"We can see that the variables with the highest number of missing values don't have an high correlation with the target. We can avoid to consider them. ","1c646aa4":"Replacing missing value using Method Parameter in order to apply Machine Learning Techniques","5aba237d":"A clear linear correlation is visible. ","bab1f8f1":"Histogram of the cleaned trainset (trainset_c), we can see that the records are deleted","e86092c0":"**Overall quality vs Target variable**"}}