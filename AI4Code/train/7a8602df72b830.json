{"cell_type":{"adae054a":"code","1f461b94":"code","548a6ac0":"code","58ffb173":"code","b9400728":"code","b13ab4fc":"markdown","5dd4236b":"markdown"},"source":{"adae054a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1f461b94":"DIR = \"\/kaggle\/input\/writing-prompts\/writingPrompts\/\"\ndata = [DIR+\"train\", DIR+\"test\", DIR+\"valid\"]\n\nTARGET_DIR = '\/kaggle\/working\/'\ntarget_data = [TARGET_DIR+\"train\", TARGET_DIR+\"test\", TARGET_DIR+\"valid\"]","548a6ac0":"from tqdm import tqdm_notebook as tqdm\n\nNUM_WORDS = 300 # originally, FAIR use 1000, but here I use 300 just to be able to train distilgpt2 quickly\n\nfor name_id in tqdm(range(len(data))):\n    fp = open(data[name_id] + \".wp_source\") \n    ft = open(data[name_id] + \".wp_target\") \n    \n    stories = ft.readlines()\n    prompts = fp.readlines()\n    \n    assert len(prompts) == len(stories)\n    \n    new_stories = [prompts[i].rstrip()+ \" <endprompts> \" + \" \".join(stories[i].split()[0:NUM_WORDS]) for i in range(len(stories))]\n    \n    \n    with open(target_data[name_id] + \".wp_combined\", \"w\") as o:\n        for line in new_stories:\n            o.write(line.strip() + \"\\n\")\n        print('finish writing',target_data[name_id] + \".wp_combined\")\n    \n    fp.close()\n    ft.close()","58ffb173":"!ls -sh","b9400728":"!head train.wp_combined\n!head test.wp_combined","b13ab4fc":"We can now directly use the generated files for GPT2 finetuning","5dd4236b":"# Introduction\nHi! \n\nWritingPrompts dataset consists of two separated files , i.e. \"prompts\" and \"stories\". \n\nTo be able to finetune using HuggingFace GPT-2, we will combine this two files into 1, and still use 1 line-by-line basis.\nAlso, as recommended by FAIR, we will truncated the stories to only first 1,000 words"}}