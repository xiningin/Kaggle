{"cell_type":{"a91c2a96":"code","e8cb5e10":"code","466dccfa":"code","bae37dab":"code","b0add329":"code","878540f2":"code","4d0dab66":"code","8b3f878d":"code","15dc3e01":"code","73e0152e":"code","e49919a0":"code","d012e2fd":"code","0f1504ea":"code","de23d72f":"code","8f282954":"code","fde96e67":"code","b34a1325":"code","514c05cd":"code","eb1a740e":"code","41f958fd":"code","bc2267b1":"code","2640fe1e":"code","c6b1e9cc":"code","88d31087":"code","73ecf671":"code","aa180902":"code","41b48a67":"code","e99a1d23":"code","1ca397c5":"code","4f0e3dc0":"code","14946030":"code","c5d69faa":"code","1774e6df":"code","7de5470a":"code","b39e27c6":"code","09531e0c":"code","ddd9db5c":"code","8d2bba3b":"code","93275a54":"code","768f52e9":"code","3e93a9be":"code","14818a7e":"code","4e5795bd":"code","239fca63":"code","a802ca60":"code","4a780e4a":"code","c6c51dd3":"code","a1d49917":"code","54b100a2":"code","f97ebc6c":"code","c93de9d0":"markdown","74f869d6":"markdown","834b403c":"markdown","fb6e6720":"markdown","ccddb3d8":"markdown","984f9886":"markdown","288eee15":"markdown","5ea35201":"markdown","68bdeecf":"markdown","c96f7d8f":"markdown","21e1f85b":"markdown","b6a9d4c8":"markdown","e0d2b389":"markdown","ddffdb36":"markdown","0bfeec17":"markdown","bdf6ff04":"markdown","22f144ac":"markdown","888de850":"markdown","3c230923":"markdown","8b3ead67":"markdown","7de00809":"markdown","877a95af":"markdown","b07c439f":"markdown","5d9f3282":"markdown","949f8e04":"markdown","a8b7cf15":"markdown","9ef26ae9":"markdown","0f33f7d9":"markdown","6c24956c":"markdown","0621afb1":"markdown","164f7a0c":"markdown","51f979ef":"markdown","6e85643e":"markdown","a81e1abe":"markdown","d0193c9e":"markdown"},"source":{"a91c2a96":"# Importing the libraries \nimport pandas as pd\nimport numpy as np\nfrom sklearn import metrics","e8cb5e10":"#importing boston dataset\nfrom sklearn.datasets import load_boston\nboston = load_boston()","466dccfa":"#initialising the data Frame\ndf=pd.DataFrame(boston.data)","bae37dab":"#seeing the dataset roughly\ndf.head(8)","b0add329":"#adding the names of features with respective data\ndf.columns=boston.feature_names\n#Adding target variable to dataframe\n","878540f2":"#cheking columns before adding features for target value i.e price\ndf.shape","4d0dab66":"df['PRICE'] = boston.target \n# Median value of owner-occupied homes in $1000s\ndf.head()","8b3f878d":"#checking columns after adding target values\ndf.shape","15dc3e01":"# all datas are properly associated with their types\ndf.dtypes","73e0152e":"#Analysing the data.\n#Statistics of dataset described.\ndf.describe()","e49919a0":"#So no data is missing since all sums are 0.\ndf.isnull().sum()","d012e2fd":"#checking outliers using boxplot\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n%matplotlib inline\n\nfig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\ncount = 0\naxs = axs.flatten()\nfor tar,var in df.items():\n    sns.boxplot(y=tar, data=df, ax=axs[count])\n    count= count+1\nplt.tight_layout(pad=0.5, w_pad=0.78, h_pad=4.0)\n","0f1504ea":"#outliers in percentage\nfor tar,var in df.items():\n    q1=var.quantile(0.25)\n    q3=var.quantile(0.75)\n    iqr=q3-q1\n    var_col=var[(var<=q1-1.5*iqr) | (var>=q3+1.5*iqr)]\n    perc=np.shape(var_col)[0]*100.0\/np.shape(df)[0]\n    print(\"Column %s outliers = %.2f%%\" % (tar, perc))             \n                  ","de23d72f":"#checking the correlation between two features.\ncorr=df.corr()\ncorr","8f282954":"#using a heatmap to see correlation between features more clearly.\nplt.figure(figsize=(20,20))\nsns.heatmap(corr.abs(), annot=True,cmap='Greens')","fde96e67":"#Checking the skewness in data\nfig,axs = plt.subplots(ncols=7, nrows=2, figsize=(24,12))\ncount = 0\naxs = axs.flatten()\nfor tar,var in df.items():\n    sns.distplot(var,ax=axs[count])\n    count = count+1\nplt.tight_layout(pad=0.5, w_pad=0.6, h_pad=5.0)","b34a1325":"# Spliting target variable and independent variables\nX = df.drop(['PRICE'], axis = 1)\ny = df['PRICE']","514c05cd":"X","eb1a740e":"#splitting the data to train and test. checking the validation of the model.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.4, random_state = 10)","41f958fd":"# Import library for Linear Regression\nfrom sklearn.linear_model import LinearRegression\n\n# Create a Linear regressor\nlm = LinearRegression()\n\n# Train the model using the training sets \nlm.fit(X_train, y_train)","bc2267b1":"# Value of y intercept\nlm.intercept_","2640fe1e":"#Converting the coefficient values to a dataframe\ncoeffcients = pd.DataFrame([X_train.columns,lm.coef_]).T\ncoeffcients = coeffcients.rename(columns={0: 'Attribute', 1: 'Coefficients'})\ncoeffcients","c6b1e9cc":"#predicting on training data\ny_pred=lm.predict(X_train)\n#Model Evaluation and error calculations\nprint('R^2 =',metrics.r2_score(y_train, y_pred))\nprint('Adjusted R^2 =',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\nprint('MAE =',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE =',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE =',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","88d31087":"# Visualizing the differences between actual prices and predicted values\nplt.scatter(y_train, y_pred)\nplt.xlabel(\"Actual Price\")\nplt.ylabel(\"Predicted Price\")\nplt.title(\"Actual Price vs Predicted Price\")\nplt.show()","73ecf671":"#Plotting Actual observations vs predicted observations\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nf = plt.figure(figsize=(14,5))\nax = f.add_subplot(121)\nsns.scatterplot(y_train,y_pred,ax=ax,color='r')\nax.set_title('Actual Vs Predicted value')\n\n# Check for Residual normality & mean\nax = f.add_subplot(122)\na=(y_train - y_pred)\nsns.distplot(a,ax=ax,color='b')\nax.axvline(a.mean(),color='k',linestyle='--')\nax.set_title('Check for Residual normality & mean: \\n Residual eror');","aa180902":"#Check for Multicollinearity\n#Variance Inflation Factor\nR_square = lm.score(X_test,y_test)\nVIF_LR = 1\/(1- R_square)\nVIF_LR","41b48a67":"#predicting the data using above model\ny_tpred= lm.predict(X_test)\n#Model Evaluation\ntpred_linreg = metrics.r2_score(y_test, y_tpred)\nprint('R^2:', tpred_linreg)\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_tpred))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_test, y_tpred))\nprint('MSE:',metrics.mean_squared_error(y_test, y_tpred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_tpred)))","e99a1d23":"#Standardising the data \nfrom sklearn.preprocessing import StandardScaler\nss= StandardScaler()\nX_train= ss.fit_transform(X_train)\nX_test= ss.transform(X_test)\n","1ca397c5":"#importing SVM regressor\nfrom sklearn import svm\nreg= svm.SVR()\n\n#training the model\nreg.fit(X_train,y_train)","4f0e3dc0":"#Predicting the model on train data\ny_pred= reg.predict(X_train)","14946030":"# Model Evaluation and error calculations\nprint('R^2 =',metrics.r2_score(y_train, y_pred))\nprint('Adjusted R^2 =',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\nprint('MAE =',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE =',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE =',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","c5d69faa":"# Visualizing the differences between actual prices and predicted values\nplt.scatter(y_train, y_pred)\nplt.xlabel(\"Actual Price\")\nplt.ylabel(\"Predicted Price\")\nplt.title(\"Actual Price vs Predicted Price\")\nplt.show()","1774e6df":"#Plotting Actual observations vs predicted observations\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nf = plt.figure(figsize=(14,5))\nax = f.add_subplot(121)\nsns.scatterplot(y_train,y_pred,ax=ax,color='r')\nax.set_title('Actual Vs Predicted value')\n\n# Check for Residual normality & mean\nax = f.add_subplot(122)\na=(y_train - y_pred)\nsns.distplot(a,ax=ax,color='b')\nax.axvline(a.mean(),color='k',linestyle='--')\nax.set_title('Check for Residual normality & mean: \\n Residual eror')","7de5470a":"#Check for Multicollinearity using Variance Inflation Factor\nR_square = lm.score(X_test,y_test)\nVIF_SVR = 1\/(1- R_square)\nVIF_SVR","b39e27c6":"#predicting the data using our test model\ny_tpred= reg.predict(X_test)\n#Model Evaluation\ntpred_svm = metrics.r2_score(y_test, y_tpred)\nprint('R^2:', tpred_svm)\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_tpred))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_test, y_tpred))\nprint('MSE:',metrics.mean_squared_error(y_test, y_tpred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_tpred)))","09531e0c":"#importing the dataset\nfrom sklearn.ensemble import RandomForestRegressor\nrfr=RandomForestRegressor()\nrfr.fit(X_train,y_train)","ddd9db5c":"#Predicting the model\ny_pred=rfr.predict(X_train)","8d2bba3b":"# Model Evaluation\nprint('R^2:',metrics.r2_score(y_train, y_pred))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE:',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","93275a54":"# Visualizing the differences between actual prices and predicted values\nplt.scatter(y_train, y_pred)\nplt.xlabel(\"Actual Price\")\nplt.ylabel(\"Predicted Price\")\nplt.title(\"Actual Price vs Predicted Price\")\nplt.show()","768f52e9":"#Plotting Actual observations vs predicted observations\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nf = plt.figure(figsize=(14,5))\nax = f.add_subplot(121)\nsns.scatterplot(y_train,y_pred,ax=ax,color='g')\nax.set_title('Actual Vs Predicted value')\n# Check for Residual normality & mean\nax = f.add_subplot(122)\na=(y_train - y_pred)\nsns.distplot(a,ax=ax,color='b')\nax.axvline(a.mean(),color='k',linestyle='--')\nax.set_title('Check for Residual normality & mean: \\n Residual eror')\n","3e93a9be":"#Check for Multicollinearity using Variance Inflation Factor\nR_square=rfr.score(X_test,y_test)\nVIF_RFR = 1\/(1-R_square)\nVIF_RFR","14818a7e":"#predicting the data using above model\ny_tpred= rfr.predict(X_test)\n#Model Evaluation\ntpred_rfr = metrics.r2_score(y_test, y_tpred)\nprint('R^2:',tpred_rfr)\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_tpred))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_test, y_tpred))\nprint('MSE:',metrics.mean_squared_error(y_test, y_tpred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_tpred)))","4e5795bd":"#importing XGBOOST regression library\nfrom xgboost import XGBRegressor\n#\nxgbr= XGBRegressor()\n#Training the model\nxgbr.fit(X_train, y_train)","239fca63":"#predicting the model\ny_pred=xgbr.predict(X_train)","a802ca60":"# Model Evaluation and error calculations\nprint('R^2 =',metrics.r2_score(y_train, y_pred))\nprint('Adjusted R^2 =',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\nprint('MAE =',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE =',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE =',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","4a780e4a":"# Visualizing the differences between actual prices and predicted values\nplt.scatter(y_train, y_pred)\nplt.xlabel(\"Actual Price\")\nplt.ylabel(\"Predicted Price\")\nplt.title(\"Actual Price vs Predicted Price\")\nplt.show()","c6c51dd3":"#Plotting Actual observations vs predicted observations\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nf = plt.figure(figsize=(14,5))\nax = f.add_subplot(121)\nsns.scatterplot(y_train,y_pred,ax=ax,color='g')\nax.set_title('Actual Vs Predicted value')\n\n# Check for Residual normality & mean\nax = f.add_subplot(122)\na=(y_train - y_pred)\nsns.distplot(a,ax=ax,color='b')\nax.axvline(a.mean(),color='k',linestyle='--')\nax.set_title('Check for Residual normality & mean: \\n Residual eror')\n","a1d49917":"#check for Multicollinearity using Variance Inflation Factor\nR_square=xgbr.score(X_test,y_test)\nVIF_XGBR = 1\/(1-R_square)\nVIF_XGBR","54b100a2":"#predicting the data using above model\ny_tpred= xgbr.predict(X_test)\n#Model Evaluation\ntpred_xgbr = metrics.r2_score(y_test, y_tpred)\nprint('R^2:',tpred_xgbr)\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_tpred))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_test, y_tpred))\nprint('MSE:',metrics.mean_squared_error(y_test, y_tpred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_tpred)))","f97ebc6c":"models = pd.DataFrame({\n    'Model': ['Linear Regression', 'Random Forest', 'XGBoost', 'Support Vector Machines'],\n    'R-squared Score': [tpred_linreg*100, tpred_rfr*100, tpred_xgbr*100, tpred_svm*100]})\nmodels.sort_values(by='R-squared Score', ascending=False)","c93de9d0":"<h4>Here DIS,CRIM,ZN,B are highly skewed.CHAS is discrete in nature. <\/h4>","74f869d6":"<h2> Predicting ML model on test data.<\/h2>","834b403c":"Actual price v\/s predicted price is almost a straight line. It can be a good model.","fb6e6720":"Its exactly coming out to be a straight line.","ccddb3d8":"<h1>4. XGBOOOST REGRESSOR<\/h1>","984f9886":"<h2> Predicting ML model on test data.<\/h2>","288eee15":"11. PTRATIO: pupil-teacher ratio by town \n12. B: 1000(Bk\u22120.63)2 where Bk is the proportion of blacks by town\n13. LSTAT: % lower status of the population\n14. MEDV: Median value of owner-occupied homes in $1000s\nWe can see that the input attributes have a mixture of units.","5ea35201":"<h1>1. Outliers Deduction<\/h1>\n<h4>Outliers are very dangerous. They significantly affect the mean and the standard deviation and thus affecting the estimators of the model. In order to visually see outliers, we need a box plot or a scatter plot. Therefore, lets see the most correlated features with sale price to plot them a gainst each others.<\/h4>","68bdeecf":"1. Split Data in Train\/Test for both X and y\n2. Fit\/Train Scaler on Training X Data\n3. Scale X Test Data\n4. Create Model\n5. Fit\/Train Model on X Train Data\n6. Evaluate Model on X Test Data (by creating predictions and comparing to Y_test)\n7. Adjust Parameters as Necessary and repeat steps 5 and 6","c96f7d8f":"<h1>2. SVM REGRESSION<\/h1>","21e1f85b":"<h1> Choosing the best model<\/h1>","b6a9d4c8":"<h2>Model Validation<\/h2>","e0d2b389":"<h4>We can see a large number of outliers in CRIM, ZN, RM, B. Lets check the percentage of their outlier. <\/h4>\n","ddffdb36":"<h5>The data was drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970.<br>\nThe attributes are de\ufb01ned as follows (taken from the UCI Machine Learning Repository1)<\/h5>","0bfeec17":"<h4>From the correlation matrix RM,LSTAT,TAX,NOX,INDUS,PTRATIO is correlated with PRICE. TAX and RAD is highly correlated with each other(0.91).So, here we get our predictors. <\/h4>","bdf6ff04":"<h1>1. LINEAR REGRESSION<\/h1>","22f144ac":"We can change above default values such that our model accuracy is increased in both train and test dataset.","888de850":"<h2> Predicting ML model on test data.<\/h2>","3c230923":"Model is having very high collinearity.","8b3ead67":"1. Actual vs Predicted price is linear in nature.\n2. Residuals are normally distributed and it follows normality assumptions.\n3. VIF<5 so moderately correlated.","7de00809":"<h2> Predicting ML model on test data.<\/h2>","877a95af":"<h2>Model Validation<\/h2>","b07c439f":"<h1>2. Feature Selection<\/h1>","5d9f3282":"<h2>Model Validation<\/h2>","949f8e04":"1. CRIM: per capita crime rate by\n2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n3. INDUS: proportion of non-retail business acres per town\n4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n5. NOX: nitric oxides concentration (parts per 10 million)\n6. RM: average number of rooms per dwelling\n7. AGE: proportion of owner-occupied units built prior to 1940\n8. DIS: weighted distances to \ufb01ve Boston employment centers\n9. RAD: index of accessibility to radial highways\n10. TAX: full-value property-tax rate per $10,000\n","a8b7cf15":"Adjusted R^2 value is very high which is good for train data. Let us see if it goes well even for test data also.","9ef26ae9":"In order to validated model we need to check few assumption of linear regression model. The common assumption for Linear Regression model are following\n\n1. Linear Relationship: In linear regression the relationship between the dependent and independent variable to be linear. This can be checked by scatter ploting Actual value Vs Predicted value\n2. The residual error plot should be normally distributed.\n3. The mean of residual error should be 0 or close to 0 as much as possible\n4. The linear regression require all variables to be multivariate normal. This assumption can best checked with Q-Q plot.\n5. Linear regession assumes that there is little or no Multicollinearity in the data. Multicollinearity occurs when the independent variables are too highly correlated with each other. The variance inflation factor VIF* identifies correlation between independent variables and strength of that correlation.  VIF=1\/(1\u2212R^2) , If VIF=1 no correlation,\nIf VIF >1 & VIF <5 moderate correlation,\nVIF > 5 critical level of multicollinearity.\n6. Homoscedasticity: The data are homoscedastic meaning the residuals are equal across the regression line. We can look at residual Vs fitted value scatter plot. If heteroscedastic plot would exhibit a funnel shape pattern.","0f33f7d9":"Types of regression algorithms.\n1. Linear Regression\n2. Ridge Regression\n3. Lasso Regression\n4. Random Forest Regressor\n5. XGBoost Regressor","6c24956c":"<h2>Model Validation<\/h2>","0621afb1":"<h1>3. Model Building and Evaluation<\/h1>","164f7a0c":"VIF_XGBR > 5. i.e model is highly collinear in nature.","51f979ef":"<h1>3. RANDOM FOREST REGRESSOR<\/h1>","6e85643e":"Adjusted R^2 value is very good.","a81e1abe":"<h5>Train || Test split procedure<\/h5>","d0193c9e":"\n<h1>OUTCOME :<\/h1>\n<h1>FOR PREDICTING THE HOUSE PRICE IN NEAR FUTURE WE WILL USE THIS XGBOOST MODEL<\/h1>"}}