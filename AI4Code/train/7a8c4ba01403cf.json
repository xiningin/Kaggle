{"cell_type":{"7c3c2390":"code","16602556":"code","fdb4d197":"code","fb6db5cf":"code","b96ab0e7":"code","456047d3":"code","5ec6f309":"code","719aa5a4":"code","8766f956":"code","92b5fa9e":"code","bee71ed5":"code","5f840768":"code","cf8c9847":"code","c1fc4f7b":"code","fccf21f4":"code","bdb149d0":"code","91093020":"code","249d4ec0":"code","45a819e1":"code","eff2945e":"markdown","48fda844":"markdown","399a780c":"markdown","cf87f7da":"markdown","a12d45de":"markdown","2b3fbff4":"markdown","e55b410b":"markdown","1094385e":"markdown","fe9b730f":"markdown","09420f4d":"markdown"},"source":{"7c3c2390":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","16602556":"cd \"\/kaggle\"","fdb4d197":"#loading the dataset\nfile_path = \"input\/heart-attack-analysis-prediction-dataset\/heart.csv\"\nheart_data = pd.read_csv(file_path)\nheart_data.head()","fb6db5cf":"import pandas as pd\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n# Set the width and height of the figure\nplt.figure(figsize=(16,6))\n\nsns.lineplot(data=heart_data)","b96ab0e7":"#creating a heatmap of cholestrol \nplt.figure(figsize=(30,20))\nplt.title(\"Heart attack analysis for number of patients\")\nsns.heatmap(data=heart_data[:50], annot=True)\n\nplt.xlabel(\"Various factors\")\n\n","456047d3":"import pandas as pd\ndf= pd.DataFrame(heart_data,columns=[\"age\",\"sex\",\"cp\",\"trtbps\",\"chol\",\"fbs\",\"restceg\",\"thalachh\",\"exng\",\"oldpeak\",\"slp\",\"caa\",\"thall\",\"output\"])\ncorrMatrix= df.corr()\nplt.figure(figsize=(20,10))\nsns.heatmap(corrMatrix, annot= True,fmt='.1g')\n\nplt.show()","5ec6f309":"sns.scatterplot(x=heart_data['cp'], y=heart_data['thalachh'], hue=heart_data['output'])","719aa5a4":"# lets do a swarm plot of age and cholestrol against the heart rate output.\nsns.swarmplot(x=heart_data['output'],\n              y=heart_data['age'])","8766f956":"sns.swarmplot(x=heart_data['output'],\n              y=heart_data['chol'])","92b5fa9e":"sns.scatterplot(x=heart_data['age'], y=heart_data['chol'], hue=heart_data['output'])","bee71ed5":"#Doing test train split\nfrom sklearn.model_selection import train_test_split\n\n#Selecting target\ny=heart_data.output\n\n#using only numerical predictors\nheart_predictors= heart_data.drop(['output'],axis=1)\nX = heart_predictors.select_dtypes(exclude=['object'])\n\n# Divide data into training and validation subsets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)","5f840768":"#We make a score dataset function to check the correctness of different approach\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=10, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","cf8c9847":"# Get names of columns with missing values\ncols_with_missing = [col for col in X_train.columns\n                     if X_train[col].isnull().any()]\nprint(cols_with_missing)\n# Drop columns in training and validation data\nreduced_X_train = X_train.drop(cols_with_missing, axis=1)\nreduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n\nprint(\"MAE from Approach 1 (Drop columns with missing values):\")\nprint(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))","c1fc4f7b":"#Performing prediction using XGBoost\nfrom sklearn import metrics\nfrom xgboost import XGBClassifier\n#Initialize the classifier\nclf= XGBClassifier()\nclf.fit(reduced_X_train,y_train)\npredictions =clf.predict(reduced_X_valid)\nmetrics.accuracy_score(y_valid,predictions)\n","fccf21f4":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import fbeta_score\nfrom xgboost import XGBClassifier\n\n#Initialize the classifier\nclf= XGBClassifier()\n\n#Parameters\nparameters =  {'nthread':[4], #when use hyperthread, xgboost may become slower\n              'objective':['binary:logistic'],\n              'learning_rate': [0.05], #so called `eta` value\n              'max_depth': [6],\n              'min_child_weight': [11],\n              'silent': [1],\n              'subsample': [0.8],\n              'colsample_bytree': [0.7],\n              'n_estimators': [5], #number of trees, change it to 1000 for better results\n              'missing':[-999],\n               'seed': [1337]}\n\n\n\nscorer = make_scorer(fbeta_score,beta=0.5)\n\ngrid_obj = GridSearchCV(clf, parameters,scorer)\n\ngrid_fit = grid_obj.fit(reduced_X_train,y_train)","bdb149d0":"# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\n# Make predictions using unoptimized and optimized model\npredictions = (clf.fit(reduced_X_train, y_train)).predict(X_valid)\nbest_predictions = best_clf.predict(X_valid)","91093020":"from sklearn.metrics import accuracy_score\n# Report the before-and-afterscores\nprint(\"Unoptimized model\\n------\")\nprint(\"Accuracy score on validation data: {:.4f}\".format(accuracy_score(y_valid, predictions)))\nprint(\"F-score on validation data: {:.4f}\".format(fbeta_score(y_valid, predictions, beta = 0.5)))\nprint(\"\\nOptimized Model\\n------\")\nprint(\"Final accuracy score on the validation data: {:.4f}\".format(accuracy_score(y_valid, best_predictions)))\nprint(\"Final F-score on the validation data: {:.4f}\".format(fbeta_score(y_valid, best_predictions, beta = 0.5)))","249d4ec0":"#implementing random forset classifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nmodel_rf= RandomForestClassifier()\nmodel_rf.fit(reduced_X_train,y_train)\n# Make predictions for the test set\ny_pred_rf = model_rf.predict(reduced_X_valid)\n# View accuracy score\nmetrics.accuracy_score(y_valid, y_pred_rf)\n","45a819e1":"#checking the accuracy using logistic regression\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=50).fit(reduced_X_train, y_train)\ny_pred_lr=clf.predict(reduced_X_valid)\nmetrics.accuracy_score(y_valid, y_pred_rf)\n","eff2945e":"* This correlation heatmap definitely provides, very good insight into the data.Cholestrol, Resting blood pressure and oldpeak have positive correlation with age.\n* There are no two columns whose values are highly correlated , so we have to keep all the columns as they are contributing to the final output independently.\n* The heart rate output is closely correlated with cp, thalachh and slp. We will be plotting scatter plots to dive deep into this insight.","48fda844":"The heatmap above is of first 50 rows , but it does tells us a story. It seems there is a correlation between age , cholestrol , resting blood pressure and highest blood pressure. We will strengthen this point using a correlation plot.","399a780c":"Accuracy achieved is 78.68% . Its a decent accuracy but it definitely can be improved.Lets try GridSearchCV to imporve the accuracy.","cf87f7da":"This line plot of the all the parameters does not provide a good insight. Lets take a look at the heat map below for a better insight into the data.","a12d45de":"So, using GridSearCV definitely increases the accuracy of XGboost classifier from 0.7869 to 0.8689","2b3fbff4":"Inference drawn from above scatter plot:\n*  If the number of major vessels are less the chances of heart attack is less. It increases a bit with the number of major vessels.\n*  As the value of maximum blood pressure is increased the heart attacks increases. basically if it is above 180 mm Hg.\n","e55b410b":"from the above scatter plot it is seems cholestrol in between 300- 200 mg\/dl for the age of 40-55 is the perfect recipe for heart attack.","1094385e":"It seems chances of heart rate are more in between the age of 40-55. It decreases above the age of 55.","fe9b730f":"**Now we will do some preprocessing on the dataset**.","09420f4d":"There are no missing values in this dataset. So handling of missing values is not required."}}