{"cell_type":{"3ec3ed73":"code","53331747":"code","c1ef45fc":"code","255b3f63":"code","a4598c60":"code","0bb50c30":"code","f085a578":"code","714988be":"code","ad1568c0":"code","7427b94a":"code","4ea3ba24":"code","e4a6ce90":"code","31a76119":"code","20948b9b":"code","3d626819":"code","58ee0b91":"code","6b7c8dd7":"code","01561b2d":"code","a0e5c383":"code","6e385b3a":"code","73a18123":"code","5a367369":"code","21ad9d13":"code","734545ae":"code","a4043c51":"code","d0458636":"code","21035207":"code","5c7fe8a9":"code","8a87affd":"code","a27112a7":"code","9c96730c":"code","eeed65db":"code","5e4652b7":"code","cec180aa":"code","ff493670":"code","592ee782":"code","44629041":"code","a4a95d7d":"code","e5ae036c":"code","22b94fcc":"code","b6030b4d":"code","b60ade6b":"code","b9f24850":"code","b1c98436":"code","5e3d16e6":"code","243091af":"code","ae24f4bb":"code","565bcc59":"code","1d62db4c":"code","d0b9e00c":"code","d079cedf":"code","67c39d7b":"code","072b3172":"code","73706308":"code","45712578":"markdown","bfe86691":"markdown","026126f3":"markdown","31d756cb":"markdown","8a34e5ff":"markdown","7bf4d35c":"markdown","86f5cbc4":"markdown","1b7c358e":"markdown","8cf062a7":"markdown","15a0cb6d":"markdown","0511c1f1":"markdown","7e6fec07":"markdown","cccaf3ed":"markdown","5a764481":"markdown","4a089dfa":"markdown","dedf7adf":"markdown","20b67bd4":"markdown","abd0627b":"markdown","cc27d8f4":"markdown","860d3d33":"markdown","6023aafd":"markdown","511f9a97":"markdown"},"source":{"3ec3ed73":"import pandas as pd\nimport sklearn\nfrom sklearn.neighbors import KNeighborsClassifier\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_val_score as vdcruz\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score","53331747":"train = pd.read_csv(\"..\/input\/adult-pmr3508\/train_data.csv\",\n        engine='python',\n        na_values=\"?\")\ntest = pd.read_csv(\"..\/input\/adult-pmr3508\/test_data.csv\",na_values=\"?\")","c1ef45fc":"train.head()\nindex = test.Id","255b3f63":"train[\"sex\"].value_counts().plot(kind=\"bar\")\n","a4598c60":"train[\"education.num\"].value_counts().plot(kind=\"bar\")\n","0bb50c30":"train[\"relationship\"].value_counts().plot(kind=\"bar\")","f085a578":"train[\"education.num\"].value_counts().plot(kind=\"box\")","714988be":"for df in [train,test]:\n    df.set_index('Id',inplace=True)\n","ad1568c0":"train['income'].astype('category')","7427b94a":"test.head()","4ea3ba24":"train.head()","e4a6ce90":"total = train.isnull().sum().sort_values(ascending = False)\npercent = ((train.isnull().sum()\/train.isnull().count())*100).sort_values(ascending = False)\ntrain_faltante = pd.concat([total, percent], axis = 1, keys = ['Total', '%'])\ntrain_faltante.head()","31a76119":"Xtrain = train.drop(columns='income')\nYtrain = train.income\nYtrain.head()","20948b9b":"Xtrain.head()","3d626819":"print(len(Xtrain))\nprint(len(Xtrain.dropna()))","58ee0b91":"for A in Xtrain.columns:\n    Xtrain[A].fillna(Xtrain[A].mode()[0], inplace=True)\nfor A in test.columns:\n    test[A].fillna(test[A].mode()[0], inplace=True)","6b7c8dd7":"Xtrain.shape\nXtrain_nb = Xtrain","01561b2d":"One_Hot_Xtrain = pd.get_dummies(Xtrain)\nOne_Hot_test = pd.get_dummies(test)\nXtrain, test = One_Hot_Xtrain.align(One_Hot_test,join='left',axis=1)\nXtrain.head()","a0e5c383":"from sklearn.preprocessing import StandardScaler\nsc_X=StandardScaler()\nXtrain=sc_X.fit_transform(Xtrain)\ntest=sc_X.transform(test)","6e385b3a":"from sklearn.impute import SimpleImputer\n\nmy_imputer = SimpleImputer()\nXtrain = my_imputer.fit_transform(Xtrain)\ntest = my_imputer.transform(test)","73a18123":"Xtrain, Xtest, Ytrain, Ytest = train_test_split(Xtrain, Ytrain, random_state=0,test_size=0.2)","5a367369":"print(len(Xtrain))\nprint(len(Ytrain))","21ad9d13":"knn = KNeighborsClassifier(n_neighbors=49)\nknn.fit(Xtrain,Ytrain)","734545ae":"accuracy_score(Ytest,knn.predict(Xtest))","a4043c51":"resultado_knn = vdcruz(knn, Xtrain, Ytrain, cv=5)\nresultado_knn.mean()","d0458636":"matriz_de_confusao = confusion_matrix(knn.predict(Xtest),Ytest)\nprint(matriz_de_confusao)","21035207":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100)\nrf.fit(Xtrain,Ytrain)","5c7fe8a9":"accuracy_score(Ytest,rf.predict(Xtest))","8a87affd":"resultado_rf = vdcruz(rf, Xtrain, Ytrain, cv=5)\nresultado_rf.mean()","a27112a7":"matriz_de_confusao = confusion_matrix(rf.predict(Xtest),Ytest)\nprint(matriz_de_confusao)","9c96730c":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(Xtrain, Ytrain)\nnb.score","eeed65db":"accuracy_score(Ytest,nb.predict(Xtest))","5e4652b7":"matriz_de_confusao = confusion_matrix(nb.predict(Xtest),Ytest)\nprint(matriz_de_confusao)","cec180aa":"resultado_nb = vdcruz(nb, Xtrain, Ytrain, cv=10)\nresultado_nb.mean()","ff493670":"from sklearn.svm import SVC\nsvc=SVC(gamma='auto')\nsvc.fit(Xtrain,Ytrain)","592ee782":"accuracy_score(Ytest,svc.predict(Xtest))","44629041":"matriz_de_confusao = confusion_matrix(svc.predict(Xtest),Ytest)\nprint(matriz_de_confusao)","a4a95d7d":"resultado_svc = vdcruz(svc, Xtrain, Ytrain, cv=5)\nresultado_svc.mean()","e5ae036c":"Ytest_KNN = knn.predict(test)\nYtest_rf = rf.predict(test)\nYtest_nb = nb.predict(test)\nYtest_svc = svc.predict(test)\n","22b94fcc":"Ctrain = pd.read_csv(\"..\/input\/california\/Ctrain.csv\",\n        engine='python',\n        na_values=\"?\")\nCtest = pd.read_csv(\"..\/input\/california\/Ctest.csv\",na_values=\"?\")","b6030b4d":"Ctrain.head()","b60ade6b":"Ctest.head()","b9f24850":"for df in [Ctrain,Ctest]:\n    df.set_index('Id',inplace=True)","b1c98436":"XCtrain = Ctrain.drop(columns='median_house_value')\nYCtrain = Ctrain.median_house_value\nYCtrain.head()","5e3d16e6":"print(len(XCtrain))\nprint(len(XCtrain.dropna()))","243091af":"from sklearn.linear_model import LinearRegression\nreg = LinearRegression()\nreg.fit(XCtrain,YCtrain)","ae24f4bb":"reg.score(XCtrain,YCtrain)","565bcc59":"reg.coef_","1d62db4c":"reg.intercept_","d0b9e00c":"resultado_reg = vdcruz(reg, XCtrain, YCtrain, cv=10)\nresultado_reg.mean()","d079cedf":"YCtest = reg.predict(Ctest)\nYCtest","67c39d7b":"from sklearn import linear_model\nlasso = linear_model.Lasso(alpha=0.1)\nlasso.fit(XCtrain,YCtrain)\n\nresultado_lasso = vdcruz(lasso, XCtrain, YCtrain, cv=10)\nresultado_lasso.mean()","072b3172":"from sklearn.tree import DecisionTreeRegressor","73706308":"scores_mean = []\nscores_std = []\n\nk_lim_inf = 1\nk_lim_sup = 30\n\nfolds = 10\n\nk_max = None\nmax_acc = 0\n\ni = 0\nprint('Finding best k...')\nfor k in range(k_lim_inf, k_lim_sup):\n    \n    regr = DecisionTreeRegressor(max_depth=k)\n    \n    score = vdcruz(regr, XCtrain, YCtrain, cv = folds)\n    \n    scores_mean.append(score.mean())\n    scores_std.append(score.std())\n    \n    if scores_mean[i] > max_acc:\n        k_max = k\n        max_acc = scores_mean[i]\n    i += 1\n    print('   k = {0} | Best CV acc = {1:2.2f}% (best k = {2})'.format(k, max_acc*100, k_max))\nprint('\\nBest k: {}'.format(k_max))","45712578":"# 2.2 LASSO","bfe86691":"# 2.1 Regress\u00e3o Linear:","026126f3":"# 1.2 Para o Trabalho 2, vamos come\u00e7ar com Random Forest","31d756cb":"# 1.3 Agora faremos com Naive Bayes","8a34e5ff":"Faltam alguns dados, vou substituir pela moda:","7bf4d35c":"# %%%% PMR3508 - Trabalho 2 - Jo\u00e3o Pedro Dias Nunes - 10805846 %%%%","86f5cbc4":"Para n\u00e3o ficar bin\u00e1rio (1,0) vou fazer uma pondera\u00e7\u00e3o","1b7c358e":"Testando o melhor k: ","8cf062a7":"# 2.3 Regress\u00e3o de \u00c1rvore de Decis\u00e3o","15a0cb6d":"# 2. Agora, a base CaliforniaIncome","0511c1f1":"Importa\u00e7\u00e3o e breve visualiza\u00e7\u00e3o dos dados das bases","7e6fec07":"scores_mean = []\nscores_std = []\n\nk_lim_inf = 1\nk_lim_sup = 60\n\nfolds = 5\n\nk_max = None\nmax_acc = 0\n\ni = 0\nprint('Finding best k...')\nfor k in range(k_lim_inf, k_lim_sup,3):\n    \n    KNNclf = KNeighborsClassifier(n_neighbors=k)\n    \n    score = vdcruz(KNNclf, Xtrain, Ytrain, cv = folds)\n    \n    scores_mean.append(score.mean())\n    scores_std.append(score.std())\n    \n    if scores_mean[i] > max_acc:\n        k_max = k\n        max_acc = scores_mean[i]\n    i += 1\n    print('   K = {0} | Best CV acc = {1:2.2f}% (best k = {2})'.format(k, max_acc*100, k_max))\nprint('\\nBest k: {}'.format(k_max))","cccaf3ed":"# 1.1 Agora podemos escolher o modelo de predi\u00e7\u00e3o. J\u00e1 hav\u00edamos feito com KNN no Trabalho 1","5a764481":"Treinamos o classificador:","4a089dfa":"Avalia\u00e7\u00e3o:","dedf7adf":"Dessa forma, utilizaremos apenas o acerto bruno (accuracy_score) para an\u00e1lise dos dados.\nDividi a base de treino em treino e teste para poder medir o resultado.\n\nKNN: 0.82\nRandom Forest: 0.85\nNaive Bayes: 0.44\nSVC: 0.84\n\nPodemos observar que, com a mesma prepara\u00e7\u00e3o de dados, tr\u00eas dos quatros classificadores atingiram resultados bons, pr\u00f3ximos ao classificador de Bayes (ideal). No entanto, Naive Bayes obteve um resultado bem abaixo do esperado, inclusive, como \u00e9 uma escolha bin\u00e1ria, com a inten\u00e7\u00e3o de classifica\u00e7\u00e3o acima de 50%, obtivemos apenas 44%. Logo, se invert\u00eassemos todas as escolhas, a acur\u00e1cia bruta seria de 56%. Isso ocorre, provavelmente, porque a hip\u00f3tese de probabilidades independentes assumidas em Naive Bayes \u00e9 muito falha. Muitas categorias possuem muitos r\u00f3tulos pouco utilizados, e isso provavelmente contribuiu para o resultado mais abaixo.\n\nJ\u00e1 KNN, K-Nearest Neighbour, que se baseia no \"vizinho mais pr\u00f3ximo\" obteve significado relvante com K=49 (coloquei o algoritmo em Markdown acima). Isso ocorre pois, na vida real pessoas com caracter\u00edstica semelhante tem alta probabilidade de ser semelhante quanto a sua renda.\n\nJ\u00e1 Random Forest funcionou bem pelo grande n\u00famero de features categ\u00f3ricas, o que facilita sua utiliza\u00e7\u00e3o, e SVC, especial principalmente em dividir o dataset em 2 partes, como a reta perperdicular \u00e0 regress\u00e3o, funcionou bem tamb\u00e9m ao calcular a fronteira entre os grupos.\n\nAbaixo, caso seja de interesse, a predi\u00e7\u00e3o da base de teste propriamente dita (n\u00e3o a utilizei pois n\u00e3o tenho o \"gabarito\" para chegar a acur\u00e1cia).","20b67bd4":"# 1. Primeiro, importamos a base adult","abd0627b":"Avalia\u00e7\u00e3o:","cc27d8f4":"Avalia\u00e7\u00e3o:","860d3d33":"# 1.4 Por fim, SVC","6023aafd":"Avalia\u00e7\u00e3o:","511f9a97":"Substitui\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas por num\u00e9ricas usando o get_dummies"}}