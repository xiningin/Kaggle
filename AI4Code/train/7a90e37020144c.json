{"cell_type":{"4d0ed0fc":"code","654339b7":"code","fb921796":"code","19164b67":"code","b483bea0":"code","ded20924":"code","ffed83a5":"code","b5eae53d":"code","7e47ffdd":"code","6da57523":"code","9e64f405":"code","0224cfc5":"code","5ef022cf":"code","3eff3bd2":"code","5c8f6dd6":"code","8084605a":"code","3b3e14bc":"code","45834305":"code","3ddd6d1e":"code","93faca76":"code","6a3d1948":"code","7fc9de39":"code","6ed982a3":"code","a5344094":"code","eb7ef08e":"code","cf187422":"code","354b3383":"code","ee9a19d3":"code","782117a8":"code","368aa209":"code","c752ba1d":"code","5ec1c4b6":"code","7974add7":"code","4acdb4d2":"code","55c8ae12":"code","b8c0fba3":"code","12298549":"code","348ef51f":"code","842aafe5":"code","332d3213":"code","fe1b691d":"code","00cf91aa":"code","62119793":"code","1fc76f03":"code","9df492f5":"code","3d0c46aa":"code","983568ff":"code","8a63e3cb":"code","ebec6cc9":"markdown","41b5a965":"markdown","9d3448d4":"markdown","0a661a09":"markdown","43dd6d1c":"markdown","2560efca":"markdown","ab1670d6":"markdown","7758cdef":"markdown","fa1dbc55":"markdown","bc62fdb6":"markdown","6784bb8d":"markdown","92589df1":"markdown","efb6209c":"markdown","e6a33866":"markdown","1ec90b32":"markdown","f8763b53":"markdown","c735a32d":"markdown","c3cad548":"markdown","f6ffcbe2":"markdown","2a330130":"markdown"},"source":{"4d0ed0fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","654339b7":"df_geoloc = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_geolocation_dataset.csv')\ndf_orders = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_orders_dataset.csv')\ndf_customers = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_customers_dataset.csv')\ndf_payments = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_order_payments_dataset.csv')","fb921796":"df_geoloc.drop_duplicates(subset=['geolocation_zip_code_prefix'],inplace=True)\n#print(df_geoloc.head())","19164b67":"#function to convert a dataframe into geojson format. \ndef df_to_geojson(df, properties, lat='geolocation_lat', lon='geolocation_lng'):\n    geojson = {'type':'FeatureCollection', 'features':[]}\n    for _, row in df.iterrows():\n        feature = {'type':'Feature',\n                   'properties':{},\n                   'geometry':{'type':'Point',\n                               'coordinates':[]}}\n        feature['geometry']['coordinates'] = [row[lon],row[lat]]\n        for prop in properties:\n            feature['properties'][prop] = row[prop]\n        geojson['features'].append(feature)\n    return geojson\n\n#converting the geoloc dataset into geojson format\ncols = df_geoloc.columns\ngeosource = df_to_geojson(df_geoloc, cols)\n#print(df_geoloc.shape)\n","b483bea0":"#map all coordinates\nimport folium\n\n#function to get the central lat,long coordinates from the dataset\ndef get_center_latlong(df_map1):\n    # get the center of my map for plotting\n    centerlat = (df_map1['geolocation_lat'].max() + df_map1['geolocation_lat'].min()) \/ 2\n    centerlong = (df_map1['geolocation_lng'].max() + df_map1['geolocation_lng'].min()) \/ 2\n    return centerlat, centerlong\n\ncenter = get_center_latlong(df_geoloc)\n# create a new map object\nm = folium.Map(location=center, zoom_start=3, tiles='Stamen Toner')","ded20924":"#plot markers on map\nfor i in range(0,len(df_geoloc)):\n    folium.Marker([df_geoloc.iloc[i]['geolocation_lat'], df_geoloc.iloc[i]['geolocation_lng']], popup=df_geoloc.iloc[i]['geolocation_zip_code_prefix']).add_to(m)\n\nfolium.LayerControl().add_to(m)","ffed83a5":"m","b5eae53d":"outliers = ['18243','78131','68275','28165','83252']\nindexNames = df_geoloc[(df_geoloc['geolocation_zip_code_prefix'] == 18243) | (df_geoloc['geolocation_zip_code_prefix'] == 78131) | (df_geoloc['geolocation_zip_code_prefix'] == 68275) | (df_geoloc['geolocation_zip_code_prefix'] == 28165) | (df_geoloc['geolocation_zip_code_prefix'] == 83252) | (df_geoloc['geolocation_zip_code_prefix'] == 83810) ].index\ndf_geoloc.drop(indexNames , inplace=True)\n#print(df_geoloc.shape)","7e47ffdd":"center = get_center_latlong(df_geoloc)\n# create a new map object\nm = folium.Map(location=center, zoom_start=3, tiles='Stamen Toner')\n\nfor i in range(0,len(df_geoloc)):\n    folium.Marker([df_geoloc.iloc[i]['geolocation_lat'], df_geoloc.iloc[i]['geolocation_lng']], popup=df_geoloc.iloc[i]['geolocation_zip_code_prefix']).add_to(m)\n\nfolium.LayerControl().add_to(m)","6da57523":"m","9e64f405":"df_new = pd.merge(df_customers,df_orders,on='customer_id')\ndf_new.columns\ntemp = df_new['customer_zip_code_prefix'].value_counts()\ntemp = temp.to_dict()\n\ndf_new2 = pd.merge(df_new,df_payments)\n#df_new2.columns","0224cfc5":"df_new2 = df_new2.sort_values(by=['customer_zip_code_prefix'])\nsum = 0\nzip = df_new2['customer_zip_code_prefix'].iloc[0]\npayment_dict = {}\npayment_dict[zip]=0\n\nfor i in range(df_new2.shape[0]):\n    if df_new2['customer_zip_code_prefix'].iloc[i]!=zip:\n        zip = df_new2['customer_zip_code_prefix'].iloc[i]\n        sum = 0\n        payment_dict[zip]=0\n    else:\n        payment_dict[zip]+=df_new2['payment_value'].iloc[i]","5ef022cf":"data_items = payment_dict.items()\npayment_list = list(data_items)\ndf_new3 = pd.DataFrame(payment_list)\n\ndf_map1 = df_new3.rename(columns = {0:'geolocation_zip_code_prefix',1:'total_payments'})\ndf_new3.rename(columns = {0:'customer_zip_code_prefix',1:'total_payments'}, inplace=True)\n#print(df_map1.head())","3eff3bd2":"data_items = temp.items()\ntemp_list = list(data_items)\ndf_new4 = pd.DataFrame(temp_list)\ndf_new4 = df_new4.rename(columns = {0:'geolocation_zip_code_prefix', 1:'total_volume'})\ndf_map1 = pd.merge(df_map1,df_new4)\n#print(df_map1.head())","5c8f6dd6":"df_map1 = pd.merge(df_map1,df_geoloc)\n#print(df_map1.head())\n","8084605a":"print(df_map1['total_payments'].min(),df_map1['total_payments'].max())","3b3e14bc":"#heatmap using folium","45834305":"from folium import plugins\nfrom folium.plugins import HeatMap\nimport branca.colormap as cm\nfrom collections import defaultdict\n\ncenter = get_center_latlong(df_map1)\n# create a new map object\nm = folium.Map(location=center, zoom_start=4, tiles='stamen toner')\n\ndf_map1['geolocation_lat'] = df_map1['geolocation_lat'].astype(float)\ndf_map1['geolocation_lng'] = df_map1['geolocation_lng'].astype(float)\n\n# Filter the DF for rows, then columns, then remove NaNs\nheat_df = df_map1\nheat_df = heat_df[['geolocation_lat', 'geolocation_lng', 'total_payments']]\nheat_df = heat_df.dropna(axis=0, subset=['geolocation_lat','geolocation_lng','total_payments'])\n\n# List comprehension to make out list of lists\nheat_data = [[row['geolocation_lat'],row['geolocation_lng'],row['total_payments']] for index, row in heat_df.iterrows()]\n\nlinear = cm.LinearColormap(['blue', 'purple', 'green','yellow','orange','red'],\n                           vmin=df_map1['total_payments'].min(), \n                           vmax=df_map1['total_payments'].max())\nlinear.add_to(m)\n# Plot it on the map\nHeatMap(heat_data,radius=10).add_to(m)\n","3ddd6d1e":"linear","93faca76":"m","6a3d1948":"center = get_center_latlong(df_map1)\n# create a new map object\nm = folium.Map(location=center, zoom_start=4, tiles='Stamen Toner')\n\ndf_map1['geolocation_lat'] = df_map1['geolocation_lat'].astype(float)\ndf_map1['geolocation_lng'] = df_map1['geolocation_lng'].astype(float)\n\n# Filter the DF for rows, then columns, then remove NaNs\nheat_df = df_map1\nheat_df = heat_df[['geolocation_lat', 'geolocation_lng', 'total_volume']]\nheat_df = heat_df.dropna(axis=0, subset=['geolocation_lat','geolocation_lng','total_volume'])\n\n# List comprehension to make out list of lists\nheat_data = [[row['geolocation_lat'],row['geolocation_lng'],row['total_volume']] for index, row in heat_df.iterrows()]\n\nlinear = cm.LinearColormap(['blue', 'purple', 'green','yellow','orange','red'],\n                           vmin=df_map1['total_volume'].min(), \n                           vmax=df_map1['total_volume'].max())\nlinear.add_to(m)\n\n# Plot it on the map\nHeatMap(heat_data,radius=10).add_to(m)\n\n","7fc9de39":"linear","6ed982a3":"m","a5344094":"df_products = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_products_dataset.csv')\ndf_prod_trans = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/product_category_name_translation.csv')","eb7ef08e":"df_products = pd.merge(df_products,df_prod_trans)\ndf_products.drop(columns=['product_category_name'],inplace=True)\ndf_products.rename(columns={'product_name_lenght':'product_name_length','product_description_lenght':'product_description_length','product_category_name_english':'product_category_name'},inplace=True)\n#print(df_products.head())","cf187422":"df_order_items = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_order_items_dataset.csv')\n#print(df_order_items.columns)","354b3383":"df_prod1 = pd.merge(df_order_items,df_products)\n#print(df_prod1.head())","ee9a19d3":"#Formatting dataframes for analysis by year\n\ndf_prod1['Year']=0\nfor i in range(len(df_prod1)):\n    if '2016' in df_prod1['shipping_limit_date'].iloc[i]:\n        df_prod1['Year'].iloc[i]=2016\n    elif '2017' in df_prod1['shipping_limit_date'].iloc[i]:\n        df_prod1['Year'].iloc[i]=2017\n    elif '2018' in df_prod1['shipping_limit_date'].iloc[i]:\n        df_prod1['Year'].iloc[i]=2018\n#print(df_prod1.head())\n#print(len(l))\n","782117a8":"#all years product categories\nprod_freq = df_prod1['product_category_name'].value_counts()\nprod_freq = prod_freq.to_dict()\ndata_items = prod_freq.items()\ntemp_list = list(data_items)\ndf_prod2 = pd.DataFrame(temp_list)\n#print(df_prod2.head())\n\ndf_prod2 = df_prod2.rename(columns = {0:'product_category_name', 1:'total_freq'})\nprint(df_prod2.head())","368aa209":"df_prod3 = df_prod2.loc[list(range(10))].set_index('product_category_name')","c752ba1d":"!pip install pyecharts","5ec1c4b6":"from pyecharts.charts import Bar\nfrom pyecharts import options as opts\nfrom pyecharts.globals import ThemeType\nbar = (\n Bar(init_opts=opts.InitOpts(theme=ThemeType.WONDERLAND))\n .add_xaxis(list(df_prod3.index))\n .add_yaxis(\"Category\", list(df_prod3['total_freq']))\n #.add_yaxis(\u201cTemperature Min\u201d, [-1,0,5,12,18,24,27,26,21,14,8,2])\n .set_global_opts(title_opts=opts.TitleOpts(title=\"Top 10 product categories\", subtitle=\"Year 2016 to 2018\"))\n)\nbar.render_notebook()","7974add7":"#2016 product categories\ndf_prod2016 = df_prod1[(df_prod1['Year']==2016)]\nprint(len(df_prod2016))\n\n#2016 product categories\ndf_prod2017 = df_prod1[(df_prod1['Year']==2017)]\nprint(len(df_prod2017))\n\n#2016 product categories\ndf_prod2018 = df_prod1[(df_prod1['Year']==2018)]\nprint(len(df_prod2018))\n\n#assert len(df_prod1)==len(df_prod2018)+len(df_prod2017)+len(df_prod2016)\n#print(len(df_prod1),len(df_prod2018)+len(df_prod2017)+len(df_prod2016))","4acdb4d2":"prod_freq = df_prod2017['product_category_name'].value_counts()\nprod_freq = prod_freq.to_dict()\ndata_items = prod_freq.items()\ntemp_list = list(data_items)\ndf_prod3 = pd.DataFrame(temp_list)\n#print(df_prod2.head())\n\ndf_prod3 = df_prod3.rename(columns = {0:'product_category_name', 1:'total_freq'})\n#print(df_prod3.head())\n\ndf_prod3 = df_prod3.loc[list(range(10))].set_index('product_category_name')","55c8ae12":"bar = (\n Bar(init_opts=opts.InitOpts(theme=ThemeType.WONDERLAND))\n .add_xaxis(list(df_prod3.index))\n .add_yaxis(\"Category\", list(df_prod3['total_freq']))\n #.add_yaxis(\u201cTemperature Min\u201d, [-1,0,5,12,18,24,27,26,21,14,8,2])\n .set_global_opts(title_opts=opts.TitleOpts(title=\"Top 10 product categories - 2017\"))\n)\nbar.render_notebook()","b8c0fba3":"prod_freq = df_prod2018['product_category_name'].value_counts()\nprod_freq = prod_freq.to_dict()\ndata_items = prod_freq.items()\ntemp_list = list(data_items)\ndf_prod3 = pd.DataFrame(temp_list)\n#print(df_prod2.head())\n\ndf_prod3 = df_prod3.rename(columns = {0:'product_category_name', 1:'total_freq'})\n#print(df_prod3.head())\n\ndf_prod3 = df_prod3.loc[list(range(10))].set_index('product_category_name')","12298549":"bar = (\n Bar(init_opts=opts.InitOpts(theme=ThemeType.WONDERLAND))\n .add_xaxis(list(df_prod3.index))\n .add_yaxis(\"Category\", list(df_prod3['total_freq']))\n #.add_yaxis(\u201cTemperature Min\u201d, [-1,0,5,12,18,24,27,26,21,14,8,2])\n .set_global_opts(title_opts=opts.TitleOpts(title=\"Top 10 product categories - 2018\"))\n)\nbar.render_notebook()","348ef51f":"#create mega dataset with all relevant columns\ndf_mega = pd.merge(df_prod1,df_new2)\n#print(df_mega.columns)","842aafe5":"#connect categories to cities\ncols = ['geoloc_zip_code_prefix']\nfor i in range(len(df_prod2)):\n    cols.append(df_prod2['product_category_name'].iloc[i])\n#print(len(cols))","332d3213":"#connect categories to zip code\ndf_zipcat = pd.DataFrame(columns = cols, index = range(19010))\n#print(df_zipcat.columns)\ndf_zipcat.fillna(0,inplace=True)\ndf_zipcat['geoloc_zip_code_prefix'].fillna(' ',inplace=True)\n\n#print(df_zipcat.head())\n","fe1b691d":"df_mega = df_mega.sort_values(['customer_zip_code_prefix','order_id'])\n#df_mega.set_index('customer_zip_code_prefix')\n#print(df_mega['customer_zip_code_prefix'].iloc[0:10])","00cf91aa":"zipcode = df_mega['customer_zip_code_prefix'].iloc[0]\n#count = 0\nindex = 0\nfor i in range(len(df_mega)):\n    if(df_mega['customer_zip_code_prefix'].iloc[i]!=zipcode):\n        #print(i,index, zipcode, df_mega['customer_zip_code_prefix'].iloc[i])\n        zipcode = df_mega['customer_zip_code_prefix'].iloc[i]\n        #count = 0\n        index+=1\n        df_zipcat['geoloc_zip_code_prefix'].loc[index] = zipcode\n    category = df_mega['product_category_name'].iloc[i]\n    category = str(category)\n    df_zipcat[category].loc[index]+=1\n\n#print(df_zipcat.head())\n    ","62119793":"#zip code | top category | percentage\n#if multiple categories have same frequency value, we take the first in occurence\ncols = ['geolocation_zip_code_prefix','top category','percentage']\ndf_ziptopcat = pd.DataFrame(columns = cols, index = range(19010))\n#print(df_zipcat.columns)\ndf_ziptopcat.fillna(' ',inplace=True)\ndf_ziptopcat['percentage'].fillna(0,inplace=True)\n\n#print(df_ziptopcat.head())","1fc76f03":"c=0\nfor ind,r in df_zipcat.iterrows():\n    #print(i)\n    df_ziptopcat['geolocation_zip_code_prefix'].loc[c] = r[0]\n    maxcat = max(r[1:])\n    s = r[1:].sum()\n    p = (maxcat\/s)*100\n    df_ziptopcat['percentage'].loc[c] = p\n    for i in df_zipcat.columns[1:]:\n        #print(i)\n        if df_zipcat[i].loc[c]==maxcat:\n            df_ziptopcat['top category'].loc[c] = i\n            break\n    c+=1\n            \n#print(df_ziptopcat.head())","9df492f5":"#add lat and long\ndf_map2 = pd.merge(df_map1,df_ziptopcat)\n#print(df_map2.head())","3d0c46aa":"#mapping top categories\n\nprint(len(df_map2))\ndf_map21 = df_map2[df_map2['top category'].str.contains('bed_bath_table')]\nprint(len(df_map21))","983568ff":"center = get_center_latlong(df_map21)\n# create a new map object\nm = folium.Map(location=center, zoom_start=4, tiles='cartodbpositron')\ntooltip = 'tooltip'\nfor i in range(0,len(df_map21)):\n    folium.Marker([df_map21.iloc[i]['geolocation_lat'], df_map21.iloc[i]['geolocation_lng']], popup=df_map21.iloc[i]['geolocation_zip_code_prefix'], tooltip=(df_map21.iloc[i]['geolocation_city'])).add_to(m)\n\n","8a63e3cb":"m","ebec6cc9":"Since we have relatively lesser data from 2016, we'll focus our analysis on data from 2017 and 2018","41b5a965":"## To do\n \n1. Time series geospatial analysis of top product categories\n2. EDA for seasonality, trends \n\n","9d3448d4":"### Locations with 'bed_bath_table' as its top category","0a661a09":"## Part 3: Customer Analysis [to-do] ","43dd6d1c":"# **Overview**\nOn a quick review of the csv files, I have broadly categorised the areas of analysis of the dataset into 3 major areas:\n1. Orders\n2. Products\n3. Customers [To do]\n\nIn this notebook I aim to explore and draw insights from these 3 areas","2560efca":"## Insights\n\nFrom the heatmap we can see that a high volume of purchases are made at coastal regions. \n","ab1670d6":"We can see that there are a few outliers. Since there are only a few, we can remove them manually","7758cdef":"Plotting new map after removing outliers","fa1dbc55":"### Geospatial Analysis of Product Categories","bc62fdb6":"### Heatmap of orders by number of purchases","6784bb8d":"### Heatmap of orders by cost","92589df1":"**Cleaning the zip codes dataset (geolocation)**","efb6209c":"We can see that quite a high number of cities have 'bed_bath_table' as their top category","e6a33866":"### To do:\n1. Order delivery route analysis","1ec90b32":"## Insights\n\nFrom the heatmap we can see that a majority of purchases are made at Sao Paulo and Rio de Janeiro. ","f8763b53":"## PART 2: Products\n\nMy focus is on analysing the product categories popularity in two ways:\n1. By year\n2. By region (zip code)\n\nAs usual, the first step is to format the data in the best format for visualisation","c735a32d":"### **Loading the data**","c3cad548":"# Cleaning the files and EDA","f6ffcbe2":"#### Mapping all the zip codes - checking for outliers","2a330130":"## Data Exploration: Orders\n\nThe dataset has a lot of information on orders. My focus is on deriving insights based on 2 questions:\n1. What is the geospatial distribution of orders by number of orders?\n2. What is the geospatial distribution of orders by cost of purchase?\nWe can answer these questions by plotting a heatmap.\n\nThe first step is to get the data in the right format to plot the heatmap."}}