{"cell_type":{"cc83d053":"code","58ee0ca0":"code","a978327a":"code","fae3e458":"code","ef7dff02":"code","b0da8352":"code","edddd62c":"code","8ea9df66":"code","32d059c9":"code","fcbf8651":"code","2f899613":"code","abb00f1f":"code","9df361fd":"code","80851072":"code","2da482bc":"code","9b5b622c":"code","604069ad":"code","a4b7703c":"code","695c674c":"code","055be3c7":"code","0758dbac":"code","15072820":"code","77d32113":"code","c3bf695f":"code","35184f14":"code","c314d409":"code","547e88fa":"code","b94626e8":"code","2cc6aa71":"code","326c949b":"code","f4e6ead4":"code","905e6f4a":"code","102f2070":"code","6f58da28":"code","5d7abc95":"code","9bdd595f":"code","828f9d19":"code","3d3173e8":"code","83e357aa":"code","01c580af":"code","69174ac3":"code","d1bc59dd":"markdown","d01bb659":"markdown","9675598b":"markdown","6ee51108":"markdown","a880fb17":"markdown","aa36b4c6":"markdown","f8f65154":"markdown","281809b0":"markdown","12145009":"markdown","07244f33":"markdown","e20b94c2":"markdown","3c5d242b":"markdown","86dc757d":"markdown","cb507970":"markdown","701235f7":"markdown","251c61dc":"markdown","5d7ced2b":"markdown","18dde667":"markdown","491b3774":"markdown","42bdd06e":"markdown","1963f8af":"markdown"},"source":{"cc83d053":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport folium\nimport plotly.graph_objects as go\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold,RepeatedStratifiedKFold,GridSearchCV,cross_val_score\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import classification_report","58ee0ca0":"#Read the dataset\ndf = pd.read_csv(\"\/kaggle\/input\/customer-churn-prediction-2020\/train.csv\")","a978327a":"# The data set on which the prediction will be done and result will be submitted for evaluation.\ntest = pd.read_csv('\/kaggle\/input\/customer-churn-prediction-2020\/test.csv')","fae3e458":"# first 5 records from the dataset\ndf.head()","ef7dff02":"df.info()","b0da8352":"df.describe()","edddd62c":"#Function to get summary statistics for categorical variable.\n\ndef dataQuality(data):\n    d={}\n    def cat_quality(data):\n        def count(x):\n            return x.count()\n        def miss_per(x):\n            return x.isnull().sum()\/len(x)\n        def unique(x):\n            return len(x.unique())\n        def freq_cat(x):\n            return x.value_counts().sort_values(ascending=False).index[0]\n        def freq_cat_per(x):\n            return x.value_counts().sort_values(ascending=False).index[0]\/len(x)\n        qr=dict()\n        #select only categorical data types\n        data=data.select_dtypes(include=[object])\n        for i in np.arange(0,len(data.columns),1):\n            xi=data.agg({data.columns[i]:[count,unique,miss_per,freq_cat]})\n            qr[data.columns[i]]=xi.reset_index(drop=True)[data.columns[i]]\n            df2=pd.DataFrame(qr)\n            #df2.index=xi.index\n        df2.index=[\"Count\",\"Unique\",\"Miss_percent\",\"Freq_Level\"]\n        return df2.T\n    d['categorical']=cat_quality(data)\n    return d","8ea9df66":"(dataQuality(df)['categorical'])","32d059c9":"plt.figure(figsize=(5,5))\nsplot=sns.countplot(data=df,x='churn',palette='GnBu')\nsns.set_style('ticks')\ntotal = float(len(df))\nfor p in splot.patches:\n    percentage = '{:.1f}%'.format(100 * p.get_height()\/total)\n    x = p.get_x() + p.get_width()\n    y = p.get_height()\n    splot.annotate(percentage,(x,y),ha = 'center', va = 'center')\nplt.title(\"Churns\")\nplt.xlabel(\"Churn\")\nplt.ylabel(\"Number of customers\")","fcbf8651":"plt.figure(figsize=(5,5))\nsns.catplot(data=df[['account_length','churn']],x='churn',y='account_length',kind=\"box\",palette='GnBu')\nplt.title(\"Loyalty and cusomer churn\")\nplt.ylabel(\"Number of months with operator\")","2f899613":"sns.catplot(data=df[['total_day_charge','churn']],x='churn',y='total_day_charge',kind=\"box\",palette='GnBu')\nplt.title(\"Day call charges Vs Churn\")\nplt.ylabel(\"Call charges in USD\")\n\nsns.catplot(data=df[['total_eve_charge','churn']],x='churn',y='total_eve_charge',kind=\"box\",palette='GnBu')\nplt.title(\"Evening call charges Vs Churn\")\nplt.ylabel(\"Call charges in USD\")\n\nsns.catplot(data=df[['total_night_charge','churn']],x='churn',y='total_night_charge',kind=\"box\",palette='GnBu')\nplt.title(\"Night call charges Vs Churn\")\nplt.ylabel(\"Call charges in USD\")\n\nsns.catplot(data=df[['total_intl_charge','churn']],x='churn',y='total_intl_charge',kind=\"box\",palette='GnBu')\nplt.title(\"International call charges Vs Churn\")\nplt.ylabel(\"Call charges in USD\")","abb00f1f":"# creaet a dataframe containing states and count of customer churns those states.\nstate_count1=df[['state','churn']]\nstate_churn=state_count1[state_count1['churn']== 'yes'].groupby('state',as_index = False).count()\n\nstate_count2=df[['state','churn']].groupby('state',as_index = False).count()\nstate_count2.rename(columns={\"churn\":\"total_cust\"},inplace=True)\n\nstate_churn['total_cust']=state_count2['total_cust']\nstate_churn['%churn']=state_churn['churn']\/state_count2['total_cust']","9df361fd":"# Coropleth\nfig = go.Figure(data=go.Choropleth(\n    locations=state_churn['state'], # Spatial coordinates\n    z = state_churn['%churn'].astype(float), # Data to be color-coded\n    locationmode = 'USA-states', # set of locations match entries in `locations`\n    colorscale = 'GnBu',\n    colorbar_title = \"churn percentage\",\n))\n\nfig.update_layout(\n    title_text = 'Customer churns by State',\n    geo_scope='usa', # limit map scope to USA\n)\n\nfig.show()","80851072":"plt.figure(figsize=(5,5))\nsplot=sns.countplot(data=df,x='area_code',palette='GnBu',hue = 'churn')\nsns.set_style('ticks')\ntotal = float(len(df))\nfor p in splot.patches:\n    percentage = '{:.1f}%'.format(100 * p.get_height()\/total)\n    x = p.get_x() + p.get_width()\n    y = p.get_height()\n    splot.annotate(percentage,(x,y),ha = 'center', va = 'center')\nplt.title(\"Customer churns in different area codes\")\nplt.xlabel('Area code')\nplt.ylabel('Number of customers')\nplt.xticks(rotation=45)\nplt.show()","2da482bc":"plt.figure(figsize=(5,5))\nsplot=sns.countplot(data=df,x='international_plan',palette='GnBu',hue = 'churn')\nsns.set_style('ticks')\ntotal = float(len(df))\nfor p in splot.patches:\n    percentage = '{:.1f}%'.format(100 * p.get_height()\/total)\n    x = p.get_x() + p.get_width()\n    y = p.get_height()\n    splot.annotate(percentage,(x,y),ha = 'center', va = 'center')\nplt.xlabel('International Plan?')\nplt.ylabel('Number of customers')\nplt.xticks(rotation=45)\nplt.show()","9b5b622c":"plt.figure(figsize=(5,5))\nsplot=sns.countplot(data=df,x='voice_mail_plan',palette='GnBu',hue = 'churn')\nsns.set_style('ticks')\ntotal = float(len(df))\nfor p in splot.patches:\n    percentage = '{:.1f}%'.format(100 * p.get_height()\/total)\n    x = p.get_x() + p.get_width()\n    y = p.get_height()\n    splot.annotate(percentage,(x,y),ha = 'center', va = 'center')\nplt.xlabel('Voice mail plan?')\nplt.ylabel('Number of customers')\nplt.xticks(rotation=45)\nplt.show()","604069ad":"plt.figure(figsize=(5,5))\nsns.catplot(data=df[['number_customer_service_calls','churn']],x='churn',y='number_customer_service_calls',kind=\"box\",palette='GnBu')\nplt.title(\"Customer support and churn\")\nplt.ylabel(\"Number of customer support calls\")","a4b7703c":"df.head()","695c674c":"df['international_plan'] = np.where(df['international_plan'].str.contains('yes'), 1, 0)\ndf['voice_mail_plan'] = np.where(df['voice_mail_plan'].str.contains('yes'), 1, 0)\ndf['churn'] = np.where(df['churn'].str.contains('yes'), 1, 0)\n\n# Do the same for our test set\ntest['international_plan'] = np.where(test['international_plan'].str.contains('yes'), 1, 0)\ntest['voice_mail_plan'] = np.where(test['voice_mail_plan'].str.contains('yes'), 1, 0)","055be3c7":"df.head()","0758dbac":"test.head()","15072820":"# get X and y from the dataset\nX=df.drop('churn',axis=1)\ny = df[['churn']]\nX.head()","77d32113":"X=pd.get_dummies(X,columns=['state','area_code'])\ntest=pd.get_dummies(test,columns=['state','area_code'])","c3bf695f":"X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=41)","35184f14":"over=SMOTE(sampling_strategy = 0.2)\nunder=RandomUnderSampler(sampling_strategy=0.6)\nsteps=[('o',over),('u',under)]\npipeline=Pipeline(steps=steps)\nX_train,y_train=pipeline.fit_resample(X_train,y_train)","c314d409":"sns.countplot(x='churn',data=y_train,palette='YlGnBu')","547e88fa":"# define model\nmodel = LogisticRegression()\n# define evaluation\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n# define search space\nspace = dict()\nspace['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\nspace['penalty'] = ['none', 'l1', 'l2', 'elasticnet']\nspace['C'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]\n# define search\nsearch = GridSearchCV(model, space, scoring='accuracy', n_jobs=-1, cv=cv)\n# execute search\nresult = search.fit(X_train, y_train['churn'])","b94626e8":"# summarize result\nprint('Best Score: %s' % result.best_score_)\nprint('Best Hyperparameters: %s' % result.best_params_)","2cc6aa71":"# Accuracy\nprint(\"score of the LogisticRegression: \",result.score(X_test,y_test))","326c949b":"forest=RandomForestClassifier()\ngrid = dict()\ngrid['n_estimators'] = [4,5,6,7]\ngrid['max_features'] = ['auto', 'sqrt']\ngrid['min_samples_leaf'] = [3,4,5]\ngrid['criterion']=['gini','entropy']\nforest.fit(X_train,y_train['churn'])\ncv = KFold(n_splits=5,random_state=1,shuffle=True)\nsearch = GridSearchCV(forest, grid, scoring='accuracy', n_jobs=-1, cv=cv,verbose=1)\nbest_model = search.fit(X_train, y_train['churn'])","f4e6ead4":"# summarize result\nprint('Best Score: %s' % best_model.best_score_)\nprint('Best Hyperparameters: %s' % best_model.best_params_)","905e6f4a":"print(\"score of the Randomforest: \",best_model.score(X_test,y_test))","102f2070":"plot_confusion_matrix(best_model, \n                      X_test, \n                      y_test,\n                      values_format='d',\n                      cmap='inferno',\n                      display_labels=[\"Did not leave\", \"Left\"])","6f58da28":"print(classification_report(y_test,best_model.predict(X_test),target_names=['Did not leave','left']))","5d7abc95":"import xgboost as xgb","9bdd595f":"clf=xgb.XGBClassifier( \n        n_estimators=2000,\n        max_depth=12, \n        learning_rate=0.02, \n        subsample=0.8,\n        colsample_bytree=0.4, \n        missing=-1, \n        eval_metric='auc')\nclf_best = clf.fit(X_train, y_train, \n        eval_set=[(X_train, y_train)],\n        verbose=50, early_stopping_rounds=100)","828f9d19":"plot_confusion_matrix(clf_best, \n                      X_test, \n                      y_test,\n                      values_format='d',\n                      cmap='inferno',\n                      display_labels=[\"Did not leave\", \"left\"])","3d3173e8":"print(classification_report(y_test,clf_best.predict(X_test),target_names=['Did not leave','left']))","83e357aa":"import pickle\nfilename = open('churn_prediction_model.pkl','wb')\npickle.dump(clf_best,filename)","01c580af":"# Features used in training the model\ncols = X_test.columns\n# Use the model to make predictions\npredicted = clf_best.predict(test[cols])","69174ac3":"submission = pd.DataFrame({'id': test.id, 'churn': predicted})\n\n#Convert DataFrame to a csv file that can be uploaded\n#This is saved in the same directory as your notebook\nfilename = 'churn.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","d1bc59dd":"### XGBoost","d01bb659":"### About the dataset\n* state: string. 2-letter code of the US state of customer residence\n* account_length: numerical. Number of months the customer has been with the current telco provider\n* area_code: string=\"area_code_AAA\" where AAA = 3 digit area code.\n* international_plan: (yes\/no). The customer has international plan.\n* voice_mail_plan: (yes\/no). The customer has voice mail plan.\n* number_vmail_messages: numerical. Number of voice-mail messages.\n* total_day_minutes: numerical. Total minutes of day calls.\n* total_day_calls: numerical. Total number of day calls.\n* total_day_charg: numerical. Total charge of day calls.\n* total_eve_minutes: numerical. Total minutes of evening calls.\n* total_eve_calls: numerical. Total number of evening calls.\n* total_eve_charge: numerical. Total charge of evening calls.\n* total_night_minutes: numerical. Total minutes of night calls.\n* total_night_calls: numerical. Total number of night calls.\n* total_night_charge: numerical. Total charge of night calls.\n* total_intl_minutes: numerical. Total minutes of international calls.\n* total_intl_calls: numerical. Total number of international calls.\n* total_intl_charge: numerical. Total charge of international calls\n* number_customer_service_calls: numerical. Number of calls to customer service\n* churn: (yes\/no) Customer churn - target variable.","9675598b":"### SMOTE with  to handle imbalanced data set\nAs we have an imbalanced dataset with 86 percent data where customer has not churned, the model prediction might get biased. If a model only predicts that the customers have not churned, then the accuracy of the model would be 86%. To get the dataset balanced we have implemented SMOTE.\nWe first oversampled the minority class and then undersampled the majority class","6ee51108":"### Splitting the data set to train and test","a880fb17":"#### Let's have a look at the confusion matrix and the classification report.Even if the accuracy is 88%, the Precision and Recall for the positive class is not decent.","aa36b4c6":"#### Conclusion:\nIt seems that the customers who have not been assisted properly by the customer service agents for the resolution of issues have finally churned. More than 50% of the churned customers had called customer service at least twice and the maximum mumber of calls being 9.","f8f65154":"#### Conclusion:\nNew Jersey has the maximum percentage of customer churns (27%) followed by California (25%) and Washington(22%)\n","281809b0":"#### Conclusion:\nFrom the above box-plots we could see that the average 'day call charges' are more for the customers who churned.\\\nThis can be one of the significant reasons why most of the customers have left.\\\nFor rest of the call charges the behaviour is more or less same for customers who have churned and who have not.","12145009":"### Random forest classifier","07244f33":"#### We can see from the data that 14% of the population have churned.","e20b94c2":"### Which area code have the maximum customer churns?","3c5d242b":"### Result Submission","86dc757d":"#### Let's now check the confusion matrix and the classification report. Certainly there is a great improvement. The accuracy is now 99% and the Precision and Recall has improved drastically.","cb507970":"### Convert yes and no values to 1 and 0","701235f7":"### Does customer service calls have impact on churn?","251c61dc":"### Which locations have the maximum number of customer churns?","5d7ced2b":"#### From the above box plot we dont see any specific behavious of account_length for customers who churned and who did not.","18dde667":"### Logistic Regression","491b3774":"### One hot encoding to create dummy variables","42bdd06e":"#### Concluion:\nArea code 415 has the maximum customer churn of 6.8%","1963f8af":"### Save model"}}