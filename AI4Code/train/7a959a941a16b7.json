{"cell_type":{"9905c244":"code","395e0f23":"code","817a9e97":"code","e2dd12eb":"code","e0220c2e":"code","4841a189":"code","dc16a1a1":"code","d32441d5":"code","3e3e4024":"code","227693f7":"code","ea987ac3":"code","79370b86":"code","291d74e4":"code","8ee353ce":"code","aca28f83":"code","773c7d5a":"code","01e76408":"code","a01b8610":"code","b93302b3":"code","3266b735":"code","2c84eec1":"code","1eb72eb9":"code","5af5540a":"code","1ec77d1c":"code","86bb37b0":"code","7f14cbb7":"code","4049efe3":"code","54ae1e8e":"code","73f1a8cf":"code","91b23120":"code","d8ee974f":"code","7ab46f5b":"code","51959422":"code","59839d2a":"code","e7921002":"code","41396398":"code","d956af37":"code","59303582":"code","b885323d":"code","9f3256a3":"code","5978e23f":"code","8250f87b":"code","36ed9e56":"code","cf472323":"code","2f32aaa9":"code","0adccd16":"code","6ed37821":"code","ba541b43":"code","25348a28":"code","baa0c7ec":"code","cd829588":"code","3c80d386":"code","d07428af":"code","3f01645f":"code","bfda7d52":"code","a5291edf":"code","8e56d03a":"code","a00c9d27":"code","ff6123c9":"code","3753ddf3":"code","f14599fd":"code","3a4bddef":"code","d9ad0d36":"code","01d1a295":"code","b25b4044":"code","ab29aacc":"code","0257190f":"code","74f20d5a":"code","af935199":"code","8a86cc4d":"code","f3271db9":"code","4e6f620b":"code","94ef8a7a":"code","77f681c3":"code","9a528ee4":"markdown","ddcc29e8":"markdown","3cf4e1f5":"markdown","64cacae4":"markdown"},"source":{"9905c244":"# !pip install emoji","395e0f23":"import emoji\nimport numpy as np\nimport pandas as pd","817a9e97":"mapping = pd.read_csv(\"..\/input\/twitter-emoji-prediction\/Mapping.csv\")\noutput = pd.read_csv(\"..\/input\/twitter-emoji-prediction\/OutputFormat.csv\")\ntrain = pd.read_csv(\"..\/input\/twitter-emoji-prediction\/Train.csv\")\ntest = pd.read_csv(\"..\/input\/twitter-emoji-prediction\/Test.csv\")","e2dd12eb":"mapping","e0220c2e":"mapping = mapping.drop(\"Unnamed: 0\", axis=1)\nmapping.head()","4841a189":"mapping = mapping.set_index('number')","dc16a1a1":"mapping.head()","d32441d5":"mapping_dict = mapping.to_dict()\nmapping_dict = mapping_dict['emoticons']\nmapping_dict","3e3e4024":"train.head()","227693f7":"train.columns","ea987ac3":"train = train.drop('Unnamed: 0', axis=1)","79370b86":"train.tail()","291d74e4":"# changing the labels to emoji\n# train = train.replace({\"Label\": mapping_dict})","8ee353ce":"train.head()","aca28f83":"X_train = train['TEXT'].values\ny_train = train['Label'].values\n\nprint(X_train.shape, y_train.shape)","773c7d5a":"# maxLen = X_train.map(len).max()","01e76408":"# maxLen, type(maxLen)","a01b8610":"file = open(\"..\/input\/glove6b50dtxt\/glove.6B.50d.txt\", encoding = 'utf8')","b93302b3":"file","3266b735":"def intialize_emb_matrix(file):\n    embedding_matrix = {}\n    for line in file:\n        values = line.split()\n        word = values[0]\n        embedding = np.array(values[1:], dtype='float64')\n        embedding_matrix[word] = embedding\n\n    return embedding_matrix \n","2c84eec1":"embedding_matrix = intialize_emb_matrix(file)","1eb72eb9":"embedding_matrix['dance']","5af5540a":"embedding_matrix['dance'].shape ","1ec77d1c":"def get_emb_data(data, max_len):\n#     max_len = 168\n    embedding_data = np.zeros((len(data), max_len, 50))  # from glove6B50d\n    \n    for idx in range(data.shape[0]):\n        words_in_sentence = data[idx].split()\n        \n        for i in range(len(words_in_sentence)):\n            if embedding_matrix.get(words_in_sentence[i].lower()) is not None:\n                embedding_data[idx][i] = embedding_matrix[words_in_sentence[i].lower()]\n                \n    return embedding_data","86bb37b0":"# representing each word in the sentence acc. to glove embedding [str --> numerical]\nX_temb = get_emb_data(X_train, 168)","7f14cbb7":"X_temb","4049efe3":"X_temb.shape","54ae1e8e":"# X_temb = X_temb.reshape(-1, 20, 50)\n# X_temb.shape","73f1a8cf":"# convert outputs to categorical variables\nfrom keras.utils.np_utils import to_categorical","91b23120":"# converting y_train to one hot vectors so that cross-entropy loss can be used.\ny_train = to_categorical(y_train)","d8ee974f":"y_train","7ab46f5b":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, SimpleRNN, Dense, Dropout","51959422":"model = Sequential()","59839d2a":"lm = len(mapping_dict)\nlm","e7921002":"model.add(LSTM(units = 256, return_sequences=True, input_shape = (168,50)))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(units=128))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(units=128, activation='relu'))\nmodel.add(Dense(units=64, activation='relu'))\nmodel.add(Dense(units=32, activation='relu'))\nmodel.add(Dense(units=20, activation='relu'))\nmodel.add(Dense(units=20, activation='softmax'))","41396398":"model.summary()","d956af37":"model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['acc'])","59303582":"# model Training\nres = model.fit(X_temb, y_train, validation_split=0.2, batch_size=32, epochs=10, verbose=2)","b885323d":"train_data = pd.read_csv(\"..\/input\/emojify\/train_emoji.csv\", header=None)\ntest_data = pd.read_csv(\"..\/input\/emojify\/test_emoji.csv\", header=None)","9f3256a3":"train_data.head()","5978e23f":"test_data.head()","8250f87b":"train_data.drop(labels=[2, 3], axis=1, inplace=True)","36ed9e56":"train_data.tail()","cf472323":"emoji.EMOJI_UNICODE['en']","2f32aaa9":"emoji.emojize(':angry_face:')","0adccd16":"emoji_mapping = {\n    '0': ':beating_heart:',\n    '1': ':baseball:',\n    '2': ':beaming_face_with_smiling_eyes:',\n    '3': ':angry_face:',\n    '4': ':face_savoring_food:'\n}","6ed37821":"for key, value in emoji_mapping.items():\n    emoji_mapping[key] = emoji.emojize(value)\n    \nprint(emoji_mapping)\n    ","ba541b43":"X_train = train_data[0].values\ny_train = train_data[1].values","25348a28":"# def intialize_emb_matrix(file):\n#     embedding_matrix = {}\n#     for line in file:\n#         values = line.split()\n#         word = values[0]\n#         embedding = np.array(values[1:], dtype='float64')\n#         embedding_matrix[word] = embedding\n\n#     return embedding_matrix \n\nfile","baa0c7ec":"# we have already created an embedding matrix using the golve vectors\nembedding_matrix","cd829588":"X_train = get_emb_data(X_train, 10)","3c80d386":"X_train","d07428af":"y_train = to_categorical(y_train)\ny_train","3f01645f":"model_tte = Sequential()\nmodel_tte.add(LSTM(units=64, input_shape=(10, 50), return_sequences=True))\nmodel_tte.add(Dropout(0.3))\nmodel_tte.add(LSTM(units=32))\nmodel_tte.add(Dropout(0.2))\nmodel_tte.add(Dense(units=10, activation='relu'))\nmodel_tte.add(Dense(units=5, activation='softmax'))","bfda7d52":"model_tte.summary()","a5291edf":"model_tte.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['acc'])","8e56d03a":"history = model_tte.fit(X_train, y_train, validation_split=0.1, verbose=2, batch_size=32, epochs=100)","a00c9d27":"model_tte2 = Sequential()\nmodel_tte2.add(LSTM(units=128, input_shape=(10, 50), return_sequences=True))\nmodel_tte2.add(Dropout(0.3))\nmodel_tte2.add(LSTM(units=64))\nmodel_tte2.add(Dropout(0.2))\nmodel_tte2.add(Dense(units=32, activation='relu'))\nmodel_tte2.add(Dense(units=20, activation='relu'))\nmodel_tte2.add(Dense(units=5, activation='relu'))","ff6123c9":"model_tte2.summary()","3753ddf3":"model_tte2.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['acc'])","f14599fd":"history2 = model_tte2.fit(X_train, y_train, validation_split=0.1, verbose=2, batch_size=32, epochs=100)","3a4bddef":"model_tte3 = Sequential()\nmodel_tte3.add(LSTM(units=128, input_shape=(10, 50), return_sequences=True))\nmodel_tte3.add(Dropout(0.3))\nmodel_tte3.add(LSTM(units=64))\nmodel_tte3.add(Dropout(0.2))\nmodel_tte3.add(Dense(units=32, activation='relu'))\nmodel_tte3.add(Dense(units=20, activation='relu'))\nmodel_tte3.add(Dense(units=5, activation='softmax'))","d9ad0d36":"model_tte3.summary()","01d1a295":"model_tte3.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['acc'])","b25b4044":"history3 = model_tte3.fit(X_train, y_train, validation_split=0.1, verbose=2, batch_size=32, epochs=100)","ab29aacc":"test_data = pd.read_csv(\"..\/input\/emojify\/test_emoji.csv\", header=None)","0257190f":"test_data.head()","74f20d5a":"# preprocessing test data\ntest_data[0] = test_data[0].apply(lambda x:x[:-1])\nX_test = test_data[0].values\ny_test = test_data[1].values","af935199":"X_test = get_emb_data(X_test, 10)\ny_test = to_categorical(y_test)","8a86cc4d":"model_tte3.evaluate(X_test, y_test)[1]*100","f3271db9":"predicted = model_tte3.predict(X_test)","4e6f620b":"classes = np.argmax(predicted,axis=1)\n","94ef8a7a":"classes","77f681c3":"for t in range(len(test)):\n    print(test_data[0].iloc[t])\n    print(\"Predictions: \", emoji.emojize(emoji_mapping[str(classes[t])]))\n    print('Actual: ', emoji.emojize(emoji_mapping[str(test_data[1].iloc[t])]))","9a528ee4":"## Text Preprocessing using Embeddings","ddcc29e8":"# Emoji Prediction using Transfer Learning","3cf4e1f5":"## Using Another dataset for Text to emoji prediction","64cacae4":"## Model Building"}}