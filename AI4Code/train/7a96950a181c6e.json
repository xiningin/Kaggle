{"cell_type":{"4b42cd9b":"code","074a2a57":"code","9dfa2f43":"code","09e4eeed":"code","6939bb10":"code","779556ae":"markdown","8b23959c":"markdown","c8193a38":"markdown","37f42111":"markdown","7b075549":"markdown","687cf042":"markdown","89fb132d":"markdown","749fe954":"markdown","ad454033":"markdown"},"source":{"4b42cd9b":"# import the Azure ML libs.\n\n!pip install azureml\n!pip install azureml.core\n!pip install azureml.widgets","074a2a57":"import azureml.core\nimport azureml.widgets \nprint(\"Ready to use Azure ML\", azureml.core.VERSION)","9dfa2f43":"from azureml.core import Workspace\n\n## in this segment you should replace the 3-parameters values according to the workspace available in the subscription\n## this experiment will not work beyond this point if these values are not appropriately inserted.\n## HENCE, THE Notebook Execution will terminate\n\n\n## Example - \n    ## ws = Workspace.get(name=\"<<MLSERVICENAME>>\", subscription_id='<<GUID - ML Service ID>>', resource_group='<<Hosting Azure Resource Group>>')\n\n# Pulling values from Kaggle Secrets\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nMLServiceName = user_secrets.get_secret(\"MLServiceName\")\naz_resource_grp = user_secrets.get_secret(\"az_resource_grp\")\nsub_id = user_secrets.get_secret(\"sub_id\")\n\n## Instanciating the Workspace object.\nws = Workspace.get(name=MLServiceName, subscription_id=sub_id, resource_group=az_resource_grp)\nprint(ws.name, \"loaded\")","09e4eeed":"from azureml.core import Experiment\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\n# Create an Azure ML experiment in your workspace\nexperiment = Experiment(workspace = ws, name = \"simple-experiment\")\n\n# Start logging data from the experiment\nrun = experiment.start_logging()\nprint(\"Starting experiment:\", experiment.name)\n\n# load the data from a local file\ndata = pd.read_csv('..\/input\/iris-flower-dataset\/IRIS.csv')\n\n# Count the rows and log the result\nrow_count = (len(data))\n\n# IMPORTANT: Log statistical value from this Experiment's run \nrun.log('observations', row_count)\nprint('Analyzing {} rows of data'.format(row_count))\n\n# IMPORTANT: Log plot image (log_image()) from this Experiment's run \niris_counts = data['species'].value_counts()\nfig = plt.figure(figsize=(6,6))\nax = fig.gca()    \niris_counts.plot.bar(ax = ax) \nax.set_title('Count of Iris Species') \nax.set_xlabel('Species') \nax.set_ylabel('Instance Count')\nplt.show()\nrun.log_image(name = 'label distribution', plot = fig)\n\n# IMPORTANT: log distinct counts as a list using log_list from the Experiment's run\nspecies = data['species'].unique()\nrun.log_list('species categories', species)\n\n# IMPORTANT: log summary statistics as a row using log_row from the Experiment's run\nfeatures_columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\nsummary_stats = data[features_columns].describe().to_dict()\nfor col in summary_stats:\n    keys = list(summary_stats[col].keys())\n    values = list(summary_stats[col].values())\n    for index in range(len(keys)):\n        run.log_row(col, stat = keys[index], value = values[index])\n        \n# Save a sample of the data and upload it to the experiment output\ndata.sample(100).to_csv('..\/sample.csv', index=False, header=True)\nrun.upload_file(name = 'outputs\/sample.csv', path_or_stream = '..\/sample.csv')\n\n# Complete the run\nrun.complete()","6939bb10":"import json\n\n# Get run details\ndetails = run.get_details()\nprint(details)\n\n# Get logged metrics\nmetrics = run.get_metrics()\nprint(json.dumps(metrics, indent=2))\n\n# Get output files\nfiles = run.get_file_names()\nprint(json.dumps(files, indent=2))","779556ae":"## View Experiment Results\n\nAfter the experiment has been finished, you can use the **run** object to get information about the run and its outputs:","8b23959c":"\n# Azure Machine Learning Series - I\nThis will be series of notebook tutorials starting from the beginning of the fundamentals of AML service.\n\n## Introduction\nAzure Machine Learning (*Azure ML*) is a cloud-based service for creating and managing machine learning solutions. It's designed to help data scientists leverage their existing data processing and model development skills and frameworks, and help them scale their workloads to the cloud. The Azure ML SDK for Python provides classes you can use to work with Azure ML in your Azure subscription.\n<br\/>**Heads-Up:** https:\/\/channel9.msdn.com\/Shows\/AI-Show\/Allup-Azure-ML\n\nIn this <u>first notebook<\/u> I will discuss about the nitty-gritty of Azure ML service: -\n* Creating an instance of Azure ML service\n* Downloading libraries\/dependencies (in Kaggle environment)\n* Various methods to connect to Azure ML service workspace: using config file and using get() method.\n* Simple ML experiment -  for data exploration. Capture the details from the experiment, logging and preserving the run details from the experiment. \n* Overview of Azure ML Service Dashboard & Experiment Dashboard.\n\n### Check the Azure ML SDK Version\nLet's start by importing the **azureml-core** package and checking the version of the SDK that is installed.\n","c8193a38":"### Digging into individual experiment's run.\ngives us the charts, attributes, tracked metrics (as described in the code), and tracked images\/charts from the run.\n![image.png](attachment:image.png)","37f42111":"## ML Experiment\n\nI am going to use an Azure ML *experiment* to run Python code and record values extracted from data. In this case, i will use a simple dataset the famous IRIS. I will run an experiment to explore the data, extracting statistics, visualizations, and data samples. In this experiment I will also show how we can use an Azure ML *experiment* to log details of the each run.","7b075549":"### Outputs\nas per the details captured in the code - \nOutput file (100 records) and Lebel Distribution Image\n\n\n![image.png](attachment:image.png)","687cf042":"## Connect to Your Workspace\nAll experiments and associated resources are managed within you Azure ML workspace. You can connect to an existing workspace, or create a new one using the Azure ML SDK.\n\n### Workspace Configuration\n> IDEALLY - In most cases, you should store the workspace configuration in a JSON configuration file. This makes it easier to reconnect without needing to remember details like your Azure subscription ID. You can download the JSON configuration file from the blade for your workspace in the Azure portal, but if you're using a Compute Instance within your workspace, the configuration file has already downloaded to the root folder.\n\n**In this example**: I am going to use *Workspace.get()* method to instantiate the previously created workspace.\n\nThe first time for this notebook session,  it will be prompt to sign into Azure by clicking the https:\/\/microsoft.com\/devicelogin link,  entering an automatically generated code, and signing into Azure.\n\n![image.png](attachment:image.png)\n","89fb132d":"### What Next - \nIn the next notebook I will explain about the details of:\n\n* Create and run the experiment using Custom Script file. Implementing simple Logistic Regression model on IRIS dataset.\n* **RunConfiguration** and **ScriptRunConfiguration**, using these AML classes to define the runtime environment for the custom script.\n* Output the model and run details to the external folder for future referencing.","749fe954":"** IMPORTANT NOTE - **\n> Please proceed with this Notebook <u>**_iff_**<\/u> you are familiar with foundation of Microsoft Azure public cloud. \n> In this notebook, the basics of Microsoft Azure and its development methodology is not covered. As it is beyond the scope of this notebook.\n\nFor those who would like to provision and understand further about Azure Machine Learning Service (AML)  - Here is the landing page on [MS Azure ML Service](https:\/\/azure.microsoft.com\/en-in\/services\/machine-learning\/)\n","ad454033":"## Azure ML workspace\nThe service is battleground for every data scientist, as it enable ML engineer to perform every aspect of Machine learning project - \n1. Curate Data, Datastores, Datasets and processing Pipelines\n2. Provision Compute - CPU\/GPU\/TPU any many more\n3. Run and track experiments.\n4. Model provsioning and source control, versioning, etc.. \n5. Model Deployments - various formats,various platforms\n6. Monitoring, troubleshootind, logging\n7. Storage and data analysis.\n\nto learn further on the introduction  pl look into this url  - \nhttps:\/\/channel9.msdn.com\/Shows\/AI-Show\/Allup-Azure-ML\n\n## ML Experiment dashboard \nFollowing are the noteworthy points from this dashboard:\n1. Name of the experiment is \"simple-experiment\"\n2. Gives the details of each run of this experiment. \n![image.png](attachment:image.png)\n\n"}}