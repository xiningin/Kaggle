{"cell_type":{"c2961a16":"code","29673a78":"code","256c4080":"code","02929851":"code","fb8451e4":"code","0da3d3c5":"code","78abc61a":"code","d1566aba":"markdown","ee3943f2":"markdown","57a89aad":"markdown","e323f4c4":"markdown","8a238e28":"markdown","7d18716e":"markdown"},"source":{"c2961a16":"import numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nfrom sklearn import preprocessing\nimport seaborn as sns\nfrom sklearn.model_selection import KFold, TimeSeriesSplit\nimport joblib\nimport pickle\nimport lightgbm as lgb\nimport gc\nimport os\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom tqdm import tqdm\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader,Dataset\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport random\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything(seed=45)\n\ndevice = ('cuda' if torch.cuda.is_available() else 'cpu')","29673a78":"def additional_features(df):\n\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['hour'] = df['date'].dt.hour\n    df['day'] = df['date'].dt.day\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['week'] = df['date'].dt.week\n\n    df['month_dummy'] = 0\n    df.loc[df['month'] > 10, 'month_dummy'] = 1\n    df.loc[df['month'] < 3, 'month_dummy'] = 1\n\n    df['hour_dummy'] = 0\n    df.loc[(df['month_dummy'] == 1) & (df['hour'] > 19) & (df['hour'] < 23), 'hour_dummy'] = 1\n    df.loc[(df['month_dummy'] == 1) & (df['hour'] > 5) & (df['hour'] < 9), 'hour_dummy'] = 1\n\n    df['day_dummy'] = 0\n    df.loc[(df['hour'] > 9) & (df['hour'] < 19), 'day_dummy'] = 1\n\n    return df\n\ndef get_data(ROOT='data'):\n\n    CAL_DTYPES = {\"latitude\": \"float64\", \"longitude\": \"float64\", \"type\": \"category\",\n                  \"source\": \"category\", \"station\": \"category\", 'aqi': 'float64'}\n\n    train = pd.read_csv(ROOT + \"\/pm_train.csv\", dtype=CAL_DTYPES)\n    test = pd.read_csv(ROOT + \"\/pm_test.csv\", dtype=CAL_DTYPES)\n    weather = pd.read_csv(ROOT + \"\/weather.csv\")\n    sub = pd.read_csv(ROOT + \"\/sample_submission.csv\")\n\n    return train, test, weather, sub","256c4080":"def preprocess_rnn(ROOT='data', cat_feat=[], rolling_means=[], shifts=[], diffs=[]):\n\n    rnn_cols = ['temperature', 'apparentTemperature', 'dewPoint', 'humidity',\n                'windSpeed', 'windBearing', 'cloudCover', 'uvIndex', 'visibility']\n\n    cat_feat = [\"type\", \"source\", \"station\"]\n\n    train, test, weather, sub = get_data(ROOT=ROOT)\n\n    df = pd.concat([train, test], axis=0)\n    df['date'] = pd.to_datetime(df['date'])\n    weather['date'] = pd.to_datetime(weather['date'])\n    \n    lower_max =  weather.loc[weather['Unnamed: 0'] < 31058, 'visibility'].max()\n    \n    weather['visibility'] = np.where(weather['visibility'].values>lower_max, lower_max, weather['visibility'].values)\n    weather['visibility'].hist()\n    plt.show()\n    \n    weather = weather.sort_values(by='date')\n\n    weather_df = pd.DataFrame()\n    weather_df['date'] = pd.date_range(start=weather.iloc[0]['date'], end=weather.iloc[-1]['date'], freq='H')\n    weather_df = pd.merge(weather_df, weather.iloc[:, 1:], on=['date'], how='left')\n\n    del weather\n\n    weather_df = additional_features(weather_df)\n\n    for col in cat_feat:\n        le = preprocessing.LabelEncoder()\n        df[col] = le.fit_transform(df[col])\n\n    for col in ['summary', 'icon', \"year\", \"dayofyear\", \"hour\"]:\n        weather_df[col] = weather_df[col].fillna(method=\"ffill\")\n        weather_df[col] = weather_df[col].fillna(method=\"bfill\")\n        le = preprocessing.LabelEncoder()\n        weather_df[col] = le.fit_transform(weather_df[col])\n\n    for col in rnn_cols:\n        weather_df[col] = weather_df[col].fillna(weather_df[col].rolling(4, min_periods=1).mean())\n        weather_df[col] = weather_df[col].fillna(weather_df.groupby([\"dayofyear\", \"hour\"])[col].transform('mean'))\n\n    for col in rnn_cols:\n        for lag in rolling_means:\n            weather_df['%s_rolling_%s_mean' % (col, lag)] = weather_df[col].rolling(lag).mean()\n\n    # for col in weather_df.drop(['date', 'summary', 'icon', 'dayofyear',\n    #                             'year'], axis=1).columns:\n    #     for lag in shifts:\n    #         weather_df['%s_shift_%s' % (col, lag)] = weather_df[col].shift(lag)\n\n#     for col in rnn_cols:\n#         for lag in diffs:\n#             weather_df['%s_diff_%s' % (col, lag)] = weather_df[col].diff(lag)\n\n    #weather_df = weather_df.dropna()\n    df_full = pd.DataFrame()\n    for stat in df['station'].unique():\n        for type in df['type'].unique():\n            temp = df.loc[(df['station']==stat) & (df['type']==type)]\n            if temp.shape[0]>0:\n                temp = pd.merge(weather_df, temp, on=[\"date\"], how='left')\n                print(temp.shape)\n                temp[['latitude', 'longitude', 'station', 'source', 'type']]=temp[\n                    ['latitude', 'longitude', 'station', 'source', 'type']].fillna(method='ffill')\n                temp[['latitude', 'longitude', 'station', 'source', 'type']] = temp[\n                    ['latitude', 'longitude', 'station', 'source', 'type']].fillna(method='bfill')\n                df_full = pd.concat([df_full, temp], axis=0)\n\n#     icols = [['type', 'station', 'month', 'hour'],\n#             ['type', 'station', 'week', 'hour'],\n#             ['type', 'station', 'dayofyear', 'hour'],\n#              ['type', 'station', 'icon', 'month', 'hour'],\n#              ['type', 'station', 'summary', 'month', 'hour']]\n\n#     temp_df = df_full.copy()\n#     temp_df = temp_df.dropna()\n#     col_fill = '_' + '_'.join(icols[0]) + '_'\n#     for col in icols:\n#         col_name = '_' + '_'.join(col) + '_'\n#         temp = temp_df[col + ['aqi']].groupby(col, as_index=False)['aqi'].mean()\n#         temp = temp.rename(columns={'aqi': 'enc%smean' % col_name})\n#         df_full = df_full.merge(temp, on=col, copy=False, how='left')\n#         df_full['enc%smean' % col_name] = df_full['enc%smean' % col_name]\n#         df_full['enc%smean' % col_name] = df_full['enc%smean' % col_name].fillna(method='ffill')\n#         df_full['enc%smean' % col_name] = df_full['enc%smean' % col_name].fillna(df_full['enc%smean' % col_fill])\n\n    return df_full, weather_df, sub","02929851":"def sliding_windows(data, seq_length=23):\n    x = []\n    for i in range(len(data)-seq_length-1):\n        _x = data[i:(i+seq_length+1)]\n        x.append(_x)\n    return np.array(x)\n\ndef normalization(df):\n    if len(df.shape)==2:\n        scaler = MinMaxScaler().fit(df.values)\n        X = scaler.transform(df.values)\n    else:\n        scaler = MinMaxScaler().fit(np.expand_dims(df.values, axis=1))\n        X = scaler.transform(np.expand_dims(df.values, axis=1))\n\n    return X, scaler\n\n\ndef make3Dinput(df_full, rnn_cols, cat_feats, seq_len = 8, test=False):\n\n    if test==True:\n        rnns = []\n        cats = {}\n        output=[]\n        k=0\n        for typ in (df_full['type'].unique()):\n            for stat in (df_full['station'].unique()):\n                df_temp = df_full.loc[(df_full['type'] == typ) & (df_full['station'] == stat)]\n                if df_temp.shape[0]>0:\n                    temp_target = df_full.loc[(df_full['type'] == typ) & (df_full['station'] == stat), 'ID'].to_numpy()[seq_len:]\n                    output.append(temp_target[~np.isnan(temp_target)])\n                    rnns.append(sliding_windows(df_temp[rnn_cols].values, seq_length=seq_len - 1)[~np.isnan(temp_target)])\n                    for v in cat_feats:\n                        if k == 0:\n                            cats[v] = list(df_temp[[v]].to_numpy()[seq_len:][~np.isnan(temp_target)])\n                        else:\n                            cats[v] += list(df_temp[[v]].to_numpy()[seq_len:][~np.isnan(temp_target)])\n                    k = k + 1\n\n        output = np.concatenate((output), axis=0)\n        rnns = np.concatenate((rnns), axis=0)\n        inputX = {}\n        inputX['rnn'] = rnns\n        for i, v in enumerate(cat_feats):\n            inputX[v] = np.concatenate((cats[v]), axis=0)\n\n    else:\n        output = []\n        rnns = []\n        cats = {}\n        k = 0\n        for typ in (df_full['type'].unique()):\n            for stat in (df_full['station'].unique()):\n                df_temp = df_full.loc[(df_full['type'] == typ) & (df_full['station'] == stat)]\n                if df_temp.shape[0] > 0:\n                    temp_target = df_full.loc[(df_full['type'] == typ) & (df_full['station'] == stat), 'aqi'].to_numpy()[seq_len:]\n                    output.append(temp_target[~np.isnan(temp_target)])\n                    rnns.append(sliding_windows(df_temp[rnn_cols].values, seq_length=seq_len - 1)[~np.isnan(temp_target)])\n                    for v in cat_feats:\n                        if k == 0:\n                            cats[v] = list(df_temp[[v]].to_numpy()[seq_len:][~np.isnan(temp_target)])\n                        else:\n                            cats[v] += list(df_temp[[v]].to_numpy()[seq_len:][~np.isnan(temp_target)])\n\n                    k = k + 1\n\n        output = np.concatenate((output), axis=0)\n        rnns = np.concatenate((rnns), axis=0)\n        inputX = {}\n        inputX['rnn'] = rnns\n        for i, v in enumerate(cat_feats):\n            inputX[v] = np.concatenate((cats[v]), axis=0)\n\n    return inputX, output\n\n\nclass MAPLoader:\n\n    def __init__(self, X, y, shuffle=True, batch_size=1000, cat_cols=[]):\n        self.X_cont = X[\"rnn\"]\n        try:\n            self.X_cat = np.concatenate([X[k] for k in cat_cols], axis=1)\n        except:\n            self.X_cat = np.concatenate([np.expand_dims(X[k], axis=1) for k in cat_cols], axis=1)\n        self.y = y\n\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.n_conts = self.X_cont.shape[1]\n        self.len = self.X_cont.shape[0]\n        n_batches, remainder = divmod(self.len, self.batch_size)\n\n        if remainder > 0:\n            n_batches += 1\n        self.n_batches = n_batches\n        self.remainder = remainder  # for debugging\n\n        self.idxes = np.array([i for i in range(self.len)])\n\n    def __iter__(self):\n        self.i = 0\n        if self.shuffle:\n            ridxes = self.idxes\n            np.random.shuffle(ridxes)\n            self.X_cat = self.X_cat[[ridxes]]\n            self.X_cont = self.X_cont[[ridxes]]\n            if self.y is not None:\n                self.y = self.y[[ridxes]]\n\n        return self\n\n    def __next__(self):\n        if self.i >= self.len:\n            raise StopIteration\n\n        if self.y is not None:\n            y = torch.FloatTensor(self.y[self.i:self.i + self.batch_size].astype(np.float32))\n\n        else:\n            y = None\n\n        xcont = torch.FloatTensor(self.X_cont[self.i:self.i + self.batch_size])\n        xcat = torch.LongTensor(self.X_cat[self.i:self.i + self.batch_size])\n\n        batch = (xcont, xcat, y)\n        self.i += self.batch_size\n        return batch\n\n    def __len__(self):\n        return self.n_batches\n\n\nclass RMSE(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mse = nn.MSELoss()\n\n    def forward(self, y_pred, y_true):\n        y_pred = y_pred.squeeze()\n        return torch.sqrt(self.mse(y_pred, y_true))\n\n\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False, delta=0):\n\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n\n    def __call__(self, val_loss, model, path):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model, path)\n        elif score < self.best_score - self.delta:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model, path)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model, path):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), path)\n        self.val_loss_min = val_loss","fb8451e4":"def init_weights(m):\n    if type(m) == nn.Linear:\n        torch.nn.init.xavier_uniform(m.weight)\n        m.bias.data.fill_(0.01)\n\n####### LSTM model for 3D input ############################\n\nclass LSTM_MAPP(nn.Module):\n\n    def __init__(self, emb_dims, input_rnn_size, hidden_size, num_layers, seq_length,\n                 device=device):\n        super(LSTM_MAPP, self).__init__()\n\n        self.num_layers = num_layers\n        self.input_rnn_size = input_rnn_size\n        self.hidden_size = hidden_size\n        self.seq_length = seq_length\n        self.device = device\n\n        # Embedding layers\n        self.emb_layers = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims])\n        n_embs = sum([y for x, y in emb_dims])\n\n        self.lstm = nn.LSTM(input_size=input_rnn_size, hidden_size=hidden_size,\n                            num_layers=num_layers, batch_first=True)\n\n        \n        inp_dim = n_embs + hidden_size\n        self.models = torch.nn.ModuleList()\n\n        self.fc0 = nn.Linear(inp_dim, 512)\n        self.drop = nn.Dropout(0.25)\n        self.fc1 = nn.Linear(512, 1)\n        \n        self.lstm.apply(init_weights)\n        self.fc0.apply(init_weights)\n        self.fc1.apply(init_weights)\n\n    def encode_data(self, cat_data):\n        xcat = [el(cat_data[:, k]) for k, el in enumerate(self.emb_layers)]\n        xcat = torch.cat(xcat, 1)\n        return xcat\n\n    def forward(self, cont_x, cat_x):\n        cont_x = cont_x.to(self.device)\n        cat_x = cat_x.to(self.device)\n\n        h_0 = Variable(torch.zeros(\n            self.num_layers, cont_x.size(0), self.hidden_size)).to(device)\n\n        c_0 = Variable(torch.zeros(\n            self.num_layers, cont_x.size(0), self.hidden_size)).to(device)\n\n        # Propagate input through LSTM\n        lstm_out, _ = self.lstm(cont_x, (h_0, c_0))\n        cat_out = self.encode_data(cat_x)\n        inp = torch.cat((lstm_out[:, -1, :], cat_out), dim=1)\n        out = F.relu(self.fc0(inp))\n        out = self.drop(out)\n        out = self.fc1(out)\n\n        return out","0da3d3c5":"\ndef training_lstm(df, sub_nn, target, dt_start, cont_cols, cat_cols,\n                Nfolds, seq_len=8, epoch=50, patience=5,\n                num_layers=1, MODEL_ROOT='models\/lstm',\n                hidden_dim=512):\n\n    uniques = {}\n\n    for i, v in enumerate((cat_cols)):\n        le = LabelEncoder()\n        le.fit(df[v].dropna())\n        df[v] = le.transform(df[v].fillna(0))\n        uniques[v] = (len(df[v].dropna().unique()))\n\n    if not os.path.exists(MODEL_ROOT):\n        os.makedirs(MODEL_ROOT)\n\n    kf = KFold(n_splits=Nfolds, shuffle=False)\n    fold_ = 0\n    scores = []\n    scores_fold = []\n\n    for dt in dt_start:\n        train = df.loc[df['date'] <= '2018-10-31']\n        val = df.loc[(df['date'] > dt)]\n        test = df.loc[df['date'] > '2018-10-31']\n\n        test[cont_cols+cat_cols] = test[cont_cols+cat_cols].fillna(0)\n        train[cont_cols + cat_cols] = train[cont_cols + cat_cols].fillna(0)\n        val[cont_cols + cat_cols] = val[cont_cols + cat_cols].fillna(0)\n        train = train.reset_index(drop=True)\n        val = val.reset_index(drop=True)\n        test = test.reset_index(drop=True)\n\n        train[cont_cols], scaler_x = normalization(train[cont_cols])\n        test[cont_cols] = scaler_x.transform(test[cont_cols].values)\n        val[cont_cols] = scaler_x.transform(val[cont_cols].values)\n\n        train_y, scaler_y = normalization(train.loc[~train[target].isnull(), target])\n        train.loc[~train[target].isnull(), target] = train_y\n        val_y = scaler_y.transform(val.loc[~val[target].isnull(), [target]])\n        val.loc[~val[target].isnull(), target] = val_y\n        test_y = scaler_y.transform(test.loc[~test[target].isnull(), [target]])\n        test.loc[~test[target].isnull(), target] = test_y\n\n        trainX, trainY = make3Dinput(df_full=train,\n                                       rnn_cols=cont_cols, cat_feats=cat_cols,\n                                       seq_len=seq_len, test=False)\n\n        validx, validy = make3Dinput(df_full=val,\n                                     rnn_cols=cont_cols, cat_feats=cat_cols,\n                                     seq_len=seq_len, test=False)\n\n        validx1, validy1 = make3Dinput(df_full=test,\n                                   rnn_cols=cont_cols, cat_feats=cat_cols,\n                                   seq_len=seq_len, test=False)\n\n        testx, testy = make3Dinput(df_full=test,\n                               rnn_cols=cont_cols, cat_feats=cat_cols,\n                               seq_len=seq_len, test=True)\n\n        test_loader = MAPLoader(testx, testy, cat_cols=cat_cols, batch_size=1024, shuffle=False)\n\n\n        model_path = MODEL_ROOT + '\/model_lstm_%s_%s.pt' % (hidden_dim, fold_)\n\n        X_train, y_train = trainX, trainY\n\n        train_loader = MAPLoader(X_train, y_train, cat_cols=cat_cols, batch_size=1024, shuffle=True)\n        val_loader = MAPLoader(validx, validy, cat_cols=cat_cols, batch_size=512, shuffle=False)\n        val_loader1 = MAPLoader(validx1, validy1, cat_cols=cat_cols, batch_size=256, shuffle=False)\n\n        # cat_feats = ['summary', 'icon', 'dayofyear', 'hour', 'year', 'month', 'day',\n        #              'dayofweek', 'station', 'type', 'source', 'week']\n        dims = [1, 1, 15, 4, 1, 3, 5, 2, 3, 1, 1, 5]\n        emb_dims = [(uniques[col], y) for col, y in zip(cat_cols, dims)]\n\n        n_cont = len(cont_cols)\n\n\n        model = LSTM_MAPP(emb_dims=emb_dims, input_rnn_size=n_cont,\n                          hidden_size=hidden_dim, num_layers=num_layers, seq_length=seq_len).to(device)\n\n        criterion = RMSE()\n\n        torch.manual_seed(42)\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3,\n                                                  max_lr=1e-2, epochs=epoch, steps_per_epoch=len(train_loader))\n        #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', patience=1, verbose=True, factor=0.25)\n\n\n        best_rmse=np.inf\n        best_model=None\n        counter=0\n        for ep in range(epoch):\n            train_loss, val_loss = 0, 0\n\n            model.train()\n            for i, (X_cont, X_cat, y) in enumerate(train_loader):\n\n                optimizer.zero_grad()\n\n                out = model(X_cont, X_cat)\n\n                loss = criterion(out, y.to(device))\n\n                loss.backward()\n                optimizer.step()\n                scheduler.step()\n\n                with torch.no_grad():\n                    train_loss += loss.item() \/ len(train_loader)\n\n            # Validation phase\n            phase='Val'\n            with torch.no_grad():\n                model.eval()\n\n                y_true = []\n                y_pred = []\n                rloss = 0\n\n                for i, (X_cont, X_cat, y) in enumerate(val_loader1):\n                    out = model(X_cont, X_cat)\n\n                    loss = criterion(out, y.to(device))\n\n                    rloss += loss.item() \/ len(val_loader1)\n                    y_pred += list(out.detach().cpu().numpy().flatten())\n                    y_true += list(y.cpu().numpy())\n\n                rmse = np.sqrt(mean_squared_error(scaler_y.inverse_transform(np.expand_dims(y_true, axis=1)),\n                                                  scaler_y.inverse_transform(np.expand_dims(y_pred, axis=1))))\n                if best_rmse>rmse:\n                    best_rmse=rmse\n                    best_model=model\n                    torch.save(best_model, model_path)\n                    counter = 0\n                else:\n                    counter = counter+1\n\n                y_true = []\n                y_pred = []\n                for i, (X_cont, X_cat, y) in enumerate(val_loader):\n                    out = model(X_cont, X_cat)\n\n                    y_pred += list(out.detach().cpu().numpy().flatten())\n                    y_true += list(y.cpu().numpy())\n\n                rmse_fold = np.sqrt(mean_squared_error(scaler_y.inverse_transform(np.expand_dims(y_true, axis=1)),\n                                                       scaler_y.inverse_transform(np.expand_dims(y_pred, axis=1))))\n\n                print(f\"[{phase}] Epoch: {ep} | Tain loss: {train_loss:.4f} | Val Loss: {rloss:.4f} | RMSE: {rmse:.4f} | RMSE fold: {rmse_fold:.4f}\")\n\n                # plt.plot(y_true)\n                # plt.plot(y_pred)\n                # plt.show()\n                #scheduler.step(rmse)\n\n            if counter>=patience:\n                print(\"Early stopping\")\n                break\n\n        fold_ = fold_ + 1\n\n        final_model = torch.load(model_path)\n\n        with torch.no_grad():\n            final_model.eval()\n\n            y_true = []\n            y_pred = []\n\n            for i, (X_cont, X_cat, y) in enumerate(val_loader1):\n                out = final_model(X_cont, X_cat)\n                y_pred += list(out.detach().cpu().numpy().flatten())\n                y_true += list(y.cpu().numpy())\n\n            rmse = np.sqrt(mean_squared_error(scaler_y.inverse_transform(np.expand_dims(y_true, axis=1)),\n                                              scaler_y.inverse_transform(np.expand_dims(y_pred, axis=1))))\n            #rmse = np.sqrt(mean_squared_error(np.asarray(y_true), np.asarray(y_pred)))\n\n            scores.append(rmse)\n            y_true = []\n            y_pred = []\n            for i, (X_cont, X_cat, y) in enumerate(val_loader):\n                out = final_model(X_cont, X_cat)\n\n                y_pred += list(out.detach().cpu().numpy().flatten())\n                y_true += list(y.cpu().numpy())\n\n            rmse_fold = np.sqrt(mean_squared_error(scaler_y.inverse_transform(np.expand_dims(y_true, axis=1)),\n                                              scaler_y.inverse_transform(np.expand_dims(y_pred, axis=1))))\n\n            #rmse_fold = np.sqrt(mean_squared_error(np.asarray(y_true), np.asarray(y_pred)))\n\n            scores_fold.append(rmse_fold)\n\n\n        y_pred = []\n        ids = []\n        with torch.no_grad():\n            final_model.eval()\n            for i, (X_cont, X_cat, y) in enumerate(test_loader):\n                out = final_model(X_cont, X_cat)\n                y_pred += list(out.detach().cpu().numpy().flatten())\n                ids += list(y.cpu().numpy())\n\n        y_pred = scaler_y.inverse_transform(np.expand_dims(y_pred, axis=1))\n        sub_nn.loc[ids, target] = sub_nn.loc[ids, target] + y_pred[:, 0]\/Nfolds\n\n    return sub_nn, scores, scores_fold","78abc61a":"#### Hyperparameters ############\ncat_feats = ['summary', 'icon', 'dayofyear', 'hour', 'year', 'month', 'day',\n             'dayofweek',  'station', 'type', 'source', 'week']\n\nuseless_columns = ['ID', 'date', 'precipIntensity', 'precipProbability',\n                   'latitude', 'longitude']\ntarget = 'aqi'\n\n\ndf, weather, sub = preprocess_rnn(ROOT='..\/input\/ulaanbaatar-city-air-pollution-prediction', cat_feat = cat_feats, rolling_means=[4, 8, 12],\n                                            shifts=[2, 4], diffs=[2, 4])\n\n\nsub_nn = sub.set_index('ID', drop=True)\nrnn_cols = list(df.drop(useless_columns+cat_feats+['aqi'], axis=1).columns)\ndt_start = ['2018-10-01', '2018-10-06', '2018-10-11', '2018-10-16', '2018-10-20']\n\n\nhidden_layer = 512\nsub_nn['aqi']=0\nprint(\"*\" * 25, \"hidden_layer\", hidden_layer, \"*\" * 25, )\nsub_nn, scores, scores_fold = training_lstm(df=df.copy(), sub_nn=sub_nn, target=target, dt_start=dt_start,\n                                          cont_cols=rnn_cols, cat_cols=cat_feats, Nfolds=5, seq_len=6, num_layers=2,\n                                          epoch=50, patience=5, MODEL_ROOT='models\/lstm',\n                                        hidden_dim=hidden_layer)\n\nprint('Neural Network CV:', np.mean(scores), '+\/-', np.std(scores))\nprint('Neural Network CV fold:', np.mean(scores_fold), '+\/-', np.std(scores_fold))\n\nsub_nn.to_csv(\"sub_lstm_%s.csv\" %hidden_layer)","d1566aba":"# LSTM model","ee3943f2":"# Import libraries","57a89aad":"****Data preprocess for LSTM","e323f4c4":"# Utils for LSTM network","8a238e28":"# Training Phase","7d18716e":"# Data import and preprocess"}}