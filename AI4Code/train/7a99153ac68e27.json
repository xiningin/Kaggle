{"cell_type":{"cfddefa3":"code","d5aa38b2":"code","9248e836":"code","0bd74ebd":"code","71658583":"code","30dddfea":"code","de5ef9f9":"code","052584ed":"code","5c5183d4":"code","5e74e20c":"code","ccd4bdee":"code","44b427a4":"code","a40bd65c":"code","e3f8f05a":"code","6f4e8476":"code","74c6c438":"code","07fc7dde":"code","c7af3d30":"code","5fa1e5e9":"code","d2d51be5":"code","66b57b9f":"code","e49cb259":"code","3b345d4c":"code","d5205f1b":"code","95cb2a51":"code","c4e8c894":"code","4d05b0cf":"code","806004e4":"code","2f6af4f9":"code","c06cedfd":"code","05ea8591":"markdown","ed762267":"markdown","bea77062":"markdown","f15e103b":"markdown","db7ec943":"markdown","e058c359":"markdown","22890b34":"markdown"},"source":{"cfddefa3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d5aa38b2":"# Importing the libraries necessary for the exercise.\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport missingno as msno\nfrom sklearn import preprocessing\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *","9248e836":"# Reading dataset\ndf = pd.read_csv(\"\/kaggle\/input\/hitters\/Hitters.csv\")","0bd74ebd":"# Looking at the first 5 rows of the data set\ndf.head()","71658583":"# First look at the dataset\ndef check_df(dataframe):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\ncheck_df(df)","30dddfea":"corr = df.corr()\nplt.figure(figsize=(18,10))\nsns.heatmap(corr, annot=True)\nplt.show()","de5ef9f9":"# To examine the effect of the league the player will play next season on the salary\nprint(\"New League A: \" ,df[df[\"NewLeague\"]==\"A\"].agg({\"Salary\":\"mean\"}))\nprint(\"New League N: \" ,df[df[\"NewLeague\"]==\"N\"].agg({\"Salary\":\"mean\"}))","052584ed":"# Examining the salary effect of the league played by the player during the season\nprint(\"League= A\" ,df[df[\"League\"]==\"A\"].agg({\"Salary\":\"mean\"}))\nprint(\"League= N\" ,df[df[\"League\"]==\"N\"].agg({\"Salary\":\"mean\"}))","5c5183d4":"# Examining the effect of the player's position on the salary\nprint(\"Division= E\" ,df[df[\"Division\"]==\"E\"].agg({\"Salary\":\"mean\"}))\nprint(\"Division= W\" ,df[df[\"Division\"]==\"W\"].agg({\"Salary\":\"mean\"}))","5e74e20c":"sns.histplot(df.Salary);","ccd4bdee":"# With this function, we were able to separate the variables in the data set as categorical and numerical.\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n    \n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n\n    return cat_cols, cat_but_car, num_cols, num_but_cat","44b427a4":"cat_cols, cat_but_car, num_cols, num_but_cat = grab_col_names(df)","a40bd65c":"# Setting an upper and lower limit for outliers\ndef outlier_thresholds(dataframe, variable):\n    quartile1 = dataframe[variable].quantile(0.10)\n    quartile3 = dataframe[variable].quantile(0.90)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit","e3f8f05a":"# The function that examines whether there is an outlier according to the threshold values we have determined.\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False","6f4e8476":"for col in num_cols:\n    print(col, check_outlier(df, col))","74c6c438":"# Replacing outliers with upper and lower limit\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit","07fc7dde":"for col in num_cols:\n        replace_with_thresholds(df, col)","c7af3d30":"msno.bar(df)\nplt.show()","5fa1e5e9":"# Drop missing values\ndf.dropna(inplace=True)","d2d51be5":"df['NEW_Hits'] = df['Hits'] \/ df['CHits']\n\ndf['NEW_RBI'] = df['RBI'] \/ df['CRBI']\n\ndf['NEW_Walks'] = df['Walks'] \/ df['CWalks']\n\ndf[\"Player_Season_Success\"] = (df[\"AtBat\"] * 4 \/ 100 + df[\"Hits\"] * 10 \/ 100 + df[\"HmRun\"] * 12 \/ 100 +\n                               df[\"Runs\"] * 12 \/ 100 + df[\"RBI\"] * 10 \/ 100 + df[\"Walks\"] * 12 \/ 100 + df[\"Assists\"] * 10 \/ 100 +\n                               df[\"PutOuts\"] * 10 \/ 100 - df[\"Errors\"] * 20 \/ 100)\n\ndf['NEW_PutOuts'] = df['PutOuts'] * df['Years']\n\ndf[\"Hits_Success\"] = (df[\"Hits\"] \/ df[\"AtBat\"]) * 100\n\ndf[\"NEW_CRBI*CATBAT\"] = df['CRBI'] * df['CAtBat']\n\ndf[\"NEW_RBI\"] = df[\"RBI\"] \/ df[\"CRBI\"]\n\ndf[\"NEW_Chits\"] = df[\"CHits\"] \/ df[\"Years\"]\n\ndf[\"NEW_CHmRun\"] = df[\"CHmRun\"] * df[\"Years\"]\n\ndf[\"NEW_CRuns\"] = df[\"CRuns\"] \/ df[\"Years\"]\n\ndf[\"NEW_Chits\"] = df[\"CHits\"] * df[\"Years\"]\n\ndf[\"NEW_RW\"] = df[\"RBI\"] * df[\"Walks\"]\n\ndf[\"NEW_RBWALK\"] = df[\"RBI\"] \/ df[\"Walks\"]\n\ndf[\"NEW_CH_CB\"] = df[\"CHits\"] \/ df[\"CAtBat\"]\n\ndf[\"NEW_CHm_CAT\"] = df[\"CHmRun\"] \/ df[\"CAtBat\"]","66b57b9f":"def label_encoder(dataframe, binary_col):\n    labelencoder = preprocessing.LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe","e49cb259":"binary_cols = [col for col in df.columns if df[col].dtypes == \"O\"\n               and len(df[col].unique()) == 2]","3b345d4c":"for col in df.columns:\n    label_encoder(df, col)","d5205f1b":"y = df[\"Salary\"]\nX = df.drop([\"Salary\"], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=17)","95cb2a51":"lgb_model = LGBMRegressor().fit(X_train, y_train)\ny_pred = lgb_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","c4e8c894":"lgb_model = LGBMRegressor()","4d05b0cf":"lgbm_params = {\"learning_rate\": [0.01, 0.1, 0.3, 0.5],\n               \"n_estimators\": [500, 800, 1200, 2000],\n               \"max_depth\": [3, 5, 8],\n               \"colsample_bytree\": [1, 0.8, 0.5]}","806004e4":"lgbm_cv_model = GridSearchCV(lgb_model,\n                             lgbm_params,\n                             cv=10,\n                             n_jobs=-1,\n                             verbose=2).fit(X_train, y_train)","2f6af4f9":"lgbm_cv_model.best_params_","c06cedfd":"lgbm_tuned = LGBMRegressor(**lgbm_cv_model.best_params_).fit(X_train, y_train)\ny_pred = lgbm_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","05ea8591":"## Reading Data","ed762267":"## Feature Engineering","bea77062":"## Outliers","f15e103b":"## Model","db7ec943":"## Label Encoding","e058c359":"## Missing Values","22890b34":"## Exploratory Data Analysis"}}