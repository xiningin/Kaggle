{"cell_type":{"61c66cb9":"code","fa752896":"code","a6cf6852":"code","048b01ce":"code","a2345ff3":"code","488003d7":"code","0454a364":"code","134c927c":"code","46c8c09a":"code","ae982479":"code","67940092":"code","ba4d4e25":"code","5a62d310":"code","eb49c553":"code","eaa1499a":"code","6b7135a5":"code","e0e08d11":"code","1c4eb0b4":"code","b80d2edb":"code","f6853b87":"code","b4755906":"code","edc589eb":"code","eb11ea88":"code","dc96fd26":"code","e2284b4b":"code","4611a966":"code","47eb7f55":"code","301a5c9c":"code","f81985e8":"markdown","f60d4437":"markdown","c210592f":"markdown","bd43cad7":"markdown","7d67e251":"markdown","e84d122d":"markdown","d4acac82":"markdown","8c694d70":"markdown","06e31adc":"markdown","dd4b1cf8":"markdown","575aa7a7":"markdown","bc79d367":"markdown","3b1e348c":"markdown","2a8b6bef":"markdown","aba7b3ba":"markdown"},"source":{"61c66cb9":"# Update kaggle-environments to the newest version.\n!pip3 install kaggle-environments -U\n\n# GFootball environment.\n!apt-get update -y\n!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n\n# Make sure that the Branch in git clone and in wget call matches !!\n!git clone -b v2.7 https:\/\/github.com\/google-research\/football.git\n!mkdir -p football\/third_party\/gfootball_engine\/lib\n\n!wget https:\/\/storage.googleapis.com\/gfootball\/prebuilt_gameplayfootball_v2.7.so -O football\/third_party\/gfootball_engine\/lib\/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .","fa752896":"!pip install pfrl==0.1.0","a6cf6852":"import os\nimport cv2\nimport sys\nimport glob \nimport random\nimport imageio\nimport pathlib\nimport collections\nfrom collections import deque\nimport numpy as np\nimport argparse\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline\n\nfrom gym import spaces\nfrom tqdm import tqdm\nfrom logging import getLogger, StreamHandler, FileHandler, DEBUG, INFO\nfrom typing import Union, Callable, List, Tuple, Iterable, Any, Dict\nfrom dataclasses import dataclass\nfrom IPython.display import Image, display\nsns.set()\n\n\n# PyTorch\nimport pfrl\nfrom pfrl.agents import CategoricalDoubleDQN\nfrom pfrl import experiments\nfrom pfrl import explorers\nfrom pfrl import nn as pnn\nfrom pfrl import utils\nfrom pfrl import replay_buffers\nfrom pfrl.wrappers import atari_wrappers\nfrom pfrl.q_functions import DistributionalDuelingDQN\n\nimport torch\nfrom torch import nn\n\n# Env\nimport gym\nimport gfootball\nimport gfootball.env as football_env\nfrom gfootball.env import observation_preprocessing","048b01ce":"# Check we can use GPU\nprint(torch.cuda.is_available())\n\n# set gpu id\nif torch.cuda.is_available(): \n    # NOTE: it is not number of gpu but id which start from 0\n    gpu = 0\nelse:\n    # cpu=>-1\n    gpu = -1","a2345ff3":"# set logger\ndef logger_config():\n    logger = getLogger(__name__)\n    handler = StreamHandler()\n    handler.setLevel(\"DEBUG\")\n    logger.setLevel(\"DEBUG\")\n    logger.addHandler(handler)\n    logger.propagate = False\n\n    filepath = '.\/result.log'\n    file_handler = FileHandler(filepath)\n    logger.addHandler(file_handler)\n    return logger\n\nlogger = logger_config()","488003d7":"# fixed random seed\n# but this is NOT enough to fix the result of rewards.Please tell me the reason.\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    utils.set_random_seed(seed)  # for PFRL\n    \n# Set a random seed used in PFRL.\nseed = 5046\nseed_everything(seed)\n\n# Set different random seeds for train and test envs.\ntrain_seed = seed\ntest_seed = 2 ** 31 - 1 - seed","0454a364":"# wrapper for env(resize and transpose channel order)\nclass TransEnv(gym.ObservationWrapper):\n    def __init__(self, env, channel_order=\"hwc\"):\n\n        gym.ObservationWrapper.__init__(self, env)\n        self.height = 84\n        self.width = 84\n        self.ch = env.observation_space.shape[2]\n        shape = {\n            \"hwc\": (self.height, self.width, self.ch),\n            \"chw\": (self.ch, self.height, self.width),\n        }\n        self.observation_space = spaces.Box(\n            low=0, high=255, shape=shape[channel_order], dtype=np.uint8\n        )\n        \n\n    def observation(self, frame):\n        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n        return frame.reshape(self.observation_space.low.shape)","134c927c":"def make_env(test):\n    # Use different random seeds for train and test envs\n    env_seed = test_seed if test else train_seed\n    \n    # env = gym.make('GFootball-11_vs_11_kaggle-SMM-v0')\n    env = football_env.create_environment(\n      env_name='11_vs_11_easy_stochastic',  # easy mode\n      stacked=False,\n      representation='extracted',  # SMM\n      rewards='scoring,checkpoints',\n      write_goal_dumps=False,\n      write_full_episode_dumps=False,\n      render=False,\n      write_video=False,\n      dump_frequency=1,\n      logdir='.\/',\n      extra_players=None,\n      number_of_left_players_agent_controls=1,\n      number_of_right_players_agent_controls=0\n    )\n    env = TransEnv(env, channel_order=\"chw\")\n\n    env.seed(int(env_seed))\n    if test:\n        # Randomize actions like epsilon-greedy in evaluation as well\n        env = pfrl.wrappers.RandomizeAction(env, random_fraction=0.0)\n    return env\n\nenv = make_env(test=False)\neval_env = make_env(test=True)","46c8c09a":"print('observation space:', env.observation_space.low.shape)\nprint('action space:', env.action_space)","ae982479":"env.reset()\naction = env.action_space.sample()\nobs, r, done, info = env.step(action)\nprint('next observation:', obs.shape)\nprint('reward:', r)\nprint('done:', done)\nprint('info:', info)","67940092":"obs_n_channels = env.observation_space.low.shape[0]\nn_actions = env.action_space.n\nprint(\"obs_n_channels: \", obs_n_channels)\nprint(\"n_actions: \", n_actions)\n\n# params based the original paper\nn_atoms = 51\nv_max = 10\nv_min = -10\nq_func = DistributionalDuelingDQN(n_actions, n_atoms, v_min, v_max, obs_n_channels)\nprint(q_func)","ba4d4e25":"# Noisy nets\npnn.to_factorized_noisy(q_func, sigma_scale=0.5)\n\n# Turn off explorer\nexplorer = explorers.Greedy()\n\n# Use the same hyper parameters as https:\/\/arxiv.org\/abs\/1710.02298\nopt = torch.optim.Adam(q_func.parameters(), 6.25e-5, eps=1.5 * 10 ** -4)\n\n# Prioritized Replay\n# Anneal beta from beta0 to 1 throughout training\nupdate_interval = 4\nbetasteps = 5 * 10 ** 7 \/ update_interval\nrbuf = replay_buffers.PrioritizedReplayBuffer(\n        10 ** 5,  # Default value is 10 ** 6 but it is too large in this notebook. I chose 10 ** 5.\n        alpha=0.5,\n        beta0=0.4,\n        betasteps=betasteps,\n        num_steps=3,\n        normalize_by_max=\"memory\",\n    )\n\n\ndef phi(x):\n    # Feature extractor\n    return np.asarray(x, dtype=np.float32) \/ 255","5a62d310":"agent = CategoricalDoubleDQN(\n        q_func,\n        opt,\n        rbuf,\n        gpu=gpu,  \n        gamma=0.99,\n        explorer=explorer,\n        minibatch_size=32,\n        replay_start_size=2 * 10 ** 4,\n        target_update_interval=32000,\n        update_interval=update_interval,\n        batch_accumulator=\"mean\",\n        phi=phi,\n    )","eb49c553":"# if you have a pretrained model, agent can load pretrained weight. \nuse_pretrained = False\npretrained_path = None\nif use_pretrained:\n    agent.load(pretrained_path)","eaa1499a":"num_steps = 100000","6b7135a5":"%%time\nexperiments.train_agent_with_evaluation(\n    agent=agent,\n    env=env,\n    steps=num_steps,\n    eval_n_steps=None,\n    eval_n_episodes=1,\n    eval_interval=3000,\n    outdir=\".\/\",\n    checkpoint_freq=100000,\n    save_best_so_far_agent=True,\n    eval_env=eval_env,\n    logger=logger\n)","e0e08d11":"import csv\n\ndef text_csv_converter(datas):\n    file_csv = datas.replace(\"txt\", \"csv\")\n    with open(datas) as rf:\n        with open(file_csv, \"w\") as wf:\n            readfile = rf.readlines()\n            for read_text in readfile:\n                read_text = read_text.split()\n                writer = csv.writer(wf, delimiter=',')\n                writer.writerow(read_text)\n\nfilename = \"scores.txt\"\ntext_csv_converter(filename)","1c4eb0b4":"import pandas as pd\nscores = pd.read_csv(\"scores.csv\")\nscores.tail()","b80d2edb":"# visualize reward each episodes\nfig = plt.figure(figsize=(15, 5))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\nax1.set_title(\"median reward\")\nax2.set_title(\"average loss\")\nsns.lineplot(x=\"episodes\", y=\"median\", data=scores, ax=ax1)\nsns.lineplot(x=\"episodes\", y=\"average_loss\", data=scores,ax=ax2)\nplt.show()","f6853b87":"# clone pfrl repo and move to the directory with main.py \n!git clone https:\/\/github.com\/pfnet\/pfrl.git\n!mkdir sub\n!mv .\/pfrl\/pfrl sub\n!rm -r .\/pfrl","b4755906":"import base64\n\nwith open(f'.\/{num_steps}_finish\/model.pt', 'rb') as f:\n    encoded_string = base64.b64encode(f.read())\n\nwith open('.\/sub\/model_weights.py', 'w') as f:\n    f.write(f'model_string={encoded_string}')","edc589eb":"%cd sub\n!ls -la ","eb11ea88":"%%writefile main.py\nimport os\nimport sys\nimport cv2\nimport collections\nimport numpy as np\nfrom gfootball.env import observation_preprocessing\n\n# PFRL\nimport torch\nimport pfrl\n\nimport base64\nfrom model_weights import model_string\n\ndef make_model():\n    # Q_function\n    model = pfrl.q_functions.DistributionalDuelingDQN(n_actions=19, n_atoms=51, v_min=-10, v_max=10, n_input_channels=4)\n    \n    # Noisy nets\n    pfrl.nn.to_factorized_noisy(model, sigma_scale=0.5)\n    \n    # load weights\n    with open(\"model.dat\", \"wb\") as f:\n        f.write(base64.b64decode(model_string))\n    weights = torch.load(\"model.dat\", map_location=torch.device('cpu'))\n    model.load_state_dict(weights)\n    return model\n\n\nmodel = make_model()\n\ndef agent(obs):\n    global model\n    \n    # Get observations for the first (and only one) player we control.\n    obs = obs['players_raw'][0]\n    # Agent we trained uses Super Mini Map (SMM) representation.\n    # See https:\/\/github.com\/google-research\/seed_rl\/blob\/master\/football\/env.py for details.\n    obs = observation_preprocessing.generate_smm([obs])[0]\n    # preprocess for obs\n    obs = cv2.resize(obs, (84,84))    # resize\n    obs = np.transpose(obs, [2,0,1])  # transpose to chw\n    obs = torch.tensor(obs).float()   # to tensor\n    obs = torch.unsqueeze(obs,0)      # add batch\n\n    actions = model(obs)\n    action = int(actions.greedy_actions.numpy()[0])  # modified\n    return [action]","dc96fd26":"!pip install stickytape","e2284b4b":"!stickytape main.py --add-python-path pfrl --add-python-path . > \/kaggle\/working\/submission.py\n!rm -r pfrl\n%cd \/kaggle\/working\n!ls -la","4611a966":"from kaggle_environments import make\nfrom submission import agent\nenv = make(\"football\", configuration={\"save_video\": True, \"scenario_name\": \"11_vs_11_kaggle\", \"running_in_notebook\": True})\nobs = env.state[0][\"observation\"]\naction = agent(obs)\nprint(action)","47eb7f55":"from kaggle_environments import make\nenv = make(\"football\", configuration={\"save_video\": True, \"scenario_name\": \"11_vs_11_kaggle\", \"running_in_notebook\": True}, debug=True)\nagent = \"submission.py\"\noutput = env.run([agent, agent])[-1]\nprint('Left player: action = %s, reward = %s, status = %s, info = %s' % (output[0][\"action\"], output[0]['reward'], output[0]['status'], output[0]['info']))\nprint('Right player: action = %s, reward = %s, status = %s, info = %s' % (output[1][\"action\"], output[1]['reward'], output[1]['status'], output[1]['info']))\nenv.render(mode=\"human\", width=800, height=600)","301a5c9c":"# to clean output folder\n!rm -r \/kaggle\/working\/football\n!rm -r \/kaggle\/working\/kaggle-environments","f81985e8":"## Environment","f60d4437":"First, I encode model weight to .py script by using base64. ","c210592f":"### check submission agent","bd43cad7":"In this notebook, I set 100K steps to reduce processing times(spend about 1 hour).    \nTo improve the agent, we will need to try following.\n- set high value steps & high value replay buffers\n- add some innovations \n- use other RL agent","7d67e251":"### Submit to Competition\n1. \"Save & Run All\" (commit) this Notebook\n2. Go to the notebook viewer\n3. Go to \"Output\" section and find submission.py file.\n4. Click \"Submit to Competition\"\n\nGo to My Submissions to view your score and episodes being played.  \nThank you.","e84d122d":"Then, make main.py.  \n  \nI've been plagued with validation episode errors for a long time.  \nHowerver it was solved by his advice. Thanks [@Tom Van de Wiele](https:\/\/www.kaggle.com\/tvdwiele).","d4acac82":"## Submission","8c694d70":"## Config","06e31adc":"Finally I'll concat script file by using stickytape.  \nmain.py + pfrl library + model_weight.py => submission.py","dd4b1cf8":"## About\n\n**I published [this notebook](https:\/\/www.kaggle.com\/kuto0633\/gfootball-rainbow-dqn-pfrl) before, but I cannot update it. The detail is [here](https:\/\/www.kaggle.com\/product-feedback\/191763). So I modified it and publish it again in this notebook. I'm so sorry to split up into multiple notebooks.**\n\nIn this notebook, I will introduce [Rainbow](https:\/\/arxiv.org\/abs\/1710.02298) and [PFRL](https:\/\/github.com\/pfnet\/pfrl ).  \n\n[21\/10\/2020] Update:\n1. modify setting rewards when creating environment  \nhttps:\/\/www.kaggle.com\/c\/google-football\/discussion\/191594   \n2. modify submission process(Submittable)  \nhttps:\/\/www.kaggle.com\/c\/google-football\/discussion\/190967  \nhttps:\/\/www.kaggle.com\/c\/google-football\/discussion\/187381#1054470\n\n\n### Rainbow\nRainbow is an Reinforcement Learning(RL) algorithm that extends the DQN. It has performed well in Atari games (benchmarking for RL) **only single GPU**. Modern high-performance RL algorithms(Ape-X, R2D2, etc) are mainly distributed RL method that use multiple distributed environments, and I guess distributed RL is effective approach in this competition. But these are little difficult to run well on the kaggle notebook and google colab environment because these approach need massively distributed computing resource. So, I try to use Rainbow-DQN in this notebook.  \n\nRainbow consists of the following seven elements.\n- DQN\n- Double D-learnig\n- Prioritized replay\n- Dueling networks\n- Multi-step learning\n- Distributional RL\n- Noisy nets\n\nPlease click [here](https:\/\/arxiv.org\/abs\/1710.02298) for details.\nI will write these components by PFRL.\n\n### PFRL\n<div align=\"center\"><img src=\"https:\/\/raw.githubusercontent.com\/pfnet\/pfrl\/master\/assets\/PFRL.png\" width=30%\/><\/div>\nPFRL is a deep reinforcement learning library that implements various state-of-the-art deep reinforcement algorithms in Python using PyTorch.  \n\nMost of the published notebooks are written in keras(TensorFlow). But there are many people who would like to use PyTorch. So, I propose PFRL. In this notebook, there is not much PyTorch-specific code since I use existing modules. But we can rewrite by PyTorch in detail if you want. \n  \nPlease check [here](https:\/\/github.com\/pfnet\/pfrl) for details. \n\n","575aa7a7":"To include pfrl library to submission file, I prepare pfrl repository.","bc79d367":"## Train","3b1e348c":"## Install","2a8b6bef":"To concat pfrl module and model weight to submission file, I use `stickytape` module.  \n  \nI refered [this topic](https:\/\/www.kaggle.com\/c\/halite\/discussion\/164005) in halite competition.Thanks [@higepon](https:\/\/www.kaggle.com\/higepon).","aba7b3ba":"## Model"}}