{"cell_type":{"993bcde9":"code","6acf295c":"code","5edf73a6":"code","6d6f5827":"code","ba8a0244":"code","12389305":"code","1fbddf52":"code","33f5288d":"code","d69db407":"code","c9b53818":"code","cc082d20":"code","14e4ec83":"code","a9f086c7":"code","aa6454e3":"code","a3366351":"code","0dc7afb5":"code","87fc79fb":"code","37af56ef":"code","89950ed6":"code","90c5c891":"code","4cee22ab":"code","0dcd3622":"code","974da756":"code","8de525b6":"code","5cd7964a":"code","aa7b44eb":"code","b493f3a6":"code","9dbfcfb8":"code","89a5e76f":"code","6658e94e":"code","ad9942c4":"code","1bbe2787":"code","95f21a8b":"code","f0aa4373":"code","c2ee4089":"code","7e05dc59":"code","fcb85ddb":"code","75336c37":"code","100d070d":"code","962141b3":"code","5c74cf09":"code","50979175":"code","d239de9f":"code","d7187ad5":"code","7543b869":"code","558a9f02":"code","4f6b086b":"code","2bba0a9a":"code","b8d24205":"code","3128298c":"code","b415c304":"code","dac4481b":"code","24cd5cff":"code","2b5ff1d0":"code","843e7287":"code","37ef9ee7":"code","cf3c8c37":"code","39d96fc0":"code","a6d17cc4":"code","3580bf6c":"code","cb4e4326":"code","43f0d50a":"code","2771c0cf":"code","13807eb3":"markdown","0db4f144":"markdown","d1cef98e":"markdown","e42f52a0":"markdown","12c178c3":"markdown","5e65724e":"markdown","e809423c":"markdown","a5a3a7ed":"markdown","b56c7f78":"markdown","d13a8021":"markdown","53bcf844":"markdown","8802cc26":"markdown","3b1f08de":"markdown","8d0c5295":"markdown","bfd03221":"markdown","5510ce71":"markdown","c9e3792f":"markdown","42ae3437":"markdown","85120d52":"markdown","f9b094f0":"markdown","8ce77abc":"markdown","e900d5bb":"markdown","ea490506":"markdown","6995c901":"markdown","ed23df7d":"markdown","7f54dc71":"markdown","40836d1c":"markdown","a6c167a8":"markdown","b5b4f674":"markdown","e12cff48":"markdown","c5a5617d":"markdown","eb7d2c8f":"markdown","a4df1674":"markdown","07b1bc11":"markdown","8b7fce4e":"markdown","4bfa00fe":"markdown","49829537":"markdown","9e16ad23":"markdown","43aa881c":"markdown","7209b2bc":"markdown"},"source":{"993bcde9":"import re\nimport string\nimport numpy as np \nimport random\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6acf295c":"def random_colours(number_of_colors):\n    '''\n    Simple function for random colours generation.\n    Input:\n        number_of_colors - integer value indicating the number of colours which are going to be generated.\n    Output:\n        Color in the following format: ['#E86DA4'] .\n    '''\n    colors = []\n    for i in range(number_of_colors):\n        colors.append(\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]))\n    return colors","5edf73a6":"import pandas as pd\nimport numpy as np\ntrain=pd.read_csv('\/kaggle\/input\/trump-tweets\/trumptweets.csv')\n","6d6f5827":"train","ba8a0244":"print(train.shape)\n","12389305":"train.describe()","1fbddf52":"train.isna().sum()","33f5288d":"train['hashtags'].unique()","d69db407":"train['mentions'].unique()","c9b53818":"train=train.drop(['link','mentions','hashtags','geo'], axis=1)","cc082d20":"train.head()","14e4ec83":"def remove_spaces(text):\n    text=text.strip()\n    text=text.split()\n    return ' '.join(text)\n    ","a9f086c7":"def edits1(word):\n    letters='abcdefghijklmnopqrstuvwxyz'\n    splits=[(word[:i], word[i:]) for i in range(len(word)+1)]\n    deletes=[L+R[1:] for L,R in splits if R]\n    transposes=[L+R[1] +R[0] + R[2:] for L,R in splits if len(R)>1]\n    replaces = [L+c+R[1:] for L,R in splits if R for c in letters]\n    inserts = [L+c+ R for L,R in splits for c in letters]\n    return set(deletes+transposes+replaces+inserts)\ndef edits2(word):\n    return(e2 for e1 in edits1(word) for e2 in edits1(e1))\n            ","aa6454e3":"contraction = {'cause':'because',\n              'aint': 'am not',\n              'aren\\'t': 'are not'}\n\ndef mapping_replacer(x,dic):\n    for words in dic.keys():\n        if ' ' + words + ' ' in x:\n            x=x.replace(' '+ words +' ' ,' '+dic[words]+' ' )\n    return x\n\n    ","a3366351":"import nltk\nnltk.download('punkit')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.stem.lancaster import LancasterStemmer\n\nnltk.LancasterStemmer\nls = LancasterStemmer()\nlem = WordNetLemmatizer()\ndef lexicon_normalization(text):\n    words = word_tokenize(text) \n    \n    \n    # 1- Stemming\n    words_stem = [ls.stem(w) for w in words]\n    \n    # 2- Lemmatization\n    words_lem = [lem.lemmatize(w) for w in words_stem]\n    return words_lem\n\n    ","0dc7afb5":"import emoji\nimport re \n#from emot.emo_unicode import UNICODE_EMO\ndef convert_emojis(text):\n    for emot in emoji.UNICODE_EMOJI:\n        text = re.sub(r'('+emot+')', \"_\".join(emoji.UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n    return text","87fc79fb":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('\\'','', text)\n    \n    return text","37af56ef":"from collections import Counter\ndef remove_stopword(text):\n    stop_words = stopwords.words('english')\n    stopwords_dict = Counter(stop_words)\n    text = ' '.join([word for word in text.split() if word not in stopwords_dict])\n    return text","89950ed6":"def tokenise(text):\n    words = word_tokenize(text) \n    return words\n","90c5c891":"import re\ntrain['content'] = train['content'].map(lambda x: re.sub(r'\\W+', ' ', x))\ntrain['content'] = train['content'].replace(r'\\W+', ' ', regex=True)\n","4cee22ab":"train.head()","0dcd3622":"train['content']=train['content'].apply(lambda x: mapping_replacer(x, contraction))","974da756":"train['content'] = train['content'].apply(lambda x:clean_text(x))","8de525b6":"train['content']=train['content'].apply(lambda x: remove_stopword(x))\n","5cd7964a":"train['content']=train['content'].apply(lambda x: lexicon_normalization(x))","aa7b44eb":"train.head()","b493f3a6":"top = Counter([item for sublist in train['content'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","9dbfcfb8":"blacklist = ['http','https','www','com', 'ev','u','ly','pic','would']\n\ndef remove_words(text):\n    text = [i for i in text if (i not in blacklist)]\n    return text","89a5e76f":"#train['content']=remove_words(train['content'])\n\ntrain['content'] = train['content'].apply(lambda x: [i for i in x if i not in blacklist])","6658e94e":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Selected Text', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","ad9942c4":"top = Counter([item for sublist in train['content'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp = temp.iloc[1:,:]\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Purples')","1bbe2787":"fig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words')\nfig.show()","95f21a8b":"from wordcloud import WordCloud, STOPWORDS , ImageColorGenerator\n\nfrom textblob import TextBlob\n\ndef get_tweet_sentiment(tweet): \n    ''' \n    Utility function to classify sentiment of passed tweet \n    using textblob's sentiment method \n    '''\n    # create TextBlob object of passed tweet text \n    analysis = TextBlob(tweet) \n    \n    # set sentiment \n    if analysis.sentiment.polarity > 0:\n        return 'positive'\n    elif analysis.sentiment.polarity == 0: \n        return 'neutral'\n    else: \n        return 'negative'","f0aa4373":"train['sentiment']=train['content'].apply(lambda x: get_tweet_sentiment(' '.join(x)))","c2ee4089":"train.head()","7e05dc59":"Positive_sent = train[train['sentiment']=='positive']\nNegative_sent = train[train['sentiment']=='negative']\nNeutral_sent = train[train['sentiment']=='neutral']","fcb85ddb":"print('Number of tweets with positive sentiment', Positive_sent['sentiment'].count())\nprint('Number of tweets with negative sentiment', Negative_sent['sentiment'].count())\nprint('Number of tweets with neutral sentiment', Neutral_sent['sentiment'].count())\n","75336c37":"#MosT common positive words\ntop = Counter([item for sublist in Positive_sent['content'] for item in sublist])\ntemp_positive = pd.DataFrame(top.most_common(20))\ntemp_positive.columns = ['Common_words','count']\ntemp_positive.style.background_gradient(cmap='Greens')","100d070d":"import numpy as np\ntop = Counter([item for sublist in Positive_sent['content'] for item in sublist])\ntemp_positive = pd.DataFrame(top.most_common(23))\ntemp_positive.columns = ['Common_words','count']\ntemp_positive['Common_words'] = temp_positive['Common_words'].map(lambda x: re.sub(r'\\W+', '', x))\ntemp_positive['Common_words'] = temp_positive['Common_words'].replace(r'\\W+', '', regex=True)\ntemp_positive['Common_words'] = temp_positive['Common_words'].apply(lambda x:remove_spaces(x))\ntemp_positive=temp_positive[~temp_positive['Common_words'].isin(['s','gre','\u201c',' * '])] #new line removing meaningless words\nmask1 = temp_positive.Common_words.str.contains('[a-zA-Z]')\nmask2 = temp_positive.Common_words.notna()\ntemp_positive = temp_positive[mask1 | mask2]\ntemp_positive.Common_words =  temp_positive.Common_words.str.replace(r\"\\s+\", \"\").replace(\"\", np.NaN)\ntemp_positive=temp_positive.dropna()\n\n\ntemp_positive.style.background_gradient(cmap='Greens')\n","962141b3":"fig = px.bar(temp_positive, x=\"count\", y=\"Common_words\", title='Most Commmon Words in Positive Sentiment tweets', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","5c74cf09":"#MosT common negative words\ntop = Counter([item for sublist in Negative_sent['content'] for item in sublist])\ntemp_negative = pd.DataFrame(top.most_common(20))\ntemp_negative = temp_negative.iloc[1:,:]\ntemp_negative.columns = ['Common_words','count']\ntemp_negative.style.background_gradient(cmap='Reds')","50979175":"#MosT common negative words\ntop = Counter([item for sublist in Negative_sent['content'] for item in sublist])\ntemp_negative = pd.DataFrame(top.most_common(22))\ntemp_negative = temp_negative.iloc[1:,:]\ntemp_negative.columns = ['Common_words','count']\n\n#Data cleaning\ntemp_negative['Common_words'] = temp_negative['Common_words'].map(lambda x: re.sub(r'\\W+', '', x))\ntemp_negative['Common_words'] = temp_negative['Common_words'].replace(r'\\W+', '', regex=True)\ntemp_negative=temp_negative[~temp_negative['Common_words'].isin(['s','t'])] #new line removing meaningless words from above\n#mask1 = temp_negative.Common_words.str.contains('[a-zA-Z]')\n#mask2 = temp_negative.Common_words.notna()\n#temp_negative = temp_negative[mask1 | mask2]\n\ntemp_negative.Common_words =  temp_negative.Common_words.replace(\"\", np.nan)\ntemp_negative = temp_negative.dropna(subset=['Common_words'])\n\ntemp_negative.style.background_gradient(cmap='Reds')","d239de9f":"fig = px.treemap(temp_negative, path=['Common_words'], values='count',title='Tree Of Most Common Words in Negative Tweets')\nfig.show()","d7187ad5":"#MosT common Neutral words\ntop = Counter([item for sublist in Neutral_sent['content'] for item in sublist])\ntemp_neutral = pd.DataFrame(top.most_common(20))\ntemp_neutral = temp_neutral.loc[1:,:]\ntemp_neutral.columns = ['Common_words','count']\ntemp_neutral.style.background_gradient(cmap='Reds')","7543b869":"\ntop = Counter([item for sublist in Neutral_sent['content'] for item in sublist])\ntemp_neutral = pd.DataFrame(top.most_common(20))\ntemp_neutral = temp_neutral.loc[1:,:]\ntemp_neutral.columns = ['Common_words','count']\n\n#Data cleaning\ntemp_neutral['Common_words'] = temp_neutral['Common_words'].map(lambda x: re.sub(r'\\W+', '', x))\ntemp_neutral['Common_words'] = temp_neutral['Common_words'].replace(r'\\W+', '', regex=True)\ntemp_neutral=temp_neutral[~temp_neutral['Common_words'].isin(['s'])] #new line removing meaningless words from above\n\ntemp_neutral.Common_words =  temp_neutral.Common_words.replace(\"\", np.nan)\ntemp_neutral = temp_neutral.dropna(subset=['Common_words'])\n\ntemp_neutral.style.background_gradient(cmap='Reds')","558a9f02":"fig = px.bar(temp_neutral, x=\"count\", y=\"Common_words\", title='Most Commmon Neutral Words', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","4f6b086b":"fig = px.treemap(temp_neutral, path=['Common_words'], values='count',title='Tree Of Most Common Neutral Words')\nfig.show()","2bba0a9a":"raw_text = [word for word_list in train['content'] for word in word_list]","b8d24205":"def words_unique(sentiment,numwords,raw_words):\n    '''\n    Input:\n        segment - Segment category (ex. 'Neutral');\n        numwords - how many specific words do you want to see in the final result; \n        raw_words - list  for item in train_data[train_data.segments == segments]['temp_list1']:\n    Output: \n        dataframe giving information about the name of the specific ingredient and how many times it occurs in the chosen cuisine (in descending order based on their counts)..\n\n    '''\n    allother = []\n    for item in train[train.sentiment != sentiment]['content']:\n        for word in item:\n            allother .append(word)\n    allother  = list(set(allother ))\n    \n    specificnonly = [x for x in raw_text if x not in allother]\n    \n    mycounter = Counter()\n    \n    for item in train[train.sentiment == sentiment]['content']:\n        for word in item:\n            mycounter[word] += 1\n    keep = list(specificnonly)\n    \n    for word in list(mycounter):\n        if word not in keep:\n            del mycounter[word]\n    \n    Unique_words = pd.DataFrame(mycounter.most_common(numwords), columns = ['words','count'])\n    \n    return Unique_words","3128298c":"Unique_Positive= words_unique('positive', 20, raw_text)\nprint(\"The top 20 unique words in Positive Tweets are:\")\nUnique_Positive.style.background_gradient(cmap='Greens')","b415c304":"Unique_Positive","dac4481b":"fig = px.treemap(Unique_Positive, path=['words'], values='count',title='Tree Of Unique Words in Positive sentiment tweets')\nfig.show()","24cd5cff":"\nfrom palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.pie(Unique_Positive['count'], labels=Unique_Positive.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique words in Positive sentiment tweets')\nplt.show()","2b5ff1d0":"Unique_Negative= words_unique('negative', 10, raw_text)\nprint(\"The top 10 unique words in Negative Tweets are:\")\nUnique_Negative.style.background_gradient(cmap='Reds')","843e7287":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.rcParams['text.color'] = 'black'\nplt.pie(Unique_Negative['count'], labels=Unique_Negative.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique words in Negative sentiment tweets')\nplt.show()","37ef9ee7":"Unique_Neutral= words_unique('neutral', 10, raw_text)\nprint(\"The top 10 unique words in Neutral Tweets are:\")\nUnique_Neutral.style.background_gradient(cmap='Oranges')","cf3c8c37":"#Data cleaning\nUnique_Neutral= words_unique('neutral', 14, raw_text)\nUnique_Neutral['words'] = Unique_Neutral['words'].map(lambda x: re.sub(r'\\W+', '', x))\nUnique_Neutral['words'] = Unique_Neutral['words'].replace(r'\\W+', '', regex=True)\nUnique_Neutral['words']=Unique_Neutral[~Unique_Neutral['words'].isin(['\u0628\u0647','\u0631\u0627','\u0627\u06cc\u0631\u0627\u0646','\u0648'])] #new line removing meaningless words from above\n\nUnique_Neutral['words'] =  Unique_Neutral['words'].replace(\"\", np.nan)\nUnique_Neutral= Unique_Neutral.dropna(subset=['words'])\n\nUnique_Neutral.style.background_gradient(cmap='Reds')","39d96fc0":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.pie(Unique_Neutral['count'], labels=Unique_Neutral.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique words in Neutral sentiment tweets')\nplt.show()","a6d17cc4":"def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), color = 'white',\n                   title = None, title_size=40, image_color=False):\n    \n    wordcloud = WordCloud(background_color=color,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=400, \n                    height=200,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \nd = '\/kaggle\/input\/trump-tweets\/'","3580bf6c":"Neutral_sent","cb4e4326":"pos_mask = np.array(Image.open('\/kaggle\/input\/tweet-mask\/tweet_mask.png'))\nplot_wordcloud(Neutral_sent.content,mask=pos_mask,color='white',max_font_size=100,title_size=30,title=\"WordCloud of Neutral Tweets\")","43f0d50a":"plot_wordcloud(Positive_sent.content,mask=pos_mask,title=\"Word Cloud Of Positive tweets\",title_size=30)","2771c0cf":"plot_wordcloud(Negative_sent.content,mask=pos_mask,title=\"Word Cloud of Negative Tweets\",color='white',title_size=30)","13807eb3":"<div id=10><h2>10. WordClouds<\/h2> <\/div>\n\n\nWe will be building three types of wordclouds:\n\n* WordCloud of Neutral Tweets\n* WordCloud of Positive Tweets\n* WordCloud of Negative Tweets","0db4f144":"<b> Stemming, lemmetisation and tokenisation\n<\/b>","d1cef98e":"# Le but de ce notebook\n\nL\u2019objectif de cet article est de r\u00e9aliser une analyse exploratoire et visuelle des tweets pr\u00e9sents dans notre jeu de donn\u00e9es. Dans un second temps, le but sera de parvenir \u00e0 classifier \u00e0 l\u2019aide de diff\u00e9rents mod\u00e8les disponibles, les sentiments des tweets selon qu\u2019ils soient plut\u00f4t positifs, neutres ou n\u00e9gatifs. Autrement dit r\u00e9unir sentiment analysis et NLP.\n\n# Dataset\n\nSe compose des teweets sur Trump.","e42f52a0":"<div id=3> <h2> 3. Handling null values <\/h2> <\/div>","12c178c3":"Since the count for positive, negative and neutral sentiments is so less, there is no use of creating word clouds with unique words. So we will create a wordcloud with all the words combined.","5e65724e":"<b>Removing links, brackets, numbers, punctuations etc. <\/b>\n","e809423c":"So We have 27486 tweets in the train set and 3535 tweets in the test set","a5a3a7ed":"**By Looking at the Unique Words of each sentiment,we now have much more clarity about the data**\n\nWe shall now proceed to create word clouds, but we shall make word cloud of all words and not just existing words since unique words are very less in number for many sentiments (10,7,9 etc.) , so an effective word cloud can be only made by considering all the words in different sentiment tweets","b56c7f78":"<b> Handling stopwords <\/b>","d13a8021":"1. hashtags: This column does not have something very significant for us to analyse. We have mostly words like #ixzz4 etc which make no sense and we wont be able to analyse their sentiment. So we can get rid of this column\n2. mentions  too does not have anything siginificant for us to analyse. Hence we can do away with that too. Let us drop columns which dont have any siginificantly useful information.\n3. geo anyways does not have any value, all values are null\n4. mentions usually mention another person and we wont really get any sentiment by analysing that","53bcf844":"<div id=1> <h2>  1. Importing packages and libraries  <\/h2> <\/div> ","8802cc26":"<div id=8.3><h3>8.3 Finding the common words for negative sentiment tweets <\/div><\/h3>","3b1f08de":"**Removing weird spaces**","8d0c5295":"<b> Handling emojis <\/b>","bfd03221":"<b> Overall tweet sentiment <\/b>","5510ce71":"<div id=5> <h2>5. Cleaning the data <\/h2> <\/div>\n\nLet's first clean the data, remove stopwords etc and perform basic pre-processing","c9e3792f":"<b> Spelling Correction <\/b>","42ae3437":"  <div id=9.3><h2>9.3. Number of unique words in tweets with neutral sentiments  <\/h2> <\/div>","85120d52":"<div id=10><h2>10.1. WordCloud for neutral sentiment<\/h2> <\/div>","f9b094f0":"<b> Contraction <\/b>","8ce77abc":"<div id=10.2><h2>10.2 WordCloud for positive sentiment<\/h2> <\/div>\n","e900d5bb":"<b><i>So we see that overall the tweets are neutral in nature, followed by positive sentiment for the time in which they are analysed.<\/i><\/b>","ea490506":"Geo column has all values as nulls, hashtags have lot of null values, mentions also have lot of null values.","6995c901":"<div id=7><h2>7. Finding the most Common words in our Text <\/h2><\/div>","ed23df7d":"<div id=8.4><h3>8.4 Finding the common words for neutral sentiment tweets <\/div><\/h3>","7f54dc71":"  <div id=9.2><h2>9.2. Number of unique words in tweets with negative sentiment  <\/h2> <\/div>","40836d1c":"<div id=2> <h2> 2. Reading the Data  <\/h2><\/div> ","a6c167a8":"<div id=4> <h2> 4. EDA  <\/h2><\/div>","b5b4f674":"<div id=8.2><h3>8.2 Finding common words for positive sentiment tweets<\/div><\/h3>","e12cff48":"Lets look at the distribution of reviews in the train set","c5a5617d":"<div id=8.1><h3>8.1 Finding the tweet sentiment <\/div><\/h3>","eb7d2c8f":"<div id = 6> <h2> 6. Applying data cleaning steps to data <\/h2> <\/div>","a4df1674":"**Below is a helper Function which generates random colors which can be used to give different colors to the plots.**","07b1bc11":"<div id = 8><h2>8. Finding Most common words Sentiments Wise<\/h2><\/div>\n\nLet's look at the most common words in different sentiments","8b7fce4e":"<div id=10.3><h2>10.3 WordCloud for Tweets with negative sentiment<\/h2> <\/div>","4bfa00fe":"<b> Tokenisation <\/b>","49829537":"Since we have words like 's', 'gre' which do not really mean anything, we will remove them and find the most common words","9e16ad23":"Cleaning Regex Expressions from data","43aa881c":"  <div id=9.1><h2>9.1. Number of unique words in tweets with positive sentiment  <\/h2> <\/div>","7209b2bc":"<div id=9><h2>9. Number of Unique Words in tweets of each type of sentiment <\/h2> <\/div>\n\nWe will look at number of unique words in each type of tweet with different sentiments:\n* Positive\n* Negative\n* Neutral"}}