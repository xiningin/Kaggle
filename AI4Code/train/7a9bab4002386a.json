{"cell_type":{"996eff8b":"code","f0fd89c1":"code","3156bfd8":"code","4df5272e":"code","6c7cce83":"code","2cf9110b":"code","62c6efaf":"code","4ea2f2d4":"code","98b13f83":"code","04986715":"code","7db0b645":"code","46762969":"code","e62be265":"code","47326cd4":"code","0b446344":"code","41beb003":"code","bba28e82":"code","fad28da3":"code","9f053e52":"code","d110e0d4":"code","6544830e":"code","d2c03798":"code","c2e587e3":"code","628e2b11":"code","143fa268":"code","95745a9b":"code","9f80c3db":"markdown","7c41ab74":"markdown","fb27809a":"markdown","9c74caca":"markdown","715f6e32":"markdown","eab1574d":"markdown","92787216":"markdown","2e1fa551":"markdown","3c4655e2":"markdown","a9947999":"markdown","23a78cc8":"markdown","0b64d842":"markdown","26e7a5d3":"markdown","f6079463":"markdown","3340170b":"markdown","43c670e2":"markdown"},"source":{"996eff8b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f0fd89c1":"df=pd.read_csv(\"..\/input\/StudentsPerformance.csv\")","3156bfd8":"df.info() ##\u00a0Lets see what we have in the data","4df5272e":"df.gender=[1 if each==\"male\" else 0for each in df.gender]","6c7cce83":"needed_data=df.drop([\"race\/ethnicity\",\"parental level of education\"\n                    ,\"lunch\", \"test preparation course\"], axis=1)","2cf9110b":"needed_data.head() # This one includes features which we want.\n","62c6efaf":"\nmale=needed_data[needed_data.gender==1]\nfemale=needed_data[needed_data.gender==0]\n#plt.plot(female[\"math score\"])\n#plt.show()\n\nplt.plot(female[\"math score\"],color=\"red\")\nplt.plot(male[\"math score\"],color=\"blue\")\nplt.show()\n","4ea2f2d4":"y=needed_data.gender.values\nx_Data=needed_data.drop([\"gender\"], axis=1)\n\nx=(x_Data-np.min(x_Data))\/(np.max(x_Data)-np.min(x_Data)) ## normalization\n","98b13f83":"from sklearn.model_selection import train_test_split\n\nx_train, x_test , y_train, y_test =train_test_split(x,y, test_size=0.15, random_state=42)\n\nx_train=x_train.T\nx_test=x_test.T\ny_train=y_train.T\ny_test=y_test.T","04986715":"def weights_and_bias(dimension): ## function for initializing of weight and bias\n    w=np.full((dimension,1),0.01)\n    b=0.0\n    return w,b","7db0b645":"def sigmoid (z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","46762969":"def forwardAndBackwardProg (w,b,x_train,y_train):\n\n    z=np.dot(w.T,x_train) + b\n    y_head=sigmoid(z)\n\n    loss=-(y_train*np.log(y_head)+(1-y_train)*np.log(1-y_head))\n    cost=(np.sum(loss))\/x_train.shape[1]\n\n    derivative_weight=np.dot(x_train,((y_head-y_train).T))\/x_train.shape[1]\n    derivative_bias=np.sum(y_head-y_train)\/x_train.shape[1]\n\n    gradients= {\"d_weight\" :derivative_weight , \"d_bias\" : derivative_bias}\n\n    return gradients,cost\n    ","e62be265":"def update(w, b, x_train, y_train, learning_rate , numOfIteration):\n    index=[]\n    cost_list=[]\n    cost_list2=[]\n    \n    for i in range(numOfIteration):\n        gradients,cost=forwardAndBackwardProg (w,b,x_train,y_train)\n        cost_list.append(cost)\n        \n        w=w-learning_rate * gradients[\"d_weight\"]\n        b=b-learning_rate * gradients[\"d_bias\"]\n        if i%100==0:\n            \n            index.append(i)\n            cost_list2.append(cost)\n            print(\"Cost after iteration %i : %f\" %(i,cost))\n        \n    parameters={\"weight\" : w , \"bias\" : b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation=\"vertical\")\n    plt.xlabel(\"Iteration\")\n    plt.ylabel(\"cost\")\n    plt.title(\"Cost graph\")\n    plt.show()\n    return parameters,gradients,cost_list\n\n        \n        \n        ","47326cd4":"def predict(w,b, x_test):\n    z=sigmoid(np.dot(w.T,x_test))\n    y_prediction=np.zeros((1,x_test.shape[1]))\n    \n    for i in range (z.shape[1]):\n        if z[0,i] >=0.5:\n            y_prediction[0,i]=1\n        else:\n            y_prediction[0,i]=0\n    return y_prediction\n        ","0b446344":"def logistic_reg(x_train,x_test,y_train,y_test,learning_rate,numOfIteration):\n    dimension=x_train.shape[0]\n    w,b= weights_and_bias(dimension)\n    parameters,gradients,cost_list=update(w, b, x_train, y_train, learning_rate , numOfIteration) \n    y_prediction_test=predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    print(\"test accuracy:{} %\" .format(100-np.mean(np.abs(y_prediction_test-y_test))*100))\n","41beb003":"logistic_reg(x_train,x_test,y_train,y_test,learning_rate=1,numOfIteration=300)\n","bba28e82":"from sklearn.model_selection import train_test_split\nx_train, x_test , y_train, y_test =train_test_split(x,y, test_size=0.15, random_state=42)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=3) #n_neighbor is k\nknn.fit(x_train,y_train)\nprediction=knn.predict(x_test)\nknn.score(x_test,y_test)","fad28da3":"from sklearn.model_selection import train_test_split\nx_train, x_test , y_train, y_test =train_test_split(x,y, test_size=0.15, random_state=42)\n\nfrom sklearn.svm import SVC\nsvm=SVC(random_state=42)\nsvm.fit(x_train,y_train)\nsvm.score(x_test,y_test)\n","9f053e52":"from sklearn.model_selection import train_test_split\nx_train, x_test , y_train, y_test =train_test_split(x,y, test_size=0.15, random_state=42)\n\nfrom sklearn.naive_bayes import GaussianNB\n\nnb=GaussianNB()\nnb.fit(x_train,y_train)\nnb.score(x_test,y_test)","d110e0d4":"from sklearn.model_selection import train_test_split\nx_train, x_test , y_train, y_test =train_test_split(x,y, test_size=0.15, random_state=42)\n\nfrom sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier()\ndt.fit(x_train,y_train)\ndt.score(x_test,y_test)","6544830e":"from sklearn.model_selection import train_test_split\nx_train, x_test , y_train, y_test =train_test_split(x,y, test_size=0.15, random_state=42)\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(n_estimators=200, random_state=42)   # 200 sub sample\nrf.fit(x_train,y_train)\nrf.score(x_test,y_test)","d2c03798":"y_pred=knn.predict(x_test)\ny_true=y_test\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_true,y_pred)\n\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5, linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.show()\n","c2e587e3":"y_pred=svm.predict(x_test)\ny_true=y_test\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_true,y_pred)\n\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5, linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.show()\n\n","628e2b11":"y_pred=nb.predict(x_test)\ny_true=y_test\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_true,y_pred)\n\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5, linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.show()\n\n","143fa268":"y_pred=dt.predict(x_test)\ny_true=y_test\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_true,y_pred)\n\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5, linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.show()\n\n","95745a9b":"y_pred=rf.predict(x_test)\ny_true=y_test\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_true,y_pred)\n\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5, linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.show()\n\n","9f80c3db":"Also we can use KNN Algorithm for this data\n\n","7c41ab74":"78 % with Decision Three and the last algorithm is Random Forest","fb27809a":"And finally, Confusion Matrix of Random Forest","9c74caca":"As we can see from the upper graph, it is hard to estimate values.","715f6e32":"Confusion Matrix of svm","eab1574d":"Confusion Matrix of Naive bayes","92787216":"We can start with logistic regression.","2e1fa551":"In this kernell, we will see some of the machine learning algorithms and their performances.","3c4655e2":"59 % accuracy with Naive Bayes classification, the other algorithm in the below is decision three classification","a9947999":"With logistic regression our test accuracy is test accuracy:58.666666666666664 % Actually this ratio is little bit low.","23a78cc8":"We have found 90% with SVM, next algorithm is Naive Bayes","0b64d842":"After the normalization process we need to determine our train and test datas.\n","26e7a5d3":"Random forest has 86% accuracy, so when we want to make a list of all classification algorithms we have:\n\n* 1. SVM = 90%\n*  2.KNN= 87%\n*  3.Random Forest=86%\n*   4.Decision Tree=80%\n*  5.Naive Bayes=59%\n*  6.Logistic Regression=58%\n","f6079463":"Confusion Matrix of Decision Tree","3340170b":"In the upper part we found 87 % accuracy with KNN.  We can also show accuracy with SVM(Support Vector Machine)\n","43c670e2":"Furthermore we can plot their confusion matrixes for detailed information. In the below you can find the confusion matrix of SVM.\n"}}