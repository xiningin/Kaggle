{"cell_type":{"60833d42":"code","81f78fdc":"code","d1c6cc35":"code","faea60e8":"code","af202333":"code","8262cafa":"code","4b45c663":"code","d54502c3":"code","68f573a8":"code","c993ae8c":"code","ddc2c1d9":"code","404a47d9":"code","8ba2a512":"code","8c26e981":"code","623c23c6":"code","39a77757":"code","8176bb45":"code","3801a98b":"code","56c80887":"code","1911cb96":"code","5a453ff4":"code","41819998":"code","cc7da2ba":"code","43e3fe56":"code","1b61e43e":"code","bfa8c40a":"code","e0618abd":"code","586143b4":"code","55f54e98":"code","a371269a":"code","f0e0756e":"code","9cbcc805":"code","d05c5a00":"code","ec13cd38":"code","22442854":"code","8cff179a":"code","c6b4cedc":"code","5ad6b0e0":"code","5614a1de":"code","f94930a4":"code","9e13ea0e":"code","935d1746":"code","0c0f8c9c":"code","d14069ce":"code","c1198208":"code","757f72da":"code","464ca364":"code","2fcf2a2d":"code","b72faec4":"code","a08bb746":"code","f6811b5b":"code","ab63a74f":"code","b45891a3":"code","14e05900":"code","af24e99e":"code","3790addd":"markdown","c1118bae":"markdown","eba2487c":"markdown","e6203501":"markdown","885c3d2c":"markdown","b692768a":"markdown","98d9c60c":"markdown","ac633d70":"markdown","9286a207":"markdown","27bdd8c4":"markdown","ff85b396":"markdown"},"source":{"60833d42":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn   # MAchine Learning Library for DS\nimport seaborn as sns # Visuallashdirma ucun\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score # Metricler modelin outputunu qiymetlendirmek ucun\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler # preproseesingde komekci modullar\nimport matplotlib.pyplot as plt # Vizuallashdirma ucun\nimport warnings # Notbukdaki xetalrin ortulmesi ucun\nwarnings.filterwarnings('ignore')\n","81f78fdc":"# Importing Data Set\n#df = sns.load_dataset('diamonds', index_col=0) # Datanin Seaborn kitabxanasindan cagirilmasi\ndf = pd.read_csv('..\/input\/diamonds\/diamonds.csv', index_col=0)","d1c6cc35":"df.head(10) # First 10 Features","faea60e8":"df.shape # how many columns and rows do we have ? ","af202333":"df.info # Full info about Data","8262cafa":"df.dtypes # Feature types","4b45c663":"df.describe()","d54502c3":"'''fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n\nfig.suptitle('Data Setdeki Outliersler')\n\nsns.boxplot(ax=axes[0, 0], data = df['price'])\nsns.boxplot(ax=axes[0, 1], data = df['depth'])\nsns.boxplot(ax=axes[0, 2], data = df['table'])\nsns.boxplot(ax=axes[1, 0], data = df['x'])\nsns.boxplot(ax=axes[1, 1], data = df['y'])\nsns.boxplot(ax=axes[1, 2], data = df['z']) '''\nsns.boxplot(df['z'])\n","68f573a8":"sns.boxplot(df['price'])","c993ae8c":"df.describe() # Statistical info","ddc2c1d9":"df.describe().T # Transpose to see it even better","404a47d9":"# using Quartile Method to eleminate Outliers in Numeric columns\nnumeric_columns = ['price','x','depth','table','z','y']\nfor c in numeric_columns:\n    Q1 = df[c].quantile(0.25)\n    Q3 = df[c].quantile(0.75)\n    IQR = Q3-Q1\n    df = df[~((df[c] < (Q1 - 1.5 * IQR)) | (df[c] > (Q3 + 1.5 * IQR)))] \n    ","8ba2a512":"df.describe() # Outlierlersiz data","8c26e981":"sns.boxplot(df['price'])","623c23c6":"# For seeing Corolation\nplt.matshow(df.corr())\nplt.show() ","39a77757":"# Even better visualizartion\ncmap = sns.diverging_palette(70,20,s=50, l=40, n=6, as_cmap=True)\nmatrix_cor_ucun = df.corr()\nf, ax = plt.subplots(figsize=(12,12))\nsns.heatmap(matrix_cor_ucun,cmap=cmap,annot=True)","8176bb45":"df.describe","3801a98b":"df.isnull().sum() # Lets see how many null values we have in our dataset","56c80887":"df['cut'].unique() # how many option do we have in specific column","1911cb96":" df['cut'].astype('category').cat.codes # change them to cagegory type","5a453ff4":"# We know info soo we do not need Label or OneHotEncoder\ncut_class_dict = {'Fair': 1, 'Good': 2, 'Very Good': 3, 'Premium': 4, 'Ideal': 5, }\nclarity_dict   = {'SI2': 2, 'SI1': 3, 'VS1': 5, 'VS2': 4, 'VVS2': 6, 'VVS1': 7, 'I1': 1, 'IF': 8 }\ncolor_dict     = {'J': 1, 'I': 2, 'H': 3, 'G': 4, 'F': 5, 'E': 5,'D': 5,} \n\ndf['cut'] = df['cut'].map(cut_class_dict)\ndf['clarity'] = df['clarity'].map(clarity_dict)\ndf['color'] = df['color'].map(color_dict)\n","41819998":"df.head()","cc7da2ba":"import sklearn\nfrom sklearn import preprocessing # Preproseesing tools","43e3fe56":"df = sklearn.utils.shuffle(df) # For shuffle Data","1b61e43e":"df.head()","bfa8c40a":"X = df.drop('price',axis=1).values # X use all columns except \"Price\" \n#X = preprocessing.scale(X)\ny = df['price'].values # Y use 'Price' 's values","e0618abd":"X.shape # Shape of X ( the column numbers decreased from 9 to 8 bcs we drop price column)","586143b4":"X","55f54e98":"y.shape","a371269a":"y","f0e0756e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.20, random_state =42) # test olunacaq datanin hissesini gosterir ve bolur","9cbcc805":"# Lets Scale our Data\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","d05c5a00":"X_test.size","ec13cd38":"#Linear regression ile\nfrom sklearn.linear_model import LinearRegression\nLR = LinearRegression()\nLR.fit(X_train,y_train)","22442854":"print('Linear Regression')\ny_pred = LR.predict(X_test)\nprint(f'MAE: {mean_absolute_error(y_test, y_pred)}')\nprint(f'MSE: {mean_squared_error(y_test, y_pred)}')\nprint(f'R2:  {r2_score(y_test, y_pred)}') ","8cff179a":"from sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor","c6b4cedc":"regressor = DecisionTreeRegressor(random_state=0).fit(X_train,y_train)","5ad6b0e0":"print('DecisionTreeRegressor')\ny_pred = regressor.predict(X_test)\nprint(f'MAE: {mean_absolute_error(y_test, y_pred)}')\nprint(f'MSE: {mean_squared_error(y_test, y_pred)}')\nprint(f'R2:  {r2_score(y_test, y_pred)}') ","5614a1de":"from sklearn.ensemble import GradientBoostingRegressor as GB\nimport xgboost as xgb","f94930a4":"ne = 2000\nMgb = GB(n_estimators=ne)\nMxbg = xgb.XGBRegressor(n_estimators=ne)","9e13ea0e":"Mgb.fit(X_train, y_train)","935d1746":"print('XGBoosting')\ny_pred = Mgb.predict(X_test)\nprint(f'MAE: {mean_absolute_error(y_test, y_pred)}')\nprint(f'MSE: {mean_squared_error(y_test, y_pred)}')\nprint(f'R2:  {r2_score(y_test, y_pred)}') ","0c0f8c9c":"Mxbg.fit(X_train, y_train)","d14069ce":"print('XG Boosting')\ny_pred = Mxbg.predict(X_test)\nprint(f'MAE: {mean_absolute_error(y_test, y_pred)}')\nprint(f'MSE: {mean_squared_error(y_test, y_pred)}')\nprint(f'R2:  {r2_score(y_test, y_pred)}') ","c1198208":"#Neural Network with Sklearn\nfrom sklearn.neural_network import MLPRegressor","757f72da":"nt = MLPRegressor()\nnt.fit(X_train,y_train)\ny_pred = nt.predict(X_test)","464ca364":"print('MLPRegressor')\nprint(f'MAE: {mean_absolute_error(y_test, y_pred)}')\nprint(f'MSE: {mean_squared_error(y_test, y_pred)}')\nprint(f'R2:  {r2_score(y_test, y_pred)}') ","2fcf2a2d":"import tensorflow as tf","b72faec4":"\ntf.random.set_seed(42) \nDiamonds_model =  tf.keras.Sequential([\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1)\n    ])","a08bb746":"Diamonds_model.compile(loss=tf.keras.losses.mae, # mae is short for mean absolute error\n            optimizer=tf.keras.optimizers.Adam(0.002), # Adam\n            metrics=[\"mae\", \"mse\"])","f6811b5b":"history = Diamonds_model.fit(X_train,y_train, epochs=30) # validation_split=0.2,","ab63a74f":"pd.DataFrame(history.history).plot()\nplt.xlabel('epochs')","b45891a3":"Diamonds_model.summary()","14e05900":"print('With Tf Keras')\ny_pred = Diamonds_model.predict(X_test)\nprint(f'MAE: {mean_absolute_error(y_test, y_pred)}')\nprint(f'MSE: {mean_squared_error(y_test, y_pred)}')\nprint(f'R2:  {r2_score(y_test, y_pred)}') ","af24e99e":"for X,y in zip(X_test, y_test):\n    Difference = y - Mxbg.predict([X])[0]\n    print(f'Model: {Mxbg.predict([X])[0]}, Actual: {y}, Xeta: {Difference:.1f}')","3790addd":"## Preprosessing","c1118bae":"# Desicion Tree Regressor istifade Ederek","eba2487c":"## Lets see Outliers ","e6203501":"# Corr Matrixi","885c3d2c":"# With Keras","b692768a":"# Lets try the best Model","98d9c60c":"### Mappting Categorical columns into numeric columns bcs we have knowledge about each column ( Given in description)","ac633d70":"# Simple Linear Regression","9286a207":"# EDA","27bdd8c4":"# XGBoosting ve GradientBoostingRegressordan Istifade","ff85b396":"### Info about Features <br>\n1. Price : Qiymet Us dollari ile (\\$326--\\$18,823)\n2. Color : Color is an indicator colum J is the worst and D is the best<br>\n3. clarity\t : Clarity colum also about the other elemets mixing value I1 is the worst but IF is best <br>\n4. Depth : Depth colum is realed with x and y size of the diaomns <br>\n5. Tabel : Table is Width of top of diamond relative to widest point (43--95)<br>\n6. X : x islength(0--10.74)mm <br>\n7. Y : y is width(0--58.9)mm <br>\n8. Z : z is depth(0--31.8)mm <br>\n9. Carat : carat colum is the best indicator shows the weight of the diamond equivalent to 200 mg. <br>"}}