{"cell_type":{"39628f50":"code","53583e9a":"code","d104ee77":"code","4b6418da":"code","869616a4":"code","467bd047":"code","1382d92d":"code","a4ed9a07":"code","30a93b20":"code","51d81a78":"code","4ca1cf6b":"code","c603cee4":"code","bd67b1cb":"code","5fce910a":"code","570e2471":"code","0d24b29a":"code","5e07d272":"code","70e4f175":"code","6a32206c":"code","a8580572":"code","6bbc15d7":"code","dc13c604":"code","d4595d33":"code","a5dea11b":"code","e0f6931a":"code","145b84a0":"code","4a44598e":"code","e1e431fc":"code","36ca5561":"code","b284b5bb":"code","89b37609":"code","aeca38b0":"code","27b460da":"code","b54085ed":"code","2c3d21e0":"code","0a724256":"code","fe3163c6":"code","54fc0cf4":"code","fb63b708":"code","f62a6274":"code","9d45a6e9":"code","d83a9166":"code","12523d20":"code","0627e184":"code","529682c9":"code","d81f852b":"code","ca2b9d1e":"code","7fd6255d":"code","be6fe43f":"code","7445eb11":"code","befb70cd":"code","6ef7c2d1":"code","47952911":"code","b58262ec":"code","376dbe90":"code","69e5c911":"code","b1f1b4b8":"code","fd04166b":"code","8139f00f":"code","b03741b5":"markdown","266f26b4":"markdown","ad6dd906":"markdown","8e05446b":"markdown","748f9035":"markdown","09d1cee1":"markdown","8a2c4e06":"markdown","98f90f3a":"markdown","19fdb3ca":"markdown","8902effb":"markdown","f55e303e":"markdown","aef0275a":"markdown","a40de479":"markdown","66703f7d":"markdown","3f4fdce6":"markdown","b5e2969b":"markdown","85632724":"markdown","b8fcc87b":"markdown","4d8df32c":"markdown","ca1f73bc":"markdown","beb573c8":"markdown","99a15858":"markdown","17a80f14":"markdown","9d20fd5c":"markdown","56a80e93":"markdown","88845d77":"markdown","66099230":"markdown","e89973d0":"markdown","a235bf33":"markdown","17726146":"markdown","78e97c7e":"markdown","e32762ed":"markdown","521e1708":"markdown","c333de96":"markdown","c40bfed4":"markdown","f59e8eed":"markdown","e3861cf7":"markdown","d5eb4ac8":"markdown","9607e0d1":"markdown","d04cd61d":"markdown","1789ff04":"markdown","e1f889ba":"markdown","680c852b":"markdown","aa128209":"markdown","248a95b1":"markdown","73af0da0":"markdown"},"source":{"39628f50":"import os \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nimport missingno as msno\nfrom scipy import stats\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","53583e9a":"from collections import Counter\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve,auc\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\nfrom xgboost import XGBClassifier\nfrom imblearn.combine import SMOTETomek\nimport optuna\nimport lightgbm as lgbm","d104ee77":"MAIN_PATH = \"..\/input\/song-popularity-prediction\/\"\ntdf = pd.read_csv(MAIN_PATH + 'train.csv')\nTdf = pd.read_csv(MAIN_PATH + 'test.csv')\nSdf = pd.read_csv(MAIN_PATH + 'sample_submission.csv')","4b6418da":"tdf.head(3)","869616a4":"Tdf.head(3)","467bd047":"\nprint(tdf['song_popularity'].describe()[0:3],'\\nunique \\\n' , tdf['song_popularity'].nunique(),'\\nnull',tdf['song_popularity'].isnull().sum())\n\nsns.kdeplot(data=tdf,x='song_popularity' ,shade=True ,color='r')\nplt.show()\na=tdf['song_popularity'][tdf['song_popularity']==0].count() \nb =tdf['song_popularity'][tdf['song_popularity']==1].count()\nprint(f\"Total 0s : {a}\\nTotal 1s : {b} \\\n\\nTotal bias towards 0s of predictor: {(a-b)\/(a+b)*100}%\")\n","1382d92d":"Sdf.head()","a4ed9a07":"# this will help to copy paste column names while we need  \ncol = list(tdf.columns)\nprint(col)","30a93b20":"tdf.info()","51d81a78":"msno.matrix(tdf)","4ca1cf6b":"msno.matrix(Tdf)","c603cee4":" missing_col =['song_duration_ms', 'acousticness', 'danceability', 'energy', 'instrumentalness', \n               'key', 'liveness', 'loudness']\ndef feature_plotter(data , col_list):\n    fig , ax = plt.subplots(4 ,2 ,figsize=(20,16))\n    for i , c in enumerate(col_list):\n        if i< 4:\n            sns.histplot(data=data , x = c  , ax= ax[i,0] ) \n            ax[i,0].axvline(data[c].median(),color='r', linestyle=':')\n            ax[i,0].axvline(np.mean(data[c]),color='b', linestyle='-')\n#             ax[i,0].axvline(stats.mode(data[c])[0][0],color='g', linestyle='-.',linewidth=2)\n          \n        else : \n            sns.histplot(data=data , x = c , ax= ax[i-4,1] )\n            ax[i-4,1].axvline(data[c].median(),color='r', linestyle=':')\n            ax[i-4,1].axvline(np.mean(data[c]),color='b', linestyle='-')\n#             ax[i-4,1].axvline(stats.mode(data[c])[0][0],color='g', linestyle='-.')\n\nfeature_plotter(tdf ,missing_col)","bd67b1cb":"pd.isnull(tdf['danceability'].loc[4])","5fce910a":"from numpy.random import default_rng\nrng = default_rng(1000)\nmissing_col =['song_duration_ms', 'acousticness', 'danceability', 'energy', 'instrumentalness', \n               'key', 'liveness', 'loudness']\ndef custom_imputer(t_df ,size = 40000):\n    for i in range(0 , size):\n        if pd.isnull(t_df['song_duration_ms'].loc[i]) : t_df['song_duration_ms'].loc[i] = rng.integers(low = 100000,high=300000) \n        if pd.isnull(t_df['acousticness'].loc[i]) :     t_df['acousticness'].loc[i]     = rng.integers(low = 0  ,high=8)*0.1 \n        if pd.isnull(t_df['danceability'].loc[i]) :     t_df['danceability'].loc[i]     = rng.integers(low = 400  ,high=800)*0.001 \n        if pd.isnull(t_df['energy'].loc[i])  :          t_df['energy'].loc[i]           = rng.integers(low = 400 ,high=900)*0.001  \n        if pd.isnull(t_df['instrumentalness'].loc[i]) : t_df['instrumentalness'].loc[i] = rng.integers(low = 0  ,high=1000)*0.0001 \n        if pd.isnull(t_df['key'].loc[i]) :              t_df['key'].loc[i]              = rng.integers(low = 0  ,high=10) \n        if pd.isnull(t_df['liveness'].loc[i]) :         t_df['liveness'].loc[i]         = rng.integers(low = 1000 ,high=3000)*0.0001 \n        if pd.isnull(t_df['loudness'].loc[i]) :         t_df['loudness'].loc[i]         = rng.integers(low = 60 ,high=120)*(-0.1)\n    return t_df\n\nt_df = custom_imputer(tdf.copy(),40000)","570e2471":"t_df.head()","0d24b29a":"msno.matrix(t_df)","5e07d272":"feature_plotter(t_df , missing_col)","70e4f175":"t_df.head(3)","6a32206c":"print(col)\nprint(len(col))","a8580572":"\nfig , ax = plt.subplots(5 ,3 ,figsize=(25,13))\n\n\nfor i , c in enumerate(col):\n        if i<5: sns.boxplot(data = t_df ,x=c, hue = \"song_popularity\" , ax=ax[i][0])\n        elif i<10 : sns.boxplot(data = t_df ,x=c, hue = \"song_popularity\" , ax=ax[i-5][1])\n        else : sns.boxplot(data = t_df ,x=c, hue = \"song_popularity\" , ax=ax[i-10][2])\nplt.tight_layout(pad=3)\nplt.show()\n\n","6bbc15d7":"feat = [col for col in t_df.columns if col not in (\"id\", \"song_popularity\")]\nscores = []","dc13c604":"X = t_df[feat]\ny = t_df['song_popularity']","d4595d33":"# finding the column index of categorical columns\ncat_cols = [\"key\", \"audio_mode\", \"time_signature\"]\n\ncat_indices = []\nfor col in cat_cols:\n    idx = list(X.columns).index(col)\n    cat_indices.append(idx)\ncat_indices","a5dea11b":"def run(trial, data=X,target=y):\n    \n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.10, random_state=42)\n    \n    params = {\n                'metric': 'auc', \n                'random_state': 22,\n                'n_estimators': 4000,\n                'boosting_type': trial.suggest_categorical(\"boosting_type\", [\"gbdt\"]),\n                'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-3, 10.0),\n                'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-3, 10.0),\n                'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n                'bagging_fraction': trial.suggest_categorical('bagging_fraction', [0.6, 0.7, 0.80]),\n                'feature_fraction': trial.suggest_categorical('feature_fraction', [0.6, 0.7, 0.80]),\n                'learning_rate': trial.suggest_categorical('learning_rate', [0.005, 0.01, 0.02, 0.03, 0.05, 0.1]),\n                'max_depth': trial.suggest_int('max_depth', 2, 12, step=1),\n                'num_leaves' : trial.suggest_int('num_leaves', 13, 148, step=5),\n                'min_child_samples': trial.suggest_int('min_child_samples', 1, 96, step=5),\n            }\n    \n    clf = lgbm.LGBMClassifier(**params)  \n    clf.fit(X_train, y_train,\n            eval_set=[(X_valid, y_valid), (X_train, y_train)],\n            categorical_feature=cat_indices,\n            callbacks=[lgbm.log_evaluation(period=100), \n                       lgbm.early_stopping(stopping_rounds=100)\n                      ],\n           )\n    \n    y_proba = clf.predict_proba(X_valid)[:, 1]\n    auc = roc_auc_score(y_valid, y_proba)\n    return auc","e0f6931a":"study = optuna.create_study(direction='maximize')\nstudy.optimize(run, n_trials=100)","145b84a0":"study.best_params","4a44598e":"print(\"Best parameters:\")\nprint(\"*\"*50)\nfor param, val in study.best_trial.params.items():\n    print(f\"{param} :\\t {val}\")\nprint(\"*\"*50)\nprint(f\"Best AUC score: {study.best_value}\")","e1e431fc":"X = t_df[feat]\ny = t_df['song_popularity']","36ca5561":"feature_plotter(Tdf,missing_col)","b284b5bb":"#Tdf=Tdf.drop(['song_popularity'],axis=1)\nT_df = custom_imputer(Tdf ,10000)","89b37609":"msno.matrix(T_df)","aeca38b0":"X_test =T_df[feat]","27b460da":"skf = StratifiedKFold(n_splits=8, shuffle=True, random_state=42)","b54085ed":"lgbm_params = study.best_params\npredictions = []\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    \n    X_train, X_valid = X.iloc[train_idx], X.iloc[val_idx]\n    y_train , y_valid = y.iloc[train_idx], y.iloc[val_idx]\n    \n    model = lgbm.LGBMClassifier(**lgbm_params)\n    \n    model.fit(X_train, y_train,\n            eval_set=[(X_valid, y_valid), (X_train, y_train)],\n            categorical_feature=cat_indices,\n            callbacks=[lgbm.log_evaluation(period=100), \n                       lgbm.early_stopping(stopping_rounds=500)\n                      ],\n           )\n    \n    valid_preds = model.predict_proba(X_valid)[:,1]\n    \n    score = roc_auc_score(y_valid, valid_preds)\n    scores.append(score)\n    print(\"*\"*200)\n    print(f\"Fold: {fold}, AUC: {score}\")\n    print(\"*\"*200)\n    test_pred = model.predict_proba(X_test)[:,1]\n    predictions.append(test_pred)","2c3d21e0":"preds = np.mean(np.column_stack(predictions), axis=1)\nsub = pd.DataFrame(columns = ['id', 'song_popularity'])\nsub['id'] = T_df['id']\nsub['song_popularity'] = preds\nsub","0a724256":"submission =[]\nsubmission.append(preds)","fe3163c6":"submission","54fc0cf4":"lgbm_params ={'boosting_type': 'gbdt',\n 'lambda_l1': 0.10332366305772285,\n 'lambda_l2': 0.854062211829853,\n 'colsample_bytree': 1.0,\n 'bagging_fraction': 0.7,\n 'feature_fraction': 0.6,\n 'learning_rate': 0.02,\n 'max_depth': 12,\n 'num_leaves': 103,\n 'min_child_samples': 36}\n \npredictions = []\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    \n    X_train, X_valid = X.iloc[train_idx], X.iloc[val_idx]\n    y_train , y_valid = y.iloc[train_idx], y.iloc[val_idx]\n    \n    model = lgbm.LGBMClassifier(**lgbm_params)\n    \n    model.fit(X_train, y_train,\n            eval_set=[(X_valid, y_valid), (X_train, y_train)],\n            categorical_feature=cat_indices,\n            callbacks=[lgbm.log_evaluation(period=100), \n                       lgbm.early_stopping(stopping_rounds=500)\n                      ],\n           )\n    \n    valid_preds = model.predict_proba(X_valid)[:,1]\n    \n    score = roc_auc_score(y_valid, valid_preds)\n    scores.append(score)\n    print(\"*\"*200)\n    print(f\"Fold: {fold}, AUC: {score}\")\n    print(\"*\"*200)\n    test_pred = model.predict_proba(X_test)[:,1]\n    predictions.append(test_pred)","fb63b708":"preds = np.max(np.column_stack(predictions), axis=1)\nsub_0 = pd.DataFrame(columns = ['id', 'song_popularity'])\nsub_0['id'] = T_df['id']\nsub_0['song_popularity'] = preds\nsub_0","f62a6274":"submission.append(preds)","9d45a6e9":"#  sub_0.to_csv(\"submission1_max_ceil_150.csv\",index=False)","d83a9166":"preds = np.mean(np.column_stack(predictions), axis=1)\nsub = pd.DataFrame(columns = ['id', 'song_popularity'])\nsub['id'] = T_df['id']\nsub['song_popularity'] = preds\nsub","12523d20":"submission.append(preds)","0627e184":"#  sub.to_csv(\"submission1_150_mean.csv\",index=False)","529682c9":"lgbm_params ={'boosting_type': 'gbdt',\n 'lambda_l1': 0.006108477977265987,\n 'lambda_l2': 2.4648575263404116,\n 'colsample_bytree': 0.5, \n 'bagging_fraction': 0.7, \n 'feature_fraction': 0.6, \n 'learning_rate': 0.005, \n 'max_depth': 11, \n 'num_leaves': 93, \n 'min_child_samples': 11}\n\npredictions = []\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    \n    X_train, X_valid = X.iloc[train_idx], X.iloc[val_idx]\n    y_train , y_valid = y.iloc[train_idx], y.iloc[val_idx]\n    \n    model = lgbm.LGBMClassifier(**lgbm_params)\n    \n    model.fit(X_train, y_train,\n            eval_set=[(X_valid, y_valid), (X_train, y_train)],\n            categorical_feature=cat_indices,\n            callbacks=[lgbm.log_evaluation(period=100), \n                       lgbm.early_stopping(stopping_rounds=500)\n                      ],\n           )\n    \n    valid_preds = model.predict_proba(X_valid)[:,1]\n    \n    score = roc_auc_score(y_valid, valid_preds)\n    scores.append(score)\n    print(\"*\"*200)\n    print(f\"Fold: {fold}, AUC: {score}\")\n    print(\"*\"*200)\n    test_pred = model.predict_proba(X_test)[:,1]\n    predictions.append(test_pred)","d81f852b":"preds = np.mean(np.column_stack(predictions), axis=1)\nsub1 = pd.DataFrame(columns = ['id', 'song_popularity'])\nsub1['id'] = T_df['id']\nsub1['song_popularity'] = preds\nsub1","ca2b9d1e":"submission.append(preds)","7fd6255d":"# sub1.to_csv(\"submission2.csv\",index=False)","be6fe43f":"lgbm_params = {\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    'subsample': 0.95312,\n    'learning_rate': 0.001635,\n    \"max_depth\": 3,\n    \"feature_fraction\": 0.2256038826485174,\n    \"bagging_fraction\": 0.7705303688019942,\n    \"min_child_samples\": 290,\n    \"reg_alpha\": 14.68267919457715,\n    \"reg_lambda\": 66.156,\n    \"max_bin\": 772,\n    \"min_data_per_group\": 177,\n    \"bagging_freq\": 1,\n    \"cat_smooth\": 96,\n    \"cat_l2\": 17,\n    \"verbosity\": -1,\n    'random_state':42,\n    'n_estimators':5000,\n    'colsample_bytree':0.1107\n    }\npredictions = []\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    \n    X_train, X_valid = X.iloc[train_idx], X.iloc[val_idx]\n    y_train , y_valid = y.iloc[train_idx], y.iloc[val_idx]\n    \n    model = lgbm.LGBMClassifier(**lgbm_params)\n    \n    model.fit(X_train, y_train,\n            eval_set=[(X_valid, y_valid), (X_train, y_train)],\n            categorical_feature=cat_indices,\n            callbacks=[lgbm.log_evaluation(period=100), \n                       lgbm.early_stopping(stopping_rounds=500)\n                      ],\n           )\n    \n    valid_preds = model.predict_proba(X_valid)[:,1]\n    \n    score = roc_auc_score(y_valid, valid_preds)\n    scores.append(score)\n    print(\"*\"*200)\n    print(f\"Fold: {fold}, AUC: {score}\")\n    print(\"*\"*200)\n    test_pred = model.predict_proba(X_test)[:,1]\n    predictions.append(test_pred)","7445eb11":"preds = np.max(np.column_stack(predictions), axis=1)\nsub1_0 = pd.DataFrame(columns = ['id', 'song_popularity'])\nsub1_0['id'] = T_df['id']\nsub1_0['song_popularity'] = preds\nsub1_0","befb70cd":"submission.append(preds)","6ef7c2d1":"preds = np.mean(np.column_stack(predictions), axis=1)\nsub1_1 = pd.DataFrame(columns = ['id', 'song_popularity'])\nsub1_1['id'] = T_df['id']\nsub1_1['song_popularity'] = preds\nsub1_1","47952911":"submission.append(preds)","b58262ec":"# sub1_0.to_csv(\"submission1_5fold_new_params.csv\",index=False)","376dbe90":"lgbm_params ={'boosting_type': 'gbdt',\n 'lambda_l1': 0.007489049841715026,\n 'lambda_l2': 1.3769273854180089,\n 'colsample_bytree': 0.8,\n 'bagging_fraction': 0.7,\n 'feature_fraction': 0.6,\n 'learning_rate': 0.005,\n 'max_depth': 11,\n 'num_leaves': 108,\n 'min_child_samples': 41}\n\npredictions = []\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    \n    X_train, X_valid = X.iloc[train_idx], X.iloc[val_idx]\n    y_train , y_valid = y.iloc[train_idx], y.iloc[val_idx]\n    \n    model = lgbm.LGBMClassifier(**lgbm_params)\n    \n    model.fit(X_train, y_train,\n            eval_set=[(X_valid, y_valid), (X_train, y_train)],\n            categorical_feature=cat_indices,\n            callbacks=[lgbm.log_evaluation(period=100), \n                       lgbm.early_stopping(stopping_rounds=500)\n                      ],\n           )\n    \n    valid_preds = model.predict_proba(X_valid)[:,1]\n    \n    score = roc_auc_score(y_valid, valid_preds)\n    scores.append(score)\n    print(\"*\"*200)\n    print(f\"Fold: {fold}, AUC: {score}\")\n    print(\"*\"*200)\n    test_pred = model.predict_proba(X_test)[:,1]\n    predictions.append(test_pred)","69e5c911":"preds = np.mean(np.column_stack(predictions), axis=1)\nsub2 = pd.DataFrame(columns = ['id', 'song_popularity'])\nsub2['id'] = T_df['id']\nsub2['song_popularity'] = preds\nsub2","b1f1b4b8":"submission.append(preds)","fd04166b":"preds = np.mean(np.column_stack(submission), axis=1)\nfinal_sub = pd.DataFrame(columns = ['id', 'song_popularity'])\nfinal_sub['id'] = T_df['id']\nfinal_sub['song_popularity'] = preds\nfinal_sub","8139f00f":"final_sub.to_csv(\"submission.csv\",index=False)","b03741b5":"#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd","266f26b4":"<a id='sub3'\/><br>\n\n## \ud83d\udca2prediction 3 : overfit 2 ","ad6dd906":"I will say this below code is one of the worst  code that I have implemented \ud83e\udd23\ud83e\udd23","8e05446b":"after watching the final outcome I can say that this re sampling and distribution of data worked fine without altering the original central tendencies\ud83d\ude05,\nIf you like the technique then I need the upvote for this one \ud83d\ude02\ud83d\ude02\n\n#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd","748f9035":"<a id=\"pa\"\/><br>\n\n## \ud83d\udd05 Predictor (`Y`) Analysis ","09d1cee1":"<a id='sub2'\/><br>\n\n## \ud83d\udca2 prediction 2 :LIGHTGBM (overfitting ) ","8a2c4e06":"#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd","98f90f3a":"<a id=\"lgm\"\/><br>\n\n### \u2b55 Model 1 : lightGBM ","19fdb3ca":"<a id=\"oa\"\/><br>\n\n### \u2b55 Outlier analysis \n\nlet's check outliers now\n","8902effb":"<a id=\"fmv\"\/><br>\n\n#### \ud83d\udca2 Fixing Missing Values\n\nlet's check how our original data looks","f55e303e":"These are audio feature , but it's very bad that we got only features of audio and not any audio data \u2639  (\u091c\u093e\u0909\u0926\u094d\u092f\u093e )   it's ok ,it makes problem more easy then \ud83d\ude05\n\n#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd ","aef0275a":"if our msno matrix is completely black then we have done half of the work \ud83d\ude43","a40de479":"<a id='sub4'\/><br>\n\n## \ud83d\udca2 prediction 4 : underfit","66703f7d":"#### \ud83d\udcdaPN : 4\n\ud83d\udcda[PN : 3](https:\/\/www.kaggle.com\/sarabhian\/free-time-learning)","3f4fdce6":"<a id='sub1'\/><br>\n\n## \ud83d\udca2prediction 1 : LightGBM ","b5e2969b":"<a id=\"p\"\/><br>\n\n# \ud83d\udca0 Predictions ","85632724":"<a id=\"mva\"\/><br>\n\n\n### \u2b55 Missing value analysis ","b8fcc87b":"<a id=\"dlp\"\/><br>\n\n# \ud83d\udca0 Data Loading & preparation  \n\nIn this part  we will look for basic EDA and learn plotting","4d8df32c":"<a id =\"welcome\"\/><br>\n# \ud83c\udf40WELCOME! \n\n\nAs usual, we will be talking with our data and our data will be checking our skills , <br>\n##  Table of Content <br>\n* 1. [welcome](#welcome)<br>\n* 2. [Intro](#intro)<br>\n* 3. [Data loading and preparation](#dlp)<br>\n    * 3.1. [Import Libraries](#il)\n    * 3.2. [Import Data](#id)\n    * 3.3. [predictor_analysis](#pa)\n    * 3.4. [sample submission analysis](#ssa)\n* 4. [Analyse Train Data](#atd)\n    * 4.1. [missing value analysis](#mva)\n         * 4.1.a. [visualize missing value](#vmv)\n         * 4.1.b. [fixing missing values](#fmv)\n     * 4.2. [outlier analysis](#oa)       \n* 5. [model](#m)\n     \n     * 5.2. [model 1 : lightGBM](#lgm)\n* 6. [prediction](#p)\n     *  [pred 1](#sub1)\n     *  [pred 2](#sub2)\n     *  [pred 3](#sub3)\n     *  [pred 4](#sub4)\n     *  [final submission](#fs)\n* 7. [conclusion](#c)<br>\nlet's start the game then\ud83c\udfca\u200d  \n\n\n( in the end tell me in comments if you feel tired or scrolling or not \ud83d\ude02)","ca1f73bc":"we can see that the test data is exactly mapped with train data in terms of features so our custom imputer will work fine on test data as well","beb573c8":"there are lot of outliers but we are not going to change them because  these outliers will improve the imputers functioning  while adding missing values to improve the code ","99a15858":"we can see there are lot of missing values in the data in columns `'song_duration_ms', 'acousticness', 'danceability', 'energy', 'instrumentalness', 'key', 'liveness', 'loudness'` \n\nlet's visualize the misssing values\n\n\n#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd","17a80f14":"Now we have to decide by which factor we should fill the missing values? for that let's plot the distributions of missing value cols \n\n#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd\n","9d20fd5c":"let's plot the features of test data to impute it","56a80e93":"yup it's complete black but let's see how our data got distributed , whether it affected the main data or not ","88845d77":"<a id=\"intro\"\/><br>\n\n# \ud83d\udca0 Intro \n\nOk let's understand the goal of the notebook \ud83d\ude09\n\nthis compition is organised by MLspace , \n>**GOAL** find the best model for the given dataset ( as it's not havinig any particular goal it's just compititon to learn)\n<br>\n\n>**Accessories (what we have with us?)** we have 3 files\n > * sample_submission.csv : data on how we have to predict \n > * test.csv :  data for testing our model\n > * train.csv : data for trtaining our model\n \n>**Evaluation(How our submission will be scored)** : we have a metric AUC , roc curves ( from just ROC I can tell  output is binary and our data could be bernoulli distribution and we have to predict the binary classification model\ud83d\ude02, we  can use SVMs , ANN , CNN ,XGB, CATboost ,LightGBM ,... ) \n \n#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd ","66099230":"<a id=\"m\"\/><br>\n\n# \ud83d\udca0 Model \n\n","e89973d0":"#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd \n\nIf you got some good info do like , share and comment ...see you! ","a235bf33":"<a id=\"ssa\"\/><br>\n\n## \ud83d\udd05 Sample Submission analysis ","17726146":"#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd","78e97c7e":"1. There is need of guessing overfitting issue (as we still don't have any guess what is there inside the test data)\n2. Refer EDA first ( read from others notebooks )\n3. Data was very much bad ( we fixed the issue of bad data in this notebook , I hope custom imputer  will fit very well even  on hidden dataset)\n5. averaging all efforts to one final submission \ud83d\ude09\n4. ending this notebook here , \n\nthanks to [isha's Notebook](https:\/\/www.kaggle.com\/isha20\/spp-eda-boosting) for helping me in this code \n\nhope you got an idea of what we have to do here, the model created in the dataset is good but not accurate , we just need to wait untill final output\nlet's meet in next notebook ...\ud83d\ude4b\u200d\u2642\ufe0f","e32762ed":"<a id=\"c\"\/><br>\n\n# \ud83c\udf40 Conclusion ","521e1708":"In the data there are `27.12%` more `0s`  than `1s` ,<br>\nwhat does this mean ? ==> if you just write all 0s in your training data you will still get 63% accuracy on train dataset\ud83d\ude09\n<br>\nok we got some info about how predictor is behaving now let's see what our sample submission needs, afterwards we will start our actual problem solving\n\n#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd ","c333de96":"#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd","c40bfed4":"from train and test data we can see our predictor `y` is `song_popularity` i.e. we have to predict song_popularity for the test set<br>\nok let's check predictor's behaviour ","f59e8eed":"so we have to drop all the columns except `id` and `song_popularity` from the data for final submission (as usuaul)\n\ntill now , we got to know about **what we have to predict ?**, **in which format we have to predict ?** ,now let's see training set and prepare it for model \n\n#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd","e3861cf7":" <a id=\"atd\"\/><br>\n\n## \ud83d\udd05 Analyse Training DATA `X` ","d5eb4ac8":"refer some work in  other running challenges <br>[TPS series](https:\/\/www.kaggle.com\/sarabhian\/free-time-learning)<br>\n[tensorflow GBR](https:\/\/www.kaggle.com\/sarabhian\/gbr-extremely-beginner-level-guide-1)","9607e0d1":"<a id='fs'\/><br>\n\n## \u2b55final Submission :LightGBM :overfit 3  ","d04cd61d":"I think I am not getting the expected line for mode but our goal is to replace `NaN` values so it doesn't matter \nlet's go ahead and copy the data, so that we don't have to reload data if anything wrong happens with our imputing function","1789ff04":"#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd","e1f889ba":"<a id=\"vmv\"\/><br>  \n\n#### \ud83d\udca2 Visualize Missing values ","680c852b":"<a id=\"id\"\/><br>\n\n## \ud83d\udd05 Import data ","aa128209":"white lines shows us the missing values, we need complete pitch black column for good model building","248a95b1":"<a id=\"il\"\/><br>\n\n## \ud83d\udd05 Import libraries  \n\n","73af0da0":"#### \ud83d\uddfd[go to top](#welcome)\ud83d\uddfd"}}