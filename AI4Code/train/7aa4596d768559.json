{"cell_type":{"7a92565a":"code","ebf446d7":"code","cd054d3d":"code","d986f47b":"code","d6880141":"code","8abdf047":"code","7c724e67":"code","03fdb7f2":"code","83a5ef61":"code","69eb1a2c":"code","a47305d3":"code","e3b445cf":"code","ba602521":"code","aca7c4fe":"code","34053900":"code","0495e644":"code","890cbede":"code","cddbeee6":"code","eff2aa10":"code","2cc8bcb2":"code","8cbc0a12":"code","0ec4f498":"code","6222c5df":"code","6f7563b4":"code","eb13e090":"code","910d13da":"code","f4ee948d":"code","8ea7941e":"code","4baed6a7":"code","69e7ac4c":"code","504585f0":"code","95d40dfa":"code","85bde6de":"code","ec3ec6aa":"code","1b61fc79":"code","32f8a73b":"code","8b83956d":"code","2abc5b68":"code","6cc5c0ac":"code","b7f16bcd":"code","11559478":"code","5e025621":"code","f42e1894":"code","d07327ed":"code","897a6fe4":"code","d1decb3f":"code","0d35445e":"code","a9496bb6":"code","cd3bb178":"code","9ba1ae2e":"code","a8145d7b":"code","206a68da":"code","7506b013":"code","1d86021b":"code","f056394e":"code","f6876e57":"code","eec10d49":"code","f7a66845":"code","9db11080":"code","dc6f08ca":"code","7bf09edc":"code","3c9096e0":"code","cb419df4":"code","0fdd858c":"code","ddaba794":"markdown","6fd1d400":"markdown","4a452eaa":"markdown","bbf87080":"markdown","7e12527d":"markdown","c99a33b8":"markdown","5f14a896":"markdown","afd0d3ad":"markdown","89389385":"markdown","df81ccd1":"markdown","fdaa6e0c":"markdown","7b728fd9":"markdown","9c1121f1":"markdown","a301b9c7":"markdown","55f81130":"markdown","deb05b5b":"markdown","323a5f38":"markdown","aa2fa202":"markdown","4e2f6dd6":"markdown"},"source":{"7a92565a":"import numpy as np\nimport pandas as pd\nimport random\nseed = 44  \nrandom.seed(seed)\nnp.random.seed(seed)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ebf446d7":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\nimport os\nfrom tqdm.notebook import tqdm\nimport gc\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns; sns.set()\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import ParameterGrid\n\nfrom scipy.stats import kurtosis, iqr, skew, gmean, hmean, mode, normaltest, shapiro, ks_2samp\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, OrdinalEncoder\nfrom logging import getLogger, Formatter, StreamHandler, FileHandler, INFO, ERROR\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nimport os, gc, sys, time, random, math\nfrom contextlib import contextmanager\nfrom matplotlib import pyplot as plt\nfrom IPython.display import display\nfrom scipy import stats, special\nfrom tqdm.notebook import tqdm\nfrom sklearn import set_config\nfrom functools import partial\nimport lightgbm as lgb\nimport seaborn as sns\nimport pandas as pd\nimport typing as tp\nimport numpy as np\nimport warnings\n\n\nwarnings.simplefilter('ignore')\n%matplotlib inline","cd054d3d":"TARGET_COL = \"diabetes_mellitus\"\ntrain = pd.read_csv(\"\/kaggle\/input\/wids-datasets\/TrainingWiDS2021.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/wids-datasets\/UnlabeledWiDS2021.csv\")\n\ndiat = train[['diabetes_mellitus']].copy()\ntarget = train.diabetes_mellitus.copy()\ntest_id = test['encounter_id']\ntest_id = test.encounter_id.values\ny = train.diabetes_mellitus.values\ndel train['diabetes_mellitus']\n\ntrain = train.rename(columns={'pao2_apache':'pao2fio2ratio_apache','ph_apache':'arterial_ph_apache'})\ntest = test.rename(columns={'pao2_apache':'pao2fio2ratio_apache','ph_apache':'arterial_ph_apache'})\n\nsubmission = pd.read_csv(\"\/kaggle\/input\/wids-datasets\/SolutionTemplateWiDS2021.csv\")\n\nprint(train.shape)\nprint(test.shape)\nprint(submission.shape)","d986f47b":"train['comorbidity_score'] = train['aids'].values * 23 + train['cirrhosis'] * 4  + train['hepatic_failure'] * 16 + train['immunosuppression'] * 10 + train['leukemia'] * 10 + train['lymphoma'] * 13 + train['solid_tumor_with_metastasis'] * 11\ntest['comorbidity_score'] = test['aids'].values * 23 + test['cirrhosis'] * 4  + test['hepatic_failure'] * 16 + test['immunosuppression'] * 10 + test['leukemia'] * 10 + test['lymphoma'] * 13 + test['solid_tumor_with_metastasis'] * 11\ntrain['comorbidity_score'] = train['comorbidity_score'].fillna(0)\ntest['comorbidity_score'] = test['comorbidity_score'].fillna(0)\ntrain['gcs_sum'] = train['gcs_eyes_apache']+train['gcs_motor_apache']+train['gcs_verbal_apache']\ntest['gcs_sum'] = test['gcs_eyes_apache']+test['gcs_motor_apache']+test['gcs_verbal_apache']\ntrain['gcs_sum'] = train['gcs_sum'].fillna(0)\ntest['gcs_sum'] = test['gcs_sum'].fillna(0)\ntrain['apache_2_diagnosis_type'] = train.apache_2_diagnosis.round(-1).fillna(-100).astype('int32')\ntest['apache_2_diagnosis_type'] = test.apache_2_diagnosis.round(-1).fillna(-100).astype('int32')\ntrain['apache_3j_diagnosis_type'] = train.apache_3j_diagnosis.round(-2).fillna(-100).astype('int32')\ntest['apache_3j_diagnosis_type'] = test.apache_3j_diagnosis.round(-2).fillna(-100).astype('int32')\ntrain['bmi_type'] = train.bmi.fillna(0).apply(lambda x: 5 * (round(int(x)\/5)))\ntest['bmi_type'] = test.bmi.fillna(0).apply(lambda x: 5 * (round(int(x)\/5)))\ntrain['height_type'] = train.height.fillna(0).apply(lambda x: 5 * (round(int(x)\/5)))\ntest['height_type'] = test.height.fillna(0).apply(lambda x: 5 * (round(int(x)\/5)))\ntrain['weight_type'] = train.weight.fillna(0).apply(lambda x: 5 * (round(int(x)\/5)))\ntest['weight_type'] = test.weight.fillna(0).apply(lambda x: 5 * (round(int(x)\/5)))\ntrain['age_type'] = train.age.fillna(0).apply(lambda x: 10 * (round(int(x)\/10)))\ntest['age_type'] = test.age.fillna(0).apply(lambda x: 10 * (round(int(x)\/10)))\ntrain['gcs_sum_type'] = train.gcs_sum.fillna(0).apply(lambda x: 2.5 * (round(int(x)\/2.5))).divide(2.5)\ntest['gcs_sum_type'] = test.gcs_sum.fillna(0).apply(lambda x: 2.5 * (round(int(x)\/2.5))).divide(2.5)\ntrain['apache_3j_diagnosis_x'] = train['apache_3j_diagnosis'].astype('str').str.split('.',n=1,expand=True)[0]\ntrain['apache_2_diagnosis_x'] = train['apache_2_diagnosis'].astype('str').str.split('.',n=1,expand=True)[0]\ntest['apache_3j_diagnosis_x'] = test['apache_3j_diagnosis'].astype('str').str.split('.',n=1,expand=True)[0]\ntest['apache_2_diagnosis_x'] = test['apache_2_diagnosis'].astype('str').str.split('.',n=1,expand=True)[0]\ntrain['apache_3j_diagnosis_split1'] = np.where(train['apache_3j_diagnosis'].isna() , np.nan , train['apache_3j_diagnosis'].astype('str').str.split('.',n=1,expand=True)[1]  )\ntest['apache_3j_diagnosis_split1']  = np.where(test['apache_3j_diagnosis'].isna() , np.nan , test['apache_3j_diagnosis'].astype('str').str.split('.',n=1,expand=True)[1]  )\ntrain['apache_2_diagnosis_split1'] = np.where(train['apache_2_diagnosis'].isna() , np.nan , train['apache_2_diagnosis'].apply(lambda x : x % 10)  )\ntest['apache_2_diagnosis_split1']  = np.where(test['apache_2_diagnosis'].isna() , np.nan , test['apache_2_diagnosis'].apply(lambda x : x % 10) )\n\nIDENTIFYING_COLS = ['age_type', 'height_type',  'ethnicity', 'gender', 'bmi_type'] \ntrain['profile'] = train[IDENTIFYING_COLS].apply(lambda x: hash(tuple(x)), axis = 1)\ntest['profile'] = test[IDENTIFYING_COLS].apply(lambda x: hash(tuple(x)), axis = 1)\nprint(f'Number of unique Profiles : {train[\"profile\"].nunique()}')","d6880141":"df1 = pd.concat([train['icu_id'], test['icu_id']])\nagg = df1.value_counts().to_dict()\ntrain['icu_id_counts'] = np.log1p(train['icu_id'].map(agg))\ntest['icu_id_counts'] = np.log1p(test['icu_id'].map(agg))\ndf2 = pd.concat([train['age'], test['age']])\nagg = df2.value_counts().to_dict()\ntrain['age_counts'] = np.log1p(train['age'].map(agg))\ntest['age_counts'] = np.log1p(test['age'].map(agg))\ntrain[\"diff_bmi\"] = train['bmi'].copy() \ntrain['bmi'] = train['weight']\/((train['height']\/100)**2)\ntrain[\"diff_bmi\"] = train[\"diff_bmi\"]-train['bmi']\ntest[\"diff_bmi\"] = test['bmi'].copy()\ntest['bmi'] = test['weight']\/((test['height']\/100)**2)\ntest[\"diff_bmi\"] = test[\"diff_bmi\"]-test['bmi']\ntrain['pre_icu_los_days'] = train['pre_icu_los_days'].apply(lambda x:special.expit(x) )\ntest['pre_icu_los_days']  = test['pre_icu_los_days'].apply(lambda x:special.expit(x) )\ntrain['abmi'] = train['age']\/train['bmi']\ntrain['agi'] = train['weight']\/train['age']\ntest['abmi'] = test['age']\/train['bmi']\ntest['agi'] = test['weight']\/train['age']","8abdf047":"d_cols = [c for c in train.columns if(c.startswith(\"d1\"))]\nh_cols = [c for c in train.columns if(c.startswith(\"h1\"))]\ntrain[\"dailyLabs_row_nan_count\"] = train[d_cols].isna().sum(axis=1)\ntrain[\"hourlyLabs_row_nan_count\"] = train[h_cols].isna().sum(axis=1)\ntrain[\"diff_labTestsRun_daily_hourly\"] = train[\"dailyLabs_row_nan_count\"] - train[\"hourlyLabs_row_nan_count\"]\ntest[\"dailyLabs_row_nan_count\"] = test[d_cols].isna().sum(axis=1)\ntest[\"hourlyLabs_row_nan_count\"] = test[h_cols].isna().sum(axis=1)\ntest[\"diff_labTestsRun_daily_hourly\"] = test[\"dailyLabs_row_nan_count\"] - test[\"hourlyLabs_row_nan_count\"]","7c724e67":"lab_col = [c for c in train.columns if((c.startswith(\"h1\")) | (c.startswith(\"d1\")))]\nlab_col_names = list(set(list(map(lambda i: i[ 3 : -4], lab_col))))\n\nprint(\"len lab_col\",len(lab_col))\nprint(\"len lab_col_names\",len(lab_col_names))\nprint(\"lab_col_names\\n\",lab_col_names)","03fdb7f2":"first_h = []\nfor v in lab_col_names:\n    first_h.append(v+\"_started_after_firstHour\")\n    colsx = [x for x in test.columns if v in x]\n    train[v+\"_nans\"] = train.loc[:, colsx].isna().sum(axis=1)\n    test[v+\"_nans\"] = test.loc[:, colsx].isna().sum(axis=1)\n    train[v+\"_d1_value_range\"] = train[f\"d1_{v}_max\"].subtract(train[f\"d1_{v}_min\"])    \n    train[v+\"_h1_value_range\"] = train[f\"h1_{v}_max\"].subtract(train[f\"h1_{v}_min\"])\n    train[v+\"_d1_h1_max_eq\"] = (train[f\"d1_{v}_max\"]== train[f\"h1_{v}_max\"]).astype(np.int8)\n    train[v+\"_d1_h1_min_eq\"] = (train[f\"d1_{v}_min\"]== train[f\"h1_{v}_min\"]).astype(np.int8)\n    train[v+\"_d1_zero_range\"] = (train[v+\"_d1_value_range\"] == 0).astype(np.int8)\n    train[v+\"_h1_zero_range\"] =(train[v+\"_h1_value_range\"] == 0).astype(np.int8)\n    train[v+\"_tot_change_value_range_normed\"] = abs((train[v+\"_d1_value_range\"].div(train[v+\"_h1_value_range\"])))#.div(df[f\"d1_{v}_max\"]))\n    train[v+\"_started_after_firstHour\"] = ((train[f\"h1_{v}_max\"].isna()) & (train[f\"h1_{v}_min\"].isna())) & (~train[f\"d1_{v}_max\"].isna())\n    train[v+\"_day_more_extreme\"] = ((train[f\"d1_{v}_max\"]>train[f\"h1_{v}_max\"]) | (train[f\"d1_{v}_min\"]<train[f\"h1_{v}_min\"]))\n    train[v+\"_day_more_extreme\"].fillna(False)    \n    test[v+\"_d1_value_range\"] = test[f\"d1_{v}_max\"].subtract(test[f\"d1_{v}_min\"])   \n    test[v+\"_h1_value_range\"] = test[f\"h1_{v}_max\"].subtract(test[f\"h1_{v}_min\"])\n    test[v+\"_d1_h1_max_eq\"] = (test[f\"d1_{v}_max\"]== test[f\"h1_{v}_max\"]).astype(np.int8)\n    test[v+\"_d1_h1_min_eq\"] = (test[f\"d1_{v}_min\"]== test[f\"h1_{v}_min\"]).astype(np.int8)\n    test[v+\"_d1_zero_range\"] = (test[v+\"_d1_value_range\"] == 0).astype(np.int8)\n    test[v+\"_h1_zero_range\"] =(test[v+\"_h1_value_range\"] == 0).astype(np.int8)\n    test[v+\"_tot_change_value_range_normed\"] = abs((test[v+\"_d1_value_range\"].div(test[v+\"_h1_value_range\"])))\n    test[v+\"_started_after_firstHour\"] = ((test[f\"h1_{v}_max\"].isna()) & (test[f\"h1_{v}_min\"].isna())) & (~test[f\"d1_{v}_max\"].isna())\n    test[v+\"_day_more_extreme\"] = ((test[f\"d1_{v}_max\"]>test[f\"h1_{v}_max\"]) | (test[f\"d1_{v}_min\"]<test[f\"h1_{v}_min\"]))\n    test[v+\"_day_more_extreme\"].fillna(False)\n\ntrain[\"total_Tests_started_After_firstHour\"] = train[first_h].sum(axis=1)\ntest[\"total_Tests_started_After_firstHour\"] = test[first_h].sum(axis=1)\ngc.collect()\ntrain[\"total_Tests_started_After_firstHour\"].describe()","83a5ef61":"groupers = ['apache_3j_diagnosis', 'profile']\n\nfor g in groupers:\n    for v in lab_col_names:\n        temp = pd.concat([train[[f\"d1_{v}_max\",g]], test[[f\"d1_{v}_max\",g]]], axis=0).groupby(g)[f\"d1_{v}_max\"].mean().to_dict()\n        train[f'mean_diff_d1_{v}_{g}_max'] = train[f\"d1_{v}_max\"]-train[g].map(temp)\n        test[f'mean_diff_d1_{v}_{g}_max'] = test[f\"d1_{v}_max\"]-test[g].map(temp)\n        temp = pd.concat([train[[f\"d1_{v}_min\",g]], test[[f\"d1_{v}_min\",g]]], axis=0).groupby(g)[f\"d1_{v}_min\"].mean().to_dict()   \n        train[f'mean_diff_d1_{v}_{g}_min'] = train[f\"d1_{v}_min\"]-train[g].map(temp)\n        test[f'mean_diff_d1_{v}_{g}_min'] = test[f\"d1_{v}_min\"]-test[g].map(temp)\n        temp = pd.concat([train[[f\"h1_{v}_max\",g]], test[[f\"h1_{v}_max\",g]]], axis=0).groupby(g)[f\"h1_{v}_max\"].mean().to_dict()   \n        train[f'mean_diff_h1_{v}_{g}_max'] = train[f\"h1_{v}_max\"]-train[g].map(temp)\n        test[f'mean_diff_h1_{v}_{g}_max'] = test[f\"h1_{v}_max\"]-test[g].map(temp)\n        temp = pd.concat([train[[f\"h1_{v}_min\",g]], test[[f\"h1_{v}_min\",g]]], axis=0).groupby(g)[f\"h1_{v}_min\"].mean().to_dict()   \n        train[f'mean_diff_h1_{v}_{g}_min'] = train[f\"h1_{v}_min\"]-train[g].map(temp)\n        test[f'mean_diff_h1_{v}_{g}_min'] = test[f\"h1_{v}_min\"]-test[g].map(temp)\ngc.collect()","69eb1a2c":"train['diasbp_indicator'] = (\n(train['d1_diasbp_invasive_max'] == train['d1_diasbp_max']) & (train['d1_diasbp_noninvasive_max']==train['d1_diasbp_invasive_max'])|\n(train['d1_diasbp_invasive_min'] == train['d1_diasbp_min']) & (train['d1_diasbp_noninvasive_min']==train['d1_diasbp_invasive_min'])|\n(train['h1_diasbp_invasive_max'] == train['h1_diasbp_max']) & (train['h1_diasbp_noninvasive_max']==train['h1_diasbp_invasive_max'])|\n(train['h1_diasbp_invasive_min'] == train['h1_diasbp_min']) & (train['h1_diasbp_noninvasive_min']==train['h1_diasbp_invasive_min'])\n).astype(np.int8)\n\n\ntrain['mbp_indicator'] = (\n(train['d1_mbp_invasive_max'] == train['d1_mbp_max']) & (train['d1_mbp_noninvasive_max']==train['d1_mbp_invasive_max'])|\n(train['d1_mbp_invasive_min'] == train['d1_mbp_min']) & (train['d1_mbp_noninvasive_min']==train['d1_mbp_invasive_min'])|\n(train['h1_mbp_invasive_max'] == train['h1_mbp_max']) & (train['h1_mbp_noninvasive_max']==train['h1_mbp_invasive_max'])|\n(train['h1_mbp_invasive_min'] == train['h1_mbp_min']) & (train['h1_mbp_noninvasive_min']==train['h1_mbp_invasive_min'])\n).astype(np.int8)\n\ntrain['sysbp_indicator'] = (\n(train['d1_sysbp_invasive_max'] == train['d1_sysbp_max']) & (train['d1_sysbp_noninvasive_max']==train['d1_sysbp_invasive_max'])|\n(train['d1_sysbp_invasive_min'] == train['d1_sysbp_min']) & (train['d1_sysbp_noninvasive_min']==train['d1_sysbp_invasive_min'])|\n (train['h1_sysbp_invasive_max'] == train['h1_sysbp_max']) & (train['h1_sysbp_noninvasive_max']==train['h1_sysbp_invasive_max'])|\n(train['h1_sysbp_invasive_min'] == train['h1_sysbp_min']) & (train['h1_sysbp_noninvasive_min']==train['h1_sysbp_invasive_min'])   \n).astype(np.int8)\n\ntrain['d1_mbp_invnoninv_max_diff'] = train['d1_mbp_invasive_max'] - train['d1_mbp_noninvasive_max']\ntrain['h1_mbp_invnoninv_max_diff'] = train['h1_mbp_invasive_max'] - train['h1_mbp_noninvasive_max']\ntrain['d1_mbp_invnoninv_min_diff'] = train['d1_mbp_invasive_min'] - train['d1_mbp_noninvasive_min']\ntrain['h1_mbp_invnoninv_min_diff'] = train['h1_mbp_invasive_min'] - train['h1_mbp_noninvasive_min']\ntrain['d1_diasbp_invnoninv_max_diff'] = train['d1_diasbp_invasive_max'] - train['d1_diasbp_noninvasive_max']\ntrain['h1_diasbp_invnoninv_max_diff'] = train['h1_diasbp_invasive_max'] - train['h1_diasbp_noninvasive_max']\ntrain['d1_diasbp_invnoninv_min_diff'] = train['d1_diasbp_invasive_min'] - train['d1_diasbp_noninvasive_min']\ntrain['h1_diasbp_invnoninv_min_diff'] = train['h1_diasbp_invasive_min'] - train['h1_diasbp_noninvasive_min']\ntrain['d1_sysbp_invnoninv_max_diff'] = train['d1_sysbp_invasive_max'] - train['d1_sysbp_noninvasive_max']\ntrain['h1_sysbp_invnoninv_max_diff'] = train['h1_sysbp_invasive_max'] - train['h1_sysbp_noninvasive_max']\ntrain['d1_sysbp_invnoninv_min_diff'] = train['d1_sysbp_invasive_min'] - train['d1_sysbp_noninvasive_min']\ntrain['h1_sysbp_invnoninv_min_diff'] = train['h1_sysbp_invasive_min'] - train['h1_sysbp_noninvasive_min']\n\ntest['diasbp_indicator'] = (\n(test['d1_diasbp_invasive_max'] == test['d1_diasbp_max']) & (test['d1_diasbp_noninvasive_max']==test['d1_diasbp_invasive_max'])|\n(test['d1_diasbp_invasive_min'] == test['d1_diasbp_min']) & (test['d1_diasbp_noninvasive_min']==test['d1_diasbp_invasive_min'])|\n(test['h1_diasbp_invasive_max'] == test['h1_diasbp_max']) & (test['h1_diasbp_noninvasive_max']==test['h1_diasbp_invasive_max'])|\n(test['h1_diasbp_invasive_min'] == test['h1_diasbp_min']) & (test['h1_diasbp_noninvasive_min']==test['h1_diasbp_invasive_min'])\n).astype(np.int8)\n\n\ntest['mbp_indicator'] = (\n(test['d1_mbp_invasive_max'] == test['d1_mbp_max']) & (test['d1_mbp_noninvasive_max']==test['d1_mbp_invasive_max'])|\n(test['d1_mbp_invasive_min'] == test['d1_mbp_min']) & (test['d1_mbp_noninvasive_min']==test['d1_mbp_invasive_min'])|\n(test['h1_mbp_invasive_max'] == test['h1_mbp_max']) & (test['h1_mbp_noninvasive_max']==test['h1_mbp_invasive_max'])|\n(test['h1_mbp_invasive_min'] == test['h1_mbp_min']) & (test['h1_mbp_noninvasive_min']==test['h1_mbp_invasive_min'])\n).astype(np.int8)\n\ntest['sysbp_indicator'] = (\n(test['d1_sysbp_invasive_max'] == test['d1_sysbp_max']) & (test['d1_sysbp_noninvasive_max']==test['d1_sysbp_invasive_max'])|\n(test['d1_sysbp_invasive_min'] == test['d1_sysbp_min']) & (test['d1_sysbp_noninvasive_min']==test['d1_sysbp_invasive_min'])|\n (test['h1_sysbp_invasive_max'] == test['h1_sysbp_max']) & (test['h1_sysbp_noninvasive_max']==test['h1_sysbp_invasive_max'])|\n(test['h1_sysbp_invasive_min'] == test['h1_sysbp_min']) & (test['h1_sysbp_noninvasive_min']==test['h1_sysbp_invasive_min'])   \n).astype(np.int8)\n\ntest['d1_mbp_invnoninv_max_diff'] = test['d1_mbp_invasive_max'] - test['d1_mbp_noninvasive_max']\ntest['h1_mbp_invnoninv_max_diff'] = test['h1_mbp_invasive_max'] - test['h1_mbp_noninvasive_max']\ntest['d1_mbp_invnoninv_min_diff'] = test['d1_mbp_invasive_min'] - test['d1_mbp_noninvasive_min']\ntest['h1_mbp_invnoninv_min_diff'] = test['h1_mbp_invasive_min'] - test['h1_mbp_noninvasive_min']\ntest['d1_diasbp_invnoninv_max_diff'] = test['d1_diasbp_invasive_max'] - test['d1_diasbp_noninvasive_max']\ntest['h1_diasbp_invnoninv_max_diff'] = test['h1_diasbp_invasive_max'] - test['h1_diasbp_noninvasive_max']\ntest['d1_diasbp_invnoninv_min_diff'] = test['d1_diasbp_invasive_min'] - test['d1_diasbp_noninvasive_min']\ntest['h1_diasbp_invnoninv_min_diff'] = test['h1_diasbp_invasive_min'] - test['h1_diasbp_noninvasive_min']\n\ntest['d1_sysbp_invnoninv_max_diff'] = test['d1_sysbp_invasive_max'] - test['d1_sysbp_noninvasive_max']\ntest['h1_sysbp_invnoninv_max_diff'] = test['h1_sysbp_invasive_max'] - test['h1_sysbp_noninvasive_max']\ntest['d1_sysbp_invnoninv_min_diff'] = test['d1_sysbp_invasive_min'] - test['d1_sysbp_noninvasive_min']\ntest['h1_sysbp_invnoninv_min_diff'] = test['h1_sysbp_invasive_min'] - test['h1_sysbp_noninvasive_min']\n\n\nfor v in ['albumin','bilirubin','bun','glucose','hematocrit','pao2fio2ratio','arterial_ph','resprate','sodium','temp','wbc','creatinine']:\n    train[f'{v}_indicator'] = (((train[f'{v}_apache']==train[f'd1_{v}_max']) & (train[f'd1_{v}_max']==train[f'h1_{v}_max'])) |\n                 ((train[f'{v}_apache']==train[f'd1_{v}_max']) & (train[f'd1_{v}_max']==train[f'd1_{v}_min'])) |\n                 ((train[f'{v}_apache']==train[f'd1_{v}_max']) & (train[f'd1_{v}_max']==train[f'h1_{v}_min'])) |\n                 ((train[f'{v}_apache']==train[f'h1_{v}_max']) & (train[f'h1_{v}_max']==train[f'd1_{v}_max'])) |\n                 ((train[f'{v}_apache']==train[f'h1_{v}_max']) & (train[f'h1_{v}_max']==train[f'h1_{v}_min'])) |\n                 ((train[f'{v}_apache']==train[f'h1_{v}_max']) & (train[f'h1_{v}_max']==train[f'd1_{v}_min'])) |\n                 ((train[f'{v}_apache']==train[f'd1_{v}_min']) & (train[f'd1_{v}_min']==train[f'd1_{v}_max'])) |\n                 ((train[f'{v}_apache']==train[f'd1_{v}_min']) & (train[f'd1_{v}_min']==train[f'h1_{v}_min'])) |\n                 ((train[f'{v}_apache']==train[f'd1_{v}_min']) & (train[f'd1_{v}_min']==train[f'h1_{v}_max'])) |\n                 ((train[f'{v}_apache']==train[f'h1_{v}_min']) & (train[f'h1_{v}_min']==train[f'h1_{v}_max'])) |\n                 ((train[f'{v}_apache']==train[f'h1_{v}_min']) & (train[f'h1_{v}_min']==train[f'd1_{v}_min'])) |\n                 ((train[f'{v}_apache']==train[f'h1_{v}_min']) & (train[f'h1_{v}_min']==train[f'd1_{v}_max'])) \n                ).astype(np.int8)\n    test[f'{v}_indicator'] = (((test[f'{v}_apache']==test[f'd1_{v}_max']) & (test[f'd1_{v}_max']==test[f'h1_{v}_max'])) |\n                 ((test[f'{v}_apache']==test[f'd1_{v}_max']) & (test[f'd1_{v}_max']==test[f'd1_{v}_min'])) |\n                 ((test[f'{v}_apache']==test[f'd1_{v}_max']) & (test[f'd1_{v}_max']==test[f'h1_{v}_min'])) |\n                 ((test[f'{v}_apache']==test[f'h1_{v}_max']) & (test[f'h1_{v}_max']==test[f'd1_{v}_max'])) |\n                 ((test[f'{v}_apache']==test[f'h1_{v}_max']) & (test[f'h1_{v}_max']==test[f'h1_{v}_min'])) |\n                 ((test[f'{v}_apache']==test[f'h1_{v}_max']) & (test[f'h1_{v}_max']==test[f'd1_{v}_min'])) |\n                 ((test[f'{v}_apache']==test[f'd1_{v}_min']) & (test[f'd1_{v}_min']==test[f'd1_{v}_max'])) |\n                 ((test[f'{v}_apache']==test[f'd1_{v}_min']) & (test[f'd1_{v}_min']==test[f'h1_{v}_min'])) |\n                 ((test[f'{v}_apache']==test[f'd1_{v}_min']) & (test[f'd1_{v}_min']==test[f'h1_{v}_max'])) |\n                 ((test[f'{v}_apache']==test[f'h1_{v}_min']) & (test[f'h1_{v}_min']==test[f'h1_{v}_max'])) |\n                 ((test[f'{v}_apache']==test[f'h1_{v}_min']) & (test[f'h1_{v}_min']==test[f'd1_{v}_min'])) |\n                 ((test[f'{v}_apache']==test[f'h1_{v}_min']) & (test[f'h1_{v}_min']==test[f'd1_{v}_max'])) \n                ).astype(np.int8)","a47305d3":"more_extreme_cols = [c for c in train.columns if(c.endswith(\"_day_more_extreme\"))]\ntrain[\"total_day_more_extreme\"] = train[more_extreme_cols].sum(axis=1)\ntest[\"total_day_more_extreme\"] = test[more_extreme_cols].sum(axis=1)\ntrain[\"d1_resprate_div_mbp_min\"] = train[\"d1_resprate_min\"].div(train[\"d1_mbp_min\"])\ntrain[\"d1_resprate_div_sysbp_min\"] = train[\"d1_resprate_min\"].div(train[\"d1_sysbp_min\"])\ntrain[\"d1_lactate_min_div_diasbp_min\"] = train[\"d1_lactate_min\"].div(train[\"d1_diasbp_min\"])\ntrain[\"d1_heartrate_min_div_d1_sysbp_min\"] = train[\"d1_heartrate_min\"].div(train[\"d1_sysbp_min\"])\ntrain[\"d1_hco3_div\"]= train[\"d1_hco3_max\"].div(train[\"d1_hco3_min\"])\ntrain[\"d1_resprate_times_resprate\"] = train[\"d1_resprate_min\"].multiply(train[\"d1_resprate_max\"])\ntrain[\"left_average_spo2\"] = (2*train[\"d1_spo2_max\"] + train[\"d1_spo2_min\"])\/3\ntest[\"d1_resprate_div_mbp_min\"] = test[\"d1_resprate_min\"].div(test[\"d1_mbp_min\"])\ntest[\"d1_resprate_div_sysbp_min\"] = test[\"d1_resprate_min\"].div(test[\"d1_sysbp_min\"])\ntest[\"d1_lactate_min_div_diasbp_min\"] = test[\"d1_lactate_min\"].div(test[\"d1_diasbp_min\"])\ntest[\"d1_heartrate_min_div_d1_sysbp_min\"] = test[\"d1_heartrate_min\"].div(test[\"d1_sysbp_min\"])\ntest[\"d1_hco3_div\"]= test[\"d1_hco3_max\"].div(test[\"d1_hco3_min\"])\ntest[\"d1_resprate_times_resprate\"] = test[\"d1_resprate_min\"].multiply(test[\"d1_resprate_max\"])\ntest[\"left_average_spo2\"] = (2*test[\"d1_spo2_max\"] + test[\"d1_spo2_min\"])\/3\ntrain[\"total_chronic\"] = train[[\"aids\",\"cirrhosis\", 'hepatic_failure']].sum(axis=1)\ntrain[\"total_cancer_immuno\"] = train[[ 'immunosuppression', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis']].sum(axis=1)\ntest[\"total_chronic\"] = test[[\"aids\",\"cirrhosis\", 'hepatic_failure']].sum(axis=1)\ntest[\"total_cancer_immuno\"] = test[[ 'immunosuppression', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis']].sum(axis=1)\ntrain[\"has_complicator\"] = train[[\"aids\",\"cirrhosis\", 'hepatic_failure',\n                            'immunosuppression', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis']].max(axis=1)\ntest[\"has_complicator\"] = test[[\"aids\",\"cirrhosis\", 'hepatic_failure',\n                            'immunosuppression', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis']].max(axis=1)\ngc.collect()\ntrain[[\"has_complicator\",\"total_chronic\",\"total_cancer_immuno\",\"has_complicator\"]].describe()\n","e3b445cf":"train['apache_3j'] = np.where(train['apache_3j_diagnosis_type']<0 , np.nan ,\n                            np.where(train['apache_3j_diagnosis_type'] < 200, 'Cardiovascular' , \n                            np.where(train['apache_3j_diagnosis_type'] < 400, 'Respiratory' , \n                            np.where(train['apache_3j_diagnosis_type'] < 500, 'Neurological' , \n                            np.where(train['apache_3j_diagnosis_type'] < 600, 'Sepsis' , \n                            np.where(train['apache_3j_diagnosis_type'] < 800, 'Trauma' ,  \n                            np.where(train['apache_3j_diagnosis_type'] < 900, 'Haematological' ,         \n                            np.where(train['apache_3j_diagnosis_type'] < 1000, 'Renal\/Genitourinary' ,         \n                            np.where(train['apache_3j_diagnosis_type'] < 1200, 'Musculoskeletal\/Skin disease' , 'Operative Sub-Diagnosis Codes' ))))))))\n                                    )\ntest['apache_3j'] = np.where(test['apache_3j_diagnosis_type']<0 , np.nan ,\n                            np.where(test['apache_3j_diagnosis_type'] < 200, 'Cardiovascular' , \n                            np.where(test['apache_3j_diagnosis_type'] < 400, 'Respiratory' , \n                            np.where(test['apache_3j_diagnosis_type'] < 500, 'Neurological' , \n                            np.where(test['apache_3j_diagnosis_type'] < 600, 'Sepsis' , \n                            np.where(test['apache_3j_diagnosis_type'] < 800, 'Trauma' ,  \n                            np.where(test['apache_3j_diagnosis_type'] < 900, 'Haematological' ,         \n                            np.where(test['apache_3j_diagnosis_type'] < 1000, 'Renal\/Genitourinary' ,         \n                            np.where(test['apache_3j_diagnosis_type'] < 1200, 'Musculoskeletal\/Skin disease' , 'Operative Sub-Diagnosis Codes' ))))))))\n                                    )","ba602521":"#helps with reducing memory usage\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","aca7c4fe":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)\ngc.collect()\ntrain.shape, test.shape","34053900":"df = train\ndf['label']='train'\ntest['label']='test'\nframes = [df,test]\njoin_df = pd.concat(frames, keys=['x', 'y'])\nassert len(join_df) == len(df) + len(test)","0495e644":"join_df['icu_id'] = join_df['icu_id'].astype(\"object\")","890cbede":"join_df['ethnicity_count']= join_df.groupby([\"ethnicity\"])[\"encounter_id\"].transform(\"count\")\njoin_df['gender_count']= join_df.groupby([\"gender\"])[\"encounter_id\"].transform(\"count\")\njoin_df['hospital_admit_source_count']= join_df.groupby([\"hospital_admit_source\"])[\"encounter_id\"].transform(\"count\")\njoin_df['icu_admit_source_count']= join_df.groupby([\"icu_admit_source\"])[\"encounter_id\"].transform(\"count\")\njoin_df['icu_stay_type_count']= join_df.groupby([\"icu_stay_type\"])[\"encounter_id\"].transform(\"count\")\njoin_df['icu_type_count']= join_df.groupby([\"icu_type\"])[\"encounter_id\"].transform(\"count\")\njoin_df['icu_id_count']= join_df.groupby([\"icu_id\"])[\"encounter_id\"].transform(\"count\")","cddbeee6":"join_df.drop(['Unnamed: 0','encounter_id', 'hospital_id'],inplace=True,axis=1)","eff2aa10":"lst = join_df.isna().sum()\/len(join_df)\np = pd.DataFrame(lst)\np.reset_index(inplace=True)\np.columns = ['a','b']\nlow_count = p[p['b']>0.8]\ntodelete=low_count['a'].values","2cc8bcb2":"todelete","8cbc0a12":"join_df.drop(todelete,axis=1,inplace=True)","0ec4f498":"join_df.shape","6222c5df":"join_df.head()","6f7563b4":"df.drop(['Unnamed: 0','encounter_id', 'hospital_id'],inplace=True,axis=1)","eb13e090":"## Print the categorical columns\nprint([c for c in df.columns if (1<df[c].nunique()) & (df[c].dtype != np.number)& (df[c].dtype != int) ])","910d13da":"## Print the categorical columns\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\nnewdf = join_df.select_dtypes(include=numerics)\nnumeric_cols = newdf.columns","f4ee948d":"numeric_cols ","8ea7941e":"join_df['apache_3j_diagnosis_x']= join_df['apache_3j_diagnosis_x'].astype(float)\njoin_df['apache_2_diagnosis_x']= join_df['apache_2_diagnosis_x'].astype(float)\njoin_df['apache_3j_diagnosis_split1']= join_df['apache_3j_diagnosis_split1'].astype(float)","4baed6a7":"# Need to do column by column due to memory constraints\ncategorical_cols =  ['elective_surgery', 'icu_id', 'apache_3j',                      \n 'ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source', 'icu_stay_type', 'icu_type','aids','cirrhosis','hepatic_failure','immunosuppression',\n 'leukemia','lymphoma','solid_tumor_with_metastasis','elective_surgery','apache_post_operative','arf_apache','fio2_apache','gcs_unable_apache','gcs_eyes_apache',\n 'gcs_motor_apache','gcs_verbal_apache','intubated_apache','ventilated_apache','solid_tumor_with_metastasis']\nfor i, v in tqdm(enumerate(categorical_cols)):\n    join_df[v] = join_df[v].fillna(join_df[v].value_counts().index[0])\n","69e7ac4c":"for i, v in tqdm(enumerate([numeric_cols])):\n    join_df[v] =join_df.groupby(['ethnicity','gender'], sort=False)[v].apply(lambda x: x.fillna(x.mean()))","504585f0":"join_df[categorical_cols].isna().sum()","95d40dfa":"from sklearn.preprocessing import OrdinalEncoder\n\n# In loop to minimize memory use\nfor i, v in tqdm(enumerate(categorical_cols)):\n    join_df[v] = OrdinalEncoder(dtype=\"int\").fit_transform(join_df[[v]])\n    \n\ngc.collect()","85bde6de":"train = join_df[join_df['label']==\"train\"]\npredict = join_df[join_df['label']=='test']","ec3ec6aa":"train.reset_index(inplace=True)\ntrain.drop(['level_0','level_1','label'],inplace=True,axis =1 )","1b61fc79":"train.shape","32f8a73b":"predict.reset_index(inplace=True)\npredict.drop(['level_0','level_1','label'],inplace=True,axis=1) ","8b83956d":"predict.shape","2abc5b68":"features = train.columns","6cc5c0ac":"num_feature = [col for col in features if col not in categorical_cols]","b7f16bcd":"num_feature = [col for col in features if col not in categorical_cols and train[col].dtype != 'object']\ndrop_columns=[]\ncorr = train[num_feature].corr()\n# Drop highly correlated features \ncolumns = np.full((corr.shape[0],), True, dtype=bool)\n\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >=0.999 :\n            if columns[j]:\n                columns[j] = False\n                print('FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(train[num_feature].columns[i] , train[num_feature].columns[j], corr.iloc[i,j]))\n        elif corr.iloc[i,j] <= -0.995:\n            if columns[j]:\n                columns[j] = False","11559478":"drop_columns = train[num_feature].columns[columns == False].values\nprint('drop_columns',len(drop_columns),drop_columns)","5e025621":"train.drop(drop_columns,inplace=True,axis =1 )\npredict.drop(drop_columns,inplace=True,axis =1 )","f42e1894":"cols = train.columns","d07327ed":"# Check and remove constant features\n#[feat for feat in train[cols] if train[feat].std() ==0]","897a6fe4":"columns_to_drop = ['readmission_status']\n\ntrain.drop(columns = columns_to_drop, inplace = True)\npredict.drop(columns = columns_to_drop, inplace = True)","d1decb3f":"print(train.shape,predict.shape)","0d35445e":"train = train.replace([np.inf, -np.inf], np.nan)\npredict = predict.replace([np.inf, -np.inf], np.nan)","a9496bb6":"train.head()","cd3bb178":"!pip install pandas_summary --quiet","9ba1ae2e":"# Octopus ML pakage - github.com\/gershonc\/octopus-ml\n!pip install octopus-ml --quiet","a8145d7b":"import warnings\nwarnings.simplefilter(\"ignore\")\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport time\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport tracemalloc\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.metrics import classification_report\n%matplotlib inline\nsns.set_style(\"whitegrid\")\n\npd.set_option('display.max_columns', None)  # or 1000\npd.set_option('display.max_rows', None)  # or 1000\npd.set_option('display.max_colwidth', -1)  # or 199\n\n#check out https:\/\/github.com\/gershonc\/octopus-ml\nimport octopus_ml as oc\n\nimport os","206a68da":"features=train.columns.to_list()\nprint ('Number of features ', len(features))","7506b013":"train = train.join(diat)   \nX=train[features]          \ny=train['diabetes_mellitus']","1d86021b":"params = {\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'metric': 'auc', \n        'learning_rate': 0.007,\n        'subsample': 1,\n        'colsample_bytree': 0.2,\n        'reg_alpha': 3,\n        'reg_lambda': 1,\n        'scale_pos_weight': 4,\n        'n_estimators': 10000,\n        'verbose': 1,\n        'max_depth': -1,\n        'seed':100, \n        'force_col_wise': True\n\n}\n# 0.5\nclf,arr_f1_weighted,arr_f1_macro,arr_f1_positive,prediction_folds,preds_folds,y_folds= oc.cv(X,y,0.2,16000,shuffle=True,params=params)","f056394e":"oc.cv_plot(arr_f1_weighted,arr_f1_macro,arr_f1_positive,'WIDS2021 Kaggle competition')","f6876e57":"print(classification_report(y_folds, prediction_folds))","eec10d49":"oc.roc_curve_plot(y_folds,preds_folds)","f7a66845":"oc.confusion_matrix_plot(y_folds,prediction_folds)","9db11080":"feature_imp_list=oc.plot_imp(clf,X,'LightGBM Mortality Kaggle',num=20)","dc6f08ca":"top_features=feature_imp_list.sort_values(by='Value', ascending=False).head(20)\ntop_features","7bf09edc":"list_for_correlations=top_features['Feature'].to_list()\nlist_for_correlations.append('diabetes_mellitus')\noc.correlations(train,list_for_correlations)\n\n# we can see that all three glucose_max\/min,apache and h1_max have a positive correleation to diabetes as expected and interestingly that d1_hemaglobin_max has opposite correlation.","3c9096e0":"def Kaggle_submission(file_name,model,test_data,ids_list):\n    if TARGET in test_data.columns:\n        test_data.drop([TARGET],axis=1,inplace=True)\n    test_pred=model.predict(test_data[features])\n    print (test_pred[1:2])\n\n    submit=pd.DataFrame()\n    submit['encounter_id'] = ids_list\n    submit['diabetes_mellitus'] = test_pred\n    submit.to_csv(file_name,index=False)\n    return submit","cb419df4":"# Categorical features on testset\n\ncategorical_features=[]\nfor c in predict.columns:\n    col_type = train[c].dtype\n    if col_type == 'object' or col_type.name == 'category':\n        predict[c] = predict[c].astype('category')\n        categorical_features.append(c)\nprint (categorical_features)\n\nTARGET=\"diabetes_mellitus\"\nsubmit=Kaggle_submission(\"LGBM_octopus_ml.csv\",clf,predict,test['encounter_id'].tolist())","0fdd858c":"submit.head(20)","ddaba794":"#Model evaluation","6fd1d400":"\n\n**Some of the feature engineering techniques applied are:**\n\n- Renaming features like: pao2_apache' & 'ph_apache'\n* Group count categorical variables using 'encounter_id'\n* Remove cols with higher than 80% nans \n* Fill na categorical variables with the most common values\n* Fill na for numeric values with mean values after [groupby(ethnicity, gender)]\n* Remove highly correlated columns\n* Remove constant variables\n* Convert categorical variables to encodings\n\n\n","4a452eaa":"### We can use the following code to delete certain colums that are empty more than 80%.\n\n**Credit:** https:\/\/www.kaggle.com\/hamzafarooq50\/lightgbm-encoding-0-85883-score","bbf87080":"#Helps with reducing memory usage","7e12527d":"**Note:** that target variable \"diabetes_mellitus\" column was initially removed for easier feature engineering and also as well stored inside the variable \"diat\" from the beggining of the notebook for later use. \n\nMerging train dataset with its target variable column \"diabetes_mellitus\" before modelling is very much paramount now. ","c99a33b8":"### Convert categorical\/binary variables to OrdinalEncoders","5f14a896":"#Group count categorical variables using 'encounter_id'","afd0d3ad":"### Import the Libraries","89389385":"#KAGGLE WIDS_2021_DATATHON \n(using **Octopus_Machine_Learning Model**)\n\nNotebook created by **Adeyinka Michael Sotunde**.\n\nhttps:\/\/www.kaggle.com\/adegladius","df81ccd1":"### Seperating train and predict(test)","fdaa6e0c":"\n### Remove correlated columns\nCredit: https:\/\/www.kaggle.com\/ankitmalik\/simple-neural-net-0-84706","7b728fd9":"# Feature Engineering:\n\n**Credit:** https:\/\/www.kaggle.com\/siavrez\/2020fatures","9c1121f1":"### Load the Data","a301b9c7":"### Joining train and test to ensure encodings done correctly","55f81130":"#OCTOPUS MACHINE LEARNING:\n\n**Credit:** https:\/\/www.kaggle.com\/celniker\/wids2021-initial-eda-and-tuned-lgbm-0-86293","deb05b5b":"#Test Submission","323a5f38":"#Convert 'icu_id' to categorical variable","aa2fa202":"### Fill categorical columns","4e2f6dd6":"#Check and remove any constant features present inside the dataset"}}