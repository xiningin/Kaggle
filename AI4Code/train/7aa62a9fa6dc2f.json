{"cell_type":{"bcea5139":"code","1ffa07c7":"code","4702a369":"code","84e8e068":"code","207a77e2":"code","52cbd0b4":"code","2824ca2e":"code","a3b48368":"code","55724b5a":"code","ca94562a":"code","f856318f":"code","abdb5eb3":"code","dff276a2":"code","e6f04d03":"code","7fefb4f9":"code","e6a55f81":"code","a5139f2e":"code","d799d456":"code","194399c5":"code","bf06fbde":"code","211c2444":"code","b5059ca9":"code","72c8d157":"code","8b3e358b":"code","f0bd98a3":"markdown","9ee92e08":"markdown","53dc7fb7":"markdown","172c871e":"markdown","f48f725a":"markdown","cccc3288":"markdown","ee780a4d":"markdown","725caf9a":"markdown"},"source":{"bcea5139":"!pip install -q efficientnet >> \/dev\/null\n!pip install tf-explain","1ffa07c7":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport datetime\nimport seaborn as sns\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import *\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport tensorflow as tf\n\nfrom tensorflow.keras import models, layers\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.applications import *\nfrom tensorflow.keras.optimizers import *\nimport efficientnet.tfkeras as efn","4702a369":"base_dir = '..\/input\/chest-xray-pneumonia\/chest_xray'\n\ntrain_dir = os.path.join(base_dir, 'train')\ntest_dir = os.path.join(base_dir, 'test')\nvalidation_dir = os.path.join(base_dir, 'val')\n\n\n# Directory with our training NORMAL\/PNEUMONIA pictures\ntrain_normal_dir = os.path.join(train_dir, 'NORMAL')\ntrain_pneumonia_dir = os.path.join(train_dir, 'PNEUMONIA')\n\n# Directory with our testing NORMAL\/PNEUMONIA pictures\ntest_normal_dir = os.path.join(test_dir, 'NORMAL')\ntest_pneumonia_dir = os.path.join(test_dir, 'PNEUMONIA')\n\n# Directory with our validation NORMAL\/PNEUMONIA pictures\nvalidation_normal_dir = os.path.join(validation_dir, 'NORMAL')\nvalidation_pneumonia_dir = os.path.join(validation_dir, 'PNEUMONIA')\n","84e8e068":"train_normal_fnames = os.listdir( train_normal_dir )\ntrain_pneumonia_fnames = os.listdir( train_pneumonia_dir )\n\nprint(train_normal_fnames[:10])\nprint(train_pneumonia_fnames[:10])","207a77e2":"print('total training normal images :', len(os.listdir(train_normal_dir) ))\nprint('total training pneumonia images :', len(os.listdir(train_pneumonia_dir ) ))\n\nprint('total testing normal images :', len(os.listdir(test_normal_dir)))\nprint('total testing pneumonia images :', len(os.listdir(test_pneumonia_dir) ))\n\nprint('total validation normal images :', len(os.listdir(validation_normal_dir ) ))\nprint('total validation pneumonia images :', len(os.listdir(validation_pneumonia_dir ) ))\n\n\nNB_IMAGES_TRAIN = len(os.listdir(train_normal_dir)) + len(os.listdir(train_pneumonia_dir))\nNB_IMAGES_TEST  = len(os.listdir(test_normal_dir)) + len(os.listdir(test_pneumonia_dir))","52cbd0b4":"def plot_lr_history(history,Title=\"\"):\n    epochs = range(len(history.history['accuracy']))\n    plt.figure(figsize=(10,5))\n    \n    plt.rc('grid', linestyle=\"--\", color='black')\n    plt.semilogx(epochs,history.history[\"lr\"],'-o',label='Learning Rate',color='#d62728')\n    x = np.argmin(history.history['lr'] ); y = np.min(history.history['lr'] )\n    xdist = plt.xlim()[1] - plt.xlim()[0];ydist = plt.ylim()[1] - plt.ylim()[0]\n    plt.scatter(x,y,s=200,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'Min lr\\n%.2E'%y,size=14)\n    plt.ylabel('Learning Rate',size=15);plt.xlabel('Epoch',size=15)\n    plt.xlim(0,len(epochs)+15)\n    plt.legend(loc=2)\n    plt.grid(True)\n    plt.show()\n\n\ndef dashboard_training(history,Title=\"\"):\n    plt.figure(figsize=(15,10))\n    plt.subplot(2, 1, 1)\n    epochs = range(len(history.history['accuracy']))\n    \n    plt.plot(epochs,history.history['accuracy'],'-o',label='Train AUC',color='#ff7f0e')\n    plt.plot(epochs,history.history['val_accuracy'],'-o',label='Val AUC',color='#1f77b4')\n    x = np.argmax(history.history['val_accuracy'] ); y = np.max( history.history['val_accuracy'] )\n    xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n    plt.scatter(x,y,s=200,color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,'max auc\\n%.2f'%y,size=14)\n    plt.ylabel('AUC',size=14); plt.xlabel('Epoch',size=15)\n    plt.legend(loc=2)\n    \n    plt2 = plt.gca().twinx()\n    plt2.plot(epochs,history.history['loss'],'-o',label='Train Loss',color='#2ca02c')\n    plt2.plot(epochs,history.history['val_loss'],'-o',label='Val Loss',color='#d62728')\n    x = np.argmin(history.history['val_loss'] ); y = np.min(history.history['val_loss'] )\n    ydist = plt.ylim()[1] - plt.ylim()[0]\n    plt.scatter(x,y,s=200,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n    plt.ylabel('Loss',size=15)\n    plt.legend(loc=3)\n    \n    plt.title(\"Model : {}\".format(Title),size=18)\n    \n    \n    plt.subplot(2, 2, 3)\n    plt.plot(epochs,history.history['accuracy'],'-o',label='Train AUC',color='#ff7f0e')\n    plt.plot(epochs,history.history['val_accuracy'],'-o',label='Val AUC',color='#1f77b4')\n    x = np.argmax(history.history['val_accuracy'] ); y = np.max( history.history['val_accuracy'] )\n    xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n    plt.scatter(x,y,s=200,color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,'max auc\\n%.2f'%y,size=14)\n    plt.ylabel('AUC',size=14); plt.xlabel('Epoch',size=15)\n    plt.legend(loc=4)\n    \n    \n    plt.subplot(2, 2, 4)\n    plt2 = plt.gca().twinx()\n    plt2.plot(epochs,history.history['loss'],'-o',label='Train Loss',color='#2ca02c')\n    plt2.plot(epochs,history.history['val_loss'],'-o',label='Val Loss',color='#d62728')\n    x = np.argmin(history.history['val_loss'] ); y = np.min(history.history['val_loss'] )\n    xdist = plt.xlim()[1] - plt.xlim()[0];ydist = plt.ylim()[1] - plt.ylim()[0]\n    plt.scatter(x,y,s=200,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n    plt.ylabel('Loss',size=14);plt.xlabel('Epoch',size=15)\n    plt.legend(loc=2)\n    \n    plt.show()","2824ca2e":"DEVICE = \"GPU\" #or \"TPU\"\nIMAGE_SIZE  = 224\nBATCH_SIZE = 8\nEPOCHS = 20\nLR = 0.001 # learning_rate","a3b48368":"if DEVICE == \"GPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        DEVICE = \"GPU\"\n\nif DEVICE != \"TPU\":\n    print(\"Using default strategy for CPU and single GPU\")\n    strategy = tf.distribute.get_strategy()\n\nif DEVICE == \"GPU\":\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n    \n\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","55724b5a":"train_datagen = ImageDataGenerator(\n      rescale=1.\/255,\n      rotation_range=45,\n      width_shift_range=0.3,\n      height_shift_range=0.3,\n      shear_range=0.1,\n      zoom_range=0.25,\n      horizontal_flip=False,\n      fill_mode='nearest')\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\n# Flow training images in batches of 20 using train_datagen generator\ntrain_generator = train_datagen.flow_from_directory(\n        train_dir,  # This is the source directory for training images\n        target_size=(IMAGE_SIZE, IMAGE_SIZE),  # All images will be resized \n        batch_size=BATCH_SIZE,\n        # Since we use binary_crossentropy loss, we need binary labels\n        class_mode='binary')\n\n# Flow validation images in batches of 20 using test_datagen generator\ntest_generator = test_datagen.flow_from_directory(\n        test_dir,\n        target_size=(IMAGE_SIZE, IMAGE_SIZE),\n        batch_size=BATCH_SIZE,\n        class_mode='binary')","ca94562a":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy',optimizer=RMSprop(lr=1e-4),metrics=['accuracy'])","f856318f":"history = model.fit(train_generator,\n                    steps_per_epoch=int(NB_IMAGES_TRAIN\/BATCH_SIZE)\/\/REPLICAS,  # Nv images = batch_size * steps\n                    epochs=EPOCHS,\n                    validation_data=test_generator,\n                    validation_steps=int(NB_IMAGES_TEST\/BATCH_SIZE)\/\/REPLICAS,  # Nb images = batch_size * steps\n                    verbose=1)","abdb5eb3":"dashboard_training(history,\"Simple Model\")","dff276a2":"def get_lr_callback(batch_size=8):\n    lr_start   = 0.000005\n    lr_max     = 0.00000125 * REPLICAS * batch_size\n    lr_min     = 0.000001\n    lr_ramp_ep = 5\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        return lr\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n    return lr_callback","e6f04d03":"# Save the model with the minimum validation loss\n# https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/ModelCheckpoint\nEarlyStopping_cb = EarlyStopping(monitor='val_loss', mode='min', patience=3,restore_best_weights=True, verbose=1)\n\n# Save best model\nCheckpoint_cb = ModelCheckpoint(\"best_model.h5\",save_best_only=True,monitor='val_loss',mode='min')\n    \n# Reduce learning rate once learning stagnates\n# https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/ReduceLROnPlateau\nReduce_lr = ReduceLROnPlateau(monitor='val_loss',factor=0.2,patience=2,min_lr=1e-8,mode='min',verbose=1)\n\nCsv_logger = tf.keras.callbacks.CSVLogger('training.log')\n\nAdamOptimizer = tf.keras.optimizers.Adam(learning_rate = LR)\nRMSpropOptimizer = tf.keras.optimizers.RMSprop(learning_rate = LR)\nSGDOptimizer = tf.keras.optimizers.SGD(learning_rate = LR)\nAdagradOptimizer = tf.keras.optimizers.Adagrad(learning_rate = LR)","7fefb4f9":"from tensorflow.keras import layers\nfrom tensorflow.keras import Model\n!wget --no-check-certificate \\\n    https:\/\/storage.googleapis.com\/mledu-datasets\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n    -O \/tmp\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\n\nlocal_weights_file = '\/tmp\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\npre_trained_model = InceptionV3(input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3), \n                                include_top = False, \n                                weights = None)\n\npre_trained_model.load_weights(local_weights_file)\n\nfor layer in pre_trained_model.layers:\n    layer.trainable = False\n\n# pre_trained_model.summary()\n\nlast_layer = pre_trained_model.get_layer('mixed7')\nprint('last layer output shape: ', last_layer.output_shape)\nlast_output = last_layer.output","e6a55f81":"from tensorflow.keras.optimizers import RMSprop\n\n# Flatten the output layer to 1 dimension\nx = layers.Flatten()(last_output)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(1024, activation='relu')(x)\n# Add a dropout rate of 0.2\nx = layers.Dropout(0.2)(x)                  \n# Add a final sigmoid layer for classification\nx = layers.Dense(1, activation='sigmoid')(x)           \n\nmodel = Model( pre_trained_model.input, x) \n\nmodel.compile(optimizer = Adam(lr=0.0001), \n              loss = tf.keras.losses.Huber(), \n              metrics = ['accuracy'])\n\n\nhistory = model.fit(train_generator,\n                    steps_per_epoch=int(NB_IMAGES_TRAIN\/BATCH_SIZE)\/\/REPLICAS,  # Nv images = batch_size * steps\n                    epochs=EPOCHS,\n                    validation_data=test_generator,\n                    validation_steps=int(NB_IMAGES_TEST\/BATCH_SIZE)\/\/REPLICAS,  # Nb images = batch_size * steps\n                    verbose=1,callbacks=[Csv_logger,Checkpoint_cb,get_lr_callback(BATCH_SIZE)])","a5139f2e":"dashboard_training(history,\"InceptionV3\")","d799d456":"EFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3, \n        efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6]\n\nEFN_NAMES = [\"EfficientNetB0\", \"EfficientNetB1\", \"EfficientNetB2\", \"EfficientNetB3\", \"EfficientNetB4\", \"EfficientNetB5\", \"EfficientNetB6\"]\n\n\ndef create_model(dim, EFNS, ef=0):\n    conv_base = EFNS[ef](input_shape=(dim,dim,3),weights='imagenet',include_top=False)\n    model = conv_base.output\n    model = layers.GlobalAveragePooling2D()(model)\n    # Add a dropout rate of 0.2\n    model = layers.Dropout(0.5)(model)\n    model = layers.Dense(1, activation = \"sigmoid\")(model)\n    model = models.Model(conv_base.input, model)\n\n    model.compile(optimizer = Adam(lr=0.0001), loss=tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])\n    return model\n","194399c5":"EF = 0\nmodel = create_model(IMAGE_SIZE, EFNS, ef=EF)\nhistory = model.fit(train_generator,\n                    steps_per_epoch=int(NB_IMAGES_TRAIN\/BATCH_SIZE)\/\/REPLICAS,  # Nv images = batch_size * steps\n                    epochs=EPOCHS,\n                    validation_data=test_generator,\n                    validation_steps=int(NB_IMAGES_TEST\/BATCH_SIZE)\/\/REPLICAS,  # Nb images = batch_size * steps\n                    verbose=1,callbacks=[Csv_logger,Checkpoint_cb,get_lr_callback(BATCH_SIZE)])","bf06fbde":"dashboard_training(history,EFN_NAMES[EF])","211c2444":"EF = 3\nmodel = create_model(IMAGE_SIZE, EFNS, ef=EF)\nhistory = model.fit(train_generator,\n                    steps_per_epoch=int(NB_IMAGES_TRAIN\/BATCH_SIZE)\/\/REPLICAS,  # Nv images = batch_size * steps\n                    epochs=EPOCHS,\n                    validation_data=test_generator,\n                    validation_steps=int(NB_IMAGES_TEST\/BATCH_SIZE)\/\/REPLICAS,  # Nb images = batch_size * steps\n                    verbose=1,callbacks=[Csv_logger,Checkpoint_cb,get_lr_callback(BATCH_SIZE)])","b5059ca9":"dashboard_training(history,EFN_NAMES[EF])","72c8d157":"!pip install tf-explain\nfrom tensorflow.keras.applications import Xception\n\n\nconv_base = tf.keras.applications.xception.Xception(input_shape=(IMAGE_SIZE,IMAGE_SIZE,3),weights='imagenet',include_top=False)\nmodel = conv_base.output\nmodel = layers.GlobalAveragePooling2D()(model)\n# Add a dropout rate of 0.2\nmodel = layers.Dropout(0.5)(model)\nmodel = layers.Dense(1, activation = \"sigmoid\")(model)\nmodel = models.Model(conv_base.input, model)\n\nmodel.compile(optimizer = Adam(lr=0.0001), loss=tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])\n\n\nhistory = model.fit(train_generator,\n                    steps_per_epoch=int(NB_IMAGES_TRAIN\/BATCH_SIZE)\/\/REPLICAS,  # Nv images = batch_size * steps\n                    epochs=EPOCHS,\n                    validation_data=test_generator,\n                    validation_steps=int(NB_IMAGES_TEST\/BATCH_SIZE)\/\/REPLICAS,  # Nb images = batch_size * steps\n                    verbose=1,callbacks=[Csv_logger,Checkpoint_cb,get_lr_callback(BATCH_SIZE)])","8b3e358b":"dashboard_training(history,\"Xception\")","f0bd98a3":"## InceptionV3","9ee92e08":"## EfficientNet","53dc7fb7":"## Simple Model","172c871e":"## Xception","f48f725a":"### Learning Rate","cccc3288":"### EfficientNetB0","ee780a4d":"### EfficientNetB3","725caf9a":"This is a common train schedule for transfer learning. The learning rate starts near zero, then increases to a maximum, then decays over time. Consider changing the schedule and\/or learning rates. Note how the learning rate max is larger with larger batches sizes. This is a good practice to follow."}}