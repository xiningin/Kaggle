{"cell_type":{"8640fa19":"code","471d1f42":"code","1a6e728c":"code","a1fffdf4":"code","ce0cb45f":"code","90cb6a68":"code","cb771726":"code","df0a3887":"code","f9ff80cb":"code","9edb461e":"code","2698f934":"code","6e0dac17":"code","0597697c":"code","5f5895e8":"code","806ac598":"code","204cdc5a":"code","0d22ca95":"code","bfae8cde":"code","d42e153a":"code","09acd540":"code","ce2a4534":"code","ccdd5ef5":"code","ea681cc7":"code","11b58981":"code","c47d4158":"code","a4ad269f":"code","d60e434a":"code","bfb0fcfa":"code","acfb8850":"markdown","b67fe324":"markdown","7a103198":"markdown","b8667265":"markdown","2380232b":"markdown","382ba582":"markdown","b83e48fe":"markdown","0a71cb80":"markdown","b27803c9":"markdown","7c90470e":"markdown","c3c8c117":"markdown","7f37320e":"markdown","d0269ccb":"markdown"},"source":{"8640fa19":"from sklearn.linear_model import LinearRegression, LassoCV\nfrom tqdm import tqdm\nimport pickle\nfrom scipy import stats, special\nimport pandas as pd\nimport numpy as np\nimport os\nimport warnings\nwarnings.simplefilter(\"ignore\")","471d1f42":"#remove columns with large number of nan values\ndef remove_nan_cols (df, threshold=0.5):\n    nan_cols = []\n    for col in df.columns:\n        nan_ratio = df[col].isnull().sum() \/ df.shape[0]\n        if nan_ratio >= threshold:\n            nan_cols.append(col)\n    df = df.drop(nan_cols, axis=1)\n    return df","1a6e728c":"#filling missing values based on linear regression and the most correlated variables\ndef fillna_using_linear_model(df, feature_cols):\n\n    correl = df[feature_cols].corr()\n\n    for col in tqdm(feature_cols):\n        nan_ratio = df[col].isnull().sum() \/ df.shape[0]\n        if nan_ratio > 0:\n            best_nan_ratio = nan_ratio\n            best_col = None\n            for id in correl.loc[(correl[col] > 0.7) | (correl[col] < -0.7), col].index:\n                nan_temp_ratio = df[id].isnull().sum() \/ df.shape[0]\n                if best_nan_ratio > nan_temp_ratio:\n                    best_nan_ratio = nan_temp_ratio\n                    best_col = id\n            if best_col != None:\n                sub = df[[col, best_col]].copy()\n                sub = sub.dropna()\n                reg = LinearRegression(fit_intercept=True).fit(np.expand_dims(sub[best_col], axis=1), sub[col])\n                print(reg.score(np.expand_dims(sub[best_col], axis=1), sub[col]))\n                if reg.score(np.expand_dims(sub[best_col], axis=1), sub[col])>0.7:\n                    if df.loc[(~df[best_col].isnull()) & (df[col].isnull()), col].shape[0] > 0:\n                        df.loc[(~df[best_col].isnull()) & (df[col].isnull()), col] = \\\n                            reg.predict(np.expand_dims(\n                                df.loc[(~df[best_col].isnull()) & (df[col].isnull()), best_col], axis=1))\n\n    return df","a1fffdf4":"train_df = pd.read_csv ( \"..\/input\/widsdatathon2021\/TrainingWiDS2021.csv\").drop(columns=['Unnamed: 0'], axis=1)\ntest_df = pd.read_csv(\"..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv\").drop(columns=['Unnamed: 0'], axis=1)\ndata_dict = pd.read_csv (\"..\/input\/widsdatathon2021\/DataDictionaryWiDS2021.csv\")\n\nagg = train_df['icu_id'].value_counts()\nagg","ce0cb45f":"# agg = train_df['icu_id'].value_counts().to_dict() #How many times icu_id is repeated?\n# train_df['icu_id_counts'] = np.log1p(train_df['icu_id'].map(agg))\n# train_df[['icu_id','icu_id_counts']]\n\n\n\n# agg = train_df['age'].value_counts().to_dict()\n\n# # train_df['age_counts'] = np.log1p(train_df['age'].map(agg))\n\ntrain_df['nan_counts'] = train_df.isnull().sum(axis=1)\ntrain_df[['nan_counts']]","90cb6a68":"train_df.to_csv('x.csv')","cb771726":"def feature_generation(df):\n\n    agg = df['icu_id'].value_counts().to_dict() #How many times icu_id is repeated?\n    df['icu_id_counts'] = np.log1p(df['icu_id'].map(agg)) #took a log of agg counts\n    agg = df['age'].value_counts().to_dict()#How many times age is repeated?\n    df['age_counts'] = np.log1p(df['age'].map(agg))#took a log of agg counts\n\n    df['nan_counts'] = df.isnull().sum(axis=1)#How many columns showed a blank for each row?\n    \n\n    df['sq_age'] = df['age'].values ** 2 #binomial age\n    df['sq_bmi'] = df['bmi'].values ** 2 #binomial bmi\n    df['bmi_age'] = df['bmi'].values \/ df['age'].values #bmi\/age\n    df['weight_age'] = df['age'].values \/ df['weight'].values #age\/weight --- what and why?\n    \n    \n    #Need to discuss this\n    df['comorbidity_score'] = df['aids'].values * 23 + df['cirrhosis'].values * 4 + df['diabetes_mellitus'].values * 1 + \\\n                                 df['hepatic_failure'].values * 16 + df['immunosuppression'].values * 10 + \\\n                              df['leukemia'].values * 10 + df['lymphoma'].values * 13 + df['solid_tumor_with_metastasis'].values * 11\n    \n    \n    #source https:\/\/www.omnicalculator.com\/health\/risk-dm\n    df['diabete_risk'] = 100 \/ (1 + np.exp(-1*(0.028*df['age'].values + 0.661*np.where(df['gender'].values==\"M\", 1, 0) +\n                                               0.412 * np.where(df['ethnicity'].values==\"Native American\", 0, 1) +\n                                               0.079 * df['glucose_apache'].values + 0.018 * df['d1_diasbp_max'].values +\n                                               0.07 * df['bmi'].values + 0.481 * df['cirrhosis'].values - 13.415)))\n\n    df['gcs_sum'] = df['gcs_eyes_apache'].values + df['gcs_motor_apache'].values + df['gcs_verbal_apache'].values\n    df['apache_2_diagnosis_type'] = df.apache_2_diagnosis.round(-1).fillna(-100).astype('int32')\n    df['apache_3j_diagnosis_type'] = df.apache_3j_diagnosis.round(-2).fillna(-100).astype('int32')\n    df['bmi_type'] = df.bmi.fillna(0).apply(lambda x: 5 * (round(int(x) \/ 5)))\n    df['height_type'] = df.height.fillna(0).apply(lambda x: 5 * (round(int(x) \/ 5)))\n    df['weight_type'] = df.weight.fillna(0).apply(lambda x: 5 * (round(int(x) \/ 5)))\n    df['age_type'] = df.age.fillna(0).apply(lambda x: 10 * (round(int(x) \/ 10)))\n    df['gcs_sum_type'] = df.gcs_sum.fillna(0).apply(lambda x: 2.5 * (round(int(x) \/ 2.5))).divide(2.5)\n    df['apache_3j_diagnosis_x'] = df['apache_3j_diagnosis'].astype('str').str.split('.', n=1, expand=True)[0]\n    df['apache_2_diagnosis_x'] = df['apache_2_diagnosis'].astype('str').str.split('.', n=1, expand=True)[0]\n    df['apache_3j_diagnosis_split1'] = np.where(df['apache_3j_diagnosis'].isna(), np.nan,\n                                                   df['apache_3j_diagnosis'].astype('str').str.split('.', n=1,\n                                                                                                        expand=True)[1])\n    df['apache_2_diagnosis_split1'] = np.where(df['apache_2_diagnosis'].isna(), np.nan,\n                                                  df['apache_2_diagnosis'].apply(lambda x: x % 10))\n\n    IDENTIFYING_COLS = ['age_type', 'gcs_sum_type', 'ethnicity', 'gender', 'bmi_type']\n    df['profile'] = df[IDENTIFYING_COLS].apply(lambda x: hash(tuple(x)), axis=1)\n\n    df[\"diff_bmi\"] = df['bmi'].copy()\n    df['bmi'] = df['weight'].values \/ ((df['height'].values \/ 100) ** 2)\n    df[\"diff_bmi\"] = df[\"diff_bmi\"].values - df['bmi'].values\n\n    df['pre_icu_los_days'] = df['pre_icu_los_days'].apply(lambda x: special.expit(x))\n\n    d_cols = [c for c in df.columns if (c.startswith(\"d1\"))]\n    h_cols = [c for c in df.columns if (c.startswith(\"h1\"))]\n    df[\"dailyLabs_row_nan_count\"] = df[d_cols].isna().sum(axis=1)\n    df[\"hourlyLabs_row_nan_count\"] = df[h_cols].isna().sum(axis=1)\n    df[\"diff_labTestsRun_daily_hourly\"] = df[\"dailyLabs_row_nan_count\"].values - df[\"hourlyLabs_row_nan_count\"].values\n\n    lab_col = [c for c in df.columns if ((c.startswith(\"h1\")) | (c.startswith(\"d1\")))]\n    lab_col_names = list(set(list(map(lambda i: i[3: -4], lab_col))))\n\n    first_h = []\n    for v in tqdm(lab_col_names):\n        colsx = [x for x in df.columns if v in x]\n        df[v + \"_nans\"] = df.loc[:, colsx].isna().sum(axis=1)\n        df[v + \"_d1_h1_max_eq\"] = (df[f\"d1_{v}_max\"] == df[f\"h1_{v}_max\"]).astype(np.int8)\n        df[v + \"_d1_h1_min_eq\"] = (df[f\"d1_{v}_min\"] == df[f\"h1_{v}_min\"]).astype(np.int8)\n\n        for freq in ['h1', 'd1']:\n            df[v + f\"_{freq}_value_range\"] = df[f\"{freq}_{v}_max\"].subtract(df[f\"{freq}_{v}_min\"])\n            df[v + f\"_{freq}_zero_range\"] = (df[v + f\"_{freq}_value_range\"] == 0).astype(np.int8)\n            df[v + f\"_{freq}_mean\"] =np.nanmean(df[[f\"{freq}_{v}_max\", f\"{freq}_{v}_min\"]].values, axis=1)\n            df[v + f\"_{freq}_std\"] = np.nanstd(df[[f\"{freq}_{v}_max\", f\"{freq}_{v}_min\"]].values, axis=1)\n\n            for g in ['apache_3j_diagnosis', 'profile', 'icu_id']:\n                for m in ['max', 'min']:\n                    temp = df[[g, f\"{freq}_{v}_{m}\"]].groupby(g)\n                    df[v + f\"_{freq}_{m}_{g}_mean\"] = temp.transform('mean')\n                    df[v + f\"_{freq}_{m}_{g}_diff\"] = df[v + f\"_{freq}_{m}_{g}_mean\"].subtract(df[f\"{freq}_{v}_{m}\"])\n                    df[v + f\"_{freq}_{m}_{g}_std\"] = temp.transform('std')\n                    df[v + f\"_{freq}_{m}_{g}_norm_std\"] = df[v + f\"_{freq}_{m}_{g}_std\"].div(df[f\"{freq}_{v}_{m}\"])\n                    df[v + f\"_{freq}_{m}_{g}_rank\"] =temp.transform('rank')\n                    df[v + f\"_{freq}_{m}_{g}_count\"] = temp.transform('count')\n                    df[v + f\"_{freq}_{m}_{g}_norm_rank\"] = df[v + f\"_{freq}_{m}_{g}_rank\"].div(df[v + f\"_{freq}_{m}_{g}_count\"])\n                    df[v + f\"_{freq}_{m}_{g}_skew\"] = temp.transform('skew')\n\n            if v + \"_apache\" in colsx:\n                for m in ['max', 'min']:\n                    df[v + f\"_apache_{freq}_{m}_ratio\"] = df[f\"{freq}_{v}_{m}\"].div(df[v + \"_apache\"])\n\n            for m in ['max', 'min']:\n                df[f\"{freq}_{v}_{m}_bmi_ratio\"] = df[f\"{freq}_{v}_{m}\"].div(df['bmi'])\n                df[f\"{freq}_{v}_{m}_weight_ratio\"] = df[f\"{freq}_{v}_{m}\"].div(df['weight'])\n\n        df[v + \"_range_between_d_h\"] = df[v + \"_d1_mean\"].values - df[v + \"_h1_mean\"].values\n        df[v + \"_d1_h1_mean\"] = np.nanmean(df[[f\"d1_{v}_max\", f\"d1_{v}_min\", f\"h1_{v}_max\", f\"h1_{v}_min\"]].values, axis=1)\n        df[v + \"_d1_h1_std\"] = np.nanstd(df[[f\"d1_{v}_max\", f\"d1_{v}_min\", f\"h1_{v}_max\", f\"h1_{v}_min\"]].values, axis=1)\n\n        df[v + \"_tot_change_value_range_normed\"] = abs((df[v + \"_d1_value_range\"].div(df[v + \"_h1_value_range\"])))\n        df[v + \"_started_after_firstHour\"] = ((df[f\"h1_{v}_max\"].isna()) & (df[f\"h1_{v}_min\"].isna())) & (~df[f\"d1_{v}_max\"].isna())\n        first_h.append(v + \"_started_after_firstHour\")\n        df[v + \"_day_more_extreme\"] = ((df[f\"d1_{v}_max\"] > df[f\"h1_{v}_max\"]) | (df[f\"d1_{v}_min\"] < df[f\"h1_{v}_min\"]))\n        df[v + \"_day_more_extreme\"].fillna(False)\n\n    df[\"total_Tests_started_After_firstHour\"] = df[first_h].sum(axis=1)\n\n    df['diasbp_indicator'] = (\n            (df['d1_diasbp_invasive_max'] == df['d1_diasbp_max']) & (\n            df['d1_diasbp_noninvasive_max'] == df['d1_diasbp_invasive_max']) |\n            (df['d1_diasbp_invasive_min'] == df['d1_diasbp_min']) & (\n                    df['d1_diasbp_noninvasive_min'] == df['d1_diasbp_invasive_min']) |\n            (df['h1_diasbp_invasive_max'] == df['h1_diasbp_max']) & (\n                    df['h1_diasbp_noninvasive_max'] == df['h1_diasbp_invasive_max']) |\n            (df['h1_diasbp_invasive_min'] == df['h1_diasbp_min']) & (\n                    df['h1_diasbp_noninvasive_min'] == df['h1_diasbp_invasive_min'])).astype(np.int8)\n\n    df['mbp_indicator'] = (\n            (df['d1_mbp_invasive_max'] == df['d1_mbp_max']) & (\n            df['d1_mbp_noninvasive_max'] == df['d1_mbp_invasive_max']) |\n            (df['d1_mbp_invasive_min'] == df['d1_mbp_min']) & (\n                    df['d1_mbp_noninvasive_min'] == df['d1_mbp_invasive_min']) |\n            (df['h1_mbp_invasive_max'] == df['h1_mbp_max']) & (\n                    df['h1_mbp_noninvasive_max'] == df['h1_mbp_invasive_max']) |\n            (df['h1_mbp_invasive_min'] == df['h1_mbp_min']) & (\n                    df['h1_mbp_noninvasive_min'] == df['h1_mbp_invasive_min'])\n    ).astype(np.int8)\n\n    df['sysbp_indicator'] = (\n            (df['d1_sysbp_invasive_max'] == df['d1_sysbp_max']) & (\n            df['d1_sysbp_noninvasive_max'] == df['d1_sysbp_invasive_max']) |\n            (df['d1_sysbp_invasive_min'] == df['d1_sysbp_min']) & (\n                    df['d1_sysbp_noninvasive_min'] == df['d1_sysbp_invasive_min']) |\n            (df['h1_sysbp_invasive_max'] == df['h1_sysbp_max']) & (\n                    df['h1_sysbp_noninvasive_max'] == df['h1_sysbp_invasive_max']) |\n            (df['h1_sysbp_invasive_min'] == df['h1_sysbp_min']) & (\n                    df['h1_sysbp_noninvasive_min'] == df['h1_sysbp_invasive_min'])\n    ).astype(np.int8)\n\n    df['d1_mbp_invnoninv_max_diff'] = df['d1_mbp_invasive_max'].div(df['d1_mbp_noninvasive_max'])\n    df['h1_mbp_invnoninv_max_diff'] = df['h1_mbp_invasive_max'].div(df['h1_mbp_noninvasive_max'])\n    df['d1_mbp_invnoninv_min_diff'] = df['d1_mbp_invasive_min'].div(df['d1_mbp_noninvasive_min'])\n    df['h1_mbp_invnoninv_min_diff'] = df['h1_mbp_invasive_min'].div(df['h1_mbp_noninvasive_min'])\n    df['d1_diasbp_invnoninv_max_diff'] = df['d1_diasbp_invasive_max'].div(df['d1_diasbp_noninvasive_max'])\n    df['h1_diasbp_invnoninv_max_diff'] = df['h1_diasbp_invasive_max'].div(df['h1_diasbp_noninvasive_max'])\n    df['d1_diasbp_invnoninv_min_diff'] = df['d1_diasbp_invasive_min'].div(df['d1_diasbp_noninvasive_min'])\n    df['h1_diasbp_invnoninv_min_diff'] = df['h1_diasbp_invasive_min'].div(df['h1_diasbp_noninvasive_min'])\n    df['d1_sysbp_invnoninv_max_diff'] = df['d1_sysbp_invasive_max'].div(df['d1_sysbp_noninvasive_max'])\n    df['h1_sysbp_invnoninv_max_diff'] = df['h1_sysbp_invasive_max'].div(df['h1_sysbp_noninvasive_max'])\n    df['d1_sysbp_invnoninv_min_diff'] = df['d1_sysbp_invasive_min'].div(df['d1_sysbp_noninvasive_min'])\n    df['h1_sysbp_invnoninv_min_diff'] = df['h1_sysbp_invasive_min'].div(df['h1_sysbp_noninvasive_min'])\n\n    more_extreme_cols = [c for c in df.columns if (c.endswith(\"_day_more_extreme\"))]\n    df[\"total_day_more_extreme\"] = df[more_extreme_cols].sum(axis=1)\n    df[\"d1_resprate_div_mbp_min\"] = df[\"d1_resprate_min\"].div(df[\"d1_mbp_min\"])\n    df[\"d1_resprate_div_sysbp_min\"] = df[\"d1_resprate_min\"].div(df[\"d1_sysbp_min\"])\n    df[\"d1_lactate_min_div_diasbp_min\"] = df[\"d1_lactate_min\"].div(df[\"d1_diasbp_min\"])\n    df[\"d1_heartrate_min_div_d1_sysbp_min\"] = df[\"d1_heartrate_min\"].div(df[\"d1_sysbp_min\"])\n    df[\"total_chronic\"] = df[[\"aids\", \"cirrhosis\", 'hepatic_failure']].sum(axis=1)\n    df[\"total_cancer_immuno\"] = df[['immunosuppression', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis']].sum(axis=1)\n    df[\"has_complicator\"] = df[[\"aids\", \"cirrhosis\", 'hepatic_failure','immunosuppression', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis']].max(axis=1)\n\n    df['apache_3j'] = np.where(df['apache_3j_diagnosis_type'] < 0, np.nan,\n                               np.where(df['apache_3j_diagnosis_type'] < 200, 'Cardiovascular',\n                                        np.where(df['apache_3j_diagnosis_type'] < 400, 'Respiratory',\n                                                 np.where(df['apache_3j_diagnosis_type'] < 500, 'Neurological',\n                                                          np.where(df['apache_3j_diagnosis_type'] < 600, 'Sepsis',\n                                                                   np.where(df['apache_3j_diagnosis_type'] < 800,\n                                                                            'Trauma',\n                                                                            np.where(\n                                                                                df['apache_3j_diagnosis_type'] < 900,\n                                                                                'Haematological',\n                                                                                np.where(df[\n                                                                                             'apache_3j_diagnosis_type'] < 1000,\n                                                                                         'Renal\/Genitourinary',\n                                                                                         np.where(df[\n                                                                                                      'apache_3j_diagnosis_type'] < 1200,\n                                                                                                  'Musculoskeletal\/Skin disease',\n                                                                                                  'Operative Sub-Diagnosis Codes')))))))))\n\n\n    return df\n","df0a3887":"#remove columns with low variation\ndef remove_feature_with_low_var(df, threshold=0.1):\n    for col in df.columns:\n        if df[col].std() < threshold:\n            df = df.drop([col], axis=1)\n    return df","f9ff80cb":"if os.path.isfile('..\/input\/wids-2021-preprocessed-data\/train_df.pkl'):\n    with open('..\/input\/wids-2021-preprocessed-data\/train_df.pkl', 'rb') as handle:\n        train_df = pickle.load(handle)\n\n    with open('..\/input\/wids-2021-preprocessed-data\/test_df.pkl', 'rb') as handle:\n        test_df = pickle.load(handle)\n\nelse:\n    \n    train_df = pd.read_csv ( \"..\/input\/widsdatathon2021\/TrainingWiDS2021.csv\").drop(columns=['Unnamed: 0'], axis=1)\n    test_df = pd.read_csv(\"..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv\").drop(columns=['Unnamed: 0'], axis=1)\n    data_dict = pd.read_csv (\"..\/input\/widsdatathon2021\/DataDictionaryWiDS2021.csv\")\n\n    train_df['which_data'] = 'train'\n    test_df['which_data'] = 'test'\n    test_df['diabetes_mellitus'] = 0\n\n    df = pd.concat([train_df, test_df[train_df.columns]], axis=0)\n\n    del train_df, test_df\n\n    df['age'] = np.where(df['age'].values==0, np.nan, df['age'].values)\n    df = df.replace([np.inf, -np.inf], np.nan)\n\n    min_max_feats=[f[:-4] for f in df.columns if f[-4:]=='_min']\n    for col in min_max_feats:\n        df.loc[df[f'{col}_min'] > df[f'{col}_max'], [f'{col}_min', f'{col}_max']] = \\\n            df.loc[df[f'{col}_min'] > df[f'{col}_max'], [f'{col}_max', f'{col}_min']].values\n\n    nan_col = False\n    if nan_col ==True:\n        df = remove_nan_cols(df, threshold=0.75)\n\n    cont_cols = []\n    for col in df.columns:\n        if df[col].dtype=='float64':\n            cont_cols.append(col)\n\n    fillna=True\n    if fillna==True:\n        df = fillna_using_linear_model(df, cont_cols)\n\n    df = feature_generation(df)\n    df = df.replace([np.inf, -np.inf], np.nan)\n        \n    cats = ['elective_surgery', 'icu_id', 'arf_apache', 'intubated_apache', 'ventilated_apache', 'cirrhosis',\n        'hepatic_failure', 'immunosuppression', 'leukemia', 'solid_tumor_with_metastasis', 'apache_3j_diagnosis_x',\n        'apache_2_diagnosis_x', 'apache_3j', 'apache_3j_diagnosis_split1', 'apache_2_diagnosis_split1', 'gcs_sum_type',\n        'hospital_admit_source', 'glucose_rate', 'glucose_wb', 'gcs_eyes_apache', 'glucose_normal', 'total_cancer_immuno',\n        'gender', 'total_chronic', 'icu_stay_type', 'apache_2_diagnosis_type', 'apache_3j_diagnosis_type']\n\n\n    df['hospital_admit_source'] = df['hospital_admit_source'].replace({'Other ICU': 'ICU','ICU to SDU':'SDU',\n                                                                             'Step-Down Unit (SDU)': 'SDU',\n                                                                             'Other Hospital':'Other','Observation': 'Recovery Room',\n                                                                             'Acute Care\/Floor': 'Acute Care'})\n\n    drop_cols = []\n    features = df.columns\n    features = [f for f in features if f not in cats]\n    for col in features:\n        if df[col].dtype!='object' and col!='diabetes_mellitus':\n            if df[col].std()==0 or df[col].std()==np.nan:\n                drop_cols.append(col)\n\n    df = df.drop(drop_cols, axis=1)\n\n    cats = [f for f in cats if f in df.columns]\n    train_df = df.loc[df['which_data']=='train']\n    test_df = df.loc[df['which_data']=='test']\n    train_df = train_df.drop(['which_data'], axis=1)\n    test_df = test_df.drop(['which_data'], axis=1)\n\n    for col in train_df.select_dtypes(exclude = np.number).columns.tolist():\n        train_only = list(set(train_df[col].unique()) - set(test_df[col].unique()))\n        test_only = list(set(test_df[col].unique()) - set(train_df[col].unique()))\n        both = list(set(test_df[col].unique()).union(set(train_df[col].unique())))\n        train_df.loc[train_df[col].isin(train_only), col] = np.nan\n        test_df.loc[test_df[col].isin(test_only), col] = np.nan\n\n        test_df[col] = test_df[col].astype('str')\n        train_df[col] = train_df[col].astype('str')\n        le = LabelEncoder()\n        le.fit(pd.concat([train_df[col], test_df[col]]))\n\n        train_df[col] = le.transform(train_df[col])\n        test_df[col] = le.transform(test_df[col])\n\n\n    for col in tqdm(cats):\n        train_only = list(set(train_df[col].unique()) - set(test_df[col].unique()))\n        test_only = list(set(test_df[col].unique()) - set(train_df[col].unique()))\n        both = list(set(test_df[col].unique()).union(set(train_df[col].unique())))\n        train_df.loc[train_df[col].isin(train_only), col] = np.nan\n        test_df.loc[test_df[col].isin(test_only), col] = np.nan\n        try:\n            le = LabelEncoder()\n            le.fit(pd.concat([train_df[col], test_df[col]]))\n            train_df[col] = le.transform(train_df[col])\n            test_df[col] = le.transform(test_df[col])\n        except:\n            test_df[col] = test_df[col].astype('str').fillna('-1')\n            train_df[col] = train_df[col].astype('str').fillna('-1')\n            le = LabelEncoder()\n            le.fit(pd.concat([train_df[col], test_df[col]]))\n            train_df[col] = le.transform(train_df[col])\n            test_df[col] = le.transform(test_df[col])\n\n        temp = pd.concat([train_df[[col]], test_df[[col]]], axis=0)\n        temp_mapping = temp.groupby(col).size()\/len(temp)\n        temp['enc'] = temp[col].map(temp_mapping)\n        temp['enc'] = stats.rankdata(temp['enc'])\n        temp = temp.reset_index(drop=True)\n        train_df[f'rank_frqenc_{col}'] = temp[['enc']].values[:train_df.shape[0]]\n        test_df[f'rank_frqenc_{col}'] = temp[['enc']].values[train_df.shape[0]:]\n        test_df[col] = test_df[col].astype('category')\n        train_df[col] = train_df[col].astype('category')\n\n\n    feature_cols = list(train_df)\n    feature_cols.remove('diabetes_mellitus')\n    feature_cols.remove('encounter_id')\n    feature_cols.remove('hospital_id')\n\n    cont_cols = feature_cols\n    cont_cols = [f for f in cont_cols if f not in cats]\n\n    for col in cont_cols:\n        r = stats.ks_2samp(train_df[col].dropna(), test_df[col].dropna())\n        print(col, r)\n        if r[0] >= 0.05:\n            cont_cols.remove(col)\n\n    '''\n    temp = pd.concat([train_df[cont_cols], test_df[cont_cols]], axis=0)\n    corr = temp[cont_cols].corr()\n\n    drop_columns=[]\n    # Drop highly correlated features\n    columns = np.full((corr.shape[0],), True, dtype=bool)\n    for i in range(corr.shape[0]):\n        for j in range(i+1, corr.shape[0]):\n            if corr.iloc[i,j] >=0.999:\n                if columns[j]:\n                    columns[j] = False\n                    print('FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(temp[cont_cols].columns[i] , temp[cont_cols].columns[j], corr.iloc[i,j]))\n            elif corr.iloc[i,j] <= -0.999:\n                if columns[j]:\n                    columns[j] = False\n                    print('FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(temp[cont_cols].columns[i], temp[cont_cols].columns[j], corr.iloc[i, j]))\n\n    drop_columns = temp[cont_cols].columns[columns == False].values\n\n    print('drop_columns',len(drop_columns),drop_columns)\n\n    cont_cols = [col for col in cont_cols if col not in drop_columns]\n    '''\n    \n    print(\"Number of input variables:\", len(cont_cols)+len(cats))\n\n    train_df = train_df[cont_cols+cats+['encounter_id', 'diabetes_mellitus']]\n    test_df = test_df[cont_cols+cats+['encounter_id']]\n\n    with open('data\/train_df.pkl', 'wb') as fout:\n        pickle.dump(train_df, fout)\n\n    with open('data\/test_df.pkl', 'wb') as fout:\n        pickle.dump(test_df, fout)\n","9edb461e":"def get_apache_meta_features(df):\n\n    df['d1_glucose_min_flag'] = np.where(df['d1_glucose_min'] > (120*.0555), 1, 0)\n\n    df['SerumPotassium_apache'] = (df['d1_potassium_min'].values + df['d1_potassium_max'].values)\/2\n    df['SerumBicarb_apache'] = (df['d1_hco3_min'].values + df['d1_hco3_max'].values)\/2\n\n    df['temperature'] = 0\n    df.loc[(df['temp_apache']< 36) | (df['temp_apache'] > 38.5), \"temperature\"] = 4\n\n    df['arterial_pressure'] = 0\n    df.loc[df['map_apache'] < 50, 'arterial_pressure'] = 4\n    df.loc[(df['map_apache'] >= 50) &\n           (df['map_apache'] < 70), 'arterial_pressure'] = 2\n    df.loc[(df['map_apache'] >= 70) &\n           (df['map_apache'] < 110), 'arterial_pressure'] = 0\n    df.loc[(df['map_apache'] >= 110) &\n           (df['map_apache'] < 130), 'arterial_pressure'] = 2\n    df.loc[(df['map_apache'] >= 130) &\n           (df['map_apache'] < 160), 'arterial_pressure'] = 3\n    df.loc[df['map_apache'] >= 160, 'arterial_pressure'] = 4\n\n    df['heart_rate_pulse'] = 0\n    df.loc[df['heart_rate_apache'] < 40, 'heart_rate_pulse'] = 4\n    df.loc[(df['heart_rate_apache'] >= 40) &\n           (df['heart_rate_apache'] < 55), 'heart_rate_pulse'] = 3\n    df.loc[(df['heart_rate_apache'] >= 55) &\n           (df['heart_rate_apache'] < 70), 'heart_rate_pulse'] = 2\n    df.loc[(df['heart_rate_apache'] >= 70) &\n           (df['heart_rate_apache'] < 110), 'heart_rate_pulse'] = 0\n    df.loc[(df['heart_rate_apache'] >= 110) &\n           (df['heart_rate_apache'] < 140), 'heart_rate_pulse'] = 2\n    df.loc[(df['heart_rate_apache'] >= 140) &\n           (df['heart_rate_apache'] < 180), 'heart_rate_pulse'] = 3\n    df.loc[df['heart_rate_apache'] >= 180, 'heart_rate_pulse'] = 4\n\n    df['respiration_rate'] = 0\n    df.loc[df['resprate_apache'] < 6, 'respiration_rate'] = 4\n    df.loc[(df['resprate_apache'] >= 6) &\n           (df['resprate_apache'] < 10), 'respiration_rate'] = 3\n    df.loc[(df['resprate_apache'] >= 10) &\n           (df['resprate_apache'] < 12), 'respiration_rate'] = 2\n    df.loc[(df['resprate_apache'] >= 12) &\n           (df['resprate_apache'] < 25), 'respiration_rate'] = 0\n    df.loc[(df['resprate_apache'] >= 25) &\n           (df['resprate_apache'] < 35), 'respiration_rate'] = 2\n    df.loc[(df['resprate_apache'] >= 35) &\n           (df['resprate_apache'] < 50), 'respiration_rate'] = 3\n    df.loc[df['resprate_apache'] >= 50, 'respiration_rate'] = 4\n\n    df['oxygenation_rate'] = 0\n    df['pao2_apache'] = df['pao2_apache'].fillna(0)\n\n    df.loc[df['pao2_apache'] < 200, 'oxygenation_rate'] = 0\n    df.loc[(df['pao2_apache'] >= 200) &\n           (df['pao2_apache'] < 350), 'oxygenation_rate'] = 2\n    df.loc[(df['pao2_apache'] >= 350) &\n           (df['pao2_apache'] < 500), 'oxygenation_rate'] = 3\n    df.loc[df['pao2_apache'] >= 500, 'oxygenation_rate'] = 4\n\n    df.drop(columns=['pao2_apache'], inplace=True)\n\n    df['serum_bicarb'] = 0\n    df.loc[df['SerumBicarb_apache'] < 15, 'serum_bicarb'] = 4\n    df.loc[(df['SerumBicarb_apache'] >= 14) &\n           (df['SerumBicarb_apache'] < 18), 'serum_bicarb'] = 3\n    df.loc[(df['SerumBicarb_apache'] >= 18) &\n           (df['SerumBicarb_apache'] < 22), 'serum_bicarb'] = 2\n    df.loc[(df['SerumBicarb_apache'] >= 22) &\n           (df['SerumBicarb_apache'] < 32), 'serum_bicarb'] = 0\n    df.loc[(df['SerumBicarb_apache'] >= 32) &\n           (df['SerumBicarb_apache'] < 41), 'serum_bicarb'] = 2\n    df.loc[(df['SerumBicarb_apache'] >= 41) &\n           (df['SerumBicarb_apache'] < 52), 'serum_bicarb'] = 3\n    df.loc[df['SerumBicarb_apache'] >= 52, 'serum_bicarb'] = 4\n\n    df['arterial_ph'] = 0\n\n    df.loc[df['ph_apache'] < 7.15, 'arterial_ph'] = 4\n    df.loc[(df['ph_apache'] >= 7.15) &\n           (df['ph_apache'] < 7.25), 'arterial_ph'] = 3\n    df.loc[(df['ph_apache'] >= 7.25) &\n           (df['ph_apache'] < 7.33), 'arterial_ph'] = 2\n    df.loc[(df['ph_apache'] >= 7.33) &\n           (df['ph_apache'] < 7.5), 'arterial_ph'] = 0\n    df.loc[(df['ph_apache'] >= 7.5) &\n           (df['ph_apache'] < 7.6), 'arterial_ph'] = 1\n    df.loc[(df['ph_apache'] >= 7.6) &\n           (df['ph_apache'] < 7.7), 'arterial_ph'] = 3\n    df.loc[df['ph_apache'] >= 7.7, 'arterial_ph'] = 4\n\n    df.drop(columns=['ph_apache'], inplace=True)\n\n    df['serum_sodium'] = 0\n    df.loc[df['sodium_apache'] < 111, 'serum_sodium'] = 4\n    df.loc[(df['sodium_apache'] >= 111) &\n           (df['sodium_apache'] < 120), 'serum_sodium'] = 3\n    df.loc[(df['sodium_apache'] >= 120) &\n           (df['sodium_apache'] < 130), 'serum_sodium'] = 2\n    df.loc[(df['sodium_apache'] >= 130) &\n           (df['sodium_apache'] < 150), 'serum_sodium'] = 0\n    df.loc[(df['sodium_apache'] >= 150) &\n           (df['sodium_apache'] < 155), 'serum_sodium'] = 1\n    df.loc[(df['sodium_apache'] >= 155) &\n           (df['sodium_apache'] < 160), 'serum_sodium'] = 2\n    df.loc[(df['sodium_apache'] >= 160) &\n           (df['sodium_apache'] < 180), 'serum_sodium'] = 3\n    df.loc[df['sodium_apache'] >= 180, 'serum_sodium'] = 4\n\n    df['serum_potassium'] = 0\n    df.loc[df['SerumPotassium_apache'] < 2.5, 'serum_potassium'] = 4\n    df.loc[(df['SerumPotassium_apache'] >= 2.5) &\n           (df['SerumPotassium_apache'] < 3), 'serum_potassium'] = 3\n    df.loc[(df['SerumPotassium_apache'] >= 3) &\n           (df['SerumPotassium_apache'] < 3.5), 'serum_potassium'] = 2\n    df.loc[(df['SerumPotassium_apache'] >= 3.5) &\n           (df['SerumPotassium_apache'] < 5.5), 'serum_potassium'] = 0\n    df.loc[(df['SerumPotassium_apache'] >= 5.5) &\n           (df['SerumPotassium_apache'] < 6), 'serum_potassium'] = 2\n    df.loc[(df['SerumPotassium_apache'] >= 6) &\n           (df['SerumPotassium_apache'] < 7), 'serum_potassium'] = 3\n    df.loc[df['SerumPotassium_apache'] >= 7, 'serum_potassium'] = 4\n\n    df['creatinine'] = 0\n    df.loc[df['creatinine_apache'] < .62, 'creatinine'] = 4\n    df.loc[(df['creatinine_apache'] >= .62) &\n           (df['creatinine_apache'] < 1.47), 'creatinine'] = 0\n    df.loc[(df['creatinine_apache'] >= 1.47) &\n           (df['creatinine_apache'] < 1.98), 'creatinine'] = 4\n    df.loc[(df['creatinine_apache'] >= 1.98) &\n           (df['creatinine_apache'] < 3.39), 'creatinine'] = 2\n    df.loc[df['creatinine_apache'] >= 3.39, 'creatinine'] = 8\n\n    df['acute_renal_failure'] = 0\n    df.loc[df['arf_apache'] == 1, 'acute_renal_failure'] = 4\n\n    df['hematocrits'] = 0\n    df.loc[df['creatinine_apache'] < 20, 'hematocrits'] = 4\n    df.loc[(df['creatinine_apache'] >= 20) &\n           (df['creatinine_apache'] < 30), 'hematocrits'] = 2\n    df.loc[(df['creatinine_apache'] >= 30) &\n           (df['creatinine_apache'] < 46), 'hematocrits'] = 0\n    df.loc[(df['creatinine_apache'] >= 46) &\n           (df['creatinine_apache'] < 50), 'hematocrits'] = 1\n    df.loc[(df['creatinine_apache'] >= 50) &\n           (df['creatinine_apache'] < 60), 'hematocrits'] = 2\n    df.loc[df['creatinine_apache'] >= 60, 'hematocrits'] = 4\n\n    df['white_blood_cells'] = 0\n    df.loc[df['wbc_apache'] < 1, 'white_blood_cells'] = 4\n    df.loc[(df['wbc_apache'] >= 1) &\n           (df['wbc_apache'] < 3), 'white_blood_cells'] = 2\n    df.loc[(df['wbc_apache'] >= 3) &\n           (df['wbc_apache'] < 15), 'white_blood_cells'] = 0\n    df.loc[(df['wbc_apache'] >= 15) &\n           (df['wbc_apache'] < 20), 'white_blood_cells'] = 1\n    df.loc[(df['wbc_apache'] >= 20) &\n           (df['wbc_apache'] < 40), 'white_blood_cells'] = 2\n    df.loc[df['wbc_apache'] >= 40, 'white_blood_cells'] = 4\n\n    df['glasgow_comma_score_gcs'] = 0\n    df.loc[(\n                   df['gcs_eyes_apache'] + df['gcs_motor_apache'] +\n                   df['gcs_verbal_apache']) <=3,\n           'glasgow_comma_score_gcs'] = 12\n\n    df.loc[(df['gcs_eyes_apache'] +\n            df['gcs_motor_apache'] +\n            df['gcs_verbal_apache']) > 3,\n           'glasgow_comma_score_gcs'] = 15 - df.loc[\n        (df['gcs_eyes_apache'] + df['gcs_motor_apache'] +\n         df['gcs_verbal_apache']) > 3, 'gcs_eyes_apache'] + \\\n                                        df.loc[(df['gcs_eyes_apache'] + df['gcs_motor_apache'] +\n                                                df['gcs_verbal_apache']) > 3, 'gcs_motor_apache'] + \\\n                                        df.loc[(df['gcs_eyes_apache'] + df['gcs_motor_apache'] +\n                                                df['gcs_verbal_apache']) > 3, 'gcs_verbal_apache']\n\n    df['age_death_prob'] = 0\n    df.loc[df['wbc_apache'] < 45, 'age_death_prob'] = 0\n    df.loc[(df['wbc_apache'] >= 45) &\n           (df['wbc_apache'] < 55), 'age_death_prob'] = 2\n    df.loc[(df['wbc_apache'] >= 55) &\n           (df['wbc_apache'] < 65), 'age_death_prob'] = 3\n    df.loc[(df['wbc_apache'] >= 65) &\n           (df['wbc_apache'] < 75), 'age_death_prob'] = 5\n    df.loc[df['wbc_apache'] >= 75, 'age_death_prob'] = 6\n\n    df['inmunodeficiencia'] = 0\n    df.loc[(df['aids'] +\n            df['cirrhosis'] +\n            df['hepatic_failure'] +\n            df['immunosuppression'] +\n            df['leukemia'] +\n            df['lymphoma'] +\n            df['solid_tumor_with_metastasis']) >= 1, 'inmunodeficiencia'] = 5\n\n    df['post_operative'] = 0\n    df.loc[df['apache_post_operative'] == 1, \"post_operative\"] = 3\n\n    df['danger_level'] = df['temperature'] + \\\n                         df['arterial_pressure'] + df['heart_rate_pulse'] + \\\n                         df['respiration_rate'] + df['oxygenation_rate'] + \\\n                         df['serum_bicarb'] + df['arterial_ph'] + \\\n                         df['serum_sodium'] + df['serum_potassium'] + \\\n                         df['creatinine'] + df['acute_renal_failure'] + \\\n                         df['hematocrits'] + df['white_blood_cells'] + \\\n                         df['glasgow_comma_score_gcs'] + df['age_death_prob'] + \\\n                         df['inmunodeficiencia'] + df['post_operative']\n\n    df.drop(columns=['temperature', 'arterial_pressure',\n                     'heart_rate_pulse', 'respiration_rate',\n                     'oxygenation_rate', 'serum_bicarb',\n                     'arterial_ph', 'serum_sodium', 'serum_potassium',\n                     'creatinine', 'acute_renal_failure', 'hematocrits',\n                     'white_blood_cells', 'glasgow_comma_score_gcs',\n                     'age_death_prob', 'inmunodeficiencia', 'post_operative'\n                     ], inplace=True)\n\n    return df['danger_level']","2698f934":"if os.path.isfile('..\/input\/wids-2021-preprocessed-data\/train_gfe.pkl'):\n    with open('..\/input\/wids-2021-preprocessed-data\/train_gfe.pkl', 'rb') as handle:\n        train_gfe = pickle.load(handle)\n\n    with open('..\/input\/wids-2021-preprocessed-data\/test_gfe.pkl', 'rb') as handle:\n        test_gfe = pickle.load(handle)\n\nelse:\n\n    train = pd.read_csv ( \"..\/input\/widsdatathon2021\/TrainingWiDS2021.csv\").drop(columns=['Unnamed: 0'], axis=1)\n    test = pd.read_csv(\"..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv\").drop(columns=['Unnamed: 0'], axis=1)\n\n    train_danger_level = get_apache_meta_features(train.copy())\n    test_danger_level = get_apache_meta_features(test.copy())\n\n    train = train.rename(columns={'pao2_apache':'pao2fio2ratio_apache','ph_apache':'arterial_ph_apache'})\n    test = test.rename(columns={'pao2_apache':'pao2fio2ratio_apache','ph_apache':'arterial_ph_apache'})\n\n    additional_cols=[]\n    for v in ['albumin','bilirubin','bun','glucose','hematocrit','pao2fio2ratio','arterial_ph','resprate','sodium','temp','wbc','creatinine']:\n        additional_cols.append(f'{v}_indicator')\n        train[f'{v}_indicator'] = (((train[f'{v}_apache']==train[f'd1_{v}_max']) & (train[f'd1_{v}_max']==train[f'h1_{v}_max'])) |\n                     ((train[f'{v}_apache']==train[f'd1_{v}_max']) & (train[f'd1_{v}_max']==train[f'd1_{v}_min'])) |\n                     ((train[f'{v}_apache']==train[f'd1_{v}_max']) & (train[f'd1_{v}_max']==train[f'h1_{v}_min'])) |\n                     ((train[f'{v}_apache']==train[f'h1_{v}_max']) & (train[f'h1_{v}_max']==train[f'd1_{v}_max'])) |\n                     ((train[f'{v}_apache']==train[f'h1_{v}_max']) & (train[f'h1_{v}_max']==train[f'h1_{v}_min'])) |\n                     ((train[f'{v}_apache']==train[f'h1_{v}_max']) & (train[f'h1_{v}_max']==train[f'd1_{v}_min'])) |\n                     ((train[f'{v}_apache']==train[f'd1_{v}_min']) & (train[f'd1_{v}_min']==train[f'd1_{v}_max'])) |\n                     ((train[f'{v}_apache']==train[f'd1_{v}_min']) & (train[f'd1_{v}_min']==train[f'h1_{v}_min'])) |\n                     ((train[f'{v}_apache']==train[f'd1_{v}_min']) & (train[f'd1_{v}_min']==train[f'h1_{v}_max'])) |\n                     ((train[f'{v}_apache']==train[f'h1_{v}_min']) & (train[f'h1_{v}_min']==train[f'h1_{v}_max'])) |\n                     ((train[f'{v}_apache']==train[f'h1_{v}_min']) & (train[f'h1_{v}_min']==train[f'd1_{v}_min'])) |\n                     ((train[f'{v}_apache']==train[f'h1_{v}_min']) & (train[f'h1_{v}_min']==train[f'd1_{v}_max']))\n                    ).astype(np.int8)\n\n        test[f'{v}_indicator'] = (((test[f'{v}_apache']==test[f'd1_{v}_max']) & (test[f'd1_{v}_max']==test[f'h1_{v}_max'])) |\n                     ((test[f'{v}_apache']==test[f'd1_{v}_max']) & (test[f'd1_{v}_max']==test[f'd1_{v}_min'])) |\n                     ((test[f'{v}_apache']==test[f'd1_{v}_max']) & (test[f'd1_{v}_max']==test[f'h1_{v}_min'])) |\n                     ((test[f'{v}_apache']==test[f'h1_{v}_max']) & (test[f'h1_{v}_max']==test[f'd1_{v}_max'])) |\n                     ((test[f'{v}_apache']==test[f'h1_{v}_max']) & (test[f'h1_{v}_max']==test[f'h1_{v}_min'])) |\n                     ((test[f'{v}_apache']==test[f'h1_{v}_max']) & (test[f'h1_{v}_max']==test[f'd1_{v}_min'])) |\n                     ((test[f'{v}_apache']==test[f'd1_{v}_min']) & (test[f'd1_{v}_min']==test[f'd1_{v}_max'])) |\n                     ((test[f'{v}_apache']==test[f'd1_{v}_min']) & (test[f'd1_{v}_min']==test[f'h1_{v}_min'])) |\n                     ((test[f'{v}_apache']==test[f'd1_{v}_min']) & (test[f'd1_{v}_min']==test[f'h1_{v}_max'])) |\n                     ((test[f'{v}_apache']==test[f'h1_{v}_min']) & (test[f'h1_{v}_min']==test[f'h1_{v}_max'])) |\n                     ((test[f'{v}_apache']==test[f'h1_{v}_min']) & (test[f'h1_{v}_min']==test[f'd1_{v}_min'])) |\n                     ((test[f'{v}_apache']==test[f'h1_{v}_min']) & (test[f'h1_{v}_min']==test[f'd1_{v}_max']))\n                    ).astype(np.int8)\n\n\n    rank_features = ['age', 'bmi', 'd1_heartrate_min', 'weight']\n    df = pd.concat([train[rank_features], test[rank_features]])\n    for f in rank_features:\n        if f in list(train.columns):\n            additional_cols.append(f + '_rank')\n            train[f + '_rank'] = np.log1p(df[f].rank()).iloc[0:train.shape[0]].values\n            test[f + '_rank'] = np.log1p(df[f].rank()).iloc[train.shape[0]:].values\n\n    train['danger_level'] = train_danger_level\n    test['danger_level'] = test_danger_level\n\n    additional_cols.append('danger_level')\n    additional_cols.append('encounter_id')\n\n    with open('train_gfe.pkl', 'wb') as fout:\n        pickle.dump(train[additional_cols], fout)\n\n    with open('test_gfe.pkl', 'wb') as fout:\n        pickle.dump(test[additional_cols], fout)","6e0dac17":"if os.path.isfile('..\/input\/wids-2021-preprocessed-data\/train_lasso.pkl'):\n    with open('..\/input\/wids-2021-preprocessed-data\/train_lasso.pkl', 'rb') as handle:\n        train_lasso = pickle.load(handle)\n\n    with open('..\/input\/wids-2021-preprocessed-data\/test_lasso.pkl', 'rb') as handle:\n        test_lasso = pickle.load(handle)\n\nelse:\n    \n    additional_fe = ['albumin_indicator', 'bilirubin_indicator', 'bun_indicator',\n                 'glucose_indicator', 'hematocrit_indicator', 'pao2fio2ratio_indicator',\n                 'arterial_ph_indicator', 'resprate_indicator', 'sodium_indicator',\n                 'temp_indicator', 'wbc_indicator', 'creatinine_indicator', 'age_rank',\n                 'bmi_rank', 'd1_heartrate_min_rank', 'weight_rank', 'danger_level']\n\n    for col in additional_fe:\n        train_df[col] = train_gfe[col].values\n        test_df[col] = test_gfe[col].values\n        \n    feature_imp = pd.read_csv('..\/input\/feature-importance\/feature_importance_v1.csv')\n    feature_cols = list(feature_imp.loc[feature_imp['Value']>250, 'Feature'])\n\n    cat_cols = [f for f in cat_cols if f in feature_cols]\n    cont_cols = feature_cols\n    cont_cols = [f for f in cont_cols if f not in cat_cols]\n\n    print(\"Number of input variables:\", len(cont_cols+cat_cols))\n\n    for col in cont_cols:\n        df = pd.concat([train_df[col], test_df[col]])\n        train_df[col] = train_df[col].fillna(df.median())\n        test_df[col] = test_df[col].fillna(df.median())\n\n    cont_cols = [f for f in cont_cols if f not in cat_cols]\n\n    for col in cat_cols:\n        train_df[col] = train_df[col].astype('int')\n        test_df[col] = test_df[col].astype('int')\n\n    cat_cols = [f for f in cat_cols if f not in cont_cols]\n    print(\"Number of input variables:\", len(cont_cols+cat_cols))\n\n    y = train_df['diabetes_mellitus'].values\n\n    number_features = np.random.randint(10, 30, 100)\n    list_features = []\n    generate_feats = []\n\n    for i, num in enumerate(number_features):\n        list_feat = np.random.randint(0, len(cont_cols), num)\n        cols = []\n        for c in list_feat:\n            cols.append(cont_cols[c])\n\n        list_features.append(cols)\n\n        X = train_df[cols].values\n        clf = LassoCV(fit_intercept=True).fit(X, y)\n        y_pred = clf.predict(X)\n        y_pred_test = clf.predict(test_df[cols].values)\n\n        generate_feats.append(f\"col_{i}\")\n        train_df[f\"col_{i}\"] = y_pred\n        test_df[f\"col_{i}\"] = y_pred_test\n\n    with open('data\/train_lasso.pkl', 'wb') as fout:\n        pickle.dump(train_df[generate_feats], fout)\n\n    with open('data\/test_lasso.pkl', 'wb') as fout:\n        pickle.dump(test_df[generate_feats], fout)\n\n    with open('data\/list_features.pkl', 'wb') as fout:\n        pickle.dump(list_features, fout)\n    ","0597697c":"from sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import roc_auc_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nimport gc","5f5895e8":"def model_train(X_train, y_train, X_test, y_test, num_iter, params_lgb):\n\n    dtrain = lgb.Dataset(X_train, y_train)\n    dvalid = lgb.Dataset(X_test, y_test, reference=dtrain)\n    model_lgb = lgb.train(params_lgb, dtrain, num_iter,\n                          valid_sets=(dtrain, dvalid),\n                          valid_names=('train', 'valid'),\n                          early_stopping_rounds=150,\n                          verbose_eval=20)\n\n    return model_lgb\n\ndef CV(X, y, params_lgb, eval_method, shuffle=True, NFolds=5, num_iter=50):\n\n    models = []\n    y_folds = np.zeros((X.shape[0], 1))\n    y_preds = np.zeros((X.shape[0], 1))\n\n    I=0\n\n    if eval_method==\"kf\":\n        skf = KFold(n_splits=NFolds, random_state=42, shuffle=shuffle)\n        for train_index, test_index in tqdm(skf.split(X, y)):\n            print('[Fold %d\/%d]' % (I + 1, NFolds))\n            print('=' * 60)\n            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n            model_lgb = model_train(X_train, y_train, X_test, y_test, num_iter, params_lgb)\n\n            preds = model_lgb.predict(X_test)\n\n            y_folds[test_index, 0] = y_test\n            y_preds[test_index, 0] = preds\n            models.append(model_lgb)\n\n            I += 1\n\n    return models, y_preds\n\ndef Kaggle_submission(file_name, models, test_data, ids_list, feature_imp, num):\n\n    submit = pd.DataFrame()\n    submit['encounter_id'] = ids_list\n    submit['diabetes_mellitus'] = 0\n\n    cols = feature_imp.loc[feature_imp['Value']>num, 'Feature']\n    nfolds = len(models)\n    for model in models:\n        test_pred = model.predict(test_data[cols], num_iteration=model.best_iteration)\n        submit['diabetes_mellitus'] += (test_pred \/ nfolds)\n\n    submit.to_csv(file_name, index=False)\n\n    return submit\n","806ac598":"## merge preprocessed datasets\ncats = ['elective_surgery', 'icu_id', 'arf_apache', 'intubated_apache', 'ventilated_apache', 'cirrhosis',\n        'hepatic_failure', 'immunosuppression', 'leukemia', 'solid_tumor_with_metastasis', 'apache_3j_diagnosis_x',\n        'apache_2_diagnosis_x', 'apache_3j', 'apache_3j_diagnosis_split1', 'apache_2_diagnosis_split1', 'gcs_sum_type',\n        'hospital_admit_source', 'glucose_rate', 'glucose_wb', 'gcs_eyes_apache', 'glucose_normal', 'total_cancer_immuno',\n        'gender', 'total_chronic', 'icu_stay_type', 'apache_2_diagnosis_type', 'apache_3j_diagnosis_type']\n\nadditional_fe = ['albumin_indicator', 'bilirubin_indicator', 'bun_indicator',\n                 'glucose_indicator', 'hematocrit_indicator', 'pao2fio2ratio_indicator',\n                 'arterial_ph_indicator', 'resprate_indicator', 'sodium_indicator',\n                 'temp_indicator', 'wbc_indicator', 'creatinine_indicator', 'age_rank',\n                 'bmi_rank', 'd1_heartrate_min_rank', 'weight_rank', 'danger_level']\n\nfor col in additional_fe:\n    train_df[col] = train_gfe[col].values\n    test_df[col] = test_gfe[col].values\n\ndel train_gfe, test_gfe\ngc.collect()\n\nprint(train_lasso.columns)\n\nfor col in train_lasso.columns:\n    train_df[col] = train_lasso[col].values\n    test_df[col] = test_lasso[col].values\n\ndel train_lasso, test_lasso\ngc.collect()","204cdc5a":"##feature importance\n#based on feature importance, decrease memory usage\nfeature_imp = pd.read_csv('..\/input\/feature-importance\/feature_importance.csv')\ncols = feature_imp.loc[feature_imp['Value']>50, 'Feature']\n\ntrain_df = train_df[list(cols)+['diabetes_mellitus', 'encounter_id']]\ntest_df = test_df[list(cols)+['encounter_id']]\ngc.collect()\n\nfeature_cols = list(train_df)\nfeature_cols.remove('diabetes_mellitus')\nfeature_cols.remove('encounter_id')\n\ncats = [f for f in cats if f in feature_cols]\ncont_cols = feature_cols\ncont_cols = [f for f in cont_cols if f not in cats]\n\nprint(\"Number of input variables:\", len(cont_cols+cats))\n\nX=train_df[feature_cols]\ny=train_df['diabetes_mellitus']\n\nparams_lgb ={\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'metric': 'auc',\n        'learning_rate': 0.015,\n        'subsample': 1,\n        'colsample_bytree': 0.2,\n        'reg_alpha': 3,\n        'reg_lambda': 1,\n        'verbose': 1,\n        'max_depth': -1,\n        'seed':100,\n        }\n\n\ncat_cols = ['icu_id']\nif os.path.isfile('..\/input\/feature-importance\/feature_importance.csv'):\n    feature_imp = pd.read_csv('..\/input\/feature-importance\/feature_importance.csv')\n    plt.figure(figsize=(16, 10))\n    sns.set(font_scale=1.1)\n    sns.barplot(\n        color=\"#3498db\",\n        x=\"Value\",\n        y=\"Feature\",\n        data=feature_imp.sort_values(by=\"Value\", ascending=False)[0:50],\n    )\n    plt.title('Feature Importance')\n    plt.tight_layout()\n    plt.show()\nelse:\n    dtrain = lgb.Dataset(X, y)\n    model_lgb = lgb.train(params_lgb, dtrain, 500, valid_sets=(dtrain),valid_names=('train'),\n                          verbose_eval=20, early_stopping_rounds=100)\n\n    feature_imp = pd.DataFrame(\n            {\n                \"Value\": model_lgb.feature_importance(importance_type=\"gain\"),\n                \"Feature\": X.columns,\n            })\n\n    feature_imp=feature_imp.sort_values(by='Value', ascending=False)\n    feature_imp.to_csv('feature_importance.csv', index=False)\n    print(feature_imp.head())\n\n    plt.figure(figsize=(16, 10))\n    sns.set(font_scale=1.1)\n    sns.barplot(\n        color=\"#3498db\",\n        x=\"Value\",\n        y=\"Feature\",\n        data=feature_imp.sort_values(by=\"Value\", ascending=False)[0:50],\n    )\n    plt.title('Feature Importance')\n    plt.tight_layout()\n    plt.show()","0d22ca95":"### training model\nmodels_list = []\npreds = []\ny_folds = []\nfeature_importance = [160]\neval_methods = ['kf']\nparams = [params_lgb]\n\nif os.path.isfile('models_lgb.pkl'):\n    with open('models_lgb.pkl', 'rb') as handle:\n        models_list = pickle.load(handle)\n\n    i = 0\n    for param in range(len(params)):\n        for num in feature_importance:\n            for method in eval_methods:\n                submit_lgb = Kaggle_submission('submission_lgb_gfe_lasso_%s_%s_%s.csv' % (method, num, param), models_list[i], test_df,\n                                               test_df['encounter_id'].tolist(), feature_imp, num)\n                i += 1\n\nelse:\n    for p, param in enumerate(params):\n        for eval in eval_methods:\n            for num_feat in feature_importance:\n                cols = feature_imp.loc[feature_imp['Value']>num_feat, 'Feature']\n                cat_cols = [f for f in cat_cols if f in cols]\n                models, y_preds = CV(X[cols], y, param, eval_method=eval, NFolds=10, num_iter=20000)\n\n                print('AUC of %s eval method with %s features type params %s' % (eval, num_feat, p), roc_auc_score(train_df['diabetes_mellitus'].values, y_preds))\n                submit_lgb = Kaggle_submission('submission_lgb_gfe_lasso_%s_%s_%s.csv' % (eval, num_feat, p), models, test_df,\n                                               test_df['encounter_id'].tolist(), feature_imp, num_feat)\n\n                train_df['pred'] = y_preds\n                train_df[['diabetes_mellitus', 'pred']].to_csv('oof_lgb_gfe_lasso_%s_%s_%s.csv' % (eval, num_feat, p), index=False)\n\n\n                models_list.append(models)\n        \ndel models_list, y_preds, submit_lgb, X, y\ngc.collect()","bfae8cde":"from catboost import Pool\nfrom catboost import CatBoostClassifier","d42e153a":"def model_train(X_train, y_train, X_test, y_test,  params_cat):\n    dtrain = Pool(X_train, y_train, cat_features=cat_cols)\n    dvalid = Pool(X_test, y_test, cat_features=cat_cols)\n    model_cat = CatBoostClassifier(**params_cat)\n    model_cat.fit(dtrain, eval_set=dvalid, use_best_model=True, verbose=20)\n    return model_cat\n\ndef CV(X, y, params_cat, eval_method, shuffle=True, NFolds=3):\n\n    models = []\n    y_folds = np.zeros((X.shape[0], 1))\n    y_preds = np.zeros((X.shape[0], 1))\n\n    I=0\n\n    if eval_method==\"kf\":\n        skf = KFold(n_splits=NFolds, random_state=42, shuffle=shuffle)\n        for train_index, test_index in tqdm(skf.split(X, y)):\n            print('[Fold %d\/%d]' % (I + 1, NFolds))\n            print('=' * 60)\n            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n            model_cat = model_train(X_train, y_train, X_test, y_test, params_cat)\n\n            preds = model_cat.predict(X_test, prediction_type='Probability')[:,1]\n\n            y_folds[test_index, 0] = y_test\n            y_preds[test_index, 0] = preds\n            models.append(model_cat)\n\n            I += 1\n\n    return models, y_preds\n\n\ndef Kaggle_submission(file_name, models, test_data,\n                      ids_list, feature_imp, num):\n\n    submit = pd.DataFrame()\n    submit['encounter_id'] = ids_list\n    submit['diabetes_mellitus'] = 0\n\n    cols = feature_imp.loc[feature_imp['Value']>num, 'Feature']\n    nfolds = len(models)\n    for model in models:\n        test_pred = model.predict(test_data[cols], prediction_type='Probability')[:,1]\n        submit['diabetes_mellitus'] += (test_pred \/ nfolds)\n\n    submit.to_csv(file_name, index=False)\n\n    return submit\n","09acd540":"for col in cats:\n    train_df[col] = train_df[col].astype('int')\n    test_df[col] = test_df[col].astype('int')\n\nfor col in cont_cols:\n    df = pd.concat([train_df[col], test_df[col]])\n    train_df[col] = train_df[col].fillna(df.median())\n    test_df[col] = test_df[col].fillna(df.median())\n\ndel df\nX=train_df[feature_cols]\ny=train_df['diabetes_mellitus']","ce2a4534":"params_cat =  {\n             'n_estimators' : 10000,\n            'learning_rate': 0.02,\n             'eval_metric' : 'AUC',\n             'early_stopping_rounds': 100,\n            'loss_function': 'Logloss',\n            'depth': 8,\n            'bootstrap_type': 'Bernoulli',\n            'class_weights': [0.25, 0.75],\n             'task_type':'GPU'\n            }","ccdd5ef5":"cat_cols = cats\nmodels_list = []\npreds = []\ny_folds = []\nnum_feats = [150]\nmethods = ['kf']\n\nif os.path.isfile('models_cat.pkl'):\n    with open('models_cat.pkl', 'rb') as handle:\n        models_list = pickle.load(handle)\n\n    i = 0\n    for num in num_feats:\n        for method in methods:\n            submit_cat = Kaggle_submission('submission_cat_gfe_lasso_%s_%s.csv' % (method, num), models_list[i], test_df,\n                                           test_df['encounter_id'].tolist(), feature_imp, num)\n            i += 1\n\nelse:\n    i = 0\n    for num_feat in num_feats:\n        cols = feature_imp.loc[feature_imp['Value']>num_feat, 'Feature']\n        cat_cols = [f for f in cat_cols if f in cols]\n        models, y_preds = CV(X[cols], y, params_cat, eval_method='kf', shuffle=True, NFolds=10)\n\n        print('AUC with %s features' % (num_feat), roc_auc_score(train_df['diabetes_mellitus'].values, y_preds))\n\n        train_df['pred'] = y_preds\n        train_df[['diabetes_mellitus', 'pred']].to_csv('oof_cat_gfe_lasso_%s.csv' % (num_feat), index=False)\n\n        models_list.append(models)\n\n        for method in methods:\n            submit_cat = Kaggle_submission('submission_cat_gfe_lasso_%s_%s.csv' % (method, num_feat), models_list[i], test_df,\n                                           test_df['encounter_id'].tolist(), feature_imp, num_feat)\n            i += 1\n\ndel models_list, submit_cat, X, y, y_preds\ngc.collect()","ea681cc7":"from sklearn.preprocessing import MinMaxScaler\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.nn.modules.loss import _WeightedLoss\nfrom torch.autograd import Variable\nimport random\n\ndevice = ('cuda' if torch.cuda.is_available() else 'cpu')","11b58981":"def normalization(df):\n    if len(df.shape)==2:\n        scaler = MinMaxScaler().fit(df.values)\n        X = scaler.transform(df.values)\n    else:\n        scaler = MinMaxScaler().fit(np.expand_dims(df.values, axis=1))\n        X = scaler.transform(np.expand_dims(df.values, axis=1))\n\n    return X, scaler\n\ndef make_2Dinput(dt, cont_cols, cat_cols):\n    input = {\"cont\": dt[cont_cols].to_numpy()}\n    for i, v in enumerate(cat_cols):\n        input[v] = dt[[v]].to_numpy()\n    return input\n\nclass Loader:\n    def __init__(self, X, y, shuffle=True, batch_size=1000, cat_cols=[]):\n        self.X_cont = X[\"cont\"]\n        try:\n            self.X_cat = np.concatenate([X[k] for k in cat_cols], axis=1)\n        except:\n            self.X_cat = np.concatenate([np.expand_dims(X[k], axis=1) for k in cat_cols], axis=1)\n        self.y = y\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.n_conts = self.X_cont.shape[1]\n        self.len = self.X_cont.shape[0]\n        n_batches, remainder = divmod(self.len, self.batch_size)\n\n        if remainder > 0:\n            n_batches += 1\n        self.n_batches = n_batches\n        self.remainder = remainder  # for debugging\n        self.idxes = np.array([i for i in range(self.len)])\n\n    def __iter__(self):\n        self.i = 0\n        if self.shuffle:\n            ridxes = self.idxes\n            np.random.shuffle(ridxes)\n            self.X_cat = self.X_cat[[ridxes]]\n            self.X_cont = self.X_cont[[ridxes]]\n            if self.y is not None:\n                self.y = self.y[[ridxes]]\n\n        return self\n\n    def __next__(self):\n        if self.i >= self.len:\n            raise StopIteration\n\n        if self.y is not None:\n            y = torch.FloatTensor(self.y[self.i:self.i + self.batch_size].astype(np.float32))\n\n        else:\n            y = None\n\n        xcont = torch.FloatTensor(self.X_cont[self.i:self.i + self.batch_size])\n        xcat = torch.LongTensor(self.X_cat[self.i:self.i + self.batch_size])\n\n        batch = (xcont, xcat, y)\n        self.i += self.batch_size\n        return batch\n\n    def __len__(self):\n        return self.n_batches\n\ndef ensemble_structure(number_models, num_features, MODEL_ROOT):\n\n    if number_models == 1:\n\n        hidden_dims = [256]\n        number_of_dims = [num_features]\n        input_dims = [np.arange(num_features)]\n\n    elif os.path.isfile('hidden_dims.pkl'):\n        with open('hidden_dims.pkl', 'rb') as handle:\n            hidden_dims = pickle.load(handle)\n\n        with open('number_of_dims.pkl', 'rb') as handle:\n            number_of_dims = pickle.load(handle)\n\n        with open('input_dims.pkl', 'rb') as handle:\n            input_dims = pickle.load(handle)\n\n    else:\n\n        hidden_dims = np.random.randint(128, 256, number_models)\n        number_of_dims = np.random.randint(int(num_features*0.75), num_features+1, number_models)\n        input_dims = []\n        for i in range(number_models):\n            input_dims.append(np.random.randint(0, num_features, number_of_dims[i]))\n\n        with open('hidden_dims.pkl', 'wb') as handle:\n            pickle.dump(hidden_dims, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n        with open('number_of_dims.pkl', 'wb') as handle:\n            pickle.dump(number_of_dims, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n        with open('input_dims.pkl', 'wb') as handle:\n            pickle.dump(input_dims, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    return hidden_dims, number_of_dims, input_dims\n\nclass WeightedFocalLoss(nn.Module):\n    \"Weighted version of Focal Loss\"\n    def __init__(self, alpha=.25, gamma=2):\n        super(WeightedFocalLoss, self).__init__()\n        self.alpha = torch.tensor([alpha, 1-alpha]).cuda()\n        self.gamma = gamma\n\n    def forward(self, inputs, targets):\n        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n        targets = targets.type(torch.long)\n        at = self.alpha.gather(0, targets.data.view(-1))\n        pt = torch.exp(-BCE_loss)\n        F_loss = at*(1-pt)**self.gamma * BCE_loss\n        return F_loss.mean()","c47d4158":"############## MLP ####################################\nclass simple_linear_layer(nn.Module):\n    def __init__(self, input_dim, out_dim):\n        super(simple_linear_layer, self).__init__()\n        self.dense = nn.Linear(input_dim, out_dim)\n        self.relu = nn.ReLU(inplace=True)\n        self.batch_norm = nn.BatchNorm1d(out_dim)\n        self.dropout =nn.Dropout(0.20)\n\n\n    def forward(self, x):\n\n        x = self.dense(x)\n        x = self.relu(x)\n        x = self.batch_norm(x)\n        x = self.dropout(x)\n\n        return x\n\nclass MLP_Model(nn.Module):\n\n    def __init__(self, input_dim, hidden_dim, out_dim=1):\n        super(MLP_Model, self).__init__()\n\n        self.block1 = simple_linear_layer(input_dim, hidden_dim)\n        self.block2 = simple_linear_layer(hidden_dim, hidden_dim)\n        self.block3 = simple_linear_layer(hidden_dim, hidden_dim)\n        self.block4 = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n\n        h = self.block1(x)\n        h = self.block2(h)\n        h = self.block3(h)\n        out = self.block4(h)\n\n        return out\n    \n####################### Final Ensemble Model ######################################\nclass Ensemble_Model(nn.Module):\n\n    def __init__(self, input_dims, emb_dims, number_of_dims, hidden_dims,\n                 model_name, out_dim=1):\n        super(Ensemble_Model, self).__init__()\n\n        # Embedding layers\n        self.emb_layers = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims])\n        n_embs = sum([y for x, y in emb_dims])\n\n        self.models = torch.nn.ModuleList()\n        self.input_dims = input_dims\n        self.model_name = model_name\n\n        for i in range(len(hidden_dims)):\n            if self.model_name=='Simple_MLP':\n                self.models.append(MLP_Model(input_dim=number_of_dims[i]+n_embs,\n                                                    hidden_dim=hidden_dims[i], out_dim=out_dim))\n\n                print(\"Please check model name. There is no this model!!!\")\n\n        # train embedding layers and concat cat and cont variables\n\n    def encode_and_combine_data(self, cat_data):\n        xcat = [el(cat_data[:, k]) for k, el in enumerate(self.emb_layers)]\n        xcat = torch.cat(xcat, 1)\n\n        return xcat\n\n    def forward(self, cont_data, cat_data):\n\n        xcat = self.encode_and_combine_data(cat_data)\n\n        out = []\n        for i in range(len(self.input_dims)):\n            temp = self.models[i](torch.cat([cont_data[:, self.input_dims[i]], xcat], dim=1))\n            out.append(temp.unsqueeze(0))\n\n        out = torch.cat(out, dim=0)\n        out = out.permute(1, 0, 2)\n\n        return out","a4ad269f":"def training_nn(df, test, target, cont_cols, cat_cols, MLP_model,\n                Nfolds, epoch=50, patience=5, MODEL_ROOT='models',\n                hidden_dim=1024):\n\n    uniques = {}\n    dims = {}\n    dt = pd.concat([df[cat_cols], test[cat_cols]], axis=0)\n    for i, v in enumerate(cat_cols):\n        uniques[v] = len(dt[v].unique())\n        dims[v] = min(int(len(dt[v].unique())\/2), 16)\n\n    if not os.path.exists(MODEL_ROOT):\n        os.makedirs(MODEL_ROOT)\n\n    kfolds = KFold(n_splits=Nfolds, random_state=42, shuffle=True)\n\n    final_test = np.zeros((test.shape[0],))\n    oof_train = np.zeros((df.shape[0], 1))\n\n    for i, (trn_ind, val_ind) in tqdm(enumerate(kfolds.split(X=df,\n                                                             y=df[target]))):\n\n        print('[Fold %d\/%d]' % (i + 1, Nfolds))\n        print('=' * 60)\n        # Split Train\/Valid\n        train = df.loc[trn_ind]\n        valid = df.loc[val_ind]\n\n        model_path = MODEL_ROOT + '\/model_nn_%s_%s.pt' % (hidden_dim, i)\n\n        # Make input for pytorch loader because we have categorical and continues features\n        X_train, y_train = make_2Dinput(train[cont_cols + cat_cols], cont_cols=cont_cols, cat_cols=cat_cols), train[target]\n        validx, validy = make_2Dinput(valid[cont_cols + cat_cols], cont_cols=cont_cols, cat_cols=cat_cols), valid[target]\n\n        # Make loader for pytorch\n        train_loader = Loader(X_train, y_train.values, cat_cols=cat_cols, batch_size=1024, shuffle=True)\n        val_loader = Loader(validx, validy.values, cat_cols=cat_cols, batch_size=2000, shuffle=False)\n\n        if os.path.isfile(model_path):\n            model_final = torch.load(model_path)\n            y_pred = []\n            with torch.no_grad():\n                model_final.eval()\n                for i, (X_cont, X_cat, y) in enumerate(val_loader):\n                    out = model_final(X_cont.to(device), X_cat.to(device))\n                    y_pred += list(np.mean(out.sigmoid().detach().cpu().numpy(), axis=1).flatten())\n\n            oof_train[val_ind, 0] = y_pred\n\n        else:\n            ## make embedding dimensions\n            emb_dims = [(uniques[col], dims[col]) for col in cat_cols]\n\n            # number of continues variables\n            n_cont = train_loader.n_conts\n\n            # # neural network model\n            hidden_dims, number_of_dims, input_dims = ensemble_structure(1,\n                                                                         n_cont,\n                                                                         MODEL_ROOT)\n\n            model = Ensemble_Model(input_dims=input_dims, emb_dims=emb_dims,\n                                   number_of_dims=number_of_dims,\n                                   hidden_dims=hidden_dims,\n                                   model_name='Simple_MLP', out_dim=1).to(device)\n\n            # loss function\n            criterion = WeightedFocalLoss()#nn.BCEWithLogitsLoss()\n\n            # adam optimizer has been used for training\n            optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n            # learning rate scheduler\n            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='max', factor=0.1, patience=3,\n                                                            min_lr=0.00001, verbose=True)\n\n\n\n            best_auc = 0\n            counter = 0\n            for ep in range(epoch):\n                train_loss, val_loss = 0, 0\n                # training phase for single epoch\n                model.train()\n                for i, (X_cont, X_cat, y) in enumerate(train_loader):\n                    optimizer.zero_grad()\n                    out = model(X_cont.to(device), X_cat.to(device))\n\n                    loss = 0 # criterion(out, y.unsqueeze(1).to(device)) #+ F.mse_loss(X_cont.to(device), x_hat)\n                    for i in range(out.shape[1]):\n                        loss = loss + criterion(out[:, i, :],\n                                              y.unsqueeze(1).to(device))\n\n                    loss.backward()\n                    optimizer.step()\n\n                    with torch.no_grad():\n                        train_loss += loss.item() \/ len(train_loader)\n\n                # Validation phase for single epoch\n                phase = 'Valid'\n                with torch.no_grad():\n                    model.eval()\n                    y_true = []\n                    y_pred = []\n                    rloss = 0\n                    for i, (X_cont, X_cat, y) in enumerate(val_loader):\n                        out = model(X_cont.to(device), X_cat.to(device))\n\n                        loss = 0 #criterion(out, y.unsqueeze(1).to(device)) #+ F.mse_loss(X_cont.to(device), x_hat)\n                        for i in range(out.shape[1]):\n                            loss = loss + criterion(out[:, i, :],\n                                                    y.unsqueeze(1).to(device))\n\n                        rloss += loss.item() \/ len(val_loader)\n                        y_pred += list(np.mean(out.sigmoid().detach().cpu().numpy(), axis=1).flatten())\n                        y_true += list(y.cpu().numpy())\n\n                    AUC= roc_auc_score(y_true, y_pred)\n                    scheduler.step(AUC)\n                    print(\n                        f\"[{phase}] Epoch: {ep} | Tain loss: {train_loss:.4f} | Val Loss: {rloss:.4f} | AUC: {AUC:.4f} \")\n\n                    if best_auc < AUC:\n                        best_auc = AUC\n                        best_model = model\n                        torch.save(best_model, model_path)\n                        counter = 0\n                        oof_train[val_ind, 0] = y_pred\n                    else:\n                        counter = counter + 1\n\n                # early stopping\n                if counter >= patience:\n                    print(\"Early stopping\")\n                    break\n\n        # call the best model for each fold\n        model_final = torch.load(model_path)\n\n        # make prediction for test set\n        testx = make_2Dinput(test[cont_cols + cat_cols], cont_cols=cont_cols, cat_cols=cat_cols)\n        test_loader = Loader(testx, None, cat_cols=cat_cols, batch_size=len(testx), shuffle=False)\n        y_pred = []\n\n        with torch.no_grad():\n            model_final.eval()\n            for i, (X_cont, X_cat, y) in enumerate(test_loader):\n                out = model_final(X_cont.to(device), X_cat.to(device))\n                y_pred += list(np.mean(out.sigmoid().detach().cpu().numpy(), axis=1).flatten())\n\n        final_test += np.asarray(y_pred) \/ Nfolds\n\n    return final_test, oof_train","d60e434a":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nseed_everything(seed=42)\n\nfeature_imp = pd.read_csv('..\/input\/feature-importance\/feature_importance.csv')\nfeature_cols = list(feature_imp.loc[feature_imp['Value']>150, 'Feature'])\n\n### data import\n\ncat_cols = ['elective_surgery', 'icu_id', 'arf_apache', 'intubated_apache', 'ventilated_apache', 'cirrhosis',\n        'hepatic_failure', 'immunosuppression', 'leukemia', 'solid_tumor_with_metastasis', 'apache_3j_diagnosis_x',\n        'apache_2_diagnosis_x', 'apache_3j', 'apache_3j_diagnosis_split1', 'apache_2_diagnosis_split1', 'gcs_sum_type',\n        'hospital_admit_source', 'glucose_rate', 'glucose_wb', 'gcs_eyes_apache', 'glucose_normal', 'total_cancer_immuno',\n        'gender', 'total_chronic', 'icu_stay_type', 'apache_2_diagnosis_type', 'apache_3j_diagnosis_type']\n\ncat_cols = [f for f in cat_cols if f in feature_cols]\ncont_cols = feature_cols\ncont_cols = [f for f in cont_cols if f not in cat_cols]\n\nprint(\"Number of input variables:\", len(cont_cols+cat_cols))\n\ntrain_df[cont_cols], scalerX = normalization(train_df[cont_cols])\ntest_df[cont_cols] = scalerX.transform(test_df[cont_cols].values)\n\n#### Model Training\nnn_pred, nn_oof = training_nn(df=train_df, test=test_df,  target='diabetes_mellitus', MLP_model=Ensemble_Model,\n                                cont_cols=cont_cols, cat_cols=cat_cols, Nfolds=10,\n                                epoch=500, patience=10, MODEL_ROOT='models',\n                                hidden_dim=512)\n\nprint(roc_auc_score(train_df['diabetes_mellitus'].values, nn_oof))\ntest_df['diabetes_mellitus']=nn_pred\ntest_df[['encounter_id', 'diabetes_mellitus']].to_csv('submission_nn_gfe_220.csv', index=False)\n\ntrain_df['pred'] = nn_oof\ntrain_df[['diabetes_mellitus', 'pred']].to_csv('oof_nn_gfe_220.csv', index=False)","bfb0fcfa":"sub_lgb = pd.read_csv('.\/submission_lgb_gfe_lasso_kf_160_0.csv')\nsub_cat = pd.read_csv('.\/submission_cat_gfe_lasso_kf_150.csv')\nsub_nn = pd.read_csv('.\/submission_nn_gfe_220.csv')\n\nsub = sub_lgb.copy()\nsub['diabetes_mellitus']=0\nsub['diabetes_mellitus'] = 0.4*sub_lgb['diabetes_mellitus'].values + 0.3*sub_cat['diabetes_mellitus'].values + 0.3*sub_nn['diabetes_mellitus'].values\n\nsub.to_csv('submission_ensemble.csv', index=False)","acfb8850":"# Lasso Based Additional Feature Engineering","b67fe324":"# Ensemble","7a103198":"# The best catboost training ","b8667265":"# The best Neural Network Model","2380232b":"The creation of additional variables played an important role in improving the competition score. This notebook https:\/\/www.kaggle.com\/siavrez\/2020fatures was used.Thank you for helping us improve our score. ","382ba582":"Data preprocess running phase. On kaggle notebook, out of memory error will be occurred. We shared our preprocessed dataset here https:\/\/www.kaggle.com\/lhagiimn\/wids-2021-preprocessed-data and feature importance https:\/\/www.kaggle.com\/lhagiimn\/feature-importance\n\n\"feature_importance_v2.csv\" file doesn't include the features of \"Additional Feature Engineering\" and \"Lasso Based Additional Feature Engineering\". \n\"feature_importance_v1.csv\" file doesn't include the features of \"Lasso Based Additional Feature Engineering\". \n\"feature_importance.csv\" file includes all features. ","b83e48fe":"# Data Engineering Phase","0a71cb80":"### Missing values are replaced by the row values in other columns that are highly correlated.","b27803c9":"****Neural Network Model","7c90470e":"We also perform additional features. Our team member @Aristotelis Charalampous performed it. ","c3c8c117":"# The best LightGBM model","7f37320e":"We calculate diabetes risk as a variable. It improved our score. Therefore, we randomly produced additional score features based on Lasso regression and important features. ","d0269ccb":"# Additional Feature Engineering"}}