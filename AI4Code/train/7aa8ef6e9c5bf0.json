{"cell_type":{"54f1af2a":"code","6f11cb1b":"code","b4b2152c":"code","60359e03":"code","db7bfb87":"code","e4b0b01a":"code","9d7092f6":"code","dd198cbc":"code","3592eae5":"code","e8d05e8f":"code","c7f56f2a":"code","7ac04636":"code","bd9396eb":"code","23e57ad1":"code","7e210b5b":"code","27f1ff17":"code","65b9ed84":"code","7ce12f38":"code","bec0f926":"code","d61ad6d1":"code","fded37fb":"code","9a3f21a5":"code","f58ebed0":"code","20d17545":"code","e4707c18":"code","d3aa3384":"code","f48ea510":"code","16030764":"code","e543d518":"code","6d49553a":"code","5563bf96":"code","d0b20ab5":"code","42887c9d":"code","7d55d8e7":"code","4748d8f9":"code","74cedf69":"code","7b7e95fb":"code","75490dab":"code","87160a49":"code","2f6cafb4":"code","20e3d62f":"code","6bd8772c":"code","0de05b77":"code","9210e0ec":"code","6fa54c2c":"code","eb6d6d99":"code","de4efffe":"code","4920b6c8":"code","7c924359":"code","71f2b7c4":"code","a677bfc8":"code","6fe46810":"code","dc73ff0c":"code","b9ef5d61":"code","8145aa67":"code","0c1c97c4":"code","c27e924e":"code","c4ccef02":"code","92f5bc00":"code","f36d51c3":"code","8ae6c57b":"code","1a49b0d9":"code","476fa422":"code","199be4e3":"code","e835cec7":"code","3fc1fb1d":"code","deaa358e":"code","cd079de1":"code","b6c19a5e":"code","0d5f9042":"code","64e12688":"code","155e2846":"code","effef9b4":"code","cb0c9013":"code","3c5e6e29":"code","8ff6a520":"code","529e168a":"code","6d8d1e6f":"code","e03760d4":"code","c3a25c13":"code","8be3ed5a":"code","cbee5d8c":"code","c21297e7":"code","ae90b607":"code","b8ec87a6":"code","3b53392d":"code","0d095b56":"code","9f12a731":"code","fce659b3":"code","6762ba00":"code","2d4b11ba":"code","63e297bd":"code","64a98654":"code","1b8e4cda":"code","c98d3897":"code","c06edeb4":"code","2987c290":"code","5b0d384e":"code","9a7454a2":"code","170a43a0":"code","677c3f45":"code","6aeb4c20":"code","4e387584":"code","8f58b2e2":"code","9012196c":"code","31c2ead0":"code","fee384b0":"code","e5babf79":"code","bf051c1a":"code","b1b2f468":"code","8eb9298a":"code","19d7e4f9":"code","8f36c18c":"code","d00b38e6":"code","60ffe561":"code","74c47905":"code","6ec88730":"code","f2e2ce1c":"code","b5353b5f":"code","b0abc869":"code","fe387927":"code","4b872670":"code","914601da":"code","dcc9cb8c":"code","6540d472":"code","afba6bc1":"code","5573f225":"code","d23b2a54":"code","45e2f423":"code","f9b5b995":"code","772cdf0a":"code","c3d7c4b0":"code","df800fed":"code","e30e337a":"code","779479a1":"code","b4d009e4":"code","06fc2fb2":"code","2d3def5b":"code","191b7c1f":"code","77e2f1bf":"code","a7863ea5":"code","32b08085":"code","0ac201b1":"code","48f40b07":"code","56310f56":"code","be312573":"code","8068d06e":"code","bde9ff1a":"code","852ba1e6":"code","19a52419":"markdown","d2cdf45e":"markdown","c791bbb7":"markdown","03fecd57":"markdown","4d381c77":"markdown","214a645c":"markdown","317a3d6f":"markdown","6d5405c5":"markdown","b5fb320a":"markdown","c1589e0a":"markdown","2df0d140":"markdown","04b491f0":"markdown","e25a96af":"markdown","85fb0db7":"markdown","c1ef1a8e":"markdown","4b8c08a1":"markdown","f9e68ebf":"markdown","6207bca9":"markdown","aeb99a68":"markdown","b8d6afe1":"markdown","2e74054a":"markdown","47e5e4ee":"markdown","0a0a2635":"markdown","7b58e1fb":"markdown","331ba861":"markdown","be3fd3dd":"markdown","c361429a":"markdown","5eba5a2d":"markdown","8e2859f0":"markdown","20a3291a":"markdown","acf1efa3":"markdown","874baced":"markdown","b6c108cd":"markdown","e9375877":"markdown","b7b18c24":"markdown","870c92b4":"markdown","c7144662":"markdown","8e34d5d8":"markdown","e3148ca5":"markdown","b8ed7f5d":"markdown","88fe4731":"markdown","a3bd9472":"markdown","e882fdfa":"markdown","38f8d54b":"markdown","10adba44":"markdown","96eee8dc":"markdown","ce02200c":"markdown","82606095":"markdown","b91b8a3d":"markdown","f2ebeeaa":"markdown","bc91072f":"markdown","aa9dfddf":"markdown","6b20df36":"markdown","b4f5d20f":"markdown","31f5a7f5":"markdown","79e7e207":"markdown","15f297f2":"markdown","988e7bee":"markdown","53a0f2db":"markdown","791980bf":"markdown","d7fabb12":"markdown","e2311626":"markdown","2556b5a5":"markdown","68dd1c4b":"markdown","a9459163":"markdown","df12f0aa":"markdown","abe483c4":"markdown","21857789":"markdown","6acfcf97":"markdown","7e41104b":"markdown","27383ce0":"markdown","6cff3dd9":"markdown","936fb81b":"markdown","2bdb4a4b":"markdown","16703245":"markdown","53baac59":"markdown","8624d2ee":"markdown","40535d3c":"markdown","7183c6a9":"markdown","03b6b29c":"markdown","aa289aa0":"markdown","6943c507":"markdown","555fe917":"markdown","43cd68ce":"markdown","c56d94e0":"markdown","72276c7a":"markdown","8f7a0d0e":"markdown","7ed575e6":"markdown","34db3c4e":"markdown","2146a6a5":"markdown","b8dc3402":"markdown","5e73f13f":"markdown","445b8bdb":"markdown","90d718b7":"markdown","01069bda":"markdown","c6b18805":"markdown","471c1334":"markdown","f126837e":"markdown","38ce4a83":"markdown","e00a91ed":"markdown","f0564192":"markdown","200bc957":"markdown","3e8714d0":"markdown","efcb289f":"markdown","13b02351":"markdown","d487aeb7":"markdown","1e548d2b":"markdown","73ba4e01":"markdown","421b6082":"markdown","a3e22e4a":"markdown","13cd8e20":"markdown","ec7951d5":"markdown","706f3d32":"markdown","d68786d8":"markdown","6ad23efd":"markdown","227a0532":"markdown"},"source":{"54f1af2a":"# from google.colab import drive\n# drive.mount('\/content\/drive')","6f11cb1b":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\nimport re\n# Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\nfrom tqdm import tqdm\nimport os\n\n# from plotly import plotly\nimport plotly.offline as offline\nimport plotly.graph_objs as go\noffline.init_notebook_mode()\nfrom collections import Counter\n# import os\n# print(os.listdir(\"..\/input\"))\nprint(\"DONE ---------------------------------------\")","b4b2152c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","60359e03":"# project_data = pd.read_csv('\/content\/drive\/My Drive\/Colab Notebooks\/train_data.csv') \n# resource_data = pd.read_csv('\/content\/drive\/My Drive\/Colab Notebooks\/resources.csv')\nproject_data = pd.read_csv('\/kaggle\/input\/donorschoosedataset\/train_data.csv') \nresource_data = pd.read_csv('\/kaggle\/input\/donorschoosedataset\/resources.csv')\n# project_data = pd.read_csv('..\/train_data.csv') \n# resource_data = pd.read_csv('..\/resources.csv')\n\nprint(\"Done\")","db7bfb87":"project_data.shape","e4b0b01a":"project_data.columns.values","9d7092f6":"prefixlist=project_data['teacher_prefix'].values\nprefixlist=list(prefixlist)\ncleanedPrefixList = [x for x in project_data['teacher_prefix'] if x != float('nan')] ## Cleaning the NULL Values in the list -> https:\/\/stackoverflow.com\/a\/50297200\/4433839\n\nlen(cleanedPrefixList)\n# print(len(prefixlist))","dd198cbc":"## Converting to Nan and Droping -> https:\/\/stackoverflow.com\/a\/29314880\/4433839\n\n# df[df['B'].str.strip().astype(bool)] \/\/ for deleting EMPTY STRINGS.\nproject_data.dropna(subset=['teacher_prefix'], inplace=True)\nproject_data.shape","3592eae5":"project_data['teacher_prefix'].head(10)","e8d05e8f":"print(\"Number of data points in train data\", resource_data.shape)\nprint(resource_data.columns.values)\nresource_data.head(2)","c7f56f2a":"project_data.columns","7ac04636":"project_data.shape","bd9396eb":"price_data = resource_data.groupby('id').agg({'price':'sum', 'quantity':'sum'}).reset_index()\nproject_data = pd.merge(project_data, price_data, on='id', how='left')","23e57ad1":"catogories = list(project_data['project_subject_categories'].values)\n# remove special characters from list of strings python: https:\/\/stackoverflow.com\/a\/47301924\/4084039\n\n# https:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/\n# https:\/\/stackoverflow.com\/questions\/23669024\/how-to-strip-a-specific-word-from-a-string\n# https:\/\/stackoverflow.com\/questions\/8270092\/remove-all-whitespace-in-a-string-in-python\ncat_list = []\nfor i in catogories:\n    temp = \"\"\n    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n        temp+=j.strip()+\" \" #\" abc \".strip() will return \"abc\", remove the trailing spaces\n        temp = temp.replace('&','_') # we are replacing the & value into \n    cat_list.append(temp.strip())\n    \nproject_data['clean_categories'] = cat_list\nproject_data.drop(['project_subject_categories'], axis=1, inplace=True)\n\nfrom collections import Counter\nmy_counter = Counter()\nfor word in project_data['clean_categories'].values:\n    my_counter.update(word.split())\n\ncat_dict = dict(my_counter)\nsorted_cat_dict = dict(sorted(cat_dict.items(), key=lambda kv: kv[1]))\nprint(\"done\")\n","7e210b5b":"cat_dict","27f1ff17":"sub_catogories = list(project_data['project_subject_subcategories'].values)\n# remove special characters from list of strings python: https:\/\/stackoverflow.com\/a\/47301924\/4084039\n\n# https:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/\n# https:\/\/stackoverflow.com\/questions\/23669024\/how-to-strip-a-specific-word-from-a-string\n# https:\/\/stackoverflow.com\/questions\/8270092\/remove-all-whitespace-in-a-string-in-python\n\nsub_cat_list = []\nfor i in sub_catogories:\n    temp = \"\"\n    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n        temp +=j.strip()+\" \"#\" abc \".strip() will return \"abc\", remove the trailing spaces\n        temp = temp.replace('&','_')\n    sub_cat_list.append(temp.strip())\n\nproject_data['clean_subcategories'] = sub_cat_list\nproject_data.drop(['project_subject_subcategories'], axis=1, inplace=True)\n\n# count of all the words in corpus python: https:\/\/stackoverflow.com\/a\/22898595\/4084039\nmy_counter = Counter()\nfor word in project_data['clean_subcategories'].values:\n    my_counter.update(word.split())\n    \nsub_cat_dict = dict(my_counter)\nsorted_sub_cat_dict = dict(sorted(sub_cat_dict.items(), key=lambda kv: kv[1]))\nprint(\"done\")","65b9ed84":"# this code removes \" \" and \"-\". ie Grades 3-5 -> grage3to5\n#  remove special characters from list of strings python: https:\/\/stackoverflow.com\/a\/47301924\/4084039\n\n# https:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/\n# https:\/\/stackoverflow.com\/questions\/23669024\/how-to-strip-a-specific-word-from-a-string\n# https:\/\/stackoverflow.com\/questions\/8270092\/remove-all-whitespace-in-a-string-in-python\nclean_grades=[]\nfor project_grade in project_data['project_grade_category'].values:\n    project_grade=str(project_grade).lower().strip().replace(' ','').replace('-','to')\n    \n    clean_grades.append(project_grade.strip())\n\nproject_data['clean_project_grade_category']=clean_grades\nproject_data.drop(['project_grade_category'],axis=1,inplace=True)\n\nmy_counter = Counter()\nfor word in project_data['clean_project_grade_category'].values:\n    my_counter.update(word.split())\n    \ngrade_dict = dict(my_counter)\nsorted_project_grade_cat_dict = dict(sorted(grade_dict.items(), key=lambda kv: kv[1]))\nprint(\"done\")\n","7ce12f38":"# merge two column text dataframe: \nproject_data[\"essay\"] = project_data[\"project_essay_1\"].map(str) +\\\n                        project_data[\"project_essay_2\"].map(str) + \\\n                        project_data[\"project_essay_3\"].map(str) + \\\n                        project_data[\"project_essay_4\"].map(str)","bec0f926":"project_data.head(2)","d61ad6d1":"#### Text PreProcessing Functions","fded37fb":"# https:\/\/stackoverflow.com\/a\/47091490\/4084039\nimport re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","9a3f21a5":"# https:\/\/gist.github.com\/sebleier\/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","f58ebed0":"# Combining all the above stundents \nfrom tqdm import tqdm\npreprocessed_essays = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(project_data['essay'].values):\n    sent = decontracted(sentance)\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n    preprocessed_essays.append(sent.lower().strip())","20d17545":"project_data.shape","e4707c18":"## new column added as PreProcessed_Essays and older unProcessed essays column is deleted\nproject_data['preprocessed_essays'] = preprocessed_essays\n# project_data.drop(['essay'], axis=1, inplace=True)","d3aa3384":"project_data.columns","f48ea510":"# after preprocesing\npreprocessed_essays[20000]","16030764":"# similarly you can preprocess the titles also\n# similarly you can preprocess the titles also\n# similarly you can preprocess the titles also\nfrom tqdm import tqdm\npreprocessed_titles = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(project_data['project_title'].values):\n    sent = decontracted(sentance)\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n    preprocessed_titles.append(sent.lower().strip())","e543d518":"#https:\/\/stackoverflow.com\/questions\/26666919\/add-column-in-dataframe-from-list\/3849072\nproject_data['preprocessed_titles'] = preprocessed_titles\n# project_data.drop(['project_title'], axis=1, inplace=True)","6d49553a":"project_data.columns","5563bf96":"project_data.shape","d0b20ab5":"price_data = resource_data.groupby('id').agg({'price':'sum', 'quantity':'sum'}).reset_index()\nprice_data.head(2)","42887c9d":"project_data = pd.merge(project_data, price_data, on='id', how='left')","7d55d8e7":"project_data.shape","4748d8f9":"# project_data['']\nfor col_type, new_col in [('project_title', 'title_size'), ('essay', 'essay_size')]:\n    print(\"Now in: \",col_type)\n    col_data = project_data[col_type]\n    print(col_data.head(10))\n    col_size = []\n    for sen in col_data:\n        sen = decontracted(sen)\n        col_size.append(len(sen.split()))\n    project_data[new_col] = col_size\n    col_size.clear()","74cedf69":"project_data.shape","7b7e95fb":"project_data.columns","75490dab":"project_data['title_size'].head(10)","87160a49":"project_data['essay_size'].head(10)","2f6cafb4":"project_bkp=project_data.copy()","20e3d62f":"project_bkp.shape","6bd8772c":"## taking random samples of 100k datapoints\nproject_data = project_bkp.sample(n = 100000) \n# resource_data = pd.read_csv('..\/resources.csv')\n\nproject_data.shape\n\n# y_value_counts = row1['project_is_approved'].value_counts()\ny_value_counts = project_data['project_is_approved'].value_counts()\nprint(\"Number of projects thar are approved for funding:     \", y_value_counts[1],\" -> \",round(y_value_counts[1]\/(y_value_counts[1]+y_value_counts[0])*100,2),\"%\")\nprint(\"Number of projects thar are not approved for funding: \", y_value_counts[0],\" -> \",round(y_value_counts[0]\/(y_value_counts[1]+y_value_counts[0])*100,2),\"%\")","0de05b77":"# # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n# from sklearn.model_selection import train_test_split\n# x_train,x_test,y_train,y_test=train_test_split(\n#     project_data.drop('project_is_approved', axis=1),\n#     project_data['project_is_approved'].values,\n#     test_size=0.3,\n#     random_state=42,\n#     stratify=project_data[['project_is_approved']])\n\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(\n    project_data,\n    project_data['project_is_approved'],\n    test_size=0.2,\n    random_state=42,\n    stratify=project_data[['project_is_approved']])\n\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test : \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test : \",y_test.shape)","9210e0ec":"# x_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train, test_size=0.25, stratify=y_train)\n\n# print(\"x_train: \",x_train.shape)\n# print(\"y_train: \",y_train.shape)\n# print(\"x_cv   : \",x_cv.shape)\n# print(\"x_cv   : \",y_cv.shape)\n# print(\"x_test : \",x_test.shape)\n# print(\"y_test : \",y_test.shape)","6fa54c2c":"# y_value_counts = row1['project_is_approved'].value_counts()\nprint(\"X_TRAIN-------------------------\")\nx_train_y_value_counts = x_train['project_is_approved'].value_counts()\nprint(\"Number of projects that are approved for funding    \", x_train_y_value_counts[1],\" -> \",round(x_train_y_value_counts[1]\/(x_train_y_value_counts[1]+x_train_y_value_counts[0])*100,2),\"%\")\nprint(\"Number of projects that are not approved for funding \",x_train_y_value_counts[0],\" -> \",round(x_train_y_value_counts[0]\/(x_train_y_value_counts[1]+x_train_y_value_counts[0])*100,2),\"%\")\nprint(\"\\n\")\n# y_value_counts = row1['project_is_approved'].value_counts()\nprint(\"X_TEST--------------------------\")\nx_test_y_value_counts = x_test['project_is_approved'].value_counts()\nprint(\"Number of projects that are approved for funding    \", x_test_y_value_counts[1],\" -> \",round(x_test_y_value_counts[1]\/(x_test_y_value_counts[1]+x_test_y_value_counts[0])*100,2),\"%\")\nprint(\"Number of projects that are not approved for funding \",x_test_y_value_counts[0],\" -> \",round(x_test_y_value_counts[0]\/(x_test_y_value_counts[1]+x_test_y_value_counts[0])*100,2),\"%\")\nprint(\"\\n\")\n# y_value_counts = row1['project_is_approved'].value_counts()\n# print(\"X_CV----------------------------\")\n# x_cv_y_value_counts = x_cv['project_is_approved'].value_counts()\n# print(\"Number of projects that are approved for funding    \", x_cv_y_value_counts[1],\" -> \",round(x_cv_y_value_counts[1]\/(x_cv_y_value_counts[1]+x_cv_y_value_counts[0])*100),2,\"%\")\n# print(\"Number of projects that are not approved for funding \",x_cv_y_value_counts[0],\" -> \",round(x_cv_y_value_counts[0]\/(x_cv_y_value_counts[1]+x_cv_y_value_counts[0])*100),2,\"%\")\n# print(\"\\n\")","eb6d6d99":"# from sklearn.utils import resample\n\n# ## splitting x_train in their respective classes\n# x_train_majority=x_train[x_train.project_is_approved==1]\n# x_train_minority=x_train[x_train.project_is_approved==0]\n\n# print(\"No. of points in the Training Dataset : \", x_train.shape)\n# print(\"No. of points in the majority class 1 : \",len(x_train_majority))\n# print(\"No. of points in the minority class 0 : \",len(x_train_minority))\n\n# print(x_train_majority.shape)\n# print(x_train_minority.shape)\n\n# ## Resampling with replacement\n# x_train_minority_upsampled=resample(\n#     x_train_minority,\n#     replace=True,\n\n#     n_samples=len(x_train_majority),\n#     random_state=123)\n\n# print(\"Resampled Minority class details\")\n# print(\"Type:  \",type(x_train_minority_upsampled))\n# print(\"Shape: \",x_train_minority_upsampled.shape)\n# print(\"\\n\")\n# ## Concatinating our Upsampled Minority class with the existing Majority class\n# x_train_upsampled=pd.concat([x_train_majority,x_train_minority_upsampled])\n\n# print(\"Upsampled Training data\")\n# print(\"Total number of Class labels\")\n# print(x_train_upsampled.project_is_approved.value_counts())\n# print(\"\\n\")\n# print(\"Old Training IMBALANCED Dataset Shape         : \", x_train.shape)\n# print(\"New Training BALANCED Upsampled Dataset Shape : \",x_train_upsampled.shape)\n\n# x_train_upsampled.to_csv ('x_train_upsampled_csv.csv',index=False)","de4efffe":"x_train.shape","4920b6c8":"x_test.shape","7c924359":"project_data.columns","71f2b7c4":"# we use count vectorizer to convert the values into one \nfrom sklearn.feature_extraction.text import CountVectorizer\n# vectorizer_sub = CountVectorizer(vocabulary=list(sorted_cat_dict.keys()), lowercase=False, binary=True)\nvectorizer_sub = CountVectorizer( lowercase=False, binary=True)\n\nvectorizer_sub.fit(x_train['clean_categories'].values)\n\nx_train_categories_one_hot = vectorizer_sub.transform(x_train['clean_categories'].values)\n# x_cv_categories_one_hot    = vectorizer_sub.transform(x_cv['clean_categories'].values)\nx_test_categories_one_hot  = vectorizer_sub.transform(x_test['clean_categories'].values)\n\n\nprint(vectorizer_sub.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> categories: x_train: \",x_train_categories_one_hot.shape)\n# print(\"Shape of matrix after one hot encoding -> categories: x_cv   : \",x_cv_categories_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> categories: x_test : \",x_test_categories_one_hot.shape)","a677bfc8":"# we use count vectorizer to convert the values into one \n# vectorizer_sub_sub = CountVectorizer(vocabulary=list(sorted_sub_cat_dict.keys()), lowercase=False, binary=True)\nvectorizer_sub_sub = CountVectorizer( lowercase=False, binary=True)\n\nvectorizer_sub_sub.fit(x_train['clean_subcategories'].values)\n\nx_train_sub_categories_one_hot = vectorizer_sub_sub.transform(x_train['clean_subcategories'].values)\n# x_cv_sub_categories_one_hot    = vectorizer_sub_sub.transform(x_cv['clean_subcategories'].values)\nx_test_sub_categories_one_hot  = vectorizer_sub_sub.transform(x_test['clean_subcategories'].values)\n\nprint(vectorizer_sub_sub.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> sub_categories: x_train: \",x_train_sub_categories_one_hot.shape)\n# print(\"Shape of matrix after one hot encoding -> sub_categories: x_cv   : \",x_cv_sub_categories_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> sub_categories: x_test : \",x_test_sub_categories_one_hot.shape)","6fe46810":"my_counter = Counter()\nfor state in project_data['school_state'].values:\n    my_counter.update(state.split())","dc73ff0c":"school_state_cat_dict = dict(my_counter)\nsorted_school_state_cat_dict = dict(sorted(school_state_cat_dict.items(), key=lambda kv: kv[1]))","b9ef5d61":"from scipy import sparse ## Exporting Sparse Matrix to NPZ File -> https:\/\/stackoverflow.com\/questions\/8955448\/save-load-scipy-sparse-csr-matrix-in-portable-data-format\nstatelist=list(project_data['school_state'].values)\n# vectorizer_state = CountVectorizer(vocabulary=set(statelist), lowercase=False, binary=True)\nvectorizer_state = CountVectorizer( lowercase=False, binary=True)\n\nvectorizer_state.fit(x_train['school_state'])\n\nx_train_school_state_one_hot = vectorizer_state.transform(x_train['school_state'].values)\n# x_cv_school_state_one_hot    = vectorizer_state.transform(x_cv['school_state'].values)\nx_test_school_state_one_hot  = vectorizer_state.transform(x_test['school_state'].values)\n\nprint(vectorizer_state.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> school_state: x_train: \",x_train_school_state_one_hot.shape)\n# print(\"Shape of matrix after one hot encoding -> school_state: x_cv   : \",x_cv_school_state_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> school_state: x_test : \",x_test_school_state_one_hot.shape)\n# school_one_hot = vectorizer.transform(statelist)\n# print(\"Shape of matrix after one hot encodig \",school_one_hot.shape)\n# print(type(school_one_hot))\n# sparse.save_npz(\"school_one_hot_export.npz\", school_one_hot) \n# print(school_one_hot.toarray())","8145aa67":"# prefixlist=project_data['teacher_prefix'].values\n# prefixlist=list(prefixlist)\n# cleanedPrefixList = [x for x in prefixlist if x == x] ## Cleaning the NULL Values in the list -> https:\/\/stackoverflow.com\/a\/50297200\/4433839\n\n# ## preprocessing the prefix to remove the SPACES,- else the vectors will be just 0's. Try adding - and see\n# prefix_nospace_list = []\n# for i in cleanedPrefixList:\n#     temp = \"\"\n#     i = i.replace('.','') # we are placeing all the '.'(dot) with ''(empty) ex:\"Mr.\"=>\"Mr\"\n#     temp +=i.strip()+\" \"#\" abc \".strip() will return \"abc\", remove the trailing spaces\n#     prefix_nospace_list.append(temp.strip())\n\n# cleanedPrefixList=prefix_nospace_list\n\n# vectorizer = CountVectorizer(vocabulary=set(cleanedPrefixList), lowercase=False, binary=True)\n# vectorizer.fit(cleanedPrefixList)\n# print(vectorizer.get_feature_names())\n# prefix_one_hot = vectorizer.transform(cleanedPrefixList)\n# print(\"Shape of matrix after one hot encodig \",prefix_one_hot.shape)\n# prefix_one_hot_ar=prefix_one_hot.todense()\n\n# ##code to export to csv -> https:\/\/stackoverflow.com\/a\/54637996\/4433839\n# # prefixcsv=pd.DataFrame(prefix_one_hot.toarray())\n# # prefixcsv.to_csv('prefix.csv', index=None,header=None)","0c1c97c4":"my_counter = Counter()\nfor teacher_prefix in project_data['teacher_prefix'].values:\n    teacher_prefix = str(teacher_prefix).lower().replace('.','').strip()\n    \n    my_counter.update(teacher_prefix.split())\nteacher_prefix_cat_dict = dict(my_counter)\nsorted_teacher_prefix_cat_dict = dict(sorted(teacher_prefix_cat_dict.items(), key=lambda kv: kv[1]))","c27e924e":"sorted_teacher_prefix_cat_dict.keys()","c4ccef02":"from sklearn.feature_extraction.text import CountVectorizer\n# vectorizer_prefix = CountVectorizer(vocabulary=list(sorted_teacher_prefix_cat_dict.keys()), lowercase=False, binary=True)\nvectorizer_prefix = CountVectorizer(lowercase=False, binary=True)\n\nvectorizer_prefix.fit(x_train['teacher_prefix'].values)\n\nx_train_prefix_one_hot = vectorizer_prefix.transform(x_train['teacher_prefix'].values)\n# x_cv_prefix_one_hot    = vectorizer_prefix.transform(x_cv['teacher_prefix'].values)\nx_test_prefix_one_hot  = vectorizer_prefix.transform(x_test['teacher_prefix'].values)\n\nprint(vectorizer_prefix.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> prefix: x_train: \",x_train_prefix_one_hot.shape)\n# print(\"Shape of matrix after one hot encoding -> prefix: x_cv   : \",x_cv_prefix_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> prefix: x_test : \",x_test_prefix_one_hot.shape)","92f5bc00":"from sklearn.feature_extraction.text import CountVectorizer\n# vectorizer_grade = CountVectorizer(vocabulary=list(sorted_project_grade_cat_dict.keys()), lowercase=False, binary=True)\nvectorizer_grade = CountVectorizer(lowercase=False, binary=True)\n\nvectorizer_grade.fit(x_train['clean_project_grade_category'].values)\n\nx_train_grade_category_one_hot = vectorizer_grade.transform(x_train['clean_project_grade_category'].values)\n# x_cv_grade_category_one_hot    = vectorizer_grade.transform(x_cv['clean_project_grade_category'].values)\nx_test_grade_category_one_hot  = vectorizer_grade.transform(x_test['clean_project_grade_category'].values)\n\nprint(vectorizer_grade.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> project_grade: x_train : \",x_train_grade_category_one_hot.shape)\n# print(\"Shape of matrix after one hot encoding -> project_grade: x_cv    : \",x_cv_grade_category_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> project_grade: x_test  : \",x_test_grade_category_one_hot.shape)\n","f36d51c3":"type(x_train_grade_category_one_hot)","8ae6c57b":"x_train_grade_category_one_hot.toarray()","1a49b0d9":"x_train['price_x'].head(10)","476fa422":"x_train['price_y'].head(10)","199be4e3":"# check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n# from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\n\n# price_standardized = standardScalar.fit(project_data['price'].values)\n# this will rise the error\n# ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# Reshape your data either using array.reshape(-1, 1)\n# transformer = Normalizer().fit(X)\nnormalizer = Normalizer()\n\nnormalizer.fit(x_train['price_x'].values.reshape(1,-1)) # finding the mean and standard deviation of this data\n# print(f\"Mean : {price_scalar.mean_[0]}, Standard deviation : {np.sqrt(price_scalar.var_[0])}\")\n\n# Now normalize the data\n\nx_train_price_normalized = normalizer.transform(x_train['price_x'].values.reshape(1, -1)).reshape(-1,1)\nx_test_price_normalized  = normalizer.transform(x_test['price_x'].values.reshape(1, -1)).reshape(-1,1)\n# x_cv_price_normalized    = normalizer.transform(x_cv['price'].values.reshape(1, -1)).reshape(-1,1)\n\n\n# print(\"Shape of matrix after normalization -> Teacher Prefix: x_train: \",x_train_prefix_one_hot.shape)\n# # print(\"Shape of matrix after normalization -> Teacher Prefix: x_cv   : \",x_cv_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Teacher Prefix: x_test : \",x_test_prefix_one_hot.shape)","e835cec7":"type(x_train_price_normalized)","3fc1fb1d":"x_train_price_normalized","deaa358e":"# # check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# # standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n# # from sklearn.preprocessing import StandardScaler\n# from sklearn.preprocessing import Normalizer\n\n# # price_standardized = standardScalar.fit(project_data['price'].values)\n# # this will rise the error\n# # ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# # Reshape your data either using array.reshape(-1, 1)\n# # transformer = Normalizer().fit(X)\n# normalizer = Normalizer()\n\n# normalizer.fit(x_train_upsampled['price'].values.reshape(-1,1)) # finding the mean and standard deviation of this data\n# # print(f\"Mean : {price_scalar.mean_[0]}, Standard deviation : {np.sqrt(price_scalar.var_[0])}\")\n\n# # Now normalize the data\n\n# x_train_price_normalized = normalizer.transform(x_train_upsampled['price'].values.reshape(-1, 1))\n# x_test_price_normalized  = normalizer.transform(x_test['price'].values.reshape(-1, 1))\n# x_cv_price_normalized    = normalizer.transform(x_cv['price'].values.reshape(-1, 1))\n\n\n# print(\"Shape of matrix after normalization -> Teacher Prefix: x_train: \",x_train_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Teacher Prefix: x_cv   : \",x_cv_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Teacher Prefix: x_test : \",x_test_prefix_one_hot.shape)","cd079de1":"# check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\nfrom sklearn.preprocessing import StandardScaler\n\n# price_standardized = standardScalar.fit(project_data['price'].values)\n# this will rise the error\n# ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# Reshape your data either using array.reshape(-1, 1)\n\nteacher_previous_proj_normalizer = Normalizer()\n# normalizer = Normalizer()\n\nteacher_previous_proj_normalizer.fit(x_train['teacher_number_of_previously_posted_projects'].values.reshape(1,-1)) # finding the mean and standard deviation of this data\n# print(f\"Mean : {teacher_previous_proj_scalar.mean_[0]}, Standard deviation : {np.sqrt(teacher_previous_proj_scalar.var_[0])}\")\n\n# Now standardize the data with above maen and variance.\nx_train_teacher_previous_proj_normalized = teacher_previous_proj_normalizer.transform(x_train['teacher_number_of_previously_posted_projects'].values.reshape(1,- 1)).reshape(-1,1)\nx_test_teacher_previous_proj_normalized  = teacher_previous_proj_normalizer.transform(x_test['teacher_number_of_previously_posted_projects'].values.reshape(1,- 1)).reshape(-1,1)\n# x_cv_teacher_previous_proj_normalized    = teacher_previous_proj_normalizer.transform(x_cv['teacher_number_of_previously_posted_projects'].values.reshape(1,- 1)).reshape(-1,1)\n\n# print(\"Shape of matrix after normalization -> Teachers Previous Projects: x_train:  \",x_train_prefix_one_hot.shape)\n# # print(\"Shape of matrix after normalization -> Teachers Previous Projects: x_cv   :  \",x_cv_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Teachers Previous Projects: x_test :  \",x_test_prefix_one_hot.shape)\n","b6c19a5e":"x_train_teacher_previous_proj_normalized","0d5f9042":"x_train.columns","64e12688":"project_data['title_size'].head(10)","155e2846":"# check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n# from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\n\n# price_standardized = standardScalar.fit(project_data['price'].values)\n# this will rise the error\n# ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# Reshape your data either using array.reshape(-1, 1)\n# transformer = Normalizer().fit(X)\nnormalizer = Normalizer()\n\nnormalizer.fit(x_train['title_size'].values.reshape(1,-1)) # finding the mean and standard deviation of this data\n# print(f\"Mean : {price_scalar.mean_[0]}, Standard deviation : {np.sqrt(price_scalar.var_[0])}\")\n\n# Now normalize the data\n\nx_train_title_normalized = normalizer.transform(x_train['title_size'].values.reshape(1, -1)).reshape(-1,1)\nx_test_title_normalized  = normalizer.transform(x_test['title_size'].values.reshape(1, -1)).reshape(-1,1)\n# x_cv_price_normalized    = normalizer.transform(x_cv['price'].values.reshape(1, -1)).reshape(-1,1)\n\n\n# print(\"Shape of matrix after normalization -> Project Title: x_train: \",x_train_title_one_hot.shape)\n# # print(\"Shape of matrix after normalization -> Teacher Prefix: x_cv   : \",x_cv_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Project Title: x_test : \",x_test_title_one_hot.shape)","effef9b4":"type(x_train_title_normalized)","cb0c9013":"x_train_title_normalized","3c5e6e29":"project_data['essay_size']","8ff6a520":"# check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n# from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\n\n# price_standardized = standardScalar.fit(project_data['price'].values)\n# this will rise the error\n# ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# Reshape your data either using array.reshape(-1, 1)\n# transformer = Normalizer().fit(X)\nnormalizer = Normalizer()\n\nnormalizer.fit(x_train['essay_size'].values.reshape(1,-1)) # finding the mean and standard deviation of this data\n# print(f\"Mean : {price_scalar.mean_[0]}, Standard deviation : {np.sqrt(price_scalar.var_[0])}\")\n\n# Now normalize the data\n\nx_train_essay_normalized = normalizer.transform(x_train['essay_size'].values.reshape(1, -1)).reshape(-1,1)\nx_test_essay_normalized  = normalizer.transform(x_test['essay_size'].values.reshape(1, -1)).reshape(-1,1)\n# x_cv_price_normalized    = normalizer.transform(x_cv['price'].values.reshape(1, -1)).reshape(-1,1)\n\n\n# print(\"Shape of matrix after normalization -> Project Title: x_train: \",x_train_title_one_hot.shape)\n# # print(\"Shape of matrix after normalization -> Teacher Prefix: x_cv   : \",x_cv_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Project Title: x_test : \",x_test_title_one_hot.shape)","529e168a":"x_train_essay_normalized","6d8d1e6f":"project_data['quantity_x']","e03760d4":"# check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n# from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\n\n# price_standardized = standardScalar.fit(project_data['price'].values)\n# this will rise the error\n# ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# Reshape your data either using array.reshape(-1, 1)\n# transformer = Normalizer().fit(X)\nnormalizer = Normalizer()\n\nnormalizer.fit(x_train['quantity_x'].values.reshape(1,-1)) # finding the mean and standard deviation of this data\n# print(f\"Mean : {price_scalar.mean_[0]}, Standard deviation : {np.sqrt(price_scalar.var_[0])}\")\n\n# Now normalize the data\n\nx_train_quantity_normalized = normalizer.transform(x_train['quantity_y'].values.reshape(1, -1)).reshape(-1,1)\nx_test_quantity_normalized  = normalizer.transform(x_test['quantity_y'].values.reshape(1, -1)).reshape(-1,1)\n# x_cv_price_normalized    = normalizer.transform(x_cv['price'].values.reshape(1, -1)).reshape(-1,1)\n\n\n# print(\"Shape of matrix after normalization -> Project Title: x_train: \",x_train_title_one_hot.shape)\n# # print(\"Shape of matrix after normalization -> Teacher Prefix: x_cv   : \",x_cv_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Project Title: x_test : \",x_test_title_one_hot.shape)","c3a25c13":"# We are considering only the words which appeared in at least 10 documents(rows or projects).\nvectorizer_essay_bow = CountVectorizer(min_df=10)\n\nvectorizer_essay_bow.fit(x_train['preprocessed_essays'])\n\nx_train_essays_bow = vectorizer_essay_bow.transform(x_train['preprocessed_essays'])\n# x_cv_essays_bow    = vectorizer_essay_bow.transform(x_cv['preprocessed_essays'])\nx_test_essays_bow  = vectorizer_essay_bow.transform(x_test['preprocessed_essays'])\n\nprint(\"Shape of matrix after BOW -> Essays: x_train: \",x_train_essays_bow.shape)\n# print(\"Shape of matrix after BOW -> Essays: x_cv   : \",x_cv_essays_bow.shape)\nprint(\"Shape of matrix after BOW -> Essays: x_test : \",x_test_essays_bow.shape)","8be3ed5a":"## bigram using countvectorizer example -> https:\/\/stackoverflow.com\/a\/24006054\/4433839\n# v = CountVectorizer(ngram_range=( 2,2))\n# print(v.fit([\"an apple a day keeps the doctor away\"]).vocabulary_)\n\n# We are considering only the words which appeared in at least 10 documents(rows or projects).\nvectorizer_essay_bow_bigram = CountVectorizer(ngram_range=(2,2),min_df=10,max_features=5000)\n\nvectorizer_essay_bow_bigram.fit(x_train['preprocessed_essays'])\n\nx_train_essays_bow_bigram = vectorizer_essay_bow_bigram.transform(x_train['preprocessed_essays'])\n# x_cv_essays_bow    = vectorizer_essay_bow.transform(x_cv['preprocessed_essays'])\nx_test_essays_bow_bigram  = vectorizer_essay_bow_bigram.transform(x_test['preprocessed_essays'])\n\nprint(\"Shape of matrix after BOW -> Essays: x_train: \",x_train_essays_bow_bigram.shape)\n# print(\"Shape of matrix after BOW -> Essays: x_cv   : \",x_cv_essays_bow.shape)\nprint(\"Shape of matrix after BOW -> Essays: x_test : \",x_test_essays_bow_bigram.shape)\n","cbee5d8c":"# print(vectorizer_essay_bow_bigram.vocabulary_)\nprint(type(x_train_essays_bow_bigram))","c21297e7":"print(type(vectorizer_essay_bow_bigram))","ae90b607":"vectorizer_title_bow = CountVectorizer(min_df=2)\n\nvectorizer_title_bow.fit(x_train['preprocessed_titles'])\n\nx_train_titles_bow = vectorizer_title_bow.transform(x_train['preprocessed_titles'])\n# x_cv_titles_bow    = vectorizer_title_bow.transform(x_cv['preprocessed_titles'])\nx_test_titles_bow  = vectorizer_title_bow.transform(x_test['preprocessed_titles'])\n\nprint(\"Shape of matrix after BOW -> Title: x_train: \",x_train_titles_bow.shape)\n# print(\"Shape of matrix after BOW -> Title: x_cv   : \",x_cv_titles_bow.shape)\nprint(\"Shape of matrix after BOW -> Title: x_test : \",x_test_titles_bow.shape)","b8ec87a6":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer_essay_tfidf_bigram = TfidfVectorizer(ngram_range=(2,2),min_df=10,max_features=5000)\n\nvectorizer_essay_tfidf_bigram.fit(x_train['preprocessed_essays'])\n\nx_train_essays_tfidf_bigram = vectorizer_essay_tfidf_bigram.transform(x_train['preprocessed_essays'])\n# x_cv_essays_tfidf    = vectorizer_essay_tfidf.transform(x_cv['preprocessed_essays'])\nx_test_essays_tfidf_bigram  = vectorizer_essay_tfidf_bigram.transform(x_test['preprocessed_essays'])\n\nprint(\"Shape of matrix after TF-IDF -> Essay: x_train: \",x_train_essays_tfidf_bigram.shape)\n# print(\"Shape of matrix after TF-IDF -> Essay: x_cv   : \",x_cv_essays_tfidf.shape)\nprint(\"Shape of matrix after TF-IDF -> Essay: x_test : \",x_test_essays_tfidf_bigram.shape)","3b53392d":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer_title_tfidf = TfidfVectorizer(min_df=2)\n\nvectorizer_title_tfidf.fit(x_train['preprocessed_titles'])\n\nx_train_titles_tfidf = vectorizer_title_tfidf.transform(x_train['preprocessed_titles'])\n# x_cv_titles_tfidf    = vectorizer_title_tfidf.transform(x_cv['preprocessed_titles'])\nx_test_titles_tfidf  = vectorizer_title_tfidf.transform(x_test['preprocessed_titles'])\n\nprint(\"Shape of matrix after TF-IDF -> Title: x_train: \",x_train_titles_tfidf.shape)\n# print(\"Shape of matrix after TF-IDF -> Title: x_cv   : \",x_cv_titles_tfidf.shape)\nprint(\"Shape of matrix after TF-IDF -> Title: x_test : \",x_test_titles_tfidf.shape)\n\n# Code for testing and checking the generated vectors\n# v1 = vectorizer.transform([preprocessed_titles[0]]).toarray()[0]\n# text_title_tfidf=pd.DataFrame(v1)\n# text_title_tfidf.to_csv('text_title_tfidf.csv', index=None,header=None)","0d095b56":"# print(STOP)","9f12a731":"'''\n# Reading glove vectors in python: https:\/\/stackoverflow.com\/a\/38230349\/4084039\ndef loadGloveModel(gloveFile):\n    print (\"Loading Glove Model\")\n    f = open(gloveFile,'r', encoding=\"utf8\")\n    model = {}\n    for line in tqdm(f):\n        splitLine = line.split()\n        word = splitLine[0]\n        embedding = np.array([float(val) for val in splitLine[1:]])\n        model[word] = embedding\n    print (\"Done.\",len(model),\" words loaded!\")\n    return model\nmodel = loadGloveModel('glove.42B.300d.txt')\n\n# ============================\nOutput:\n    \nLoading Glove Model\n1917495it [06:32, 4879.69it\/s]\nDone. 1917495  words loaded!\n\n# ============================\n\nwords = []\nfor i in preproced_texts:\n    words.extend(i.split(' '))\n\nfor i in preproced_titles:\n    words.extend(i.split(' '))\nprint(\"all the words in the coupus\", len(words))\nwords = set(words)\nprint(\"the unique words in the coupus\", len(words))\n\ninter_words = set(model.keys()).intersection(words)\nprint(\"The number of words that are present in both glove vectors and our coupus\", \\\n      len(inter_words),\"(\",np.round(len(inter_words)\/len(words)*100,3),\"%)\")\n\nwords_courpus = {}\nwords_glove = set(model.keys())\nfor i in words:\n    if i in words_glove:\n        words_courpus[i] = model[i]\nprint(\"word 2 vec length\", len(words_courpus))\n\n\n# stronging variables into pickle files python: http:\/\/www.jessicayung.com\/how-to-use-pickle-to-save-and-load-variables-in-python\/\n\nimport pickle\nwith open('glove_vectors', 'wb') as f:\n    pickle.dump(words_courpus, f)\n\n\n'''\n\n\n# words_train_essays = []\n# for i in preprocessed_essays_train :\n#     words_train_essays.extend(i.split(' '))\n\n# ## Find the total number of words in the Train data of Essays.\n# print(\"all the words in the corpus\", len(words_train_essays))\n\n# ## Find the unique words in this set of words\n# words_train_essay = set(words_train_essays)\n# print(\"the unique words in the corpus\", len(words_train_essay))\n\n# ## Find the words present in both Glove Vectors as well as our corpus.\n# inter_words = set(model.keys()).intersection(words_train_essay)\n# print(\"The number of words that are present in both glove vectors and our corpus are {} which is nearly {}% \".format(len(inter_words), np.round((float(len(inter_words))\/len(words_train_essay))*100)))\n\n# words_corpus_train_essay = {}\n# words_glove = set(model.keys())\n# for i in words_train_essay:\n#     if i in words_glove:\n#         words_corpus_train_essay[i] = model[i]\n# print(\"word 2 vec length\", len(words_corpus_train_essay))","fce659b3":"# stronging variables into pickle files python: http:\/\/www.jessicayung.com\/how-to-use-pickle-to-save-and-load-variables-in-python\/\n# make sure you have the glove_vectors file\n# with open('\/content\/drive\/My Drive\/Colab Notebooks\/glove_vectors', 'rb') as f:\nwith open('\/kaggle\/input\/donorschoosedataset\/glove_vectors\/glove_vectors', 'rb') as f:\n# with open('..\/glove_vectors', 'rb') as f:\n    model = pickle.load(f)\n    glove_words =  set(model.keys())","6762ba00":"# average Word2Vec\n# compute average word2vec for each review.\nx_train_essays_avg_w2v_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\n# for sentence in tqdm(x_train_upsampled['preprocessed_essays']): # for each review\/sentence\nfor sentence in tqdm(x_train['preprocessed_essays']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if word in glove_words:\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector \/= cnt_words\n    x_train_essays_avg_w2v_vectors.append(vector)\n\nprint(\"Total number of vectors  : AVG-W2V -> Essay: x_train: \",len(x_train_essays_avg_w2v_vectors))\nprint(\"Length of a Single vector: AVG-W2V -> Essay: x_train: \",len(x_train_essays_avg_w2v_vectors[0]))","2d4b11ba":"# ## NOT DONE CV\n\n# x_cv_essays_avg_w2v_vectors = [];\n# for sentence in tqdm(x_cv['preprocessed_essays']): # for each review\/sentence\n#     vector = np.zeros(300) # as word vectors are of zero length\n#     cnt_words =0; # num of words with a valid vector in the sentence\/review\n#     for word in sentence.split(): # for each word in a review\/sentence\n#         if word in glove_words:\n#             vector += model[word]\n#             cnt_words += 1\n#     if cnt_words != 0:\n#         vector \/= cnt_words\n#     x_cv_essays_avg_w2v_vectors.append(vector)\n\n# print(\"Total number of vectors  : AVG-W2V -> Essay: x_cv   : \",len(x_cv_essays_avg_w2v_vectors))\n# print(\"Length of a Single vector: AVG-W2V -> Essay: x_cv   : \",len(x_cv_essays_avg_w2v_vectors[0]))","63e297bd":"x_test_essays_avg_w2v_vectors = [];\nfor sentence in tqdm(x_test['preprocessed_essays']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if word in glove_words:\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector \/= cnt_words\n    x_test_essays_avg_w2v_vectors.append(vector)\n\nprint(\"Total number of vectors  : AVG-W2V -> Essay: x_test : \",len(x_test_essays_avg_w2v_vectors))\nprint(\"Length of a Single vector: AVG-W2V -> Essay: x_test : \",len(x_test_essays_avg_w2v_vectors[0]))","64a98654":"x_train_titles_avg_w2v_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\n# for sentence in tqdm(x_train_upsampled['preprocessed_titles']): # for each review\/sentence\nfor sentence in tqdm(x_train['preprocessed_titles']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if word in glove_words:\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector \/= cnt_words\n    x_train_titles_avg_w2v_vectors.append(vector)\n\nprint(\"Total number of vectors  : AVG-W2V -> Titles: x_train: \",len(x_train_titles_avg_w2v_vectors))\nprint(\"Length of a Single vector: AVG-W2V -> Titles: x_train: \",len(x_train_titles_avg_w2v_vectors[0]))","1b8e4cda":"# ### NOT DONE HERE\n\n\n# x_cv_titles_avg_w2v_vectors = [];\n# for sentence in tqdm(x_cv['preprocessed_titles']): # for each review\/sentence\n#     vector = np.zeros(300) # as word vectors are of zero length\n#     cnt_words =0; # num of words with a valid vector in the sentence\/review\n#     for word in sentence.split(): # for each word in a review\/sentence\n#         if word in glove_words:\n#             vector += model[word]\n#             cnt_words += 1\n#     if cnt_words != 0:\n#         vector \/= cnt_words\n#     x_cv_titles_avg_w2v_vectors.append(vector)\n\n# print(\"Total number of vectors  : AVG-W2V -> Titles: x_cv   : \",len(x_cv_titles_avg_w2v_vectors))\n# print(\"Length of a Single vector: AVG-W2V -> Titles: x_cv   : \",len(x_cv_titles_avg_w2v_vectors[0]))","c98d3897":"x_test_titles_avg_w2v_vectors = [];\nfor sentence in tqdm(x_test['preprocessed_titles']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if word in glove_words:\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector \/= cnt_words\n    x_test_titles_avg_w2v_vectors.append(vector)\n\nprint(\"Total number of vectors  : AVG-W2V -> Titles: x_test : \",len(x_test_titles_avg_w2v_vectors))\nprint(\"Length of a Single vector: AVG-W2V -> Titles: x_test : \",len(x_test_titles_avg_w2v_vectors[0]))","c06edeb4":"# S = [\"abc def pqr\", \"def def def abc\", \"pqr pqr def\"]\ntfidf_model = TfidfVectorizer()\n# tfidf_model.fit(x_train_upsampled['preprocessed_essays'])\ntfidf_model.fit(x_train['preprocessed_essays'])\n# we are converting a dictionary with word as a key, and the idf as a value\ndictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\ntfidf_words = set(tfidf_model.get_feature_names())","2987c290":"# average Word2Vec\n# compute average word2vec for each review.\nx_train_essays_tfidf_w2v_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\n# for sentence in tqdm(x_train_upsampled['preprocessed_essays']): # for each review\/sentence\nfor sentence in tqdm(x_train['preprocessed_essays']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if (word in glove_words) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector \/= tf_idf_weight\n    x_train_essays_tfidf_w2v_vectors.append(vector)\n\nprint(\"Total number of vectors  : TFIDF-W2V -> Essays: x_train : \",len(x_train_essays_tfidf_w2v_vectors))\nprint(\"Length of a Single vector: TFIDF-W2V -> Essays: x_train : \",len(x_train_essays_tfidf_w2v_vectors[0]))","5b0d384e":"# ### NOT DONE HERE\n\n\n# x_cv_essays_tfidf_w2v_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\n# for sentence in tqdm(x_cv['preprocessed_essays']): # for each review\/sentence\n#     vector = np.zeros(300) # as word vectors are of zero length\n#     tf_idf_weight =0; # num of words with a valid vector in the sentence\/review\n#     for word in sentence.split(): # for each word in a review\/sentence\n#         if (word in glove_words) and (word in tfidf_words):\n#             vec = model[word] # getting the vector for each word\n#             # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n#             tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n#             vector += (vec * tf_idf) # calculating tfidf weighted w2v\n#             tf_idf_weight += tf_idf\n#     if tf_idf_weight != 0:\n#         vector \/= tf_idf_weight\n#     x_cv_essays_tfidf_w2v_vectors.append(vector)\n\n# print(\"Total number of vectors  : TFIDF-W2V -> Essays: x_cv : \",len(x_cv_essays_tfidf_w2v_vectors))\n# print(\"Length of a Single vector: TFIDF-W2V -> Essays: x_cv : \",len(x_cv_essays_tfidf_w2v_vectors[0]))","9a7454a2":"x_test_essays_tfidf_w2v_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\nfor sentence in tqdm(x_test['preprocessed_essays']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if (word in glove_words) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector \/= tf_idf_weight\n    x_test_essays_tfidf_w2v_vectors.append(vector)\n\nprint(\"Total number of vectors  : TFIDF-W2V -> Essays: x_test : \",len(x_test_essays_tfidf_w2v_vectors))\nprint(\"Length of a Single vector: TFIDF-W2V -> Essays: x_test : \",len(x_test_essays_tfidf_w2v_vectors[0]))","170a43a0":"# S = [\"abc def pqr\", \"def def def abc\", \"pqr pqr def\"]\ntfidf_title_model = TfidfVectorizer()\n# tfidf_title_model.fit(x_train_upsampled['preprocessed_titles'])\ntfidf_title_model.fit(x_train['preprocessed_titles'])\n# we are converting a dictionary with word as a key, and the idf as a value\ndictionary = dict(zip(tfidf_title_model.get_feature_names(), list(tfidf_title_model.idf_)))\ntfidf_title_words = set(tfidf_title_model.get_feature_names())","677c3f45":"# average Word2Vec\n# compute average word2vec for each title.\nx_train_tfidf_w2v_title_vectors = []; # the avg-w2v for each title is stored in this list\n# for sentence in x_train_upsampled['preprocessed_titles']: # for each review\/sentence\nfor sentence in x_train['preprocessed_titles']: # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    tf_idf_title_weight =0; # num of words with a valid vector in the title\n    for word in sentence.split(): # for each word in a title\n        if (word in glove_words) and (word in tfidf_title_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_title_weight += tf_idf\n    if tf_idf_title_weight != 0:\n        vector \/= tf_idf_title_weight\n    x_train_tfidf_w2v_title_vectors.append(vector)\n\nprint(\"Total number of vectors  : TFIDF-W2V -> Titles: x_train : \",len(x_train_tfidf_w2v_title_vectors))\nprint(\"Length of a Single vector: TFIDF-W2V -> Titles: x_train : \",len(x_train_tfidf_w2v_title_vectors[0]))","6aeb4c20":"# ### NOT DONE HERE\n\n\n# # average Word2Vec\n# # compute average word2vec for each title.\n# x_cv_tfidf_w2v_title_vectors = []; # the avg-w2v for each title is stored in this list\n# for sentence in x_cv['preprocessed_titles']: # for each review\/sentence\n#     vector = np.zeros(300) # as word vectors are of zero length\n#     tf_idf_title_weight =0; # num of words with a valid vector in the title\n#     for word in sentence.split(): # for each word in a title\n#         if (word in glove_words) and (word in tfidf_title_words):\n#             vec = model[word] # getting the vector for each word\n#             # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n#             tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n#             vector += (vec * tf_idf) # calculating tfidf weighted w2v\n#             tf_idf_title_weight += tf_idf\n#     if tf_idf_title_weight != 0:\n#         vector \/= tf_idf_title_weight\n#     x_cv_tfidf_w2v_title_vectors.append(vector)\n\n# print(\"Total number of vectors  : TFIDF-W2V -> Titles: x_cv : \",len(x_cv_tfidf_w2v_title_vectors))\n# print(\"Length of a Single vector: TFIDF-W2V -> Titles: x_cv : \",len(x_cv_tfidf_w2v_title_vectors[0]))","4e387584":"# average Word2Vec\n# compute average word2vec for each title.\nx_test_tfidf_w2v_title_vectors = []; # the avg-w2v for each title is stored in this list\nfor sentence in x_test['preprocessed_titles']: # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    tf_idf_title_weight =0; # num of words with a valid vector in the title\n    for word in sentence.split(): # for each word in a title\n        if (word in glove_words) and (word in tfidf_title_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_title_weight += tf_idf\n    if tf_idf_title_weight != 0:\n        vector \/= tf_idf_title_weight\n    x_test_tfidf_w2v_title_vectors.append(vector)\n\nprint(\"Total number of vectors  : TFIDF-W2V -> Titles: x_test : \",len(x_test_tfidf_w2v_title_vectors))\nprint(\"Length of a Single vector: TFIDF-W2V -> Titles: x_test : \",len(x_test_tfidf_w2v_title_vectors[0]))","8f58b2e2":"from scipy.sparse import hstack\n\nx_train_onehot = hstack((x_train_categories_one_hot, x_train_sub_categories_one_hot, x_train_school_state_one_hot, x_train_prefix_one_hot, x_train_grade_category_one_hot, x_train_price_normalized, x_train_teacher_previous_proj_normalized))\n# x_cv_onehot    = hstack((x_cv_categories_one_hot, x_cv_sub_categories_one_hot, x_cv_school_state_one_hot, x_cv_prefix_one_hot, x_cv_grade_category_one_hot,x_cv_price_normalized, x_cv_teacher_previous_proj_normalized ))\nx_test_onehot  = hstack((x_test_categories_one_hot, x_test_sub_categories_one_hot, x_test_school_state_one_hot, x_test_prefix_one_hot, x_test_grade_category_one_hot, x_test_price_normalized, x_test_teacher_previous_proj_normalized))\n\nprint(\"Type -> One Hot -> x_train: \",type(x_train_onehot))\nprint(\"Type -> One Hot -> x_test : \",type(x_test_onehot))\n# print(\"Type -> One Hot -> x_cv        : \",type(x_cv_onehot))\nprint(\"\\n\")\nprint(\"Shape -> One Hot -> x_train: \",x_train_onehot.shape)\nprint(\"Shape -> One Hot -> x_test : \",x_test_onehot.shape)\n# print(\"Shape -> One Hot -> x_cv         : \",x_cv_onehot.shape)","9012196c":"x_train_onehot.shape\n","31c2ead0":"x_train_onehot_bow = hstack((x_train_onehot,x_train_titles_bow,x_train_essays_bow_bigram)).tocsr()### Merging all ONE HOT features\n# x_cv_onehot_bow    = hstack((x_cv_onehot, x_cv_titles_bow, x_cv_essays_bow)).tocsr()### Merging all ONE HOT features\nx_test_onehot_bow  = hstack((x_test_onehot, x_test_titles_bow, x_test_essays_bow_bigram)).tocsr()### Merging all ONE HOT features\nprint(\"Type -> One Hot BOW -> x_train_cv_test: \",type(x_train_onehot_bow))\n# print(\"Type -> One Hot BOW -> cv             : \",type(x_cv_onehot_bow))\nprint(\"Type -> One Hot BOW -> x_test         : \",type(x_test_onehot_bow))\nprint(\"\\n\")\nprint(\"Shape -> One Hot BOW -> x_train_cv_test: \",x_train_onehot_bow.shape)\n# print(\"Shape -> One Hot BOW -> cv             : \",x_cv_onehot_bow.shape)\nprint(\"Shape -> One Hot BOW -> x_test         : \",x_test_onehot_bow.shape)","fee384b0":"x_train_onehot_tfidf = hstack((x_train_onehot,x_train_titles_tfidf, x_train_essays_tfidf_bigram)).tocsr()\n# x_cv_onehot_tfidf    = hstack((x_cv_onehot,x_cv_titles_tfidf, x_cv_essays_tfidf)).tocsr()\nx_test_onehot_tfidf  = hstack((x_test_onehot,x_test_titles_tfidf, x_test_essays_tfidf_bigram)).tocsr()\nprint(\"Type -> One Hot TFIDF -> x_train_cv_test: \",type(x_train_onehot_tfidf))\n# print(\"Type -> One Hot TFIDF -> cv             : \",type(x_cv_onehot_tfidf))\nprint(\"Type -> One Hot TFIDF -> x_test         : \",type(x_test_onehot_tfidf))\nprint(\"\\n\")\nprint(\"Shape -> One Hot TFIDF -> x_train_cv_test: \",x_train_onehot_tfidf.shape)\n# print(\"Shape -> One Hot TFIDF -> cv             : \",x_cv_onehot_tfidf.shape)\nprint(\"Shape -> One Hot TFIDF -> x_test         : \",x_test_onehot_tfidf.shape)### SET 1:  Merging ONE HOT with BOW (Title and Essay) features","e5babf79":"x_train_onehot_avg_w2v = hstack((x_train_onehot, x_train_titles_avg_w2v_vectors, x_train_essays_avg_w2v_vectors)).tocsr()\n# x_cv_onehot_avg_w2v    = hstack((x_cv_onehot, x_cv_titles_avg_w2v_vectors, x_cv_essays_avg_w2v_vectors)).tocsr()\nx_test_onehot_avg_w2v  = hstack((x_test_onehot, x_test_titles_avg_w2v_vectors, x_test_essays_avg_w2v_vectors)).tocsr()\nprint(\"Type -> One Hot AVG W2V-> x_train_cv_test: \",type(x_train_onehot_avg_w2v))\n# print(\"Type -> One Hot AVG W2V-> cv             : \",type(x_cv_onehot_avg_w2v))\nprint(\"Type -> One Hot AVG W2V-> x_test         : \",type(x_test_onehot_avg_w2v))\nprint(\"\\n\")\nprint(\"Shape -> One Hot AVG W2V-> x_train_cv_test: \",x_train_onehot_avg_w2v.shape)\n# print(\"Shape -> One Hot AVG W2V-> cv             : \",x_cv_onehot_avg_w2v.shape)\nprint(\"Shape -> One Hot AVG W2V-> x_test         : \",x_test_onehot_avg_w2v.shape)### SET 1:  Merging ONE HOT with BOW (Title and Essay) features","bf051c1a":"x_train_onehot_tfidf_w2v = hstack((x_train_onehot, x_train_tfidf_w2v_title_vectors, x_train_essays_tfidf_w2v_vectors)).tocsr()\n# x_cv_onehot_tfidf_w2v    = hstack((x_cv_onehot, x_cv_tfidf_w2v_title_vectors, x_cv_essays_tfidf_w2v_vectors)).tocsr()\nx_test_onehot_tfidf_w2v  = hstack((x_test_onehot, x_test_tfidf_w2v_title_vectors, x_test_essays_tfidf_w2v_vectors)).tocsr()\nprint(\"Type -> One Hot TFIDF W2V -> x_train_cv_test: \",type(x_train_onehot_tfidf_w2v))\n# print(\"Type -> One Hot TFIDF W2V -> cv             : \",type(x_cv_onehot_tfidf_w2v))\nprint(\"Type -> One Hot TFIDF W2V -> x_test         : \",type(x_test_onehot_tfidf_w2v))\nprint(\"\\n\")\nprint(\"Shape -> One Hot TFIDF W2V -> x_train_cv_test: \",x_train_onehot_tfidf_w2v.shape)\n# print(\"Shape -> One Hot TFIDF W2V -> cv             : \",x_cv_onehot_tfidf_w2v.shape)\nprint(\"Shape -> One Hot TFIDF W2V -> x_test         : \",x_test_onehot_tfidf_w2v.shape)### SET 1:  Merging ONE HOT with BOW (Title and Essay) features","b1b2f468":"from sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nimport math\n","8eb9298a":"[10**x for x in range(-4,5)]","19d7e4f9":"y_test.shape","8f36c18c":"from sklearn.model_selection import GridSearchCV\nlogreg_bow= LogisticRegression(class_weight='balanced')\n# parameters = {'lambda':[0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 2500, 5000, 10000]}\nparameters = {'C':[0.0001,0.001,0.01,0.1,1,10,100,1000,10000]}\nclf = GridSearchCV(logreg_bow, parameters, cv= 3, scoring='roc_auc',verbose=1,return_train_score=True,n_jobs=-1)\n# clf.fit(x_cv_onehot_bow, y_cv)\nclf.fit(x_train_onehot_bow,y_train)\ntrain_auc= clf.cv_results_['mean_train_score']\ntrain_auc_std= clf.cv_results_['std_train_score']\ncv_auc = clf.cv_results_['mean_test_score']\ncv_auc_std= clf.cv_results_['std_test_score']\nbestC_1=clf.best_params_['C']\nbestScore_1=clf.best_score_\nprint(\"BEST ALPHA: \",clf.best_params_['C'],\" BEST SCORE: \",clf.best_score_) #clf.best_estimator_.alpha","d00b38e6":"# alphas = [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 2500, 5000, 10000]\nalphas = [0.0001,0.001,0.01,0.1,1,10,100,1000,10000]\nlog_alphas =[]\nfor a in tqdm(alphas):\n#     b = math.log(a)\n    b = np.log10(a)\n    log_alphas.append(b)\nplt.figure(figsize=(10,5))\nplt.plot(log_alphas, train_auc, label='Train AUC')\n# this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\nplt.gca().fill_between(log_alphas,train_auc - train_auc_std,train_auc + train_auc_std,alpha=0.3,color='darkblue')\nplt.plot(log_alphas, cv_auc, label='CV AUC')\n# this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\nplt.gca().fill_between(log_alphas,cv_auc - cv_auc_std,cv_auc + cv_auc_std,alpha=0.3,color='darkorange')\nplt.scatter(log_alphas, train_auc, label='Train AUC points')\nplt.scatter(log_alphas, cv_auc, label='CV AUC points')\nplt.legend()\nplt.xlabel(\"alpha: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"alpha: hyperparameter v\/s AUC\")\nplt.grid()\nplt.show()","60ffe561":"#https:\/\/scikitlearn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\nlogreg_bow_testModel = LogisticRegression(class_weight='balanced',C=bestC_1)\nlogreg_bow_testModel.fit(x_train_onehot_bow, y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n# y_train_pred = batch_predict(mnb_bow_testModel, x_train_onehot_bow)\ny_train_pred=logreg_bow_testModel.predict_proba(x_train_onehot_bow)[:,1]\n# y_test_pred = batch_predict(mnb_bow_testModel, x_test_onehot_bow)\ny_test_pred=logreg_bow_testModel.predict_proba(x_test_onehot_bow)[:,1]\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\nax = plt.subplot()\n\nauc_set1_train=auc(train_fpr, train_tpr)\nauc_set1_test=auc(test_fpr, test_tpr)\n\n\nax.plot(train_fpr, train_tpr, label=\"Train AUC =\"+str(auc(train_fpr, train_tpr)))\nax.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"False Positive Rate(FPR)\")\nplt.ylabel(\"True Positive Rate(TPR)\")\nplt.title(\"AUC\")\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax.set_facecolor(\"white\")\nplt.show()","74c47905":"def predict(proba, threshould, fpr, tpr):\n    t = threshould[np.argmax(tpr*(1-fpr))]\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    predictions = []\n    for i in proba:\n        if i>=t:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","6ec88730":"## TRAIN\nprint(\"=\"*100)\nfrom sklearn.metrics import confusion_matrix\nprint(\"Train confusion matrix\")\nprint(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds, train_fpr, train_tpr)))\n\nconf_matr_df_train = pd.DataFrame(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds,train_fpr, train_tpr)), range(2),range(2))\n\n## Heatmaps -> https:\/\/likegeeks.com\/seaborn-heatmap-tutorial\/\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_train, annot=True,annot_kws={\"size\": 26}, fmt='g',cmap=\"YlGnBu\")","f2e2ce1c":"## TEST\nprint(\"=\"*100)\nprint(\"Test confusion matrix\")\nprint(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)))\n\nconf_matr_df_test = pd.DataFrame(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_test, annot=True,annot_kws={\"size\": 16}, fmt='g')","b5353b5f":"from sklearn.model_selection import GridSearchCV\nlogreg_tfidf = LogisticRegression(class_weight='balanced')\n# parameters = {'lambda':[0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 2500, 5000, 10000]}\nparameters = {'C':[0.0001,0.001,0.01,0.1,1,10,100,1000,10000]}\nclf = GridSearchCV(logreg_tfidf, parameters, cv= 3, scoring='roc_auc',verbose=1,return_train_score=True,n_jobs=-1)\n# clf.fit(x_cv_onehot_bow, y_cv)\nclf.fit(x_train_onehot_tfidf,y_train)\ntrain_auc= clf.cv_results_['mean_train_score']\ntrain_auc_std= clf.cv_results_['std_train_score']\ncv_auc = clf.cv_results_['mean_test_score']\ncv_auc_std= clf.cv_results_['std_test_score']\nbestC_2=clf.best_params_['C']\nbestScore_2=clf.best_score_\nprint(\"BEST C: \",clf.best_params_['C'],\" BEST SCORE: \",clf.best_score_) #clf.best_estimator_.alpha","b0abc869":"# alphas = [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 2500, 5000, 10000]\nalphas = [0.0001,0.001,0.01,0.1,1,10,100,1000,10000]\n\nlog_alphas =[]\nfor a in tqdm(alphas):\n#     b = math.log(a)\n    b = np.log10(a)\n    log_alphas.append(b)\nplt.figure(figsize=(10,5))\nplt.plot(log_alphas, train_auc, label='Train AUC')\n# this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\nplt.gca().fill_between(log_alphas,train_auc - train_auc_std,train_auc + train_auc_std,alpha=0.3,color='darkblue')\nplt.plot(log_alphas, cv_auc, label='CV AUC')\n# this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\nplt.gca().fill_between(log_alphas,cv_auc - cv_auc_std,cv_auc + cv_auc_std,alpha=0.3,color='darkorange')\n\nax = plt.subplot()\nax.scatter(log_alphas, train_auc, label='Train AUC points')\nax.scatter(log_alphas, cv_auc, label='CV AUC points')\nplt.legend()\nplt.xlabel(\"alpha: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"alpha: hyperparameter v\/s AUC\")\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax.set_facecolor(\"white\")\nplt.show()","fe387927":"#https:\/\/scikitlearn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\nlogref_tfidf_testModel = LogisticRegression(class_weight='balanced',C=bestC_2)\nlogref_tfidf_testModel.fit(x_train_onehot_tfidf, y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n# y_train_pred = batch_predict(mnb_tfidf_testModel, x_train_onehot_tfidf)\n# y_test_pred = batch_predict(mnb_tfidf_testModel, x_test_onehot_tfidf)\ny_train_pred=logref_tfidf_testModel.predict_proba(x_train_onehot_tfidf)[:,1]\ny_test_pred=logref_tfidf_testModel.predict_proba(x_test_onehot_tfidf)[:,1]\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\nax = plt.subplot()\n\nauc_set2_train=auc(train_fpr, train_tpr)\nauc_set2_test=auc(test_fpr, test_tpr)\n\n# plt.plot(train_fpr, train_tpr, label=\"Train AUC =\"+str(auc(train_fpr, train_tpr)))\n# plt.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(auc(test_fpr, test_tpr)))\nax.plot(train_fpr, train_tpr, label=\"Train AUC =\"+str(auc(train_fpr, train_tpr)))\nax.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(auc(test_fpr, test_tpr)))\n\nplt.legend()\nplt.xlabel(\"False Positive Rate(FPR)\")\nplt.ylabel(\"True Positive Rate(TPR)\")\nplt.title(\"AUC\")\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax.set_facecolor(\"white\")\nplt.show()\nprint(type(y_test_pred))\nprint(type(tr_thresholds))\nprint(type(test_fpr))\nprint(type(test_tpr))\nprint(y_test_pred.shape)\nprint(tr_thresholds.shape)\nprint(test_fpr.shape)\nprint(test_tpr.shape)","4b872670":"def predict(proba, threshould, fpr, tpr):\n    t = threshould[np.argmax(tpr*(1-fpr))]\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    predictions = []\n    for i in proba:\n        if i>=t:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","914601da":"## TRAIN\nprint(\"=\"*100)\nfrom sklearn.metrics import confusion_matrix\nprint(\"Train confusion matrix\")\nprint(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds, train_fpr, train_tpr)))\n\nconf_matr_df_train = pd.DataFrame(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds,train_fpr, train_tpr)), range(2),range(2))\n\n## Heatmaps -> https:\/\/likegeeks.com\/seaborn-heatmap-tutorial\/\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_train, annot=True,annot_kws={\"size\": 26}, fmt='g',cmap=\"YlGnBu\")","dcc9cb8c":"## TEST\nprint(\"=\"*100)\nprint(\"Test confusion matrix\")\nprint(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)))\n\nconf_matr_df_test = pd.DataFrame(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_test, annot=True,annot_kws={\"size\": 16}, fmt='g')","6540d472":"from sklearn.model_selection import GridSearchCV\nlogreg_avgw2v = LogisticRegression(class_weight='balanced')\n# parameters = {'lambda':[0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 2500, 5000, 10000]}\nparameters = {'C':[0.0001,0.001,0.01,0.1,1,10,100,1000,10000]}\nclf = GridSearchCV(logreg_avgw2v, parameters, cv= 3, scoring='roc_auc',verbose=1,return_train_score=True,n_jobs=-1)\n# clf.fit(x_cv_onehot_bow, y_cv)\nclf.fit(x_train_onehot_avg_w2v,y_train)\ntrain_auc= clf.cv_results_['mean_train_score']\ntrain_auc_std= clf.cv_results_['std_train_score']\ncv_auc = clf.cv_results_['mean_test_score']\ncv_auc_std= clf.cv_results_['std_test_score']\nbestC_3=clf.best_params_['C']\nbestScore_3=clf.best_score_\nprint(\"BEST C: \",clf.best_params_['C'],\" BEST SCORE: \",clf.best_score_) #clf.best_estimator_.alpha","afba6bc1":"# alphas = [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 2500, 5000, 10000]\nalphas = [0.0001,0.001,0.01,0.1,1,10,100,1000,10000]\nlog_alphas =[]\nfor a in tqdm(alphas):\n#     b = math.log(a)\n    b = np.log10(a)\n    log_alphas.append(b)\nplt.figure(figsize=(10,5))\nplt.plot(log_alphas, train_auc, label='Train AUC')\n# this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\nplt.gca().fill_between(log_alphas,train_auc - train_auc_std,train_auc + train_auc_std,alpha=0.3,color='darkblue')\nplt.plot(log_alphas, cv_auc, label='CV AUC')\n# this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\nplt.gca().fill_between(log_alphas,cv_auc - cv_auc_std,cv_auc + cv_auc_std,alpha=0.3,color='darkorange')\n\nax = plt.subplot()\nax.scatter(log_alphas, train_auc, label='Train AUC points')\nax.scatter(log_alphas, cv_auc, label='CV AUC points')\nplt.legend()\nplt.xlabel(\"alpha: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"alpha: hyperparameter v\/s AUC\")\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax.set_facecolor(\"white\")\nplt.show()","5573f225":"#https:\/\/scikitlearn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\nlogref_avgw2v_testModel = LogisticRegression(class_weight='balanced',C=bestC_3)\nlogref_avgw2v_testModel.fit(x_train_onehot_avg_w2v, y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n# y_train_pred = batch_predict(mnb_tfidf_testModel, x_train_onehot_tfidf)\n# y_test_pred = batch_predict(mnb_tfidf_testModel, x_test_onehot_tfidf)\ny_train_pred=logref_avgw2v_testModel.predict_proba(x_train_onehot_avg_w2v)[:,1]\ny_test_pred=logref_avgw2v_testModel.predict_proba(x_test_onehot_avg_w2v)[:,1]\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\nax = plt.subplot()\n\nauc_set3_train=auc(train_fpr, train_tpr)\nauc_set3_test=auc(test_fpr, test_tpr)\n\n# plt.plot(train_fpr, train_tpr, label=\"Train AUC =\"+str(auc(train_fpr, train_tpr)))\n# plt.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(auc(test_fpr, test_tpr)))\nax.plot(train_fpr, train_tpr, label=\"Train AUC =\"+str(auc(train_fpr, train_tpr)))\nax.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(auc(test_fpr, test_tpr)))\n\nplt.legend()\nplt.xlabel(\"False Positive Rate(FPR)\")\nplt.ylabel(\"True Positive Rate(TPR)\")\nplt.title(\"AUC\")\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax.set_facecolor(\"white\")\nplt.show()\nprint(type(y_test_pred))\nprint(type(tr_thresholds))\nprint(type(test_fpr))\nprint(type(test_tpr))\nprint(y_test_pred.shape)\nprint(tr_thresholds.shape)\nprint(test_fpr.shape)\nprint(test_tpr.shape)","d23b2a54":"def predict(proba, threshould, fpr, tpr):\n    t = threshould[np.argmax(tpr*(1-fpr))]\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    predictions = []\n    for i in proba:\n        if i>=t:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","45e2f423":"## TRAIN\nprint(\"=\"*100)\nfrom sklearn.metrics import confusion_matrix\nprint(\"Train confusion matrix\")\nprint(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds, train_fpr, train_tpr)))\n\nconf_matr_df_train = pd.DataFrame(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds,train_fpr, train_tpr)), range(2),range(2))\n\n## Heatmaps -> https:\/\/likegeeks.com\/seaborn-heatmap-tutorial\/\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_train, annot=True,annot_kws={\"size\": 26}, fmt='g',cmap=\"YlGnBu\")","f9b5b995":"## TEST\nprint(\"=\"*100)\nprint(\"Test confusion matrix\")\nprint(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)))\n\nconf_matr_df_test = pd.DataFrame(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_test, annot=True,annot_kws={\"size\": 16}, fmt='g')","772cdf0a":"from sklearn.model_selection import GridSearchCV\nlogreg_tfidf_w2v = LogisticRegression(class_weight='balanced')\n# parameters = {'lambda':[0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 2500, 5000, 10000]}\nparameters = {'C':[0.0001,0.001,0.01,0.1,1,10,100,1000,10000]}\nclf = GridSearchCV(logreg_tfidf_w2v, parameters, cv= 3, scoring='roc_auc',verbose=1,return_train_score=True,n_jobs=-1)\n# clf.fit(x_cv_onehot_bow, y_cv)\nclf.fit(x_train_onehot_tfidf_w2v,y_train)\ntrain_auc= clf.cv_results_['mean_train_score']\ntrain_auc_std= clf.cv_results_['std_train_score']\ncv_auc = clf.cv_results_['mean_test_score']\ncv_auc_std= clf.cv_results_['std_test_score']\nbestC_4=clf.best_params_['C']\nbestScore_4=clf.best_score_\nprint(\"BEST C: \",clf.best_params_['C'],\" BEST SCORE: \",clf.best_score_) #clf.best_estimator_.alpha","c3d7c4b0":"# alphas = [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 2500, 5000, 10000]\nalphas = [0.0001,0.001,0.01,0.1,1,10,100,1000,10000]\nlog_alphas =[]\nfor a in tqdm(alphas):\n#     b = math.log(a)\n    b = np.log10(a)\n    log_alphas.append(b)\nplt.figure(figsize=(10,5))\nplt.plot(log_alphas, train_auc, label='Train AUC')\n# this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\nplt.gca().fill_between(log_alphas,train_auc - train_auc_std,train_auc + train_auc_std,alpha=0.3,color='darkblue')\nplt.plot(log_alphas, cv_auc, label='CV AUC')\n# this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\nplt.gca().fill_between(log_alphas,cv_auc - cv_auc_std,cv_auc + cv_auc_std,alpha=0.3,color='darkorange')\n\nax = plt.subplot()\nax.scatter(log_alphas, train_auc, label='Train AUC points')\nax.scatter(log_alphas, cv_auc, label='CV AUC points')\nplt.legend()\nplt.xlabel(\"alpha: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"alpha: hyperparameter v\/s AUC\")\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax.set_facecolor(\"white\")\nplt.show()","df800fed":"#https:\/\/scikitlearn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\nlogref_tfidf_w2v_testModel = LogisticRegression(class_weight='balanced',C=bestC_4)\nlogref_tfidf_w2v_testModel.fit(x_train_onehot_tfidf_w2v, y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n# y_train_pred = batch_predict(mnb_tfidf_testModel, x_train_onehot_tfidf)\n# y_test_pred = batch_predict(mnb_tfidf_testModel, x_test_onehot_tfidf)\ny_train_pred=logref_tfidf_w2v_testModel.predict_proba(x_train_onehot_tfidf_w2v)[:,1]\ny_test_pred=logref_tfidf_w2v_testModel.predict_proba(x_test_onehot_tfidf_w2v)[:,1]\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\nax = plt.subplot()\n\nauc_set4_train=auc(train_fpr, train_tpr)\nauc_set4_test=auc(test_fpr, test_tpr)\n\n# plt.plot(train_fpr, train_tpr, label=\"Train AUC =\"+str(auc(train_fpr, train_tpr)))\n# plt.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(auc(test_fpr, test_tpr)))\nax.plot(train_fpr, train_tpr, label=\"Train AUC =\"+str(auc(train_fpr, train_tpr)))\nax.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(auc(test_fpr, test_tpr)))\n\nplt.legend()\nplt.xlabel(\"False Positive Rate(FPR)\")\nplt.ylabel(\"True Positive Rate(TPR)\")\nplt.title(\"AUC\")\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax.set_facecolor(\"white\")\nplt.show()\nprint(type(y_test_pred))\nprint(type(tr_thresholds))\nprint(type(test_fpr))\nprint(type(test_tpr))\nprint(y_test_pred.shape)\nprint(tr_thresholds.shape)\nprint(test_fpr.shape)\nprint(test_tpr.shape)","e30e337a":"print(type(y_test_pred))\nprint(type(tr_thresholds))\nprint(type(test_fpr))\nprint(type(test_tpr))\nprint(y_test_pred.shape)\nprint(tr_thresholds.shape)\nprint(test_fpr.shape)\nprint(test_tpr.shape)","779479a1":"def predict(proba, threshould, fpr, tpr):\n    print(\"--------------------->\",threshould)\n    t = threshould[np.argmax(tpr*(1-fpr))]\n    print(type(proba))\n    print(type(threshould))\n    print(type(fpr))\n    print(type(tpr))\n    \n    print(\"Shape\",proba.shape)\n    print(\"Shape\",threshould.shape)\n    print(\"Shape\",fpr.shape)\n    print(\"Shape\",tpr.shape)\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    predictions = []\n    \n    for i in proba:\n        if i>=t:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","b4d009e4":"## TRAIN\nprint(\"=\"*100)\nfrom sklearn.metrics import confusion_matrix\nprint(\"Train confusion matrix\")\nprint(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds, train_fpr, train_tpr)))\n\nconf_matr_df_train = pd.DataFrame(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds,train_fpr, train_tpr)), range(2),range(2))\n\n## Heatmaps -> https:\/\/likegeeks.com\/seaborn-heatmap-tutorial\/\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_train, annot=True,annot_kws={\"size\": 26}, fmt='g',cmap=\"YlGnBu\")","06fc2fb2":"print(type(y_test_pred))\nprint(type(tr_thresholds))\nprint(type(test_fpr))\nprint(type(test_tpr))\nprint(y_test_pred.shape)\nprint(tr_thresholds.shape)\nprint(test_fpr.shape)\nprint(test_tpr.shape)","2d3def5b":"## TEST\nprint(\"=\"*100)\nprint(\"Test confusion matrix\")\nprint(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)))\n\nconf_matr_df_test = pd.DataFrame(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_test, annot=True,annot_kws={\"size\": 16}, fmt='g')","191b7c1f":"import nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# nltk.download('vader_lexicon')\n\nsid = SentimentIntensityAnalyzer()\n\nessays = x_train['essay']\nessays_sentiments = []\n\nfor essay in tqdm(essays):\n    res = sid.polarity_scores(essay)\n    essays_sentiments.append(res['compound']) #Considering compound as a criteria.\n\nx_train['essay_sentiment_train'] = essays_sentiments","77e2f1bf":"import nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# nltk.download('vader_lexicon')\n\nsid = SentimentIntensityAnalyzer()\n\nessays_test = x_test['essay']\nessays_sentiments = []\n\nfor essay in tqdm(essays_test):\n    res = sid.polarity_scores(essay)\n    essays_sentiments.append(res['compound']) #Considering compound as a criteria.\n\nx_test['essay_sentiment_test'] = essays_sentiments","a7863ea5":"sentiment_test=x_test['essay_sentiment_test'].values.reshape(-1,1)\nsentiment_train=x_train['essay_sentiment_train'].values.reshape(-1,1)","32b08085":"from scipy.sparse import hstack\n\nx_train_onehot_noText = hstack((x_train_school_state_one_hot, x_train_categories_one_hot, x_train_sub_categories_one_hot, x_train_grade_category_one_hot, x_train_prefix_one_hot,x_train_quantity_normalized,x_train_teacher_previous_proj_normalized, x_train_price_normalized, sentiment_train,x_train_title_normalized,x_train_essay_normalized))\n# x_cv_onehot    = hstack((x_cv_categories_one_hot, x_cv_sub_categories_one_hot, x_cv_school_state_one_hot, x_cv_prefix_one_hot, x_cv_grade_category_one_hot,x_cv_price_normalized, x_cv_teacher_previous_proj_normalized ))\nx_test_onehot_noText  = hstack((x_test_school_state_one_hot, x_test_categories_one_hot, x_test_sub_categories_one_hot, x_test_grade_category_one_hot, x_test_prefix_one_hot,x_test_quantity_normalized,x_test_teacher_previous_proj_normalized, x_test_price_normalized, sentiment_test,x_test_title_normalized,x_test_essay_normalized))\n\nprint(\"Type -> One Hot -> x_train: \",type(x_train_onehot_noText))\nprint(\"Type -> One Hot -> x_test : \",type(x_test_onehot_noText))\n# print(\"Type -> One Hot -> x_cv        : \",type(x_cv_onehot))\nprint(\"\\n\")\nprint(\"Shape -> One Hot -> x_train: \",x_train_onehot_noText.shape)\nprint(\"Shape -> One Hot -> x_test : \",x_test_onehot_noText.shape)\n# print(\"Shape -> One Hot -> x_cv         : \",x_cv_onehot.shape)","0ac201b1":"from sklearn.model_selection import GridSearchCV\nlogreg_noText = LogisticRegression(class_weight='balanced')\n# parameters = {'lambda':[0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 2500, 5000, 10000]}\nparameters = {'C':[0.0001,0.001,0.01,0.1,1,10,100,1000,10000]}\nclf = GridSearchCV(logreg_noText, parameters, cv= 3, scoring='roc_auc',verbose=1,return_train_score=True,n_jobs=-1)\n# clf.fit(x_cv_onehot_bow, y_cv)\nclf.fit(x_train_onehot_noText,y_train)\ntrain_auc= clf.cv_results_['mean_train_score']\ntrain_auc_std= clf.cv_results_['std_train_score']\ncv_auc = clf.cv_results_['mean_test_score']\ncv_auc_std= clf.cv_results_['std_test_score']\nbestC_5=clf.best_params_['C']\nbestScore_5=clf.best_score_\nprint(\"BEST C: \",clf.best_params_['C'],\" BEST SCORE: \",clf.best_score_) #clf.best_estimator_.alpha","48f40b07":"# alphas = [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 2500, 5000, 10000]\nalphas = [0.0001,0.001,0.01,0.1,1,10,100,1000,10000]\nlog_alphas =[]\nfor a in tqdm(alphas):\n#     b = math.log(a)\n    b = np.log10(a)\n    log_alphas.append(b)\nplt.figure(figsize=(10,5))\nplt.plot(log_alphas, train_auc, label='Train AUC')\n# this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\nplt.gca().fill_between(log_alphas,train_auc - train_auc_std,train_auc + train_auc_std,alpha=0.3,color='darkblue')\nplt.plot(log_alphas, cv_auc, label='CV AUC')\n# this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\nplt.gca().fill_between(log_alphas,cv_auc - cv_auc_std,cv_auc + cv_auc_std,alpha=0.3,color='darkorange')\n\nax = plt.subplot()\nax.scatter(log_alphas, train_auc, label='Train AUC points')\nax.scatter(log_alphas, cv_auc, label='CV AUC points')\nplt.legend()\nplt.xlabel(\"alpha: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"alpha: hyperparameter v\/s AUC\")\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax.set_facecolor(\"white\")\nplt.show()","56310f56":"#https:\/\/scikitlearn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\nlogref_noText_testModel = LogisticRegression(class_weight='balanced',C=bestC_5)\nlogref_noText_testModel.fit(x_train_onehot_noText, y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n# y_train_pred = batch_predict(mnb_tfidf_testModel, x_train_onehot_tfidf)\n# y_test_pred = batch_predict(mnb_tfidf_testModel, x_test_onehot_tfidf)\ny_train_pred=logref_noText_testModel.predict_proba(x_train_onehot_noText)[:,1]\ny_test_pred=logref_noText_testModel.predict_proba(x_test_onehot_noText)[:,1]\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\nax = plt.subplot()\n\nauc_set5_train=auc(train_fpr, train_tpr)\nauc_set5_test=auc(test_fpr, test_tpr)\n\n# plt.plot(train_fpr, train_tpr, label=\"Train AUC =\"+str(auc(train_fpr, train_tpr)))\n# plt.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(auc(test_fpr, test_tpr)))\nax.plot(train_fpr, train_tpr, label=\"Train AUC =\"+str(auc(train_fpr, train_tpr)))\nax.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(auc(test_fpr, test_tpr)))\n\nplt.legend()\nplt.xlabel(\"False Positive Rate(FPR)\")\nplt.ylabel(\"True Positive Rate(TPR)\")\nplt.title(\"AUC\")\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax.set_facecolor(\"white\")\nplt.show()","be312573":"def predict(proba, threshould, fpr, tpr):\n    t = threshould[np.argmax(tpr*(1-fpr))]\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    predictions = []\n    for i in proba:\n        if i>=t:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","8068d06e":"## TRAIN\nprint(\"=\"*100)\nfrom sklearn.metrics import confusion_matrix\nprint(\"Train confusion matrix\")\nprint(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds, train_fpr, train_tpr)))\n\nconf_matr_df_train = pd.DataFrame(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds,train_fpr, train_tpr)), range(2),range(2))\n\n## Heatmaps -> https:\/\/likegeeks.com\/seaborn-heatmap-tutorial\/\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_train, annot=True,annot_kws={\"size\": 26}, fmt='g',cmap=\"YlGnBu\")","bde9ff1a":"## TEST\nprint(\"=\"*100)\nprint(\"Test confusion matrix\")\nprint(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)))\n\nconf_matr_df_test = pd.DataFrame(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_test, annot=True,annot_kws={\"size\": 16}, fmt='g')","852ba1e6":"from prettytable import PrettyTable\n    \nx = PrettyTable()\n\nx.field_names = [\"Vectorizer\", \"Model\", \"Hyperparameter: C\", \"Train AUC\", \"Test AUC\"]\n# auc_set2_train=auc(train_fpr, train_tpr)\n# auc_set2_test=auc(test_fpr, test_tpr)\n\nx.add_row([\"BOW\", \"Logistic Regression\", bestC_1, round(auc_set1_train,2),round(auc_set1_test,2)])\nx.add_row([\"TF-IDF\", \"Logistic Regression\", bestC_2, round(auc_set2_train,2),round(auc_set2_test,2)])\nx.add_row([\"Avg W2V\", \"Logistic Regression\", bestC_3, round(auc_set3_train,2),round(auc_set3_test,2)])\nx.add_row([\"TFIDF W2V\", \"Logistic Regression\", bestC_4, round(auc_set4_train,2),round(auc_set4_test,2)])\nx.add_row([\"All non Text\", \"Logistic Regression\", bestC_5, round(auc_set5_train,2),round(auc_set5_test,2)])\n\nprint(x)","19a52419":"# ->-> 6.3.1: BOW","d2cdf45e":"# -> 8.3:<font color='red'> SET 3<\/font>  Applying Logistic Regression on AvgW2V.","c791bbb7":"**Query 1.1: PreProcessing Teacher Prefix Done <br>\nAction Taken: Removed '.' from the prefixes and converted to lower case**","03fecd57":"## -> 3.1: Text Preprocessing: Essays","4d381c77":"# -> 8.1:<font color='red'> SET 1<\/font>  Applying Logistic Regresion on BOW (Set 1).","214a645c":"\n## ->-> 8.4.2: <font color='red'> SET 4<\/font> TESTING the performance of the model on test data, plotting ROC Curves.","317a3d6f":"### ->->-> 8.5.3.1: <font color='red'> SET 5<\/font> Confusion Matrix: Train","6d5405c5":"# -> 8.2:<font color='red'> SET 2<\/font>  Applying Logistic Regression on TFIDF.","b5fb320a":"## -> 7.3: SET 2:  Merging All ONE HOT with TF-IDF (Title and Essay) features","c1589e0a":"###   ->->-> 6.3.4.1. B: TF-IDF W2V: Essays -> CV","2df0d140":"### ->->-> 8.1.3.1: <font color='red'> SET 1<\/font> Confusion Matrix: Train","04b491f0":"# -> 6.3: VECTORIZING TEXT DATA","e25a96af":"# -> 6.2: VECTORIZING NUMERICAL DATA","85fb0db7":"# 3. TEXT PROCESSING","c1ef1a8e":"\n## ->-> 8.2.1: <font color='red'> SET 2<\/font> Hyper parameter tuning to find best 'C' using GRIDSEARCHCV","4b8c08a1":"### ->->-> 8.2.3.1: <font color='red'> SET 2<\/font> Confusion Matrix: Train","f9e68ebf":"### ->->-> 8.1.3.2: <font color='red'> SET 1<\/font> Confusion Matrix: Test","6207bca9":"## ->-> 6.1.3 Vectorizing Categorical data: School State","aeb99a68":"# -> 8.4:<font color='red'> SET 4<\/font>  Applying Logistic Regression on TFIDF_W2V.","b8d6afe1":"# 2: PRE-PROCESSING","2e74054a":"# 8. LOGISTIC REGRESSION\n","47e5e4ee":"## -> 7.4: SET 3:  Merging All ONE HOT with AVG W2V (Title and Essay) features","0a0a2635":"\n## ->-> 8.4.1: <font color='red'> SET 4<\/font> Hyper parameter tuning to find best 'C' using GRIDSEARCHCV","7b58e1fb":"#### Checking total number of enteries with NaN values","331ba861":"## ->-> 6.2.5: Normalizing Numerical data: Quantity","be3fd3dd":"## Adding Quantity in the dataset","c361429a":"According to Andrew Ng, in the Coursera MOOC on Introduction to Machine Learning, the general rule of thumb is to partition the data set into the ratio of ***3:1:1 (60:20:20)*** for training, validation and testing respectively.","5eba5a2d":"## -> 3.3: Calculating the size of Title and Essay","8e2859f0":"## -> 3.2: Text Preprocessing: Title","20a3291a":"## Merging all the non text features.","acf1efa3":"**Query 1.2: PreProcessing Project Grade Done <br>\nAction Taken: Removed ' ' and '-' from the grades and converted to lower case**","874baced":"## -> 1.1: REMOVING NaN:<br>\n**As it is clearly metioned in the dataset details that TEACHER_PREFIX has NaN values, we need to handle this at the very beginning to avoid any problems in our future analysis.**","b6c108cd":"## ->-> 6.1.1: Vectorizing Categorical data: Clean Subject Categories","e9375877":"## -> 7.2: SET 1:  Merging All ONE HOT with BOW (Title and Essay) features","b7b18c24":"Reference: https:\/\/elitedatascience.com\/imbalanced-classes","870c92b4":"- https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/handling-categorical-and-numerical-features\/","c7144662":"# 9. CONCLUSION","8e34d5d8":"# 4. SAMPLING\n## -> 4.1: Taking Sample from the complete dataset\n## NOTE: A sample of 10000 Datapoints is taken due to lack computational resource.","e3148ca5":"### ->->-> 8.4.3.2: <font color='red'> SET 4<\/font> Confusion Matrix: Test","b8ed7f5d":"### ->->-> 8.1.3.2: <font color='red'> SET 2<\/font> Confusion Matrix: Test","88fe4731":"## ->-> 6.3.4.1: ESSAYS","a3bd9472":"### -> 4.3: Details of our Training, CV and Test datasets.","e882fdfa":"## ->->-> 6.3.2.1: TF-IDF: Essays (Train, CV, Test)","38f8d54b":"\n## ->-> 8.5.2: <font color='red'> SET 5<\/font> TESTING the performance of the model on test data, plotting ROC Curves.","10adba44":"**Observation:**\n1. The proportion of Majority class of 85% and Minority class of 15% is maintained in Training, CV and Testing dataset.","96eee8dc":"# B. OBJECTIVE\nThe primary objective is to implement the k-Nearest Neighbor Algo on the DonorChoose Dataset and measure the accuracy on the Test dataset.","ce02200c":"# -> 6.1: VECTORIZING CATEGORICAL DATA","82606095":"## ->-> 6.1.4 Vectorizing Categorical data: Teacher Prefix","b91b8a3d":"\n## ->-> 8.1.2: <font color='red'> SET 1<\/font> TESTING the performance of the model on test data, plotting ROC Curves.","f2ebeeaa":"### -> -> 4.2.2: Splitting the TRAINING Dataset further into Training and CV sets.","bc91072f":"###  ->->->6.3.3.1. C: Avg W2V: Essays -> Test","aa9dfddf":"\n## ->-> 8.5.3: <font color='red'> SET 5<\/font> Confusion Matrix","6b20df36":"# 6. PREPARING DATA FOR MODELS","b4f5d20f":"###   ->->-> 6.3.4.2. A: TF-IDF W2V: Title -> Train","31f5a7f5":"# Performing sentiment analysis","79e7e207":"## A. DATA INFORMATION \n### <br>About the DonorsChoose Data Set\n\nThe `train.csv` data set provided by DonorsChoose contains the following features:\n\nFeature | Description \n----------|---------------\n**`project_id`** | A unique identifier for the proposed project. **Example:** `p036502`   \n**`project_title`**    | Title of the project. **Examples:**<br><ul><li><code>Art Will Make You Happy!<\/code><\/li><li><code>First Grade Fun<\/code><\/li><\/ul> \n**`project_grade_category`** | Grade level of students for which the project is targeted. One of the following enumerated values: <br\/><ul><li><code>Grades PreK-2<\/code><\/li><li><code>Grades 3-5<\/code><\/li><li><code>Grades 6-8<\/code><\/li><li><code>Grades 9-12<\/code><\/li><\/ul>  \n **`project_subject_categories`** | One or more (comma-separated) subject categories for the project from the following enumerated list of values:  <br\/><ul><li><code>Applied Learning<\/code><\/li><li><code>Care &amp; Hunger<\/code><\/li><li><code>Health &amp; Sports<\/code><\/li><li><code>History &amp; Civics<\/code><\/li><li><code>Literacy &amp; Language<\/code><\/li><li><code>Math &amp; Science<\/code><\/li><li><code>Music &amp; The Arts<\/code><\/li><li><code>Special Needs<\/code><\/li><li><code>Warmth<\/code><\/li><\/ul><br\/> **Examples:** <br\/><ul><li><code>Music &amp; The Arts<\/code><\/li><li><code>Literacy &amp; Language, Math &amp; Science<\/code><\/li>  \n  **`school_state`** | State where school is located ([Two-letter U.S. postal code](https:\/\/en.wikipedia.org\/wiki\/List_of_U.S._state_abbreviations#Postal_codes)). **Example:** `WY`\n**`project_subject_subcategories`** | One or more (comma-separated) subject subcategories for the project. **Examples:** <br\/><ul><li><code>Literacy<\/code><\/li><li><code>Literature &amp; Writing, Social Sciences<\/code><\/li><\/ul> \n**`project_resource_summary`** | An explanation of the resources needed for the project. **Example:** <br\/><ul><li><code>My students need hands on literacy materials to manage sensory needs!<\/code<\/li><\/ul> \n**`project_essay_1`**    | First application essay<sup>*<\/sup>  \n**`project_essay_2`**    | Second application essay<sup>*<\/sup> \n**`project_essay_3`**    | Third application essay<sup>*<\/sup> \n**`project_essay_4`**    | Fourth application essay<sup>*<\/sup> \n**`project_submitted_datetime`** | Datetime when project application was submitted. **Example:** `2016-04-28 12:43:56.245`   \n**`teacher_id`** | A unique identifier for the teacher of the proposed project. **Example:** `bdf8baa8fedef6bfeec7ae4ff1c15c56`  \n**`teacher_prefix`** | Teacher's title. One of the following enumerated values: <br\/><ul><li><code>nan<\/code><\/li><li><code>Dr.<\/code><\/li><li><code>Mr.<\/code><\/li><li><code>Mrs.<\/code><\/li><li><code>Ms.<\/code><\/li><li><code>Teacher.<\/code><\/li><\/ul>  \n**`teacher_number_of_previously_posted_projects`** | Number of project applications previously submitted by the same teacher. **Example:** `2` \n\n<sup>*<\/sup> See the section <b>Notes on the Essay Data<\/b> for more details about these features.\n\nAdditionally, the `resources.csv` data set provides more data about the resources required for each project. Each line in this file represents a resource required by a project:\n\nFeature | Description \n----------|---------------\n**`id`** | A `project_id` value from the `train.csv` file.  **Example:** `p036502`   \n**`description`** | Desciption of the resource. **Example:** `Tenor Saxophone Reeds, Box of 25`   \n**`quantity`** | Quantity of the resource required. **Example:** `3`   \n**`price`** | Price of the resource required. **Example:** `9.95`   \n\n**Note:** Many projects require multiple resources. The `id` value corresponds to a `project_id` in train.csv, so you use it as a key to retrieve all resources needed for a project:\n\nThe data set contains the following label (the value you will attempt to predict):\n\nLabel | Description\n----------|---------------\n`project_is_approved` | A binary flag indicating whether DonorsChoose approved the project. A value of `0` indicates the project was not approved, and a value of `1` indicates the project was approved.","15f297f2":"\n## ->-> 8.4.3: <font color='red'> SET 4<\/font> Confusion Matrix","988e7bee":"**Conlusion**\n1. **UPSAMPLING** needs to be done on the Minority class to avoid problems related to Imbalanced dataset.\n1. Upsampling will be done by _**\"Resample with replacement strategy\"**_","53a0f2db":"\n## ->-> 8.2.3: <font color='red'> SET 2<\/font> Confusion Matrix","791980bf":"As we can observe, the Set 5 which had all non text features, has performed sufficiently well in Test AUC and the lowest in Train AUC. Since is a good indication that the model is not overfitted and has performed in a balanced way in Training and Testing. The gap between the Training AUC and Testing AUC is very minimal which is good sign of performance. Thus this is an interesting observation as can infer that we can obtain good result in Testing even without including the text data.","d7fabb12":"### ->->-> 8.5.3.2: <font color='red'> SET 5<\/font> Confusion Matrix: Test","e2311626":"### ->->-> 8.3.3.1: <font color='red'> SET 3<\/font> Confusion Matrix: Train","2556b5a5":"###   ->->-> 6.3.4.2. B: TF-IDF W2V: Title -> CV","68dd1c4b":"## ->-> 6.1.5 Vectorizing Categorical data: Project Grade","a9459163":"**Conclusion:** Now the number of rows reduced from 109248 to 109245 in project_data.","df12f0aa":"#### -> Merging Price with Project Data","abe483c4":"## -> 7.1: Merging all ONE HOT features","21857789":"## ->-> 6.3.3.1: Avg W2v: ESSAYS","6acfcf97":"\n## ->-> 8.3.2: <font color='red'> SET 3<\/font> TESTING the performance of the model on test data, plotting ROC Curves.","7e41104b":"###   ->->->6.3.3.2. B: Avg W2V: Title -> CV","27383ce0":"###  ->->->6.3.3.1. B:  Avg W2V: Essays -> CV","6cff3dd9":"**Observation:** 3 Rows had NaN values and they are not considered, thus the number of rows reduced from 109248 to 109245. <br>\n**Action:** We can safely delete these, as 3 is very very small and its deletion wont impact as the original dataset is very large. <br>\nStep1: Convert all the empty strings with Nan \/\/ Not required as its NaN not empty string <br> \nStep2: Drop rows having NaN values","936fb81b":"# 5. UPSAMPLING THE MINORITY CLASS WITH REPLACEMENT STRATEGY: NOT DONE ","2bdb4a4b":"\n## ->-> 8.3.3: <font color='red'> SET 3<\/font> Confusion Matrix","16703245":"## -> 7.5: SET 4:  Merging All ONE HOT with TF-IDF W2V (Title and Essay) features","53baac59":"## ->-> 6.3.4.2 TITLE","8624d2ee":"### ->->-> 8.3.3.2: <font color='red'> SET 3<\/font> Confusion Matrix: Test","40535d3c":"## ->-> 6.3.3.2: TITLE","7183c6a9":"**Teacher Prefix has NAN values, that needs to be cleaned.\nRef: https:\/\/stackoverflow.com\/a\/50297200\/4433839**","03b6b29c":"## -> 2.2: Preprocessing: Project Subject Sub Categories","aa289aa0":"## ->-> 6.2.3: Normalizing Numerical data: Title Size","6943c507":"### ->->-> 8.4.3.1: <font color='red'> SET 4<\/font> Confusion Matrix: Train","555fe917":"**Conclusion:**\n1. Resampling is performed on the Training data.\n1. Training data in now **BALANCED**.","43cd68ce":"## -> 4.2: Splitting the dataset into Train, CV and Test datasets. (60:20:20)","c56d94e0":"## ->-> 6.1.2: Vectorizing Categorical data: Clean Subject Sub-Categories","72276c7a":"## ->->-> 6.3.2.2: TF-IDF: Title (Train, CV, Test)","8f7a0d0e":"**Observation:**\n1. Dataset is highly **IMBALANCED**.\n1. Approved Class (1) is the Majority class. And the Majority class portion in our sampled dataset: ~85%\n1. Unapproved class (0) is the Minority class. And the Minority class portion in our sampled dataset: ~15%","7ed575e6":"###   ->->-> 6.3.4.2. C: TF-IDF W2V: Title -> Test","34db3c4e":"###  ->->->6.3.3.1. A:  Avg W2V: Essays -> Train","2146a6a5":"\n## ->-> 8.5.1: <font color='red'> SET 5<\/font> Hyper parameter tuning to find best 'C' using GRIDSEARCHCV","b8dc3402":"###   ->->->6.3.3.2. C: Avg W2V: Title -> Test","5e73f13f":"## -> 2.3: Preprocessing: Project Grade Category","445b8bdb":"## ->-> 6.2.4: Normalizing Numerical data: Essay Size","90d718b7":"#### Merging Project Essays 1 2 3 4 into Essays","01069bda":"## 1. READING DATA","c6b18805":"## ->-> 6.2.2: Normalizing Numerical data: Teacher's Previous Projects","471c1334":"\n## ->-> 8.3.1: <font color='red'> SET 3<\/font> Hyper parameter tuning to find best 'C' using GRIDSEARCHCV","f126837e":"# ->6.3.4: TFIDF - W2V","38ce4a83":"###   ->->-> 6.3.4.1. A: TF-IDF W2V: Essays -> Train","e00a91ed":"###   ->->->6.3.3.2. A: Avg W2V: Title -> Train","f0564192":"\n## ->-> 8.2.2: <font color='red'> SET 2<\/font> TESTING the performance of the model on test data, plotting ROC Curves.","200bc957":"# -> 8.5:<font color='red'> SET 5<\/font>  Applying Logistic Regression on ALL EXCEPT TEXT.","3e8714d0":"## ->->-> 6.3.1.2: BOW: Title (Train, CV, Test)","efcb289f":"# ->-> 6.3.2: TF-IDF","13b02351":"# Logistic Regression: DonorsChoose Dataset\n\n### A. DATA INFORMATION\n### B. OBJECTIVE\n## 1. READING THE DATASET\n- **1.1 Removing Nan**\n- **1.2 Adding Quantity in Dataset**\n\n## 2. PREPROCESSING \n- **2.1 Preprocessing: Project Subject Categories**\n- **2.2 Preprocessing: Project Subject Sub Categories**\n- **2.3 Preprocessing: Project Grade**\n\n## 3. TEXT PROCESSING\n- **3.1 Text Preprocessing: Essays**\n- **3.2 Text Preprocessing: Title**\n- **3.3 Text Preprocessing: Calculating Size of Title and Essay**\n\n## 4. SAMPLING\n- **4.1 Taking Sample from the complete dataset.**\n- **4.2 Splitting the dataset into Train, CV and Test datasets. (60:20:20)**\n    - 4.2.1: Splitting the FULL DATASET into Training and Test sets. (80:20)\n    - 4.2.2: Splitting the TRAINING Dataset further into Training and CV sets. (Not required  as we are using GridSearch)\n- **4.3 Details of our Training, CV and Test datasets.**\n\n## 5. UPSAMPLING THE MINORITY CLASS WITH REPLACEMENT STRATEGY (Not required  as we are using GridSearch)\n\n## 6. PREPARING DATA FOR MODELS\n- **6.1: VECTORIZING CATEGORICAL DATA**\n    - 6.1.1: Vectorizing Categorical data: Clean Subject Categories.\n    - 6.1.2: Vectorizing Categorical data: Clean Subject Sub-Categories.\n    - 6.1.3: Vectorizing Categorical data: School State.\n    - 6.1.4: Vectorizing Categorical data: Teacher Prefix.\n    - 6.1.5: Vectorizing Categorical data: Project Grade\n        \n- **6.2: VECTORIZING NUMERICAL DATA**\n    - 6.2.1: Standarizing Numerical data: Price\n    - 6.2.2: Standarizing Numerical data: Teacher's Previous Projects\n    - 6.2.3: Standarizing Numerical data: Title Size\n    - 6.2.4: Standarizing Numerical data: Essay Size\n    - 6.2.5: Standarizing Numerical data: Quantity\n    \n        \n- **6.3: VECTORIZING TEXT DATA**\n    - **6.3.1: BOW**\n        - 6.3.1.1: BOW: Essays (Train, CV, Test)\n        - 6.3.1.2: BOW: Title (Train, CV, Test)\n            \n    - **6.3.2: TF-IDF**\n        - 6.3.2.1: TF-IDF: Essays (Train, CV, Test)\n        - 6.3.2.2: TF-IDF: Title (Train, CV, Test)\n        \n    - **6.3.3: AVG Word2VecF**\n        - 6.3.3.1: Avg Word2Vec: Essays (A. Train, B. CV, C. Test)\n        - 6.3.3.2: Avg Word2Vec: Title (A. Train, B. CV, C. Test)\n    - **6.3.4: TF-IDF Word2VecF**\n        - 6.3.4.1: TF-IDF Word2Vec: Essays (A. Train, B. CV, C. Test)\n        - 6.3.4.2: TF-IDF Word2Vec: Title (A. Train, B. CV, C. Test)\n                \n## 7. MERGING FEATURES\n- 7.1: Merging all ONE HOT features.\n- 7.2: SET 1: Merging All ONE HOT with BOW (Title and Essay) features.\n- 7.3: SET 2: Merging All ONE HOT with TF-IDF (Title and Essay) features.\n- 7.3: SET 3: Merging All ONE HOT with Avg W2v (Title and Essay) features.\n- 7.4: SET 4: Merging All ONE HOT with TF-IDF W2v (Title and Essay) features.\n    \n## 8. LOGISTIC REGRESSION\n- **8.1: SET 1 Applying Logistic Regression on BOW.**\n    - 8.1.1: SET 1 Hyper parameter tuning to find the best \"C\".\n    - 8.1.2: SET 1 TESTING the performance of the model on test data, plotting ROC Curves.\n    - 8.1.3: SET 1 Confusion Matrix\n        - 8.1.3.1: SET 1 Confusion Matrix: Train\n        - 8.1.3.2: SET 1 Confusion Matrix: Test\n            \n- **8.2: SET 2 Applying Logistic Regression on TF-IDF.**\n    - 8.2.1: SET 2 Hyper parameter tuning to find the best \"C\".\n    - 8.2.2: SET 2 TESTING the performance of the model on test data, plotting ROC Curves.\n    - 8.2.3: SET 2 Confusion Matrix\n        - 8.2.3.1: SET 2 Confusion Matrix: Train\n        - 8.2.3.2: SET 2 Confusion Matrix: Test\n        \n- **8.3: SET 3 Applying Logistic Regression on Avg W2V.**\n    - 8.3.1: SET 3 Hyper parameter tuning to find the best \"C\".\n    - 8.3.2: SET 3 TESTING the performance of the model on test data, plotting ROC Curves.\n    - 8.3.3: SET 3 Confusion Matrix\n        - 8.3.3.1: SET 3 Confusion Matrix: Train\n        - 8.3.3.2: SET 3 Confusion Matrix: Test\n        \n- **8.4: SET 4 Applying Logistic Regression on TF-IDF W2V.**\n    - 8.4.1: SET 4 Hyper parameter tuning to find the best \"C\".\n    - 8.4.2: SET 4 TESTING the performance of the model on test data, plotting ROC Curves.\n    - 8.4.3: SET 4 Confusion Matrix\n        - 8.4.3.1: SET 4 Confusion Matrix: Train\n        - 8.4.3.2: SET 4 Confusion Matrix: Test\n        \n- **8.5: SET 5 Applying Logistic Regression on ALL (Expect Text Features)TF-IDF W2V.**\n    - **Performing Sentiment Analysis**\n    - **Merging All the non text features**\n    - 8.5.1: SET 5 Hyper parameter tuning to find the best \"C\".\n    - 8.5.2: SET 5 TESTING the performance of the model on test data, plotting ROC Curves.\n    - 8.5.3: SET 5 Confusion Matrix\n        - 8.5.3.1: SET 5 Confusion Matrix: Train\n        - 8.5.3.2: SET 5 Confusion Matrix: Test\n\n\n## 9. CONCLUSION ","d487aeb7":"## ->->-> 6.3.1.1: BOW: Essays (Train, CV, Test)","1e548d2b":"\n## ->-> 8.1.1: <font color='red'> SET 1<\/font> Hyper parameter tuning to find best 'C' using GRIDSEARCHCV","73ba4e01":"# 7. MERGING FEATURES","421b6082":"## -> 2.1: Preprocessing: Project Subject Categories","a3e22e4a":"# -> 6.3.3:  AVG Word2Vec","13cd8e20":"###   ->->-> 6.3.4.1. C: TF-IDF W2V: Essays -> Test","ec7951d5":"## ->-> 6.2.1: Normalizing Numerical data: Price","706f3d32":"**Observation:** \n1. 3 Rows had NaN values and they are not considered, thus the number of rows reduced from 109248 to 109245.","d68786d8":"###    -> -> 4.2.1: Splitting the FULL DATASET into Training and Test sets. (80:20)","6ad23efd":"\n## ->-> 8.1.3: <font color='red'> SET 1<\/font> Confusion Matrix","227a0532":"we are going to consider\n\n       - school_state : categorical data\n       - clean_categories : categorical data\n       - clean_subcategories : categorical data\n       - project_grade_category : categorical data\n       - teacher_prefix : categorical data\n       \n       - project_title : text data\n       - text : text data\n       - project_resource_summary: text data (optinal)\n       \n       - quantity : numerical (optinal)\n       - teacher_number_of_previously_posted_projects : numerical\n       - price : numerical"}}