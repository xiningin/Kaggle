{"cell_type":{"b9bb77f0":"code","37207845":"code","a2b186c9":"code","db70a825":"code","21c817a2":"code","285fc823":"code","77c8a9af":"code","c5a9eb26":"code","a6bd36e4":"code","199fc649":"code","eade5fab":"code","6de92500":"code","ea39ed79":"code","0afb6605":"code","ba226631":"code","848a4628":"code","442f9b55":"code","d4dddd22":"code","8d0dfe1d":"code","516c32d5":"code","2f1e0d13":"code","e04e0e0b":"code","1c42eba3":"code","fc71b6ea":"code","4fa0ad3e":"code","70c0935e":"code","f96c08db":"markdown","c196839e":"markdown","dd541d6a":"markdown","f3dd9317":"markdown","3c0662c7":"markdown","215bfdfb":"markdown","b2d4fc70":"markdown","026d53f4":"markdown","b1014d54":"markdown","1168bec5":"markdown","68eae9ff":"markdown","94ba5188":"markdown","06911165":"markdown","fdea7aef":"markdown","2fca13a5":"markdown","08af18ab":"markdown"},"source":{"b9bb77f0":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt #\u00a0for graphics\nimport seaborn as sns #\u00a0too for graphics","37207845":"#\u00a0this is for show the dataset route\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a2b186c9":"#\u00a0the dataset path\npath = '..\/input\/star-type-classification\/Stars.csv'\n# read the dataset\ndataset = pd.read_csv(path)\n\n#\u00a0lets rename a few columns\nnew_names = {'L': 'Relative_Luminosity', 'R': 'Relative_Radius', 'A_M': 'Absolute_Magnitude'}\ndataset = dataset.rename(columns=new_names)\n# see the first dataset's rows\ndataset.head()","db70a825":"# This is only for give an image of the data\nY = np.log10(dataset['Relative_Luminosity'].values)\nX = np.log10(dataset['Temperature'].values)\nclusters = dataset['Type'].values\n\ncdict = {0: 'skyblue', 1: 'indigo', 2: 'orange', 3: 'yellow', 4: 'limegreen', 5: 'lightcoral'}\n\n\nfig, ax = plt.subplots()\nplt.title('H-R Diagram')\nfor g in np.unique(clusters):\n    ix = np.where(clusters == g)\n    ax.scatter(X[ix], Y[ix], c=cdict[g], label=g)\nax.legend()\nplt.show()","21c817a2":"#dataset['Color'].value_counts().plot(kind='bar')\ndataset['Color'].value_counts()","285fc823":"# there few are repeated colors that are not counted by the same column\n# so it's needed to replace some names for 'repair' the names and then\n#\u00a0do not have too much columns, that's going to be important\nfixed_names = {\n    'Red' : 'Red', \n    'Blue' : 'Blue', \n    'Blue-white' : 'Blue-white', \n    'Blue White' : 'Blue-white', \n    'yellow-white' : 'White-Yellow', \n    'White' : 'White',\n    'Blue white' : 'Blue-white', \n    'white' : 'White', \n    'Yellowish White' : 'White-Yellow', \n    'yellowish' : 'Yellow', \n    'Orange' : 'Orange',\n    'Whitish' : 'White', \n    'Yellowish' : 'Yellow', \n    'Blue-White' : 'Blue-white', \n    'Pale yellow orange' : 'Orange',\n    'Orange-Red' : 'Orange', \n    'White-Yellow' : 'White-Yellow' \n}\n\n#\u00a0this line renames values of Color depending on the dict\ndataset['Color'] = dataset['Color'].map(fixed_names).astype('category')\n#\u00a0then we visualize the values of color\nfigure = plt.figure(figsize=(20,8))\nsns.barplot(x=\"Color\", y=\"Type\", data=dataset)\nplt.show()","77c8a9af":"# convert the type to categorical for get ids\ndataset['Color'] = dataset['Color'].astype('category')\n# add the categorical numbers in a new column\ndataset['Color_cats'] = dataset['Color'].cat.codes\n\n#\u00a0get the categorical numbers of each Color\nids = list(dataset['Color_cats'].value_counts().index)\n# get the colors, both are in the same order\ncolors = list(dataset['Color'].value_counts().index)\ndataset.head()","c5a9eb26":"# create a df with the ids and the colors\ndf = pd.DataFrame(list(zip(ids, colors)),columns=['Ids', 'Color'])\n# and then use the oneHotEncoder in a new dataframe\nencoding = pd.get_dummies(dataset['Color'], prefix='Color')\n# finally there's to concat the encoding df to dataset\ndataset = pd.concat([dataset, encoding], axis=1, join=\"inner\")\n#\u00a0and drop the Color_cats column\ndataset.drop(['Color_cats', 'Color'], axis=1, inplace=True)\nprint(dataset.shape)\ndataset.head()","a6bd36e4":"figure = plt.figure(figsize=(20,8))\nsns.barplot(x=\"Spectral_Class\", y=\"Type\", data=dataset)\nplt.show()","199fc649":"#\u00a0for this it's okey to repeat the previous process\ncol = 'Spectral_Class'\n\n# convert the type to categorical for get ids\ndataset[col] = dataset[col].astype('category')\n# add the categorical numbers in a new column\ndataset[f'{col}_cats'] = dataset[col].cat.codes\n\n#\u00a0get the categorical numbers of each Color\nids = list(dataset[f'{col}_cats'].value_counts().index)\n# get the colors, both are in the same order\ncolors = list(dataset[col].value_counts().index)\n\n# create a df with the ids and the colors\ndf = pd.DataFrame(list(zip(ids, colors)),columns=['Ids', 'Color'])\n# and then use the oneHotEncoder in a new dataframe\nencoding = pd.get_dummies(dataset[col], prefix=col)\n# finally there's to concat the encoding df to dataset\ndataset = pd.concat([dataset, encoding], axis=1, join=\"inner\")\n#\u00a0and drop the Color_cats column\ndataset.drop([f'{col}_cats', col], axis=1, inplace=True)\ndataset.head()","eade5fab":"from sklearn.preprocessing import StandardScaler #\u00a0the scaler\n\ncols = ['Temperature','Absolute_Magnitude','Relative_Luminosity','Relative_Radius' ]\n\n#\u00a0instance the scaler\ntemp_scaler = StandardScaler()\n#\u00a0fit the scaler with the data\ntemp_scaler.fit(dataset[cols])\n#\u00a0transform the data\nscaled_vals = temp_scaler.transform(dataset[cols])\n#\u00a0and save the changes\ndataset['Temperature'] = scaled_vals[:,0]\ndataset['Absolute_Magnitude'] = scaled_vals[:,1]\ndataset['Relative_Luminosity'] = scaled_vals[:,2]\ndataset['Relative_Radius'] = scaled_vals[:,3]\ndataset.head()","6de92500":"# select the target\ntarget = dataset['Type'].values\n#\u00a0delete the target from the dataset\ndataset.drop(['Type'], axis=1, inplace=True)\ntarget","ea39ed79":"dataset.head()","0afb6605":"dataset.shape","ba226631":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(dataset, target, test_size=.2, random_state=2021)","848a4628":"# many models to prove how is them accuracy\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n#\u00a0and import the cross validation score to eval the models\nfrom sklearn.model_selection import cross_val_score","442f9b55":"#\u00a0instance and fit all the models with the data\n\n# the solver=\"liblinear\" is better for small datasets\nlj = LogisticRegression(solver=\"liblinear\").fit(X_train, Y_train)\nknn = KNeighborsClassifier().fit(X_train, Y_train)\ndtc = DecisionTreeClassifier(random_state=0).fit(X_train, Y_train)\nrfc = RandomForestClassifier(random_state=0,verbose=False).fit(X_train, Y_train)\ngbmc = GradientBoostingClassifier(verbose=False).fit(X_train, Y_train)\n\n#\u00a0save the models in a list to visualize the accuracy of each\nmodels = [lj, knn, dtc, rfc, gbmc]","d4dddd22":"#\u00a0iterate the models to evaluate them and see the accuracy\nfor model in models:\n    #\u00a0select the name\n    name = model.__class__.__name__\n    #\u00a0evaluate with cross validation and get the mean\n    score = cross_val_score(model,X_test, Y_test,cv=5,verbose=False).mean()\n    #\u00a0calculate the error metric value with neg_mean_squared_error and same the mean value\n    error = -cross_val_score(model,X_test, Y_test,cv=5,scoring=\"neg_mean_squared_error\",verbose=False).mean()\n    #\u00a0show the results\n    print(\"-> \" + name + \": \")\n    print('Accuracy:', score)\n    #\u00a0aply the sqrt as it is squared error\n    print('Error:', np.sqrt(error))\n    print(\"*\" * 20)\n    ","8d0dfe1d":"# we will try to eval the models with a largest cv\n\n# set the results in a dataframe\nresults = pd.DataFrame(columns=[\"model\",\"score\"])\n\n#\u00a0iterate the models\nfor model in models:\n    # select the name\n    name = model.__class__.__name__\n    # evaluate the model \n    score = cross_val_score(model, X_test, Y_test, cv=5, verbose=False).mean()\n    # set the result on a dataframe\n    result = pd.DataFrame([[name,score]], columns=[\"model\",\"score\"])\n    #\u00a0and append the result on the results dataframe\n    results = results.append(result)\n\n#\u00a0then plot the results\nfigure = plt.figure(figsize=(20,8))   \nsns.barplot(x=\"score\",y=\"model\",data=results)\nplt.xlabel(\"Accuracy\")\nplt.ylabel(\"Model\")\n# set the limit on 100 that is the score limit\nplt.xlim(0,1)\nplt.title(\"Model Accuracy Score\")\nplt.show()","516c32d5":"#\u00a0do the same but for the error values\n\n# set the results in a dataframe\nresults = pd.DataFrame(columns=[\"model\",\"error\"])\n\n#\u00a0iterate the models\nfor model in models:\n    # select the name\n    name = model.__class__.__name__\n    # evaluate the model \n    error = -1 * cross_val_score(model, X_test, Y_test, cv=5, scoring=\"neg_mean_squared_error\", verbose=False).mean()\n    # set the result on a dataframe\n    result = pd.DataFrame([[name, np.sqrt(error)]], columns=[\"model\",\"error\"])\n    #\u00a0and append the result on the results dataframe\n    results = results.append(result)\n\n#\u00a0then plot the results\nfigure = plt.figure(figsize=(20,8))   \nsns.barplot(x=\"error\",y=\"model\",data=results)\nplt.xlabel(\"Error\")\nplt.ylabel(\"Model\")\n# set the limit on 100 that is the score limit\nplt.xlim(0,1)\nplt.title(\"Model Error Score\")\nplt.show()","2f1e0d13":"#\u00a0I will repeat the reading of the dataset\n# but with other name, data\ndata = pd.read_csv(path)\n\n#\u00a0lets rename a few columns\nnew_names = {'L': 'Relative_Luminosity', 'R': 'Relative_Radius', 'A_M': 'Absolute_Magnitude'}\ndata = data.rename(columns=new_names)\n\n# FIRST: have the new measurements, for example:\n# select the first star in the dataset\n\n# get the values and the column names\nvals = data.drop(['Type'], axis=1).loc[0].values\ncols = data.drop(['Type'], axis=1).columns\n\n# set the data in a DataFrame\ndata = pd.DataFrame(data=[vals], columns=cols)","e04e0e0b":"#\u00a0First, the scaler, temp_scaler\nnum_cols = ['Temperature','Absolute_Magnitude','Relative_Luminosity','Relative_Radius']\n\n#\u00a0apply the transformation with the scaler\nscal_vals = temp_scaler.transform(data[num_cols])\n# set the scaled values\ndata['Temperature'] = scal_vals[:,0]\ndata['Absolute_Magnitude'] = scal_vals[:,1]\ndata['Relative_Luminosity'] = scal_vals[:,2]\ndata['Relative_Radius'] = scal_vals[:,3]\ndata\n# expected: -0.779382\t-0.598624\t-0.459210\t1.11674","1c42eba3":"# Second, the one hot encodind\n#\u00a0I will use an extra function to apply one hot encoding\n#\u00a0and I will use it with Color and Spectral_Class\n\n# target col will be 'Color' in data\n# cols will be the color columns in dataset\ndef set_encoding(value, target_col, cols):\n    res = pd.DataFrame()\n    #\u00a0col with value 1\n    col_name = target_col + '_' + value\n    # iterate all the columns\n    for col in cols:\n        #\u00a0look for the special column with value 1\n        if col == col_name:\n            res[col] = [1]\n        # add the column to the dataset with value 0\n        else:\n            res[col] = [0]\n    return res\n\n\n#\u00a0collect the required parameters\ncolor_cols = [col for col in dataset.columns if col[:5] == 'Color']\ncolor_val = data['Color'].values[0]\n\nspectral_cols = [col for col in dataset.columns if col[:8] == 'Spectral']\nspectral_val = data['Spectral_Class'].values[0]\n\n\n# then use the function to Color and Spectral Class\ncolor_encoding = set_encoding(color_val, 'Color', color_cols)\nspectral_encoding = set_encoding(spectral_val, 'Spectral_Class', spectral_cols)\ncolor_encoding","fc71b6ea":"# concat the results to the measurements dataframe\n# and delete the Color and Spectral_Class cols\ndata = pd.concat([data, color_encoding], axis=1)\ndata = data.drop(['Color'], axis=1)\n\ndata = pd.concat([data, spectral_encoding], axis=1)\ndata = data.drop(['Spectral_Class'], axis=1)\ndata","4fa0ad3e":"# we can compare data with the frist row of dataset\nfirst_row = dataset.loc[0]\ndata.values == first_row.values","70c0935e":"#\u00a0I will use the DecisionTreeClassifier\npred = dtc.predict(data)\nresult = target[0]\nprint('Prediction:', pred)\nprint('Answer:', result)","f96c08db":"## Processing Spectral Class - One Hot Encoding","c196839e":"## 2 - Apply the Scaler and Encoder to the data\nThese must be the same, or have the same parameters.","dd541d6a":"# The Models\nTheres many models that can be used for this practise. Since there's no many data use a neural network may not be the best option. These models that are included are from sklearn, and are used for classify. These are the models:\n\n* [LogisticRegression](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html)\n* [KNeighborsClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html)\n* [DecisionTreeClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html)\n* [RandomForestClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html)\n* [GradientBoostingClassifier](https:\/\/scikit-learn.org\/0.15\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html)\n* [LGBMClassifier]()\n* [KMeans](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.KMeans.html?highlight=kmeans#sklearn.cluster.KMeans)","f3dd9317":"## Processing Temperature, Luminosity, Radius and Magnitude - Standard Scaler","3c0662c7":"# Reading the dataset","215bfdfb":"# Visualize the evaluations","b2d4fc70":"## 1 - Set the new measurements in a DataFrame\nFor example the first star from the dataset.","026d53f4":"## 3 - Finally use a model for predict the measurements","b1014d54":"## Processing Color - One Hot Encoding","1168bec5":"# Classifying Stars - Context\nAs far as I know about classifying stars there's many classes we can use depending on the star qualities. This dataset as I see requires to classify stars with categories that appear to be 'by size'. I found a [video](https:\/\/www.youtube.com\/watch?v=Y5VU3Mp6abI) which explains that categories. `The target is the type`.\n\nThe video talks about a ***H-R Diagram*** which could shows the 6 clusters of stars that we want to classify. This plot is `composed by the temperature in X and the luminosity in Y`.","68eae9ff":"## Split the dataset on train and test","94ba5188":"# Data procesing","06911165":"## The target: Type","fdea7aef":"# Conclusion to comment\nThats what I will try to do to answer the question. Also if it's needed to do the prediction process in a single script .py, it will probably be the best option to compact this final process to a single function. The information needed to do that will be:\n\n* The model and scaler parameterss, in the Scikit-Learn documentation tells how to extract the params and then you can save it with numpy files\n* The column names from the original dataset are going to be needed in this case as I did not use OneHotEncoding module, which Scikit-Learn has [one](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html).\n\nThat parameters are going to be needed if you want to run the model an use it to predict in a single script. Here there are documentation about how to save parameters:\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html\nhttps:\/\/scikit-learn.org\/stable\/modules\/model_persistence.html","2fca13a5":"# Results\n\nAs we can see the **DecisionTreeClassifier** looks like the best option as it has 100% of accuracy and 0 error.\nHowever we can see that **RandomForestClassifier** and **GradientBoostingClassifier**, both have a good performance in more than 90% and an not so high error value.","08af18ab":"# Answer to a comment:\nSo from here, how might one go about using this model if given some measurements on an unclassified star?\n\nI will try to set the measurements in a **DataFrame** and then use the trained **OneHotEncoder and Standard Scaler** to normalize the measurements and then use that normalized data to predict with a model. The next cells develop the process."}}