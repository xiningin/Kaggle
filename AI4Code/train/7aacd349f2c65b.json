{"cell_type":{"f9751fd1":"code","bd2a6838":"code","d9064e47":"code","bb503bca":"code","a8b09720":"code","fd03a06b":"code","d877d83b":"code","f862e03f":"code","c4f362ee":"code","2ad86f2b":"markdown"},"source":{"f9751fd1":"import numpy as np\nimport pandas as pd\nimport sys, os\nimport PIL\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import StepLR\nfrom torchvision import datasets, transforms\nfrom sklearn.model_selection import train_test_split\n\n# debugger for handling errors\nfrom IPython.core.debugger import set_trace","bd2a6838":"# Custom Dataset\n# Ref. https:\/\/pytorch.org\/tutorials\/recipes\/recipes\/custom_dataset_transforms_loader.html\nclass SIGNSDataset(Dataset):\n    def __init__(self, filenames, labels, transform):      \n        self.filenames = filenames\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        #return size of dataset\n        return len(self.filenames)\n\n    def __getitem__(self, idx):\n        #open image, apply transforms and return with label\n        image = Image.open(self.filenames[idx])  # PIL image\n        image = self.transform(image)\n        return image, self.labels[idx]","d9064e47":"# Once experimentation is done is the preprocessing to be done, transformation and preprocessing should be done as part of data loader\n\ntrain_transformer = transforms.Compose([\n                    transforms.Resize(64),              # resize the image to 64x64 \n                    transforms.RandomHorizontalFlip(),  # randomly flip image horizontally\n                    transforms.ToTensor()])             # transform it into a PyTorch Tensor\n\nval_transformer =   transforms.Compose([\n                    transforms.Resize(64),              # resize the image to 64x64 \n                    transforms.ToTensor()])             # transform it into a PyTorch Tensor","bb503bca":"import glob\nimages = glob.glob('..\/input\/leapgestrecog\/leapGestRecog\/**\/**\/*.png')\n\n# extract label number from filename and reduce by 1 so that it ranges from 0 to 9 (instead of 1 to 10). Otherwise loss function will complain\nlabels = [int(os.path.basename(i).split('_')[2])-1 for i in images]\nx_train, x_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n","a8b09720":"train_dataset = SIGNSDataset(x_train, y_train, train_transformer)\nval_dataset = SIGNSDataset(x_val, y_val, val_transformer)","fd03a06b":"bs = 32\nlr = 0.01\ntrain_dl = DataLoader(train_dataset, bs, shuffle=True)\nval_dl = DataLoader(val_dataset, bs)","d877d83b":"# Model Arch\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 32, 3, 1)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(79680, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output","f862e03f":"import numpy as np\n\ndef get_model():\n    model = Net()\n    return model, optim.SGD(model.parameters(), lr=lr)\n\n\ndef loss_batch(model, loss_func, xb, yb, opt=None):\n    yhat = model(xb)\n    loss = loss_func(yhat, yb)\n\n    if opt is not None:\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n    return loss.item(), len(xb)\n\n\ndef fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n    for epoch in range(epochs):\n        model.train()\n        for xb, yb in train_dl:\n            loss_batch(model, loss_func, xb, yb, opt)\n\n        model.eval()\n        with torch.no_grad():\n            losses, nums = zip(\n                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n            )\n        val_loss = np.sum(np.multiply(losses, nums)) \/ np.sum(nums)\n\n        print(epoch, val_loss)\n\nmodel, opt = get_model()\nloss_func = F.cross_entropy\n","c4f362ee":"fit(1, model, loss_func, opt, train_dl, val_dl)","2ad86f2b":"## Overview\nThis notebook explains the basic cnn implementation from the torch apis only to predict the `hand-sign` gesture.\n\nThere are ofcourse better methods to write the same implementation. For e.g. fastai's CNNLearner, pytorch-lightning module.\n\nFor people getting started with pytorch, I would recommend to first cover:\n1. [Pytorch 60 min blitz](https:\/\/pytorch.org\/tutorials\/beginner\/deep_learning_60min_blitz.html#deep-learning-with-pytorch-a-60-minute-blitz)\n2. [Pytorch nn tutorial](https:\/\/pytorch.org\/tutorials\/beginner\/nn_tutorial.html)"}}