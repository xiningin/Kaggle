{"cell_type":{"a2f54b7a":"code","a150f0fb":"code","1a95adb9":"code","5910bdb0":"code","6ac727a2":"code","3217162b":"code","2ce732fd":"code","017ab9cd":"code","4a4f2887":"code","aa226a94":"code","18aee49a":"code","27fbbf8f":"code","e06536a9":"code","441447eb":"code","49d9020f":"code","c7e4411a":"code","4bd493e1":"code","d437dd77":"code","c26ef608":"code","03ade465":"code","b4b10098":"code","9804add9":"code","0708c72c":"code","c899d989":"code","7adb3be4":"code","8d30576d":"code","6ffbe00f":"code","8282aa04":"code","f89ad974":"code","16bd9841":"code","e911acd7":"code","263adb7b":"code","41fc96ed":"code","e5c6a750":"code","84ad672a":"code","a85fa14f":"code","8e9f08aa":"code","bfb9bb85":"markdown","a6ade1a9":"markdown","e43f8a72":"markdown","a10bfe6f":"markdown","6234db40":"markdown","b2f4963c":"markdown","e797dc0b":"markdown","b04a2cdb":"markdown","8247a4b8":"markdown","78e2278a":"markdown","9a822652":"markdown","942f7b52":"markdown","b342ea6c":"markdown"},"source":{"a2f54b7a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns; sns.set()\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support as score\nimport warnings\n\n\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a150f0fb":"train = pd.read_csv('..\/input\/titanic\/train.csv')\n","1a95adb9":"train.shape","5910bdb0":"#to understand the underline structure of the data using '.shape'\ntrain.isnull()","6ac727a2":"#Checking NaN\ntrain.isnull().sum()","3217162b":"sns.heatmap(train.isnull(),yticklabels=False,cbar='False',cmap='viridis') ","2ce732fd":"train['Survived'].value_counts()","017ab9cd":"sns.set_style('whitegrid')\nsns.countplot(x='Survived', data=train)","4a4f2887":"sns.set_style('whitegrid')\nsns.countplot(x='Survived', hue='Sex', data=train, palette='RdBu_r')","aa226a94":"train['Pclass'].value_counts()","18aee49a":"sns.set_style('whitegrid')\nsns.countplot(x='Survived', hue='Pclass', data=train, palette=\"rainbow\")","27fbbf8f":"train['Age'].hist(bins=30, color='darkred', alpha=0.3)","e06536a9":"sns.countplot(x='SibSp', data=train)","441447eb":"train['Fare'].hist(color='blue', bins=40, figsize=(8, 4))","49d9020f":"import cufflinks as cf\ncf.go_offline()","c7e4411a":"train['Fare'].iplot(kind='hist', bins=30, color='green')","4bd493e1":"plt.figure(figsize=(12,7))\nsns.boxplot(x='Pclass', y=\"Age\", data=train, palette='winter')","d437dd77":"def impute_age(cols):\n    Age=cols[0]\n    Pclass=cols[1]\n    \n    if pd.isnull(Age):\n        \n        if Pclass == 1:\n            return 37\n        elif Pclass == 2:\n            return 29\n        \n        else:\n            return 24\n    else:\n        return Age","c26ef608":"train['Age'] = train[['Age', 'Pclass']].apply(impute_age, axis=1)","03ade465":"sns.heatmap(train.isnull(), yticklabels=False, cbar=False, cmap='viridis')","b4b10098":"train = train.dropna()","9804add9":"train.head()","0708c72c":"train.dropna(inplace=True)\n","c899d989":"# Preprocessing \n\n# Droping Cabin column\ntrain = train.drop(columns=['Cabin','Name', 'PassengerId', 'Ticket'], axis=1) \n\n# Filling missing value in Age column\ntrain['Age'].fillna(train['Age'].mean(), inplace=True)\n\n# Replacing missing values with mode()\ntrain[\"Embarked\"].fillna(train[\"Embarked\"].mode()[0], inplace=True) \n\n# Finding number of the survived\ntrain[\"Survived\"].value_counts() ","7adb3be4":"train.isnull().sum()","8d30576d":"# Visualizating the data for better understanding\n\nfig, ax = plt.subplots(1,4 , figsize=(16, 8))\n\n\n# Countplot for survived based on Gender\nsns.countplot(\"Sex\", hue=\"Survived\", data=train, ax=ax[0]) \n\n# Countplot for survived base on Pclass\nsns.countplot(\"Pclass\", hue='Survived', data=train, ax=ax[1]) \n\n# Countplot for survived base on Embarked\nsns.countplot(\"Embarked\", hue='Survived', data=train, ax=ax[2]) \n\n\nsns.countplot(\"SibSp\", hue='Survived', data=train, ax=ax[3]) \nfig.subplots_adjust(wspace=0.6)\nplt.show()","6ffbe00f":"# Converting categorical data to numbers\nlabel_encoder = LabelEncoder()\ntrain['Sex'] = label_encoder.fit_transform(train['Sex'])\ntrain['Embarked'] = label_encoder.fit_transform(train['Embarked'])\n\n\n# for standardscaler of the Age and Fare we can use these 3 lines\n#features = ['Age','Fare']\n#scaler = StandardScaler()\n#df_train[features] = scaler.fit_transform(df_train[features])\n\ntrain.head()","8282aa04":"# Defingin X and y for training\n\ny = train['Survived']\nX = train.drop(columns='Survived', axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, shuffle=True, random_state=20)","f89ad974":"# Stratified K-Folds cross-validator\n# Shuffle each class\u2019s samples before splitting into batches\n# Number of folds is n_splits\n\ncv = StratifiedKFold(n_splits=7, shuffle=True, random_state=20)\n\n# Maximum number of iterations taken for the solvers to converge\nmax_iter = [20, 100, 200, 500, 1000, 5000, 10000]                  \n\n# Algorithm to use in the optimization problem\nsolver =  ['newton-cg', 'lbfgs', 'liblinear']\n\n# Inverse of regularization strength\nC_range = np.logspace(-4, 5, 14) \n\n# The \u201cbalanced\u201d mode uses the values of y to automatically adjust weights inversely proportional\n# to class frequencies in the input data \nclass_weight =  ['balanced']\n\n# Crating a dictionary of the hyper-parameters\nparam_grid = dict(max_iter=max_iter, solver=solver, C=C_range,class_weight=class_weight  )\n\nmodel = LogisticRegression()\n\n# Exhaustive search over param_grid values for an estimator\ngrid = GridSearchCV(model, param_grid=param_grid, cv=cv)\n\n# Fit the X_train and y_train\ngrid.fit(X_train, y_train)\nprint(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))","16bd9841":"# Modelling with best parameters\nmodel = LogisticRegression(max_iter=100, solver='newton-cg', C=0.0587, class_weight='balanced' )\nmodel.fit(X_train, y_train)\n\nyhat_training = model.predict(X_train)\nPrediction = model.predict(X_test)\nprint('Accuracy of the training is: ', \"{:.2f}\".format(accuracy_score(y_train, yhat_training) * 100))\nprint('Accuracy of the testing is: ', \"{:.2f}\".format(accuracy_score(y_test, Prediction) * 100))","e911acd7":"precision, recall, fscore, support = score(y_test, Prediction)\n\nprint('precision: {}'.format(precision))\nprint('recall: {}'.format(recall))\nprint('fscore: {}'.format(fscore))\nprint('support: {}'.format(support))","263adb7b":"test = pd.read_csv('..\/input\/titanic\/test.csv')\ntest.isnull().sum()","41fc96ed":"train.shape","e5c6a750":"# Droping Cabin column\ntest = test.drop(columns=['Cabin','Name', 'PassengerId', 'Ticket'], axis=1) \n\n# Filling missing value in Age column\ntest['Age'].fillna(test['Age'].mean(), inplace=True)\n\n# Filling missing value in Fare column\ntest['Fare'].fillna(test['Fare'].mean(), inplace=True)\n\n# Replacing missing values with mode()\ntest[\"Embarked\"].fillna(test[\"Embarked\"].mode()[0], inplace=True) \n\n# Converting categorical data to numbers\ntest['Sex'] = label_encoder.fit_transform(test['Sex'])\ntest['Embarked'] = label_encoder.fit_transform(test['Embarked'])\ntest.isnull().sum()","84ad672a":"train.head()","a85fa14f":"gender_submission = model.predict(test)","8e9f08aa":"#Preparing data for submission\ncolumn_names = ['PassengerId', 'Survived']\nSubmission = pd.DataFrame(columns = column_names)\n\nSubmission['PassengerId'] = pd.read_csv('..\/input\/titanic\/test.csv')['PassengerId']\nSubmission['Survived'] = pd.DataFrame(gender_submission)\nSubmission.reset_index(drop=True, inplace=True)\n\n#Saving prediction\nSubmission.to_csv('submission.csv', index=False)\nSubmission.head()","bfb9bb85":"<font size=\"5\">**Converting Categorial Features**\n\nWe'll need to convert categorial features to dummy variables using pandas! Otherwise our machine learning algorithm won't be able to directly take in those features as inputs. ","a6ade1a9":"In the missing value data roughly 20 percent of the Age is missing. The proportion of the Age missing is likely small enough for resonable replacement with some form of imputation. Looking at the Cabin column. It looks like we are just missing too much of that data to do something useful with at a basic level. We'll probably drop that later, or change it to another feature like \"Cabin Known 1 to 0\"","e43f8a72":"<font size=\"5\">**Missing Value**\n\nWe can use seaborn to creat a heatmap to see where we are missing data!","a10bfe6f":"Let's move ahead and drop the Cabin column and the row in Embarked that is Nan\n","6234db40":"We can see the weather passenger in the higher classes tend to be older, which makes sense. We'll use these average age values to impute based on Pclass for Age. \n","b2f4963c":"<font size=\"5\">**Train Test Split**","e797dc0b":"<font size=\"5\">**Data Cleaning**\n\nWe want to fit in missing age data instead of just dropping the missing age data rows. One way to do this is by this is by filling in the mean age of all the passengers (imputation). However we can be smater about this and check the average age by passenger class. \nFor example: ","b04a2cdb":"<font size=\"5\">**Building a Logistic Regression Model**\n\nLet's start by splitting our data into a training data and test set (there is another test csv file that you cann play around with in case you wanr to use all this data for training). ","8247a4b8":"Now, let's check that heat map again","78e2278a":"Now apply that function!","9a822652":"<font size=\"5\">****Cufflinks for plots****\n    \n Let's take a quick moment to show an example of cufflinks","942f7b52":"Great Our data is ready for our model","b342ea6c":"<font size=\"5\">**Training and Predicting**"}}