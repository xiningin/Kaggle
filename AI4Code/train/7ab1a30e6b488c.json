{"cell_type":{"63c978de":"code","c5c81814":"code","201c5e3b":"code","944d07fc":"code","14810c78":"code","bba96907":"code","c4445fd1":"code","096b3986":"code","f7b9cf1a":"code","3b7963ad":"code","64d869e0":"code","27754965":"code","5a1851b9":"code","42484639":"code","40c5ffcf":"code","e18af900":"code","3f31bcf2":"code","2d2d05ef":"code","39b2fece":"code","c985f2f1":"code","f34dbcfd":"code","151a0949":"code","7488fbfb":"code","d1da6005":"code","b09ef960":"code","ec9a736f":"code","3b318157":"code","ca7a950e":"code","3692cf9d":"code","061265a5":"code","e78841d1":"code","1e663ac2":"code","5a9c2d9d":"code","14f8e6d3":"code","01afb353":"code","4c44be7c":"code","d8eae0fd":"code","1ffe89af":"code","1cde320b":"code","49358f0e":"code","f2691d93":"code","acc1d433":"code","213d4f5c":"code","8d08f7f1":"code","cd5581a6":"code","641cd145":"code","8c80355b":"code","eda0ab88":"code","19211e59":"code","1dd23182":"code","551186a8":"code","eed1ad5a":"code","c537bed5":"code","b21990da":"code","cb18b292":"code","69cf6c98":"code","62b2c371":"code","cf7fc8da":"code","e3680e5b":"code","257adfb7":"code","5cdf50af":"code","83324281":"code","5477900e":"code","50b2b459":"code","5c26b28c":"code","240c78af":"code","2917ec72":"code","fef499d3":"code","c4650025":"code","3966678a":"code","8425424f":"code","89eb144a":"code","e1923fa1":"code","1d39f007":"code","acd60f2b":"code","fb6d93f9":"code","1bb4381a":"code","193a511a":"code","09ac03f0":"code","552317e4":"code","0df8c093":"code","141a1a51":"code","48418cfa":"code","8329f555":"code","befe5309":"code","f2a579ba":"code","95c06f66":"code","8b229a18":"code","156cdde1":"code","95e2d833":"code","e602da1c":"code","a81ae9a6":"code","9d795b0f":"code","0aeae1cb":"code","b887685e":"code","50e34ced":"code","1610a817":"code","c720eba9":"code","e4858ea2":"code","26a3e4d5":"code","18910739":"code","18d649c8":"code","75e9a5b6":"code","37e8a769":"code","2a94590f":"code","4d4ace1b":"code","ee797e95":"code","311090e3":"markdown","d5e8d98c":"markdown","f8d4561d":"markdown","6725258a":"markdown","18cd618f":"markdown","cc2463c9":"markdown","eea9f564":"markdown","5cb507ab":"markdown","9cc96d16":"markdown","fdcb0d0a":"markdown","27616ac5":"markdown","cfbdb81b":"markdown","37ec861e":"markdown","d9fee77d":"markdown","86c2056f":"markdown","f3d6f803":"markdown","4d8bf8fe":"markdown","954d92dd":"markdown","a3e8e9ae":"markdown","2576061e":"markdown","9d1681fa":"markdown","586a25f5":"markdown","c353b5db":"markdown","a30ca4dd":"markdown","7afc4d79":"markdown","128cb64e":"markdown","d7c6873c":"markdown","b59c433b":"markdown","603c3b0b":"markdown","687a2349":"markdown","6865d6b5":"markdown","77729bb3":"markdown","104063a2":"markdown","96d511c3":"markdown","17a2bc2c":"markdown","f265cee4":"markdown","945800cc":"markdown","89fc44a3":"markdown","1dec908b":"markdown","d6b20647":"markdown","a0abb85b":"markdown","6d491edc":"markdown","ae3c685e":"markdown","0587a6c0":"markdown","a85b2da5":"markdown","4d53a559":"markdown","7e782606":"markdown","83a03b88":"markdown","6e6cd304":"markdown","062771e8":"markdown","bea8334e":"markdown","0037c604":"markdown","b19c20d8":"markdown"},"source":{"63c978de":"#Importing the libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px","c5c81814":"#Importing the train and test datasets into respective dataframes with read_csv function\ndf = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","201c5e3b":"#To display maximum column width\npd.set_option('display.max_colwidth',None)","944d07fc":"#Observing the first few records of the train dataset\ndf.head(10)","14810c78":"#Checking the dimension of the train and test datasets\nprint(df.shape)\nprint(df_test.shape)","bba96907":"#Basic information on the dataframes\ndf.info()","c4445fd1":"#NULL count in train and test\nprint(df.isnull().sum())\nprint(df_test.isnull().sum())","096b3986":"#Initial statistics for the columns\n#We will see describe function even for the object column, so that we can get some pointers on duplicate values.\ndf.describe(include='all')","f7b9cf1a":"df.head()","3b7963ad":"#count for each category target\n#Target 1 stands for real disaster and 0 stands for any other tweets\nclasses = df['target'].value_counts()\nclasses","64d869e0":"#Creating a simple dataframe with percentage of each class\nclass_0 = classes[0]\/df['target'].count()*100\nclass_1 = classes[1]\/df['target'].count()*100\ndist_df = pd.DataFrame({'Percentage':[class_0,class_1]},index=['Normal_Tweets','Disaster_Tweets'])\ndist_df.style.background_gradient(cmap='coolwarm')","27754965":"#Barplot for the classes\nplt.title(\"Percentage of Tweet Classes\",fontweight='bold')\nsns.barplot(x=dist_df.index,y=dist_df['Percentage'],palette='Blues')\nplt.show()","5a1851b9":"df['keyword'].value_counts()[:10]","42484639":"df_not_disaster = df.loc[df['target']==0]\ndf_disaster = df.loc[df['target']==1]","40c5ffcf":"#Top 10 keywords in the disaster tweets\ndf_disaster['keyword'].value_counts()[:10]","e18af900":"#Top 10 keywords in the other tweets\ndf_not_disaster['keyword'].value_counts()[:10]","3f31bcf2":"#Barplots for the above\n#Interpretation is always easier with the plots\nplt.figure(figsize=(16,5))\nplt.subplot(121)\nplt.xlabel('Mentions')\nplt.title('Top 10 keywords - DISASTER',fontweight='bold')\nsns.barplot(y=df_disaster['keyword'].value_counts()[:10].index,x=df_disaster['keyword'].value_counts()[:10])\nplt.subplot(122)\nplt.xlabel('Mentions')\nplt.title('Top 10 keywords - OTHER',fontweight='bold')\nsns.barplot(y=df_not_disaster['keyword'].value_counts()[:10].index,x=df_not_disaster['keyword'].value_counts()[:10])\nplt.show()","2d2d05ef":"#Top 10 locations based on the count\ndf['location'].value_counts()[:10]","39b2fece":"#Maximum Tweet Length\ndf['text'].str.len().max()","c985f2f1":"#Tweet with maximum length present in the train set\ndf.loc[df['text'].str.len()==df['text'].str.len().max()]['text']","f34dbcfd":"#Minimum Tweet length\ndf['text'].str.len().min()","151a0949":"df.loc[df['text'].str.len()==df['text'].str.len().min()]","7488fbfb":"#Average tweet length\ndf['text'].str.len().mean()","d1da6005":"#Average Word length of the tweet of our train corpus\ndf['text'].str.split().apply(lambda x: len(x)).mean()","b09ef960":"#Average Word length of the tweet for seperate Disaster and other set\nwl_not_disaster = df_not_disaster['text'].str.split().apply(lambda x: len(x))\nwl_disaster = df_disaster['text'].str.split().apply(lambda x: len(x))\nprint(wl_not_disaster.mean())\nprint(wl_disaster.mean())","ec9a736f":"#Tweets with maximum word count in our train set\ndf.loc[df['text'].str.split().apply(lambda x: len(x))==df['text'].str.split().apply(lambda x: len(x)).max()]","3b318157":"plt.figure(figsize=(15,5))\nplt.subplot(121)\nplt.title(\"Mean Word count-Other Tweets\",fontweight='bold')\nsns.distplot(wl_not_disaster.map(lambda x: np.mean(x)),color='grey')\nplt.xlabel('count')\nplt.subplot(122)\nplt.title(\"Mean Word count-Disaster Tweets\",fontweight='bold')\nsns.distplot(wl_disaster.map(lambda x: np.mean(x)))\nplt.xlabel('count')\nplt.show()","ca7a950e":"#Trying to find if there are tweets present with a web link.\ndf.loc[df['text'].str.contains('http')]","3692cf9d":"#Trying to find if there are hashtags alongside\ndf.loc[df['text'].str.contains('#')]","061265a5":"def corpus_build(column):\n    \"\"\"Function to create a corpus list for all the words present in the tweets.Pass in the \n    dataframe column\"\"\"\n    text_corpus = []\n    for i in column.str.split():\n        for word in i:\n            text_corpus.append(word)\n    return text_corpus","e78841d1":"#Text_corpus for the train dataset tweets\ntext_corpus = corpus_build(df['text'])\n#Text corpus for the test dataset tweets\ntext_corpus_test = corpus_build(df_test['text'])","1e663ac2":"#Total Number of words present in the tweets\nlen(text_corpus)","5a9c2d9d":"#Importing the stopwords\nfrom nltk.corpus import stopwords","14f8e6d3":"#We can see the stopwords listed with this.\nprint(stopwords.words('english'))","01afb353":"#Count of stopwords present in our tweets\ncorpus_stopwords = {}\nfor word in text_corpus:\n    if word in stopwords.words('english'):\n        if word in corpus_stopwords:\n            corpus_stopwords[word] += 1\n        else:\n            corpus_stopwords.update({word: 1})\ncorpus_stopwords","4c44be7c":"#We will try to sort this words in terms of frequency - higher to lower and find out top10 frequent stopwords\ncorpus_stopwords_sorted = sorted(corpus_stopwords.items(),key=lambda x:x[1],reverse=True)","d8eae0fd":"corpus_stopwords_10 = corpus_stopwords_sorted[:10]\ncorpus_stopwords_10","1ffe89af":"top_corpus_stopwords = pd.DataFrame(corpus_stopwords_10,columns=[\"Word\",\"Frequency\"])\ntop_corpus_stopwords.style.background_gradient(cmap='Blues')","1cde320b":"#Treemap for the corpus top stopwords\nfig = px.treemap(top_corpus_stopwords,path=['Word'],values='Frequency',title=\"Top 10 stopwords in the corpus\")\nfig.show()","49358f0e":"keys = []\nvalues = []\nfor i in corpus_stopwords_10:\n    keys.append(i[0])\n    values.append(i[1])","f2691d93":"#Plotting the top appearing stopwords and their corresponding frequency\nplt.title(\"Top appearing STOPWORDS\",fontweight='bold')\nplt.bar(keys,values,color='grey')\nplt.show()","acc1d433":"# The top appearing stopwords in our corpus are -\nkeys","213d4f5c":"#Example tweet with link present. We will try to check function on this\nlink =\"Link to Regex basics - https:\/\/www.w3schools.com\/python\/python_regex.asp\"","8d08f7f1":"#Importing the regular expression function\nimport re","cd5581a6":"#Function to remove the links in the text\ndef remove_url(input):\n    \"\"\"Function to remove the URLs present in the text. Feed in the text data as input to function\"\"\"\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',input)","641cd145":"remove_url(link)","8c80355b":"#Some of the records with urls before our process\ndf.iloc[31:34]","eda0ab88":"df['text'] = df['text'].apply(remove_url)\ndf_test['text'] = df_test['text'].apply(remove_url)","19211e59":"#After url removal\ndf.iloc[31:34]","1dd23182":"#Importing the String module\nimport string","551186a8":"#Python provides a constant called string.punctuation that provides a great list of punctuation characters. \nprint(string.punctuation)","eed1ad5a":"def remove_punctuation(input1):\n    \"\"\"To remove all the punctuations present in the text. Input the text to the function\"\"\"\n    table = str.maketrans('','',string.punctuation)\n    return input1.translate(table)","c537bed5":"#Some of the records with hash before our process\ndf.iloc[3:6]","b21990da":"df['text'] = df['text'].apply(remove_punctuation)\ndf_test['text'] = df_test['text'].apply(remove_punctuation)","cb18b292":"#After Punctuation removal\ndf.iloc[3:6]","69cf6c98":"#Converting text column to all lowercase\ndf['text'] = df['text'].str.lower()\ndf_test['text'] = df_test['text'].str.lower()","62b2c371":"df.loc[df['text'].str.contains(\"\\n\")][:5]","cf7fc8da":"def remove_linebreaks(input1):\n    \"\"\"Function to remove the line breaks  present in the text. Feed in the text data as input to function\"\"\"\n    text = re.compile(r'\\n')\n    return text.sub(r' ',input1)","e3680e5b":"df['text'] = df['text'].apply(remove_linebreaks)\ndf_test['text'] = df_test['text'].apply(remove_linebreaks)","257adfb7":"#Importing the word_tokenize\nfrom nltk.tokenize import word_tokenize","5cdf50af":"#We can tokenize all the tweets using word_tokenize\ndf['text'] = df['text'].apply(word_tokenize)\ndf_test['text'] = df_test['text'].apply(word_tokenize)","83324281":"df.head()","5477900e":"def remove_stopwords(input1):\n    \"\"\"Function to remove the stopwords present in the text. Feed in the text data as input to function\"\"\"\n    words = []\n    for word in input1:\n        if word not in stopwords.words('english'):\n            words.append(word)\n    return words","50b2b459":"df['text']=df['text'].apply(remove_stopwords)\ndf_test['text'] = df_test['text'].apply(remove_stopwords)","5c26b28c":"df.head(10)","240c78af":"from nltk.stem import WordNetLemmatizer\nlem = WordNetLemmatizer()","2917ec72":"def lemma_wordnet(input1):\n    \"\"\"Lemmatization function\"\"\"\n    return [lem.lemmatize(w) for w in input1]","fef499d3":"df['text'].apply(lemma_wordnet)[:10]","c4650025":"from nltk.stem.porter import PorterStemmer\nstemmer = PorterStemmer()","3966678a":"def stemming_porter(input1):\n    \"\"\"Stemming using Porter Stemmer\"\"\"\n    return [stemmer.stem(w) for w in input1]","8425424f":"df['text'].apply(stemming_porter)[:10]","89eb144a":"from nltk.stem.snowball import SnowballStemmer\nstemmer_snowball = SnowballStemmer(\"english\")","e1923fa1":"def stemming_snowball(input1):\n    \"\"\"Stemming using Snowball Stemmer\"\"\"\n    return [stemmer_snowball.stem(w) for w in input1]","1d39f007":"df['text'].apply(stemming_snowball)[:10]","acd60f2b":"df['text'] = df['text'].apply(lemma_wordnet)\ndf_test['text'] = df_test['text'].apply(lemma_wordnet)","fb6d93f9":"df.head()","1bb4381a":"def combine_text(input1):\n    \"\"\"Function to combine the list words\"\"\"\n    combined = ' '.join(input1)\n    return combined","193a511a":"df['text'] = df['text'].apply(combine_text)\ndf_test['text'] = df_test['text'].apply(combine_text)","09ac03f0":"df.head()","552317e4":"#Importing the CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer","0df8c093":"#Object for the CountVectorizer function\nvectorizer = CountVectorizer()\nbow_model_train = vectorizer.fit_transform(df['text'])\nbow_model_test = vectorizer.transform(df_test['text'])","141a1a51":"#Complete sparse array\nbow_model_train.toarray()","48418cfa":"#Importing TF-IDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer","8329f555":"vectorizer_tfidf = TfidfVectorizer()\ntfidf_model_train = vectorizer_tfidf.fit_transform(df['text'])\ntfidf_model_test = vectorizer_tfidf.transform(df_test['text'])","befe5309":"#Complete sparse array\ntfidf_model_train.toarray()","f2a579ba":"#CountVectorizer with ngram_range=(2,2) will give us bigrams. We will fit_transform our text column with this.\nvectorizer_bigram = CountVectorizer(ngram_range=(2,2),analyzer='word')\nsparse_matrix = vectorizer_bigram.fit_transform(df['text'])","95c06f66":"#We are creating here a dataframe for the bigrams which shows the frequency of this bigrams\nfrequencies = sum(sparse_matrix).toarray()[0]\nbigram_df = pd.DataFrame(frequencies,index=vectorizer_bigram.get_feature_names(),columns=['frequency'])","8b229a18":"#Sorting the bigram dataframe based on the frequency\nbigram_df.sort_values(['frequency'],axis=0,ascending=False,inplace=True)","156cdde1":"#Top bigrams from our train tweets\nbigram_df[:10].style.background_gradient(cmap='Purples')","95e2d833":"bigram_df.reset_index(inplace=True)","e602da1c":"bigram_df_top20 = bigram_df[:20]","a81ae9a6":"fig = px.treemap(bigram_df_top20,path=['index'],values='frequency',title='Tree of most occuring Bigrams')\nfig.show()","9d795b0f":"#Importing the xgboost\nimport xgboost as xgb","0aeae1cb":"#Setting the hyperparameters for the xgb model\nxgb_param = xgb.XGBClassifier(max_depth=5, n_estimators=300, colsample_bytree=0.8, \n                                subsample=0.8, nthread=10, learning_rate=0.1)","b887685e":"#Importing the model_selection\nfrom sklearn import model_selection","50e34ced":"#Cross Validation scores with XGBoost model and bag of words representaion\nscores = model_selection.cross_val_score(xgb_param, bow_model_train, df[\"target\"], cv=5, scoring=\"f1\")\nscores","1610a817":"#Cross Validation scores with XGBoost model and TF-IDF representaion\nscores = model_selection.cross_val_score(xgb_param, tfidf_model_train, df[\"target\"], cv=5, scoring=\"f1\")\nscores","c720eba9":"#Importing\nfrom sklearn.naive_bayes import MultinomialNB","e4858ea2":"mnb = MultinomialNB()","26a3e4d5":"scores = model_selection.cross_val_score(mnb, bow_model_train, df[\"target\"], cv=5, scoring=\"f1\")\nscores","18910739":"scores = model_selection.cross_val_score(mnb, tfidf_model_train, df[\"target\"], cv=5, scoring=\"f1\")\nscores","18d649c8":"mnb.fit(tfidf_model_train,df[\"target\"])","75e9a5b6":"df_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","37e8a769":"df_submission.shape","2a94590f":"df_submission['target'] = mnb.predict(tfidf_model_test)","4d4ace1b":"df_submission.loc[df_submission['target']==1].shape[0]","ee797e95":"df_submission.to_csv(\"submission.csv\",index=False)","311090e3":"An interesting thing what we can observe here is even for tweets not mentioned as disaster, there are some keywords like fear,siren,explode being used. Maybe these words are used in a sarcastic or funny context.","d5e8d98c":"Most number of tweets in the dataset are from USA. But interestingly, there are some issues with this column, where we can a different mention of Unites states and also some cities listed seperately and not adding to the tally of the country.<br> We have also seen a huge amount of missing values also for this column.<br> Since our primary goal is around the text field, we can shift our focus from this, otherwise should have handled.","f8d4561d":"## BAG OF WORDS VS TF-IDF","6725258a":"So as we have said earlier, we are interested in the text column here, which contains the actualy tweets. There can be possible cleanliness issues in the text. Maybe some links attached, some hashtags and any other issues which needs to be address. We can try observing the same now.","18cd618f":"## NAIVE BAYES","cc2463c9":"### REFERENCES\n\n- https:\/\/www.kaggle.com\/szelee\/simpletransformers-hyperparam-tuning-k-fold-cv\n- https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert\n- https:\/\/www.kaggle.com\/holfyuen\/basic-nlp-on-disaster-tweets\n- https:\/\/www.kaggle.com\/parulpandey\/getting-started-with-nlp-feature-vectors\n- https:\/\/www.kaggle.com\/parulpandey\/getting-started-with-nlp-a-general-intro\n- https:\/\/www.kaggle.com\/friskycodeur\/nlp-with-disaster-tweets-bert-explained\n- https:\/\/www.kaggle.com\/sahib12\/document-embedding-techniques\n\n","eea9f564":"The average Tweet length seems to be around 101 characters.","5cb507ab":"We can now see about the location from which these tweets are posted.","9cc96d16":"We are having a basic idea now, about things to be taken care of as part of Data Cleaning or Text preprocessing here - \n1. Removing the Website Links present within the tweets\n2. Removing Hash associated with hashtags or any special character present with the tweets\n3. Converting all text into either a single case - either Lower or Upper\n4. Removing Line Breaks if any\n5. Tokenization\n6. Handling the stopwords\n7. Stemming\/Lemmatization","fdcb0d0a":"We will use CountVectorizer for this. This converts a collection of text documents to a matrix of token counts.","27616ac5":"We will observe the difference between these and select the appropriate one for this scenario.","cfbdb81b":"#### TF-IDF REPRESENTATION","37ec861e":"#### Data Description\nid - a unique identifier for each tweet <br>\ntext - the text of the tweet<br>\nlocation - the location the tweet was sent from (may be blank)<br>\nkeyword - a particular keyword from the tweet (may be blank)<br>\ntarget - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)<br>","d9fee77d":"We can find that there are some line breaks present in different tweets which we will try removing.","86c2056f":"* We can find quite some bigram words which actually denotes some disaster such as - `suicide bomber`,`burning building`,`mass murder` etc. <br>\nWe can also find some other tweets with large no of mentions with bigrams such as - `youtube video`,`prebreak best` etc.<br>\nAlso we can see 41 mentions of the bigram `Northern California` which can depict either most of the tweets are coming from this location or the data itself might be collected mostly from this location.","f3d6f803":"We can see the train dataset is almost balanced, with almost equal proportion of tweets from either classes.","4d8bf8fe":"Since the data with us is in text format, there is a need to convert these appropriate inorder to feed to our ML models. Both `Bag of Words` and `TF-IDF` are ways of accomplishing these task.<br>\nBag of Words represents each document or sentence as a seperate row and with every word in the complete set of documents as a column, wherein you fill either the presence of the word in the document with a 0\/1 or you fill the frequency of the word in the document, hence building up a matrix kind of representation. <br>\nBut the problem here with the bag of words model is that, it gives equal importance to each word, but we know that is not the case everywhere. Some words might be more important than the other, based on their usage in a single sentence\/document or if it is more frequently present throughout the documents. This is taken care by TF-IDF(Term Frequency-Inverse Document Frequency) representation.<br>\nTF-IDF gives more weight to those words which are present frequently in a document but are rare across other documents.","954d92dd":"### 1.Removing the Website Links","a3e8e9ae":"We know from the data description that `Keyword` is a particular important word from the tweet. Hence we can observe this column to understand the possible tweet mentions. We can see the top few most often listed keywords.","2576061e":"### <font color = 'brown'> Please do upvote if you really liked my Kernel. It would really push me to write more of such notebooks! CHEERS!","9d1681fa":"Tokenization is a technique used to split the text into smaller elements, which can be either words,sentences or even paragraphs.<br>\nRather than using split() function to tokenize these tweets to words, we can try use any nltk tokenizers.<br>","586a25f5":"### 6. Stopwords Removal","c353b5db":"We are getting a better score with the Naive Bayes model and with TF-IDF. Hence we will try to apply this on the train and test.","a30ca4dd":"Comparing all three, I guess it would be better to go with Lemmatization in our case.","7afc4d79":"Some or all of these words looks like real disaster mentions. Maybe what we can do is creating separate dataframes for disaster\/non-disaster tweets and then observing the mostly mentioned keywords.","128cb64e":"<b>Term Frequency = Frequency of a term in the document \/ Total terms in that document <br>\nInverse Document Frequency = log10 Total Number of Documents \/ Total documents that have the term <\/b>","d7c6873c":"## XGBOOST","b59c433b":"### 7. Stemming and Lemmatization","603c3b0b":"We can see from the dataset that there are 7613 records in total in the train dataset wherein some columns are having NULL values present within.<br>\nAlso, the columns keyword,location and text are of object type<br>\nWe can go ahead and find out the count of NULL values for each column","687a2349":"## BIGRAMS","6865d6b5":"We can see the unique count of text column to be 7503 which means there are some duplicate tweet records present within. We can address this issue at a later point of time.<br>\nOnto EDA!!","77729bb3":"## Real or Not? NLP with Disaster Tweets","104063a2":"#### TWEET LENGTH - DISASTER AND NORMAL","96d511c3":"## DATA UNDERSTANDING","17a2bc2c":"If you observe the tweets now, we can see we have got a cleaner version now and all tweets represented with just keywords only now. <br>\nBut you can also observe some other issues as well <br>\nSay for example - see the 6th row; flood-flooding represents a single word which can be handled by techniques like Stemming and Lemmatization.<br>\nIt is always better to make sure different variations of a word to be represented by a single word hence maintaining the needed importance of the word. <br> These methods are part of token normalisation.","f265cee4":"### Balanced or Imbalanced?","945800cc":"### 4. Removing Line Breaks","89fc44a3":"## LEXICAL PROCESSING","1dec908b":"### 3. Converting all text to Lower","d6b20647":"We can see that our F1 score metrics are really low(around 53% only) with XGBoost model. Hence we can try it out with other models as well.","a0abb85b":"Now what we can do is try build a text corpus with all the words present in these tweets. For this, we can try to pick up each tweet in the dataset break down into words and try and append into a single list.","6d491edc":"NOW BACK TO BUSINESS!!!","ae3c685e":"Now what we have to keep in mind is that there are words which would be used more than once with different tweets. Also, there can be many stopwords present with the tweets which doesnt add any value to the actual meaning of the sentence, but rather can be seen as just sentence builder\/connectors.","0587a6c0":"Oops thats a huge number - 3971 tweets of our train contains links alongside!","a85b2da5":"We can now see some of the characteristics related to the tweet length","4d53a559":"#### BAG OF WORDS REPRESENTATION","7e782606":"### 5. Tokenization","83a03b88":"## EXPLORATORY DATA ANALYSIS","6e6cd304":"These words as you can observe, needs to be removed since doesnt provide any value adds. Before removing such words if any from our tweets, we can run a quick check for this as well.","062771e8":"Both Stemming and Lemmatization are almost same, but there is some difference between them.<br>\nStemming is a rule based technique which chops off the suffix of the word to get root form.<br>\nPopular Stemmers -> `Porter Stemmer` and `Snowball Stemmer`<br>\nLemmarization searches for base word or lemma by recursively going through all the variations of dictionary words.\nPopular Lemmatizer -> `WordNet Lemmatizer`","bea8334e":"Twitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n","0037c604":"Location is one column which is having many missing values in both train and test. The text column which we are interested in, is having no NULL values.","b19c20d8":"### 2. Removing Hashes and other punctuation"}}