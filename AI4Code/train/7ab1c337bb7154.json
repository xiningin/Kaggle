{"cell_type":{"903849c3":"code","19117606":"code","dbaafb5c":"code","4c87b92b":"code","489f8db4":"code","5eccad3f":"code","d7566272":"code","edfd8364":"code","a5118672":"code","affae1fc":"code","0dd8283c":"code","388df1d2":"code","4a2b3747":"code","beb9f3cf":"code","f91df0ea":"code","5b0f373b":"code","2f799572":"code","b93d0091":"code","f503e879":"code","2a098cad":"code","925780b3":"code","5a13e81c":"code","72fa2ece":"code","7da5925b":"code","665f9126":"code","618e96ee":"code","7186aa4a":"code","61b10d38":"code","3579805b":"code","2e19a24b":"code","9db3d1e0":"code","cab0a689":"code","d279163a":"code","45f27e91":"code","7453d7a2":"code","6600de0e":"code","006a8ac2":"code","31906284":"code","8beb94c8":"code","dcee9641":"code","a3d3307c":"code","d65a6ed5":"code","dfd09a7d":"code","cceca133":"markdown","8eb1a8b4":"markdown","e3491ab5":"markdown","060920c3":"markdown","4405f085":"markdown","7c091795":"markdown","b0da19f6":"markdown","023e56cd":"markdown","00783d84":"markdown","bdba2223":"markdown","a9c9a372":"markdown","d0749b94":"markdown","d615561f":"markdown","677511b7":"markdown","1a889ab8":"markdown","fc51c5f1":"markdown","dd43e666":"markdown","2d5106c1":"markdown","42274e09":"markdown","8820ebce":"markdown","13702da1":"markdown","27a8dc0a":"markdown","f715f283":"markdown","2300cac8":"markdown","950b313d":"markdown","af228a7f":"markdown","1d2a2fed":"markdown","dd334cd5":"markdown","c442bebf":"markdown","f247617d":"markdown","56063605":"markdown","41a20eba":"markdown","86a13fa2":"markdown","1822bed9":"markdown","671a2e30":"markdown","202db177":"markdown"},"source":{"903849c3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","19117606":"train_df=pd.read_csv('\/kaggle\/input\/income\/train.csv')\ntest_df=pd.read_csv('\/kaggle\/input\/income\/test.csv')\n\ntrain_df.shape, test_df.shape","dbaafb5c":"train_df['label']='train'\ntest_df['label']='test'\n\ncombined_data_df=pd.concat([train_df,test_df])\ncombined_data_df.shape","4c87b92b":"combined_data_df.info()","489f8db4":"combined_data_df.isnull().sum()","5eccad3f":"combined_data_df.dropna(subset=['workclass','occupation','native-country'],axis=0,inplace=True)\ncombined_data_df.isnull().sum()","d7566272":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_theme(style=\"darkgrid\")\n\n#frequency distribution of work class\nplt.figure(figsize=(10,10))\nsns.countplot(data= combined_data_df, x = combined_data_df['workclass'])","edfd8364":"combined_data_df.drop(combined_data_df.index[combined_data_df['workclass'] == 'Without-pay'], inplace=True)\ncombined_data_df.shape","a5118672":"plt.figure(figsize=(10,15))\nsns.countplot(data= combined_data_df, y = \"native-country\")","affae1fc":"combined_data_df=combined_data_df[combined_data_df['native-country']=='United-States']\ncombined_data_df.shape","0dd8283c":"combined_data_df=combined_data_df.drop(columns='native-country',axis=1)\ncombined_data_df.shape","388df1d2":"#frequency distribution of education class\n\nplt.figure(figsize=(20,10))\nsns.countplot(data= combined_data_df, x = \"education\")","4a2b3747":"combined_data_df['education'] = combined_data_df['education'].replace(['1st-4th','5th-6th'],'elementary-school')\ncombined_data_df['education'] = combined_data_df['education'].replace(['7th-8th'],'middle-school')\ncombined_data_df['education'] = combined_data_df['education'].replace(['9th','10th','11th','12th'],'high-school')\ncombined_data_df['education'] = combined_data_df['education'].replace(['Doctorate','Bachelors','Some-college','Masters','Prof-school','Assoc-voc','Assoc-acdm'],'postsecondary-education')","beb9f3cf":"plt.figure(figsize=(20,10))\nsns.countplot(data= combined_data_df, x = \"education\")","f91df0ea":"plt.figure(figsize=(20,10))\nsns.countplot(data= combined_data_df, x = \"marital-status\")","5b0f373b":"combined_data_df['marital-status'] = combined_data_df['marital-status'].replace(['Divorced','Never-married','Widowed'],'single')\ncombined_data_df['marital-status'] = combined_data_df['marital-status'].replace(['Married-civ-spouse','Separated','Married-spouse-absent','Married-AF-spouse'],'married')","2f799572":"plt.figure(figsize=(20,10))\nplt.figure()\nsns.countplot(data= combined_data_df, x = \"marital-status\")","b93d0091":"plt.figure(figsize=(20,10)) \nsns.countplot(data= combined_data_df, y = \"occupation\")","f503e879":"plt.figure(figsize=(20,10)) \nsns.countplot(data= combined_data_df, x = \"relationship\")","2a098cad":"plt.figure(figsize=(20,10))\nsns.countplot(data= combined_data_df, x = \"race\")","925780b3":"plt.figure(figsize=(10,10))\nsns.countplot(data= combined_data_df, x = \"gender\")","5a13e81c":"combined_data_df.shape","72fa2ece":"combined_data_df.info()","7da5925b":"#categorical\ncat_columns = [ col for col in list(combined_data_df.columns) if combined_data_df[col].dtype =='object' and col!= 'label']\n\ncat_columns","665f9126":"#numberical\nnum_columns = [ col for col in list(combined_data_df.columns) if combined_data_df[col].dtype in ['int64','float64']]\nnum_columns","618e96ee":"fig= plt.figure(figsize=(15,15))\ncorr_matrix = combined_data_df.corr()\nsns.heatmap(data=corr_matrix,annot=True)\nplt.show()","7186aa4a":"combined_data_df.drop(columns='fnlwgt',inplace=True)\ncombined_data_df.shape","61b10d38":"#get dummies\nfeatures_df = pd.get_dummies(data=combined_data_df, columns=cat_columns)\nfeatures_df.shape","3579805b":"features_df.columns","2e19a24b":"#split your data\ntrain_df = features_df[features_df['label'] == 'train']\ntest_df = features_df[features_df['label'] == 'test']\n\n# Drop your labels\ntrain_df = train_df.drop('label', axis=1)\ntest_df = test_df.drop(columns=['label','income_>50K'], axis=1)\n\n\ntrain_df.shape, test_df.shape","9db3d1e0":"train_df.columns","cab0a689":"train_df.isnull().sum().sum()","d279163a":"y=train_df['income_>50K']\ntrain_df=train_df.drop('income_>50K',axis=1)\ntrain_df.shape,y.shape","45f27e91":"from sklearn.model_selection import train_test_split\n\nX = train_df\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.25)\nX_train.shape, X_test.shape,y_train.shape,y_test.shape","7453d7a2":"from sklearn.tree import DecisionTreeClassifier\n\ndtc=DecisionTreeClassifier()\ndtc.fit(X_train,y_train)\nprint(\"DecisionTreeClassifier train data's mean accuracy = {} %\".format(round((dtc.score(X_train,y_train)*100),2)))\nprint(\"DecisionTreeClassifier test data's mean accuracy = {} %\".format(round((dtc.score(X_test,y_test)*100),2)))","6600de0e":"from sklearn import metrics\n\ny_pred_dtc= dtc.predict(X_test)\ndtc_accuracy = metrics.accuracy_score(y_test,y_pred_dtc)\nprint(\"DecisionTreeClassifier model accuracy = {} %\".format(round((dtc_accuracy*100),2)))","006a8ac2":"cm=metrics.confusion_matrix(y_test,y_pred_dtc)\ncm","31906284":"print(\"Precision = {} %\".format(round(metrics.precision_score(y_test,y_pred_dtc)*100,2)))","8beb94c8":"print(\"Recall\/Senitivity for DecisionTreeClassifier model = {} %\".format(round(metrics.recall_score(y_test,y_pred_dtc)*100,2)))","dcee9641":"from sklearn import ensemble\n\ngbc = ensemble.GradientBoostingClassifier()\ngbc.fit(X_train,y_train)\nprint(\"GradientBoostingClassifier train data's mean accuracy = {} %\".format(round((gbc.score(X_train,y_train)*100),2)))\nprint(\"GradientBoostingClassifier test data's mean accuracy = {} %\".format(round((gbc.score(X_test,y_test)*100),2)))\n\ny_pred_gbc = gbc.predict(X_test)\nprint(\"Recall\/sensitivity of GradientBoostingClassifier model = {} %\".format(round(metrics.recall_score(y_test,y_pred_gbc)*100,2)))","a3d3307c":"rfc = ensemble.RandomForestClassifier()\nrfc.fit(X_train,y_train)\n\nprint(\"RandomForestClassifier train data's mean accuracy = {} %\".format(round((rfc.score(X_train,y_train)*100),2)))\nprint(\"RandomForestClassifier test data's mean accuracy = {} %\".format(round((rfc.score(X_test,y_test)*100),2)))\n\ny_pred_rfc=rfc.predict(X_test)\n\nprint(\"Recall\/sensitivity of RandomForestClassifier model = {} %\".format(round(metrics.recall_score(y_test,y_pred_rfc)*100,2)))","d65a6ed5":"y_pred = rfc.predict(test_df)\n\nsubmission_df = pd.DataFrame(columns=['income>50k'])\nsubmission_df['income>50k']=y_pred.astype(int)\nsubmission_df.head()","dfd09a7d":"submission_df.to_csv('final-output.csv',header = True, index=False)","cceca133":"This is binary classification problem. If income greater than $50k, then its 1 otherwise 0. \n\n**Which algorithm is best for binary classification problem?**\n\nI found [this](https:\/\/medium.com\/@alex.ortner.1982\/top-10-binary-classification-algorithms-a-beginners-guide-feeacbd7a3e2) article good for beginners. I'm going to try few algorithms based *on my understanding and time*. ","8eb1a8b4":"**Data types in data set:**\n\n* Categorical  = 10\n* Numerical  = 5\n* Target  =1 ","e3491ab5":"Missing values are all categorical variables.\n\nFor workclass, 2498 \/ 43957 = 0.05 ~5% of training data. Since missing values are 5% of training data, i'm going to drop them.\n\n**Likweise, i'm going to drop the missing values in all categorical variables except target variable (i.e.,income_>50k)**\n\nPS: I find [this](https:\/\/www.analyticsvidhya.com\/blog\/2021\/04\/how-to-handle-missing-values-of-categorical-variables\/) article good in explaining how to handle missing values of categorical variables.","060920c3":"**Confusion Matrix**\n\n* 0,0 = true negatives(TN) i.e., model correctly predicted people who have income < 50k (i.e.,target = 0)\n* 0,1 = false positives(FP) i.e., model predicted  people have income < 50k however, in reality\/actually they have > 50k\n* 1,0 = false negatives(FN) i.e., model predicted people have income > 50k however, actually they have < 50k\n* 1,1 = true positives(TP) i.e., people correctly predicted people who have income > 50k (target =1)\n\n\n[TN, FP]\n[FN, TP]","4405f085":"# Introduction\n\n**Task:** We need to predict income greater than $50k per annum\n\nThis is my first public notebook. I'm implementating whatever I learnt and understand about Exploratory Data Analysis (EDA). This is beginner level kernel. Feel free to post your comments, suggestions, improvments, even blame. Hope this helps the beginners. ","7c091795":"Its confirmed that, there are more male than female from our findings in relationship feature","b0da19f6":"This graph clearly shows that the given dataset in from US. We can remove other countries since almost of them are US origin.","023e56cd":"**2. Gradient Boosting Classifier**","00783d84":"**Prediction Accuracy**","bdba2223":"**Note:** We can find the accuracy by in-build score() in the model, as well as by accuracy_score() from metrics. I'll use in-build method instead of one extra import","a9c9a372":"**1. Decision Tree**","d0749b94":"**Takeaway from the plot:**\n\n* Most of the people had completed High School grade. When I Googled for High School Grade - *student had finished at least high school with a diploma (in the U.S. grade 12), or hold a high school equivalency (HSE) credential.* This data can be either grade 12 or HSE. So, lets keep this cateogry as-is.\n* We can combine, Grade 1 - 6 as Elementary School ( since 5th-6th are combined category, I've to put in one category), 7-8 as Middle school,9-12 grade as High School. Refer to US education [here](https:\/\/en.wikipedia.org\/wiki\/Education_in_the_United_States#K%E2%80%9312_education)\n* In the above link, if you see, any education above grade 12 is Post-Secondary education. So, we can combine all of them.","d615561f":"**Conclusion:**\n\nSince,Random Forest model have the highest True Positive Ratio (TPR), i.e., **63%**, i'm going to use that model for submission.\n\n**Final Note:**\n\nWe created 3 models with *default* parameters ( also called as hyperparameters ) to these models and got these results. However, we are unsure whether, this is better results, since all these models have lot of parameters with various value for each. We need to find optimal parameters which will give us better results. It will be time consuming process to find one ( trial and error method ). As engineers, when we have problem, we find\/build the solutions. In this scenario, solution is *Hyper Parameter Tunning*. Right now, I understand why we need Hyper Parameter Tunning. I'm learning how to do it. So, i'm ignore that option as of now. I understand, in real world scenario, we need to do Hyper Parameter Tunning and find the best\/better model with optimal parameters.","677511b7":"**3. Split the dataset into categorical and numerical values**","1a889ab8":"**2. Univariate analysis via. Data Visualization**\n\nThis will helps us to understand each feature and its distribution. ","fc51c5f1":"# Model","dd43e666":"* Most of the features have positive correlation  with target variable except fnlwgt. Even, with other features, it have very low positive or almostly negative correlation. So, we can altogether drop that feature. \n\n* Remember, anything more than 0.5 is consider as strong positive correlation\n\n**Note:** Ideally, i should have created numerical dataframe and check the correlation matrix. I came to know that after the fact. However, I learned that, heatmap by default consider ONLY numerical variables and excluding categorical variables which is good part.","2d5106c1":"**Hint:** For binary classification model, apart from Confusion matrix,Accuracy, precision, recall, you can use F1 Score and ROC curve metrics to evaluate the model","42274e09":"**Recall** - *how many relevant items are selected?*\n\nIn binary classification, recall is called **sensitivity**. \n\n**Note:** you can find Recall with the confusion matrix results as well. Same for Precision. However, I choose to use in-built method. ","8820ebce":"From the above graph, it seems there are more male (i.e, Husband ) than female ( i.e., Wife ). Let's cross-check this finding with gender column. ","13702da1":"**Takeaway from the plot:**\n\n* Most of the people in the given dataset, are working in Private sector\n* Without-pay category is very less count. Also, our objective is to find the people with income > $50k.So, we can drop that category.\n\nNote: If you want to save the plot to png image using the below code\n\n> plt.savefig('work-class-distribution.png')\n","27a8dc0a":"I excluded label feature, since, I added for labelling training and testing dataset.\n\n**PS:** If you need to create categorical data frame, you can use the below code \n\n> df_cat= train.select_dtypes(include='object')","f715f283":"The reasons for combining both training and test dataset are:\n1. To find missing values in both the datasets\n2. If we need transform\/remove any features, we can do it in both datasets at one time\n3. To convert categorical variable to numerical variable in both datasets\n","2300cac8":"# EDA [ Exploratory Data Analysis ] - Data Preparation Step\n\n1. Handle the missing value\n\n2. Univariate analysis via. Data Visualization\n\n3. Split the dataset into categorical and numerical values\n\n4. Converting categorical variables to numerical ( Dummy variables )","950b313d":"Since, now we only have US as the only native country, this feature is useless. We can drop this feature altogether","af228a7f":"**Important Note:** Again, our task is find the people who have income more than $50k (i.e., True Positive). *In our task*,Accuracy is out of scope. So, i'm considering only Recall\/Sensitivity. So, DecisionTreeClassifier model will correctly predict **62%** of people with salary > 50k\n\nNow, we need to repeat these steps for various model to find out the best model for our task.","1d2a2fed":"**Hint:** The target feature will also be part of now 'features_df' dataframe (which means it will be part of test dataset as well). So,either you can drop that target feature in test data or use the below code to exclude the target feature and then add concat the target feature with the training data. I go with first one which is much easier. \n\n> pd.get_dummies(data=combined_data_df[ combined_data_df.columns.difference(['income_>50K'],sort=False)], columns=cat_columns)","dd334cd5":"**1. Handling missing Value**","c442bebf":"As per census [website](https:\/\/www.census.gov\/programs-surveys\/cps\/technical-documentation\/subject-definitions.html#maritalstatus), Single, when used as a marital status category, is the sum of never-married, widowed, and divorced people. The category \"married\" is further divided into \"married, spouse present,\" \"separated,\" and \"other married, spouse absent.\"\n\ndivorced+never-married+widowed = single\n\nmarried-civ-spouse+separated+married-spouse-absent+married-AF-spouse = married","f247617d":"**Conclusion:**\n\n* Handled the missing value, by dropping them from the dataset \n* From the data visualization, combined\/categorized the features \n* Dropped the features which has low correlation with target variable and low frequency of data\n* Using dummy variable, converted categorical variable to numerical variable to create better model\n","56063605":"**Precision** - *how many selected items are relevant?* ","41a20eba":"**PS:** If you need to create numerical data frame, you can use the below code\n\n> numerics = ['int64', 'float64']\n> df_num = train.select_dtypes(include=numerics)","86a13fa2":"There are more White people in the given dataset.","1822bed9":"**4. Converting categorical variables to numerical ( Dummy variables )**\n\nMachine learning (ML) models works better with numbers. So, we need to convert categorical to numerical, so that model is able to understand and extract valuable information. As per my understanding, since most of the ML models build based on mathetical concept. So, as the proverb goes, *be roman when you are in rome*. Free feel to correct me, if my understanding is wrong.\n\nThere are various encoding methods available. If you are interested in various encoding methods,check [here](https:\/\/www.analyticsvidhya.com\/blog\/2020\/08\/types-of-categorical-data-encoding\/). \n\n**Dummy variable:**\nI'm going to use dummy variable encoding. It transforms the categorical variable into a set of binary variables (also known as dummy variables). Dummy encoding uses *N-1 features to represent N labels\/categories*. Encoding training dataset only is wrong approach ( remember we are building predictive model and test with test dataset ) and it will fail with the below error ( from my own experience ):\n\n> ValueError: Found input variables with inconsistent numbers of samples: \n\nwhy? In our training data set, we have 'Executive-managers' as one occupation category. However, same category may or may not be there in test dataset. In other scenario, test dataset may have 'ML Engineer' as one occupation cateogry which may be missing in training dataset. In both the case, we'll have inconsistent number of features. To avoid this error, we need to merge train and test data and then encoding it, so that we will have consistent features in both the dataset.\n \n*Always, remember this rule, test data is unknown data ( In real world scenario, we are unsure what type of data might come).*\n\n**Open question:** How to choose which one encoding method to use? dummy vs OneHot encoding vs others? TBD","671a2e30":"**3. Random Forest**","202db177":"It's good mix of occupations, ranging from Exceutive managers to Farming-fishing. Lets keep it as-is."}}