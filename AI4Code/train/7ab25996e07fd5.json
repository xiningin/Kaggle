{"cell_type":{"adbd8fd0":"code","3d50910e":"code","23ee10b2":"code","ad576d42":"code","478bbe4f":"code","6f7092a5":"code","cd26c49c":"code","7657d327":"code","09f92b21":"code","7de11c0c":"code","d2245b3b":"code","57a6e38a":"code","ec22813d":"code","6c7d5400":"code","fdbfd48c":"code","18b815ce":"code","236a2712":"code","c6ef426d":"code","7e5b9e0e":"code","391ab6ee":"code","84e59d29":"code","6ec81cce":"code","d1fab07b":"code","ef7b724d":"code","29b09c3c":"code","a42f1f48":"code","f1340330":"markdown","a06aeb61":"markdown","171d05c0":"markdown","e0e50193":"markdown","7b6683f7":"markdown","679fbe49":"markdown","c316d177":"markdown","39595fe0":"markdown","b487a013":"markdown","c40d0924":"markdown","c7902337":"markdown","a0d61c8b":"markdown","35f5f975":"markdown","bb780fbf":"markdown","3c9a9d1a":"markdown","296319f8":"markdown","fa85807d":"markdown","a692f494":"markdown","68156a21":"markdown","35360084":"markdown","573107fb":"markdown","cc38a814":"markdown","60c62631":"markdown","c5342ce5":"markdown","1b36e29c":"markdown","c99827b5":"markdown","564c6af4":"markdown","5929710f":"markdown","eaa2a3fb":"markdown","9a66164d":"markdown","5b718131":"markdown","d6f2e6ec":"markdown","6726135d":"markdown","c2d162d1":"markdown"},"source":{"adbd8fd0":"# This is importatnt to reproduce the same results\nfrom numpy.random import seed\nseed(1)\nfrom tensorflow import set_random_seed\nset_random_seed(2)\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_curve, auc, precision_recall_curve\n\nimport seaborn as sns\nimport tensorflow as tf\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, BatchNormalization, Activation\nfrom tensorflow.keras.optimizers import Adam","3d50910e":"def get_features_to_keep(data, correlation_threshold = 0):\n  # This function returns the list of features to keep.\n  # This list contains any features whose correlation with Class field is strictly greater than correlation_threshold\n  # If correlation_threshold is None, we will be keeping all features.\n  if correlation_threshold is not None:\n    list_of_features = corr.iloc[:, data.columns == 'Class']\n    list_of_features = list_of_features[list_of_features['Class'] > correlation_threshold].index.tolist()\n  else:\n    list_of_features = []\n  \n  return list_of_features\n\ndef get_intersections(c1, c2):\n    # This function checks the intersection between 2 curves c1 and c2\n    # First it checks if there is an exact match of values.\n    # If not, it will check where there was a change of sign for c1 - c2\n    intersections = np.argwhere(c1 == c2).flatten()\n    if len(intersections) == 0:\n        intersections = np.argwhere(np.diff(np.sign(c1 - c2))).flatten()\n    \n    return intersections","23ee10b2":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        data_file = os.path.join(dirname, filename)\n\n# Read data and print the dataframe head\ndata = pd.read_csv(data_file)\nprint(\"Data dimension: \" + str(data.shape))\ndata.head()","ad576d42":"# Finding any missing value treatment in the dataset.\ndata_na = data.isna().any()\n\nnum_na = (data_na == True).sum()\nif num_na > 0:\n    data_na_positions = np.where(data_na == True)\n    print(\"Data unavailable for the following columns:\")\n    print(data_na_positions)\nelse:\n    print(\"No missing data\")","478bbe4f":"data['normalizedAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))\ndata = data.drop(['Amount'],axis=1)\ndata = data.drop(['Time'],axis=1)\n\nprint(\"Amount column has been normalised under the name of NormalizedAmount\")\nprint(\"Time column has been dropped\")","6f7092a5":"# Correlation\ndata.corrwith(data.Class).plot.bar(\n        figsize = (20, 10), title = \"Correlation with class\", fontsize = 15,\n        rot = 45, grid = True)","cd26c49c":"# HeatMap\n\n# Include all the columns that you do not want them to be included in the heatmap\ncolumns_not_to_include = [] #[\"Amount\", \"Time\"]\n\nif len(columns_not_to_include) > 0:\n    corr = data.iloc[:, np.all([data.columns != c for c in columns_not_to_include], axis=0)].corr()\nelse:\n    corr = data.corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(18, 15))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","7657d327":"# Change the second arguments if you want to minimise the input data by keeping only features that have a correlation with Class that is greater than 0.2 for example\nlist_of_features = get_features_to_keep(data, correlation_threshold=None)\n\nif len(list_of_features) > 0:\n  reduced_data = data.iloc[:, np.any([data.columns == c for c in list_of_features], axis=0)]\n  print(\"Input size reduced from %d to %d\" % (data.shape[1] - 1, reduced_data.shape[1] - 1))\nelse:\n  reduced_data = data\n  print(\"Input size remains the same: %d\" % (reduced_data.shape[1] - 1))\n\n\nX = reduced_data.iloc[:, reduced_data.columns != 'Class']\ny = reduced_data.iloc[:, reduced_data.columns == 'Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0)\n\nprint(\"Train dataset has %d samples, %d input features and %d output features\" % (X_train.shape[0], X_train.shape[1], y_train.shape[1]))\nprint(\"Test dataset has %d samples, %d input features and %d output features\" % (X_test.shape[0], X_test.shape[1], y_test.shape[1]))","09f92b21":"%%time\ndecision_tree = DecisionTreeClassifier(random_state=0,\n                                       criterion='gini',\n                                       max_depth=10,\n                                       max_leaf_nodes=10)\ndecision_tree.fit(X_train, y_train)","7de11c0c":"# Predicting Test Set\ny_pred      = decision_tree.predict(X_test)\ny_pred_prob = decision_tree.predict_proba(X_test)\n\naccuracy  = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall    = recall_score(y_test, y_pred)\nf1        = f1_score(y_test, y_pred)\n\nprint(\"Decision Tree Predicion on Test Set:\")\nprint(\"------------------------------------\")\nprint(\"Accuracy:\\t%.4f\" % (accuracy))\nprint(\"Precision:\\t%.4f\" % (precision))\nprint(\"Recall:\\t\\t%.4f\" % (recall))\nprint(\"F1 Score:\\t%.4f\" % (f1))\n\nresult_dict = {\n    'Model': 'Decision Tree',\n    'Accuracy': accuracy,\n    'Precision': precision,\n    'Recall': recall,\n    'F1 Score': f1\n}\n\ntry:\n    results = results.append(result_dict, ignore_index=True)\nexcept NameError:\n    results = pd.DataFrame([['Decision Tree', accuracy, precision, recall, f1, 0]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC AUC'])","d2245b3b":"precisions, recalls, ths = precision_recall_curve(y_test, y_pred_prob[:,1])\n\n# Get the middle threshold, the closest to 0.5 which is used in the classification\nth_idx = np.abs(ths - 0.5).argmin()\n\n#plt.figure(figsize=(8,5))\n\n# Plotting the precision, recall curves\nplt.plot(ths, precisions[:-1], \"b--\", label=\"Precision\")\nplt.plot(ths, recalls[:-1], \"g--\", label=\"Recall\")\n\n# Plotting a vertical line at threshold = 0.5 which is the one used to predict\nplt.axvline(x=ths[th_idx], color='r', linestyle='--')\n\n# Plotting the intersection of the precision and recall curves\n#idx = np.argwhere(np.diff(np.sign(precisions - recalls))).flatten()\nidx = get_intersections(precisions, recalls)\nplt.plot(ths[idx], recalls[idx], 'ro')\n\nfor i in range(len(idx)):\n    if i > 0:\n        if ((idx[i] - idx[i-1]) \/ idx[i]) > 0.01:\n            plt.annotate('thr = %.1f, prec, rec = %.4f' % (ths[idx[i]], recalls[idx[i]]),\n                         xy=(ths[idx[i]], precisions[idx[i]]),\n                         xytext=(-40, -50),\n                         textcoords='offset points',\n                         arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"))\n    else:\n        plt.annotate('thr = %.1f, prec, rec = %.4f' % (ths[idx[i]], recalls[idx[i]]),\n                     xy=(ths[idx[i]], precisions[idx[i]]),\n                     xytext=(-40, -50),\n                     textcoords='offset points',\n                     arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"))\n\nplt.plot(ths[th_idx], precisions[th_idx], 'ro')\nplt.annotate('precision = %.4f' % (precisions[th_idx]),\n             xy=(ths[th_idx], precisions[th_idx]),\n             xytext=(10, 20),\n             textcoords='offset points',\n             arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"))\nplt.plot(ths[th_idx], recalls[th_idx], 'ro')\nplt.annotate('recall = %.4f' % (recalls[th_idx]),\n             xy=(ths[th_idx], recalls[th_idx]),\n             xytext=(0, -30),\n             textcoords='offset points',\n             arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"))\n\nplt.xlabel('Threshold')\nplt.title('Precision \/ Recall Curves')\nplt.legend()\n\nplt.show()","57a6e38a":"cm    = confusion_matrix(y_test, y_pred) # rows = truth, cols = prediction\ndf_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\n\nsns.heatmap(df_cm, annot=True, fmt='g')","ec22813d":"dt_fpr, dt_tpr, thresholds = roc_curve(y_test, y_pred_prob[:,1])\nroc_auc = auc(dt_fpr, dt_tpr)\n\nresults.iloc[-1, results.columns.get_loc('ROC AUC')] = roc_auc\n\nplt.title('Desicion Tree ROC Curve')\n\nplt.plot(dt_fpr, dt_tpr, label='AUC = %0.4f'% roc_auc)\nplt.legend(loc='lower right')\n\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.001, 1])\nplt.ylim([0, 1.001])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.show()","6c7d5400":"%%time\nrandom_forest = RandomForestClassifier(random_state=0,\n                                       max_depth=12,\n                                       n_estimators=250,\n                                       max_leaf_nodes=100,\n                                       n_jobs=-1)\nrandom_forest.fit(X_train, np.ravel(y_train))","fdbfd48c":"# Predicting Test Set\ny_pred      = random_forest.predict(X_test)\ny_pred_prob = random_forest.predict_proba(X_test)\n\naccuracy  = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall    = recall_score(y_test, y_pred)\nf1        = f1_score(y_test, y_pred)\n\nprint(\"Random Forest Predicion on Test Set:\")\nprint(\"------------------------------------\")\nprint(\"Accuracy:\\t%.4f\" % (accuracy))\nprint(\"Precision:\\t%.4f\" % (precision))\nprint(\"Recall:\\t\\t%.4f\" % (recall))\nprint(\"F1 Score:\\t%.4f\" % (f1))\n\nresult_dict = {\n    'Model': 'Random Forest',\n    'Accuracy': accuracy,\n    'Precision': precision,\n    'Recall': recall,\n    'F1 Score': f1\n}\n\ntry:\n    results = results.append(result_dict, ignore_index=True)\nexcept NameError:\n    results = pd.DataFrame([['Random Forest', accuracy, precision, recall, f1, 0]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score','ROC AUC'])","18b815ce":"precisions, recalls, ths = precision_recall_curve(y_test, y_pred_prob[:,1])\n\n# Get the middle threshold, the closest to 0.5 which is used in the classification\nth_idx = np.abs(ths - 0.5).argmin()\n\n# Plotting the precision, recall curves\nplt.plot(ths, precisions[:-1], \"b--\", label=\"Precision\")\nplt.plot(ths, recalls[:-1], \"g--\", label=\"Recall\")\n\n# Plotting a vertical line at threshold = 0.5 which is the one used to predict\nplt.axvline(x=ths[th_idx], color='r', linestyle='--')\n\n# Plotting the intersection of the precision and recall curves\nidx = get_intersections(precisions, recalls)\nplt.plot(ths[idx], recalls[idx], 'ro')\n\nfor i in range(len(idx)):\n    if i > 0:\n        if ((idx[i] - idx[i-1]) \/ idx[i]) > 0.01:\n            plt.annotate('thr = %.2f, prec, rec = %.4f' % (ths[idx[i]], recalls[idx[i]]),\n                         xy=(ths[idx[i]], precisions[idx[i]]),\n                         xytext=(0, -50),\n                         textcoords='offset points',\n                         arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"))\n    else:\n        plt.annotate('thr = %.2f, prec, rec = %.4f' % (ths[idx[i]], recalls[idx[i]]),\n                     xy=(ths[idx[i]], precisions[idx[i]]),\n                     xytext=(-40, -50),\n                     textcoords='offset points',\n                     arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"))\n        \n\n\nplt.plot(ths[th_idx], precisions[th_idx], 'ro')\nplt.annotate('precision = %.4f' % (precisions[th_idx]),\n             xy=(ths[th_idx], precisions[th_idx]),\n             xytext=(30, -10),\n             textcoords='offset points',\n             arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"))\nplt.plot(ths[th_idx], recalls[th_idx], 'ro')\nplt.annotate('recall = %.4f' % (recalls[th_idx]),\n             xy=(ths[th_idx], recalls[th_idx]),\n             xytext=(30, 10),\n             textcoords='offset points',\n             arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"))\n\n# Plotting an even better threshold\nth_idx = np.abs(ths - 0.39).argmin()\nplt.axvline(x=ths[th_idx], color='orange', linestyle='--')\n\nplt.plot(ths[th_idx], precisions[th_idx], 'ro')\nplt.annotate('%.4f' % (precisions[th_idx]),\n             xy=(ths[th_idx], precisions[th_idx]),\n             xytext=(-70, 10),\n             textcoords='offset points',\n             arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"))\nplt.plot(ths[th_idx], recalls[th_idx], 'ro')\nplt.annotate('%.4f' % (recalls[th_idx]),\n             xy=(ths[th_idx], recalls[th_idx]),\n             xytext=(-50, -10),\n             textcoords='offset points',\n             arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"))\n\nplt.xlabel('Threshold')\nplt.title('Precision \/ Recall Curves')\nplt.legend()\n\nplt.show()","236a2712":"cm    = confusion_matrix(y_test, y_pred) # rows = truth, cols = prediction\ndf_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\n\nsns.heatmap(df_cm, annot=True, fmt='g')","c6ef426d":"rf_fpr, rf_tpr, thresholds = roc_curve(y_test, y_pred_prob[:,1])\nroc_auc = auc(rf_fpr, rf_tpr)\n\nresults.iloc[-1, results.columns.get_loc('ROC AUC')] = roc_auc\n\nplt.title('Random Forest ROC Curve')\n\nplt.plot(rf_fpr, rf_tpr, label='AUC = %0.4f'% roc_auc)\nplt.legend(loc='lower right')\n\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.001, 1])\nplt.ylim([0, 1.001])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.show()","7e5b9e0e":"# Create model\nnn_model = Sequential()\n\nnn_model.add(Dense(16, input_dim = X_train.shape[1]))\nnn_model.add(BatchNormalization())\nnn_model.add(Activation('relu'))\nnn_model.add(Dense(12))\nnn_model.add(BatchNormalization())\nnn_model.add(Activation('relu'))\nnn_model.add(Dense(1, activation = 'sigmoid'))\n\nnn_model.compile(optimizer=Adam(lr=1e-3) , loss='binary_crossentropy', metrics = [tf.keras.metrics.AUC()])\n\n# Model Summary\nnn_model.summary()","391ab6ee":"%%time\n\nepochs=20\nbatch_size=512\n\nh = nn_model.fit(X_train, y_train,\n                 batch_size=batch_size,\n                 epochs=epochs)","84e59d29":"y_pred_prob = nn_model.predict(X_test)\ny_pred      = (y_pred_prob > 0.5)\n\naccuracy  = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall    = recall_score(y_test, y_pred)\nf1        = f1_score(y_test, y_pred)\n\nprint(\"Neural Network Predicion on Test Set:\")\nprint(\"-------------------------------------\")\nprint(\"Accuracy:\\t%.4f\" % (accuracy))\nprint(\"Precision:\\t%.4f\" % (precision))\nprint(\"Recall:\\t\\t%.4f\" % (recall))\nprint(\"F1 Score:\\t%.4f\" % (f1))\n\nresult_dict = {\n    'Model': 'Keras NN',\n    'Accuracy': accuracy,\n    'Precision': precision,\n    'Recall': recall,\n    'F1 Score': f1\n}\n\ntry:\n    results = results.append(result_dict, ignore_index=True)\nexcept NameError:\n    results = pd.DataFrame([[' Neural Networks', accuracy, precision, recall, f1, 0]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'])","6ec81cce":"precisions, recalls, ths = precision_recall_curve(y_test, y_pred_prob)\n\n# Get the middle threshold, the closest to 0.5 which is used in the classification\nth_idx = np.abs(ths - 0.5).argmin()\n\n# Plotting the precision, recall curves\nplt.plot(ths, precisions[:-1], \"b--\", label=\"Precision\")\nplt.plot(ths, recalls[:-1], \"g--\", label=\"Recall\")\n\n# Plotting a vertical line at threshold = 0.5 which is the one used to predict\nplt.axvline(x=ths[th_idx], color='r', linestyle='--')\n\n# Plotting the intersection of the precision and recall curves\nidx = get_intersections(precisions, recalls)\nplt.plot(ths[idx], recalls[idx], 'ro')\n\nfor i in range(len(idx)):\n    if i > 0:\n        if ((idx[i] - idx[i-1]) \/ idx[i]) > 0.01:\n            plt.annotate('thr = %1.f, prec, rec = %.4f' % (ths[idx[i]], recalls[idx[i]]),\n                         xy=(ths[idx[i]], precisions[idx[i]]),\n                         xytext=(0, -50),\n                         textcoords='offset points',\n                         arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"))\n    else:\n        plt.annotate('thr = %.1f, prec, rec = %.4f' % (ths[idx[i]], recalls[idx[i]]),\n                     xy=(ths[idx[i]], precisions[idx[i]]),\n                     xytext=(-20, -50),\n                     textcoords='offset points',\n                     arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"))\n        \n\n\nplt.plot(ths[th_idx], precisions[th_idx], 'ro')\nplt.annotate('precision = %.4f' % (precisions[th_idx]),\n             xy=(ths[th_idx], precisions[th_idx]),\n             xytext=(15, 20),\n             textcoords='offset points',\n             arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"))\nplt.plot(ths[th_idx], recalls[th_idx], 'ro')\nplt.annotate('recall = %.4f' % (recalls[th_idx]),\n             xy=(ths[th_idx], recalls[th_idx]),\n             xytext=(30, 0),\n             textcoords='offset points',\n             arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"))\n\n# Plotting an even better threshold\nth_idx = np.abs(ths - 0.42).argmin()\nplt.axvline(x=ths[th_idx], color='orange', linestyle='--')\n\nplt.plot(ths[th_idx], precisions[th_idx], 'ro')\nplt.annotate('%.4f' % (precisions[th_idx]),\n             xy=(ths[th_idx], precisions[th_idx]),\n             xytext=(-70, 10),\n             textcoords='offset points',\n             arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"))\nplt.plot(ths[th_idx], recalls[th_idx], 'ro')\nplt.annotate('%.4f' % (recalls[th_idx]),\n             xy=(ths[th_idx], recalls[th_idx]),\n             xytext=(-50, -10),\n             textcoords='offset points',\n             arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"))\n\n\nplt.xlabel('Threshold')\nplt.title('Precision \/ Recall Curves')\nplt.legend()\n\nplt.show()","d1fab07b":"cm    = confusion_matrix(y_test, y_pred) # rows = truth, cols = prediction\ndf_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\n\nsns.heatmap(df_cm, annot=True, fmt='g')","ef7b724d":"nn_fpr, nn_tpr, thresholds = roc_curve(y_test, y_pred_prob)\nroc_auc = auc(nn_fpr, nn_tpr)\n\nresults.iloc[-1, results.columns.get_loc('ROC AUC')] = roc_auc\n\nplt.title('Keras Model ROC Curve')\n\nplt.plot(nn_fpr, nn_tpr, label='AUC = %0.4f'% roc_auc)\nplt.legend(loc='lower right')\n\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.001, 1])\nplt.ylim([0, 1.001])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.show()","29b09c3c":"print(results)","a42f1f48":"plt.title('ROC Curve - Model Comparison')\n\nplt.plot(dt_fpr, dt_tpr, label='Decision Tree')\nplt.plot(rf_fpr, rf_tpr, label='Random Forest')\nplt.plot(nn_fpr, nn_tpr, label='Keras')\nplt.legend(loc='lower right')\n\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.001, 1])\nplt.ylim([0, 1.001])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.show()","f1340330":"In this section, we will create the training and testing sets by splitting the data into **70%, 30%** split.\nChange this value by changing the `test_size` parameter's value in the `train_test_split` function\n\nYou can also remove features that are not very well correlated with `Class`.   \nTo do so, change the `correlation_threshold` parameter's value in `get_features_to_keep`","a06aeb61":"# Data Manipulation","171d05c0":"### Training","e0e50193":"# Conclusion\nLet's print all the results as well as plot all ROC curves","7b6683f7":"We can see that our curve is not bad with an area under the curve `AUC=0.9114`\n\nLet's see if we can improve it using a different model","679fbe49":"### Analysis","c316d177":"### Prediction","39595fe0":"### Analysis","b487a013":"### Training","c40d0924":"## Decision Tree\nThe first model that we will create is a `Decision Tree` one that has the following characteristics:\n* `max_depth=10`\n* `max_leaf_nodes=10`\n\nThe rest will be set to the default values as described in [Decision Tree Classifier Manual](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html)","c7902337":"## Random Forest\nOur second model is a `Random Forst` one which in fact is an ensemble of Decision Tree models. We will use the following characteristics:\n* `max_depth=12`\n* `n_estimators=250`\n* `max_leaf_nodes=10`\n\nThe rest will be set to the default values as described in [Random Forest Classifier Manual](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html)\n\nNote that `n_estimators=250` means that we are creating an ensemble of 250 Decision Tree classifiers.","a0d61c8b":"#### Precision & Recall Curves\nLet's first look at the precision and recall curves and see where they intersect, what values correspond to the threshold taken and see if any other threshold value gives a better result for precision and recall.\nRemember that:\n* `Recall` is the ratio of the detected positives from all the available or true positives\n* `Precision` is the ratio of the true positives out of the predicted ones   \n\nIn other words, if precision is equal 0.72 and recall is 0.76 it means that a model can detect 76% of the fraud transactions and when it claims that a transaction is fraud, it is correct only 72% of the times.   \nOne would like both to be high, unfortunately when precision increases, recall decreases. So a compromise needs to be done for each specific application.   \nIn the case of credit card fraud detection, a high recall is desirable in order to detect all fraud transactions but that means that the precision is low hence many genuine transactions will be flagged as fraud. This is very embarrasing since the credit card company will get heaps of complaints from people whose transaction got rejected.   \nLet's now see how the graphs look like, what threshold value was taken and if any other value would be relatively better.","35f5f975":"### Prediction","bb780fbf":"The threshold used is just above 0.5.   \nThis gave a recall of 77% and a precision of 94%   \nIf we look at the intersection of the 2 curves, we can see that for a threshold of 0.15, recall increases by 7% while precision decreases by 11% which I believe is too much for this small recall increase. Our system can detect an additional 7% of fraud traffic but 11% increase in people screaming at us!!   \nOn the other hand, if we look at the orange vertical line at threshold of 0.39, we can see that we get a recall of 80% (a 3% increase) and a precision of 94.4% (also a slight increase of 0.23%). This is definitely a better choice to consider for our model.","3c9a9d1a":"# Useful Functions\nFunctions that we will be using in different sections below","296319f8":"# Acknowledgement\n* This work has been highly inspired by the work done **Aniruddha Choudhury** [found here](https:\/\/blog.usejournal.com\/credit-card-fraud-detection-by-neural-network-in-keras-4bd81cc9e7fe)\n* A special thanks to **Aur\u00e9lien G\u00e9ron** for his book `Hands-on Machine Learning with Scikit-Learn, Keras & Tensorflow (Second Edition)` which has not been released yet (August 2019) but I had the priviledge to check ... [Check it out](https:\/\/www.amazon.com\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1492032646\/ref=sr_1_1?crid=2T8QXHIGNL9LL&keywords=aurelien+geron&qid=1566807558&s=gateway&sprefix=Geron+au%2Caps%2C250&sr=8-1)","fa85807d":"# Overview \nThis notebook will walk you through the different approaches that can be used to create a robust fraud detection classifier.\nIt starts with the simple Decision Tree model, then the Random Forest model and finally a neural network one.   \n\n**PS:** *In order to get the same results, make sure to switch off GPU*\n\n### Dataset\nThe dataset is composed of 284,807 transactions made by credit cards during two days in September 2013 by European cardholders.\nIt contains 492 frauds out of the 284,807 (0.172% of all transactions) which makes it a highly unbalanced dataset.\nFor more info on the dataset, please check its [Kaggle page](https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud)","a692f494":"It shows that `V2`, `V4` and `V11` are the most correlated with `Class` whereas `V17` is the least    correlated as it has a negative correlation.   \nLet's double check this by plotting the heatmap","68156a21":"From `AUC=0.9114` for the `Decision Tree`, to `AUC=0.9762` a `Random Forest` model, we slightly improved to `AUC=0.9772` for a TensorFlow Keras network","35360084":"The confusion matrix contains 4 sections with rows representing the true values and columns representing the predicted values.\n* ** True Negatives *(Top left)*:** Number of samples that are not fraud which the model correctly predicted as not fraud \n* ** True Positives *(Bottom right)*:** Number of samples that are fraud which the model correctly predicted as fraud\n* ** False Negatives *(Bottom Left)*:** Number of samples that are fraud which the model predicted as not fraud i.e. this is the number of transactions that were able to fool the model\n* ** False Positives *(Top right)*:** Number of samples that are not fraud which the model predicted as fraud i.e. this is the number of transactions that the model mistankenly tried to stop","573107fb":"# Models","cc38a814":"* Read the data from the csv file using `pandas.read_csv`\n* Print the data dimensions which are: `(284807, 31)` i.e. `284807` samples each having 31 columns i.e. 30 input features and 1 output which is the `Class` column\n* Print the first 5 data rows","60c62631":"## TensorFlow Keras Model","c5342ce5":"Here we can see that the threshold that was used is 0.5.   \nThis gave a recall of 74% and a precision of 87%   \nIf we look at the intersection of the 2 curves, we can see that for a threshold of 0.2, recall increases by about 6% while precision decreases by almost 6%   \nIf we look at the orange vertical line at threshold of 0.42, we can see that we get a recall of 77% (a 3% increase) and a precision of 85% (a 2% decrease ... it's even 1.5% decrease).","1b36e29c":"Let's now check the correlation between the different input features with the `Class` column i.e. with the output feature.   \nThis is important in order to determine which inputs are more important than others.","c99827b5":"### Prediction","564c6af4":"Next, let's check if the data is complete","5929710f":"> # Libraries\n\nLet's import the libraries that we will need.\nWe will be mainly using:\n* `pandas` for data handling\n* `sklearn` for DecisionTree and RandomForest\n* `tensoforflow.keras` for the neural network model\nAlong with many others","eaa2a3fb":"Here we can see that the threshold being used is just above 0.6.   \nThis gave a recall of 78% and a precision of 87%   \nIf we look at the intersection of the 2 curves, we can see that for a threshold of 0.2, recall increases by just 1% while precision decreases by 8% which I believe is too much for this small increase in recall. Our system can detect an additional 1% of fraud traffic but 8% increase in people screaming at us!!","9a66164d":"### Analysis","5b718131":"### Training","d6f2e6ec":"The TF Keras NN gave a result almost identical to the Random Forest one however training time is much faster.\n\n## What to do next?\nFeel free to try changing the parameters in order to get better performances for these models.   \nYou can try the following:\n* Change the parameters for the 3 different models in order to get better or bigger models (more units, more layers, different max_depth, ...)\n* What happens when amount is not normalised? Or when V1, ... V28 are normalised?\n* What happens if Time is not dropped?\n* What if the threshold is decreased or increased? Are there better options to get better performance parameters? Especially if the models have been changed and the recall \/ precision analysis is no longer valid.\n\n**All comments are welcome!!**   \nFeel free to upvote if you find this kernel helpful.   \nThank you!!","6726135d":"From `AUC=0.9114` for the `Decision Tree`, we have improved to `AUC=0.9762` for an ensemble of them: a `Random Forest` model.\n\nLet's see if we can improve it using a neural network model","c2d162d1":"Normalise and drop unwanted features"}}