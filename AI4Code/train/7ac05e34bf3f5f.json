{"cell_type":{"1270d7b5":"code","5ffb6c3d":"code","da3256f2":"code","93495294":"code","2c9c0e95":"code","743f5b5f":"code","ee1b1f70":"code","a8055c72":"code","83c4658e":"code","4a360bee":"code","5d3e8dde":"code","49071be5":"code","d478ed21":"code","c1653551":"code","680eb640":"code","8e44b7d2":"code","0f4c27bb":"code","e85a0661":"markdown","f08c2802":"markdown","e874416f":"markdown","325371e1":"markdown","70ea4a7e":"markdown","86202a4e":"markdown"},"source":{"1270d7b5":"import os\nimport time\nimport numpy as np\nimport pandas as pd\nfrom seaborn import countplot,lineplot, barplot\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nfrom bayes_opt import BayesianOptimization\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb","5ffb6c3d":"tr = pd.read_csv('..\/input\/X_train.csv')\nte = pd.read_csv('..\/input\/X_test.csv')\ntarget = pd.read_csv('..\/input\/y_train.csv')\nss = pd.read_csv('..\/input\/sample_submission.csv')","da3256f2":"tr.head()","93495294":"tr.shape, te.shape","2c9c0e95":"countplot(y = 'surface', data = target)\nplt.show()","743f5b5f":"len(tr.measurement_number.value_counts())","ee1b1f70":"tr.shape[0] \/ 128, te.shape[0] \/ 128","a8055c72":"# https:\/\/stackoverflow.com\/questions\/53033620\/how-to-convert-euler-angles-to-quaternions-and-get-the-same-euler-angles-back-fr?rq=1\ndef quaternion_to_euler(x, y, z, w):\n    import math\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.atan2(t0, t1)\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.asin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.atan2(t3, t4)\n\n    return X, Y, Z\n\ndef fe(actual):\n    new = pd.DataFrame()\n    actual['total_angular_velocity'] = (actual['angular_velocity_X'] ** 2 + actual['angular_velocity_Y'] ** 2 + actual['angular_velocity_Z'] ** 2) ** 0.5\n    actual['total_linear_acceleration'] = (actual['linear_acceleration_X'] ** 2 + actual['linear_acceleration_Y'] ** 2 + actual['linear_acceleration_Z'] ** 2) ** 0.5\n    \n    actual['acc_vs_vel'] = actual['total_linear_acceleration'] \/ actual['total_angular_velocity']\n    \n    x, y, z, w = actual['orientation_X'].tolist(), actual['orientation_Y'].tolist(), actual['orientation_Z'].tolist(), actual['orientation_W'].tolist()\n    nx, ny, nz = [], [], []\n    for i in range(len(x)):\n        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n        nx.append(xx)\n        ny.append(yy)\n        nz.append(zz)\n    \n    actual['euler_x'] = nx\n    actual['euler_y'] = ny\n    actual['euler_z'] = nz\n    \n    actual['total_angle'] = (actual['euler_x'] ** 2 + actual['euler_y'] ** 2 + actual['euler_z'] ** 2) ** 5\n    actual['angle_vs_acc'] = actual['total_angle'] \/ actual['total_linear_acceleration']\n    actual['angle_vs_vel'] = actual['total_angle'] \/ actual['total_angular_velocity']\n    \n    def f1(x):\n        return np.mean(np.diff(np.abs(np.diff(x))))\n    \n    def f2(x):\n        return np.mean(np.abs(np.diff(x)))\n    \n    for col in actual.columns:\n        if col in ['row_id', 'series_id', 'measurement_number']:\n            continue\n        new[col + '_mean'] = actual.groupby(['series_id'])[col].mean()\n        new[col + '_min'] = actual.groupby(['series_id'])[col].min()\n        new[col + '_max'] = actual.groupby(['series_id'])[col].max()\n        new[col + '_std'] = actual.groupby(['series_id'])[col].std()\n        new[col + '_max_to_min'] = new[col + '_max'] \/ new[col + '_min']\n        \n        # Change. 1st order.\n        new[col + '_mean_abs_change'] = actual.groupby('series_id')[col].apply(f2)\n        \n        # Change of Change. 2nd order.\n        new[col + '_mean_change_of_abs_change'] = actual.groupby('series_id')[col].apply(f1)\n        \n        new[col + '_abs_max'] = actual.groupby('series_id')[col].apply(lambda x: np.max(np.abs(x)))\n        new[col + '_abs_min'] = actual.groupby('series_id')[col].apply(lambda x: np.min(np.abs(x)))\n\n    return new","83c4658e":"%%time\ntr = fe(tr)\nte = fe(te)\ntr.head()","4a360bee":"tr.head()","5d3e8dde":"le = LabelEncoder()\ntarget['surface'] = le.fit_transform(target['surface'])","49071be5":"tr.fillna(0, inplace = True)\nte.fillna(0, inplace = True)","d478ed21":"tr.replace(-np.inf, 0, inplace = True)\ntr.replace(np.inf, 0, inplace = True)\nte.replace(-np.inf, 0, inplace = True)\nte.replace(np.inf, 0, inplace = True)","c1653551":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=546789)\nsub_preds_rf = np.zeros((te.shape[0], 9))\noof_preds_rf = np.zeros((tr.shape[0]))\nscore = 0\nfor i, (train_index, test_index) in enumerate(folds.split(tr, target['surface'])):\n    print('-'*20, i, '-'*20)\n    \n    clf =  RandomForestClassifier(n_estimators = 200, n_jobs = -1)\n    clf.fit(tr.iloc[train_index], target['surface'][train_index])\n    oof_preds_rf[test_index] = clf.predict(tr.iloc[test_index])\n    sub_preds_rf += clf.predict_proba(te) \/ folds.n_splits\n    score += clf.score(tr.iloc[test_index], target['surface'][test_index])\n    print('score ', clf.score(tr.iloc[test_index], target['surface'][test_index]))\n    importances = clf.feature_importances_\n    indices = np.argsort(importances)\n    features = tr.columns\n\n    hm = 30\n    plt.figure(figsize=(7, 10))\n    plt.title('Feature Importances')\n    plt.barh(range(len(indices[:hm])), importances[indices][:hm], color='b', align='center')\n    plt.yticks(range(len(indices[:hm])), [features[i] for i in indices])\n    plt.xlabel('Relative Importance')\n    plt.show()\n\nprint('Avg Accuracy', score \/ folds.n_splits)","680eb640":"# https:\/\/www.kaggle.com\/artgor\/where-do-the-robots-drive\nimport itertools\n\ndef plot_confusion_matrix(truth, pred, classes, normalize=False, title=''):\n    cm = confusion_matrix(truth, pred)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=(10, 10))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion matrix', size=15)\n    plt.colorbar(fraction=0.046, pad=0.04)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.tight_layout()","8e44b7d2":"plot_confusion_matrix(target['surface'], oof_preds_rf, le.classes_)","0f4c27bb":"ss['surface'] = le.inverse_transform(sub_preds_rf.argmax(axis=1))\nss.to_csv('rf.csv', index=False)\nss.head(10)","e85a0661":"# Robots are smart\u2026 by design !!\n\nWho make those Robots smart? Its you Machine Learning guys !\nIn this project, our task is to help robots recognize the floor surface they\u2019re standing on using data collected from Inertial Measurement Units (IMU sensors).\n\nHope you guys will learn something from this sensor data. Its kind of IOT data, as in IOT, we usually work with sensor data..  \n\n## Its a golden chance to help humanity, by helping Robots !","f08c2802":"## To be Continued..","e874416f":"What's that?\nEach series has 128 measurements. ","325371e1":"So, we have 3810 train series, and 3816 test series.\nLet's engineer some features!","70ea4a7e":"## Feature Engineering","86202a4e":"We need to classify on which surface our robot is standing.\n\nSo, its a simple classification task. Multi-class to be specific."}}