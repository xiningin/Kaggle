{"cell_type":{"cc14c7c8":"code","24bb2d62":"code","a2b8bea3":"code","3b7dcf50":"code","9619992c":"code","94b0e787":"code","62cbb527":"code","193102ce":"code","7a602e76":"code","be37f48e":"code","f59cc8f7":"code","4ca893c0":"code","079bc851":"code","1100f4fb":"code","8fb272c1":"code","1a2f4f70":"code","c220b907":"code","e471d271":"code","69d11387":"code","5c83abfb":"code","4bae64fb":"code","fb7c64e1":"code","38e4d5d5":"code","0cfa86f0":"code","6b4ca089":"code","6c6d380b":"code","d01eb524":"code","9bbff909":"code","e4fbf2bf":"code","504892ac":"code","d4dc5733":"code","a8ba3a85":"code","3e5c0702":"code","667ba209":"code","2c1bf1d0":"code","8109592b":"code","647f91db":"code","b210f320":"code","96db7fa8":"code","961a685e":"code","b45996c8":"code","06658ba3":"code","36a9b137":"code","92c0d20a":"code","d311457c":"code","4e8f9929":"code","7a187f46":"code","ec7bc72f":"code","de9c3d60":"code","bd32fe47":"code","b64a2b2b":"code","9071acac":"code","75cf4e13":"code","3bcd599a":"code","00def6e9":"code","8c8231b3":"code","acc09b36":"code","cff4b896":"code","dfc7fdd0":"code","b75bc3da":"code","a8df5fea":"code","ff09cb4f":"code","fc114b5a":"code","21cc5400":"code","69517006":"code","439d1076":"code","fd59f104":"code","28793ab7":"code","91c97de8":"code","cbdfd6d7":"code","55cc2844":"code","677c0b2c":"code","9f74fc65":"code","5ae45f90":"code","7e6354c9":"code","6d6a5bdf":"code","b2da96b0":"code","7910910c":"code","627f3250":"code","2fadd017":"code","649e2b08":"code","fe440baa":"code","18651eeb":"code","af301130":"code","70e636d8":"code","f31ec37c":"code","d4d58bb2":"code","40cefa20":"code","9963bb44":"code","ee5b0f68":"code","184eac48":"code","9862ae09":"code","dd5df1e2":"code","54aacfbd":"code","f509bd54":"code","fd31a429":"code","bcb709c5":"code","d35b318e":"code","9d4453a1":"code","f5742278":"code","e82c0f59":"code","175624e9":"code","079bc524":"code","0484e779":"code","8cd36b57":"code","2982d7e1":"code","7883c036":"code","efc4a708":"code","3bd42ec6":"code","1d95eb05":"code","93ce1aa5":"code","9df0a526":"code","8f81971e":"code","99dd4d1d":"code","d713130b":"code","a2a91d53":"code","6a74ee30":"code","c1d9551a":"code","8a4a922c":"code","ae2e4052":"code","f8b0e9de":"code","12cb6999":"code","75193aaa":"code","abd656f8":"code","4b06a156":"code","3603a65f":"code","682be8a8":"code","a1940ac7":"code","5bae1c7a":"code","8bca77b3":"code","6c02f53b":"code","1c3b06df":"code","bc8afa56":"code","a86a5f0c":"code","a198ae5e":"code","c251ab48":"code","5757d54e":"code","f7431533":"code","b2d45fb1":"code","dab26239":"code","125d4026":"code","f3d519d0":"code","6987a4e1":"code","57922c9c":"code","71272750":"code","37c0d9b5":"code","4b687223":"code","d698906b":"code","d59732eb":"code","e3ad7bb2":"code","02c8cc83":"code","bd7431b4":"code","7304b4a1":"code","1002a6dd":"code","2b64379f":"code","fdf8e8ef":"code","eb9cdafc":"code","e01ef355":"code","f77f05f8":"code","5c158eaf":"code","a4ae70b2":"code","7c173fc1":"code","5fff562c":"code","7da391c2":"code","4b7fb34d":"code","a34ecad1":"code","313424a3":"code","26b1c3e7":"code","deb3c733":"code","f39733cb":"markdown","c1e390a9":"markdown","5d5064ba":"markdown","5c2ef158":"markdown","71ae9eff":"markdown","974fce4f":"markdown","73962638":"markdown","072ddb88":"markdown","0b8e23e2":"markdown","f3c22ab6":"markdown","de724605":"markdown","4ffe7771":"markdown","77821916":"markdown","184c3739":"markdown","9765ce93":"markdown","5777066c":"markdown","a80948b2":"markdown","9ef6eb4a":"markdown","235c4073":"markdown","d5e923ed":"markdown","9c1fa5dd":"markdown","41dc4381":"markdown","1ed21fad":"markdown","95c0e408":"markdown","aa7cdb4f":"markdown","fd526632":"markdown","6250e901":"markdown","d0e287ae":"markdown","c34d6240":"markdown","0e915297":"markdown","9f1eb558":"markdown","5dc6751c":"markdown","8899b537":"markdown","4ed1cc1d":"markdown","4d36365c":"markdown","833dc5dd":"markdown","bfc11044":"markdown","c1a223a7":"markdown","42c862ad":"markdown","8b8d3677":"markdown","432a78e8":"markdown","3e25feea":"markdown","7fbcce5c":"markdown","23fdd3d6":"markdown","85d12fb1":"markdown","65e30699":"markdown","63349ef2":"markdown","9dc8db3e":"markdown","c8ac2b2b":"markdown","972addd5":"markdown","f6799152":"markdown","2c517eca":"markdown","ee362d2b":"markdown","e732ac8a":"markdown","dd6b6160":"markdown","c6dd4503":"markdown","efe37535":"markdown","5da9eb4f":"markdown","3ac84172":"markdown","e62b95ce":"markdown","06ee658c":"markdown","a49f42e0":"markdown","08788a32":"markdown","5d7531d0":"markdown","b76facd5":"markdown","ad47be40":"markdown","67201228":"markdown","02de3c6a":"markdown","b93c38a4":"markdown","46093201":"markdown","bb87e221":"markdown","43acde26":"markdown","e40ab5c1":"markdown","8ea035b8":"markdown","c7b42233":"markdown","7dea95e7":"markdown","6da25068":"markdown","eea98944":"markdown","049dd0dd":"markdown","7ff9d82b":"markdown","1f4ccefa":"markdown","68062a7c":"markdown","a7812e82":"markdown","b89f91b6":"markdown","4eb043d5":"markdown","35114765":"markdown","046ff728":"markdown","b493bf22":"markdown","67131329":"markdown","88122502":"markdown","dbae4455":"markdown","1d6e1047":"markdown","04c8af90":"markdown","96cd37c4":"markdown","ded4a438":"markdown","bdd8af89":"markdown","f98fe779":"markdown","2208b07b":"markdown","b62d9376":"markdown","27c228e3":"markdown","73a18c2c":"markdown","25db5fa7":"markdown","94f10cca":"markdown","42f89c74":"markdown","81992d5a":"markdown","caef446b":"markdown","c8274982":"markdown","b760b836":"markdown","d164a610":"markdown","c5a10e4b":"markdown","b9ccadd8":"markdown","0e4c650f":"markdown","f329782a":"markdown","3b4fb7ab":"markdown","4912a726":"markdown","58c18f2c":"markdown","c4a90be1":"markdown","0121a3bc":"markdown","c0eb618c":"markdown","0a35a7d2":"markdown","7a727da7":"markdown","adc5653f":"markdown","0c8641e6":"markdown","683380e0":"markdown","544c3e40":"markdown","203a7fb5":"markdown","0c161ee6":"markdown","0a4b5b75":"markdown","da8b714f":"markdown","c52c1ef6":"markdown","6372f97b":"markdown","49645b90":"markdown","204d6a10":"markdown","15c90db6":"markdown","395b3abd":"markdown","1d69a033":"markdown","59604d61":"markdown","482e2028":"markdown","24bba365":"markdown","3db0da3d":"markdown","5e48908b":"markdown","fcf1f993":"markdown","71e67b37":"markdown","18c8d1c8":"markdown","c249c352":"markdown","3a8a79d5":"markdown","fb9f9f55":"markdown","7a0482e6":"markdown","8260286a":"markdown","f5ed0fd5":"markdown","7e076907":"markdown","92d07bba":"markdown","ba4a55d0":"markdown","50d88ca7":"markdown","1890abcc":"markdown","389d3f2b":"markdown","8ff521c1":"markdown","6a9fcdb7":"markdown","f3d75b14":"markdown","9dfd7dd8":"markdown","9cc641e8":"markdown","00429ce5":"markdown","1a9b8eb8":"markdown","19ae3265":"markdown","fce5655e":"markdown","add95706":"markdown","fa801357":"markdown","4bccbf84":"markdown","6db1dbea":"markdown","c2270252":"markdown","605bb722":"markdown","26156c1f":"markdown","dd98faa7":"markdown","96fb1edf":"markdown","117b5c84":"markdown","f82579c5":"markdown","8b7b9a7d":"markdown","cdad870a":"markdown","9c8fd2c2":"markdown","0410156f":"markdown","0d5ee6e5":"markdown","091c6698":"markdown","1c52e200":"markdown"},"source":{"cc14c7c8":"# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)\n\n#for data import and basic oprtaion\nimport pandas as pd\nimport numpy as np\n \n#for visulization and plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n \n#to view the plots in the jupyter notebook inline\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics\nfrom IPython.core.display import display, HTML\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import precision_recall_curve\n \n%matplotlib inline\n \n#to create the confusion matrix\nfrom sklearn import metrics\n \n#to split the dataset into train and test\nfrom sklearn.model_selection import train_test_split\n#or! Earlier train_test_split was in cross_validation\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:90% !important; }<\/style>\"))\npd.set_option('display.max_columns', None)  \npd.set_option('display.expand_frame_repr', False)\npd.set_option('max_colwidth', -1)\n\n","24bb2d62":"# Read columns for previous Leads Data Dictionary source file.\ndata_dic = pd.read_excel('..\/input\/Leads Data Dictionary.xlsx',encoding = \"ISO-8859-1\" \n                         ,skiprows=range(1, 4), usecols=range(1,3))\npd.set_option('display.max_colwidth', -1)\npd.set_option('display.max_columns', None)\ndata_dic = data_dic.rename(columns={ 'Unnamed: 1':'Column', 'Unnamed: 2':'Description'})\ndata_dic","a2b8bea3":"def drop_columns(dataframe, axis =1, percent=0.3):\n    '''\n    * drop_columns function will remove the rows and columns based on parameters provided.\n    * dataframe : Name of the dataframe  \n    * axis      : axis = 0 defines drop rows, axis =1(default) defines drop columns    \n    * percent   : percent of data where column\/rows values are null,default is 0.3(30%)\n    '''\n    df = dataframe.copy()\n    ishape = df.shape\n    if axis == 0:\n        rownames = df.transpose().isnull().sum()\n        rownames = list(rownames[rownames.values > percent*len(df)].index)\n        df.drop(df.index[rownames],inplace=True) \n        print(\"\\nNumber of Rows dropped\\t: \",len(rownames))\n    else:\n        colnames = (df.isnull().sum()\/len(df))\n        colnames = list(colnames[colnames.values>=percent].index)\n        df.drop(labels = colnames,axis =1,inplace=True)        \n        print(\"Number of Columns dropped\\t: \",len(colnames))\n        \n    print(\"\\nOld dataset rows,columns\",ishape,\"\\nNew dataset rows,columns\",df.shape)\n\n    return df\n\ndef column_univariate(df,col,type,nrow=None,ncol=None,hue =None):\n    \n    '''\n    * column_univariate function will plot the graphs based on the parameters.\n    * df      : dataframe name\n    * col     : Column name\n    * nrow    : no of rows in sub plot\n    * ncol    : no of cols in sub plot\n    * type : variable type : continuos or categorical\n                Continuos(0)   : Distribution, Violin & Boxplot will be plotted.\n                Categorical(1) : Countplot will be plotted.\n                Categorical(2) : Subplot-Countplot will be plotted.\n    * hue     : It's only applicable for categorical analysis.\n    \n    '''\n    sns.set(style=\"darkgrid\")\n    \n    if type == 0:\n        \n        fig, axes =plt.subplots(nrows =1,ncols=3,figsize=(12,6))\n        axes [0].set_title(\" Distribution Plot\")\n        sns.distplot(df[col],ax=axes[0])\n        axes [1].set_title(\"Violin Plot\")\n        sns.violinplot(data =df, x=col,ax=axes[1], inner=\"quartile\")\n        axes [2].set_title(\" Box Plot\")\n        sns.boxplot(data =df, x=col,ax=axes[2],orient='v')\n        \n        for ax in axes:\n            ax.set_xlabel('Common x-label')\n            ax.set_ylabel('Common y-label')\n        \n    if  type == 1:\n        total_len = len(df[col])\n        percentage_labels = round((df[col].value_counts()\/total_len)*100,4)\n    \n        temp = pd.Series(data = hue)\n        \n        fig, ax=plt.subplots(nrows =1,ncols=1,figsize=(12,4))\n        ax.set_title(\"Count Plot\")\n        width = len(df[col].unique()) + 6 + 4*len(temp.unique())\n        fig.set_size_inches(width , 7)\n        sns.countplot(data = df, x= col,\n                           order=df[col].value_counts().index,hue = hue)  \n        mystring = col.replace(\"_\", \" \").upper()\n        plt.xlabel(mystring)\n          \n        \n        if len(temp.unique()) > 0:\n            for p in ax.patches:\n                ax.annotate('{:1.1f}%'.format((p.get_height()*100)\/float(len(df))),\n                            (p.get_x()+0.05, p.get_height()+20))  \n        else:\n            for p in ax.patches:\n                height = p.get_height()\n                ax.text(p.get_x() + p.get_width()\/2.,\n                height + 2,'{:.2f}%'.format(100*(height\/total_len)),\n                        fontsize=14, ha='center', va='bottom')\n        del temp\n        \n    elif type == 2:\n        fig, ax = plt.subplots(nrow, ncol, figsize=(14, 10))\n        for variable, subplot in zip(col, ax.flatten()):\n            total = float(len(df[variable]))\n            ax=sns.countplot(data = df, x= variable,ax=subplot,\n                           order=df[variable].value_counts().index) \n            for p in ax.patches:    \n                height = p.get_height()\n                ax.text(p.get_x()+p.get_width()\/2., height + 3, '{:1.2f}%'.format((height\/total)*100),\n                        ha=\"center\") \n    else:\n        exit\n    \n    plt.tight_layout() \n\ndef get_column_dummies(col,df_in):\n    '''\n    * get_column_dummies will get\/map column dummies values based on the parameters.\n    * col   : column name\n    * df_in  : dataframe\n    '''\n    temp = pd.get_dummies(df_in[col], drop_first = True,prefix=col)\n    df_out = pd.concat([df_in, temp], axis = 1)\n    df_out.drop([col], axis = 1, inplace = True)  \n    return df_out\n \ndef column_describe(df,col):\n    '''\n    * column_describe: Describe stastical based on the parameters.\n    * df    : dataframe input\n    * col   : column name\n    '''\n    print(df[col].describe())\n    \ndef box_plot(df_in,numerical_cols):\n    '''\n    * box_plot: Plot box plot  based on the parameters.\n    * df_in    : dataframe input\n    * numerical_cols   : numerical column name\n    '''\n    plt.figure(figsize=(15,10))\n    for var in numerical_cols:\n        plt.subplot(2,5,numerical_cols.index(var)+1)\n        sns.boxplot(y=var,palette='BuGn_r', data=df_in)\n     \n    plt.tight_layout()\n    plt.show()\n","3b7dcf50":"#load the dataset\ndf = pd.DataFrame(pd.read_csv('..\/input\/Leads.csv'))\n#check top records\ndf.head(2) ","9619992c":"#check size of dataset \ndf.shape","94b0e787":"#check dataset columns\ndf.columns","62cbb527":"#check info about the data\ndf.info()","193102ce":"#check info about data types\ndf.dtypes.value_counts()","7a602e76":"#check number of unique classes in dataset\ndf.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","be37f48e":"#check first few records\ndf.head(3)","f59cc8f7":"#check last few records\ndf.tail(3)","4ca893c0":"#describe the dataset for stastical view\ndf.describe()","079bc851":"#list numerical variables\/features\nnumerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\nnumerical_cols","1100f4fb":"#list categorical vriables\/features\ncategorical_cols = df.select_dtypes(include=[np.object]).columns.tolist()\ncategorical_cols","8fb272c1":"#Replace 'NaN' vales with \"Select\" \ndf = df.replace('Select', np.nan)","1a2f4f70":"#Check duplicate values\/row in dataset\nsum(df.duplicated(subset = 'Prospect ID')) == 0","c220b907":"df.isnull().sum()","e471d271":"#Compute percentage missing vallues \nround(100*(df.isnull().sum()\/len(df.index)), 2) ","69d11387":"#Calculate and print percentage of missing values greater than 30% from each column.\npercent_missing = round(df.isnull().sum() * 100 \/ len(df),2)\nmissing_value_df = pd.DataFrame({'column_name': df.columns,'percent_missing': percent_missing})\nmissing_value_df.sort_values('percent_missing', inplace=True)\nmissing_value_df=missing_value_df[missing_value_df['percent_missing'] >30]\nmissing_value_df","5c83abfb":"#Plot percentage column wise missing values.\nfig, ax = plt.subplots(figsize=(10,5))    \nx = missing_value_df.column_name\ny = missing_value_df.percent_missing \n\nsns.set()\nsns.barplot(x,y)\nax = plt.gca()\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x() + p.get_width()\/2.,\n            height + 2,\n            int(height),\n            fontsize=14, ha='center', va='bottom')\nsns.set(font_scale=1.5)\nax.set_xlabel(\"Column Name\")\nax.set_ylabel(\"Frequency of missing values [in %ge] \")\nplt.title(\"Distribution of column wise missing values\")\nplt.xticks(rotation=90)\nplt.show()","4bae64fb":"#Drop missing values more than 70% from column\ndf = df.drop(df.loc[:,list(round(100*(df.isnull().sum()\/len(df.index)), 2)>70)].columns, 1)","fb7c64e1":"#compute column missing values percentage \nprint('Percent of missing \"Lead Quality is %.2f%%' %((df['Lead Quality'].isnull().sum()\/df.shape[0])*100))","38e4d5d5":"#load the describe value\ncolumn_describe(df,'Lead Quality')","0cfa86f0":"#plot countplot for column\ncolumn_univariate(df,'Lead Quality',1)","6b4ca089":"#check unque value for column\ndf['Lead Quality'].unique()","6c6d380b":"#Replace Lead quality missing values with \"Not Sure\"\ndf['Lead Quality'] = df['Lead Quality'].replace(np.nan,'Not Sure')","d01eb524":"#Plot Countplot for column missing values\ncolumn_univariate(df,'Lead Quality',1)","9bbff909":"#compute column missing values percentage \nprint('Percent of missing \"Asymmetrique Activity Index is %.2f%%' %((df['Asymmetrique Activity Index'].isnull().sum()\/df.shape[0])*100))\nprint('Percent of missing \"Asymmetrique Activity Score is %.2f%%' %((df['Asymmetrique Activity Score'].isnull().sum()\/df.shape[0])*100))\nprint('Percent of missing \"Asymmetrique Profile Index is %.2f%%' %((df['Asymmetrique Profile Index'].isnull().sum()\/df.shape[0])*100))\nprint('Percent of missing \"Asymmetrique Profile Score is %.2f%%' %((df['Asymmetrique Profile Score'].isnull().sum()\/df.shape[0])*100))","e4fbf2bf":"#Drop Asymmetrique columns since they got the high missing values which doesn't make sense\ndf = df.drop(['Asymmetrique Activity Index','Asymmetrique Activity Score','Asymmetrique Profile Index','Asymmetrique Profile Score'],1)","504892ac":"#Compute missing values post dropping the Asymmetrique columns.\nround(100*(df.isnull().sum()\/len(df.index)), 2)","d4dc5733":"#Compute missing values for City\nprint('Percent of missing \"City is %.2f%%' %((df['City'].isnull().sum()\/df.shape[0])*100))","a8ba3a85":"#Check City unique values\ndf.City.unique()","3e5c0702":"#Desribe the City column values\ncolumn_describe(df,\"City\")","667ba209":"#Plot countplot to inspect City missing values\ncolumn_univariate(df,\"City\",1)","2c1bf1d0":"#Replace City Column  null values with 'Mumbai'since it appears aprox. 35%.\ndf['City'] = df['City'].replace(np.nan, 'Mumbai')\ncolumn_univariate(df,\"City\",1)","8109592b":"#Compute percentage missing values for Specailization\nprint('Percent of missing \"Specialization is %.2f%%' %((df['Specialization'].isnull().sum()\/df.shape[0])*100))","647f91db":"#Check unique values for Specailization\ndf.Specialization.unique()","b210f320":"#Desribe the Specailization values for stastical view.\ncolumn_describe(df,\"Specialization\")","96db7fa8":"#Plot countplot for Specailization column \ncolumn_univariate(df,\"Specialization\",1)\n  ","961a685e":"#Replace Specailization missing values with \"Others\"\ndf['Specialization'] = df['Specialization'].replace(np.nan, 'Others')\ncolumn_univariate(df,\"Specialization\",1)","b45996c8":"#Check missing values aftr imputing the value.\ndf['Specialization'].isnull().sum()","06658ba3":"#Compute percentage missing values for Tags\nprint('Percent of missing \"Tags is %.2f%%' %((df['Tags'].isnull().sum()\/df.shape[0])*100))","36a9b137":"#check unique value for Tags\ndf.Tags.unique()","92c0d20a":"#Desribe Tags values for stastical view\ncolumn_describe(df,\"Tags\")","d311457c":"#Plot Countplot for Tags\ncolumn_univariate(df,\"Tags\",1)","4e8f9929":"#Replace Tags missing values with \"Will revert after reading the email\"\ndf['Tags'] = df['Tags'].replace(np.nan, 'Will revert after reading the email')\ncolumn_univariate(df,\"Tags\",1)","7a187f46":"#Check null value after imputing na value.\ndf['Tags'].isnull().sum()","ec7bc72f":"#Compute ' What matters most to you in choosing a course' percentage missing values\nprint('Percent of missing \"What matters most to you in choosing a course is %.2f%%' %((df['What matters most to you in choosing a course'].isnull().sum()\/df.shape[0])*100))","de9c3d60":"#Plot countplot for 'What matters most to you in choosing a course'\ncolumn_univariate(df,'What matters most to you in choosing a course',1)","bd32fe47":"#Replace Na values and plot countplot\ndf['What matters most to you in choosing a course'] = df['What matters most to you in choosing a course'].replace(np.nan, 'Better Career Prospects')\ncolumn_univariate(df,'What matters most to you in choosing a course',1)","b64a2b2b":"#Check null values aftr imputing nan value.\ndf['What matters most to you in choosing a course'].isnull().sum()","9071acac":"#Compute percentage missing values for Current occupation\nprint('Percent of missing \"What is your current occupation is %.2f%%' %((df['What is your current occupation'].isnull().sum()\/df.shape[0])*100))","75cf4e13":"#Desribe occupation values for stastical view\ncolumn_describe(df,'What is your current occupation')","3bcd599a":"#Plot countplot for missing values\ncolumn_univariate(df,'What is your current occupation',1)","00def6e9":"#Replace occupation missing values and plot countplot \ndf['What is your current occupation'] = df['What is your current occupation'].replace(np.nan, 'Unemployed')\ncolumn_univariate(df,'What is your current occupation',1)","8c8231b3":"#Check missing values after imputing na value\ndf['What is your current occupation'].isnull().sum()","acc09b36":"#Compute percentage missing values for country\nprint('Percent of missing \"Country is %.2f%%' %((df['Country'].isnull().sum()\/df.shape[0])*100))","cff4b896":"#Describe Country values for stastical view\ncolumn_describe(df,\"Country\")","dfc7fdd0":"#plot countplot for missing values\ncolumn_univariate(df,\"Country\",1)","b75bc3da":"#replace missing values with India and plot countplot for missing values\ndf['Country'] = df['Country'].replace(np.nan, 'India')\ncolumn_univariate(df,\"Country\",1)","a8df5fea":"#Check missing values after imputing the na.\nround(100*(df['Country'].isnull().sum()\/len(df.index)), 2)","ff09cb4f":"#Drop all na values from dataset\ndf.dropna(inplace = True)","fc114b5a":"round(100*(df.isnull().sum()\/len(df.index)), 2)","21cc5400":"#Numerica column list\nnumerical_cols=df.iloc[:, (np.where((df.dtypes == np.int64) | (df.dtypes == np.float64)))[0]].columns.tolist()\nnumerical_cols","69517006":"#Plot outliers using box plot\ncols = ['TotalVisits','Total Time Spent on Website','Page Views Per Visit']\nbox_plot(df,cols)","439d1076":"fig, ax = plt.subplots(figsize=(8,6))\nax.scatter(df['TotalVisits'], df['Total Time Spent on Website'])\nax.set_xlabel('TotalVisits')\nax.set_ylabel('Total Time Spent on Website')\nplt.show()","fd59f104":"sns.jointplot(df['Page Views Per Visit'],df['Total Time Spent on Website'], color=\"b\")\nplt.show()","28793ab7":"#Remove the upper outlier(s) with the 95th percentile and the lower one(s) with the 5th percentile.\nfor col in numerical_cols:\n    percentiles = df[col].quantile([0.05,0.95]).values\n    df[col][df[col] <= percentiles[0]] = percentiles[0]\n    df[col][df[col] >= percentiles[1]] = percentiles[1]","91c97de8":"#Plot outliers using box plot\nbox_plot(df,cols)","cbdfd6d7":"#Count the target value frequency\ndf['Converted'].value_counts()","55cc2844":"#Plot target variable\nlabels = ['Converted', 'Not Converted']\nfig, axs = plt.subplots(1,2,figsize=(14,5))\nsns.countplot(x='Converted',data=df,ax=axs[0])\naxs[0].set_title(\"Lead Conversion Rate\")\naxs[0].set_xlabel(\"0 Lead Not Converted , 1 Lead Converted Successfully\")\naxs[0].set_ylabel(\"Number of Leads\")\ndf.Converted.value_counts().plot(x=None,y=None, kind='pie', ax=axs[1],autopct='%1.2f%%',labels=labels)\naxs[1].set_title(\"Percentage of Lead Conversion Rate\")\nplt.show()\n","677c0b2c":"#Compute percentage of Lead Converted or not \ncount_not_converted = len(df[df['Converted']==0])\ncount_converted = len(df[df['Converted']==1])\npct_of_not_converted = count_not_converted\/(count_converted+count_converted)\nprint(\"percentage of Lead not converted is\", round(pct_of_not_converted*100,2))\npct_of_converted = count_converted\/(count_not_converted+count_converted)\nprint(\"percentage of Lead converted\", round(pct_of_converted*100,2))","9f74fc65":"#Plot histogrm for numerical variables\ncols=['Lead Number',\n 'TotalVisits',\n 'Total Time Spent on Website',\n 'Page Views Per Visit']\nplt.figure(figsize=(20,7))\ndf[cols].hist(bins=10, figsize=(20, 6), layout=(1, 5));\n","5ae45f90":"#Plot countplot for conversion rate\nplt.figure(figsize=(15,7))\nplt.subplot(1,2,1)\nsns.countplot(df[\"Do Not Email\"], hue = df[\"Converted\"]) \nplt.subplot(1,2,2)\nsns.countplot(df[\"Do Not Call\"], hue = df[\"Converted\"])\nplt.show()","7e6354c9":"#Plot countplot for conversion rate\nplt.figure(figsize=(12,6))\nplt.subplot(1,1,1)\nplt.title('Total Visits')\nsns.boxplot(y = 'TotalVisits', x = 'Converted', data = df)\nplt.show()","6d6a5bdf":"#Plot countplot for conversion rate\nplt.figure(figsize=(12,6))\nplt.subplot(1,1,1)\nplt.title('Total Time Spent on Website vs Converted')\nsns.boxplot(y = 'Total Time Spent on Website', x = 'Converted', data = df)\nplt.show()","b2da96b0":"#Plot countplot for conversion rate\nplt.figure(figsize=(10,6))\nplt.subplot(1,1,1)\nplt.title('Page Views Per Visit vs Converted')\nsns.boxplot(y = 'Page Views Per Visit', x = 'Converted', data = df)\nplt.show()","7910910c":"#Plot countplot for conversion rate\ncolumn_univariate(df,\"Lead Origin\",type=1,hue='Converted')","627f3250":"#Plot countplot for conversion rate\ncolumn_univariate(df,\"Lead Source\",type=1 ,hue='Converted')","2fadd017":"#Replace Lead Source value with proper Case\ndf['Lead Source'] = df['Lead Source'].replace(['google'], 'Google')\ndf['Lead Source'] = df['Lead Source'].replace(['Click2call', 'Live Chat', 'NC_EDM', 'Pay per Click Ads', 'Press_Release',\n  'Social Media', 'WeLearn', 'bing', 'blog', 'testone', 'welearnblog_Home', 'youtubechannel'], 'Others')","649e2b08":"#Plot countplot for conversion rate\ncolumn_univariate(df,\"Lead Source\",1,hue='Converted')","fe440baa":"#Describe the column values for stastical view\ncolumn_describe(df,'Last Activity')","18651eeb":"column_univariate(df,\"Last Activity\",1, hue = \"Converted\")","af301130":"#Replace Last Activity col. values with Other Activity.\ndf['Last Activity'] = df['Last Activity'].replace(['Had a Phone Conversation', \n                                                   'View in browser link Clicked', \n                                                   'Visited Booth in Tradeshow',\n                                                   'Approached upfront',\n                                                    'Resubscribed to emails',\n                                                   'Email Received', \n                                                   'Email Marked Spam'], \n                                                   'Other_Activity')","70e636d8":"#plot countplot for conversion rate\ncolumn_univariate(df,\"Last Activity\",1, hue = \"Converted\")","f31ec37c":"#Describe country values for stastical view.\ncolumn_describe(df,\"Country\")","d4d58bb2":"#Describe country values for stastical view.\ncolumn_describe(df,'Specialization')","40cefa20":"#Replace Specialization vlues with other Specialization.\ndf['Specialization'] = df['Specialization'].replace(['Others'], 'Other_Specialization')","9963bb44":"#Plot countplot for conversion raste\ncolumn_univariate(df,\"Specialization\",1, hue = \"Converted\")","ee5b0f68":"#Describe occupation for stastical views\ncolumn_describe(df,'What is your current occupation')","184eac48":"#Replace occupation with other occupation\ndf['What is your current occupation'] = df['What is your current occupation'].replace(['Other'], 'Other_Occupation')","9862ae09":"#plot countplot for conversion rate\ncolumn_univariate(df,'What is your current occupation',1,hue = \"Converted\")","dd5df1e2":"#Describe column values for stastical view\ncolumn_describe(df, 'What matters most to you in choosing a course')","54aacfbd":"#Plot countplot for conversion rate\ncolumn_univariate(df,'What matters most to you in choosing a course',1)","f509bd54":"#Plot countplot for less informative features\ncol=['Search', \n'Magazine',\n'Newspaper Article',\n'X Education Forums',\n'Newspaper',\n'Digital Advertisement',\n'Through Recommendations',\n'Receive More Updates About Our Courses',\n'Update me on Supply Chain Content',\n'Get updates on DM Content',\n'I agree to pay the amount through cheque',\n'A free copy of Mastering The Interview']\n \ncolumn_univariate(df,col,2,3,4)","fd31a429":"#Desribe col values for stastical view\ncolumn_describe(df,'Tags')","bcb709c5":"#Plot countplot for conversion rate\ncolumn_univariate(df,\"Tags\",1,hue = \"Converted\")","d35b318e":"#Replace Tags values with Other Activity.\ndf['Tags'] = df['Tags'].replace(['In confusion whether part time or DLP', 'in touch with EINS','Diploma holder (Not Eligible)',\n                                     'Approached upfront','Graduation in progress','number not provided', 'opp hangup','Still Thinking',\n                                    'Lost to Others','Shall take in the next coming month','Lateral student','Interested in Next batch',\n                                    'Recognition issue (DEC approval)','Want to take admission but has financial problems',\n                                    'University not recognized'], 'Other_Tags')","9d4453a1":"#Plot countplot for conversion rate\ncolumn_univariate(df,\"Tags\",1,hue = \"Converted\")","f5742278":"#Desribe column values for stastical view\ncolumn_describe(df,'Lead Quality')","e82c0f59":"#Plot countplot for conversion rate\ncolumn_univariate(df,\"Lead Quality\",1,hue = \"Converted\" )","175624e9":"#Desribe column values for stastical view\ncolumn_describe(df,'City')","079bc524":"#Plot countplot for conversion rate\ncolumn_univariate(df,\"City\",1,hue = \"Converted\" )","0484e779":"#Desribe column values for stastical view\ncolumn_describe(df,'Last Notable Activity')","8cd36b57":"#Plot countplot for conversion rate\ncolumn_univariate(df,'Last Notable Activity',1,hue = \"Converted\" )","2982d7e1":"#Drop less information columns.\ndf = df.drop(['Lead Number','What matters most to you in choosing a course',\n              'Search','Magazine','Newspaper Article','X Education Forums',\n              'Newspaper',  'Digital Advertisement','Through Recommendations',\n              'Receive More Updates About Our Courses','Update me on Supply Chain Content',\n              'Get updates on DM Content','I agree to pay the amount through cheque',\n              'A free copy of Mastering The Interview','Country'],1)","7883c036":"#Check columns name.\ndf.columns","efc4a708":"#List the first few records\ndf.head()","3bd42ec6":"#Binary values conversion using encodning technique.\ncols =  ['Do Not Email', 'Do Not Call']\n\ndef binary_map_value(x):\n    return x.map({'Yes': 1, \"No\": 0})\n\ndf[cols] = df[cols].apply(binary_map_value)","1d95eb05":"#Convert categorical variable to dummy\ncategorical = ['Lead Origin', 'Lead Source', 'Last Activity', \n               'Specialization','What is your current occupation',\n                'Tags','Lead Quality','City','Last Notable Activity']\nfor col in categorical:\n    df=get_column_dummies(col,df)\ndf.head()","93ce1aa5":"# Putting feature variable to X and outcmoe variable to y\nX = df.drop(['Prospect ID','Converted'], axis=1)\ny = df['Converted']","9df0a526":"#Check first few records\nX.head()","8f81971e":"#Split the dataset \nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","99dd4d1d":"#Create StandardScaler instance\nscaler = StandardScaler()","d713130b":"#Fit and transform scaler\ncols = ['TotalVisits','Total Time Spent on Website','Page Views Per Visit']\nX_train[cols] = scaler.fit_transform(X_train[cols])\nX_train.head()","a2a91d53":"#Compute percentage of Lead Converted or not \ncount_not_converted = len(df[df['Converted']==0])\ncount_converted = len(df[df['Converted']==1])\npct_of_not_converted = count_not_converted\/(count_converted+count_converted)\nprint(\"Percentage of Lead not converted is\", round(pct_of_not_converted*100,2))\npct_of_converted = count_converted\/(count_not_converted+count_converted)\nprint(\"Percentage of Lead converted\", round(pct_of_converted*100,2))","6a74ee30":"# Logistic regression model\nX = build_state_model(X_train,y_train)","c1d9551a":"# Create correlation matrix\ncorr_matrix = X_train.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.7 and less than -0.7\nto_drop = [column for column in upper.columns if any(upper[column] > 0.7) | any(upper[column] < -0.7)]\nto_drop","8a4a922c":"plt.figure(figsize = (18,12))  \nresult_corr=X_train.corr(method='pearson')\nsns.heatmap(result_corr[(result_corr >= 0.7) | (result_corr <= -0.7)], \n            cmap='YlGnBu', vmax=1.0, vmin=-1.0, linewidths=0.1,\n            annot=True, annot_kws={\"size\": 10}, square=True);","ae2e4052":"X_train.drop(X_train[to_drop], axis=1)\nX_train.columns.tolist()","f8b0e9de":"from sklearn.linear_model import LogisticRegression","12cb6999":"logreg = LogisticRegression()","75193aaa":"from sklearn.feature_selection import RFE\nrfe = RFE(logreg, 20)              \nrfe = rfe.fit(X_train, y_train)","abd656f8":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","4b06a156":"X_train_rfe = X_train[X_train.columns[rfe.support_]]\nX_train_rfe.columns.tolist()","3603a65f":"X_train_rfe = build_state_model(X_train_rfe,y_train)","682be8a8":"X_train_rfe = X_train_rfe.drop([\"Tags_invalid number\"], axis = 1)\nX_train_rfe = build_state_model(X_train_rfe,y_train)","a1940ac7":"X_train_rfe = X_train_rfe.drop([\"Tags_wrong number given\"], axis = 1)\nX_train_rfe = build_state_model(X_train_rfe,y_train)","5bae1c7a":"X_train_rfe = X_train_rfe.drop([\"Lead Origin_Lead Import\"], axis = 1)\nX_train_rfe = sm.add_constant(X_train_rfe) #Adding the constant\nlm = sm.GLM(y_train,X_train_rfe,family = sm.families.Binomial()).fit() # fitting the model\nprint(\"----------------------------------- Model Summary -----------------------------------\\n\") # model summary\nprint(lm.summary()) # model summary\nprint(\"\\n-------------------------------------------------------------------------------------\")\nprint(get_vifs(X_train_rfe))\nprint(\"\\n-------------------------------------------------------------------------------------\")\n","8bca77b3":"X_train_rfe.corr()\nplt.figure(figsize = (18,12))  \nresult_corr=X_train_rfe.corr(method='pearson')\nsns.heatmap(result_corr[(result_corr >= 0.5) | (result_corr <= -0.5)], \n            cmap='YlGnBu', vmax=1.0, vmin=-1.0, linewidths=0.1,\n            annot=True, annot_kws={\"size\": 10}, square=True);","6c02f53b":"# Getting the predicted values on the train set\ny_train_pred = lm.predict(X_train_rfe)\ny_train_pred[:10]","1c3b06df":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","bc8afa56":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Converted_Prob':y_train_pred})\ny_train_pred_final['Prospect ID'] = y_train.index\ny_train_pred_final.head()","a86a5f0c":"y_train_pred_final['Predicted'] = y_train_pred_final.Converted_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","a198ae5e":"printConfusionMatrixValues(y_train_pred_final.Converted, y_train_pred_final.Predicted)","c251ab48":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Converted_Prob, drop_intermediate = False )","5757d54e":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Converted_Prob)","f7431533":"auc = auc_value(fpr,tpr)\nround(auc,2)","b2d45fb1":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Converted_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","dab26239":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","125d4026":"# Let's plot accuracy sensitivity and specificity for various probabilities.\nplt.figure(figsize=(10, 10))\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.plot((0.3,0.3),(0.0,0.97),'r-',linestyle='--')\nplt.show()","f3d519d0":"#### From the curve above, 0.3 is the optimum point to take it as a cutoff probability.\ny_train_pred_final['Final_Predicted'] = y_train_pred_final.Converted_Prob.map( lambda x: 1 if x > 0.3 else 0)\ny_train_pred_final.head()","6987a4e1":"y_train_pred_final['Lead_Score'] = y_train_pred_final.Converted_Prob.map( lambda x: round(x*100))\ny_train_pred_final.head(5)\n\n","57922c9c":"y_train_pred_final.sort_values('Prospect ID', ascending=False).drop_duplicates('Lead_Score').head(5).sort_values('Lead_Score')","71272750":"from sklearn.metrics import classification_report\nprint(classification_report(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted))","37c0d9b5":"printConfusionMatrixValues(y_train_pred_final.Converted, y_train_pred_final.Final_Predicted)","4b687223":"y_train_pred_final.head()","d698906b":"#Looking at the confusion matrix again\nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.Predicted )\nconfusion","d59732eb":"round(precision_score(y_train_pred_final.Converted , y_train_pred_final.Predicted),2)","e3ad7bb2":"round(recall_score(y_train_pred_final.Converted, y_train_pred_final.Predicted),2)","02c8cc83":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Converted_Prob)","bd7431b4":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.plot((0.43,0.43),(0.0,0.97),'b-',linestyle='--')\nplt.show()","7304b4a1":"X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.transform(X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])\nX_test.head()","1002a6dd":"temp_df = X_train_rfe.drop('const',axis=1)\nX_test = X_test[temp_df.columns]\nX_test.head()","2b64379f":"X_test_sm = sm.add_constant(X_test)\ny_test_pred = lm.predict(X_test_sm)\ny_test_pred[:10]","fdf8e8ef":"# Converting y_pred to a dataframe which is an array\ny_pred_df = pd.DataFrame(y_test_pred)\ny_pred_df= y_pred_df.rename(columns={ 0 : 'Converted_Prob'})\ny_pred_df.head()","eb9cdafc":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","e01ef355":"y_test_df['Prospect ID'] = y_test_df.index\ny_test_df.head()","f77f05f8":"# Removing index for both dataframes to append them side by side \ny_pred_df.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","5c158eaf":"# Appending y_test_df and y_pred_df\ny_pred_final = pd.concat([y_test_df, y_pred_df],axis=1)\ny_pred_final.head()","a4ae70b2":"# Rearranging the columns\ny_pred_final = y_pred_final.reindex_axis(['Prospect ID','Converted','Converted_Prob'], axis=1)\ny_pred_final.head()","7c173fc1":"fpr, tpr, thresholds = metrics.roc_curve( y_test_df.Converted, y_pred_df.Converted_Prob, drop_intermediate = False )\n\ndraw_roc(y_test_df.Converted, y_pred_df.Converted_Prob)","5fff562c":"auc = auc_value(fpr,tpr)\nround(auc,2)","7da391c2":"y_pred_final['Lead_Score'] = y_pred_final.Converted_Prob.map( lambda x: round(x*100))\ny_pred_final.head()","4b7fb34d":" y_pred_final.sort_values('Prospect ID', ascending=False).drop_duplicates('Lead_Score').head(5).sort_values('Lead_Score')\n","a34ecad1":"y_pred_final['Final_Predicted'] = y_pred_final.Converted_Prob.map(lambda x: 1 if x > 0.3 else 0)\ny_pred_final.head()","313424a3":"printConfusionMatrixValues(y_pred_final.Converted, y_pred_final.Final_Predicted)","26b1c3e7":"feature_importance(lm,X_test_sm,3)","deb3c733":"factors = [('Dataset', ['Train','Train','Test' ]),\n           ('Threshhold Value', ['0.5','0.30','0.30']),\n           ('Accuracy'        , ['0.92','0.92','0.91']),\n           ('Sensitivity'        , ['0.88','0.92','0.91']),\n           ('Specificity'        , ['0.95','0.92','0.90']),\n           ('False Postive Rate'        , ['0.05','0.08','0.1']),\n           ('Positive Predictive Value'        , ['0.91','0.87','0.84']),\n           ('Negative Predictive value'        , ['0.93','0.95','0.95']),\n           ('Precision'        , ['0.91','0.90','0.85']),\n           ('Recall'        , ['0.91','0.92','0.90']),\n           ('F1 value'        , ['0.91','0.92','0.87']),\n           ('AUC'        , ['0.91','0.96','0.96'])\n         ]\ndfsi = pd.DataFrame.from_items(factors)\ndfsi\n\n","f39733cb":"Since customer didn't enter or left blanks in the this column we may be imputed by 'Better Career Prospects')","c1e390a9":"### 5.3 Feature Selection Using Recursive Feature Elimination(RFE)\nWe will be using Recursive feature elimination is an optimization technique for selecting the best performing subset of features.","5d5064ba":" It can be viewed from above output that there are quite few numbers of columns having missing values.","5c2ef158":"#### 3.3.7 What matters most to you in choosing a course\nAn option selected by the customer indicating what is their main motto behind doing this course","71ae9eff":"- It can been seen that Rest of missing values are less than 1.5% Sot its better to drop these rows.Let us check the missing value in each column to understand the frequency of missing values.","974fce4f":"   - Asymmetrique Activity Index  \n   - Asymmetrique Profile Index    \n   - Asymmetrique Activity Score     \n   - Asymmetrique Profile Score   ","73962638":"- All variables have p-value < `0.05`. \n- All the features have very low VIF values, meaning, there is `hardly any muliticollinearity` among the features.\n- The overall accuracy of `0.91` at a probability threshold of 0.3 on the test dataset is also very acceptable. \nThe dependent variable value was predicted as per the following threshold values of Conversion probability using Logistic Regression","072ddb88":"- No missing values in column.","0b8e23e2":"- Most of customer prefered - Do Not Email & Do Not Call option.","f3c22ab6":"### 5.12 Determine Most Important Feature  \nWe will determine and list `Top 3` most important feature thast contribute most to a Lead gets converted ","de724605":"Import LogisticRegression() module to create Logistic Regression instacne to proceed with RFE process.","4ffe7771":"### 3.1 Predict Variable (Target Variable), 'Converted'","77821916":"#### Observations:\n- A heat map consisting of the final 17 features proves that there is no significant correlation between the independent variables\/features.\n","184c3739":"- At about a `threshold of 0.3`, the curves of accuracy, sensitivity and specificity intersect, and they all take a value of around `90%`.","9765ce93":"#### 3.3.1 Lead Origin Source Conversion Rate\nThe origin identifier with which the customer was identified to be a lead. Includes API, Landing Page Submission, etc.","5777066c":"We will replace the upper outlier(s) with the 95th percentile and the lower one(s) with the 5th percentile.","a80948b2":"As we have learnt from dataset that many columns having \"Select\" values.The main reason behind this is because customer\/website visitor did not\nselect any option from the list.Hence,column data shows select which is default value that being stored as part of workflow.However,\nthe select vslues sre as good as NULL since there was no selection\/action from customer.\nAs we can see that there are select values for many column.\nWe will convert 'Select' values to NaN.","9ef6eb4a":"###  1.3 Number of unique data in each Object Column","235c4073":"Converted is the target variable, Indicates whether a **lead** has been successfully **Converted ?(binary: \u201c1\u201d, means \u201cYes\u201d, \u201c0\u201d means \u201cNo\u201d)**","d5e923ed":"Around 35% of the data is Mumbai so we can impute Mumbai in the missing values.","9c1fa5dd":"### 4.6 Checking the Conversion Rate","41dc4381":"#### 3.3.2 Lead Source","1ed21fad":"#### 5.14.1 Assigning 'Final_Predicted' value for Test Data based on cut-off 0.3","95c0e408":"#### 2.3.3 Handling - City\nThe city of the customer.","aa7cdb4f":"- There are mix types of columns -Objects,Float,and Integer.","fd526632":"#### 2.4.3 Visualizing  the Outliers Post Removal using Box Plot","6250e901":"- Precision is the ability of a classifier  not to label an instance positive that is actually negative. \n- Recall is the ability of a classifier to find all positive instances.\n- The F1 score is a weighted harmonic mean of precision and recall such that the best `score is 1.0` and the worst is `0.0`. \n- Support is the number of actual occurrences of the class in the specified dataset.","d0e287ae":"## 4. Data Preparation","c34d6240":"#### 2.3.8 Handling - Country\nThe country of the customer.","0e915297":"#### 5.9.1 Creating Confusion Matrix for Model Validation","9f1eb558":"Let's keep considerable last activities as such and club all others to \"Other_Activity\"","5dc6751c":"- Around 20% are numerical variables in dataset.","8899b537":"- It maybe the case that custmor has not entered any specialization if his\/her option is not availabe on the list,\nmay not have any specialization or is a student or category that not listed in option.\nHence we can make a category \"Others\" for missing values.We will replace 'NaN'values with 'Others'.\n","4ed1cc1d":"### 1.5 Check Last few records of the data","4d36365c":"Blanks in the tag column may be imputed by 'Will revert after reading the email'.","833dc5dd":"#### 5.13.5.6  Top 5 Lead Score ","bfc11044":"#### 3.2.3 Total Time Spent on website by the Customer\nThe total number of visits made by the customer on the website.","c1a223a7":"- ROC curve is more towards the top left corner of the graph, the model is deemed to be more accurate. Hence, a `greater area(0.96)` under the curve(AUC) would mean the model is more accurate.","42c862ad":"### 2.3 Column wise Missing value Handling","8b8d3677":"### 2.4 Outliers Analysis","432a78e8":"- We have almost `38% leads conversion rate`.","3e25feea":"- `Precision(0.92)`, that a predicted `Converted` is actually a `Converted`.","7fbcce5c":"- No missing values in column.","23fdd3d6":"- `Accuracy(0.92)` refers to the closeness of a measured value to actual value. Value is high indicates model is good.\n- `Sensitivity(0.92)` also called true positive rate, number of actual 'Converted' correctly predicted.\n- `Specificity(0.92)` also called true negative rate, number of actual 'Not Converted' correctly predicted.\n- `False Positive Rate(0.08)` also called (1-Specificity), number of actual 'Not Converted' incorrectly predicted.\n- `Positive Predictive Value(0.87)`, acutal Converted predicted correctly.\n- `Negative Predictive Value(0.95), acutal Not Converted predicted correctly.","85d12fb1":"#### 5.13.3 Predicting values for Test Data","65e30699":"#### 2.2.5 Droping columns having more than 70% missing values.","63349ef2":"#### 5.2.1 Looking at the highly correlated Columns","9dc8db3e":"#### 2.4.2 Removing Outliers ","c8ac2b2b":"#### 5.2.3 Handling highlty Correlated Features\n- Remove the highly correlated (>=0.7 and <=-0.7) features.","972addd5":"## Problem Statement\nAn education company named **X Education** sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses. \n\n \n\nThe company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos.  \n\nWhen these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals. \n\nOnce these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not.  The typical lead conversion rate at X education is around 30%.\n \n\nNow, although X Education gets a lot of leads, its lead conversion rate is very poor. For example, if, say, they acquire **100 leads in a day, only about 30** of them are converted. To make this process more efficient, the company wishes to identify the most potential leads, also known as  \u2018**Hot Leads**\u2019 \n\nIf they successfully identify this set of leads, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone. A typical lead conversion process can be represented using the following funnel:\n![image.jpg](attachment:image.jpg)\n\n\n\nLead Conversion Process - Demonstrated as a funnel\nAs you can see, there are a lot of leads generated in the initial stage (top) but only a few of them come out as paying customers from the bottom. \n\nIn the middle stage, you need to nurture the potential leads well (i.e. educating the leads about the product, constantly communicating etc. ) in order to get a higher lead conversion. \n\nX Education has appointed you to help them select the most promising leads, i.e. the leads that are most likely to convert into paying customers.  \nThe company requires you to build a model wherein you need to assign a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance.\n\nThe **CEO**, in particular, has given a ballpark of the target lead conversion rate to be around **80%**.\n\n\n \n\n### Data\n\nYou have been provided with a leads dataset from the past with around 9000 data points. This dataset consists of various attributes such as Lead Source, Total Time Spent on Website, Total Visits, Last Activity, etc. which may or may not be useful in ultimately deciding whether a lead will be converted or not. The target variable, in this case, is the column **\u2018Converted\u2019** which tells whether a past lead was converted or not wherein **1 means it was converted and 0 means it wasn\u2019t converted**.\n\nAnother thing that you also need to check out for are the levels present in the categorical variables.Many of the categorical variables have a level called **'Select'** which needs to be handled because it is as good as a null value.  \n\n\n\n \n\n### Goal\n\n\nBuild a logistic regression model to assign a lead score between 0 and 100 to each of the leads which can be used by the company to target potential leads. A **higher score** would mean that the lead is **hot**, i.e. is most likely to convert whereas a lower score would mean that the lead is **cold** and will mostly not get converted.\n\n**Acknowledgement** : Jointly worked with Naman Mehta.","f6799152":"- Numerical variables seems to be normally distributed with little skewness except the Converted feature.","2c517eca":"### 1.7 List out Numercial and Categorical variable","ee362d2b":"- As noticed above,the highest outliers are from 'TotalVisits' and 'Page Views Per Visit' which can be noticed visuallly in box plot.","e732ac8a":"#### 2.2.3 Percentage of missing values greater than 30 % in dataset.","dd6b6160":"- Median for converted and unconverted leads is the same.Nothing can be said specifically for lead conversion from Page Views Per Visit","c6dd4503":"- No missing values in column.","efe37535":"#### 2.4.1 Visualizing  Outliers using Box Plot","5da9eb4f":"- \"Tags_invalid number\" has the high P-Value so dropping the column","3ac84172":"- Most entries are 'No'.Hence,no observation can be made.","e62b95ce":"- \"Tags_wrong number given\" has the high P-Value so dropping the column","06ee658c":"#### 2.3.4 Handling - Specailization\nThe industry domain in which the customer worked before. Includes the level 'Select Specialization' which means the customer had not selected this option while filling the form.","a49f42e0":"#### 3.3.4 Country","08788a32":"- It can be noticed from box plot there are no outliers.","5d7531d0":"The most important driver variable for prediction of price after Uni\/Bivariate analysis :\n    \n- Converted\n- Last Notable Activity\n- City\n- Lead Quality\n- Tags\n- What matters most to you in choosing a course\n- What is your current occupation\n- Specialization\n- Last Activity\n- Lead Source\n- Lead Origin\n- Page Views Per Visit\n- Total Time Spent on Website \n- TotalVisits\n- Do Not Email\n- Do Not Call","b76facd5":"####  5.13.4.1 Plotting the ROC Curve For Test Data","ad47be40":"- There are few columns have missing\/NA values range varies between 37% to 79%.","67201228":"### 5.6 Building 4th Model after Removing \"Tags_wrong number given \"","02de3c6a":"#### 5.11.3 Creating Confusion Matrix for Model Validation","b93c38a4":"__Conclusion__\n\nThe aim is to build a logistic regression model to assign a lead score between 0 and 100 to each of the leads which can be used by the company to target potential leads. A\u00a0higher score\u00a0would mean that the lead is\u00a0hot, i.e. is most likely to convert whereas a lower score would mean that the lead is\u00a0cold\u00a0and will mostly not get converted. \nAn Exploratory data analysis has been used to understand, analyze  and visualizing the data to assess patterns, identify the data characteristics to summarize the contents of a dataset. The different graphs were generated, many observations have been made and information's were inferred.\n\nLogistic Regression model has been built to predict the Lead Conversion probability for each lead such that overall model accuracy: 91%, Sensitivity: 91% and Specificity: 90%. Having probability threshold of 0.3 on the test dataset is very acceptable which in line with business goals. The features which contributed the most to a Lead Conversion were identified to understand how a lead score increases\/decreases with each features. ","46093201":"### 5.4 Building 2nd Model after Feature Selection using RFE","bb87e221":"#### 5.11.1 Assigning Lead Score\nLet us calculate the Lead Score using equation below.\n\n__Lead Score = 100 * Converted_Prob__","43acde26":"### 1.2 Get Insights from dataset","e40ab5c1":"There is too much variation in thes parameters so its not reliable to impute any value in it.As we have noticed earlier that there are 45% null values in each column which doesn't make any sense.Hence,we need to drop these columns.\n\n- Asymmetrique Activity Index : 45.65%\n- Asymmetrique Profile Index  : 45.65%\n- Asymmetrique Activity Score : 45.65%\n- Asymmetrique Profile Score  : 45.65%\n","8ea035b8":"### 5.1 Build 1st Model with all Features ","c7b42233":"### 4.2 Binary Encoding \nWe will use binary encodning to convert column binary values :Yes\/No to 1\/0","7dea95e7":"- Lead Quality,'Might be'has high conversion rate.","6da25068":"- Our classes are imbalanced, and the ratio of no-converted to converted instances is *82:37* ","eea98944":"#### 2.2.4 Graphical representation of the Missing Values","049dd0dd":"#### 2.4.1.1 Visualizing  Outliers using Scatter Plot","7ff9d82b":"#### 3.2.2 Total Number of Visits made by the Customer on the website  ","1f4ccefa":"Approx. 62% entries are of Unemployed so we can impute \"Unemployed\" in it.","68062a7c":"- There is no duplicste values in dataset","a7812e82":"### Summary Exploratory Data Analysis:","b89f91b6":"#### 5.13.2 Selecting same set of Columns as Train Data","4eb043d5":"#### Model Evaluation on Test Data Set based on sensitivity-specificity view (having cut-off @ `0.3`)\n\n- `Accuracy(0.91)`refers to the closeness of a measured value to actual value. Value is high indicates model is good.\n- `Sensitivity(0.91)` also called true positive rate, number of actual 'Converted' correctly predicted.\n- `Specificity(0.9)` also called true negative rate, number of acutal 'Not Converted' correctly predicted.\n- `FalsePositive Rate(0.1)` also called (1-Specificity), number of actual 'Not Converted' incorrectly predicted.\n- `Positive Predictive Value(0.84)`, acutal Converted predicted correctly.\n- `Negative Predictive Value(0.95)`, acutal Not Converted predicted correctly.","35114765":"- There are 9240 rows and 37 columns in the dataset.Let is review the Summary of the basic information about this DataFrame and its data.","046ff728":"#### 3.2.1 Customer Preference to inform about Course through Email or Call?\n- Do Not Email : An indicator variable selected by the customer wherein they select whether of not they want to be emailed about the course or not.\n- Do Not Call  : An indicator variable selected by the customer wherein they select whether of not they want to be called about the course or not.","b493bf22":"#### 5.7.1 Correlation Matrix","67131329":"- Working Professionals going for the course have high chances of joining it.\n- Unemployed leads are the most in numbers but has around 30-35% conversion rate.","88122502":"### 2.2 Handling Missing Data","dbae4455":"### Import Libraries and set required parameters","1d6e1047":"- Most of the lead have their Email opened as their last activity.\n- Conversion rate for leads with last activity as SMS Sent is almost 60%.","04c8af90":"#### 5.13.1 Scalling Test Data","96cd37c4":"- The summary above indicates that there are  three quartiles, mean, count, minimum and maximum values and the standard deviation for data distribution. In dataset 'mean is almost three times of standard deviation' which indicates that there are few outliers in some of numeric columns.However,median value is very close to mean value.","ded4a438":"### 5.7 Building 5th Model after Removing \"Lead Origin_Lead Import\"","bdd8af89":"#### 2.3.7 Handling - Occupation\nIndicates whether the customer is a student, umemployed or employed.","f98fe779":"#### 5.11.1.1 Top 5 Lead Score","2208b07b":"#### 2.1.2 Checking Duplicate Values\/Row","b62d9376":"- Most of variables are categorical type in dataset.","27c228e3":"### 5.13 Making predictions on the Test Set","73a18c2c":"- SMS Sent ,Email-Opened last and Modified Last notable activity for successful conversion rate.While other activities are very minimal.","25db5fa7":"- As per data dictionary the Lead quality is based on the intution of employee, so if left blank does not make any sense.Hence,Its better to impute **'Not Sure'** in NaN safely to keep column informative.","94f10cca":"- Leads spending more time on the website are more likely to be converted.Website should be made more engaging to make leads spend more time to add more relevant high quality content in website.","42f89c74":"#### 3.3.10 Lead Quality\nIndicates the quality of lead based on the data and intuition the the employee who has been assigned to the lead.","81992d5a":"### 4.1 Dropping Less Informative Cloumns","caef446b":"- Since most values are 'India' no such specific observstion can be drawn from the data.","c8274982":"#### 3.3.6 Customer Occupation Type\nIndicates whether the customer is a student, umemployed or employed","b760b836":"## 2. Data Cleaning and Preparation","d164a610":"- Most leads are from mumbai with around 28% conversion rate.","c5a10e4b":"- Most of clustmore are from Mumbai City or Mahastra state after missing values treatment.","b9ccadd8":"- Here y is the actual data which we are going to predict, everything else is going to be the features(x).\n- We will use sklearn python lib to `split 70% of our data into training set and rest in test set`. Setting random_state will give the same training and test set everytime on running the code.","0e4c650f":"## 5. Model Building\nWe will be using Generalized Linear Models from StatsModels to build the Logistic Regression model.","f329782a":"- As noticed above,outliers are clearly visible in both scatter and join plot.","3b4fb7ab":"### 5.14 Two views of the evaluation metrics \n- sensitivity-specificity view (having `cut-off @ 0.3`)\n- precision-recall view (having `cut-off @ 0.4`)\n\nYou can select any of the view for Final Prediction, we are using sensitivity-specificity view (having `cut-off @ 0.3`).","4912a726":"#### Observations:\n- P-value of all the features are less than signifiance level`(0.05)`.\n- VIF value of all the features are less than `2` except the constant.","58c18f2c":"#### 2.3.2 Handling - Asymmetrique\n An index and score assigned to each customer based on their activity and their profile","c4a90be1":"#### 5.13.4 Creating Data Frame for Test Data","0121a3bc":"### Common Function","c0eb618c":"- API and Landing Page Submission have 30% conversion rate but count of lead originated from them are significant in nature.\n- Lead Add Form has more than 90% conversion rate but count of lead are not very high.\n- Lead Import are very less in count.\n\n  To improve overall lead conversion rate, we need to focus more on improving lead converion of API and Landing Page Submission origin and generate more leads from Lead Add Form.","0a35a7d2":"- Aftering replacing missing values 'Not Sure' overall percentage gone up.","7a727da7":"## 1. Data Sourcing\n### 1.1 Read dataset from csv file ","adc5653f":"#### 2.1.1 Handling 'Select' Values for Column","0c8641e6":"#### 5.13.5 Assigning Lead Score for Test Data\nWe will use Lead Score using equation again given below.\n\n__Lead Score = 100 * Converted_Prob__","683380e0":"- Two columns in the dataset where all the values are NA.We will remove all the columns where NA values are more than or equal to 70%.","544c3e40":"#### 5.2.2 Ploting highly correlated Columns","203a7fb5":"We will keep considerable last activities as such and club all others to \"Other_Activity\"","0c161ee6":"#### 2.2.2 Percentage Missing values in each column","0a4b5b75":"- Focus should be more on the Specialization with high conversion rate.","da8b714f":"### 5.9 Creating new column 'Predicted' with 1 if Converted_Prob > 0.5 else 0","c52c1ef6":"Import RFE module and Running RFE with the output number of the variable equal to 20.","6372f97b":"### 4.3 Dummy Variables\nWe will be converting a categorical input variable into continuous variable using dummy variable methods.","49645b90":"- Google and Direct traffic generates maximum number of leads.\n- Conversion Rate of reference leads and leads through welingak website is high.\n\nTo improve overall lead conversion rate, focus should be on improving lead converion of olark chat, organic search, direct traffic, and google  leads and generate more leads from reference and welingak website.","204d6a10":"- This is the first mode which build with all the columns showing `very high P-values and high VIFs` so need to optimize the model. Need to use feature selection using RFE.","15c90db6":"## 3. Exploratory Data Analysis ","395b3abd":"- It can been seen,there is no missing values in any of column after columnwise missing values treatment.","1d69a033":"### List of features  which contribute most to a Lead gets converted successfully.\n__Top 3 featres which contribute most to conversion probability of a lead increases with increase in values of the following features in descending order with Negative Coefficient Values:__\n- __Tags_Lost to EINS:__                           `100.00 `\n- __Tags_Closed by Horizzon:__                     `90.28`\n- __Tags_Will revert after reading the email:__    `41.35`  \n\n__Top 3 features which contribute most to conversion probability of a lead increases with decrease in values of the following features in descending order with Negative Coefficient Values:__\n- __Lead Quality_Worst:__                            `-43.40` \n- __Lead Quality_Not Sure:__                         `-34.96` \n- __Tags_switched off:__                             `-27.85` \n \n\n__Observstions__.\n- The increase or decrease the probability threshold value with in churn will decrease or increase the Sensitivity and increase or decrease the Specificity of the model.\n- High Sensitivity will ensure that almost all leads  are likely to Convert are correctly predicted.\n- High Specificity will ensure that leads that are on the brink of the probability of getting Converted or not are not selected.  ","59604d61":"#### 5.14.2 Printing Confusion Matrix for Test Data","482e2028":"### 1.4 Check First few records of the data","24bba365":"- All features in dataset have been `normalized`.","3db0da3d":"### 3.3 Analyzing Categorical Variables","5e48908b":"#### 5.8.1 Creating a dataframe with the actual Converted flag and the Predicted Converted flag","fcf1f993":"- Most of the categorical variables have a relatively large number of unique entries. We will need to find a way to deal with these categorical variables except few variables having less than two unique entries.","71e67b37":"#### 3.3.3 Last Activity\nLast activity performed by the customer. Includes Email Opened, Olark Chat Conversation, etc","18c8d1c8":"- Most value is India.We will impute the same in missing values.","c249c352":"#### 2.3.5 Handling - Tags\nTags assigned to customers indicating the current status of the lead.","3a8a79d5":"#### 3.3.5 Customer Specialization Area or Industry domain \nThe industry domain in which the customer worked before. Includes the level 'Select Specialization' which means the customer had not selected this option while filling the form.","fb9f9f55":"- No missing values in column.","7a0482e6":"- ROC curve is more towards the top left corner of the graph, the model is deemed to be more accurate. Hence, a `greater area(0.97)` under the curve(AUC) would mean the model is more accurate. ","8260286a":"#### 2.4.1.2 Visualizing  Outliers using Scatter Plot","f5ed0fd5":"#### 2.3.6 Handling - What matters most to you in choosing a course\nAn option selected by the customer indicating what is their main motto behind doing this course","7e076907":"### 5.8 Prdicting the Values based on above Model","92d07bba":"### 5.12 Precision and Recall","ba4a55d0":"#### 1.7.2 Categorical variable","50d88ca7":"### 5.11 Creating new column 'Final_Predicted' with 1 if Converted_Prob > 0.3 else 0","1890abcc":"### 2.1 Data Cleaning","389d3f2b":"#### 3.3.11 City\nThe city of the customer.","8ff521c1":"#### 1.7.1 Numercial  variable","6a9fcdb7":"There are many columns are not adding any information to the model.Hence,will drop those variable.","f3d75b14":"#### 3.3.12 Last Notable Activity\nThe last notable acitivity performed by the student\/visitor.","9dfd7dd8":"### 5.10 Plotting the ROC Curve For Train Data","9cc641e8":"### 5.5 Building 3rd Model after Removing \"Tags_invalid number\"","00429ce5":"#### 2.2.1 List missing values from each column","1a9b8eb8":"### 1.6 Summary : Descriptive Statistics of Numeric Columns.","19ae3265":"#### 2.3.1 Handling - Lead Quality\nWhich Indicates the quality of lead based on the data and intuition the the employee who has been assigned to the lead.","fce5655e":"- `Accuracy(0.92)` refers to the closeness of a measured value to actual value. Value is high indicates model is good.\n- `Sensitivity(0.88)` also called true positive rate, number of actual 'Converted' correctly predicted.\n- `Specificity(0.95)` also called true negative rate, number of acutal 'Not Converted' correctly predicted.\n- `False Positive Rate(0.05)` also called `(1-Specificity)`, number of actual `Not Converted` incorrectly predicted.\n- `Positive Predictive Value(0.91)`, actual Converted predicted correctly.\n- `Negative Predictive Value(0.93)`, actual Not Converted predicted correctly.","add95706":"#### 3.2.4 Page Views per Visit by the Customer\nThe total time spent by the customer on the website.","fa801357":"### Columns description view of data source file : Leads Data Dictionary.xlsx","4bccbf84":"ROC curve demonstrates several things:\n\n- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","6db1dbea":"### 4.5 Feature Scaling\nWe will use StandardScaler to bring the distributions into the same scale for numerical Columns","c2270252":"### 4.4 Train Test Split for dataset.\nX will contain all the features and y will contain the target variable","605bb722":"Let us re-visualize 'Lead Quality' column to note the impute value changes.","26156c1f":"- `Recall(0.88)`, that an actual `Converted` case is predicted correctly.","dd98faa7":"## Summary","96fb1edf":"- Assigning the Leas Score based on the Probability","117b5c84":"### 3.2 Analyzing Numerical Variables","f82579c5":"- Median for converted and not converted leads are the same.Hence,nothng conclusive can be inferred on the basis of Total Visits.","8b7b9a7d":"#### 5.12.1 Precision and recall tradeoff","cdad870a":"#### 5.10.1 Determine  Optimal Probability Threshold\/Cutoff Point\nOptimal cutoff probability is that prob where we get balanced sensitivity and specificity","9c8fd2c2":"- Tags,'Will revert after reading the email' has highest lead conversion rate.","0410156f":"#### 3.3.9 Tags - Customers Lead Status \nTags : Tags assigned to customers indicating the current status of the lead.","0d5ee6e5":"#### 5.11.2 Classification Report for Model Validation","091c6698":"#### 3.3.8 Less Informative  Features","1c52e200":"### 5.2 Correlation Matrix\n- Check the correlation coefficients between variables which are `highly correlated (>=0.7 and <=-0.7)`."}}