{"cell_type":{"adac2ef6":"code","2b9bfd1c":"code","c9872284":"code","849b1765":"code","e64f1b0d":"code","77465c19":"code","47db3a2a":"code","7b7208e9":"code","b02b79cb":"code","827c5ca6":"code","affa2520":"code","7b5ffb14":"code","3077424c":"code","899940b2":"code","8da40a07":"code","1b643e01":"code","fe1053da":"code","81d77a2a":"code","3daa136e":"code","abf374c8":"markdown","210a0765":"markdown","c33403b7":"markdown","3df12ff5":"markdown","13b0762a":"markdown","92832866":"markdown","6056274c":"markdown","571602fd":"markdown","546912dd":"markdown","c7161017":"markdown","e807f53f":"markdown","9b8c66f9":"markdown","533c17e7":"markdown","0ed5747c":"markdown"},"source":{"adac2ef6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2b9bfd1c":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom matplotlib import cm","c9872284":"# load the dataset\npath = \"\/kaggle\/input\/coursera-ml\"\ndataset = pd.read_csv(os.path.join(path,\"ex1data1.txt\"), header=None)\ndataset.columns = [\"Population\",\"Profit\"]\ndataset.head()","849b1765":"# split the datset into feature(X) and target(y)\nX = dataset[\"Population\"].values.reshape(-1,1)\ny = dataset[\"Profit\"].values.reshape(-1,1)\nm = y.shape[0] #number of data examples\n\n#Visualization by ploting\nplt.scatter(X,y,marker='x', color='red')\nplt.xlabel(\"Population of city in 10,000s\")\nplt.ylabel(\"Profit in $10,000s\")\nplt.title(\"Fig: Scatter plot of training data\")\nplt.show()","e64f1b0d":"def compute_cost( X, y, theta ):\n    m = y.shape[0]\n    h_theta_x = np.dot(X,theta.T)\n    cost = np.sum( np.power(h_theta_x-y,2), axis=0 )\/(2*m)\n    return cost\n\ndef gradient_descent(X, y, theta, iterations, learning_rate):\n    cost = np.zeros((iterations,1))\n    for itr in range(iterations):\n        delta = np.dot(X,theta.T) - y\n        theta = theta - learning_rate * np.dot(delta.T, X)\/m\n        cost[itr][0] = compute_cost(X,y,theta)\n    return cost, theta\n\ndef predict( X, theta):\n    predictions = np.dot(X,theta.T)\n    return predicitons","77465c19":"# XX = np.stack((np.ones(m),X.flatten()),axis=-1)\nXX = np.append(np.ones((m,1)),X,axis=1) # adding ones as intercept term\ntheta = np.zeros((1,XX.shape[1]))\niterations = 1000\nalpha = 0.01","47db3a2a":"cost, theta = gradient_descent(XX,y,theta,iterations, alpha)","7b7208e9":"predictions = np.dot(XX,theta.T)","b02b79cb":"plt.scatter(X,y,marker='x', color='red')\nplt.plot(X,predictions)\nplt.xlabel(\"Population of city in 10,000s\")\nplt.ylabel(\"Profit in $10,000s\")\nplt.title(\"Fig: Scatter plot of training data\")\nplt.legend(['Linear Regression', 'Data Points'])\nplt.show()","827c5ca6":"theta0 = np.linspace(-10,10,100)\ntheta1 = np.linspace(-1,4,100)\nJ = np.zeros((len(theta0),len(theta1)))\nfor i in range(len(theta0)):\n    for j in range(len(theta1)):\n        temp_theta = np.array( [theta0[i],theta1[j]] ).reshape(1,-1)\n        J[i][j] = compute_cost(XX,y,temp_theta)","affa2520":"\nfig = plt.figure()\nax = fig.gca(projection='3d')\n\n# Plot the surface.\nsurf = ax.plot_surface(theta0, theta1, J, cmap=cm.coolwarm,\n                       linewidth=0, antialiased=False)","7b5ffb14":"fig = plt.figure()\nax = plt.subplot(111)\nplt.contour(theta0, theta1,J)","3077424c":"# Load the dataset\ntrain_data = pd.read_csv(os.path.join(path,\"ex1data2.txt\"), header=None)\ntrain_data.columns = ['size','#bedrooms','price']\ntrain_data.head()","899940b2":"# extract the features and target from the dataset\n\ntrain_X = train_data.values[:,:2]\ntrain_y = train_data['price'].values.reshape(-1,1)\nm = train_y.shape[0]","8da40a07":"#Visualize the data set\n\nfig, axes = plt.subplots(figsize=(12,4), nrows=1, ncols=2)\naxes[0].scatter(train_X[:,0],train_y, marker='x', color='red')\naxes[0].set_xlabel(\"Size of the house\")\naxes[0].set_ylabel(\"Price of house\")\naxes[0].set_title(\"Price vs Size of the house\")\n\naxes[1].scatter(train_X[:,1],train_y, marker='x', color='red')\naxes[1].set_xlabel(\"Number of the house\")\naxes[1].set_ylabel('Price of house')\naxes[1].set_title(\"Price vs Number of the house\")\nplt.tight_layout()\nplt.show()","1b643e01":"def normalize_features(X):\n    mean = np.mean(X,axis=0)\n    std = np.std(X,axis=0)\n    normalize_x = (X-mean)\/std\n    return normalize_x, mean, std","fe1053da":"#get the normalized data and related parameters\nnorm_X, mean, std = normalize_features(train_X)\n\n# add the intercept term to the feature set \nXX = np.append(np.ones((m,1)),norm_X,axis=1)\n\ntheta = np.zeros((1, XX.shape[1]))\n\ncost, theta = gradient_descent(X=XX, y=train_y, theta=theta, iterations=400, learning_rate=0.02)","81d77a2a":"plt.plot(cost)\nplt.xlabel(\"# iteraions\")\nplt.ylabel(\"Cost\")\nplt.title(\"Convergence of gradient descent with an appropriate learning rate\")\nplt.show()","3daa136e":"result = {\n    \"cost\":[],\n    \"alpha\":[0.001, 0.002, 0.01, 0.02, 0.1, 0.2]\n}\n\nfor alpha in result[\"alpha\"]:\n    cost, theta_ = gradient_descent(X=XX, y=train_y, theta=theta, iterations=iterations, learning_rate=alpha)\n    result['cost'].append(cost)\nfor _x in result['cost']:\n    plt.plot(_x)\nplt.xlabel(\"# iteraions\")\nplt.ylabel(\"Cost\")\n# plt.title(\"Convergence of gradient descent with different learning rates\")\nplt.legend(result['alpha'])\nplt.show()\n","abf374c8":"### Visualize the linear fit to data","210a0765":"### Feature Scaling\n**What?**<br>\nIt's a technique which comes under the data preprocessing. FS(Feature Scaling) is the technique to standarize the independent feature into a fixed range.\n\n\n**Why do we need?**<br>\nto scale the data into a fixed range\nAll the features contribute equally to the result.\nNeural network gradient descent converge much faster with feature scaling than without it.\nOne more reason is saturation, like in the case of sigmoid activation in Neural Network, scaling would help not to saturate too fast.\n\n**When to use FS?**<br>\nAlgorithms that use distance calculations or assumes normality like K Nearest Neighbor, Regression, SVMs, etc are the ones that require feature scaling.\nAlgorithms that do not use distance calculations like Naive Bayes, Tree-based models, LDA do not require feature scaling.\nWhich feature technique to use, varies from case to case. For e.g. PCA that deals with variance data, it is advisable to use standardization.\n\n**Few key points to note :**<br>\n* Mean centering does not affect the covariance matrix\n* Scaling of variables does affect the covariance matrix\n* Standardizing affects the covariance\n\n**Below are the few ways we can do feature scaling.**<br>\n1) Min Max Scaler\n2) Standard Scaler\n3) Max Abs Scaler\n4) Robust Scaler\n5) Quantile Transformer Scaler\n6) Power Transformer Scaler\n7) Unit Vector Scaler\n[More Reading](https:\/\/towardsdatascience.com\/all-about-feature-scaling-bcc0ad75cb35)\n\n\n**But most commonly used are**\n\nMean Normalization: $x^\\prime = \\frac{x_i - \\bar{x}}{\\max{(x)} - \\min{(x)}}$<br>\nThis distribution will have values between $-1$ and $1$ with $\\mu=0$.\n\nMin-Max Scaling:\n$x^\\prime=\\frac{x_i - \\min{(x)}}{\\max{(x)} - \\min{(x)}}$<br>\nThis scaling brings the value between 0 and 1.\n\nStandardization: $$ x^\\prime = \\frac{x_i - \\bar{x}}{\\sigma} $$\n\nStandardization replaces the values by their $Z scores$. This redistributes the features with their mean $\u03bc=0$ and standard deviation $\u03c3=1$.\nStandardisation and Mean Normalization can be used for algorithms that assumes zero centric data like Principal Component Analysis(PCA).\n\n<b>Unit Vector:<\/b>\n$$x^\\prime=\\frac{x}{||x||}$$\nMin-Max Scaling and Unit Vector techniques produces values of range [0,1]. When dealing with features with hard boundaries this is quite useful. For example, when dealing with image data, the colors can range from only 0 to 255.","c33403b7":"#### Package Importing","3df12ff5":"The Big Question \u2013 Normalize or Standardize?\nNormalization vs. standardization is an eternal question among machine learning newcomers. Let me elaborate on the answer in this section.\n\nNormalization is good to use when you know that the distribution of your data does not follow a Gaussian distribution. This can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors and Neural Networks.\nStandardization, on the other hand, can be helpful in cases where the data follows a Gaussian distribution. However, this does not have to be necessarily true. Also, unlike normalization, standardization does not have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization.\nHowever, at the end of the day, the choice of using normalization or standardization will depend on your problem and the machine learning algorithm you are using. There is no hard and fast rule to tell you when to normalize or standardize your data. You can always start by fitting your model to raw, normalized and standardized data and compare the performance for best results.\n\nIt is a good practice to fit the scaler on the training data and then use it to transform the testing data. This would avoid any data leakage during the model testing process. Also, the scaling of target values is generally not required.\n[More Reading](https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/feature-scaling-machine-learning-normalization-standardization\/)","13b0762a":"## Linear Regression: one variable\/one features","92832866":"# Linear Regression","6056274c":"* **Hypothesis** for linear regression is\n$$h_\\theta (x) = \\theta^T x = \\theta_0 + \\theta_1 x_1$$\n* **Cost** function is\n$$J(\\theta) = \\frac{1}{2m} \\sum\\limits_{i=1}^{m} (h_\\theta (x^i)-y^i)^2$$\n\n\nObjective of the Linear Regression is minimize the cost function $J(\\theta)$<br>\nTo minimize the cost function we have to adjust the value of the $\\theta_j$. Using batch **Gradient Descent** we can do this.\n\nFor each iteration the batch **gradient descent** performs the update which is:\n$$\\theta_j := \\theta_j-\\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta (x^i)-y^i)x^i_j$$","571602fd":"### Load and Visualize the dataset","546912dd":"This notebook is part of the practice that I had learned from the [ML Course](https:\/\/www.coursera.org\/learn\/machine-learning) and various blogs.","c7161017":"#### Normalize the features","e807f53f":"#### Visualize convergence graph with different learning rates","9b8c66f9":"#### Visualize how cost is getting minimum per iterations","533c17e7":"## Linear Regression with Multiple Varables","0ed5747c":"### Load dataset and Visualize "}}