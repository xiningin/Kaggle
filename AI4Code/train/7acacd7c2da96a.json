{"cell_type":{"513071ff":"code","99a355cb":"code","c2f99b70":"code","715d6d1f":"code","91da925c":"code","bcd02ac2":"code","673b3d18":"code","b4e47f58":"code","71abd41e":"code","dea07401":"code","a47874ba":"code","dbb8f51e":"code","cb12640b":"code","12f41e52":"code","fb5f3d04":"code","c3b447c8":"markdown","20773974":"markdown","3ed89248":"markdown","ffc01bef":"markdown","fa59157b":"markdown","09638bd5":"markdown","4ce81459":"markdown"},"source":{"513071ff":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\n","99a355cb":"creditCardData = pd.read_csv(\"..\/input\/creditcard.csv\")\ncreditCardData.head()","c2f99b70":"print ('# of columns: ', len(creditCardData.columns))\ncreditCardData.describe()","715d6d1f":"#Checking for missing data\ncreditCardData.isnull().any().sum()","91da925c":"#Plotting a heatmap to visualize the correlation between the variables\nsns.heatmap(creditCardData.corr())","bcd02ac2":"# As the time provided is in seconds we can use it as seconds since epoch as we won't care about years\ndef convert_totime(seconds):\n    return datetime.datetime.fromtimestamp(seconds);\n\ntimeAnalysis = creditCardData[['Time', 'Amount', 'Class']].copy()\ntimeAnalysis['datetime'] = timeAnalysis.Time.apply(convert_totime)\n# As the max time is 172792 seconds and 172792 \/ (60*60) is about 48 hrs so we only have data for 2 days so only \n# plotting data against hours make sense\ntimeAnalysis['hour of the day'] = timeAnalysis.datetime.dt.hour\ntimeAnalysisGrouped = timeAnalysis.groupby(['Class', 'hour of the day'])['Amount'].count()","673b3d18":"plt.figure(figsize = (10, 6))\nvalidTransactions = timeAnalysisGrouped[0].copy()\nvalidTransactions.name = 'Number of transactions'\nvalidTransactions.plot.bar(title = '# of legitimate credit card transactions per hour', legend = True)","b4e47f58":"## Run this section only if your distribution is somewhat off like it shows most transactions \n## happened during the night\ntimeDelta = datetime.datetime.utcnow() - datetime.datetime.now() \nplt.figure(figsize = (10, 6))\ntimeAnalysis['hour of the day'] = timeAnalysis.datetime + timeDelta\ntimeAnalysis['hour of the day'] = timeAnalysis['hour of the day'].dt.hour\ntimeAnalysisGrouped = timeAnalysis.groupby(['Class', 'hour of the day'])['Amount'].count()\nvalidTransactions = timeAnalysisGrouped[0].copy()\nvalidTransactions.name = 'Number of transactions'\nvalidTransactions.plot.bar(title = '# of legitimate credit card transactions per hour', legend = True)","71abd41e":"plt.figure(figsize = (10, 6))\nfraudTransactions = timeAnalysisGrouped[1].copy()\nfraudTransactions.name = 'Number of transactions'\nfraudTransactions.plot.bar(title = '# of fraud credit card transactions per hour', legend = True)","dea07401":"# Valid Transactions\ntimeAnalysis[timeAnalysis.Class == 0].Amount.plot.hist(title = 'Histogram of valid transactions')","a47874ba":"# As the value of most transaction seems to be only about 2K - 2.5K. Lets limit the data further\ntimeAnalysis[(timeAnalysis.Class == 0) & (timeAnalysis.Amount <= 4000)].Amount.plot.hist(title = 'Histogram of valid transactions (Amount <= 4K)')","dbb8f51e":"# Now lets look at the Fraudulent transactions\ntimeAnalysis[timeAnalysis.Class == 1].Amount.plot.hist(title = 'Histogram of fraudulent transactions')","cb12640b":"population = timeAnalysis[timeAnalysis.Class == 0].Amount\nsample = timeAnalysis[timeAnalysis.Class == 1].Amount\nsampleMean = sample.mean()\npopulationStd = population.std()\npopulationMean = population.mean()","12f41e52":"z_score = (sampleMean - populationMean) \/ (populationStd \/ sample.size ** 0.5)\nz_score","fb5f3d04":"# Gettting the PDA columns\nPDA_columns = [x for x in creditCardData.columns if 'V' in x]\n\nvalid_transactions = creditCardData[creditCardData.Class == 0]\nFraud_transactions = creditCardData[creditCardData.Class == 1]\n#Getting the number of rows\nsample_size = Fraud_transactions.shape[0]\nfor col in PDA_columns:\n    mean = valid_transactions[col].mean()\n    std = valid_transactions[col].std()\n    zScore = (Fraud_transactions[col].mean() - mean) \/ (std\/sample_size**0.5)\n    print ('Column', col, 'is', 'Significant' if abs(zScore) >= 3.37 else 'insignificant')","c3b447c8":"## Conclusion\nThe amount spend on fraudulent transactions is on average significantly higher than normal transactions but in absolute terms higher amounts are spent on valid transaction. This means we can't really create an additional boolean feature such as 'If amount spent is higher than a given value', on the other hand there is significant difference in average amount spent, maybe it can be used to identify frauds.\n\nAlso, as it would seem as per my calculation the fraudulent transactions are more spread out during the day as compared to normal transactions. Maybe scrutinizing late night transactions can lead to a better detection rate. Finally, features - V13, V15, V22, V23, V25, V26 are not very good at differentiating between fraud and valid transactions. So, maybe removing them will lead to a better result. ","20773974":"Note: An interesting thing happened here. When I did this calculation on my laptop, the distribution did not look right, so I added 7 hours to each transaction and I got something like the figure above. I think its is due to the fact that the Kaggle's server must be running in UTC while my system is in MST (the difference between UTC and MST is 7 hours)","3ed89248":"2 A.M. has an unsual uptick for the number of frauds committed. But it could also be that my assumption that the first transaction happened at 7 A.M. is incorrect. One thing is clear though that the fraud transactions are better spread out than the legitimate transactions. This can be due to the fact that there are very few fradulent transactions and hence they won't have a clear trend like in the case of legitimate transactions\n\n\n## Analysis 2 - Are fraudulent transactions of higher value than normal transactions\nIt would be interesting to see if fraudulent transactions are in general of higher value than normal transactions or not. To check this lets setup a hypothesis test. Lets define our Null and Alternative hypothesis\n\n- H<sub>0<\/sub> : Fraudulent transactions are of similar or lower value as normal transactions\n- H<sub>A<\/sub> : Fraudulent transactions are of higher value as normal transactions\n\nI took H<sub>0<\/sub> to be similar or lower because H<sub>0<\/sub> and H<sub>A<\/sub> should together cover all the possibilities\n\nBefore we begin lets first look at the distribution of amounts of transaction done","ffc01bef":"The features V1-V28 are totally uncorrelated which should be the case as they are obtained by performing PDA on the original dataset\n\n## Analysis 1: When do people shop\nWe will visualize when people shop and when credit fraud happens and if there is a pattern. For this however we need to convert time from seconds to days, hours and weeks","fa59157b":"As the z-score is more than 2.326 we reject the Null hypothesis. So there is a 99% chance that the amount spend on fraudulent transactions are on average significantly higher than normal transactions. But as we observed in the histograms in absolute terms normal transactions are of higher value.\n\n## Going all out on hypothesis testing\nLets now perform a hypothesis test for each of the 28 features to see if the feature value for the fraud data is significantly different from the valid transaction or not. The significance level for this experiment will be 99% and it will be a 2-tailed test. The corresponding z-critical value is 3.37","09638bd5":"Hmmmm, there doesn't appears to be any difference visually. But lets wait till we perform the hypothesis test to draw the final conclusion.\n\nFor the hypothesis test I will be performing a Z-test, with the valid transactions acting as the population. Though a T-test can also be performed but given that our sample set (fraudulent transactions) is of size 492 there shouldn't be any difference, as for sample set >= 30 the t distribution and z distribution are nearly the same.\n\nLets start. We will be performing the test for 99% significance level, this means that we should get a z-score of atleast 2.326 or higher. If someone does not know the formula for z-score, here it is\n\n$$ z-score = (\\bar{x} - \\mu) \/ S.E$$\n\nWhere\n- $\\bar{x}$ : mean of the sample\n- $\\mu$ : population mean\n- S.E : Standard Error\n\nThe standard error in our case is given by the formula : $\\sigma\/\\sqrt{n}$, where $\\sigma$ is the Standard deviation of the population and n is the sample size","4ce81459":"So as per the description there are 284807 rows with 28 transformed feature columns V1-V28 and 2 original features Time and Value and a Class label "}}