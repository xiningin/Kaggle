{"cell_type":{"cb3b4c8c":"code","e9cffce5":"code","cdc302db":"code","883c4766":"code","d42a08b3":"code","8983cb2b":"code","e54f909a":"code","9bb978ed":"code","6602e1eb":"code","c7faa4c1":"code","847790cb":"code","f09402c4":"code","15145b6e":"code","89d85a4a":"code","36121037":"code","38c1a1cd":"code","e6a23d67":"code","136d5b74":"markdown","80f89326":"markdown","f1ed0d36":"markdown","4b4071f1":"markdown","8a0bb04b":"markdown","9ff06094":"markdown","47b0fc3f":"markdown","4dd0266f":"markdown","c4b0d0bd":"markdown","607e203c":"markdown","1ccd0adf":"markdown","1ecdce19":"markdown","82d4ad9a":"markdown","f655442b":"markdown","64384207":"markdown","b221a0db":"markdown"},"source":{"cb3b4c8c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mutual_info_score, mean_squared_error\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.linear_model import LogisticRegression, Ridge\nimport warnings","e9cffce5":"data = pd.read_csv('..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv')\ndata.head()","cdc302db":"features  = [\n    'neighbourhood_group',\n    'room_type',\n    'latitude',\n    'longitude',\n    'price',\n    'minimum_nights',\n    'number_of_reviews',\n    'reviews_per_month',\n    'calculated_host_listings_count',\n    'availability_365'\n]\ndf = data[features]\n\"\"\"Checking description of this project's data set\"\"\"\ndf.describe()","883c4766":"from IPython.display import display\nmissing_vals = df.isnull().sum()\nprint(\"Before Imputing Missing Values\")\ndisplay(missing_vals.to_frame().reset_index().rename({'index': 'Variables', 0: 'Missing Values'}, axis = 1).sort_values(by = 'Missing Values', ascending = False).style.background_gradient('Blues'))\n\n\ndf.fillna(0, inplace = True)\nprint(\"After Imputing Missing Values\")\ndisplay(df.isnull().sum().to_frame().reset_index().rename({'index': 'Variables', 0: 'Missing Values'}, axis = 1).style.background_gradient('Blues'))","d42a08b3":"print(\"Mode for variable 'neighbourhood_group': %s\" %(df['neighbourhood_group'].value_counts().head(1)))","8983cb2b":"from sklearn.model_selection import train_test_split\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\ndf_train, df_val = train_test_split(df_full_train, test_size=0.2, random_state=42)\nlen(df_train), len(df_val), len(df_test)\n\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)","e54f909a":"\"\"\"Creating List of Numerical and Categorical columns\"\"\"\ncategorical = [col for col in df.columns if df[col].dtype == 'object']\nnumerical = [col for col in df.columns if col not in categorical]\n\n\"\"\"Correlation of Numerical Columns\"\"\"\ndisplay(df[numerical].corr())\n","9bb978ed":"\"\"\"price variable from numeric into binary.\"\"\"\ndf_full_train['above_average'] = (df_full_train['price'] >= 152).values.astype(int)","6602e1eb":"\"\"\"Mutual Information\"\"\"\ndef mutual_info_bin_score(series):\n    return mutual_info_score(series, df_full_train.above_average)\n\nmi = df_full_train[categorical].apply(mutual_info_bin_score)\nmi.round(2).sort_values(ascending=False)    ","c7faa4c1":"\"\"\"Taking Care of Categorical variables\"\"\"\ny_train = (df_train['price'] >= 152).values.astype(int)\ny_val = (df_val['price'] >= 152).values.astype(int)\ny_test = (df_test['price'] >= 152).values.astype(int)\n\ndel df_train['price']\ndel df_val['price']\ndel df_test['price']\n\nnumerical.remove('price')\n\ndecision = (y_val >= 152).astype(int)","847790cb":"def calculate_accuracy(features):\n    # one-hot encoding datasets\n    dv = DictVectorizer(sparse=False)\n\n    train_dict = df_train[features].to_dict(orient='records')\n    val_dict = df_val[features].to_dict(orient='records')\n\n    X_train = dv.fit_transform(train_dict)\n    X_val = dv.transform(val_dict)\n\n    \"\"\"Fitting the Model on Training Set\"\"\"\n    model = LogisticRegression(solver='liblinear', C=1.0, random_state=42)\n    model.fit(X_train, y_train)\n\n    \"\"\"Using the model on validation\"\"\"\n    y_pred = model.predict_proba(X_val)[:,1]\n\n    \"\"\"Setting up Decision Threshold to 0.5\"\"\"\n    decision = (y_pred >= 0.5)\n\n    \"\"\"Calculating accuracy\"\"\"\n    accuracy = (y_val == decision).mean()\n    \n\n    df_pred = pd.DataFrame()\n    df_pred['probability'] = y_pred\n    df_pred['prediction'] = decision\n    df_pred['actual'] = y_val\n    df_pred['correct'] = df_pred.prediction == df_pred.actual\n    return accuracy, df_pred\nacc, df_pred = calculate_accuracy(numerical+categorical)    \n\nprint(acc)\ndf_pred.head()","f09402c4":"all_vars_accuracy,_ = calculate_accuracy(numerical+categorical)\nall_vars_accuracy.round(2)","15145b6e":"useful_features = numerical + categorical\ndiff = {}\nfor i in useful_features:\n  features = useful_features.copy()\n  features.remove(i)\n  acc,_ = calculate_accuracy(features)\n  diff[\"Difference in accuracy when removing %s\"%i] = all_vars_accuracy - acc\n\ndiff","89d85a4a":"pd.DataFrame(diff.values(), index = diff.keys()).rename({0: 'differences'}, axis = 1).sort_values(by = 'differences', ascending = True).style.background_gradient('Blues')","36121037":"\"\"\"Preparing Data for Linear Regression with Price Variable included\"\"\"\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\ndf_train, df_val = train_test_split(df_full_train, test_size=0.2, random_state=42)\nlen(df_train), len(df_val), len(df_test)\n\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\n\n\"\"\"Creating List of Numerical and Categorical columns\"\"\"\ncategorical = [col for col in df.columns if df[col].dtype == 'object']\nnumerical = [col for col in df.columns if col not in categorical]\n\n\"\"\"Apply the log transformation to the price variable using the np.log1p() function.\"\"\"\ny_train = np.log1p(df_train['price'].values)\ny_val = np.log1p(df_val['price'].values)\ny_test = np.log1p(df_test['price'].values)\n\n\n\"\"\"Make sure that the target value ('price') is not in your dataframe.\"\"\"\ndel df_train['price']\ndel df_val['price']\ndel df_test['price']\n\n\"\"\"Taking care of Categorical Variables\"\"\"\ndv = DictVectorizer(sparse=False)\n\ntrain_dict = df_train[features].to_dict(orient='records')\nval_dict = df_val[features].to_dict(orient='records')\n\nX_train = dv.fit_transform(train_dict)\nX_val = dv.transform(val_dict)","38c1a1cd":"scores = {}\nfor alpha in [0, 0.01, 0.1, 1, 10]:\n    model = Ridge(alpha=alpha)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_val)\n    score = np.sqrt(mean_squared_error(y_val, y_pred))\n    scores[alpha] = score.round(3)\n    print(\"RMSE with alpha = %s and not rounding to 3 digits: %s\"%(alpha, score) )\nprint(\" \\nRMSE with rounding off to 3 digits\")\nprint(scores)","e6a23d67":"print(\"Table of RMSE rounded to 3 digits\")\npd.DataFrame(scores.values(), index = scores.keys()).rename({0: 'RMSE'}, axis = 1).sort_values(by = 'RMSE', ascending = True).style.background_gradient('Blues')","136d5b74":"### Smallest Difference:\n\n    number_of_reviews (0.000895)\n","80f89326":"## Split the data\n\n    * Split your data in train\/val\/test sets, with 60%\/20%\/20% distribution.\n    * Use Scikit-Learn for that (the train_test_split function) and set the seed to 42.\n    * Make sure that the target value ('price') is not in your dataframe.\n","f1ed0d36":"## Q5:\n\n    * We have 9 features: 7 numerical features and 2 categorical.\n    * Let's find the least useful one using the feature elimination technique.\n    * Train a model with all these features (using the same parameters as in Q4).\n    * Now exclude each feature from this set and train a model without it. Record the accuracy for each model.\n    * For each feature, calculate the difference between the original accuracy and the accuracy without the feature.\n    * Which of following feature has the smallest difference?\n        - neighbourhood_group\n        - room_type\n        - number_of_reviews\n        - reviews_per_month","4b4071f1":"## Q1. What is the most frequent observation (mode) for the column 'neighbourhood_group'?","8a0bb04b":"## Make price binary\n\n    * We need to turn the price variable from numeric into binary.\n    * Let's create a variable above_average which is 1 if the price is above (or equal to) 152.\n","9ff06094":"Q3:\n\n    * Calculate the mutual information score with the (binarized) price for the two categorical variables that we have. Use the training set only.\n    * Which of these two variables has bigger score?\n    * Round it to 2 decimal digits using round(score, 2)\n","47b0fc3f":"## Q4:\n\n    * Now let's train a logistic regression\n    * Remember that we have two categorical variables in the data. Include them using one-hot encoding.\n    * Fit the model on the training dataset.\n        - To make sure the results are reproducible across different versions of Scikit-Learn, fit the model with these parameters:\n        - model = LogisticRegression(solver='lbfgs', C=1.0, random_state=42)\n    * Calculate the accuracy on the validation dataset and rount it to 2 decimal digits.\n","4dd0266f":"## Importing Libraries","c4b0d0bd":"## Highest Correlation is between\n\n    * reviews_per_month and number_of_reviews: 0.589407\n","607e203c":"## Loading and Reading Data ","1ccd0adf":"## Question 6:\n\n    * For this question, we'll see how to use a linear regression model from Scikit-Learn\n    * We'll need to use the original column 'price'. Apply the logarithmic transformation to this column.\n    * Fit the Ridge regression model on the training data.\n    * This model has a parameter alpha. Let's try the following values: [0, 0.01, 0.1, 1, 10]\n    * Which of these alphas leads to the best RMSE on the validation set? Round your RMSE scores to 3 decimal digits.\n","1ecdce19":"## Q2:\n\n    * Create the correlation matrix for the numerical features of your train dataset.\n        - In a correlation matrix, you compute the correlation coefficient between every pair of features in the dataset.\n    * What are the two features that have the biggest correlation in this dataset?\n","82d4ad9a":"## Features used for this Project","f655442b":"alpha that leads to the best RMSE: 0","64384207":"## Missing Values? Impute them with 0","b221a0db":"## This Week 3 Questions\nNotebook is a part of FREE ML course by Glexey Grigorev. [List Of The Questions](https:\/\/github.com\/alexeygrigorev\/mlbookcamp-code\/blob\/master\/course-zoomcamp\/03-classification\/homework.md)"}}