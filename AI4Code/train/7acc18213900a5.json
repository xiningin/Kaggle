{"cell_type":{"47b13555":"code","1cbeb7e7":"code","e706db4f":"code","12d6a16d":"code","94f3a617":"code","39fc1a9e":"code","8778dda2":"code","ab67b019":"code","d5eb011c":"code","d6d5a977":"code","e86d0c3f":"code","97b60a7b":"code","e0991631":"code","8ef044e0":"code","e7c4b112":"code","98885579":"code","1c357df2":"code","1bce13be":"code","a8cd02e9":"code","daa9f042":"code","b0f8402f":"code","f8c42ab1":"code","3c971f24":"code","14b79ed4":"code","e0737cc7":"code","4b5071e4":"code","9627a637":"code","3af4647c":"code","dff0c828":"code","8208b6e4":"code","a3a59975":"code","1e7bfca4":"code","68dfe68a":"code","29315531":"code","2af4aa64":"markdown","9938d471":"markdown","58ebf45e":"markdown","03b1dd4e":"markdown","507abe32":"markdown","79c093e7":"markdown","4b42e791":"markdown","ffc87cbc":"markdown","8e0b274f":"markdown","ed232b85":"markdown","8a18304f":"markdown","20bb641d":"markdown","89f377f7":"markdown","92f1fcd0":"markdown","de044a8e":"markdown","ee08f405":"markdown","375ccd35":"markdown","3217a85b":"markdown","7c56fe43":"markdown","cd941d86":"markdown","53db5dc9":"markdown"},"source":{"47b13555":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n#import libraries and packages:\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import SimpleRNN\nfrom keras.layers import Dropout\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport math\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1cbeb7e7":"dataset_train = pd.read_csv('..\/input\/google-stock-price\/Google_Stock_Price_Train.csv')","e706db4f":"dataset_train.head()","12d6a16d":"#open'\u0131 kullan\u0131caz sadece:\ntrain = dataset_train.loc[:, ['Open']].values #array'e \u00e7evirdik\ntrain","94f3a617":"from sklearn.preprocessing import MinMaxScaler #bununla, 0-1 aras\u0131na scale ettik\nscaler = MinMaxScaler(feature_range = (0, 1))\ntrain_scaled = scaler.fit_transform(train)\ntrain_scaled","39fc1a9e":"plt.plot(train_scaled)","8778dda2":"#ilk 1-50 yi al\u0131p X_train'e, 51. data point'i de y_train'e,\n#2-51'i al\u0131p X_train'e, 52'yi y_train'e ...olacak \u015fekilde data frame i olu\u015fturuyoruz:\nX_train = []\ny_train = []\ntimesteps = 50\n\nfor i in range(timesteps, 1250):\n    X_train.append(train_scaled[i - timesteps:i, 0])\n    y_train.append(train_scaled[i, 0])\n    \nX_train, y_train = np.array(X_train), np.array(y_train)","ab67b019":"#Reshaping:\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nX_train.shape","d5eb011c":"y_train.shape","d6d5a977":"# ***********************************\n# Model of Recurrent Neural Network\n# RNN\n# ***********************************\n\n#Initialize RNN:\nregressor = Sequential()\n\n#Adding the first RNN layer and some Dropout regularization\nregressor.add(SimpleRNN(units = 50, activation='tanh', return_sequences=True, input_shape= (X_train.shape[1],1)))\nregressor.add(Dropout(0.2))\n\n#Adding the second RNN layer and some Dropout regularization\nregressor.add(SimpleRNN(units = 50, activation='tanh', return_sequences=True))\nregressor.add(Dropout(0.2))\n\n#Adding the third RNN layer and some Dropout regularization\nregressor.add(SimpleRNN(units = 50, activation='tanh', return_sequences=True))\nregressor.add(Dropout(0.2))\n\n#Adding the fourth RNN layer and some Dropout regularization\nregressor.add(SimpleRNN(units = 50))\nregressor.add(Dropout(0.2))\n\n#Adding the output layer\nregressor.add(Dense(units = 1))\n\n#Compile the RNN\nregressor.compile(optimizer='adam', loss='mean_squared_error')\n\n#Fitting the RNN to the Training set\nregressor.fit(X_train, y_train, epochs=100, batch_size=32)\n","e86d0c3f":"# ***********************************\n# Model of Long Short Term Memory (LSTM)\n# LSTM model\n# ***********************************\nregressor1 = Sequential()\nregressor1.add(LSTM(10, input_shape= (X_train.shape[1],1))) # 10 lstm neuron(block)\nregressor1.add(Dense(1))\nregressor1.compile(loss='mean_squared_error', optimizer='adam')\nregressor1.fit(X_train, y_train, epochs=30, batch_size=1)","97b60a7b":"dataset_test = pd.read_csv('..\/input\/google-stock-pricetest\/Google_Stock_Price_Test.csv')\ndataset_test.head()","e0991631":"real_stock_price = dataset_test.loc[:, ['Open']].values\n#real_stock_price","8ef044e0":"#Getting the predicted stock price\ndataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis=0)\ninputs = dataset_total[len(dataset_total)-len(dataset_test) - timesteps:].values.reshape(-1,1)\ninputs = scaler.transform(inputs) #minmax scaler\n#inputs","e7c4b112":"X_test = []\ny_test = []\nfor i in range(timesteps, 70):\n    X_test.append(inputs[i-timesteps:i,0])\n    y_test.append(inputs[i, 0])\nX_test = np.array(X_test)\ny_test = np.array(y_test)\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\npredicted_stock_price = regressor.predict(X_test)\npredicted_stock_price1 = regressor1.predict(X_test)\npredicted_stock_price_tr = regressor.predict(X_train)\npredicted_stock_price1_tr = regressor1.predict(X_train)\npredicted_stock_price_RNN = scaler.inverse_transform(predicted_stock_price)\npredicted_stock_price_LSTM = scaler.inverse_transform(predicted_stock_price1)\npredicted_stock_price_RNN_tr = scaler.inverse_transform(predicted_stock_price_tr)\npredicted_stock_price_LSTM_tr = scaler.inverse_transform(predicted_stock_price1_tr)\n#inverse_transform ile, scale edildikten sonra predict edilen de\u011ferleri ger\u00e7ek de\u011fer aral\u0131\u011f\u0131na \u00e7ekiyoruz ","98885579":"y_train1 = scaler.inverse_transform([y_train])\ny_test1 = scaler.inverse_transform([y_test])\n# calculate root mean squared error for RNN\ntrainScore_RNN = math.sqrt(mean_squared_error(y_train1[0], predicted_stock_price_RNN_tr[:,0]))\nprint('Train Score of RNN: %.2f RMSE' % (trainScore_RNN))\ntestScore_RNN = math.sqrt(mean_squared_error(y_test1[0], predicted_stock_price_RNN[:,0]))\nprint('Test Score of RNN: %.2f RMSE' % (testScore_RNN))\n# calculate root mean squared error for LSTM\ntrainScore_LSTM = math.sqrt(mean_squared_error(y_train1[0], predicted_stock_price_LSTM_tr[:,0]))\nprint('Train Score of LSTM: %.2f RMSE' % (trainScore_LSTM))\ntestScore_LSTM = math.sqrt(mean_squared_error(y_test1[0], predicted_stock_price_LSTM[:,0]))\nprint('Test Score of LSTM: %.2f RMSE' % (testScore_LSTM))\n","1c357df2":"\nplt.plot(scaler.inverse_transform(y_train.reshape(-1,1)), color='red', label='Real Google Stock Price')\nplt.plot(predicted_stock_price_RNN_tr, color='blue', label='Predicted Google Stock Price with RNN')\nplt.plot(predicted_stock_price_LSTM_tr, color='black', label='Predicted Google Stock Price with LSTM')\nplt.title('Google Stock Price Prediction for Training')\nplt.xlabel('Time')\nplt.ylabel('Google Stock Price')\nplt.legend()\nplt.subplots_adjust(bottom=0.5, top=1)\nplt.show()\n\n\nplt.plot(real_stock_price, color='red', label='Real Google Stock Price')\nplt.plot(predicted_stock_price_RNN, color='blue', label='Predicted Google Stock Price with RNN')\nplt.plot(predicted_stock_price_LSTM, color='black', label='Predicted Google Stock Price with LSTM')\nplt.title('Google Stock Price Prediction for Testing')\nplt.xlabel('Time')\nplt.ylabel('Google Stock Price')\nplt.legend()\n\nplt.subplots_adjust(bottom=0.5, top=1)\nplt.show()","1bce13be":"data = pd.read_csv('..\/input\/international-airline-passengers\/international-airline-passengers.csv')\ndata.head()","a8cd02e9":"dataset = data.iloc[:, 1].values\nplt.plot(dataset)\nplt.xlabel('time')\nplt.ylabel('number of passengers (in thousands)')\nplt.title('Passengers')\nplt.show()","daa9f042":"dataset = dataset.reshape(-1,1) #(145, ) iken (145,1)e \u00e7evirdik\ndataset = dataset.astype('float32')\ndataset.shape","b0f8402f":"scaler = MinMaxScaler(feature_range= (0,1))\ndataset = scaler.fit_transform(dataset)","f8c42ab1":"train_size = int(len(dataset)*0.5)\ntest_size = len(dataset)- train_size\n\ntrain = dataset[0:train_size, :]\ntest = dataset[train_size:len(dataset), :]\n\nprint('train size: {}, test size: {}'.format(len(train), len(test)))","3c971f24":"dataX = []\ndatay = []\ntimestemp = 10\n\nfor i in range(len(train)- timestemp -1):\n    a = train[i:(i+timestemp), 0]\n    dataX.append(a)\n    datay.append(train[i + timestemp, 0])\n\n    \ntrainX, trainy = np.array(dataX), np.array(datay)","14b79ed4":"dataX = []\ndatay = []\nfor i in range(len(test)- timestemp -1):\n    a = test[i:(i+timestemp), 0]\n    dataX.append(a)\n    datay.append(test[i + timestemp, 0])\n\n    \ntestX, testy = np.array(dataX), np.array(datay)","e0737cc7":"trainX.shape","4b5071e4":"trainX = np.reshape(trainX, (trainX.shape[0],1,  trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0],1,  testX.shape[1]))","9627a637":"trainX.shape","3af4647c":"# ***********************************\n# Model of Long Short Term Memory (LSTM)\n# LSTM model\n# ***********************************\n\nmodel = Sequential()\nmodel.add(LSTM(10, input_shape=(1, timestemp))) # 10 lstm neuron(block)\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(trainX, trainy, epochs=100, batch_size=1)","dff0c828":"# ***********************************\n# Model of Recurrent Neural Network\n# RNN\n# ***********************************\n\n#Initialize RNN:\nmodel1 = Sequential()\n\n#Adding the first RNN layer and some Dropout regularization\nmodel1.add(SimpleRNN(units = 50, activation='tanh', return_sequences=True, input_shape=(1, timestemp)))\nmodel1.add(Dropout(0.2))\n\n#Adding the second RNN layer and some Dropout regularization\nmodel1.add(SimpleRNN(units = 50, activation='tanh', return_sequences=True))\nmodel1.add(Dropout(0.2))\n\n#Adding the third RNN layer and some Dropout regularization\nmodel1.add(SimpleRNN(units = 50, activation='tanh', return_sequences=True))\nmodel1.add(Dropout(0.2))\n\n#Adding the fourth RNN layer and some Dropout regularization\nmodel1.add(SimpleRNN(units = 50))\nmodel1.add(Dropout(0.2))\n\n#Adding the output layer\nmodel1.add(Dense(units = 1))\n\n#Compile the RNN\nmodel1.compile(optimizer='adam', loss='mean_squared_error')\n\n#Fitting the RNN to the Training set\nmodel1.fit(trainX, trainy, epochs=200, batch_size=32)","8208b6e4":"#make predictions\ntrainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\ntrainPredict1 = model1.predict(trainX)\ntestPredict1 = model1.predict(testX)","a3a59975":"# invert predictions\ntrainy1 = scaler.inverse_transform([trainy])\ntesty1 = scaler.inverse_transform([testy])\ntrainPredict_RNN = scaler.inverse_transform(trainPredict1)\ntestPredict_RNN = scaler.inverse_transform(testPredict1)\ntrainPredict_LSTM = scaler.inverse_transform(trainPredict)\ntestPredict_LSTM = scaler.inverse_transform(testPredict)","1e7bfca4":"# calculate root mean squared error\ntrainScore_RNN = math.sqrt(mean_squared_error(trainy1[0], trainPredict_RNN[:,0]))\nprint('Train Score: %.2f RMSE for RNN' % (trainScore_RNN))\ntestScore_RNN = math.sqrt(mean_squared_error(testy1[0], testPredict_RNN[:,0]))\nprint('Test Score: %.2f RMSE for RNN' % (testScore_RNN))\ntrainScore_LSTM = math.sqrt(mean_squared_error(trainy1[0], trainPredict_LSTM[:,0]))\nprint('Train Score: %.2f RMSE for LSTM' % (trainScore_LSTM))\ntestScore_LSTM = math.sqrt(mean_squared_error(testy1[0], testPredict_LSTM[:,0]))\nprint('Test Score: %.2f RMSE for LSTM' % (testScore_LSTM))","68dfe68a":"\nplt.plot(trainy1.T, color='red', label='Real number of passengers')\nplt.plot(trainPredict_RNN, color='blue', label='Predicted number of passengers with RNN')\nplt.plot(trainPredict_LSTM, color='black', label='Predicted number of passengers with LSTM')\nplt.title('Passengers for Training')\nplt.xlabel('Time')\nplt.ylabel('number of passengers (in thousands)')\nplt.legend()\nplt.subplots_adjust(bottom=0.5, top=1)\nplt.show()\n\n\nplt.plot(testy1.T, color='red', label='Real number of passengers')\nplt.plot(testPredict_RNN, color='blue', label='Predicted number of passengers with RNN')\nplt.plot(testPredict_LSTM, color='black', label='Predicted number of passengers with LSTM')\nplt.title('Passengers for Testing')\nplt.xlabel('Time')\nplt.ylabel('number of passengers (in thousands)')\nplt.legend()\n\nplt.subplots_adjust(bottom=0.5, top=1)\nplt.show()\n","29315531":"# shifting train\ntrainPredictPlot = np.empty_like(dataset)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[timestemp:len(trainPredict)+timestemp, :] = trainPredict\n# shifting test predictions for plotting\ntestPredictPlot = np.empty_like(dataset)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(trainPredict)+(timestemp*2)+1:len(dataset)-1, :] = testPredict\n# plot baseline and predictions\nplt.plot(scaler.inverse_transform(dataset))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()","2af4aa64":"<a id=\"10\"><\/a>\n**Prediction and Visualization of RNN Model**","9938d471":"<a id=\"2\"><\/a>\n**Recurrent Neural Network (RNN)**\n\n* RNN is able to remember important things about the input received, which enables them to be very precise in predicting what's coming next.\n\n* This is the reason why they are preferred algorithm for sequential data like time series, speech, text, financial data, audio, video because they can perform a much deeper understanding of a sequence and its content compared to the other algorithms.\n\n* Not only feeds output but also gives feed backs into itself. Because RNN has internal memory. How? RNN has hidden layers which have temporal loop (kendini besleyen zamansal d\u00f6ng\u00fcye sahiptir ve ge\u00e7ici belle\u011fe sahiptir). \n\n![temporal%20loop.jpg](attachment:temporal%20loop.jpg)\n\n* \u00d6rnek RNN Yap\u0131lar\u0131:\n\n    * One to Many: input image, output bunla ilgili bir c\u00fcmle\n    \n    ![one%20to%20many.jpg](attachment:one%20to%20many.jpg)\n    \n    * Many to one: input c\u00fcmle, output bu c\u00fcmleyle ilgili bir duygu \n    \n    ![many%20to%20one.jpg](attachment:many%20to%20one.jpg)\n    \n    * Many to many: mesela google translate kullanarak bir c\u00fcmle translate etmek\n    \n    ![many%20to%20many.jpg](attachment:many%20to%20many.jpg)\n    \n    \n    \n* Not: RNN short term memory'ye sahip, ancak LSTM long term memory'ye sahip olabilir. \n\n* Exploiding Gradients: Gradient'in \u00e7ok b\u00fcy\u00fck olmas\u0131 ve gereksiz yere weight'lere \u00f6nem kazand\u0131rmas\u0131 durumu.\n\n* Vanishing Gradients: Gradient'in \u00e7ok k\u00fc\u00e7\u00fck olmas\u0131 ve yava\u015f \u00f6\u011frenme durumu\n\n* Gradient: Weight'lerdeki cost'a g\u00f6re de\u011fi\u015fim","58ebf45e":"<a id=\"13\"><\/a>\n**Loading Data**","03b1dd4e":"## Content\n* [Sequence Models](#1)\n* [Recurrent Neural Network (RNN)](#2)\n    * [Implementing Recurrent Neural Network with Keras](#3)\n        * [Importing and Preprocessing Data](#4)\n            * [Loading Data](#5)\n            * [Feature Scaling](#6)\n            * [Create Data Structure](#7)\n            * [Reshape](#8)\n        * [Create RNN Model](#9)\n        * [Prediction and Visualization of RNN Model](#10)\n* [Long Short Term Memory (LSTM)](#11) \n    * [Implementing Long Short Term Memory with Keras](#12)\n        * [Loading Data](#13)\n        * [Preprocessing Data](#14)\n            * [Reshape](#15)\n            * [Scaling](#16)\n            * [Train Test Split](#17)\n        * [Create LSTM Model](#18)\n        * [Predictions and Visualising LSTM Model](#19)\n","507abe32":"<a id=\"3\"><\/a>\n**Implementing RNN with KERAS**","79c093e7":"<a id=\"5\"><\/a>\n**Loading Data:**","4b42e791":"<a id=\"11\"><\/a>\n**Long Short Term Memory (LSTM)**\n\n* LSTM, RNN'in bir t\u00fcr\u00fcd\u00fcr.\n\n* RNN'den farkl\u0131 olarak long term memory'den bahsedebiliriz.\n\n* LSTM architecture:\n\n    * x: Scaling of information (x=0 ise 1'den gelen bilgi Ct-1 okuna dahil olamayacak)\n    \n    * +: Adding information (+da gelen bilginin dahil olamamas\u0131 gibi bir durum s\u00f6z konusu de\u011fil, +'n\u0131n a\u015fa\u011f\u0131s\u0131ndaki oktan gelen bilgi her zaman eklenecek)\n    \n    * Sigmoid layer: Sigmoid memory'den bir\u015feyi hat\u0131rlamak i\u00e7in ya da unutmak i\u00e7in kullan\u0131l\u0131r.  1 ya da 0'd\u0131r. gate olarak adland\u0131r\u0131l\u0131r.\n    \n    * tanh: Activation function olarak kullan\u0131l\u0131r \u00e7\u00fcnk\u00fc t\u00fcrevi hemen 0'a inmez, bu da vanishing gradient(yava\u015f \u00f6\u011frenme) problemini \u00e7\u00f6zer.\n    \n    * h(t-1): Output of LSTM unit (bir \u00f6nceki layer'dan gelen output).\n    \n    * c(t-1): Memory from previous LSTM unit\n    \n    * X(t): input\n    \n    * c(t): new updated memory\n    \n    * h(t): output\n    \n    * From c(t-1) to c(t) is memory pipeline\n    \n    * Oklar vekt\u00f6rel de\u011ferler\n    \n![lstm.jpg](attachment:lstm.jpg)\n\n\n* Yap\u0131lar (g\u00f6rselde 1, 2, 3)\n    * 1-Forget Gate: Input olarak X(t) ve h(t-1) al\u0131r. Gelen bilginin unutulup unutulmayaca\u011f\u0131na karar verir.\n    \n    * 2-Input Gate: Hangi bilginin memoryde depolan\u0131p depolanmayaca\u011f\u0131na karar verir. 2 numaral\u0131 oka gelen bilgi sigmoid'den ge\u00e7erek 0 ya da 1 de\u011ferini alarak tanh ile aktivasyona giriyor. Yani, yukar\u0131daki gibi sigmoid varsa 0-1.\n    \n    * 3-Output Gate: Hangi bilginin output olup olmayaca\u011f\u0131na karar veriyor.","ffc87cbc":"<a id=\"14\"><\/a>\n**Preprocessing Data**","8e0b274f":"<a id=\"8\"><\/a>\n**Reshape:**","ed232b85":"<a id=\"1\"><\/a>\n**Sequence Models**\n\n* Sequence models plays an over time.\n\n* Speech recognition, NLP, music generation (Apple siri, Goggle's voice search...)\n\n* Sentiment classification (duygu s\u0131n\u0131fland\u0131rma). \u00d6rne\u011fin, bir c\u00fcmlenin olumlu - olumsuz olma durumunu anlama\n\n* \u00d6zetle, Sequence models time series modeldir.","8a18304f":"<a id=\"7\"><\/a>\n**Create Data Structure:**","20bb641d":"<a id=\"16\"><\/a>\n**Scaling:**","89f377f7":"<a id=\"17\"><\/a>\n**Train Test Split:**","92f1fcd0":"<a id=\"6\"><\/a>\n**Feature Scaling:**","de044a8e":"<a id=\"18\"><\/a>\n**Create LSTM Model**","ee08f405":"<a id=\"15\"><\/a>\n**Reshape:**","375ccd35":"<a id=\"12\"><\/a>\n**Implementing LSTM with KERAS**","3217a85b":"<a id=\"19\"><\/a>\n**Prediction**","7c56fe43":"* Epochs art\u0131r\u0131lbilir\n* Batch size de\u011fi\u015ftirilebilir\n* Activation functionlar de\u011fi\u015ftirilebilir\n* Layer say\u0131s\u0131 art\u0131r\u0131labilir","cd941d86":"<a id=\"9\"><\/a>\n**Create RNN Model**","53db5dc9":"<a id=\"4\"><\/a>\n**Importing and Preprocessind Data**"}}