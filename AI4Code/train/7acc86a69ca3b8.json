{"cell_type":{"780169e0":"code","7b67a2a5":"code","e20761b6":"code","f24f2e63":"code","0a6492f6":"code","3080b747":"code","5722e543":"code","40f796a6":"code","76aff295":"code","5bbabdd9":"code","a47dfe05":"code","4e4545e2":"code","bb52fc7f":"code","713492fa":"code","87f17967":"code","2adbef52":"code","b7da40c7":"code","fd36ea6b":"code","a0db8767":"code","c3cf0861":"code","8f522ddd":"code","213220d0":"code","d215edcd":"code","8bc74169":"code","ac505f8d":"code","b4265126":"code","8da7aea7":"code","ace33f85":"code","77148982":"code","1d16f83a":"code","86ca35c9":"code","ec26372e":"code","19128279":"code","40cf3754":"code","84fbb57c":"code","2da0651a":"code","df842688":"code","95988140":"code","5aedda56":"code","1f54effd":"code","69e4e0f0":"code","3856ce33":"markdown","ad765964":"markdown","64a46e82":"markdown","ec39d55f":"markdown","5c0dd957":"markdown","c00982bd":"markdown","2c8b184d":"markdown","f7b6e157":"markdown","59aa5a36":"markdown","5191b74b":"markdown","0538a906":"markdown","7cfa6f2a":"markdown","4dbd10c5":"markdown","a65bea2b":"markdown","6135558d":"markdown","d4a372b9":"markdown","b6564a2d":"markdown","55fba4bf":"markdown","a55a30f3":"markdown","8a108227":"markdown","84ff3d35":"markdown","afe26415":"markdown","f075ac45":"markdown","3133859b":"markdown","ebfdcb61":"markdown","74b27ca3":"markdown","41937a21":"markdown","f369acf0":"markdown","ee0e628b":"markdown","6261a543":"markdown","8cb7b3c1":"markdown","ec2a7306":"markdown","98eb3fb1":"markdown","e0cafe32":"markdown","46e195e1":"markdown","09c10a85":"markdown"},"source":{"780169e0":"#First we import a few libraries we will use extensively\nimport pandas as pd\nimport numpy as np\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","7b67a2a5":"from sklearn import model_selection, preprocessing\n\ndef open_data(file):\n    #Readind training data\n    data = pd.read_csv(\"..\/input\/\"+file)\n\n    #Droping Name, Cabin and Ticket as probably irrelevant\n    data = data.drop([\"Name\", \"Ticket\", \"Cabin\"], 1)\n    \n    #Converting string features into integers\n    le = preprocessing.LabelEncoder()\n    data[\"Sex\"] = le.fit_transform(list(data[\"Sex\"]))\n    data[\"Embarked\"] = le.fit_transform(list(data[\"Embarked\"]))\n\n    #Filling NaN values for age with the average value\n    data[\"Age\"] = data[\"Age\"].fillna(value = data.Age.mean())\n    data[\"Fare\"] = data[\"Fare\"].fillna(value = data.Fare.mean())\n    \n    return data","e20761b6":"def param_label(data):\n    data = data.drop([\"PassengerId\"], 1)\n    return data.drop([\"Survived\"], 1), data[[\"Survived\"]]","f24f2e63":"def subset_data(X, Y, n):\n    return model_selection.train_test_split(X, Y, test_size = n)","0a6492f6":"data = open_data(\"train.csv\")\n\ny_true = data[[\"Survived\"]]\ny_test = np.array([1 for i in range(len(y_true))])\nprint(\"Accuracy for survived = 1: \",metrics.accuracy_score(y_true, y_test))","3080b747":"data = open_data(\"train.csv\")\n\ny_true = data[[\"Survived\"]]\ny_test = np.array([random.choice((0, 1)) for i in range(len(y_true))])\nprint(\"Accuracy for random survival: \",metrics.accuracy_score(y_true, y_test))","5722e543":"data = open_data(\"train.csv\")\n\ny_true = data[[\"Survived\"]]\ny_test = np.array([0 for i in range(len(y_true))])\nprint(\"Accuracy for survived = 1: \",metrics.accuracy_score(y_true, y_test))","40f796a6":"#Loading test sample\ndata_test = open_data(\"test.csv\")\n\n#Setting all values to 0\nsolution = pd.DataFrame(np.array([[data_test.PassengerId.iloc[i], 0] for i in range(len(data_test))]),\n                        columns=['PassengerId', 'Survived'])\n\n#Saving as csv\nsolution.to_csv(\"solution_naive.csv\", index=False)","76aff295":"from sklearn.neighbors import KNeighborsClassifier","5bbabdd9":"data = open_data(\"train.csv\")\nX, Y = param_label(data)\nx_train, x_test, y_train, y_test = subset_data(X, Y, 0.2)","a47dfe05":"#Model creation\nmodel = KNeighborsClassifier()\nmodel.fit(x_train, y_train)\nacc = metrics.accuracy_score(model.predict(x_test), y_test)\n\nprint(\"Accuracy : \" + str(acc))","4e4545e2":"neighboors = [i for i in range(1, 101)]\n\naverages = []\nmins = []\nmaxs = []\n\nfor n in neighboors:\n    average_acc = 0\n    min_acc = 1\n    max_acc = 0\n    \n    #The accurracy may vary depending on the subset used, so we try 100 times with different subsets to get a better assessment.\n    for i in range(100):\n        x_train, x_test, y_train, y_test = subset_data(X, Y, 0.2)\n        model = KNeighborsClassifier(n_neighbors = n)\n        model.fit(x_train, y_train)\n        acc = metrics.accuracy_score(model.predict(x_test), y_test)\n        \n        average_acc = average_acc + acc\n        if acc > max_acc: max_acc = acc\n        if acc < min_acc: min_acc = acc\n        \n    averages = averages + [average_acc\/100]\n    mins = mins + [min_acc]\n    maxs = maxs + [max_acc]\n    \n#Ploting results\nplt.figure(figsize=(24,8))\nplt.plot(averages, color = 'r', linewidth=2)\nplt.plot(mins, color = 'r', linestyle='--')\nplt.plot(maxs, color = 'r', linestyle='--')\nplt.xticks(neighboors)","bb52fc7f":"#Model training\nmodel = KNeighborsClassifier(n_neighbors = 12)\nmodel.fit(X, Y)\n\n#Results prediction and submission\ndata_test = open_data(\"test.csv\")\nprediction = model.predict(data_test.drop([\"PassengerId\"], 1))\nsolution = pd.DataFrame(np.array([[data_test.PassengerId.iloc[i], prediction[i]] for i in range(len(data_test))]),\n                        columns=['PassengerId', 'Survived'])\nsolution.to_csv(\"solution_KNN_allfeatures.csv\", index=False)","713492fa":"data = open_data(\"train.csv\")\nX, Y = param_label(data)\nX = X[[\"Pclass\", \"Sex\"]]\nx_train, x_test, y_train, y_test = subset_data(X, Y, 0.2)\n\nmodel = KNeighborsClassifier()\nmodel.fit(x_train, y_train)\nacc = metrics.accuracy_score(model.predict(x_test), y_test)\n\nprint(\"Accuracy : \" + str(acc))","87f17967":"model = KNeighborsClassifier(n_neighbors = 8)\nmodel.fit(x_train, y_train)\nacc = metrics.accuracy_score(model.predict(x_test), y_test)\n\nprint(\"Accuracy : \" + str(acc))","2adbef52":"#Model training\nmodel = KNeighborsClassifier(n_neighbors = 8)\nmodel.fit(X[[\"Pclass\", \"Sex\"]], Y)\n\n#Results prediction and submission\ndata_test = open_data(\"test.csv\")\nprediction = model.predict(data_test[[\"Pclass\", \"Sex\"]])\nsolution = pd.DataFrame(np.array([[data_test.PassengerId.iloc[i], prediction[i]] for i in range(len(data_test))]),\n                        columns=['PassengerId', 'Survived'])\nsolution.to_csv(\"solution_KNN_Class_Sex.csv\", index=False)","b7da40c7":"data[\"FamilyMembers\"] = data[\"SibSp\"]+data[\"Parch\"]\n\nX, Y = param_label(data)\nX = X[[\"Pclass\", \"Sex\", \"FamilyMembers\"]]\nx_train, x_test, y_train, y_test = subset_data(X, Y, 0.2)\n\nmodel = KNeighborsClassifier(n_neighbors = 8)\nmodel.fit(x_train, y_train)\nacc = metrics.accuracy_score(model.predict(x_test), y_test)\n\nprint(\"Accuracy : \" + str(acc))","fd36ea6b":"#Model training\nmodel = KNeighborsClassifier(n_neighbors = 8)\nmodel.fit(X[[\"Pclass\", \"Sex\", \"FamilyMembers\"]], Y)\n\n#Results prediction and submission\ndata_test = open_data(\"test.csv\")\ndata_test[\"FamilyMembers\"] = data_test[\"SibSp\"]+data_test[\"Parch\"]\nprediction = model.predict(data_test[[\"Pclass\", \"Sex\",\"FamilyMembers\"]])\nsolution = pd.DataFrame(np.array([[data_test.PassengerId.iloc[i], prediction[i]] for i in range(len(data_test))]),\n                        columns=['PassengerId', 'Survived'])\nsolution.to_csv(\"solution_KNN_Class_Sex_Family.csv\", index=False)","a0db8767":"from sklearn import svm\n\ndata = open_data(\"train.csv\")\nX, Y = param_label(data)\nx_train, x_test, y_train, y_test = subset_data(X, Y, 0.2)\n\nmodel = svm.SVC()\nmodel.fit(x_train, y_train)\n\ny_predict = model.predict(x_test)\n    \nacc = metrics.accuracy_score(y_predict, y_test)\n    \nprint(\"Accuracy:\",acc)","c3cf0861":"kernels = [\"rbf\", \"linear\", \"sigmoid\", \"poly\"]\n\nfor kernel in kernels:\n    model = svm.SVC(kernel = kernel)\n    model.fit(x_train, y_train)\n    \n    y_predict = model.predict(x_test)\n    \n    acc = metrics.accuracy_score(y_predict, y_test)\n    \n    print(\"Accuracy with kernel =\", kernel, \": \",acc)","8f522ddd":"#Model training\nmodel = svm.SVC(kernel = 'linear')\nmodel.fit(X,Y)\n\n#Results prediction and submission\ndata_test = open_data(\"test.csv\")\nprediction = model.predict(data_test.drop([\"PassengerId\"], 1))\nsolution = pd.DataFrame(np.array([[data_test.PassengerId.iloc[i], prediction[i]] for i in range(len(data_test))]),\n                        columns=['PassengerId', 'Survived'])\nsolution.to_csv(\"solution_SVM.csv\", index=False)","213220d0":"cs = [1, 5, 10, 15, 20]\ngammas = [0.005, 0.01, 0.02, 0.05, 0.1]\n\nfor gamma in gammas:\n\n    for c in cs:\n\n        model = svm.SVC(kernel = 'rbf', C = c, gamma = gamma)\n        model.fit(x_train, y_train)\n        acc = metrics.accuracy_score(model.predict(x_test), y_test)\n        print(\"Accuracy with gamma = \",gamma,\"c = \",c,\": \",acc)\n        \n    print(\"\")","d215edcd":"#Model training\nmodel = svm.SVC(kernel = 'rbf', gamma =  0.01, C = 10)\nmodel.fit(X,Y)\n\n#Results prediction and submission\ndata_test = open_data(\"test.csv\")\nprediction = model.predict(data_test.drop([\"PassengerId\"], 1))\nsolution = pd.DataFrame(np.array([[data_test.PassengerId.iloc[i], prediction[i]] for i in range(len(data_test))]),\n                        columns=['PassengerId', 'Survived'])\nsolution.to_csv(\"solution_SVM_para.csv\", index=False)","8bc74169":"from sklearn.ensemble import RandomForestClassifier","ac505f8d":"data = open_data(\"train.csv\")\nX, Y = param_label(data)\nx_train, x_test, y_train, y_test = subset_data(X, Y, 0.2)","b4265126":"model = RandomForestClassifier()\nmodel.fit(x_train, y_train)\n\ny_predict = model.predict(x_test)\ny_predict\nacc = metrics.accuracy_score(y_predict, y_test)\n\nprint(acc)","8da7aea7":"trees = [5, 10, 20, 50, 100]\n\naverages = []\nmins = []\nmaxs = []\n\nfor tree in trees:\n    average_acc = 0\n    min_acc = 1\n    max_acc = 0\n    \n    for i in range(100):\n        x_train, x_test, y_train, y_test = subset_data(X, Y, 0.2)\n\n        model = RandomForestClassifier(n_estimators = tree)\n        model.fit(x_train, y_train)\n\n        y_predict = model.predict(x_test)\n        y_predict\n        acc = metrics.accuracy_score(y_predict, y_test)\n\n        average_acc = average_acc + acc\n        if acc > max_acc: max_acc = acc\n        if acc < min_acc: min_acc = acc\n\n    averages = averages + [average_acc\/100]\n    mins = mins + [min_acc]\n    maxs = maxs + [max_acc]\n    \n#Ploting results\nplt.figure(figsize=(24,8))\nplt.plot(averages, color = 'r', linewidth=2)\nplt.plot(mins, color = 'r', linestyle='--')\nplt.plot(maxs, color = 'r', linestyle='--')","ace33f85":"depths = [1, 2, 5, 10, 15, 20]\n\naverages = []\nmins = []\nmaxs = []\n\nfor depth in depths:\n    average_acc = 0\n    min_acc = 1\n    max_acc = 0\n    \n    for i in range(100):\n        x_train, x_test, y_train, y_test = subset_data(X, Y, 0.2)\n\n        model = RandomForestClassifier(n_estimators = 20, max_depth = depth)\n        model.fit(x_train, y_train)\n\n        y_predict = model.predict(x_test)\n        y_predict\n        acc = metrics.accuracy_score(y_predict, y_test)\n\n        average_acc = average_acc + acc\n        if acc > max_acc: max_acc = acc\n        if acc < min_acc: min_acc = acc\n\n    averages = averages + [average_acc\/100]\n    mins = mins + [min_acc]\n    maxs = maxs + [max_acc]\n    \n#Ploting results\nplt.figure(figsize=(24,8))\nplt.plot(averages, color = 'r', linewidth=2)\nplt.plot(mins, color = 'r', linestyle='--')\nplt.plot(maxs, color = 'r', linestyle='--')","77148982":"#Model training\nmodel = RandomForestClassifier(n_estimators = 20, max_depth = 10)\nmodel.fit(X,Y)\n\n#Results prediction and submission\ndata_test = open_data(\"test.csv\")\nprediction = model.predict(data_test.drop([\"PassengerId\"], 1))\nsolution = pd.DataFrame(np.array([[data_test.PassengerId.iloc[i], prediction[i]] for i in range(len(data_test))]),\n                        columns=['PassengerId', 'Survived'])\nsolution.to_csv(\"solution_Random_Forest.csv\", index=False)","1d16f83a":"feature_w = pd.Series(model.feature_importances_,index=X.columns).sort_values(ascending=False)\nprint(feature_w)","86ca35c9":"#Model training\nmodel = RandomForestClassifier(n_estimators = 20, max_depth = 10)\nmodel.fit(X[[\"Sex\", \"Age\", \"Fare\", \"Pclass\"]],Y)\n\n#Results prediction and submission\ndata_test = open_data(\"test.csv\")\nprediction = model.predict(data_test[[\"Sex\", \"Age\", \"Fare\", \"Pclass\"]])\nsolution = pd.DataFrame(np.array([[data_test.PassengerId.iloc[i], prediction[i]] for i in range(len(data_test))]),\n                        columns=['PassengerId', 'Survived'])\nsolution.to_csv(\"solution_Random_Forest_4features.csv\", index=False)","ec26372e":"import tensorflow as tf\nfrom tensorflow import keras","19128279":"data = open_data(\"train.csv\")\nX, Y = param_label(data)\nx_train, x_test, y_train, y_test = subset_data(X, Y, 0.2)","40cf3754":"#I use .describe() to get mean and standard deviation\nstats = X.describe()\nstats = stats.transpose()\n\ndef norm(x):\n    return (x - stats['mean']) \/ stats['std']\n\nnormed_x_train = norm(x_train)\nnormed_x_test = norm(x_test)","84fbb57c":"def build_model():\n    model = keras.Sequential()\n    model.add(keras.layers.Dense(32, activation='relu', kernel_initializer = 'uniform', input_shape=[len(X.keys())]))\n    model.add(keras.layers.Dense(12, activation='relu', kernel_initializer = 'uniform'))\n    model.add(keras.layers.Dense(1, activation='sigmoid', kernel_initializer = 'uniform'))\n    \n    model.compile(loss='binary_crossentropy',\n                optimizer='adam',\n                metrics=['accuracy'])\n    return model\n\nmodel = build_model()\nmodel.summary()","2da0651a":"def plot_history(history):\n    hist = pd.DataFrame(history.history)\n    hist['epoch'] = history.epoch\n\n    plt.figure()\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.plot(hist['epoch'], hist['acc'])\n    plt.ylim([0,1])\n    plt.legend()\n\n    plt.show()","df842688":"model = build_model()\n\n# Display training progress by printing a single dot for each completed epoch\nclass PrintDot(keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs):\n        if epoch % 100 == 0: print('')\n        print('.', end='')\n\nEPOCHS = 1000\n\n# The patience parameter is the amount of epochs to check for improvement\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n\nhistory = model.fit(normed_x_train, y_train, epochs=EPOCHS, \n                    validation_split = 0.2, verbose=0, callbacks=[early_stop, PrintDot()])\n\nplot_history(history)","95988140":"y_pred = model.predict(x_test)\ny_pred = (y_pred > 0.5).astype(int).reshape(x_test.shape[0])\nmetrics.accuracy_score(y_pred, y_test)","5aedda56":"architectures = [[12, 6],\n                 [32, 16],\n                 [64, 32],\n                 [12, 12, 6],\n                 [32, 16, 8],\n                 [64, 32, 16],\n                 [12, 12, 6, 6],\n                 [32, 32, 16, 8],\n                 [64, 32, 16, 8],\n                 [64, 64, 32, 16, 8]]","1f54effd":"def build_model(architecture):\n    model = keras.Sequential()\n    model.add(keras.layers.Dense(architecture[0], activation='relu', kernel_initializer = 'uniform', input_shape=[len(X.keys())]))\n    \n    for i in range(1, len(architecture)):\n        n = architecture[i]\n        model.add(keras.layers.Dense(n, activation='relu', kernel_initializer = 'uniform'))\n        \n    model.add(keras.layers.Dense(1, activation='sigmoid', kernel_initializer = 'uniform'))\n    \n    model.compile(loss='binary_crossentropy',\n                optimizer='adam',\n                metrics=['accuracy'])\n    return model","69e4e0f0":"EPOCHS = 1000\n\naverages = []\nmins = []\nmaxs = []\n\nfor architecture in architectures:\n    model = build_model(architecture)\n    \n    average_acc = 0\n    min_acc = 1\n    max_acc = 0\n    \n    for i in range(100):\n        x_train, x_test, y_train, y_test = subset_data(X, Y, 0.2)\n        history = model.fit(normed_x_train, y_train, epochs=EPOCHS, \n                        validation_split = 0.2, verbose=0, callbacks=[early_stop])\n\n        y_pred = model.predict(x_test)\n        y_pred = (y_pred > 0.5).astype(int).reshape(x_test.shape[0])\n        acc = metrics.accuracy_score(y_pred, y_test)\n        \n        average_acc = average_acc + acc\n        if acc > max_acc: max_acc = acc\n        if acc < min_acc: min_acc = acc\n            \n    averages = averages + [average_acc\/100]\n    mins = mins + [min_acc]\n    maxs = maxs + [max_acc]\n    \n    print(\"Accuracy with architecture \", architecture, \": \", average_acc\/100)\n    \n#Ploting results\nplt.figure(figsize=(24,8))\nplt.plot(averages, color = 'r', linewidth=2)\nplt.plot(mins, color = 'r', linestyle='--')\nplt.plot(maxs, color = 'r', linestyle='--')","3856ce33":"What if we just use the four most important features? It seems we get consistently better results...","ad765964":"Great start. Maybe we can improve by changing the number of trees in our forest?","64a46e82":"We get approximately 0.8 with train data but far lower with test data. Our neural network is obviously overfitting. In addition, results seems to vary a lot on each run sometimes as low as 0.3 and sometimes above 0.6.\n\nI modified the code to easily test various architecture:","ec39d55f":"**Support vector classification**\n\nFirst, let's try the SVM module from sklearn with default parameters:","5c0dd957":"We will first normalize our data:","c00982bd":"We got a 0.62679 accuraccy and ranked somewhere in the 11.200... Not great but a few hundreds people did worse.\n\nHistorically approximately two thirds of the passengers died, as a result if we set everyone to 0 we will get an accurracy of approximately 0.6. This will be our reference any approach that get a higher accurracy is somewhat good and any idea that get less than 0.6, well, is just s**t.","2c8b184d":"**Neural Network**","f7b6e157":"Nope. That wasn't better. We just got 0.72248 this time. Maybe it's time to move to another method?","59aa5a36":"This version got a disapointing 0.71770 accuracy rate. Let see if we can use another algorithme to improve our best score","5191b74b":"It seems we will get the best results with the number of neighboors somewhere around 12. Let's try that...","0538a906":"**Random Forest**","7cfa6f2a":"We score at 0.77990 and gain a few 3000 places. Not bad...\n\nNow, let's move to another approach.","4dbd10c5":"2. Random survival","a65bea2b":"10 seems to be a good guess. Let's try that:","6135558d":"This one generate a random subset we can use to test each approach.","d4a372b9":"**Naive approachs**\n\nI began with very naive approachs : what if everyone survive ? No one survive ? survival is random ?","b6564a2d":"3. Everyone die ","55fba4bf":"example_batch = normed_x_train[:10]\nexample_result = model.predict(example_batch)\nexample_result","a55a30f3":"1. Everybody survive","8a108227":"It's official: my neural network don't pass the test. I tried other architectures but almost never got more that 0.7. I probably did something wrong but I can't figure out what...\n\nAnyway, that's all for this notebook. What did I learn in this notebook?\n1. Trying multiple solutions and multiple parameters seems to be an efficient way to get better result even though I admit I used brute force more than intuition.\n2. The choice of features appears to be crucial to obtain better results\n3. Despite all the hype, neural network are hard to make work correctly. A simple algoritm with good parameters delivers better results than a neural network with a random architecture.\n\nIf you know other algorithms that I did not try or if you spot a way to improve the accuracy, feel free to share them in comment!","84ff3d35":"We got 0.76076. Not bad.\n\nIn addition RandomForest has a nice tool that allow you to see the relative weight of each features:","afe26415":"This time, we get 0.66507. OK, it's better but not really that better.\n\nWhat if we reduce the number of features? For instance, if we just keep class and sex.","f075ac45":"**Functions for data preprocessing**\n\nThese function will be used for all approach","3133859b":"This function open the training data, drop some parameters that seems less usefull, convert string into integers for Sex and Embarked and fill NaN values","ebfdcb61":"This function separate parameters from labels.","74b27ca3":"Hey! 0.62 is not bad. Let's summit that and see how we performe...\n\nThe following lines generate a csv to be sumbmitted.","41937a21":"We get the best results with a linear kernel. Poly give us more or less the same accuracy but is far more time consuming. So let's go with linear:","f369acf0":"This time we get 0.75598! Almost 10 points more than our previous attempt. And we enter the top 10.000. \n\nThe key for a successful KNN classification seems to choose a limited amount of high-impact features. Maybe we can try to reduce the number of features from the original data set? For instance, instead of having 2 features SibSp and Parch we can have only one FamilyMembers which is the sum of the two previous one?","ee0e628b":"**About this notebook**\n\nThis is my first try with kaggle and virtually my first AI project although I have some experience with python. Beforehand I saw a few videos about machine learning, as a result I had some ideas of what I could try but not the first clue about the best strategy to solve this problem.\n\nIn this notebook I tried various solutions from the most naive to more complex. For each approach I tried to improve the performance by testing different parameters. The key idea here is trial and error. I tried the following algoritms:\n1. k-nearest neighbors\n2. Support vector classification\n3. Random forest\n4. Neural Network\n\nIn this process, I was able to learn a lot and see the accuracy of my submissions grow from 0.62 to 0.78. \n\nI think this notebook can be a source of inspiration if you are new to AI and have no idea where to start. It may also be valuable if you already have one solution for this competition and want to browse quickly other approachs.","6261a543":"That's, hem, slightly better. Maybe we can get a better accuracy by changing the number of neighboors?","8cb7b3c1":"We got 0.76555, our best score so far. Great!\n\nLet's try to tweak other parameters like C (Penalty parameter) and gamma:","ec2a7306":"**k-nearest neighbors**","98eb3fb1":"This look promising. Maybe we can try with different kernel?","e0cafe32":"Let see if it's working...","46e195e1":"Whaou! That's a breakthrough. We can get an even better result by adjusting the number of neighbours:","09c10a85":"It seems the results are nor really improving when we add more than 20 trees. What abot changing the max depth of a tree?"}}