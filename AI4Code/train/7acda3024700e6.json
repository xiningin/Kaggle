{"cell_type":{"99eb634e":"code","562bbc87":"code","5a4f7347":"code","747c00a9":"code","9aa912b1":"code","52fae691":"code","a3dbf26e":"code","bf287e7d":"code","f6b087d1":"code","507782b2":"code","e12233b1":"code","db6b62e6":"code","f0a52460":"code","f30c5bb7":"code","08ef6791":"code","50602a4b":"code","aaa134de":"code","59690d96":"code","57133ffe":"markdown","5ab943fe":"markdown","070b6c85":"markdown","a3b7d14f":"markdown","d5220566":"markdown","e6858f33":"markdown","483de5b2":"markdown","19264618":"markdown","8c146507":"markdown","57a60aed":"markdown","c4a16791":"markdown","48e7e62a":"markdown","5829ac1c":"markdown","2f813687":"markdown","bd848036":"markdown","6e25f700":"markdown","64ac5dc3":"markdown"},"source":{"99eb634e":"from sklearn.model_selection import train_test_split\nfrom plotly.offline import init_notebook_mode, iplot\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier,Pool\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.gridspec as gridspec\nfrom IPython.display import display\nimport matplotlib.patches as patch\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nfrom scipy.stats import norm\nimport plotly.plotly as py\nfrom sklearn import svm\nimport lightgbm as lgb\nimport catboost as cb\nimport xgboost as xgb\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport pickle\nimport time\nimport glob\nimport sys\nimport os\nimport gc\ngc.enable()","562bbc87":"#fold_n=5\n#folds = StratifiedKFold(n_splits=fold_n, shuffle=True, random_state=10)\n%matplotlib inline\n%precision 4\nwarnings.filterwarnings('ignore')\nplt.style.use('ggplot')\nnp.set_printoptions(suppress=True)\npd.set_option(\"display.precision\", 15)","5a4f7347":"train= pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv('..\/input\/test.csv')","747c00a9":"#Based on this great kernel https:\/\/www.kaggle.com\/arjanso\/reducing-dataframe-memory-size-by-65\ndef reduce_mem_usage(df):\n    start_mem_usg = df.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in df.columns:\n        if df[col].dtype != object:  # Exclude strings\n            \n            # Print current column type\n            print(\"******************************\")\n            print(\"Column: \",col)\n            print(\"dtype before: \",df[col].dtype)\n            \n            # make variables for Int, max and min\n            IsInt = False\n            mx = df[col].max()\n            mn = df[col].min()\n            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(df[col]).all(): \n                NAlist.append(col)\n                df[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = df[col].fillna(0).astype(np.int64)\n            result = (df[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        df[col] = df[col].astype(np.uint8)\n                    elif mx < 65535:\n                        df[col] = df[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        df[col] = df[col].astype(np.uint32)\n                    else:\n                        df[col] = df[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                df[col] = df[col].astype(np.float32)\n            \n            # Print new column type\n            print(\"dtype after: \",df[col].dtype)\n            print(\"******************************\")\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = df.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg\/start_mem_usg,\"% of the initial size\")\n    return df, NAlist","9aa912b1":"train, NAlist = reduce_mem_usage(train)\nprint(\"_________________\")\nprint(\"\")\nprint(\"Warning: the following columns have missing values filled with 'df['column_name'].min() -1': \")\nprint(\"_________________\")\nprint(\"\")\nprint(NAlist)","52fae691":"test, NAlist = reduce_mem_usage(test)\nprint(\"_________________\")\nprint(\"\")\nprint(\"Warning: the following columns have missing values filled with 'df['column_name'].min() -1': \")\nprint(\"_________________\")\nprint(\"\")\nprint(NAlist)","a3dbf26e":"\ninit_notebook_mode(connected=True)\ncnt_trgt = train.target.value_counts()\n\niplot([go.Bar(x=cnt_trgt.index, y=cnt_trgt.values)])","bf287e7d":"cols=[\"target\",\"ID_code\"]\nX = train.drop(cols,axis=1)\ny = train[\"target\"]","f6b087d1":"#from sklearn.feature_selection import RFECV\n#rfr=RandomForestClassifier(random_state=0)\n#rfecv = RFECV(estimator=rfr, step=10, min_features_to_select=50, cv=StratifiedKFold(2),\n#              scoring='accuracy', verbose =2)\n#rfecv.fit(X, y)","507782b2":"#cols = rfecv.get_support(indices=True)\n# X= X.iloc[:,cols]","e12233b1":"features= ['var_0', 'var_1', 'var_2', 'var_5', 'var_6', 'var_9', 'var_12',\n       'var_13', 'var_18', 'var_21', 'var_22', 'var_26', 'var_33', 'var_34',\n       'var_40', 'var_44', 'var_53', 'var_56', 'var_67', 'var_75', 'var_76',\n       'var_78', 'var_80', 'var_81', 'var_86', 'var_91', 'var_92', 'var_93',\n       'var_94', 'var_95', 'var_99', 'var_108', 'var_109', 'var_110',\n       'var_115', 'var_118', 'var_121', 'var_122', 'var_123', 'var_133',\n       'var_139', 'var_146', 'var_147', 'var_148', 'var_154', 'var_155',\n       'var_157', 'var_164', 'var_165', 'var_166', 'var_169', 'var_170',\n       'var_174', 'var_177', 'var_179', 'var_184', 'var_188', 'var_190',\n       'var_191', 'var_198']\ntrain_df = X.loc[:,features]\ntest_df = test.loc[:,features]\n","db6b62e6":"\ncolumns = train_df.columns\n\npos = train.target == 1\nneg = train.target == 0\n\ngrid = gridspec.GridSpec(100, 2)\nplt.figure(figsize=(15,100*4))\n\nfor n, col in enumerate(train_df[columns]):\n    ax = plt.subplot(grid[n])\n    sns.distplot(train[col][pos], bins = 50, color='c') #Will receive the \"semi-salmon\" violin\n    sns.distplot(train[col][neg], bins = 50, color='y') #Will receive the \"ocean\" color\n    ax.set_ylabel('Density')\n    ax.set_title(str(col))\n    ax.set_xlabel('')\nplt.show()","f0a52460":"def fit_lgb(X_fit, y_fit, X_val, y_val, counter, lgb_path, name):\n    \n    model = lgb.LGBMClassifier(max_depth=-1,\n                               n_estimators=999999,\n                               learning_rate=0.02,\n                               colsample_bytree=0.3,\n                               num_leaves=2,\n                               metric='auc',\n                               objective='binary', \n                               n_jobs=-1)\n     \n    model.fit(X_fit, y_fit, \n              eval_set=[(X_val, y_val)],\n              verbose=0, \n              early_stopping_rounds=1000)\n                  \n    cv_val = model.predict_proba(X_val)[:,1]\n    \n    #Save LightGBM Model\n    save_to = '{}{}_fold{}.txt'.format(lgb_path, name, counter+1)\n    model.booster_.save_model(save_to)\n    \n    return cv_val\n","f30c5bb7":"def fit_xgb(X_fit, y_fit, X_val, y_val, counter, xgb_path, name):\n    \n    model = xgb.XGBClassifier(max_depth=2,\n                              n_estimators=999999,\n                              colsample_bytree=0.3,\n                              learning_rate=0.02,\n                              objective='binary:logistic', \n                              n_jobs=-1)\n     \n    model.fit(X_fit, y_fit, \n              eval_set=[(X_val, y_val)], \n              verbose=0, \n              early_stopping_rounds=1000)\n              \n    cv_val = model.predict_proba(X_val)[:,1]\n    \n    #Save XGBoost Model\n    save_to = '{}{}_fold{}.dat'.format(xgb_path, name, counter+1)\n    pickle.dump(model, open(save_to, \"wb\"))\n    \n    return cv_val","08ef6791":"def fit_cb(X_fit, y_fit, X_val, y_val, counter, cb_path, name):\n    \n    model = cb.CatBoostClassifier(iterations=999999,\n                                  max_depth=2,\n                                  learning_rate=0.02,\n                                  colsample_bylevel=0.03,\n                                  objective=\"Logloss\")\n                                  \n    model.fit(X_fit, y_fit, \n              eval_set=[(X_val, y_val)], \n              verbose=0, early_stopping_rounds=1000)\n              \n    cv_val = model.predict_proba(X_val)[:,1]\n    \n    #Save Catboost Model          \n    save_to = \"{}{}_fold{}.mlmodel\".format(cb_path, name, counter+1)\n    model.save_model(save_to, format=\"coreml\", \n                     export_parameters={'prediction_type': 'probability'})\n                     \n    return cv_val","50602a4b":"def train_stage(df, lgb_path, xgb_path, cb_path):\n    \n    print('Load Train Data.')\n    df = train_df\n    print('\\nShape of Train Data: {}'.format(df.shape))\n    \n    y_df = y                        \n    df_ids = np.array(df.index)                     \n    #df.drop(['ID_code', 'target'], axis=1, inplace=True)\n    \n    lgb_cv_result = np.zeros(df.shape[0])\n    xgb_cv_result = np.zeros(df.shape[0])\n    cb_cv_result  = np.zeros(df.shape[0])\n    \n    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n    skf.get_n_splits(df_ids, y_df)\n    \n    print('\\nModel Fitting...')\n    for counter, ids in enumerate(skf.split(df_ids, y_df)):\n        print('\\nFold {}'.format(counter+1))\n        X_fit, y_fit = df.values[ids[0]], y_df[ids[0]]\n        X_val, y_val = df.values[ids[1]], y_df[ids[1]]\n    \n        print('LigthGBM')\n        lgb_cv_result[ids[1]] += fit_lgb(X_fit, y_fit, X_val, y_val, counter, lgb_path, name='lgb')\n        print('XGBoost')\n        xgb_cv_result[ids[1]] += fit_xgb(X_fit, y_fit, X_val, y_val, counter, xgb_path, name='xgb')\n        print('CatBoost')\n        cb_cv_result[ids[1]]  += fit_cb(X_fit,  y_fit, X_val, y_val, counter, cb_path,  name='cb')\n        \n        del X_fit, X_val, y_fit, y_val\n        gc.collect()\n    \n    auc_lgb  = round(roc_auc_score(y_df, lgb_cv_result),4)\n    auc_xgb  = round(roc_auc_score(y_df, xgb_cv_result),4)\n    auc_cb   = round(roc_auc_score(y_df, cb_cv_result), 4)\n    auc_mean = round(roc_auc_score(y_df, (lgb_cv_result+xgb_cv_result+cb_cv_result)\/3), 4)\n    auc_mean_lgb_cb = round(roc_auc_score(y_df, (lgb_cv_result+cb_cv_result)\/2), 4)\n    print('\\nLightGBM VAL AUC: {}'.format(auc_lgb))\n    print('XGBoost  VAL AUC: {}'.format(auc_xgb))\n    print('Catboost VAL AUC: {}'.format(auc_cb))\n    print('Mean Catboost+LightGBM VAL AUC: {}'.format(auc_mean_lgb_cb))\n    print('Mean XGBoost+Catboost+LightGBM, VAL AUC: {}\\n'.format(auc_mean))","aaa134de":"def prediction_stage(df_path, lgb_path, xgb_path, cb_path):\n    \n    print('Load Test Data.')\n    df = df_path\n    print('\\nShape of Test Data: {}'.format(df.shape))\n    \n    #df.drop(['ID_code'], axis=1, inplace=True)\n    \n    lgb_models = sorted(os.listdir(lgb_path))\n    xgb_models = sorted(os.listdir(xgb_path))\n    cb_models  = sorted(os.listdir(cb_path))\n    \n    lgb_result = np.zeros(df.shape[0])\n    xgb_result = np.zeros(df.shape[0])\n    cb_result  = np.zeros(df.shape[0])\n    \n    print('\\nMake predictions...\\n')\n    \n    print('With LightGBM...')\n    for m_name in lgb_models:\n        #Load LightGBM Model\n        model = lgb.Booster(model_file='{}{}'.format(lgb_path, m_name))\n        lgb_result += model.predict(df.values)\n     \n    print('With XGBoost...')    \n    for m_name in xgb_models:\n        #Load Xgboost Model\n        model = pickle.load(open('{}{}'.format(xgb_path, m_name), \"rb\"))\n        xgb_result += model.predict(df.values)\n    \n    print('With CatBoost...')        \n    for m_name in cb_models:\n        #Load Catboost Model\n        model = cb.CatBoostClassifier()\n        model = model.load_model('{}{}'.format(cb_path, m_name), format = 'coreml')\n        cb_result += model.predict(df.values, prediction_type='Probability')[:,1]\n    \n    lgb_result \/= len(lgb_models)\n    xgb_result \/= len(xgb_models)\n    cb_result  \/= len(cb_models)\n    \n    submission = pd.read_csv('..\/input\/sample_submission.csv')\n    submission['target'] = (lgb_result+xgb_result+cb_result)\/3\n    submission.to_csv('xgb_lgb_cb_starter_submission.csv', index=False)\n    submission['target'] = (lgb_result+cb_result)\/2\n    submission.to_csv('lgb_cb_starter_submission.csv', index=False)\n    submission['target'] = xgb_result\n    submission.to_csv('xgb_starter_submission.csv', index=False)\n    submission['target'] = lgb_result\n    submission.to_csv('lgb_starter_submission.csv', index=False)\n    submission['target'] = cb_result\n    submission.to_csv('cb_starter_submission.csv', index=False)","59690d96":"train_path = train_df\ntest_path  = test_df\n    \nlgb_path = '.\/lgb_models_stack\/'\nxgb_path = '.\/xgb_models_stack\/'\ncb_path  = '.\/cb_models_stack\/'\n\n    #Create dir for models\nos.mkdir(lgb_path)\nos.mkdir(xgb_path)\nos.mkdir(cb_path)\n\nprint('Train Stage.\\n')\ntrain_stage(train_path, lgb_path, xgb_path, cb_path)\n    \nprint('Prediction Stage.\\n')\nprediction_stage(test_path, lgb_path, xgb_path, cb_path)","57133ffe":"<a id='3'>3. Reduce the size of the Dataset<\/a>","5ab943fe":"<a id='4'>4. Check the Distribution of the target variable<\/a>","070b6c85":"**Contents**\n- <a href='#1'>1. Import the nesscescary libraries<\/a>\n\n- <a href='#2'>2. Read the dataset<\/a>\n\n- <a href='#3'>3. Reduce the size of the Dataset<\/a>\n\n- <a href='#4'>4. Check the Distribution of the target variable<\/a>\n   \n- <a href='#5'>5. Using RFECV for feature selection<\/a>\n\n- <a href='#6'>6. Visualization of the distribution of the selected featured with respect to the target variable<\/a>\n\n- <a href='#7'>7. Training the model with LGBM,XGBoost,CatBoost and making perdictions<\/a>\n","a3b7d14f":"<a id='5'>5. Using RFECV for feature selection<\/a>","d5220566":"Here **60** features were selected. We get the columns selected by RFECV by the get_support attribute.","e6858f33":"The below list are the columns selected.","483de5b2":"<a id='2'>2. Read dataset<\/a>","19264618":"We see the dataset is highly imbalanced. We see most of the customers have NOT made a transaction. Thus during the training, we will use StratifiedKFold.This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.","8c146507":"<a id='1'>1. Import the nesscescary libraries<\/a>","57a60aed":"Here I will try to reduce the features by using the RFECV library and then use a blend of LGBM, XGBoost and CATBoostClassifier to finally fromulate the result.","c4a16791":"The following code reduces the size of the dataset by 50%.","48e7e62a":"** Finally we run our model and submit the predictions**","5829ac1c":"<a id='7'>7. Training the model with LGBM,XGBoost,CatBoost and making perdictions<\/a>","2f813687":"I have commented out the code for kernel runtime constrains. Basically RFECV does feature ranking with recursive feature elimination and cross-validated selection of the best number of features. Let me explain some parameters used in the library:\n<ul>\n <li>**estimator** : A supervised learning estimator with a fit method that provides information about feature importance either through a coef_ attribute or through a feature_importances_ attribute. Here I have used RandomForrestClassifier.<\/li>\n\n <li>**step** : step corresponds to the (integer) number of features to remove at each iteration. Here I have used step = 10 so 10 features will be removed after each iteration.<\/li>\n\n <li>**min_features_to_select** : It is the minimum number of features to be selected. This number of features will always be scored, even if the difference between the original feature count and min_features_to_select isn\u2019t divisible by step.<\/li>\n<\/ul>","bd848036":"# Hi! Welcome to my Kernel.","6e25f700":"<a id='6'>6. Visualization of the distribution of the selected featured with respect to the target variable<\/a>","64ac5dc3":"Here we see the distribution of the variables we selected. "}}