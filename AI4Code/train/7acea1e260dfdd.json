{"cell_type":{"9b56a7a0":"code","066e34b1":"code","de8f4305":"code","90356ae5":"code","4e0a5981":"code","35532e55":"code","e6c85d6e":"code","8002f912":"code","54d73dd1":"code","324105bc":"code","db99a5b4":"code","18fd42c3":"code","c4263465":"code","bc388a71":"code","03b1b353":"code","809a0e4a":"code","086c85a5":"code","4ef7f65d":"code","4a45fe2b":"code","64d13585":"code","4a4a61e4":"code","2d6ad04b":"code","0aeb3034":"code","a602212b":"code","a90c1f56":"code","e66dfb76":"code","4458a048":"code","af9a2c84":"code","0fe21b3f":"code","5382c838":"code","64ed1ace":"code","504c5eb3":"code","52103d81":"code","6cc97ae8":"markdown","78eef888":"markdown","36648744":"markdown","afdee18e":"markdown","e218c5fb":"markdown","af2443d8":"markdown","f06badcf":"markdown","2fa51d1b":"markdown","2210dd23":"markdown","a1c18422":"markdown","9fc94c4e":"markdown","79ef7ba1":"markdown","cc859d72":"markdown","bd34ac15":"markdown","c408a06c":"markdown","49897c32":"markdown","f78bf628":"markdown","581b8788":"markdown","5ffc8aad":"markdown","e9679b38":"markdown","dc151dda":"markdown","c6d6ad05":"markdown","b4efec6b":"markdown","c64b78a7":"markdown","04d85088":"markdown","7c5d8671":"markdown","4c5c1ab4":"markdown","e97efb72":"markdown","e7f1d0e4":"markdown","1959b401":"markdown","2d102913":"markdown","ed997e70":"markdown","786afcfb":"markdown","509099ad":"markdown","86f8ff56":"markdown","ddbb77f3":"markdown","c7b59ea6":"markdown","83833f71":"markdown","856f9520":"markdown","f953f2a5":"markdown","ce63863e":"markdown","abfcc064":"markdown","6401592d":"markdown","5546a7db":"markdown","8c20c532":"markdown","1d0d845c":"markdown","c2490652":"markdown","0e239fff":"markdown","8cb5f4ba":"markdown"},"source":{"9b56a7a0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport networkx as nx\nfrom sklearn.metrics import roc_auc_score, accuracy_score, log_loss, confusion_matrix\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import rankdata, distributions\nfrom sklearn.model_selection import StratifiedKFold\n\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 20)\nnp.random.seed(1729)\n\nTRAIN = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/train.csv')\nTEST = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/test.csv')\nDATA = TRAIN.append(TEST, ignore_index=True, sort=False)\nfeatures = [f for f in TRAIN.columns if f not in ['PassengerId','Survived']]\ncat_features = []\ndel(TRAIN, TEST)\ngc.collect()","066e34b1":"DATA['NanCount'] = DATA[features].isnull().sum(axis=1)\ncat_features += ['NanCount','Sex']\n\n# -------------------------------------------\nfig = plt.figure(figsize=(16, 8), facecolor='white')\ngs = fig.add_gridspec(1, 2)\n\n# -------------------------------------------\nX = 'NanCount'\ntrn = DATA[DATA['Survived'].notnull()].groupby([X])['Survived'].agg(['mean','size'])\ntrn.columns = ['SurvivalRate','TrainRows']\ntrn.reset_index(drop=False, inplace=True)\ntst = DATA[DATA['Survived'].isnull()].groupby([X])['Survived'].agg(['mean','size'])\ntst.columns = ['SurvivalRate','TestRows']\ntst.reset_index(drop=False, inplace=True)\ndata = trn.append(tst, ignore_index=True)\ndata = data.groupby(X, as_index=False).sum()\nax0 = fig.add_subplot(gs[0, 0])\nx = np.arange(len(data[X]))  # the label locations\nwidth = 0.35  # the width of the bars\ntrbars = ax0.bar(x - width\/2, data['TrainRows'], width, label='Train Data', color='navy', edgecolor='grey')\ntebars = ax0.bar(x + width\/2, data['TestRows'], width, label='Test Data', color='lightgrey',edgecolor='black')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax0.set_ylabel('Rows', fontsize=12, weight='bold')\nax0.set_title(X+' & Survival Rates', fontsize=14, weight='bold')\nax0.set_xticks(x)\nax0.set_xticklabels(data[X], fontsize=12, weight='bold')\nax0.legend(fontsize=12)\nax0.bar_label(trbars, labels=np.round(data['SurvivalRate'],2), padding=3)\n\n# -------------------------------------------\nX = 'Sex'\ntrn = DATA[DATA['Survived'].notnull()].groupby([X])['Survived'].agg(['mean','size'])\ntrn.columns = ['SurvivalRate','TrainRows']\ntrn.reset_index(drop=False, inplace=True)\ntst = DATA[DATA['Survived'].isnull()].groupby([X])['Survived'].agg(['mean','size'])\ntst.columns = ['SurvivalRate','TestRows']\ntst.reset_index(drop=False, inplace=True)\ndata = trn.append(tst, ignore_index=True)\ndata = data.groupby(X, as_index=False).sum()\nax1 = fig.add_subplot(gs[0, 1])\nx = np.arange(len(data[X]))  # the label locations\nwidth = 0.35  # the width of the bars\ntrbars = ax1.bar(x - width\/2, data['TrainRows'], width, label='Train Data', color='navy', edgecolor='grey')\ntebars = ax1.bar(x + width\/2, data['TestRows'], width, label='Test Data', color='lightgrey',edgecolor='black')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax1.set_ylabel('Rows', fontsize=12, weight='bold')\nax1.set_title(X+' & Survival Rates', fontsize=14, weight='bold')\nax1.set_xticks(x)\nax1.set_xticklabels(data[X], fontsize=12, weight='bold')\nax1.legend(fontsize=12)\nax1.bar_label(trbars, labels=np.round(data['SurvivalRate'],2), padding=3)\nfig.tight_layout()\nplt.show()","de8f4305":"def age_group(A):\n    if A<=3:\n        return 'AgeGrp0'\n    elif (A>=4)&(A<=10):\n        return 'AgeGrp1'\n    elif (A>=11)&(A<=17):\n        return 'AgeGrp2'\n    elif (A>=18)&(A<=39):\n        return 'AgeGrp3'\n    elif (A>=40)&(A<=70):\n        return 'AgeGrp4'\n    else:\n        return 'AgeGrp5'\n\nDATA['Age'].fillna((DATA['Age'].mean()), inplace=True)\nDATA.loc[DATA['Age']<1, 'Age'] = 0\nDATA['AgeGroup'] = DATA['Age'].apply(age_group)\ncat_features += ['AgeGroup']\n\n# -------------------------------------------\nfig = plt.figure(figsize=(16, 8), facecolor='white')\ngs = fig.add_gridspec(1, 2)\n# -------------------------------------------\ndata = DATA.groupby(['Age'], as_index=False)['Survived'].mean()\n\nax0 = fig.add_subplot(gs[0, 0])\nax0.scatter(data['Age'],data['Survived'], s=25, color='navy')\nax0.set_xlabel('Age', fontsize=10, weight='bold')\nax0.set_ylabel('Survival Rate', fontsize=10, weight='bold')\nax0.set_title('Survival Rate by Age', fontsize=12, weight='bold')\n\n# -------------------------------------------\nX = 'AgeGroup'\ntrn = DATA[DATA['Survived'].notnull()].groupby([X])['Survived'].agg(['mean','size'])\ntrn.columns = ['SurvivalRate','TrainRows']\ntrn.reset_index(drop=False, inplace=True)\ntst = DATA[DATA['Survived'].isnull()].groupby([X])['Survived'].agg(['mean','size'])\ntst.columns = ['SurvivalRate','TestRows']\ntst.reset_index(drop=False, inplace=True)\ndata = trn.append(tst, ignore_index=True)\ndata = data.groupby(X, as_index=False).sum()\nax1 = fig.add_subplot(gs[0, 1])\nx = np.arange(len(data[X]))  # the label locations\nwidth = 0.35  # the width of the bars\ntrbars = ax1.bar(x - width\/2, data['TrainRows'], width, label='Train Data', color='navy', edgecolor='grey')\ntebars = ax1.bar(x + width\/2, data['TestRows'], width, label='Test Data', color='lightgrey',edgecolor='black')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax1.set_ylabel('Rows', fontsize=10, weight='bold')\nax1.set_title(X+' & Survival Rates', fontsize=12, weight='bold')\nax1.set_xticks(x)\nax1.set_xticklabels(data[X], fontsize=10, weight='bold')\nax1.legend(fontsize=12)\nax1.bar_label(trbars, labels=np.round(data['SurvivalRate'],2), padding=3)\n\nfig.tight_layout()\nplt.show()","90356ae5":"DATA['SibSp'] = np.clip(DATA['SibSp'],0,2)\nDATA['Parch'] = np.clip(DATA['Parch'],0,2)\nDATA['FamilySize'] = DATA['SibSp']+DATA['Parch']+1\ncat_features += ['FamilySize','SibSp','Parch']\n\n# -------------------------------------------\nfig = plt.figure(figsize=(16, 16), facecolor='white')\ngs = fig.add_gridspec(2, 2)\n\n# -------------------------------------------\ndata = DATA.groupby(['SibSp','Parch'], as_index=False)['Survived'].mean()\ncm = plt.cm.get_cmap('RdYlBu')\nax0 = fig.add_subplot(gs[0, 0])\nsc0 = ax0.scatter(data['SibSp'],data['Parch'], s=50, c=data['Survived'], cmap=cm)\nax0.set_xlabel('SibSp', fontsize=10, weight='bold')\nax0.set_ylabel('Parch', fontsize=10, weight='bold')\nax0.set_title('Survival Rate by Family', fontsize=12, weight='bold')\nplt.colorbar(sc0) #, cax=ax0)\n\n# -------------------------------------------\nX = 'FamilySize'\ntrn = DATA[DATA['Survived'].notnull()].groupby([X])['Survived'].agg(['mean','size'])\ntrn.columns = ['SurvivalRate','TrainRows']\ntrn.reset_index(drop=False, inplace=True)\ntst = DATA[DATA['Survived'].isnull()].groupby([X])['Survived'].agg(['mean','size'])\ntst.columns = ['SurvivalRate','TestRows']\ntst.reset_index(drop=False, inplace=True)\ndata = trn.append(tst, ignore_index=True)\ndata = data.groupby(X, as_index=False).sum()\nax1 = fig.add_subplot(gs[0, 1])\nx = np.arange(len(data[X]))  # the label locations\nwidth = 0.35  # the width of the bars\ntrbars = ax1.bar(x - width\/2, data['TrainRows'], width, label='Train Data', color='navy', edgecolor='grey')\ntebars = ax1.bar(x + width\/2, data['TestRows'], width, label='Test Data', color='lightgrey',edgecolor='black')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax1.set_ylabel('Rows', fontsize=10, weight='bold')\nax1.set_title(X+' & Survival Rates', fontsize=12, weight='bold')\nax1.set_xticks(x)\nax1.set_xticklabels(data[X], fontsize=10, weight='bold')\nax1.legend(fontsize=12)\nax1.bar_label(trbars, labels=np.round(data['SurvivalRate'],2), padding=3)\n\n# -------------------------------------------\nX = 'SibSp'\ntrn = DATA[DATA['Survived'].notnull()].groupby([X])['Survived'].agg(['mean','size'])\ntrn.columns = ['SurvivalRate','TrainRows']\ntrn.reset_index(drop=False, inplace=True)\ntst = DATA[DATA['Survived'].isnull()].groupby([X])['Survived'].agg(['mean','size'])\ntst.columns = ['SurvivalRate','TestRows']\ntst.reset_index(drop=False, inplace=True)\ndata = trn.append(tst, ignore_index=True)\ndata = data.groupby(X, as_index=False).sum()\nax2 = fig.add_subplot(gs[1, 0])\nx = np.arange(len(data[X]))  # the label locations\nwidth = 0.35  # the width of the bars\ntrbars = ax2.bar(x - width\/2, data['TrainRows'], width, label='Train Data', color='navy', edgecolor='grey')\ntebars = ax2.bar(x + width\/2, data['TestRows'], width, label='Test Data', color='lightgrey',edgecolor='black')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax2.set_ylabel('Rows', fontsize=10, weight='bold')\nax2.set_title(X+' & Survival Rates', fontsize=12, weight='bold')\nax2.set_xticks(x)\nax2.set_xticklabels(data[X], fontsize=10, weight='bold')\nax2.legend(fontsize=12)\nax2.bar_label(trbars, labels=np.round(data['SurvivalRate'],2), padding=3)\n\n# -------------------------------------------\nX = 'Parch'\ntrn = DATA[DATA['Survived'].notnull()].groupby([X])['Survived'].agg(['mean','size'])\ntrn.columns = ['SurvivalRate','TrainRows']\ntrn.reset_index(drop=False, inplace=True)\ntst = DATA[DATA['Survived'].isnull()].groupby([X])['Survived'].agg(['mean','size'])\ntst.columns = ['SurvivalRate','TestRows']\ntst.reset_index(drop=False, inplace=True)\ndata = trn.append(tst, ignore_index=True)\ndata = data.groupby(X, as_index=False).sum()\nax3 = fig.add_subplot(gs[1, 1])\nx = np.arange(len(data[X]))  # the label locations\nwidth = 0.35  # the width of the bars\ntrbars = ax3.bar(x - width\/2, data['TrainRows'], width, label='Train Data', color='navy', edgecolor='grey')\ntebars = ax3.bar(x + width\/2, data['TestRows'], width, label='Test Data', color='lightgrey',edgecolor='black')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax3.set_ylabel('Rows', fontsize=10, weight='bold')\nax3.set_title(X+' & Survival Rates', fontsize=12, weight='bold')\nax3.set_xticks(x)\nax3.set_xticklabels(data[X], fontsize=10, weight='bold')\nax3.legend(fontsize=12)\nax3.bar_label(trbars, labels=np.round(data['SurvivalRate'],2), padding=3)\n\nfig.tight_layout()\nplt.show()","4e0a5981":"DATA['Embarked'].fillna('S', inplace=True)\ncat_features += ['Embarked','Pclass']\n\n# -------------------------------------------\nfig = plt.figure(figsize=(16, 8), facecolor='white')\ngs = fig.add_gridspec(1, 2)\n\n# -------------------------------------------\nX = 'Pclass'\ntrn = DATA[DATA['Survived'].notnull()].groupby([X])['Survived'].agg(['mean','size'])\ntrn.columns = ['SurvivalRate','TrainRows']\ntrn.reset_index(drop=False, inplace=True)\ntst = DATA[DATA['Survived'].isnull()].groupby([X])['Survived'].agg(['mean','size'])\ntst.columns = ['SurvivalRate','TestRows']\ntst.reset_index(drop=False, inplace=True)\ndata = trn.append(tst, ignore_index=True)\ndata = data.groupby(X, as_index=False).sum()\nax1 = fig.add_subplot(gs[0, 0])\nx = np.arange(len(data[X]))  # the label locations\nwidth = 0.35  # the width of the bars\ntrbars = ax1.bar(x - width\/2, data['TrainRows'], width, label='Train Data', color='navy', edgecolor='grey')\ntebars = ax1.bar(x + width\/2, data['TestRows'], width, label='Test Data', color='lightgrey',edgecolor='black')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax1.set_ylabel('Rows', fontsize=10, weight='bold')\nax1.set_title(X+' & Survival Rates', fontsize=12, weight='bold')\nax1.set_xticks(x)\nax1.set_xticklabels(data[X], fontsize=10, weight='bold')\nax1.legend(fontsize=12)\nax1.bar_label(trbars, labels=np.round(data['SurvivalRate'],2), padding=3)\n\n# -------------------------------------------\nX = 'Embarked'\ntrn = DATA[DATA['Survived'].notnull()].groupby([X])['Survived'].agg(['mean','size'])\ntrn.columns = ['SurvivalRate','TrainRows']\ntrn.reset_index(drop=False, inplace=True)\ntst = DATA[DATA['Survived'].isnull()].groupby([X])['Survived'].agg(['mean','size'])\ntst.columns = ['SurvivalRate','TestRows']\ntst.reset_index(drop=False, inplace=True)\ndata = trn.append(tst, ignore_index=True)\ndata = data.groupby(X, as_index=False).sum()\nax2 = fig.add_subplot(gs[0, 1])\nx = np.arange(len(data[X]))  # the label locations\nwidth = 0.35  # the width of the bars\ntrbars = ax2.bar(x - width\/2, data['TrainRows'], width, label='Train Data', color='navy', edgecolor='grey')\ntebars = ax2.bar(x + width\/2, data['TestRows'], width, label='Test Data', color='lightgrey',edgecolor='black')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax2.set_ylabel('Rows', fontsize=10, weight='bold')\nax2.set_title(X+' & Survival Rates', fontsize=12, weight='bold')\nax2.set_xticks(x)\nax2.set_xticklabels(data[X], fontsize=10, weight='bold')\nax2.legend(fontsize=12)\nax2.bar_label(trbars, labels=np.round(data['SurvivalRate'],2), padding=3)\n\nfig.tight_layout()\nplt.show()","35532e55":"DATA['Deck'] = DATA['Cabin'].apply(lambda x: 'Z' if pd.isnull(x) else x[0])\nDATA['CabinNumber'] = DATA['Cabin'].apply(lambda x: np.nan if pd.isnull(x) else int(x[1:]))\nDATA['CabinSection'] = DATA['Cabin'].apply(lambda x: np.nan if pd.isnull(x) else int(x[1:-3]))\nDATA['CabinRoom'] = DATA['Cabin'].apply(lambda x: np.nan if pd.isnull(x) else int(x[-3:]))\nDATA['HasCabin'] = DATA['Deck'].apply(lambda x: 'Cabin' if x!='Z' else 'NoCabin')\ncat_features += ['Deck','CabinSection','CabinRoom','HasCabin']\n\n# -------------------------------------------\nfig = plt.figure(figsize=(16, 16), facecolor='white')\ngs = fig.add_gridspec(2, 2)\n\n# -------------------------------------------\nX = 'HasCabin'\ntrn = DATA[DATA['Survived'].notnull()].groupby([X])['Survived'].agg(['mean','size'])\ntrn.columns = ['SurvivalRate','TrainRows']\ntrn.reset_index(drop=False, inplace=True)\ntst = DATA[DATA['Survived'].isnull()].groupby([X])['Survived'].agg(['mean','size'])\ntst.columns = ['SurvivalRate','TestRows']\ntst.reset_index(drop=False, inplace=True)\ndata = trn.append(tst, ignore_index=True)\ndata = data.groupby(X, as_index=False).sum()\nax0 = fig.add_subplot(gs[0, 0])\nx = np.arange(len(data[X]))  # the label locations\nwidth = 0.35  # the width of the bars\ntrbars = ax0.bar(x - width\/2, data['TrainRows'], width, label='Train Data', color='navy', edgecolor='grey')\ntebars = ax0.bar(x + width\/2, data['TestRows'], width, label='Test Data', color='lightgrey',edgecolor='black')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax0.set_ylabel('Rows', fontsize=10, weight='bold')\nax0.set_title(X+' & Survival Rates', fontsize=12, weight='bold')\nax0.set_xticks(x)\nax0.set_xticklabels(data[X], fontsize=10, weight='bold')\nax0.legend(fontsize=12)\nax0.bar_label(trbars, labels=np.round(data['SurvivalRate'],2), padding=3)\n\n# -------------------------------------------\nX = 'Deck'\ntrn = DATA[DATA['Survived'].notnull()].groupby([X])['Survived'].agg(['mean','size'])\ntrn.columns = ['SurvivalRate','TrainRows']\ntrn.reset_index(drop=False, inplace=True)\ntst = DATA[DATA['Survived'].isnull()].groupby([X])['Survived'].agg(['mean','size'])\ntst.columns = ['SurvivalRate','TestRows']\ntst.reset_index(drop=False, inplace=True)\ndata = trn.append(tst, ignore_index=True)\ndata = data.groupby(X, as_index=False).sum()\nax1 = fig.add_subplot(gs[0, 1])\nx = np.arange(len(data[X]))  # the label locations\nwidth = 0.35  # the width of the bars\ntrbars = ax1.bar(x - width\/2, data['TrainRows'], width, label='Train Data', color='navy', edgecolor='grey')\ntebars = ax1.bar(x + width\/2, data['TestRows'], width, label='Test Data', color='lightgrey',edgecolor='black')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax1.set_ylabel('Rows', fontsize=10, weight='bold')\nax1.set_title(X+' & Survival Rates', fontsize=12, weight='bold')\nax1.set_xticks(x)\nax1.set_xticklabels(data[X], fontsize=10, weight='bold')\nax1.legend(fontsize=12)\nax1.bar_label(trbars, labels=np.round(data['SurvivalRate'],2), padding=3)\n\n# -------------------------------------------\nX = 'CabinSection'\ntrn = DATA[DATA['Survived'].notnull()].groupby([X])['Survived'].agg(['mean','size'])\ntrn.columns = ['SurvivalRate','TrainRows']\ntrn.reset_index(drop=False, inplace=True)\ntst = DATA[DATA['Survived'].isnull()].groupby([X])['Survived'].agg(['mean','size'])\ntst.columns = ['SurvivalRate','TestRows']\ntst.reset_index(drop=False, inplace=True)\ndata = trn.append(tst, ignore_index=True)\ndata = data.groupby(X, as_index=False).sum()\nax2 = fig.add_subplot(gs[1, 0])\nx = np.arange(len(data[X]))  # the label locations\nwidth = 0.35  # the width of the bars\ntrbars = ax2.bar(x - width\/2, data['TrainRows'], width, label='Train Data', color='navy', edgecolor='grey')\ntebars = ax2.bar(x + width\/2, data['TestRows'], width, label='Test Data', color='lightgrey',edgecolor='black')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax2.set_ylabel('Rows', fontsize=10, weight='bold')\nax2.set_title(X+' & Survival Rates', fontsize=12, weight='bold')\nax2.set_xticks(x)\nax2.set_xticklabels(data[X], fontsize=10, weight='bold')\nax2.legend(fontsize=12)\nax2.bar_label(trbars, labels=np.round(data['SurvivalRate'],2), padding=3)\n\n# -------------------------------------------\nX = 'CabinRoom'\ndata = DATA.groupby([X], as_index=False)['Survived'].mean()\nax3 = fig.add_subplot(gs[1, 1])\nax3.scatter(data[X],data['Survived'], s=25, color='navy')\nax3.set_xlabel(X, fontsize=10, weight='bold')\nax3.set_ylabel('Survival Rate', fontsize=10, weight='bold')\nax3.set_title('Survival Rate by Cabin', fontsize=12, weight='bold')\n\nfig.tight_layout()\nplt.show()","e6c85d6e":"fig = plt.figure(figsize=(16, 16), facecolor='white')\ngs = fig.add_gridspec(2, 2)\nax0 = fig.add_subplot(gs[0, 0])\nax0.hist(DATA['Fare'], bins=100, color='navy')\nax0.set_xlabel('Fare', fontsize=10, weight='bold')\nax0.set_ylabel('Rows', fontsize=10, weight='bold')\nax1 = fig.add_subplot(gs[0, 1])\nsns.violinplot(data=DATA, x='HasCabin', y='Fare', hue='Survived', split=True, inner=\"quart\", linewidth=1, palette={0: 'grey', 1: 'navy'})\nax2 = fig.add_subplot(gs[1, 0])\nsns.violinplot(data=DATA, x='Embarked', y='Fare', hue='Survived', split=True, inner=\"quart\", linewidth=1, palette={0: 'grey', 1: 'navy'})\nax3 = fig.add_subplot(gs[1, 1])\nsns.violinplot(data=DATA, x='Pclass', y='Fare', hue='Survived', split=True, inner=\"quart\", linewidth=1, palette={0: 'grey', 1: 'navy'})\nfig.tight_layout()\nplt.show()\n","8002f912":"DATA['LNFare'] = DATA['Fare'].map(lambda x: np.log(x) if x > 0 else 0)\nfig = plt.figure(figsize=(16, 16), facecolor='white')\ngs = fig.add_gridspec(2, 2)\nax0 = fig.add_subplot(gs[0, 0])\nax0.hist(DATA['LNFare'], bins=100, color='navy')\nax0.set_xlabel('Log(Fare)', fontsize=10, weight='bold')\nax0.set_ylabel('Rows', fontsize=10, weight='bold')\nax1 = fig.add_subplot(gs[0, 1])\nsns.violinplot(data=DATA, x='HasCabin', y='LNFare', hue='Survived', split=True, inner=\"quart\", linewidth=1, palette={0: 'grey', 1: 'navy'})\nax2 = fig.add_subplot(gs[1, 0])\nsns.violinplot(data=DATA, x='Embarked', y='LNFare', hue='Survived', split=True, inner=\"quart\", linewidth=1, palette={0: 'grey', 1: 'navy'})\nax3 = fig.add_subplot(gs[1, 1])\nsns.violinplot(data=DATA, x='Pclass', y='LNFare', hue='Survived', split=True, inner=\"quart\", linewidth=1, palette={0: 'grey', 1: 'navy'})\nfig.tight_layout()\nplt.show()\n","54d73dd1":"faremeans = DATA.groupby(['Embarked','Pclass','HasCabin'], as_index=False)['Fare'].mean()\nfarecats = list(map(lambda x,y,z: (x,y,z), faremeans['Embarked'],faremeans['Pclass'],faremeans['HasCabin']))\nfaredict = dict(zip(farecats, faremeans['Fare'].tolist()))\nDATA['Fare'] = list(map(lambda w,x,y,z: z if pd.notnull(z) else faredict[(w,x,y)], DATA['Embarked'],DATA['Pclass'],DATA['HasCabin'],DATA['Fare']))\ndel(faremeans, farecats)\nDATA['LNFare'] = DATA['Fare'].map(lambda x: np.log(x) if x > 0 else 0)\n\nmodel = KMeans(n_clusters=4)\nclusters = model.fit_predict(np.array(DATA['LNFare']).reshape(-1,1))\ncluster_dict = dict(zip([0,1,2,3],rankdata([x[0] for x in model.cluster_centers_])))\nDATA['FareCluster'] = list(map(lambda x: cluster_dict[x], clusters))\ncat_features += ['FareCluster']\n\n# -------------------------------------------\n\nfig = plt.figure(figsize=(16, 8), facecolor='white')\ngs = fig.add_gridspec(1, 2)\n\nax0 = fig.add_subplot(gs[0, 0])\nbin_list = np.linspace(DATA['Fare'].min(),DATA['Fare'].max(),100)\nax0.hist(DATA.loc[DATA['FareCluster']==1,'Fare'], bins=bin_list, color='navy')\nax0.hist(DATA.loc[DATA['FareCluster']==2,'Fare'], bins=bin_list, color='blue')\nax0.hist(DATA.loc[DATA['FareCluster']==3,'Fare'], bins=bin_list, color='lightsteelblue')\nax0.hist(DATA.loc[DATA['FareCluster']==4,'Fare'], bins=bin_list, color='slategrey')\nax0.set_xlabel('Fare', fontsize=10, weight='bold')\nax0.set_ylabel('Rows', fontsize=10, weight='bold')\n\n# -------------------------------------------\nX = 'FareCluster'\ntrn = DATA[DATA['Survived'].notnull()].groupby([X])['Survived'].agg(['mean','size'])\ntrn.columns = ['SurvivalRate','TrainRows']\ntrn.reset_index(drop=False, inplace=True)\ntst = DATA[DATA['Survived'].isnull()].groupby([X])['Survived'].agg(['mean','size'])\ntst.columns = ['SurvivalRate','TestRows']\ntst.reset_index(drop=False, inplace=True)\ndata = trn.append(tst, ignore_index=True)\ndata = data.groupby(X, as_index=False).sum()\nax1 = fig.add_subplot(gs[0, 1])\nx = np.arange(len(data[X]))  # the label locations\nwidth = 0.35  # the width of the bars\ntrbars = ax1.bar(x - width\/2, data['TrainRows'], width, label='Train Data', color='navy', edgecolor='grey')\ntebars = ax1.bar(x + width\/2, data['TestRows'], width, label='Test Data', color='lightgrey',edgecolor='black')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax1.set_ylabel('Rows', fontsize=10, weight='bold')\nax1.set_title(X+' & Survival Rates', fontsize=12, weight='bold')\nax1.set_xticks(x)\nax1.set_xticklabels(data[X], fontsize=10, weight='bold')\nax1.legend(fontsize=12)\nax1.bar_label(trbars, labels=np.round(data['SurvivalRate'],2), padding=3)\n\nfig.tight_layout()\nplt.show()","324105bc":"def ticket_type(t):\n    T = str(t).upper().strip()\n    T = T.split()\n    T = [str(x).strip() for x in T]\n    if T[0]=='NAN':\n        return np.nan\n    elif T[0][0].isalpha():\n        T = T[0].replace('.','')\n        T = T.replace('SOTON','STON')\n        T = T.replace('A\/4','A4')\n        T = T.replace('A\/5','A5')\n        T = T.replace('A\/S','A5')\n        T = T.replace('CA5TON','CA\/STON')\n        return T\n    else:\n        return np.nan\n\ndef ticket_number(t):\n    T = str(t).upper().strip()\n    T = T.split()\n    T = [str(x).strip() for x in T]\n    if T[0].isnumeric():\n        return int(T[0])\n    elif (T[0][0].isalpha())&(len(T)==1):\n        return np.nan\n    else:\n        return int(T[1])\n\nDATA['TicketType'] = DATA['Ticket'].apply(ticket_type)\nDATA['TicketNumber'] = DATA['Ticket'].apply(ticket_number)\nDATA['TicketType'].fillna('NoType', inplace=True)\nDATA['TicketNumber'].fillna(-1, inplace=True)\nDATA['TicketCat'] = DATA['TicketType'].apply(lambda x: x[0])\nDATA['TicketLen'] = DATA['TicketNumber'].apply(lambda x: len(str(x)))\ncat_features += ['TicketType','TicketCat','TicketLen']\n\n# -------------------------------------------\n\nfig = plt.figure(figsize=(16, 16), facecolor='white')\ngs = fig.add_gridspec(2, 2)\n\n# -------------------------------------------\nX = 'TicketType'\ntrn = DATA[DATA['Survived'].notnull()].groupby([X])['Survived'].agg(['mean','size'])\ntrn.columns = ['SurvivalRate','TrainRows']\ntrn.reset_index(drop=False, inplace=True)\ntst = DATA[DATA['Survived'].isnull()].groupby([X])['Survived'].agg(['mean','size'])\ntst.columns = ['SurvivalRate','TestRows']\ntst.reset_index(drop=False, inplace=True)\ndata = trn.append(tst, ignore_index=True)\ndata = data.groupby(X, as_index=False).sum()\nax0 = fig.add_subplot(gs[0, 0])\nx = np.arange(len(data[X]))  # the label locations\nwidth = 0.35  # the width of the bars\ntrbars = ax0.barh(x - width\/2, data['TrainRows'], width, label='Train Data', color='navy', edgecolor='grey')\ntebars = ax0.barh(x + width\/2, data['TestRows'], width, label='Test Data', color='lightgrey',edgecolor='black')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax0.set_xlabel('Rows', fontsize=10, weight='bold')\nax0.set_title(X+' & Survival Rates', fontsize=12, weight='bold')\nax0.set_yticks(x)\nax0.set_yticklabels(data[X], fontsize=10, weight='bold')\nax0.legend(fontsize=12)\nax0.bar_label(trbars, labels=np.round(data['SurvivalRate'],2), padding=3)\n\n# -------------------------------------------\nX = 'TicketCat'\ntrn = DATA[DATA['Survived'].notnull()].groupby([X])['Survived'].agg(['mean','size'])\ntrn.columns = ['SurvivalRate','TrainRows']\ntrn.reset_index(drop=False, inplace=True)\ntst = DATA[DATA['Survived'].isnull()].groupby([X])['Survived'].agg(['mean','size'])\ntst.columns = ['SurvivalRate','TestRows']\ntst.reset_index(drop=False, inplace=True)\ndata = trn.append(tst, ignore_index=True)\ndata = data.groupby(X, as_index=False).sum()\nax0 = fig.add_subplot(gs[0, 1])\nx = np.arange(len(data[X]))  # the label locations\nwidth = 0.35  # the width of the bars\ntrbars = ax0.bar(x - width\/2, data['TrainRows'], width, label='Train Data', color='navy', edgecolor='grey')\ntebars = ax0.bar(x + width\/2, data['TestRows'], width, label='Test Data', color='lightgrey',edgecolor='black')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax0.set_ylabel('Rows', fontsize=10, weight='bold')\nax0.set_title(X+' & Survival Rates', fontsize=12, weight='bold')\nax0.set_xticks(x)\nax0.set_xticklabels(data[X], fontsize=10, weight='bold')\nax0.legend(fontsize=12)\nax0.bar_label(trbars, labels=np.round(data['SurvivalRate'],2), padding=3)\n\n# -------------------------------------------\nX = 'TicketLen'\ntrn = DATA[DATA['Survived'].notnull()].groupby([X])['Survived'].agg(['mean','size'])\ntrn.columns = ['SurvivalRate','TrainRows']\ntrn.reset_index(drop=False, inplace=True)\ntst = DATA[DATA['Survived'].isnull()].groupby([X])['Survived'].agg(['mean','size'])\ntst.columns = ['SurvivalRate','TestRows']\ntst.reset_index(drop=False, inplace=True)\ndata = trn.append(tst, ignore_index=True)\ndata = data.groupby(X, as_index=False).sum()\nax0 = fig.add_subplot(gs[1, 0])\nx = np.arange(len(data[X]))  # the label locations\nwidth = 0.35  # the width of the bars\ntrbars = ax0.bar(x - width\/2, data['TrainRows'], width, label='Train Data', color='navy', edgecolor='grey')\ntebars = ax0.bar(x + width\/2, data['TestRows'], width, label='Test Data', color='lightgrey',edgecolor='black')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax0.set_ylabel('Rows', fontsize=10, weight='bold')\nax0.set_title(X+' & Survival Rates', fontsize=12, weight='bold')\nax0.set_xticks(x)\nax0.set_xticklabels(data[X], fontsize=10, weight='bold')\nax0.legend(fontsize=12)\nax0.bar_label(trbars, labels=np.round(data['SurvivalRate'],2), padding=3)\n# -------------------------------------------\n\nfig.tight_layout()\nplt.show()\n\n\n","db99a5b4":"DATA['FirstName'] = DATA['Name'].apply(lambda x: x.split(',')[1].strip())\nDATA['LastName'] = DATA['Name'].apply(lambda x: x.split(',')[0].strip())\n\nfirsts = DATA.groupby(['FirstName']).size()\nDATA['FirstNameFreq'] = DATA['FirstName'].apply(lambda x: firsts[x])\nlasts = DATA.groupby(['LastName']).size()\nDATA['LastNameFreq'] = DATA['LastName'].apply(lambda x: lasts[x])\ncat_features += ['FirstNameFreq','LastNameFreq']\n\n# -------------------------------------------\n\nfig = plt.figure(figsize=(16, 8), facecolor='white')\ngs = fig.add_gridspec(1, 2)\n\n# -------------------------------------------\nX = 'FirstNameFreq'\ndata = DATA.groupby([X], as_index=False)['Survived'].mean()\nax0 = fig.add_subplot(gs[0, 0])\nax0.scatter(data[X],data['Survived'], s=25, color='navy')\nax0.set_xlabel(X, fontsize=10, weight='bold')\nax0.set_ylabel('Survival Rate', fontsize=10, weight='bold')\nax0.set_title('Survival Rate by FirstNameFreq', fontsize=12, weight='bold')\n\n# -------------------------------------------\nX = 'LastNameFreq'\ndata = DATA.groupby([X], as_index=False)['Survived'].mean()\nax1 = fig.add_subplot(gs[0, 1])\nax1.scatter(data[X],data['Survived'], s=25, color='navy')\nax1.set_xlabel(X, fontsize=10, weight='bold')\nax1.set_ylabel('Survival Rate', fontsize=10, weight='bold')\nax1.set_title('Survival Rate by LastNameFreq', fontsize=12, weight='bold')\n\n# -------------------------------------------\n\nfig.tight_layout()\nplt.show()\n\n","18fd42c3":"model = KMeans(n_clusters=2)\nclusters = model.fit_predict(np.array(DATA['FirstNameFreq']).reshape(-1,1))\ncluster_dict = dict(zip([0,1],rankdata([x[0] for x in model.cluster_centers_])))\nDATA['FirstNameCluster'] = list(map(lambda x: cluster_dict[x], clusters))\ncat_features += ['FirstNameCluster']\n\n# -------------------------------------------\n\nfig = plt.figure(figsize=(16, 8), facecolor='white')\ngs = fig.add_gridspec(1, 2)\n\n# -------------------------------------------\nX = 'FirstNameCluster'\ntrn = DATA[DATA['Survived'].notnull()].groupby([X])['Survived'].agg(['mean','size'])\ntrn.columns = ['SurvivalRate','TrainRows']\ntrn.reset_index(drop=False, inplace=True)\ntst = DATA[DATA['Survived'].isnull()].groupby([X])['Survived'].agg(['mean','size'])\ntst.columns = ['SurvivalRate','TestRows']\ntst.reset_index(drop=False, inplace=True)\ndata = trn.append(tst, ignore_index=True)\ndata = data.groupby(X, as_index=False).sum()\nax0 = fig.add_subplot(gs[0, 0])\nx = np.arange(len(data[X]))  # the label locations\nwidth = 0.35  # the width of the bars\ntrbars = ax0.bar(x - width\/2, data['TrainRows'], width, label='Train Data', color='navy', edgecolor='grey')\ntebars = ax0.bar(x + width\/2, data['TestRows'], width, label='Test Data', color='lightgrey',edgecolor='black')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax0.set_ylabel('Rows', fontsize=10, weight='bold')\nax0.set_title(X+' & Survival Rates', fontsize=12, weight='bold')\nax0.set_xticks(x)\nax0.set_xticklabels(data[X], fontsize=10, weight='bold')\nax0.legend(fontsize=12)\nax0.bar_label(trbars, labels=np.round(data['SurvivalRate'],2), padding=3)\n# -------------------------------------------\n\nfig.tight_layout()\nplt.show()","c4263465":"data = DATA[(DATA['CabinNumber'].notnull())&(DATA['TicketNumber']!=-1)].copy()\n\n# -------------------------------------------\n\nfig = plt.figure(figsize=(16, 16), facecolor='white')\ngs = fig.add_gridspec(2, 2)\n\n# -------------------------------------------\nax0 = fig.add_subplot(gs[0, 0])\nax0.ticklabel_format(style='plain')\nax0.scatter(data['TicketNumber'],data['CabinNumber'], s=5, color='navy')\nax0.set_xlabel('TicketNumber', fontsize=10, weight='bold')\nax0.set_ylabel('CabinNumber', fontsize=10, weight='bold')\nax0.set_title('TicketNumber vs. CabinNumber', fontsize=12, weight='bold')\n\n# -------------------------------------------\nax1 = fig.add_subplot(gs[0, 1])\nax1.ticklabel_format(style='plain')\nax1.scatter(data['TicketNumber'],data['Deck'], s=5, color='navy')\nax1.set_xlabel('TicketNumber', fontsize=10, weight='bold')\nax1.set_ylabel('Deck', fontsize=10, weight='bold')\nax1.set_title('TicketNumber vs. Deck', fontsize=12, weight='bold')\n# -------------------------------------------\n\nfig.tight_layout()\nplt.show()\n","bc388a71":"cabinpops = pd.pivot_table(data=DATA, index='Cabin', columns='Pclass', values='PassengerId', aggfunc='count')\ncabinpops['CabinPop'] = cabinpops.sum(axis=1)\ncabinpops.reset_index(drop=False, inplace=True)\ncabinpops.fillna(0, inplace=True)\ncabinpops.rename(columns={1:'CabinPop1',2:'CabinPop2',3:'CabinPop3'}, inplace=True)\nDATA = DATA.merge(cabinpops, on=['Cabin'], how='left')\nDATA['CabinClassCount'] = (DATA[['CabinPop1','CabinPop2','CabinPop3']]>0).sum(axis=1)\nDATA['MeanCabinClass'] = np.round((DATA['CabinPop1'] + (DATA['CabinPop2']*2) + (DATA['CabinPop3']*3))\/DATA['CabinPop'],1)\nfor c in ['CabinPop1','CabinPop2','CabinPop3','CabinPop','MeanCabinClass']:\n    DATA[c].fillna(-1, inplace=True)\ncat_features += ['CabinPop','CabinClassCount','MeanCabinClass']\n\n# -------------------------------------------\n\nfig = plt.figure(figsize=(16, 16), facecolor='white')\ngs = fig.add_gridspec(2, 2)\n\n# -------------------------------------------\nX = 'CabinClassCount'\ntrn = DATA[DATA['Survived'].notnull()].groupby([X])['Survived'].agg(['mean','size'])\ntrn.columns = ['SurvivalRate','TrainRows']\ntrn.reset_index(drop=False, inplace=True)\ntst = DATA[DATA['Survived'].isnull()].groupby([X])['Survived'].agg(['mean','size'])\ntst.columns = ['SurvivalRate','TestRows']\ntst.reset_index(drop=False, inplace=True)\ndata = trn.append(tst, ignore_index=True)\ndata = data.groupby(X, as_index=False).sum()\nax0 = fig.add_subplot(gs[0, 0])\nx = np.arange(len(data[X]))  # the label locations\nwidth = 0.35  # the width of the bars\ntrbars = ax0.bar(x - width\/2, data['TrainRows'], width, label='Train Data', color='navy', edgecolor='grey')\ntebars = ax0.bar(x + width\/2, data['TestRows'], width, label='Test Data', color='lightgrey',edgecolor='black')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax0.set_ylabel('Rows', fontsize=10, weight='bold')\nax0.set_title(X+' & Survival Rates', fontsize=12, weight='bold')\nax0.set_xticks(x)\nax0.set_xticklabels(data[X], fontsize=10, weight='bold')\nax0.legend(fontsize=12)\nax0.bar_label(trbars, labels=np.round(data['SurvivalRate'],2), padding=3)\n\n# -------------------------------------------\nX = 'CabinPop'\ntrn = DATA[DATA['Survived'].notnull()].groupby([X])['Survived'].agg(['mean','size'])\ntrn.columns = ['SurvivalRate','TrainRows']\ntrn.reset_index(drop=False, inplace=True)\ntst = DATA[DATA['Survived'].isnull()].groupby([X])['Survived'].agg(['mean','size'])\ntst.columns = ['SurvivalRate','TestRows']\ntst.reset_index(drop=False, inplace=True)\ndata = trn.append(tst, ignore_index=True)\ndata = data.groupby(X, as_index=False).sum()\nax1 = fig.add_subplot(gs[0, 1])\nx = np.arange(len(data[X]))  # the label locations\nwidth = 0.35  # the width of the bars\ntrbars = ax1.bar(x - width\/2, data['TrainRows'], width, label='Train Data', color='navy', edgecolor='grey')\ntebars = ax1.bar(x + width\/2, data['TestRows'], width, label='Test Data', color='lightgrey',edgecolor='black')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax1.set_ylabel('Rows', fontsize=10, weight='bold')\nax1.set_title(X+' & Survival Rates', fontsize=12, weight='bold')\nax1.set_xticks(x)\nax1.set_xticklabels(data[X], fontsize=10, weight='bold')\nax1.legend(fontsize=12)\nax1.bar_label(trbars, labels=np.round(data['SurvivalRate'],2), padding=3)\n\n# -------------------------------------------\nX = 'MeanCabinClass'\ntrn = DATA[DATA['Survived'].notnull()].groupby([X])['Survived'].agg(['mean','size'])\ntrn.columns = ['SurvivalRate','TrainRows']\ntrn.reset_index(drop=False, inplace=True)\ntst = DATA[DATA['Survived'].isnull()].groupby([X])['Survived'].agg(['mean','size'])\ntst.columns = ['SurvivalRate','TestRows']\ntst.reset_index(drop=False, inplace=True)\ndata = trn.append(tst, ignore_index=True)\ndata = data.groupby(X, as_index=False).sum()\nax2 = fig.add_subplot(gs[1, 0])\nx = np.arange(len(data[X]))  # the label locations\nwidth = 0.35  # the width of the bars\ntrbars = ax2.bar(x - width\/2, data['TrainRows'], width, label='Train Data', color='navy', edgecolor='grey')\ntebars = ax2.bar(x + width\/2, data['TestRows'], width, label='Test Data', color='lightgrey',edgecolor='black')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax2.set_ylabel('Rows', fontsize=10, weight='bold')\nax2.set_title(X+' & Survival Rates', fontsize=12, weight='bold')\nax2.set_xticks(x)\nax2.set_xticklabels(data[X], fontsize=10, weight='bold')\nax2.legend(fontsize=12)\nax2.bar_label(trbars, labels=np.round(data['SurvivalRate'],2), padding=3)\n# -------------------------------------------\n\nfig.tight_layout()\nplt.show()\n","03b1b353":"def cabin_sex(p):\n    if p==0:\n        return 'male'\n    elif p==1:\n        return 'female'\n    else:\n        return 'mixed'\n\n\nDATA['female'] = (DATA['Sex']=='female').astype(int)\nfemper = DATA.groupby(['Cabin'], as_index=False)['female'].mean()\nfemper.rename(columns={'female':'CabinFemalePerc'}, inplace=True)\nDATA = DATA.merge(femper, on=['Cabin'], how='left')\nDATA['CabinSex'] = DATA['CabinFemalePerc'].apply(cabin_sex)\ncat_features += ['CabinSex']\n\n# -------------------------------------------\nfig = plt.figure(figsize=(16, 8), facecolor='white')\ngs = fig.add_gridspec(1, 2)\n\n# -------------------------------------------\nX = 'CabinSex'\ntrn = DATA[DATA['Survived'].notnull()].groupby([X])['Survived'].agg(['mean','size'])\ntrn.columns = ['SurvivalRate','TrainRows']\ntrn.reset_index(drop=False, inplace=True)\ntst = DATA[DATA['Survived'].isnull()].groupby([X])['Survived'].agg(['mean','size'])\ntst.columns = ['SurvivalRate','TestRows']\ntst.reset_index(drop=False, inplace=True)\ndata = trn.append(tst, ignore_index=True)\ndata = data.groupby(X, as_index=False).sum()\nax0 = fig.add_subplot(gs[0, 0])\nx = np.arange(len(data[X]))  # the label locations\nwidth = 0.35  # the width of the bars\ntrbars = ax0.bar(x - width\/2, data['TrainRows'], width, label='Train Data', color='navy', edgecolor='grey')\ntebars = ax0.bar(x + width\/2, data['TestRows'], width, label='Test Data', color='lightgrey',edgecolor='black')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax0.set_ylabel('Rows', fontsize=10, weight='bold')\nax0.set_title(X+' & Survival Rates', fontsize=12, weight='bold')\nax0.set_xticks(x)\nax0.set_xticklabels(data[X], fontsize=10, weight='bold')\nax0.legend(fontsize=12)\nax0.bar_label(trbars, labels=np.round(data['SurvivalRate'],2), padding=3)\n\n# -------------------------------------------\n\nfig.tight_layout()\nplt.show()","809a0e4a":"cabinfare = pd.pivot_table(data=DATA, index='Cabin', columns='Pclass', values='Fare', aggfunc='sum')\ncabinfare.rename(columns={1:'CabinPrice1',2:'CabinPrice2',3:'CabinPrice3'}, inplace=True)\ncabinfare['CabinPrice'] = cabinfare.sum(axis=1)\ncabinfare['CabinFareRange'] = cabinfare[['CabinPrice1','CabinPrice2','CabinPrice3']].max(axis=1)-cabinfare[['CabinPrice1','CabinPrice2','CabinPrice3']].min(axis=1)\ncabinfare.reset_index(drop=False, inplace=True)\ncabinfare.fillna(0, inplace=True)\nDATA = DATA.merge(cabinfare, on=['Cabin'], how='left')\nDATA['MeanCabinFare'] = DATA['CabinPrice']\/DATA['CabinPop']\n\n# -------------------------------------------\nfig = plt.figure(figsize=(16, 16), facecolor='white')\ngs = fig.add_gridspec(2, 2)\n\n# -------------------------------------------\nax0 = fig.add_subplot(gs[0, 0])\nax0.hist(DATA['CabinPrice'], bins=100, color='navy')\nax0.set_xlabel('CabinPrice', fontsize=10, weight='bold')\nax0.set_ylabel('Rows', fontsize=10, weight='bold')\n# -------------------------------------------\nax1 = fig.add_subplot(gs[0, 1])\nax1.hist(DATA['MeanCabinFare'], bins=100, color='navy')\nax1.set_xlabel('MeanCabinFare', fontsize=10, weight='bold')\nax1.set_ylabel('Rows', fontsize=10, weight='bold')\n# -------------------------------------------\nax2 = fig.add_subplot(gs[1, 0])\nax2.hist(DATA['CabinFareRange'], bins=100, color='navy')\nax2.set_xlabel('CabinFareRange', fontsize=10, weight='bold')\nax2.set_ylabel('Rows', fontsize=10, weight='bold')\n# -------------------------------------------\nfig.tight_layout()\nplt.show()","086c85a5":"sig = pd.DataFrame({'Rows':range(5000), 'Weight':np.zeros(5000)})\nsig['Weight'] = sig['Rows'].apply(lambda n: 1.0\/(1 + np.exp( (2500-n)\/500 )))\n\nfig = plt.figure(figsize=(16, 8), facecolor='white')\nX = 'FirstNameFreq'\nplt.plot(sig['Rows'],sig['Weight'], color='red', linewidth=2.0)\nplt.xlabel('Rows', fontsize=14, weight='bold')\nplt.ylabel('Weight for Feature Value Rate', fontsize=14, weight='bold')\nplt.title('Survival Rate Weighting', fontsize=16, weight='bold')\nplt.tight_layout()\nplt.show()","4ef7f65d":"train = DATA[DATA['Survived'].notnull()].copy()\ntrain.reset_index(drop=True, inplace=True)\n\nbin_features = []\nmean_y = train['Survived'].mean()\nlower_y = mean_y - 3.0*np.sqrt(mean_y*(1-mean_y)\/100000)\nupper_y = mean_y + 3.0*np.sqrt(mean_y*(1-mean_y)\/100000)\nprint('MEAN TARGET: ',mean_y)\nprint('CONFIDENCE INTERVAL FOR MEAN: (',np.round(lower_y,5),' to ',np.round(upper_y,5),')')\nfor C in cat_features:\n    lvls = sorted(train[C].unique().tolist())\n    for lvl in lvls:\n        n  = train[train[C]==lvl].shape[0]\n        if n==0:\n            continue\n        s  = train.loc[train[C]==lvl, 'Survived'].mean()\n        wt = 1.0\/(1 + np.exp( (2500-n)\/500 ))\n        p = wt*s + (1-wt)*mean_y\n        upper = p + 3*np.sqrt(p*(1-p)\/n)\n        lower = p - 3*np.sqrt(p*(1-p)\/n)\n        if (upper_y < lower)|(lower_y > upper):\n            bin_col = C+'_'+str(lvl)\n            DATA[bin_col] = (DATA[C]==lvl).astype(int)\n            bin_features += [bin_col]\n            print(C+' = '+str(lvl)+' has estimated Survival Rate of:', np.round(p,5))\ndel(train)\ngc.collect()","4a45fe2b":"best_features = ['Sex','Pclass','Embarked','Deck','FamilySize','FirstNameCluster','TicketCat','AgeGroup','FareCluster']\nmean_y = DATA['Survived'].mean()\n\nfor f in best_features:\n    means = DATA.groupby(f, as_index=False)['Survived'].mean()\n    means.fillna(mean_y, inplace=True)\n    means[f+'_L'] = means['Survived'].rank(method='dense')\n    DATA = DATA.merge(means[[f, f+'_L']], on=f, how='left')\n\nlabel_features = [f+'_L' for f in best_features]","64d13585":"best_features.remove('FareCluster')\nbest_features.remove('Sex')\ntrain = DATA[DATA['Survived'].notnull()].copy()\ntrain.reset_index(drop=True, inplace=True)\ntest  = DATA[DATA['Survived'].isnull()].copy()\ntest.reset_index(drop=True, inplace=True)\nmean_male = train.loc[train['Sex']=='male','Survived'].mean()\nmean_female = train.loc[train['Sex']=='female','Survived'].mean()\npriors = {'male':mean_male, 'female':mean_female}\n\nfor f in best_features:\n    # Encode the test data based on all of training data\n    means = train.groupby(['Sex',f])['Survived'].agg(['mean','count'])\n    means.reset_index(drop=False, inplace=True)\n    means['weight'] = 1.0\/(1 + np.exp( (1250-n)\/250 ))\n    means['prior_mean'] = means['Sex'].map(priors)\n    means[f+'_T'] = means['weight']*means['mean'] + (1-means['weight'])*means['prior_mean']\n    test = test.merge(means[['Sex',f,f+'_T']], on=['Sex',f], how='left')\n    \n    # Encode the training data\n    encoded = pd.DataFrame()\n    skf = StratifiedKFold(n_splits=10)\n    for train_idx, valid_idx in skf.split(train, train['Sex_female']):\n        trn = train.loc[train_idx,['Sex',f,'Survived']]\n        val = train.loc[valid_idx,['PassengerId','Sex',f]]\n        \n        means = trn.groupby(['Sex',f])['Survived'].agg(['mean','count'])\n        means.reset_index(drop=False, inplace=True)\n        means['weight'] = 1.0\/(1 + np.exp( (1250-n)\/250 ))\n        means['prior_mean'] = means['Sex'].map(priors)\n        means[f+'_T'] = means['weight']*means['mean'] + (1-means['weight'])*means['prior_mean']\n        val = val.merge(means[['Sex', f, f+'_T']], on=['Sex',f], how='left')\n        encoded = encoded.append(val, ignore_index=True, sort=False)\n    \n    encoded = encoded.append(test[['PassengerId','Sex',f,f+'_T']], ignore_index=True, sort=False)\n    DATA = DATA.merge(encoded[['PassengerId',f+'_T']], on=['PassengerId'], how='left')\n    DATA.loc[DATA['Sex']=='male', f+'_T'].fillna(mean_male)\n    DATA.loc[DATA['Sex']=='female', f+'_T'].fillna(mean_female)\n\ndel(means,train, test, encoded)\ngc.collect()\n\ntgtenc_features = [f+'_T' for f in best_features]+['LNFare']\n","4a4a61e4":"import lightgbm as lgb\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n#------------------------------------------------------------------------------\ndef add_folds(trn, numfolds):\n    temp0 = trn[(trn['Survived']==0)&(trn['Sex']=='female')]\n    temp0['fold'] = np.random.randint(numfolds, size=temp0.shape[0])\n    temp1 = trn[(trn['Survived']==0)&(trn['Sex']=='male')]\n    temp1['fold'] = np.random.randint(numfolds, size=temp1.shape[0])\n    temp2 = trn[(trn['Survived']==1)&(trn['Sex']=='female')]\n    temp2['fold'] = np.random.randint(numfolds, size=temp2.shape[0])\n    temp3 = trn[(trn['Survived']==1)&(trn['Sex']=='male')]\n    temp3['fold'] = np.random.randint(numfolds, size=temp3.shape[0])\n    trn = temp0.append(temp1, ignore_index=True)\n    trn = trn.append(temp2, ignore_index=True)\n    trn = trn.append(temp3, ignore_index=True)\n    trn.sort_values(['PassengerId'], inplace=True)\n    trn.reset_index(drop=True, inplace=True)\n    return trn\n\n#------------------------------------------------------------------------------\ndef lgbm_model(train, test, features, param_dict, subpreds):\n    MAX_ROUNDS = 10000\n    STOP_ROUNDS = 50\n    VERBOSE_EVAL = 500\n    \n    mean_score = 0.0\n    test_preds = np.zeros(test.shape[0])\n    oof_preds  = np.zeros(train.shape[0])\n    import_scores = np.zeros(len(features))\n    for fold in range(5):\n        trn_X = np.array(train.loc[train['fold']!=fold, features])\n        trn_y = np.array(train.loc[train['fold']!=fold, 'Survived'])\n        val_X = np.array(train.loc[train['fold']==fold, features])\n        val_y = np.array(train.loc[train['fold']==fold, 'Survived'])\n        val_idx = train[train['fold']==fold].index.tolist()\n        \n        model = lgb.LGBMClassifier(**param_dict, n_estimators=MAX_ROUNDS, n_jobs=-1)\n        model.fit(trn_X, trn_y, eval_set=(val_X, val_y), verbose=VERBOSE_EVAL, early_stopping_rounds=STOP_ROUNDS)\n        \n        val_preds = model.predict_proba(val_X)[:,1]\n        oof_preds[val_idx] = val_preds\n        score = roc_auc_score(val_y, val_preds)\n        acc = accuracy_score(val_y, np.round(val_preds))\n        mean_score += acc\/5\n        import_scores += model.feature_importances_\/5\n        print('Fold AUC Score: ', score)\n        print('Fold ACC Score: ', acc)\n        print('--------------------------------')\n        \n        if subpreds:\n            test_preds += model.predict_proba(np.array(test[features]))[:,1]\/5\n    \n    print('Accuracy CV Score: ', mean_score)\n    imps = pd.DataFrame({'Feature':features, 'Importance':import_scores})\n    imps.sort_values(['Importance'], ascending=True, inplace=True)\n    return oof_preds, test_preds, mean_score, imps\n\n#------------------------------------------------------------------------------\ndef lgbm_imp_chart(imp_df, title):\n    fig = plt.figure(figsize=(16, 12), facecolor='white')\n    labels = imp_df['Feature'].tolist()\n    widths = imp_df['Importance'].tolist()\n    plt.barh(labels, widths, height=0.5, color='navy', edgecolor='grey')\n    plt.xlabel('Importance', fontsize=10, weight='bold')\n    plt.title(title, fontsize=12, weight='bold')\n    plt.ylabel('Feature', fontsize=10, weight='bold')\n    plt.tight_layout()\n    plt.show()\n    return\n\n#------------------------------------------------------------------------------\ndef sklearn_model(train, test, features, model, subpreds):\n    mean_score = 0.0\n    test_preds = np.zeros(test.shape[0])\n    oof_preds  = np.zeros(train.shape[0])\n    for fold in range(5):\n        trn_X = np.array(train.loc[train['fold']!=fold, features])\n        trn_y = np.array(train.loc[train['fold']!=fold, 'Survived'])\n        val_X = np.array(train.loc[train['fold']==fold, features])\n        val_y = np.array(train.loc[train['fold']==fold, 'Survived'])\n        val_idx = train[train['fold']==fold].index.tolist()\n    \n        model.fit(trn_X, trn_y)\n    \n        val_preds = model.predict_proba(val_X)[:,1]\n        oof_preds[val_idx] = val_preds\n        score = roc_auc_score(val_y, val_preds)\n        acc = accuracy_score(val_y, np.round(val_preds))\n        mean_score += acc\/5\n        print('Fold AUC Score: ', score)\n        print('Fold ACC Score: ', acc)\n        print('--------------------------------')\n        \n        if subpreds:\n            test_preds += model.predict_proba(np.array(test[features]))[:,1]\/5\n    \n    print('Accuracy CV Score: ', mean_score)\n    return oof_preds, test_preds, mean_score\n\n#------------------------------------------------------------------------------\n","2d6ad04b":"TRAIN = DATA.loc[DATA['Survived'].notnull()].copy()\nTRAIN.reset_index(drop=True, inplace=True)\nTEST  = DATA.loc[DATA['Survived'].isnull()].copy()\nTEST.reset_index(drop=True, inplace=True)\nTRAIN = add_folds(TRAIN, 5)\n#------------------------------------------------------------------------------\nparams = {}\nparams['boosting_type']    = 'gbdt'\nparams['objective']        = 'binary'\nparams['metric']           = 'auc'\nparams['num_leaves']       = 51\nparams['learning_rate']    = 0.01\nparams['colsample_bytree'] = 0.8\nparams['subsample']        = 0.9\nparams['max_depth']        = 21\nparams['subsample_freq']   = 1\nparams['bagging_seed']     = 351\nparams['verbosity']        = -1\n\nTRAIN['RANDOM'] = np.random.randint(2, size=TRAIN.shape[0])\nbin_features += ['RANDOM']\noof, tst, score, importancesB = lgbm_model(TRAIN, TEST, bin_features, params, False)\nlgbm_imp_chart(importancesB, 'Binary Feature Importances')\nbin_features.remove('RANDOM')\nimportancesB = importancesB[importancesB['Feature']!='RANDOM']\n\nparams = {}\nparams['boosting_type']    = 'gbdt'\nparams['objective']        = 'binary'\nparams['metric']           = 'auc'\nparams['num_leaves']       = 31\nparams['learning_rate']    = 0.01\nparams['colsample_bytree'] = 0.8\nparams['subsample']        = 0.9\n#params['max_depth']        = 5\nparams['subsample_freq']   = 1\nparams['bagging_seed']     = 351\nparams['verbosity']        = -1\n\nTRAIN['RANDOM'] = np.random.randint(10, size=TRAIN.shape[0])\nlabel_features += ['RANDOM']\noof, tst, score, importancesL = lgbm_model(TRAIN, TEST, label_features, params, False)\nlgbm_imp_chart(importancesL, 'Label Encoded Feature Importances')\nlabel_features.remove('RANDOM')\nimportancesL = importancesL[importancesL['Feature']!='RANDOM']\n\nTRAIN['RANDOM'] = np.random.uniform(size=TRAIN.shape[0])\ntgtenc_features += ['RANDOM']\noof, tst, score, importancesT = lgbm_model(TRAIN, TEST, tgtenc_features, params, False)\nlgbm_imp_chart(importancesT, 'Target Encoded Feature Importances')\ntgtenc_features.remove('RANDOM')\nimportancesT = importancesT[importancesT['Feature']!='RANDOM']","0aeb3034":"F = len(bin_features)\nsim_matrix = np.zeros((F,F))\nfor i in range(F):\n    for j in range(F):\n        sim_matrix[i,j] = accuracy_score(DATA[bin_features[i]], DATA[bin_features[j]])\n\nfig = plt.figure(figsize=(16, 16), facecolor='white')\nfig, ax = plt.subplots(figsize=(16,16))\ncax = ax.matshow(sim_matrix)\nax.grid(True)\nplt.title('Binary Feature Similarity')\nplt.xticks(range(F), bin_features, rotation=90);\nplt.yticks(range(F), bin_features);\nfig.colorbar(cax, ticks=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\nplt.show()\n\n# -------------------------------------------\nF = len(label_features)\ncorL_matrix = DATA[label_features].corr()\n\nfig = plt.figure(figsize=(16, 16), facecolor='white')\nfig, ax = plt.subplots(figsize=(16,16))\ncax = ax.matshow(corL_matrix)\nax.grid(True)\nplt.title('Label Encoded Feature Correlations')\nplt.xticks(range(F), label_features, rotation=90);\nplt.yticks(range(F), label_features);\nfig.colorbar(cax)\nplt.show()\n\n# -------------------------------------------\nF = len(tgtenc_features)\ncorT_matrix = DATA[tgtenc_features].corr()\n\nfig = plt.figure(figsize=(16, 16), facecolor='white')\nfig, ax = plt.subplots(figsize=(16,16))\ncax = ax.matshow(corT_matrix)\nax.grid(True)\nplt.title('Target Encoded Feature Correlations')\nplt.xticks(range(F), tgtenc_features, rotation=90);\nplt.yticks(range(F), tgtenc_features);\nfig.colorbar(cax)\nplt.show()","a602212b":"def feature_similarity(adj_matrix, imp_df):\n    features = imp_df['Feature'].tolist()\n    G = nx.from_numpy_matrix(np.matrix(adj_matrix))  #  Create a graph from the similarity adjacency matrix\n    components = sorted(nx.connected_components(G), key = len, reverse=True)  # separate out the connected components\n    print('Components: ',len(components))\n    i=0\n    out=[]\n    while i<len(components):\n        comp = list(components[i])\n        for n in comp:\n            if len(comp)>1:\n                f = features[n]\n                out = out + [(f,i)]\n        i=i+1\n    C = pd.DataFrame(out,columns=['Feature','SimComponent'])\n    imp_df = imp_df.merge(C, on='Feature', how='left')\n\n    uncorr = imp_df[imp_df['SimComponent'].isnull()].copy()\n    corrgp = imp_df[imp_df['SimComponent'].notnull()].copy()\n    corrgp.sort_values(['SimComponent','Importance'], ascending=[True,False], inplace=True)\n    corrgp.reset_index(drop=True, inplace=True)\n    corrgp.drop_duplicates('SimComponent', keep='first', inplace=True)\n\n    features = uncorr['Feature'].tolist() + corrgp['Feature'].tolist()\n    print('Best Features:')\n    print('------------------------')\n    print('Features that are not similar to any of the other features:')\n    print('-------------')\n    print(uncorr['Feature'].tolist())\n    print('-------------')\n    print('Features with the higest LGBM importance out of their similarity component:')\n    print('-------------')\n    print(corrgp['Feature'].tolist())\n    features = [f for f in features if f!='RANDOM']\n    return features\nprint('----------------------------------------------')\nprint('BINARY FEATURES')\nprint('----------------------------------------------')\nBmatrix = np.where((sim_matrix>0.95)|(sim_matrix<0.05), 1, 0)\nbin_features = feature_similarity(Bmatrix, importancesB)\nprint('----------------------------------------------')\nprint('LABEL-ENCODED FEATURES')\nprint('----------------------------------------------')\nLmatrix = np.where(np.absolute(corL_matrix)>0.95, 1, 0)\nlabel_features = feature_similarity(Lmatrix, importancesL)\nprint('----------------------------------------------')\nprint('TARGET-ENCODED FEATURES')\nprint('----------------------------------------------')\nTmatrix = np.where(np.absolute(corT_matrix)>0.95, 1, 0)\ntgtenc_features = feature_similarity(Tmatrix, importancesT)","a90c1f56":"OOF_PRED = TRAIN[['PassengerId','fold','Survived']].copy()\nTESTPRED = pd.DataFrame({'PassengerId':TEST['PassengerId'],'LGBM':np.zeros(TEST.shape[0])})\nCV_Scores = {}","e66dfb76":"params = {}\nparams['boosting_type']    = 'gbdt'\nparams['objective']        = 'binary'\nparams['metric']           = 'auc'\nparams['num_leaves']       = 51\nparams['learning_rate']    = 0.01\nparams['colsample_bytree'] = 0.8\nparams['subsample']        = 0.9\nparams['max_depth']        = 21\nparams['subsample_freq']   = 1\nparams['bagging_seed']     = 351\nparams['verbosity']        = -1\nprint('--------------------------------------------------')\nprint('LightGBM Model')\nprint('--------------------------------------------------')\noof, tst, score, importances = lgbm_model(TRAIN, TEST, bin_features, params, True)\nOOF_PRED['LGBM_B'] = oof\nTESTPRED['LGBM_B'] = tst\nCV_Scores['LGBM_B'] = score\nprint('--------------------------------------------------')\nprint('Multi-Layer Perceptron Model')\nprint('--------------------------------------------------')\noof, tst, score = sklearn_model(TRAIN, TEST, bin_features, MLPClassifier(), True)\nOOF_PRED['MLP_B'] = oof\nTESTPRED['MLP_B'] = tst\nCV_Scores['MLP_B'] = score\nprint('--------------------------------------------------')\nprint('Logistic Regression Model')\nprint('--------------------------------------------------')\noof, tst, score = sklearn_model(TRAIN, TEST, bin_features, LogisticRegression(), True)\nOOF_PRED['LOG_B'] = oof\nTESTPRED['LOG_B'] = tst\nCV_Scores['LOG_B'] = score\nprint('--------------------------------------------------')\nprint('KNeighbors Classifier Model')\nprint('--------------------------------------------------')\noof, tst, score = sklearn_model(TRAIN, TEST, bin_features, KNeighborsClassifier(n_neighbors=101, weights='distance'), True)\nOOF_PRED['KNN_B'] = oof\nTESTPRED['KNN_B'] = tst\nCV_Scores['KNN_B'] = score\nprint('--------------------------------------------------')\nprint('Quadratic Discriminant Analysis Model')\nprint('--------------------------------------------------')\noof, tst, score = sklearn_model(TRAIN, TEST, bin_features, QuadraticDiscriminantAnalysis(reg_param=0.2), True)\nOOF_PRED['QDA_B'] = oof\nTESTPRED['QDA_B'] = tst\nCV_Scores['QDA_B'] = score\n","4458a048":"params = {}\nparams['boosting_type']    = 'gbdt'\nparams['objective']        = 'binary'\nparams['metric']           = 'auc'\nparams['num_leaves']       = 31\nparams['learning_rate']    = 0.01\nparams['colsample_bytree'] = 0.8\nparams['subsample']        = 0.9\n#params['max_depth']        = 5\nparams['subsample_freq']   = 1\nparams['bagging_seed']     = 351\nparams['verbosity']        = -1\n\nprint('--------------------------------------------------')\nprint('LightGBM Model')\nprint('--------------------------------------------------')\noof, tst, score, importances = lgbm_model(TRAIN, TEST, label_features, params, True)\nOOF_PRED['LGBM_L'] = oof\nTESTPRED['LGBM_L'] = tst\nCV_Scores['LGBM_L'] = score\nprint('--------------------------------------------------')\nprint('Multi-Layer Perceptron Model')\nprint('--------------------------------------------------')\noof, tst, score = sklearn_model(TRAIN, TEST, label_features, MLPClassifier(), True)\nOOF_PRED['MLP_L'] = oof\nTESTPRED['MLP_L'] = tst\nCV_Scores['MLP_L'] = score\nprint('--------------------------------------------------')\nprint('Logistic Regression Model')\nprint('--------------------------------------------------')\noof, tst, score = sklearn_model(TRAIN, TEST, label_features, LogisticRegression(), True)\nOOF_PRED['LOG_L'] = oof\nTESTPRED['LOG_L'] = tst\nCV_Scores['LOG_L'] = score\nprint('--------------------------------------------------')\nprint('KNeighbors Classifier Model')\nprint('--------------------------------------------------')\noof, tst, score = sklearn_model(TRAIN, TEST, label_features, KNeighborsClassifier(n_neighbors=101, weights='distance'), True)\nOOF_PRED['KNN_L'] = oof\nTESTPRED['KNN_L'] = tst\nCV_Scores['KNN_L'] = score\nprint('--------------------------------------------------')\nprint('Quadratic Discriminant Analysis Model')\nprint('--------------------------------------------------')\noof, tst, score = sklearn_model(TRAIN, TEST, label_features, QuadraticDiscriminantAnalysis(reg_param=0.2), True)\nOOF_PRED['QDA_L'] = oof\nTESTPRED['QDA_L'] = tst\nCV_Scores['QDA_L'] = score","af9a2c84":"print('--------------------------------------------------')\nprint('LightGBM Model')\nprint('--------------------------------------------------')\noof, tst, score, importances = lgbm_model(TRAIN, TEST, tgtenc_features, params, True)\nOOF_PRED['LGBM_T'] = oof\nTESTPRED['LGBM_T'] = tst\nCV_Scores['LGBM_T'] = score\nprint('--------------------------------------------------')\nprint('Multi-Layer Perceptron Model')\nprint('--------------------------------------------------')\noof, tst, score = sklearn_model(TRAIN, TEST, tgtenc_features, MLPClassifier(), True)\nOOF_PRED['MLP_T'] = oof\nTESTPRED['MLP_T'] = tst\nCV_Scores['MLP_T'] = score\nprint('--------------------------------------------------')\nprint('Logistic Regression Model')\nprint('--------------------------------------------------')\noof, tst, score = sklearn_model(TRAIN, TEST, tgtenc_features, LogisticRegression(), True)\nOOF_PRED['LOG_T'] = oof\nTESTPRED['LOG_T'] = tst\nCV_Scores['LOG_T'] = score\nprint('--------------------------------------------------')\nprint('KNeighbors Classifier Model')\nprint('--------------------------------------------------')\noof, tst, score = sklearn_model(TRAIN, TEST, tgtenc_features, KNeighborsClassifier(n_neighbors=101, weights='distance'), True)\nOOF_PRED['KNN_T'] = oof\nTESTPRED['KNN_T'] = tst\nCV_Scores['KNN_T'] = score\nprint('--------------------------------------------------')\nprint('Quadratic Discriminant Analysis Model')\nprint('--------------------------------------------------')\noof, tst, score = sklearn_model(TRAIN, TEST, tgtenc_features, QuadraticDiscriminantAnalysis(reg_param=0.2), True)\nOOF_PRED['QDA_T'] = oof\nTESTPRED['QDA_T'] = tst\nCV_Scores['QDA_T'] = score","0fe21b3f":"models = ['LGBM','MLP','LOG','KNN','QDA']\nmodel_list = [f+'_B' for f in models]+[f+'_L' for f in models]+[f+'_T' for f in models]\nF = len(model_list)\noof_corr_matrix = OOF_PRED[model_list].corr()\ntest_corr_matrix = TESTPRED[model_list].corr()\n# -------------------------------------------\nfig = plt.figure(figsize=(16, 8), facecolor='white')\ngs = fig.add_gridspec(1, 2)\n\n# -------------------------------------------\nax0 = fig.add_subplot(gs[0, 0])\ncax = ax0.matshow(oof_corr_matrix)\nax0.grid(True)\nax0.set_title('OOF Prediction Correlations', fontsize=12, weight='bold')\nax0.set_xticks(range(F))\nax0.set_xticklabels(model_list, fontsize=10, weight='bold')\nax0.set_yticks(range(F))\nax0.set_yticklabels(model_list, fontsize=10, weight='bold')\n\n# -------------------------------------------\nax1 = fig.add_subplot(gs[0, 1])\ncax = ax1.matshow(test_corr_matrix)\nax1.grid(True)\nax1.set_title('Test Prediction Correlations', fontsize=12, weight='bold')\nax1.set_xticks(range(F))\nax1.set_xticklabels(model_list, fontsize=10, weight='bold')\nax1.set_yticks(range(F))\nax1.set_yticklabels(model_list, fontsize=10, weight='bold')\n\n# -------------------------------------------\nfig.colorbar(cax, ticks=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\nplt.tight_layout()\nplt.show()\n# -------------------------------------------\nprint('Correlations --------')\nprint(oof_corr_matrix)\nprint('CV Scores -----------')\nprint(CV_Scores)","5382c838":"from scipy.optimize import minimize\n\ndef RMSE(x,a,s):\n    return np.sqrt(np.mean((np.matmul(a,x)-s)**2))\n\noof = np.array(OOF_PRED[model_list].copy())\nS = np.array(OOF_PRED['Survived'])\nresult = minimize(RMSE, x0=np.zeros(len(model_list)), args=(oof,S))        \nout = pd.DataFrame({'Model':model_list, 'Weight':result.x})\nout['Weight'] = out['Weight']\/out['Weight'].sum()\nprint(out)","64ed1ace":"OOF_PRED['EnsPred'] = np.matmul(oof, np.array(out['Weight']))\nscore = accuracy_score(OOF_PRED['Survived'], np.round(OOF_PRED['EnsPred']))\nprint('Ensemble Accuracy Score: ', score)\n\ntst = np.array(TESTPRED[model_list])\nTESTPRED['Survived'] = np.round(np.matmul(tst, np.array(out['Weight']))).astype(int)\nSUB = TESTPRED[['PassengerId','Survived']]\nSUB.to_csv('EnsembleSubmission.csv', index=False)\n\npreds = np.round(oof).astype(int)\nOOF_PRED['VotePred'] = np.round(preds.mean(axis=1)).astype(int)\nscore = accuracy_score(OOF_PRED['Survived'], OOF_PRED['VotePred'])\nprint('Voting Accuracy Score: ', score)\n","504c5eb3":"outcomes = ['Died','Survived']\nF = len(outcomes)\nconfusion = confusion_matrix(OOF_PRED['Survived'], np.round(OOF_PRED['EnsPred']))\nfig = plt.figure(figsize=(16, 8), facecolor='white')\ngs = fig.add_gridspec(1, 2)\n\n# -------------------------------------------\nax0 = fig.add_subplot(gs[0, 0])\ncax = ax0.matshow(confusion)\nax0.set_title('Ensemble Prediction Confusion Matrix', fontsize=12, weight='bold')\nax0.set_xlabel('Actual Outcomes', fontsize=10, weight='bold')\nax0.set_xticks(range(F))\nax0.set_xticklabels(outcomes, fontsize=10, weight='bold')\nax0.set_ylabel('Model Predictions', fontsize=10, weight='bold')\nax0.set_yticks(range(F))\nax0.set_yticklabels(outcomes, fontsize=10, weight='bold')\nax0.annotate(str(confusion[0,0]),(0,0), fontsize=10, weight='bold')\nax0.annotate(str(confusion[0,1]),(0,1), fontsize=10, weight='bold')\nax0.annotate(str(confusion[1,0]),(1,0), fontsize=10, weight='bold')\nax0.annotate(str(confusion[1,1]),(1,1), fontsize=10, weight='bold')\n#fig.colorbar(cax)\nplt.tight_layout()\nplt.show()","52103d81":"TRAIN['EnsPred'] = np.round(OOF_PRED['EnsPred'])\nfor F in ['Sex','Pclass','AgeGroup','FareCluster','FamilySize']:\n    vals = sorted(TRAIN[F].unique().tolist())\n    for v in vals:\n        temp = TRAIN[TRAIN[F]==v]\n        acc = accuracy_score(temp['Survived'], temp['EnsPred'])\n        print(F+' == '+str(v)+' Accuracy:', np.round(acc,3))","6cc97ae8":"## **2.2 Multi-Sex Cabins**","78eef888":"Well, there doesn't seem to much of relationship between the two.","36648744":"## **1.8 Name**\nBecause of the extreme number of **Name** values we'll first break the name into first and last names and then we will *frequency* encode them by replacing the names with the number of rows of data that have that first or last name.","afdee18e":"OK, it looks like have some differences in the Fare prices based on Embarkation, Cabin and Class.  So we'll use those three variables to fill in the missing fares.\n\nThe raw fare values look like they have 4 main cluster points (the peaks in the histogram).  So we'll use the log value (so the extreme values don't skew the clusters towards the high end) and do a quick k-means clustering to build 4 bins of fare values then look at survival rates in each bin.","e218c5fb":"## **Things to Note:**\n* Having a Cabin looks like its good for survival\n* The actual numeric part of the cabin number doesn't look to valuable at the moment but the Deck may be of use","af2443d8":"## **Things to Note:**\n* Travelling without family may indicate a lower survival rate\n* There appears to be more family in the test dataset than in the train set","f06badcf":"## **5.1 Binary Data Models**","2fa51d1b":"## **3.2 Label-Encoded Features**\nFor this dataset we'll be conservative again, and a bit subjective, and pull out the basic features that show up the most in the set of binary features we identified above.  Looking at the list of binary features we can see the following show up repeatedly:\n* **Sex** shows up on its own and as part of CabinSex.  It is also the single most predictive feature for survival.\n* **Pclass** shows up on its own and as part of MeanCabinClass and CabinClassCount, so we'll keep Pclass but not the Cabin-Class features.\n* **Embarked** shows up on its own so we'll keep that.\n* **Cabin** shows up as part of several binary features but usually in regard to whether or not a passenger has a cabin.  For example the HasCabin feature is the same as Deck==Z.  So for this dataset we'll keep the **Deck** feature.\n* **Parch & SibSp** Parch shows up on its own while SibSp shows up as part of the FamilySize feature.  So for this dataset we'll keep the **FamilySize** feature.\n* **Name** The only part of the name that we found usable in the bianry features is the **FirstNameCluster** so we will keep that.\n* **Ticket**  TicketType and TicketCat show up, but its apparent that having a TicketType is really the driver of these.  So we'll keep the **TicketCat**, where TicketType=N means no TicketType.\n* **Age**  A couple of the AgeGroup levels show up, so we'll keep **AgeGroup**.\n* **Fare**  The **FareCluster** shows up so we'll keep that.\n\nNow let's label encode the features we have chosen but we will do this so that they have a more \"ordinal\" encoding.  To do this we'll sort the feature levels based on the survival rate of each level and encode them in that order.  For example, if we were to apply the sklearn LabelEncoder to the Sex feature, 'female' would be encoded as 0 while \"male\" is encoded with a 1.  This is because the sklearn LabelEncoder sorts the feature values and encodes them in (in this case) alphatbetical order.  But females have a much higher survival rate than males and we want to have the lowest survival rate encoded with 0 and progress up from there.  So female should be encoded with 1 while male is encoded with 0.\n\nFor tree-based models this sorted encoding is probably not necessary since the relative sizes of the encoded value matter less to the tree-splitting.  But for other model types the order & size of the encoded values do matter.","2210dd23":"It looks like we have some highly-similar features in the binary data.  We'll fudge a bit here and say that any two binary features that have more than 95% in common or less than 5% in common are \"similar.\"  Then we'll only keep the feature that has the higher importance in our LightGBM model.\n\nNow, it's not just pairs of features we need to consider because there may be groups of features that are mutually similar.  In order to see these groups we're going to use a graph.  A [graph](http:\/\/en.wikipedia.org\/wiki\/Graph_theory#:~:text=In%20mathematics%2C%20graph%20theory%20is,also%20called%20links%20or%20lines) is a set of nodes (or vertices), connected by edges.  We'll say each feature is a node and two features are connected by an edge if they are \"similar\", or in the case of label-encoded and target-encoded data, correlated.\n\nWe'll then look at \"Connected Components\" of the graph.  These are the sets of mutually connected nodes\/features and from each of these connected components we will keep the features with the highest importance to the LightGBM model.","a1c18422":"## **Things to Note:**\n* Age Groups 3 & 4 look useful for predicting the survivability\n* But there are very different amounts of Group3 and Group4 between train and test","9fc94c4e":"It looks like these name features don't get us much except that high-cardinality first names have a low survival rate.  So let's bin the first name frequencies and see if we can use it.","79ef7ba1":"## **3.3 Target-Encoded Features**\nFor target encoding you replace the values of the categorical features with the mean of the target feature for each categorical value.  For example, the mean of the target feature (Survived) is 0.2058 for all males (i.e. Sex==male), so we would replace \"male\" with 0.2058 in the \"Sex\" feature.  We need to be careful when doing this as it can easily lead to overfitting especially for rare categorical values.  We're going to do one thing that could increase overfitting, but then also doa few things to mitigate any overfitting.\n\nIncreasing the overfitting risk, we'll segregate the male from the females since their survival rates differ so much.  This essentially doubles the number of categorical values we need to replace with the target mean.\n\nTo be conservative against overfitting we'll use the same sigmoid function and weighted average method we used in section 3.1 for identifying binary features.  In short we'll use the adjusted survival estimates as the numeric value for each categorical value.  And we'll use an out-of-fold method to do the encoding.\n\nFor these features we'll use the LNFare feature instead of the FareCluster because after target-encoding we'll have data that looks more continuous than categorical, and the Fare feature is the only real continuous variable in the data.\n","cc859d72":"## **Things to Note:**\n* Names don't seem to have much value except for the FirstNameCluster","bd34ac15":"## **6.2 Linear Optimization to Weight the Models**\n\nTo blend the models we'll use a weighted average.  To determine the weights we'll use a linear optimization problem to determine the best weights for each model.","c408a06c":"# **6.0 ENSEMBLING**\n\n## **6.1 Model Prediction Correlations**\nWe want to look at the correlations of the different model predictions to see if they are highly correlated.  If two models are essentially predicting the same survival probabilities we don't need to include both in the ensemble (also called a blend of models).  The cutoff for what is \"highly\" correlated is a bit subjective.  You need to consider the CV scores and the model types when deciding what to keep.  A good blned of models includes models of different types (tree-based, neural netwroks, nearest neighbors, etc.).  We'll keep all 5 here since they are all of different types (even though a couple have some high correlations).","49897c32":"# **4.0 MODEL FUNCTIONS**\nIn thi section we're going to create a couple of models and build them as functions that we can call with different data and feature sets.","f78bf628":"## **2.2 Cabin Population by Class**\n","581b8788":"## **1.4 Class & Embarked**\nIt certainly appears that class has a big impact on survivability.  Also, there appears to be a lot more third-class passengers in the test data than there are in the train data.  This could mean that the test data will have a lower overall survivability rate than the train data.\n\nThere are a few missing values in the Embarked feature and we'll fill those with S since that is where the vast majority of passengers embarked from.  It also appears that leaving from Southhampton (Embarked==S) has a much lower survival rate than leaving from other locations.","5ffc8aad":"## **1.2 Age**\nWe have a number of missing values in the Age column so we'll fill the missing values with the overal mean Age.  There are also a number of \"babies on board.\"  These are listed with ages by the tenth of a year (e.g., 0.4 or 0.6) which we will replace with zeros.  Age is an ordinal variable for everyone over the age of one.  This means someone who is 22.8 years old is recorded as being 22.  To make the age differences consistent we should make the babies all 0 years old.\n\nWe'll also do some binning based on Age Groups.  The Age groups are a bit arbitrary.  They are based on what the definitions of infant, toddler, teenager, etc.  We also break the \"Adults\" into 18-39, 40-70 and 70+ groups based on looking at the changes in the survival rates of various ages.  The bins are named \"AgeGroupX\" so that when we label encode them they stay in order from youngest to oldest. The Groups are as follows:\n* AgeGroup0 = 0-3 years\n* AgeGroup1 = 4-10 years\n* AgeGroup2 = 11-17 years\n* AgeGroup3 = 18-39 years\n* AgeGroup4 = 40-70 years\n* AgeGroup5 = >70 years","e9679b38":"## **2.4 Cabin Prices**\n","dc151dda":"## **6.3 Ensemble & Output**","c6d6ad05":"Thanks for checking out our notebook!!  We built it as part of the Kaggle mentor program as a set of tips, tricks and methods for use on tabular data in a Kaggle competition.  We examine the data, do some feature engineering including four different types of categorical encoding and some clustering, do some feature selection and build five different model types on three different sets of data.  Then we ensemble the 15 models for a submission file.  We hope you find it useful!","b4efec6b":"# **4.0 FEATURE SELECTION**\nSo now we have a whole bunch of features that we think will be useful for modeling.  But let's check them and see how they do.  We'll do checks here, the first is a RANDOM feature trap using feature importances and the second is a correlation trap to remove highly correlated features.\n","c64b78a7":"# **3.0 FEATURE SETS & CATEGORICAL ENCODING OPTIONS**\nWe've done a bit of feature engineering while we explored each feature, mainly through binning & clustering the values.  Now we'll build three datasets that we can use for modeling.  The first data set will consist of binary features only and the second will be a smaller set of the most impactful features, label-endcoded instead of binary.  The third set will use the same features as the label-encoded set but we will target-encode them instead of label encoding.\n\n\n## **3.1 Binary Features**\nFor this set of data we'll make use of the power of confidence intervals and see which ones will really get us the most bang-for-the-buck.  What we want to do here is build binary features for each of the feature values that are the most indicative of survival.  The thought being that a limited number of specific feature values will be best at predicting without overfitting.\n\nTo do this we will be VERY conservative, and adandon statistical rigor a bit.  We want to be careful with feature values that are rare in the dataset but have extreme survivability rates.  For example, Deck==E has a survival rate of 62% but it's less prevalent than some of the other Deck values and may not be predictive because it doesn't occur as frequently.  To handle this we'll adjust the survival rate for each feature value by using a weighted average with the overal survival rate, *S<sub>r* = 0.42774.  The greater the number of rows of data we have the more we will weight the feature value's actual survival rate.  We'll use the weights based on the following sigmoid curve, which says that we only fully trust the feature-value survival rate if there at least 5,000 rows of train data with that feature value.","04d85088":"## **4.1 RANDOM Feature Trap**\nThe idea here is to identify the features that don't really add anything to the model becuase they are no more useful than a column of random numbers.  What we'll do is add a column of random numbers and then run a LightGBM model and look at the feature importances.  Any feature that is less important than the RANDOM column is suspect.\n\n","7c5d8671":"## **Things to Note:**\n* These both look like good features for indicating survival\n* But again, there are very different amounts of third-class passengers between train and test","4c5c1ab4":"## **1.3 Family Sizes (SibSp and Parch)**\nThere is a lot to look at with the SibSp and Parch features and there combination into \"Family Sizes.\"  In particular, in looks like those passengers travelling without any other family members (FamilySize==1) survive somewhat less frequently.","e97efb72":"## **4.2 Feature Similarity\/Correlation**\nWhat we are looking for here are different features that provide the model the same information.  If we have two features that are the \"same\" we don't need both and including both can make the model look better than it is.\n\nFor the binary features we'll look at similarity, not correlation, because correlation doesn't tell us much when the features are binary.  We'll use the accuracy_score function to measure the similarity between features.  All this does is tell us what percentage of the two columns contain the same values.  While we want to see what features pairs are the \"same\" (accuracy_Score==1.0 or the two features are identical) we also want to check which features are mirror-images of each other (accuracy_score==0.0 or the two functions are the inverse of each other).  Binary features that are mirror-images of each other provide the same data to a model making one of them redundant.  For example, Sex==male is the same as Sex!=female, so we don't need both columns.\n\nFor the label-encoded data and the target-encoded data we'll stick with regular correlations.","e7f1d0e4":"## **Things to Note:**\n* Fare Clusters look to have some value in predicting survival\n* There is some imbalance between train and test datasets for the fare","1959b401":"# **1.0 FEATURE EXPLORATION**\nIn this section we'll go through some the features of the data, fill in missing values, and create some new features.  All of the data we have here is categorical with the exception of the **Fare** feature.  Some of the features are integers where order matters (i.e. ordinal features), such as **Age** of **Pclass**, but we we'll view them as categorical because each value is discrete from other values.  We'll also build some charts or graphs to visualize the feature data.\n\nThe goal is to explore each of the features to see which ones (or combinations of more than one) expose different survival rates.  Actually we'll be looking for statistically *significant* differences.  Of the 100,000 passengers in the train data, 42.774% of them survive.  We'll call this overall survival rate *S<sub>r*.  And,\n$$S_r = .42774$$\n\n**Confidence Interval**\n\nViewing the train dataset as a sample of the population of all 200,000 passengers (train & test) we can use a simple 95% confidence interval on a sample survival rate to indicate feature values that show a significantly different survival rate from the overall survival rate.  The confidence interval for a proportion is given as:\n\n$$(p' - z_\\alpha * \\sqrt{\\frac{p' * (1-p')}{N}} , p' + z_\\alpha * \\sqrt{\\frac{p' * (1-p')}{N}})$$\n\nFor a 99.7% Confidence interval on our overall survival rate would be:\n\n$$S_r \\pm 3.0 * \\sqrt{\\frac{S_r*(1-S_r)}{100,000}} = (0.42305, 0.43243)$$\n\nSo to start with we will be especially interested in feature or feature values that can show us portions of the data where the survival rates are lower than 42.467% or higher than 43.080%.  This is just to start with, we will also have to take into account how much data we have for each feature value.  But we'll do that later ...\n\n**High-Cardinality Categorical Variables**\n\nThis data contains a few high-cardinality categorical features.  That is, features that have a large number values in relation to the size of the dataset.  For example, there are 174,854 unique **Name** values in the 200,000 rows of data.  Similarly, there are 132,613 **Ticket** values and 45,442 **Cabin** values.  Simply label encoding these can lead to overfitting when models predict for the unique combinations of these types of features.  To try and control this we'll \"bin\" these feature's values into smaller blocks or clusters.","2d102913":"We noticed some imbalances between the train and test datasets earlier which we'll want to look at and see what impacts they could have.  So let's take a look at how our predictions score on certain slices of the data.  In particular we noticed that:\n\n**Sex**\n* 56% of the train data set has Sex==male, however, 70% of the test data set is male.\n\n**Pclass**\n* 41% of the train data set has Pclass==3, however, 64% of the test data set falls in 3rd class.\n* 29% of the train data set has Pclass==2, however, only 9% of the test data set falls in 2nd class.\n\n**Age**\n* 41% of the train data set has AgeGroup==AgeGrp3, however, 65% of the test data set falls in AgeGrp3.\n* 45% of the train data set has AgeGroup==AgeGrp4, however, only 24% of the test data set falls in AgeGrp4.\n\n**FareCluster**\n* 42% of the train data set has FareCluster==1, however, 55% of the test data set falls in Cluster1.\n* 36% of the train data set has FareCluster==2, however, 21% of the test data set falls in Cluster2.\n*  8% of the train data set has FareCluster==4, however, 12% of the test data set falls in Cluster4.\n\n**Family Sizes**\n* 62% of the train data set has FamilySize==1, however, only 55% of the test data set has FamilySize==1.\n* 73% of the train data set has SibSp==0, however, only 62% of the test data set has SibSp==0.\n* 20% of the train data set has SibSp==1, however, 31% of the test data set has SibSp==1.\n","ed997e70":"## **5.2 Label-Encoded Data Models**","786afcfb":"## **5.3 Target-Encoded Data Models**","509099ad":"**LOAD LIBRARIES & DATA FILES**","86f8ff56":"## **Things to Note:**\n* NanCounts 0 & 1 look like good indicators of survivability\n* There are similar amounts of data for each NanCount in the train and test datasets\n* Sex looks like a VERY good indicator of survivability\n* There is some imbalance of sexes between train and test - there are a fair amount more males in the test dataset than in the train dataset","ddbb77f3":"## **Things to Note:**\n* Passengers not having a TicketType looks like it may survive more than those with one\n* Low (4-digit) TicketNumbers and high (9-digit) TicketNumbers may be useful for modeling","c7b59ea6":"Continuing with the Deck==E example, we see that in the train dataset Deck==E occurs 1,749 times with a mean survival rate of 61.7496%.  The above curve says we weight Deck D's survival rate at 0.1821 and weight the overall rate, *S<sub>r* with 1-0.1821 = 0.8179.  This means we are estimating the actual survival rate for Deck==E as (0.1821 * 0.617496)+(0.8179 * 0.42774) = 0.46229.\n\nNow we check to see if a confidence interval around the new estimated survival rate for Deck==E overlaps the 99.7% confidence interval we calculated for *S<sub>r* , (0.42305, 0.43243).  This is where we abandon statistical rigor a bit since this isn't really how one calculates a confidence interval on an adjusted estimate (i.e., the 0.46229 survival rate).  And again, we will be VERY conservative by using a 99.7% confidence interval on the adjusted estimate.  The formulas remain the same as those in section 1.0.\n\nFor Deck==E this gives an interval of (0.42653, 0.49806).  And because this interval overlaps with the *S<sub>r* interval we cannot say that the survival rate for Deck==E is significantly different from the overall survival rate and indicates a greater chance of survival.\n\nSo let's do this for all of the features we've identified and see which ones are the most significant.","83833f71":"# **2.0 FEATURE COMBINATIONS**\n","856f9520":"## **6.4 How good are our predictions?**\nLet's take a quick look at the confusion matrix for our predictions:","f953f2a5":"These charts are a little hard to read since there are some extreme fare values.  A good way to adjust a distribution with some extreme values like this is to take the Log of all the values.  Let's see how this changes the visualizations.","ce63863e":"# **5.0 FINAL MODELS**","abfcc064":"Wow.  The RANDOM feature finishes pretty high!!  That means there is a lot of randomness in the data and we should consider simpler models.  Simpler models generalize better - that is, they may not have as high a CV Score (or Public LB score) but they are more likely to have consistent performance when applied to more unseen data (i.e., the Private Leaderboard).  Another way of saying this is that more complex models may identify patterns that only exist in the randomness of the training dataset.\n\nSo we need to look at simplifying a bit.  We'll do this two ways:\n* Whittle down the number of features by removing some that are redundant by examining the similarity of the features\n* Take a look at less powerful models (LightGBM is pretty powerful)\n\nNormally we could remove any features that are less important than the RANDOM feature and, with some tuning, the LGBM model performance could improve.  But for what we're doing here we'll leave all the features in place (since most finish with less improtance than RANDOM) and let the correlation trap whittle down the number of features.\n\nAlso, note that the feature importances really only apply to tree-based models.  Other model types may stress some of the features that have low feature importances here for the LightGBM model.","6401592d":"## **1.5 Cabin Numbers**\nWe can do a couple of things with the Cabin Number, even though only about 30% of all passengers have a cabin (And it is certainly advantageous to have a cabin).  61,303 out of the 200,000 passengers have a cabin number.  There are 45,442 cabin numbers and each one begins with a letter.  For the Titanic data this letter indicated the Deck that the cabin was on and there are 8 decks (A-G and T) in the Synthantic data.  We'll pull the cabin number apart a bit to provide some \"bins\" of cabins and see what that shows.\n\nWe'll break the cabin number into three pieces, Deck (first letter), Section (thousands portion of the numerical part), and Room (last three numerical digits).  We'll call the fall numeric part of cabin the CabinNumber.  We'll also create a binary feature called \"HasCabin.\"","5546a7db":"OK, we've gone through some modeling and some feature selection and arrived at a robust set of \"important\" features.  Let's use them in a couple of different types of models and then do some ensembling (also called blending).  In order to explore the ensembling opportunities later, we'll keep track of the out-of-fold predictions and the test predictions.  We'll make all of predictions as probabilities vs binary outputs and save them in dataframes OOF_PRED and TESTPRED.","8c20c532":"# **7.0 Next Steps**\nSo we built a whole bunch of stuff here and introduced a lot of tricks and techniques.  To improve more I'd suggest exploring:\n1. Feature Engineering.  There is almost no end to the possibilities of feature engineering.  For example, we did not explore PCA or ICA methods of dimensionality reduction.\n2. Model Tuning.  Almost all of the models used here are not \"tuned\" to an optimal performance.  Tuning the various parameters of each of them could improve scores further.\n3. Ensembling.  Ensembling can be done in a variety of ways and we only use one here.  Other methods could use Logistic Regression, neural networks or various types of means (like a geometric mean).  Also, you may note that we used the same fold-structure in our Cross-Validation scheme for all of the models.  This can be changed up, but it also provides a way to see if different models perform differently on different folds, which can give some additional insight into the data.\n","1d0d845c":"## **1.6 Fare**\n\nFare is really the only continuous feature in the data and the values range from 0.00 to 744.60 with a few missing values.  The first thing we will do is try to fill in the missing fare values.  Let's look at the distribution of fare values based on where the passenger embarked from, the class of ticket they have and whether or not they have a cabin, all things we would assume to impact the fare prices.","c2490652":"## **1.1 NaNs & Sex**\nOne of the first features we'll look at is a simple count of the number of missing values in a row of the data.  We'll aslo take a quick look at the Sex of the passenger which is a dominant indicator of survival.  According to Wikipedia, the phrase \"women and children first\" is a code of conduct dating from 1852, whereby the lives of women and children were to be saved first in a life-threatening situation, typically abandoning ship, when survival resources such as lifeboats were limited.  The phrase is, however, most famously associated with the sinking of RMS Titanic in 1912.  It certainly seems to apply here, although not as much with the children...","0e239fff":"## **1.7 Ticket**\nThe ticket numbers are interesting.  A few have only a text vale, most have only a numeric value, and some have both a text and numeric value.  And we have 9,804 missing values across the train and test datasets.  We will break the ticket value into its text portion and call it \"TicketType\" and the numeric portion which we'll call \"TicketNumber\".  Then we'll bin the TicketType a bit more by just looking at the first letter of the \"Type\" and call that the \"TicketCat.\"  We'll also bin the TicketNumbers based on their length (from 4-9 digits) and call it \"TicketLen.\"","8cb5f4ba":"## **2.1 Cabin Number & Ticket Number**\nA significant amount of the passengers have no Cabin number.  Let's see if there's a relationship between Ticket Numbers and Cabin Numbers.  Maybe we can use Ticket to fill in Cabin."}}