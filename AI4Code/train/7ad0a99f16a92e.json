{"cell_type":{"1687a343":"code","6edf2ebe":"code","35fb341e":"code","2fe58083":"code","2272bb36":"code","0329fdd0":"code","834137ba":"code","9b8432af":"code","5ad0306c":"code","33442179":"code","532dedfd":"code","0bc6a1f2":"code","54c5140b":"code","592a04cf":"code","b1fb4895":"code","f21d5468":"code","2fae9ce2":"markdown","9a0368df":"markdown","0af4b64f":"markdown","a4288752":"markdown","027b2979":"markdown","11a1dae4":"markdown","16a8f555":"markdown","3c18b22a":"markdown","8e66bffd":"markdown","9adff4fb":"markdown","0e41d2dd":"markdown"},"source":{"1687a343":"!pip install -q pyicu\n!pip install -q pycld2\n!pip install -q polyglot\n!pip install -q textstat\n!pip install -q googletrans","6edf2ebe":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport gc\nimport re\nimport folium\nimport textstat\nfrom scipy import stats\nfrom colorama import Fore, Back, Style, init\n\nimport math\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\n\nimport random\nimport networkx as nx\nfrom pandas import Timestamp\n\nfrom PIL import Image\nfrom IPython.display import SVG\nfrom keras.utils import model_to_dot\n\nimport requests\nfrom IPython.display import HTML\n\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\ntqdm.pandas()\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nplt.style.use('ggplot')","35fb341e":"train = pd.read_csv('..\/input\/contradictory-my-dear-watson\/train.csv')\ntrain.head()","2fe58083":"lang_list = sorted(list(set(train[\"language\"])))\ncounts = [list(train[\"language\"]).count(cont) for cont in lang_list]\ndf = pd.DataFrame(np.transpose([lang_list, counts]))\ndf.columns = [\"Language\", \"Count\"]\ndf[\"Count\"] = df[\"Count\"].apply(int)\n\n\nfig = px.bar(df, x=\"Language\", y=\"Count\", title=\"Language of train data\", color=\"Language\", text=\"Count\")\nfig.update_layout(template=\"plotly_white\")\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[0].marker.line.width = 0.5\nfig.data[1].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[1].marker.line.width = 0.5\nfig.data[0].textfont.color = \"black\"\nfig.data[0].textposition = \"outside\"\nfig.data[1].textfont.color = \"black\"\nfig.data[1].textposition = \"outside\"\nfig","2272bb36":"def get_country(language):\n    if language == \"German\":\n        return \"Germany\"\n    if language == \"Bulgarian\":\n        return \"Bulgary\"\n    if language == \"Chinese\":\n        return \"China\"\n    if language == \"Arabic\":\n        return \"Saudi Arabia\"\n    if language == \"Hindi\":\n        return \"India\"\n    if language == \"Thai\":\n        return \"Thailand\"\n    if language == \"Urdu\":\n        return \"Pakistan\"\n    if language == \"Swahili\":\n        return \"Tanzania\"\n    if language == \"English\":\n        return \"United Kingdom\"\n    if language == \"Hindi\":\n        return \"India\"\n    if language == \"French\":\n        return \"France\"\n    if language == \"Greek\":\n        return \"Greece\"\n    if language == \"Spanish\":\n        return \"Spain\"\n    if language == \"Russian\":\n        return \"Russia\"\n    if language == \"Vietnamese\":\n        return \"Vietnam\"\n    return \"None\"\n    \ndf[\"country\"] = df[\"Language\"].progress_apply(get_country)","0329fdd0":"fig = px.choropleth(df.query(\"Language != 'English' and Language != 'un' and country != 'None'\").query(\"Count >= 5\"),\n                    locations=\"country\", hover_name=\"country\",\n                     projection=\"natural earth\", locationmode=\"country names\", title=\"Countries of train\",\n                    color=\"Count\", template=\"plotly\", color_continuous_scale=\"agsunset\")\n# fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n# fig.data[0].marker.line.width = 0.2\nfig.show()","834137ba":"def new_len(x):\n    if type(x) is str:\n        return len(x.split())\n    else:\n        return 0\n\ntrain[\"premise\"] = train[\"premise\"].apply(new_len)\nnums = train.query(\"premise != 0 and premise < 200\").sample(frac=0.1)[\"premise\"]\nfig = ff.create_distplot(hist_data=[nums],\n                         group_labels=[\"All premise\"],\n                         colors=[\"coral\"])\n\nfig.update_layout(title_text=\"Premise words\", xaxis_title=\"Premise words\", template=\"simple_white\", showlegend=False)\nfig.show()","9b8432af":"train[\"hypothesis\"] = train[\"hypothesis\"].apply(new_len)\nnums = train.query(\"hypothesis != 0 and hypothesis < 200\").sample(frac=0.1)[\"hypothesis\"]\nfig = ff.create_distplot(hist_data=[nums],\n                         group_labels=[\"All hypothesis\"],\n                         colors=[\"coral\"])\n\nfig.update_layout(title_text=\"Hypothesis words\", xaxis_title=\"hypothesis words\", template=\"simple_white\", showlegend=False)\nfig.show()","5ad0306c":"df = pd.DataFrame(np.transpose([lang_list, train.groupby(\"language\").mean()[\"hypothesis\"]]))\ndf.columns = [\"Language\", \"Average_comment_words\"]\ndf[\"Average_comment_words\"] = df[\"Average_comment_words\"].apply(float)\ndf = df.query(\"Average_comment_words < 500\")\nfig = go.Figure(go.Bar(x=df[\"Language\"], y=df[\"Average_comment_words\"]))\n\nfig.update_layout(xaxis_title=\"Language\", yaxis_title=\"Average hypothesis words\", title_text=\"Average hypothesis words vs. language\", template=\"plotly_white\")\nfig.show()","33442179":"df = pd.DataFrame(np.transpose([lang_list, train.groupby(\"language\").mean()[\"premise\"]]))\ndf.columns = [\"Language\", \"Average_comment_words\"]\ndf[\"Average_comment_words\"] = df[\"Average_comment_words\"].apply(float)\ndf = df.query(\"Average_comment_words < 500\")\nfig = go.Figure(go.Bar(x=df[\"Language\"], y=df[\"Average_comment_words\"]))\n\nfig.update_layout(xaxis_title=\"Language\", yaxis_title=\"Average premise words\", title_text=\"Average premise words vs. language\", template=\"plotly_white\")\nfig.show()","532dedfd":"fig = go.Figure(data=[\n    go.Pie(labels=train['label'].value_counts().index,\n           values=train['label'].value_counts().values, marker=dict(colors=px.colors.qualitative.Plotly))\n])\nfig.update_traces(textposition='outside', textfont=dict(color=\"black\"))\nfig.update_layout(title_text=\"Pie chart of labels\")\nfig.show()","0bc6a1f2":"per_lang = train.groupby(by=['language', 'label']).count()[['id']]\n\ndata=[]\nfor lang in train['language'].unique():\n      y = per_lang[per_lang.index.get_level_values('language') == lang].values.flatten()\n      data.append(go.Bar(name=lang, x=['entailment', 'contradiction', 'neutral'], y=y))\nfig = go.Figure(data=data)\nfig.update_layout(\n    title='Language distribution in the train dataset',\n    barmode='group'\n)\nfig.show()","54c5140b":"from wordcloud import WordCloud, STOPWORDS\ntrain = pd.read_csv('..\/input\/contradictory-my-dear-watson\/train.csv')\n\nrnd_comments = train[train['label'] == 0]['hypothesis'].values\nwc = WordCloud(background_color=\"black\", max_words=2000)\nwc.generate(\" \".join(rnd_comments))\n\nplt.figure(figsize=(20,10))\nplt.axis(\"off\")\nplt.title(\"Frequent words in premise\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'viridis' , random_state=17), alpha=0.98)\nplt.show()","592a04cf":"from wordcloud import WordCloud, STOPWORDS\n\nrnd_comments = train[train['label'] == 0]['premise'].values\nwc = WordCloud(background_color=\"black\", max_words=2000)\nwc.generate(\" \".join(rnd_comments))\n\nplt.figure(figsize=(20,10))\nplt.axis(\"off\")\nplt.title(\"Frequent words in premise\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'viridis' , random_state=17), alpha=0.98)\nplt.show()","b1fb4895":"from sklearn.feature_extraction.text import CountVectorizer\n# Define helper functions\ndef get_top_n_words(n_top_words, count_vectorizer, text_data):\n    '''\n    returns a tuple of the top n words in a sample and their \n    accompanying counts, given a CountVectorizer object and text sample\n    '''\n    vectorized_headlines = count_vectorizer.fit_transform(train.hypothesis.values)\n    vectorized_total = np.sum(vectorized_headlines, axis=0)\n    word_indices = np.flip(np.argsort(vectorized_total)[0,:], 1)\n    word_values = np.flip(np.sort(vectorized_total)[0,:],1)\n    \n    word_vectors = np.zeros((n_top_words, vectorized_headlines.shape[1]))\n    for i in range(n_top_words):\n        word_vectors[i,word_indices[0,i]] = 1\n\n    words = [word[0].encode('ascii',errors=\"ignore\").decode('utf-8',errors=\"ignore\") for \n             word in count_vectorizer.inverse_transform(word_vectors)]\n\n    return (words, word_values[0,:n_top_words].tolist()[0])","f21d5468":"count_vectorizer = CountVectorizer(stop_words='english')\nwords, word_values = get_top_n_words(n_top_words=25,\n                                     count_vectorizer=count_vectorizer, \n                                     text_data=train.hypothesis.values)\n\nfig, ax = plt.subplots(figsize=(10,4))\nax.bar(range(len(words)), word_values);\nax.set_xticks(range(len(words)));\nax.set_xticklabels(words, rotation='vertical');\nax.set_title('Top words in headlines dataset (excluding stop words)');\nax.set_xlabel('Word');\nax.set_ylabel('Number of occurences');\nplt.show()","2fae9ce2":"## Distribute label of language ","9a0368df":"## Wordcloud of premise ","0af4b64f":"## CountWord ","a4288752":"## Languages","027b2979":"## Average hypothesis words vs Language","11a1dae4":"## Distribution of words","16a8f555":"## Average premise words vs Language","3c18b22a":"## Wordcloud of hypothesis","8e66bffd":"## Pie chart of label","9adff4fb":"## Install package","0e41d2dd":"![watson](https:\/\/i.gr-assets.com\/images\/S\/compressed.photo.goodreads.com\/books\/1399214379l\/21724945.jpg)\nNatural Language Inferencing (NLI) is a popular NLP problem that involves determining how pairs of sentences (consisting of a premise and a hypothesis) are related.\n\nYour task is to create an NLI model that assigns labels of 0, 1, or 2 (corresponding to entailment, neutral, and contradiction) to pairs of premises and hypotheses. To make things more interesting, the train and test set include text in fifteen different languages! You can find more details on the dataset by reviewing the Data page."}}