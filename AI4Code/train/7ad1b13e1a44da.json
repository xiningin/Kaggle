{"cell_type":{"7d60c649":"code","377b5c60":"code","8100ab79":"code","34c830e1":"markdown","d6110dd1":"markdown","650eba10":"markdown","6736ba0c":"markdown"},"source":{"7d60c649":"import numpy as np\n# !pip install pulp\nimport pulp as pl # to compute steady states\nfrom scipy.stats import poisson\nfrom itertools import product","377b5c60":"# we've got four states \/0,1,2,3\/\nP = np.zeros((4,4))\n\n# inventory: 0,      demand: k>=3\nP[0, 0] = 1 - poisson.cdf(2, 1)\n# inventory: 0,      demand: 2\nP[0, 1] = poisson.pmf(2, 1)\n# inventory: 0,      demand: 1\nP[0, 2] = poisson.pmf(1, 1)\n# inventory: 0,      demand: 0\nP[0, 3] = poisson.pmf(0, 1)\n\n# inventory: 1,      demand: k>=1\nP[1, 0] = 1 - poisson.cdf(0, 1)\n# inventory: 1,      demand: 0\nP[1, 1] = poisson.pmf(0, 1)\n\n# inventory: 2,      demand: k>=2\nP[2, 0] = 1 - poisson.cdf(1, 1)\n# inventory: 2,      demand: 1\nP[2, 1] = poisson.pmf(1, 1)\n# inventory: 2,      demand: 0\nP[2, 2] = poisson.pmf(0, 1)\n\n# inventory: 3,      demand: k>=3\nP[3, 0] = 1 - poisson.cdf(2, 1)\n# inventory: 3,      demand: 2\nP[3, 1] = poisson.pmf(2, 1)\n# inventory: 3,      demand: 1\nP[3, 2] = poisson.pmf(1, 1)\n# inventory: 3,      demand: 0\nP[3, 3] = poisson.pmf(0, 1)\n\nprint('Transition Matrix\\n\\n',P)","8100ab79":"def get_steady_state(P):\n    _model = pl.LpProblem(f\"Steady State Probabilities\", pl.LpMaximize)\n    J = P.shape[0]\n    pi = [pl.LpVariable(f'pi_{j}', cat=pl.LpContinuous, lowBound=0) for j in range(J)]\n    _model += pl.lpSum([pi[j] for j in range(J)]) == 1, 'sums2one'\n\n    for j in range(J):\n        _model += pl.lpSum([P[i, j] * pi[i] for i in range(J)] + [-pi[j]]) == 0, f'Constraint_{j}'\n\n    _model.solve()\n    sol = np.array([pi[j].varValue for j in range(J)], dtype=np.float)\n    np.testing.assert_array_almost_equal(np.dot(np.transpose(P), sol), sol, err_msg=\"valid solution\", verbose=True)\n\n    return sol\n    \n    \npi = get_steady_state(P)\nprint('Steady-State Probabilities\\n\\n',pi)","34c830e1":"# Discrete-Time Markov Chains (DTMCs)\n\n## Introduction\n\nSuppose now we take a series of observations of that random variable. A stochastic process is an indexed collection of random variables $\\{X_t\\}$, where $t$ is the index from a given set $T$.\n\nStochastic process:\n* Discrete-time process: possible observations for state of $\\{X_t\\}$ are made at discrete points in time. These states could be finite or infinite but has to be nutually exclusive and exhaustive.\n* Continuous-time process: continuously make observations\n\nThe value of $\\{X_t\\}$ is the characteristic of interest and may follow a certain distribution. For instance, $X_t$ could be inventory level and follow Poisson probability mass function (PMF),\n$$P( X_t =n) = \\frac{e^{ - \\lambda } \\lambda ^n }{n!}, \\qquad n=1,2,...$$\n\n## Markovian Property (memorylessness)\n\"a simple stochastic process in which the distribution of future states depends only on the present state and not on how it arrived in the present state\" [source](https:\/\/mathworld.wolfram.com\/MarkovProcess.html)\n\nA stochastic process {Xt} satisfies the Markovian property if\n$$P(X_{t+1}=j | X_0=k_0, X_1=k_1, \u2026 , X_t-1=k_{t-1}, X_t=i) = P(X_{t+1}=j | X_t=i)\n\\qquad \\forall \\ t = 0, 1, 2, \u2026$$\n\nIt means the state of the system at time t+1 depends only on the state of the system at time $t$.\n\n## Transition Probabilities\nThe conditional probabilities $P(X_{t+1}=j | X_t=i)$ are called the one-step transition probabilitiesand are stationary if,\n$$P(X_{t+1}=j | X_t=i)= P(X_1=j | X_0=i) = p_{ij}$$\n\n\nInterpretation: Transition probabilities are independent of time ($t$)\nThe transition probabilities are usually denoted in a matrix form where columns\/rows correspond to states. $\\sum_i p_{ij}=1$","d6110dd1":"* What does happen as n (number of transitions) gets large?\n* What is the probability of being in any state in long-run?\n\nThese probabilities are called the steady state probabilities\n\n$$\\lim_{n \\to +\\infty} p_{ij}^{(n)}=\\pi_j$$\n\nUseful interpretation is that $\\pi_j$ is the fraction of time the process is in state $j$ (in the long-run)\n\nQuestion: When does this limit exists?\n* Short answer: If the Markov chain is _irreducible ergodic_","650eba10":"### Problem Statement\n\nWe want to have a (s=1,S=3) inventory policy where we order to maintain inventory at level S if it falls bellow s.\nLet rv $X_{t}$ denotes the inventory level. ($X_0=0$ is the initial inventory)\nAssume $D_t$ represents demand for items during time period $t$, are i.i.d, and follow Poisson distribution.\n\n$$\nX_{t+1} = \\begin{cases} \n  max(S-D_{t+1},0), \\quad X_t<s\\\\\n  max(X_t-D_{t+1},0), \\quad \\text{otherwise.}\\\\\n\\end{cases}\n$$","6736ba0c":"#### Definitions\n* Path\n* Reachable (accesible) state\n* A set of states S in a Markov chain is a **closed set** if no state outside of S is reachable from any state in S.\n* Two states i and j are said to **communicate** if j is reachable from i, and i is reachable from j.\n* Class\n* Irreducibility\n* A state i is an **absorbing** state if $p_{ii}=1$\n* A state i is a **transient state** if there exists a state j that is reachable from i, but the state i is not reachable from state j.\n* If a state is not transient, it is called a **recurrent** state.\n* A state i is **periodic** with period $k>1$ if k is the smallest number such that all paths leading from state i back to state i have a length that is a multiple of k. If a recurrent state is not periodic, it is referred to as **aperiodic**.\n* If all states in a chain are recurrent, aperiodic, and communicate with each other, the chain is said to be **ergodic**.\n* Steady-State Probabilities from solving steady-state equations, $\\pi^T P=\\pi^T$\n"}}