{"cell_type":{"fa6e699f":"code","d0180653":"code","12352f78":"code","8e22daab":"code","15026eef":"code","e31eac75":"code","0ec0baa6":"code","cfb99b78":"code","d7402b2b":"code","e4746816":"code","334c9a5c":"code","dd53e616":"code","4f1bf967":"code","1ef81834":"code","2d64ec27":"code","4d44eb3a":"code","2660be0e":"code","40f49f48":"code","660be2b4":"code","b6ae4cab":"code","3e81c71c":"code","22f4feb6":"code","598acffe":"code","6d8c8997":"code","c6679f89":"code","a6b1648a":"code","1c24b857":"code","a0e3702c":"code","ccd8399e":"code","c46e9494":"code","d6851d7a":"code","52b83b79":"code","0a2b9c57":"code","cb35bd41":"markdown","cf386185":"markdown","5cdf9f81":"markdown","fbff65d4":"markdown","eb868351":"markdown","489a5667":"markdown","4e2b1bbe":"markdown","6f341889":"markdown","fa9d6e53":"markdown","8a80b422":"markdown","b672f41e":"markdown","03f48c74":"markdown","6d619ecd":"markdown","0ba546dc":"markdown","b25be7d3":"markdown","3a4040bc":"markdown","d9404c2b":"markdown","21f4b9b5":"markdown"},"source":{"fa6e699f":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport os\nimport cv2\nfrom glob import glob\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\n\nfrom keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import RMSprop,Adam\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nprint(tf.__version__)","d0180653":"BASE_DIR=('..\/input\/chestxraypneumonia\/chest_xray\/')\ntrain_dir=os.path.join(BASE_DIR,'train\/')\nval_dir=os.path.join(BASE_DIR,'val\/')\ntest_dir=os.path.join(BASE_DIR,'test\/')\n\nprint('Number of images in training set = ',str(len(glob(train_dir+'*\/*'))))\nprint('Number of images in validation set = ',str(len(glob(val_dir+'*\/*'))))\nprint('Number of images in testing set = ',str(len(glob(test_dir+'*\/*'))))\n","12352f78":"training_images = tf.io.gfile.glob('..\/input\/chestxraypneumonia\/chest_xray\/train\/*\/*')\nvalidation_images = tf.io.gfile.glob('..\/input\/chestxraypneumonia\/chest_xray\/val\/*\/*')\n\n\ntotal_files = training_images\ntotal_files.extend(validation_images)\nprint(f'Total number of images : training_images + validation_images = {len(total_files)}\\n')\n\n#spliting 80:20\ntrain_images, val_images = train_test_split(total_files, test_size = 0.2)\nprint(f'After division of 80:20')\nprint(f'Total number of training images = {len(train_images)}')\nprint(f'Total number of validation images = {len(val_images)}')","8e22daab":"tf.io.gfile.makedirs('\/kaggle\/working\/val_dataset\/NORMAL\/')\ntf.io.gfile.makedirs('\/kaggle\/working\/val_dataset\/PNEUMONIA\/')\ntf.io.gfile.makedirs('\/kaggle\/working\/train_dataset\/NORMAL\/')\ntf.io.gfile.makedirs('\/kaggle\/working\/train_dataset\/PNEUMONIA\/')","15026eef":"for ele in train_images:\n    parts_of_path = ele.split('\/')\n\n    if 'PNEUMONIA' == parts_of_path[-2]:\n        tf.io.gfile.copy(src = ele, dst = '\/kaggle\/working\/train_dataset\/PNEUMONIA\/' +  parts_of_path[-1])\n    else:\n        tf.io.gfile.copy(src = ele, dst = '\/kaggle\/working\/train_dataset\/NORMAL\/' +  parts_of_path[-1])","e31eac75":"for ele in val_images:\n    parts_of_path = ele.split('\/')\n\n    if 'PNEUMONIA' == parts_of_path[-2]:\n        tf.io.gfile.copy(src = ele, dst = '\/kaggle\/working\/val_dataset\/PNEUMONIA\/' +  parts_of_path[-1])\n    else:\n        tf.io.gfile.copy(src = ele, dst = '\/kaggle\/working\/val_dataset\/NORMAL\/' +  parts_of_path[-1])","0ec0baa6":"print('Pneumonia x-ray images in training set after split = ',len(os.listdir('\/kaggle\/working\/train_dataset\/PNEUMONIA\/')))\nprint('Normal x-ray images in training set after split = ',len(os.listdir('\/kaggle\/working\/train_dataset\/NORMAL\/')))\nprint('Pneumonia x-ray images in validation set after split = ',len(os.listdir('\/kaggle\/working\/val_dataset\/PNEUMONIA\/')))\nprint('Normal x-ray images in validation set after split = ',len(os.listdir('\/kaggle\/working\/val_dataset\/NORMAL\/')))\nprint('Pneumonia x-ray images in test set = ',len(os.listdir('..\/input\/chestxraypneumonia\/chest_xray\/test\/PNEUMONIA\/')))\nprint('Normal x-ray images in test set = ',len(os.listdir('..\/input\/chestxraypneumonia\/chest_xray\/test\/NORMAL')))\n","cfb99b78":"train_dir='\/kaggle\/working\/train_dataset\/'\nval_dir='\/kaggle\/working\/val_dataset\/'\ntest_dir='..\/input\/chestxraypneumonia\/chest_xray\/test\/'\n\ntrain_normal_dir='\/kaggle\/working\/train_dataset\/NORMAL'\ntrain_pneumonia_dir='\/kaggle\/working\/train_dataset\/PNEUMONIA'\nval_normal_dir='\/kaggle\/working\/val_dataset\/NORMAL'\nval_pneumonia_dir='\/kaggle\/working\/val_dataset\/PNEUMONIA'\n","d7402b2b":"train_normal_fnames=os.listdir(train_normal_dir)\ntrain_pneumonia_fnames=os.listdir(train_pneumonia_dir)\n\nprint(train_normal_fnames[:10])\nprint(train_pneumonia_fnames[:10])\n","e4746816":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nno_cols=4\nno_rows=4\n\npic_index=0","334c9a5c":"fig=plt.gcf()\nfig.set_size_inches(no_cols*4,no_rows*4)\n\npic_index+=8\n\nnormal_pix=[os.path.join(train_normal_dir,fname) for fname in train_normal_fnames[pic_index-8:pic_index]]\npneumonia_pix=[os.path.join(train_pneumonia_dir,fname) for fname in train_pneumonia_fnames[pic_index-8:pic_index]]\n\nfor i,img_path in enumerate(normal_pix+pneumonia_pix):\n    sp=plt.subplot(no_rows,no_cols,i+1)\n    sp.axis()\n    \n    img=mpimg.imread(img_path)\n    plt.imshow(img,cmap='gray')\n    \nplt.show()\n\n# first 8 images are normal x-ray images and next 8 images are pnemonia x-ray images","dd53e616":"model=tf.keras.models.Sequential([\n    # This is the first convolution\n    tf.keras.layers.Conv2D(16, (3,3), activation='relu',padding='same', input_shape=(180, 180, 1)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    # The second convolution\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu',padding='same'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    # The third convolution\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu',padding='same'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    # The fourth convolution\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu',padding='same'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    # The fifth convolution\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu',padding='same'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    # Flatten the results to feed into a DNN\n    tf.keras.layers.Flatten(),\n    # 512 neuron hidden layer\n    tf.keras.layers.Dense(512, activation='relu'),\n    # Only 1 output neuron. It will contain a value from 0-1 \n    tf.keras.layers.Dense(1, activation='sigmoid')\n])","4f1bf967":"model.compile(optimizer=RMSprop(lr=0.001),\n              loss='binary_crossentropy',\n              metrics=['accuracy','Precision','Recall'])","1ef81834":"model.summary()\n","2d64ec27":"\n\ntrain_datagen=ImageDataGenerator(rescale=1.0\/255,\n                                 rotation_range=30,\n                                 width_shift_range=0.2,\n                                 height_shift_range=0.2,\n                                 zoom_range=0.2,\n                                 )\n\nval_datagen=ImageDataGenerator(rescale=1.0\/255)\n\ntest_datagen=ImageDataGenerator(rescale=1.0\/255)\n\ntrain_generator=train_datagen.flow_from_directory(train_dir,color_mode=\"grayscale\",target_size=(180,180),batch_size=128,class_mode='binary')\n\nval_generator=val_datagen.flow_from_directory(val_dir,color_mode=\"grayscale\",target_size=(180,180),batch_size=128,class_mode='binary')\n\ntest_generator=test_datagen.flow_from_directory(test_dir,color_mode=\"grayscale\",target_size=(180,180),batch_size=128,class_mode='binary')\n","4d44eb3a":"history=model.fit(train_generator,validation_data=val_generator,epochs=30,verbose=2)","2660be0e":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\ntrain_precision=history.history['precision']\nval_precision=history.history['val_precision']\n\ntrain_recall=history.history['recall']\nval_recall=history.history['val_recall']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, train_precision, 'r', label='Training precision')\nplt.plot(epochs, val_precision, 'b', label='Validation precision')\nplt.title('Training and validation precision')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, train_recall, 'r', label='Training recall')\nplt.plot(epochs, val_recall, 'b', label='Validation recall')\nplt.title('Training and validation recall')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","40f49f48":"eval_result1 = model.evaluate_generator(test_generator, 624)\nprint('loss  :', eval_result1[0])\nprint('accuracy  :', eval_result1[1])\nprint('Precision :', eval_result1[2])\nprint('Recall :', eval_result1[3])","660be2b4":"train_datagen2=ImageDataGenerator(rescale=1.0\/255,\n                                 rotation_range=30,\n                                 width_shift_range=0.2,\n                                 height_shift_range=0.2,\n                                 zoom_range=0.2,\n                                 )\n\nval_datagen2=ImageDataGenerator(rescale=1.0\/255)\n\ntest_datagen2=ImageDataGenerator(rescale=1.0\/255)\n\ntrain_generator2=train_datagen2.flow_from_directory(train_dir,target_size=(180,180),batch_size=128,class_mode='binary')\n\nval_generator2=val_datagen2.flow_from_directory(val_dir,target_size=(180,180),batch_size=128,class_mode='binary')\n\ntest_generator2=test_datagen2.flow_from_directory(test_dir,target_size=(180,180),batch_size=128,class_mode='binary')\n","b6ae4cab":"from tensorflow.keras.applications.resnet50 import ResNet50","3e81c71c":"pretrained_model2 = ResNet50(weights= 'imagenet', include_top=False, input_shape= (180,180,3))\n\n#freazing the trained layers\nfor layers in pretrained_model2.layers:\n    layers.trainable = False\n#pretrained_model3.summary()","22f4feb6":"last_layer=pretrained_model2.get_layer('conv5_block3_1_relu')\nlast_output = last_layer.output\n\nx=tf.keras.layers.Flatten()(last_output)\nx=tf.keras.layers.Dense(1024,activation='relu')(x)\nx=tf.keras.layers.Dropout(0.2)(x)\nx=tf.keras.layers.Dense(256,activation='relu')(x)\nx=tf.keras.layers.Dropout(0.2)(x)\nx=tf.keras.layers.Dense(1,activation='sigmoid')(x)\n\nmodel2=tf.keras.Model(pretrained_model2.input,x)\n\nmodel2.compile(optimizer=RMSprop(lr=0.001),\n              loss='binary_crossentropy',\n               metrics=['accuracy','Precision','Recall'])\n\n#model3.summary()","598acffe":"history2=model2.fit(train_generator2,validation_data=val_generator2,epochs=30,verbose=2)\n","6d8c8997":"acc2 = history2.history['accuracy']\nval_acc2 = history2.history['val_accuracy']\n\ntrain_precision2=history2.history['precision']\nval_precision2=history2.history['val_precision']\n\ntrain_recall2=history2.history['recall']\nval_recall2=history2.history['val_recall']\n\nloss2 = history2.history['loss']\nval_loss2 = history2.history['val_loss']\nepochs = range(len(acc2))\n\nplt.plot(epochs, acc2, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc2, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(epochs, train_precision2, 'r', label='Training precision')\nplt.plot(epochs, val_precision2, 'b', label='Validation precision')\nplt.title('Training and validation precision')\nplt.legend()\nplt.show()\n\nplt.plot(epochs, train_recall2, 'r', label='Training recall')\nplt.plot(epochs, val_recall2, 'b', label='Validation recall')\nplt.title('Training and validation recall')\nplt.legend()\nplt.show()\n\nplt.plot(epochs, loss2, 'r', label='Training Loss')\nplt.plot(epochs, val_loss2, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","c6679f89":"eval_result2 = model2.evaluate_generator(test_generator2, 624)\nprint('loss  :', eval_result2[0])\nprint('accuracy  :', eval_result2[1])\nprint('Precision :', eval_result2[2])\nprint('Recall :', eval_result2[3])","a6b1648a":"from tensorflow.keras.applications.inception_v3 import InceptionV3","1c24b857":"pretrained_model3=InceptionV3(input_shape=(180,180,3),\n                             include_top=False,\n                             weights='imagenet')\n#freazing the trained layers\nfor layers in pretrained_model3.layers:\n    layers.trainable=False\n    ","a0e3702c":"last_layer=pretrained_model3.get_layer('mixed10')\nlast_output = last_layer.output","ccd8399e":"x=tf.keras.layers.Flatten()(last_output)\nx=tf.keras.layers.Dense(1024,activation='relu')(x)\nx=tf.keras.layers.Dropout(0.2)(x)\nx=tf.keras.layers.Dense(256,activation='relu')(x)\nx=tf.keras.layers.Dropout(0.2)(x)\nx=tf.keras.layers.Dense(1,activation='sigmoid')(x)\n\nmodel3=tf.keras.Model(pretrained_model3.input,x)\n\nmodel3.compile(optimizer=RMSprop(lr=0.001),\n              loss='binary_crossentropy',\n               metrics=['accuracy','Precision','Recall'])\n# model4.summary()\n","c46e9494":"history3=model3.fit(train_generator2,validation_data=val_generator2,epochs=30,verbose=2)","d6851d7a":"acc3 = history3.history['accuracy']\nval_acc3 = history3.history['val_accuracy']\n\ntrain_precision3=history3.history['precision']\nval_precision3=history3.history['val_precision']\n\ntrain_recall3=history3.history['recall']\nval_recall3=history3.history['val_recall']\n\nloss3 = history3.history['loss']\nval_loss3 = history3.history['val_loss']\nepochs = range(len(acc3))\n\nplt.plot(epochs, acc3, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc3, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(epochs, train_precision3, 'r', label='Training precision')\nplt.plot(epochs, val_precision3, 'b', label='Validation precision')\nplt.title('Training and validation precision')\nplt.legend()\nplt.show()\n\nplt.plot(epochs, train_recall3, 'r', label='Training recall')\nplt.plot(epochs, val_recall3, 'b', label='Validation recall')\nplt.title('Training and validation recall')\nplt.legend()\nplt.show()\n\nplt.plot(epochs, loss3, 'r', label='Training Loss')\nplt.plot(epochs, val_loss3, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","52b83b79":"eval_result3 = model3.evaluate_generator(test_generator2, 624)\nprint('loss  :', eval_result3[0])\nprint('accuracy  :', eval_result3[1])\nprint('Precision :', eval_result3[2])\nprint('Recall :', eval_result3[3])","0a2b9c57":"# model.save('PNP.h5')","cb35bd41":"# Model 1\n### First model which we are going to train is a simple CNN model.","cf386185":"## Copying the images in new directories ","5cdf9f81":"# Importing Libraries","fbff65d4":"### So let's train models to daignose chest x-ray image, if the patient has pneumonia or not","eb868351":"## Importing and loading pretrained ResNet50 model","489a5667":"## Having a look over the dataset after the split","4e2b1bbe":"# Loading the data\n* Gven dataset has training,validation and test set, but validation set has only 16 images whereas training set has 5216 images.\n* So first we need to create a proper distribution set with 80% as training data and 20% as validation data.\n* So we'll merge training and validation set and then split them in the ratio of 80:20 repectively.\n","6f341889":"# Model 2 (ResNet-50)\n * We'll user pre-trained model provided by keras and add some layers on the top.\n * The pre-trained ResNet model in keras takes in input of exactly three input channel, but our input image is of grayscale.\n * So inorder to avoid mismatch of shape we'll let our Image data generator use the default color_mode ie rgb instead of specifying it to be gray_scale.","fa9d6e53":"## Overview of model","8a80b422":"## Setting the path of training directory and validation directory","b672f41e":"### If you found this notebook helpful please upvote and suggest changes if any.","03f48c74":"### Creating data generators \n","6d619ecd":"### Importing and loading pretrained Inception model","0ba546dc":"# What is Pnemonia?\nPneumonia is an infection that inflames the air sacs in one or both lungs. The air sacs may fill with fluid or pus (purulent material), causing cough with phlegm or pus, fever, chills, and difficulty breathing. A variety of organisms, including bacteria, viruses and fungi, can cause pneumonia.\n\nPneumonia can range in seriousness from mild to life-threatening. It is most serious for infants and young children, people older than age 65, and people with health problems or weakened immune systems.","b25be7d3":"# Comparing the models\n* As this is the case of medical diagnosis accuracy cannot be the oly metric to evaluate.\n* In medical daignosis it is very important to correctly predict the true values.\n* We cannot incorrectly diagnose a patient as normal event after the true report of diagnosis shows that patient has pnemonia.\n* So along with higher accuracy we need higher recall.\n\n### Model 1 performed best with accuracy=0.8766 and recall=0.9949 on test set so we'll save this model","3a4040bc":"## Visualizing pneumonia x-ray images and normal x-ray images","d9404c2b":" # Model 3 (Inception Model)\n \n * We'll user pre-trained model provided by keras and add some layers on the top.\n * The pre-trained Inception model in keras takes in input of exactly three input channel, but our input image is of grayscale.\n * So inorder to avoid mismatch of shape we'll let our Image data generator use the default color_mode ie rgb instead of specifying it to be gray_scale.\n * So we'll use same data generator as used in ResNet50 model","21f4b9b5":"## Making new directories for training set and validation set"}}