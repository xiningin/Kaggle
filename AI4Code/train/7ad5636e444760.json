{"cell_type":{"d156fbad":"code","7a8539bb":"code","0ef66213":"code","90fffda6":"code","591a6f0c":"code","13ecba65":"code","e32d1e74":"code","08990f2e":"code","7e93ee81":"code","c15a3bfa":"code","666c12ce":"code","724f45d9":"code","09587868":"code","ace0e6e2":"code","120ed944":"code","0c8b2036":"code","de3d4d03":"code","c06fd076":"code","9288485d":"code","c38e6b2d":"code","4fee84b2":"code","b56ca3f2":"code","13b2b8f6":"code","bd3d1ff6":"code","2caed591":"code","a1dba509":"code","b0bd851a":"code","81f4fa8c":"code","4fad725a":"code","c06e5262":"markdown","cf491a75":"markdown","6865fb4b":"markdown","f9d80d5c":"markdown","902bdb32":"markdown","67ad93dc":"markdown","0754d529":"markdown","26f461de":"markdown","98643351":"markdown","5fb85d54":"markdown","eaf0f9ad":"markdown","ba199d3c":"markdown","e67c6d0d":"markdown","94a6771b":"markdown","c7b85fd5":"markdown","93bf9bf5":"markdown","11022467":"markdown","76ee93d0":"markdown","56edd9f6":"markdown","1a23f673":"markdown","35b244c0":"markdown"},"source":{"d156fbad":"import os\nimport sys\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline\n\nimport missingno as msno\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport xgboost as xgb\n\nfrom tqdm import tqdm_notebook\nfrom sklearn.metrics import roc_auc_score,roc_curve,auc,confusion_matrix\nfrom sklearn.model_selection import RandomizedSearchCV\nimport gc\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.filterwarnings('ignore')","7a8539bb":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","0ef66213":"train_identity = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_identity.csv\")\ntrain_transaction = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_transaction.csv\")\n\ntest_identity = pd.read_csv(\"..\/input\/ieee-fraud-detection\/test_identity.csv\")\ntest_transaction = pd.read_csv(\"..\/input\/ieee-fraud-detection\/test_transaction.csv\")\n\ntrain_identity = reduce_mem_usage(train_identity)\ntrain_transaction = reduce_mem_usage(train_transaction)\ntest_identity = reduce_mem_usage(test_identity)\ntest_transaction = reduce_mem_usage(test_transaction)","90fffda6":"print(train_transaction.info(), \"\\n\")\nprint(train_identity.info(), \"\\n\")\nprint(test_transaction.info(), \"\\n\")\nprint(test_identity.info(), \"\\n\")","591a6f0c":"def differentcolumns(traincols, testcols):\n    for i in traincols:\n        if i not in testcols:\n            print(i)\n            \ndifferentcolumns(train_identity.columns, test_identity.columns) ","13ecba65":"test_identity = test_identity.rename(columns={\"id-01\": \"id_01\", \"id-02\": \"id_02\", \"id-03\": \"id_03\",\n                             \"id-06\": \"id_06\", \"id-05\": \"id_05\", \"id-04\": \"id_04\",\n                             \"id-07\": \"id_07\", \"id-08\": \"id_08\", \"id-09\": \"id_09\",\n                             \"id-10\": \"id_10\", \"id-11\": \"id_11\", \"id-12\": \"id_12\",\n                             \"id-15\": \"id_15\", \"id-14\": \"id_14\", \"id-13\": \"id_13\",\n                             \"id-16\": \"id_16\", \"id-17\": \"id_17\", \"id-18\": \"id_18\",\n                             \"id-21\": \"id_21\", \"id-20\": \"id_20\", \"id-19\": \"id_19\",\n                             \"id-22\": \"id_22\", \"id-23\": \"id_23\", \"id-24\": \"id_24\",\n                             \"id-27\": \"id_27\", \"id-26\": \"id_26\", \"id-25\": \"id_25\",\n                             \"id-28\": \"id_28\", \"id-29\": \"id_29\", \"id-30\": \"id_30\",\n                             \"id-31\": \"id_31\", \"id-32\": \"id_32\", \"id-33\": \"id_33\",\n                             \"id-34\": \"id_34\", \"id-35\": \"id_35\", \"id-36\": \"id_36\",\n                             \"id-37\": \"id_37\", \"id-38\": \"id_38\"})\ntest_identity.head() ","e32d1e74":"differentcolumns(train_identity.columns, test_identity.columns) ","08990f2e":"train = pd.merge(train_transaction, train_identity, on = 'TransactionID', how = 'left')\ntest = pd.merge(test_transaction, test_identity, on = 'TransactionID', how = 'left')","7e93ee81":"del train_identity, train_transaction, test_identity, test_transaction","c15a3bfa":"#train.info(verbose=True, null_counts=True)\ntrain.describe()","666c12ce":"def getnulls(data):\n    total = data.isnull().sum().sort_values(ascending=False)\n    percent = (data.isnull().sum() \/ data.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['total', 'percent'])\n    return missing_data","724f45d9":"#Train data missing values\nmissing_data_train = getnulls(train)\nmissing_data_train.head(350).T ","09587868":"# Test Data Missing Values\nmissing_data_test = getnulls(test)\nmissing_data_test.head(350).T ","ace0e6e2":"def top_missing_cols(df,n=10,thresh=80):\n    \"\"\"\n    returns missing columns in dataframe with missing values percent > thresh\n    if n=None. It will gave whole dataframe with missing values percent > thresh\n    \"\"\"\n    \n    dff = (df.isnull().sum()\/df.shape[0])*100\n    dff = dff.reset_index()\n    dff.columns = ['col','missing_percent']\n    dff = dff.sort_values(by=['missing_percent'],ascending=False).reset_index(drop=True)\n    print(f'There are {df.isnull().any().sum()} columns in this dataset with missing values.')\n    print(f'There are {dff[dff[\"missing_percent\"] > thresh].shape[0]} columns with missing percent values than {thresh}%')\n    if n:\n        return dff.head(n)\n    else:\n        return dff","120ed944":"df_missing = top_missing_cols(train,n=None,thresh=50)\n# Taking all column with missing percentage over the value of 50%\nmissing_cols = df_missing['col']","0c8b2036":"# I will take all columns and group them based on missing percentage\nnan_dict = {}\nfor col in missing_cols:\n    count = train[col].isnull().sum()\n    try:\n        nan_dict[count].append(col)\n    except:\n        nan_dict[count] = [col]\n        \nfor k,v in nan_dict.items():\n    print(f'#####' * 4)\n    print(f'NAN count = {k} percent: {(int(k)\/train.shape[0])*100} %')\n    print(v)","de3d4d03":"def coorelation_analysis(cols,title='Coorelation Analysis',size=(12,12)):\n    cols = sorted(cols)\n    fig,axes = plt.subplots(1,1,figsize=size)\n    df_corr = train[cols].corr()\n    sns.heatmap(df_corr,annot=True,cmap='RdBu_r')\n    axes.title.set_text(title)\n    plt.show()","c06fd076":"def reduce_groups(grps):\n    '''\n    determining columns that have more unique values among a group of atttributes\n    '''\n    use = []\n    for col in grps:\n        max_unique = 0\n        max_index = 0\n        for i,c in enumerate(col):\n            n = train[c].nunique()\n            if n > max_unique:\n                max_unique = n\n                max_index = i\n        use.append(col[max_index])\n    return use","9288485d":"# column details\ncat_cols = (['ProductCD'] + \n            ['card%d' % i for i in range(1, 7)] + \n            ['addr1', 'addr2', 'P_emaildomain', 'R_emaildomain'] + \n            ['M%d' % i for i in range(1, 10)] + \n            ['DeviceType', 'DeviceInfo'] +\n            ['id_%d' % i for i in range(12, 39)])\n\n\ntype_map = {c: str for c in cat_cols}\ntrain[cat_cols] = train[cat_cols].astype(type_map, copy=False)\ntest[cat_cols] = test[cat_cols].astype(type_map, copy=False)\n\n######################################################################################\n\nid_cols = ['TransactionID', 'TransactionDT']\ntarget = 'isFraud'\n\nnumeric_cols =  [\n    'TransactionAmt', 'dist1', 'dist2', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', \n    'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', \n    'D12', 'D13', 'D14', 'D15', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', \n    'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', \n    'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', \n    'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', \n    'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', \n    'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', \n    'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', \n    'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', \n    'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', \n    'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', \n    'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', \n    'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', \n    'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', \n    'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', \n    'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', \n    'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', \n    'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', \n    'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', \n    'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', \n    'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', \n    'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', \n    'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', \n    'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', \n    'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', \n    'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', \n    'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', \n    'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', \n    'V337', 'V338', 'V339', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', \n    'id_09', 'id_10', 'id_11'\n]\n\n\nreduced_vcols = ['V1', 'V3', 'V4', 'V6', 'V8', 'V11', 'V13', 'V14', 'V17', 'V20', \n 'V23', 'V26', 'V27', 'V30', 'V36', 'V37', 'V40', 'V41', 'V44', 'V47', 'V48', 'V54', 'V56', 'V59', \n 'V62', 'V65', 'V67', 'V68', 'V70', 'V76', 'V78', 'V80', 'V82', 'V86', 'V88', 'V89', 'V91', 'V96', \n 'V98', 'V99', 'V104', 'V107', 'V108', 'V111', 'V115', 'V117', 'V120', 'V121', 'V123', 'V124', 'V127', \n 'V129', 'V130', 'V136', 'V138', 'V139', 'V142', 'V147', 'V156', 'V162', 'V165', 'V160', 'V166', 'V178',\n 'V176', 'V173', 'V182', 'V187', 'V203', 'V205', 'V207', 'V215', 'V169', 'V171', 'V175', 'V180', 'V185', \n 'V188', 'V198', 'V210', 'V209', 'V218', 'V223', 'V224', 'V226', 'V228', 'V229', 'V235', 'V240', 'V258', \n 'V257', 'V253', 'V252', 'V260', 'V261', 'V264', 'V266', 'V267', 'V274', 'V277', 'V220', 'V221', 'V234', \n 'V238', 'V250', 'V271', 'V294', 'V284', 'V285', 'V286', 'V291',\n 'V297', 'V303', 'V305', 'V307', 'V309', 'V310', 'V320', 'V281', 'V283', 'V289', 'V296', 'V301', 'V314', 'V332', 'V325', 'V335', 'V338']","c38e6b2d":"# droping v cols as they are highly correlated \ndrop_cols = [col for col in train.columns if col[0] == 'V' and col not in reduced_vcols]\n\nprint(f'dropping {len(drop_cols)} columns')\ntrain = train.drop(columns=drop_cols)\ntest = test.drop(columns=drop_cols)","4fee84b2":"## Train and test split\u00b6\ny_train = train['isFraud']\nX_train = train.drop(columns=['isFraud'])\nX_test = test.copy()\n\nprint(X_train.shape)\nprint(X_test.shape)\ngc.collect()","b56ca3f2":"# Label encoding all categorical features\nfor col in X_train.columns:\n    \n    if col in cat_cols:\n        # label encode all categorical columns\n        dff = pd.concat([X_train[col],X_test[col]])\n        dff,_ = pd.factorize(dff,sort=True)\n        if dff.max()>32000: \n            print(col,'needs int32 datatype')\n            \n        X_train[col] = dff[:len(X_train)].astype('int16')\n        X_test[col] = dff[len(X_train):].astype('int16')","13b2b8f6":"rem_cols = []\nrem_cols.extend(['TransactionDT','TransactionID'])\n\ncols = [col for col in X_train.columns if col not in rem_cols]\nlen(cols)","bd3d1ff6":"# Scaling numeric features\nfor col in cols:\n    if col not in cat_cols:\n        # min max scalar\n        dff = pd.concat([X_train[col],X_test[col]])\n        dff = (dff - dff.min())\/(dff.max() - dff.min())\n        dff.fillna(-1,inplace=True)\n\n        X_train[col] = dff[:len(X_train)]\n        X_test[col] = dff[len(X_train):]\n\ndel dff","2caed591":"gc.collect()","a1dba509":"x_train = X_train[cols]\nx_test = X_test[cols]\n\n\nidx_train = x_train.index[:int(x_train.shape[0]*0.75)]  \nidx_validation = x_train.index[int(x_train.shape[0]*0.75):]\n    \nprint(f'fitting model on {len(cols)} columns')\nclf = xgb.XGBClassifier( \n        n_estimators=2000,\n        max_depth=12, \n        learning_rate=0.02, \n        subsample=0.8,\n        colsample_bytree=0.4, \n        missing=-1, \n        eval_metric='auc',\n        tree_method='gpu_hist' \n          )\nmodel = clf.fit(x_train.loc[idx_train,cols], y_train[idx_train], \n            eval_set=[(x_train.loc[idx_validation,cols],y_train[idx_validation])],\n            verbose=50, early_stopping_rounds=100)","b0bd851a":"y_train_pred = model.predict(x_train.iloc[idx_train])\ny_test_pred = model.predict(x_train.iloc[idx_validation])\n\ntrain_fpr, train_tpr, thresholds = roc_curve(y_train.iloc[idx_train], model.predict_proba(x_train.iloc[idx_train])[:,1])\ntest_fpr, test_tpr, thresholds = roc_curve(y_train.iloc[idx_validation], model.predict_proba(x_train.iloc[idx_validation])[:,1])\n\n#Area under ROC curve\nprint('Area under train roc {}'.format(auc(train_fpr, train_tpr)))\nprint('Area under test roc {}'.format(auc(test_fpr, test_tpr)))","81f4fa8c":"train_cf = confusion_matrix(y_train.iloc[idx_train],y_train_pred)\nplt.figure(figsize=(7,5))\nsns.heatmap(train_cf,annot=True,annot_kws={\"size\": 16},fmt=\"0\")\nplt.title('Train confusion matrix')\nplt.show()\n\n\ncv_cf = confusion_matrix(y_train.iloc[idx_validation],y_test_pred)\nplt.figure(figsize=(7,5))\nsns.heatmap(cv_cf,annot=True,annot_kws={\"size\": 16},fmt=\"0\")\nplt.title('Test confusion matrix')\nplt.show()","4fad725a":"y_pred_test = model.predict_proba(x_test)[:,1]\nsubmission = pd.read_csv('..\/input\/ieee-fraud-detection\/sample_submission.csv')\nsubmission['isFraud'] = y_pred_test\nsubmission.to_csv('..\/submission_xgboost_gpu.csv',index=False)","c06e5262":"## 5.2. Looking at Missing Data","cf491a75":"# 8. Feature Encoding","6865fb4b":"# 11. Creating Submission File","f9d80d5c":"# 10. Confusion Matrix After Training","902bdb32":"The dataset description below is provided by the competition host [here](https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/101203#latest-605862).\n\n### Transaction Table\n* TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n* TransactionAMT: transaction payment amount in USD\n* ProductCD: product code, the product for each transaction\n* card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n* addr: address\n* dist: distance\n* P_ and (R__) emaildomain: purchaser and recipient email domain\n* C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n* D1-D15: timedelta, such as days between previous transaction, etc.\n* M1-M9: match, such as names on card and address, etc.\n* Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n\nCategorical Features:\n* ProductCD\n* card1 - card6\n* addr1, addr2\n* P_emaildomain\n* R_emaildomain\n* M1 - M9\n\n### Identity Table\n\nVariables in this table are identity information \u2013 network connection information (IP, ISP, Proxy, etc) and digital signature (UA\/browser\/os\/version, etc) associated with transactions.\nThey're collected by Vesta\u2019s fraud protection system and digital security partners.\n(The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n\nCategorical Features:\n* DeviceType\n* DeviceInfo\n* id_12 - id_38","67ad93dc":"# 5. Data Engineering\n\n## 5.1. Different Data Types","0754d529":"### My Submission:\n\n* Private Score: **0.899958**\n\n* Public Score: **0.922061**","26f461de":"# 7. Data Splitting for Training","98643351":"We have lot of NAN values present in our data. Taking all columns is unneccessary. Removing columns with high coorelation makes our model more fast and accurate.\n\n### How to reduce number of columns based on number of NaN values and coorelation:\nGroup columns based on number of missing values Eg. if there are 4 columsn v1,v2,v3 and v4. If v1 and v3 have 56 missing values and v2 have 21 and v4 have 5 missing values, we have 3 groups ['v1','v3'] , ['v2'] and ['v4']\nFor each group: 2.1] For each column in that group find the coorelation with other columns and take only columns with coorelation coeffcient > 0.75\n\n2.2] Take the largest list with common elemnts as a subgroup.Each group contains several subgroups\nFor eg: if we have [[v1,v2],[v6],[v1,v4,v2,v5],[v5,v4]] ,our output will be [[v1,v2,v4,v5],[v6]]\n\n2.3] Now from each subgroups choose the column with most number of unique values.\n\nFor eg, in subgroup [v1,v2,v4,v5], let v2 have most unique values so our output becomes\n[v2,v6]","5fb85d54":"References:\n1. https:\/\/www.kaggle.com\/kabure\/almost-complete-feature-engineering-ieee-data\n2. https:\/\/www.kaggle.com\/artgor\/eda-and-models","eaf0f9ad":"# 2. Importing Data and Reducing Memory Usage","ba199d3c":"### Future Improvements:\n1. Handling Target Class Imbalance\n2. More EDA\n3. More In-depth Feature Engineering\n4. Application of Ensemble Method","e67c6d0d":"*This is my first submission in a Kaggle competition. Let me know about your suggestions on improving this notebook. Thank you.*","94a6771b":"# 9. Training","c7b85fd5":"### Using garbage collector to clear memory","93bf9bf5":"The below function is adapted from [Konstantin](https:\/\/www.kaggle.com\/kyakovlev\/ieee-data-minification)'s approach.","11022467":"# 3. Solving Column Mismatch","76ee93d0":"# 4. Joining Transaction and Identity Data:","56edd9f6":"Function for counting missing values","1a23f673":"# 1. Importing Packages and Libraries","35b244c0":"# 6. Feature  Engineering"}}