{"cell_type":{"88237c09":"code","9360f4b5":"code","6a68b3c5":"code","ef136a12":"code","d947897b":"code","c725c402":"code","b6b4eb23":"code","f46cc154":"code","539ad945":"code","3af1d975":"code","649edfd3":"code","de6d12d9":"code","c3f35806":"code","97770d70":"code","f567784c":"code","8bca4f7c":"code","04b6d3be":"code","5149651f":"code","32a495be":"code","0dd2628f":"code","9ac11ed6":"markdown","4d73f6a4":"markdown","a4ee3065":"markdown","1ee6d81f":"markdown","0597074f":"markdown","c68efd74":"markdown","18616341":"markdown","d75aa0f3":"markdown","a7bfeb59":"markdown","d61d38c1":"markdown","7877cd32":"markdown","b3ed582e":"markdown","7dc4eeaa":"markdown","a6b2382b":"markdown","b40129cf":"markdown","c76840a3":"markdown","71d69759":"markdown","6848d976":"markdown","c420dd79":"markdown","0e76f17d":"markdown","898b713b":"markdown","24beed05":"markdown","1366f16f":"markdown","b1cbda5d":"markdown","5725f744":"markdown","05390047":"markdown","03ef647f":"markdown","d97d7ef6":"markdown","683b417c":"markdown","deba9f73":"markdown","830d147d":"markdown","590e7b23":"markdown","50bd7368":"markdown","a98ce308":"markdown","3144812f":"markdown","6e5e2783":"markdown","6b974fc1":"markdown","ebcfa035":"markdown","6120a3e3":"markdown","4295f2ba":"markdown","919c89fe":"markdown","483ebba0":"markdown","71f76bb8":"markdown","f2674469":"markdown","eb487840":"markdown","a79967bc":"markdown"},"source":{"88237c09":"### Download data from google drive. You need not mess with this code.\n\nimport requests\n\ndef download_file_from_google_drive(id, destination):\n    URL = \"https:\/\/docs.google.com\/uc?export=download\"\n\n    session = requests.Session()\n\n    response = session.get(URL, params = { 'id' : id }, stream = True)\n    token = get_confirm_token(response)\n\n    if token:\n        params = { 'id' : id, 'confirm' : token }\n        response = session.get(URL, params = params, stream = True)\n\n    save_response_content(response, destination)    \n\ndef get_confirm_token(response):\n    for key, value in response.cookies.items():\n        if key.startswith('download_warning'):\n            return value\n\n    return None\n\ndef save_response_content(response, destination):\n    CHUNK_SIZE = 32768\n\n    with open(destination, \"wb\") as f:\n        for chunk in response.iter_content(CHUNK_SIZE):\n            if chunk: # filter out keep-alive new chunks\n                f.write(chunk)\n                \nif __name__ == \"__main__\":\n    file_id = '1e_Azf9zGvSWsDhM9PP2sfMNKC72-iWAK'\n    destination = 'data.txt'\n    download_file_from_google_drive(file_id, destination)","9360f4b5":"with open('data.txt', 'r') as f:\n  data_raw = f.readlines()","6a68b3c5":"def first_five_in_list(l):\n  \"\"\"\n  Inputs: \n  l: Python list\n\n  Outputs:\n  l_5 : python list, first five elements of list if length of list greater than 5; None otherwise\n  \"\"\"\n  l_5 = [] \n  if len(l) >= 5:\n    for i in range(0,5):\n      l_5.append(l[i])\n    return l_5\n  else:\n    return None","ef136a12":"def remove_trailing_newlines(s):\n  \"\"\"\n  Function that removes all trailing newlines at the end of it\n  Inputs:\n    s : string\n\n  Outputs:\n    s_clean : string, string s but without newline characters at the end \n  \"\"\"\n  s_clean = s.strip('\\n')\n  return s_clean","d947897b":"def mapl(f, l):\n  \"\"\"\n  Function that applies f over all elements of l\n  Inputs:\n    f : function, f takes elements of type t1 and returns elements of type t2\n    l : list, list of elements of type t1\n\n  Ouptuts:\n    f_l : list, list of elements of type t2 obtained by applying f over each element of l\n  \"\"\"\n  f_l = []\n  for i in range(0,len(l)):\n    f_l.append(f(l[i]))\n\n  return f_l","c725c402":"data_clean = mapl(remove_trailing_newlines, data_raw)","b6b4eb23":"def split_at_s(text, s):\n  \"\"\"Function that splits string text into two parts at the first occurence of string s\n  Inputs:\n    text: string, string to be split\n    s : string, string of length 1 at which to split\n  \n  Outputs:\n    split_text: tuple of size 2, contains text split in two (do not include the string s at which split occurs in any of the split parts) \n  \"\"\"\n  s1 = text[0:text.index(s)]\n  s2 = text[text.index(s) + 1:len(text)]\n  split_text = (s1,s2)\n  return split_text","f46cc154":"split_at_tab = lambda text: split_at_s(text,'\\t')","539ad945":"data_clean2 = []\nfor i in range(0,len(data_clean)):\n  data_clean2.append(split_at_tab(data_clean[i]))","3af1d975":"import string\ndef remove_punctuations_and_lower(text):\n  \"\"\"Function that removes punctuations in a text\n  Inputs:\n    text: string\n  Outputs:\n    text_wo_punctuations\n  \"\"\"\n  return (text.translate(str.maketrans(\"\",\"\", string.punctuation))).lower()","649edfd3":"dataset = []\nfor i in range(0,len(data_clean2)):\n  t = data_clean2[i]\n  f = []\n  for j in range(0,2):\n   f.append(remove_punctuations_and_lower(t[j]))\n  dataset.append(tuple(f))","de6d12d9":"def counter(l, f):\n  \"\"\"\n  Function that returns a dictionary of counts of unique values obtained by applying f over elements of l\n  Inputs:\n    l: list; list of elements of type t\n    f: function; f takes arguments of type t and returns values of type u\n  \n  Outputs:\n    count_dict: dictionary; keys are elements of type u, values are ints\n  \"\"\"\n  count_dict = {}\n  t = []\n  for i in l:\n    t.append(f(i))\n  r = set(t)\n  for i in r:\n    count_dict[i] = t.count(i)\n  return count_dict","c3f35806":"def aux_func(i):\n  return(i[0])\ncounter(dataset,aux_func)","97770d70":"def random_shuffle(l):\n  import random\n  \"\"\"Function that returns a randomly shuffled list\n  Inputs:\n    l: list\n  Outputs:\n    l_shuffled: list, contains same elements as l but randomly shuffled\n  \"\"\"\n  random.shuffle(l)\n  l_shuffled = [] \n  for i in l:\n    l_shuffled.append(i)\n  return l_shuffled","f567784c":"n = random_shuffle(dataset)\nl1 = (int)(0.8 * len(n))\ndata_train = []\ndata_test = []\nfor i in range(0,l1):\n  data_train.append(n[i])\nfor i in range(l1,len(n)):\n  data_test.append(n[i])","8bca4f7c":"vocab = []\nfor i in data_train:\n  j = i[1].split(\" \")\n  for k in j:\n    if k not in vocab:\n      vocab.append(k)","04b6d3be":"dict_spam = {}\ndict_ham = {}\n\nfor i in vocab:\n  dict_spam[i] = 0\n  dict_ham[i] = 0\n\nfor i in vocab:\n  count = 0\n  for j in data_train:\n    if(j[0] == \"spam\"):\n      t = j[1].split(\" \")\n      for k in t:\n        if(i == k):\n          count += 1\n  dict_spam[i] = count\n\nfor i in vocab:\n  count = 0\n  for j in data_train:\n    if(j[0] == \"ham\"):\n      t = j[1].split(\" \")\n      for k in t:\n        if(i == k):\n          count +=1\n  dict_ham[i] = count\n","5149651f":"dict_prob_spam = {}\ndict_prob_ham = {}\nsumspam = 0\nsumham = 0\nsumspam = sum(dict_spam.values())\nsumham = sum(dict_ham.values())\nfor i in vocab:\n  dict_prob_spam[i] = (dict_spam[i] + 1)\/(len(vocab) + sumspam) \nfor i in vocab:\n  dict_prob_ham[i] = (dict_ham[i] + 1)\/(len(vocab) + sumham)","32a495be":"def predict(text, dict_prob_spam, dict_prob_ham, data_train):\n  \"\"\"Function which predicts the label of the sms\n  Inputs:\n    text: string, sms\n    dict_prob_spam: dictionary, contains dict_prob_spam as defined above\n    dict_prob_spam: dictionary, contains dict_prob_ham as defined above\n    data_train: list, list of tuples of type(label, sms), contains training dataset\n\n  Outputs:\n    prediction: string, one of two strings - either 'spam' or 'ham'\n  \"\"\"\n  prediction = ''\n  f = text.split(\" \")\n  d = counter(data_train,aux_func)\n  spam_score = d[\"spam\"]\/len(data_train)\n  ham_score = d[\"ham\"]\/len(data_train)\n  for j in f:\n    if j in vocab:\n      spam_score = spam_score*dict_prob_spam[j]\n      ham_score = ham_score*dict_prob_ham[j]\n  if spam_score > ham_score:\n    prediction = 'spam'\n  else:\n    prediction = 'ham'\n  return prediction\n","0dd2628f":"def accuracy(data_test, dict_prob_spam, dict_prob_ham, data_train):\n  \"\"\"Function which finds accuracy of model\n  Inputs:\n    data_test: list, contains tuples of data (label, sms) \n    dict_prob_spam: dictionary, contains dict_prob_spam as defined above\n    dict_prob_spam: dictionary, contains dict_prob_ham as defined above\n    data_train: list, list of tuples of type(label, sms), contains training dataset\n\n\n  Outputs:\n    accuracy: float, value of accuracy\n  \"\"\"\n  c = 0\n  for i in data_test:\n    o = i[0]\n    p = predict(i[1],dict_prob_spam, dict_prob_ham, data_train)\n    if p == o:\n      c += 1\n  accuracy = c\/len(data_test)\n  return accuracy\naccuracy(data_test,dict_prob_spam, dict_prob_ham, data_train)","9ac11ed6":"A function named ```aux_func``` can be passed to ```counter``` along with the list ```dataset``` to get a dictionary containing counts of ```ham``` and ```spam``` ","4d73f6a4":"Now use the function remove_punctuations to remove punctuations from the text part of all of the tuples in ```data_clean2``` and assign it to a variable named ```dataset```","a4ee3065":"We need to test our model on ```data_test``` . \nFor each sample of ```data_test```, prediction procedure is as follows: \n- For all words common to the sample and vocabulary, find ```spam_score``` and ```ham_score```\n- If ```spam_score``` is higher than ```ham_score```, then we predict the sample to be spam and vice versa.\n- ```spam_score``` = $P(spam)*\\tilde{P}(w_1 | \\text{ spam }) *  \\tilde{P}(w_2 | \\text{ spam }) * \\ldots$ where $w_1, w_2, \\ldots$ are words which occur both in the test sms and vocabulary.\n- Similary, ```ham_score``` = $P(ham)*\\tilde{P}(w_1 | \\text{ ham }) *  \\tilde{P}(w_2 | \\text{ ham }) * \\ldots$ where $w_1, w_2, \\ldots$ are words which occur both in the test sms and vocabulary. <br>\nHere $P(spam) = \\frac{\\text{Number of samples of type spam in training set}}{\\text{Total number of samples in training set}}$ <br>\nSimilarly, $P(ham) = \\frac{\\text{Number of samples of type ham in training set}}{\\text{Total number of samples in training set}}$ <br>\n(Note: The above is prediction procedure for a single sample in data_test) <br>\nNow we will write a function ```predict``` which does this.","1ee6d81f":"Use lambda expressions and ```split_at_s``` to write a function, ```split_at_tab``` that takes only one argument - ```text``` and splits at the first occurence of ```'\\t'``` character. (If you can't understand lambda expressions, just define the function in the ususal way)","0597074f":"If we apply ```remove_trailing_newlines``` to first element of data_raw, we get <br>\n<img src=\"https:\/\/drive.google.com\/uc?id=1vu-awFwqGC9sNgk-QgHMIamnFp6d996R\"> <br>\nYou can see that the newline at the end has disappeared. <br>\n","c68efd74":"####1.4","18616341":"####1.6","d75aa0f3":"This is a dataset of text messages. We have to classify this into spam or ham. Ham means non-spam relevant text messages. More details can be found here - http:\/\/archive.ics.uci.edu\/ml\/datasets\/SMS+Spam+Collection <br>\n<br>\nYou can see that each line starts by specifying whether the message is ```ham``` or ```spam``` and then there is a tab character, ```\\t``` followed by actual text message.\n<br>\nNow we need to split the lines to extract the two components - data label (```ham``` or ```spam```) and data sample (the text message).","a7bfeb59":"You can see that the first five elements in our list look like this - \n<img src=\"https:\/\/drive.google.com\/uc?id=1JnA0TxI-jWR4mJHYztnQQZZNRRTacsaa\"> \\\\\n<br>\nYou can clearly see that each line ends with a newline character. We want to remove these new line characters. \\\\\nNow we will write a function that removes all extra newline characters (number of newline characters maybe greater than or equal to 0) at the end of any string that is passed to it.","d61d38c1":"First five elements of data_clean look like this: <br>\n<img src = \"https:\/\/drive.google.com\/uc?id=17U7h87_7VjZs5CpRtJawHUfM5O1EztcS\">","7877cd32":"####3.1","b3ed582e":"Now we can use mapl to apply remove_trailing_newlines to all lines in data_raw","7dc4eeaa":"Now let us split our dataset into training and test sets. We'll first shuffle the elements of the dataset, then we'll use 80% of data for training and 20% for testing.","a6b2382b":"### 2.Data Modeling\nWe shall use Naive Bayes for modelling our classifier. You can read about Naive Bayes from here (https:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_classifier#Multinomial_naive_Bayes). But you don't actually need to read it, because we are going to move step by step in building this classifier.","b40129cf":"Now we will write a function that takes a list, randomly shuffles it and then returns it. <br>\n(Use the random library of python - https:\/\/docs.python.org\/3\/library\/random.html)","c76840a3":"After splitting at '\\t' character, one data point looks like this - <br>\n<img src = \"https:\/\/drive.google.com\/uc?id=1TVblRl9K_4HFLOncSWZW20u3ztsfoCzY\">\n","71d69759":"\n1.10","6848d976":"####1.3","c420dd79":"But we now we need to apply this function to the whole list. \\\\\nWe will write a function named mapl, that takes two arguments - a function on elements of type $t$ and a list $l$ of elements of type $t$ and applies the function over all elements of the list $l$ and returns them as a list. ","0e76f17d":"First we need to find the probabilities $P(w_i | C)$ <br>\nWe read $P(A | B)$ as probability of event A, given event B. <br>\n$P(w_i | C)$ is probability that word $w_i$ occurs in the sms given that the sms belongs to class $C$ where $C$ can be either ```spam``` or ```ham``` .\n<br>\nBut we will be finding $\\tilde{P}(w_i|C)$ which is the smoothed probability function to take care of words with 0 probabilities that may cause problems.","898b713b":"Now split the shuffled list. Take 80% (4459) samples and assign them to a variable called ```data_train``` . Put the rest in a variable called ```data_test```","24beed05":"###3. Prediction","1366f16f":"Find the vocabulary - list of unique words in all smses of ```data_train``` and assign it to the variable ```vocab```","b1cbda5d":"####1.5","5725f744":"####1.2","05390047":"####1.8","03ef647f":"#### 2.2","d97d7ef6":"$\\tilde{P}(w_i|C) = \\frac{\\text{Number of occurences of } w_i \\text{ in all samples of class C} + 1}{\\text{Total number of words in all samples of class C } + \\text{ Vocabulary size}}$","683b417c":"Now we will write a function ```counter``` that takes two arguments - \n- a list $l$ of elements of type $t$\n- a function $f: t \\rightarrow u$ (means $f$ takes an argument of type $t$ and returns values of type $u$)\n\nCounter returns a dictionary whose keys are $u_1, u_2, \\ldots etc$ - unique values of type $u$ obtained by applying $f$ over elements of $l$. <br>\nThe values corresponding to the keys are the the number of times a particular key say $u_1$ is obtained when we apply $f$ over elements of $l$","deba9f73":"We will first write a function that returns first five elements of the list if length of list is greater than or equal to 5 and None value otherwise.","830d147d":"First 5 elements of ```dataset``` look like this now. <br>\n<img src=\"https:\/\/drive.google.com\/uc?id=19TomF6uXvsFALRsX6KLOPseKzxADMgRp\">","590e7b23":"1. Data preparation\nNow the entire data is stored in the list ```data_raw```. \nEvery line in the file is a different element of the list.\nFirst let us look at the first five elements of the list. \n","50bd7368":"####1.9","a98ce308":"Now let us count number of occurences of ```ham``` and ```spam``` in our dataset.","3144812f":"####2.1","6e5e2783":"####3.2","6b974fc1":"Now let us remove the punctuations in an sms.","ebcfa035":"Now we will write a function ```split_at_s``` that takes two strings - ```text``` and ```s```. <br>\nIt splits the string text into two parts at the first occurence of s. <br>\nThen it wraps both parts in a tuple and returns it.","6120a3e3":"####2.3","4295f2ba":"Now find accuracy of the model. Apply function predict to all the samples in data_test. <br>\n$\\text{accuracy} = \\frac{\\text{number of correct predictions}}{\\text{size of test set}}$ <br>\nNow we will write a function accuracy which applies predict to all ```samples``` in data_test and returns ```accuracy```","919c89fe":"For every word $w_i$ in vocab, find the smoothed probability $\\tilde{P}(w_i | \\text{ spam })$ and put in a dictionary named ```dict_prob_spam```. \nIn a similar way, define the dictionary ```dict_prob_ham``` which contains smoothed probabilities $\\tilde{P}(w_i | \\text{ ham })$","483ebba0":"Python has a very handy feature used to define short functions called lambda expressions. This is from official python docs <br>\n<img src = \"https:\/\/drive.google.com\/uc?id=1kBLTdhWT6SNrhlk7vYH7ql2KxcSKvu29\">","71f76bb8":"The counts of ```ham``` and ```spam``` as we can see are ```{'ham': 4827, 'spam': 747}```","f2674469":"####1.7 ","eb487840":"For every word $w_i$ in vocab, find the count (total number of occurences) of $w_i$ in all smses of type ```spam```. Put these counts in a dictionary and assign it to a variable named ```dict_spam``` where key is the word $w_i$ and value is the count. <br>\nIn a similar way, create a variable called ```dict_ham``` which contains counts of each word in vocabulary in smses of type ```ham```. (This is only w.r.t samples in ```data_train```) ","a79967bc":"Now apply split_at_tab function over elements of list ```data_clean``` and assign it to variable named ```data_clean2```"}}