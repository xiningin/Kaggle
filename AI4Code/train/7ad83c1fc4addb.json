{"cell_type":{"813d1729":"code","375d5777":"code","f2a63b74":"code","463e205a":"code","f5f53f1c":"code","e3abae31":"code","81d688f1":"code","51d44373":"code","6f33e8ef":"code","e46eb4e5":"code","241f9991":"code","65ad79ff":"code","a020611a":"code","0cc3917c":"code","42164280":"code","395eeb1a":"code","9d77019d":"code","01a40eed":"code","e3d8e72e":"code","138a06a9":"code","e6fce90b":"code","d3b4b5b2":"code","4b6163f9":"code","b026e4e5":"code","d3d30cf4":"code","d0cd1613":"code","13d361df":"code","65ee6a9c":"code","9e7af4b6":"code","cd5d8f96":"code","3ed531a0":"code","730a3e3a":"code","fb040268":"code","18313751":"code","fedb9acb":"code","dd6902c1":"code","87fd12e9":"code","76da8200":"code","75c12c32":"code","3d409058":"code","80febd48":"code","c9233f07":"code","8465bc4f":"code","f05424dd":"code","1923a475":"code","67d3e0fa":"code","2d34add5":"code","0dcabeba":"code","7b1d46ef":"code","ecf26d3e":"code","9998e5ad":"markdown"},"source":{"813d1729":"# Importing all required packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport sys\nimport pandas as pd\nimport numpy as np\nimport sklearn\nimport matplotlib\nimport keras\n\nprint('Python: {}'.format(sys.version))\nprint('Pandas: {}'.format(pd.__version__))\nprint('Numpy: {}'.format(np.__version__))\nprint('Sklearn: {}'.format(sklearn.__version__))\nprint('Matplotlib: {}'.format(matplotlib.__version__))\nprint('Keras: {}'.format(keras.__version__))","375d5777":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\ndf= pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\ndf","f2a63b74":"df.shape","463e205a":"df.info()\n\n# let's look at the statistical aspects of the dataframe\ndf.describe()","f5f53f1c":"df.isna().sum()","e3abae31":"df.columns","81d688f1":"sns.countplot(x=\"target\", data=df)\nplt.show()","51d44373":"countNoDisease = len(df[df.target == 0])\ncountHaveDisease = len(df[df.target == 1])\nprint(\"Percentage of Patients Haven't Heart Disease: {:.2f}%\".format((countNoDisease \/ (len(df.target))*100)))\nprint(\"Percentage of Patients Have Heart Disease: {:.2f}%\".format((countHaveDisease \/ (len(df.target))*100)))\n","6f33e8ef":"plt.figure(figsize = (16, 10))\nsns.heatmap(df.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","e46eb4e5":"pd.crosstab(df.age,df.target).plot(kind=\"bar\",figsize=(20,6))\nplt.title('Heart Disease Frequency for Ages')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.savefig('heartDiseaseAndAges.png')\nplt.show()","241f9991":"plt.scatter(x=df.age[df.target==1], y=df.thalach[(df.target==1)], c=\"green\")\nplt.scatter(x=df.age[df.target==0], y=df.thalach[(df.target==0)], c=\"red\")\nplt.legend([\"Disease\", \"Not Disease\"])\nplt.xlabel(\"Age\")\nplt.ylabel(\"Maximum Heart Rate\")\nplt.show()","65ad79ff":"pd.crosstab(df.fbs,df.target).plot(kind=\"bar\",figsize=(15,6),color=['#FFC700','#FE1845' ])\nplt.title('Heart Disease Frequency According To FBS')\nplt.xlabel('FBS - (Fasting Blood Sugar > 120 mg\/dl) (1 = true; 0 = false)')\nplt.xticks(rotation = 0)\nplt.legend([\"Haven't Disease\", \"Have Disease\"])\nplt.ylabel('Frequency of Disease or Not')\nplt.show()","a020611a":"df.head()","0cc3917c":"a = pd.get_dummies(df['cp'], prefix = \"cp\")\nb = pd.get_dummies(df['thal'], prefix = \"thal\")\nc = pd.get_dummies(df['slope'], prefix = \"slope\")\n\n\nframes = [df, a, b, c]\ndf = pd.concat(frames, axis = 1)\ndf.head()","42164280":"df = df.drop(columns = ['cp', 'thal', 'slope'])\ndf.head()","395eeb1a":"y = df.target.values\nX = df.drop(['target'], axis = 1)","9d77019d":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\nX[X.columns] = scaler.fit_transform(X[X.columns])\n\nX.head()","01a40eed":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","e3d8e72e":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","138a06a9":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.75, test_size = 0.25, random_state = 123)","e6fce90b":"# Let's now take a look at the train dataset\n\nX_train.head()","d3b4b5b2":"# Logistic regression model using sklearn\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\n\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg, n_features_to_select=15)             # running RFE with 13 variables as output\nrfe = rfe.fit(X_train, y_train)","4b6163f9":"rfe.support_","b026e4e5":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","d3d30cf4":"col = X_train.columns[rfe.support_]","d0cd1613":"X_train.columns[~rfe.support_]","13d361df":"accuracies = {}\n\nlr = LogisticRegression(max_iter=10000)\nlr.fit(X_train,y_train)\nacc = lr.score(X_test,y_test)*100\n\naccuracies['Logistic Regression'] = acc\nprint(\"Test Accuracy {:.2f}%\".format(acc))","65ee6a9c":"# KNN Model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 4)  # n_neighbors means k\nknn.fit(X_train, y_train)\nprediction = knn.predict((X_test))\n\nprint(\"{} NN Score: {:.2f}%\".format(2, knn.score(X_test,y_test)*100))","9e7af4b6":"# try ro find best k value\nscoreList = []\nfor i in range(1,20):\n    knn2 = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n    knn2.fit(X_train, y_train)\n    scoreList.append(knn2.score(X_test, y_test))\n    \nplt.plot(range(1,20), scoreList)\nplt.xticks(np.arange(1,20,1))\nplt.xlabel(\"K value\")\nplt.ylabel(\"Score\")\nplt.show()\n\nacc = max(scoreList)*100\naccuracies['KNN'] = acc\nprint(\"Maximum KNN Score is {:.2f}%\".format(acc))","cd5d8f96":"from sklearn.svm import SVC\nsvm = SVC(random_state = 5)\nsvm.fit(X_train, y_train)\n\nacc = svm.score(X_test,y_test)*100\naccuracies['SVM'] = acc\nprint(\"Test Accuracy of SVM Algorithm: {:.2f}%\".format(acc))","3ed531a0":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train, y_train)\n\nacc = nb.score(X_test,y_test)*100\naccuracies['Naive Bayes'] = acc\nprint(\"Accuracy of Naive Bayes: {:.2f}%\".format(acc))","730a3e3a":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\n\nacc = dtc.score(X_test, y_test)*100\naccuracies['Decision Tree'] = acc\nprint(\"Decision Tree Test Accuracy {:.2f}%\".format(acc))","fb040268":"# Random Forest Classification\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 1000, random_state = 123)\nrf.fit(X_train, y_train)\n\nacc = rf.score(X_test,y_test)*100\naccuracies['Random Forest'] = acc\nprint(\"Random Forest Algorithm Accuracy Score : {:.2f}%\".format(acc))","18313751":"colors = [\"purple\", \"green\", \"orange\", \"magenta\",\"#CFC60E\",\"#0FBBAE\"]\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,100,10))\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Algorithms\")\nsns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)\nplt.show()","fedb9acb":"# Read the given CSV file, and view some sample records\n\ndata = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\n# data=data.drop('oldpeak',axis=1)","dd6902c1":"a = pd.get_dummies(data['cp'], prefix = \"cp\")\nb = pd.get_dummies(data['thal'], prefix = \"thal\")\nc = pd.get_dummies(data['slope'], prefix = \"slope\")\n\n\nframes = [data, a, b, c]\ndata = pd.concat(frames, axis = 1)\ndata.head()","87fd12e9":"data = data.drop(columns = ['cp', 'thal', 'slope'])\ndata.head()","76da8200":"data","75c12c32":"# transform data to numeric to enable further analysis\ndata = data.apply(pd.to_numeric)\ndata.dtypes","3d409058":"X = np.array(data.drop(['target'], 1))\ny = np.array(data['target'])","80febd48":"X[0]","c9233f07":"mean = X.mean(axis=0)\nX -= mean\nstd = X.std(axis=0)\nX \/= std","8465bc4f":"X[0]","f05424dd":"# create X and Y datasets for training\nfrom sklearn import model_selection\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, stratify=y, random_state=42, test_size = 0.2)","1923a475":"# convert the data to categorical labels\nfrom keras.utils.np_utils import to_categorical\n\nY_train = to_categorical(y_train, num_classes=None)\nY_test = to_categorical(y_test, num_classes=None)\nprint (Y_train.shape)\nprint (Y_train[:10])","67d3e0fa":"X_train[0]","2d34add5":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom keras.layers import Dropout\nfrom keras import regularizers\n\n# define a function to build the keras model\ndef create_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(13, input_dim=21, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(6, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.003), activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(2, activation='softmax'))\n    \n    # compile model\n    adam = Adam(lr=0.001)\n    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n    return model\n\nmodel = create_model()\n\nprint(model.summary())","0dcabeba":"# fit the model to the training data\nhistory=model.fit(X_train, Y_train, validation_data=(X_test, Y_test),epochs=500, batch_size=10)","7b1d46ef":"import matplotlib.pyplot as plt\n%matplotlib inline\n# Model accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()\n","ecf26d3e":"# Model Losss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()","9998e5ad":"# Creating Model for Logistic Regression\n\nWe can use sklearn library or we can write functions ourselves. Let's them both. Firstly we will write our functions after that we'll use sklearn library to calculate score."}}