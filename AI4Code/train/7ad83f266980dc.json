{"cell_type":{"dc47727f":"code","4cc85c05":"code","c74c92e2":"code","6d52064f":"code","340c3be1":"code","a962d0ae":"code","9ef55b1a":"code","c1c7564b":"code","3cd83da8":"code","bf338824":"code","cb036885":"code","c614f7b0":"code","07eca4ea":"code","3f7d610c":"code","3e620e09":"code","39cec449":"code","a32a66d4":"code","a9268d23":"code","89ab3ee0":"code","14b34841":"code","1bb297b2":"code","e6dd8e2a":"code","38b0dcd1":"code","369d5d2e":"code","cb1eb933":"code","d4f21215":"code","b85db42e":"code","4350b3c6":"code","bc2ef618":"code","f1226a0a":"code","f7950288":"code","0e3170c0":"code","9721f593":"code","2f4662da":"code","22ceb4b5":"code","4cdc043c":"code","df8b8780":"code","1c062629":"code","d53e9658":"code","e1adc67a":"code","8cdb4cd9":"code","0116308b":"code","811632e4":"code","ff565988":"code","7753ec3b":"code","cc16a896":"code","9270afe2":"code","c5959f5b":"code","f31bf698":"code","f6c340ae":"code","196597be":"code","fcc37c21":"code","13934947":"code","c321a66a":"code","0fba5e99":"code","ecffe780":"code","cd0caba9":"code","71904ded":"code","70872c03":"code","f465f566":"code","2ab45215":"code","26b871ec":"code","13b6e319":"code","7f8675cc":"code","9df41652":"code","97329017":"code","5f2463c4":"code","7f9464c6":"code","d49c4890":"code","0e5e5d86":"code","c0739192":"code","67c23752":"code","55294ee2":"markdown","90675265":"markdown","10fead4d":"markdown","6b179318":"markdown","eae7d0e5":"markdown","c88872cb":"markdown","209367f4":"markdown","e5255464":"markdown","79dc76bf":"markdown","46675f00":"markdown","51bc28b5":"markdown","cc1e6338":"markdown","164830fe":"markdown","93f0d170":"markdown","a9c1ee95":"markdown","b7ba6be0":"markdown","c9c17de4":"markdown","ff00a963":"markdown","d52b818b":"markdown","072a05a4":"markdown","ebd52e5d":"markdown","48728473":"markdown","a628c56e":"markdown","b6449816":"markdown","f3ab6506":"markdown","60382875":"markdown","60bd995d":"markdown","46f395bb":"markdown","2f75dc2e":"markdown","00225396":"markdown","11b3287e":"markdown","723d444e":"markdown","57cf82c0":"markdown","f0d31990":"markdown","602cf1cc":"markdown","fd0dc9e7":"markdown","03e58c1e":"markdown","06fbd5ca":"markdown","a9fb6675":"markdown","0f4e8741":"markdown","b6cef3b7":"markdown","75c259d3":"markdown","a84b1300":"markdown","6aaad929":"markdown","fb9973a7":"markdown","4fe50036":"markdown","7bb2974c":"markdown","b45d46bd":"markdown","3ff4406f":"markdown","95494c17":"markdown","355dc8ed":"markdown","94a48187":"markdown","bdcfe92e":"markdown","6af84032":"markdown","84753195":"markdown","e2f08017":"markdown","442cd81e":"markdown","9c3f1c1c":"markdown","ae42c539":"markdown","2ed0fff4":"markdown","19c6425c":"markdown","113931df":"markdown","aeefb1ab":"markdown","e7144d05":"markdown","06b3cd2b":"markdown","f0e2955e":"markdown","94fd03cd":"markdown","838679c8":"markdown","70da5a80":"markdown","8850552d":"markdown","1ee02583":"markdown","688b1b8b":"markdown","1731bc23":"markdown","63caea70":"markdown","680f4808":"markdown"},"source":{"dc47727f":"import pandas as pd\nimport plotly.express as px\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport scipy \nfrom scipy import stats\nimport scipy.stats as ss\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nnp.random.seed(123)\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC \nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\nfrom sklearn.model_selection import GridSearchCV\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nfrom sklearn.tree import plot_tree\nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\nimport shap\n","4cc85c05":"from IPython.display import HTML\n\nHTML('''<script>\ncode_show=true; \nfunction code_toggle() {\n if (code_show){\n $('div.input').hide();\n } else {\n $('div.input').show();\n }\n code_show = !code_show\n} \n$( document ).ready(code_toggle);\n<\/script>\n<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on\/off the raw code.\"><\/form>''')","c74c92e2":"# !jupyter nbconvert --to html MathProject.ipynb","6d52064f":"# !pip install shap","340c3be1":"header = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', \n              'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved','exercise_induced_angina', \n              'st_depression', 'st_slope', 'num_major_vessels', 'thalassemia', 'target']\n\ndf = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\ndf.columns = header\ndf.target = df.target.replace({0:1, 1:0})\ndf.head(10)\n\nmissing_values =df.isna().sum().sum()\nprint(f'Missing values found: {missing_values}')","a962d0ae":"df['sex'][df['sex'] == 0] = 'female'\ndf['sex'][df['sex'] == 1] = 'male'\n\ndf['chest_pain_type'][df['chest_pain_type'] == 0] = 'typical angina'\ndf['chest_pain_type'][df['chest_pain_type'] == 1] = 'atypical angina'\ndf['chest_pain_type'][df['chest_pain_type'] == 2] = 'non-anginal pain'\ndf['chest_pain_type'][df['chest_pain_type'] == 3] = 'asymptomatic'\n\ndf['thalassemia'][df['thalassemia'] == 0] = 'non present'\ndf['thalassemia'][df['thalassemia'] == 1] = 'normal'\ndf['thalassemia'][df['thalassemia'] == 2] = 'fixed defect'\ndf['thalassemia'][df['thalassemia'] == 3] = 'reversable defect'\n\ndf['fasting_blood_sugar'][df['fasting_blood_sugar'] == 0] = 'lower than 120mg\/ml'\ndf['fasting_blood_sugar'][df['fasting_blood_sugar'] == 1] = 'greater than 120mg\/ml'\n\ndf['rest_ecg'][df['rest_ecg'] == 0] = 'normal'\ndf['rest_ecg'][df['rest_ecg'] == 1] = 'ST-T wave abnormality'\ndf['rest_ecg'][df['rest_ecg'] == 2] = 'left ventricular hypertrophy'\n\ndf['exercise_induced_angina'][df['exercise_induced_angina'] == 0] = 'no'\ndf['exercise_induced_angina'][df['exercise_induced_angina'] == 1] = 'yes'\n\ndf['st_slope'][df['st_slope'] == 0] = 'upsloping'\ndf['st_slope'][df['st_slope'] == 1] = 'flat'\ndf['st_slope'][df['st_slope'] == 2] = 'downsloping'\n\ndf['num_major_vessels'][df['num_major_vessels'] == 4] = 'no occlusion'\ndf['num_major_vessels'][df['num_major_vessels'] == 3] = 'slight occlusion'\ndf['num_major_vessels'][df['num_major_vessels'] == 2] = 'medium occlusion'\ndf['num_major_vessels'][df['num_major_vessels'] == 1] = 'high occlusion'\ndf['num_major_vessels'][df['num_major_vessels'] == 0] = 'severe occlusion'\n\n\ncategorical = ['sex','chest_pain_type','fasting_blood_sugar','rest_ecg','exercise_induced_angina','st_slope',\n               'thalassemia','num_major_vessels']\nnumeric = ['age', 'resting_blood_pressure', 'cholesterol', 'max_heart_rate_achieved','target']\n\ndf[categorical] = df[categorical].astype('category')\ndf[numeric] = df[numeric].astype(int).astype('int64')\ndf.head(10)\n","9ef55b1a":"df[['age', 'resting_blood_pressure', 'cholesterol', 'max_heart_rate_achieved','st_depression']].describe().T","c1c7564b":"palette = ['blue', 'red']","3cd83da8":"fig = px.histogram(df, x = 'target', nbins =5, color = 'target', \n                   title = 'Count healthy and unhealthy patients (healthy = 0)', height = 400, \n                   width = 500, color_discrete_sequence= palette)\nfig.show()\ncountDisease, countNoDisease = df.groupby(['target']).size()\nprint(f'Percentage of people having heart disease: {(countDisease\/(countDisease+countNoDisease))*100:.1f}%')\nprint(f'Percentage of people not having heart disease: {(countNoDisease\/(countDisease+countNoDisease))*100:.1f}%')","bf338824":"df_cross = pd.crosstab(df.sex, df.target)\ndata = []\n#use for loop on every name to create bar data\nfor i,x in enumerate(df_cross.columns):\n    data.append(go.Bar(name=str(x), x=df_cross.index, y=df_cross[x], marker_color= palette[i]))\n\nfigure = go.Figure(data)\nfigure.update_layout(barmode = 'group', \n                     title=\"Sex vs Heart Disease (healthy = 0)\", width = 500, height = 400)\n\n# barmode = 'group','stack','relative'\n#For you to take a look at the result use\nfigure.show()\n\ncountFemale, countMale = df.groupby(['sex']).size()\nprint(f'Percentage of female patients: {(countFemale\/(countMale+countFemale))*100:.1f}%')\nprint(f'Percentage of male patients: {(countMale\/(countMale+countFemale))*100:.1f}%')","cb036885":"df_cross = pd.crosstab(df.age, df.target)\ndata = []\n#use for loop on every name to create bar data\nfor i,x in enumerate(df_cross.columns):\n    data.append(go.Bar(name=str(x), x=df_cross.index, y=df_cross[x], marker_color= palette[i]))\n\nfigure = go.Figure(data)\nfigure.update_layout(barmode = 'group', \n                     title=\"Age vs Heart Disease (healthy = 0)\")\n\n# barmode = 'group','stack','relative'\n#For you to take a look at the result use\nfigure.show()","c614f7b0":"for i,attr in enumerate(['age', 'resting_blood_pressure', 'cholesterol', 'max_heart_rate_achieved','st_depression']):\n    \n    f, (ax_box1,ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (0.2,0.8)}, \n                                                 figsize = (10,7))\n    f.suptitle(f'{attr}')\n    sns.boxplot(df[attr], ax=ax_box1, color = sns.color_palette('hsv')[i])\n    sns.distplot(df[attr], ax=ax_hist, color = sns.color_palette('hsv')[i])\n    ax_box1.set_xlabel('')\n","07eca4ea":"\nfor attr in ['age', 'resting_blood_pressure', 'cholesterol', 'max_heart_rate_achieved','st_depression']:\n    f, (ax_box1,ax_box2, ax_hist) = plt.subplots(3, sharex=True, gridspec_kw={\"height_ratios\": (.12, .12, .76)}, \n                                                 figsize = (12,8))\n    f.suptitle(f'{attr} (0 = healthy, 1 = unhealthy)')\n    # Add a graph in each part\n    sns.boxplot(df[attr][df[\"target\"]==0], ax=ax_box1, color = palette[0])\n    sns.boxplot(df[attr][df[\"target\"]==1], ax=ax_box2,color =palette[1])\n    sns.distplot(df[attr][df[\"target\"]==0], ax=ax_hist, color = palette[0])\n    sns.distplot(df[attr][df[\"target\"]==1], ax=ax_hist, color = palette[1])\n    # Remove x axis name for the boxplot\n    ax_box1.set_ylabel('0', rotation='horizontal')\n    ax_box2.set_ylabel('1', rotation='horizontal')\n    ax_box1.set_xlabel('')\n    ax_box2.set_xlabel('')\n    \n","3f7d610c":"for i,attr in enumerate([\"age\",\"resting_blood_pressure\",\"cholesterol\",\"max_heart_rate_achieved\",\"st_depression\"]):\n    fig = px.box(df, x = \"sex\", y = attr, color = 'target', width = 800, height = 400, color_discrete_sequence= palette[::-1])\n    fig.show()","3e620e09":"scaler = StandardScaler()\nX_scaled = pd.DataFrame(scaler.fit_transform(df[['age', 'resting_blood_pressure', \n                                    'cholesterol', 'max_heart_rate_achieved','st_depression']]), columns = ['age', 'resting_blood_pressure', \n                                    'cholesterol', 'max_heart_rate_achieved','st_depression'])\nX_scaled['target'] = df['target']","39cec449":"plt.figure(figsize=(16, 10))\nsns.pairplot(X_scaled,\n             hue='target', palette = palette,\n           markers=['o','o'], plot_kws=dict(s=25, alpha=0.75, ci=None)\n            )\nplt.show()","a32a66d4":"plt.figure(figsize = (10,6))\nsns.heatmap(df.corr(), annot=True)","a9268d23":"f1, axs = plt.subplots(2,2,figsize=(15,10))\n\nf1.tight_layout()\n\nsns.countplot(df.sex,data=df,ax=axs[0][0], palette = \"magma\")\nfor p in axs[0][0].patches:\n    height = p.get_height()\n    axs[0][0].text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/float(len(df.sex))),\n            ha=\"center\")         \n        \nsns.countplot(df.exercise_induced_angina,data=df,ax=axs[0][1], palette =\"magma\")\nfor p in axs[0][1].patches:\n    height = p.get_height()\n    axs[0][1].text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/float(len(df.exercise_induced_angina))),\n            ha=\"center\") \n    \nsns.countplot(df.st_slope,data=df,ax=axs[1][1],palette = \"magma\")\nfor p in axs[1][1].patches:\n    height = p.get_height()\n    axs[1][1].text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/float(len(df.st_slope))),\n            ha=\"center\") \n    \nsns.countplot(df.chest_pain_type,data=df,ax=axs[1][0], palette =\"magma\")\nfor p in axs[1][0].patches:\n    height = p.get_height()\n    axs[1][0].text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/float(len(df.chest_pain_type))),\n            ha=\"center\") \n\nf2, axs2 = plt.subplots(2,2,figsize=(15,8))\n\nf2.tight_layout()\nsns.countplot(df.fasting_blood_sugar,data=df,ax=axs2[0][0], palette = \"magma\")\nfor p in axs2[0][0].patches:\n    height = p.get_height()\n    axs2[0][0].text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/float(len(df.fasting_blood_sugar))),\n            ha=\"center\") \n    \nsns.countplot(df.rest_ecg,data=df,ax=axs2[1][0], palette = \"magma\")\nfor p in axs2[1][0].patches:\n    height = p.get_height()\n    axs2[1][0].text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/float(len(df.rest_ecg))),\n            ha=\"center\")\n    \n    \nsns.countplot(df.num_major_vessels,data=df,ax=axs2[0][1],palette =\"magma\")\nfor p in axs2[0][1].patches:\n    height = p.get_height()\n    axs2[0][1].text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/float(len(df.num_major_vessels))),\n            ha=\"center\")\n    \n    \nsns.countplot(df.thalassemia,data=df,ax=axs2[1][1], palette = \"magma\")\nfor p in axs2[1][1].patches:\n    height = p.get_height()\n    axs2[1][1].text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/float(len(df.thalassemia))),\n            ha=\"center\")","89ab3ee0":"f1, axs = plt.subplots(2,2,figsize=(15,10))\n\nf1.tight_layout()\n\nsns.countplot(df.target,hue=\"sex\",data=df,ax=axs[0][0], palette = \"magma\")\nfor p in axs[0][0].patches:\n    height = p.get_height()\n    axs[0][0].text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/float(len(df.sex))),\n            ha=\"center\") \n    \nsns.countplot(df.target,hue=\"exercise_induced_angina\",data=df,ax=axs[0][1], palette = \"magma\")\nfor p in axs[0][1].patches:\n    height = p.get_height()\n    axs[0][1].text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/float(len(df.exercise_induced_angina))),\n            ha=\"center\") \n    \nsns.countplot(df.target,hue=\"st_slope\",data=df,ax=axs[1][1],palette = \"magma\")\nfor p in axs[1][1].patches:\n    height = p.get_height()\n    axs[1][1].text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/float(len(df.st_slope))),\n            ha=\"center\") \n\n\nsns.countplot(df.target,hue=\"chest_pain_type\",data=df,ax=axs[1][0], palette = \"magma\")\nfor p in axs[1][0].patches:\n    height = p.get_height()\n    axs[1][0].text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/float(len(df.chest_pain_type))),\n            ha=\"center\") \n\nf2, axs2 = plt.subplots(2,2,figsize=(15,8))\n\nf2.tight_layout()\nsns.countplot(df.target,hue=\"fasting_blood_sugar\",data=df,ax=axs2[0][0], palette = \"magma\")\nfor p in axs2[0][0].patches:\n    height = p.get_height()\n    axs2[0][0].text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/float(len(df.fasting_blood_sugar))),\n            ha=\"center\") \n\nsns.countplot(df.target,hue=\"rest_ecg\",data=df,ax=axs2[1][0], palette = \"magma\")\nfor p in axs2[1][0].patches:\n    height = p.get_height()\n    axs2[1][0].text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/float(len(df.rest_ecg))),\n            ha=\"center\")\n\nsns.countplot(df.target,hue=\"num_major_vessels\",data=df,ax=axs2[0][1],palette = \"magma\")\nfor p in axs2[0][1].patches:\n    height = p.get_height()\n    axs2[0][1].text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/float(len(df.num_major_vessels))),\n            ha=\"center\")\n\nsns.countplot(df.target,hue=\"thalassemia\",data=df,ax=axs2[1][1], palette = \"magma\")\nfor p in axs2[1][1].patches:\n    height = p.get_height()\n    axs2[1][1].text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}'.format(height\/float(len(df.thalassemia))),\n            ha=\"center\")","14b34841":"X = pd.get_dummies(df, drop_first=True)\nX.head(10)","1bb297b2":"rand = 42\ntarget = X['target']\nX = X.drop('target', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.80, stratify = target, random_state = rand)","e6dd8e2a":"scaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train_std = pd.DataFrame(scaler.fit_transform(X_train), index = X_train.index, columns = X_train.columns)\nX_test_std = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)","38b0dcd1":"plt.figure(figsize =(8,8))\nfor i in [\"age\",\"resting_blood_pressure\",\"cholesterol\",\"max_heart_rate_achieved\",\"st_depression\"]:\n    sns.distplot(X_train_std[i], hist = False, label = i)","369d5d2e":"X_train_IQR = X_train_std\ny_train_IQR = y_train\n\nfor attr in ['age', 'resting_blood_pressure', 'cholesterol','max_heart_rate_achieved','st_depression']:\n    indexToDrop = []\n    Q1 = X_train_IQR[attr].quantile(0.25)\n    Q3 = X_train_IQR[attr].quantile(0.75)\n    IQR = Q3 - Q1\n    low = Q1 - IQR*1.5\n    upp = Q3 + IQR*1.5\n    ind = X_train_IQR[(X_train_IQR[attr] < low ) | (X_train_IQR[attr] > upp)].index\n    if len(ind) != 0:\n        X_train_IQR = X_train_IQR.drop(ind)\n        y_train_IQR = y_train_IQR.drop(ind)","cb1eb933":"fig = make_subplots(rows=1, cols=5, subplot_titles=(\"Age\", \n                                                    \"resting blood pressure\",\n                                                    \"cholesterol\", \n                                                    \"max_heart_rate_achieved\",\n                                                   \"st_depression\"))\n\nfig.add_trace(\n    go.Box(y=X_train_std.age, boxpoints = 'outliers', name = 'age'),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Box(y=X_train_std.resting_blood_pressure, boxpoints = 'outliers', name = 'resting blood pressure'),\n    row=1, col=2\n)\nfig.add_trace(\n    go.Box(y=X_train_std.cholesterol, boxpoints = 'outliers', name = 'cholesterol'),\n    row=1, col=3\n)\nfig.add_trace(\n    go.Box(y=X_train_std.max_heart_rate_achieved, boxpoints = 'outliers', name = 'max heart rate'),\n    row=1, col=4\n)\nfig.add_trace(\n    go.Box(y=X_train_std.st_depression, boxpoints = 'outliers', name = 'ST depression'),\n    row=1, col=5\n)\nfig.update_layout(height=450, width=1100,showlegend=True)\nfig.update_traces(orientation='v')\nfig.show()","d4f21215":"fig = make_subplots(rows=1, cols=5, subplot_titles=(\"Age\", \n                                                    \"resting blood pressure\",\n                                                    \"cholesterol\", \n                                                    \"max_heart_rate_achieved\",\n                                                   \"st_depression\"))\n\nfig.add_trace(\n    go.Box(y=X_train_IQR.age, boxpoints = 'outliers', name = 'age'),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Box(y=X_train_IQR.resting_blood_pressure, boxpoints = 'outliers', name = 'resting blood pressure'),\n    row=1, col=2\n)\nfig.add_trace(\n    go.Box(y=X_train_IQR.cholesterol, boxpoints = 'outliers', name = 'cholesterol'),\n    row=1, col=3\n)\nfig.add_trace(\n    go.Box(y=X_train_IQR.max_heart_rate_achieved, boxpoints = 'outliers', name = 'max heart rate'),\n    row=1, col=4\n)\nfig.add_trace(\n    go.Box(y=X_train_IQR.st_depression, boxpoints = 'outliers', name = 'ST depression'),\n    row=1, col=5\n)\nfig.update_layout(height=450, width=1100,showlegend=True)\nfig.update_traces(orientation='v')\nfig.show()","b85db42e":"outlierDetector = IsolationForest(n_estimators=100, max_samples='auto', contamination=float(.06), \\\n                        max_features=1.0, bootstrap=False, n_jobs=-1, random_state=rand, verbose=0)\noutlierDetector.fit(X_train_std)\npred = outlierDetector.predict(X_train_std)\nindexToDrop = X_train_std[pred==-1].index\n\nprint(f'Number of outliers found: {len(indexToDrop)}')","4350b3c6":"pca =  PCA(n_components  = 3)\nX_train_red  =  pd.DataFrame(pca.fit_transform(X_train_std), index = X_train_std.index)\n\nX_train_red.columns = ['component1', 'component2','component3']\nX_train_red['outliers'] = [1]*len(X_train_red)\nX_train_red.loc[indexToDrop,'outliers'] = -1\n\nfig = go.Figure(data=[go.Scatter3d(\n    x=X_train_red['component1'], y=X_train_red['component2'], z=X_train_red['component3'],\n    mode='markers',\n    marker=dict(\n        size=3,\n        color = X_train_red['outliers'],\n        colorscale=palette[::-1],\n        opacity=0.8\n    )\n)])\nfig.update_layout(title = '3D visualization of the outliers')\nfig.show()","bc2ef618":"X_train_clean = X_train_std.drop(indexToDrop)\ny_train_clean = y_train.drop(indexToDrop)","f1226a0a":"pca =  PCA(n_components = 3)\nX_train_red  =  pd.DataFrame(pca.fit_transform(X_train_clean), index = X_train_clean.index)\n\nX_train_red.columns = ['component1', 'component2','component3']\n\nfig = go.Figure(data=[go.Scatter3d(\n    x=X_train_red['component1'], y=X_train_red['component2'], z=X_train_red['component3'],\n    mode='markers',\n    marker=dict(\n        size=7,\n        color = y_train_clean,\n        colorscale=palette,\n        opacity=0.8\n    )\n)])\nfig.update_layout(title = '3D visualization of the data')\nfig.show()","f7950288":"X_train_clean = X_train_std.drop(indexToDrop)\ny_train_clean = y_train.drop(indexToDrop)","0e3170c0":"pca = PCA().fit(X_train_clean)\nnVar = len(X_train_clean.columns)\n\nfig = make_subplots(rows=1, cols=1, subplot_titles=(\"Cumulative Variance Explained and Variance Explained\"))\n\nfig.add_trace(\n    go.Scatter(x=np.arange(1,nVar+1), y=pca.explained_variance_ratio_.cumsum(),\n        name='Cumulative Variance Explained',\n        line=dict(color='blue'),\n        connectgaps=True),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Bar(x=np.arange(1,nVar+1), y=pca.explained_variance_ratio_,\n        name='Variance Explained'),\n    row=1, col=1\n)\nfig.add_shape(\n            type=\"line\",\n            x0=10,\n            y0=0,\n            x1=10,\n            y1=1,\n            line=dict(\n                color=\"green\",\n                width=2,\n                dash=\"dot\",\n            )\n)\n\nfig.update_layout(width = 1000, height = 600)\n\nfig.show()","9721f593":"pca = PCA(n_components = 0.70)\ndef build_pipeline(pipeline, params, X_train, y_train):\n    grid = GridSearchCV(pipeline, params, scoring = 'f1_weighted',\n                            return_train_score=True, n_jobs = -1)\n    grid.fit(X_train, y_train)\n    print(f\"Best parameters set found on development set: {grid.best_params_}\")\n    return grid\n\ndef apply_model(grid, X_test, y_test):\n    y = grid.predict(X_test)\n    \n    c = confusion_matrix(y_test, y)\n    sensitivity = c[0,0]\/(c[1,0]+c[0,0])\n    acc = accuracy_score(y_test,y)\n    f1 = f1_score(y_test,y)\n    specificity = c[1,1]\/(c[1,1]+c[0,1])    \n\n    print(classification_report(y_test,y))\n    print()\n    plt.figure(figsize = (7,5))\n    sns.heatmap(c, annot=True)\n    plt.title('Confusion Matrix')\n    print()\n    \n    return acc,f1,specificity,sensitivity","2f4662da":"from imblearn.over_sampling import RandomOverSampler\nfrom imblearn.pipeline import Pipeline\n\nRF_PCA_ros_pip = Pipeline([('pca', pca), ('ros', RandomOverSampler(random_state=rand)),\n                     ('classifier', RandomForestClassifier(random_state = rand))])\n\nRF_PCA_pip = Pipeline([('pca', pca),('classifier', RandomForestClassifier(random_state = rand))])\n\nparams_tree = {'classifier__n_estimators': [25,50,100], 'classifier__max_depth' : [5, 8], \n               'classifier__max_features': ['auto', 1,2], 'classifier__criterion' :['gini']}","22ceb4b5":"tree_pca_ros_metrics = []\ntree_grid_pca_ros = build_pipeline(RF_PCA_ros_pip, params_tree, X_train_clean, y_train_clean)\ntree_pca_ros_metrics = apply_model(tree_grid_pca_ros, X_test_std, y_test)\n","4cdc043c":"tree_pca_metrics = []\ntree_grid_pca = build_pipeline(RF_PCA_pip, params_tree, X_train_clean, y_train_clean)\ntree_pca_metrics = apply_model(tree_grid_pca, X_test_std, y_test)","df8b8780":"svm_PCA_ros_pip = Pipeline([('pca', pca),('ros', RandomOverSampler(random_state=rand)),\n                     ('classifier', SVC(random_state = rand))])\nsvm_PCA_pip = Pipeline([('pca', pca),('classifier', SVC(random_state = rand))])\n\nparams_svm = {'classifier__kernel':['linear', 'rbf'], 'classifier__C' : [0.01,0.1, 1, 10], 'classifier__gamma': ['auto','scale']}","1c062629":"svm_pca_ros_metrics = []\nsvm_grid_pca_ros = build_pipeline(svm_PCA_ros_pip, params_svm, X_train_clean, y_train_clean)\nsvm_pca_ros_metrics = apply_model(svm_grid_pca_ros, X_test_std, y_test)","d53e9658":"svm_pca_metrics = []\nsvm_grid_pca = build_pipeline(svm_PCA_pip, params_svm, X_train_clean, y_train_clean)\nsvm_pca_metrics = apply_model(svm_grid_pca, X_test_std, y_test)","e1adc67a":"lr_PCA_ros_pip = Pipeline([('pca', pca),('ros', RandomOverSampler(random_state=rand)),\n                     ('classifier', LogisticRegression(random_state = rand))])\nlr_PCA_pip = Pipeline([('pca', pca),('classifier', LogisticRegression(random_state = rand))])\n\nparams_lr = {'classifier__penalty':['l1', 'l2'], 'classifier__C' : [0.01,0.1, 1, 10]}","8cdb4cd9":"lr_pca_ros_metrics = []\nlr_grid_pca_ros = build_pipeline(lr_PCA_ros_pip, params_lr, X_train_clean, y_train_clean)\nlr_pca_ros_metrics = apply_model(lr_grid_pca_ros, X_test_std, y_test)","0116308b":"lr_pca_metrics = []\nlr_grid_pca = build_pipeline(lr_PCA_pip, params_lr, X_train_clean, y_train_clean)\nlr_pca_metrics = apply_model(lr_grid_pca, X_test_std, y_test)","811632e4":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_PCA_ros_pip = Pipeline([('pca', pca),('ros', RandomOverSampler(random_state=rand)),\n                     ('classifier', KNeighborsClassifier())])\nknn_PCA_pip = Pipeline([('pca', pca), ('classifier', KNeighborsClassifier())])\n\nparams_knn = {'classifier__n_neighbors':[3,5,7,8], 'classifier__p':[1,2]}","ff565988":"knn_pca_ros_metrics = []\nknn_grid_pca_ros = build_pipeline(knn_PCA_ros_pip, params_knn, X_train_clean, y_train_clean)\nknn_pca_ros_metrics = apply_model(knn_grid_pca_ros, X_test_std, y_test)","7753ec3b":"knn_pca_metrics = []\nknn_grid_pca = build_pipeline(knn_PCA_pip, params_knn, X_train_clean, y_train_clean)\nknn_pca_metrics = apply_model(knn_grid_pca, X_test_std, y_test)","cc16a896":"def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n   \n    if axes is None:\n        _, axes = plt.subplots(1, 2, figsize=(20, 5))\n    axes.set_title(title)\n    if ylim is not None:\n        axes.set_ylim(*ylim)\n    axes.set_xlabel(\"Training examples\")\n    axes.set_ylabel(\"Score\")\n    \n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    \n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    axes.grid()\n    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes.legend(loc=\"best\")\n    return plt","9270afe2":"fig, axes = plt.subplots(2, 2, figsize=(17, 10))\n\ntitle = \"Learning Curves (Random Forest) - PCA ROS\"\n\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n\nplot_learning_curve(tree_grid_pca_ros.best_estimator_, title, X_train_clean, y_train_clean, axes = axes[0][0], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\n\ntitle = r\"Learning Curves (SVM) - PCA ROS\"\nplot_learning_curve(svm_grid_pca_ros.best_estimator_, title,  X_train_clean, y_train_clean, axes = axes[0][1], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\n\ntitle = r\"Learning Curves (Logistic Regression) - PCA ROS\"\n\nplot_learning_curve(lr_grid_pca_ros.best_estimator_, title,  X_train_clean, y_train_clean, axes = axes[1][0], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\n\ntitle = r\"Learning Curves (KNN) - PCA ROS\"\nplot_learning_curve(knn_grid_pca_ros.best_estimator_, title,  X_train_clean, y_train_clean, axes = axes[1][1], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\nplt.show()","c5959f5b":"fig, axes = plt.subplots(2, 2, figsize=(17, 10))\n\ntitle = \"Learning Curves (Random Forest) - PCA \"\n\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\nplot_learning_curve(tree_grid_pca.best_estimator_, title, X_train_clean, y_train_clean, axes = axes[0][0], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\n\ntitle = r\"Learning Curves (SVM) - PCA\"\nplot_learning_curve(svm_grid_pca.best_estimator_, title,  X_train_clean, y_train_clean, axes = axes[0][1], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\n\ntitle = r\"Learning Curves (Logistic Regression) - PCA\"\nestimator = lr_grid_pca_ros.best_estimator_\nplot_learning_curve(lr_grid_pca, title,  X_train_clean, y_train_clean, axes = axes[1][0], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\n\ntitle = r\"Learning Curves (KNN) - PCA\"\n\nplot_learning_curve(knn_grid_pca.best_estimator_, title,  X_train_clean, y_train_clean, axes = axes[1][1], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\nplt.show()","f31bf698":"tree_ros_pip = Pipeline([('ros', RandomOverSampler(random_state=rand)),\n                     ('classifier', RandomForestClassifier(random_state = rand))])\ntree_pip = Pipeline([('classifier', RandomForestClassifier(random_state = rand))])\n\nparams_tree = {'classifier__n_estimators': [25,50,100], 'classifier__max_depth' : [5], \n               'classifier__max_features': ['auto',1,2], 'classifier__criterion' :['gini']}","f6c340ae":"tree_ros_metrics = []\ntree_grid_ros = build_pipeline(tree_ros_pip, params_tree, X_train_clean, y_train_clean)\ntree_ros_metrics = apply_model(tree_grid_ros, X_test_std, y_test)","196597be":"tree_metrics = []\ntree_grid = build_pipeline(tree_pip, params_tree, X_train_clean, y_train_clean)\ntree_metrics = apply_model(tree_grid, X_test_std, y_test)","fcc37c21":"importances = tree_grid_ros.best_estimator_[1].feature_importances_\nstd = np.std([importances for tree in tree_grid.best_estimator_[0].estimators_],\n             axis=0)\nindices = np.argsort(importances)\n\np = sns.color_palette('Spectral', len(indices))\n\nplt.figure(figsize = (12,5))\nplt.title(\"Feature importances - RandomForest\")\nplt.barh(range(X.shape[1]), importances[indices], color = 'blue', yerr=std[indices], align=\"center\")\nplt.yticks(range(X.shape[1]), X_train_clean.columns[indices], rotation = 'horizontal')\n# plt.xlim([-1, X.shape[1]])\nplt.show()\n","13934947":"svm_ros_pip = Pipeline([('ros', RandomOverSampler(random_state=rand)),\n                     ('classifier', SVC(random_state = rand))])\nsvm_pip = Pipeline([('classifier', SVC(random_state = rand))])\n\nparams_svm = {'classifier__kernel':['linear'], 'classifier__C' : [0.01,0.1, 1, 10]}","c321a66a":"svm_ros_metrics = []\nsvm_grid_ros = build_pipeline(svm_ros_pip, params_svm, X_train_clean, y_train_clean)\nsvm_ros_metrics = apply_model(svm_grid_ros, X_test_std, y_test)","0fba5e99":"svm_metrics = []\nsvm_grid = build_pipeline(svm_pip, params_svm, X_train_clean, y_train_clean)\nsvm_metrics = apply_model(svm_grid, X_test_std, y_test)","ecffe780":"imp,names = zip(*sorted(zip(svm_grid_ros.best_estimator_[1].coef_[0],X_train_clean.columns)))\ncolors = ['red' if c < 0 else 'blue' for c in imp]\np1 = sns.color_palette('Spectral', len(names))\n\nplt.figure(figsize = (12,5))\nplt.barh(range(len(names)), imp, align='center', color = colors)\nplt.yticks(range(len(names)), names)\nplt.title(\"Feature importances - SVM\")\nplt.legend()\nplt.show()\n","cd0caba9":"lr_ros_pip = Pipeline([('ros', RandomOverSampler(random_state=rand)),\n                     ('classifier', LogisticRegression(random_state = rand))])\nlr_pip = Pipeline([('classifier', LogisticRegression(random_state = rand))])\n\nparams_lr = {'classifier__C' : [0.01,0.1, 1, 10]}","71904ded":"lr_ros_metrics = []\nlr_grid_ros = build_pipeline(lr_ros_pip, params_lr, X_train_clean, y_train_clean)\nlr_ros_metrics = apply_model(lr_grid_ros, X_test_std, y_test)","70872c03":"lr_metrics = []\nlr_grid = build_pipeline(lr_pip, params_lr, X_train_clean, y_train_clean)\nlr_metrics = apply_model(lr_grid, X_test_std, y_test)","f465f566":"imp,names = zip(*sorted(zip(lr_grid_ros.best_estimator_[1].coef_[0],X_train_clean.columns)))\ncolors = ['red' if c < 0 else 'blue' for c in imp]\np1 = sns.color_palette('Spectral', len(names))\n\nplt.figure(figsize = (12,5))\nplt.barh(range(len(names)), imp, align='center', color = colors)\nplt.yticks(range(len(names)), names)\nplt.title(\"Feature importances - Logistic Regression\")\nplt.legend()\nplt.show()","2ab45215":"knn_ros_pip = Pipeline([('ros', RandomOverSampler(random_state=rand)),\n                     ('classifier', KNeighborsClassifier())])\nknn_pip = Pipeline([('classifier', KNeighborsClassifier())])\n\nparams_knn = {'classifier__n_neighbors':[3,4,7,8], 'classifier__p':[1,2]}","26b871ec":"knn_ros_metrics = []\nknn_grid_ros = build_pipeline(knn_ros_pip, params_knn, X_train_clean, y_train_clean)\nknn_ros_metrics = apply_model(knn_grid_ros, X_test_std, y_test)","13b6e319":"knn_metrics = []\nknn_grid = build_pipeline(knn_pip, params_knn, X_train_clean, y_train_clean)\nknn_metrics = apply_model(knn_grid, X_test_std, y_test)","7f8675cc":"fig, axes = plt.subplots(2, 2, figsize=(17, 10))\n\ntitle = \"Learning Curves (Random Forest) - ROS \"\n\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\nplot_learning_curve(tree_grid_ros.best_estimator_, title, X_train_red, y_train_clean, axes = axes[0][0], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\n\ntitle = r\"Learning Curves (SVM) - ROS\"\nplot_learning_curve(svm_grid_ros.best_estimator_, title,  X_train_red, y_train_clean, axes = axes[0][1], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\n\ntitle = r\"Learning Curves (Logistic Regression) - ROS\"\nplot_learning_curve(lr_grid_ros.best_estimator_, title,  X_train_red, y_train_clean, axes = axes[1][0], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\n\ntitle = r\"Learning Curves (KNN) - ROS\"\n\nplot_learning_curve(knn_grid_ros, title,  X_train_clean, y_train_clean, axes = axes[1][1], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\nplt.show()","9df41652":"fig, axes = plt.subplots(2, 2, figsize=(17,10))\n\ntitle = \"Learning Curves (Random Forest) \"\n\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=rand)\nplot_learning_curve(tree_grid.best_estimator_, title, X_train_red, y_train_clean, axes = axes[0][0], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\n\ntitle = r\"Learning Curves (SVM) \"\nplot_learning_curve(svm_grid.best_estimator_, title,  X_train_red, y_train_clean, axes = axes[0][1], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\n\ntitle = r\"Learning Curves (Logistic Regression)\"\nplot_learning_curve(lr_grid.best_estimator_, title,  X_train_red, y_train_clean, axes = axes[1][0], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\ntitle = r\"Learning Curves (KNN)\"\nestimator = knn_grid.best_estimator_\nplot_learning_curve(knn_grid.best_estimator_, title,  X_train_clean, y_train_clean, axes = axes[1][1], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\nplt.show()","97329017":"explainer = shap.KernelExplainer(lr_grid_ros.best_estimator_[1].predict_proba, data = X_train, link = 'logit')\nshap_values = explainer.shap_values(X_test, n_samples = 50)","5f2463c4":"print(f'length of SHAP values: {len(shap_values)} as the number of classes') #as the number of classes","7f9464c6":"shap.initjs()\nshap.force_plot(explainer.expected_value[0], shap_values[0][12,:], features = X_test.iloc[12,:], link = 'logit')","d49c4890":"shap.force_plot(explainer.expected_value[1], shap_values[1][12,:], features = X_test.iloc[12,:], link = 'logit')","0e5e5d86":"explainer = shap.KernelExplainer(lr_grid_ros.best_estimator_.predict_proba, data = X_train_clean, link = 'logit')\nshap_values = explainer.shap_values(X_test_std, n_samples = 50)","c0739192":"shap.summary_plot(shap_values[0], X_test_std)","67c23752":"metrics = pd.DataFrame(np.array([tree_pca_ros_metrics, tree_pca_metrics, tree_ros_metrics, tree_metrics,\n                                svm_pca_ros_metrics, svm_pca_metrics, svm_ros_metrics, svm_metrics,\n                                lr_pca_ros_metrics, lr_pca_metrics, lr_ros_metrics, lr_metrics,\n                                knn_pca_ros_metrics, knn_pca_metrics, knn_ros_metrics, knn_metrics]).reshape(4, 16),\n                       index = ['Accuracy', 'F1_score', 'Specifivity', 'Sensitivity'],\n                       columns = ['RandomForest PCA ROS', 'RandomForest PCA', 'RandomForest ROS', 'RandomForest',\n                                'SVM PCA ROS', 'SVM PCA','SVM ROS','SVM',\n                                'LogisticRegression PCA ROS','LogisticRegression PCA',\n                                'LogisticRegression ROS','LogisticRegression',\n                                 'KNN PCA ROS', 'KNN PCA', 'KNN ROS','KNN']\n                      )\npx.bar(metrics, barmode = 'group', height = 450, width = 1000)","55294ee2":"# 5.2.1 Feature importance - SVM \nIt is possible to determine which are the most contributing features in this classification problem by observing the returned coefficients. However this is possible only when the kernel is Linear, because, for other kernels such as rbf, data are transformed by a kernel method to another space, which is not related to input space.\nIn fact, in linear SVM the separating hyperplane is in the same space as the input features so that we can view its coefficients as weights of the input \"dimensions\".\n\nThen, once a linear SVM is trained, the coefficients can be accessed from the model by using svm.coef_. Recalling that a linear SVM creates a hyperplane that uses support vectors to maximise the distance between the two classes, the weights obtained from the coefficients represent the vector coordinates which are orthogonal to the hyperplane and their direction indicates the predicted class. The absolute size of the coefficients in relation to each other can then be used to determine feature importance for the data separation task.","90675265":"Below each attribute is analyzed in each label: ","10fead4d":"# 1. Dataset Preparation\nFirst, I checked for missing values. No missing values were found.\nImportant note, by doing some exploration over the data I noticed that many of the parameters that I would expect from my basic knowledge of heart disease and ECG analysis to be positively correlated, were actually pointed in the opposite direction, such as the level of cholesterol for healthy people were in general higher than for unhealthy peopele, or the fact that the younger a patient the more high the risk of observing a heart disease. \nTherefore I assumed that the target variable was reversed.  \n","6b179318":"This simple representation shows the percentages of each category for every categorical variable. It can be helpful to understand how many samples related to each category are there in the dataset.","eae7d0e5":"# 5. No dimensionality reduction\nIn this case I will experiment the same algorithm. Contrarily to the previous experiment I will no exploit the Principal Component Analysis, this is because in this experiment I want to mantain a certain degree of interpretability in order to give an intuitive interpretation of the model exploiting Feature Importances methods and the SHAP values, in order to interpret the impact of having a certain value for a given feature in comparison to the prediction we'd make if that feature took some baseline value.","c88872cb":"# 2. Exploratory Data Analysis\nIn statistics, exploratory data analysis (EDA) is an approach to analyze data sets in order to summarize\ntheir main characteristics, often with visual methods. Primarily EDA is exploited in order to capture \nsome information about the distribution of its components by using visual tools and statistical tools. \nThe goal of EDA is to give a clear and understandable description of the data.\nI decided to split the analysis into two parts: one related to the *quantitative* or numerical attributes, and one related to the *qualitative* or categorical attributes.\nWe can define categorical whose values are taken from a defined set of values, such as boolean values,and numerical features are instead features whose values are continuous or integer-valued. They are represented by numbers and possess most of the properties of numbers. \n\nIn order get an idea of how data are distributed, below some statistics about the numerical features:\n* Count: the number of samples\n* Mean: the average value\n* Std: the deviation from the mean value\n* Min: the minimal value\n* Max: the maximal value\n* Quartiles: Indexes that divide in four equal parts the data distributions, in particular\n25%: the lower quartile of the distribution\n50%: the median quartile\n75%: the upper quartile of the distribution**","209367f4":"### Random Forest PCA","e5255464":"A standard method used to evaluate the number of components that should be retained, is the one related to the cumulative variance. The cumulative percentage of total variance is the percentage of variance explained by the first $m$ components.\nIf $l_k$ is the variance of the $k^{th}$ Principal Component then $m$, number of components, is chosen to be the smallest $m$ such that $$t_m = \\frac {\\sum \\limits_{k=1}^{m}l_k}{\\sum \\limits_{k=1}^{N}l_k}$$\nis between 70% and 90%.\n\n\nHence I first draw the graph of the cumulative Explained Variance Ratio of transformed training data and transformed dimensions. What we can understand from the graph is that if we want to retain at least 70% of variance explained we have to keep 10 features. Which means that with a halved number of features we should still mantain much of the information retained in the original dataset. \n\n\n*[How Many Components should be Retained from\na Multivariate Time Series PCA? Alethea Rea1 and William Rea2, 1. Centre for Applied Statistics, University of Western Australia, Australia 2. Department of Economics and Finance, University of Canterbury, New Zealand October 21, 2016\narXiv:1610.03588v2]*","79dc76bf":"# 5.3 Logistic Regression","46675f00":"### Learning Curves without any preprocessing technique","51bc28b5":"# 3. Data Preprocessing\nData preprocessing is mainly the most important step in a Machine Learning pipeline. \nIn this phase raw data are transformed into model-suitable data.","cc1e6338":"### Learning Curves PCA","164830fe":"# 3.4 PCA - Dimensionality Reduction as feature Extractor\nIn the previous part we already knew what was the number of components in which to reduce the data, but if we want to reduce the dataset in order to retain a certain amount of variance explained we need to assess how much variance is explained by each attribute of the dataset. For this purpose I exploited the visualization tool *Pareto diagram* in order to understand what is a good trade off for the number of components.","93f0d170":"By observing the boxplots we can assume that for many attributes there are not many significative differences between the distributions associated with the target variables, such as the Resting blood pressure or the level of cholesterol (which is by the way slightly higher for people who suffer from heart disease). However the Heart Rate achieved by healthy people is way more high than the heart rate achieved by ill people. ","a9c1ee95":"Below histograms and boxplots sharing x-axis:","b7ba6be0":"### Random Forest PCA + RandomOverSampler","c9c17de4":"# 3.3 Outlier detection\n\n*\u201cObservation which deviates so much from other observations as to arouse suspicion it was generated by a different mechanism\u201d \u2014 Hawkins(1980).*\n \nOutliers are extreme values that deviate from other observations on data, they may indicate a variability in a measurement, experimental errors or a novelty. In other words, an outlier is an observation that diverges from an overall pattern on a sample.\nOutliers can come from\n* Data Entry Errors: Human errors such as errors caused during data collection, recording, or entry can cause outliers in data.\n* Measurement Error: It is the most common source of outliers. This is caused when the measurement instrument used turns out to be faulty.\n* Natural Outlier: When an outlier is not artificial (due to error), it is a natural outlier. Most of real world data belong to this category.\n\nI will present two methods for outlier detection:\n1. outlier detection based on IQR\n2. outlier detection exploiting Isolation Forests","ff00a963":"Overall the data distribution can be considered normal. Only the ST depression attribute has a particular skewed distribution.","d52b818b":"Is it possible to observe that:\n* Random Forest algorithm suffers from poor generalization. The gap between the training curve and the validation curve is very large (in validation it looses about 15% in accuracy). Wheter in the SVM and the Logistic Regression the amount of overfitting is negligible. \n* Adding more samples enhance the performances of the algorithm visibly for the Logistic Regression\n* KNN suffers from overfitting","072a05a4":"# 6. Explanation of the model - KernelSHAP - Logistic Regression\nInterpretability is the degree to which a human can understand the cause of a decision.\nInterpretability of a model is required in many cases in order to understand why a certain prediction is made by the model itself. Not only it can be helpful to detecting biases at the time of model development, but in many application the model requires a high building trust. In case of data driven method to medical decision, is preferred to have an explanation of what's happening in the black box. For this purpose the SHAP method is introduced.\n\nThe question that SHAP approach want to answer is, for an instance $x$, with a predicted output $y$, what made the model do this decision? In the case of this Heart Disease dataset, \nif a patient has been classified as healthy, what is the contribution of each feature to this prediction?\n\n\nSHAP (SHapley Additive exPlanation) is an approach that comes from game theory. The goal of SHAP is to explain the prediction of any instance $x_i$ by computing the sum of the contributions from each individual feature to the prediction and then compute the average of the marginal contributions across all the possible permutations. In SHAP approach each feature is supposed to be in a cooperative game, and in this setting Shapley values provide a means to fairly distribute the contributions across the features.\n\nOne innovation that SHAP brings to the table is that the Shapley value explanation is represented as an additive feature attribution method, a linear model.\n\n\n\nIn other words, the feature values of a data instance act as players in a coalitional game. Shapley values tell us how to fairly distribute the prediction among the features. A *player* can be an individual feature value or can also be a group of feature values. In the context of a machine learning model, individual feature values of the instance $x_i$ are the players and the prediction $y_i$ minus the average prediction for whole training data is the *payout*. However the exact computation of the Shapley values can be quite challenging, since, if there are $n$ *players*, then $n$-factorial possible sequences are considered, therefore there are some methods that apply to machine learning algorithms that optimize these computations.\n\nKernel SHAP is a model agnostic method to estimate SHAP values for any model. Because it makes no assumptions about the model type. \nFor this purpose I use SHAP framework, that enables any Machine Learning algorithm to essentially become \u201cexplained\u201d by providing intuitive and interactive visualizations that aim at showing which features are more relevant for a certain prediction and for the model as a whole.\n\n![image.png](attachment:image.png)","ebd52e5d":"# 2.4 EDA for categorical variables","48728473":"### SVM  PCA + RandomOverSampler","a628c56e":"# 5.3.1 Features Importance - Logistic Regression\n\nFor each class, the vector of coefficients $\\beta_1, \\beta_2 ... \\beta_k$ can be used to interpret the model globally; in the absence of interaction terms, the coefficient of a predictor (i.e., independent variable) represents the change in log odds when the predictor changes by one unit while all other variables are kept at fixed values. Equivalently, the exponentiated coefficient is equivalent to a change in odds. Since the transformation from odds to outcome probabilities is monotonic, a change in odds also implies a change in the outcome probability in the same direction. Thus, the magnitudes of the feature coefficients measure the effect of a predictor on the output and thus one can globally interpret the logistic regression model.","b6449816":"# 4.5 Learning Curves\n\nLearning Curves are a useful visualization tool that shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much we benefit from adding more training data and whether the estimator suffers more from a variance error or a bias error, hence from overfitting or underfitting.\n\nThe bias-variance dilemma is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set. Therefore a *high bias* can cause an algorithm to miss the relevant relations between features and target outputs (underfitting), whether a high variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting).\n\nBias and variance are intrinsic properties of an estimator and it can be helpful, in order to select learning algorithms and hyperparameter in order to have both bias and variance are as low as possible, to have a visualization of these curves. The Train Learning Curve can indicates how well the model is learning, wether the Validation Learning Curve shows how well the model is generalizing. \n\n","f3ab6506":"# 3.3.4 PCA - Visualization and Feature Extraction\n\nPCA is an unsupervised algorithm that is used in the machine learning context to achieve dimensionality reduction as a tool for *data visualization* (to visualize the data for exploratory analysis by mapping the input\ndata into two- or three-dimensional spaces) or as a tool for *feature extraction* (to generate a smaller and more effective set of features).\n\nIn fact it is often desirable to find a lower dimensional representation preserving some of the main properties of the original dataset. This technique aims to alleviate the *curse of dimensionality*, since when we observe a very high-dimensional feature space or when there are few data points with respect to the number of features it may happen that that these data points result to be very sparse in the feature space. This causes every observation appearing equidistant from all the other data points making distance-based algorithm, such as SVM, meaningless. Moreover, there is a huge risk of overfitting. Then, dimensionality reduction techniques such as PCA can handle these issues by mapping the high-dimensional data into a lower-dimensional space.\n\nTherefore this technique allows us to find the best representation of the given dataset in a lower subspace, by projecting data samples onto the found principal components. \n\nIn this part of the project I exploit the Principal Component Analyisis in order to obtain a clear visualization in the feature space of the outliers individuated by the Isolation Forest algorithm.\nIn the next part I will exploit PCA to reduce my dataset to keep only meaningful features for the classification task.\n\n\n**How PCA works?**\nLet $x_1,...,x_m$ be m vectors in $R^d$. We would like to reduce the dimensionality of these vectors using a *linear* transformation. A matrix $W\u2208R^{n,d}$, where $n < d$, induces a mapping $x \u2192 Wx$, where $Wx$ is the lower dimensionality representation of $x$. Then, a second matrix $U\u2208R^{d,n}$ can be used to (approximately) recover each original vector $x$ from its compressed version. $W$ is a *compression* matrix, $U$ is a *recovery* matrix.\n\nThat is, for a compressed vector $y = Wx$, where $y$ is in the low dimensional space $R^n$, we can construct $x\u0304 = Uy$, so that $x\u0304$ is the recovered version of $x$ and resides in the original high dimensional space $R^d$.\nIn PCA, we aim to find the compression matrix $W$ and the recovering matrix $U$ so that the total squared distance between the original and recovered vectors is minimal:\n\n$$\\underset{W\u2208R^{n,d},U\u2208R^{d,n}}{\\operatorname{arg min}} \\sum_{i=1}^{m}{|| x_i - UWx_i||^2}_2$$\n\nThis means minimizing the reconstruction error.\n\nIt can be proved that the below formulation is equivalent to the first one:\n    $$\\underset{U\u2208R^{d,n}:U^{T}U=I}{\\operatorname{arg max}} trace(U^T\\sum_{i=1}^{m}{(x_ix_j)^TU})$$\n\n\nSo the problem can be riformulated by letting $A = \\sum_{i=1}^{m}{x_ix_i^T}$ and letting $u_1,...,u_n$ being $n$ eigenvectors of the matrix $A$ corresponding to the largest $n$ eigenvalues of $A$. \n\nThen, the solution to the PCA optimization problem is to set $U$ to be the matrix whose columns are $u_1,...,u_n$, $U = [u_1,...,u_n]$, and to set $W = U^T$.\n\n$U$ contains the so called principal components.\nIt is good practice to remove the mean from the data before applying PCA, so that we have $(x_1-\\mu), (x_2-\\mu)...(x_m-\\mu)$. Then the problem can be interpreted as a Variance Maximization, in which the PCs captures a certain amount of variance in the data. \n\nIn other words, PCA technique projects onto the subspace of maximal variance and therefore the first principal Component can be derived from the projection onto the direction of maximal variance. \nAfter, in hierarchical order, the other principal components and the amount of variance that they capture in the data. In other terms the first $n$ principal components of PCA can be seen as the largest $n$ eigenvectors extracted from the covariance matrix $\u03a3$, $\u03a3 = XX' = \\sum_{i=1}^{m}{x_ix_i^T}$ built from data vector $X$, $(x_1-\\mu), (x_2-\\mu)...(x_m-\\mu)$.\n\nWe can say that the PCA provides us a hierarchical data driven coordinate system (each component is orthogonal to the other) to represent the statistical variation in the data, built on the directions that captures the maximum amount of variance in the data.","60382875":"The two force plots are associated with the same input, but separated for each target:\nthe first one explains that the output probability of the input belonging to class = 0 is 0.06, and in order to make that decision the values in red have positively contributed to return a very low probability of being in class 0, and the values in blue has negatively contributed (make the probability lower). Therefore they give us an explanation on \"why\" this patient has a low risk of heart disease. In the other graph explained how each feature has contributed negatively to the classification, therefore \"why\" this patient is instead classified as having a high risk.","60bd995d":"# 2.2 Sex and Age - an insight\nSex and age are two variables that  can affect primarily the insurgence of heart disease. \nThe graph is showing the distribution of the Genders with respect to the target variable. At first glance we can notice that females are way less subject to heart disease than males, considering that in the dataset there are 31.7% samples associated with females and 68.3% samples associated with males.","46f395bb":"### KNN PCA","2f75dc2e":"# 2.3 EDA on numerical attributes\nAs previously said, this dataset contains both categorical and numerical attributes that have to be treated and analyzed separately. \nThe first group of attributes is formed by variables that are Integer\/Real-valued therefore *quantitative*.\n\nTo examine the distribution of the numeric attributes in each label, Histograms, Boxplots and Pairplots were exploited.\n### Boxplot\nboxplots are tools to display the data distribution based on a five number summary (\u201cminimum\u201d, first quartile (Q1), median, third quartile (Q3), and \u201cmaximum\u201d).\n\n* median (Q2\/50th Percentile): the middle value of the dataset.\n* first quartile (Q1\/25th Percentile): the middle number between the smallest number (not the \u201cminimum\u201d) and the median of the dataset.\n* third quartile (Q3\/75th Percentile): the middle value between the median and the highest value (not the \u201cmaximum\u201d) of the dataset.\n* interquartile range (IQR): 25th to the 75th percentile.\n* \u201cmaximum\u201d: Q3 + 1.5*IQR\n* \u201cminimum\u201d: Q1 -1.5*IQR\n* The spacings between the different parts of the box indicate the degree of dispersion (spread), skewness in the data, and show outliers. In fact, Box plot are not only excellent tools to understand the distribution of an attribute but also for detecting outliers. \n\n\n### Histograms\nHistograms are the most commonly used graphs to show the underlying frequency distribution (shape) of a set of continuous data. This allows the inspection of the data for its underlying distribution (e.g., normal distribution), outliers or skewness. \n\n\n### PairPlot\nPairPlots are useful tools provided by Seaborn. A pairplot plot a pairwise relationships in a dataset. The pairplot function creates a grid of Axes such that each variable in data will by shared in the y-axis across a single row and in the x-axis across a single column.","00225396":"# 4. Model implementation and selection\nthe models I will use are the \n* Random Forest algorithm\n* Support Vector Machine Classifier\n* Logistic Regression algorithm \n* KNN\n\n\nIn the first part I will experiment the behaviour of the algorithms using the reduced dataset, in the second part\nI will not use the reduced dataset but the complete dataset in order to analyze two possible uses of the dataset itself:\nDimensionality reduction make computation faster and usually more performant, but we lose interpretability of the features.\nIn case of medical\/clinical data, as previously said, readability and interpretability are two main characteristic.\nI will experiment if in this case there are any advantages in implementing dimensionality reduction or it is better to guarantee interpretability of the model.\n\n## **Random OverSampling**\nAs shown previously I started with a dataset that was slightly imbalanced. Since this dataset is actually a small dataset, removing outliers had however some impact on the imbalance of the dataset. Therefore I decided to apply a very easy and robust technique in order to re-balance the dataset. I decided to use Random Oversampling because we only need to introduce a small amount of samples in the dataset.\n\nWhat Random Oversampling is about? It provides a naive technique for rebalancing the class distribution for an imbalanced dataset by randomly duplicating examples in the minority class. Random Oversampling is referred to as \"naive resampling\" method because it assume nothing about the data and no heuristics are used. \nThis makes this technique intuitive, simple to implement and fast to execute, which is somehow desirable in some cases. However abusing of this technique may lead to extreme overfitting. I assumed that since the imbalance is very tiny, it won't affect too much the generalization ability of the algorithms.\n![image-5.png](attachment:image-5.png)\n\n## **Model Selection and Hyperparameter tuning**:\nFor each model I tuned the hyperparameters by exploiting the ***GridSearchCV*** tool. In order to perform an exhaustive search of the right hyperparameters I implemented a ***GridSearch that exploit a Stratified K fold Cross Validation Splitting strategy*** (variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class). Cross validation is one of the best method to evaluate the performances of a model and consequently choose a determined model guaranteeing the right variance-bias trade off. It operates by splitting the training data into $K$ subsets, using $K-1$ of them in order to train a model and validate it on the remaining subset. $K$ fold cross validation is a computationally expensive algorithm that cannot be applied in case very large dataset.  \n\n\n![image-6.png](attachment:image-6.png)\n\n\n## **Performance evaluation**: \nIn order to assess the performance of the classification algorithm I exploit the below cited metrics. In particular F1-score, Sensitivity, Specificity and Accuracy. By defining:\n* True Positive (TP): Correctly predicted positive values. Both of predicted and actual values are\npositive.\n* True Negative (TN): Correctly predicted negative values. Both of predicted and actual values are\nnegative.\n* False Positive (FP): Mispredicted positive values. When actual class is negative and predicted class\nis positive.\n* False Negative (FN): Mispredicted negative values. When actual class is positive but predicted\nclass in negative.\n\n\n### Accuracy\nIt is one of the most used metrics for evaluating classification models:  \n$\\\\Accuracy = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$\nA classification model\u2019s accuracy is defined as the percentage of predictions it got right. However, it\u2019s important to understand that it becomes less reliable when there is a severe imbalance between the classes, becoming less ideal as a stand-alone metric.\n\n### Precision\nPrecision attempts to answer the following question: What proportion of positive identifications was actually correct?\nPrecision is a measure that tells what proportion of patients that has been diagnosed as having heart disease, actually had heart disease.\nComputed as: \n$\\\\Precision = \\frac{\\text{TP}}{\\text{TP + FP}}$\n\n\n### Recall (or Sensitivity)\nRecall attempts to answer the following question: What proportion of actual positives was identified correctly? \nRecall is a measure that tells us what proportion of patients that actually had heart disease was diagnosed by the algorithm as having heart disease.\nComputed as: \n$\\\\Recall = \\frac{\\text{TP}}{\\text{TP + FN}}$\nThe true positive ratecorresponds to the proportion of positive data points that are correctly considered as positive, with respect to all positive data points.\n\n\n### F1-score\nThe *F1-score*  can be interpreted as the harmonic mean of the precision and recall, it tells how precise your classifier is (how many instances it classifies correctly), as well as how robust it is (it does not miss a significant number of instances). The formula for the F1 score is: \n$\\\\F1 score = 2*\\frac{\\text{(precision * recall)}}{\\text{(precision + recall)}}$\n\n\n### Specificity \nSpecificity is also called False positive rate and corresponds to the proportion of negative data points that are mistakenly considered as positive, with respect to all negative data points. Specificity in fact, is a measure that tells us what proportion of patients that did NOT have a heart disease, were predicted by the model as NOT having a heart disease.\n$\\\\Specificity = \\frac{\\text{FP}}{\\text{FP + TN }}$\n\n\n### Confusion matrix \nThe confusion matrix can be used as a visual tool in order to have a more clear and unserstandable view of the performances of the algorithm. \n![image-3.png](attachment:image-3.png)\n\n\n","11b3287e":"# 5.2 SVM - no PCA","723d444e":"### SVM + Random OverSampler","57cf82c0":"# 3.2 Train Test Split and Standardization\nMachine Learning Algorithms have to be first trained on a subset of data called *train set* and then tested in order to assess the performances of the algorithm itself. Training data is data used to let the machine learning algorithm to build the model. The model tries to learn the dataset and its characteristic in order to be able to perform well on unseen data or *test data*. Test data is the part of the dataset used to test our model hypothesis. This is a keypoint, the test data must not be seen by the model, or the model will overfit on them.\nAs previously said test data is left untouched until the model and hyperparameters are decided, and it's only when the model is applied on them that we can get an accurate measure of how it would perform when deployed on real-world data.\n\nIn order to do that, data is split into the two parts by specifying the parameter *split ratio* which is highly dependent on the type of model we are building and the dataset itself. If our dataset and model are such that a lot of training is required, then we use a larger chunk of data just for training purposes. In this case I set the split ratio at 0.80, so that 80% of the data goes to train set and 20% of the data goes on test set. ","f0d31990":"### KNN + Random OverSampler","602cf1cc":"# 3.3.2 Outlier detection with Isolation Forests\nIsolation Forest is one of the most efficient algorithms for outlier detection especially in high dimensional datasets.\nIsolation forest is an unsupervised learning algorithm to identify multivariate anomalies in the dataset and it is based on the binary decision Tree.\n\nIts basic principle is that outliers are few and far from the rest of the observations: anomalies are more susceptible to isolation under *random* partitioning. \n\nLet $X$ = { $x_1, ..., x_n$ } be a set of d-dimensional points. An Isolation Tree (iTree) is defined as having the following properties:\n* for each node T in the Tree, T is either an external-node with no child, or an internal-node with one condition and exactly two child nodes, $T_{left}$ and $T_{right}$.\n* a *condition* at node T consists of an attribute $q$ and a split value $p$ such that the test $q < p$ determines wheter  a data point \"goes\" to either $T_{left}$ or $T_{right}$.\n\nIn order to build an iTree, the algorithm recursively divides $X$ by randomly selecting an attribute $q$ and a split value $p$, until the node has only one instance or all data at the node have the same values.\n\nWhen the iTree is fully grown, each point in $X$ is isolated at one of the external nodes. Intuitively, the anomalous points are those (easier to isolate, hence) with the smaller path length in the tree, where the path length $h(x_i)$ of point \n$x_i \u2208 X$ is defined as the number of edges $x_i$ traverses from the root node to get to an external node.\n\nIn other words, during training, Trees are constructed by recursively *randomly* partitioning the given training set until instances are isolated or a specific tree height is reached.\nOnce the iterations are over,an anomaly score for each point\/instance is generated, suggesting its likeliness to be an anomaly. The anomaly score is computed as:\n $$s(x,m) = 2^{-\\frac{E(h(x))}{c(m)}}$$\n\nWhere $E(h(x))$ is the average value of $h(x)$ from a collection of iTrees, $c(n)$ a normalization factor.\nThe anomaly score is strictly related to the deepness of the path that a data point have created.\nIn fact, data points that have been isolated in few iterations are more likely to be outliers. \nFor any given instance $x_i$:\n\n* $s > 0.5$ means that $x_i$ is very likely to be an *anomaly*\n* $s < 0.5$ means that $x_i$ is very likely to be a *normal value*\n* $s = 0.5$ means that it is safe to assume that the sample doesn't have any anomalies\n\nFor each tree, the following steps are evaluated:\n1. Get a random sample of the data\n2. Randomly select a dimension\n3. Randomly pick a value in that dimension\n4. Draw a *straight line* through the data at that value and split data\n5. Repeat until tree is complete\n6. Generate an ensemble of trees\n\n![image.png](attachment:image.png)\nAn important hyperparameter is the *Contamination*, this is a parameter that the algorithm is quite sensitive to, in fact it refers to the expected proportion of outliers in the data set.\n\n*Isolation Forest, Fei Tony Liu ; Kai Ming Ting ; Zhi-Hua Zhou DOI: [10.1109\/ICDM.2008.17]*","fd0dc9e7":"# 4.4 KNN\nThe K-Nearest Neighbors algorithm is a supervised learning algorithm that is often used for classification. The classification is based on the calculation of the distance ( Euclidean distance is the most used) between a new data point and all the other already in the training dataset. The class of the K-nearest-neighbor will determine the class of the new data point. KNN is a proper \u201cdistance based\u201d classifier. \nThere are many possible distance functions that can be used to compute the distance such as Manhattan distance and Euclidean distance.\n* Manhattan distance: $$\\sum_{i=1}^{k}{|x_i - y_i|}$$\n\n* Euclidean distance: $${\\sqrt{\\sum _{k=1}^{n}{(x_{k}-y_{k})^{2}}}}$$\n\nIf K = 1, then the instance is simply assigned to the same class of its nearest neighbor.\nIn addition to its simplicity there is the fact that there are few hyperparameters that have to be tuned (K value and distance function) in fact KNN is a nonparametric algorithm because it avoids a priori assumptions about the shape of the class boundary and can thus adapt better to nonlinear boundaries as the amount of training data increases. Furthermore KNN does not need a proper training phase, since it uses all the instances of the dataset in order to classify a data point. In fact KNN in test phase will require all the training dataset to compute for every new data point the distance from all the train data and so to implement a classification rule.\n\nGiven a K value and a distance metric the algorithm computes:\n* For each point $x_i$ in the test data, the distance between the instance and each point of training data\n\n* Then based on the distance, sort the distances in ascending order\n* Consider the first K computed distances\n* Assign a class to the test point $x_i$ based on majority voting.\n![image.png](attachment:image.png)\n\nBeyond the distance metrics, the K parameter has to be correctly tuned since if we set K = 1, we let the algorithm consider, for each new point that has to be classified, only the class of its only nearest neighbor, in this case the decision boundary (defined as a surface that separates points belonging to different classes) is very irregular and granulated because we are forcing the classifier to restrict at the minimum the region of a given prediction. Under this condition, every single point creates around itself a little \u2018island\u2019 of likely incorrect decisions, also points in the middle of other \u2018clusters\u2019, and this often means that the algorithm suffers of poor generalization capacities. On the other hand if K is too big (also with respect to the size of the dataset) there is a high risk of eliminating some important details, so there is the risk of a too high level of generalization.","03e58c1e":"What we can understand from the correlation matrix is that our numeric attributes are not heavily correlated between them. None of them in fact shows a Correlation coefficient > 0.5 (meaning stron positive correlation).\nEach of them may be useful in classification task, carrying its own amount of information.\nFurthermore, there are some interesting correlations with the target variable: the most correlated is the variable related to the shape (depression) of ST segment in the ECG wave. Probably many ECG traces showed a very heavy deformation of the ST wave concurrently to a possible heart disease. \n \n\n\nThe Pearson correlation matrix can help us to understand that maybe we probably must include the majority of all the features.\nTherefore,\n* data are very less correlated between them, so this means that in our model we will need the majority of them \n* The max heart rate achieved shows a positive correlation with the target meaning that lower heart capacity can be a sign of heart disease \n* Then the shape of the ST segment may be significative in identifying a heart failure \n* The heart rate *variability* is negatively correlated to the possibility of heart disease. In literature, a high heart variability is often a sign of healthy heart.","06fbd5ca":"### Logistic Regression  PCA  + Random OverSampler","a9fb6675":"### Random Forest + Random OverSampler","0f4e8741":"# 3.1 Feature Encoding\nFeature encoding is basically given by transformations on the data such that it can be easily accepted as input for machine learning algorithms while still retaining its original meaning.\nIn this case I encoded the categorical variables using dummy variables and dropping the first one.\nDummy coding is very useful since it allows to turn categories into something that the model can treat as having a high (1) and low (0) score.","b6cef3b7":"# 6.2 Summary Plot\nThe chart below provide a visualization of a general evaluation of feature relevance on the model as a whole. \nThe summary plot is built by considering:\n1. Feature importance: Variables are ranked in descending order.\n2. Impact: The horizontal location shows whether the effect of that value is associated with a higher or lower prediction.\n3. Original value: Color shows whether that variable is high (in red) or low (in blue) for that observation.\n4. The x-axis represents the SHAP value, which is the impact on the model output.\n\n\nThe first chart has some additional information, since it considers the feature relevance for each individual prediction (depicted as a point). Also, for each feature, it shows how a higher or lower value of that feature influences the fact that it agrees or disagrees with the prediction. The summary plot instead gives us a general overview on how each feature contributes in general to the model.\n\n","75c259d3":"# 5.5 Learning Curves","a84b1300":"# 3.3.1 Outlier detection based on IQR\nA commonly used rule says that a data point is an outlier if it is more than 1.5 * IQR above the third quartile or below the first quartile. Said differently, low outliers are below Q1 \u2013 1.5 * IQR and high outliers are above Q3 + 1.5 * IQR\nRouglhy speaking, the interquartile range can be used to detect outliers. Even if it is a robust technique, we have to notice that the interquartile range allow us to identify univariate outliers, so a data point that consists of an extreme value on one variable.  A multivariate outlier is instead a combination of unusual scores on at least two variables. \nThe procedure is:\n1.\tCalculate the interquartile range for the data.\n2.\tMultiply the interquartile range (IQR) by 1.5 (a constant used to discern outliers).\n3.\tAdd 1.5 x (IQR) to the third quartile. Any number greater than this is a suspected outlier.\n4.\tSubtract 1.5 x (IQR) from the first quartile. Any number less than this is a suspected outlier.\n\nIn figure below, the boxplots evidences the presence of several outliers that can be therefore easily detected and removed by exploiting the IQR. \nHowever this technique allow us to identify outliers that are 'univariate', so outliers that can be found when looking at a distribution of values in a single feature space.","6aaad929":"Then, since for medical data, interpretability and readability are two of the most important characteristic, I converted the numeric values associated with categorical variables into descriptive values based on the description of the dataset previously mentioned. \n[https:\/\/archive.ics.uci.edu\/ml\/datasets\/heart+disease](http:\/\/)","fb9973a7":"# 4.3 Logistic Regression\nLogistic Regression is a generalized linear model for classification that tries to assign a label by exploiting the probability that a certain sample belongs to a certain class.\nLogistic Regression is mainly used to perform binary classification tasks being a more appropriate approach than simple linear regression.\nRather than fitting a line, logistic regression fits a non-linear function, the Sigmoid function, in order to have as a result a value between 0 and 1, that can be interpreted as class probability, more precisely, conditional probability that the resulting class is 1. Therefore the class is assigned if the output probability is greater\/lower than a *threshold* (usually 0.5).\n\n$P(x) = P(Y = 1 | x;w)$ is the output probability, describing the conditional probability that a sample $x$ \"belongs\" to class 1 and $h_w(x) = \\frac{e^{\u3008w, x\u3009}}{1 + e^{\u3008w, x\u3009}}$ is the logistic function.\n\nThe logistic regression model is described by the equation:\n$$P(x)  =  \\frac{e^{\u3008w, x\u3009}}{1 + e^{\u3008w, x\u3009}} = \\frac{e^{\\beta_0 + \\beta_1x}}{1 + e^{\\beta_0 + \\beta_1x}}$$\n\nIn which the $\u3008w, x\u3009$ is a compact representation of the linear combination of $\\beta$ and $x$.\n\n![image-2.png](attachment:image-2.png)\n\n\nTherefore the linear relationship is expressed between the logits of the output and the parameters $$logit(p(x)) = log(\\frac{p(x)}{1 - p(x)}) = \u3008w, x\u3009$$ \n\nthat are estimated by exploiting the maximum likelihood estimation.\n\nSince the MLE gives the probablity of the observed classes, this means that the parameters $w$ are chosen in order to maximize it. However, the problem of Maximizing the Likelihood is equivalent to minimizing the *logistic loss function*\n$$L(y_i, x_i; w) = log(1 + e^{-y_i\u3008w, x_i\u3009})$$\n\nFor correctly classified points $\u2212y_i\u3008w, x_i\u3009$ is negative, and $log(1 + e^{-y_i\u3008w, x_i\u3009})$ is near zero.\nFor incorrectly classified points $\u2212y_i\u3008w, x_i\u3009$ is positive, and $log(1 + e^{-y_i\u3008w, x_i\u3009})$ can be large.\n\n\nThen the following optimization problem has to be solved:\n$$\\underset{W\u2208R^{d}}{\\operatorname{arg min}} \\frac{1}{m}\\sum_{i=1}^{m}{L(y_i, x_i; w)}$$\n\nFor logistic regression, this loss function is conveniently convex, therefore the problem can be solved by using methods such as gradient descent.\n\nAlso in this case, C is introduced as Regularization parameter and, more precisely, the smallest the value of C, the stronger the regularization.","4fe50036":"I decided to apply Isolation Forest algorithm in order to remove outliers. ","7bb2974c":"Pearson's correlation coefficient is a statistic that measures linear correlation between two variables X and Y and it is computed as the covariance of the two variables divided by the product of their standard deviations. \nStandard Deviation measures the absolute variability of a datasets\u2019 distribution.\nWhen dividing covariance values by the standard deviation, we essentially scale the value\ndown to a limited range of [-1, +1].\n> $$\\rho = \\frac{\\text{cov}(X,Y)}{\\sigma_x \\sigma_y}$$\n\nA correlation coefficient of 1 means that for every positive increase in one variable, there is a positive increase of a fixed proportion in the other.\nA correlation coefficient of -1 means that for every positive increase in one variable, there is a negative decrease of a fixed proportion in the other.\nZero means that for every increase, there isn\u2019t a positive or negative increase. The two just aren\u2019t related by a linear relationship.","b45d46bd":"# 5.1 Random Forest - no PCA","3ff4406f":"# 2.3.4 Correlation - numerical Attributes\nIn order to understand the relationships between the attributes Correlation Matrix is used.\nComputing the correlation matrix means to compute the Pearson Correlation coefficient between every pair of attributes.\nIn fact correlation is a technique for investigating the relationship between two quantitative, continuous variables (i.e. levels of cholesterol and age) and Pearson's correlation coefficient is a measure of the strength of the association between the two variables.\n ","95494c17":"# 2.1 Labels distribution\nOne of the first most important thigs to do before applying any models, is to understand if the labels are equally distributed across the samples. In this case we say that the dataset is balanced. \nIn our case the dataset is slightly imbalanced, in fact there are 9% more samples associated with the label \"healthy = 1\". In fact we can say that any dataset in which there is a disproportion between the minority class and the majority class is technically imbalanced. However, in this case we are not in the case of \"Severe Imbalance\" (1:100 or more), because the distribution of examples is uneven by a small amount, in fact the ratio between the amount of samples of the two classes does not overcome the \"rule of thumb\" ratio of 6:4 (about 1.5). I assume that the classification task will not be much skewed toward the majority class.","355dc8ed":"### KNN PCA + Random OverSampling","94a48187":"### Logistic Regression + Random OverSampler","bdcfe92e":"### SVM and no Preprocessing Techinques","6af84032":"In order to get an idea on how the outliers are distributed across the samples, I introduced PCA in order to compute a dimensionality reduction to 3 dimensions and have a clear visualization of the outliers.","84753195":"It can be seen that this method has removed every single univariate outlier.","e2f08017":"# Conclusions\nIn conclusion, PCA didn't affected much the results of the classification task.\nHowever the introduction of the Random Oversampling has stabilized the f1_score, making the algorithms less biased toward the class 1 (the majority).\nHowever, since the number of samples is quite limited results, in terms of accuracy reached at most (roughly) 85%. \nThe model for which the Sensitivity is more balanced with the Specificity is the LogisticRegression, this means that the model is good for catching actual cases of the disease but they also come with a fairly high rate of false positives.\n\nNevertheless, the limited size of the dataset introduced other issues such as the impossibility of reducing the variance of the models under a certain threshold. In fact, the performances of the models depends on the split, making the algorithms not so robust.\nOverall, for this kind of dataset neither with the introduction of many preprocessing steps it is possible reach realiable levels of accuracy. Therefore I think that the main goal for this dataset can be to give an insight on the models black box and not to obtain a powerful predictor.","442cd81e":"### KNN and no Prepreprocessing techniques","9c3f1c1c":"# 4.1 Random Forest\n\nThe main core element of the Random Forest Algorithm is the Decision Tree Algorithm. \nA decision tree is a simple, interpretable and non-parametric algorithm, that predicts the label associated with an instance $x$ by traveling from the root node of a tree to the leaves. Decision trees are built by splitting the predictor space into some distinct and non-overlapping hyper-rectangles, and points in the same region are classified with the same label.\n\nA general framework for growing a decision tree is as follows:\nWe start with a tree with a single leaf (the root) and assign this leaf a label according to a majority vote among all labels over the training set. We now perform a series of iterations. On each iteration, we examine the effect of splitting a single leaf. We define some \"gain\" measure that quantifies the improvement due to this split. \n![image-5.png](attachment:image-5.png)\n\n\nThen, among all possible splits, we either choose the one that maximizes the gain and perform it, or choose not to split the leaf at all. In general, two measures are often used in order to chose the best split:\n* Gini index: defined as\n$$G = 1 - \\sum_{k=1}{P_{i,k} ^2}$$\n\n\nwhere $p_i,k$ is the ratio of class $k$ instances among the training instances in the $i^{th}$ node.\nIt represents a measure of node \"purity\", therefore the smaller is the value of the Gini Index, the purest is the node. Purity is associated to the number of observations that come from the different classes, therefore a low Gini Index value means that most of the observations comes from one single class.\n* Information Gain: defined in terms of Entropy\n\n$$Entropy = \\sum_{i=1}{-p_ilog_2(p_i)}$$\n\n\n$$ IG  = Entropy(parent node) - WeightedAverage * Entropy(children)$$\n\n\nInformation Gain is the expected reduction in entropy caused by partitioning the examples according to a given attribute\n\nDecision Trees construction adopts a top-down, greedy strategy called recursive binary splitting in which the best attribute is selected locally. \nfor Decision Trees the generalization ability is overall very limited, and the risk of overfitting is constantly high.\n\nRandom Forest Algorithm was introduced in order to reduce the danger of overfitting (if using a single tree). It is an ensemble of decision trees that guarantees more robustness to the classification task and combines the predictions of the estimators to produce a more accurate prediction. \n![image-3.png](attachment:image-3.png)\nIt exploits the *bagging* method, a technique that aims to reduce the variance of a statistical learning method.\nBelow the main steps for building a RandomForest:\n1. **Bagging**\nGiven the training set of $N$ examples, it repeatedly sample subsets of the training data of size $n$ (the boostrapped training set), where $n$ is lower than $N$. Sampling is done at random but with *replacement*. This subsampling of a training set is called bootstrap aggregating, or bagging, for short. The advantage of bagging method is that the trees inside a Random Forest are decorrelated, reducing the variance of the model.\n\n\n2. **Random subspace method**\nIf each training example has M features, the algorithm takes a subset of them of size $p \u2264 M$ to train each estimator.Typically $ p = \\sqrt{M}$.\nSo no estimator sees the full training set, each estimator sees only $p$ features of $n$ training examples.\n\n\n3. **Training estimators**\nThe algorithm then creates $N_{tree}$ decision trees, or estimators, and train each one on a different set of $p$ features and $n$ training examples. The trees are not pruned, as they would be in the case of training a simple decision tree classifier.\n\n\n4. **Perform inference by aggregating predictions of estimators**\nTo make a prediction for a new incoming example, the algorithm passes the relevant features of this example to each of the $N_{tree}$ estimators. We will obtain $N_{tree}$ predictions, which we need to combine to produce the overall prediction of the random forest. In the case of classification, it will use majority voting to decide on the predicted class.\n\nThe random forest's ensemble design allows the random forest to generalize well to unseen data, including data with missing values. Random forests are also good at handling large datasets with high dimensionality and heterogeneous feature types (for example, if one column is categorical and another is numerical) and have few hyperparameters. \n\nSome hyperparameters that is important to tune:\n* max features: number of features to consider when training a tree. If None, all the features will be used to train the trees. The more this value is small, the lower is the risk of overfitting.\n* max_depth: Sets a limit to the growth of the tree. Better if low.\n* criterion: Gini or Entropy.\n* n_estimators: sets the number of trees that the algorithm has to build.","ae42c539":"Below displayed the boxplots of the distributions without outliers:","2ed0fff4":"### Logistic Regression PCA","19c6425c":"# 5.1.1 Feature Importance - Random Forest\n*Why do we need feature importance?* \nFeature importance scores can provide insight into the dataset and into the model. The relative scores can highlight which features may be the most relevant to the target, and the converse, which features are the least relevant. This may be interpreted by a domain expert and could be used as the basis for gathering more or different data. This is a type of model interpretation that can be performed for those models that support it.\n\nFeature importance in Random Forest is calculated as the *decrease in node impurity* weighted by the probability of reaching that node. The node probability can be calculated by the number of samples that reach the node, divided by the total number of samples. The higher the value the more important the feature.\nIn scikit-learn feature importance is computed for each tree by using the Gini Index or *mean decrease impurity*.\nSince in the internal node, the selected feature is used to make decision on how to divide the data set into two separate sets, the importance of node j is computed as:\n![image.png](attachment:image.png)\nwhere\n$w_j$ is the weighted number of samples reaching node j ($w_{left_j}$ and $w_{right_j}$ represents respectively the child node from left and from right split). $C_j$ is the impurity value of node j.\nThe feature importance on each tree is computed as:\n![image-2.png](attachment:image-2.png)\nwhere\n$fi_i$ is the importance of feature i\nand $ni_j$ is the importance of node j\nThen, each $fi_i$ is normalized, and finally at the Random Forest level, the feature importance is computed by averaging its value over all the trees. The sum of the feature\u2019s importance value on each trees is calculated and divided by the total number of trees:\n![image-3.png](attachment:image-3.png)\n$RFfi_i$ is the importance of feature i calculated from all trees in the Random Forest model\n$normfi_ij$ is then the normalized feature importance for i in tree j and \n$T$ is the total number of trees","113931df":"### Random Forest and no preprocessing techinques","aeefb1ab":"For the pairplots visualization it is important to Standardize (see par. 3.2 for details) the data since we cannot compare different measures on different scales:","e7144d05":"### SVM  PCA","06b3cd2b":"# Heart Disease - classification \n\n![image-3.png](attachment:image-3.png)\n\n## Introduction\nThe heart disease dataset is a multivariate dataset containing 14 features, both numerical (Real and Integer) and categorical, more precisely 7 of them are categorical and the other 7 are numerical. This dataset is associated with a binary classification task.\nIt includes 303 rows associated with patients affected from a heart disease. The presence\/absence of the disease is encoded by exploiting a binary variable. The value 0 is associated to the presence of a heart disease, the value 1 is associated with the absence of heart disease. \n\nNumerical features description:\n* Age: age of the patient, expressed in years (*Integer values*)\n* Resting Blood Pressure:Blood pressure at rest, measured in mmHg at the time of hospitalization (*Integer values*)\n* Cholesterol: level of cholesterol, measured in mg\/dl (*Integer values*)\n* Max heart rate achieved: maximum heart rate achieved (*Integer values*)\n* ST wave depression: ST depression induced by exercise relative to rest (*Real values*)\n* Num of major vessels: Number of major vessels colored by fluoroscopy (0-3) (*Integer values*)\n\nCategorical features description:\n* Sex: patient' sex (male = 1 and female = 0)\n* Chest pain type: chest pain experienced (1 = typical angina, 2 = atypical angina, 3 = non-anginal pain, 4 = asymptomatic)\n* Fasting blood sugar (or Diabetes): measured in mg\/dl (> 120 mg\/dl). (1 = true, 0 = false)\n* ECG at rest: resting electrocardiographic measurements. (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable left ventricular hypertropy)\n* Exercise induced angina: Exercise induced angina (1 = yes, 0 = no) \n* ST wave slope: slope of the peak exercise ST segment (1 = upsloping, 2 = flat, 3 = downsloping)\n* Thalassemia:\tBlood disease Thalassemia (3 = normal, 6 = fixed defect, 7 = reversable defect)\n","f0e2955e":"For Random Forest algorithm introducing the PCA has somehow increased the amount of overfitting.\nFor the KNN the level of overfitting is sligthly decreased.\nHowever for SVM and Logistic Regression the two curves converges minimizing the tiny amount of overfitting observed before. In fact adding more training samples seems to likely increase the generalization ability of the two algorithms.","94fd03cd":"### Learning Curves PCA + RandomOverSampler","838679c8":"# 4.2 Support Vector Classifier\n\nSupport Vector Machine is a large margin classifier, since it tries to build a hyperplane that separates the classes in the feature space, by maximizing the margin. The separating hyperplane is defined as $(w,b)$ such that $\u2200i, y_i(\u3008w, x_i\u3009+ b)  > 0$, where $y_i$ \u2208 {$-1,1$}. \nThe margin is defined as the distance of the closest data point to the hyperplane, $\\underset{i\u2208m}{\\operatorname{min}} |(\u3008w, x_i\u3009 + b|$\n\n\nAs previously said the optimization problem is actually maximization problem that at first comes as Hard-margin problem, or equivalently:\n![image-7.png](attachment:image-7.png)\n\nThe aforementioned formulation is related to the Hard-margin SVM, in which we assume data are linearly separable.\nThanks to the introduction of the Langrange variables **$\\alpha$** = $[\\alpha_1,...\\alpha_m]$, with $\\alpha_i \u2265 0$, the weight vector $w$ at the solution of the SVM problem is a linear combination of the training set vectors $x_1,... x_m$. A vector $x_i$ appears\nin the solution iff $\\alpha_i\u22600$. Such vectors are called support vectors. Support vectors lie on the hyperplane and the solution depends only on them.\n\n\nHowever, since it's often impossible to find a hyperplane that perfectly separates real-world data (solving the \"hard margin\" problem), SVM in general solves a more \"relaxed\" optimization problem, which sees the introduction of the slack variables in order to allow the algorithm to make a few mistakes and therefore paying a cost for each classified example:\n![image-3.png](attachment:image-3.png)\n\n\nThe optimization problem becomes:\n![image-5.png](attachment:image-5.png)\nThe optimization problem is then related to trading off how large it can be the margin versus how many points are going to be misclassified. \n\nIn other words soft-margin SVMs minimize training error traded off against margin, where the parameter C determines the trade-off between margin-maximization (C is small) and the minimization of the slack penalty (C is large). Let's note that C is inversely proportional to the regularization ($C = \\frac{1}{\\lambda}$ ) , so it has to be accurately tuned.\n![image.png](attachment:image.png)\n\nHowever, for data highly non-linearly separable, neither the introduction of a certain amount slack is sufficient. Therefore the kernel-trick is introduced.\nThe idea is then about mapping the non-linear separable data into a higher dimensional space where we can find a hyperplane that can separate the samples. The trick is that we don't have to compute directly the feature transformation, we only have to know that this equality holds: \n\n<$\\phi(x_j)$ , $\\phi(x)$> = $K(x_j, x)$\n\nEssentially the kernel trick helps us to define a Kernel Function $K$ in terms of original space itself without even defining (or in fact, even knowing), what the transformation function  $\\phi$  is. Therefore the prediction on a new instance can be computed as ![image-6.png](attachment:image-6.png)\n![image-2.png](attachment:image-2.png)\n\nIn the hyperparameter search I considered the *linear* kernel and the *Radial basis function* kernel. \nThe rbf kernel, or Gaussian kernel:\n\n$K(x,x_i) = e^{-\\gamma||x - x_i||^2}$ where $\\gamma$ can be considered as the spread of the kernel that has the effect on the dimension of the decision region since it is inversely proportional to the variance of the gaussian.","70da5a80":"### Logistic Regression and no PreProcessing Techinques","8850552d":"## Standardization\nStandardize the data is an important step that have to be applied when we compare measurements that have different units.\nIn fact variables that are measured at different scales may not equally contribute to the analysis resulting in some bias.\nTherefore, I tried to standardize my data in order bring data at the same scale. \nData standardization is a preprocessing technique that allows rescaling data to have a\n0 mean and a standard deviation of 1. The standard score of every value is calculated in this way: \n> $$Z = \\frac{x \u2212 \u00b5}\u03c3$$\n\nAfter standardization data looks like:","1ee02583":"\nBelow the analysis of each attribute by considering age and sex. So these visualizations provide us information about how each of the quantitative attributes is distributed with respect to age and sex:","688b1b8b":"# 5.4 KNN no PCA","1731bc23":"Below plotted the distribution of the age with respect to the target variable. As expected, the dataset proves how the age influences the insurgence of heart disorders. In fact the Graph suggests that the highest number of people suffering from heart diseases are in the age group of 55-65 years and the patients in the age group under 50 are very less likely to suffer from heart diseases.","63caea70":"Now each category, for every categorical variable, is plotted with respect to the target variable. This is useful to understand relationships between a specific category and the target.","680f4808":"# 6.1 Force Plot\nThis plot allow us to give explainability to a single model prediction.\nIn particular red and blue arrows are associated with each feature, in fact each arrow shows how much the feature impacts the model (bigger arrows are associated with a high impact on the decision). Moreover, a red arrow increases the model output value, while the blue arrow decreases the model output value, starting from the baseline. The baseline for Shapley values is the average of all predictions."}}