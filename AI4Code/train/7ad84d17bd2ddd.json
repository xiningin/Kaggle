{"cell_type":{"3a3d30cc":"code","b92015f7":"code","0a17d201":"code","58296bf0":"code","9f40fecd":"code","2410da20":"code","d4ca76c3":"code","9308f723":"code","3dc2a259":"code","fdf94fd0":"code","0bde7cd0":"code","d6714a09":"code","5edfc441":"code","544799eb":"code","763129ab":"code","83e25ce9":"code","32171f22":"code","3c028734":"code","c3a706b2":"code","d3914fac":"code","9fb647c2":"code","ec444e5b":"code","5750f5c5":"code","98304003":"code","db50f4b6":"code","10a829e1":"code","35b4b22b":"code","4773052c":"code","4edc80fb":"code","6b3da5ad":"code","742d3bdc":"code","14f6c241":"code","aa93b04c":"code","27e5088c":"code","59055f36":"code","25a4f345":"code","e099d6b5":"code","f83c86e7":"code","53e5255f":"code","9e464d36":"code","057a1240":"code","e068f6ab":"code","9f7da99b":"code","0f620506":"code","c2087cdc":"code","454a09fa":"code","ac6fef1e":"code","61adbb2e":"code","2051bbb8":"code","7bfe6595":"code","2ca37731":"code","99c6139f":"code","fdeab5bc":"code","5185e177":"code","11934a31":"code","779279d3":"code","28658650":"code","79ccd483":"code","92789c99":"code","28a21e4e":"code","a4ed7991":"code","3a8324c0":"code","1828bc40":"code","c04f829e":"code","84e75432":"code","9c5c36c9":"code","e962ebff":"code","7a31a321":"code","612d9478":"code","b32c5ff9":"code","bd718f79":"code","2b6cda59":"code","8d18d016":"code","fbe7e2d4":"code","61d73f50":"code","a7f7492a":"code","adf1f1d2":"code","07660f1a":"code","f1306ecf":"code","9e6ffb28":"code","381bc3bf":"code","3b57616a":"code","47fd32aa":"code","ac261e0d":"code","7396b4a9":"code","3c4d8f86":"code","2342132c":"code","5313cba1":"code","a7e858ff":"code","28f40258":"code","9b96cbe8":"code","c134edb5":"code","b52351fe":"code","aead7e5f":"code","c80f2358":"code","2f8c216a":"code","86611864":"code","c4858a99":"code","8b158bf1":"code","7658bb76":"code","852f37a9":"code","dddd0cd4":"code","63287154":"code","2cebc385":"code","4c344af0":"code","710414f5":"code","3b6583c5":"code","2e7aef3b":"code","252b7e11":"code","03c095cd":"code","8a776123":"code","94ef18be":"markdown","237f2923":"markdown","7c235aa5":"markdown","075ac3b7":"markdown","b3bcf3b9":"markdown","b56c13d8":"markdown","6a83a09a":"markdown","0e868b2e":"markdown","a7e62630":"markdown","1ec48469":"markdown","39c2c1fc":"markdown","470aab50":"markdown","79658f4c":"markdown","8e106ca2":"markdown","337d60e2":"markdown","38827685":"markdown","7b9379ac":"markdown","d436c775":"markdown","47808dec":"markdown","2d73b23f":"markdown","0a1b0927":"markdown","459484ec":"markdown","32c40a0f":"markdown","d8fb9d9e":"markdown","46800337":"markdown","5dd96adb":"markdown","73d17dd5":"markdown","3041a7de":"markdown","41c96538":"markdown","853350a0":"markdown","313247ce":"markdown","39701fde":"markdown","ba2ded62":"markdown","8617871d":"markdown","5c689f3f":"markdown","d06e0846":"markdown","2027480e":"markdown","52b159e3":"markdown","3f96634f":"markdown","0b6b3a8e":"markdown","705148bf":"markdown","50f0c2a8":"markdown","1b10a72f":"markdown","98d5f409":"markdown","ae85445e":"markdown","12ef4e3f":"markdown","828eb522":"markdown","0d05d221":"markdown","fbf3234b":"markdown","7f2ffd90":"markdown","5b02b2fc":"markdown","0d0b2d10":"markdown","4fc8cb49":"markdown"},"source":{"3a3d30cc":"!pip install seaborn","b92015f7":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms, models\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import ConcatDataset\nimport random\n\nimport time\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport os\nfrom PIL import Image\nimport seaborn as sn\nfrom IPython.display import display\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","0a17d201":"with Image.open('..\/input\/chest-xray-pneumonia\/chest_xray\/train\/NORMAL\/IM-0115-0001.jpeg') as im:\n    plt.imshow(im,cmap='gray')","58296bf0":"with Image.open('..\/input\/chest-xray-pneumonia\/chest_xray\/train\/PNEUMONIA\/person1000_bacteria_2931.jpeg') as im :\n    plt.imshow(im, cmap='gray')\n    print(im.size)","9f40fecd":"# Location of the images in my current machine\nroot = '..\/input\/chest-xray-pneumonia\/chest_xray'\nimg_names = []\n\nfor folders,subfolders,filenames in os.walk(root):\n    for img in filenames:\n        img_names.append(folders+ \"\/\" + img)","2410da20":"len(img_names)","d4ca76c3":"img_sizes = []\nrejected = []\n\nfor item in img_names:\n    try:\n        with Image.open(item) as img:\n            img_sizes.append(img.size)\n    \n    except:\n        rejected.append(item)","9308f723":"df = pd.DataFrame(img_sizes)","3dc2a259":"df","fdf94fd0":"df[0].describe()","0bde7cd0":"df[1].describe()","d6714a09":"df.describe()","5edfc441":"train_transform = transforms.Compose([\n        transforms.Resize(244),             # resize shortest side to 244 pixels\n        transforms.CenterCrop(244),         # crop longest side to 244 pixels at center\n        transforms.ToTensor(),\n    ])\n\ntest_transform = transforms.Compose([\n        transforms.Resize(244),\n        transforms.CenterCrop(244),\n        transforms.ToTensor(),\n    ])","544799eb":"# tranforming our data to Tensors \ntrain_data = datasets.ImageFolder(os.path.join(root,'train'), transform=train_transform)\ntest_data = datasets.ImageFolder(os.path.join(root,'test'), transform=test_transform)","763129ab":"class_names = train_data.classes\nprint(class_names)\nprint(f'Training images available: {len(train_data)}')\nprint(f'Testing images available:  {len(test_data)}')","83e25ce9":"train_loader = DataLoader(train_data,batch_size=16,shuffle=True, pin_memory=True)\ntest_loader = DataLoader(test_data,batch_size=16,shuffle=False, pin_memory=True)","32171f22":"class_names = train_data.classes\nprint(class_names)\nprint(f'Training loaders available: {len(train_loader)}')\nprint(f'Testing laoder available:  {len(test_loader)}')","3c028734":"def showImg(im, inv=False):\n    if inv == False:\n        plt.figure(figsize=(24,12))\n        plt.imshow(np.transpose(im.numpy(),(1,2,0)))\n    else:\n        inv_normalize = transforms.Normalize(mean=[-0.485\/0.229, -0.456\/0.224, -0.406\/0.225],\n                                             std=[1\/0.229, 1\/0.224, 1\/0.225])\n        im_inv = inv_normalize(im)\n        plt.figure(figsize=(24,12))\n        plt.imshow(np.transpose(im_inv.numpy(), (1, 2, 0)));","c3a706b2":"# Grab the first batch of 16 images\nfor images,labels in train_loader: \n    break\n\n# Print the labels\nprint('Label:', labels.numpy())\nprint('Class:', *np.array([class_names[i] for i in labels]))\n\nim = make_grid(images, nrow=8)  # the default nrow is 8\n\n# Print the images (Left to right)\nshowImg(im)","d3914fac":"# Here we have a batch of data with 16 images and the 3  color channels Red,Green,Blue that are 128 by 128\nimages.shape","9fb647c2":"conv = nn.Conv2d(3, 8, 3, 1) # input channels, amount of kernel to randomly create, kernel size (3x3), the stride=1\nfor x,labels in train_loader:\n    print('Orig size:',x.shape)\n    break\nx = conv(x)\nprint('Down size:',x.shape)","ec444e5b":"x = F.max_pool2d(x, 2, 2) # input data, kernel size (2x2), stride=2\nprint('Down size:',x.shape)","5750f5c5":"conv = nn.Conv2d(8, 16, 3, 1) # input this time is the amount of kernel from previous convolution, 8 random kernels, kernel size(3x3),\n                             # and stride = 1\nx = conv(x)\nprint('Down size:',x.shape)","98304003":"x = F.max_pool2d(x, 2, 2) # input channels, amount of kernel to randomly create, kernel size (3x3), the stride=1\nprint('Down size:',x.shape)","db50f4b6":"conv = nn.Conv2d(16, 32, 3, 1) # input this time is the amount of kernel from previous convolution, 8 random kernels, kernel size(3x3),\n                             # and stride = 1\nx = conv(x)\nprint('Down size:',x.shape)","10a829e1":"x = F.max_pool2d(x, 2, 2) # input channels, amount of kernel to randomly create, kernel size (3x3), the stride=1\nprint('Down size:',x.shape)","35b4b22b":"class CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 8, 3, 1)\n        self.conv2 = nn.Conv2d(8, 16, 3, 1)\n        self.conv3 = nn.Conv2d(16, 32, 3, 1)\n        self.fc1 = nn.Sequential(nn.Linear(28*28*32, 1028),\n                                        nn.ReLU(),\n                                        nn.Linear(1028, 512),\n                                        nn.ReLU(),\n                                        nn.Linear(512,2))\n\n    def forward(self, X):\n        X = F.relu(self.conv1(X))\n        X = F.max_pool2d(X, 2, 2)\n        X = F.relu(self.conv2(X))\n        X = F.max_pool2d(X, 2, 2)\n        X = F.relu(self.conv3(X))\n        X = F.max_pool2d(X, 2, 2)\n        X = X.view(-1, 28*28*32)\n        X = self.fc1(X)\n        return F.log_softmax(X, dim=1)","4773052c":"model = CNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nmodel","4edc80fb":"# number of parameters\ntotal=0\nfor p in model.parameters():\n    total += p.numel()\n    print(p.numel())\ntotal","6b3da5ad":"if torch.cuda.is_available():  \n    dev = \"cuda:0\" \nelse:  \n    dev = \"cpu\" ","742d3bdc":"dev","14f6c241":"device = torch.device(dev)  ","aa93b04c":"model = model.to(dev)","27e5088c":"start_time = time.time()\n\nepochs = 8 # number of iterations to train for\ntrain_losses = [] # keep track of all train losses\ntest_losses = [] # keep track of all test losses\ntrain_correct = [] # keep count on how many corrects prediction is done with the traiting batches\ntest_correct = [] # keep count on many corrects prediction is done with the traiting batches\n\nfor i in range(epochs):\n    trn_corr = 0\n    tst_corr = 0\n    \n    # Run the training batches\n    for b, (X_train, y_train) in enumerate(train_loader):\n        b+=1\n        \n        # Move the data to gpu if available\n        X_train = X_train.to(dev)\n        y_train = y_train.to(dev)\n        \n        # Apply the model\n        y_pred = model(X_train)\n        loss = criterion(y_pred, y_train)\n \n        # Tally the number of correct predictions\n        predicted = torch.max(y_pred.data, 1)[1]\n        batch_corr = (predicted == y_train).sum()\n        trn_corr += batch_corr\n        \n        # Update parameters\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Print batch results\n        if b%163 == 0:\n            print(f'epoch: {i:2}  batch: {b:4} [{16*b:6}\/5216]  loss: {loss.item():10.8f}  \\\naccuracy: {trn_corr.item()*100\/(16*b):7.3f}%')\n        \n    train_losses.append(loss)\n    train_correct.append(trn_corr)\n        \n    # Run the testing batches\n    with torch.no_grad():\n        for b, (X_test, y_test) in enumerate(test_loader):\n\n            X_test = X_test.to(dev)\n            y_test = y_test.to(dev)\n            \n            # Apply the model\n            y_val = model(X_test)\n\n            # Tally the number of correct predictions\n            predicted = torch.max(y_val.data, 1)[1] \n            tst_corr += (predicted == y_test).sum()\n            \n    loss = criterion(y_val, y_test)\n    test_losses.append(loss)\n    test_correct.append(tst_corr)\n        \nprint(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed            ","59055f36":"train_losses","25a4f345":"test_losses","e099d6b5":"plt.plot(train_losses, label='training loss')\nplt.plot(test_losses, label='test loss')\nplt.title('Loss at the end of each epoch')\nplt.legend();","f83c86e7":"train_correct","53e5255f":"test_correct","9e464d36":"for t in train_correct:\n    print(t\/float(len(train_data)) )","057a1240":"for t in test_correct:\n    print(t\/float(len(test_data)) )","e068f6ab":"plt.plot([t\/float(len(train_data)) for t in train_correct], label='training acc')\nplt.plot([t\/float(len(test_data)) for t in test_correct], label='testing acc')\nplt.title(\"acc\")\nplt.legend()","9f7da99b":"# Create a loader for the entire the test set\ntest_load_all = DataLoader(test_data, batch_size=624, shuffle=False)\nmodel.eval()\nmodel = model.cpu()\nwith torch.no_grad():\n    correct = 0\n    for X_test, y_test in test_load_all:\n             \n        y_val = model(X_test)\n        predicted = torch.max(y_val,1)[1]\n        correct += (predicted == y_test).sum()\n        \nprint(f'Test accuracy: {correct.item()}\/{len(test_data)} = {correct.item()*100\/(len(test_data)):7.3f}%')\narr = confusion_matrix(y_test.view(-1).cpu(), predicted.view(-1).cpu())\ndf_cm = pd.DataFrame(arr, class_names, class_names)\nplt.figure(figsize = (9,6))\nsn.heatmap(df_cm, annot=True, fmt=\"d\", cmap='BuGn')\nplt.xlabel(\"prediction\")\nplt.ylabel(\"label (ground truth)\")\nplt.show();","0f620506":"def runModel(model,criterion,optimizer,train_loader,test_loader,scheduler=None,epochs=8, print_every=163,c=1):\n    start_time = time.time()\n\n    train_losses = []\n    test_losses = []\n    train_correct = []\n    test_correct = []\n    model = model.to(dev)\n    for i in range(epochs):\n        \n        if scheduler==None:\n            pass\n        else:\n            scheduler.step()\n        \n        trn_corr = 0\n        tst_corr = 0\n        model.train()\n        # Run the training batches\n        for b, (X_train, y_train) in enumerate(train_loader):\n            b+=1\n            \n            X_train = X_train.to(dev)\n            y_train = y_train.to(dev)\n            #print(X_train)\n            # Apply the model\n            y_pred = model(X_train)\n            loss = criterion(y_pred, y_train)\n \n            # Tally the number of correct predictions\n            predicted = torch.max(y_pred.data, 1)[1]\n            batch_corr = (predicted == y_train).sum()\n            trn_corr += batch_corr\n        \n            # Update parameters\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        \n            # Print batch results\n            if b%print_every == 0:\n                print(f'epoch: {i:2}  batch: {b:4} [{16*b:6}\/{c*5216}]  loss: {loss.item():10.8f}  \\\n    accuracy: {trn_corr.item()*100\/(16*b):7.3f}%')\n        \n        train_losses.append(loss)\n        train_correct.append(trn_corr)\n        \n        # Run the testing batches\n        with torch.no_grad():\n            model.eval()\n            for b, (X_test, y_test) in enumerate(test_loader):\n\n                X_test = X_test.to(dev)\n                y_test = y_test.to(dev)\n            \n                # Apply the model\n                y_val = model(X_test)\n\n                # Tally the number of correct predictions\n                predicted = torch.max(y_val.data, 1)[1] \n                tst_corr += (predicted == y_test).sum()\n           \n        loss = criterion(y_val, y_test)\n        test_losses.append(loss)\n        test_correct.append(tst_corr)\n        \n    print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed \n    return train_losses,train_correct,test_losses,test_correct","c2087cdc":"train_transform = transforms.Compose([\n        transforms.RandomRotation(10),      # rotate +\/- 10 degrees\n        transforms.RandomHorizontalFlip(),  # reverse 50% of images\n        transforms.Resize(244),             # resize shortest side to 224 pixels\n        transforms.CenterCrop(244),         # crop longest side to 224 pixels at center\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], # mean use by imagenet for the RGB channel\n                             [0.229, 0.224, 0.225]) # standard deviation use by imagenet\n    ])\n\ntest_transform = transforms.Compose([\n        transforms.Resize(244),\n        transforms.CenterCrop(244),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ])","454a09fa":"train_data = datasets.ImageFolder(os.path.join(root,'train'), transform=train_transform)\ntest_data = datasets.ImageFolder(os.path.join(root,'test'), transform=test_transform)","ac6fef1e":"train_loader = DataLoader(train_data,batch_size=16,shuffle=True, pin_memory=True)\ntest_loader = DataLoader(test_data,batch_size=16,shuffle=False, pin_memory=True)","61adbb2e":"# Grab the first batch of 16 images\nfor images,labels in train_loader: \n    break","2051bbb8":"# Grab the first batch of 16 images\nfor images,labels in train_loader: \n    break\n\n# Print the labels\nprint('Label:', labels.numpy())\nprint('Class:', *np.array([class_names[i] for i in labels]))\n\nim = make_grid(images, nrow=8)  # the default nrow is 8\n\n# Print the images (Left to right)\nshowImg(im,inv=False)","7bfe6595":"# Print the images (Left to right)\nshowImg(im,inv=True)","2ca37731":"model = CNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nmodel","99c6139f":"train_losses,train_correct,test_losses,test_correct = runModel(model=model,\n                                                               criterion=criterion,\n                                                               optimizer=optimizer,\n                                                               train_loader=train_loader,\n                                                               test_loader=test_loader,\n                                                               )","fdeab5bc":"plt.plot(train_losses, label='training loss')\nplt.plot(test_losses, label='test loss')\nplt.title('Loss at the end of each epoch')\nplt.legend();","5185e177":"plt.plot([t\/float(len(train_data)) for t in train_correct], label='training acc')\nplt.plot([t\/float(len(test_data)) for t in test_correct], label='testing acc')\nplt.title(\"acc\")\nplt.legend()","11934a31":"# Create a loader for the entire the test set\ntest_load_all = DataLoader(test_data, batch_size=624, shuffle=False)\nmodel.eval()\nmodel = model.cpu()\nwith torch.no_grad():\n    correct = 0\n    for X_test, y_test in test_load_all:\n             \n        y_val = model(X_test)\n        predicted = torch.max(y_val,1)[1]\n        correct += (predicted == y_test).sum()\n        \nprint(f'Test accuracy: {correct.item()}\/{len(test_data)} = {correct.item()*100\/(len(test_data)):7.3f}%')\narr = confusion_matrix(y_test.view(-1).cpu(), predicted.view(-1).cpu())\ndf_cm = pd.DataFrame(arr, class_names, class_names)\nplt.figure(figsize = (9,6))\nsn.heatmap(df_cm, annot=True, fmt=\"d\", cmap='BuGn')\nplt.xlabel(\"prediction\")\nplt.ylabel(\"label (ground truth)\")\nplt.show();","779279d3":"model = CNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nmodel","28658650":"train_losses,train_correct,test_losses,test_correct = runModel(model=model,\n                                                               criterion=criterion,\n                                                               optimizer=optimizer,\n                                                               train_loader=train_loader,\n                                                               test_loader=test_loader,\n                                                               )","79ccd483":"plt.plot(train_losses, label='training loss')\nplt.plot(test_losses, label='test loss')\nplt.title('Loss at the end of each epoch')\nplt.legend();","92789c99":"plt.plot([t\/float(len(train_data)) for t in train_correct], label='training acc')\nplt.plot([t\/float(len(test_data)) for t in test_correct], label='testing acc')\nplt.title(\"acc\")\nplt.legend()","28a21e4e":"# Create a loader for the entire the test set\ntest_load_all = DataLoader(test_data, batch_size=624, shuffle=False)\nmodel = model.cpu()\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    for X_test, y_test in test_load_all:\n             \n        y_val = model(X_test)\n        predicted = torch.max(y_val,1)[1]\n        correct += (predicted == y_test).sum()\n        \nprint(f'Test accuracy: {correct.item()}\/{len(test_data)} = {correct.item()*100\/(len(test_data)):7.3f}%')\narr = confusion_matrix(y_test.view(-1).cpu(), predicted.view(-1).cpu())\ndf_cm = pd.DataFrame(arr, class_names, class_names)\nplt.figure(figsize = (9,6))\nsn.heatmap(df_cm, annot=True, fmt=\"d\", cmap='BuGn')\nplt.xlabel(\"prediction\")\nplt.ylabel(\"label (ground truth)\")\nplt.show();","a4ed7991":"class CNN2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 8, 3, 1)\n        self.conv2 = nn.Conv2d(8, 16, 3, 1)\n        self.conv3 = nn.Conv2d(16, 32, 3, 1)\n        self.fc1 = nn.Sequential(nn.Linear(28*28*32, 1028),\n                                        nn.ReLU(),\n                                        nn.Dropout(0.5),\n                                        nn.Linear(1028, 512),\n                                        nn.ReLU(),\n                                        nn.Dropout(0.5),\n                                        nn.Linear(512,2))\n        \n\n    def forward(self, X):\n        X = F.relu(self.conv1(X))\n        X = F.max_pool2d(X, 2, 2)\n        X = F.relu(self.conv2(X))\n        X = F.max_pool2d(X, 2, 2)\n        X = F.relu(self.conv3(X))\n        X = F.max_pool2d(X, 2, 2)\n        X = X.view(-1, 28*28*32)\n        X = self.fc1(X)\n        return F.log_softmax(X, dim=1)","3a8324c0":"model2 = CNN2()\ncriterion2 = nn.CrossEntropyLoss()\noptimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)\nmodel2","1828bc40":"total=0\nfor p in model2.parameters():\n    total += p.numel()\n    print(p.numel())\ntotal","c04f829e":"train_losses2,train_correct2,test_losses2,test_correct2 = runModel(model=model2,\n                                                               criterion=criterion2,\n                                                               optimizer=optimizer2,\n                                                               train_loader=train_loader,\n                                                               test_loader=test_loader,\n                                                               )","84e75432":"plt.plot(train_losses2, label='training loss')\nplt.plot(test_losses2, label='test loss')\nplt.title('Loss at the end of each epoch')\nplt.legend()","9c5c36c9":"plt.plot([t\/float(len(train_data)) for t in train_correct2], label='training acc')\nplt.plot([t\/float(len(test_data)) for t in test_correct2], label='testing acc')\nplt.title(\"acc\")\nplt.legend()","e962ebff":"# Create a loader for the entire the test set\ntest_load_all = DataLoader(test_data, batch_size=624, shuffle=False)\nmodel2 = model2.cpu()\nmodel2.eval()\nwith torch.no_grad():\n    correct = 0\n    for X_test, y_test in test_load_all:\n             \n        y_val = model2(X_test)\n        predicted = torch.max(y_val,1)[1]\n        correct += (predicted == y_test).sum()\n        \nprint(f'Test accuracy: {correct.item()}\/{len(test_data)} = {correct.item()*100\/(len(test_data)):7.3f}%')\narr = confusion_matrix(y_test.view(-1).cpu(), predicted.view(-1).cpu())\ndf_cm = pd.DataFrame(arr, class_names, class_names)\nplt.figure(figsize = (9,6))\nsn.heatmap(df_cm, annot=True, fmt=\"d\", cmap='BuGn')\nplt.xlabel(\"prediction\")\nplt.ylabel(\"label (ground truth)\")\nplt.show();","7a31a321":"criterion2 = nn.CrossEntropyLoss()\noptimizer2 = torch.optim.Adam(model2.parameters(), lr=0.0001)","612d9478":"train_losses2,train_correct2,test_losses2,test_correct2 = runModel(model=model2,\n                                                               criterion=criterion2,\n                                                               optimizer=optimizer2,\n                                                               train_loader=train_loader,\n                                                               test_loader=test_loader,\n                                                               )","b32c5ff9":"test_losses2","bd718f79":"train_losses2","2b6cda59":"plt.plot(train_losses2, label='training loss')\nplt.plot(test_losses2, label='test loss')\nplt.title('Loss at the end of each epoch')\nplt.legend()","8d18d016":"plt.plot([t\/float(len(train_data)) for t in train_correct2], label='training acc')\nplt.plot([t\/float(len(test_data)) for t in test_correct2], label='testing acc')\nplt.title(\"acc\")\nplt.legend()","fbe7e2d4":"# Create a loader for the entire the test set\ntest_load_all = DataLoader(test_data, batch_size=624, shuffle=False)\nmodel2 = model2.cpu()\nmodel2.eval()\nwith torch.no_grad():\n    correct = 0\n    for X_test, y_test in test_load_all:\n             \n        y_val = model2(X_test)\n        predicted = torch.max(y_val,1)[1]\n        correct += (predicted == y_test).sum()\n        \nprint(f'Test accuracy: {correct.item()}\/{len(test_data)} = {correct.item()*100\/(len(test_data)):7.3f}%')\narr = confusion_matrix(y_test.view(-1).cpu(), predicted.view(-1).cpu())\ndf_cm = pd.DataFrame(arr, class_names, class_names)\nplt.figure(figsize = (9,6))\nsn.heatmap(df_cm, annot=True, fmt=\"d\", cmap='BuGn')\nplt.xlabel(\"prediction\")\nplt.ylabel(\"label (ground truth)\")\nplt.show();","61d73f50":"class CNN3(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 8, 3, 1)\n        self.conv2 = nn.Conv2d(8, 16, 3, 1)\n        self.conv3 = nn.Conv2d(16, 32, 3, 1)\n        self.fc1 = nn.Sequential(nn.Linear(28*28*32, 1028),\n                                        nn.BatchNorm1d(1028),\n                                        nn.ReLU(),\n                                        nn.Linear(1028, 512),\n                                        nn.BatchNorm1d(512),\n                                        nn.ReLU(),\n                                        nn.Linear(512,2))\n        \n\n    def forward(self, X):\n        X = F.relu(self.conv1(X))\n        X = F.max_pool2d(X, 2, 2)\n        X = F.relu(self.conv2(X))\n        X = F.max_pool2d(X, 2, 2)\n        X = F.relu(self.conv3(X))\n        X = F.max_pool2d(X, 2, 2)\n        X = X.view(-1, 28*28*32)\n        X = self.fc1(X)\n        return F.log_softmax(X, dim=1)","a7f7492a":"model3 = CNN3()\ncriterion3 = nn.CrossEntropyLoss()\noptimizer3 = torch.optim.Adam(model3.parameters(), lr=0.001)\nmodel3","adf1f1d2":"total=0\nfor p in model3.parameters():\n    total += p.numel()\n    print(p.numel())\ntotal","07660f1a":"train_losses3,train_correct3,test_losses3,test_correct3 = runModel(model=model3,\n                                                               criterion=criterion3,\n                                                               optimizer=optimizer3,\n                                                               train_loader=train_loader,\n                                                               test_loader=test_loader,\n                                                               )","f1306ecf":"plt.plot(train_losses3, label='training loss')\nplt.plot(test_losses3, label='test loss')\nplt.title('Loss at the end of each epoch')\nplt.legend();","9e6ffb28":"plt.plot([t\/float(len(train_data)) for t in train_correct3], label='training acc')\nplt.plot([t\/float(len(test_data)) for t in test_correct3], label='testing acc')\nplt.title(\"acc\")\nplt.legend()","381bc3bf":"# Create a loader for the entire the test set\ntest_load_all = DataLoader(test_data, batch_size=624, shuffle=False)\nmodel3 = model3.cpu()\nmodel3.eval()\nwith torch.no_grad():\n    correct = 0\n    for X_test, y_test in test_load_all:\n             \n        y_val = model3(X_test)\n        predicted = torch.max(y_val,1)[1]\n        correct += (predicted == y_test).sum()\n\nprint(f'Test accuracy: {correct.item()}\/{len(test_data)} = {correct.item()*100\/(len(test_data)):7.3f}%')\narr = confusion_matrix(y_test.view(-1).cpu(), predicted.view(-1).cpu())\ndf_cm = pd.DataFrame(arr, class_names, class_names)\nplt.figure(figsize = (9,6))\nsn.heatmap(df_cm, annot=True, fmt=\"d\", cmap='BuGn')\nplt.xlabel(\"prediction\")\nplt.ylabel(\"label (ground truth)\")\nplt.show();","3b57616a":"class CNN4(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 8, 3, 1)\n        self.conv2 = nn.Conv2d(8, 16, 3, 1)\n        self.conv3 = nn.Conv2d(16, 32, 3, 1)\n        self.fc1 = nn.Sequential(nn.Linear(28*28*32, 1028),\n                                        nn.BatchNorm1d(1028),\n                                        nn.ReLU(),\n                                        nn.Dropout(.5),\n                                        nn.Linear(1028, 512),\n                                        nn.BatchNorm1d(512),\n                                        nn.ReLU(),\n                                        nn.Dropout(.5),\n                                        nn.Linear(512,2))\n        \n\n    def forward(self, X):\n        X = F.relu(self.conv1(X))\n        X = F.max_pool2d(X, 2, 2)\n        X = F.relu(self.conv2(X))\n        X = F.max_pool2d(X, 2, 2)\n        X = F.relu(self.conv3(X))\n        X = F.max_pool2d(X, 2, 2)\n        X = X.view(-1, 28*28*32)\n        X = self.fc1(X)\n        return F.log_softmax(X, dim=1)","47fd32aa":"model4 = CNN4()\ncriterion4 = nn.CrossEntropyLoss()\noptimizer4 = torch.optim.Adam(model4.parameters(), lr=0.001)\nmodel4","ac261e0d":"total=0\nfor p in model4.parameters():\n    total += p.numel()\n    print(p.numel())\ntotal","7396b4a9":"train_losses4,train_correct4,test_losses4,test_correct4 = runModel(model=model4,\n                                                               criterion=criterion4,\n                                                               optimizer=optimizer4,\n                                                               train_loader=train_loader,\n                                                               test_loader=test_loader,\n                                                               )","3c4d8f86":"plt.plot(train_losses4, label='training loss')\nplt.plot(test_losses4, label='test loss')\nplt.title('Loss at the end of each epoch')\nplt.legend();","2342132c":"plt.plot([t\/float(len(train_data)) for t in train_correct4], label='training acc')\nplt.plot([t\/float(len(test_data)) for t in test_correct4], label='testing acc')\nplt.title(\"acc\")\nplt.legend()","5313cba1":"# Create a loader for the entire the test set\ntest_load_all = DataLoader(test_data, batch_size=624, shuffle=False)\nmodel4 = model4.cpu()\nmodel4.eval()\nwith torch.no_grad():\n    correct = 0\n    for X_test, y_test in test_load_all:\n             \n        y_val = model4(X_test)\n        predicted = torch.max(y_val,1)[1]\n        correct += (predicted == y_test).sum()\n        \n\nprint(f'Test accuracy: {correct.item()}\/{len(test_data)} = {correct.item()*100\/(len(test_data)):7.3f}%')\narr = confusion_matrix(y_test.view(-1).cpu(), predicted.view(-1).cpu())\ndf_cm = pd.DataFrame(arr, class_names, class_names)\nplt.figure(figsize = (9,6))\nsn.heatmap(df_cm, annot=True, fmt=\"d\", cmap='BuGn')\nplt.xlabel(\"prediction\")\nplt.ylabel(\"label (ground truth)\")\nplt.show();","a7e858ff":"train_transform1 = transforms.Compose([\n        transforms.RandomRotation(25),      # rotate +\/- 25 degrees\n        transforms.RandomHorizontalFlip(1),  # reverse 100% of images\n        transforms.Resize(244),             # resize shortest side to 224 pixels\n        transforms.CenterCrop(244),         # crop longest side to 224 pixels at center\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ])\n\ntrain_transform2 = transforms.Compose([\n        transforms.RandomRotation(25),      # rotate +\/- 25 degrees\n        transforms.RandomHorizontalFlip(0), \n        transforms.Resize(244),             # resize shortest side to 224 pixels\n        transforms.CenterCrop(244),         # crop longest side to 224 pixels at center\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ])","28f40258":"train_data1 = datasets.ImageFolder(os.path.join(root,'train'), transform=train_transform1)\ntrain_data2 = datasets.ImageFolder(os.path.join(root,'train'), transform=train_transform2)\nt = []\nt.append(train_data1)\nt.append(train_data2)","9b96cbe8":"t","c134edb5":"concat_data = ConcatDataset(t)","b52351fe":"concat_loader = DataLoader(concat_data,batch_size=16,shuffle=True, pin_memory=True)","aead7e5f":"len(concat_loader)","c80f2358":"class CNN5(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 8, 3, 1)\n        self.conv2 = nn.Conv2d(8, 16, 3, 1)\n        self.conv3 = nn.Conv2d(16, 32, 3, 1)\n        self.fc1 = nn.Sequential(nn.Linear(28*28*32, 1028),\n                                        nn.BatchNorm1d(1028),\n                                        nn.ReLU(),\n                                        nn.Dropout(.5),\n                                        nn.Linear(1028, 512),\n                                        nn.BatchNorm1d(512),\n                                        nn.ReLU(),\n                                        nn.Dropout(.5),\n                                        nn.Linear(512,2))\n        \n\n    def forward(self, X):\n        X = F.relu(self.conv1(X))\n        X = F.max_pool2d(X, 2, 2)\n        X = F.relu(self.conv2(X))\n        X = F.max_pool2d(X, 2, 2)\n        X = F.relu(self.conv3(X))\n        X = F.max_pool2d(X, 2, 2)\n        X = X.view(-1, 28*28*32)\n        X = self.fc1(X)\n        return F.log_softmax(X, dim=1)","2f8c216a":"model5 = CNN5()\ncriterion5 = nn.CrossEntropyLoss()\noptimizer5 = torch.optim.Adam(model5.parameters(), lr=0.001)\nmodel5","86611864":"train_losses5,train_correct5,test_losses5,test_correct5 = runModel(model=model5,\n                                                               criterion=criterion5,\n                                                               optimizer=optimizer5,\n                                                               train_loader=concat_loader,\n                                                               test_loader=test_loader,\n                                                               c=2)","c4858a99":"plt.plot(train_losses5, label='training loss')\nplt.plot(test_losses5, label='test loss')\nplt.title('Loss at the end of each epoch')\nplt.legend();","8b158bf1":"for t in test_correct5:\n    print(t\/float(len(test_data)))","7658bb76":"plt.plot([t\/float(len(concat_data)) for t in train_correct5], label='training acc')\nplt.plot([t\/float(len(test_data)) for t in test_correct5], label='testing acc')\nplt.title(\"acc\")\nplt.legend()","852f37a9":"# Create a loader for the entire the test set\ntest_load_all = DataLoader(test_data, batch_size=624, shuffle=False)\nmodel5 = model5.cpu()\nmodel5.eval()\nwith torch.no_grad():\n    correct = 0\n    for X_test, y_test in test_load_all:\n             \n        y_val = model5(X_test)\n        predicted = torch.max(y_val,1)[1]\n        correct += (predicted == y_test).sum()\n        \n\nprint(f'Test accuracy: {correct.item()}\/{len(test_data)} = {correct.item()*100\/(len(test_data)):7.3f}%')\narr = confusion_matrix(y_test.view(-1).cpu(), predicted.view(-1).cpu())\ndf_cm = pd.DataFrame(arr, class_names, class_names)\nplt.figure(figsize = (9,6))\nsn.heatmap(df_cm, annot=True, fmt=\"d\", cmap='BuGn')\nplt.xlabel(\"prediction\")\nplt.ylabel(\"label (ground truth)\")\nplt.show();","dddd0cd4":"alexNet = models.alexnet(pretrained=False) # loading the AlexNet model ","63287154":"for param in alexNet.features.parameters(): # freezing all layers so no learning is required in the convolutional section\n    param.require_grad = False","2cebc385":"alexNet","4c344af0":"alexNet.classifier = nn.Sequential(nn.Linear(9216, 4096), # Got to keep the flatten input features, in this case 9261\n                                 nn.ReLU(),\n                                 nn.Dropout(.5),\n                                 nn.Linear(4096, 4096),\n                                 nn.ReLU(),\n                                 nn.Dropout(.5),\n                                 nn.Linear(4096,2))","710414f5":"alexNet\ncriterion6 = nn.CrossEntropyLoss()\noptimizer6 = torch.optim.Adam(alexNet.parameters(), lr=0.001)","3b6583c5":"train_losses6,train_correct6,test_losses6,test_correct6 = runModel(model=alexNet,\n                                                               criterion=criterion6,\n                                                               optimizer=optimizer6,\n                                                               train_loader=train_loader,\n                                                               test_loader=test_loader,\n                                                               c=2)","2e7aef3b":"plt.plot(train_losses6, label='training loss')\nplt.plot(test_losses6, label='test loss')\nplt.title('Loss at the end of each epoch')\nplt.legend();","252b7e11":"for t in test_correct6:\n    print(t\/float(len(test_data)))","03c095cd":"plt.plot([t\/float(len(train_data)) for t in train_correct6], label='training acc')\nplt.plot([t\/float(len(test_data)) for t in test_correct6], label='testing acc')\nplt.title(\"acc\")\nplt.legend()","8a776123":"# Create a loader for the entire the test set\ntest_load_all = DataLoader(test_data, batch_size=624, shuffle=False)\nalexNet = alexNet.cpu()\nalexNet.eval()\nwith torch.no_grad():\n    correct = 0\n    for X_test, y_test in test_load_all:\n             \n        y_val = alexNet(X_test)\n        predicted = torch.max(y_val,1)[1]\n        correct += (predicted == y_test).sum()\n        \n\nprint(f'Test accuracy: {correct.item()}\/{len(test_data)} = {correct.item()*100\/(len(test_data)):7.3f}%')\narr = confusion_matrix(y_test.view(-1).cpu(), predicted.view(-1).cpu())\ndf_cm = pd.DataFrame(arr, class_names, class_names)\nplt.figure(figsize = (9,6))\nsn.heatmap(df_cm, annot=True, fmt=\"d\", cmap='BuGn')\nplt.xlabel(\"prediction\")\nplt.ylabel(\"label (ground truth)\")\nplt.show();","94ef18be":"## Putting it all together\n\nNow we will perform both the dropout method and batch normalization together to see if we get any improvement in our model.","237f2923":"# Conclusion\n\nIn this project, we have introduced the basic process of Deep Neural Networks for image processing. We have used the power of the Convolution Neural Network to try and classify medical images to see if it can tell the difference between a Normal or Pneumonia x-ray. \n\nTo recap we have study about what makes a Fully Connected Convolutional Neural Network which is the Convolutional Layers that consist of mathematical computation using Convolution and Max Pooling. After the Convolutional Layer, we have the Fully Connected Layer that grabs the input from the Convolutional Layer and then feed forward the information through the hidden layer to give a classification in the output layer.\n\nThrough all these layers we have an activation function that is mathematical equations applied to each neuron to produce an output. Though the output layer uses the softmax function that converts the output layer to a probability vector that ranges from 0 to 1 and the sum of the vector should be 1. Then we have other functions to move through gradient descent smoothly that is known as the Loss Function and the Optimizer, these two operate hand in hand to achieve a minimum for our multidimensional classification.\n\nThen when experiencing overfitting, which is when our model learns too much from our training dataset and does not perform when new data is thrown at it. To reduce overfitting we use approaches like Dropout, Normalization, increasing learning rate, and more varying data.\n\nFinally, we finished it off with a Deep Learning approach call transfer learning. This where we use a pre-made model and change the fully connected layer to our needs. In turn, we need to also make sure our data has the same properties as it was trained, we did this by changing the image size to at least 224 by 244 and normalizing the images with ImageNet's Mean and Standard deviation. ","7c235aa5":"## ImageNet\n\nConvolutional Neural Networks (CNNs) models are commonly used for object recognition. Advances in Artificial intelligence are reflected in the compilation of publicly available datasets like ImageNet. ImageNet contains more than 15 million high-resolution labeled images into 22 thousand classes. CNNs are hard to apply to high-resolution images; it requires a significant amount of computational power and time. AlexNet is the name of a CNN designed by Alex Krizhevsky winner of the ImageNet Large Scale Visual Recognition Challenge on September 30 of the year 2012. AlexNet showed a vast improvement over other participant networks by making use of GPUs during training. AlexNet won the 2012 ImageNet competition with a top-5 error rate of 15.3%, compared to the second-place top-5 error rate of 26.2%. The architecture consists of five convolutional layers and three fully connected layers. Also, the use of the ReLu (Rectified Linear Units) instead of the standard tahn function made it possible to reach a 25% error six times faster than a CNN using the than function. The use of multiple GPUs is a key feature in AlexNet. Training half of the model\u2019s neurons in one GPU and the other half on another GPU makes it possible to train a larger set and cut the time in half. AlexNet architecture is a leading model in object detection showing huge potential in computer vision.\n\n![AlexNet.png](attachment:AlexNet.png)\n\nMore on ImageNet: <br>\nhttps:\/\/towardsdatascience.com\/alexnet-the-architecture-that-challenged-cnns-e406d5297951 <br>\nhttps:\/\/papers.nips.cc\/paper\/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf <br>\nhttps:\/\/en.wikipedia.org\/wiki\/Convolutional_neural_network <br>\nhttps:\/\/en.wikipedia.org\/wiki\/AlexNet <br>","075ac3b7":"#### Confusion Matrix\n\nA confusion matrix is a table that is often used that describes the performance of a classification model on a set of test data for which the true values are known. This helps us visualize how our model predicted","b3bcf3b9":"#### Steps for transfer learning\n* We will be performing transfer learning by first loading the model and grabbing its pretrained weights.\n* Then we will freeze all the convolutional layers.\n* Then we will replace the fully connected layer thats on the pre-loaded AlexNet model to fit it to our dataset.","b56c13d8":"**Dividing the total corrects by  the total number of images there are in total for both the train_data(5216) and the test data(624)**","6a83a09a":"# Problem Statement:\nPneumonia is an infection that inflames the air sacs in one or both lungs. It results in difficulty in breathing due to the air sacs filling with fluid. Pneumonia can be very serious for infants and young children. Patients older than age 65 or with existing conditions are also at risk of complications from pneumonia. Symptoms from pneumonia include chest pain when breathing, confusion, cough, fatigue, fever, nausea, and shortness of breath. Pneumonia is a common condition and is easily diagnosed using medical imaging. The Institute of Medicine at the National Academies of Science, Engineering, and Medicine reports that \u201cdiagnostic errors contribute to approximately 10 percent of patient deaths\u201d. Additionally, 6 to 17 percent of hospital complications are also the result of incorrect diagnosis. Not only diagnostic errors can affect the recovery of a patient, but the availability of health care personnel is also an issue in developing countries. To address this problem, researchers and companies are levering deep learning to improve medical diagnostics. The adoption and standardization of machine learning in the health industry is expected to improve the likelihood of correct diagnosis and prioritize treatment. Manual observation of images has remained relatively unchanged for over a century. The application of machine learning has the potential to impact how health care systems approach diagnostics and the availability of rapid diagnosis and therefore faster treatment. Even though Artificial intelligence in medical diagnostics is still a relatively new approach it shows a great promise. Some advances have been made to support the adoption of AI algorithms in the healthcare industry. The Food and Drug Administration has approved more than 30 AI algorithms for use in healthcare to date. Furthermore, AI algorithms will support critical steps in the early detection of diseases and the prioritization of treatment. The more optimal distribution of the healthcare personnel is key in revolutionizing the modern healthcare sector. AI adoption by the health industry will have a significant impact on traditional procedures improving diagnosis speed and possibly correctness for pneumonia and other diseases.","0e868b2e":"More on Machine Learning on Medical field: <br>\nhttps:\/\/emerj.com\/ai-sector-overviews\/machine-learning-medical-diagnostics-4-current-applications\/ <br>\nhttps:\/\/hms.harvard.edu\/news\/better-together <br>\nhttps:\/\/www.thoracic.org\/patients\/patient-resources\/resources\/top-pneumonia-facts.pdf<br>\nhttps:\/\/www.healthcaredive.com\/news\/ai-just-as-effective-as-clinicians-in-diagnostics-study-suggests\/563605\/","a7e62630":"## Analysis result 7\n\nEven with more data used for training the model has to improve at all and it actually got worst at the end. Although at some point it reach the same accuracy of the previous model,. So it seems that with the current model we have it has reached its maximum capacity of learning without any further changes. I think is very possible to reach higher accuracy with a more complex model. Although this is a Deep Neural Network I think this is considered a shallow neural network in today's standards, meaning that it's actually not a very complex model.\n\nWe will further test more models by using a larger and more complex model known as AlexNet from ImageNet.","1ec48469":"### Accuracy\n\nBelow we look at how many correct prediction we will see for each epoch in the model. ","39c2c1fc":"### The Fully Connected Layer\n\nThe role of the fully connected layer on CNN is to take the results of the convolution and pooling process and use them to classify the image into a label. The output of the convolution\/pooling is first flattened into a single vector of values and then performs its feedforward and backpropagation algorithm to update the weights in the model and improve the weights of the model to learn the data. Below is an image of a fully connected network after the convolution\/pooling process.\n\n![FCN.png](attachment:FCN.png)\n\nMore on Fully Connected Convolutional Neural Networks: <br> https:\/\/missinglink.ai\/guides\/convolutional-neural-networks\/fully-connected-layers-convolutional-neural-networks-complete-guide\/","470aab50":"### Convolution\n\nThe first thing to understand is how convolution works. It is basically a moving window or filter called a kernel that strides over an image. This moving window applies a mathematical calculation to create another matrix that holds the information for each kernel movement. Below you can see a basic representation of how convolution functions. This is a kernel size of 2x2 with a stride of 1.\n\n![Moving-filter.jpg](attachment:Moving-filter.jpg)\n\nAfter going through convolution the next step is to pass the output through a non-linear activation function such as ReLU. This will provide the standard non-linear behavior that neural networks have. This process is also known as feature mapping.\n\n","79658f4c":"#### Image Normalization\n\nNormalization is a technique often applied as part of data preparation for machine learning. The goal is to change the numeric values of the dataset to a common scale without distorting the difference in the ranges of value. But every dataset does not necessarily require normalization. We used the Mean and Standard deviation of ImageNet to normalize the image data as we will use it further down the line.\n\nMore on Normalzition: <br>\nhttps:\/\/medium.com\/@urvashilluniya\/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029 <br>\nhttps:\/\/arthurdouillard.com\/post\/normalization\/","8e106ca2":"## Initial Data Exploration\n\nWe will first like to explore our data by displaying the images we will be looking at. Below we will look at a normal patient and a Pneumonia patient and see if we can spot the difference with an untrained eye.","337d60e2":"## Dropout\n\nAnother way to reduce overfitting is though a process called Dropout. It turns off a neuron in the Neural network and makes it so that it is not considered in the feedforward and backpropagation of the model. It randomly chooses all you do is pass how much you want to turn off by giving it a percent. so if you set it to 50%, 50% of the neurons will be picked at random and turned off.\n\nYou might wonder why we want to turn off neurons? If they are meant to assist in classifying the data why would we shut it down? Well since the fully connected layer occupies most of the parameters in a CNN, the neurons tend to develop a co-dependency amongst each other during the training process. So some neurons become so important that it skews the favor to this one neuron that is present in the training dataset but not in the testing dataset.\n\nMore on dropout: <br>\nhttps:\/\/medium.com\/@amarbudhiraja\/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5","38827685":"**Here is how the images look when reverted back to their original form**","7b9379ac":"**Now we have finished exploring the initial data exploration and obviously, with the untrained eye we cannot tell the difference between a patient with or without Pneumonia we will see how our first model will perform on classifying an image with or without Pneumonia**","d436c775":"## Analyzing results 1\n\nNow that the model has finish training we can see the results. Now the metrics that we will look closely into is the accuracy and the loss at each epoch of the model.","47808dec":"## Analyzing results 2\n\nFrom the results we see from this model, normalzing the data with the ImageNet Means and Standard deviation no change to accuracy has occured, although the gap between the training and testing loss has close by a minimal amount it was still not big enough to improve accuracy of the model.","2d73b23f":"### Training on GPU\ndesign for extensive graphical and mathematical computations that helps alleviate the workload on the CPU. The main reason GPU is very important hardware is that it was basically designed for computing matrix calculations very efficiently which is what Deep Learning is when it comes to the calculation run in the code.\n\nThe three core comparison between training on CPU vs GPU comes down to this:\n* Training a model in deep learning requires a huge amount of Datasets, thus a large amount of memory is needed for large computational operations.\n* GPU performs all this large computational in parallel. Not saying CPU does not work in parallel, but GPU has much more parallelism. For example in my current machine, my CPU the Ryzen 5 2600 has 6 cores and 12 threads, meaning we can have at least 12 processes running to perform computation. But compare to my GPU the RTX2070 Super with 2560 CUDA cores to do computational analysis. \n* Latency on GPU is very low compare to CPU. Thus GPU bandwidth is very high under thread parallelism hence GPU would be the better choice for deep learning models\n\nBelow is a representation of how well GPU performs compare to server-level CPU.\n\n![gpu-floating.png](attachment:gpu-floating.png)\n\n**We will be training on gpu to accelerate the run time process of the model this is not necessary for this project.** <br>\nYou can see the benefits of GPU for deep learning here: <br> \nhttps:\/\/medium.com\/@shachishah.ce\/do-we-really-need-gpu-for-deep-learning-47042c02efe2 <br>\nhttps:\/\/towardsdatascience.com\/what-is-a-gpu-and-do-you-need-one-in-deep-learning-718b9597aa0d","0a1b0927":"**From the results of the training accuracy and testing accuracy we can tell that the model is not doing very well in classifying the two types of images in both the training and testing datasets.**","459484ec":"# Project Overview:\n\nAbout 1 million adults in the US seek care in a hospital due to pneumonia every year, and 50,000 die from this disease. Pneumonia is an infection of the lungs and it is the world\u2019s leading cause of death among children under 5 years of age. In developing nations, pneumonia can be further aggravated due to the dearth of medical resources and personnel.  For vulnerable populations, an accurate and fast diagnosis means everything. To overcome this problem, a neural network model is introduced to automatically perform fast and accurate classification using a deep neural network architecture. The CNN (Convolutional neural network) is a special type of neural networks that can automatically extract features from images. CNN-based deep learning algorithms have become the standard choice for medical image classification. With the help of the introduced CNN model, our implementation of a diagnostic tool is presented for the screening of patients with pneumonia. Optimizing diagnosis procedures of vulnerable patients will significantly improve the probability of faster recovery. ","32c40a0f":"**Looking at how much we have correct in each epoch**","d8fb9d9e":"### Loss\n\nSo below are the results of the Cross-Entropy Loss in the end of each epoch as you can see it is a relatively small numbers but typically we want the training loss and testing loss to be as low as possible and for both of them to be about the same as this correlates to how well our model performs with random new data set.","46800337":"## Analysis results 5\n\nIt seems like batch normalization was not necessary for this data set has it holds no improvements over the basic neural network. This probably means that the inputs and parameters in the neural network are already normalized so batch normalization causes no changes to the model","5dd96adb":"**We made the training procedure into a function to have an easier time and a clearer view of our exploratory process**","73d17dd5":"### Transfer learning\nTransfer learning is a machine learning method where a model developed for a similar task is reused as the starting point for a model on a second task. It allows rapid progress or improved performance when applied to the second task. For transfer learning, the features used on the first task must be general so that they are suitable for the second task. An example of transfer learning is the use of a pre-trained model for a large challenging image classification task like the ImageNet 100-class photograph classification competition. These models can be downloaded and integrated into new models that expect image data as input. Transfer learning is a shortcut or optimization to saving time or benefiting from the performance of the reused model. A common usage pattern for transfer learning is the classifier where the pre-trained model is used directly to classify new images.\n\nMore on transfer learning: <br>\nhttps:\/\/machinelearningmastery.com\/transfer-learning-for-deep-learning\/\nhttps:\/\/machinelearningmastery.com\/how-to-use-transfer-learning-when-developing-convolutional-neural-network-models\/","3041a7de":"**In 3D it will be like hitting the pockets in the plane and getting stuck in one without being able to reach the deepest pocket.**\n\n![convex_cost_function.jpg](attachment:convex_cost_function.jpg)","41c96538":"**Here is how the images looks like after applying the normalization**","853350a0":"**The DataLoader function is used to create batches and let us iterate through it so when we perform the Deep Neural Network\nwe can update the weights more frequently rather than one big chunk to converge quicker to our ideal weights**","313247ce":"### Preprocessing Data\n\nAs you can see from the analysis using pandas we have many images with varying degrees of sizes. To perform a convolutional neural network we would make sure that all images are the same size in both height and width. In the following block of code, we will be creating a function that will be used to transform the data to Tensors that will also modify the images to the way we see fit. Then we will further be analyzing our data.\n\n","39701fde":"## Analysis result 6\n\nWith both dropout and batch normalization we can see that the model still shows sign of overfitting but the losses don't have such a big gap. Although the performance increase by a small percentage, about 1-2%, it still improvement over the previous model. The closer we get to 99% the better it is since we want to reach this number and then it might be considered to be used in real life applications.","ba2ded62":"**See the info about the images' width in the data**","8617871d":"## Analyzing results 3\n\nNow we are seeing better results about 16 to 17 percent accuracy improvement so changing the learning rate was the right step to do. This is clear indicative example of how gradient descent can have many local minima and by changing the learning rate we can land on a better local minima since this is a multidimension model. The best way to show this in a 2D graph is to see the amount of minimas we have.\n\n ![minima.png](attachment:minima.png)\n","5c689f3f":"**See the info about the images' height in the data**","d06e0846":"## The model\n\nWe will start building the Fully connected convolutional neural network (CNN) model. We will first explain how each function work and the results of it and then start actually building the model. Below is an image of how a typical Fully connected layer looks like.\n\n![FcCNN.png](attachment:FcCNN.png)","2027480e":"We believe that for this level of work the best way to improve the models are two things. Be able to dynamically change the learning rate as the models is being train and also recording the model with the best results. Many times we see in the graph that a high point of accuracy has been reach but was discarded since it went trough another epoch of training. Also there were alot of sporodic changes in the loss function and accuracy that leads us to believe is because after training a certain amount the learning rate is too high and not converge correctly","52b159e3":"### Further Analysis 4\nIt seems like lowering the learning rate made it jump to an unwanted local minimum although the losses might look similar. It seems like we heavily overfitted as the training accuracy has reached almost to 99%, so even with 50% neurons turned off at each neuron some became very important and hold a big impact on classifying. Perhaps a bigger network is required or we have to reduce the amount being turn off by 25% instead of 50%.","3f96634f":"### Loss Function and Optimizer\n\n#### Cross Entropy Loss (Log Loss)\nThe loss function we will be using is the Cross-Entropy function for very simple reasons. The Cross-entropy loss measures the performance of a classification model whose outputs are a probability value between 0 and 1. So it works perfectly well with our softmax function has it produces a vector of numbers between the number from 0 to 1.\n\n\n#### Optimizer\nDuring the training process, we tweak and change the weights of our model through our loss function. We want to minimize the output of the loss function, and slowly make our predictions better. but how much do we change the weights is very important as we don't want the weights to go haywire and never converge to a good prediction. This is where the optimizer comes in, they tie together the loss function and model parameters through updating the weights in the right direction. This is what helps gradient descent converge to the closest or the lowest cost value.\n\nWe will be using Adam which is an adaptive learning rate optimizer algorithm that was designed specifically for training deep neural networks and will be using the traditional value of 0.01 for its learning rate.\n\nMore on Cross-Entropy: <br>\nhttps:\/\/machinelearningmastery.com\/cross-entropy-for-machine-learning\/ <br>\nhttps:\/\/ml-cheatsheet.readthedocs.io\/en\/latest\/loss_functions.html\n\nMore on  Optimizer: <br>\nhttps:\/\/algorithmia.com\/blog\/introduction-to-optimizers <br>\nhttps:\/\/towardsdatascience.com\/adam-latest-trends-in-deep-learning-optimization-6be9a291375c\n\nAdam publication : <br> https:\/\/arxiv.org\/abs\/1412.6980","0b6b3a8e":"## Improving the model\n\n### Overfitting\nWhen we run train our model on our current dataset, we allow the overall cost to become smaller with the more iterations or epochs. So over a long span of iterations at one point, it will lead to a minimal overall change of cost. This also means that the line separating the two classes will be fit into all points including the noise in the images, so secondary unnecessary patterns will be picked up by the model that is detrimental to the generalization of the model. \n\nSo seeing as the previous model was overfitting meaning it is learning too much from the training dataset's feature and causes it to not perform well in the testing dataset, we will now look into how to reduce overfitting to increase our accuracy and also close the gap between our training and testing losses.\n\nMore on Overfitting: <br>\nhttps:\/\/towardsdatascience.com\/what-are-overfitting-and-underfitting-in-machine-learning-a96b30864690","705148bf":"**We have seem to reach a road block in our models, it seems like we cannot get past the 80s% accuracy rate and we think it might be due to lack of data so we will manipulate the data we already have and increase the training dataset.**","50f0c2a8":"### Activation Functions\n\nThe activation functions in Neural networks are an important component in deep learning. Activation functions are mathematical equations applied to each neuron to produce an output. They determine the output of a deep learning model, its accuracy, and also the computational efficiency of the training model. They are also the ones that impact in converging for the model. Activation functions are what make or breaks neural networks. Activation functions are mathematical equations applied to each neuron to produce an output. \n\n#### ReLU\n\nThe Rectified Linear Unit is an activation function which mathematical equation is ***y = max(0,x)***. It is one of the most commonly used activation function mainly because it is cheap to compute as it is only grabbing the biggest value and if is 0 or less it sets it to zero, it typically converges faster due to its linearity, since ReLU is 0 for all negative value it is sparsely activated which is usually a desirable property we want from some neurons. \n\nThe main downside of ReLU is that when inputs approach zero the network cannot perform backpropagation anymore and cannot learn anymore thus the model cannot learn anymore.\n\n**ReLU function:**\n![ReLU.png](attachment:ReLU.png)\n\n#### Softmax function \nThe softmax function is to be able to handle multiple classes (Normal and Pneumonia in this case). It does this by normalizing the outputs for each class from 0 to 1 and then divides by their sum, giving the probability of the input value being in a specific class. This activation function was chosen for the output layer because although this is only two classes that can probably be done with other methods through binary cross-entropy, we want to show that even though this classifies images with the Pneumonia symptom we want to see what are the probability it is. Since the model will never be perfect knowing that the analysis of the image shows a probability of a 20% chance of being Normal and 80% of being Pneumonia can be very important for doctors using such an approach to diagnose a patient. \n\n![softmax.png](attachment:softmax.png)\n\nMore on Activation Functions: <br>\nhttps:\/\/missinglink.ai\/guides\/neural-network-concepts\/7-types-neural-network-activation-functions-right\/ <br>\nhttps:\/\/medium.com\/@danqing\/a-practical-guide-to-relu-b83ca804f1f7","1b10a72f":"**Combine we can see the general info of the height * width of the data**","98d5f409":"### Max pooling\n\nPooling is another sliding window type technique, but instead of applying weights, that can be trained, it applies statistical functions of some type over the contents of its window. In **Max Pooling** it is getting the highest signal in the kernel. The main benefits of pooling in CNN are that it reduces the number of parameters in the model also known as down-sampling, so reduces run time. Also, it makes feature detection more robust to object orientation and scale changes. Below is an example of max pooling with a kernel size of 2x2 and a stride of 2.\n\n![Max-pooling.jpg](attachment:Max-pooling.jpg)\n\nMore on Convolution and pooling: https:\/\/adventuresinmachinelearning.com\/convolutional-neural-networks-tutorial-in-pytorch\/","ae85445e":"## Analyzing results 8\n\nIt seems like the pre-made model performs about the same as our model. This brings me to believe that our dataset is not diverse enough for it to be trained in a model to reach high accuracy. It seems to always do well in the training accuracy but its learning too much of a feature in there. I believe that the accuracy can be easily increased if we had either more data(images) or more data correlating to Pneumonia. In this case, we would need a new type of model that takes both image data and clinical data.","12ef4e3f":"### Data analysis\n\nIn the following snippets of code, we will explore our data. We will see how much data we have, and then explore there images' sizes. We will also be using the Pandas library to see our dataset easier and also let Pandas describe our dataset so we can see information such as the mean and standard deviation of the carrying image sizes.","828eb522":"## Analyzing results 4\n\nIt seems like the dropout function worked and has improved the accuracy by about 3%, the loss trend seems to be running sporadically again so I will lower the learning rate again to see if there is any more improvement to the model.","0d05d221":"**Here we can see how much training and testing images we have available to us in this data set**","fbf3234b":"### Lowering learning rate\n\nBy looking at the previous models and their training and testing loss graph we can see that the training loss function is changing sporadically. Typically when we see major changes like those is because the learning rate might be too high so we will lower it to see what we get as a result.","7f2ffd90":"## Batch Normalization\n\nJust like there is normalization in the inputs we can also normalize inside the network. Deep Neural Networks like CNN does suffer from internal covariate shift meaning that the distribution of each layer's inputs changes during training as the parameters of the previous layers changes. So one solution to fix this problem was to perform Batch Normalization before going through the activation function. So Batch normalization computes the mean and standard deviation during training, but during testing Batch Normalization is used across the whole dataset using a moving mean and standard deviation. Batch Normalization has shown a considerable training acceleration to existing architectures. Its weakness is that it does not work well with small batches or non-independent and identically distributed random variables.\n\nMore on Batch Normalzation:\nhttps:\/\/arthurdouillard.com\/post\/normalization\/\nhttps:\/\/arxiv.org\/abs\/1502.03167","5b02b2fc":"Through this project we found it very interesting that A Convolutional Neural Network can classify these two classes. Seeing as we are not medical professional we can not identify any problems whatsoever from these images but the CNN model is actually picking up features that. \n\nOne of the most difficult problem as a beginner is trying to figure out what parameters to change to reduce overfitting since it takes about 15 to 30 minute to train each model with just 8 epochs every parameter we change have to have a solid reason has time is wasted during this. We can't imagine how this works with heavier and complex models as those might take even longer to converge to an answer. ","0d0b2d10":"#### Image Normalization\n\nNormalization is a technique often applied as part of data preparation for machine learning. The goal is to change the numeric values of the dataset to a common scale without distorting the difference in the ranges of value. But every dataset does not necessarily require normalization. We used the Mean and Standard deviation of ImageNet to normalize the image data as we will use it further down the line.\n\nMore on Normalzition: <br>\nhttps:\/\/medium.com\/@urvashilluniya\/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029 <br>\nhttps:\/\/arthurdouillard.com\/post\/normalization\/","4fc8cb49":"**As you can see our current model is just predicting that all our images are Pneumonia images and can't find the features that show a sign of Pneumonia in the images. Since it assume all images were Pneumonia it got 62.5% correct because 62.5% of the test images were of Pneumonia**"}}