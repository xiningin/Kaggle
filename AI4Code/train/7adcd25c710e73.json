{"cell_type":{"784ba359":"code","97faeac0":"code","654e4e69":"code","2e75f17d":"code","75755d17":"code","9ec22d5d":"code","9ccdded5":"code","ce99510d":"code","dfa4c6a3":"code","be4a3596":"code","05c65cab":"code","3a0734c4":"code","14f6e3f9":"code","4ec00ae5":"code","7b894efd":"code","3e9e4675":"code","f62cef89":"code","b681f8e3":"code","dbe64434":"code","b84afa8c":"code","66edfe34":"code","6472c4fb":"code","c723d41d":"code","677e2d6a":"markdown","14a5f30f":"markdown","33a26317":"markdown","9161f7d7":"markdown","e76edae7":"markdown","f5000bb4":"markdown","29dae810":"markdown","9d5ce72e":"markdown","a8a184f0":"markdown","f8a8b449":"markdown","3e463335":"markdown","1509f738":"markdown","e4b4635f":"markdown","a9a3767c":"markdown","7065d51a":"markdown","55468357":"markdown","25e68f6d":"markdown","7683b7a9":"markdown","42a40fca":"markdown","f4926226":"markdown","1536469d":"markdown","61a8ca52":"markdown"},"source":{"784ba359":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random as rand\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\nimport matplotlib.pyplot as plt # Plotting\nimport seaborn as sns # Plotting\nfrom IPython.display import Image, display # Display images\nimport cv2 # Opencv for computer vision tasks\n\nfrom keras.models import Sequential, Model, load_model  # for building sequential model\nfrom keras.layers import Conv2D, Dense, Dropout, Activation, MaxPooling2D, Flatten, BatchNormalization\nfrom keras.layers.advanced_activations import LeakyReLU\n\nfrom keras.applications import InceptionV3, ResNet50, VGG16\nfrom keras.applications.vgg16 import preprocess_input, decode_predictions\nfrom keras.applications.inception_v3 import preprocess_input, decode_predictions\n\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nfrom tqdm import tqdm # Visualize loop progress\nprint(os.listdir(\"..\/input\"))\n\n# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n# Any results you write to the current directory are saved as output.","97faeac0":"print(os.listdir(\"..\/input\/flowers\/flowers\/\"))","654e4e69":"flower_path = \"..\/input\/flowers\/flowers\/\"\nflowers_dict = {}\n\nfor flower in os.listdir(flower_path):\n    folder_path = os.path.join(flower_path, flower)\n    flowers = os.listdir(folder_path)\n    \n    flowers_dict[flower] = [folder_path, flowers]\n    img_idx = rand.randint(0,len(flowers)-1)\n    flwr_img_path = os.path.join(flower_path, flower, flowers[img_idx])\n    print(flwr_img_path)\n    print('Name of Flower: ', flower, u'\\u2193', 'at index: ', img_idx)\n    display(Image(filename=flwr_img_path))","2e75f17d":"def get_unique_image_shapes(imgs, path):\n    shape_set = set()\n    for img in imgs:\n        img_path = os.path.join(path, img)\n        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        \n        if image is not None:\n            shape_set.add(image.shape)\n    return shape_set\n\ndef get_shape_ranges(shapes):\n    shapes = list(shapes)\n    \n    min_row = min(shapes, key=lambda item: item[0])[0]\n    max_row = max(shapes, key=lambda item: item[0])[0]\n    \n    row_range = (min_row, max_row)\n    \n    min_col = min(shapes, key=lambda item: item[1])[1]\n    max_col = max(shapes, key=lambda item: item[1])[1]\n    \n    col_range = (min_col, max_col)\n\n    return row_range, col_range\n\ndef plot_shape_hist(shapes):\n    rows = [s[0] for s in shapes]\n    cols = [s[1] for s in shapes]\n    \n    sns.distplot(rows, label='rows')\n    sns.distplot(cols, label='col')\n    \n    rowhist, rowbins = np.histogram(rows, bins =50)\n    colhist, colbins = np.histogram(cols, bins =50)\n    print(\"Most common row size: \",round(rowbins[np.argmax(rowhist)]))\n    print(\"Most common col size: \",round(colbins[np.argmax(colhist)]))\n    plt.legend()\n    \n    plt.show()\n    \nfor i in flowers_dict:\n    path = flowers_dict[i][0]\n    imgs = flowers_dict[i][1]\n    shapes = get_unique_image_shapes(imgs, path)\n    row_range, col_range = get_shape_ranges(shapes)\n    print('Row range for {} images: {}'.format(i, row_range))\n    print('Column range for {} images: {}'.format(i, col_range))\n    \n    plot_shape_hist(shapes)","75755d17":"IMG_ROW = 150\nIMG_COL = 150\nNUM_CLASSES = 5","9ec22d5d":"parent_dir = \"..\/input\/flowers\/flowers\/\"\ndef prepare_datadf(parent_dir):\n    df = pd.DataFrame(columns = ['path', 'label'])\n    \n    for flower in os.listdir(parent_dir):\n        folder_path = os.path.join(parent_dir, flower)\n        flowers = os.listdir(folder_path)\n        for i in flowers:\n            df = df.append(pd.DataFrame({'path':[os.path.join(flower, i)], 'label':[flower]}), \n                           ignore_index=True)\n    \n    # Shuffling for randomness\n    df = df.sample(frac=1.0).reset_index(drop=True)\n    return df","9ccdded5":"# Create a dataframe with paths and labels\ndatadf = prepare_datadf(parent_dir)\ntrain, test = train_test_split(datadf, test_size=0.053)\nval, test = train_test_split(test, test_size=0.04)","ce99510d":"# Creating training and validation generators with data augmentation in train generator\ngen = ImageDataGenerator(rotation_range=10, # in degrees 0-180\n                        zoom_range=0.1, # 10% zoom\n                        width_shift_range = 0.1, # 10% of horizontal shift\n                        height_shift_range = 0.1, # 10% vertical shift\n                        horizontal_flip = True, # flip horizontally\n                        shear_range = 0.1, # 10% shear\n                        rescale = 1.\/255) # bring all pixel values between 0 and 1.\n\nvalgen = ImageDataGenerator(rescale = 1.\/255)","dfa4c6a3":"# Creating train and validation generator instances to read image paths from dataframe \ntrain_generator=gen.flow_from_dataframe(dataframe=train, \n                                        directory=parent_dir, x_col=\"path\", y_col=\"label\", \n                                        class_mode=\"categorical\", target_size=(IMG_ROW,IMG_COL), \n                                        batch_size=256)\nval_generator = valgen.flow_from_dataframe(dataframe=val,\n                                          directory=parent_dir,x_col=\"path\", y_col=\"label\", \n                                          class_mode=\"categorical\", target_size=(IMG_ROW,IMG_COL), \n                                           batch_size=64)","be4a3596":"# Defining our custom cnn model\ndef cnn_model():\n    model = Sequential()\n    \n    # First Conv layer\n    model.add(Conv2D(filters=32,kernel_size=(5,5), padding='same', input_shape=(IMG_ROW, IMG_COL, 3)))\n    model.add(LeakyReLU(alpha=0.02)) # Activation layer\n    model.add(MaxPooling2D(pool_size=(2,2))) # Pooling layer\n    \n    # Second Conv layer\n    model.add(Conv2D(filters=196, kernel_size=(5,5)))\n    model.add(LeakyReLU(alpha=0.02))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    \n    # Third Conv layer\n    model.add(Conv2D(filters=256, kernel_size=(5,5)))\n    model.add(LeakyReLU(alpha=0.02))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n\n    # Forth Conv layer\n    model.add(Conv2D(filters=512, kernel_size=(5,5)))\n    model.add(LeakyReLU(alpha=0.02))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    \n    # Flatten\n    model.add(Flatten())\n    \n    # Fully connected layer 1\n    model.add(Dense(1024))\n    model.add(LeakyReLU(alpha=0.02))\n    \n    # Fully connected layer 2\n    model.add(Dense(512))\n    model.add(LeakyReLU(alpha=0.02))\n    \n    # Output Layer\n    model.add(Dense(NUM_CLASSES, activation='softmax'))\n    \n    # Compile the model\n    model.compile(optimizer = 'adam', loss ='categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","05c65cab":"# Define training params\nbatch_size = 400\nepochs = 1 # Set to 1 for demo, for good results set 100, Running for 100 epochs will take time.\ntrain_steps_per_epoch = train_generator.n\/\/train_generator.batch_size\nval_steps_per_epoch = val_generator.n\/\/val_generator.batch_size","3a0734c4":"# Train the model\nmodel = cnn_model()\nmodel.fit_generator(generator=train_generator,\n                    steps_per_epoch=train_steps_per_epoch,\n                    validation_data=val_generator,\n                    validation_steps=val_steps_per_epoch,\n                    epochs=epochs)","14f6e3f9":"## epoch set 1 for demo, set 100 for good results","4ec00ae5":"# Inception Net\ndef inception():\n    # Build a Sequential Model\n    model = Sequential()\n    \n    # Add Inception module\n    base_model = InceptionV3(include_top=False, input_shape=(IMG_ROW, IMG_COL, 3))\n    \n    ### Freezing initial layers\n    for layer in base_model.layers[:249]:\n        layer.trainable = False\n    \n    # Setting last layers as trainable | Unfreezing later layers\n    for layer in base_model.layers[249:]:\n        layer.trainable = True\n\n    model.add(base_model)\n    \n    # Flatten *** Most Important *** Never forget to flatten a conv output before dense\n    ## This is necessary for resolving dimension errors \n    model.add(Flatten()) \n    \n    # Fully connected layer 1\n    model.add(Dense(1024))\n    model.add(LeakyReLU(alpha=0.02))\n    model.add(Dropout(rate=0.1))\n    \n    # Fully connected layer 2\n    model.add(Dense(512))\n    model.add(LeakyReLU(alpha=0.02))\n    \n    # Output Layer\n    model.add(Dense(NUM_CLASSES, activation='softmax'))\n    \n    # Compile the model\n    model.compile(optimizer = 'adam', loss ='categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","7b894efd":"# Train the model\nincept_mod1 = inception()\nincept_mod1.fit_generator(generator=train_generator,\n                    steps_per_epoch=train_steps_per_epoch,\n                    validation_data=val_generator,\n                    validation_steps=val_steps_per_epoch,\n                    epochs=epochs)","3e9e4675":"# Resnet\ndef resnet():\n    # Build a Sequential Model\n    model = Sequential()\n    \n    # Add Inception module\n    model.add(ResNet50(include_top=False, input_shape=(IMG_ROW, IMG_COL, 3)))\n    \n    # Flatten *** Most Important *** Never forget to flatten a conv output before dense\n    ## This is necessary for resolving dimension errors \n    model.add(Flatten()) \n    \n    # Fully connected layer\n    model.add(Dense(1024))\n    model.add(LeakyReLU(alpha=0.02))\n    \n    # Fully connected layer\n    model.add(Dense(512))\n    model.add(LeakyReLU(alpha=0.02))\n    \n    # Output Layer\n    model.add(Dense(NUM_CLASSES, activation='softmax'))\n    \n    # Set trainable false to use pretrained weights and not update them\n    model.layers[0].trainable = False\n    \n    # Compile the model\n    model.compile(optimizer = 'adam', loss ='categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","f62cef89":"# Train the model\nresmod1 = resnet()\nresmod1.fit_generator(generator=train_generator,\n                    steps_per_epoch=train_steps_per_epoch,\n                    validation_data=val_generator,\n                    validation_steps=val_steps_per_epoch,\n                    epochs=epochs)","b681f8e3":"def get_intermediate_activations(img):\n    \n    # Read image from path\n    image = cv2.imread(img, cv2.IMREAD_COLOR)\n    plt.imshow(image)\n    \n    # Preprocess the image\n    img_resized = cv2.resize(image, (299, 299))\n    x = np.expand_dims(img_resized, axis=0)\n    x = preprocess_input(x)\n    \n    # Load pretrained model\n    base_model = InceptionV3(weights='imagenet')\n    \n    # Select layers for getting activations\n    layer_outs = [layer.output for layer in base_model.layers[5:]]\n    model = Model(inputs=base_model.input, outputs=layer_outs)\n    \n    # Make predictions\n    act = model.predict(x)\n    pred = base_model.predict(x)\n    return act, pred\n","dbe64434":"img = '..\/input\/flowers\/flowers\/daisy\/4511693548_20f9bd2b9c_m.jpg'\nact,pred  = get_intermediate_activations(img)\ndecode_predictions(pred)","b84afa8c":"def display_activation(activations, col_size, row_size, act_index): \n    activation = activations[act_index]\n    activation_index=0\n    cnn_imgs = []\n    fig, ax = plt.subplots(row_size, col_size, figsize=(row_size*2.5,col_size*1.5))\n    for row in range(0,row_size):\n        for col in range(0,col_size):\n            cnn_imgs.append(activation[0, :, :, activation_index:activation_index+3])\n            ax[row][col].imshow(((activation[0, :, :, activation_index:activation_index+3])*255).\n                                astype(np.uint8))\n            activation_index += 1\n    return cnn_imgs","66edfe34":"k =display_activation(act,5,5,1)    ","6472c4fb":"plt.imshow((k[24]*255).astype(np.uint8))","c723d41d":"model.save('mycnn_model.h5') # Save a model as HDF5 file\n\nloaded_model = load_model('mycnn_model.h5') # Load the saved model for predictions","677e2d6a":"#### 5.2 Resnet <a id='5.2'><\/a>\n1. Imagenet weights\n2. No unfreezing, using default imagenet weights for all layers","14a5f30f":"#### 1.2 View a few data points","33a26317":"<a id='5'><\/a>\n### Transfer Learning \n\n#### 5.1 Inception Net <a id='5.1'><\/a>\n1. Imagenet weights\n2. A few layers unfreezed - so that they can be trained with our data**","9161f7d7":"## epoch set 1 for demo, set 100 for good results","e76edae7":"<a id='2.1'><\/a>\n#### 2.1 Understand image shapes\n\n**It's okay, Don't worry if you want to skip this section and move to [2.2](#2.2) \n","f5000bb4":"![](http:\/\/)[Go back to Contents](#contents)\n#### It's been a long EPOCH, THANKS FOR STAYING TILL THE END and NOT EARLY STOPPING :)\n#### Your comments, corrections, advice are whole heartedly welcome","29dae810":"[Go back to Contents](#contents)","9d5ce72e":"# --------------------- Let's Dig in --------------\n\n<a id='1.1'><\/a>\n#### 1.1 Understand folder structure","a8a184f0":"[Go back to Contents](#contents)","f8a8b449":"<a id='3.1'><\/a>\n#### 3.1 Generator and Data Augmentation","3e463335":"## epoch set 1 for demo, set 100 for good results","1509f738":"## epoch set 1 for demo, set 100 for good results","e4b4635f":"<a id='4'><\/a>\n### 4 Build custom CNN Model\n\n#### 4.1 Define model function","a9a3767c":"[Go back to Contents](#contents)","7065d51a":"[Go back to Contents](#contents)","55468357":"![](http:\/\/)[Go back to Contents](#contents)","25e68f6d":"<a id='7'><\/a>\n### 7 Viewing intermediate CNN layers activations","7683b7a9":"[Go back to Contents](#contents)","42a40fca":"<a id='contents'><\/a>\n## Contents:\n\n### 1. [Understand folder structure, view a few data points](#1.1)\n### 2. [Understand data - understand images(shapes, dimensions)](#2.1)\n### 3. [Prepare data for modelling - generator and data augmentation](#3.1)\n### 4. [Build a custom cnn model](#4)\n### 5. [Transfer Learning](#5) -  [Inception Net](#5.1), [Resnet](#5.2)\n### 6. [Make Predictions with Pretrained Inception Net](#6)\n### 7. [View Intermediate Activations](#7)\n### 9. [Save and Load a trained model](#8)","f4926226":"> <a id='2.2'><\/a>\n#### 2.2 Make Uniform and Load","1536469d":"<a id='8'><\/a>\n### 8 Save and Load a trained model","61a8ca52":"<a id='6'><\/a>\n### 6 Making predictions using pretrained('imagenet' weights) Inception net"}}