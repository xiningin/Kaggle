{"cell_type":{"fd206520":"code","a89bbcea":"code","c26d7a57":"code","a4740395":"code","0318747e":"code","aff616ae":"code","cca3fedf":"code","ba76d13b":"code","d3b65c52":"code","6b22f022":"code","7ebc9af6":"code","b3faf36e":"code","4f608760":"code","48dd04c6":"code","110118ec":"code","00f694d4":"code","ebdc3c83":"code","f46b68e0":"code","8599ab1d":"code","c47e7fb2":"code","a1f99f14":"code","df333398":"code","0db2d300":"code","37377caf":"code","e3d246a3":"markdown","780bdf44":"markdown","02981429":"markdown","e5c7118a":"markdown","ed20f2df":"markdown","b0b519d9":"markdown","3eae86ba":"markdown","93e0fd61":"markdown","b4804943":"markdown","ead89535":"markdown","d0e00bee":"markdown","7e275670":"markdown","9b7f5502":"markdown","be17844c":"markdown","f6a68126":"markdown","3fc5e163":"markdown"},"source":{"fd206520":"import nltk\nimport string\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n\nfrom nltk.stem.snowball import SnowballStemmer","a89bbcea":"df_train = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ndf_test = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n\ndf_submission = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n\nprint('Training shape : {}'.format(df_train.shape))\nprint('Testing shape : {}'.format(df_test.shape))","c26d7a57":"df_train.head()","a4740395":"df_train.info()","0318747e":"df_train.describe()","aff616ae":"df_train.isnull().sum()","cca3fedf":"def transform(sentence):\n    sentence = sentence.lower()\n    sentence = sentence.replace('\\n', ' ')\n    sentence = sentence.translate(str.maketrans('','', string.punctuation))\n    return sentence","ba76d13b":"df_train['excerpt'] = df_train['excerpt'].apply(transform)\ndf_train.head()","d3b65c52":"df_train['excerpt'][0]","6b22f022":"def stemWord(text):\n    stemmer = SnowballStemmer(language='english')\n    tokens = text.split()\n    clean_text = ' '\n    for token in tokens:\n        clean_text = clean_text + \" \" + stemmer.stem(token)      \n    return clean_text","7ebc9af6":"df_train['excerpt'] = df_train['excerpt'].apply(stemWord)","b3faf36e":"df_train['excerpt'][0]","4f608760":"X = df_train['excerpt'].copy()\ny = df_train['target'].copy()","48dd04c6":"def plot_loss(history):\n    plt.plot(history.history['loss'],\n             label='loss')\n    plt.plot(history.history['val_loss'],\n             label='val_loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Error')\n    plt.legend()\n    plt.grid(True)\n\n\ndef plot_rmse(history):\n    plt.plot(history.history['root_mean_squared_error'],\n             label='root_mean_squared_error')\n    plt.plot(history.history['val_root_mean_squared_error'],\n             label='val_root_mean_squared_error')\n    plt.xlabel('Epoch')\n    plt.ylabel('root mean squared error')\n    plt.legend()\n    plt.grid(True)","110118ec":"text = X\nvocab_size = 60000\nembedding_dim = 64\nmax_length = 60\ntrunc_type='post'\npad_type='post'\noov_tok = \"<OOV>\"","00f694d4":"tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(text)\nword_index = tokenizer.word_index","ebdc3c83":"training_sequences = tokenizer.texts_to_sequences(text)\ntraining_padded = pad_sequences(training_sequences,maxlen=max_length, truncating=trunc_type, padding=pad_type)\ntraining_labels_final = np.array(y)","f46b68e0":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Conv1D(embedding_dim, 5, activation='relu'),\n    tf.keras.layers.GlobalMaxPooling1D(), \n    tf.keras.layers.Dense(24, activation='relu', kernel_regularizer = tf.keras.regularizers.l2(0.01)),\n    tf.keras.layers.Dense(1)\n])\n\nmodel.summary()","8599ab1d":"model.compile(loss='mean_squared_error',\n              optimizer=tf.keras.optimizers.Adam(0.0001), \n              metrics=[RootMeanSquaredError()])","c47e7fb2":"his = model.fit(training_padded,\n                training_labels_final,\n                epochs=30,\n                validation_split=0.1)","a1f99f14":"plot_loss(his)","df333398":"plot_rmse(his)","0db2d300":"sample_sequences = tokenizer.texts_to_sequences(df_test['excerpt'])\nexcerpt_padded = pad_sequences(sample_sequences, padding='post', maxlen=max_length) \nclasses = model.predict(excerpt_padded)\n\nsample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')","37377caf":"submit = sample\nsubmit[\"target\"] = classes\nsubmit.to_csv(\"submission.csv\", index=False)\nsubmit","e3d246a3":"## Data split","780bdf44":"## Imports","02981429":"### Train model","e5c7118a":"## Read Dataset","ed20f2df":"### Check null data","b0b519d9":"### Functions to plot loss and rmse history","3eae86ba":"### Create tokenizer","93e0fd61":"### Define parameters","b4804943":"### Function to clean out highkey chars, punctuaction and line breaking.","ead89535":"### Function to convert words with suffix to root word","d0e00bee":"### Plot graphs","7e275670":"## Submission","9b7f5502":"# CommonLit Readability","be17844c":"## Train Model","f6a68126":"## Data Processing","3fc5e163":"### Create model"}}