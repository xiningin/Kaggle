{"cell_type":{"89ef4418":"code","2b1019ca":"code","9bcc7b80":"code","8f4f770a":"code","e8895cde":"code","7b99a240":"code","af50ed91":"code","cb259ec8":"code","0c42cd1a":"code","b5c8deec":"code","b4f6178c":"code","1b57a0d6":"code","9f9d488c":"code","32caafdb":"code","d95da5f1":"code","b6e9fd1d":"code","013463c9":"code","49da01aa":"code","b3d53ff2":"code","2376c5e6":"code","6a2cd2ed":"code","5b28e746":"code","1d504753":"code","20f806b1":"code","330d2862":"code","6435d073":"code","dcf86cee":"code","03a1acda":"code","84463fa7":"code","977262fe":"code","dfb81d38":"code","c0187403":"code","5466fbf5":"code","094eb3ff":"code","923f9918":"code","d5b5bd5e":"code","5b829ecb":"code","512519f9":"code","73cc9151":"code","7e323b0b":"code","c120f3fe":"code","734c4546":"code","a1db4593":"code","cba09925":"code","e4aa06ab":"code","5ca816fb":"code","5271b5ae":"code","a2fe1d6c":"code","c70faec0":"code","52979b6b":"code","004e2b7a":"code","acea90f4":"code","5bf5e026":"code","26f3dc3c":"code","ccb3c944":"code","72972ae1":"code","36a91888":"code","6ebc1694":"code","5b5053e4":"code","8d3a82fa":"code","ef65062f":"code","15257733":"code","94f95d25":"code","9c969963":"code","93764820":"code","44e2ad8d":"code","1106a425":"code","20a242cd":"code","f5dfa0f0":"code","60f7a3c0":"code","309adc65":"code","98cd6c98":"code","ac6c1b9a":"code","776d7807":"code","01a5aecd":"code","712cf983":"code","0759b075":"code","b9188e10":"code","e19181ba":"code","f9d73388":"code","df3e3f13":"code","7485a097":"code","0b747b6a":"code","725dee30":"code","af1ea353":"code","6458511f":"code","05685628":"code","2b40599c":"code","d5d8d878":"code","8b52055e":"markdown","c4a3f66a":"markdown","036a787d":"markdown","6b79f3e0":"markdown","afaadef5":"markdown","5e8c7a00":"markdown","83ce12c6":"markdown","95153d31":"markdown","2a01df5c":"markdown","7105186e":"markdown","ecae7494":"markdown","c52c0d68":"markdown","c3dfb56c":"markdown","0004f17b":"markdown","5175123d":"markdown","7ac03786":"markdown","c296de0e":"markdown","2e5d35df":"markdown","8948ffb2":"markdown","d87806aa":"markdown","0a114567":"markdown","aa029607":"markdown","502c9fd3":"markdown","580e7a67":"markdown","59ec8b96":"markdown","1d5c2d10":"markdown","c69d6b5f":"markdown","a7e6dc0a":"markdown","6e612a80":"markdown"},"source":{"89ef4418":"from tensorflow.keras.models import Sequential\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport pydicom as dcm\nimport os\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.image import ImageDataGenerator, load_img\nfrom matplotlib.patches import Rectangle\nimport cv2\nfrom tensorflow.keras.applications.mobilenet import preprocess_input\nimport tensorflow\nfrom tensorflow.keras.layers import *\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport keras\nimport gc\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam","2b1019ca":"label_data = pd.read_csv(\"..\/input\/rsna-pneumonia-detection-challenge\/stage_2_train_labels.csv\")","9bcc7b80":"label_data.head()","8f4f770a":"class_info = pd.read_csv(\"..\/input\/rsna-pneumonia-detection-challenge\/stage_2_detailed_class_info.csv\")","e8895cde":"class_info.head(4)","7b99a240":"df = pd.merge(left = class_info, right = label_data, how = 'left', on = 'patientId')\ndf = df.drop_duplicates()\ndf.info()","af50ed91":"def check_for_missing_data(df):\n    total = df.isnull().sum().sort_values(ascending=False)   # total number of null values\n    percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False) #percentage of values that are null\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent']) # putting the above two together\n    return missing_data # return the dataframe","cb259ec8":"check_for_missing_data(label_data)","0c42cd1a":"check_for_missing_data(class_info)","b5c8deec":"label_data.fillna(0, inplace=True)","b4f6178c":"label_data.info()","1b57a0d6":"label_data[label_data[\"Target\"]==1].describe().T","9f9d488c":"np.unique(class_info[\"class\"])","32caafdb":"np.unique(label_data[\"Target\"])","d95da5f1":"sns.countplot(label_data[\"Target\"])\n","b6e9fd1d":"sns.countplot(class_info[\"class\"])","013463c9":"class_info[class_info[\"patientId\"]=='0004cfab-14fd-4e49-80ba-63a80b6bddd6']","49da01aa":"label_data[label_data['patientId']=='0004cfab-14fd-4e49-80ba-63a80b6bddd6']","b3d53ff2":"class_info[class_info[\"patientId\"]=='003d8fa0-6bf1-40ed-b54c-ac657f8495c5']","2376c5e6":"label_data[label_data['patientId']=='003d8fa0-6bf1-40ed-b54c-ac657f8495c5']","6a2cd2ed":"class_info[class_info[\"patientId\"]=='00436515-870c-4b36-a041-de91049b9ab4']","5b28e746":"label_data[label_data['patientId']=='00436515-870c-4b36-a041-de91049b9ab4']","1d504753":"pd.pivot_table(df,index=[\"class\"], values=['patientId'], aggfunc='count')","20f806b1":"column_list = [\"Patient ID\", \"Patient Sex\", \"Patient's Age\", \"View Position\", \"Image Size\"]\nfile_meta_Data = pd.DataFrame(columns=column_list)","330d2862":"def add_meta_data_to_df(df, loc, from_list):\n    data = []\n    for filename in from_list:\n            imagePath = loc+filename\n            data_row_img_data = dcm.read_file(imagePath)\n            values = []\n            values.append(data_row_img_data.PatientID)\n            values.append(data_row_img_data.PatientSex)\n            values.append(data_row_img_data.PatientAge)\n            values.append(data_row_img_data.ViewPosition)\n            values.append(f\"{data_row_img_data.Rows}x{data_row_img_data.Columns}\")\n            zipped_val = dict(zip(column_list, values))\n            df = df.append(zipped_val, True)\n    return df","6435d073":"# Images Example\ntrain_images_dir = '..\/input\/rsna-pneumonia-detection-challenge\/stage_2_train_images\/'\ntrain_images = [f for f in os.listdir(train_images_dir) if os.path.isfile(os.path.join(train_images_dir, f))]\ntest_images_dir = '..\/input\/rsna-pneumonia-detection-challenge\/stage_2_train_images\/'\ntest_images = [f for f in os.listdir(test_images_dir) if os.path.isfile(os.path.join(test_images_dir, f))]\nprint('5 Training images', train_images[:5]) # Print the first 5","dcf86cee":"file_meta_Data = add_meta_data_to_df(file_meta_Data, \"..\/input\/rsna-pneumonia-detection-challenge\/stage_2_train_images\/\", train_images)","03a1acda":"file_meta_Data.info()","84463fa7":"df[\"class\"].value_counts().plot(kind='pie',autopct='%1.0f%%', shadow=True, subplots=False)\nplt.show()","977262fe":"sns.countplot(file_meta_Data['View Position'])","dfb81d38":"sns.distplot(file_meta_Data[\"Patient's Age\"])","c0187403":"print('Number of train images:', len(train_images))\nprint('Number of test images:', len(test_images))","5466fbf5":"dicom_file_path = os.path.join(\"..\/input\/rsna-pneumonia-detection-challenge\/stage_2_train_images\/05eebe4c-bca2-40d4-bb20-54fc60e2bcea.dcm\")\nfile = dcm.read_file(dicom_file_path)\nfile","094eb3ff":"gc.collect()","923f9918":"def show_dicom_images_with_boxes(data):\n    img_data = list(data.T.to_dict().values())\n    f, ax = plt.subplots(3,3, figsize=(16,18))\n    for i,data_row in enumerate(img_data):\n        patientImage = data_row['patientId']+'.dcm'\n        imagePath = f\"..\/input\/rsna-pneumonia-detection-challenge\/stage_2_train_images\/{patientImage}\"\n        data_row_img_data = dcm.read_file(imagePath)\n        modality = data_row_img_data.Modality\n        age = data_row_img_data.PatientAge\n        sex = data_row_img_data.PatientSex\n        data_row_img = dcm.dcmread(imagePath)\n        ax[i\/\/3, i%3].imshow(data_row_img.pixel_array, cmap=plt.cm.bone) \n        ax[i\/\/3, i%3].axis('off')\n        ax[i\/\/3, i%3].set_title('ID: {}\\nModality: {} Age: {} Sex: {} Target: {}'.format(\n                data_row['patientId'],modality, age, sex, data_row['Target']))\n        rows = label_data[label_data['patientId']==data_row['patientId']]\n        box_data = list(rows.T.to_dict().values())\n        for j, row in enumerate(box_data):\n            ax[i\/\/3, i%3].add_patch(Rectangle(xy=(row['x'], row['y']),\n                        width=row['width'],height=row['height'], \n                        color=\"yellow\",alpha = 0.1))   \n    plt.show()","d5b5bd5e":"def show_images(data):\n    img_data = list(data.T.to_dict().values())\n    f, ax = plt.subplots(3,3, figsize=(16,18))\n    for i,data_row in enumerate(img_data):\n        patientImage = data_row['patientId']+'.dcm'\n        imagePath = f\"..\/input\/rsna-pneumonia-detection-challenge\/stage_2_train_images\/{patientImage}\"\n        data_row_img_data = dcm.read_file(imagePath)\n        modality = data_row_img_data.Modality\n        age = data_row_img_data.PatientAge\n        sex = data_row_img_data.PatientSex\n        data_row_img = dcm.dcmread(imagePath)\n        ax[i\/\/3, i%3].imshow(data_row_img.pixel_array, cmap=plt.cm.bone) \n        ax[i\/\/3, i%3].axis('off')\n        ax[i\/\/3, i%3].set_title('ID: {}\\nModality: {} Age: {} Sex: {} Target: {}\\nWindow: {}:{}:{}:{}'.format(\n                data_row['patientId'],\n                modality, age, sex, data_row['Target'], \n                data_row['x'],data_row['y'],data_row['width'],data_row['height']))\n    plt.show()","5b829ecb":"show_images(label_data[label_data['Target']==1].sample(9))","512519f9":"show_images(label_data[label_data['Target']==0].sample(9))","73cc9151":"show_dicom_images_with_boxes(label_data[label_data['Target']==1].sample(9))","7e323b0b":"class_info[class_info['patientId']=='00436515-870c-4b36-a041-de91049b9ab4']","c120f3fe":"class_info.drop_duplicates(subset='patientId', keep='last', inplace=True)","734c4546":"class_info.shape","a1db4593":"IMAGE_SIZE = 1024#sample_pixel_array.shape[0]\nADJUSTED_IMAGE_SIZE=224\nMASK_IMAGE_SIZE = 28\nFACTOR = MASK_IMAGE_SIZE\/IMAGE_SIZE","cba09925":"class_info = shuffle(class_info)","e4aa06ab":"class_train, class_val = train_test_split(class_info, test_size=0.10, random_state=42, stratify=class_info['class'])","5ca816fb":"X_feature_tr = []\ny_feature_target_tr = []\ny_feature_coordinates_tr = []\nfrom PIL import Image","5271b5ae":"def create_mask(datafm):\n    X = []\n    y=[]\n    masks = np.zeros((int(datafm.shape[0]), MASK_IMAGE_SIZE, MASK_IMAGE_SIZE))\n    for index, patient_id in enumerate(datafm['patientId'].T.to_dict().values()):\n        image_path = train_images_dir+patient_id+\".dcm\"\n        img = dcm.read_file(image_path)\n        img = img.pixel_array\n        img = cv2.resize(img, (ADJUSTED_IMAGE_SIZE, ADJUSTED_IMAGE_SIZE), interpolation=cv2.INTER_NEAREST)\n        img = Image.fromarray(img)\n        img = img.convert('RGB')\n        img = preprocess_input(np.array(img, dtype=np.float32))\n        X.append(img)\n        rows = label_data[label_data['patientId']==patient_id]\n        y.append(rows['Target'].values[0])\n\n        row_data = list(rows.T.to_dict().values())\n        for row in row_data:\n            x1 = int(row['x']*FACTOR)\n            x2 = int((row['x']*FACTOR)+(row['width']*FACTOR))\n            y1 = int(row['y']*FACTOR)\n            y2 = int((row['y']*FACTOR)+(row['height']*FACTOR))\n            masks[index][y1:y2, x1:x2] = 1\n    del img,row,row_data\n    gc.collect()\n    X=np.array(X)\n    y=np.array(y)\n    return X, y, masks","a2fe1d6c":"X_train, y_tr_target, y_train = create_mask(class_train)\nX_val, y_val_target, y_val = create_mask(class_val)","c70faec0":"plt.imshow(y_train[18])","52979b6b":"pidtemp = class_train.iloc[[18]]['patientId']","004e2b7a":"#show_dicom_images_with_boxes(label_data[label_data['patientId']==pidtemp])","acea90f4":"from tqdm import tqdm\ndef process_dicom_data(data_df):\n    for n, pid in tqdm(enumerate(data_df['patientId'].unique())):\n        # the patientId can have multiple bounding boxes \n        # so there could be different row for each of the bounding box. we jus take the unique patient ids       \n        dcm_file = '..\/input\/rsna-pneumonia-detection-challenge\/stage_2_train_images\/%s.dcm' % pid #each image file provided are named with patient id\n        dcm_data = dcm.read_file(dcm_file)   #read the file using pydicom\n        idx = (data_df['patientId']==dcm_data.PatientID)\n        data_df.loc[idx,'Modality'] = dcm_data.Modality\n        data_df.loc[idx,'PatientAge'] = pd.to_numeric(dcm_data.PatientAge)\n        data_df.loc[idx,'PatientSex'] = dcm_data.PatientSex\n        data_df.loc[idx,'BodyPartExamined'] = dcm_data.BodyPartExamined\n        data_df.loc[idx,'ViewPosition'] = dcm_data.ViewPosition\n        \n    return data_df","5bf5e026":"dicom_df = process_dicom_data(df)","26f3dc3c":"dicom_df = dicom_df.astype({\"PatientAge\": int})\ndicom_df.fillna(0.0, inplace=True)\ndicom_df.head()","ccb3c944":"dicom_df.dtypes","72972ae1":"plt.figure(figsize = (30, 10))\nsns.countplot(x = 'PatientAge', hue = 'Target', data = dicom_df)\nplt.show()","36a91888":"sns.countplot(x = 'PatientSex', hue = 'Target', data = dicom_df)\nplt.show()","6ebc1694":"from tensorflow.keras.applications.mobilenet import MobileNet\nfrom tensorflow.keras.layers import Concatenate, Conv2D, Reshape, UpSampling2D\nfrom tensorflow.keras.models import Model, load_model","5b5053e4":"ALPHA = 1","8d3a82fa":"#this function will creat U-net model\ndef create_model(trainable=True):\n    model = MobileNet(input_shape=(224, 224, 3), include_top=False, alpha=ALPHA, weights=\"imagenet\")\n\n    for layer in model.layers:\n        layer.trainable = trainable\n\n    block1 = model.get_layer(\"conv_pw_5_relu\").output\n    block2 = model.get_layer(\"conv_pw_11_relu\").output\n    block3 = model.get_layer(\"conv_pw_13_relu\").output\n\n    x = Concatenate()([UpSampling2D()(block3), block2])\n    x = Concatenate()([UpSampling2D()(x), block1])\n\n    x = Conv2D(1, kernel_size=1, activation=\"sigmoid\")(x)\n    x = Reshape((28, 28))(x)\n\n    return Model(inputs=model.input, outputs=x)","ef65062f":"model = create_model(False)\nmodel.summary()","15257733":"def dice_coefficient(y_true, y_pred):\n    numerator = 2 * tensorflow.reduce_sum(y_true * y_pred)\n    denominator = tensorflow.reduce_sum(y_true + y_pred)\n\n    return numerator \/ (denominator + tensorflow.keras.backend.epsilon())","94f95d25":"def loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) - tensorflow.keras.backend.log(dice_coefficient(y_true, y_pred) + tensorflow.keras.backend.epsilon())","9c969963":"X_train.shape","93764820":"y_train.shape","44e2ad8d":"X_val.shape","1106a425":"y_val.shape","20a242cd":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import binary_crossentropy\n\noptimizer = Adam(lr=1e-2, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\nmodel.compile(loss=loss, optimizer=optimizer, metrics=[dice_coefficient])","f5dfa0f0":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\ncheckpoint = ModelCheckpoint(\"model-{val_loss:.2f}.h5\", monitor=\"val_loss\", verbose=1, save_best_only=True, save_weights_only=True)\n\nstop = EarlyStopping(monitor=\"val_loss\", patience=2)\n\nreduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=2, min_lr=1e-6, verbose=1)","60f7a3c0":"model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs=8, batch_size=12, verbose=1, callbacks=[checkpoint, stop, reduce_lr])","309adc65":"mask_predicated = model.predict(X_val)","98cd6c98":"%%writefile model.py\n\"\"\"\nPython script to prepare FasterRCNN model.\n\"\"\"\n\nimport torch\nimport torchvision\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import  FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\ndef model():\n    # load the COCO pre-trained model\n    # we will keep the image size to 1024 pixels instead of the original 800,\n    # this will ensure better training and testing results, although it may...\n    # ... increase the training time (a tarde-off)\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, \n                                                                 min_size=1024)\n    # one class is pneumonia, and the other is background\n    num_classes = 2\n    # get the input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace pre-trained head with our features head\n    # the head layer will classify the images based on our data input features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    return model","ac6c1b9a":"%%writefile dataset.py\n\n\"\"\"\nPython script to prepare the dataset\n\"\"\"\n\nimport numpy as np\nimport cv2\nimport re\nimport torch\n\nfrom torch.utils.data import Dataset\n\nclass RSNADataset(Dataset):\n    def __init__(self, dataframe, image_dir, transforms=None):\n        super().__init__()\n\n        self.image_ids = dataframe['patientId'].unique()\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n        \n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        records = self.df[self.df['patientId'] == image_id]\n\n        image = cv2.imread(f'{self.image_dir}\/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n\n        boxes = records[['x', 'y', 'width', 'height']].values\n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n        \n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        area = torch.as_tensor(area, dtype=torch.float32)\n\n        # there is only one class\n        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n        \n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        # target['masks'] = None\n        target['patientId'] = torch.tensor([index])\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.stack(tuple(map(torch.FloatTensor, zip(*sample['bboxes'])))).permute(1, 0)\n\n        return image, target, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","776d7807":"%%writefile engine.py\n\nimport pandas as pd\nimport dataset\nimport albumentations as A\nimport time\nimport torch\n\nimport numpy as np\nfrom sklearn import metrics\nfrom torch.utils.data import DataLoader\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom tqdm import tqdm\nfrom albumentations import (\n    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose\n)\n\n\"\"\"\nComplete mAP code here => https:\/\/gist.github.com\/tarlen5\/008809c3decf19313de216b9208f3734\n\"\"\"\n\ndef calculate_image_precision(gts, preds, thresholds = (0.5, ), form = 'coco') -> float:\n    # https:\/\/www.kaggle.com\/sadmanaraf\/wheat-detection-using-faster-rcnn-train\n    \"\"\"Calculates image precision.\n\n    Args:\n        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n               sorted by confidence value (descending)\n        thresholds: (float) Different thresholds\n        form: (str) Format of the coordinates\n\n    Return:\n        (float) Precision\n    \"\"\"\n    n_threshold = len(thresholds)\n    image_precision = 0.0\n    \n    ious = np.ones((len(gts), len(preds))) * -1\n    # ious = None\n    TP,FP,FN,TN = 0,0,0,0\n\n    for threshold in thresholds:\n        precision_at_threshold,tp ,fp ,fn ,tn  = calculate_precision(gts.copy(), preds, threshold=threshold,\n                                                     form=form, ious=ious)\n        image_precision += precision_at_threshold \/ n_threshold\n        TP += tp\n        FP += fp\n        FN += fn\n        TN += tn\n    accuracy = (TP + FP)\/(TP+FP+FN+TN)\n    return image_precision,TP\/4, FP\/4, FN\/4, TN\/4 , accuracy\n\n\ndef calculate_iou(gt, pr, form='pascal_voc') -> float:\n    # https:\/\/www.kaggle.com\/sadmanaraf\/wheat-detection-using-faster-rcnn-train\n    \"\"\"Calculates the Intersection over Union.\n\n    Args:\n        gt: (np.ndarray[Union[int, float]]) coordinates of the ground-truth box\n        pr: (np.ndarray[Union[int, float]]) coordinates of the prdected box\n        form: (str) gt\/pred coordinates format\n            - pascal_voc: [xmin, ymin, xmax, ymax]\n            - coco: [xmin, ymin, w, h]\n    Returns:\n        (float) Intersection over union (0.0 <= iou <= 1.0)\n    \"\"\"\n    if form == 'coco':\n        gt = gt.copy()\n        pr = pr.copy()\n\n        gt[2] = gt[0] + gt[2]\n        gt[3] = gt[1] + gt[3]\n        pr[2] = pr[0] + pr[2]\n        pr[3] = pr[1] + pr[3]\n\n    # Calculate overlap area\n    dx = min(gt[2], pr[2]) - max(gt[0], pr[0]) + 1\n    \n    if dx < 0:\n        return 0.0\n    dy = min(gt[3], pr[3]) - max(gt[1], pr[1]) + 1\n\n    if dy < 0:\n        return 0.0\n\n    overlap_area = dx * dy\n\n    # Calculate union area\n    union_area = (\n            (gt[2] - gt[0] + 1) * (gt[3] - gt[1] + 1) +\n            (pr[2] - pr[0] + 1) * (pr[3] - pr[1] + 1) -\n            overlap_area\n    )\n    #print(\"IOU Calculated... \"+str(overlap_area \/ union_area))\n    return overlap_area \/ union_area\n\n\ndef find_best_match(gts, pred, pred_idx, threshold = 0.5, form = 'pascal_voc', ious=None) -> int:\n    # https:\/\/www.kaggle.com\/sadmanaraf\/wheat-detection-using-faster-rcnn-train\n    \"\"\"Returns the index of the 'best match' between the\n    ground-truth boxes and the prediction. The 'best match'\n    is the highest IoU. (0.0 IoUs are ignored).\n\n    Args:\n        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n        pred: (List[Union[int, float]]) Coordinates of the predicted box\n        pred_idx: (int) Index of the current predicted box\n        threshold: (float) Threshold\n        form: (str) Format of the coordinates\n        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n\n    Return:\n        (int) Index of the best match GT box (-1 if no match above threshold)\n    \"\"\"\n    best_match_iou = -np.inf\n    best_match_idx = -1\n    for gt_idx in range(len(gts)):\n        \n        if gts[gt_idx][0] < 0:\n            # Already matched GT-box\n            continue\n        \n        iou = -1 if ious is None else ious[gt_idx][pred_idx]\n\n        if iou < 0:\n            iou = calculate_iou(gts[gt_idx], pred, form=form)\n            \n            if ious is not None:\n                ious[gt_idx][pred_idx] = iou\n\n        if iou < threshold:\n            continue\n\n        if iou > best_match_iou:\n            best_match_iou = iou\n            best_match_idx = gt_idx\n\n    return best_match_idx\n\ndef calculate_precision(gts, preds, threshold = 0.5, form = 'coco', ious=None) -> float:\n    # https:\/\/www.kaggle.com\/sadmanaraf\/wheat-detection-using-faster-rcnn-train\n    \"\"\"Calculates precision for GT - prediction pairs at one threshold.\n\n    Args:\n        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n               sorted by confidence value (descending)\n        threshold: (float) Threshold\n        form: (str) Format of the coordinates\n        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n\n    Return:\n        (float) Precision\n    \"\"\"\n    n = len(preds)\n    tp = 0\n    fp = 0\n    fn = 0\n    tn = 0\n    \n    for pred_idx in range(n):\n\n        best_match_gt_idx = find_best_match(gts, preds[pred_idx], pred_idx,\n                                            threshold=threshold, form=form, ious=ious)\n\n        if best_match_gt_idx >= 0:\n            # True positive: The predicted box matches a gt box with an IoU above the threshold.\n            tp += 1\n            # Remove the matched GT box\n            gts[best_match_gt_idx] = -1\n        else:\n            # No match\n            # False positive: indicates a predicted box had no associated gt box.\n            fp += 1\n\n    # False negative: indicates a gt box had no associated predicted box.\n    \n    fn = (gts.sum(axis=1) > 0).sum()\n    tn = (gts.sum(axis=1) < 0).sum()\n\n    return tp \/ (tp + fp + fn) ,tp ,fp ,fn ,tn\n\n\n# Albumentations\ndef get_train_transform():\n    return A.Compose([\n        A.Flip(0.5),\n        A.RandomRotate90(0.5),\n        MotionBlur(p=0.2),\n        MedianBlur(blur_limit=3, p=0.1),\n        Blur(blur_limit=3, p=0.1),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef get_valid_transform():\n    return A.Compose([\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ndef prepare_data():\n    DIR_INPUT = '..\/input\/rsna-pneumonia-detection-2018\/input'\n    DIR_TRAIN = f\"{DIR_INPUT}\/images\/\"\n\n    train_df = pd.read_csv(f\"{DIR_INPUT}\/stage_2_train_labels.csv\")\n    print(train_df.shape)\n    train_df.head()\n\n    train_df_pos = pd.DataFrame(columns=['patientId', 'x', 'y', 'width', 'height'])\n\n    k = 0\n    for i in range(len(train_df)):\n        if train_df.loc[i]['Target'] == 1:\n            train_df_pos.loc[k] = train_df.loc[i]\n            k += 1\n\n    image_ids = train_df_pos['patientId'].unique()\n    valid_ids = image_ids[-300:]\n    train_ids = image_ids[:-300]\n    print(f\"Training instance: {len(train_ids)}\")\n    print(f\"Validation instances: {len(valid_ids)}\")\n\n    valid_df = train_df_pos[train_df_pos['patientId'].isin(valid_ids)]\n    train_df = train_df_pos[train_df_pos['patientId'].isin(train_ids)]\n\n    valid_df.shape, train_df.shape\n    \n    train_dataset = dataset.RSNADataset(train_df, DIR_TRAIN, get_train_transform())\n    valid_dataset = dataset.RSNADataset(valid_df, DIR_TRAIN, get_valid_transform())\n    \n    return train_dataset, valid_dataset\n    \ndef get_data_loader(batch_size):\n    \n    train_dataset, valid_dataset = prepare_data()\n    \n    train_data_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=4, # else showing broken pipe error\n        collate_fn=collate_fn\n    )\n\n    valid_data_loader = DataLoader(\n        valid_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=4, # else showing broken pipe error\n        collate_fn=collate_fn\n    )\n    return train_data_loader, valid_data_loader\n\nclass Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total \/ self.iterations\n\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n        \ndef train(dataloader, lr_scheduler, model, optimizer, \n          device, epoch, loss_hist, itr):\n    model.train()\n    start = time.time()\n    loss_hist.reset()\n    for images, targets, image_ids in dataloader:\n        \n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n\n\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        loss_hist.send(loss_value)\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        if itr % 50 == 0:\n            print(f\"Epoch #{epoch} iteration #{itr} loss: {loss_value}\")\n\n        itr += 1\n    \n    end = time.time()\n    return loss_hist, end, start\n\ndef validate(dataloader, model, device, iou_thresholds):\n    valid_image_precision = []\n    valid_accuracy = []\n    v_TP, v_FP, v_FN, v_TN = [],[],[],[]\n    model.eval()\n    with torch.no_grad():\n        for images, targets, image_ids in dataloader:\n\n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            outputs = model(images)\n    #print(outputs)\n    for i, image in enumerate(images):\n        boxes = outputs[i]['boxes'].data.cpu().numpy()\n        scores = outputs[i]['scores'].data.cpu().numpy()\n        gt_boxes = targets[i]['boxes'].cpu().numpy()\n        preds_sorted_idx = np.argsort(scores)[::-1]\n        preds_sorted = boxes[preds_sorted_idx]\n        image_precision, TP, FP, FN,TN, accuracy = calculate_image_precision(preds_sorted,\n                                                        gt_boxes,\n                                                        thresholds=iou_thresholds,\n                                                        form='coco')\n        valid_image_precision.append(image_precision)\n        valid_accuracy.append(accuracy)\n        v_TP.append(TP)\n        v_FP.append(FP)\n        v_FN.append(FN)\n        v_TN.append(TN)\n    valid_prec = np.mean(valid_image_precision)\n    valid_acc = np.mean(valid_accuracy)\n    cmat = [[v_TP, v_FN], [v_FP, v_TN]]\n    return valid_prec, cmat, valid_acc","01a5aecd":"%%writefile train.py\n\nimport torch\nimport engine\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport argparse\nimport cv2\nimport seaborn as sns\nfrom engine import get_data_loader, Averager, train, validate\nfrom model import model\n# from torch.utils.data.sampler import SequentialSampler\n\nmatplotlib.style.use('ggplot')\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nparser = argparse.ArgumentParser()\nparser.add_argument('-s', '--show-sample', dest='show_sample', default='no', \n                 help='whether to visualize a wheat sample with bboxes or not')\nargs = vars(parser.parse_args())\n\n# learning parameters\nnum_epochs = 1\nlr = 0.001\nbatch_size = 8\n\nmodel = model().to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=lr, momentum=0.9, weight_decay=0.0005)\n# optimizer = torch.optim.Adam(params, lr=0.01)\n# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\nlr_scheduler = None\n\n# initialize the Averager\nloss_hist = engine.Averager()\n# get the dataloader\ntrain_data_loader, valid_data_loader = get_data_loader(batch_size)\n\nif args['show_sample'] == 'yes':\n    images, targets, image_ids = next(iter(train_data_loader))\n    images = list(image.to(device) for image in images)\n    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n    boxes = targets[2]['boxes'].cpu().numpy().astype(np.int32)\n    sample = images[2].permute(1,2,0).cpu().numpy()\n    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\n    for box in boxes:\n        cv2.rectangle(sample,\n                      (box[0], box[1]),\n                      (box[2], box[3]),\n                      (220, 0, 0), 3)\n    \n    ax.set_axis_off()\n    ax.imshow(sample)\n    plt.show()\n\niou_thresholds = [x for x in np.arange(0.5, 0.76, 0.05)]\n\ntrain_loss = []\nprecision = []\nfor epoch in range(num_epochs):\n    itr = 1\n    train_loss_hist, end, start = train(train_data_loader, lr_scheduler,\n                                        model, optimizer, device,\n                                        epoch, loss_hist, itr)\n    valid_prec, cmat, valid_acc = validate(valid_data_loader, model, device, iou_thresholds)\n    plt.figure(figsize = (6,6))\n    sns.heatmap(cmat\/np.sum(cmat), cmap=\"Reds\", annot=True, fmt = '.2%', square=1,   linewidth=2.)\n    plt.xlabel(\"predictions\")\n    plt.ylabel(\"real values\")\n    plt.show()\n    print(f\"Took {(end-start)\/60:.3f} minutes for epoch# {epoch} to train\")\n    print(f\"Epoch #{epoch} Train loss: {train_loss_hist.value}\")  \n    print(f\"Epoch #{epoch} Validation Precision: {valid_prec}\")  \n    train_loss.append(train_loss_hist.value)\n    precision.append(valid_prec)\n    \n    # update the learning rate\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n\ntorch.save(model.state_dict(), 'fasterrcnn_resnet50_fpn.pth')\n\n# plot and save the training loss\nplt.figure()\nplt.plot(train_loss, label='Training loss')\nplt.legend()\nplt.show()\nplt.savefig('loss.png')\n\n# plot and save the validation precision\nplt.figure()\nplt.plot(precision, label='Validation precision')\nplt.legend()\nplt.show()\nplt.savefig('precision.png')","712cf983":"!python train.py --show-sample yes","0759b075":"!zip fasterrcnn.zip ","b9188e10":"%%writefile test.py\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport os\nimport re\nimport albumentations as A\nimport torch\nimport torchvision\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGeneratoar\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\nfrom PIL import Image\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nDIR_INPUT = '..\/input\/rsna-pneumonia-detection-2018\/input'\nDIR_TEST = f\"{DIR_INPUT}\/samples\"\ntest_images = os.listdir(DIR_TEST)\nprint(f\"Validation instances: {len(test_images)}\")\n\n# load a model; pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, min_size=1024)\nnum_classes = 2  # 1 class (pnueomonia) + background\n# get the number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\nos.makedirs('..\/validation_predictions', exist_ok=True)\nmodel.load_state_dict(torch.load('..\/input\/rsna-pytorch-hackathon-fasterrcnn-resnet-training\/fasterrcnn_resnet50_fpn.pth'))\nmodel.to(device)\n\ndef format_prediction_string(boxes, scores):\n    pred_strings = []\n    for j in zip(scores, boxes):\n        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], \n                                                             int(j[1][0]), int(j[1][1]), \n                                                             int(j[1][2]), int(j[1][3])))\n\n    return \" \".join(pred_strings)\n\ndetection_threshold = 0.9\nimg_num = 0\nresults = []\nmodel.eval()\nwith torch.no_grad():\n    for i, image in tqdm(enumerate(test_images), total=len(test_images)):\n\n        orig_image = cv2.imread(f\"{DIR_TEST}\/{test_images[i]}\", cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        image = np.transpose(image, (2, 0, 1)).astype(np.float)\n        image = torch.tensor(image, dtype=torch.float).cuda()\n        image = torch.unsqueeze(image, 0)\n\n        model.eval()\n        cpu_device = torch.device(\"cpu\")\n\n        outputs = model(image)\n        \n        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n        if len(outputs[0]['boxes']) != 0:\n            for counter in range(len(outputs[0]['boxes'])):\n                boxes = outputs[0]['boxes'].data.cpu().numpy()\n                scores = outputs[0]['scores'].data.cpu().numpy()\n                boxes = boxes[scores >= detection_threshold].astype(np.int32)\n                draw_boxes = boxes.copy()\n                boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n                boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n                \n            for box in draw_boxes:\n                cv2.rectangle(orig_image,\n                            (int(box[0]), int(box[1])),\n                            (int(box[2]), int(box[3])),\n                            (0, 0, 255), 3)\n        \n            plt.imshow(cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB))\n            plt.axis('off')\n            plt.savefig(f\"{test_images[i]}\")\n            plt.close()\n                \n            result = {\n                'patientId': test_images[i].split('.')[0],\n                'PredictionString': format_prediction_string(boxes, scores)\n            }\n            results.append(result)\n        else:\n            result = {\n                'patientId': test_images[i].split('.')[0],\n                'PredictionString': None\n            }\n            results.append(result)\n\nsub_df = pd.DataFrame(results, columns=['patientId', 'PredictionString'])\nprint(sub_df.head())\nsub_df.to_csv('submission.csv', index=False)","e19181ba":"!python test.py","f9d73388":"from IPython.display import Image\nImage(filename='.\/loss.png')\n","df3e3f13":"Image(filename='.\/precision.png')","7485a097":"LR = 0.005\nEPOCHS = 2\nBATCHSIZE = 32\nCHANNELS = 64\nIMAGE_SIZE = 256\nNBLOCK = 6 \nDEPTH = 2\nMOMENTUM = 0.9\n\nfrom skimage import measure\nfrom skimage.transform import resize\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport os\nfrom matplotlib import pyplot as plt","0b747b6a":"import csv\nimport random\n# Load pneumonia locations\n\n# empty dictionary\npneumonia_locations = {}\n# load table\nwith open(os.path.join('..\/input\/rsna-pneumonia-detection-challenge\/stage_2_train_labels.csv'), mode='r') as infile:\n    # open reader\n    reader = csv.reader(infile)\n    # skip header\n    next(reader, None)\n    # loop through rows\n    for rows in reader:\n        # retrieve information\n        filename = rows[0]\n        location = rows[1:5]\n        pneumonia = rows[5]\n        # if row contains pneumonia add label to dictionary\n        # which contains a list of pneumonia locations per filename\n        if pneumonia == '1':\n            # convert string to float to int\n            location = [int(float(i)) for i in location]\n            # save pneumonia location in dictionary\n            if filename in pneumonia_locations:\n                pneumonia_locations[filename].append(location)\n            else:\n                pneumonia_locations[filename] = [location]\n                \n                \n# Load filenames\n\n# load and shuffle filenames\nfolder = '..\/input\/rsna-pneumonia-detection-challenge\/stage_2_train_images'\nfilenames = os.listdir(folder)\nfilenames = filenames[:2000]\nrandom.shuffle(filenames)\n# split into train and validation filenames\nn_valid_samples = 2560\ntrain_filenames = filenames[:1800]\nvalid_filenames = filenames[1800:]\nprint('n train samples', len(train_filenames))\nprint('n valid samples', len(valid_filenames))\nn_train_samples = len(filenames) - n_valid_samples","725dee30":"# Data generator\n\nclass generator(keras.utils.Sequence):\n    \n    def __init__(self, folder, filenames, pneumonia_locations=None, batch_size=BATCHSIZE, \n                 image_size=IMAGE_SIZE, shuffle=True, augment=False, predict=False):\n        self.folder = folder\n        self.filenames = filenames\n        self.pneumonia_locations = pneumonia_locations\n        self.batch_size = batch_size\n        self.image_size = image_size\n        self.shuffle = shuffle\n        self.augment = augment\n        self.predict = predict\n        self.on_epoch_end()\n        \n    def __load__(self, filename):\n        # load dicom file as numpy array\n        img = dcm.dcmread(os.path.join(self.folder, filename)).pixel_array\n        # default negative\n        target = 0\n        # get filename without extension\n        filename = filename.split('.')[0]\n        # if image contains pneumonia\n        if filename in pneumonia_locations:\n            target = 1\n        # resize both image and mask\n        img = resize(img, (self.image_size, self.image_size), mode='reflect')\n        # if augment then horizontal flip half the time\n        if self.augment and random.random() > 0.5:\n            img = np.fliplr(img)\n        # add trailing channel dimension\n        img = np.expand_dims(img, -1)\n        return img, target\n    \n    def __loadpredict__(self, filename):\n        # load dicom file as numpy array\n        img = dcm.dcmread(os.path.join(self.folder, filename)).pixel_array\n        # resize image\n        img = resize(img, (self.image_size, self.image_size), mode='reflect')\n        # add trailing channel dimension\n        img = np.expand_dims(img, -1)\n        return img\n        \n    def __getitem__(self, index):\n        # select batch\n        filenames = self.filenames[index*self.batch_size:(index+1)*self.batch_size]\n        # predict mode: return images and filenames\n        if self.predict:\n            # load files\n            imgs = [self.__loadpredict__(filename) for filename in filenames]\n            # create numpy batch\n            imgs = np.array(imgs)\n            return imgs, filenames\n        # train mode: return images and masks\n        else:\n            # load files\n            items = [self.__load__(filename) for filename in filenames]\n            # unzip images and masks\n            imgs, targets = zip(*items)\n            # create numpy batch\n            imgs = np.array(imgs)\n            targets = np.array(targets)\n            return imgs, targets\n        \n    def on_epoch_end(self):\n        if self.shuffle:\n            random.shuffle(self.filenames)\n        \n    def __len__(self):\n        if self.predict:\n            # return everything\n            return int(np.ceil(len(self.filenames) \/ self.batch_size))\n        else:\n            # return full batches only\n            return int(len(self.filenames) \/ self.batch_size)","af1ea353":"# Network\n\ndef convlayer(channels, inputs, size=3, padding='same'):\n    x = keras.layers.BatchNormalization(momentum=MOMENTUM)(inputs)\n    x = keras.layers.LeakyReLU(0)(x)\n    x = keras.layers.Conv2D(channels, size, padding=padding, use_bias=False)(x)\n    return x\n\ndef just_downsample(inputs, pool=2):\n    x = keras.layers.BatchNormalization(momentum=MOMENTUM)(inputs)\n    x = keras.layers.LeakyReLU(0)(x)\n    x = keras.layers.MaxPool2D(pool)(x)\n    return x\n\ndef convblock(inputs, channels1, channels2):\n    x = convlayer(channels1, inputs)\n    x = convlayer(channels2, x)\n    x = keras.layers.Concatenate()([inputs, x])\n    return x\n\ndef denseblock(inputs, nblocks=6, channels1=128, channels2=32):\n    x = inputs\n    for i in range(nblocks):\n        x = convblock(x, channels1, channels2)\n    x = keras.layers.SpatialDropout2D(.2)(x)\n    return x\n\ndef transition(inputs, channels, pool=2):\n    x = convlayer(channels, inputs)\n    x = keras.layers.AveragePooling2D(pool)(x)\n    return x\n    \ndef create_network(input_size, channels=64, channels2=32, n_blocks=NBLOCK, depth=DEPTH):\n    # input\n    inputs = keras.Input(shape=(input_size, input_size, 1))\n    x = keras.layers.Conv2D(channels, 3, padding='same', strides=2, use_bias=False)(inputs)\n    x = just_downsample(x)\n\n    # densenet blocks\n    nchan = channels\n    for d in range(depth-1):\n        x = denseblock(x)\n        nchan = ( nchan + n_blocks*channels2 ) \/\/ 2\n        x = transition(x, nchan)\n    x = denseblock(x)\n\n    # output\n    x = convlayer(channels, x)\n    x = keras.layers.BatchNormalization(momentum=0.9)(x)\n    x = keras.layers.GlobalAveragePooling2D()(x)\n    x = keras.layers.LeakyReLU(0)(x)\n    x = keras.layers.Dropout(.5)(x)\n    output = keras.layers.Dense(1, activation='sigmoid')(x)\n    model = keras.Model(inputs=inputs, outputs=output)\n    return model","6458511f":"# create network and compiler\nmodel = create_network(input_size=IMAGE_SIZE, channels=CHANNELS, n_blocks=NBLOCK, depth=DEPTH)\nmodel.summary()","05685628":"model.compile(optimizer=keras.optimizers.Adam(lr=LR),\n              loss=keras.losses.binary_crossentropy, metrics=['accuracy'])","2b40599c":"train_gen = generator(folder, train_filenames, pneumonia_locations, batch_size=BATCHSIZE, \n                      image_size=IMAGE_SIZE, shuffle=True, augment=True, predict=False)\nvalid_gen = generator(folder, valid_filenames, pneumonia_locations, batch_size=BATCHSIZE, \n                      image_size=IMAGE_SIZE, shuffle=False, predict=False)\n\nhistory = model.fit_generator(train_gen, validation_data=valid_gen, \n                              epochs=EPOCHS, shuffle=True, verbose=2)","d5d8d878":"\ngc.collect()","8b52055e":"We may need to upsample 1 so that the model doesn't biase","c4a3f66a":"![Screen Shot 2021-06-21 at 1.56.18.png](attachment:1fe6021a-79b5-4673-a389-38431b446aec.png)","036a787d":"**Model Creation**","6b79f3e0":"![Screen Shot 2021-06-21 at 1.50.18.png](attachment:e9371552-32d1-4ae0-8e62-e019cb92a09d.png)","afaadef5":"**Model I : U-Net**","5e8c7a00":"Now we have to count of Train and Test Data here","83ce12c6":"![Screen Shot 2021-06-21 at 1.57.22.png](attachment:0efae77a-0ae6-4d30-b1a1-9807a6d32b9a.png)","95153d31":"![Screen Shot 2021-06-21 at 1.58.20.png](attachment:43e8e679-0131-46c6-aa55-bd4328371e57.png)","2a01df5c":"**Model III : DenseNet**","7105186e":"![Screen Shot 2021-06-21 at 2.00.50.png](attachment:1a1ba4d7-3e69-4e4c-839a-202d28884e35.png)","ecae7494":"Now let's create the mask images are per the given coordinates","c52c0d68":"**Load Datasets**","c3dfb56c":"**Model II: Faster RCNN**","0004f17b":"x, y, width, height are float type therefore we can say that they all have the numerical values","5175123d":"No missing value found in the class_info table\n\nAll the target values are filled therefore there is no empty value, cell with Nan or empty cell are having no phenumonia so we should replace those value with 0","7ac03786":"Noraml has been classified as 0 too","c296de0e":"No Lung Opacity \/ Not Normal has been classified as Target 0","2e5d35df":"Let display some images that has been classified as a case of phenumonia","8948ffb2":"The above gives more information on the individual data, but in our modeling case these are not relevant","d87806aa":"Also display the image of normal lungs so that we can have idea about how normal lungs will look","0a114567":"We can divide the dataset in 3 classes\n\n1. confirmed phenumoia\n2. partial phenumonia\n3. Normal Lungs","aa029607":"![Screen Shot 2021-06-21 at 1.59.19.png](attachment:1e4fe86e-55bb-4fb2-8592-dbe5c179fc45.png)","502c9fd3":"![Screen Shot 2021-06-20 at 11.28.12.png](attachment:68f00087-62d4-4303-a40e-ff5279194a3f.png)\n<center>\n<span style=\"font-family: Arial; font-weight:bold;font-size:1.9em;color:#9900ff\"> <h3> Pneumonia Detection Using computer vision\n<\/center>\n<div>\n    <span style=\"font-family: Arial; font-weight:bold;color:#ff00ff\">\n\n##### OverView:\n    Pneumonia accounts for over 15% of all deaths of children under 5 years old internationally. In 2015, 920,000 children under the age of 5 died from the disease. In the United States, pneumonia accounts for over 500,000 visits to emergency departments [1] and over 50,000 deaths in 2015 [2], keeping the ailment on the list of top 10 causes of death in the country.\n    While common, accurately diagnosing pneumonia is a tall order. It requires review of a chest radiograph (CXR) by highly trained specialists and confirmation through clinical history, vital signs and laboratory exams. Pneumonia usually manifests as an area or areas of increased opacity [3] on CXR. However, the diagnosis of pneumonia on CXR is complicated because of a number of other conditions in the lungs such as fluid overload (pulmonary edema), bleeding, volume loss (atelectasis or collapse), lung cancer, or post-radiation or surgical changes. Outside of the lungs, fluid in the pleural space (pleural effusion) also appears as increased opacity on CXR. When available, comparison of CXRs of the patient taken at different time points and correlation with clinical symptoms and history are helpful in making the diagnosis.\n        CXRs are the most commonly performed diagnostic imaging study. A number of factors such as positioning of the patient and depth of inspiration can alter the appearance of the CXR [4], complicating interpretation further. In addition, clinicians are faced with reading high volumes of images every shift.\n        The RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for ML to automate initial detection (imaging screening) of potential pneumonia cases in order to prioritize and expedite their review.\n        \n##### Objective:\n    In this capstone project, the goal is to build a pneumonia detection system, to locate the position of inflammation in an image.\n\n##### Exploratory Data Analysis\n    - Univariate Analysis - Outlier and Frequency Analysis\n    - Bivariate Analysis - Visu**alization\n    - Variable Reduction - Multicollinearity\n\n##### Data Pre-Processing - \n    - Missing Values Treatment \n    - The IoU of a set of predicted bounding boxes and ground truth bounding boxes is calculated as:\n        \ud835\udc3c\ud835\udc5c\ud835\udc48(\ud835\udc34,\ud835\udc35)=\ud835\udc34\u2229\ud835\udc35\ud835\udc34\u222a\ud835\udc35.\n    -The average precision of a single image is calculated as the mean of the above precision values at each IoU  threshold =   1|\ud835\udc61\u210e\ud835\udc5f\ud835\udc52\ud835\udc60\u210e\ud835\udc5c\ud835\udc59\ud835\udc51\ud835\udc60|\u2211\ud835\udc61\ud835\udc47\ud835\udc43(\ud835\udc61)\ud835\udc47\ud835\udc43(\ud835\udc61)+\ud835\udc39\ud835\udc43(\ud835\udc61)+\ud835\udc39\ud835\udc41(\ud835\udc61).\n    \n##### Model Build and Model Diagnostics\n    - Train and Test split\n    - MobileNet\n    - Resnet50\n    - DenseNet121\n\n##### Model Validation\n       -Dice Coeffiecient\n       -Precision\n       -Accuracy\n    - \n##### Choose the best model\n    -  yet to tune the models\n\n\n<\/div>","580e7a67":"**Database table Preprocessing**","59ec8b96":"**Import the Necessary Libraries**","1d5c2d10":"**Data Visualisation**","c69d6b5f":"Lung Opacity has been classified as 1 and other classes has been classified as Normal indicated as 0\n\nThe class value 'No Lung Opacity \/ Not Normal ' is classified as 0 as it could be case of other complication which is not phenumonia","a7e6dc0a":"from here we can get an idea of mean width and mean height of the phenumoniatic image areas","6e612a80":"We have 2 view positions in the images"}}