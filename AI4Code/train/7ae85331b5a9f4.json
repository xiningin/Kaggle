{"cell_type":{"6b18022d":"code","12102cc5":"code","d4f37505":"code","eb142530":"code","420cfed3":"code","9de7fa4f":"code","517de4f2":"code","1531fd50":"code","4d09cfb8":"code","64bf33b2":"code","cfd1bdc6":"code","b8b6888c":"code","cf45de23":"code","1f91e690":"code","30a86ae8":"code","28bffd23":"code","c67ec1f2":"code","c1845ec5":"code","1d4366c9":"code","e73c5811":"code","9a3b2c03":"code","7d63e2de":"code","8338b54f":"code","72ba642b":"code","868845f0":"markdown","42df2a03":"markdown","81cd13cf":"markdown","82adcdcd":"markdown","09fc66e9":"markdown","ae7a0c29":"markdown","c764c21b":"markdown","88dcb398":"markdown","c2f8f605":"markdown","4998ac20":"markdown","c22c1709":"markdown","41110995":"markdown","7222440f":"markdown","ff27d837":"markdown","cd6e08d1":"markdown","c824f469":"markdown","318203b2":"markdown","d8cfe722":"markdown","247dfb1e":"markdown"},"source":{"6b18022d":"import os\nimport cv2\nimport glob\nimport time\nimport urllib\nimport requests\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\nfrom jax import make_jaxpr\nfrom jax.config import config\nfrom jax import grad, vmap, pmap, jit\n\n\n\n%config IPCompleter.use_jedi = False","12102cc5":"if 'TPU_NAME' in os.environ:\n    if 'TPU_DRIVER_MODE' not in globals():\n        url = 'http:' + os.environ['TPU_NAME'].split(':')[1] + ':8475\/requestversion\/tpu_driver_nightly'\n        resp = requests.post(url)\n        config.FLAGS.jax_xla_backend = \"tpu_driver\"\n        config.FLAGS.jax_backend_target = os.environ['TPU_NAME']\n        print('Registered TPU:', config.FLAGS.jax_backend_target)\n        print(\"\")\n        print(\"TPU devices found:\")\n        for device in jax.devices():\n            print(device)\nelse:\n    print('No TPUs found!\".')","d4f37505":"def dot_product(array1, array2):\n    \"\"\"Performs dot product on two jax arrays.\"\"\"\n    return jnp.dot(array1, array2)","eb142530":"def print_results(array1, array2, res, title=\"\"):\n    \"\"\"Utility to print arrays and results\"\"\"\n    if title:\n        print(title)\n        print(\"\")\n    print(\"First array => Shape: \", array1.shape)\n    print(array1)\n    print(\"\")\n    print(\"Second array => Shape: \", array2.shape)\n    print(array2)\n    print(\"\")\n    print(\"Results => Shape: \", res.shape)\n    print(res)","420cfed3":"array1 =  jnp.array([1, 2, 3, 4])\narray2 =  jnp.array([5, 6, 7, 8])\nres = dot_product(array1, array2)\n\nprint_results(array1, array2, res, title=\"Dot product of two vectors\")","9de7fa4f":"# What if we want to do this for a batch of vectors?\narray1 = jnp.stack([jnp.array([1, 2, 3, 4]) for i in range(5)])\narray2 = jnp.stack([jnp.array([5, 6, 7, 8]) for i in range(5)])\n\n# First way to do batch vector product using loops\nres1 = []\nfor i in range(5):\n    res1.append(dot_product(array1[i], array2[i]))\nres1 = jnp.stack(res1)\n\n    \n# In numpy, we can use `einsum` for the same\nres2 = np.einsum('ij,ij-> i', array1, array2)\n\n# We can even simplify einsum and chain two oprations to\n# achieve the same\nres3 = np.sum(array1*array2, axis=1)\n\n# Let's check the results\nprint_results(array1,\n              array2,\n              res1,\n              title=\"1. Dot product on a batch of vectors using loop\")\nprint(\"=\"*70, \"\\n\")\nprint_results(array1,\n              array2,\n              res2,\n              title=\"2. Dot product on a batch of vectors in numpy using einsum\")\nprint(\"=\"*70, \"\\n\")\nprint_results(array1,\n              array2,\n              res3,\n              title=\"3. Dot product on a batch of vectors using elementwise multiplication and sum\")","517de4f2":"# Transform the `dot_product` function defined above\n# using the `vmap` transformation\nbatch_dot_product = vmap(dot_product, in_axes=(0, 0))\n\n# What does the transformation return?\nbatch_dot_product","1531fd50":"# Using vmap transformed function\nres4 = batch_dot_product(array1, array2)\nprint_results(array1,\n              array2,\n              res4,\n              title=\"Dot product of a batch of vectors using vmap\")","4d09cfb8":"# A vector\narray1 = jnp.array([1, 2, 3, 4])\n\n# We have a batch of vectors as well already `array2` which looks like this\n# [[5 6 7 8]\n# [5 6 7 8]\n# [5 6 7 8]\n# [5 6 7 8]\n# [5 6 7 8]]\n\n# We will now perform the dot product of array1 (a single vetor) with a batch\n# of vectors (array2 in this case). We will pass `None` in the `in_axes(..)` argument\n# to say that the first input doesn't have a batch dimension\n\nres5 = vmap(dot_product, in_axes=(None, 0))(array1, array2)\nprint_results(array1,\n              array2,\n              res5,\n              title=\"Only one of the inputs in batched\")","64bf33b2":"# Like JIT, you can inpsect the transformation using jaxprs\nmake_jaxpr(vmap(dot_product, in_axes=(None, 0)))(array1, array2)","cfd1bdc6":"def download_images():\n    urllib.request.urlretrieve(\"https:\/\/i.imgur.com\/Bvro0YD.png\", \"elephant.png\")\n    urllib.request.urlretrieve(\"https:\/\/images-eu.ssl-images-amazon.com\/images\/I\/A1WuED4KiRL.jpg\", \"cat.jpg\")\n    urllib.request.urlretrieve(\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/1\/18\/Dog_Breeds.jpg\", \"dog.jpg\")\n    urllib.request.urlretrieve(\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/1\/1e\/The_Korean_Lucky_Bird_%28182632069%29.jpeg\", \"bird.jpg\")\n    urllib.request.urlretrieve(\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/ea\/Vervet_Monkey_%28Chlorocebus_pygerythrus%29.jpg\", \"monkey.jpg\")\n    urllib.request.urlretrieve(\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fa\/Puppy.JPG\", \"puppy.jpg\")\n    urllib.request.urlretrieve(\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/2\/2c\/Lion-1.jpg\", \"lion.jpg\")\n    urllib.request.urlretrieve(\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/4\/41\/Siberischer_tiger_de_edit02.jpg\", \"tiger.jpg\")\n    print(\"Downloading finished\")\n    \n    \n# Download the images\ndownload_images()","b8b6888c":"def read_images(size=(800, 800)):\n    \"\"\"Read jpg\/png images from the disk.\n    \n    Args:\n        size: Size to be used while resizing\n    Returns:\n        A JAX array of images\n    \"\"\"\n    png_images = sorted(glob.glob(\"*.png\"))\n    jpg_images = sorted(glob.glob(\"*.jpg\"))\n    all_images = png_images + jpg_images\n    \n    images = []\n    \n    for img in all_images:\n        img = cv2.imread(img)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, size)\n        images.append(img)\n        \n    return jnp.array(images)\n\n\n# Read and resize\nimages = read_images()\nprint(\"Total number of images: \", len(images))","cf45de23":"# Utility function for plotting the images\ndef plot_images(images, batch_size, num_cols=4, figsize=(15, 8), title=\"Images \"):\n    num_rows = batch_size \/\/ num_cols\n    \n    _, ax = plt.subplots(num_rows, num_cols, figsize=figsize)\n    \n    for i, img in enumerate(images):\n        ax[i \/\/ num_cols, i % num_cols].imshow(images[i])\n        ax[i \/\/ num_cols, i % num_cols].axis(\"off\")\n        #ax[i \/\/ num_cols, i % num_cols].set_title(str(i+1))\n        \n    plt.tight_layout()\n    plt.suptitle(title, x=0.5, y=1.0, fontsize=16)\n    plt.show()","1f91e690":"def rotate_img(img):\n    return jnp.rot90(img, axes=(0, 1))\n\ndef identity(img):\n    return img\n\ndef random_rotate(img, rotate):\n    \"\"\"Randomly rotate an image by 90 degrees.\n    \n    Args:\n        img: Array representing the image\n        rotate: Boolean for rotating or not\n    Returns:\n        Either Rotated or an identity image\n    \"\"\"\n    \n    return jax.lax.cond(rotate, rotate_img, identity, img)","30a86ae8":"# Run the pipeline on a single image\n\n# Get an image\nimg = images[0]\nimg_copy = img.copy()\n\n# Pass the image copy to augmentation pipeline\naugmented = random_rotate(img_copy, 1)\n\n# Plot the original image and the augmented image\n_, ax = plt.subplots(1, 2, figsize=(12, 8))\n\nax[0].imshow(img)\nax[0].axis(\"off\")\nax[0].set_title(\"Original Image\")\n\nax[1].imshow(augmented)\nax[1].axis(\"off\")\nax[1].set_title(\"Augmented Image\")\n\nplt.show()","28bffd23":"# Using the same original image\nimg_copy = img.copy()\n\n# Batch size of the output as well as for the boolean array\n# used to tell whether to rotate an input image or not\nbatch_size = 8\n\n# We use seed for anything that involves `random`\nkey = random.PRNGKey(1234)\n\n# Although splitting is not necessary as the key is only used once,\n# I will just leave the original key as it is\nkey, subkey = random.split(key)\nrotate = random.randint(key, shape=[batch_size], minval=0, maxval=2)\n\n# Return identical or flipped image via augmentation pipeline\n# We will transform the original `random_rotate(..)` function\n# using vmap\naugmented = vmap(random_rotate, in_axes=(None, 0))(img_copy, rotate)\n\nprint(\"Number of images to generate: \", batch_size)\nprint(\"Rotate-or-not array: \", rotate)\nplot_images(augmented,\n            batch_size=8,\n            title=\"Multiple augmenetd images from a single input image\"\n           )","c67ec1f2":"# Original images\nplot_images(images, batch_size=8, title=\"Original images\")","c1845ec5":"# Augment a batch of input images using the same augmentation pipeline\naugmented = vmap(random_rotate, in_axes=(0, 0))(images, rotate)\nplot_images(augmented, batch_size=8, title=\"Augmented Images\")","1d4366c9":"# JIT the vmapped function\nvmap_jitted = jit(vmap(random_rotate, in_axes=(0, 0)))\n\n# Run the pipeline again using the jitted function\naugmented = (vmap_jitted(images, rotate)).block_until_ready()\n\n# Plot the images and check the results\nplot_images(augmented, batch_size=8, title=\"Jitting vmapped function\")","e73c5811":"# Use jaxpr to see how the transformation ops are executed\nmake_jaxpr(jit(vmap(random_rotate, in_axes=(0, 0))))(images, rotate)","9a3b2c03":"def rotate_90(img):\n    \"\"\"Rotates an image by 90 degress k times.\"\"\"\n    return jnp.rot90(img, k=1, axes=(0, 1))\n\n\ndef identity(img):\n    \"\"\"Returns an image as it is.\"\"\"\n    return img\n\n\ndef flip_left_right(img):\n    \"\"\"Flips an image left\/right direction.\"\"\"\n    return jnp.fliplr(img)\n\n\ndef flip_up_down(img):\n    \"\"\"Flips an image in up\/down direction.\"\"\"\n    return jnp.flipud(img)\n\n\ndef random_rotate(img, rotate):\n    \"\"\"Randomly rotate an image by 90 degrees.\n    \n    Args:\n        img: Array representing the image\n        rotate: Boolean for rotating or not\n    Returns:\n        Rotated or an identity image\n    \"\"\"\n\n    return jax.lax.cond(rotate, rotate_90, identity, img)\n\n\ndef random_horizontal_flip(img, flip):\n    \"\"\"Randomly flip an image vertically.\n    \n    Args:\n        img: Array representing the image\n        flip: Boolean for flipping or not\n    Returns:\n        Flipped or an identity image\n    \"\"\"\n    \n    return jax.lax.cond(flip, flip_left_right, identity, img)\n    \n    \ndef random_vertical_flip(img, flip):\n    \"\"\"Randomly flip an image vertically.\n    \n    Args:\n        img: Array representing the image\n        flip: Boolean for flipping or not\n    Returns:\n        Flipped or an identity image\n    \"\"\"\n    \n    return jax.lax.cond(flip, flip_up_down, identity, img)\n\n\n# Get the jitted version of our augmentation functions\nrandom_rotate_jitted = jit(vmap(random_rotate, in_axes=(0, 0)))\nrandom_horizontal_flip_jitted = jit(vmap(random_horizontal_flip, in_axes=(0, 0)))\nrandom_vertical_flip_jitted = jit(vmap(random_vertical_flip, in_axes=(0, 0)))\n\n\ndef augment_images(images, key):\n    \"\"\"Augment a batch of input images.\n    \n    Args:\n        images: Batch of input images as a jax array\n        key: Seed\/Key for random functions for generating booleans\n    Returns:\n        Augmented images with the same shape as the input images\n    \"\"\"\n    \n    batch_size = len(images)\n    \n    # 1. Rotation\n    key, subkey = random.split(key)\n    rotate = random.randint(key, shape=[batch_size], minval=0, maxval=2)\n    augmented = random_rotate_jitted(images, rotate)\n    \n    # 2. Flip horizontally\n    key, subkey = random.split(key)\n    flip = random.randint(key, shape=[batch_size], minval=0, maxval=2)\n    augmented = random_horizontal_flip_jitted(augmented, flip)\n    \n    # 3. Flip vertically\n    key, subkey = random.split(key)\n    flip = random.randint(key, shape=[batch_size], minval=0, maxval=2)\n    augmented = random_vertical_flip_jitted(augmented, flip)\n    \n    return augmented.block_until_ready()","7d63e2de":"# Because we are jitting the transformations, we will record the\n# time taken for augmentation on subsequent calls\n\nfor i in range(3):\n    print(\"Call: \", i + 1, end=\" => \")\n    key=random.PRNGKey(0)\n    start_time = time.time()\n    augmented = augment_images(images, key)\n    print(f\"Time taken to generate augmentations: {time.time()-start_time:.2f}\")\n\n# Plot the augmented images    \nplot_images(augmented, batch_size=8, title=\"Augmenetd Images\")","8338b54f":"# Augment images function without `jit`\n# as jitting is not required while using pmap\n\n# Get the vmapped version of our augmentation functions\nrandom_rotate_vmapped = vmap(random_rotate, in_axes=(0, 0))\nrandom_horizontal_flip_vmapped = vmap(random_horizontal_flip, in_axes=(0, 0))\nrandom_vertical_flip_vmapped = vmap(random_vertical_flip, in_axes=(0, 0))\n\n\ndef augment_images(images, key):\n    \"\"\"Augment a batch of input images.\n    \n    Args:\n        images: Batch of input images as a jax array\n        key: Seed\/Key for random functions for generating booleans\n    Returns:\n        Augmented images with the same shape as the input images\n    \n    \"\"\"\n    \n    batch_size = len(images)\n    \n    # 1. Rotation\n    key, subkey = random.split(key)\n    rotate = random.randint(key, shape=[batch_size], minval=0, maxval=2)\n    augmented = random_rotate_vmapped(images, rotate)\n    \n    # 2. Flip horizontally\n    key, subkey = random.split(key)\n    flip = random.randint(key, shape=[batch_size], minval=0, maxval=2)\n    augmented = random_horizontal_flip_vmapped(augmented, flip)\n    \n    # 3. Flip vertically\n    key, subkey = random.split(key)\n    flip = random.randint(key, shape=[batch_size], minval=0, maxval=2)\n    augmented = random_vertical_flip_vmapped(augmented, flip)\n    \n    return augmented\n\n\n# Generate a big batch of 64\nbig_batch_images = jnp.stack([images for i in range(8)])\nprint(\"Number of images in batch: \", big_batch_images.shape[0])\n\n# Generate a batch of keys as well as the augment_images\n# function accepts a key as well\nkey = random.PRNGKey(123)\nbig_batch_keys = [key]\n\nfor i in range(7):\n    key, subkey = random.split(key)\n    big_batch_keys.append(key)\n    \nbig_batch_keys = jnp.stack(big_batch_keys)","72ba642b":"# Augment images parallely on multiple devices\npmapped_augment_images = pmap(augment_images, in_axes=(0, 0))\n\n# We will run it more than once\nfor i in range(3):\n    print(\"Call: \", i + 1, end=\" => \")\n    start_time = time.time()\n    augmented_parallel = pmapped_augment_images(big_batch_images, big_batch_keys)\n    print(f\"Time taken to generate augmentations: {time.time()-start_time:.2f}\")\n    \n\n# Plot the augmenetd images\naugmented_parallel = augmented_parallel.reshape(64, 800, 800, 3)\nplot_images(augmented_parallel,\n            batch_size=64,\n            title=\"Augmentation on parallel devices\",\n            figsize=(20, 25)\n           )","868845f0":"# References\n\n1. https:\/\/jax.readthedocs.io\/en\/latest\/jax-101\/03-vectorization.html\n2. https:\/\/github.com\/deepmind\/dm_pix\/\n3. http:\/\/matpalm.com\/blog\/crazy_large_batch_sizes\/","42df2a03":"## Step 4: Generating multiple augmented images from a single image\n\nWe will now generate `n` images from the same pipeline using `vmap`. We will keep `n=8` to keep it simple. This is the point where you will start to realize how powerful `vmap` is. ","81cd13cf":"## Dot product on a batch of vectors\n\nIf you have a batch of vectors, how would you perform the dot product for each pair of vectors in two arrays?\nA few options are:\n1. Using a loop: Loop over the batch size and perform dot product for each pair, store the result and return\n2. Using vectorized operations like `einsum`, or\n3. Using a combination of two or more vectorized operations like `elementwise product` followed by `elementwise` sum. Here `element` refers to a vector in a batch\n\nLet's see these in action now","82adcdcd":"**Note:** Both the arguments necessarily do not need to have a batch dimension. For example, we can take one vector and perform the dot product with a batch of some vectors. For the input that doesn't have a batch dimension, you can just pass `None` in the `in_axes(..)` argument. Let's take an example to make it clear.","09fc66e9":"We will now augment this input batch of images. Look at the inputs to the `in_axes()` argument carefully","ae7a0c29":"Did you notice that? `batch_dot_product` is just another function, a transformed version of the original`dot_product`\nfunction. And this is all that you need to do to get a version that runs in a vectorized fashion. Isn't that amazing?\n\nLet's use the transformed version `batch_dot_product` now to do dot product for a batch of vectors efficiently.","c764c21b":"# Data Augmentation - Building a simple, fast, and scalable pipeline\n\nWhat we did till now barely shows the real power of `vmap`. We will take a more complex op example to showcase how powerful and flexible `vmap` really is. This will also give you an idea of why using `vmap` and `pmap`(we will look into it later) feels like a **superpower**. \n\nBecause I love images, we will build an image data augmentation pipeline purely in JAX. We will then scale it using `vmap` and `pmap`. We will do the following steps in this whole process:\n\n1. Download a bunch of images from Google images\n2. Read the images and possibly resize them to a fairly moderate size\n3. We will build a pipeline to do augmentation on a single image\n4. We will use the same pipeline to generate a bunch of augmentations for the same image\n5. We will then use the same pipeline to do augmentation on a batch of images\n6. We will then scale the same pipeline to do augmentation on a much bigger batch size on parallel devices (GPUs\/TPUs)\n\n## Step 1: Download a bunch of images","88dcb398":"So, we just reused the same pipeline to generate multiple images that we wrote for augmenting a single image. At this point, you can say `Well, that's not very impressive. With a little more lines I could have done that without using vmap`. \n\nAnd if you are thinking in that direction, just hold tight! I am gonna change your perception by the end of the notebook\n\n## Step 4: Using the same augmentation pipeline to augment a batch of images\n\nIn the previous step, we used one image to generate a bunch of augmented images in one go. It was similar to the example where we saw that one of the inputs to the vmapped function is a single example while the others are batched. Now we will reuse the same pipeline to augment a batch of input images i.e.\n\n1. Provide a batch of input images to the pipeline\n2. Augment the input batch\n3. Get a batch of augmented images\n\nWe will now use all the `8` images we downloaded in the beginning as the input batch. Before we run the augmentation pipeline on these images, let's plot the original images once","c2f8f605":"## Step 2: Read and Resize\n\nAlthough we can read and resize on the fly, we only have 8 images, so we will read and resize the images before augmenting them. Again, these two steps will\/should be a part of your pipeline, we are just simplifying the example. We will use `(800, 800)` as the final size of the images","4998ac20":"# Introduction to `vmap`\n\nIn the above example, you can use the last two operations in JAX as well but we will take a look into a transformation that is literally the best of all. As I said earlier also, this is one of my favorite transformations in JAX - `vmap`\n\n## What is `vmap`?\n`vmap` is just another transformation like jit. It takes a function as an input along with the dimensions for the inputs and the outputs where the functions is to be mapped over to create a vectorized function. The syntax of `vmap` is like this: `vmap(function, in_axes, out_axes, ...)`\n\nHere `function` is the function that you want to vectorize. `in_axes` is the axis indices that represent the batch dimension in the inputs to the original function. Similarly, `out_axes` are the axis indices that represent the batch dimension in the output. I highly suggest going through the [vmap documentation](https:\/\/jax.readthedocs.io\/en\/latest\/jax.html#jax.vmap)\n\n**Pro-tip**: Read carefully about the `positional` and `keyword` arguments and their effects on `in_axes` and `out_axes`.\n\nWhen you transform a function using `vmap`, it returns a function that is a vectorized version of the original function. Let's see it in action","c22c1709":"So, we saw that I can write a function that acts on a single image, and\n\n1. Transform the same function to operate on batches using `vmap`\n2. Can generate multiple augmented images from a single image\n3. Can apply augmentation to different images in a batch\n4. We can jit the entire vmap transformation\n5. We can inspect the whole thing using jaxprs\n\n<a href=\"https:\/\/emoji.gg\/emoji\/MindBlown\"><img src=\"https:\/\/emoji.gg\/assets\/emoji\/MindBlown.png\" width=\"64px\" height=\"64px\" alt=\"MindBlown\" align=\"center\"><\/a>\n\n\nThis is not the end. I told you that by the end of the notebook I will convince you that `vmap` and `pmap` are a big deal. So, let's do something cool now\n\n## Step 5: Fast and Scalable Data Augmentation Pipeline\n\nWe will add more augmentations to make it, even more, compute-heavy. You can define your augmentation as well if you like, I will stick to the simple augmentations for the time being. We will use three different augmentations namely,\n\n1. Random Rotation by degrees\n2. Random Horizontal Flip\n3. Random Vertical Flip","41110995":"**Update - 23rd Dec, 2021**\n\nWe have completed the TF-JAX tutorials series. 10 notebooks that covers every fundamental aspect of both TensorFlow and JAX. Here are the links to the notebooks along with the Github repo details:\n\n### TensorFlow Notebooks:\n\n* [TF_JAX_Tutorials - Part 1](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part1)\n* [TF_JAX_Tutorials - Part 2](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part2)\n* [TF_JAX_Tutorials - Part 3](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part3)\n\n### JAX Notebooks:\n\n* [TF_JAX_Tutorials - Part 4 (JAX and DeviceArray)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-4-jax-and-devicearray)\n* [TF_JAX_Tutorials - Part 5 (Pure Functions in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-5-pure-functions-in-jax\/)\n* [TF_JAX_Tutorials - Part 6 (PRNG in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-6-prng-in-jax\/)\n* [TF_JAX_Tutorials - Part 7 (JIT in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-7-jit-in-jax)\n* [TF_JAX_Tutorials - Part 8 (Vmap and Pmap)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-8-vmap-pmap)\n* [TF_JAX_Tutorials - Part 9 (Autodiff in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-9-autodiff-in-jax)\n* [TF_JAX_Tutorials - Part 10 (Pytrees in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-10-pytrees-in-jax)\n\n### Github Repo with all notebooks in one place\nhttps:\/\/github.com\/AakashKumarNain\/TF_JAX_tutorials\n\n---\n\n\n<img src=\"https:\/\/raw.githubusercontent.com\/google\/jax\/main\/images\/jax_logo_250px.png\" width=\"300\" height=\"300\" align=\"center\"\/><br>\n\nWelcome to another JAX tutorial. I hope you all have been enjoying the JAX Tutorials so far. If you haven't gone through the previous tutorials, I highly suggest going through them. Here are the links:\n\n1. [TF_JAX_Tutorials - Part 1](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part1)\n2. [TF_JAX_Tutorials - Part 2](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part2)\n3. [TF_JAX_Tutorials - Part 3](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part3)\n4. [TF_JAX_Tutorials - Part 4 (JAX and DeviceArray)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-4-jax-and-devicearray)\n5. [TF_JAX_Tutorials - Part 5 (Pure Functions in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-5-pure-functions-in-jax\/)\n6. [TF_JAX_Tutorials - Part 6 (PRNG in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-6-prng-in-jax\/)\n7. [TF_JAX_Tutorials - Part 7 (JIT in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-7-jit-in-jax)\n\n\n\nPeople often ask me this: **If I know TensorFlow\/Torch, why should I bother learning about JAX?**<br>\nAlthough there are **n** reasons, I will present one of the concepts in JAX that would be enough to convince you to try it. Today, we will look into **`Auto Vectorization`**. We will talk about two transformations namely, **`vmap`** and **`pmap`**. Sit tight, you gonna experience something really cool today!","7222440f":"## Dot product of two vectors","ff27d837":"Not only did we auto-vectorize the code, but we also made it run on multiple devices, **without making any major changes to the code**. \n\n<a href=\"https:\/\/emoji.gg\/emoji\/MindBlown\"><img src=\"https:\/\/emoji.gg\/assets\/emoji\/MindBlown.png\" width=\"64px\" height=\"64px\" alt=\"MindBlown\" align=\"center\"><\/a>\n\n\nA few things that you should note:\n\n1. You can write code that works on a single example, vectorize it using `vmap` to run it on a batch, and you can run the same code on multiple devices using `pmap`\n2. The philosophy behind vmap is: **Write for a single example, run it for a batch**\n3. Notice the use of `seed` in the code. Because the random sequence is guaranteed to be the same, it is easy to debug the whole thing end-to-end\n4. Once you get used to the mental model of `vmap` and `pmap`, there is no turning back as you have witnessed how powerful these two transformations are\n\n\nThat's it for Part8! Only a few more fundamental concepts remaining, and we will soon dive into the `NNs in JAX` part.\nI hope you enjoyed the tutorial. I am also pretty sure that by now you are convinced that `vmap` and `pmap` are superpowers to have","cd6e08d1":"## Step 3: A very simple augmentation pipeline\n\nWe will start with a very basic and simple augmentation pipeline. We will use a single image as an input and the pipeline will return an augmented version of the image. The augmentation pipeline will either rotate the input image or keep it as it is depending on a boolean value where `0` means no rotating while `1` says to rotate the image by 90 degrees.","c824f469":"# Vmap and Jaxpr\n\nLike JIT, you can take the `vmap` transformed function and inspect the corresponding jaxpr to get an idea of how the ops are executed. This is another cool thing about JAX. You can take any transformed function and insect the corresponding `jaxpr`. Let's do that for our `batch_dot_product` function","318203b2":"# Parallelize the whole thing on multiple devices using `pmap`\n\n`pmap` is very similar to `vmap` in terms of API. While `vmap` does SIMD and `pamp` does SMPD. In the simplest terms, `pmap` takes your Python program and replicates it across multiple devices to run everything in parallel. So, instead of using only one GPU\/TPU, you can parallelize the workload across multiple GPUs\/TPUs. You can read about the API in detail [here](https:\/\/jax.readthedocs.io\/en\/latest\/jax.html#jax.pmap)\n\n**Note:** `pmap` compiles the underlying function. Although it can be combined with `jit`, it\u2019s usually unnecessary.\n\nTo apply `pamp` on our data augmentation pipeline, we will perform the following steps:\n1. Define a new version of the `augment_images` function as we don't need `jit` with `pmap`\n2. Instead of having one batch of size 8 (as we have a total of 8 images), we will use a batch size of 64 (we have 8 TPU devices, and we will run a batch size of 8 on a single device)\n3. I will use the same eight images to stack them to create a big batch size of 64. You can use different images as well\n4. As our original `augment_images(..)` function accepts a key as well. We need to generate a batch of keys as well\n\nLet's see it in action","d8cfe722":"Since `vmap` is just another transformation and we all know that JAX allows the composition of these transformations. To make this pipeline **even faster**, we can **`jit`** the `vmapped` function \ud83d\ude0e\ud83d\ude0e\ud83d\ude0e\n\nNot only we can jit it, we can inspect the **`jitted-vmapped`** function using `jaxprs`","247dfb1e":"# Introduction\n\nSimply put Automatic Vectorization is a way of converting a process that operates on a single example to something that can operate on a vector. Let's take a very basic example to understand this. \n\nLets' say you have two arrays: <br>\n\nArray 1 => 1 2 3 4 5 <br>\nArray 2 => 10 20 30 40 50<br>\n\nYou want to perform elementwise `addition` operation on these two arrays. One way to do this is using a loop like this<br>\n```python\nresult = []\nfor i in range(5):\n    res.append(array_1[i] + array_2[i])\n```\n\nAlthough this is right, this is doing one operation at a time. The addition procedure is the same for all the elements. Hence a better way to perform all the elementwise additions in one go. For example, the way you do it in `numpy`<br>\n```python\nres = array_1 + array_2\n```\n\nI hope it gave you some sense of what auto-vectorization is. You can read about it in detail [here](https:\/\/www.coursera.org\/lecture\/parallelism-ia\/2-2-vectorizing-your-code-VkFwo)\n\nBefore jumping to `vmap` transformation, we will take a few basic examples to demonstrate the need and ower of `vmap`. The first example focuses on the simple dot product of two vectors. Without any further due, let's jump in the code "}}