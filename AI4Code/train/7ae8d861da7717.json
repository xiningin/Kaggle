{"cell_type":{"73def763":"code","8cd7dd32":"code","e7d1545c":"code","75c49879":"code","e5b781c4":"code","418e37b6":"code","50865624":"code","49535643":"code","30c767b2":"markdown","85591024":"markdown","5e2191d6":"markdown","46cf4b16":"markdown","e5f0cd76":"markdown","e4a0ea4b":"markdown","4a9f68e2":"markdown","aa1cbc47":"markdown","52b90a29":"markdown"},"source":{"73def763":"#basic imports\n%matplotlib inline\n\nimport sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport torch\ntorch.set_default_tensor_type('torch.cuda.FloatTensor') #all tensors use the GPU\n\nfrom torchvision.models import vgg16\nfrom torch.nn import Module, Sequential, MSELoss, L1Loss\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\n#activation to transform input images between 0 and 1 to VGG expected input\ndef caffeActivation(x):\n    x = 255 * x\n    x1 = x[:,2:,:,:] - 103.939\n    x2 = x[:,1:2,:,:] - 116.779\n    x3 = x[:,:1,:,:] - 123.68\n\n    return torch.cat([x1,x2,x3], 1)\n\n#reshapes loaded images to pytorch convolutional networks\ndef torchFormat(x):\n    #creates batch size dimension and move channels to dim 1\n    reshaped = np.moveaxis(np.expand_dims(x, 0), -1,1)\n    return Variable(torch.tensor(reshaped, dtype=torch.float), requires_grad=False)\n\n#reshapes images from torch for plotting\ndef plotFormat(x):\n    #moves channels to last dimension\n    return np.moveaxis(x, 1, -1)[0]\n\n#opens an image and returns it resized and in range between 0 and 1\ndef openImage(file, size):\n    return np.asarray(Image.open(file).resize(resizeSize,Image.LANCZOS))\/255.\n\n","8cd7dd32":"#a plotting function\ndef plotImages(columns,*images, **kwargs):\n    #kwargs handling\n    rowHeight = kwargs.pop('rowHeight',6)\n    writeValues = kwargs.pop('writeValues', False)\n    titles = kwargs.pop('titles', None)\n    bottomTexts = kwargs.pop('bottomTexts',None)\n    \n    if len(kwargs) > 0:\n        warnings.warn('plotImages got some invalid arguments: ' + str([key for key in kwargs]))\n    \n        \n    #actual function\n    n = len(images)\n    rowsDiv = divmod(n,columns)\n    rows = rowsDiv[0]\n    if rowsDiv[1] > 0: rows+=1\n    \n    fig, ax = plt.subplots(ncols=columns, nrows=rows,squeeze=False, figsize=(16,rowHeight * rows))\n    \n    hspace=0\n    counter = Incrementer2D(columns)\n    for i in range(n):\n        image = images[i]\n        if image.max() > 1:\n            image = image\/255.\n        ax[counter.i,counter.j].imshow(image,cmap=cm.Greys_r if len(image.shape) == 2 else None)\n        ax[counter.i,counter.j].get_xaxis().set_ticks([])\n        ax[counter.i,counter.j].get_yaxis().set_ticks([])\n        \n        if writeValues:\n            for (y,x),label in np.ndenumerate(image):\n                txt = str(label)[:5] if len(str(label)) >= 5 else str(label)\n                ax[counter.i,counter.j].text(x,y,txt,ha='center',va='center',color='blue')\n        \n        if not titles is None:\n            ax[counter.i,counter.j].set_title(titles[i])\n            hspace+=.06\n        \n        if not bottomTexts is None:\n            ax[counter.i,counter.j].annotate(bottomTexts[i], (0,0), (0, -5), xycoords='axes fraction', textcoords='offset points', va='top')   \n            hspace+=.06\n        \n        counter.increment()\n            \n    \n    for i in range(n,columns*rows):\n        ax[counter.i,counter.j].axis('off')\n        counter.increment()\n        \n    if hspace > 0: plt.subplots_adjust(hspace=hspace\/(rowHeight*rows))\n    plt.show()\n    \n#helper for the plotting function\nclass Incrementer2D:\n    def __init__(self, limitForJ, startI = 0, startJ = 0):\n        self.limitForJ = limitForJ\n        self.i = startI\n        self.j = startJ\n        \n    def increment(self):\n        self.j += 1\n        if self.j == self.limitForJ:\n            self.j = 0\n            self.i += 1\n            \nprint(\"A hidden image plotting function is also defined here\")","e7d1545c":"def gramMatrix(imageTensor):\n    #shape and pixels\n    shp = list(imageTensor.size()) #shape\n    pixels = shp[2] * shp[3]\n    \n    #flattening the spatial dimensions, keeping channels and batch\n    original = imageTensor.view(shp[0], shp[1], pixels)\n    \n    #normalization of the final result - to keep the same order at the end\n    original = torch.sqrt(original \/ pixels)\n    \n    #gram matrix\n    transposed = original.permute(0,2,1)    \n    return torch.matmul(original, transposed)\n\n#loss to be used with gram matrices\ndef gramLoss(yPred, yTrue):\n#     #yPred = features extracted from trainable input\n#     #yTrue = features extracted from the style image\n    \n    #the loss from gram matrices:\n    trueGram = gramMatrix(yTrue)\n    predGram = gramMatrix(yPred)\n    \n    return L1Loss()(predGram,trueGram)\n","75c49879":"class VGGFeatureExtractor(Module):\n    def __init__(self, vggModel, layerIndices):\n        super(VGGFeatureExtractor,self).__init__()\n        \n        #get the sequential convolutional part of the VGG\n        #each model is built differently, so you'd need different approaches\n        self.features = vggModel.features\n        \n        #desired layers\n        self.layerIndices = layerIndices\n        self.outputCount = len(layerIndices)\n        \n        #making this model's parameters untrainble\n        for p in self.parameters():\n            p.requires_grad = False\n        \n    #outputs from the selected layers\n    def forward(self,x):\n        outputs = list()\n        \n        #for each layer in the VGG\n        for i, layer in enumerate(self.features.children()):\n            x = layer(x) #apply the layer\n            \n            #if this layer is a desired layer, store its outputs\n            if i in self.layerIndices:\n                outputs.append(x)\n                \n                #check if we got all desired layers\n                if i == self.layerIndices[-1]:\n                    return outputs\n                    \n        return outputs","e5b781c4":"class StyleTransferModel(object):\n    \n    def __init__(self,imageShape,inputActivation,baseFeatureExtractor,styleFeatureExtractor,\n                 baseWeights = 1,styleWeights = 1):\n        super(StyleTransferModel, self).__init__()\n        \n        #shape\n        self.imageShape = imageShape\n        self.inputActivation = inputActivation \n        self.batchShape = (1,) + imageShape\n        \n        #feature extraction models\n        self.baseModel = baseFeatureExtractor\n        self.styleModel = styleFeatureExtractor\n        \n        #output feature counts\n        self.baseOutputCount = baseFeatureExtractor.outputCount\n        self.styleOutputCount = styleFeatureExtractor.outputCount\n        \n        #weights applied to each selected layer\n        self.baseWeights = [baseWeights]* self.baseOutputCount\n        self.styleWeights = [styleWeights]* self.styleOutputCount\n        self.allWeights = self.baseWeights + self.styleWeights\n        \n\n        #preparing loss lists - one loss per output feature \n        self.baseLosses = [L1Loss()]*self.baseOutputCount\n        self.styleLosses = [gramLoss]*self.styleOutputCount\n        self.allLosses = self.baseLosses + self.styleLosses\n        \n        #trainable image input\n        imageInputTensor = torch.rand(self.batchShape) #random noise image\n        self.imageInput = Variable(imageInputTensor, requires_grad=True) #trainable input\n\n    #get features from the trainable input\n    def forward_preds(self):\n        \n        activatedImage = self.inputActivation(self.imageInput)\n        \n        #trainable predictions:\n        basePredictions = []\n        stylePredictions = []\n        if self.baseOutputCount > 0:\n            basePredictions = self.baseModel(activatedImage)\n        if self.styleOutputCount > 0:\n            stylePredictions = self.styleModel(activatedImage)\n\n        \n        outputs = basePredictions + stylePredictions\n        return outputs\n        \n\n    #calculates losses between trainable input and features from true images\n    def forward_loss(self, trainableFeatures, trueFeatures):\n        \n        #getting losses for each of the selected outputs\n        resultLosses = []\n        for trainable, true, w, lossFunction in zip(trainableFeatures, trueFeatures, \n                                                    self.allWeights, self.allLosses):\n            loss = w*lossFunction(trainable,true)\n            resultLosses.append(loss)\n\n        #summing all losses\n        finalLoss = resultLosses[0]\n        for l in resultLosses[1:]:\n            finalLoss = finalLoss + l\n        return finalLoss\n        \n    #gets the trainable input image\n    def getInputImage(self):\n        return plotFormat(self.imageInput.data.cpu().numpy())\n    \n    #set the trainable input image\n    def setInputImage(self,image):\n        image = torchFormat(image)\n        self.imageInput.data = torch.Tensor(image)\n    \n\n    def fit(self,baseImage,styleImage, epochs, optimizer, lr, patience = 30, bestLoss = sys.float_info.max, verbose=1):\n        \n        #activated images\n        baseImage = self.inputActivation(torchFormat(baseImage))\n        styleImage = self.inputActivation(torchFormat(styleImage))\n        \n        #static values for base and style image features extracted from vgg\n        baseImageFeatures = self.baseModel.forward(baseImage) \n        styleImageFeatures = self.styleModel.forward(styleImage)\n        trueFeatures = baseImageFeatures + styleImageFeatures        \n        \n        #the optimizer will train only the input image, nothing else\n        optimizer = optimizer([self.imageInput], lr=lr)\n        \n        if verbose > 0:\n            print('called fit, inital img, lr:', lr)\n            self.plot()\n        \n        #train\n        patienceCounter = 0\n        for e in range(epochs):\n            #standard training\n            optimizer.zero_grad()\n            loss = self.forward_loss(self.forward_preds(), trueFeatures)\n            loss.backward()\n            optimizer.step()\n            \n            #since we want our image between 0 and 1, let's clam it\n            self.imageInput.data.clamp_(min=0,max=1)\n            \n            #store best loss and checks if we should interrupt training early\n            currLoss = loss.data.item()\n            if currLoss < bestLoss:\n                bestLoss  = currLoss\n                patienceCounter = 0\n            else:\n                patienceCounter += 1\n                if patienceCounter > patience:\n                    break\n            \n            if verbose > 0:\n                if e % 200 == 0:\n                    print('loss:', loss)\n                    self.plot()\n                    \n    def fitManyLRs(self, baseImage, styleImage, epochList, lrList):\n        for epochs, lr in zip(epochList, lrList):\n            self.fit(baseImage, styleImg, epochs=epochs, \n                     optimizer=torch.optim.Adam, lr=lr, verbose=0)\n    \n    \n    #plots the trainable image\n    def plot(self):\n        image = self.getInputImage()\n        print(image.min(),image.max())\n        plotImages(1,image)\n    ","418e37b6":"imgSize = 500\nresizeSize = (imgSize, imgSize)\ninShape = (3,imgSize,imgSize)\nvgg = vgg16(pretrained=True)\n\n#for base layers, we're taking a few avoiding too many at the beginning:\nbaseExtractorForCat = VGGFeatureExtractor(vgg, [3,11,13,15,18,20,22,27,29])\nbaseExtractorForCity = VGGFeatureExtractor(vgg,[3,6,8,13,15,18,20,22,27,29])\n\n#for style layers, let's take all of them (you can try different combinations)\n#styleExtractor = VGGFeatureExtractor(vgg, list(range(1,31)))\nstyleExtractor = VGGFeatureExtractor(vgg, [3,6,8,11,13,15,18,20,22,25,27,29])\n\n#style transfer model, the styleWeights = 50 seems to give good results\nstyleTransferModel = StyleTransferModel(inShape,caffeActivation, \n                                        baseExtractorForCat,styleExtractor,\n                                        baseWeights = 1,styleWeights = 20)#13000000)\n","50865624":"#cat face as base\nbaseImage = openImage('..\/input\/CatFace.jpg', imgSize)\nprint(\"base image:\")\nplotImages(1, baseImage)\n\n#lots of styles to test\nstyleFiles = ['StarryNight.jpg', 'TwistedTreePainting.jpg', 'GoldenBalls2.jpg', \n              'RandomPainting.jpg', 'ThunderBlueDense.jpg', 'FieldPainting.jpeg', 'OGrito.jpg']\nstyleImages = [openImage('..\/input\/' + img, imgSize) for img in styleFiles]\n\n#for each style image, train:\nfor file, styleImg, in zip(styleFiles, styleImages):\n    print('training for ' + file)\n    \n    #setting base image\n    styleTransferModel.setInputImage(baseImage)\n    \n    #training with different learning rates\n    styleTransferModel.fitManyLRs(baseImage,styleImg,epochList=[100, 100], lrList=[0.1, 0.01])\n\n    \n    #results\n    plotImages(2, styleImg, styleTransferModel.getInputImage())\n    \n    ","49535643":"#a city as base\nbaseImage = openImage('..\/input\/City2.jpg', imgSize)\nprint(\"base image:\")\nplotImages(1, baseImage)\n\n\n#lots of styles to test\nstyleFiles = ['StarryNight.jpg', 'TwistedTreePainting.jpg', 'BrusselsPainting.jpeg',\n              'RandomPainting.jpg', 'ThunderBlueDense.jpg', 'FieldPainting.jpeg', 'OGrito.jpg']\nstyleImages = [openImage('..\/input\/' + img, imgSize) for img in styleFiles]\nstyleWeights = [10,20,35]#[3200000,6000000,20000000]\n\n\n#for each style image, train:\nfor file, styleImg, in zip(styleFiles, styleImages):\n    print('training for ' + file)\n    \n    #for each style weight\n    for w in styleWeights:\n        print(\"style weight = \" + str(w))\n        \n        #new model for this weight\n        styleTransferModel = StyleTransferModel(inShape,caffeActivation, \n                                    baseExtractorForCity,styleExtractor,\n                                    baseWeights = 1,styleWeights = w)\n        \n        #fitting from random image\n        styleTransferModel.fitManyLRs(baseImage,styleImg,epochList=[100, 100], lrList=[0.1, 0.01])\n        imageFromRandom = styleTransferModel.getInputImage()\n    \n        #fitting from base image\n        #setting base image\n        styleTransferModel.setInputImage(baseImage)\n        styleTransferModel.fitManyLRs(baseImage,styleImg,epochList=[100, 100], lrList=[0.1, 0.01])\n        imageFromBase = styleTransferModel.getInputImage()\n\n        #results\n        print('style image \/ random initialization \/ base initialization')\n        plotImages(3, styleImg, imageFromRandom, imageFromBase)\n    ","30c767b2":"### The Gram Matrix and the Gram Loss\n\nHere we implement the gram matrix and make some normalization to avoid big numbers (we want loss values comparable with the base losses without gram matrices).\n\n","85591024":"### Getting features from the VGG model\n\nLet's create an alternative VGG model from the pretrained one so we can get outputs for many different layers as we wish.    \n**Important:** we don't want the VGG model to be trainable, we want it to keep it's intelligence as original. ","5e2191d6":"### Building the Style Transfer model\n\nThis model will be responsible for passing the input image through the feature extraction models and create the losses to be minimized.\n\nExplanation:\n\n- Has a trainable input image as variable in `self.imageInput`   \n- Has a `forward_preds`  method that passes the trainable input through the base and style feature extractor models    \n- Has a `forward_loss`\n","46cf4b16":"# Easy Style Transfer with Pytorch\n\nOne of the very interesting things deep learning can do is interpret images in a semantic way in order to abstract features from them.   \n\nStyle transfer is a technique that uses abstracted features from two or more images and combine them in an output image that carry mixed features, resulting an application that can transfer the painting style from an image to another.\n\nThe theory behing it can be found in this paper: https:\/\/www.cv-foundation.org\/openaccess\/content_cvpr_2016\/papers\/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf\n\nHere we're going to implement a quick way of making style transfer models in Pytorch\n\n## The basic theory \n\nStyle transfer is made by using pretrained networks in order to extract features from different layers of the network.   \nGiven an input image, you feed it to a network and get outputs of different layers. These outputs are features at different levels of abstraction.   \n\n![Image: Getting Abstract Features](https:\/\/pix.toile-libre.org\/upload\/img\/1545408910.jpg)\n\n\n### Reconstituting the image from abstracted features\n\nFrom these features, it's possible to reconstitute the input image by training an input noise through the same network to minimize the differences between the outputs and the features from the image:\n\n![Image: Reconstituting base images](https:\/\/pix.toile-libre.org\/upload\/img\/1545409250.jpg)\n\nBy doing this, you can reconstitute the input image quite fairly, but from an \"intelligent\" point of view, considering features extracted by powerful networks. \nThis kind of reconstitution is useful for the \"base\" image, where we want to identify actual objects **near their correct locations**.   \n\n\n### Getting \"style features\" from the abstracted features   \n\nBut, when we want to transfer the \"style\" of an image, we cannot tranfer the \"location\" of these features, otherwise we would end up with just summing two images and getting a sort of supperposition.   \nFor styles, we need to get a general relation between the abstracted features, considering their existence and relative positions, but not their exact location.   \n\nFor this, the paper comes with the solution of calculating a Gram Matrix from the extracted features. This is calculated by:\n\n- Flattening the spatial dimensions of the image, resulting in `(total_pixels, channels)`   \n- Matrix multiplying this with its transposed matrix, resulting in `(channels, channels)`\n    - Curiosity: testing the multiplication in a different order to result in `(total_pixels, total_pixels)` created tensors that couldn't fit the GPU\n    \nAnd then, the same type of reconstitution is made, now with the results of the Gram Matrices:\n\n![Image: Reconstituting styles](https:\/\/pix.toile-libre.org\/upload\/img\/1545410140.jpg)\n\n**Example of a reconstitution from the Starry Night**\n\n![Image: Style reconstitution example](http:\/\/pix.toile-libre.org\/upload\/original\/1545410647.jpg)\n\nNotice that all features are there in similar proportions, but spread randomly around. You can see the black tower segments, bright yellow balls and sky traces. The quality of the reconstitution and which features are seen depend of course on the network and layers selected to get the features from. \n\n### Transfering styles\n\nThe style transfer technique takes these two types of reconstitution and join them together for a single input image:\n\n![Image: style transfer network](http:\/\/pix.toile-libre.org\/upload\/original\/1545411495.jpg)\n\nThe balance between an image very similar to the base or very stylish is given by chosen weights in each minimization loss function. \n\nAlthough they can be the same network, the two networks in the image above are not necessarily related. Depending on which kind of intelligence you want, or which kinds of features you want to reconstitute from each image, etc., you can select specific networks and specific layers to gather the desired features. \n\nThere is a number of possibilities and different configurations you can explore. For style features, it's recommended to get a big number of layer outpus, so you can reconstitute the style in detail. But for the base image, you might not want to get outputs from initial layers as they're going to force a very exact reconstitution of the input image. \n\n**Interesting hint:** Instead of starting from a random noise, tests show that we can start from the base image. This will result in a result that is more similar to the base image, and yet full of style.","e5f0cd76":"# Making great images!\n\nNow let's finally put this to work and get a few styled images. \nLet's start loading the pretrained VGG model and defining the desired layers for feature extractions, then let's define the style transfer model:","e4a0ea4b":"## The city example\n\nHere, let's see the classic examples with cities, using:\n\n- Style weights: 10, 20 and 35\n- Trainable input initialization: random, base image\n\n","4a9f68e2":"## Nice kitties","aa1cbc47":"## Interesting conclusions from the images above\n\nSince our selected features for base layers are more advanced in the models (not the initial layers), there is a lot of additional noise. This noise is more evident in images that were randomly initialized.   \nWhen training starts from the base image, the loss regarding the base features starts at zero and increases based on the style features that include initial layers, thus these images are much less noisy. \n\nWe can also see that when the initialization is random, there is more space for style features to appear randomly positioned, like strange colors in the sky, while images initialized from the base tend to keep the sky more clear.  \n\nProducing very pretty images such as in the page needs fine tuning of the weights, and probably attributing different weights for different layers. Some layers may have a better attention to certain features than others. (The normalization made in the gram matrix loss in this Kernel may also be a source of differences in how weights are balanced between layers in the style feature extractor)   \n\n## Hints for future works\n\n- Implement some kind of denoising filter or autoencoder to be applied at the results so those squares dissapear\n- Select less style layers to get faster results: hint, every conv layer has a relu layer afterwards, take only the Conv2d outputs and discard ReLU and MaxPooling2d outputs. (Doing this might need readjusting weights)   \n- Use different weights for different layers to explore different results\n- Try different ways to normalize the gram matrix loss as the normalizatoin may change the relation between the weights for each layer.   \n- Try different image sizes, they can change the way feature extractors interpret the images    ","52b90a29":"## Implementing in Pytorch\n\nHere, we're going to implement a quick style tranfer code in Pytorch following the methods above. With the created model, we're going to show a lot of interesting examples.\n\n### Basic imports:\n\nHere, we're going to use the pretrained VGG16 model provided by Torch Vision. Our base and style networks are going to be the same model, although we can extract features from different layers for each side."}}