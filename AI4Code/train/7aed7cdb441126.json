{"cell_type":{"de6575b2":"code","995e3652":"code","0041df8d":"code","38881ae3":"code","cc3ee559":"code","d8d57dad":"code","b2c05a21":"code","60b034a2":"code","2321d8e6":"code","e3ad60d2":"code","9571c6d6":"code","8fcbc004":"code","c36fff7b":"code","adaae87e":"code","557b0629":"code","353375ed":"code","a681806f":"code","5e392628":"code","9ca80303":"code","9cb2ad0c":"code","32cf2db2":"code","e703b74f":"code","4c5810e8":"code","936d1cc4":"code","9fd1e2a0":"code","a1bb4d56":"code","f86a7e11":"markdown","de6805ed":"markdown","51738815":"markdown","385534e5":"markdown","634171ca":"markdown","818add61":"markdown","af5f1ea1":"markdown","17cd0e4d":"markdown","17b256bd":"markdown","e6f207ea":"markdown","84106322":"markdown","0c925411":"markdown","e448bb95":"markdown","09818113":"markdown"},"source":{"de6575b2":"!pip install fastai2 --quiet","995e3652":"from fastai2.basics import *\nfrom fastai2.tabular.all import *\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0041df8d":"PATH = '\/kaggle\/input\/covid19-global-forecasting-week-5\/'\ntrain_df = pd.read_csv(PATH + 'train.csv', parse_dates=['Date'])\ntest_df = pd.read_csv(PATH + 'test.csv', parse_dates=['Date'])\nexample_submit = pd.read_csv(PATH + 'submission.csv')","38881ae3":"assert test_df.shape[0] * 3 == example_submit.shape[0]","cc3ee559":"loc_key = ['Country_Region', 'Province_State', 'County']\nday_key = loc_key + ['Date']\n\ndef fill_unknown_state(df):\n    df.fillna({x : 'Unknown' for x in day_key}, inplace=True)\n    \nfor d in [train_df, test_df]:\n    fill_unknown_state(d)","d8d57dad":"def with_time_range(df, min_date):\n    return df[df['Date'] >= min_date]\n\nMIN_DATE = '2020-02-20'\n\ntrain_df = with_time_range(train_df, MIN_DATE)\ntest_df = with_time_range(test_df, MIN_DATE)","b2c05a21":"def get_target(target_name):\n    def get_cases(df):\n        res = df[df.Target == target_name]\n        return res.drop(columns=['Target']).rename({'TargetValue': target_name}, axis=1)\n    return get_cases\n\nget_cases = get_target('ConfirmedCases')\nget_fatalities = get_target('Fatalities')\n\ndef cases_fatalities_merged(df):\n    cases = get_cases(df)\n    fats = get_fatalities(df).drop(columns=['Population'])\n    \n    cases = pd.merge(cases, fats, how='left', on=day_key, suffixes=('_c', '_f'))\n    return cases\n    \n\ntrain_df = cases_fatalities_merged(train_df)\ntest_df = cases_fatalities_merged(test_df)","60b034a2":"def day_reached_cases(df, name, no_cases=1):\n    \"\"\"For each country\/province get first day of year with at least given number of cases.\"\"\"\n    gb = df[df['ConfirmedCases'] >= no_cases].groupby(loc_key)\n    return gb.Dayofyear.first().reset_index().rename(columns={'Dayofyear': name})","2321d8e6":"def additional_features(df):\n    add_datepart(df, 'Date', drop=False)\n    first_nonzero = day_reached_cases(train_df, 'FirstCaseDay', 1)\n    first_twenty = day_reached_cases(train_df, 'First20CasesDay', 20)\n    \n    df = pd.merge(df, first_nonzero, how='left')\n    df = pd.merge(df, first_twenty, how='left')\n    \n    df['DaysSinceFirst'] = df['Dayofyear'] - df['FirstCaseDay']\n    df['DaysSince20'] = df['Dayofyear'] - df['First20CasesDay']\n    return df\n","e3ad60d2":"train_df_final = additional_features(train_df)\ntest_df_final = additional_features(test_df)","9571c6d6":"cat_vars = [\n    'Country_Region', 'Province_State', 'County', \n]\n\ncont_vars = [\n    'DaysSinceFirst', 'DaysSince20', 'Dayofyear', 'Dayofweek',\n    'Population'\n]\n\nMAX_TRAIN_DATE = test_df.Date.min()\nprint(MAX_TRAIN_DATE)","8fcbc004":"import pdb\n\nquants = [.05, .5, .95]\n\nclass QuantileLossL1(nn.Module):\n    def __init__(self, quantiles):\n        super().__init__()\n        self.quantiles = quantiles\n        \n    def forward(self, preds, target):\n        assert not target.requires_grad\n        assert preds.size(0) == target.size(0)\n        losses = []\n\n        for i, q in enumerate(self.quantiles):\n            errors = target[:, 0] - preds[:, i]\n\n            losses.append(\n                torch.max(\n                   (q-1) * errors, \n                   q * errors\n            ).unsqueeze(1))\n            \n        loss = torch.mean(\n            torch.sum(torch.cat(losses, dim=1), dim=1))\n        return loss\n\n    \nclass QuantileLossL2(nn.Module):\n    def __init__(self, quantiles):\n        super().__init__()\n        self.quantiles = quantiles\n        \n    def forward(self, preds, target):\n        assert not target.requires_grad\n        assert preds.size(0) == target.size(0)\n        losses = []\n\n        for i, q in enumerate(self.quantiles):\n            errors = target[:, 0] - preds[:, i]\n            err_sq = errors**2\n            less_than_target = errors > 0\n            err_sq[less_than_target] \/= (1 - q)\n            err_sq[~less_than_target] \/= q\n\n            losses.append(err_sq.unsqueeze(1))\n\n        loss = torch.mean(\n            torch.sum(torch.cat(losses, dim=1), dim=1))\n        return loss","c36fff7b":"def pinball(preds, target):\n    assert preds.size(0) == target.size(0)\n    target_vals = target[:, 0]\n    target_weights = target[:, 1]\n    \n    losses = []\n\n    for i, q in enumerate(quants):\n        errors = (target_vals - preds[:, i]) * target_weights\n        losses.append(\n            torch.max(\n               (q-1) * errors, \n               q * errors\n            ).unsqueeze(1)\n        )\n\n    return torch.mean(\n        torch.mean(torch.cat(losses, dim=1), dim=1)\n    )\n\ndef interval(preds, target):\n    assert preds.size(0) == target.size(0)\n    target_vals = target[:, 0]\n    pred_mins = preds[:, 0]\n    pred_maxs = preds[:, 2]\n    \n    goods1 = pred_mins <= target_vals\n    goods2 = target_vals <= pred_maxs\n    \n    return torch.sum(goods1 & goods2).item() \/ target.size(0)","adaae87e":"class Predictor():\n    def __init__(self, train_df, test_df, target_colname, weight_colname,\n                 categoricals=cat_vars, continuous=cont_vars,\n                 max_train_date=MAX_TRAIN_DATE,\n                 batch_size=1024):\n        self._target_colname = target_colname\n        self._weight_colname = weight_colname\n        \n        self._cat_vars = categoricals\n        self._cont_vars = continuous\n        self._dep_var = [target_colname, weight_colname]\n        \n        self._train_df = self._train_df_processed(train_df)\n        \n        self._MAX_TRAIN_IDX = self._train_df[self._train_df['Date'] < max_train_date].shape[0]\n        self._df_wrapper = self._prepare_df_wrapper(self._train_df)\n        \n        self._path = '\/kaggle\/working\/'\n        self._dls = self._df_wrapper.dataloaders(bs=batch_size, path=self._path)\n        \n        self._dls.c = len(quants) # Number of outputs of our network is number of quantiles to be predicted.\n        \n        self._learn = tabular_learner(self._dls, layers=[1000, 500, 250],\n                        opt_func=ranger, loss_func=QuantileLossL1(quants), metrics=[interval])\n        \n        self._test_dls = self._prepare_test_dl(test_df)\n        \n    def _train_df_processed(self, train_df):\n        df = train_df[self._cont_vars + self._cat_vars + self._dep_var + ['Date']].copy().sort_values('Date')\n        df = df[df[self._target_colname] >= 0] # Filter negatives - bugs in dataset\n        df[self._target_colname] = np.log1p(df[self._target_colname])\n        return df\n    \n    def _prepare_df_wrapper(self, train_df_processed):\n        procs=[FillMissing, Categorify, Normalize]\n\n        splits = list(range(self._MAX_TRAIN_IDX)), (list(range(self._MAX_TRAIN_IDX, len(train_df_processed))))\n\n        to = TabularPandas(train_df_processed, procs,self._cat_vars.copy(), self._cont_vars.copy(), self._dep_var,y_block=TransformBlock(), splits=splits)\n        return to\n    \n    def _prepare_test_dl(self, test_df_raw):\n        to_tst = self._df_wrapper.new(test_df_raw)\n        to_tst.process()\n        return self._dls.valid.new(to_tst)\n        \n        \n    @property\n    def learn(self):\n        return self._learn\n    \n    def predict(self) -> np.ndarray:\n        tst_preds,_ = self._learn.get_preds(dl=self._test_dls)\n        tst_preds = tst_preds.data.numpy()\n        return np.expm1(tst_preds)\n    \n    def lc(self):\n        emb_szs = get_emb_sz(self._df_wrapper); print(emb_szs)\n        self._dls.show_batch()\n        self._test_dls.show_batch()","557b0629":"train_df_final","353375ed":"blackbox_cases = Predictor(train_df_final, test_df_final, 'ConfirmedCases', 'Weight_c')\nblackbox_fats = Predictor(train_df_final, test_df_final, 'Fatalities', 'Weight_f')","a681806f":"blackbox_cases.learn.lr_find()","5e392628":"blackbox_cases.learn.fit_one_cycle(20, lr_max=0.004)","9ca80303":"blackbox_fats.learn.lr_find()","9cb2ad0c":"blackbox_fats.learn.fit_one_cycle(20, lr_max=0.002)","32cf2db2":"pred_cases = blackbox_cases.predict()\npred_fats = blackbox_fats.predict()","e703b74f":"print(pred_cases.mean(axis=0), pred_fats.mean(axis=0), pred_cases.std(axis=0), pred_fats.std(axis=0))","4c5810e8":"def prepare_submission_target(test_df, model_preds, forecast_id_col, submit_df):\n    res = submit_df.copy()\n    assert(len(test_df) == model_preds.shape[0])\n    tmp_target = res.TargetValue.copy()\n  \n    preds_flattened = model_preds.flatten()\n    \n    indices = 3 * np.repeat(test_df[forecast_id_col].to_numpy() - 1, 3)\n    indices += np.tile(np.array([0,1,2]), len(test_df))\n    \n    tmp_target.loc[indices] = preds_flattened\n    res.TargetValue = tmp_target\n    return res","936d1cc4":"def prepare_submission(cases_preds, fatality_preds):\n    submit = prepare_submission_target(test_df_final, cases_preds, 'ForecastId_c', example_submit)\n    submit = prepare_submission_target(test_df_final, fatality_preds, 'ForecastId_f', submit)\n    return submit","9fd1e2a0":"submit = prepare_submission(pred_cases, pred_fats)","a1bb4d56":"submit.to_csv('submission.csv', index=False)","f86a7e11":"# Custom metrics - pinball\n\nJust an attempt to implement weighted pinball loss - something is probably wrong here, but nevertheless it gives us some information in spite of being pretty different than this one from the leaderboard.","de6805ed":"# Add temporal features\n\nSome basic features like number of days since the first case in each country\/province with analogous feature for 20 cases may be particularly worth adding.","51738815":"# Train data filtering\n\nJust a random guess - based on the disease history and having much more accurate data from March and April, targets before February the 20th are probably a total garbage.","385534e5":"Check the original I forked","634171ca":"# Input DataFrame compression\n\nFor each entity: (Place, Day) where Place is determined by `['Country_Region', 'Province_State', 'County']` we want to have one row instead of two in the original DataFrame, preserving target indices in additional columns - that's what we do in the merging function below","818add61":"# Training\n\nAs we can see, our predictor class is pretty flexible (although probably not equally open to extensions, but that doesn't matter here). To train we simply use `learn` property of the `Predictor` objects, to get a fast.ai Learner.","af5f1ea1":"# Loss function\n\nThe L1-like loss function is actually not that bad, we can stick with it.","17cd0e4d":"# Predictor class\n\nA convenience class to make predicting 2 values (cases and fatalities) easier - it contains all steps needed to initialize a fast.ai model, from input dataframes to predictions.","17b256bd":"# Load input data\n\nNo additional metadata here - let the model learn everything from scratch from competition data","e6f207ea":"# Quick look at predicted values\n\nWe take a look at predicted cases and fatalities, just to make sure our predictions make any sense.","84106322":"# Custom loss functions\n\nOne of the most important parts of this notebook - neither fast.ai nor PyTorch have builtin losses for quantile regression, so we have to define ones on our own.\n\nShould you notice any mistake in the code in the cell below, please write a comment since I don't feel pretty familiar with quantile regression yet.\n\n`QuantileLossL1` function is a standard loss which, for a given quantile `q` depends on `q * abs(target - pred)`. However, it is well known that neural nets don't\noptimize well on linear functions, that's why I tried to create another loss function based on this one - `QuantileLossL2`. \n\nThe function works as follows: if the predicted `pred` is smaller than the actual `target`, we return `(target - pred)**2 \/ (1 - q)`. Else we return simply\n`(target - pred)**2 \/ q`. I'm not sure whether this function is actually any good, it just seemed intuitively to be something reasonable and analogous to the original\n`QuantileLossL1`. ","0c925411":"# Feature selection (both for ConfirmedCases and Fatalities)\n\nIn fast.ai we can easily select categorical and continuous variables for training.\n\nI decided not to choose any external data in baseline model. Adding numerical values from country data provided in this notebook doesn't seem to improve the validation score much.\n\n**Avoiding leakage is also very important! MAX_TRAIN_DATE is a global constant indicating minimum date of our test set, and should be used to limit rows in our train set to prevent leaky modeling.**","e448bb95":"# Getting predictions\n\nWith our `Predictor` class, getting test set prediction values as numpy arrays is extremely simple.","09818113":"# Preparing submission\n\nThe submission format is pretty weird, so it requires some numpy\/pandas code to prepare it from our predicted targets."}}