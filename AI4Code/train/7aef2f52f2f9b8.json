{"cell_type":{"992e4264":"code","f63b1b3b":"code","af226f83":"code","3bb7aa0b":"code","0654472e":"code","f95a8378":"code","883ee890":"code","02d492ba":"code","676015d4":"code","b19c1303":"code","ef61e122":"code","f97c9567":"code","25205362":"code","2cf8ffc2":"code","ed7ca268":"code","b7848579":"code","8e354126":"markdown","85166b49":"markdown"},"source":{"992e4264":"import numpy as np\nimport pandas as pd\n\ntrain = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train.csv', parse_dates=['timestamp'])\ntest = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/test.csv', parse_dates=['timestamp'])\ntrain_labels = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train_labels.csv')\n# specs = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/specs.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/sample_submission.csv')\ntrain = train[train.installation_id.isin(train_labels.installation_id)]\n\nprint(train.shape, test.shape, sep = '\\n')","f63b1b3b":"#Make Mapping\ndef Make_map_Assessment_attempts():\n    Assessment = list(train[train.type == 'Assessment']['title'].drop_duplicates())\n    Code = [4110 if x == 'Bird Measurer (Assessment)' else 4100 for x in Assessment]\n    map_Assessment_attempts = pd.DataFrame({'title': Assessment, 'event_code': Code})\n    return map_Assessment_attempts\n\nmap_Assessment_attempts = Make_map_Assessment_attempts()","af226f83":"#Make dictionary\ndef Make_Dict(i):\n    lst = list(set(np.append(train[i].unique(), test[i].unique())))\n    dct = pd.DataFrame({'Key':lst})\n    return dct\n\ndef Reset_Dict():\n    event_id = Make_Dict('event_id')\n    event_code = Make_Dict('event_code')\n    title = Make_Dict('title')\n    type_ = Make_Dict('type')\n    world =  Make_Dict('world')\n    attempt = pd.DataFrame({'Key':['Correct', 'Incorrect']})\n    return event_id, event_code, title, type_, world, attempt","3bb7aa0b":"def RemoveRecordAfterLastAssessment(df):\n#     df = train[train.installation_id.isin(['0006a69f'])] \n    df2 = df.merge(map_Assessment_attempts, on = ['title', 'event_code'])\n    df3 = df2[['installation_id', 'game_session']].drop_duplicates()\n    df4 = df.merge(df3, on = ['installation_id', 'game_session'])\n    df5 = df4.groupby(['installation_id', 'game_session']).agg({'timestamp':'min'}).reset_index()\n    df6 = df5.groupby(['installation_id']).agg({'timestamp':'max'}).reset_index().rename(columns={'timestamp':'timestamp_'})\n    df7 = df.merge(df6, on = 'installation_id')\n    df8 = df7[df7.timestamp <= df7.timestamp_].drop(columns=['timestamp_'])\n    return df8\n\ntrain = RemoveRecordAfterLastAssessment(train)\nprint(train.shape, test.shape, sep = '\\n')","0654472e":"#Prepare Labels\ndef PrepareLabel():\n    df = train.groupby(['installation_id']).agg({'timestamp':'max'}).reset_index()\n    df1 = df.merge(train, on = ['installation_id', 'timestamp'])[['installation_id', 'game_session']]\n    df2 = df1.merge(train_labels, on = ['installation_id', 'game_session'])[['installation_id', 'accuracy_group']]\n    df2 = df2.rename(columns=({'accuracy_group':'accuracy_group_for_train'}))\n    return df2","f95a8378":"#Add column for attempt\ndef AddColumnForAttempt(df):\n    df1 = df[(df['title'] == 'Bird Measurer (Assessment)') & (df['event_code'] == 4110) |\\\n    (df['title'] != 'Bird Measurer (Assessment)') & (df['event_code'] == 4100) & (df['type'] == 'Assessment')]\n    df2 = df1['event_data'].str.contains('\"correct\":true')\n    df2 = df2.rename('attempt')\n    df2 = df2.apply(lambda x: 'Correct' if x == True else 'Incorrect')\n    df3 = df.merge(df2, left_index = True, right_index= True, how = 'left')\n    return df3\n\ntrain = AddColumnForAttempt(train)\ntest = AddColumnForAttempt(test)\nprint(train.shape, test.shape, sep = '\\n')","883ee890":"def Count(df, item, item_df):\n    df1 = df.groupby(['installation_id', item]).agg({item:'count'}).rename(columns={item:'count'}).reset_index().fillna(0)\n    df1 = item_df.merge(df1, left_on = 'Key', right_on = item, how = 'left').drop(columns=[item]).fillna(0)\n    df1 = pd.pivot_table(df1, values = 'count', columns=['Key'], index = ['installation_id']).reset_index().fillna(0)\n    return df1\n\ndef CountByGameSession(df, item, item_df):\n    df1 = df.groupby(['installation_id', 'game_session', item]).agg({item:'count'}).rename(columns={item:'count'}).reset_index().fillna(0)\n    df1['count'] = 1\n    df2 = df1.groupby(['installation_id', item]).agg({'count':'sum'}).reset_index().fillna(0)\n    df3 = item_df.merge(df2, left_on = 'Key', right_on = item, how = 'left').drop(columns=[item]).fillna(0)\n    df4 = pd.pivot_table(df3, values = 'count', columns=['Key'], index = ['installation_id']).reset_index().fillna(0)\n    return df4\n\ndef CountLastAttemptbygame_session(df):\n    df2 = df.groupby(['installation_id', 'game_session', 'attempt', 'timestamp']).agg({'timestamp':'count'}).rename(columns={'timestamp':'count'}).reset_index()\n    df3 = df2.groupby(['installation_id', 'game_session']).agg({'timestamp':'max'}).reset_index().drop(columns=['game_session'] )\n    df4 = df3.merge(df2, on = ['installation_id', 'timestamp'])[['installation_id', 'attempt', 'count']]\n    df5 = df4.groupby(['installation_id','attempt']).agg({'count':'sum'}).reset_index()\n    df6 = pd.pivot_table(df5, values = 'count', columns=['attempt'], index = ['installation_id']).reset_index().fillna(0)\n    return df6\n\ndef GetMaxMeanGameTime(df):\n    df1 = df.groupby(['installation_id']).agg({'game_time':['max', 'mean']}).reset_index()\n    df1.columns = [''.join(col).strip() for col in df1.columns.values]\n    return df1\n\ndef GetMeanGameTime_byGameSession(df):\n    df1 = df.groupby(['installation_id', 'game_session']).agg({'game_time':'max'}).reset_index()\n    df2 = df1[df1.game_time > 0]\n    df3 = df2.groupby(['installation_id']).agg({'game_time':'mean'}).reset_index()\n    return df3\n\ndef GetAccuracyGroupcount(df):\n\n    AccuracyGroup_dict = pd.DataFrame({'AccuracyGroup':[0, 1, 2, 3]})\n\n    # df = train[train.installation_id.isin(['0006a69f', 'ffeb0b1b'])] \n    df1 = df.groupby(['installation_id', 'game_session', 'attempt']).agg({'attempt':'count'}).rename(columns={'attempt':'count'}).reset_index()\n    df2 = pd.pivot_table(df1, values = 'count', columns=['attempt'], index = ['installation_id', 'game_session']).reset_index().fillna(0)\n    def AccuracyGroup(row):\n        if (row['Correct'] == 1) & (row['Incorrect'] ==0):\n            return 3\n        elif (row['Correct'] == 1) & (row['Incorrect'] ==1):\n            return 2\n        elif (row['Correct'] == 1) & (row['Incorrect'] >1):\n            return 1\n        else:\n            return 0\n    df2['AccuracyGroup'] = df2.apply(lambda row: AccuracyGroup(row), axis = 1)\n    df3 = df2.groupby(['installation_id', 'AccuracyGroup']).agg({'AccuracyGroup':'count'}).rename(columns={'AccuracyGroup':'count'}).reset_index()\n    df4 = AccuracyGroup_dict.merge(df3, on = 'AccuracyGroup', how = 'left')\n    df5 = pd.pivot_table(df4, values = 'count', columns=['AccuracyGroup'], index = ['installation_id']).reset_index().fillna(0)\n    return df5\n\ndef GetDayCount(df):\n    # df = train[train.installation_id.isin(['0006a69f', 'ffeb0b1b'])] \n    df1 = df.copy()\n    df1['timestamp'] = df1['timestamp'].dt.date\n    df2 = df1.groupby(['installation_id', 'timestamp']).agg({'timestamp':'nunique'}).rename(columns={'timestamp':'nunique'}).reset_index()\n    df3 = df2.groupby(['installation_id']).agg({'nunique': 'sum'}).reset_index()\n    return df3\n\ndef GetDayDiff(df):\n#     df = train[train.installation_id.isin(['0006a69f', 'ffeb0b1b'])] \n    df1 = df.copy()\n    df1['timestamp'] = df1['timestamp'].dt.date\n    df2 = df1.groupby(['installation_id', 'timestamp']).agg({'timestamp':'nunique'}).rename(columns={'timestamp':'nunique'}).reset_index()\n    df3 = df2.groupby(['installation_id']).agg({'timestamp':['min', 'max']}).reset_index()\n    df3.columns = [''.join(col).strip() for col in df3.columns.values]\n    df3['DateDiff'] = (df3.timestampmax - df3.timestampmin).dt.days\n    df4 = df3[['installation_id', 'DateDiff']]\n    return df4\n\ndef TimeSpendingBy(df, item, item_df):\n#     df = train[train.installation_id.isin(['0006a69f', 'ffeb0b1b'])] \n    df2 = df.groupby(['installation_id', 'game_session', item]).agg({'game_time': 'max'}).reset_index().fillna(0)\n    df3 = df2.groupby(['installation_id', item]).agg({'game_time':'mean'}).reset_index().fillna(0)\n    df4 = item_df.merge(df3, left_on = 'Key', right_on = item, how = 'left').drop(columns=[item]).fillna(0)\n    df5 = pd.pivot_table(df4, values = 'game_time', columns=['Key'], index = ['installation_id']).reset_index().fillna(0)\n    return df5","02d492ba":"def PrepareResultSet(df):\n    event_id, event_code, title, type_, world, attempt = Reset_Dict()\n    event_id_count = Count(df, 'event_id', event_id)\n    event_code_count = Count(df, 'event_code', event_code)\n    title_count = Count(df, 'title', title)\n    type_count = Count(df, 'type', type_)\n    world_count = Count(df, 'world', world)\n    attempt_count = Count(df, 'attempt', attempt)    \n    attempt_count['Attempt'] = attempt_count['Correct'] + attempt_count['Incorrect']\n    attempt_count['CorrectRate'] = attempt_count['Correct']\/attempt_count['Attempt']\n    event_id_count_ = CountByGameSession(df, 'event_id', event_id)\n    event_code_count_ = CountByGameSession(df, 'event_code', event_code)\n    title_count_ = CountByGameSession(df, 'title', title)\n    type_count_ = CountByGameSession(df, 'type', type_)\n    world_count_ = CountByGameSession(df, 'world', world)\n    attempt_count_ = CountLastAttemptbygame_session(df)\n    attempt_count_['Attempt'] = attempt_count_['Correct'] + attempt_count_['Incorrect']\n    attempt_count_['CorrectRate'] = attempt_count_['Correct']\/attempt_count_['Attempt']\n    game_time = GetMaxMeanGameTime(df)\n    game_time_ = GetMeanGameTime_byGameSession(df)\n    AccuracyGroupcount = GetAccuracyGroupcount(df)\n    DayCount = GetDayCount(df)\n    DayDiff = GetDayDiff(df)\n    event_id_time = TimeSpendingBy(df, 'event_id', event_id)\n    event_code_time = TimeSpendingBy(df, 'event_code', event_code)\n    title_time = TimeSpendingBy(df, 'title', title)\n    type_time = TimeSpendingBy(df, 'type', type_)\n    world_time = TimeSpendingBy(df, 'world', world)\n    attempt_time = TimeSpendingBy(df, 'attempt', attempt)    \n    \n    return event_id_count,event_code_count,title_count,\\\n           type_count,world_count,attempt_count,event_id_count_,event_code_count_,\\\n           title_count_,type_count_,world_count_,attempt_count_,game_time,game_time_,\\\n           AccuracyGroupcount,DayCount,DayDiff,event_id_time,event_code_time,title_time,\\\n           type_time,world_time,attempt_time","676015d4":"def MakeReusltSet(df, IsTrain):\n    event_id_count,event_code_count,title_count,\\\n    type_count,world_count,attempt_count,event_id_count_,event_code_count_,\\\n    title_count_,type_count_,world_count_,attempt_count_,game_time,game_time_,\\\n    AccuracyGroupcount,DayCount,DayDiff,event_id_time,event_code_time,title_time,\\\n    type_time,world_time,attempt_time = PrepareResultSet(df)\n    result_set = pd.DataFrame({'installation_id':df.installation_id.drop_duplicates()})\n    result_set2 = result_set\\\n    .merge(event_id_count, on = 'installation_id', how = 'left')\\\n    .merge(event_code_count, on = 'installation_id', how = 'left')\\\n    .merge(title_count, on = 'installation_id', how = 'left')\\\n    .merge(type_count, on = 'installation_id', how = 'left')\\\n    .merge(world_count, on = 'installation_id', how = 'left')\\\n    .merge(attempt_count, on = 'installation_id', how = 'left')\\\n    .merge(event_id_count_, on = 'installation_id', how = 'left')\\\n    .merge(event_code_count_, on = 'installation_id', how = 'left')\\\n    .merge(title_count_, on = 'installation_id', how = 'left')\\\n    .merge(type_count_, on = 'installation_id', how = 'left')\\\n    .merge(world_count_, on = 'installation_id', how = 'left')\\\n    .merge(attempt_count_, on = 'installation_id', how = 'left')\\\n    .merge(game_time, on = 'installation_id', how = 'left')\\\n    .merge(game_time_, on = 'installation_id', how = 'left')\\\n    .merge(AccuracyGroupcount, on = 'installation_id', how = 'left')\\\n    .merge(DayCount, on = 'installation_id', how = 'left')\\\n    .merge(DayDiff, on = 'installation_id', how = 'left')\\\n    .merge(event_id_time, on = 'installation_id', how = 'left')\\\n    .merge(event_code_time, on = 'installation_id', how = 'left')\\\n    .merge(title_time, on = 'installation_id', how = 'left')\\\n    .merge(type_time, on = 'installation_id', how = 'left')\\\n    .merge(world_time, on = 'installation_id', how = 'left')\\\n    .merge(attempt_time, on = 'installation_id', how = 'left')\\\n    .fillna(0)\n    if IsTrain==True:\n        Labels = PrepareLabel()\n        result_set2 = result_set2.merge(Labels, on = 'installation_id', how = 'left')    \n    return result_set2","b19c1303":"def stardardize(df):\n    result = df.copy()\n    for feature_name in df.columns:\n        mean_value = df[feature_name].mean()\n        std_value = (df[feature_name].var())**(1\/2)\n        result[feature_name] = (df[feature_name] - mean_value) \/ (std_value)\n    return result.fillna(0)\n\ndef normalize(df):\n    result = df.copy()\n    for feature_name in df.columns:\n        max_value = df[feature_name].max()\n        min_value = df[feature_name].min()\n        result[feature_name] = (df[feature_name] - min_value) \/ (max_value - min_value)\n    return result.fillna(0)\n\ndef stardardizeV2(df, df2):\n    result = df.copy()\n    result2 = df2.copy()\n    result3 = pd.concat([df, df2])\n    for feature_name in df.columns:\n        mean_value = result3[feature_name].mean()\n        std_value = (result3[feature_name].var())**(1\/2)\n        result[feature_name] = (df[feature_name] - mean_value) \/ (std_value)\n    return result.fillna(0)","ef61e122":"ReusltSettrain = MakeReusltSet(train, IsTrain=True)\nReusltSettest = MakeReusltSet(test, IsTrain=False)","f97c9567":"x_train = ReusltSettrain[ReusltSettrain.columns[1:-1]]\nfeatures_names = np.array(x_train.columns.tolist())\nx_pre = ReusltSettest[ReusltSettest.columns[1:]]\n\ny_train = ReusltSettrain['accuracy_group_for_train']\ny_train_onehot = np.zeros((y_train.size, y_train.max() + 1))\ny_train_onehot[np.arange(y_train.size), y_train] = 1\n\n\nx_train_std = stardardize(x_train)\nx_train_nor = normalize(x_train)\nx_train_std_nor = normalize(stardardize(x_train))\n\nx_pre_std = stardardize(x_pre)\nx_pre_nor = normalize(x_pre)\nx_pre_std_nor = normalize(stardardize(x_pre))                       \n\ny_pre = sample_submission","25205362":"import tensorflow as tf\ndef kappa_loss(y_pred, y_true, y_pow=2, eps=1e-10, N=4, bsize=256, name='kappa'):\n\n    with tf.name_scope(name):\n        y_true = tf.cast(y_true, tf.float64)\n        repeat_op = tf.cast(tf.tile(tf.reshape(tf.range(0, N), [N, 1]), [1, N]), tf.float64)\n        repeat_op_sq = tf.square((repeat_op - tf.transpose(repeat_op)))\n        weights = repeat_op_sq \/ tf.cast((N - 1) ** 2, tf.float64)\n    \n        pred_ = y_pred ** y_pow\n        try:\n            pred_norm = pred_ \/ (eps + tf.reshape(tf.reduce_sum(pred_, 1), [-1, 1]))\n        except Exception:\n            pred_norm = pred_ \/ (eps + tf.reshape(tf.reduce_sum(pred_, 1), [bsize, 1]))\n    \n        hist_rater_a = tf.reduce_sum(pred_norm, 0)\n        hist_rater_b = tf.reduce_sum(y_true, 0)\n        \n  \n        \n        conf_mat = tf.linalg.matmul(tf.transpose(pred_norm), y_true)\n    \n        nom = tf.reduce_sum(weights * conf_mat)\n        denom = tf.reduce_sum(weights * tf.matmul(\n            tf.reshape(hist_rater_a, [N, 1]), tf.reshape(hist_rater_b, [1, N])) \/\n                              tf.cast(bsize, tf.float64))\n    \n        return nom \/ (denom + eps)\n\n","2cf8ffc2":"\n\nimport tensorflow as tf\nfrom keras import regularizers\nfrom sklearn.model_selection import train_test_split\n\n# # activation_ = ['sigmoid', 'tanh', 'elu', 'hard_sigmoid', 'linear', 'relu', 'selu', 'softplus', 'softsign']\ninput_shape = x_train.shape[1]\n# X_train, X_test, y_train_, y_test = train_test_split(x_train_std, y_train_onehot, test_size=0.3, shuffle=True)\nX_train, y_train_ = x_train_std, y_train_onehot\n\nSIZE = X_train.shape[0]\n#2, 13, 139\nbatch = int(X_train.shape[0]\/139)\n\ntf.keras.backend.clear_session()\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(5000, input_shape=(input_shape,) , activation='elu'))\n#                                 ,kernel_regularizer=regularizers.l2(0.001)\n#                                 ,activity_regularizer=regularizers.l1(0.001)))\n# model.add(tf.keras.layers.Dropout(0.1))\nmodel.add(tf.keras.layers.Dense(70, activation='elu'))\nmodel.add(tf.keras.layers.Dense(8, activation='elu'))\nmodel.add(tf.keras.layers.Dense(4, activation = 'softmax'))\n#                                 ,kernel_regularizer=regularizers.l2(0.001)\n#                                 ,activity_regularizer=regularizers.l1(0.001)))\nopt = tf.keras.optimizers.Adamax()\n\nm = tf.keras.metrics.Accuracy()\nm2 = tf.keras.metrics.AUC(num_thresholds=3)\nm3 = tf.keras.metrics.CategoricalCrossentropy()\n\n# Here I create the custom learning loop for the model, so that I can use the custom kappa_loss function for loss\nfor i in range(100):\n    for j in range(0, SIZE, batch):\n        x = X_train[j+1:j+1+batch].to_numpy()\n        y = y_train_[j+1:j+1+batch]\n        with tf.GradientTape() as tape:\n            y_pre = model(x)\n            y_true = y\n            loss = kappa_loss(y_true, y_pre, bsize=batch)\n        grads = tape.gradient(loss, model.trainable_variables)\n        processed_grads = [g for g in grads]\n        grads_and_vars = zip(processed_grads, model.trainable_variables)\n        opt.apply_gradients(grads_and_vars)\n    print ('Model: ', i, ' : ', np.round(loss.numpy(), 2), end = ' ')\n    m.update_state(y_true, y_pre)\n    m2.update_state(y_true, y_pre)\n    m3.update_state(y_true, y_pre)\n    print('Result: ', np.round(m.result().numpy(), 2), np.round(m2.result().numpy(), 2), np.round(m3.result().numpy(), 2))\n    \nsub = model.predict(x_pre_std)\nsub = sub.argmax(axis = 1)\nsample_submission['accuracy_group'] = sub\nsample_submission.to_csv('submission.csv', index = False)\n\n","ed7ca268":"# # Neural Network\n\n# import tensorflow as tf\n# from sklearn.model_selection import train_test_split\n\n# X_train, X_test, y_train_, y_test = train_test_split(x_train_std, y_train_onehot, test_size=0.3, shuffle=True)\n# # activation_ = ['sigmoid', 'tanh', 'elu', 'hard_sigmoid', 'linear', 'relu', 'selu', 'softplus', 'softsign']\n\n# # tfa.metrics.CohenKappa\n\n# EPOCHS = 10\n# model = tf.keras.Sequential()\n# model.add(tf.keras.layers.Dense(3000, input_shape=(x_train.shape[1],) , activation='linear', kernel_initializer='glorot_uniform'))\n# model.add(tf.keras.layers.Dropout(0.01))\n# model.add(tf.keras.layers.Dense(y_train_onehot.shape[1], activation = 'softmax'))\n# model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy', 'mse', 'AUC'])\n# model.fit(x=X_train, y=y_train_, epochs=EPOCHS, verbose=1)\n# model.evaluate(x=X_test, y=y_test)\n\n# predict = model.predict(X_test)\n# predict_int = predict.argmax(axis = 1)\n# y_test = y_test.argmax(axis = 1)\n# print (qwk3(predict_int, y_test))\n\n\n# sub = model.predict(x_pre_std)\n# sub = sub.argmax(axis = 1)\n# sample_submission['accuracy_group'] = sub\n# sample_submission.to_csv('submission.csv', index = False)\n","b7848579":"y_train.hist()\nsample_submission.hist()\nprint (y_train.value_counts(normalize = True))\nprint (sample_submission.accuracy_group.value_counts(normalize=True))","8e354126":"elu Model:  99  :  0.46 Result:  0.34 0.65 7.74","85166b49":"**-------------------------------**"}}