{"cell_type":{"b30bcbee":"code","70ff39c3":"code","69d9af5d":"code","c11800e8":"code","ed3b03fd":"code","bed84883":"code","746e2c6d":"code","ff0eedba":"code","9881e46f":"code","2c433f14":"code","bc02de48":"code","64ddeae9":"code","eab762da":"code","820ae864":"code","f8e25011":"code","94e21ca0":"code","178d2d1c":"code","73d5fd88":"code","9e3b4785":"code","0a8dabf1":"code","6210387b":"code","ad5caefb":"code","c126c5c3":"code","a8c1f8ef":"code","3057ce09":"code","bce13a0b":"code","cb901eee":"code","750f339b":"code","dec51775":"code","87dadd2d":"code","7e8241cf":"code","d0f2d519":"code","e1b7462c":"code","09accab1":"code","eaea2a21":"code","0ce0bf16":"code","25975d7c":"code","582e7efb":"code","d93750ea":"code","d532addd":"code","4640413d":"code","c2a9d9c0":"code","982d7ce3":"code","514f0fbf":"code","57ec95fe":"code","301e5bf3":"code","c643906d":"code","45111c7f":"code","95df30e9":"code","06f7de4d":"code","7bb1632f":"code","21516ce2":"code","0552c124":"code","6c005ddb":"code","f4bfc4b0":"code","f3e99060":"code","ad43361f":"code","9e5a2120":"code","7a28ea2c":"code","da0620ee":"code","4c25fd11":"code","ce285b4a":"code","9ab57998":"code","fc61f284":"code","b60942ea":"code","47d1e77f":"markdown","23b5f8dc":"markdown","11ddb8da":"markdown","4c713cd9":"markdown","7e117d98":"markdown","e797c9c0":"markdown","edcfe83f":"markdown","843b867a":"markdown","5c4c9360":"markdown","5959f133":"markdown","2df026ac":"markdown","fc30edb4":"markdown","4820f4e2":"markdown","80230a01":"markdown","9c01ecd2":"markdown","60e41dfa":"markdown","51d1715e":"markdown","7dbb6214":"markdown","fbed5543":"markdown","41300598":"markdown","b3a3fdd4":"markdown","81cc9511":"markdown","bf660ea6":"markdown","7fabfab4":"markdown","5082c34a":"markdown","c7c45b6f":"markdown","01f49364":"markdown","0da46a35":"markdown","1c6fabec":"markdown","d74d8b8b":"markdown","63d7c323":"markdown","20db5126":"markdown","de90e2ac":"markdown","bc7f6f6a":"markdown","a21bba62":"markdown","24a53547":"markdown","029ece56":"markdown","5f02f0c1":"markdown","15ea4b8d":"markdown","74f3e774":"markdown","0c668e08":"markdown","f898eeaf":"markdown","a3c87a20":"markdown"},"source":{"b30bcbee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","70ff39c3":"import warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"ggplot\")\n\n#to display all rows columns \npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)  \npd.set_option('display.expand_frame_repr', False)\npd.set_option('max_colwidth', -1)","69d9af5d":"cars = pd.read_csv('\/kaggle\/input\/car-price-prediction\/CarPrice_Assignment.csv')\n","c11800e8":"# Shape of Data\ncars.shape","ed3b03fd":"# datatype of dataset\ncars.info()","bed84883":"# contents of Dataset\ncars.head()","746e2c6d":"# Statistical Overview\n\ncars.describe()","ff0eedba":"sns.countplot(cars['symboling'],palette= 'Accent')\nplt.show()","9881e46f":"#Check each aspiration attribute's count\n\nsns.countplot(cars['aspiration'], palette=\"Accent\")\nplt.show()","2c433f14":"#Check each drivewheel attribute's count\n\nsns.countplot(cars['drivewheel'], palette='Accent')\nplt.show()","bc02de48":"# converting symboling to categorical by changing its datatype to object\ncars = cars.astype({'symboling': object})","64ddeae9":"# CarName: first few entries (upto 30)\ncars['CarName'][:30]","eab762da":"# Extracting carname\n\n# Method 1: str.split() by space\n# carnames = cars['CarName'].str.split(\" \").str[0]\n\ncarnames = cars['CarName'].apply(lambda x : x.split(\" \")[0])\n\n# Print CarName: first few entries (upto 30)\n\n# carnames.head(30)\ncarnames[:30]","820ae864":"# Method 2: Use regular expressions\nimport re\n\n# regex: any alphanumeric sequence before a space, may contain a hyphen\np = re.compile(r'\\w+-?\\w+')\n\n#apply above regex pattern to CarName\ncarnames = cars['CarName'].apply(lambda x: re.findall(p,x)[0])\n\n#print carnames\nprint(carnames)","f8e25011":"# New column car_company\ncars['car_company'] = cars['CarName'].apply(lambda x: re.findall(p,x)[0])","94e21ca0":"# look at all values under car_company\ncars['car_company'].value_counts()","178d2d1c":"# replacing misspelled car_company names using loc\n\n# volkswagen\ncars.loc[(cars.car_company == 'vw'),'car_company'] = 'volkswagen'\n# porsche\ncars.loc[(cars.car_company == 'porcshce'),'car_company'] = 'porsche'\n# toyota\ncars.loc[(cars.car_company == 'toyouta'),'car_company'] = 'toyota'\n# nissan\ncars.loc[(cars.car_company == 'Nissan'),'car_company'] = 'nissan'\n# mazda\ncars.loc[(cars.car_company == 'maxda'),'car_company'] = 'mazda'","73d5fd88":"# again print all the values under car_company\ncars['car_company'].value_counts()","9e3b4785":"# drop carname variable\ncars = cars.drop('CarName', axis = 1)","0a8dabf1":"# cars basic information\ncars.info()","6210387b":"# plot wheetbase distribution\n# wheelbase: distance between centre of front and rarewheels\nsns.distplot(cars['wheelbase'], color= 'green')\nplt.show()","ad5caefb":"# plot curbweight distribution\n# curbweight: weight of car without occupants or baggage\nsns.distplot(cars['curbweight'], color = 'blue')\nplt.show()","c126c5c3":"# plot stroke dsitribution\n# stroke: volume of the engine (the distance traveled by the piston in each cycle)\nsns.distplot(cars['stroke'],color = 'purple')\nplt.show()","a8c1f8ef":"# plot compressionratio distribution\n# compressionration: ratio of volume of compression chamber at largest capacity to least capacity\nsns.distplot(cars['compressionratio'], color = 'maroon')\nplt.show()","3057ce09":"# Price distribution\n# target variable: price of car\nsns.distplot(cars['price'], color = 'green')\nplt.show()","bce13a0b":"(cars.groupby(['carbody'])['price'].mean().sort_values(ascending = False)).plot.bar(cmap='ocean')\nplt.title('Car Type vs Average Price')\nplt.xlabel(\"Model_Type\")\nplt.ylabel(\"Average Price\")\nplt.show()","cb901eee":"# Distribution of Prices over Brands\n(cars.groupby(['car_company'])['price'].mean().sort_values(ascending = False).nlargest(10)).plot.bar(cmap='rocket')\nplt.title('Brands vs Average Price')\nplt.xlabel(\"Brands\")\nplt.ylabel(\"Average Price\")\nplt.show()","750f339b":"# Distribution of Price over Fuel Type\n(cars.groupby(['fueltype'])['price'].mean().sort_values(ascending = False)).plot.bar(cmap = 'prism')\nplt.title('Fuel Type vs Average Price')\nplt.xlabel(\"Fuel_Type\")\nplt.ylabel(\"Average Price\")\nplt.show()","dec51775":"# Distribution of Price over  Car's Body Type\n\n(cars.groupby(['carbody'])['price'].mean().sort_values(ascending = False)).plot.bar(cmap='Accent')\nplt.title('Car Type vs Average Price')\nplt.xlabel(\"Body_Type\")\nplt.ylabel(\"Average Price\")\nplt.show()","87dadd2d":"plt.title('Door Number vs Price')\nsns.boxplot(x=cars.doornumber, y=cars.price,palette = 'cubehelix')\nplt.show()","7e8241cf":"plt.title('Aspiration vs Price')\nsns.boxplot(x=cars.aspiration, y=cars.price, palette='Wistia')\nplt.show()","d0f2d519":"plt.title('EngineLocation vs Price')\nsns.boxplot(x=cars.enginelocation, y=cars.price, palette='bone')\nplt.show()","e1b7462c":"plt.title('Cylinder vs Price')\nsns.boxplot(x=cars.cylindernumber, y=cars.price, palette='gist_earth')\nplt.show()","09accab1":"plt.title('FuelSystem vs Price')\nsns.boxplot(x=cars.fuelsystem, y=cars.price, palette='gnuplot')\nplt.show()","eaea2a21":"plt.title('DrivingWheel vs Price')\nsns.boxplot(x=cars.drivewheel, y=cars.price, palette='inferno')\nplt.show()","0ce0bf16":"# all numeric (float and int) variables in the dataset\ncars_numeric = cars.select_dtypes(include=['float64','int64'])\n# cars_numeric = cars.loc[:, cars.dtypes != object]\n\n#head\ncars_numeric.head()","25975d7c":"# dropping  car_ID \ncars_numeric.drop('car_ID', axis=1, inplace=True)\n\ncars_numeric.head()","582e7efb":"# paiwise scatter plot for all variables in cars_numeric\nplt.figure(figsize=(20,10))\nsns.pairplot(cars_numeric)\nplt.show()","d93750ea":"# plotting correlations on a heatmap\n\n# figure size\nplt.figure(figsize=(16,8))\n\n# heatmap\nsns.heatmap(cars_numeric.corr(), annot= True,cmap='Accent')\n\nplt.show()","d532addd":"#Define X\nX = cars.loc[:,['symboling', 'fueltype', 'aspiration', 'doornumber',\n       'carbody', 'drivewheel', 'enginelocation', 'wheelbase', 'carlength',\n       'carwidth', 'carheight', 'curbweight', 'enginetype', 'cylindernumber',\n       'enginesize', 'fuelsystem', 'boreratio', 'stroke', 'compressionratio',\n       'horsepower', 'peakrpm', 'citympg', 'highwaympg',\n       'car_company']]\n\n# Define y\ny = cars['price']","4640413d":"# subset all categorical variables\n\ncars_categorical = cars.select_dtypes(include = ['object'])\n\n# cars_categorical head\ncars_categorical.head()","c2a9d9c0":"# convert into dummies\ncars_dummies = pd.get_dummies(cars_categorical, drop_first = True)\n\n# cars_dummies head\ncars_dummies.head()","982d7ce3":"# drop categorical variables from X\nX = X.drop(list(cars_categorical.columns), axis = 1)\n\n# concat dummy variables with X\nX = pd.concat([X, cars_dummies], axis = 1)","514f0fbf":"# importing library for scaling\nfrom sklearn.preprocessing import scale\n\n\"\"\"storing column names in cols, since column names are (annoyingly) lost after \nscaling (the df is converted to a numpy array)\"\"\"\ncols = X.columns\n\n# scaling X and converting to Dtaframe\nX =  pd.DataFrame(scale(X))\n\n# #renaming X columns as cols\nX.columns = cols\n\n# #print columns in X\nX.columns","57ec95fe":"# split into train and test with train_size=70% and random_state=100\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size= 0.7, random_state = 100)","301e5bf3":"# importing Libraries\n\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import GridSearchCV","c643906d":"# list of alphas to tune\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n\n#initialising Ridge() function\nridge = Ridge()\n\n\n# defining cross validation folds as 5\nfolds = 5","45111c7f":"# Define GridSearchCV \ngrid_cv = GridSearchCV(estimator=ridge, \n                    param_grid=params,\n                    scoring='neg_mean_absolute_error', \n                    cv= folds,\n                    return_train_score = True,\n                    verbose=1)\n\n# fit GridSearchCV() with X_train and y_train\n\ngrid_cv.fit(X_train, y_train)","95df30e9":"# Save GridSearchCV results into a dataframe \ncv_results = pd.DataFrame(grid_cv.cv_results_)\n\n# filter cv_results with all param_alpha less than or equal to 200\ncv_results = cv_results.loc[(cv_results['param_alpha'] <= 200)]\n\n# cv_results head\ncv_results.head()","06f7de4d":"# change datatype of 'param_alpha' into int\ncv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\n\n# plotting\nplt.style.use('ggplot')\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\nplt.title('Negative Mean Absolute error and alpha')\nplt.legend(['train score', 'test score'], loc= 'upper left')\nplt.show()","7bb1632f":"# checking best alpha from model_cv\nprint(grid_cv.best_params_)","21516ce2":"#sel alpha as 10\nalpha = 10\n\n# Initialise Ridge() with above alpha\nridge = Ridge(alpha=alpha)\n\n#fit model\nridge.fit(X_train, y_train)\n#print ridge coeficients\nridge.coef_","0552c124":"# Initialise Lasso()\nlasso = Lasso()\n\n# cross validation and Hyperparameter tuning using lasso\n#use same attributes used for Ridge tuning except estimator here would be lasso\ngrid_cv = GridSearchCV(estimator=lasso,\n                       param_grid=params,\n                       scoring='neg_mean_absolute_error', \n                       cv= folds,\n                       return_train_score = True,\n                       verbose=1)\n#fit model_cv\ngrid_cv.fit(X_train, y_train)\n","6c005ddb":"# Save model_cv results into a dataframe\ncv_results = pd.DataFrame(grid_cv.cv_results_)\n\n# cv_results head\ncv_results.head()","f4bfc4b0":"# change param_alpha datatype to float\ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float')\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\nplt.title('Negative Mean Absolute error and alpha')\nplt.legend(['train score', 'test score'], loc= 'upper left')\nplt.show()\n","f3e99060":"# Checking best  alpha from model_cv\nprint(grid_cv.best_params_)","ad43361f":"# Set alpha =100\nalpha = 100\n\n# Define lasso with above alpha\nlasso = Lasso(alpha=alpha)\n  \n# fit lasso\nlasso.fit(X_train,y_train)\n\n# print lasso coeficients\nlasso.coef_","9e5a2120":"# Initialise ElasticNet()\nelasticnet = ElasticNet()\n\n# cross validation and Hyperparameter tuning using ElasticNet\n#use same attributes used for Ridge tuning except estimator here would be ElasticNet\ngrid_cv = GridSearchCV(estimator=elasticnet,\n                    param_grid=params,\n                    scoring='neg_mean_absolute_error', \n                    cv= folds,\n                    return_train_score = True,\n                    verbose=1)\n\n#fit model_cv\ngrid_cv.fit(X_train, y_train)","7a28ea2c":"# Save model_cv results into a dataframe\ncv_results = pd.DataFrame(grid_cv.cv_results_)\n\n# cv_results head\ncv_results.head()","da0620ee":"# change param_alpha datatype to float\ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\nplt.title('Negative Mean Absolute error and alpha')\nplt.legend(['train score', 'test score'], loc= 'upper left')\nplt.show()\n","4c25fd11":"# Checking best  alpha from model_cv\n\ngrid_cv.best_params_","ce285b4a":"# Set alpha =0.2\nalpha = 0.2\n\n# Define ElasticNet with above alpha\nelasticnet = ElasticNet(alpha=alpha)\n  \n# fit elastic net\nelasticnet.fit(X_train,y_train)\n\n# print ElasticNet coeficients\nprint(elasticnet.coef_)","9ab57998":"# import mean_squared_error module\nfrom sklearn.metrics import mean_squared_error","fc61f284":"# Calculate all 3 predictions \npred_l = lasso.predict(X_test)\npred_r = ridge.predict(X_test)\npred_en = elasticnet.predict(X_test)","b60942ea":"# print RMSE for all 3 techniques\nprint('Lasso RMSE ', np.sqrt(mean_squared_error(y_test,pred_l)))\nprint('Ridge RMSE ', np.sqrt(mean_squared_error(y_test,pred_r)))\nprint('ElasticNet RMSE ', np.sqrt(mean_squared_error(y_test,pred_en)))","47d1e77f":"#### Findings","23b5f8dc":"**Jaguar** and **Buick** brands have highest average price.\n\n**Diesel** has higher average price than gas.\n\n**Hardtop** and **Convertible** body type have higher average price.","11ddb8da":"### Importing Libraries and Reading the Dataset","4c713cd9":"Notice that the carname is what occurs before a space, e.g. alfa-romero, audi, chevrolet, dodge, bmx etc.\n\nThus, we need to simply extract the string before a space.","7e117d98":"### Scaling the features","e797c9c0":"#### plotting mean test and train scores with alpha for Lasso","edcfe83f":"## Data Visualization","843b867a":"We've seen that there are no missing values in the dataset. We've also seen that variables are in the correct format, except symboling, which should rather be a categorical variable (so that dummy variable are created for the categories).\n\nNote that it can be used in the model as a numeric variable also.","5c4c9360":"The car_company variable looks okay now. Let's now drop the car name variable.","5959f133":"### Model evaluation\n\nLets compare all three model result using error term . Here we will check RMSE.","2df026ac":"### Multivariate Analysis","fc30edb4":"The heatmap shows some useful insights:\n\nCorrelation of price with independent variables:\n\nPrice is highly (positively) correlated with wheelbase, carlength, carwidth, curbweight, enginesize, horsepower (notice how all of these variables represent the size\/weight\/engine power of the car)\n\nPrice is negatively correlated to citympg and highwaympg (-0.70 approximately). This suggest that cars having high mileage may fall in the 'economy' cars category, and are priced lower (think Maruti Alto\/Swift type of cars, which are designed to be affordable by the middle class, who value mileage more than horsepower\/size of car etc.)\n\nCorrelation among independent variables:\n\nMany independent variables are highly correlated (look at the top-left part of matrix): wheelbase, carlength, curbweight, enginesize etc. are all measures of 'size\/weight', and are positively correlated\nThus, while building the model, we'll have to pay attention to multicollinearity (especially linear models, such as linear and logistic regression, suffer more from multicollinearity).","4820f4e2":"### Ridge, Lasso and ElasticNet Regression\n\nLet's now try predicting car prices, a dataset used in simple linear regression, to perform ridge, lasso and elasticNet regression.","80230a01":"### ElasticNet Regression\n\nCross validation and Hyperparameter tuning: GridSearchCV","9c01ecd2":"### Numerical Data Visualization","60e41dfa":"Note: The training results depend on the way the train data is splitted in cross validation. Each time you run, the data is splitted randomly and hence you observe minor differences in your answer","51d1715e":"### Lasso\n\nCross validation and Hyperparameter tuning: GridSearchCV","7dbb6214":"Notice that some car-company names are misspelled - \n\nvw and vokswagen should be volkswagen, \n\nporcshce should be porsche, \n\ntoyouta should be toyota,\n\nNissan should be nissan,\n\nmaxda should be mazda etc.\n\nThis is a data quality issue, let's solve it.","fbed5543":"### Understanding the unique value distribution","41300598":"As you can see that train and test scores start to become parallel to each other after apha crosses 0.2. So lets check our Elastic model on alpha 0.2.","b3a3fdd4":"### Ridge Regression","81cc9511":"## General Investigation","bf660ea6":"## Data Cleaning","7fabfab4":"#### plotting mean test and train scores with alpha for Ridge","5082c34a":"## Data Preparation","c7c45b6f":"### Cross validation and Hyperparameter tuning: GridSearchCV\n\ninitialising GridSearchCV function with folowing attributes:","01f49364":"As you can see that trai and test scores start to become parallel to each other after apha crosses 10. So lets check our ridge model on alpha 10.","0da46a35":"### plotting mean test and train scores with alpha for ElasticNet","1c6fabec":"### Univariate Analysis","d74d8b8b":"This is quite hard to read, and we can rather plot correlations between variables. Also, a heatmap is pretty useful to visualise multiple correlations in one plot.","63d7c323":"Let's now prepare the data and build the model.\n\nsplit into X and y","20db5126":"##  Model Building and Evaluation","de90e2ac":"aspiration: An (internal combustion) engine property showing whether the oxygen intake is through standard (atmospheric pressure)or through turbocharging (pressurised oxygen intake)","bc7f6f6a":"As you can see that trai and test scores start to become parallel to each other after apha crosses 100. So lets check our Lasso model on alpha 100.","a21bba62":"1) **Doornumber** doesnot impact the price. As there is not much difference in price range between both the category 2 doors & 4 doors.\n\n2) In **Aspiration**  feature, *turbo* have higher price range than the *std*. \n    *std* category has some outliers compare to *turbo*.\n    \n3) In **Enginelocation** categories *rear* starting price range is heigher than *front* location. Most         preffered are *front* location one.\n\n4) Eight **cylinders** cars have the highest price range. Most preffered are Six, Five & Four in order.\n\n5) *mpfi* and *2bbl* are most common type of **fuel** systems. *mpfi* and *idi* having the highest price      range.\n\n6) A very significant difference in **drivewheel** category. Most high ranged cars seeme to prefer *rwd*            drivewheel.","24a53547":"Let's create a new column to store the compnay name and check whether it looks okay","029ece56":"### Findings\n\nThere are intotal 205 rows and 26 columns. Dataset has no missing value as such.Great for us.Total 10 columns are of categorical type, which means we need to check there unique values to figure out how to use them in our analysis and model building.\n\nAlso if we look closer at **symboling** features,will notice, its has categorical features in the form of integer. So, we need to change its data types, after full investigation only.","5f02f0c1":"drivewheel: frontwheel, rarewheel or four-wheel drive","15ea4b8d":"### Creating dummy variables for categorical variables","74f3e774":"#### We need to Extract Company Name from the from the column CarName","0c668e08":"**As you can see for our problem statement Ridge as a regularization technique gave us the best result. You can also check for other metrics , so that you choose the best model.**","f898eeaf":"### Splitting into test train","a3c87a20":"From above output we can see that symboling parameter in cars dataset shows -2 (least risky) to +3 most risky but most of the cars are 0,1,2."}}