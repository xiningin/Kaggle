{"cell_type":{"79c0c42e":"code","2de72819":"code","ef406435":"code","3c418eae":"code","1b239f91":"code","c3380ac9":"markdown","f4b49b07":"markdown","10da80db":"markdown","339e7f19":"markdown","db0d75da":"markdown"},"source":{"79c0c42e":"import gym\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt","2de72819":"env = gym.make('FrozenLake-v0')\n\nfrom gym.envs.registration import register\n# To make enviroment deterministic uncomment this area\n#register(\n#        id='FrozenLakeNotSlippery-v0',\n#        entry_point='gym.envs.toy_text:FrozenLakeEnv',\n#        kwargs={'map_name' : '4x4', 'is_slippery':False},\n#        max_episode_steps = 100,\n#        reward_threshold = 0.78)\n\nq_table = np.zeros([env.observation_space.n, env.action_space.n])","ef406435":"alpha = 0.1\ngamma = 0.9 \nepsilon = 0.1\n# plotting metric\nreward_list = []\ndropout_list = []","3c418eae":"episode_number = 30000\n\nfor i in range(1,episode_number):\n    \n    # init enviroment\n    state = env.reset()\n    \n    reward_count = 0\n    dropouts = 0\n    \n    while True:\n        \n        # exploit vs explore to find action epsilon 0.1 => %10 explore %90 explotit\n        if random.uniform(0,1) < epsilon:\n            action = env.action_space.sample()\n        else:\n            action = np.argmax(q_table[state])\n            \n        # action process and take reward\/ take observation\n        next_state, reward, done, _ = env.step(action)\n        \n        # q learning funt\n        \n        old_value = q_table[state, action]  #old value\n        next_max = np.max(q_table[next_state]) #next max\n        \n        next_value = (1-alpha)*old_value + alpha*(reward + gamma*next_max)\n        # q table update\n        q_table[state,action] = next_value\n        \n        # update state\n        state = next_state\n        \n        reward_count  += reward\n    \n        if done:\n            break\n        \n    if i%10 == 0:\n        dropout_list.append(dropouts)\n        reward_list.append(reward_count)\n        print(\"Episode: {}, reward {}\".format(i, reward_count))","1b239f91":"plt.plot(reward_list)","c3380ac9":"**alpha = learning rate**\n\n**gamma = discount factor**\n\n**epsilon = exploration rate**","f4b49b07":"**Importing enviroment and creating q table**","10da80db":"SFFF       (S: starting point, safe)\n\nFHFH       (F: frozen surface, safe)\n\nFFFH       (H: hole, fall to your doom)\n\nHFFG       (G: goal, where the frisbee is located)","339e7f19":"Another classical reinforcement learning example with q learning algorithm and frozenlake enviroment. You can check q learning algorithm [here.](https:\/\/towardsdatascience.com\/simple-reinforcement-learning-q-learning-fcddc4b6fe56)","db0d75da":"![1_frozen.png](attachment:1_frozen.png)"}}