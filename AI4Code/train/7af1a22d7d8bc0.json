{"cell_type":{"d18050fe":"code","903ebaf9":"code","366f9761":"code","52df951a":"code","7f8d71a9":"code","65757105":"code","d29e31a5":"code","4ccd8f4c":"code","e61f27ba":"code","5bb660af":"code","b2ae7515":"code","e253bfe6":"code","460a7995":"code","3002e3f2":"code","f28cc1ea":"code","8652638a":"code","eb49d3aa":"code","9c3a966c":"code","d48a496a":"code","512395fc":"code","bf6bd69c":"code","3cb055bc":"code","2c93c4ab":"code","58828e20":"code","5a8c2e19":"code","219dc9f5":"code","abac55f9":"code","5abb12d0":"code","249e9b03":"code","f82830f5":"code","8a30b0db":"code","d43fd197":"code","caf66689":"code","cdbd5ea3":"markdown","f5c786dd":"markdown","3f8b56fb":"markdown","59c958e6":"markdown","8e2b62d5":"markdown","9d595304":"markdown","375543a1":"markdown","9240d952":"markdown","8d9dc780":"markdown","a41fe084":"markdown","54fd30b3":"markdown"},"source":{"d18050fe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","903ebaf9":"import matplotlib.pyplot as plt\nrcParams = plt.rcParams.copy()\nimport seaborn as sns\nplt.rcParams = rcParams\n% matplotlib inline\nplt.rcParams[\"figure.dpi\"] = 100\nnp.set_printoptions(precision=3, suppress=True)\nplt.style.use(['fivethirtyeight'])","366f9761":"# Load data to dataframe\nraw_train = pd.read_csv(\"..\/input\/train.csv\")\nraw_test = pd.read_csv(\"..\/input\/test.csv\")\nraw_train.head(50)\n","52df951a":"raw_test.head(50)","7f8d71a9":"# Create training and test data\nX_train = raw_train.drop(columns=[\"ID_code\",\"target\"])\ny_train = raw_train[[\"target\"]] \nX_test = raw_test.drop(columns=[\"ID_code\"])\ny_test = raw_test[[\"ID_code\"]]","65757105":"X_train.head()","d29e31a5":"y_train.head(20)","4ccd8f4c":"# Check X training data dimensions\nX_train.shape","e61f27ba":"X_train.describe()","5bb660af":"# Check y training data distribution\nsns.countplot(x=\"target\", data=y_train)\nprint(y_train['target'].value_counts()\/y_train.shape[0])\nprint('{} samples are positive'.format(np.sum(y_train['target'] == 1)))\nprint('{} samples are negative'.format(np.sum(y_train['target'] == 0)))","b2ae7515":"df1 = pd.concat([X_train.apply(lambda x: sum(x.isnull())).rename(\"num_missing\"),\n                 X_train.apply(lambda x: sum(x==0)).rename(\"num_zero\"),\n                 X_train.apply(lambda x: len(np.unique(x))).rename(\"num_unique\")],axis=1).sort_values(by=['num_unique'])\ndf1","e253bfe6":"df2 = pd.concat([X_test.apply(lambda x: sum(x.isnull())).rename(\"num_missing\"),\n                 X_test.apply(lambda x: sum(x==0)).rename(\"num_zero\"),\n                 X_test.apply(lambda x: len(np.unique(x))).rename(\"num_unique\")],axis=1).sort_values(by=['num_unique'])\ndf2","460a7995":"# No missing value comfirmed\nnp.sum(df1['num_missing']!=0)","3002e3f2":"sns.distplot(a=X_train['var_71'],rug=True)","f28cc1ea":"sns.distplot(a=X_train['var_131'],rug=True)","8652638a":"#create a function which makes the plot:\nfrom matplotlib.ticker import FormatStrFormatter\ndef visualize_numeric(ax1, ax2, ax3, df, col, target):\n    #plot histogram:\n    df.hist(column=col,ax=ax1,bins=200)\n    ax1.set_xlabel('Histogram')\n    \n    #plot box-whiskers:\n    df.boxplot(column=col,by=target,ax=ax2)\n    ax2.set_xlabel('Transactions')\n    \n    #plot top 10 counts:\n    cnt = df[col].value_counts().sort_values(ascending=False)\n    cnt.head(10).plot(kind='barh',ax=ax3)\n    ax3.invert_yaxis()  # labels read top-to-bottom\n#     ax3.yaxis.set_major_formatter(FormatStrFormatter('%.2f')) #somehow not working \n    ax3.set_xlabel('Count')","eb49d3aa":"for col in list(df1.index[:20]):\n    fig, axes = plt.subplots(1, 3,figsize=(10,3))\n    ax11 = plt.subplot(1, 3, 1)\n    ax21 = plt.subplot(1, 3, 2)\n    ax31 = plt.subplot(1, 3, 3)\n    fig.suptitle('Feature: %s'%col,fontsize=5)\n    visualize_numeric(ax11,ax21,ax31,raw_train,col,'target')\n    plt.tight_layout()","9c3a966c":"#get vars except target:\nx_vars = X_train.columns","d48a496a":"from sklearn.pipeline import make_pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import RobustScaler\n\npca = make_pipeline(RobustScaler(), PCA(n_components=2))\ntrain_pca = pca.fit_transform(X_train[x_vars])\nplt.scatter(train_pca[:, 0], train_pca[:, 1], c=y_train['target'], alpha=.1)\nplt.xlabel(\"first principal component\")\nplt.ylabel(\"second principal component\")","512395fc":"from sklearn.pipeline import Pipeline\npca_50 = PCA(n_components=50)\n# pipe = Pipeline(steps=[('sampler', RobustScaler()),('pca', pca_50)])\n# pca_result_50 = pipe.fit_transform(X_train[x_vars])\npca_result_50 = pca_50.fit_transform(X_train[x_vars])\nprint('Cumulative explained variation for 50 principal components: {}'.format(np.sum(pca_50 .explained_variance_ratio_)))\n","bf6bd69c":"# from sklearn.manifold import TSNE\n# from matplotlib.ticker import NullFormatter\n# from time import time\n# tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n# tsne_results = tsne.fit_transform(pca_result_50)\n\n# print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n","3cb055bc":"# ax.set_title(\"Perplexity=%d\" % perplexity)\n# ax.scatter(Y[red, 0], Y[red, 1], c=\"r\")\n# ax.scatter(Y[green, 0], Y[green, 1], c=\"g\")\n# ax.xaxis.set_major_formatter(NullFormatter())\n# ax.yaxis.set_major_formatter(NullFormatter())\n# ax.axis('tight')","2c93c4ab":"# from sklearn.cluster import KMeans\n# def process_data(train_df, test_df):\n# #     logger.info('Features engineering - numeric data')\n#     idx = [c for c in train_df.columns if c not in ['ID_code', 'target']]\n#     for df in [test_df, train_df]:\n#         for feat in idx:\n#             df['r2_'+feat] = np.round(df[feat], 2)\n#             df['r2_'+feat] = np.round(df[feat], 2)\n#         df['sum'] = df[idx].sum(axis=1)  \n#         df['min'] = df[idx].min(axis=1)\n#         df['max'] = df[idx].max(axis=1)\n#         df['mean'] = df[idx].mean(axis=1)\n#         df['std'] = df[idx].std(axis=1)\n#         df['skew'] = df[idx].skew(axis=1)\n#         df['kurt'] = df[idx].kurtosis(axis=1)\n#         df['med'] = df[idx].median(axis=1)\n#         tmp=np.array(df[['var_0', 'var_2', 'var_26', 'var_76','var_81','var_139','var_191']])\n#         kms=KMeans(n_clusters=30)\n#         y=kms.fit_predict(tmp)\n#         df['category'] = y\n#         df['category'] = df['category'].astype('category')\n#     print('Train and test shape:',train_df.shape, test_df.shape)\n#     return train_df, test_df\n","58828e20":"# X_train1, X_test1 = process_data(raw_train, raw_test)\n# X_train1.head(30)\n# X_test1.head(30)","5a8c2e19":"X_train1[['var_1','r2_var_1']].iloc[0]","219dc9f5":"# #Add PCA features:\n# X_train = pd.concat([X_train, pd.DataFrame(train_pca,columns=['comp1_pca','comp2_pca'])],axis=1)\n# #Add t-SNE features:\n# # X_train = pd.concat([X_train, pd.DataFrame(tsne_results,columns=['comp1_tsne','comp2_tsne'])],axis=1)\n# #get PCA test components:\n# test_pca = pca.transform(X_test[x_vars])\n# X_test = pd.concat([X_test, pd.DataFrame(test_pca,columns=['comp1_pca','comp2_pca'])],axis=1)\n# # #get t-SNE test components:\n# # test_pca_50 = pca_50.transform(X_test[x_vars])\n# # test_tsne = tsne.transform(test_pca_50)\n# # X_test = pd.concat([X_test, pd.DataFrame(test_pca,columns=['comp1_pca','comp2_pca'])],axis=1)\n# # X_test = pd.concat([X_test, pd.DataFrame(test_tsne,columns=['comp1_tsne','comp2_tsne'])],axis=1)\n# #check shape: (4 more columns added)\n# X_train.shape, X_test.shape","abac55f9":"# plt.figure(figsize=(80,80))\n# sns.heatmap(X_train.corr(),\n#            annot=True, fmt=\".2f\")","5abb12d0":"import warnings\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.pipeline import make_pipeline\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\nfrom sklearn.preprocessing import StandardScaler","249e9b03":"# from sklearn.linear_model import LogisticRegression\n\n# warnings.filterwarnings('ignore')\n# param_grid = {'logisticregression__C': np.logspace(-1, 1, 7)}\n# pipe = make_pipeline(RandomUnderSampler(), StandardScaler(), LogisticRegression(class_weight='balanced',\n#                                                                                 random_state=0))\n# model_01 = GridSearchCV(pipe, param_grid, cv=5)\n# model_01.fit(X_train, y_train)\n\n# score_01 = np.mean(cross_val_score(model_01, X_train, y_train, scoring='roc_auc', cv=7))\n# print('Average ROC AUC Score: {:.3f}'.format(score_01))\n","f82830f5":"# y_pred_01 = model_01.predict(X_test)\n# #  print('   Test ROC AUC Score: {:.3f}'.format(roc_auc_score(y_test, y_pred)))\n# y_test['target_01'] = y_pred_01\n# y_test_01 = y_test[['ID_code','target_01']].copy()\n# y_test_01.head()","8a30b0db":"# from xgboost import XGBClassifier\n\n# model_02 = XGBClassifier(max_depth=2,\n#                          learning_rate=1,\n#                          min_child_weight = 1,\n#                          subsample = 0.5,\n#                          colsample_bytree = 0.1,\n#                          scale_pos_weight = round(sum(y_train.target == 1)\/len(y_train.target),2),\n#                          #gamma=3,\n#                          seed=0)\n# model_02.fit(X_train, y_train.values)\n\n# score_02 = np.mean(cross_val_score(model_02, X_train, y_train.values, scoring='roc_auc', cv=7))\n# print('Average ROC AUC Score: {:.3f}'.format(score_02))","d43fd197":"# y_pred_02 = model_02.predict(X_test)\n# #  print('   Test ROC AUC Score: {:.3f}'.format(roc_auc_score(y_test, y_pred)))\n\n# y_test['target'] = y_pred_02\n# y_test_02 = y_test[['ID_code','target']].copy()\n# y_test_02.head()\n# y_test_02.to_csv('..\/input\/sample_submission.csv', encoding='utf-8', index=False)","caf66689":"# from sklearn.ensemble import ExtraTreesClassifier\n\n# model_08 = make_pipeline(RandomUnderSampler(), ExtraTreesClassifier(n_estimators=150,\n#                                                                     criterion='entropy',\n#                                                                     max_depth=8,\n#                                                                     min_samples_split=300,\n#                                                                     min_samples_leaf=15,\n#                                                                     random_state=0,\n#                                                                     class_weight='balanced_subsample'))\n# model_08.fit(X_train, y_train)\n\n# score_08 = np.mean(cross_val_score(model_08, X_train, y_train, scoring='roc_auc', cv=7))\n# print('Average ROC AUC Score: {:.3f}'.format(score_08))","cdbd5ea3":"**1.2 Check missing values**  \nWe'll check nan values and zero values (may represent missing values). Also I'll add a column for number of unique values because that'll be interesting to know.","f5c786dd":"**1.4 Get PCA and t-SNE for visualization:**  \nPCA requires scaled data but we need not scale the data for both model sets. So we'll use a pipeline here. We saw earlier that data has outliers so we'll use the robust scaler. Its a safe bet because if the data is normal, it'll work similar to standard scaler.","3f8b56fb":"This looks like a highly non-linear relationship between the data and targets. Two classes are concentrated seperately.\n\nWe could try creating PCA as new features\n","59c958e6":"**1.1 Check Imbalance**   \nThe result below suggests high imbalance in training data. Sampling methods should be considered when developing models","8e2b62d5":"Let's check if zero values should be treated as Nan values. From the density and rug plots below, the number of zero value samples are closed to that of other values. Therefore, we should not replace them with any imputed values such as average or median.","9d595304":"**Step1 - Exploration and Preparation**\n\nIn this step, I will perform the following actions:\n\n* Check the imbalance in data \n* Check missing values\n* Understand the data better using plots\n* Perform basic feature engineering which will be used accross all model sets.\n* Make some hypothesis using the plots and try to make some features representing them. Note that these features might\/might not work because they are just hypothesis.","375543a1":"**Step0 - Import Libraries, Load Data**","9240d952":"I tried to first calculate 50 principle components and applied t-SNE. Since t-SNE is very computationally expensive, we abandoned adding t-SNE component as new features.","8d9dc780":"**1.3 Visualize Features:**  \nWe'll make 2 plots to visualize these features:\n\n* histogram\n* box-whiskers plot with the outcome\n* counts of top 10 most occuring unique values to see if there are any dominating ones\n","a41fe084":"Some interesting fact can be found in:   \n*var_68* :  periodic spikes can be observed. (Other kaggler said this column may be time. From the histgram, its periodicity supports this opnion as well).  \n*var_108* :  a peak can be found at 14.1999 and 14.2. Also, the top values are very close to each other, we may consider round them up to less decimals.  \n*var_12* : a peak can be found at 13.5545. it shows similar features as *var_108*  \n  \nGeneral insights:  \n* Most of the columns follow normal distribution  \n* Since there are lot of outliers, using a robustscalar might make more sense\n* polynomial features are not very intuitive here, but can be tried later\n","54fd30b3":"**Step2 - ModelSet1\nIn this step, we expect you to perform the following steps relevant to the models you choose for set1:**\n\nfeature engineering  \nvalidation  \nfeature selection  \nfinal model selection  "}}