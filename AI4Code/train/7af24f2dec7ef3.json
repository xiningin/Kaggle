{"cell_type":{"3fd3ba59":"code","349256d5":"code","f96928ee":"code","3d3b4660":"code","be1e8586":"code","9c49abf0":"code","c4f021c1":"code","b9c8adaf":"code","bd5a3f2f":"code","7335a8e8":"code","bb680321":"code","03569762":"code","2cbd2f12":"code","51cbef36":"code","5bca3291":"code","fd872809":"code","917a8a85":"code","10d41032":"code","790a6c1e":"code","31d40e23":"code","f05941c4":"code","32f63b7c":"code","e71c4d7b":"code","55e74c61":"code","81bedbec":"code","c8772cb8":"code","32834a8d":"code","90c98c85":"code","97da770b":"code","9222c894":"code","83c20a91":"code","4ac2691a":"code","2541e33f":"code","a2ed437a":"code","a2dc20fb":"code","da496052":"code","837cfb21":"code","c22f5938":"code","ea3605b0":"code","ffa79705":"code","b3da9eb9":"code","017bd5c5":"code","a5fc331b":"code","3eef1b49":"code","1bf73f22":"code","1190c862":"code","8cdb8fd6":"code","3225dd43":"code","9267ce85":"markdown","7608f57b":"markdown","48c5fad9":"markdown","f71b5cff":"markdown","e40994c1":"markdown","1f5f0590":"markdown","0cdb439d":"markdown","5b252791":"markdown","ac994bf6":"markdown"},"source":{"3fd3ba59":"from collections import Counter\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\nimport nltk\nfrom tqdm.notebook import tqdm\n\nfrom statsmodels import api as smf\nfrom scipy import stats as sts\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBRegressor\nplt.style.use('seaborn')\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","349256d5":"RANDOM_STATE = 42  # set the random state","f96928ee":"train = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntrain.head()","3d3b4660":"test = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\ntest.head()","be1e8586":"sample_submission = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\nsample_submission.head()","9c49abf0":"train.describe()","c4f021c1":"plt.figure(figsize=(12, 6))\nsns.histplot(train.target, bins=40, kde=True)\nplt.show()","b9c8adaf":"plt.figure(figsize=(12, 6))\nsns.histplot(train.standard_error, bins=40, kde=True)\nplt.show()","bd5a3f2f":"train.isna().sum(axis=0) \/ len(train)  # 70% of NaNs","7335a8e8":"plt.figure(figsize=(12, 6))\nsns.heatmap(train.isna())\nplt.show()","bb680321":"plt.figure(figsize=(12, 6))\nsns.histplot(train.excerpt.apply(len), bins=40, kde=True)\nplt.show()","03569762":"all(train.isna()['url_legal'] == train.isna()['license']) == True  # both have NaNs at the same posistions","2cbd2f12":"train['text_length'] = train['excerpt'].apply(len)  # new simple feature, text length\ntrain.head()","51cbef36":"def count_punctuation(text, which=['.', ',', r'\"', ':', ';', '!', '?', r\"'\", r'[', r']', r'{', r'}']):\n    count = 0\n    for symbol in which:\n        count += text.count(symbol)\n        \n    return count","5bca3291":"train['punct_count'] = train['excerpt'].apply(count_punctuation)  # simple feature, punctuation count\ntrain.head()","fd872809":"plt.figure(figsize=(40, 40))\nsns.pairplot(train[['target', 'standard_error', 'text_length', 'punct_count']])\nplt.show()","917a8a85":"train = train[train['standard_error'] > 0.4]","10d41032":"# new binary feature, that shows whether licence available or not\ntrain['license_available'] = (~train['license'].isna()).astype(int)","790a6c1e":"plt.figure(figsize=(40, 40))\nsns.pairplot(train[['target', 'standard_error', 'text_length', 'punct_count', 'license_available']], hue='license_available')\nplt.show()","31d40e23":"for column in ['target', 'standard_error', 'text_length', 'punct_count']:\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(train[column])\n    plt.pause(1)","f05941c4":"x_ols = train[['text_length', 'punct_count', 'license_available']]\ny_ols = train['target']\n\n\nmodel = smf.OLS(y_ols, x_ols)\nres = model.fit()\nres.summary()","32f63b7c":"# function for cross validation\ndef cross_validate(model, x, y, n_splits=5):\n    metrics = []\n    kfolds = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n    \n    for train_index, test_index in kfolds.split(x):\n        print('Fitting...')\n        x_tr, y_tr = x[train_index], y[train_index]\n        x_vl, y_vl = x[test_index], y[test_index]\n        \n        model.fit(x_tr, y_tr)\n        preds = model.predict(x_vl)\n        metrics.append(mean_squared_error(y_vl, preds, squared=False))\n    return metrics","e71c4d7b":"# simple LinReg\nmodel = LinearRegression()\nmetrics_baseline = cross_validate(model, x_ols.values, y_ols.values)\nnp.mean(metrics_baseline)","55e74c61":"test['text_length'] = test['excerpt'].apply(len)\ntest['license_available'] = (~test['license'].isna()).astype(int)\ntest['punct_count'] = test['excerpt'].apply(count_punctuation)\ntest.head()","81bedbec":"x_test_ols = test[['text_length', 'punct_count', 'license_available']]\nbaseline_submission = test[['id']].copy()\nbaseline_submission['target'] = model.predict(x_test_ols)\n# baseline_submission.to_csv('submission.csv', index=False)\nbaseline_submission  # 0.977 public, moving","c8772cb8":"def tokenize_text(text):\n    return re.findall(r'[\\w]+', text.lower())\n\ndef tokenize_corpus(corpus):\n    return [tokenize_text(text) for text in tqdm(corpus)]\n\ndef join_tokenized_sentences(tokenized_sentences):\n    sentence_list = []\n    for doc in tqdm(tokenized_sentences):\n        sentence = str()\n        for token in doc:\n            sentence += token + ' '\n        sentence_list.append(sentence.rstrip())\n    return sentence_list","32834a8d":"corpus_train = train['excerpt'].values\ndocs_train = tokenize_corpus(corpus_train)\n\ncounter = Counter()\nfor sentence in tqdm(docs_train):\n    for word in sentence:\n        counter[word] += 1\n        \nwords = np.array(counter.most_common())\nsubset_of_rare_words = set(words[words[:, 1].astype(int) < 200][:, 0])  # threshold for this may be different\nsubset_of_frequent_words = set(words[words[:, 1].astype(int) > 1500][:, 0])","90c98c85":"def check_if_rare_words(text, subset):\n    amount_of_rare_words = 0\n    for word in text:\n        if word.lower() in subset:\n            amount_of_rare_words += 1\n    return amount_of_rare_words\n\ndef check_if_rare_in_corpus(corpus, subset):\n    return [check_if_rare_words(text, subset) for text in tqdm(corpus)]\n# create some text features\ntrain['amount_of_rare_words'] = check_if_rare_in_corpus(train.excerpt.values, subset_of_rare_words)\ntrain['amount_of_frequent_words'] = check_if_rare_in_corpus(train.excerpt.values, subset_of_frequent_words)","97da770b":"# and some more simple features, count of '\\n' and amount of capital letters\ntrain['\\n_amount'] = train['excerpt'].apply(lambda x: x.count('\\n'))\ntrain['capital_let_amount'] = train['excerpt'].apply(lambda x: len(re.findall(r'[A-Z]+', x)))","9222c894":"repeating = []\nfor doc in docs_train:\n    repeating.append(len(doc) - len(set(doc)))\ntrain['repeating'] = repeating  # measure of \"repeat\" of the text","83c20a91":"mean_word_len = []\nfor doc in tqdm(docs_train):\n    len_w = 0\n    for word in doc:\n        len_w += len(word)\n    mean_word_len.append(len_w \/ len(doc))\ntrain['mean_word_len'] = mean_word_len  # and mean word len in each text","4ac2691a":"plt.figure(figsize=(12, 10))\nsns.heatmap(train.corr(), annot=True, cmap='viridis');","2541e33f":"x_ols = train[[\n    'text_length',\n    'punct_count',\n    'license_available',\n    'amount_of_rare_words',\n    'amount_of_frequent_words',\n    '\\n_amount',\n    'capital_let_amount',\n    'repeating',\n    'mean_word_len'\n]]\ny_ols = train['target']\n\nscaler = StandardScaler()  # scaling\nscaler.fit(x_ols)\nx_ols = scaler.transform(x_ols)\n\n# model = Ridge(0.15)\nmodel = XGBRegressor(n_estimators=700, max_depth=5, learning_rate=0.005, n_jobs=-1)  # there was GridSearch for params\nmetrics_baseline = cross_validate(model, x_ols, y_ols.values)\nprint(np.mean(metrics_baseline))","a2ed437a":"# do all the same for test set\n\ncorpus_test = test['excerpt'].values\ndocs_test = tokenize_corpus(corpus_test)\n\ntest['amount_of_rare_words'] = check_if_rare_in_corpus(test.excerpt.values, subset_of_rare_words)\ntest['amount_of_frequent_words'] = check_if_rare_in_corpus(test.excerpt.values, subset_of_frequent_words)\ntest['\\n_amount'] = test['excerpt'].apply(lambda x: x.count('\\n'))\ntest['capital_let_amount'] = test['excerpt'].apply(lambda x: len(re.findall(r'[A-Z]+', x)))\nrepeating = []\nfor doc in docs_test:\n    repeating.append(len(doc) - len(set(doc)))\ntest['repeating'] = repeating\n\nmean_word_len = []\nfor doc in tqdm(docs_test):\n    len_w = 0\n    for word in doc:\n        len_w += len(word)\n    mean_word_len.append(len_w \/ len(doc))\ntest['mean_word_len'] = mean_word_len\ntest.head()","a2dc20fb":"model.fit(x_ols, y_ols.values)  # fit on all data","da496052":"x_test_freq_words = test[\n    [\n        'text_length',\n        'punct_count',\n        'license_available',\n        'amount_of_rare_words',\n        'amount_of_frequent_words',\n        '\\n_amount',\n        'capital_let_amount',\n        'repeating',\n        'mean_word_len'\n    ]\n]\nx_test_freq_words = scaler.transform(x_test_freq_words)\nsubmission_text = test[['id']].copy()\nsubmission_text['target'] = model.predict(x_test_freq_words)\n# submission_text.to_csv('submission.csv', index=False)\nsubmission_text","837cfb21":"# stemming and deleting stopwords\nstopwords = set(nltk.corpus.stopwords.words('english'))\nstemmer = nltk.stem.snowball.SnowballStemmer('english')\ndocs_train = [[stemmer.stem(token) for token in text if token not in stopwords] for text in tqdm(docs_train)]\n\ntrain['cleared_text'] = join_tokenized_sentences(docs_train)","c22f5938":"# tfidf vectorizer for ngrams in range 2-4\nvectorizer = TfidfVectorizer(lowercase=True, ngram_range=(2, 4),\n                            stop_words=stopwords, min_df=5, max_df=0.9, analyzer='char_wb')\nvectorizer.fit(train['cleared_text'])","ea3605b0":"x_texts = vectorizer.transform(train['cleared_text'])","ffa79705":"model = Ridge(1.01)\ntext_ols_results = cross_validate(model, x_texts, y_ols.values, 5)\nnp.mean(text_ols_results)","b3da9eb9":"test.head()","017bd5c5":"model.fit(x_texts, y_ols.values)  # fit on all data","a5fc331b":"plt.scatter(y_ols.values, model.predict(x_texts))","3eef1b49":"mean_squared_error(y_ols.values, model.predict(x_texts), squared=False)  # train rmse","1bf73f22":"docs_test = [[stemmer.stem(token) for token in text if token not in stopwords] for text in tqdm(docs_test)]\n\ntest['cleared_text'] = join_tokenized_sentences(docs_test)","1190c862":"x_texts_test = vectorizer.transform(test['cleared_text'])","8cdb8fd6":"baseline_submission_texts = test[['id']].copy()\nbaseline_submission_texts['target'] = model.predict(x_texts_test)\n# baseline_submission_texts.to_csv('submission.csv', index=False)\nbaseline_submission_texts  # 0.807","3225dd43":"# and final step - blending XGBoost on simple feats and Ridge on tfidf feats\nsubmission = baseline_submission.copy()\nsubmission['target'] = 0.05*submission_text['target'] + 0.95*baseline_submission_texts['target']\nsubmission.to_csv('submission.csv', index=False)\nsubmission  # 0.725 public","9267ce85":"# CommonLit Readability\n### With simple features and preprocessing","7608f57b":"### Take a look on some visualizations","48c5fad9":"## NLP","f71b5cff":"## Load the data","e40994c1":"### We have a long tail in std, so let's count it as an outlier","1f5f0590":"## EDA","0cdb439d":"### As mentioned above, here's outlier, let us get rid of it","5b252791":"## Text features","ac994bf6":"## Baseline OLS"}}