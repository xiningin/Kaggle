{"cell_type":{"41463cbe":"code","ff350582":"code","e10c2a9a":"code","4545b600":"code","3888e0e2":"code","afe4a504":"code","3f5f35cb":"code","8f17538a":"code","2e59609f":"code","dcc85df4":"code","a9e24794":"code","c3ac3da7":"code","edbd52f5":"code","7dc99def":"code","63fc05c9":"code","9903ff68":"code","929324d9":"code","dd539ee6":"code","050248d2":"code","6d1a3b9a":"code","b7adb9a2":"code","0a4b4943":"code","ea47e9a8":"code","3ffbd6aa":"code","36cde045":"code","38f64835":"code","e6300259":"code","4e6713a2":"code","3cd0045c":"code","a0f8ecd1":"code","529cdbc9":"code","4c261f66":"code","e4c890eb":"markdown","8bf2757a":"markdown","5fa868f2":"markdown","b03bab26":"markdown","5a936d9d":"markdown","5a23d0be":"markdown","6610f2c5":"markdown","ae631bce":"markdown"},"source":{"41463cbe":"!nvcc -V\n!gcc --version","ff350582":"!pip install -U torch==1.7.1+cu110 torchvision==0.8.2+cu101 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html\n!pip install mmcv-full\n!rm -rf mmdetection\n!git clone https:\/\/github.com\/open-mmlab\/mmdetection.git\n%cd mmdetection\n!pip install -e .\n!pip install Pillow==7.0.0","e10c2a9a":"import torch, torchvision\nprint(torch.__version__, torch.cuda.is_available())\n\nimport mmdet\nprint(mmdet.__version__)\n\nfrom mmcv.ops import get_compiling_cuda_version, get_compiler_version\nprint(get_compiling_cuda_version())\nprint(get_compiler_version())","4545b600":"!mkdir checkpoints\n!wget -c https:\/\/download.openmmlab.com\/mmdetection\/v2.0\/mask_rcnn\/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco\/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth \\\n      -O checkpoints\/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth","3888e0e2":"from mmdet.apis import inference_detector, init_detector, show_result_pyplot\n\n# Choose to use a config and initialize the detector\nconfig = 'configs\/mask_rcnn\/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco.py'\n# Setup a checkpoint file to load\ncheckpoint = 'checkpoints\/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'\n# initialize the detector\nmodel = init_detector(config, checkpoint, device='cuda:0')","afe4a504":"img = '.\/demo\/demo.jpg'\nresult = inference_detector(model, img)","3f5f35cb":"show_result_pyplot(model, img, result, score_thr=0.3)","8f17538a":"%cd ..","2e59609f":"import os\nfrom mmdet.datasets import build_dataset\nfrom mmdet.models import build_detector\nfrom mmdet.apis import train_detector\nimport glob\nimport cv2\nimport shutil\nimport random\nimport os.path as osp\nimport json\nimport mmcv\nimport re\nimport xml.etree.ElementTree as ET\nfrom typing import Dict, List","dcc85df4":"#Stolen from https:\/\/www.kaggle.com\/sid4sal\/converting-txt-labels-to-xml-labels\ndef convert_txt(source):\n\n    for txt_file in glob.glob(source + '\/*.txt'):\n\n        f = open(txt_file)\n        f_str = f.read()\n        f.close()\n\n        lst = list(map(int, f_str.split()))\n        n = lst[0]\n\n        fx = open(txt_file.replace(\".txt\",\".xml\"), \"x\")\n\n        fx.write(\"<annotation>\\n\")\n\n        fx.write(\"  <filename>{}.jpeg<\/filename>\\n\".format(txt_file.replace(source,\"\").replace(\".txt\",\"\").replace(\"\/\",\"\").replace(\"\\\\\",\"\")))\n        im = cv2.imread(txt_file.replace(\".txt\",\".jpeg\"))\n        h,w,c = im.shape\n        fx.write(\"  <size>\\n\")\n        fx.write(\"      <width>{}<\/width>\\n\".format(w))\n        fx.write(\"      <height>{}<\/height>\\n\".format(h))\n        fx.write(\"      <depth>{}<\/depth>\\n\".format(c))\n        fx.write(\"  <\/size>\\n\")\n\n        fx.write(\"  <segmented>0<\/segmented>\\n\")\n\n        for i in range(n):\n            xmin = lst[(i*4)+1]\n            ymin = lst[(i*4)+2]\n            xmax = lst[(i*4)+3]\n            ymax = lst[(i*4)+4]\n            fx.write(\"  <object>\\n\")\n            fx.write(\"      <name>Gun<\/name>\\n\")\n            fx.write(\"      <bndbox>\\n\")\n            fx.write(\"          <xmin>{}<\/xmin>\\n\".format(xmin))\n            fx.write(\"          <ymin>{}<\/ymin>\\n\".format(ymin))\n            fx.write(\"          <xmax>{}<\/xmax>\\n\".format(xmax))\n            fx.write(\"          <ymax>{}<\/ymax>\\n\".format(ymax))\n            fx.write(\"      <\/bndbox>\\n\")\n            fx.write(\"  <\/object>\\n\")\n        fx.write(\"<\/annotation>\")\n        fx.close()","a9e24794":"!mkdir \/kaggle\/working\/xml-labels","c3ac3da7":"!cp -a ..\/input\/guns-object-detection\/Images\/. \/kaggle\/working\/xml-labels\n!cp -a ..\/input\/guns-object-detection\/Labels\/. \/kaggle\/working\/xml-labels\nconvert_txt(\"\/kaggle\/working\/xml-labels\")","edbd52f5":"for file in os.listdir('\/kaggle\/working\/xml-labels'):\n    if file[-3:] == 'txt':\n        os.remove('\/kaggle\/working\/xml-labels\/' + file)","7dc99def":"!mkdir images","63fc05c9":"for file in os.listdir('\/kaggle\/working\/xml-labels'):\n    if file[-4:] == 'jpeg':\n        shutil.move('\/kaggle\/working\/xml-labels\/' + file, '\/kaggle\/working\/images')","9903ff68":"!mkdir val-xml-labels\n!mkdir val-images","929324d9":"val_ids = random.sample(range(1, 333), 30)","dd539ee6":"for ids in val_ids:\n    shutil.move('\/kaggle\/working\/xml-labels\/' + str(ids) + '.xml', '\/kaggle\/working\/val-xml-labels')\n    shutil.move('\/kaggle\/working\/images\/' + str(ids) + '.jpeg', '\/kaggle\/working\/val-images')    ","050248d2":"%%writefile labels.txt\nGun","6d1a3b9a":"len(os.listdir('xml-labels'))","b7adb9a2":"f = open(\"train.txt\", \"x\")\nf.close()\nlines = []\nfor file in os.listdir('\/kaggle\/working\/xml-labels'):\n    lines.append('\/kaggle\/working\/xml-labels\/' + file)\nwith open('train.txt', 'w') as f:\n    for line in lines:\n        f.write(line)\n        f.write('\\n')\nf.close()","0a4b4943":"f = open(\"val.txt\", \"x\")\nf.close()\nlines = []\nfor file in os.listdir('\/kaggle\/working\/val-xml-labels'):\n    lines.append('\/kaggle\/working\/val-xml-labels\/' + file)\nwith open('val.txt', 'w') as f:\n    for line in lines:\n        f.write(line)\n        f.write('\\n')\nf.close()","ea47e9a8":"#Stolen from https:\/\/github.com\/yukkyo\/voc2coco\ndef get_label2id(labels_path: str) -> Dict[str, int]:\n    with open(labels_path, 'r') as f:\n        labels_str = f.read().split()\n    labels_ids = list(range(1, len(labels_str)+1))\n    return dict(zip(labels_str, labels_ids))\n\n\ndef get_annpaths(ann_dir_path: str = None,\n                 ann_ids_path: str = None,\n                 ext: str = '',\n                 annpaths_list_path: str = None) -> List[str]:\n    # If use annotation paths list\n    if annpaths_list_path is not None:\n        with open(annpaths_list_path, 'r') as f:\n            ann_paths = f.read().split()\n        return ann_paths\n\n    # If use annotaion ids list\n    ext_with_dot = '.' + ext if ext != '' else ''\n    with open(ann_ids_path, 'r') as f:\n        ann_ids = f.read().split()\n    ann_paths = [os.path.join(ann_dir_path, aid+ext_with_dot) for aid in ann_ids]\n    return ann_paths\n\n\ndef get_image_info(annotation_root, extract_num_from_imgid=True):\n    path = annotation_root.findtext('path')\n    if path is None:\n        filename = annotation_root.findtext('filename')\n    else:\n        filename = os.path.basename(path)\n    img_name = os.path.basename(filename)\n    img_id = os.path.splitext(img_name)[0]\n    if extract_num_from_imgid and isinstance(img_id, str):\n        img_id = int(re.findall(r'\\d+', img_id)[0])\n\n    size = annotation_root.find('size')\n    width = int(size.findtext('width'))\n    height = int(size.findtext('height'))\n\n    image_info = {\n        'id': img_id,\n        'width': width,\n        'height': height,\n        'file_name': filename,\n    }\n    return image_info\n\n\ndef get_coco_annotation_from_obj(obj, label2id):\n    label = obj.findtext('name')\n#     assert label in label2id, f\"Error: {label} is not in label2id !\"\n    category_id = label2id[label]\n    bndbox = obj.find('bndbox')\n    xmin = int(float(bndbox.findtext('xmin'))) - 1\n    ymin = int(float(bndbox.findtext('ymin'))) - 1\n    xmax = int(float(bndbox.findtext('xmax')))\n    ymax = int(float(bndbox.findtext('ymax')))\n    assert xmax > xmin and ymax > ymin, f\"Box size error !: (xmin, ymin, xmax, ymax): {xmin, ymin, xmax, ymax}\"\n    o_width = xmax - xmin\n    o_height = ymax - ymin\n    ann = {\n        'category_id': category_id,\n        'segmentation': [],  # This script is not for segmentation\n        'area': o_width * o_height,\n        'bbox': [xmin, ymin, o_width, o_height],\n        'iscrowd': 0,\n    }\n    return ann\n\n\ndef convert_xmls_to_cocojson(annotation_paths: List[str],\\\n                             label2id: Dict[str, int],\n                             output_jsonpath: str,\n                             extract_num_from_imgid: bool = True):\n    output_json_dict = {\n        \"images\": [],\n        \"annotations\": [],\n        \"categories\": []\n    }\n    bnd_id = 1  # START_BOUNDING_BOX_ID, TODO input as args ?\n    print('Start converting !')\n    for a_path in annotation_paths:\n        # Read annotation xml\n        ann_tree = ET.parse(a_path)\n        ann_root = ann_tree.getroot()\n\n        img_info = get_image_info(annotation_root=ann_root,\n                                  extract_num_from_imgid=extract_num_from_imgid)\n        img_id = img_info['id']\n        output_json_dict['images'].append(img_info)\n\n        for obj in ann_root.findall('object'):\n            ann = get_coco_annotation_from_obj(obj=obj, label2id=label2id)\n            annot = {'id': bnd_id, 'image_id': img_id,}\n            annot.update(ann)\n            output_json_dict['annotations'].append(annot)\n            bnd_id = bnd_id + 1\n\n    for label, label_id in label2id.items():\n        category_info = {'id': label_id, 'name': label, 'supercategory': 'none'}\n        output_json_dict['categories'].append(category_info)\n\n    with open(output_jsonpath, 'w') as f:\n        output_json = json.dumps(output_json_dict)\n        f.write(output_json)","3ffbd6aa":"def convert_to_coco(ann_path_list='\/kaggle\/working\/train.txt', labels='\/kaggle\/working\/labels.txt', output='\/kaggle\/working\/output.json'):\n    label2id = get_label2id(labels_path=labels)\n    ann_paths = get_annpaths(\n        annpaths_list_path=ann_path_list\n    )\n    convert_xmls_to_cocojson(\n        annotation_paths=ann_paths,\n        label2id=label2id,\n        output_jsonpath=output\n    )\n","36cde045":"convert_to_coco()","38f64835":"convert_to_coco(ann_path_list='\/kaggle\/working\/val.txt', labels='\/kaggle\/working\/labels.txt', output='\/kaggle\/working\/val_output.json')","e6300259":"os.listdir('.\/mmdetection\/checkpoints')","4e6713a2":"!wget -c https:\/\/download.openmmlab.com\/mmdetection\/v2.0\/faster_rcnn\/retinanet_r101_fpn_1x_coco\/retinanet_r101_fpn_1x_coco_20200130-7a93545f.pth \\\n      -O \/kaggle\/working\/mmdetection\/checkpoints\/retinanet_r101_fpn_1x_coco_20200130-7a93545f.pth","3cd0045c":"from mmcv import Config\ncfg = Config.fromfile('\/kaggle\/working\/mmdetection\/configs\/retinanet\/retinanet_r101_fpn_1x_coco.py')","a0f8ecd1":"from mmdet.apis import set_random_seed\n\n# Modify dataset type and path\ncfg.dataset_type = 'CocoDataset'\ncfg.classes = '\/kaggle\/working\/labels.txt'\ncfg.data_root = '\/kaggle\/working'\ncfg.model.bbox_head.num_classes = 1\ncfg.data.test.type = 'CocoDataset'\ncfg.data.test.classes = 'labels.txt'\ncfg.data.test.data_root = '\/kaggle\/working'\ncfg.data.test.ann_file = 'val_output.json'\ncfg.data.test.img_prefix = 'val-images'\n\ncfg.data.train.type = 'CocoDataset'\ncfg.data.train.data_root = '\/kaggle\/working'\ncfg.data.train.ann_file = 'output.json'\ncfg.data.train.img_prefix = 'images'\ncfg.data.train.classes = 'labels.txt'\n\ncfg.data.val.type = 'CocoDataset'\ncfg.data.val.data_root = '\/kaggle\/working'\ncfg.data.val.ann_file = 'val_output.json'\ncfg.data.val.img_prefix = 'val-images'\ncfg.data.val.classes = 'labels.txt'\n\n# modify num classes of the model in box head\n# cfg.model.roi_head.bbox_head.num_classes = 1\n# We can still use the pre-trained Mask RCNN model though we do not need to\n# use the mask branch\ncfg.load_from = '..\/input\/retinanet\/retinanet_r101_fpn_1x_coco_20200130-7a93545f (1).pth'\n\n# Set up working dir to save files and logs.\ncfg.work_dir = '.\/'\n\n# The original learning rate (LR) is set for 8-GPU training.\n# We divide it by 8 since we only use one GPU.\ncfg.optimizer.lr = 0.01 \/ 8\ncfg.optimizer_config.grad_clip = dict(max_norm=35, norm_type=2)\n# cfg.lr_config.warmup = None\n# cfg.lr_config.policy = 'cyclic'\n# cfg.log_config.interval = 10\ncfg.lr_config.policy = 'step'\ncfg.lr_config.step = 7\ncfg.data.samples_per_gpu = 1\ncfg.data.workers_per_gpu = 1\ncfg.evaluation.metric = 'bbox'\ncfg.evaluation.interval = 4\ncfg.checkpoint_config.interval = 12\ncfg.runner.max_epochs = 24\ncfg.log_config.interval = 100\n\ncfg.seed = 0\nset_random_seed(0, deterministic=False)\ncfg.gpu_ids = range(1)\n\n\n# We can initialize the logger for training and have a look\n# at the final config used for training\nprint(f'Config:\\n{cfg.pretty_text}')","529cdbc9":"datasets = [build_dataset(cfg.data.train)]\nmodel = build_detector(\n cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\nmodel.CLASSES = datasets[0].CLASSES\n\nmmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\ntrain_detector(model, datasets, cfg, distributed=False, validate=True)","4c261f66":"model = init_detector(cfg, '\/kaggle\/working\/epoch_24.pth')\nfor i in range(len(val_ids)):\n    img = mmcv.imread('\/kaggle\/working\/val-images\/' + str(val_ids[i]) + '.jpeg')\n    result = inference_detector(model, img)\n    show_result_pyplot(model, img, result)","e4c890eb":"# **Downloading RetinaNet Checkpoint**","8bf2757a":"# **Convert Txt Files to Xml**","5fa868f2":"# **Install MMDetection and Related Libraries**","b03bab26":"# **Visualized Predictions with RetinaNet**","5a936d9d":"# **Convert from VOC to COCO annotation format**","5a23d0be":"# **Config File**","6610f2c5":"# **Test MMDetection**","ae631bce":"# **Train RetinaNet**"}}