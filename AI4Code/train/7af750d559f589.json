{"cell_type":{"9864579c":"code","582ba5d8":"code","f46050d9":"code","9245941e":"code","f63e5cdc":"code","0f4e3db3":"code","a15168d5":"code","ce65ebab":"code","b31a72af":"code","e79da8f2":"code","2bc017dd":"code","10d659a8":"code","d339360b":"code","98b8e4c2":"code","e3f9b41e":"code","35c329a9":"code","65edd5ce":"code","7a5c4ce4":"code","015c5f13":"code","702be1f5":"code","8e9a5b76":"code","5e81c259":"code","78267e29":"code","b775fc17":"code","3555d0b0":"code","20f994f5":"code","f65d61a4":"code","b0da656b":"code","d7075382":"code","612e5470":"code","ea393a5c":"code","f6729461":"code","4bd37e87":"code","8bca2153":"code","239be963":"code","1cafc6c8":"code","fa9dc37a":"code","cf04af4a":"code","dd388d34":"code","82d8022e":"code","36f3d22f":"code","6ad197bf":"code","d5c2b516":"code","0168bcdd":"code","e3c4a768":"code","dd5a1f9e":"code","88c7bb96":"code","62eceee6":"code","b7208be4":"code","a4e00025":"code","8e9ceb26":"code","23506229":"code","ed52f2a7":"code","c2b33bf2":"code","77d644e7":"code","47503162":"code","90e9e60b":"code","82954207":"code","ecd4006e":"code","8b582a75":"code","af589d79":"code","0da3694f":"code","e37cd45c":"code","2f611305":"code","64d9134f":"markdown","36a4bd88":"markdown","e1dfc5a3":"markdown","83369426":"markdown","9bd60a28":"markdown","723ed603":"markdown","cf4205a4":"markdown","57944523":"markdown","232d6151":"markdown","a9c88652":"markdown","c22ea404":"markdown","fe2aadfa":"markdown","1424f2a3":"markdown","567d7c0d":"markdown","c6754aee":"markdown","e5072f80":"markdown","ee0384ac":"markdown","90d6035e":"markdown","6e4273b1":"markdown","6f2b7b83":"markdown","ef58d38f":"markdown","6ce66027":"markdown","7f6644be":"markdown","be710a51":"markdown","51eae3b0":"markdown","5ed8fa40":"markdown","3aece62f":"markdown","6108f954":"markdown"},"source":{"9864579c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.linear_model import LinearRegression,Lasso,Ridge,LassoCV\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\n%matplotlib inline\npd.options.display.max_columns = None","582ba5d8":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain.head()","f46050d9":"train.describe()","9245941e":"train.shape, test.shape","f63e5cdc":"train.isna().sum().sort_values(ascending=False)[:20]","0f4e3db3":"train.sample(3)","a15168d5":"#We can see that PoolQC is nan if pool area is 0. So we can substitute 0 for PoolQC.\ntrain.PoolQC.fillna(0,inplace=True)\ntest.PoolQC.fillna(0,inplace=True)","ce65ebab":"train.MiscFeature.unique()","b31a72af":"train.MiscFeature.fillna(train.MiscFeature.mode().iloc[0],inplace=True)\ntest.MiscFeature.fillna(test.MiscFeature.mode().iloc[0],inplace=True)\n","e79da8f2":"train.Alley.unique()","2bc017dd":"train.Alley.fillna(train.Alley.mode().iloc[0],inplace=True)\ntest.Alley.fillna(test.Alley.mode().iloc[0],inplace=True)","10d659a8":"train.Fence.unique()","d339360b":"train.Fence.fillna(train.Fence.mode().iloc[0],inplace=True)\ntest.Fence.fillna(test.Fence.mode().iloc[0],inplace=True)","98b8e4c2":"#We can see that FireplaceQu is nan where Fireplaces is 0. So we can substitute 0 for FireplaceQu.\ntrain.FireplaceQu.fillna(0,inplace=True)\ntest.FireplaceQu.fillna(0,inplace=True)","e3f9b41e":"#LotFrontage could be anything, let's substitue it by mean\ntrain.LotFrontage.fillna(train.LotFrontage.mean(),inplace=True)\ntest.LotFrontage.fillna(test.LotFrontage.mean(),inplace=True)","35c329a9":"train.loc[train.GarageCond.isnull()]","65edd5ce":"#All Garage Features should be 0 since it doesn't look like null ones have garages.","7a5c4ce4":"train.loc[train.BsmtFinType2.isnull()][:6]","015c5f13":"#All the Basement features are null for 38 houses, which means that they might not have basements at all.","702be1f5":"train.loc[train.MasVnrArea.isnull()]","8e9a5b76":"train.MasVnrType.unique()","5e81c259":"train.MasVnrType.fillna(train.MasVnrType.mode().iloc[0],inplace=True)\ntest.MasVnrType.fillna(test.MasVnrType.mode().iloc[0],inplace=True)","78267e29":"train.MasVnrArea.fillna(train.MasVnrArea.median(),inplace=True)\ntest.MasVnrArea.fillna(test.MasVnrArea.median(),inplace=True)","b775fc17":"train.isna().sum().sort_values(ascending=False)[:5]","3555d0b0":"test.fillna(0,inplace=True)\ntrain.fillna(0,inplace=True)","20f994f5":"train.isna().sum().sort_values(ascending=False)[:4]","f65d61a4":"train.drop(['Id'],axis=1, inplace=True)\ntest.drop(['Id'],axis=1, inplace=True)","b0da656b":"pd.DataFrame(train.skew().sort_values(ascending=False)[:15])","d7075382":"pd.DataFrame(train.skew().sort_values(ascending=True)[:3])","612e5470":"pd.DataFrame(train.kurtosis().sort_values(ascending=False)[:16])","ea393a5c":"train['MiscVal'] = np.log1p(train['MiscVal']) #np.log1p= log(1+x) #inverse = np.expm1\ntrain['PoolArea'] = np.log1p(train['PoolArea'])\ntrain['LotArea'] = np.log1p(train['LotArea'])\ntrain['3SsnPorch'] = np.log1p(train['3SsnPorch'])\ntrain['LowQualFinSF'] = np.log1p(train['LowQualFinSF'])\ntrain['LotFrontage'] = np.log1p(train['LotFrontage'])\ntrain['KitchenAbvGr'] = np.log1p(train['KitchenAbvGr'])\ntrain['BsmtFinSF2'] = np.log1p(train['BsmtFinSF2'])\ntrain['ScreenPorch'] = np.log1p(train['ScreenPorch'])\ntrain['BsmtHalfBath'] = np.log1p(train['BsmtHalfBath'])\ntrain['TotalBsmtSF'] = np.log1p(train['TotalBsmtSF'])\ntrain['BsmtFinSF1'] = np.log1p(train['BsmtFinSF1'])\ntrain['EnclosedPorch'] = np.log1p(train['EnclosedPorch'])\ntrain['MasVnrArea'] = np.log1p(train['MasVnrArea'])\ntrain['SalePrice'] = np.log1p(train['SalePrice'])\ntrain['1stFlrSF'] = np.log1p(train['1stFlrSF'])\ntrain['2ndFlrSF'] = np.log1p(train['2ndFlrSF'])\n\n\ntest['MiscVal'] = np.log1p(test['MiscVal']) \ntest['PoolArea'] = np.log1p(test['PoolArea'])\ntest['LotArea'] = np.log1p(test['LotArea'])\ntest['3SsnPorch'] = np.log1p(test['3SsnPorch'])\ntest['LowQualFinSF'] = np.log1p(test['LowQualFinSF'])\ntest['LotFrontage'] = np.log1p(test['LotFrontage'])\ntest['KitchenAbvGr'] = np.log1p(test['KitchenAbvGr'])\ntest['BsmtFinSF2'] = np.log1p(test['BsmtFinSF2'])\ntest['ScreenPorch'] = np.log1p(test['ScreenPorch'])\ntest['BsmtHalfBath'] = np.log1p(test['BsmtHalfBath'])\ntest['TotalBsmtSF'] = np.log1p(test['TotalBsmtSF'])\ntest['BsmtFinSF1'] = np.log1p(test['BsmtFinSF1'])\ntest['EnclosedPorch'] = np.log1p(test['EnclosedPorch'])\ntest['MasVnrArea'] = np.log1p(test['MasVnrArea'])\ntest['1stFlrSF'] = np.log1p(test['1stFlrSF'])\ntest['2ndFlrSF'] = np.log1p(test['2ndFlrSF'])\n\n","f6729461":"train.shape, test.shape","4bd37e87":"plt.figure(figsize=(15,15))\ncorr = train.corr()\ncorr_target = pd.DataFrame(corr.SalePrice.sort_values(ascending=False)[:20])\ncorr_target.head(15)","8bca2153":"train.sample(3)","239be963":"# Additional Features\ntrain['Lot_frontage_area_ratio'] = train['LotFrontage'] \/ train['LotArea']\ntrain['Remodeled_built_difference'] = train['YearRemodAdd'] - train['YearBuilt']\ntrain['Total_SF_Area_House'] = train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF'] \n\ntest['Lot_frontage_area_ratio'] = test['LotFrontage'] \/ test['LotArea']\ntest['Remodeled_built_difference'] = test['YearRemodAdd'] - test['YearBuilt']\ntest['Total_SF_Area_House'] = test['TotalBsmtSF'] + test['1stFlrSF'] + test['2ndFlrSF'] \n","1cafc6c8":"total_data = pd.concat((train.loc[:,'MSSubClass':'Total_SF_Area_House'], test.loc[:,'MSSubClass':'Total_SF_Area_House']))\ntotal_data = pd.get_dummies(total_data)\ntotal_data.head()","fa9dc37a":"total_data.shape","cf04af4a":"dummies_train = total_data[:train.shape[0]]\ndummies_test = total_data[train.shape[0]:]","dd388d34":"dummies_train.shape, dummies_test.shape","82d8022e":"dummies_train['SalePrice'] = train['SalePrice']","36f3d22f":"X = dummies_train.drop(['SalePrice'],axis=1)\ny = dummies_train.SalePrice\n\nX_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size=0.25, random_state=21)\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)","6ad197bf":"lr = LinearRegression()\nlr.fit(X_train, y_train)\npred = lr.predict(X_valid)\nr2 = r2_score(pred,y_valid)\nrmse = np.sqrt(mean_squared_error(pred,y_valid))\nprint('R2 Score of Multiple Linear Regression Model',r2)\nprint('RMSE of Multiple Linear Regression Model',rmse)","d5c2b516":"rf = RandomForestRegressor() #default features\nrf_fit = rf.fit(X_train, y_train)\npred = rf_fit.predict(X_valid)\nr2 = r2_score(pred,y_valid)\nrmse = np.sqrt(mean_squared_error(pred, y_valid))\nprint('R2 Score of Multiple Linear Regression Model',r2)\nprint('RMSE of Multiple Linear Regression Model',rmse)\n#print('Cross Val RMSE of Random Forest Regression Model',rmse_cv(rf).mean())","0168bcdd":"\n\nrr = Ridge()\nrr_fit = rr.fit(X_train, y_train)\npred = rr_fit.predict(X_valid)\nr2 = r2_score(pred,y_valid)\nrmse = np.sqrt(mean_squared_error(pred, y_valid))\nprint('R2 Score of Ridge Regression Model',r2)\nprint('RMSE of Ridge Regression Model',rmse)\n\nalphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\ncv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() \n            for alpha in alphas]","e3c4a768":"plt.figure(figsize=(8,6))\nplt.style.use('ggplot')\ncv_ridge = pd.Series(cv_ridge, index = alphas)\ncv_ridge.plot(title = \"Validation\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")","dd5a1f9e":"cv_ridge.min()","88c7bb96":"lcv = LassoCV(cv=5,alphas=(1,0.1,0.01,0.001,0.0005),max_iter=10000,tol=0.001)     #alpha is main hyperparameter for Lasso and Ridge Regression\nlcv.fit(X_train, y_train)\npred = lcv.predict(X_valid)\nr2 = r2_score(pred,y_valid)\nrmse = np.sqrt(mean_squared_error(pred, y_valid))\nprint('R2 Score of Lasso Regression Model',r2)\nprint('RMSE of Lasso Regression Model',rmse.mean())\nprint('Cross Val RMSE of Lasso Regression Model',rmse_cv(lcv).mean())","62eceee6":"dummies_train.shape, dummies_test.shape","b7208be4":"dummies_test.drop('SalePrice',axis=1,inplace=True)","a4e00025":"lasso_pred = np.expm1(lcv.predict(dummies_test))","8e9ceb26":"coef = pd.Series(lcv.coef_, index = X_train.columns)","23506229":"print(\"Variables picked \" + str(sum(coef != 0)) + \" and variables eliminated: \" +  str(sum(coef == 0)))","ed52f2a7":"plt.figure(figsize=(6,8))\nimp_coef = pd.concat([coef.sort_values().head(10),coef.sort_values().tail(10)])\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")","c2b33bf2":"import xgboost as xgb\nfrom xgboost.sklearn import XGBRegressor\nparams = {'objective': 'reg:linear', \n          'eta': 0.01, \n          'max_depth': 6, \n          'min_child_weight': 3,\n          'subsample': 0.85,\n          'colsample_bytree': 0.8,\n          'colsample_bylevel': 0.50, \n          'gamma': 0.1, \n          'nthread':4,\n          'scale_pos_weight':1,\n          'eval_metric':'rmse', \n          'seed': 12, \n          'silent': True}\n# create dataset for xgboost\nxgb_data = [(xgb.DMatrix(X_train, y_train), 'train'), (xgb.DMatrix(X_valid, y_valid), 'valid')]\nprint('Starting training...')\n# train\nxgb_model = xgb.train(params, \n                  xgb.DMatrix(X_train, y_train),\n                  10000,  \n                  xgb_data, \n                  verbose_eval=300,\n                  early_stopping_rounds=300)","77d644e7":"xgb_pred = np.expm1(xgb_model.predict(xgb.DMatrix(dummies_test)))","47503162":"fig, ax = plt.subplots(figsize=(20,12))\nxgb.plot_importance(xgb_model, max_num_features=30, height = 0.8, ax = ax)\nplt.title('XGBOOST Features (avg over folds)')\nplt.show()","90e9e60b":"from catboost import CatBoostRegressor\ncat_model = CatBoostRegressor(iterations=100000,\n                                 learning_rate=0.005,\n                                 depth=5,\n                                 eval_metric='RMSE',\n                                 colsample_bylevel=0.8,\n                                 random_seed = 21,\n                                 bagging_temperature = 0.2,\n                                 metric_period = None,\n                                 early_stopping_rounds=200\n                                )\ncat_model.fit(X_train, y_train,eval_set=(X_valid, y_valid),use_best_model=True,verbose=500)\n    \nval_pred = cat_model.predict(X_valid)\nprint('RMSE',np.sqrt(mean_squared_error(val_pred,y_valid)))\ntest_pred_cat = np.expm1(cat_model.predict(dummies_test))","82954207":"def plot_feature_importance(importance,names,model_type):\n    \n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n    \n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n    \n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n    \n    #Define size of bar plot\n    plt.figure(figsize=(10,8))\n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'][:30])\n    #Add chart labels\n    plt.title(model_type + ' FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","ecd4006e":"#plot the catboost result\nplot_feature_importance(cat_model.get_feature_importance(),X_train.columns,'CATBOOST')","8b582a75":"import lightgbm as lgb\nparams = {'objective':'regression',\n         'num_leaves' : 30,\n         'min_data_in_leaf' : 20,\n         'max_depth' : 6,\n         'learning_rate': 0.005,\n         #'min_child_samples':100,\n         'feature_fraction':0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         'lambda_l1': 0.2,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         #'subsample':.8, \n          #'colsample_bytree':.9,\n         \"random_state\" : 11,\n         \"verbosity\": -1}\nrecord = dict()\ngbm_model = lgb.train(params\n                      , lgb.Dataset(X_train, y_train)\n                      , num_boost_round = 100000\n                      , valid_sets = [lgb.Dataset(X_valid, y_valid)]\n                      , verbose_eval = 500\n                      , early_stopping_rounds = 500\n                      , callbacks = [lgb.record_evaluation(record)]\n                     )\nbest_idx = np.argmin(np.array(record['valid_0']['rmse']))\n\n#val_pred = gbm_model.predict(X_valid, num_iteration = gbm_model.best_iteration)\ntest_pred_gbm = np.expm1(gbm_model.predict(dummies_test, num_iteration = gbm_model.best_iteration))","af589d79":"plot_feature_importance(gbm_model.feature_importance(),X_train.columns,'LightGBM')","0da3694f":"test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nensemble_pred = 0.2*lasso_pred + 0.3*xgb_pred + 0.5*test_pred_cat\nsubmission = pd.DataFrame({'id':test.Id, 'SalePrice':ensemble_pred})\nsubmission.to_csv(\"Final.csv\", index = False)","e37cd45c":"ensemble_pred = 0.2*lasso_pred + 0.2*test_pred_gbm + 0.3*test_pred_cat + 0.3*xgb_pred\nsubmission = pd.DataFrame({'id':test.Id, 'SalePrice':ensemble_pred})\nsubmission.to_csv(\"Final3.csv\", index = False)","2f611305":"ensemble_pred = 0.2*lasso_pred + 0.2*test_pred_gbm + 0.4*test_pred_cat + 0.2*xgb_pred\nsubmission = pd.DataFrame({'id':test.Id, 'SalePrice':ensemble_pred})\nsubmission.to_csv(\"Final4.csv\", index = False)","64d9134f":"Now we will check the skewness of the data and make apply log(x+1) to the skewed columns.\nAcceptable values of skewness fall between \u2212 3 and + 3, and kurtosis is appropriate from a range of \u2212 10 to + 10","36a4bd88":"#### Lasso Regression","e1dfc5a3":"We will normalize the top 15 skewed features which we got from kurtosis, along with the SalePrice.","83369426":"#### Data Cleaning","9bd60a28":"#### Ridge Regression","723ed603":"#### Linear Regression","cf4205a4":"**If you use parts of this notebook in your scripts\/notebooks, giving some kind of credit would be very much appreciated :) You can for instance link back to this notebook. Thanks!**\n\nIntroduction\nKaggle describes this competition as follows:\n\nAsk a home buyer to describe their dream house, and they probably won\u2019t begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition\u2019s dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.","57944523":"Lasso picked 111 features and eliminated the rest of the variables. But these may not be the best variables indeed,\nbecause features could be collinear to each other. One idea to try here is run Lasso a few times on boostrapped samples and see how stable the feature selection is.","232d6151":"Our data is now normalized and clean, now let's the correlation between features.","a9c88652":"#### Random Forest Regressor","c22ea404":"Let's split our model into train and validation set and check their performance.","fe2aadfa":"I have also created a Hack Notebook which is giving the score of 0.00044. Don't believe? Check it out here. [Best Score Hack](http:\/\/www.kaggle.com\/adamyanayyar\/hack-notebook-fork-and-upvote-for-best-score)","1424f2a3":"The cat model gives us better performance and the variables also makes sense. Let's try lightGBM now and see if we could decrease our error further.","567d7c0d":"### Modeling","c6754aee":"We can see, LightGBM has it's own important features. \nCatBoost gave us the least error, followed by lasso, XGB and LightGBM. Hence, for our final model, we can create different files and check which model is giving us the best prediction.","e5072f80":"Nice! The lasso performs even better so we'll just use this one to predict on the test set. Another neat thing about the Lasso is that it does feature selection for you - setting coefficients of features it deems unimportant to zero. Let's take a look at the coefficients:","ee0384ac":"R-squared (R2), which is the proportion of variation in the outcome that is explained by the predictor variables. In multiple regression models, R2 corresponds to the squared correlation between the observed outcome values and the predicted values by the model. The Higher the R-squared, the better the model.\n\nRoot Mean Squared Error (RMSE), which measures the average error performed by the model in predicting the outcome for an observation. Mathematically, the RMSE is the square root of the mean squared error (MSE), which is the average squared difference between the observed actual outome values and the values predicted by the model. So, MSE = mean((observeds - predicteds)^2) and RMSE = sqrt(MSE). The lower the RMSE, the better the model.\n\n","90d6035e":"The above models, gave me the best scores. Try out for yourself, different combinations, which works best for you.","6e4273b1":"The sale price depends the most on the Overall Quailty of the house, followed by Ground Living Areas, followed by Garage space and area and other features.\nThis actually makes sense, and it shows that we are on the right track.","6f2b7b83":"XGBoost gave us much better prediction, also important features makes much more sense now.\nI'll use XGBoost as my final model.\nI'll also make an ensemble model of XGBoost and Lasso's result, sometimes averaging uncorrelated results gives\nbetter predictions.","ef58d38f":"Now let's see the categorical features and make some sense out of it.","6ce66027":"#### XGBoost Regressor","7f6644be":"The most correlated features like HouseQuality and Ground Living Area is not in the top 10 features, we wonder why.\nAlso note that unlike the feature importance you'd get from a random forest these are actual coefficients in your model - so you can say precisely why the predicted price is what it is. The only issue here is that we log_transformed both the target and the numeric features so the actual magnitudes are a bit hard to interpret.\nLet's check the XGBooster model now ","be710a51":"Let's do one hot encoding and convert these features to usable features for modeling.","51eae3b0":"Amazing! We now have 311 features(excluding SalePrice), all ready to be modelled.","5ed8fa40":"In order to understand our data, we can look at each variable and try to understand their meaning and relevance to this problem. I know this is time-consuming, but it will give us the flavour of our dataset","3aece62f":"Let's check the dataframe and try to substitute values for some of the features.","6108f954":"Do drop comments where you think I can improve the model or features. Upvote if you liked what you saw. Thanks and much more to come ;)"}}