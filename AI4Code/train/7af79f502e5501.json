{"cell_type":{"554a54bc":"code","d0376f79":"code","05a51ca5":"code","1001aa2c":"code","009cee6f":"code","90bc8b61":"code","b4ad565e":"code","640ad08e":"code","47281659":"code","4fec6b7b":"code","4543e186":"code","09676362":"code","068549a9":"code","99c8547c":"code","01bc0dfd":"code","312c9a63":"code","ff7ffe98":"code","762281c8":"code","96ed63bb":"code","cb21bc01":"code","be2cf814":"code","966f11ce":"code","16815f75":"code","434ce90b":"code","7c84c973":"code","5b30fdc6":"code","a43fb9cb":"code","803c8a9e":"code","688b2b49":"code","a385c17e":"code","97aa0824":"code","94249a43":"code","93829cc2":"code","b86df7a1":"code","c47464d9":"code","615c382f":"code","9bd90c40":"code","4bad462e":"code","2a8d8bd8":"code","7f4360f8":"code","aeb0f25f":"code","7040aff9":"code","1bc08a10":"code","6da63fa3":"code","e048fbc8":"code","15b299cc":"code","2c3706bd":"code","d8c5a886":"code","2b5db207":"code","021698cc":"code","c0c8e2a4":"code","5d934f7f":"code","73ad39a1":"code","9b57b3e5":"code","195e1535":"code","7c57f662":"markdown","e570f915":"markdown","323f993d":"markdown","04edd497":"markdown","41fdd864":"markdown","364491c0":"markdown","65dc8811":"markdown","71bdfa97":"markdown","cdaa9aad":"markdown","19b633e9":"markdown","b28e6c37":"markdown","cd7a2a63":"markdown","8073e599":"markdown","27fa5929":"markdown","c60ce13c":"markdown","56810248":"markdown","1493e900":"markdown","41e78fa8":"markdown","21423c15":"markdown","01c99803":"markdown","2c401f0b":"markdown","fae5323b":"markdown","f3936a3d":"markdown","95780afa":"markdown","3189192f":"markdown"},"source":{"554a54bc":"import csv\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d0376f79":"train = pd.read_csv('..\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv')\ntest = pd.read_csv('..\/input\/loan-prediction-problem-dataset\/test_Y3wMUE5_7gLdaTN.csv')","05a51ca5":"train.describe()","1001aa2c":"train.info()","009cee6f":"train.head()","90bc8b61":"obj_data = []\nfor col in train:\n    if train[col].dtype ==object:\n        obj_data.append(col)\n\nobj_data.pop(0)#remove the loan_id \nobj_data.append('Credit_History')","b4ad565e":"f, axes =plt.subplots(3,3, figsize=(19,17))\n\nx=0 #the axes coordinates\ny=0\nfor col in obj_data:\n    ax_ = sns.countplot(train[col], hue=train.Loan_Status, ax=axes[x,y])\n    ax_.set(ylabel='')\n    if y<2:\n        y+=1\n    else:\n        y=0\n        x+=1\n        \naxes[2,2].remove()# there's only 8 plots on this 3x3 grid","640ad08e":"num_data = []\nfor col in train:\n    if col != 'Loan_ID' and col not in obj_data:\n        num_data.append(col)","47281659":"num_data.remove('Loan_Amount_Term')# this is giving a bandwith error so it'll be plotted separately","4fec6b7b":"fig= sns.FacetGrid(train, hue='Loan_Status',aspect=4)\n# we need to provide a bandwith because the kde function is unable to find one. It is obvious that there is no correlation here\nfig.map(sns.kdeplot,'Loan_Amount_Term',shade=True, bw=1.2)\noldest=train['Loan_Amount_Term'].max()\n#fig.set(xlim=(0,oldest))\nfig.add_legend()","4543e186":"for col in num_data:\n    fig= sns.FacetGrid(train, hue='Loan_Status',aspect=4) \n    fig.map(sns.kdeplot,col,shade=True)\n    oldest=train[col].max()\n    fig.set(xlim=(0,oldest), xlabel = col)\n    fig.add_legend()","09676362":"tt =[train, test] #creating a list so we can clean the data for both test and train at the same time","068549a9":"train['Loan_Status'].loc[train['Loan_Status'] =='Y'] = 1\ntrain['Loan_Status'].loc[train['Loan_Status'] =='N'] = 0\ntrain['Loan_Status'] = train['Loan_Status'].astype(int)#change the dtype to integer","99c8547c":"for col in num_data:\n    sns.lmplot(col,'Loan_Status', train, height=6,aspect=2)\n","01bc0dfd":"corr = train[num_data+['Loan_Status']].corr()","312c9a63":"sns.heatmap(corr, annot = True)","ff7ffe98":"to_remove = num_data\nto_remove.append('Loan_ID')\nto_remove.append('Loan_Amount_Term')","762281c8":"for df in tt:\n    df.drop(to_remove, axis =1,  inplace = True)","96ed63bb":"f, axes =plt.subplots(3,3, figsize=(17,14))\n\nx=0\ny=0\nfor col in obj_data:\n    if col!='Loan_Status':\n        ax_ = sns.pointplot(col, 'Loan_Status', data=train, kind = 'point', ax=axes[x,y])\n        ax_.set(ylabel='')\n    if y<2:\n        y+=1\n    else:\n        y=0\n        x+=1\naxes[2,2].remove()\naxes[2,0].remove()","cb21bc01":"for df in tt:\n    df.drop(['Gender','Self_Employed'], axis =1, inplace = True)","be2cf814":"train.head()","966f11ce":"from scipy.stats import spearmanr\n\ncorr, _ = spearmanr(train.Property_Area, train.Loan_Status)\nprint(f'Spearmans correlation between Proeprty Area and loan status: {corr:.3f} ')","16815f75":"#Defining a function that finds the most common value, in order to replace nan with it\ndef find_most_common(col):\n    l = train[col].tolist() #get the column values to a list\n    common_len = l.count(l[0])#the count of the first element of the list\n    common_value = l[0]\n    l = [element for element in l if element!=common_value]#delete all occurences of that element\n    while len(set(l))>1: #when the sets length is 1, it means only the nan values left in it\n        current = l[0] \n        if l.count(current) > common_len:# check for each value in the column if it's the most common one\n            common_len = l.count(current)\n            common_value = current\n        l = [element for element in l if element!=current]# delete it and move on the next\n\n    return common_value      ","434ce90b":"for data in tt:\n    for col in test.columns:\n        if data[col].dtype==object:\n            replacement =find_most_common(col) \n            data[col].fillna(replacement, inplace = True)\n        else:\n            data[col].fillna(train[col].median(), inplace = True)","7c84c973":"for data in tt:\n\n    data['Married'].loc[data['Married'] =='No'] = 0\n    data['Married'].loc[data['Married'] =='Yes'] = 1\n\n    data['Dependents'].loc[data['Dependents'] =='3+'] = 3\n\n    data['Education'].loc[data['Education'] =='Not Graduate'] = 0\n    data['Education'].loc[data['Education'] =='Graduate'] = 1\n\n    data['Property_Area'].loc[data['Property_Area'] =='Urban'] = 0\n    data['Property_Area'].loc[data['Property_Area'] =='Rural'] = 1\n    data['Property_Area'].loc[data['Property_Area'] =='Semiurban'] = 2\n    \n    for column in data:\n        data[column] = data[column].astype(int)","5b30fdc6":"train.head()","a43fb9cb":"obj_data_correlation = [col for col in train.columns if col!= 'Property_Area']","803c8a9e":"corr = train[obj_data_correlation].corr()","688b2b49":"sns.heatmap(corr, annot = True)","a385c17e":"train","97aa0824":"for data in tt:\n    data.drop('Dependents', axis = 1, inplace = True)","94249a43":"import sklearn\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n#from sklearn import metrics\n#import statsmodels.api as sm\n\nclassifiers = [\n    KNeighborsClassifier(4),\n    SVC(probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    LogisticRegression()]\n","93829cc2":"#A function to test our models accuracy and precision \n\nfrom sklearn.metrics import precision_score , recall_score, f1_score, log_loss, accuracy_score, matthews_corrcoef\n\ndef test_model(y_test, y_pred):    \n    pre = precision_score(y_test, y_pred)\n    rec = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    loss = log_loss(y_test, y_pred)\n    acc = accuracy_score(y_test, y_pred)\n    matc = matthews_corrcoef(y_test, y_pred)\n    \n    \n    print('  pre: %.3f\\n  rec: %.3f\\n  f1: %.3f\\n  loss: %.3f\\n  acc: %.3f\\n  matc: %.3f' % (pre, rec, f1, loss, acc, matc))","b86df7a1":"def model_and_test(X, y):\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\n    for train_, test_ in sss.split(X, y):\n        X_train, X_test = X.iloc[train_], X.iloc[test_]\n        y_train, y_test = y.iloc[train_], y.iloc[test_]\n    for model in classifiers:\n        #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)# splitting the data into 70%\/30%\n        this_model = model.__class__.__name__ #get the name of the classifier\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        \n        print(f'{this_model} results:')\n        test_model(y_test, y_pred)\n        print('\\n')","c47464d9":"X =train.drop('Loan_Status', axis = 1)\ny=train.Loan_Status\nmodel_and_test(X, y)","615c382f":"sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train_, test_ in sss.split(X, y):\n    X_train, X_test = X.iloc[train_], X.iloc[test_]\n    y_train, y_test = y.iloc[train_], y.iloc[test_]\n\ny_pred = LogisticRegression().fit(X_train, y_train).predict(X_test)","9bd90c40":"y_test_ = np.ravel(y_test)","4bad462e":"cred =np.ravel(X_test['Credit_History'])","2a8d8bd8":"print(cred == y_pred)","7f4360f8":"train0 = pd.read_csv('..\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv')","aeb0f25f":"train0.head()","7040aff9":"c = 0\nfor i in range(len(train0)):\n    if train0.Credit_History[i] == 1.0 and train0.Loan_Status[i] =='N':\n        c+=1\n    elif train0.Credit_History[i] ==0 and train0.Loan_Status[i] =='Y':\n        c+=1\n\np = c\/len(train0) *100\nprint(f'{c} False predictions({p:.2f}%)')","1bc08a10":"no_cred = 0\ngot_loan=0\nfor i in range(len(train0)):\n    if train0.Credit_History[i] ==0: \n        if train0.Loan_Status[i] =='Y':\n            got_loan+=1\n        no_cred += 1\n\nfalse_negatives = got_loan\/no_cred\n#print('  pre: %.3f\\n  rec: %.3f\\n  f1: %.3f\\n  loss: %.3f\\n  acc: %.3f' % (pre, rec, f1, loss, acc))\nprint(f'{no_cred} applicants with no credit history: ; only {got_loan} of them got a loan ({false_negatives:.2f}%)')\n\ncred = 0\nno_loan=0\nfor i in range(len(train0)):\n    if train0.Credit_History[i] ==1: \n        if train0.Loan_Status[i] =='N':\n            no_loan+=1\n        cred += 1\nfalse_positives = no_loan\/ cred        \nprint(f'{cred} applicants with credit history: ; {no_loan} of them did not get a loan({false_positives:.2f}%)')","6da63fa3":"false_neg = train0[(train0['Credit_History'] == 0) & (train0['Loan_Status'] == 'Y')]\n","e048fbc8":"false_neg","15b299cc":"from scipy.stats import spearmanr\n\ndef spearman(col):\n    corr, _ = spearmanr(train0[col], train0.Loan_Status)\n    print(f'Spearmans correlation between {col} and loan status: {corr:.3f} ')\n\n#num_data.remove('Loan_ID')\nfor col in num_data:\n    spearman(col)\n    \n    ","2c3706bd":"loan_score = []\nfor i in range(len(train0)):\n    scr = (train0.ApplicantIncome[i] + train0.CoapplicantIncome[i])\/train0.LoanAmount[i]\n    loan_score.append(scr)","d8c5a886":"corr, _ = spearmanr(loan_score, train0.Loan_Status)\nprint(f'Spearmans correlation between {col} and loan status: {corr:.3f} ')","2b5db207":"loan_score0 = []\nfor i in range(len(train0)):\n    scr = (train0.ApplicantIncome[i] + train0.CoapplicantIncome[i]) - train0.LoanAmount[i]*train0.Loan_Amount_Term[i]\n    loan_score0.append(scr)","021698cc":"corr, _ = spearmanr(loan_score0, train0.Loan_Status)\nprint(f'Spearmans correlation between thisnew score and loan status: {corr:.3f} ')","c0c8e2a4":"train0['loan_score'] = loan_score0","5d934f7f":"train0.info()","73ad39a1":"data = train0.copy()\ndata.drop(['Gender','Self_Employed', 'Loan_ID' ,'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',\n          'Loan_Amount_Term','Dependents'], axis =1, inplace = True)\ndata['Loan_Status'].loc[data['Loan_Status'] =='Y'] = 1\ndata['Loan_Status'].loc[data['Loan_Status'] =='N'] = 0\ndata['Loan_Status'] = data['Loan_Status'].astype(int)#change the dtype to integer\n\nfor col in data.columns:\n    if data[col].dtype==object:\n        replacement =find_most_common(col) \n        data[col].fillna(replacement, inplace = True)\n    elif col=='Credit_History':\n        data[col].fillna(data[col].mean(), inplace = True)\n    else:\n        data[col].fillna(data[col].median(), inplace = True)\n\ndata['Married'].loc[data['Married'] =='No'] = 0\ndata['Married'].loc[data['Married'] =='Yes'] = 1\n\ndata['Education'].loc[data['Education'] =='Not Graduate'] = 0\ndata['Education'].loc[data['Education'] =='Graduate'] = 1\n\n\ndata['Property_Area'].loc[data['Property_Area'] =='Urban'] = 0\ndata['Property_Area'].loc[data['Property_Area'] =='Rural'] = 1\ndata['Property_Area'].loc[data['Property_Area'] =='Semiurban'] = 2\n\nfor column in data:\n    if column!='Credit_History':\n        data[column] = data[column].astype(int)\n    else:\n        data[col] = 100* data[col]","9b57b3e5":"data.head()","195e1535":"acc_dict = {}\nlog_cols = [\"Classifier\", \"Accuracy\"]\nlog  = pd.DataFrame(columns=log_cols)\n\n\nfor clf in classifiers:\n    name = clf.__class__.__name__\n    clf.fit(X_train, y_train)\n    train_predictions = clf.predict(X_test)\n    acc = matthews_corrcoef(y_test, y_pred)\n    if name in acc_dict:\n        acc_dict[name] += acc\n    else:\n        acc_dict[name] = acc\n        \nfor clf in acc_dict:\n    acc_dict[clf] = acc_dict[clf] \/ 10.0\n    log_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n    log = log.append(log_entry)\n    \nplt.xlabel('Accuracy')\nplt.title('Classifier Accuracy')\n\nsns.set_color_codes(\"muted\")\nsns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")","7c57f662":"Before getting into that, let's remove the non-object dtypes, loan_id, and turn the object dtypes into numeric data","e570f915":"Eeven though coapplicant income and loan amount appear to significantly negatively correlate with the loan status, that may be an artefact of the small amount of data available, the error margin(shaded area) being very high.","323f993d":"To visualize the relation between numerical data and loan status, we need to update the 'Loan_Staus' column, turning 'No' values to 0, and 'Yes' to 1","04edd497":"Using Spearman correlation we see that Property Area has close to 0 correlation with loan status\n\n**Before we can work on the ML models, we must fill nan values and turn all data types to numeric**","41fdd864":"Although the column Credit_history is stored as type float, it stores binary data, so it'll be treated as an object dtype.\nNote: we could turn it to object type, however that messes with some of the code down the line.\n\nThe Nan values are ignored for now, later we'll see whether they impact the models in any way","364491c0":"We can try to use this in our model, although the correlation is still very low and we cannot expect it to make a big difference, it may be worth a try.\n\nOne other thing to try is, instead of replacing null credit score with the most common value 1, replace them with the mean 0.69, so perhaps for those rows the other variables will have a bigger impact.","65dc8811":"Look at false negatives\nLook at false positives\nLook at nan values\n","71bdfa97":"For object dtypes, the most common occurence is assigned to null values\nFor the float types: I chose mean for LoanAmount, but for Loan_Amount_Term, I went for the median, since the vast majority of loan terms is 360","cdaa9aad":"Removing all Null values for the next model","19b633e9":"In order to visualize the data, let's do countplot for all columns with object type data","b28e6c37":"Next let's look at the numerical data","cd7a2a63":"The property area is categorical non-binary data, so this correlation value has no meaning. We'd have to use something such \nas spearman correlation for it.\n\nIt seems that there is a significant correlation between whether someone is married and if they have dependents. Moreover, \nthe correlation between loan status and dependents is just 0.01, so we can just remove it.\n\nIt's also clear that the credit history is by far the most important variable in determining whether someone will get the loan \nor not.\nWe'll be comparing the accuracy of our multi-variable model with a model that only takes in the credit history ","8073e599":"Although loan amount and applicant income showed little to no correlation with the loan status, it makes sense to think that's\nbecause most of the times, applicants ask for a sensible sum given their income, however adding a column that combines \napplicant, coapplicant incomes together with loan amount might be useful in spotting some exceptions","27fa5929":"Except for KNN, all models are predicting the same values\n\nWe're getting very high recall scores, close to 100%. \n\nWhat this indicates is that the model is correctly classifying almost all true positives, however as the precision lies around\n80%, it's overestimating the amount of positives.\n\nMatthews correlation coefficient takes into account all four values of the confusion matrix, and it indicates how well both \nclasses are represented. Given this dataset, it constitutes a better alternative for quantifying the usefulness of the model, as the amount of people who got the loan is almost an order of magnitude higher than those who didn't.\n\nThe discrepancy between accuracy and Matthews corrcoef suggests further suggests that the amount of people with positive loan status is overestimated.","c60ce13c":"*It seems that the model simply follows the credit history, as the correlation with the other variables is too low.*\n\n**If you enjoyed this kernel, or if you have any comments or suggestions, please feel free to let me know!**","56810248":"Currently, all models follow cred history","1493e900":"## * * Loan status classification\n\nIn this kernel I attempt to classify whether someone was awarded a loan or not.\n\nFirst a few graphs are plotted to familiarize ourselves with the data.\n\nThen, a variety of models are tested:\n\n* StratifiedShuffleSplit\n* LogisticRegression\n* KNeighborsClassifier\n* SVC\n* DecisionTreeClassifier\n* RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n\nAnd scored using: \n* precision_score\n* recall_score\n* f1_score\n* log_loss\n* accuracy_score\n* matthews_corrcoef\n\nFinally, we conclude that the best mode simply follows the credit history status.","41e78fa8":"All quantitative columns show correlation factor absolute values <0.1 in respect to the the loan status, so they will be removed for the initial model","21423c15":"Let's begin by importing the libraries and reading the data in","01c99803":"To avoid sampling bias, will use stratified shuffle split","2c401f0b":"Now there are no more null values","fae5323b":"We can already tell from the previous graphs which data may significantly relate to the loan status:\n\nmarriage status, education, property area, and by far the most important one: credit history\n\nFor a quantitave representation of correlation, the following graphs are plotted:","f3936a3d":"Before we modify anything, let's look at the cases with credit history when applicants didn't get a loan, as well as the scenarios\nwhen applicants didn't have credit history but got a loan anyways","95780afa":"Things that should logically increase the chances someone gets a loan is the income and a factor that should decrease it is the sum they are asking for in relation to the income","3189192f":"Self-employment status and gender are irrelevant too"}}