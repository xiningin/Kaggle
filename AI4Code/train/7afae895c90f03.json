{"cell_type":{"216a9582":"code","e1c159e8":"code","d1d5524d":"code","6550db8d":"code","7c4b2490":"code","36f720e8":"code","4824d01f":"code","7aa18210":"code","251875cd":"code","37584863":"code","9944a78d":"code","b0f32ecd":"code","b98fe75a":"code","515e92c2":"code","3420654f":"code","bae663b7":"code","7e3b9eeb":"code","ea9c3641":"code","61631be7":"code","b4111d7b":"code","9c4c9d20":"code","ed8600ff":"code","96b8a3d0":"code","dba10889":"code","cd9a7998":"code","61c9e61e":"code","f88e2b4e":"code","23b21330":"code","e50b392e":"code","57202f40":"code","18b2b71e":"code","f9595982":"code","9f46e3ab":"code","cdcfb753":"code","7aa7fa7e":"code","a7c20f93":"code","047172c5":"code","4dd3a9c8":"code","649a5de4":"markdown","6be80bf2":"markdown","ed172c8c":"markdown","9a60343b":"markdown","59e9a418":"markdown","0834ec54":"markdown","7f8a119e":"markdown","101831a4":"markdown","457f093b":"markdown","2a020e3e":"markdown","262d7ece":"markdown","cb9b7bc1":"markdown","45ccf781":"markdown","3bdbd849":"markdown","58ddd87f":"markdown","7fec7c34":"markdown","2c3b7f27":"markdown","09f0d392":"markdown","6e863a93":"markdown","630dd0cf":"markdown","67f1aebf":"markdown","e8365a39":"markdown","96d97ab3":"markdown"},"source":{"216a9582":"!pip install -q ffmpeg-python","e1c159e8":"import warnings\nwarnings.filterwarnings('ignore')\nimport cv2\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport ffmpeg\nfrom IPython.display import Video\nfrom tqdm import tqdm\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport logging\nfrom itertools import cycle\n\nlogging.disable(logging.WARNING)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n\n\nplt.style.use('ggplot')\ncm = sns.light_palette(\"green\", as_cmap=True)\npd.option_context('display.max_colwidth', 100)\ncolor_pal = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\ncolor_cycle = cycle(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])","d1d5524d":"# SEED EVERYTHING\nrandom.seed(hash(\"setting random seeds\") % 2**32 - 1)\nnp.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n\n# config\nclass config:\n    BASE_DIR = \"..\/input\/tensorflow-great-barrier-reef\/train_images\/\"","6550db8d":"img_og = plt.imread('..\/input\/tensorflow-great-barrier-reef\/train_images\/video_1\/9101.jpg')\nimg_9101 = cv2.imread('..\/input\/tensorflow-great-barrier-reef\/train_images\/video_1\/9101.jpg')","7c4b2490":"df = pd.read_csv('..\/input\/tensorflow-great-barrier-reef\/train.csv')\ntrain_dir = \"..\/input\/tensorflow-great-barrier-reef\/train_images\"\ndf['image_path'] = train_dir + \"\/video_\" + df['video_id'].astype(str) + \"\/\" + df['video_frame'].astype(str) + \".jpg\"\ndf.head().style.set_properties(**{'background-color': 'black',\n                           'color': 'lawngreen',\n                           'border-color': 'white'})","36f720e8":"df.info() # lets check more details about the data","4824d01f":"df[df.annotations.str.len() > 2].head(5).style.background_gradient(cmap=cm) # filling up the annotation column","7aa18210":"df['annotations'] = df['annotations'].apply(eval)\ndf_train_v2 = df[df.annotations.str.len() > 0 ].reset_index(drop=True)\ndf_train_v2.head(5).style.background_gradient(cmap='Reds')","251875cd":"df_train_v2[\"no_of_bbox\"] = df_train_v2[\"annotations\"].apply(lambda x: len(x))\ndf_train_v2[\"sequence\"].value_counts(), len(df_train_v2[\"sequence\"].value_counts())","37584863":"for i in range(3):\n    print(df_train_v2[\"sequence\"][df_train_v2[\"video_id\"] == i].unique(), \n          df_train_v2[\"sequence\"][df_train_v2[\"video_id\"] == i].nunique())","9944a78d":"def plot_with_count(df,vid):\n    names = df[\"bbox_typ\"].to_list()\n    values = df[\"counts\"].to_list()\n\n    N = len(names)\n    menMeans = values\n    ind = np.arange(N)\n\n    plt.rcParams[\"figure.figsize\"] = [7.00, 3.50]\n    plt.rcParams[\"figure.autolayout\"] = True\n    fig, ax = plt.subplots(figsize=(15,6))\n\n    ax.bar(ind,menMeans,width=0.4)\n    plt.xticks(np.arange(0, N, step=1))\n    plt.title(f\"Number of bounding box VS Count of Bounding Box: Video{vid} \",fontsize=20)\n\n    plt.xlabel('Number of bounding box', fontsize=18)\n    plt.ylabel('Count', fontsize=16)\n\n    for index,data in enumerate(menMeans):\n        plt.text(x=index , y =data+1 , s=f\"{data}\" , fontdict=dict(fontsize=15))","b0f32ecd":"vid = 0\ndf_vod0_bbox_cnt = df_train_v2[\"no_of_bbox\"][df_train_v2[\"video_id\"] == vid].value_counts().reset_index() # LEARNING .to_frame() and .reset_index()\ndf_vod0_bbox_cnt.columns = ['bbox_typ', 'counts']\nplot_with_count(df_vod0_bbox_cnt,vid)","b98fe75a":"vid = 1\ndf_vod1_bbox_cnt = df_train_v2[\"no_of_bbox\"][df_train_v2[\"video_id\"] == vid].value_counts().reset_index() # LEARNING .to_frame() and .reset_index()\ndf_vod1_bbox_cnt.columns = ['bbox_typ', 'counts']\nplot_with_count(df_vod1_bbox_cnt,vid)","515e92c2":"vid = 2\ndf_vod2_bbox_cnt = df_train_v2[\"no_of_bbox\"][df_train_v2[\"video_id\"] == vid].value_counts().reset_index() # LEARNING .to_frame() and .reset_index()\ndf_vod2_bbox_cnt.columns = ['bbox_typ', 'counts']\nplot_with_count(df_vod2_bbox_cnt,vid)","3420654f":"# https:\/\/www.kaggle.com\/julian3833\/reef-a-cv-strategy-subsequences\ndf = pd.read_csv(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/train.csv\")\ndf['annotations'] = df['annotations'].apply(eval)\ndf['n_annotations'] = df['annotations'].str.len()\ndf['has_annotations'] = df['annotations'].str.len() > 0\ndf['has_2_or_more_annotations'] = df['annotations'].str.len() >= 2\ndf['doesnt_have_annotations'] = df['annotations'].str.len() == 0\ndf['image_path'] = config.BASE_DIR + \"video_\" + df['video_id'].astype(str) + \"\/\" + df['video_frame'].astype(str) + \".jpg\"","bae663b7":"df_agg = df.groupby([\"video_id\", 'sequence']).agg({'sequence_frame': 'count', 'has_annotations': 'sum', 'doesnt_have_annotations': 'sum'})\\\n           .rename(columns={'sequence_frame': 'Total Frames', 'has_annotations': 'Frames with at least 1 object', 'doesnt_have_annotations': \"Frames with no object\"})\ndf_agg","7e3b9eeb":"def RecoverCLAHE(sceneRadiance):\n    clahe = cv2.createCLAHE(clipLimit=7, tileGridSize=(14, 14))\n    for i in range(3):\n        sceneRadiance[:, :, i] = clahe.apply((sceneRadiance[:, :, i]))\n    return sceneRadiance\n\ndest_path1 = \".\/clahe_img\"\nos.mkdir(dest_path1)\n\nfor img_path in tqdm(df_train_v2[\"image_path\"][0:400]):\n\n    image = plt.imread(img_path)\n    image_cv = cv2.imread(img_path)\n    img_clahe = RecoverCLAHE(image_cv)\n    file_name = img_path.split(\"\/\")[-1]\n    \n    cv2.imwrite(dest_path1+\"\/\"+file_name, img_clahe)","ea9c3641":"dest_path1 = \".\/annot_img\"\nos.mkdir(dest_path1)\n\nidx = 0\nfor img_idx in tqdm(df_train_v2[\"image_path\"][0:400]):\n    file_name = img_idx.split(\"\/\")[-1] \n    img_path = os.path.join(\".\/clahe_img\",file_name)\n    image = plt.imread(img_path)\n\n\n    for i in range(len(df_train_v2[\"annotations\"][idx])):\n        file_name = img_path.split('\/')[-1]\n        b_boxs = df_train_v2[\"annotations\"][idx][i]\n        x,y,w,h = b_boxs[\"x\"],b_boxs[\"y\"],b_boxs[\"width\"],b_boxs[\"height\"]\n\n        image = cv2.rectangle(image, (x, y), (x + w, y + h), (36,255,12), 3)\n        image = cv2.putText(image, 'starfish', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n\n    cv2.imwrite(dest_path1+\"\/\"+file_name, image)\n    idx +=1","61631be7":"plt.figure(figsize = (12,15))\n\n\nplt.rcParams[\"figure.figsize\"] = [20.00, 10.50]\nplt.rcParams[\"figure.autolayout\"] = True\nim = plt.imread('..\/input\/random-images-dataset\/Wow (1).png') # insert local path of the image.\nfig, ax = plt.subplots(figsize=(20,6))\n\nax.imshow(plt.imread(\"..\/input\/tensorflow-great-barrier-reef\/train_images\/video_0\/40.jpg\"));\nnewax = fig.add_axes([0.3,0.3,0.6,0.7], anchor='NE', zorder=1)\nnewax.imshow(im);\n\nnewax.axis('off')\nplt.show();","b4111d7b":"img_sizes = []\nfor i in df_train_v2[\"image_path\"]:\n    img_sizes.append(plt.imread(i).shape)\n\nnp.unique(img_sizes)","9c4c9d20":"# lets check total number of images with annotations\nlen(df_train_v2)","ed8600ff":"plt.figure(figsize = (12,15))\n\n\nplt.figure(figsize = (12,15))\n\n\nplt.rcParams[\"figure.figsize\"] = [20.00, 10.50]\nplt.rcParams[\"figure.autolayout\"] = True\nim = plt.imread('..\/input\/random-images-dataset\/Wow.png') # insert local path of the image.\nfig, ax = plt.subplots(figsize=(20,6))\n\nax.imshow(plt.imread(\".\/annot_img\/40.jpg\"))\n\nnewax = fig.add_axes([0.26,0.2,0.6,0.6], anchor='NE', zorder=1)\nnewax.imshow(im)\n\nnewax.axis('off')\nplt.show()","96b8a3d0":"count_bbox = []\nfor i in df[\"annotations\"]:\n    count_bbox.append(len(i))\n    \nfrom collections import defaultdict\n\n\nbbox_dict = defaultdict(int)\n\nfor val in count_bbox:\n    bbox_dict[val] += 1\n    \n\nnames = list(bbox_dict.keys())\nvalues = list(bbox_dict.values())\n\nN = len(list(bbox_dict.values()))\nmenMeans = list(bbox_dict.values())\nind = np.arange(N)\n\nplt.rcParams[\"figure.figsize\"] = [20.00, 10.50]\nplt.rcParams[\"figure.autolayout\"] = True\nim = plt.imread('..\/input\/random-images-dataset\/Number of bounding box VS Count of Bounding Boxlittle one.png') # insert local path of the image.\nfig, ax = plt.subplots(figsize=(20,6))\n\nax.bar(ind,menMeans,width=0.4)\nplt.xticks(np.arange(0, N, step=1))\nplt.title(\"Number of bounding box VS Count of Bounding Box\",fontsize=20)\n\nplt.xlabel('Number of bounding box', fontsize=18)\nplt.ylabel('Count', fontsize=16)\nfor index,data in enumerate(menMeans):\n    plt.text(x=index , y =data+1 , s=f\"{data}\" , fontdict=dict(fontsize=20))\nnewax = fig.add_axes([0.3,0.35,0.6,0.5], anchor='NE', zorder=1)\nnewax.imshow(im)\n\nnewax.axis('off')\n\nplt.show()","dba10889":"def he_hsv(img_demo):\n    img_hsv = cv2.cvtColor(img_demo, cv2.COLOR_RGB2HSV)\n\n    # Histogram equalisation on the V-channel\n    img_hsv[:, :, 2] = cv2.equalizeHist(img_hsv[:, :, 2])\n\n    # convert image back from HSV to RGB\n    image_hsv = cv2.cvtColor(img_hsv, cv2.COLOR_HSV2RGB)\n    \n    return image_hsv","cd9a7998":"def plot_img(img_dir,num_items,func,mode):\n    img_list = random.sample(os.listdir(img_dir), num_items)\n\n    for i in range(len(img_list)):\n        full_path = img_dir + '\/' + img_list[i]\n        img_temp1 = plt.imread(full_path)\n        img_temp_cv = cv2.imread(full_path)\n        plt.figure(figsize=(20,15))\n        plt.subplot(1,2,1)\n        plt.imshow(img_temp1);\n        plt.subplot(1,2,2)\n        if mode == 'plt':\n            plt.imshow(func(img_temp1));\n        elif mode == 'cv2':\n            plt.imshow(func(img_temp_cv));","61c9e61e":"vid_0_dir = \"..\/input\/tensorflow-great-barrier-reef\/train_images\/video_0\"\nnum_items1 = 4\nplot_img(vid_0_dir,num_items1,he_hsv,\"plt\")","f88e2b4e":"def RecoverHE(sceneRadiance):\n    for i in range(3):\n        sceneRadiance[:, :, i] =  cv2.equalizeHist(sceneRadiance[:, :, i])\n    return sceneRadiance\n\nvid_0_dir = \"..\/input\/tensorflow-great-barrier-reef\/train_images\/video_0\"\nnum_items1 = 4\nplot_img(vid_0_dir,num_items1,RecoverHE,\"cv2\")","23b21330":"def clahe_hsv(img):\n    hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n\n    h, s, v = hsv_img[:,:,0], hsv_img[:,:,1], hsv_img[:,:,2]\n    clahe = cv2.createCLAHE(clipLimit = 15.0, tileGridSize = (20,20))\n    v = clahe.apply(v)\n\n    hsv_img = np.dstack((h,s,v))\n\n    rgb = cv2.cvtColor(hsv_img, cv2.COLOR_HSV2RGB)\n    \n    return rgb\n\n\nvid_0_dir = \"..\/input\/tensorflow-great-barrier-reef\/train_images\/video_0\"\nnum_items1 = 4\nplot_img(vid_0_dir,num_items1,clahe_hsv,\"cv2\")","e50b392e":"def RecoverCLAHE(sceneRadiance):\n    clahe = cv2.createCLAHE(clipLimit=7, tileGridSize=(14, 14))\n    for i in range(3):\n\n        \n        sceneRadiance[:, :, i] = clahe.apply((sceneRadiance[:, :, i]))\n\n\n    return sceneRadiance\n\nvid_0_dir = \"..\/input\/tensorflow-great-barrier-reef\/train_images\/video_0\"\nnum_items1 = 4\nplot_img(vid_0_dir,num_items1,RecoverCLAHE,\"cv2\")","57202f40":"def gamma_enhancement(image,gamma):\n    R = 255.0\n    return (R * np.power(image.astype(np.uint32)\/R, gamma)).astype(np.uint8)\n\nplt.figure(figsize=(20,15))\nplt.subplot(2,2,1)\nplt.imshow(img_og);\nplt.subplot(2,2,2)\nplt.imshow(gamma_enhancement(img_9101,1\/0.6))\n\nplt.subplot(2,2,3)\nplt.imshow(img_og);\nplt.subplot(2,2,4)\nplt.imshow(gamma_enhancement(img_og,1\/0.6))\n","18b2b71e":"def RecoverGC(sceneRadiance):\n    sceneRadiance = sceneRadiance\/255.0\n    \n    for i in range(3):\n        sceneRadiance[:, :, i] =  np.power(sceneRadiance[:, :, i] \/ float(np.max(sceneRadiance[:, :, i])), 3.2)\n    sceneRadiance = np.clip(sceneRadiance*255, 0, 255)\n    sceneRadiance = np.uint8(sceneRadiance)\n    return sceneRadiance\n\nvid_0_dir = \"..\/input\/tensorflow-great-barrier-reef\/train_images\/video_0\"\nnum_items1 = 4\nplot_img(vid_0_dir,num_items1,RecoverGC,\"cv2\")","f9595982":"import numpy as np\n\ndef global_stretching(img_L,height, width):\n    I_min = np.min(img_L)\n    I_max = np.max(img_L)\n    I_mean = np.mean(img_L)\n\n    array_Global_histogram_stretching_L = np.zeros((height, width))\n    for i in range(0, height):\n        for j in range(0, width):\n            p_out = (img_L[i][j] - I_min) * ((1) \/ (I_max - I_min))\n            array_Global_histogram_stretching_L[i][j] = p_out\n\n    return array_Global_histogram_stretching_L\n\ndef stretching(img):\n    height = len(img)\n    width = len(img[0])\n    for k in range(0, 3):\n        Max_channel  = np.max(img[:,:,k])\n        Min_channel  = np.min(img[:,:,k])\n        for i in range(height):\n            for j in range(width):\n                img[i,j,k] = (img[i,j,k] - Min_channel) * (255 - 0) \/ (Max_channel - Min_channel)+ 0\n    return img\n\nfrom skimage.color import rgb2hsv,hsv2rgb\nimport numpy as np\n\n\n\ndef  HSVStretching(sceneRadiance):\n    height = len(sceneRadiance)\n    width = len(sceneRadiance[0])\n    img_hsv = rgb2hsv(sceneRadiance)\n    h, s, v = cv2.split(img_hsv)\n    img_s_stretching = global_stretching(s, height, width)\n\n    img_v_stretching = global_stretching(v, height, width)\n\n    labArray = np.zeros((height, width, 3), 'float64')\n    labArray[:, :, 0] = h\n    labArray[:, :, 1] = img_s_stretching\n    labArray[:, :, 2] = img_v_stretching\n    img_rgb = hsv2rgb(labArray) * 255\n\n    \n\n    return img_rgb\n\ndef sceneRadianceRGB(sceneRadiance):\n\n    sceneRadiance = np.clip(sceneRadiance, 0, 255)\n    sceneRadiance = np.uint8(sceneRadiance)\n\n    return sceneRadiance","9f46e3ab":"def RecoverICM(img1):\n    img = stretching(img1)\n    sceneRadiance = sceneRadianceRGB(img)\n    sceneRadiance = HSVStretching(sceneRadiance)\n    sceneRadiance = sceneRadianceRGB(sceneRadiance)\n    \n    return sceneRadiance\n\n\nvid_0_dir = \"..\/input\/tensorflow-great-barrier-reef\/train_images\/video_0\"\nnum_items1 = 4\nplot_img(vid_0_dir,num_items1,RecoverICM,\"cv2\")","cdcfb753":"def plot_img_tf(img_dir,num_items,func,mode):\n    img_list = random.sample(os.listdir(img_dir), num_items)\n    full_path = img_dir + '\/' + img_list[0]\n    img_temp_plt = plt.imread(full_path)\n    img_temp_cv = cv2.imread(full_path)\n    if mode==\"plt\":\n        \n        img_stack = np.hstack((img_temp_plt,func(img_temp_plt)))\n        plt.figure(figsize=(20,15))\n        plt.imshow(img_stack);\n        plt.title(\"Original Image VS Enhanced Image\",fontsize=25)\n        plt.axis(\"off\")\n        plt.show()\n    if mode==\"cv2\":\n        \n        img_stack = np.hstack((img_temp_cv,func(img_temp_cv)))\n        plt.figure(figsize=(20,15))\n        plt.imshow(img_stack);\n        plt.title(\"Original Image VS Enhanced Image\",fontsize=25)\n        plt.axis(\"off\")\n        plt.show()\n    \n    \n    for i in range(1, len(img_list)):\n        full_path = img_dir + '\/' + img_list[i]\n        img_temp_plt = plt.imread(full_path)\n        img_temp_cv = cv2.imread(full_path)\n        if mode==\"plt\":\n            img_stack = np.hstack((img_temp_plt,func(img_temp_plt)));\n            plt.figure(figsize=(20,15))\n            plt.imshow(img_stack);\n            plt.axis(\"off\")\n            plt.show()\n        if mode==\"cv2\":\n            img_stack = np.hstack((img_temp_cv,func(img_temp_cv)));\n            plt.figure(figsize=(20,15))\n            plt.imshow(img_stack);\n            plt.axis(\"off\")\n            plt.show()\n\nimg_dir = \"..\/input\/tensorflow-great-barrier-reef\/train_images\/video_0\"\nnum_items = 4\nplot_img_tf(img_dir,num_items,tfa.image.equalize,\"plt\")","7aa7fa7e":"(\n    ffmpeg.input('.\/clahe_img\/*.jpg', pattern_type='glob', framerate=25)\n    .output('img_movie.mp4')\n    .run()\n)","a7c20f93":"(\n    ffmpeg.input('.\/annot_img\/*.jpg', pattern_type='glob', framerate=25)\n    .output('annot_movie.mp4')\n    .run()\n)","047172c5":"Video(\".\/img_movie.mp4\")","4dd3a9c8":"Video(\".\/annot_movie.mp4\")","649a5de4":"# \ud83c\udfaf **Main Working Code:**\n> I am using **`tfa.image.equalize`** for the preprocessing. You can also add this like that in the data augmentation pipeline. Im passing this as a function in the **`plot_img_tf`** function.","6be80bf2":"# \ud83e\udde9 **HE code from the repo:**\n\n- below is the code from the repo\n- check out the Code from the repo [here](https:\/\/github.com\/wangyanckxx\/Single-Underwater-Image-Enhancement-and-Color-Restoration\/tree\/master\/Underwater%20Image%20Enhancement\/HE )\n- It is basically allpying Histogram Equalizers in each channel","ed172c8c":"# \ud83e\udd80 **Lets see what Mr: krabe has to say about the annotations:** ","9a60343b":"# Bounding box analysis in each video:","59e9a418":"> <font size=\"4\" face=\"verdana\">\n            <b>\"professor\" Squidward:<\/b>  Well seem like there are more empty bounding boxes, the fill ones.\n        <\/font>\n        \n<center><div class=\"alert alert-block alert-warning\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    There are alot of things to discover from this dataset. So keep a eye on the EDA part Im surely going to update that.\n<\/div><\/center>","0834ec54":"# \ud83d\udd2e **CLAHE: without repo code:**\n\n> Contrast Limited AHE (CLAHE) differs from adaptive histogram equalization in its contrast limiting. In the case of CLAHE, the contrast limiting procedure is applied to each neighborhood from which a transformation function is derived. CLAHE was developed to prevent the over amplification of noise that adaptive histogram equalization can give rise to.\n\n> We apply CLAHE to color images, where usually it is applied on the luminance channel and the results after equalizing only the luminance channel of an HSV image are much better than equalizing all the channels of the BGR image. \n\n<div class=\"alert alert-block alert-info\">\n<b>Note:<\/b> Each pixel in a image has brightness level, called luminance. This value is between 0 to 1, where 0 means complete darkness (black), and 1 is brightest (white)\n<\/div>\n\n","7f8a119e":"# \ud83d\udccd **ICM: with repo code**","101831a4":"# \ud83d\udcda **Importing Libraries:**","457f093b":"# \ud83e\uddfd **Lets see that SpongeBob thinks about the images:**","2a020e3e":"# \ud83d\udcea **80% of the data has not objects:** ","262d7ece":"# \ud83d\udcca **How to perform histogram equalization?**\nHistogram Equalization is a method of contrast adjustment based on the image's histogram.\n\n> In this image the pixel values are are between 0-255, but we will not find any pixel values which are exactly 0 or 255, it does not have any image which is pure white or pure black.If we apply the histogram equalization then it will reduce the color depth.Currently the minimum pixel value is 52 and the highest is 255.\nAfter you apply histogram equalization, you will find the the min pixel value now got transformed to zero and the max got converted to 255. So, notice again how the min and max values are equalized between 0 and 255, we also see less shade of gray.(view fig1 to 2)\n\n<img src=\"https:\/\/i.imgur.com\/mWTXqZ4.png\" style=\"width:500px;height:250px;\">\n<center><p style=\"padding-left:380px;color:red\">Fig1:before applying histogram equalization<\/p><\/center>\n\n<img src=\"https:\/\/i.imgur.com\/5diUO7c.png\" style=\"width:500px;height:250px;\">\n<center><p style=\"padding-left:380px;color:red\">Fig2: After applying histogram equalization<\/p><\/center>\n\n> Again, now we have a image on the left hand side and it's coresponding histogram(in red) on the right the black line is nothing but the cumulative of the pixel values.So, after we apply the Histogram equalizer that cumulative changes to linear step function. Notice that we don't literally flatten out the histogram we only just focus on the cumulative linear. And the real mathematics behind the Histogram equalization is just like that.(view fig 3 to 5)\n<img src=\"https:\/\/i.imgur.com\/uIBnzbu.png\" style=\"width:500px;height:250px;\">\n\n<center><p style=\"padding-left:380px;color:red\">Fig3:before applying histogram equalization<\/p><\/center>\n\n<img src=\"https:\/\/i.imgur.com\/fuIGrCi.png\" style=\"width:500px;height:250px;\">\n\n<center><p style=\"padding-left:380px;color:red\">Fig4: After applying histogram equalization<\/p><\/center>\n\n<img src=\"https:\/\/i.imgur.com\/9q3NVIg.png\" style=\"width:500px;height:250px;\">\n<center><p style=\"padding-left:380px;color:red\">Fig5:Main difference <\/p><\/center>\n\nWe mainly use histogram equalization when we need to increase the contrast of the image.","cb9b7bc1":"# \ud83e\ude80 **GC: without repo code:**\n\n> Different camera or video recorder devices do not correctly capture luminance. (they are not linear) Different display devices (monitor, phone screen, TV) do not display luminance correctly neither. So, one needs to correct them, therefore the gamma correction function. Gamma correction function is a function that maps luminance levels to compensate the non-linear luminance effect of display devices (or sync it to human perceptive bias on brightness). For example check out the below image,\n\n\n<p align=\"center\">\n    <img width=\"600\" src=\"https:\/\/i.imgur.com\/2GLA50Q.png\">\n<\/p>\n\n\n<div class=\"alert alert-block alert-info\">\n<b>Source:<\/b> https:\/\/medium.com\/giscle\/how-we-utilized-gamma-correction-for-increasing-our-training-data-47c16a040adc\n<br>\n<b>Check out more about GC, here :<\/b> https:\/\/en.wikipedia.org\/wiki\/Gamma_correction\n<\/div>\n","45ccf781":"# \ud83d\udcfa **Lets checkout How it looks like as a video:**","3bdbd849":"### Future work:\n- Add plots on distribution of height and width of the bbox provided\n- Add plots on distribution of area of the bbox provided","58ddd87f":"> <font size=\"4\" face=\"verdana\">\n            From the image, it seems like all the rocks, plants, and other underwater objects are on the ground and sunlight falling upon them, not inside a sea.\n        <\/font>","7fec7c34":"### Lets check we have images with same size or not:","2c3b7f27":"# \ud83e\udd4f **CLAHE: with repo code:**","09f0d392":"# What is Sequence and its properties:","6e863a93":"<h1 style=\"text-align: center; font-family: Verdana; font-size: 32px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; font-variant: small-caps; letter-spacing: 3px; color: #468282; background-color: #ffffff;\">Tensorflow - Help Protect the Great Barrier Reef<\/h1>\n<h2 style=\"text-align: center; font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: underline; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">Learning to Sea: Underwater img enhancement<\/h2>\n\n\n<p align=\"center\">\n    <img src=\"https:\/\/i.imgur.com\/tTOe7cV.png\">\n<\/p>\n\n\n# \ud83d\udccc**Introduction:**\n<!-- <h1 style=\"font-family: times-new-roman\">\ud83d\udcccIntroduction<\/h1> -->\n> <p style=\"font-family: times-new-roman\">Seems like <b>Patrick<\/b> the Star is lost, and <b>Spongebob, Mr. Krabs, Squidward, and the others<\/b> need our help to find him. To help them out <b>Kaggle<\/b> is also trying their best and has put together a very informative dataset the dataset is created based on the places where Patrick most certainly be going, and we need to take the help of AI and computer vision and perform Object detection to find him from that dataset.  We still don't know whether Patrick is lost or has left the city because of the last night's argument between him and Spongebob. Well, this will stay as a mystery until we find Patrik. Spongebob, Mr. Krabs, Squidward, and Sandy are all very concerned and doing their best to find Patrick. So let's do our best too, and get Patrick back to his friends.<\/p>\n\n# \ud83d\udcd1 **About the Notebook:**\n> <p style=\"font-family: times-new-roman\">As per the Discussion thread I posted earlier, I said that I will post the results. So, to do that Im creating this NB. The discussion thred is listed just right below. After playing with data for a bit it was clear that the images were extreamly hezzy because of the thickness of the water, and most of the data are colored in greenish blue. After seeing a Disussion thread on this, I started searching some techniques for underwater image enhancement. After sometime, I stumbled upon this repo which talks about some techiques for Underwater color restoration and color enhancement, there were total 8 methods for image color Enhancement.Don't know how important Color Restoration would be [please let me know if you think color restoration is also important] but Image Enhancement would be very much required. The mentioned techniques are,<\/p>\n\n> - [8 Methods on Underwater Image Enhancement and Color Restoration, With Code](https:\/\/www.kaggle.com\/c\/tensorflow-great-barrier-reef\/discussion\/291063)\n\n> ## \u2b55 **Underwater Image Enhancement**\n> > - **CLAHE**: Contrast limited adaptive histogram equalization (1994)\n> > - **Fusion-Matlab**: Enhancing underwater images and videos by fusion (2012)\n> > - **GC**: Gamma Correction\n> > - **HE**: Image enhancement by histogram transformation (2011)\n> > - **ICM**: Underwater Image Enhancement Using an Integrated Colour Model (2007)\n> > - **UCM**: Enhancing the low-quality images using Unsupervised Colour Correction Method (2010)\n> > - **RayleighDistribution**: Underwater image quality enhancement through composition of dual-intensity images and Rayleigh-stretching (2014)\n> > - **RGHS**: Shallow-Water Image Enhancement Using Relative Global Histogram Stretching Based on Adaptive Parameter Acquisition (2018)\n\n\n\n\n> > <p style=\"font-family: times-new-roman\"> From these 8 methods I was able to perform, 4 and 3 of them are implemented in two different ways, one is done using the repo's code and the other is implemetnted by me. Results are good but not too good. Because there are papers like <b>sea-thru<\/b>, <b>FUnIE-GAN<\/b> and <b>Water-GAN<\/b> which yields much better results. Check about the GANs <a style=\"font-family: times-new-roman\" href=\"https:\/\/github.com\/xahidbuffon\/FUnIE-GAN\">here<\/a><\/p>\n\n> ## \ud83d\udc40 **Sea-thru**: \n> > <p style=\"font-family: times-new-roman\">This is a great paper, not only because of its good results, also the way the problem is tackled. Most of the time people ends up training large model with a huge pile of data for days to get a decent result. but this paper tries to break down the problem to its core components and solves it. This paper has a extensive use of depth map. Itried to find the trained wights but it seems like its not available. the main concept this paper goes like this,<\/p>\n\n> > - <p style=\"font-family: times-new-roman\"><b><i>As a result of their research, Akkaynak and Treibitz contend that wavelengths of observed colors depend on speed, distance, and the original color of objects. As with the speed of light through water, the source hue is constant. In this case, if we know the color of an object and its distance from us, we can reverse-engineer the formula that produces the observed color.<\/i><\/b><\/p>\n\n\n> > To know more check out these two links,\n> > - Article: [Sea-Thru: Removing Water from Underwater Images](https:\/\/towardsdatascience.com\/sea-thru-removing-water-from-underwater-images-935288e13f7d)\n> >- Paper: [Sea-Thru: A Method for Removing Water From Underwater Images](https:\/\/openaccess.thecvf.com\/content_CVPR_2019\/html\/Akkaynak_Sea-Thru_A_Method_for_Removing_Water_From_Underwater_Images_CVPR_2019_paper.html)\n> > - Github: https:\/\/github.com\/hainh\/sea-thru\n> > - Dataset: http:\/\/csms.haifa.ac.il\/profiles\/tTreibitz\/datasets\/sea_thru\/index.html\n\n> > <p align=\"center\">\n<img width = \"600\" src=\"https:\/\/i.postimg.cc\/6qpZCTFK\/sea-thru3.jpg\">\n<\/p>\n\n\n\n> ## \ud83e\uddea **Importance of image Pre-processing:**\n> > <p style=\"font-family: times-new-roman\"> Image preprocessing in one of the most important part of a Model building pipeline. It helps to improve the quality of your image, we apply different kind of filters and methods to enhance sertain aspect of the image, may be the given image is very blurry, or may be the image in very noisy, or maybe the image size across the data is not similar, or may be the labeled data is not accurate etc. In these cases we need to use Image preprocessing to clean the data. In some cases it also helps by decreasing model training time and increasing model inference speed. One of the incredible image preprocessing that I saw, was the hair removal in the Melanoma Classification comp. <\/p>\n\n> ## \ud83c\udf81 **Main aim of this Notebook:**\n> > - <p style=\"font-family: times-new-roman\"> I tried doing EDA for the provided dataset. I plan to dig much dipper and understand the data in a much better way.<\/p>\n> > - <p style=\"font-family: times-new-roman\"> I tried different approaches to get a decent preprocessed image as mentioned above. The best preprocessed Images this Notebook offers are shown below. We will go through all the methods(except few) and then look into the working code. <b>This is certainly not my work, it is taken from <b><a href=\"https:\/\/www.kaggle.com\/ipythonx\">@M.Innat's<\/a><\/b> comment from<\/p><\/b>\n\n> > > <li> <a href=\"https:\/\/www.kaggle.com\/c\/tensorflow-great-barrier-reef\/discussion\/290584#1599835\">Fast underwater image enhancement for Improved Visual Perception [github+Paper]<\/a><\/li>\n\n> > > <p align=\"center\">\n<img src=\"https:\/\/i.imgur.com\/wc9rY2J.png\">\n<\/p>\n\n> > > <p align=\"center\">\n<img src=\"https:\/\/i.imgur.com\/ho1D0Ye.png\">\n<\/p>\n\n> > > <p align=\"center\">\n<img src=\"https:\/\/i.imgur.com\/pwBNYJr.png\">\n<\/p>\n\n","630dd0cf":"# \ud83d\udd8c **GC: with repo code:**","67f1aebf":"# \ud83c\udfa8 **Some Data Preparation:**","e8365a39":"# \ud83d\udd0e **Lets checkout the training data:**","96d97ab3":"The point I want to make over here is that, In the pipeline we need to add more images with no lables, because there are many images in the private test case which has zero objects associated with it. But we cant actually infuse some unlabeled\/no object image into the pipeline coz there are 80% of the total dataset present in that unlabeled\/no object iamge categoury. So to do that we first need to choose a percentage in which we want to make the combiniation of both labeled and unlabeled image. So, say we deceide to take 6k img where 5k has object and 1k has not. Now the problem comes down to how to choose these 1k images. I would say choose few sequence and keep all of the unlabeled\/no object images in the dataset and do that untill you get near 1k images. You might need to hand pich for each fold. I was thinking about using the fold CSVs generated from julian's NB and instade of filtering out all the unlabeled\/no object images, keep some of them and train the whole network using that."}}