{"cell_type":{"bea87931":"code","6aa7f6cd":"code","e752a758":"code","91cb7e0e":"code","0d4018ca":"code","4f3a8b89":"code","64e2223e":"code","f49f2a47":"code","55f6272d":"code","3f9bbe6c":"code","ca673967":"code","a7bca1a1":"code","b41e568d":"code","59689662":"code","d05ac33d":"code","e6e0469d":"code","ca53b68a":"code","c4ef26c5":"code","db30d0db":"code","67abbae0":"code","c4bb1a55":"code","d22fd3a0":"code","749a0f50":"code","e6fc9824":"code","1f263d2f":"code","cc34327e":"code","893ebd34":"code","458f8754":"code","24e73481":"code","cb8132b3":"code","fd2f949c":"code","3a68d928":"code","ac1dc7e8":"code","1c1fffa7":"code","0c128695":"code","607d361c":"code","6caa8ead":"code","07f277b3":"code","0b99a6e9":"code","90098328":"code","2d826c52":"code","ddcd57f5":"code","ba92ec87":"code","73ce4f1d":"markdown","4bb1bc25":"markdown","55c9a76b":"markdown","d1f82d85":"markdown","477dccba":"markdown","67aef548":"markdown","b3548d81":"markdown","597120de":"markdown","e8816d4d":"markdown","07d9b210":"markdown","8691e8bd":"markdown","dac9e3ef":"markdown","a22c8c65":"markdown","21774254":"markdown","fc308933":"markdown","4b11c638":"markdown","d1f69156":"markdown","217a1076":"markdown","7ac266b5":"markdown"},"source":{"bea87931":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6aa7f6cd":"#imports\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LinearRegression\nimport sklearn","e752a758":"df =  pd.read_csv('\/kaggle\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv')","91cb7e0e":"df.head()","0d4018ca":"df.describe()","4f3a8b89":"df.isnull().sum()","64e2223e":"df.drop(['Serial No.'],axis=1,inplace=True)\ndf.head()","f49f2a47":"df = df.rename(columns={'GRE Score': 'GRE', 'TOEFL Score': 'TOEFL','University Rating':'UniversityRating','Chance of Admit':'Chance'})\ndf.head()","55f6272d":"df.rename(columns={ df.columns[4]: \"LOR\" }, inplace = True)","3f9bbe6c":"df.rename(columns={ df.columns[-1]: \"Chance\" }, inplace = True)\ndf.head()","ca673967":"df.corr()","a7bca1a1":"df.corr()['Chance'].sort_values()","b41e568d":"style.use('fivethirtyeight')\nsns.heatmap(df.corr())","59689662":"df_pred = df.sample(frac=1).reset_index(drop=True)","d05ac33d":"scaler = StandardScaler()\nnumeric_features = ['GRE','TOEFL','UniversityRating','SOP','LOR','CGPA']\n\ndf_pred[numeric_features]  = scaler.fit_transform(df_pred[numeric_features])\ndf_pred.head()","e6e0469d":"X_train, X_test, y_train, y_test = train_test_split(df_pred.drop('Chance',axis=1), df_pred['Chance'], test_size = .2, random_state=10)","ca53b68a":"bins = [df.Chance.min(), 0.75, 1]\nlabels = [0,1]\ndf_cat = df.copy()\ndf_cat['Admission'] = pd.cut(df['Chance'],bins=bins,labels=labels)\ndf_cat.head()","c4ef26c5":"df_cat.Admission.value_counts(normalize=True)","db30d0db":"\ng = sns.FacetGrid(df_cat, hue=\"Admission\")\nplt.figure(figsize=(15,20))\ng.map(sns.scatterplot, \"GRE\", \"TOEFL\", alpha=.7)\ng.add_legend()","67abbae0":"plt.figure(figsize=(8,6))\nsns.scatterplot(data=df_cat,x=\"GRE\", y=\"CGPA\",hue='Admission', alpha=.7)","c4bb1a55":"plt.figure(figsize=(8,6))\nsns.scatterplot(data=df_cat,x=\"GRE\", y=\"TOEFL\",hue='Admission', alpha=.7)","d22fd3a0":"df.Research.value_counts()","749a0f50":"labels=['1','0']\nplt.figure(figsize=(6,6))\nplt.title('Resarch Experience')\nplt.pie(df.Research.value_counts(), labels=labels, autopct='%1.1f%%', shadow=True);","e6fc9824":"g = sns.FacetGrid(df_cat, col=\"Research\", hue=\"Admission\",height=5,aspect=1)\ng.map(sns.scatterplot, \"GRE\", \"CGPA\", alpha=.7)\ng.add_legend()","1f263d2f":"\ndf_risky = df_cat[(df_cat.GRE > 300) & (df_cat.GRE < 320)& (df_cat.CGPA > 8.2) & (df_cat.CGPA<=9)]","cc34327e":"df_risky.head()","893ebd34":"g = sns.FacetGrid(df_risky, col=\"Research\", hue=\"Admission\",height=5,aspect=1)\ng.map(sns.scatterplot, \"GRE\", \"CGPA\", alpha=.7)\ng.add_legend()","458f8754":"df_risky['Admission'].value_counts(normalize=True).plot(kind='pie')","24e73481":"df_cat.groupby('Research')['Admission'].value_counts(normalize=True).unstack('Research')","cb8132b3":"ax = df_cat.groupby('Research')['Admission'].value_counts(normalize=True).unstack('Research').plot(kind='bar',figsize=(12,8),rot=0)\nax.legend(['Not Admitted',' Admitted'])\nax.set_xlabel('Research');\nax.set_ylabel('Percentages');","fd2f949c":"ax = df_risky.groupby('Research')['Admission'].value_counts(normalize=True).unstack('Admission').plot(kind='bar',figsize=(12,8),rot=0)\nax.legend(['Not Admitted',' Admitted'])\nax.set_xlabel('Research');\nax.set_ylabel('Percentages');","3a68d928":"def get_rmse(model,X_test,y_test):\n    mse = sklearn.metrics.mean_squared_error(y_test, model.predict(X_test));\n    return np.sqrt(mse)","ac1dc7e8":"import sklearn\nfrom sklearn.model_selection import GridSearchCV\n\n","1c1fffa7":"def get_best_model_and_accuracy(model, params, X, y):\n    grid = GridSearchCV(model, params, error_score=0.)\n    grid.fit(X, y) # fit the model and parameters\n    # our classical metric for performance\n    print (\"Best Accuracy: {}\".format(grid.best_score_))\n    # the best parameters that caused the best accuracy\n    print (\"Best Parameters: {}\".format(grid.best_params_))\n    # the average time it took a model to fit to the data (in seconds)\n    print('RMSE on test set: {}'.format(get_rmse(grid,X_test,y_test)))\n    return grid.best_params_\n\n","0c128695":"from sklearn.linear_model import ElasticNet\n\nelastic_net = ElasticNet()\nelastic_net_params = {'alpha':[0.25,0.5,1], 'l1_ratio':[0.25,0.5,1]}\n\n","607d361c":"get_best_model_and_accuracy(elastic_net, elastic_net_params, X_train, y_train);","6caa8ead":"linreg = LinearRegression()\nlinreg_params = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\nlinreg_best_params = get_best_model_and_accuracy(linreg, linreg_params, X_train, y_train)","07f277b3":"import keras\nimport tensorflow as tf\nfrom keras import layers\nfrom keras.layers import Input\nfrom keras.layers.core import  Dense, Activation\nfrom keras.models import Model\nfrom keras.preprocessing import image\nfrom keras.utils import layer_utils\nfrom keras.utils import plot_model\nfrom keras.layers import Dropout\nfrom keras.regularizers import l2\nfrom keras.regularizers import l1\nnp.random.seed(10)\n","0b99a6e9":"def model_nn(input_shape):\n    \n\n    # Define the input placeholder as a tensor with shape input_shape\n    X_input = Input(input_shape)\n\n  \n    X = Dense(512)(X_input)\n    X = Activation('relu')(X)\n    X = Dropout(0.5,seed=10)(X)\n    X = Dense(256)(X)\n    X = Activation('relu')(X)\n    X = Dropout(0.25,seed=10)(X)\n    X = Dense(16)(X)\n    X = Activation('relu')(X)\n    X = Dropout(0.25)(X)\n    X = Dense(1, activation='relu')(X)\n\n    # Create model. \n    model = Model(inputs = X_input, outputs = X, name='nnModel')\n\n    return model","90098328":"nnModel = model_nn(X_train.shape)\nnnModel.compile(optimizer=\"adam\",loss=\"mse\",metrics=['mean_squared_error'])\nnnModel.fit(x = X_train, y = y_train, epochs = 250, batch_size = 32)","2d826c52":"preds = nnModel.evaluate(x= X_test, y=y_test)\n#print()\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Rmse on test set = \" , np.sqrt(preds[1]))","ddcd57f5":"regressor = LinearRegression(linreg_best_params)\nregressor.fit(X_train,y_train);","ba92ec87":"plt.scatter(X_test.index.values, y_test, color = 'red')\nplt.scatter(X_test.index.values, regressor.predict(X_test), color = 'blue')\nplt.title('True values(Red) vs Prediction(Blue) (Test set) Linear Regression')\nplt.xlabel('Index')\nplt.ylabel('Chance')\nplt.show()","73ce4f1d":"We'll construct a new column as Admission where students with chance of admission higher than 0.75 have a value of 1 and students with a chance of admission lower than 0.75 have a value of 0.","4bb1bc25":"Get ready for prediction!","55c9a76b":"Finally, let's visualize our prediction from linear regression vs true values","d1f82d85":"Although rmse valus are very close, we still did not beat linear regression model with a neural network.","477dccba":"Although, we have students with lower GPA who have a chance of admission higher than compared to students with no research experience, the distinction I think is not clear from this graph.It seems like students with GRE lower than 300 have low chance of admission and GRE higher than 320 has high chance regardless of their research experience . Thus let's concentrate on this part of the graph.","67aef548":"We have selected the students packed in the middle spectrum in our df_risky dataframe, thus addmission rate is lower for this group.","b3548d81":"We know from the correlation table that GPA and GRE score is important for admission chance. Although there are some exceptions, students with higher GPA also did well on GRE. Let's look at the relationship between GRE and TOEFL.","597120de":"First let's look at the distribution of admission among this group","e8816d4d":"First lets look at the role of research experience in the whole dataset","07d9b210":"Admission ratio for those who have research experience is 0.26 whereas admission ratio for those that don't have any research experience is 0.13. So if any undergrad student with relatively low GPA is reading this, convince your professors to participate in their studies!","8691e8bd":"There is a considerable improvement with linear regression. Sometimes simplest model is the one that is more appropriate\nfor the data in hand.","dac9e3ef":"Shuffling the data","a22c8c65":"We have no null values in our dataset.","21774254":"From this plot, it looks like research experience is important. But keep in mind that student with high GPAs and GRE scores more likely to have also research experience compared to student with low GPAs. To really gauge the effect of research, let's plot the same thing for df_risky.","fc308933":"Our dataset after categorization looks balanced, that is nice, it is time to do some analysis!","4b11c638":"As expected GPA,GRE and TOEFL score looks very correlated to our target variable. I always thought that research experience is very beneficial for graduate admission but it is the least correlated variable in the dataset.","d1f69156":"Let's try a Neural Network and see if we can improve rmse further","217a1076":"Let's look at the correlation of our features with the target feature 'Chance'","7ac266b5":"Relationship bw GRE and TOEFL looks similar to relationship bw GRE and TOEFL. I was curious about the role of resarch experience to get in the grad school while doing my bachelor. Let's dive into this."}}