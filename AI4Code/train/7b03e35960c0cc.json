{"cell_type":{"e514166d":"code","e6fb2285":"code","6dc74c76":"code","8d48a66b":"code","e91d76c0":"code","c028d750":"code","8ace1242":"code","a1916aa1":"code","c989d0fb":"code","e7576d6a":"code","4bc049ac":"code","c5a81346":"code","651bb6e5":"code","1d56b88e":"code","dfb571f0":"code","24a5fbc3":"code","04c57649":"markdown","32dd8f73":"markdown","3fab15e9":"markdown","1f2a9d18":"markdown","45e23f91":"markdown","376f98f1":"markdown","ce707c37":"markdown","5c215f32":"markdown","7a6bdcad":"markdown","b7324910":"markdown","d9c0eed3":"markdown"},"source":{"e514166d":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom pathlib import Path\nfrom IPython.display import display\nfrom datetime import datetime\nfrom pandas import DataFrame\nfrom typing import List, NamedTuple, Tuple\n\n# allow plots to appear directly in the notebook\n%matplotlib \n\n# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n","e6fb2285":"pd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 50)\npd.set_option('display.width', 1000)\n\nROOT_DIR = Path(\"\/kaggle\/input\/bike-sharing-demand\")\nTRAIN_DATA_PATH = ROOT_DIR \/ \"train.csv\"\nTEST_DATA_PATH = ROOT_DIR \/ \"test.csv\"","6dc74c76":"def load(path: Path) -> DataFrame:\n    return pd.read_csv(path, parse_dates=True, index_col=\"datetime\")\n\ndef expanded_index_datetime_col(data: DataFrame) -> DataFrame:\n    data = data.copy()\n    data[\"hour\"] = data.index.hour\n    data[\"weekday\"] = data.index.weekday\n    data[\"month\"] = data.index.month\n    data[\"year\"] = data.index.year\n    return data\n\n\noriginal_train: DataFrame = load(TRAIN_DATA_PATH)\n#Not used in test data\noriginal_train = original_train.drop([\"casual\", \"registered\"], axis=1, errors=\"ignore\")\noriginal_train = expanded_index_datetime_col(original_train)\n\ndisplay(original_train.head())","8d48a66b":"sns.pairplot(original_train, x_vars=['weather', 'temp', 'atemp', 'humidity', 'windspeed'], y_vars='count'\n            ,size=5, aspect=1)","e91d76c0":"sns.pairplot(original_train[['weather', 'temp', 'atemp', 'humidity', 'windspeed']])","c028d750":"sns.heatmap(original_train[['weather', 'temp', 'atemp', 'humidity', 'windspeed']].corr(), annot=True)","8ace1242":"# temp\ub294 \ub4f1\ubd84\uc0b0\uc131\uc744 \ub9cc\uc871\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\nsns.residplot(x=original_train['temp'], y=original_train['count'], lowess=True)","a1916aa1":"# weather\ub294 \ub4f1\ubd84\uc0b0\uc131\uc744 \ub9cc\uc871\ud569\ub2c8\ub2e4.\nsns.residplot(x=original_train['weather'], y=original_train['count'], lowess=True)","c989d0fb":"x = original_train[['temp']]\ny = original_train[[\"count\"]]\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1)\n\nmodel = LinearRegression()\nresult = model.fit(x_train, y_train)\ny_pred = model.predict(x_test)\n\nsns.distplot((y_test-y_pred),bins=100, color = 'gray')","e7576d6a":"def calculate_r2(model, x, y):\n    return model.score(x, y)\n    \ndef calculate_adjust_r2(model, x, y):\n    r2 = model.score(x, y)\n    N = len(x)\n    P = len(x.columns)\n    return 1-(1-r2)*(N-1)\/(N-P-1)\n\ndef calculate_rmse(model, x, y):\n    pred = model.predict(x)\n    return np.sqrt(metrics.mean_squared_error(y, pred))","4bc049ac":"print('R^2 : {:.3f}'.format(calculate_r2(model, x_test, y_test)))\nprint('Adjust R^2 : {:.3f}'.format(calculate_adjust_r2(model, x_test, y_test)))\nprint('RMSE : {:.2f}'.format(calculate_rmse(model, x_test, y_test)))","c5a81346":"def prepare(df: DataFrame) -> DataFrame:\n    df = df.copy()\n    df = replaced_with_onehot_cols(df, col_names=[\"season\", \"holiday\", \"workingday\", \"weather\", \"weekday\", \"month\", \"year\"])\n    df = df.drop(['atemp', 'windspeed'], axis=1)\n    \n    \n    return df\n\ndef replaced_with_onehot_cols(data: DataFrame, col_names: List[str]) -> DataFrame:\n    data = data.copy()\n    \n    for col_name in col_names:\n        one_hot = pd.get_dummies(data[col_name], prefix=col_name)\n        data = data.join(one_hot)\n        \n        # Original column is not needed anymore\n        del data[col_name]\n    return data\n\ntrain_vals: DataFrame = prepare(original_train)","651bb6e5":"x = train_vals.drop(\"count\", axis=1)\ny = train_vals[[\"count\"]]\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1)\n\nmodel = LinearRegression()\n\nresult = model.fit(x_train, y_train)\n\nprint('R^2 : {:.3f}'.format(calculate_r2(model, x_test, y_test)))\nprint('Adjust R^2 : {:.3f}'.format(calculate_adjust_r2(model, x_test, y_test)))\nprint('RMSE : {:.2f}'.format(calculate_rmse(model, x_test, y_test)))","1d56b88e":"from sklearn.linear_model import Lasso\n\nmodel = Lasso(alpha=0.1, normalize=True)\n\nmodel.fit(x_train, y_train)\n\nprint('R^2 : {:.3f}'.format(calculate_r2(model, x_test, y_test)))\nprint('Adjust R^2 : {:.3f}'.format(calculate_adjust_r2(model, x_test, y_test)))\nprint('RMSE : {:.2f}'.format(calculate_rmse(model, x_test, y_test)))","dfb571f0":"from sklearn.linear_model import Ridge\n\nmodel = Ridge(alpha=0.1, normalize=True)\n\nmodel.fit(x_train, y_train)\n\nprint('R^2 : {:.3f}'.format(calculate_r2(model, x_test, y_test)))\nprint('Adjust R^2 : {:.3f}'.format(calculate_adjust_r2(model, x_test, y_test)))\nprint('RMSE : {:.2f}'.format(calculate_rmse(model, x_test, y_test)))","24a5fbc3":"from sklearn.linear_model import ElasticNet\n\nmodel = ElasticNet(alpha=0.1, l1_ratio=0.1)\n\nmodel.fit(x_train, y_train)\n\nprint('R^2 : {:.3f}'.format(calculate_r2(model, x_test, y_test)))\nprint('Adjust R^2 : {:.3f}'.format(calculate_adjust_r2(model, x_test, y_test)))\nprint('RMSE : {:.2f}'.format(calculate_rmse(model, x_test, y_test)))","04c57649":"# ** Ridge Regression**\n\nRidge\ub294 Lasso\uc640 \ube44\uc2b7\ud558\uac8c Linear regression\uc758 cost function\uc5d0 L2-norm penalty\ub97c \uc8fc\ub294 \ubc29\uc2dd\uc73c\ub85c cost \uac12\uc744 \uacc4\uc0b0\ud558\uc5ec MSE\uc640 penalty\uac00 \ucd5c\uc18c\uac00 \ub418\ub3c4\ub85d \ud559\uc2b5\ud558\ub294 \ubaa9\uc801\uc744 \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n\n### <u> Cost Function <\/u> \n- Cost function for MSE: $J_{\\theta} = \\frac{1}{n} \\sum_{i = 1}^{n}(Y - \\hat{Y})^2$\n- Cost function for Lasso: $MSE + penalty = \\frac{1}{n} \\sum_{i = 1}^{n}(Y - \\hat{Y})^2 + \\alpha \\sum_{j = 0}^{p}(w_j)^2$\n- \uc5ec\uae30\uc11c m\uc740 feature\uc758 \uac1c\uc218\uac00 \ub418\uace0,  $\\alpha$ \ub294 \ud328\ub110\ud2f0\uc758 \ub2c8\uacfc\ub97c \uc870\uc808\ud574\uc8fc\ub294 \ud30c\ub77c\ubbf8\ud130 \uc774\ub2e4. $\\alpha$ \uac00 \uc791\uc544\uc9c0\uba74 \uc120\ud615\ud68c\uadc0\uc640 \uac19\uc740 \ubaa8\ud615\uc774 \ub418\uace0, $\\alpha$ \uac00 \ucee4\uc9c0\uba74 \ud328\ub110\ud2f0\uc758 \uc601\ud5a5\ub825\uc774 \ucee4\uc9d1\ub2c8\ub2e4. \n\n### \ud2b9\uc131\n- Ridge\uc5d0\uc11c $\\alpha$ \ub294 \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130\ub85c \uc0ac\uc6a9\uc790\uac00 \uc9c1\uc811 \uc9c0\uc815\uc744 \ud574\uc918\uc57c \ud569\ub2c8\ub2e4. \n- Penalty\ub97c \ud1b5\ud574 \ubaa8\ub378\uc758 \uacfc\uc801\ud569\uc744 \ub9c9\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \n- $w_j$\uac00 0\uc5d0 \uac00\uae4c\uc6b8 \uacbd\uc6b0 \ud574\ub2f9 feature\ub294 \ubaa8\ub378\uc5d0 \uc601\ud5a5\uc744 \uc8fc\uc9c0 \uc54a\ub294 \uac83\uc744 \uc54e\uc73c\ub85c\uc368 (\uc5b4\ub5a4 feature\uac00 \ub354 \ud070 \uc601\ud5a5\uc744 \ubbf8\uce60 \uc218 \uc788\ub294\uc9c0) \ubaa8\ub378\uc5d0 \ub300\ud55c \ud574\uc11d\ub825\uc774 \uc88b\uc544\uc9d1\ub2c8\ub2e4.\n- Lasso\uc640\uc758 \ub2e4\ub978\uc810\uc740 \uac00\uc911\uce58 $w_j$\uac00 0\uc5d0 \uac00\uae4c\uc6cc\uc9c8 \ubfd0 0\uc774 \ub418\uc9c0\ub294 \uc54a\uc2b5\ub2c8\ub2e4. \n- \ub9ce\uc740 feature\ub4e4 \uc911 \uc77c\ubd80\ubd84\ub9cc \uc911\uc694\ud558\ub2e4\uba74 Lasso\uac00 \ub354 \ub192\uc740 \uc131\ub2a5\uc744 \ub0b4\uace0, \uc804\uccb4\uc801\uc73c\ub85c \ube44\uc2b7\ud558\ub2e4\uba74 Ridge\uac00 \ub354 \ub192\uc740 \uc131\ub2a5\uc744 \ub0c5\ub2c8\ub2e4.","32dd8f73":"# ** Elastic Net**\n\nLlastic Net\uc740 Ridge\uc640 Lasso\uc758 penalty\ub97c \ubaa8\ub450 \uac00\uc9c0\ub294 regression model\uc785\ub2c8\ub2e4.\n\n### <u> Cost Function <\/u> \n- Cost function for MSE: $J_{\\theta} = \\frac{1}{n} \\sum_{i = 1}^{n}(Y - \\hat{Y})^2$\n- Cost function for ElasticNet: $MSE + penalty = \\frac{1}{n} \\sum_{i = 1}^{n}(Y - \\hat{Y})^2 + \\alpha \\sum_{j = 0}^{p}(w_j)^2 + \\alpha \\sum_{j = 0}^{p}|w_j|$\n- \uc5ec\uae30\uc11c m\uc740 feature\uc758 \uac1c\uc218\uac00 \ub418\uace0,  $\\alpha$ \ub294 \ud328\ub110\ud2f0\uc758 \ub2c8\uacfc\ub97c \uc870\uc808\ud574\uc8fc\ub294 \ud30c\ub77c\ubbf8\ud130 \uc774\ub2e4. $\\alpha$ \uac00 \uc791\uc544\uc9c0\uba74 \uc120\ud615\ud68c\uadc0\uc640 \uac19\uc740 \ubaa8\ud615\uc774 \ub418\uace0, $\\alpha$ \uac00 \ucee4\uc9c0\uba74 \ud328\ub110\ud2f0\uc758 \uc601\ud5a5\ub825\uc774 \ucee4\uc9d1\ub2c8\ub2e4. \n","3fab15e9":"## \uc120\ud615\uc131\n - \uc124\uba85 \ubcc0\uc218\uc640 \ubc18\uc751 \ubcc0\uc218 \uac04\uc758 \uad00\uacc4 \ubd84\ud3ec\uac00 \uc120\ud615\uc758 \uad00\uacc4\ub97c \uac00\uc9c4\ub2e4.\n - \uc544\ub798\uc758 pairplot\uc73c\ub85c \uc120\ud615\uc131\uc744 \uac00\uc9c0\ub294 feature\ub294 \uc874\uc7ac\ud558\uc9c0 \uc54a\uc558\ub2e4.","1f2a9d18":"\ub2e4\uc74c\uc740 x\uc758 feature\ub97c \uc5ec\ub7ec \uac1c\ub85c \uc124\uc815\ud558\uc5ec \uc608\uce21\ud558\ub294 \ubaa8\ub378\uc744 \ud14c\uc2a4\ud2b8 \ud569\ub2c8\ub2e4.","45e23f91":"## \uc815\uaddc\uc131\n - \uc794\ucc28\ub294 \uc815\uaddc\ubd84\ud3ec\ub97c \ub9cc\uc871\ud574\uc57c \ud569\ub2c8\ub2e4.\n - qq plot\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub370\uc774\ud130\uac00 \uc815\uaddc \ubd84\ud3ec\uc5d0\uc11c \ub098\uc628 \uac83\uc778\uc9c0 \uc720\ucd94 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc815\uaddc\ubd84\ud3ec\ub77c\uba74 \ud50c\ub86f\uc740 \uc0c1\ub2f9\ud788 \uc9c1\uc120\uc73c\ub85c \ub098\ud0c0\ub0a9\ub2c8\ub2e4. \uc624\ucc28\uc758 \uc815\uaddc\uc131 \ubd80\uc7ac\ub294 \uc9c1\uc120\uc758 \ud3b8\ucc28\ub85c \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n - \uc815\uaddc\uc131 \ud655\uc778\uc744 \uc704\ud574 \uac04\ub2e8\ud55c Linear regression model\uc744 \ub9cc\ub4e4\uc5b4 \ud655\uc778\ud574\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.","376f98f1":"# ** Lasso Regression**\n\nLasso\ub294 Linear regression\uc758 cost function\uc5d0 L1-norm penalty\ub97c \uc8fc\ub294 \ubc29\uc2dd\uc73c\ub85c cost \uac12\uc744 \uacc4\uc0b0\ud558\uc5ec MSE\uc640 penalty\uac00 \ucd5c\uc18c\uac00 \ub418\ub3c4\ub85d \ud559\uc2b5\ud558\ub294 \ubaa9\uc801\uc744 \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n\n### <u> Cost Function <\/u> \n- Cost function for MSE: $J_{\\theta} = \\frac{1}{n} \\sum_{i = 1}^{n}(Y - \\hat{Y})^2$\n- Cost function for Lasso: $MSE + penalty = \\frac{1}{n} \\sum_{i = 1}^{n}(Y - \\hat{Y})^2 + \\alpha \\sum_{j = 0}^{p}|w_j|$\n- \uc5ec\uae30\uc11c m\uc740 feature\uc758 \uac1c\uc218\uac00 \ub418\uace0,  $\\alpha$ \ub294 \ud328\ub110\ud2f0\uc758 \ub2c8\uacfc\ub97c \uc870\uc808\ud574\uc8fc\ub294 \ud30c\ub77c\ubbf8\ud130 \uc774\ub2e4. $\\alpha$ \uac00 \uc791\uc544\uc9c0\uba74 \uc120\ud615\ud68c\uadc0\uc640 \uac19\uc740 \ubaa8\ud615\uc774 \ub418\uace0, $\\alpha$ \uac00 \ucee4\uc9c0\uba74 \ud328\ub110\ud2f0\uc758 \uc601\ud5a5\ub825\uc774 \ucee4\uc9d1\ub2c8\ub2e4. \n\n### \ud2b9\uc131\n- Lasso\uc5d0\uc11c  $\\alpha$ \ub294 \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130\ub85c \uc0ac\uc6a9\uc790\uac00 \uc9c1\uc811 \uc9c0\uc815\uc744 \ud574\uc918\uc57c \ud569\ub2c8\ub2e4. \n- Penalty\ub97c \ud1b5\ud574 \ubaa8\ub378\uc758 \uacfc\uc801\ud569\uc744 \ub9c9\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \n- $w_j$\uac00 0\uc77c \uacbd\uc6b0 \ud574\ub2f9 feature\ub294 \ubaa8\ub378\uc5d0 \uc601\ud5a5\uc744 \uc8fc\uc9c0 \uc54a\ub294 \uac83\uc744 \uc54e\uc73c\ub85c\uc368 (\uc5b4\ub5a4 feature\uac00 \ub354 \ud070 \uc601\ud5a5\uc744 \ubbf8\uce60 \uc218 \uc788\ub294\uc9c0) \ubaa8\ub378\uc5d0 \ub300\ud55c \ud574\uc11d\ub825\uc774 \uc88b\uc544\uc9d1\ub2c8\ub2e4.\n","ce707c37":"## \ubaa8\ub378\uc758 \uc131\ub2a5\nLinear Model\uc758 \uc131\ub2a5 \ud3c9\uac00\ub97c \uc704\ud574 \uc6b0\ub9ac\ub294 $R^2$ \ub610\ub294 $\\text{Adjusted }R^2$ \uac12\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\n$$R^2 = 1 - \\frac{(Y - \\hat{Y})^2}{(Y - \\bar{Y})^2}$$\n\n$R^2$ \uc740 \ubaa8\ub378\uc774 \uc5bc\ub9c8\ub098 \uc88b\uc740\uc9c0\ub97c \uc54c\ub824\uc8fc\ub294 \uac12\uc785\ub2c8\ub2e4.\uc740$R^2$\uc740 \ud56d\uc0c1 0\uacfc 1\uc0ac\uc774\uc758 \uac12\uc774\uba70 1\uc5d0 \uac00\uae4c\uc6b8 \uc218\ub85d \ub192\uc740 \uc131\ub2a5\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.\n\n\uadf8\ub7ec\ub098 $R^2$ \uc758 \ubb38\uc81c\uc810\uc740 \ucd9c\ub825 \ubcc0\uc218\uc640 \uad00\uacc4\uc5c6\ub294 \uacbd\uc6b0\uc5d0\ub3c4 \ub354 \ub9ce\uc740 feature\ub97c \ucd94\uac00\ud558\uba74 \ud558\ub098\uc758 feature\uc77c\ub54c\uc640 \ub3d9\uc77c\ud55c \uac12 \ud639\uc740 \uc99d\uac00\ud55c \uac12\uc744 \ub098\ud0c0\ub0b4\uac8c \ub429\ub2c8\ub2e4. \uadf8\ub798\uc11c \uc6b0\ub9ac\ub294 $\\text{Adjusted }R^2$ \ub97c \uac19\uc774 \ube44\uad50\ud574\uc57c \ud569\ub2c8\ub2e4. $\\text{Adjusted }R^2$ \ub294 \uae30\uc874 \ubaa8\ub378\uc744 \uac1c\uc120\ud558\uc9c0 \uc54a\ub294 \ubcc0\uc218\ub97c \ucd94\uac00\ud560 \uacbd\uc6b0 \ubd88\uc774\uc775\uc744 \uc8fc\uac8c \ub429\ub2c8\ub2e4.\n\n$$\\text{Adjusted }R^2 = 1 - \\frac{(1 - R^2)(N - 1)}{N -P - 1}$$\n\n\uc5ec\uae30\uc11c $N$ \uc740 \uc778\uc2a4\ud134\uc2a4\uc758 \uc218\uc774\uba70 $P$ \ub294 feature\uc758 \uac1c\uc218\uc785\ub2c8\ub2e4.\n\n\ub530\ub77c\uc11c \uc5ec\ub7ec \ubcc0\uc218\uc5d0 \ub300\ud574 \uc120\ud615 \ud68c\uadc0\ub97c \uc791\uc131\ud558\ub294 \uacbd\uc6b0 \ud56d\uc0c1 \uc870\uc815 \ub41c R \uc81c\uacf1\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ud615\uc758 \uc6b0\uc218\uc131\uc744 \ud310\ub2e8\ud558\ub294 \uac83\uc774 \uc88b\uc2b5\ub2c8\ub2e4. \uc785\ub825 \ubcc0\uc218\uac00 \ud558\ub098 \ubfd0\uc778 \uacbd\uc6b0 R- \uc81c\uacf1\uacfc \uc870\uc815 \ub41c R \uc81c\uacf1\uc740 \uc815\ud655\ud788 \uac19\uc2b5\ub2c8\ub2e4.\n\n\uc77c\ubc18\uc801\uc73c\ub85c \ubaa8\ud615\uc5d0 \uc720\uc758\ud558\uc9c0 \uc54a\uc740 \ubcc0\uc218\ub97c \ub354 \ub9ce\uc774 \ucd94\uac00\ud560\uc218\ub85d R- \uc81c\uacf1\uacfc \uc870\uc815 \ub41c R- \uc81c\uacf1\uc758 \uac04\uaca9\uc774 \uc99d\uac00\ud569\ub2c8\ub2e4.","5c215f32":"## \ub4f1\ubd84\uc0b0\uc131(homoscedasticity)\n>  - \uc794\ucc28\uac00 \ud2b9\uc815\ud55c \ud328\ud134\uc744 \ubcf4\uc774\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\n - \uc794\ucc28\uc758 \ubd84\ud3ec\uc5d0 \ud2b9\uc815\ud55c \ud328\ud134\uc774 \uc788\uc73c\uba74 \ub370\uc774\ud130\uac00 \uc774\ubd84\uc0b0\uc131\uc744 \ub098\ud0c0\ub0b4\uace0 \uc774\ub294 \ube44\uc120\ud615\uc131\uc744 \ub098\ud0c0\ub0b4\uac8c \ub429\ub2c8\ub2e4.\n - \uc794\ucc28\ub294 \ub3c5\ub9bd\ubcc0\uc218\uc758 \uac12\uc5d0 \uad00\uacc4\uc5c6\uc774 \ub3d9\uc77c\ud55c \ubd84\uc0b0\uc744 \uac00\uc838\uc57c \ud569\ub2c8\ub2e4.\n - resid plot :\uc120\ud615 \ud68c\uadc0\uc758 \uc794\ucc28\ub97c \ud50c\ub85c\ud305\ud569\ub2c8\ub2e4. \uc774 \ud568\uc218\ub294 x\uc5d0\uc11c y\ub97c \ud68c\uadc0\ud558\uace0 \uc794\ucc28\uc758 \uc0b0\uc810\ub3c4\ub97c \uadf8\ub9bd\ub2c8\ub2e4.","7a6bdcad":"# ** Linear Regression**\n\nRegression\uc740 \ud558\ub098\uc758 \uc885\uc18d\ubcc0\uc218 Y\uc640 \uc77c\ub828\uc758 \ub3c5\ub9bd \ubcc0\uc218 X \uc0ac\uc774 \uad00\uacc4\uc758 \ud615\ud0dc\ub97c \ud0d0\uc0c9\ud558\uace0 \uacb0\uc815\ud558\ub294 \ud1b5\uacc4 \uce21\uc815\ubc29\ubc95\uc785\ub2c8\ub2e4. \n$$y = a_0 + a_1 x_1 + a_2 x_2 + ... + a_n x_n$$\n\nLinear regression\uc740 \ubaa8\ub4e0 \uc778\uc2a4\ud134\uc2a4\uc5d0 \ub300\ud55c \uc624\ucc28\ub97c \ucd5c\uc18c\ud654\ud558\ub3c4\ub85d \uacc4\uc218\ub97c \ucd5c\uc801\ud654\ud569\ub2c8\ub2e4.\n\n\uc624\ucc28\ub294 \uc544\ub798\uc758 \uc218\uc2dd\uc73c\ub85c \uacc4\uc0b0\uc774 \ub429\ub2c8\ub2e4:\n\n- Sum of residuals $\\sum_{i = 1}^{n} (Y - \\hat{Y})$\n     - \uc591\uc218 \ubc0f \uc74c\uc218\uc758 \uc624\ub958\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4.\n- Sum of the absolute value of residuals $\\sum_{i = 1}^{n} |Y - \\hat{Y}|$\n     - \uc624\ub958\ucc28\uc758 \uc808\ub300\uac12\uc758 \ud569\uc73c\ub85c \uacc4\uc0b0\ud569\ub2c8\ub2e4.\n     - Outlier\uc5d0 \ubcf4\ub2e4 robust \ud569\ub2c8\ub2e4.     \n- Sum of square of residuals $\\sum_{i = 1}^{n} (Y - \\hat{Y})^2$\n     - \uc624\ucc28\uac12\uc5d0 \ub300\ud574 \ubcf4\ub2e4 \ub192\uc740 \ud328\ub110\ud2f0\ub97c \uc90d\ub2c8\ub2e4. \n     - Outlier\uc5d0 \uc704 \uc218\uc2dd\ubcf4\ub2e4 \ub35c robust \ud569\ub2c8\ub2e4.\n\n### <u> Cost Function <\/u> \nCost Function\uc740 \ubaa8\ub378\uc758 \uc624\ub958 \uac12\uc744 \uce21\uc815\ud558\ub294\ub370 \uc815\uc758\ub41c \ud568\uc218\uc785\ub2c8\ub2e4.\n\n- Cost function for MSE: $J_{\\theta} = \\frac{1}{n} \\sum_{i = 1}^{n}(Y - \\hat{Y})^2$\n\n\uc608\uce21\uac12\uc744 \ub192\uc774\uae30 \uc704\ud574, \uc6b0\ub9ac\ub294 cost function\uc744 \ucd5c\uc18c\ud654\ud574\uc57c \ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ubaa9\uc801\uc73c\ub85c \uc6b0\ub9ac\ub294 gradient descent algorithm\uc744 \uc0ac\uc6a9\ud558\uac8c \ub429\ub2c8\ub2e4.\n\n### <u> Gradient Descent <\/u>\nGradient Descent\ub294 cost function \uac12\uc774 \ucd5c\uc18c\ud654 \ub418\ub294 \uc9c0\uc810\uc744 \ucc3e\uae30 \uc704\ud574 \ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \uc810\uc9c4\uc801\uc73c\ub85c \uc5c5\ub370\uc774\ud2b8\ud569\ub2c8\ub2e4.\n\n\n### <u> Regression\uc758 4\uac00\uc9c0 \uc870\uac74 <\/u>\n    1. \uc120\ud615\uc131 : \uc124\uba85 \ubcc0\uc218\uc640 \ubc18\uc751 \ubcc0\uc218 \uac04\uc758 \uad00\uacc4 \ubd84\ud3ec\uac00 \uc120\ud615\uc758 \uad00\uacc4\ub97c \uac00\uc9c4\ub2e4.\n    2. \ub3c5\ub9bd\uc131 : \uc124\uba85 \ubcc0\uc218\uc640 \ub2e4\ub978 \uc124\uba85 \ubcc0\uc218 \uac04\uc5d0 \uc0c1\uad00\uad00\uacc4\uac00 \uc801\ub2e4.\n    3. \ub4f1\ubd84\uc0b0\uc131 : \uc794\ucc28\uac00 \ud2b9\uc815\ud55c \ud328\ud134\uc744 \ubcf4\uc774\uc9c0 \uc54a\ub294\ub2e4. (\uc810\uc810 \ucee4\uc9c0\uac70\ub098 \uc791\uc544\uc9c0\ub294 \ud328\ud134\uc774 \uc5c6\ub2e4.)\n    4. \uc815\uaddc\uc131 : \uc794\ucc28\uac00 \uc815\uaddc\ubd84\ud3ec\uc774\ub2e4.    \n    * \uc794\ucc28 : \uc608\uce21\uac12 - \uad00\uce21\uac12 ","b7324910":"## \ub3c5\ub9bd\uc131\n - \uc124\uba85 \ubcc0\uc218\uc640 \ub2e4\ub978 \uc124\uba85 \ubcc0\uc218 \uac04\uc5d0 \uc0c1\uad00\uad00\uacc4(\ub2e4\uc911\uacf5\uc120\uc131)\uac00 \uc801\ub2e4.\n - \ud53c\uccd0\uac04\uc5d0 \ub2e4\uc911\uacf5\uc120\uc131\uc774 \uc874\uc7ac\ud558\ub294 \uacbd\uc6b0 \ubaa8\ub378\uc774 \uac01 \ud53c\uccd0\uc640 \ub300\uc0c1 \uac04\uc758 \uad00\uacc4\ub97c \ub3c5\ub9bd\uc801\uc73c\ub85c \ucd94\uc815\ud558\uae30\uac00 \uc5b4\ub835\uc2b5\ub2c8\ub2e4. \n - \uc0c1\uad00\uad00\uacc4\uac00 \ub192\uc740 2\uac1c\uc758 \ud53c\uccd0\uac00 \uc788\ub294 \uacbd\uc6b0, \ud558\ub098\uc758 \ud53c\uccd0\ub97c \uc0ad\uc81c\ud558\uac70\ub098 2\uac1c\uc758 \ud53c\uccd0\ub97c \uacb0\ud569\ud558\uc5ec \uc0c8\ub85c\uc6b4 \ud53c\uccd0\ub97c \ub9cc\ub4e6\uc73c\ub85c\uc368 \uc608\uce21\uc5d0 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n - PairPlot\uacfc heatmap(correlation matrix)\ub97c \ud1b5\ud574 \uc0c1\uad00\uad00\uacc4\uac00 \ub192\uc740 \ud2b9\uc9d5\ub4e4\uc744 \ucc3e\uc544 \ub0bc \uc218 \uc788\uc2b5\ub2c8\ub2e4.**** ","d9c0eed3":"\uc704\uc758 \ub450 \uacbd\uc6b0\ub97c \ubcf4\uba74 temp\uc640 atemp\uac00 \uac15\ud55c \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\uc774\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4."}}