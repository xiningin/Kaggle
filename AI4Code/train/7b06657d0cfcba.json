{"cell_type":{"5e7b88c9":"code","4e0bfa2d":"code","a159bda8":"code","43f9041b":"code","bf02ed7e":"code","105412bc":"code","7dd6cfa1":"code","a7b9dd26":"code","657df20c":"code","836bf188":"code","8e7efc30":"code","f020c717":"code","8761bb08":"code","189e031d":"code","dc1ac540":"code","b16b6da9":"code","f6a77a32":"code","a3387c73":"code","9b1a3fc5":"code","7152306b":"code","d79a8bc9":"code","d54de812":"code","efdd5f43":"code","2ad29957":"code","560b84d1":"code","dcce8143":"code","5a61401c":"code","5c366ddf":"markdown","ef30d765":"markdown","6fe18391":"markdown","7daa2d38":"markdown","3e29ee02":"markdown","c793b707":"markdown","95ac4529":"markdown","c72e52f7":"markdown","39c202f1":"markdown","89d60319":"markdown","0315e277":"markdown","87140051":"markdown","6857ca58":"markdown","b2542782":"markdown","5f79b186":"markdown","ecea08db":"markdown","098b1aeb":"markdown","cc700ffb":"markdown","0ddbbbf3":"markdown"},"source":{"5e7b88c9":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix, classification_report \nfrom sklearn.utils import class_weight\n\nimport tensorflow as tf\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization,  Flatten, Embedding, MaxPooling1D, Conv1D\nfrom keras.layers.merge import concatenate\nfrom keras.utils import plot_model\nfrom tensorflow.keras import activations,callbacks\nfrom keras import backend as K\n\n\nfrom tqdm.notebook import tqdm\nfrom IPython.display import Image , display\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","4e0bfa2d":"RANDOM_STATE = 2021\n\nNUM_HEADS = 2 # Let's create Hydra with 4 heads \n\nHEAD_EPOCHS = 50\nHYDRA_EPOCHS = 30\n\nTRAIN_VERBOSE = 2\n\nNUM_CLASS = 4","a159bda8":"train = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/train.csv\", index_col = 'id')\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/test.csv\", index_col = 'id')\n\nX = train.drop('target', axis = 1)\n\nlencoder = LabelEncoder()\ny = pd.DataFrame(lencoder.fit_transform(train['target']), columns=['target'])\n\ndf_all = pd.concat([X, test], axis = 0)\n\ndf_all = df_all.apply(lencoder.fit_transform)\n\nX, test = df_all[:len(train)], df_all[len(train):]\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y, random_state= RANDOM_STATE)","43f9041b":"NUM_FEATURES = len(X_train.columns)","bf02ed7e":"# inspired from mhttps:\/\/www.kaggle.com\/pourchot\/lb-1-0896-keras-nn-with-20-folds\n\nes = callbacks.EarlyStopping(monitor = 'val_loss', \n                             min_delta = 0.0000001, \n                             patience = 2,\n                             mode = 'min',\n                             baseline = None, \n                             restore_best_weights = True,\n                             verbose = 1)\n\nplateau  = callbacks.ReduceLROnPlateau(monitor = 'val_loss',\n                                       factor = 0.5, \n                                       patience = 2, \n                                       mode = 'min', \n                                       min_delt = 0.0000001,\n                                       cooldown = 0, \n                                       min_lr = 1e-8,\n                                       verbose = 1) ","105412bc":"# Lets define different architecture (brains) for each head :) \n# Let each hydra think differently.\n\n# You can play with configurations - i just randomly created 4 nn architectures\n\nh_model_1 = [\n        Dense(64, input_dim = NUM_FEATURES, activation='relu', kernel_initializer='he_uniform'),\n        Dropout(0.3),\n        Dense(32, activation='relu', kernel_initializer='he_uniform'),\n        Dropout(0.2),\n        Dense(128, activation='softmax', kernel_initializer='he_uniform'),\n        Dropout(0.2),\n        Dense(NUM_CLASS, activation='softmax', kernel_initializer='he_uniform')\n    ]\n\nh_model_2 = [\n        Dense(150, input_dim = NUM_FEATURES, activation='relu', kernel_initializer='he_uniform'),\n        Dropout(0.3),\n        Dense(150, activation='relu', kernel_initializer='he_uniform'),\n        Dropout(0.2),\n        Dense(150, activation='softmax', kernel_initializer='he_uniform'),\n        Dropout(0.2),\n        Dense(NUM_CLASS, activation='softmax', kernel_initializer='he_uniform')\n    ]\n\n\nh_model_3 = [\n        Dense(50, input_dim = NUM_FEATURES, activation='relu', kernel_initializer='he_uniform'),\n        Dropout(0.25),\n        Dense(25, activation='relu', kernel_initializer='he_uniform'),\n        Dropout(0.25),\n        Dense(10, activation='softmax', kernel_initializer='he_uniform'),\n        Dropout(0.2),\n        Dense(NUM_CLASS, activation='softmax', kernel_initializer='he_uniform')\n    ]\n\nh_model_4 = [\n        Dense(512, input_dim = NUM_FEATURES, activation='relu', kernel_initializer='he_uniform'),\n        BatchNormalization(),\n        Dropout(0.3),\n        Dense(256, activation='relu', kernel_initializer='he_uniform'),\n        BatchNormalization(),\n        Dropout(0.2),\n        Dense(128, activation='relu', kernel_initializer='he_uniform'),\n        BatchNormalization(),\n        Dropout(0.2),\n        Dense(NUM_CLASS, activation='softmax', kernel_initializer='he_uniform')\n    ]\n\n\nh_model_5 = [\n    Embedding(100, 4, input_length = NUM_FEATURES),\n    Flatten(),\n    Dense(64, activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(NUM_CLASS, activation='softmax')\n]\n\nh_model_6 = [\n    Embedding(100, 16, input_length = NUM_FEATURES),\n    Flatten(),\n    Dense(128, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.5),\n    Dense(40, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.25),\n    Dense(NUM_CLASS, activation='softmax')\n]\n\nhead_nn_models = [h_model_1, h_model_6]","7dd6cfa1":"# custom  weighted categorical crossentropy for Keras\n\ndef weight_categorical_crossentropy(weights):\n    \n    weights = K.variable(weights)\n\n    def loss(y_true, y_pred):\n        y_pred \/= K.sum(y_pred, axis=-1, keepdims=True)\n        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n        loss = y_true * K.log(y_pred) * weights\n        loss = -K.sum(loss, -1)\n        return loss\n\n    return loss","a7b9dd26":"class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train.target), y_train.target)\nprint(class_weights)","657df20c":"# Here is our playground for testing with weights\n\nclass_weights = [1.5, 0.5, 1, 1]\n","836bf188":"def fit_hydra_head_model(fX_train, fy_train, fX_valid, fy_valid, n_model):\n    oy_train = to_categorical(fy_train)\n    oy_valid = to_categorical(fy_valid)\n    \n    model = Sequential(head_nn_models[n_model])\n    \n    if WEIGHTED_TRAINING:\n        model.compile(loss = weight_categorical_crossentropy(class_weights), optimizer = tf.keras.optimizers.Adam(), metrics=['accuracy'])\n    else:\n        model.compile(loss = \"categorical_crossentropy\", optimizer = tf.keras.optimizers.Adam(), metrics=['accuracy'])\n        \n    history = model.fit(fX_train, oy_train, epochs = HEAD_EPOCHS, \n                        verbose = TRAIN_VERBOSE, \n                        validation_data=(fX_valid, oy_valid), \n                        callbacks = [es, plateau], \n                        batch_size = 128)\n    \n    return model, history","8e7efc30":"def plot_model_learning(history):\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()","f020c717":"def train_hydra_df():\n    if NUM_HEADS <= len(head_nn_models):     \n        for i in tqdm(range(NUM_HEADS)):\n            print(f'\\n>>>>>>>>>>> Training head {i+1} ... <<<<<<<<<<<')\n            model, history = fit_hydra_head_model(X_train, y_train, X_valid, y_valid, i)\n            hydra_heads_models_df.append(model)\n            print(\"\\n\")\n            plot_model_learning(history)\n    else:\n        print(\"ERROR: param NUM_HEADS > list of defined model heads [head_nn_models].\")","8761bb08":"# If you want to turn on weighted training with custom weighted entropy loss change to True\n\nWEIGHTED_TRAINING = False","189e031d":"hydra_heads_models_df = []\ntrain_hydra_df()","dc1ac540":"from keras import regularizers\n\ndef define_hydra_model(heads):\n    for i in range(len(heads)):\n        model = heads[i]\n        \n        # Lets freeze all head layers \n        for layer in model.layers:\n            layer.trainable = False\n            layer._name = 'hydra_head' + str(i+1) + '_' + layer.name\n\n    \n    hydra_visible = [model.input for model in heads]\n    hydra_outputs = [model.output for model in heads]\n    merge = concatenate(hydra_outputs)\n    \n    # Create Hydra heart layers and train them \n    \n    hidden = Dense(NUM_HEADS * NUM_CLASS, activation='relu')(merge)\n    #x = Dense(10, activation='softmax', kernel_initializer='he_uniform')(hidden)\n    output = Dense(4, activation='softmax')(hidden)\n    \n    # Architecture will be examined later ... below you can find my experiment\n    #x = BatchNormalization()(hidden)\n    #x = Dropout(0.3)(x)\n    #x = Dense(32, activation='relu')(x)\n    #x = BatchNormalization()(x)\n    #x = Dropout(0.2)(x)\n    # output = Dense(4, activation='softmax')(x)\n    \n    model = Model(inputs = hydra_visible, outputs = output)\n    \n    if WEIGHTED_TRAINING:\n        model.compile(loss = weight_categorical_crossentropy(class_weights), optimizer='adam',  metrics=['accuracy'])\n    else:\n        model.compile(loss = \"categorical_crossentropy\", optimizer='adam',  metrics=['accuracy'])\n    \n    return model","b16b6da9":"# Define Hydra model\nhydra_model_df = define_hydra_model(hydra_heads_models_df)","f6a77a32":"# Since models are the same (could be different :)) lets plot one of them \n\nplot_model(hydra_model_df, show_shapes=True, show_layer_names=True, to_file='model.png')\ndisplay(Image('model.png'))","a3387c73":"hydra_model_df.summary()","9b1a3fc5":"def prepare_input_data(model, X_in):\n    X = [X_in for _ in range(len(model.input))]\n    return X","7152306b":"y_train = to_categorical(y_train)\ny_valid = to_categorical(y_valid)","d79a8bc9":"# Now it is time to fit Hydras heart .... to see the world througt emotion :)  \nprint(\"\\n\\n>>>>>>>>>>  Fit  HydraNet <<<<<<<<<<<<\")\nhistory = hydra_model_df.fit(prepare_input_data(hydra_model_df, X_train), \n                             y_train, \n                             epochs = HYDRA_EPOCHS, \n                             verbose = TRAIN_VERBOSE, \n                             validation_data=(prepare_input_data(hydra_model_df, X_valid), y_valid),\n                             callbacks = [es, plateau], \n                             batch_size = 128)\nplot_model_learning(history)","d54de812":"# Lets look what Hydra sees\ny_valud_preds_df = hydra_model_df.predict(prepare_input_data(hydra_model_df, X_valid), verbose=0)","efdd5f43":"base_class = np.argmax(y_valid, axis = 1)\npreds = np.argmax(y_valud_preds_df, axis = 1)\n\nsns.heatmap(pd.DataFrame(confusion_matrix(base_class, preds)), annot=True, linewidths=.5, fmt=\"d\")","2ad29957":"sub_preds_df = hydra_model_df.predict(prepare_input_data(hydra_model_df, test), verbose=0)\npredictions_df = pd.DataFrame(sub_preds_df, columns = [\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\"])","560b84d1":"blend_l1 = pd.read_csv(\"..\/input\/tps05blender-v2\/tps05-remek-blender_v2.csv\")\n\n\noutput = predictions_df.copy()\noutput[\"Class_1\"] = (predictions_df.Class_1 * 0.3 + blend_l1.Class_1 * 0.7)\noutput[\"Class_2\"] = (predictions_df.Class_2 * 0.3 + blend_l1.Class_2 * 0.7)\noutput[\"Class_3\"] = (predictions_df.Class_3 * 0.3 + blend_l1.Class_3 * 0.7) \noutput[\"Class_4\"] = (predictions_df.Class_4 * 0.3 + blend_l1.Class_4 * 0.7) ","dcce8143":"sub = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv\")\n\npredictions_df = pd.DataFrame(output, columns = [\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\"])\npredictions_df['id'] = sub['id']\npredictions_df.to_csv(\"TPS-05-hydra_df_blended_submission.csv\", index = False)","5a61401c":"predictions_df.drop(\"id\", axis=1).describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","5c366ddf":"# 1. CREATE HYDRA HEADS","ef30d765":"IMPORT MODULES","6fe18391":"### CREATE CUSTOM WEIGHTED CROSSENTROPY LOSS","7daa2d38":"# This is story about Hydra ... HydraNet for ... TPS-05\n\n### Each hydra has many heads. So let's teach each one to look through a world of different data. For this purpose, we will build a neural network consisting of heads  and the heart of Hydra will be another neural network. So we are really building a solution that implements the Stacked Ensemble.\n\n- 11.05.2021 - first release (one head type, kfold trining)\n- 12.05.2021 - multi-head architecture (I decided to remove kfold training - one method of training is enough to understand way of creating Keras Ensemble Stacking, second reason - to speed up learning proces - I do not see any big differences on score)\n- 12.05.2021 - **WEIGHTED TRAINING (!) - custom weighted crossentropy loss function implemented**\n- 26.05.2021 - probabilities blending\n\nInteresting in my TPS-05 notebooks?\n- [Pytorch NN for tabular - step by step](https:\/\/www.kaggle.com\/remekkinas\/tps-5-pytorch-nn-for-tabular-step-by-step)\n- [CNN (2D Convolution) for solving TPS-05](https:\/\/www.kaggle.com\/remekkinas\/cnn-2d-convolution-for-solving-tps-05)\n- [SHAP + LGBM - looking for best features](https:\/\/www.kaggle.com\/remekkinas\/shap-lgbm-looking-for-best-features)\n- [Weighted training - XGB, RF, LR, ... SMOTE](https:\/\/www.kaggle.com\/remekkinas\/tps-5-weighted-training-xgb-rf-lr-smote)\n","3e29ee02":"# 6. LET'S LOOK HOW HYDRA SEES","c793b707":"# 4. PLOT HYDRA MODEL","95ac4529":"## 1B. HEAD NN ARCHITECTURE","c72e52f7":"## HYDRA TRAINING (+ WEIGHTED)","39c202f1":"SET NOTEBOOK PARAMS","89d60319":"If you like it I appreciate any feedback and further development. \n\n## Next solution is coming ....... stay tuned :)  ","0315e277":"# 3. DEFINE HYDRA HEART","87140051":"# 7. LET'S PREDICT AND SUBMIT TO TPS-05","6857ca58":"LOAD AND PREPROCESS DATA","b2542782":"# 5. LET'S LEARN THE HYDRA TO SEE THE WHOLE WORLD","5f79b186":"# 2. TRAIN HYDRA HEADS","ecea08db":"# TO DO\n- [x] NN head architecture improvements\n- [x] NN heart architecure improvements\n- [X] Better imput data preparation\n- [X] Hyperparameter tuning (mainly LR)\n- [X] Learning callbacks \n- [X] Training metrics plots","098b1aeb":"### CALCULATE CLASS WEIGHTS","cc700ffb":"![](https:\/\/vignette3.wikia.nocookie.net\/dragon\/images\/7\/78\/Image.jpg\/revision\/latest?cb=20130506000229)","0ddbbbf3":"### DEFINE WEIGHTED TRAINIG LOOP "}}