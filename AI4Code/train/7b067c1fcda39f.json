{"cell_type":{"3c3f5b1a":"code","159b4909":"code","dbc5675b":"code","a2ea93b6":"code","81ce3f60":"code","664c2ea8":"code","98f3b5cb":"code","c98d8e12":"code","f5e9ec7e":"code","b9f0ae91":"code","720840ca":"code","21591d82":"code","10321987":"code","d381fbf5":"code","e16df003":"code","e0729c8b":"code","3b4eb2fb":"code","d3321d38":"code","1073228e":"code","54fd1ede":"code","5a01807b":"code","866d9bb7":"code","9866f6cd":"code","90968247":"code","a38cc1e6":"code","a094830c":"code","31fd6f4c":"code","bd6ed298":"code","79402747":"code","d3c61723":"code","5d40523c":"code","7bc44369":"code","589fe42f":"code","905b8004":"code","6330203d":"code","b2f187f1":"code","3c3118cf":"code","95d4734e":"code","5d89f73d":"code","e5e72c4e":"code","046768ff":"code","93c1e0de":"code","8d975361":"markdown","c7bfe587":"markdown","2e3173eb":"markdown","d7db3156":"markdown","7b94e0b0":"markdown","a2b636a4":"markdown","e5fd1c79":"markdown","249f99fb":"markdown","bc5646be":"markdown","73bb3419":"markdown"},"source":{"3c3f5b1a":"import pandas as pd \nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns","159b4909":"df=pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\ndf=df.drop(['Id'],axis=1)\nresponse=df['Pawpularity'] #response variable for regression\nX=df.drop(['Pawpularity'],axis=1)","dbc5675b":"labels=np.unique(X,axis=0)","a2ea93b6":"labels","81ce3f60":"labels.shape","664c2ea8":"class_map={tuple(x):i for i,x in enumerate(labels)}","98f3b5cb":"Y=[]\nfor i in tqdm(range(X.shape[0])):\n    l=tuple(X.loc[i])\n    Y.append(class_map[l])\nY=np.array(Y)","c98d8e12":"plt.figure()\nsns.histplot(Y)\nplt.xlabel('label')\nplt.show()","f5e9ec7e":"unique, counts = np.unique(Y, return_counts=True)","b9f0ae91":"counts.shape","720840ca":"counts","21591d82":"def weight_map(count):\n    total=Y.shape[0]\n    return (1\/count)*(total\/unique.shape[0])","10321987":"class_weights=list(map(weight_map,counts))","d381fbf5":"plt.figure(figsize=(12,12))\nplt.barh(unique,class_weights)\n\nplt.ylabel('class weights')\nplt.xlabel('weights')\nplt.show()","e16df003":"import tensorflow as tf\nimport tensorflow_datasets as tfds","e0729c8b":"pth='..\/input\/petfinder-pawpularity-score'\n\n\nds=tf.keras.preprocessing.image_dataset_from_directory(\n    directory=pth,\n    image_size=(128,128),\n    batch_size=1,\n    seed=0,\n    shuffle=False\n)\n","3b4eb2fb":"dataset=[]\n\nfor x,type_ in tqdm(ds):\n    if type_==0:\n        continue\n    dataset.append(x[0])","d3321d38":"dataset=tf.concat([dataset],axis=0)","1073228e":"dataset.shape,Y.shape","54fd1ede":"from tensorflow.keras import layers,optimizers,losses\nfrom tensorflow.keras.applications.efficientnet import EfficientNetB0,preprocess_input","5a01807b":"class Model(tf.keras.Model):\n    def __init__(self,img_size,num_class,tau):\n        super().__init__()\n        self.emb=layers.Embedding(num_class,num_class)\n        self.eff=EfficientNetB0(include_top=False,pooling='avg',input_shape=(img_size,img_size,3)) #7x7x1280\n        self.eff.trainable=False\n        \n        self.out1=tf.keras.Sequential([\n            layers.Dense(2048,activation='relu'),\n            layers.Dense(num_class)\n        ])\n        \n        self.out2=tf.keras.Sequential([\n            layers.Dense(2048,activation='relu'),\n            layers.Dense(num_class)\n        ])\n        \n        self.tau=tau\n\n        \n    def call(self,inputs):\n        '''\n        inputs:[x,y]\n        x:(batch,h,w,c)\n        y:(batch,)\n        '''\n        x,y,y_ohe=inputs\n        \n        F=self.eff(preprocess_input(x))\n        \n        #soft target\n        emb=self.emb(y)\n        soft_target=tf.nn.softmax(emb,axis=-1)\n        \n        \n        #output classification\n        o1=self.out1(F)\n        o2=self.out2(tf.stop_gradient(F))\n        \n        \n        #prob\n        o2_prob=tf.nn.softmax(o2,axis=-1)\n        tau2_prob=tf.stop_gradient(tf.nn.softmax(o2\/self.tau,axis=-1))\n        \n        #mask\n        mask=tf.stop_gradient(tf.cast(tf.equal(tf.argmax(o2,axis=-1), tf.argmax(y_ohe,axis=-1)), tf.float32))\n        \n        return o1,o2,emb,soft_target,o2_prob,tau2_prob,mask","866d9bb7":"tau=2\nalpha=0.9\nbeta=0.5\n\nlr=1e-3\nbatch_size=64\n\nepochs=100","9866f6cd":"ds=tf.data.Dataset.from_tensor_slices((dataset,tf.convert_to_tensor(Y))).batch(batch_size)","90968247":"model=Model(128,unique.shape[0],tau)","a38cc1e6":"opt=optimizers.Adam(learning_rate=lr)","a094830c":"def ce(labels,logits,class_weights,mask=None):\n    y_ohe=tf.one_hot(labels,depth=len(class_weights))\n    weights=tf.math.multiply(class_weights,y_ohe)\n    weights=tf.reduce_sum(weights,axis=-1)\n    loss=tf.nn.softmax_cross_entropy_with_logits(labels=y_ohe,logits=logits)*weights\n    if mask!=None:\n        loss=tf.reduce_sum(loss*mask)\/tf.reduce_sum(mask+1e-8)\n    else:\n        loss=tf.reduce_mean(loss)\n    return loss","31fd6f4c":"def soft_ce(labels,soft_labels,logits,class_weights,mask=None):\n    y_ohe=tf.one_hot(labels,depth=len(class_weights))\n    weights=tf.math.multiply(class_weights,y_ohe)\n    weights=tf.reduce_sum(weights,axis=-1)\n    \n    loss=tf.nn.softmax_cross_entropy_with_logits(labels=soft_labels,logits=logits)*weights\n    \n    if mask!=None:\n        loss=tf.reduce_sum(loss*mask)\/tf.reduce_sum(mask+1e-8)\n    else:\n        loss=tf.reduce_mean(loss)\n    return loss","bd6ed298":"@tf.function\ndef step(x,y,opt):\n    y_ohe=tf.one_hot(y,depth=len(class_weights))\n    with tf.GradientTape() as tape:\n        \n        o1,o2,emb,soft_target,o2_prob,tau2_prob,mask=model([x,y,y_ohe])\n        \n        l_o1_y=ce(y,o1,class_weights)\n\n        l_o1_emb=soft_ce(y,tf.stop_gradient(soft_target),o1,class_weights)\n\n        l_o2_y=ce(y,o2,class_weights)\n\n        l_o2_emb= soft_ce(y,tau2_prob,emb,class_weights,mask)\n        \n        #regularzation term\n        l_re = tf.reduce_sum(tf.nn.relu(tf.reduce_sum(o2_prob*y_ohe,axis=-1)-alpha))\n        \n        \n        loss = beta*l_o1_y + (1-beta)*l_o1_emb +l_o2_y +l_o2_emb +l_re\n        \n    grad=tape.gradient(loss,model.trainable_weights)\n    opt.apply_gradients(zip(grad,model.trainable_weights))\n    \n    return l_o1_y,l_o1_emb,l_o2_y,l_o2_emb,l_re","79402747":"def train():\n    ckpt = tf.train.Checkpoint(model=model)\n    ckpt_manager = tf.train.CheckpointManager(ckpt,'.\/ckpt', max_to_keep=1)\n    if ckpt_manager.latest_checkpoint :\n        ckpt.restore(ckpt_manager.latest_checkpoint)\n        print('---ckpt restored----')\n     \n    for epoch in range(epochs):\n        loop=tqdm(ds)\n        for x,y in loop:\n            l_o1_y,l_o1_emb,l_o2_y,l_o2_emb,l_re=step(x,y,opt)\n            \n            loop.set_postfix(loss=f'epoch:{epoch}, l_o1_y:{l_o1_y}'\n                            f'l_o1_emb:{l_o1_emb}, l_o2_y:{l_o2_y}'\n                            f'l_o2_emb:{l_o2_emb}, l_re:{l_re}')\n            break\n        if epoch%5==0:\n            ckpt_manager.save()","d3c61723":"train()","5d40523c":"from lightgbm import LGBMRegressor","7bc44369":"embs=model.emb(Y) #(9912, nclass)","589fe42f":"lgbm=LGBMRegressor()","905b8004":"lgbm.fit(embs.numpy(),np.array(response))","6330203d":"sns.heatmap(model.emb.weights[0],cmap=\"YlGnBu\") #similarity between labels","b2f187f1":"df=pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')\n\nId=df['Id']\n\ndf=df.drop(['Id'],axis=1)","3c3118cf":"class_matrix=np.array(list(class_map.keys()))","95d4734e":"class_matrix","5d89f73d":"def cosine_similarity(y):\n    return class_matrix@np.array(y)\/(np.linalg.norm(class_matrix,axis=-1)*np.linalg.norm(y)+1e-8)","e5e72c4e":"pred=[]\nfor i in range(df.shape[0]):\n    y=df.loc[i]\n    try:\n        y=class_map[tuple(y)]\n    except:\n        #if label does not in training set, use similarity, pick heighest similarity label\n        sim=cosine_similarity(y)\n        candidates=np.array(list(class_map.keys()))[sim==np.min(sim)]\n        idx=np.random.choice(range(candidates.shape[0]))\n        y=candidates[idx]\n    inputs=class_map[tuple(y)]\n    x=model.emb(inputs)\n    prediction=lgbm.predict(x[np.newaxis,:])\n    \n    pred.append(prediction)\n    \npred=np.array(pred)","046768ff":"submit=pd.concat([Id,pd.Series(pred[:,0].astype('float32'))],axis=1)\n\nsubmit=submit.rename(columns={0:'Pawpularity'})","93c1e0de":"submit.to_csv('.\/submission.csv',index=False)","8d975361":"# Submit","c7bfe587":"* There are imbalance data, now I just use class weight for computing losses, maybe there exist better solution","2e3173eb":"# Regression","d7db3156":"* Number of unique Label","7b94e0b0":"# Learning Label Embedding From Images","a2b636a4":"* Mapping label","e5fd1c79":"* Reference : [Label Embedding Network](https:\/\/arxiv.org\/abs\/1710.10393)","249f99fb":"# Prepare images and label dataset","bc5646be":"* Class Map","73bb3419":"# Tabular Data"}}