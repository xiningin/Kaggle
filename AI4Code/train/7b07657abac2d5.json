{"cell_type":{"1f1b047f":"code","c1ab324f":"code","1ea85db5":"code","a8ff5bb6":"code","d1020a8b":"code","6db76029":"code","3e45cda4":"code","036cbf16":"code","4c3df112":"code","3c1ce3dc":"code","40c69fd9":"code","1ad43140":"code","42fdf6c7":"code","378c5332":"code","03d4f6fd":"code","72e28ce6":"code","b580bd5d":"code","f82fd5ab":"code","7ae65388":"code","673ab345":"code","c9f6098f":"markdown","c09fe11f":"markdown","171609ce":"markdown"},"source":{"1f1b047f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import confusion_matrix\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import (Flatten, Dense, Conv2D, \n                                     BatchNormalization, MaxPooling2D,\n                                    Dropout)\n\nfrom tensorflow.keras.models import Sequential\n\n\nnp.random.seed(101)\ntf.random.set_seed(101)\n\nimport warnings\nwarnings.filterwarnings('ignore')","c1ab324f":"(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()","1ea85db5":"CLASSES = ['T-shirt\/top', 'Trouser', 'Pullover',\n          'Dress', 'Coat', 'Sandal', 'Shirt', \n           'Sneaker', 'Bag', 'Ankle boot']\n\nnum_classes = len(CLASSES)","a8ff5bb6":"X_train.shape","d1020a8b":"fig, axs = plt.subplots(nrows=2, ncols=5, figsize=(12, 5))\naxs = np.ravel(axs)\n\nfor i in range(10):\n    plt.sca(axs[i])\n    plt.imshow(X_train[i], cmap='gray')\n    plt.title(CLASSES[y_train[i]], fontdict={'size':16})\n    plt.axis('off')\n    \nplt.tight_layout()\nplt.show()","6db76029":"X_train_scaled = X_train \/ 255.0\nX_test_scaled = X_test \/ 255.0\n\n## one hot encoding of y_train, and y_test\ny_train_new = keras.utils.to_categorical(y_train, num_classes=num_classes)\ny_test_new = keras.utils.to_categorical(y_test, num_classes=num_classes)","3e45cda4":"X_train_scaled = tf.expand_dims(X_train_scaled, axis=-1)\nX_test_scaled = tf.expand_dims(X_test_scaled, axis=-1)","036cbf16":"buffer_size = 1024\nbatch_size = 32\n\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train_scaled, y_train_new))\ntrain_ds = train_ds.shuffle(buffer_size).batch(batch_size).prefetch(1)\n\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test_scaled, y_test_new))\ntest_ds = test_ds.batch(batch_size)","4c3df112":"def build_network():\n    \n    model = Sequential([\n        Conv2D(128, 3, strides=1, padding='same', activation='relu', input_shape=X_train_scaled[0].shape),\n        BatchNormalization(),\n        MaxPooling2D(2),\n        Conv2D(128, 3, padding='same', activation='relu'),\n        BatchNormalization(),\n        MaxPooling2D(2),\n        Flatten(),\n        Dense(256, 'relu'),\n        Dropout(0.2),\n        Dense(128, 'relu'),\n        Dense(num_classes, 'softmax')\n    ])\n    \n    return model","3c1ce3dc":"tf.random.set_seed(10)\n\nmodel_one = build_network()\n\nmodel_one.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel_one.summary()","40c69fd9":"tf.random.set_seed(10)\n\nhistory_one = model_one.fit(train_ds, epochs=10, validation_data=test_ds)","1ad43140":"# plot the validation and training curves separately\n\ndef plot_history(history, metrics=['accuracy', 'loss']):\n    \"\"\"\n    Returns the history plots for training & validation data\n    \"\"\"\n\n    fig = plt.figure(figsize=(15,6))\n\n    plt.subplot(121)\n\n    plt.plot(history.history[metrics[0]], label='Training')\n    plt.plot(history.history['val_'+metrics[0]], label='Validation')\n    plt.legend()\n    plt.title(f\"{ metrics[0] } Curve\")\n\n    plt.subplot(122)\n    plt.plot(history.history[metrics[1]], label='Training')\n    plt.plot(history.history['val_'+metrics[1]], label='Validation')\n    plt.legend()\n    plt.title(f\"{ metrics[1] } Curve\")\n\n    plt.show()","42fdf6c7":"plot_history(history_one)","378c5332":"tf.random.set_seed(10)\n\ngenerator = keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True, rescale=1\/255.0)\n\ntrain_generator = generator.flow(np.expand_dims(X_train, axis=-1), y_train_new, batch_size=batch_size)","03d4f6fd":"tf.random.set_seed(10)\n\nmodel_two = build_network()\n\nmodel_two.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory_two = model_two.fit(train_generator, \n                            steps_per_epoch=len(X_train)\/\/batch_size, \n                            epochs=10, \n                            validation_data=(X_test_scaled, y_test_new))","72e28ce6":"predictions = model_two.predict(X_test_scaled).argmax(axis=1)","b580bd5d":"conf_mat = confusion_matrix(y_test, predictions)\n\nplt.figure(figsize=(10,8))\nsns.heatmap(conf_mat, annot=True, fmt='0g', cmap=plt.cm.coolwarm_r)\nplt.xlabel(\"predicted\")\nplt.ylabel(\"Actual\")\n\nplt.show()","f82fd5ab":"np.fill_diagonal(conf_mat, val=0)","7ae65388":"row_sums = conf_mat.sum(axis=1, keepdims=True)\n\nfinal_mat = conf_mat \/ row_sums\n\n\nplt.figure(figsize=(10,8))\nsns.heatmap(final_mat, annot=True, fmt='.3f', cmap=plt.cm.coolwarm_r)\nplt.xlabel(\"predicted\")\nplt.ylabel(\"Actual\")\nplt.show()","673ab345":"CLASSES[0], CLASSES[6]","c9f6098f":"## Base Model","c09fe11f":"## Error Analysis","171609ce":"## Data Augmentation"}}