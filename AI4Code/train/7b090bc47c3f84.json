{"cell_type":{"fe1121d4":"code","06bbaf7c":"code","6c15c788":"code","bf61f2c6":"code","4c41bad1":"code","822b0647":"code","cfec5723":"code","5a1ee586":"code","bcc3ed8b":"code","ae85c5b2":"code","1866ef60":"code","b379504e":"code","2f573eb8":"code","10aaeb95":"code","6b16cbbd":"code","73fc02ef":"markdown","2c1ee68c":"markdown","72f400df":"markdown","fc64b498":"markdown","b4ce2972":"markdown","5434f508":"markdown","bd77287b":"markdown","cb5fb93c":"markdown","5a1525cf":"markdown","de37fffe":"markdown","22e2c12c":"markdown","c0ab1784":"markdown","6e63a73f":"markdown","5d388dea":"markdown","2b6760cb":"markdown","a7dcf13b":"markdown","50727ff7":"markdown","d9a8b1da":"markdown","6037e965":"markdown","3fa3d07d":"markdown"},"source":{"fe1121d4":"import pandas as pd \nimport seaborn as sns\nsns.set()\ngiving_data = [ # from https:\/\/www.philanthropy.com\/article\/Gifts-to-Charity-Dropped-17\/246511\n    [ 2018, 292_090 ],\n    [ 2017, 302_510 ],\n    [ 2016, 292_300 ],\n    [ 2015, 280_430 ],\n    [ 2014, 267_560 ],\n    [ 2013, 261_320 ],\n    [ 2012, 267_280 ],\n    [ 2011, 238_790 ],\n    [ 2010, 239_520 ],\n    [ 2009, 235_000 ],\n    [ 2008, 249_310 ],\n    [ 2007, 282_240 ]\n]\ngiving_df = pd.DataFrame(giving_data, columns=['Year','Individual Giving'])\ngiving_df = giving_df[::-1]  # reverse data \nax = giving_df.plot.barh(x=0,y=1,rot=0,figsize=(12,6),legend=False, width=.8)\nax.set_title('Individual Giving for U.S. Charities', fontsize=18)\nfor p in ax.patches:\n    #print(p)\n    ax.annotate('$'+format(p.get_width()*1_000_000,',d'), (p.get_width() - 64_000, p.get_y()+.25), color='white', weight='bold')  \nax.axes.get_xaxis().set_ticklabels([])\n_ = 0 # be quiet, matplotlib","06bbaf7c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas.plotting as pp\nfrom IPython.display import display, HTML\nimport os\nimport warnings\nwarnings.filterwarnings('always')\ndef print_files():\n    files = [] \n    for dirname, _, filenames in os.walk('\/kaggle\/input'):\n        for filename in filenames:\n            files.append(os.path.join(dirname, filename))\n    files.sort()\n    for file in files:\n        print(file)\ndef print_full(x):\n    pd.set_option('display.max_rows', len(x))\n    pd.set_option('display.max_columns', None)\n    pd.set_option('display.width', 2000)\n    pd.set_option('display.float_format', '{:20,.2f}'.format)\n    pd.set_option('display.max_colwidth', -1)\n    x = x.style.set_properties(**{'text-align': 'left'})\n    display(x) # print(x)\n    pd.reset_option('display.max_rows')\n    pd.reset_option('display.max_columns')\n    pd.reset_option('display.width')\n    pd.reset_option('display.float_format')\n    pd.reset_option('display.max_colwidth')\n# kaggle ML and DS survey - 2019\nk19mr = pd.read_csv(\"\/kaggle\/input\/kaggle-survey-2019\/multiple_choice_responses.csv\",skiprows=[1])\nk19mh = pd.read_csv('\/kaggle\/input\/kaggle-survey-2019\/multiple_choice_responses.csv',nrows=1)\nk19mh = pd.Series(k19mh.transpose()[0]) # questions keyed by column \nk19tr = pd.read_csv(\"\/kaggle\/input\/kaggle-survey-2019\/other_text_responses.csv\",skiprows=[1])\nk19th = pd.read_csv(\"\/kaggle\/input\/kaggle-survey-2019\/other_text_responses.csv\",nrows=1)\nk19th = pd.Series(k19th.transpose()[0]) # questions keyed by column \nk19sr = pd.read_csv(\"\/kaggle\/input\/kaggle-survey-2019\/survey_schema.csv\",skiprows=[1])\nk19sh = pd.read_csv(\"\/kaggle\/input\/kaggle-survey-2019\/survey_schema.csv\",nrows=1)\nk19sh = pd.Series(k19sh.transpose()[0]) # questions keyed by column \nk19qh = pd.read_csv(\"\/kaggle\/input\/kaggle-survey-2019\/questions_only.csv\")\nk19qh = pd.Series(k19qh.transpose()[0]) # questions keyed by column \nk19mr_usa = (k19mr['Q3'] == 'United States of America')\nk19mr_mid = (k19mr['Q6'] == '50-249 employees')\nsp = k19mr[['Q1','Q2','Q3','Q4','Q5','Q6','Q7','Q10','Q23']]\norders = {\n    1: None,\n    2: None,\n    3: None,\n    4: [\"No formal education past high school\", \"Professional degree\", \"Some college\/university study without earning a bachelor\u2019s degree\", \"Bachelor\u2019s degree\", \"Master\u2019s degree\", \"Doctoral degree\", \"I prefer not to answer\"],\n    5: None,\n    6: [\"0-49 employees\", \"50-249 employees\", \"250-999 employees\", \"1000-9,999 employees\", \"> 10,000 employees\"],\n    7: [\"0\", \"1-2\", \"3-4\", \"5-9\", \"10-14\", \"15-19\", \"20+\"],\n    8: [\"No (we do not use ML methods)\", \"We are exploring ML methods (and may one day put a model into production)\", \"We use ML methods for generating insights (but do not put working models into production)\", \"We recently started using ML methods (i.e., models in production for less than 2 years)\", \"We have well established ML methods (i.e., models in production for more than 2 years)\", \"I do not know\"],\n    10: [\"$0-999\", \"1,000-1,999\", \"2,000-2,999\", \"3,000-3,999\", \"4,000-4,999\", \"5,000-7,499\", \"7,500-9,999\", \"10,000-14,999\", \"15,000-19,999\", \"20,000-24,999\", \"25,000-29,999\", \"30,000-39,999\", \"40,000-49,999\", \"50,000-59,999\", \"60,000-69,999\", \"70,000-79,999\", \"80,000-89,999\", \"90,000-99,999\", \"100,000-124,999\", \"125,000-149,999\", \"150,000-199,999\", \"200,000-249,999\", \"250,000-299,999\", \"300,000-500,000\", \"> $500,000\"],\n    11: [\"$0 (USD)\", \"$1-$99\", \"$100-$999\", \"$1000-$9,999\", \"$10,000-$99,999\", \"> $100,000 ($USD)\"],\n    15: [\"I have never written code\", \"< 1 years\", \"1-2 years\", \"3-5 years\", \"5-10 years\", \"10-20 years\", \"20+ years\"],\n    19: None,\n    22: [\"Never\", \"Once\", \"2-5 times\", \"6-24 times\", \"> 25 times\"],\n    23: [\"< 1 years\", \"1-2 years\", \"2-3 years\", \"3-4 years\", \"4-5 years\", \"5-10 years\", \"10-15 years\", \"20+ years\"]    \n}","6c15c788":"### Nathan starts here \n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom IPython.display import display\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\n%matplotlib inline\nimport seaborn as sns\nsns.set()\n\n# Graphics in retina format are more sharp and legible\n%config InlineBackend.figure_format = 'retina' \n\n# Increase the default plot size and set the color scheme\nplt.rcParams['figure.figsize'] = 8, 5\nplt.rcParams['image.cmap'] = 'viridis'\n\n\nimport plotly.offline as py\nimport pycountry\n\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom plotly.offline import init_notebook_mode, iplot \ninit_notebook_mode(connected=True)\n\nimport folium \nfrom folium import plugins\n\nimport re\n\ncolors = [\"steelblue\",\"dodgerblue\",\"lightskyblue\",\"powderblue\",\"deepskyblue\",\"cyan\",\"darkturquoise\",\"paleturquoise\",\"turquoise\"]\n\n#Importing the 2019 Dataset\ndf_2019 = pd.read_csv('..\/input\/kaggle-survey-2019\/multiple_choice_responses.csv')\ndf_2019.columns = df_2019.iloc[0]\ndf_2019=df_2019.drop([0])\npd.options.display.max_columns = None\n\n#Importing the 2018 Dataset\ndf_2018 = pd.read_csv('..\/input\/kaggle-survey-2018\/multipleChoiceResponses.csv')\ndf_2018.columns = df_2018.iloc[0]\ndf_2018=df_2018.drop([0])\n\n#Importing the 2017 Dataset\ndf_2017=pd.read_csv('..\/input\/kaggle-survey-2017\/multipleChoiceResponses.csv',encoding='ISO-8859-1')\n\n#Removing everyone that took less than 4 minutes\nless3 = df_2019[round(df_2019.iloc[:,0].astype(int) \/ 60) <= 4].index\ndf_2019 = df_2019.drop(less3, axis=0)\n\nless3 = df_2018[round(df_2018.iloc[:,0].astype(int) \/ 60) <= 4].index\ndf_2018 = df_2018.drop(less3, axis=0)\ndisplay(df_2017)\ndf_2017.columns.tolist().index('Tenure')\n\n\n\n#Creating a smaller subset of the data\ncompanyInfo17 = df_2017[df_2017['Country'] == 'United States'].iloc[:,[1,54,8,56]]\ncompanyInfo18 = df_2018[df_2018['In which country do you currently reside?'] == 'United States of America'].iloc[:,[4,5,7,127]]\ncompanyInfo17.columns = companyInfo18.columns = ['Country', 'Degree', 'Title','Experience']\nUSA_2019 = df_2019[df_2019['In which country do you currently reside?'] == 'United States of America']\ncompanyInfo19 = USA_2019.iloc[:,[4,5,6,8,9,10,11,12,13,14,15,16,17,18,20,21,55]].copy()\n#companyInfo18 = df_2018.iloc[:,[4,5,7,10,11,12,13,14,15,16,17,18,20,21]].copy()\n#Renaming Columns\ncols = ['Country', 'Degree', 'Title', 'Size of Company', 'Size of Team', 'Machine Learning Methods', 'Analyze and understand data to influence product or business decisions', 'Build and_or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data','Build prototypes to explore applying machine learning to new areas', 'Build and\/or run a machine learning service that operationally improves my product or workflows', 'Experimentation and iteration to improve existing ML models', 'Do research that advances the state of the art of machine learning', 'None', 'Other', 'Compensation', 'Money Spent on Product','Experience']\ncompanyInfo19.columns = cols\nmed_companyInfo19 = companyInfo19[companyInfo19['Size of Company']== '250-999 employees']\n\n#Help 2017 Titles Match\nchangeF = ['Software Developer\/Software Engineer', 'Scientist\/Researcher', 'Researcher']\nchangeT = ['Software Engineer', 'Research Scientist', 'Research Scientist']\ncompanyInfo17 = companyInfo17.replace(changeF,changeT)\n\ndisplay(companyInfo19)\n","bf61f2c6":"##AVG TEAM SIZE BY COMPANY SIZE\n\nnumericalTeamSizes = []\nfor size in companyInfo19['Size of Team']:\n    if size == '20+':\n        numericalTeamSizes.append(20)\n    elif size == '15-19':\n        numericalTeamSizes.append(17)\n    elif size == '10-14':\n        numericalTeamSizes.append(12)\n    elif size == '5-9':\n        numericalTeamSizes.append(7)\n    elif size == '3-4':\n        numericalTeamSizes.append(3.5)\n    elif size == '1-2':\n        numericalTeamSizes.append(1.5)\n    elif size == '0':\n        numericalTeamSizes.append(0)\n    else:\n        numericalTeamSizes.append(np.nan)\n        \ncompanyInfo19['numericalTeamSizes'] = numericalTeamSizes\nmeanTmSz=[companyInfo19[companyInfo19['Size of Company'] == companySize].numericalTeamSizes.mean() for companySize in companyInfo19['Size of Company'].unique()]\n\ncompanySizes = companyInfo19['Size of Company'].unique()\ncorrectOrder = [5, 1, 2, 4, 0]\nchartData = np.vstack(([meanTmSz[i] for i in correctOrder],[companySizes[i] for i in correctOrder]))\n\nfig = go.Figure([go.Bar(x=chartData[1,:], y=chartData[0,:], hovertemplate = '<i>Company Size: %{x} <\/i> <br> Mean Team Size: %{y} <extra><\/extra>')])\nfig.update_xaxes(title_text='Company Size')\nfig.update_yaxes(title_text='Mean Data Science Team Size')\nfig.update_layout(\n    hoverlabel_align = 'right', \n    title = \"Data Science Team Size vs. Company Size\")\n\nfig.show()","4c41bad1":"import pandas as pd\nimport matplotlib.pyplot as plt\n#import seaborn as sns\ncdata = pd.DataFrame(sp[k19mr_usa])\ncompany_size_category = pd.api.types.CategoricalDtype(categories=orders[6][::-1], ordered=True)\nml_team_size_category = pd.api.types.CategoricalDtype(categories=orders[7], ordered=True)\ncdata['Organization Size'] = cdata['Q6'].astype(company_size_category)\ncdata['ML Team Size'] = cdata['Q7'].astype(ml_team_size_category)\ndel cdata['Q6']\ndel cdata['Q7']\ndf2 = cdata.groupby(['Organization Size', 'ML Team Size'])['Organization Size'].count().unstack('ML Team Size').fillna(0)\ndf2.plot(kind='barh', stacked=True, figsize=(14,5));","822b0647":"## Nathan continued\n##TITLE BY TEAMZISE SUNBURST PLOT\n#Get titles for each teamsize\ntitles = companyInfo19.iloc[:,2].dropna().unique()\nteamSizes = companyInfo19.iloc[:,4].dropna().unique()\nteamCounts = []\ni = 0\ntitleCounts = np.zeros((teamSizes.__len__(), titles.__len__()))\n#titlesByTeamSize\nfor team in teamSizes:\n    tempSubset = companyInfo19[companyInfo19['Size of Team'] == team]\n    teamCounts.append(tempSubset.iloc[:,4].count())\n    tempList = []\n    for title in titles:\n        tempList.append(tempSubset[tempSubset['Title'] == title].iloc[:,2].count())  \n    titleCounts[i,:] = tempList\n    i += 1\n#Get data in the correct format for a sunburst plot\nimport plotly.graph_objects as go\ncenterText = 'North American Companies'\nlabels1 = np.concatenate(('Team Size: ' + teamSizes, titles, titles, titles, titles, titles, titles, titles), axis=0)\nparents1 = np.concatenate((np.repeat(centerText,7),np.repeat(\"20+\",12),np.repeat(\"1-2\",12),\\\n                            np.repeat(\"10-14\",12),np.repeat(\"3-4\",12),np.repeat(\"5-9\",12),np.repeat(\"15-19\",12),np.repeat(\"0\",12)), axis=0)\n\nvalues1 = np.concatenate((np.sum(titleCounts,axis=1), titleCounts[0,:], titleCounts[1,:], titleCounts[2,:], titleCounts[3,:], titleCounts[4,:], titleCounts[5,:], titleCounts[6,:]), axis=0)\n\nids1 = np.concatenate((teamSizes, ['20+' + title for title in titles], ['1-2' + title for title in titles], ['10-14' + title for title in titles],\\\n                       ['3-4' + title for title in titles], ['5-9' + title for title in titles], ['15-19' + title for title in titles], ['0' + title for title in titles]), axis=0)\n\n\nsunburst = pd.DataFrame({'Ids': np.insert(ids1,0,centerText),\n                        'Labels': np.insert(labels1,0,centerText),\n                        'Parents': np.insert(parents1,0,\"\"),\n                       'Values': np.insert(values1,0,np.sum(titleCounts))})\nsunburst['Percents'] = Percents = sunburst.Values\/[sunburst[sunburst.Ids == parent].Values for parent in sunburst.Parents]*100\n\n#RemoveSlices with zero values\nsunburst = sunburst[sunburst['Values']!=0]\n\n\n#Plot Data\nfig =go.Figure(go.Sunburst(\n    ids = sunburst.Ids,\n    labels = sunburst.Labels,\n    parents = sunburst.Parents,\n    values = sunburst.Values,\n    branchvalues = \"total\",\n    hovertemplate='<b>%{label} <\/b> <br> Responses: %{value}<br> <extra><\/extra>',\n))\nfig.update_layout(margin = dict(t=0, l=0, r=0, b=0))\nfig.update_layout(showlegend=True)\nfig.show()","cfec5723":"##TITLE BY TEAMZISE SUNBURST PLOT FOR MEDIUM COMPANIES ONLY\n#Get titles for each teamsize\ntitles = med_companyInfo19.iloc[:,2].dropna().unique()\nteamSizes = med_companyInfo19.iloc[:,4].dropna().unique()\nteamCounts = []\ni = 0\ntitleCounts = np.zeros((teamSizes.__len__(), titles.__len__()))\n#titlesByTeamSize\nfor team in teamSizes:\n    tempSubset = med_companyInfo19[med_companyInfo19['Size of Team'] == team]\n    teamCounts.append(tempSubset.iloc[:,4].count())\n    tempList = []\n    for title in titles:\n        tempList.append(tempSubset[tempSubset['Title'] == title].iloc[:,2].count())  \n    titleCounts[i,:] = tempList\n    i += 1\n\n\n#Get data in the correct format for a sunburst plot\nimport plotly.graph_objects as go\ncenterText = 'Mid-Size<br>North American Companies'\ntitleNum = len(titles)\nteamSizeNum = len(teamSizes)\n\nlabels1 = np.concatenate(('Team Size: ' + teamSizes, 'team size' + titles, titles, titles, titles, titles, titles, titles), axis=0)\n\nparents1 = np.concatenate((np.repeat(centerText,teamSizeNum),np.repeat(\"10-14\",titleNum),np.repeat(\"3-4\",titleNum),\\\n                            np.repeat(\"20+\",titleNum),np.repeat(\"5-9\",titleNum),np.repeat(\"1-2\",titleNum),np.repeat(\"15-19\",titleNum),np.repeat(\"0\",titleNum)), axis=0)\n\nvalues1 = np.concatenate((np.sum(titleCounts,axis=1), titleCounts[0,:], titleCounts[1,:], titleCounts[2,:], titleCounts[3,:], titleCounts[4,:], titleCounts[5,:], titleCounts[6,:]), axis=0)\n\nids1 = np.concatenate((teamSizes, ['20+' + title for title in titles], ['1-2' + title for title in titles], ['10-14' + title for title in titles],\\\n                       ['3-4' + title for title in titles], ['5-9' + title for title in titles], ['15-19' + title for title in titles], ['0' + title for title in titles]), axis=0)\n\n\nsunburst = pd.DataFrame({'Ids': np.insert(ids1,0,centerText),\n                        'Labels': np.insert(labels1,0,centerText),\n                        'Parents': np.insert(parents1,0,\"\"),\n                       'Values': np.insert(values1,0,np.sum(titleCounts))})\nsunburst['Percents'] = Percents = sunburst.Values\/[sunburst[sunburst.Ids == parent].Values for parent in sunburst.Parents]*100\n\n#RemoveSlices with zero values\nsunburst = sunburst[sunburst['Values']!=0]\n\n\n#Plot Data\nfig =go.Figure(go.Sunburst(\n    ids = sunburst.Ids,\n    labels = sunburst.Labels,\n    parents = sunburst.Parents,\n    values = sunburst.Values,\n    branchvalues = \"total\",\n    hovertemplate='<b>%{label} <\/b> <br> Responses: %{value}<br> <extra><\/extra>',\n))\nfig.update_layout(margin = dict(t=0, l=0, r=0, b=0))\nfig.update_layout(showlegend=True)\nfig.show()","5a1ee586":"titlesToCount = ['DBA\/Database Engineer', 'Statistician', 'Data Scientist', 'Software Engineer', 'Data Analyst', 'Research Scientist', 'Business Analyst']\n\n\ntitleCount19 = [companyInfo19[companyInfo19.Title == title].Title.count()\/companyInfo19.Title.count()*100 for title in titlesToCount]\ntitleCount18 = [companyInfo18[companyInfo18.Title == title].Title.count()\/companyInfo18.Title.count()*100 for title in titlesToCount]\ntitleCount17 = [companyInfo17[companyInfo17.Title == title].Title.count()\/companyInfo17.Title.count()*100 for title in titlesToCount]\n\ntitleCountByYear = pd.DataFrame([titleCount17,titleCount18, titleCount19], columns = titlesToCount)\ntitleCountByYear.index = [2017,2018,2019]\n\n    \nfig = go.Figure()\nfor title in titlesToCount:\n    fig.add_trace(go.Scatter(x=[2017, 2018, 2019], y=titleCountByYear[title],\n                             mode='lines',\n                             name=title,\n                            ))\nfig.update_xaxes(title_text='Survey Year', dtick=1)\nfig.update_yaxes(title_text='Response Frequency (Percent)')\nfig.update_layout(\n    hoverlabel_align = 'right', \n    title = \"Title Response Frequency by Year\")\n   \n\nfig.show()","bcc3ed8b":"##TITLE BY TEAMZISE SUNBURST PLOT\n#Get titles for each teamsize\ntitles = companyInfo19.iloc[:,2].dropna().unique()\ncompanySizes = companyInfo19['Size of Company'].dropna().unique()\n\ncorrectOrder = [4,1,2,3,0]\ncompanySizes = [companySizes[i] for i in correctOrder]\n\nteamCounts = []\ni = 0\ntitleCounts = np.zeros((companySizes.__len__(), titles.__len__()))\n#titlesByTeamSize\nfor size in companySizes:\n    tempSubset = companyInfo19[companyInfo19['Size of Company'] == size]\n    teamCounts.append(tempSubset.iloc[:,3].count())\n    tempList = []\n    for title in titles:\n        tempList.append(tempSubset[tempSubset['Title'] == title].iloc[:,2].count())  \n    titleCounts[i,:] = tempList\n    i += 1\n\n\n#Get data in the correct format for a sunburst plot\nimport plotly.graph_objects as go\ncenterText = 'North American Companies'\nrows = len(companySizes)\ncols = len(titles)\n\nlabels1 = np.concatenate((companySizes, np.tile(titles,rows)), axis=0)\n\nparents1 = np.concatenate((np.repeat(centerText,rows),np.repeat(companySizes,cols)), axis=0)\n\nvalues1 = np.concatenate((np.sum(titleCounts,axis=1), np.asarray(titleCounts).reshape(-1)), axis=0)\n\nids1 = np.concatenate((companySizes, np.asarray([[company + title for title in titles]for company in companySizes]).reshape(-1)), axis=0)\n\n\n\nsunburst = pd.DataFrame({'Ids': np.insert(ids1,0,centerText),\n                        'Labels': np.insert(labels1,0,centerText),\n                        'Parents': np.insert(parents1,0,\"\"),\n                       'Values': np.insert(values1,0,np.sum(titleCounts))})\n#sunburst['Percents'] = sunburst.Values\/[sunburst[sunburst.Ids == parent].Values for parent in sunburst.Parents]*100\n\n#RemoveSlices with zero values\nsunburst = sunburst[sunburst['Values']!=0]\n\n\n#Plot Data\nfig =go.Figure(go.Sunburst(\n    ids = sunburst.Ids,\n    labels = sunburst.Labels,\n    parents = sunburst.Parents,\n    values = sunburst.Values,\n    branchvalues = \"total\",\n    hovertemplate = '<b>%{label}<\/b><br><br>Responses: %{value}<extra><\/extra>'\n   # hovertemplate =\n    #'<b>%{label}<\/b>'+\n    #'<br>Percent: %{hovertext}<br>'+ \n    #'Responses: %{value}<br> <extra><\/extra>',\n    #hovertext = ['{}%'.format(i) for i in sunburst.Percents]\n))\nfig.update_layout(margin = dict(t=0, l=0, r=0, b=0))\nfig.update_layout(title = 'Role Prevalence by Company Size')\nfig.show()","ae85c5b2":"#AVG TEAM SIZE BY COMPANY SIZE\n\nnumericalYearsExperience = []\nfor years in companyInfo19.Experience:\n    if years == 'I have never written code':\n        numericalYearsExperience.append(0)\n    elif years == '3-5 years':\n        numericalYearsExperience.append(4)\n    elif years == '< 1 years':\n        numericalYearsExperience.append(0.5)\n    elif years == '1-2 years':\n        numericalYearsExperience.append(1.5)\n    elif years == '5-10 years':\n        numericalYearsExperience.append(7.5)\n    elif years == '10-20 years':\n        numericalYearsExperience.append(15)\n    elif years == '20+ years':\n        numericalYearsExperience.append(20)\n    else:\n        numericalYearsExperience.append(np.nan)\n        \ncompanyInfo19['numericalYearsExperience'] = numericalYearsExperience\nmeanYearsExperience=[companyInfo19[companyInfo19['Size of Company'] == companySize].numericalYearsExperience.mean() for companySize in companyInfo19['Size of Company'].unique()]\n\ncompanySizes = companyInfo19['Size of Company'].unique()\ncorrectOrder = [5, 1, 2, 4, 0]\nchartData = np.vstack(([meanYearsExperience[i] for i in correctOrder],[companySizes[i] for i in correctOrder]))\n\nfig = go.Figure([go.Bar(x=chartData[1,:], y=chartData[0,:], hovertemplate = '<i>Company Size: %{x} <\/i> <br> Mean Data Science Experience: %{y} <extra><\/extra>')])\nfig.update_xaxes(title_text='Company Size')\nfig.update_yaxes(title_text='Mean Data Science Experience (years)')\nfig.update_layout(\n    hoverlabel_align = 'right', \n    title = \"Data Science Experience vs. Company Size\")\n\nfig.show()","1866ef60":"import pandas as pd\nimport matplotlib.pyplot as plt\n#import seaborn as sns\ncdata = pd.DataFrame(sp[k19mr_usa])\ncompany_size_category = pd.api.types.CategoricalDtype(categories=orders[6][::-1], ordered=True)\neducation_category = pd.api.types.CategoricalDtype(categories=orders[23], ordered=True)\ncdata['Company Size'] = cdata['Q6'].astype(company_size_category)\ncdata['Experience'] = cdata['Q23'].astype(education_category)\ndel cdata['Q6']\ndel cdata['Q23']\ndf2 = cdata.groupby(['Company Size', 'Experience'])['Company Size'].count().unstack('Experience').fillna(0)\ndf2.plot(kind='barh', stacked=True, figsize=(14,5));","b379504e":"companyInfo19['numericalYearsExperienceCompat'] = companyInfo19.numericalYearsExperience.replace([15,20],10)\n\nnumericalYearsExperience = []\nfor years in companyInfo18.Experience:\n    if (years == 'I have never written code but I want to learn')|(years == 'I have never written code and I do not want to learn'):\n        numericalYearsExperience.append(0)\n    elif years == '3-5 years':\n        numericalYearsExperience.append(4)\n    elif years == '< 1 years':\n        numericalYearsExperience.append(0.5)\n    elif years == '1-2 years':\n        numericalYearsExperience.append(1.5)\n    elif years == '5-10 years':\n        numericalYearsExperience.append(7.5)\n    elif years == '10-20 years':\n        numericalYearsExperience.append(10)\n    elif (years == '20-30 years')|(years == '30-40 years')|(years == '40+ years'):\n        numericalYearsExperience.append(10)\n    else:\n        numericalYearsExperience.append(np.nan)        \ncompanyInfo18['numericalYearsExperience'] = numericalYearsExperience\n\nnumericalYearsExperience = []\nfor years in companyInfo17.Experience:\n    if years == 'I don\\'t write code to analyze data':\n        numericalYearsExperience.append(0)\n    elif years == '3 to 5 years':\n        numericalYearsExperience.append(4)\n    elif years == 'Less than a year':\n        numericalYearsExperience.append(0.5)\n    elif years == '1 to 2 years':\n        numericalYearsExperience.append(1.5)\n    elif years == '6 to 10 years':\n        numericalYearsExperience.append(7.5)\n    elif years == 'More than 10 years':\n        numericalYearsExperience.append(10)\n    else:\n        numericalYearsExperience.append(np.nan)        \ncompanyInfo17['numericalYearsExperience'] = numericalYearsExperience\n\n    \nfig = go.Figure()\nfig.add_trace(go.Scatter(x=[2017, 2018, 2019], y=[companyInfo17['numericalYearsExperience'].mean(),companyInfo18['numericalYearsExperience'].mean(),companyInfo19['numericalYearsExperienceCompat'].mean()], mode='lines', name=title))\nfig.update_xaxes(title_text='Survey Year', dtick=1)\nfig.update_yaxes(title_text='Mean Data Science Experience (Years)', range = [0.1, 10])\nfig.update_layout(\n    hoverlabel_align = 'right', \n    title = \"Data Science Experience by Survey Year\")\n   \n\nfig.show()","2f573eb8":"#Avg Year of Education BY COMPANY SIZE\neducation = []\nfor degree in companyInfo19.Degree:\n    if degree == 'No formal education past high school':\n        education.append(0)\n    elif degree == 'Some college\/university study without earning a bachelor\u2019s degree':\n        education.append(2)\n    elif degree == 'Professional degree':\n        education.append(7)\n    elif degree == 'Bachelor\u2019s degree':\n        education.append(4)\n    elif degree == 'Master\u2019s degree':\n        education.append(6)\n    elif degree == 'Doctoral degree':\n        education.append(11)\n    else:\n        education.append(np.nan)\n        \ncompanyInfo19['Education'] = education\nmeanYearsEducation=[companyInfo19[companyInfo19['Size of Company'] == companySize].Education.mean() for companySize in companyInfo19['Size of Company'].unique()]\n\ncompanySizes = companyInfo19['Size of Company'].unique()\ncorrectOrder = [5, 1, 2, 4, 0]\nchartData = np.vstack(([meanYearsEducation[i] for i in correctOrder],[companySizes[i] for i in correctOrder]))\n\nfig = go.Figure([go.Bar(x=chartData[1,:], y=chartData[0,:], hovertemplate = '<i>Company Size: %{x} <\/i> <br> Mean Years of Education: %{y} <extra><\/extra>')])\nfig.update_xaxes(title_text='Company Size')\nfig.update_yaxes(title_text='Mean Years of Secondary Education')\nfig.update_layout(\n    hoverlabel_align = 'right', \n    title = \"Data Science Employee Education by Company Size\")\n\nfig.show()","10aaeb95":"import pandas as pd\nimport matplotlib.pyplot as plt\n#import seaborn as sns\ncdata = pd.DataFrame(sp[k19mr_usa])\ncompany_size_category = pd.api.types.CategoricalDtype(categories=orders[6][::-1], ordered=True)\neducation_category = pd.api.types.CategoricalDtype(categories=orders[4], ordered=True)\ncdata['Company Size'] = cdata['Q6'].astype(company_size_category)\ncdata['Education'] = cdata['Q4'].astype(education_category)\ndel cdata['Q6']\ndel cdata['Q4']\ndf2 = cdata.groupby(['Company Size', 'Education'])['Company Size'].count().unstack('Education').fillna(0)\ndf2.plot(kind='barh', stacked=True, figsize=(14,5));","6b16cbbd":"import pandas as pd\nimport matplotlib.pyplot as plt\n#import seaborn as sns\ncdata = pd.DataFrame(sp[k19mr_usa])\ncompany_size_category = pd.api.types.CategoricalDtype(categories=orders[6][::-1], ordered=True)\nsalary_category = pd.api.types.CategoricalDtype(categories=orders[10], ordered=True)\ncdata['Company Size'] = cdata['Q6'].astype(company_size_category)\ncdata['Salary'] = cdata['Q10'].astype(salary_category)\ndel cdata['Q6']\ndel cdata['Q10']\ndf2 = cdata.groupby(['Company Size', 'Salary'])['Company Size'].count().unstack('Salary').fillna(0)\ndf2.plot(kind='barh', stacked=True, figsize=(14,5)).legend(loc='center left', bbox_to_anchor=(1.0, 0.5));\n#plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))","73fc02ef":"Now we have a better idea of the positions of individuals that are involved with practicing Data Science.  But what kind of experience do they have?   \n\n## How experienced are the individuals?\n\nData Science is a rapidly growing field.  What kind of real experience do individuals working in it have?  The following chart shows the average level of experience for respondents across organization sizes.  It's interesting how the smallest size organizations have the second highest level of experience. ","2c1ee68c":"Now let's take a look at the relationship between organization size and position.  If we assume that a disproportionately high percentage of the smallest companies are consulting firms (which seems like a fair assumption) we can see that software enginners are much bigger deal in the biggest companies and, conversely, data analysts are not quite as common.","72f400df":"For individual giving, the reported decline of over 10 billion dollars in 2018 is the biggest since the decline of over 32 billion dollars in 2008.  The financial crisis of 2007-2008 and subsequent great recession are likely factors for the 2008 decline.  A 2018 decline in giving might be a sign that the economy is heading towards a recession, but there are other reasons to consider.    \n\nOne of the reasons giving maybe down is because of the 2017 tax changes. According to [Forbes magazine](https:\/\/www.forbes.com\/sites\/nextavenue\/2019\/06\/18\/charitable-giving-took-a-hit-due-to-tax-reform\/#3184c5b6f6ff): \n> The chief reason: the doubling of the standard deduction (to $24,000 for married couples filing jointly) would mean it wouldn\u2019t pay for many to itemize on their 2018 tax returns, and without a charitable contribution deduction, they\u2019d be less inclined to give. New data shows the fear seems to have been proven out. Typically, individual giving will track with growth in GPD [Gross Domestic Product], which was pretty robust in 2018, and with growth in household income, which was also strong,\u2019 said Rick Dunham, chair of the Giving USA Foundation. The tax changes may be why individual giving was flat overall.\u2019\n\nCharities may also feel the effects of a bad stock market, as they did in 2018:\n\n<img align=\"left\" style=\"margin-left: -20px; height: 300px;\" \nsrc=\"https:\/\/ei.marketwatch.com\/Multimedia\/2018\/10\/24\/Photos\/ZH\/MW-GS378_stock__20181024171646_ZH.jpg?uuid=1ceb36f0-d7d2-11e8-97f7-ac162d7bc1f7\">","fc64b498":"## How much are individuals being paid?","b4ce2972":"When you further filter this chart to only include medium size companies, the team size of 3-4 members becomes the most common.  In addition, the Statistician and DBA\/Database Engineer positions have disappeared.","5434f508":"## Which positions are being staffed for data science?\n\nThe obvious answer to this question and largest single group of respondents are Data Scientists.  But how big a part do they actually play?  And what other positions are working with them?  Lets take a look at the survey data from North America. \n\nData Scientists appear to make up approximately a third of the positions reported.  Other prominent positions represented are those in Engineer and Analyst positions.  Data Science is famously represented as requiring the convergence of Subject Matter, Programming, and Mathematics expertise.  These requirements correlate to the primary positions of Analysts (Subject Matter), Engineers (Programming), and Scientists (Mathematics).  And while it's more likely that a lot of the smaller teams from 1-4 members don't have a Data Scientist at all, the likelihood of having one appears to increase with team size.    ","bd77287b":"We have some opportunities to make it better with data science. \n\n- An automatic monthly giving program is perhaps the most effective way to increase donor retention that many charities have still yet to put into practice.  The capability is easy to acquire, but signing up donors takes time.  In this case, a supervised machine learning algorithm could be used to identify the donors that are the most likely to sign up for an automated monthly giving program so we can streamline their adoption.  \n- Supervised machine learning could also be used to model the probability that a donor will lapse, so a charity can prioritize their follow-ups and minimize losses. \n- A model for determining the next best action to take with a donor can be trained from a history of other donor actions with their results.  The lack of action history and their results might make this harder.  A charity should consider the use of a donor management system so that contact history and results can be captured for training.    \n\n### Donor Acquisition\nAccording to [DonorSearch](https:\/\/www.donorsearch.net\/),one major gift can be the difference between a charity meeting its goal and that same organization coming up short. Successful charities understand this fact and know the significance of running a top-tier program. As with Donor Retention, we have [some opportunities](https:\/\/www.donorsearch.net\/artificial-intelligence-for-nonprofits\/) to use Machine Learning and Data Science to make it better. Here are some others:\n\n- Supervised learning could be used to model the lifetime value of acquired donors, so that candidate donors can be prioritized based on their value. This is even more powerful if the model can be trained with donors from multiple charities.  \n- Unsupervised machine learning could be used to cluster segments of candidates based on their similarity to each other, so a charity can strategize and make effective solicitation campaigns.   \n\n### Gift Optimization\nCharities could improve their growth in giving results by applying Machine Learning and Data Science for optimizing the best time and amount to ask the donor.\n\n- Supervised machine learning could be used to train a model with past ask amounts and their results, so that we can predict ask amounts that maximize contributions and minimize losses. This is especially effective with a monthly giving program.  \n- A model for determining the next best action to take could also work toward increasing gift amounts. \n\nSo there it is.  We have more than enough opportunities to make good use of Machine Learning and Data Science. Now lets explore how people in organizations are approaching their data science practice.  Specifically, we want to explore and fully understand the adoption, costs, tools, team composition, roles, practices and tools involved with applying Machine Learning and Data Science for a small organization like a charity. ","cb5fb93c":"The 2019 Kaggle Machine Learning and Data Science survey data has been published, and now we have another opportunity for great insights with the most comprehensive dataset available on the state of Machine Learning and Data Science.  Lets do some good!\n\nIts fitting that the deadline for this challenge comes on the eve of [GivingTuesday](https:\/\/www.givingtuesday.org\/), a day created during the busy holiday season to focus on giving back. Charities use this day to remind people not to forget the causes that need their money, time and attention. The people who start or work at these charities run the gamut from full time professionals devoted to fundraising, to a person who simply has a passionate interest in helping a cause that's dear to them. Their knowledge of how to raise money varies too in terms of experience, tools and resources. Perhaps there is no greater area where machine learning and data science can be employed to help so many achieve so much for the good of others.\n\n# Challenges for Charities\n\n2018 was a tough year for many charities. According to an [article](https:\/\/www.philanthropy.com\/article\/Gifts-to-Charity-Dropped-17\/246511) published earlier this year by The Chronicle of Philanthropy, the [Giving USA Foundation](https:\/\/givingusa.org\/) estimates a total decline in 2018 U.S. giving of over 7 billion dollars.  And individual donors, the largest group of U.S. givers, are alone estimated to have a decline of over 10 billion dollars:  ","5a1525cf":"So how do team sizes get distributed across difference organization sizes?  The following chart shows the breakdown of team sizes in the U.S. within each of the surveyed organization sizes.  A particularly interesting insight from this chart is how common small teams are at larger organizations. As for the smaller organizations, the majority appear to use Data Science teams of up to 4 individuals.  ","de37fffe":"## *Exploratory Analysis of the 2019 Kaggle ML and DS Survey* \n# Helping Small Charities Thrive\n<img style=\"height: 320px;\" align=\"left\" src=\"https:\/\/www.publicdomainpictures.net\/pictures\/290000\/velka\/charity-donation.jpg\">","22e2c12c":"The [2019 Kaggle ML & DS Survey](https:\/\/www.kaggle.com\/c\/kaggle-survey-2019) provides the most comprehensive dataset available on the state of machine learning and data science, and can help us answer important questions about how organizations are applying Machine Learning and Data Science.  Let's explore a few of them... \n","c0ab1784":"## How big are data science teams?\n\nSo how big are the teams working on data science initiatives? And how does it relate to organization size? \n\nThe Kaggle survey included the following questions:\n\n> 6. What is the size of the company where you are employed?\n> 7. Approximately how many individuals are responsible for data science workloads at your place of business?\n\nAs we might expect, larger companies tend to have larger teams.  And on average, smaller organizations tend to have between 3 and 6 people working in Data Science.   ","6e63a73f":"So lets take a look at how experience is distributed across company size.  The next chart reveals that half of the respondents have 3 or less years experience!  This is a classic example og the median (~3) being different than the average (~7).   ","5d388dea":"Also, a growing number of charities are competing for scarce resources. According to [Bloomerang](https:\/\/bloomerang.co\/blog\/should-hundreds-of-new-nonprofits-be-created-each-year\/):\n> The number of 501c3 organizations grew by 29.7% from 2003 through 2013.\n\nThey listed other issues facing nonprofits. \n\n- Most new nonprofits overlap services and missions with existing orgs who are better equipped to properly deliver\n- The majority of new nonprofits never even reach $100,000 in annual revenue and may be unable to truly perform the mission intended\n- Underfunded nonprofits may do as much harm as good\n- Proper staffing is hard for brand new nonprofits with tiny budgets\n- Founders may not be equipped to lead if successful\n- Volunteers, which are often critical may be spread too thin\n\nThey stated the odds of survival after 5 years for small nonprofits are below 2-3%.  This means all those people who take the time, effort and risk to help a case their passionate about won\u2019t be able to do so.\n\nIf that wasn't challenging enough, [recent reductions](https:\/\/patimes.org\/bracing-for-government-funding-cuts-a-call-to-action-for-nonprofit-leaders\/) in government funding have placed further demands on charities.  \n\n> when experiencing uncertainty or declining revenues, state and local government agencies that provide financial support to nonprofits may significantly reduce funding, in turn requiring nonprofits take on more responsibilities with fewer resources.\n\nThe collective challenges are significant. Charitable giving in the U.S. is on the decline, and charities are suffering as a result.","2b6760cb":"So how has the breakdown of these positions changes over the past few years.  Machine Learning and Data Science are rapidly growing fields.  Lets get some insight into the matter.  \n\nThis chart shows the response rates for various data science job positions over the past three years based on the US kaggle survey responses. To make this chart, title options that had no corollary with other years were eliminated. Additionally the 2017 survey's titles 'Scientist\/Researcher' and 'Researcher' were both folded into the \"research scientist\" category for that year (this may explain the 10% drop in that category from 2017 to 2019).\n\nIn general, we see here that \"Data Scientist is by far the most common job title among this demographic.  Notice how both Scientist positions have increased in percent while the Engineers and Analysts were comparatively flat.  ","a7dcf13b":"Ok, that gives us a better understand of the experience of individuals working with Machine Learning and Data Science.  Now lets look at education. \n\n## How educated are the individuals?","50727ff7":"# How are small organizations applying Machine Learning and Data Science?\n<img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/16394\/logos\/header.png\" align=\"left\">","d9a8b1da":"Now lets take a look at how experience has changed.  The average data science experience of respondents has stayed relatively the same over the last 3 years. This would suggest that the number of newcomers is enough to offset the inveitable aging of the current population. Unfortunately the largest experience option for the 2017 survey was 'More than 10 years,' thus the averages for all options greater than this in the 2018 and 2019 surveys were set to 10 in order to normalize data across survey years.","6037e965":"So now we have a better idea of how big the teams are based on the size of the organization.  Next we'll take a look at the various positions on these teams. ","3fa3d07d":"# How Can Machine Learning & Data Science Help?\n\nWith these challenges, charities need new ways to improve their fundraising results.  Here's a look at some of the opportunities for improvement and where it might make sense to apply Machine Learning and Data Science:\n\n### Donor Retention \nThe [Fundraising Effectiveness Project](http:\/\/afpfep.org\/blog\/fundraising-effectiveness-project-quarterly-fundraising-report-for-q4-2018\/) has been collecting data on donor retention since 2006, and they have prioritized donor retention as a primary measure to improve fundraising effectiveness. Unfortunately, the situation has not improved greatly over the years: overall YTD Donor Retention at the end of 2018 fell 6.3% to 44.5%. Smaller charities are more likely to lose donors than larger ones because they are not equipped with the resources to keep them.\n\n<img align=\"left\" src=\"http:\/\/afpfep.org\/wp-content\/uploads\/2019\/02\/fepq42018.png\">\n"}}