{"cell_type":{"ae2a4285":"code","832d4e89":"code","d9f73b57":"code","70c1dcfa":"code","7fa5761d":"code","61f96b4c":"code","c470db0f":"markdown","ebbf28c4":"markdown","39127864":"markdown"},"source":{"ae2a4285":"# \u5bfc\u5165\u5fc5\u8981\u7684\u5305\nimport tensorflow as tf\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPool2D\nfrom tensorflow.keras.layers import concatenate\nfrom tensorflow.keras.layers import ReLU\n\n# \u6838\u521d\u59cb\u5316\nkernel_init = tf.keras.initializers.glorot_uniform()\n\n# \u504f\u7f6e\u521d\u59cb\u5316\nbias_init = tf.keras.initializers.Constant(value=0.2)\n\n\n# \u751f\u6210\u6f5c\u6df1\u6a21\u5757\uff08Inception Module\uff09\u7684\u51fd\u6570\ndef inception_module(x,\n                     filters_1x1,\n                     filters_3x3_reduce,\n                     filters_3x3,\n                     filters_5x5_reduce,\n                     filters_5x5,\n                     filters_pool_proj,\n                     name=None):\n    \"\"\"\n    \u751f\u6210GoogleNet\u7684\u6f5c\u6df1\u6a21\u5757\n    \u8fd9\u91cc\u6709\u56db\u4e2a\u8f93\u5165x,\u6d88\u89e3\u503c\u5f97\u662f\u8f93\u5165\u4e3ax\u7684\u6a21\u5757\n    Args:\n        x: \u4e0a\u4e00\u5c42\u8f93\u5165\n        filters_1x1:          1\u00d71\u5377\u79ef\u6838\u6570\u91cf\n        filters_3x3_reduce:   \u6d88\u89e3\u964d\u7ef43x3\u5377\u79ef\u76841\u00d71\u5377\u79ef\u6838\u6570\u91cf\n        filters_3x3:          3\u00d73\u5377\u79ef\u6838\u6570\u91cf\n        filters_5x5_reduce:   \u6d88\u89e3\u964d\u7ef45x5\u5377\u79ef\u76841\u00d71\u5377\u79ef\u6838\u6570\u91cf\n        filters_5x5:          5\u00d75\u5377\u79ef\u6838\u6570\u91cf\n        filters_pool_proj:    \u6d88\u89e3\u964d\u7ef4\u6700\u5927\u6c60\u5316\u76841\u00d71\u5377\u79ef\u6838\u6570\u91cf\n        name:                 \u6f5c\u6df1\u6a21\u5757\u540d\u79f0\n\n    Returns\n        \u751f\u6210\u7684\u6f5c\u6df1\u6a21\u5757\u5806\u53e0\u5408\u5e76\u7279\u5f81\u56fe\n    \"\"\"\n    # \u53ef\u4ee5\u5c06Relu\u5199\u5728conv2D\u6216\u8005\u5355\u72ec\u4f5c\u4e3a\u4e00\u5c42\n    # 1\u00d71\u5377\u79ef,\u4e8c\u884c\u7b2c\u56db\u4e2a\n    conv_1x1 = Conv2D(filters_1x1,\n                      (1, 1),\n                      padding='same',\n                      activation='relu',\n                      kernel_initializer=kernel_init,\n                      bias_initializer=bias_init)(x)\n    conv_1x1 = BatchNormalization()(conv_1x1)\n\n    # \u6d88\u89e3\u964d\u7ef43x3\u5377\u79ef\u76841\u00d71\u5377\u79ef\uff0c\u4e00\u884c\u7b2c\u4e8c\u4e2a\n    conv_1x1_for_3x3 = Conv2D(filters_3x3_reduce,\n                      (1, 1),\n                      padding='same',\n                      activation='relu',\n                      kernel_initializer=kernel_init,\n                      bias_initializer=bias_init)(x)\n    conv_1x1_for_3x3 = BatchNormalization()(conv_1x1_for_3x3)\n\n    # 3x3\u5377\u79ef\uff0c\u4e8c\u884c\u7b2c\u4e8c\u4e2a\n    conv_3x3 = Conv2D(filters_3x3,\n                      (3, 3),\n                      padding='same',\n                      activation='relu',\n                      kernel_initializer=kernel_init,\n                      bias_initializer=bias_init)(conv_1x1_for_3x3)\n    conv_3x3 = BatchNormalization()(conv_3x3)  # \u4e3a\u4e86\u907f\u514d\u68af\u5ea6\u6d88\u5931\n\n    # \u6d88\u89e3\u964d\u7ef45x5\u5377\u79ef\u76841\u00d71\u5377\u79ef\uff0c\u4e00\u884c\u7b2c\u4e8c\u4e2a\n    conv_1x1_for_5x5 = Conv2D(filters_5x5_reduce,\n                      (1, 1),\n                      padding='same',\n                      activation='relu',\n                      kernel_initializer=kernel_init,\n                      bias_initializer=bias_init)(x)\n    conv_1x1_for_5x5 = BatchNormalization()(conv_1x1_for_5x5)\n\n    # 5x5\u5377\u79ef\uff0c\u4e8c\u884c\u7b2c\u4e09\u4e2a\n    conv_5x5 = Conv2D(filters_5x5, (5, 5),\n                      padding='same',\n                      activation='relu',\n                      kernel_initializer=kernel_init,\n                      bias_initializer=bias_init)(conv_1x1_for_5x5)\n    conv_5x5 = BatchNormalization()(conv_5x5)\n\n    # \u6700\u5927\u6c60\u5316\uff0c\u4e00\u884c\u7b2c\u4e00\u4e2a\n    pool = MaxPool2D((3, 3), strides=(1, 1), padding='same')(x)\n\n    # \u6d88\u89e3\u964d\u7ef4\u6700\u5927\u6c60\u5316\u76841\u00d71\u5377\u79ef\uff0c\u4e8c\u884c\u7b2c\u4e00\u4e2a\n    pool_proj = Conv2D(filters_pool_proj,\n                       (1, 1),\n                       padding='same',\n                       activation='relu',\n                       kernel_initializer=kernel_init,\n                       bias_initializer=bias_init)(pool)\n    pool_proj = BatchNormalization()(pool_proj)\n\n    # \u5806\u53e0\u5408\u5e76\uff0c\u6700\u540e\n    output = concatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis=3, name=name)\n\n    return output","832d4e89":"# \u5bfc\u5165\u5fc5\u987b\u7684\u5305\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\n\n\n# \u5b9a\u4e49 GoogleNet\u7c7b\nclass GoogleNet:\n    @staticmethod\n    def build(width, height, channel, classes):\n        \"\"\"\n        \u6839\u636e\u8f93\u5165\u6837\u672c\u7684\u7ef4\u5ea6\uff08width\u3001height\u3001channel\uff09\uff0c\u5206\u7c7b\u6570\u91cf\u521b\u5efaGoogleNet\u7f51\u7edc\u6a21\u578b\n        Args:\n            width:   \u8f93\u5165\u6837\u672c\u7684\u5bbd\u5ea6\n            height:  \u8f93\u5165\u6837\u672c\u7684\u9ad8\u5ea6\n            channel: \u8f93\u5165\u6837\u672c\u7684\u901a\u9053\n            classes: \u5206\u7c7b\u6570\u91cf\n\n        Returns:\n           GoogleNet\u7f51\u7edc\u6a21\u578b\u5bf9\u8c61\n\n        \"\"\"\n\n        input_layer = Input(shape=(width, height, channel))\n\n        # \u6838\u521d\u59cb\u5316\n        kernel_init = tf.keras.initializers.glorot_uniform()\n\n        # \u504f\u7f6e\u521d\u59cb\u5316\n        bias_init = tf.keras.initializers.Constant(value=0.2)\n\n        # \u5377\u79ef\n        x = Conv2D(64,\n                   (7, 7),\n                   padding='same',\n                   strides=(2, 2),\n                   activation='relu',\n                   name='conv_1_7x7\/2')(input_layer)\n        x = BatchNormalization()(x)\n\n        # \u6700\u5927\u6c60\u5316\n        x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_1_3x3\/2')(x)\n\n        # \u5377\u79ef\n        x = Conv2D(64,\n                   (1, 1),\n                   padding='same',\n                   strides=(1, 1),\n                   activation='relu',\n                   name='conv_2a_3x3\/1')(x)\n        x = BatchNormalization()(x)\n\n        # \u5377\u79ef\n        x = Conv2D(192,\n                   (3, 3),\n                   padding='same',\n                   strides=(1, 1),\n                   activation='relu',\n                   name='conv_2b_3x3\/1')(x)\n        x = BatchNormalization()(x)\n\n        # \u6700\u5927\u6c60\u5316\n        x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_2_3x3\/2')(x)\n\n        # \u6f5c\u6df1\u6a21\u5757\n        x = inception_module(x,\n                             filters_1x1=64,\n                             filters_3x3_reduce=96,\n                             filters_3x3=128,\n                             filters_5x5_reduce=16,\n                             filters_5x5=32,\n                             filters_pool_proj=32,\n                             name='inception_3a')\n\n        # \u6f5c\u6df1\u6a21\u5757\n        x = inception_module(x,\n                             filters_1x1=128,\n                             filters_3x3_reduce=128,\n                             filters_3x3=192,\n                             filters_5x5_reduce=32,\n                             filters_5x5=96,\n                             filters_pool_proj=64,\n                             name='inception_3b')\n\n        # \u6700\u5927\u6c60\u5316\n        x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_3_3x3\/2')(x)\n\n        # \u6f5c\u6df1\u6a21\u5757\n        x = inception_module(x,\n                             filters_1x1=192,\n                             filters_3x3_reduce=96,\n                             filters_3x3=208,\n                             filters_5x5_reduce=16,\n                             filters_5x5=48,\n                             filters_pool_proj=64,\n                             name='inception_4a')\n\n\n        # \u8f85\u52a9\u5206\u7c7b\u5668\n        x1 = AveragePooling2D((5, 5), strides=3)(x)\n        x1 = Conv2D(128, (1, 1), padding='same', activation='relu')(x1)\n        x1 = Flatten()(x1)\n        x1 = Dense(1024, activation='relu')(x1)\n        x1 = Dropout(0.3)(x1)\n        x1 = Dense(classes, activation='softmax', name='auxilliary_output_1')(x1)\n\n\n        # \u6f5c\u6df1\u6a21\u5757\n        x = inception_module(x,\n                             filters_1x1=160,\n                             filters_3x3_reduce=112,\n                             filters_3x3=224,\n                             filters_5x5_reduce=24,\n                             filters_5x5=64,\n                             filters_pool_proj=64,\n                             name='inception_4b')\n\n        # \u6f5c\u6df1\u6a21\u5757\n        x = inception_module(x,\n                             filters_1x1=128,\n                             filters_3x3_reduce=128,\n                             filters_3x3=256,\n                             filters_5x5_reduce=24,\n                             filters_5x5=64,\n                             filters_pool_proj=64,\n                             name='inception_4c')\n\n        # \u6f5c\u6df1\u6a21\u5757\n        x = inception_module(x,\n                             filters_1x1=112,\n                             filters_3x3_reduce=144,\n                             filters_3x3=288,\n                             filters_5x5_reduce=32,\n                             filters_5x5=64,\n                             filters_pool_proj=64,\n                             name='inception_4d')\n\n\n        # \u8f85\u52a9\u5206\u7c7b\u5668\n        x2 = AveragePooling2D((5, 5), strides=3)(x)\n        x2 = Conv2D(128, (1, 1), padding='same', activation='relu')(x2)\n        x2 = Flatten()(x2)\n        x2 = Dense(1024, activation='relu')(x2)\n        x2 = Dropout(0.3)(x2)\n        x2 = Dense(classes, activation='softmax', name='auxilliary_output_2')(x2)\n\n\n        # \u6f5c\u6df1\u6a21\u5757\n        x = inception_module(x,\n                             filters_1x1=256,\n                             filters_3x3_reduce=160,\n                             filters_3x3=320,\n                             filters_5x5_reduce=32,\n                             filters_5x5=128,\n                             filters_pool_proj=128,\n                             name='inception_4e')\n\n        # \u6700\u5927\u6c60\u5316\n        x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_4_3x3\/2')(x)\n\n        # \u6f5c\u6df1\u6a21\u5757\n        x = inception_module(x,\n                             filters_1x1=256,\n                             filters_3x3_reduce=160,\n                             filters_3x3=320,\n                             filters_5x5_reduce=32,\n                             filters_5x5=128,\n                             filters_pool_proj=128,\n                             name='inception_5a')\n\n        # \u6f5c\u6df1\u6a21\u5757\n        x = inception_module(x,\n                             filters_1x1=384,\n                             filters_3x3_reduce=192,\n                             filters_3x3=384,\n                             filters_5x5_reduce=48,\n                             filters_5x5=128,\n                             filters_pool_proj=128,\n                             name='inception_5b')\n\n        # \u5168\u5c40\u5e73\u5747\u6c60\u5316\n        x = GlobalAveragePooling2D(name='avg_pool_5_3x3\/1')(x)\n\n        # \u968f\u673a\u5931\u6d3b\n        x = Dropout(0.40)(x)\n\n        # \u5168\u8fde\u63a5\n        x = Dense(classes, activation='softmax', name='output')(x)\n\n        # \u521b\u5efaGoogleNet\u6a21\u578b\n        return Model(input_layer, [x, x1, x2], name='inception_v1')\n        # return Model(input_layer, x, name='inception_v1')\n\n\n# \u6d4b\u8bd5GoogleNet\u7c7b\u5b9e\u4f8b\u5316\u5e76\u8f93\u51faGoogleNet\u6a21\u578b\u7684\u6982\u8981\u4fe1\u606f\nif __name__ == \"__main__\":\n    model = GoogleNet.build(width=224, height=224, channel=3, classes=196)\n    print(model.summary())","d9f73b57":"import matplotlib.pyplot as plt\nimport numpy as np\n\n\n# \u8bad\u7ec3\u65e5\u5fd7\u7ed8\u56fe\u7c7b\nclass HistoryGraph:\n    def __init__(self, history, epochs, title, file_path):\n        # \u8bad\u7ec3\u65e5\u5fd7\n        self.history = history\n        # \u8bad\u7ec3\u8d9f\u6570\n        self.epochs = epochs\n        # \u56fe\u6807\u9898\n        self.title = title\n        # \u56fe\u50cf\u5b58\u76d8\u6587\u4ef6\u8def\u5f84\u540d\n        self.file_path = file_path\n\n    def draw(self):\n        figure, (ax1, ax2) = plt.subplot(1, 2, figsize=(12, 4))\n        figure.suptitle(self.title, fontsize=12)\n        figure.subplots_adjust(top=0.85, wspace=0.3)\n        epoch_list = list(range(1, self.epochs + 1))\n        ax1.plot(epoch_list,\n                 self.history.history['accuracy'],\n                 label='Train Accuracy'\n                 )\n        ax1.plot(epoch_list,\n                 self.history, history['accuracy'],\n                 label='Validation Accuracy', )\n        ax1.set_xticks(np.arange(0, self.epochs + 1, 5))\n        ax1.set_ylabel(\"Accuracy Value\")\n        ax1.set_xlabel(\"Epoch #\")\n        ax1.set_title(\"Accuracy\")\n        ax1.legend(loc=\"best\")\n\n        ax2.plot(epoch_list, self.history.history[\"loss\"], label=\"Training Loss\")\n        ax2.plot(epoch_list, self.history.history[\"val_loss\"], label=\"Validation Loss\")\n        ax2.set_xticks(np.arange(0, self.epochs + 1, 5))\n        ax2.set_ylabel(\"Loss Value\")\n        ax2.set_xlabel(\"Epoch #\")\n        ax2.set_title(\"Loss\")\n        ax2.legend(loc=\"best\")\n        plt.savefig(self.file_path)\n        plt.close()\n\n\n# \u8bad\u7ec3\u65e5\u5fd7\u7ed8\u56fe\u7c7b(\u652f\u6301Inception v1\u7684\u4e24\u4e2a\u65c1\u8def\u5206\u7c7b\u5668)\nclass HistoryGraphV1:\n    def __init__(self, history, epochs, title, file_path):\n        # \u8bad\u7ec3\u65e5\u5fd7\n        self.history = history\n        # \u8bad\u7ec3\u8d9f\u6570\n        self.epochs = epochs\n        # \u56fe\u6807\u9898\n        self.title = title\n        # \u56fe\u50cf\u5b58\u76d8\u6587\u4ef6\u8def\u5f84\u540d\n        self.file_path = file_path\n\n    def draw(self):\n        figure, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n        figure.suptitle(self.title, fontsize=12)\n        figure.subplots_adjust(top=0.85, wspace=0.3)\n\n        epoch_list = list(range(1, self.epochs + 1))\n        ax1.plot(epoch_list,\n                 self.history.history['output_accuracy'],\n                 label='Train Accuracy'\n                 )\n        ax1.plot(epoch_list,\n                 self.history.history['val_output_accuracy'],\n                 label='Validation Accuracy', )\n        ax1.set_xticks(np.arange(0, self.epochs + 1, 5))\n        ax1.set_ylabel(\"Accuracy Value\")\n        ax1.set_xlabel(\"Epoch #\")\n        ax1.set_title(\"Accuracy\")\n        ax1.legend(loc=\"best\")\n\n        ax2.plot(epoch_list, self.history.history[\"output_loss\"], label=\"Training Loss\")\n        ax2.plot(epoch_list, self.history.history[\"val_output_loss\"], label=\"Validation Loss\")\n        ax2.set_xticks(np.arange(0, self.epochs + 1, 5))\n        ax2.set_ylabel(\"Loss Value\")\n        ax2.set_xlabel(\"Epoch #\")\n        ax2.set_title(\"Loss\")\n        ax2.legend(loc=\"best\")\n        plt.savefig(self.file_path)\n        plt.close()","70c1dcfa":"# \u5bfc\u5165\u5fc5\u8981\u7684\u5305\nimport os\n\nfrom tensorflow.keras.callbacks import Callback\n\n\n# \u6a21\u578b\u5b58\u76d8\u68c0\u67e5\u70b9\uff0c\u6bcf\u8bad\u7ec310\u8d9f\u4fdd\u5b58\u4e00\u6b21\u6a21\u578b\nclass EpochCheckpoint(Callback):\n    def __init__(self, output_path, every=10, start_at=0):\n        # \u8c03\u7528\u7236\u7c7b\u7684\u6784\u9020\u51fd\u6570\n        super(Callback, self).__init__()\n        self.output_path = output_path  # \u6a21\u578b\u4fdd\u5b58\u76ee\u5f55\n        # \u95f4\u9694\u8d9f\u6570\n        self.every = every\n        # \u8d77\u59cb\u8d9f\u6570\uff08\u5f53\u524d\u8d9f\u6570\uff09\n        self.start_epoch = start_at\n\n    def on_epoch_end(self, epoch, logs={}):\n        # \u68c0\u67e5\u662f\u5426\u8981\u5411\u78c1\u76d8\u4fdd\u5b58\u6a21\u578b\n        if (self.start_epoch + 1) % self.every == 0:\n            p = os.path.sep.join([self.output_path,\n                                  \"epoch_{}.h5\".format(self.start_epoch + 1)])\n            self.model.save(p, overwrite=True)\n        # \u589e\u52a0\u5185\u90e8\u7684\u8d9f\u6570\u8ba1\u6570\u5668\n        self.start_epoch += 1","7fa5761d":"from tensorflow.keras.optimizers import SGD, Adam, Adamax\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport math\n\n# \u8bad\u7ec3\u6837\u672c\u5168\u8def\u5f84\u6587\u4ef6\u540d\u79f0\ntrain_dirs = '\/kaggle\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/train'\n# \u6d4b\u8bd5\u6837\u672c\u5168\u8def\u5f84\u6587\u4ef6\u540d\u79f0\ntest_dirs ='\/kaggle\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/test'\n\nMODEL_FILE = \"\/kaggle\/working\/googlenet.h5\"\n\nOUTPUT_PATH = \"\/kaggle\/working\"\n\n# \u521d\u59cb\u5316\u4f18\u5316\u5668\nepochs = 50\nbatch_size = 64\ninitial_lrate = 0.01\n\n\n#  \u968f\u8bad\u7ec3\u8d9f\u6570\u964d\u4f4e\u5b66\u4e60\u7387\ndef decay(epoch, steps=100):\n    initial_lrate = 0.01\n    drop = 0.96\n    epochs_drop = 8\n    lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) \/ epochs_drop))\n    return lrate\n\n\n# \u521d\u59cb\u5316\u5b66\u4e60\u8c03\u5ea6\u5668\nlr_scheduler = LearningRateScheduler(decay, verbose=1)\n\n\n# \u6784\u9020\u7528\u4e8e\u6570\u636e\u589e\u5f3a\u7684\u8bad\u7ec3\u56fe\u50cf\u751f\u6210\u5668\ntrain_datagen = ImageDataGenerator(rotation_range=20,\n                                   zoom_range=0.15,\n                                   width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n                                   height_shift_range=0.2, # randomly shift images vertically (fraction of total height))\n                                   shear_range=0.15,\n                                   horizontal_flip=True,\n                                   rescale=1.\/255,\n                                   fill_mode=\"nearest\")  \n\nval_datagen = ImageDataGenerator(rescale=1.\/255)\n\n\ntrainGen = train_datagen.flow_from_directory(\n        train_dirs,\n        target_size=(224, 224),\n        batch_size=batch_size,\n        shuffle=True)\n\nvalGen = val_datagen.flow_from_directory(\n        test_dirs,\n        target_size=(224, 224),      \n        batch_size=batch_size,\n        shuffle=True)\n\n\nopt = SGD(lr=initial_lrate, momentum=0.9, nesterov=False)\n#opt = Adamax()\nmodel = GoogleNet.build(width=224, height=224, channel=3, classes=196)\n\nmodel.compile(loss=[\"categorical_crossentropy\",\n                    \"categorical_crossentropy\",\n                    \"categorical_crossentropy\"],\n              loss_weights=[1.0, 0.3, 0.3],\n              optimizer=opt,\n              metrics=[\"accuracy\"])\n\n\n\n# 8\u3001\u6784\u9020\u8bad\u7ec3\u56de\u8c03\u5217\u8868\uff0c\u8fd9\u91cc\u4e3b\u8981\u662f\u7ed8\u56fe\u56de\u8c03\u548c\u5b66\u4e60\u901f\u7387\u8c03\u6574\u56de\u8c03\n\ncallbacks = [\n    EpochCheckpoint(OUTPUT_PATH, every=10, start_at=0),\n    lr_scheduler]\n\nhistory = model.fit_generator(trainGen,\n                              steps_per_epoch=8144 \/\/ batch_size,\n                              epochs=epochs,\n                              validation_data=valGen,\n                              validation_steps=8041 \/\/ batch_size,\n                              max_queue_size=batch_size * 2,\n                              callbacks=callbacks,\n                              verbose=1)\n# 10\u3001\u5c06\u8bad\u7ec3\u5f97\u5230\u7684\u6a21\u578b\u4fdd\u5b58\u5230\u6587\u4ef6\nprint(\"[\u4fe1\u606f] \u4fdd\u5b58\u6a21\u578b...\")\nmodel.save(MODEL_FILE, overwrite=True)","61f96b4c":"import matplotlib.pyplot as plt\nimport numpy as np\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('320182359_Dc GoogleNet Training Performance', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(1,51))\nax1.plot(epoch_list, history.history['output_accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_output_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 51, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch #')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['output_loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_output_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 51, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch #')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","c470db0f":"\u521b\u5efa\u7ed8\u56fe\u7c7b","ebbf28c4":"\u5b9a\u4e49GoogleNet\u6a21\u578b","39127864":"training"}}