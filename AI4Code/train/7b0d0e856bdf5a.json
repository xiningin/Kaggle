{"cell_type":{"54d1747a":"code","3919aa3a":"code","dde03269":"code","bad703b1":"code","3f21cc56":"code","aef865ce":"code","f8a73a0c":"code","c9f9997f":"code","48517e22":"code","bc8d7f02":"code","1afa4304":"code","2b114e1a":"code","730a5c81":"code","e45a15f4":"code","614acad3":"code","a3118bfc":"code","a9821195":"code","240aed9c":"code","838292d0":"code","d6bb76ce":"markdown","887bafab":"markdown","c10f6462":"markdown","fd73050a":"markdown","827ccc83":"markdown","94d202d8":"markdown","5b1a2c2a":"markdown","b67c2d43":"markdown","3af543c2":"markdown","7f2de11e":"markdown","387a1332":"markdown","3388022b":"markdown","f58fd809":"markdown","b31f217f":"markdown"},"source":{"54d1747a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tensorflow.compat.v1 import ConfigProto, Session\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom tensorflow.keras.optimizers import SGD\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error as MSE\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nconfig = ConfigProto(intra_op_parallelism_threads=2, \n                        inter_op_parallelism_threads=2,\n                        log_device_placement=True,\n                        allow_soft_placement=True,\n                        device_count = {'CPU': 2})\n\nsession = Session(config=config)\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3919aa3a":"train_data = pd.read_csv('..\/input\/cap-4611-2021-fall-assignment-3\/train.csv')\neval_data = pd.read_csv('..\/input\/cap-4611-2021-fall-assignment-3\/eval.csv')","dde03269":"train_data.head()","bad703b1":"eval_data.head()","3f21cc56":"train_data.isna().sum().value_counts()","aef865ce":"eval_data.isna().sum().value_counts()","f8a73a0c":"train_data.dtypes.value_counts()","c9f9997f":"corr_matrix = train_data.corr()\ncorr_matrix = corr_matrix.abs()\nprint(corr_matrix['Eat'].sort_values(ascending=False)[0:50])\nprint()\nprint(corr_matrix['Eat'].sort_values(ascending=False)[1220:1277])","48517e22":"X = train_data.drop(['id', 'Eat'], axis=1)\ny = train_data['Eat']","bc8d7f02":"ss = StandardScaler()\nX = pd.DataFrame(ss.fit_transform(X))","1afa4304":"es = EarlyStopping(patience=10, restore_best_weights=True)\n\nmodel_1 = Sequential()\nmodel_1.add(Dense(64, activation='tanh', input_shape=(1276,)))\nmodel_1.add(Dense(128, activation='tanh'))\nmodel_1.add(Dense(1))\nmodel_1.compile(optimizer='sgd', loss='mse', metrics=[RootMeanSquaredError()])\n# X_train, X_test, y_train, y_test = train_test_split(X, y)\n# model_1.fit(X_train, y_train, epochs= 500, batch_size=10, validation_split=0.2, callbacks=[es])\n# print()\n# model_1.evaluate(X_test, y_test)","2b114e1a":"model_2 = Sequential()\nmodel_2.add(Dense(64, activation='tanh', input_shape=(1276,)))\nmodel_2.add(Dense(128, activation='tanh'))\nmodel_2.add(Dense(256, activation='tanh'))\nmodel_2.add(Dense(1))\nmodel_2.compile(optimizer='sgd', loss='mse', metrics=[RootMeanSquaredError()])\n# X_train, X_test, y_train, y_test = train_test_split(X, y)\n# model_2.fit(X_train, y_train, epochs= 500, batch_size=10, validation_split=0.2, callbacks=[es])\n# print()\n# model_2.evaluate(X_test, y_test)","730a5c81":"def build_model(n_hidden=1, n_neurons=64, multiplier=1, learning_rate=0.0009, activation='tanh'):\n    model = Sequential()\n    model.add(Dense(n_neurons, activation=activation, input_shape=(1276,)))\n    for i in range(n_hidden):\n        neurons = n_neurons * (multiplier ** (i + 1))\n        model.add(Dense(neurons, activation=activation))\n    model.add(Dense(1))\n    sgd = SGD(learning_rate=learning_rate)\n    model.compile(optimizer=sgd, loss='mse', metrics=[RootMeanSquaredError()])\n    return model\n    \n# X_train, X_test, y_train, y_test = train_test_split(X, y)    \n# model_3 = KerasRegressor(build_model)\n\n# param_grid = {\n#     'n_hidden': [1, 2, 3, 4],\n#     'n_neurons': [16, 32, 64],\n#     'multiplier': [0.5, 1, 2],\n#     'learning_rate': [0.01, 0.001, 0.0009, 0.0001],\n#     'activation': ['tanh', 'sigmoid']\n# }\n\n# random_search = RandomizedSearchCV(model_3, param_grid, n_iter=10, cv=3)\n# random_search.fit(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[es])\n# optimal = random_search.best_params_\n# print(optimal)\n\nmodel_3 = build_model(4, 32, 1, 0.0009, 'tanh')","e45a15f4":"models=[model_1, model_2, model_3]\nsum_train_scores = []\nsum_test_scores = []\n\nfor j in range(len(models)):\n    print(\"Model \", (j + 1))\n    model = models[j]\n    train_scores=[]\n    test_scores=[]\n    for i in range(5):\n        X_train, X_test, y_train, y_test = train_test_split(X, y)\n        model.fit(X_train, y_train, epochs= 500, batch_size=10, validation_split=0.2, callbacks=[es], verbose=0)\n        prediction = model.predict(X_test)\n        test_score = np.sqrt(MSE(y_test, prediction))\n        pred = model.predict(X_train)\n        train_score = np.sqrt(MSE(y_train, pred))\n        test_scores.append(test_score)\n        train_scores.append(train_score)\n        print(\"CV pass\", (i + 1))\n    sum_train_scores.append(train_scores)\n    sum_test_scores.append(test_scores)\nsum_train_scores = pd.DataFrame(sum_train_scores)\nsum_test_scores = pd.DataFrame(sum_test_scores)","614acad3":"import matplotlib.pyplot as plt\nprint('Test Scores:')\nsum_test_scores.iloc[0].hist()\nplt.show()\nprint()\nprint(\"Model 1 score statistics :\")\nprint(sum_test_scores.iloc[0].describe())\nprint()\nprint('Training scores:')\nsum_train_scores.iloc[0].hist()\nplt.show()\nprint()\nprint(sum_train_scores.iloc[0].describe())","a3118bfc":"import matplotlib.pyplot as plt\nprint('Test Scores:')\nsum_test_scores.iloc[1].hist()\nplt.show()\nprint()\nprint(\"Model 2 score statistics :\")\nprint(sum_test_scores.iloc[1].describe())\nprint()\nprint('Training scores:')\nsum_train_scores.iloc[1].hist()\nplt.show()\nprint()\nprint(sum_train_scores.iloc[1].describe())","a9821195":"import matplotlib.pyplot as plt\nprint('Test Scores:')\nsum_test_scores.iloc[2].hist()\nplt.show()\nprint()\nprint(\"Model 3 score statistics :\")\nprint(sum_test_scores.iloc[2].describe())\nprint()\nprint('Training scores:')\nsum_train_scores.iloc[2].hist()\nplt.show()\nprint()\nprint(sum_train_scores.iloc[2].describe())","240aed9c":"models[2].fit(X, y, epochs= 500, batch_size=10, callbacks=[es])","838292d0":"X_test = eval_data.drop('id', axis=1)\nX_test = pd.DataFrame(ss.transform(X_test))\npredictions = models[2].predict(X_test)\noutput = pd.DataFrame({'id': eval_data.id, 'Eat': predictions[:, 0]})\nprint(output.to_string())\noutput.to_csv('submission.csv', index=False)","d6bb76ce":"First, I will check the columns of the train and eval data to make sure all features are valid","887bafab":"The features match up, so no need to drop any yet.\n\n# Exploratory Data Analysis\n\nFirst, I will check for null values in both datasets","c10f6462":"After examining each score statistic, I decided that I will use the third model to predict the evaluation data. This is based on having the best score statistics as well as being the model that was run through a grid search.\n\nNow I will retrain this model on the entire dataset to get the best possible model.","fd73050a":"There are no null values, so no data cleaning needs to be done","827ccc83":"# Loading the data","94d202d8":"# Model Selection","5b1a2c2a":"All features are numerical, so I will go ahead and check their correlation to the target feature.","b67c2d43":"After looking at correlation values, it seems like almost every feature is decently correlated to the target feature. In addition, Neural Networks are good at finding patterns without the need for feature engineering. Therefore, I will keep all of the features for training. However, I will drop the id feature since it is just a unique identifier for each row.","3af543c2":"First, I created two preliminary models to see how the simple models perform on my input data. I commented out my initial training and evaluation to make for a faster submission time. Since both models produce RMSE values under 0.5, I believe that the training data has good enough quality to produce a good model.\n\nNow, I will use RandomizedSearchCV to search for the best model based on the number of layers, the number of neurons in each layer, the learning rate of SGD, and the activation function for each layer.","7f2de11e":"After running random search, I found that the best model involves the hyperparameters given above. I commented out the parameter search code to again make the submission runtime shorter.\n\nNow, I will generate an evaluation distribution for each of the three models.","387a1332":"Now that the model is ready to predict the evaluation data, I will scale the eval data, predict the Eat feature based on my model, and create my submission data.","3388022b":"Now that my training data is ready, I will create my models and fit on the training data.\n\n# Model Creation","f58fd809":"I will now use a StandardScalar, since Neural Networks can recognize patterns better when all numerical features are on the same scale.","b31f217f":"# Creating and Submitting Predictions"}}