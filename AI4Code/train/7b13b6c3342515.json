{"cell_type":{"925c6e30":"code","3deb4e16":"code","c03d60e7":"code","3365393e":"code","5bd09157":"code","138bf520":"code","01e9e511":"code","31f84880":"code","5ce11cb2":"code","030af367":"code","b8c56d9a":"code","5b3b162d":"code","cef352bc":"code","467bb8f2":"code","110007e2":"code","e0320268":"code","5b9278a7":"code","88d1aea4":"code","bd5f2677":"code","e087f789":"code","4f0369aa":"code","b63dc68d":"code","e12cd542":"code","d258bee2":"code","429fbc40":"code","649d6c41":"code","6fee986d":"code","22719326":"code","09254bd2":"code","b9c62662":"code","9db02d06":"code","a875bfa5":"code","1b6ef59b":"code","246202fd":"code","9203f485":"code","816aa8bb":"code","449d98a9":"code","2618c9a4":"code","92805f64":"code","c8b9541a":"code","a9588ac0":"code","56e09615":"code","c0d4f02b":"code","3d1b8310":"code","2e988bb9":"code","40b14f39":"code","f0c8be8c":"code","07830b85":"code","228a3759":"code","da7e10dd":"code","5c42ff12":"code","8db40e0a":"code","394066fe":"code","c61da7d2":"code","1117f4fc":"code","126b0031":"code","c905cf3b":"code","285abfce":"code","b70ebeb3":"code","ea77a6b1":"code","05e3fba3":"code","65e32b51":"code","e52df496":"code","35c87d24":"code","2861247b":"code","381e3ae6":"code","442959c0":"code","c2dee60e":"code","6250dcc3":"code","ad298311":"code","789a7fb5":"code","56ac1d9f":"code","e83e162a":"code","29a11a04":"code","b817b8be":"code","d2209fc7":"code","26badae9":"code","481b962f":"code","fe19fd9d":"code","3068fae8":"code","54d0681d":"code","8903eae4":"code","921d5947":"code","813ddc7d":"code","96e36148":"code","132c9919":"code","b720235c":"code","91d85477":"code","e656fe21":"code","eb0a9993":"code","e4870f3e":"code","6f87e42c":"code","e0753448":"code","2890aff6":"code","9655bdfc":"code","9a3f18bd":"code","7f139de3":"code","ef9ce76e":"code","724fa7a0":"code","4abe0286":"code","3f421eeb":"code","ec112f39":"code","efe451aa":"code","ac8ba3f0":"code","26167dae":"code","de629a8e":"code","90bf3f99":"code","df9eeb03":"code","ea7ea050":"code","d225d86e":"code","6eac497e":"code","de5baf2a":"code","80d78ea6":"code","4cc293e6":"code","c07b1ebf":"code","dee7585b":"code","2444de9f":"code","dbd488a7":"code","52369768":"code","bdf55090":"code","50c202cf":"code","11eecc10":"markdown","97c67711":"markdown","ef84d9dd":"markdown","3da10d74":"markdown","d90dec72":"markdown","1aa66c64":"markdown","710754f0":"markdown","3997e331":"markdown","a9eca28b":"markdown","03b3aa5a":"markdown","3684cd0f":"markdown","8e25a136":"markdown","ea7acde7":"markdown","a4aa69c3":"markdown","043cb1dc":"markdown","7eb376bc":"markdown","7ed5ebad":"markdown","80818fa5":"markdown","34ab9f39":"markdown","aee66a2c":"markdown","5ec6fe47":"markdown","2e320b40":"markdown","fffb0ca4":"markdown","04be40e8":"markdown","26c17e75":"markdown","17e551cd":"markdown","b83233e6":"markdown","1c93173a":"markdown","f74df8e5":"markdown","7367dcab":"markdown","27aff84a":"markdown","225d23e8":"markdown","d77eddc7":"markdown","9ce4b248":"markdown","6fd8d0bf":"markdown","73ba9146":"markdown","f206a72a":"markdown","55ded907":"markdown","8027a294":"markdown","8992fd2c":"markdown","62613adf":"markdown","ab39e0ba":"markdown","ad1273ab":"markdown","ca751464":"markdown","259e414f":"markdown","670ba822":"markdown","15221eb7":"markdown","abd87b38":"markdown","3d8b2be8":"markdown","3f80463f":"markdown","4258e521":"markdown","f94288f0":"markdown","3b9a10e6":"markdown","cf49bb1c":"markdown","9ccf8270":"markdown","5ee04fe4":"markdown","95fa2c06":"markdown","da86444a":"markdown","1d7ec23a":"markdown","b45687bd":"markdown","48844436":"markdown","c456402e":"markdown","7836ef14":"markdown","cb573a9b":"markdown","5fb29817":"markdown","1409afd8":"markdown","64013870":"markdown","2d0b17b3":"markdown","9dc9509e":"markdown","b2d97a26":"markdown","306a4877":"markdown","0bdaab67":"markdown","b24b58eb":"markdown","213191a6":"markdown","ee9cef0d":"markdown","d23cc103":"markdown","89321bb5":"markdown","963144a5":"markdown","2e9cf3ea":"markdown","880cc4c0":"markdown","bcc25575":"markdown","0c14a419":"markdown","0f1b98ee":"markdown","9ebe9f82":"markdown","dde28335":"markdown","21971eb4":"markdown","bed0ffc7":"markdown","feeef261":"markdown","e83200a5":"markdown","ddbb532b":"markdown","bd77af62":"markdown","e5de7379":"markdown","c0a2362d":"markdown","533af255":"markdown","33d6ea9c":"markdown","f490b006":"markdown","c61e566b":"markdown","4b4dec7b":"markdown","54e1ca17":"markdown","a7d85068":"markdown","9cb9b81a":"markdown","12761dbf":"markdown","63d23d74":"markdown","b2c1cc04":"markdown","23e1867b":"markdown","cab94d9b":"markdown","9a66fb25":"markdown","4305cfd5":"markdown","5c08b88a":"markdown","47a88025":"markdown","4f3c9add":"markdown","d45161b7":"markdown","98ee52a1":"markdown","82f3dc20":"markdown","2c367682":"markdown","3badc00b":"markdown","8526b626":"markdown","3dd4ff9c":"markdown","36bd1b26":"markdown","61593a67":"markdown","c323e702":"markdown","90d2b9cb":"markdown","c1945602":"markdown","3bd7edc8":"markdown","41c02685":"markdown","c677868e":"markdown","99942aad":"markdown","7aaaace1":"markdown","3db891ed":"markdown","37ac2444":"markdown","abefb056":"markdown","10488abf":"markdown","7d820564":"markdown","2ae76d97":"markdown","d8b9f4d6":"markdown","5b4d2405":"markdown","d015e952":"markdown","720e218e":"markdown","5ec588fe":"markdown","22712f78":"markdown","cc43a6cd":"markdown","d343b7f4":"markdown","0232a152":"markdown","fb6cf115":"markdown","73f8422d":"markdown"},"source":{"925c6e30":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfull_data = pd.read_csv(\"\/kaggle\/input\/fifa19\/data.csv\")\ndata = full_data[[\"ID\", \"Agility\", \"Position\", \"Overall\", \"Aggression\", \"StandingTackle\", \"SlidingTackle\", \"Preferred Foot\"]].dropna(axis=0)","3deb4e16":"data[\"Tackling\"] = (data[\"StandingTackle\"]+data[\"SlidingTackle\"])\/2\nattacker_positions = ['RF', 'LF', 'ST', 'CF', 'RW', 'LW']\ndata[\"Attacker\"] = data.Position.apply(lambda x: 1 if x in attacker_positions else 0)\ndata = data.drop([\"StandingTackle\", \"SlidingTackle\", \"Position\"], axis=1)\ndata.head()","c03d60e7":"# creating a file for our filtered data\ndata.to_csv('filtered_data.csv',index=False)","3365393e":"def q25(x):\n    return x.quantile(0.25)\n\ndef q75(x):\n    return x.quantile(0.75)\n\ndata.Agility.agg(['min', q25, 'median', q75, 'max', 'mean'])","5bd09157":"data.Agility.plot(kind='hist',\n        alpha=0.7,\n        bins=30,\n        title='Histogram Of Agility Score',\n        rot=45,\n        grid=True,\n        figsize=(12,8),\n        fontsize=15, \n        color=['#A0E8AF', '#FFCF56'])\nplt.xlabel('Agility score')\nplt.ylabel(\"Number Of players\");","138bf520":"data.Agility.plot(kind='box',\n        title='BoxPlot Of Agility Score',\n        grid=True,\n        figsize=(12,8),\n        fontsize=15)\nplt.ylabel(\"Score\", fontsize=15)\nQ1 = data.Agility.quantile(0.25)\nQ3 = data.Agility.quantile(0.75)\nIQR = Q3 - Q1\nprint(f\"num lower end outliers: {(data.Agility < (Q1 - 1.5 * IQR)).sum()}\")\nprint(f\"num upper end outliers: {(data.Agility > (Q3 + 1.5 * IQR)).sum()}\")","01e9e511":"data.Overall.agg(['min', q25, 'median', q75, 'max', 'mean'])","31f84880":"data.Overall.plot(kind='hist',\n        alpha=0.7,\n        bins=30,\n        title='Histogram Of Overall Rating',\n        rot=45,\n        grid=True,\n        figsize=(12,8),\n        fontsize=15, \n        color=['#A0E8AF', '#FFCF56'])\nplt.xlabel('Overall rating')\nplt.ylabel(\"Number Of players\");","5ce11cb2":"data.Overall.plot(kind='box',\n        title='BoxPlot Of Overall Score',\n        grid=True,\n        figsize=(12,8),\n        fontsize=15)\nplt.ylabel(\"Score\", fontsize=15)\nQ1 = data.Overall.quantile(0.25)\nQ3 = data.Overall.quantile(0.75)\nIQR = Q3 - Q1\nprint(f\"num lower end outliers: {(data.Overall < (Q1 - 1.5 * IQR)).sum()}\")\nprint(f\"num upper end outliers: {(data.Overall > (Q3 + 1.5 * IQR)).sum()}\")","030af367":"data.Aggression.agg(['min', q25, 'median', q75, 'max', 'mean'])","b8c56d9a":"data.Aggression.plot(kind='hist',\n        alpha=0.7,\n        bins=30,\n        title='Histogram Of Aggression Score',\n        rot=45,\n        grid=True,\n        figsize=(12,8),\n        fontsize=15, \n        color=['#A0E8AF', '#FFCF56'])\nplt.xlabel('Aggression score')\nplt.ylabel(\"Number Of players\");","5b3b162d":"data.Aggression.plot(kind='box',\n        title='BoxPlot Of Aggression Score',\n        grid=True,\n        figsize=(12,8),\n        fontsize=15)\nplt.ylabel(\"Score\", fontsize=15)\nQ1 = data.Aggression.quantile(0.25)\nQ3 = data.Aggression.quantile(0.75)\nIQR = Q3 - Q1\nprint(f\"num lower end outliers: {(data.Aggression < (Q1 - 1.5 * IQR)).sum()}\")\nprint(f\"num upper end outliers: {(data.Aggression > (Q3 + 1.5 * IQR)).sum()}\")","cef352bc":"data.Tackling.agg(['min', q25, 'median', q75, 'max', 'mean'])","467bb8f2":"data.Tackling.plot(kind='hist',\n        alpha=0.7,\n        bins=30,\n        title='Histogram Of Tackling Score',\n        rot=45,\n        grid=True,\n        figsize=(12,8),\n        fontsize=15, \n        color=['#A0E8AF', '#FFCF56'])\nplt.xlabel('Tackling Score')\nplt.ylabel(\"Number Of players\");","110007e2":"data.Tackling.plot(kind='box',\n        title='BoxPlot Of Tackling Score',\n        grid=True,\n        figsize=(12,8),\n        fontsize=15)\nplt.ylabel(\"Score\", fontsize=15)\nQ1 = data.Tackling.quantile(0.25)\nQ3 = data.Tackling.quantile(0.75)\nIQR = Q3 - Q1\nprint(f\"num lower end outliers: {(data.Tackling < (Q1 - 1.5 * IQR)).sum()}\")\nprint(f\"num upper end outliers: {(data.Tackling > (Q3 + 1.5 * IQR)).sum()}\")","e0320268":"attacker_df = data.groupby('Attacker').agg(num_of_players=('ID', 'count'))\nattacker_df.head()","5b9278a7":"attacker_df.plot(kind='pie', x='Attacker', y='num_of_players',ylabel=\"is the player an Attacker?\" ,title = 'percentage of Attackers in total players', autopct=\"%.1f%%\", figsize = (10,6), legend=False);","88d1aea4":"foot_df = data.groupby('Preferred Foot').agg(num_of_players=('ID', 'count'))\nfoot_df.head()","bd5f2677":"foot_df.plot(kind='pie', x='Preferred Foot', y='num_of_players',title = 'right footed players vs left footed players', autopct=\"%.1f%%\", figsize = (10,6), legend=False);","e087f789":"left_overall_data = data[data['Preferred Foot']=='Left'].Overall\nright_overall_data = data[data['Preferred Foot']=='Right'].Overall","4f0369aa":"average_left = left_overall_data.agg('mean')\naverage_right = right_overall_data.agg('mean')\n\nprint(f'average Overall of left footed players: {round(average_left, 3)}')\nprint(f'average Overall of right footed players: {round(average_right, 3)}')","b63dc68d":"alpha = 0.05\nse_l = left_overall_data.sem(axis =0)  # unbiased estimator for the se of the mean\nse_r = right_overall_data.sem(axis =0)\n\nfrom scipy.stats import norm\n\nz_0025 = norm.ppf(1-alpha\/2)\nleft_CI = (average_left - z_0025 * se_l, average_left + z_0025 * se_l)\nright_CI = (average_right - z_0025 * se_r, average_right + z_0025 * se_r)\nprint(f'confidence interval for mu_L: {left_CI}')\nprint(f'confidence interval for mu_R: {right_CI}')","e12cd542":"plt.fill_between(x = np.arange(1,2,0.1), y1=left_CI[0], y2 = left_CI[1], color='orange', label='Left footed')\nplt.fill_between(x = np.arange(1,2,0.1), y1=right_CI[0], y2 = right_CI[1], color='blue', label = 'Right footed')\nplt.legend()\nplt.ylabel('Overall Score')\nplt.tick_params(\n    axis='x',          # changes apply to the x-axis\n    which='both',      # both major and minor ticks are affected\n    bottom=False,      # ticks along the bottom edge are off\n    top=False,         # ticks along the top edge are off\n    labelbottom=False) # labels along the bottom edge are off\nplt.title(\"Visualization of the Confidence Intervals\")","d258bee2":"left_min, left_max = left_overall_data.min(), left_overall_data.max()\nright_min, right_max = right_overall_data.min(), right_overall_data.max()\nprint(left_min-average_left, left_max-average_left)\nprint(right_min-average_right, right_max-average_right)","429fbc40":"left_overall_data.plot(kind='hist',\n        alpha=0.7,\n        bins=30,\n        title='Histogram Of Overall Score - Left Footed Players',\n        grid=True,\n        figsize=(12,8),\n        fontsize=15, \n        color=['#A0E8AF', '#FFCF56'])\nplt.xlabel('Overall Score')\nplt.ylabel(\"Number Of players\");","649d6c41":"right_overall_data.plot(kind='hist',\n        alpha=0.7,\n        bins=30,\n        title='Histogram Of Overall Score - Right Footed Players',\n        grid=True,\n        figsize=(12,8),\n        fontsize=15, \n        color=['#A0E8AF', '#FFCF56'])\nplt.xlabel('Overall Score')\nplt.ylabel(\"Number Of players\");","6fee986d":"left_overall_data.plot(kind='box',\n        title='BoxPlot Of Overall Score - Left Footed Players',\n        grid=True,\n        figsize=(12,8),\n        fontsize=15)\nplt.ylabel(\"Score\", fontsize=15)\nQ1 = data.Overall.quantile(0.25)\nQ3 = data.Overall.quantile(0.75)\nIQR = Q3 - Q1\nprint(f\"num lower end outliers: {(data.Overall < (Q1 - 1.5 * IQR)).sum()}\")\nprint(f\"num upper end outliers: {(data.Overall > (Q3 + 1.5 * IQR)).sum()}\")","22719326":"left_overall_data.plot(kind='hist',\n        alpha=0.7,\n        density = True,\n        bins=40,\n        title='Histogram Of Overall Score - Left Footed Players and normal distribution',\n        grid=True,\n        figsize=(12,8),\n        fontsize=15, \n        color=['#A0E8AF', '#FFCF56'])\nplt.xlabel('Overall Score')\nplt.ylabel(\"Frequency\")\nx = np.arange(left_min,left_max,0.5)\nN_left = len(left_overall_data)\nplt.plot(x, norm.pdf(x, average_left, se_l*np.sqrt(N_left)))  # we multiply by sqrt(N) to get the se of a single sample out of the se of the mean","09254bd2":"right_overall_data.plot(kind='hist',\n        alpha=0.7,\n        density = True,\n        bins=40,\n        title='Histogram Of Overall Score - Right Footed Players and normal distribution',\n        grid=True,\n        figsize=(12,8),\n        fontsize=15, \n        color=['#A0E8AF', '#FFCF56'])\nplt.xlabel('Overall Score')\nplt.ylabel('Frequency')\nx = np.arange(right_min,right_max,0.5)\nN_right = len(right_overall_data)\nplt.plot(x, norm.pdf(x, average_right, se_r*np.sqrt(N_right)))  # we multiply by sqrt(N) to get the se of a single sample out of the se of the mean","b9c62662":"se_all = data.Overall.sem()\nleft_overall_data.plot(kind='hist',\n        alpha=0.7,\n        density = True,\n        bins=40,\n        title='Histogram Of Overall Score - Left Footed Players and normal distribution, se_all',\n        grid=True,\n        figsize=(12,8),\n        fontsize=15, \n        color=['#A0E8AF', '#FFCF56'])\nplt.xlabel('Overall Score')\nplt.ylabel(\"Frequency\")\nx = np.arange(left_min,left_max,0.5)\nN = len(data)\nplt.plot(x, norm.pdf(x, average_left, se_all*np.sqrt(N)))","9db02d06":"right_overall_data.plot(kind='hist',\n        alpha=0.7,\n        density = True,\n        bins=40,\n        title='Histogram Of Overall Score - right Footed Players and normal distribution, se_all',\n        grid=True,\n        figsize=(12,8),\n        fontsize=15, \n        color=['#A0E8AF', '#FFCF56'])\nplt.xlabel('Overall Score')\nplt.ylabel(\"Frequency\")\nx = np.arange(right_min,right_max,0.5)\nplt.plot(x, norm.pdf(x, average_right, se_all*np.sqrt(N)))","a875bfa5":"sd_L = se_l * np.sqrt(N_left - 1)\nsd_R = se_r * np.sqrt(N_right - 1)\nprint(f'absolute error: {abs(sd_L-sd_R)}')\nprint(f'relative error: {abs(sd_L-sd_R)\/sd_L*100}%')","1b6ef59b":"Sp_squared = ((N_left-1)*sd_L**2 + (N_right-1)*sd_R**2)\/(N_right-1+N_left-1)\n\nT_statistic = (average_left-average_right)\/(np.sqrt(Sp_squared *(1\/N_left+1\/N_right)))\nfrom scipy.stats import t \nt_quantile = t.ppf(1-alpha\/2, N_right-1+N_left-1)\nprint(f\"According to the T test H0 (mu_L = mu_R) is: {abs(T_statistic)<=t_quantile}\")","246202fd":"sigma_L = sd_L\nsigma_R = sd_R\nWald_statistic = (average_left - average_right)\/(np.sqrt((sigma_L **2)\/N_left + (sigma_R **2)\/N_right))\nt_quantile = t.ppf(1-alpha\/2, N_right-1+N_left-1)\nprint(f\"According to the Wald test H0 (mu_L = mu_R) is: {abs(Wald_statistic)<=t_quantile}\")","9203f485":"pv_T = (1-t.cdf(T_statistic, N_right-1+N_left-1))*2\npv_Wald = (1-t.cdf(Wald_statistic, N_right-1+N_left-1))*2\nprint(f\"P-Value for the T test: {pv_T}\")\nprint(f\"P-Value for the Wald test: {pv_Wald}\")","816aa8bb":"from scipy.stats import norm\n\ndef compute_CI(sampleL, sampleR):\n    se_l = sampleL.sem(axis =0)  # unbiased estimator for the se of the mean\n    se_r = sampleR.sem(axis =0)\n    \n    sampleL_avg = sampleL.mean()\n    sampleR_avg = sampleR.mean()\n    z_0025 = norm.ppf(1-alpha\/2)\n    temp_left_CI = (sampleL_avg - z_0025 * se_l, sampleL_avg + z_0025 * se_l)\n    temp_right_CI = (sampleR_avg - z_0025 * se_r, sampleR_avg + z_0025 * se_r)\n    return temp_left_CI, temp_right_CI\n\ndef compute_Wald_test(sampleL, sampleR):\n    # returns wether H0 is rejected\n    # sample_left_overall = sample[sample['Preferred Foot'] == 'Left'].Overall\n    # sample_right_overall = sample[sample['Preferred Foot'] == 'Right'].Overall\n    \n    n_left = len(sampleL)\n    n_right = len(sampleR)\n    \n    sigma_L = sampleL.sem() * np.sqrt(n_left - 1)\n    sigma_R = sampleR.sem() * np.sqrt(n_right - 1)\n    t_average_left = sampleL.mean()\n    t_average_right = sampleR.mean()\n    \n    Wald_statistic = (t_average_left - t_average_right)\/(np.sqrt((sigma_L **2)\/n_left + (sigma_R **2)\/n_right))\n    t_quantile = t.ppf(1-alpha\/2, n_right-1+n_left-1)\n    return abs(Wald_statistic)<=t_quantile, (1-t.cdf(Wald_statistic, n_right-1+n_left-1))*2\n\n\nsizes = (30, 50, 100, 500)\nL_CI_s = {} # CI for the mean of the Overall rating of left footed players\nR_CI_s = {} # same for right footed players\nWald_rejects = {}\nnp.random.seed(1)\n\nfor size in sizes:\n    sample = data.sample(size)\n    sampleL = sample[sample['Preferred Foot'] == 'Left'].Overall\n    sampleR = sample[sample['Preferred Foot'] == 'Right'].Overall\n    while (len(sampleL) < 2) or (len(sampleR) < 2):\n        sample = data.sample(size)\n        sampleL = sample[sample['Preferred Foot'] == 'Left'].Overall\n        sampleR = sample[sample['Preferred Foot'] == 'Right'].Overall\n    t_left_CI, t_right_CI = compute_CI(sampleL, sampleR)\n    H_0_rejected, _ = compute_Wald_test(sampleL, sampleR)\n    if size in L_CI_s.keys():\n        L_CI_s[size] += [t_left_CI]\n        R_CI_s[size] += [t_right_CI]\n        Wald_rejects[size] += [1 if H_0_rejected else 0]\n    else:\n        L_CI_s[size] = [t_left_CI]\n        R_CI_s[size] = [t_right_CI]\n        Wald_rejects[size] = [1 if H_0_rejected else 0]\n\n\nprint(L_CI_s)\nprint(R_CI_s)\nprint(Wald_rejects)\n    ","449d98a9":"from scipy.stats import norm\n\n\n# Our CI from Q1\nalpha = 0.05\nse_l = left_overall_data.sem(axis =0)  # unbiased estimator for the se of the mean\nse_r = right_overall_data.sem(axis =0)\n\nz_0025 = norm.ppf(1-alpha\/2)\nleft_CI = (average_left - z_0025 * se_l, average_left + z_0025 * se_l)\nright_CI = (average_right - z_0025 * se_r, average_right + z_0025 * se_r)\nplt.fill_between(x = np.arange(left_CI[0],left_CI[1],0.01), y1=0, y2 = np.log(N), color='orange', label='Left footed', alpha=0.75)\nplt.fill_between(x = np.arange(right_CI[0],right_CI[1],0.01), y1=15-np.log(N), y2 = 15, color='blue', label = 'Right footed', alpha=0.75)\n\nfor n in sizes:\n    plt.fill_between(x = np.arange(L_CI_s[n][0][0],L_CI_s[n][0][1] ,0.1), y1=0, y2 = np.log(n), color='orange', alpha=0.5)\n    plt.fill_between(x = np.arange(R_CI_s[n][0][0],R_CI_s[n][0][1] ,0.1), y1=15-np.log(n), y2 = 15, color='blue', alpha=0.5)\n\nplt.legend()\nplt.xlabel('Overall Score')\nplt.title(\"Visualization of the Confidence Intervals\")\n","2618c9a4":"L_CI_s = {n:[] for n in sizes} # CI for the mean of the Overall rating of left footed players\nR_CI_s = {n:[] for n in sizes} # same for right footed players\nWald_rejects = {n:[] for n in sizes}\npv_dict = {n:[] for n in sizes}\nfor i in range(100):\n    for size in sizes:\n        sample = data.sample(size)\n        sampleL = sample[sample['Preferred Foot'] == 'Left'].Overall\n        sampleR = sample[sample['Preferred Foot'] == 'Right'].Overall\n        while (len(sampleL) < 2) or (len(sampleR) < 2):\n            sample = data.sample(size)\n            sampleL = sample[sample['Preferred Foot'] == 'Left'].Overall\n            sampleR = sample[sample['Preferred Foot'] == 'Right'].Overall\n        t_left_CI, t_right_CI = compute_CI(sampleL, sampleR)\n        H_0_rejected, pv = compute_Wald_test(sampleL, sampleR)\n        L_CI_s[size].append(t_left_CI)\n        R_CI_s[size].append(t_right_CI)\n        Wald_rejects[size].append(1 if H_0_rejected else 0)\n        pv_dict[size].append(pv)\n\n","92805f64":"plt.fill_between(x = np.arange(left_CI[0],left_CI[1],0.01), y1=0, y2 = np.log(N), color='orange', label='Left footed', alpha=0.75)\nplt.fill_between(x = np.arange(right_CI[0],right_CI[1],0.01), y1=20-np.log(N), y2 = 20, color='blue', label = 'Right footed', alpha=0.75)\n\nfor n in sizes:\n    mean_lci = [np.mean([L_CI_s[n][i][0] for i in range(100)]), np.mean([L_CI_s[n][i][1] for i in range(100)])]\n    mean_rci =[np.mean([R_CI_s[n][i][0] for i in range(100)]), np.mean([R_CI_s[n][i][1] for i in range(100)])]\n    plt.fill_between(x = np.arange(mean_lci[0],mean_lci[1] ,0.01), y1=0, y2 = np.log(n), color='orange', alpha=0.2 )\n    plt.fill_between(x = np.arange(mean_rci[0],mean_rci[1] ,0.01), y1=20-np.log(n), y2 = 20, color='blue', alpha=0.2)\n\n\nplt.legend()\nplt.xlabel('Overall Score')\nplt.title(\"Visualization of the Mean Confidence Intervals\")","c8b9541a":"L_CI_len = {n:[L_CI_s[n][i][1]-L_CI_s[n][i][0] for i in range(100)] for n in sizes}\nR_CI_len = {n:[R_CI_s[n][i][1]-R_CI_s[n][i][0] for i in range(100)] for n in sizes}\n\nfor n in sizes:\n    plt.hist(L_CI_len[n], label=f'size = {n}', alpha=0.5, bins = 20)\nplt.legend()\nplt.title(\"Histograms of the lengths(CI) for left footed players for different sample sizes\")\nplt.xlabel('length of CI')\nplt.ylabel('number of occurences')\nplt.show()\n\nfor n in sizes:\n    plt.hist(R_CI_len[n], label=f'size = {n}', alpha=0.5, bins = 20)\nplt.legend()\nplt.title(\"Histograms of the lengths(CI) for right footed players for different sample sizes\")\nplt.xlabel('length of CI')\nplt.ylabel('number of occurences')\nplt.show()","a9588ac0":"average_left = left_overall_data.mean()\naverage_right = right_overall_data.mean()\n\nL_CI_contains_avg = {n:np.mean([1.0 if L_CI_s[n][i][1]>= average_left >= L_CI_s[n][i][0] else 0.0 for i in range(100)]) for n in sizes}\nR_CI_contains_avg = {n:np.mean([1.0 if R_CI_s[n][i][1]>= average_right >= R_CI_s[n][i][0] else 0.0 for i in range(100)]) for n in sizes}\n\nprint(L_CI_contains_avg)\nprint(R_CI_contains_avg)","56e09615":"xaxes = [pv_dict[n] for n in sizes]\nyaxes = ['y1','y2','y3','y4']\ntitles = ['t1','t2','t3','t4'] \n\nf,a = plt.subplots(2,2)\na = a.ravel()\nfor idx,ax in enumerate(a):\n    ax.hist(data[idx])\n    ax.set_title(titles[idx])\n    ax.set_xlabel(xaxes[idx])\n    ax.set_ylabel(yaxes[idx])\nplt.tight_layout()\nplt.show()\n","c0d4f02b":"# 2\nnp.random.seed(1)\nlr_data = full_data[[\"Potential\", \"Age\", \"Preferred Foot\" , \"Overall\"]].dropna(axis=0)\nlr_sample = lr_data.sample(n=200)\nlr_sample","3d1b8310":"f,a = plt.subplots(1,2, figsize=(16,9))\na = a.ravel()\na[0].hist(lr_sample.Potential, bins=15)\na[0].set_xlabel('Potential Score', fontsize='14')\na[0].set_ylabel('amount',fontsize='14')\na[1].boxplot(lr_sample.Potential, showmeans=True)\na[1].set_ylabel('Potential Score',fontsize='14')\nf.suptitle('Histogram and Boxplot of the Potential score for players in our sample', fontsize=16)\nplt.tight_layout()\nplt.show()","2e988bb9":"f,a = plt.subplots(1,2, figsize=(16,9))\na = a.ravel()\na[0].hist(lr_sample.Age, bins=15)\na[0].set_xlabel('Age', fontsize='14')\na[0].set_ylabel('amount',fontsize='14')\na[1].boxplot(lr_sample.Age, showmeans=True)\na[1].set_ylabel('Age',fontsize='14')\nf.suptitle('Histogram and Boxplot of the Age of players in our sample', fontsize=16)\nplt.tight_layout()\nplt.show()","40b14f39":"f,a = plt.subplots(1,2, figsize=(16,9))\na = a.ravel()\na[0].hist(lr_sample.Overall, bins=15)\na[0].set_xlabel('Overall', fontsize='14')\na[0].set_ylabel('amount',fontsize='14')\na[1].boxplot(lr_sample.Overall, showmeans=True)\na[1].set_ylabel('Overall',fontsize='14')\nf.suptitle('Histogram and Boxplot of the Overall score of players in our sample', fontsize=16)\nplt.tight_layout()\nplt.show()","f0c8be8c":"foot_df = lr_sample.groupby('Preferred Foot').agg(num_of_players=('Age','count'))\nfoot_df.head()\n","07830b85":"foot_df.plot(kind='pie', x='Preferred Foot', y='num_of_players',title = 'right footed players vs left footed players', autopct=\"%.1f%%\", figsize = (10,6), legend=False)","228a3759":"f,a = plt.subplots(1,2, figsize=(16,9))\na = a.ravel()\nfor v, explain in enumerate(['Age', 'Potential']):\n    x, y = [], []\n    for i, row in lr_sample.iterrows():\n        x.append(row[explain])\n        y.append(row['Overall'])\n    a[v].scatter(x,y)\n    a[v].set_ylabel('Overall', fontsize='14')\n    a[v].set_xlabel(explain, fontsize='14')\n    a[v].set_title('Overall vs '+explain)","da7e10dd":"import seaborn as sns\nimport matplotlib.ticker as ticker\n\nsns.set(rc={\"figure.figsize\":(16, 12)})\nax = sns.boxplot(data=lr_sample, x='Preferred Foot', y='Overall')\nax.yaxis.set_major_locator(ticker.MultipleLocator(5))\nax.yaxis.set_major_formatter(ticker.ScalarFormatter())","5c42ff12":"from numpy.linalg import inv\n\nls_data = lr_sample\nls_data['bias'] =1\nls_data['Right_footed'] = ls_data['Preferred Foot'].replace(['Left', 'Right'], [0,1])\nX = ls_data[['bias', 'Age', 'Potential', 'Right_footed']].to_numpy()\nY = ls_data['Overall'].to_numpy()\n\n\nn = X.shape[0]\np = X.shape[1]\ny_bar = np.mean(Y)\nb_hat = inv(X.T@X)@X.T@Y\nprint(b_hat)","8db40e0a":"C = inv(X.T@X)\nP = X@C@X.T\nY_hat = X@b_hat\n\nSST = np.sum(np.power(Y-y_bar,2))\nSSres = Y.T@(np.eye(n)-P)@Y\nSSreg = Y.T@(P-np.ones(n)*1\/n)@Y\n\nMSres = SSres\/(n-p)\nMSreg = SSreg\/(p-1)\nF_statistic = MSreg\/MSres\n\nrow = ['Reg', 'Res', 'Total']\ncol = ['SS', 'df', 'MS', 'F']\ndf = pd.DataFrame([[SSreg, p-1, MSreg, F_statistic], [SSres, n-p, MSres, None], [SST, n-1, None, None]], row, col)\ndf","394066fe":"from scipy.stats import f\n\ndfn, dfd = p-1, n-p\ntheoretical_f_percentile = f.ppf(1-alpha, dfn, dfd)\nprint(f'According to the F test, the hypothesis that our model is the null model is {F_statistic <= theoretical_f_percentile}')","c61da7d2":"R_squared = np.sum(np.power((Y_hat-y_bar),2))\/SST\nR_squared_adj = R_squared*(n-1)\/(n-p) + (1-p)\/(n-p)\nprint(f'We got R_squared={R_squared}, R_squared_adj={R_squared_adj}')","1117f4fc":"e = Y_hat- Y\nsd_eps_sqrd = (1\/(n-p))*np.linalg.norm(e)**2\nlowerbound = b_hat - 2*np.sqrt(np.diag(C)*sd_eps_sqrd)\nupperbound = b_hat + 2*np.sqrt(np.diag(C)*sd_eps_sqrd)\n\nprint(f'our CI base on the sample is: \\n{np.array([lowerbound, upperbound]).T}')","126b0031":"lr_data['bias'] = 1\nlr_data['Right_footed'] = lr_data['Preferred Foot'].replace(['Left', 'Right'], [0,1])\nX_all = lr_data[['bias', 'Age', 'Potential', 'Right_footed']].to_numpy()\nY_all = lr_data['Overall'].to_numpy()\n\nbeta_hat_all = np.linalg.lstsq(X_all,Y_all, rcond=None)[0]\nprint(beta_hat_all)\nprint(f'Is beta_star_i in our CI:\\n{(beta_hat_all >= lowerbound) & (beta_hat_all <= upperbound)}')","c905cf3b":"from scipy.stats import t\nimport math\n\nk = len(beta_hat_all)\ntheoretical_t = t.ppf(1-alpha\/2, len(lr_data)-k-1)\nfor i in range(k):\n    T_i = abs(beta_hat_all[i])\/ (math.sqrt(sd_eps_sqrd * C[i][i]))\n    print(f'According to the T test, beta_{i} is 0: {T_i <= theoretical_t}')\n    print(f'pv = {2*(1- t.cdf(T_i, len(lr_data)-k-1))}')","285abfce":"e = -(Y - Y_hat)\nplt.scatter(Y_hat, e)\nplt.plot(Y_hat, np.zeros(n), color=\"orange\")\nplt.xlabel(\"Y_hat values\")\nplt.ylabel(\"residual values\")\nplt.title(\"Residual values as function of Y_hat\")","b70ebeb3":"not_in_sample = lr_data[~lr_data.index.isin(lr_sample.index)]\nnp.random.seed(0)\ndisjoint_sample = not_in_sample.sample(n=1000)\n\ndisjoint_sample.head()","ea77a6b1":"# predicting the value using the b_hast obtained by the first smaple of 200 rows\nfrom scipy.stats import norm\n\nalpha = 0.05\nsd_eps_sqrd = (1\/(n-p))*np.linalg.norm(e)**2\n\ndef CI_dist_from_mean(x):\n    vec_x = x[['bias', 'Age', 'Potential', 'Right_footed']].to_numpy()\n    return math.sqrt((vec_x.T @ C @ vec_x + 1) * sd_eps_sqrd) * norm.ppf(1 - alpha\/2)\n\n\ndisjoint_sample['predicted_Overall'] = disjoint_sample[['bias', 'Age', 'Potential', 'Right_footed']].apply(lambda x: b_hat.T @ x.to_numpy(), axis=1)\ndisjoint_sample['lower_bound'] = disjoint_sample.apply(lambda x: x['predicted_Overall'] - CI_dist_from_mean(x), axis=1)\ndisjoint_sample['upper_bound'] = disjoint_sample.apply(lambda x: x['predicted_Overall'] + CI_dist_from_mean(x), axis=1)\ndisjoint_sample.head()","05e3fba3":"percetage_in_CI = disjoint_sample.apply(lambda x: 1 if x['lower_bound']<=x['Overall'] <= x['upper_bound'] else 0, axis=1).sum()\/ len(disjoint_sample)\nprint(f'{percetage_in_CI * 100 : .2f}% of the true Overall scores weere in the CI')","65e32b51":"import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n\nmodel = ols('Overall ~ bias + Age + Potential + Right_footed + Age * Potential + Age * Right_footed + Potential * Right_footed', data=lr_sample).fit()\nanova_table = sm.stats.anova_lm(model, typ=2)\nanova_table","e52df496":"from itertools import combinations\nlr_sample_plus = lr_sample\nfor x1, x2 in combinations(['Age', 'Potential', 'Right_footed'], 2):\n    lr_sample_plus[f'{x1} * {x2}'] = lr_sample_plus[x1] * lr_sample_plus[x2]\n    \nX_plus = lr_sample_plus [['bias', 'Age', 'Potential', 'Right_footed', 'Age * Potential', 'Age * Right_footed', 'Potential * Right_footed']].to_numpy()\nY_plus = lr_sample_plus['Overall'].to_numpy()\nbeta_plus = np.linalg.lstsq(X_plus, Y_plus, rcond=None)[0]\n\nC_plus = inv(X_plus.T@X_plus)\nP_plus = X_plus@C_plus@X_plus.T\nY_hat_plus = X_plus@beta_plus\n\nn_p, p_p = X_plus.shape\n\nSST_plus = np.sum(np.power(Y_plus-Y_plus.mean(),2))\nSSres_plus = Y_plus.T@(np.eye(n_p)-P_plus)@Y_plus\nSSreg_plus = Y_plus.T@(P_plus-np.ones(n_p)*1\/n_p)@Y_plus\n\nR_sq_plus = SSreg_plus\/SST_plus\nR_adj_plus = R_sq_plus*(n_p-1)\/(n_p-p_p) + (1-p_p)\/(n_p-p_p)\nprint(f'We got R_squared={R_sq_plus}, R_squared_adj={R_adj_plus}')\nprint(f'Recall that previously we got R_squared={R_squared}, R_squared_adj={R_squared_adj}')","35c87d24":"np.random.seed(42)\nlogr_data = data[['Tackling', 'Aggression', \"Preferred Foot\" , \"Attacker\"]].dropna(axis=0)\nlogr_sample = logr_data.sample(n=200, replace=False)\nlogr_sample.head()","2861247b":"f,a = plt.subplots(1,2, figsize=(16,9))\na = a.ravel()\na[0].hist(logr_sample.Tackling, bins=25)\na[0].set_xlabel('Tackling', fontsize='14')\na[0].set_ylabel('amount',fontsize='14')\na[1].boxplot(logr_sample.Tackling, showmeans=True)\na[1].set_ylabel('Tackling',fontsize='14')\nf.suptitle('Histogram and Boxplot of the Tackling score of players in our sample', fontsize=16)\nplt.tight_layout()\nplt.show()","381e3ae6":"f,a = plt.subplots(1,2, figsize=(16,9))\na = a.ravel()\na[0].hist(logr_sample.Aggression, bins=25)\na[0].set_xlabel('Aggression', fontsize='14')\na[0].set_ylabel('amount',fontsize='14')\na[1].boxplot(logr_sample.Aggression, showmeans=True)\na[1].set_ylabel('Aggression',fontsize='14')\nf.suptitle('Histogram and Boxplot of the Aggression score of players in our sample', fontsize=16)\nplt.tight_layout()\nplt.show()","442959c0":"attacker_df = logr_sample.groupby('Attacker').agg(num_of_players=('Aggression','count'))\nattacker_df.head()","c2dee60e":"attacker_df.plot(kind='pie', x='Attacker', y='num_of_players',title = 'Attacker vs non-Attacker', autopct=\"%.1f%%\", figsize = (10,6), legend=False)","6250dcc3":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(solver='liblinear', random_state=0)\nlogr_sample['Right_footed']= logr_sample['Preferred Foot'].replace(['Left', 'Right'], [0,1])\nX_log = logr_sample[['Tackling', 'Aggression', \"Right_footed\"]].to_numpy()\nY_log = logr_sample[\"Attacker\"].to_numpy()\nmodel.fit(X_log, Y_log)\nbeta_hat = model.coef_\nbias = model.intercept_\nprint(f'bias = {bias}, beta_hat = {beta_hat}')","ad298311":"from scipy.special import expit # sigmoid\n\nX_log_with_bias = np.vstack([np.ones(X_log.shape[0]), X_log.T]).T\n# print(X_log_with_bias)\nbeta_hat_with_bias = np.hstack([[bias], beta_hat])[0]\n# print(beta_hat_with_bias)\npi = expit(X_log_with_bias @ beta_hat_with_bias)\n# print(pi.shape)\nV = np.diag(pi * (1-pi))\nC_log = np.linalg.inv(X_log_with_bias.T @ V @ X_log_with_bias)\nlower_bound_log = beta_hat_with_bias - norm.ppf(1 - alpha\/2) * np.sqrt([C_log[i][i] for i in range(len(beta_hat_with_bias))])\nupper_bound_log = beta_hat_with_bias + norm.ppf(1 - alpha\/2) * np.sqrt(np.diag(C_log))\nCI_log = np.array([lower_bound_log, upper_bound_log]).T\nprint(f'Our CIs for the betas are:\\n {CI_log}')\n","789a7fb5":"all_Model = LogisticRegression(solver='liblinear', random_state=0)\nlogr_data['Right_footed']= logr_data['Preferred Foot'].replace(['Left', 'Right'], [0,1])\nX_log_all = logr_data[['Tackling', 'Aggression', \"Right_footed\"]].to_numpy()\nY_log_all = logr_data[\"Attacker\"].to_numpy()\nmodel.fit(X_log_all, Y_log_all)\nbeta_hat_all = model.coef_\nbias_all = model.intercept_\nbeta_all_with_bias = np.hstack([[bias_all], beta_hat_all])[0]\nprint(f'The coefficients based on the entire data are {beta_all_with_bias}')\nprint(f' beta_j is in the CI: {(lower_bound_log <= beta_all_with_bias) & (beta_all_with_bias <= upper_bound_log)}')","56ac1d9f":"from itertools import combinations\nlogr_sample_plus = logr_sample\nfor x1, x2 in combinations(['Tackling', 'Aggression', \"Right_footed\"], 2):\n    logr_sample_plus[f'{x1} * {x2}'] = logr_sample_plus[x1] * logr_sample_plus[x2]\n    \nX_logr_plus = logr_sample_plus[['Tackling', 'Aggression', \"Right_footed\"] + [f'{x1} * {x2}' for x1, x2 in combinations(['Tackling', 'Aggression', \"Right_footed\"], 2)]].to_numpy()\nY_logr_plus = logr_sample_plus['Attacker'].to_numpy()\n\nmodel_plus = LogisticRegression(solver='liblinear', random_state=0)\nmodel_plus.fit(X_logr_plus, Y_logr_plus)\nbeta_hat_plus = model_plus.coef_\nbias_plus = model_plus.intercept_\nbeta_plus_with_bias = np.hstack([[bias_plus], beta_hat_plus])[0]\nprint(f'The coefficients based on the saample with multiplications are:\\n {beta_plus_with_bias}')","e83e162a":"likelihhod_simple = np.sum(model.predict_log_proba(X_log).T[1] * Y_log) + np.sum((model.predict_log_proba(X_log).T[0]) *(1- Y_log))\n\n# model.predict_log_proba(X_log).T[1] = log of the probability for being an attacker. multiply by Y to sum only the actual attackers.\n# model.predict_log_proba(X_log).T[0]) = log of the probability for not being an attacker. multiply by (1-Y) to sum only the actual non-attackers.\n\nBIC_simple = likelihhod_simple - 4\/2 * np.log(len(X_log)) # S = 4\nprint(f'For our simple model we got BIC = {BIC_simple}')\n\nliklihood_plus = np.sum(model_plus.predict_log_proba(X_logr_plus).T[1] * Y_logr_plus) + np.sum((model_plus.predict_log_proba(X_logr_plus).T[0]) *(1- Y_logr_plus))\nBIC_plus = liklihood_plus - (len(beta_plus_with_bias))\/2 * np.log(len(X_logr_plus))\nprint(f'For our model with the added variables we got BIC = {BIC_plus}')\n","29a11a04":"lin_reg_sample = lr_sample[[\"bias\", \"Age\", \"Potential\", \"Preferred Foot\", \"Right_footed\", \"Overall\"]]  # discarding products of variables added in Part 3\nlin_reg_sample","b817b8be":"# computing beta_hat:\n\nX = lin_reg_sample[['bias', 'Age', 'Potential', 'Right_footed']].to_numpy()\nY = lin_reg_sample['Overall'].to_numpy()\n\n\nn = X.shape[0]\np = X.shape[1]\ny_bar = np.mean(Y)\nC = inv(X.T@X)\nb_hat = C@X.T@Y\nprint(b_hat)","d2209fc7":"Y_hat = X @ b_hat\ne = Y_hat- Y\nsd_eps_sqrd = (1\/(n-p))*np.linalg.norm(e)**2\nlowerbound = b_hat - 2*np.sqrt(np.diag(C)*sd_eps_sqrd)\nupperbound = b_hat + 2*np.sqrt(np.diag(C)*sd_eps_sqrd)\n\nprint(f'our CI base on the sample is: \\n{np.array([lowerbound, upperbound]).T}')","26badae9":"B = 400\nbootstrap_estimators = []\nnp.random.seed(1)\n\nfor i in range(B):\n    bootsrap_sample = lin_reg_sample.sample(200, replace=True)\n    X_bootstrap = bootsrap_sample[['bias', 'Age', 'Potential', 'Right_footed']].to_numpy()\n    Y_bootstrap = bootsrap_sample['Overall'].to_numpy()\n    bootstrap_b_hat = np.linalg.lstsq(X_bootstrap, Y_bootstrap, rcond=None)[0]\n    bootstrap_estimators.append(bootstrap_b_hat)\n\nbootstrap_estimators = np.array(bootstrap_estimators)\n\nbootstrap_se = np.sqrt(np.var(bootstrap_estimators, axis=0))\n\nfrom scipy.stats import norm\nalpha = 0.05\nz_05alpha = norm.ppf(1-alpha\/2)\nbootstrap_se_lower, bootstrap_se_upper = b_hat - z_05alpha * bootstrap_se, b_hat + z_05alpha * bootstrap_se\n\nprint(f'our CI with confidence level {(1-alpha)*100}% using bootstrap to estimate se: \\n{np.array([bootstrap_se_lower, bootstrap_se_upper]).T}')","481b962f":"import math\n\nsorted_coefs = np.sort(math.sqrt(n) * (bootstrap_estimators - b_hat) ,axis=0)\ng_lower = sorted_coefs[math.floor(B * alpha\/2), :]\ng_upper = sorted_coefs[math.ceil(B * (1-alpha\/2))]\n\nbootstrap_pivot_upper, bootstrap_pivot_lower = b_hat - 1\/math.sqrt(n) * g_lower, b_hat - 1\/math.sqrt(n) * g_upper\nprint(f'our CI based on the pivot method with confidence level {(1-alpha)*100}%: \\n{np.array([bootstrap_pivot_lower, bootstrap_pivot_upper]).T}')","fe19fd9d":"sorted_coefs = np.sort(bootstrap_estimators ,axis=0)\nbootstrap_percentile_lower = sorted_coefs[math.floor(B * alpha\/2)]\nbootstrap_percentile_upper = sorted_coefs[math.ceil(B * (1-alpha\/2))]\nprint(f'our CI based on the percentiles method with confidence level {(1-alpha)*100}%: \\n{np.array([bootstrap_percentile_lower, bootstrap_percentile_upper]).T}')","3068fae8":"normal_len = upperbound - lowerbound\nboot_se_len = bootstrap_se_upper - bootstrap_se_lower\npivot_len = bootstrap_pivot_upper - bootstrap_pivot_lower\npercentile_len = bootstrap_percentile_upper - bootstrap_percentile_lower\n\nX_all = lr_data[['bias', 'Age', 'Potential', 'Right_footed']].to_numpy()\nY_all = lr_data['Overall'].to_numpy()\nbeta_hat_all = np.linalg.lstsq(X_all,Y_all, rcond=None)[0]\n\nmethods = ['Normal', 'Bootstrap se', 'Pivot method', 'Percentiles method']\ncolumns_ = [(m + ' lower', m + ' upper', m + ' length', m + ' contains true val') for m in methods]\ncolumns = []\nfor c in columns_:\n    for j in c:\n        columns.append(j)\n        \ndef arrange_data(lower, upper):\n    length = upper - lower\n    contain = (lower <= beta_hat_all) & (upper >= beta_hat_all)\n    return np.array([lower, upper, length, contain]).T\n\ndata_summary = np.hstack([arrange_data(lowerbound, upperbound), arrange_data(bootstrap_se_lower, bootstrap_se_upper),\\\n                         arrange_data(bootstrap_pivot_lower, bootstrap_pivot_upper), arrange_data(bootstrap_percentile_lower, bootstrap_percentile_upper)])\n\nsummary_df = pd.DataFrame(data = data_summary, index = ['bias coef', 'Age coef', 'Potential cpef', 'Right_footed coef'], columns = columns)\nsummary_df.T","54d0681d":"not_in_sample = lr_data[~lr_data.index.isin(lin_reg_sample.index)]\nnp.random.seed(0)\ndisjoint_sample = not_in_sample.sample(n=100)[['bias', 'Age', 'Potential', 'Right_footed', 'Overall']]\ndisjoint_sample","8903eae4":"X_new = disjoint_sample[['bias', 'Age', 'Potential', 'Right_footed']].to_numpy()\nY_new = disjoint_sample['Overall'].to_numpy()\n\nY_pred_new = X_new @ b_hat\nprint(Y_pred_new[:10]) # the rest is similar...","921d5947":"import math\n\n\n# estimating the variance of the predictions:\nestimations_matrix = X_new @ bootstrap_estimators.T  # each row is 400 estimates to Y_new, each based on different bootstrap sample\npivots_matrix = np.sort(math.sqrt(n) * (estimations_matrix.T - Y_pred_new).T ,axis=1)\n\ng_lower = pivots_matrix[:, math.floor(B * alpha\/2)]\ng_upper = pivots_matrix[:, math.ceil(B * (1-alpha\/2))]\n\nY_pred_upper, Y_pred_lower = Y_pred_new - 1\/math.sqrt(n) * g_lower, Y_pred_new - 1\/math.sqrt(n) * g_upper\nprint(f\"CIs for the first 10 predicted Y values:\\n {np.array([Y_pred_lower ,Y_pred_upper]).T[:10]}\")","813ddc7d":"contains_real = (Y_pred_lower <= Y_new) & (Y_new <= Y_pred_upper)\nprint(f\"{np.mean(contains_real)*100}% of the CIs contained the real value\")","96e36148":"np.random.seed(16)\n\ntests_sample = lr_data.sample(n=200)[['Overall', 'Right_footed']]\ntests_sample","132c9919":"sample_left = tests_sample[tests_sample['Right_footed'] == 0].Overall\nsample_right = tests_sample[tests_sample['Right_footed'] == 1].Overall\n\naverage_left = sample_left.mean()\naverage_right = sample_right.mean()\ndiff = average_left - average_right\n\nprint(f'average for left footed players: {average_left}\\naverage for right footed players: {average_right}')\nprint(f'the difference between the means: {diff}')","b720235c":"var_left = np.var(sample_left.to_numpy())\nvar_right = np.var(sample_right.to_numpy())\nlower, upper = diff - norm.ppf(1-alpha\/2) * math.sqrt(var_left + var_right), diff + norm.ppf(1-alpha\/2) * math.sqrt(var_left + var_right)\n\nprint(f'Our CI is {(lower, upper)}.')","91d85477":"true_diff = lr_data[lr_data['Right_footed'] == 0].Overall.mean() - lr_data[lr_data['Right_footed'] == 1].Overall.mean()\nres = '' if lower <= true_diff <= upper else 'not '\nprint(f'The true difference in means (based on the entire data) is {true_diff}, which is {res}in the CI')","e656fe21":"# wald test:\nalpha = 0.05\nse_hat = math.sqrt(var_left + var_right)\nW = diff \/ se_hat\nwald_result = W <= norm.ppf(1 - alpha)\n\nprint(f'According to the Wald test H_0 is {wald_result}')","eb0a9993":"N_left = len(sample_left)\nN = len(tests_sample)\n\npv = 0\n\nB = 400\nnp.random.seed(21)\nfor i in range(B):\n    temp_left = tests_sample.sample(N_left, replace=True)\n    temp_right = tests_sample[~tests_sample.index.isin(temp_left.index)]\n    temp_diff = temp_left.Overall.mean() - temp_right.Overall.mean()\n    if temp_diff >= diff:\n        pv += 1\/B\n\npermutation_tests_res = pv >= alpha\n\nprint(f'According to the Permutations test H_0 is {permutation_tests_res}, p value = {pv}')  ","e4870f3e":"median_L = sample_left.median()\nmedian_R = sample_right.median()\nmedian_diff = median_L - median_R\n\nprint(f'The medians are: m_l = {median_L}, m_r = {median_R}, the difference is {median_diff}')","6f87e42c":"B = 400\ndifferences = []\nnp.random.seed(37)\nfor i in range(B):\n    temp_l = sample_left.sample(N_left, replace=True).median()\n    temp_r = sample_right.sample(N - N_left, replace=True).median()\n    differences.append(temp_l - temp_r)\ndifferences.sort()\nl, u = differences[math.floor(alpha \/ 2 * B)], differences[math.ceil(B * (1-alpha\/2))]\nprint(f'Our CI for the difference in means is {l, u}')","e0753448":"true_median_diff  = lr_data[lr_data['Right_footed'] == 0].Overall.median() - lr_data[lr_data['Right_footed'] == 1].Overall.median()\nres = '' if l <= true_median_diff <= u else 'not '\nprint(f'The true difference in means (based on the entire data) is {true_median_diff}, which is {res}in the CI')","2890aff6":"N_left = len(sample_left)\nN = len(tests_sample)\n\npv = 0\n\nB = 400\nnp.random.seed(21)\nfor i in range(B):\n    temp_left = tests_sample.sample(N_left, replace=True)\n    temp_right = tests_sample[~tests_sample.index.isin(temp_left.index)]\n    temp_diff = temp_left.Overall.median() - temp_right.Overall.median()\n    if temp_diff >= median_diff:\n        pv += 1\/B\n\npermutation_tests_res = pv >= alpha\n\nprint(f'According to the Permutations test H_0 is {permutation_tests_res}, p value = {pv}')  ","9655bdfc":"tests_sample['rank'] = tests_sample.Overall.rank()\nrank_sums = tests_sample.groupby(by='Right_footed')['rank'].sum()\nsum_ranks_l = rank_sums.iloc[0]\nsum_ranks_r = rank_sums.iloc[1]\n\n# using resampling\npv = 0\nB = 400\nnp.random.seed(71)\nfor i in range(B):\n    temp_left = tests_sample.sample(N_left, replace=True)['rank'].sum()\n    if temp_left >= sum_ranks_l:\n        pv += 1\/B\n\npermutation_tests_res = pv >= alpha\nprint(f'According to the Permutations test using sampling H_0 is {permutation_tests_res}, p value = {pv}')   ","9a3f18bd":"# using normal approximation\n\nE_ws = N_left * (N + 1) \/ 2 \nVar_ws = N_left * (N - N_left) * (N + 1) \/ 12\nT = (sum_ranks_l - E_ws)\/math.sqrt(Var_ws)\nnormal_approx_res = T <= norm.ppf(1-alpha)\n\nprint(f'According to the Permutations test using normal approximation H_0 is {normal_approx_res}')   ","7f139de3":"np.random.seed(40)\nsize = 200\nobserved = data.sample(size)\nobservedL = observed[observed['Preferred Foot'] == 'Left'].Overall\nobservedR = observed[observed['Preferred Foot'] == 'Right'].Overall\nwhile (len(observedL) < 2) or (len(observedR) < 2):\n    observed = data.sample(size)\n    observedL = observed[observed['Preferred Foot'] == 'Left'].Overall\n    observedR = observed[observed['Preferred Foot'] == 'Right'].Overall\nprint(observedL.size, observedR.size)","ef9ce76e":"size = 1000\ntestdf = data[~data.isin(observed)].dropna().sample(size)\ntestdf.head()","724fa7a0":"tau = observed[\"Overall\"].median()\nobserved[\"Z\"] = observed[\"Overall\"].apply(lambda x: 1 if x > tau else 0)\nobserved","4abe0286":"P_L = len(observedL[observed[\"Z\"] == 1])\/len(observedL) # P1\nP_R = len(observedR[observed[\"Z\"] == 1])\/len(observedR) # P2\nprint(f\"P_L = {P_L}, P_R = {P_R}\")","3f421eeb":"from scipy.special import logit \npsi = logit(P_L) - logit(P_R)\nprint(psi)","ec112f39":"np.random.seed(40)\nB = 400\nsample_size = len(observed)\npsi_boots = []\nfor i in range(B):\n    boot_sample = observed.sample(sample_size, replace=True)\n    boot_sample_L = boot_sample[boot_sample['Preferred Foot'] == 'Left']\n    boot_sample_R = boot_sample[boot_sample['Preferred Foot'] == 'Right']\n    while (len(boot_sample_L) < 2) or (len(boot_sample_R) < 2):\n        boot_sample =  observed.sample(sample_size, replace=True)\n        boot_sample_L = boot_sample[boot_sample['Preferred Foot'] == 'Left']\n        boot_sample_R = boot_sample[boot_sample['Preferred Foot'] == 'Right']\n    P_L = len(boot_sample_L[boot_sample_L[\"Z\"] == 1])\/len(boot_sample_L)\n    P_R = len(boot_sample_R[boot_sample_R[\"Z\"] == 1])\/len(boot_sample_R)\n    psi = logit(P_L) - logit(P_R)\n    psi_boots.append(psi)\n","efe451aa":"alpha = 0.05\npsi_boots = sorted(psi_boots)\nlower_CI = psi_boots[int(B*alpha\/2)]\nupper_CI = psi_boots[int(B*(1-alpha\/2))]\nCI = (lower_CI, upper_CI)\nprint(f\"the CI using bootsrap is: {CI}\")","ac8ba3f0":"from scipy.stats import beta\n\nnp.random.seed(40)\nB = 400\nalpha = 0.05\nn_L = len(observedL)\nn_R = len(observedR)\ns_L = len(observedL[observed[\"Z\"] == 1])\ns_R = len(observedR[observed[\"Z\"] == 1])\n\nP_L = beta.rvs(s_L+1, n_L-s_L+1, size=B)\nP_R = beta.rvs(s_R+1, n_R-s_R+1, size=B)\npsi = logit(P_L) - logit(P_R)\npsi = sorted(psi)\nCI = (psi[int(B*alpha\/2)], psi[int(B*(1-alpha\/2))])\nprint(f\" the Credible Interval is: {CI}\")","26167dae":"np.random.seed(40)\nB = 400\nalpha = 0.05\nn_L = len(observedL)\nn_R = len(observedR)\ns_L = len(observedL[observed[\"Z\"] == 1])\ns_R = len(observedR[observed[\"Z\"] == 1])\n\nP_L = beta.rvs(s_L+0.5, n_L-s_L+0.5, size=B)\nP_R = beta.rvs(s_R+0.5, n_R-s_R+0.5, size=B)\npsi = logit(P_L) - logit(P_R)\npsi = sorted(psi)\nCI = (psi[int(B*alpha\/2)], psi[int(B*(1-alpha\/2))])\nprint(f\" the Credible Interval is: {CI}\")","de629a8e":"np.random.seed(10)\nB = 400\nsample_size = len(observed)\nP_L = []\nP_R = []\nfor i in range(B):\n    boot_sample = observed.sample(sample_size, replace=True)\n    boot_sample_L = boot_sample[boot_sample['Preferred Foot'] == 'Left']\n    boot_sample_R = boot_sample[boot_sample['Preferred Foot'] == 'Right']\n    while (len(boot_sample_L) < 2) or (len(boot_sample_R) < 2):\n        boot_sample =  observed.sample(sample_size, replace=True)\n        boot_sample_L = boot_sample[boot_sample['Preferred Foot'] == 'Left']\n        boot_sample_R = boot_sample[boot_sample['Preferred Foot'] == 'Right']\n    P_L.append(len(boot_sample_L[boot_sample_L[\"Z\"] == 1])\/len(boot_sample_L))\n    P_R.append(len(boot_sample_R[boot_sample_R[\"Z\"] == 1])\/len(boot_sample_R))\nP_L = np.array(P_L)\nP_R = np.array(P_R)\n\na_L, b_L, *_ = beta.fit(P_L)\na_R, b_R, *_ = beta.fit(P_R)","90bf3f99":"np.random.seed(40)\nB = 400\nalpha = 0.05\nn_L = len(observedL)\nn_R = len(observedR)\ns_L = len(observedL[observed[\"Z\"] == 1])\ns_R = len(observedR[observed[\"Z\"] == 1])\n\nP_L = beta.rvs(s_L+a_L, n_L+b_L-s_L, size=B)\nP_R = beta.rvs(s_R+a_R, n_R+b_R-s_R, size=B)\npsi = logit(P_L) - logit(P_R)\npsi = sorted(psi)\nCI = (psi[int(B*alpha\/2)], psi[int(B*(1-alpha\/2))])\nprint(f\" the Credible Interval is: {CI}\")","df9eeb03":"np.random.seed(3)\ndata[\"bias\"] = 1\ndf = data[[\"Agility\", \"Tackling\", \"Attacker\", \"bias\", \"Aggression\"]].dropna().copy()\ndf = df.sample(1000)","ea7ea050":"from numpy.linalg import inv\ndef create_CI_for_regression_params(df, to_print = True):\n    X = df[[\"Agility\", \"Tackling\", \"Attacker\", \"bias\"]].to_numpy()\n    Y = df['Aggression'].to_numpy()\n\n    n = X.shape[0]\n    p = X.shape[1]\n    y_bar = np.mean(Y)\n    b_hat = inv(X.T@X)@X.T@Y\n    C = np.linalg.inv(X.T@X)\n    P = X@C@X.T\n    Y_hat = X@b_hat\n\n    SSres = Y.T@(np.eye(n)-P)@Y\n    MSres = SSres\/(n-p)\n    se = np.sqrt(np.diag(C)*MSres)\n    # using z(0.975) = 1.96 ~ 2\n    lower_CI = b_hat - 2*se\n    upper_CI = b_hat + 2*se\n    CI_matrix = np.array([lower_CI.T, upper_CI.T])\n    if to_print:\n        print(f\" the confidence interval is:\\n {CI_matrix.T}\")\n    return b_hat, CI_matrix","d225d86e":"b_hat_all_vars, CI_all_vars = create_CI_for_regression_params(df)","6eac497e":"np.random.seed(17)\nprob_lower = sorted(np.random.uniform(low=1\/5, high=1\/2, size=len(df)\/\/2 -1))\nprob_higher = sorted(np.random.uniform(low=1\/2, high=4\/5, size=len(df)\/\/2))\nprob = np.hstack([prob_lower, np.array([1\/2]), prob_higher])\nran = np.random.rand(*prob.shape)\nbernoulli = ran <= prob\n\nmissing_df = df.sort_values('Aggression')\nagg = missing_df[\"Aggression\"].to_numpy()\nagg[bernoulli] = None\nmissing_df[\"Aggression\"] = agg","de5baf2a":"missing_df_without_missing = missing_df.dropna().copy()\nb_hat_missing_vars, CI_missing_vars = create_CI_for_regression_params(missing_df_without_missing)","80d78ea6":"regression_imputation = missing_df.copy()\nX = regression_imputation[[\"Agility\", \"Tackling\", \"Attacker\", \"bias\"]].to_numpy()\nY = regression_imputation['Aggression'].to_numpy()\n\nY_hat = X@b_hat\nregression_imputation[\"Aggression\"] = Y_hat\n\nb_hat_reg_imp, CI_reg_imp = create_CI_for_regression_params(regression_imputation)","4cc293e6":"from scipy.stats import norm\nnp.random.seed(19)\nX = missing_df_without_missing[[\"Agility\", \"Tackling\", \"Attacker\", \"bias\"]].to_numpy()\nY = missing_df_without_missing['Aggression'].to_numpy()\nn = X.shape[0]\np = X.shape[1]\ny_bar = np.mean(Y)\nb_hat = inv(X.T@X)@X.T@Y\nC = np.linalg.inv(X.T@X)\nP = X@C@X.T\nSSres = Y.T@(np.eye(n)-P)@Y\n\nM = 50\nb_hats = np.zeros(shape = (M,p))\nX = missing_df[[\"Agility\", \"Tackling\", \"Attacker\", \"bias\"]].to_numpy()\nn = X.shape[0]\np = X.shape[1]\nY_hat = X@b_hat\nfor i in range(M):\n    r = np.random.normal(0, np.sqrt(SSres\/(n-p)), size = (n,1))\n    approx_Y_hat = Y_hat.reshape(-1,1) + r\n    approx_b_hat = inv(X.T@X)@X.T@approx_Y_hat\n    b_hats[i, :] = approx_b_hat.reshape(-1,)\nprint(f\"the approximated regression parameters are:{b_hats.mean(axis=0)}\")","c07b1ebf":"C = np.linalg.inv(X.T@X)\nb_hat_rubin = b_hats.mean(axis=0)\ntemp = b_hats - b_hat_rubin\nrubin_var = np.power(temp, 2)\nrubin_var = (M+1)\/(M*(M-1)) * rubin_var.sum(0)\nrubin_var += np.diag(C)\n\nlower_CI = b_hats_star - 2* np.sqrt(rubin_var)\nupper_CI = b_hats_star + 2* np.sqrt(rubin_var)\nCI_rubin = np.array([lower_CI.T, upper_CI.T])\nprint(f\" the confidence interval is:\\n {CI_rubin.T}\")","dee7585b":"from sklearn.linear_model import LogisticRegression\n\ndf_for_logit = missing_df.copy()\ndf_for_logit[\"R\"] = df_for_logit[\"Aggression\"].apply(lambda x: 1 if x>0 else 0)\nX = df_for_logit[[\"Agility\", \"Tackling\", \"Attacker\", \"bias\"]].to_numpy()\nY = df_for_logit[\"R\"].to_numpy()\nclf = LogisticRegression(random_state=0, fit_intercept = False).fit(X, Y)\nprint(clf.coef_)\nprint(f\"the probability for R=1 is: {(sum(clf.predict_proba(X))\/len(X))[1]}\")","2444de9f":"from numpy.linalg import lstsq\ndf_for_logit[\"predicted val\"] = clf.predict_proba(df_for_logit[[\"Agility\", \"Tackling\", \"Attacker\", \"bias\"]].to_numpy())[:,1]\ndf_for_logit[\"IPW Term\"] = (df_for_logit[\"R\"]\/df_for_logit[\"predicted val\"])\ndf_for_logit_without_null = df_for_logit.dropna()\nX = df_for_logit_without_null[[\"Agility\", \"Tackling\", \"Attacker\", \"bias\"]].to_numpy()\nY = df_for_logit_without_null[\"Aggression\"].to_numpy()\nn = X.shape[0]\nW = np.zeros((n,n))\nnp.fill_diagonal(W, df_for_logit_without_null[\"IPW Term\"].to_numpy())\n\nbeta_hat_IPW, *_ = lstsq(X.T@W@X, X.T@W@Y, rcond = None)\nprint(f\"the beta_hat parameters using IPW: {beta_hat_IPW}\")","dbd488a7":"def get_beta_from_IPW(df):\n    temp_df = df.dropna().copy()\n    temp_df[\"predicted val\"] = clf.predict_proba(temp_df[[\"Agility\", \"Tackling\", \"Attacker\", \"bias\"]].to_numpy())[:,1]\n    temp_df[\"IPW Term\"] = (temp_df[\"R\"]\/temp_df[\"predicted val\"])\n    temp_X = temp_df[[\"Agility\", \"Tackling\", \"Attacker\", \"bias\"]].to_numpy()\n    temp_Y = temp_df[\"Aggression\"].to_numpy()\n    temp_n = temp_X.shape[0]\n    temp_W = np.zeros((temp_n,temp_n))\n    np.fill_diagonal(temp_W, temp_df[\"IPW Term\"].to_numpy())\n\n    temp_beta, *_ = lstsq(temp_X.T@temp_W@temp_X, temp_X.T@temp_W@temp_Y, rcond = None)\n    return temp_beta","52369768":"np.random.seed(20)\nB = 400\nalpha = 0.05\nsample_size = len(missing_df)\nbetas = np.zeros(shape = (B,p))\nmissing_df[\"R\"] = missing_df[\"Aggression\"].apply(lambda x: 1 if x>0 else 0)\nfor i in range(B):\n    bootstrap_df = missing_df.sample(sample_size, replace = True)\n    beta = get_beta_from_IPW(bootstrap_df)\n    betas[i, :] = beta.reshape(-1,)\nbetas = np.sort(betas, axis=0)\nlower_CI = betas[int(B*alpha\/2), :]\nupper_CI = betas[int(B*(1-alpha\/2)), :]\nCI_IPW = np.array([lower_CI, upper_CI])\nprint(f\"the confidence interval using IPW:\\n {CI_IPW.T}\")","bdf55090":"print(f\"using all the variables including the data that we removed, we got: {b_hat_all_vars}\\n{CI_all_vars}\")\nprint()\nprint(f\"using all the variables excluding the data that we removed, we got: {b_hat_missing_vars}\\n{CI_missing_vars}\")\nprint()\nprint(f\"using all the variables with regression imputation we got: {b_hat_reg_imp}\\n{CI_reg_imp}\")\nprint()\nprint(f\"using all the variables with MICE we got: {b_hat_rubin}\\n{CI_rubin}\")\nprint()\nprint(f\"using all the variables with IPW we got: {beta_hat_IPW}\\n{CI_IPW}\")","50c202cf":"nrows = 2\nncols = 2\nnum_of_params = len(b_hat_all_vars)\nfig, ax = plt.subplots(nrows=nrows, ncols=ncols)\ncolors = [\"orange\", \"green\", \"blue\", \"red\", \"purple\"]\nlabels = [\"Agility\", \"Tackling\", \"Attacker\", \"bias\"]\ncounter = 0\nfor i in range(nrows):\n    for j in range(ncols):\n        b = counter\n        data_dict = {}\n        data_dict['category'] = ['all vars','remove vars','reg imp', 'MICE', 'IPW']\n        data_dict['lower'] = [CI_all_vars[0,b], CI_missing_vars[0,b], CI_reg_imp[0,b], CI_rubin[0,b], CI_IPW[0,b]]\n        data_dict['upper'] = [CI_all_vars[1,b], CI_missing_vars[1,b], CI_reg_imp[1,b], CI_rubin[1,b], CI_IPW[1,b]]\n        dataset = pd.DataFrame(data_dict)\n        ax[i][j].set_title(labels[b])\n        for lower,upper,y in zip(dataset['lower'],dataset['upper'],range(len(dataset))):\n            ax[i][j].plot((lower,upper),(y,y),'o-',color=colors[y])\n        counter+=1\n\nplt.setp(ax,yticks=range(len(dataset)), yticklabels=list(dataset['category']))\nfig.suptitle(\"the IC for every b_hat parameter using every method\")\nplt.tight_layout()","11eecc10":"#   Part 2\n---\n---","97c67711":"we can clearly see that for most players the agility stat is between 55-75, and  there are 192 outliers, all in the lower end.","ef84d9dd":"8.\n","3da10d74":"### Aggression","d90dec72":"4.e.","1aa66c64":"2.a Our Hypothesizes are:\n\n$ H_0 = \\mu_L = \\mu_R $\n\n$ H_1 = \\mu_L \\ne \\mu_R $","710754f0":"We saw in the lecture the lemma which allows us to use normal approximations.","3997e331":"We can see that increasing the age in a year will increase the Overall in 1.09. Increasing the potential in 1 will increase the Overall in 0.93. We can see that being right footed will decrease your rating in 0.19.","a9eca28b":"2. ","03b3aa5a":"We see that as we increase the sample size, the percentage of times the true avarage is in the CI approaches to 95%, as expected.","3684cd0f":"Both p-Values are smaller than $10^{-8}$, so we will reject $H_0$ for every reasonable level.","8e25a136":"We chose to compare the model based on the BIC criteria:","ea7acde7":"### Attacker","a4aa69c3":"We can see the Overall score does distribute normally, and looks quite symmetric. We can see the mean and the median are almost identical. We can see the Overall looks very similar to the Potential, only shifted by 5. We can see that like the Potential, we have 2 upper outlier and 1 lower outlier.","043cb1dc":"We can see that almost 1\/4 of the total players are left-footed.\nIt is intresting to see that there are more left-footed football players than their precentile in the total population","7eb376bc":"1.b: We will assume both of the distributions distribute normally. In more formal notation:\n\n\n$ Overall_{Left} \\sim N(\\mu_L, \\sigma_L^2)$\n\n\n$ Overall_{Right} \\sim N(\\mu_R, \\sigma_R^2)$\n\nThis assumtion looks very reasonable when looking in the histograms of the distribution (below). In addition, because of the smaller of the 2 samples having more than 4000 samples, the distribution will be very similar to normal because of the Central Limit Theorem.\n\nHence, confidence intervals for $\\mu_L, \\mu_R$ will be:\n\n$[\\overline{Overall_{Left}} \\pm z_{(\\alpha\/2)}\\widehat{s.e._L}]$\n\nand similarly:\n$[\\overline{Overall_{Right}} \\pm z_{(\\alpha\/2)}\\widehat{s.e._R}]$\n\nWe will use $\\alpha = 0.05$\n","7ed5ebad":"7.\nOur test for every $0 <= i <= 4$ is:\n\n$H_0:  \\beta^*_i = 0$\n\n$H_1:  \\beta^*_i \\ne 0 $\n\n$T.S: T_i = \\frac{\\beta^*_i}{\\hat{se}(\\beta_i)}$ where $\\hat{se}(\\beta_i) = \\sqrt{\\hat{\\sigma_{\\epsilon}^2}C_{ii}}$\n\n$R.R: |T_i| > t_{\\alpha \/2, n-k-1}$\n\nThe test is based on the assimptotic normality of the least squares estimator.","80818fa5":"3. a.","34ab9f39":"We can see that the linearity assumption holds, because the points are symmetricaly spread around the line e = 0. The constant variance assumption seem to hold as well, as we do not see a significant change in the scattering as Y_hat increases.","aee66a2c":"3.d\nBoxplot of the overall of left footed players and the overall of right footed players","5ec6fe47":"1.a: The avarage Overall score in each category is:","2e320b40":"2.","fffb0ca4":"Increasing the Tackling in 1 unit decreases the chance of being an attacker by $e^{0.11439448}$\n\nIncreasing the Aggression in 1 unit increases the chance of being an attacker by $e^{0.05936607}$\n\nBeing Right footed increases the chance of being an attacker by $e^{0.0735827}$","04be40e8":"3.c","26c17e75":"as we can see approximately 1\/6 of all player play an offensive role.\nevery player has a position he plays in, therefore there is no missing data","17e551cd":"## Q3\n___","b83233e6":"We can see that like the entire data, most of the players are right footed. Interesting to note that in the entire population of the world only ~10% of the people are left handed, but moer than quarter of the players are left footed.","1c93173a":"The distribution we got does not look symmetrical at all. It does look like mirrored Gamma distribution. We can see some difference between the mean and the median, probably caused because of the left tail. There are no outliers, but the IQR is very large, so it is not surprising.","f74df8e5":"2.d: We will use the Wald Test. As we showed above, we assume that the Overall Rating distributes normaly for both left and right footed players. Therfore, the test will be (similar to the one in lecture 3):\n\n$\\large H_0 = \\mu_L = \\mu_R $\n\n$\\large H_1 = \\mu_L \\ne \\mu_R $\n\n$\\large  T.S. = T := \\frac{\\overline{Overall_{Left}} - \\overline{Overall_{Left}} - 0}{\\sqrt{\\frac{\\hat{\\sigma_{Left}}^2}{N_{Left}} +\n\\frac{\\hat{\\sigma_{Right}}^2}{N_{Right}}}}$\n\n$ R.R. = \\left\\{|T|>t_{(\\frac{\\alpha}{2},N_{Right} + N_{Left} -2)}\\right\\} $\n\n","7367dcab":"##### Surprisingly, we see that the Confidence Intervals do not collide, even though thr difference in the means is only 0.8. That is caused by the $\\widehat{s.e.}$ of both being very low, even in comparison to the small differnce in the means. We hypothesize that the differnce is caused by the fact that thre are more than 3 times the number of right footed players than left footed players. Therefore, every left footed outlier has stronger influence on the mean than a right footed outlier. In addition, in the code below we show that the distance between the mean of each category and the minimal value in that category is considerably smaller than the distance between the mean and the maximal value.","27aff84a":"a. CI based on the asimptotic normality of the MLE is:","225d23e8":"1. Our question is: Does the distribution of the Overall rating of left footed players is greater than the distribution of the Overall rating of right footed players in the \"usual stochastic order\".","d77eddc7":"We got an assymetric distribution that resembles the sum of 2 normal distributions, one with less samples than the other. As a result we see some difference between the mean and the median. There are no outliers but the variance is quite high.","9ce4b248":"we can see that the 3 middle approximations are very similar in the b_hat parameters, but not in the CIs.\nmeanwhile, the IPW's beta hat is the closest to the real values.\n\nthe CI of the regression imputation method is the smallest, which is expected because we adding the missing data from\na predicted model that does not adding any variance to the data.\n\nit is interesting to see that the IPW method yields the largest CI, meanwhile having the most accurate b_hat parameters.","6fd8d0bf":"Research Question number 1:\nDoes having a higher aggression score affect your tackling score?\n\nResearch Question number 2:\nDoes having a higher aggility score affect your probability of playing an attacker role?\n\nResearch Question number 3:\nDoes the distribution of overall ranking differ between left-footed and right-footed players?","73ba9146":"3.d.\nout prior distibution is: $f(p_j) \\sim Beta(a, b)$\nlets find those parameter by using simulations","f206a72a":"As for the test itself:","55ded907":"4.c.","8027a294":"1. Our question is how does the Potential stat (continuous), the Age (discrete) and the Preffered Foot (binary) of a player affect its Overall score.","8992fd2c":"We can see an interesting (but expected) results:\n As we enlarge the sample size the confidence interval shrinks.\n\n\nWe can see this phenomenon in the following graph (the taller the rectangle the bigger the size) :","62613adf":"1. how does the overall score change with the preferred foot of the player?","ab39e0ba":"We can see the distribution of the lengths of the CI in the following graphs:","ad1273ab":"We hypothesize that the graph showing the histograms for right footed players is much more \"well seperated\" because more than 75% of the players are right footed, hence in every sample it is very likely that the majority of players were right footed which in turn made the CI shorter.","ca751464":"We can see the distribution of Age does not fit a known distribution, and is not symmetric. We can also see from the boxplot that the age has very high variances due to the long IQR length and the long step size. We can also see that there are no outliers.","259e414f":"c. CI based on the pivot method:\n","670ba822":"c. ","15221eb7":"### Agility stat","abd87b38":"4.c","3d8b2be8":"## Question 1\n***","3f80463f":"3.c. by lesson 10\nout prior distibution is: $f(p_j) = {(p(1-p))}^{-1\/2}$\nthus:\n\n$ f(p_j| X^n) = \\frac{L_n(p_j)*f(p_j)}{c_n} \\propto L_n(p_j)*f(p_j)$\n\n$ s_j := \\sum{X_i}$\n\n$ L_n(p_j)*f(p_j) = L_n(p_j)*{(p(1-p))}^{-1\/2} = p_j^{s_j-0.5} * {(1-p_j)}^{n-s_j-0.5}$\n\n$ f(p_j| X^n) \\sim Beta(s_j+0.5, n-s_j+0.5)$\n","4258e521":"## Question 2\n***\nIn our data, we have 4 continuous attributes and Two binary attributes, there is no missing data due to removal.\nWe decided to describe the continuous values using an histogram, a boxplot and with statistical measurements.\nUsing the histogram we can see the distribution of the attribute in the population.\nUsing the boxplot we can depict the data outliers and also see the data variance (using the quantiles)\nWe describe the binary values using pie chart for seeing the probability of the different values.","f94288f0":"Like we  have seen in task 2, the left footed players tend to have slightly higher Overall rating.","3b9a10e6":"4.g.","cf49bb1c":"a. ","9ccf8270":"for an observation x= [x.agility, x.tackling, x.attacker]\nthe probability of x label to not be missing is:\n$ P(R=1|x_1, x_2, x_3) = \\frac{e^{4.73999269e-04*x_1 -2.55397636e-02*x_2 -3.02329827e-01*x_3  +1.22014171e+00}}{1+e^{4.73999269e-04*x_1 -2.55397636e-02*x_2 -3.02329827e-01*x_3  +1.22014171e+00}}$","5ee04fe4":"From the historgram we can infer that the Overall rating is normally distributed around 66 (median = 66, mean = 66.66). there are outliers in the upper and lower ends- mostly in the upper ends(110 in the upper end, and 53 in the lower end).","95fa2c06":"### Tests\n___","da86444a":"Wald Test:\n\n$H_0: \\mu_L = \\mu_R $\n\n$H_1: \\mu_L > \\mu_R $\n\n$T.S.: W = \\frac{\\mu_L - \\mu_R}{\\hat{s.e._{\\delta}}} $\n\n$R.R: W > z_{\\alpha} $","1d7ec23a":"10.","b45687bd":"## Bayesian probability\n***","48844436":"We can see that Potential has high positive correlation with the Overall. We can see that the Age does have some positive correlation with the overall.","c456402e":"# Part 1\n\n---\n---\n\n","7836ef14":"the different positions values are: ['RF' 'ST' 'LW' 'GK' 'RCM' 'LF' 'RS' 'RCB' 'LCM' 'CB' 'LDM' 'CAM' 'CDM'\n 'LS' 'LCB' 'RM' 'LAM' 'LM' 'LB' 'RDM' 'RW' 'CM' 'RB' 'RAM' 'CF' 'RWB'\n 'LWB']\n \n the attacker role is defined as: ['RF', 'LF', 'ST', 'CF', 'RW', 'LW']","cb573a9b":"5","5fb29817":"3.b\n\nWe will use an approximated CI using the Normal approximation.","1409afd8":"### Wald Tests","64013870":"9.","2d0b17b3":"3.d","9dc9509e":"We will use bootstrap to approximate the variance of the difference. Then we will build a CI using the percentiles method.","b2d97a26":"# Part 3\n\n___\n___","306a4877":"## Question 3\n***","0bdaab67":"3.d: We will show similar vizualization for the mean confidence intervals","b24b58eb":"We got that the simple model has higher BIC, so according to the BIC criteria it is the better model.","213191a6":"## Question 1\n***\nThe dataset we chose is the FIFA19 player database which contains data over the players in the FIFA19 videogame.\nEvery single player has attributes such as overall rating, wage, age, nationallity and more.\nAfter dropping players with missing data (null values) we ended up with 18147 records.\n\nWe only chose the attribue that are rellevant to our research questions which are:\nplayer's Agility- discrete number between 1 to 99\nplayer's overall rating - discrete number between 1 to 99\nis the player an attacker (1 if play specific positions, else 0)\nplayer's aggretion - discrete number between 1 to 99\nplayer's tackling score (mean of tackling scores) - mean of two discrete number between 1 to 99\n\nwe will treat the discrete numbers as continuous for our reserch questions.\n","ee9cef0d":"b. CI where we apporximate the s.e. by using bootstrap:","d23cc103":"Notice that in contrary to task 2, where we have shown that left footed players tend to have significantly higher Overalls scoes, we got that it is very likely that the preferred foot of a player does not affect its Overall in the linear regression model. We hypothrsize that this difference comes from the fact that there are now more explanatory variables which might distribute differently between left and right footed players, thus making the Right_footed variable unecessary.","89321bb5":"4.h.","963144a5":"Histogram and boxplot for Age:","2e9cf3ea":"3.c","880cc4c0":"1.\nWe chose the variables Tackling (continuous), Aggression (discrete) and Preferred Foot (binary) as our explanatory variables explainig Attacker (which is also binary).\nOur question is how does those variables affect the chance of playing in one of the attacker roles.","bcc25575":"Histogram and boxplot for Overall:","0c14a419":"2.b We will treat the standard diviation of both distributions as equal but unknown. We will further assume both samples are from a normal distribution. This assumtion seems very reasonable, as we can see in the histograms below.\nOur test will be:\n\n$ H_0 = \\mu_L = \\mu_R $\n\n$ H_1 = \\mu_L \\ne \\mu_R $\n\n$T.S. = T := \\frac{\\overline{Overall_{Left}} - \\overline{Overall_{Left}} - 0}{S_p\\sqrt{\\frac{1}{N_{Left}} +\n\\frac{1}{N_{Right}}}}$\n\nWhere  $S_p = \\frac{(N_{left}-1)S_{Left}^2 + (N_{Right}-1)S_{Right}^2}{N_{Right} + N_{Left} -2}$ And $ S^2 = \\frac{1}{n-1}\\sum_{i=1}^{N}{(X_i - \\bar{X})^2}$\n\n$R.R = |T| > t_{(\\frac{\\alpha}{2},N_{Right} + N_{Left} -2)}$\n\n","0f1b98ee":"3.a","9ebe9f82":"We got that the new model has better $R^2_{adj}$. From that we can conclude that the new model is slightly more accurate, but more complicated as well. We personally prefer the old model as it gives only slightly worse preformance but is much simpler, so it is less likely to overfit.","dde28335":"the aggression score has more variance than the other attributes that we have looked before. it is intresting to see that there are no outliers (as we can see in the boxplot diagram)","21971eb4":"___\n\n## Logistic Regression\n___","bed0ffc7":"We got that all the CIs contained the real coefficients for bias, Potential and Right_footed. All CIs have very similar lengths.","feeef261":"### Histogram and Boxplot for left, right footed players","e83200a5":"# Part 4\n___\n___","ddbb532b":"2.","bd77af62":"Table for distribution between Left footefd players and Right footed players","e5de7379":"We can see both the absolute error and the relative error are somewhat high, so the assumption that the Variances are equal might be incorrect.","c0a2362d":"4.e\n\nWe could use both Wald and t tests for 4.d. This is because we saw in task 2 that the Overall rating distributes normally for both categories, therefore fullfiling the assumptions of both tests.","533af255":"## Linear Regression\n___\n","33d6ea9c":"the CIs are all different, they are very short due to entering a lot of artificial data that fully explained by the regression,\nwhich got us a very small variance in the data.","f490b006":"## Missing Data\n***","c61e566b":"5.\n","4b4dec7b":"The estimator is a MLE because $\\mu_{L}, \\mu_R$ are MLEs and $\\delta = \\mu_{L}-\\mu_R$ is a function of the MLEs therefore an MLE","54e1ca17":"3.b.\n\n$ f(p_j| X^n) = \\frac{L_n(p_j)*f(p_j)}{c_n} \\propto L_n(p_j)*f(p_j)$\n\n$ s_j := \\sum{X_i}$\n\n$ L_n(p_j)*f(p_j) = L_n(p_j) = p_j^{s_j} * {(1-p_j)}^{n-s_j}$\n\n$ f(p_j| X^n) \\sim Beta(s_j+1, n-s_j+1)$\n","a7d85068":"### Overall","9cb9b81a":"4.f.\nlinear regression as least squares problem:\n$ argmin_\\beta(||X@\\beta - Y||_2^2)$\n\nwith weights:\n$ argmin_\\beta(||diag(\\sqrt{w_1},\\dotsc, \\sqrt{w_n})X@\\beta - Y||_2^2)$","12761dbf":"In both tests we got that $H_0$ is correct.","63d23d74":"### Confidence Intervals","b2c1cc04":"4.b","23e1867b":"3","cab94d9b":"As for both standard diviations being the same, we can look at the unbiased estimators we get for each category:","9a66fb25":"5.","4305cfd5":"as we said before, we can see the length of the different IC clearly,\nit is nice to see how to IPW remove vars and MICE are always centered on the same point\nwith regression imputation that centered on the same point, but has so short CI that it is looking like it has only one point.","5c08b88a":"3.a+b. Histogram and boxplot for Potential:\nNote that while creating the data we omitted null values, so we have no missing attributes.","47a88025":"_________________________________________________________","4f3c9add":"### making the data","d45161b7":"# Part 5\n\n---\n---\n","98ee52a1":"Recall our question from part 3: how does the Potential stat (continuous), the Age (discrete) and the Preffered Foot (binary) of a player affect its Overall score.","82f3dc20":"calculating the score function:\n\n$ L(Y_i,\\beta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} * e^{\\frac{{(Y_i-X_i^T\\beta)}^2}{2 * \\sigma^2}}$\n\n$ l(Y_i,\\beta) = log(L(Y_i,\\beta)) = -1\/2*log(2\\pi\\sigma^2) + \\frac{{(Y_i-X_i^T\\beta)}^2}{2 * \\sigma^2}$\n\n$ s(Y_i, \\beta) = \\frac{d\\beta}{d l(Y_i,\\beta)} = \\frac{1}{2\\sigma^2}*2(Y_i-X_i^T\\beta)X_i = \\frac{1}{\\sigma^2}(Y_i-X_i^T\\beta)X_i$\n\n$ 0 = \\frac{1}{n}\\sum^n \\frac{R_i}{\\pi(W_i)}(Y_i-X_i^T\\beta)X_i$\n\ndenoting W as a matrix which : $ W_{ii} = \\frac{R_i}{\\pi(W_i)}$\n\n$ 0 = \\frac{1}{n}\\sum^n W_{ii}(Y_i-X_i^T\\beta)X_i$\n\n$ (X^TWX)\\beta = X^TWY$\n\n$\\beta = (X^TWX)^{\\dagger}X^TWY$","2c367682":"we can see that each CI is smaller than its previous one- this is happening due to adding more assumptions on the data.","3badc00b":"4. d","8526b626":"3.","3dd4ff9c":"the histogram of Tackling is very intriguing. we can see that there are really two different distributions for the tackling score. we theorise that the first peak (around 15) is mostly attackers- which don't need to shine in tackling- while the defensive players are the majority in the second peak (around 65)","36bd1b26":"We got that just more than 95% of the times our predicted value was in the CI. This means that our assumption of the residuals being normaly distributed does hold. ","61593a67":"d. CI based on the percentiles method:","c323e702":"We got that the percentage of the CIs that contain the real value is not close to 95%","90d2b9cb":"### Preferred Foot","c1945602":"2.e: In both the tests we rejected $H_0$, meaning $mu_{Left} \\ne \\mu_{right}$. This does not surprise us, as the Confidence Intervals we calculated at question 1 did not overlap eachother.\n\nWe will compute the P-Values for both test. In both cases computing the P-Value is plain simple: We will compute cdf(T.S.) of the appropriate distribution, and the pv will be:\n\n$pv = 2(1-cdf(T.S))$","3bd7edc8":"4.a","41c02685":"b. We will use the Pivot Method.","c677868e":"4.d.","99942aad":"2. ","7aaaace1":"6.","3db891ed":"6. We have seen that an aprroximate CI for $[{\\beta_j}^*$ is $\\hat{\\beta_j} \\pm z_{\\alpha\/2}\\sqrt{\\hat{\\sigma_{\\epsilon}^2}C_{jj}}] $","37ac2444":"We got that the true value of $\\beta^*_1$ is not in the Confidence interval we built, but for the other 3 entries of beta we got that the rea value was in the confidence interval.","abefb056":"2.","10488abf":"4.a.","7d820564":"From the histograms we can see the data fits very well to the normal distribution.\nMoreover, the assumption that the Overall rating of a left footed players has the same standard diviation as the Overall rating of a right footed player seems reasonable, as we can see from the following histograms using the s.e. computed over all the data:","2ae76d97":"We will show our assumptions hold:\nFrom the histogrmas we can clearly see both the data for left footed and for right footed players ditributes normaly. ","d8b9f4d6":"4. ","5b4d2405":"Permutations Test:\n\n$H_0: F_{L} = F_R $\n\n$H_1: F_L \\succ F_R $\n\n$T.S. \\mu_L - \\mu_R $\n\n$ p-value = \\frac{\\# I_{\\delta_{permutation}>=\\delta}}{B}$  (use sampling instead of iterating over all $N \\choose N_{left}$ permutations)\n\n$R.R: p-value < \\alpha $","d015e952":"1.explanatory variables: Agility (con.), Attacker (binary), Tackling (con.)\nwe will try to explain the Aggression variable","720e218e":"We can see the Potential is distributed somewhat like a normal distribution. From the boxplot we can see the mean and the median are very close to each other, but the right tail decreases less abruptly than the left tail. We can see there is a single upper outlier and a single lower outlier.\nThe distribution is almost symmetrical but has a slight favor for large values.","5ec588fe":"4.b.","22712f78":"4.","cc43a6cd":"## Question 2\n***","d343b7f4":"$ f(p_j| X^n) = \\frac{L_n(p_j)*f(p_j)}{c_n} \\propto L_n(p_j)*f(p_j)$\n\n$ s_j := \\sum{X_i}$\n\n$ L_n(p_j)*f(p_j) = p_j^{s_j} * {(1-p_j)}^{n-s_j}* \\frac{p_j^{a_j-1}{(1-p_j)}^{b_j-1}}{B(a_j, b_j)}$\n$\\propto p_j^{s_j+a_j-1} * {(1-p_j)}^{n+b_j-s_j-1}$\n\n$ f(p_j| X^n) \\sim Beta(s_j+a_j, n+b_j-s_j)$\n","0232a152":"Statistic Theory Project\n========================\n\nBy: Yam Berent 212267504, Tomer Gavriel 322230392","fb6cf115":"### Tackling","73f8422d":"6.\n\nIn all the tests we used we got that $H_0$ is correct, even though we saw in task 2 that $H_1$ is correct with tiny p-value. We assume that this is becasue we used a small sample (200 samples instead of 17,000 in task 2) and becasue the difference is small (difference in means is 0.84 in task 2, difference in medians is 1). Therefore, it is very reasonalbe that we could not show with statistical significance that $F_L \\succ F_R$."}}