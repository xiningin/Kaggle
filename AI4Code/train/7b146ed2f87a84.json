{"cell_type":{"519a7c95":"code","4e3b1108":"code","3c80f48d":"code","7d6f6355":"code","d0493903":"code","7251fd91":"code","0597f0fe":"code","b97f8ab5":"code","3f55a0ab":"code","898cbd04":"code","8b5c41f1":"code","da28ba46":"code","11059225":"code","f5bbed87":"code","25bac818":"code","5d9ca823":"code","47b53e45":"code","402a5d89":"code","c9fdc1b3":"code","de81fb96":"code","c2fc6e36":"code","50cf861f":"code","9f1bdc0a":"code","4cd5c7a8":"markdown","fb335b16":"markdown","db20c6b1":"markdown","f522fc72":"markdown","8a4e090b":"markdown","88de3d45":"markdown","37c3bb67":"markdown","afd09721":"markdown","05b0915b":"markdown","6327a05f":"markdown","9b8368f1":"markdown","f387d970":"markdown","d9c5f8f1":"markdown","c534e34f":"markdown","49b86625":"markdown","275d598f":"markdown","dcb2bdd2":"markdown","32f55947":"markdown"},"source":{"519a7c95":"import tensorflow as tf\nimport tensorflow_datasets as tfds\nimport keras\nimport pandas as pd\nimport numpy as np\n\nfrom numpy.random import seed\nseed(42)\n\ntf.random.set_seed(42)","4e3b1108":"dataset, info = tfds.load('tf_flowers', as_supervised=True, with_info=True)\ndataset_size=info.splits[\"train\"].num_examples\nclass_names=info.features[\"label\"].names\nn_classes=info.features[\"label\"].num_classes","3c80f48d":"dataset_size, n_classes, class_names","7d6f6355":"test_set, val_set, train_set = tfds.load('tf_flowers', split=['train[:10%]', 'train[10%:25%]', 'train[25%:]'],as_supervised=True)\n                                                            ","d0493903":"len(test_set), len(val_set), len(train_set), dataset_size","7251fd91":"def preprocess(image, label):\n    resized_image=tf.image.resize(image, [224,224])\n    final_image=keras.applications.xception.preprocess_input(resized_image)\n    return final_image, label","0597f0fe":"batch_size=32\ntrain_set=train_set.shuffle(1000).repeat()\ntrain_set=train_set.map(preprocess).batch(batch_size).prefetch(1)\ntest_set=test_set.map(preprocess).batch(batch_size).prefetch(1)\nval_set=val_set.map(preprocess).batch(batch_size).prefetch(1)","b97f8ab5":"base_model=keras.applications.xception.Xception(weights='imagenet', include_top=False)\navg=keras.layers.GlobalAveragePooling2D()(base_model.output)\noutput=keras.layers.Dense(n_classes, activation='softmax')(avg)\nmodel=keras.models.Model(inputs=base_model.input, outputs=output)","3f55a0ab":"for layer in base_model.layers:\n    layer.trainable=False","898cbd04":"optimizer=keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.1)\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nhistory=model.fit(train_set, steps_per_epoch =int(0.75 * dataset_size\/batch_size), validation_data=val_set, \n                 validation_steps=int(0.15 * dataset_size \/ batch_size), epochs=5)","8b5c41f1":"for layer in base_model.layers:\n    layer.trainable=True\n    \noptimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.001)\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nhistory=model.fit(train_set, steps_per_epoch =int(0.75 * dataset_size\/batch_size), validation_data=val_set, \n                 validation_steps=int(0.15 * dataset_size \/ batch_size), epochs=5)                               ","da28ba46":"df_xception=pd.DataFrame(history_xception.history)\ndf_xception.plot(figsize= (8,6))\nplt.gca()\nplt.grid(True)\nplt.ylim(0,1)\nplt.show()","11059225":"# Import major libraries\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport keras\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# set random seeds\n\nfrom numpy.random import seed\nseed(42)\ntf.random.set_seed(42)","f5bbed87":"# Importing dataset\ndataset, info = tfds.load('tf_flowers', with_info=True, as_supervised=True)\n\n# checking dataset details\ndataset_size = info.splits['train'].num_examples\nclass_names = info.features['label'].names\nn_classes = info.features['label'].num_classes","25bac818":"dataset_size, class_names, n_classes","5d9ca823":"test_set, val_set, train_set = tfds.load('tf_flowers', split=['train[:10%]', 'train[10%:25%]', 'train[25%:]'], as_supervised=True)","47b53e45":"# Let us verify the shape of the datasets\nlen(test_set), len(val_set), len(train_set), dataset_size","402a5d89":"def preprocess_xception(image, label):\n    \n    resized_image=tf.image.resize(image,[224,224])\n    final_image=keras.applications.xception.preprocess_input(resized_image)\n    \n    return final_image, label\n    ","c9fdc1b3":"# Let us apply this preprocess function to all 3 datasets along with shuffle and add batching and prefetching to all\nbatch_size=32\ntrain_set=train_set.shuffle(1000).repeat()\ntrain_set=train_set.map(preprocess_xception).batch(batch_size).prefetch(1)\ntest_set=test_set.map(preprocess_xception).batch(batch_size).prefetch(1)\nval_set=val_set.map(preprocess_xception).batch(batch_size).prefetch(1)","de81fb96":"base_model_xception = keras.applications.xception.Xception(weights='imagenet', include_top=False)\navg_xception=keras.layers.GlobalAveragePooling2D()(base_model_xception.output)\noutput_xception=keras.layers.Dense(n_classes, activation='softmax')(avg_xception)\nmodel_xception=keras.models.Model(inputs=base_model_xception.input, outputs=output_xception)\n","c2fc6e36":"# we will keep the learning rates high to make greater changes on the top layers' weights as per predefined weights of lower layers\n\nfor layer in base_model_xception.layers:\n    layer.trainable=False\n    \noptimizer=keras.optimizers.Adam(learning_rate=0.05, decay=0.1)\nmodel_xception.compile(optimizer=optimizer, \n                       loss='sparse_categorical_crossentropy', \n                       metrics=['accuracy'])\nhistory_xception=model_xception.fit(train_set, steps_per_epoch=int(0.75 * dataset_size \/ batch_size), \n                                                        validation_data=val_set,\n                                                       validation_steps=int(0.15 * dataset_size \/ batch_size)\n                                    , epochs=5)\n                                    ","50cf861f":"# compiling the model again and running again\n\nfor layer in base_model_xception.layers:\n    layer.trainable=True\n    \noptimizer1=keras.optimizers.Adam()\nmodel_xception.compile(optimizer=optimizer1, \n                       loss='sparse_categorical_crossentropy', \n                       metrics=['accuracy'])\nhistory_xception=model_xception.fit(train_set, \n                                    steps_per_epoch=int(0.75 * dataset_size\/ batch_size), \n                                                        validation_data=val_set,\n                                                       validation_steps=int(0.15 * dataset_size\/batch_size), epochs=5)","9f1bdc0a":"df_xception=pd.DataFrame(history_xception.history)\ndf_xception.plot(figsize= (8,6))\nplt.gca()\nplt.grid(True)\nplt.ylim(0,1)\nplt.show()","4cd5c7a8":"As it can been above that the accuracy improved from 87 to 91% with just 5 epochs","fb335b16":"### Now let's us unfreeze the lower layers as top layers are trained now and compile the model again with lower learning rate as we don't want to make large changes now","db20c6b1":"## Compile and fit the model","f522fc72":"## Importing major libraries","8a4e090b":"## Importing FLOWERS dataset","88de3d45":"## Modeling","37c3bb67":"## Preprocessing","afd09721":"As we can see that SGD performed much better once layers were made trainable than ADAM optimizer","05b0915b":"# Modeling","6327a05f":"# Now with ADAM OPTIMIZER","9b8368f1":"we exclude top layers i.e Global average pooling and dense output layer","f387d970":"# Preprocessing Images","d9c5f8f1":"### Let us freeze the layers of the base model and we will not train them at the moment, We will first train top layers of the model using predefined weights in the pretrained Xception model","c534e34f":"Now, all our datasets are ready. Let's build model now","49b86625":"# WITH SGD OPTIMIZER","275d598f":"### Dataset only contains train set and hence we need to conver the dataset into train, test and validation set","dcb2bdd2":"we split the dataset into 10% test set, 15% val set and 75% train set","32f55947":"# THIS NOTEBOOK IS TO DEMONSTRATE BENCHMARKING OF SGD AND ADAM OPTIMIZER ON FLOWERS DATASET FROM TENSORFLOW"}}