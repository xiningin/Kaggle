{"cell_type":{"17065a3e":"code","f31c77cb":"code","f572c8d6":"code","7bebd658":"code","13af7cc3":"code","ce08875a":"code","1e7d0f48":"code","8afbf335":"code","2ecb48fc":"code","86eff971":"code","64134cac":"code","bb1aa4a8":"code","dc5910aa":"code","4cccab8a":"code","6589f7d0":"code","631f83b7":"code","e5c7125a":"code","86c905dd":"code","d0f4763d":"code","0b3a3fd6":"code","c8d10898":"code","0eab08e3":"code","708f2440":"code","c0024374":"code","e16b6e95":"code","76c02ab3":"code","948b27e5":"code","401924fc":"code","925ab3ca":"code","b062ff0a":"code","776ed7bd":"markdown","7487658d":"markdown","54235e2a":"markdown","3c35351f":"markdown","2d7472a6":"markdown","7a4d0275":"markdown","f66e5cde":"markdown","68768594":"markdown","ce4d9e54":"markdown","3170beb9":"markdown","e25ac8ed":"markdown","b9abf6a6":"markdown","8f6b7849":"markdown","e573f6ca":"markdown","3a90354d":"markdown","0f28e5bf":"markdown","cd34daff":"markdown","067ea0cc":"markdown","80f58197":"markdown","1b0c5ed9":"markdown","72cb13d4":"markdown","8462af4d":"markdown"},"source":{"17065a3e":"#!pip install -U scikit-learn\nimport pandas as pd\ngender=pd.read_csv('..\/input\/titanic\/gender_submission.csv')\ntest=pd.read_csv('..\/input\/titanic\/test.csv')\ntrain=pd.read_csv('..\/input\/titanic\/train.csv')","f31c77cb":"train.head()","f572c8d6":"total = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head()","7bebd658":"train.drop([\"Cabin\",\"Ticket\"], axis = 1, inplace = True)\ntest.drop([\"Cabin\",\"Ticket\"], axis = 1, inplace = True)\ntrain.drop(train.loc[train['Embarked'].isnull()].index, inplace = True)\ntrain.drop(train.loc[train['Age'].isnull()].index, inplace = True)","13af7cc3":"train.info()","ce08875a":"train.describe()","1e7d0f48":"train.describe(include=['O'])","8afbf335":"train['Title'] = train.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ntrain.drop([\"Name\"], axis = 1, inplace = True)\npd.crosstab(train['Title'], train['Sex'])","2ecb48fc":"train['Title'] = train['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntrain['Title'] = train['Title'].replace('Mlle', 'Miss')\ntrain['Title'] = train['Title'].replace('Ms', 'Miss')\ntrain['Title'] = train['Title'].replace('Mme', 'Mrs')","86eff971":"test['Title'] = test.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest.drop([\"Name\"], axis = 1, inplace = True)\ntest['Title'] = test['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntest['Title'] = test['Title'].replace('Mlle', 'Miss')\ntest['Title'] = test['Title'].replace('Ms', 'Miss')\ntest['Title'] = test['Title'].replace('Mme', 'Mrs')","64134cac":"train.head()\ntest.drop([\"PassengerId\"], axis = 1, inplace = True)\ntrain.drop([\"PassengerId\"], axis = 1, inplace = True)","bb1aa4a8":"from sklearn.model_selection import train_test_split\nimport numpy as np\n\ntrain_set, test_set = train_test_split(train,test_size=0.2)\ntrain_y=train_set[\"Survived\"]\ntrain_x=train_set.drop([\"Survived\"], axis=1)\ntest_y=test_set[\"Survived\"]\ntest_x=test_set.drop([\"Survived\"], axis=1)\ntrain_set_num=train_x.select_dtypes(include=[np.number])\ntrain_set_ob=train_x.select_dtypes(include=[object])\nnum_column=list(train_set_num.columns)\nob_column=list(train_set_ob.columns)","dc5910aa":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\n\nnum_pipeline = Pipeline([\n        ('std_scaler', StandardScaler()),\n    ])\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_column),\n        (\"cat\", OrdinalEncoder(), ob_column),\n    ])\n\ntrain_x = full_pipeline.fit_transform(train_x)\ntest_x = full_pipeline.fit_transform(test_x)","4cccab8a":"from sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import PrecisionRecallDisplay, ConfusionMatrixDisplay, RocCurveDisplay\n\n\ndef eval(model,plots=True):\n    ymul=model.predict(test_x)\n    print(\"Model score: %.4f\" % model.score(test_x,test_y))  \n    print(\"The cross val score with 3 folds: \", cross_val_score(model, train_x, train_y, cv=3, scoring=\"accuracy\"))\n    print(\"F1-score: %.4f\" % f1_score(test_y , ymul) )\n    if(plots):\n        dis = ConfusionMatrixDisplay.from_estimator(model,test_x, test_y)\n        fig, ax = plt.subplots(figsize=(6, 6))\n        PrecisionRecallDisplay.from_estimator(model, test_x, test_y,ax=ax)\n        ax.set_title(\"2-class Precision-Recall curve\")\n        dis1 = RocCurveDisplay.from_estimator(model, test_x, test_y)","6589f7d0":"from sklearn.tree import DecisionTreeClassifier\n\ntree_reg = DecisionTreeClassifier()\ntree_reg.fit(train_x,train_y)\neval(tree_reg,False)","631f83b7":"from sklearn.ensemble import RandomForestClassifier\n\nforest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nforest_clf.fit(train_x,train_y)\neval(forest_clf,False)","e5c7125a":"from sklearn.neighbors import KNeighborsClassifier\n\nk=3\n\nneigh = KNeighborsClassifier(n_neighbors = k)\nneigh.fit(train_x,train_y)\neval(neigh,False)","86c905dd":"Ks = 15\nmean_acc = np.zeros((Ks-1))\n\nfor n in range(1,Ks):\n    #Train Model and Predict  \n    neigh = KNeighborsClassifier(n_neighbors = n).fit(train_x,train_y)\n    yhat=neigh.predict(test_x)\n    mean_acc[n-1] = neigh.score(test_x,test_y)\n\nplt.plot(range(1,Ks),mean_acc,'g')\nplt.legend(('Accuracy ', '+\/- 1xstd','+\/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Neighbors (K)')\nplt.tight_layout()\nplt.show()","d0f4763d":"new_k=7\nneigh2 = KNeighborsClassifier(n_neighbors = new_k)\nneigh2.fit(train_x,train_y)\neval(neigh2,False)","0b3a3fd6":"from sklearn.svm import SVC\n\nsvc_lin=SVC(kernel=\"linear\", C=0.025)\nsvc_lin.fit(train_x,train_y)\neval(svc_lin,False)","c8d10898":"from sklearn.svm import SVC\n\nsvc_gamma=SVC(gamma=2, C=1)\nsvc_gamma.fit(train_x,train_y)\neval(svc_gamma,False)","0eab08e3":"from sklearn.ensemble import AdaBoostClassifier\n\nclf = AdaBoostClassifier(n_estimators=100, random_state=0)\nclf.fit(train_x,train_y)\neval(clf,False)","708f2440":"from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ngnb.fit(train_x,train_y)\neval(gnb,False)","c0024374":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\nlog_reg.fit(train_x,train_y)\neval(log_reg,False)","e16b6e95":"from sklearn.ensemble import GradientBoostingClassifier\n\ngrad_boost = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\ngrad_boost.fit(train_x, train_y)\neval(grad_boost,False)","76c02ab3":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(use_label_encoder=False)\nxgb.fit(train_x, train_y)\neval(xgb,False)","948b27e5":"from sklearn.ensemble import VotingClassifier\n\neclf = VotingClassifier( estimators=[('Grad', grad_boost), ('rf', log_reg), ('gnb', forest_clf),('Knn',neigh2),('svm',svc_lin)],voting='hard')\neclf.fit(train_x, train_y)\neval(eclf,False)","401924fc":"for clf, label in zip([tree_reg, forest_clf, neigh2, svc_lin, svc_gamma,clf,gnb,log_reg,grad_boost], ['Decision Tree', 'Random Forest', 'k-Nearest Neighbors','SVM lin','SVM gamma','Adaboost', 'naive Bayes', 'Logistic Regression','Grad Boost']):\n    scores = cross_val_score(clf, train_x, train_y, scoring='accuracy', cv=5)\n    print(\"Accuracy: %0.5f (+\/- %0.5f) [%s]\" % (scores.mean(), scores.std(), label))","925ab3ca":"!pip install auto-sklearn","b062ff0a":"import autosklearn.classification\n\nautoml = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task=3600) #The default time for the task is 3600s but this can be reduced \nautoml.fit(train_x, train_y)\nprint(\"Model score: %.4f\" % automl.score(test_x,test_y))  ","776ed7bd":"# Titanic Survial \nUsing various classification methods to demonstrate how they work and explain them. The data is from https:\/\/www.kaggle.com\/c\/titanic","7487658d":"This shows that my votting classifier my acutally perform very well with the data that I have. This indicates that to improve this score we will need to look at feature engineering","54235e2a":"This shows a high increase in the accuracy of the model when using k=7 instead of 3.","3c35351f":"## Note \nI will look at imputing the ages and make more features at a future point, for now I will move onto actually training the models.","2d7472a6":"### Trying to Improve model","7a4d0275":"This model is highly dependent on the value of k. This means that it is key to optimise this value. One way to do this is to loop over different values of k and plot the accuracy of the model against K, this will give you the best value for k.","f66e5cde":"# Data Clearning and Feature Engineering","68768594":"### Logistic Regression\nThis estimates the probability of the class belonging to one of the options, this will be between 0 and 1. If it is lower than 0.5 it will be assigned as 0.","ce4d9e54":"#### Metrics and Graphs\nThe metrics that are used to measure the quality of a classifier are:\n* **Model score (score):** This method computes the accuracy score (accuracy is #correct_preds \/ #all_preds)\n* **Cross val score:** As some classifiers have the tendency to overfit it is sometimes useful to do a cross val score. This is when the training set in divided up into different folds and it goes through each fold as a test set while the rest the model is train on.\n* **f1 score:** The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. This is only for binary classifiers though.\n* **Confusion Matrix:** This is a very easy way to identify the number of mislabelled events. Simply it counts the number of class A's identified as class A and class B.\n* **Precision Recall:** The precision-recall curve shows the trade-off between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).\n* **ROC curve:** This is very similar to the precision recall graph however it plots true positive rate (recall) against false positive rate. This is the ratio of negative instances that are incorrectly classified as positive and is equal to 1-true negative rate.","3170beb9":"## Machine Learning\nNow we can move onto the actual machine learning, first however we will need to make a test train split and a evaluation function. Then the following models will be trained.\n* Decision Tree\n* Random Forrest\n* KNN or k-Nearest Neighbors\n* Support Vector Machines\n* Adaboost\n* Naive Bayes classifier\n* Logistic Regression","e25ac8ed":"### Naive Bayes\nThis is done by applying Bayes theorem with the assumption that each pair of features are independent. There are a number of different bayes classifiers but they mainly differ by the distribution. These classifiers tend to be fast compared to more complex methods but still provide excellent results.","b9abf6a6":"Cabin will now be dropped as it is to hard to fill and is not the most useful. As embarked is the how purpose of this project the 2 missing values will also be dropped. Age however will be looking into replacing as it will probably be very useful.","8f6b7849":"As expected the best classifier is the voting classifier combining different models.","e573f6ca":"### AdaBoost\nUsing adaboost is when the next training instance pays more attention to previous training instances where it underfitted. This will keep going focussing on the harder to fit cases. For example when the first algorithm trains it will have some misclassified training instances, on the next instance it will increase the weight of the misclassified instances and then trains again. This will produces the final model.","3a90354d":"### K Nearest Neighbors\nK-Nearest Neighbors is a supervised learning algorithm. Where the data is 'trained' with data points corresponding to their classification. To predict the class of a given data point, it takes into account the classes of the 'K' nearest data points and chooses the class in which the majority of the 'K' nearest data points belong to as the predicted class.","0f28e5bf":"### SVC\nThis simply try's to divide the training data but keeps are far as possible away from any points in the training set. It can be thought of as fitting a street, with the edges of the street being the as a training instance.","cd34daff":"## Data Exploration","067ea0cc":"## Extra\nI read about a automatic machine learning package called auto sklearn so I decided to try this out. This package allows for a full optimization of a full \npipeline from data transformation to ensembled learning. This should provide a very good result. Should be noted that it working with sklearn 0.24 while the rest of this was written in v1. It also doesnt work in windows machines. https:\/\/towardsdatascience.com\/auto-sklearn-an-automl-tool-based-on-bayesian-optimization-91a8e1b26c22 talks you through it.","80f58197":"### Desesion Trees and Random Forest\nA decision tree is normally used for classification but can also but done for regression. A decision tree works by asking a initial question at the root node for instance in this case is the sale condition normal and then depending on the answer you go down one of the nodes to another question if needed (for simple datasets it may not be needed to be able to divide the data up). You measure the impurity of the node called gini, if the node is completely pure then it would =0. In skearn it uses the CART algorithm, this works by splitting the data set into 2 subsets using a single feature and a threshold, it then continues to do this until the max depth is reached or a split will not reduce the impurity.                    \nA random forest algorithm is a ensemble of decision trees trained using the bagging method. This involves using the same algorithm and training them on a subsets of the training data, once this is done the ensemble makes predictions by aggregating the result (most frequent for classification and average for regression). This will normally perform better than a decision tree.","1b0c5ed9":"This has now dealt with the name column and turns into something we are more likely to be able to use.","72cb13d4":"The first thing that needs to be done though is make a test train split for the data set. The data then needs to be transformed by scaling the features and encoding the categorical variables.","8462af4d":"# Summary"}}