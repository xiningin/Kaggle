{"cell_type":{"b7a28a15":"code","f5f71233":"code","d757c203":"code","49cccf5e":"code","bdc7063a":"code","3a9df87e":"code","bdbc312b":"code","e69df3e5":"code","8ecf5743":"code","cb5d42e9":"code","32275323":"code","6e1e1fb3":"code","7c6aab3b":"code","3bd6740f":"code","92030397":"code","2d91744b":"code","fd99f736":"code","f0324200":"code","a9b1e991":"code","4d710a25":"code","2849e71a":"code","dcde52af":"code","0101e5a8":"code","9bf00a7a":"code","1b5547ec":"code","0ffaec94":"code","a9b34fd7":"code","b6c6cfc1":"code","54b01554":"code","e12efccf":"code","2a7abf67":"code","ef9a7a56":"code","2ad8469d":"code","094e8cf9":"code","24c83e80":"code","bfe91b12":"markdown","9a9c2af4":"markdown","b3ecb65c":"markdown","2401ddbd":"markdown","d323d6c3":"markdown","bfcc64f8":"markdown","9f008f54":"markdown","d098e922":"markdown","e40df813":"markdown","b2940edf":"markdown","4446f3bb":"markdown"},"source":{"b7a28a15":"#making the imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","f5f71233":"#reading the data set\n\ndf = pd.read_csv('..\/input\/auto-mpg.csv')","d757c203":"df.head()","49cccf5e":"#checking for columns and their data types\ndf.info()","bdc7063a":"#checking for any nulls\ndf.isnull().sum()","3a9df87e":"#checking for shape\ndf.shape","bdbc312b":"#checking for value counts in horse power column\ndf['horsepower'].value_counts()","e69df3e5":"#below rows have the horse power values missing. \ndf[df['horsepower'] == '?']","8ecf5743":"#removing the missing values\ndf = df[df['horsepower'] != '?']","cb5d42e9":"#checking the shape of new df\ndf.shape","32275323":"#converting the data type of horsepower column\ndf.horsepower = df.horsepower.astype('float')","6e1e1fb3":"#checking for data types\ndf.dtypes","7c6aab3b":"plt.figure(figsize = (8,5))\nsns.set_style('dark')\nsns.distplot(df['mpg'])\nplt.show()","3bd6740f":"plt.figure(figsize = (8,5))\nsns.set_style('dark')\nsns.distplot(df['horsepower'])\nplt.show()","92030397":"plt.figure(figsize = (8,5))\nsns.set_style('dark')\nsns.distplot(df['displacement'])\nplt.show()","2d91744b":"plt.figure(figsize = (8,5))\nsns.set_style('dark')\nsns.countplot(df['cylinders'])\nplt.show()","fd99f736":"#checking the pair plot for numerical columns\nsns.pairplot(df.drop(['car name'], axis =1))\nplt.show()","f0324200":"#heat map of numerical columns\ntemp_df = df.drop(['car name'], axis = 1)\ncorr = temp_df.corr()\nplt.figure(figsize = (10,8))\nsns.heatmap(corr, cmap='coolwarm')\nplt.show()","a9b1e991":"#define the scaling function\n\ndef scaling_func(x):\n    \n    y = (x - x.min())\/(x.max() - x.min())\n    return y\n","4d710a25":"#apply the scaling to numerical columns\n\ndf['displacement'] = scaling_func(df['displacement'])\ndf['horsepower'] = scaling_func(df['horsepower'])\ndf['acceleration'] = scaling_func(df['acceleration'])\ndf['weight'] = scaling_func(df['weight'])\ndf['cylinders'] = scaling_func(df['cylinders'])\ndf['model year'] = scaling_func(df['model year'])\ndf['origin'] = scaling_func(df['origin'])","2849e71a":"df.head()","dcde52af":"#splitting the data into train and test with 80\/20 ratio\ntrain_dataset = df.sample(frac=0.8,random_state=0)\ntest_dataset = df.drop(train_dataset.index)","0101e5a8":"#separating the labels\ntrain_labels = train_dataset.pop('mpg')\ntest_labels = test_dataset.pop('mpg')","9bf00a7a":"#dropping the car name column\ntest_dataset.drop('car name', axis =1 , inplace = True)\ntrain_dataset.drop('car name', axis = 1, inplace = True)\n","1b5547ec":"#converting to a np array\ntrain_dataset = train_dataset.values\ntrain_labels = train_labels.values\n\ntest_dataset = test_dataset.values\ntest_labels = test_labels.values","0ffaec94":"# TensorFlow and kera import\nimport tensorflow as tf\nfrom tensorflow import keras","a9b34fd7":"#define the model\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(128, activation = tf.nn.relu))\nmodel.add(tf.keras.layers.Dense(64, activation = tf.nn.relu))\nmodel.add(tf.keras.layers.Dense(1))","b6c6cfc1":"#compile the model\n\nmodel.compile(loss= 'mean_squared_error', optimizer= 'RMSprop', metrics= ['mean_absolute_error', 'mean_squared_error'])","54b01554":"# Display training progress by printing a single dot for each completed epoch\nclass PrintDot(keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs):\n    if epoch % 100 == 0: print('')\n    print('.', end='')\n\nEPOCHS = 1000\n\nhistory = model.fit(\n  train_dataset, train_labels,\n  epochs=EPOCHS, validation_split = 0.2, verbose=0,\n  callbacks=[PrintDot()])","e12efccf":"hist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\nhist.tail()","2a7abf67":"#plotting function for MAE and MSE\ndef plot_history(history):\n  hist = pd.DataFrame(history.history)\n  hist['epoch'] = history.epoch\n\n  plt.figure()\n  plt.xlabel('Epoch')\n  plt.ylabel('Mean Abs Error [MPG]')\n  plt.plot(hist['epoch'], hist['mean_absolute_error'],\n           label='Train Error')\n  plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n           label = 'Val Error')\n  plt.ylim([0,5])\n  plt.legend()\n\n  plt.figure()\n  plt.xlabel('Epoch')\n  plt.ylabel('Mean Square Error [$MPG^2$]')\n  plt.plot(hist['epoch'], hist['mean_squared_error'],\n           label='Train Error')\n  plt.plot(hist['epoch'], hist['val_mean_squared_error'],\n           label = 'Val Error')\n  plt.ylim([0,20])\n  plt.legend()\n  plt.show()\n\n\nplot_history(history)","ef9a7a56":"# The patience parameter is the amount of epochs to check for improvement\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n\nhistory = model.fit(train_dataset, train_labels, epochs=EPOCHS,\n                    validation_split = 0.2, verbose=0, callbacks=[early_stop, PrintDot()])\n\nplot_history(history)","2ad8469d":"loss, mae, mse = model.evaluate(test_dataset, test_labels, verbose=0)\n\nprint(\"Testing set Mean Abs Error: {:5.2f} MPG\".format(mae))","094e8cf9":"test_predictions = model.predict(test_dataset).flatten()\n\nplt.figure(figsize = (8,6))\nplt.scatter(test_labels, test_predictions)\nplt.xlabel('True Values [MPG]')\nplt.ylabel('Predictions [MPG]')\nplt.axis('equal')\nplt.axis('square')\nplt.xlim([0,plt.xlim()[1]])\nplt.ylim([0,plt.ylim()[1]])\n_ = plt.plot([-100, 100], [-100, 100])","24c83e80":"#let's check for the error distribution\n\nerror = test_predictions - test_labels\nplt.figure(figsize = (8,6))\nplt.hist(error, bins = 25, color = '#6f93b7')\nplt.xlabel(\"Prediction Error [MPG]\")\n_ = plt.ylabel(\"Count\")","bfe91b12":"As we can see that after about 100 epochs no significant improvement is observed in the Validation Error. Now we will use early stopping to stop the training if there is no significant improvement after a certain number of epochs. ","9a9c2af4":"In **Summary** we can say that:\n\n**Scaling** is important to bring all features on a similar scale.\n\nWhen there is less data then we should use a small neural network with **less Hidden Layers** to avoid **Overfitting**.\n\n**Early Stopping** is also a useful technique to prevent overfitting. \n\n\n","b3ecb65c":"## Checking the Model performance on Test Data","2401ddbd":"## Building the Neural Net Model","d323d6c3":"# Elploratory Data Analysis (EDA)","bfcc64f8":"# Scaling the data","9f008f54":"**Source:**\n\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. The dataset was used in the 1983 American Statistical Association Exposition.\n\n\n**Data Set Information:**\n\nThis dataset is a slightly modified version of the dataset provided in the StatLib library. In line with the use by Ross Quinlan (1993) in predicting the attribute \"mpg\", 8 of the original instances were removed because they had unknown values for the \"mpg\" attribute. The original dataset is available in the file \"auto-mpg.data-original\". \n\n\"The data concerns city-cycle fuel consumption in miles per gallon, to be predicted in terms of 3 multivalued discrete and 5 continuous attributes.\" (Quinlan, 1993)\n\n\n**Attribute Information:**\n\n1. mpg: continuous \n2. cylinders: multi-valued discrete \n3. displacement: continuous \n4. horsepower: continuous \n5. weight: continuous \n6. acceleration: continuous \n7. model year: multi-valued discrete \n8. origin: multi-valued discrete \n9. car name: string (unique for each instance)\n\n\n\nBelow description can be found for this data set. \n\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/auto+mpg","d098e922":"Lets do some EDA to check for distribution of different columns.","e40df813":"## Making the Predictions\nLets make predictions and compare them with actual values. ","b2940edf":"Now we can choose to replace the missing values with the mean of 'horse power' column or the mode value.  But we will eliminate the missing values. ","4446f3bb":"We can see that there are 6 values with question mark (missing). "}}