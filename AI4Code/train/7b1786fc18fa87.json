{"cell_type":{"5c295676":"code","e1f486ae":"code","8aa67361":"code","9a897d11":"code","fb46deaa":"code","32baa095":"code","bec336fa":"code","971bb790":"code","9a31620b":"code","bd6b25a5":"code","85e056ca":"code","315fcc30":"code","46cc7419":"code","e85841a3":"code","eac904a7":"code","bebf0c2e":"markdown","107260bc":"markdown","55e8dec7":"markdown","ab3e17ed":"markdown","2e46a19c":"markdown","a5e14e10":"markdown","05e2b293":"markdown","bd242313":"markdown","2537312d":"markdown","8a8e4a29":"markdown"},"source":{"5c295676":"!pip install timm","e1f486ae":"import torch\nfrom torch import nn, optim\nfrom torchvision import transforms\nimport timm\n\nfrom sklearn.model_selection import KFold\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport time","8aa67361":"train_csv = pd.read_csv(\"..\/input\/classify-leaves\/train.csv\")\n\nleaves_labels = sorted(list(set(train_csv['label'])))\nn_classes = len(leaves_labels)\nclass_to_num = dict(zip(leaves_labels, range(n_classes)))\nnum_to_class = {v : k for k, v in class_to_num.items()}\n\ndef kfold(data, k=5):\n    \"\"\" K\u6298\u4ea4\u53c9\u9a8c\u8bc1 \"\"\"\n    \n    KF = KFold(n_splits=k, shuffle=False)\n    for train_idxs, test_idxs in KF.split(data):\n        train_data = data.loc[train_idxs].reset_index(drop=True)\n        valid_data = data.loc[test_idxs].reset_index(drop=True)\n        train_iter = torch.utils.data.DataLoader(\n            ReadData(train_data, train_transform), batch_size=64,\n            shuffle=True, num_workers=3, pin_memory=True\n        )\n\n        valid_iter = torch.utils.data.DataLoader(\n            ReadData(valid_data, valid_transform), batch_size=64,\n            shuffle=True, num_workers=3, pin_memory=True\n        )\n        \n        yield train_iter, valid_iter","9a897d11":"class ReadData(torch.utils.data.Dataset):\n    def __init__(self, csv_data, transform=None):\n        super(ReadData, self).__init__()\n        self.data = csv_data\n        self.transform = transform\n        \n    def __getitem__(self, idx):\n        img = Image.open(\"..\/input\/classify-leaves\/\" + self.data.loc[idx, \"image\"])\n        label = class_to_num[self.data.loc[idx, \"label\"]]\n        \n        if self.transform:\n            img = self.transform(img)\n        \n        return img, label\n        \n    def __len__(self):\n        return len(self.data)\n    \ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(p=.5),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nvalid_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n","fb46deaa":"def mixup_data(x, y, alpha=1.0, use_cuda=True):\n    \"\"\" Mixup \u6570\u636e\u589e\u5f3a -> \u968f\u673a\u53e0\u52a0\u4e24\u5f20\u56fe\u50cf \"\"\"\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    batch_size = x.size()[0]\n    if use_cuda:\n        index = torch.randperm(batch_size).cuda()\n    else:\n        index = torch.randperm(batch_size)\n\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    \n    return mixed_x, y_a, y_b, lam","32baa095":"def color(x, y, alpha=1.0, use_cuda=True):\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n        \n    batch_size = x.size()[0]\n    if use_cuda:\n        index = torch.randperm(batch_size).cuda()\n    else:\n        index = torch.randperm(batch_size)\n    new = transforms.transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)\n    new_x = new(x)\n    y_a, y_b = y, y[index]\n    return new_x, y_a, y_b, lam","bec336fa":"def flip_data(x, y, alpha=1.0, use_cuda=True):\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n        \n    batch_size = x.size()[0]\n    if use_cuda:\n        index = torch.randperm(batch_size).cuda()\n    else:\n        index = torch.randperm(batch_size)\n    new = transforms.RandomRotation(degrees=(90, 180))\n    new_x = new(x)\n    y_a, y_b = y, y[index]\n    return new_x, y_a, y_b, lam\n","971bb790":"def rand_bbox(size, lam):\n    \n    W, H = size[2], size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = np.int(W * cut_rat)\n    cut_h = np.int(H * cut_rat)\n\n    # uniform\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w \/\/ 2, 0, W)\n    bby1 = np.clip(cy - cut_h \/\/ 2, 0, H)\n    bbx2 = np.clip(cx + cut_w \/\/ 2, 0, W)\n    bby2 = np.clip(cy + cut_h \/\/ 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\ndef cutmix_data(x, y, alpha=1.0, use_cuda=True):\n    \"\"\" Cutmix \u6570\u636e\u589e\u5f3a -> \u968f\u673a\u5bf9\u4e3b\u56fe\u50cf\u8fdb\u884c\u88c1\u526a, \u52a0\u4e0a\u566a\u70b9\u56fe\u50cf\n    W: \u6dfb\u52a0\u88c1\u526a\u56fe\u50cf\u5bbd\n    H: \u6dfb\u52a0\u88c1\u526a\u56fe\u50cf\u9ad8\n    \"\"\"\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    batch_size = x.size()[0]\n    if use_cuda:\n        index = torch.randperm(batch_size).cuda()\n    else:\n        index = torch.randperm(batch_size)\n    \n    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n    x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) \/ (x.size()[-1] * x.size()[-2]))\n    y_a, y_b = y, y[index]\n    \n    return x, y_a, y_b, lam","9a31620b":"def mixup_criterion(pred, y_a, y_b, lam):\n    c = nn.CrossEntropyLoss()\n    return lam * c(pred, y_a) + (1 - lam) * c(pred, y_b)\ncriterion = mixup_criterion","bd6b25a5":"def get_models(k=5):\n    models = {}\n    for mk in range(k):\n        model = timm.create_model(\"resnest50d_4s2x40d\", False, drop_rate=.5)\n        model.fc = nn.Sequential(\n            nn.Linear(model.fc.in_features, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(.3),\n            nn.Linear(512, len(num_to_class))\n        )\n        # for param in model.layer4.parameters():\n        #     if isinstance(param, nn.Conv2d):\n        #         nn.init.xavier_normal_(param.weight)\n        # for param in model.fc.parameters():\n        #     if isinstance(param, nn.Linear):\n        #         nn.init.kaiming_normal_(param.weight)\n        model.load_state_dict(torch.load(f\"..\/input\/resnest50dnew\/Resnest50d_new.pth\"))\n        for i, param in enumerate(model.children()):\n            if i == 6:\n                break\n            param.requires_grad = False\n\n        model.cuda()\n\n        opt = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, 10, T_mult=2)\n        models[f\"model_{mk}\"] = {\n            \"model\": model,\n            \"opt\": opt,\n            \"scheduler\": scheduler,\n            \"last_acc\": .97\n        }\n    \n    return models\n\nmodels = get_models()","85e056ca":"def train_model():\n    for epoch in range(20):\n        flod_train_acc = []\n        flod_valid_acc = []\n        for k, (train_iter, valid_iter) in enumerate(kfold(train_csv, 5)):\n            model = models[f\"model_{k}\"][\"model\"]\n            opt = models[f\"model_{k}\"][\"opt\"]\n            scheduler = models[f\"model_{k}\"][\"scheduler\"]\n            s = time.time()\n            model.train()\n            train_loss = []\n            train_acc = 0\n            length = 0\n            for x, y in train_iter:\n                x, y = x.cuda(), y.cuda()\n                random_num = np.random.random()\n                if random_num <= 1\/4:\n                    x, y_a, y_b, lam = mixup_data(x, y, use_cuda=True)\n                elif random_num <= 1\/2:\n                    x, y_a, y_b, lam = cutmix_data(x, y, use_cuda=True)\n                elif random_num <= 3\/4:\n                    x, y_a, y_b, lam = flip_data(x, y, use_cuda=True)\n                else:\n                    x, y_a, y_b, lam = mixup_data(x, y, alpha=0, use_cuda=True)\n                x, y_a, y_b = map(torch.autograd.Variable, (x, y_a, y_b))\n                output = model(x)\n                loss = criterion(output, y_a, y_b, lam)\n                train_loss.append(loss.item())\n                predict = output.argmax(dim=1)\n                length += x.shape[0]\n                train_acc += lam * (predict == y_a).cpu().sum().item() + \\\n                            (1 - lam) * (predict == y_b).cpu().sum().item()\n                opt.zero_grad()\n                loss.backward()\n                opt.step()\n                scheduler.step()\n\n            model.eval()\n            valid_acc = []\n            with torch.no_grad():\n                for x, y in valid_iter:\n                    x, y = x.cuda(), y.cuda()\n                    pre_x = model(x)\n                    valid_acc.append((pre_x.argmax(1) == y).float().mean().item())\n\n            k_train_ = train_acc \/ length\n            k_valid_ = sum(valid_acc) \/ len(valid_acc)\n            if k_valid_ > models[f\"model_{k}\"][\"last_acc\"]:\n                #torch.save(model.state_dict(), f\"\/kaggle\/working\/Resnest50d_{k}.pth\")\n                models[f\"model_{k}\"][\"last_acc\"] = k_valid_\n\n            response = f\"Epoch {epoch + 1}-Fold{k + 1} \u2014\u2014 \" + \\\n                    f\"Train Loss: {sum(train_loss) \/ len(train_loss) :.3f}, \" + \\\n                    f\"Train Accuracy: {k_train_ * 100 :.2f}%, \" + \\\n                    f\"Valid Accuracy: {k_valid_ * 100 :.2f}%, \" + \\\n                    f\"Learning Rate: {opt.param_groups[0]['lr'] :.6f}, \" + \\\n                    f\"Time Out: {time.time() - s :.1f}s\"\n            print(response)\n            flod_train_acc.append(k_train_)\n            flod_valid_acc.append(k_valid_)\n\n        t_accuracy = np.mean(flod_train_acc)\n        v_accuracy = np.mean(flod_valid_acc)\n        print(f\"Epoch {epoch + 1} \u2014\u2014 \" + \\\n            f\"Train Accuracy: {t_accuracy * 100 :.2f}%, \" + \\\n            f\"Valid Accuracy: {v_accuracy * 100 :.2f}%\\n\")\n\ntrain_model()","315fcc30":"test_csv = pd.read_csv(\"..\/input\/classify-leaves\/test.csv\")","46cc7419":"class ReadData(torch.utils.data.Dataset):\n    def __init__(self, csv_data, transform=None):\n        super(ReadData, self).__init__()\n        self.data = csv_data\n        self.transform = transform\n        \n    def __getitem__(self, idx):\n        img = Image.open(\"..\/input\/classify-leaves\/\" + self.data.loc[idx, \"image\"])\n        \n        if self.transform:\n            img = self.transform(img)\n        \n        return img\n        \n    def __len__(self):\n        return len(self.data)\n\ntest_iter = torch.utils.data.DataLoader(\n    ReadData(test_csv, valid_transform), batch_size=64,\n    num_workers=2, pin_memory=True\n)","e85841a3":"predict = None\nwith torch.no_grad():\n    for x in test_iter:\n        x = x.cuda()\n        p = torch.zeros((x.size()[0], len(class_to_num)))\n        for k in range(5):\n            model = models[f\"model_{k}\"][\"model\"]\n            p += model(x).detach().cpu()\n        if predict is None:\n            predict = p.argmax(1)\n        else:\n            predict = torch.cat([predict, p.argmax(1)])","eac904a7":"df = pd.read_csv(\"..\/input\/classify-leaves\/sample_submission.csv\")\ndf.label = predict.cpu().numpy()\ndf.label = df.label.apply(lambda x: num_to_class[x])\ndf.to_csv(\"\/kaggle\/working\/result.csv\", index=False)","bebf0c2e":"# \u6570\u636e\u589e\u5f3a\n### \u8fd9\u90e8\u5206\u9700\u8981\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u89c2\u5bdf\uff0c\u5bf9\u4e8e\u4e0d\u540c\u7684\u6570\u636e\u96c6\u9009\u62e9\u5408\u9002\u7684\u6570\u636e\u589e\u5f3a\u624b\u6bb5","107260bc":"# \u6570\u636e\u5206\u5272","55e8dec7":"# Train","ab3e17ed":"### \u56fe\u50cf\u53e0\u52a0","2e46a19c":"### \u56fe\u50cf\u6309\u4e00\u5b9a\u89d2\u5ea6\u7ffb\u8f6c","a5e14e10":"# \u8bfb\u53d6\u6570\u636e","05e2b293":"# TestModel","bd242313":"### \u5229\u7528\u8fc1\u79fb\u5b66\u4e60\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u5bf9\u8f93\u51fa\u5c42\u8fdb\u884c\u4fee\u6539\uff0c\u5bf9\u8d85\u53c2\u6570\u8fdb\u884c\u786e\u5b9a","2537312d":"### \u56fe\u50cf\u989c\u8272\u3001\u4eae\u5ea6\u3001\u8272\u8c03\u7b49","8a8e4a29":"### \u968f\u673a\u88c1\u526a"}}