{"cell_type":{"4a497c67":"code","6d52384d":"code","de4717a3":"code","fa2f1132":"code","3e4dcdad":"code","e2933a7b":"code","5d47c39b":"code","1ed2d301":"code","3c94832b":"code","0e9e2304":"code","8ffa28aa":"code","16800830":"code","a9489370":"code","3032ab3a":"code","992e42c2":"code","8120f889":"code","2be6f404":"code","37c75765":"code","b1782b2f":"code","37f5b852":"code","30cc1480":"code","745cd6ac":"code","abd14d7b":"code","1b1d1a57":"code","911c5e56":"code","c567c4a9":"code","1a1503e8":"code","fecc2845":"code","f08cacd8":"code","3f2f2bd5":"code","125d9d49":"code","343ec7fe":"code","2edf246b":"code","abae5a6c":"code","456be432":"code","a4800377":"code","cb97699f":"code","c82da28f":"code","a5d2bbfa":"code","f079874f":"code","2ecb357c":"code","baeb87fe":"code","383d1470":"code","674da166":"markdown","4eaafa31":"markdown","12cffc51":"markdown","097b09b2":"markdown"},"source":{"4a497c67":"# We have been provided with a dataset which consist of credit card details their transaction and \n#complete behavioural pattern of a user.Our task is group them into clusters based on \n#their activites in the past. \n","6d52384d":"#import all the required libraries \nimport pandas as pd \nimport numpy as np\nimport seaborn as sns\nfrom sklearn.preprocessing import PowerTransformer,StandardScaler\nfrom sklearn.decomposition import PCA\nfrom scipy.stats import zscore","de4717a3":"df = pd.read_csv('..\/input\/ccdata\/CC GENERAL.csv')\n","fa2f1132":"df.head()","3e4dcdad":"df.describe().T","e2933a7b":"df.info()","5d47c39b":"df.isnull().sum()\/df.shape[0]","1ed2d301":"#Fill those null values with mean ","3c94832b":"df['MINIMUM_PAYMENTS']=df['MINIMUM_PAYMENTS'].fillna(value=df['MINIMUM_PAYMENTS'].mean())\ndf['CREDIT_LIMIT']=df['CREDIT_LIMIT'].fillna(value=df['CREDIT_LIMIT'].mean())","0e9e2304":"#check for skewness","8ffa28aa":"import matplotlib.pyplot as plt ","16800830":"for i in df.select_dtypes(['int','float64']).columns:\n    sns.distplot(df[i])\n    plt.show()","a9489370":"#sing power transformer to deal with the outliers.","3032ab3a":"x=df.drop('CUST_ID',axis=1,inplace=True)","992e42c2":"from sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer()\n\nx_kmean = pt.fit_transform(df)\nx_kmean = pd.DataFrame(x_kmean,columns=df.columns)","8120f889":"for i in x_kmean.select_dtypes(['int','float64']).columns:\n    sns.distplot(x_kmean[i])\n    plt.show()","2be6f404":"x_kmean","37c75765":"import matplotlib.pyplot as plt","b1782b2f":"plt.figure(figsize=[12,12])\nsns.heatmap(df.corr(), annot=True)\n","37f5b852":"from sklearn.decomposition import PCA\npca = PCA(n_components=0.95)\nX_reduced = pca.fit_transform(x_kmean)\n","30cc1480":"from sklearn.cluster import KMeans\n\ncluster_range = np.arange(1,16)\ncluster_error = []\n\nfor i in cluster_range:\n    k = KMeans(n_clusters=i,n_init=14)\n    k.fit(x_kmean)\n    cluster_error.append(k.inertia_)\n    \nk_means = [KMeans(n_clusters=i,random_state=4).fit(x_kmean) for i in range(1,16)]","745cd6ac":"pd.DataFrame({'No of Clusters':cluster_range,'Cluster Errors':cluster_error})","abd14d7b":"plt.plot(cluster_range,cluster_error,marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Cluster Error')\nplt.show()","1b1d1a57":"kmeans = KMeans(n_clusters=3, random_state=23)\nkmeans.fit(X_reduced)\n","911c5e56":"df['label']=kmeans.labels_","c567c4a9":"plt.title('Predicted Classes')\nsns.scatterplot(data=df,x='CREDIT_LIMIT', y='ONEOFF_PURCHASES', hue='label')\nplt.show()","1a1503e8":"plt.title('Predicted Classes')\nsns.scatterplot(data=df,x='PAYMENTS', y='BALANCE_FREQUENCY', hue='label')\nplt.show() ","fecc2845":"plt.title('Predicted Classes')\nsns.scatterplot(data=df,x='PAYMENTS', y='PURCHASES_TRX', hue='label')\nplt.show() ","f08cacd8":"plt.title('Predicted Classes')\nsns.scatterplot(data=df,x='PAYMENTS', y='ONEOFF_PURCHASES', hue='label')\nplt.show()","3f2f2bd5":"from sklearn.preprocessing import PowerTransformer","125d9d49":"df.drop('CUST_ID',axis=1,inplace=True)","343ec7fe":"pt = PowerTransformer()\n\nx_ag = pt.fit_transform(df)\nx_ag = pd.DataFrame(x_ag,columns=df.columns)","2edf246b":"from sklearn.decomposition import PCA\npca = PCA(n_components=0.95)\nX_reducedagg = pca.fit_transform(x_ag)\n","abae5a6c":"from scipy.cluster.hierarchy import dendrogram,linkage\nplt.figure(figsize=(25,30))\nmerg=linkage(X_reducedagg,method='ward')\ndendrogram(merg,p=30,leaf_rotation=90,color_threshold=25,leaf_font_size=10,truncate_mode='level')\nplt.show()","456be432":"from sklearn.cluster import AgglomerativeClustering\nhie_clus=AgglomerativeClustering(n_clusters=3,affinity='euclidean',linkage='ward')\n","a4800377":"hie_clus.fit(X_reducedagg)","cb97699f":"df['class'] = hie_clus.labels_","c82da28f":"#Please note that lables is for k means and class denotes agglomerative clustering ","a5d2bbfa":"import matplotlib.pyplot as plt","f079874f":"plt.title('Predicted Classes')\nsns.scatterplot(data=df,x='PAYMENTS', y='ONEOFF_PURCHASES', hue='class')\nplt.show()","2ecb357c":"plt.title('Predicted Classes')\nsns.scatterplot(data=df,x='CREDIT_LIMIT', y='ONEOFF_PURCHASES', hue='class')\nplt.show()","baeb87fe":"#agglomerative clustering  \ndf.groupby('class').size()","383d1470":"#kmeans clustering\ndf.groupby('label').size()","674da166":"Now we are going to build our machine learning clustering models. \n\nwe will be using two methods; **k-means clustering** and **hierarchical clustering\/Agglomerative clustering** algorithms.\n\nFor k-means clustering algorithm:\n\n* First of all I will find the best k value.\n* Than I will use this k value to create a k-means model.\n* And I will compare my original and k-means clustered datas.\n\nFor hierarchical clustering algorithm:\n\n* First I will apply a dendrogram in order to find how many classes do I have in my data.\n* Than I will use this class number to apply a hierarchical clustering algorithm.\n* Lastly I will compare my original, k-means and hierarchical clustered datas.\n","4eaafa31":"# Agglomerative clustering","12cffc51":"# K means clustering","097b09b2":"# As we find that data  has mulitcollinearity we can deal with this using PCA. it Uses the information content that is delveloped by the two independant variables that is developed coherently.we take the independant variables and standardize the data.we then capture the information between the independant variables x1 and x2 using covariance matrix.(Note-This information would not have been captured if we had passed x as just an independant variable).Now we apply eigen funtion or decomposition,We get eigen vectors and eigen values. Eigen vector is nothing but the new axes. (The new axes is the orthogonal axes after standardazing and capturing the covariance matrix) We get 2 eigen vectors for x1 and x2 which are the new components.The variance captured by one vector is eigen value.Here we are passing the 95% to keep the number of components which captures 95%variance."}}