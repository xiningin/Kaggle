{"cell_type":{"36f288b7":"code","2e5cc2f3":"code","e3d258ff":"code","52d988a3":"code","fb475a1e":"code","5e5c1b85":"code","e6249a70":"code","bae471ff":"code","31053575":"code","93f8cc49":"code","95a54e83":"code","7fbe74f1":"code","ee457d2e":"code","bc99f9dd":"code","4efaa43c":"code","83178aba":"code","03c68c7e":"code","93ce300e":"code","a629c6c7":"code","10860b1c":"code","b51cbc09":"code","daa84759":"code","af1524f2":"code","8935d1ec":"code","18d9f624":"code","63f51f0e":"code","b50c57d3":"code","be2438f3":"code","c61bb70f":"code","f0cc911d":"code","672d3d3f":"code","f4b4cbb5":"code","494b579f":"code","de8375e4":"markdown","ece4b402":"markdown","dfbfb5db":"markdown","88110f75":"markdown","6aebb6b2":"markdown","1584b819":"markdown"},"source":{"36f288b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Plotting libraries\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(color_codes = True)\n%matplotlib inline\n\n##### Scikit Learn modules needed for Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder,MinMaxScaler , StandardScaler\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2e5cc2f3":"##loading data\ntrain_df =pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")","e3d258ff":"##now joinning train and testdataset for data processing\nfull_df = train_df.append(test_df, ignore_index=True)\nprint(f'There are {full_df.shape[0]} rows and {full_df.shape[1]} columns in the full dataframe.')","52d988a3":"full_df.head()","fb475a1e":"full_df.info()\n","5e5c1b85":"full_df.describe()","e6249a70":"sns.set_style('darkgrid')\nsns.countplot(data=full_df,x='Survived')\nplt.show()\n","bae471ff":"## here we can see that 350 peoples are survived and near about 550 people died in this disaster","31053575":"#function for converting male and female into 0 and 1\n\ndef clean_string(x):\n    if x == 'female':\n        return 1\n    else:\n        return 0\n","93f8cc49":"df_clean = full_df\ndf_clean['Sex'] = df_clean['Sex'].apply(clean_string)","95a54e83":"df_clean.head()","7fbe74f1":"## correlation will give relation between each variable\ncorrelation = df_clean.corr()\nplt.figure(figsize=(15,10) )\nsns.heatmap(correlation,annot = True,cmap = 'Blues')","ee457d2e":"df_clean.drop(['PassengerId','Name','SibSp','Parch','Ticket','Cabin','Embarked'],axis='columns',inplace=True)\n## dropping column which dont have any relation in data","bc99f9dd":"df_clean.head()","4efaa43c":"df_clean.isnull().sum()","83178aba":"df_clean1 = df_clean.dropna()","03c68c7e":"df_clean1.isnull().sum()","93ce300e":"df_clean1.info","a629c6c7":"inputs = df_clean1.drop('Survived',axis='columns')\ntarget = df_clean1.Survived","10860b1c":"inputs","b51cbc09":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(inputs,target,test_size=0.3,random_state= 20)","daa84759":"clf = LogisticRegression()\nclf.fit(X_train, y_train)\ny_predicted = clf.predict(X_test)\nscore = clf.score(X_test, y_test)","af1524f2":"print(score)","8935d1ec":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(inputs,target,test_size=0.3)","18d9f624":"from sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()","63f51f0e":"model.fit(X_train,y_train)","b50c57d3":"model.score(X_test,y_test)","be2438f3":"from sklearn.ensemble import RandomForestClassifier","c61bb70f":"# Build Model\nclf = RandomForestClassifier(criterion= \"entropy\",bootstrap = False,n_estimators = 1000,n_jobs = 2,verbose = 1,max_features =3)","f0cc911d":"clf.fit(X_train, y_train)\ny_predicted = clf.predict(X_test)\nscore= clf.score(X_test, y_test)","672d3d3f":"print(score)","f4b4cbb5":"##knn\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nmodel1 = KNeighborsClassifier(n_neighbors = 4)\nmodel1.fit(X_train,y_train)\nprediction_knn=model1.predict(X_test)\nscore= model1.score(X_test, y_test)","494b579f":"print(score)","de8375e4":"## Conclusion - in this problem logistic regression gives best result which is 81.39%","ece4b402":"### 7. Naive bayes model","dfbfb5db":"### above plot we can see that for survived  sex which depend +0.54 and fare depend +0.26 ","88110f75":"### 6. Build Logistic Regression Model","6aebb6b2":"### 8. Random forest model","1584b819":"## 4. visualize data"}}