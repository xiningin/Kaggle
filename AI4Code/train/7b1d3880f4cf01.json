{"cell_type":{"7eb8ab28":"code","6fb2c67c":"code","378d25c7":"code","8454e321":"code","d48e64f8":"code","26370c90":"code","09110e02":"code","3959a90b":"code","7f44f171":"code","35393116":"code","cc2c3514":"code","456a942c":"code","14e0b7ca":"markdown","4903073e":"markdown","4c1cf386":"markdown","4a806689":"markdown","3fdc27a9":"markdown","587ee15f":"markdown","73a23f91":"markdown"},"source":{"7eb8ab28":"import numpy as np \nimport pandas as pd \nimport matplotlib.pylab as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.simplefilter('ignore')","6fb2c67c":"data_credit = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')[:100_000] # Load data","378d25c7":"data_credit.head() # Checking the first 5 rows of data","8454e321":"data_credit.tail() # Checking the las 5 rows of data","d48e64f8":"X = data_credit.drop(['Time', 'Amount', 'Class'], axis=1).values # Data X\ny = data_credit['Class'].values # Data y","26370c90":"f\"Shapes of X={X.shape} y={y.shape} #Fraud Cases={y.sum()}\" # Let's check the shape","09110e02":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(class_weight={0: 1, 1: 2}, max_iter=1000) # Class 0 is Non Fraud, and 1 is Fraud\nmodel.fit(X, y).predict(X).sum()","3959a90b":"from sklearn.model_selection import GridSearchCV # Importing GridSearchCV for our model\nfrom sklearn.metrics import precision_score, recall_score, make_scorer # Importing Metrics for score\n\ngrid = GridSearchCV( # Accomodate GridSearchCV\n    estimator=LogisticRegression(max_iter=1000), # Let's set the iteration to 1000\n    param_grid={'class_weight': [{0:1, 1: v} for v in np.linspace(1, 20, 30)]}, # Create a linearspace\n    scoring={'precision': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, # Create a scoring for our model\n    refit='precision', # Let's focus on precision, which mean fraud card\n    return_train_score=True, # Return train core\n    cv=10, # Let's cross validate to 10, for more accurate\n    n_jobs=-1 # n_jobs to -1\n)\n\ngrid.fit(X, y) # Fit the model","7f44f171":"data = pd.DataFrame(grid.cv_results_) # Accomodate the result to Data Frame\ndata.head() # Let's see the first 5 rows of this cv result","35393116":"plt.figure(figsize=(12, 4)) # Figure the size\nfor score in ['mean_test_recall_score', 'mean_test_precision']: # Make for loop to mean test recall and precision\n    plt.plot([_[1] for _ in data['param_class_weight']], # plt the param class weight\n             data[score], # data score\n             label=score  # label to score too\n)\n\nplt.xlabel('class weight') # Set the x label\nplt.ylabel('score') # Set the y label\nplt.legend() # Make a legend","cc2c3514":"plt.figure(figsize=(12, 4)) # Figure the size\nfor score in ['mean_train_recall_score', 'mean_train_precision']: # Make for loop to mean test recall and precision\n    plt.plot([_[1] for _ in data['param_class_weight']], # plt the param class weight\n             data[score], # data score\n             label=score # label to score too\n)\n\nplt.xlabel('class weight') # Set the x label\nplt.ylabel('score') # Set the y label\nplt.legend() # Make a legend","456a942c":"??model.score ","14e0b7ca":"# **Divide Data to X and y**","4903073e":"# **Models**","4c1cf386":"# **Description**","4a806689":"# **Load Data**","3fdc27a9":"# **Plotting**","587ee15f":"If you looks the output above, it seems like the data is unbalanced","73a23f91":"# **Import Libraries**"}}