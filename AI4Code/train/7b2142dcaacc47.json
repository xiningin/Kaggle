{"cell_type":{"dfefc208":"code","6cd0fb6c":"code","62e31faa":"code","015825cc":"code","373270d4":"code","3d66af30":"code","75f9f2b4":"code","353c9460":"code","167bd17d":"code","e26943ca":"code","7edafdaf":"code","eadcac91":"code","abc80912":"code","4cbb24f4":"code","2acdc89c":"code","7e026888":"code","30c1c0f1":"code","2b4679c7":"code","5b070749":"code","ec95b359":"code","368a7533":"code","2a5a3350":"code","791d252e":"code","1741e775":"code","ab1e24b1":"code","bf3fc23f":"code","499a4e4c":"code","c4293e3a":"code","7ce869f0":"code","c58adc7a":"code","309ead94":"code","bbbdad72":"code","5f0e6e53":"code","d7413feb":"code","4b8117b0":"code","e52855bf":"code","b5fbb165":"code","791d21b3":"code","afe3bc08":"code","5548a813":"code","f9e9d6dd":"code","4b816e58":"code","7b91cf6c":"markdown","ab800e8e":"markdown","e4ccc062":"markdown","b56c4903":"markdown"},"source":{"dfefc208":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6cd0fb6c":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf","62e31faa":"# random seed fixed\nnp.random.seed(5)\ntf.random.set_seed(5)","015825cc":"train = pd.read_csv('\/kaggle\/input\/dacon-stage1\/train.csv')\ntrain.head(3)","373270d4":"test = pd.read_csv('\/kaggle\/input\/dacon-stage1\/test.csv')\ntest = test.drop('id', axis=1)\ntest.head(3)","3d66af30":"plt.figure(figsize=(18, 5))\nsns.countplot(x=train['layer_1'])","75f9f2b4":"plt.figure(figsize=(18, 5))\nsns.countplot(x=train['layer_2'])","353c9460":"plt.figure(figsize=(18, 5))\nsns.countplot(x=train['layer_3'])","167bd17d":"plt.figure(figsize=(18, 5))\nsns.countplot(x=train['layer_4'])","e26943ca":"plt.figure(figsize=(20, 15))\nsns.heatmap(train.corr())","7edafdaf":"plt.figure(figsize=(18, 5))\nsns.boxplot(train['layer_1'], train['1'])","eadcac91":"plt.figure(figsize=(18, 5))\nsns.boxplot(train['layer_1'], train['100'])","abc80912":"plt.figure(figsize=(18, 5))\nsns.boxplot(train['layer_1'], train['200'])","4cbb24f4":"plt.figure(figsize=(18, 5))\nsns.boxplot(train['layer_1'], train['225'])","2acdc89c":"train.describe().T","7e026888":"train.groupby('layer_1').mean().T","30c1c0f1":"train.groupby('layer_1').max().T","2b4679c7":"train.groupby('layer_1').min().T","5b070749":"# using keras model\n# train data -> train \/ validation split. case 1.\n\nX = train.drop(['layer_1', 'layer_2', 'layer_3', 'layer_4'], axis=1)\nY = train[['layer_1', 'layer_2', 'layer_3', 'layer_4']]\n\nX_train_1, X_vali_1, Y_train_1, Y_vali_1 = train_test_split(X, Y, random_state=1, test_size=0.05)\nX_train_1.shape, Y_train_1.shape, X_vali_1.shape, Y_vali_1.shape","ec95b359":"# train data -> train \/ validation split. case 2.\nX_train_2, X_vali_2, Y_train_2, Y_vali_2 = train_test_split(X, Y, random_state=2, test_size=0.05)\nX_train_2.shape, Y_train_2.shape, X_vali_2.shape, Y_vali_2.shape","368a7533":"# train data -> train \/ validation split. case 3.\nX_train_3, X_vali_3, Y_train_3, Y_vali_3 = train_test_split(X, Y, random_state=3, test_size=0.05)\nX_train_3.shape, Y_train_3.shape, X_vali_3.shape, Y_vali_3.shape","2a5a3350":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, BatchNormalization, LeakyReLU\nfrom keras.optimizers import adam\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.callbacks import ModelCheckpoint\n\nimport warnings\nwarnings.filterwarnings('ignore')","791d252e":"# Generate Keras Model. case 1.\nmodel_1 = Sequential()\nmodel_1.add(Dense(units=2048, input_dim=226))\nmodel_1.add(BatchNormalization())\nmodel_1.add(LeakyReLU(alpha=0.1))\nmodel_1.add(Dense(units=2048))\nmodel_1.add(BatchNormalization())\nmodel_1.add(LeakyReLU(alpha=0.1))\nmodel_1.add(Dense(units=2048))\nmodel_1.add(BatchNormalization())\nmodel_1.add(LeakyReLU(alpha=0.1))\nmodel_1.add(Dense(units=2048))\nmodel_1.add(BatchNormalization())\nmodel_1.add(LeakyReLU(alpha=0.1))\nmodel_1.add(Dense(units=2048))\nmodel_1.add(BatchNormalization())\nmodel_1.add(LeakyReLU(alpha=0.1))\nmodel_1.add(Dense(units=4))\nmodel_1.summary()","1741e775":"# Generate Keras Model. case 2.\nmodel_2 = Sequential()\nmodel_2.add(Dense(units=2048, input_dim=226))\nmodel_2.add(BatchNormalization())\nmodel_2.add(LeakyReLU(alpha=0.1))\nmodel_2.add(Dense(units=2048))\nmodel_2.add(BatchNormalization())\nmodel_2.add(LeakyReLU(alpha=0.1))\nmodel_2.add(Dense(units=2048))\nmodel_2.add(BatchNormalization())\nmodel_2.add(LeakyReLU(alpha=0.1))\nmodel_2.add(Dense(units=2048))\nmodel_2.add(BatchNormalization())\nmodel_2.add(LeakyReLU(alpha=0.1))\nmodel_2.add(Dense(units=2048))\nmodel_2.add(BatchNormalization())\nmodel_2.add(LeakyReLU(alpha=0.1))\nmodel_2.add(Dense(units=4))\nmodel_2.summary()","ab1e24b1":"# Generate Keras Model. case 3.\nmodel_3 = Sequential()\nmodel_3.add(Dense(units=2048, input_dim=226))\nmodel_3.add(BatchNormalization())\nmodel_3.add(LeakyReLU(alpha=0.1))\nmodel_3.add(Dense(units=2048))\nmodel_3.add(BatchNormalization())\nmodel_3.add(LeakyReLU(alpha=0.1))\nmodel_3.add(Dense(units=2048))\nmodel_3.add(BatchNormalization())\nmodel_3.add(LeakyReLU(alpha=0.1))\nmodel_3.add(Dense(units=2048))\nmodel_3.add(BatchNormalization())\nmodel_3.add(LeakyReLU(alpha=0.1))\nmodel_3.add(Dense(units=2048))\nmodel_3.add(BatchNormalization())\nmodel_3.add(LeakyReLU(alpha=0.1))\nmodel_3.add(Dense(units=4))\nmodel_3.summary()","bf3fc23f":"# opti = keras.optimizers.Adam(lr=0.002)\n# model.compile(loss='mae', optimizer=opti, metrics=['mae'])","499a4e4c":"# model compile\nmodel_1.compile(loss='mae', optimizer='adam', metrics=['mae'])\nmodel_2.compile(loss='mae', optimizer='adam', metrics=['mae'])\nmodel_3.compile(loss='mae', optimizer='adam', metrics=['mae'])","c4293e3a":"# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n#                               patience=3, min_lr=0.000125)","7ce869f0":"check_best_1 = keras.callbacks.ModelCheckpoint(filepath='best_model_1_ep250', monitor='val_loss',\nverbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n\ncheck_best_2 = keras.callbacks.ModelCheckpoint(filepath='best_model_2_ep250', monitor='val_loss',\nverbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n\ncheck_best_3 = keras.callbacks.ModelCheckpoint(filepath='best_model_3_ep250', monitor='val_loss',\nverbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)","c58adc7a":"from keras.callbacks import LearningRateScheduler\nimport math\n# learning rate schedule\ndef step_decay(epoch):\n\tinitial_lrate = 0.001\n\tdrop = 0.2\n\tepochs_drop = 60\n\tlrate = initial_lrate * math.pow(drop, math.floor((1+epoch)\/epochs_drop))\n\treturn lrate\n\n# learning schedule callback\nlrate = LearningRateScheduler(step_decay)","309ead94":"callback_list_1 = [lrate, check_best_1]\ncallback_list_2 = [lrate, check_best_2]\ncallback_list_3 = [lrate, check_best_3]","bbbdad72":"# model_1 training\nhistory_1 = model_1.fit(X_train_1, Y_train_1, epochs=5, batch_size=256,\n                        validation_data=(X_vali_1, Y_vali_1), callbacks=callback_list_1)\n# Originally used epochs = 250","5f0e6e53":"# model_2 training\nhistory_2 = model_2.fit(X_train_2, Y_train_2, epochs=5, batch_size=256,\n                        validation_data=(X_vali_2, Y_vali_2), callbacks=callback_list_2)\n# Originally used epochs = 250","d7413feb":"# model_3 training\nhistory_3 = model_3.fit(X_train_3, Y_train_3, epochs=5, batch_size=256,\n                        validation_data=(X_vali_3, Y_vali_3), callbacks=callback_list_3)\n# Originally used epochs = 250","4b8117b0":"# Plot training & validation loss values\nplt.figure(figsize=(10,7))\nplt.plot(history_1.history['loss'])\nplt.plot(history_1.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","e52855bf":"# best model loading\nfrom tensorflow.keras.models import load_model\nbest_model_1 = load_model('best_model_1_ep250')\nbest_model_2 = load_model('best_model_2_ep250')\nbest_model_3 = load_model('best_model_3_ep250')\n\n# validation mae\npred_vali_1 = best_model_1.predict(X_vali_1)\npred_vali_2 = best_model_2.predict(X_vali_2)\npred_vali_3 = best_model_3.predict(X_vali_3)\nprint(mean_absolute_error(Y_vali_1, pred_vali_1))\nprint(mean_absolute_error(Y_vali_2, pred_vali_2))\nprint(mean_absolute_error(Y_vali_3, pred_vali_3))","b5fbb165":"# predict test set\npred_test_1 = best_model_1.predict(test)\npred_test_2 = best_model_2.predict(test)\npred_test_3 = best_model_3.predict(test)","791d21b3":"# load submission file\nsample_sub = pd.read_csv('\/kaggle\/input\/dacon-stage1\/sample_submission.csv', index_col=0)\nsample_sub.head(3)","afe3bc08":"# blend the predictions.\nsub = sample_sub + (0.34*pred_test_1 + 0.33*pred_test_2 + 0.33*pred_test_3)\nsub.head()","5548a813":"# post-processing\nprint((sub['layer_1'] < 10).sum())\nprint((sub['layer_2'] < 10).sum())\nprint((sub['layer_3'] < 10).sum())\nprint((sub['layer_4'] < 10).sum())\nprint((sub['layer_1'] > 300).sum())\nprint((sub['layer_2'] > 300).sum())\nprint((sub['layer_3'] > 300).sum())\nprint((sub['layer_4'] > 300).sum())","f9e9d6dd":"sub.loc[sub['layer_1'] < 10, 'layer_1'] = 10\nsub.loc[sub['layer_2'] < 10, 'layer_2'] = 10\nsub.loc[sub['layer_3'] < 10, 'layer_3'] = 10\nsub.loc[sub['layer_4'] < 10, 'layer_4'] = 10\n\nsub.loc[sub['layer_1'] > 300, 'layer_1'] = 300\nsub.loc[sub['layer_2'] > 300, 'layer_2'] = 300\nsub.loc[sub['layer_3'] > 300, 'layer_3'] = 300\nsub.loc[sub['layer_4'] > 300, 'layer_4'] = 300\n\nprint((sub['layer_1'] < 10).sum())\nprint((sub['layer_2'] < 10).sum())\nprint((sub['layer_3'] < 10).sum())\nprint((sub['layer_4'] < 10).sum())\nprint((sub['layer_1'] > 300).sum())\nprint((sub['layer_2'] > 300).sum())\nprint((sub['layer_3'] > 300).sum())\nprint((sub['layer_4'] > 300).sum())","4b816e58":"# save the csv file\nsub.to_csv('dacon_stage_1_submission_pred_test_post_processing.csv')","7b91cf6c":"**Read data & Simple EDA**","ab800e8e":"**Modeling**","e4ccc062":"This code participated in the 'Monthly Dacon 1 Semiconductor Thin Film Thickness Analysis(\uc6d4\uac04 \ub370\uc774\ucf58 1 \ubc18\ub3c4\uccb4 \ubc15\ub9c9 \ub450\uaed8 \ubd84\uc11d)' conducted by DACON, a Korean data science \/ machin learning platform.\n\nUsing the code above, I was able to get high scores(within top 10%).\n\nHowever, even though the seed is fixed, the score changes slightly each time, and I don't know why. I could use this code to get a MAE between 0.5 and 0.6.\n\nThe dataset is Dacon's asset, so maybe I can't upload it and attach an address this competition\/dataset.\n\nThank you.\n\nAddress: https:\/\/dacon.io\/competitions\/official\/235554\/overview\/description\/\n\nps. I'm still a student studying machine learning, so there may be some inefficient code. I hope you understand that.","b56c4903":"**Best model load. Test set prediction**"}}