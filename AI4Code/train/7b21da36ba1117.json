{"cell_type":{"db07fbbd":"code","be1eec1d":"code","defbb026":"code","6f39370c":"code","3382ed10":"code","e7dfe27f":"code","fb4f3ab3":"code","1f3d40cb":"code","17a38b55":"code","a3f9a68e":"code","f69090c0":"code","35f00203":"code","23f8eba1":"code","61d7a9ba":"code","6c82fd1a":"code","51dc1907":"code","871d450e":"code","a3568bd5":"code","29befe1e":"code","d62e7194":"code","49daf74b":"code","1170cb68":"code","9224f04f":"code","8a612a63":"code","45f3579c":"code","247f4dbc":"code","51e9c697":"markdown"},"source":{"db07fbbd":"%matplotlib inline\n\nimport gc\n\nfrom category_encoders.ordinal import OrdinalEncoder\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve\nfrom sklearn.metrics import recall_score, classification_report, auc, roc_curve\nfrom sklearn.metrics import precision_recall_fscore_support, f1_score, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom tqdm import tqdm","be1eec1d":"input_dir = '..\/input\/ieee-fraud-detection\/'\ntrain_id_file = input_dir + 'train_identity.csv'\ntrain_trans_file = input_dir + 'train_transaction.csv'\n\ntest_id_file = input_dir + 'test_identity.csv'\ntest_trans_file = input_dir + 'test_transaction.csv'","defbb026":"df_id = pd.read_csv(train_id_file, index_col='TransactionID')\ndf_trans = pd.read_csv(train_trans_file, index_col='TransactionID')\ntrain = df_trans.merge(df_id, how='left', left_index=True, right_index=True)\ndel df_id, df_trans\ntrain.shape","6f39370c":"gc.collect()","3382ed10":"X = train.drop('isFraud', axis=1)\ny = train['isFraud'].copy()\ndel train\ngc.collect()","e7dfe27f":"cat_fea = ['ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n           'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain',\n           'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n           'DeviceType', 'DeviceInfo'] + ['id_' + str(i) for i in range(12, 39)]\nnum_fea = []\ndrop_fea = []","fb4f3ab3":"cat_fea = list(set(cat_fea) - set(drop_fea))","1f3d40cb":"num_fea += list(X.loc[:, ~X.columns.isin(cat_fea + drop_fea)])","17a38b55":"X = X.drop(drop_fea, axis=1)","a3f9a68e":"%%time\n\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('std_scaler', StandardScaler()),\n], verbose=True)\n\nX[num_fea] = num_pipeline.fit_transform(X[num_fea], y)","f69090c0":"X = X.fillna(-999)","35f00203":"for col in tqdm(num_fea):\n    if X[col].dtype == 'float64':\n        X[col] = X[col].astype(np.float32)\n    if (X[col].dtype == 'int64') or (col in cat_fea):\n        X[col] = X[col].astype(np.int32)","23f8eba1":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=False)","61d7a9ba":"def rounded_accuracy(y_true, y_pred):\n    return keras.metrics.binary_accuracy(tf.round(y_true), tf.round(y_pred))\n\nK = keras.backend\ndef kl_divergence(p, q):\n    return p * K.log(p \/ q) + (1 - p) * K.log((1 - p) \/ (1 - q))\n\nclass KLDivergenceRegularizer(keras.regularizers.Regularizer):\n    def __init__(self, weight, target=0.1):\n        self.weight = weight\n        self.target = target        \n        \n    def __call__(self, inputs):\n        mean_activities = K.mean(inputs)\n        return self.weight * (\n            kl_divergence(self.target, mean_activities) +\n            kl_divergence(1. - self.target, 1. - mean_activities))\n    \n    def get_config(self):\n        return {\"weight\": self.weight, 'target': self.target}","6c82fd1a":"def sparse_autoencoder(n_input):   \n    tf.random.set_random_seed(42)\n    np.random.seed(42)\n\n    kld_reg = KLDivergenceRegularizer(weight=0.05, target=0.1)\n    sparse_kl_encoder = keras.models.Sequential([\n        keras.layers.Dense(100, activation=\"selu\", input_shape=(n_input,), \n                           kernel_initializer='lecun_normal'),\n        keras.layers.Dense(300, activation=\"sigmoid\", activity_regularizer=kld_reg)\n    ])\n    sparse_kl_decoder = keras.models.Sequential([\n        keras.layers.Dense(100, activation=\"selu\", input_shape=[300], \n                           kernel_initializer='lecun_normal'),\n        keras.layers.Dense(n_input, activation=None),        \n    ])\n    \n    sparse_kl_ae = keras.models.Sequential([sparse_kl_encoder, sparse_kl_decoder])\n    \n    sparse_kl_ae.compile(loss=\"mean_squared_error\", \n                         optimizer='nadam',\n                         metrics=['acc', rounded_accuracy])\n    \n    return sparse_kl_ae","51dc1907":"train = X_train[num_fea]\nval = X_val[num_fea]\n\nmodel = sparse_autoencoder(train.shape[1])\nlre = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n                                        patience=3, \n                                        verbose=1, \n                                        factor=0.5, \n                                        min_lr=0.00001)\n\nes = keras.callbacks.EarlyStopping(monitor='val_loss', verbose=1, patience=6)\n\nmodel.fit(train, train,\n          callbacks=[lre, es],     \n          validation_data=[val, val],\n          batch_size=32, \n          epochs=50, \n          verbose=2)","871d450e":"X_val_pred = model.predict(val)\nmse = np.mean(np.power(val - X_val_pred, 2), axis=1)\nerror_df = pd.DataFrame({'Reconstruction_error': mse, 'True_class': y_val})\nerror_df.describe()","a3568bd5":"false_pos_rate, true_pos_rate, thresholds = roc_curve(error_df.True_class, error_df.Reconstruction_error)\nroc_auc = auc(false_pos_rate, true_pos_rate,)\n\nplt.plot(false_pos_rate, true_pos_rate, linewidth=5, label='AUC = %0.3f'% roc_auc)\nplt.plot([0,1],[0,1], linewidth=5)\n\nplt.xlim([-0.01, 1])\nplt.ylim([0, 1.01])\nplt.legend(loc='lower right')\nplt.title('Receiver operating characteristic curve (ROC)')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","29befe1e":"precision_rt, recall_rt, threshold_rt = precision_recall_curve(error_df.True_class, error_df.Reconstruction_error)\nplt.plot(recall_rt, precision_rt, linewidth=5, label='Precision-Recall curve')\nplt.title('Recall vs Precision')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.show()","d62e7194":"plt.plot(threshold_rt, precision_rt[1:], label=\"Precision\",linewidth=5)\nplt.plot(threshold_rt, recall_rt[1:], label=\"Recall\",linewidth=5)\nplt.title('Precision and recall for different threshold values')\nplt.xlabel('Threshold')\nplt.ylabel('Precision\/Recall')\nplt.legend()\nplt.show()","49daf74b":"threshold_fixed = 2\ngroups = error_df.groupby('True_class')\nfig, ax = plt.subplots()\n\nfor name, group in groups:\n    ax.plot(group.index.values, group.Reconstruction_error.values, marker='o', ms=3.5, linestyle='',\n            label= \"Fraud\" if name == 1 else \"Normal\")    \n    \nax.hlines(threshold_fixed, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\nax.legend()\nplt.title(\"Reconstruction error for different classes\")\nplt.ylabel(\"Reconstruction error\")\nplt.xlabel(\"Data point index\")\nplt.gcf().set_size_inches(15, 10)\nplt.show();","1170cb68":"y_val_pred = [1 if e > threshold_fixed else 0 for e in error_df.Reconstruction_error.values]\nconf_matrix = confusion_matrix(error_df.True_class, y_val_pred)\n\nplt.figure(figsize=(12, 12))\nsns.heatmap(conf_matrix, xticklabels=[\"Normal\",\"Fraud\"], yticklabels=[\"Normal\",\"Fraud\"], annot=True, fmt=\"d\");\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","9224f04f":"print(classification_report(y_val, y_val_pred))","8a612a63":"def min_max_normalization(x):\n    x_min = x.min()\n    x_max = x.max()\n    x_norm = (x-x_min) \/ (x_max-x_min)\n    return x_norm","45f3579c":"roc_auc_score(y_val, min_max_normalization(error_df.Reconstruction_error.values))","247f4dbc":"confusion_matrix(y_val, y_val_pred)","51e9c697":"I've played with training autoencoder for anomaly detection. Best one I found so far is sparse autoencoder with kl-divergence regularizer. Right now I'm using only numerical features without any engineering (only base standarization). Score on the validation set (headout - last 20% of training data) maybe is not the best - only 0.763, but analyzing reconstruction error provides some interesting clues about data (especially correlation with time) and could be useful for further feature engineering for the other models. I must also say that this topic is simply fun :)"}}