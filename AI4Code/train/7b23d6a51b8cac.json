{"cell_type":{"14482450":"code","b3c00db7":"code","495b2b00":"code","75638287":"code","5139acda":"code","ce96a666":"code","3343abf5":"code","cec3aeac":"code","815ac669":"code","1889cb04":"code","9114f826":"code","c28f5ecd":"code","8eac2cd5":"code","cbedafba":"code","4d953e0d":"code","29aa31cd":"code","ab23aec1":"code","7bfe5c80":"code","77eda50c":"code","dc3ed0d9":"code","9b498659":"code","be181797":"code","ebd5f69f":"code","a32c83c0":"code","12b1b59c":"code","f94c5746":"code","8e787dae":"code","e4886d6b":"code","dc800b69":"code","24bbb679":"code","9013ba9c":"code","fff9d168":"code","b4b39b53":"code","a0edba7e":"code","458805e5":"code","cdff9425":"code","760ec8b2":"code","b7c06344":"code","22626265":"code","031b5e0c":"code","965e60f4":"markdown","19279107":"markdown","774326e9":"markdown","7d39e81d":"markdown","81197799":"markdown","11b4ba28":"markdown","f1bbab79":"markdown","b3a742f0":"markdown","1878e372":"markdown","dc70a9f6":"markdown","2b73bb53":"markdown","aa12f68e":"markdown","d2efc5bf":"markdown"},"source":{"14482450":"!pip install -U transformers","b3c00db7":"import torch, torchvision\nprint(torch.__version__, torch.cuda.is_available())","495b2b00":"!python -m pip install 'git+https:\/\/github.com\/facebookresearch\/detectron2.git'","75638287":"!pip install -U datasets seqeval","5139acda":"import pandas as pd\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm\nfrom tqdm import tqdm_notebook as tqdm # progress bar\nimport warnings\nwarnings.filterwarnings('ignore') #Ignore \"future\" warnings and Data-Frame-Slicing warnings.\n","ce96a666":"df1 = pd.read_csv('..\/input\/segmented-articles\/Data_OCR\/Data_OCR\/df_valid.csv')\ndf1[\"imgdir\"]=\"..\/input\/segmented-articles\/Segmented_articles\/Segmented_articles\/Validation\"\nprint(df1.shape)\n\ndf2 = pd.read_csv('..\/input\/segmented-articles\/Data_OCR\/Data_OCR\/df_train1.csv')\ndf2[\"imgdir\"]=\"..\/input\/segmented-articles\/Segmented_articles\/Segmented_articles\/Training\"\nprint(df2.shape)\n\ndf3 = pd.read_csv('..\/input\/segmented-articles\/Data_OCR\/Data_OCR\/df_train2.csv')\ndf3[\"imgdir\"]=\"..\/input\/segmented-articles\/Segmented_articles\/Segmented_articles\/Training\"\nprint(df3.shape)\n\ndf = pd.concat([df1, df2,df3])\nprint(df.shape)\n","3343abf5":"df[\"left\"]=(1000\/df[\"width_img\"])*df[\"left\"]\ndf[\"width\"]=(1000\/df[\"width_img\"])*df[\"width\"]\ndf[\"top\"]=(1000\/df[\"height_img\"])*df[\"top\"]\ndf[\"height\"]=(1000\/df[\"height_img\"])*df[\"height\"]","cec3aeac":"df=df[df.text.apply(lambda x: len(str(x))>1)]\n#df=df[df.text.apply(lambda x: len(str(x))<15)]\ndf=df[df.label!=\"section\"]\ndf=df[df.label!=\"Title\"]\ndf=df.reset_index(drop=True)\nprint(df.shape)\ndf.head()","815ac669":"labels=df['label'].unique()\nlabels=labels.tolist()\nprint(type(labels))\nprint(labels)","1889cb04":"id2label = {v: k for v, k in enumerate(labels)}\nlabel2id = {k: v for v, k in enumerate(labels)}\nprint(label2id)\nprint(id2label)","9114f826":"df.groupby(['label']).size()","c28f5ecd":"label2color = {\"header\": \"Red\",\"article\": \"Green\",\"ad\": \"Yellow\"}\n\n#['shsy5y', 'astro', 'cort']\ndef Draw_BBox(image_path,_bbox_labeled: pd.DataFrame):\n    actual_boxes = []\n    for idx, row in _bbox_labeled.iterrows():\n        x0 = row[\"left\"]\n        y0 = row[\"top\"]\n        x1 = row[\"width\"]+row[\"left\"]\n        y1 = row[\"height\"]+row[\"top\"]\n        label= row['label']\n        conf= row['conf']\n        color=label2color[label]\n        actual_box = [x0, y0, x1, y1] \n        draw = ImageDraw.Draw(image, \"RGB\")\n        draw.rectangle(actual_box, outline=color,width=1)\n        draw.text((actual_box[0] + 10, actual_box[1] - 10), text=f'{label}', fill=color)\n        #new_image = image.resize((1000, 1000))\n    return image\n\nsample_filename=\"alqabas 28-7.pdf page 24.png\"\n\nfrom PIL import Image, ImageDraw, ImageFont\nimage = Image.open(f\"..\/input\/segmented-articles\/Segmented_articles\/Segmented_articles\/Validation\/{sample_filename}\")\nimage = image.convert(\"RGB\").resize((1000, 1000))\nsample_image_df = df[df['image_id'] == sample_filename]\n#image\nsample_image_df=sample_image_df.sample(200)\nDraw_BBox(image,sample_image_df)","8eac2cd5":"#imgdir=\"..\/input\/segmented-articles\/Segmented_articles\/Segmented_articles\/Validation\"\ndf_meta=df\ndf_meta=df_meta.drop_duplicates(subset=['image_id'])\ndf_meta=df_meta.reset_index(drop=True)\ndf_meta[\"id\"]=df_meta.index\ndf_meta[\"image_path\"]=df_meta[\"imgdir\"] +'\/'+ df_meta[\"image_id\"] \ndf_meta=df_meta[[\"image_id\",\"image_path\"]]\ndf_meta[\"split\"]=\"train\"\n","cbedafba":"print(df_meta.shape)\n####\nratio_split=int(len(df_meta)*0.15)\ndf_meta.loc[df_meta.index[-ratio_split:], 'split'] = \"test\" # last 20 well be validation dataset\nprint(\"size of training_dataset=\",len(df_meta[df_meta.split==\"train\"]),\"---- size of test_dataset=\",len(df_meta[df_meta.split==\"test\"]))\n#########\ndf_meta","4d953e0d":"from datasets import Dataset\ntrain_meta=df_meta[df_meta.split==\"train\"]\ntrain_meta=train_meta.drop(columns=['split'])\ntrain_meta=train_meta.reset_index(drop=True)\ndataset_train = Dataset.from_pandas(train_meta)\nprint(dataset_train)\n#######################################################################\ntest_meta=df_meta[df_meta.split==\"test\"]\ntest_meta=test_meta.drop(columns=['split'])\ntest_meta=test_meta.reset_index(drop=True)\ndataset_test = Dataset.from_pandas(test_meta)\nprint(dataset_test)","29aa31cd":"print(type(dataset_train[0]))\ndataset_train[0]","ab23aec1":"def get_words_and_boxes(examples):\n    image_id = examples['image_id']\n    record = {}\n    objs1 = []\n    objs2 = []\n    objs3 = []\n    for index2, row in df.query(\"image_id == @image_id\").iterrows():                       \n        class_id =label2id[row[\"label\"]]\n        word = row[\"text\"]\n        bbox_resized = [\n            int(row[\"left\"]),\n            int(row[\"top\"]),\n            int(row[\"left\"])+int(row[\"width\"]),\n            int(row[\"top\"])+int(row[\"height\"]),\n                        ]         \n        obj1 = word\n        obj2 = bbox_resized\n        obj3 = class_id\n        objs1.append(obj1)\n        objs2.append(obj2)\n        objs3.append(obj3)\n        record[\"words\"] = objs1\n        record[\"bboxes\"] = objs2\n        record[\"classes\"] = objs3\n    #examples.append(record) \n    examples['words'] = [objs1]\n    examples['bboxes'] = [objs2]\n    examples['classes'] = [objs3]\n    return examples\n    \ndataset_train_with_bboxs = dataset_train.map(get_words_and_boxes, batched=True, batch_size=1)\ndataset_test_with_bboxs = dataset_test.map(get_words_and_boxes, batched=True, batch_size=1)","7bfe5c80":"print(type(dataset_train_with_bboxs))\nprint(\"##############################\")\nprint(type(dataset_train_with_bboxs[0]))\nprint(\"##############################\")\ndataset_train_with_bboxs","77eda50c":"#print(dataset_with_bboxs[0][\"words\"])","dc3ed0d9":"from PIL import Image, ImageDraw, ImageFont\nexample = dataset_train_with_bboxs[0]\nprint(example['image_path'])\nprint(\"--------------------\")\nprint(example.keys())\nprint(\"--------------------\")\nimage = Image.open(example['image_path'])\nimage = image.convert(\"RGB\")\nnew_image = image.resize((400, 600))\nnew_image","9b498659":"from PIL import Image\nfrom transformers import LayoutLMv2Processor\nfrom datasets import Features, Sequence, ClassLabel, Value, Array2D, Array3D\nfrom transformers import pipeline\n\nprocessor = LayoutLMv2Processor.from_pretrained(\"microsoft\/layoutlmv2-base-uncased\", revision=\"no_ocr\")\n\n# we need to define custom features\nfeatures = Features({\n    'image': Array3D(dtype=\"int64\", shape=(3, 224, 224)),\n    'input_ids': Sequence(feature=Value(dtype='int64')),\n    'attention_mask': Sequence(Value(dtype='int64')),\n    'token_type_ids': Sequence(Value(dtype='int64')),\n    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\n    'labels': Sequence(ClassLabel(names=labels)),\n})\n#.resize((224, 224))\ndef preprocess_data(examples):\n  images = [Image.open(path).convert(\"RGB\") for path in examples['image_path']]\n  words = examples['words']\n  boxes = examples['bboxes']\n  word_labels = examples['classes']\n  \n  encoded_inputs = processor(image, words,boxes=boxes,word_labels=word_labels,padding=\"max_length\", truncation=True)\n  \n  return encoded_inputs\n\n\ntrain_dataset = dataset_train_with_bboxs.map(preprocess_data, batched=True, batch_size=1, \n                                       remove_columns=dataset_train_with_bboxs.column_names,\n                                       features=features)\ntest_dataset = dataset_test_with_bboxs.map(preprocess_data, batched=True, batch_size=1, \n                                       remove_columns=dataset_test_with_bboxs.column_names,\n                                       features=features)","be181797":"train_dataset[0].keys()","ebd5f69f":"print(train_dataset[0][\"labels\"])","a32c83c0":"processor.tokenizer.decode(train_dataset['input_ids'][11])","12b1b59c":"train_dataset.set_format(type=\"torch\", device=\"cuda\")\ntest_dataset.set_format(type=\"torch\", device=\"cuda\")","f94c5746":"train_dataset.features.keys()","8e787dae":"from torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=2)","e4886d6b":"batch = next(iter(train_dataloader))\nfor k,v in batch.items():\n  print(k, v.shape)","dc800b69":"from transformers import LayoutLMv2ForTokenClassification, AdamW\nimport torch\n#model = LayoutLMv2ForTokenClassification.from_pretrained('microsoft\/layoutlmv2-base-uncased',num_labels=len(labels))\nmodel = LayoutLMv2ForTokenClassification.from_pretrained('microsoft\/layoutxlm-base',num_labels=len(labels))\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #\nmodel.to(device)\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\nglobal_step = 0\nnum_train_epochs = 40\n\nt_total = len(train_dataloader) * num_train_epochs # total number of training steps \n\n#put the model in training mode\nmodel.train() \nfor epoch in range(num_train_epochs):  \n   print(\"Epoch:\", epoch)\n   for batch in tqdm(train_dataloader):\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(**batch) \n        loss = outputs.loss\n        \n        # print loss every 100 steps\n        if global_step % 180 == 0:\n          print(f\"Loss after {global_step} steps: {loss.item()}\")\n\n        loss.backward()\n        optimizer.step()\n        global_step += 1","24bbb679":"from datasets import load_metric\n\nmetric = load_metric(\"seqeval\")\n\n# put model in evaluation mode\nmodel.eval()\nfor batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n    with torch.no_grad():\n        input_ids = batch['input_ids'].to(device)\n        bbox = batch['bbox'].to(device)\n        image = batch['image'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        labels = batch['labels'].to(device)\n\n        # forward pass\n        outputs = model(input_ids=input_ids, bbox=bbox, image=image, attention_mask=attention_mask, \n                        token_type_ids=token_type_ids, labels=labels)\n        \n        # predictions\n        predictions = outputs.logits.argmax(dim=2)\n\n        # Remove ignored index (special tokens)\n        true_predictions = [\n            [id2label[p.item()] for (p, l) in zip(prediction, label) if l != -100]\n            for prediction, label in zip(predictions, labels)\n        ]\n        true_labels = [\n            [id2label[l.item()] for (p, l) in zip(prediction, label) if l != -100]\n            for prediction, label in zip(predictions, labels)\n        ]\n\n        metric.add_batch(predictions=true_predictions, references=true_labels)\n\nfinal_score = metric.compute()\nprint(final_score)","9013ba9c":"example = dataset_test_with_bboxs[11]\nfrom PIL import Image, ImageDraw, ImageFont\nimage = Image.open(example['image_path'])\nimage = image.convert(\"RGB\")\nnew_image = image.resize((800, 1000))\nnew_image","fff9d168":"encoded_inputs = processor(image, example['words'], boxes=example['bboxes'], word_labels=example['classes'],\n                           padding=\"max_length\", truncation=True, return_tensors=\"pt\")","b4b39b53":"labels = encoded_inputs.pop('labels').squeeze().tolist()\nfor k,v in encoded_inputs.items():\n  encoded_inputs[k] = v.to(device)","a0edba7e":"encoded_inputs","458805e5":"# forward pass\noutputs = model(**encoded_inputs)\n#outputs.logits.shape","cdff9425":"def unnormalize_box(bbox, width, height):\n     return [\n         width * (bbox[0] \/ 1000),\n         height * (bbox[1] \/ 1000),\n         width * (bbox[2] \/ 1000),\n         height * (bbox[3] \/ 1000),\n     ]\n\npredictions = outputs.logits.argmax(-1).squeeze().tolist()\ntoken_boxes = encoded_inputs.bbox.squeeze().tolist()\n\nwidth, height = image.size\n\ntrue_predictions = [id2label[prediction] for prediction, label in zip(predictions, labels) if label != -100]\ntrue_labels = [id2label[label] for prediction, label in zip(predictions, labels) if label != -100]\ntrue_boxes = [unnormalize_box(box, width, height) for box, label in zip(token_boxes, labels) if label != -100]","760ec8b2":"print(true_predictions)","b7c06344":"print(true_labels)","22626265":"from PIL import ImageDraw\n\ndraw = ImageDraw.Draw(image)\n\nfont = ImageFont.load_default()\n\ndef iob_to_label(label):\n    label = label#[2:]\n    if not label:\n      return 'other'\n    return label\n\n\nfor prediction, box in zip(true_predictions, true_boxes):\n    predicted_label = iob_to_label(prediction).lower()\n    draw.rectangle(box, outline=label2color[predicted_label])\n    draw.text((box[0] + 10, box[1] - 10), text=predicted_label, fill=label2color[predicted_label], font=font)\n\nimage","031b5e0c":"image = Image.open(example['image_path'])\nimage = image.convert(\"RGB\")\n\ndraw = ImageDraw.Draw(image)\n\nfor word, box, label in zip(example['words'], example['bboxes'], example['classes']):\n  actual_label = iob_to_label(id2label[label]).lower()\n  box = unnormalize_box(box, width, height)\n  draw.rectangle(box, outline=label2color[actual_label], width=2)\n  draw.text((box[0] + 10, box[1] - 10), actual_label, fill=label2color[actual_label], font=font)\n\nnew_image = image.resize((1000, 1000))\nimage","965e60f4":"https:\/\/github.com\/NielsRogge\/Transformers-Tutorials\/blob\/master\/LayoutLMv2\/FUNSD\/Fine_tuning_LayoutLMv2ForTokenClassification_on_FUNSD.ipynb","19279107":"# **2-Newspaper Segmentation (Training)**","774326e9":"* Transformers (for the LayoutLMv2 model)\n* Detectron2 (which LayoutLMv2 requires for its visual backbone)\n* Seqeval (for metrics)\n* Datasets (our dataset)\n","7d39e81d":"# Scaling","81197799":"# Inference\nLet's test the trained model on the first image of the test set:","11b4ba28":"# Train the model\nHere we train the model in native PyTorch. We use the AdamW optimizer.","f1bbab79":"pip install googletrans==3.1.0a0\nfrom googletrans import Translator\ndetector = Translator()\nprint(detector.detect('\u0628\u0627\u0628\u0627').lang)","b3a742f0":"# Ground truth \nCompare this to the ground truth:","1878e372":"# Split Dataset","dc70a9f6":"# Install dependencies","2b73bb53":"# Predictions","aa12f68e":"# Evaluation\nNext, let's evaluate the model on the test set.","d2efc5bf":"# Visualizing Dataset"}}