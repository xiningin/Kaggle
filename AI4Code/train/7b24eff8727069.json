{"cell_type":{"27e4d277":"code","ecb31279":"code","50eb918b":"code","9e64469a":"code","8ad6d2bb":"code","86e6a7dd":"code","9ad0c13d":"code","bc2bffa2":"code","92e784e9":"code","0c0eb203":"code","45aa21b0":"code","6bf43844":"code","a27c299a":"code","0f167a2e":"code","f18a751b":"code","173ffb02":"code","5bca0be7":"code","162760bf":"code","a7a98789":"code","0d412a40":"code","3b9187b6":"code","b229ac39":"code","25d95f16":"code","69c9fc80":"code","eb809e00":"code","79de3870":"code","322ca257":"code","679fb3e9":"code","663b9282":"code","67b2919d":"code","e7f04bfc":"code","424031f4":"code","36a476a3":"code","c07698cb":"code","45127516":"code","18ee43dc":"code","28652464":"code","fafead8c":"code","0b728041":"code","4193a2ec":"markdown","aea950e5":"markdown","2f435c13":"markdown","f69e233c":"markdown","2f397f0b":"markdown","0b8e9921":"markdown","9c086b8a":"markdown","28dddf44":"markdown","4bb738d5":"markdown","4c8d6279":"markdown","0a8d231b":"markdown","d0c25b66":"markdown","c69179f9":"markdown","8ab2ec64":"markdown","c649a5bb":"markdown","28bb649d":"markdown","659e5064":"markdown","aa06990a":"markdown","a6b140f3":"markdown","f2de9455":"markdown","b1429286":"markdown","8355e21a":"markdown","904a58de":"markdown","fb1a091b":"markdown","243a5f7f":"markdown","002b0f9e":"markdown","2dd53f88":"markdown","25ea9166":"markdown","8dacf342":"markdown","f85fd74a":"markdown","8ad8f2a3":"markdown","c2b34647":"markdown","16fbf9ee":"markdown"},"source":{"27e4d277":"#Import libraries - already imported in Project 1, just to avoid any errors\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","ecb31279":"df_1 = pd.read_csv(\"..\/input\/banking-dataset\/Part2 - Data1.csv\")\ndf_2= pd.read_csv(\"..\/input\/banking-dataset\/Part2 -Data2.csv\")","50eb918b":"df_1.head()","9e64469a":"df_2.head()","8ad6d2bb":"df_1.shape","86e6a7dd":"df_2.shape","9ad0c13d":"df1=df_1.merge(df_2, on='ID')","bc2bffa2":"df1.shape","92e784e9":"df1.head()","0c0eb203":"df1.info()","45aa21b0":"df1.isna().sum()","6bf43844":"col=['HiddenScore','Level','Security','FixedDepositAccount', 'InternetBanking', 'CreditCard', 'LoanOnCard']\nfor i in col:\n    df1[i]=df1[i].astype('category')","a27c299a":"df1.dtypes","0f167a2e":"df1.info()","f18a751b":"df1.isna().sum()","173ffb02":"df1.dropna(axis=0,inplace=True)","5bca0be7":"df1.isnull().sum()","162760bf":"df1.describe().T.style.bar(['mean','std'])","a7a98789":"num_cols = df1.select_dtypes(exclude='category').columns.to_list()\nfor i in num_cols:\n    fig,ax=plt.subplots(nrows=1, ncols=2, figsize=(15,3))\n    plt.suptitle(\"Histogram & Barplot for {} feature\".format(i), ha='center')\n    sns.histplot(data=df1,x=i,ax=ax[0],fill=True, kde=True)\n    sns.boxplot(data=df1,x=i,ax=ax[1])","0d412a40":"df1.drop(['ID'], axis=1, inplace=True)","3b9187b6":"cat=df1.select_dtypes(include='category').columns.to_list()\nfig, ax= plt.subplots(nrows=3, ncols=3, figsize=(16,8),)\nax=ax.flatten()\nj=0\nfor i in cat:\n    ax[j].set_title(\"Count plot on '{}' feature\".format(i))\n    sns.countplot(data=df1, x=i, ax=ax[j])\n    j=j+1\n    ","b229ac39":"# Lets take LoanOnCard as the Target variable\nnum_cols = df1.select_dtypes(exclude='category').columns.to_list()\nfor i in num_cols:\n    fig,ax=plt.subplots(nrows=1, ncols=2, figsize=(15,3))\n    plt.suptitle(\"Histogram & Barplot for {} feature\".format(i), ha='center' )\n    sns.histplot(data=df1,x=i,ax=ax[0],fill=True, kde=True, hue='LoanOnCard')\n    sns.boxplot(data=df1,x=i,ax=ax[1], y='LoanOnCard')","25d95f16":"# Target features vs Categorical features\ncat=df1.select_dtypes(include='category').columns.to_list()\ncat.remove('LoanOnCard')\nfig, ax= plt.subplots(nrows=2, ncols=3, figsize=(16,6))\nax=ax.flatten()\nj=0\nfor i in cat:\n    ax[j].set_title(\"Count plot on '{}' feature\".format(i))\n    sns.countplot(data=df1, x=i, ax=ax[j], hue='LoanOnCard')\n    j=j+1","69c9fc80":"df1.groupby(by=['LoanOnCard']).mean()","eb809e00":"from scipy.stats import ttest_ind\nfor i in num_cols:\n    sts, p_value = ttest_ind(df1[df1['LoanOnCard']==0][i],df1[df1['LoanOnCard']==1][i])\n    if p_value<0.05:\n        print(\"P_value: {:.2f} , Reject Null Hypothesis: Feature {} has impact on LoanOnCard\".format(p_value,i))\n    else:\n        print(\"P_value: {:.2f} , Accept Null Hypothesis: Feature {} has no impact on LoanOnCard\".format(p_value,i))\n    ","79de3870":"#Let us check the same for Categorical variable\nfrom scipy.stats import chi2_contingency\nfor i in cat:\n    crosstab = pd.crosstab(df1['LoanOnCard'],df1[i])\n    sts, p_value,_,_ = chi2_contingency(crosstab)\n    if p_value<0.05:\n        print(\"P_value: {:.2f} , Reject Null Hypothesis: Feature {} has impact on LoanOnCard Classes\".format(p_value,i))\n    else:\n        print(\"P_value: {:.2f} , Accept Null Hypothesis: Feature {} has no impact on LoanOnCard Classes\".format(p_value,i))","322ca257":"sns.pairplot(df1, hue='LoanOnCard' )","679fb3e9":"sns.heatmap(df1.corr(), annot=True)","663b9282":"df1['LoanOnCard'].value_counts()","67b2919d":"sample_data=pd.concat([df1[df1['LoanOnCard']==0].sample(480),df1[df1['LoanOnCard']==1]])\ndf1_sample = sample_data.drop(['Security','InternetBanking','CreditCard','Age','CustomerSince','ZipCode'], axis=1)\nX=df1_sample.drop('LoanOnCard', axis=1)\ny=df1_sample['LoanOnCard']\n\n\n#under_sample_data.shape","e7f04bfc":"X.shape","424031f4":"y.shape","36a476a3":"y.value_counts()","c07698cb":"from sklearn.model_selection import train_test_split\n# Split X and y into training and test set in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=10)","45127516":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, log_loss\nreg = LogisticRegression()\n\n#Try with dataset before downsampling\ndf1_no_sample = df1.drop(['Security','InternetBanking','CreditCard','Age','CustomerSince','ZipCode'], axis=1)\nX1=df1_no_sample.drop('LoanOnCard', axis=1)\ny1=df1_no_sample['LoanOnCard']\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.30, random_state=10)\n\nreg.fit(X_train1, y_train1)\npred=reg.predict(X_test1)\n\nprint(\"Accuracy score Traing dataset:{:.2f}\".format(reg.score(X_train1, y_train1)))\nprint(\"Accuracy score Traing dataset:{:.2f}\".format(reg.score(X_test1, y_test1)))\nprint(\"Accuracy score:{:.2f}\".format(accuracy_score(y_test1, pred)))\nprint(\"Log loss:{:.2f}\".format(log_loss(y_test1, pred)))\nsns.heatmap(confusion_matrix(y_test1, pred), annot=True, fmt='.2f')\nprint(classification_report(y_test1,pred))","18ee43dc":"#Try with down sampled data\n\nreg.fit(X_train, y_train)\npred=reg.predict(X_test)\n\nprint(\"Accuracy score:{:.2f} %\".format(accuracy_score(y_test, pred)*100))\nprint(\"Log loss:{:.2f}\".format(log_loss(y_test, pred)))\nsns.heatmap(confusion_matrix(y_test, pred), annot=True, fmt='.2f')\nprint(classification_report(y_test,pred))","28652464":"from sklearn.naive_bayes import GaussianNB, BernoulliNB,CategoricalNB,ComplementNB\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\npred=model.predict(X_test)\nprint(\"Accuracy score Traing dataset:{:.2f}%\".format(model.score(X_train, y_train)))\nprint(\"Accuracy score Traing dataset:{:.2f}%\".format(model.score(X_test, y_test)))\nprint(\"Accuracy score: test vs Predicion: {:.2f}%\".format(accuracy_score(y_test, pred)))\n\nsns.heatmap(confusion_matrix(y_test, pred), annot=True, fmt='.2f')\nprint(classification_report(y_test,pred))","fafead8c":"model = BernoulliNB()\nmodel.fit(X_train, y_train)\npred=model.predict(X_test)\nprint(\"Accuracy score Traing dataset:{:.2f}%\".format(model.score(X_train, y_train)*100))\nprint(\"Accuracy score Traing dataset:{:.2f}%\".format(model.score(X_test, y_test)*100))\nprint(\"Accuracy score: test vs Predicion: {:.2f}%\".format(accuracy_score(y_test, pred)*100))\n\n\nsns.heatmap(confusion_matrix(y_test, pred), annot=True, fmt='.2f')\nprint(classification_report(y_test,pred))","0b728041":"model = ComplementNB()\nmodel.fit(X_train, y_train)\npred=model.predict(X_test)\nprint(\"Accuracy score Traing dataset:{:.2f}%\".format(model.score(X_train, y_train)))\nprint(\"Accuracy score Traing dataset:{:.2f}%\".format(model.score(X_test, y_test)))\nprint(\"Accuracy score: test vs Predicion: {:.2f}%\".format(accuracy_score(y_test, pred)))\n\nsns.heatmap(confusion_matrix(y_test, pred), annot=True, fmt='.2f')\nprint(classification_report(y_test,pred))","4193a2ec":"### Explore for null values in the attributes and if required drop or impute values.","aea950e5":"### Multivariated Analysis","2f435c13":"**Conclusion**\n1. As a Datascientist, i would prefer to choose Logistic regression (balanced data) or GaussianNB as model.\n2. Overfitting is less is Logistic regress. \n3. precision & recall values are better predicting the protential customers.\n4. Banking domain prefer to see the pression than the recall as to avoid false negative. \n","f69e233c":"### Bivariated analysis","2f397f0b":"***Data frame consiste of either integer or float datatype***","0b8e9921":"#### Merge dataframe","9c086b8a":"### Observations:\n1. Feature Security, InternetBanking, CreditCard has no impact on LoanOncard classes with 5% significant value. So, we can consider remvoing those feature in model building.","28dddf44":"***There is imbalance in Target variable with almost 90% data in 0 and 10% data in 1. Let us try to undersample it using sample option in Padas***","4bb738d5":"### Observations:\n1. Mortgage - values are mostly spreaded around 0 to 100, and outliers are preset above 280+ approximately.\n2. MonthlyAverageSpend - values are distributed around 0 to 10, and most values are between 1 & 3. with right skewn. outliers are available around 6 values.\n3. ZipCode - i believe, we should consider this are contineous rather than the discreate values.\n4. HighestSpend - values are between 0 and 200. with right skew.\n5. CustomerSince - Normally distributed with mean as 20. most customer are between 10 and 30.\n6. Age - is a contineous variables, and most customer are between the age 35 to 55. \n7. ID - we can drop the feature as it is the ID of the customer which doesnt give much info.","4c8d6279":"# 5. Model training, testing and tuning: \n### Logistic regression","0a8d231b":"1. There is no difference on Age in terms of LoanOnCard. both mean values are near 45.\n2. CustoemrSince feature is also more or less similar with just 1 value difference.\nHowever, we will do T-Test to check if it is significantly same\n  \n**Hypothesis:**\n***H0 - Both independent Sample are same. \nH1 - Both independent samples are not same.***\n\nSignificant level - 5% (0.05)","d0c25b66":"### Univariated analysis","c69179f9":"### Naive Bayes Classification","8ab2ec64":"### Observations:\n1. Hidden score - is distribured between 1, 2, 3 & 4.\n2. Level - is distributed between 1,2 & 3\n3. Security - value is 0 or 1. most cases it is 0\n4. FixedDeposit - 0 or 1 for yes or no, mostly it is 0\n5. InternetBanking - 0 or 1 for yes or no, mostly customer has Internetbanking.\n6. CreditCard - 0 or 1 for yes or no. most customers doesn't have Creditcard\n7. LoanOnCard - 0 or 1 for yes or no. most customer doesn't have loan on card.","c649a5bb":"## 1. Import and warehouse data: \n### Import all the given datasets and explore shape and size of each","28bb649d":"***Zipcode has higher mean and std, however we can't consider Zipcode as descrite variable. same is applicable for ID. other features can have skewness, lets explore further***","659e5064":"**Observations:**\n1. Though the accuracy score is 95%, we can't consider this as the good model as most of data it predict as 0 (No Loan on Card). this because of data imbalance.\n2. If we see the classification report.  \n    a. Precission : we are predicting 96% of 0 out of over all No Loan on card & 84% of times as Loan on card out of over all dataset.  \n    b. Recall : out of over all dataset, our model can predict 99% of times as No loan on card & 64% of times as Loan on card from test dataset.  \n    For Banks, it is important to hold the potential customers. so our model should predict the less false positive to find our Loan on Card. so we should consider Precission.\n\n","aa06990a":"## 2.  Data cleansing: \n### Explore and if required correct the datatypes of each attribute ","a6b140f3":"### Read Data","f2de9455":"### Observation:\n1. Age - is assubmed to be evenly splitted across LoanOnCard. There is no much difference otherthan min and max value.\n2. CustomerSince - same as Age, there is no much difference in old or new customer in terms of LoanOnCard\n3. HighestSpend - customer having LoanOnCard have higher highest spend, where as Lower spend for the customer have no LoanOnCard. there are outliers in the Highestspend interms of no LoanOnCard. \n4. ZipCode - doesn't make much difference, otherthan the outlier value 0. which needs to be handled.\n5. MonthlyAverageSpend -Customers having LoanOnCard has high MonthlyAverageSpend compared to no loan customers.\n6. Mortgage - Mortgage value is merely starting at 0, however the customer having loan on card has high numbers.","b1429286":"### Observation:\n1. Feature Age, CustomerSince & ZipCode has not much impact on TargetVariable LoanOnCard. we can consider removing the feature in modeling.","8355e21a":"***Data1 & Date 2 has different set of columns, we need to combaine both data columns with ID features***","904a58de":"***There are 20 null values in LoanOnCard feature. LoanOnCard teslls us if the customer has loan in credit card. let check the values to either remove those 20 rows or replance nan with anyother info***","fb1a091b":"## 4. Data pre-processing: \n\n### Segregate predictors vs target attributes","243a5f7f":"***Dataframe row counts remains the same and the number of features increased to 14 on merging two dataframes***","002b0f9e":"***imbalance in the dataset has been handled with downsampling thing data. there are other techiniques like SMOTE oversampling & downsampling techinques ect. i have planned to go with manual sampling based the course***","2dd53f88":"# <center> Predict Potential Customer <\/center>\n\n**Domain**: Banking\nA bank X is on a massive digital transformation for all its departments. Bank has a growing customer base whee majority of them are liability customers (depositors) vs borrowers (asset customers).  The bank is interested in expanding the borrowers base rapidly to bring in more business via loan interests. A campaign that the bank ran in last quarter showed an average single digit conversion rate. Digital transformation being the core strength of the business strategy, marketing department wants to devise effective campaigns with better target marketing to increase the conversion ratio to double digit with same budget as per last campaign. \n\n**Objective:**\nBuild an AIML model to perform focused marketing by predicting the potential customers who will convert using the historical dataset. ","25ea9166":"### Observations:\n1. HiddenScore - feature has similar count for all classes for LoanOnCard.\n2. Level - Leval count is similarly distributed for both classes. \n3. CreditCard - Count of people having credit card is less, in which LoanOnCard distribution is also low. \n4. FixedDepositAccount - People having FixedDeposit, have equal disctibution on LoanOnCard. however, people have no Fixed deposits have less numbers in LoanOnCard.\n5. InternetBanking - Customers having Internet banking have LoanOnCard is slightly higher than no InternetBanking.\n6. CreditCard - Customers having no credit cars also opted for LoanOncard. which is high number compared to customers having credit card. however, proportion may defer.","8dacf342":"**Observations:**\n1. Though the accuracy score is reduced to 88%, we can say that the model is peforming well in balanced data.\n2. If we see the classification report.  \n    a. Precission : we are predicting 99% of 1 out of over all Loan on card & 89% of times as Loan on card out of over all dataset.  \n    b. Recall : out of over all dataset, our model can predict 89% of times correctly as No loan on card & 88% of times as Loan on card correctly.  \n    Comparing with the imbalanced dataset, the balanced dataset perform better with high precission value. However, there are room for improvements to reduce the False Negative. \n    Lets try with different modeling technique","f85fd74a":"## 3. Data analysis & visualisation: \n####  Perform detailed statistical analysis on the data","8ad8f2a3":"***Missing values have been dropped and formated the dataset with right datatypes***","c2b34647":"### Perform train-test split. ","16fbf9ee":"## Hypothesis Testing\n"}}