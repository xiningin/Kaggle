{"cell_type":{"e59b368b":"code","8de2e1b5":"code","c470bd90":"code","23e2f75a":"code","af06a0dd":"code","9192834c":"code","d6b18189":"code","12523dab":"code","f9803be1":"code","696d5d44":"code","7179c4bb":"code","e7449df3":"code","753531c8":"code","0c778392":"code","83f663d3":"code","aa53a19d":"code","2edb8a2d":"code","1fbb5fde":"code","39b07b7f":"code","9ecc3b25":"code","d4de22e5":"code","9a8bb32d":"code","0b809e31":"code","e61a2430":"code","19885fe3":"code","17a1e1ca":"code","a024b5df":"code","4dd4828b":"code","dbd7ceca":"code","335b6f04":"code","06478caa":"code","f1ae0364":"code","efe2bbe4":"code","900968d2":"markdown","62cae3bb":"markdown","35cbe650":"markdown","9e6a5376":"markdown","e8c2c0d8":"markdown","7ce2f443":"markdown","543d5ed2":"markdown","0c18c604":"markdown","7f6a59bb":"markdown","e3162974":"markdown","e3875eb6":"markdown","abac0b6b":"markdown","6f4a722c":"markdown","1f02ed06":"markdown","b38d49ef":"markdown"},"source":{"e59b368b":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","8de2e1b5":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c470bd90":"import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport gc, datetime, random\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import roc_auc_score\n\npd.options.display.max_rows = 4000\n\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nSEED = 42\nseed_everything(SEED)\nSTART_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')","23e2f75a":"%%time\ntrain_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/sample_submission.csv', index_col='TransactionID')\n\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(\"Train shape: \", train.shape)\nprint(\"Test shape: \", test.shape)\n\ny = train['isFraud'].copy()\ndel train_transaction, train_identity, test_transaction, test_identity\n\n# Drop target, fill in NaNs\ntrain = train.drop('isFraud', axis=1)","af06a0dd":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","9192834c":"def addNewFeatures(data): \n    data['uid'] = data['card1'].astype(str)+'_'+data['card2'].astype(str)\n\n    data['uid2'] = data['uid'].astype(str)+'_'+data['card3'].astype(str)+'_'+data['card5'].astype(str)\n\n    data['uid3'] = data['uid2'].astype(str)+'_'+data['addr1'].astype(str)+'_'+data['addr2'].astype(str)\n\n    data['D9'] = np.where(data['D9'].isna(),0,1)\n    \n    return data\n\ntrain = addNewFeatures(train)\ntest = addNewFeatures(test)","d6b18189":"i_cols = ['card1','card2','card3','card5','uid','uid2','uid3']\n\nfor col in i_cols:\n    for agg_type in ['mean','std']:\n        new_col_name = col+'_TransactionAmt_'+agg_type\n        temp_df = pd.concat([train[[col, 'TransactionAmt']], test[[col,'TransactionAmt']]])\n        #temp_df['TransactionAmt'] = temp_df['TransactionAmt'].astype(int)\n        temp_df = temp_df.groupby([col])['TransactionAmt'].agg([agg_type]).reset_index().rename(\n                                                columns={agg_type: new_col_name})\n\n        temp_df.index = list(temp_df[col])\n        temp_df = temp_df[new_col_name].to_dict()   \n\n        train[new_col_name] = train[col].map(temp_df)\n        test[new_col_name]  = test[col].map(temp_df)\n\ntrain = train.replace(np.inf,999)\ntest = test.replace(np.inf,999)","12523dab":"train['TransactionAmt'] = np.log1p(train['TransactionAmt'])\ntest['TransactionAmt'] = np.log1p(test['TransactionAmt'])","f9803be1":"emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other',\n          'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft',\n          'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', \n          'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other',\n          'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo',\n          'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo',\n          'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo',\n          'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo',\n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other',\n          'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple',\n          'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other',\n          'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\nus_emails = ['gmail', 'net', 'edu']\n\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    train[c + '_bin'] = train[c].map(emails)\n    test[c + '_bin'] = test[c].map(emails)\n    \n    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n    \n    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","696d5d44":"p = 'P_emaildomain'\nr = 'R_emaildomain'\nuknown = 'email_not_provided'\n\ndef setDomain(df):\n    df[p] = df[p].fillna(uknown)\n    df[r] = df[r].fillna(uknown)\n    \n    # Check if P_emaildomain matches R_emaildomain\n    df['email_check'] = np.where((df[p]==df[r])&(df[p]!=uknown),1,0)\n\n    df[p+'_prefix'] = df[p].apply(lambda x: x.split('.')[0])\n    df[r+'_prefix'] = df[r].apply(lambda x: x.split('.')[0])\n    \n    return df\n    \ntrain=setDomain(train)\ntest=setDomain(test)","7179c4bb":"def setTime(df):\n    df['TransactionDT'] = df['TransactionDT'].fillna(df['TransactionDT'].median())\n    # Temporary\n    df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n    df['DT_M'] = (df['DT'].dt.year-2017)*12 + df['DT'].dt.month\n    df['DT_W'] = (df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear\n    df['DT_D'] = (df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear\n    \n    df['DT_hour'] = df['DT'].dt.hour\n    df['DT_day_week'] = df['DT'].dt.dayofweek\n    df['DT_day'] = df['DT'].dt.day\n    \n    return df\n    \ntrain=setTime(train)\ntest=setTime(test)","e7449df3":"train[\"lastest_browser\"] = np.zeros(train.shape[0])\ntest[\"lastest_browser\"] = np.zeros(test.shape[0])\n\ndef setBrowser(df):\n    df.loc[df[\"id_31\"]==\"samsung browser 7.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"opera 53.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"mobile safari 10.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"google search application 49.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"firefox 60.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"edge 17.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 69.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 67.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for ios\",'lastest_browser']=1\n    return df\n\ntrain=setBrowser(train)\ntest=setBrowser(test)","753531c8":"def setDevice(df):\n    df['DeviceInfo'] = df['DeviceInfo'].fillna('unknown_device').str.lower()\n    \n    df['device_name'] = df['DeviceInfo'].str.split('\/', expand=True)[0]\n\n    df.loc[df['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n    df.loc[df['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n    df.loc[df['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n    df.loc[df['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n    df.loc[df['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n    df.loc[df['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n    df.loc[df['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n    df.loc[df['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n\n    #df.loc[df.device_name.isin(df.device_name.value_counts()[df.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n    df['had_id'] = 1\n    gc.collect()\n    \n    return df\n\ntrain=setDevice(train)\ntest=setDevice(test)","0c778392":"i_cols = ['card1','card2','card3','card5',\n          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n          'D1','D2','D3','D4','D5','D6','D7','D8',\n          'addr1','addr2',\n          'dist1','dist2',\n          'P_emaildomain', 'R_emaildomain',\n          'DeviceInfo','device_name',\n          'id_30','id_33',\n          'uid','uid2','uid3',\n         ]\n\nfor col in i_cols:\n    temp_df = pd.concat([train[[col]], test[[col]]])\n    fq_encode = temp_df[col].value_counts(dropna=False).to_dict()   \n    train[col+'_fq_enc'] = train[col].map(fq_encode)\n    test[col+'_fq_enc']  = test[col].map(fq_encode)\n\n\nfor col in ['DT_M','DT_W','DT_D']:\n    temp_df = pd.concat([train[[col]], test[[col]]])\n    fq_encode = temp_df[col].value_counts().to_dict()\n            \n    train[col+'_total'] = train[col].map(fq_encode)\n    test[col+'_total']  = test[col].map(fq_encode)\n        \n\nperiods = ['DT_M','DT_W','DT_D']\ni_cols = ['uid']\nfor period in periods:\n    for col in i_cols:\n        new_column = col + '_' + period\n            \n        temp_df = pd.concat([train[[col,period]], test[[col,period]]])\n        temp_df[new_column] = temp_df[col].astype(str) + '_' + (temp_df[period]).astype(str)\n        fq_encode = temp_df[new_column].value_counts().to_dict()\n            \n        train[new_column] = (train[col].astype(str) + '_' + train[period].astype(str)).map(fq_encode)\n        test[new_column]  = (test[col].astype(str) + '_' + test[period].astype(str)).map(fq_encode)\n        \n        train[new_column] \/= train[period+'_total']\n        test[new_column]  \/= test[period+'_total']","83f663d3":"def get_too_many_null_attr(data):\n    many_null_cols = [col for col in data.columns if data[col].isnull().sum() \/ data.shape[0] > 0.9]\n    return many_null_cols\n\ndef get_too_many_repeated_val(data):\n    big_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n    return big_top_value_cols\n\ndef get_useless_columns(data):\n    too_many_null = get_too_many_null_attr(data)\n    print(\"More than 90% null: \" + str(len(too_many_null)))\n    too_many_repeated = get_too_many_repeated_val(data)\n    print(\"More than 90% repeated value: \" + str(len(too_many_repeated)))\n    cols_to_drop = list(set(too_many_null + too_many_repeated))\n    #cols_to_drop.remove('isFraud')\n    return cols_to_drop","aa53a19d":"cols_to_drop = get_useless_columns(train)","2edb8a2d":"train = train.drop(cols_to_drop, axis=1)\ntest = test.drop(cols_to_drop, axis=1)","1fbb5fde":"print(train.shape)\nprint(test.shape)\nprint(y.shape)","39b07b7f":"numerical_cols = train.select_dtypes(exclude = 'object').columns\ncategorical_cols = train.select_dtypes(include = 'object').columns","9ecc3b25":"categorical_cols[:5]","d4de22e5":"# Label Encoding\nfor f in train.columns:\n    if train[f].dtype.name =='object' or test[f].dtype.name =='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train[f].values) + list(test[f].values))\n        train[f] = lbl.transform(list(train[f].values))\n        test[f] = lbl.transform(list(test[f].values))","9a8bb32d":"train = train.fillna(-999)\ntest = test.fillna(-999)","0b809e31":"print(train.isnull().sum().max())\nprint(test.isnull().sum().max())","e61a2430":"train.head()","19885fe3":"X = train.drop(['TransactionDT', 'DT'], axis=1)\nX_test = test.drop(['TransactionDT', 'DT'], axis=1)","17a1e1ca":"print(\"X:\", X.shape)\nprint(\"y_train:\", y.shape)\nprint(\"X_test:\", X_test.shape)","a024b5df":"params = {'num_leaves': 546,\n          'min_child_weight': 0.03454472573214212,\n          'feature_fraction': 0.1797454081646243,\n          'bagging_fraction': 0.2181193142567742,\n          'min_data_in_leaf': 106,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.005883242363721497,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3299927210061127,\n          'reg_lambda': 0.3885237330340494,\n          'random_state': 42,\n}","4dd4828b":"%%time\n\nNFOLDS = 5\nfolds = KFold(n_splits=NFOLDS)\n\ncolumns = X.columns\nsplits = folds.split(X, y)\ny_preds = np.zeros(X_test.shape[0])\ny_oof = np.zeros(X.shape[0])\nscore = 0\n\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = columns\n  \nfor fold_n, (train_index, valid_index) in enumerate(splits):\n    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    dtrain = lgb.Dataset(X_train, label=y_train)\n    dvalid = lgb.Dataset(X_valid, label=y_valid)\n\n    clf = lgb.train(params, dtrain, 10000, valid_sets = [dtrain, dvalid], verbose_eval=200, early_stopping_rounds=500)\n    \n    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n    \n    y_pred_valid = clf.predict(X_valid)\n    y_oof[valid_index] = y_pred_valid\n    print(f\"Fold {fold_n + 1} | AUC: {roc_auc_score(y_valid, y_pred_valid)}\")\n    \n    score += roc_auc_score(y_valid, y_pred_valid) \/ NFOLDS\n    y_preds += clf.predict(X_test) \/ NFOLDS\n    \n    del X_train, X_valid, y_train, y_valid\n    gc.collect()\n    \nprint(f\"\\nMean AUC = {score}\")\nprint(f\"Out of folds AUC = {roc_auc_score(y, y_oof)}\")","dbd7ceca":"submission = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/sample_submission.csv')","335b6f04":"submission['isFraud'] = y_preds","06478caa":"submission.head()","f1ae0364":"submission.to_csv('submission.csv', index=False)","efe2bbe4":"feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(folds.n_splits)]].mean(axis=1)\nfeature_importances.to_csv('feature_importances.csv')\n\nplt.figure(figsize=(16, 16))\nsns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\nplt.title('50 TOP feature importance over {} folds average'.format(folds.n_splits));","900968d2":"### <span id=\"8\"><\/span> ** Handle Device Type **","62cae3bb":"### <span id=\"4\"><\/span> ** Handle Email Domains **","35cbe650":"### <span id=\"5\"><\/span> ** Handle P Email Domain and R Email Domain **","9e6a5376":"## <span id=\"11\"><\/span> ** 4. Model **","e8c2c0d8":"### <span id=\"6\"><\/span> ** Set Time **","7ce2f443":"### <span id=\"9\"><\/span> ** Set Frequency **","543d5ed2":"## <span id=\"13\"><\/span> ** 5. Submission **","0c18c604":"### <span id=\"3\"><\/span> ** Add New Features **","7f6a59bb":"<hr\/>\n[**Tolgahan Cepel**](https:\/\/www.kaggle.com\/tolgahancepel)\n<hr\/>\n<font color=green>\n* 1. [Importing Libraries and Reading the Dataset](#1)\n* 2. [Feature Engineering](#2)\n    * [Add New Features](#3)\n    * [Handle Email Domains](#4) \n    * [Handle P Email Domain and R Email Domain](#5) \n    * [Set Time](#6) \n    * [Handle Browser Version](#7) \n    * [Handle Device Type](#8)\n    * [Set Frequency](#9)\n* 3. [Data Preprocessing](#10) \n* 4. [Models](#11)\n    * [LightGBM](#12) \n* 5. [Submission](#13)\n<hr\/>","e3162974":"## <span id=\"2\"><\/span> ** 2. Feature Engineering **","e3875eb6":"<b><font color=\"red\">Don't forget to <\/font><\/b> <b><font color=\"green\">UPVOTE <\/font><\/b> if you liked this kernel, thank you. \ud83d\ude42\ud83d\udc4d","abac0b6b":"## <span id=\"1\"><\/span> ** 1. Importing Libraries and Reading the Dataset **","6f4a722c":"### <span id=\"12\"><\/span> ** LightGBM **","1f02ed06":"### <span id=\"7\"><\/span> ** Handle Browser Version **","b38d49ef":"## <span id=\"10\"><\/span> ** 3. Data Preprocessing **"}}