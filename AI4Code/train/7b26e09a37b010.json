{"cell_type":{"8d5746a0":"code","f64c472c":"code","7c345fff":"code","57fddc16":"code","7396ff95":"code","e5714d5d":"code","5fff25bd":"code","372616cd":"code","0ced3002":"code","6be03b3b":"code","801f434d":"code","7deb43ba":"code","a35633da":"code","8df7041b":"code","bed10810":"code","a0da91b9":"code","07c737a0":"code","3443fda7":"code","99da5109":"code","793b59f7":"code","0dec9d87":"code","b676e0b0":"code","a248b01b":"code","3df0adfb":"code","c31b7c98":"code","9e302dfe":"code","df2a5d77":"code","80e66de4":"code","17d4b761":"code","530338f1":"code","9daaf2a7":"code","5e82b0a3":"code","94d73deb":"code","21256568":"code","4a55fd71":"code","656c4918":"code","0d4808d8":"code","8d931447":"code","7cc2b50b":"code","0e88a590":"code","6f78cf01":"code","82c2d566":"code","1586bca2":"code","6270ec8f":"markdown","f7f5482b":"markdown","65644ce0":"markdown","cd00ca18":"markdown","fa7bbc61":"markdown","9c0259c8":"markdown","490f5352":"markdown","7f7f8052":"markdown","f95d391d":"markdown","a8397d6c":"markdown","231701c5":"markdown","87b004d1":"markdown","e258b21b":"markdown","babd332d":"markdown","a62c5ee6":"markdown","16079546":"markdown"},"source":{"8d5746a0":"# F\u00fcr dieses Projekt erforderliche Bibliotheken importieren\nimport sys\n#!{sys.executable} -m pip install pandas-profiling\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom pandas_profiling import ProfileReport\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\n%matplotlib inline\n\n#Daten importieren\ntry:\n    df = pd.read_csv(\"..\/input\/wholesale-customers-data-set\/Wholesale customers data.csv\")\n    print(\"Wholesale customers dataset has {} samples with {} features each.\".format(*df.shape))\nexcept:\n    print(\"Dataset could not be loaded.\")","f64c472c":"data = df.rename(columns={'Delicassen': 'Delicatessen'}) #Rechtschreibung korrigieren\ndata.drop(['Region', 'Channel'], axis = 1, inplace = True)\ndata.head()","7c345fff":"# allgemeine Beschreibung des Datensatzes\ndisplay(data.describe())","57fddc16":"#Ein detailierter Bericht von allen Variablen\ndata.profile_report()","7396ff95":"# Channel plotten\na = df.pop('Channel')\ndata = data.join(a)\nsubjects = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicatessen']\nchannel = data.groupby('Channel')[subjects].sum()\nchannel1 = channel.T\nprint(channel1)\ncolors = ['c','coral','limegreen']\nax = channel1.plot(kind='bar', figsize = (9,7),stacked= True, title = '', color = colors)\nax.tick_params(axis='x', rotation=0)\nax.legend(['Channel 1', 'Channel 2'],loc=1, prop={'size': 16})","e5714d5d":"#Region plotten\na = df.pop('Region')\ndata = data.join(a)\nregion = data.groupby('Region')[subjects].sum()\nregion1 = region.T\nprint(region1)\nax = region1.plot(kind='bar', figsize = (8,6),stacked= True, title = '', color = colors)\nax.tick_params(axis='x', rotation=0)\nax.legend(['Region 1', 'Region 2','Region 3'],loc=1, prop={'size': 16})","5fff25bd":"data.drop(['Region', 'Channel'], axis = 1, inplace = True)\n\n# drei beliebige Beobachtungen\nindices = [43, 344, 122]\n\n# Ein DataFrame f\u00fcr die Stichproben erzeugen\nsamples = pd.DataFrame(data.loc[indices], columns = data.columns)\nprint (\"Chosen samples of wholesale customers dataset:\")\ndisplay(samples)\n\n# Durchschnitt berechnen \nmean_data = data.describe().loc['mean', :]\nsamples_bar = samples.append(mean_data)\n\n# Index kontruieren\nsamples_bar.index = indices + ['mean']\n\n# Barplot\nax=samples_bar.plot(kind='bar', figsize=(14,8))\nax.tick_params(rotation=0)","372616cd":"#Vergleichen mit durchschnittlichen Wert\n(samples - data.mean()) \/ data.std()","0ced3002":"# \u00dcberpr\u00fcfen, ob es Missings gibt\nprint(data.isnull().values.any())\n# Es gibt keine Null-Werte bzw. Missings im Datensatz","6be03b3b":"# \u00dcberpr\u00fcfen, ob es Ausrei\u00dfer gibt\n_ = sns.pairplot(data, diag_kind = 'kde')","801f434d":"plt.figure(figsize = (20,8))\n_ = sns.barplot(data=data, palette=\"Set2\")","7deb43ba":"plt.figure(figsize = (20,8))\n_ = sns.boxplot(data=data, orient='h', palette=\"Set2\")","a35633da":"# die Daten mit dem nat\u00fcrlichen Logarithmus skalieren\nlog_data = np.log(data)\n\n# die Stichprobedaten mit dem nat\u00fcrlichen Logarithmus skalieren\nlog_samples = np.log(samples)\n\n# eine Streumatrix f\u00fcr jedes Paar neu transformierter Merkmale erstellen\n_ = sns.pairplot(log_data, diag_kind = 'kde')\n\n# die Protokolltransformierten Beispieldaten anzeigen\ndisplay(log_samples)\nax=log_samples.plot(kind='bar', figsize=(8,6))\nax.tick_params(rotation=0)\nax.set_title('Samples nach dem Log-Transformation')","8df7041b":"outliers_list = []\n# F\u00fcr jedes Merkmal die Datenpunkte mit extrem hohen oder niedrigen Werten finden\nfor feature in log_data.keys():\n    \n    # Q1 (25. Perzentil der Daten) f\u00fcr das gegebene Merkmal berechnen\n    Q1 = np.percentile(log_data[feature], 25)\n    \n    # Q3 (75. Perzentil der Daten) f\u00fcr das gegebene Merkmal berechnen\n    Q3 = np.percentile(log_data[feature], 75)\n    \n    # den Interquartilabstand verwenden, um einen Ausrei\u00dferschritt zu berechnen (1,5-facher Interquartilabstand)\n    step = (Q3 - Q1) * 1.5\n    \n    # Ausrei\u00dfer anzeigen\n    print(\"Data points considered outliers for the feature '{}':\".format(feature))\n    outliers = list(log_data[~((log_data[feature] >= Q1 - step) & (log_data[feature] <= Q3 + step))].index.values)\n    display(log_data[~((log_data[feature] >= Q1 - step) & (log_data[feature] <= Q3 + step))])\n    outliers_list.extend(outliers)\n    \nprint(\"List of Outliers -> {}\".format(outliers_list))\nduplicate_outliers_list = list(set([x for x in outliers_list if outliers_list.count(x) >= 2]))\nduplicate_outliers_list.sort()\nprint(\"\\nList of Common Outliers -> {}\".format(duplicate_outliers_list))\n\n# die Indizes f\u00fcr Datenpunkte ausw\u00e4hlen, die wir entfernen m\u00f6chten\noutliers  = duplicate_outliers_list\n\n# die Ausrei\u00dfer entfernen, falls welche angegeben wurden\ngood_data = log_data.drop(log_data.index[outliers]).reset_index(drop = True)\ndisplay(good_data)","bed10810":"# \u00dcberpr\u00fcfen, ob die Stichproben Ausrei\u00dfer sind\n# => Sie sind nicht Ausrei\u00dfer. Die obere Auswertung kann so bleiben.\nprint(43 in outliers_list)\nprint(344 in outliers_list)\nprint(122 in outliers_list)","a0da91b9":"#Hauptkomponentenanalyse\nfrom sklearn.decomposition import PCA\n\n# PCA auf die guten Daten mit der gleichen Anzahl von Dimensionen wie Merkmale anwenden\npca = PCA()\npca.fit_transform(good_data)\n\n# die gleiche PCA-Transformation auf die drei Stichproben-Datenpunkte anwenden\npca_samples = pca.transform(log_samples)","07c737a0":"# Die Varianz, die durch jede Hauptkomponente erkl\u00e4rt wird\nexplained_variances = pca.explained_variance_ratio_\n\nprint(\"Proportion of the variance explained by each dimension\")\nprint(\"\\n\".join([\"{}: {:1.3f}\".format(i+1,val) for i,val in enumerate(explained_variances)]))\n\n# Alle Varianz, die durch die n-te Hauptkomponente erkl\u00e4rt wird\ncumulative_variance = [explained_variances[:i+1].sum() for i in range(len(explained_variances))]\nprint(\"\\nTotal variance explained by the first N principal compoments\")\nprint(\"\\n\".join([\"{}: {:1.3f}\".format(i+1,val) for i,val in enumerate(cumulative_variance)]))","3443fda7":"def pca_results(good_data, pca):\n\t'''\n\tCreate a DataFrame of the PCA results\n\tIncludes dimension feature weights and explained variance\n\tVisualizes the PCA results\n\t'''\n\n\t# Dimensionsindexierung\n\tdimensions = dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]\n\n\t# PCA Komponenten\n\tcomponents = pd.DataFrame(np.round(pca.components_, 4), columns = list(good_data.keys()))\n\tcomponents.index = dimensions\n\n\t# PCA explained variance\n\tratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\n\tvariance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])\n\tvariance_ratios.index = dimensions\n\n\t# eine Barplot-Visualisierung\n\tfig, ax = plt.subplots(figsize = (14,8))\n\n\t# die Merkmalsgewichte als Funktion der Komponenten plotten\n\tcomponents.plot(ax = ax, kind = 'bar');\n\tax.set_ylabel(\"Feature Weights\")\n\tax.set_xticklabels(dimensions, rotation=0)\n\n\n\t# die erkl\u00e4rten Varianzverh\u00e4ltnisse anzeigen\n\tfor i, ev in enumerate(pca.explained_variance_ratio_):\n\t\tax.text(i-0.40, ax.get_ylim()[1] + 0.05, \"Explained Variance\\n          %.4f\"%(ev))\n\n\t# Einen verketteten DataFrame zur\u00fcckgeben\n\treturn pd.concat([variance_ratios, components], axis = 1)","99da5109":"pca_results = pca_results(good_data, pca)","793b59f7":"# PCA mit nur zwei Dimensionen an die guten Daten anpassen\npca = PCA(n_components=2,random_state=0)\npca.fit(good_data)\n\n# eine PCA-Transformation der guten Daten anwenden\nreduced_data = pca.transform(good_data)\n\n# eine PCA-Transformation auf die drei Stichproben-Datenpunkte anwenden\npca_samples = pca.transform(log_samples)\n\n# einen DataFrame f\u00fcr die reduzierten Daten erstellen\nreduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'])\n#display(reduced_data)","0dec9d87":"fig, ax = plt.subplots(1, 1, figsize=(10, 6))\nfig.suptitle('Scatterplot of First Two Principal Components', fontsize=15)\nimg = ax.scatter(reduced_data[\"Dimension 1\"], reduced_data[\"Dimension 2\"],\n                 c=\"#9A0EEA\", s=100, alpha=0.4, linewidths=0)","b676e0b0":"# GMM und silhouette_score importieren\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import silhouette_score\n\n# Unterschiedliche Werte f\u00fcr die Anzahl der zu verwendenden Cluster (von 2 bis 11)\nnum_clusters_list = range(2, 11+1)\nnum_cluster_values = len(num_clusters_list)\n\n# Initialisieren die Listen, um Vorhersagen, Schwerpunkte und Bewertungspunkte zu speichern\npreds_list = [[]] * num_cluster_values         # Predictions\nsample_preds_list = [[]] * num_cluster_values  # Predictions for sample data\ncenters_list = [0] * num_cluster_values        # Centers\nscore_list = [0] * num_cluster_values          # scores\n\n# For each value of clusters to consider, perform clustering and make\n# Vorhersagen, die Zentren speichern und die Punktzahl berechnen\nfor i, num_clusters  in enumerate(num_clusters_list):\n    # den Clustering-Algorithmus auf die reduzierten Daten anwenden\n    clusterer = GaussianMixture(n_components=num_clusters, covariance_type='diag', random_state=4)\n    clusterer.fit(reduced_data)\n\n    # Vorhersage des Clusters f\u00fcr jeden Datenpunkt\n    preds_list[i] = clusterer.predict(reduced_data)\n\n    # die Clusterzentren finden\n    centers_list[i] = clusterer.means_\n\n    # Vorhersage des Clusters f\u00fcr jeden transformierten Stichproben-Datenpunkte\n    sample_preds_list[i] = clusterer.predict(pca_samples)\n\n    # den mittleren Silhouetteskoeffizienten f\u00fcr die Anzahl der ausgew\u00e4hlten Cluster berechnen\n    score_list[i] = silhouette_score(reduced_data, preds_list[i],\n                                     metric='euclidean', sample_size=None,\n                                     random_state=4)","a248b01b":"import operator\n\n# Holt den k-Wert, der zum besten Clustering-Modell f\u00fchrte\nbest_k_index = max(enumerate(score_list), key=operator.itemgetter(1))[0]\nk = num_clusters_list[best_k_index]\n\n# Holt die Vorhersagen, Zentren und Punkte f\u00fcr das beste Clustering-Modell\npreds = preds_list[best_k_index]\ncenters = centers_list[best_k_index]\nscore = score_list[best_k_index]\n\nprint(\"Bestes Ergebnis kann es erreichen wenn k={} (score={:0.3f})\".format(k, score))","3df0adfb":"# Farbpalette f\u00fcr Clusterpunkte\nPALLETTE =  [\"#FF8000\", \"#5BA1CF\", \"#9621E2\", \"#FF4F40\", \"#73AD21\",\n            \"#FFEC48\", \"#DE1BC2\", \"#29D3D1\", \"#B4F924\", \"#666666\", \"#AF2436\"]\n\n# F\u00fcr jeden der Anzahl der ber\u00fccksichtigten Clusterwerte ein Unterdiagramm erstellen\nf, axes = plt.subplots(int(np.ceil(len(num_clusters_list) \/ 2.0)), 2,\n                       figsize=(10, 14), sharex=True, sharey=True)\naxes = [item for row in axes for item in row] # Unroll axes to a flat list\n\nfor i in range(len(num_clusters_list)):\n    # Datenpunkte plotten, wobei je nach Clusterzuordnung eine andere Farbe zugewiesen wird\n    ax = axes[i]\n    colors  = [PALLETTE[y] for y in preds_list[i]]\n    ax.scatter(reduced_data[\"Dimension 1\"],\n               reduced_data[\"Dimension 2\"],\n               c=colors, s=100, alpha=0.4, linewidths=0)\n\n    # den Titel f\u00fcr den Nebenplot festlegen, einschlie\u00dflich der Anzahl der verwendeten Cluster und\n    # die Silhouetten-Punktzahl.\n    ax.set_title(\n        \"K = {k}    (Silhouettenkoeffizieten = {score:.3f})\"\\\n        \"\".format(k=num_clusters_list[i], score=score_list[i]),\n        fontdict= {\"style\": \"italic\", \"size\": 10})\n\n# den Titel f\u00fcr die Figur festlegen\nt = f.suptitle('Ergebnissen des Clusterings (and Silhouettenkoeffizienten) mithilfe GMM',\n               fontsize=15,\n               fontdict={\"fontweight\": \"extra bold\"})","c31b7c98":"def cluster_viz(reduced_data, labels, centers=None, reduced_samples=None, title=\"\", legend_labels=[\"Segment 1\",\"Segment 2\"]):\n   \n    f, ax = plt.subplots(1, 1,  figsize=(12, 10))\n\n    PALLETTE =  [\"#FF8000\", \"#5BA1CF\"]\n    colors  = [PALLETTE[y] for y in labels]\n\n    classes = np.unique(labels)\n    for class_id in classes:\n        ax.scatter(reduced_data[\"Dimension 1\"][labels==class_id],\n                   reduced_data[\"Dimension 2\"][labels==class_id],\n                   label=legend_labels[class_id],\n                   c=PALLETTE[class_id], s=100, alpha=0.4, linewidths=0)\n\n#     ax.scatter(reduced_data[\"Dimension 1\"][labels==class_id],\n#        reduced_data[\"Dimension 2\"][labels==class_id],\n#        c=colors, s=100, alpha=0.4, linewidths=0)\n\n\n    # Mittelpunkte plotten\n    if centers is not None:\n        for i, c in enumerate(centers):\n            ax.scatter(x = c[0], y = c[1], color = 'white', edgecolors=PALLETTE[i], \\\n                       alpha=1, linewidth=2, marker = 'o', s=300);\n            ax.scatter(x = c[0], y = c[1], marker='${}$'.format(i), alpha=1, s=100);\n\n    # die Stichprobe-Datenpunkte plotten\n    if reduced_samples is not None:\n        ax.scatter(x = reduced_samples[:,0], y = reduced_samples[:,1], \\\n                   s = 300,\n                   linewidth = 2,\n                   color = 'black',\n                   facecolors = 'none',\n                   edgecolors='black',\n                   marker = 'o');\n\n        for i in range(len(reduced_samples)):\n            ax.scatter(x = reduced_samples[i,0]+0.4, y = reduced_samples[i,1], marker='$({})$'.format(i), alpha = 1, color='black', s=350);\n\n    ax.legend(loc=\"lower right\", frameon=False)\n\n    # Titel des Plots festlegen\n    ax.set_title(title);","9e302dfe":"cluster_viz(reduced_data,\n            labels=preds,\n            centers=centers,\n            reduced_samples=pca_samples,\n            title=\"Customer Segments Learned by Model on PCA-Reduced Data\")","df2a5d77":"# die Inversion der Transformation der Zentren\nlog_centers = pca.inverse_transform(centers)\n\n# die Zentren potenzieren (Umkehrung der Log-Transformation)\ntrue_centers = np.exp(log_centers)\n\n# die wahren Zentrenn anzeigen\nsegments = ['Segment {}'.format(i) for i in range(0,len(centers))]\ntrue_centers = pd.DataFrame(np.round(true_centers), columns = data.keys())\ntrue_centers.index = segments\ndisplay(true_centers)","80e66de4":"(true_centers - data.mean())","17d4b761":"sample_preds = sample_preds_list[best_k_index]\n\n# die Vorhersagen ausdrucken\npotential_cust_segments = [\"Einzelhandel\", \"Restaurant\"]\nfor i, pred in enumerate(sample_preds):\n    print(\"Client {} predicted to be in Segment {} ({})\".format(i, pred,\n                                                 potential_cust_segments[pred]))","530338f1":"(true_centers - data.mean()) \/ data.std()","9daaf2a7":"# die vollst\u00e4ndigen Daten laden\ntry:\n    full_data = pd.read_csv(\"..\/input\/wholesale-customers-data-set\/Wholesale customers data.csv\")\nexcept:\n    print(\"Dataset could not be loaded.\")","5e82b0a3":"def channel_results(reduced_data, outliers, pca_samples):\n\n\n\t# \u00dcberpr\u00fcfen,ob das Dataset ladbar ist.\n\ttry:\n\t    full_data = pd.read_csv(\"..\/input\/wholesale-customers-data-set\/Wholesale customers data.csv\")\n\texcept:\n\t    print(\"Dataset could not be loaded. Is the file missing?\")       \n\t    return False\n\n\t# den Channel-DataFrame erstellen\n\tchannel = pd.DataFrame(full_data['Channel'], columns = ['Channel'])\n\tchannel = channel.drop(channel.index[outliers]).reset_index(drop = True)\n\tlabeled = pd.concat([reduced_data, channel], axis = 1)\n\t\n\t# das Cluster-Plot generieren\n\tfig, ax = plt.subplots(figsize = (14,8))\n\n\t# Farbkarte\n\tcmap = cm.get_cmap('gist_rainbow')\n\n\t# die Punkte basierend auf dem zugewiesenen Kanal f\u00e4rben\n\tlabels = ['Hotel\/Restaurant\/Cafe', 'Retailer']\n\tgrouped = labeled.groupby('Channel')\n\tfor i, channel in grouped:   \n\t    channel.plot(ax = ax, kind = 'scatter', x = 'Dimension 1', y = 'Dimension 2', \\\n\t                 color = cmap((i-1)*1.0\/2), label = labels[i-1], s=30);\n\t    \n\t# die transformierte Datenpunkte plotten \n\tfor i, sample in enumerate(pca_samples):\n\t\tax.scatter(x = sample[0], y = sample[1], \\\n\t           s = 200, linewidth = 3, color = 'black', marker = 'o', facecolors = 'none');\n\t\tax.scatter(x = sample[0]+0.25, y = sample[1]+0.3, marker='$%d$'%(i), alpha = 1, s=125);\n\n\t# Set plot title\n\tax.set_title(\"PCA-Reduced Data Labeled by 'Channel'\\nTransformed Sample Data Circled\");","94d73deb":"# die Clustering-Ergebnisse basierend auf \"Channel\"-Daten anzeigen\nchannel_results(reduced_data, outliers, pca_samples)","21256568":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split","4a55fd71":"# Daten auf den urspr\u00fcnglichen Zustand transformieren\ndata_preds = preds_list[best_k_index]\n\nnew_data = np.exp(good_data)\n\nnew_data = pd.DataFrame(new_data, columns = data.keys())\nsegment = preds_list[best_k_index]\n\n# Ein neues Merkmal hinzuf\u00fcgen\nnew_data['Segment'] = segment\n\n# Neuer Datensatz sieht so aus\nnew_data.head()","656c4918":"#Datensatz in der Merkmale und Zielvariable aufteilen\nfeature_cols = ['Fresh', 'Milk', 'Grocery', 'Frozen','Detergents_Paper','Delicatessen']\nX = new_data[feature_cols] # erkl\u00e4rende Variablen\ny = new_data.Segment # Zielvariablen","0d4808d8":"# X und y in Trainings- und Tests\u00e4tze aufteilen (25-75)\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)","8d931447":"# Instanziieren des Modells (mit den Standardparametern)\n# Das neue Model definieren\nlr_model = LogisticRegression()\n\n# das Modell mit Daten anpassen\nlr_model.fit(X_train,y_train)\n\n# Mit dem Testingdatensatz testen\ny_pred=lr_model.predict(X_test)","7cc2b50b":"a = pd.DataFrame({'Actual value': y_test, 'Predicted value':y_pred})\na.head()","0e88a590":"# Validierung mit der Konfusionsmatrix\n# die Metrikklasse importieren\nfrom sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\ncnf_matrix","6f78cf01":"class_names=[0,1] #Name der Klasse\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n\n# Heatmap erstellen\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","82c2d566":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))","1586bca2":"# Angenommen, die neuen Kunden haben folgende Ausgaben\nnew_customer = {'Fresh': [15000,22276,7000,30500,11020,1000,9858,34000,9621,25000],\n                  'Milk': [4312,2000,3353,9831,5100,5000,12000,11500,1373,3470],\n                  'Grocery': [11008,9876,43245,11246,5611,14100,2200,23000,985,18000],\n                'Frozen': [9882,3712,631,16500,5686,800,230,18800,8530,6200],\n                'Detergents_Paper': [1451,4231,3256,1221,5545,2500,650,1609,980,3800],\n                'Delicatessen': [400,1001,3534,9832,10000,800,916,1800,60,2489]\n                  }\n\npredict = pd.DataFrame(new_customer,columns= ['Fresh', 'Milk','Grocery','Frozen','Detergents_Paper','Delicatessen'])\ny_pred = lr_model.predict(predict)\npredict['Segment'] = y_pred\ndisplay(predict)","6270ec8f":"## Ausrei\u00dferbehandlung","f7f5482b":"## 1.2 \u00dcberpr\u00fcfung auf Missings und Ausrei\u00dfer\nDas Erkennen von Missings und Ausrei\u00dfern des Datensatzes ist ein wichtiger Teil der Datenexplorationsarbeit. Wir \u00fcberpr\u00fcfen auf Ausrei\u00dfer durch Visualisierung mittels *Pairplot*, *Barplot* und *Boxplot*.","65644ce0":"## Cluster erzeugen","cd00ca18":"## 1.1 Samples\nUm die Daten besser zu verstehen, w\u00e4re es am besten, ein paar Beobachtungen als Stichproben auszuw\u00e4hlen und sie genauer zu untersuchen. Im Codeblock unten werden drei Indizes der Wahl zur Liste hinzugef\u00fcgt, die die zu verfolgenden Kunden repr\u00e4sentieren werden.","fa7bbc61":"## Dimensionsreduktion","9c0259c8":"## Wiedereinf\u00fchrung des Merkmals *Channel* in den Datensatz","490f5352":"## 1. Datenexploration\nIn diesem Kapitel erforscht man die Daten durch Visualisierungen, um zu verstehen, wie jedes Merkmal mit den anderen zusammenh\u00e4ngt. Das Ziel der Datenexploration ist es, zu sehen, ob etwas von Interesse ist.","7f7f8052":"## Zentroid berechnen","f95d391d":"## Ergebnisse","a8397d6c":"## Fazit: \n\n* Mithilfe der Clusteringsmethode k\u00f6nnen wir die Kunden im Gro\u00dfhandel in zwei Segmente klassifizieren.\n\n* Nach dem Clustering kann man das Modell auf die Verkauf\/Marketing\/Lieferservice-Strategien verwenden.","231701c5":"Zuerst wollen wir *Channel* und *Region* einzig untersuchen, da die beide Merkmalen wahrscheinlich mit unserer Analyse nicht relevant sind. Es erscheint, dass es nicht so viele Unterschiede zwischen den beiden.","87b004d1":"# Projektarbeit: Absatz bei Kunden im Gro\u00dfhandel (Statistical Learning)\n* **Ziel des Projekts**: Einteilung der Gro\u00dfhandelskunden in sinnvolle Gruppen mithilfe einer Clustermethode","e258b21b":"## 3. Projekterweiterung\nVorhersagen von Segmentierung der Neukunden durch die logistische Regression.\n* **Problemstellung**: Angenommen, wir haben 10 neue Kunden. Zu welchem Segment geh\u00f6ren sie? \n* **Erkl\u00e4rende Variablen**: Fresh, Milk, Grocery, Detergents_Paper, Delicatessen\n* **Zielvariablen**: Segment","babd332d":"## Normalisierung","a62c5ee6":"## 2. Implementierung\n## 2.1 Datenverarbeitung\nVor der Clusteranalyse m\u00fcssen folgende Voraussetzungen erf\u00fcllt sein:\n* **Normalisierung**: Da die Daten verzehrt sind, fangen wir mit der Normalisierung an, indem wir einfach den nat\u00fcrlichen Logarithmus auf die Daten verwenden.\n* **Ausrei\u00dfer**: Es ist festgestellt, der Datensatz von vielen Ausrei\u00dfer zerst\u00f6rt wird. Ausgew\u00e4hlt ist die Methode nach Tukey mit Hilfe des Interquartilabstandes, um Ausrei\u00dfer zu erkennen.\n* **Dimensionsreduktion**: mittels Hauptkomponentenanalyse (PCA). Sie dient dazu, umfangreiche Datens\u00e4tze zu struktieren, zu vereinfachen und zu veranschaulichen.","16079546":"## 2.2 Modellierung\nEine wichtige Frage ist es, welche Clustering-Methode geeignet ist. Hier ist das Gau\u00dfsische Mischungsmodell ausgew\u00e4hlt, da es uns aufgefallen ist, dass die Daten eine Tendenz zur \u00dcberlappung haben. Daher ist GMM die beste Option."}}