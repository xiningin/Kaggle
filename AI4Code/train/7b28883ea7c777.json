{"cell_type":{"5aa3bc3c":"code","66a799a5":"code","d7d2f181":"code","1d3c41fd":"code","8f54c962":"code","f403914b":"code","b3cd1b6a":"code","0d819c8f":"code","d32dade4":"code","21f9332a":"code","b1f0bd97":"code","736f0599":"code","a2eaeb6c":"code","1fa2887f":"code","a929b8f0":"code","c03eff10":"code","0abb8d1c":"code","dabfcf43":"code","9176aced":"code","dbc68168":"code","25e30929":"code","44b73d41":"code","ded36787":"code","839258ee":"code","02451c9b":"code","c14514ac":"code","694c5c08":"code","92ff1c45":"code","e1069d68":"code","9c298b09":"code","3e4fdef5":"code","ef90f4fa":"code","a304fc49":"code","2289ea3e":"code","b46bdfe3":"code","ae59fc0f":"code","2fcaa3d3":"code","d28f6f72":"markdown","812f9f1d":"markdown","862a27bb":"markdown","55a6d7cd":"markdown","4c2386ba":"markdown","ab89410d":"markdown","b5cd0741":"markdown","928b352e":"markdown","38daf3b7":"markdown","45245134":"markdown","7c4ad6ef":"markdown","8465a67c":"markdown","ddc39603":"markdown","d4445d2e":"markdown","e40b91cc":"markdown","1f73ae0e":"markdown","e6c5b02c":"markdown","a0dbf0ee":"markdown","1fc98c4a":"markdown","493cafad":"markdown","f84f587b":"markdown","44dc2e4d":"markdown","f5d05f63":"markdown","8a2d7598":"markdown","474869fd":"markdown","342ff3da":"markdown","dd0803da":"markdown","6c03bce9":"markdown","0b5977e2":"markdown","d569913b":"markdown","e971ec34":"markdown","925aea26":"markdown","326ce388":"markdown","e58addb4":"markdown","d9f727fa":"markdown","90ca8130":"markdown","8d129a2c":"markdown","66dbeba5":"markdown","badbc15e":"markdown","a4122f6b":"markdown","1f627bc9":"markdown","8447a9a0":"markdown","9c8d41b4":"markdown","31161799":"markdown","e1c79a35":"markdown","19459134":"markdown","bf15134a":"markdown","ae0d67d7":"markdown","a0d562dc":"markdown","eb06972a":"markdown"},"source":{"5aa3bc3c":"#Importing the data analysis libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\n\n#Importing the visualization libraries\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#Ensuring that we don't see any warnings while running the cells\nimport warnings\nwarnings.filterwarnings('ignore') \n\n#Importing the counter\nfrom collections import Counter\n\n#Importing sci-kit learn libraries that we will need for this project\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n","66a799a5":"train = pd.read_csv(\"..\/input\/health-insurance-cross-sell-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/health-insurance-cross-sell-prediction\/test.csv\")","d7d2f181":"train.sample(10)","1d3c41fd":"train.describe(include=\"all\")","8f54c962":"print(pd.isnull(train).sum())","f403914b":"df = pd.concat(objs = [train, test], axis = 0).reset_index(drop=True)\ndf.describe(include=\"all\")","b3cd1b6a":"print(pd.isnull(df).sum())","0d819c8f":"numerical_data = df.select_dtypes(include='number')\ncategorical_data = df.select_dtypes(exclude='number')","d32dade4":"numerical_data.describe(include='all')","21f9332a":"categorical_data.head()","b1f0bd97":"sn = sns.heatmap(df[[\"Response\",\n                \"Age\",\n                \"Driving_License\", \n                \"Region_Code\", \n                \"Previously_Insured\", \n                \"Vehicle_Age\", \n                \"Vehicle_Damage\", \n                \"Annual_Premium\",\n                \"Policy_Sales_Channel\",\n                \"Vintage\"]].corr(), cmap = 'coolwarm', annot = True)","736f0599":"#A function to visualize and determine the fraction of responses in each category for a certain feature\ndef bar_plot(feature):\n    \n    feature_categories = df[feature].sort_values().unique()\n    for category in feature_categories:\n        temp_series = df[\"Response\"][df[feature] == category].value_counts(normalize = True)\n        #This code is used to solve problem when there are no Responses for a category, which causes an error in runtime\n        if temp_series.shape == (1,):\n            temp_series = temp_series.append(pd.Series([0], index=[1]))\n        elif temp_series.shape == (0,):\n            continue\n        print(\"Percentage of individuals having {}: {}, who got the insurance: {:.2f} %\".format(feature, category, temp_series[1]*100))\n    #visualize\n    sns.barplot(x = df[feature],y = df[\"Response\"],  data = df).set_title('Fraction Who Got Insurance With Respect To {}'.format(feature))","a2eaeb6c":"bar_plot(\"Gender\")","1fa2887f":"bar_plot(\"Vehicle_Age\")","a929b8f0":"bar_plot(\"Vehicle_Damage\")","c03eff10":"bar_plot(\"Driving_License\")","0abb8d1c":"bar_plot(\"Previously_Insured\")","dabfcf43":"sn = sns.heatmap(df[[\"Response\",\n                    \"Age\", \n                    \"Region_Code\",\n                    \"Vehicle_Age\",  \n                    \"Policy_Sales_Channel\",\n                    \"Vintage\"]].corr(), cmap = 'coolwarm', annot = True)","9176aced":"# A function that takes in a feature and returns the histogram\ndef histograms(feature):\n    fig = px.histogram(\n        train, \n        feature, \n        color='Response',\n        nbins=100, \n        title=('{} Vs Response'.format(feature)), \n        width=700,\n        height=500\n    )\n    fig.show()","dbc68168":"histograms(\"Age\")","25e30929":"histograms(\"Vintage\")","44b73d41":"histograms(\"Region_Code\")","ded36787":"histograms(\"Policy_Sales_Channel\")","839258ee":"histograms(\"Annual_Premium\")","02451c9b":"df[\"Vehicle_Age_Encoded\"] = df[\"Vehicle_Age\"].map({\"< 1 Year\": 0, \"1-2 Year\": 1, \"> 2 Years\": 2})","c14514ac":"df[\"Gender_Encoded\"] = df[\"Gender\"].map({\"Male\": 0, \"Female\": 1})","694c5c08":"df[\"Vehicle_Damage_Encoded\"] = df[\"Vehicle_Damage\"].map({\"No\": 0, \"Yes\": 1})","92ff1c45":"df.head()","e1069d68":"df = df.drop([\"Vehicle_Age\", \"Vehicle_Damage\", \"Gender\"], axis = 1)","9c298b09":"customer_ID = pd.Series(df[\"id\"], name = \"CustomerId\")\ndf = df.drop([\"id\", \"Vintage\"], axis = 1)","3e4fdef5":"df.sample(5)","ef90f4fa":"train = df[:train.shape[0]]\ntest = df[train.shape[0]:].drop([\"Response\"], axis = 1)","a304fc49":"#StratifiedKFold aims to ensure each class is (approximately) equally represented across each test fold\nk_fold = StratifiedKFold(n_splits=5)\n\nX_train = train.drop(labels=\"Response\", axis=1)\ny_train = train[\"Response\"]\n\n# Feature Scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\n\n# Creating objects of each classifier\nLG_classifier = LogisticRegression(random_state=0)\nSVC_classifier = SVC(kernel=\"rbf\", random_state=0)\nKNN_classifier = KNeighborsClassifier()\nNB_classifier = GaussianNB()\nDT_classifier = DecisionTreeClassifier(criterion=\"entropy\", random_state=0)\nRF_classifier = RandomForestClassifier(n_estimators=200, criterion=\"entropy\", random_state=0)\n\n#putting the classifiers in a list so I can iterate over there results easily\ninsurance_classifiers = [LG_classifier]\n\n#This dictionary is just to grad the name of each classifier\nclassifier_dict = {\n    0: \"Logistic Regression\",\n    1: \"Support Vector Classfication\",\n    2: \"K Nearest Neighbor Classification\",\n    3: \"Naive bayes Classifier\",\n    4: \"Decision Trees Classifier\",\n    5: \"Random Forest Classifier\",\n}\n\ninsurance_results = pd.DataFrame({'Model': [],'Mean Accuracy': [], \"Standard Deviation\": []})\n\n#Iterating over each classifier and getting the result\nfor i, classifier in enumerate(insurance_classifiers):\n    classifier_scores = cross_val_score(classifier, X_train, y_train, cv=k_fold, n_jobs=2, scoring=\"accuracy\")\n    insurance_results = insurance_results.append(pd.DataFrame({\"Model\":[classifier_dict[i]], \n                                                           \"Mean Accuracy\": [classifier_scores.mean()],\n                                                           \"Standard Deviation\": [classifier_scores.std()]}))","2289ea3e":"print (insurance_results.to_string(index=False))","b46bdfe3":"# from sklearn.model_selection import GridSearchCV\n\n# RF_classifier = RandomForestClassifier()\n\n\n# ## Search grid for optimal parameters\n# RF_paramgrid = {\"max_depth\": [None],\n#                   \"max_features\": [1, 3, 10],\n#                   \"min_samples_split\": [2, 3, 10],\n#                   \"min_samples_leaf\": [1, 3, 10],\n#                   \"bootstrap\": [False],\n#                   \"n_estimators\" :[100,200,300],\n#                   \"criterion\": [\"entropy\"]}\n\n\n# RF_classifiergrid = GridSearchCV(RF_classifier, param_grid = RF_paramgrid, cv=k_fold, scoring=\"accuracy\", n_jobs= -1, verbose=1)\n\n# RF_classifiergrid.fit(X_train,y_train)\n\n# RFC_optimum = RF_classifiergrid.best_estimator_\n\n# # Best Accuracy Score\n# RF_classifiergrid.best_score_","ae59fc0f":"IDtest = customer_ID[train.shape[0]:].reset_index(drop = True)","2fcaa3d3":"\nX_train = train.drop(labels=\"Response\", axis=1)\ny_train = train[\"Response\"]\n\n# Feature Scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(test)\n\nLG_classifier.fit(X_train, y_train)\n\ntest_predictions = pd.Series(LG_classifier.predict(X_test).astype(int), name=\"Response\")\ninsurance_results = pd.concat([IDtest, test_predictions], axis = 1)\ninsurance_results.to_csv('submission.csv', index=False)","d28f6f72":"### 5.1.1 - Vehicle_Age Column","812f9f1d":"## 3.3 - Joining Train\/Test","862a27bb":"### 5.2.1 - Dropping categorical columns","55a6d7cd":"# Predicting of Car Insurance\nStep by Step Guide:\n1. Importing the Necessary Libraries\n2. Importing the Dataset \n3. Dataset Analysis\n   * 3.1 Observing the data  \n   * 3.2 Determining missing values\n   * 3.3 Joining Train\/Test Data\n4. Visualizing and Comparing Features\n   * 4.1 Correlation heatmap \n   * 4.2 Comparing the effect of different categorical features and features with 2 categories on the target(Response)\n       * 4.2.1 Gender\n       * 4.2.2 Vehicle Age\n       * 4.2.3 Vehicle Damage \n       * 4.2.4 Driving_License\n       * 4.2.5 Previoulsy_Insured\n   * 4.3 Comparing the effect of different numerical features on the target(Response)   \n5. Feature Engineering\n   * 5.1 Converting categorical columns to numerical values\n       * 5.1.1 Mapping categorical Vehicle_Age feature\n       * 5.1.2 Mapping categorical Gender feature\n       * 5.1.3 Mapping categorical Vehicle_Damage feature \n   * 5.2 Dropping non-essential columns\n       * 5.2.1 Dropping categorical columns\n       * 5.2.2 Dropping Id column and Vintage column\n6. Building\/Training\/Evaluating our models\n   * 6.1 Seperating Train\/Test dataset\n   * 6.2 Modelling various classifiers\n   * 6.3 Hyperparameter tuning\n   * 6.4 Submitting","4c2386ba":"Observations:\n* There are certain spikes at certain Policy Channels\n* Most policy Channels have very few customers","ab89410d":"### Observations and Discussion:\n* The rate of people getting insurance gets higher as the age of the vehicle increases\n* Almost 30% of people who have vehicles that are older than 2 years, got the insurance\n* This shows that Vehicle_Age plays a huge part in whether people will get insurance or not","b5cd0741":"## 5.2 - Dropping non-essential columns","928b352e":"## 4.2 - Comparing the effect of different categorical features and features with 2 categories on the target(Response)","38daf3b7":"Comments:\n* I have already converted these categorical columns into numerical representations, thus these columns can now be dropped","45245134":"### 4.3.5 Policy_Sales_Channel Vs Response Distribution","7c4ad6ef":"### 4.2.3 - Vehicle Damage","8465a67c":"### 6.1 - Seperating Train\/Test dataset","ddc39603":"### Seperating categorical and numerical data","d4445d2e":"### 4.3.4 Region Code Vs Response Distribution","e40b91cc":" ### 6.2 - Modelling various classifiers","1f73ae0e":"## 6 - Building\/Training\/Evaluating our models","e6c5b02c":"# 2-Importing the Dataset","a0dbf0ee":"# 5 - Feature Engineering","1fc98c4a":"# 3-Data Analysis","493cafad":"### Obervations\/Discussion:\n* Diving deeper into the details of the dataset, we can observe that the dataset has no missing values\n* We will still need to ensure whether the values are correct though\n* The NaNs represent Cateogorical features which we will convert to Quantitative variables","f84f587b":"## 5.1 - Converting categorical columns to numerical values","44dc2e4d":"### Observations and Discussion:\n* From this graph it can be deduced that of those that did get the insurance, the fraction of males was slighty higher than the fraction of females\n* Reasons for this difference cannot be determined just now and require further analysis\n* From this observation it can be duduced that males have a 4% greater chance of getting insurance as compared to females\n* Thus Gender also plays a part in determining whether an individual will get the insurance or not","f5d05f63":"### 4.2.4 - Driving License","8a2d7598":"### Observations and Discussion:\n* The 1st graph shows that majority of those who do not go for insurance are in their 20s \n* The 2nd graph shows that the majority of people who do go for an insurance are between the ages of 38 and 50.\n* Thus age plays a significant role in determining whether an individual will get vehicle insurance or not","474869fd":"First we will combine the train and test data to ensure that we implement the feature engineering on all data, and we don't have discrepancies when modeling and evaluating. We will split the dataframe again after the feature engineering process","342ff3da":"### Observations and Discussion\n* The missing values represent the missing responses from the dataset\n* Apart from the response missing values, there are no missing values in the dataset","dd0803da":"## 3.1-Observing the data","6c03bce9":"### 4.3.2 Age Vs Response Distribution","0b5977e2":"### Observations and Discussion:\n* The rate of people getting insurance is significantly higher for people without prior insurance\n* Almost 23% of people who didn't have previous insurance, got the insurance\n* As compared to only 0.09% of people who got the insurance, who already had a previous insurance\n* This shows that Previous Insurance status plays a huge part in whether people will get insurance or not\n* This makes sense that people who already have an insurance will not be looking for further insurance or a new insurance program\n* People who are uninsured will be looking for insurance and thus explaining the difference in rate","d569913b":"### 4.2.5 - Previously Insured","e971ec34":"Observations:\n* The vast majority of the customers have an annual premium of less than 100k\n* As the annual premium increases, the chance of response = Yes(0), decreases\n* Higher premiums usually deter customers from the insurance offer which can explain the point stated above","925aea26":"### Observations and Discussion:\n* From this graph it is pretty evident that Vintage features is evenly distributed\n* This verifies the observation from the heatmap which states that Vintage plays an insignificant role in determining the response of the individual to getting an insurance or not\n* This is one of the features that can be removed from modelling, again because of the fact that it plays a very insignificant role","326ce388":"### 4.2.2 - Vehicle Age","e58addb4":"Comments:\n* The Id of the particilar customer plays no part in determining the outcome of the response thus must be dropped\n* Vintage feature has a neglible correlation with the response and other features as evident from the confusion matrix, thus it also can be dropped","d9f727fa":"### 4.3.3 Vintage Vs Response Distribution","90ca8130":"# 1-Importing the Necessary Libraries","8d129a2c":"### 5.1.3 - Vehicle_Damage Column","66dbeba5":"### Observations and Discussion:\n* The rate of people getting insurance is significantly higher for people with vehicle damage\n* Almost 25% of people who have vehicles that are damaged, got the insurance\n* Just 0.5% of people who have vehicles that are not damaged, got the insurance\n* This shows that Vehicle Damage plays a huge part in whether people will get insurance or not","badbc15e":"Observations:\n* Region code = 28 has a very large number of counts as compared to other regions\n* There are small spikes everywhere else but nothing as substantial as Region Code = 28","a4122f6b":"### 4.3.1 - Correlation heatmap for numerical data","1f627bc9":"## 4.1 - Correlation heatmap\n","8447a9a0":"### 4.2.1 - Gender","9c8d41b4":"\n### Obervations\/Discussion:\n* As we can see from the sampling that the dataset has a mixture of both Quantitative variables and Categorical Variables\n* This mix up of variables will cause problems while training our model\n* We need to convert the categorical varibales into Quantitative variables so that our ML model doesn't encounter trouble is training and predicting","31161799":"# 4 - Visualizing and Comparing the Features","e1c79a35":"### 5.2.2 - Dropping Id column and Vintage column","19459134":"## 4.3 - Comparing the effect of different numerical features on the target(Response)","bf15134a":"### 5.1.2 - Gender Column","ae0d67d7":"### 6.3 - Hyperparameter Tuning","a0d562dc":"### Observations and Discussion:\n* The rate of people getting insurance is more than double for people who have a Driving License as compared to those who don't\n* Almost 12% of people who have a Driving License, got the insurance\n* This shows that Driving License plays a significant part in whether people will get insurance or not","eb06972a":"### 6.4 - Submitting"}}