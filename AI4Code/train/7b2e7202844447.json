{"cell_type":{"f5e1cda6":"code","75cc9e31":"code","46f3877f":"code","9939907c":"code","617d805c":"code","299a381e":"code","ebd151c6":"code","06927a0e":"code","2f3ad0ca":"code","b5a209a2":"code","fc4be22e":"code","8f476d46":"code","35534890":"code","bf8164a6":"code","cb0d105a":"code","d4d3b14f":"code","9467394f":"code","63727031":"code","4a3eb8bd":"code","748d5c06":"code","54ebc40b":"code","959beec3":"code","41a658ba":"code","75f8ee6e":"code","58d4ded4":"code","9e044ee7":"code","1580c423":"code","f5713f12":"code","564e9000":"markdown","c5643bd9":"markdown","93c9c628":"markdown","ea2cd64c":"markdown","c631c934":"markdown","a2bcb92e":"markdown","064f302e":"markdown","d6ee4731":"markdown","b9520f1e":"markdown","c2a7cd2b":"markdown","cbe2b2db":"markdown","7fb67728":"markdown","e82c11a3":"markdown","90e6cdc4":"markdown","9d616a53":"markdown","9ad89f83":"markdown","4e0fb5cd":"markdown","4b569dd8":"markdown","3ac84311":"markdown","20eaa467":"markdown","a8495d13":"markdown"},"source":{"f5e1cda6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport xgboost as xgb\nimport optuna\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_val_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","75cc9e31":"# Reading training data\ntrain_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/train.csv')\n\n# Display the first line of training data, check the summary\nprint(train_df.head())\nprint(train_df.describe())\n","46f3877f":"# Reading test data\ntest_df  = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/test.csv')\n\n# Display the first line of test data, check the summary\nprint(test_df.head())\nprint(test_df.describe())\n","9939907c":"print(train_df.info())\nprint(test_df.info())","617d805c":"# Create a heat map of the correlation matrix of training data\ncor = train_df.corr()\n\nplt.figure(figsize=(10,8))\nsns.heatmap(cor, cmap= sns.color_palette('coolwarm', 10),\n            vmin = -1, vmax = 1);\n","299a381e":"column = train_df.columns[-1]\n\nfig, ax1 = plt.subplots(1,1)\n\nax1.hist(train_df[column], bins=50)\nax1.set_title(column);","ebd151c6":"# Draw a histogram of training data\ntrain_cols = [col for col in list(train_df) if col != 'id']\ntrain_df[train_cols].hist(figsize=(20,20), bins=100, color='blue', alpha=0.5)\nplt.show()\n","06927a0e":"# Draw a histogram of test data\ntest_cols = train_cols.copy()\ntest_cols.pop(-1)\n\ntest_df[test_cols].hist(figsize=(20,20), bins=100, color='orange', alpha = 0.5)\nplt.show()","2f3ad0ca":"fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(20,20))\n\nfor i, col in enumerate(test_cols):\n    sns.distplot(train_df[col],bins=50, hist=True, color='blue', ax=axes[i\/\/4, i%4])\n    sns.distplot(test_df[col],bins=50, hist=True, color='orange', ax=axes[i\/\/4, i%4])\n    fig.subplots_adjust(wspace=0.2, hspace=0.2);\n","b5a209a2":"# Store features in x_train and'target' in y_train\nfeatures = [feature for feature in train_df.columns if feature not in ['id', 'target']]\nX_train = train_df[features]\ny_train = train_df['target']\nX_test = test_df[features]","fc4be22e":"%%time\n\n# Random forest\n\nforest_reg = RandomForestRegressor(random_state=121, n_jobs=-1)\n# Learn using training data and calculate score by cross-validation (CV = 4)\n# Random forest learning takes time, so I am running it with CV = 4.\nscores = cross_val_score(forest_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=4)\n\nforest_rmse_scores = np.sqrt(-scores)\nprint('Random Forest performance:', forest_rmse_scores)\nprint('Random Forest performance_mean:', forest_rmse_scores.mean())\n","8f476d46":"%%time\n# xgboost\n\nxgb_reg = XGBRegressor(random_state=121, objective = 'reg:squarederror', n_jobs=-1)\n\nscores = cross_val_score(xgb_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\nxgb_rmse_scores = np.sqrt(-scores)\nprint('XGBoost performance:', xgb_rmse_scores)\nprint('XGBoost performance_mean:', xgb_rmse_scores.mean())\n","35534890":"%%time\n# lightGBM\nlgbm_reg = LGBMRegressor(random_state=121)\n\nscores = cross_val_score(lgbm_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\nlgbm_rmse_scores = np.sqrt(-scores)\nprint('LGBM performance:', lgbm_rmse_scores)\nprint('LGBM performance_mean:', lgbm_rmse_scores.mean())\n","bf8164a6":"# LASSO regression model\n# The default regularization strength is alpha = 1.0.\n\nlasso_reg = Lasso()\n\nscores = cross_val_score(lasso_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\nlasso_rmse_scores = np.sqrt(-scores)\nprint('LASSO performance:', lasso_rmse_scores)\nprint('LASSO performance_mean:', lasso_rmse_scores.mean())","cb0d105a":"# Ridge regression model\n# The default regularization strength is alpha = 1.0.\n\nridge_reg = Ridge()\n\nscores = cross_val_score(ridge_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\nridge_rmse_scores = np.sqrt(-scores)\nprint('Ridge performance:', ridge_rmse_scores)\nprint('Ridge performance_mean:', ridge_rmse_scores.mean())","d4d3b14f":"# Change the value of alpha and try to improve the model.\n\n# To try and compare multiple alpha conditions, \n# create a function that calculates rmse_scores and returns the mean.\ndef rmse_mean(model):\n    \"\"\"\n    RMSE averaging function\n    \n    \"\"\"\n    rmse_scores_mean = np.sqrt(\n        -cross_val_score(   # Calculate score by cross-validation\n            model, X_train, y_train, # Model, training data, correct value\n            scoring=\"neg_mean_squared_error\", \n            cv=5,          # Divide the data into 5 and use 80% for training\n            )).mean()       # Calculate the average of RMSE\n    return(rmse_scores_mean)\n\n# Prepare 6 patterns of L1 regularization intensity (alphas)\n# Larger alpha makes a simple model\nalphas = [1, 10**-1, 10**-2, 10**-3, 10**-4, 10**-5]\n\n# Perform lasso regression at each intensity of regularization\n# Calculate RMSE with CV = 5, get the average and assign it to the list\nlasso_regs = [rmse_mean(Lasso(alpha = alpha)) for alpha in alphas]\n\n# Convert lasso_regs to Pandas Series object\nlasso_regs = pd.Series(lasso_regs, index=alphas)\n\n# Output score\nprint(\"LASSO RMSE loss:\")\nprint(lasso_regs, \"\\n\")\n\n# Output the minimum score\nprint(\"LASSO RMSE best_alpha :\", lasso_regs.idxmin())\n# Outputs the regularization term parameter at the minimum score\nprint(\"LASSO RMSE best_score value :\", lasso_regs.min(), \"\\n\")\n\n# Graph the score for each intensity of regularization\nplt.figure(figsize=(10, 5))\nplt.plot(lasso_regs)\nplt.grid()\nplt.title(\"LASSO: Validation_score - by regularization strength\")\nplt.xlabel(\"Alpha\")\nplt.ylabel(\"RMSE\")\nplt.show()","9467394f":"# Prepare 9 patterns of L2 regularization intensity (alphas)\n# Larger alpha makes a simple model\nalphas = [ 1, 5, 8, 10, 11, 12, 13, 15, 20]\n\n# Perform Ridge regression at each intensity of regularization\n# Calculate RMSE with CV = 5, get the average and assign it to the list\nredge_regs = [rmse_mean(Ridge(alpha = alpha)) for alpha in alphas]\n\n# Convert redge_regs to Pandas Series object\nredge_regs = pd.Series(redge_regs, index=alphas)\n\n# Output score\nprint(\"Ridge RMSE loss:\")\nprint(redge_regs, \"\\n\")\n\n# Output the minimum score\nprint(\"Ridge RMSE best_alpha :\", redge_regs.idxmin())\n# Outputs the regularization term parameter at the minimum score\nprint(\"Ridge RMSE Loss best_score value :\", redge_regs.min(), \"\\n\")\n\n# Graph the score for each intensity of regularization\nplt.figure(figsize=(10, 5))\nplt.plot(redge_regs)\nplt.grid()\nplt.title(\"Ridge: Validation_score - by regularization strength\")\nplt.xlabel(\"Alpha\")\nplt.ylabel(\"RMSE\")\nplt.show()\n","63727031":"\n# Store training data in X and'target' in y\n\nX = train_df[features]\ny = train_df['target']","4a3eb8bd":"def objective(trial,data=X,target=y):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.15,random_state=42)\n    param = {\n        'tree_method':'gpu_hist',  # 'gpu_hist'this parameter means using the GPU \n        \n        'lambda': \n            trial.suggest_loguniform('lambda', 1e-3, 1),\n        'alpha': \n            trial.suggest_loguniform('alpha', 1e-3, 1),\n        'colsample_bytree': \n            trial.suggest_categorical('colsample_bytree', \n                                      [0.1, 0.2, 0.3,0.5,0.7,0.9]),\n        'subsample': \n            trial.suggest_categorical('subsample', \n                                      [0.1, 0.2,0.3,0.4,0.5,0.8,1.0]),\n        'learning_rate': \n            trial.suggest_categorical('learning_rate', \n                                      [0.0008, 0.01, 0.015, 0.02,0.03, 0.05,0.08,0.1]),\n        'n_estimators': 4000,\n        'max_depth': \n            trial.suggest_categorical('max_depth', \n                                      [5,7,9,11,13,15,17,20,23,25]),\n        'random_state': 48,\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 400),\n    }\n    \n    model = xgb.XGBRegressor(**param)\n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)], \n              early_stopping_rounds=100,verbose=False)\n    \n    preds = model.predict(test_x)\n    \n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse\n\n\n","748d5c06":"# Create a study object and record the learning content.\nstudy = optuna.create_study(direction='minimize') #\u3001Minimize the objective function","54ebc40b":"# This cell will take some time.\n# The search may be inadequate, but if it takes too long, reduce the number of attempts.\n# You can add the number of searches by executing this cell multiple times.\n\nstudy.optimize(objective, n_trials=25)","959beec3":"# Output the number of trials and best parameters\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","41a658ba":"study_data_table = study.trials_dataframe()\nstudy_data_table.to_csv('study_xgboost.csv', index=False)\n\nstudy_data_table","75f8ee6e":"best_trial_paras = {'tree_method':'gpu_hist', 'lambda': 0.03349655513592068, \n                    'alpha': 0.12097952030992898, 'colsample_bytree': 0.5, \n                    'subsample': 0.4, 'learning_rate': 0.01, \n              'n_estimators': 4000, 'max_depth': 11, 'min_child_weight': 179, \n              'random_state': 2021 \n              }","58d4ded4":"# At CV = 5, add up each rmse and use the average value.\n\npreds = np.zeros(test_df.shape[0])\nkf = KFold(n_splits=5,random_state=48,shuffle=True)\nrmse=[]  # list contains rmse for each fold\nn=0\nfor trn_idx, test_idx in kf.split(train_df[features],train_df['target']):\n    X_tr,X_val=train_df[features].iloc[trn_idx],train_df[features].iloc[test_idx]\n    y_tr,y_val=train_df['target'].iloc[trn_idx],train_df['target'].iloc[test_idx]\n    model = xgb.XGBRegressor(**best_trial_paras)\n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100, verbose=False)\n    preds+=model.predict(test_df[features])\/kf.n_splits\n    rmse.append(mean_squared_error(y_val, model.predict(X_val), squared=False))\n    print(n+1,rmse[n])\n    n+=1\n","9e044ee7":"np.mean(rmse)","1580c423":"# Reading submission data\nsub = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/sample_submission.csv')\nprint(sub.head())","f5713f12":"sub['target']=preds\nprint(sub.head())\nsub.to_csv('xgboost_submission.csv', index=False)","564e9000":"There seems to be no missing values.  \n\nCheck the correlation between cont1 to 14 and target.  ","c5643bd9":"There is no correlation between target and cont1-14.   \nCont2 and cont14 have a small correlation with other variables. Weak negative correlations are rarely seen between cont3-1 and cont3-9.\nOn the other hand, there are many positively correlated variables such as cont1-6,9,10,12, cont6-9,10,11,12,13, and cont11-12 has a particularly strong correlation.  \n\nCheck the distribution of each variable. First from target.  ","93c9c628":"For XGboost, try optimizing hyperparameters using Optuna.  \n\nThe code is taken from the notebook below.  \n\n+ https:\/\/www.kaggle.com\/hamzaghanmi\/xgboost-hyperparameter-tuning-using-optuna\n+ https:\/\/www.kaggle.com\/sakuraandblackcat\/leaning-validation-curve-and-optuna-for-gbdts\n\n\n","ea2cd64c":"## Submission","c631c934":"You can check the search results in the data frame.   \nIt is also possible to output and save as a csv file.","a2bcb92e":"## Modeling and Prediction","064f302e":"Learning Random Forest takes time.  \nNext, let's predict with xgboost and lightGBM.\n","d6ee4731":"It is not a single distribution, but it seems that the shape is like two distributions overlapping.\n\nCheck the distribution of each cont for train and test.","b9520f1e":"lightGBM learns faster than xgboost.  \nI can't expect much, but let's take a look at Lasso and Ridge for linear regression.","c2a7cd2b":"Check the distribution by overlaying the training data and test data","cbe2b2db":"Thank you for reading my notebook.  \nI hope the content of the article will be useful to you.","7fb67728":"Make predictions with the parameters with the best score.","e82c11a3":"The best parameters when creating a notebook were as follows.  \nNumber of finished trials: 25  \n\n\nTrial 22 finished with value: 0.6936483703969993 and parameters: {'lambda': 0.03349655513592068, 'alpha': 0.12097952030992898, 'colsample_bytree': 0.5, 'subsample': 0.4, 'learning_rate': 0.01, 'max_depth': 11, 'min_child_weight': 179}. Best is trial 22 with value: 0.6936483703969993.\n\n","90e6cdc4":"  \nLet's try Random Forest first.  ","9d616a53":"Since the distribution shape is the same, if the training data gives a good prediction, the test data is likely to give a good prediction.  \nLet's actually predict with a model.\n\n\nI refer to the following notebooks.\n+ https:\/\/www.kaggle.com\/dwin183287\/tps-jan-2021-eda-models\n","9ad89f83":"The performance of both LASSO and Ridge is not high.  \nTry to improve the prediction performance by tuning the parameters.  ","4e0fb5cd":"## Data visualization","4b569dd8":"Alpha = 11 was the best parameter in the searched range. Unfortunately, no major improvement can be expected.","3ac84311":"The optimum point was not found in the searched range.  \nSimilarly, try increasing the strength of the regularization of the Ridge regression to improve the model.","20eaa467":"There are 300,000 training data and 200,000 test data.  \nThere are 14 variables from cont1 to cont14, and there is no category data.  \n\nNext, check if there are any missing values.  ","a8495d13":"xgboost is faster to learn and more predictive than Random Forest."}}