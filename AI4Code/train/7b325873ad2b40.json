{"cell_type":{"e763b378":"code","8682f33c":"code","250b55ae":"code","b7eb6c5b":"code","c3822b97":"code","a3bfe24c":"code","94577882":"code","a3d64819":"code","3e38f657":"code","0e07d382":"code","5f4e4a9a":"code","1160c561":"code","9f544b0a":"code","67817dcb":"code","3aa1d02e":"code","086a25bd":"code","d53d8552":"code","4e1cd2a0":"code","28a9183f":"markdown","8724c9d5":"markdown","c610b3f8":"markdown","6b83871a":"markdown","a66b4370":"markdown","33a21df0":"markdown","4bb313e2":"markdown","04b62412":"markdown","736b3595":"markdown","e60dfda4":"markdown","9567c947":"markdown","eefa0950":"markdown","a4dd8a7f":"markdown","78257d2b":"markdown"},"source":{"e763b378":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","8682f33c":"import gc\nimport glob\nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport pprint\n\nimport numpy as np\nimport pandas as pd\n\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom sklearn.metrics import cohen_kappa_score, make_scorer\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\nimport scipy as sp\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nfrom functools import partial\nfrom collections import Counter\n\nimport random\nimport math","250b55ae":"#%% \u8bc4\u4ef7\u51fd\u6570 Metric used for this competition \n# (Quadratic Weigthed Kappa aka Quadratic Cohen Kappa Score)\ndef metric(y1,y2):\n    return cohen_kappa_score(y1, y2, weights = 'quadratic')\n\n\n# Make scorer for scikit-learn\nscorer = make_scorer(metric)","b7eb6c5b":"from sklearn.model_selection import StratifiedKFold\n\n#\ndef split_score(model, x_train, y_train, x_test, n=10):\n    \n    y_pre = np.zeros(x_train.shape[0])\n    oof_test = np.zeros((x_test.shape[0], n))\n    \n    kfold = StratifiedKFold(n_splits=n, random_state=4)\n    aaa = 0\n    \n    for train_index, test_index in kfold.split(x_train, y_train):\n        model.fit(x_train.iloc[train_index], y_train.iloc[train_index])\n        y_pre[test_index] = model.predict(x_train.iloc[test_index])\n        test_pred = model.predict(x_test)\n        oof_test[:, aaa] = test_pred\n        \n        print(aaa)\n        aaa += 1\n    \n#    score = metric(y_pre, y)\n    print(\"{}\u6298\u540e\u7684Kappa\u52a0\u6743\u5f97\u5206\u4e3a:\u5e26\u8865\u5145\".format(n))\n    \n    return y_pre, oof_test\n\n#\ndef fix_y(y, coef):\n    y_fix = np.copy(y)\n    for i, pred in enumerate(y_fix):\n        if pred < coef[0]:\n            y_fix[i] = 0\n        elif pred >= coef[0] and pred < coef[1]:\n            y_fix[i] = 1\n        elif pred >= coef[1] and pred < coef[2]:\n            y_fix[i] = 2\n        elif pred >= coef[2] and pred < coef[3]:\n            y_fix[i] = 3\n        else:\n            y_fix[i] = 4    \n    return y_fix\n\n# \ndef _kappa_loss(y, y_true, coef):\n    y_fix = np.copy(y)\n    for i, pred in enumerate(y_fix):\n        if pred < coef[0]:\n            y_fix[i] = 0\n        elif pred >= coef[0] and pred < coef[1]:\n            y_fix[i] = 1\n        elif pred >= coef[1] and pred < coef[2]:\n            y_fix[i] = 2\n        elif pred >= coef[2] and pred < coef[3]:\n            y_fix[i] = 3\n        else:\n            y_fix[i] = 4\n            \n    loss = metric(y_fix, y_true)\n    return -loss\n\n# \u5bfb\u627e\u5206\u7c7b\u7684\u6700\u4f73\u53c2\u6570\ndef search_coef(x1, x2):\n    loss_partial = partial(_kappa_loss, x1, x2)\n    initial_coef = [1.55, 2.05, 2.5, 3]\n    coef = sp.optimize.basinhopping(loss_partial, initial_coef, niter=500, T=1,\n                                              stepsize=0.2, minimizer_kwargs={\"method\": 'nelder-mead'}, \n                                              take_step=None, accept_test=None, callback=None, \n                                              interval=100, disp=True, niter_success=10, seed=None)\n\n    return coef","c3822b97":"df_train  = pd.read_csv('..\/input\/petfinder-adoption-prediction\/train\/train.csv')\ndf_test   = pd.read_csv('..\/input\/petfinder-adoption-prediction\/test\/test.csv')\n\ntrain = df_train.copy()\ntest  = df_test.copy()\n\nlabels_breed = pd.read_csv('..\/input\/petfinder-adoption-prediction\/breed_labels.csv')\nlabels_state = pd.read_csv('..\/input\/petfinder-adoption-prediction\/color_labels.csv')\nlabels_color = pd.read_csv('..\/input\/petfinder-adoption-prediction\/state_labels.csv')","a3bfe24c":"def extract_sentiment_feature(i, x):    \n#    feature_sentiment = pd.DataFrame(columns=['PetID', 'token', 'sentence_magnitude', 'sentence_score','document_magnitude', 'document_score'])\n    feature_sentiment = pd.DataFrame()\n\n    if x == 'train':\n        set_file = 'train'\n    else:\n        set_file = 'test' \n        \n    file_name = '..\/input\/petfinder-adoption-prediction\/{}_sentiment\/{}.json'.format(set_file,i)\n    try:\n        with open(file_name, 'r', encoding='utf-8') as f:\n            sentiment_file = json.load(f)\n\n            token = [x['name'] for x in sentiment_file['entities']]\n            token = ' '.join(token)\n\n            sentences_sentiment = [x['sentiment'] for x in sentiment_file['sentences']]\n            sentences_sentiment = pd.DataFrame.from_dict(\n                sentences_sentiment, orient='columns')\n\n            \n            docementSentiment_magnitude = sentiment_file['documentSentiment']['magnitude']\n            documentSentiment_score     = sentiment_file['documentSentiment']['score']\n            \n            new = pd.DataFrame(\n                    {'PetID'    : i, \n\n                    'magnitude_sum' : sentences_sentiment['magnitude'].sum(axis=0),\n                    'score_sum'     : sentences_sentiment['score'].sum(axis=0),\n                    'magnitude_mean': sentences_sentiment['magnitude'].mean(axis=0),\n                    'score_mean'    : sentences_sentiment['score'].mean(axis=0),\n                    'magnitude_var' : sentences_sentiment['magnitude'].var(axis=0),\n                    'score_var'     : sentences_sentiment['score'].var(axis=0),\n\n                    'document_magnitude'  : [docementSentiment_magnitude], \n                    'document_score'      : [documentSentiment_score]})  \n            feature_sentiment = feature_sentiment.append(new)\n    except:\n        print('{}\u6ca1\u627e\u5230'.format(file_name))\n    \n    for each in feature_sentiment.columns:\n        if each not in ['PetID','token']:\n            feature_sentiment[each] = feature_sentiment[each].astype(float)\n\n    return feature_sentiment\n\n#%%\ntrain_feature_sentiment = Parallel(n_jobs=8, verbose=1)(\n        delayed(extract_sentiment_feature)(i, 'train') for i in train.PetID)\ntrain_feature_sentiment = [x for x in train_feature_sentiment]\ntrain_feature_sentiment = pd.concat(train_feature_sentiment, ignore_index=True, sort=False)\n\ntest_feature_sentiment = Parallel(n_jobs=8, verbose=1)(\n        delayed(extract_sentiment_feature)(i, 'test') for i in test.PetID)\ntest_feature_sentiment = [x for x in test_feature_sentiment]\ntest_feature_sentiment = pd.concat(test_feature_sentiment, ignore_index=True, sort=False)","94577882":"picture_metadata  = pd.read_csv(r'..\/input\/32111111\/picture.csv')\npicture_size      = pd.read_csv(r'..\/input\/32111111\/picture_size.csv')\nimg_features      = pd.read_csv(r'..\/input\/32111111\/img_features.csv')\n\n#%% \u8fde\u63a5sentiment\u548cmetadata\u548c\u539f\u59cb\u6570\u636e\nx_train = df_train.merge(train_feature_sentiment, how='left', on='PetID')\nx_train = x_train.merge(picture_metadata, how='left', on='PetID')\nx_train = x_train.merge(picture_size, how='left', on='PetID')\nx_train = x_train.merge(img_features, how='left', on='PetID')\n\ny_train = x_train['AdoptionSpeed']\nx_train.drop(['AdoptionSpeed'], axis=1, inplace=True)\n\nx_test = df_test.merge(test_feature_sentiment, how='left', on='PetID')\nx_test = x_test.merge(picture_metadata, how='left', on='PetID')\nx_test = x_test.merge(picture_size, how='left', on='PetID')\nx_test  = x_test.merge(img_features, how='left', on='PetID')\n\n","a3d64819":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import SparsePCA, TruncatedSVD, LatentDirichletAllocation, NMF\n\ncol_text = ['Description']\n\nx = x_train.append(x_test).reset_index()\nx = x[['Description', 'PetID']]\n\nn_components = 50\n\nx[col_text] = x[col_text].fillna('MISSING')\ntext_features = []\n\n\nfor i in  ['Description']:\n    svd_ = TruncatedSVD(n_components=n_components)\n        \n    tfv = CountVectorizer(min_df=3,  \n                          max_df=0.9,\n                          stop_words = 'english')\n    \n    tfidf_col = tfv.fit_transform(x.loc[:, i])\n\n    \n    svd_col = svd_.fit_transform(tfidf_col)\n    svd_col = pd.DataFrame(svd_col)\n    svd_col = svd_col.add_prefix('SVD_{}_'.format(i))\n\n\n    text_features.append(svd_col)    \n    \n    x.drop(i, axis=1, inplace=True)\n    \n# Combine all extracted features:\ntext_features = pd.concat(text_features, axis=1)\n\n# Concatenate with main DF:\nx = pd.concat([x, text_features], axis=1)\n\nx_train = x_train.merge(x, how='left', on='PetID')\nx_test  = x_test.merge(x, how='left', on='PetID')\n","3e38f657":"from textblob import TextBlob\n\nx = x_train.append(x_test)\nx = x[['PetID', 'Description']]\n\nx['Description'] = x['Description'].fillna(\"Missing\")\nx['Description'] = x['Description'].apply(lambda x:TextBlob(x))\n\nx['polarity']     = x['Description'].apply(lambda x:x.sentiment[0])\nx['subjectivity'] = x['Description'].apply(lambda x:x.sentiment[1])\n\n#\u5bf9\u60c5\u611f\u8fdb\u884c\u5206\u7bb1\nbin=[-2,0,0.3,2]\nx['polarity'] = pd.cut(x['polarity'], bins=bin, labels=range(3))\nx['polarity'] = x['polarity'].astype(np.int32)\n\nx_train = x_train.merge(x[['PetID', 'polarity']], how='left', on='PetID')\nx_test  = x_test.merge(x[['PetID', 'polarity']], how='left', on='PetID')","0e07d382":"#\u662f\u5426\u662f\u514d\u8d39\u7684\nx_train['IsFree'] = x_train['Fee'].apply(lambda x:True if x == 0 else False)\nx_test['IsFree']  = x_test['Fee'].apply(lambda x:True if x == 0 else False)\n\n#\u5e74\u9f84\uff08\u6309\u5e74\uff09\nx_train['Year'] = x_train['Age'].apply(lambda x:math.floor(x\/12))\nx_test['Year']  = x_test['Age'].apply(lambda x:math.floor(x\/12))\n\n#\u5e74\u9f84\u5206\u7bb1,\u6309\u9891\u5212\u62105\u4efd\nx = x_train.append(x_test)\nx['Age_qcut'] = pd.qcut(x['Age'], 5,  duplicates='drop')\nx['Age_qcut'] = pd.factorize(x['Age_qcut'])[0]\nx_train = x_train.merge(x[['PetID','Age_qcut']], how='left', on='PetID')\nx_test  = x_test.merge(x[['PetID','Age_qcut']], how='left', on='PetID')\n\n\n#\u8840\u7f18\u7684\u79cd\u7c7b\nx = x_train.append(x_test).reset_index()\nBreed1_count = x.groupby('Breed1').size().to_frame('Breed1_count').reset_index()\nx_train = x_train.merge(Breed1_count, how='left', on='Breed1')\nx_test  = x_test.merge(Breed1_count, how='left', on='Breed1')\n\n#\u662f\u5426\u7a00\u6709\na = x['Breed1'].value_counts().sort_values(ascending = False).cumsum()\/len(x)\nrare1_index = a[a > 0.85].index.tolist()\nx_train['IsRare1'] = x_train['Breed1'].isin(rare1_index).apply(lambda x:True if x == True else False)\nx_test['IsRare1']  = x_test['Breed1'].isin(rare1_index).apply(lambda x:True if x == True else False)\nrare2_index = a[a > 0.72].index.tolist()\nx_train['IsRare2'] = x_train['Breed1'].isin(rare2_index).apply(lambda x:True if x == True else False)\nx_test['IsRare2']  = x_test['Breed1'].isin(rare2_index).apply(lambda x:True if x == True else False)\n\n#\u662f\u5426\u5e38\u89c1\nx_train['Is_COMMON'] = x_train['Breed1'].apply(lambda x:True if (x == 265 or x == 307 or x == 266) else False)\nx_test['Is_COMMON']  = x_test['Breed1'].apply(lambda x:True if (x == 265 or x == 307 or x == 266) else False)\n\n#\u7167\u7247\u5206\u7bb1\nbin=[-0.5,0.5,1.5,4.5,1000]\nx_train['Photo_cut'] = pd.cut(x_train['PhotoAmt'], bins=bin, labels=range(4)).astype(np.int32)\nx_test['Photo_cut']  = pd.cut(x_test['PhotoAmt'], bins=bin, labels=range(4)).astype(np.int32)\n\n# \u662f\u5426\u662f\u7a00\u6709\u989c\u8272\nx_train['Is_rare_color1'] = x_train['Color1'].apply(lambda x:True if x==5 or x==6 or x==7 else False)\nx_test['Is_rare_color1'] = x_test['Color1'].apply(lambda x:True if x==5 or x==6 or x==7 else False)\nx_train['Is_rare_color2'] = x_train['Color2'].apply(lambda x:True if x==6 else False)\nx_test['Is_rare_color2'] = x_test['Color2'].apply(lambda x:True if x==6 else False)\n\n#\u5e74\u9f84\u662f\u5426\u5c0f\u4e8e\u4e8c\u6708\nx_train['Is_less_than_2month']= x_train['Age'].apply(lambda x:True if x<3 else False)\nx_test['Is_less_than_2month'] = x_test['Age'].apply(lambda x:True if x<3 else False)\n","5f4e4a9a":"#%% RescuerID \u5904\u7406\n\ndf = df_train.append(df_test)\ndata_rescuer = df.groupby(['RescuerID'])['PetID'].size().reset_index()\ndata_rescuer.columns = ['RescuerID', 'RescuerID_count']\n#data_rescuer['rank_Rescuer_count'] = data_rescuer['RescuerID_count'].rank(pct=True)\n\nx_train = x_train.merge(data_rescuer, how='left', on='RescuerID')\nx_test  = x_test.merge(data_rescuer, how='left', on='RescuerID')\n\nx = x_train.append(x_test)\nx['RescuerID_count_cut'] = pd.qcut(x['RescuerID_count'], 5, labels=range(4), duplicates='drop').astype(np.int32)\n\nx_train = x_train.merge(x[['PetID', 'RescuerID_count_cut']], how='left', on='PetID')\nx_test  = x_test.merge(x [['PetID', 'RescuerID_count_cut']], how='left', on='PetID')\n\n#x_train.drop(['RescuerID_count'], axis=1, inplace=True)\n#x_test.drop(['RescuerID_count'], axis=1, inplace=True)","1160c561":"# \u589e\u52a0\u7279\u5f81 \u662f\u5426\u6709\u7b2c\u4e8c\u8840\u7edf\nx_train['HasSecondBreed'] = x_train['Breed2'].map(lambda x:True if x != 0 else False)\nx_test['HasSecondBreed'] = x_test['Breed2'].map(lambda x:True if x != 0 else False)\n\ntrain_breed_main = x_train[['Breed1']].merge(\n    labels_breed, how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))\n\ntrain_breed_main = train_breed_main.iloc[:, 2:]\ntrain_breed_main = train_breed_main.add_prefix('main_breed_')\n\ntrain_breed_second = x_train[['Breed2']].merge(\n    labels_breed, how='left',\n    left_on='Breed2', right_on='BreedID',\n    suffixes=('', '_second_breed'))\n\ntrain_breed_second = train_breed_second.iloc[:, 2:]\ntrain_breed_second = train_breed_second.add_prefix('second_breed_')\n\nx_train = pd.concat(\n    [x_train, train_breed_main, train_breed_second], axis=1)\n\n##############\ntest_breed_main = x_test[['Breed1']].merge(\n    labels_breed, how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))\n\ntest_breed_main = test_breed_main.iloc[:, 2:]\ntest_breed_main = test_breed_main.add_prefix('main_breed_')\n\ntest_breed_second = x_test[['Breed2']].merge(\n    labels_breed, how='left',\n    left_on='Breed2', right_on='BreedID',\n    suffixes=('', '_second_breed'))\n\ntest_breed_second = test_breed_second.iloc[:, 2:]\ntest_breed_second = test_breed_second.add_prefix('second_breed_')\n\nx_test = pd.concat(\n    [x_test, test_breed_main, test_breed_second], axis=1)\n\nprint(x_train.shape, x_test.shape)\n\ncategorical_columns = ['main_breed_BreedName', 'second_breed_BreedName']\n#for i in categorical_columns:\n#    x_train.loc[:, i] = pd.factorize(x_train.loc[:, i])[0]\n#    x_test.loc[:,i]   = pd.factorize(x_test.loc[:, i])[0]\n\n# \u589e\u52a0\u7279\u5f81 \u662f\u5426\u7eaf\u79cd\nx_train['True_Pure'] = False\nx_train.loc[(x_train['main_breed_BreedName'] != 'Mixed Breed')&\n                    ((x_train['main_breed_BreedName'] == x_train['second_breed_BreedName'])|\n                   (x_train['second_breed_BreedName'].isnull())),'True_Pure'] = True\n\n\nx_test['True_Pure'] = False\nx_test.loc[(x_test['main_breed_BreedName'] != 'Mixed Breed')&\n                    ((x_test['main_breed_BreedName'] == x_test['second_breed_BreedName'])|\n                   (x_test['second_breed_BreedName'].isnull())),'True_Pure'] = True\n\n# \u662f\u5426\u7eaf\u79cd\u72d7\nx_train['Is_Pure_Dog'] = (x_train['True_Pure'] == True) & (x_train['Type'] == 1)\nx_test['Is_Pure_Dog']  = (x_test['True_Pure'] == True)  & (x_test['Type'] == 1)\n\n\n\n#\u5220\u9664\u6ca1\u7528\u7279\u5f81\nx_train.drop(['main_breed_BreedName', 'second_breed_BreedName', 'main_breed_Type', 'second_breed_Type'], axis=1, inplace=True)\nx_test.drop(['main_breed_BreedName', 'second_breed_BreedName', 'main_breed_Type', 'second_breed_Type'], axis=1, inplace=True)","9f544b0a":"drop_columns = ['Name', 'RescuerID', 'Description', 'PetID', 'token', 'annots_top_desc']\ndrop_columns = ['Name', 'RescuerID', 'Description', 'PetID']\n\nx_train.drop(drop_columns, axis=1, inplace=True)\nx_test.drop(drop_columns, axis=1, inplace=True)\n\nx_train = x_train.fillna(0)\nx_test  = x_test.fillna(0)\n","67817dcb":"#\u5c5e\u6027\u6807\u7b7e\nc = ['Breed1', 'Breed2', 'Gender', 'Color1', 'Color2', 'Color3',\n    'Vaccinated', 'Dewormed', 'Sterilized', 'Health', 'Quantity',\n    'State', 'IsFree', 'Year', 'Age_qcut', 'IsRare1', 'IsRare2',\n    'Is_COMMON', 'Photo_cut', 'Is_rare_color1', 'Is_rare_color2', 'Is_less_than_2month',\n    'RescuerID_count_cut', 'HasSecondBreed', 'True_Pure', 'Is_Pure_Dog']\n\nfor each in c:\n    x_train[each] = x_train[each].astype('category')\n    x_test[each] = x_test[each].astype('category')","3aa1d02e":"from lightgbm.sklearn import LGBMRegressor\n\nmodel_lgb = LGBMRegressor(\n        learning_rate    = 0.01,\n        n_estimators     = 2000,\n        max_depth        = 4,\n        num_leaves       = 12 ,\n        subsample        = 0.8,      #\u8bad\u7ec3\u65f6\u91c7\u6837\u4e00\u5b9a\u6bd4\u4f8b\u7684\u6570\u636e\t\n        colsample_bytree = 0.8,\n        n_jobs           = -1,\n        random_state     = 44,\n        objective        = 'regression',\n#        reg_alpha        = 1,\n        eval_metric      = 'scorer',\n        min_child_samples = 15         #\u53f6\u5b50\u8282\u70b9\u5177\u6709\u7684\u6700\u5c0f\u8bb0\u5f55\u6570\t\n        )\n        ","086a25bd":"y_lgb, y_test_pre = split_score(model_lgb, x_train, y_train, x_test)\ny_test_pre = y_test_pre.mean(axis=1)\n\ncoe = search_coef(y_lgb, y_train)\nbest_lgb_coe = coe['x']\nprint('lgb\u7684\u6700\u4f73\u7cfb\u6570\u4e3a{}'.format(best_lgb_coe))\n\nmodel_lgb.fit(x_train, y_train)\nresult_lgb_fix = fix_y(y_test_pre, best_lgb_coe)\nprint('lgb\u540e\u7684\u5206\u5e03:',Counter(result_lgb_fix))","d53d8552":"submission_lgb = pd.DataFrame({'PetID': df_test['PetID'].values, 'AdoptionSpeed': result_lgb_fix.astype(np.int32)})\nsubmission_lgb.to_csv('submission.csv', index=False)","4e1cd2a0":"#submission_xgb = pd.DataFrame({'PetID': df_test['PetID'].values, 'AdoptionSpeed': result_xgb_fix.astype(np.int32)})\n#submission_xgb.to_csv('submission.csv', index=False)","28a9183f":"\u540e\u5904\u7406","8724c9d5":"LGB","c610b3f8":"# \u6570\u636e\u6e05\u7406","6b83871a":"# RescuerID\u5904\u7406","a66b4370":"# \u589e\u52a0\u65b0\u7279\u5f81","33a21df0":"# \u5047\u5982\u60c5\u611f\u5143\u7d20","4bb313e2":"\u8bfb\u53d6\u6570\u636e","04b62412":"Cross\u9a8c\u8bc1\u51fd\u6570","736b3595":"# \u52a0\u5165metadata\u548c\u56fe\u50cf\u5927\u5c0f\u3001\u795e\u7ecf\u7f51\u7edc\u63d0\u53d6\u7684\u5143\u7d20","e60dfda4":"# \u8f6c\u6362\u6210\u7c7b\u522b\u5c5e\u6027","9567c947":"# \u5904\u7406Breed","eefa0950":"\u8bc4\u4ef7\u51fd\u6570","a4dd8a7f":"> **** \u63d0\u53d6 sentiment \u7684\u7279\u5f81","78257d2b":"# NLP"}}