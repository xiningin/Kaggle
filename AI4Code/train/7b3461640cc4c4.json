{"cell_type":{"3d602673":"code","9d0160f9":"code","5b1e0630":"code","19845d69":"code","fe92e44b":"code","1cd60cdc":"code","dee1d41c":"code","673d32fe":"code","b567a3ef":"code","c254bf3b":"code","35d253a3":"code","98d01fd9":"code","30ab530d":"code","ad4c7a51":"code","e89c2eb2":"code","e1cba8a4":"code","33efe4d6":"code","90bf801e":"code","4120f8aa":"code","5bc7e415":"code","e3038380":"code","cd9ab01d":"code","780ecbeb":"code","3e79de2b":"code","8e6f4c52":"code","935f93db":"code","66ce5d67":"code","7db42434":"code","b5b33944":"code","1db1a41c":"code","40836e3c":"code","183b2197":"code","7ec4f01b":"code","2ac730e1":"code","5384cfdc":"code","1bbf3522":"code","a8a0b8a0":"code","d1d2abc1":"code","3bb2fee0":"code","e5c4dd0a":"code","50946cd1":"code","c6da07f0":"markdown","74682283":"markdown","8b4c9c2e":"markdown","57d84553":"markdown","0fdb4b35":"markdown","aa875db7":"markdown","54b6dfae":"markdown","f1202c6e":"markdown","c0293187":"markdown","2ce95cbc":"markdown","f3965e59":"markdown","9fdefe03":"markdown","b57bd1d4":"markdown","2488c07f":"markdown","3907d77a":"markdown","e39c5bbf":"markdown","77686998":"markdown","8f32d1da":"markdown"},"source":{"3d602673":"# packages\n\n# standard\nimport numpy as np\nimport pandas as pd\nimport time\n\n# plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.mosaicplot import mosaic\n\n# machine learning tools\nimport h2o\nfrom h2o.estimators import H2OGeneralizedLinearEstimator, H2ORandomForestEstimator","9d0160f9":"# load data + first glance\ndf_train = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv')\ndf_sub = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/sample_submission.csv')\n\n# first glance (training data)\ndf_train.head()","5b1e0630":"# dimensions\ndf_train.shape","19845d69":"df_train.info()","fe92e44b":"# basic stats\nprint(df_train.target.value_counts())\ndf_train.target.value_counts().plot(kind='bar')\nplt.grid()\nplt.show()","1cd60cdc":"features_num = ['cont0', 'cont1', 'cont2', 'cont3', \n                'cont4', 'cont5', 'cont6', 'cont7',\n                'cont8', 'cont9', 'cont10']","dee1d41c":"# plot distribution of numerical features\nfor f in features_num:\n    plt.figure(figsize=(8,4))\n    df_train[f].plot(kind='hist', bins=100)\n    plt.title(f)\n    plt.grid()\n    plt.show()","673d32fe":"corr_pearson = df_train[features_num].corr(method='pearson')\ncorr_spearman = df_train[features_num].corr(method='spearman')\n\nfig = plt.figure(figsize = (10,8))\nsns.heatmap(corr_pearson, annot=True, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Pearson Correlation')\nplt.show()\n\nfig = plt.figure(figsize = (10,8))\nsns.heatmap(corr_spearman, annot=True, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Spearman Correlation')\nplt.show()","b567a3ef":"# example of scatter plot - we pick pair having highest (Pearson) correlation\nsns.jointplot(data=df_train, x='cont1', y='cont2', kind='hex')\nplt.show()","c254bf3b":"features_cat = ['cat0', 'cat1', 'cat2', 'cat3',\n                'cat4', 'cat5', 'cat6', 'cat7',\n                'cat8', 'cat9', 'cat10', 'cat11',\n                'cat12', 'cat13', 'cat14', 'cat15',\n                'cat16', 'cat17', 'cat18']","35d253a3":"# plot distribution of categorical features\nfor f in features_cat:\n    plt.figure(figsize=(14,4))\n    df_train[f].value_counts().plot(kind='bar')\n    plt.title(f)\n    plt.grid()\n    plt.show()","98d01fd9":"# count different values\/levels\ncat10_freq = df_train.cat10.value_counts()\nprint(cat10_freq)\n\n# and plot frequency distribution using log scale\nfig, ax = plt.subplots(figsize=(12,4))\nax.plot(np.log10(cat10_freq))\nax.xaxis.set_major_locator(plt.MaxNLocator(20)) # reduce number of x-axis labels\nplt.title('cat10 - Frequencies')\nplt.ylabel('log10(Frequency)')\nplt.grid()\nplt.show()","30ab530d":"# evaluate mean of target by level\ncat10_target = df_train.groupby(['cat10']).agg({\n    'target' : ['mean','count']})\n# ... and sort by frequency of level\ncat10_target = cat10_target.sort_values([('target','count')], ascending=False)\n\n# plot mean of target by level; bubble area ~ frequency\nfig, ax = plt.subplots(figsize=(12,6))\nax.scatter(cat10_target.index, cat10_target[('target','mean')],\n           s=2*np.sqrt(cat10_target[('target','count')]),\n           alpha = 0.5)\nax.xaxis.set_major_locator(plt.MaxNLocator(20)) # reduce number of x-axis labels\nplt.title('cat10 - Average target by level (bubble area ~ frequency)')\nplt.grid()\nplt.show()","ad4c7a51":"# let's give it a try: define levels to be kept\nn_keep = 201\ncat10_keep = cat10_freq[0:n_keep].index.tolist()\nprint(cat10_keep)","e89c2eb2":"# add new column with reduced number of levels\ndf_train['cat10_reduced'] = df_train.cat10.where(df_train.cat10.isin(cat10_keep), '_OTHER_')\ndf_train.cat10_reduced.value_counts()","e1cba8a4":"# check frequency of _OTHER_ category\ndf_train[df_train.cat10_reduced=='_OTHER_'].cat10_reduced.value_counts()","33efe4d6":"# same for test set!\ndf_test['cat10_reduced'] = df_test.cat10.where(df_test.cat10.isin(cat10_keep), '_OTHER_')\ndf_test.cat10_reduced.value_counts()","90bf801e":"# update feature list accordingly\nfeatures_cat = ['cat0', 'cat1', 'cat2', 'cat3',\n                'cat4', 'cat5', 'cat6', 'cat7',\n                'cat8', 'cat9', 'cat10_reduced', 'cat11',\n                'cat12', 'cat13', 'cat14', 'cat15',\n                'cat16', 'cat17', 'cat18']","4120f8aa":"# plot target vs binned numerical features using mosaic plot\nplt_para_save = plt.rcParams['figure.figsize'] # remember plot settings\n\nfor f in features_num:\n    \n    # add binned version of each numerical feature first\n    new_var = f + '_bin'\n    df_train[new_var] = pd.qcut(df_train[f], 10)\n    \n    # then create mosaic plot\n    plt.rcParams[\"figure.figsize\"] = (16,6) # increase plot size for mosaics\n    mosaic(df_train, [new_var, 'target'], title='Target vs ' + f + ' [binned]')\n    plt.show()\n    \n# reset plot size again\nplt.rcParams['figure.figsize'] = plt_para_save","5bc7e415":"# plot target vs features using mosaic plot\nplt_para_save = plt.rcParams['figure.figsize'] # remember plot settings\n\nfor f in features_cat:\n    plt.rcParams[\"figure.figsize\"] = (16,6) # increase plot size for mosaics\n    mosaic(df_train, [f, 'target'], title='Target vs ' + f + ' [binned]')\n    plt.show()\n    \n# reset plot size again\nplt.rcParams['figure.figsize'] = plt_para_save","e3038380":"# select predictors\npredictors = features_num + features_cat\nprint('Number of predictors: ', len(predictors))\nprint(predictors)","cd9ab01d":"# start H2O\nh2o.init(max_mem_size='12G', nthreads=4) # Use maximum of 12 GB RAM and 4 cores","780ecbeb":"# upload data frames in H2O environment\nt1 = time.time()\ntrain_hex = h2o.H2OFrame(df_train)\ntest_hex = h2o.H2OFrame(df_test)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))\n\n# force categorical target\ntrain_hex['target'] = train_hex['target'].asfactor()","3e79de2b":"# fit GLM model\nn_cv = 5\nfit_1 = H2OGeneralizedLinearEstimator(nfolds=n_cv,\n                                      family='binomial',\n                                      alpha=0,\n                                      Lambda=0,\n                                      seed=999)\n\n# train model\nt1 = time.time()\nfit_1.train(x=predictors,\n            y='target',\n            training_frame=train_hex)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","8e6f4c52":"# show cross validation metrics\nfit_1.cross_validation_metrics_summary()","935f93db":"# basic version\nfit_1.varimp_plot(20)","66ce5d67":"# training performance\nperf_train = fit_1.model_performance(train=True)\nperf_train.plot()","7db42434":"# cross validation performance\nperf_cv = fit_1.model_performance(xval=True)\nperf_cv.plot()","b5b33944":"# Random Forest model\nn_cv = 5\nfit_2 = H2ORandomForestEstimator(nfolds=n_cv,\n                                 distribution='bernoulli',\n                                 ntrees=100,\n                                 mtries=-1, # automatic selection\n                                 max_depth=20,\n                                 score_each_iteration=True,\n                                 stopping_metric='auc',\n                                 stopping_rounds=5,\n                                 stopping_tolerance=0.0001,\n                                 seed=999)\n\n# train model\nt1 = time.time()\nfit_2.train(x=predictors,\n            y='target',\n            training_frame=train_hex)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","1db1a41c":"# show cross validation metrics\nfit_2.cross_validation_metrics_summary()","40836e3c":"# show scoring history - training vs cross validations\nfor i in range(n_cv):\n    cv_model_temp = fit_2.cross_validation_models()[i]\n    df_cv_score_history = cv_model_temp.score_history()\n    my_title = 'CV ' + str(1+i) + ' - Scoring History [AUC]'\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.training_auc, \n                c='blue', label='training')\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.validation_auc, \n                c='darkorange', label='validation')\n    plt.title(my_title)\n    plt.xlabel('Number of Trees')\n    plt.ylabel('AUC')\n    plt.ylim(0.7,1)\n    plt.legend()\n    plt.grid()\n    plt.show()\n","183b2197":"# training performance\nperf_train = fit_2.model_performance(train=True)\nperf_train.plot()","7ec4f01b":"# cross validation performance\nperf_cv = fit_2.model_performance(xval=True)\nperf_cv.plot()","2ac730e1":"# predict on test set (extract probabilities only)\npred_test_GLM = fit_1.predict(test_hex)['p1']\npred_test_GLM = pred_test_GLM.as_data_frame().p1\n\n# plot test set predictions (probabilities)\nplt.figure(figsize=(6,4))\nplt.hist(pred_test_GLM, bins=100)\nplt.title('Predictions on Test Set - GLM')\nplt.grid()\nplt.show()","5384cfdc":"# predict on test set (extract probabilities only)\npred_test_RF = fit_2.predict(test_hex)['p1']\npred_test_RF = pred_test_RF.as_data_frame().p1\n\n# plot test set predictions (probabilities)\nplt.figure(figsize=(6,4))\nplt.hist(pred_test_RF, bins=100)\nplt.title('Predictions on Test Set - RF')\nplt.grid()\nplt.show()","1bbf3522":"# combine predictions in one data frame\ndf_preds_test = pd.DataFrame({'GLM': pred_test_GLM.values, 'RF': pred_test_RF.values})","a8a0b8a0":"# scatter plot of two prediction sets\nsns.jointplot(data=df_preds_test, x='GLM', y='RF',\n              joint_kws={'s' : 2},\n              alpha=0.1)\nplt.show()","d1d2abc1":"# correlation\ndf_preds_test.corr()","3bb2fee0":"# GLM submission\ndf_sub_GLM = df_sub.copy()\ndf_sub_GLM.target = df_preds_test.GLM\ndisplay(df_sub_GLM.head())\n# save to file\ndf_sub_GLM.to_csv('submission_GLM.csv', index=False)","e5c4dd0a":"# RF submission\ndf_sub_RF = df_sub.copy()\ndf_sub_RF.target = df_preds_test.RF\ndisplay(df_sub_RF.head())\n# save to file\ndf_sub_RF.to_csv('submission_RF.csv', index=False)","50946cd1":"# blend two model results\ndf_sub_blend = df_sub.copy()\ndf_sub_blend.target = 0.5*df_preds_test.GLM + 0.5*df_preds_test.RF\ndisplay(df_sub_blend.head())\n# save to file\ndf_sub_blend.to_csv('submission_blend.csv', index=False)","c6da07f0":"<a id='5'><\/a>\n# Build GLM Model","74682283":"### Blend two models","8b4c9c2e":"#### We are lucky, no missing values!","57d84553":"#### Well, \"cat10\" has lots of different values, this might require a closer look...","0fdb4b35":"<a id='7'><\/a>\n# Predict on Test Set and prepare submissions","aa875db7":"### Variable Importance","54b6dfae":"<a id='3'><\/a>\n# Categorical Features","f1202c6e":"<a id='2'><\/a>\n# Numerical Features","c0293187":"# Let's try something different than Gradient Boosting...\n\n## Table of Contents\n* [Target Exploration](#1)\n* [Numerical Features](#2)\n* [Categorical Features](#3)\n* [Target vs Features](#4)\n* [Build GLM Model](#5)\n* [Build Random Forest Model](#6)\n* [Predict on Test Set and prepare submissions](#6)","2ce95cbc":"## Categorical Features","f3965e59":"### This time we have a categorical (binary) target!","9fdefe03":"## Numerical Features","b57bd1d4":"<a id='4'><\/a>\n# Target vs Features","2488c07f":"## Check performance on training data \/ cross validations","3907d77a":"<a id='6'><\/a>\n# Build Random Forest Model","e39c5bbf":"#### => It could be beneficial to group the less frequent levels (e. g. right of \"CP\") into a group \"other\".","77686998":"## Feature Correlations","8f32d1da":"<a id='1'><\/a>\n# Target Exploration"}}