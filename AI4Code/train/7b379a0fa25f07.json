{"cell_type":{"e2c4c82c":"code","0277ccd6":"code","b74dff5d":"code","4f390195":"code","7503182d":"code","cf74f340":"code","39bd36eb":"code","eaa57c32":"code","784fb0d0":"code","876aed5a":"code","e3efc93b":"code","51ede99e":"code","caa12009":"code","d41d8ba4":"code","3c7a83de":"code","ad197594":"code","8885d258":"code","c0ccc74b":"code","e56fb5f0":"code","79ac4561":"code","fee13e9d":"code","d3348d1a":"code","30e56833":"code","3af2bcf0":"code","4367a8f1":"code","36491d1e":"code","563e6f28":"code","cc896de0":"code","586970d1":"code","77ba7f20":"code","78aac32e":"code","bc80c3ae":"code","27de79e1":"code","12f62c78":"code","e0cdb8f3":"code","a374f4b1":"code","320a84e8":"code","549f3eed":"code","59f6d19c":"markdown","aa2a39bd":"markdown","1c6d107c":"markdown","f5c120f7":"markdown","323b7a79":"markdown","94912ebe":"markdown","b942412d":"markdown","49f4cb9b":"markdown","6dca51a4":"markdown"},"source":{"e2c4c82c":"import collections\nimport os\nimport shutil\nfrom IPython.display import display\nfrom tqdm import tqdm_notebook as tqdm\n\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torchvision","0277ccd6":"# version\ndisplay(torch.__version__)\ndisplay(torch.version.cuda)\ndisplay(torch.backends.cudnn.version())\n# display(torch.cuda.get_device_name(0))","b74dff5d":"# fix the random seed\nSEED = 620402\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)","4f390195":"torch.cuda.is_available()","7503182d":"# set cudnn benchmark\ntorch.backends.cudnn.benchmark = True\n# avoid some wave jitting in the results, we can set\ntorch.backends.cudnn.deterministic = True","cf74f340":"# release some memory in the GPU memory\ntorch.cuda.empty_cache()","39bd36eb":"# or use the command line to reset gpu\n#!nvidia-smi --gpu-reset -i [gpu_id]","eaa57c32":"# basic information in tensor\n# tensor.type()\n# tensor.size()\n# tensor.dim()\n\nx = torch.randn(3, 3)\ndisplay(x)\n\ndisplay(x.type(), x.size(), x.dim())","784fb0d0":"# set default tensor type, \ntorch.set_default_tensor_type(torch.FloatTensor)\n\n# x = x.cuda()\ndisplay(x.type())\nx = x.cpu()\ndisplay(x.type())\nx = x.long()\ndisplay(x.type())\nx = x.float()\ndisplay(x.type())\n\n# a common way for defining the tensor (cpu or gpu)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nx = x.to(device)\ndisplay(x.type())","876aed5a":"# translation between torch.Tensor and np.ndarray\n\n# torch.Tensor --> np.ndarray\nndarray = x.detach().cpu().numpy()\ndisplay(ndarray.dtype, ndarray.shape)\n\n# np.ndarray --> torch.Tensor\ntensor = torch.from_numpy(ndarray).float()\ndisplay(tensor.type())\n# if ndarray has negative stride\ntensor = torch.from_numpy(ndarray.copy()).float()\ndisplay(tensor.type())","e3efc93b":"# torch.Tensor and PIL.Image\n# torch\u3002Tensor --> PIL.Image\nx = torch.rand(3, 128, 128)\nimage = Image.fromarray(torch.clamp(x * 255, min=0, max=255).byte().permute(1, 2, 0).cpu().numpy())\ndisplay(image)\n# using torchvision will be more convinient \nimage = torchvision.transforms.functional.to_pil_image(x)\ndisplay(image)\nimage.save('hello.jpg')\n\n# PIL.Image --> torch.Tensor\n!ls\nimage_tensor = torch.from_numpy(np.asarray(Image.open('hello.jpg'))).permute(2, 0, 1).float()\/255.0\ndisplay(image_tensor.size())\nimage_tensor = torchvision.transforms.functional.to_tensor(Image.open('hello.jpg'))\ndisplay(image_tensor.size())","51ede99e":"# np.ndarray and PIL.Image\n# np.ndarray --> PIL.Image\nndarray = np.random.rand(128, 128, 3) * 255.0\n\nimage = Image.fromarray(ndarray.astype(np.uint8))\ndisplay(image)\n\n# PIL.Image --> np.ndarray\nndarray = np.asarray(image)\ndisplay(ndarray.shape)","caa12009":"# get the value of a tensor with one element\nx = torch.rand(1)\ndisplay(x)\nx_value = x.item()\ndisplay(x_value)","d41d8ba4":"# reshape tensor, which will deal with the tensor contiguously.\ntensor_x = torch.rand(64, 256, 16, 16)\nfc_input = tensor_x.reshape(64, -1)\ndisplay(fc_input.size())","3c7a83de":"# shuffle the data\ntensor_x = torch.rand(64, 128, 16, 16)\ndisplay(tensor_x.size())\ntensor_x_random = tensor_x[torch.randperm(tensor_x.size(0))]\ndisplay(tensor_x_random.size())","ad197594":"tensor = tensor_x[:, :, :, torch.arange(tensor_x.size(3)-1, -1, -1).long()]\ndisplay(tensor.size())","8885d258":"# copy tensor\n# tensor.clone()\n# tensor.detach()\n# tensor.detach.clone()","c0ccc74b":"# concat and stack tensor\ntensor_list = [torch.rand(3, 224, 224) for i in range(16)]\ndisplay(len(tensor_list))\n\n# concat\nconcat_tensor = torch.cat(tensor_list, dim=0)\ndisplay(concat_tensor.size())\nstack_tensor = torch.stack(tensor_list, dim=0)\ndisplay(stack_tensor.size())","e56fb5f0":"# one-hot label, which will be used in the last layer of a model.\nnum_classes = 10\n# The tensor below is the label\ntensor = torch.randint(0, num_classes, size=(64,1))\n# display(tensor)\nN = tensor.size(0)\n\n\none_hot = torch.zeros(N, num_classes).long()\ndisplay(one_hot)\none_hot.scatter_(1, tensor, 1)\ndisplay(one_hot)","79ac4561":"# Get the nonzero or zero elements.\ntensor = torch.randint(0, 2, size=(6, 1))\ndisplay(tensor)\n\ndisplay(torch.nonzero(tensor))\ndisplay(torch.nonzero(tensor==0))\ndisplay(torch.nonzero(tensor).size(0))\ndisplay(torch.nonzero(tensor==0).size(0))","fee13e9d":"tensor_size = (8, 2)\ntensor1 = torch.ones(tensor_size)\ntensor2 = torch.ones(tensor_size)\ntensor3 = torch.zeros(tensor_size)\n\n# tensors equal\n# float tensor\ndisplay(torch.allclose(tensor1, tensor2), torch.allclose(tensor1, tensor3))\n# long tensor\ntensor1 = tensor1.long()\ntensor2 = tensor2.long()\ntensor3 = tensor3.long()\ndisplay(torch.equal(tensor1, tensor2), torch.equal(tensor1, tensor3))","d3348d1a":"# expand tensor\nsize = (64, 512)\ntensor = torch.randn(size)\nreshape_tensor = torch.reshape(tensor, size+(1, 1))\ndisplay(reshape_tensor.size())\nexpand_tensor = reshape_tensor.expand(size + (7, 7))\ndisplay(expand_tensor.size())","30e56833":"# matrix multiplication\n# a matrix A with dimension (m, n) multiply the matrix B with dimension (n, p)\n# (m, n) * (n, p) -> (m, p)\nM = 3\nN = 4\nP = 5\nA = torch.randn(M, N)\nB = torch.randn(N, P)\nC = torch.mm(A, B)\ndisplay(C.size())\n\n# Batch matrix multiplication\nbatch_size = 64\nA = torch.randn(batch_size, M, N)\nB = torch.randn(batch_size, N, P)\nC = torch.bmm(A, B)\ndisplay(C.size())\n\n# element-wise multiplication\nA = torch.randn(M, N)\nB = torch.randn(M, N)\nC = A * B\ndisplay(C.size())","3af2bcf0":"# calculate the distance of A and B\nm = 3\nn = 4\nd = 5\nx = torch.randn(m, d)\ny = torch.randn(n, d)\n\n# the result may have m * n elements. The code can be used in VQ-VAE\nsum_inter = torch.sum((x[:, None, :] - y)**2, dim=2)\ndisplay(sum_inter.size())\ndist = torch.sqrt(sum_inter)\ndisplay(dist.size())","4367a8f1":"# get features from models with pretrained by imagenet\nvgg16 = torchvision.models.vgg16(pretrained=True)\ndisplay(vgg16)\ndisplay('*******************************')\ndisplay(vgg16.features[:-1])\ndisplay('*******************************')\ndisplay(vgg16.features)\ndisplay('*******************************')\ndisplay(vgg16.classifier)","36491d1e":"vgg16.classifier = torch.nn.Sequential(*list(vgg16.classifier.children())[:-3])\ndisplay(vgg16.classifier)","563e6f28":"resnet18 = torchvision.models.resnet18(pretrained=True)\ndisplay(resnet18)\ndisplay('**********************************')\ndisplay(resnet18.layer1)\ndisplay('**********************************')\ndisplay(resnet18.fc)","cc896de0":"list(resnet18.named_children())\n\nresnet18_features = torch.nn.Sequential(collections.OrderedDict(\n    list(resnet18.named_children())[:-1]\n))\nresnet18_features\n\nimage = torch.rand(32, 3, 224, 224)\nwith torch.no_grad():\n    resnet18_features.eval()\n    output_features = resnet18_features(image)\ndisplay(output_features.size())","586970d1":"list(resnet18.named_children())","77ba7f20":"%%time\n# use some layer features\nclass feature_extractor(torch.nn.Module):\n    def __init__(self, pretrained_model, layers_to_extractor):\n        super(feature_extractor, self).__init__()\n        self._model = pretrained_model\n        self._model.eval()\n        self._layers = layers_to_extractor\n    \n    def forward(self, x):\n        with torch.no_grad():\n            conv_representation = []\n            for name, layer in self._model.named_children():\n                x = layer(x)\n                if name in self._layers:\n                    conv_representation.append(x)\n            return conv_representation\n        \n# test the function of feature_extractor\nresnet152 = torchvision.models.resnet152(pretrained=True)\nresnet152_features = torch.nn.Sequential(collections.OrderedDict(list(resnet152.named_children())[:-1]))\nresnet152_features\nlayers = ['layer1', 'layer2', 'layer3', 'layer4']\nfeature_extractor_resnet152 = feature_extractor(pretrained_model=resnet152_features, layers_to_extractor=layers)\n\nimage = torch.rand(32, 3, 224, 224)\nconv_representation = feature_extractor_resnet152(image)\ndisplay(len(conv_representation))\nfor i in range(len(conv_representation)):\n    display(conv_representation[i].size())","78aac32e":"# fine-tuning the network with the last fc layer\nresnet18 = torchvision.models.resnet18(pretrained=True)\n\nfor param in resnet18.parameters():\n    param.required_grad = False\nresnet18.fc = torch.nn.Linear(512, 100)\noptimizer = torch.optim.SGD(resnet18.fc.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)","bc80c3ae":"# fine-tuning the network with the low lr of conv layer and high lr of fc layer.\nresnet18 = torchvision.models.resnet18(pretrained=True)\nfinetuned_parameters = list(map(id, resnet18.fc.parameters()))\ndisplay(finetuned_parameters)\nconv_parameters = (p for p in resnet18.parameters() if id(p) not in finetuned_parameters)\n\nparameters = [{'params':conv_parameters, 'lr':1e-3},\n              {'params':resnet18.fc.parameters()}]\n\noptimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4)","27de79e1":"# the most commom conv layer definition\nin_feature = 3\nout_feature = 16\nconv3 = torch.nn.Conv2d(in_feature, out_feature, 3, 1, 1, bias=True)\nconv1 = torch.nn.Conv2d(in_feature, out_feature, 1, 1, 0, bias=True)\n\nimage = torch.rand(32, 3, 224, 224)\nout_conv3 = conv3(image)\nout_conv1 = conv1(image)\n\ndisplay(out_conv3.size(), out_conv1.size())","12f62c78":"# Global average pooling\ngap = torch.nn.AdaptiveAvgPool2d(output_size = 1)\nout_gap = gap(image)\ndisplay(out_gap.size())","e0cdb8f3":"# bilinear pooling\nN = 32\nD = 64\nH = 112\nW = 112\n\nX = torch.rand(N, D, H, W)\nX = X.reshape(N, D, H*W)\nX = torch.bmm(X, torch.transpose(X, 1, 2)) \/ (H * W)\ndisplay(X.size())\nX = X.reshape(N, D*D)\nX = torch.sign(X) * torch.sqrt(torch.abs(X) + 1e-5)\nX = torch.nn.functional.normalize(X)\ndisplay(X.size())","a374f4b1":"resnet18 = torchvision.models.resnet18(pretrained=True)\nnum_parameters = sum(torch.numel(parameter) for parameter in resnet18.parameters())\ndisplay(num_parameters)","320a84e8":"# Note: model.modules() is different from model.children()\n# common practise for initialization\nfor layer in resnet18.modules():\n    if isinstance(layer, torch.nn.Conv2d):\n        torch.nn.init.kaiming_normal_(layer.weight,  mode='fan_out', nonlinearity='relu')\n        if layer.bias is not None:\n            torch.nn.init.constatn_(layer.bias, val=0.0)\n            \n    if isinstance(layer, torch.nn.BatchNorm2d):\n        torch.nn.init.constant_(layer.weight, val=1.0)\n        torch.nn.init.constant_(layer.bias, val=0.0)\n    \n    if isinstance(layer, torch.nn.Linear):\n        torch.nn.init.xavier_normal_(layer.weight)\n        if layer.bias is not None:\n            torch.nn.init.constant_(layer.bias, val=0.0)","549f3eed":"# label smoothing\nN = 32\nC = 10\nlabels = torch.randint(0, 10, size=(32, 1))\nsmooth_labels = torch.full(size=(N, C), fill_value=0.1\/(C-1))\ndisplay(smooth_labels)\nsmooth_labels.scatter_(1, labels, value=0.9)\ndisplay('*******************************')\ndisplay(smooth_labels)","59f6d19c":"## 1 basis","aa2a39bd":"## 2 tensor","1c6d107c":"## 3 data","f5c120f7":"The script is a example code for pytorch.\n- to remember deeper\n- to get more knowledge\n- for coding in competesion","323b7a79":"## 5 train","94912ebe":"## 4 model","b942412d":"# Content\n- 1 basis\n- 2 tensor\n- 3 data\n- 4 model\n- 5 train","49f4cb9b":"- label smoothing\n- mixup","6dca51a4":"**we may use torch.nn.utils.data.Dataset and DataLoader to get the train and validation data.**"}}