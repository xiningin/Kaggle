{"cell_type":{"f939de05":"code","b6c6094b":"code","d694774a":"code","35c9061c":"code","3894b3ee":"code","8b518a1e":"code","3ad6436e":"code","360dab90":"code","673cceed":"code","59b993f4":"code","0396b092":"code","5c057fa3":"code","39313e73":"code","4be8f0b9":"code","8d30cdcd":"code","6472ea8f":"code","99a8dc9e":"code","8a0bf9cb":"code","6d9b4a0c":"code","66684c1d":"code","46342f86":"code","5a7002ab":"code","76059d89":"code","03715667":"code","46d4dd2e":"code","90c70864":"code","d1b9733a":"code","d083ad17":"code","f7c0bd49":"code","e8a8a343":"code","7380c61a":"code","4a4cd2b1":"code","9b79255c":"code","5ff4b6b7":"code","8d5ab574":"code","48d93e02":"code","d331c7cc":"code","d20f79a9":"code","e1a1c568":"code","2bcf1e43":"code","cd3af342":"code","d32be777":"code","c97e962f":"code","2ae65750":"code","f1afd10d":"code","405da069":"code","34af8372":"code","0efbc811":"code","49c5fd86":"code","58dcf924":"code","0b7f372f":"code","d47c9f9c":"markdown","fce81fb0":"markdown","2177374d":"markdown","2b16af92":"markdown","c06e377c":"markdown","fb3ca3c8":"markdown","6979db09":"markdown","ab11fb3c":"markdown","f5b99643":"markdown","06d58ddd":"markdown","e4520379":"markdown","3c267147":"markdown","151320d1":"markdown","d6deb730":"markdown","b099e762":"markdown","ab01be51":"markdown","ba204b28":"markdown","308f7f5f":"markdown","f1801135":"markdown","dca02868":"markdown","98d05ec4":"markdown","5a7e8075":"markdown","1dbf99df":"markdown","8c60582d":"markdown","9142560a":"markdown","ead9efd8":"markdown","e29965b3":"markdown","f9b6733b":"markdown","8e3d9055":"markdown","55bf9f91":"markdown","7c6c5479":"markdown","f19a0ac8":"markdown","04d7bb58":"markdown","a6551cb1":"markdown","5bd1f5b8":"markdown","88ecd2f1":"markdown","6f4dc553":"markdown","e8c4eca1":"markdown","ac69a0f7":"markdown","eeb873d9":"markdown","1c2ccbf8":"markdown","264a63dc":"markdown","c9a8ea6e":"markdown","67f584a5":"markdown","7f749b3b":"markdown","54caf4ae":"markdown","affd4f4c":"markdown","8cbe2c0a":"markdown","6984a504":"markdown","e19767e6":"markdown","84ac5e20":"markdown","2e076e91":"markdown"},"source":{"f939de05":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b6c6094b":"import seaborn as sns\n\nfrom scipy import stats\nfrom scipy.stats import ttest_ind\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier,GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.linear_model import SGDClassifier,LogisticRegression,LogisticRegressionCV,Perceptron, RidgeClassifier, PassiveAggressiveClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF,DotProduct\n\n\nfrom sklearn.naive_bayes import GaussianNB,BernoulliNB, MultinomialNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis,LinearDiscriminantAnalysis\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.metrics import accuracy_score , recall_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV,RandomizedSearchCV\n\nimport matplotlib.pyplot  as plt\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, classification_report","d694774a":"BT = pd.read_csv(\"..\/input\/SMEMI309-final-evaluation-challenge-2020\/BT_train.csv\", sep=',')\nAT2 = pd.read_csv(\"..\/input\/SMEMI309-final-evaluation-challenge-2020\/AT2_train.csv\", sep=',')\nAT4 = pd.read_csv(\"..\/input\/SMEMI309-final-evaluation-challenge-2020\/AT4_train.csv\", sep=',')","35c9061c":"def t_test(df1,df2,alpha):   # df1,df2 sont deux dataframes\n    difference = 0          # sert a compter le nombre de colonnes utiles \u00e0 la conclusion de notre test\n    \n    for i in df1.columns:\n        res,pvalue = ttest_ind(df1[i],df2[i]) # t_test entre nos colonnes \"i\", la pvalue indique la probabilite que H0 soit vraie\n        \n        if pvalue <= alpha:                 # Rejet de H0 si la proba est inf\u00e9rieure \u00e0 alpha          \n            print(\"The column \",i,\" is significantly different with the chosen alpha, the pvalue is:\",pvalue)\n            difference += 1\n            \n    print(\" \")\n    print(\"The number of significants differents columns is:\",difference)","3894b3ee":"t_test(BT,AT2,0.05)","8b518a1e":"t_test(BT,AT4,0.05)","3ad6436e":"t_test(AT2,AT4,0.05)","360dab90":"plt.matshow(BT.corr())\nplt.colorbar()\nplt.title('Correlation Matrix');\nplt.show()","673cceed":"sns.pairplot(BT)","59b993f4":"plt.matshow(AT2.corr())\nplt.colorbar()\nplt.title('Correlation Matrix');\nplt.show()","0396b092":"sns.pairplot(AT2)","5c057fa3":"plt.matshow(AT4.corr())\nplt.colorbar()\nplt.title('Correlation Matrix');\nplt.show()","39313e73":"sns.pairplot(AT4)","4be8f0b9":"train = pd.read_csv(\"..\/input\/SMEMI309-final-evaluation-challenge-2020\/AT4_train.csv\", sep=',')\ntest = pd.read_csv(\"..\/input\/SMEMI309-final-evaluation-challenge-2020\/AT4_test.csv\", sep=',')","8d30cdcd":"train.describe()","6472ea8f":"x_train,x_test,y_train,y_test= train_test_split(train.drop(['label'],1), train.label, test_size=0.25,random_state=0,shuffle=True)","99a8dc9e":"#On pr\u00e9pare train\ntrain_x=train[[ 'onsc1', 'onsc2', 'onsc3', 'onsc4', 'onsc5', 'onsc6', 'onsc7',\n       'onsc8', 'onsc9', 'onsc10', 'onstf1', 'onstf2', 'onstf3', 'onstf4',\n       'onstf5', 'onstf6', 'onstf7', 'onstf8', 'onstf9', 'onstf10', 'onsto1',\n       'onsto2', 'onsto3', 'onsto4', 'onsto5', 'onsto6', 'onsto7', 'onsto8',\n       'onsto9', 'onsto10']]\ntrain_x = train_x[:960] # 75% de 1280 = 960\ntrain_x.shape\ntrain_y = train['label']\ntrain_y = train_y[:960]\n#On pr\u00e9pare test\ntest_x = train[[ 'onsc1', 'onsc2', 'onsc3', 'onsc4', 'onsc5', 'onsc6', 'onsc7',\n       'onsc8', 'onsc9', 'onsc10', 'onstf1', 'onstf2', 'onstf3', 'onstf4',\n       'onstf5', 'onstf6', 'onstf7', 'onstf8', 'onstf9', 'onstf10', 'onsto1',\n       'onsto2', 'onsto3', 'onsto4', 'onsto5', 'onsto6', 'onsto7', 'onsto8',\n       'onsto9', 'onsto10']]\ntest_x = test_x[960:1280]\nlabels = train['label']\nexpected = labels[960:1280].tolist()","8a0bf9cb":"def fit_train (model, x_train, y_train,x_test,y_test):\n    model.fit(x_train,y_train)\n    predicted = model.predict(x_test)\n    acc=accuracy_score(y_test, predicted)\n    rec=recall_score(y_test, predicted, average= 'weighted')\n    return(acc,rec)\n    \ndef test_model(model,test):\n    predicted=model.predict(test)\n    res = pd.DataFrame({'id':np.arange(len(predicted)), 'label': predicted})\n    return(res)","6d9b4a0c":"clf = [RandomForestClassifier(), AdaBoostClassifier(), BaggingClassifier(), GradientBoostingClassifier(),ExtraTreesClassifier()]\nclf_name = [\"RandomForest\", \"AdaBoost\", \"BaggingClassifier\", \"GradientBoostingClassifier\",\"ExtraTrees\"]\nrecall=[]\naccuracy=[]\nj=0\nfor model in clf:\n    A=fit_train(model,x_train,y_train,x_test,y_test)\n    recall.append(A[1])\n    accuracy.append(A[0])\n    print(\"The accuracy of \", clf_name[j],\" is :\" , A[0],\" and its Recall is : \", A[1])\n    j+=1","66684c1d":"label_graph=[\"b^\",\"gs\",\"rs\",\"cs\",\"ms\"]\nax=plt.figure().add_subplot()\nplt.axis([0,1, 0, 1])\nfor i in range(len(clf)):\n    ax.plot(accuracy[i],recall[i],label_graph[i],label=clf_name[i])\n    ax.legend()\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5,-0.2)) \nplt.xlabel('Accuracy')\nplt.ylabel('Recall')\nplt.suptitle('Cat\u00e9gorie: Ensemble', size = 'x-large')\nplt.grid(True)\nplt.show()","46342f86":"classifiers = [ LogisticRegression(max_iter=10000),Perceptron(),RidgeClassifier(),PassiveAggressiveClassifier()]\nrecall=[]\naccuracy=[]\nfor model in classifiers: \n    A=fit_train(model,x_train,y_train,x_test,y_test)\n    recall.append(A[1])\n    accuracy.append(A[0])\n    print(\"The accuracy of\" , model , \" is :\" , A[0],\"and its recall is : \", A[1])\nlabel_graph=[\"bs\",\"gs\",\"rs\",\"cs\"]\nax=plt.figure().add_subplot()\nfor i in range(len(classifiers)):\n    ax.plot(accuracy[i],recall[i],label_graph[i],label=classifiers[i])\n    ax.legend()\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5,-0.2)) \nplt.xlabel('Accuracy')\nplt.ylabel('Recall')\nplt.suptitle('Cat\u00e9gorie: Mod\u00e8le lin\u00e9aire', size = 'x-large')\nplt.grid(True)\nplt.show()","5a7002ab":"classifiers = [ SVC(),LinearSVC(),NuSVC()]\nlabel_graph=[\"bs\",\"gs\",\"rs\"]\nrecall=[]\naccuracy=[]\nfor model in classifiers: \n    A=fit_train(model,x_train,y_train,x_test,y_test)\n    recall.append(A[1])\n    accuracy.append(A[0])\n    print(\"The accuracy of: \" , model , \" is :\" , A[0],\"and its recall is : \"  , \":\" , A[1])\nax=plt.figure().add_subplot()\nfor i in range(len(classifiers)):\n    ax.plot(accuracy[i],recall[i],label_graph[i],label=classifiers[i])\n    ax.legend()\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5,-0.2)) \nplt.xlabel('Accuracy')\nplt.ylabel('Recall')\nplt.suptitle('Cat\u00e9gorie : Support Vector Machine', size = 'x-large')\nplt.grid(True)\nplt.show()","76059d89":"classifiers = [GaussianNB(),BernoulliNB()]\nlabel_graph=[\"bs\",\"gs\",\"rs\"]\nrecall=[]\naccuracy=[]\nfor model in classifiers: \n    A=fit_train(model,x_train,y_train,x_test,y_test)\n    recall.append(A[1])\n    accuracy.append(A[0])\n    print(\"The accuracy of \" , model , \"is:\" , A[0],\"and its recall is : \"  , \":\" , A[1])\nax=plt.figure().add_subplot()\nfor i in range(len(classifiers)):\n    ax.plot(accuracy[i],recall[i],label_graph[i],label=classifiers[i])\n    ax.legend()\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5,-0.2)) \nplt.xlabel('Accuracy')\nplt.ylabel('Recall')\nplt.suptitle('Cat\u00e9gorie: NaiveBayes', size = 'x-large')\nplt.grid(True)\nplt.show()","03715667":"classifiers = [KNeighborsClassifier(),XGBClassifier(), MLPClassifier(max_iter=10000),DecisionTreeClassifier()]\nrecall=[]\naccuracy=[]\n\nfor model in classifiers: \n    A=fit_train(model,x_train,y_train,x_test,y_test)\n    recall.append(A[1])\n    accuracy.append(A[0])\n    print(\"The accuracy of: \" , model , \"is:\" , A[0],\"and its recall is: \", A[1])\nlabel_graph=[\"bs\",\"gs\",\"rs\",\"cs\"]\nax=plt.figure().add_subplot()\nplt.axis([0.8,1, 0.8, 1])\nfor i in range(len(classifiers)):\n    ax.plot(accuracy[i],recall[i],label_graph[i],label=classifiers[i])\n    ax.legend()\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5,-0.2)) \nplt.xlabel('Accuracy')\nplt.ylabel('Recall')\nplt.title('Mod\u00e8le de Classification')\nplt.grid(True)\nplt.show()","46d4dd2e":"classifiers = [ExtraTreesClassifier(),XGBClassifier(), MLPClassifier(max_iter=10000),GradientBoostingClassifier(),LogisticRegression(max_iter=5000)]\nrecall=[]\naccuracy=[]\nfor model in classifiers: \n    A=fit_train(model,x_train,y_train,x_test,y_test)\n    recall.append(A[1])\n    accuracy.append(A[0])\n    print(\"The accuracy of: \" , model , \"is:\" , A[0],\"and its recall is: \", A[1])\n    test_model(model,test)\nlabel_graph=[\"bs\",\"gs\",\"rs\",\"cs\",\"b^\"]\nax=plt.figure().add_subplot()\nplt.axis([0.8,1, 0.8, 1])\nfor i in range(len(classifiers)):\n    ax.plot(accuracy[i],recall[i],label_graph[i],label=classifiers[i])\n    ax.legend()\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5,-0.2)) \nplt.xlabel('Accuracy')\nplt.ylabel('Recall')\nplt.title('Comparaison des mod\u00e8les entre eux')\nplt.grid(True)\nplt.show()","90c70864":"models=[GradientBoostingClassifier(loss=\"deviance\",n_estimators=100,max_depth=3,random_state=0),\n         GradientBoostingClassifier(loss=\"deviance\",n_estimators=100,max_depth=5,random_state=0),\n        GradientBoostingClassifier(loss=\"deviance\",n_estimators=150,max_depth=3,random_state=0),\n        GradientBoostingClassifier(loss=\"deviance\",n_estimators=350,max_depth=10,random_state=0),\n        GradientBoostingClassifier(loss=\"deviance\",n_estimators=150,max_depth=10,random_state=0),\n        GradientBoostingClassifier(loss=\"deviance\",n_estimators=50,max_depth=3,random_state=0)\n       ]\ntable_acc=[]\ntable_rec=[]\nfor model in models:\n    b=fit_train(model,train_x,train_y,test_x,expected)\n    table_acc.append(b[0])\n    table_rec.append(b[1])\nprint(\"Accuracy : \", table_acc, \"Recall :\", table_rec)","d1b9733a":"MAX_ITER=[500,1000,1500,2000,10000]\nALPHA= [0.01,0.05,0.1,0.2]\nfor i in MAX_ITER:\n    for j in ALPHA: \n        model=MLPClassifier(solver='lbfgs', activation='logistic', alpha=j,random_state=1, max_iter=i,hidden_layer_sizes=[250, 250])\n        A=fit_train(model,x_train,y_train,x_test,y_test)\n        print(\"For alpha =\",j,\" and max_iter =\",i,\". The accuracy is :\",A[0])","d083ad17":"#On pr\u00e9pare les param\u00e8tres qu'on souhaite optimiser\nN_ESTIMATOR= np.arange(50,150,10)\nhyper_para = dict(n_estimators=N_ESTIMATOR)\n\n#On pr\u00e9pare le mod\u00e8le qu'on souhaite optimiser\nmodel = XGBClassifier(max_depth=10,random_state=0)\n\n#On cr\u00e9e notre entrainement \u00e0 partir de la fonction GridSearch.\ngrid = GridSearchCV(estimator=model, param_grid=hyper_para)\nhistory = grid.fit(x_train, y_train)\n\n#Affichage des r\u00e9sultats\nprint(history.best_params_)\nprint(history.best_score_)\n\n#On pr\u00e9pare le mod\u00e8le qu'on souhaite optimiser\nmodel = XGBClassifier(n_estimators=100,random_state=0)\n\n#On cr\u00e9e notre entrainement \u00e0 partir de la fonction GridSearch.\ngrid = GridSearchCV(estimator=model, param_grid=dict(max_depth=np.arange(3,15,2)))\nhistory = grid.fit(x_train, y_train)\n\n#Affichage des r\u00e9sultats\nprint(history.best_params_)\nprint(history.best_score_)","f7c0bd49":"train_df = pd.read_csv(\"..\/input\/SMEMI309-final-evaluation-challenge-2020\/AT4_train.csv\")\ntest_df = pd.read_csv(\"..\/input\/SMEMI309-final-evaluation-challenge-2020\/AT4_test.csv\") \n\n\ntrain_x = train_df.iloc[:, 1:].values     # on recupere les valeurs de train_df sauf le label                        \ntrain_y = train_df.iloc[:, 0].values      # on recupere que le label\ntest_x = test_df.iloc[:, 1:].values\ntest_y = test_df.iloc[:,0].values\n\n#Cross validation\n\nstratified_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, indices in enumerate(stratified_fold.split(train_x, train_y)):\n    \n    X_train_, y_train_ = train_x[indices[0]], train_y[indices[0]]\n    X_test_, y_test_ = train_x[indices[1]], train_y[indices[1]]\n    \n    estimator = MLPClassifier(solver='lbfgs', activation='logistic', alpha=0.05,random_state=1, max_iter=1000,hidden_layer_sizes=[250, 250])\n    estimator.fit(X_train_, y_train_)    \n    predictions = estimator.predict(X_test_) \n    \n    print(f\"Classification report for Fold {fold + 1}:\")\n    print(classification_report(y_test_, predictions, digits=3), end=\"\\n\\n\")\n    \n    print(f\"Confusion Matrix for Fold {fold + 1}:\")\n    print(confusion_matrix(y_test_, predictions), end=\"\\n\\n\")\n    \n    del X_train_\n    del X_test_\n    del y_train_\n    del y_test_\n","e8a8a343":"model=MLPClassifier(solver='lbfgs', activation='logistic', alpha=0.05,random_state=1, max_iter=1000,hidden_layer_sizes=[250, 250])\nA=fit_train(model,x_train,y_train,x_test,y_test)\nprint(\"Accuracy :\", A[0])\nres=test_model(model,test)","7380c61a":"train = pd.read_csv(\"..\/input\/SMEMI309-final-evaluation-challenge-2020\/BT_train.csv\", sep=',')\ntest = pd.read_csv(\"..\/input\/SMEMI309-final-evaluation-challenge-2020\/BT_test.csv\", sep=',')","4a4cd2b1":"x_train,x_test,y_train,y_test= train_test_split(train.drop(['label'],1), train.label, test_size=0.25,random_state=0,shuffle=True)","9b79255c":"#On pr\u00e9pare train\ntrain_x=train[[ 'onsc1', 'onsc2', 'onsc3', 'onsc4', 'onsc5', 'onsc6', 'onsc7',\n       'onsc8', 'onsc9', 'onsc10', 'onstf1', 'onstf2', 'onstf3', 'onstf4',\n       'onstf5', 'onstf6', 'onstf7', 'onstf8', 'onstf9', 'onstf10', 'onsto1',\n       'onsto2', 'onsto3', 'onsto4', 'onsto5', 'onsto6', 'onsto7', 'onsto8',\n       'onsto9', 'onsto10']]\ntrain_x = train_x[:960] # 75% de 1280 = 960\ntrain_x.shape\ntrain_y = train['label']\ntrain_y = train_y[:960]\n#On pr\u00e9pare test\ntest_x = train[[ 'onsc1', 'onsc2', 'onsc3', 'onsc4', 'onsc5', 'onsc6', 'onsc7',\n       'onsc8', 'onsc9', 'onsc10', 'onstf1', 'onstf2', 'onstf3', 'onstf4',\n       'onstf5', 'onstf6', 'onstf7', 'onstf8', 'onstf9', 'onstf10', 'onsto1',\n       'onsto2', 'onsto3', 'onsto4', 'onsto5', 'onsto6', 'onsto7', 'onsto8',\n       'onsto9', 'onsto10']]\ntest_x = test_x[960:1280]\nlabels = train['label']\nexpected = labels[960:1280].tolist()","5ff4b6b7":"def fit_train (model, x_train, y_train,x_test,y_test):\n    model.fit(x_train,y_train)\n    predicted = model.predict(x_test)\n    acc=accuracy_score(y_test, predicted)\n    rec=recall_score(y_test, predicted, average= 'weighted')\n    return(acc,rec)\n    \ndef test_model(model,test):\n    predicted=model.predict(test)\n    res = pd.DataFrame({'id':np.arange(len(predicted)), 'label': predicted})\n    return(res)","8d5ab574":"clf = [RandomForestClassifier(), AdaBoostClassifier(), BaggingClassifier(), GradientBoostingClassifier(),ExtraTreesClassifier()]\nclf_name = [\"RandomForest\", \"AdaBoost\", \"BaggingClassifier\", \"GradientBoostingClassifier\",\"ExtraTrees\"]\nrecall=[]\naccuracy=[]\nj=0\nfor model in clf:\n    A=fit_train(model,x_train,y_train,x_test,y_test)\n    recall.append(A[1])\n    accuracy.append(A[0])\n    print(\"The accuracy of \", clf_name[j],\" is :\" , A[0],\" and its Recall is : \", A[1])\n    j+=1","48d93e02":"label_graph=[\"b^\",\"gs\",\"rs\",\"cs\",\"ms\"]\nax=plt.figure().add_subplot()\nplt.axis([0,1, 0, 1])\nfor i in range(len(clf)):\n    ax.plot(accuracy[i],recall[i],label_graph[i],label=clf_name[i])\n    ax.legend()\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5,-0.2)) \nplt.xlabel('Accuracy')\nplt.ylabel('Recall')\nplt.suptitle('Cat\u00e9gorie: Ensemble', size = 'x-large')\nplt.grid(True)\nplt.show()","d331c7cc":"classifiers = [ LogisticRegression(max_iter=10000),Perceptron(),RidgeClassifier(),PassiveAggressiveClassifier()]\nrecall=[]\naccuracy=[]\nfor model in classifiers: \n    A=fit_train(model,x_train,y_train,x_test,y_test)\n    recall.append(A[1])\n    accuracy.append(A[0])\n    print(\"The accuracy of\" , model , \" is :\" , A[0],\"and its recall is : \", A[1])\nlabel_graph=[\"bs\",\"gs\",\"rs\",\"cs\"]\nax=plt.figure().add_subplot()\nfor i in range(len(classifiers)):\n    ax.plot(accuracy[i],recall[i],label_graph[i],label=classifiers[i])\n    ax.legend()\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5,-0.2)) \nplt.xlabel('Accuracy')\nplt.ylabel('Recall')\nplt.suptitle('Cat\u00e9gorie: Mod\u00e8le lin\u00e9aire', size = 'x-large')\nplt.grid(True)\nplt.show()","d20f79a9":"classifiers = [ SVC(),LinearSVC(),NuSVC()]\nlabel_graph=[\"bs\",\"gs\",\"rs\"]\nrecall=[]\naccuracy=[]\nfor model in classifiers: \n    A=fit_train(model,x_train,y_train,x_test,y_test)\n    recall.append(A[1])\n    accuracy.append(A[0])\n    print(\"The accuracy of: \" , model , \" is :\" , A[0],\"and its recall is : \"  , \":\" , A[1])\nax=plt.figure().add_subplot()\nfor i in range(len(classifiers)):\n    ax.plot(accuracy[i],recall[i],label_graph[i],label=classifiers[i])\n    ax.legend()\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5,-0.2)) \nplt.xlabel('Accuracy')\nplt.ylabel('Recall')\nplt.suptitle('Cat\u00e9gorie : Support Vector Machine', size = 'x-large')\nplt.grid(True)\nplt.show()","e1a1c568":"classifiers = [GaussianNB(),BernoulliNB()]\nlabel_graph=[\"bs\",\"gs\",\"rs\"]\nrecall=[]\naccuracy=[]\nfor model in classifiers: \n    A=fit_train(model,x_train,y_train,x_test,y_test)\n    recall.append(A[1])\n    accuracy.append(A[0])\n    print(\"The accuracy of \" , model , \"is:\" , A[0],\"and its recall is : \"  , \":\" , A[1])\nax=plt.figure().add_subplot()\nfor i in range(len(classifiers)):\n    ax.plot(accuracy[i],recall[i],label_graph[i],label=classifiers[i])\n    ax.legend()\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5,-0.2)) \nplt.xlabel('Accuracy')\nplt.ylabel('Recall')\nplt.suptitle('Cat\u00e9gorie: NaiveBayes', size = 'x-large')\nplt.grid(True)\nplt.show()","2bcf1e43":"classifiers = [KNeighborsClassifier(),XGBClassifier(), MLPClassifier(max_iter=10000),DecisionTreeClassifier()]\nrecall=[]\naccuracy=[]\n\nfor model in classifiers: \n    A=fit_train(model,x_train,y_train,x_test,y_test)\n    recall.append(A[1])\n    accuracy.append(A[0])\n    print(\"The accuracy of: \" , model , \"is:\" , A[0],\"and its recall is: \", A[1])\nlabel_graph=[\"bs\",\"gs\",\"rs\",\"cs\"]\nax=plt.figure().add_subplot()\nplt.axis([0.5,1, 0.5, 1])\nfor i in range(len(classifiers)):\n    ax.plot(accuracy[i],recall[i],label_graph[i],label=classifiers[i])\n    ax.legend()\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5,-0.2)) \nplt.xlabel('Accuracy')\nplt.ylabel('Recall')\nplt.title('Mod\u00e8le de Classification')\nplt.grid(True)\nplt.show()","cd3af342":"classifiers = [RandomForestClassifier(),MLPClassifier(max_iter=10000),RidgeClassifier(),NuSVC()]\nrecall=[]\naccuracy=[]\nfor model in classifiers: \n    A=fit_train(model,x_train,y_train,x_test,y_test)\n    recall.append(A[1])\n    accuracy.append(A[0])\n    print(\"The accuracy of: \" , model , \"is:\" , A[0],\"and its recall is: \", A[1])\n    test_model(model,test)\nlabel_graph=[\"bs\",\"gs\",\"rs\",\"cs\",\"b^\"]\nax=plt.figure().add_subplot()\nplt.axis([0.5,1, 0.5, 1])\nfor i in range(len(classifiers)):\n    ax.plot(accuracy[i],recall[i],label_graph[i],label=classifiers[i])\n    ax.legend()\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5,-0.2)) \nplt.xlabel('Accuracy')\nplt.ylabel('Recall')\nplt.title('Comparaison des mod\u00e8les entre eux')\nplt.grid(True)\nplt.show()","d32be777":"MAX_ITER=[500,1000,1500,2000,10000]\nALPHA= [0.01,0.05,0.1,0.2]\nmaxi = 0 \ni1 = 0\ni2 = 0\nfor i in MAX_ITER:\n    for j in ALPHA: \n        model=MLPClassifier(solver='lbfgs', activation='logistic', alpha=j,random_state=1, max_iter=i,hidden_layer_sizes=[250, 250])\n        A=fit_train(model,x_train,y_train,x_test,y_test)\n        print(\"For alpha =\",j,\" and max_iter =\",i,\". The accuracy is :\",A[0])\n        if A[0] > maxi:\n            maxi = A[0]\n            i1 = j\n            i2 = i\n            \nprint(\"The maximum accuracy is:\",maxi, \"with alpha =\",i1,\"and max_iter =\", i2)","c97e962f":"\n\nstratified_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\ngrid_params = {\n    \"kernel\": ['linear', 'poly','rbf','sigmoid'],\n    \"nu\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n}\nestimator = NuSVC()\ngrid_estimator = GridSearchCV(estimator, # Base estimator\n                              grid_params, # Parameters to tune\n                              cv=stratified_fold, # cross-validation strategy\n                              verbose=2, # Verbosity of the logs\n                              n_jobs=-1) # Number of jobs to be run concurrently with -1 meaning all the processors\n\n\n\n# Fitting the estimator with training data\ngrid_estimator.fit(x_train, y_train)\n\nprint(f\"Best Score: {grid_estimator.best_score_}\", end=\"\\n\\n\")\nprint(\"Grid Search CV results:\")\nresults_df = pd.DataFrame(grid_estimator.cv_results_)\nresults_df","2ae65750":"stratified_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\ngrid_params = {\n    \"n_estimators\": np.arange(50,150,10),\n    \"criterion\": ['gini','entropy'],\n    \"max_depth\" : np.arange(50,100,10)\n}\nestimator = RandomForestClassifier()\ngrid_estimator = GridSearchCV(estimator, # Base estimator\n                              grid_params, # Parameters to tune\n                              cv=stratified_fold, # cross-validation strategy\n                              verbose=2, # Verbosity of the logs\n                              n_jobs=-1) # Number of jobs to be run concurrently with -1 meaning all the processors\n\n\n\n# Fitting the estimator with training data\ngrid_estimator.fit(x_train, y_train)\n\nprint(f\"Best Score: {grid_estimator.best_score_}\", end=\"\\n\\n\")\nprint(\"Grid Search CV results:\")\nresults_df = pd.DataFrame(grid_estimator.cv_results_)\nresults_df","f1afd10d":"model=MLPClassifier(solver='lbfgs', activation='logistic', alpha=0.2,random_state=1, max_iter=2000,hidden_layer_sizes=[250, 250])\nA=fit_train(model,x_train,y_train,x_test,y_test)\nprint(A)\nres2=test_model(model,test)","405da069":"res2.to_csv('samplesubmissionBT.csv',header=True, sep=',', index=False)","34af8372":"train_df = pd.read_csv(\"..\/input\/my-datas\/SMEMI309-final-evaluation-challenge-2020\/BT_train.csv\", sep=',')\ntest_df = pd.read_csv(\"..\/input\/my-datas\/SMEMI309-final-evaluation-challenge-2020\/BT_test.csv\", sep=',')\n\n\n\ntrain_x = train_df.iloc[:, 1:].values     # on recupere les valeurs de train_df sauf le label                        \ntrain_y = train_df.iloc[:, 0].values      # on recupere que le label\ntest_x = test_df.iloc[:, 1:].values\ntest_y = test_df.iloc[:,0].values\n\n#Cross validation\n\nstratified_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, indices in enumerate(stratified_fold.split(train_x, train_y)):\n    \n    X_train_, y_train_ = train_x[indices[0]], train_y[indices[0]]\n    X_test_, y_test_ = train_x[indices[1]], train_y[indices[1]]\n    \n    estimator = MLPClassifier(solver='lbfgs', activation='logistic', alpha=0.05,random_state=1, max_iter=1000,hidden_layer_sizes=[250, 250])\n    estimator.fit(X_train_, y_train_)    \n    predictions = estimator.predict(X_test_) \n    \n    print(f\"Classification report for Fold {fold + 1}:\")\n    print(classification_report(y_test_, predictions, digits=3), end=\"\\n\\n\")\n    \n    print(f\"Confusion Matrix for Fold {fold + 1}:\")\n    print(confusion_matrix(y_test_, predictions), end=\"\\n\\n\")\n    \n    del X_train_\n    del X_test_\n    del y_train_\n    del y_test_\n\n","0efbc811":"AT2_train = pd.read_csv(\"..\/input\/SMEMI309-final-evaluation-challenge-2020\/AT2_train.csv\", sep=',')\nAT2_test = pd.read_csv(\"..\/input\/SMEMI309-final-evaluation-challenge-2020\/AT2_test.csv\", sep=',')\nAT4_train = pd.read_csv(\"..\/input\/SMEMI309-final-evaluation-challenge-2020\/AT4_train.csv\", sep=',')\nAT4_test = pd.read_csv(\"..\/input\/SMEMI309-final-evaluation-challenge-2020\/AT4_test.csv\", sep=',')","49c5fd86":"x2_train,x2_test,y2_train,y2_test = train_test_split(AT2_train.drop(['label'],1), AT2_train.label, test_size=0.25,random_state=0,shuffle=True)\nx4_train,x4_test,y4_train,y4_test = train_test_split(AT4_train.drop(['label'],1), AT4_train.label, test_size=0.25,random_state=0,shuffle=True)","58dcf924":"model = MLPClassifier(solver='lbfgs', activation='logistic', alpha=0.05,random_state=1, max_iter=1000,hidden_layer_sizes=[250, 250])\nA = fit_train(model,x2_train,y2_train,x2_test,y2_test)\nprint(\"Accuracy of AT2 :\" ,A[0])\n\nA2 = fit_train(model,x4_train,y4_train,x4_test,y4_test)\nprint(\"Accuracy of AT4 :\" ,A2[0])\n","0b7f372f":"res.to_csv('samplesubmission.csv',header=True, sep=',', index=False)","d47c9f9c":"Le mod\u00e8le semble adapt\u00e9. Nous pouvons donc le soumettre \u00e0 la comp\u00e9tition.","fce81fb0":"### 4. Cat\u00e9gorie Naive Bayes : GaussianNB, BernouilliNB","2177374d":"## C. Etude des mod\u00e8les par cat\u00e9gorie\n### 1. Cat\u00e9gorie Ensemble : RandomForest, AdaBoost, Bagging, GradientBoosting, Extratrees","2b16af92":"# E - Conclusion et soummission des r\u00e9sultats","c06e377c":"### 2. Fonction utile pour l'analyse : fit_train & test_model\n#### **Ces deux fonctions vont nous permettre d'\u00e9viter des lignes de codes.**","fb3ca3c8":"\n# -Projet Prisca BAHI & Maxime Larcanch\u00e9-\n-----------------","6979db09":"On retiendra ainsi:  :\n\n* Random Forest\n* Ridge classifier\n* NUSVC\n* MLPC","ab11fb3c":"# Enregistrement des pr\u00e9dictions","f5b99643":"-----------------\n# I. **Etude statistique : t-test**","06d58ddd":"# D - Optimisations","e4520379":"## A. **Importation des donn\u00e9es pour l'apprentissage et le test**\n","3c267147":"-----------------\n# **0. Importation des librairies n\u00e9c\u00e9ssaires \u00e0 l'\u00e9tude et \u00e9tude statistique**","151320d1":"# 5. Comparaison des mod\u00e8les bonus","d6deb730":"## B - Etude des mod\u00e8les par cat\u00e9gories","b099e762":"### 6. Conclusion.\nApr\u00e8s l'affichage des graphiques pour les diff\u00e9rentes cat\u00e9gories, on recup\u00e8re les mod\u00e8les poss\u00e9dant une valeur d'accuracy et de recall proche de 1, c'est \u00e0 dire ceux dont le point sur la figure est proche du coin en haut \u00e0 droite.\nDans certains cas, on observe des valeurs \u00e9gales. Il est important de r\u00e9cuperer les deux mod\u00e8les pour les optimiser. \\\nLes heureux \u00e9lus sont donc :\n           * ExtraTressClassifier\n           * XGBClassifier\n           * MLPClassifier\n           * GradientBoostingClassifier\n           * LogisticRegression","ab01be51":"Tous ces r\u00e9sultats montrent que nous pouvons rejeter H0 presque partout et ainsi conclure que les donn\u00e9es sont diff\u00e9rentes. En d'autres termes, l'apprentissage SNN est effectif.","ba204b28":"## E. Validation croisee","308f7f5f":"Dans cette partie, nous allons reprendre la m\u00e9thode pr\u00e9c\u00e9dente pour entrainer le dataset BT_train","f1801135":"### 2. Cr\u00e9ation des donn\u00e9es train et test","dca02868":"-----------------\n# I. Etude d'un mod\u00e8le de pr\u00e9diction des classes pour AT4","98d05ec4":"# C. Etudes des mod\u00e8les entre eux","5a7e8075":"# 1 - MLP","1dbf99df":"### En optimisant les derniers mod\u00e8les, on remarque que MLPClassfier poss\u00e8de de meilleurs r\u00e9sultats. On conservera ainsi ce mod\u00e8le pour notre \u00e9tude et nous l'appliquerons sur test.","8c60582d":"# 3 - Random Forest","9142560a":"On voit bien que les corr\u00e9lations sont diff\u00e9rentes et que par cons\u00e9quent les dataframes diff\u00e8rent. SUrtout entre BT et les AT.","ead9efd8":"# La meilleure accuracy est de 73%, pour MLP avec alpha = 0.2 and max_iter = 2000","e29965b3":"## 1.  **Matrices de corr\u00e9lations**","f9b6733b":"## B. M\u00e9thode pour le choix du mod\u00e8le pour notre \u00e9tude.\n### 1 . Processus d'analyse\n1. **Etude des mod\u00e8les par cat\u00e9gorie** :\n     * pour chaque cat\u00e9gorie, on \u00e9tudie les mod\u00e8les avec les param\u00e8tres par d\u00e9faut pour \u00e9liminer les mod\u00e8les moins performants. \n             Ensemble : RandomForest AdaBoost etc..\n             Neighbors\n             SVM \n             Neural Network\n             Naive Bayes\n             Gaussian Process\n2. **Etude des mod\u00e8les entre eux**:\n    * on compare les mod\u00e8les, toujours avec les param\u00e8tres par d\u00e9faut, pour faire ressortir les mod\u00e8les les plus adapt\u00e9s. \n3. **Etude des mod\u00e8les s\u00e9lectionn\u00e9s** :\n    * on optimisera les param\u00e8tres pour chaque mod\u00e8le choisi, soit avec l'aide de la foncton GridSearchCV soit manuellement avec une boucle (pour \u00e9viter un temps d'ex\u00e9cution long pour certains mod\u00e8les) pour conserver les mod\u00e8les avec les param\u00e8tres les plus performants.\n4. **Comparaison des mod\u00e8les optimaux** : \n    * \u00e0 la suite de cette comparaison, on aura trouv\u00e9 le mod\u00e8le le plus adapt\u00e9 \u00e0 notre \u00e9tude. \n5. **Conclusion**\n\n**NB : seule la premi\u00e8re cat\u00e9gorie sera d\u00e9taill\u00e9e.**\n","8e3d9055":"## F. Conclusion","55bf9f91":"### 2. Cat\u00e9gorie mod\u00e8le lin\u00e9aire : LogisticRegression,Perceptron, RigdeClassifier, PassiveAggressiveClassifier","7c6c5479":"### Optimisation du mod\u00e8le MLP","f19a0ac8":"# 2 - NuSVC","04d7bb58":"# II. Train on BT_train","a6551cb1":"### 3. Cat\u00e9gorie SupportVectorMachine","5bd1f5b8":"### On fait de m\u00eame avec le mod\u00e8le XGBoost","88ecd2f1":"# 3. Cat\u00e9gorie SupportVectorMachine","6f4dc553":"# **4. Cat\u00e9gorie Naive Bayes : GaussianNB, BernouilliNB**","e8c4eca1":"### 1. Visualisation des donn\u00e9es","ac69a0f7":"**Conclusion:** les meilleurs sont dans l'ordre:  MLP, Random Forest et NuSVC","eeb873d9":"# F - Validation crois\u00e9e","1c2ccbf8":"Nous allons appliquer **un t-test** entre **chacune des colonnes** de nos dataframes. On pose:\n\n* Alternative hypothesis (H1) : Les donn\u00e9es sont diff\u00e9rentes\n* Null hypothesis (H0) : Les donn\u00e9es sont identiques\n\nL'objectif est de montrer que nous pouvons **rejeter H0**\n\nNous fixerons par la suite alpha = 5%","264a63dc":"Premi\u00e8rement, il convient d'optimiser le mod\u00e8le gradient boosting classifier. \nNous allons en tester 6 choisis par nos soins.\nOn aurait pu appliquer ici la fonction GridSearchCV. Cependant,apr\u00e8s avoir essay\u00e9 \u00e0 plusieurs reprises, le temps demand\u00e9 pour cette fonction \u00e9tait trop long.","c9a8ea6e":"# 2. Cat\u00e9gorie mod\u00e8le lin\u00e9aire : LogisticRegression,Perceptron, RigdeClassifier, PassiveAggressiveClassifier","67f584a5":"## C. Etudes des mod\u00e8les entre eux","7f749b3b":"### 5. Comparaison des mod\u00e8les bonus","54caf4ae":"## A - Pr\u00e9paration des ensembles de tests et d'apprentissages","affd4f4c":" -------------------------------\n # III. Continous Learning Network","8cbe2c0a":"### Conclusion :\n#### Graphiquement, on remarque que les classifieurs MLP, GradientBoosting et XGBoost sont les classifiers les plus performants sans \u00eatre optimis\u00e9s. On retient ces derniers afin de pouvoir les optimier et les comparer entre eux.","6984a504":"Pour aller plus loin dans l'analyse, observons si les corr\u00e9lations entre les variables diff\u00e8rent ou non.","e19767e6":"\n## D. Optimisation des param\u00e8tres pour chaque mod\u00e8le.","84ac5e20":"Avant de remettre notre syst\u00e8me de pr\u00e9diction, il convient de l'\u00e9valuer. Nous avons choisi la m\u00e9thode de validation crois\u00e9ee pour ce faire.","2e076e91":"# 1. Cat\u00e9gorie Ensemble : RandomForest, AdaBoost, Bagging, GradientBoosting, Extratree"}}