{"cell_type":{"671ffc39":"code","e2db6138":"code","8ba5b4e9":"code","53df78dc":"code","122a7e3e":"code","517d9f4c":"code","906d520b":"code","dcda4be8":"code","fc9ffe84":"code","1328f19d":"code","27d69bcb":"code","6d4d9fe4":"code","eb67adf5":"code","548ee009":"code","33409336":"markdown"},"source":{"671ffc39":"import pandas as pd\nimport numpy as np\nimport sys\nfrom itertools import combinations, groupby\nfrom collections import Counter\nfrom IPython.display import display\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Function that returns the size of an object in MB\ndef size(obj):\n    return \"{0:.2f} MB\".format(sys.getsizeof(obj) \/ (1000 * 1000))\n\norders = pd.read_csv('..\/input\/dataset_group.csv')\nprint('orders -- dimensions: {0};   size: {1}'.format(orders.shape, size(orders)))\ndisplay(orders.head())\n# Any results you write to the current directory are save","e2db6138":"# Convert from DataFrame to a Series, with order_id as index and item_id as value\norders.columns= ['datum','Transaction','Item']\norders = orders.set_index('Transaction')['Item'].rename('item_id')\ndisplay(orders.head(10))\ntype(orders)","8ba5b4e9":"print('dimensions: {0};   size: {1};   unique_orders: {2};   unique_items: {3}'\n      .format(orders.shape, size(orders), len(orders.index.unique()), len(orders.value_counts())))","53df78dc":"# Returns frequency counts for items and item pairs\ndef freq(iterable):\n    if type(iterable) == pd.core.series.Series:\n        return iterable.value_counts().rename(\"freq\")\n    else: \n        return pd.Series(Counter(iterable)).rename(\"freq\")\n\n    \n# Returns number of unique orders\ndef order_count(order_item):\n    return len(set(order_item.index))\n\n\n# Returns generator that yields item pairs, one at a time\ndef get_item_pairs(order_item):\n    order_item = order_item.reset_index().as_matrix()\n    for order_id, order_object in groupby(order_item, lambda x: x[0]):\n        item_list = [item[1] for item in order_object]\n              \n        for item_pair in combinations(item_list, 2):\n            yield item_pair\n            \n\n# Returns frequency and support associated with item\ndef merge_item_stats(item_pairs, item_stats):\n    return (item_pairs\n                .merge(item_stats.rename(columns={'freq': 'freqA', 'support': 'supportA'}), left_on='item_A', right_index=True)\n                .merge(item_stats.rename(columns={'freq': 'freqB', 'support': 'supportB'}), left_on='item_B', right_index=True))\n\n\n# Returns name associated with item\ndef merge_item_name(rules, item_name):\n    columns = ['itemA','itemB','freqAB','supportAB','freqA','supportA','freqB','supportB', \n               'confidenceAtoB','confidenceBtoA','lift']\n    rules = (rules\n                .merge(item_name.rename(columns={'item_name': 'itemA'}), left_on='item_A', right_on='item_id')\n                .merge(item_name.rename(columns={'item_name': 'itemB'}), left_on='item_B', right_on='item_id'))\n    return rules[columns]   ","122a7e3e":"def association_rules(order_item, min_support):\n\n    print(\"Starting order_item: {:22d}\".format(len(order_item)))\n\n\n    # Calculate item frequency and support\n    item_stats             = freq(order_item).to_frame(\"freq\")\n    item_stats['support']  = item_stats['freq'] \/ order_count(order_item) * 100\n\n\n    # Filter from order_item items below min support \n    qualifying_items       = item_stats[item_stats['support'] >= min_support].index\n    order_item             = order_item[order_item.isin(qualifying_items)]\n\n    print(\"Items with support >= {}: {:15d}\".format(min_support, len(qualifying_items)))\n    print(\"Remaining order_item: {:21d}\".format(len(order_item)))\n\n\n    # Filter from order_item orders with less than 2 items\n    order_size             = freq(order_item.index)\n    qualifying_orders      = order_size[order_size >= 2].index\n    order_item             = order_item[order_item.index.isin(qualifying_orders)]\n\n    print(\"Remaining orders with 2+ items: {:11d}\".format(len(qualifying_orders)))\n    print(\"Remaining order_item: {:21d}\".format(len(order_item)))\n\n\n    # Recalculate item frequency and support\n    item_stats             = freq(order_item).to_frame(\"freq\")\n    item_stats['support']  = item_stats['freq'] \/ order_count(order_item) * 100\n\n\n    # Get item pairs generator\n    item_pair_gen          = get_item_pairs(order_item)\n\n\n    # Calculate item pair frequency and support\n    item_pairs              = freq(item_pair_gen).to_frame(\"freqAB\")\n    item_pairs['supportAB'] = item_pairs['freqAB'] \/ len(qualifying_orders) * 100\n\n    print(\"Item pairs: {:31d}\".format(len(item_pairs)))\n\n\n    # Filter from item_pairs those below min support\n    item_pairs              = item_pairs[item_pairs['supportAB'] >= min_support]\n\n    print(\"Item pairs with support >= {}: {:10d}\\n\".format(min_support, len(item_pairs)))\n\n\n    # Create table of association rules and compute relevant metrics\n    item_pairs = item_pairs.reset_index().rename(columns={'level_0': 'item_A', 'level_1': 'item_B'})\n    item_pairs = merge_item_stats(item_pairs, item_stats)\n    \n    item_pairs['confidenceAtoB'] = item_pairs['supportAB'] \/ item_pairs['supportA']\n    item_pairs['confidenceBtoA'] = item_pairs['supportAB'] \/ item_pairs['supportB']\n    item_pairs['lift']           = item_pairs['supportAB'] \/ (item_pairs['supportA'] * item_pairs['supportB'])\n    \n    \n    # Return association rules sorted by lift in descending order\n    return item_pairs.sort_values('lift', ascending=False)","517d9f4c":"%%time\nrules = association_rules(orders, 0.3)  ","906d520b":"rules.sort_values('supportAB', ascending=False)","dcda4be8":"orders","fc9ffe84":"orders=orders.reset_index()\norders","1328f19d":"from mlxtend.frequent_patterns import apriori # Data pattern exploration\nfrom mlxtend.frequent_patterns import association_rules # Association rules conversion\nfrom mlxtend.preprocessing import OnehotTransactions # Transforming dataframe for apriori\n\n#basket_sets = \norders['teller']=1.0\nbasket_sets=pd.pivot_table(orders, columns='item_id', index='Transaction', values='teller')\nbasket_sets","27d69bcb":"# Apriori aplication: frequent_itemsets\n# Note that min_support parameter was set to a very low value, this is the Spurious limitation, more on conclusion section\nfrequent_itemsets = apriori(basket_sets.fillna(0), min_support=0.053, use_colnames=True)\nfrequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n\n# Advanced and strategical data frequent set selection\nfrequent_itemsets[ (frequent_itemsets['length'] > 2) &\n                   (frequent_itemsets['support'] >= 0.12) ].head()","6d4d9fe4":"# Generating the association_rules: rules\n# Selecting the important parameters for analysis\nrules2 = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\nrules2[['antecedents', 'consequents', 'support', 'confidence', 'lift']].sort_values('support', ascending=False).head(100)","eb67adf5":"rules2.plot.scatter(x='antecedent support',y='consequent support',c='lift',colormap='viridis')","548ee009":"rules2.sort_values('conviction', ascending=False)\n\n","33409336":"https:\/\/paginas.fe.up.pt\/~ec\/files_1112\/week_04_Association.pdf\n\nPS (or Leverage):\n\uf06e is the proportion of additional elements covered by both the\npremise and consequence above the expected if independent.\n\nConviction\n\uf06e conviction of X=>Y can be interpreted as the\n\uf06e ratio of the expected frequency that X occurs without Y (that is to\nsay, the frequency that the rule makes an incorrect prediction) if\nX and Y were independent\n\uf06e divided by the observed frequency of incorrect predictions.\n\uf06e A conviction value of 1.2 shows that the rule would be incorrect\n20% more often (1.2 times as often) if the association between X\nand Y was purely random chance."}}