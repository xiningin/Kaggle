{"cell_type":{"4bf9aab5":"code","be8e7ce6":"code","2c300d80":"code","6cb789e0":"code","5edb8549":"code","fbaf64fb":"code","6dde583f":"code","4b809179":"code","e92c8e34":"code","544205f3":"code","27122312":"code","14918c24":"code","955bb608":"code","4c078976":"code","def1bb2d":"code","4f524f0e":"code","998d6887":"code","eb3c540d":"code","115dca87":"code","e63ed3e8":"code","c66d4897":"code","be7f8d8f":"code","5ad2789c":"code","132559d0":"code","11f97780":"code","2f447355":"code","850b2438":"code","ee34a5d1":"markdown","5d9b3860":"markdown","72c6a3cf":"markdown","4d54f202":"markdown","59e3af73":"markdown","2dcb9776":"markdown","ec9a3ef7":"markdown","a865129c":"markdown","37d522e5":"markdown","64c7b686":"markdown","a66a718a":"markdown","cea86b65":"markdown","3d93270a":"markdown","b9f78d37":"markdown"},"source":{"4bf9aab5":"import os\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimport xml.etree.ElementTree as ET\n\nimport numpy as np\nimport pandas as pd\n\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom PIL import Image\nfrom imgaug import augmenters as iaa\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom keras.utils import *\nfrom keras.callbacks import *\n\nfrom keras.applications.densenet import DenseNet121, preprocess_input","be8e7ce6":"breed_list = os.listdir(\"..\/input\/stanford-dogs-dataset\/images\/Images\/\")\n\nnum_classes = len(breed_list)\nprint(\"{} breeds\".format(num_classes))\n\nn_total_images = 0\nfor breed in breed_list:\n    n_total_images += len(os.listdir(\"..\/input\/stanford-dogs-dataset\/images\/Images\/{}\".format(breed)))\nprint(\"{} images\".format(n_total_images))","2c300d80":"label_maps = {}\nlabel_maps_rev = {}\nfor i, v in enumerate(breed_list):\n    label_maps.update({v: i})\n    label_maps_rev.update({i : v})","6cb789e0":"def show_dir_images(breed, n_to_show):\n    plt.figure(figsize=(16,16))\n    img_dir = \"..\/input\/stanford-dogs-dataset\/images\/Images\/{}\/\".format(breed)\n    images = os.listdir(img_dir)[:n_to_show]\n    for i in range(n_to_show):\n        img = mpimg.imread(img_dir + images[i])\n        plt.subplot(n_to_show\/4+1, 4, i+1)\n        plt.imshow(img)\n        plt.axis('off')","5edb8549":"print(breed_list[0])\nshow_dir_images(breed_list[0], 16)","fbaf64fb":"%%time\n\n# copy from https:\/\/www.kaggle.com\/gabrielloye\/dogs-inception-pytorch-implementation\n# reduce the background noise\n\nos.mkdir('data')\nfor breed in breed_list:\n    os.mkdir('data\/' + breed)\nprint('Created {} folders to store cropped images of the different breeds.'.format(len(os.listdir('data'))))\n\nfor breed in os.listdir('data'):\n    for file in os.listdir('..\/input\/stanford-dogs-dataset\/annotations\/Annotation\/{}'.format(breed)):\n        img = Image.open('..\/input\/stanford-dogs-dataset\/images\/Images\/{}\/{}.jpg'.format(breed, file))\n        tree = ET.parse('..\/input\/stanford-dogs-dataset\/annotations\/Annotation\/{}\/{}'.format(breed, file))\n        xmin = int(tree.getroot().findall('object')[0].find('bndbox').find('xmin').text)\n        xmax = int(tree.getroot().findall('object')[0].find('bndbox').find('xmax').text)\n        ymin = int(tree.getroot().findall('object')[0].find('bndbox').find('ymin').text)\n        ymax = int(tree.getroot().findall('object')[0].find('bndbox').find('ymax').text)\n        img = img.crop((xmin, ymin, xmax, ymax))\n        img = img.convert('RGB')\n        img = img.resize((224, 224))\n        img.save('data\/' + breed + '\/' + file + '.jpg')","6dde583f":"def paths_and_labels():\n    paths = list()\n    labels = list()\n    targets = list()\n    for breed in breed_list:\n        base_name = \".\/data\/{}\/\".format(breed)\n        for img_name in os.listdir(base_name):\n            paths.append(base_name + img_name)\n            labels.append(breed)\n            targets.append(label_maps[breed])\n    return paths, labels, targets\n\npaths, labels, targets = paths_and_labels()\n\nassert len(paths) == len(labels)\nassert len(paths) == len(targets)\n\ntargets = np_utils.to_categorical(targets, num_classes=num_classes)","4b809179":"batch_size = 64\n\nclass ImageGenerator(Sequence):\n    \n    def __init__(self, paths, targets, batch_size, shape, augment=False):\n        self.paths = paths\n        self.targets = targets\n        self.batch_size = batch_size\n        self.shape = shape\n        self.augment = augment\n        \n    def __len__(self):\n        return int(np.ceil(len(self.paths) \/ float(self.batch_size)))\n    \n    def __getitem__(self, idx):\n        batch_paths = self.paths[idx * self.batch_size : (idx + 1) * self.batch_size]\n        x = np.zeros((len(batch_paths), self.shape[0], self.shape[1], self.shape[2]), dtype=np.float32)\n        y = np.zeros((self.batch_size, num_classes, 1))\n        for i, path in enumerate(batch_paths):\n            x[i] = self.__load_image(path)\n        y = self.targets[idx * self.batch_size : (idx + 1) * self.batch_size]\n        return x, y\n    \n    def __iter__(self):\n        for item in (self[i] for i in range(len(self))):\n            yield item\n            \n    def __load_image(self, path):\n        image = imread(path)\n        image = preprocess_input(image)\n        if self.augment:\n            seq = iaa.Sequential([\n                iaa.OneOf([\n                    iaa.Fliplr(0.5),\n                    iaa.Flipud(0.5),\n                    iaa.CropAndPad(percent=(-0.25, 0.25)),\n                    iaa.Crop(percent=(0, 0.1)),\n                    iaa.Sometimes(0.5,\n                        iaa.GaussianBlur(sigma=(0, 0.5))\n                    ),\n                    iaa.Affine(\n                        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n                        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n                        rotate=(-180, 180),\n                        shear=(-8, 8)\n                    )\n                ])\n            ], random_order=True)\n            image = seq.augment_image(image)\n        return image","e92c8e34":"train_paths, val_paths, train_targets, val_targets = train_test_split(paths, \n                                                  targets,\n                                                  test_size=0.15, \n                                                  random_state=1029)\n\ntrain_gen = ImageGenerator(train_paths, train_targets, batch_size=32, shape=(224,224,3), augment=True)\nval_gen = ImageGenerator(val_paths, val_targets, batch_size=32, shape=(224,224,3), augment=False)","544205f3":"inp = Input((224, 224, 3))\nbackbone = DenseNet121(input_tensor=inp,\n                       weights=\"..\/input\/densenet-keras\/DenseNet-BC-121-32-no-top.h5\",\n                       include_top=False)\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(1024, activation=\"relu\")(x)\nx = Dropout(0.5)(x)\nx = Dense(512, activation=\"relu\")(x)\nx = Dropout(0.5)(x)\noutp = Dense(num_classes, activation=\"softmax\")(x)\n\nmodel = Model(inp, outp)","27122312":"for layer in model.layers[:-6]:\n    layer.trainable = False","14918c24":"model.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"acc\"])","955bb608":"history = model.fit_generator(generator=train_gen, \n                              steps_per_epoch=len(train_gen), \n                              validation_data=val_gen, \n                              validation_steps=len(val_gen),\n                              epochs=20)","4c078976":"plt.rcParams['figure.figsize'] = (6,6)\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\n\nplt.legend()\n\nplt.show()","def1bb2d":"for layer in model.layers[:]:\n    layer.trainable = True","4f524f0e":"# a check point callback to save our best weights\ncheckpoint = ModelCheckpoint('dog_breed_classifier_model.h5', \n                             monitor='val_acc', \n                             verbose=1, \n                             save_best_only=True, \n                             mode='max', \n                             save_weights_only=True)\n\n# a reducing lr callback to reduce lr when val_loss doesn't increase\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                                   patience=1, verbose=1, mode='min',\n                                   min_delta=0.0001, cooldown=2, min_lr=1e-7)\n\n# for early stop\nearly_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)","998d6887":"history = model.fit_generator(generator=train_gen, \n                              steps_per_epoch=len(train_gen), \n                              validation_data=val_gen, \n                              validation_steps=len(val_gen),\n                              epochs=20,\n                              callbacks=[checkpoint, reduce_lr, early_stop])","eb3c540d":"plt.rcParams['figure.figsize'] = (6,6)\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\n\nplt.figure()\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\n\nplt.legend()\n\nplt.show()","115dca87":"print(max(val_acc))","e63ed3e8":"def download_and_predict(url, filename):\n    # download and save\n    os.system(\"curl -s {} -o {}\".format(url, filename))\n    img = Image.open(filename)\n    img = img.convert('RGB')\n    img = img.resize((224, 224))\n    img.save(filename)\n    # show image\n    plt.figure(figsize=(4, 4))\n    plt.imshow(img)\n    plt.axis('off')\n    # predict\n    img = imread(filename)\n    img = preprocess_input(img)\n    probs = model.predict(np.expand_dims(img, axis=0))\n    for idx in probs.argsort()[0][::-1][:5]:\n        print(\"{:.2f}%\".format(probs[0][idx]*100), \"\\t\", label_maps_rev[idx].split(\"-\")[-1])","c66d4897":"download_and_predict(\"https:\/\/cdn.pixabay.com\/photo\/2018\/08\/12\/02\/52\/belgian-mallinois-3599991_1280.jpg\",\n                     \"test_1.jpg\")","be7f8d8f":"download_and_predict(\"https:\/\/cdn.pixabay.com\/photo\/2016\/07\/25\/00\/06\/corgi-1539598_1280.jpg\",\n                     \"test_2.jpg\")","5ad2789c":"download_and_predict(\"https:\/\/cdn.pixabay.com\/photo\/2019\/02\/24\/20\/15\/chihuahua-4018429_1280.jpg\",\n                     \"test_3.jpg\")","132559d0":"download_and_predict(\"https:\/\/cdn.pixabay.com\/photo\/2018\/03\/31\/06\/31\/dog-3277416_1280.jpg\",\n                     \"test_4.jpg\")","11f97780":"download_and_predict(\"https:\/\/cdn.pixabay.com\/photo\/2016\/02\/19\/15\/46\/dog-1210559_1280.jpg\",\n                     \"test_5.jpg\")","2f447355":"!rm -rf data\/* ","850b2438":"!rm -f test_*.jpg","ee34a5d1":"# train test split","5d9b3860":"# show some pictures","72c6a3cf":"# label strings and numbers mapping","4d54f202":"# predict new images","59e3af73":"# image generator with augment","2dcb9776":"# how many breeds and pictures we have","ec9a3ef7":"# cleaning","a865129c":"# crop and save pictures","37d522e5":"download some dog images from pixabay for testing","64c7b686":"# only train our last 6 layers","a66a718a":"# keras pretrain densenet121 model","cea86b65":"# prepare X and y","3d93270a":"# now train all layers","b9f78d37":"# imports"}}