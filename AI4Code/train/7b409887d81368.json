{"cell_type":{"5943bb2c":"code","ff931af1":"code","4c4ef0ea":"code","b5d45187":"code","601e7e7d":"code","ed29748b":"code","a9616a04":"code","ee7bf243":"code","38ad1859":"code","9a6e90e4":"code","ce076fe9":"code","36e64b1f":"code","7ac0b20d":"code","5e88db3b":"code","589bf580":"code","3b162e66":"code","9b3fb38d":"code","4bff8ee4":"code","c9b4d4b8":"code","27ae923e":"code","8a332830":"code","7849db32":"code","9e8a9a4a":"code","9b80c078":"code","f713801d":"code","f46272c3":"code","d5920cf1":"code","cf7c7f98":"code","8fe5e457":"code","5e635984":"code","ac108a3a":"code","fa5512bb":"code","499f1ba8":"code","7f0c796b":"code","9a970fb3":"code","bb0e12bc":"code","b0f65b32":"code","21d41e8e":"code","30ee9161":"code","6d35ac76":"code","cd204547":"code","26fa4631":"code","b5950c20":"code","1e1be165":"code","6ca6834b":"code","346c4007":"code","90993a5d":"code","7be7d0fb":"code","bfdb2bbb":"code","a1cc8218":"code","88bc62b4":"markdown","0b5958ac":"markdown","6306eb40":"markdown","2b571187":"markdown","0da35b13":"markdown","f927bc47":"markdown","6205b2e8":"markdown","4e871632":"markdown","fea56845":"markdown","69c4fa52":"markdown","b25de4ac":"markdown","ac6dfeb7":"markdown","76e1263f":"markdown","fdfdbb96":"markdown","72b6b9af":"markdown","8cb296c4":"markdown","6a45157c":"markdown","10e0bba9":"markdown","3c316eda":"markdown","b718e072":"markdown","b3998067":"markdown","1ba0bd17":"markdown","3eb7d441":"markdown","50b4783a":"markdown","416510f8":"markdown","318ff503":"markdown","9c49828b":"markdown","6feaa243":"markdown"},"source":{"5943bb2c":"# Import libraries\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n\nimport warnings\nwarnings.simplefilter('ignore')","ff931af1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4c4ef0ea":"# Load the datasets\ntrain = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/train.csv', index_col='PassengerId')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/test.csv', index_col='PassengerId')\nsubmission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/sample_submission.csv')","b5d45187":"submission.head()","601e7e7d":"# Check top 5 rows from train and test sets\ntrain.head()","ed29748b":"# Check top 5 rows from train and test sets\ntest.head()","a9616a04":"# Check for missing values\ntrain.isnull().sum()","ee7bf243":"# Same is the case with test set\ntest.isnull().sum()","38ad1859":"# Age column\ntrain['Age'] = train['Age'].fillna(train['Age'].mean())\ntest['Age'] = test['Age'].fillna(test['Age'].mean())\n\n# Ticket column\ntrain['Ticket'] = train['Ticket'].fillna('X').map(lambda x: str(x).split()[0] if len(str(x).split()) > 1 else 'X')\ntest['Ticket'] = test['Ticket'].fillna('X').map(lambda x: str(x).split()[0] if len(str(x).split()) > 1 else 'X')\n","9a6e90e4":"# Fare column\nfare_map = train[['Fare', 'Pclass']].dropna().groupby('Pclass').mean().to_dict()\ntrain['Fare'] = train['Fare'].fillna(train['Pclass'].map(fare_map['Fare']))\ntrain['Fare'] = np.log1p(train['Fare'])\ntest['Fare'] = test['Fare'].fillna(test['Pclass'].map(fare_map['Fare']))\ntest['Fare'] = np.log1p(test['Fare'])","ce076fe9":"# Cabin column\ntrain['Cabin'] = train['Cabin'].fillna('X').map(lambda x: x[0].strip())\ntest['Cabin'] = test['Cabin'].fillna('X').map(lambda x: x[0].strip())\n\n# Embarked column\ntrain['Embarked']  = train['Embarked'].fillna('X')\ntest['Embarked'] = test['Embarked'].fillna('X')","36e64b1f":"train.isnull().sum()","7ac0b20d":"test.isnull().sum()","5e88db3b":"# Check train top 5 rows\ntrain.head()","589bf580":"# Drop Name column as we'll not use that.\ntrain.drop('Name', axis=1, inplace=True)\ntest.drop('Name', axis=1, inplace=True)","3b162e66":"# Define type of columns\nnum_cols = ['Age', 'Fare']\nlabel_cols = ['Pclass', 'SibSp', 'Parch', 'Ticket', 'Cabin']\nohe_cols = ['Sex', 'Embarked']","9b3fb38d":"# Handle label columns\nfor col in label_cols:\n    le = LabelEncoder()\n    train[col] = le.fit_transform(train[col])\n    test[col] = le.transform(test[col])","4bff8ee4":"# Handle ohe columns\nohe_encoded_train_df = pd.get_dummies(train[ohe_cols], drop_first=True)\nohe_encoded_test_df = pd.get_dummies(test[ohe_cols], drop_first=True)","c9b4d4b8":"ohe_encoded_train_df[:5]","27ae923e":"train = pd.concat([train, ohe_encoded_train_df], axis=1)\ntrain.drop(ohe_cols, axis=1, inplace=True)","8a332830":"test = pd.concat([test, ohe_encoded_test_df], axis=1)\ntest.drop(ohe_cols, axis=1, inplace=True)","7849db32":"# Scale numeric columns\nscaler = StandardScaler()\ntrain[num_cols] = scaler.fit_transform(train[num_cols])\ntest[num_cols] = scaler.transform(test[num_cols])","9e8a9a4a":"train.head()","9b80c078":"test.head()","f713801d":"!pip install scikit-learn-intelex --progress-bar off >> \/tmp\/pip_sklearnex.log","f46272c3":"# Enable Intel(R) Extension for sk-learn\nfrom sklearnex import patch_sklearn\npatch_sklearn()","d5920cf1":"# Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\n\nimport optuna","cf7c7f98":"features = train.drop('Survived', axis=1)\ntarget = train.Survived","8fe5e457":"# Split train into train_, valid_ datasets\nX_train, X_valid, y_train, y_valid = train_test_split(features, target, test_size=0.3, random_state=41)\n\nprint(X_train.shape, y_train.shape)\nprint(X_valid.shape, y_valid.shape)","5e635984":"RANDOM_STATE = 41\nFOLDS = 10","ac108a3a":"lr = LogisticRegression(random_state=RANDOM_STATE)\nscores = cross_val_score(lr, X_train, y_train, cv=FOLDS, scoring='accuracy')\nprint(f'LogisticRegression(CV): {scores.mean()}')","fa5512bb":"%%time\ndt = DecisionTreeClassifier(random_state=RANDOM_STATE)\nscores = cross_val_score(dt, X_train, y_train, cv=FOLDS, scoring='accuracy')\nprint(f'DecisionTree(CV): {scores.mean()}')","499f1ba8":"%%time\nrf = RandomForestClassifier(random_state=RANDOM_STATE,\n                           max_depth=15,\n                           min_samples_leaf=8)\nscores = cross_val_score(rf, X_train, y_train, cv=FOLDS, scoring='accuracy')\nprint(f'RandomForest(CV): {scores.mean()}')","7f0c796b":"# define objective function so that accuracy for RandomForest can be optimized using Optuna\ndef objective(trial):\n    params = {\n        'random_state': RANDOM_STATE,\n        'max_depth': trial.suggest_int('max_depth', 10, 25),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 5, 12)\n    }\n    \n    rf_ = RandomForestClassifier(**params)\n    rf_.fit(X_train, y_train)\n    return rf_.score(X_valid, y_valid)","9a970fb3":"study = optuna.create_study(sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n                           direction='maximize',\n                           pruner=optuna.pruners.MedianPruner())\nstudy.optimize(objective, n_trials=100, show_progress_bar=True)","bb0e12bc":"# After optuna optimization, print best params\nprint(f'Best Accuracy: {study.best_trial.value}')\nprint(f'Best Params: {study.best_params}')","b0f65b32":"%%time\nkf = KFold(n_splits=FOLDS, shuffle=True, random_state=RANDOM_STATE)\ny_pred = np.zeros(test.shape[0])\n\nfor fold, (train_index, valid_index) in enumerate(kf.split(features, target)):\n    print(\"Running Fold {}\".format(fold + 1))\n    X_train, X_valid = pd.DataFrame(features.iloc[train_index]), pd.DataFrame(features.iloc[valid_index])\n    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n    rf_ = RandomForestClassifier(**study.best_params)\n    rf_.fit(X_train, y_train)\n    print(\"  Accuracy: {}\".format(accuracy_score(y_valid, rf_.predict(X_valid))))\n    y_pred += rf_.predict(test)\n\ny_pred \/= FOLDS\n\nprint(\"\")\nprint(\"Done!\")","21d41e8e":"submission['Survived'] = np.round(y_pred).astype(int)\nsubmission.to_csv('rf_10_folds_optuna.csv', index=False) # Kaggle Score: 0.79805","30ee9161":"def objective(trial):\n    params = {\n        'C': trial.suggest_loguniform('C', 0.1, 0.5),\n        'gamma': trial.suggest_categorical('gamma', ['auto']),\n        'kernel': trial.suggest_categorical('kernel', ['rbf']),\n    }\n    \n    svc_ = SVC(**params)\n    svc_.fit(X_train, y_train)\n    return svc_.score(X_valid, y_valid)","6d35ac76":"study = optuna.create_study(sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n                           direction='maximize',\n                           pruner=optuna.pruners.MedianPruner())\nstudy.optimize(objective, n_trials=5, show_progress_bar=True)","cd204547":"%%time\nkf = KFold(n_splits=FOLDS, shuffle=True, random_state=RANDOM_STATE)\ny_pred = np.zeros(test.shape[0])\n\nfor fold, (train_index, valid_index) in enumerate(kf.split(features, target)):\n    print(\"Running Fold {}\".format(fold + 1))\n    X_train_, X_valid_ = pd.DataFrame(features.iloc[train_index]), pd.DataFrame(features.iloc[valid_index])\n    y_train_, y_valid_ = target.iloc[train_index], target.iloc[valid_index]\n    svc_ = SVC(**study.best_params)\n    svc_.fit(X_train_, y_train_)\n    print(\"  Accuracy: {}\".format(accuracy_score(y_valid_, svc_.predict(X_valid_))))\n    y_pred += svc_.predict(test)\n\ny_pred \/= FOLDS\n\nprint(\"\")\nprint(\"Done!\")","26fa4631":"knc = KNeighborsClassifier(n_neighbors=1)\nscores = cross_val_score(knc, X_train, y_train, cv=FOLDS, scoring='accuracy')\nprint(f'KNeighbors: {scores.mean()}')","b5950c20":"def objective(trial):\n    params = {\n        'max_features':trial.suggest_float('max_features', 0.45, 0.6),\n        'min_samples_leaf':trial.suggest_int('min_samples_leaf', 6, 10),\n        'min_samples_split':trial.suggest_int('min_samples_split', 3, 5)\n    }\n    \n    etc = ExtraTreesClassifier(random_state=RANDOM_STATE, n_estimators=100, **params)\n    etc.fit(X_train, y_train)\n    return etc.score(X_valid, y_valid)","1e1be165":"study = optuna.create_study(sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n                           direction='maximize',\n                           pruner=optuna.pruners.MedianPruner())\nstudy.optimize(objective, n_trials=10, show_progress_bar=True)","6ca6834b":"%%time\nkf = KFold(n_splits=FOLDS, shuffle=True, random_state=RANDOM_STATE)\ny_pred = np.zeros(test.shape[0])\n\nfor fold, (train_index, valid_index) in enumerate(kf.split(features, target)):\n    print(\"Running Fold {}\".format(fold + 1))\n    X_train_, X_valid_ = pd.DataFrame(features.iloc[train_index]), pd.DataFrame(features.iloc[valid_index])\n    y_train_, y_valid_ = target.iloc[train_index], target.iloc[valid_index]\n    etc_ = ExtraTreesClassifier(random_state=RANDOM_STATE, n_estimators=100, **study.best_params)\n    etc_.fit(X_train_, y_train_)\n    print(\"  Accuracy: {}\".format(accuracy_score(y_valid_, etc_.predict(X_valid_))))\n    y_pred += etc_.predict(test)\n\ny_pred \/= FOLDS\n\nprint(\"\")\nprint(\"Done!\")","346c4007":"submission['Survived'] = np.round(y_pred).astype(int)\nsubmission.to_csv('extratrees_10_folds_optuna.csv', index=False)","90993a5d":"def objective(trial):\n#     trial_params = {\n#         'min_child_samples': trial.suggest_int('min_child_samples', 145, 160),\n#         'num_leaves': trial.suggest_int('num_leaves', 15, 25),\n#         'max_depth': trial.suggest_int('max_depth', 14, 16)\n#     }\n    lgb_params = {\n        'metric': 'binary_logloss',\n        'n_estimators': 100,\n        'objective': 'binary',\n        'random_state': RANDOM_STATE,\n        'learning_rate': 0.01,\n        'min_child_samples': 150,\n        'reg_alpha': 3e-5,\n        'reg_lambda': 9e-2,\n        'num_leaves': 20,\n        'max_depth': 16,\n        'colsample_bytree': 0.8,\n        'subsample': 0.8,\n        'subsample_freq': 2,\n        'max_bin': 240,\n    }\n    \n    lgbm_ = LGBMClassifier(**lgb_params)\n    lgbm_.fit(X_train, y_train)\n    return lgbm_.score(X_valid, y_valid)","7be7d0fb":"lgb_params = {\n    'metric': 'binary_logloss',\n    'n_estimators': 100,\n    'objective': 'binary',\n    'random_state': RANDOM_STATE,\n    'learning_rate': 0.01,\n    'min_child_samples': 150,\n    'reg_alpha': 3e-5,\n    'reg_lambda': 9e-2,\n    'num_leaves': 20,\n    'max_depth': 16,\n    'colsample_bytree': 0.8,\n    'subsample': 0.8,\n    'subsample_freq': 2,\n    'max_bin': 240,\n}\n\nlgbm_ = LGBMClassifier(**lgb_params)\nscores = cross_val_score(lgbm_, \n                      X_train, \n                      y_train,\n                      cv=5,\n                      scoring='accuracy')\nscores","bfdb2bbb":"# metric='binary_logloss',\n#                            n_estimators=100,\n#                            objective='binary',\n#                            random_state=RANDOM_STATE,\n#                            learning_rate=0.01,\n#                            reg_alpha=3e-5,\n#                            reg_lambda=9e-2,\n#                            colsample_bytree=0.8,\n#                            subsample=0.8,\n#                            subsample_freq=2,\n#                            max_bin=240, ","a1cc8218":"study = optuna.create_study(sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n                           direction='maximize',\n                           pruner = optuna.pruners.MedianPruner())\nstudy.optimize(objective, n_trials=10, show_progress_bar=True)","88bc62b4":"### Model: RandomForestClassifier","0b5958ac":"### Install scikit-learn-intelex","6306eb40":"Not improved compared to LogisticRegression model","2b571187":"### Model: LGBMClassifier","0da35b13":"This is almost similar to RandomForestClassifier, but we'll try to submit these predictions.","f927bc47":"Let's do cross validation and then check score","6205b2e8":"As we can see cross validation on RandomForest is taking time(almost a minute), we'll try to use library `scikit-learn-intelex` which was recommended in the referenced notebook.","4e871632":"Prepare RandomForest model for prediction","fea56845":"Let's try RandomForest with the optimized params","69c4fa52":"### Model: LogisticRegression","b25de4ac":"I've worked on different notebooks and my final submission score has been around 0.78824.\nIn this notebook trying to improve the score with different concepts learned from other notebooks in this comptetition.\nI've linked few for the references:\n+ [Ensemble-learning meta-classifier for stacking](https:\/\/www.kaggle.com\/remekkinas\/ensemble-learning-meta-classifier-for-stacking)\n+ [TPS04 - SVM with scikit-learn-intelex](https:\/\/www.kaggle.com\/napetrov\/tps04-svm-with-scikit-learn-intelex)\n+ _will add more as and when I find them..._","ac6dfeb7":"As we can see, we have categorical features with strings, which is our next hurdle to convert them to numeric values.\nWe'll be using LabelEncoder, OneHotEncoder.\n\nAlso, we'll scale numerical features using StandardScaler.","76e1263f":"### Model: ExtraTreesClassifier","fdfdbb96":"We'll do following to handle missing values:\n+ Age - Impute with mean value\n+ Ticket - Impute with 'X' and take only first word if more than 1 word else use 'X'\n+ Fare - Impute with mean value based on Pclass group\n+ Cabin - Impute with 'X' and take first letter\n+ Embarked - Impute with 'X'\n\n_Perform same imputations on train and test set_","72b6b9af":"## Load dataset","8cb296c4":"### Model: DecisionTreeClassifier","6a45157c":"### Short introduction on library `Optuna`\n\nOptuna is used to optimize hyperparameters for an algorithm.\n``` python\nimport optuna\n```\nConventionally, functions to be optimized are named `objective`.\n``` python\ndef objective(trial):\n    x = trial.suggest_float(\"x\", -10, 10)\n    return (x - 2) ** 2\n```\nThis function returns the value of (x-2)^2. Our goal is to find the value of `x` that minimizes the output of the `objective` function. This is the **optimization**. During optimization, Optuna repeatedly calls and evaluates the objective function with different values of `x`.\n\nA `Trial` object corresponds to a single execution of the objective function and is internally instantiated upon each invocation of the function.\n\n`suggest` APIs (`suggest_float()`) are called inside the objective function to obtain parameters for a trial. `suggest_float()` selects parameters uniformly within the range provided. In above example, -10 to 10.\n\nTo start the optimization, we create a study object and pass the objective function to method `optimize()` as follows:\n``` python\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=100)\n```\n\nYou can get the best parameter as follows:\n``` python\nbest_params = study.best_params\nfound_x = best_params[\"x\"]\n```\n_When used to search for hyperparameters in machine learning, usually the objective function would return the loss or accuracy of the model._","10e0bba9":"This has improved score than LogisticRegression. Let's try parameter tuning using library `Optuna`.","3c316eda":"Not a good performance compared to RandomForestClassifier.","b718e072":"## Classifiers","b3998067":"## Cleaning data","1ba0bd17":"Great, we've crossed our 1st hurdle by handling missing values.","3eb7d441":"Let's have a look at our cleaned train and test datasets}","50b4783a":"### Model: SVM","416510f8":"This is not an improvement compared to RandomForest classifier.","318ff503":"Now, let's perform preprocessing to make the data clean and usable in our Classifier models","9c49828b":"### Model: KNeighborsClassifier","6feaa243":"# Table of contents\n+ [Load dataset](#Load-dataset)\n+ [Cleaning data and Feature engineering](#Cleaning-data)\n+ [Classifiers](#Classifiers)\n  + [Logistic regression](#Model:-LogisticRegression)\n  + [DecisionTree classifier](#Model:-DecisionTreeClassifier)\n  + [RandomForest classifier](#Model:-RandomForestClassifier)\n  + [SVM](#Model:-SVM)"}}