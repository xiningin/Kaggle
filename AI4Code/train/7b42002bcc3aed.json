{"cell_type":{"7fdf6c92":"code","b1569ecd":"code","ffea49cf":"code","80b261e8":"code","8e3987f0":"code","06901f94":"code","e9f61650":"code","578a3712":"code","1c0881a1":"code","b5a93634":"code","2c9f4e47":"code","0a2cba1e":"code","21f9e9d7":"code","35ae1309":"code","cd17a408":"code","e6e7b578":"code","03589da0":"code","90e9f248":"markdown","257e44d1":"markdown","1db33e00":"markdown","1f381976":"markdown","18c3fcb0":"markdown","63b75eb5":"markdown","f08694df":"markdown","fac3403f":"markdown","b49c2138":"markdown","2264b4bf":"markdown","06eaffb3":"markdown","ab300f28":"markdown","582da3ff":"markdown","a63a519d":"markdown","74c3d55e":"markdown","bbc6efcb":"markdown","a9536c97":"markdown","ce38dfb6":"markdown","c836af22":"markdown","a6443ea9":"markdown","eb11b6ca":"markdown"},"source":{"7fdf6c92":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gensim\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n%matplotlib inline","b1569ecd":"df = pd.read_csv(\"..\/input\/train.csv\")\ndf.head()","ffea49cf":"df.shape","80b261e8":"def read_questions(row,column_name):\n    return gensim.utils.simple_preprocess(str(row[column_name]).encode('utf-8'))\n    \ndocuments = []\nfor index, row in df.iterrows():\n    documents.append(read_questions(row,\"question1\"))\n    if row[\"is_duplicate\"] == 0:\n        documents.append(read_questions(row,\"question2\"))","8e3987f0":"print(\"List of lists. Let's confirm: \", type(documents), \" of \", type(documents[0]))","06901f94":"model = gensim.models.Word2Vec(size=150, window=10, min_count=2, sg=1, workers=10)\nmodel.build_vocab(documents)  # prepare the model vocabulary","e9f61650":"model.train(sentences=documents, total_examples=len(documents), epochs=model.iter)","578a3712":"word_vectors = model.wv\ncount = 0\nfor word in word_vectors.vocab:\n    if count<10:\n        print(word)\n        count += 1\n    else:\n        break","1c0881a1":"len(word_vectors.vocab)","b5a93634":"vector = model.wv[\"immigration\"]  # numpy vector of a word\nlen(vector)","2c9f4e47":"vector","0a2cba1e":"wanted_words = []\ncount = 0\nfor word in word_vectors.vocab:\n    if count<150:\n        wanted_words.append(word)\n        count += 1\n    else:\n        break\nwanted_vocab = dict((k, word_vectors.vocab[k]) for k in wanted_words if k in word_vectors.vocab)\nwanted_vocab","21f9e9d7":"X = model[wanted_vocab] # X is an array of word vectors, each vector containing 150 tokens\ntsne_model = TSNE(perplexity=40, n_components=2, init=\"pca\", n_iter=5000, random_state=23)\nY = tsne_model.fit_transform(X)","35ae1309":"#Plot the t-SNE output\nfig, ax = plt.subplots(figsize=(20,10))\nax.scatter(Y[:, 0], Y[:, 1])\nwords = list(wanted_vocab)\nfor i, word in enumerate(words):\n    plt.annotate(word, xy=(Y[i, 0], Y[i, 1]))\nax.set_yticklabels([]) #Hide ticks\nax.set_xticklabels([]) #Hide ticks\n_ = plt.show()","cd17a408":"w1 = \"phone\"\nmodel.wv.most_similar(positive=w1, topn=5)","e6e7b578":"w1 = [\"women\",\"rights\"]\nw2 = [\"girls\"]\nmodel.wv.most_similar (positive=w1,negative=w2,topn=2)","03589da0":"model.wv.doesnt_match([\"government\",\"corruption\",\"peace\"])","90e9f248":"We finished training our Word2Vec model. What's next? The funny thing is that next we abandon the softmax layer and just use the 47.366 x 150 weight matrix as our word embedding lookup table. Sounds weird, isn't it? The only goal was to get the weights of the hidden layer. These weights are essentially the word embeddings that we had to learn. Once we know these word embeddings, we do not need the whole Word2Vec network anymore.","257e44d1":"We can now create a two-dimensional semantic representation of word embeddings using t-SNE.","1db33e00":"Next, given positive and negative words, we will find top 2 words that are similar to positivie words and opposite to negative words.","1f381976":"**Word2Vec itself is not a deep neural network**, but it turns input text into a numerical form that deep neural networks can process as inputs.","18c3fcb0":"The embedded vectors for a specific token are stored in a KeyedVectors instance in \"model.wv\". We can see that the length of a word vector is equal to 150 as we defined by the parameter \"size\":","63b75eb5":"# Simply about Word2Vec (Quora dataset)\n\n***Liana Napalkova***\n\n***21 October 2018***\n\n## Table of contents\n1. [Introduction](#introduction)\n2. [What is Word2Vec?](#word2vec)\n3. [Training of Word2Vec](#training)\n4. [Exploring the trained Word2Vec model](#exploring)\n5. [Playing with the trained Word2Vec model ](#playing)\n6. [When can we use Word2Vec?](#usage)\n\n**I hope you find this kernel helpful and some <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated.**\n\n## Introduction <a name=\"introduction\"><\/a>\n\nIf you have never used Word2Vec, then this notebook is for you! Here I will apply Word2Vec to the questions asked in Quora. The goal of the notebook is to better understand how Word2Vec works. Basically I wanted to resolve my personal doubts regarding Word2Vec. I will be very glad if my small analysis would be useful for someone else.","f08694df":"\n## When can we use Word2Vec? <a name=\"usage\"><\/a>\n\nThere are many application scenarios for Word2Vec: sentiment analysis, recommender systems, etc. For example, imagine that you have tags for a large amount of questions in Quora. Given a tag, you want to find similar tags in order to recommend them to a user for exploration. You can do this by treating each set of co-occuring tags as a \"sentence\" and train a Word2Vec model on this data. ","fac3403f":"![](https:\/\/preview.ibb.co\/iuzji0\/Word2-Vec-skip-gram.jpg)","b49c2138":"Finally, I will use Word2Vec to find odd items given a list of items \"government\",\"corruption\" and \"peace\". For most of people this is a rhetorical question.","2264b4bf":"![](http:\/\/adriancolyer.files.wordpress.com\/2016\/04\/word2vec-distributed-representation.png?w=566&zoom=2)\n\nRef: [The amazing power of word vectors](http:\/\/blog.acolyer.org\/2016\/04\/21\/the-amazing-power-of-word-vectors\/)","06eaffb3":"## Playing with the trained Word2Vec model <a name=\"playing\"><\/a>\n\nNow we will see what we can actually do with our trained Word2Vec model. This overview does not pretend to be exhaustive. I will only check the most popular functionalities:\n* Given a word, find most similar words\n* Given words A and B, find other words that are similar to A and opposite to B\n* Given a sequence of words, find an odd word\n\nThis first example shows a simple case of looking up top 5 words similar to the word \"phone\". To do this, I will call the \"most_similar\" function and provide the word \"phone\" as the positive example. This returns the top 5 similar words: \"mobile\", \"phones\", \"smartphone\", \"iphone\" and \"device\". \n\nYou may notice that there is no word \"motorola\" in the list of top 5 similar words, while the word \"motorola\" appears in t-SNE graphic. Please remember that t-SNE was applied on a small subset of vocabulary (150 words). Therefore, the words \"mobile\", \"phones\", \"smartphone\", etc. were simply not included in these 150 words and therefore they have not appeared on the t-SNE graphic.","ab300f28":"## Training of Word2Vec <a name=\"training\"><\/a>\n\nTo train the Word2Vec two-layer neural network, the only thing that we need to do is to change the format of Quora dataset. In particular, I will transform the column \"question1\" into a list of lists. I will use the function \"simple_preprocess\" of GenSim package in order to transform each question (row) into a set of tokens, for example: \"what is your name\" -> \"what\", \"is\", \"your\", \"name\". ","582da3ff":"Now Word2Vec will retrieve all unique words from all sub-lists of \"documents\", thereby constructing the vocabulary (unique words). ","a63a519d":"Not that bad. It's interesting to see that, for example, the word pairs \"motorola\" and \"phone\" or \"youtube\" and \"video\" are located quite close to each other. It means that these words appeared in the same context in the corpus of text.","74c3d55e":"Do you agree with the opinion of Word2Vec?:)","bbc6efcb":"## Exploring the trained Word2Vec model <a name=\"exploring\"><\/a>\n\nThe learned vocabulary of tokens (words) is stored in \"model.wv.vocab\". In our case, the vocabulary includes 47.366 unique words. ","a9536c97":"## What is Word2Vec? <a name=\"word2vec\"><\/a>\n\nWord2Vec is one of many different word embedding techniques. In turn, word embedding is one of the most popular representation of document vocabulary. Thus, we get the following hierarchy:\n\n* Document vocabulary representation -> Word embedding -> Word2Vec\n\nSo, what is word embedding? **Word embedding** is a vector representation of a word. For example, let's consider the folling three phases: \n* Agama is a reptile.\n* Snake is a reptile.\n* Reptile is a cold-blooded animal.\n\nThe **dictionary** of this corpus of text (three phrases) consists of all unique words:\n> [\"agama\", \"reptile\", \"snake\", \"a\", \"is\", \"cold-blooded\", \"animal\"]\n\nA simple vector representation of the word \"agama\" would be:\n> [1, 0, 0, 0, 0, 0, 0]\n\nwhere 1 stands for the word \"agama\" in our dictionary. This word embedding is called one-hot encoding. For any word, the length of a vector is always 7 in our example, because the dictionary consists of 7 unique words. This word embedding technique is very simple, but has a tremendous disadvantage. Let's take vector representations of two words: \n> \"agama\" and \"reptile\" -> [1, 0, 0, 0, 0, 0, 0] and [0, 1, 0, 0, 0, 0, 0]\n\nThese two vectors provide no useful information regarding the relationships that may exist between the \"agama\" and \"reptile\". In other words, the similarity between words is not captured by the one-hot encoding. Loosely speaking, one-hot encoding does not enable the system making conclusions about the word \"agama\" given the word \"reptile\" (for example, such that agama is a cold-blooded animal).\n\n**Word2Vec is a more sophisticated word embedding technique**. This technique is based on the idea that words that occur in the same **contexts** tend to have similar meanings. I like this definition from Wikipedia \n\n> Word2Vec is a **two-layer neural network** that takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space (Wikipedia).\n\nBelow I provide one of the best graphic representations of Word2Vec's word embedding that I found on Internet. This picture gives an intuition about how word embedding looks like in Word2Vec.","ce38dfb6":"Below I provide the definition of four parameters that we used to define a Word2Vec model:\n\n* **size:** The size means the dimensionality of word vectors. It defines the number of tokens used to represent each word. For example, rake a look at the picture above. The size would be equal to 4 in this example. Each input word would be represented by 4 tokens: King, Queen, Women, Princess.  Rule-of-thumb: If a dataset is small, then size should be small too. If a dataset is large, then size should be greater too. It's the question of tuning.\n\n* **window:** The maximum distance between the target word and its neighboring word. For example, let's take the phrase \"agama is a reptile \" with 4 words (suppose that we do not exclude the stop words). If window size is 2, then the vector of word \"agama\" is directly affected by the word \"is\" and \"a\". Rule-of-thumb: a smaller window should provide terms that are more related (of course, the exclusion of stop words should be considered).\n\n* **min_count:** Ignores all words with total frequency lower than this. For example, if the word frequency is extremally low, then this word might be considered as unimportant.\n\n* **sg:** Selects training algorithm: 1 for Skip-Gram; 0 for CBOW (Continuous Bag of Words).\n\n* **workers:** The number of worker threads used to train the model.\n\nMore details about input parameters can be found [here](http:\/\/radimrehurek.com\/gensim\/models\/word2vec.html).","c836af22":"Let's see what happens when the function \"train\" is executed. I found a diagram (see below) that shows a Skip-Gram model architecture to produce a distributed representation of words. To understand this diagram, we should consider the size of vocablurary and the dimensionality of the word vectors (the parameter \"size\"). In our example, these are equal to 47.366 words and 150 tokens, respectively. If we take the word \u201cimmigration\u201d, it will be one of the words in the 47.366 word vocabulary.  Therefore we can initially represent the word \"immigration\" as a 47.366 length one-hot vector.  We then link this input vector to a 150 node hidden layer.  There are totally 150 weights connecting the input layer and the hidden layer.  The activations of the nodes in this hidden layer are simply linear summations of the weighted inputs, e.g. `ohe*w1+ohe*w2+...+ohe*w150`.  These nodes are then fed into a softmax output layer.  During training, we want to change the weights of this neural network so that words surrounding \u201cimmigration\u201d have a higher probability in the softmax output layer.  So, for instance, if our text data set has a lot of questions related to the immigration in Canada, we would want our network to assign large probabilities to words like \u201cCanada\u201d, \"Australia\"  (given lots of sentences containing \u201cthe immigration to Canada or Australia\u201d).","a6443ea9":"Once the vocabulary is created and the Word2Vec model is specified, I will train this model by calling \"train\" function.","eb11b6ca":"The Quora dataset consists of 404.290 rows and 6 columns. To reach the goal of this notebook, I am only interested in columns that contain questions. These columns are \"question1\" and \"question2\". To reduce training time, I will only use the column \"question1\"."}}