{"cell_type":{"d4510051":"code","7c0cc316":"code","9552a269":"code","3e6eaabd":"code","5eed000c":"code","07e56400":"code","64c6d2e8":"code","0c56706b":"code","a7fb114a":"code","fa141bef":"code","5d471633":"code","44531dd6":"code","a40236ed":"code","d71e7994":"code","61d6dfda":"code","3773c087":"code","ff9c31a7":"code","7e41a82c":"code","b38c9e5e":"code","f95080d1":"code","4a3bb6d9":"code","1758c29e":"code","37f4a6fe":"code","08a857aa":"code","40c5c306":"code","f477e7d1":"code","c88254fe":"markdown","9b63cab8":"markdown","b2fda635":"markdown","450d7066":"markdown","01a805db":"markdown","2857faf1":"markdown","6d9736ba":"markdown","83f93a1e":"markdown","6bd1d98c":"markdown","a5300dd5":"markdown","c7024182":"markdown","caea8a3b":"markdown","17e165aa":"markdown","fb645249":"markdown","149fefe1":"markdown","71f0e9ae":"markdown","69f28191":"markdown","dd2aded5":"markdown","97dbe5f1":"markdown","5e41c2cd":"markdown","8bfc6246":"markdown","328d711b":"markdown","834927ab":"markdown","b614f65f":"markdown"},"source":{"d4510051":"import os\nimport gc\nimport time\nimport tqdm\nimport pickle\nimport random\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7c0cc316":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.legend_handler import HandlerBase\nfrom matplotlib.text import Text\n\nimport warnings\nwarnings.filterwarnings('ignore')","9552a269":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.utils import class_weight\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder","3e6eaabd":"import lightgbm as lgb","5eed000c":"SEED = 123      \nrandom.seed(SEED)\n\nTEST_SIZE = 0.20\nVAL_SIZE = 0.15","07e56400":"data = pd.read_csv(\"\/kaggle\/input\/fetal-health-classification\/fetal_health.csv\")","64c6d2e8":"data","0c56706b":"plt.figure(figsize=(6,5))\nax = sns.countplot(x = data['fetal_health'])\n\n#1-Normal\n#2-Suspect\n#3-Pathological","a7fb114a":"label = LabelEncoder()\nlabel.fit(data['fetal_health'])\ndata['fetal_health'] = label.transform(data['fetal_health'])","fa141bef":"X = data.drop(columns='fetal_health')\ny = data['fetal_health']","5d471633":"X_scaled = MinMaxScaler().fit_transform(X.values)\nX=pd.DataFrame(X_scaled, index=X.index, columns=X.columns)\nX.head()","44531dd6":"print('\\nData split:\\nTest size: {}\\nVal  size: {}\\n'.format(TEST_SIZE,VAL_SIZE))\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, \n                                                    test_size = TEST_SIZE, \n                                                    random_state=SEED)\n\nprint('TRAIN: {} & {}'.format(X_train.shape, y_train.shape))\nprint('TEST:  {} & {}'.format(X_test.shape, y_test.shape))\n\nval_len = int(X_train.shape[0]*VAL_SIZE)","a40236ed":"# Calculate class weights from sklearn\nclass_weight_array = class_weight.compute_class_weight('balanced', \n                                                       np.unique(y_train), \n                                                       y_train)\nprint('\\nClass weights: {}'.format(class_weight_array)) \n\n# Class weights as dictionary for Keras\nkeys = [0,1,2] \nclass_weight_dict = dict(zip(keys, class_weight_array.T))\nprint('\\nClass weights dict: {}'.format(class_weight_dict))","d71e7994":"X_val = X_train[:val_len]\ny_val = y_train[:val_len]\n\nX_train_cut = X_train[val_len:]\ny_train_cut = y_train[val_len:]","61d6dfda":"fit_params={\"early_stopping_rounds\":10, \n            \"eval_metric\" : 'auc_mu', \n            \"eval_set\" : [(X_val,y_val)],\n            'eval_names': ['valid'],\n            'verbose': 100,\n            'feature_name': 'auto'\n           }","3773c087":"clf = lgb.LGBMClassifier(boosting_type = 'gbdt',\n                         objective='multiclass',\n                         metric='auc_mu', \n                         class_weight=class_weight_dict,\n                         n_estimators=100,\n                         num_leaves= 31,\n                         colsample_bytree=1.0,\n                         subsample=1.0,\n                         learning_rate=0.1,\n                         max_depth=-1, \n                         random_state=SEED, \n                         #silent=True, \n                         #n_jobs=4, \n)","ff9c31a7":"clf.fit(X_train_cut, y_train_cut, **fit_params)","7e41a82c":"plt.figure(figsize=(15,9))\n\nplt.title('LightGBM feature importances', fontsize=18)\nplt.xlabel('Importance', fontsize=16)\nplt.ylabel('Feature', fontsize=16)\n\nfeat_imp = pd.Series(clf.feature_importances_, index=X.columns)\nfeat_imp.nlargest(34).plot(kind='barh', color='tab:orange')\n\nplt.tight_layout()\n\nplt.savefig('lgbm_kfold_feature_importances_default.png')","b38c9e5e":"# These parameters will be used in gridsearch and will be found optimum parameters\nparam_grid = {\n    'class_weight': [class_weight_dict],\n    'boosting_type': ['gbdt', 'dart'],\n    'objective': ['multiclass'],\n    'metric': ['auc_mu'],\n    'n_estimators': [100, 500],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'num_leaves': [50, 100, 200],\n    'colsample_bytree': [0.6, 0.8],\n    'subsample': [0.7, 0.8],\n    'max_depth': [5, 10, 50],\n    'random_state': [SEED]\n}\n# 2160 fits (432x5), keep it not too big","f95080d1":"lgbm = lgb.LGBMClassifier() \n\n#lgbm.fit(X_train_cut, y_train_cut)","4a3bb6d9":"%%time\n\n# Grid search with 5-fold cross-validation\nlgbm_cv = GridSearchCV(lgbm, \n                       param_grid, \n                       cv=5, \n                       n_jobs=None, \n                       verbose=0) # set to 2 for long output\n\nlgbm_cv.fit(X_train_cut, y_train_cut)","1758c29e":"# Print optimum parameters\nprint('\\nGrid search optimum parameters:\\n{}'.format(lgbm_cv.best_params_))\n# Save the model parameters\nfilename = 'model_lgbm_kfold_gridsearch_best_params.pickle'\npickle.dump(lgbm_cv.best_params_, open(filename, 'wb'))\n# Load the model paarmeters\nwith open(filename, 'rb') as file:\n    best_params = pickle.load(file)\nprint('\\nGrid search optimum parameters [loaded]:\\n{}'.format(best_params))","37f4a6fe":"# Tuned model with optimum parameters\nclf_tuned = lgb.LGBMClassifier(**best_params) # == lgbm_cv.best_params_\n\n# Fit model\nclf_tuned.fit(X_train_cut, y_train_cut, **fit_params)\n\n# Predict\ny_test_pred_tuned = clf_tuned.predict(X_test) \n# Find the accuracy of y_test and predicitons, and round the result\nacc_tuned = round(accuracy_score(y_test, y_test_pred_tuned), 4) \nprint('Accuracy: {}%'.format(acc_tuned*100))","08a857aa":"plt.figure(figsize=(15,9))\nplt.title('LightGBM feature importances [tuned] | acc = {}%'.format(acc_tuned*100), fontsize=18)\nplt.xlabel('Importance', fontsize=16)\nplt.ylabel('Feature', fontsize=16)\nfeat_imp = pd.Series(clf_tuned.feature_importances_, index=X.columns)\nfeat_imp.nlargest(50).plot(kind='barh', color='tab:orange')\nplt.tight_layout()\nplt.savefig('lgbm_5fold_feature_importances_tuned.png')","40c5c306":"lgbm_cv.cv_results_","f477e7d1":"best_feat_10 = list(feat_imp.nlargest(10).index)\nbest_feat_10","c88254fe":"<h1 style=\"background-color:LightSeaGreen; font-family:newtimeroman; font-size:200%; text-align:left;\"> 3.2 Data scaling <\/h1>","9b63cab8":"<h1 style=\"background-color:LightSeaGreen; font-family:newtimeroman; font-size:200%; text-align:left;\"> 5.2 Fit model<\/h1>","b2fda635":"<h1 style=\"background-color:LightSeaGreen; font-family:newtimeroman; font-size:200%; text-align:left;\"> 5.1 Grid parameters <\/h1>","450d7066":"<h1 style=\"background-color:LightSeaGreen; font-family:newtimeroman; font-size:200%; text-align:left;\"> 1.1 Kaggle and other imports <\/h1>","01a805db":"<h1 style=\"background-color:LightSeaGreen; font-family:newtimeroman; font-size:200%; text-align:left;\"> 1 Import packages <\/h1>","2857faf1":"<h1 style=\"background-color:LightSeaGreen; font-family:newtimeroman; font-size:200%; text-align:left;\"> 1.3 Import Scikit-Learn <\/h1>","6d9736ba":"<h1 style=\"background-color:LightSeaGreen; font-family:newtimeroman; font-size:200%; text-align:left;\"> 3.1 Class imbalance <\/h1>","83f93a1e":"<h1 style=\"background-color:LightSeaGreen; font-family:newtimeroman; font-size:200%; text-align:left;\"> 5.4 Fit tuned model<\/h1>","6bd1d98c":"<h1 style=\"background-color:LightSeaGreen; font-family:newtimeroman; font-size:200%; text-align:left;\"> 3 Dataset <\/h1>","a5300dd5":"<h1 style=\"background-color:LightSeaGreen; font-family:newtimeroman; font-size:200%; text-align:left;\"> 3.4 Class weights <\/h1>","c7024182":"<h1 style=\"background-color:LightSeaGreen; font-family:newtimeroman; font-size:200%; text-align:left;\"> 1.2 Visualization imports <\/h1>","caea8a3b":"<h1 style=\"background-color:LightSeaGreen; font-family:newtimeroman; font-size:200%; text-align:left;\"> 5 Grid search <\/h1>","17e165aa":"<h1 style=\"background-color:LightSeaGreen; font-family:newtimeroman; font-size:200%; text-align:left;\"> 4.1 Build model <\/h1>","fb645249":"<h1 style=\"background-color:LightSeaGreen; font-family:newtimeroman; font-size:200%; text-align:left;\"> 5.3 Save optimum parameters <\/h1>","149fefe1":"<h1 style=\"background-color:LightSeaGreen; font-family:newtimeroman; font-size:200%; text-align:left;\"> 4.3 Feature importance <\/h1>","71f0e9ae":"<h1 style=\"background-color:LightSeaGreen; font-family:newtimeroman; font-size:200%; text-align:left;\"> 3.5 Validation set <\/h1>","69f28191":"<h2 style=color:Teal align=\"left\"> Table of Contents <\/h2>\n\n### 1 Import packages\n#### 1.1 Kaggle and other imports\n#### 1.2 Visuzalization imports\n#### 1.3 Import Scikit-Learn\n#### 1.4 Import LightGBM\n### 2 Configs\n### 3 Dataset\n#### 3.1 Class imbalance\n#### 3.2 Data scaling\n#### 3.3 Data split\n#### 3.4 Class weights\n#### 3.5 Validation set\n### 4 Model\n#### 4.1 Build model\n#### 4.2 Fit model\n#### 4.3 Feature importance\n### 5 Grid search\n#### 5.1 Grid parameters\n#### 5.2 Fit model\n#### 5.3 Save optimum parameters\n#### 5.4 Fit tuned model\n#### 5.5 Feature importance for tuned model\n","dd2aded5":"<h1 style=\"background-color:LightSeaGreen; font-family:newtimeroman; font-size:200%; text-align:left;\"> 5.5 Feature importance for tuned model<\/h1>","97dbe5f1":"<div align=\"center\">\n<font size=\"4\"> Context  <\/font>  \n<\/div> \n\nReduction of child mortality is reflected in several of the United Nations' Sustainable Development Goals and is a key indicator of human progress. The UN expects that by 2030, countries end preventable deaths of newborns and children under 5 years of age, with all countries aiming to reduce under\u20115 mortality to at least as low as 25 per 1,000 live births.\n\nParallel to notion of child mortality is of course maternal mortality, which accounts for 295 000 deaths during and following pregnancy and childbirth (as of 2017). The vast majority of these deaths (94%) occurred in low-resource settings, and most could have been prevented.\n\nIn light of what was mentioned above, Cardiotocograms (CTGs) are a simple and cost accessible option to assess fetal health, allowing healthcare professionals to take action in order to prevent child and maternal mortality. The equipment itself works by sending ultrasound pulses and reading its response, thus shedding light on fetal heart rate (FHR), fetal movements, uterine contractions and more.\n\n<div align=\"center\">\n<font size=\"4\"> Data  <\/font>  \n<\/div> \n\nThis dataset contains **2126 records** of features extracted from Cardiotocogram exams, which were then classified by three expert obstetritians into **3 classes**:\n\n- Normal\n- Suspect\n- Pathological\n\nLink to dataset is [here](https:\/\/www.kaggle.com\/andrewmvd\/fetal-health-classification).","5e41c2cd":"<h1 style=\"background-color:LightSeaGreen; font-family:newtimeroman; font-size:200%; text-align:left;\"> 4.2 Fit model <\/h1>","8bfc6246":"<h1 style=\"background-color:LightSeaGreen; font-family:newtimeroman; font-size:200%; text-align:left;\"> 4 Model <\/h1>","328d711b":"<h1 style=\"background-color:LightSeaGreen; font-family:newtimeroman; font-size:200%; text-align:left;\"> 3.3 Data split <\/h1>","834927ab":"<h1 style=\"background-color:LightSeaGreen; font-family:newtimeroman; font-size:200%; text-align:left;\"> 1.4 Import LightGBM <\/h1>","b614f65f":"<h1 style=\"background-color:LightSeaGreen; font-family:newtimeroman; font-size:200%; text-align:left;\"> 2 Configs <\/h1>"}}