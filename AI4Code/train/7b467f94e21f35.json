{"cell_type":{"5a6fc2ba":"code","3380c382":"code","af24433f":"code","b26f25b8":"code","a5f82969":"code","fdc734d8":"code","ca64805d":"code","ae6592d4":"code","93bbb221":"code","749bfb90":"code","eb8e653a":"markdown","0ec0aa38":"markdown","2ae8293e":"markdown","8f479787":"markdown","fae3e6ee":"markdown"},"source":{"5a6fc2ba":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook\n\ndf = pd.read_csv('..\/input\/cities.csv')\ncity_coordinates = df[['X','Y']].values\nnum_cities,_ = city_coordinates.shape","3380c382":"# Eratosthene's sieve to determine cities' primality\nprimes = [True for x in range(num_cities + 1)]\nprimes[0] = primes[1] = False\nfor i in tqdm_notebook(range(2, num_cities + 1)):\n    if primes[i]:\n        q = i * 2\n        while q <= num_cities:\n            primes[q] = False\n            q += i\nprimes = np.asarray(primes)\nnot_primes = np.asarray([not x for x in primes])\n\nmask_tens = [.1 if (x % 10) == 9 else 0 for x in range(num_cities)]","af24433f":"def create_best_first():\n    north_pole = np.array(city_coordinates[0])\n\n    current_city_coords = np.array(city_coordinates[0])\n    num_cities = city_coordinates.shape[0]\n\n    # keep the list of unvisited cities\n    unvisited_cities = [x for x in range(1, num_cities)]\n    unvisited_coordinates = np.array(city_coordinates[1:]) # actually remove the startup city from the coordinates 2D array\n\n    unvisited = num_cities - 1\n\n    path = []\n    total_distance = 0\n\n    #trace the route backwards (by step number)\n    for step in tqdm_notebook(range(num_cities, 1, -1)):\n        # unvisited_coordinates[:unvisited] contains for each row the coordinates of an unvisited city \n        # (unvisited_coordinates[X] has the coordinates of unvisited_cities[X])\n        distances = np.linalg.norm(unvisited_coordinates[:unvisited] - current_city_coords, axis=1)\n\n        if step % 10 == 0:\n            distances += np.multiply(distances, not_primes[unvisited_cities[:unvisited]]) * 0.1\n\n        closest_city_index = np.argmin(distances)\n        closest_city = unvisited_cities[closest_city_index]\n        current_city_coords = np.array(unvisited_coordinates[closest_city_index]) \n\n        total_distance += distances[closest_city_index]\n        path.append(closest_city)\n\n        # \"Remove\" closest_city from the two lists\n        unvisited_coordinates[closest_city_index] = unvisited_coordinates[unvisited - 1]\n        unvisited_cities[closest_city_index] = unvisited_cities[unvisited - 1]\n        \n        unvisited -= 1\n    \n    path = [0] + path[::-1] + [0] # reverse the path and add the North Pole at both ends\n    last_dist = np.linalg.norm(north_pole - current_city_coords)\n    total_distance += last_dist\n    return (path, total_distance)\n    ","b26f25b8":"best_path, score = create_best_first()\nprint (\"Expected score: {}\".format(score))","a5f82969":"import matplotlib.pyplot as plt\nplt.figure(figsize=(30,20))\n_ = plt.plot(city_coordinates[best_path, 0], city_coordinates[best_path, 1])","fdc734d8":"from collections import deque\ndef cost_around_point(index, point, before, after):\n    ret  = np.linalg.norm(city_coordinates[point] - city_coordinates[before]) * (1 + (mask_tens[index - 1] * not_primes[before]))\n    ret += np.linalg.norm(city_coordinates[point] - city_coordinates[after] ) * (1 + (mask_tens[index]     * not_primes[point] ))\n    return ret\n\ndef simulated_annealing(curr_path, score):\n    new_score = score\n    new_path = list(curr_path)\n    T0 = 2.0\n    alpha = 0.9\n    T_steps = 90\n    Tvals = [T0 * alpha**x for x in range(T_steps)]\n    np.random.seed(666)\n    score_history = {'T':[],'avg_dists':[], 'anneals':[]}\n    increase_counter = 0\n    tried_swaps = deque(maxlen=100000)\n    deltas = deque([3, 3,100, 100])\n    for T in tqdm_notebook(Tvals):\n        delta = deltas.popleft()\n        annealing_steps = 0\n        scores = []\n        for _ in range(100000):\n            while True:\n                i1 = np.random.randint(1, num_cities)\n                i2 = max(1, (i1 + np.random.poisson(delta)) % num_cities)\n                \n                if (i1,i2) not in tried_swaps and np.abs(i1-i2) >= 3:\n                    break\n\n            prev_i1i2  = cost_around_point(i1, new_path[i1], new_path[i1 - 1], new_path[i1 + 1]) \n            prev_i1i2 += cost_around_point(i2, new_path[i2], new_path[i2 - 1], new_path[i2 + 1])\n\n            new_i1i2   = cost_around_point(i1, new_path[i2], new_path[i1 - 1], new_path[i1 + 1]) \n            new_i1i2  += cost_around_point(i2, new_path[i1], new_path[i2 - 1], new_path[i2 + 1])\n\n            tentative_score = new_score - prev_i1i2 + new_i1i2\n\n            if (tentative_score < new_score) or (np.random.rand() < np.exp((new_score - tentative_score) \/ T )):\n                scores.append(tentative_score)\n                tried_swaps.append((i1,i2))\n                if tentative_score > new_score:\n                    annealing_steps += 1\n                new_path[i1], new_path[i2] = new_path[i2], new_path[i1]\n                new_score = tentative_score\n        if len(scores) > 0:\n            score_history['T'].append(T)\n            score_history['avg_dists'].append(np.average(scores))\n            score_history['anneals'].append(annealing_steps)\n        deltas.append(delta)\n    return new_path, new_score, pd.DataFrame(score_history)","ca64805d":"new_best_path, score, score_history = simulated_annealing(best_path, score)","ae6592d4":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,10))\n_ = score_history.plot(x='T', y='avg_dists',ax=axes[0])\n_ = score_history.plot(x='T', y='anneals',ax=axes[1])\naxes[0].invert_xaxis()\naxes[1].invert_xaxis()","93bbb221":"plt.figure(figsize=(80,30))\nplt.subplot(1,2,1)\nplt.plot(city_coordinates[best_path, 0], city_coordinates[best_path, 1])\n_ = plt.title(\"Original solution\", fontsize=40)\nplt.subplot(1,2,2)\nplt.plot(city_coordinates[new_best_path, 0], city_coordinates[best_path, 1])\n_ = plt.title(\"Improved solution\", fontsize=40)","749bfb90":"submission = pd.DataFrame({\"Path\": new_best_path})\nsubmission.to_csv(\"submission.csv\", index=None)","eb8e653a":"# Disclaimer\nI am very sceptical that these techniques are in any way competitionally viable. But I'm a newbie at writing up public notebooks, so I wanna have some fun instead!\nLet's see how far we can get by using a best-first search (in reverse order, to properly account for the 10% penalty when leaving non-prime id cities every 10th step) and improving it with simulated annealing.","0ec0aa38":"# Getting the data","2ae8293e":"# (Backwards) Best First Search\n\n## Motivation\nWe first generate a \"good enough\" solution using a best first search. This means that we just greedily choose at every step (starting from the North Pole) the nearest unvisited city. However, the problem is complicated by the fact that the distance at every 10th step is 10% larger if **starting** from a city that has a prime number Id.\nIt actually makes more sense to build the route backwards. That way, you're treating the prime Id cities as targets instead of starting points.\n\n## Boring implementation details\nTo make this faster, we use vectorised operations. Using numpy's broadcasting rules, you can compute the distance from one city to all other cities in only a couple of operations - assuming you have all the coordinates as rows (or columns) in a 2D array, resulting in a 1D array. of distances Every 10th step, these distances need to be multiplied for non-prime index cities. \n\nRemoving a city from the list of visited cities (and its coordinates from the coordinates 2D array) takes a bit too long (perhaps using np.delete would mitigate that). Instead, we use a simple trick: keep track of how many unvisited cities there are, and whenever we want to remove a city from a given index, we just overwrite the city at that index with the one at the end of the list (or we can swap them around).  We also need to make sure to only use *slices* with size equal to the number of unvisited cities. Note that due to the fact that np.argmin already has O(n) complexity, using this trick doesn't lower the *complexity class* of the solution, that's still O(n^2)\n\n","8f479787":"# Precompute which city ids are prime (and not prime)\n","fae3e6ee":"# Simulated annealing\n\nThe Traveling Santa Problem is NP-complete, so no **polynomial complexity** algorithms exist to solve it exactly. A better approach is to start with an approximate solution (see above) and then use heuristics to try to improve it (i.e. 2-opt, 3-opt, Lin-Kernighan). These methods work really well, so let's try something stupid instead: simulated annealing!\n   \nAnnealing is a process in metallurgy, where a metal is heated to a high temperature and then let to cool down slowly - making it stronger in the process. By analogy, simulated annealing is an optimisation technique which relies on an artificial 'temperature'. The temperature is used to determine the probability  ( in the form of $e^{({cost}_{new}-{cost}_{old})\/T}$) that a neighbouring solution which is worse than the current solution gets chosen, and is iteratively decayed by a small factor. The exponent will always be negative, so the probability is proportional with T.  If the neighboring solution being explored improves the current solution, it is selected by default (no probability computations).\nThe motivation for choosing bad solutions in the beginning is that it makes it less likely to get stuck in a local optima, consequently making it more likely to reach the global optima.\n\n## More boring implementation details\n\nFrom a given solution, we randomly try swapping two cities on the route, and see how that changes the total distance. If the two cities to exchange are, say, $city_i$ and $city_j$ (where $i$ and $j$ represent their indices in the current solution), then the delta in distance can be determined by only looking at their immediate neighbors (i.e. $city_{i-1}$, $city_{i+1}$, $city_{j-1}$ and $city_{j+1}$ ) before and after the relocation. \nTo make computations less prone to corner cases, we sample  $i$ and $j$ such that $j = i + \\delta$, where $\\delta > 3$. $i$ is sampled from a uniform random distribution, whereas $\\delta$ is sampled from a Poisson distribution with $\\lambda$ values which cycle between 3, and 100. \nThere is no mathematical justification for this, it just seemed like a cool idea. I just noticed that cycling between using small values and large values helps make sure that new optima get found.\n\n"}}