{"cell_type":{"3793f621":"code","e2d22d10":"code","8c803d87":"code","d37b834e":"code","07bd405d":"code","2d22ab9d":"code","b3067128":"code","b51d7c3d":"code","ad893135":"code","81524c88":"code","0e7b83ea":"code","9ad0c5db":"code","16de6a1e":"code","96fb3d33":"code","a0ee66e5":"code","7e0a41fb":"code","68ea86a2":"code","22526f49":"code","cc21996d":"code","e69f6a6c":"code","0b71d11e":"code","5f1e722b":"code","7d230e34":"code","1c973825":"code","0d36e6fb":"code","756ee4b4":"code","0deb8ee8":"code","674082d9":"code","7ed80463":"code","be1e60da":"code","3b5f22ea":"code","80e84796":"code","715ff2c9":"code","657fc385":"code","ac599416":"code","1614278d":"markdown","d81c97ad":"markdown","b364e7a3":"markdown","f34fba04":"markdown","ceb6dd96":"markdown","dddbf22c":"markdown","38c2abe7":"markdown","6a9b86fa":"markdown","2404e263":"markdown","9ae32853":"markdown","f3d087e3":"markdown","e0a30edc":"markdown","bf99a715":"markdown","1582e0f4":"markdown","80647752":"markdown","ae61d691":"markdown","165be78a":"markdown","3b9efc46":"markdown","6de20fa5":"markdown","8583fdad":"markdown","0d42af3c":"markdown","2665576a":"markdown","a9c950a3":"markdown","3679fac7":"markdown","32c40684":"markdown","6fb13a75":"markdown","432794fd":"markdown"},"source":{"3793f621":"## Import all required libraries\nimport numpy as np \nimport pandas as pd \nimport json\nimport os\nfrom tqdm import tqdm,tqdm_notebook\nimport gc","e2d22d10":"# Fetching all the json files from Kaggle which contains research papers\n\n# this finds our json files\npath_to_json = '\/kaggle\/input\/CORD-19-research-challenge\/document_parses\/pdf_json'\njson_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\njson_files","8c803d87":"len(json_files)","d37b834e":"# here I define my pandas Dataframe with the columns I want to get from the json\n# jsons_data = pd.DataFrame(columns=['country', 'city', 'long\/lat'])\njsons_data = pd.DataFrame(columns=['paper_id', 'abstract', 'body_text'])\n\nid2abstract = []\n# we need both the json and an index number so use enumerate()\nfor index, js in enumerate(json_files[:1000]):     # Using 1000 files only to reduce memory load and resources\n    with open(os.path.join(path_to_json, js)) as json_file:\n        json_text = json.load(json_file)\n\n        # here you need to know the layout of your json and each json has to have\n        # the same structure (obviously not the structure I have here)\n        paper_id = json_text['paper_id']\n#         abstract = json_text['abstract'][0]['text']\n        abstract=''\n        for entry in json_text['abstract']:\n                abstract += entry['text']\n        id2abstract.append({paper_id:abstract})\n                #abstract.append(entry['text'])\n        body_text=\"\"\n        for entry in json_text['body_text']:\n                body_text += entry['text']\n                #body_text.append(entry['text'])\n                \n        # here I push a list of data into a pandas DataFrame at row given by 'index'\n        jsons_data.loc[index] = [paper_id, abstract, body_text]\n\n# now that we have the pertinent json data in our DataFrame let's look at it\nprint(jsons_data)","07bd405d":"# Display the dataframe\njsons_data","2d22ab9d":"df = jsons_data","b3067128":"# Describe the New processed Dataframe\ndf.describe()","b51d7c3d":"# # Converting the columns in list to string for easy processing of the columns while tokenising\n# df['abstract'] = [' '.join(map(str, l)) for l in df['abstract']]\n# df['body_text'] = [' '.join(map(str, l)) for l in df['body_text']]\n# df.head()","ad893135":"# Count of all tokens in the columns and unique words in the columns\ndf['abstract_word_count'] = df['abstract'].apply(lambda x: len(x.strip().split()))  # word count in abstract\ndf['body_word_count'] = df['body_text'].apply(lambda x: len(x.strip().split()))  # word count in body\ndf['body_unique_words']=df['body_text'].apply(lambda x:len(set(str(x).split())))  # number of unique words in body\ndf.head()","81524c88":"# Check if NULL values are present in the dataset along with count of the NULL values\nfor col in df.columns:\n    print(col, df[col].isnull().sum())","0e7b83ea":"# Thanks to https:\/\/www.codegrepper.com\/code-examples\/delphi\/delete+rows+pandas+if+column+contains\n# Thanks to https:\/\/stackoverflow.com\/questions\/49841989\/python-drop-value-0-row-in-specific-columns\n\n# Let\u2019s delete all rows for whose word count is 0\ndf.drop(df.index[df['abstract_word_count'] == 0], inplace = True)\ndf.drop(df.index[df['body_word_count'] == 0], inplace = True)\n# df = df.loc[~((df['abstract_word_count'] == 0) | (df['body_word_count'] == 0))]\ndf","9ad0c5db":"# Converting strings to Lower case\ndf[\"abstract\"] = df[\"abstract\"].str.lower() \ndf[\"body_text\"] = df[\"body_text\"].str.lower() \ndf.head(10)","16de6a1e":"### load stopwords \nimport nltk\nnltk.download('stopwords')","96fb3d33":"# Thanks to https:\/\/medium.com\/analytics-vidhya\/text-summarization-on-covid-19-research-data-a5ab28695e11\n\nstopwords_custom = nltk.corpus.stopwords.words('english')\ncustomize_stop_words = ['common','review','describes','abstract','retrospective','chart','patients','study','may',\n                        'associated','results','including','high''found','one','well','among','Abstract','provide',\n                        'objective','objective:','background','range','features','participates','doi', 'preprint', \n                        'copyright', 'org', 'https', 'et ','et' 'al', 'author', 'figure', 'table', 'rights', 'reserved', \n                        'permission', 'use', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', \n                        'Elsevier', 'PMC', 'CZI']\n### append custom stopwords to default stopwords from NLTK\nfor i in customize_stop_words:\n    stopwords_custom.append(i)","a0ee66e5":"df['abstract'] = df['abstract'].apply(lambda x: \" \".join([word for word in x.split() if word not in (stopwords_custom)]))\ndf['body_text'] = df['body_text'].apply(lambda x: \" \".join([word for word in x.split() if word not in (stopwords_custom)]))\ndf","7e0a41fb":"# Installing the Contractions Package\n!pip install contractions","68ea86a2":"# Using contractions library for replacing the contracted words in English \nimport contractions\ndf['abstract'] = df['abstract'].apply(lambda x: \" \".join([contractions.fix(word) for word in x.split()]))\ndf['body_text'] = df['body_text'].apply(lambda x: \" \".join([contractions.fix(word) for word in x.split()]))\ndf.head()","22526f49":"from nltk.tokenize import RegexpTokenizer\ntokenizer_pattern = RegexpTokenizer('\\w+')\ndf['abstract'] = df['abstract'].apply(lambda x: \" \".join(tokenizer_pattern.tokenize(x.lower())))\ndf['body_text'] = df['body_text'].apply(lambda x: \" \".join(tokenizer_pattern.tokenize(x.lower())))\ndf","cc21996d":"df = df.reset_index(drop=True)\ndf","e69f6a6c":"# Thanks to https:\/\/towardsdatascience.com\/simple-wordcloud-in-python-2ae54a9f58e5\n\n# Import packages\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Define a function to plot word cloud\ndef plot_cloud(wordcloud):\n    # Set figure size\n    plt.figure(figsize=(40, 30))\n    # Display image\n    plt.imshow(wordcloud) \n    # No axis details\n    plt.axis(\"off\");","0b71d11e":"abstract_text = str(df.abstract)\n\n# Import package\nfrom wordcloud import WordCloud\n# Generate word cloud\nwordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='black', colormap='Set2', collocations=False, stopwords = stopwords_custom).generate(abstract_text)\n# Plot\nplot_cloud(wordcloud)","5f1e722b":"text = str(df.body_text)\n\n# Import package\nfrom wordcloud import WordCloud\n# Generate word cloud\nwordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='black', colormap='Set2', collocations=False, stopwords = stopwords_custom).generate(text)\n# Plot\nplot_cloud(wordcloud)","7d230e34":"# from transformers import pipeline\n# summarizer = pipeline(\"summarization\")","1c973825":"# # Summarization\n# for i, text in enumerate(df_test['abstract']):\n#     summary_text = summarizer(df_test['abstract'].iloc[i], max_length=1000, min_length=30)\n#     print(summary_text)\n#     print()\n#     df_test['abstract'].iloc[i] = summary_text[0]['summary_text']\n#     #print(summary_text[0]['summary_text'])","0d36e6fb":"# BERT QA model\nfrom transformers import pipeline\nnlp = pipeline('question-answering',model = 'bert-large-cased-whole-word-masking-finetuned-squad')","756ee4b4":"query_sample = \"How to prevent Corona ?\"\nrelevant_sentence = df['abstract'].values\nnlp(question = query_sample, context = relevant_sentence)","0deb8ee8":"from nltk.translate.bleu_score import sentence_bleu   # Library to compare two sentences and calculate score using BLEU\nfrom nltk.tokenize import word_tokenize  # NLTK library to tokenize the expected and acquired results\n\ndef compare_bleu(result, expected):    \n    token_res = word_tokenize(result)    # Tokenize the acquired results\n    token_exp = word_tokenize(expected)  # Tokenize the expected results\n    score = sentence_bleu(token_res, token_exp)   # Compare two sentences and calculate score\n    print(\"The BLEU score of accuracy is: \", score)","674082d9":"# Define the expected answer to a new variable\nexpected_Summary_text_1 = \"Coronavirus disease 2019 (COVID-19) is a respiratory illness that can spread from person to person. CDC has information on COVID-19 symptoms and caring for yourself and others. COVID-19 is a new disease, caused by a novel (or new) coronavirus that has not previously been seen in humans. A novel coronavirus is a new coronavirus that has not been previously identified. The virus causing coronavirus disease 2019 (COVID-19), is not the same as the coronaviruses that commonly circulate among humans and cause mild illness, like the common cold.\"\nexpected_Summary_text_2 = \"The incubation period for COVID-19 is thought to extend to 14 days, with a median time of 4-5 days from exposure to symptoms onset. One study reported that 97.5% of persons with COVID-19 who develop symptoms will do so within 11.5 days of SARS-CoV-2 infection.\"","7ed80463":"compare_bleu(Summary_text_4, expected_Summary_text_1)","be1e60da":"compare_bleu(Summary_text_5, expected_Summary_text_2)","3b5f22ea":"# Thanks to https:\/\/pypi.org\/project\/rouge-score\/\n!pip install rouge-score","80e84796":"from rouge_score import rouge_scorer\n\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\nscores = scorer.score(Summary_text_4, expected_Summary_text_1)\nscores","715ff2c9":"scores = scorer.score(Summary_text_5, expected_Summary_text_2)\nscores","657fc385":"from transformers import pipeline\nsummarizer = pipeline(\"summarization\")","ac599416":"def Summary_Model(pd,count,model):\n    summary_text = ''\n    total_abstract = ''\n    for i in range(len(pd[:count])):\n        abss, ans = pd.loc[i,['abstract_by_ans','Answer']].values\n        total_abstract += (abss + \".\")\n    ARTICLE_TO_SUMMARIZE = total_abstract\n    print(ARTICLE_TO_SUMMARIZE)\n    summary_text = summarizer(ARTICLE_TO_SUMMARIZE)\n    return summary_text","1614278d":"# Method - 3 BERT QA & BERT Pipeline Summarization","d81c97ad":"### Delete all rows for whose word count is 0","b364e7a3":"# Data Description\nThis dataset was created by the Allen Institute for AI in partnership with the Chan Zuckerberg Initiative, Georgetown University\u2019s Center for Security and Emerging Technology, Microsoft Research, IBM, and the National Library of Medicine - National Institutes of Health, in coordination with The White House Office of Science and Technology Policy. This is a periodically updated dataset and as of now it is of 25.35 GB.","f34fba04":"### Reasoning for Converting to Lower case\n\nWe are converting all the strings to lower case because, when training the model, it should not treat the capitalized words and non-capitalized words differently which may decrease the accuracy of the model later. To treat them equal, we are converting all of the string to Lower case.","ceb6dd96":"# Data Source\nThe dataset is acquired from Kaggle competition \"COVID-19 Open Research Dataset Challenge (CORD-19)\"\n\nLink to the dataset: https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge","dddbf22c":"# Data Preprocessing","38c2abe7":"Below image represents the BLEU and ROUGE score for this model","6a9b86fa":"# BERT QA Model & Text summarization\n\n### Reasoning for the models chosen\n\n#### Why BERT?\n\nBERT (Bidirectional Encoder Representations from Transformers) is a transformer-based method of learning language representations. It is a bidirectional transformer pre-trained model developed using a combination of two tasks namely: masked language modeling objective and next sentence prediction on a large corpus. Since we are dealing with large quantity of text corpus (COVID related articles) and BERT also had many pre-trained QA models (pipelines) which are trained on huge quantity of data. There are also models like \"deepset\/covid_bert_base\" (trained on COVID data), \"dmis-lab\/biobert-base-cased-v1.1\" (trained on medicine data) which supports QA modeling along with the sufficient data. So we opted out for BERT.\n\n#### Why bert-large-cased-whole-word-masking-finetuned-squad?\n\nThe models that QA pipeline use is the model that have been fine-tuned on a question answering task. (Source: huggingface). Since we need to build QA model we opted for this model. It will be somewhat easy to fine tune a pre trained model according to our requirements instead of writing all the deep learning codes and training that model on huge and quality data to perform well. So we used pre-trained QA model and fine tuned according to our requirements of the dataset.\n\n#### Why Summarization?\n\nSummarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content. Summarization has been used many practical applications \u2014 summarizing articles, summarizing multiple documents on the same topic, summarizing video content to generate highlights in sports etc. Summarization in text, specifically, can be extremely useful as it saves the users having to read long articles, blogs or other pieces of text. \n\n#### Why transformers Summarization pipeline model (transformers.QuestionAnsweringPipeline)?\nFor the process of summarization we used the pre tr****ained summarizer from transformers library. We used this model because its implementation is very simple and easy to understand and also it is from transformers library where BERT also belongs to. So we thought they could complement each other to tune the model and chode it.","2404e263":"## Contraction Words","9ae32853":"![WhatsApp%20Image%202020-12-12%20at%207.09.19%20PM.jpeg](attachment:WhatsApp%20Image%202020-12-12%20at%207.09.19%20PM.jpeg)","f3d087e3":"![2.jpeg](attachment:2.jpeg)","e0a30edc":"![1.jpeg](attachment:1.jpeg)","bf99a715":"### Reasoning for Replacing Contraction English words\n\nFor example,Who're you --> Who are you; Both sentences give the same meaning. In order to make the model understand that both are same and it should not treat them as seperate, we are replacing the contracted words.","1582e0f4":"## ROUGE Score","80647752":"## Lower Case","ae61d691":"## Stop Words","165be78a":"## Check NULL Values","3b9efc46":"# Data Visualization\n\n## Word Cloud","6de20fa5":"# Fetch the Dataset from Kaggle","8583fdad":"Word cloud is a technique for visualising frequent words in a text where the size of the words represents their frequency.","0d42af3c":"## Word count","2665576a":"### Reasoning for removing Stopwords\n\nStop words are a set of commonly used words in any language. For example, in English, \u201cthe\u201d, \u201cis\u201d and \u201cand\u201d, would easily qualify as stop words. In NLP and text mining applications, stop words are used to eliminate unimportant words, allowing applications to focus on the important words instead.\n\nThe common language specific stop word list generally DOES NOT cover such domain specific terms. Similarly, for tweets, terms like \u201c#\u201d \u201cRT\u201d, \u201c@username\u201d can be potentially regarded as stop words.\n\nIn our dataset, we are dealing with research papers. So the words like 'doi', 'copyright','org', 'https', 'et', 'al', 'author', 'figure', 'rights', 'reserved', 'permission', 'fig', etc. are used more frequently which will not be considered as common stopwords in English. Hence we're customizing the stopwords list.","a9c950a3":"# Exploratory Data Analysis (EDA)","3679fac7":"## BLEU Score","32c40684":"## Remove punctuation marks","6fb13a75":"# References\n\n- BERTScore: https:\/\/github.com\/Tiiiger\/bert_score\n- BLEU Score: https:\/\/machinelearningmastery.com\/calculate-bleu-score-for-text-python\/#:~:text=Crash%2DCourse%20Now-,Bilingual%20Evaluation%20Understudy%20Score,in%20a%20score%20of%200.0.\n- QA Using bert pretrained model by SQuAD: https:\/\/github.com\/google-research\/bert\n- All tasks list: https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/tasks\n- BERT model: https:\/\/www.kaggle.com\/dirktheeng\/anserini-bert-squad-for-semantic-corpus-search\n- Dataset: https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge","432794fd":"### Reasoning for Checking for NULL values\nWe have checked for NULL values, we need to remove all the NULL values before we train the model. This is because, NULL values don't have any values and they may cause a problem when the model is predicting."}}