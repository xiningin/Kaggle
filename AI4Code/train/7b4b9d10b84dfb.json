{"cell_type":{"e5a90372":"code","0d6e80b7":"code","7c62cf85":"code","80ded72a":"code","904ebd45":"code","301da531":"code","c31e7a8c":"code","c4b75b3e":"code","63c267e8":"code","9fb38b7d":"code","4baa2cbe":"code","04ada30a":"code","8441ff3b":"code","31077c5d":"code","3caff2c2":"code","0686c3ab":"code","b4dfde2e":"code","ae98cf3c":"code","2daedecc":"code","c05a42d9":"code","ef649f6a":"code","1e55cbc0":"markdown","750e5c39":"markdown","48d0be0a":"markdown","ee309b45":"markdown","956af1fd":"markdown"},"source":{"e5a90372":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport math\npd.set_option('display.max_colwidth', -1)\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0d6e80b7":"! pip install faiss-gpu ##Installing GPU version of faiss\n\n","7c62cf85":"! pip install sentence_transformers ## For textual similarity, using pretrained models","80ded72a":"import faiss\nfrom sentence_transformers import SentenceTransformer, util","904ebd45":"model = SentenceTransformer('paraphrase-distilroberta-base-v1',device=\"cuda\") ## On GPU Loads the distil roberta model,whcih was trained on millions of data","301da531":"imdb_movies=pd.read_csv(\"\/kaggle\/input\/imdb-extensive-dataset\/IMDb movies.csv\")\nimdb_movies.shape","c31e7a8c":"imdb_movies.head()","c4b75b3e":"## Dropping off rows where Movie Description is NULL\nimdb_movies=imdb_movies[pd.notnull(imdb_movies['description'])]\nimdb_movies=imdb_movies.reset_index(drop=True)","63c267e8":"imdb_movies['id']=imdb_movies.index\n","9fb38b7d":"imdb_movies.head()","4baa2cbe":"sentences=imdb_movies['description'].tolist()\nprint(\"Number of Sentences in Movie Description \",len(sentences))","04ada30a":"embeddings=model.encode(sentences)\nfaiss.normalize_L2(embeddings) ## Normalising the Embeddings","8441ff3b":"print(\"Shape of the EMbeddings is \",embeddings.shape)","31077c5d":"## We get a 768 dimension vector using Roberta. So we will create FAISS index with dimaensions - 768\n\ndim=768\nncentroids=50 ## This is a hyperparameter, and indicates number of clusters to be split into\nm=16 ## This is also a hyper parameter\nquantiser = faiss.IndexFlatL2(dim)\nindex = faiss.IndexIVFPQ (quantiser, dim,ncentroids, m , 8)\nindex.train(embeddings) ## This step, will do the clustering and create the clusters\nprint(index.is_trained)\nfaiss.write_index(index, \"trained.index\")\n","3caff2c2":"### We have to add the embeddings to the Trained Index.\nids=imdb_movies['id'].tolist()\nids=np.array(ids)\nindex.add_with_ids(embeddings,ids)\nprint(index.ntotal)\n\n","0686c3ab":"faiss.write_index(index,\"block.index\")\n","b4dfde2e":"def searchFAISSIndex(data,id_col_name,query,index,nprobe,model,topk=20):\n    ## Convert the query into embeddings\n    query_embedding=model.encode([query])[0]\n    dim=query_embedding.shape[0]\n    query_embedding=query_embedding.reshape(1,dim)\n    faiss.normalize_L2(query_embedding)\n  \n    \n    index.nprobe=nprobe\n    \n    D,I=index.search(query_embedding,topk) \n    ids=[i for i in I][0]\n    L2_score=[d for d in D][0]\n    inner_product=[calculateInnerProduct(l2) for l2 in L2_score]\n    search_result=pd.DataFrame()\n    search_result[id_col_name]=ids\n    search_result['cosine_sim']=inner_product\n    search_result['L2_score']=L2_score\n    dat=data[data[id_col_name].isin(ids)]\n    dat=pd.merge(dat,search_result,on=id_col_name)\n    dat=dat.sort_values('cosine_sim',ascending=False)\n    return dat","ae98cf3c":"\ndef calculateInnerProduct(L2_score):\n    return (2-math.pow(L2_score,2))\/2","2daedecc":"query=\"A seventeen-year-old aristocrat falls in love with a kind but poor artist\"\nsearch_result=searchFAISSIndex(imdb_movies,\"id\",query,index,nprobe=10,model=model,topk=20)\nsearch_result=search_result[['id','description','title','cosine_sim','L2_score']]","c05a42d9":"search_result","ef649f6a":"query=\"Former Football player  to train an  football team\"\nsearch_result=searchFAISSIndex(imdb_movies,\"id\",query,index,nprobe=10,model=model,topk=20)\nsearch_result=search_result[['id','description','title','cosine_sim','L2_score']]\nsearch_result","1e55cbc0":"The data contains description columns -which we will use to search for similar movie by plot summaries. Sentence Transformers will be used to encode sentences into FAISS. To encode into FAISS, we need to have an unique ID (numeric) assigned to each sentence. For this we will use the index of the row of the movie description","750e5c39":"### Let us use the FAISS index to search for similar movie plots","48d0be0a":"## Extract the Embeddings for movie description","ee309b45":"### Load the movies data","956af1fd":"**There are many types of FAISS Indices - you can use a Simple FLAT Index, or you can use the concept of Inverted Index and Product Quantisation to index the data.**\n\n**IVF with Product Quantisation, allows us to efficiently compress the data - it uses nearest neighbour search**\n\nWith IVF with PRoduct Quantisation, the vectors are first partitioned into clusters using k-means  => This step is known as training the index."}}