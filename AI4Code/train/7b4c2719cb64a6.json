{"cell_type":{"bf872abd":"code","b91cd33a":"code","b81a4559":"code","eb04a984":"code","69f934d1":"code","094366ab":"code","db26aa86":"code","749e63b2":"code","e5c44623":"code","a77bf79c":"code","9535b44d":"code","5a80ce68":"code","4d4c0bdb":"code","1c1dfcd2":"code","015ac89c":"markdown"},"source":{"bf872abd":"import os\nimport gc\nimport sys\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data import RandomSampler, SequentialSampler, Sampler\nfrom torch.nn.functional import mse_loss\nfrom transformers import AutoModel,AutoTokenizer,get_cosine_schedule_with_warmup, AutoConfig, AdamW\nfrom torch.cuda.amp import autocast, GradScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn-talk')\n# print(plt.style.available)\nfrom time import time\nfrom colorama import Fore, Back, Style\nr_ = Fore.RED\nb_ = Fore.BLUE\ng_ = Fore.GREEN\ny_ = Fore.YELLOW\nw_ = Fore.WHITE\nbb_ = Back.BLACK\nsr_ = Style.RESET_ALL","b91cd33a":"from IPython.core.magic import register_cell_magic\nimport os\nfrom pathlib import Path\n\nPath('\/kaggle\/working\/scripts').mkdir(exist_ok=True)\nmodels_dir = Path('\/kaggle\/working\/models')\nmodels_dir.mkdir(exist_ok=True)","b81a4559":"class Config:\n    model_name = '..\/input\/hatebert\/hateBERT'\n    output_hidden_states = True\n    epochs = 1\n#     evaluate_interval = 40\n    batch_size = 32\n    device = 'cuda'\n    seed = 42\n    max_len = 300\n    lr = 2e-5\n    wd = 0.01\n#     eval_schedule = [(float('inf'), 40), (0.5, 30), (0.49, 20), (0.48, 10), (0.47, 3), (0, 0)]\n    eval_schedule = [(float('inf'), 40), (0.47, 20), (0.46, 10), (0, 0)]\n\n    gradient_accumulation = 2","eb04a984":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=Config.seed)","69f934d1":"seed = 42\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \n    \ndef show_hist(df):\n    hist_df = df.bins.value_counts()*100\/df.shape[0]\n    hist_df = hist_df.sort_index()\n    ax = sns.barplot(list(map(str,hist_df.index)),hist_df.values) \n    ax.set_xlabel(\"bin num\")\n    ax.set_ylabel(\"size\")\n    plt.show()\n    \ntrain_data = pd.read_csv('..\/input\/jigsaw-regression-based-data\/train_data_version2.csv')\ntrain_data.columns = ['text','target']\nnum_bins = int(np.floor(1 + np.log2(len(train_data))))\ntrain_data.loc[:,'bins'] = pd.cut(train_data['target'], bins=num_bins,labels=False)\nbins = train_data.bins.to_numpy()\ntarget = train_data.target.to_numpy()\n# print(train_data.bins.value_counts())\nshow_hist(train_data)","094366ab":"from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, StratifiedKFold\ntrain_data['fold'] = 0\nkfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\nfor fold, (train_idx,valid_idx) in enumerate(kfold.split(X=train_data, y=bins)):\n    train_data.loc[valid_idx,'fold'] = fold\n    show_hist(train_data.loc[valid_idx])\ntrain_data.fold.value_counts()\n\ntrain_data.to_csv('kfold.csv', index=False)","db26aa86":"kfold_df = pd.read_csv('kfold.csv')","749e63b2":"from torch.utils.data import Dataset\nimport torch\n\ndef convert_examples_to_features(text, tokenizer, max_len):\n\n    tok = tokenizer.encode_plus(\n        text, \n        max_length=max_len, \n        truncation=True,\n        padding='max_length',\n    )\n    return tok\n\n\nclass JIGSAWDataset(Dataset):\n    def __init__(self, data, tokenizer, max_len, is_test=False):\n        self.data = data\n        self.excerpts = self.data.text.tolist()\n        if not is_test:\n            self.targets = self.data.target.tolist()\n            \n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        if not self.is_test:\n            excerpt = self.excerpts[item]\n            label = self.targets[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, self.max_len\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n                'label':torch.tensor(label, dtype=torch.float),\n            }\n        else:\n            excerpt = self.excerpts[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, self.max_len\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n            }\n","e5c44623":"import torch\nimport torch.nn as nn\n\nclass AttentionHead(nn.Module):\n    def __init__(self, h_size, hidden_dim=512):\n        super().__init__()\n        self.W = nn.Linear(h_size, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        \n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n\nclass JigSawModel(nn.Module):\n    def __init__(self,transformer,config):\n        super(JigSawModel,self).__init__()\n        self.h_size = config.hidden_size\n        self.transformer = transformer\n        self.head = AttentionHead(self.h_size*4)\n        self.linear = nn.Linear(self.h_size*2, 1)\n        self.linear_out = nn.Linear(self.h_size*8, 1)\n\n              \n    def forward(self, input_ids, attention_mask):\n        transformer_out = self.transformer(input_ids, attention_mask)\n       \n        all_hidden_states = torch.stack(transformer_out.hidden_states)\n        cat_over_last_layers = torch.cat(\n            (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),-1\n        )\n        \n        cls_pooling = cat_over_last_layers[:, 0]   \n        head_logits = self.head(cat_over_last_layers)\n        y_hat = self.linear_out(torch.cat([head_logits, cls_pooling], -1))\n        \n        return y_hat","a77bf79c":"def create_optimizer(model):\n    named_parameters = list(model.named_parameters())    \n    \n    roberta_parameters = named_parameters[:389]    \n    attention_parameters = named_parameters[391:395]\n    regressor_parameters = named_parameters[395:]\n        \n    attention_group = [params for (name, params) in attention_parameters]\n    regressor_group = [params for (name, params) in regressor_parameters]\n\n    parameters = []\n    parameters.append({\"params\": attention_group})\n    parameters.append({\"params\": regressor_group})\n    \n    # increase lr every second layer\n    increase_lr_every_k_layer = 1\n    lrs = np.linspace(1, 5, 24 \/\/ increase_lr_every_k_layer)\n    for layer_num, (name, params) in enumerate(roberta_parameters):\n        weight_decay = 0.0 if \"bias\" in name else 0.01\n        splitted_name = name.split('.')\n        lr = Config.lr\n        if len(splitted_name) >= 4 and str.isdigit(splitted_name[3]):\n            layer_num = int(splitted_name[3])\n            lr = lrs[layer_num \/\/ increase_lr_every_k_layer] * Config.lr \n\n        parameters.append({\"params\": params,\n                           \"weight_decay\": weight_decay,\n                           \"lr\": lr})\n\n    return optim.AdamW(parameters)","9535b44d":"class DynamicPadCollate:\n    def __call__(self,batch):\n                \n        out = {'input_ids' :[],\n               'attention_mask':[],\n                'label':[]\n        }\n        \n        for i in batch:\n            for k,v in i.items():\n                out[k].append(v)\n                \n        max_pad =0\n\n        for p in out['input_ids']:\n            if max_pad < len(p):\n                max_pad = len(p)\n                    \n\n        for i in range(len(batch)):\n            \n            input_id = out['input_ids'][i]\n            att_mask = out['attention_mask'][i]\n            text_len = len(input_id)\n            \n            out['input_ids'][i] = (out['input_ids'][i].tolist() + [1] * (max_pad - text_len))[:max_pad]\n            out['attention_mask'][i] = (out['attention_mask'][i].tolist() + [0] * (max_pad - text_len))[:max_pad]\n        \n        out['input_ids'] = torch.tensor(out['input_ids'],dtype=torch.long)\n        out['attention_mask'] = torch.tensor(out['attention_mask'],dtype=torch.long)\n        out['label'] = torch.tensor(out['label'],dtype=torch.float)\n        \n        return out\n\n\nclass AvgCounter:\n    def __init__(self):\n        self.reset()\n        \n    def update(self, loss, n_samples):\n        self.loss += loss * n_samples\n        self.n_samples += n_samples\n        \n    def avg(self):\n        return self.loss \/ self.n_samples\n    \n    def reset(self):\n        self.loss = 0\n        self.n_samples = 0\n\nclass EvaluationScheduler:\n    def __init__(self, evaluation_schedule, penalize_factor=1, max_penalty=8):\n        self.evaluation_schedule = evaluation_schedule\n        self.evaluation_interval = self.evaluation_schedule[0][1]\n        self.last_evaluation_step = 0\n        self.prev_loss = float('inf')\n        self.penalize_factor = penalize_factor\n        self.penalty = 0\n        self.prev_interval = -1\n        self.max_penalty = max_penalty\n\n    def step(self, step):\n        # should we to make evaluation right now\n        if step >= self.last_evaluation_step + self.evaluation_interval:\n            self.last_evaluation_step = step\n            return True\n        else:\n            return False\n        \n            \n    def update_evaluation_interval(self, last_loss):\n        # set up evaluation_interval depending on loss value\n        cur_interval = -1\n        for i, (loss, interval) in enumerate(self.evaluation_schedule[:-1]):\n            if self.evaluation_schedule[i+1][0] < last_loss < loss:\n                self.evaluation_interval = interval\n                cur_interval = i\n                break\n#         if last_loss > self.prev_loss and self.prev_interval == cur_interval:\n#             self.penalty += self.penalize_factor\n#             self.penalty = min(self.penalty, self.max_penalty)\n#             self.evaluation_interval += self.penalty\n#         else:\n#             self.penalty = 0\n            \n        self.prev_loss = last_loss\n        self.prev_interval = cur_interval\n        \n          \n        \ndef make_dataloader(data, tokenizer, is_train=True):\n    dataset = JIGSAWDataset(data, tokenizer=tokenizer, max_len=Config.max_len)\n    if is_train:\n        sampler = RandomSampler(dataset)\n    else:\n        sampler = SequentialSampler(dataset)\n\n    batch_dataloader = DataLoader(dataset, sampler=sampler, batch_size=Config.batch_size, pin_memory=True, collate_fn=DynamicPadCollate())\n    return batch_dataloader\n                   \n            \nclass Trainer:\n    def __init__(self, train_dl, val_dl, model, optimizer, scheduler, scaler, criterion, model_num):\n        self.train_dl = train_dl\n        self.val_dl = val_dl\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.device = Config.device\n        self.batches_per_epoch = len(self.train_dl)\n        self.total_batch_steps = self.batches_per_epoch * Config.epochs\n        self.criterion = criterion\n        self.model_num = model_num\n        \n        self.scaler = scaler\n                \n    def run(self):\n        record_info = {\n            'train_loss': [],\n            'val_loss': [],\n        }\n        \n        best_val_loss = float('inf')\n        evaluation_scheduler = EvaluationScheduler(Config.eval_schedule)\n        train_loss_counter = AvgCounter()\n        step = 0\n        \n        for epoch in range(Config.epochs):\n            \n            print(f'Epoch: {epoch+1}\/{Config.epochs}')\n            start_epoch_time = time()\n            \n            for batch_num, batch in enumerate(self.train_dl):\n                train_loss = self.train(batch, step)\n#                 print(f'{epoch+1}#[{step+1}\/{len(self.train_dl)}]: train loss - {train_loss.item()}')\n\n                train_loss_counter.update(train_loss, len(batch))\n                record_info['train_loss'].append((step, train_loss.item()))\n\n                if evaluation_scheduler.step(step):\n                    val_loss = self.evaluate()\n                    \n                    record_info['val_loss'].append((step, val_loss.item()))        \n                    print(f'{epoch+1}#[{batch_num+1}\/{self.batches_per_epoch}]: train loss - {train_loss_counter.avg()} | val loss - {val_loss}',)\n                    train_loss_counter.reset()\n\n                    if val_loss < best_val_loss:\n                        best_val_loss = val_loss.item()\n                        print(f\"Val loss decreased from {best_val_loss} to {val_loss}{sr_}\")\n                        torch.save(self.model, models_dir \/ f'best_model_{self.model_num}.pt')\n                        \n                    evaluation_scheduler.update_evaluation_interval(val_loss.item())\n                        \n            \n                step += 1\n            end_epoch_time = time()\n            print(f'The epoch took {end_epoch_time - start_epoch_time} sec..{sr_}')\n\n        return record_info, best_val_loss\n            \n\n    def train(self, batch, batch_step):\n        self.model.train()\n        sent_id, mask, labels = batch['input_ids'].to(self.device), batch['attention_mask'].to(self.device), batch['label'].to(self.device)\n        with autocast():\n            preds = self.model(sent_id, mask)\n            train_loss = self.criterion(preds, labels.unsqueeze(1))\n        \n        self.scaler.scale(train_loss).backward()\n#         train_loss.backward()\n        \n        if (batch_step + 1) % Config.gradient_accumulation or batch_step+1 == self.total_batch_steps:\n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n#             self.optimizer.step()\n            self.model.zero_grad() \n        self.scheduler.step()\n        return torch.sqrt(train_loss)\n\n    def evaluate(self):\n        self.model.eval()\n        val_loss_counter = AvgCounter()\n\n        for step,batch in enumerate(self.val_dl):\n            sent_id, mask, labels = batch['input_ids'].to(self.device), batch['attention_mask'].to(self.device), batch['label'].to(self.device)\n            with torch.no_grad():\n                with autocast():\n                    preds = self.model(sent_id, mask)\n                    loss = self.criterion(preds,labels.unsqueeze(1))\n                val_loss_counter.update(torch.sqrt(loss), len(labels))\n        return val_loss_counter.avg()\n    \n    \ndef mse_loss(y_true,y_pred):\n    return nn.functional.mse_loss(y_true,y_pred)","5a80ce68":"import os\nfrom pathlib import Path\nin_folder_path = Path('..\/input\/jigsaw-hatebert-regression')\n\ntest_df = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\ntokenizer = torch.load('..\/input\/mytokenizers\/bert-tokenizer.pt')\nmodels_folder_path = Path(in_folder_path \/ 'models')\nmodels_preds = []\nn_models = 5\n\nfor model_num in range(n_models):\n    print(f'Inference#{model_num+1}\/{n_models}')\n    test_ds = JIGSAWDataset(data=test_df, tokenizer=tokenizer, max_len=Config.max_len, is_test=True)\n    test_sampler = SequentialSampler(test_ds)\n    test_dataloader = DataLoader(test_ds, sampler = test_sampler, batch_size=Config.batch_size)\n    model = torch.load(models_folder_path \/ f'best_model_{model_num}.pt').to(Config.device)\n\n    all_preds = []\n    \n    model.eval()\n\n    for step,batch in enumerate(test_dataloader):\n        sent_id, mask = batch['input_ids'].to(Config.device), batch['attention_mask'].to(Config.device)\n        with torch.no_grad():\n            preds = model(sent_id, mask)\n            all_preds += preds.flatten().cpu().tolist()\n            \n    models_preds.append(all_preds)\ndel model, tokenizer, test_dataloader, test_sampler\ngc.collect()\ntorch.cuda.empty_cache()","4d4c0bdb":"test_df['score'] = np.array(models_preds).mean(axis=0)","1c1dfcd2":"from scipy.stats import rankdata\ntest_df[\"score\"] = rankdata(test_df[\"score\"], method='ordinal')\ntest_df[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)\n\n","015ac89c":"# Split Data"}}