{"cell_type":{"2e83668d":"code","7878c488":"code","e4eac2f2":"code","2d02136d":"code","8aa2a9ac":"code","2c3f3641":"code","76d9f9ac":"code","6ff57027":"code","ea20efd1":"code","2426fe64":"code","5a5965d3":"code","78a72f87":"code","cccc8ad6":"code","30e6d03e":"code","3838979f":"code","58a08525":"code","d4fdad2a":"code","66d8de36":"code","5c4470de":"code","7b8af8e4":"code","0e853bd2":"code","533e2db1":"code","3edf0874":"code","2e3ad95a":"code","bb49c7a0":"code","128e586e":"code","81335487":"code","adebf99a":"code","60107bf4":"code","3ac392a3":"code","fe26e966":"code","16fa19eb":"code","8e25a810":"code","2263d861":"code","1850faf8":"code","12b0d6a2":"code","5d2d2395":"code","4913dc77":"code","370d70a3":"markdown","7a02dd8c":"markdown","615e0059":"markdown","28789146":"markdown","7129f3d4":"markdown","517c55ce":"markdown","f567441d":"markdown","337d8038":"markdown","a7c2e293":"markdown","4fb9a778":"markdown","bae871b3":"markdown","25c0ff6c":"markdown","d65d75e6":"markdown","05157943":"markdown","ad963e4f":"markdown","95b41d13":"markdown","a5a0c0d0":"markdown","3ee1849e":"markdown","9d446bc8":"markdown","0d6ea2de":"markdown","af39aa26":"markdown","92fab463":"markdown","bb4295ed":"markdown","1b42ab51":"markdown","8c3298aa":"markdown","ccfed81e":"markdown","047571fc":"markdown","11a3ea36":"markdown","e184d247":"markdown","5ed0e9f5":"markdown","6957ca07":"markdown","f3ac046a":"markdown","ad42f1a3":"markdown","c0933c0c":"markdown","76c9a070":"markdown","5bbf0b0a":"markdown"},"source":{"2e83668d":"#!pip install surprise\nimport os\nimport pandas as pd\nimport numpy as np\nfrom surprise import Reader\nfrom surprise import Dataset\nfrom surprise.model_selection import KFold\nfrom surprise.model_selection import cross_validate\nfrom surprise import NormalPredictor\nfrom surprise import SVD\nfrom surprise import NMF\nfrom surprise import SlopeOne\nfrom surprise import CoClustering\nfrom surprise.accuracy import rmse\nfrom surprise import accuracy\nfrom surprise.model_selection import train_test_split\nfrom surprise.model_selection import GridSearchCV\nfrom collections import defaultdict\n\n\nfrom collections import deque\nfrom plotly.offline import init_notebook_mode, plot, iplot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)","7878c488":"# Loading the data and getting into form\n# Load data for all movies\nmovie_titles = pd.read_csv('..\/input\/netflix-prize-data\/movie_titles.csv', \n                           encoding = 'ISO-8859-1', \n                           header = None, \n                           names = ['Id', 'Year', 'Name']).set_index('Id')\n\nprint('Shape Movie-Titles:\\t{}'.format(movie_titles.shape))\nmovie_titles.sample(5)","e4eac2f2":"# Load a movie metadata dataset\nmovie_metadata = pd.read_csv('..\/input\/the-movies-dataset\/movies_metadata.csv', low_memory=False)[['original_title', 'overview', 'vote_count']].set_index('original_title').dropna()\n# Remove the long tail of rarly rated moves\nmovie_metadata = movie_metadata[movie_metadata['vote_count']>10].drop('vote_count', axis=1)\n\nprint('Shape Movie-Metadata:\\t{}'.format(movie_metadata.shape))\nmovie_metadata.sample(5)","2d02136d":"#messy mixture of json and csv.\n\n# Load single data-file\ndf_raw = pd.read_csv('..\/input\/netflix-prize-data\/combined_data_1.txt', header=None, names=['User', 'Rating', 'Date'], usecols=[0, 1, 2])\n\n\n# Find empty rows to slice dataframe for each movie\ntmp_movies = df_raw[df_raw['Rating'].isna()]['User'].reset_index()\nmovie_indices = [[index, int(movie[:-1])] for index, movie in tmp_movies.values]\n\n# Shift the movie_indices by one to get start and endpoints of all movies\nshifted_movie_indices = deque(movie_indices)\nshifted_movie_indices.rotate(-1)\n\n\n# Gather all dataframes\nuser_data = []\n\n# Iterate over all movies\nfor [df_id_1, movie_id], [df_id_2, next_movie_id] in zip(movie_indices, shifted_movie_indices):\n    \n    # Check if it is the last movie in the file\n    if df_id_1<df_id_2:\n        tmp_df = df_raw.loc[df_id_1+1:df_id_2-1].copy()\n    else:\n        tmp_df = df_raw.loc[df_id_1+1:].copy()\n        \n    # Create movie_id column\n    tmp_df['Movie'] = movie_id\n    \n    # Append dataframe to list\n    user_data.append(tmp_df)\n\n# Combine all dataframes\nrating = pd.concat(user_data)\ndel user_data, df_raw, tmp_movies, tmp_df, shifted_movie_indices, movie_indices, df_id_1, movie_id, df_id_2, next_movie_id\nprint('Shape User-Ratings:\\t{}'.format(rating.shape))\nrating.sample(5)","8aa2a9ac":"rating.shape","2c3f3641":"rating = rating[0:300000]","76d9f9ac":"ratings = rating[['User','Movie','Rating']]","6ff57027":"ratings.columns = ['userId','movieId','rating']","ea20efd1":"ratings","2426fe64":"ratings_dict = {'itemID': list(ratings.movieId),\n                'userID': list(ratings.userId),\n                'rating': list(ratings.rating)}\n\ndf = pd.DataFrame(ratings_dict)\ndf.shape","5a5965d3":"# Define the format\nreader = Reader(line_format='user item rating timestamp', sep='\\t')\n\nreader = Reader(rating_scale=(0.5, 5.0))\n\ndata = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n\n# Split data into 5 folds\n\n#data.split(n_folds=5)","78a72f87":"data","cccc8ad6":"ratings.groupby('userId')['rating'].count().reset_index().sort_values('rating', ascending=False)[:10]","30e6d03e":"data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n\nbenchmark = []\n# Iterate over all algorithms\nfor algorithm in [SVD(n_epochs = 1, n_factors = 20),]:\n    # Perform cross validation\n    results = cross_validate(algorithm, data, measures=['RMSE'], cv=3, verbose=False)\n    \n    # Get results & append algorithm name\n    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]],index=['Algorithm']))\n    benchmark.append(tmp)\n\n                     \n                                        ","3838979f":"surprise_results = pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')\nsurprise_results","58a08525":"#Run this for get the best hyperparameters\n'''param_grid = {'n_factors': [25, 30, 35, 40], 'n_epochs': [15, 20, 25], 'lr_all': [0.001, 0.003, 0.005, 0.008],\n              'reg_all': [0.08, 0.1, 0.15]}\ngs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\ngs.fit(data)\nalgo = gs.best_estimator['rmse']\nprint(gs.best_score['rmse'])\nprint(gs.best_params['rmse'])\n\n#Assigning values\nt = gs.best_params\nfactors = t['rmse']['n_factors']\nepochs = t['rmse']['n_epochs']\nlr_value = t['rmse']['lr_all']\nreg_value = t['rmse']['reg_all']'''","d4fdad2a":"#Run for fast pass\nparam_grid = {'n_factors': [30], 'n_epochs': [35], 'lr_all': [0.001],\n              'reg_all': [0.08]}\ngs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\ngs.fit(data)\nalgo = gs.best_estimator['rmse']\nprint(gs.best_score['rmse'])\nprint(gs.best_params['rmse'])\n\n#Assigning values\nt = gs.best_params\nfactors = t['rmse']['n_factors']\nepochs = t['rmse']['n_epochs']\nlr_value = t['rmse']['lr_all']\nreg_value = t['rmse']['reg_all']","66d8de36":"trainset, testset = train_test_split(data, test_size=0.25)\nalgo = SVD(n_factors=factors, n_epochs=epochs, lr_all=lr_value, reg_all=reg_value)\npredictions = algo.fit(trainset).test(testset)\naccuracy.rmse(predictions)","5c4470de":"predictions","7b8af8e4":"def get_Iu(uid): \"\"\" args: uid: the id of the user returns: the number of items rated by the user \"\"\"\n    try: return len(trainset.ur[trainset.to_inner_uid(uid)]) except ValueError: # user was not part of the trainset return 0\n\ndef get_Ui(iid): \"\"\" args: iid: the raw id of the item returns: the number of users that have rated the item. \"\"\"\n    try: \n        return len(trainset.ir[trainset.to_inner_iid(iid)]) \n    except \n        ValueError: return 0\n\ndf_predictions = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details']) df_predictions['Iu'] = df_predictions.uid.apply(get_Iu) df_predictions['Ui'] = df_predictions.iid.apply(get_Ui) df_predictions['err'] = abs(df_predictions.est - df_predictions.rui)","0e853bd2":"df_predictions.head()","533e2db1":"best_predictions = df_predictions.sort_values(by='err')[:10]\nworst_predictions = df_predictions.sort_values(by='err')[-10:]","3edf0874":"best_predictions","2e3ad95a":"worst_predictions","bb49c7a0":"ratings[0:10]","128e586e":"ratings.loc[ratings['movieId'] == 33]['rating'].describe()","81335487":"def configure_plotly_browser_state():\n  import IPython\n  display(IPython.core.display.HTML('''\n        <script src=\"\/static\/components\/requirejs\/require.js\"><\/script>\n        <script>\n          requirejs.config({\n            paths: {\n              base: '\/static\/base',\n              plotly: 'https:\/\/cdn.plot.ly\/plotly-latest.min.js?noext',\n            },\n          });\n        <\/script>\n        '''))","adebf99a":"# To create plots\nimport matplotlib.pyplot as plt\n\n# To create interactive plots\nfrom plotly.offline import init_notebook_mode, plot, iplot\nimport plotly.graph_objs as go\n%matplotlib inline \n\ninit_notebook_mode(connected=True)\ntemp = ratings.loc[ratings['movieId'] == 33]['rating']\nconfigure_plotly_browser_state()\n\n# Create trace\ntrace = go.Histogram(x = temp.values,\n                     name = 'Ratings',\n                     xbins = dict(start = 0,\n                                  end = 5, size=.3))\n # Create layout\nlayout = go.Layout(title = 'Number of ratings Movie 1 has received',\n                   xaxis = dict(title = 'Number of Ratings Per Movie'),\n                   yaxis = dict(title = 'Count'),\n                   bargap = 0.2)\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)","60107bf4":"\n#Calculate TP,FP,TN,FN at every threshold level (0.0 - 5.0)\n\nfinal = []\n\nfor threshold in np.arange(0, 5.5, 0.5):\n  tp=0\n  fn=0\n  fp=0\n  tn=0\n  temp = []\n\n  for uid, _, true_r, est, _ in predictions:\n    if(true_r>=threshold):\n      if(est>=threshold):\n        tp = tp+1\n      else:\n        fn = fn+1\n    else:\n      if(est>=threshold):\n        fp = fp+1\n      else:\n        tn = tn+1   \n\n    if tp == 0:\n      precision = 0\n      recall = 0\n      f1 = 0\n    else:\n      precision = tp \/ (tp + fp)\n      recall = tp \/ (tp + fn)\n      f1 = 2 * (precision * recall) \/ (precision + recall)  \n\n  temp = [threshold, tp,fp,tn ,fn, precision, recall, f1]\n  final.append(temp)\n\nresults = pd.DataFrame(final)\nresults.rename(columns={0:'threshold', 1:'tp', 2: 'fp', 3: 'tn', 4:'fn', 5: 'Precision', 6:'Recall', 7:'F1'}, inplace=True)\nresults","3ac392a3":"def precision_recall_at_k(predictions, k, threshold):\n    '''Return precision and recall at k metrics for each user.'''\n\n    # First map the predictions to each user.\n    user_est_true = defaultdict(list)\n    for uid, _, true_r, est, _ in predictions:\n        user_est_true[uid].append((est, true_r))\n\n    precisions = dict()\n    recalls = dict()\n    for uid, user_ratings in user_est_true.items():\n\n        # Sort user ratings by estimated value\n        user_ratings.sort(key=lambda x: x[0], reverse=True)\n\n        # Number of relevant items\n        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n\n        # Number of recommended items in top k\n        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n\n        # Number of relevant and recommended items in top k\n        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n                              for (est, true_r) in user_ratings[:k])\n\n        # Precision@K: Proportion of recommended items that are relevant\n        precisions[uid] = n_rel_and_rec_k \/ n_rec_k if n_rec_k != 0 else 1\n\n        # Recall@K: Proportion of relevant items that are recommended\n        recalls[uid] = n_rel_and_rec_k \/ n_rel if n_rel != 0 else 1\n\n    #tp = n_rel_and_rec_k\n    #fn =  n_rel - tp\n    #fp = n_rec_k - tp\n    return precisions, recalls\n    \n    \n###\n","fe26e966":"\nresults=[]\nfor i in range(2, 11):\n    precisions, recalls = precision_recall_at_k(predictions, k=i, threshold=2.5)\n\n    # Precision and recall can then be averaged over all users\n    prec = sum(prec for prec in precisions.values()) \/ len(precisions)\n    rec = sum(rec for rec in recalls.values()) \/ len(recalls)\n    results.append({'K': i, 'Precision': prec, 'Recall': rec})\n    \n\nresults\n\n###\n","16fa19eb":"Rec=[]\nPrecision=[]\nRecall=[]\nfor i in range(0,9):\n    Rec.append(results[i]['K'])\n    Precision.append(results[i]['Precision'])\n    Recall.append(results[i]['Recall'])\n\nfrom matplotlib import pyplot as plt\nplt.plot(Rec, Precision)\nplt.xlabel('# of Recommendations')\nplt.ylabel('Precision')\nplt2 = plt.twinx()\nplt2.plot(Rec, Recall, 'r')\nplt.ylabel('Recall')\nfor tl in plt2.get_yticklabels():\n    tl.set_color('r')\n    ###","8e25a810":"trainset = data.build_full_trainset()   #Build on entire data set\nalgo = SVD(n_factors=factors, n_epochs=epochs, lr_all=lr_value, reg_all=reg_value)\nalgo.fit(trainset)\n\n# Predict ratings for all pairs (u, i) that are NOT in the training set.\ntestset = trainset.build_anti_testset()\n\n#Predicting the ratings for testset\npredictions = algo.test(testset)","2263d861":"def get_all_predictions(predictions):\n    \n    # First map the predictions to each user.\n    top_n = defaultdict(list)    \n    for uid, iid, true_r, est, _ in predictions:\n        top_n[uid].append((iid, est))\n\n    # Then sort the predictions for each user\n    for uid, user_ratings in top_n.items():\n        user_ratings.sort(key=lambda x: x[1], reverse=True)\n\n    return top_n\n","1850faf8":"\nall_pred = get_all_predictions(predictions)\n","12b0d6a2":"all_pred","5d2d2395":"#To get top 4 reommendation\nn = 3\n\nfor uid, user_ratings in all_pred.items():\n    user_ratings.sort(key=lambda x: x[1], reverse=True)\n    all_pred[uid] = user_ratings[:n]\n","4913dc77":"all_pred","370d70a3":"# **THE DATA**","7a02dd8c":"## Recall and precision at K\nRecall and precision are the classical evaluation metric and are used to evaluate the binary metric and so we have to convert our rating which is scaled from (1-5) into a binary problem relevant and not relevant items.","615e0059":"As per the results above, the optimal value for threshold is **2.5**.\n\nThe next step is to find the optimal K value, and to find it we have to first calculate precision and recall for all the K values(2-10) having threshold value 2.5.\n\nBelow is the function to calculate **precision and recall @ K.**","28789146":"In some circumstances, we might know that we need to maximize either **recall** or **precision** at the cost of the other metric. For example, in disease screening of patients, we would probably want a recall near 1.0 i.e. we want to find all patients who have the disease. \n\nHowever, in cases where we want to obtain an optimal blend of precision and recall, we can use **F1** score which is the harmonic mean of precision and recall taking both metrics into account while calculating it.\n\n**f1 score = 2 * (precision * recall) \/ (precision + recall)**","7129f3d4":"# Surprise - Model Selection\n","517c55ce":"Surprise is a Python scikit building and analyzing recommender systems that deal with explicit rating data.\nMaintained by **Nicolas Hug.**\n\nhttp:\/\/surpriselib.com\/","f567441d":"# Movie Recommender System Using Surprise Library","337d8038":"The minimumn number of ratings given by a user is 20, where as the most productive user is user-414 giving 2698 rating.\nThe data is properly distributed and the big chunk of ratings is between 3 and 5 which means people are not very hard towards rating","a7c2e293":"#### As we have all the predicted rating, We'll subset to only top **K** movies for every user, where K is 4","4fb9a778":"The below function computes precision and recall and F1 socre as explained above.","bae871b3":"# Time to recommend some movies to users","25c0ff6c":"# **Introduction** \n","d65d75e6":"To load a data set from the above pandas data frame, we will use the **load_from_df()** method, we will also need a **Reader object**, and the **rating_scale** parameter must be specified. \n\nThe data frame must have three columns, corresponding to the user ids, the item ids, and the ratings in this order. ","05157943":"# Training and Testing ","ad963e4f":"## **What are recommender systems?**\nIt helps the user to select the right item by suggesting a presumable list of items and so it has become an integral part of e-commerce, movie and music rendering sites and the list goes on.\nThey are becoming one of the most popular applications of machine learning which has gained importance in recent years.\nThe two most popular ways it can be approached\/built are:\n\n1- Content-based recommendations (link).\n\n2- Collaborative FIltering (link).\n","95b41d13":"## **Why do we need recommender systems?**\nAll entertainment websites or online stores have millions\/billions of items. It becomes challenging for the customer to select the right one.\nAt this place, recommender systems come into the picture and help the user to find the right item by minimizing the options.\n","a5a0c0d0":"## Definition of Precision and Recall\n**Precision:** It tries to answer \"What proportion of positive identifications was actually correct?\"\ni.e True positive \/ (True positive+False Positive)\nin RecSys we can say (# of recommended items @k that are relevant) \/ (# of recommended items @k)\n\n**Recall:** It tries to answer \"What proportion of actual positives were identified correctly?\"\ni.e True positive \/ (True Positive+False Negative)\nin RecSys we can say (# of recommended items @k that are relevant) \/ (# of relevant items @k)\n","3ee1849e":"Some understanding on the algorithms before we start applying.\n\n**1: Normal Predictor:** It predicts a random rating based on the distribution of the training set, which is assumed to be normal.\nIt's a basic algorithm that does not do much work but that is still useful for comparing accuracies.\n\n**2: SVD:** It got popularized by Simon Funk during the Netflix prize and is a Matrix Factorized algorithm. If baselines are not used, it is equivalent to PMF.\n\n**3: NMF:** It is based on Non-negative matrix factorization and is similar to SVD.\n\n**4: KNN Basic:** This is a basic collaborative filtering algorithm method.\n","9d446bc8":"Here we will be using **build_anti_testset()** method to get the data for testset as we have to predict ratings for the (user, item) pairs which are not present.","0d6ea2de":"### Let's see the 10 best and worst predictions our model made","af39aa26":"In this post, we will be focussing on the Collaborative filtering method that is: *the user is recommended items that people with similar tastes and preferences liked in the past.*\n\n\nWe will be working with MoiveLens Dataset, a movie rating dataset, to develop a recommendation system using the surprise library. Let's get started!\n\n","92fab463":"## Deciding 'k'\nIn recommendation systems, we are interested in showing the top N items to users and so the best is to compute precision and recall on top N values instead of calculating on all the items.\n","bb4295ed":"### Voila, It's done!!","1b42ab51":"### Now as we know the optimal number of recommendations to provie, it's time to give recommendations to users. To do so we have to predict ratings for the movies which user has not yet watched.","8c3298aa":"## Conversion to binary\nTo do the translation we have to select an arbitrary value on which we can say any rating above that will be considered relevant. There are many methods on selecting that value but for now, we will select 3.5 as the threshold, which means any true rating above 3.5 will be considered relevant and below will be not relevant.","ccfed81e":"**Default values for svd( ):**\n\n**n_factors**  - 100   \n**n_epochs**  - 20   \n**lr_all**  \u2013 0.005   \n**reg_all**   \u2013 0.02 ","047571fc":"#### Let's check how good or bad our predictions are:\nThe following function will create a pandas data frame which will consist of these columns:\n\n**UID:** user-id\n\n**iid:** Movieid\n\n**Rui:** the rating given by the user\n\n**est:** rating estimated by the model\n\n**Iu:** No of items rated by the user\n\n**UI:** number of users that have rated this item\n\n**err:** abs difference between predicted rating and the actual rating.\n\n\ndef get_Iu(uid):\n    \"\"\" \n    args: \n      uid: the id of the user\n    returns: \n      the number of items rated by the user\n    \"\"\"\n    try:\n        return len(trainset.ur[trainset.to_inner_uid(uid)])\n    except ValueError: # user was not part of the trainset\n        return 0\n    \ndef get_Ui(iid):\n    \"\"\" \n    args:\n      iid: the raw id of the item\n    returns:\n      the number of users that have rated the item.\n    \"\"\"\n    try: \n        return len(trainset.ir[trainset.to_inner_iid(iid)])\n    except ValueError:\n        return 0\n\n    \ndf_predictions = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])\ndf_predictions['Iu'] = df_predictions.uid.apply(get_Iu)\ndf_predictions['Ui'] = df_predictions.iid.apply(get_Ui)\ndf_predictions['err'] = abs(df_predictions.est - df_predictions.rui)","11a3ea36":"# **K Recommendations**","e184d247":"# Loading Required Libraries","5ed0e9f5":"As the graph states, Precision drops significantly when K=3\n\nSo we will consider the value of K to be 3.","6957ca07":"# Tunining algorithm parameters with <font color = red> GridSearchCV <\/font> to find the best parameters for the algorithm.","f3ac046a":"Below is the function to get all the prediction sorted.","ad42f1a3":"## Definition of Relevant and Recommended\n**Relevant:** True Rating > = 2.5\n\n**Irrelevant:** True Rating < 2.5\n***\n**Recommended item:** Predicted Rating > = 3.5\n\n**Not Recommended item:** Predicted Rating > = 3.5","c0933c0c":"We use the **train_test_split()** to sample a trainset and a testset with given sizes, and use the accuracy metric of rmse. \n\nWe\u2019ll then use the **fit()** method which will train the algorithm on the trainset, and the **test()** method which will return the predictions made from the testset","76c9a070":"The worst predictions look pretty surprise. Let's look in more details of item \"3996\",  rated 0.5, our SVD algorithm predicts 4.4\n","5bbf0b0a":"**While recall expresses the ability to find all relevant instances in a dataset, precision expresses the proportion of the data points our model says was relevant actually were relevant.**"}}