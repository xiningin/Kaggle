{"cell_type":{"dfed963a":"code","0b713774":"code","f21d4f3a":"code","7795c3be":"code","13bca478":"code","fcdc5f56":"code","89db7770":"code","c2034e4c":"code","fcac8052":"code","e5675502":"code","979d453f":"code","309d35cf":"code","97590523":"code","94707b15":"code","f023c79b":"code","66398ac2":"code","3556f83b":"code","c9de6059":"code","a911b8a8":"code","f8463426":"code","59b1bfd1":"code","e6ac1542":"code","8f462cd2":"code","da9aba3f":"code","1f3f2be7":"code","f3e210d5":"code","7452b1c9":"code","ee5bbc72":"code","00c98161":"code","f95c2446":"code","96e048ca":"code","2b482108":"code","a6c10df5":"code","98bf3b91":"code","66554b19":"code","591cf647":"code","cf6b701c":"code","7ffa9f24":"code","51556bf0":"code","a632608b":"code","06acfe3a":"code","dbb83955":"code","8b8d3f13":"code","0391ecf4":"code","017adcd1":"code","566dda17":"code","dd9dbe4c":"code","1e67ae5d":"code","729f21b1":"code","6a44bcb9":"code","06e7667f":"code","f7d214d3":"code","3dc5afa1":"code","06f0a713":"code","dff4eccc":"code","cce52687":"code","0c88671a":"code","8d8fb05b":"code","6e713a78":"code","bb0ebdb7":"code","7b7f69e8":"code","9972acde":"code","48df5cdd":"code","f1685393":"code","fd9f4175":"code","81070fb9":"code","3fca47e5":"code","e22d056f":"code","2369db86":"code","32e34185":"code","afd55a42":"code","638d9a3a":"code","4f1b840c":"code","eb38be73":"code","273be0c2":"code","a14fd382":"code","b1fcd18c":"code","a3b6f257":"code","ba5bf35f":"code","2d52948e":"code","973cec82":"code","356dd416":"code","02785af2":"code","5534aaa6":"code","07849baa":"code","3c3874c5":"code","c95f834f":"code","1a4c44b2":"code","2da24bdb":"code","9bf678ff":"code","9551a176":"code","43d43708":"code","9d50b776":"code","7bafe755":"code","e0af5678":"code","f72ceb04":"code","168735aa":"code","d490d5ff":"code","81e2e64a":"code","b7a8e59e":"code","6d598bad":"code","7022e4f7":"code","31af879a":"code","fca14794":"code","5e81ab1e":"code","ba54b025":"code","5e172554":"code","ecd0ecc9":"code","fc2d164f":"code","874b9f78":"code","e0a5f6d4":"code","db4c58d3":"code","84a6e114":"code","29e504e2":"code","453f7746":"code","9d7dd5ae":"code","3464c840":"code","3feb58db":"code","3b2178e0":"code","859b8142":"code","2ed34ef4":"code","3185a517":"code","e149291f":"code","ea1d3998":"code","d9cc558a":"code","13d7649b":"code","4ad06759":"code","880ec435":"code","3a7b0cbf":"code","79aa2f5a":"code","69b42633":"code","736b7378":"code","a18b9786":"code","fc8bfe2b":"code","7032f945":"code","672c4fb0":"code","ab4e663c":"code","2beabcea":"code","8e80cdd3":"code","b77b7b1f":"code","7264af1a":"code","0044a64f":"code","50bcdddb":"code","0f9a52ab":"code","9e10fa0b":"code","a08785a3":"code","94e918fd":"code","0fcd87d6":"code","6ddb7354":"code","d815da65":"code","35811869":"code","2556d505":"code","76a47344":"code","48a8b4ac":"code","aab691ce":"code","0f96a5a7":"code","0b3845bc":"code","1a8a1b06":"code","58cb58f7":"code","a6cdc042":"code","4320193a":"code","d92b296a":"code","d02f9b6e":"code","683664ce":"code","441615e9":"code","4b8c7179":"code","237aeef1":"code","48843b09":"code","9b0be81a":"code","b2622603":"code","5542b23e":"code","9ec99e0d":"code","4b64ad1e":"code","b17badf9":"code","ab4345ee":"code","80362bda":"code","d5f13a2a":"code","a9fbd2d5":"code","94af6982":"code","0d4887a6":"code","aa797389":"code","3667eed8":"markdown","31b350df":"markdown","b3485fba":"markdown","62008104":"markdown","2c44bb67":"markdown","9e05f024":"markdown","8744b164":"markdown","d964d688":"markdown","bee1efe6":"markdown","9b29e2d3":"markdown","601e7762":"markdown","48abb859":"markdown","2c320de2":"markdown","e88ff8b8":"markdown","9fd6bf7e":"markdown","79aa6d9e":"markdown","27a9a2d1":"markdown","36b68890":"markdown","9966ee15":"markdown","f073888b":"markdown","840341ba":"markdown","a439b398":"markdown","ec1df75f":"markdown","38b79555":"markdown","8867ad16":"markdown","71184be4":"markdown","f90eaeba":"markdown","eaea69fd":"markdown","fa8a82d7":"markdown"},"source":{"dfed963a":"import numpy as np\nimport pandas as pd\nimport os\nimport math\nfrom scipy import stats\nimport time\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom sklearn import metrics\nfrom sklearn import linear_model\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport eli5\nimport shap\nfrom IPython.display import HTML\nimport json\nimport altair as alt\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline","0b713774":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport pandas as pd\ntest_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv')\ntrain_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv')\ntest_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/sample_submission.csv')\ntrain_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv')","f21d4f3a":"def check_dataframe(df):\n    \n    # excution time\n    start_time = time.time()\n\n    dict_check_value = {}\n    list_check_value = []\n    total_rows = len(df)\n    for col in df.columns:\n    #     print(col)\n    \n        value_count_base = df[col].value_counts()\n        value_count_index = list(value_count_base.index)\n        value_count_value = list(value_count_base.values)\n        null_value = df[col].isnull().sum()\n        unique_value = len(value_count_index)\n        unique_value_exam = value_count_index[:5]\n#         unique_value_exam = df[col][~df[col].isnull()].unique()\n#         unique_value = len(unique_value_exam)\n#         unique_value_exam = unique_value_exam[:5]\n        value_type = df[col].dtype\n    \n        # include null value\n        in_null_entropy = round(stats.entropy(df[col].value_counts()\/total_rows), 4)\n        \n        # except null value\n        except_null_entropy = round(stats.entropy(df[col].value_counts()\/total_rows-null_value), 4)\n\n\n        list_check_value = [unique_value, null_value, unique_value_exam, value_type, in_null_entropy, except_null_entropy]\n\n\n        dict_check_value[col] = list_check_value\n    \n    new_df = pd.DataFrame.from_dict(dict_check_value, orient='index', columns=['uniques', 'missing', 'values_exam_top5', 'dtypes', 'total_entropy', 'exp_null_entropy'])\n    \n    print(f'The Excution Time(minutes) is {(time.time()-start_time)\/60}')\n    return new_df","7795c3be":"# temp_df = train_transaction[:1000].copy(deep=True)\n","13bca478":"train_transaction_chk = check_dataframe(train_transaction)\ntrain_transaction_chk","fcdc5f56":"'....'.join([str(k)+\" Data Types: \"+str(v)+'' for (k, v) in train_transaction_chk.groupby('dtypes').size().to_dict().items()])","89db7770":"\n# Object Data Types\n\n\n# basic_df \n# cross_check_df\n# columns\n# target columns\n\n\ndef check_obj_col(df, column):\n    \"\"\"\n    # From: https:\/\/www.kaggle.com\/kabure\/extensive-eda-and-modeling-xgb-hyperopt\n    parameter : df, column\n    df(dataframe): Reference Dataframe \n    column(string): column to be based on\n    \"\"\"\n    total = len(df)\n    df[col] = df[column].fillna(\"Miss\")\n    cross_df = pd.crosstab(df[column], df['isFraud'], normalize='index')*100\n    cross_df = cross_df.reset_index()\n    cross_df.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\n    plt.figure(figsize=(14,10))\n    subtile_str = '{} Distributions'.format(column)\n    plt.suptitle(subtile_str, fontsize=22)\n    plt.subplot(221)\n    g = sns.countplot(x=column, data=df)\n    \n    g.set_title(subtile_str, fontsize=19)\n    g.set_xlabel(\"{} Name\".format(column), fontsize=17)\n    g.set_ylabel(\"Count\", fontsize=17)\n    g.set_ylim(0,500000)\n    for p in g.patches:\n        height = p.get_height()\n        g.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total*100),\n                ha=\"center\", fontsize=14)\n        \n    plt.subplot(222)\n    g1 = sns.countplot(x=column, hue='isFraud', data=df)\n    plt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\n    gt = g1.twinx()\n    \n    order_xcol = [t.get_text()  for t in g.get_xticklabels()]\n    \n    gt = sns.pointplot(x=column, y='Fraud', data=cross_df, color='black', order=order_xcol, legend=False)\n    gt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n\n    g1.set_title(\"{} by Target(isFraud)\".format(column), fontsize=19)\n    g1.set_xlabel(\"{} Name\".format(column), fontsize=17)\n    g1.set_ylabel(\"Count\", fontsize=17)\n\n    plt.subplot(212)\n    g3 = sns.boxenplot(x=column, y='TransactionAmt', hue='isFraud', \n                  data=df[df['TransactionAmt'] <= 2000] )\n    g3.set_title(\"Transaction Amount Distribuition by {} and Target\".format(column), fontsize=20)\n    g3.set_xlabel(\"{} Name\".format(column), fontsize=17)\n    g3.set_ylabel(\"Transaction Values\", fontsize=17)\n    \n    plt.subplots_adjust(hspace = 0.6, top = 0.85)\n    \n    return plt.show()","c2034e4c":"object_col = [col for col in list(train_transaction_chk[train_transaction_chk['dtypes']=='object'].index) if col not in ['P_emaildomain', 'R_emaildomain']]\n# print(object_col) # unique \uac1c\uc218\uac00 \ub108\ubb34 \ub9ce\uc740 \uceec\ub7fc \uc81c\uc678\n\n# check_obj_col(train_transaction, column='ProductCD')\n\n# From: https:\/\/www.kaggle.com\/kabure\/extensive-eda-and-modeling-xgb-hyperopt\nfor col in object_col:\n    check_obj_col(train_transaction, column=col)\n    \n    \nprint(object_col)","fcac8052":"train_transaction_chk[(train_transaction_chk.index == 'P_emaildomain') | (train_transaction_chk.index == 'R_emaildomain')]\n# num_col_chk = [col for col in train_transaction_chk[train_transaction_chk['dtypes']!='object'].index if col not in ['TransactionID', 'isFraud', 'TransactionDT']]","e5675502":"train_transaction[['P_emaildomain', 'R_emaildomain']].head()\n# NaN, NaN\n# value, NaN\n# value, value\n# NaN, value","979d453f":"total = len(train_transaction)\nprint(f'p_emaildomain\uc740 {(total-94456)\/total}%\ub85c \uac12\uc774 \uc874\uc7ac')\nprint(f'r_emaildomain\uc740 {(total-453249)\/total}%\ub85c \uac12\uc774 \uc874\uc7ac')","309d35cf":"\n\n# P is Null, R is Not Null\ntrain_transaction[(train_transaction['P_emaildomain'].isnull()) & (train_transaction['R_emaildomain'].notnull())].groupby('isFraud').size()\np_null_r_notnull = np.array(train_transaction[(train_transaction['P_emaildomain'].isnull()) & (train_transaction['R_emaildomain'].notnull())].groupby('isFraud').size())\n\ntrain_transaction[(train_transaction['P_emaildomain'].notnull()) & (train_transaction['R_emaildomain'].isnull())].groupby('isFraud').size()\np_notnull_r_null = np.array(train_transaction[(train_transaction['P_emaildomain'].notnull()) & (train_transaction['R_emaildomain'].isnull())].groupby('isFraud').size())\n\ntrain_transaction[(train_transaction['P_emaildomain'].isnull()) & (train_transaction['R_emaildomain'].isnull())].groupby('isFraud').size()\np_null_r_null = np.array(train_transaction[(train_transaction['P_emaildomain'].isnull()) & (train_transaction['R_emaildomain'].isnull())].groupby('isFraud').size())\n\ntrain_transaction[(train_transaction['P_emaildomain'].notnull()) & (train_transaction['R_emaildomain'].notnull())].groupby('isFraud').size()\np_notnull_r_notnull = np.array(train_transaction[(train_transaction['P_emaildomain'].notnull()) & (train_transaction['R_emaildomain'].notnull())].groupby('isFraud').size())\n\nemaildomain_df = pd.DataFrame(np.stack((p_null_r_notnull, p_notnull_r_null, p_null_r_null, p_notnull_r_notnull)), columns=['Normal', 'isFraud'], index=['p_null_r_notnull', 'p_notnull_r_null', 'p_null_r_null', 'p_notnull_r_notnull'])\n\nemaildomain_df['Normal_ratio'], emaildomain_df['isFraud_ratio'] = list(zip(*emaildomain_df.apply(lambda x: (x['Normal']\/total, x['isFraud']\/total), axis=1)))\nemaildomain_df['isFraud_ratio'] = emaildomain_df.apply(lambda x: x['isFraud']\/(x['Normal']+x['isFraud']), axis=1)\n\nfraud_total = len(train_transaction[train_transaction['isFraud']==1])\nemaildomain_df['fraudRatio_per_Fraud'] = emaildomain_df.apply(lambda x: x['isFraud']\/fraud_total, axis=1)\nemaildomain_df","97590523":"# from https:\/\/www.kaggle.com\/jesucristo\/fraud-complete-eda\nfig, ax = plt.subplots(1, 3, figsize=(32,10))\n\nsns.countplot(y=\"P_emaildomain\", ax=ax[0], data=train_transaction)\nax[0].set_title('P_emaildomain', fontsize=14)\nsns.countplot(y=\"P_emaildomain\", ax=ax[1], data=train_transaction.loc[train_transaction['isFraud'] == 1])\nax[1].set_title('P_emaildomain isFraud = 1', fontsize=14)\nsns.countplot(y=\"P_emaildomain\", ax=ax[2], data=train_transaction.loc[train_transaction['isFraud'] == 0])\nax[2].set_title('P_emaildomain isFraud = 0', fontsize=14)\nplt.show()","94707b15":"# from https:\/\/www.kaggle.com\/jesucristo\/fraud-complete-eda\nfig, ax = plt.subplots(1, 3, figsize=(32,10))\n\nsns.countplot(y=\"R_emaildomain\", ax=ax[0], data=train_transaction)\nax[0].set_title('R_emaildomain', fontsize=14)\nsns.countplot(y=\"R_emaildomain\", ax=ax[1], data=train_transaction.loc[train_transaction['isFraud'] == 1])\nax[1].set_title('R_emaildomain isFraud = 1', fontsize=14)\nsns.countplot(y=\"R_emaildomain\", ax=ax[2], data=train_transaction.loc[train_transaction['isFraud'] == 0])\nax[2].set_title('R_emaildomain isFraud = 0', fontsize=14)\nplt.show()","f023c79b":"train_transaction[(train_transaction['P_emaildomain'].notnull()) & (train_transaction['R_emaildomain'].notnull()) & (train_transaction['isFraud']==1)].apply(lambda x: (x['P_emaildomain']+'-'+x['R_emaildomain']), axis=1).value_counts()","66398ac2":"train_transaction[(train_transaction['P_emaildomain'].notnull()) & (train_transaction['R_emaildomain'].notnull()) & (train_transaction['isFraud']==0)].apply(lambda x: (x['P_emaildomain']+'-'+x['R_emaildomain']), axis=1).value_counts()","3556f83b":"# https:\/\/www.kaggle.com\/kabure\/extensive-eda-and-modeling-xgb-hyperopt \ndef ploting_cnt_amt(df, col, lim=2000):\n    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n    \n    plt.figure(figsize=(16,14))    \n    plt.suptitle(f'{col} Distributions ', fontsize=24)\n    \n    plt.subplot(211)\n    g = sns.countplot( x=col,  data=df, order=list(tmp[col].values))\n    gt = g.twinx()\n    gt = sns.pointplot(x=col, y='Fraud', data=tmp, order=list(tmp[col].values),\n                       color='black', legend=False, )\n    gt.set_ylim(0,tmp['Fraud'].max()*1.1)\n    gt.set_ylabel(\"%Fraud Transactions\", fontsize=16)\n    g.set_title(f\"Most Frequent {col} values and % Fraud Transactions\", fontsize=20)\n    g.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g.set_ylabel(\"Count\", fontsize=17)\n    g.set_xticklabels(g.get_xticklabels(),rotation=45)\n    sizes = []\n    for p in g.patches:\n        height = p.get_height()\n        sizes.append(height)\n        g.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total*100),\n                ha=\"center\",fontsize=12) \n        \n    g.set_ylim(0,max(sizes)*1.15)\n    \n    #########################################################################\n    perc_amt = (df.groupby(['isFraud',col])['TransactionAmt'].sum() \\\n                \/ df.groupby([col])['TransactionAmt'].sum() * 100).unstack('isFraud')\n    perc_amt = perc_amt.reset_index()\n    perc_amt.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n    amt = df.groupby([col])['TransactionAmt'].sum().reset_index()\n    perc_amt = perc_amt.fillna(0)\n    plt.subplot(212)\n    g1 = sns.barplot(x=col, y='TransactionAmt', \n                       data=amt, \n                       order=list(tmp[col].values))\n    g1t = g1.twinx()\n    g1t = sns.pointplot(x=col, y='Fraud', data=perc_amt, \n                        order=list(tmp[col].values),\n                       color='black', legend=False, )\n    g1t.set_ylim(0,perc_amt['Fraud'].max()*1.1)\n    g1t.set_ylabel(\"%Fraud Total Amount\", fontsize=16)\n    g.set_xticklabels(g.get_xticklabels(),rotation=45)\n    g1.set_title(f\"{col} by Transactions Total + %of total and %Fraud Transactions\", fontsize=20)\n    g1.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g1.set_ylabel(\"Transaction Total Amount(U$)\", fontsize=16)\n    g1.set_xticklabels(g.get_xticklabels(),rotation=45)    \n    \n    for p in g1.patches:\n        height = p.get_height()\n        g1.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total_amt*100),\n                ha=\"center\",fontsize=12) \n        \n    plt.subplots_adjust(hspace=.4, top = 0.9)\n    plt.show()","c9de6059":"# https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100400#latest-579480\nimport datetime\n\nSTART_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\ntrain_transaction[\"Date\"] = train_transaction['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n\ntrain_transaction['_Hours'] = train_transaction['Date'].dt.hour\n\ntrain_transaction['_Weekdays'] = train_transaction['Date'].dt.dayofweek","a911b8a8":"# total_amt = train_transaction.groupby(['isFraud'])['TransactionAmt'].sum().sum()\ntotal_amt = train_transaction['TransactionAmt'].sum()\nploting_cnt_amt(train_transaction, '_Hours')","f8463426":"ploting_cnt_amt(train_transaction, '_Weekdays')","59b1bfd1":"train_transaction.drop(['_Weekdays', 'Date'], axis=1, inplace=True)","e6ac1542":"# print(train_transaction_chk[train_transaction_chk['dtypes']=!'object'])\nprint('Numeric Columns {} in train_transaction'.format(len(train_transaction.select_dtypes(['float', 'int']).columns)))","8f462cd2":"list(train_transaction.select_dtypes(['float', 'int']).columns)\n# TransactionID, isFraud, TransactionDT, TransactionAmt,_Hours \uc81c\uc678","da9aba3f":"eda_col_list = [col for col in list(train_transaction.select_dtypes(['float', 'int']).columns) if col not in ['TransactionID', 'isFraud', 'TransactionDT', '_Hours', 'TransactionAmt']]\n# sns.pairplot(train_transaction[eda_col_list], kind=\"scatter\", hue='type', corner=True)","1f3f2be7":"train_transaction_chk.loc[eda_col_list, :] # dfObj.loc[ 'b' , : ]","f3e210d5":"train_transaction_chk.loc[eda_col_list, :].apply(lambda x: (total-x['missing'])\/total, axis=1)","7452b1c9":"# This code from https:\/\/www.kaggle.com\/nroman\/eda-for-cis-fraud-detection\n# def plot_numerical(feature):\n#     \"\"\"\n#     Plot some information about a numerical feature for both train and test set.\n#     Args:\n#         feature (str): name of the column in DataFrame\n#     \"\"\"\n#     fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(16, 18))\n#     sns.kdeplot(train[feature], ax=axes[0][0], label='Train');\n#     sns.kdeplot(test[feature], ax=axes[0][0], label='Test');\n\n#     sns.kdeplot(train[train['isFraud']==0][feature], ax=axes[0][1], label='isFraud 0')\n#     sns.kdeplot(train[train['isFraud']==1][feature], ax=axes[0][1], label='isFraud 1')\n\n#     test[feature].index += len(train)\n#     axes[1][0].plot(train[feature], '.', label='Train');\n#     axes[1][0].plot(test[feature], '.', label='Test');\n#     axes[1][0].set_xlabel('row index');\n#     axes[1][0].legend()\n#     test[feature].index -= len(train)\n\n#     axes[1][1].plot(train[train['isFraud']==0][feature], '.', label='isFraud 0');\n#     axes[1][1].plot(train[train['isFraud']==1][feature], '.', label='isFraud 1');\n#     axes[1][1].set_xlabel('row index');\n#     axes[1][1].legend()\n\n#     pd.DataFrame({'train': [train[feature].isnull().sum()], 'test': [test[feature].isnull().sum()]}).plot(kind='bar', rot=0, ax=axes[2][0]);\n#     pd.DataFrame({'isFraud 0': [train[(train['isFraud']==0) & (train[feature].isnull())][feature].shape[0]],\n#                   'isFraud 1': [train[(train['isFraud']==1) & (train[feature].isnull())][feature].shape[0]]}).plot(kind='bar', rot=0, ax=axes[2][1]);\n\n#     fig.suptitle(feature, fontsize=18);\n#     axes[0][0].set_title('Train\/Test KDE distribution');\n#     axes[0][1].set_title('Target value KDE distribution');\n#     axes[1][0].set_title('Index versus value: Train\/Test distribution');\n#     axes[1][1].set_title('Index versus value: Target distribution');\n#     axes[2][0].set_title('Number of NaNs');\n#     axes[2][1].set_title('Target value distribution among NaN values');\n    \n\ndef plot_numerical(train, test, feature):\n    \"\"\"\n    Fix a little code from https:\/\/www.kaggle.com\/nroman\/eda-for-cis-fraud-detection\n    Plot some information about a numerical feature for both train and test set.\n    Args:\n        train : train DataFrame\n        test : test Dataframe\n        feature (str): name of the column in DataFrame\n    \"\"\"\n    fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(16, 18))\n    sns.kdeplot(train[feature], ax=axes[0][0], label='Train');\n    sns.kdeplot(test[feature], ax=axes[0][0], label='Test');\n\n    sns.kdeplot(train[train['isFraud']==0][feature], ax=axes[0][1], label='isFraud 0')\n    sns.kdeplot(train[train['isFraud']==1][feature], ax=axes[0][1], label='isFraud 1')\n\n    test[feature].index += len(train)\n    axes[1][0].plot(train[feature], '.', label='Train');\n    axes[1][0].plot(test[feature], '.', label='Test');\n    axes[1][0].set_xlabel('row index');\n    axes[1][0].legend()\n    test[feature].index -= len(train)\n\n    axes[1][1].plot(train[train['isFraud']==0][feature], '.', label='isFraud 0');\n    axes[1][1].plot(train[train['isFraud']==1][feature], '.', label='isFraud 1');\n    axes[1][1].set_xlabel('row index');\n    axes[1][1].legend()\n\n    pd.DataFrame({'train': [train[feature].isnull().sum()], 'test': [test[feature].isnull().sum()]}).plot(kind='bar', rot=0, ax=axes[2][0]);\n    pd.DataFrame({'isFraud 0': [train[(train['isFraud']==0) & (train[feature].isnull())][feature].shape[0]],\n                  'isFraud 1': [train[(train['isFraud']==1) & (train[feature].isnull())][feature].shape[0]]}).plot(kind='bar', rot=0, ax=axes[2][1]);\n\n    fig.suptitle(feature, fontsize=18);\n    axes[0][0].set_title('Train\/Test KDE distribution');\n    axes[0][1].set_title('Target value KDE distribution');\n    axes[1][0].set_title('Index versus value: Train\/Test distribution');\n    axes[1][1].set_title('Index versus value: Target distribution');\n    axes[2][0].set_title('Number of NaNs');\n    axes[2][1].set_title('Target value distribution among NaN values');\n    ","ee5bbc72":"# https:\/\/stackoverflow.com\/questions\/11350770\/select-by-partial-string-from-a-pandas-dataframe\n# Error\uac00 \ub098\ubbc0\ub85c \uc774\ud6c4 \uc218\uc815 \ud544\uc694\nfor col in [col for col in train_transaction.columns if ('card' in col) & (col not in ['card4', 'card6'])]:\n    print(col)\n    try:\n        plot_numerical(train_transaction, test_transaction, col)\n    except ValueError as e:\n        print(e, col)\n        continue","00c98161":"plot_numerical(train_transaction, test_transaction, 'card1')\n# https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100340#latest-578626","f95c2446":"plot_numerical(train_transaction, test_transaction, 'card2')","96e048ca":"plot_numerical(train_transaction, test_transaction, 'card5')","2b482108":"# Target value\uc5d0 \ub530\ub978, Feature\uc758 isnull \uac12\uc758 \ube44\uc728\n\n# temp_cols = [col for col in train_transaction.columns if ('card' in col) & (col not in ['card4', 'card6'])]\n\ntemp_arr = np.empty([0, 3])\n\n# np.array(pd.crosstab(train_transaction['card5'].fillna('missing'), train_transaction['isFraud'].fillna('missing'), margins=True).loc['missing', :])\nfor col in train_transaction.columns:\n    print(col)\n    try:\n        new_arr = np.array(pd.crosstab(train_transaction[col].fillna('missing'), train_transaction['isFraud'].fillna('missing'), margins=True).loc['missing', :])\n    except KeyError:\n        new_arr = np.array([0, 0, 0])\n#     print(pd.crosstab(train_transaction[col].fillna('missing'), train_transaction['isFraud'].fillna('missing'), margins=True))\n#     print(np.array(pd.crosstab(train_transaction[col].fillna('missing'), train_transaction['isFraud'].fillna('missing'), margins=True).loc['missing', :]))\n    temp_arr = np.vstack((temp_arr, new_arr))\n\nnull_df = pd.DataFrame(temp_arr, columns=['isFraud0', 'isFraud1', 'Null_Total'], index=train_transaction.columns)","a6c10df5":"import math\ndef fraudNull_perTotalNull(a, b):\n    val = a\/b*100\n    if math.isnan(val):\n        val = 0\n    return val\n# null_df.apply(lambda x: x['isFraud1']\/x['Null_Total']*100, axis=1)\nnull_df['fraudNull_perTotalNull'] = null_df.apply(lambda x: fraudNull_perTotalNull(x['isFraud1'], x['Null_Total']), axis=1)","98bf3b91":"# null\uac12\uc740 \uc758\ubbf8\uac00 \uc788\uc744\uae4c?\n# Null\uac12\uc911 Fraud\uac00 \ucc28\uc9c0\ud558\ub294 \ube44\uc728\nnull_df['fraudNull_perTotalFraud'] = null_df.apply(lambda x: np.ceil(x['isFraud1']\/fraud_total*100), axis=1)","66554b19":"train_transaction_chk = pd.concat([null_df[['isFraud1', 'fraudNull_perTotalNull', 'fraudNull_perTotalFraud']], train_transaction_chk], axis=1)","591cf647":"train_transaction_chk.head()","cf6b701c":"train_transaction_chk","7ffa9f24":"[col for col in train_transaction.columns if ('C' in col) & ('ProductCD' not in col)]","51556bf0":"plot_numerical(train_transaction, test_transaction, 'C1')","a632608b":"plot_numerical(train_transaction, test_transaction, 'C2')","06acfe3a":"plot_numerical(train_transaction, test_transaction, 'C6')","dbb83955":"plot_numerical(train_transaction, test_transaction, 'C9')","8b8d3f13":"plot_numerical(train_transaction, test_transaction, 'C11')","0391ecf4":"plot_numerical(train_transaction, test_transaction, 'C13')","017adcd1":"plot_numerical(train_transaction, test_transaction, 'C14')","566dda17":"for col in [col for col in train_transaction.columns if ('D' in col) & (col not in ['TransactionID', 'TransactionDT', 'ProductCD', 'Date'])]:\n    try:\n        plot_numerical(train_transaction, test_transaction, col)\n    except ValueError:\n        print(\"{} Could not convert data to an integer.\".format(col))\n        continue\n    except Exception as e:\n        print(\"error\", e)\n        continue","dd9dbe4c":"# Set columns\neda_col_list = [col for col in list(train.columns) if col not in ['id', 'fiberID', 'type']]\n\n# plot\nfig, ax = plt.subplots(figsize=(12, 15), sharex=True)\nsns.despine(left=True)\n\nnum_fig = len(eda_col_list)\nncols_fig = 3\nnrows_fig = math.ceil(num_fig\/ncols_fig)\n\ngs = gridspec.GridSpec(nrows_fig, ncols_fig)\n\nfig.legend([plt.plot([], [], c=c)[0] for c in new_colors], unique_labels, loc='upper right', bbox_to_anchor=(1.2, 0.5))\n\n\nfor i, n in enumerate(range(num_fig)):\n  # if i < 5:\n    ax = fig.add_subplot(gs[n])\n\n  # Plot a kernel density estimate and rug plot\n    sns.distplot(df[eda_col_list[n]], hist=False, rug=True)\n\n    ax.set_title(eda_col_list[n])\n  # else:\n    # break\nplt.tight_layout()\nplt.show()","1e67ae5d":"# Set columns\neda_col_list = [col for col in list(train.columns) if col not in ['type']]\n\n# Box Plot\nnum_fig = len(eda_col_list)\nncols_fig = 4\nnrows_fig = math.ceil(num_fig\/4)\ngs = gridspec.GridSpec(nrows_fig, ncols_fig)\nfig, axs = plt.subplots(figsize=(15,20))\nred_square = dict(markerfacecolor='r', marker='s')\n\nfor n in range(num_fig):\n    ax = fig.add_subplot(gs[n])\n    ax.boxplot(df[eda_col_list[n]], flierprops=red_square, notch='True',patch_artist=True)\n    ax.set_title(eda_col_list[n])","729f21b1":"import itertools\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nimport math\nimport numpy as np\n\n# ! pip3 install colorspacious\nfrom matplotlib import cm\nfrom colorspacious import cspace_converter\nfrom collections import OrderedDict\n\n\neda_col_list = [col for col in train.columns if col not in ['fiberID', 'type', 'id']]\n\nprint(eda_col_list)\n\nunique_labels = list(train['type'].unique())\n\n# for Color\n# https:\/\/stackoverflow.com\/questions\/53283813\/legend-in-separate-subplot-and-grid\nlabel2idx = {val: i for i, val in enumerate(unique_labels)}\nnew_colors = ['C'+str(label2idx[label]) for label in unique_labels]\n\n# plot\nfig, ax = plt.subplots(figsize=(12, 15))\n\nnum_fig = len(eda_col_list)\nncols_fig = 3\nnrows_fig = math.ceil(num_fig\/ncols_fig)\n\n# gs = gridspec.GridSpec(rows, cols)\ngs = gridspec.GridSpec(nrows_fig, ncols_fig)\n# figure = plt.figure()\n# plt.clf()\n\nfig.legend([plt.plot([], [], c=c)[0] for c in new_colors], unique_labels, loc='upper right')\n\n\nfor i, n in enumerate(range(num_fig)):\n  # if i < 5:\n  ax = fig.add_subplot(gs[n])\n      ax.scatter(train.id, train[eda_col_list[n]], c=['C'+str(label2idx[label]) for label in train.type.values], cmap=plt.cm.RdYlGn)\n  # ax.text(train.id+.03, train[eda_col_list[n]]+.03, train['type'])\n  ax.set_title(eda_col_list[n])\n  # else:\n  #   break\nplt.show()","6a44bcb9":"colormap = plt.cm.viridis\nplt.figure(figsize=(25,25))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)","06e7667f":"len(train_transaction.columns)","f7d214d3":"# len(train_transaction_chk[train_transaction_chk['dtypes']!='object'].index)","3dc5afa1":"len(train_transaction.select_dtypes('object').columns)","06f0a713":"len(train_transaction.select_dtypes(['float', 'int']).columns)","dff4eccc":"train_transaction_chk[train_transaction_chk['dtypes']=='int64']","cce52687":"# max_val\ub85c numeric value \uccb4\ud06c\ntrain_transaction[[col for col in train_transaction_chk[train_transaction_chk['dtypes']!='object'].index if col not in ['TransactionID', 'isFraud', 'TransactionDT']]].max()","0c88671a":"num_col_chk = [col for col in train_transaction_chk[train_transaction_chk['dtypes']!='object'].index if col not in ['TransactionID', 'isFraud', 'TransactionDT']]","8d8fb05b":"plt.hist(train_transaction[num_col_chk].max().values)","6e713a78":"# avg - max\n# avg - std\n# median - iqr","bb0ebdb7":"np.percentile(train_transaction[num_col_chk], q = 75)","7b7f69e8":"train_transaction[num_col_chk].agg([np.percentile(75)])","9972acde":"train_transaction[num_col_chk].describe(percentiles =[0.75, 0.25])","48df5cdd":"def percentile(n):\n    def percentile_(x):\n        return np.quantile(n)\n    percentile_.__name__ = 'percentile_{:2.0f}'.format(n*100)\n    return percentile_","f1685393":"train_transaction[num_col_chk].quantile([.25, .75])","fd9f4175":"train_transaction[num_col_chk].quantile([.75-.25])","81070fb9":"train_transaction[num_col_chk].agg(['quantile'])","3fca47e5":"train_transaction[num_col_chk].quantile([.25, .75]).T.apply(lambda x: x[0.75]-x[0.25], axis=1)","e22d056f":"np.quantile()","2369db86":"scatter_df = train_transaction[num_col_chk].agg(['max', 'mean']).T\ng = sns.scatterplot(x='mean', y='max', data=scatter_df)\n\n\n\nfor i in range(len(scatter_df)):\n    \n    g.text(scatter_df['mean'][i], scatter_df['max'][i], scatter_df.index[i], rotation=45)\n\n\n\n# (idea) \uac01 fraud\uc640 non-fraud\uc5d0\uc11c \uac16\ub294 \uac12\ub4e4\uc744 \ube44\uad50.. \uc77c\uc815\uc774\uc0c1 \ucc28\uc774\uac00 \ub098\ub294 \uac83\uacfc \uc548\ub098\ub294 \uac83\uacfc \ub2e4\ub978 labeling\n## \ucc38\uace0 : https:\/\/seaborn.pydata.org\/generated\/seaborn.scatterplot.html","32e34185":"train_transaction_chk[train_transaction_chk.index=='V160']","afd55a42":"train_transaction['V160'].value_counts()","638d9a3a":"train_transaction['card1'].hist()","4f1b840c":"\nfor i in range(len(scatter_df)):\n#     if i > 10:\n#         break\n#     print(i)\n#     print(scatter_df['max'][i])\n#     print(scatter_df['mean'][i])\n#     print(scatter_df.index[i])\n    \n    g.text(scatter_df['mean'][i], scatter_df['max'][i], scatter_df.index[i])\n\n    \n    \n# for p in g.patches:\n#     height = p.get_height()\n#     g.text(p.get_x()+p.get_width()\/2., height + 3, '{:1.2f}%'.format(height\/total*100),\n#           ha=\"center\", fontsize=10)\n","eb38be73":"train_transaction[num_col_chk].agg(['max', 'mean']).T.plot.scatter(x='mean', y='max',c='DarkBlue')","273be0c2":"train_transaction[num_col_chk].agg(['std', 'mean']).T.plot.scatter(x='std', y='mean',c='DarkBlue')","a14fd382":"train_transaction[num_col_chk].std().max()","b1fcd18c":"train_transaction[num_col_chk].std().min()","a3b6f257":"train_transaction[num_col_chk].max().max()","ba5bf35f":"# max_val\ub85c numeric value \uccb4\ud06c\nsns.distplot(train_transaction[[col for col in train_transaction_chk[train_transaction_chk['dtypes']!='object'].index if col not in ['TransactionID', 'isFraud', 'TransactionDT']]].max().values, hist=True)","2d52948e":"train_transaction_chk[train_transaction_chk['dtypes']=='int64'].index","973cec82":"# Using From this kernael : https:\/\/www.kaggle.com\/artgor\/eda-and-models\nimport os\nimport time\nimport datetime\nimport json\nimport gc\nfrom numba import jit\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn import metrics\n\nfrom itertools import product\n\nimport altair as alt\nfrom altair.vega import v5\nfrom IPython.display import HTML\n\n# using ideas from this kernel: https:\/\/www.kaggle.com\/notslush\/altair-visualization-2018-stackoverflow-survey\n\n# using ideas from this kernel: https:\/\/www.kaggle.com\/notslush\/altair-visualization-2018-stackoverflow-survey\ndef prepare_altair():\n    \"\"\"\n    Helper function to prepare altair for working.\n    \"\"\"\n\n    vega_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega@' + v5.SCHEMA_VERSION\n    vega_lib_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lib'\n    vega_lite_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lite@' + alt.SCHEMA_VERSION\n    vega_embed_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-embed@3'\n    noext = \"?noext\"\n    \n    paths = {\n        'vega': vega_url + noext,\n        'vega-lib': vega_lib_url + noext,\n        'vega-lite': vega_lite_url + noext,\n        'vega-embed': vega_embed_url + noext\n    }\n    \n    workaround = f\"\"\"    requirejs.config({{\n        baseUrl: 'https:\/\/cdn.jsdelivr.net\/npm\/',\n        paths: {paths}\n    }});\n    \"\"\"\n    \n    return workaround\n\n\n\n\n\ndef add_autoincrement(render_func):\n    # Keep track of unique <div\/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n\n\n\n\n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    \"\"\"\n    Helper function to plot altair visualizations.\n    \"\"\"\n    chart_str = \"\"\"\n    <div id=\"{id}\"><\/div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    <\/script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n    \n    \ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage(deep=True).sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage(deep=True).sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\n\n@jit\ndef fast_auc(y_true, y_prob):\n    \"\"\"\n    fast roc_auc computation: https:\/\/www.kaggle.com\/c\/microsoft-malware-prediction\/discussion\/76013\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    nfalse = 0\n    auc = 0\n    n = len(y_true)\n    for i in range(n):\n        y_i = y_true[i]\n        nfalse += (1 - y_i)\n        auc += y_i * nfalse\n    auc \/= (nfalse * (n - nfalse))\n    return auc\n\n\ndef eval_auc(y_true, y_pred):\n    \"\"\"\n    Fast auc eval function for lgb.\n    \"\"\"\n    return 'auc', fast_auc(y_true, y_pred), True\n\n\ndef group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n    \"\"\"\n    Fast metric computation for this competition: https:\/\/www.kaggle.com\/c\/champs-scalar-coupling\n    Code is from this kernel: https:\/\/www.kaggle.com\/uberkinder\/efficient-metric\n    \"\"\"\n    maes = (y_true-y_pred).abs().groupby(types).mean()\n    return np.log(maes.map(lambda x: max(x, floor))).mean()\n\n\n\ndef train_model_regression(X, X_test, y, params, folds=None, model_type='lgb', eval_metric='mae', columns=None, plot_feature_importance=False, model=None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3):\n    \"\"\"\n    A function to train a variety of regression models.\n    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n    \n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns is None else columns\n    X_test = X_test[columns]\n    splits = folds.split(X) if splits is None else splits\n    n_splits = folds.n_splits if splits is None else n_folds\n    \n    # to set up scoring parameters\n    metrics_dict = {'mae': {'lgb_metric_name': 'mae',\n                        'catboost_metric_name': 'MAE',\n                        'sklearn_scoring_function': metrics.mean_absolute_error},\n                    'group_mae': {'lgb_metric_name': 'mae',\n                        'catboost_metric_name': 'MAE',\n                        'scoring_function': group_mean_log_mae},\n                    'mse': {'lgb_metric_name': 'mse',\n                        'catboost_metric_name': 'MSE',\n                        'sklearn_scoring_function': metrics.mean_squared_error}\n                    }\n\n    \n    result_dict = {}\n    \n    # out-of-fold predictions on train data\n    oof = np.zeros(len(X))\n    \n    # averaged predictions on train data\n    prediction = np.zeros(len(X_test))\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(splits):\n        if verbose:\n            print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        if model_type == 'lgb':\n            model = lgb.LGBMRegressor(**params, n_estimators = n_estimators, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=verbose, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict(X_test).reshape(-1,)\n        \n        if model_type == 'cat':\n            model = CatBoostRegressor(iterations=20000,  eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        if eval_metric != 'group_mae':\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n        else:\n            scores.append(metrics_dict[eval_metric]['scoring_function'](y_valid, y_pred_valid, X_valid['type']))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction \/= n_splits\n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] \/= n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n        \n    return result_dict\n\n\n\n\n\ndef train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3, averaging='usual', n_jobs=-1):\n    \"\"\"\n    A function to train a variety of classification models.\n    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n    \n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns is None else columns\n    n_splits = folds.n_splits if splits is None else n_folds\n    X_test = X_test[columns]\n    \n    # to set up scoring parameters\n    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n                        'catboost_metric_name': 'AUC',\n                        'sklearn_scoring_function': metrics.roc_auc_score},\n                    }\n    \n    result_dict = {}\n    if averaging == 'usual':\n        # out-of-fold predictions on train data\n        oof = np.zeros((len(X), 1))\n\n        # averaged predictions on train data\n        prediction = np.zeros((len(X_test), 1))\n        \n    elif averaging == 'rank':\n        # out-of-fold predictions on train data\n        oof = np.zeros((len(X), 1))\n\n        # averaged predictions on train data\n        prediction = np.zeros((len(X_test), 1))\n\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        if model_type == 'lgb':\n            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = n_jobs)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict_proba(X_valid)[:, 1]\n            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)[:, 1]\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict_proba(X_test)\n        \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n                                      loss_function=Logloss)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        if averaging == 'usual':\n            \n            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n            \n            prediction += y_pred.reshape(-1, 1)\n\n        elif averaging == 'rank':\n                                  \n            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n                                  \n            prediction += pd.Series(y_pred).rank().values.reshape(-1, 1)        \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction \/= n_splits\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] \/= n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n            result_dict['top_columns'] = cols\n        \n    return result_dict\n","356dd416":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport pandas as pd\ntest_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv')\ntrain_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv')\ntest_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/sample_submission.csv')\ntrain_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv')","02785af2":"check_dataframe(train_transaction)","5534aaa6":"train_transaction['V335'].isnull().sum()","07849baa":"# unique()\ub294 nan\uc744 \ud3ec\ud568\ud55c\ub2e4\nlen(train_transaction['V335'].unique())","3c3874c5":"# value_counts()\ub294 nan\uc744 \ud3ec\ud568 \ud558\uc9c0 \uc54a\ub294\ub2e4. \nlen(train_transaction['V335'].value_counts().index)","c95f834f":"def resumetable(df):\n    \n    ## Take This Function From https:\/\/www.kaggle.com\/kabure\/extensive-eda-and-modeling-xgb-hyperopt\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    \n    \n    ## Add new\n    for name in df.columns:\n        ","1a4c44b2":"print('{} Columns ---> {}\\n'.format('train_identity', len(test_identity.columns)))\n# print(train_identity.info())\n# train_identity.head()\n\nprint('{} Columns ---> {}\\n'.format('train_transaction', len(test_transaction.columns)))\n# print(train_transaction.info())\n# train_transaction.head()\n\nprint(\"<Compare TransactinId>\")\n# TransactionID\ub85c \ub9e4\ud551\uc774 \ub41c\ub2e4\uace0 \ud588\uc9c0\ub9cc.. identity \uc815\ubcf4\uac00 \ud6e8\uc52c \uc801\uc74c\nprint('train_transaction table -> Unique Of transactionId {} '.format(train_transaction['TransactionID'].nunique()))\nprint('train_identity table -> Unique Of transactionId {}'.format(train_identity['TransactionID'].nunique()))\nprint(str(math.floor(train_identity['TransactionID'].nunique()\/train_transaction['TransactionID'].nunique()*100))+'%')","2da24bdb":"# missing value count\ndef missing_values_count(df):\n    missing_values_count = df.isnull().sum()\n    total_cells = np.product(df.shape) # ((590540, 394) -> (590540 * 394))\n    total_missing = missing_values_count.sum()\n    return \"% of missing data = \",(total_missing\/total_cells) * 100\n\nprint(missing_values_count(train_transaction))\nprint(missing_values_count(train_identity))\n\n","9bf678ff":"summary = pd.DataFrame(train_transaction.dtypes, columns=['dtypes'])","9551a176":"summary","43d43708":"summary = summary.reset_index()\nsummary","9d50b776":"summary['Name'] = summary['index']\nsummary","7bafe755":"summary = summary[['Name', 'dtypes']]\nsummary","e0af5678":"summary.isnull().sum().values","f72ceb04":"summary = summary[['Name', 'dtypes']]\n\nsummary['Missing'] = train_transaction.isnull().sum().values\nsummary['Uniques'] = train_transaction.nunique().values\nsummary['First Value'] = train_transaction.loc[0].values\nsummary['Second Value'] = train_transaction.loc[1].values\nsummary['Third Value'] = train_transaction.loc[2].values","168735aa":"summary","d490d5ff":"for name in summary['Name'].value_counts().index:\n    print(name)","81e2e64a":"# stats.entropy \n# scipy.stats.entropy(pk, qk=None, base=None, axis=0)[source]\n# Calculate the entropy of a distribution for given probability values.\n# About Entropy [entropy](https:\/\/datascienceschool.net\/view-notebook\/d3ecf5cc7027441c8509c0cae7fea088\/)\n\nimport scipy.stats as stats\n\nfor name in summary['Name']:\n    print(name)\n#     summary.loc[summary['Name'] == name, 'Entropy']\n    print(round(stats.entropy(train_transaction[name].value_counts(normalize=True), base=2), 2))","b7a8e59e":"# from scipy.stats import entropy\nimport scipy","6d598bad":"# TransactionID\ub85c \ub450 \ud14c\uc774\ube14\uc744 \uc870\uc778\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n\n# del train_identity, train_transaction, test_transaction, test_identity\n\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","7022e4f7":"print(f'There are {train.isnull().any().sum()} columns in train dataset with missing values')","31af879a":"# Value check each columns\n\none_value_cols = [col for col in train.columns if train[col].nunique() <= 1]\none_value_cols_test = [col for col in test.columns if test[col].nunique() <= 1]\nprint(one_value_cols == one_value_cols_test)\nprint(one_value_cols)\nprint(one_value_cols_test)\n# ['a', 'b'] ==  ['b', 'a'] \ub3c4 False\uac00 \ub098\uc624\ubbc0\ub85c \ub2e4\ub978 \ubc29\ubc95 \ube44\uad50 \ud544\uc694","fca14794":"pd.merge(train_transaction, train_identity, on='TransactionID', how='inner')['TransactionID'].nunique()","5e81ab1e":"print(f'{train_identity.shape}')\nprint(f'{train_transaction.shape}')","ba54b025":"# \uac01 \uac12\ub4e4\uc758 unique\ud55c \uac12\uacfc null\uac12\uc744 \uc5bc\ub9c8\ub098 \ud3ec\ud568\ud558\uace0 \uc788\ub294\uc9c0 \ud655\uc778\n# \uc774\ub4e4\uc744 \uadf8\ub798\ud504\ub85c \uadf8\ub824\ubcf4\uc790\nprint(train_identity.columns)\nprint(train_transaction.columns)","5e172554":"def check_dataframe(df):\n\n    dict_check_value = {}\n    list_check_value = []\n    for col in df.columns:\n    #     print(col)\n        null_value = df[col].isnull().sum()\n        unique_value_exam = df[col][~df[col].isnull()].unique()\n        unique_value = len(unique_value_exam)\n        unique_value_exam = unique_value_exam[:5]\n        value_type = df[col].dtype\n\n\n        list_check_value = [unique_value, null_value, unique_value_exam, value_type]\n\n\n        dict_check_value[col] = list_check_value\n    \n    new_df = pd.DataFrame.from_dict(dict_check_value, orient='index', columns=['unique_value', 'isNullcnt', 'value_exam', 'value_type'])\n    return new_df","ecd0ecc9":"check_identity = check_dataframe(train_identity)\ncheck_transaction = check_dataframe(train_transaction)","fc2d164f":"check_identity.head()","874b9f78":"check_transaction.head()","e0a5f6d4":"# identity \ubd80\ud130 \uc0b4\ud3b4\ubcf4\uba74 \ncheck_identity['value_type'].value_counts().to_frame()\ncheck_identity['value_type'].value_counts().keys()\ncheck_identity['value_type'].value_counts().tolist() # https:\/\/stackoverflow.com\/questions\/35523635\/extract-values-in-pandas-value-counts","db4c58d3":"# \uac01 \ud14c\uc774\ube14\uc740 \uc5b4\ub5a0\ud55c \ub370\uc774\ud130\ud0c0\uc785\uc758 \ub370\uc774\ud130\ub85c \uc774\ub8e8\uc5b4\uc838 \uc788\ub294\uc9c0\ndef split_ValueCounts(a, b):\n    return [a, b]\n\ndef make_String(stat_df):\n    \"\"\"\n    Using Statistics DataFrame\n    \"\"\"\n    total = len(stat_df.index)\n    stat_list = stat_df['value_type'].value_counts().to_frame().reset_index().apply(lambda x: split_ValueCounts(x['index'], x['value_type']), axis=1)\n    stat_str = ','.join([(str(x[0])+\"\ub294 \"+str(x[1])+'\uac1c('+str(math.floor(x[1]\/total*100))+'%)') for x in stat_list])\n    return f'{total}\uac1c \uceec\ub7fc \uc911 {stat_str}\uc774\ub2e4.'\n\nprint(make_String(check_identity))\nprint(make_String(check_transaction))\n# aa= check_identity['value_type'].value_counts().to_frame().reset_index().apply(lambda x: split_value(x['index'], x['value_type']), axis=1)\n\nprint(make_String(check_dataframe(train)))","84a6e114":"import matplotlib.pyplot as plt\ndef check_DataEDA_withlineChart(df, columns): # figsize -> default (30, 20), \ub123\uc744 \uc218 \uc788\ub3c4\ub85d.. \n    \"\"\"\n    df = Stats DataFrame\n    columns = df.columns.to_list(), type=[]\n    \"\"\"\n    \n    print(len(columns))\n    # Working With Columns that they are Selected\n    new_df = df[df.index.isin(columns)].copy(deep=True)\n#     new_df = df[columns].copy(deep=True)\n    \n    \n    # Check Ordering and Ordered Number .. \n    col_2_idx_dict = {}\n    for i, column in enumerate(columns):\n        col_2_idx_dict[column] = i\n    \n    # inverted_dict \n    idx_2_col_dict = {val: key for key, val in col_2_idx_dict.items()}\n    \n    cols_idx = sorted(idx_2_col_dict.keys())\n    \n#     print(cols_idx)\n#     print(len(cols_idx))\n    # Draw Line Chart\n    fig = plt.figure(figsize=(30, 20))\n    \n    \n    x1 = new_df['unique_value']\n    x2 = new_df['isNullcnt']\n    \n    \n    plt.plot(cols_idx, new_df['unique_value'][idx_2_col_dict], label='unique_value', linestyle='-', marker='x')\n    plt.plot(cols_idx, new_df['isNullcnt'][idx_2_col_dict], label='isNullcnt', linestyle='--', marker='o')\n    \n    plt.legend()\n    plt.xlabel(\"Number Of Columns\")\n    plt.ylabel(\"Values\")\n    plt.title(\"CheckData with LineChart that NullVal and UniqVal\")\n    \n    # Annotate Infomation\n    print(idx_2_col_dict)\n    for i in sorted(idx_2_col_dict.keys()):\n#         print(i)\n#         print(idx_2_col_dict[i])\n\n        # Annotate DataType\n        width = i\n        height = new_df['unique_value'][idx_2_col_dict[i]]\n        text = str(new_df['value_type'][idx_2_col_dict[i]])\n        plt.annotate(text, xy=(width, height), xytext=(width+0.05, height+0.3), rotation=70)\n        \n        # Annotate Uniq Values Number\n        text = str(height)+'(n)' # str(check_identity['value_type'][i])\n        plt.annotate(text, xy=(width, height), xytext=(1, +50), textcoords=\"offset points\", va=\"top\", color='b', rotation=0)\n        \n        # Annotate Null Values \n#         total_num = max([new_df['unique_value'].max(), new_df['isNullcnt'].max()])\n        \n        # Stat Dataframe\uc5d0 \uc0ac\uc6a9\ub41c \uae30\ubcf8 \ub370\uc774\ud130\ud504\ub808\uc784\n        total_num = len(train_identity)\n    \n        text = str(math.floor(new_df['isNullcnt'][idx_2_col_dict[i]]\/total_num*100))+'%'\n        plt.annotate(text, xy=(width, height), xytext=(1, +70), textcoords=\"offset points\", va=\"top\", color='orange', rotation=0)\n        \n        \n    fx1, fx2, fy1, fy2 = plt.axis() # fig.axis()\n    plt.annotate(\"The Red Text: Num of Unique Values\", xy=(fx2, fy2), xytext=(1, 1), textcoords=\"offset points\", ha=\"right\", va=\"bottom\", color='b')\n    plt.annotate(\"The Black Text: type Of Data\", xy=(fx2, fy2), xytext=(1, 10), textcoords=\"offset points\", ha=\"right\", va=\"bottom\")\n    plt.annotate(\"The Orange Text: Percentage Of Null value\", xy=(fx2, fy2), xytext=(1, 20), textcoords=\"offset points\", ha=\"right\", va=\"bottom\", color='orange')\n    plt.xticks(cols_idx, list([val for key, val in sorted(idx_2_col_dict.items())]), rotation=90)\n    \n    return fig\n    \n    \n    ","29e504e2":"columns = check_identity.index\ncheck_DataEDA_withlineChart(check_identity, columns)","453f7746":"import matplotlib.pyplot as plt\n# NULL\uac12\uc744 \ub9ce\uc774 \ud3ec\ud568\ud558\ub294 \uacbd\uc6b0\ncheck_identity['isNullper'] = check_identity['isNullcnt'].apply(lambda x: x\/train_identity.shape[0])\n\n# \ubaa8\ub4e0 \uceec\ub7fc \uc870\ud68c\n# select_col = check_identity.index\n\n# Null\uac12\uc774 \ub9ce\uc740 \uceec\ub7fc \uccb4\ud06c\nselect_col = check_identity[check_identity['isNullper']>0.9].index\nprint(f'{len(select_col)}\uacfc {select_col}')\n\n\nplt.hist(check_identity[check_identity.index.isin(select_col)]['isNullper'])","9d7dd5ae":"check_identity[check_identity.index.isin(select_col)]","3464c840":"# \uc0ac\uae30\uac70\ub798 \uc911 97%\uac12\uc740 \ube44\uc5b4\uc788\ub2e4. \n[train[train['isFraud']==1][col].isnull().sum()\/train[train['isFraud']==1].shape[0] for col in select_col]","3feb58db":"# \uc0ac\uae30\uac70\ub798\uac00 \uc544\ub2cc \uac83 \uc911 99%\uac00 \ube44\uc5b4\uc788\ub2e4. .. \uc774 \ucc28\uc774\ub294 \uc720\uc758\ubbf8\ud55c\uac00?\n[train[train['isFraud']==0][col].isnull().sum()\/train[train['isFraud']==0].shape[0] for col in select_col]","3b2178e0":"remove_cols_identity = ['id_07', 'id_08', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26','id_27']","859b8142":"# Null value\uac00 \ub9ce\uc740 \uac12 \uc81c\uc678\n# Null\uac12\uc774 \ub9ce\uc740 \uceec\ub7fc\uc744 \uc81c\uc678\ud55c \uce7c\ub7fc\ub4e4\uc744 \uc800\uc7a5\nselect_col = check_identity[~check_identity.index.isin(select_col)].index\nprint(f'{len(select_col)}\uacfc {select_col}')\n\n\nplt.hist(check_identity[check_identity.index.isin(select_col)]['isNullper'])","2ed34ef4":"remian_identity_col = ['TransactionID', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06',\n       'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16',\n       'id_17', 'id_18', 'id_19', 'id_20', 'id_28', 'id_29', 'id_30', 'id_31',\n       'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\n       'DeviceType', 'DeviceInfo']","3185a517":"columns = check_identity[check_identity.index.isin(select_col)].index\ncheck_DataEDA_withlineChart(check_identity, columns)","e149291f":"check_identity[(check_identity['value_type']=='object') & (check_identity.index.isin(select_col))]","ea1d3998":"# \ub108\ubb34 \ub9ce\uc740 \uac12\ub4e4\uc744 \uac00\uc9c4 \uacbd\uc6b0\ub294 \ub098\uc911\uc5d0 \ucc98\ub9ac\n\ntooMany_col_identity = check_identity[(check_identity['value_type']=='object') & (check_identity.index.isin(select_col))&(check_identity['unique_value']>=5)].index\nprint(tooMany_col_identity)","d9cc558a":"tooMany_col_identity = tooMany_col_identity.to_list()\nremove_cols_identity.extend(tooMany_col_identity)","13d7649b":"remove_cols_identity = ['id_07', 'id_08', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_30', 'id_31', 'id_33', 'DeviceInfo']","4ad06759":"del check_identity","880ec435":"# NULL\uac12\uc744 \ub9ce\uc774 \ud3ec\ud568\ud558\ub294 \uacbd\uc6b0\ncheck_transaction['isNullper'] = check_transaction['isNullcnt'].apply(lambda x: x\/train_transaction.shape[0])\n\n# \ubaa8\ub4e0 \uceec\ub7fc \uc870\ud68c\n# select_col = check_identity.index\n\n# Null\uac12\uc774 \ub9ce\uc740 \uceec\ub7fc \uccb4\ud06c\nselect_col = check_transaction[check_transaction['isNullper']>0.7].index\nprint(f'{len(select_col)}\uacfc {select_col}')\n\nplt.hist(check_transaction['isNullper'])\nplt.hist(check_transaction[check_transaction.index.isin(select_col)]['isNullper'])\n\nprint(len(select_col))\nprint(select_col)","3a7b0cbf":"# \uc6b0\uc120 Null \uac12\uc774 \ub9ce\uc740 \uce7c\ub7fc\uc744 \uc81c\uac70\ud558\uace0, object\uc911 \uac12\uc774 \ub192\uc740 \uac83\ub4e4\ub3c4 \uc81c\uc678... \nremove_cols_transacton = select_col.copy()\n\ncheck_transaction[(~check_transaction.index.isin(remove_cols_transacton))&(check_transaction['value_type']=='object')]\n\n\nremove_cols_transacton = remove_cols_transacton.to_list() # .append(\"P_emaildomain\")\n\nremove_cols_transacton.append(\"P_emaildomain\")\n\nremove_cols_transacton\n\n# remove_cols_transacton = ['dist2','R_emaildomain','D6','D7','D8','D9','D12','D13','D14','V138','V139','V140','V141','V142','V143','V144','V145','V146','V147','V148','V149','V150','V151','V152','V153','V154','V155','V156','V157','V158','V159','V160','V161','V162','V163','V164','V165','V166','V167','V168','V169','V170','V171','V172','V173','V174','V175','V176','V177','V178','V179','V180','V181','V182','V183','V184','V185','V186','V187','V188','V189','V190','V191','V192','V193','V194','V195','V196','V197','V198','V199','V200','V201','V202','V203','V204','V205','V206','V207','V208','V209','V210','V211','V212','V213','V214','V215','V216','V217','V218','V219','V220','V221','V222','V223','V224','V225','V226','V227','V228','V229','V230','V231','V232','V233','V234','V235','V236','V237','V238','V239','V240','V241','V242','V243','V244','V245','V246','V247','V248','V249','V250','V251','V252','V253','V254','V255','V256','V257','V258','V259','V260','V261','V262','V263','V264','V265','V266','V267','V268','V269','V270','V271','V272','V273','V274','V275','V276','V277','V278','V322','V323','V324','V325','V326','V327','V328','V329','V330','V331','V332','V333','V334','V335','V336','V337','V338','V339','P_emaildomain']","79aa2f5a":"remain_cols = [col for col in train.columns if col not in remove_cols_identity+remove_cols_transacton ]","69b42633":"print(remain_cols)\n# remian_cols = ['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D10', 'D11', 'D15', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_28', 'id_29', 'id_32', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType']","736b7378":"# Columns \uc9c0\uc6b0\uae30 \uc804\uc5d0 train \uce7c\ub7fc \ubcf4\uad00\n# _train = train.copy(deep=True)\ntrain = train[remain_cols]","a18b9786":"train.shape  # \uce7c\ub7fc \uac1c\uc218\ub97c 434\uac1c\uc5d0\uc11c 252\uac1c\ub85c .. ","fc8bfe2b":"test.columns = [val.replace('-', '_') if 'id' in val else val for val in list(test.columns)]","7032f945":"# _test = test.copy(deep=True)\nremain_test_cols = [col for col in remain_cols if col not in ['isFraud']]\ntest = test[remain_test_cols]","672c4fb0":"train.info()\nnew_train = check_dataframe(train)\nnew_train.head()","ab4e663c":"print(len(train.select_dtypes('object').columns))\nprint(len(new_train[new_train['value_type']=='object']))\ncat_cols = new_train[new_train['value_type']=='object'].index.to_list()","2beabcea":"del new_train","8e80cdd3":"from sklearn import preprocessing\nfor col in cat_cols:\n    if col in train.columns:\n        le = preprocessing.LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values))","b77b7b1f":"X = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT', 'TransactionID'], axis=1)\n","7264af1a":"y = train.sort_values('TransactionDT')['isFraud']\n\nX_test = test.drop(['TransactionDT', 'TransactionID'], axis=1)\n\ntest = test[['TransactionDT', 'TransactionID']]","0044a64f":"# null \ucc98\ub9ac\ndef clean_inf_nan(df):\n    return df.replace([np.inf, -np.inf], np.nan)\n\n# Cleaning infinite values to NaN\nX = clean_inf_nan(X)\nX_test = clean_inf_nan(X_test)","50bcdddb":"import gc\ngc.collect()\nprint(\"Garbage collection thresholds: {}\".format(gc.get_threshold()))\n\n# \ucd9c\ucc98: https:\/\/weicomes.tistory.com\/277 [25%]","0f9a52ab":"from sklearn.model_selection import train_test_split, cross_val_predict, TimeSeriesSplit, KFold, cross_val_score\nn_fold = 5\nfolds = TimeSeriesSplit(n_splits=n_fold)\nfolds = KFold(n_splits=5)","9e10fa0b":"categorical_features_indices = np.where(X.dtypes != np.float)[0]\n\nclf = CatBoostClassifier(random_seed=rnd_state)\n\nclf.fit(X_train, y_train, cat_features=categorical_features_indices)\nclf.score(X_val, y_val)","a08785a3":"params = {'num_leaves': 256,\n          'min_child_samples': 79,\n          'objective': 'binary',\n          'max_depth': 13,\n          'learning_rate': 0.03,\n          \"boosting_type\": \"gbdt\",\n          \"subsample_freq\": 3,\n          \"subsample\": 0.9,\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3,\n          'reg_lambda': 0.3,\n          'colsample_bytree': 0.9,\n          #'categorical_feature': cat_cols\n         }\nresult_dict_lgb = train_model_classification(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='lgb', eval_metric='auc', plot_feature_importance=True,\n                                                      verbose=500, early_stopping_rounds=200, n_estimators=5000, averaging='usual', n_jobs=-1)","94e918fd":"sample_submission['isFraud']","0fcd87d6":"sample_submission['isFraud'] = result_dict_lgb['prediction']","6ddb7354":"sample_submission.to_csv('submission.csv', index=False)","d815da65":"sample_submission.head()","35811869":"pd.DataFrame(result_dict_lgb['oof']).to_csv('lgb_oof.csv', index=False)","2556d505":"! ls","76a47344":"def check_DataEDA_withlineChart(df, columns): # figsize -> default (30, 20), \ub123\uc744 \uc218 \uc788\ub3c4\ub85d.. \n    \"\"\"\n    df = DataFrame\n    columns = df.columns.to_list(), type=[]\n    \"\"\"\n    \n    print(len(columns))\n    # Working With Columns that they are Selected\n    new_df = df[df.index.isin(columns)].copy(deep=True)\n#     new_df = df[columns].copy(deep=True)\n    \n    \n    # Check Ordering and Ordered Number .. \n    col_2_idx_dict = {}\n    for i, column in enumerate(columns):\n        col_2_idx_dict[column] = i\n    \n    # inverted_dict \n    idx_2_col_dict = {val: key for key, val in col_2_idx_dict.items()}\n    \n    cols_idx = sorted(idx_2_col_dict.keys())\n    \n    print(cols_idx)\n    print(len(cols_idx))\n    # Draw Line Chart\n    fig = plt.figure(figsize=(30, 20))\n    \n    \n    x1 = new_df['unique_value']\n    x2 = new_df['isNullcnt']\n    \n    \n    plt.plot(cols_idx, new_df['unique_value'][idx_2_col_dict], label='unique_value', linestyle='-', marker='x')\n    plt.plot(cols_idx, new_df['isNullcnt'][idx_2_col_dict], label='isNullcnt', linestyle='--', marker='o')\n    \n    plt.legend()\n    plt.xlabel(\"Number Of Columns\")\n    plt.ylabel(\"Values\")\n    plt.title(\"CheckData with LineChart that NullVal and UniqVal\")\n    \n    # Annotate Infomation\n    print(idx_2_col_dict)\n    for i in sorted(idx_2_col_dict.keys()):\n        print(i)\n        print(idx_2_col_dict[i])\n\n        \n        # Annotate DataType\n        width = i\n        height = new_df['unique_value'][idx_2_col_dict[i]]\n        text = str(new_df['value_type'][idx_2_col_dict[i]])\n        plt.annotate(text, xy=(width, height), xytext=(width+0.05, height+0.3), rotation=70)\n        \n        # Annotate Uniq Values Number\n        text = str(height)+'(n)' # str(check_identity['value_type'][i])\n        plt.annotate(text, xy=(width, height), xytext=(1, +50), textcoords=\"offset points\", va=\"top\", color='b', rotation=0)\n        \n        # Annotate Null Values \n#         total_num = max([new_df['unique_value'].max(), new_df['isNullcnt'].max()])\n        total_num = len(train)\n        text = str(math.floor(new_df['isNullcnt'][idx_2_col_dict[i]]\/total_num*100))+'%'\n        plt.annotate(text, xy=(width, height), xytext=(1, +70), textcoords=\"offset points\", va=\"top\", color='orange', rotation=0)\n        \n        \n    fx1, fx2, fy1, fy2 = plt.axis() # fig.axis()\n    plt.annotate(\"The Red Text: Num of Unique Values\", xy=(fx2, fy2), xytext=(1, 1), textcoords=\"offset points\", ha=\"right\", va=\"bottom\", color='b')\n    plt.annotate(\"The Black Text: type Of Data\", xy=(fx2, fy2), xytext=(1, 10), textcoords=\"offset points\", ha=\"right\", va=\"bottom\")\n    plt.annotate(\"The Orange Text: Percentage Of Null value\", xy=(fx2, fy2), xytext=(1, 20), textcoords=\"offset points\", ha=\"right\", va=\"bottom\", color='orange')\n    plt.xticks(cols_idx, list([val for key, val in sorted(idx_2_col_dict.items())]), rotation=90)\n    \n    return fig\n    \n    \n    ","48a8b4ac":"select_col = check_train[(check_train['value_type']!='object') & (check_train['isNullper']<0.6) & (check_train['isNullper']>0.4)].index\ncheck_DataEDA_withlineChart(check_train, select_col)","aab691ce":"check_train[check_train.index.isin(['dist1', 'D2', 'D3'])]","0f96a5a7":"select_col = check_train[check_train['value_type']!='object'].index\nprint(len(select_col))\nplt.hist(check_train[check_train.index.isin(select_col)]['isNullper'])\nplt.title(\"Histogram Of Percetage Of Null Data\")","0b3845bc":"select_col = check_train.index\nprint(len(select_col))\nplt.hist(check_train[check_train.index.isin(select_col)]['isNullper'])\nplt.title(\"Histogram Of Percetage Of Null Data\")","1a8a1b06":"check_train[(check_train['value_type']!='object') & (check_train['isNullper']<0.6) & (check_train['isNullper']>0.4)]","58cb58f7":"train['card1'].isnull().sum()","a6cdc042":"train['card1'].nunique()","4320193a":"print(len(check_train[check_train['value_type']!='object'].index))\nprint(len(check_train[(check_train['value_type']!='object') & ((check_train['isNullper']<0.05))].index))","d92b296a":"check_train[check_train.index.isin(select_col)]","d02f9b6e":"\nplt.hist(check_train['isNullper'])\nplt.title(\"Histogram Of Percetage Of Null Data\")\ncheck_train[check_train['isNullper']<0.3].index","683664ce":"check_train[check_train.index=='card3']","441615e9":"select_col = list(check_train[(check_train['value_type']!='object')&(check_train['isNullper']<0.3)].index)\ncheck_DataEDA_withlineChart(check_train, select_col)","4b8c7179":"idx2col= {val: key for key, val in column_number.items()}","237aeef1":"cols_idx = list(column_number.values())\n\nfor i in idx2col.keys():\n    print(i)\n    print(check_identity['unique_value'][idx2col[i]])","48843b09":"plt.figure(figsize=(30, 20)) # width, height\n# plt.plot(check_identity.index, check_identity['unique_value'], label='unique_value', linestyle='-', marker='x')\n# plt.plot(check_identity.index, check_identity['isNullcnt'], label='isNullcnt', linestyle='--', marker='o')\nplt.plot(list(column_number.values()), check_identity['unique_value'], label='unique_value', linestyle='-', marker='x')\nplt.plot(list(column_number.values()), check_identity['isNullcnt'], label='isNullcnt', linestyle='--', marker='o')\n# plt.annotate\nplt.legend()\nplt.xlabel(\"Columns\")\nplt.ylabel(\"Values Each Cols\")\nfor i in column_number.values():\n    width = list(column_number.values())[i]\n    height = check_identity['unique_value'][i]\n#     print(width, height)\n    plt.annotate(str(check_identity['value_type'][i]), xy=(width, height), xytext=(width+0.05, height+0.3), rotation=70)\n    \nfor i in column_number.values():\n    width = list(column_number.values())[i]\n    height = check_identity['unique_value'][i]\n#     print(width, height)\n    plt.annotate(str(check_identity['unique_value'][i]), xy=(width, height), xytext=(width+0.05, height+5000), color='b', rotation=0)\n# plt.annotate(check_identity['value_type'], xy=(list(column_number.values()), check_identity['isNullcnt']+2))\n\n# null value\uc758 percentage\nfor i in column_number.values():\n    total_num = int(check_identity[check_identity.index=='TransactionID']['unique_value'])\n    width = list(column_number.values())[i]\n    height = check_identity['unique_value'][i]\n#     print(width, height)\n    plt.annotate(str(math.floor(check_identity['isNullcnt'][i]\/total_num*100))+'%', xy=(width, height), xytext=(1, +70), textcoords=\"offset points\", va=\"top\", color='orange', rotation=0)\n\n\nx1, x2, y1, y2 = plt.axis()\nplt.annotate(\"The Red Text: Num of Unique Values\", xy=(x2, y2), xytext=(1, 1), textcoords=\"offset points\", ha=\"right\", va=\"bottom\", color='b')\nplt.annotate(\"The Black Text: type Of Data\", xy=(x2, y2), xytext=(1, 10), textcoords=\"offset points\", ha=\"right\", va=\"bottom\")\nplt.annotate(\"The Orange Text: Percentage Of Null value\", xy=(x2, y2), xytext=(1, 20), textcoords=\"offset points\", ha=\"right\", va=\"bottom\", color='orange')\nplt.xticks(list(column_number.values()), list(column_number.keys()), rotation=90)\n# plt.xticks(rotation=90, fontsize=20)\n# plt.xticks(list(column_number.keys()))\n\nplt.title(\"Check unique and null values\")","9b0be81a":"check_train[check_train['value_type'] == 'object']","b2622603":"check_train['value_type'].value_counts()","5542b23e":"check_identity","9ec99e0d":"a = np.arange(5)\nb = np.arange(5, 10)\nplt.plot(a, b)\nbbox_args = dict(boxstyle=\"round\", fc=\"0.8\")\nfor i in range(len(a)):\n    plt.annotate('a', xy=(a[i], b[i]), xytext=(a[i]+0.1, b[i]+0))\n    plt.annotate('b', xy=(a[i], b[i]), xytext=(a[i], b[i]), color='b')\n\nx1, x2, y1, y2 = plt.axis()\nplt.annotate('test', xy=(x2, y2), textcoords=\"offset points\", ha=\"left\", va=\"bottom\")","4b64ad1e":"plt.axis()","b17badf9":"locs, labels = plt.xticks()","ab4345ee":"locs","80362bda":"labels","d5f13a2a":"labels = ('Tom', 'Dick', 'Harry', 'Sally', 'Sue')\nplt.xticks(np.arange(5), labels)","a9fbd2d5":"plt.plot([1, 2, 3, 4], [0.1, 0.2, 0.3, 0.4], \n         [1, 2, 3, 4], [2, 2, 2, 2])","94af6982":"import matplotlib.pyplot as plt\nplt.plot('unique_value', 'isNullcnt')\nplt.show()\n# plt.plot(y = 'isNullcnt',kind='line')","0d4887a6":"check_identity.index","aa797389":"list(check_identity.index)","3667eed8":"### References Study:\n- (1) scipy.stats.entropy : [About Entropy](https:\/\/datascienceschool.net\/view-notebook\/d3ecf5cc7027441c8509c0cae7fea088\/)\n- (2) heatmap : [Better Heatmaps and Correlation Matrix Plots in Python](https:\/\/towardsdatascience.com\/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec)\n- (3) missng value : [missingno github](https:\/\/github.com\/ResidentMario\/missingno), [How to Handle Missing Data](https:\/\/towardsdatascience.com\/how-to-handle-missing-data-8646b18db0d4)","31b350df":"# 2-3. Numeric Feature\n* \uac70\uc758 \ub300\ubd80\ubd84\uc774 `Numeric` Data\uc5d0 \ud574\ub2f9","b3485fba":"## Prepare data for modeling","62008104":"\uc704 \uadf8\ub798\ud504\uc5d0\uc11c identity\uc758 \uac01 \ud2b9\uc9d5\ub9c8\ub2e4\uc758 null value\uc758 \uc815\ub3c4, \uadf8\ub9ac\uace0 \uc5bc\ub9c8\ub098 \ub9ce\uc740 unique value\ub97c \uac00\uc9c0\uace0 \uc788\ub294\uc9c0 \uc0b4\ud3b4\ubcf4\ub824\uace0 \ud558\uc600\ub2e4. \n- \uce7c\ub7fc\uac2f\uc218\uac00 \ub9ce\uc740 \ub9cc\ud07c \ud558\ub098\uc529 \uc0b4\ud3b4\ubcf4\uae30 \uc5b4\ub824\uc6c0\n- \uba3c\uc800 \uc0b4\ud3b4\ubcfc \uc218 \uc788\ub294 \ub370\uc774\ud130\ub97c \uc815\ud558\uace0 \uc2f6\uc5c8\uc74c. \n- object\uc5d0\uc11c\ub294 \ub300\ub7b5\uc801\uc778 \uc5b4\ub5a4 \uac12\uc744 \uc4f8 \uc218 \uc788\ub294\uc9c0 \uc0b4\ud3b4\ubcf4\uace0 \uc2f6\uc5c8\uc74c\n- Float\ub77c\uba74 null value\uac00 \ub192\uc740 \uac12\uc744 \uc4f8 \uc218 \uc788\uc744\uc9c0 \ubbf8\uc9c0\uc218 \n\n-> \uc704\uc758 \uadf8\ub798\ud504\ub294 \uac12\ub4e4\uc774 \uc5b4\ub5bb\uac8c \ubd84\ud3ec\ub418\uc5b4\uc788\ub294\uc9c0\uc5d0 \ub300\ud574\uc11c\ub294 \uc54c \uc218 \uc5c6\uc74c","2c44bb67":"`C*` Columns","9e05f024":"# 2-2. TimeDelta Feature\n* \uc0ac\uae30\ud589\uac01\uc774 \uc2dc\uac04\uc5d0 \uc601\ud5a5\uc744 \ubc1b\ub294\uc9c0 \uc54c\uc544\ubcf8\ub2e4\n\n[Reference Kaggle](https:\/\/www.kaggle.com\/kabure\/extensive-eda-and-modeling-xgb-hyperopt)","8744b164":"`train_identity Table`\uc740 `train_transaction Table`\uc5d0 \ube44\ud574 \ub9ce\uc740 **TransactionID**\ub97c \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\ub2e4.\n* chek1. `Unique Of TransactionId` \uac1c\uc218\uac00 \ub2e4\ub978\ub370 \uc774\ub4e4\uc744 \uadf8\ub0e5 join \ud558\ub294\uc9c0\uc5d0 \ub300\ud55c \uc5ec\ubd80 \ud655\uc778","d964d688":"## IEEE Fraud Detection competition\nIn this kernel I work with [IEEE Fraud Detection competition](https:\/\/www.kaggle.com\/c\/ieee-fraud-detection?rvi=1)\n\n\n# 1. Overview\n\nCheck Kaggle Data Description [IEEE Fraud Detection competition Data Description](https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/data)\n\n# 1-1. Goal\n\n> In this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target isFraud\n\n# 1-2. Overview and Data Description\n- \uc628\ub77c\uc778 \uc0ac\uae30 \uac70\ub798\ub97c \ud0d0\uc9c0\ud558\ub294 Competition\uc73c\ub85c \uc0ac\uae30\uac70\ub798 \ub370\uc774\ud130\uc5d0\ub294 `isFraud`\uc5d0 1\ub85c \ud45c\uc2dc\n- \ub370\uc774\ud130\ub294 \ub450\uac1c\uc758 \ud30c\uc77c\ub85c \ub098\ub204\uc5b4\uc838 \uc788\ub2e4. `identity`\uc640 `transaction`\uc740 `TransactionID`\ub85c \ub9e4\ud551\uc2dc\ud0ac \uc218 \uc788\ub2e4. \ud558\uc9c0\ub9cc \ubaa8\ub4e0 \uac70\ub798\uc5d0 \uc2e0\ubd84\uc815\ubcf4\uac00 \uc788\ub294 \uac83\uc740 \uc544\ub2d8\uc73c\ub85c \uc8fc\uc758\ud560 \uac83. transaction\uc5d0 \ube44\ud574 identity\uc5d0\ub294 24% \uc815\ub3c4\uc758 trasactionID\ub9cc \uc874\uc7ac\n- `TransactionDT`\uc740 \uc2e4\uc81c timestamp\uac00 \uc544\ub2d8\n- \ub370\uc774\ud130\uc5d0 \ub300\ud55c \ub354 \ub9ce\uc740 \uc815\ubcf4\ub294 `Lynn@Vesta`\uac00 Discussion\uc5d0 \uc62c\ub824\uc900 \ub9ac\uc2a4\ud2b8\ub97c \ucc38\uace0\ud568 [Data Description (Details and Discussion)](https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/101203)\n- train_identity \ud14c\uc774\ube14\uc5d0\ub294 \ucd1d 41\uac1c\uc758 \uceec\ub7fc\uacfc train_transaction \ud14c\uc774\ube14\uc5d0\ub294 393\uac1c\uc758 \uceec\ub7fc\uc774 \uc788\uc74c\n\n\n## (1). Tansaction Table\n\n|  \t| Transaction Table \t| Is Categorical Feature \t|\n|--------------------------\t|---------------------------------------------------------------------------------------------------------------------------\t|------------------------\t|\n| TransactionDT \t| timedelta from a given reference datetime (not an actual timestamp) \t| False \t|\n| TransactionAMT \t| transaction payment amount in USD \t| False \t|\n| ProductCD \t| product code, the product for each transaction \t| True \t|\n| card1 - card6 \t| payment card information, such as card type, card category, issue bank, country, etc. \t| True \t|\n| addr \t| address \t| True \t|\n| dist \t| distance \t| False \t|\n| P_ and (R__) emaildomain \t| purchaser and recipient email domain \t| True \t|\n| C1-C14 \t| counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked. \t| False \t|\n| D1-D15 \t| timedelta, such as days between previous transaction, etc. \t| False \t|\n| M1-M9 \t| match, such as names on card and address, etc. \t| True \t|\n| Vxxx \t| Vesta engineered rich features, including ranking, counting, and other entity relations. \t| False \t|\n\n\n\n\n<br>\n<br>\n\n\n\n## (2). Identity Table\n\n\n----\n\n> Variables in this table are identity information - network connection information (IP, IPS, Proxy, etc) and digital signature (UA\/Browser\/os\/version, etc) associated with transactions.<br>They're collected by Vesta's fraud protection system and digital security partners.<br>(The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement) \n\n\n\n\n","bee1efe6":"* P_emaildomain \ubcf4\ub2e4 R_emaindomain\uc774 \uae30\uc785\uc774 \uc548\ub418\uc5b4 \uc788\ub294 \uacbd\uc6b0\uac00 \ub9ce\ub2e4.","9b29e2d3":"- indentity \ud14c\uc774\ube14\uc5d0\ub294 Object\ud615\uc758 \ub370\uc774\ud130\ub3c4 17\uac1c \uc874\uc7ac\n- transaction \ud14c\uc774\ube14\uc5d0\ub294 \ub300\ubd80\ubd84 Float\ud615\ud0dc\uc758 \ub370\uc774\ud130\uc774\uba74\uc11c object\uc758 \ud615\ud0dc\uc758 \ub370\uc774\ud130\ub3c4 3%\uc815\ub3c4 \uc874\uc7ac\ud55c\ub2e4. \n\n\n\uc758\ubbf8\uc788\ub294 \ub370\uc774\ud130\ub294 \ubb34\uc5c7\uc77c\uc9c0, \uc5b4\ub5a4 \ub370\uc774\ud130\ub97c \ubc84\ub824\uc57c\ud560\uc9c0\ub97c \ucc3e\uc744 \uc218 \uc788\uc744\uae4c?\n- \uac01 \ud14c\uc774\ube14\uc744 join\ud574\uc11c \ubcfc \uacbd\uc6b0, identity\ub294 \uc0c1\ub300\uc801\uc73c\ub85c Null\uac12\uc758 \ube44\uc728\uc744 \ube44\uad50\ud560 \uc218 \uc5c6\uc73c\ubbc0\ub85c \uac01\uc790 \uc9c4\ud589\ud574, \uac01 \ud14c\uc774\ube14\uc5d0 \uc720\uc6a9\ud574\ubcf4\uc774\ub294 \ub370\uc774\ud130\uc640 \uc544\ub2cc \uacbd\uc6b0\ub97c \ubf51\uc544\ubcf4\uc790","601e7762":"`card*`\ub294 \uce74\ud14c\uace0\ub9ac\uceec \ub370\uc774\ud130\ub77c\uace0 \uc4f0\uc5ec\uc788\uc9c0\ub9cc value type\uc740 Int\uc640 Float\uc600\ub2e4. Unique \uac2f\uc218\ub3c4 \ub192\uc740\uac83\uc73c\ub85c \ubd10\uc11c, \uc2e4\uc81c Numeric \ub370\uc774\ud130\ub85c \ubd10\uc57c\ud560\uc9c0\ub97c \uc0b4\ud3b4\ubd10\uc57c \ud568","48abb859":"* `D*` Columns","2c320de2":"- TransactinID\ub294 \uc81c\uc678\n- Float\ub294 \uc2a4\ucf00\uc77c\ub9c1 \ucc98\ub9ac\n- object\ub294 uniq\ud55c \uac12\uc5d0 \ub530\ub77c 2\uac1c, 3\uac1c, 65\uac1c, 130\uac1c, 260\uac1c, 4\uac1c, 1786\uac1c\ub97c \uc5b4\ub5bb\uac8c \ub2e4\ub8f0\uc9c0 \uc0b4\ud3b4\ubcf4\uc790","e88ff8b8":"## Data Exploration","9fd6bf7e":"## ML","79aa6d9e":"* `ProductCD` : **W**\uac00 70%\uc774\uc0c1\uc744 \ucc28\uc9c0\ud558\uace0, \ub2e4\ub978 4\uac1c\uc758 item\ub4e4\uc740 10%\uc804\ud6c4\ub85c \uac70\uc758 \ube44\uc2b7\ud558\ub2e4\uace0 \ud560 \uc218 \uc788\ub2e4. \ub2e8, **C**\uac00 Fraud\ube44\uc728\uc740 \uac00\uc7a5 \ub192\ub2e4.\n* `card4` : **Visa**\uac00 60%\uc774\uc0c1\uc744 \ucc28\uc9c0\ud558\uba70, \uadf8 \ub4a4\ub85c\ub294 mastercard\uac00 \ub4a4\ub97c \uc787\ub294\ub2e4. \uadf8 \uc678 discover, american express, Miss\uac00 \ube44\uc2b7\ud55c \ube44\uc728\ub85c \ub098\ud0c0\ub0ac\ub2e4. Fraud\uc758 \ube44\uc728\uc740 discover\uac00 \uac00\uc7a5 \ub192\uc558\ub2e4. \n* `card6` : **debit**\uc774 70%\uc774\uc0c1\uc73c\ub85c \uac00\uc7a5 \ub9ce\uc740 \ubd80\ubd84\uc744 \ucc28\uc9c0\ud558\uace0, \uadf8 \ub2e4\uc74c\uc73c\ub85c credit\uc73c\ub85c 25%, debit or credit, change card, Miss \ubd80\ubd84\uc740 3%\uc774\ud558 \ubd80\ubd84\uc744 \ucc28\uc9c0\ud55c\ub2e4. debit\uc774 \ud6e8\uc52c \ub9ce\uc740 \ube44\uc728\uc744 \ucc28\uc9c0\ud558\uc9c0\ub9cc, credit\uc774 Fraud \ube44\uc728\uc774 \uac00\uc7a5 \ub192\ub2e4. \n* `M1` : T, F, miss\ub85c True\ub791 Miss \ubd80\ubd84\uc774 \uc5c7\ube44\uc2b7\ud558\uace0, Fraud\ube44\uc728\uc740 Miss\uac00 \ub192\uc74c\n* `M2` : T, F, miss, True\ub791 Miss \ubd80\ubd84\uc774 \uc5c7\ube44\uc2b7, F\ube44\uc728\uc774 6%\ub85c \uac00\uc7a5 \ub0ae\ub2e4. \ud558\uc9c0\ub9cc Miss\uac00 Fraud \ube44\uc728\uc774 \uac00\uc7a5 \ub192\uace0, False Fraud \ube44\uc728\uc774 \uadf8 \ub2e4\uc74c\uc73c\ub85c \ub192\ub2e4.\n* `M3` : T, F, miss, True\ub791 Miss \uc5c7\ube44\uc2b7\ud558\uace0, False \ube44\uc728\uc740 11%\uc774\ub2e4. Fraud \ube44\uc728\uc740 Miss, Fals\uc21c\uc774\ub2e4. \n* `M4` : M2, M0, Miss, M1,Fraud \ube44\uc728\uc774 M2\uac00 \uac00\uc7a5 \ub192\uc74c\n* `M5`: Miss\uac00 50% \uc774\uc0c1\uc774\uace0, False, True\uac00 \ube44\uc2b7\ud55c \ube44\uc728\uc744 \ucc28\uc9c0\ud55c\ub2e4. Fraud \ube44\uc728\uc740 True\ub791 Miss\uac00 \ube44\uc2b7","27a9a2d1":"* \ud2b9\uc815\uc2dc\uac04\uc5d0 \uadf8 \uc2dc\uac04\ub300\uc758 \uce74\uc6b4\ud2b8\uc5d0 \ube44\ud574 \uc0ac\uae30\ud589\uac01\uc774 \ub192\uc544\uc9c0\ub294 \uac83\uc744 \ubcfc \uc218 \uc788\ub2e4\n* \uc694\uc77c\ubcc4\ub85c \uc0b4\ud3b4\ubcf4\uc558\uc744 \ub54c \uc57d\uac04\uc758 \ucc28\uc774\ub294 \uc788\uc9c0\ub9cc \ucc28\uc774\ub294 \ub9ce\uc774 \ub098\uc9c0 \uc54a\uc74c\uc744 \ubcfc \uc218 \uc788\ub2e4. ","36b68890":"* p_emaildomain\uc740 \uac70\uc758 \ub370\uc774\ud130\uac00 \uc788\ub2e4. \n* p_emaildomain\uc774 \uac12\uc774 \uc788\uc744 \ub54c, r_emaildomain\uc774 Null\uac12\uc778 \uacbd\uc6b0\uac00 \uac00\uc7a5 \ub9ce\uc9c0\ub9cc, Fraud \ube44\uc728\uc774 \uac00\uc7a5 \ub192\uc740\uac74 p_emaildomin\uc774 \uae30\uc785\ub418\uc5b4 \uc788\uace0, r_emaildomain\ub3c4 \uae30\uc785\ub418\uc5b4 \uc788\ub294 \uacbd\uc6b0\uc774\ub2e4.\n\n> `\uc77c\ubc18\uc801\uc778 Email \uc0ac\uc6a9`\uc5d0\uc11c \uc0ac\uae30\ud589\uac01\uc774 \ub9ce\uc774 \uc77c\uc5b4\ub098\ub294 \uac83\uacfc, p_email\uc774 \uc788\ub294 \uacbd\uc6b0 \uc0ac\uae30\ud589\uac01 \ube44\uc728\uc774 \ub192\uc740 \uac83\uace0, `\ud2b9\ud788 r_emaildomain\ub3c4 \uc788\ub294 \uacbd\uc6b0\uc5d0 \uc0ac\uae30\ud589\uac01 \ube44\uc728\uc774 \ub192\uc740 \uac83`\uc73c\ub85c \ubd10\uc11c, emaildomain\uc740 \uc704\uc870\ub418\uae30 \uc26c\uc6b4 \uac83\uc73c\ub85c \ubcf4\uc778\ub2e4.","9966ee15":"### Following refer\n- (1) https:\/\/www.kaggle.com\/jesucristo\/fraud-complete-eda\n- (2) https:\/\/www.kaggle.com\/nroman\/eda-for-cis-fraud-detection\n- (3) https:\/\/www.kaggle.com\/kabure\/extensive-eda-and-modeling-xgb-hyperopt\n- (4) https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100167#latest-577688","f073888b":"* \uc0ac\uae30\ud589\uac01\uc744 \ubc8c\uc77c \ub54c, \uae30\uc785\ud558\ub294 \uc774\uba54\uc77c\uc740 \uc6b0\ub9ac\uac00 \uc77c\ubc18\uc801\uc73c\ub85c \uc0ac\uc6a9\ud558\ub294 Email\uc744 \uae30\uc785\ud55c\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4. ","840341ba":"#### Exploratory Transaction Table\n\n- Data Type \uccb4\ud06c\n> int64 Data Types: 4....float64 Data Types: 376....object Data Types: 14\n\n\n\n**\uc704\uc640 \uac19\uc740 \uacb0\uacfc\uc5d0 \ub530\ub77c, \uc0c1\ub300\uc801\uc73c\ub85c \uc801\uc740 \uac2f\uc218\ub97c \ucc28\uc9c0\ud558\ub294 Object\ubd80\ud130 \uc0b4\ud3b4\ubcfc \uac83**","a439b398":"# 2. EDA\n## Load Data","ec1df75f":"### \uceec\ub7fc\uacfc PK \ud655\uc778","38b79555":"### Imbalanced Data","8867ad16":"## Numeric Data\ub97c \uc0b4\ud3b4\ubcf4\uace0, NaN\uac12\uc758 \ube44\uc728\ub3c4 \uc0b4\ud3b4\ubcf8\ub2e4\n* int Value\ub294 `card1`\ubfd0, \ub098\uba38\uc9c0\ub294 Float \ud615\uc2dd\uc744 \ub754\n* `card1`\uacfc `C*`\ub294 \uac12\uc774 \ubaa8\ub450 \uc788\ub2e4. \n* `card*`\ub3c4 \uac70\uc758 \uac12\uc774 \uc788\uace0, `addr*`\ub3c4 \ub9ce\uc774 \uac12\uc774 \uc788\uc74c\n* `dist*`\ub294 \uac12\uc774 50% \uc5c6\uace0, `V*`\ub294 \uac12\uc774 \uac70\uc758 \uc788\ub294 \uac83\ubd80\ud130 10%\ub9cc \uac12\uc774 \uc788\ub294 \uacbd\uc6b0\ub85c \ub2e4\uc591\ud558\ub2e4","71184be4":"### transaction\n\n\ud2b8\ub79c\uc7ad\uc158\uc740 \ub108\ubb34 \ub9ce\uc73c\ubbc0\ub85c identy\uc640 \ube44\uc2b7\ud55c \ud504\ub85c\uc138\uc2a4\ub85c \uc784\uc758\ub85c \uc9c4\ud589","f90eaeba":"## \ud53c\uccd0 \uc5d4\uc9c0\ub2c8\uc5b4\ub9c1\uc740 Skip","eaea69fd":"### NaN\uac12 \ud655\uc778","fa8a82d7":"# 2-1. Object Feature"}}