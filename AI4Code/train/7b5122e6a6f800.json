{"cell_type":{"362d6935":"code","42623003":"code","4d773746":"code","8dd7021d":"code","5503cb75":"code","b0e6c8af":"code","ae3209ea":"code","8a004b36":"code","4cf20632":"code","69e70d24":"code","81f90b99":"code","8b64f0d2":"code","3c4b4d56":"code","4949f754":"code","40277c85":"code","fe7186ba":"code","3fb21042":"code","0e6ce6d8":"code","f76772b4":"code","4995fb7a":"code","d85b3758":"code","75ad69bb":"code","bae825d3":"code","cb4b113f":"code","cbea590f":"code","47cb453b":"code","69d1fb7e":"code","31bac6c9":"code","c87d7926":"code","19da7b89":"code","19d2a4b7":"code","9b48c649":"code","0c792462":"code","aeff36dc":"code","4b356943":"code","58d3b3b9":"code","c108555b":"code","5b5b6bf8":"code","d3331d74":"code","ab548648":"code","7a78c42a":"code","e894ba9b":"code","0282a6d9":"code","e54dcf75":"code","3e3221ca":"code","3dd83213":"code","d321f226":"code","7247217f":"code","875bdcbf":"code","0e08a7eb":"code","8a0649af":"code","ca9e97d4":"code","8f2e8755":"code","f34ba842":"code","42064f26":"code","c12bd471":"code","4d8ba79c":"code","16f10dd2":"code","869128e7":"code","1df70b6b":"code","baf48929":"code","795d7c67":"code","c2c6c49d":"code","5404688e":"code","8d2671a9":"code","3be1426e":"code","82e12fb4":"code","83a360aa":"code","f4726a7c":"code","b8278b81":"code","7b60261f":"code","09de9e18":"code","c853056b":"markdown","5c4ff05f":"markdown","268f143b":"markdown","ad6e4bc2":"markdown","986ee352":"markdown","a99a98f5":"markdown","28a0dc7e":"markdown","229a892b":"markdown","06ea481c":"markdown","b6522dd5":"markdown","c9f69d1f":"markdown","699f0d2a":"markdown","6b913aa2":"markdown","66cd4dd3":"markdown","941f1b5d":"markdown","cee9aa1f":"markdown","ffd4ece7":"markdown","fd98a197":"markdown","45177331":"markdown","4028cce7":"markdown","5e66e5dd":"markdown","7caed586":"markdown","8e622d5a":"markdown","bd07b0fc":"markdown","97e3c1f3":"markdown"},"source":{"362d6935":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","42623003":"import matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set_style('whitegrid')\n\nprint(pd.__version__)\nprint(matplotlib.__version__)\nprint(sns.__version__)","4d773746":"trandata = pd.read_csv('\/kaggle\/input\/quantium-data-analytics-virtual-experience-program\/Transactions.csv')\ntrandata.shape","8dd7021d":"trandata.columns","5503cb75":"trandata.info()","b0e6c8af":"from datetime import datetime, timedelta\n\ndef from_excel_ordinal(ordinal, _epoch0=datetime(1899, 12, 31)):\n    if ordinal >= 60:\n        ordinal -= 1  # Excel leap year bug, 1900 is not a leap year!\n    return (_epoch0 + timedelta(days=ordinal)).replace(microsecond=0)\n\ntrandata.DATE = trandata.DATE.apply(from_excel_ordinal)","ae3209ea":"trandata.head()","8a004b36":"trandata.describe()","4cf20632":"print(trandata.STORE_NBR.nunique())\nprint(trandata.TXN_ID.nunique())\nprint(trandata.PROD_NBR.nunique())\nprint(trandata.TOT_SALES.nunique())\nprint(trandata.LYLTY_CARD_NBR.nunique())","69e70d24":"trandata.TXN_ID.value_counts()","81f90b99":"trandata[trandata.TXN_ID.isin(['102237'])]","8b64f0d2":"trandata.columns","3c4b4d56":"print(trandata.DATE.min(), trandata.DATE.max())","4949f754":"# sns.distplot(trandata.DATE, kde=True)\ntrandata.DATE.hist()\n# looks pretty much balanced\n#  we have the transaction data of a year from july 18 to june 19","40277c85":"# missed a missing DATE\n\ntrandata.DATE.describe(datetime_is_numeric=False)","fe7186ba":"# 1 date is missing as there are 364 unique dates\n# lets find out with the help of a line chart\n\ngraph = trandata[['DATE','TXN_ID']].groupby('DATE').count().sort_values(by='DATE')\n# plt.figure(figsize=(20,6))\n# ax = sns.lineplot(data=graph) # seaborn does not breaks line in case of missing dates (not NAN\/null case)\n#                               # advised to use matplotlib here  \n\nax = graph.plot(figsize=(20,6))\n# ax.xaxis.set_major_locator(matplotlib.dates.MonthLocator(interval=1))\nplt.show()","3fb21042":"# the above graph does not shows any line breaks, which it should.\n# i looked up on SO and decided to manually lookup for missing date instead\n\ndates = trandata[['DATE']]\ndates.drop_duplicates('DATE', inplace=True)\ndates['month'] = dates.DATE.dt.month_name()\ngrp = dates.groupby('month').count()\ngrp","0e6ce6d8":"# looks like we don't have all the dates in the month of December\n# lets deepdive in December month\n\ndates[dates.month=='December'].sort_values(['DATE'])","f76772b4":"# missing 25 dec. its a holiday \n# so no sales on that day","4995fb7a":"trandata.STORE_NBR.hist()\nplt.show()","d85b3758":"trandata.PROD_NBR.hist()","75ad69bb":"# missed the bulk chip buyer last time\n# tackling this time\n\ntrandata.PROD_QTY.describe()","bae825d3":"# we see that there is atleast 1 transaction where 200 packets are bought, which\n# is not normal. Lets examine these transactions\n\ntrandata[trandata.PROD_QTY==10]","cb4b113f":"# there are infact, 2 transactions of big sales, that we should not consider in our analysis.\n# lets check if the customer's other transactions\n\ntrandata[trandata.LYLTY_CARD_NBR == 226000]","cbea590f":"# so there are no any more transactions with the same buyer.\n# lets remove these records\n\nprint(trandata.shape)\ntrandata = trandata[trandata.LYLTY_CARD_NBR != 226000]\nprint(trandata.shape)","47cb453b":"# removing non-chip data\n# text analysis of prod names column\n\nfrom nltk.corpus import stopwords\nimport re\n\nstopwords = set(stopwords.words('english'))\n\nprod_words=[]\n\nfor doc in trandata.PROD_NAME:\n    docx = re.sub(r'\\d+g', '', doc)\n    clean_doc = re.sub(r'[^A-Za-z_ ]+', '', docx) \n    clean_words = [w.strip() for w in clean_doc.split()]\n    for w in clean_words:\n        if w not in stopwords:\n            prod_words.append(w)\n    \nprod_words = pd.Series(prod_words)\nprod_words.value_counts()\n","69d1fb7e":"prod_words.value_counts()[:20]","31bac6c9":"print(trandata.shape)\nindexes = [not bool(re.search(r'Salsa', name, re.IGNORECASE)) for name in trandata.PROD_NAME]\ntrandata = trandata[indexes]\n\nprint(trandata.shape)","c87d7926":"pbdata = pd.read_csv('..\/input\/quantium-data-analytics-virtual-experience-program\/PurchaseBehaviour.csv')\npbdata.shape","19da7b89":"pbdata.head()","19d2a4b7":"pbdata.info()","9b48c649":"# checking for outliers in loyalty card numbers\npbdata.LYLTY_CARD_NBR.nunique()","0c792462":"set(pbdata.LYLTY_CARD_NBR.unique()) == set(trandata.LYLTY_CARD_NBR.unique())","aeff36dc":"# checking distribution of LIFESTAGE\npbdata.LIFESTAGE.value_counts().plot(kind='bar')\nplt.show()","4b356943":"# checking distribution of PREMIUM CUSTOMER\npbdata.PREMIUM_CUSTOMER.value_counts().plot(kind='bar')\nplt.show()","58d3b3b9":"trandata['PROD_WTT'] = trandata.PROD_NAME.str[-4:-1]","c108555b":"trandata.PROD_WTT.value_counts()","5b5b6bf8":"# one particular product name 'Kettle 135g Swt Pot Sea Salt' does not ends with\n# product weight in name, hence we will update this information manually \n# in the prod_WTT column\n\ntrandata[trandata.PROD_NAME.str.endswith('Salt')].PROD_NAME.value_counts()","d3331d74":"indexes = trandata[trandata.PROD_WTT == 'Sal'].index\ntrandata.loc[indexes, 'PROD_WTT'] = 135\n\ntrandata.PROD_WTT.value_counts()","ab548648":"# trandata.PROD_WTT.dtype # this results in dtype('O')\ntrandata.PROD_WTT=trandata.PROD_WTT.astype('int')","7a78c42a":"# adding the column for unit price of product\n\ntrandata['PROD_UPRICE'] = trandata['TOT_SALES'] \/ trandata['PROD_QTY']","e894ba9b":"# looking out to extract brand names\nproducts = trandata['PROD_NAME'].unique()\nproducts.sort()\nproducts[:10]","0282a6d9":"trandata.PROD_NAME.describe(include='all')","e54dcf75":"trandata['PROD_BRAND']= trandata['PROD_NAME'].apply(lambda x:x.split()[0])","3e3221ca":"# Essentialy, words like Dorito\/Doritos, Smith\/Smiths, Grain\/GrnWves etc. are same\n# we need to correct these\n# Some of the names like Burger need complete name for better understanding\n\ntrandata.PROD_BRAND.value_counts().sort_index()","3dd83213":"replacements = {'Dorito':'Doritos', 'Grain':'GrnWves', 'Infzns':'Infuzions', 'Smiths':'Smith', 'Snbts':'Sunbites',\\\n                'Burger':'Burger Rings', 'French': 'French Fries', 'Natural': 'Natural Chip Co', 'Old':'Old El Paso', \\\n                'Red': 'Red Rock Deli', 'RRD': 'Red Rock Deli', 'NCC': 'Natural Chip Co', 'WW': 'Woolworths'}\ntrandata.PROD_BRAND.replace(to_replace=replacements, inplace=True)\ntrandata.PROD_BRAND.value_counts().sort_index()","d321f226":"masterdf=trandata.merge(pbdata, how='left', on='LYLTY_CARD_NBR')\nmasterdf.shape","7247217f":"# lets find out who are our primary customers\n\nfig, ax = plt.subplots(1,2, figsize=(16\/2,9\/2))\n\nmasterdf.LIFESTAGE.value_counts().plot(kind='bar', ax=ax[0])\n\nmasterdf[['LIFESTAGE','TOT_SALES']].groupby('LIFESTAGE').sum().sort_values(by='TOT_SALES',ascending=False).plot(kind='bar',ax=ax[1])\n","875bdcbf":"# after taking reference from model answer\n# from the above figure, we cannot answer how premium our target customers are.\n# lets find out\ntotal = masterdf.TOT_SALES.sum()\npvt = pd.pivot_table(masterdf, index=['LIFESTAGE'],columns=['PREMIUM_CUSTOMER'], values=['TOT_SALES'],aggfunc='sum')\npvt=pvt.applymap(lambda x: round(x*100\/total,2))\npvt.plot(kind='bar', stacked=False, )","0e08a7eb":"# lets run the numbers of graph above in heatmap for a better comparison\nsns.heatmap(pvt, annot=True)","8a0649af":"# we are getting most sales from budget-old families, mainstream-retirees and maintream young\/single couples\n\n# lets check if this is due to more number of shoppers\ntotal_customers = masterdf.LYLTY_CARD_NBR.nunique()\nnpo = pd.pivot_table(masterdf, index=['LIFESTAGE'],columns=['PREMIUM_CUSTOMER'], values=['LYLTY_CARD_NBR'],aggfunc=pd.Series.nunique)\nnpo=npo.applymap(lambda x: round(x*100\/total_customers,1))\nsns.heatmap(npo, annot=True)\n","ca9e97d4":"# it is observed that while old-budget families contribute highest to sales, but the customer group is not large. Means they tend to buy in larger quantities.\n# converse, there are a roughly double mainstream-young\/single couples customers, but they don't buy loads of chips.\n\n# lets try to validate this by calculating the average number of chips units bought in a transaction per customer ","8f2e8755":"masterdf.columns","f34ba842":"# average number of chips units bought in a transaction\n\n# aucpc = masterdf[['TXN_ID','PROD_QTY']].groupby(['TXN_ID']).sum()\nlyl = pd.pivot_table(masterdf, index='LIFESTAGE', columns='PREMIUM_CUSTOMER', values=['LYLTY_CARD_NBR'], aggfunc=[pd.Series.nunique])\naucpc = pd.pivot_table(masterdf, index='LIFESTAGE', columns='PREMIUM_CUSTOMER', values=['PROD_QTY'], aggfunc='sum')\n# aucpc=aucpc.applymap(lambda x:round(x,3))\naucpc","42064f26":"temp=masterdf[(masterdf.LIFESTAGE=='NEW FAMILIES') & (masterdf.PREMIUM_CUSTOMER=='Budget')][['LYLTY_CARD_NBR','PROD_QTY']]\ntemp.PROD_QTY.sum()\/temp.LYLTY_CARD_NBR.nunique()","c12bd471":"testdf = pd.DataFrame(cols=['AVG_PROD_QTY'])\ngroupdf=masterdf[['LIFESTAGE','PREMIUM_CUSTOMER','PROD_QTY','LYLTY_CARD_NBR']].groupby(['LIFESTAGE','PREMIUM_CUSTOMER'])\nfor name, df in groupdf:\n    ","4d8ba79c":"testdf = pd.DataFrame(columns=['AVG_PROD_QTY'])","16f10dd2":"testdf.append(['1'], index=name)","869128e7":"name, ser = next(iter(x[['LYLTY_CARD_NBR','PREMIUM_CUSTOMER']]))","1df70b6b":"name","baf48929":"ser","795d7c67":"ser.PROD_QTY.sum()\/ser.LYLTY_CARD_NBR.nunique()","c2c6c49d":"# lets find out how do the customer segments like to spend their money\n\npvt = pd.pivot_table(pbdata, index=['LIFESTAGE'],columns=['PREMIUM_CUSTOMER'], values=['LYLTY_CARD_NBR'],aggfunc='count')\npvt.columns=['Budget', 'Mainstream', 'Premium']\npvt.sort_values(['Premium'], inplace=True)\npvt.plot(kind='bar')\nplt.show()","5404688e":"# overall monthly chip consumption in grams per lifestage\nmasterdf['PUR_MONTH']= masterdf.DATE.dt.month_name()\npvt = pd.pivot_table(masterdf, index='LIFESTAGE', values='PROD_WTT', columns='PUR_MONTH',aggfunc='sum')\ncols= ['July', 'August', 'September','October','November','December', 'January',  'February',\n       'March', 'April', 'May', 'June'  ]\npvt.columns = cols\npvt = pvt.transpose()\npvt.plot(kind='line', figsize=(12,8))\n","8d2671a9":"# average transactions size\n","3be1426e":"# average brand price for 150 gm pack","82e12fb4":"# adding graphs from reference\n\n# total sales by ","83a360aa":"# top 10 selling products\nmasterdf.PROD_NAME.value_counts()[:10]","f4726a7c":"# top 10 selling brands\nmasterdf.PROD_BRAND.value_counts()[:10]","b8278b81":"# top performing stores\nmasterdf[['STORE_NBR','TOT_SALES']].groupby('STORE_NBR').sum().sort_values(by='TOT_SALES', ascending=False)[:10]","7b60261f":"# distribution of product weights\nmasterdf.PROD_WTT.hist(bins=10)","09de9e18":"# Distribution of unit chip packet \nmasterdf.PROD_UPRICE.hist()","c853056b":"2. Adding column `PROD_UPRICE` - product unit price","5c4ff05f":"# Objective\nTo analyse the data to understand the current purchasing trends and behaviours, in particular to customer segments and chip purchasing behaviour.\n\nTasks:\n1. Create and Interpret High level summaries\n2. Outlier detection and removal\n3. Checking data format and correction\n4. Feature engg\n    1. extra features such as packs size and brand name\n5. Metrics: consider what metrics will help describe customers' purchasing behaviour\n    1. who spends on chips\n    2. what drives spends for each customer segment\n    \n    You will also want to derive extra features such as pack size and brand name from the data and define metrics of interest to enable you to draw insights on who spends on chips and what drives spends for each customer segment. Remember our end goal is to form a strategy based on the findings to provide a clear recommendation to Julia the Category Manager so make sure your insights can have a commercial application.","268f143b":"> Since the number of unique loyalty cards is same in both the purchase behavior data as well as transaction data, I think we have the purchase behaviour details of chips-buying customers only.  \n> To validate this assumption, let's check if both the data have same set of loyalty cards or not","ad6e4bc2":"removing non-chip data <a id='nonchip'><\/a>  \ntext analysis of prod names column\n","986ee352":"___\n___","a99a98f5":"---\n## Examining Transaction Data - Observation Summary <a id='tranxn'><\/a>\n1. [Dates in DATE column were transformed. Found a missing date  \u2714](#standard)\n2. It is seen that people can buy mutiple products together \ud83e\udd14\n3. No any missing values were observed.\n4. [Removed the chip bulk buyer outliers.](#tranoutliers)\n5. [Removed non-chip transactions from transactions data.](#nonchip)\n","28a0dc7e":"> Its is observed from the graph that, \n1. Product packs falling in price range of 2.5 to 4.5 approx. are most bought.","229a892b":"Now checking for outliers in transaction data <a id='tranoutliers'><\/a>","06ea481c":"# another possible, elegant solution\npd.to_datetime(trandata.DATE, unit='D')","b6522dd5":"It is seen that people have bought multiple products together, which is expected and normal. See the below records, for example.","c9f69d1f":"Metrics: consider what metrics will help describe customers' purchasing behaviour\n\nwho spends on chips\n\nwhat drives spends for each customer segment\n\nYou will also want to derive extra features such as pack size and brand name from the data and define metrics of interest to enable you to draw insights on who spends on chips and what drives spends for each customer segment. Remember our end goal is to form a strategy based on the findings to provide a clear recommendation to Julia the Category Manager so make sure your insights can have a commercial application.","699f0d2a":"3. Adding column `PROD_BRAND` - product brand name","6b913aa2":"### Standardising dates values to YYYY-MM-DD format <a id='standard'><\/a>","66cd4dd3":"> Its is observed from the graphs that, \n1. Old and retired people, older families, young and older families buy a lot chips. This observation in coherent with their spendings on chips.\n2. Young and Old families tend to shop more chips then new families. Theory supports this as new families have relatively younger children who still not eat a lot of chips as well as they are more stringent on expenses.","941f1b5d":"# Table of Contents\n1. [Examining Transaction Data](#tranxn)\n2. [Examining Purchase Behaviour Data](#pb)\n3. [Adding new Features](#features)\n4. [Data Analysis](#ana)","cee9aa1f":"> Its is observed from the graph that, \n1. Young singles\/couples and Retirees tend to buy the mainstream or popular products.\n2. All of the families prefer budget buying over other options.\n3. Old singles\/ couples are equally likely to spend in any category.","ffd4ece7":"## Analysis <a id='ana'><\/a>\n\n> First, we will combine both of our data source, and then answer the below questions:\n1. Discover who are our primary shoppers\n2. Find out how customer segments like to spend their money\n3. High level summary - Top selling brands, products, weight categories\n4. ","fd98a197":"Data analysis and customer segments \u2013 in your analysis make sure you define the metrics \u2013 look at total sales, drivers of sales, where the highest sales are coming from etc. Explore the data, create charts and graphs as well as noting any interesting trends and\/or insights you find. These will all form part of our report to Julia.\n\n","45177331":"### High level Summaries","4028cce7":"---\n## Features: <a id='features'><\/a>\n1. The `PROD_NAME` contains brand name, product variant and weight as well. We can separate this information for better analysis.\n2. We can derive `PROD_UPRICE` - product's unit price from `PROD_QTY` and `TOT_SALES`\n3. Similarly, we can get product brand name under `PROD_BRAND`","5e66e5dd":"> Its is observed from the graph that, \n1. The weight category of 150 to 200 gm pack is the most sold.\n2. Other large or smaller packings are not so popular.","7caed586":"> Both the datasets share the same number of loyalty cards with same numbers. Hence, we conclude, the transaction data and the purchase behaviour data, both belongs to our target, chip-buying customers. Yay! \ud83d\ude4c","8e622d5a":"___\n* define metrics of interest to enable you to draw insights on who spends on chips and what drives spends for each customer segment. ","bd07b0fc":"---\n## Examining Purchase Behaviour Data - Observation Summary <a id='pb'><\/a>\n1. This data contains information with unique loyalty card numbers.\n2. There are 7 clusters of buyers based on Lifestage.\n3. Based on the buying choices and prices, buyers are identified in 3 classes as in `PREMIUM_CUSTOMER`\n4. No any missing\/ null values or outliers were found\n5. *All the purchase behaviour data we have is based on chip purchases*","97e3c1f3":"1. Adding column `PROD_WTT` - product weight"}}