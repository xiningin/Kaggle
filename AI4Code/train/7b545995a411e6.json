{"cell_type":{"f696d264":"code","aaaf99ce":"code","e42e5ec2":"code","35e5a035":"code","348269e0":"code","3bf82c0b":"code","d46c9034":"code","da1d7c00":"code","e79bbbcc":"code","ef53add7":"code","227485de":"code","0ea36e16":"code","84ce4249":"code","7284f3c3":"code","e2abcc14":"code","9c09472c":"code","ff428c8f":"code","7b51befe":"code","381d1af4":"code","6608b95f":"code","72520982":"code","07577514":"code","dc6093fb":"code","3e191f98":"code","534187ac":"code","5db0683b":"code","affb8835":"code","c4332445":"code","5fdafc82":"code","afb5e9f5":"code","c8c00c7f":"code","bd7605f7":"code","a8bae4a5":"code","a164038b":"code","2d20cbdc":"code","5dab7576":"code","57890896":"code","ac37a154":"code","beeb106e":"code","2ac5f721":"code","63c771a5":"code","79c8c36a":"code","2985eb08":"code","67000760":"code","a7643410":"code","eb51e048":"code","a38cf97f":"code","863f11e9":"code","601e8a74":"code","a779170d":"code","e54943ea":"code","245eb473":"code","0056891f":"code","6d950a33":"code","78917528":"code","d02c5047":"code","b5e3be68":"code","5d3ac449":"code","dc7d25b0":"code","21276f47":"markdown","3dc3d467":"markdown","550c1acb":"markdown","e9ee3e1f":"markdown","621cfa21":"markdown","6a0729e9":"markdown","ee16d6d0":"markdown","227728c2":"markdown","2a56badb":"markdown","df60ad36":"markdown"},"source":{"f696d264":"# Check input data\n!ls '..\/input\/review-lapak-sentiment\/'","aaaf99ce":"import pandas as pd\nimport numpy as np","e42e5ec2":"raw_data = pd.read_csv('..\/input\/review-lapak-sentiment\/train.csv')\nraw_data.head(10)","35e5a035":"# Count Labels\nraw_data['label'].value_counts()","348269e0":"# First make a function to delete repetitive alphabet\nimport itertools\n\ndef remove_repeating_characters(text):\n    return ''.join(''.join(s)[:1] for _, s in itertools.groupby(text))\n\n# Check our function\nremove_repeating_characters('oooofel')","3bf82c0b":"# Second make a function to remove non alphanumeric\nimport re\n\ndef remove_nonalphanumeric(text):\n    text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n    return text\n\n# Check our function\nremove_nonalphanumeric('o,,,f!!e;;l')","d46c9034":"# Last make a function to convert string to lower case\n\ndef to_lower_case(text):\n    return text.lower()\n\n# Check our function\nto_lower_case('OFEL')","da1d7c00":"# Make function that combine them all\n\ndef preprocessing_text(text):\n    text = remove_repeating_characters(text)\n    text = remove_nonalphanumeric(text)\n    text = to_lower_case(text)\n    \n    return text\n\n# Check our function\npreprocessing_text('Bagus\\n\\n\\nNamun Akan Lebih Baik Apabila Lebih')","e79bbbcc":"# Apply function to column 'review_sangat_singkat'\n\nraw_data['review_sangat_singkat'] = raw_data['review_sangat_singkat'].apply(lambda x: preprocessing_text(x))\nraw_data.head()","ef53add7":"# Make a vector to contain all unique word in 'review sangat singkat'\n\nunique_string = set()\nfor x in raw_data['review_sangat_singkat']:\n    for y in x.split():\n        unique_string.add(y)\n        \nlen(unique_string)","227485de":"# Count statistics of number of word in review\n\nlen_data = [len(x.split()) for x in raw_data['review_sangat_singkat']]\nprint(np.mean(len_data))\nprint(np.median(len_data))\nprint(np.std(len_data))\nprint(np.min(len_data))\nprint(np.max(len_data))\nprint(np.percentile(len_data, 98))","0ea36e16":"embed_size = 100 # how big is each word vector\nmax_features = 23000 # how many unique words to use\nmaxlen = 20 # max number of words in a comment to use","84ce4249":"# Example\nfrom keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(num_words = 4)\ntokenizer.fit_on_texts([\"ini sebuah kalimat hehehe\"])\nexamples = tokenizer.texts_to_sequences([\"ini contoh kalimat juga\"])\nprint(examples[0])\n","7284f3c3":"# Real one\n\ntokenizer = Tokenizer(num_words = max_features)\ntokenizer.fit_on_texts(raw_data['review_sangat_singkat'])\nlist_tokenized_train = tokenizer.texts_to_sequences(raw_data['review_sangat_singkat'].values)\n","e2abcc14":"list_tokenized_train[0]","9c09472c":"# Example\n\nfrom keras.preprocessing.sequence import pad_sequences\npad_sequences(examples, maxlen = maxlen)","ff428c8f":"# Real one\n\nX_t = pad_sequences(list_tokenized_train, maxlen= maxlen )","7b51befe":"X_t[0]","381d1af4":"import gensim\nDIR_DATA_MISC = \"..\/input\/word2vec-100-indonesian\"\npath = '{}\/idwiki_word2vec_100.model'.format(DIR_DATA_MISC)\nid_w2v = gensim.models.word2vec.Word2Vec.load(path)\nprint(id_w2v.most_similar('itb'))","6608b95f":"index2word_set = set(id_w2v.wv.index2word)","72520982":"word_index = tokenizer.word_index\nnb_words = max_features\nembedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)\nunknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1.\nfor word, i in word_index.items():\n    cur = word\n    if cur in index2word_set:\n        embedding_matrix[i] = id_w2v[cur]\n        continue\n        \n    embedding_matrix[i] = unknown_vector","07577514":"# Import needed packages\n# And make needed function\n\n\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, SpatialDropout1D, GlobalMaxPooling1D, Concatenate\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras import callbacks\n\nfrom keras import backend as K\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n    \n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))\n\ndef get_model():\n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    x = SpatialDropout1D(0.3)(x)\n    x1 = Bidirectional(LSTM(32, return_sequences=True))(x)\n    x2 = Bidirectional(GRU(32, return_sequences=True))(x1)\n    max_pool1 = GlobalMaxPooling1D()(x1)\n    max_pool2 = GlobalMaxPooling1D()(x2)\n    conc = Concatenate()([max_pool1, max_pool2])\n    x = Dense(1, activation=\"sigmoid\")(conc)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1])\n    return model","dc6093fb":"from sklearn.model_selection import KFold\ndef get_kfold():\n    return KFold(n_splits=5, shuffle=True, random_state=1)","3e191f98":"X = X_t\ny = raw_data[\"label\"].values\n\npred_cv = np.zeros(len(y))\ncount = 0\n\nfor train_index, test_index in get_kfold().split(X, y):\n    count += 1\n    print(count, end='')\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    es = callbacks.EarlyStopping(monitor='val_f1', min_delta=0.0001, patience=8,\n                                             verbose=1, mode='max', baseline=None, restore_best_weights=True)\n\n    rlr = callbacks.ReduceLROnPlateau(monitor='val_f1', factor=0.5,\n                                      patience=3, min_lr=1e-6, mode='max', verbose=1)\n    \n    \n    model = get_model()\n    model.fit(X_train, \n             y_train, batch_size=16, epochs=4,\n             validation_data=(X_test, y_test),\n             callbacks=[es, rlr],\n             verbose=1)\n    \n    pred_cv[[test_index]] += model.predict(X_test)[:,0]","534187ac":"model.save('UseeTV Go Tweets Texts Sentiment Analysis Model.h5')","5db0683b":"useetv_twitter_data = pd.read_csv( \"..\/input\/useetv-go-twitter-tweets-texts-stemmed-dataset\/Text_Preprocessing.csv\")\n\nuseetv_twitter_data.head()","affb8835":"print(\"Dataset Tweets Tweets dari Tanggal {} sampai Tanggal {}\".format( useetv_twitter_data[\"date\"].min() , useetv_twitter_data[\"date\"].max() ))\nprint(\"Dataset Tweets Tweets User dari {} User\".format( len( useetv_twitter_data[\"user_id\"].unique() ) ))","c4332445":"#Lihat Distribusi Bahasa Twieets Dataset Tweets\n\nimport seaborn as sns\n\n#sns.set_theme()\n\nuseetv_twitter_data[\"language\"].value_counts().plot( kind = \"bar\" , figsize = ( 20 , 13 ))","5fdafc82":"useetv_twitter_data[\"date\"] = pd.to_datetime(useetv_twitter_data[\"date\"], infer_datetime_format=True)","afb5e9f5":"useetv_twitter_data.dtypes","c8c00c7f":"useetv_twitter_data[\"tweet\"]","bd7605f7":"useetv_twitter_data = useetv_twitter_data.set_index( \"created_at\" )","a8bae4a5":"useetv_twitter_data[\"date\"].resample(\"M\").count().plot(figsize=(12,5), grid=True, title=\"Total Number of Tweets by Date\").set(xlabel=\"Date\", ylabel=\"Total Number of Tweets\")","a164038b":"import nltk\nnltk.download('stopwords')\nnltk.download('punkt')","2d20cbdc":"import re, string, unicodedata  #modul regular expression\nimport nltk\nfrom nltk import word_tokenize, sent_tokenize  #Paket ini membagi teks input menjadi kata-kata.,                                  \nfrom nltk.corpus import stopwords","5dab7576":"#preprocessing\n\nNew_word_stop_list = ['kak' , 'admin' , 'useetvgo' , 'go' , 'tv' , 'u' , 'see' , \n'yg', 'ya', 'dg', 'dgn', 'nich', 'nih', 'sih', 'si', 'nya', \n              \t'lg', 'dulu', 'jgn', 'kl', 'klu', 'klo', 'kalo', 'nge',\n                   \t'sip', 'spt', 'hallo', 'halo', 'ny', 'd', 'biar', 'skrg',\n                   \t'bikin', 'bilang', 'tau', 'utk', 'jd', 'yah', 'loh', 'lho', 'aj', 'aja',\n                   \t'cm', 'banget', 'deh', 'dimana', 'for', 'i', 'coba', 'it', 'sy', 'lumayan',\n                   \t'kasih', 'tingkat', 'apk', 'usee', 'useetv', 'channel'\n                     \"min\" , \"winda\" , \"youtube\" , \"spotify\" , \"dana\" ]\n\n\ndef removeStopword(str):\n    stop_words = set(stopwords.words('indonesian'))\n    word_tokens = word_tokenize(str)\n    filtered_sentence = [w for w in word_tokens if not w in stop_words and w not in New_word_stop_list ]\n    return ' '.join(filtered_sentence)\n#remove sentence which contains only one word\ndef removeSentence(str): \n    word = str.split()\n    wordCount = len(word)\n    if(wordCount<=1):\n        str = ''\n    \n    return str\ndef cleaning(str):\n    #remove non-ascii\n    str = unicodedata.normalize('NFKD', str).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    #remove URLs\n    str = re.sub(r'(?i)\\b((?:https?:\/\/|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}\/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019]))', '', str)\n    #remove punctuations\n    str = re.sub(r'[^\\w]|_',' ',str)\n    #remove digit from string\n    str = re.sub(\"\\S*\\d\\S*\", \"\", str).strip()\n    #remove digit or numbers\n    str = re.sub(r\"\\b\\d+\\b\", \" \", str)\n    #to lowercase\n    str = str.lower()\n    #Remove additional white spaces\n    str = re.sub('[\\s]+', ' ', str)\n       \n    return str\ndef preprocessing( tweet):\n\n    tweet = str( tweet )\n    tweet = removeSentence( tweet )\n    tweet = cleaning( tweet )\n    tweet  = removeStopword( tweet )\n\n    #print( str )\n    \n    return tweet","57890896":"#test the code\nsentences = [\"dimana lokasi kuliner jogja yang murah\",\"alamat gudeg yu djum dimana sih, yang enak\",\"s\"]\nfor st in sentences:\n    r = preprocessing(st)\n    print(r)","ac37a154":"# Preprocessing Data\n\nuseetv_twitter_data[\"preprocessed_tweets_text\"] = useetv_twitter_data[\"tweet\"].apply( lambda row : preprocessing( row ) )\n\nuseetv_twitter_data.head()","beeb106e":"# Normalisasi Data\n\nnormalized_word = pd.read_csv(\"..\/input\/normalisasi-indonesian-natural-language-processing\/normalisasi.csv\")\n\nnormalized_word_dict = {}\n\nfor index, row in normalized_word.iterrows():\n    if row[0] not in normalized_word_dict:\n        normalized_word_dict[row[0]] = row[1] \n\ndef normalized_term(document):\n    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n\ndef normalized_tweets( tweets_text ):\n\n    tweets_words_list = tweets_text.split( \" \" )\n\n    normalized_tweets_text = \"\"\n\n    for word in tweets_words_list :\n\n        if word in normalized_word_dict :\n          normalized_tweets_text = normalized_tweets_text + \" \" + normalized_word_dict[ word ]\n        else :\n          normalized_tweets_text = normalized_tweets_text + \" \" + word\n    \n    return normalized_tweets_text\n\n\nuseetv_twitter_data[\"normalized_and_preprocessed_tweets_text\"] = useetv_twitter_data[\"preprocessed_tweets_text\"].apply( lambda row : normalized_tweets( row ) )\n\nuseetv_twitter_data.head()","2ac5f721":"export = useetv_twitter_data.to_csv( \"Normalized_And_Preprocess_Tweets_Text.csv\" )","63c771a5":"useetv_twitter_data_after_june_2020 = useetv_twitter_data[ useetv_twitter_data[\"date\"] >= \"2020-06-01\"]\n\nuseetv_twitter_data_after_june_2020.head()","79c8c36a":"useetv_twitter_data_after_june_2020 = useetv_twitter_data_after_june_2020.set_index( \"created_at\")","2985eb08":"useetv_twitter_data_after_june_2020[\"username\"].value_counts()","67000760":"Not_From_User_List = ['indihomemaster', 'indihomecs1', 'indihomecare', 'indihome',\n       'indihomeborneo', 'indihomekepri', 'indihomebdg70',\n       'info_indihome', 'indihomedigital', 'smgindihome',\n       'indihome_jakbar', 'bandarindihome', 'indihome_sbs',\n       'indihome_ind', 'indihome_diy', 'telkomsel', 'telkomindonesia', 'telkomjabar', 'telkomkti',\n       'telkomcare', 'telkompromo' , \"useetvcom\" , \"indihome\" , \"telkomsel\" ]\n\ndef get_from_user_or_not( tweets_username ):\n\n    if tweets_username in Not_From_User_List :\n        return \"False\"\n    else :\n        return \"True\"\n\nuseetv_twitter_data_after_june_2020[\"from_user_or_not\"] = useetv_twitter_data_after_june_2020[\"username\"].apply( lambda row : get_from_user_or_not( row ) )\n\nuseetv_twitter_data_after_june_2020_from_user = useetv_twitter_data_after_june_2020[ useetv_twitter_data_after_june_2020[\"from_user_or_not\"] == \"True\" ]\n\nuseetv_twitter_data_after_june_2020_from_user.head()","a7643410":"useetv_twitter_data_after_june_2020_not_from_user = useetv_twitter_data_after_june_2020[ useetv_twitter_data_after_june_2020[\"from_user_or_not\"] == \"False\"]","eb51e048":"useetv_twitter_data_after_june_2020_from_user.describe()","a38cf97f":"useetv_twitter_data_after_june_2020_not_from_user.describe()","863f11e9":"print(\"Dataset Tweets Tweets dari Tanggal {} sampai Tanggal {}\".format( useetv_twitter_data_after_june_2020_from_user[\"date\"].min() , useetv_twitter_data_after_june_2020_from_user[\"date\"].max() ))\nprint(\"Dataset Tweets Tweets User dari {} User\".format( len( useetv_twitter_data_after_june_2020_from_user[\"user_id\"].unique() ) ))","601e8a74":"useetv_twitter_data_after_june_2020[\"date\"].resample(\"M\").count().plot( figsize = ( 12, 5 ) , grid= True , title = \"Total Number of Tweets by Date\").set( xlabel = \"Date\" , ylabel = \"Total Number of Tweets\")\nuseetv_twitter_data_after_june_2020_not_from_user[\"date\"].resample(\"M\").count().plot( figsize = ( 12, 5 ) , grid = True , title = \"Total Number of Tweets by Date\").set( xlabel = \"Date\" , ylabel = \"Total Number of Tweets\" )\nuseetv_twitter_data_after_june_2020_from_user[\"date\"].resample(\"M\").count().plot(figsize=(12,5), grid=True, title=\"Total Number of Tweets by Date\").set(xlabel=\"Date\", ylabel=\"Total Number of Tweets\")","a779170d":"# Real one\n\n#tokenizer = Tokenizer(num_words = max_features)\n#tokenizer.fit_on_texts( useetv_twitter_data_after_june_2020_from_user[\"normalized_and_preprocessed_tweets_text\"] )\nlist_tokenized_train = tokenizer.texts_to_sequences( useetv_twitter_data_after_june_2020_from_user[\"normalized_and_preprocessed_tweets_text\"].values)","e54943ea":"list_tokenized_train[0]","245eb473":"# Real one\n\nX_t = pad_sequences(list_tokenized_train, maxlen= maxlen )","0056891f":"X_t[0]","6d950a33":"sentiment_probability_prediction_vec = model.predict( X_t )","78917528":"sentiment_probability_prediction_vec","d02c5047":"def convert_probability_to_sentiment_prediction( sentiment_probability ):\n    \n    if sentiment_probability >= 0.5 :\n        return 1\n    else :\n        return 0\n    \n    \nsentiment_prediction_vec = []\n\nsentiment_probability_prediction_vec_new = []\n\nfor sentiment_probability_prediction in sentiment_probability_prediction_vec :\n    \n    sentiment_prediction_vec.append( convert_probability_to_sentiment_prediction( sentiment_probability_prediction ))\n    \n    sentiment_probability_prediction_vec_new.append( sentiment_probability_prediction )\n    \n\nsentiment_prediction_vec\n\n","b5e3be68":"useetv_twitter_tweets_text_sentiment_analysis_data = pd.DataFrame({\"Tweets Text\" : useetv_twitter_data_after_june_2020_from_user[\"tweet\"] , \n                                                                   \"Normalized and Preprocessed Tweets Text\" : useetv_twitter_data_after_june_2020_from_user[\"normalized_and_preprocessed_tweets_text\"] , \n                                                                   \"Sentiment Probability Prediction\" : sentiment_probability_prediction_vec_new , \n                                                                   \"Sentiment Prediction\" : sentiment_prediction_vec})\n\nuseetv_twitter_tweets_text_sentiment_analysis_data.head()","5d3ac449":"useetv_twitter_tweets_text_sentiment_analysis_data.head( 30 )","dc7d25b0":"useetv_twitter_tweets_text_sentiment_analysis_data[\"Sentiment Prediction\"].value_counts()","21276f47":"## Preprocessing UseeTV Go Tweets Text Dataset","3dc3d467":"We will use word embedding, check this out at: https:\/\/www.kaggle.com\/ilhamfp31\/word2vec-100-indonesian","550c1acb":"## Feature Engineering","e9ee3e1f":"We will use pad sequneces, check this out at: https:\/\/keras.io\/preprocessing\/sequence\/","621cfa21":"## Preprocessing","6a0729e9":"We will use Tokenizer, check this out at: https:\/\/keras.io\/preprocessing\/text\/ ","ee16d6d0":"## Model","227728c2":"## Import Package","2a56badb":"## Input Data","df60ad36":"## Extra Preprocessing"}}