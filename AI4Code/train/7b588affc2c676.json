{"cell_type":{"ca09cbd3":"code","0b365907":"code","a91bbedd":"code","1d0d525d":"code","655539e5":"code","c5a36c19":"code","20f9d727":"code","bde14b15":"code","32602f41":"code","db7ab5b2":"code","023251f3":"code","20ad801f":"code","63b9d512":"markdown","2af27ff6":"markdown","e6d771b0":"markdown","498e9b53":"markdown","5fbc4696":"markdown","ee5814b8":"markdown","717b9df3":"markdown","a62af75f":"markdown","fbaf1724":"markdown","a4496d57":"markdown","f81cf141":"markdown","2bb2e5fa":"markdown"},"source":{"ca09cbd3":"# Importing Libraries and Dependencies\nimport numpy as np \nimport pandas as pd\nimport os\nimport random\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mlxtend.plotting import plot_confusion_matrix\n\n\nimport keras.backend as K\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\nfrom keras.layers import Conv2D, SeparableConv2D, MaxPool2D, LeakyReLU, Activation\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nimport tensorflow as tf","0b365907":"seed = 240\nnp.random.seed(seed)\ntf.random.set_seed(seed)","a91bbedd":"# Importing Dataset\n\ninputs = ('\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/')\n\nfig, ax = plt.subplots(2, 3, figsize=(15, 7))\nax = ax.ravel()\nplt.tight_layout()\n\nfor i, _set in enumerate(['train', 'val', 'test']):\n    set_path = inputs+_set\n    ax[i].imshow(plt.imread(set_path+'\/NORMAL\/'+os.listdir(set_path+'\/NORMAL')[0]), cmap='gray')\n    ax[i].set_title('Set: {}, Condition: Normal'.format(_set))\n    ax[i+3].imshow(plt.imread(set_path+'\/PNEUMONIA\/'+os.listdir(set_path+'\/PNEUMONIA')[0]), cmap='Reds')\n    ax[i+3].set_title('Set: {}, Condition: Pneumonia'.format(_set))\n","1d0d525d":"for _set in ['train', 'val', 'test']:\n    n_normal = len(os.listdir(inputs + _set + '\/NORMAL'))\n    n_infect = len(os.listdir(inputs + _set + '\/PNEUMONIA'))\n    print('Set: {}, normal images: {}, pneumonia images: {}'.format(_set, n_normal, n_infect))","655539e5":"def data_processing(img_dims, batch_size):\n    \n    #Data Augmentation Images\n    train_datagen = ImageDataGenerator(rescale=1.\/255, zoom_range=0.3, vertical_flip=True)\n    test_val_datagen = ImageDataGenerator(rescale=1.\/255)\n    \n    train_gen = train_datagen.flow_from_directory(directory=inputs+'train', target_size=(img_dims, img_dims), batch_size=batch_size, class_mode='binary', shuffle=True)\n    test_gen = test_val_datagen.flow_from_directory(directory=inputs+'test', target_size=(img_dims, img_dims), batch_size=batch_size, class_mode='binary', shuffle=True)\n    \n    test_data = []\n    test_labels = []\n    \n    for cond in ['\/NORMAL\/', '\/PNEUMONIA\/']:\n        for img in (os.listdir(inputs + 'test' + cond)):\n            img = plt.imread(inputs+'test'+cond+img)\n            img = cv2.resize(img, (img_dims, img_dims))\n            img = np.dstack([img, img, img])\n            img = img.astype('float32') \/ 255\n            if cond=='\/NORMAL\/':\n                label = 0\n            elif cond=='\/PNEUMONIA\/':\n                label = 1\n            test_data.append(img)\n            test_labels.append(label)\n        \n    test_data = np.array(test_data)\n    test_labels = np.array(test_labels)\n    \n    return (train_gen, test_gen, test_data, test_labels)","c5a36c19":"img_dims = 150\nepochs = 10\nbatch_size = 32\n\ntrain_gen, test_gen, test_data, test_labels = data_processing(img_dims, batch_size)","20f9d727":"input_shape = Input(shape=(img_dims, img_dims, 3))\n\n# First convulution block\nx = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(input_shape)\nx = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\n\n\n# Second convolution block\nx = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\n\n\n# Third convolution block\nx = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\n\n# Fourth convolution block\nx = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\nx = Dropout(rate=0.2)(x)\n\n# Fifth convolution block\nx = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\nx = Dropout(rate=0.2)(x)","bde14b15":"# Flattening Layer , Dropout - Deactivate some neurons while training to reduce overfitting\nx = Flatten()(x)\nx = Dense(units=512, activation='relu')(x)\nx = Dropout(rate=0.7)(x)\nx = Dense(units=128, activation='relu')(x)\nx = Dropout(rate=0.5)(x)\nx = Dense(units=64, activation='relu')(x)\nx = Dropout(rate=0.3)(x)\n\n# Output layer\noutput = Dense(units=1, activation='sigmoid')(x)","32602f41":"# Creating model\nmodel = Model(inputs=input_shape, outputs=output)\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Callbacks\ncheckpoint = ModelCheckpoint(filepath='best_weights.hdf5', save_best_only=True, save_weights_only=True)\nlr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1, mode='min')","db7ab5b2":"hist = model.fit_generator(\n           train_gen, steps_per_epoch=train_gen.samples \/\/ batch_size, \n           epochs=epochs, validation_data=test_gen, \n           validation_steps=test_gen.samples \/\/ batch_size, callbacks=[checkpoint, lr_reduce])","023251f3":"fig, ax = plt.subplots(1, 2, figsize=(10, 3))\nax = ax.ravel()\n\nfor i, met in enumerate(['accuracy', 'loss']):\n    ax[i].plot(hist.history[met])\n    ax[i].plot(hist.history['val_' + met])\n    ax[i].set_title('Model {}'.format(met))\n    ax[i].set_xlabel('epochs')\n    ax[i].set_ylabel(met)\n    ax[i].legend(['train', 'val'])","20ad801f":"from sklearn.metrics import accuracy_score, confusion_matrix\n\npreds = model.predict(test_data)\n\nmodel_acc = accuracy_score(test_labels, np.round(preds))*100\ncm = confusion_matrix(test_labels, np.round(preds))\ntn, fp, fn, tp = cm.ravel()\n\nprint('CONFUSION MATRIX')\nplt.figure()\nplot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True, cmap=plt.cm.Blues)\nplt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.show()\n\nprint('\\nTESTING METRICS')\nprecision = tp\/(tp+fp)*100\nrecall = tp\/(tp+fn)*100\nprint('Accuracy: {}%'.format(model_acc))\nprint('Precision: {}%'.format(precision))\nprint('F1-score: {}'.format(2*precision*recall\/(precision+recall)))\nprint('Recall: {}%'.format(recall))\n\n\nprint('\\nTRAINING METRICS')\nprint('Train accuracy: {}'.format(np.round((hist.history['accuracy'][-1])*100, 2)))","63b9d512":"### **Splitting dataset**\n\n1. Pneumonia Images: Train - 3875 , Validation - 8, Test - 390\n2. Normal Images: Train - 1341, Validation - 8, Test - 234","2af27ff6":"## **What is Pneumonia ?**\n\nPneumonia is a infection caused by bacteria, virus, fungi in both the lungs. This leads to inflammation of airsacs in both the lungs called as alveoli. The alveoli gets filled with fluid and pus making breathing difficult. Fast detection and diagnosis is very essential. \n\n![Pneumonia](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcQY7ascBNxR4LJL-sMgSrdEhSWy8ieFn2D4OpJJyPYV-3-3wRyp&usqp=CAU)","e6d771b0":"### **Visualizing Output**","498e9b53":"### **Data Augmentation**\n\nData Augmentation is an effective way of increasing the training set size. It allows model to see more diversed datapoints that are representative during training.\n\nWhy rescale to 1.\/255 ?\n\nEvery image image is made by pixel having value in range 0-255. Where 0 is black and 255 is white. Since maximum pixel value is 255, we transform every pixel value from range [0,255] -> [0,1]\n\nKnow more about Rescaling from this LinkedIN article : [Keras Image Processing](https:\/\/www.linkedin.com\/pulse\/keras-image-preprocessing-scaling-pixels-training-adwin-jahn\/)","5fbc4696":"`Recall` - Out of all positive classes how much is predicted correctly. The higher the better.\n![Recall](https:\/\/miro.medium.com\/max\/246\/1*BT3awaBdZHsit5s41LPb9A.png)\n\n`Precision` - Out of all positive classes and our prediction, how many are actually positive.\n![Precision](https:\/\/miro.medium.com\/max\/249\/1*QRIZDkk_FffXKs_07ZlhZw.png)\n\n`F1 score` - F-score helps to measure Recall and Precision at the same time. It uses Harmonic Mean in place of Arithmetic Mean.\n![F1 Score](https:\/\/miro.medium.com\/max\/716\/1*98FaAKfPWo-EBTbjsxm4GA.png)\n\nSimple representation of `Confusion matrix`\n![Confusion matrix](https:\/\/miro.medium.com\/max\/924\/1*7EYylA6XlXSGBCF77j_rOA.png)","ee5814b8":"### **Constructing CNN**\n1. `Five concolution blocks` used comprises of convolutional layers, max-pooling and batch normalization\n2. dropouts are used to reduce over-fitting\n3. Relu - activation function is used, on last layer Sigmoid is used because its a binary classification problem\n4. Adam optimizer and cross-entropy as loss\n\n![Full CNN](https:\/\/miro.medium.com\/max\/1400\/0*usI_HmpFeF2iPBEM.png)\n\n`Callbacks` - Callbacks are used to perform actions on various stages of training; start or end of epochs\n\n`EarlyStopping` - A`model.fit()` training loop will check at end of every epoch whether the loss is no longer decreasing, considering the min_delta and patience if applicable. \n\n`ModelCheckpoint` - With large abouts of data multiple iterations are required to get good result, thus model checkpoint is used to save a copy of best performing model only when epoch that improves the metrics ends.\n\n![Early Stopping](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2018\/04\/Screen-Shot-2018-04-04-at-12.31.56-AM.png) ","717b9df3":"### **Optimizers and Loss**\n\n`adam optimizer` - The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients.It achieves good results faster.\n\n`binary_crossentropy` - Used in case of only 2 labelled classes (0 and 1)","a62af75f":"### Glimpse on the dataset\n\nThe dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (Pneumonia\/Normal). There are 5,863 X-Ray images (JPEG) and 2 categories (Pneumonia\/Normal). Chest X-ray images (anterior-posterior) were selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Children\u2019s Medical Center, Guangzhou. All chest X-ray imaging was performed as part of patient's routine clinical care. For the analysis of chest x-ray images, all chest radiographs were initially screened for quality control by removing all low quality or unreadable scans.\n\n1. ravel() - Library level function that returns an original array\n2. tight_layout() - Adjusts subplot params and fits the plot in area","fbaf1724":"***Seed***\n\nSeed in machine learning is a pseudorandom number generator. By setting value of seed we will be able to generate same pattern of numbers.","a4496d57":"### **Additional Layers**\n\nIn addition to Convolution layer there are other important layers called pooling and flattening layers.\n\n`Pooling` - Poiling is the process of merging, this reduces the size of the data. Getting more meaning full data rather than loosing. By removing some noise in the data and extracting only the significant one, we can reduce overfitting and speed up the computation.\n![Pooling Layers](https:\/\/miro.medium.com\/max\/1400\/1*-3-9b0tAakAsdozzhNlEww.png)\n\n`Flattening Layer` - Last stage of CNN. It is the used to convert data into 1-D array for providing input to next layer. We flatten the output of the convolutional layers to create a single long feature vector. And it is connected to the final classification model, which is called a fully-connected layer. In other words, we put all the pixel data in one line and make connections with the final layer.\n\n![Flattening Layer](https:\/\/miro.medium.com\/max\/1400\/1*IWUxuBpqn2VuV-7Ubr01ng.png)\n","f81cf141":"### **Parameters**\n\n`img_dims` = 150\n\n`epochs` - Number of times the training vectors are used to update weights\n\n`batch_size` - Number of samples propagated through network, from total trainig samples the algorithm takes first 128 training dataset from (1 to 128) and trains the network. And subsequently trains with set of 128 till end.\n[](http:\/\/)","2bb2e5fa":"So, our model has a 98% recall and 88% precision. In such problems, a good recall value is expected. Precision and Recall follows a trade-off, and you need to find a point where your recall, as well as your precision, is more than good but both can't increase simultaneously.\n\n### I hope you enjoyed this kernel. Happy Kaggling!!"}}