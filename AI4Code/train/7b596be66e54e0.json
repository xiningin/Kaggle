{"cell_type":{"8782f88a":"code","66e66bc5":"code","c967457f":"code","3b9032f0":"code","0d2ad118":"code","65488921":"code","313dda44":"code","926c935a":"code","84a71949":"code","a1e46dd1":"code","3b7f0049":"code","b20b12ad":"code","6e0c0ce5":"code","17f41256":"code","9aeafc7a":"code","adcd5183":"code","0de73f76":"code","ed730db0":"code","a560aa08":"code","b915d5ff":"markdown","72f73f01":"markdown","700ab58c":"markdown","0831a473":"markdown","deb0687a":"markdown","782b3603":"markdown","15516439":"markdown","f6758010":"markdown","ca593921":"markdown","f1ec7944":"markdown"},"source":{"8782f88a":"# Import libraries\n# Importo le librerie\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split","66e66bc5":"# Read csv files\n# Leggo i files .csv\n\ndf_train = pd.read_csv('..\/input\/train.csv', index_col=0)    #train dataset import. Set first column 'passengerId' as index\ndf_test = pd.read_csv('..\/input\/test.csv', index_col=0)    #test dataset import. Set first column 'passengerId' as index\n\n# Concatenate two dataframes without 'Survived' column, to better management.\n# concateno i due dataframe senza la colonna 'Survived', per una miglior gestione\n\ndf_all = pd.concat([df_train, df_test], axis=0, sort=True).drop(['Survived'], axis = 1)\n","c967457f":"# First 5 train rows\n# Prime 5 righe dela df train\n\ndf_all.head()","3b9032f0":"# Plot a heatmap to show how nan are distributed, first on train dataset....\n# Plotto una heatmap  per visualizzare come sono distribuiti i Nan, primo sul dataset di train...\n\nsns.heatmap(data=df_all.isnull())","0d2ad118":"# fill age's nan with the median of age's values grouped by 'Class' and 'Sex'\n# Riempio i nan della colonna 'age' con la mediana dei valori di 'age' raggruppati per 'Class' e 'Sex'\n\ndf_all['Age'] = df_all.groupby(['Pclass','Sex'], sort=False)['Age'].apply(lambda x: x.fillna(x.median()))\n","65488921":"# Count 'Embarked' values...\n# Conto i valori della colonna 'Embarked'...\n\ndf_all['Embarked'].value_counts(dropna=False).plot(kind='bar', figsize=(6,4), title='Embarked')","313dda44":"# ... and fill Nan with the most frequent value ('S')\n# ... e riempi i Nan con il valore pi\u00f9 frequente ('S')\n\ndf_all['Embarked'] = df_all['Embarked'].fillna('S')","926c935a":"# fill also the unique Nan in 'Fare' test DF's column with the median of the class\n# riempi l'unico 'Nan' nella colonna 'Fare' del dataframe DF\n\ndf_all['Fare'] = df_all.groupby(['Pclass'], sort=False)['Fare'].apply(lambda x: x.fillna(x.median()))\n\ndf_all.info()","84a71949":"# From 'Name' column, Extract titles name\n# Estraggo il titolo dalla colonna 'Nome'\n\ndf_all['Title'] = df_all['Name'].apply(lambda x: x[x.find(',')+2:x.find('.')])\n\n# Try to define the correct title classification for un-classified values ..\n# Cerco di definire la corretta classificazione per i titoli..\n\n\ndf_all['Title'] = df_all['Title'].replace(['Mme','Ms'], 'Mrs')\ndf_all['Title'] = df_all['Title'].replace(['Mlle','Lady'], 'Miss')\n\n# ...and group the less common titles under 'other' label\n# ... \n\ndf_all['Title'] = df_all['Title'].replace(['the Countess',\n                                               'Capt', 'Col','Don', \n                                               'Dr', 'Major', 'Rev', \n                                               'Sir', 'Jonkheer', 'Dona'], 'Others')\n\ndf_all['Title'].value_counts().plot(kind='bar', figsize=(6,4), title='Title')","a1e46dd1":"# The get_dummies converts categorical variables in dummies variables (boolean)\ndf_all = pd.get_dummies(df_all,columns=['Sex','Embarked','Title'])","3b7f0049":"# Remove Cabin, Name and Ticket Columns because are unuseful\ndf_all.drop(['Cabin','Name','Ticket'], axis=1, inplace=True)","b20b12ad":"# With heatmap we try to find correlation among features\n# Con heatmap cerchiamo correlazioni tra le features\n\nplt.figure(figsize= (10,10), dpi=100)\nsns.heatmap(df_all.corr(), square=True, annot=True)","6e0c0ce5":"# I use my train dataframe to assign values to y (predicted label) and x\ny = df_train['Survived']\nx = df_all.iloc[:891,1:] # original 891 train rows\n# con iloc assegno ad una variabile le colonne dalla 1 (seconda) in poi (i primi : significano tutte le righe)","17f41256":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nbestfeatures = SelectKBest(score_func=chi2, k='all')\nfit = bestfeatures.fit(x,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(x.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(15,'Score'))  #print 10 best features","9aeafc7a":"# With ExtraTreesClassifier i try to find correlations between features\n# Con l'ExtraTreesClassifier cercco di trovare correlazioni tra le features\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(x,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=x.columns)\nfeat_importances.nlargest(15).plot(kind='barh')\nplt.show()","adcd5183":"# With train_test_split i divide my train df in two parts, so i can run my test and show how my predictions works\nX_train, X_test, Y_train, Y_test = train_test_split(x.drop(['Fare','Title_Others','Embarked_C','Embarked_S','Embarked_Q'], axis=1), y, test_size=0.3, random_state=42)","0de73f76":"# Test with deep learning with 2 model, activation sigmoid, optimizer adam learning rate .1 (0.799 in kaggle )\n# Test con deep learning con 2 modelli, activation sigmoid, optimizer adam e learning rate .1 (0.799 in kaggle )\n\n# Test with deep learning\n\nfrom keras import models\nfrom keras import layers\nfrom keras import optimizers\n\n# initialize the network\nmodel = models.Sequential()\n\n# add nodes to the network\nmodel.add( \n    layers.Dense(1,                    # no. of neurons\n                 activation='sigmoid', # activation function\n                 input_shape=(14,)      # shape of the input\n                ))\n\nmodel.add( \n    layers.Dense(1,                    # no. of neurons\n                 activation='sigmoid', # activation function\n                ))\n\n# finalize the network\nmodel.compile(\n    optimizers.Adam(lr=.01)\n, # lr is the learning rate\n    loss='binary_crossentropy',       # loss function\n    metrics=['accuracy'] )   # additional quality measure\n\n# train the network\nhist = model.fit( x=x, # training examples\n           y=y, # desired output\n           epochs=200, # number of training epochs \n           verbose=1)\n# 0.79425 in kaggle with 2 model, activation sigmoid, optimizer adam learning rate .1\nmodel.evaluate(x, y)","ed730db0":"# Plot accuracy and loss\n# Plotto accuracy and loss\n\nfig, axes = plt.subplots(figsize=(6,6))\n\naxes.plot(hist.history['loss'], label='Loss')\naxes.plot(hist.history['acc'], label='Accuracy')\n\naxes.set_title(\"Training History\", fontsize=18)\naxes.set_xlabel(\"Epochs\", fontsize=18)\naxes.legend(fontsize=20)\n\n# Final accuracy\nprint (\"Accuracy:\", hist.history['acc'][-1])","a560aa08":"# write results in a new 'Survived' column df test dataframe. \n# I take only last 418 rows of df_all dataframe, because they represent my test df\ndf_test['Survived'] = model.predict_classes(df_all.iloc[891:,1:])\ndf_test['Survived'].to_csv(\"..\/prediction.csv\", header=\"PassengerId,Survived\")\ndf_test.drop(['Survived'], axis=1, inplace=True)","b915d5ff":"## **Try to find correlations between features - Cerco correlazioni tra le features**","72f73f01":"## **Manage Nan values - Gestione dei valori Nan**","700ab58c":"* Nan remains only in column 'Cabin' , but doesn't matter, i'll drop this column later\n* Rimangono Nan values solo nella colonna Cabin ma non importa, verr\u00e0 droppata pi\u00f9 tardi","0831a473":"Fare, Title_Others, Embarked_S, Embarked_C, , Embarked_Q are poorly correlated, so i drop them","deb0687a":"## **Data visualization - Visualizzazione dei dati**","782b3603":"## **Data Manipulation - Manipolazione dei dati**","15516439":"## **Prediction with deep learning**","f6758010":"## **Read data from files - leggo i dati dai files**","ca593921":"## **Data Analysis**","f1ec7944":"## I tried to simplify the analysis of Titanic dataset as much as possible, using Keras model and Deep Learning. Hope it will be useful\n##  Ho cercato di semplificare il pi\u00f9 possibile l'analisi del dataset del Titanic, utilizzando Keras e deep learning. Spero vi sia utile"}}