{"cell_type":{"ed896a47":"code","0a91cb00":"code","9d41e1b0":"code","a6cfd7fe":"code","51829e5f":"code","19a20090":"code","d3e9dcdf":"code","bae584d9":"code","19019e6c":"code","da50eaa5":"code","1bafba9f":"code","9b333bfc":"code","c28a7420":"code","c52aa3d5":"code","cabb4900":"code","9a3bbf37":"code","4197a36e":"code","0427111b":"code","3eceb32b":"code","111ac431":"code","9c2b3125":"code","cb1ec253":"code","2ec36cfa":"code","4334824e":"code","6c8e9826":"code","d318fa4e":"code","94b85456":"code","25b10309":"code","2dcf1214":"code","63a66e2c":"code","3ee55d21":"code","e9cc6ed8":"code","5916192a":"code","1bfc2749":"code","8c16c5c1":"code","1c7ea03a":"code","79e051bd":"code","fb5bbd56":"code","9e25e456":"code","7e2dd5c8":"code","c3f9a58f":"code","a375b3ef":"code","43aba34f":"code","9357caab":"code","9b5ac71d":"code","c9085586":"code","45661eaa":"code","423fba21":"code","1a54be2d":"code","5c592ae9":"code","28f95cc4":"code","af020e36":"code","5beca885":"code","30d4f017":"code","3179c2e2":"code","b81be00a":"code","14063ce4":"code","13c75720":"code","8ec2d923":"code","597e9f4c":"code","fc632dc9":"code","9041ce51":"code","ce18ce0c":"code","f8b4169b":"code","6090722c":"code","0fd2a690":"code","415f565a":"code","b28c3056":"code","ace39323":"code","11ad054f":"code","fd1a6dc1":"code","1deee0b3":"code","d20fe810":"code","31cfbb08":"code","fb0e9a1c":"code","261a8b34":"markdown","21c46e34":"markdown","d42abe10":"markdown","86e13480":"markdown","38df4b43":"markdown","55bed100":"markdown","d7ace6e4":"markdown","cf6421c9":"markdown","0e58d5bb":"markdown","e7ffd326":"markdown","e238dd38":"markdown","d43aa2c6":"markdown","a38a9d7d":"markdown","985b06aa":"markdown","965a9aca":"markdown","ee4ef8b8":"markdown","8b2b6bc6":"markdown","6273d9a1":"markdown","2d32fdd8":"markdown","b3438e52":"markdown","9bce1a5d":"markdown","286162ad":"markdown","a50eeb05":"markdown","3c6ea37e":"markdown","2e40348a":"markdown","93e4928d":"markdown","e09545f8":"markdown","32cc57ba":"markdown","78a32a66":"markdown","7ad22783":"markdown","b3e0cda6":"markdown","d6a162cd":"markdown","01019eaf":"markdown","d722e781":"markdown","dd237873":"markdown","b886daea":"markdown","e01a0841":"markdown","d734798b":"markdown"},"source":{"ed896a47":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import classification_report,accuracy_score,mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nimport seaborn as sbn","0a91cb00":"os.chdir('..\/input')","9d41e1b0":"os.listdir()","a6cfd7fe":"data = pd.read_csv('energydata_complete.csv')","51829e5f":"data.shape","19a20090":"data.columns","d3e9dcdf":"data.dtypes","bae584d9":"data.isnull().sum()","19019e6c":"data['date']","da50eaa5":"data['date'] = pd.to_timedelta(data['date'].astype('datetime64')).dt.seconds","1bafba9f":"data['date'].dtype","9b333bfc":"y = data['Appliances']","c28a7420":"X = data\ndel X['Appliances']","c52aa3d5":"X.columns","cabb4900":"X.shape","9a3bbf37":"pd.plotting.scatter_matrix(data.iloc[:,:7],figsize = (10,10))","4197a36e":"X = X.drop(['T2','RH_2','T3'],axis=1)","0427111b":"X.shape","3eceb32b":"X['date'].describe()","111ac431":"plt.plot(X['date'])","9c2b3125":"plt.scatter(np.arange(len(X['date'])),X['date'])","cb1ec253":"X = X.drop(['date'],axis=1)","2ec36cfa":"X.shape","4334824e":"plt.hist(X['lights'])","6c8e9826":"X['lights'].describe()","d318fa4e":"pd.plotting.scatter_matrix(data.iloc[:,14:23],figsize = (10,10))","94b85456":"X = X.drop(['T7','T8','RH_7','RH_8'],axis=1)","25b10309":"X.shape","2dcf1214":"pd.plotting.scatter_matrix(data.iloc[:,23:28],figsize=(10,10))","63a66e2c":"del X['rv1']","3ee55d21":"plt.scatter(np.arange(len(X['rv2'])),X['rv2'])","e9cc6ed8":"del X['rv2']","5916192a":"X.shape","1bfc2749":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)","8c16c5c1":"X_train.shape,X_test.shape","1c7ea03a":"X.dtypes","79e051bd":"scaler = StandardScaler()","fb5bbd56":"X_train = scaler.fit_transform(X_train)","9e25e456":"X_train.shape","7e2dd5c8":"def adjusted_r_square(n,k,y,yhat):\n    SS_Residual = sum((y-yhat)**2)\n    SS_Total = sum((y-np.mean(y))**2)\n    r_squared = 1 - (float(SS_Residual))\/SS_Total\n    adjusted_r_squared = 1 - (((1-r_squared)*(k-1))\/(n-k-1))\n    return r_squared,adjusted_r_squared","c3f9a58f":"linReg = LinearRegression()\nlinReg.fit(X_train,y_train)","a375b3ef":"linReg.score(X_train,y_train)","43aba34f":"y_predLinReg = linReg.predict(scaler.transform(X_test))","9357caab":"adjusted_r_square(X.shape[0],X.shape[1],y_test,y_predLinReg)","9b5ac71d":"mean_squared_error(y_predLinReg,y_test)","c9085586":"DecisonReg = DecisionTreeRegressor()\nDecisonReg","45661eaa":"DecisonReg.fit(X_train,y_train)\nDecisonReg.score(X_train,y_train)","423fba21":"y_predDecisonReg = DecisonReg.predict(scaler.transform(X_test))\n\nmean_squared_error(y_predDecisonReg,y_test)","1a54be2d":"adjusted_r_square(X.shape[0],X.shape[1],y_test,y_predDecisonReg)","5c592ae9":"DecisonReg.feature_importances_","28f95cc4":"X.columns","af020e36":"RandomForestReg = RandomForestRegressor(n_estimators=600)\nRandomForestReg","5beca885":"RandomForestReg.fit(X_train,y_train)\nRandomForestReg.score(X_train,y_train)","30d4f017":"y_predRandomForestReg = RandomForestReg.predict(scaler.transform(X_test))\n\nmean_squared_error(y_predRandomForestReg,y_test)","3179c2e2":"adjusted_r_square(X.shape[0],X.shape[1],y_test,y_predRandomForestReg)","b81be00a":"svmReg = SVR(C = 0.75)\nsvmReg","14063ce4":"svmReg.fit(X_train,y_train)\nsvmReg.score(X_train,y_train)","13c75720":"y_predsvmReg = svmReg.predict(scaler.transform(X_test))\n\nmean_squared_error(y_predsvmReg,y_test)","8ec2d923":"adjusted_r_square(X.shape[0],X.shape[1],y_test,y_predsvmReg)","597e9f4c":"svmReg = SVR(kernel='linear')\nsvmReg","fc632dc9":"svmReg.fit(X_train,y_train)\nsvmReg.score(X_train,y_train)","9041ce51":"y_predsvmReg = svmReg.predict(scaler.transform(X_test))\n\nmean_squared_error(y_predsvmReg,y_test)","ce18ce0c":"adjusted_r_square(X.shape[0],X.shape[1],y_test,y_predsvmReg)","f8b4169b":"svmReg = SVR(kernel='poly')\nsvmReg\n\n","6090722c":"svmReg.fit(X_train,y_train)\nsvmReg.score(X_train,y_train)\n\n","0fd2a690":"y_predsvmReg = svmReg.predict(scaler.transform(X_test))\n\nmean_squared_error(y_predsvmReg,y_test)","415f565a":"adjusted_r_square(X.shape[0],X.shape[1],y_test,y_predsvmReg)","b28c3056":"gradBoostReg = GradientBoostingRegressor(loss='ls',n_estimators=500)\ngradBoostReg","ace39323":"gradBoostReg.fit(X_train,y_train)\ngradBoostReg.score(X_train,y_train)","11ad054f":"y_predgradBoostReg = gradBoostReg.predict(scaler.transform(X_test))\n\nmean_squared_error(y_predgradBoostReg,y_test)","fd1a6dc1":"adjusted_r_square(X.shape[0],X.shape[1],y_test,y_predgradBoostReg)","1deee0b3":"XGBoostReg = xgb.XGBRegressor(objective=\"reg:linear\",n_estimators=500)\nXGBoostReg","d20fe810":"XGBoostReg.fit(X_train,y_train)\nXGBoostReg.score(X_train,y_train)","31cfbb08":"y_predXGBoostReg = XGBoostReg.predict(scaler.transform(X_test))\n\nmean_squared_error(y_predXGBoostReg,y_test)","fb0e9a1c":"adjusted_r_square(X.shape[0],X.shape[1],y_test,y_predXGBoostReg)","261a8b34":"#### Lets look at date","21c46e34":"#### So we keep T_out, RH_out, RH_9 and T9. All else RH and T we remove","d42abe10":"### It is highly skewed, So we need to see waht information is important here.","86e13480":"### As we can see that rv2 does not have any significant variation in it. So, we can remove it.","38df4b43":"- #### Decision Tree Regression","55bed100":"#### Lets look at the other features","d7ace6e4":"### Now lets look at the distribution of each feature to understand what extra preprocessing we need to do","cf6421c9":"#### Lets look at the lights","0e58d5bb":"### Lets scale the features","e7ffd326":"### Lets divide the dataset into train and test","e238dd38":"### Load the dataset","d43aa2c6":"### Lets check for null values","a38a9d7d":"### Some insights from the above plot\n#### - T1,RH_1, T2,RH_2,T3 are normally distributed. No extra preprocessing needed.\n#### - lights seems to be skewed. Need preprocessing\n#### - date seems to be same for all the examples\n#### - Each pair of RH and T (RH_1&RH_2 or T1&T2) are positively correlated. This make sense as well as each T is the temperature in different regions of the house.So, they will vary together. Similar for the RH as each of them is the Humidity in different regions of the house.\n","985b06aa":"### Lastly we try XGBoost","965a9aca":"### Lets see the data type of each column","ee4ef8b8":"### Import all the necessary packages","8b2b6bc6":"- #### RandomForest","6273d9a1":"## Here we conclude\n- ### Our best result was from Random forest with 600 estimators. A training R-Square of 93.9, testing R_square of 53.65, mse of 4950.\n- ### Other ensemble learning models are overfitting currently.","2d32fdd8":"#### Here decison tree seems to have over fitted","b3438e52":"### Thank you","9bce1a5d":"### Now we can start making the model","286162ad":"### As we can see the data type is float or int everywhere. So, we don't need to do any special preprocessing for each column","a50eeb05":"- #### Gradient Boosting ","3c6ea37e":"### Lets convert the format of the dat to seconds","2e40348a":"### What we can infer:\n- ### rv1 and rv2 are highly correlated\n- ### Both rv seems to have very less variance in them.\n- ### Except rv's all the other feature have a normal distribution\n- ### We will analyse Windspeed, Tdewpoint, visibility independently","93e4928d":"- #### The linear kernel is performing badly. Lets try polynomial kernel","e09545f8":"### Lets download the dataset","32cc57ba":"#### What we can infer from above.\n#### - Except T_out and Rh_out all other T and RH are correlated\n#### - RH_out is a bit skewed\n#### - Pressure_mm is normally distributed","78a32a66":"- #### Here we can see that the RBF kernel is performing badly. Lets try Linear kernel","7ad22783":"- #### Logistic Regression","b3e0cda6":"#### As all the T and RH bring the same information. So, its is better to keep just one of them. We will keep T1 and RH_1","d6a162cd":"### We need to predict the Application Energy","01019eaf":"### Lets see the columns which are here","d722e781":"- ### Support vector machine ","dd237873":"### For the next stage we can work with dimensionality reduction algorithms like PCA","b886daea":"### This function will return first the R-Squared and second the Adjusted R Squared","e01a0841":"### As we ca see that currently the ensemble learning models are performing the best. So, lets try another ensemble learning \n### classifier Graient boosting ","d734798b":"#### Here we worked with all type of loss functions and the best was least square(ls). At higher number of estimator it is overfitting."}}