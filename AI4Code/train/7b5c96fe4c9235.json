{"cell_type":{"4b31ea7f":"code","7b796932":"code","9d5176e4":"code","21342b91":"code","2386ce87":"code","d77b5984":"code","be7b0b95":"code","79fcb141":"code","f51401cf":"code","8895ab84":"code","416748f5":"code","3c832d0b":"code","457b12b6":"code","37a8474c":"code","ec53be2d":"code","a92e6f5d":"code","282c457e":"code","dde6dfa6":"code","30bc236e":"code","eb334498":"code","72c03fd9":"code","33ee2dd4":"code","0d71295c":"code","7b6b189b":"code","42cd919e":"code","971700b2":"code","3e61af70":"code","4de19919":"markdown","00d0a5f5":"markdown","87547c30":"markdown","508a5167":"markdown","ba720f2d":"markdown","f756b95f":"markdown","94342930":"markdown","dfd031f4":"markdown","29978eec":"markdown","1162ed0a":"markdown","0978cb95":"markdown","1cc1121e":"markdown","6945bffe":"markdown"},"source":{"4b31ea7f":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\nfrom datetime import datetime\nfrom pytz import timezone\ndatetime.now(timezone('Asia\/Tokyo')).strftime('%Y\/%m\/%d %H:%M:%S')\n\ndef refer_args(x):\n    if type(x) == 'method':\n        print(*x.__code__.co_varnames.split(), sep='\\n')\n    else:\n        print(*[x for x in dir(x) if not x.startswith('__')], sep='\\n')","7b796932":"from collections import Counter, defaultdict\nimport os\nfrom operator import itemgetter\nimport re\nimport string\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom nltk.tokenize import word_tokenize\nimport gensim\nfrom gensim.models import word2vec\nfrom wordcloud import WordCloud\nfrom janome.tokenizer import Tokenizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\n \nplt.style.use('ggplot')\nstop = set(stopwords.words('english')) | {\n            'i','im','you','youre','they','theyre','he','hes','she','shes','we','our','us','were','arent',\\\n            'can','cant','could','couldnt','will','wont','would','wouldnt','should','shouldnt','may',\\\n            'dont','didnt','doesnt'}\n\npd.set_option('display.max_colwidth', 200)","9d5176e4":"tweet = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntweet.head(3)","21342b91":"df = pd.concat([tweet,test],sort=False)","2386ce87":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\ndf['text']=df['text'].apply(lambda x: remove_URL(x))\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\ndf['text']=df['text'].apply(lambda x: remove_html(x))\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\ndf['text']=df['text'].apply(lambda x: remove_emoji(x))\n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\ndf['text']=df['text'].apply(lambda x: remove_punct(x))","d77b5984":"tweet=df[:len(tweet)]\ntest=df[len(tweet):]\ntweet['target']=tweet['target'].apply(lambda x:int(x))\ntweet.head()","be7b0b95":"disaster=tweet[tweet['target']==1]\nnot_disaster=tweet[tweet['target']==0]\ndisaster.head(10)","79fcb141":"def CountWord(S):\n    cntD,cntN=0,0\n    for s in disaster['text']:\n        isok=False\n        for c in s.split():\n            if c.lower()==S:\n                isok=True\n                break\n        if isok:\n            cntD+=1\n            \n    for s in not_disaster['text']:\n        isok=False\n        for c in s.split():\n            if c.lower()==S:\n                isok=True\n                break\n        if isok:\n            cntN+=1\n    return cntD,cntN","f51401cf":"def words_list(is_target):\n    words=[]\n    for x in tweet[tweet['target']==is_target]['text'].str.split():\n        for s in x:\n            if not s.lower() in stop:\n                words.append(s.lower())\n    return words","8895ab84":"dwords=words_list(is_target=1)\nd=defaultdict(int)\nfor word in dwords:\n    d[word]+=1\ntop30=sorted(d.items(), key=itemgetter(1), reverse=True)[:30]","416748f5":"fig = plt.figure(figsize=(8.0, 8.0))\nx, y = zip(*top30)\n_ = plt.barh(x[::-1],y[::-1])\n_ = plt.title('Common words in disaster tweets')","3c832d0b":"d=defaultdict(int)\nfor word in nwords:\n    d[word]+=1\ntop=sorted(d.items(),key=itemgetter(1),reverse=True)[:300]\n\nLI=[]\nfor w,c in top:\n    cntD,cntN=CountWord(w)\n    LI.append((w,cntD,cntN,(cntD\/(cntD+cntN))))\nLI.sort(key=itemgetter(3))\n\nfor w,a,b,p in LI[:31]:\n    print(w)\n    print('disaster:{:.5f}%'.format(p*100))\n    print('non-disaster:{:.5f}%'.format(100-p*100))\n    print('{0}\/{1}'.format(a,a+b))\n    print()","457b12b6":"nwords=words_list(0)\nn_d=defaultdict(int)\nfor word in nwords:\n    n_d[word]+=1\ntop30=sorted(n_d.items(),key=lambda x:x[1],reverse=True)[:30]","37a8474c":"fig=plt.figure(figsize=(8.0, 8.0))\nx,y=zip(*top30)\n_ = plt.barh(x[::-1], y[::-1])\n_ = plt.title('Common words in non-disaster tweets')","ec53be2d":"d=defaultdict(int)\nfor word in nwords:\n    d[word]+=1\ntop=sorted(d.items(),key=itemgetter(1),reverse=True)[:300]\n\nLI=[]\nfor w,c in top:\n    cntD,cntN=CountWord(w)\n    LI.append((w,cntD,cntN,(cntD\/(cntD+cntN))))\nLI.sort(key=itemgetter(3))\n\nfor w,a,b,p in LI[:31]:\n    print(w)\n    print('disaster:{:.5f}%'.format(p*100))\n    print('non-disaster:{:.5f}%'.format(100-p*100))\n    print('{0}\/{1}'.format(a,a+b))\n    print()","a92e6f5d":"#target==is_target\u3067W\u3092\u542b\u3080\u30c4\u30a4\u30fc\u30c8\u3092\u5168\u3066\u8868\u793a\n\ndef view(W, is_target):\n    for s in tweet[tweet['target']==is_target]['text']:\n        for c in s.split():\n            if c.lower()==W:\n                print(s)\n                break","282c457e":"view('happy',1)","dde6dfa6":"view('apocalypse',0)","30bc236e":"def create_wordcloud(text,stop,is_target):\n    wordcloud=WordCloud(background_color=\"white\" if not is_target else \"black\",width=900,height=500,\\\n                       stopwords=stop).generate(text)\n    plt.figure(figsize=(15,12))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.show()","eb334498":"create_wordcloud(' '.join(dwords),stop,1)","72c03fd9":"create_wordcloud(' '.join(nwords),stop,0)","33ee2dd4":"Disaster_Corpus=[]\nNon_disaster_Corpus=[]","0d71295c":"for s in tweet[tweet['target']==1]['text'].str.split():\n    arr=[]\n    for c in s:\n        arr.append(c.lower())\n    Disaster_Corpus.append(arr.copy())\nDisaster_Corpus[:10]","7b6b189b":"for s in tweet[tweet['target']==0]['text'].str.split():\n    arr=[]\n    for c in s:\n        arr.append(c.lower())\n    Non_disaster_Corpus.append(arr.copy())\nNon_disaster_Corpus[:10]","42cd919e":"Corpus=Disaster_Corpus+Non_disaster_Corpus","971700b2":"model=word2vec.Word2Vec(Corpus, window=10, min_count=3)","3e61af70":"model.wv.doesnt_match([\"murder\",\"terrorism\",\"youtube\"])","4de19919":"# WordCloud","00d0a5f5":"# Corpus","87547c30":"### \u666e\u901a\u306e\u30c4\u30a4\u30fc\u30c8","508a5167":"> \u4e8b\u4ef6\u30fb\u707d\u5bb3\u306e\u8a18\u4e8b\u3078\u306e\u30ea\u30f3\u30af\u304c\u7e70\u308a\u8fd4\u3057\u30c4\u30a4\u30fc\u30c8\u3055\u308c\u3066\u3044\u308b\u3084\u3064\n\n* **\"16yr\", \"pkk\"**\n\nhttps:\/\/twitter.com\/charles_lister\/status\/628275514332041216\n\n\n* **\"mh370\", \"debris\":**\n\nhttps:\/\/www.bbc.com\/news\/world-asia-34145127\n\n\n* **\"northern\", \"wildfire\", \"california\"**\n\nhttps:\/\/www.latimes.com\/california\/story\/2020-08-26\/california-fires-burn-more-than-1-600-structures-but-total-losses-could-top-3-000-officials-say\n\n* **\"legionnaires\"**\n\nhttps:\/\/www.bbc.com\/news\/uk-scotland-edinburgh-east-fife-33803309\n\n* **\"projected\", \"refugio\"**\n\nhttps:\/\/www.independent.com\/2015\/08\/05\/refugio-oil-spill-likely-far-larger-than-projected\/\n\n* **\"helicopter\"**\n\nhttps:\/\/www.reuters.com\/article\/us-pakistan-crash\/twelve-feared-killed-in-pakistani-air-ambulance-helicopter-crash-idUSKCN0QB1WN20150807","ba720f2d":"# word2vec","f756b95f":"# [adkkhn_a_note200918](https:\/\/www.kaggle.com\/proribone\/adkkhn-a-note200918)","94342930":"#### [Reference](https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove)","dfd031f4":"### \u707d\u5bb3\u30c4\u30a4\u30fc\u30c8","29978eec":"### \u707d\u5bb3\u30c4\u30a4\u30fc\u30c8","1162ed0a":"# Word Analysis","0978cb95":"# Text Cleaning","1cc1121e":"### \u666e\u901a\u306e\u30c4\u30a4\u30fc\u30c8","6945bffe":"> **\u5404\u5358\u8a9e\u304c\u51fa\u73fe\u3059\u308b\u6587\u7ae0\u306e\u707d\u5bb3\u30c4\u30a4\u30fc\u30c8\u7387**"}}