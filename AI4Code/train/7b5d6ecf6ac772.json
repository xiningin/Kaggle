{"cell_type":{"3a898d7c":"code","6f821ce9":"code","1be915cc":"code","0b221e3c":"code","72d66147":"code","f33e4021":"code","57cf4a34":"code","c00aff18":"code","2944b35a":"code","e866452f":"code","bc536907":"code","428df545":"code","e3781694":"code","d5006bbe":"code","2e97d71b":"code","73aaaefc":"code","e9e9efcb":"code","f633c994":"code","16b8d372":"code","34d8a563":"code","40371bb3":"code","e1cfda58":"code","b3eee510":"code","95876990":"code","0a4e906e":"code","1b407cbc":"code","ca32a3fc":"code","212692ad":"code","0a67379f":"code","ef4029d7":"code","5d01b432":"code","06ae6320":"code","7d617566":"code","d6f96728":"code","fe731c39":"code","e47d7acd":"code","58f0c931":"code","0fb42ea3":"code","d591aeaf":"code","d9919b6a":"markdown","653c0f28":"markdown","c3ab9427":"markdown","66e5be54":"markdown","fad0e734":"markdown","3ff9fced":"markdown","82196db8":"markdown","e0da1884":"markdown","df23288f":"markdown","06408491":"markdown","8cbbd221":"markdown","b864b785":"markdown","1302039f":"markdown","80ca6b22":"markdown","96a4f5e4":"markdown","cc6cb6f8":"markdown","188a78ec":"markdown","9caf28fc":"markdown","956c04b0":"markdown","2acf4ff5":"markdown","3d35e976":"markdown","4d0803b0":"markdown","606c99f1":"markdown","14a87fab":"markdown","a9df472f":"markdown"},"source":{"3a898d7c":"# import required libraries for dataframe and visualization\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime as dt\n\n# import required libraries for clustering\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster.hierarchy import cut_tree","6f821ce9":"# Reading the data on which analysis needs to be done\n\nretail = pd.read_csv('..\/input\/online-retail-customer-clustering\/OnlineRetail.csv', sep=\",\", encoding=\"ISO-8859-1\", header=0)\nretail.head()","1be915cc":"# shape of df\n\nretail.shape","0b221e3c":"# df info\n\nretail.info()","72d66147":"# df description\n\nretail.describe()","f33e4021":"# Calculating the Missing Values % contribution in DF\n\ndf_null = round(100*(retail.isnull().sum())\/len(retail), 2)\ndf_null","57cf4a34":"# Droping rows having missing values\n\nretail = retail.dropna()\nretail.shape","c00aff18":"# Changing the datatype of Customer Id as per Business understanding\n\nretail['CustomerID'] = retail['CustomerID'].astype(str)","2944b35a":"# New Attribute : Monetary\n\nretail['Amount'] = retail['Quantity']*retail['UnitPrice']\nrfm_m = retail.groupby('CustomerID')['Amount'].sum()\nrfm_m = rfm_m.reset_index()\nrfm_m.head()","e866452f":"# New Attribute : Frequency\n\nrfm_f = retail.groupby('CustomerID')['InvoiceNo'].count()\nrfm_f = rfm_f.reset_index()\nrfm_f.columns = ['CustomerID', 'Frequency']\nrfm_f.head()","bc536907":"# Merging the two dfs\n\nrfm = pd.merge(rfm_m, rfm_f, on='CustomerID', how='inner')\nrfm.head()","428df545":"# New Attribute : Recency\n\n# Convert to datetime to proper datatype\n\nretail['InvoiceDate'] = pd.to_datetime(retail['InvoiceDate'],format='%d-%m-%Y %H:%M')","e3781694":"# Compute the maximum date to know the last transaction date\n\nmax_date = max(retail['InvoiceDate'])\nmax_date","d5006bbe":"# Compute the difference between max date and transaction date\n\nretail['Diff'] = max_date - retail['InvoiceDate']\nretail.head()","2e97d71b":"# Compute last transaction date to get the recency of customers\n\nrfm_p = retail.groupby('CustomerID')['Diff'].min()\nrfm_p = rfm_p.reset_index()\nrfm_p.head()","73aaaefc":"# Extract number of days only\n\nrfm_p['Diff'] = rfm_p['Diff'].dt.days\nrfm_p.head()","e9e9efcb":"# Merge tha dataframes to get the final RFM dataframe\n\nrfm = pd.merge(rfm, rfm_p, on='CustomerID', how='inner')\nrfm.columns = ['CustomerID', 'Amount', 'Frequency', 'Recency']\nrfm.head()","f633c994":"# Outlier Analysis of Amount Frequency and Recency\n\nattributes = ['Amount','Frequency','Recency']\nplt.rcParams['figure.figsize'] = [10,8]\nsns.boxplot(data = rfm[attributes], orient=\"v\", palette=\"Set2\" ,whis=1.5,saturation=1, width=0.7)\nplt.title(\"Outliers Variable Distribution\", fontsize = 14, fontweight = 'bold')\nplt.ylabel(\"Range\", fontweight = 'bold')\nplt.xlabel(\"Attributes\", fontweight = 'bold')","16b8d372":"# Removing (statistical) outliers for Amount\nQ1 = rfm.Amount.quantile(0.05)\nQ3 = rfm.Amount.quantile(0.95)\nIQR = Q3 - Q1\nrfm = rfm[(rfm.Amount >= Q1 - 1.5*IQR) & (rfm.Amount <= Q3 + 1.5*IQR)]\n\n# Removing (statistical) outliers for Recency\nQ1 = rfm.Recency.quantile(0.05)\nQ3 = rfm.Recency.quantile(0.95)\nIQR = Q3 - Q1\nrfm = rfm[(rfm.Recency >= Q1 - 1.5*IQR) & (rfm.Recency <= Q3 + 1.5*IQR)]\n\n# Removing (statistical) outliers for Frequency\nQ1 = rfm.Frequency.quantile(0.05)\nQ3 = rfm.Frequency.quantile(0.95)\nIQR = Q3 - Q1\nrfm = rfm[(rfm.Frequency >= Q1 - 1.5*IQR) & (rfm.Frequency <= Q3 + 1.5*IQR)]","34d8a563":"# Rescaling the attributes\n\nrfm_df = rfm[['Amount', 'Frequency', 'Recency']]\n\n# Instantiate\nscaler = StandardScaler()\n\n# fit_transform\nrfm_df_scaled = scaler.fit_transform(rfm_df)\nrfm_df_scaled.shape","40371bb3":"rfm_df_scaled = pd.DataFrame(rfm_df_scaled)\nrfm_df_scaled.columns = ['Amount', 'Frequency', 'Recency']\nrfm_df_scaled.head()","e1cfda58":"# k-means with some arbitrary k\n\nkmeans = KMeans(n_clusters=4, max_iter=50)\nkmeans.fit(rfm_df_scaled)","b3eee510":"kmeans.labels_","95876990":"# Elbow-curve\/SSD\n\nssd = []\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\nfor num_clusters in range_n_clusters:\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(rfm_df_scaled)\n    \n    ssd.append(kmeans.inertia_)\n    \n# plot the SSDs for each n_clusters\nplt.plot(ssd)","0a4e906e":"# Silhouette analysis\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n\nfor num_clusters in range_n_clusters:\n    \n    # intialise kmeans\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(rfm_df_scaled)\n    \n    cluster_labels = kmeans.labels_\n    \n    # silhouette score\n    silhouette_avg = silhouette_score(rfm_df_scaled, cluster_labels)\n    print(\"For n_clusters={0}, the silhouette score is {1}\".format(num_clusters, silhouette_avg))\n    \n    ","1b407cbc":"# Final model with k=3\nkmeans = KMeans(n_clusters=3, max_iter=50)\nkmeans.fit(rfm_df_scaled)","ca32a3fc":" kmeans.labels_","212692ad":"# assign the label\nrfm['Cluster_Id'] = kmeans.labels_\nrfm.head()","0a67379f":"# Box plot to visualize Cluster Id vs Frequency\n\nsns.boxplot(x='Cluster_Id', y='Amount', data=rfm)","ef4029d7":"# Box plot to visualize Cluster Id vs Frequency\n\nsns.boxplot(x='Cluster_Id', y='Frequency', data=rfm)","5d01b432":"# Box plot to visualize Cluster Id vs Recency\n\nsns.boxplot(x='Cluster_Id', y='Recency', data=rfm)","06ae6320":"# Single linkage: \n\nmergings = linkage(rfm_df_scaled, method=\"single\", metric='euclidean')\ndendrogram(mergings)\nplt.show()","7d617566":"# Complete linkage\n\nmergings = linkage(rfm_df_scaled, method=\"complete\", metric='euclidean')\ndendrogram(mergings)\nplt.show()","d6f96728":"# Average linkage\n\nmergings = linkage(rfm_df_scaled, method=\"average\", metric='euclidean')\ndendrogram(mergings)\nplt.show()","fe731c39":"# 3 clusters\ncluster_labels = cut_tree(mergings, n_clusters=3).reshape(-1, )\ncluster_labels","e47d7acd":"# Assign cluster labels\n\nrfm['Cluster_Labels'] = cluster_labels\nrfm.head()","58f0c931":"# Plot Cluster Id vs Amount\n\nsns.boxplot(x='Cluster_Labels', y='Amount', data=rfm)","0fb42ea3":"# Plot Cluster Id vs Frequency\n\nsns.boxplot(x='Cluster_Labels', y='Frequency', data=rfm)","d591aeaf":"# Plot Cluster Id vs Recency\n\nsns.boxplot(x='Cluster_Labels', y='Recency', data=rfm)","d9919b6a":"<a id=\"2\"><\/a> <br>\n## Step 2 : Data Cleansing","653c0f28":"#### If this Kernel helped you in any way, some <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated","c3ab9427":"### Silhouette Analysis\n\n$$\\text{silhouette score}=\\frac{p-q}{max(p,q)}$$\n\n$p$ is the mean distance to the points in the nearest cluster that the data point is not a part of\n\n$q$ is the mean intra-cluster distance to all the points in its own cluster.\n\n* The value of the silhouette score range lies between -1 to 1. \n\n* A score closer to 1 indicates that the data point is very similar to other data points in the cluster, \n\n* A score closer to -1 indicates that the data point is not similar to the data points in its cluster.","66e5be54":"### K-Means Clustering","fad0e734":"## Overview\n<a href=\"https:\/\/archive.ics.uci.edu\/ml\/datasets\/online+retail\">Online retail is a transnational data set<\/a> which contains all the transactions occurring between 01\/12\/2010 and 09\/12\/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n\n## Business Goal\nWe aim to segement the Customers based on RFM so that the company can target its customers efficiently.\n\n#### The steps are broadly divided into:\n\n1. [Step 1: Reading and Understanding the Data](#1)\n1. [Step 2: Data Cleansing](#2)\n1. [Step 3: Data Preparation](#3)\n1. [Step 4: Model Building](#4)\n1. [Step 5: Final Analysis](#5)","3ff9fced":"<a id=\"5\"><\/a> <br>\n## Step 5 : Final Analysis","82196db8":"### Finding the Optimal Number of Clusters","e0da1884":"#### This kernel is based on the assignment by IIITB collaborated with upgrad","df23288f":"Hierarchical Clustering with 3 Cluster Labels\n- Customers with Cluster_Labels 2 are the customers with high amount of transactions as compared to other customers.\n- Customers with Cluster_Labels 2 are frequent buyers.\n- Customers with Cluster_Labels 0 are not recent buyers and hence least of importance from business point of view.","06408491":"<a id=\"4\"><\/a> <br>\n## Step 4 : Building the Model","8cbbd221":"![](https:\/\/www.finplus.co.in\/wp-content\/uploads\/2017\/10\/Top-Categories-In-Online-Retail-In-India.jpg)","b864b785":"### Rescaling the Attributes\n\nIt is extremely important to rescale the variables so that they have a comparable scale.|\nThere are two common ways of rescaling:\n\n1. Min-Max scaling \n2. Standardisation (mean-0, sigma-1) \n\nHere, we will use Standardisation Scaling.","1302039f":"### If this Kernel helped you in any way, some <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated","80ca6b22":"#### There are 2 types of outliers and we will treat outliers as it can skew our dataset\n- Statistical\n- Domain specific","96a4f5e4":"K-means clustering is one of the simplest and popular unsupervised machine learning algorithms.<br>\n\nThe algorithm works as follows:\n\n- First we initialize k points, called means, randomly.\n- We categorize each item to its closest mean and we update the mean\u2019s coordinates, which are the averages of the items categorized in that mean so far.\n- We repeat the process for a given number of iterations and at the end, we have our clusters.","cc6cb6f8":"**Average Linkage:<br>**\n\nIn average linkage hierarchical clustering, the distance between two clusters is defined as the average distance between each point in one cluster to every point in the other cluster. For example, the distance between clusters \u201cr\u201d and \u201cs\u201d to the left is equal to the average length each arrow between connecting the points of one cluster to the other.\n![](https:\/\/www.saedsayad.com\/images\/Clustering_average.png)","188a78ec":"<a id=\"3\"><\/a> <br>\n## Step 3 : Data Preparation","9caf28fc":"#### Elbow Curve to get the right number of Clusters\nA fundamental step for any unsupervised algorithm is to determine the optimal number of clusters into which the data may be clustered. The Elbow Method is one of the most popular methods to determine this optimal value of k.","956c04b0":"### Hierarchical Clustering\n\nHierarchical clustering involves creating clusters that have a predetermined ordering from top to bottom. For example, all files and folders on the hard disk are organized in a hierarchy. There are two types of hierarchical clustering, \n- Divisive \n- Agglomerative.","2acf4ff5":"<a id=\"1\"><\/a> <br>\n## Step 1 : Reading and Understanding Data","3d35e976":"### Inference:\nK-Means Clustering with 3 Cluster Ids\n- Customers with Cluster Id 1 are the customers with high amount of transactions as compared to other customers.\n- Customers with Cluster Id 1 are frequent buyers.\n- Customers with Cluster Id 2 are not recent buyers and hence least of importance from business point of view.","4d0803b0":"#### We are going to analysis the Customers based on below 3 factors:\n- R (Recency): Number of days since last purchase\n- F (Frequency): Number of tracsactions\n- M (Monetary): Total amount of transactions (revenue contributed)","606c99f1":"**Single Linkage:<br>**\n\nIn single linkage hierarchical clustering, the distance between two clusters is defined as the shortest distance between two points in each cluster. For example, the distance between clusters \u201cr\u201d and \u201cs\u201d to the left is equal to the length of the arrow between their two closest points.\n![](https:\/\/www.saedsayad.com\/images\/Clustering_single.png)","14a87fab":"**Complete Linkage<br>**\n\nIn complete linkage hierarchical clustering, the distance between two clusters is defined as the longest distance between two points in each cluster. For example, the distance between clusters \u201cr\u201d and \u201cs\u201d to the left is equal to the length of the arrow between their two furthest points. \n![](https:\/\/www.saedsayad.com\/images\/Clustering_complete.png)","a9df472f":"#### Cutting the Dendrogram based on K"}}