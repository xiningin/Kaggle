{"cell_type":{"b03868ba":"code","93b1fb66":"code","7e3e2d48":"code","77be4625":"markdown","7e279ca2":"markdown"},"source":{"b03868ba":"%%writefile Spinoza.py\n\n# = * = * = * = * = * = * \n\nmax_limit = 23  # can be modified\nadd_rotations = True\n\n# number of predictors\nnumPre = 6\nif add_rotations:\n\tnumPre *= 3\n\n# number of meta-predictors\nnumMeta = 4\nif add_rotations:\n\tnumMeta *= 3\n\n# saves history\nmoves = ['', '', '']\n\nbeat = {'R':'P', 'P':'S', 'S':'R'}\ndna =  {'RP':0, 'PS':1, 'SR':2,\n\t\t'PR':3, 'SP':4, 'RS':5,\n\t\t'RR':6, 'PP':7, 'SS':8}\n\np = [\"P\"]*numPre\nm = [\"P\"]*numMeta\npScore = [[0]*numPre for i in range(8)]\nmScore = [0]*numMeta\n\nlength = 0\nthreat = 0\noutput = \"P\"\n\n\ndef myagent(observation, configuration):    \n\tglobal max_limit, add_rotations, \\\n\t\tnumPre, numMeta, moves, beat, dna, \\\n\t\tp, m, pScore, mScore, length, threat, output\n\n\tif observation.step < 2:\n\t\toutput = beat[output]\n\t\treturn {'R':0, 'P':1, 'S':2}[output]\n\n\t# - - - -\n\n\tinput = \"RPS\"[observation.lastOpponentAction]\n\n\t# threat of opponent\n\toutcome = (beat[input]==output) - (input==beat[output])\n\tthreat = 0.9*threat - 0.1*outcome\n\t\n\t# refresh pScore\n\tfor i in range(numPre):\n\t\tpp = p[i]\n\t\tbpp = beat[pp]\n\t\tbbpp = beat[bpp]\n\t\tpScore[0][i] = 0.9*pScore[0][i] + 0.1*((input==pp)-(input==bbpp))\n\t\tpScore[1][i] = 0.9*pScore[1][i] + 0.1*((output==pp)-(output==bbpp))\n\t\tpScore[2][i] = 0.8*pScore[2][i] + 0.3*((input==pp)-(input==bbpp)) + \\\n\t\t\t\t\t\t0.1*(length % 3 - 1)\n\t\tpScore[3][i] = 0.8*pScore[3][i] + 0.3*((output==pp)-(output==bbpp)) + \\\n\t\t\t\t\t\t0.1*(length % 3 - 1)\n\n\t# refresh mScore\n\tfor i in range(numMeta):\n\t\tmScore[i] = 0.9*mScore[i] + 0.1*((input==m[i])-(input==beat[beat[m[i]]])) + \\\n\t\t\t\t\t0.05*(length % 5 - 2)\n\n\t# refresh moves\n\tmoves[0] += str(dna[input+output])\n\tmoves[1] += input\n\tmoves[2] += output\n\n\t# refresh length\n\tlength += 1\n\n\t# new predictors\n\tlimit = min([length,max_limit])\n\tfor y in range(3):\t# my moves, his, and both\n\t\tj = limit\n\t\twhile j>=1 and not moves[y][length-j:length] in moves[y][0:length-1]:\n\t\t\tj-=1\n\t\tif j>=1:\n\t\t\ti = moves[y].rfind(moves[y][length-j:length],0,length-1)\n\t\t\tp[0+2*y] = moves[1][j+i] \n\t\t\tp[1+2*y] = beat[moves[2][j+i]]\n\n\t# rotations of predictors\n\tif add_rotations:\n\t\tfor i in range(int(numPre\/3),numPre):\n\t\t\tp[i]=beat[beat[p[i-int(numPre\/3)]]]\n\n\t# new meta\n\tfor i in range(0,4,2):\n\t\tm[i] = p[pScore[i].index(max(pScore[i]))]\n\t\tm[i+1] = beat[p[pScore[i+1].index(max(pScore[i+1]))]]\n\n\t# rotations of meta\n\tif add_rotations:\n\t\tfor i in range(4,12):\n\t\t\tm[i]=beat[beat[m[i-4]]]\n\t\n\t# - - -\n    \n\toutput = beat[m[mScore.index(max(mScore))]]\n\n\tif threat > 0.4:\n\t\t# ah take this!\n\t\toutput = beat[beat[output]]\n\n\treturn {'R':0, 'P':1, 'S':2}[output]","93b1fb66":"import os\nimport pandas as pd\nimport contextlib\nwith contextlib.redirect_stdout(None):\n    import kaggle_environments\nfrom datetime import datetime\nimport multiprocessing as pymp\nfrom tqdm import tqdm\nimport ray.util.multiprocessing as raymp\n\n\n# function to return score\ndef get_result(match_settings):\n    start = datetime.now()\n    outcomes = kaggle_environments.evaluate(\n        'rps', [match_settings[0], match_settings[1]], num_episodes=match_settings[2])\n    won, lost, tie, avg_score = 0, 0, 0, 0.\n    for outcome in outcomes:\n        score = outcome[0]\n        if score > 0: won += 1\n        elif score < 0: lost += 1\n        else: tie += 1\n        avg_score += score\n    elapsed = datetime.now() - start\n    return match_settings[1], won, lost, tie, elapsed, float(avg_score) \/ float(match_settings[2])\n\n\ndef eval_agent_against_baselines(agent, baselines, num_episodes=10, use_ray=False):\n    df = pd.DataFrame(\n        columns=['wins', 'loses', 'ties', 'total_time', 'avg_score'],\n        index=baselines + [\"summary\"]\n    )\n    \n    if use_ray:\n        pool = raymp.Pool()\n    else:\n        pool = pymp.Pool()\n    matches = [[agent, baseline, num_episodes] for baseline in baselines]\n    \n    results = []\n    for content in tqdm(pool.imap_unordered(get_result, matches), total=len(matches)):\n        results.append(content)\n    \n    for baseline_agent, won, lost, tie, elapsed, avg_score in results:\n        df.loc[baseline_agent, 'wins'] = won\n        df.loc[baseline_agent, 'loses'] = lost\n        df.loc[baseline_agent, 'ties'] = tie\n        df.loc[baseline_agent, 'total_time'] = elapsed\n        df.loc[baseline_agent, 'avg_score'] = avg_score\n    \n    # add summary\n    df.loc[\"summary\",\"wins\"] = df[\"wins\"].mean()\n    df.loc[\"summary\",\"loses\"] = df[\"loses\"].mean()\n    df.loc[\"summary\",\"ties\"] = df[\"ties\"].mean()\n    df.loc[\"summary\",\"avg_score\"] = df[\"avg_score\"].mean()\n        \n    return df\n","7e3e2d48":"blue_belt_agents = [os.path.join('..\/input\/some-agents\/blue_belt', agent)\n    for agent in os.listdir('..\/input\/some-agents\/blue_belt')]\nblack_belt_agents = [os.path.join('..\/input\/some-agents\/black_belt', agent)\n    for agent in os.listdir('..\/input\/some-agents\/black_belt')]\n\n# too long to simulate\nblack_belt_agents.remove(\"..\/input\/some-agents\/black_belt\/multi_armed_bandit_v15.py\")\nblack_belt_agents.remove(\"..\/input\/some-agents\/black_belt\/multi_armed_bandit_v32.py\")\n\nmy_agent = \".\/Spinoza.py\"\n\neval_agent_against_baselines(my_agent, black_belt_agents, num_episodes = 10)","77be4625":"Simple agent based on rfind (looks for history patterns). Good for starting point, and definitively improvable.\n\nFeatures:\n* Completely deterministic;\n* Multiple predictors: my history, his history and both;\n* Multiple meta-strategies;\n* Threat: if opponent is having a good streak, start playing around him.\n\nAgent inspired from http:\/\/www.rpscontest.com\/entry\/498002.","7e279ca2":"Let's run a basic test agaist good opponents (thanks to the notebook https:\/\/www.kaggle.com\/chankhavu\/rps-dojo)"}}