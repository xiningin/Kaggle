{"cell_type":{"27abcfb6":"code","72ba0df5":"code","61a9ac22":"code","a8f4b172":"code","b80086b7":"code","cb5ac0e3":"code","339ae2f9":"code","84f65a2b":"code","78f62cd5":"code","31f96715":"code","f00812df":"code","7d6b2c6a":"code","849f2a0b":"code","cc7bf5e9":"code","5c6d4e03":"code","eddc80e9":"markdown","af02df7a":"markdown","53d5362e":"markdown","7cf8beca":"markdown","2fdca00c":"markdown","f0fa6464":"markdown"},"source":{"27abcfb6":"import tensorflow as tf\nprint(tf.__version__)\nimport torch\nprint(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")\n\nimport os\nimport gc\nimport cv2\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom shutil import copyfile\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut","72ba0df5":"gpus = tf.config.list_physical_devices('GPU')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)","61a9ac22":"# Read the submisison file\nsub_df = pd.read_csv('\/kaggle\/input\/siim-covid19-detection\/sample_submission.csv')\nprint(len(sub_df))\nsub_df","a8f4b172":"study_df = sub_df.loc[sub_df.id.str.contains('_study')]\nlen(study_df)","b80086b7":"image_df = sub_df.loc[sub_df.id.str.contains('_image')]\nlen(image_df)","cb5ac0e3":"\ndef read_xray(path, voi_lut = True, fix_monochrome = True):\n    # Original from: https:\/\/www.kaggle.com\/raddar\/convert-dicom-to-np-array-the-correct-way\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \n    # \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n        \n    return data\n\ndef resize_xray(array, size, keep_ratio=False, resample=Image.LANCZOS):\n    # Original from: https:\/\/www.kaggle.com\/xhlulu\/vinbigdata-process-and-resize-to-image\n    im = Image.fromarray(array)\n    \n    if keep_ratio:\n        im.thumbnail((size, size), resample)\n    else:\n        im = im.resize((size, size), resample)\n    \n    return im","339ae2f9":"TEST_PATH = f'\/kaggle\/tmp\/test\/'\nIMG_SIZE = 1024\n\ndef prepare_test_images():\n    image_id = []\n    dim0 = []\n    dim1 = []\n\n    os.makedirs(TEST_PATH, exist_ok=True)\n\n    for dirname, _, filenames in tqdm(os.walk(f'..\/input\/siim-covid19-detection\/test')):\n        for file in filenames:\n            # set keep_ratio=True to have original aspect ratio\n            xray = read_xray(os.path.join(dirname, file))\n            im = resize_xray(xray, size=IMG_SIZE)  \n            im.save(os.path.join(TEST_PATH, file.replace('dcm', 'png')))\n\n            image_id.append(file.replace('.dcm', ''))\n            dim0.append(xray.shape[0])\n            dim1.append(xray.shape[1])\n            \n    return image_id, dim0, dim1","84f65a2b":"YOLO_MODEL_PATH = '..\/input\/happyhappyyolov5xtrain\/best.pt'\nTEST_PATH = '..\/input\/siim-covide-1024-resized-happyhappy\/test'\n!python ..\/input\/kaggle-yolov5\/detect.py --weights {YOLO_MODEL_PATH} \\\n                                      --source {TEST_PATH} \\\n                                      --img {IMG_SIZE} \\\n                                      --conf 0.16  \\\n                                      --iou-thres 0.5 \\\n                                      --max-det 10 \\\n                                      --save-txt \\\n                                      --save-conf","78f62cd5":"PRED_PATH = 'runs\/detect\/exp2\/labels'\nprediction_files = os.listdir(PRED_PATH)\nprint(f'Number of opacity predicted by YOLOv5: {len(prediction_files)}')","31f96715":"\n# The submisison requires xmin, ymin, xmax, ymax format. \n# YOLOv5 returns x_center, y_center, width, height\ndef correct_bbox_format(bboxes):\n    correct_bboxes = []\n    for b in bboxes:\n        xc, yc = int(np.round(b[0]*IMG_SIZE)), int(np.round(b[1]*IMG_SIZE))\n        w, h = int(np.round(b[2]*IMG_SIZE)), int(np.round(b[3]*IMG_SIZE))\n\n        xmin = xc - int(np.round(w\/2))\n        xmax = xc + int(np.round(w\/2))\n        ymin = yc - int(np.round(h\/2))\n        ymax = yc + int(np.round(h\/2))\n        \n        correct_bboxes.append([xmin, xmax, ymin, ymax])\n        \n    return correct_bboxes\n\n# Read the txt file generated by YOLOv5 during inference and extract \n# confidence and bounding box coordinates.\ndef get_conf_bboxes(file_path):\n    confidence = []\n    bboxes = []\n    with open(file_path, 'r') as file:\n        for line in file:\n            preds = line.strip('\\n').split(' ')\n            preds = list(map(float, preds))\n            confidence.append(preds[-1])\n            bboxes.append(preds[1:-1])\n    return confidence, bboxes\n\n# Read the submisison file\nsub_df = pd.read_csv('..\/input\/siim-covid19-detection\/sample_submission.csv')\nsub_df.tail()\n\n# Prediction loop for submission\npredictions = []\n\nfor i in tqdm(range(len(sub_df))):\n    row = sub_df.loc[i]\n    id_name = row.id.split('_')[0]\n    id_level = row.id.split('_')[-1]\n    \n    if id_level == 'study':\n        # do study-level classification\n        predictions.append(\"Negative 1 0 0 1 1\") # dummy prediction\n        \n    elif id_level == 'image':\n        # we can do image-level classification here.\n        # also we can rely on the object detector's classification head.\n        # for this example submisison we will use YOLO's classification head. \n        # since we already ran the inference we know which test images belong to opacity.\n        if f'{id_name}.txt' in prediction_files:\n            # opacity label\n            confidence, bboxes = get_conf_bboxes(f'{PRED_PATH}\/{id_name}.txt')\n            bboxes = correct_bbox_format(bboxes)\n            pred_string = ''\n            for j, conf in enumerate(confidence):\n                pred_string += f'opacity {conf} ' + ' '.join(map(str, bboxes[j])) + ' '\n            predictions.append(pred_string[:-1]) \n        else:\n            predictions.append(\"None 1 0 0 1 1\")\n\n\n\nsub_df['PredictionString'] = predictions","f00812df":"sub_df","7d6b2c6a":"!rm -rf .\/runs\/detect","849f2a0b":"# study_df = pd.read_csv('..\/input\/siimhappyhappysubmission\/study_submission.csv')\n# study_df = study_df[study_df.apply(lambda x : x['id'].endswith(\"study\"),axis=1)]","cc7bf5e9":"# # image_df = pd.read_csv('image_submission.csv')\n# image_df = image_df[image_df.apply(lambda x : x['id'].endswith(\"image\"),axis=1)]","5c6d4e03":"# sub_df = pd.concat([study_df, image_df])\nsub_df.to_csv('.\/submission.csv', index=False)\nsub_df","eddc80e9":"# YOLOv5 Inferencecontains","af02df7a":"# Install Dependencies","53d5362e":"# Prepare Image Level Test Images","7cf8beca":"# Imports","2fdca00c":"# Load Submission ","f0fa6464":"# Utils to extract images and resize them"}}