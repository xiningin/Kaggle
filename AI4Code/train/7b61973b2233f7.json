{"cell_type":{"1089239d":"code","cb8b2959":"code","4bfc2e1a":"code","f19b17f6":"code","bc485b38":"code","533f799f":"code","6b447839":"code","ebd76586":"code","74be2a9c":"code","254fcadc":"code","8a8ca603":"code","17dd5133":"code","c6ed572a":"code","2d1b8af3":"code","f5becdc9":"code","bb97e4f3":"code","c67ff01a":"code","3978a3b9":"code","8976d9a8":"code","729129b6":"code","ab4fb31e":"code","e2af3ca2":"code","60cb42ab":"code","6a90d1be":"code","a43c46da":"code","724e89d0":"code","3845ea25":"code","e24a7de3":"code","e93073af":"code","3233c4f6":"code","ee74239f":"code","c7ad4d08":"code","4b6eb6a1":"code","22ed1027":"code","704fc0b3":"code","648d8071":"code","3843f28a":"code","2a8c2f2a":"code","269f8755":"code","7bab8a21":"code","047b04f2":"code","3695096f":"code","29957946":"code","96e09ba7":"code","868b0ae4":"code","b2ed2b00":"code","a6a7f715":"code","d4545285":"code","df31c232":"code","9ac77b98":"code","18ecdc48":"code","6f13e411":"code","eface6f5":"code","516e90ea":"code","d44bb5d8":"code","245843ff":"code","9e662517":"code","d13503e2":"code","f3e8f524":"code","a767d451":"code","b54182be":"code","eed285df":"code","260e7fdd":"code","0303c382":"code","d53b53ba":"code","88564a67":"code","69a8fe11":"code","c3faadb7":"code","6ed3a7b5":"code","493b8e64":"code","95fc02f0":"code","2d4c8b1f":"code","83296379":"code","9c2469bd":"code","cc74bb62":"code","b9dbd01c":"code","11c3b8ef":"code","bd1260df":"code","ac9bf3d0":"code","9ffe118e":"code","0db6438d":"code","450ef804":"code","15101bda":"code","8aab12f5":"code","809056a2":"code","a4dbd8f6":"code","aace2dfe":"code","67e9e12c":"code","bd2f788a":"code","c8e9c5c8":"code","7591bebb":"code","bcb1d271":"code","15b985c0":"code","62253579":"code","afda44bb":"code","ca369bd6":"markdown","af98f70c":"markdown","f19548c7":"markdown","de084bd9":"markdown","1adb0a21":"markdown","c56ab9b6":"markdown","109b800d":"markdown","5154884e":"markdown","37604beb":"markdown","895276ad":"markdown","ee3223a9":"markdown","5cf05f17":"markdown","4494fe2a":"markdown","6706a31d":"markdown","7ca66be4":"markdown","a00db98f":"markdown","5d7d7795":"markdown","146d4f0c":"markdown","9b07383d":"markdown","6e94702f":"markdown","0302740d":"markdown","2a63f35a":"markdown","15ef2fcb":"markdown","9963ade1":"markdown","03fabf5b":"markdown","347613fd":"markdown","6c58c16b":"markdown","8b7a8d87":"markdown","39db0b40":"markdown","dc5823fc":"markdown","fa730e17":"markdown","d9c42d23":"markdown","93f8c225":"markdown","280ab47c":"markdown","bf4eb283":"markdown","26e24b3b":"markdown","9dec0bd0":"markdown","1e656d1f":"markdown","8b26aebe":"markdown","b2dcbad3":"markdown","326c973e":"markdown","33624a26":"markdown","61e9bff8":"markdown","edd71646":"markdown","277cd4b3":"markdown","0786ac1f":"markdown","98c31939":"markdown","7e26e70c":"markdown","4f67c6df":"markdown","7d50c886":"markdown","b83a91bb":"markdown","0e494e76":"markdown","f138d9ce":"markdown","f44af46f":"markdown","ecfe922f":"markdown","6ce4cb4a":"markdown","0a1c27c9":"markdown","0d4bf3fd":"markdown","dc8af052":"markdown","42222afe":"markdown","d0d8f031":"markdown","32a8c865":"markdown","1a07c30e":"markdown","f0e68894":"markdown","9b7427f3":"markdown","9ebd8793":"markdown","253fa342":"markdown","57d0bc29":"markdown","cb4bfd2d":"markdown","57496fb1":"markdown","f6c64e21":"markdown"},"source":{"1089239d":"# Importing the required libraries\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontProperties\n\n# Importing the sklearn modules required\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.exceptions import ConvergenceWarning\nimport joblib\n\n# Importing additional models\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostRegressor, CatBoostClassifier, Pool\nimport shap\n\n# Importing optuna for model tuning\nimport optuna\nfrom optuna.samplers import TPESampler\n\n# To see optuna progress we can comment these rows:\n#import warnings\n#optuna.logging.set_verbosity(optuna.logging.WARNING)\n#warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n\n# Importing the ensemble builder\nfrom mlens.ensemble import SuperLearner\n\n# Setting the styles\nsns.set_theme('notebook')\nsns.set_style('darkgrid')\nsns.set_palette('bright')\n%matplotlib inline","cb8b2959":"# Loading the data\ndf = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","4bfc2e1a":"df.head(5)","f19b17f6":"def graph(column, ax, continuous=True):\n    \"\"\"\n    Short function build a chart to understand the data better\n    \"\"\"\n    if continuous:\n        sns.histplot(x=df[column], ax=ax)\n    else:\n        sns.countplot(x=df[column], ax=ax)\n    return fig\n\nfig,axs = plt.subplots(ncols=4, nrows=2, figsize=(24,12))\nax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8 = axs[0,0], axs[0,1], axs[0,2], axs[0,3], axs[1,0], axs[1,1], axs[1,2], axs[1,3]\ngraph('Survived',ax1,continuous=False), ax1.set_title('Survival count', fontsize=12, fontweight='bold')\ngraph('Pclass',ax2,continuous=False), ax2.set_title('Passenger class', fontsize=12, fontweight='bold')\ngraph('Sex',ax3,continuous=False), ax3.set_title('Passenger sex', fontsize=12, fontweight='bold')\ngraph('Age',ax4,continuous=True), ax4.set_title('Passenger age', fontsize=12, fontweight='bold')\ngraph('SibSp',ax5,continuous=False), ax5.set_title('# siblings\/spouse', fontsize=12, fontweight='bold')\ngraph('Parch',ax6,continuous=False), ax6.set_title('# children\/parents', fontsize=12, fontweight='bold')\ngraph('Fare',ax7,continuous=True), ax7.set_title('Fare paid', fontsize=12, fontweight='bold')\ngraph('Embarked',ax8,continuous=False), ax8.set_title('Embarked', fontsize=12, fontweight='bold');","bc485b38":"# Defining a function to add values to my bar graphs:\n\ndef add_value_labels(ax, fontsize=12, label_format=\"{:.0f}\", spacing=1):\n    \"\"\"\n    Functions to add labels to the end of each bar in a bar chart.\n    - ax (matplotlib.axes.Axes): The matplotlib object containing the axes of the plot to annotate.\n    - spacing (int): The distance between the labels and the bars\n    \"\"\"\n\n    # For each bar: Place a label\n    for rect in ax.patches:\n        # Get X and Y placement of label from rect.\n        y_value = rect.get_height()\n        x_value = rect.get_x() + rect.get_width() \/ 2\n\n        # Number of points between bar and label:\n        space = spacing\n        # Vertical alignment for positive values:\n        va = 'bottom'\n\n        # If value of bar is negative, place label below bar\n        if y_value < 0:\n            # Invert space to place label below\n            space *= -1\n            # Vertically align label at top\n            va = 'top'\n\n        # Use Y value as label and format number with 2 decimal places\n        label = label_format.format(y_value)\n\n        # Create annotation\n        ax.annotate(\n            label,                          # Use `label` as label\n            (x_value, y_value),             # Place label at end of the bar\n            xytext=(0, space),              # Vertically shift label by `space`\n            textcoords=\"offset points\",     # Interpret `xytext` as offset in points\n            fontsize=fontsize,              # Font size\n            ha='center',                    # Horizontally center label\n            va=va)                          # Vertically align label","533f799f":"def graph_overview(column, title, continuous=True):\n    \"\"\"\n    Short function to return a count and distribution plot of the metric. \n    - column (string) is the column name of the metric to be analysed\n    - title (string) title of the graph\n    - continuous (bool) whether the variable is continuous\n    - returns two charts \n    \"\"\"\n    \n    # Format the fonts\n    font = FontProperties(size=12)\n    font.set_style('italic')\n    \n    # Set the figure up\n    fig, (ax1,ax2) = plt.subplots(ncols=2, nrows=1, figsize=(18,4))\n    \n    if continuous:\n        # Histogram\n        graph1 = sns.histplot(x=df[column], ax=ax1)\n        ax1.set_title(title+' histogram\\n', fontsize=14, fontweight='bold')\n        # Boxplot\n        graph2 = sns.boxplot(data=df, x=column, y='Survived', ax=ax2, orient='h')\n        ax2.set_title(title+' by survival rate\\n', fontsize=14, fontweight='bold')\n        ax2.set_ylabel('Survival status', fontproperties=font)\n    \n    else:\n        # Countplot\n        graph2 = sns.countplot(x=df[column], ax=ax1)\n        ax1.set_title(title+' count\\n', fontsize=14, fontweight='bold')\n        add_value_labels(ax1, spacing=2)  # Adds the values above the bars\n        \n        #\u00a0Bar chart\n        survival_rate = df.groupby(column)['Survived'].value_counts(normalize=True).loc[:,1]\n        graph2 = sns.barplot(y=survival_rate.values, x=survival_rate.index, ax=ax2)\n        ax2.set_title(title+\" by survival rate\\n\", fontsize=14, fontweight='bold')\n        ax2.set_ylabel('Survival rate', fontproperties=font)\n        add_value_labels(ax2, label_format=\"{:.2f}\")  # Adds the values above the bars\n    \n    ax1.set_ylabel('Count', fontproperties=font)\n    ax1.set_xlabel(title, fontproperties=font)\n    ax2.set_xlabel(title, fontproperties=font)","6b447839":"graph_overview('Pclass', 'Passenger class', continuous=False)","ebd76586":"graph_overview('Sex', 'Passenger sex', continuous=False)","74be2a9c":"graph_overview('Age', 'Passenger age', continuous=True)","254fcadc":"graph_overview('SibSp', \"# siblings\/spouse onboard\", continuous=False)","8a8ca603":"graph_overview('Parch', '# children\/parents onboard', continuous=False)","17dd5133":"graph_overview('Fare', \"Fare paid\", continuous=True)","c6ed572a":"graph_overview('Embarked', \"Port of embarkation\", continuous=False)","2d1b8af3":"# Return the number of missing data by column\ndf.isna().sum(axis = 0)","f5becdc9":"df[df['Embarked'].isna()].head()","bb97e4f3":"df.Embarked.fillna('S', inplace=True)","c67ff01a":"# The name information may have some useful data contained within\ndf['Title'] = df.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())","3978a3b9":"def draw_graph(title, x_data, x_label, y_label, x_rotation=0, sorted=False):\n    # Let's graph the titles\n    fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(12,4))\n    ax.set_title(title+'\\n', fontsize=14, fontweight='bold')\n\n    # Format the fonts:\n    font = FontProperties(size=14)\n    font.set_style('italic')\n    plt.xticks(rotation=x_rotation, fontsize=12)\n    plt.yticks(fontsize=12)\n\n    # Setting up the plot\n    if sorted: sns.countplot(x=x_data, data=df, order = df[x_data].value_counts().index)\n    else: sns.countplot(x=x_data, data=df)\n    ax.set_ylabel(y_label, fontproperties=font)\n    ax.set_xlabel(x_label, fontproperties=font);\n    add_value_labels(ax, fontsize=12);","8976d9a8":"draw_graph('Passenger titles', x_rotation=45, x_data='Title', x_label='Passenger title', y_label='Count', sorted=True)","729129b6":"titles = ['Mr', 'Mrs', 'Miss', 'Master', 'Other']\ndf.loc[~df.Title.isin(titles), \"Title\"] = \"Other\"","ab4fb31e":"draw_graph('Passenger titles', x_data='Title', x_label='Passenger title', y_label='Count', sorted=True)","e2af3ca2":"df.drop('Name', axis=1, inplace=True)","60cb42ab":"graph_overview('Title', \"Passenger title\", continuous=False)","6a90d1be":"df['Cabin_let'] = df['Cabin'].str[0]\ndf['Cabin_let'].value_counts(dropna=False)","a43c46da":"# Fill the unknowns with 'Unknown'\ndf.Cabin_let.fillna(value='Unknown', inplace=True)\n\n# Now we can drop the old 'Cabin' column and change the name of the 'Cabin_let' column\ndf.drop('Cabin', axis=1, inplace=True)\ndf.rename(columns={'Cabin_let': 'Cabin'}, inplace=True)\n\n# Change the tail of the cabin letters to 'Other'\nlet_to_replace = ['F','G','T']\nreplace_with = 'Other'\n\nfor i in let_to_replace:\n    df.Cabin.loc[df['Cabin']==i] = replace_with\n\n# Checking the output:\ndraw_graph('Passenger cabin', x_data='Cabin', x_label='Passenger cabin', y_label='Count', sorted=True)","724e89d0":"graph_overview('Cabin', 'Passenger cabin', continuous=False)","3845ea25":"# Let's create a 'family' column to show who had children\/parents\/spouse onboard\ndf['Family'] = df.SibSp + df.Parch\n# ...and kill the original columns\ndf.drop('SibSp', axis=1, inplace=True)\ndf.drop('Parch', axis=1, inplace=True)\n# ...and look at the distribution\ngraph_overview('Family', '# family members onboard', continuous=False)","e24a7de3":"# Creating the 'None' bucket\ndf.Family.loc[df['Family']==0] = 'None'\n\n# Creating the 'Small_family' bucket\nsmall_num_to_replace = [1,2,3]\nsmall_fam = 'Small_family'\nfor i in small_num_to_replace:\n    df.Family.loc[df['Family']==i] = small_fam\n    \n# Creating the 'Large_family' bucket\nlarge_num_to_replace = [4,5,6,7,10]\nlarge_fam = 'Large_family'\nfor i in large_num_to_replace:\n    df.Family.loc[df['Family']==i] = large_fam\n\n# Checking output\ngraph_overview('Family', '# family members onboard', continuous=False)","e93073af":"# Exploring the fare distribution \nfig,ax = plt.subplots(ncols=1, nrows=1, figsize=(8,4))\nfont = FontProperties(size=12)\nfont.set_style('italic')\nax.set_ylabel('Count', fontproperties=font)\nax.set_xlabel('Fare', fontproperties=font);\nsns.histplot(df[\"Fare\"], ax=ax, label=\"Skewness : %.2f\"%(df[\"Fare\"].skew()))\nplt.legend(loc=\"best\");","3233c4f6":"# Taking the log of the fare to reduce skewness:\ndf[\"Fare\"] = df[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)","ee74239f":"# Checking that skewness has reduced:\nfig,ax = plt.subplots(ncols=1, nrows=1, figsize=(8,4))\nfont = FontProperties(size=12)\nfont.set_style('italic')\nax.set_ylabel('Count', fontproperties=font)\nax.set_xlabel('Fare', fontproperties=font);\nsns.histplot(df[\"Fare\"], ax=ax, label=\"Skewness : %.2f\"%(df[\"Fare\"].skew()))\nplt.legend(loc=\"best\");","c7ad4d08":"df.Ticket.describe()","4b6eb6a1":"df.drop('Ticket', axis=1, inplace=True)","22ed1027":"df.PassengerId.value_counts()","704fc0b3":"df.drop('PassengerId', axis=1, inplace=True)","648d8071":"# Let's split the dataframe based on whether the passenger's age is available\ndf_test = df.copy().loc[df['Age'].isin([np.nan])]\ndf_train_val = df.copy().loc[~df['Age'].isin([np.nan])]","3843f28a":"# Now we need to define our independent variables\nage_var = ['Pclass', 'Sex', 'Fare', 'Embarked', 'Title', 'Family', 'Cabin']\n# And label the catgeorical features for Catboost\ncat_features = ['Sex', 'Embarked', 'Cabin', 'Title', 'Family']","2a8c2f2a":"X = df_train_val[age_var]\ny = df_train_val['Age']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","269f8755":"# Initialise data\npool_train = Pool(X_train, y_train, cat_features=cat_features)\npool_val = Pool(X_val, y_val, cat_features=cat_features)\n\n# Initialise CatBoostRegressor\ncb_model = CatBoostRegressor(iterations=1000,\n                          learning_rate=0.03,\n                          depth=6,\n                          loss_function='RMSE',\n                          eval_metric='RMSE',\n                          cat_features=cat_features,\n                          verbose=0,\n                          use_best_model=True\n                         )\n# Fit model\ncb_model.fit(pool_train, eval_set=pool_val);","7bab8a21":"# Create the test set \nX_test = df_test[age_var]\n# Make predictions\ny_pred = cb_model.predict(X_test)","047b04f2":"# Format the fonts:\nfont = FontProperties(size=12)\nfont.set_style('italic')\n\n# Set up the figure:\nfig, ax = plt.subplots(ncols=1, nrows=1, figsize=(12,4))\nax.set_title('Predicted passenger ages\\n', fontsize=14, fontweight='bold')\nax.set_ylabel('Count', fontproperties=font)\nax.set_xlabel('Passenger age', fontproperties=font)\n    \n# What does the distribution look like? \nsns.histplot(y_pred);","3695096f":"# Creating the pools for pulling the shap info\npool1 = Pool(data=X_test, label=y_pred, cat_features=cat_features)\n# Get the feature importances\nshap_info = cb_model.get_feature_importance(data=pool1, type='ShapValues', verbose=0)\nshap_values = shap_info[:, : -1]\nbase_values = shap_info[:, -1]\n# Plot the values\nshap.summary_plot(shap_values, X_test)","29957946":"# What are the model's most important features? \nimportances = cb_model.get_feature_importance(prettified=True)","96e09ba7":"# What does the distribution look like? \n\n# Set up the figure:\nfig, ax = plt.subplots(ncols=1, nrows=1, figsize=(12,4))\nax.set_title('Importance score\\n', fontsize=14, fontweight='bold')\nax.set_ylabel('Importances', fontproperties=font)\nax.set_ylim((0,40))\nax.set_xlabel('Model feature', fontproperties=font)\nplt.yticks(fontsize=12)\nplt.xticks(fontsize=12)\n\n# What does the distribution look like? \nsns.barplot(x=importances['Feature Id'], y=importances['Importances']);\nax.set_xlabel('Model feature', fontproperties=font)\nadd_value_labels(ax, fontsize=12)","868b0ae4":"# Turn the predictions into a data frame\ny_df = pd.DataFrame(y_pred, columns=['Age'])\n# Add index as a colum\ny_df['Index'] = X_test.index\n#\u00a0Set the index\ny_df.set_index('Index', inplace=True)\n# Put the predicted values back into the test df\ndf.fillna(y_df, axis=1, inplace=True)","b2ed2b00":"cat_features = ['Sex','Embarked','Title','Cabin','Family']\ndf = pd.get_dummies(df,columns=cat_features,prefix=cat_features)","a6a7f715":"var = ['Pclass','Age','Fare','Sex_female','Sex_male','Embarked_C','Embarked_Q','Embarked_S',\n       'Title_Master','Title_Miss','Title_Mr','Title_Mrs','Title_Other',\n       'Cabin_A','Cabin_B','Cabin_C','Cabin_D','Cabin_E','Cabin_Other','Cabin_Unknown',\n       'Family_Large_family','Family_None','Family_Small_family']\ny_train = df['Survived']\nX_train = df[var]","d4545285":"scale_data = True\nshuffle = True # Needs to be turned off if we are scaling the data as the ensemble model fails to fit otherwise\nscaler = MinMaxScaler()\n\n# Scaling the data\nif scale_data:\n    X_train = scaler.fit_transform(X_train)\n    X_train = pd.DataFrame(data=X_train, columns=var)","df31c232":"# Let's set the class weights to add to all our models:\nn = y_train.sum()\/len(y_train)\nclass_weights = {0:(1-n), 1:n};","9ac77b98":"# Setting our random state:\nrandom_state = 42","18ecdc48":"# Cross validate model with Kfold stratified cross val\nfolds = 10 \nkfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=random_state)","6f13e411":"%%time\n\n# Setting up all the classifiers to iterate over below:\nclassifiers = [SVC(random_state=random_state, max_iter=5000), \n               DecisionTreeClassifier(random_state=random_state),\n               AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),\n                                  random_state=random_state,learning_rate=0.1),\n               RandomForestClassifier(random_state=random_state),\n               ExtraTreesClassifier(random_state=random_state,class_weight=class_weights),\n               GradientBoostingClassifier(random_state=random_state),\n               KNeighborsClassifier(),\n               LogisticRegression(random_state=random_state, max_iter=500),\n               LinearDiscriminantAnalysis(),\n               GaussianNB(),\n               BaggingClassifier(n_estimators=100),\n               MLPClassifier(random_state=random_state, max_iter=1000),\n               LGBMClassifier(random_state=random_state),\n               XGBClassifier(random_state=random_state, use_label_encoder=False, \n                             eval_metric='logloss', silent=1),\n               CatBoostClassifier(random_state=random_state, early_stopping_rounds=100, iterations=1000)\n              ]\n\n# Appending each classifier's results to a list\ncv_results = []\nfor classifier in classifiers:\n    cv_results.append(cross_val_score(classifier, X_train, y=y_train, scoring=\"accuracy\", cv=kfold, n_jobs=4))\n\n# Calculating the mean performance and standard deviation for comparison later:\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n    \n# Creating a dataframe of the results:\nmodel_names = [\"SVC\",\"DecisionTree\",\"AdaBoost\",\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\n               \"KNeighbours\",\"LogisticRegression\",\n               \"LinearDiscriminantAnalysis\", \"GaussianNB\", \"BaggingClassifier\",\n               \"MultipleLayerPerceptron\",\n               \"LightGBM\", \"XGBoost\", \"CatBoost\"\n              ]\ncv_res = pd.DataFrame({\"CV_means\":cv_means,\"CV_std\": cv_std,\"Algorithm\":model_names})","eface6f5":"# Plotting the results:\nfig, ax = plt.subplots(ncols=1, nrows=1, figsize=(16,4))\nax.set_title('Model performance\\n', fontsize=14, fontweight='bold')\nax.set_ylim((0.75,0.9))\nplt.yticks(fontsize=12)\nplt.xticks(fontsize=12, rotation=45, horizontalalignment='right')\nsns.barplot(y=\"CV_means\", x=\"Algorithm\", data=cv_res, **{'yerr':cv_std}, errcolor='r', errwidth=1, capsize=5)\nax.set_xlabel('Algorithm', fontproperties=font)\nax.set_ylabel('Cross validation - mean score', fontproperties=font)\nadd_value_labels(ax, fontsize=12, spacing=5, label_format=\"{:.2f}\");  # Adds the values above the bars","516e90ea":"# Setting up an empty dictionary to include our predictions\npredictions = {}\n\n# Loop through all the models and add predictions to the dictionary\nfor index, classifier in enumerate(classifiers):\n    model_name = model_names[index]\n    fit_model = classifier.fit(X_train, y_train)\n    predictions[model_name] = fit_model.predict(X_train)\n\n# Turn our base prediction dictionary into a dataframe\nbase_predictions_train = pd.DataFrame(predictions)\n\n# Calculate the correlations between all the models' predictions\ndata = base_predictions_train.corr()","d44bb5d8":"# Setting up the plot\nfig,ax = plt.subplots(ncols=1, nrows=1, figsize=(16,8))\n\n# Set the font options:\nfont = FontProperties(size=12)\nfont.set_style('italic')\n\n# Build the chart\nfig = sns.heatmap(data=data, annot=True, cmap='rainbow', linewidths=1, linecolor='white',\n                  fmt=\".2f\", annot_kws={\"size\":12}, cbar_kws={'label': '\\nCorrelation of predictions'})\nfig.figure.axes[-1].yaxis.label.set_size(14)\n    \n# Setting the font size for the colorbar \ncbar = fig.collections[0].colorbar\ncbar.ax.tick_params(labelsize=12)\n\n# X-axis and tick mark labels\nfig.set_xlabel('Model', fontproperties=font)\nplt.xticks(fontsize=12)\n    \n# y-axis and tick mark labels\nfig.set_ylabel('Model', horizontalalignment='center', fontproperties=font)\nplt.yticks(fontsize=12, verticalalignment='center')\n\n# Setting the graph title\nax.set_title(\"Correlation of respective model predictions\\n\", fontsize=16, fontweight=\"bold\");","245843ff":"# Creating an empty dictionary:\nmodel_correlation_dict = {}\n# Adding the model names and means to the dictionary:\nfor name in model_names:\n    model_correlation_dict[name] = 1- data[name].mean()\n# Turning the dictionary into a dataframe:\nmodel_correlation = pd.Series(model_correlation_dict)","9e662517":"# Plotting the outputs\nfig, ax = plt.subplots(ncols=1, nrows=1, figsize=(16,4))\n\n# Setting the title\nax.set_title('Average model prediction correlations\\n', fontsize=14, fontweight='bold')\n\n# Setting y-axis range\nax.set_ylim((0.1,0.3))\n\n# Setting label ticks size:\nplt.yticks(fontsize=12)\nplt.xticks(fontsize=12, rotation=45, horizontalalignment='right')\n\n# Creating the bar plot\nsns.barplot(x=model_correlation.index, y=model_correlation);\n\n# Creating the axis labels:\nax.set_xlabel('Model', fontproperties=font)\nax.set_ylabel('1 - Mean correlation with other models', fontproperties=font)\n\n# Adding the data label\nadd_value_labels(ax, fontsize=12, spacing=5, label_format=\"{:.2f}\");  # Adds the values above the bars","d13503e2":"# Create a new df with the model correlation and CV performance data\nscatter_data = pd.concat((model_correlation, cv_res.set_index('Algorithm')), axis=1)\n# Drop the standard deviation of the CV performance as we don't need it for this analysis\nscatter_data.drop('CV_std', axis=1, inplace=True)\n# Rename the remaining columns\nscatter_data.rename(columns={0: 'Correlation', 'CV_means':'Mean CV score'}, inplace=True)\n# Add the model name as another column for use in the scatter chart below:\nscatter_data['Model'] = scatter_data.index","f3e8f524":"def scatter_text(x, y, text_column, data, title, xlabel, ylabel):\n    \"\"\"Scatter plot with country codes on the x y coordinates\n       Based on this answer: https:\/\/stackoverflow.com\/a\/54789170\/2641825\"\"\"\n\n    # Creating the figure:\n    fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(16,10))\n\n    # Create the scatter plot    \n    p1 = sns.scatterplot(x=x, y=y, data=data, ax=ax, s=100, legend=False)\n    \n    # Set title\n    ax.set_title(title, fontsize=14, fontweight='bold')\n    \n    # Creating the axis labels:\n    ax.set_xlabel(xlabel, fontproperties=font)\n    ax.set_ylabel(ylabel, fontproperties=font)\n    \n    # Setting label ticks size:\n    plt.yticks(fontsize=12)\n    plt.xticks(fontsize=12)\n\n    # Add text besides each point\n    for line in range(0,data.shape[0]):\n         p1.text(data[x][line]+0.002, data[y][line], \n                 data[text_column][line], \n                 horizontalalignment='left', \n                 size='small', \n                 color='black',\n                 #weight='semibold'\n                )\n    \n    return p1\n\n# Draw the scatter chart:\nscatter_text(x='Correlation', \n             y='Mean CV score', \n             text_column='Model', \n             data=scatter_data, \n             title='Model performance vs. correlation\\n', \n             ylabel='Mean CV score', \n             xlabel='1- mean correlation with other models');","a767d451":"# First let's split our training data into training and validation:\nX, X_val, y, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=random_state)","b54182be":"class Optimiser:\n    def __init__(self, metric, trials=100):\n        self.metric = metric\n        self.trials = trials\n        self.sampler = TPESampler(seed=42)\n        \n    def objective(self, trial):\n        model = create_model(trial)\n        model.fit(X, y)\n        preds = model.predict(X_val)\n        if self.metric == 'acc':\n            return accuracy_score(y_val, preds)\n        else:\n            return f1_score(y_val, preds)\n            \n    def optimise(self):\n        study = optuna.create_study(direction=\"maximize\", sampler=self.sampler)\n        study.optimize(self.objective, n_trials=self.trials, timeout=900)\n        return study.best_params","eed285df":"%%time\n\n# Setting up the classifier:\nxgb = XGBClassifier(random_state=random_state, use_label_encoder = False, eval_metric='logloss')\nxgb.fit(X, y)\npreds = xgb.predict(X_val)\n\n# Calculating model scores:\nxgb_acc_score = accuracy_score(y_val, preds)\nxgb_f1_score = f1_score(y_val, preds)\n\ndef create_model(trial):\n    # Set up the trial parameters:\n    max_depth = trial.suggest_int(\"max_depth\", 2, 8)\n    n_estimators = trial.suggest_int(\"n_estimators\", 1, 1500)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0001, 1)\n    gamma = trial.suggest_uniform('gamma', 0.0000001, 1)\n    subsample = trial.suggest_uniform('subsample', 0.8, 1.0)\n    \n    # Set up the model:\n    model = XGBClassifier(max_depth=max_depth, \n                          n_estimators=n_estimators, \n                          learning_rate=learning_rate,\n                          gamma=gamma, \n                          subsample=subsample,\n                          use_label_encoder=False,\n                          eval_metric='logloss',\n                          random_state=random_state\n                         )\n    return model\n\n# Optimise on accuracy\noptimiser = Optimiser('acc')\nxgb_acc_params = optimiser.optimise()\nxgb_acc_params['random_state'] = random_state\nxgb_acc_params['use_label_encoder'] = False\nxgb_acc_params['eval_metric'] = 'logloss'\nxgb_acc = XGBClassifier(**xgb_acc_params)\nxgb_acc.fit(X, y)\npreds = xgb_acc.predict(X_val)\n\n# Calculating model scores:\nxgb_acc_acc_score = accuracy_score(y_val, preds)\nxgb_acc_f1_score = f1_score(y_val, preds)\n\n# Optimise on F1 score\noptimiser = Optimiser('f1')\nxgb_f1_params = optimiser.optimise()\nxgb_f1_params['random_state'] = random_state\nxgb_f1_params['use_label_encoder'] = False\nxgb_f1_params['eval_metric'] = 'logloss'\nxgb_f1 = XGBClassifier(**xgb_f1_params)\nxgb_f1.fit(X, y)\npreds = xgb_f1.predict(X_val)\n\n# Calculating model scores:\nxgb_f1_acc_score = accuracy_score(y_val, preds)\nxgb_f1_f1_score = f1_score(y_val, preds)","260e7fdd":"# Print pre-optimised accuracy\nprint('Pre-optimised XGBoost:')\nprint(' - accuracy:', f'{xgb_acc_score:.3f}')\nprint(' - f1-score:', f'{xgb_f1_score:.3f}' + '\\n')\n\n# Print the output:\nprint('Accuracy optimised XGBoost:')\nprint(' - accuracy:', f'{xgb_acc_acc_score:.3f}')\nprint(' - f1-score:', f'{xgb_acc_f1_score:.3f}' + '\\n')\n\n# Print the output:\nprint('F1 optimised XGBoost:')\nprint(' - accuracy:', f'{xgb_f1_acc_score:.3f}')\nprint(' - f1-score:', f'{xgb_f1_f1_score:.3f}'+ '\\n')","0303c382":"%%time\n\n# Setting up the classifier:\nlgb = LGBMClassifier(random_state=random_state)\nlgb.fit(X, y)\npreds = lgb.predict(X_val)\n\n# Calculating model scores:\nlgb_acc_score = accuracy_score(y_val, preds)\nlgb_f1_score = f1_score(y_val, preds)\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 8)\n    n_estimators = trial.suggest_int(\"n_estimators\", 1, 1000)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0000001, 1)\n    num_leaves = trial.suggest_int(\"num_leaves\", 2, 3000)\n    min_child_samples = trial.suggest_int('min_child_samples', 3, 200)\n    model = LGBMClassifier(\n        learning_rate=learning_rate, \n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        num_leaves=num_leaves, \n        min_child_samples=min_child_samples,\n        random_state=random_state\n    )\n    return model\n\n# Optimise on accuracy\noptimiser = Optimiser('acc')\nlgb_acc_params = optimiser.optimise()\nlgb_acc_params['random_state'] = random_state\nlgb_acc = LGBMClassifier(**lgb_acc_params)\nlgb_acc.fit(X, y)\npreds = lgb_acc.predict(X_val)\n\n# Calculating model scores:\nlgb_acc_acc_score = accuracy_score(y_val, preds)\nlgb_acc_f1_score = f1_score(y_val, preds)\n\n# Optimise on F1 score\noptimiser = Optimiser('f1')\nlgb_f1_params = optimiser.optimise()\nlgb_f1_params['random_state'] = random_state\nlgb_f1 = LGBMClassifier(**lgb_f1_params)\nlgb_f1.fit(X, y)\npreds = lgb_f1.predict(X_val)\n\n# Calculating model scores:\nlgb_f1_acc_score = accuracy_score(y_val, preds)\nlgb_f1_f1_score = f1_score(y_val, preds)","d53b53ba":"# Printing the output \nprint('Pre-optimised LightGBM:')\nprint(' - accuracy:', f'{lgb_acc_score:.3f}')\nprint(' - f1-score:', f'{lgb_f1_score:.3f}' + '\\n')\n\n# Print the output:\nprint('Accuracy optimised LightGBM:')\nprint(' - accuracy:', f'{lgb_acc_acc_score:.3f}')\nprint(' - f1-score:', f'{lgb_acc_f1_score:.3f}' + '\\n')\n\n# Print the output:\nprint('F1 optimised LightGBM:')\nprint(' - accuracy:', f'{lgb_f1_acc_score:.3f}')\nprint(' - f1-score:', f'{lgb_f1_f1_score:.3f}'+ '\\n')","88564a67":"%%time\n\n# Setting up the classifier:\ngbc = GradientBoostingClassifier()\ngbc.fit(X, y)\npreds = gbc.predict(X_val)\n\n# Calculating model scores:\ngbc_acc_score = accuracy_score(y_val, preds)\ngbc_f1_score = f1_score(y_val, preds)\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 10)\n    n_estimators = trial.suggest_int(\"n_estimators\", 1, 300)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.00001, 1)\n    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 19, 3)\n    max_features = trial.suggest_uniform('max_features', 0.1, 1.0)\n    model = GradientBoostingClassifier(\n        max_depth=max_depth, \n        learning_rate=learning_rate, \n        n_estimators=n_estimators, \n        min_samples_leaf=min_samples_leaf,\n        random_state=random_state\n        )\n    return model\n\n# Optimising on accuracy\noptimiser = Optimiser('acc')\ngbc_acc_params = optimiser.optimise()\ngbc_acc_params['random_state'] = random_state\ngbc_acc = GradientBoostingClassifier(**gbc_acc_params)\ngbc_acc.fit(X, y)\npreds = gbc_acc.predict(X_val)\n\n# Calculating model scores:\ngbc_acc_acc_score = accuracy_score(y_val, preds)\ngbc_f1_acc_score = f1_score(y_val, preds)\n\n# Optimising on F1 score:\noptimiser = Optimiser('f1')\ngbc_f1_params = optimiser.optimise()\ngbc_f1_params['random_state'] = random_state\ngbc_f1 = GradientBoostingClassifier(**gbc_f1_params)\ngbc_f1.fit(X, y)\npreds = gbc_f1.predict(X_val)\n\n# Calculating model scores:\ngbc_f1_acc_score = accuracy_score(y_val, preds)\ngbc_f1_f1_score = f1_score(y_val, preds)","69a8fe11":"# Printing scores\nprint('Pre-optimised Gradient Boosting classifier:')\nprint(' - accuracy:', f'{gbc_acc_score:.3f}')\nprint(' - f1-score:', f'{gbc_f1_score:.3f}' + '\\n')\n\n# Print the output:\nprint('Accuracy optimised Gradient Boosting:')\nprint(' - accuracy:', f'{gbc_acc_acc_score:.3f}')\nprint(' - f1-score:', f'{gbc_f1_acc_score:.3f}' + '\\n')\n\n# Print the output:\nprint('F1 optimised Gradient Boosting:')\nprint(' - accuracy:', f'{gbc_f1_acc_score:.3f}')\nprint(' - f1-score:', f'{gbc_f1_f1_score:.3f}'+ '\\n')","c3faadb7":"%%time\n\n# Setting up the classifier:\ncb = CatBoostClassifier(random_state=random_state, verbose=0, early_stopping_rounds=100, \n                        iterations=1000, \n                        boosting_type = 'Plain'\n                       )\ncb.fit(X, y)\npreds = cb.predict(X_val)\n\n# Calculating model scores:\ncb_acc_score = accuracy_score(y_val, preds)\ncb_f1_score = f1_score(y_val, preds)\n\n# Print output for CatBoost\nprint('CatBoost classifier:')\nprint(' - accuracy:', f'{cb_acc_score:.3f}')\nprint(' - f1-score:', f'{cb_f1_score:.3f}' + '\\n')","6ed3a7b5":"%%time\n\n# Instantiate the model\nlogreg = LogisticRegression(random_state=random_state, max_iter=2000)\nlogreg.fit(X, y)\npreds = logreg.predict(X_val)\n\n# Calculating model scores:\nlr_acc_score = accuracy_score(y_val, preds)\nlr_f1_score = f1_score(y_val, preds)\n\nprint('Logistic regression accuracy score:', f'{lr_acc_score:.3f}')\nprint('Logistic regression f1-score: ', f'{lr_f1_score:.3f}'+ '\\n')","493b8e64":"%%time\n\n# Setting up the classifier:\nmlp = MLPClassifier(random_state=random_state, max_iter=2000)\nmlp.fit(X, y)\npreds = mlp.predict(X_val)\n\n# Calculating model scores:\nmlp_acc_score = accuracy_score(y_val, preds)\nmlp_f1_score = f1_score(y_val, preds)\n\ndef create_model(trial):\n    learning_rate_init = trial.suggest_uniform(\"learning_rate_init\", 0.0001, 0.01)\n    hidden_layer_sizes = trial.suggest_int('hidden_layer_sizes', 8, 128)\n    max_iter = trial.suggest_int('max_iter', 500, 3000)\n    model = MLPClassifier(\n        learning_rate_init = learning_rate_init,\n        hidden_layer_sizes = hidden_layer_sizes,\n        max_iter = max_iter,\n        random_state=random_state\n        )\n    return model\n\n# Optimising on accuracy:\noptimiser = Optimiser('acc')\nmlp_acc_params = optimiser.optimise()\nmlp_acc_params['random_state'] = random_state\nmlp_acc = MLPClassifier(**mlp_acc_params)\nmlp_acc.fit(X, y)\npreds = mlp_acc.predict(X_val)\n\n# Calculating model scores:\nmlp_acc_acc_score = accuracy_score(y_val, preds)\nmlp_acc_f1_score = f1_score(y_val, preds)\n\n# Optimising on f1 score\noptimiser = Optimiser('f1')\nmlp_f1_params = optimiser.optimise()\nmlp_f1_params['random_state'] = random_state\nmlp_f1 = MLPClassifier(**mlp_f1_params)\nmlp_f1.fit(X, y)\npreds = mlp_f1.predict(X_val)\n\n# Calculating model scores:\nmlp_f1_acc_score = accuracy_score(y_val, preds)\nmlp_f1_f1_score = f1_score(y_val, preds)","95fc02f0":"# Print the output\nprint('Pre-optimised MLP classifier:')\nprint(' - accuracy:', f'{mlp_acc_score:.3f}')\nprint(' - f1-score:', f'{mlp_f1_score:.3f}' + '\\n')\n\n# Print the output:\nprint('Accuracy optimised MLP:')\nprint(' - accuracy:', f'{mlp_acc_acc_score:.3f}')\nprint(' - f1-score:', f'{mlp_acc_f1_score:.3f}' + '\\n')\n\n# Print the output:\nprint('F1 optimised MLP:')\nprint(' - accuracy:', f'{mlp_f1_acc_score:.3f}')\nprint(' - f1-score:', f'{mlp_f1_f1_score:.3f}'+ '\\n')","2d4c8b1f":"%%time\n\nlda = LinearDiscriminantAnalysis()\nlda.fit(X, y)\npreds = lda.predict(X_val)\n\nlda_acc_score = accuracy_score(y_val, preds)\nlda_f1_score = f1_score(y_val, preds)\n\nprint('Linear discriminant analysis accuracy score:', f'{accuracy_score(y_val, preds):.3f}')\nprint('Linear discriminant analysis f1-score: ', f'{f1_score(y_val, preds):.3f}'+ '\\n')","83296379":"%%time\n\n# Setting up the classifier:\nsvc = SVC(random_state=random_state, class_weight=class_weights, max_iter=2000)\nsvc.fit(X, y)\npreds = svc.predict(X_val)\n\n# Calculate the scores\nsvc_acc_score = accuracy_score(y_val, preds)\nsvc_f1_score = f1_score(y_val, preds)\n\n# Print the scores\nprint('SVC classifier:')\nprint(' - accuracy:', f'{svc_acc_score:.3f}')\nprint(' - f1-score:', f'{svc_f1_score:.3f}' + '\\n')","9c2469bd":"%%time\n\n# Instantiate the classifier:\nbc = BaggingClassifier(random_state=random_state)\nbc.fit(X, y)\npreds = bc.predict(X_val)\n\n# Calculating the accuracy scores:\nbc_acc_score = accuracy_score(y_val, preds)\nbc_f1_score = f1_score(y_val, preds)\n\n# Set the terms for the trials\ndef create_model(trial):\n    n_estimators = trial.suggest_int('n_estimators', 2, 200)\n    max_samples = trial.suggest_int('max_samples', 1, 100)\n    model = BaggingClassifier(\n        n_estimators=n_estimators, \n        max_samples=max_samples, \n        random_state=random_state\n    )\n    return model\n\n# Optimise on accuracy\noptimiser = Optimiser('acc')\nbc_acc_params = optimiser.optimise()\nbc_acc_params['random_state'] = random_state\nbc_acc = BaggingClassifier(**bc_acc_params)\nbc_acc.fit(X, y)\npreds = bc_acc.predict(X_val)\n\n# Calculating the accuracy scores:\nbc_acc_acc_score = accuracy_score(y_val, preds)\nbc_acc_f1_score = f1_score(y_val, preds)\n\n# Optimise on f1 scores\noptimiser = Optimiser('f1')\nbc_f1_params = optimiser.optimise()\nbc_f1_params['random_state'] = random_state\nbc_f1 = BaggingClassifier(**bc_f1_params)\nbc_f1.fit(X, y)\npreds = bc_f1.predict(X_val)\n\n# Calculating the accuracy scores:\nbc_f1_acc_score = accuracy_score(y_val, preds)\nbc_f1_f1_score = f1_score(y_val, preds)","cc74bb62":"# Print the scores\nprint('Pre-optimised bagging classifier:')\nprint(' - accuracy:', f'{bc_acc_score:.3f}')\nprint(' - f1-score:', f'{bc_f1_score:.3f}' + '\\n')\n\n# Print the output:\nprint('Accuracy optimised bagging classifier:')\nprint(' - accuracy:', f'{bc_acc_acc_score:.3f}')\nprint(' - f1-score:', f'{bc_acc_f1_score:.3f}' + '\\n')\n    \n# Print the output:\nprint('F1 optimised bagging classifier:')\nprint(' - accuracy:', f'{bc_f1_acc_score:.3f}')\nprint(' - f1-score:', f'{bc_f1_f1_score:.3f}'+ '\\n')","b9dbd01c":"%%time\n\ngnb = GaussianNB()\ngnb.fit(X, y)\npreds = gnb.predict(X_val)\n\ngnb_acc_score = accuracy_score(y_val, preds)\ngnb_f1_score = f1_score(y_val, preds)\n\nprint('Gaussian naive bayes analysis accuracy score:', f'{accuracy_score(y_val, preds):.3f}')\nprint('Gaussian naive bayes analysis f1-score: ', f'{f1_score(y_val, preds):.3f}'+ '\\n')","11c3b8ef":"%%time\n\n# Instantiate the classifier:\nknn = KNeighborsClassifier()\nknn.fit(X, y)\npreds = knn.predict(X_val)\n\n# Calculating the accuracy scores:\nknn_acc_score = accuracy_score(y_val, preds)\nknn_f1_score = f1_score(y_val, preds)\n\n# Set the terms for the trials\ndef create_model(trial):\n    n_neighbors = trial.suggest_int('n_neighbors', 2, 30, 2)\n    weights = trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"])\n    model = KNeighborsClassifier(\n        n_neighbors=n_neighbors, \n        weights=weights\n    )\n    return model\n\n# Optimise on accuracy\noptimiser = Optimiser('acc')\nknn_acc_params = optimiser.optimise()\nknn_acc = KNeighborsClassifier(**knn_acc_params)\nknn_acc.fit(X, y)\npreds = knn_acc.predict(X_val)\n\n# Calculating the accuracy scores:\nknn_acc_acc_score = accuracy_score(y_val, preds)\nknn_acc_f1_score = f1_score(y_val, preds)\n\n# Optimise on f1 scores\noptimiser = Optimiser('f1')\nknn_f1_params = optimiser.optimise()\nknn_f1 = KNeighborsClassifier(**knn_f1_params)\nknn_f1.fit(X, y)\npreds = knn_f1.predict(X_val)\n\n# Calculating the accuracy scores:\nknn_f1_acc_score = accuracy_score(y_val, preds)\nknn_f1_f1_score = f1_score(y_val, preds)","bd1260df":"# Print the scores\nprint('Pre-optimised bagging classifier:')\nprint(' - accuracy:', f'{knn_acc_score:.3f}')\nprint(' - f1-score:', f'{knn_f1_score:.3f}' + '\\n')\n\n\n# Print the output:\nprint('Accuracy optimised bagging classifier:')\nprint(' - accuracy:', f'{knn_acc_acc_score:.3f}')\nprint(' - f1-score:', f'{knn_acc_f1_score:.3f}' + '\\n')\n\n# Print the output:\nprint('F1 optimised bagging classifier:')\nprint(' - accuracy:', f'{knn_f1_acc_score:.3f}')\nprint(' - f1-score:', f'{knn_f1_f1_score:.3f}'+ '\\n')","ac9bf3d0":"acc_dict = {\n    'GBC': gbc_acc_score, 'OAGBC': gbc_acc_acc_score, 'OFGBC': gbc_f1_acc_score,\n    'LR': lr_acc_score,\n    'LDA': lda_acc_score,\n    'SVC': svc_acc_score,\n    'BC': bc_acc_score, 'OABC': bc_acc_acc_score, 'OFBC': bc_f1_acc_score,\n    'GNB': gnb_acc_score,\n    'KNN': knn_acc_score, 'OAKNN': knn_acc_acc_score, 'OFKNN': knn_f1_acc_score,\n    'MLP': mlp_acc_score, 'OAMLP': mlp_acc_acc_score, 'OFMLP': mlp_f1_acc_score,\n    'LGBM': lgb_acc_score, 'OALGBM': lgb_acc_acc_score, 'OFLGBM': lgb_f1_acc_score,\n    'XGB': xgb_acc_score, 'OAXGB': xgb_acc_acc_score, 'OFXGB': xgb_f1_acc_score,\n    'CB': cb_acc_score,\n}\ntuned_models = pd.DataFrame.from_dict(acc_dict, orient='index')\ntuned_models.rename({0: 'Accuracy'}, axis=1, inplace=True)","9ffe118e":"# Plotting the outputs\nfig, ax = plt.subplots(ncols=1, nrows=1, figsize=(20,4))\n\n# Setting the title\nax.set_title('Tuned model accuracy\\n', fontsize=14, fontweight='bold')\n\n# Setting y-axis range\nax.set_ylim((0.8,0.92))\n\n# Setting label ticks size:\nplt.yticks(fontsize=12)\nplt.xticks(fontsize=12, rotation=45, horizontalalignment='right')\n\n# Creating the bar plot\nsns.barplot(x=tuned_models.index, y=tuned_models['Accuracy']);\n\n# Creating the axis labels:\nax.set_xlabel('Model', fontproperties=font)\nax.set_ylabel('Accuracy', fontproperties=font)\n\n# Adding the data label\nadd_value_labels(ax, fontsize=12, spacing=5, label_format=\"{:.3f}\");  # Adds the values above the bars","0db6438d":"# Set up our superlearner with folds \nensemble = SuperLearner(folds=folds, random_state=random_state, \n                        shuffle=False, scorer = accuracy_score, sample_size=len(X))\n\n# Add our models as the base layer models\nensemble.add([\n    gbc, \n    svc, \n    bc,\n    gnb,\n    knn,\n    mlp, \n    lgb,\n    cb, \n    xgb,\n])\n\n# Add Catboost as the metalearner\nensemble.add_meta(LogisticRegression(max_iter=20000))\n\n# Now to fit the model on the training data\nensemble.fit(X, y)\n\n# Make the predictions using the validation data:\npreds = ensemble.predict(X_val)\n\nprint('SuperLearner accuracy: ', f'{accuracy_score(y_val, preds): .3f}')\nprint('SuperLearner f1-score: ', f'{f1_score(y_val, preds): .3f}')","450ef804":"mdict = {\n    'GBC': GradientBoostingClassifier(random_state=random_state), \n    'OAGBC': GradientBoostingClassifier(**gbc_acc_params), \n    'OFGBC': GradientBoostingClassifier(**gbc_f1_params),\n    'MLP': MLPClassifier(random_state=random_state, max_iter=2000), \n    'OAMLP': MLPClassifier(**mlp_acc_params), \n    'OFMLP': MLPClassifier(**mlp_f1_params),\n    'LR': LogisticRegression(random_state=random_state,max_iter=2000),\n    'LDA': LinearDiscriminantAnalysis(),\n    'SVC': SVC(random_state=random_state, class_weight=class_weights), \n    'LGBM': LGBMClassifier(random_state=random_state), \n    'OALGBM': LGBMClassifier(**lgb_acc_params), \n    'OFLGBM': LGBMClassifier(**lgb_f1_params),\n    'XGB': XGBClassifier(random_state=random_state,use_label_encoder=False), \n    'OAXGB': XGBClassifier(**xgb_acc_params), \n    'OFXGB': XGBClassifier(**xgb_f1_params),\n    'CB': CatBoostClassifier(random_state=random_state, verbose=0, early_stopping_rounds=100), \n    'BC': BaggingClassifier(random_state=random_state), \n    'OABC': BaggingClassifier(**bc_acc_params), \n    'OFBC': BaggingClassifier(**bc_f1_params),\n    'GNB': GaussianNB(),\n    'KNN': KNeighborsClassifier(), \n    'OAKNN': KNeighborsClassifier(**knn_acc_params), \n    'OFKNN': KNeighborsClassifier(**knn_f1_params),\n}","15101bda":"def create_model(trial):\n    model_names = list()\n    \n    # List the models to use in the ensemble\n    models_list = list(mdict.keys())\n    \n    # List the models to be used in the head\n    head_list = list(mdict.keys())\n    \n    n_models = trial.suggest_int(\"n_models\", 3, 15)\n    for i in range(n_models):\n        model_item = trial.suggest_categorical('model_{}'.format(i), models_list)\n        if model_item not in model_names:\n            model_names.append(model_item)\n    \n    # Instantiate the ensemble model:\n    ensemble = SuperLearner(folds=folds, random_state=random_state, shuffle=False, \n                            scorer = accuracy_score, sample_size=len(X))\n    \n    # Add all the models to the ensemble:\n    models = [mdict[item] for item in model_names]\n    ensemble.add(models)\n    \n    # Cycle through possible 'heads'\n    head = trial.suggest_categorical('head', head_list)\n    ensemble.add_meta(mdict[head])\n        \n    return ensemble\n\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(X, y)\n    preds = model.predict(X_val)\n    score = accuracy_score(y_val, preds)\n    return score","8aab12f5":"%%time\n# Creating our sampler\nsampler = TPESampler(seed=random_state)\n\n# creating the study:\nstudy = optuna.create_study(direction=\"maximize\", sampler=sampler)\n\n# Optimising our study:\nstudy.optimize(objective, n_trials=350, timeout=7200,\n              )","809056a2":"# Define the best parameters of the study:\nparams = study.best_params\nhead = params['head']\ndel params['head'], params['n_models']\nresult = list()\nfor key, value in params.items():\n    if value not in result:\n        result.append(value)\n\nprint(\"Head model is: \"+str(head))\nprint(\"Component models are: \"+str(result))","a4dbd8f6":"# Instantiate our optimised ensemble model:\nensemble = SuperLearner(folds=folds, random_state=random_state)\n\n# Add the models from the result above to the ensemble:\nmodels = [mdict[item] for item in result]\nensemble.add(models)\n\n# Add the meta model from the above:\nensemble.add_meta(mdict[head])\n\n# Fit the optimised ensemble model:\nensemble.fit(X, y)\n\n#\u00a0Make the final predictions:\npreds = ensemble.predict(X_val)\n\n# Print our output:\nprint('Optimised SuperLearner accuracy: ', f'{accuracy_score(y_val, preds):.3f}')\nprint('Optimised SuperLearner f1-score: ', f'{f1_score(y_val, preds):.3f}')","aace2dfe":"test.isna().sum()","67e9e12c":"# Creating the \u2018Title\u2019 column\ntest['Title'] = test.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\n\ntitles = ['Mr', 'Mrs', 'Miss', 'Master', 'Other']\ntest.loc[~test.Title.isin(titles), \"Title\"] = \"Other\"\n\n# Dropping the name column\ntest.drop('Name', axis=1, inplace=True) \n\n# Create a cabin letter column and fill the unknowns with 'Unknown'\ntest['Cabin_let'] = test['Cabin'].str[0]\ntest.Cabin_let.fillna(value='Unknown', inplace=True)\n\n# Now we can drop the old 'Cabin' column and change the name of the 'Cabin_let' column\ntest.drop('Cabin', axis=1, inplace=True)\ntest.rename(columns={'Cabin_let': 'Cabin'}, inplace=True)\n\n# Change the tail of the cabin letters to 'Other'\nlet_to_replace = ['F','G','T']\nreplace_with = 'Other'\n\nfor i in let_to_replace:\n    test.Cabin.loc[test['Cabin']==i] = replace_with\n\n# Let's create a 'family' column to show who had children\/parents\/spouse onboard\ntest['Family'] = test.SibSp + test.Parch\n\n# ...and kill the original columns\ntest.drop('SibSp', axis=1, inplace=True)\ntest.drop('Parch', axis=1, inplace=True)\n\n# Creating the 'None' bucket\ntest.Family.loc[test['Family']==0] = 'None'\n\n# Creating the 'Small_family' bucket\nsmall_num_to_replace = [1,2,3]\nsmall_fam = 'Small_family'\nfor i in small_num_to_replace:\n    test.Family.loc[test['Family']==i] = small_fam\n    \n# Creating the 'Large_family' bucket\nlarge_num_to_replace = [4,5,6,7,8,9,10]\nlarge_fam = 'Large_family'\nfor i in large_num_to_replace:\n    test.Family.loc[test['Family']==i] = large_fam\n\n# Taking the log of the fare to reduce skewness:\ntest[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\n\n# Drop the ticket and Passenger ID\ntest.drop('Ticket', axis=1, inplace=True)\n#test.drop('PassengerId', axis=1, inplace=True)\n\n# Imputing the missing 'Fare' value\ntest['Fare'].fillna(test['Fare'].median(), inplace=True)","bd2f788a":"# Let's split the dataframe based on whether the passenger's age is available\ntest_test = test.copy().loc[test['Age'].isin([np.nan])]\ntest_train_val = test.copy().loc[~test['Age'].isin([np.nan])]\n\n# Now we need to define our independent variables\nage_var = ['Pclass', 'Sex', 'Fare', 'Embarked', 'Title', 'Family', 'Cabin']\n\n# And label the categorical features for Catboost\ncat_features = ['Sex', 'Embarked', 'Cabin', 'Title', 'Family']\n\nX = test_train_val[age_var]\ny = test_train_val['Age']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialise data\npool_train = Pool(X_train, y_train, cat_features=cat_features)\npool_val = Pool(X_val, y_val, cat_features=cat_features)\n\n# Initialise CatBoostRegressor\ncb_model = CatBoostRegressor(iterations=1000,\n                          learning_rate=0.03,\n                          depth=6,\n                          loss_function='RMSE',\n                          eval_metric='RMSE',\n                          cat_features=cat_features,\n                          verbose=0,\n                          use_best_model=True\n                         )\n# Fit model\ncb_model.fit(pool_train, eval_set=pool_val, plot=False);\n\n# Create the test set \nX_test = test_test[age_var]\n\n# Make predictions\ny_pred = cb_model.predict(X_test)\n\n# Turn the predictions into a data frame\ny_test = pd.DataFrame(y_pred, columns=['Age'])\n\n# Add index as a colum\ny_test['Index'] = X_test.index\n\n#\u00a0Set the index\ny_test.set_index('Index', inplace=True)\n\n# Put the predicted values back into the test df\ntest.fillna(y_test, axis=1, inplace=True)","c8e9c5c8":"# One hot encoding on the test set:\ncat_features = ['Sex','Embarked','Title','Cabin','Family']\ntest = pd.get_dummies(test,columns=cat_features,prefix=cat_features)","7591bebb":"var = ['Pclass','Age','Fare','Sex_female','Sex_male','Embarked_C','Embarked_Q','Embarked_S',\n       'Title_Master','Title_Miss','Title_Mr','Title_Mrs','Title_Other',\n       'Cabin_A','Cabin_B','Cabin_C','Cabin_D','Cabin_E','Cabin_Other','Cabin_Unknown',\n       'Family_Large_family','Family_None','Family_Small_family']\n\nX_test = test[var]","bcb1d271":"# Now we need to scale the data in the same way that we did with the training data:\nscale_data = True\nshuffle = True # Needs to be turned off if we are scaling the data as the ensemble model fails to fit otherwise\nscaler = MinMaxScaler()\n\n# Scaling the data\nif scale_data:\n    X_test = scaler.fit_transform(X_test)\n    X_test = pd.DataFrame(data=X_test, columns=var)","15b985c0":"# Make the predictions \npredictions = ensemble.predict(X_test).astype(int)","62253579":"# Creating our output:\noutput = pd.DataFrame({'PassengerId': test['PassengerId'],\n                       'Survived': predictions})","afda44bb":"# Creating the csv output file for submission\noutput.to_csv('ODP_submission.csv', index=False)","ca369bd6":"## 4.5. Gradient boosting","af98f70c":"I now need to take the predicted ages, add and index and then add the predicted ages back into the training set.","f19548c7":"## 4.4. LightGBM","de084bd9":"### 3.2. Name\/Title\nThe 'Name' field may have some useful data contained within it, as there may be trends we could identify like title, surname, etc. ","1adb0a21":"Where did the two people who are missing from the 'Embarked' column get onboard?","c56ab9b6":"This is a metric with a high degree of skew. This represents a problem for modelling as outliers in our features can have an outsized impact on feature importance and hence final accuracy. I will therefore take the log of fare for my modelling. ","109b800d":"Thos with 1 or 2 siblings onboard are much more like to survive than those with none, but less likely once in a large family (3-4). ","5154884e":"## 4.2. Scaling the data\nNext I will scale the feature data.","37604beb":"As expected, 1st class passengers had the highest survival rates, and wewre more than twice as likely to survive as 3rd class passengers. ","895276ad":"Optimising the SuperLearner:","ee3223a9":"### 3.7. Passenger ID","5cf05f17":"There are a large number of names, including several unique names. I should therefore rationalise the titles to improve computational efficiency and reduce noise in the feature. ","4494fe2a":"## 4.10. Support vector classifier","6706a31d":"### 3.8. Age\nThere are quite a few missing age numbers. However, we could try and predict the age by building a model to fill in these numbers, using the other metrics as training data. ","7ca66be4":"### 2.5. Number of children\/parents onboard","a00db98f":"## 4.8. Multi layer perceptron","5d7d7795":"# 7. Conclusion \n\nThis model returns a 78% accuracy in the predictions, which is about 6,000th in the top 28,000 submissions (including the people who have obviously cheated!). Which is a pretty good result! Feel free to copy\/reuse elements of code from the above. ","146d4f0c":"And now I need to define our training features:","9b07383d":"### 3.3. Cabin\nMost of the cabin data appear to be missing. Perhaps we can categorise the cabins, split by their first letters.","6e94702f":"Next, we should check how correlated the predictions are, as ensemble models composed of uncorrelated input models often outperform those that are highly correlated.","0302740d":"## 4.11. Bagging classifier","2a63f35a":"Finally, let's create simple none\/small\/large family groups to simplify the feature.","15ef2fcb":"Women and children first! Women are more than 3x as likely to survive as men. ","9963ade1":"### 2.4. Number of siblings\/spouse onboard","03fabf5b":"## 4.7. Logistic regression","347613fd":"Filling in the missing 'Age' column.","6c58c16b":"## 4.3. XGBoost","8b7a8d87":"So now I will calculate the mean correlation of each of the models and subtract it from 1 so that the higher the correlations the better.","39db0b40":"Showing all the metrics visually will give a sense of all the distributions.","dc5823fc":"Now there is no useful information to glean from the 'Name' column, I will drop that from the dataset. ","fa730e17":"Now for the hyperparameter tuning! This process is very slow, so go and make a cup of tea once you click 'run'...","d9c42d23":"# 1. Importing libraries and loading the data","93f8c225":"Now that I have created a prediction for the missing age range using CatBoost, let's see what the distribution looks like.","280ab47c":"All the passenger IDs are unique and hence provide no value as model features. I will therefore drop the column from the dataframe.","bf4eb283":"First, I need to analyse the test set to see if any data are missing.","26e24b3b":"# 6. Making a prediction","9dec0bd0":"# 3. Feature engineering\nWhere are the data missing? ","1e656d1f":"Running through each of the variables in turn and checking how they impact survival rate.","8b26aebe":"### 3.6. Ticket","b2dcbad3":"We see a similar relationship with children\/parents as siblings\/spouses, i.e. some relatives on board enhances the chance of survival, but only to a point. ","326c973e":"Let's have a peek at the data:","33624a26":"### 4.2.2. Optimiser class\nSetting up an optimiser class","61e9bff8":"# 5. Ensembling\nWe will now build an ensemble model using our base predictors above and a logistic regression meta model. ","edd71646":"### 2.6. Fare paid","277cd4b3":"Rerunning the analysis to check it has worked.","0786ac1f":"So as we can see from the above there are a number of models which are less correlated than the others, notably the SVC, K-nearest neighbours, and Gaussian Naive Bayes. Let's compare the correlation metrics to the accuracy scores to see which models could form the best elements of an ensemble model.","98c31939":"### 3.5. Fare","7e26e70c":"## 4.2. Hyperparameter tuning\n### 4.2.1. Training and validation data sets","4f67c6df":"### 2.1. Passenger class","7d50c886":"Looking at a mean of the accuracy scores across the 10 folds and including standard deviation of model performance too:","b83a91bb":"## 4.6. Catboost","0e494e76":"# 4. Modelling\n## 4.1. One hot encoding\nThe first step I need to take is to use one hot encoding so that our models are dealing with only numerical variables. ","f138d9ce":"# Titanic competition","f44af46f":"## 4.13. K-Nearest Neighbours","ecfe922f":"So I will need to fill the missing age data and alter the missing Cabin data as I did before. \n\nNow I need to change the test set to match the inputs to our model.","6ce4cb4a":"### 3.4. Family\nI will now create a `'Family'` column which is simply the sum of the sibling\/spouses and children\/parent columns and kill the original columns. This will reduce the complexity and should help the predictive power of the model by reducing overfitting.","0a1c27c9":"### 2.3. Age","0d4bf3fd":"## 4.3. Summary of tuned models' performance\nNow I will build a chart showing how the accuracy of the models compares.","dc8af052":"# 2. Exploratory data analysis","42222afe":"Miss Amelie Icard is clearly French and hence is more likely to have boarding in Cherbourg, but Mrs Stone is probably English and hence more likely to have boarded in Southampton. However, they have the same ticket number and are both in cabin B28, so are unlikely to have boarded at different ports! As the vast majority of passengers boarded in Southampton I will assume they embarked in Southampton.","d0d8f031":"Ticket has 681 unique numbers in 891 records. This means that the vast majority are unique and, from looking at around 100 examples, the only information which could be gleaned from the ticket numbers is class or possible cabin, both of which we have already. As a result, I will drop this metric. ","32a8c865":"### 3.1. Embarked","1a07c30e":"## 4.9. Linear discriminant analysis","f0e68894":"## 4.1. Model selection\nI will trial the following classification models:\n1. Support vector classifier\n2. Decision Tree\n3. AdaBoost\n4. Random Forest\n5. Extra Trees\n6. Gradient Boosting\n7. Multiple layer perceprton (neural network)\n8. K-nearest neighbours\n9. Logistic regression\n10. Linear Discriminant Analysis\n11. Gaussian Naive Bayes\n12. Bagging classifier\n13. Light GBM\n14. XGBoost\n15. Catboost\n\nLet's perform a cross validation with K-folds and compare model performance.","9b7427f3":"## 4.12. Gaussian NB","9ebd8793":"Now I will replace the NaNs with 'Unknown' and group the tail of cabin letters as 'Other'","253fa342":"### 2.2. Sex","57d0bc29":"So based on the above outputs, I will choose Gradient Boosting, CatBoost, Bagging classifier, LightGBM, MLP, Logistic Regression and LDA as they have the best cross-validation scores. I will also use SVC as it is uncorrelated with the other models and XGBoost as its performance can be significantly enhanced through tuning.","cb4bfd2d":"Finally, let's see how the survival rate varies with the passengers' titles. ","57496fb1":"Younger passengers are slightly more likely to survive but the effect is not as pronounced as class\/sex.","f6c64e21":"Which features are most important in determining the passengers' missing age?"}}