{"cell_type":{"681d917e":"code","a2ddca63":"code","4be55b09":"code","11fb5ab5":"code","ab8ac1b7":"code","29c11518":"code","61e914be":"code","82148758":"code","07e1bcb5":"code","5ed5d43c":"code","72a98a95":"code","d2177003":"code","a2a5efd6":"code","d4f132c4":"code","3cd8d575":"code","427d0456":"code","93dcbdb0":"code","cfd407a4":"code","c414f21b":"code","062fe007":"code","bd3a0d5d":"code","02641e12":"code","1e70b4cc":"code","16e97c84":"code","82938981":"code","1d1864b0":"code","66598b77":"code","cb92588a":"code","eee2cc3f":"code","69825bee":"code","694eacb6":"code","dded0df3":"code","47dec5ec":"code","f08bcf60":"code","77ebe4e0":"code","9a2c5d8f":"code","271e530c":"code","2cf25ed8":"code","0b914d6f":"code","6524b7cb":"code","025ad054":"code","bd104b49":"code","4ec54e05":"code","99050dac":"code","35e2d409":"code","8e9e4faf":"code","3d6a42cf":"code","f5f46352":"code","2e5205e2":"code","49aa27d4":"markdown","08272b05":"markdown","dc7ce07e":"markdown","e92d5e46":"markdown","97edeb4f":"markdown","02bbf4cf":"markdown","1fd3a511":"markdown","704107a4":"markdown","d9191ee1":"markdown","91d26b76":"markdown","47ee04a5":"markdown","8276858f":"markdown","43600cda":"markdown","0fb829ce":"markdown","b62d5ad9":"markdown","ac848467":"markdown","b116b84d":"markdown","fa0540de":"markdown","7a0d4923":"markdown","60b27986":"markdown","51c990b4":"markdown","0f397e67":"markdown","560edcb3":"markdown","21d3c874":"markdown","49c2927d":"markdown","814f234d":"markdown","2bed8a28":"markdown","1bd4ed3c":"markdown","58c5887c":"markdown","55dff0ff":"markdown","2ead81c0":"markdown","7e954f13":"markdown","46e3b5f1":"markdown","d41e984b":"markdown"},"source":{"681d917e":"import os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a2ddca63":"!ls '..\/input'\n!ls '..\/input\/ipricristik20'","4be55b09":"#STEP 1: IMPORTING LIBRARIES AND DATASET\n\n#import some necessary librairies\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, cross_validate, GridSearchCV\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier, XGBRegressor\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_absolute_error\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\n\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points","11fb5ab5":"# Importing dataset from kaggle\n\ntrain = pd.read_csv('..\/input\/ipricristik20\/train.csv')\ntest = pd.read_csv('..\/input\/ipricristik20\/test.csv')","ab8ac1b7":"train['price'].describe()","29c11518":"train.head(5)","61e914be":"test.head(5)","82148758":"print(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n# Save the 'Id' column\ntrain_ID = train['id']\ntest_ID = test['id']\n\n# Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"id\", axis = 1, inplace = True)\ntest.drop(\"id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","07e1bcb5":"sns.distplot(train['price'] , fit=norm);\n\n#Now plot the distribution\nplt.ylabel('Frequency')\nplt.title('price distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['price'], plot=plt)\nplt.show()","5ed5d43c":"#Log transformation\n\n#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"price\"] = np.log1p(train[\"price\"])\n\n#Check the new distribution \nsns.distplot(train['price'] , fit=norm);\n\nplt.ylabel('Frequency')\nplt.title('price distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['price'], plot=plt)\nplt.show()","72a98a95":"#Correlation map to see how features are correlated with price\ncorrmat = train.corr()\nplt.subplots(figsize=(12,12))\nsns.heatmap(corrmat, vmax=0.9, square=True)","d2177003":"data = train.corr()[\"price\"].sort_values()[::-1]\nplt.figure(figsize=(12, 8))\nsns.barplot(x=data.values, y=data.index)\nplt.title(\"Correlation with price\")\nplt.xlim(-0.2, 1)\nplt.show()","a2a5efd6":"#deleting outliers points by index --> room_size\nvar = 'room_size'\ntemp = pd.concat([train[var], train['price']], axis=1)\ntemp.plot.scatter(x=var, y='price')\ntemp.sort_values(by = var, ascending = True)\n\ntrain = train.drop(train[train['price'] < 12].index, axis=0)","d4f132c4":"#deleting outliers points by index --> room_size\nvar = 'distance_poi_A2'\ntemp = pd.concat([train[var], train['price']], axis=1)\ntemp.plot.scatter(x=var, y='price')\ntemp.sort_values(by = var, ascending = True)\n\n# train = train.drop(train[(train['price'] > 14.5) & train['distance_poi_A2'] > 10000].index, axis=0)\n# train = train.drop(train[(train['price'] > 14.0) & train['distance_poi_A2'] > 25000].index, axis=0)\n\ntrain = train.drop(train[train['distance_poi_A2'] > 12500].index, axis=0)","3cd8d575":"#deleting outliers points by index --> room_size\nvar = 'distance_poi_A1'\ntemp = pd.concat([train[var], train['price']], axis=1)\ntemp.plot.scatter(x=var, y='price')\ntemp.sort_values(by = var, ascending = True)\n\ntrain = train.drop(train[train['distance_poi_A1'] > 16250].index, axis=0)","427d0456":"#deleting outliers points by index --> room_size\nvar = 'distance_poi_A3'\ntemp = pd.concat([train[var], train['price']], axis=1)\ntemp.plot.scatter(x=var, y='price')\ntemp.sort_values(by = var, ascending = True)\n\n# train = train.drop(train[(train['price'] > 14.5) & train['distance_poi_A3'] > 10000].index, axis=0)\n# train = train.drop(train[(train['price'] > 14.0) & train['distance_poi_A3'] > 30000].index, axis=0)\n\ntrain = train.drop(train[(train['price'] > 14.0) & train['distance_poi_A3'] > 10000].index, axis=0)\ntrain = train.drop(train[train['distance_poi_A3'] > 15000].index, axis=0)","93dcbdb0":"#deleting outliers points by index --> room_size\nvar = 'distance_poi_B3'\ntemp = pd.concat([train[var], train['price']], axis=1)\ntemp.plot.scatter(x=var, y='price')\ntemp.sort_values(by = var, ascending = True)\n\n# train = train.drop(train[(train['price'] > 14.0) & train['distance_poi_B3'] > 15000].index, axis=0)\n# # train = train.drop(train[(train['price'] > 14.0) & train['distance_poi_A2'] > 30000].index, axis=0)\n\ntrain = train.drop(train[train['distance_poi_B3'] > 13000].index, axis=0)","cfd407a4":"#deleting outliers points by index --> room_size\nvar = 'distance_poi_B4'\ntemp = pd.concat([train[var], train['price']], axis=1)\ntemp.plot.scatter(x=var, y='price')\ntemp.sort_values(by = var, ascending = True)\n\n# train = train.drop(train[(train['price'] > 14.0) & train['distance_poi_B4'] > 15000].index, axis=0)\n# train = train.drop(train[(train['price'] > 14.0) & train['distance_poi_A2'] > 30000].index, axis=0)\n\ntrain = train.drop(train[train['distance_poi_B4'] > 14000].index, axis=0)","c414f21b":"#deleting outliers points by index --> room_size\nvar = 'longitude'\ntemp = pd.concat([train[var], train['price']], axis=1)\ntemp.plot.scatter(x=var, y='price')\ntemp.sort_values(by = var, ascending = True)\n\n# train = train.drop(train[(train['price'] > 14.0) & train['distance_poi_B4'] > 15000].index, axis=0)\n# train = train.drop(train[(train['price'] > 14.0) & train['distance_poi_A2'] > 30000].index, axis=0)\n\ntrain = train.drop(train[train['longitude'] < 110.28].index, axis=0)","062fe007":"# #deleting outliers points by index --> room_size\n# var = 'latitude'\n# temp = pd.concat([train[var], train['price']], axis=1)\n# temp.plot.scatter(x=var, y='price')\n# temp.sort_values(by = var, ascending = True)\n\n# # train = train.drop(train[(train['price'] > 14.0) & train['distance_poi_B4'] > 15000].index, axis=0)\n# # train = train.drop(train[(train['price'] > 14.0) & train['distance_poi_A2'] > 30000].index, axis=0)\n\n# train = train.drop(train[train['longitude'] < -7.88].index, axis=0)","bd3a0d5d":"#STEP 3: DATA PRE-RPOCESSING AND FEATURE ENGINEERING ON COMBINED DATASET\n\nntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.price.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\n# all_data.drop(['price'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","02641e12":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","1e70b4cc":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","16e97c84":"#karena secret_2 dan secret_9 semuanya 1 dan 0. Lebih baik di drop saja.\nall_data = all_data.drop(['secret_2', 'secret_9'],axis=1)","82938981":"all_data = all_data.drop(['item_4'],axis=1)","1d1864b0":"all_data = all_data.drop(['secret_7'],axis=1)","66598b77":"all_data = all_data.drop(['secret_10'],axis=1)","cb92588a":"all_data = all_data.drop(['distance_poi_A4', 'distance_poi_A6', 'distance_poi_A5', 'distance_poi_B2', 'distance_poi_B1'],axis=1)","eee2cc3f":"for col in ('distance_poi_A1', 'distance_poi_A2', 'distance_poi_A3', 'distance_poi_B3', 'distance_poi_B4','room_size'):\n    all_data[col] = all_data[col].fillna((all_data[col].mean()))","69825bee":"all_data['room_size'] = all_data['room_size'].fillna((all_data['room_size'].mean()))","694eacb6":"for col in ('facility_1', 'facility_2', 'facility_3','facility_4', 'facility_5', 'female', 'male', 'item_1', 'item_2', 'item_3', 'item_5','secret_1','secret_3','secret_4','secret_5','secret_6','secret_8'):\n    all_data[col] = all_data[col].fillna((all_data[col].mode()[0]))","dded0df3":"all_data['latitude'] = all_data['latitude'].abs()\n\n#change the latitude value to positive.","47dec5ec":"for col in ('longitude', 'latitude'):\n    all_data[col] = all_data[col].fillna((all_data[col].interpolate(method='linear')))","f08bcf60":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","77ebe4e0":"\n# Analysing and normalising target variable\nvar = 'room_size'\nsns.distplot(all_data[var], fit=norm);\nfig = plt.figure()\nres = stats.probplot(all_data[var], plot=plt)\n\n# Applying log transformation to resolve skewness\nall_data[var] = np.log1p(all_data[var])\nsns.distplot(all_data[var], fit=norm);\nfig = plt.figure()\nres = stats.probplot(all_data[var], plot=plt)\n\n# Analysing and normalising target variable\nvar = 'distance_poi_A1'\nsns.distplot(all_data[var], fit=norm);\nfig = plt.figure()\nres = stats.probplot(all_data[var], plot=plt)\n\n# Applying log transformation to resolve skewness\nall_data[var] = np.log1p(all_data[var])\nsns.distplot(all_data[var], fit=norm);\nfig = plt.figure()\nres = stats.probplot(all_data[var], plot=plt)\n\n# Analysing and normalising target variable\nvar = 'distance_poi_A2'\nsns.distplot(all_data[var], fit=norm);\nfig = plt.figure()\nres = stats.probplot(all_data[var], plot=plt)\n\n# Applying log transformation to resolve skewness\nall_data[var] = np.log1p(all_data[var])\nsns.distplot(all_data[var], fit=norm);\nfig = plt.figure()\nres = stats.probplot(all_data[var], plot=plt)\n\n# Analysing and normalising target variable\nvar = 'distance_poi_A3'\nsns.distplot(all_data[var], fit=norm);\nfig = plt.figure()\nres = stats.probplot(all_data[var], plot=plt)\n\n# Applying log transformation to resolve skewness\nall_data[var] = np.log1p(all_data[var])\nsns.distplot(all_data[var], fit=norm);\nfig = plt.figure()\nres = stats.probplot(all_data[var], plot=plt)\n\n# Analysing and normalising target variable\nvar = 'distance_poi_B3'\nsns.distplot(all_data[var], fit=norm);\nfig = plt.figure()\nres = stats.probplot(all_data[var], plot=plt)\n\n# Applying log transformation to resolve skewness\nall_data[var] = np.log1p(all_data[var])\nsns.distplot(all_data[var], fit=norm);\nfig = plt.figure()\nres = stats.probplot(all_data[var], plot=plt)\n\n# Analysing and normalising target variable\nvar = 'distance_poi_B4'\nsns.distplot(all_data[var], fit=norm);\nfig = plt.figure()\nres = stats.probplot(all_data[var], plot=plt)\n\n# Applying log transformation to resolve skewness\nall_data[var] = np.log1p(all_data[var])\nsns.distplot(all_data[var], fit=norm);\nfig = plt.figure()\nres = stats.probplot(all_data[var], plot=plt)\n\n# Analysing and normalising target variable\nvar = 'longitude'\nsns.distplot(all_data[var], fit=norm);\nfig = plt.figure()\nres = stats.probplot(all_data[var], plot=plt)\n\n# Applying log transformation to resolve skewness\nall_data[var] = np.log1p(all_data[var])\nsns.distplot(all_data[var], fit=norm);\nfig = plt.figure()\nres = stats.probplot(all_data[var], plot=plt)\n\n# Analysing and normalising target variable\nvar = 'latitude'\nsns.distplot(all_data[var], fit=norm);\nfig = plt.figure()\nres = stats.probplot(all_data[var], plot=plt)\n\n# Applying log transformation to resolve skewness\nall_data[var] = np.log1p(all_data[var])\nsns.distplot(all_data[var], fit=norm);\nfig = plt.figure()\nres = stats.probplot(all_data[var], plot=plt)","9a2c5d8f":"#STEP 4: XGBOOST MODELING WITH PARAMETER TUNING\n\n#Creating train_test_split for cross validation\nX = all_data.loc[all_data['price']>0]\nX = X.drop(['price'], axis=1)\ny = all_data[['price']]\ny = y.drop(y.loc[y['price'].isnull()].index, axis=0)\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2, random_state=8)\n\n#Creating DMatrices for XGBoost\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)","271e530c":"#Setting initial parameters\nparams = {\n    # Parameters that we are going to tune.\n    'max_depth':5,\n    'min_child_weight': 1,\n    'eta':0.3,\n    'subsample': 0.80,\n    'colsample_bytree': 0.80,\n    'reg_alpha': 0,\n    'reg_lambda': 0,\n    # Other parameters\n    'objective':'reg:squarederror',\n}\n\n\n#Setting evaluation metrics - MAE from sklearn.metrics\nparams['eval_metric'] = \"mae\"\n\nnum_boost_round = 5000\n\n#Begin training of XGB model\nmodel = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=50\n)\n\nprint(\"Best MAE: {:.2f} with {} rounds\".format(\n                 model.best_score,\n                 model.best_iteration+1))\n\n#replace num_boost_round with best iteration + 1\nnum_boost_round = model.best_iteration+1\n\n#Establishing baseline MAE\ncv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    seed=8,\n    nfold=5,\n    metrics={'mae'},\n    early_stopping_rounds=50\n)\ncv_results\ncv_results['test-mae-mean'].min()","2cf25ed8":"#Parameter-tuning for max_depth & min_child_weight (First round)\ngridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in range(0,12,2)\n    for min_child_weight in range(0,12,2)\n]\n\nmin_mae = float(\"Inf\")\nbest_params = None\nfor max_depth, min_child_weight in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}\".format(\n                             max_depth,\n                             min_child_weight))\n    # Update our parameters\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=8,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=50\n    )\n    # Update best MAE\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].idxmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (max_depth,min_child_weight)\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))\n\n\n#Parameter-tuning for max_depth & min_child_weight (Second round)\ngridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in [3,4,5]\n    for min_child_weight in [3,4,5]\n]\n\nmin_mae = float(\"Inf\")\nbest_params = None\nfor max_depth, min_child_weight in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}\".format(\n                             max_depth,\n                             min_child_weight))\n    # Update our parameters\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=8,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=50\n    )\n    # Update best MAE\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].idxmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (max_depth,min_child_weight)\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))\n\n\n#Parameter-tuning for max_depth & min_child_weight (Third round)\ngridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in [3]\n    for min_child_weight in [i\/10. for i in range(30,50,2)]\n]\n\nmin_mae = float(\"Inf\")\nbest_params = None\nfor max_depth, min_child_weight in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}\".format(\n                             max_depth,\n                             min_child_weight))\n    # Update our parameters\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=8,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=50\n    )\n    # Update best MAE\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].idxmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (max_depth,min_child_weight)\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))","0b914d6f":"#Updating max_depth and mind_child_weight parameters\nparams['max_depth'] = 3\nparams['min_child_weight'] = 3.0","6524b7cb":"#Recalibrating num_boost_round after parameter updates\nnum_boost_round = 5000\n\nmodel = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=50\n)\n\nprint(\"Best MAE: {:.2f} with {} rounds\".format(\n                 model.best_score,\n                 model.best_iteration+1))\n\n#replace num_boost_round with best iteration + 1\nnum_boost_round = model.best_iteration+1","025ad054":"#Parameter-tuning for subsample & colsample (First round)\ngridsearch_params = [\n    (subsample, colsample)\n    for subsample in [i\/10. for i in range(3,11)]\n    for colsample in [i\/10. for i in range(3,11)]\n]\n\nmin_mae = float(\"Inf\")\nbest_params = None\n# We start by the largest values and go down to the smallest\nfor subsample, colsample in reversed(gridsearch_params):\n    print(\"CV with subsample={}, colsample={}\".format(\n                             subsample,\n                             colsample))\n    # We update our parameters\n    params['subsample'] = subsample\n    params['colsample_bytree'] = colsample\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=8,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=50\n    )\n    # Update best score\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].idxmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (subsample,colsample)\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))\n\n#Parameter-tuning for subsample & colsample (Second round)\ngridsearch_params = [\n    (subsample, colsample)\n    for subsample in [i\/100. for i in range(80,100)]\n    for colsample in [i\/100. for i in range(70,90)]\n]\n\nmin_mae = float(\"Inf\")\nbest_params = None\n# We start by the largest values and go down to the smallest\nfor subsample, colsample in reversed(gridsearch_params):\n    print(\"CV with subsample={}, colsample={}\".format(\n                             subsample,\n                             colsample))\n    # We update our parameters\n    params['subsample'] = subsample\n    params['colsample_bytree'] = colsample\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=8,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=50\n    )\n    # Update best score\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].idxmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (subsample,colsample)\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))\n","bd104b49":"#Updating subsample and colsample parameters\nparams['subsample'] = 0.85\nparams['colsample'] = 0.88","4ec54e05":"#Recalibrating num_boost_round after parameter updates\nnum_boost_round = 5000\n\nmodel = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=50\n)\n\nprint(\"Best MAE: {:.2f} with {} rounds\".format(\n                 model.best_score,\n                 model.best_iteration+1))\n\n#replace num_boost_round with best iteration + 1\nnum_boost_round = model.best_iteration+1","99050dac":"#Parameter-tuning for reg_alpha & reg_lambda\ngridsearch_params = [\n    (reg_alpha, reg_lambda)\n    for reg_alpha in [1e-5, 1e-4, 1e-3, 1e-2, 0.1]\n    for reg_lambda in [1e-5, 1e-4, 1e-3, 1e-2, 0.1]\n]\n\nmin_mae = float(\"Inf\")\nbest_params = None\n\nfor reg_alpha, reg_lambda in gridsearch_params:\n    print(\"CV with reg_alpha={}, reg_lambda={}\".format(\n                             reg_alpha,\n                             reg_lambda))\n    # We update our parameters\n    params['reg_alpha'] = reg_alpha\n    params['reg_lambda'] = reg_lambda\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=8,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=50\n    )\n    # Update best score\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].idxmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (reg_alpha,reg_lambda)\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))","35e2d409":"#Updating reg_alpha and reg_lambda parameters\nparams['reg_alpha'] = 0.0001\nparams['reg_lambda'] = 1e-05","8e9e4faf":"#Resetting num_boost_round to 5000\nnum_boost_round = 5000\n\n#Parameter-tuning for eta\nmin_mae = float(\"Inf\")\nbest_params = None\nfor eta in [0.3, 0.2, 0.1, 0.05, 0.01, 0.005]:\n    print(\"CV with eta={}\".format(eta))\n\n    params['eta'] = eta\n    cv_results = xgb.cv(\n            params,\n            dtrain,\n            num_boost_round=num_boost_round,\n            seed=8,\n            nfold=5,\n            metrics=['mae'],\n            early_stopping_rounds=50\n          )\n    # Update best score\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].idxmin()\n    print(\"\\tMAE {} for {} rounds\\n\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = eta\nprint(\"Best params: {}, MAE: {}\".format(best_params, min_mae))","3d6a42cf":"params['eta'] = 0.01","f5f46352":"model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=5000,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=50\n)\n\nnum_boost_round = model.best_iteration + 1\nbest_model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")]\n)\n\nmean_absolute_error(best_model.predict(dtest), y_test)","2e5205e2":"testdf = all_data.loc[all_data['price'].isnull()]\ntestdf = testdf.drop(['price'],axis=1)\nsub = pd.DataFrame()\nsub['id'] = test_ID\ntestdf = xgb.DMatrix(testdf)\n\ny_pred = np.expm1(best_model.predict(testdf))\nsub['price'] = y_pred\n\nsub.to_csv('submission.csv', index=False)","49aa27d4":"So based on initial analysis, we can see a total count of 3304 data count in the labelled train set, centred around the mean of ~893,211. However we will not say that the data is normally distributed, and have demonstrated positive skewness. The probability plot is a technique picked up from a guide - normally distributed data should be following the diagonal line closely.\n\nSo the next step would be to resolve the skewness and normalise our target variable. This is important as most machine learning techniques are either built on, or simply works better with normally distributed data. A simple technique would be to apply log transformation to resolve the skewness.","08272b05":"I think outliers that every dots on the distance morethan 16000.","dc7ce07e":"Dikarenakan secret_5 dan item_4 kemungkinan memiliki data yang sama, lebih baik kita drop salah satunya","e92d5e46":"Firstly, we'll combine the train and test dataset, so that any data transformation will be applied to all data uniformly. Once done, we'll need to find out what exactly are the missing data we need to handle","97edeb4f":"#**STEP 3: DATA PRE-PROCESSING AND FEATURE ENGINEERING ON COMBINED DATASET**","02bbf4cf":"Personally, I find that it is important to understand the problem that we're trying to solve right from the start. In this particular problem, we are to predict an independent variable 'price' based ona ling list of dependent variables. The first step for me, would then be understand more about the 'price' variable - We're interested in what's the count of data and how are they distributed?","1fd3a511":"Latitude longitude diisi dengan interpolate","704107a4":"I think it's varies to every person for how many data that you think is an outlier and not. For me this room_size attributes has one outlier on the bottom. ","d9191ee1":"Because secret_2 and secret_9 always has the same value, it's better just to drop them.","91d26b76":"Now that all ideal parameters are found, let us recap:\n\nparams = { 'max_depth':3, 'min_child_weight': 3.0, 'eta':0.005, 'subsample': 0.85, 'colsample_bytree': 0.89, 'reg_alpha': 0.0001, 'reg_lambda': 0.01, }","47ee04a5":"Then we continue to drop all the outlier on numerical attributes. But not on the boolean attributes..","8276858f":"#**STEP 4: XGBOOST MODELING WITH PARAMETER TUNING**","43600cda":"Next we'll set the initial parameters for the model. I have followed the logics laid out in this guide for the setting of original parameters:\nhttps:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/\n\nAnd with the initial parameters, we'll determine the ieal 'num_boost_round' and set a baseline MAE to beat. This way, we'll know whether the parameters we're tuning will indeed be resulting in a lower MAE.","0fb829ce":"In the final step of data pre-processing, we need to ensure that all faetures are holding numerical values, so that we can run them through the XGBRegressor model. We need to do the following:\n\n*   Recast any numerical features that are actually categorical\n*   Conduct Label Encoding for ordinal features\n*   Conduct OneHot Encoder for remaining categorical features\n\nBut we don't have to do anything categorycal in this data.\n\n\n","b62d5ad9":"This is the best way to see all the correlation between attributes. There are few attributes that got my attentions. Secret_2 and secret_9 are always the same so we will drop them off. Then you can see that relationsheep between distances are so strong that it could means multicollinearity. we can conclude that they give almost the same information so multicollinearity really occurs. And then, there are many data that similar in value like secret_7 and item_3.","ac848467":"For this journal, I have chosen a single model of XGBRegressor. The approach I have adopted for parameter-tuning can be found here:\nhttps:\/\/blog.cambridgespark.com\/hyperparameter-tuning-in-xgboost-4ff9100a3b2f\n\nI have gone with the common 'Mean Absolute Error' as the measuring metric, and will be applying a 5-fold cross-validation technique for training. Before we begin, we'll do a train_test_split, and load them into DMatrix (data format required for XGB models).","b116b84d":"Semua selain latitude longitude, diisi dengan mode","fa0540de":"Now that all missing data have been handled, we'll take a look at the distribution pattern for some of our critical features. As mentioned at the beginning of this journal, most machine learning techniques work better with normalised data. I'll be using the same technique to identify and correct for skewness for every numerical features.","7a0d4923":"There! Now that we have a normally distributed target variable, the next step would be to explore the remaining variables. Let's begin with numerical features.\n\nAs our dataset has a plethora of independet variable, feature selection is more critical than feature engineering in this particular problem. Thankfully, we can use seaborn to plot a correlation matrix. Seaborn not only helps us to identify 2 important things:\n\n\n\n1.   Correlation between numerical features and our target variable\n2.   Correlation between numerical features and other key features\n\nYou can choose to plot the entire features map, but personally I find it overwhelming\n\n","60b27986":"We can see all the relatable data from the biggest to the smallest. The 5 biggest are item_2, secret_4, facility_1, item_1, secret_7","51c990b4":"#**Import File**","0f397e67":"Dropping Id and put it in a variable.","560edcb3":"Semua yang distance dari poi A1 ampe b4 diisi pake rata2","21d3c874":"We can see that the baseline MAE to beat is ~0.215986. And initial num_boost_round will be set as 16. From here on, we will begin parameter tuning in 3 phases:\n\n*   Tuning max_depth & min_child_weight (Tree-specific parameter)\n*   Tuning subsample & colsample (Tree-specific parameter)\n\n*   Tuning reg_alpha & reg_lambda (Regularisation parameter)\n*   Tuning eta (Learning Rate)\n\nBy using gridsearch technique, we can narrow down on various values we want to test for each phase. Once the ideal value is determined, we need to update the parameters and recalibrate num_boost_round.\n\n\n\n","49c2927d":"This notebook seek to serve as my test to be accepted at RISTEK FASILKOM UI 2020\n\nI am fairly new to Data Science, and have depended on the guides made public by other Kagglers. Namely I have adopted much of my data exploration techniques from a certain topic of kaggle. However I have also processed the data using my own understanding, and at certain points have chosen to make different decisions from the guides that already on the topics.\n\n**Disclaimers** As this is my first time publishing a notebook on Kaggle I hope you find this notebook helpful in some ways. If you find any areas for imporvement, please feel free to suggest new approaches I may adopt. I welcome all feedbacks.\n\n**Content Outline** My approach can be largely cagtegorised into 4 major steps: STEP 1: IMPORTING LIBRARIES AND DATASET STEP 2 : EXPLORATORY DATA ANALYSIS ON TRAINING SET STEP 3: DATA PRE-PROCESSING AND FEATURE ENGINEERING ON COMBINED DATASET STEP 4: XGBOOST MODELING WITH PARAMETER TUNING\n\nSkuy!","814f234d":"Here you can see many outliers. I think every think many dots are concentrated on distance that less than 13000. The rest are outliers.","2bed8a28":"Ini nyoba2 doang2","1bd4ed3c":"#**STEP 2: EXPLORATORY DATA ANALYSIS ON TRAIN DATASET**","58c5887c":"Skewed Features","55dff0ff":"Dikarenakan item_3 dan secret_7 kemungkinan memiliki data yang sama lebih baik kita drop saja secret_7nya","2ead81c0":"Dikarenakan secret_10 sama room_size kemungkinan memiliki data yang sama, lebih baik kita drop salah satunya gue sih maunya secret_10","7e954f13":"#**STEP 1: IMPORTING LIBRARIES AND DATASET**","46e3b5f1":"I believe this step begins like all other kaggle submission, importing the relevant libaries for data science challanges, and of course importing of train and test data set.","d41e984b":"There are multiple methods to handle missing data:\n\n\n*   Fill missing data with or 0 (Common for Numerical Features)\n*   Fill missing data with 'None' ( Commong for Categorical Features)\n\n*   Fill missing data with Mean (Commong for Numerical Features)\n*   Fill missing data with Mode (Common for Categorical Features)\n\n*   Drop the Row (Common for rare occurances among data)\n*   Drop the Column (Common for features with large percentage of missing data)\n\n*   Replace with any other value you deem logical\n\nIn our case, our data doesn't have that many missing value. So putting mode on boolean features  and mean on numerical features will just do good.\n\n\n\n\n\n\n\n"}}