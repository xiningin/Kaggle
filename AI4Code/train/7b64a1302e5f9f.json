{"cell_type":{"cfcff7d8":"code","e4ff5e03":"code","385c4fcc":"code","9b9e69bd":"code","89d55c39":"code","260f0ed5":"code","44a5d1e1":"code","395d5411":"code","aa102d2d":"code","f7e40072":"code","8a05818d":"code","a7039720":"code","baa32649":"code","1f7bc147":"code","73b5ee35":"code","e3214d67":"code","96c07848":"code","2c41d9c1":"code","c1ed676f":"code","77565155":"code","4b106201":"code","aef2e701":"code","097a045e":"code","63e8e3aa":"code","9d433879":"code","65685869":"code","657c515d":"code","4d3757e4":"code","e61c24b7":"code","801443a8":"code","255156b1":"code","257dcbfe":"code","65bb101a":"code","dd008bb6":"code","8d20099f":"code","c76e9746":"code","58e6413b":"code","cbbf6eff":"code","8b1c38e4":"code","921a991c":"code","76e9776f":"code","41acfc72":"code","adc68b9b":"code","06782ef5":"code","3f96aa29":"code","57f0a8af":"code","1e64b331":"code","f745d69f":"code","f305a1d6":"code","bc8db9ec":"code","c0f59c16":"code","ef516a52":"code","32c6eb61":"code","c34df67a":"code","9275671a":"code","35a37368":"code","ae8e2f26":"code","1f3348be":"code","0f9bde81":"code","f58bb23d":"code","265e187f":"code","75bb339d":"code","ff318e1b":"code","3fce2860":"markdown","3be6da09":"markdown","0e59682d":"markdown","94ff32ab":"markdown","38e0ab82":"markdown","a7b73c3b":"markdown","8920b1bb":"markdown","7694d24f":"markdown","5770eac9":"markdown","73c7bea1":"markdown","e9585f13":"markdown","9282adf2":"markdown","d9801c7d":"markdown","50416ab8":"markdown","e372394d":"markdown","4eefb3df":"markdown","f2dd4c6d":"markdown","2aff0d5b":"markdown","1980cb52":"markdown","36ec39ca":"markdown","b7b647aa":"markdown"},"source":{"cfcff7d8":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport soundfile as sf\nimport scipy.signal as signal\nimport matplotlib.pyplot as plt # to support librosa display\nimport seaborn as sns\nimport IPython.display as ipd # for playing audio\nfrom collections import Counter\nfrom tqdm import tqdm\nimport gc\nimport os\nimport sys\nimport shutil\nimport random\n\nfrom joblib import Parallel, delayed\nfrom functools import partial\n\nimport librosa # audio proccessing\nimport librosa.display # cool audio visuals\nfrom librosa import feature as lf\n\nimport tensorflow\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy\nfrom tensorflow.keras.optimizers import Adam","e4ff5e03":"os.listdir(\"\/kaggle\")","385c4fcc":"os.listdir(\"\/kaggle\/working\")","9b9e69bd":"data_tp=pd.read_csv('\/kaggle\/input\/rfcx-species-audio-detection\/train_tp.csv')\ndata_fp=pd.read_csv('\/kaggle\/input\/rfcx-species-audio-detection\/train_fp.csv')","89d55c39":"data_tp.head(10)","260f0ed5":"len(data_tp['recording_id'].unique())\n#print(\"Number of rows in train data:'l_train'\") # length of unique recording IDs","44a5d1e1":"len(data_tp['recording_id']) # Total number of recording IDs","395d5411":"len(data_tp), len(data_fp)","aa102d2d":"data_tp['recording_id'].drop_duplicates()","f7e40072":"data_tp.info()","8a05818d":"data_fp.head()","a7039720":"data_fp.info()","baa32649":"data_fp['recording_id'].drop_duplicates()","1f7bc147":"\nplt.figure(figsize = (10, 5), dpi = 300)\nplt.style.use('ggplot')\nsns.distplot(data_tp['f_min'], color='red')\nsns.distplot(data_tp['f_max'], color='Green')\nplt.title('Min and Max frequencies')\nplt.legend(['Min_Frequency', 'Max_Frequency']);","73b5ee35":"def plot_count(feature, title, df, size=1):\n    '''\n    Plot count of classes \/ feature\n    param: feature - the feature to analyze\n    param: title - title to add to the graph\n    param: df - dataframe from which we plot feature's classes distribution \n    param: size - default 1.\n    '''\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:30], palette='Set1')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n    plt.show()  ","e3214d67":"plot_count('species_id', 'TP: Species ID', data_tp, size=4)","96c07848":"plot_count('songtype_id', 'TP: Songtype ID', data_tp, size=2)","2c41d9c1":"plot_count('species_id', 'FP: Species ID', data_fp, size=4)","c1ed676f":"plot_count('songtype_id', 'FP: Songtype ID', data_fp, size=2)","77565155":"def plot_feature_distribution(data_df, feature, feature2, title, kde_mode=False, hist_mode=True):\n    f, ax = plt.subplots(1,1, figsize=(12,6))\n    for item in list(data_df[feature2].unique()):\n        d_df = data_df.loc[data_df[feature2]==item]\n        try:\n            sns.distplot(d_df[feature], kde=kde_mode, hist=hist_mode, label=item)\n        except:\n            pass\n    plt.legend(labels=list(data_df[feature2].unique()), bbox_to_anchor=(1, 1), loc='upper right', ncol=2)\n    plt.title(title)\n    plt.show()","4b106201":"plot_feature_distribution(data_tp, 'f_max', 'species_id', \"Maximum frequency distribution, FP data, grouped by species id\")","aef2e701":"plot_feature_distribution(data_tp, 'f_max', 'songtype_id', \"Maximum frequency distribution, TP data, grouped by songtype id\")","097a045e":"plot_feature_distribution(data_tp, 't_min', 'species_id', \n                          \"Minimum time distribution, TP data, grouped by species id\", kde_mode=True, hist_mode=False)","63e8e3aa":"plot_feature_distribution(data_tp, 't_max', 'species_id', \n                          \"Max time distribution, TP data, grouped by species id\", kde_mode=True, hist_mode=False)","9d433879":"plot_feature_distribution(data_tp, 't_min', 'songtype_id', \n                          \"Minimum time distribution, TP data, grouped by songtype id\", kde_mode=True, hist_mode=False)","65685869":"plot_feature_distribution(data_tp, 't_max', 'songtype_id', \n                          \"Max time distribution, TP data, grouped by songtype id\", kde_mode=True, hist_mode=False)","657c515d":"plot_feature_distribution(data_fp, 't_min', 'species_id', \n                          \"Minimum time distribution, FP data, grouped by species id\", kde_mode=True, hist_mode=False)","4d3757e4":"plot_feature_distribution(data_fp, 't_max', 'species_id', \n                          \"Max time distribution, FP data, grouped by species id\", kde_mode=True, hist_mode=False)","e61c24b7":"plot_feature_distribution(data_fp, 't_min', 'songtype_id', \n                          \"Minimum time distribution, FP data, grouped by songtype id\", kde_mode=True, hist_mode=False)","801443a8":"plot_feature_distribution(data_fp, 't_max', 'songtype_id', \n                          \"Max time distribution, FP data, grouped by songtype id\", kde_mode=True, hist_mode=False)","255156b1":"def plot_audio_file(data_df, idx):\n    audio_file_path = '\/kaggle\/input\/rfcx-species-audio-detection\/train\/'+data_df.recording_id[idx]+'.flac'\n    plt.figure(figsize=(12,6))\n    x , sr = librosa.load(audio_file_path)\n    librosa.display.waveplot(x, sr=sr)\n    plt.gca().set_title(f\"Waveplot - file: {data_df.recording_id[idx]}\")\n    plt.show()","257dcbfe":"def plot_spectrogram(data_df, idx):\n    audio_file_path = '\/kaggle\/input\/rfcx-species-audio-detection\/train\/'+data_df.recording_id[idx]+'.flac'\n    plt.figure(figsize=(12,6))\n    x , sr = librosa.load(audio_file_path)\n    xs = librosa.stft(x)\n    xdb = librosa.amplitude_to_db(abs(xs))\n    librosa.display.specshow(xdb, sr=sr, x_axis='time', y_axis='hz')\n    plt.gca().set_title(f\"Spectrogram - file: {data_df.recording_id[idx]}\")\n    plt.colorbar()","65bb101a":"def plot_mel_spectrogram(data_df, idx):\n    audio_file_path = '\/kaggle\/input\/rfcx-species-audio-detection\/train\/'+data_df.recording_id[idx]+'.flac'\n    plt.figure(figsize=(12,6))\n    x , sr = librosa.load(audio_file_path)\n    xs = librosa.feature.melspectrogram(x)\n    xdb = librosa.amplitude_to_db(abs(xs))\n    librosa.display.specshow(xdb, sr=sr, x_axis='time', y_axis='hz')\n    plt.gca().set_title(f\"Mel spectrogram - file: {data_df.recording_id[idx]}\")\n    plt.colorbar()","dd008bb6":"def plot_harmonics_and_perceptual(data_df, idx):\n    audio_file_path = '\/kaggle\/input\/rfcx-species-audio-detection\/train\/'+data_df.recording_id[idx]+'.flac'\n    plt.figure(figsize=(12,6))\n    x , sr = librosa.load(audio_file_path)\n    y_harmonics, y_perceptual = librosa.effects.hpss(x)\n    plt.plot(y_perceptual, color = '#BBAA12')\n    plt.plot(y_harmonics, color = '#12AABB')\n    plt.legend((\"Perceptual\", \"Harmonics\"))\n    plt.title(f\"Harmonics and Perceptual - file: {data_df.recording_id[idx]}\")","8d20099f":"def plot_chroma_feature(data_df, idx):\n    hop_length=12\n    audio_file_path = '\/kaggle\/input\/rfcx-species-audio-detection\/train\/'+data_df.recording_id[idx]+'.flac'\n    plt.figure(figsize=(12,6))\n    x , sr = librosa.load(audio_file_path)\n    chromagram = librosa.feature.chroma_stft(x)\n    librosa.display.specshow(chromagram, sr=sr, x_axis='time', y_axis='chroma',hop_length=hop_length, cmap='coolwarm')\n    plt.title(f\"Chroma feature - file: {data_df.recording_id[idx]}\")","c76e9746":"def play_sound(data_df, idx):\n    audio_file_path = '\/kaggle\/input\/rfcx-species-audio-detection\/train\/'+data_df.recording_id[idx]+'.flac'\n    return ipd.Audio(audio_file_path)","58e6413b":"plot_audio_file(data_tp, 20)","cbbf6eff":"plot_audio_file(data_tp, 13)","8b1c38e4":"plot_audio_file(data_tp, 1)","921a991c":"plot_audio_file(data_tp, 5)","76e9776f":"plot_chroma_feature(data_tp, 20)","41acfc72":"plot_chroma_feature(data_tp, 12)","adc68b9b":"plot_harmonics_and_perceptual(data_tp, 20)","06782ef5":"plot_harmonics_and_perceptual(data_tp, 2)","3f96aa29":"plot_harmonics_and_perceptual(data_tp, 17)","57f0a8af":"play_sound(data_tp, 20)","1e64b331":"play_sound(data_tp, 12)","f745d69f":"plot_audio_file(data_fp, 10)","f305a1d6":"plot_spectrogram(data_fp, 10)","bc8db9ec":"fig= plot_mel_spectrogram(data_fp, 10)\n","c0f59c16":"fig= plot_mel_spectrogram(data_fp, 13)","ef516a52":"plot_chroma_feature(data_fp, 10)","32c6eb61":"play_sound(data_fp, 10)","c34df67a":"plot_audio_file(data_fp, 70)","9275671a":"# parameters for data saving as .npy files\n\nclass MFCC:\n    # number of MFCC coeffcs\n    n_mfcc = 32\n    # hop length\n    hop_length = 512\n    pass\n\nclass FFT:\n    pass\n\nclass STFT:\n    pass\n\nclass MelSpec:\n    pass","35a37368":"import os\ntrain = \"..\/input\/rfcx-species-audio-detection\/train\" \ntest = \"..\/input\/rfcx-species-audio-detection\/test\"\ntrain_files = os.listdir(train)\ntest_files = os.listdir(test) \noutput_dir = \".\/\"","ae8e2f26":"%%time\n\nfrom librosa import feature as lf\n\n# take sample example for understanding\n\nsample_path = os.path.join(train, data_tp.loc[np.random.randint(0, len(data_tp)), \"recording_id\"] + \".flac\")\nsample, sr = librosa.load(sample_path)\n\n# target features\n# mfcc\nmfcc_sample_20 = lf.mfcc(sample, sr=sr)\nprint(\"shape of the mfcc_sample with 20 Coeff: \", mfcc_sample_20.shape)\nplt.figure(figsize=(15, 7))\nlibrosa.display.specshow(mfcc_sample_20, sr=sr, x_axis='time')\nplt.show()\n\nmfcc_sample = lf.mfcc(sample, sr=sr, n_mfcc=MFCC.n_mfcc)\nprint(f\"shape of the mfcc_sample with {MFCC.n_mfcc} Coeff: \", mfcc_sample.shape)\nplt.figure(figsize=(15, 7))\nlibrosa.display.specshow(mfcc_sample, sr=sr, x_axis='time')\nplt.show()","1f3348be":"# source : Librosa documentations\n# Set the hop length; at 22050 Hz, 512 samples ~= 23ms\nhop_length = 512\n\n# Separate harmonics and percussives into two waveforms\ny_harmonic, y_percussive = librosa.effects.hpss(sample)\n\n# Beat track on the percussive signal\ntempo, beat_frames = librosa.beat.beat_track(y=y_percussive,\n                                             sr=sr)\n\n# Compute MFCC features from the raw signal\nmfcc = librosa.feature.mfcc(y=sample, sr=sr, hop_length=hop_length, n_mfcc=32)\n\n# And the first-order differences (delta features)\nmfcc_delta = librosa.feature.delta(mfcc)\n\n# Stack and synchronize between beat events\n# This time, we'll use the mean value (default) instead of median\nbeat_mfcc_delta = librosa.util.sync(np.vstack([mfcc, mfcc_delta]),\n                                    beat_frames)\n\n# Compute chroma features from the harmonic signal\nchromagram = librosa.feature.chroma_cqt(y=y_harmonic,\n                                        sr=sr)\n\n# Aggregate chroma features between beat events\n# We'll use the median value of each feature between beat frames\nbeat_chroma = librosa.util.sync(chromagram,\n                                beat_frames,\n                                aggregate=np.median)\n\n# Finally, stack all beat-synchronous features together\nbeat_features = np.vstack([beat_chroma, beat_mfcc_delta])","0f9bde81":"beat_features.shape","f58bb23d":"def process_mfcc(idx, data):\n    file_path = os.path.join(data, idx)\n    assert os.path.exists(file_path), file_path\n    \n    # load the pcm audio and sr\n    data_wav, data_sr = librosa.load(file_path)\n    # separate harmonics and percussives\n    data_harmonic, data_precussive = librosa.effects.hpss(sample)\n    # beat track on the precussive signal\n    tempo, beat_frames = librosa.beat.beat_track(y=data_precussive, sr=sr)\n    # compute mfcc\n    data_mfcc = lf.mfcc(data_wav, sr=data_sr, n_mfcc=MFCC.n_mfcc, hop_length=MFCC.hop_length)\n    \n    # dynamic mfcc features\n    # delta-mfcc -> first order derivative\n    mfcc_delta = librosa.feature.delta(data_mfcc)\n    # stack and synchronize beat events\n    beat_mfcc_delta = librosa.util.sync(np.vstack([data_mfcc, mfcc_delta]), beat_frames)\n    # chroma features\n    chroma = librosa.feature.chroma_cqt(y=data_harmonic, sr=sr)\n    # aggregrate chroma features\n    beat_chroma = librosa.util.sync(chroma,\n                                   beat_frames,\n                                   aggregate=np.median)\n    # stack all the features\n    beat_features = np.vstack([beat_chroma, beat_mfcc_delta])\n    beat_features = np.expand_dims(beat_features, axis=2)\n    assert beat_features.ndim == 3, beat_features.shape\n                \n    fn = idx.split(\".\")[0]\n    np.save(os.path.join(target_sub_dir, f\"{fn}\" + \".npy\"), beat_features)\n    assert os.path.exists(os.path.join(target_sub_dir, f\"{fn}\" + \".npy\")), os.path.join(target_sub_dir, f\"{fn}\" + \".npy\")\n    pass\n\n\n","265e187f":"def save_mfcc(folder=\"MFCC\", tag=\"train\"):\n    target_dir = os.path.join(folder, tag)\n    global target_sub_dir \n    target_sub_dir = os.path.join(output_dir, target_dir)\n    \n    # if exists, delete\n    if os.path.exists(target_sub_dir):\n        print(\"Deleting existing folder...\")\n        shutil.rmtree(target_sub_dir)\n        assert not os.path.exists(target_sub_dir), os.path.exists(target_sub_dir)\n       \n    if not os.path.exists(target_sub_dir):\n        os.makedirs(target_sub_dir)\n        \n        batch =0\n    if tag==\"train\":\n        print(\"\\n\", \"=\"*50)\n        print(\"Creating MFCC features from train files\")\n        Parallel(-1, verbose=1)(delayed(partial(process_mfcc, data=train))(x) for x in train_files[batch:batch+1000]) #1050 to 2050\n        pass\n    else:\n        print(\"\\n\", \"=\"*50)\n        print(\"Creating MFCC features from test audio files\")\n        Parallel(-1, verbose=1)(delayed(partial(process_mfcc, data=test))(x) for x in test_files[batch:batch+1000])\n        pass\n\n    pass\n\n#n_jobs=-1 determines the number of jobs to use which in parallel doesn't work on windows all the time.\n#n_jobs = -2, all CPUs but one are used. \n#If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. \n","75bb339d":"%%time \n\nprint(\"[INFO]Creating training MFCCs...\")\nsave_mfcc()\nprint(\"[INFO]Creating test MFCCs...\")\nsave_mfcc(tag=\"test\")\n\nprint(\"=\"*50)\nprint(\"Total Train files (TP+FP): \", len(os.listdir(\".\/MFCC\/train\")))\nprint(\"Total Test files: \", len(os.listdir(\".\/MFCC\/test\")))","ff318e1b":"#def save_in_batches(start, end, df = data,segments = 6):\n    \n   # batch_segments = {}\n    \n   # for i in range(segments):\n      #  batch_segments[i] = []\n        \n   # for j in tqdm(range(start,end)):\n      #  process_mfcc(idx, data)\n       # save_mfcc(folder=\"MFCC\", tag=\"train\")\n    #return batch_segments","3fce2860":"- The first step in any automatic speech recognition system is to extract features i.e. identify the components of the audio signal that are good for identifying the linguistic content and discarding all the other stuff which carries information like background noise, emotion etc.\n\n- The feature that is useful to extract is the Mel Frequency Cepstral Coefficients (MFCCs). This can give you information about the timbral\/textural aspects of the audio, and approximate how the human auditory system interprets sound. This is especially useful in speech recognition, but could prove very important for this competition as well!\n- http:\/\/practicalcryptography.com\/miscellaneous\/machine-learning\/guide-mel-frequency-cepstral-coefficients-mfccs\/\n","3be6da09":"<h1 style='background:#3FBBBB; border:0; color:black'><center> Column Description <\/center><\/h1> ","0e59682d":"Remember that this is the magnitude of the frequencies throughout the whole duration of the audio. A more useful graph would show us what frequencies are present on a time axis. One idea to implement this would be to make a bunch of these frequency-domain graphs for short durations in the audio, and then combine them together to form a time axis. This is what Short Time Fourier Transforms (STFT) do, and the visuals we can produce with this are know as spectrograms.","94ff32ab":"<h1 style='background:#A3C6AD; border:0; color:black'><center>Mel Frequency Cepstal Coefficients (MFCCs)<\/center><\/h1> ","38e0ab82":"<h1 style='background:#E3C6AD; border:0; color:black'><center>Training and Test Data set<\/center><\/h1> ","a7b73c3b":"# Displaying a Waveform\u00b6","8920b1bb":"- Total Train files (TP+FP):  4727\n- Total Test files:  1992\n- CPU times: user 1min 34 sec s, sys: 2.84 s, total: 59.5 s\n- Wall time: 5h 54min 35s","7694d24f":"<h1 style='background:#A23C4D; border:0; color:white'><center> Import Libraries and Packages<\/center><\/h1> ","5770eac9":"<h1 style='background:#B3C6ED; border:0; color:black'><center>MFCC features Engineering \u00b6<\/center><\/h1> \n\n### 1.  Separate harmonics and percussives into two waveforms#\nWhen listening to our environment, there exists a wide variety of different sounds. However, on a\nvery coarse level, many sounds can be categorized to belong in either one of two classes: harmonic\nor percussive sounds. Harmonic sounds are the ones which we perceive to have a certain pitch\nsuch that we could for example sing along to them. The sound of a violin is a good example of a\nharmonic sound. Percussive sounds often stem from two colliding objects like for example the two\nshells of castanets. An important characteristic of percussive sounds is that they do not have a\npitch but a very clear localization in time. Many real-world sounds are mixtures of harmonic and\npercussive components. For example, a note played on a piano has a percussive onset (resulting\nfrom the hammer hitting the strings) preceding the harmonic tone (resulting from the vibrating\nstring).\n\n### 2.  Beat track on the percussive signal \nWhat is Beat tracking?\nAudio beat tracking is commonly defined as determining the time instances in an audio recording, where a human listener is likely to tap his\/her foot to the music. Audio beat tracking enables the \u201cbeat-synchronous\u201d analysis of music.","73c7bea1":"<h1 style='background:#B3C6ED; border:0; color:black'><center>MFCC Coefficients \u00b6<\/center><\/h1> ","e9585f13":"# Chroma\nA chroma vector (Wikipedia) (FMP, p. 123) is a typically a 12-element feature vector indicating how much energy of each pitch class, {C, C#, D, D#, E, ..., B}, is present in the signal.\n\nChroma energy normalized statistics (CENS) (FMP, p. 375). The main idea of CENS features is that taking statistics over large windows smooths local deviations in tempo, articulation, and musical ornaments such as trills and arpeggiated chords. CENS are best used for tasks such as audio matching and similarity.","9282adf2":"<h2 style='background:#B67B65; border:0; color:black'><center>Sound samples from FP set<\/center><\/h2> ","d9801c7d":"<h1 style='background:#E3C6AD; border:0; color:black'><center>Sounds of the rainforest<\/center><\/h1> ","50416ab8":"<h1 style='background:#1CC788; border:0; color:black'><center> Importing Data<\/center><\/h1> ","e372394d":"<h1 style='background:#07D700; border:0; color:black'><center> Data Visualization<\/center><\/h1> ","4eefb3df":"<h2 style='background:#B65EA3; border:0; color:black'><center>Sound samples from TP set<\/center><\/h2> ","f2dd4c6d":"- recording_id - unique identifier for recording\n- species_id - unique identifier for species\n- songtype_id - unique identifier for songtype\n- t_min - start second of annotated signal\n- f_min - lower frequency of annotated signal\n- t_max - end second of annotated signal\n- f_max - upper frequency of annotated signal\n- is_tp - [tfrecords only] an indicator of whether the label is from the train_tp (1) or train_fp (0) file.","2aff0d5b":"# Understanding the audio data\n- https:\/\/towardsdatascience.com\/understanding-audio-data-fourier-transform-fft-spectrogram-and-speech-recognition-a4072d228520\n- https:\/\/medium.com\/x8-the-ai-community\/audio-classification-using-cnn-coding-example-f9cbd272269e\n\nHere we define functions for display of:\n- Waveplots\n- Spectrograms  \n- Mel spectrograms  \n- Chroma feature\n- Harmonics and Perceptual sound wave components\n\nAnd for playing the sound.\n\nWe then show Waveplots, Spectrograms, Mel Spectrograms, Chroma feature and combined Harmonics and Perceptual graphs for few of the recordings, from both the TP and FP train sets.","1980cb52":"<h1 style='background:#E3C6AD; border:0; color:black'><center> Distribution of f_min and f_max <\/center><\/h1> ","36ec39ca":"# Define the model layers\nAs seen above 1 audio has two kinds of images associated with it.\nAudio signal : Amplitude v\/s Time\nSpectrogram : Freqeuncy Content v\/s Time\n\nLogically both of them can be used to train our CNN. We tried doing that and observed that pure audio signal yields a test-accuracy low as compared to the spectrograms. We will use our beat frame features .npy output here. \n\nThe CNN we use has the following layers:\n1. Convolution layer with kernel size : 3x3\n2. Convolution layer with kernel size : 3x3\n3. Max Pooling layer with pool size : 2x2\n4. Dropout layer\n5. Flattening layer\n6. 2 Dense layered Neural Network at the end","b7b647aa":"### 84 recordings ids are repeated in this data.\n\n### Dropping the repeated ones:"}}