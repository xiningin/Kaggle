{"cell_type":{"45cea2a8":"code","b82e6938":"code","dde1bf32":"code","1d9ac568":"code","08fc95eb":"code","69f8fa0d":"code","37b7a741":"code","de2ca76d":"code","414a5f3f":"code","47edd6ea":"code","8293cfc6":"code","0ef313c2":"code","a7f13863":"markdown","0b86caef":"markdown","5354a1c6":"markdown","62ff0c94":"markdown","ed84882f":"markdown","1ea41a65":"markdown","a14d94ad":"markdown","83cbf2e1":"markdown"},"source":{"45cea2a8":"import pandas as pd\nimport numpy as np","b82e6938":"df = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/train.csv\", header=0, index_col=0)\ndf.head(10)","dde1bf32":"# Get the feature and target columns\nfeature_cols = df.columns[:-1]\ntarget_cols = df.columns[-1]\n\n# Get the data as a numpy matrix\nfeatures = df[feature_cols].to_numpy()\ntarget = df[target_cols].to_numpy()\nprint(f\"Data shape: features -> {features.shape}, and Target -> {target.shape}\") ","1d9ac568":"from sklearn.preprocessing import StandardScaler","08fc95eb":"# Create the StandardScaler object\nscaler = StandardScaler()\n\n# Transform the features\nfeatures = scaler.fit_transform(features)","69f8fa0d":"# Check if it worked or not\nfeatures_mean = np.mean(features, axis=0)\nfeatures_std = np.std(features, axis=0)\n\nprint(f\"Feature mean: {features_mean} \\n Features standard deviation: {features_std}\")","37b7a741":"# Get the covariance matrix\nfeatures_cov = np.cov(features.T)\n\n# Apply eigen value decomposition\neigen_vals, eigen_vecs = np.linalg.eig(features_cov)\n\n# normalize the eigen values to compare in %\neigen_vals \/= np.sum(eigen_vals)\neigen_vals *= 100","de2ca76d":"import seaborn as sns\nimport matplotlib.pyplot as plt","414a5f3f":"plt.figure(figsize = [35, 7])\nep = sns.scatterplot(x = feature_cols, y = eigen_vals, hue = np.log(eigen_vals), legend = False)","47edd6ea":"# Get the sort indexs\nidx = np.flip(np.argsort(eigen_vals))\n\n# plot the sorted eigen values and corresponsing features\nplt.figure(figsize = [35, 7])\nep = sns.scatterplot(x = feature_cols[idx], y = eigen_vals[idx], hue = np.log(eigen_vals[idx]), legend = False)","8293cfc6":"eigen_vals_cumulative = np.cumsum(eigen_vals[idx])\nplt.figure(figsize = [35, 7])\ngraph = sns.scatterplot(x = feature_cols[idx], y = eigen_vals_cumulative, hue = np.log(eigen_vals_cumulative), legend = False)\ngraph.axhline(99)\nplt.show()","0ef313c2":"sns.histplot(x = target)","a7f13863":"#### Lets look at the cumulative behaviour of eigen values for each feature","0b86caef":"## Huge chunk of features let's analyse which feature contains more rational information about data\n\n### Applying Principal Component Analysis(PCA)\n\n- PCA is applied to analyse that how much of information about data is contained in a certain feature\n- Basically PCA is Eigen Value Decomposition (eigen value for each feature defines the variance contained by that feature)\n- Sometimes EVD is replaced with Singular Value Decomposition (SVD). In this the Singular values acts similar to eigen values.","5354a1c6":"#### Sort eigen values and features for clear visualization","62ff0c94":"### Visualize the eigen values","ed84882f":"### Normalize the features\n\n- Normalization is something that is needed by PCA.\n- PCA is suceptible to variance in feature observations.\n- If some features have significantly different scales, then the features with lower scale can get supressed while decomposition.\n- We need to transform each feature to have standard deviation 1 unit, and mean to 0 unit.","1ea41a65":"## Read the dataset\n\n- Using `pd.read_csv` for reading the tabular data from csv files.\n- The methods returns a pandas `DataFrame` object, that can be explored in an interactive manner. For more details [follow](https:\/\/pandas.pydata.org\/pandas-docs\/dev\/reference\/api\/pandas.DataFrame.html?highlight=dataframe#)\n- `read_csv` function returns an iterable object when provided with `chunksize: int` or `iterable: True`.\n- Using `next` functionality we can iterate through the object to get data wit `chunksize`","a14d94ad":"### Apply EVD","83cbf2e1":"> We can easily observe in the previous figure that the change in cumulative behaviour of eigen values is gradual through out the feature space(simply, the 99% data retention is only on the cost of a single feature). Thus the PCA based feature selection won't be useful and if applied will create problem of data loss, without significant advantage on feature dismissal."}}