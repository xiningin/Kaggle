{"cell_type":{"1102ef33":"code","809a6049":"code","1cf465d4":"code","14da6058":"code","7f093249":"code","33e87503":"code","aa49cf3e":"code","1df613da":"code","015cb324":"code","6797623e":"code","554ac637":"code","ed23c84c":"code","7531c5f0":"code","c2430641":"code","506e29e2":"code","93c83193":"code","ad3e4801":"code","aa68e174":"code","6ca417e7":"code","728f4542":"code","5a4bbd42":"code","9c861f72":"code","b870fe13":"code","e7210cb9":"code","78a5bb7b":"code","d3bb2a92":"code","a498c8e8":"code","e5aa0719":"code","6568d039":"code","e7af21e9":"code","0146bdf0":"code","4427de86":"code","a364199a":"code","73bf3d1e":"code","448f876a":"code","0faad777":"code","96e3489f":"code","698b004b":"code","711ed683":"code","222d2a8e":"code","d0a154af":"code","4c3106cd":"code","73791956":"code","714ef7ee":"code","b906360e":"code","4a068cc1":"code","758798c2":"code","529a6295":"code","01550bf5":"code","77e8263c":"code","9d42c7de":"code","a468ca6a":"code","6f8f65dd":"code","3e05990f":"code","42202822":"code","ea14a72d":"code","ab4407a2":"code","aa84dc4b":"code","9209eefd":"code","90ca8a26":"code","c06b890f":"code","d1bcbab5":"code","a70ce6e9":"code","89edc928":"code","205ab797":"code","375c196d":"code","530ce8dc":"code","80f55781":"code","3ffef058":"code","7572d0f7":"code","4e3044f1":"code","ea86cbd8":"code","6df7bb4b":"code","1a48afa6":"code","49228b84":"code","e6fef5ec":"code","b564e36c":"code","08bf3c8c":"code","dcad3134":"code","b8ba863b":"code","7999d3e7":"code","f3d3314b":"code","71359605":"code","4d2768b3":"code","33ae4d68":"code","84ae8f1c":"code","38352cbb":"code","344507c4":"markdown","ecd45eb6":"markdown"},"source":{"1102ef33":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(action='ignore')","809a6049":"data=pd.read_csv('..\/input\/diabetes.csv')","1cf465d4":"data.isna().sum()","14da6058":"data.head(6)","7f093249":"data.info()","33e87503":"data.describe()","aa49cf3e":"data.shape","1df613da":"data['Outcome'].value_counts()","015cb324":"#univariate Analysis\ndata.iloc[:,:-1].hist(bins=20, figsize=(20,10), grid=False, edgecolor='black', alpha=0.5, color='pink')","6797623e":"sns.countplot(data['Outcome'], palette='coolwarm')","554ac637":"###################################################################################################","ed23c84c":"#Upsampling\n#As the data is less for outcome 1 so its better if we do upsampling for data\nfrom sklearn.utils import resample\ndata_majority= data.loc[data['Outcome']==0]\ndata_minority= data.loc[data['Outcome']==1]","7531c5f0":"data_majority.shape","c2430641":"data_minority_resampled= resample(data_minority, replace=True, n_samples=500, random_state=0)","506e29e2":"data1= pd.concat([data_majority, data_minority_resampled])","93c83193":"data1['Outcome'].value_counts()","ad3e4801":"#######################################################################################################","aa68e174":"#BOXPlot #univariate analysis\nfig,ax = plt.subplots(nrows=2, ncols=4, figsize=(20,10))\nfor i in range(0,4):\n    sns.boxplot(y=data1.iloc[:,i], ax=ax[0,i])\nfor j in range(4,8):\n    sns.boxplot(y=data1.iloc[:,j], ax=ax[1,j-4])\n#it seems that there are outliers in my dataset. But lets try to build model with outliers only            ","6ca417e7":"#Bivariate Analysis\nsns.pairplot(data, hue='Outcome', diag_kind='kde')","728f4542":"#Predictive Modelling\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB","5a4bbd42":"X= data1.iloc[:,:-1]\nY= data1.iloc[:,-1]","9c861f72":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test= train_test_split(X, Y, random_state=0, test_size=0.25)","b870fe13":"#Logistic Regression\nlogreg= LogisticRegression()\nlogreg.fit(X_train, Y_train)\nfrom sklearn import metrics\nmetrics.accuracy_score(Y_test, logreg.predict(X_test))","e7210cb9":"#SVM\nsvc= SVC(kernel='rbf', random_state=1)\nsvc.fit(X_train, Y_train)\nfrom sklearn import metrics\nmetrics.accuracy_score(Y_test, svc.predict(X_test))","78a5bb7b":"#Decisison tree\ndtree= DecisionTreeClassifier(criterion='entropy', random_state=0)\ndtree.fit(X_train, Y_train)\nmetrics.accuracy_score(Y_test, dtree.predict(X_test))","d3bb2a92":"#Random Forest\nrforest= RandomForestClassifier(n_estimators=50, criterion='entropy', random_state=0)\nrforest.fit(X_train, Y_train)\nmetrics.accuracy_score(Y_test, rforest.predict(X_test))","a498c8e8":"#naive Bayes\nnb= GaussianNB()\nnb.fit(X_train, Y_train)\nmetrics.accuracy_score(Y_test, nb.predict(X_test))","e5aa0719":"#KNN\n#using optimum value of k to predict the accuracy \naccuracy=[]\nfor k in np.arange(2,20,1):\n    knn= KNeighborsClassifier(n_neighbors=k, metric='minkowski', p=2)\n    knn.fit(X_train, Y_train)\n    acc=metrics.accuracy_score(Y_test, knn.predict(X_test))  \n    accuracy.append(acc)\n    \nprint(accuracy)    ","6568d039":"plt.plot(np.arange(2,20,1), accuracy)\nplt.xlim(1,21)","e7af21e9":"#KNN\n#taking sqrt of len of X_train values as k value\nknn= KNeighborsClassifier(n_neighbors=int(np.sqrt(X_train.shape[0])), metric='minkowski', p=2)\nknn.fit(X_train, Y_train)\nmetrics.accuracy_score(Y_test, knn.predict(X_test))","0146bdf0":"###############################################################################################################","4427de86":"#In a nutshell\nac= []\nlist=[LogisticRegression(), SVC(), DecisionTreeClassifier(), RandomForestClassifier(), KNeighborsClassifier(), GaussianNB()]\nfor i in list:\n    model= i\n    model.fit(X_train, Y_train)\n    a=metrics.accuracy_score(Y_test, model.predict(X_test))\n    ac.append(a)\n    \nprint(pd.Series(data=ac, index=['LogisticRegression', 'SVC', 'DecisionTreeClassifier', 'RandomForestClassifier', 'KNeighborsClassifier', 'NaiveBayes']))   ","a364199a":"##########################################################################################################################","73bf3d1e":"#Lets build each model in depth\n#check the corr\ndata.corr()","448f876a":"#Heatmap\nsns.heatmap(data.corr(), linecolor='black', linewidths=0.2, annot=True)","0faad777":"#Logistic regression\nlogreg1= LogisticRegression()\nlogreg1.fit(X_train, Y_train)","96e3489f":"logreg1.predict(X_test)","698b004b":"metrics.confusion_matrix(Y_test, logreg1.predict(X_test))","711ed683":"metrics.accuracy_score(Y_test, logreg1.predict(X_test))","222d2a8e":"logreg1.coef_.ravel()","d0a154af":"plot=sns.barplot(x=X_train.columns.tolist(),y=logreg1.coef_.ravel())\nplot.set_xticklabels(plot.get_xticklabels(), rotation=90)","4c3106cd":"#as per coffecient using only 4 features to predict the linear model\na=pd.Series(logreg1.coef_.ravel(), index=X_train.columns.tolist()).sort_values(ascending=False)\na","73791956":"var1= a.head(4).index.tolist()\nvar1","714ef7ee":"logreg2= LogisticRegression()\nlogreg2.fit(X_train[var1], Y_train)\nmetrics.confusion_matrix(Y_test, logreg2.predict(X_test[var1]))","b906360e":"print('The model accuracy for Log regression is {}'.format(metrics.accuracy_score(Y_test, logreg2.predict(X_test[var1]))))","4a068cc1":"#ROC-AUC curve\nfrom sklearn.metrics import auc, roc_auc_score, roc_curve\nfpr, tpr, thres= roc_curve(Y_test, logreg2.predict_proba(X_test[var1])[:,1])","758798c2":"roc_auc= roc_auc_score(Y_test, logreg2.predict(X_test[var1]))\nprint(roc_auc)\nplt.plot(fpr, tpr, label='area={}'.format(roc_auc))\nplt.plot(np.arange(0,1.1,0.1), np.arange(0,1.1,0.1), linestyle='--')\nplt.legend()\nplt.show()","529a6295":"############################################################################################################################","01550bf5":"#SVC with rbf kernel\nsvc1= SVC(kernel='rbf',random_state=1)\nsvc1.fit(X_train, Y_train)\nmetrics.accuracy_score(Y_test, svc1.predict(X_test))","77e8263c":"from sklearn.model_selection import cross_val_score\nsvc1= SVC(kernel='rbf',random_state=1)\ncv_score= cross_val_score(svc1, X, Y, cv=10)\ncv_score","9d42c7de":"#lets try to find optimum value of C for Linear kernel\nacc=[]\nfor c in np.arange(0.1,100,10).tolist():\n    svc2= SVC(kernel='rbf',C=c,random_state=1)\n    cv_score= cross_val_score(svc2, X, Y, cv=10)\n    acc.append(cv_score.mean())\n    \nprint(acc)    ","a468ca6a":"plt.plot(np.arange(0.1,100,10).tolist(), acc)","6f8f65dd":"# C value in between 1 to 10\nacc=[]\nfor c in np.arange(1,10,1).tolist():\n    svc2= SVC(kernel='rbf',C=c,random_state=1)\n    cv_score= cross_val_score(svc2, X, Y, cv=10)\n    acc.append(cv_score.mean())\n    \nprint(acc)    \n#lets take c=1","3e05990f":"#optimal value of gaama\nac=[]\ngaama=[0.0001, 0.001, 0.01, 0.1, 1, 10]\nfor g in gaama:\n    svc2= SVC(kernel='rbf',gamma=g ,random_state=1)\n    cvs_score=cross_val_score(svc2, X, Y, cv=10)\n    ac.append(cvs_score.mean())\n    \nprint(ac)    ","42202822":"plt.plot(gaama, ac)","ea14a72d":"#lets perform Grid search cv for optimum value of c and gaama\nfrom sklearn.model_selection import GridSearchCV\nparam_grid={'C':[0.1, 1, 10, 100], 'gamma':[1,0.1,0.01,0.001],'kernel':['rbf']}\nsvc_model=SVC()\ngrid= GridSearchCV(svc_model, param_grid=param_grid, cv=5, refit=True)","ab4407a2":"grid.fit(X_train, Y_train)\ngrid.predict(X_test)\nmetrics.accuracy_score(Y_test, grid.predict(X_test))","aa84dc4b":"#Print Hyperparameter\nprint('Best parameter : {}'.format(grid.best_params_))\nprint('Best Score: {}'.format(grid.best_score_))","9209eefd":"########################################################################################################","90ca8a26":"#Decision tree\nfrom sklearn.tree import DecisionTreeClassifier\ndt= DecisionTreeClassifier(criterion='entropy', random_state=1)","c06b890f":"dt.fit(X_train, Y_train)\nmetrics.accuracy_score(Y_test, dt.predict(X_test))","d1bcbab5":"from sklearn.metrics import roc_auc_score\ndt_roc_auc= roc_auc_score(Y_test,dt.predict_proba(X_test)[:,1])\ndt_roc_auc","a70ce6e9":"from sklearn.metrics import roc_curve\nfpr, tpr, thres= roc_curve(Y_test,dt.predict_proba(X_test)[:,1])","89edc928":"plt.plot(fpr, tpr)\nplt.plot(np.arange(0,1.1,0.1), np.arange(0,1.1,0.1), linestyle='--')","205ab797":"#lets draw the tree to analyse better\nfrom sklearn.externals.six import StringIO\nfrom IPython.display import Image\nfrom sklearn.tree import export_graphviz\nfrom sklearn import tree\nimport pydot","375c196d":"#Create DOT data\ndot_data= StringIO()\nexport_graphviz(dt, out_file=dot_data, feature_names=X_train.columns.tolist(), class_names=['0','1'], rounded=True, filled=True)\n\n#Draw Graph\ngraph= pydot.graph_from_dot_data(dot_data.getvalue())\n\n#Show Graph\nImage(graph[0].create_png())","530ce8dc":"plot1=sns.barplot(X_train.columns.tolist(), dt.feature_importances_)\nplot1.set_xticklabels(plot.get_xticklabels(), rotation=90)","80f55781":"#Now lets try to tune the hyper parameters in Decision tree. We have 4 parameters to tune\n#1 max_depth\n#2 min_samples_split\n#3 min_samples_leaf\n#4 max_features","3ffef058":"#Lets keep max_depth from 1 to 10 and check accuracy\nmax_depth= np.linspace(1,20,20).tolist()\ntrain_result=[]\ntest_result=[]\nfor max_depth in max_depth:\n    dtr=DecisionTreeClassifier(criterion='entropy', max_depth=max_depth)\n    dtr.fit(X_train, Y_train)\n    \n    acc_train= metrics.accuracy_score(Y_train, dtr.predict(X_train))\n    acc_test= metrics.accuracy_score(Y_test, dtr.predict(X_test))\n    \n    train_result.append(acc_train)\n    test_result.append(acc_test)\n    \nprint(train_result)\nprint('\\n')\nprint(test_result) \nplt.bar(np.linspace(1,20,20).tolist(), train_result, )\nplt.bar(np.linspace(1,20,20).tolist(), test_result, color='pink')\n","7572d0f7":"#Lets check min_samples_split to tune the model\n#lets take it to 10 to 100%\nmin_samples_split= np.linspace(0.1,1,10).tolist()\ntrain_result=[]\ntest_result=[]\nfor min_samples_split in min_samples_split:\n    dtr=DecisionTreeClassifier(criterion='entropy', min_samples_split=min_samples_split)\n    dtr.fit(X_train, Y_train)\n    \n    acc_train= metrics.accuracy_score(Y_train, dtr.predict(X_train))\n    acc_test= metrics.accuracy_score(Y_test, dtr.predict(X_test))\n    \n    train_result.append(acc_train)\n    test_result.append(acc_test)\n    \nprint(train_result)\nprint('\\n')\nprint(test_result)\nplt.bar(np.arange(0.1, 1.1, 0.1).tolist(), train_result, edgecolor='black',linewidth=0.2, width=0.08)\nplt.bar(np.arange(0.1, 1.1, 0.1).tolist(), test_result, color='pink', edgecolor='black',linewidth=0.2, width=0.08)\n","4e3044f1":"#Lets check max_features to tune the model\n\nmax_features= np.arange(1, (len(X.columns.tolist())+1)).tolist()\ntrain_result=[]\ntest_result=[]\nfor max_features in max_features:\n    dtr=DecisionTreeClassifier(criterion='entropy', max_features= max_features)\n    dtr.fit(X_train, Y_train)\n    \n    acc_train= metrics.accuracy_score(Y_train, dtr.predict(X_train))\n    acc_test= metrics.accuracy_score(Y_test, dtr.predict(X_test))\n    \n    train_result.append(acc_train)\n    test_result.append(acc_test)\n    \nprint(train_result)\nprint('\\n')\nprint(test_result)\nplt.bar(np.arange(1, (len(X.columns.tolist())+1)).tolist(), train_result, edgecolor='black',linewidth=0.2, )\nplt.bar(np.arange(1, (len(X.columns.tolist())+1)).tolist(), test_result, color='pink', edgecolor='black',linewidth=0.2)\n","ea86cbd8":"#Grid Search CV\nfrom sklearn.model_selection import GridSearchCV\nparam={'max_depth':np.arange(1,21).tolist(), 'min_samples_split':np.linspace(0.1,1,10).tolist(),\n       'max_features':[\"auto\", \"sqrt\",\"log2\"]}\nmodel= DecisionTreeClassifier(criterion='entropy', )\ngrid= GridSearchCV(model, param_grid=param, refit=True)","6df7bb4b":"grid.fit(X_train, Y_train)","1a48afa6":"grid.best_params_","49228b84":"grid_pred= grid.predict(X_test)\ngrid_pred","e6fef5ec":"print('Tuned HyperParameter K : {}'.format(grid.best_params_))\nprint('Best Score : {}'.format(grid.best_score_))\nprint('Accuracy Score: {}'.format(metrics.accuracy_score(Y_test, grid_pred)))","b564e36c":"##################################################################################################################","08bf3c8c":"#Random forest\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nacc=[]\nfor i in np.arange(10,300,10).tolist():\n    rfr= RandomForestClassifier(n_estimators=i, criterion='entropy', random_state=1)\n    rfr.fit(X_train, Y_train)\n    a= metrics.accuracy_score(Y_test, rfr.predict(X_test))\n    acc.append(a)\n\nprint(acc)    ","dcad3134":"plt.plot(np.arange(10,300,10), acc)\nplt.show()\n#Seems like n_estimator= 20 has max accuracy","b8ba863b":"#Grid Search CV\nrfr= RandomForestClassifier(n_estimators=20, criterion='entropy', random_state=1)\nparam_grid= {'max_depth':np.arange(1,21).tolist(), 'min_samples_split':np.linspace(0.1,1,10).tolist(), \n             'max_features':[\"auto\", \"sqrt\",\"log2\"]}\nfrom sklearn.model_selection import GridSearchCV\ngrid_random= GridSearchCV(rfr, param_grid=param_grid, refit=True)","7999d3e7":"grid_random.fit(X_train, Y_train)","f3d3314b":"print('Tuned HyperParameter K : {}'.format(grid.best_params_))\nprint('Best Score : {}'.format(grid.best_score_))\nprint('Accuracy Score: {}'.format(metrics.accuracy_score(Y_test, grid_random.predict(X_test))))","71359605":"###################################################################################################################","4d2768b3":"#Using boosting techniques\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn import model_selection","33ae4d68":"#Adaboost Classifier\nkfold = model_selection.KFold(n_splits=10, random_state=5)\nmodel1= AdaBoostClassifier(n_estimators=20, random_state=5)\ncvscore= model_selection.cross_val_score(model1, X, Y, cv=kfold)\ncvscore.mean()","84ae8f1c":"#XGBoost Classifier\nk= model_selection.KFold(n_splits=10, random_state=10)\nmodel2= XGBClassifier(random_state=10)\ncvs= model_selection.cross_val_score(model2, X, Y, cv=k)\ncvs.mean()","38352cbb":"#GradientBoostingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nmodel3= GradientBoostingClassifier(n_estimators=150, random_state=1)\nmodel3.fit(X_train, Y_train)\nmetrics.accuracy_score(Y_test, model3.predict(X_test))","344507c4":"# Pregnancies Number of times pregnant\n#Glucose Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n#BloodPressure Diastolic blood pressure (mm Hg)\n#SkinThickness Triceps skin fold thickness (mm)\n#Insulin 2-Hour serum insulin (mu U\/ml)\n#BMI Body mass index (weight in kg\/(height in m)^2)\n#DiabetesPedigreeFunction Diabetes pedigree function\n#Age Age (years)\n#Outcome Class variable (0 or 1) 268 of 768 are 1, the others are 0","ecd45eb6":"######Performing K fold validations with diff kernels#######"}}