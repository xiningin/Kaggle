{"cell_type":{"701911df":"code","c4727f50":"code","d8bc1784":"code","0db3caa2":"code","26bef45a":"code","19aeb134":"code","33d43bcc":"code","e7208b4d":"code","77bfdf71":"code","3ea140eb":"code","46629f17":"code","8aa50f12":"code","99770c5a":"code","42d03bce":"code","6f38389e":"code","0ed4e2a1":"code","a26dd7f8":"code","7ae78aa6":"code","93a4c2ba":"code","5eb44cc4":"code","c0e81f8b":"code","fb987ea0":"code","18bd16f4":"code","a4928c88":"code","9bba3181":"code","36fd0776":"code","bdc328a4":"code","0a934de4":"code","0b3fdc89":"code","2611e2d7":"code","e2f8e3e2":"code","99c12807":"code","fd342b04":"code","386e70ec":"code","47aa2e08":"code","5eeb3a84":"code","9d21b5ce":"code","adaf00d2":"code","534aab1a":"code","f08291e0":"code","8a3175a2":"code","6b057256":"code","067c7256":"code","24f3b6fc":"code","648d9e0b":"code","3f1c50f6":"code","5071b59a":"code","304ae76b":"code","1e9adf71":"code","5f2eb204":"code","bec08a3c":"code","fabaecce":"code","77b6aeb6":"code","3bc247fb":"code","f613df11":"code","1a853eaf":"code","3c5b0ac4":"code","bda21693":"code","bada3540":"code","cecba87a":"code","011781f3":"code","da4161ea":"code","0c4e03fd":"code","15a100d3":"code","0e20fbbf":"code","6413ca17":"code","8b5b39e7":"code","a3166a6e":"code","df4e705c":"code","597c9848":"code","78328394":"code","493d7edc":"code","1f3c6801":"code","3ef6b14e":"code","444b1c40":"code","c600e306":"code","f89c4a09":"code","ce5d19c6":"code","0c7ef6c3":"code","5c0a6e2e":"code","23b1d259":"code","efc7a32a":"code","c767a80b":"code","a1158000":"code","640a1498":"code","b2104653":"code","28289404":"code","8b5cc45b":"code","b2638ccc":"code","c2a47df8":"code","e4faacb1":"code","3de05f72":"code","c23a4d3e":"code","555fd7df":"code","0bb266f3":"code","eef8501f":"code","49835abe":"code","a8d2137e":"code","4cf4b6cf":"code","b0743c84":"code","c6e12210":"code","b4c73280":"code","ca1313d8":"code","f8bd5c89":"code","cd16d4e3":"code","3052f0ab":"code","18b9153b":"code","517ede25":"code","b2cea13d":"code","953d470f":"code","783ba2d9":"code","4aa7e766":"code","5abe9b0b":"code","3df62c51":"code","f758d4dd":"code","24728739":"code","4822bf14":"code","63c29e9e":"markdown","8eea938c":"markdown","ab89da55":"markdown","c6c7cae4":"markdown","7b6aeef3":"markdown","19105da9":"markdown","3c30ec38":"markdown","46323dee":"markdown","4247206d":"markdown","5600adc0":"markdown","4f8e3972":"markdown","94753cad":"markdown","f6414100":"markdown","29595e1f":"markdown","a044b7fb":"markdown","454a7441":"markdown","0e1b8a9a":"markdown","695b3813":"markdown","cfa4a5d7":"markdown","18c6ef43":"markdown","1e2754f4":"markdown","5126125e":"markdown","fe117f94":"markdown","3b3081de":"markdown","e1e3d112":"markdown","ba78f418":"markdown","8016bb4d":"markdown","e0d322e9":"markdown","0f605c29":"markdown","ea7292ac":"markdown","dbfa8cde":"markdown","cee90c5b":"markdown","0fd4cd03":"markdown","990ba7ee":"markdown","f56b0540":"markdown","0f3111a9":"markdown","27b8213e":"markdown","5ba5fd55":"markdown","c691d47f":"markdown","8f662567":"markdown","89fca337":"markdown","cdcaf5ce":"markdown","5fe43465":"markdown","152353b1":"markdown","2965b583":"markdown","5faffcef":"markdown","f0e44093":"markdown","87e32f76":"markdown","bc7dd755":"markdown","9e5d9afb":"markdown","049f9b03":"markdown","93c257ac":"markdown","0505c51d":"markdown","f0264da5":"markdown","b4ce12e6":"markdown","f2d9dc0c":"markdown","7fb2c8c7":"markdown","23bb1397":"markdown","c2c199f2":"markdown","6ee84355":"markdown","e067d185":"markdown","89492625":"markdown","e862f4b4":"markdown","b5e2b09a":"markdown","1372e828":"markdown","e10d499d":"markdown","fe06e958":"markdown","9e744c97":"markdown","7a24ea04":"markdown","ce4c39f3":"markdown","28a22d35":"markdown","9dcf158b":"markdown","37f86da9":"markdown","b3c58bba":"markdown","64876fcc":"markdown","9d7799c0":"markdown","de068e2b":"markdown","1ee08998":"markdown","2127eda4":"markdown","b0af5adb":"markdown","55222a0f":"markdown","b0ef77c9":"markdown","922e6edf":"markdown","4d36207e":"markdown","35c6d718":"markdown","bf4a01b9":"markdown","ee55448d":"markdown","17dbcd58":"markdown","373ddf77":"markdown","01d37203":"markdown","a3ace8b2":"markdown","4576fda3":"markdown"},"source":{"701911df":"# Import Libraries\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# import cufflinks as cf\nimport plotly\nimport datetime\nimport math\nimport matplotlib\nimport sklearn\nfrom IPython.display import HTML\nfrom IPython.display import YouTubeVideo\n\nimport pickle\nimport os\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\n# Print versions of libraries\nprint(f\"Numpy version : Numpy {np.__version__}\")\nprint(f\"Pandas version : Pandas {pd.__version__}\")\nprint(f\"Matplotlib version : Matplotlib {matplotlib.__version__}\")\nprint(f\"Seaborn version : Seaborn {sns.__version__}\")\nprint(f\"SkLearn version : SkLearn {sklearn.__version__}\")\n# print(f\"Cufflinks version : cufflinks {cf.__version__}\")\nprint(f\"Plotly version : plotly {plotly.__version__}\")\n\n# Magic Functions for In-Notebook Display\n%matplotlib inline\n\n# Setting seabon style\nsns.set(style='darkgrid', palette='colorblind')","c4727f50":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv', encoding='latin_1')","d8bc1784":"# Converting all column names to lower case\ndf.columns = df.columns.str.lower()","0db3caa2":"df.head()","26bef45a":"df.tail()","19aeb134":"# Customising default values to view all columns\npd.options.display.max_rows = 100\npd.options.display.max_columns = 100\n\n# pd.set_option('display.max_rows',1000)","33d43bcc":"df.head(10)","e7208b4d":"df.info()","77bfdf71":"print(df['class'].value_counts())\nprint('\\n')\nprint(df['class'].value_counts(normalize=True))","3ea140eb":"df[\"class\"].value_counts().plot(kind = 'pie',explode=[0, 0.1],figsize=(6, 6),autopct='%1.1f%%',shadow=True)\nplt.title(\"Fraudulent and Non-Fraudulent Distribution\",fontsize=20)\nplt.legend([\"Fraud\", \"Genuine\"])\nplt.show()","46629f17":"df[['time','amount']].describe()","8aa50f12":"# Dealing with missing data\ndf.isnull().sum().max()","99770c5a":"plt.figure(figsize=(8,6))\nplt.title('Distribution of Transaction Amount', fontsize=14)\nsns.distplot(df['amount'], bins=100)\nplt.show()","42d03bce":"fig, axs = plt.subplots(ncols=2,figsize=(16,4))\nsns.distplot(df[df['class'] == 1]['amount'], bins=100, ax=axs[0])\naxs[0].set_title(\"Distribution of Fraud Transactions\")\n\nsns.distplot(df[df['class'] == 0]['amount'], bins=100, ax=axs[1])\naxs[1].set_title(\"Distribution of Genuine Transactions\")\n\nplt.show()","6f38389e":"print(\"Fraud Transaction distribution : \\n\",df[(df['class'] == 1)]['amount'].value_counts().head())\nprint(\"\\n\")\nprint(\"Maximum amount of fraud transaction - \",df[(df['class'] == 1)]['amount'].max())\nprint(\"Minimum amount of fraud transaction - \",df[(df['class'] == 1)]['amount'].min())","0ed4e2a1":"print(\"Genuine Transaction distribution : \\n\",df[(df['class'] == 0)]['amount'].value_counts().head())\nprint(\"\\n\")\nprint(\"Maximum amount of Genuine transaction - \",df[(df['class'] == 0)]['amount'].max())\nprint(\"Minimum amount of Genuine transaction - \",df[(df['class'] == 0)]['amount'].min())","a26dd7f8":"plt.figure(figsize=(8,6))\nsns.boxplot(x='class', y='amount',data = df)\nplt.title('Amount Distribution for Fraud and Genuine transactions')\nplt.show()","7ae78aa6":"plt.figure(figsize=(8,6))\nplt.title('Distribution of Transaction Time', fontsize=14)\nsns.distplot(df['time'], bins=100)\nplt.show()","93a4c2ba":"fig, axs = plt.subplots(ncols=2, figsize=(16,4))\n\nsns.distplot(df[(df['class'] == 1)]['time'], bins=100, color='red', ax=axs[0])\naxs[0].set_title(\"Distribution of Fraud Transactions\")\n\nsns.distplot(df[(df['class'] == 0)]['time'], bins=100, color='green', ax=axs[1])\naxs[1].set_title(\"Distribution of Genuine Transactions\")\n\nplt.show()","5eb44cc4":"plt.figure(figsize=(12,8))\nax = sns.boxplot(x='class', y='time',data = df)\n\n# Change the appearance of that box\nax.artists[0].set_facecolor('#90EE90')\nax.artists[1].set_facecolor('#FA8072')\n\nplt.title('Time Distribution for Fraud and Genuine transactions')\nplt.show()","c0e81f8b":"fig, axs = plt.subplots(nrows=2,sharex=True,figsize=(16,6))\n\nsns.scatterplot(x='time',y='amount', data=df[df['class']==1], ax=axs[0])\naxs[0].set_title(\"Distribution of Fraud Transactions\")\n\nsns.scatterplot(x='time',y='amount', data=df[df['class']==0], ax=axs[1])\naxs[1].set_title(\"Distribution of Genue Transactions\")\n\nplt.show()","fb987ea0":"# Finging unique values for each column\ndf[['time','amount','class']].nunique()","18bd16f4":"fig = px.scatter(df, x=\"time\", y=\"amount\", color=\"class\", \n                 marginal_y=\"violin\",marginal_x=\"box\", trendline=\"ols\", template=\"simple_white\")\nfig.show()","a4928c88":"df[['time','amount','class']].corr()['class'].sort_values(ascending=False).head(10)","9bba3181":"plt.title('Pearson Correlation Matrix')\nsns.heatmap(df[['time', 'amount','class']].corr(),linewidths=0.25,vmax=0.7,square=True,cmap=\"winter\",\n            linecolor='w',annot=True);","36fd0776":"df.shape","bdc328a4":"df['class'].value_counts(normalize=True)","0a934de4":"# Converting time from second to hour\ndf['time'] = df['time'].apply(lambda sec : (sec\/3600))","0b3fdc89":"# Calculating hour of the day\ndf['hour'] = df['time']%24   # 2 days of data\ndf['hour'] = df['hour'].apply(lambda x : math.floor(x))","2611e2d7":"# Calculating First and Second day\ndf['day'] = df['time']\/24   # 2 days of data\ndf['day'] = df['day'].apply(lambda x : 1 if(x==0) else math.ceil(x))","e2f8e3e2":"df[['time','hour','day','amount','class']]","99c12807":"# calculating fraud transaction daywise\ndayFrdTran = df[(df['class'] == 1)]['day'].value_counts()\n\n# calculating genuine transaction daywise\ndayGenuTran = df[(df['class'] == 0)]['day'].value_counts()\n\n# calculating total transaction daywise\ndayTran = df['day'].value_counts()\n\nprint(\"No of transaction Day wise:\")\nprint(dayTran)\n\nprint(\"\\n\")\n\nprint(\"No of fraud transaction Day wise:\")\nprint(dayFrdTran)\n\nprint(\"\\n\")\n\nprint(\"No of genuine transactions Day wise:\")\nprint(dayGenuTran)\n\nprint(\"\\n\")\n\nprint(\"Percentage of fraud transactions Day wise:\")\nprint((dayFrdTran\/dayTran)*100)","fd342b04":"fig, axs = plt.subplots(ncols=3, figsize=(16,4))\n\nsns.countplot(df['day'], ax=axs[0])\naxs[0].set_title(\"Distribution of Total Transactions\")\n\nsns.countplot(df[(df['class'] == 1)]['day'], ax=axs[1])\naxs[1].set_title(\"Distribution of Fraud Transactions\")\n\nsns.countplot(df[(df['class'] == 0)]['day'], ax=axs[2])\naxs[2].set_title(\"Distribution of Genuine Transactions\")\n\nplt.show()","386e70ec":"# Time plots \nfig , axs = plt.subplots(nrows = 1 , ncols = 2 , figsize = (15,8))\n\nsns.distplot(df[df['class']==0]['time'].values , color = 'green' , ax = axs[0])\naxs[0].set_title('Genuine Transactions')\n\nsns.distplot(df[df['class']==1]['time'].values , color = 'red' ,ax = axs[1])\naxs[1].set_title('Fraud Transactions')\n\nfig.suptitle('Comparison between Transaction Frequencies vs Time for Fraud and Genuine Transactions')\nplt.show()","47aa2e08":"# Let's see if we find any particular pattern between time ( in hours ) and Fraud vs Genuine Transactions\n\nplt.figure(figsize=(12,10))\n\nsns.distplot(df[df['class'] == 0][\"hour\"], color='green') # Genuine - green\nsns.distplot(df[df['class'] == 1][\"hour\"], color='red') # Fraudulent - Red\n\nplt.title('Fraud vs Genuine Transactions by Hours', fontsize=15)\nplt.xlim([0,25])\nplt.show()","5eeb3a84":"plt.figure(figsize=(8,6))\ndf[['time','hour','day','amount','class']].groupby('hour').count()['class'].plot()\nplt.show()","9d21b5ce":"df.hist(figsize = (25,25))\nplt.show()","adaf00d2":"df.reset_index(inplace = True , drop = True)","534aab1a":"# Scale amount by log\n# Adding a small amount of 0.0001 to amount as log of zero is infinite.\ndf['amount_log'] = np.log(df.amount + 0.0001)","f08291e0":"from sklearn.preprocessing import StandardScaler # importing a class from a module of a library\n\nss = StandardScaler() # object of the class StandardScaler ()\ndf['amount_scaled'] = ss.fit_transform(df['amount'].values.reshape(-1,1))","8a3175a2":"from sklearn.preprocessing import MinMaxScaler\n\nmm = MinMaxScaler() # object of the class StandardScaler ()\ndf['amount_minmax'] = mm.fit_transform(df['amount'].values.reshape(-1,1))","6b057256":"#Feature engineering to a better visualization of the values\n\n# Let's explore the Amount by Class and see the distribuition of Amount transactions\nfig , axs = plt.subplots(nrows = 1 , ncols = 4 , figsize = (16,4))\n\nsns.boxplot(x =\"class\",y=\"amount\",data=df, ax = axs[0])\naxs[0].set_title(\"Class vs Amount\")\n\nsns.boxplot(x =\"class\",y=\"amount_log\",data=df, ax = axs[1])\naxs[1].set_title(\"Class vs Log Amount\")\n\nsns.boxplot(x =\"class\",y=\"amount_scaled\",data=df, ax = axs[2])\naxs[2].set_title(\"Class vs Scaled Amount\")\n\nsns.boxplot(x =\"class\",y=\"amount_minmax\",data=df, ax = axs[3])\naxs[3].set_title(\"Class vs Min Max Amount\")\n\n# fig.suptitle('Amount by Class', fontsize=20)\nplt.show()","067c7256":"df[['time','hour','day','amount','amount_log','amount_scaled','amount_minmax','class']]","24f3b6fc":"CreditCardFraudDataCleaned = df\n\n# Saving the Python objects as serialized files can be done using pickle library\n# Here let us save the Final Data set after all the transformations as a file\nwith open('CreditCardFraudDataCleaned.pkl', 'wb') as fileWriteStream:\n    pickle.dump(CreditCardFraudDataCleaned, fileWriteStream)\n    # Don't forget to close the filestream!\n    fileWriteStream.close()\n    \nprint('pickle file is saved at Location:',os.getcwd())","648d9e0b":"# Reading a Pickle file\nwith open('CreditCardFraudDataCleaned.pkl', 'rb') as fileReadStream:\n    CreditCardFraudDataFromPickle = pickle.load(fileReadStream)\n    # Don't forget to close the filestream!\n    fileReadStream.close()\n    \n# Checking the data read from pickle file. It is exactly same as the DiamondPricesData\ndf = CreditCardFraudDataFromPickle\ndf.head()","3f1c50f6":"df.shape","5071b59a":"df.head()","304ae76b":"df.columns","1e9adf71":"# Separate Target Variable and Predictor Variables\n# Here I am keeping the log amount and dropping the amount and scaled amount columns.\nX = df.drop(['time','class','hour','day','amount','amount_minmax','amount_scaled'],axis=1)\ny = df['class']","5f2eb204":"X","bec08a3c":"# Load the library for splitting the data\nfrom sklearn.model_selection import train_test_split","fabaecce":"# Split the data into training and testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=101)","77b6aeb6":"# Quick sanity check with the shapes of Training and testing datasets\nprint(\"X_train - \",X_train.shape)\nprint(\"y_train - \",y_train.shape)\nprint(\"X_test - \",X_test.shape)\nprint(\"y_test - \",y_test.shape)","3bc247fb":"from sklearn.linear_model import LogisticRegression # Importing Classifier Step","f613df11":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train) ","1a853eaf":"y_pred = logreg.predict(X_test)","3c5b0ac4":"from sklearn import metrics","bda21693":"# https:\/\/en.wikipedia.org\/wiki\/Precision_and_recall\nprint(metrics.classification_report(y_test, y_pred))","bada3540":"print('Accuracy :{0:0.5f}'.format(metrics.accuracy_score(y_pred , y_test))) \nprint('AUC : {0:0.5f}'.format(metrics.roc_auc_score(y_test , y_pred)))\nprint('Precision : {0:0.5f}'.format(metrics.precision_score(y_test , y_pred)))\nprint('Recall : {0:0.5f}'.format(metrics.recall_score(y_test , y_pred)))\nprint('F1 : {0:0.5f}'.format(metrics.f1_score(y_test , y_pred)))\n# print('Confusion Matrix : \\n', cnf_matrix)\nprint(\"\\n\")","cecba87a":"# Predicted values counts for fraud and genuine of test dataset\npd.Series(y_pred).value_counts()","011781f3":"# Actual values counts for fraud and genuine of test dataset\npd.Series(y_test).value_counts()","da4161ea":"101\/144","0c4e03fd":"cnf_matrix = metrics.confusion_matrix(y_test,y_pred)\ncnf_matrix","15a100d3":"# Heatmap for Confusion Matrix\np = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, annot_kws={\"size\": 25}, cmap=\"winter\" ,fmt='g')\n\nplt.title('Confusion matrix', y=1.1, fontsize = 22)\nplt.ylabel('Actual',fontsize = 18)\nplt.xlabel('Predicted',fontsize = 18)\n\n# ax.xaxis.set_ticklabels(['Genuine', 'Fraud']); \n# ax.yaxis.set_ticklabels(['Genuine', 'Fraud']);\n\nplt.show()","0e20fbbf":"88\/144","6413ca17":"metrics.roc_auc_score(y_test , y_pred) ","8b5b39e7":"y_pred_proba = logreg.predict_proba(X_test)\ny_pred_proba","a3166a6e":"# plot ROC Curve\n\nplt.figure(figsize=(8,6))\n\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n\nauc = metrics.roc_auc_score(y_test, y_pred)\nprint(\"AUC - \",auc,\"\\n\")\n\nplt.plot(fpr,tpr,linewidth=2, label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\n\nplt.plot([0,1], [0,1], 'k--' )\n\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for Predicting a credit card fraud detection')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\n\nplt.show()","df4e705c":"# calculate precision-recall curve\nprecision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred)\n\n# calculate F1 score\nf1 = metrics.f1_score(y_test, y_pred)\n\nprint('f1=%.3f' % (f1))\n\n# plot no skill\nplt.plot([0, 1], [0.5, 0.5], linestyle='--')\n\n# plot the roc curve for the model\nplt.plot(recall, precision, marker='.')\n\n# show the plot\nplt.show()","597c9848":"# Import imbalace technique algorithims\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler","78328394":"from collections import Counter # counter takes values returns value_counts dictionary\nfrom sklearn.datasets import make_classification","493d7edc":"print('Original dataset shape %s' % Counter(y_train))\n\n# Undersampling only on train\nrus = RandomUnderSampler(random_state=42)\nX_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n\nprint('Resampled dataset shape %s' % Counter(y_train_rus))","1f3c6801":"# Undersampling with Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train_rus, y_train_rus)\n\ny_pred_rus = logreg.predict(X_test)","3ef6b14e":"print('Accuracy :{0:0.5f}'.format(metrics.accuracy_score(y_pred_rus , y_test))) \nprint('AUC : {0:0.5f}'.format(metrics.roc_auc_score(y_test , y_pred_rus)))\nprint('Precision : {0:0.5f}'.format(metrics.precision_score(y_test , y_pred_rus)))\nprint('Recall : {0:0.5f}'.format(metrics.recall_score(y_test , y_pred_rus)))\nprint('F1 : {0:0.5f}'.format(metrics.f1_score(y_test , y_pred_rus)))","444b1c40":"# plot ROC Curve\n\nplt.figure(figsize=(8,6))\n\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_rus)\n\nauc = metrics.roc_auc_score(y_test, y_pred_rus)\nprint(\"AUC - \",auc,\"\\n\")\n\nplt.plot(fpr,tpr,linewidth=2, label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\n\nplt.plot([0,1], [0,1], 'k--' )\n\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for Predicting a credit card fraud detection')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\n\nplt.show()","c600e306":"# calculate precision-recall curve\nprecision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred_rus)\n\n# calculate F1 score\nf1 = metrics.f1_score(y_test, y_pred_rus)\nprint('f1=%.3f' % (f1))\n\n# plot no skill\nplt.plot([0, 1], [0.5, 0.5], linestyle='--')\n\n# plot the roc curve for the model\nplt.plot(recall, precision, marker='.')\n\n# show the plot\nplt.show()","f89c4a09":"# Heatmap for Confusion Matrix\n\ncnf_matrix = metrics.confusion_matrix(y_test , y_pred_rus)\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, annot_kws={\"size\": 25}, cmap=\"winter\" ,fmt='g')\n\nplt.title('Confusion matrix', y=1.1, fontsize = 22)\nplt.xlabel('Predicted',fontsize = 18)\nplt.ylabel('Actual',fontsize = 18)\n\n# ax.xaxis.set_ticklabels(['Genuine', 'Fraud']); \n# ax.yaxis.set_ticklabels(['Genuine', 'Fraud']);\n\nplt.show()","ce5d19c6":"from imblearn.over_sampling import RandomOverSampler","0c7ef6c3":"print('Original dataset shape %s' % Counter(y_train))\n\nros = RandomOverSampler(random_state=42)\nX_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\n\nprint('Resampled dataset shape %s' % Counter(y_train_ros))","5c0a6e2e":"# Oversampling with Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train_ros, y_train_ros)\n\ny_pred_ros = logreg.predict(X_test)","23b1d259":"print('Accuracy :{0:0.5f}'.format(metrics.accuracy_score(y_test , y_pred_ros))) \nprint('AUC : {0:0.5f}'.format(metrics.roc_auc_score(y_test , y_pred_ros)))\nprint('Precision : {0:0.5f}'.format(metrics.precision_score(y_test , y_pred_ros)))\nprint('Recall : {0:0.5f}'.format(metrics.recall_score(y_test , y_pred_ros)))\nprint('F1 : {0:0.5f}'.format(metrics.f1_score(y_test , y_pred_ros)))","efc7a32a":"# plot ROC Curve\n\nplt.figure(figsize=(8,6))\n\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_ros)\n\nauc = metrics.roc_auc_score(y_test, y_pred_ros)\nprint(\"AUC - \",auc,\"\\n\")\n\nplt.plot(fpr,tpr,linewidth=2, label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\n\nplt.plot([0,1], [0,1], 'k--' )\n\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for Predicting a credit card fraud detection')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\n\nplt.show()","c767a80b":"# calculate precision-recall curve\nprecision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred_ros)\n\n# calculate F1 score\nf1 = metrics.f1_score(y_test, y_pred_ros)\nprint('f1=%.3f' % (f1))\n\n# plot no skill\nplt.plot([0, 1], [0.5, 0.5], linestyle='--')\n\n# plot the roc curve for the model\nplt.plot(recall, precision, marker='.')\n\n# show the plot\nplt.show()","a1158000":"# Heatmap for Confusion Matrix\n\ncnf_matrix = metrics.confusion_matrix(y_test , y_pred_ros)\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, annot_kws={\"size\": 25}, cmap=\"winter\" ,fmt='g')\n\nplt.title('Confusion matrix', y=1.1, fontsize = 22)\nplt.xlabel('Predicted',fontsize = 18)\nplt.ylabel('Actual',fontsize = 18)\n\n# ax.xaxis.set_ticklabels(['Genuine', 'Fraud']); \n# ax.yaxis.set_ticklabels(['Genuine', 'Fraud']);\n\nplt.show()","640a1498":"from imblearn.over_sampling import SMOTE, ADASYN","b2104653":"print('Original dataset shape %s' % Counter(y_train))\n\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n\nprint('Resampled dataset shape %s' % Counter(y_train_smote))","28289404":"# SMOTE Sampling with Logistic Regression\nlogreg = LogisticRegression(max_iter=1000)\nlogreg.fit(X_train_smote, y_train_smote)\n\ny_pred_smote = logreg.predict(X_test)","8b5cc45b":"print('Accuracy :{0:0.5f}'.format(metrics.accuracy_score(y_test , y_pred_smote))) \nprint('AUC : {0:0.5f}'.format(metrics.roc_auc_score(y_test , y_pred_smote)))\nprint('Precision : {0:0.5f}'.format(metrics.precision_score(y_test , y_pred_smote)))\nprint('Recall : {0:0.5f}'.format(metrics.recall_score(y_test , y_pred_smote)))\nprint('F1 : {0:0.5f}'.format(metrics.f1_score(y_test , y_pred_smote)))","b2638ccc":"# plot ROC Curve\n\nplt.figure(figsize=(8,6))\n\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_smote)\n\nauc = metrics.roc_auc_score(y_test, y_pred_smote)\nprint(\"AUC - \",auc,\"\\n\")\n\nplt.plot(fpr,tpr,linewidth=2, label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\n\nplt.plot([0,1], [0,1], 'k--' )\n\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for Predicting a credit card fraud detection')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\n\nplt.show()","c2a47df8":"# calculate precision-recall curve\nprecision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred_smote)\n\n# calculate F1 score\nf1 = metrics.f1_score(y_test, y_pred_smote)\nprint('f1=%.3f' % (f1))\n\n# plot no skill\nplt.plot([0, 1], [0.5, 0.5], linestyle='--')\n\n# plot the roc curve for the model\nplt.plot(recall, precision, marker='.')\n\n# show the plot\nplt.show()","e4faacb1":"# Heatmap for Confusion Matrix\ncnf_matrix = metrics.confusion_matrix(y_test , y_pred_smote)\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, annot_kws={\"size\": 25}, cmap=\"winter\" ,fmt='g')\n\nplt.title('Confusion matrix', y=1.1, fontsize = 22)\nplt.xlabel('Predicted',fontsize = 18)\nplt.ylabel('Actual',fontsize = 18)\n\n# ax.xaxis.set_ticklabels(['Genuine', 'Fraud']); \n# ax.yaxis.set_ticklabels(['Genuine', 'Fraud']);\n\nplt.show()","3de05f72":"print('Original dataset shape %s' % Counter(y_train))\n\nadasyn = ADASYN(random_state=42)\n\nX_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train, y_train)\nprint('Resampled dataset shape %s' % Counter(y_train_adasyn))","c23a4d3e":"#  ADASYN Sampling with Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train_adasyn, y_train_adasyn)\n\ny_pred_adasyn = logreg.predict(X_test)","555fd7df":"print('Accuracy :{0:0.5f}'.format(metrics.accuracy_score(y_pred , y_pred_adasyn))) \nprint('AUC : {0:0.5f}'.format(metrics.roc_auc_score(y_test , y_pred_adasyn)))\nprint('Precision : {0:0.5f}'.format(metrics.precision_score(y_test , y_pred_adasyn)))\nprint('Recall : {0:0.5f}'.format(metrics.recall_score(y_test , y_pred_adasyn)))\nprint('F1 : {0:0.5f}'.format(metrics.f1_score(y_test , y_pred_adasyn)))","0bb266f3":"# plot ROC Curve\n\nplt.figure(figsize=(8,6))\n\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_adasyn)\n\nauc = metrics.roc_auc_score(y_test, y_pred_adasyn)\nprint(\"AUC - \",auc,\"\\n\")\n\nplt.plot(fpr,tpr,linewidth=2, label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\n\nplt.plot([0,1], [0,1], 'k--' )\n\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for Predicting a credit card fraud detection')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\n\nplt.show()","eef8501f":"# calculate precision-recall curve\nprecision, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred_adasyn)\n\n# calculate F1 score\nf1 = metrics.f1_score(y_test, y_pred_adasyn)\nprint('f1=%.3f' % (f1))\n\n# plot no skill\nplt.plot([0, 1], [0.5, 0.5], linestyle='--')\n\n# plot the roc curve for the model\nplt.plot(recall, precision, marker='.')\n\n# show the plot\nplt.show()","49835abe":"# Heatmap for Confusion Matrix\n\ncnf_matrix = metrics.confusion_matrix(y_test , y_pred_adasyn)\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, annot_kws={\"size\": 25}, cmap=\"winter\" ,fmt='g')\n\nplt.title('Confusion matrix', y=1.1, fontsize = 22)\nplt.xlabel('Predicted',fontsize = 18)\nplt.ylabel('Actual',fontsize = 18)\n\n# ax.xaxis.set_ticklabels(['Genuine', 'Fraud']); \n# ax.yaxis.set_ticklabels(['Genuine', 'Fraud']);\n\nplt.show()","a8d2137e":"from sklearn.decomposition import PCA","4cf4b6cf":"X_reduced_pca_im = PCA(n_components=2, random_state=42).fit_transform(X_train)","b0743c84":"# Generate and plot a synthetic imbalanced classification dataset\nplt.figure(figsize=(12,8))\n\nplt.scatter(X_reduced_pca_im[:,0], X_reduced_pca_im[:,1], c=(y_train == 0), label='No Fraud', cmap='coolwarm', linewidths=1)\nplt.scatter(X_reduced_pca_im[:,0], X_reduced_pca_im[:,1], c=(y_train == 1), label='Fraud', cmap='coolwarm', linewidths=1)\n\nplt.title(\"Scatter Plot of Imbalanced Dataset\")\nplt.legend()\nplt.show()","c6e12210":"X_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X_train_smote)","b4c73280":"# Oversample and plot imbalanced dataset with ADASYN\nplt.figure(figsize=(12,8))\n\nplt.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y_train_smote == 0), cmap='coolwarm', label='No Fraud', linewidths=1)\nplt.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y_train_smote == 1), cmap='coolwarm', label='Fraud', linewidths=1)\n\nplt.title(\"Scatter Plot of Imbalanced Dataset With Adaptive Synthetic Sampling \\(ADASYN\\)\")\nplt.legend()\nplt.show()","ca1313d8":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB","f8bd5c89":"names_lst = []\n\n# Empty list to capture performance matrix for train set\naucs_train_lst = []\naccuracy_train_lst = []\nprecision_train_lst = []\nrecall_train_lst = []\nf1_train_lst = []\n\n# Empty list to capture performance matrix for test set\naucs_test_lst = []\naccuracy_test_lst = []\nprecision_test_lst = []\nrecall_test_lst = []\nf1_test_lst = []\n\n# Function for model building and performance measure\n\ndef build_measure_model(models):\n    plt.figure(figsize=(12,6))\n\n    for name, model, X_train, y_train, X_test, y_test in models:\n        \n        names_lst.append(name)\n\n        # Build model\n        model.fit(X_train, y_train)\n        \n        # Predict\n        y_train_pred = model.predict(X_train)\n        y_test_pred = model.predict(X_test)\n\n        # calculate accuracy\n        Accuracy_train = metrics.accuracy_score(y_train, y_train_pred)\n        accuracy_train_lst.append(Accuracy_train)\n        \n        Accuracy_test = metrics.accuracy_score(y_test, y_test_pred)\n        accuracy_test_lst.append(Accuracy_test)\n\n        # calculate auc\n        Aucs_train = metrics.roc_auc_score(y_train, y_train_pred)\n        aucs_train_lst.append(Aucs_train)\n        \n        Aucs_test = metrics.roc_auc_score(y_test , y_test_pred)\n        aucs_test_lst.append(Aucs_test)\n\n        # calculate precision\n        PrecisionScore_train = metrics.precision_score(y_train , y_train_pred)\n        precision_train_lst.append(PrecisionScore_train)\n        \n        PrecisionScore_test = metrics.precision_score(y_test , y_test_pred)\n        precision_test_lst.append(PrecisionScore_test)\n\n        # calculate recall\n        RecallScore_train = metrics.recall_score(y_train , y_train_pred)\n        recall_train_lst.append(RecallScore_train)\n        \n        RecallScore_test = metrics.recall_score(y_test , y_test_pred)\n        recall_test_lst.append(RecallScore_test)\n\n        # calculate f1 score\n        F1Score_train = metrics.f1_score(y_train , y_train_pred)\n        f1_train_lst.append(F1Score_train)\n        \n        F1Score_test = metrics.f1_score(y_test , y_test_pred)\n        f1_test_lst.append(F1Score_test)\n\n        #print('F1 Score of '+ name +' model : {0:0.5f}'.format(F1Score_test))\n\n        # draw confusion matrix\n        cnf_matrix = metrics.confusion_matrix(y_test , y_test_pred)\n\n        print(\"Model Name :\", name)\n        \n        print('Train Accuracy :{0:0.5f}'.format(Accuracy_train)) \n        print('Test Accuracy :{0:0.5f}'.format(Accuracy_test))\n        \n        print('Train AUC : {0:0.5f}'.format(Aucs_train))\n        print('Test AUC : {0:0.5f}'.format(Aucs_test))\n        \n        print('Train Precision : {0:0.5f}'.format(PrecisionScore_train))\n        print('Test Precision : {0:0.5f}'.format(PrecisionScore_test))\n        \n        print('Train Recall : {0:0.5f}'.format(RecallScore_train))\n        print('Test Recall : {0:0.5f}'.format(RecallScore_test))\n        \n        print('Train F1 : {0:0.5f}'.format(F1Score_train))\n        print('Test F1 : {0:0.5f}'.format(F1Score_test))\n        \n        print('Confusion Matrix : \\n', cnf_matrix)\n        print(\"\\n\")\n\n\n        # plot ROC Curve\n        fpr, tpr, thresholds = metrics.roc_curve(y_test, y_test_pred)\n        auc = metrics.roc_auc_score(y_test, y_test_pred)\n        plt.plot(fpr,tpr,linewidth=2, label=name + \", auc=\"+str(auc))\n    \n        #---------- For loops ends here--------#\n\n\n    plt.legend(loc=4)\n    plt.plot([0,1], [0,1], 'k--' )\n    plt.rcParams['font.size'] = 12\n    plt.title('ROC curve for Predicting a credit card fraud detection')\n    plt.xlabel('False Positive Rate (1 - Specificity)')\n    plt.ylabel('True Positive Rate (Sensitivity)')\n    plt.show()","cd16d4e3":"#------------------ Logistic Regression (LR) ------------------#\nLRmodels = []\n\nLRmodels.append(('LR imbalance', LogisticRegression(solver='liblinear', multi_class='ovr'), X_train,y_train,X_test,y_test))\nLRmodels.append(('LR Undersampling', LogisticRegression(solver='liblinear', multi_class='ovr'),X_train_rus,y_train_rus,X_test,y_test))\nLRmodels.append(('LR Oversampling', LogisticRegression(solver='liblinear', multi_class='ovr'),X_train_ros,y_train_ros,X_test,y_test))\nLRmodels.append(('LR SMOTE', LogisticRegression(solver='liblinear', multi_class='ovr'),X_train_smote,y_train_smote,X_test,y_test))\nLRmodels.append(('LR ADASYN', LogisticRegression(solver='liblinear', multi_class='ovr'),X_train_adasyn,y_train_adasyn,X_test,y_test))\n\n# Call function to create model and measure its performance\nbuild_measure_model(LRmodels)","3052f0ab":"#-----------------Decision Tree (DT)------------------#\nDTmodels = []\n\ndt = DecisionTreeClassifier()\n\nDTmodels.append(('DT imbalance', dt,X_train,y_train,X_test,y_test))\nDTmodels.append(('DT Undersampling', dt,X_train_rus,y_train_rus,X_test,y_test))\nDTmodels.append(('DT Oversampling', dt,X_train_ros,y_train_ros,X_test,y_test))\nDTmodels.append(('DT SMOTE', dt,X_train_smote,y_train_smote,X_test,y_test))\nDTmodels.append(('DT ADASYN', dt,X_train_adasyn,y_train_adasyn,X_test,y_test))\n\n# Call function to create model and measure its performance\nbuild_measure_model(DTmodels)","18b9153b":"#-----------------Random Forest (RF) ------------------#\nRFmodels = []\n\nRFmodels.append(('RF imbalance', RandomForestClassifier(),X_train,y_train,X_test,y_test))\nRFmodels.append(('RF Undersampling', RandomForestClassifier(),X_train_rus,y_train_rus,X_test,y_test))\nRFmodels.append(('RF Oversampling', RandomForestClassifier(),X_train_ros,y_train_ros,X_test,y_test))\nRFmodels.append(('RF SMOTE', RandomForestClassifier(),X_train_smote,y_train_smote,X_test,y_test))\nRFmodels.append(('RF ADASYN', RandomForestClassifier(),X_train_adasyn,y_train_adasyn,X_test,y_test))\n\n# Call function to create model and measure its performance\nbuild_measure_model(RFmodels)","517ede25":"# #------------------ K-Nearest Neighbors (KNN) ------------------#\n# KNNmodels = []\n\n# KNNmodels.append(('KNN imbalance', KNeighborsClassifier(),X_train,y_train,X_test,y_test))\n# KNNmodels.append(('KNN Undersampling', KNeighborsClassifier(),X_train_rus,y_train_rus,X_test,y_test))\n# KNNmodels.append(('KNN Oversampling', KNeighborsClassifier(),X_train_ros,y_train_ros,X_test,y_test))\n# KNNmodels.append(('KNN SMOTE', KNeighborsClassifier(),X_train_smote,y_train_smote,X_test,y_test))\n# KNNmodels.append(('KNN ADASYN', KNeighborsClassifier(),X_train_adasyn,y_train_adasyn,X_test,y_test))\n\n# Call function to create model and measure its performance\n# build_measure_model(KNNmodels)","b2cea13d":"# #------------------ Support Vector Machines (SVM) ------------------#\n# SVMmodels = []\n\n# SVMmodels.append(('SVM imbalance', SVC(gamma='auto'),X_train,y_train,X_test,y_test))\n# SVMmodels.append(('SVM Undersampling', SVC(gamma='auto'),X_train_rus,y_train_rus,X_test,y_test))\n# SVMmodels.append(('SVM Oversampling', SVC(gamma='auto'),X_train_ros,y_train_ros,X_test,y_test))\n# SVMmodels.append(('SVM SMOTE', SVC(gamma='auto'),X_train_smote,y_train_smote,X_test,y_test))\n# SVMmodels.append(('SVM ADASYN', SVC(gamma='auto'),X_train_adasyn,y_train_adasyn,X_test,y_test))\n\n# Call function to create model and measure its performance\n# build_measure_model(SVMmodels)","953d470f":"#------------------ Gaussian Naive Bayes (NB) ------------------#\nNBmodels = []\n\nNBmodels.append(('NB imbalance', GaussianNB(),X_train,y_train,X_test,y_test))\nNBmodels.append(('NB Undersampling', GaussianNB(),X_train_rus,y_train_rus,X_test,y_test))\nNBmodels.append(('NB Oversampling', GaussianNB(),X_train_ros,y_train_ros,X_test,y_test))\nNBmodels.append(('NB SMOTE', GaussianNB(),X_train_smote,y_train_smote,X_test,y_test))\nNBmodels.append(('NB ADASYN', GaussianNB(),X_train_adasyn,y_train_adasyn,X_test,y_test))\n\n# Call function to create model and measure its performance\nbuild_measure_model(NBmodels)","783ba2d9":"data = {'Model':names_lst,\n       #'Accuracy_Train':accuracy_train_lst,\n       'Accuracy_Test':accuracy_test_lst,\n       #'AUC_Train':aucs_train_lst,\n       'AUC_Test':aucs_test_lst,\n       #'PrecisionScore_Train':precision_train_lst,\n       'PrecisionScore_Test':precision_test_lst,\n       #'RecallScore_Train':recall_train_lst,\n       'RecallScore_Test':recall_test_lst,\n       #'F1Score_Train':f1_train_lst,\n       'F1Score_Test':f1_test_lst}\n\nprint(\"Performance measures of various classifiers: \\n\")\nperformance_df = pd.DataFrame(data) \nperformance_df.sort_values(['F1Score_Test','RecallScore_Test','AUC_Test'],ascending=False)","4aa7e766":"YouTubeVideo('Gol_qOgRqfA', width=800, height=400)","5abe9b0b":"# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV","3df62c51":"#------------ Logistic Regression ------------#\nlog_reg_params = {\"solver\": ['saga'],\n                  \"penalty\": ['l1', 'l2'], \n                  'C':  [0.01, 0.1, 1, 10, 100], \n                  \"max_iter\" : [100000]},\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_train_rus,y_train_rus)\n\n# Logistic Regression best estimator\nprint(\"Logistic Regression best estimator : \\n\",grid_log_reg.best_estimator_)\n\n# predict test dataset\ny_pred_lr = grid_log_reg.predict(X_test)\n\n# f1 score\nprint('\\nLogistic Regression f1 Score : {0:0.5f}'.format(metrics.f1_score(y_test , y_pred_lr)))","f758d4dd":"#------------ K Nearest Neighbour ------------#\nknears_params = {\"n_neighbors\": list(range(2,60,1)), \n                 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\n\ngrid_knears.fit(X_train_rus,y_train_rus)\n\n# KNears best estimator\nprint(\"KNN best estimator : \\n\",grid_knears.best_estimator_)\n\n# predict test dataset\ny_pred_knn = grid_knears.predict(X_test)\n\n# f1 score\nprint('\\nKNN f1 Score : {0:0.5f}'.format(metrics.f1_score(y_test , y_pred_knn)))","24728739":"#------------ Support Vector Classifier ------------#\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], \n              'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train_rus,y_train_rus)\n\n# SVC best estimator\nprint(\"SVC best estimator : \\n\",grid_svc.best_estimator_)\n\n# predict test dataset\ny_pred_svc = grid_svc.predict(X_test)\n\n# f1 score\nprint('\\nSVC f1 Score : {0:0.5f}'.format(metrics.f1_score(y_test , y_pred_svc)))","4822bf14":"#------------ DecisionTree Classifier ------------#\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \n               \"max_depth\": list(range(2,4,1)), \n               \"min_samples_leaf\": list(range(5,7,1))}\n\ngrid_tree = GridSearchCV(estimator = DecisionTreeClassifier(),\n                        param_grid = tree_params,\n                        scoring = 'accuracy', \n                        cv = 5, \n                        verbose = 1,\n                        n_jobs = -1)\n\n\ngrid_tree.fit(X_train_rus,y_train_rus)\n\n# tree best estimator\nprint(\"Decision Tree best estimator : \\n\",grid_tree.best_estimator_)\n\n# predict test dataset\ny_pred_dt = grid_tree.predict(X_test)\n\n\n# f1 score\nprint('\\nf1 Score : {0:0.5f}'.format(metrics.f1_score(y_test , y_pred_dt)))","63c29e9e":"# <a id='spatial'>Spatial nature of class imbalance<\/a>\n\nI will reduce 29 columns to 2 columns with the help of **Principal Component Analysis** so that I can look at them in a plot! (because to plot graph we need two dimensions)","8eea938c":"### <a id='modevel'>Model Evolution<\/a>","ab89da55":"**Highlights**\n\n* Dataset contains details of 284807 transactions with 31 features.\n* There is no missing data in our dataset, every column contain exactly 284807 rows.\n* All data types are float64, except 1: Class \n* All data types are float64, except 1: Class \n* 28 columns have Sequential Names and values that don't make any logical sense - > V1, V2 ....V28\n* 3 columns: TIME, AMOUNT and CLASS which can be analysed for various INSIGHTS! \n* Memory Usage: 67.4 MB, not so Harsh !!","c6c7cae4":"**Highlights**\n\n* Total number of transaction on Day 1 was 144787, out of which 281 was a fraud and 144506 was genuine. Fraud transaction was 0.19% of the total transaction on day 1.\n\n* Total number of transaction on Day 2 was 140020, out of which 211 was a fraud and 139809 was genuine. Fraud transaction was 0.15% of the total transaction on day 2.\n\n* Most of the transaction including the fraud transaction happened on day 1.\n\nLet's see the above numbers in the graph.","7b6aeef3":"## <a id='gridsearchLR'> 1. Grid Search with Logistic Regression<\/a>","19105da9":"**Highlights**\n\n**Above graph shows that most of the Fraud transactions are happening at night time (0 to 7 hours) when most of the people are sleeping and Genuine transaction are happening during day time (9 to 21 hours).**","3c30ec38":"**Highlights**\n\nOur model predicted 101 transactions as fraud and 85342 transactions as genuine from the test dataset.","46323dee":"## <a id='scalenorm'>3. Scale  amount by Normalization<\/a>\n\nNormalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling.\n\n$$ x_{norm} = \\frac{x_i - x_{min}}{x_{max}-x_{min}} $$","4247206d":"**There are originally 144 fraud transactions and our model predicted only 101 fraud transaction. So the accuracy of our model should be ${101}\\over{144}$, right?**","5600adc0":"**Highlights**\n\nIt looks like that no features are highly correlated with any other features.","4f8e3972":"## <a id='unique'>Count unique values of label<\/a>","94753cad":"## <a id='scalelog'>1. Scale amount by Log<\/a>\n\n**Scaling using the log**: There are two main reasons to use logarithmic scales in charts and graphs. \n* The first is to respond to skewness towards large values; i.e., cases in which one or a few points are much larger than the bulk of the data. \n* The second is to show per cent change or multiplicative factors.\u00a0","f6414100":"### Predict from Test set","29595e1f":"## <a id='catcont'>Categorical vs Continuous Features<\/a>\n\nFinding unique values for each column to understand which column is categorical and which one is Continuous","a044b7fb":"### Distribution of Amount for Fradulent & Genuine transactions","454a7441":"### Converting time from second to hour","0e1b8a9a":"### We have seen that imbalance dataset have Recall score of only 61.11%. It means that creating a model from the imbalanced dataset is highly biased towards genuine transactions and creates a model which is not able to predict the fraudulent transactions correctly. However, the balanced dataset has Recall score of above 94.444%.","695b3813":"# <a id='eda'>Exploratory Data Analysis<\/a>\n\nOnce the data is read into python, we need to explore\/clean\/filter it before processing it for machine learning It involves adding\/deleting few columns or rows, joining some other data, and handling qualitative variables like dates.\n\nNow that we have the data, I wanted to run a few initial comparisons between the three columns - Time, Amount, and Class.","cfa4a5d7":"# <a id='modelbaseline'>Baseline for models<\/a>\n\nWe will train four types of classifiers and decide which classifier will be more effective in detecting **fraud transactions**.","18c6ef43":"### Calculating First and Second Day","1e2754f4":"* **Due to confidentiality issue, original features V1, V2,... V28 have been transformed with PCA, however, we may guess that these features might be originally credit card number, expiry date, CVV, cardholder name, transaction location, transaction date-time, etc.** \n\n* The only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. \n\n* Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.","5126125e":"### Naive Bayes (NB)","fe117f94":"## <a id='logregovrsamp'>1.3.Logistic Regression with Random Oversampling technique<\/a>","3b3081de":"### Thanks \n\nThanks a lot to @satvikgarg and @drscarlat, who find the **data leakage** problem in my notebook.  The modifications that I have done in the version 5 are below;\n\n* Instead of balancing the dataset first and then split into train\/test, I have first split the dataset into train\/test and then balanced the TRAIN PART ONLY using Undersampling, Oversampling, SMOTE and ADASYN.\n\n\n* Focused on F1 as ROC\/AUC is not a good metric for imbalanced datasets.","e1e3d112":"**Highlights**\n\nThis dataset has 492 frauds out of 284,315 transactions. The dataset is **highly unbalanced**, the positive class (frauds) account for 0.172% of all transactions. Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis, our algorithms will probably overfit since it will \"assume\" that most transactions are not a fraud. But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!","ba78f418":"### Visualising Data for detecting any particular Pattern or Anomaly using Histogram Plots\n\nFinally visulaising all columns once and for all to observe any abnormality","8016bb4d":"## <a id='logregadasyn'>1.5 Logistic Regression with ADASYN data<\/a>","e0d322e9":"**Highlights**\n\nBy seeing the graph, we can see there are two peaks in the graph and even there are some local peaks. We can think of these as the time of the day like the peak is the day time when most people do the transactions and the depth is the night time when most people just sleeps. We already know that data contains a credit card transaction for only two days, so there are two peaks for day time and one depth for one night time.","0f605c29":"**Hightlights**\n\nAfter training each of the models, these are the final results. All of the scores for Random Forest with Oversampling technique and the Random Forest with SMOTE technique models are very promising for our dataset! Each model has a high true positive rate and a low false-positive rate, which is exactly what we\u2019re looking for.\n\n\nIn the ROC graph above, the AUC scores for Random Forest with Oversampling technique is pretty high, which is what we\u2019d like to see. As we move further right along the curve, we both capture more True Positives but also incur more False Positives. This means we capture more fraudulent transactions, but also flag even more normal transactions as fraudulent.\n\u00a0\n**So Random Forest with Oversampling technique \u00a0is our final model, as this gives highest F1 score of 86.47% on test datasets.**","ea7292ac":"**Highlights**\n\nMost the transaction amount falls between 0 and about 3000 and we have some outliers for really big amount transactions and it may actually make sense to drop those outliers in our analysis if they are just a few points that are very extreme. Also, we should be conscious about that these **outliers should not be the fraudulent transaction**. Generally, fraudulent transactions of the very big amount and removing them from the data can make the predicting model bais. \n\nSo we can essentially build a model that realistically predicts transaction as fraud without affected by outliers. It may not be really useful to actually have our model train on these extreme outliers.","dbfa8cde":"**Highlights**\n\nThis graph shows that most of the fraud transaction amount is less than 500 dollars. This also shows that the fraud transaction is very high for an amount near to 0, let's find that amount.","cee90c5b":"### Distribution of time w.r.t. transactions types","0fd4cd03":"**Highlights**\n\nThere are no missing values present in the dataset. It is not necessary that missing values are present in the dataset in the form of \u00a0NA, NAN, Zeroes etc, it may be present by some other values also that can be explored by analysing each feature.","990ba7ee":"# <a id='classimbalance'>Class Imbalance<\/a>\n\nImbalanced data typically refers to a problem with classification problems where the classes are not represented equally.  If one applies classifiers on the dataset, they are likely to predict everything as the majority class. This was often regarded as a problem in learning from highly imbalanced datasets.\n\n<img src='https:\/\/i.imgur.com\/uqh1peJ.gif' \/>\n\n\nLet's Fix the class Imbalance and apply some sampling techniques.\n\n\nRef : https:\/\/machinelearningmastery.com\/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset\/","f56b0540":"# <a id='scaleamount'>Scale Amount Feature<\/a>\n\n* It is a good idea to scale the data so that the column(feature) with lesser significance might not end up dominating the objective function due to its larger range. like a column like age has a range between 0 to 80, but a column like a salary has ranged from thousands to lakhs, hence, salary column will dominate to predict the outcome even if it may not be important.\n* In addition, features having different unit should also be scaled thus providing each feature equal initial weightage. Like Age in years and Sales in Dollars must be brought down to a common scale before feeding it to the ML algorithm\n* This will result in a better prediction model.\n\n\n\n**PCA Transformation**: The description of the data says that all the features went through a PCA transformation (Dimensionality Reduction technique) except for time and amount.\n\n**Scaling**: Keep in mind that in order to implement a PCA transformation features need to be previously scaled.","0f3111a9":"# <a id='introduction'>Introduction<\/a>\n\nIt is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase. Eventually, it is also important for companies NOT to detect transactions which are genuine as fraudulent, otherwise, companies would keep blocking the credit card, and which may lead to customer dissatisfaction. So here are two important expects of this analysis:\n\n* What would happen when the company will not able to detect the fraudulent transaction and would not confirm from a customer about this recent transaction whether it was made by him\/her.\n\n* In contract, what would happen when the company will detect a genuine transaction as fraudulent and keep calling customer for confirmation or might block the\u00a0card.\n\nThe datasets contain transactions that have 492 frauds out of 284,807 transactions. So the dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions. When we try to build the prediction model with this kind\u00a0of unbalanced dataset, then the model will be more inclined towards to detect new unseen transaction as genuine as our dataset contains about 99% genuine data.\n\nAs our dataset is highly imbalanced, so we shouldn't use accuracy score as a metric because it will be usually high and misleading, instead use we should focus on f1-score, precision\/recall score or confusion matrix.","27b8213e":"### Distribution of transaction type w.r.t amount","5ba5fd55":"# Before Starting:\n\nIf you liked this kernel please don't forget to upvote the project, this will keep me motivated to other kernels in the future. I hope you enjoy our deep exploration into this dataset. Let's begin!","c691d47f":"## Import the Dataset","8f662567":"## Import imbalace technique algorithims","89fca337":"## <a id='timefeateng'>Feature engineering on Time<\/a>","cdcaf5ce":"# Table of Contents\n\n- [Credit Card Fraud Detection Introduction](#introduction)\n- [Dataset Understanding](#dataset)\n- [Exploratory Data Analysis](#eda)\n    - [Feature Scaling](#feature)\n    - [Concise Summary](#info)\n    - [Uique Labels](#unique)\n    - [Descriptive Statistics](#describe)\n    - [Finding null values](#null)\n    - [Distribution of Amount](#amountdist)\n    - [Removal of Outliers](#outliers)\n    - [Categorical vs Continuous Features](#catcont)\n    - [Correlation Among Explanatory Variables](#corr)\n- [Feature Engineering](#feateng)\n    - [Feature engineering on Time](#timefeateng)\n- [Scaling](#scaleamount)\n    - [Scale amount by Log](#scalelog)\n    - [Scale  amount by Standardization](#scalestand)\n    - [Scale  amount by Normalization](scalenorm)\n- [Saving preprossed data](#pickle)\n- [Split data](#splitdata)\n- [Baseline for models](#modelbaseline)\n- [Class Imbalance](#classimbalance)\n    - [Under Sampling and Over Sampling](#undovrsamp)\n    - [Synthetic Minority OverSampling Technique (SMOTE)](#smote)\n    - [Adaptive Synthetic Sampling Method for Imbalanced Data (ADASYN)](#adasyn)\n- [Model Building](#modelbuild)\n    - [Logistic Regression](#logreg)\n        - [Logistic Regression with imbalanced data](#logregim)\n            - [Model Evolution](#modevel)\n            - [Model Evolution Matrix](#modevelmatrix)\n            - [Receiver Operating Characteristics (ROC)](#roccurve)\n        - [Logistic Regression with Random Undersampling technique](#logregundsamp)\n        - [Logistic Regression with Random Oversampling technique](#logregovrsamp)\n        - [Logistic Regression with SMOTE technique](#logregsomote)\n        - [Logistic Regression with ADASYN technique](#logregadasyn)\n- [Spatial nature of class imbalance](#spatial)\n    - [Distribution of balaced dataset](#distimbds)\n    - [Distribution of balaced dataset](#distbalds)\n- [Building different models with different balanced datasets](#modelwith)\n    - [Undersampled Data](#usdata)\n    - [Oversampled Data](#osdata)\n    - [SMOTE Data](#smotedata)\n    - [ADASYN Data](#adasyndata)\n- [Grid Search](#)\n    - [Grid Search with Logistic Regression](#gridsearchLR)\n    - [Grid Search with K Nearest Neighbour Classifier](#gridsearchKNN)\n    - [Grid Search with Support Vector Classifier](#gridsearchSVC)\n    - [Grid Search with Decision Tree Classifier](#gridsearchDT)\n- [Conclusion](#concl)","5fe43465":"**Highlights**\n\n* We have created few new features like an hour, day, scaled amount. However, these are just for visualization purpose only, not for building the model.","152353b1":"## <a id='gridsearchSVC'> 3. Grid Search with Support Vector Classifier<\/a>","2965b583":"### Decision Tree (DT)","5faffcef":"## Lets check the data again after cleaning","f0e44093":"# <a id='pickle'>Saving preprossed data as serialized files<\/a>\n* To deploy the predictive models built we save them along with the required data files as serialized file objects\n* We save cleaned and processed input data, tuned predictive models as files so that they can later be re-used\/shared","87e32f76":"**Highlights**\n* On an average, credit card transaction is happening at every 94813.86 seconds.\n* Average transaction amount is 88.35 with a standard deviation of 250, with a minimum amount of 0.0 and the maximum amount 25,691.16. By seeing the 75% and the maximum amount, it looks like the feature 'Amount' is highly ** positively skewed**. We will check the distribution graph of the amount to get more clarity.","bc7dd755":"## <a id='logregsomote'>1.4 Logistic Regression with SMOTE data<\/a>","9e5d9afb":"So 70.39% should be our accuracy.","049f9b03":"# <a id='splitdata'>Splitting data into Training and Testing samples<\/a>\n\nWe don't use the full data for creating the model. Some data is randomly selected and kept aside for checking how good the model is. This is known as Testing Data and the remaining data is called Training data on which the model is built. Typically 70% of data is used as training data and the rest 30% is used as testing data.","93c257ac":"## <a id='logregundsamp'>1.2.Logistic Regression with Random Undersampling technique<\/a>","0505c51d":"### Distribution of Amount w.r.t Class","f0264da5":"### Calculating hour of the day","b4ce12e6":"**Highlights**\n\n* There are 113 fraud transactions for just one dollor and 27 fraud transaction for $99.99. And higest fraud transaction amount was 2125.87 and lowest was just 0.00.\n* There are 27 fraud transaction for zero amount. Zero Authorization is an account verification method for credit cards that is used to verify a cardholders information without charging the consumer. Instead, an amount of zero is charged on the card to store the credit card information in the form of a token and to determine whether the card is legitimate or not. After creating the token, is then possible to charge the consumer with a new transaction with either Tokenization or Recurring Payments\n\nRef : https:\/\/docs.multisafepay.com\/tools\/zero-authorization\/what-is-zero-authorization\/","f2d9dc0c":"## <a id='gridsearchDT'> 4. Grid Search with Decision Tree Classifier<\/a>","7fb2c8c7":"## <a id='null'>Finding null values<\/a>","23bb1397":"### Performance measures of various classifiers","c2c199f2":"# <a id='concl'>Conclusion<\/a>\n\nWe were able to accurately identify fraudulent credit card transactions using a random forest model with oversampling technique. We, therefore, chose the random forest model with oversampling technique as the better model, which obtained highest F1 score of 86.47%   on the test set.\n\n**I hope I was able to explain my findings well and thanks so much for reading!**\n\n\n### I welcome comments, suggestions, corrections and of course votes also.\n","6ee84355":"## Reset the index","e067d185":"**Hightlights**\n\n* We can see a slight difference in the log amount of our two Classes. \n* The IQR of fraudulent transactions are higher than normal transactions, but normal transactions have the highest values.\n* **By seeing the above three graphs, I think scaling the amount by log will best suit for our model.**","89492625":"### Confusion Matrix","e862f4b4":"#### We already know that we have 144 fraud transaction in our test dataset, but our model predicted only 88 fraud transaction. So the real accuracy of our model is ${88}\\over{144}$","b5e2b09a":"## <a id='timedist'>Distribution of Time<\/a>","1372e828":"### Load preprocessed data","e10d499d":"# <a id='dataset'>Load Data<\/a>","fe06e958":"## <a id='undovrsamp'>Under Sampling and Over Sampling<\/a>\n\nOversampling and undersampling in data analysis are techniques used to adjust the class distribution of a data set. \n\n* Random oversampling duplicates examples from the minority class in the training dataset and can result in overfitting for some models.\n\n* Random undersampling deletes examples from the majority class and can result in losing information invaluable to a model.\n\n<img src = 'https:\/\/github.com\/dktalaicha\/Kaggle\/blob\/master\/CreditCardFraudDetection\/images\/under_over_sampling.jpg?raw=true'>\n\n## <a id='smote'>Synthetic Minority OverSampling Technique (SMOTE)<\/a>\n\nIn this technique, instead of simply duplicating data from the minority class, we synthesize new data from the minority class. This is a type of data augmentation for tabular data can be very effective. This approach to synthesizing new data is called the Synthetic Minority Oversampling TEchnique, or SMOTE for short. \n\n<img src='https:\/\/github.com\/dktalaicha\/Kaggle\/blob\/master\/CreditCardFraudDetection\/images\/smote.png?raw=true'>\n\n## <a id='adasyn'>Adaptive Synthetic Sampling Method for Imbalanced Data (ADASYN)<\/a>\n\nADASYN (Adaptive Synthetic) is an algorithm that generates synthetic data, and its greatest advantages are not copying the same minority data, and generating more data for \u201charder to learn\u201d examples.\n\nRef : https:\/\/medium.com\/@ruinian\/an-introduction-to-adasyn-with-code-1383a5ece7aa","9e744c97":"## \u00a0<a id='distimbds'>Distribution of balaced dataset<\/a>\n\nFinally, we can create a scatter plot of the dataset and colour the examples for each class a different colour to clearly see the spatial nature of the class imbalance.\n\nA scatter plot of the dataset is created showing the large mass of points that belong to the minority class (red) and a small number of points spread out for the minority class (blue). We can see some measure of overlap between the two classes.","7a24ea04":"#### Let's Discuss Next Steps - \n\n1  __Classification Models__\n\n- Logistic Regression\n- Decision Trees\n- Random Forest\n- Naive Bayes Classifier \n\n2  __Class Imbalance Solutions__\n\n- Under Sampling\n- Over Sampling\n- SMOTE\n- ADASYN\n\n3  __Metrics__\n\n- Accuracy Score\n- Confusion Matrix\n- Precision Score\n- Recall Score\n- ROC_AUC\n- F1 Score","ce4c39f3":"# <a id='modelbuild'>Model Building<\/a>\n\n##### We are aware that our dataset is highly imbalanced, however, we check the performance of imbalance dataset first and later we implement some techniques to balance the dataset and again check the performance of balanced dataset. Finally, we will compare each regression models performance.","28a22d35":"**However, this not the case. Actually there are originally 144 fraud transactions and 85299 genuine transactions in the test dataset. However, our model predicted only 101 fraud transaction. Also, it should be kept\u00a0in mind that these 101 predicted fraud transaction may not be identified correctly. It means that these predicted 101 fraud transactions are NOT only from 144 originally fraud transaction, but they may also be from genuine transactions as well.**\n\nWe will see our real accuracy in below cells.","9dcf158b":"## <a id='info'>Checking concise summary of dataset<\/a>\n\nIt is also a good practice to know the features and their corresponding data types, along with finding whether they contain null values or not.","37f86da9":"### Logistic Regression (LR)","b3c58bba":"# <a id='feateng'>Feature Engineering<\/a> ","64876fcc":"### Fraud and Genuine transaction Day wise","9d7799c0":"## <a id='gridsearchKNN'> 2. Grid Search with K Nearest Neighbour Classifier<\/a>","de068e2b":"# <a id='logreg'>1. Logistic Regression<\/a>","1ee08998":"### <a id='roccurve'>Receiver Operating Characteristics (ROC)<\/a>\n\nThe ROC is a performance measurement for classification problems at various thresholds. It is essentially a probability curve, and the higher the Area Under the Curve (AUC) score the better the model is at predicting fraudulent\/non-fraudulent transactions.\n\nIt is an evaluation metric that helps identify the strength of the model to **distinguish between two outcomes**. It defines if a model can create a clear boundary between the postive and the negative class. \n\n<div style='width:100%;'>\n   <div style='width:30%; float:left;'> <img  src ='https:\/\/i.imgur.com\/fzBGUDy.jpg' \/> <\/div>\n   <div style=''> <img  src ='https:\/\/i.imgur.com\/hZQiNCn.png' \/> <\/div>\n<\/div>\n\n\nLet's talk about some definitions first: \n\n##### __Sensitivity__ or __Recall__\n\nThe sensitivity of a model is defined by the proportion of actual positives that are classified as Positives , i.e = TP \/ ( TP + FN )\n\n$$ \\text{Recall or Sensitivity} = \\frac{(TP)}{( TP + FN )} $$\n\n<img src = \"https:\/\/github.com\/dktalaicha\/Kaggle\/blob\/master\/CreditCardFraudDetection\/images\/sens.png?raw=true\">\n\n##### __Specificity__\n\nThe specificity of a model is defined by the proportion of actual negatives that are classified as Negatives , i.e = TN \/ ( TN + FP )\n\n$$ \\text{Specificity} = \\frac{(TN)}{( TN + FP )} $$\n\n<img src = \"https:\/\/github.com\/dktalaicha\/Kaggle\/blob\/master\/CreditCardFraudDetection\/images\/spec.png?raw=true\">\n\nAs we can see that both are independent of each other and lie in teo different quadrants , we can understand that they are inversely related to each other. Thus as Sensitivity goes up , Specificity goes down and vice versa.\n\n### ROC CURVE\n\nIt is a plot between Sesitivity and ( 1 - Specificity ) , which intuitively is a plot between True Positive Rate and False Positive Rate. \nIt depicts if a model can clearly identify each class or not\n\nHigher the area under the curve , better the model and it's ability to seperate the positive and negative class.\n\n<img src = \"https:\/\/github.com\/dktalaicha\/Kaggle\/blob\/master\/CreditCardFraudDetection\/images\/tpfpfntn.jpeg?raw=true\">\n<img src = \"https:\/\/github.com\/dktalaicha\/Kaggle\/blob\/master\/CreditCardFraudDetection\/images\/auc.png?raw=true\">\n<img src = \"https:\/\/github.com\/dktalaicha\/Kaggle\/blob\/master\/CreditCardFraudDetection\/images\/auc2.png?raw=true\">\n\n<img src='https:\/\/i.imgur.com\/GRuZpez.gif'>","2127eda4":"# <a id='gridsearch'>Grid Search<\/a>\n\nGrid search is the process of performing hyper parameter tuning in order to determine the optimal values for a given model. This is significant as the performance of the entire model is based on the hyper parameter values specified.\n\nA **model hyperparameter** is a characteristic of a model that is external to the model and whose value cannot be estimated from data. The value of the hyperparameter has to be set before the learning process begins. For example, c in Support Vector Machines, k in k-Nearest Neighbors, the number of hidden layers in Neural Networks.\n\nIn contrast, a **parameter** is an internal characteristic of the model and its value can be estimated from data. Example, beta coefficients of linear\/logistic regression or support vectors in Support Vector Machines.\n\nRef: \n* https:\/\/medium.com\/datadriveninvestor\/an-introduction-to-grid-search-ff57adcc0998\n* https:\/\/towardsdatascience.com\/grid-search-for-hyperparameter-tuning-9f63945e8fec\n* https:\/\/www.youtube.com\/watch?v=Gol_qOgRqfA\n\n**Youtube**","b0af5adb":"## <a id='distbalds'>Distribution of balaced dataset<\/a>\n\nFinally, a scatter plot of the transformed dataset is created.\n\nIt shows many more examples in the minority class created along the lines between the original examples in the minority class.","55222a0f":"## <a id='logregim'>1.1 Logistic Regression with imbalanced data<\/a>","b0ef77c9":"**Highlights**\n\nMost the transaction amount falls between 0 and about 3000 and we have some outliers for really big amount transactions and it may actually make sense to drop those outliers in our analysis if they are just a few points that are very extreme.\n\nMost daily transactions are not extremely expensive, but it\u2019s likely where most fraudulent transactions are occurring as well.","922e6edf":"## <a id='amountdist'>Distribution of Amount<a id=''>","4d36207e":"# <a id='modelwith'>Building different models with different balanced datasets<\/a>\nLet's now try different models , first by creating multiple datasets for undersampled , oversampled and SMOTE sampled","35c6d718":"So, **61.11%** is the real accuracy of our model, which is nothing but the **Recall Score**. So we have the emphasis on Recall score and F1 score to measure the performance of our model, not the accuracy.","bf4a01b9":"## <a id='describe'>Generate descriptive statistics<\/a>\n\nThe describe() function generates descriptive statistics that summarize the central tendency, dispersion and shape of a dataset's distribution, excluding ``NaN`` values.\n\nLet's summarize the central tendency, dispersion and shape of a dataset's distribution. Out of all the columns, the only ones that made the most sense were Time, Amount, and Class (fraud or not fraud). The other 28 columns were transformed using what seems to be a PCA dimensionality reduction in order to protect user identities.\n\nThe data itself is short in terms of time (it\u2019s only 2 days long), and these transactions were made by European cardholders.","ee55448d":"## <a id='corr'>Correlation Among Explanatory Variables<\/a>\n\nHaving **too many features** in a model is not always a good thing because it might cause overfitting and worse results when we want to predict values for a new dataset. Thus, **if a feature does not improve your model a lot, not adding it may be a better choice.**\n\nAnother important thing is **correlation. If there is a very high correlation between two features, keeping both of them is not a good idea most of the time not to cause overfitting.** However, this does not mean that you must remove one of the highly correlated features. \n\nLet's find out top 10 features which are highly correlated with a price.","17dbcd58":"# **Credit Card Fraud Detection**\n**Anonymized credit card transactions labeled as fraudulent or genuine**\n\n<img src=\"https:\/\/i.imgur.com\/lBuWqxx.png\" \/>","373ddf77":"### <a id='modevelmatrix'>Model Evolution Matrix<\/a>\n\nEvery problem is different and derives a different set of values for a particular business use case , thus every model must be evaluated differently.\n\n**Let's get to know the terminology and Structure first**\n\nA confusion matrix is defined into four parts : __{ TRUE , FALSE } (Actual) ,{POSITIVE , NEGATIVE} (Predicted)__\nPositive and Negative is what you predict , True and False is what you are told\n\nWhich brings us to 4 relations : True Positive , True Negative , False Positive , False Negative <br>\n__P__ redicted - __R__ ows and __A__ ctual as __C__ olumns <br>\n\n<img src = 'https:\/\/github.com\/dktalaicha\/Kaggle\/blob\/master\/CreditCardFraudDetection\/images\/final_cnf.png?raw=true'>\n\n\n### Accuracy , Precision and Recall\n\n##### __Accuracy__ : The most used and classic classification metric : Suited for binary classification problems.\n\n$$  \\text{Accuracy} = \\frac{( TP + TN ) }{ (TP + TN + FP + FN )}$$\n\nBasically Rightly predicted results amongst all the results , used when the classes are balanced\n\n##### __Precision__ : What proportion of predicted positives are truly positive ? Used when we need to predict the positive thoroughly, sure about it !\n\n$$ \\text{Precision} = \\frac{( TP )}{( TP + FP )} $$\n\n##### __Sensitivity or Recall__ : What proportion of actual positives is correctly classified ? choice when we want to capture as many positives as possible\n\n$$ \\text{Recall} = \\frac{(TP)}{( TP + FN )} $$\n\n##### __F1 Score__ : Harmonic mean of Precision and Recall. It basically maintains a balance between the precision and recall for your classifier\n\n$$ F1 = \\frac{2 * (\\text{ precision } * \\text{ recall })}{(\\text{ precision } + \\text{ recall } )} $$\n\n<img src='https:\/\/i.imgur.com\/IYuqqic.gif' \/>\n\n**Precision as the name says, says how precise (how sure) is our model in detecting fraud transactions while recall is the amount of fraud cases our model is able to detect.**\n\n\n**In reference of our case**:\n\nRecall (True Positive Rate): % of all fraudulent transactions cases captured.\n\nPrecision: Out of all items labeled as fraud, what percentage of them is actually fraud?\n\nAccuracy: How correct the model is (misleading for fraud\/imbalanced data)\n\nF1 score: combination of recall and precision into one metric. F1 score is the weighted average of precision and recall, taking BOTH false positives and false negatives into account. Usually much more useful than accuracy, especially with uneven classes.","01d37203":"### Random Forest (RF)","a3ace8b2":"**There are 88 transaction recognised as True Postive, means they are orignally fraud transactions and our model precited them as fraud.**\n\n**True Negative** - 85286 (truely saying negative - genuine transaction correctly identified as genuine)\n\n**True Postive** - 88 (truely saying positive - fraud transaction correctly identified as fraud)\n\n**False Negative** - 56 ( falsely saying negative - fraud transaction incorrectly identified as genuine)\n\n**False Positive** - 13 ( falsely saying positive - genuine transaction incorrectly identified as fraud)","4576fda3":"## <a id='scalestand'>2. Scale  amount by Standardization<\/a>\n\nStandardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation.\n\n$$ z = \\frac{x_i - \\mu}{\\sigma} $$"}}