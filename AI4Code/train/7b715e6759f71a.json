{"cell_type":{"2f1194a1":"code","6b4f3389":"code","c07ead5e":"code","315270fa":"code","7883964d":"code","81340d5f":"code","00ae1d20":"code","d7893b93":"code","068a15b4":"code","02aa674b":"code","9e30e6db":"code","f6cfe97e":"code","0d4e9d96":"code","0d53d68f":"code","8b5bdd0a":"code","73dc6fd4":"code","1d9c1c34":"code","2b522497":"code","0fd75c32":"code","f7241eff":"code","423c5f1f":"code","9297f4e5":"code","a8cf5190":"code","42887707":"code","ea327261":"code","7931e38f":"code","b7765097":"code","117cab72":"code","cffd9f62":"code","eabf431a":"code","e2c5c9d6":"code","3c9ca83c":"code","2940ad6e":"code","ea9784ca":"code","3614915d":"code","d175f7d2":"code","205216c2":"code","bd58daa4":"code","d7df3aac":"code","e0e49cdb":"code","40d78db2":"code","58e021ca":"code","1701fb54":"code","8f1d5e34":"code","50b22433":"code","775dd1d7":"code","f11443d5":"code","7f3a3fcb":"code","1e07785b":"code","440ca69f":"code","59213a26":"code","72947d95":"code","4356c0dd":"code","8e40289a":"code","b9b9d72d":"code","bed8922a":"code","e69f9a0d":"code","a6b31f72":"code","209c02b9":"code","764764fa":"code","77be1e03":"code","dc834d58":"code","e08b5a9b":"code","5a610a5e":"code","7978fd0d":"code","cd093712":"code","562dbeda":"code","f39bebed":"code","7bcc1584":"code","6e03e418":"code","198a6f30":"code","5bf4f4eb":"code","3bbcea30":"code","81d2bef6":"code","a5a1331c":"code","4c48000c":"code","2713457a":"code","bf033341":"code","4680ad80":"code","79ed72af":"code","66811485":"code","f041f9ae":"code","7f99692c":"code","898ea385":"code","e8e8d484":"code","aa23825c":"code","d21dc8d5":"code","2e1495cd":"code","99394a4e":"code","cad5dd74":"code","5d6d9fc2":"code","0b2555a7":"code","b349ef7a":"code","287f7afe":"code","4c8678de":"code","7b4dba96":"code","697046e2":"code","4e7a994a":"code","5c8dd97f":"code","42b67b6b":"code","0c9ab18f":"code","5ed15687":"code","ff9a7335":"code","10404fdb":"code","e3b14fb2":"code","a0f2f8a0":"code","e46236de":"code","31504ad6":"code","ab09b93d":"code","3e4d6883":"code","529e15cc":"code","e3550361":"code","977052ac":"code","509925a3":"code","2f78b789":"code","f26fccde":"code","2d52973b":"code","d2882782":"code","bb6efcfc":"code","62d3b6cb":"code","c10e1e25":"code","079945c7":"code","7f7e8aee":"code","527d792e":"code","40aff0fd":"code","f6adb5cb":"markdown","763b8f65":"markdown","df7070cb":"markdown","7fea1611":"markdown","604d92f5":"markdown","238d4dba":"markdown","d5afd390":"markdown","6f2f9ce6":"markdown","dd8e7696":"markdown","6d6db19f":"markdown","471f2ac6":"markdown","91ca03e9":"markdown","a0bff463":"markdown","8529e92b":"markdown","48e20528":"markdown","a6f083d1":"markdown","7fdd9939":"markdown","214f96a4":"markdown","42cde12a":"markdown","a9adff30":"markdown","ae07b124":"markdown","e9650704":"markdown","eca7f3bc":"markdown","8411a43c":"markdown","617e20d6":"markdown","717da115":"markdown","aa1a10c4":"markdown","772dc49f":"markdown","7d42216e":"markdown","6efee71c":"markdown","b474874b":"markdown","0246adc3":"markdown","3705cec6":"markdown","bbf77065":"markdown","6c343007":"markdown"},"source":{"2f1194a1":"import numpy as np \nimport pandas as pd \n\nimport shap\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom sklearn.preprocessing import StandardScaler","6b4f3389":"train_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-apr-2021\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-apr-2021\/test.csv\")\nsample = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-apr-2021\/sample_submission.csv\")","c07ead5e":"train_df.head()","315270fa":"train_df.info()","7883964d":"test_df.info()","81340d5f":"import missingno as no","00ae1d20":"no.matrix(train_df)","d7893b93":"no.matrix(test_df)","068a15b4":"100 * train_df.isnull().sum()\/ len(train_df)","02aa674b":"100 * test_df.isnull().sum()\/ len(test_df)","9e30e6db":"train_df.drop(\"PassengerId\", axis=1).groupby(\"Sex\").median()[\"Age\"]","f6cfe97e":"def fill_age(df):\n    \"\"\"\n    Function impute NaN values in Age column based\n    on age median per Sex\n    \"\"\"\n    age_median_per_sex = train_df.drop(\"PassengerId\", axis=1).groupby(\"Sex\").median()[\"Age\"]\n    m_idx = df[df[\"Sex\"] == \"male\"].index\n    f_idx = df[df[\"Sex\"] == \"female\"].index\n    \n    df.loc[m_idx, \"Age\"] = df[df[\"Sex\"] == \"male\"].fillna(value=age_median_per_sex.values[1])\n    df.loc[f_idx, \"Age\"] = df[df[\"Sex\"] == \"female\"].fillna(value=age_median_per_sex.values[0])\n    df[\"Age\"] = df[\"Age\"].apply(lambda x: int(x))\n\nfill_age(train_df)\nfill_age(test_df)","0d4e9d96":"train_df[\"Age_interval\"] = pd.cut(train_df[\"Age\"], 9, \n                                  labels=[\"0-9\",\"10-19\",\"20-29\",\"30-39\",\"40-49\",\"50-59\",\"60-69\",\"70-79\",\"80-89\"])\ntest_df[\"Age_interval\"] = pd.cut(test_df[\"Age\"], 9, \n                                 labels=[\"0-9\",\"10-19\",\"20-29\",\"30-39\",\"40-49\",\"50-59\",\"60-69\",\"70-79\",\"80-89\"])","0d53d68f":"def plot_barplot(df, col):\n    x = df[col].value_counts().index\n    y = df[col].value_counts().values\n    \n    fig = go.Figure(data=[go.Bar(x=x, y=y)])\n    fig.update_traces(marker_color=px.colors.sequential.Greens, \n                      marker_line_color=\"rgb(8, 48, 107)\",\n                      marker_line_width=1.5, opacity=0.6)\n    fig.update_layout(title_text=f\"{col}\")\n    #fig.update_layout(xaxis=dict(ticktext=[\"Not Survived\", \"Survived\"],\n                                 #tickvals=[0,1]))\n    fig.show()","8b5bdd0a":"plot_barplot(train_df, \"Age\")","73dc6fd4":"def cabin_feat(df, col):\n    # Fill NaN values with None string\n    df[col] = df[col].fillna(\"None\")\n    \n    # Create new features\n    df[f\"has_{col}\"] = df[col].apply(lambda x: 1 if x != \"None\" else 0)\n    df[\"Deck\"] = df[col].apply(lambda x: x[0])\n    df.drop(col, axis=1, inplace=True)\n    return df","1d9c1c34":"train_df = cabin_feat(train_df, \"Cabin\")\ntest_df = cabin_feat(test_df, \"Cabin\")","2b522497":"plot_barplot(train_df, \"has_Cabin\")","0fd75c32":"train_df.head()","f7241eff":"train_df[\"Ticket\"] = train_df[\"Ticket\"].fillna(\"N\").map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else \"N\")\ntest_df[\"Ticket\"] = test_df[\"Ticket\"].fillna(\"N\").map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else \"N\")","423c5f1f":"train_df['Ticket'].unique()","9297f4e5":"train_df[\"Name\"].apply(lambda x: x.split(\",\")[0]).value_counts()","a8cf5190":"train_df[\"last_name\"] = train_df[\"Name\"].apply(lambda x: x.split(\",\")[0])\ntest_df[\"last_name\"] = test_df[\"Name\"].apply(lambda x: x.split(\",\")[0])\n\ntrain_df.drop(\"Name\", axis=1, inplace=True)\ntest_df.drop(\"Name\", axis=1, inplace=True)","42887707":"train_df.head()","ea327261":"#last_name = train_df[\"last_name\"].value_counts()\n#last_name = last_name[last_name.values > 100]\n\n#train_df[\"last_name\"] = train_df[\"last_name\"].apply(lambda x: \"others\" if x not in last_name.index else x)\n#test_df[\"last_name\"] = test_df[\"last_name\"].apply(lambda x: \"others\" if x not in last_name.index else x)","7931e38f":"from sklearn.preprocessing import LabelEncoder\ncol_names = [\"last_name\", \"Ticket\"]\nfor col in col_names:\n    le = LabelEncoder()\n    le.fit(train_df[col].values.tolist() + test_df[col].values.tolist())\n    train_df[col] = le.transform(train_df[col].values)\n    test_df[col] = le.transform(test_df[col].values)","b7765097":"plot_barplot(train_df, \"SibSp\")","117cab72":"plot_barplot(train_df, \"Parch\")","cffd9f62":"train_df[\"family\"] = train_df[\"SibSp\"] + train_df[\"Parch\"]\ntest_df[\"family\"] = test_df[\"SibSp\"] + test_df[\"Parch\"]","eabf431a":"train_df[\"family\"]","e2c5c9d6":"def family_size(x):\n    if x == 0:\n        return \"alone\"\n    elif  0 < x <= 5:\n        return \"small\"\n    elif 5 < x <= 10:\n        return \"medium\"\n    else:\n        return \"large\"","3c9ca83c":"train_df[\"family\"] = train_df[\"family\"].apply(family_size)\ntest_df[\"family\"] = test_df[\"family\"].apply(family_size)","2940ad6e":"plot_barplot(train_df, \"family\")","ea9784ca":"from sklearn.preprocessing import OrdinalEncoder","3614915d":"family_df = pd.DataFrame({\"family\": train_df[\"family\"].values})\nfamily_df_test = pd.DataFrame({\"family\": test_df[\"family\"].values})","d175f7d2":"ordinal_enc = OrdinalEncoder()\ntrain_df[\"family\"] = ordinal_enc.fit_transform(family_df).reshape(-1)\ntest_df[\"family\"] = ordinal_enc.transform(family_df_test).reshape(-1)","205216c2":"train_df.head()","bd58daa4":"plot_barplot(train_df, \"Pclass\")","d7df3aac":"print(f\" Missing values in Fare column = {train_df.Fare.isna().sum()}\")","e0e49cdb":"train_df[\"Fare\"] = train_df[\"Fare\"].fillna(value=train_df[\"Fare\"].median())\ntest_df[\"Fare\"] = test_df[\"Fare\"].fillna(value=train_df[\"Fare\"].median())","40d78db2":"train_df['Embarked'].mode()[0]","58e021ca":"train_df['Embarked'].value_counts(dropna=False)","1701fb54":"train_df[\"Embarked\"].fillna(value=train_df[\"Embarked\"].mode()[0], inplace=True)\ntest_df[\"Embarked\"].fillna(value=test_df[\"Embarked\"].mode()[0], inplace=True)","8f1d5e34":"train2 = train_df.drop(\"PassengerId\", axis=1).copy()","50b22433":"test2 = test_df.drop(\"PassengerId\", axis=1).copy()","775dd1d7":"dummy_cols = [\"Sex\", \"Age\", \"Embarked\", \"Deck\",\"Age_interval\"] ","f11443d5":"def convert_columns(df, cols):\n    \n    dummies_df = pd.get_dummies(df[cols], drop_first=True)\n    df.drop(cols, axis=1, inplace=True)\n    new_df = pd.concat([df, dummies_df], axis=1)\n    new_df.drop(\"PassengerId\", axis=1, inplace=True)\n    \n    return new_df","7f3a3fcb":"new_train = convert_columns(train_df, dummy_cols)\nnew_test = convert_columns(test_df, dummy_cols)","1e07785b":"new_train.head()","440ca69f":"corr_map = new_train.corr(method=\"spearman\")\nmask = np.zeros_like(corr_map)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(20,10))\nsns.heatmap(corr_map,\n            mask=mask,\n            annot=True,\n            linewidth=1,\n            linecolor=\"w\",\n            #square=True,\n            cbar=False,\n            cmap=\"coolwarm\")","59213a26":"new_train = new_train.drop(\"has_Cabin\", axis=1)\nnew_test = new_test.drop(\"has_Cabin\", axis=1)","72947d95":"cols = new_train.drop(\"Survived\", axis=1).columns","4356c0dd":"fig, axes = plt.subplots(6, 3, figsize=(16, 20))\n\nfor col, ax in zip(cols, axes.flatten()):\n    sns.histplot(x=new_train[col], ax=ax)\n    sns.histplot(x=new_test[col], ax=ax, color=\"red\", alpha=0.4)\n    plt.tight_layout()","8e40289a":"from sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=10)\nnew_train[\"kfold\"] = -1\n\nnew_train = new_train.sample(frac=1).reset_index(drop=True)\n\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(X=new_train, y=new_train[\"Survived\"])):\n    new_train.loc[valid_idx, \"kfold\"] = fold","b9b9d72d":"def run_training(algo, df, test_df, fold, oof):\n    train_df = df[df.kfold != fold].reset_index(drop=True)\n    valid_df = df[df.kfold == fold].reset_index(drop=True)\n    \n    xtrain = train_df.drop([\"Survived\", \"kfold\"], axis=1)\n    xvalid = valid_df.drop([\"Survived\", \"kfold\"], axis=1)\n    \n    sc = StandardScaler()\n    xtrain = sc.fit_transform(xtrain)\n    xvalid = sc.transform(xvalid)\n    test_df = sc.transform(test_df)\n    \n    ytrain = train_df[\"Survived\"].values\n    yvalid = valid_df[\"Survived\"].values\n    \n    algo.fit(xtrain, ytrain)\n    preds = algo.predict(xvalid)\n    sub_proba = algo.predict_proba(test_df)[:, 1]\n    train_proba = algo.predict_proba(xvalid)[:, 1]\n    \n    fold_acc = accuracy_score(yvalid, preds)\n    \n    print(f\"fold={fold+1}, accuracy={fold_acc}\")\n    oof[valid_idx] += fold_acc\n    \n    return oof, sub_proba, algo, train_proba","bed8922a":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score\n\nrfc = RandomForestClassifier(n_estimators=150)\n\nlevel2_df = pd.DataFrame()\ndf_proba = pd.DataFrame()\n\ntest_proba = np.zeros(len(new_test))\noof = np.zeros(len(new_train))\ntrain_pred = []\nfor fold in range(10):\n    oof, proba, rfc_model, tt_pred = run_training(rfc,new_train, new_test, fold, oof)\n    test_proba += proba\n    train_pred.append(tt_pred)\n    \nlevel2_df[\"randomforest\"] = np.hstack(train_pred)  \ndf_proba[\"randomforest\"] = test_proba \/ 10\nprint(f\"Mean accuracy after 10 folds {np.mean(oof)}\")","e69f9a0d":"#from sklearn.neighbors import KNeighborsClassifier\n#knn = KNeighborsClassifier(n_jobs=-1)\n\n#test_proba = np.zeros(len(new_test))\n#oof = np.zeros(len(new_train))\n#for fold in range(5):\n    #oof, proba = run_training(knn,new_train, new_test, fold, oof)\n    #test_proba += proba\n    \n#df_proba[\"knn\"] = test_proba \/ 5\n#print(f\"Mean accuracy after 5 folds {np.mean(oof)}\")","a6b31f72":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(use_label_encoder=False)\n\ntest_proba = np.zeros(len(new_test))\noof = np.zeros(len(new_train))\ntrain_pred = []\nfor fold in range(10):\n    oof, proba, xgb_model, tt_pred = run_training(xgb,new_train, new_test, fold, oof)\n    test_proba += proba\n    train_pred.append(tt_pred)\n    \nlevel2_df[\"xgboost\"] = np.hstack(train_pred)\ndf_proba[\"xgboost\"] = test_proba \/ 10\nprint(f\"Mean accuracy after 10 folds {np.mean(oof)}\")","209c02b9":"from lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier()\n\ntest_proba = np.zeros(len(new_test))\noof = np.zeros(len(new_train))\ntrain_pred = []\nfor fold in range(10):\n    oof, proba, lgbm_model, tt_pred = run_training(lgbm,new_train, new_test, fold, oof)\n    test_proba += proba\n    train_pred.append(tt_pred)\n\nlevel2_df[\"lgbm\"] = np.hstack(train_pred)\ndf_proba[\"lgbm\"] = test_proba \/ 10\nprint(f\"Mean accuracy after 10 folds {np.mean(oof)}\")","764764fa":"#df_proba[\"sum\"] = df_proba.sum(axis=1) \/ 4\n#df_proba[\"binary\"] = np.where(df_proba[\"sum\"] > 0.5, 1, 0)","77be1e03":"df_proba[\"wavg\"] = 0.2 * df_proba[\"randomforest\"] + 0.2 * df_proba[\"xgboost\"] + 0.5 * df_proba[\"lgbm\"]\ndf_proba[\"binary_wavg\"] = np.where(df_proba[\"wavg\"] > 0.5, 1, 0)","dc834d58":"df_proba.head()","e08b5a9b":"submission = sample.copy()\n\nsubmission[\"Survived\"] = df_proba[\"binary_wavg\"].values\nsubmission.to_csv(\"ensemble_sub_avg.csv\",index=False)","5a610a5e":"import optuna\nimport lightgbm as lgbm\nfrom sklearn.model_selection import train_test_split\nfrom optuna.pruners import SuccessiveHalvingPruner","7978fd0d":"X = new_train.drop([\"Survived\", \"kfold\"], axis=1)\ny = new_train[\"Survived\"].values","cd093712":"def objective(trial):\n    \n    xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, test_size=0.2, random_state=45)\n    sc = StandardScaler()\n    xtrain = sc.fit_transform(xtrain)\n    xvalid = sc.transform(xvalid)\n    \n    dtrain = lgbm.Dataset(xtrain, label=ytrain)\n    dvalid = lgbm.Dataset(xvalid, label=yvalid)\n    \n    params = {\n        \"objective\":\"binary\",\n        \"metric\":\"binary_logloss\",\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-1),\n        \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n        \"lambda_l2\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100)\n    }\n    \n    gbm = lgbm.train(params, dtrain)\n    preds = gbm.predict(xvalid)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(yvalid, pred_labels)\n    return accuracy\n\nstudy = optuna.create_study(direction=\"maximize\", pruner=SuccessiveHalvingPruner())\nstudy.optimize(objective, n_trials=100)\n\nprint(\"Number of finished trials:\", len(study.trials))\nprint(\"Best trial:\", study.best_trial.params)","562dbeda":"study.best_value","f39bebed":"lgbm_final = LGBMClassifier(**study.best_params)\n\ntest_proba = np.zeros(len(new_test))\noof = np.zeros(len(new_train))\ntrain_pred = []\nfor fold in range(10):\n    oof, proba, lgbm_optuna, tt_pred = run_training(lgbm_final,new_train, new_test, fold, oof)\n    test_proba += proba\n    train_pred.append(tt_pred)\n\nlevel2_df[\"lgbm_optuna_10fold\"] = np.hstack(train_pred)\ndf_proba[\"lgbm_optuna_10fold\"] = test_proba \/ 5\nprint(f\"Mean accuracy after 10 folds {np.mean(oof)}\")","7bcc1584":"submission = sample.copy()\n\nsc = StandardScaler()\nXtrain_full = sc.fit_transform(X)\nXtest = sc.transform(new_test)\n\nlgbm_model = LGBMClassifier(**study.best_params)\nlgbm_model.fit(Xtrain_full, y)\n\nsub_preds = lgbm_model.predict(Xtest)\nsubmission[\"Survived\"] = sub_preds\nsubmission.to_csv(\"lgbm_optuna10fold.csv\",index=False)","6e03e418":"train2.head()","198a6f30":"price_per_deck = train2.groupby(\"Deck\").mean()[\"Fare\"]\ntrain2[\"fare_mean_per_deck\"] = train2[\"Deck\"].map(price_per_deck.to_dict())\ntest2[\"fare_mean_per_deck\"] = test2[\"Deck\"].map(price_per_deck.to_dict())","5bf4f4eb":"cols = [\"Sex\", \"Embarked\", \"Deck\", \"Age_interval\"]\nle = LabelEncoder()\nfor col in cols:\n    le.fit(train2[col].values.tolist() + test2[col].values.tolist())\n    train2[col] = le.transform(train2[col].values)\n    test2[col] = le.transform(test2[col].values)","3bbcea30":"fig, axes = plt.subplots(5, 3, figsize=(16, 15))\n\nfor col, ax in zip(train2.drop(\"Survived\", axis=1).columns, axes.flatten()):\n    sns.histplot(x=train2[col], ax=ax)\n    sns.histplot(x=test2[col], ax=ax, color=\"red\",alpha=0.4)\n    plt.tight_layout()","81d2bef6":"skf = StratifiedKFold(n_splits=10)\ntrain2[\"kfold\"] = -1\n\ntrain2 = train2.sample(frac=1).reset_index(drop=True)\n\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(X=train2, y=train2[\"Survived\"])):\n    train2.loc[valid_idx, \"kfold\"] = fold","a5a1331c":"log_cols = [\"Fare\"]\nfor col in log_cols:\n    train2[col] = np.log1p(train2[col])\n    test2[col] = np.log1p(test2[col])","4c48000c":"plt.figure(figsize=(12,8))\nsns.histplot(x=train2[\"Fare\"])","2713457a":"clf = RandomForestClassifier(n_estimators=150)\n\ntest_proba = np.zeros(len(test2))\noof = np.zeros(len(train2))\ntrain_pred = []\nfor fold in range(10):\n    oof, proba, rfc_model2, tt_pred = run_training(rfc,train2, test2, fold, oof)\n    test_proba += proba\n    train_pred.append(tt_pred)\n\nlevel2_df[\"randomforest2\"] = np.hstack(train_pred)\ndf_proba[\"randomforest2\"] = test_proba \/ 10\nprint(f\"Mean accuracy after 10 folds {np.mean(oof)}\")","bf033341":"xgb = XGBClassifier(use_label_encoder=False)\n\ntest_proba = np.zeros(len(test2))\noof = np.zeros(len(train2))\ntrain_pred = []\nfor fold in range(10):\n    oof, proba, xgb_model2, tt_pred = run_training(xgb,train2, test2, fold, oof)\n    test_proba += proba\n    train_pred.append(tt_pred)\n    \nlevel2_df[\"xgboost2\"] = np.hstack(train_pred)\ndf_proba[\"xgboost2\"] = test_proba \/ 10\nprint(f\"Mean accuracy after 10 folds {np.mean(oof)}\")","4680ad80":"lgbm = LGBMClassifier()\n\ntest_proba = np.zeros(len(new_test))\noof = np.zeros(len(new_train))\ntrain_pred = []\nfor fold in range(10):\n    oof, proba, lgbm_model2, tt_pred = run_training(lgbm, train2, test2, fold, oof)\n    test_proba += proba\n    train_pred.append(tt_pred)\n    \nlevel2_df[\"lgbm2\"] = np.hstack(train_pred)\ndf_proba[\"lgbm2\"] = test_proba \/ 10\nprint(f\"Mean accuracy after 10 folds {np.mean(oof)}\")","79ed72af":"df_proba[\"wavg2\"] = 0.2 * df_proba[\"randomforest2\"] + 0.2 * df_proba[\"xgboost2\"] + 0.5 * df_proba[\"lgbm2\"]\ndf_proba[\"binary_wavg2\"] = np.where(df_proba[\"wavg2\"] > 0.5, 1, 0)\ndf_proba.head()","66811485":"submission = sample.copy()\n\nsubmission[\"Survived\"] = df_proba[\"binary_wavg2\"].values\nsubmission.to_csv(\"ensemble_sub1.csv\",index=False)\nsubmission[\"Survived\"] = np.where(df_proba[\"lgbm2\"] > 0.5, 1, 0)\nsubmission.to_csv(\"lgbm_sub10fold.csv\",index=False)","f041f9ae":"X = train2.drop([\"kfold\", \"Survived\"], axis=1)\ny = train2[\"Survived\"].values","7f99692c":"import lightgbm as lgbm\n\ndef objective(trial):\n    \n    xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, test_size=0.2, random_state=45)\n    sc = StandardScaler()\n    xtrain = sc.fit_transform(xtrain)\n    xvalid = sc.transform(xvalid)\n    \n    dtrain = lgbm.Dataset(xtrain, label=ytrain)\n    dvalid = lgbm.Dataset(xvalid, label=yvalid)\n    \n    params = {\n        \"objective\":\"binary\",\n        \"metric\":\"binary_logloss\",\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-1),\n        \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n        \"lambda_l2\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100)\n    }\n    \n    gbm = lgbm.train(params, dtrain)\n    preds = gbm.predict(xvalid)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(yvalid, pred_labels)\n    return accuracy\n\nstudy = optuna.create_study(direction=\"maximize\", pruner=SuccessiveHalvingPruner())\nstudy.optimize(objective, n_trials=100)\n\nprint(\"Number of finished trials:\", len(study.trials))\nprint(\"Best trial:\", study.best_trial.params)","898ea385":"study.best_value","e8e8d484":"submission = sample.copy()\n\nsc = StandardScaler()\nXtrain_full = sc.fit_transform(X)\nXtest = sc.transform(test2)\n\nlgbm_model = LGBMClassifier(**study.best_params)\nlgbm_model.fit(Xtrain_full, y)\n\nsub_preds = lgbm_model.predict(Xtest)\nsubmission[\"Survived\"] = sub_preds\nsubmission.to_csv(\"lgbm_optuna2.csv\",index=False)","aa23825c":"study.best_value","d21dc8d5":"lgbm = LGBMClassifier(**study.best_params)\n\ntest_proba = np.zeros(len(new_test))\noof = np.zeros(len(new_train))\ntrain_pred = []\nfor fold in range(10):\n    oof, proba, lgbm_model, tt_pred = run_training(lgbm,train2, test2, fold, oof)\n    test_proba += proba\n    train_pred.append(tt_pred)\n\nlevel2_df[\"lgbm_optuna2\"] = np.hstack(train_pred)\ndf_proba[\"lgbm_optuna2\"] = test_proba \/ 10\nprint(f\"Mean accuracy after 10 folds {np.mean(oof)}\")","2e1495cd":"submission = sample.copy()\nsubmission[\"Survived\"] = np.where(df_proba[\"lgbm_optuna2\"] > 0.5, 1, 0)\nsubmission.to_csv(\"lgbm_optuna2_10folds_2.csv\", index=False)","99394a4e":"shap.initjs()","cad5dd74":"explainer = shap.TreeExplainer(lgbm_model)\nshap_values = explainer.shap_values(X)","5d6d9fc2":"shap.force_plot(explainer.expected_value[1], shap_values[1][0,:], X.iloc[0,:])","0b2555a7":"shap.force_plot(explainer.expected_value[1], shap_values[1][:1000,:], X.iloc[:1000,:])","b349ef7a":"shap.summary_plot(shap_values[1], X)","287f7afe":"shap.summary_plot(shap_values, X)","4c8678de":"train2.head()","7b4dba96":"X = train2.drop([\"Survived\", \"kfold\"], axis=1)\ny = train2[\"Survived\"].values","697046e2":"import xgboost as xgb","4e7a994a":"def objective(trial):\n    \n    Xtrain, Xvalid, ytrain, yvalid = train_test_split(X, \n                                                      y, \n                                                      test_size=0.2,\n                                                      random_state=101)\n    \n    sc = StandardScaler()\n    Xtrain = sc.fit_transform(Xtrain)\n    Xvalid = sc.transform(Xvalid)\n    \n    dtrain = xgb.DMatrix(Xtrain, label=ytrain)\n    dvalid = xgb.DMatrix(Xvalid, label=yvalid)\n    \n    params = {\n        \"objective\": \"binary:logistic\",\n        \"use_label_encoder\": False,\n        \"eta\": trial.suggest_loguniform(\"eta\", 1e-2, 2e-1),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 12),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n        \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-4, 1.0),\n        \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-4, 1.0),\n        \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-4, 1.0),\n        \"subsample\": trial.suggest_uniform(\"subsample\", 0.3, 1.0),\n        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.3, 1.0)      \n    }\n    xgb_optuna = xgb.train(params, dtrain)\n    preds = xgb_optuna.predict(dvalid)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(yvalid, pred_labels)\n    return accuracy\n\nstudy = optuna.create_study(direction=\"maximize\", pruner=SuccessiveHalvingPruner())\nstudy.optimize(objective, n_trials=100)\n\nprint(\"Number of finished trials:\", len(study.trials))\nprint(\"Best trial:\", study.best_trial.params)","5c8dd97f":"study.best_value","42b67b6b":"xgb = XGBClassifier(**study.best_params, use_label_encoder=False)\n\ntest_proba = np.zeros(len(test2))\noof = np.zeros(len(train2))\ntrain_pred = []\nfor fold in range(10):\n    oof, proba, xgb_model2, tt_pred = run_training(xgb,train2, test2, fold, oof)\n    test_proba += proba\n    train_pred.append(tt_pred)\n    \nlevel2_df[\"xgboost_optuna_5fold2\"] = np.hstack(train_pred)\ndf_proba[\"xgboost_optuna_5fold2\"] = test_proba \/ 10\nprint(f\"Mean accuracy after 10 folds {np.mean(oof)}\")","0c9ab18f":"explainer = shap.TreeExplainer(xgb_model2, X)\nshap_values = explainer.shap_values(X)","5ed15687":"shap.initjs()\nshap.force_plot(explainer.expected_value, shap_values[1], X.iloc[0,:].index)","ff9a7335":"shap.summary_plot(shap_values, X)","10404fdb":"shap.force_plot(explainer.expected_value, shap_values[:1000,:], X.iloc[:1000,:])","e3b14fb2":"submission[\"Survived\"] = np.where(df_proba[\"xgboost_optuna_5fold2\"] > 0.5, 1, 0)\nsubmission.to_csv(\"xgboost_optuna_10fold_2.csv\", index=False)","a0f2f8a0":"df_proba = df_proba.drop(['wavg', 'binary_wavg','wavg2','binary_wavg2'], axis=1)","e46236de":"new_train2 = pd.concat([level2_df, train2], axis=1)\nnew_test2 = pd.concat([df_proba, test2], axis=1)","31504ad6":"from sklearn.linear_model import LogisticRegressionCV\n\nlr_model = LogisticRegressionCV(max_iter=100000)\n\ntest_proba = np.zeros(len(test2))\noof = np.zeros(len(train2))\nfor fold in range(10):\n    oof, proba, lr_model,_ = run_training(lr_model, new_train2, new_test2, fold, oof)\n    test_proba += proba\n    \nfinal_preds = test_proba \/ 10\nprint(f\"Mean accuracy after 10 folds {np.mean(oof)}\")","ab09b93d":"submission[\"Survived\"] = np.where(final_preds > 0.5, 1, 0)\nsubmission.to_csv(\"level2_sub2.csv\", index=False)","3e4d6883":"X = new_train2.drop([\"Survived\", \"kfold\"], axis=1)\ny = new_train2[\"Survived\"].values\n\nimport lightgbm as lgbm\n\ndef objective(trial):\n    \n    xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, test_size=0.2, random_state=45)\n    sc = StandardScaler()\n    xtrain = sc.fit_transform(xtrain)\n    xvalid = sc.transform(xvalid)\n    \n    dtrain = lgbm.Dataset(xtrain, label=ytrain)\n    dvalid = lgbm.Dataset(xvalid, label=yvalid)\n    \n    params = {\n        \"objective\":\"binary\",\n        \"metric\":\"binary_logloss\",\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-1),\n        \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n        \"lambda_l2\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100)\n    }\n    \n    gbm = lgbm.train(params, dtrain)\n    preds = gbm.predict(xvalid)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(yvalid, pred_labels)\n    return accuracy\n\nstudy = optuna.create_study(direction=\"maximize\", pruner=SuccessiveHalvingPruner())\nstudy.optimize(objective, n_trials=100)\n\nprint(\"Number of finished trials:\", len(study.trials))\nprint(\"Best trial:\", study.best_trial.params)","529e15cc":"study.best_value","e3550361":"lgbm = LGBMClassifier(**study.best_params)\n\ntest_proba = np.zeros(len(test2))\noof = np.zeros(len(train2))\nfor fold in range(10):\n    oof, proba,_ ,_ = run_training(lgbm, new_train2, new_test2, fold, oof)\n    test_proba += proba\n    \nfinal_preds = test_proba \/ 10\nprint(f\"Mean accuracy after 10 folds {np.mean(oof)}\")","977052ac":"submission[\"Survived\"] = np.where(final_preds > 0.48, 1, 0)\nsubmission.to_csv(\"level2_sub3.csv\", index=False)","509925a3":"from sklearn.metrics import roc_auc_score\nfrom functools import partial\nfrom scipy.optimize import fmin","2f78b789":"class OptimizerAUC:\n    def __init__(self):\n        self.coef_ = 0\n        \n    def auc_(self, coef, X, y):\n        x_coef = X * coef\n        predictions = np.sum(x_coef, axis=1)\n        auc_score = roc_auc_score(y, predictions)\n        return -1.0 * auc_score\n    \n    def fit(self, X, y):\n        partial_loss = partial(self.auc_, X=X, y=y)\n        init_coef = np.random.dirichlet(np.ones(X.shape[1]))\n        self.coef_ = fmin(partial_loss, init_coef, disp=True)\n        \n    def predict(self, X):\n        x_coef = X * self.coef_\n        predictions = np.sum(x_coef, axis=1)\n        return predictions\n    \n    \ndef run_training2(pred_df, fold, col_names):\n    train_df = pred_df[pred_df.kfold != fold].reset_index(drop=True)\n    valid_df = pred_df[pred_df.kfold == fold].reset_index(drop=True)\n    \n    xtrain = train_df[col_names].values\n    xvalid = valid_df[col_names].values\n    \n    sc = StandardScaler()\n    xtrain = sc.fit_transform(xtrain)\n    yvalid = sc.transform(xvalid)\n    \n    ytrain = train_df.Survived.values\n    yvalid = valid_df.Survived.values\n    \n    opt = OptimizerAUC()\n    opt.fit(xtrain, ytrain)\n    preds = opt.predict(xvalid)\n    \n    fold_auc = roc_auc_score(yvalid, preds)\n    print(f\"fold={fold}, auc={fold_auc}\")\n    \n    return opt.coef_","f26fccde":"level2_df[\"Survived\"] = train2[\"Survived\"].values\nlevel2_df[\"kfold\"] = train2[\"kfold\"].values","2d52973b":"col_names = [col for col in level2_df.columns if col not in [\"Survived\", \"kfold\"]]","d2882782":"level2_df.columns","bb6efcfc":"col_names = ['lgbm_optuna_10fold', 'lgbm2', 'xgboost_optuna_5fold2']","62d3b6cb":"coef = []\nfor j in range(10):\n    coef.append(run_training2(level2_df, j, col_names))","c10e1e25":"coef = np.array(coef)\ncoef_mean = np.mean(coef, axis=0)\nprint(coef_mean)","079945c7":"col_names","7f7e8aee":"wt_avg = (\n    coef_mean[0] * level2_df[\"lgbm_optuna_10fold\"].values\n    + coef_mean[1] * level2_df[\"lgbm2\"].values\n    + coef_mean[2] * level2_df[\"xgboost_optuna_5fold2\"].values\n)\nprint(\"Optimal acc after finding coefs\")\nwt_acc = accuracy_score(level2_df[\"Survived\"], np.where(wt_avg > 0.5, 1, 0))\nprint(f\"Optimized weighted avg of acc: {wt_acc}\")","527d792e":"wt_avg_sub = (\n    coef_mean[0] * df_proba[\"lgbm_optuna_10fold\"].values\n    + coef_mean[1] * df_proba[\"lgbm2\"].values\n    + coef_mean[2] * df_proba[\"xgboost_optuna_5fold2\"].values\n)","40aff0fd":"submission[\"Survived\"] = np.where(wt_avg_sub > 0.5, 1, 0)\nsubmission.to_csv(\"optimal_weights_sub.csv\", index=False)","f6adb5cb":"**Cabin**","763b8f65":"## Fill NaN values first","df7070cb":"**Pclass**","7fea1611":"I wonder how often and what name was in the orginal titanic dataset. Does anyone remember? ","604d92f5":"I am going to drop \"Name\" column and leave first_name column for label encoding. I want to check if any feature engineering will have any efect on our results by training model with this feature and evaluate effect.","238d4dba":"We can see that newly created column has_Cabin is perfectly correlated with Deck_N column, therefore I will drop has_Cabin column. ","d5afd390":"**Name**","6f2f9ce6":"## Ensemble submission","dd8e7696":"Mean accuracy after 5 folds 0.7819000000000002 to beat","6d6db19f":"## Different approach","471f2ac6":"## Xgboost + optuna","91ca03e9":"## Ensemble submission 2","a0bff463":"## Explain LGBMClassifier predictions with Shap","8529e92b":"Hi Kagglers\nFinally we have some meaningful features we can do some feature engineering so in this notebook I just want to play with those features and have some fun. ","48e20528":"**Fare**","a6f083d1":"**SibSp and Parch**\n\nWe could create a family column divided into e.g. categories like (no family, small, medium, large)","7fdd9939":"pclass - Passenger Ticket class : Class 1, 2 and 3.\n\nName - Name of the passenger\n\nsex - Sex of the Passenger\n\nAge - Age in years of the Passenger\n\nsibsp - Number of siblings \/ spouses aboard the Titanic\n\nparch - Number of parents \/ children aboard the Titanic\n\nTicket - Ticket number\n\nFare - Passenger fare\n\nCabin - Cabin number\n\nEmbarked - Port of Embarkation shows the port from which the passenger boarded the titanic\n\n       C - Cherbourg\n       Q - Queenstown\n       S - Southampton","214f96a4":"**Embarked**","42cde12a":"Fill missing values with column mode.","a9adff30":"## Lightgbm hyperparameter optimalization with optuna","ae07b124":"I am going to fill nan values with median of this column.","e9650704":"Mean accuracy after 5 folds 0.7647400000000004","eca7f3bc":"**Heatmap**","8411a43c":"Mean accuracy after 5 folds 0.7789","617e20d6":"**Dummy variables**","717da115":"## Creating folds with StratifiedKFold","aa1a10c4":"The best prediction I've achieved so far is with lgbm_optuna2_5folds_2 which scored 0.80423","772dc49f":"## Finding optimal weights","7d42216e":"**Age column**","6efee71c":"## Xgboost predictions with shap","b474874b":"## LGBM + optuna","0246adc3":"**Columns distribution**","3705cec6":"**Ticket**","bbf77065":"## Level 2 model aproach","6c343007":"## Submission"}}