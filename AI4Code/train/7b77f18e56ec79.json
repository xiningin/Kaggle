{"cell_type":{"88376619":"code","3c59e5dc":"code","837e1831":"code","fa1419dc":"code","631b7d97":"code","3b5c76fa":"code","94e1ce67":"code","9fdcd43c":"code","ea7d69b5":"code","eeb475bd":"code","0c71bcd9":"code","990df95e":"code","edaa86b2":"code","d05e0233":"code","a6ad673c":"code","896321e1":"code","42fe8862":"code","df370547":"code","7a9c2390":"code","cc316370":"code","74fd0af3":"code","980af89a":"code","5ac54f4a":"code","0feb3bda":"code","ec8ff227":"code","6db9433a":"code","5089f141":"code","0448ba6a":"code","e8aa3465":"code","2353f9bc":"code","0c23871e":"code","d6b9b5bc":"markdown","6d6c3e2e":"markdown","9c65b775":"markdown","e586c91e":"markdown","ed61ae0f":"markdown","242b1548":"markdown","277b9c0a":"markdown","67eb109c":"markdown","6e882833":"markdown","390e5e3a":"markdown","43ed307d":"markdown"},"source":{"88376619":"import pandas as pd\nimport numpy as np\nfrom datetime import date, timedelta\nfrom collections import Counter\nfrom operator import itemgetter\nimport os\nimport warnings\n\n#Visualisation Library\nimport matplotlib.pyplot as plt\nimport cufflinks as cf\nimport seaborn as sns\nfrom wordcloud import WordCloud \n\n#Preprocessing Libraries\nfrom nltk.corpus import stopwords\nimport string\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom gensim.utils import simple_preprocess\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#Topic Modelling Libraries\n\nfrom sklearn.decomposition import NMF\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer, TfidfTransformer\nfrom gensim.models.coherencemodel import CoherenceModel\nfrom gensim.corpora.dictionary import Dictionary\nfrom gensim.models.nmf import Nmf\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3c59e5dc":"from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\ncf.go_offline()","837e1831":"warnings.filterwarnings(\"ignore\", category=DeprecationWarning)","fa1419dc":"dataset = pd.read_csv('..\/input\/reddit-vaccine-myths\/reddit_vm.csv', error_bad_lines=False);\ndataset.shape","631b7d97":"dataset.head()","3b5c76fa":"#Creation of StopWords\n\nstop_words = stopwords.words('english') #Call the StopWords Function from the Library\nstop_words.extend(['from', 'subject', 're', 'edu', 'use', 'op']) #I am removing some extra words like 'OP' and 'Edu' by extending the stopwords list\nstop_words[0:5] #Sample of StopWords","94e1ce67":"#Functions to Preprocess \n\ndef pre_process(s):\n    s = s.str.lower()\n    s = s.str.replace(r'(?i)\\brt\\b', \"\")\n    s = s.str.replace(' via ',\"\") \n    s = s.replace(r'@\\w+', \"\", regex = True)\n    s = s.replace(r'http\\S+', '', regex = True)\n    s = s.replace(r'www.[^ ]+', '', regex = True)\n    s = s.replace(r'[0-9]+', '', regex = True)\n    s = s.replace(r'''[\u00ac!\"#$%&()*+,-.\/:;<=>?@[\\]\u2019^'_`\\{|}~]''', '', regex=True)\n    return s\n\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef lemmatizing(words):\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef final_text(words):\n     return ' '.join(words)\n","9fdcd43c":"dataset.body = pre_process(dataset['body']) \ndataset = dataset.dropna(subset = ['body']) #Drop Empty Rows\n\ndataset['token'] = remove_stopwords(dataset['body']) \ndataset['token'] = dataset['token'].apply(lambda x: lemmatizing(x)) \ndataset['body_combined_text'] = dataset['token'].apply(lambda x: final_text(x))\n\ndataset[dataset['body_combined_text'].duplicated(keep=False)].sort_values('body_combined_text').head() #View Duplicates\ndataset = dataset.drop_duplicates(['body_combined_text']) #Remove Duplicates\n\n#Plot Comments Per Month\n\ndataset['date'] = pd.to_datetime(dataset['timestamp'])\ndataset['Hour'] = dataset['date'].apply(lambda x: x.hour)\ndataset['Month'] = dataset['date'].apply(lambda x: x.month)\ndataset['Month'] = dataset['Month'].replace({1:'Jan',2:'Feb',3:'Mar',4:'Apr',5:'May',6:'Jun', 7:'Jul',8:'Aug',9:'Sep',10:'Oct',11:'Nov',12:'Dec'})\ndataset_month = dataset.Month.value_counts().reindex(['Jan', 'Feb', 'Mar', 'Apr', \"May\", 'Jun','Jul', \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"])\ndataset_month.iplot()","ea7d69b5":"a = dataset['token']\na = [x for i in a for x in i]\ntop_20 = pd.DataFrame(Counter(a).most_common(20), columns=['word', 'freq']) #Check Word Frequency via Dataframe sorted by mos frequent.\nprint(top_20)","eeb475bd":"no_of_unique_words = len(set(a)) #Check Number of Unique Words in the Dataset.\nprint(\"There are \" + str(no_of_unique_words) + \" unique words in the dataset.\")","0c71bcd9":"#Functions For Analysit\n\ndef top_words(topic, n_top_words):\n    return topic.argsort()[:-n_top_words - 1:-1]  \n\n\ndef topic_table(model, feature_names, n_top_words):\n    topics = {}\n    for topic_idx, topic in enumerate(model.components_):\n        t = (topic_idx)\n        topics[t] = [feature_names[i] for i in top_words(topic, n_top_words)]\n    return pd.DataFrame(topics)\n\ncoherence_scores = []\n\ndef find_cv(): #Find Number of K\n    for num in topic_nums:\n        nmf = Nmf(corpus=corpus, num_topics=num, id2word=dictionary,normalize=True)\n        cm = CoherenceModel(model=nmf, texts=texts, dictionary=dictionary, coherence='c_v')\n        coherence_scores.append(round(cm.get_coherence(), 5))\n\ndef compare_cv():\n    for m, cv in zip(topic_nums, coherence_scores):\n        print(\"K =\", m, \" CV: \", round(cv, 2))\n    scores = list(zip(topic_nums, coherence_scores))\n    best_cv = sorted(scores, key=itemgetter(1), reverse=True)[0][0]\n    print('\\n')\n    return best_cv","990df95e":"topic_nums = list(np.arange(5, 55 + 1, 3)) #The range that we will run NMF on\ntexts = dataset.token\ndictionary = Dictionary(texts) #create dictionary \ndictionary.filter_extremes(no_below=5, no_above=0.8, keep_n=2000) \ncorpus = [dictionary.doc2bow(text) for text in texts] ","edaa86b2":"#Call the Functions\nfind_cv()  \n#Store the return value of compare_cv function to best_cv to use in the NMF\nbest_cv = compare_cv()\n","d05e0233":"tfidf_vectorizer = TfidfVectorizer(min_df=3, max_df=0.85, max_features=5000, ngram_range=(1, 2), preprocessor=' '.join)\ntfidf = tfidf_vectorizer.fit_transform(texts)\ntfidf_fn = tfidf_vectorizer.get_feature_names()\nnmf = NMF(n_components= best_cv , init='nndsvd', solver='cd',random_state=42).fit(tfidf)\ndocweights = nmf.transform(tfidf_vectorizer.transform(texts))\n\nn_top_words = 10\n\ntopic_df = topic_table(nmf, tfidf_fn, n_top_words).T\ntopic_df\n\ntopic_df['topics'] = topic_df.apply(lambda x: [' '.join(x)], axis=1) \ntopic_df['topics'] = topic_df['topics'].str[0] \n\ntopic_df = topic_df['topics'].reset_index()\ntopic_df.columns = ['topic_num', 'topics']\n\ntopic_df.head()","a6ad673c":"docweights = nmf.transform(tfidf_vectorizer.transform(texts))\n\nn_top_words = 10\n\ntopic_df = topic_table(nmf, tfidf_fn, n_top_words).T #Creates a Topic Table to see the Top Words in each topic number.\ntopic_df","896321e1":"topic_df['topics'] = topic_df.apply(lambda x: [' '.join(x)], axis=1) #Joins all words in one column for easy interpretation of topics.\ntopic_df['topics'] = topic_df['topics'].str[0]  \n\ntopic_df = topic_df['topics'].reset_index()\ntopic_df.columns = ['topic_num', 'topics']\n\ntopic_df.head()","42fe8862":"dataset['topic_num'] = docweights.argmax(axis=1)\ndataset = dataset.merge(topic_df[['topic_num','topics']],\"left\") #Merge the Topic dataset to our main dataset to find which Reddit post belongs to which topic\ncolumns = ['score', 'url', 'comms_num', 'created', 'body_combined_text', 'date', 'Hour', 'Month']\ndataset.drop(columns = columns, inplace = True) #Removes Unncessary Columns\ndataset.head()","df370547":"A = tfidf_vectorizer.transform(texts)\nW = nmf.components_\nH = nmf.transform(A)\n\nprint('A = {} x {}'.format(A.shape[0], A.shape[1]))\nprint('W = {} x {}'.format(W.shape[0], W.shape[1]))\nprint('H = {} x {}'.format(H.shape[0], H.shape[1]))","7a9c2390":"r = np.zeros(A.shape[0])\n\nfor row in range(A.shape[0]):\n    r[row] = np.linalg.norm(A[row, :] - H[row, :].dot(W), 'fro')\n\nsum_sqrt_res = round(sum(np.sqrt(r)), 3)\nprint(sum_sqrt_res)\n\ndataset['resid'] = r","cc316370":"resid_data = dataset[['topic_num','resid']].groupby('topic_num').mean().sort_values(by='resid') \n\n\nresid_data.iplot( kind = 'bar', title = 'Average Residuals by Topic', xTitle = 'Topic Number', yTitle = 'Residuals')","74fd0af3":"resid_data.head()","980af89a":"def word_cloud(df_weights, n_top_words=20, is_print=True, is_plot=True):\n    s_word_freq = pd.Series(df_weights['count'])\n    s_word_freq.index = df_weights['word']\n    di_word_freq = s_word_freq.to_dict()\n    cloud = WordCloud(width=1600, height=800, background_color='white').generate_from_frequencies(di_word_freq)\n    plt.figure(1,figsize=(13, 10))\n    if is_print:\n        print(df_weights.iloc[:n_top_words,:])\n    if is_plot:\n        plt.imshow(cloud)\n        plt.axis('off')\n        plt.show()\n    return cloud","5ac54f4a":"dataset['joined'] = dataset['token'].apply(lambda x: final_text(x))\nfrequent_NN = pd.Series(' '.join(dataset['joined']).split()).value_counts()\n","0feb3bda":"cv = CountVectorizer(max_df = 0.6, min_df = 10, max_features=None, ngram_range=(1,4))\nX = cv.fit_transform(dataset['joined'])\ncvec = cv.fit(dataset.joined)\nbag_of_words = cvec.transform(dataset.joined)\nfeature_names = cvec.get_feature_names()","ec8ff227":"transformer = TfidfTransformer()\ntfidf = transformer.fit_transform(bag_of_words)\nword_cnts = np.asarray(bag_of_words.sum(axis=0)).ravel().tolist()  # for each word in column, sum all row counts\ndf_cnts = pd.DataFrame({'word': feature_names, 'count': word_cnts})\ndf_cnts = df_cnts.sort_values('count', ascending=False)\nweights = np.asarray(tfidf.mean(axis=0)).ravel().tolist()\ndf_weights = pd.DataFrame({'word': feature_names, 'weight': weights})\ndf_weights = df_weights.sort_values('weight', ascending=False)\n\ndf_weights = df_weights.merge(df_cnts, on='word', how='left')\ndf_weights = df_weights[['word', 'count', 'weight']]","6db9433a":"topic_no_0 = dataset[dataset['topic_num'] == 0]","5089f141":"topic_no_0","0448ba6a":"topic_no_0 = dataset[dataset['topic_num'] == 0] #Filter to get only those comments which belongs to Topic 0\nfrequent_NN = pd.Series(' '.join(topic_no_0['joined']).split()).value_counts()  #Creates a new dataframe that joins and count the frequency of the number\n","e8aa3465":"cv = CountVectorizer(max_df = 0.2, min_df = 2, max_features=None, ngram_range=(1,2))\nX = cv.fit_transform(topic_no_0['joined'])\ncvec = cv.fit(topic_no_0.joined)\nbag_of_words = cvec.transform(topic_no_0.joined)\nfeature_names = cvec.get_feature_names()\n","2353f9bc":"transformer = TfidfTransformer()\ntfidf = transformer.fit_transform(bag_of_words)\n\nword_counts = np.asarray(bag_of_words.sum(axis=0)).ravel().tolist()\ndf_cnts = pd.DataFrame({'word': feature_names, 'count': word_counts})\ndf_cnts = df_cnts.sort_values('count', ascending=False)\nweights = np.asarray(tfidf.mean(axis=0)).ravel().tolist()\n\n\ndf_weights = pd.DataFrame({'word': feature_names, 'weight': weights})\ndf_weights = df_weights.sort_values('weight', ascending=False)\ndf_weights","0c23871e":"df_weights = df_weights.merge(df_cnts, on='word', how='left')\ndf_weights = df_weights[['word', 'count', 'weight']]\ndf_weights\ncloud_all = word_cloud(df_weights, is_print=True)","d6b9b5bc":"### **Import the Dataset**","6d6c3e2e":"### **Word Cloud of Topic O**","9c65b775":"**About the Dataset:**\n    \n    This dataset is a collection of posts from the subreddit r\/VaccineMyths. \n    The dataset is unfiltered and may contain phrases that may be considered offensive by some. \n\n    The dataset has eight columns:\n        title - relevant for posts\n        score - relevant for posts - based on impact, number of comments\n        id - unique id for posts\/comments\n        url - relevant for posts - url of post thread\n        commns_num - relevant for post - number of comments to this post\n        created - date of creation\n        body - relevant for posts\/comments - text of the post or comment\n        timestamp - timestamp\n\n    For this notebook, I will be running the dataset through Non-Negative Matrix Factorisation (NMF) Topic Modelling. NMF is an unsupervised dimensionality-reduction technique by decomposing vectors into lower dimensional representation.","e586c91e":"### **Data Preprocessing**","ed61ae0f":"### **Call the Functions on the Data (Plotting)**","242b1548":"### **Calculating the number of topics (k)**\n\nNMF is an unsupervised machine learning techniques. All we need as input are the dataset and the number of topics (k). For this notebook, I will\ncalculate the coherence score to determine the appropriate number of topics. As such, I am running NMF against the range number of topics I have picked. As you can see below, I made a range of 5, 55. The k with the highest number of coherence score is the optimum number of k for this given dataset.","277b9c0a":"### **Import Libraries**","67eb109c":"# Topic Modelling Using Non-Negative Matrix Factorisation","6e882833":"### **Topic Modeling using NMF**","390e5e3a":"### **Calculating Residuals**\n\nA residual of zero means that the topic has approximated the text perfectly. We will visualise which topic number has the lowest residual score and create a wordcloud out of that topic number.","43ed307d":"### **Most Common Words in the Dataset**"}}