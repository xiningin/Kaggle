{"cell_type":{"458d2fb6":"code","e733826f":"code","c142cf4d":"code","1f0b6dbf":"code","07dda9fd":"code","446cd218":"code","1448890f":"code","0e399b28":"code","d9f1823a":"code","be4d63ee":"markdown","90b897e8":"markdown","79dcd111":"markdown","cd1bf8cb":"markdown","ac9e61bd":"markdown","de797a6e":"markdown","54881233":"markdown","6d5b7a1c":"markdown","80b689b9":"markdown","284a607f":"markdown"},"source":{"458d2fb6":"import numpy as np\nimport numpy.linalg as LA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n%matplotlib inline","e733826f":"exp5_feature=np.genfromtxt(\"\/kaggle\/input\/Exp5_featureVector_training.txt\",delimiter=',')\nexp5_featureNorm=LA.norm(exp5_feature,axis=1)\nexp5_unit_feature=exp5_feature\/exp5_featureNorm[:,None]\nexp5_dotproduct=np.dot(exp5_unit_feature,exp5_unit_feature.T)\nplt.figure(figsize=(10,10))\nsns.heatmap(exp5_dotproduct,cmap='coolwarm')","c142cf4d":"feature= np.genfromtxt('..\/input\/featureVector.txt',delimiter=',')\nlabelfile = open(\"..\/input\/label\", \"r\")\nlabel = labelfile.readlines()","1f0b6dbf":"featureNorm=LA.norm(feature,axis=1)\nunit_feature=feature\/featureNorm[:,None]\ndotproduct=np.dot(unit_feature,unit_feature.T)\nplt.figure(figsize=(10,10))\n\nfig, axarr = plt.subplots(1, 2,figsize=(20,10),gridspec_kw={'width_ratios': [3, 3]})\nsns.heatmap(dotproduct,cmap='coolwarm',ax=axarr[0])\nsns.violinplot(dotproduct,ax=axarr[1])","07dda9fd":"l=[]\nfor i in range(495):\n    l.append((dotproduct[i] > 0.8).sum())\nproblem=np.where(np.array(l)> 5)[0].tolist() # counts on those value > 0.8 for each row\nm=[]\nfor i in problem:\n    m.append(label[i])\nprint(m)\nprint(len(m))","446cd218":"remove_dim=np.delete(dotproduct,problem,axis=0)\nremove_dim=np.delete(remove_dim,problem,axis=1)\nremove_dim.shape\nfig, axarr = plt.subplots(1, 2,figsize=(20,10),gridspec_kw={'width_ratios': [3, 3]})\nsns.heatmap(remove_dim,cmap='coolwarm',ax=axarr[0])\nsns.violinplot(remove_dim,ax=axarr[1])\nprint(\"Means of the dot product Matrix: {:.4f}\".format(np.mean(remove_dim)))\nprint(\"Median of the dot product Matrix: {:.4f}\".format(np.median(remove_dim)))\nprint(\"25% quantile of the dot product Matrix: {:.4f}\".format(np.quantile(remove_dim,0.25)))\nprint(\"75% quantile of the dot product Matrix: {:.4f}\".format(np.quantile(remove_dim,0.75)))\n\nl=[]\nfor i in range(328):\n    l.append((remove_dim[i] > 0.8).sum())\nprint(\"dot product martix value > 0.8(count >=2 ) for each row :\"+str(len(np.where(np.array(l)>= 2)[0].tolist())))","1448890f":"from sklearn.decomposition import PCA\npca=PCA()\npca.fit(unit_feature)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');","0e399b28":"pca=PCA(n_components=256)\npca.fit(unit_feature)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\npca_unit_feature=pca.transform(unit_feature)\nprint(pca_unit_feature.shape)","d9f1823a":"pca_unit_featureNorm=LA.norm(pca_unit_feature,axis=1)\npca_unit_feature=pca_unit_feature\/pca_unit_featureNorm[:,None]\npca_dotproduct=np.dot(pca_unit_feature,pca_unit_feature.T)\npca_dotproduct=np.delete(pca_dotproduct,problem,axis=0) # remove those images which is dim in compare to others \npca_dotproduct=np.delete(pca_dotproduct,problem,axis=1) # remove those images which is dim in compare to others \nfig, axarr = plt.subplots(1, 2,figsize=(20,10),gridspec_kw={'width_ratios': [3, 3]})\nsns.heatmap(pca_dotproduct,cmap='coolwarm',ax=axarr[0])\nsns.violinplot(pca_dotproduct,ax=axarr[1])\n\nprint(\"Means of the PCA product Matrix: {:.4f}\".format(np.mean(pca_dotproduct)))\nprint(\"Median of the PCA dot product Matrix: {:.4f}\".format(np.median(pca_dotproduct)))\nprint(\"25% quantile of the PCA dot product Matrix: {:.4f}\".format(np.quantile(pca_dotproduct,0.25)))\nprint(\"75% quantile of the PCA dot product Matrix: {:.4f}\".format(np.quantile(pca_dotproduct,0.75)))\n\nl=[]\nfor i in range(328):\n    l.append((pca_dotproduct[i] > 0.8).sum())\nprint(\"PCA dot product martix value > 0.8(count >=2 ) for each row :\"+str(len(np.where(np.array(l)>= 2)[0].tolist())))","be4d63ee":"It is easy to shows that those problem images are much more darker than those are normal, so, the brightness of the image got a huge effect on the feature as well, may be it is due to the edge is not as sharp as those with normal images.\n\nSo, we will remove those image from the dot product matrix. ","90b897e8":"# Low cost Diamond Feature extraction using Heart and Arrow Image - Preliminary investigation summary\n\n### Exclusive Summay\n\nIn recent year, the demand of technology development for diamond tracking has show an rising trend due to the increasing business need for XXXX. Those technologies includes blockchain, laser mark and nano mark etc has been using widely in the industry to build up the trust on the source of diamond. However, while the widely application of blockchain on the industry is only focusing on transaction tracking, the development of diamond identification is comparatively slow and with high entry boundary. In usual, the technology for diamond identifcation usually include material analysis, optical analysis or combine method to classify rather the diamond is the same, but the cost of investement is high in the begining and the process require specialist to operate,thus, the question of is there anyother method which is lower in cost and easy to use for diamond identification has been rised.\n\nFollowing the direction, the report has analysis the fessibility on the use of machine learning tecnology to accomplish the target, the result has shows evidence that the use of Heart and Arrow image on machine learning can be used on diamond identification purpose.\n\n### Introduction\nHeart and Arrow image are usually appear on round brilliant diamonds, the formation of image is variate due to the cut and symmerty of the polishing process of the diamond accroding to gemsociety. In result, evey polished diamond will result in a different H&A image and the idea is to apply Deep learning model on those image to extract feature for each diamond that can be distinguish from others. Here is two sample H&A image source from SRK diamond : \n\n\n[![1829560131.jpg](https:\/\/i.postimg.cc\/zBWZDyFL\/1829560131.jpg)](https:\/\/postimg.cc\/jwskZd9K)\n[![1829380011.jpg](https:\/\/i.postimg.cc\/fb1Qhcmn\/1829380011.jpg)](https:\/\/postimg.cc\/94PndqLJ)\n\n### Data Source\nData soruce to used in this report, souce from SRK diamond \n\n|     | Universal Set|\n|-----|---------------|\n|size |   2478        |\n|Diamond Color| D-M   | \n|Diamond Cut  | 3EX   |\n|Diamond Clarity| IF-VVS2|   \n\nA subset of 249 image(set A) is selected from the universal set to perform initial study to reduce computational time.\n\nFurthermore, as the image is come in pair, preprocessing on the image is done to separate into heart image and arrow image and each of them are resized to 256x256 for learning\n\n### Model\nThe model used in the report is an autoencoder with VGG16 as encoding model, the decoder is construct with 9 conv2D layer with BCEwithLogisticLoss as loss function.\n\nHere is a sample visualization of the model source from springer, detail implementation is different.  \n\n<img src=\"https:\/\/media.springernature.com\/full\/springer-static\/image\/art%3A10.1007%2Fs11042-019-08597-8\/MediaObjects\/11042_2019_8597_Fig2_HTML.png?as=webp\" width=\"1024\">\n\nThe bottleneck in betweeen encoder and decoder will be a 2048(8x8x32) feature vector which we are interesting in.\n\n#### First few runs \nIn the first few experiment, heart image is used to feed into the autoencoder to see if any useful feature can be capture from it, the avg reconstruction error(bceloss) is around 0.27(training eror) in the best run. Here is a sample of reconstruction by the model (tranining sample).\n\n\nReconstruction from the model\n\n\n| Original Image | Reconstruct Image |\n|--------------|-----------------|\n|<img width=\"200\">[![1833330270-jpg-transformation.jpg](https:\/\/i.postimg.cc\/L8xS6ynG\/1833330270-jpg-transformation.jpg)](https:\/\/postimg.cc\/mP1Jmy3S)|<img width=\"200\">[![1833330270-jpg-reconstruct.jpg](https:\/\/i.postimg.cc\/6QTX7dZq\/1833330270-jpg-reconstruct.jpg)](https:\/\/postimg.cc\/p9N745qM)|\n\n\nThe image shows a pretty result on image reconstuction, but what about the feature(those 8 * 8 * 32 feature maps)? As there is only 1 sample per diamond, the data size is insufficient for any types of classifier to work with, so there is no way to figure out rather the feature is useful or not. However, we can borrow the concept of dot product form geometry to perform an initial examination on those feature extracted and hope it can give some insight if the feature is fine or not. In this case , we can reshape the 8 * 8 * 32 feature map to a vector with 2048 feature(deminsion) and use dot product to measure how similarity are they. \n\nHere, we give a short summary on dot product,dot product takes in 2 vector as input and return a single value (1 to -1) to represent the angle between them  : \n\n| Value | Meaning |\n|-----|--------|\n|  1  | means that they are in the same direction (0 degree) |\n|  0  | means they are orthogonal (90 degree) |\n| -1  | mean they are in opposit direction (180 degree) |","79dcd111":"Summary of statistics on original feature dot product and PCA dot product :\n\n| |Original(2048)|PCA(256)|\n|-|--------------|--------|\n|means|0.0817|0.1091|\n|median|0.0391|0.0729|\n|25% quantile|-0.0048|0.0294|\n|75% quantile|0.1223|0.1495|\n|count(>0.8)|35\/2~18|49\/2~25|\n** dot product is a symmerty matrix\n\nFrom the above statistics, it is shows that distribution of PCA is right shifted in compare to the original one while it still maintain a reasonable error rate around 7% (25\/328). So, there is evidence suggest the feature of the arrow image can be further reduce to under 300, which is around 6 times smaller than the original one. ","cd1bf8cb":"### Limitation\n- The model validation set is not large enough\n- The target value used in training is 1 or 0 , which has restrict to orthogonal, it may result to find a orthogonal subspace for those vectors only, but it reality, it may more than that, the restriction should be relax\n- rotation,other types of transformation is not included in this report\n- A classifier should be use instead of dot product measure\n- Single image limitation for classification.\n\n### Conclusion\n\nFor the next steps : \n1. Reduce the data set to only those with normal brightness, but it is almost reduce 30% of data, with the drop of 8% accuracy\n2. Reduce the feature space to 256\n3. Rotation image testing\n4. build up image set with rotation for single image limitation\n5. relax the target value from (1,0) to (1,-1) ? ","ac9e61bd":"From the above diagram, The dot product result shows that the feature from heart image after the encoder is almost pointing to the same direction, the avg result is 0.99, it is not a good sign in team of using the encoded feature to distinct between diamond, moreover, the same result appear for the arrow image, however, it is discover that arrow image has contains much more information that heart.\n\nIn conclusion :\n\n- the autoencoder model succefully archive the image reconstruction pupose, but the extracted feature is not much useful in terms of classification.\n- We should focus on arrow image as it should provide more information on feature.\n- may be the encoder doesn't extract feature at all, the pretty reconstructed image is due to a powerful decoder which even if you give garbage to it, it will also output a image you want. Or\n\n<h3>May be the problem is about the question itself<\/h3>\n\nLet's look at the H&A image from the introduction again and ask what is the feature of it ? You can easily answer with 8 Hearts and 8 Arrows present in 8 directions, the size is variate somehow and it is where the problem is, autoencoder actually has done a greate job on extraction, it has extract feature on those images, but it is a general feature among those sample present to the model. \n\n<center>The real question to the problem is that can the model extract feature that can distinct each other given that those images are in general similar.<\/center><br>\n\n\nFollowing the direction, we have come up with a model that is similar to the autoencoder above but with some modification on the loss function as follow :\n\n1. The model will takes 2 images as input and feed to the autoencoder one by one, then set target to 1 if two images is the same, otherwise set to 0.\n2. together with those 2 reconstructed images, 2 feature codes is also return as well\n3. Loss function is change to constructed with 2 part : similarity loss and reconstruction loss \n\n    a.similarity loss is calculated with dot product between those codes and the target with MSE\n    \n    b.reconstruction loss is the same above with BCEloss, but we will only taking either one of the image loss.\n    \n    c.Total loss = Similarity Loss + Reconstruction Loss\n\nHere is a simple diagram to illustrate the model\n\n<img width=\"1024\">[![pairinput3.jpg](https:\/\/i.postimg.cc\/X7zMRrm0\/pairinput3.jpg)](https:\/\/postimg.cc\/YhYygCtX)\n\n\nThe modified model is applied on arrow image with the following arrangement:\n\n- The data set apply the usual schema with 80-20 split for training and validation purpose (Training : 1983 , Validation : 495)\n- For the training set and validation set, each set is separated in to half with half is belongs to different pair and the othe half is assigned to same pair of image.\n- In terms of accuracy, a threshold of 0.05 is defined for the differrence between dot product and target, if it is larger than it, it is count as wrong.\n\n\nReuslt of the modified model :\n\nAfter running the model for 45 epochs, the total training loss has converage to around 62 and the validation accuracy is flipping between 400 to 412 out of 495 validation sample.\n\n<b>(Update on 29\/7\/2020 - The modle is re-trained after those \"dim\" images is removed, the result is plot in \"Remove Dim\" , Data size 1689 , Training:1351,validation: 338 ) <\/b>\n    \nHere is the testing and validation loss plotting related to this model :\n\n|    | Original Training | Remove Dim Training | Original Validation | Remove Dim Validation|\n|----|-------------------|---------------------|---------------------|----------------------|\n|Total loss|<img width=\"300\">![training%20loss.svg](attachment:training%20loss.svg)|<img width=\"300\">![remove_dark_training%20loss.svg](attachment:remove_dark_training%20loss.svg)|<img width=\"300\">![validation%20loss.svg](attachment:validation%20loss.svg)|<img width=\"300\">![remove_dark_validation%20loss.svg](attachment:remove_dark_validation%20loss.svg)|\n|Accuracy|![training%20acc.svg](attachment:training%20acc.svg)|![remove_dark_training%20acc.svg](attachment:remove_dark_training%20acc.svg)|![validation%20acc.svg](attachment:validation%20acc.svg)|![remove_dark_validation%20acc.svg](attachment:remove_dark_validation%20acc.svg)|\n|Similarityt loss|![training%20similiarty%20loss.svg](attachment:training%20similiarty%20loss.svg)|![remove_dark_training%20similiarty%20loss.svg](attachment:remove_dark_training%20similiarty%20loss.svg)|![validation%20similiarty%20loss.svg](attachment:validation%20similiarty%20loss.svg)|![remove_dark_validation%20similiarty%20loss.svg](attachment:remove_dark_validation%20similiarty%20loss.svg)|\n|Reconstruction loss|![training%20reconstruction%20loss.svg](attachment:training%20reconstruction%20loss.svg)|![remove_dark_training%20reconstruction%20loss.svg](attachment:remove_dark_training%20reconstruction%20loss.svg)|![validation%20reconstruction%20loss.svg](attachment:validation%20reconstruction%20loss.svg)|![remove_dark_validation%20reconstruction%20loss.svg](attachment:remove_dark_validation%20reconstruction%20loss.svg)|","de797a6e":"### Analysis\n\nFrom the above training and validation plot, both experiment shows that the total loss is start to converge after epoch 10, also, the contribution is mostly from reconstruction loss in compare to similarity It is easy to observe that the model on similarity has suffer from over fit as the loss of training is nearly zero(both experiment) after epoch 10 while on the validation side, the error is running between 0.082-0.078(original) and between 0.022-0.018(remove dim).\n\nOne interesting pattern is that, the validation accuracy for the Remove dim data set is rapidly approch the highest point(256\/338) at around 10 epoch and start to decline afterward with not much incerase in the similiarty loss. Furthermore, it is shows that after removing those dim images, both training and validaion curve is much more smoother than the origianl one.\n\nThe result from the modified model gives evidence to support the investigation on rather H&A image can be use as feature to identify diamond with the application of deep learning. For further analysis, the result model is applied on the validation set to extract feature for individal diamond.\n\n\n** Heart image is also re-trained using the modified model above, however, the reconstruction loss is converge to 0.4 for both training and validation and from the log console, the sum of dot product is all equal to 0 after 15 epoch, so the experiment also confirm that the feature of arrow image is much more richer than heart image.\n\n#### Dot Product Matrix for Validation Set (Original set)\nA dot product matrix is created to shows the similarity between them .","54881233":"From the above plot, it is shows that the cumulative variance is almost fully capture by around 200~300 components, so it suggested that the original feature space with 2048 dimension can be further reduce to 200-300 features, in result , we will try to project the unit_feature matrix to a lower dimension(with those problem images deleted) space with 256 components and see how is the dot product is affected.","6d5b7a1c":"From the above heat map, it is shows that most of the diamond feature are with value smaller than 0.5 and the result is confirm by the distribution plot. Hoever, from the heat map, it is also spot that some of the feature vector shows a pattern with intermediate red dot appear, it is representing that the vector is getting dot product value that is near to 1 with other's, it can also reconfirmed by the distribution plot that there is a small distribution build up in the range between 0.8 and 1.0, thus, we will sort out those dot product values that is greater than 0.8 for further analysis.","80b689b9":"There are 167 diamond feature vectors that has at least 5 dot product values with others that is greater than 0.8, here is a snapshot on part of it: <\/br>\n![dim_arrow.jpg](attachment:dim_arrow.jpg)\nHere is a comparison between those normal images<\/br>\n![normal_arrow.jpg](attachment:normal_arrow.jpg)","284a607f":"After removing those problem images from the data, the heat map and distribution plot has shows a clear message on those feature, which is they can be separatable at least by dot product similiarty measure.<\/br>\n\n#### Feature (Original set)\nTo further asscess the properties of the extract feature, the distribution of those feature vectors value will be study, the unit_feature data will be separated into 2 subset , one is for those 167 diamond feature vectors that has at least 5 dot product values with others that is greater than 0.8 , and the other one is the rest of them. However, as 2048 feature is hard to visualize and carry out analysis, it is suggested to apply PCA to see if any linear dependence is found on those feature and see if the dim can be reduce. "}}