{"cell_type":{"6312dc25":"code","291a162f":"code","7278e4cf":"code","e09a1ba5":"code","9a38d345":"code","d4a5fd11":"code","4a077cbb":"code","759e6527":"code","78010b2c":"code","78d06add":"code","8abedfc9":"code","dd75a750":"code","bc39c575":"code","2eb77764":"code","7255403b":"code","3ca40414":"code","eaf4d1c7":"code","45db6473":"code","b1f7e1a0":"code","3c58ff8d":"code","1275a4e8":"code","a4ea4f1b":"code","e357324f":"code","22dea17c":"code","0e15ef39":"code","37e58689":"code","22588f3a":"code","a9ccf5be":"code","26cfc181":"code","be5b0905":"markdown","5e07cc28":"markdown","5ac6a36b":"markdown","99b62167":"markdown","9abef17d":"markdown","9f7e2c38":"markdown","b19fbe42":"markdown","7c299940":"markdown","a21ef5d5":"markdown","352a1493":"markdown","5b61bee8":"markdown","5db2e66d":"markdown","a5dc72ba":"markdown","a0ed6f71":"markdown","20930a97":"markdown","ea162b18":"markdown","a59737ff":"markdown","8f996b32":"markdown","a2e19021":"markdown","3fb8e8cb":"markdown","138229cf":"markdown","9e9130ec":"markdown","5005b86e":"markdown","866e41a5":"markdown","f2e64d08":"markdown","ac00f18b":"markdown","42dc8c0b":"markdown","0cec4d8f":"markdown","674574b4":"markdown","e9359efd":"markdown","00c7a043":"markdown","e16c8f5b":"markdown","4405b5bf":"markdown","bfd41604":"markdown","9026aed0":"markdown","0f0d4935":"markdown"},"source":{"6312dc25":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nwarnings.filterwarnings('ignore')","291a162f":"dataset = pd.read_csv('..\/input\/employee-future-prediction\/Employee.csv')\ndataset","7278e4cf":"dataset.isnull().sum()","e09a1ba5":"for col in dataset:\n  print(col, '=', dataset[col].unique())","9a38d345":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndataset['Education']=le.fit_transform(dataset['Education'])\ndataset['Gender']=le.fit_transform(dataset['Gender'])\ndataset['EverBenched']=le.fit_transform(dataset['EverBenched'])\ndataset","d4a5fd11":"dataset = pd.get_dummies(dataset, drop_first=False)\ndataset","4a077cbb":"leave_or_not = dataset['LeaveOrNot']\ndataset = dataset.drop('LeaveOrNot', axis=1)\ndataset.insert(loc=len(dataset.columns), column='LeaveOrNot', value=leave_or_not)\ndataset","759e6527":"#Pandas style correlation table,\ndummy_df = dataset.drop(['City_Bangalore', 'City_New Delhi', 'City_Pune'], axis=1)\ncorr = dummy_df.corr()\ncorr.style\\\n    .background_gradient(cmap='bwr')\\\n    .set_precision(2)","78010b2c":"#Seaborn heatmap visualization\nplt.figure(figsize = (9, 9))\nplt.title(\"Feature Correlations\", fontsize = 14)\nsns.heatmap(corr, annot = True, cmap = \"Blues\")","78d06add":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif[\"features\"] = dummy_df.columns\nvif[\"VIF\"] = [variance_inflation_factor(dummy_df.values, i) for i in range(dummy_df.shape[1])]\nvif","8abedfc9":"dummy_df = dummy_df.drop('JoiningYear', axis=1)\nvif.drop(vif.index, inplace=True)\nvif[\"features\"] = dummy_df.columns\nvif[\"VIF\"] = [variance_inflation_factor(dummy_df.values, i) for i in range(dummy_df.shape[1])]\nvif","dd75a750":"dummy_df = dummy_df.drop('PaymentTier', axis=1)\nvif.drop(vif.index, inplace=True)\nvif[\"features\"] = dummy_df.columns\nvif[\"VIF\"] = [variance_inflation_factor(dummy_df.values, i) for i in range(dummy_df.shape[1])]\nvif","bc39c575":"dataset = dataset.drop(['JoiningYear', 'PaymentTier'], axis=1)\ndataset","2eb77764":"x = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values","7255403b":"print(x)","3ca40414":"print(y)","eaf4d1c7":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=27)","45db6473":"print(x_train)","b1f7e1a0":"print(y_train)","3c58ff8d":"print(x_test)","1275a4e8":"print(y_test)","a4ea4f1b":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","e357324f":"print(x_train)","22dea17c":"print(x_test)","0e15ef39":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=27)\nclassifier.fit(x_train, y_train)","37e58689":"y_pred = classifier.predict(x_test)","22588f3a":"result_np = np.concatenate((y_pred.reshape(len(y_pred), 1), (y_test.reshape(len(y_test), 1))), 1)\nresult = pd.DataFrame(result_np, columns=['Prediction', 'Real_Value'])\nresult","a9ccf5be":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","26cfc181":"from sklearn.model_selection import cross_val_score\nval_score = cross_val_score(estimator=classifier, X = x_train, y=y_train, cv=10)\nprint(\"Accuracy: {:.2f} %\".format(val_score.mean()*100))\nprint(\"Std. Dev: {:.2f} %\".format(val_score.std()*100))","be5b0905":"### 3.1 Separated the Training Set and Test Set","5e07cc28":"#### 2.5.2 Variance Inflation Factor (VIF)","5ac6a36b":"Surprisingly, the accuracy is low. only 74% from the prediction is right.","99b62167":"Wow. From the table shown above (only first 5 and last 5), there are no missing prediction.","9abef17d":"The VIF is reduced. It is acceptable to build a model from this features. Don't forget to drop those 2 features from the original dataset.","9f7e2c38":"## Step 3 - Data Preprocessing","b19fbe42":"### 2.5 Finding Correlation for Every Feature","7c299940":"### 4.1 Model Building and Training with the Training Set","a21ef5d5":"### 3.2 Scaling the Training Set","352a1493":"Scale the value of training set to between -3 and 3 to make sure no feature overwhelm the others. The test set is scaled using same scale as training set.","5b61bee8":"There are no blank value or missing value in the dataset.","5db2e66d":"Reordering the LeaveOrNot column to the rightmost of the dataset. This is to mark the LeaveOrNot column as the dependant variable.","a5dc72ba":"### 3.1 Transforming Dataset to Array","a0ed6f71":"### 5.2 Get the Accuracy Report with K-Fold Cross Validation","20930a97":"### 5.1 Get the Accuracy Report","ea162b18":"## Step 1 - Importing Library","a59737ff":"## Step 2 - Dataset Preparation","8f996b32":"### 4.2 Predicting the Test Set Result","a2e19021":"#### 2.5.1 Feature Correlation","3fb8e8cb":"## Step 5 - Measuring Model Accuracy","138229cf":"From the dataset description, we see a linear logic for education. Higher education means higher value for a person. Therefore, label encoder is suitable for education feature.","9e9130ec":"We can see there are 4,653 set of data for 9 features. From this function, we could see which feature is categorical or numerical also. Most of the features is categorical.","5005b86e":"## Step 4 - Classification Model Building using Random Forest Classifier","866e41a5":"### 2.4 Dataset Reordering","f2e64d08":"### 2.2 Checking Unique Value for Every Categorical Features","ac00f18b":"Separated the features as x and the dependant variable as y. Both is transformed to numpy array for modelling function to work.","42dc8c0b":"Possible colinearity in JoiningYear, PaymentTier, and Age. Logically, people joining earlier, now will become older and have better payroll. Let's try to drop the biggest VIF, JoiningYear.","0cec4d8f":"Again, lets drop PaymentTier.","674574b4":"### 2.3 Encoding Categorical  Value","e9359efd":"The accuracy measured using 10-Fold Cross Validation is 71-75% using Random Forest Classifier.","00c7a043":"We drop the City becuase there are no linear order in it. It is interesting indeed, from the graph, we can see that every feature correlation to LeaveOrNot is very small. We proceed to analyze the VIF (Variance Inflation Factor).","e16c8f5b":"Random state is set to 27 because...again...my lucky number.","4405b5bf":"Separated the overall dataset to training set to train the classification model later on and test set to validate and calculate accuracy. Random state is 27 because.....just my lucky number.","bfd41604":"Encoded Education to : 0 if Bachelor, 1 if Master, 2 if PHD.<br>\nEncoded Gender to : 1 if Male, 0 if Female.<br>\nEncoded Everbenched to : 1 ig Yes, 0 if No.","9026aed0":"### 2.2 Checking Blank Value or Missing Value","0f0d4935":"One-Hot Encoded the remaining categorical features which is City."}}