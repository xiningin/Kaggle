{"cell_type":{"92baa4a9":"code","4f4e84e1":"code","6312fc6f":"code","a5fd8bf5":"code","59b61269":"code","b2d91041":"code","796dad82":"code","a9566d5e":"code","f3851cbf":"code","34e07dfd":"code","96e39f8e":"code","f9873a93":"code","dad83003":"code","9cc090a6":"code","c126597e":"code","52518bee":"code","3d1c3516":"markdown"},"source":{"92baa4a9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4f4e84e1":"!pip install glove-python-binary\n!pip install pyspellchecker","6312fc6f":"!python -m spacy download 'en_core_web_md'\n","a5fd8bf5":"#Direct loading of the model fails at times, so created a link for model\n!python -m spacy link en_core_web_md en_core_web_md_link","59b61269":"#To remove all stop words\nstopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]","b2d91041":"#Some text like names, team names and cricket specific terms modified to be a single word\nmodify_dict = {'Rashid Khan' : 'Rashid-Khan',\n'du Plessis' : 'du-Plessis',\n'de Kock' : 'de-Kock',\n'Rohit Sharma' : 'Rohit',\n'Axar Patel' : 'Axar',\n'Avesh Khan' : 'Avesh-khan',\n'Shivam Mavi' : 'Shivam-Mavi',\n'Sam Curran' : 'Sam-Curran',\n'Tom Curran' : 'Tom-Curran',\n'JPR Scantlebury-Searles' : 'Searles',\n'M Nabi' : 'Nabi',\n'Barinder Sran' : 'Barinder-Sran',\n'Monu Kumar' : 'Monu-Kumar',\n'Shahbaz Ahmed' : 'Shabaz-Ahmed',\n'Kartik Tyagi' : 'Karthik-Tyagi',\n'S Dhawan' : 'Dhawan',\n'Arshdeep Singh' : 'Arshdeep-Singh',\n'Abdul Samad' : 'Samad',\n'Harpreet Brar' : 'Harpreet-Brar',\n'Ravi Bishnoi' : 'Ravi-Bishnoi',\n'Chennai Super Kings': 'csk',\n'Mumbai Indians':'mi',\n'Kolkata Knight Riders':'kkr',\n'Delhi Capitals':'dc',\n'Kings XI Punjab':'kxip',\n'Rajastan Royals':'rr',\n'Sunrisers Hyderabad':'srh',\n'Royal Challengers Banglore':'rcb',\n'Rinku Singh': 'Rinku-Singh',\n'1 run':'single',\n'no run':'dot-ball',\n'2 runs':'two-runs',\n'3 runs':'three-runs',\n'4 runs': 'four',\n'FOUR runs': 'four',\n'5 runs':'five-run',\n'6 runs': 'six',\n'SIX runs': 'six',\n'no ball':'no-ball',\n'1 wide':'1-wide',\n'leg bye':'leg-bye',\n'direct hit': 'direct-hit',\n' off stump ':' off-stump ',\n' leg stump ':' leg-stump ',\n' off side ':' off-side ',\n' leg side ':' leg-side ',\n'run out':'run-out',\n' silly mid on':' silly-mid-on',\n' silly mid off':' silly-mid-off',\n' silly point':' silly-point',\n' leg slip':' leg-slip',\n' deep midwicket':' deep-midwicket',\n' deep backward point':' deep-backward-point',\n' deep cover':' deep-cover',\n' deep square leg':' deep-square-leg',\n' deep point':' deep-point',\n' deep fine leg':' deep-fine-leg',\n' deep extra cover':' deep-extra-cover',\n' short square leg':' short-square-leg',\n' short fine leg':' short-fine-leg',\n' short midwicket':' short-midwicket',\n' short third man':' short-third-man',\n' backward point':' backward-point',\n' square leg':' square-leg',\n' extra cover':' extra-cover',\n' fine leg':' fine-leg',\n' long off':' long-off',\n' long on':' long-on',\n' mid on':' mid-on',\n' mid off':' mid-off',\n' third man':' third-man',\n' cover point':' cover-point',\n' short leg':' short-leg',\n' long leg':' long-leg',\n' leg gully':' leg-gully',\n' leg slip': ' leg-slip',\n' hard length ':' hard-length ',\n' short ball ':' short-ball ',\n' in the slot ':' in-the-slot ',\n' low full toss ':' low-full-toss ',\n' full toss ':' full-toss ',\n' good length ':' good-length ',\n' run out ':' run-out ',\n' down leg ':' down-leg ',\n' slower ball ':' slower-ball ',\n' slog swept ':' slog-swept ',\n' outside off ':' outside-off ',\n' knuckle ball ':' knuckle-ball ',\n' knuckleball ':' knuckle-ball ',\n' yorker length ':' yorker ',\n}","796dad82":"from spellchecker import SpellChecker\nfrom collections import Counter\nfrom tqdm import tqdm\nimport spacy\n\ndef get_spell_errors(text_input):\n  inputArray = []\n  for row in text_input:\n    rowArray = []\n    for word in row.split(\" \"):\n      rowArray.append(word)\n    #converted df to a list of lists to spell check\n    inputArray.append(rowArray)  \n  \n  spell = SpellChecker()\n  errors = list()\n  for array in inputArray:\n    out = spell.unknown(array)\n    #Here unknown returns words with spelling mistakes\n    errors.extend(out)\n  counts = Counter(errors)\n  spell_errors = list()\n  for key in counts:\n    # If a word with spelling mistake occurs more than twice, its most likely a cricket \n    # specific term (eg: off-side) so we shoudnt remove those\n    if(counts[key]<3):\n      spell_errors.append(key)\n  return spell_errors\n\ndef get_lemma(input):\n  nlp = spacy.load('en_core_web_md_link', disable=['parser','ner','tagger'])\n  lemma_dict = {}\n  docs = nlp.pipe(input, n_threads = 4)\n  for doc in tqdm(docs):\n      word_seq = []\n      for token in doc:\n          if (token.text not in lemma_dict) and (token.pos_ is not \"PUNCT\"):\n              lemma_dict[token.text] = token.lemma_\n  del docs\n  return lemma_dict  ","a9566d5e":"import pandas as pd\n\ndef joinData():\n    ipl2020 = pd.read_csv(r'\/kaggle\/input\/cricinfo-ipl-commentary\/IPL2020 - Commentary Data.csv')\n    ipl2019 = pd.read_csv(r'\/kaggle\/input\/cricinfo-ipl-commentary\/IPL2019 - Commentary Data.csv')\n    ipl2018 = pd.read_csv(r'\/kaggle\/input\/cricinfo-ipl-commentary\/IPL2018 - Commentary Data.csv')\n\n    txtInput = pd.concat([ipl2018, ipl2019, ipl2020])\n\n    txtInput['short_text'] = txtInput['short_text'].str.replace(\",\", \"\")\n    txtInput['long_text'] = txtInput['long_text'].fillna(\"\")\n\n    #remove all entries with ball speeds as it is not available always\n    for special_char in ['kph','kmph', 'km\/h', '\\.', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0']:\n        txtInput['long_text'] = txtInput['long_text'].str.replace(special_char, \"\")\n\n    #Adding traling whitespace to help with replace operation, ie otherwise first or last word wont be replaced\n    txtInput['commentary'] = \" \" + txtInput['short_text'] + \" \" + txtInput['long_text'] + \" \"\n    txtInput = txtInput.drop([\"over\", \"short_text\", \"long_text\"], axis=1)\n\n    #Sometimes removal of special characters results in a valid word\n    for special_char in [';', '\\'', ',', '?', '!', '\"', ')', '(', '\\'s ', '\\'ll', '\\[','\\]']:\n        txtInput['commentary'] = txtInput['commentary'].str.replace(special_char, \"\")\n\n    for word in modify_dict.keys():\n        txtInput['commentary'] = txtInput['commentary'].str.replace(word, modify_dict[word])\n\n    for word in stopwords:\n      token = \" \" + word + \" \"\n      txtInput['commentary'] = txtInput['commentary'].str.replace(token, \" \")\n    \n    spell_errors = get_spell_errors(txtInput['commentary'])\n\n    for word in spell_errors:\n        token = \" \" + word + \" \"\n        txtInput['commentary'] = txtInput['commentary'].str.replace(token, \" \")\n\n    #Removing leading and trailing space that was added before\n    txtInput['commentary'] = txtInput['commentary'].str.strip()    \n\n    txtInput['commentary'] = txtInput['commentary'].str.lower().replace(\"  \", \" \")\n    \n    for _ in range(10):\n    #since we are replacing stop words and errors with space, extra space might be created if they are present continously  \n        txtInput['commentary'] = txtInput['commentary'].str.replace(\"  \", \" \")\n\n    input_array = []\n    #lemma_dict to replace each word with its base form\n    lemma_dict = get_lemma(txtInput['commentary'])\n    for sentence in txtInput['commentary']:\n        sentenceArray =[]\n        for word in sentence.split(\" \"):\n            # Adding try here as words with - are split in lemmatization\n            try:\n              sentenceArray.append(lemma_dict[word])\n            except:\n              sentenceArray.append(word)\n        input_array.append(sentenceArray)\n    #txtInput.to_csv(r'combined.csv', index=False)\n    return input_array\n\ndef generate_glove(input, num_epochs=100):\n    # importing the glove library\n    from glove import Corpus, Glove\n    # creating a corpus object\n    corpus = Corpus()\n    # training the corpus to generate the co occurence matrix which is used in GloVe\n    corpus.fit(input, window=10)\n    # creating a Glove object which will use the matrix created in the above lines to create embeddings\n    # We can set the learning rate as it uses Gradient Descent and number of components\n    glove = Glove(no_components=50, learning_rate=0.03)\n\n    glove.fit(corpus.matrix, epochs=num_epochs, no_threads=4, verbose=True)\n    glove.add_dictionary(corpus.dictionary)\n    glove.save('glove.model')\n    return glove","f3851cbf":"inputArray = joinData()\nglove = generate_glove(inputArray, 100)\nlen(glove.dictionary)","34e07dfd":"#to save the generated encodings in a single file\nimport numpy as np\n\nglove_dict = glove.dictionary\nglove_vecs = glove.word_vectors\nword_vecs = []\nfor item in glove_dict:\n  #print(glove_vecs[glove_dict[item]])\n  word_vec= []\n  word_vec.append(item)\n  word_vec.extend(glove_vecs[glove_dict[item]])\n  word_vecs.append(word_vec)\n\nnp_array = np.array(word_vecs)#.reshape(-1, 51)\nnp.savetxt('\/kaggle\/working\/file.txt', np_array, fmt='%s')","96e39f8e":"# Get the interactive Tools for Matplotlib\n%matplotlib inline\nimport matplotlib.pyplot as plt\n#plt.style.use('ggplot')\n\nfrom sklearn.decomposition import PCA\n\nfrom gensim.test.utils import datapath, get_tmpfile\nfrom gensim.models import KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\n\nglove_file = datapath('\/kaggle\/working\/file.txt')\nword2vec_glove_file = get_tmpfile(\"file.word2vec.txt\")\nglove2word2vec(glove_file, word2vec_glove_file)\n\nmodel = KeyedVectors.load_word2vec_format(word2vec_glove_file)","f9873a93":"model.most_similar('six')","dad83003":"model.most_similar(positive=['csk','mi'])","9cc090a6":"result = model.most_similar(positive=['chahal','chahar','kuldeep'], negative=['krunal'])\nprint(\"{}: {:.4f}\".format(*result[1]))","c126597e":"def display_pca_scatterplot(model, words=None, sample=0):\n    if words == None:\n        if sample > 0:\n            words = np.random.choice(list(model.vocab.keys()), sample)\n        else:\n            words = [ word for word in model.vocab ]\n        \n    word_vectors = np.array([model[w] for w in words])\n    twodim = PCA().fit_transform(word_vectors)[:,:2]\n\n    plt.figure(figsize=(10,10))\n    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n    for word, (x,y) in zip(words, twodim):\n        plt.text(x+0.05, y+0.05, word)","52518bee":"#Similar players are grouped together\ndisplay_pca_scatterplot(model,words=['ashwin','tahir','kuldeep','bumrah','rabada','mishra','chahal','jadeja','krunal'])","3d1c3516":"Not lets play around with the generated data"}}