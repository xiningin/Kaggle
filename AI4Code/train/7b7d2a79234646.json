{"cell_type":{"fbf5fdcd":"code","65212e65":"code","a5e8ffec":"code","3e9915d8":"code","6b052e18":"code","1dbfc6af":"code","98446a60":"code","0a43813e":"code","434c504f":"code","7edbb01a":"code","d46d790e":"code","26ffef5d":"code","8f75308e":"code","5514884e":"code","476d02a5":"markdown"},"source":{"fbf5fdcd":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nimport lightgbm as lgb\nimport plotly.express as px\n!pip install bhtsne\nfrom bhtsne import tsne\n!pip install umap-learn\nimport umap\nimport scipy.sparse\nfrom lightgbm import plot_importance","65212e65":"DATA_PATH = '\/kaggle\/input\/tabular-playground-series-may-2021\/'\nsample = pd.read_csv(DATA_PATH + 'sample_submission.csv')\ntrain = pd.read_csv(DATA_PATH + 'train.csv')\n\n# Drop \"id\" as well since lightgbm used it as the most important feature\n# https:\/\/www.kaggle.com\/mstkmyhr\/2021-05-15-tps-baseline-submission-by-lightgbm\/edit\/run\/62983993\ntrain_x = train.drop(['id', 'target'], axis=1)\ntrain_y = train['target']\n# Convert target values to integer (e.g. Convert \"Class_1\" into 0)\ntrain_y = train_y.map(lambda x: int(x.split('_')[1]) - 1)\n\ntest_x = pd.read_csv(DATA_PATH + 'test.csv')\ntest_x = test_x.drop(['id'], axis=1)","a5e8ffec":"train.drop(['id'], axis=1).describe().T\\\n        .style.bar(subset=['mean'], color=px.colors.qualitative.G10[0])\\\n        .background_gradient(subset=['std'], cmap='Greens')\\\n        .background_gradient(subset=['50%'], cmap='BuGn')","3e9915d8":"# Applying t-SNE takes forever...\n# Y = tsne(train_x.astype(np.float64))\n# plt.scatter(Y[:, 0], Y[:, 1], c=train_y)\n# plt.show()\n\n# Instead, use UMAP\nreducer = umap.UMAP(\n    n_components=2,\n)","6b052e18":"%%time\nembedding = reducer.fit_transform(train_x)\nembedding.shape","1dbfc6af":"plt.scatter(\n    embedding[:, 0],\n    embedding[:, 1],\n    c=train_y,\n    s=.5)\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar(boundaries=np.arange(5)-0.5).set_ticks(np.arange(4))\nplt.title('UMAP projection of the dataset', fontsize=16)","98446a60":"reducer = umap.UMAP(\n    n_components=2\n)","0a43813e":"%%time\nembedding_test = reducer.fit_transform(test_x)\nembedding_test.shape","434c504f":"train_x['umap_x'] = embedding[:, 0]\ntrain_x['umap_y'] = embedding[:, 1]\ntest_x['umap_x'] = embedding_test[:, 0]\ntest_x['umap_y'] = embedding_test[:, 1]","7edbb01a":"params = {\n    'objective': 'multiclassova',\n    'verbose': -1,\n    'seed': 71,\n    'metrics': 'multi_logloss',\n    'num_class': 4\n}\nnum_round = 100\n\nscores = []\nkf = KFold(n_splits=4, shuffle=True, random_state=71)\nfor tr_idx, va_idx in kf.split(train_x):\n    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n    \n    lgb_train = lgb.Dataset(tr_x, tr_y)\n    lgb_eval = lgb.Dataset(va_x, va_y)\n    model = lgb.train(params, lgb_train, num_boost_round=num_round, valid_sets=[lgb_train, lgb_eval])\n    va_pred = model.predict(va_x)\n    score = log_loss(va_y, va_pred)\n    scores.append(score)","d46d790e":"print(f'logloss: {np.mean(scores):.4f}')","26ffef5d":"lgb_train = lgb.Dataset(train_x, train_y)\nmodel = lgb.train(params, lgb_train, num_boost_round=num_round)\npred = model.predict(test_x)\n\ndf_pred = pd.DataFrame(pred, columns=['Class_1', 'Class_2', 'Class_3', 'Class_4'])\ndf_pred['id'] = pd.read_csv(DATA_PATH + 'test.csv').iloc[:, 0]\nsubmission = df_pred[['id', 'Class_1', 'Class_2', 'Class_3', 'Class_4']]\nsubmission.to_csv('submission.csv', index=False)","8f75308e":"plot_importance(model, figsize=(8,16), importance_type='split')","5514884e":"plot_importance(model, figsize=(8,16), importance_type='gain')","476d02a5":"- \u3042\u307e\u308a\u826f\u3044\u7279\u5fb4\u91cf\u304c\u5f97\u3089\u308c\u305f\u3088\u3046\u306a\u6c17\u306f\u3057\u306a\u3044\u304c\u3001\u7279\u5fb4\u91cf\u306b\u8ffd\u52a0\u3057\u3066\u307f\u308b\u3002"}}