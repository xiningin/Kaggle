{"cell_type":{"5c833b1f":"code","bf87aa02":"code","619baddf":"code","beca76a2":"code","926bbb70":"code","6da55dd8":"code","9c9b9a63":"code","7ea07835":"code","82e4f618":"code","24df5330":"code","4198db7f":"code","5bc3b4c0":"code","3b6a3674":"code","53bfaea5":"code","defa699b":"code","2ab81671":"code","3829234c":"code","005ce0e9":"code","49d7dbde":"code","149d17bd":"code","46f3f3dc":"code","cc69b69f":"code","1db1a771":"code","6dc0f9ac":"code","ac4c573e":"code","0667bf37":"code","19617ece":"code","2a7c088c":"code","0342388a":"code","2d935801":"code","03180790":"code","c34fd5e8":"code","dacf8761":"code","679517aa":"code","26ef325c":"code","f33c2eae":"code","e6b1ed70":"code","30a597f8":"code","2322cdbe":"code","9aadc5a0":"code","d6f264f7":"code","a0982beb":"code","f3fb9f57":"code","f4d4c5bd":"code","b28675c0":"code","ca717369":"code","3ddb6ded":"code","1b5cdac5":"code","7fc49c8c":"code","fe36c21c":"code","cc2fb9e8":"code","9cce80af":"code","d298cdf7":"code","3497ffce":"code","2e22b63e":"code","6e302672":"code","4af663cb":"code","deb611dc":"code","75058416":"code","e1ee0c0e":"code","f88e8b38":"code","b310001f":"code","e9e93786":"code","a57ebef3":"code","813d3437":"code","0ffca144":"code","ca314dfc":"code","0f6bb387":"code","8f528227":"code","29026432":"code","b7c61e96":"code","63f9dd0b":"markdown","6785340e":"markdown","e52df802":"markdown","4860afd4":"markdown","51a5483f":"markdown","ff067aa6":"markdown","4901f108":"markdown","ed8f9062":"markdown","579ce32e":"markdown","e798fadd":"markdown","5d0cfa60":"markdown","2e6516d7":"markdown","ea583840":"markdown","b0e113c3":"markdown","c8b15e96":"markdown","8cae5975":"markdown","e7898243":"markdown","d85c854f":"markdown","962f9da2":"markdown","100828ab":"markdown","452ef0ef":"markdown","573b260c":"markdown","3a484fdc":"markdown","ae039e3b":"markdown","bd515932":"markdown","94134f43":"markdown","ecd79e37":"markdown","a45af483":"markdown","1399fc8c":"markdown","b757dc5d":"markdown","aeaca604":"markdown","1c7b4b5e":"markdown","1d6a20c2":"markdown","af9bfa24":"markdown","958b45d1":"markdown","90edab2e":"markdown","92b9bedf":"markdown","13cc7889":"markdown","6574724c":"markdown","9c993bbc":"markdown","590293f7":"markdown","3d32b440":"markdown","381c8a02":"markdown","1b6cebf6":"markdown","99f0b637":"markdown","162085b0":"markdown","ac85ab10":"markdown","ff961606":"markdown","18722e85":"markdown","e54003a6":"markdown","7beba9ac":"markdown","e90025a9":"markdown","89db6ce5":"markdown","5bf98ac8":"markdown","c05830d9":"markdown","87b8dbdc":"markdown","fa766049":"markdown","c89bd040":"markdown","d26f3183":"markdown","821eb6a4":"markdown","2e340162":"markdown","273547ca":"markdown","7f7735ce":"markdown","103bae18":"markdown","471ee485":"markdown","897ab634":"markdown","7f906a4d":"markdown","96d79640":"markdown","e22f0fb2":"markdown","2dcce947":"markdown","d91b86a3":"markdown","21dbe6b8":"markdown","4029c5d0":"markdown","01ca437b":"markdown","ae6a5318":"markdown","3deb496f":"markdown","9108ea7a":"markdown","d11fd1c7":"markdown","648860e5":"markdown","43968b36":"markdown","a80a15f7":"markdown","79bbfcb6":"markdown","abb5a204":"markdown","5734acc2":"markdown","3c55f785":"markdown","4545ab68":"markdown","17450144":"markdown","4a105b55":"markdown","f65dfa79":"markdown","b1b8ce9c":"markdown","bb830ed8":"markdown","0ab9fb2e":"markdown","8d23c8f0":"markdown","e47c9973":"markdown","50b6a4b5":"markdown","5d3dcbf2":"markdown","66e2c538":"markdown","08db6025":"markdown","d994b7be":"markdown","547761fb":"markdown","9d39fe61":"markdown","03322d98":"markdown","ca0df74c":"markdown","f5cc594f":"markdown","edb8457f":"markdown","8717d51f":"markdown","89806d38":"markdown","7d3fb47d":"markdown","fc3a6ecd":"markdown"},"source":{"5c833b1f":"import numpy as np # linear algebra\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom matplotlib import cm\nfrom math import log10\n\nwarnings.filterwarnings('ignore')","bf87aa02":"data = pd.read_csv('\/kaggle\/input\/kaggle-survey-2020\/kaggle_survey_2020_responses.csv', low_memory = False)\ndata.drop([0], inplace=True)","619baddf":"surveydf = data[(data.Q2=='Woman')| (data.Q2=='Nonbinary')]","beca76a2":"# 1. Age\nage = surveydf['Q1'].value_counts(normalize=True)*100\n\nstyle = dict(size=10, color='black')\nsns.set(rc={'figure.figsize':(15.7,6.27)})\nax = sns.barplot(x=age.index, y=age, palette=\"husl\")\nax.set(xlabel=\"Age Groups\", ylabel = \"% of Respondents\", title = 'Repondents of Varying Age Groups')\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone\n\nfrom IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n}\n<\/style>\n\"\"\")","926bbb70":"# Horizontal Barplot Y axis labels\ndef show_values_on_bars(axs, h_v=\"h\", space=0.4):\n# Code from https:\/\/stackoverflow.com\/questions\/43214978\/seaborn-barplot-displaying-values\n    def _show_on_single_plot(ax):\n        if h_v == \"v\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() \/ 2\n                _y = p.get_y() + p.get_height()\n                value = round(p.get_height(), 2)\n                ax.text(_x, _y, value, ha=\"center\", fontsize = 8) \n        elif h_v == \"h\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() + float(space)\n                _y = p.get_y() + p.get_height()\n                value = round(p.get_width(), 2)\n                ax.text(_x, _y, value, ha=\"left\", fontsize = 8)\n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)","6da55dd8":"#3. Country\nsurveydf.loc[surveydf['Q3']=='United Kingdom of Great Britain and Northern Ireland','Q3']='UK and Northern Ireland'\ncountry = surveydf['Q3'].value_counts(normalize=True)*100\n\nsns.set(rc={'figure.figsize':(15.7,15.27)})\nax = sns.barplot(y=country.index, x=country, palette=\"husl\")\nax.set(ylabel=\"Countries\", xlabel = \"% of Respondents\", title = \"Respondents' County of Residence\")\nshow_values_on_bars(ax)\nNone","9c9b9a63":"# 4. Qualifications\nqualification = surveydf['Q4'].value_counts(normalize=True)*100\nn = len(qualification)\ncolors = [cm.Set2(i \/ n) for i in range(n)]\n\npie, ax = plt.subplots(figsize=[13,10])\nlabels = qualification.index\nplt.pie(x=qualification, autopct=\"%.1f%%\", explode=[0.01]*qualification.shape[0], labels=labels, colors=colors, pctdistance=0.5, textprops={'fontsize': 9})\nplt.title(\"Qualification of the Repondents\", fontsize=14);","7ea07835":"# 5. Current Role\nrole = surveydf['Q5'].value_counts(normalize=True)*100\n\nsns.set(rc={'figure.figsize':(18.7,6.27)})\nax = sns.barplot(x=role.index, y=role, palette=\"husl\")\nax.set(xlabel=\"Current Role\", ylabel = \"% of Respondents\", title = \"Current\/Recent Job Titles\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=55)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","82e4f618":"# 6. Coding experience\nexp = surveydf['Q6'].value_counts(normalize=True)*100\nn = len(exp)\ncolors = [cm.Set2(i \/ n) for i in range(n)]\n\npie, ax = plt.subplots(figsize=[13,10])\nlabels = exp.index\nplt.pie(x=exp, autopct=\"%.1f%%\", explode=[0.02]*exp.shape[0], labels=labels, colors=colors, pctdistance=0.5, textprops={'fontsize': 10})\nplt.title(\"Years of Coding Experience\", fontsize=14);","24df5330":"# Function for questions with multiple answers\nimport operator\ndef multiple_ans(quest):\n    ans_dict = {}\n    i = 0\n    for col in surveydf.columns:\n        if col.startswith(quest):\n            try:\n                surveydf[col] = surveydf[col].str.strip()\n                key = surveydf[col].value_counts().index[0]\n                ans_dict[key] = surveydf[col].count()\n            except IndexError:\n                continue\n\n    # Get percentages of the values\n    for keys in ans_dict:\n        ans_dict[keys] = round(ans_dict[keys] * 100\/surveydf.shape[0], 2)\n\n    # Sort Dictionary values\n    sorted_items = {k: v for k, v in sorted(ans_dict.items(), key=lambda item: item[1], reverse=True)}\n    return sorted_items","4198db7f":"# Function for mutiple answers of alternate questions\ndef multiple_ans_alt(quest, df):\n    ans_dict = {}\n    i = 0\n    for col in df.columns:\n        if col.startswith(quest):\n            try:\n                df[col] = df[col].str.strip()\n                key = df[col].value_counts().index[0]\n                ans_dict[key] = df[col].count()\n            except IndexError:\n                continue\n\n    # Get percentages of the values\n    for keys in ans_dict:\n        ans_dict[keys] = round(ans_dict[keys] * 100\/df.shape[0], 2)\n\n    # Sort Dictionary values\n    sorted_items = {k: v for k, v in sorted(ans_dict.items(), key=lambda item: item[1], reverse=True)}\n    return sorted_items","5bc3b4c0":"# 7. Regular programming language\nvalues = multiple_ans('Q7')\n\nsns.set(rc={'figure.figsize':(14.7,6.27)})\nax = sns.barplot(x=list(values.keys()), y=list(values.values()), palette=\"husl\")\nax.set(xlabel=\"Programming Language\", ylabel = \"% of Respondents\", title = \"Regular Programming Language\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=55)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","3b6a3674":"# 7.1 Respondents who used Python\npython_users = surveydf[surveydf['Q7_Part_1'].notna()]\nlanguage_py = {}\ni = 0\nfor col in python_users.columns:\n    if col.startswith('Q7'):\n        try:\n            key = python_users[col].value_counts().index[0]\n            language_py[key] = python_users[col].count()\n        except IndexError:\n            continue","53bfaea5":"# Get percentages of the values\nfor keys in language_py:\n    language_py[keys] = round(language_py[keys] * 100\/python_users.shape[0], 2)\n\n# Sort Dictionary values\nvalues = {k: v for k, v in sorted(language_py.items(), key=lambda item: item[1], reverse = True)}\ndel values['Python']\n\nsns.set(rc={'figure.figsize':(14.7,6.27)})\nax = sns.barplot(x=list(values.keys()), y=list(values.values()), palette=\"husl\")\nax.set(xlabel=\"Programming Language\", ylabel = \"% of Respondents\", title = \"Programming Language Used With Python\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=55)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","defa699b":"# 7.2 Respondents who didn't use Python\nnon_python_users = surveydf[surveydf['Q7_Part_1'].isna()]\nlanguage_non_py = {}\ni = 0\nfor col in non_python_users.columns:\n    if col.startswith('Q7'):\n        try:\n            key = non_python_users[col].value_counts().index[0]\n            language_non_py[key] = non_python_users[col].count()\n        except IndexError:\n            continue","2ab81671":"# Get percentages of the values\nfor keys in language_non_py:\n    language_non_py[keys] = round(language_non_py[keys] * 100\/non_python_users.shape[0], 2)\n\n# Sort Dictionary values\nvalues = {k: v for k, v in sorted(language_non_py.items(), key=lambda item: item[1], reverse = True)}\n\n\nsns.set(rc={'figure.figsize':(14.7,6.27)})\nax = sns.barplot(x=list(values.keys()), y=list(values.values()), palette=\"husl\")\nax.set(xlabel=\"Programming Language\", ylabel = \"% of Respondents\", title = \"Years of Writing Code\/Programming\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=55)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","3829234c":"# 8. Language recommendation\nlang = surveydf['Q8'].value_counts(normalize=True)*100\n\nsns.set(rc={'figure.figsize':(15.7,6.27)})\nax = sns.barplot(x=lang.index, y=lang, palette=\"husl\")\nax.set(ylabel=\"% of Respondents\", xlabel = \"% of Respondents\", title = \"Most Recommended Language\")\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","005ce0e9":"# 9. Regular IDE \nide = multiple_ans('Q9')\nide['Jupyter'] = ide.pop('Jupyter (JupyterLab, Jupyter Notebooks, etc)')\nide['Visual Studio Code'] = ide.pop('Visual Studio Code (VSCode)')\nide = {k: v for k, v in sorted(ide.items(), key=lambda item: item[1], reverse=True)}\n\nsns.set(rc={'figure.figsize':(14.7,6.27)})\nax = sns.barplot(x=list(ide.keys()), y=list(ide.values()), palette=\"husl\")\nax.set(xlabel=\"Programming Language\", ylabel = \"% of Respondents\", title = \"Integrated Development Environments (IDE) Used Regularly\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=55)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","49d7dbde":"# 10. Hosted Notebook Products\nnb = multiple_ans('Q10')\n\nsns.set(rc={'figure.figsize':(23.7,6.27)})\nax = sns.barplot(x=list(nb.keys()), y=list(nb.values()), palette=\"husl\")\nax.set(xlabel=\"IDEs\", ylabel = \"% of Respondents\", title = \"Integrated Development Environments (IDE) Used Regularly\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=65)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\n\nNone","149d17bd":"# 11. Platform preference\nplatform = surveydf['Q11'].value_counts(normalize=True)*100\nn = len(platform)\ncolors = [cm.Set2(i \/ n) for i in range(n)]\n\npie, ax = plt.subplots(figsize=[13,10])\nlabels = surveydf['Q11'].value_counts().index\nplt.pie(x=surveydf['Q11'].value_counts(normalize=True)*100, autopct=\"%.1f%%\", explode=[0.02]*platform.shape[0], labels=labels, colors=colors, pctdistance=0.7, textprops={'fontsize': 11})\nplt.title(\"Platform Preference\", fontsize=14);\nNone","46f3f3dc":"# 12. Specialised Hardware\nhw = multiple_ans('Q12')\n\nsns.set(rc={'figure.figsize':(15.7,6.27)})\nax = sns.barplot(x=list(hw.keys()), y=list(hw.values()), palette=\"husl\")\nax.set(xlabel=\"Hardware\", ylabel = \"% of Respondents\", title = \"Specialized Hardware Used Regularly\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=65)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\n\nNone","cc69b69f":"# 13. TPU frequency\ntpu = surveydf['Q13'].value_counts(normalize=True)*100\nn = len(tpu)\ncolors = [cm.Set2(i \/ n) for i in range(n)]\n\npie, ax = plt.subplots(figsize=[13,10])\nlabels = tpu.index\nplt.pie(x=tpu, autopct=\"%.1f%%\", explode=[0.02]*tpu.shape[0], labels=labels, colors = colors, pctdistance=0.7, textprops={'fontsize': 10})\nplt.title(\"Number of Times TPU (Tensor Processing Unit) Was Used\", fontsize=14);","1db1a771":"# 14. Visualization Libraries\nlib = multiple_ans('Q14')\n\nsns.set(rc={'figure.figsize':(15.7,6.27)})\nax = sns.barplot(x=list(lib.keys()), y=list(lib.values()), palette=\"husl\")\nax.set(xlabel=\"IDEs\", ylabel = \"% of Respondents\", title = \"Visualization Libraries Used Regularly\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=65)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\n\nNone","6dc0f9ac":"# 15. Experience in ML\nml = surveydf['Q15'].value_counts(normalize=True)*100\nn = len(ml)\ncolors = [cm.tab20(i \/ n) for i in range(n)]\n\npie, ax = plt.subplots(figsize=[13,10])\nlabels = ml.index\nplt.pie(x=ml, autopct=\"%.1f%%\", explode=[0.01]*ml.shape[0], labels=labels, colors = colors, pctdistance=0.7, textprops={'fontsize': 9})\nplt.title(\"Years Spent Using Machine Learning\", fontsize=14);","ac4c573e":"# 16. ML frameworks\nfw = multiple_ans('Q16')\n\nsns.set(rc={'figure.figsize':(15.7,6.27)})\nax = sns.barplot(x=list(fw.keys()), y=list(fw.values()), palette=\"husl\")\nax.set(xlabel=\"ML Framework\", ylabel = \"% of Respondents\", title = \"Machine Learning Framework Used Regularly\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=65)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\n\nNone","0667bf37":"# 17. ML algorithm\nalgo = multiple_ans('Q17')\nalgo['Gradient Boosting Machines'] = algo.pop('Gradient Boosting Machines (xgboost, lightgbm, etc)')\nalgo['Transformer Networks'] = algo.pop('Transformer Networks (BERT, gpt-3, etc)')\nalgo['Dense Neural Networks'] = algo.pop('Dense Neural Networks (MLPs, etc)')\nalgo = {k: v for k, v in sorted(algo.items(), key=lambda item: item[1], reverse=True)}\n\nsns.set(rc={'figure.figsize':(15.7,6.27)})\nax = sns.barplot(x=list(algo.keys()), y=list(algo.values()), palette=\"husl\")\nax.set(xlabel=\"ML Algorithms\", ylabel = \"% of Respondents\", title = \"Machine Learning Algorithms Used Regularly\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=65)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","19617ece":"# Alternate Question based on answers in Q17\ncomputer_vison_df = surveydf[['Q18_Part_1', 'Q18_Part_2', 'Q18_Part_3', 'Q18_Part_4', 'Q18_Part_5', 'Q18_Part_6', 'Q18_OTHER']]\ncomputer_vison_df = computer_vison_df.dropna(how='all')","2a7c088c":"# 18. Computer vision\ncn = multiple_ans_alt('Q18',computer_vison_df)\ncn['Image classification, general purpose'] = cn.pop('Image classification and other general purpose networks (VGG, Inception, ResNet, ResNeXt, NASNet, EfficientNet, etc)')\ncn['Image segmentation methods'] = cn.pop('Image segmentation methods (U-Net, Mask R-CNN, etc)')\ncn['General purpose image\/video tools'] = cn.pop('General purpose image\/video tools (PIL, cv2, skimage, etc)')\ncn['Object detection methods'] = cn.pop('Object detection methods (YOLOv3, RetinaNet, etc)')\ncn['Generative Networks'] = cn.pop('Generative Networks (GAN, VAE, etc)')\n\ncn = {k: v for k, v in sorted(cn.items(), key=lambda item: item[1], reverse=True)}\n\nsns.set(rc={'figure.figsize':(15.7,6.27)})\nax = sns.barplot(x=list(cn.keys()), y=list(cn.values()), palette=\"husl\")\nax.set(xlabel=\"Computer Vison Methods\", ylabel = \"% of Respondents\", title = \"Computer Vision Methods Used Regularly\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=65)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","0342388a":"# Alternate Question based on answers in Q17\nnlp_df = surveydf[['Q19_Part_1', 'Q19_Part_2', 'Q19_Part_3', 'Q19_Part_4', 'Q19_Part_5', 'Q19_OTHER']]\nnlp_df = nlp_df.dropna(how='all')","2d935801":"# 19. NLP\nnlp =  multiple_ans_alt('Q19',nlp_df)\nnlp['Word embeddings\/vectors'] = nlp.pop('Word embeddings\/vectors (GLoVe, fastText, word2vec)')\nnlp['Encoder-decorder models'] = nlp.pop('Encoder-decorder models (seq2seq, vanilla transformers)')\nnlp['Transformer language models'] = nlp.pop('Transformer language models (GPT-3, BERT, XLnet, etc)')\nnlp['Contextualized embeddings'] = nlp.pop('Contextualized embeddings (ELMo, CoVe)')\n\nnlp = {k: v for k, v in sorted(nlp.items(), key=lambda item: item[1], reverse=True)}\n\nsns.set(rc={'figure.figsize':(15.7,6.27)})\nax = sns.barplot(x=list(nlp.keys()), y=list(nlp.values()), palette=\"husl\")\nax.set(xlabel=\"NLP Methods\", ylabel = \"% of Respondents\", title = \"Natural Language Processing Methods Used Regularly\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=65)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","03180790":"# 20. Company size\nsize = surveydf['Q20'].value_counts(normalize=True)*100\nn = len(size)\ncolors = [cm.Set2(i \/ n) for i in range(n)]\n\npie, ax = plt.subplots(figsize=[13,10])\nlabels = size.index\nplt.pie(x=size, autopct=\"%.1f%%\", explode=[0.02]*size.shape[0], labels=labels, colors = colors, pctdistance=0.7, textprops={'fontsize': 10})\nplt.title(\"Size of Company Respondents Employed at\", fontsize=14);","c34fd5e8":"# 21. Data Science employees\nemp = surveydf['Q21'].value_counts(normalize=True)*100\nn = len(emp)\ncolors = [cm.Set2(i \/ n) for i in range(n)]\n\n#Using matplotlib\npie, ax = plt.subplots(figsize=[13,10])\nlabels = emp.index\nplt.pie(x=emp, autopct=\"%.1f%%\", explode=[0.02]*emp.shape[0], labels=labels, colors = colors, pctdistance=0.7, textprops={'fontsize': 10})\nplt.title(\"Number of Individuals Responsible for Data Science at Workplace\", fontsize=14);","dacf8761":"# 22. ML methods at work\nmethods = surveydf['Q22'].value_counts(normalize=True)*100\nn = len(methods)\ncolors = [cm.Set2(i \/ n) for i in range(n)]\n\n#Using matplotlib\npie, ax = plt.subplots(figsize=[13,10])\nlabels = methods.index\nplt.pie(x=methods, autopct=\"%.1f%%\", explode=[0.02]*methods.shape[0], colors = colors, labels=labels, pctdistance=0.7, textprops={'fontsize': 10})\nplt.title(\"Extent of Machine Learning Methods Used at Work\", fontsize=14);","679517aa":"# 23. Job role duties\nrole = multiple_ans('Q23')\n\nsns.set(rc={'figure.figsize':(20.7,9.27)})\nax = sns.barplot(y=list(role.keys()), x=list(role.values()), palette=\"husl\")\nax.set(ylabel=\"Job Role Duties\", xlabel = \"% of Respondents\")\nax.set_title(\"Natural Language Processing Methods Used Regularly\",fontsize=15)\nax.set_yticklabels(ax.get_yticklabels())\nshow_values_on_bars(ax, \"h\", 0.3)\nNone","26ef325c":"# 24. Salary\n# surveydf['Q24'].value_counts(normalize=True)*100\nsalary_dict = {\n    '$0-999': 28.596594,\n    '1,000-1,999': 7.105109,\n    '2,000-2,999': 3.875514,\n    '3,000-3,999': 2.701116,\n    '4,000-4,999': 2.701116,\n    '5,000-7,499': 4.932472,\n    '7,500-9,999': 3.053435,\n    '10,000-14,999': 5.226072,\n    '15,000-19,999': 4.051674,\n    '20,000-24,999': 3.523194,\n    '25,000-29,999': 2.055197,\n    '30,000-39,999': 4.286553,\n    '40,000-49,999': 4.286553,\n    '50,000-59,999': 3.405755,\n    '60,000-69,999': 2.877275,\n    '70,000-79,999': 2.348796,\n    '80,000-89,999': 2.172637,\n    '90,000-99,999': 2.172637,\n    '100,000-124,999': 4.521433,\n    '125,000-149,999': 2.290076,\n    '150,000-199,999': 2.290076,\n    '200,000-249,999': 0.645919,\n    '250,000-299,999': 0.293600,\n    '300,000-500,000': 0.293600,\n    '> $500,000': 0.293600\n}","f33c2eae":"sns.set(rc={'figure.figsize':(20.7,8.27)})\nmy_range=range(1,len(salary_dict)+1)\n\nplt.hlines(y=my_range,  xmin=0, xmax=list(salary_dict.values()), color='red')\nplt.plot(list(salary_dict.values()), my_range, \"o\")\n\nplt.yticks(my_range, list(salary_dict.keys()))\nplt.title(\"Level of Compensation ($USD)\", loc='center', fontsize=14)\nplt.xlabel('% of Respondents')\nplt.ylabel('Compensation')\nNone","e6b1ed70":"# Low salary Countries in dataframe\nlow_sal_country = surveydf[surveydf['Q24']=='$0-999']['Q3'].unique()\nlow_sal_country_df = pd.DataFrame(low_sal_country, columns=['Country'])","30a597f8":"# Get country codes\nimport pycountry \ndef alpha3code(column):\n    CODE=[]\n    for country in column:\n        try:\n            code=pycountry.countries.get(name=country).alpha_3\n           # .alpha_3 means 3-letter country code \n           # .alpha_2 means 2-letter country code\n            CODE.append(code)\n        except:\n            CODE.append('None')\n    return CODE\n# create a column for code \nlow_sal_country_df['CODE']=alpha3code(low_sal_country_df.Country)","2322cdbe":"import plotly.express as px\nfig = px.choropleth(data_frame = low_sal_country_df,\n                    locations= \"CODE\",\n                    color= \"Country\",  \n                    hover_name= \"Country\",\n                    title = 'Countries of Unemployed\/Low Paid Respondents')  \n\nfig.show()","9aadc5a0":"# 25. Money spent on tools\nmoney = surveydf['Q25'].value_counts(normalize=True)*100\nn = len(money)\ncolors = [cm.Set2(i \/ n) for i in range(n)]\n\npie, ax = plt.subplots(figsize=[13,10])\nlabels = money.index\nplt.pie(x=money, autopct=\"%.1f%%\", explode=[0.02]*money.shape[0], labels=labels, colors=colors, pctdistance=0.7, textprops={'fontsize': 10})\nplt.title(\"Money (USD) Spent on Machine Learning and\/or Cloud Computing Services in the Past 5 Years\", fontsize=14);","d6f264f7":"# Function for questions with multiple answers with Part A and Part B\ndef multiple_ans_ab(quest, df):\n    ans_dict = {}\n    i = 0\n    for col in df.columns:\n        if col.startswith(quest):\n            try:\n                df[col] = df[col].str.strip()\n                key = df[col].value_counts().index[0]\n                ans_dict[key] = df[col].count()\n            except IndexError:\n                continue\n\n    # Get percentages of the values\n    for keys in ans_dict:\n        ans_dict[keys] = round(ans_dict[keys] * 100\/df.shape[0], 2)\n\n    # Sort Dictionary values\n    sorted_items = {k: v for k, v in sorted(ans_dict.items(), key=lambda item: item[1], reverse = True)}\n    return sorted_items","a0982beb":"# Filter data for non-spenders and spenders\nspender1 = surveydf[surveydf['Q25']!='$0 ($USD)']\nnonspender1 = surveydf[surveydf['Q25']=='$0 ($USD)']","f3fb9f57":"spender = spender1[['Q25', 'Q26_A_Part_1',\n 'Q26_A_Part_2',\n 'Q26_A_Part_3',\n 'Q26_A_Part_4',\n 'Q26_A_Part_5',\n 'Q26_A_Part_6',\n 'Q26_A_Part_7',\n 'Q26_A_Part_8',\n 'Q26_A_Part_9',\n 'Q26_A_Part_10',\n 'Q26_A_Part_11',\n 'Q26_A_OTHER',\n 'Q27_A_Part_1',\n 'Q27_A_Part_2',\n 'Q27_A_Part_3',\n 'Q27_A_Part_4',\n 'Q27_A_Part_5',\n 'Q27_A_Part_6',\n 'Q27_A_Part_7',\n 'Q27_A_Part_8',\n 'Q27_A_Part_9',\n 'Q27_A_Part_10',\n 'Q27_A_Part_11',\n 'Q27_A_OTHER',\n 'Q28_A_Part_1',\n 'Q28_A_Part_2',\n 'Q28_A_Part_3',\n 'Q28_A_Part_4',\n 'Q28_A_Part_5',\n 'Q28_A_Part_6',\n 'Q28_A_Part_7',\n 'Q28_A_Part_8',\n 'Q28_A_Part_9',\n 'Q28_A_Part_10',\n 'Q28_A_OTHER',\n 'Q29_A_Part_1',\n 'Q29_A_Part_2',\n 'Q29_A_Part_3',\n 'Q29_A_Part_4',\n 'Q29_A_Part_5',\n 'Q29_A_Part_6',\n 'Q29_A_Part_7',\n 'Q29_A_Part_8',\n 'Q29_A_Part_9',\n 'Q29_A_Part_10',\n 'Q29_A_Part_11',\n 'Q29_A_Part_12',\n 'Q29_A_Part_13',\n 'Q29_A_Part_14',\n 'Q29_A_Part_15',\n 'Q29_A_Part_16',\n 'Q29_A_Part_17',\n 'Q29_A_OTHER',\n 'Q30',\n 'Q31_A_Part_1',\n 'Q31_A_Part_2',\n 'Q31_A_Part_3',\n 'Q31_A_Part_4',\n 'Q31_A_Part_5',\n 'Q31_A_Part_6',\n 'Q31_A_Part_7',\n 'Q31_A_Part_8',\n 'Q31_A_Part_9',\n 'Q31_A_Part_10',\n 'Q31_A_Part_11',\n 'Q31_A_Part_12',\n 'Q31_A_Part_13',\n 'Q31_A_Part_14',\n 'Q31_A_OTHER',\n 'Q32',\n 'Q33_A_Part_1',\n 'Q33_A_Part_2',\n 'Q33_A_Part_3',\n 'Q33_A_Part_4',\n 'Q33_A_Part_5',\n 'Q33_A_Part_6',\n 'Q33_A_Part_7',\n 'Q33_A_OTHER',\n 'Q34_A_Part_1',\n 'Q34_A_Part_2',\n 'Q34_A_Part_3',\n 'Q34_A_Part_4',\n 'Q34_A_Part_5',\n 'Q34_A_Part_6',\n 'Q34_A_Part_7',\n 'Q34_A_Part_8',\n 'Q34_A_Part_9',\n 'Q34_A_Part_10',\n 'Q34_A_Part_11',\n 'Q34_A_OTHER',\n 'Q35_A_Part_1',\n 'Q35_A_Part_2',\n 'Q35_A_Part_3',\n 'Q35_A_Part_4',\n 'Q35_A_Part_5',\n 'Q35_A_Part_6',\n 'Q35_A_Part_7',\n 'Q35_A_Part_8',\n 'Q35_A_Part_9',\n 'Q35_A_Part_10',\n 'Q35_A_OTHER']]","f4d4c5bd":"nonspender = nonspender1[['Q25', 'Q26_B_Part_1',\n 'Q26_B_Part_2',\n 'Q26_B_Part_3',\n 'Q26_B_Part_4',\n 'Q26_B_Part_5',\n 'Q26_B_Part_6',\n 'Q26_B_Part_7',\n 'Q26_B_Part_8',\n 'Q26_B_Part_9',\n 'Q26_B_Part_10',\n 'Q26_B_Part_11',\n 'Q26_B_OTHER',\n 'Q27_B_Part_1',\n 'Q27_B_Part_2',\n 'Q27_B_Part_3',\n 'Q27_B_Part_4',\n 'Q27_B_Part_5',\n 'Q27_B_Part_6',\n 'Q27_B_Part_7',\n 'Q27_B_Part_8',\n 'Q27_B_Part_9',\n 'Q27_B_Part_10',\n 'Q27_B_Part_11',\n 'Q27_B_OTHER',\n 'Q28_B_Part_1',\n 'Q28_B_Part_2',\n 'Q28_B_Part_3',\n 'Q28_B_Part_4',\n 'Q28_B_Part_5',\n 'Q28_B_Part_6',\n 'Q28_B_Part_7',\n 'Q28_B_Part_8',\n 'Q28_B_Part_9',\n 'Q28_B_Part_10',\n 'Q28_B_OTHER',\n 'Q29_B_Part_1',\n 'Q29_B_Part_2',\n 'Q29_B_Part_3',\n 'Q29_B_Part_4',\n 'Q29_B_Part_5',\n 'Q29_B_Part_6',\n 'Q29_B_Part_7',\n 'Q29_B_Part_8',\n 'Q29_B_Part_9',\n 'Q29_B_Part_10',\n 'Q29_B_Part_11',\n 'Q29_B_Part_12',\n 'Q29_B_Part_13',\n 'Q29_B_Part_14',\n 'Q29_B_Part_15',\n 'Q29_B_Part_16',\n 'Q29_B_Part_17',\n 'Q29_B_OTHER',\n 'Q31_B_Part_1',\n 'Q31_B_Part_2',\n 'Q31_B_Part_3',\n 'Q31_B_Part_4',\n 'Q31_B_Part_5',\n 'Q31_B_Part_6',\n 'Q31_B_Part_7',\n 'Q31_B_Part_8',\n 'Q31_B_Part_9',\n 'Q31_B_Part_10',\n 'Q31_B_Part_11',\n 'Q31_B_Part_12',\n 'Q31_B_Part_13',\n 'Q31_B_Part_14',\n 'Q31_B_OTHER',\n 'Q33_B_Part_1',\n 'Q33_B_Part_2',\n 'Q33_B_Part_3',\n 'Q33_B_Part_4',\n 'Q33_B_Part_5',\n 'Q33_B_Part_6',\n 'Q33_B_Part_7',\n 'Q33_B_OTHER',\n 'Q34_B_Part_1',\n 'Q34_B_Part_2',\n 'Q34_B_Part_3',\n 'Q34_B_Part_4',\n 'Q34_B_Part_5',\n 'Q34_B_Part_6',\n 'Q34_B_Part_7',\n 'Q34_B_Part_8',\n 'Q34_B_Part_9',\n 'Q34_B_Part_10',\n 'Q34_B_Part_11',\n 'Q34_B_OTHER',\n 'Q35_B_Part_1',\n 'Q35_B_Part_2',\n 'Q35_B_Part_3',\n 'Q35_B_Part_4',\n 'Q35_B_Part_5',\n 'Q35_B_Part_6',\n 'Q35_B_Part_7',\n 'Q35_B_Part_8',\n 'Q35_B_Part_9',\n 'Q35_B_Part_10',\n 'Q35_B_OTHER']]","b28675c0":"# Remove unused rows\nnonspender = nonspender.dropna(how='all')\nspender = spender.dropna(how='all')","ca717369":"# 26_A. Cloud Computing Platforms\ncc1 = multiple_ans_ab('Q26_A',spender)\n\nsns.set(rc={'figure.figsize':(15.7,6.27)})\nax = sns.barplot(x=list(cc1.keys()), y=list(cc1.values()), palette=\"husl\")\nax.set(xlabel=\"Cloud Computing Platforms\", ylabel = \"% of Respondents\", title = \"Cloud Computing Platforms Used Regularly\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=65)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","3ddb6ded":"# Alternate Question based on answers in Q26A\ncc_prd_df = spender[['Q27_A_Part_1','Q27_A_Part_2','Q27_A_Part_3','Q27_A_Part_4','Q27_A_Part_5','Q27_A_Part_6','Q27_A_Part_7','Q27_A_Part_8','Q27_A_Part_9','Q27_A_Part_10','Q27_A_Part_11','Q27_A_OTHER']]\ncc_prd_df = cc_prd_df.dropna(how='all')","1b5cdac5":"# 27_A. Could Computing Products\ncc2 = multiple_ans_ab('Q27_A',cc_prd_df)\n\nsns.set(rc={'figure.figsize':(15.7,6.27)})\nax = sns.barplot(x=list(cc2.keys()), y=list(cc2.values()), palette=\"husl\")\nax.set(xlabel=\"Cloud Computing Products\", ylabel = \"% of Respondents\", title = \"Cloud Computing Products Used Regularly\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=65)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","7fc49c8c":"# Alternate Question based on answers in Q26A\nml_prd_df = spender[['Q28_A_Part_1','Q28_A_Part_2','Q28_A_Part_3','Q28_A_Part_4','Q28_A_Part_5','Q28_A_Part_6','Q28_A_Part_7','Q28_A_Part_8','Q28_A_Part_9','Q28_A_Part_10','Q27_A_OTHER']]\nml_prd_df = ml_prd_df.dropna(how='all')","fe36c21c":"# 28_A. Machine Learning Products\nmlp= multiple_ans_ab('Q28_A',ml_prd_df)\nmlp['GC AI Platform\/ML Engine'] = mlp.pop('Google Cloud AI Platform \/ Google Cloud ML Engine')\nmlp = {k: v for k, v in sorted(mlp.items(), key=lambda item: item[1], reverse=True)}\n\nsns.set(rc={'figure.figsize':(15.7,6.27)})\nax = sns.barplot(x=list(mlp.keys()), y=list(mlp.values()), palette=\"husl\")\nax.set(xlabel=\"Machine Learning Products\", ylabel = \"% of Respondents\", title = \"Machine Learning Products Used Regularly\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=65)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","cc2fb9e8":"# 29_A. Big Data Products\nbdp = multiple_ans_ab('Q29_A',spender)\nbdp['Microsoft Azure Data Lake'] = bdp.pop('Microsoft Azure Data Lake Storage')\nbdp = {k: v for k, v in sorted(bdp.items(), key=lambda item: item[1], reverse=True)}\n\nsns.set(rc={'figure.figsize':(20.7,6.27)})\nax = sns.barplot(x=list(bdp.keys()), y=list(bdp.values()), palette=\"husl\")\nax.set(xlabel=\"Big Data Products\", ylabel = \"% of Respondents\", title = \"Big Data Products Used Regularly\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=65)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","9cce80af":"# 30. Big Data Products used most often\nbdp2 = spender['Q30'].value_counts(normalize=True)*100\nn = len(bdp2)\ncolors = [cm.tab20(i \/ n) for i in range(n)]\n\npie, ax = plt.subplots(figsize=[13,10])\nlabels = bdp2.index\nplt.pie(x=bdp2, autopct=\"%.1f%%\", explode=[0.02]*bdp2.shape[0], labels=labels, colors=colors, pctdistance=0.7, textprops={'fontsize': 10})\nplt.title(\"Big Data Products Used Most Often\", fontsize=14);","d298cdf7":"# 31_A. Business Intelligence Tools\nbit = multiple_ans_ab('Q31_A',spender)\n\nsns.set(rc={'figure.figsize':(15.7,6.27)})\nax = sns.barplot(x=list(bit.keys()), y=list(bit.values()), palette=\"husl\")\nax.set(xlabel=\"Business Intelligence Tools\", ylabel = \"% of Respondents\", title = \"Business Intelligence Tools Used Regularly\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=65)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","3497ffce":"# 32. Big Intelligence Products used most often\nbdp2 = spender['Q32'].value_counts(normalize=True)*100\nn = len(bdp2)\ncolors = [cm.tab20(i \/ n) for i in range(n)]\n\n#Using matplotlib\npie, ax = plt.subplots(figsize=[15,13])\nlabels = bdp2.index\nplt.pie(x=bdp2, autopct=\"%.1f%%\", explode=[0.02]*bdp2.shape[0], labels=labels, colors=colors, pctdistance=0.7, textprops={'fontsize': 10})\nplt.title(\"Business Intelligence Tools Used Most Often\", fontsize=14);","2e22b63e":"# 33_A. Automated Machine Learning Tools\nauto_ml = multiple_ans_ab('Q33_A',spender)\nsns.set(rc={'figure.figsize':(20.7,8.27)})\nax = sns.barplot(y=list(auto_ml.keys()), x=list(auto_ml.values()), palette=\"husl\")\nax.set(ylabel=\"Automated Machine Learning Tools\", xlabel = \"% of Respondents\")\nax.set_title(\"Automated Machine Learning Tools Used Regularly\",fontsize=14)\nax.set_yticklabels(ax.get_yticklabels())\nshow_values_on_bars(ax, \"h\", 0.3)\nNone","6e302672":"auto_ml_df = spender[spender['Q33_A_Part_7']!='No \/ None']\nauto_ml_df.shape","4af663cb":"# 34_A. Automated Machine Learning Tools on regular basis\nauto_ml2 = multiple_ans_ab('Q34_A',auto_ml_df)\n\nsns.set(rc={'figure.figsize':(15.7,6.27)})\nax = sns.barplot(x=list(auto_ml2.keys()), y=list(auto_ml2.values()), palette=\"husl\")\nax.set(xlabel=\"Automated Machine Learning Tools\", ylabel = \"% of Respondents\", title = \"Automated Machine Learning Tools Used Regularly\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=65)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","deb611dc":"# 35_A. Tools to help manage machine learning experiments\nml_tool = multiple_ans_ab('Q35_A',spender)\n\nsns.set(rc={'figure.figsize':(15.7,6.27)})\nax = sns.barplot(x=list(ml_tool.keys()), y=list(ml_tool.values()), palette=\"husl\")\nax.set(xlabel=\"ML Managing Tools\", ylabel = \"% of Respondents\", title = \"Tools to Manage Machine Learning Experiments\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=65)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","75058416":"# 26_B. Cloud computing platforms you hope to become more familiar with in the next 2 years\ncc_ptfm = multiple_ans_ab('Q26_B',nonspender)\n\nsns.set(rc={'figure.figsize':(15.7,6.27)})\nax = sns.barplot(x=list(cc_ptfm.keys()), y=list(cc_ptfm.values()), palette=\"husl\")\nax.set(xlabel=\"Cloud Computing Platforms\", ylabel = \"% of Respondents\", title = \"Desired Cloud Computing Platforms to Become Familiar With\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=65)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","e1ee0c0e":"# 27_B. Cloud computing products you hope to become more familiar with in the next 2 years\ncc_prd = multiple_ans_ab('Q27_B',nonspender)\n\nsns.set(rc={'figure.figsize':(18.7,6.27)})\nax = sns.barplot(x=list(cc_prd.keys()), y=list(cc_prd.values()), palette=\"husl\")\nax.set(xlabel=\"Cloud Computing Products\", ylabel = \"% of Respondents\", title = \"Desired Cloud Computing Products to Become Familiar With\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=65)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","f88e8b38":"# 28_B. Machine Learning products you hope to become more familiar with in the next 2 years\nml_prd = multiple_ans_ab('Q28_B',nonspender)\nml_prd['GC AI Platform\/ ML Engine'] = ml_prd.pop('Google Cloud AI Platform \/ Google Cloud ML Engine')\nml_prd = {k: v for k, v in sorted(ml_prd.items(), key=lambda item: item[1], reverse=True)}\n\nsns.set(rc={'figure.figsize':(18.7,6.27)})\nax = sns.barplot(x=list(ml_prd.keys()), y=list(ml_prd.values()), palette=\"husl\")\nax.set(xlabel=\"Machine Learning Products\", ylabel = \"% of Respondents\", title = \"Machine Learning Products to Become Familiar With\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=65)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","b310001f":"# 29_B. Big Data products you hope to become more familiar with in the next 2 years\nbd_prd = multiple_ans_ab('Q29_B',nonspender)\nbd_prd['Microsoft Azure Data Lake'] = bd_prd.pop('Microsoft Azure Data Lake Storage')\nbd_prd = {k: v for k, v in sorted(bd_prd.items(), key=lambda item: item[1], reverse=True)}\n\nsns.set(rc={'figure.figsize':(20.7,6.27)})\nax = sns.barplot(x=list(bd_prd.keys()), y=list(bd_prd.values()), palette=\"husl\")\nax.set(xlabel=\"Big Data Products\", ylabel = \"% of Respondents\", title = \"Big Data Products to Become Familiar With\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=65)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","e9e93786":"# 31_B. Business Intelligence products you hope to become more familiar with in the next 2 years\nbi_prd = multiple_ans_ab('Q31_B',nonspender)\n\nsns.set(rc={'figure.figsize':(15.7,6.27)})\nax = sns.barplot(x=list(bi_prd.keys()), y=list(bi_prd.values()), palette=\"husl\")\nax.set(xlabel=\"Big Intelligence Products\", ylabel = \"% of Respondents\", title = \"Big Intelligence Products to Become Familiar With\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=65)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","a57ebef3":"# 33_B. Categories of automated machine learning tools you hope to become more familiar with in the next 2 years\nauto_ml_prd = multiple_ans_ab('Q33_B',nonspender)\nsns.set(rc={'figure.figsize':(20.7,8.27)})\nax = sns.barplot(y=list(auto_ml_prd.keys()), x=list(auto_ml_prd.values()), palette=\"husl\")\nax.set(ylabel=\"Automated Machine Learning Tools\", xlabel = \"% of Respondents\")\nax.set_title(\"Categories of Automated Machine Learning Tools To Become Familiar With\",fontsize=14)\nax.set_yticklabels(ax.get_yticklabels())\nshow_values_on_bars(ax, \"h\", 0.3)\nNone","813d3437":"# 34_B. Specific automated machine learning tools you hope to become more familiar with in the next 2 years\nauto_ml_prd = multiple_ans_ab('Q34_B',nonspender)\n\nsns.set(rc={'figure.figsize':(15.7,6.27)})\nax = sns.barplot(x=list(auto_ml_prd.keys()), y=list(auto_ml_prd.values()), palette=\"husl\")\nax.set(xlabel=\"Automated Machine Learning Tools\", ylabel = \"% of Respondents\", title = \"Specific Automated Machine Learning Tools to Become Familiar With\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=65)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","0ffca144":"# 35B. Tools for managing ML experiments  you hope to become more familiar with in the next 2 years\nauto_ml_prd = multiple_ans_ab('Q35_B',nonspender)\n\nsns.set(rc={'figure.figsize':(15.7,6.27)})\nax = sns.barplot(x=list(auto_ml_prd.keys()), y=list(auto_ml_prd.values()), palette=\"husl\")\nax.set(xlabel=\"Tools for Managing Machine Learning\", ylabel = \"% of Respondents\", title = \"Tools for Managing Machine Learning Experiments to Become Familiar With\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=65)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","ca314dfc":"# select only answered rows\nonline = surveydf[['Q36_Part_1','Q36_Part_2','Q36_Part_3','Q36_Part_4','Q36_Part_5','Q36_Part_6','Q36_Part_7','Q36_Part_8','Q36_Part_9','Q36_OTHER']]\nonline = online.dropna(how=\"all\")","0f6bb387":"# 36. Public sharing sites used\nshare = multiple_ans_alt('Q36', online)\n\nsns.set(rc={'figure.figsize':(15.7,6.27)})\nax = sns.barplot(x=list(share.keys()), y=list(share.values()), palette=\"husl\")\nax.set(xlabel=\"Public Sharing Sites\", ylabel = \"% of Respondents\", title = \"Public Sharing Sites Used\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=65)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","8f528227":"# 37. Begun or completed data science courses\ncourse = multiple_ans('Q37')\ncourse['University Courses'] = course.pop('University Courses (resulting in a university degree)')\ncourse['Cloud-certification programs'] = course.pop('Cloud-certification programs (direct from AWS, Azure, GCP, or similar)')\ncourse = {k: v for k, v in sorted(course.items(), key=lambda item: item[1], reverse=True)}\n\nsns.set(rc={'figure.figsize':(15.7,6.27)})\nax = sns.barplot(x=list(course.keys()), y=list(course.values()), palette=\"husl\")\nax.set(xlabel=\"Data Science Course Platforms\", ylabel = \"% of Respondents\", title = \"Platforms Data Science Courses Taken\/Completed On\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=65)\nfor rect in ax.patches:\n    ax.text (rect.get_x() + rect.get_width() \/ 2,rect.get_height(),\"%.1f%%\"% rect.get_height(), ha='center', **style)\nNone","29026432":"# 38. Primary tool to analyse data\nanalyse = surveydf['Q38'].value_counts(normalize=True)*100\nn = len(bdp2)\ncolors = [cm.tab20(i \/ n) for i in range(n)]\n\n#Using matplotlib\npie, ax = plt.subplots(figsize=[15,10])\nlabels = analyse.index\nplt.pie(x=analyse, autopct=\"%.1f%%\", explode=[0.02]*analyse.shape[0], labels=labels, colors=colors, pctdistance=0.7, textprops={'fontsize': 10})\nplt.title(\"Primary Tool to Analyse Data\", fontsize=14);","b7c61e96":"# 39. Favorite media sources\nmedia = multiple_ans('Q39')\nsns.set(rc={'figure.figsize':(20.7,8.27)})\nax = sns.barplot(y=list(media.keys()), x=list(media.values()), palette=\"husl\")\nax.set(ylabel=\"Data Science Course Sources\", xlabel = \"% of Respondents\")\nax.set_title(\"Favorite Data Science Media Sources\",fontsize=14)\nax.set_yticklabels(ax.get_yticklabels())\nshow_values_on_bars(ax, \"h\", 0.3)\nNone","63f9dd0b":"### Years of experience with Machine Learning","6785340e":"Although most professionals did not use automated Machine Learning tools, the most desired tools to be learnt are automated model selection (e.g. auto-sklearn) and ML pipeline automation (e.g. Google Cloud AutoML). Automated tools like feature engineering\/selection, data augmentation and hyperparameter tuning were shown great similar level of interest in.","e52df802":"Very few respondents have used TPU, with a marginally small number using it often.","4860afd4":"### Cloud Computing platforms used regularly","51a5483f":"### Computer Vision methods used regularly","ff067aa6":"### Natural Language Processing (NLP) methods used regularly","4901f108":"Analysing data that affect business decisions and products is the most common job role of the selected respondents. Data storage roles come in second where are experimentation of using machine learning methods follow immedate suit. ","ed8f9062":"## Conclusion","579ce32e":"### Tools for managing Machine Learning experiments ","e798fadd":"Logistic and linear regression have always been the most commonly used algorithm, but what these algorithms can't do can be caputured and analysed in decision tree algorithms and for better accuracy and increased complexity, the convolusional neural network algorithm. ","5d0cfa60":"Kaggle was proven worthy by the selected respondents as the favorite media source to report on data science topics. Useful notebooks by different users is a great source of learning the use of data and learning new skills and techniques. YouTube videos are a great way for audio visual learning that mimics the learning structure of a teacher-student relationship. Blogs are equally helpful to understand concepts. The reason these 3 channels are most favored is they are relatable to students and professionals over the globe. These channels are written by and for data enthusiasts who provide solutions for the simplest problem to the most complicated ones with easy guided and relatable solutions. This does not belittle the information from other sources like journals, newsletters, podcasts etc.  ","2e6516d7":"## 3. Machine Learning","ea583840":"### Business Intelligence tools used regularly","b0e113c3":"## 5. Tools","c8b15e96":"### 5B. Questions asked to the non-professionals\n\nNon-professionals received questions that what tools they hope to become familiar with in the next 2 years instead of asking what tools they use on\na regular basis.","8cae5975":"62% of the selected population did not prefer automated Machine Learning tools, indicating that these respondents preferred designing models manually.","e7898243":"### Tools used to manage Machine Learning Experiments","d85c854f":"Highest number of female and non-binary respondents were students. While 10% of these respondents were unemployed, the top job titles were Data Scientist and Data Analyst.","962f9da2":"### Automated Machine Learning tools used regularly","100828ab":"In contrast to those that didn't use Python, R while SQL still remained the most used programming language.","452ef0ef":"Python is the most recommended language given its ease and compatibility features. Although the syntax in R is similar to Python in the manner of handling data, it is still far from being recommended when compared to Python.","573b260c":"### Primary tool to analyze data","3a484fdc":"Respondents from atleast 55 countries participated in the survey, with majority of these respondents present in India, followed by the United States. The rest of the 54.4% of selected respondents are from different parts of the world.","ae039e3b":"### Programming language used regularly","bd515932":"Most widely used frameworks are those that support more complicated algorithms and data processing techniques.  ","94134f43":"The choropleth map below shows all the counties where the selected respondents are either paid the lowest yearly compensation or are unemployed.","ecd79e37":"As preferred by the professionals, non-professionals have shown great interest in learning Tableau and PowerBI along with Google Data Studio.","a45af483":"While 35.8% of the companies use Machine Learning streamlined processes, 25% of the companies are venturing into using Machine Learning methods. This shows companies expanding data science avenues in their businesses. ","1399fc8c":"With courses from distinguished universities around the world, Coursera was the used the most for data science courses. Udemy, Kaggle Learn and University courses had a similar level of audience for data science courses. While Fast.ai was the least used, it should be gain more recognition for its free courses and applications that are most known for solving computer vision and NLP problems during different flavors of convolutional neural networks.","b757dc5d":"51.53% of the selected respondents have between 1 to 5 years of experience writing code. Those with more programming experinece are less in number, albiet a small portion (8.8%) of respondents have never written code. ","aeaca604":"With various online platforms providing the resources to share data analysis or data science projects, GitHub was the most preferred site followed by Kaggle.","1c7b4b5e":"### Automated Machine Learning tools used regularly","1d6a20c2":"Most of the selected respondents prefer using personal systems over online hosted systems, thus supporting the previous statement that respondents prefer local systems.","af9bfa24":"A little over a quarter of the population do not prefer hosted notebooks, indicating that locally hosted notebooks are usually preferred.","958b45d1":"While not many have shown interest in learning the tools that manage Machine Learning tasks and experiments, TensorBoard was selected as the most desired tool to learn.","90edab2e":"### Country of Residence","92b9bedf":"### Job Role Duties","13cc7889":"While most selected respondents do not prefer additional hardware, the most used additional hardware are GPUs.","6574724c":"Not many preferred using tools to manage the machine learning experiments and applications. This could be used an an opportunity to promote the use of such tools to encourage exploring machine learning. ","9c993bbc":"### Countries with respondents who are unpaid\/paid less","590293f7":"MySQL ranked the highest in Big Data products desired to be learnt, with the NoSQL database software MongoDB being the second most sought after.","3d32b440":"### Big Data products","381c8a02":"### Specialized hardware used regularly","1b6cebf6":"While 37% of the selected population did not use Business Intelligent tools, the most frequently used were Tableau and PowerBI.","99f0b637":"### Machine Learning products","162085b0":"## 4. Organization","ac85ab10":"### Machine Learning products used regularly","ff961606":"### Cloud Computing products used regularly","18722e85":"### Experience with Writing Code","e54003a6":"## 2. Technical Information","7beba9ac":"Jupyter notebooks for the win! The provision for scripting, visualizations and using narrative text while supporting languages like R and Python make them easy to use. With different products supporting Jupyter notebooks, it is not suprising to know that this IDE is the most used tool.","e90025a9":"About 70% of the respondents use Python, making it the most used language among others. ","89db6ce5":"## 6. Online Platform and Resources","5bf98ac8":"Most female and non-binary users are between the ages of 18 and 29, with the highest being in their early 20s. Its interesting to note the users who are about the age of 50, especially the 2 reposndents over the age of 70!","c05830d9":"### Money spent on Machine Learning\/Cloud Computing services","87b8dbdc":"### Big Data products used regularly","fa766049":"It is encouraging to note that a higher percentage of repondents preferred using R and Python on RStudio and Jupyterlab to analyse data than those who used the conventional software like Microsoft Excel and Google Sheets. While it can be argued that some datasets are small and not too complex to use local development environments, business intelligence software like Salesforce, PowerBI and Tableau can be used for these datasets, although the component of monthly\/yearly subscription payment for these tools could hinder the desire to use them. ","c89bd040":"### Big Data products used most often","d26f3183":"### Programming language recommendation","821eb6a4":"While most respondents identified they belonged to small sized companies, it is worth noting that this includes respondents who are unemployed. Medium sized and larger company respondents are distributed fairly equally.","2e340162":"## 1. Demographic\n\nThere is a vast difference of genders who use Kaggle. The number of female users is alarmingly low as is the number of non-binary people. No assumptions can be made regarding those who chose not to say and self-describe. Due to this disparity, the analysis will focus only on female and non-binary users. ","273547ca":"### Machine Learning methods used at work","7f7735ce":"### Use of Tensor Processing Unit (TPU)","103bae18":"### Individuals responsible for Data Science at workplace","471ee485":"From the automated ML tools categories above, Auto-Sklearn and Google Cloud AutoML are the most sought after tools to be learnt.","897ab634":"The graph shows that the smallest salary bracket has the highest respondents. It should be noted that this indicates that most respondents are either paid very less or are unemployed. A very small portion of respondents are paid above $100,000 USD a year. This could be assumed to be those are higher positions with mutiple years of experience.","7f906a4d":"### Machine Learning frameworks used regularly","96d79640":"### Data Science course platforms","e22f0fb2":"### Languages used along with Python","2dcce947":"### Qualifications","d91b86a3":"### Current Role","21dbe6b8":"### Visualization libraries used regularly","4029c5d0":"### 5A. Questions asked to the professionals","01ca437b":"More than 55% of those who asked this question did not use specific Machine learning products.","ae6a5318":"More than 50% of the selected respondents have spent money on Machine Learning or cloud services at home or at work. Based on this, the survey questions were designed to ask specific questions to those who spent money on the tools and services against those that didn't. Those who spent money were defined as professionals (group A), whereas those that didn't were defined as non-professionals (group B).","3deb496f":"Known for its simple and easy depiction of data, Matplotlib was the most used data visualization library followed by its dependent library Seaborn. Ggplot took the lead for R users. ","9108ea7a":"Google Cloud products took precedence over Azure and Amazon products for non-professionals to learn about.","d11fd1c7":"### Specific automated Machine Learning tools","648860e5":"### Business Intelligence tools used most often","43968b36":"A total of 89.63% of the selected respondents have an official university degree, with the largest group having a masters degree.","a80a15f7":"Among those that who answered this question, Image classification and general purpose computer vison models stood out with generative networks being used the least by these respondents. ","79bbfcb6":"### Public sites used to share analysis or applications","abb5a204":"Those professional respondents who did use automated Machine Learning tools used more of Auto-Sklearn.","5734acc2":"### Categories of Automated Machine Learning tools","3c55f785":"Inspiringly, a little less than half the selected respondents have been using machine learning methods for less than a year. Those with more experience seem to be in smaller numbers comparitively. ","4545ab68":"### Cloud Computing platforms","17450144":"Flavors of SQL were the most preferred Big Data products used by professionals.","4a105b55":"### Most preferred computing platform","f65dfa79":"### Machine Learning algorithms used regularly","b1b8ce9c":"Those that used Python also used more of SQL and less of languages like R and C.","bb830ed8":"### Business Intelligence products","0ab9fb2e":"### Cloud Computing products","8d23c8f0":"### Languages used other than Python","e47c9973":"### Size of the company respondents employed at","50b6a4b5":"Of those who were asked about Cloud Computing products used regularly, Amazon EC2 ranked the highest, followed by Azure Cloud Services, GCC Engine and AWS Lambda.","5d3dcbf2":"Among those who used multiple BI tools, Tableau was the most preferred which is closely followed by PowerBI. With very similar functionalities, the use of these two tools boils down to business requriments.","66e2c538":"There are either too few or quite many individuals at these companies that are responsible for data science workloads. 37% of the companies have at the most 4 employees that work on data using data science tools and techniques.","08db6025":"### Regular hosted notebook products used","d994b7be":"Just as the professionals, non-professionals have shown great interest in familiarising AWS and GCP platforms.","547761fb":"From those who used more than one Big Data product, one quarter of them prefered MySQL while almost another quarter of respondents preferred other flavors of SQL rather than cloud based products.","9d39fe61":"Amazon Web Services and Google Cloud Platform were the most regularly used platforms.","03322d98":"### Integrated Development Environments (IDE's) used regularly","ca0df74c":"As Cloud Computing products, Google Cloud machine learning products were shown more interest in than the Azure or Amazon services.","f5cc594f":"### Age","edb8457f":"### Yearly Compensation in USD","8717d51f":"### Favorite media sources ","89806d38":"## Introduction\n\n2020 is over, which calls for a retrospect. The 2020 Kaggle survey includes users from different backgrounds and parts of society. The global pandemic of 2020 affected female workers in every sector across the world, with evidence based reports from the [United States](https:\/\/www.americanprogress.org\/issues\/women\/reports\/2020\/10\/30\/492582\/covid-19-sent-womens-workforce-progress-backward\/) and [India](https:\/\/www.business-standard.com\/article\/current-affairs\/covid-impact-women-workforce-disappearing-most-affected-in-urban-india-120121500259_1.html). \n\nThere has always been a massive disparity between females and males in the IT sector, with males dominating the industry from its inception. The global pandemic only made it worse. Due to the current nature of the unstable work economy, this report specifically studies the responses of female and non-binary users in an attempt to learn about the services and tools that can be provided to this population to help bridge the gender gap in the industry. I have decided to include the non-binary respondents as well since it is encouraging to analyse the diversity in the user base.   ","7d3fb47d":"For those who were asked this question, word embeddings took higher precedence over other methods.","fc3a6ecd":"Of the selected female and non-binary respondents, most have been exploring various tools, methods and products available to explore and use data. Various tools to manage data and its applications have been invested in by some respondents, while the non-professionals have shown great interest in familiarising with advanced tools and technology to expands their machine learning and cloud computing skills. Although, automated tools that benefit coders in helping them manage the data and applications should be promoted to encourage exploring machine learning techniques and cloud computing capabilities. \n\nOf all the survey respondents, only 20.6% of them identified as female or non-binary. This goes to show that a great deal needs to be done to encourage females and gender non-conforming individuals to explore the avenues of data science and subsequently use the resources available. It is promising to note that most of these selected respondents were in their early 20s, proving that recent generations are eager to venture into the information technology and more specifically into data science. This community is growing far and wide and more data enthusiasts - young and old are discovering the joys of exploring data with the tools that are known and unknown. Promoting these tools are essential to help data entusiats build their skills and grow in large numbers with greater diversity."}}