{"cell_type":{"c576c297":"code","37f7275d":"code","7d02740a":"code","f6ffcffe":"code","0eb33792":"code","6583e446":"code","64ccf74b":"code","47c9b02a":"code","da73b818":"code","197bcad9":"code","b6c066e7":"code","4b2d1daa":"code","29b0f852":"code","d9039ff6":"code","906e3d4e":"code","a1a8dcb8":"code","28e9fa08":"code","24cbb761":"code","afd8dd1f":"code","cd0728f1":"code","f22d0602":"code","cf6429ad":"code","ddec42df":"code","81cbb17b":"code","55d3a6ae":"code","239bff1f":"code","f2b8b512":"code","ca668968":"code","54c3245c":"code","3ad700f2":"code","f1bc9a20":"code","c83d457f":"code","2c95b101":"code","00082cc1":"code","ef9d74dc":"code","cbe8d6f2":"code","d1b1562f":"code","a08e3584":"code","337ff05a":"code","fe209afa":"code","b1754283":"code","2703634a":"code","f172b62d":"code","e0f3a691":"code","ab50349f":"code","231ddad0":"code","27aa67ab":"code","a56b9baa":"code","c830bf31":"code","256fe669":"code","ed3b10e8":"code","38cd814a":"code","890908cc":"code","415d2535":"code","1d950689":"code","ea200811":"code","5cafa4ab":"code","929f1ff6":"code","987f16ad":"code","53cedd74":"code","93605a53":"code","8e799924":"code","94244020":"code","eb296313":"code","3edf3654":"code","452cdb2d":"code","917f8dd9":"code","96399290":"code","5388f00e":"code","52476552":"code","d3213063":"code","69dec2ad":"code","e21c692e":"code","35fccece":"markdown","2130dfbc":"markdown","8589d016":"markdown","6fff3237":"markdown","bc284c58":"markdown","feb9509a":"markdown","30c34ab4":"markdown","d79edeea":"markdown","00d8a1f5":"markdown","b9f321fa":"markdown","ed59383d":"markdown","9d3857b4":"markdown","0d6c5842":"markdown","82413919":"markdown","a98b2037":"markdown","989328d2":"markdown","7d052bbd":"markdown","448a2c26":"markdown","c323bb7c":"markdown","e1f923fb":"markdown","e3fa1e25":"markdown","a7b4d777":"markdown","45a24522":"markdown","e782279a":"markdown","539bae39":"markdown","c8db7319":"markdown","37bcf4c4":"markdown","2db59726":"markdown","2b1cb237":"markdown","c897d53a":"markdown","65303863":"markdown","32bda09f":"markdown","edeaa902":"markdown","74fb8743":"markdown","262133f9":"markdown","b87d7a7d":"markdown","729c6731":"markdown","c3a41f16":"markdown","f51bed5c":"markdown","74c2d590":"markdown","8ffa70d2":"markdown","b23e4529":"markdown"},"source":{"c576c297":"# Location of dataset\n# Kaggle URL https:\/\/www.kaggle.com\/keplersmachines\/kepler-labelled-time-series-data","37f7275d":"# Load necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn as sk\nimport scipy\nimport seaborn as sns\nimport scipy.stats as ss\nimport statsmodels.stats.weightstats as ws\nfrom  scipy import ndimage\nimport requests # for importing dataset from Google Drive\nfrom io import StringIO # for importing dataset from Google Drive\nfrom sklearn.model_selection import train_test_split \nimport sklearn.linear_model as sklm # includes Logistic Regression, which will be tested for predictive capability\nimport sklearn.decomposition as skdc # includes Principal Component Analysis, a method of dimensionality reduction\nimport sklearn.pipeline as skpl # convenient module for calculating PCs and using them in logistic regression\nimport sklearn.model_selection as m_s\nimport sklearn.naive_bayes as n_b\n\n\n%matplotlib inline","7d02740a":"def gdrive_cvs_to_df(id):\n    session = requests.Session()\n    URL = \"https:\/\/docs.google.com\/uc?export=download\"\n    response = session.get(URL, params = { 'id' : id }, stream = True)\n    token = None\n    for key, value in response.cookies.items():\n        if key.startswith('download_warning'):\n            token = value\n    if token:\n        params = { 'id' : id, 'confirm' : token }\n        response = session.get(URL, params = params, stream = True)\n    csv_raw = StringIO(response.text)\n    df = pd.read_csv(csv_raw)\n    #df = pd.read_csv(csv_raw,index_col=0) # makes dataset same as earlier, with no index column - majigger\n                                            # previous checkpoint of this notebook to to verify that original code still works\n    return df\n\ndata = gdrive_cvs_to_df('14euHb8Yb7sQpbq_S_qxi_fCPxx2smoWV')\ndata2 = data.copy() # Milestone 2 analysis\ndata3 = data.copy() # Milestone 3 analysis\ndata4 = data.copy() # Naive Bayes analysis","f6ffcffe":"print(data.head())\ndata.tail()","0eb33792":"data.shape","6583e446":"data.dtypes","64ccf74b":"data.columns","47c9b02a":"print(data.dtypes)\ndata.tail()","da73b818":"\nlabels = data.LABEL\ndata_nolabels = data.reset_index(drop=True)\ndata = data.drop('LABEL', axis=1)\nprint(data[labels==2].shape)","197bcad9":"data_nolabels.head()","b6c066e7":"data_nolabels.shape","4b2d1daa":"# f, axes = plt.subplots(2, figsize=(12, 12), sharex=True)\n# sns.distplot( data.iloc[1,:] , color=\"skyblue\", ax=axes[0])\n# sns.distplot( data.iloc[2,:] , color=\"olive\", ax=axes[1])\nplt.figure(figsize=(30, 7.5))\nax = sns.lineplot(x=np.arange(3197), y=data.iloc[1,:], data=data)\nsns.set(font_scale=3.5)\nax.set(xlabel='Time', ylabel='Flux')\nax.set_title(\"Lightcurve from Star 1\")\nax.set_xlim(0, 3197)\n\nplt.figure(figsize=(30, 7.5))\nax = sns.regplot(x=np.arange(3197), y=data.iloc[1,:], fit_reg=False, scatter_kws={\"s\": 10})\nsns.set(font_scale=3.5)\nax.set(xlabel='Time', ylabel='Flux')\nax.set_title(\"Lightcurve from Star 1\")\nax.set_xlim(0, 3197)","29b0f852":"plt.figure(figsize=(30, 7.5))\nax = sns.lineplot(x=np.arange(3197), y=data.iloc[100,:], data=data)\nsns.set(font_scale=3.5)\nax.set(xlabel='Time', ylabel='Flux')\nax.set_title(\"Lightcurve from Star 100\")\nax.set_xlim(0, 3197)\n\nplt.figure(figsize=(30, 7.5))\nax = sns.regplot(x=np.arange(3197), y=data.iloc[100,:], fit_reg=False, scatter_kws={\"s\": 10})\nsns.set(font_scale=3.5)\nax.set(xlabel='Time', ylabel='Flux')\nax.set_title(\"Lightcurve from Star 100\")\nax.set_xlim(0, 3197)","d9039ff6":"# data.loc[:, 'DateFormat'] = pd.to_datetime(data.loc[:, 'DateFormat'])\n# data.set_index('DateFormat', inplace = True)","906e3d4e":"# ax = sns.regplot(time, flux, fit_reg=False, scatter_kws={\"s\": 10})\n# sns.set(font_scale=2.5)\n# ax.set(xlabel='Time', ylabel='Flux')\n# ax.set_title(\"Lightcurve from Star 1\")\n# ax.set_xlim(0, 3197)","a1a8dcb8":"df = data.copy()","28e9fa08":"def stats_plots(df):\n    means = df.mean(axis=1)\n    medians = df.median(axis=1)\n    std = df.std(axis=1)\n    maxval = df.max(axis=1)\n    minval = df.min(axis=1)\n    skew = df.skew(axis=1)\n    \n    fig = plt.figure(figsize=(26,26))\n    \n    ax = fig.add_subplot(231)\n    ax.hist(means,alpha=0.8,bins=100)\n    ax.set_xlabel('Mean Intensity')\n    ax.set_ylabel('Num. of Stars')\n    \n    ax = fig.add_subplot(232)\n    ax.hist(medians,alpha=0.8,bins=100)\n    ax.set_xlabel('Median Intensity')\n    ax.set_ylabel('Num. of Stars')\n    \n    ax = fig.add_subplot(233)\n    ax.hist(std,alpha=0.8,bins=100)\n    ax.set_xlabel('Intensity Standard Deviation')\n    ax.set_ylabel('Num. of Stars')\n    \n    ax = fig.add_subplot(234)\n    ax.hist(maxval,alpha=0.8,bins=100)\n    ax.set_xlabel('Maximum Intensity')\n    ax.set_ylabel('Num. of Stars')\n    \n    ax = fig.add_subplot(235)\n    ax.hist(minval,alpha=0.8,bins=100)\n    ax.set_xlabel('Minimum Intensity')\n    ax.set_ylabel('Num. of Stars')\n    \n    ax = fig.add_subplot(236)\n    ax.hist(skew,alpha=0.8,bins=100)\n    ax.set_xlabel('Intensity Skewness')\n    ax.set_ylabel('Num. of Stars')","24cbb761":"stats_plots(data)\nplt.show()","afd8dd1f":"def stats_plots_label(df):\n    means1 = df[labels==1].mean(axis=1)\n    medians1 = df[labels==1].median(axis=1)\n    std1 = df[labels==1].std(axis=1)\n    maxval1 = df[labels==1].max(axis=1)\n    minval1 = df[labels==1].min(axis=1)\n    skew1 = df[labels==1].skew(axis=1)\n    means2 = df[labels==2].mean(axis=1)\n    medians2 = df[labels==2].median(axis=1)\n    std2 = df[labels==2].std(axis=1)\n    maxval2 = df[labels==2].max(axis=1)\n    minval2 = df[labels==2].min(axis=1)\n    skew2 = df[labels==2].skew(axis=1)\n    fig = plt.figure(figsize=(26,26))\n    \n    ax = fig.add_subplot(231)\n    ax.hist(means1,alpha=0.6,bins=50,color='b',density=True,range=(-250,250))\n    ax.hist(means2,alpha=0.6,bins=50,color='r',density=True,range=(-250,250))\n    ax.get_legend()\n    ax.set_xlabel('Mean Intensity')\n    ax.set_ylabel('Num. of Stars')\n    ax = fig.add_subplot(232)\n    ax.hist(medians1,alpha=0.6,bins=50,color='b',density=True,range=(-0.1,0.1))\n    ax.hist(medians2,alpha=0.6,bins=50,color='r',density=True,range=(-0.1,0.1))\n    ax.get_legend()\n\n    ax.set_xlabel('Median Intensity')\n    ax.set_ylabel('Num. of Stars')\n    ax = fig.add_subplot(233)    \n    ax.hist(std1,alpha=0.6,bins=50,density=True,color='b',range=(0,4000))\n    ax.hist(std2,alpha=0.6,bins=50,density=True,color='r',range=(0,4000))\n    ax.get_legend()\n\n    ax.set_xlabel('Intensity Standard Deviation')\n    ax.set_ylabel('Num. of Stars')\n    ax = fig.add_subplot(234)\n    ax.hist(maxval1,alpha=0.6,bins=50,density=True,color='b',range=(-10000,50000))\n    ax.hist(maxval2,alpha=0.6,bins=50,density=True,color='r',range=(-10000,50000))\n    ax.get_legend()\n\n    ax.set_xlabel('Maximum Intensity')\n    ax.set_ylabel('Num. of Stars')\n    ax = fig.add_subplot(235)\n    ax.hist(minval1,alpha=0.6,bins=50,density=True,color='b',range=(-50000,10000))\n    ax.hist(minval2,alpha=0.6,bins=50,density=True,color='r',range=(-50000,10000))\n    ax.get_legend()\n\n    ax.set_xlabel('Minimum Intensity')\n    ax.set_ylabel('Num. of Stars')\n    ax = fig.add_subplot(236)\n    ax.hist(skew1,alpha=0.6,bins=50,density=True,color='b',range=(-40,60))\n    ax.hist(skew2,alpha=0.6,bins=50,density=True,color='r',range=(-40,60)) \n    ax.get_legend()\n\n    ax.set_xlabel('Intensity Skewness')\n    ax.set_ylabel('Num. of Stars')\n    ax.get_legend()\n\nstats_plots_label(data)\nplt.show()","cd0728f1":"data[labels==1].median(axis=1).describe()","f22d0602":"data[labels==2].median(axis=1).describe()","cf6429ad":"# exoplanet stars\nfig = plt.figure(figsize=(20,50))\n#x = np.array(range(3197))\nx=np.arange(3197)\nfor i in range(37):\n    ax = fig.add_subplot(13,3,i+1)\n    ax.scatter(x,df[labels==2].iloc[i,:])\n    #ax = sns.regplot(x=np.arange(3197), y=data.iloc[100,:], fit_reg=False, scatter_kws={\"s\": 10})","ddec42df":"# non-exoplanet stars\nfig = plt.figure(figsize=(20,50))\nx = np.array(range(3197))\nfor i in range(37):\n    ax = fig.add_subplot(13,3,i+1)\n    ax.scatter(x,df[labels==1].iloc[i,:])","81cbb17b":"data2 = data2.reset_index(drop=True)\nlabels = data2.LABEL\ndata2.LABEL.values","55d3a6ae":"def t_test(a, b, alpha, alternative='two-sided'):\n    diff = a.mean() - b.mean()\n\n    res = ss.ttest_ind(a, b, equal_var=False)\n      \n    means = ws.CompareMeans(ws.DescrStatsW(a), ws.DescrStatsW(b))\n    confint = means.tconfint_diff(alpha=alpha, alternative=alternative, usevar='unequal') \n    degfree = means.dof_satt()\n\n    index = ['DegFreedom', 'Difference', 'Statistic', 'PValue', 'Low95CI', 'High95CI']\n    return pd.Series([degfree, diff, res[0], res[1], confint[0], confint[1]], index = index)","239bff1f":"data2[\"meanflux\"] = data2.mean(axis=1)\n# data2.head","f2b8b512":"notdetected = data2[data2.LABEL.values == 1]['meanflux']\n# notdetected = data[data.index.values == 1]['meanflux']\ndetected = data2[data2.LABEL.values == 2]['meanflux']\n#detected = data[data.index.values == 2]['meanflux']\n\ntest = t_test(notdetected, detected, 0.05)\ntest","ca668968":"grouped = data2.groupby('LABEL')\narrays = []\nfor k in grouped.groups:\n    arrays.append(np.array(grouped.get_group(k)['meanflux']))\nf_statistic, p_value = ss.f_oneway(*arrays)\nprint('F statistic = ' + str(f_statistic))\nprint('P-value = ' + str(p_value))","54c3245c":"detected = data2[data2.iloc[:,0]==1] # all the stars with detected exoplanets\n# detected = data[labels==1]\nnotdetected =  data2[data2.iloc[:,0]==2] # all the stars with no detected exoplanets\n# notdetected = data[labels==2]\nlen(detected), len(notdetected)","3ad700f2":"def plot_hist(x, p=5):\n    # Plot the distribution and mark the mean\n    plt.hist(x, alpha=.5)\n    plt.axvline(x.mean())\n    # 95% confidence interval    \n    plt.axvline(np.percentile(x, p\/2.), color='red', linewidth=3)\n    plt.axvline(np.percentile(x, 100-p\/2.), color='red', linewidth=3)\n    plt.xticks(fontsize = 20)\n    plt.yticks(fontsize = 20)\n    \ndef plot_dists(a, b, nbins, a_label='pop_A', b_label='pop_B', p=5):\n    # Create a single sequence of bins to be shared across both\n    # distribution plots for visualization consistency.\n    combined = pd.concat([a, b])\n    breaks = np.linspace(\n        combined.min(), \n        combined.max(), \n        num=nbins+1)\n\n    ax = plt.figure(figsize=(15, 10)).gca()\n    plt.xticks(fontsize = 20)\n    plt.yticks(fontsize = 20)\n#fig, ax = plt.subplots(figsize=(20, 10)\n    plt.subplot(2, 1, 1)\n    plot_hist(a)\n    plt.title(a_label, fontsize = 30)\n    \n    plt.subplot(2, 1, 2)\n    plot_hist(b)\n    plt.title(b_label, fontsize = 30)\n    plt.xticks(fontsize = 20)\n    plt.yticks(fontsize = 20)\n    \n    plt.tight_layout()\n    \n# plot_dists(notdetected.iloc[0], detected.iloc[0], 30, a_label='notdetected', b_label='detected')\nplot_dists(notdetected.meanflux, detected.meanflux, 30, a_label='notdetected', b_label='detected')\n\nplt.show()","f1bc9a20":"n_replicas = 1000\nnotdetected_bootstrap_means = pd.Series([\n    notdetected.meanflux.sample(frac=1, replace=True).mean()\n    for i in range(n_replicas)])\n\ndetected_bootstrap_means = pd.Series([\n        detected.meanflux.sample(frac=1, replace=True).mean()\n        for i in range(n_replicas)])\n\nplot_dists(notdetected_bootstrap_means, detected_bootstrap_means, \n           nbins=80, a_label='notdetected', b_label='detected')\n\nplt.show()","c83d457f":"diffs = []\nfor i in range(n_replicas):\n    sample = data2.sample(frac=1.0, replace=True)\n    notdetected_sample_mean = np.mean(sample[sample.LABEL.values == 1]['meanflux'])\n    # notdetected_sample_mean = np.mean(sample[sample.index.values == 1]['meanflux'])\n    detected_sample_mean = np.mean(sample[sample.LABEL.values == 2]['meanflux'])\n    # detected_sample_mean = np.mean(sample[sample.index.values == 2]['meanflux'])\n#     notdetected_sample_mean = np.mean(sample[sample.iloc[:,0] == \"1\"]['mean_flux']) # same code different way\n#     detected_sample_mean = np.mean(sample[sample.iloc[:,0] == \"2\"]['mean_flux']) # same code different way\n    diffs.append(notdetected_sample_mean - detected_sample_mean)\n\ndiffs = pd.Series(diffs)\ndif_mean = diffs.mean()\nprint(dif_mean)\n\nplot_hist(diffs)\nplt.title(\"Bootstrap Difference in Means\", fontdict = {'fontsize' : 30})\nplt.xticks(fontsize = 20)\nplt.yticks(fontsize = 20)","2c95b101":"# confidence interval\nalpha = 0.05\n\nleft = np.percentile(diffs, alpha\/2*100)\nright = np.percentile(diffs, 100-alpha\/2*100)\n\nprint((1-alpha)*100,'%','confidence interval for the difference between means:', (round(left,2), round(right,2)))","00082cc1":"num_samples = 1000\n\nnotdetectedb = data2[data2.LABEL.values == 1].sample(n=num_samples, replace=True) \n\ndetectedb = data2[data2.LABEL.values == 2].sample(n=num_samples, replace=True)\n\n\nax = plt.figure(figsize=(12, 8)).gca()\nplt.title('Histograms of Mean Flux Intensity of Notdetected and Detected', fontdict = {'fontsize' : 30})\nnotdetectedb.meanflux.hist(label='notdetected', bins=30, alpha=.7)\ndetectedb.meanflux.hist(label='detected', bins=30, alpha=.7)\nplt.xlabel('Mean flux', fontsize = 25)\nplt.xticks(fontsize = 20)\nplt.yticks(fontsize = 20)\nplt.legend(fontsize = 20)\n\nmean_of_means= np.concatenate([notdetectedb.meanflux,detectedb.meanflux]).mean()\nprint('Means of Mean Fluxes: %.1f' % mean_of_means)","ef9d74dc":"def plot_ci(p, post, num_samples, lower_q, upper_q):\n    # This function computes a credible interval using an assumption\n    # of symetry in the bulk of the distribution to keep the \n    # calculation simple. \n    # Compute a large sample by resampling with replacement\n    samples = np.random.choice(p, size=num_samples, replace=True, p=post)\n    ci = scipy.percentile(samples, [lower_q*100, upper_q*100]) # compute the quantiles\n    \n    interval = upper_q - lower_q\n    ax = plt.figure(figsize=(12, 8)).gca()\n    plt.title('Posterior density with %.3f credible interval' % interval, fontsize = 30)\n    plt.plot(p, post, color='blue')\n    plt.xlabel('Parameter value', fontsize = 25)\n    plt.ylabel('Density', fontsize = 25)\n    plt.xticks(fontsize = 20)\n    plt.yticks(fontsize = 20)\n    plt.axvline(x=ci[0], color='red')\n    plt.axvline(x=ci[1], color='red')\n    print('The %.3f credible interval is %.3f to %.3f' \n          % (interval, lower_q, upper_q))","cbe8d6f2":"def posterior(prior, like):\n    post = prior * like # compute the product of the probabilities\n    return post \/ sum(post) # normalize the distribution to sum to unity","d1b1562f":"N = 1000\n\np = np.linspace(-100, 100, num=N)\n\npp_notdetected = scipy.stats.norm.pdf(p, loc=notdetectedb.meanflux.mean())\npp_notdetected = pp_notdetected \/ pp_notdetected.sum() # normalize\n\ndef comp_like(p, x): \n    variance = np.std(x)**2 # sigmasqr\n    x_mean = np.asarray(x).mean()  # xbar\n    print('Mean = %.3f, Standard deviation = %.3f' % (x_mean, np.std(x)))\n    n = len(x)\n    l = np.exp(-n * np.square(x_mean - p) \/ (2 * variance))\n    return l \/ l.sum()\n\n\nlike_notdetected = comp_like(p,notdetectedb.meanflux)\npost_notdetected = posterior(pp_notdetected, like_notdetected)\n\nax = plt.figure(figsize=(12, 8)).gca()\nplt.plot(p, pp_notdetected, label='prior')\nplt.plot(p, like_notdetected, label='likelihood', alpha=.3, linewidth=10)\nplt.plot(p, post_notdetected, label='posterior')\nplt.xticks(fontsize = 20)\nplt.yticks(fontsize = 20)\nplt.legend(fontsize = 20)\n\nplt.show()","a08e3584":"N = 1000\n\np = np.linspace(-100, 100, num=N)\n\npp_detected = scipy.stats.norm.pdf(p, loc=detectedb.meanflux.mean())\npp_detected = pp_detected \/ pp_detected.sum() # normalize\n\ndef comp_like(p, x): \n    variance = np.std(x)**2 # sigmasqr\n    x_mean = np.asarray(x).mean()  # xbar\n    print('Mean = %.3f, Standard deviation = %.3f' % (x_mean, np.std(x)))\n    n = len(x)\n    l = np.exp(-n * np.square(x_mean - p) \/ (2 * variance))\n    return l \/ l.sum()\n\n\nlike_detected = comp_like(p,detectedb.meanflux)\npost_detected = posterior(pp_detected, like_detected)\n\nax = plt.figure(figsize=(12, 8)).gca()\nplt.plot(p, pp_detected, label='prior')\nplt.plot(p, like_detected, label='likelihood', alpha=.3, linewidth=10)\nplt.plot(p, post_detected, label='posterior')\nplt.legend(fontsize = 20)\nplt.title('Posterior Distribution of detected meanflux', fontsize = 30)\nplt.xticks(fontsize = 20)\nplt.yticks(fontsize = 20)\nplt.show()","337ff05a":"num_samples = 100000\n\nplot_ci(p, post_notdetected, num_samples, lower_q=.025, upper_q=.975)","fe209afa":"num_samples = 100000\n\nplot_ci(p, post_detected, num_samples, lower_q=.025, upper_q=.975)","b1754283":"# use 1000 data points\n\nnum_samples =  1000\n\nlike_notdetected_1000 = comp_like(p, notdetectedb.meanflux)\npost_notdetected_1000 = posterior(pp_notdetected, like_notdetected)\nlike_detected_1000 = comp_like(p, detectedb.meanflux)\npost_detected_1000 = posterior(pp_detected, like_detected)\n\n# Plotting the Posterior distributions and CIs\n\nplot_ci(p, post_notdetected_1000, num_samples, lower_q=.025, upper_q=.975)\nplot_ci(p, post_detected_1000, num_samples, lower_q=.025, upper_q=.975)","2703634a":"# Get target and original x-matrix\n\ntarget_label = 'LABEL'\nfeature_labels = [x for x in data3.columns if x not in [target_label]]\n\n\nX = data3[feature_labels].values\ny = data3[target_label]","f172b62d":"# train\/test split\nX_train,X_test, y_train , y_test = train_test_split(X,y,test_size=0.2)","e0f3a691":"from sklearn.preprocessing import StandardScaler # standard scale the data\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\npca_orig = skdc.PCA() # initialize model object","ab50349f":"pcafit = pca_orig.fit_transform(X,y) # apply dimensionality reduction to X","231ddad0":"var_explained = pca_orig.explained_variance_ratio_ # ratio of variance each PC explains\nprint(pd.Series(var_explained))\n# Since 3197 components aren't necessary, most will be disregarded \n# because they explain less than.01 of the variance\nprint(sum(var_explained[0:10]))","27aa67ab":"pca = skdc.PCA(n_components = 10) # only include first 10 components\nlogreg = sklm.LogisticRegression()# empty model space, initialize model object\npipeline = skpl.Pipeline([('pca', pca), ('logistic', logreg)]) #create pipeline from pca to logistic regression space","a56b9baa":"# from sklearn.pipeline import Pipeline\npipeline.fit(X_train, y_train)","c830bf31":"# Plot the explained variance for all principal components\nax = plt.figure(figsize=(10, 8)).gca()\nplt.plot([i + 1 for i in range(10)], pca.explained_variance_)\n\nplt.title('Explained variance by Principal Component Num', fontsize = 30)\nplt.xlabel('Principal Component', fontsize = 25)\nplt.ylabel('Explained Variance', fontsize = 25)\nplt.xticks(fontsize = 20)\nplt.yticks(fontsize = 20)","256fe669":"pca_result = pcafit.copy()","ed3b10e8":"pca_result.shape","38cd814a":"column_names = ['pc' + str(ix+1) for ix in range(X_train.shape[1])]\npca_df = pd.DataFrame(data = pca_result, columns=column_names)\npca_df[target_label] = y_test","890908cc":"pca_df","415d2535":"# Perform linear regression with the first N columns.\nimport statsmodels.formula.api as sm\nn = 10\nformula_start = target_label + ' ~ '\nformula_terms = ['pc' + str(x+1) for x in range(n)]\nformula_end = ' + '.join(formula_terms)\nformula_final = formula_start + formula_end\n\npcr_model = sm.ols(formula = formula_final, data=pca_df)\n\nresults = pcr_model.fit()\n\n# Get most of the linear regression statistics we are interested in:\nprint(results.summary())\n\n# Plot a histogram of the residuals\nsns.distplot(results.resid, hist=True)\nplt.xticks(fontsize = 20)\nplt.yticks(fontsize = 20)\nplt.xlabel('Residual', fontsize = 25)\nplt.ylabel('Frequency', fontsize = 25)\nplt.title('Residual Histogram', fontsize = 30)","1d950689":"# Perform linear regression with the first N columns.\nimport statsmodels.formula.api as sm\nn = 6\nformula_start = target_label + ' ~ '\nformula_terms = ['pc' + str(x+1) for x in range(n)]\nformula_end = ' + '.join(formula_terms)\nformula_final = formula_start + formula_end\n\npcr_model = sm.ols(formula = formula_final, data=pca_df)\n\nresults = pcr_model.fit()\n\n# Get most of the linear regression statistics we are interested in:\nprint(results.summary())\n\n# Plot a histogram of the residuals\nsns.distplot(results.resid, hist=True)\nplt.xticks(fontsize = 20)\nplt.yticks(fontsize = 20)\nplt.xlabel('Residual', fontsize = 25)\nplt.ylabel('Frequency', fontsize = 25)\nplt.title('Residual Histogram', fontsize = 30)","ea200811":"y_pred = pipeline.predict(y_test.values.reshape(-1, 1))","5cafa4ab":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\ncm","929f1ff6":"sns.heatmap(cm, annot=True)\nplt.xticks(fontsize = 20)\nplt.yticks(fontsize = 20)\nplt.show()","987f16ad":"accuracy_pcr=pipeline.score(X_train,y_train)\nprint(\"Pipeline accuracy:%{}\".format(accuracy_pcr*100))","53cedd74":"# Fitting Logistic Regression to the Training set\nclassifier = sklm.LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train)","93605a53":"# Predicting the Test set results\ny_pred = classifier.predict(X_test)\nprint(y_pred)","8e799924":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\ncm","94244020":"sns.heatmap(cm, annot=True)\nplt.xticks(fontsize = 20)\nplt.yticks(fontsize = 20)\nplt.show()","eb296313":"accuracy_lr=classifier.score(X_test,y_test)\nprint(\"logistic regression accuracy:%{}\".format(accuracy_lr*100))","3edf3654":"import sklearn.metrics as sm\n\ndf = data4.copy()\n\nlabel_col = 'LABEL'","452cdb2d":"# Convert columns to category\n\ncol_names = list(df)\nprint(\"\\nNames of dataframe columns\")\nprint(col_names)\n\n# loop to change each column to category type\nfor col in col_names:\n    df[col] = df[col].astype('category',copy=False)\n\nprint(\"\\nExample data changed to category type\")\nprint(df)\nprint(df.dtypes)","917f8dd9":"# Create label vector\ny = df[label_col].cat.codes\n\n# Create 2D Feature array and labels\nX = []\nfor c in df.columns:\n    if c != label_col:\n        if str(df[c].dtype) == 'category':\n            X.append(df[c].cat.codes)\n        else:\n            X.append(df[c])\nX = np.array(X).T\ndel df\nprint('X:', X.shape, 'y:', y.shape)\n\n# Split data for training and testing\nX_train, X_test, y_train, y_test = m_s.train_test_split(X, y) # 25% for testing\ndel X, y\nprint('X_train:', X_train.shape, 'y_train:', y_train.shape)\nprint('X_test:', X_test.shape, 'y_test:', y_test.shape)","96399290":"# Create the Model using training data\nmodelNB = n_b.MultinomialNB(alpha=3)\n\n# A wrapper for the sklearn confusion matrix\ndef confusion_matrix(actual_labels, predicted_labels, classes=['0', '1']):\n    return pd.DataFrame(\n        sm.confusion_matrix(actual_labels, predicted_labels),\n        index=['Actual ' + classes[0], 'Actual ' + classes[1]],\n        columns=classes)\n\n# Test Model using standard accuracy measures\n# Returns F1 score.  Only prints if Title is not empty\ndef testModel(PredictiveModel, XTest, yTest, Title=''):\n    predicted_probabilities = PredictiveModel.predict_proba(XTest)\n    predicted_labels = (predicted_probabilities[:,1] > 0.5).astype('int')\n    cm = confusion_matrix(yTest, predicted_labels, classes=['0', '1'])\n    TN, FP, FN, TP = cm.values.flatten()\n    accuracy = (TN + TP)\/(TN + FP + FN + TP)\n    recall = TP\/(TP + FN)\n    precision = TP\/(TP + FP)\n    f1 = sm.f1_score(yTest, predicted_labels)\n    auc = sm.roc_auc_score(yTest, predicted_probabilities[:,1])\n#     if len(Title) > 0:\n    print( \n              'accuracy: %4.2F' % accuracy, \n              'recall: %4.2F' % recall, \n              'precision: %4.2F' % precision, \n              'F1: %4.2F' % f1, \n              'AUC: %4.2F' % auc)\n    print(cm, '\\n----------')\n    return f1","5388f00e":"# BELOW CODE ERRORS -> consider commenting out Confusion Matrix code","52476552":"# f1 = []\n# ListOfNumberOfRows = [25, 50, 100, 500, 1000, 2000, 5087]\n# for NumberOfRows in ListOfNumberOfRows:\n#     NumberOfRows = min(NumberOfRows, y_train.shape[0])\n#     # Train the model with training data\n#     modelNB.fit(X_train[:NumberOfRows,:], y_train[:NumberOfRows])\n#     # Test the model with training data\n#     #f1_value = testModel(modelNB, X_train[:NumberOfRows,:], y_train[:NumberOfRows], X_train[:NumberOfRows,:].shape)\n#     f1_value = testModel(modelNB, X_train[:NumberOfRows,:], y_train[:NumberOfRows])\n#     f1.append(f1_value)\n#     if (NumberOfRows >= y_train.shape[0]):\n#         break\n        \n# # Plot F1 vs Row Number\n# plt.plot(np.log(ListOfNumberOfRows), f1)\n# plt.ylim(bottom = 0)\n# plt.ylabel('F1')\n# plt.xlabel('log(# Rows)')\n# plt.xticks(fontsize = 20)\n# plt.yticks(fontsize = 20)\n# plt.show()","d3213063":"f1 = []\nListOfNumberOfRows = [25, 50, 100, 500, 1000, 2000, 5087]\nfor NumberOfRows in ListOfNumberOfRows:\n    NumberOfRows = min(NumberOfRows, y_train.shape[0])\n    # Train the model with training data\n    modelNB.fit(X_train[:NumberOfRows,:], y_train[:NumberOfRows])\n    # Test the model with testing data\n    #f1_value = testModel(modelNB, X_test, y_test, X_train[:NumberOfRows,:].shape)\n    f1_value = testModel(modelNB, X_test, y_test)\n    f1.append(f1_value)\n    if (NumberOfRows >= y_train.shape[0]):\n        break\n        \n# Plot F1 vs Row Number\nplt.plot(np.log(ListOfNumberOfRows), f1)\nplt.xticks(fontsize = 20)\nplt.yticks(fontsize = 20)\nplt.ylim(bottom = 0)\nplt.ylabel('F1', fontsize = 25)\nplt.xlabel('log(# Rows)', fontsize = 25)\nplt.show()","69dec2ad":"f1 = []\nListOfNumberOfFeatures = [2, 10, 100, 200, 500, 1000, 1272]\nfor NumberOfFeatures in ListOfNumberOfFeatures:\n    NumberOfFeatures = min(NumberOfFeatures, X_train.shape[1])\n    # Train the model with training data\n    modelNB.fit(X_train[:,:NumberOfFeatures], y_train)\n    # Test the model with training data\n    #f1_value = testModel(modelNB, X_train[:,:NumberOfFeatures], y_train, X_train[:,:NumberOfFeatures].shape)\n    f1_value = testModel(modelNB, X_train[:,:NumberOfFeatures], y_train)\n    f1.append(f1_value)\n    if (NumberOfFeatures >= X_train.shape[1]):\n        break\n        \n# Plot F1 vs Number of Features\nplt.plot(np.log(ListOfNumberOfFeatures), f1)\nplt.xticks(fontsize = 20)\nplt.yticks(fontsize = 20)\nplt.ylim(bottom = 0)\nplt.ylabel('F1', fontsize = 25)\nplt.xlabel('log(# Features)', fontsize = 25)\nplt.show()","e21c692e":"f1 = []\nListOfNumberOfFeatures = [2, 10, 100, 200, 500, 1000, 1272]\nfor NumberOfFeatures in ListOfNumberOfFeatures:\n    NumberOfFeatures = min(NumberOfFeatures, X_train.shape[1])\n    # Train the model with training data\n    modelNB.fit(X_train[:,:NumberOfFeatures], y_train)\n    # Test the model with test data\n    #f1_value = testModel(modelNB, X_test[:,:NumberOfFeatures], y_test, X_train[:,:NumberOfFeatures].shape)\n    f1_value = testModel(modelNB, X_test[:,:NumberOfFeatures], y_test)\n    f1.append(f1_value)\n    if (NumberOfFeatures >= X_train.shape[1]):\n        break\n        \n# Plot F1 vs Number of Features\nplt.plot(np.log(ListOfNumberOfFeatures), f1)\nplt.xticks(fontsize = 20)\nplt.yticks(fontsize = 20)\nplt.ylim(bottom = 0)\nplt.ylabel('F1', fontsize = 25)\nplt.xlabel('log(# Features)', fontsize = 25)\nplt.show()","35fccece":"# Abstract \n\nThe Kepler space telescope is a retired space telescope launched by NASA to discover Earth-size planets orbiting other stars. Designed to survey a portion of Earth's region of the Milky Way to discover Earth-size exoplanets in or near habitable zones and estimate how many of the billions of stars in the Milky Way have such planets, Kepler's sole scientific instrument is a photometer that continually monitored the brightness of approximately 150,000 main sequence stars in a fixed field of view. These data are transmitted to Earth, then analyzed to detect periodic dimming caused by exoplanets that cross in front of their host star. Only planets whose orbits are seen edge-on from Earth can be detected. During its over nine and a half years of service, Kepler observed 530,506 stars and detected 2,662 planets. The data presented here are cleaned and are derived from observations made in late summer 2016.\n\nProblem - Can a machine learning classifier be created to detect exoplanets? The data describe the change in flux (light intensity) of several thousand stars observed by the Kepler space telescope. Each star has a binary label of 1 or 2. The label 2 indicates that that the star is confirmed to have at least one exoplanet in orbit; some observations are multi-planet systems.\n\nPlanets themselves do not emit light, but the stars that they orbit do. If said star is observed over several months or years, there may be a regular 'dimming' of the flux (the light intensity), this is called 'transiting' and the absolute percentage of light being dimmed is astronomically small - on the order of a measuring the decrease in flux from a lighthouse when a fly transits, observed from many miles away. This flux dimming is evidence that there may be an orbiting body around the star; such a star could be considered to be a 'candidate' system. Further study of our candidate system, for example by a satellite that captures light at a different wavelength, could solidify the belief that the candidate can in fact be 'confirmed' for having exoplanents.\n\nThe dataset is highly unbalanced, so a fully automated analysis will be quite difficult. Caution needs to be exercised since there are many astronomical features that can affect measured flux, including instrument noise and rotating spots. It is naive to conclude that any dimming in a star is caused by a transiting exoplanet, the dimming needs to be periodic with the transit duration.\n\nTrainset:\n\n5087 rows or observations. 3198 columns or features. Column 1 is the label vector. Columns 2 - 3198 are the flux values over time. 37 confirmed exoplanet-stars and 5050 non-exoplanet-stars.\n\nTestset:\n\n570 rows or observations. 3198 columns or features. Column 1 is the label vector. Columns 2 - 3198 are the flux values over time. 5 confirmed exoplanet-stars and 565 non-exoplanet-stars. For my analysis I will drop the index column so that the there are only 3197 features\/observations in the beginning.\n\nI would like to cite the following kernel on Kaggle for the source of some of this code: https:\/\/www.kaggle.com\/muonneutrino\/exoplanet-data-visualization-and-exploration\n\n## Section 1 - Can a machine learning classifier be created to detect exoplanets?\n\nThe data describe the change in flux (light intensity) of several thousand stars observed by the Kepler space telescope. Each star has a binary label of 1 or 2. The label 2 indicates that that the star is confirmed to have at least one exoplanet in orbit; some observations are multi-planet systems.\n\nPlanets themselves do not emit light, but the stars that they orbit do. If said star is observed over several months or years, there may be a regular 'dimming' of the flux (the light intensity), this is called 'transiting' and the absolute percentage of light being dimmed is astronomically small - on the order of a measuring the decrease in flux from a lighthouse when a fly transits, observed from many miles away. This flux dimming is evidence that there may be an orbiting body around the star; such a star could be considered to be a 'candidate' system. Further study of our candidate system, for example by a satellite that captures light at a different wavelength, could solidify the belief that the candidate can in fact be 'confirmed' for having exoplanents.\n\nThe dataset is highly unbalanced, so a fully automated analysis will be quite difficult. Caution needs to be exercised since there are many astronomical features that can affect measured flux, including instrument noise and rotating spots.\n\nTrainset:\n\n5087 rows or observations.\n3198 columns or features.\nColumn 1 is the label vector. Columns 2 - 3198 are the flux values over time.\n37 confirmed exoplanet-stars and 5050 non-exoplanet-stars.\n\nTestset:\n\n570 rows or observations.\n3198 columns or features.\nColumn 1 is the label vector. Columns 2 - 3198 are the flux values over time.\n5 confirmed exoplanet-stars and 565 non-exoplanet-stars.\n","2130dfbc":"## Section 2 - Investigate differences between classes using feature engineered attribute \"mean_x\" via resampling methods.\nI want to investigate if there is a difference between the flux intensity of stars with detected exoplanets and stars with no detected exoplanets. My hypothesis is that it does not matter, but a distinction might show up in the data from instrument bias. Brighter stars are easier to get good signal\/noise ratios from and therefore easier to detect exoplanets. Depending on the error of the instruments or the amount of noise the data for dimmer stars might yield \"false negatives\". I do not know the characteristic frequencies of noise, and thus would not know know to distinguish between the likelihood of noise artifact to a true outlier in the data, so I will not remove any outliers. I believe that resampling many times will do a decent job of minimizing their leverage on the mean intensity. Despite this mission not detecting exoplanets around 5050 stars, they likely are present since current astronnomical best estimates assume conservatively that at least 50% of stars should have exoplanets, if not well over 90%. A known limitation of the transit method is that it only works for orbital plane happens to closely align with our own.\n\nThere has been postprocessing(https:\/\/arxiv.org\/pdf\/1001.0258.pdf) of the dataset, the notdetected category of stars have been normalized in an unknown way on top of the \"Science Pipeline\" used to effectively parallelize data processing and allow for \"robust elimination of false positives.\" As a result the not detected category has a beautiful normal distribution. There are some extreme outliers in the detected data, which is to be expected, because star size and intensity span numerous orders of magnitudes. Despite knowing the ultimate source of the data - I do not know how this dataset was ultimately \"synthesized\" from the raw data and thusly can only offer broad interpretations of the problem, whether or not the mean intensities of stars with confirmed earth like exoplanets can be distinguised from stars with no observed exo-earths transiting in observed plane of motion.","8589d016":"### Calculate PCR and Adj r_squared for 6 most significant PC","6fff3237":"There are many features in this dataset. We see both longer-scale structure and wildly different amounts of seemingly random noise. If you zoom in, a lot of noise looks sinusoidal. There are a lot of anomalous points, particularly at high intensity. The clearest exoplanet signatures are fairly regular downward dips in intensity. Other astronomical features are at play, including star type, presence of gas clouds, and more. Further work needs to be done to remove noise.\n\nThere are also upward pulses of increased intensity - not sure what this means. Instrument noise is a likely probability, as is interference from other sources. Less clear are some possible signals of an almost rectangular pulse in the overall light curve structure.","bc284c58":"### Intensity Time Series Visualizations\nFrom earlier, there are 37 exoplanets in the dataset. I plot all 37 exoplanet stars and an equal number of non-exoplanet stars","feb9509a":"# Milestone 4 Conclusion","30c34ab4":"### ANOVA Analysis","d79edeea":"Picking more sensible axes and look at the exoplanet stars vs non-exoplanet stars. Red = exoplanets, blue = non-exoplanets","00d8a1f5":"### Star with no exoplanets orbiting in plane of view\nNote how something causes dips in flux intensity in the latter quarter of the observations. Upon further analysis this was ruled to not be an exoplanet.","b9f321fa":"### Perform Logistic Regression on first 10 PC components via Pipeline","ed59383d":"### Star with exoplanet(s) - note flux dips periodically","9d3857b4":"### Does not fit the test data very well, around 1000 rows provides highest F1 score.","0d6c5842":"## Section 3 Conclusion","82413919":"The distributions are pretty similar except for Median Intensity, where the data look restricted to a limited number of values. The Skewness is also different, but that is expected because exoplanets are associated with dips in intensity. Without knowing NASAs data-processing pipeline it will be a little difficult to classify based solely on this data.","a98b2037":"#### Thus far the T test and the ANOVA test determine that the mean intensities are not different. However, the pvalue for the T test is close to the significance threshold of 0.05. The F statistic is below 1 and the associated p-value is quite large - so there are conflicting results thus far: T-test shows borderline significance, but 95% CI contains 0, and ANOVA strongly fails to reject null hypothesis.\n\nI believe that resampling of the data is needed for deeper analysis. I suspect that the second category having two fewer orders of magnitude worth of observations might be confounding the classical analysis.","989328d2":"The dataset is very unbalanced, so care needs to be taken with regards to interpreting the results of otherwise properly implemented and trained models - there appears to be severe over training. This classification dataset of timeseries star fluxes with exoplanets detected via the transit-method is does appear to be well-suited for machine learning classification, given a larger population of confirmed exoplanets to train any models on. From the first Milestone we learned that the data are suited for classification. In the second Milestone we learned from resampling methods on an engineered attribute that the flux difference between the binary classes is very significant across multiple resampling methods and Bayesian analysis. In the third Milestone we learned that PCR with logistic regression chained in a model yields wonderful accuracy but Adjusted_r_squared values of zero and very high AIC\/BIC for the 10 principle components that explained 95+% of the variance. And in Milestone 4 the Naive Bayes classifier model improved on the accuracy over PCR, but the F1 score was very low. The model is a slight improvement, but only a moderate one. \n\nFor future analysis, assuming I cannot locate more data for exoplanets to balance out the factor of 100 difference, I would bootstrap resample the confirmed exoplanet data to balance the binary classes, then I would pipe the data into a deep model comprised of many Artificial Neual Networks (ANN) to predict the binary classification. There are many models available for a data scientist and they all paint the best, most interpretable picture when combined as an ensemble. As the models increased in sophstication and complexity, the accuracy results improved, but accuracy is the worst of the classical stastic results. AUC is the best, but the highest values I obtained were around .6, which is pretty bad, only slightly above the worst possible value of .5. The F1 score, taking the harmonic mean of precision and recall (relating false positives and false negatives) demonstrate the general deficiencies of training models on such an unbalanced dataset. Further confounding analysis is the presence of many astronmicial phenomena that can obscure, mimic, ampify, or transform a decent signal into noise. I believe that given more exoplanet data to train on the F1 score and AUC of Naive Bayes, PCR with resampling can create a decent series of ensemble models that can have an F1 score approaching.9\/.95 and an AUC approaching .8\/.85. Currently, the models are massively overtrained on insufficient amounts of class data, but there is always a risk of overfitting even if the balance is accounted for. Nonetheless I believe these models can be improved to better detect and analyze such data as we continue to obtain ever more. Also, an interesting use of such models would be to analyze historical datasets from the last century and see if any results were missed!","7d052bbd":"### Bayesian Analysis of Means\n\nNote extreme outliers in detected stars, but will assume that data is normally distributed for prior distributions, like in plots for classical analysis above","448a2c26":"### Data Wrangling\nAll subsequent EDA will be done only on the test set.","c323bb7c":"### Feature engineer new attribute\nMeanflux is the mean of all 3197 flux observations for each of the 5087 observations.","e1f923fb":"### Arguably 6 or 9 PC explain most variance, but 10 is nice enough compared to original 3197","e3fa1e25":"## Author - James Godwin\n\n","a7b4d777":"### Compare against a traditional Logistic Regression model\nLogistic regression models are a good starting point for classification analysis. While this model might not be the best, it serves as a useful barometer of whether all the models are working and if there are trends and information to extract to classify exoplanets. I will compare the confusion matrices of the PCR and LR models at the end.","45a24522":"## Section 2 Conclusion\nAfter resampling the data many times, I confidently conclude that there is a significant difference in the means of the two categories. My assumptions about the prior distributions for Bayesian analysis might be off, hence why the difference in means is not the same as the bootstrapping one (the one in which I am most confident in my personal understanding and interpretation of the code). The classical tests did not reject the null hypothesis as the 95% CI contained 0 and the pvalues were not significant because they were larger than the signicance threshold of 0.05. \n\nMy suspicion that this was a misleading result was due to the fact that one category had over 100x as many observations. Thus I conducted my resampling analysis using both Bootstrapping and Bayesian analysis with posterior distributions. The 95% confidence intervals of the bootstrapped difference in means (itself bootstrapped from the means of the two categories) and the 95% credible intervals of the two posterior distributions do not contain 0, and in the case of the posterior distributions, do not overlap. Therefore I conclude that these resampling results are more valid and indicative of the true nature of the problem instead of the classically obtained results and confidence intervals because the resampling removed the huge (100x) population imbalance in the categories.\n\nI cannot interpret what the difference in the mean flux intensities means, but I suspect it has to do with the bias of the types of stars the mission looked at and the specific sensitivities of the many instruments. Thus for future analysis a machine-learning model can partly be trained to look at the characteristc mean flux intensities of stars to give a good first impression of viable candidates for exoplanets vs flux dimmings caused by a myriad of other astronomical \"noise\" unrelated to transiting exoplanets (e.g., comets, dust, nebulas, and even gravitational lensing).","e782279a":"Exoplanet data and non-exoplanet data were processed slightly differently. The median for non-exoplanet data seems to have been defined to be zero.","539bae39":"Despite the Pipeline(PCA\/LR) model having 100% accuracy on the test data, it is far from a good model. There is a massive imbalance between the classes and thus I do not think this model will generalize well, despite technically being better than the logistic regression results; as a sanity check I performed a standard logistic regression analysis on the data and got a classification accuracy of 98%, which is the same as the 100% one for the Pipeline model.\n\nPCA is a popular approach for deriving a low-dimensional set of features from a large one (3197). After performing PCR on this classification dataset, there are no components that are signficant, as the 10 pvalues of the largest PCA \"components\" that explained over 95% of the variance, end up not explaining much in the regression. The summary results threw an error suggesting that some of the data might be correlated, but I do not see how that is possible since all these stars are distinct. I suspect that some sort of \"resonance\" might have been detected with certain stars having flux peaks\/dips at similar intervals.\n\nAccording to the scree plot, most of the variance in the linear regression model is explained by 6 or 9 components. These components have the largest explained variance of the dataset, and I chose to use the 10\/6 largest and analyzed them both separately. The results were the same - an Adj r_squared of 0! Large AICs of ~2200. Stars come in so many sizes, have so may types (I do not know if Cepheid Variable stars were included in this dataset or not) and since the signal to noise ratio is probably to the best due to instrument errors, I am not surprised that linear combinations of stars do not predict exoplants. Exoplanets form at different sizes, densities, and distance from the parent star and as a result it is not unexpected that linear combinations of thousands of exosolar systems is little distinguishable from noise. \n\n10 component PCR model: The AIC was large and the Adj-r_squared zero. The y-intercept is one. The 10 most significant components all have p-values that are all larger than the significance threshold of 0.05 and their 95% CI's do contain zero. The coefficients of the PCR all have approximately the same size (0). The residuals are not normally distributed, implying that more analysis could be done, but I do not think it would come close to making this model even slightly useful. \n\n6 component PCR model: The AIC was large and the Adj-r_squared zero. The y-intercept is one. The 10 most significant components all have p-values that are all larger than the significance threshold of 0.05 and their 95% CI's do contain zero. The coefficients of the PCR all have approximately the same size (0). The residuals are not normally distributed, implying that more analysis could be done, but I do not think it would come close to making this model even slightly useful. \n\nMy conclusion is that these PCR models are terrible for fitting the given data. Nevertheless, they give you a set of results that can be used to compare with other models such as using neural networks. PCA is just one of the tools in machine learning. I suspect neural networks might be well suited for this analysis.","c8db7319":"### T Test \nThe 95% CI for the Mean Flux of the two categories of stars contains 0. Also the T Test cannot reject the null hypothesis because the pvalue is larger than .05, but barely so. The difference in means could be significant but requires further analysis.","37bcf4c4":"The exoplanets are located at the beginning of the dataset. Will drop label for now for more EDA","2db59726":"### F1 peak around ln(7)","2b1cb237":"### More comparision analysis of the means of two flux intensity categories (exoplanets detected vs not detected)\nI separate the data based on the categorical value in its index. The stars with exoplanets will be in detected and stars with no observed exoplanets will be in notdetected. I then will compare the means of the flux intensities of the two groups and plot their associated 95% CI's at 0.05 significance.","c897d53a":"## Section 4 Conclusion\n\nMy regression model for Milestone 3 was very accurate - it was at least 98% accurate, occasionaly being perfectly accurate. This does not make it a good model because the dataset is tremendously unbalanced as there are two orders of magnitudes fewer records with exoplanets as there are random flux changes not caused by exoplanets. I did not expect the Naive Bayes model to be as accurate, but the code failed silently and did not output the confusion matrix or accuracy score. The F1 score, a much better metric for 'accuracy' than accuracy is generally better for more rows, up to a limit. This makes sense as the nature of the unbalanced dataset requires as many instances of the exoplanet class to give the model the best chance to make the right classification. For small row numbers, F1 is incredibly small and variable. The F1 score for my previous Milestone 3 project has not been calculated, but since the Adj_r_squared was 0, I am going to extrapolate an equally low F1 score. Therefore the Naive Bayes model, while not great, does perform a modicum better than the one in my Milestone 3.\n\nThe Naive Bayes classifier is a good classifier to use for it is probabilistic and works only with categorical data - of which my dataset is: a classification dataset. I converted the data to categories to be used for the analysis. The code did not output the traditional statistics, so I can only go off of the F1 score, which is not terrible for the training data. However, there is a significantly lower F1 score for the test data than the train data, indicating the Naive Bayes model was overfitted, and does not generalize well. This makes sense as many of the features in the non-exoplanet category can be caused by what amounts to random noise. The model might be detecting \"features\" in the dataset that are really artifacts of mechanisms other than exoplanets causing the dips in flux intensity.","65303863":"### Bayesian Analysis\nSince the 95% credible intervals of the two means do not overlap, I can confidently say that the posterior distributions of the means of the two categories are infact different. As noted earlier, the stars with no exoplanets detected appear to have been normalized in postprocessing, so I am not sure as to how that specifically affects the analysis - notwithstanding, the Bayes Posterior distribution 95% credible intervals do not overlap, I am confident that this resampling method reaches the same conclusion as the bootstrapping analysis from above - that the difference in mean flux intensities of stars is significant and that they are different. The Posterior distribution of the notdetected stars also shows the postprocess normalization of the data as mean flux intensities are close to zero.","32bda09f":"# Section 3 - PCR as a classifier\nI want to investigate if PCR is a good model for classifiying my exoplanet dataset. My hypothesis is that, like with logistic regression, a very accurate model can be made, but it will have a terribly small Adj r_2 and very large AIC values, meaning that the model will not provide much value over a simple logistic regression model. \n\nI will perform PCA on all 3197 flux measurements and determine the most signficant ones, then I will pipe them into a logistic regression\/PCR model to determine the accuracy and then calculate the model's Adj r_squared and AIC values. For a sanity check I will compare these results against a traditional logistic regression model and see I can improve on ~97% accuracy.","edeaa902":"## Plotting Flux Diagrams\n\nIn the following diagram, a star is orbited by an exoplanet. Note the starlight intensity drops because it is partially obscured by the exoplanet, given our position. The starlight rises back to its original value (normalized to 0). The graph in each box shows the measured flux (light intensity) at each time interval.","74fb8743":"### Bootstrap meanflux\nWhile both 95% CI's contain 0, they barely do and it looks like they have a significant difference in means.","262133f9":"### Function to import dataset from Google Drive\n\nRequired to get around 'download_warning' for large files","b87d7a7d":"### The first 10 PC explain ~95% of the variance in the data\nThe remaining 3187 PCA explain vanishingly small amounts of variance so as to be easily ignored","729c6731":"## Section 1 Conclusion\n\nIt seems quite possible to be able to build a machine classifier for exoplanet detection. There are many unique features in light curves, but the most easily recognizabe exoplanet signatures are regular downward dips in intensity during exoplanet transit. There are many plots with outliers, especially at high flux ranges so these outliers will need to be removed for better\/smoother classificiation.\n\nFurther analysis needs to be done on noise removal as most plots appear to have transient outlier features that appear to be unrelated to the observed stars.","c3a41f16":"### Bootstrap difference in means","f51bed5c":"# Section 4 Problem - Naive Bayes as a classifier","74c2d590":"### Calculate Adjusted r_squared from PCR model with top 10 PC","8ffa70d2":"### Analysis of Bootstrapping\nI confidently reject the null hypothesis that the means are the same, or that they are drawn from the same population with no distinction made for categorical difference. The 95%CI does not contain 0 and the mean difference is ~180. Therefore I conclude that the two populations have distinct mean fluxes.","b23e4529":"### Peak F1 at around 1000 records"}}