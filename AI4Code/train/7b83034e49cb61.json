{"cell_type":{"ec95f29b":"code","17eee9f3":"code","77138fb1":"code","bf3d033c":"code","04341b1b":"code","ee71ae71":"code","7457b785":"code","c7ed9343":"code","ca9d8a3b":"code","2f0d7914":"code","9d823f38":"code","7ec6fa3f":"code","280ff263":"code","bd3c5031":"code","d3138cfd":"code","42fa8e62":"code","a2f8776f":"code","c5269d2b":"code","017fe85f":"code","f584665c":"code","5bc07615":"code","c911e07d":"code","c3868da1":"code","4ece2eff":"code","f6fc5bd9":"code","14663060":"markdown","867f9215":"markdown","6456d6ea":"markdown","8ede82dc":"markdown","67021c88":"markdown","f3b43bfe":"markdown"},"source":{"ec95f29b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport operator\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# import warnings\nimport warnings\n# ignore warnings\nwarnings.filterwarnings(\"ignore\")\nfrom subprocess import check_output\nfrom IPython.display import Markdown as md\nfrom prettytable import PrettyTable\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom numpy import sqrt\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","17eee9f3":"data = pd.read_csv('..\/input\/vertebralcolumndataset\/column_2C.csv')","77138fb1":"# to see features and target variable\ndata.head()","bf3d033c":"# Well know question is is there any NaN value and length of this data so lets look at info\ndata.info()","04341b1b":"data.describe()","ee71ae71":"color_list = ['red' if i=='Abnormal' else 'green' for i in data.loc[:,'class']]\npd.plotting.scatter_matrix(data.loc[:, data.columns != 'class'],\n                                       c=color_list,\n                                       figsize= [15,15],\n                                       diagonal='hist',\n                                       alpha=0.5,\n                                       s = 200,\n                                       marker = '*',\n                                       edgecolor= \"black\")\nplt.show()","7457b785":"sns.countplot(x=\"class\", data=data)\ndata.loc[:,'class'].value_counts()","c7ed9343":"# create data1 that includes pelvic_incidence that is feature and sacral_slope that is target variable\ndata1 = data[data['class'] =='Abnormal']\nx = np.array(data1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(data1.loc[:,'sacral_slope']).reshape(-1,1)\n# Scatter\nplt.figure(figsize=[10,10])\nplt.scatter(x=x,y=y)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.show()","ca9d8a3b":"# LinearRegression\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\npredict_space = np.linspace(min(x), max(x)).reshape(-1,1)\n# Fit\nreg.fit(x,y)\n# Predict\npredicted = reg.predict(predict_space)\n# # R^2 \n# print('R^2 score: ',reg.score(x, y))\ndegree = 1\n\nx = np.array(data1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(data1.loc[:,'sacral_slope']).reshape(-1,1)\npolynomial_features= PolynomialFeatures(degree=degree)\nx_poly = polynomial_features.fit_transform(x)\n\nmodel = LinearRegression()\nmodel.fit(x_poly, y)\ny_poly_pred = model.predict(x_poly)\n\nrmse = sqrt(mean_squared_error(y,y_poly_pred))\nr2 = r2_score(y,y_poly_pred)\n\n\n# tabel parameter\ntabel_parameter = PrettyTable(['parameter', 'nilai'])\ntabel_parameter.add_row(['Polynomial\\nDegree', degree])\ntabel_parameter.add_row(['RMSE','{:.10}'.format(rmse)])\ntabel_parameter.add_row(['R^2', '{:.10}'.format(r2)])\n\n# Plot regression line and scatter\nplt.plot(predict_space, predicted, color='black', linewidth=3)\nplt.scatter(x=x,y=y)\nplt.title('Biomechanical features of orthopedic patients\\n')\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.show()\nprint(tabel_parameter)","2f0d7914":"def display_equation(model):\n    for coef in model.coef_:\n        pass\n    streq = \"$y = \" +str(model.intercept_[0])\n    for i,c in enumerate(coef):\n        j = len(coef)-i-1\n        if abs(c) > c:\n            sign = \"-\"\n        else:\n            sign = \"+\"\n        if i > 1:\n            streq += sign +str(abs(c)) + \" \\cdot x^{\"+str(i)+\"}\"\n        elif i == 1:\n            streq += sign +str(abs(c)) + \" \\cdot x\"\n\n    streq =   streq + \"$\"\n    return md(streq)\ndisplay_equation(model)","9d823f38":"#Polynomial\ndegree = 10\n\nx = np.array(data1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(data1.loc[:,'sacral_slope']).reshape(-1,1)\npolynomial_features= PolynomialFeatures(degree=degree)\nx_poly = polynomial_features.fit_transform(x)\n\nmodel = LinearRegression()\nmodel.fit(x_poly, y)\ny_poly_pred = model.predict(x_poly)\n\nrmse = sqrt(mean_squared_error(y,y_poly_pred))\nr2 = r2_score(y,y_poly_pred)","7ec6fa3f":"# tabel parameter\ntabel_parameter = PrettyTable(['parameter', 'nilai'])\ntabel_parameter.add_row(['Polynomial\\nDegree', degree])\ntabel_parameter.add_row(['RMSE','{:.10}'.format(rmse)])\ntabel_parameter.add_row(['R^2', '{:.10}'.format(r2)])","280ff263":"plt.scatter(x, y, s=10)\nsort_axis = operator.itemgetter(0)\nsorted_zip = sorted(zip(x,y_poly_pred), key=sort_axis)\nx, y_poly_pred = zip(*sorted_zip)\nplt.title('Biomechanical features of orthopedic patients\\n')\nplt.xlabel('\\npelvic_incidence')\nplt.ylabel('sacral_slope')\n\nplt.plot(x, y_poly_pred, color='m')\nplt.show()\nprint(tabel_parameter)","bd3c5031":"def display_equation(model):\n    for coef in model.coef_:\n        pass\n    streq = \"$y = \" +str(model.intercept_[0])\n    for i,c in enumerate(coef):\n        j = len(coef)-i-1\n        if abs(c) > c:\n            sign = \"-\"\n        else:\n            sign = \"+\"\n        if i > 1:\n            streq += sign +str(abs(c)) + \" \\cdot x^{\"+str(i)+\"}\"\n        elif i == 1:\n            streq += sign +str(abs(c)) + \" \\cdot x\"\n\n    streq =   streq + \"$\"\n    return md(streq)\ndisplay_equation(model)","d3138cfd":"# As you can see there is no labels in data\nplt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'])\nplt.xlabel('pelvic_radius')\nplt.ylabel('degree_spondylolisthesis')\nplt.show()","42fa8e62":"# KMeans Clustering\ndata2 = data.loc[:,['degree_spondylolisthesis','pelvic_radius']]\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters = 2)\nkmeans.fit(data2)\nlabels = kmeans.predict(data2)\nplt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'],c = labels)\nplt.xlabel('pelvic_radius')\nplt.ylabel('degree_spondylolisthesis')\nplt.show()","a2f8776f":"# cross tabulation table\ndf = pd.DataFrame({'labels':labels,\"class\":data['class']})\nct = pd.crosstab(df['labels'],df['class'])\nprint(ct)","c5269d2b":"# inertia\ninertia_list = np.empty(8)\nfor i in range(1,8):\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(data2)\n    inertia_list[i] = kmeans.inertia_\nplt.plot(range(0,8),inertia_list,'-o')\nplt.xlabel('Number of cluster')\nplt.ylabel('Inertia')\nplt.show()\nkmeans.inertia_","017fe85f":"K_clusters = range(1,10)\nkmeans = [KMeans(n_clusters=i) for i in K_clusters]\n\nX_axis = np.array(data1.loc[:,'pelvic_incidence']).reshape(-1,1)\nY_axis = np.array(data1.loc[:,'sacral_slope']).reshape(-1,1)\n\nscore = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]\n# Visualize\nplt.plot(K_clusters, score)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.title('Elbow Curve')\nplt.show()","f584665c":"from sklearn.datasets.samples_generator import make_blobs\nX, y_true = make_blobs(n_samples=300, centers=3,cluster_std=0.60, shuffle=True, random_state=0)\nplt.scatter(X[:, 0], X[:, 1], edgecolor='blue', s=50);","5bc07615":"from sklearn.cluster import KMeans\n\nkm = KMeans(\n    n_clusters=3, init='random',\n    n_init=10, max_iter=300, \n    tol=1e-04, random_state=0\n)\ny_km = km.fit_predict(X)","c911e07d":"# plot the 3 clusters\nplt.scatter(\n    X[y_km == 0, 0], X[y_km == 0, 1],\n    s=50, c='lightgreen',\n    marker='s', edgecolor='black',\n    label='cluster 1'\n)\n\nplt.scatter(\n    X[y_km == 1, 0], X[y_km == 1, 1],\n    s=50, c='orange',\n    marker='o', edgecolor='black',\n    label='cluster 2'\n)\n\nplt.scatter(\n    X[y_km == 2, 0], X[y_km == 2, 1],\n    s=50, c='lightblue',\n    marker='v', edgecolor='black',\n    label='cluster 3'\n)\n\n# plot the centroids\nplt.scatter(\n    km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],\n    s=250, marker='*',\n    c='red', edgecolor='black',\n    label='centroids'\n)\nplt.legend(scatterpoints=1)\nplt.grid()\nplt.show()","c3868da1":"# calculate distortion for a range of number of cluster\ndistortions = []\nfor i in range(1, 11):\n    km = KMeans(\n        n_clusters=i, init='random',\n        n_init=10, max_iter=300,\n        tol=1e-04, random_state=0\n    )\n    km.fit(X)\n    distortions.append(km.inertia_)\n\n# plot\nplt.plot(range(1, 11), distortions, marker='o')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.show()","4ece2eff":"from yellowbrick.cluster import SilhouetteVisualizer\nfrom sklearn.cluster import KMeans\nmodel = SilhouetteVisualizer(KMeans(n_clusters=3))\nmodel.fit(X)\nmodel.show()","f6fc5bd9":"from yellowbrick.cluster import SilhouetteVisualizer\n\nfig, ax = plt.subplots(2, 2, figsize=(15,8))\nfor i in [2,3,4,5]:\n    '''\n    Create KMeans instance for different number of clusters\n    '''\n    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)\n    q, mod = divmod(i, 2)\n    '''\n    Create SilhouetteVisualizer instance with KMeans instance\n    Fit the visualizer\n    '''\n    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])\n    visualizer.fit(X)","14663060":"Here is the code to create a Silhouette plot for K-Means clusters with n_cluster as 2, 3, 4, 5.","867f9215":"Check optimal number of clusters using Elbow Curve analysis","6456d6ea":"Silhouette Plot of KMeans","8ede82dc":"Model Polynomial Regression (Degree = 2)","67021c88":"KMEANS CLUSTERING","f3b43bfe":"> ******REGRESSION**"}}