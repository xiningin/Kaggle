{"cell_type":{"47d0cc0e":"code","517cd2d3":"code","a007f812":"code","cf14cc69":"code","0dad1f37":"code","dcc96015":"code","b0a1997a":"code","2a68a829":"code","38d06fe4":"code","8d5cc797":"code","0f3af662":"code","607b4b96":"code","3f7772bc":"code","a31d3b89":"code","b2b75a8c":"code","5d1fc8d5":"code","ff80b7ab":"code","88327af8":"code","b0ac2601":"code","5eccdd57":"code","154cd9cd":"code","189087d3":"code","05ec1318":"code","e46c78b1":"code","3aef42e9":"code","a8799cf1":"code","ebf99d2c":"code","5cee5445":"code","6c35e17d":"code","93bd9699":"code","1355dbbf":"code","24eb07a7":"code","349eb42d":"code","28d391f0":"code","1473b2dd":"code","fa8913d3":"code","d53b0afb":"code","099a90d5":"code","c26a2b7f":"code","0ef43786":"code","7c50eab2":"code","25ede614":"code","8cf8424c":"code","37cc59c7":"code","24746e56":"code","0537a762":"code","5cb71aa2":"code","0dba1185":"code","26d056fc":"code","96bcdfe2":"code","e54a33e7":"code","25c42eb0":"code","b2213a37":"code","fe972cc5":"code","5f00b8d1":"code","bfa65d4b":"code","665b5e89":"code","e5b0a2f3":"code","bb089427":"code","65459bc7":"code","373c068c":"code","4576be28":"code","cbb29964":"code","62b5d13e":"code","bd141aaf":"code","f715d7da":"code","fea114b6":"code","c23dedd7":"code","3b0f003e":"code","989d0c53":"code","043df9f4":"markdown","e5830f21":"markdown"},"source":{"47d0cc0e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","517cd2d3":"train = pd.read_csv('\/kaggle\/input\/datasetamazonmobrev2\/trainRevFin.csv')\ntest = pd.read_csv('\/kaggle\/input\/datasetamazonmobrev2\/testRevFin.csv')","a007f812":"import re\nimport random\nimport unicodedata\nimport autocorrect\n\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nenglish_stemmer=nltk.stem.SnowballStemmer('english')\n\nfrom sklearn.feature_selection.univariate_selection import SelectKBest, chi2, f_classif\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report, roc_auc_score, balanced_accuracy_score\nimport random\nimport itertools\n\nimport sys\nimport os\nimport argparse\nfrom sklearn.pipeline import Pipeline\nfrom scipy.sparse import csr_matrix\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport six\nfrom abc import ABCMeta\nfrom scipy import sparse\nfrom scipy.sparse import issparse\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils import check_X_y, check_array\nfrom sklearn.utils.extmath import safe_sparse_dot\nfrom sklearn.preprocessing import normalize, binarize, LabelBinarizer\nfrom sklearn.svm import LinearSVC\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n%matplotlib inline\nplt.style.use('ggplot')","cf14cc69":"pip install pyspellchecker","0dad1f37":"from spellchecker import SpellChecker \n\ndef review_to_wordlist( review, remove_stopwords=True):\n    # Function to convert a document to a sequence of words,\n    # optionally removing stop words.  Returns a list of words.\n    #\n    # 1. Remove HTML\n    review_text = BeautifulSoup(review).get_text()\n\n    #\n    # 2. Remove non-letters\n    review_text = re.sub(\"[^a-zA-Z]\",\" \", review)\n    #\n    # 3. Convert words to lower case and split them\n    words = review_text.lower().split()\n    \n    spell = SpellChecker()\n    misspelled = spell.unknown(words)\n    \n    for correctWord in misspelled:\n        words = spell.correction(correctWord)\n        return words\n    #\n    # 4. Optionally remove stop words (True by default)\n    if remove_stopwords:\n        Stop_Words = set(stopwords.words(\"english\"))\n        Stop_Words.update(('mobile', 'phones', 'camera', 'phone', 'cell phone', 'cell'))\n        stops = set(Stop_Words)\n        words = [w for w in words if not w in stops]\n\n    b=[]\n    stemmer = english_stemmer #PorterStemmer()\n    \n    for word in words:\n        c = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n        b.append(c)\n        b.append(stemmer.stem(word))\n\n    # 5. Return a list of words\n    return(b)","dcc96015":"random.seed(13082020)","b0a1997a":"train.shape","2a68a829":"train","38d06fe4":"test.shape","8d5cc797":"test","0f3af662":"# data = total[total['Reviews'].isnull()==False]\n# data.shape","607b4b96":"# train, test  = train_test_split(total, test_size= 0.3)","3f7772bc":"# sns.countplot(data['Rating'])","a31d3b89":"clean_train_reviews = []\nfor review in train['Reviews']:\n    clean_train_reviews.append( \" \".join(review_to_wordlist(review)))\n    \nclean_test_reviews = []\nfor review in test['Reviews']:\n    clean_test_reviews.append( \" \".join(review_to_wordlist(review)))","b2b75a8c":"vectorizer = TfidfVectorizer( min_df=2, max_df=0.95, max_features = 200391, ngram_range = (2, 2),\n                              sublinear_tf = True )\n\nvectorizer = vectorizer.fit(clean_train_reviews)\ntrain_features = vectorizer.transform(clean_train_reviews)","5d1fc8d5":"test_features = vectorizer.transform(clean_test_reviews)","ff80b7ab":"fselect = SelectKBest(chi2 , k=15000)\ntrain_features = fselect.fit_transform(train_features, train[\"Rating\"])\ntest_features = fselect.transform(test_features)","88327af8":"model1 = MultinomialNB(alpha=0.001)\nmodel1.fit( train_features, train[\"Rating\"] )","b0ac2601":"pred_1 = model1.predict( test_features.toarray() )","5eccdd57":"model2 = SGDClassifier(loss='modified_huber', n_iter_no_change=5, random_state=0, shuffle=True)\nmodel2.fit( train_features, train[\"Rating\"] )","154cd9cd":"pred_2 = model2.predict( test_features.toarray() )","189087d3":"model3 = RandomForestClassifier()\nmodel3.fit( train_features, train[\"Rating\"] )","05ec1318":"pred_3 = model3.predict( test_features.toarray() )","e46c78b1":"model4 = GradientBoostingClassifier()\nmodel4.fit( train_features, train[\"Rating\"] )","3aef42e9":"pred_4 = model4.predict( test_features.toarray() )","a8799cf1":"class NBSVM(six.with_metaclass(ABCMeta, BaseEstimator, ClassifierMixin)):\n\n    def __init__(self, alpha=1.0, C=1.0, max_iter=10000):\n        self.alpha = alpha\n        self.max_iter = max_iter\n        self.C = C\n        self.svm_ = [] # fuggly\n\n    def fit(self, X, y):\n        X, y = check_X_y(X, y, 'csr')\n        _, n_features = X.shape\n\n        labelbin = LabelBinarizer()\n        Y = labelbin.fit_transform(y)\n        self.classes_ = labelbin.classes_\n        if Y.shape[1] == 1:\n            Y = np.concatenate((1 - Y, Y), axis=1)\n\n        # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.\n        # so we don't have to cast X to floating point\n        Y = Y.astype(np.float64)\n\n        # Count raw events from data\n        n_effective_classes = Y.shape[1]\n        self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\n        self.ratios_ = np.full((n_effective_classes, n_features), self.alpha,\n                                 dtype=np.float64)\n        self._compute_ratios(X, Y)\n\n        # flugglyness\n        for i in range(n_effective_classes):\n            X_i = X.multiply(self.ratios_[i])\n            svm = LinearSVC(C=self.C, max_iter=self.max_iter)\n            Y_i = Y[:,i]\n            svm.fit(X_i, Y_i)\n            self.svm_.append(svm) \n\n        return self\n\n    def predict(self, X):\n        n_effective_classes = self.class_count_.shape[0]\n        n_examples = X.shape[0]\n\n        D = np.zeros((n_effective_classes, n_examples))\n\n        for i in range(n_effective_classes):\n            X_i = X.multiply(self.ratios_[i])\n            D[i] = self.svm_[i].decision_function(X_i)\n        \n        return self.classes_[np.argmax(D, axis=0)]\n        \n    def _compute_ratios(self, X, Y):\n        \"\"\"Count feature occurrences and compute ratios.\"\"\"\n        if np.any((X.data if issparse(X) else X) < 0):\n            raise ValueError(\"Input X must be non-negative\")\n\n        self.ratios_ += safe_sparse_dot(Y.T, X)  # ratio + feature_occurrance_c\n        normalize(self.ratios_, norm='l1', axis=1, copy=False)\n        row_calc = lambda r: np.log(np.divide(r, (1 - r)))\n        self.ratios_ = np.apply_along_axis(row_calc, axis=1, arr=self.ratios_)\n        check_array(self.ratios_)\n        self.ratios_ = sparse.csr_matrix(self.ratios_)\n\n        #p_c \/= np.linalg.norm(p_c, ord=1)\n        #ratios[c] = np.log(p_c \/ (1 - p_c))\n\n\ndef f1_class(pred, truth, class_val):\n    n = len(truth)\n\n    truth_class = 0\n    pred_class = 0\n    tp = 0\n\n    for ii in range(0, n):\n        if truth[ii] == class_val:\n            truth_class += 1\n            if truth[ii] == pred[ii]:\n                tp += 1\n                pred_class += 1\n                continue;\n        if pred[ii] == class_val:\n            pred_class += 1\n\n    precision = tp \/ float(pred_class)\n    recall = tp \/ float(truth_class)\n\n    return (2.0 * precision * recall) \/ (precision + recall)\n\n\ndef semeval_senti_f1(pred, truth, pos=2, neg=0): \n\n    f1_pos = f1_class(pred, truth, pos)\n    f1_neg = f1_class(pred, truth, neg)\n\n    return (f1_pos + f1_neg) \/ 2.0;\n\n\ndef main(train_file, test_file, ngram=(1, 3)):\n    print('loading...')\n    train = pd.read_csv(train_file, delimiter='\\t', encoding='utf-8', header=0,\n                        names=['text', 'label'])\n\n    # to shuffle:\n    #train.iloc[np.random.permutation(len(df))]\n\n    test = pd.read_csv(test_file, delimiter='\\t', encoding='utf-8', header=0,\n                        names=['text', 'label'])\n\n    print('vectorizing...')\n    vect = CountVectorizer()\n    classifier = NBSVM()\n\n    # create pipeline\n    clf = Pipeline([('vect', vect), ('nbsvm', classifier)])\n    params = {\n        'vect__token_pattern': r\"\\S+\",\n        'vect__ngram_range': ngram, \n        'vect__binary': True\n    }\n    clf.set_params(**params)\n\n    #X_train = vect.fit_transform(train['text'])\n    #X_test = vect.transform(test['text'])\n\n    print('fitting...')\n    clf.fit(train['text'], train['label'])\n\n    print('classifying...')\n    pred = clf.predict(test['text'])\n   \n    print('testing...')\n    acc = accuracy_score(test['label'], pred)\n    f1 = semeval_senti_f1(pred, test['label'])\n    print('NBSVM: acc=%f, f1=%f' % (acc, f1))","ebf99d2c":"model5 = NBSVM(C=0.01)\nmodel5.fit( train_features, train[\"Rating\"] )","5cee5445":"pred_5 = model5.predict( test_features )","6c35e17d":"print(classification_report(test['Rating'], pred_3, target_names=['1','2','3','4','5'], digits=5))","93bd9699":"print(classification_report(test['Rating'], pred_5, target_names=['1','2','3','4','5'], digits=5))","1355dbbf":"model6 = LogisticRegression(C=0.01)\nmodel6.fit( train_features, train[\"Sentiment\"] )","24eb07a7":"pred_6 = model6.predict( test_features )","349eb42d":"print(classification_report(test['Rating'], pred_2, target_names=['1','2','3','4','5'], digits=5))\nprint(classification_report(test['Rating'], pred_3, target_names=['1','2','3','4','5'], digits=5))\nprint(classification_report(test['Rating'], pred_5, target_names=['1','2','3','4','5'], digits=5))","28d391f0":"print(classification_report(test['Sentiment'], pred_1, target_names=['Positive', 'Negative'], digits=5))\nprint(classification_report(test['Sentiment'], pred_2, target_names=['Positive', 'Negative'], digits=5))\nprint(classification_report(test['Sentiment'], pred_3, target_names=['Positive', 'Negative'], digits=5))\nprint(classification_report(test['Sentiment'], pred_4, target_names=['Positive', 'Negative'], digits=5))\nprint(classification_report(test['Sentiment'], pred_5, target_names=['Positive', 'Negative'], digits=5))\nprint(classification_report(test['Sentiment'], pred_6, target_names=['Positive', 'Negative'], digits=5))","1473b2dd":"print('Multinomial Na\u00efve-Bayes model accuracy: ', balanced_accuracy_score(test['Sentiment'], pred_1))\nprint('SGD Classifier accuracy: ', balanced_accuracy_score(test['Sentiment'], pred_2))\nprint('Random Forst Classifier accuracy: ', balanced_accuracy_score(test['Sentiment'], pred_3))\nprint('Gradient Boosting Classifier accuracy: ', balanced_accuracy_score(test['Sentiment'], pred_4))\nprint('Na\u00efve-Bayes SVM model accuracy: ', balanced_accuracy_score(test['Sentiment'], pred_5))\nprint('Logistic Regression model accuracy: ', balanced_accuracy_score(test['Sentiment'], pred_6))","fa8913d3":"from textblob import TextBlob","d53b0afb":"polarity = lambda x: TextBlob(x).sentiment.polarity\nsubjectivity = lambda x: TextBlob(x).sentiment.subjectivity","099a90d5":"train1 = train","c26a2b7f":"test1 = test","0ef43786":"train1['polarity'] = train['Reviews'].apply(polarity)\ntrain1['subjectivity'] = train['Reviews'].apply(subjectivity)","7c50eab2":"test1['polarity'] = test['Reviews'].apply(polarity)\ntest1['subjectivity'] = test['Reviews'].apply(subjectivity)","25ede614":"# # Let\u2019s plot the results\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n\n# plt.rcParams['figure.figsize'] = [10, 8]\n\n# for index, Reviews in enumerate(train1.index):\n#     x = train1.polarity.loc[Reviews]\n#     y = train1.subjectivity.loc[Reviews]\n#     plt.scatter(x, y, color='Red')\n \n \n# plt.title('Sentiment Analysis', fontsize = 20)\n# plt.xlabel('\u2190 Negative \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 Positive \u2192', fontsize=15)\n# plt.ylabel('\u2190 Facts \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 Opinions \u2192', fontsize=15)\n# plt.show()","8cf8424c":"def ratio(x):\n    if x > 0:\n        return \"Positive\"\n    else:\n        return \"Negative\"","37cc59c7":"train1['analysis'] = train1['polarity'].apply(ratio)","24746e56":"test1['analysis'] = test1['polarity'].apply(ratio)","0537a762":"train1['analysis'].value_counts()","5cb71aa2":"train['Sentiment'].value_counts()","0dba1185":"sns.countplot(train1['Sentiment'])","26d056fc":"sns.countplot(train1['analysis'])","96bcdfe2":"sns.countplot(test1['Sentiment'])","e54a33e7":"sns.countplot(test1['Sentiment'])","25c42eb0":"train1.to_csv('trainFinal.csv')","b2213a37":"test1.to_csv('testFinal.csv')","fe972cc5":"# trainFeature= pd.DataFrame(train1['analysis'])","5f00b8d1":"# testFeature= pd.DataFrame(test1['analysis'])","bfa65d4b":"# type(trainFeature)\n# type(testFeature)","665b5e89":"# trainingFeatures = trainFeature\n# testingFeature = testFeature","e5b0a2f3":"# # import preprocessing from sklearn\n# from sklearn import preprocessing","bb089427":"# # TODO: create a LabelEncoder object and fit it to each feature in X\n\n\n# # 1. INSTANTIATE\n# # encode labels with value between 0 and n_classes-1.\n# le = preprocessing.LabelEncoder()","65459bc7":"# # 2\/3. FIT AND TRANSFORM\n# # use df.apply() to apply le.fit_transform to all columns\n# X_2_tr = trainingFeatures.apply(le.fit_transform)\n# X_2_te = testingFeature.apply(le.fit_transform)","373c068c":"# # TODO: create a OneHotEncoder object, and fit it to all of X\n\n# # 1. INSTANTIATE\n# enc_train = preprocessing.OneHotEncoder()\n# enc_test = preprocessing.OneHotEncoder()\n\n# # 2. FIT\n# enc_train.fit(X_2_tr)\n# enc_test.fit(X_2_te)\n\n# # 3. Transform\n# onehotlabelsTrain = enc_train.transform(X_2_tr).toarray()\n\n# onehotlabelsTest = enc_test.transform(X_2_te).toarray()\n\n# # but now you've so many more columns due to how we changed all the categorical data into numerical data\n\n","4576be28":"# onehotlabelsTrain.shape","cbb29964":"# onehotlabelsTest.shape","62b5d13e":"# model1 = MultinomialNB(alpha=0.001)\n# model1.fit( onehotlabelsTrain, train1['Sentiment'] )","bd141aaf":"# pred_1 = model1.predict( onehotlabelsTest.toarray() )","f715d7da":"# model2 = SGDClassifier(loss='modified_huber', n_iter_no_change=5, random_state=0, shuffle=True)\n# model2.fit( onehotlabelsTrain, train1[\"analysis\"] )","fea114b6":"# pred_2 = model2.predict( onehotlabelsTest.toarray() )","c23dedd7":"# model3 = RandomForestClassifier()\n# model3.fit( onehotlabelsTrain, train1[\"analysis\"] )","3b0f003e":"# pred_3 = model3.predict( onehotlabelsTest.toarray() )","989d0c53":"# print(classification_report(test1['analysis'], pred_1, target_names=['Positive', 'Negative']))\n# print(classification_report(test1['analysis'], pred_2, target_names=['Positive', 'Negative']))\n# print(classification_report(test1['analysis'], pred_3, target_names=['Positive', 'Negative']))","043df9f4":"As stated by the documentation, the fit method \"learn(s) a vocabulary dictionary of all tokens in the raw documents\", i.e. it creates a dictionary of tokens (by default the tokens are words separated by spaces and punctuation) that maps each single token to a position in the output matrix. Fitting on the training set and transforming on the training and test set assures that, given a word, the word is correctly always mapped on the same column, both in the training and test set. ","e5830f21":"**THE FOLLOING CODE IS IGNORED**"}}