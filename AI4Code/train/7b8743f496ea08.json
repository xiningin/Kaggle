{"cell_type":{"fda937a4":"code","e6e886dd":"code","7332c479":"code","3aec8209":"code","b97a8bba":"code","9b8d4f1e":"code","eb5d0c3e":"code","9672b39f":"code","5d38bbee":"code","1a354382":"code","5f84a808":"code","071edd2a":"code","9f345f3a":"code","9c059ea1":"code","347a99c6":"code","4e7ae2ca":"code","bb17f7d9":"code","14fd4911":"code","eae8178b":"code","7ee6e2c3":"code","2bdb521d":"code","013d3849":"code","3e4b8ade":"code","e22a5f18":"code","42f82d84":"code","a028e94d":"code","34444c21":"code","1b7f9a6f":"code","cf0b5d6b":"code","ada66004":"code","408c3d53":"code","283b7b2d":"code","677658a5":"code","156f2eb9":"code","85f160d9":"code","4a8bbe84":"code","bf2daccd":"code","90901e61":"code","e78081ec":"code","dcdb5836":"code","db7b7093":"code","04a502d8":"code","b5116193":"code","483dcbd1":"code","c49a2025":"code","a40d366c":"code","5002f785":"code","6dbb4711":"code","144ed076":"code","68be84fd":"code","e36e4d4a":"code","f8480022":"code","cac50e0c":"code","a14cc0d6":"code","ce4ea44a":"code","8fc4b5df":"code","d04f03f8":"code","dbb726e4":"code","9364f05f":"code","1f494c8c":"code","5e90231b":"code","a72737f2":"code","31655285":"code","f95db906":"code","6ce5449c":"code","ca5f5577":"code","e9514f73":"code","17bd92c4":"code","0c576697":"code","54248817":"code","8bdf64a3":"code","2035d873":"code","28c7d5fd":"code","392d2515":"code","68027ec8":"code","93a384d4":"code","ef426280":"code","8a40e77d":"markdown","4460e8a7":"markdown","b8918185":"markdown","bc80b782":"markdown","8a44f77c":"markdown","4a4dc645":"markdown","a65b2d71":"markdown","8d734b69":"markdown","a8fc8715":"markdown","fa629e46":"markdown","73dde4a4":"markdown","8bb0d146":"markdown","220a3c6d":"markdown","e3fe4863":"markdown","de6d18ba":"markdown"},"source":{"fda937a4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e6e886dd":"#general imports\nimport os\nimport sys\nassert sys.version_info >= (3,5)\n#data manipulation\nimport numpy as np\nimport pandas as pd\n#visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\n%matplotlib inline\n#handle warnings\nimport warnings\nwarnings.filterwarnings(action='ignore',message='')\n#consistent sized plots\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 15,7\nrcParams['axes.labelsize'] = 14\nrcParams['xtick.labelsize'] = 14\nrcParams['ytick.labelsize'] = 14\nrcParams['axes.titlesize'] = 'large'\n\nfrom IPython.display import HTML\nfrom IPython.display import display\nfrom IPython.display import Image\n\n# data transformations and preparations\nimport sklearn\nassert sklearn.__version__ >= '0.20'\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PowerTransformer\n#ML models\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\n\n#view all the columns of the dataframe\npd.options.display.max_columns = None\n\n#tensorflow for deep learning model\nimport tensorflow as tf\n\n#evaluation metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import classification_report","7332c479":"'''load the train and test data'''\ntrain = pd.read_csv('..\/input\/deloitte-challenge\/train.csv',delimiter=',',engine='python')\ntest = pd.read_csv('..\/input\/deloitte-challenge\/test.csv',delimiter=',',engine='python')","3aec8209":"'''view and basic exploration of the data'''\ntrain.head()","b97a8bba":"'''check basic stats of the data'''\ntrain.describe().transpose()","9b8d4f1e":"'''check the info'''\ntrain.info()","eb5d0c3e":"'''check explicitly for the null values'''\ntrain.isnull().sum()","9672b39f":"'''check for the data balance based on the target label'''\nsns.countplot(train['Loan Status'])\nplt.title('Plot of the count of the Loan Status')\nplt.grid()\nplt.show()","5d38bbee":"'''number of unique customers'''\ntrain['ID'].nunique()","1a354382":"'''drop the ID column from both the train and test data'''\ntrain.drop('ID',axis=1,inplace=True)\ntest_backup = test.copy() \ntest.drop('ID',axis=1,inplace=True)\ntest.drop('Loan Status',axis=1,inplace=True)","5f84a808":"'''separate the categorical and the numerical features'''\ncat_features = train.select_dtypes(include='object').columns.to_list()\nnum_features = train.select_dtypes(exclude='object').columns.to_list()\n\nprint('Categorical Features \\n {}'.format(cat_features))\nprint('Numerical Features \\n {}'.format(num_features))","071edd2a":"'''plot histogram of all the numerical features'''\ntrain[num_features].hist(bins=50, figsize=(35,27))\nplt.show()","9f345f3a":"'''check the statistical variance of the numerical features'''\nimport statistics\nvariance = dict()\nfor feature in num_features:\n    var = statistics.variance(train[feature])\n    print(f'Feature Name {feature}, Variance:................ {var}')\n    variance[feature] = var","9c059ea1":"'''Plot of the Feature Variance'''\nx = np.arange(0,len(train[num_features].columns),1)\ny = variance.values()\nlabels = variance.keys()\n\n#plt.plot(x,y)\nplt.figure(figsize=(12,7))\nplt.bar(x,height=y,color='blue',alpha=0.5)\n# You can specify a rotation for the tick labels in degrees or with keywords.\nplt.xticks(x, labels, rotation='vertical')\n# Pad margins so that markers don't get clipped by the axes\nplt.margins(0.2)\nplt.grid()\nplt.title('Plot of Variance within the Features')\nplt.ylabel('Variance')\nplt.xlabel('Feature Names')\n# Tweak spacing to prevent clipping of tick-labels\nplt.subplots_adjust(bottom=0.15)\nplt.show()","347a99c6":"'''list containing features to be dropped'''\ndrop_list = []\n# Accounts delinquent is always 0 and useless feature to be part of the model\ndrop_list.append('Accounts Delinquent')","4e7ae2ca":"'''Plot of correlation heatmap of the numerical features'''\n#calculate the correlation\ncorr = train[num_features].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n#define the cmap to be used\ncmap = sns.diverging_palette(500,30, as_cmap=True)\n\n#plot the heatmap\nplt.figure(figsize=(15,12))\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.title('Feature Correlation Heatmap')\nplt.show()","bb17f7d9":"for feature in cat_features:\n    ax = sns.countplot(train[feature],order = train[feature].value_counts().index)\n    plt.title('Countplot of {}'.format(feature))\n\n    for p, label in zip(ax.patches, train[feature].value_counts()):\n        \n        ax.annotate(label, (p.get_x()+0.375, p.get_height()+0.35))     \n        plt.xticks(rotation='vertical')\n    plt.show()","14fd4911":"train['Loan Title'].value_counts().sort_values(ascending=False)","eae8178b":"'''replace all with the similar loan title name'''\ntrain['Loan Title'].replace('Debt consolidation','Debt Consolidation',inplace=True)\ntest['Loan Title'].replace('Debt consolidation','Debt Consolidation',inplace=True)","7ee6e2c3":"train['Loan Title'].value_counts().sort_values(ascending=False)[:15]","2bdb521d":"'''replace debt consolidation loan with debt consolidation'''\ntrain['Loan Title'].replace('Debt Consolidation Loan','Debt Consolidation',inplace=True)\ntest['Loan Title'].replace('Debt Consolidation Loan','Debt Consolidation',inplace=True)","013d3849":"'''all home related loan types into one loan type'''\nlen(train[train['Loan Title'].str.startswith('Home')==True])","3e4b8ade":"train[train['Loan Title'].str.startswith('Home')==True]['Loan Title'].unique()","e22a5f18":"home = ['Home improvement', 'Home Improvement', 'Home buying', 'Home','Home Improvement Loan', 'Home loan',\n       'home improvement','House','Bathroom','Pool']\n\n'''replace all home related loan title with one Home Loan'''\nfor hm in home:    \n    train['Loan Title'].replace(hm,'Home Loan',inplace=True)\n    test['Loan Title'].replace(hm,'Home Loan',inplace=True)","42f82d84":"train[train['Loan Title'].str.startswith('Credit')==True]['Loan Title'].unique()","a028e94d":"credit = ['Credit card refinancing', 'Credit Consolidation', 'Credit Cards',\n          'Credit card payoff', 'Credit Card Consolidation',\n          'Credit Card Refi', 'Credit Card Loan', 'Credit Card Debt',\n          'Credit Card', 'Credit Card Refinance', 'Credit Loan',\n          'Credit Card consolidation', 'Credit Card Paydown',\n          'Credit card pay off', 'Credit card refinance', 'Credit',\n          'Credit Card Payoff', 'Credit payoff',\n          'Credit Card Refinance Loan','CC','credit card refinancing']\n\n'''replace all home related loan title with one Home Loan'''\nfor cred in credit:    \n    train['Loan Title'].replace(cred,'Credit Loan',inplace=True)\n    test['Loan Title'].replace(cred,'Credit Loan',inplace=True)","34444c21":"train['Loan Title'].value_counts().sort_values(ascending=False)[:15]","1b7f9a6f":"'''all consolidation loan into one'''\ntrain[train['Loan Title'].str.startswith('Consolida')==True]['Loan Title'].unique()","cf0b5d6b":"train[train['Loan Title'].str.startswith('debt')==True]['Loan Title'].unique()","ada66004":"consolidation = ['Consolidation', 'Consolidate', 'Consolidated','Consolidation Loan',\n                 'debt', 'debt consolidation', 'debt loan','debt consolidation loan',\n                'consolidation','DEBT CONSOLIDATION','consolidate','Card Consolidation','Cards',\n                'credit card consolidation','credit card',\n                'consolidation loan','Bill consolidation','CC Consolidation','CC Refi','CC-Refinance',\n                'get out of debt','CONSOLIDATE','pay off bills',\n                'Debt Consolidation 2013','Refinance','Dept consolidation',\n                'Debt Payoff', 'Pay Off', 'Debt Loan','CONSOLIDATION','Debt', 'cards',\n                'CC Refinance', 'CC consolidation', 'payoff','Bill Consolidation', 'credit card refinance',\n                'Debt Free', 'conso', 'Loan Consolidation', 'Debt payoff','CC Loan',\n                 'credit pay off', 'Payoff', 'My Loan', 'Loan', 'Bill Payoff','Debt Reduction','CC','bills',\n                'Refinance Loan','Get Debt Free','MYLOAN','Freedom','refi','relief','Credit card refinancing']\n\n'''replace all consolidation related loan title with Debt Consolidation'''\nfor consolidate in consolidation:    \n    train['Loan Title'].replace(consolidate,'Debt Consolidation',inplace=True)\n    test['Loan Title'].replace(consolidate,'Debt Consolidation',inplace=True)","408c3d53":"train['Loan Title'].value_counts().sort_values(ascending=False)[:15]","283b7b2d":"train[train['Loan Title'].str.startswith('Car')==True]['Loan Title'].unique()","677658a5":"train[train['Loan Title'].str.contains(\"Car\")]['Loan Title'].unique()","156f2eb9":"auto = ['Car financing','Car Loan']\nfor aut in auto:    \n    train['Loan Title'].replace(aut,'Auto Loan',inplace=True)\n    test['Loan Title'].replace(aut,'Auto Loan',inplace=True)","85f160d9":"personal = ['Personal loan','Personal','personal']\nfor per in personal:\n    train['Loan Title'].replace(per,'Personal Loan',inplace=True)\n    test['Loan Title'].replace(per,'Personal Loan',inplace=True)\n    ","4a8bbe84":" '''all credit card and debt consolidation are the same'''\ntrain['Loan Title'].replace('Credit Loan','Debt Consolidation',inplace=True)\ntest['Loan Title'].replace('Credit Loan','Debt Consolidation',inplace=True)","bf2daccd":"medical = ['Medical expenses','Medical','Medical loan']\nfor med in medical:    \n    train['Loan Title'].replace(med,'Medical Loan',inplace=True)\n    test['Loan Title'].replace(med,'Medical Loan',inplace=True)","90901e61":"train['Loan Title'].replace('vacation','Vacation',inplace=True)\ntest['Loan Title'].replace('vacation','Vacation',inplace=True)","e78081ec":"train['Loan Title'].replace('loan1','Other',inplace=True)\ntest['Loan Title'].replace('loan1','Other',inplace=True)\n\ntrain['Loan Title'].replace('Loan 1','Other',inplace=True)\ntest['Loan Title'].replace('Loan 1','Other',inplace=True)\n\ntrain['Loan Title'].replace('Lending loan','Lending Club',inplace=True)\ntest['Loan Title'].replace('Lending loan','Lending Club',inplace=True)","dcdb5836":"train['Loan Title'].unique()","db7b7093":"train['Loan Title'].value_counts().sort_values(ascending=False)","04a502d8":"train[train['Loan Title']=='Home Loan']['Loan Status'].value_counts()","b5116193":"''' percentage of defaulters per loan type'''\nloan_type = train['Loan Title'].unique().tolist()\n\nfor loan in loan_type:\n    print(loan)\n    print(train[train['Loan Title']==loan]['Loan Status'].value_counts()\/len(train[train['Loan Title']==loan]['Loan Status'])*100)\n    ","483dcbd1":"'''all the single digit loan title can be merged with Others'''\nsmall_loan =  ['Wedding Loan','Getting Ahead']\n\nfor small in small_loan:    \n    train['Loan Title'].replace(small,'Other',inplace=True)\n    test['Loan Title'].replace(small,'Other',inplace=True)","c49a2025":"for features in cat_features:\n    print(features)\n    print(train[features].value_counts())","a40d366c":"'''drop the payment plan feature as it has only one type of value'''\ndrop_list.append('Payment Plan')","5002f785":"for feature in drop_list:\n    train.drop(feature,axis=1,inplace=True)\n    test.drop(feature,axis=1,inplace=True)","6dbb4711":"train.shape, test.shape","144ed076":"sns.histplot(data=train,y='Loan Amount',hue='Loan Title')\nplt.show()","68be84fd":"'''split the data into input and target features'''\nX = train.drop('Loan Status',axis=1)\ny = train[['Loan Status']]","e36e4d4a":"num_features = X.select_dtypes(exclude='object').columns.tolist()\ncat_features = X.select_dtypes(include='object').columns.tolist()","f8480022":"from statsmodels.stats.outliers_influence import variance_inflation_factor","cac50e0c":"def calc_vif(X):\n    '''This function calculates and returns the VIF score in a dataframe format'''\n\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n    return(vif.sort_values(by='VIF',ascending=False))","a14cc0d6":"calc_vif(X[num_features])","ce4ea44a":"num_features.remove('Term')","8fc4b5df":"calc_vif(X[num_features])","d04f03f8":"num_features.remove('Interest Rate')","dbb726e4":"calc_vif(X[num_features])","9364f05f":"num_features.remove('Debit to Income')\ncalc_vif(X[num_features])","1f494c8c":"'''drop the features to reduce the multi-collinearity'''\nX_orig = X.copy()\nX.drop(['Term','Interest Rate','Debit to Income'],axis=1,inplace=True)\ntest.drop(['Term','Interest Rate','Debit to Income'],axis=1,inplace=True)\n","5e90231b":"X.head(2)","a72737f2":"'''split the dataset into train and dev set '''\nX_train,X_dev,y_train,y_dev = train_test_split(X,y,random_state=42,test_size=0.1,stratify=y['Loan Status'])","31655285":"X_train.shape, y_train.shape","f95db906":"X_train.head(4)","6ce5449c":"'''label encode the cat features'''\ncat_features = X_train.select_dtypes(include='object').columns.tolist()\nnum_features = X_train.select_dtypes(exclude='object').columns.tolist()\n\nencoder = LabelEncoder()\n\n\nfor col in cat_features:\n    X_train[col] = encoder.fit_transform(X_train[col])\n    X_dev[col] = encoder.transform(X_dev[col])\n    test[col] = encoder.transform(test[col])   \n    ","ca5f5577":"X_train.shape, X_dev.shape, test.shape","e9514f73":"''' scale the features'''\nscaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_dev_scaled = scaler.transform(X_dev)\ntest_scaled = scaler.transform(test)","17bd92c4":"'''make the features Gaussian'''\npt = PowerTransformer(method='yeo-johnson')\n\nX_train_pt = pt.fit_transform(X_train_scaled)\nX_dev_pt = pt.transform(X_dev_scaled)\ntest_pt = pt.transform(test_scaled)","0c576697":"'''change target to a numpy array'''\ny_train = np.ravel(y_train)\ny_dev = np.ravel(y_dev)","54248817":"X_train_pt","8bdf64a3":"y_train","2035d873":"'''define a neural network model'''\ndef nn_model(optimizer='adam',activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01)):\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Dense(units=64,input_dim=X_train_pt.shape[1],activation=activation))\n    model.add(tf.keras.layers.Dense(units=32,activation=activation,kernel_regularizer=kernel_regularizer))\n    model.add(tf.keras.layers.Dense(units=16,activation=activation,kernel_regularizer=kernel_regularizer))\n    model.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))\n    model.compile(loss=tf.keras.losses.BinaryCrossentropy(),optimizer=optimizer,metrics=[tf.keras.metrics.AUC()])\n    return model  ","28c7d5fd":"model = nn_model(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),kernel_regularizer=tf.keras.regularizers.l2(0.001))\nhistory = model.fit(X_train_pt,y_train,epochs=100,validation_data=(X_dev_pt,y_dev),batch_size=64)","392d2515":"'''visualize the nn model results'''\nnn_results = pd.DataFrame(history.history)\nnn_results.head()","68027ec8":"#predictions = np.argmax(model.predict(test_pt), axis=-1)\npredictions = model.predict(test_pt)","93a384d4":"predictions.shape","ef426280":"submissions = pd.DataFrame(data=predictions, columns=['Loan Status'])\nsubmissions.to_csv('submission.csv',index=False)","8a40e77d":"- _There are multiple clusters in the loan amount and the funded amount_\n- _Accounts Delinquent is 0.0 and perhaps does not add any value in the dataset_\n- _Many of the features indicate they only have one constant value as 0.0, example Recoveries, Total received late fee etc._","4460e8a7":"# _Import Libraries_","b8918185":"# _Deloitte Machine Hack Challenge_\n\n<b> Problem Statement<\/b>\n\nDeloitte refers to one or more of Deloitte Touche Tohmatsu Limited (\u201cDTTL\u201d), its global network of member firms, and their related entities (collectively, the \u201cDeloitte organization\u201d). DTTL (also referred to as \u201cDeloitte Global\u201d) and each of its member firms and related entities are legally separate and independent entities, which cannot obligate or bind each other in respect of third parties. DTTL and each DTTL member firm and related entity is liable only for its own acts and omissions, and not those of each other. DTTL does not provide services to clients. Please see  www.deloitte.com\/about  to learn more.\n\nAll the facts and figures that talk to our size and diversity and years of experiences, as notable and important as they may be, are secondary to the truest measure of Deloitte: the impact we make in the world.\n\nSo, when people ask, \u201cwhat\u2019s different about Deloitte?\u201d the answer resides in the many specific examples of where we have helped Deloitte member firm clients, our people, and sections of society to achieve remarkable goals, solve complex problems or make meaningful progress. Deeper still, it\u2019s in the beliefs, behaviours and fundamental sense of purpose that underpin all that we do. Deloitte Globally has grown in scale and diversity\u2014more than 345,000 people in 150 countries, providing multidisciplinary services yet our shared culture remains the same.\n\n(C) 2021 Deloitte Touche Tohmatsu India LLP\u201d\n\n \n\nBanks run into losses when a customer doesn't pay their loans on time. Because of this, every year, banks have losses in crores, and this also impacts the country's economic growth to a large extent. In this hackathon, we look at various attributes such as funded amount, location, loan, balance, etc., to predict if a person will be a loan defaulter or not. \n\nTo solve this problem, MachineHack has created a training dataset of 67,463 rows and 35 columns and a testing dataset of 28,913 rows and 34 columns. The hackathon demands a few pre-requisite skills like big dataset, underfitting vs overfitting, and the ability to optimise \u201clog_loss\u201d to generalise well on unseen data. ","bc80b782":"# _Handling Multi-collinearity_\n\nVariance Inflation Factor\n\n- _VIF or Variance Inflation Factor is a statistical technique to determine the degree of multi-collinearity of the features_\n- _Multi-collinearity can be fixed by dropping a feature which has relatively higher VIF score. Usually above 5 which is used as a benchmark_\n- _The strategy to drop the features is to drop one at a time starting with the feature with the highest VIF, recalculate the VIF and then to continue the same step till all the remaning features are closer to the acceptable VIF score_","8a44f77c":"- _Not a very nice looking plots and there are categorical features which have a lot of unique values like the loan title_","4a4dc645":"# _Data Preparation for Modeling_","a65b2d71":"- _The plot of the feature variance is not reflecting the true picture as the features have very high differing scale_","8d734b69":"- _Credit card refinancing is the leading loan type followed byb debt consolidation_\n- _there are two categories of Debt consolition and they are different with respect to the spelling only_","a8fc8715":"# _Deep Learning Model_","fa629e46":"# _Load the data_","73dde4a4":"_There are a lot more of 0's vs 1's. The data is definitely imbalanced_","8bb0d146":"## _Split into train and dev set_","220a3c6d":"- _Corr plot does not indicate any strong correlation between the features_\n- _Accounts delinquent has no correlation as it is always 0_","e3fe4863":"# _EDA & Data Cleaning_","de6d18ba":"- _There are no null values in the train dataset_\n- _There is a mix of numeric and categorical data in the dataset_\n- _There are over 67,000 entries or observations. Also to be checked if they are all for unique customers or not_\n- _There is large variation in the scale of the data in the various numerical features of the dataset_\n- _The data type matches with the actual data reflected in each of the features_\n"}}