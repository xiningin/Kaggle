{"cell_type":{"8e563fe0":"code","dd8508c4":"code","f42dfafe":"code","839ceb84":"code","1a2aa924":"code","3b3dc779":"code","31a64458":"code","4f24f092":"code","6ae58eef":"code","af29a240":"code","d5a31579":"code","7f6a09a4":"code","c1f12b73":"code","cd99a057":"code","71731dc7":"code","08a13c46":"code","635e1f6b":"code","d08a6032":"code","ac4f350d":"code","d8d0bd3e":"code","2c0df7a5":"code","b702b278":"code","bb5ad74f":"code","9b8bacb8":"code","68bf1844":"code","09144902":"code","06da953a":"code","6bcf560f":"code","85bc6dea":"code","91af7221":"code","be39d9a9":"code","b8e85599":"code","690243e2":"code","0e799ca1":"code","34d8d07c":"markdown","935edd25":"markdown","f20e9d02":"markdown","60f39ea9":"markdown","d40e3527":"markdown","48d192f3":"markdown","dea7e1e3":"markdown","f93b5bb9":"markdown","024ef22b":"markdown","1f6a827d":"markdown","1ff3f822":"markdown","79aefdc2":"markdown","e0be0035":"markdown","67206f67":"markdown","d1c9d527":"markdown","d54884b9":"markdown","8e3f5652":"markdown","c3571eaa":"markdown","2f97f92a":"markdown"},"source":{"8e563fe0":"!pip install wordcloud","dd8508c4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU,SimpleRNN\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D, Input\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport seaborn as sns\nfrom pandas_profiling import ProfileReport\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f42dfafe":"train_df = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip') \ntest_df = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip')\nlabels_df = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/test_labels.csv.zip')\n\nprint('The Training data has {} rows and {} columns '.format(train_df.shape[0], train_df.shape[1]))","839ceb84":"train_df.head()","1a2aa924":"test_df.head()","3b3dc779":"labels_df.head()","31a64458":"ProfileReport(train_df)","4f24f092":"labels = train_df['toxic'].value_counts().index\nvalues = train_df['toxic'].value_counts().values\ncolor = ['green', 'lightblue']\n\nplt.figure(figsize=(10,10))\nfig = go.Figure(data=[go.Pie(labels=labels, textinfo='label+percent', values=values, marker=dict(colors=color))])\nfig.show()","6ae58eef":"train_df.info()","af29a240":"plt.figure(figsize=(10,5))\ncolormap = plt.cm.plasma\nsns.heatmap(train_df.corr(), annot=True, cmap=colormap)","d5a31579":"train_df['comment_text'][0]","7f6a09a4":"train_df['comment_text'][2]","c1f12b73":"sns.countplot(train_df['toxic'])","cd99a057":"sns.countplot(y=train_df['obscene'])","71731dc7":"texts = train_df['comment_text'][0]\nwordcloud = WordCloud().generate(texts)\n\n# Display the generated image:\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","08a13c46":"texts = train_df['comment_text'][2]\nwordcloud = WordCloud().generate(texts)\n\n# Display the generated image:\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","635e1f6b":"def roc_auc(predictions, target):\n    \n    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n    roc_auc = metrics.auc(fpr, tpr)\n    return roc_auc","d08a6032":"try:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","ac4f350d":"train_df['char_length'] = train_df['comment_text'].apply(lambda x : len(str(x)))","d8d0bd3e":"test_df['char_length'] = test_df['comment_text'].apply(lambda x : len(str(x)))","2c0df7a5":"import re\n\ndef clean_text(texts):\n    texts = texts.lower()\n    texts = re.sub(r\"what's\", \"what is \", texts)\n    texts = re.sub(r\"\\'s\", \" \", texts)\n    texts = re.sub(r\"\\'ve\", \" have \", texts)\n    texts = re.sub(r\"can't\", \"cannot \", texts)\n    texts = re.sub(r\"n't\", \" not \", texts)\n    texts = re.sub(r\"i'm\", \"i am \", texts)\n    texts = re.sub(r\"\\'re\", \" are \", texts)\n    texts = re.sub(r\"\\'d\", \" would \", texts)\n    texts = re.sub(r\"\\'ll\", \" will \", texts)\n    texts = re.sub(r\"\\'scuse\", \" excuse \", texts)\n    texts = re.sub('\\W', ' ', texts)\n    texts = re.sub('\\s+', ' ', texts)\n    texts = texts.strip(' ')\n    return texts","b702b278":"# clean the comment_text in train_df [Thanks to Pulkit Jha for the useful pointer.]\ntrain_df['comment_text'] = train_df['comment_text'].map(lambda com : clean_text(com))\n# clean the comment_text in test_df [Thanks, Pulkit Jha.]\ntest_df['comment_text'] = test_df['comment_text'].map(lambda com : clean_text(com))","bb5ad74f":"train_df = train_df.drop('char_length',axis=1)\nx = train_df.comment_text\nx_test = test_df.comment_text","9b8bacb8":"x.shape","68bf1844":"x_test.shape","09144902":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\nvect = TfidfVectorizer(max_features=5000,stop_words='english')\nvect","06da953a":"x_train = vect.fit_transform(x)","6bcf560f":"x_test = vect.transform(x_test)","85bc6dea":"cols = ['obscene','insult','toxic','severe_toxic','identity_hate','threat']","91af7221":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nrfc = RandomForestClassifier()\n\nsubmission_binary = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/sample_submission.csv.zip')\n\n#to get predictions specific to each columns in the dataset, binary relevance based approach\nfor labels in cols:\n    print('Started with {}'.format(labels))\n    y = train_df[labels]\n    rfc.fit(x_train,y)\n    \n    y_preds = rfc.predict(x_train)\n    print('Validation accuracy is {}'.format(accuracy_score(y, y_preds)))\n    # compute the predicted probabilities for x_test\n    y_prob = rfc.predict_proba(x_test)[:,1]\n    submission_binary[labels] = y_prob","be39d9a9":"rc = roc_auc(y_preds, y)\nrc","b8e85599":"cf = classification_report(y_preds, y)\nprint('The Classification Report {} \\n '.format(cf))","690243e2":"submission_binary.head()","0e799ca1":"submission_binary.to_csv('submission_binary',index=False)\nprint('Submission file is successfully created!!')","34d8d07c":"# Let's Dive into EDA!!","935edd25":"**As can be seen from above, the Dataset does not contain any missing values, so our work becomes easier now!!**","f20e9d02":"# Wordcloud\n\n**It is one of great visualization techniques that can be used to see what words are repeated more often, thus getting an insight to the emotions of the person who might have commented**","60f39ea9":"# Cleaning comments","d40e3527":"***This dataset contains data, which were not used for scoring***","48d192f3":"**Let's define ROC_AUC curve metrics for our dataset**","dea7e1e3":"# Loading Libraries","f93b5bb9":"**Let's see the Classification Report**","024ef22b":"# Submission","1f6a827d":"# Loading Dataset","1ff3f822":"# Correlation\n\n***We do correlation plot to understand the correlations among columns, this helps in selecting certain specific models for training while developing the model***","79aefdc2":"**Let's start with using Pandas profiling to get a concised exploration of the training data**","e0be0035":"# Introduction\n\n![](https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/jigsaw\/003-avatar.png)\n\n**Understanding the Dataset**\n\n**The Dataset has Wikipedia comments which have been labeled by human raters for toxic behavior, these are shown below, this is part of competion [here](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/overview)**\n* toxic\n* severe_toxic\n* obscene\n* threat\n* insult\n* identity_hate\n\n**You must create a model which predicts a probability of each type of toxicity for each comment.**\n\n1. train.csv - the training set, contains comments with their binary labels\n2. test.csv - the test set, you must predict the toxicity probabilities for these comments. To deter hand labeling, the test set contains some comments which are not included in scoring.\n3. sample_submission.csv - a sample submission file in the correct format\n4. test_labels.csv - labels for the test data; value of -1 indicates it was not used for scoring","67206f67":"**Let's check the character length and distribution of it in test data**","d1c9d527":"# Tokenization\n**We will be using sklearn library tools for tokenizing using vectorizer for our comments from dataset**","d54884b9":"**Let's check the character length and distribution of it in train data**","8e3f5652":"**Let's look at some Toxic and Non Toxic Comments**","c3571eaa":"**TO DO Neural Networks part**","2f97f92a":"**So as we can see, Obscene has good correlation with Toxic, Insult and Obscene has the highest correlation among the columns at 0.74**"}}