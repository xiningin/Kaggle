{"cell_type":{"6f2e0397":"code","13defd7d":"code","19707bac":"code","3efd43a8":"code","218e3481":"code","23c1c5c2":"code","fd8d76d9":"code","343e9d39":"code","d0093839":"code","17070a9f":"code","5363c971":"code","b6e80c24":"code","09e90b89":"code","0aa8e097":"code","b4e8b592":"code","b4fdcacc":"markdown","232fac04":"markdown","361fa33c":"markdown","759d85a5":"markdown","43f069c2":"markdown","d5e0b16e":"markdown"},"source":{"6f2e0397":"import tensorflow as tf\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nimport nltk","13defd7d":"class Config:\n    vocab_size = 5000\n    embed_size = 100\n    filters = 256\n    num_words = 3\n    batch_size = 64\n    epochs = 20\n    maxlen = 100\n    model_path = \"model.tf\"\n    \nconfig = Config() ","19707bac":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","3efd43a8":"train.head()","218e3481":"test.head()","23c1c5c2":"train[\"sentence_preprocessed\"] = train[\"text\"].apply(lambda sentence: \" \". join(nltk.word_tokenize(sentence.lower())))\ntest[\"sentence_preprocessed\"] = test[\"text\"].apply(lambda sentence: \" \". join(nltk.word_tokenize(sentence.lower())))","fd8d76d9":"train.head()","343e9d39":"vectorizor = keras.layers.TextVectorization(max_tokens=config.vocab_size)\nvectorizor.adapt(list(train[\"sentence_preprocessed\"]) + list(test[\"sentence_preprocessed\"]))","d0093839":"x_train, x_val, y_train, y_val = train_test_split(train[\"sentence_preprocessed\"], train[\"target\"], test_size=0.2, random_state=42)\nx_train.shape, x_val.shape, y_train.shape, y_val.shape","17070a9f":"keras.backend.clear_session()\nmodel = keras.Sequential([\n    keras.Input(shape=(None, ), dtype=\"string\"),\n    vectorizor,\n    keras.layers.Embedding(config.vocab_size, config.embed_size, input_length=config.maxlen),\n    keras.layers.SpatialDropout1D(0.2),\n    keras.layers.Conv1D(filters=config.filters, kernel_size=config.num_words, activation=\"relu\"),\n    keras.layers.GlobalMaxPooling1D(),\n    keras.layers.Dense(1, activation=\"sigmoid\")\n])","5363c971":"keras.utils.plot_model(model, show_shapes=True)","b6e80c24":"model.summary()","09e90b89":"checkpoint = keras.callbacks.ModelCheckpoint(config.model_path, monitor=\"val_accuracy\", save_best_only=True, save_weights_only=True)\nearly_stop = keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nhistry = model.fit(\n    x_train, y_train, \n    batch_size=config.batch_size, \n    epochs=config.epochs,\n    validation_data=(x_val, y_val),\n    callbacks=[checkpoint, early_stop]\n)","0aa8e097":"model.load_weights(config.model_path)","b4e8b592":"pred = np.array(model.predict(test[\"sentence_preprocessed\"]) > 0.5, dtype=int)\nsample_submission[\"target\"] = pred\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission.head()","b4fdcacc":"## Text Vectorization","232fac04":"## Modeling","361fa33c":"## Disaster Tweets Classification with Conv1D","759d85a5":"## Submission","43f069c2":"## Preprocessing","d5e0b16e":"## Train Validation Split"}}