{"cell_type":{"6f08b7a3":"code","e13e8461":"code","94efb876":"code","f9505898":"code","491ab9e6":"code","b63c8f05":"code","81630ca9":"code","1fe2e683":"code","fec9c058":"code","6a404056":"markdown","d38ba933":"markdown","164d6258":"markdown","60fd9032":"markdown","2d2126f3":"markdown","912f4996":"markdown","7934734a":"markdown"},"source":{"6f08b7a3":"# import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve\nnp.random.seed(42)","e13e8461":"# size of the validation set\nVAL_SIZE = 0.9\n\n# define model parameters\nLEARNING_RATE = 0.1\nN_ESTIMATORS  = [10, 25, 50, 75, 100, 125, 150, 200]\nMAX_DEPTH     = [4, 9, 14]\nN_JOBS        = 16\nTREE_METHOD   = 'hist'\nVERBOSITY     = 1","94efb876":"train = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')","f9505898":"train.head()","491ab9e6":"bids = train['id'].values\ntrain_bids = np.random.choice(bids, replace=False, size=int((1 - VAL_SIZE) * len(train)))\nvalid_bids = np.array([x for x in bids if x not in train_bids])","b63c8f05":"y_train = train[train['id'].isin(train_bids)]['target'].values\nX_train = train[train['id'].isin(train_bids)].drop(['id', 'target'], axis = 1)\ny_valid = train[train['id'].isin(valid_bids)]['target'].values\nX_valid = train[train['id'].isin(valid_bids)].drop(['id', 'target'], axis = 1)","81630ca9":"auc = [[] for x in MAX_DEPTH]\nroc  = [[] for x in MAX_DEPTH]\n\nfor i, md in enumerate(MAX_DEPTH):\n    \n    print(f'Max depth {md}')\n    \n    for nest in N_ESTIMATORS:\n        \n        print(f'Running {nest}')\n        \n        # define the model\n        xgb = XGBClassifier(learning_rate = LEARNING_RATE, n_estimators = nest, max_depth = md, \n                            n_jobs = N_JOBS, tree_method = TREE_METHOD, verbosity=VERBOSITY, \n                            eval_metric = 'logloss', use_label_encoder = False)\n\n        # fit the model\ton the train set\n        model_xgb = xgb.fit(X_train,y_train)\n\n        # predict on the validation set\n        y_valid_pred = model_xgb.predict_proba(X_valid)[:,1]\n\n        # save area under the curve and roc curve\n        auc[i].append(roc_auc_score(y_valid, y_valid_pred))\n        roc[i].append(roc_curve(y_valid, y_valid_pred))","1fe2e683":"for i, md in enumerate(MAX_DEPTH):\n    plt.scatter(N_ESTIMATORS, auc[i], label = f'Max depth {md}')\nplt.legend()\nplt.ylabel('AUC')\nplt.xlabel('Number of estimators');","fec9c058":"which = 0\n\nfor i in range(len(N_ESTIMATORS)):\n    plt.plot(roc[which][i][0], roc[which][i][1], label = N_ESTIMATORS[i]);\nplt.legend();\nplt.title(f'ROC curve for max_depth = {MAX_DEPTH[which]} and various number of estimators');","6a404056":"Plot the ROC curve for the results obtained with the smallest trees to see its convergence","d38ba933":"Check AUC as a function of the number of estimators - we see that we get pretty good results already with a small number of estimators. Somewhat interestingly, the smallest trees do the best job.","164d6258":"In this notebook we focus on XGBoost classifier and investigate how quickly it learns the data (as a function of the number of estimators and the maximal depth of the individual tree)","60fd9032":"Define parameters:","2d2126f3":"Train various XGBoost classifiers","912f4996":"Load the data","7934734a":"Separate train and validation sets"}}