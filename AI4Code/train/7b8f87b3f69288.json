{"cell_type":{"39f084a4":"code","df5c746a":"code","7f68646e":"code","5916dd32":"code","6d005ed1":"code","539c9aab":"code","3f5fb72f":"code","f0cfd9bf":"code","9bd1880b":"code","96e44580":"code","d6be080c":"code","f3023843":"code","44ac5f6b":"code","029adafe":"code","c12acbb4":"code","e542eb68":"code","79eb6a4d":"code","750c10c5":"code","81f0f8fd":"markdown","da0ec089":"markdown","a111721a":"markdown","8cec5d00":"markdown","ee068db4":"markdown","a9ff5b14":"markdown","2b5a2500":"markdown","e2d6ddc8":"markdown","68a88ca3":"markdown","d5088fd8":"markdown","5cdca588":"markdown","cbdc7390":"markdown","e310cec2":"markdown","65b8ebbd":"markdown","d363f362":"markdown"},"source":{"39f084a4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plot\nimport seaborn as sns\n\nsns.set()\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nsns.set()\n# Any results you write to the current directory are saved as output.","df5c746a":"data = pd.read_csv(\"..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")","7f68646e":"data.head()","5916dd32":"for col in data.columns:\n    if not np.issubdtype(data[col].dtype, np.number):\n        if len(data[col].unique()) < 11:\n            _dat = pd.get_dummies(data[col], prefix=col).iloc[:,1:]\n            data = pd.concat([data, _dat], 1)\n            data = data.drop(col, 1)\n        else:\n            if \"Charges\" in col:\n                data[col] = pd.to_numeric(data[col].replace(\" \", 0))\n\nY = data[\"Churn_Yes\"]\nX = data.drop([\"Churn_Yes\", \"customerID\"], 1)\nX.head()","6d005ed1":"from sklearn.cluster import KMeans\n\nwcss = []\nnumber = range(2,10)\n\nfor n in number:\n    kmeans = KMeans(n)\n    kmeans.fit(X)\n    \n    wcss.append(kmeans.inertia_)\n    \nplot.plot(number, wcss, \"-o\")\nplot.show()\n","539c9aab":"kmeans = KMeans(4)\nkmeans.fit(X)\n\nclusters = kmeans.predict(X)","3f5fb72f":"def PlotClusters(X, v1, v2, clusters):\n    Xc = pd.concat([X, pd.Series(clusters).rename(\"cluster\")],1)\n\n    Xc = Xc[[v1, v2, \"cluster\"]]\n\n    for i in range(np.max(clusters)+1):\n        _Xc = Xc[Xc[\"cluster\"]==i]\n        plot.scatter(_Xc[v1], _Xc[v2])\n\n    plot.xlabel(v1)\n    plot.ylabel(v2)","f0cfd9bf":"plot.figure(figsize=(15,15))\nplot.subplot(221)\nPlotClusters(X, \"tenure\", \"TotalCharges\", clusters)\nplot.subplot(222)\nPlotClusters(X, \"tenure\", \"MonthlyCharges\", clusters)\nplot.subplot(223)\nPlotClusters(X, \"MonthlyCharges\", \"TotalCharges\", clusters)\n\nplot.show()","9bd1880b":"import statsmodels.api as sm\n\ntreshold=0.05\nX2 = sm.add_constant(X.drop(\"TotalCharges\", 1))\npdroplist=[\"TotalCharges\"]\n\nwhile True:\n    ols = sm.OLS(Y, X2).fit()\n    \n    if ols.pvalues.max() > treshold:\n        col = ols.pvalues.argmax()\n        print(\"Dropping \"+str(col))\n        X2 = X2.drop(col,1)\n        pdroplist.append(col)\n    else:\n        break","96e44580":"ols = sm.OLS(Y, X2).fit()\nprint(ols.summary())","d6be080c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import Imputer, StandardScaler\n\nX3=X2.drop(\"const\",1)\n\nimputer = Imputer()\nX3 = imputer.fit_transform(X3)\n\nscaler = StandardScaler()\nX3 = scaler.fit_transform(X3)","f3023843":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, Y_train, Y_val = train_test_split(X3, Y, test_size=0.2)","44ac5f6b":"from sklearn.metrics import accuracy_score\n\nreg = LogisticRegression()\nreg.fit(X_train, Y_train)\n\nacc = accuracy_score(Y_val, reg.predict(X_val))\nprint(\"Accuracy: \"+str(acc))","029adafe":"xvals = X2.columns[1:]\ncoeffs =np.exp(reg.coef_[0])-1.0\n\nplot.figure(figsize=(10,5))\nplot.bar(xvals, coeffs)\nplot.xticks(rotation=90)\nplot.ylabel(\"Coefficients [a.u.]\")\nplot.show()","c12acbb4":"inet_customers = data[data[\"InternetService_No\"]==0]\nfo_data = inet_customers[[\"InternetService_Fiber optic\", \"Churn_Yes\"]]","e542eb68":"c00 = fo_data[(fo_data[\"InternetService_Fiber optic\"]==0) & (fo_data[\"Churn_Yes\"]==0)].shape[0]\nc01 = fo_data[(fo_data[\"InternetService_Fiber optic\"]==0) & (fo_data[\"Churn_Yes\"]==1)].shape[0]\nc10 = fo_data[(fo_data[\"InternetService_Fiber optic\"]==1) & (fo_data[\"Churn_Yes\"]==0)].shape[0]\nc11 = fo_data[(fo_data[\"InternetService_Fiber optic\"]==1) & (fo_data[\"Churn_Yes\"]==1)].shape[0]","79eb6a4d":"from scipy.stats import chi2_contingency\n\nc_table = np.array([[c00, c01], [c10, c11]])\nchi2, p, dof, expected = chi2_contingency(c_table)\n\nprint(\"P-Value: \"+str(p))","750c10c5":"leaving = reg.predict(X_val)\nleaving = np.sum(leaving)\/len(leaving)\n\nprint(\"Customers currently leaving:\\t\\t\\t\"+str(np.round(leaving*100,2))+\"%\")\n\nreg2 = LogisticRegression()\n\nreg2.coef_=np.copy(reg.coef_)\nreg2.coef_[0,7]=0\n#reg2.coef_[0,2]=reg2.coef_[0,2]*np.exp(0.25)\nreg2.intercept_ = reg.intercept_\nreg2.classes_ = reg.classes_\n\nleaving_new = reg2.predict(X_val)\nleaving_new = np.sum(leaving_new)\/len(leaving_new)\n\n\nprint(\"Customers leaving with better fiber optics:\\t\"+str(np.round(leaving_new*100,2))+\"%\")\n","81f0f8fd":"**Predicting improvements**\n\nA main issue seems to be the fiber optics product as well as the total charge. While reducing the price of the products may not be possible easily for the company, an improvement of the fiber optics product should be enforced. How many customers  less would leave the company if fiber optics was neutral?","da0ec089":"**Closer look at the Fiber Optic product**\n\nIn order to find out whether there is really something wrong with the fibre optic product, we can perform a chi-squared test.","a111721a":"Calculating the contingency table:","8cec5d00":"We can see that the two main features that distiguish the generated groups are tenure and the total charges.\n\n**Backward Elimination**\n\nFro backward elimination we are using an automated algorithm. We calculate the pvalues and drop the largest one if it is larger than our treshold. Otherwise we are finished.","ee068db4":"**Reading the data**\n\nFor reading the data we are using pandas and its read_csv method.","a9ff5b14":"Let us look at our statistical summary.","2b5a2500":"Looks good so far. The remaining features all seem to be quite significant.\n\n**Preprocessing**\n\nNext we are preprocessing our data. Therefore we are replacing NaNs with the respective mean value and we are standardizing our data.","e2d6ddc8":"**Clustering**\n\nWe can now cluster our customers. In order to find a good number of clusters we are using the Elbow method.","68a88ca3":"The p-value is very small. It is therefore reasonable to say that the fiber optic product has a significant impact on the customer churn.","d5088fd8":"In order to be able to assess the accuracy of our model we are using a train-test split.","5cdca588":"We can now investigate all available features Our target feature is \"Churn\". Let us convert all features in numeric ones:","cbdc7390":"**How can we prevent customers from leaving the company?**\n\nIn order to find out we are going to build a logistic regression model and identify important factors.\nWe are going to perform the following steps:\n* Load the data\n* Preprocess the data\n* Create customer clusters using KMeans to get an overview\n* Perform a backward elemination of the features to filter out only the significant features\n* Identify reasons why customers are leaving\n* Propose solutions","e310cec2":"Our ideal number of clusters seems to be either 4 our 5. I am chosing 4 here.","65b8ebbd":"We can now see that the main factors are\n* tenure\n    * Long time customers are less likely to leave the company.\n    * Loyality\n* TotalCharges\n    * More paying customers are more likely to leave the company.\n    * Are there cheaper offers from competitors?\n* InternetService_Fiber optic\n    * People having Fiber optic are more likely to leave.\n    * Is there something wrong with the companies fiber optics product?\n* OnlineSecurity Yes\n    * People having this product are more loyal.\n    * How can this product attract a wider audience?\n* TechSupport Yes\n    * Same like OnlineSecurity\n* Streaming Movie\/TV\n    * Are customers unhappy with these products?\n    * How can they be improved?\n* Contact One\/Two year\n    * This one is very obvious.\n    * Can the company make this kind of contract more attractive?\n* PaperlessBilling and Electronic Payment\n    * People using these services are more likely to leave.\n    * More technology affine people ready to change frequently?\n    * Easier to change when anything can be done online.\n   \n","d363f362":"That is a quite good accuracy for such a simple model. The advantage of using such a simple model is that we easily can extract valuable insights. Let us look at the coefficients of our logistic regression. We are using the following transformation in order to make them comparable and more readable.\n\n$x \\rightarrow (e^x-1)$"}}