{"cell_type":{"6bb58667":"code","01c76dab":"code","11baaf02":"code","0067066a":"code","75a136ad":"code","485c8fc7":"code","f52a310e":"code","826e3012":"code","acae4fa8":"code","be45a9d2":"code","6c17f2fd":"code","76a2f871":"code","e3fec03b":"code","d08f2856":"code","e82942e4":"code","318a5bd1":"code","9f5c4b74":"code","4f445ddc":"code","ffc495ee":"code","490a2182":"markdown","aba73111":"markdown","d47f3d37":"markdown","7b9873b5":"markdown","ef4e996b":"markdown","b0f309a9":"markdown","922ca610":"markdown","0972ada2":"markdown"},"source":{"6bb58667":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","01c76dab":"netflix = pd.read_csv('..\/input\/netflix-shows\/netflix_titles.csv')\nnetflix.head()","11baaf02":"#Importing relevant libraries\nimport os\nimport nltk\nfrom nltk.util import ngrams\n#from nltk.lm import MLE\nfrom nltk import word_tokenize\n# we need to download a special component that is used by the tokenizer below \nnltk.download('punkt')\nimport tensorflow as tf\nimport numpy as np\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport tensorflow as tf\nimport numpy\nimport pandas as pd\nimport tensorflow_hub as hub\nimport tensorflow_datasets as tfds\nimport nltk\nnltk.download('stopwords')\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\n\n#Employing Early stopping as a means of Validation\nfrom tensorflow.keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n\n#Deciding on number of epochs\n#Number of Epochs is how many time the entire dataset has to pass through the network\nnum_epochs = 100\n#Using pre-trained embedding\n#embedding = \"https:\/\/tfhub.dev\/google\/tf2-preview\/gnews-swivel-20dim\/1\"\n#embedding = \"https:\/\/tfhub.dev\/google\/tf2-preview\/nnlm-en-dim50\/1\"\nembedding = \"https:\/\/tfhub.dev\/google\/tf2-preview\/nnlm-en-dim128\/1\"\nhub_layer = hub.KerasLayer(embedding, input_shape=[], #output_shape=[128], \n                           dtype=tf.string, trainable=False)","0067066a":"all_text =[]\ncounter =0\nfor text in netflix.description:\n  all_text.append(text)\n  counter+=1\n\n#Total rows traverse in every dataframe - negative and positive   \nprint(\"Total number of movie descriptions added to our corpus:\",counter)\n#Creating strings of all descriptions \nall_text = \" \".join(all_text)\n\n#Reviweing sample text\nprint(\"Text Sample : \",all_text[:500])\n","75a136ad":"#Setting up vocabularies and datasets\ntextall = all_text\n\n# extract an ordered vocabulary - this will be letters, some numbers etc. \nvocaball = sorted(set(textall))\n\n# Create mappings between vocab and numeric indices\nchar2idxall = {u:i for i, u in enumerate(vocaball)}\nidx2charall = np.array(vocaball)\n\n# Map all the training text over to a numeric representation\ntext_as_intall = np.array([char2idxall[c] for c in textall])\n\n# The maximum length sentence we want for a single input in characters\nseq_length = 100\nexamples_per_epochall = len(textall)\/\/(seq_length+1)\n\n# Create training examples \/ targets\nchar_datasetall = tf.data.Dataset.from_tensor_slices(text_as_intall)\nsequencesall = char_datasetall.batch(seq_length+1, drop_remainder=True)\n\n# This function would create input and output training data for our model on the fly\ndef split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\n\n# Mapping the above function to entire dataset \ndatasetall = sequencesall.map(split_input_target)\n\n#Setting up hyperparameters\nembedding_dim = 256\nBATCH_SIZE = 64\nBUFFER_SIZE = 10000\nrnn_units = 2048\n\n#Tensorflow wil use this buffer size and batch size to work with the problem data\ndatasetall = datasetall.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\n","485c8fc7":"#To generate some new text we first define a function that can be used to generate this text\n#The function takes the trained neural language model as input and a chunk of text that is used to 'prime' or seed the generator\ndef generate_text(model, start_string, chr2,idx2):\n\n    char2idx = chr2\n    idx2char = idx2\n\n    # Number of characters to generate\n    num_generate = 100\n\n    # Converting our start string to numbers (vectorizing)\n    input_eval = [char2idx[s] for s in start_string]\n    input_eval = tf.expand_dims(input_eval, 0)\n\n    # Empty string to store our results\n    text_generated = []\n\n    # Low temperatures results in more predictable text.\n    # Higher temperatures results in more surprising text.\n    # Experiment to find the best setting.\n    temperature = 1.0\n\n    # Here batch size == 1\n    model.reset_states()\n    text_ids_gen = []\n    for i in range(num_generate):\n        predictions = model(input_eval)\n        # remove the batch dimension\n        predictions = tf.squeeze(predictions, 0)\n\n        # using a categorical distribution to predict the character returned by the model\n        predictions = predictions \/ temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n        # We pass the predicted character as the next input to the model\n        # along with the previous hidden state\n        input_eval = tf.expand_dims([predicted_id], 0)\n\n        text_ids_gen.append(predicted_id)\n        text_generated.append(idx2char[predicted_id])\n\n    return (start_string + ''.join(text_generated))\n","f52a310e":"#Desiging our model - ALL Text\nmall = tf.keras.Sequential()\n#An embedding layer to capture meaning of words in a real valued space.\nmall.add(tf.keras.layers.Embedding(len(vocaball), embedding_dim,\n                          batch_input_shape=[BATCH_SIZE, None]))\n#Recurrent layer - Long Short Term Memory\nmall.add(tf.keras.layers.LSTM(rnn_units,\n                    return_sequences=True,\n                    stateful=True,\n                    recurrent_initializer='glorot_uniform'))\n#Final output layer with one neuron per vovab entry\nmall.add(tf.keras.layers.Dense(len(vocaball)))\n#Displaying the model parameters \nmall.summary()\n#Cross entropy as it is a multiple class classification problem. Using ADAM optimizer for it best performs in case of classification\ndef loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\nmall.compile(optimizer='adam', loss=loss)\n","826e3012":"#To save checkpoints or models\nmy_temp_folder = \"..\/kaggle\/working\/\"","acae4fa8":"# Directory where the checkpoints will be saved for Negative, Positive and ALL\ncheckpoint_dirall = my_temp_folder+'training_checkpoints_lstmall'\nimport shutil\ntry:\n    shutil.rmtree(checkpoint_dirall)\nexcept:\n    print(\"directory not used yet.\")","be45a9d2":"# Setting up temporary checkpoints for Negative, Positive and ALL\n# Creating callback object that can be supplied to the keras fit function. \n# Name of the checkpoint files\ncheckpoint_prefixall = os.path.join(checkpoint_dirall, \"ckpt_\")\n\ncball=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefixall,\n    monitor='loss',\n    save_weights_only=True,\n    save_best_only=True)\n","6c17f2fd":"#All Text\nhistoryall = mall.fit(datasetall, epochs=50,callbacks=[cball],verbose=1)\nprint(\"training complete for ALL Descriptions.\")\n","76a2f871":"\nhistoryall.history.values() \nhistoryall.history.keys() \n\n\nimport pandas as pd\nlossdfall=pd.DataFrame(historyall.history)\nlossdfall.plot()","e3fec03b":"\nmodelall = tf.keras.Sequential()\nmodelall.add(tf.keras.layers.Embedding(len(vocaball), embedding_dim,\n                          batch_input_shape=[1, None]))\nmodelall.add(tf.keras.layers.LSTM(rnn_units,\n                    return_sequences=True,\n                    stateful=True,\n                    recurrent_initializer='glorot_uniform'))\nmodelall.add(tf.keras.layers.Dense(len(vocaball)))\n#Reloading and changing batch size to 1\nmodelall.load_weights(tf.train.latest_checkpoint(checkpoint_dirall))\nmodelall.build(tf.TensorShape([1, None]))\n","d08f2856":"netflix.description[0]","e82942e4":"#Generating Text for ALL movie descriptions with seed words as the initial movie descriptions \n#Lets generate five sentences, so that i can calculate copus BLEU score\n\nall_text = generate_text(modelall, start_string=netflix.description[0],chr2=char2idxall, idx2=idx2charall)\nprint(all_text)\n","318a5bd1":"print(netflix.description[1])","9f5c4b74":"#Generating Text for ALL movie descriptions with seed words as the initial movie descriptions \n#Lets generate five sentences, so that i can calculate copus BLEU score\n\nall_text = generate_text(modelall, start_string=netflix.description[1],chr2=char2idxall, idx2=idx2charall)\nprint(all_text)\n","4f445ddc":"print(netflix.description[2])","ffc495ee":"#Generating Text for ALL movie descriptions with seed words as the initial movie descriptions \n#Lets generate five sentences, so that i can calculate copus BLEU score\n\nall_text = generate_text(modelall, start_string=netflix.description[2],chr2=char2idxall, idx2=idx2charall)\nprint(all_text)\n","490a2182":"## Designing the Neural Languagage Model","aba73111":"So , I have tried epoch as small as 20 and as high as 100 here. The training loss starts to increase after 45 epochs, thus I have chosen the number of epochs as 50 which is around 45 only.\n\n![image.png](attachment:image.png)","d47f3d37":"One more example and I will end the kernel here -","7b9873b5":"Preparing my model on all description texts -\n\n* Preparing vocabulary with indices\n* Setting up dataset\n* Setting up input and output data for training","ef4e996b":"Since, purpose of me writing up this kernel is to help a reader understand how can a neural network be used to generate engligh language text. I am not going to put more time and effort in reducing the loss further. ","b0f309a9":"# **Writing my own movie descriptions**\n\nGetting the Text data from descriptions of movies given in this dataset, I am going to predict next sentences to them. \n","922ca610":"Lets look at how our **Text Generator** is performing by seeding the moddel with first movie description in the dataset.","0972ada2":"Here you can see that the model is able to formulate proper words most of the time, and semantically the sentence generated is aalso correct. To make the sentences more meaningful, we can train the model further by increasing the number of epochs. Reeader can try this out in his own time.\n\nHere, I am displaying two more samples for your undertanding -\n"}}