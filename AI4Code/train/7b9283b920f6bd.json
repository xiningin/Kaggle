{"cell_type":{"99b3eb9a":"code","e7b0c58a":"code","b34a7aba":"code","2a3b166a":"code","566fa6f2":"code","bc69c748":"code","e87957a1":"code","07678f3a":"code","b9703e41":"code","cde021e5":"code","1269176b":"code","d098b5ce":"code","792cf1f3":"code","d4055166":"code","6bc454d5":"code","16489b27":"code","d711764c":"code","e60a8a54":"code","6af1c3ab":"code","d7a44082":"code","e3ee50f3":"code","de037f68":"code","7ea947da":"code","e8878e39":"code","40c695c8":"code","9627dd1d":"code","ddadbae1":"code","f2bbb754":"code","a5e96ac7":"code","d2c9e83d":"code","d29b68b8":"code","c4169f55":"code","5b022042":"code","3cc701ba":"code","a1baf9d0":"code","f34b5b18":"code","5d46eee7":"code","b3c8c335":"code","17c01990":"code","e419d58f":"code","59c863d8":"code","be011cd5":"code","7537b776":"code","80086194":"code","94b65466":"markdown","64a6b3a0":"markdown","b87e0bba":"markdown","87d23d41":"markdown","303928a9":"markdown","573cf337":"markdown","6587e3b1":"markdown","2b441abc":"markdown","6facd0e1":"markdown","5601611d":"markdown","2b4e0dd0":"markdown","818fcdd7":"markdown","f0769a4f":"markdown"},"source":{"99b3eb9a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        ","e7b0c58a":"training = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\ntraining['train_test'] = 1\ntest['train_test'] = 0\ntest['Survived'] = np.NaN\nall_data = pd.concat([training,test])\n\n%matplotlib inline\nall_data.columns","b34a7aba":"# Understand nature of the data .info() .describe()\n# Histograms and boxplots \n# Value counts \n# Missing data \n# Correlation between the metrics \n# Explore interesting themes \n    # Wealthy survive? \n    # By location \n    # Age scatterplot with ticket price \n    # Young and wealthy Variable? \n    # Total spent? \n# Feature engineering \n# preprocess data together or use a transformer? \n    # use label for train and test   \n# Scaling?\n\n# Model Baseline \n# Model comparison with CV ","2a3b166a":"#quick look at our data types + null counts \ntraining.info()","566fa6f2":"# to better understand the numeric data, we want to use the .describe() method.\ntraining.describe() # This gives us an understanding of the central tendencies of the data ","bc69c748":"# separating numeric columns\ntraining.describe().columns","e87957a1":"# look at numeric and categorical values separately \ndf_num = training[['Age','SibSp','Parch','Fare']]\ndf_cat = training[['Survived','Pclass','Sex','Ticket','Cabin','Embarked']]\n\nfor i in df_num.columns:\n    plt.hist(df_num[i])\n    plt.title(i)\n    plt.show()","07678f3a":"print(df_num.corr())\nsns.heatmap(df_num.corr())","b9703e41":"pd.pivot_table(training, index = 'Survived', values = ['Age','SibSp','Parch','Fare'])","cde021e5":"for i in df_cat.columns:\n    sns.barplot(df_cat[i].value_counts().index,df_cat[i].value_counts()).set_title(i)\n    plt.show()","1269176b":"# Now, let's compare survival and each of these categorical variables on their Pclass\nprint(pd.pivot_table(training, index = 'Survived', columns = 'Pclass', values = 'Ticket' ,aggfunc ='count'))\nprint()\nprint(pd.pivot_table(training, index = 'Survived', columns = 'Sex', values = 'Ticket' ,aggfunc ='count'))\nprint()\nprint(pd.pivot_table(training, index = 'Survived', columns = 'Embarked', values = 'Ticket' ,aggfunc ='count'))","d098b5ce":"df_cat.Cabin\ntraining['cabin_multiple'] = training.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n# after looking at this, we may want to look at cabin by letter or by number.\n\ntraining['cabin_multiple'].value_counts()","792cf1f3":"pd.pivot_table(training, index = 'Survived', columns = 'cabin_multiple', values = 'Ticket' ,aggfunc ='count')","d4055166":"#creates categories based on the cabin letter (n stands for null)\n#in this case we will treat null values like it's own category\n\ntraining['cabin_adv'] = training.Cabin.apply(lambda x: str(x)[0])","6bc454d5":"#comparing surivial rate by cabin\nprint(training.cabin_adv.value_counts())\npd.pivot_table(training,index='Survived',columns='cabin_adv', values = 'Name', aggfunc='count')","16489b27":"#understand ticket values better \n#numeric vs non numeric \ntraining['numeric_ticket'] = training.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ntraining['ticket_letters'] = training.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\n\n","d711764c":"training['numeric_ticket'].value_counts()","e60a8a54":"#lets us view all rows in dataframe through scrolling. This is for convenience \npd.set_option(\"max_rows\", None)\ntraining['ticket_letters'].value_counts()","6af1c3ab":"#difference in numeric vs non-numeric tickets in survival rate \npd.pivot_table(training,index='Survived',columns='numeric_ticket', values = 'Ticket', aggfunc='count')","d7a44082":"#survival rate across different tyicket types \npd.pivot_table(training,index='Survived',columns='ticket_letters', values = 'Ticket', aggfunc='count')","e3ee50f3":"#feature engineering on person's title \ntraining.Name.head(50)\ntraining['name_title'] = training.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n#mr., ms., master. etc","de037f68":"training['name_title'].value_counts()","7ea947da":"#create all categorical variables that we did above for both training and test sets \nall_data['cabin_multiple'] = all_data.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\nall_data['cabin_adv'] = all_data.Cabin.apply(lambda x: str(x)[0])\nall_data['numeric_ticket'] = all_data.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\nall_data['ticket_letters'] = all_data.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\nall_data['name_title'] = all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n\n#impute nulls for continuous data \n#all_data.Age = all_data.Age.fillna(training.Age.mean())\nall_data.Age = all_data.Age.fillna(training.Age.median())\n#all_data.Fare = all_data.Fare.fillna(training.Fare.mean())\nall_data.Fare = all_data.Fare.fillna(training.Fare.median())\n\n#drop null 'embarked' rows. Only 2 instances of this in training and 0 in test \nall_data.dropna(subset=['Embarked'],inplace = True)\n\n#tried log norm of sibsp (not used)\nall_data['norm_sibsp'] = np.log(all_data.SibSp+1)\nall_data['norm_sibsp'].hist()\n\n# log norm of fare (used)\nall_data['norm_fare'] = np.log(all_data.Fare+1)\nall_data['norm_fare'].hist()\n\n# converted fare to category for pd.get_dummies()\nall_data.Pclass = all_data.Pclass.astype(str)\n\n#created dummy variables from categories (also can use OneHotEncoder)\nall_dummies = pd.get_dummies(all_data[['Pclass','Sex','Age','SibSp','Parch','norm_fare','Embarked','cabin_adv','cabin_multiple','numeric_ticket','name_title','train_test']])\n\n#Split to train test again\nX_train = all_dummies[all_dummies.train_test == 1].drop(['train_test'], axis =1)\nX_test = all_dummies[all_dummies.train_test == 0].drop(['train_test'], axis =1)\n\n\ny_train = all_data[all_data.train_test==1].Survived\ny_train.shape","e8878e39":"# Scale data \nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nall_dummies_scaled = all_dummies.copy()\nall_dummies_scaled[['Age','SibSp','Parch','norm_fare']]= scale.fit_transform(all_dummies_scaled[['Age','SibSp','Parch','norm_fare']])\nall_dummies_scaled\n\nX_train_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 1].drop(['train_test'], axis =1)\nX_test_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 0].drop(['train_test'], axis =1)\n\ny_train = all_data[all_data.train_test==1].Survived\n\n","40c695c8":"from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC","9627dd1d":"#Using Naive Bayes as a baseline for my classification tasks \ngnb = GaussianNB()\ncv = cross_val_score(gnb,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","ddadbae1":"lr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","f2bbb754":"lr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","a5e96ac7":"dt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","d2c9e83d":"dt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","d29b68b8":"knn = KNeighborsClassifier()\ncv = cross_val_score(knn,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","c4169f55":"knn = KNeighborsClassifier()\ncv = cross_val_score(knn,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","5b022042":"rf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","3cc701ba":"rf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","a1baf9d0":"svc = SVC(probability = True)\ncv = cross_val_score(svc,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","f34b5b18":"from xgboost import XGBClassifier\nxgb = XGBClassifier(random_state =1)\ncv = cross_val_score(xgb,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","5d46eee7":"#Voting classifier takes all of the inputs and averages the results.\n# For a \"hard\" voting classifier each classifier gets 1 vote \"yes\" or \"no\" and the result is just a popular vote.\n# For this, you generally want odd numbers\n# A \"soft\" classifier averages the confidence of each of the models. If a the average confidence is > 50% that it is a 1 it will be counted as such\nfrom sklearn.ensemble import VotingClassifier\nvoting_clf = VotingClassifier(estimators = [('lr',lr),('knn',knn),('rf',rf),('gnb',gnb),('svc',svc),('xgb',xgb)], voting = 'soft') ","b3c8c335":"cv = cross_val_score(voting_clf,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","17c01990":"voting_clf.fit(X_train_scaled,y_train)\ny_hat_base_vc = voting_clf.predict(X_test_scaled).astype(int)\nbasic_submission = {'PassengerId': test.PassengerId, 'Survived': y_hat_base_vc}\nbase_submission = pd.DataFrame(data=basic_submission)\nbase_submission.to_csv('base_submission.csv', index=False)","e419d58f":"from sklearn.model_selection import GridSearchCV \nfrom sklearn.model_selection import RandomizedSearchCV \n\n#simple performance reporting function\ndef clf_performance(classifier, model_name):\n    print(model_name)\n    print('Best Score: ' + str(classifier.best_score_))\n    print('Best Parameters: ' + str(classifier.best_params_))\n    \n    \n\nlr = LogisticRegression()\nparam_grid = {'max_iter' : [2000],\n              'penalty' : ['l1', 'l2'],\n              'C' : np.logspace(-4, 4, 20),\n              'solver' : ['liblinear']}\n\nclf_lr = GridSearchCV(lr, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_lr = clf_lr.fit(X_train_scaled,y_train)\nclf_performance(best_clf_lr,'Logistic Regression')","59c863d8":"knn = KNeighborsClassifier()\nparam_grid = {'n_neighbors' : [3,5,7,9],\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree','kd_tree'],\n              'p' : [1,2]}\nclf_knn = GridSearchCV(knn, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_knn = clf_knn.fit(X_train_scaled,y_train)\nclf_performance(best_clf_knn,'KNN')","be011cd5":"svc = SVC(probability = True)\nparam_grid = tuned_parameters = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10],\n                                  'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['poly'], 'degree' : [2,3,4,5], 'C': [.1, 1, 10, 100, 1000]}]\nclf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_svc = clf_svc.fit(X_train_scaled,y_train)\nclf_performance(best_clf_svc,'SVC')","7537b776":"rf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [400,450,500,550],\n               'criterion':['gini','entropy'],\n                                  'bootstrap': [True],\n                                  'max_depth': [15, 20, 25],\n                                  'max_features': ['auto','sqrt', 10],\n                                  'min_samples_leaf': [2,3],\n                                  'min_samples_split': [2,3]}\n                                  \nclf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_rf = clf_rf.fit(X_train_scaled,y_train)\nclf_performance(best_clf_rf,'Random Forest')","80086194":"best_rf = best_clf_rf.best_estimator_.fit(X_train_scaled,y_train)\nfeat_importances = pd.Series(best_rf.feature_importances_, index=X_train_scaled.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","94b65466":"## Data Preprocessing TO-DO's for Model::\n### 1) Drop null values from Embarked (only 2) \n\n### 2) Include only relevant variables (Since we have limited data, I wanted to exclude things like name and passanger ID so that we could have a reasonable number of features for our models to deal with) \nVariables:  'Pclass', 'Sex','Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'cabin_adv', 'cabin_multiple', 'numeric_ticket', 'name_title'\n\n### 3) Do categorical transforms on all data. Usually we would use a transformer, but with this approach we can ensure that our traning and test data have the same colums. We also may be able to infer something about the shape of the test data through this method. I will stress, this is generally not recommend outside of a competition (use onehot encoder). \n\n### 4) Impute data with mean for fare and age (Should also experiment with median) \n\n### 5) Normalized fare using logarithm to give more semblance of a normal distribution \n\n### 6) Scaled data 0-1 with standard scaler \n\n","64a6b3a0":"## Model Building (Baseline Validation Performance based on the tutorial)\nBefore going further, I like to see how various different models perform with default parameters. I tried the following models using 5 fold cross validation to get a baseline. With a validation set basline, we can see how much tuning improves each of the models. Just because a model has a high basline on this validation set doesn't mean that it will actually do better on the eventual test set. \n\n- Naive Bayes (72.6%)\n- Logistic Regression (82.1%)\n- Decision Tree (77.6%)\n- K Nearest Neighbor (80.5%)\n- Random Forest (80.6%)\n- **Support Vector Classifier (83.2%)**\n- Xtreme Gradient Boosting (81.8%)\n- Soft Voting Classifier - All Models (82.8%)","b87e0bba":"## Project Planning\nFor this competition as well as this final project, below is the rough outline for the remaining code for project planning purposes.","87d23d41":"# Now, let's compare survival rate across Age, SibSp, Parch, and Fare below:::","303928a9":"\n### From here, it brings to questions several concerns:\n### 1) Does the type of cabin impact survival rates? \n\n### 2) Tickets - Do different ticket types impact survival rates?\n\n### 3) Does a person's title relate to survival rates? ","573cf337":"## Model Tuned Performance \nAfter getting the baselines, let's see if we can improve on the indivdual model results!I mainly used grid search to tune the models. I also used Randomized Search for the Random Forest and XG boosted model to simplify testing time. \n\n|Model|Baseline|Tuned Performance|\n|-----|--------|-----------------|\n|Naive Bayes| 72.6%| NA|\n|Logistic Regression| 82.1%| 82.6%|\n|Decision Tree| 77.6%| NA|\n|K Nearest Neighbor| 80.5%|83.0%|\n|Random Forest| 80.6%| 83.6|\n|Support Vector Classifier| 83.2%| 83.2%|\n|Xtreme Gradient Boosting| 81.8%| 85.3%|\n\n","6587e3b1":"# Hello!\n\n# My name is Aldrin Brillante. This is my Machine Learning Final Project.\n\n# Assistance with this project was given with a tutorial on the processing datasets in kaggle along with some titanic dataset help. With the help of this tutorial and the content I learned in ACS, I was able to apply what I've learned from class to use training sets to predict who died during the crash of the Titanic.\n\n# This project gave me the opportunity to also participate in my first Kaggle competition, **\"TITANIC - MACHINE LEARNING FROM DISASTER\"** linked here: https:\/\/www.kaggle.com\/c\/titanic.\n\nALL DATA NEEDED is available on the \"data\" tab on the link provided.\n\nThe idea with this competition\/dataset is ***we are questioning WHO survived using test sets***. They give us a training set, we train our model, and we actually try to predict which of the people in the test set survived this crash. Sounds cool, right? Let's get started!\n\n# As per the instruction of the final project documentation, here is the info needed:\n1. **Where your data is coming from?**** The data is coming from the datasets avaialble at https:\/\/www.kaggle.com\/c\/titanic at the data tab, or more specifically pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n\n2. **What questions were you looking to answer and predictions you wanted to make?**** THe question I wanted to answer by intergrating machine learning project with the titanic dataset was to answer the question of WHO survived using a variety of test sets. As per the competition I am following, they give us the training set, we train the model, and we actually predict which of the people in the test set survived the crash.\n\n3. **Graphs and other visualizations used throughout project?**** Throughout this project, there will be usage of histograms, heatmaps, barplots, and pivot tables to properly categorize and analyze the data. GRAPHS ARE AVAILABLE AND SEEN THROUGHOUT THE CODE\n* For numeric data, I used:\n    - histograms to understand distributions\n    - Pivot table comparing survival rate across numeric variables\n    \n* For categorical data, I used:\n    - bar charts to understand balance of classes\n    - pivot tables to understand relationship with survival\n\n","2b441abc":"# ***EXECUTIVE SUMMARY:***\n\nI used this ML DS FInal Project as an opportunity to finally submit my very first Kaggle competition submission. *As per the instruction of the executive summary, I am tasked with stating my dataset, my methodology, and my findings*. Firstly, the ***dataset*** I used is specifically linked with the Kaggle competition for \"Titanic - Machine Learning From Disaster\" in which I was also able to recevie assistance from a data science tutorial as well as intergrating what I have learned throughout my time witht he Data Science curriculum. The data set is available with the kaggle competition as well as on the data tab of the link provided (https:\/\/www.kaggle.com\/c\/titanic). Regarding ***methodologies*** I used throughout this project, I used a variety. To begin, I integrated histograms and boxplots to be able to compare and contrast the variety of subgroups within the titanic dataset. I wanted to see a clear answer as to which groups were more likely to survive from the dataset (ie welthy vs poor, cabin class, vs gender etc). I also used support vector classifier so we can see how much tuning improves each of the models throughout the training. Majority of my comparison were with tables and histograms of different groups to see the chances of survavibility. From my ***findings***,there were specific variables that had the greatest impact on predicting if someone will survive or not. From the data, the variables of how much a person paid for their fare, their age, their title of \"Mr.\", the male sex, and third class seats (normal 'economy' class) were the largest impacts of their survivability, meaning they had higher chances of surviving. The next large imapcts after those were if the passengers has children of if they had multiple cabins.  \n\n![image.png](attachment:d990c9a4-4ad4-4170-9570-dc5c2184e1b8.png)","6facd0e1":"# **Histogram analysis: the histograms above show the istributions for all numeric variables \n# \n# **","5601611d":"### Now, let's compare surivial rate by cabin: ","2b4e0dd0":"**Here we import the data.**","818fcdd7":"Analysis: above are the cabin and ticket graphs. Although they are very messy, it helps us understand the various distribution between different data groups. ","f0769a4f":"# To run the code below, please\n\n**- sign in to Kaggle, and**\n\n**- choose the \"Copy and Edit\" option on the top right to be able to run the below code on your own.**\n\n# Happy Hacking!"}}