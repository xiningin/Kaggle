{"cell_type":{"08e63d3d":"code","43af1b59":"code","396ed424":"code","3782ad6b":"code","1de92b4b":"code","15b17683":"code","5e079243":"code","c64c6887":"code","6cae3ca0":"code","e91e058f":"code","3d1183f6":"code","afc4b0ab":"code","c91eb6a1":"code","6f786ef8":"code","b7a6572e":"code","99e2b967":"code","14253d33":"markdown","48e7a048":"markdown","4de619e1":"markdown","5ab93eb4":"markdown","8b4ccbd5":"markdown","4ffca76e":"markdown"},"source":{"08e63d3d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix, classification_report,accuracy_score\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression","43af1b59":"dataset = pd.read_csv('..\/input\/indian_liver_patient.csv')\ndataset.head()","396ed424":"dataset.info()","3782ad6b":"# fill the missing values with mean of the coressponding column\ndataset['Albumin_and_Globulin_Ratio'] = dataset.Albumin_and_Globulin_Ratio.fillna(value = dataset['Albumin_and_Globulin_Ratio'].mean())","1de92b4b":"# let's build a correlation matrix and use seaborn to plot the heatmap of these\n# correlation matrix\ncorr_matrix = dataset.corr()\nsns.heatmap(corr_matrix,cmap=\"YlGnBu\")\n# from the heatmap, dark shades represent positive correlation and light shades represent negative correlation","15b17683":"fig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(corr_matrix,cmap=\"YlGnBu\", annot=True, linewidths=.5, ax = ax)","5e079243":"# classifying data into independent and dependent variable\nX = dataset.drop(['Dataset'],axis = 1).values\ny = dataset['Dataset'].values","c64c6887":"# encoding the categorical data of Gender varaible\nlabelencoder = LabelEncoder()\nX[:,1] = labelencoder.fit_transform(X[:,1])\nonehotencoder = OneHotEncoder(categorical_features = [1])\nX = onehotencoder.fit_transform(X).toarray()","6cae3ca0":"# avoid dummy varaible trap\nX = X[:,1:]","e91e058f":"# creating test and training set data\nX_train,X_test,y_train,y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","3d1183f6":"# feature scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","afc4b0ab":"rf_classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nrf_classifier.fit(X_train,y_train)\ny_pred = rf_classifier.predict(X_test)\nrf_cm = confusion_matrix(y_test, y_pred)\nrf_cr = classification_report(y_test, y_pred)\nprint(\"Accuracy Score in percentage : \\n\",round(accuracy_score(y_test,y_pred) * 100,2))\nprint(\"Random Forrest Confusion Matrix : \\n\",rf_cm)\nprint(\"Random Forrest Classification Report : \\n\",rf_cr)","c91eb6a1":"# let's remove the highly correlated params and apply random forrest see whether the precision improves\n# classifying data into independent and dependent variable\nX_dropped = dataset.drop(['Dataset','Direct_Bilirubin','Aspartate_Aminotransferase','Albumin'],axis = 1).values\ny_dropped = dataset['Dataset'].values\n\n# encoding the categorical data of Gender varaible\nlabelencoder = LabelEncoder()\nX_dropped[:,1] = labelencoder.fit_transform(X_dropped[:,1])\nonehotencoder = OneHotEncoder(categorical_features = [1])\nX_dropped = onehotencoder.fit_transform(X_dropped).toarray()\n\n# avoid dummy varaible trap\nX_dropped = X_dropped[:,1:]\n\n# creating test and training set data\nX_train_dropped,X_test_dropped,y_train_dropped,y_test_dropped = train_test_split(X_dropped, y_dropped, test_size = 0.2, random_state = 0)\n\n# feature scaling\nsc = StandardScaler()\nX_train_dropped = sc.fit_transform(X_train_dropped)\nX_test_dropped = sc.transform(X_test_dropped)","6f786ef8":"rf_classifier_dropped = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nrf_classifier_dropped.fit(X_train_dropped,y_train_dropped)\ny_pred_dropped = rf_classifier_dropped.predict(X_test_dropped)\nrf_cm_dropped = confusion_matrix(y_test_dropped, y_pred_dropped)\nrf_cr_dropped = classification_report(y_test_dropped, y_pred_dropped)\nprint(\"Accuracy Score in percentage : \\n\",round(accuracy_score(y_test_dropped,y_pred_dropped) * 100,2))\nprint(\"Random Forrest Confusion Matrix : \\n\",rf_cm_dropped)\nprint(\"Random Forrest Classification Report : \\n\",rf_cr_dropped)","b7a6572e":"# let's create Naive Bayes model and fit the dataset\nclassifier_NB = GaussianNB()\nclassifier_NB.fit(X_train,y_train)\ny_pred = classifier_NB.predict(X_test)\ngnb_cm = confusion_matrix(y_test, y_pred)\ngnb_cr = classification_report(y_test, y_pred)\nprint(\"Accuracy Score in percentage : \\n\",round(accuracy_score(y_test,y_pred) * 100,2))\nprint(\"Naive Bayes Confusion Matrix : \\n\",gnb_cm)\nprint(\"Naive Bayes Classification Report : \\n\",gnb_cr)","99e2b967":"# applying logistic regression model to training set\n# here we are considering dropped varaibles which are correlated\nclassifier_logistic = LogisticRegression()\nclassifier_logistic.fit(X_train_dropped,y_train_dropped)\ny_pred_dropped = classifier_logistic.predict(X_test_dropped)\nlog_cm = confusion_matrix(y_test_dropped, y_pred_dropped)\nlog_cr = classification_report(y_test_dropped, y_pred_dropped)\nprint(\"Accuracy Score in percentage : \\n\",round(accuracy_score(y_test_dropped,y_pred_dropped) * 100,2))\nprint(\"Logistic Regression Confusion Matrix : \\n\",log_cm)\nprint(\"Logistic Regression Classification Report : \\n\",log_cr)\n","14253d33":"after removing the correlated variables, accuracy decreased to 62% from 71%","48e7a048":"from the Naive Bayes model, accuracy is at 60%","4de619e1":"from the above we can see that 'Albumin_and_Globulin_Ratio' prpoerty have some value missing","5ab93eb4":"from the above heatmap following independent varaibles are highly correlated\n1. Total_Bilirubin &  Direct_Bilirubin\n2. Alamine_Aminotransferase & Aspartate_Aminotransferase\n3. Total_Proteins & Albumin\n4. Albumin & Albumin_and_Globulin_Ratio","8b4ccbd5":"from the classification report, random forrest classifier is 71% accurate","4ffca76e":"accuracy of logistic regression is 68%"}}