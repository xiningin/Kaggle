{"cell_type":{"c305559f":"code","14057412":"code","13ec3f65":"code","55803c78":"code","488a863d":"code","83966467":"code","faf7536f":"code","c04759c5":"code","0d23ea42":"code","3d1e70b0":"code","aa806339":"code","a0f44b4b":"code","ea01b3f4":"code","38767394":"code","90958342":"code","39792117":"code","60827739":"code","586859d8":"code","22541a13":"markdown","62dbcc07":"markdown","99f05d27":"markdown","57887037":"markdown","ab5eec19":"markdown","8b41ceaf":"markdown","5e4c7152":"markdown","d22d8742":"markdown","3d94ac49":"markdown","ebdd8116":"markdown"},"source":{"c305559f":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom scipy.stats import probplot\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRFRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRFRegressor, XGBRegressor\nimport catboost as ctb\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder\nfrom sklearn.preprocessing import StandardScaler","14057412":"df = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/akhil14shukla\/Summer-of-Analytics-IITG-Project\/master\/Train_Data.csv\")\ntest=pd.read_csv(\"https:\/\/raw.githubusercontent.com\/akhil14shukla\/Summer-of-Analytics-IITG-Project\/master\/Test_Data.csv\")","13ec3f65":"df.shape","55803c78":"fig, axes = plt.subplots(4, 4, figsize=(25, 20))\nfor i,ax in zip(df.columns,axes.flat):\n    sns.histplot(df[i],ax=ax)\n#     probplot(df[i],dist='norm',plot=ax)\n#     probplot(np.log((df.iloc[:,i])),dist='norm',plot=ax)\n    ax.set_title(i)\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()","488a863d":"sns.scatterplot(df['adgroup'],df['revenue'])","83966467":"le = LabelEncoder()\n# lb = LabelBinarizer()\ndf['adgroup'] = le.fit_transform(df['adgroup'])\ntest['adgroup']=le.transform(test['adgroup'])\n# df = df.join(pd.DataFrame(lb.fit_transform(df['adgroup'])))\n# test = test.join(pd.DataFrame(lb.transform(test['adgroup'])))\n# df.drop(['adgroup'],axis=1)\n# test.drop(['adgroup'],axis=1)\ndf = df.set_index(df['date'])\ntest = test.set_index(test['date'])\n\ndf[\"impressions\"] = np.log(df[\"impressions\"])\ndf[\"clicks\"] = np.log(df[\"clicks\"])\ndf[\"cost\"] = np.log(df[\"cost\"])\n\ntest[\"impressions\"] = np.log(test[\"impressions\"])\ntest[\"clicks\"] = np.log(test[\"clicks\"])\ntest[\"cost\"] = np.log(test[\"cost\"])\n\ndf.drop(['date', 'campaign'],axis=1,inplace=True)\ntest.drop(['date','campaign'],axis=1,inplace=True)\n","faf7536f":"for i in range(len(df)):\n    df['ad'][i] = int(df['ad'][i][3:])\nfor i in range(len(test)):\n    test['ad'][i]=int(test['ad'][i][3:])\ndf['ad']=pd.to_numeric(df['ad'])\ntest['ad']=pd.to_numeric(test['ad'])\ndf.drop(['ad'],axis=1,inplace=True)","c04759c5":"# sns.pairplot(df,hue=\"revenue\")","0d23ea42":"df['CTR']=df['clicks']\/df['impressions']\ndf['CPC']=df['cost']\/df['clicks']\ndf['CPA']=df['cost']\/df['conversions']\n\ntest['CTR']=test['clicks']\/test['impressions']\ntest['CPC']=test['cost']\/test['clicks']\ntest['CPA']=test['cost']\/test['conversions']\n\ndf['CPC'].fillna(df.CPC.interpolate(),inplace=True)\ndf['CPA'].fillna(df.CPA.interpolate(),inplace=True)\n\ntest['CPC'].fillna(test.CPC.interpolate(),inplace=True)\ntest['CPA'].fillna(test.CPA.interpolate(),inplace=True)\n\ndf.replace([-np.inf], 0,inplace=True)\ntest.replace([-np.inf], 0,inplace=True) \ndf.replace([np.inf,np.nan], 65,inplace=True)\ntest.replace([np.inf,np.nan], 65,inplace=True)","3d1e70b0":"sns.heatmap(df.corr(),cmap=\"YlGnBu\")","aa806339":"sns.pairplot(df)","a0f44b4b":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import *\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn import metrics               \nfrom sklearn import preprocessing\nfrom sklearn import utils\nfrom sklearn.metrics import mean_absolute_error\n\n","ea01b3f4":"# df = np.array(df)\nfrom sklearn_pandas import DataFrameMapper","38767394":"g = df.groupby('adgroup')\n\n# model0 = XGBRFRegressor(n_estimators=10,max_depth=6, min_child_weight=2,subsample=0,learning_rate = 0.3)\n# model0 = lg(kernel='rbf', degree=2, gamma='auto', C = 0.1)\nj=6\n# model0 = ctb.CatBoostRegressor(random_state=0,verbose=False,depth=4,l2_leaf_reg=5,learning_rate=0.4, n_estimators=10000)\n# model1 = ctb.CatBoostRegressor(random_state=0,verbose=False,depth=6,l2_leaf_reg=3,learning_rate=0.01, n_estimators=10000,)\n# model2 = ctb.CatBoostRegressor(random_state=0,verbose=False,depth=5,l2_leaf_reg=8,learning_rate=0.03, n_estimators=10000)\n# model3 = ctb.CatBoostRegressor(random_state=0,verbose=False,depth=5,l2_leaf_reg=8,learning_rate=0.01, n_estimators=10000)\n\nmodel0 = XGBRFRegressor(n_estimators=4000)\nmodel1 = XGBRFRegressor(n_estimators=4000)\nmodel2 = XGBRFRegressor(n_estimators=4000)\nmodel3 = XGBRFRegressor(n_estimators=4000)\n\ndf0 = pd.DataFrame(g.get_group(0))\ny0 = df0['revenue']\ndf0.drop(['revenue'],axis=1,inplace=True)\nmapper0 = DataFrameMapper([(df0.columns, StandardScaler())])\nscaled_features = mapper0.fit_transform(df0.copy())\ndf0 = pd.DataFrame(scaled_features, columns=df0.columns)\n\ndf1 = pd.DataFrame(g.get_group(1))\ny1 = df1['revenue']\ndf1.drop(['revenue'],axis=1,inplace=True)\nmapper1 = DataFrameMapper([(df1.columns, StandardScaler())])\nscaled_features = mapper1.fit_transform(df1.copy())\ndf1 = pd.DataFrame(scaled_features, columns=df1.columns)\n\ndf2 = pd.DataFrame(g.get_group(2))\ny2 = df2['revenue']\ndf2.drop(['revenue'],axis=1,inplace=True)\nmapper2 = DataFrameMapper([(df2.columns, StandardScaler())])\nscaled_features = mapper2.fit_transform(df2.copy())\ndf2 = pd.DataFrame(scaled_features, columns=df2.columns)\n\ndf3 = pd.DataFrame(g.get_group(3))\ny3 = df3['revenue']\ndf3.drop(['revenue'],axis=1,inplace=True)\nmapper3 = DataFrameMapper([(df3.columns, StandardScaler())])\nscaled_features = mapper3.fit_transform(df3.copy())\ndf3 = pd.DataFrame(scaled_features, columns=df3.columns)\n\n# data[f\"df{i}\"]\n\nfor i in range(4):\n    # d_train,d_test,y1_train,y1_test = train_test_split(df,y,random_state=0)\n    grid = {'learning_rate': [0.01, 0.03, 0.1],\n        'depth': [4, 6, 8],\n        'l2_leaf_reg': [0.5, 1, 3],\n        'iteration' : [1500,2000]}\n    if(i==0):\n        d_train,d_test,y1_train,y1_test = train_test_split(df0,y0,random_state=0)\n        model0.fit(d_train,y1_train)\n        # grid_search_result = model0.grid_search(grid, \n        #                                X=d_train, \n        #                                y=y1_train, \n        #                                verbose=False,refit=True, search_by_train_test_split=True)\n        y_pred=model0.predict(d_test)\n        \n    elif (i==1):\n        d_train,d_test,y1_train,y1_test = train_test_split(df1,y1,random_state=0)\n        model1.fit(d_train,y1_train)\n        # grid_search_result = model1.grid_search(grid, \n        #                                X=d_train, \n        #                                y=y1_train, \n        #                                verbose=False,refit=True, search_by_train_test_split=True)\n        y_pred=model1.predict(d_test)\n    elif (i==2):\n        d_train,d_test,y1_train,y1_test = train_test_split(df2,y2,random_state=0)\n        model2.fit(d_train,y1_train)\n        # grid_search_result = model2.grid_search(grid, \n        #                                X=d_train, \n        #                                y=y1_train, \n        #                                verbose=False,refit=True, search_by_train_test_split=True)\n        y_pred=model2.predict(d_test)\n    elif (i==3):\n        d_train,d_test,y1_train,y1_test = train_test_split(df3,y3,random_state=0)\n        model3.fit(d_train,y1_train)\n        # grid_search_result = model3.grid_search(grid, \n        #                                X=d_train, \n        #                                y=y1_train, \n        #                                verbose=False,refit=True, search_by_train_test_split=True) \n        y_pred=model3.predict(d_test)\n    \n    for i in range(len(d_test)):\n        if(y_pred[i]<0):\n            y_pred[i] = 0\n    rms = mean_squared_error(y1_test, y_pred, squared=False)\n    print(rms)\n","90958342":"y_pred=[]\n# x_test.drop('revenue',axis=1,inplace=True)\nx_test = test\nfor i in range(len(x_test)):\n    if(x_test['adgroup'][i]==0):\n        curr = mapper0.transform(pd.DataFrame(x_test.iloc[i]).transpose())\n        y_pred = np.append(y_pred,model0.predict(curr)[0])\n    elif (x_test['adgroup'][i]==1):\n        curr = mapper1.transform(pd.DataFrame(x_test.iloc[i]).transpose())\n        y_pred = np.append(y_pred,model1.predict(curr)[0])\n    elif (x_test['adgroup'][i]==2):\n        curr = mapper2.transform(pd.DataFrame(x_test.iloc[i]).transpose())\n        y_pred = np.append(y_pred,model2.predict(curr)[0])\n    elif (x_test['adgroup'][i]==3):\n        curr = mapper3.transform(pd.DataFrame(x_test.iloc[i]).transpose())\n        y_pred = np.append(y_pred,model3.predict(curr)[0])\n","39792117":"for i in range(len(y_pred)):\n    if(y_pred[i]<0):\n        y_pred[i]=0","60827739":"y_pred.shape# checking the shape of the final array","586859d8":"pd.DataFrame({'revenue':y_pred}).to_csv(\"submission_4.csv\") # Exporting to CSV file","22541a13":"**Created a custom function for finding Predictive Power Score. Check out the [repository here](https:\/\/github.com\/akhil14shukla\/PyCustom).**","62dbcc07":"## Exporting to CSV","99f05d27":"Model might predict negative revenue in some cases, to take care of that, replaced negative values with 0.","57887037":"Removed the \"ad \" from _ad_ column and converted the rest to numerical type","ab5eec19":"## Predicting\nPredicting the values for Test Dataset. <br><br>\nIterated through each value of test data set. and depending on its _adgroup_, applied corresponding scaling (using the model created before this cell), and predicted the value using the corresponding model.","8b41ceaf":"## Building the model\nI tried various models CatBoost, XGBoost, RandomForest, XGB turned out to be the best.<br><br>\nThinking behind building the model:<br>\nCreated 4 model for each _adgroup_. As each _adgroup_ might have different distribution or different approach of advertising, which might generate different revenues. To do that, first grouped them based on _adgroup_, then created 4 models, trained them on each _adgroup_(4 in this case) one by one using _for_ loop.<br> \nBut before doing that we need to scale the data, for better performance. Used _StandardScaler()_, so any remaining outliers will be taken care of. But as we are training different models, I decided to use different scaling for each _adgroup_. It seems a lot of work but it was finally worth it.","5e4c7152":"## Understanding the Data","d22d8742":"Used the below library, to ease working with Standard Scaler","3d94ac49":"## Data Cleaning and Transformation\n\nPerformed label encoding, dropped the columns (_campaign_ had same value for all the tuples), created new features, which might be useful while building a model.","ebdd8116":"Created new features, and while doing so, we created infinite or NaN values unknowingly. So, replaced those values, as these are not accepted by models. <br>\nreplaced -infinity with 0 and infinity and NaN with a number greater than the current maximum (randomly chose 65, it could be 100, 10000,...). We just need to preserve the essence of _-infinity_ and _infinity_"}}