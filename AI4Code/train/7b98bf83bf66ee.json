{"cell_type":{"1c9e78a9":"code","26b5ca6e":"code","48fe0166":"code","f1e92233":"code","e2258ada":"code","d9367198":"code","ffc31ffc":"code","dec6798a":"code","5cf56600":"code","2c68a9cc":"code","06e2c43b":"code","9497cf57":"code","06d4c619":"code","9b9d8e0c":"code","33e1511e":"markdown","ac0920a9":"markdown","8519b59a":"markdown","b3136dfa":"markdown"},"source":{"1c9e78a9":"# TabNet\n!pip install pytorch-tabnet","26b5ca6e":"import os\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport time\nfrom datetime import datetime\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nfrom pytorch_tabnet.pretraining import TabNetPretrainer\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")","48fe0166":"pd.options.display.max_rows = None\npd.options.display.max_columns = None","f1e92233":"def timer(myFunction):\n    def functionTimer(*args, **kwargs):\n        start_time = time.time()\n        result = myFunction(*args, **kwargs)\n        end_time = time.time()\n        computation_time = round(end_time - start_time, 2)\n        print(\"{} is excuted\".format(myFunction.__name__))\n        print('Computation took: {:.2f} seconds'.format(computation_time))\n        return result\n    return functionTimer","e2258ada":"@timer\ndef prepareInputs(df: \"pd.dataFrame\") -> \"pd.dataFrame\":\n    \"\"\"Prepare the input for training\n\n    Args:\n        df (pd.DataFrame): raw data\n        \n    Process:\n        1. Exclude missing values\n        2. Clean the target variable\n        3. Create dummy variables for categorical variables\n        4. Create age features\n        5. Impute missing value\n    \n    Return: pd.dataFrame\n    \"\"\"\n    \n    # 1. Exclude missing values\n    df = df[df[\"POL_STATUS\"].notnull()]\n    \n    # 2. Clean the target variable\n    df = df[df[\"POL_STATUS\"] != \"Unknown\"]\n    df[\"lapse\"] = np.where(df[\"POL_STATUS\"] == \"Lapsed\", 1, 0)\n    \n    # 3. Create dummy variables for categorical variables\n    categorical_cols = [\"CLAIM3YEARS\", \"BUS_USE\", \"AD_BUILDINGS\",\n                        \"APPR_ALARM\", \"CONTENTS_COVER\", \"P1_SEX\",\n                        \"BUILDINGS_COVER\", \"P1_POLICY_REFUSED\", \n                        \"APPR_LOCKS\", \"FLOODING\",\n                        \"NEIGH_WATCH\", \"SAFE_INSTALLED\", \"SEC_DISC_REQ\",\n                        \"SUBSIDENCE\", \"LEGAL_ADDON_POST_REN\", \n                        \"HOME_EM_ADDON_PRE_REN\",\"HOME_EM_ADDON_POST_REN\", \n                        \"GARDEN_ADDON_PRE_REN\", \"GARDEN_ADDON_POST_REN\", \n                        \"KEYCARE_ADDON_PRE_REN\", \"KEYCARE_ADDON_POST_REN\", \n                        \"HP1_ADDON_PRE_REN\", \"HP1_ADDON_POST_REN\",\n                        \"HP2_ADDON_PRE_REN\", \"HP2_ADDON_POST_REN\", \n                        \"HP3_ADDON_PRE_REN\", \"HP3_ADDON_POST_REN\", \n                        \"MTA_FLAG\", \"OCC_STATUS\", \"OWNERSHIP_TYPE\",\n                        \"PROP_TYPE\", \"PAYMENT_METHOD\", \"P1_EMP_STATUS\",\n                        \"P1_MAR_STATUS\"\n                        ]\n    \n    for col in categorical_cols:\n        dummies = pd.get_dummies(df[col], \n                                 drop_first = True,\n                                 prefix = col\n                                )\n        df = pd.concat([df, dummies], 1)\n    \n    # 4. Create age features\n    df[\"age\"] = (datetime.strptime(\"2013-01-01\", \"%Y-%m-%d\") - pd.to_datetime(df[\"P1_DOB\"])).dt.days \/\/ 365\n    df[\"property_age\"] = 2013 - df[\"YEARBUILT\"]\n    df[\"cover_length\"] = 2013 - pd.to_datetime(df[\"COVER_START\"]).dt.year\n    \n    # 5. Impute missing value\n    df[\"RISK_RATED_AREA_B_imputed\"] = df[\"RISK_RATED_AREA_B\"].fillna(df[\"RISK_RATED_AREA_B\"].mean())\n    df[\"RISK_RATED_AREA_C_imputed\"] = df[\"RISK_RATED_AREA_C\"].fillna(df[\"RISK_RATED_AREA_C\"].mean())\n    df[\"MTA_FAP_imputed\"] = df[\"MTA_FAP\"].fillna(0)\n    df[\"MTA_APRP_imputed\"] = df[\"MTA_APRP\"].fillna(0)\n\n    return df","d9367198":"# Split train and test\n@timer\ndef splitData(df: \"pd.DataFrame\", FEATS: \"list\"):\n    \"\"\"Split the dataframe into train and test\n    \n    Args:\n        df: preprocessed dataframe\n        FEATS: feature list\n        \n    Returns:\n        X_train, y_train, X_test, y_test\n    \"\"\"\n    \n    train, test = train_test_split(df, test_size = .3, random_state = 42)\n    train, test = prepareInputs(train), prepareInputs(test)\n    \n    return train[FEATS], train[\"lapse\"], test[FEATS], test[\"lapse\"]","ffc31ffc":"# Standardise the data sets\n@timer\ndef standardiseNumericalFeats(X_train, X_test):\n    \"\"\"Standardise the numerical features\n    \n    Returns:\n        Standardised X_train and X_test\n    \"\"\"\n\n    numerical_cols = [\n        \"age\", \"property_age\", \"cover_length\", \"RISK_RATED_AREA_B_imputed\", \n        \"RISK_RATED_AREA_C_imputed\", \"MTA_FAP_imputed\", \"MTA_APRP_imputed\",\n        \"SUM_INSURED_BUILDINGS\", \"NCD_GRANTED_YEARS_B\", \"SUM_INSURED_CONTENTS\", \n        \"NCD_GRANTED_YEARS_C\", \"SPEC_SUM_INSURED\", \"SPEC_ITEM_PREM\", \n        \"UNSPEC_HRP_PREM\", \"BEDROOMS\", \"MAX_DAYS_UNOCC\", \"LAST_ANN_PREM_GROSS\"\n    ]\n\n    for col in numerical_cols:\n        scaler = StandardScaler()\n\n        X_train[col] = scaler.fit_transform(X_train[[col]])\n        X_test[col] = scaler.transform(X_test[[col]])\n        \n    return X_train, X_test","dec6798a":"@timer\ndef tabNetPretrain(X_train):\n    \"\"\"Pretrain TabNet model\n    \n    Return:\n        TabNet pretrainer obj\n    \"\"\"\n    tabnet_params = dict(n_d=8, n_a=8, n_steps=3, gamma=1.3,\n                             n_independent=2, n_shared=2,\n                             seed=42, lambda_sparse=1e-3,\n                             optimizer_fn=torch.optim.Adam,\n                             optimizer_params=dict(lr=2e-2,\n                                                   weight_decay=1e-5\n                                                  ),\n                             mask_type=\"entmax\",\n                             scheduler_params=dict(max_lr=0.05,\n                                                   steps_per_epoch=int(X_train.shape[0] \/ 256),\n                                                   epochs=200,\n                                                   is_batch_level=True\n                                                  ),\n                             scheduler_fn=torch.optim.lr_scheduler.OneCycleLR,\n                             verbose=10\n                        )\n\n    pretrainer = TabNetPretrainer(**tabnet_params)\n\n    pretrainer.fit(\n        X_train=X_train.to_numpy(),\n        eval_set=[X_train.to_numpy()],\n        max_epochs = 100,\n        patience = 10, \n        batch_size = 256, \n        virtual_batch_size = 128,\n        num_workers = 1, \n        drop_last = True)\n    \n    return pretrainer","5cf56600":"@timer\ndef trainTabNetModel(X_train, y_train, pretrainer):\n    \"\"\"Train TabNet model\n    \n    Args:\n        pretrainer: pretrained model. If not using this, use None\n        \n    Return:\n        TabNet model obj\n    \"\"\"\n    \n    tabNet_model = TabNetClassifier(\n                                   n_d=16,\n                                   n_a=16,\n                                   n_steps=4,\n                                   gamma=1.9,\n                                   n_independent=4,\n                                   n_shared=5,\n                                   seed=42,\n                                   optimizer_fn = torch.optim.Adam,\n                                   scheduler_params = {\"milestones\": [150,250,300,350,400,450],'gamma':0.2},\n                                   scheduler_fn=torch.optim.lr_scheduler.MultiStepLR\n                                  )\n\n    tabNet_model.fit(\n        X_train = X_train.to_numpy(),\n        y_train = y_train.to_numpy(),\n        eval_set=[(X_train.to_numpy(), y_train.to_numpy()),\n                  (X_test.to_numpy(), y_test.to_numpy())],\n        max_epochs = 100,\n        batch_size = 256,\n        patience = 10,\n        from_unsupervised = pretrainer\n        )\n    \n    return tabNet_model","2c68a9cc":"# Make predictions\ndef makePredictions(X_test, tabNet_model) -> \"pd.DataFrame\":\n    \"\"\"Make predictions\n    \n    Return:\n        Predictions\n    \"\"\"\n    \n    return tabNet_model.predict_proba(X_test.to_numpy())[:,1]","06e2c43b":"# Evaluation\ndef evaluate(y_tabNet_pred) -> None:\n    \"\"\"Evaluate the predictions\n    \n    Process:\n        Print ROC AUC and F1 score\n    \"\"\"\n    \n    print(\"The ROC AUC score of TabNet model is \" +\n          str(round(roc_auc_score(y_test, y_tabNet_pred), 4))\n         )\n\n    print(\"The F1 score of TabNet model at threshold = 0.27 is \" +\n          str(round(f1_score(y_test, np.where(y_tabNet_pred > 0.27, 1, 0)), 4))\n         )","9497cf57":"FEATS = [\n         \"CLAIM3YEARS_Y\", \"BUS_USE_Y\", \"AD_BUILDINGS_Y\",\n         \"CONTENTS_COVER_Y\", \"P1_SEX_M\", \"P1_SEX_N\", \"BUILDINGS_COVER_Y\", \n         \"P1_POLICY_REFUSED_Y\", \"APPR_ALARM_Y\", \"APPR_LOCKS_Y\", \"FLOODING_Y\", \n         \"NEIGH_WATCH_Y\", \"SAFE_INSTALLED_Y\", \"SEC_DISC_REQ_Y\", \"SUBSIDENCE_Y\", \n         \"LEGAL_ADDON_POST_REN_Y\", \"HOME_EM_ADDON_PRE_REN_Y\", \n         \"HOME_EM_ADDON_POST_REN_Y\", \"GARDEN_ADDON_PRE_REN_Y\",\n         \"GARDEN_ADDON_POST_REN_Y\", \"KEYCARE_ADDON_PRE_REN_Y\", \n         \"KEYCARE_ADDON_POST_REN_Y\", \"HP1_ADDON_PRE_REN_Y\", \"HP1_ADDON_POST_REN_Y\", \n         \"HP2_ADDON_PRE_REN_Y\", \"HP2_ADDON_POST_REN_Y\", \"HP3_ADDON_PRE_REN_Y\", \n         \"HP3_ADDON_POST_REN_Y\", \"MTA_FLAG_Y\", \"OCC_STATUS_LP\",\n         \"OCC_STATUS_PH\", \"OCC_STATUS_UN\", \"OCC_STATUS_WD\",\n         \"OWNERSHIP_TYPE_2.0\", \"OWNERSHIP_TYPE_3.0\", \"OWNERSHIP_TYPE_6.0\", \n         \"OWNERSHIP_TYPE_7.0\", \"OWNERSHIP_TYPE_8.0\", \"OWNERSHIP_TYPE_11.0\", \n         \"OWNERSHIP_TYPE_12.0\", \"OWNERSHIP_TYPE_13.0\", \"OWNERSHIP_TYPE_14.0\", \n         \"OWNERSHIP_TYPE_16.0\", \"OWNERSHIP_TYPE_17.0\", \n         \"OWNERSHIP_TYPE_18.0\", \"PROP_TYPE_2.0\", \"PROP_TYPE_3.0\", \"PROP_TYPE_4.0\", \n         \"PROP_TYPE_7.0\", \"PROP_TYPE_9.0\", \"PROP_TYPE_10.0\", \n         \"PROP_TYPE_16.0\", \"PROP_TYPE_17.0\", \"PROP_TYPE_18.0\", \"PROP_TYPE_19.0\", \n         \"PROP_TYPE_20.0\", \"PROP_TYPE_21.0\", \"PROP_TYPE_22.0\", \"PROP_TYPE_23.0\", \n         \"PROP_TYPE_24.0\", \"PROP_TYPE_25.0\", \"PROP_TYPE_26.0\", \"PROP_TYPE_27.0\", \n         \"PROP_TYPE_29.0\", \"PROP_TYPE_30.0\", \"PROP_TYPE_31.0\", \n         \"PROP_TYPE_32.0\", \"PROP_TYPE_37.0\", \"PROP_TYPE_39.0\", \n         \"PROP_TYPE_40.0\", \"PROP_TYPE_44.0\", \"PROP_TYPE_45.0\", \"PROP_TYPE_47.0\", \n         \"PROP_TYPE_48.0\", \"PROP_TYPE_51.0\", \"PROP_TYPE_52.0\", \"PROP_TYPE_53.0\", \n         \"PAYMENT_METHOD_NonDD\", \"PAYMENT_METHOD_PureDD\", \"P1_EMP_STATUS_C\", \n         \"P1_EMP_STATUS_E\", \"P1_EMP_STATUS_F\", \"P1_EMP_STATUS_H\", \"P1_EMP_STATUS_I\", \n         \"P1_EMP_STATUS_N\", \"P1_EMP_STATUS_R\", \"P1_EMP_STATUS_S\", \"P1_EMP_STATUS_U\", \n         \"P1_EMP_STATUS_V\", \"P1_MAR_STATUS_B\", \"P1_MAR_STATUS_C\", \"P1_MAR_STATUS_D\", \n         \"P1_MAR_STATUS_M\", \"P1_MAR_STATUS_N\", \"P1_MAR_STATUS_O\", \"P1_MAR_STATUS_P\", \n         \"P1_MAR_STATUS_S\", \"P1_MAR_STATUS_W\", \n         \"age\", \"property_age\", \"cover_length\", \"RISK_RATED_AREA_B_imputed\", \n         \"RISK_RATED_AREA_C_imputed\", \"MTA_FAP_imputed\", \"MTA_APRP_imputed\",\n         \"SUM_INSURED_BUILDINGS\", \"NCD_GRANTED_YEARS_B\", \"SUM_INSURED_CONTENTS\", \n         \"NCD_GRANTED_YEARS_C\", \"SPEC_SUM_INSURED\", \"SPEC_ITEM_PREM\", \n         \"UNSPEC_HRP_PREM\", \"BEDROOMS\", \"MAX_DAYS_UNOCC\", \"LAST_ANN_PREM_GROSS\"\n        ]\n\n\nprint(\"Reading the data\")\ndf = pd.read_csv(\"..\/input\/home-insurance\/home_insurance.csv\")\n\nprint(\"Preprocessing the data\")\nX_train, y_train, X_test, y_test = splitData(df, FEATS)\nX_train, X_test = standardiseNumericalFeats(X_train, X_test)\n\nprint(\"The ratio of lapse class in training set is \" +\n      str(round(y_train.sum()\/len(y_train) * 100, 2)) +\n      \"%\"\n     )\n\nprint(\"The ratio of lapse class in test set is \" +\n      str(round(y_test.sum()\/len(y_test) * 100, 2)) +\n      \"%\"\n     )\n\nprint(\"Pretrain TabNet model\")\npretrainer = tabNetPretrain(X_train)\n\nprint(\"Training TabNet model\")\ntabNet_model = trainTabNetModel(X_train, y_train, pretrainer)\n\nprint(\"Making predictions\")\ny_tabNet_pred = makePredictions(X_test, tabNet_model)\n\nprint(\"Evaluation of the model\")\nevaluate(y_tabNet_pred)","06d4c619":"# TabNet model\nimportance_tabNet = pd.DataFrame(tabNet_model.feature_importances_,index=X_train.columns).sort_values(0, ascending = False)\nimportance_tabNet.columns = [\"importance\"]\nimportance_tabNet","9b9d8e0c":"plt.hist(y_tabNet_pred, bins = 100)\nplt.title(\"Prediction distribution of pretrained TabNet\")\nplt.show()","33e1511e":"## TabNet","ac0920a9":"# Deep learning model's performance on tabular data compared to GBDT and TabNet model\nThis is part of my experiment to compare the performance of MLP, GBDT and TabNet model. This notebook trains a model with using TabNet with pre-training to predict home insurance lapse on home insurance dataset. The main notebook can be found [here](https:\/\/www.kaggle.com\/kyosukemorita\/deep-learning-vs-gbdt-model-on-tabular-data).","8519b59a":"## Prediction distribution","b3136dfa":"## Feature importance"}}