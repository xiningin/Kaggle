{"cell_type":{"961711cf":"code","3f23b341":"code","f5d51f30":"code","0d7c4a73":"code","8c1709d1":"code","5f026855":"code","b1c2502a":"code","987601dd":"code","8825548e":"code","dc283e22":"code","75129929":"code","50d7cbeb":"code","b0187037":"code","80d09484":"code","3479c55f":"code","4b0ab689":"code","9cb565af":"code","7f510a01":"code","4c83b810":"code","f1c80545":"code","d9cdce9f":"code","a35949c1":"code","69b567bc":"code","47c9027a":"code","46882c65":"code","924d8453":"markdown","95d62d0a":"markdown","91c033cc":"markdown","324459e8":"markdown","56f1536a":"markdown","b71d605d":"markdown","be55a7a3":"markdown","66be93c0":"markdown","4ede50b2":"markdown","a2b97e62":"markdown","df620846":"markdown","f1289470":"markdown","fd2f88a2":"markdown","9fae2654":"markdown","1903d029":"markdown","a6acf5e5":"markdown","f12a58c0":"markdown"},"source":{"961711cf":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPRegressor as mlp_r\nfrom sklearn.neural_network import MLPClassifier as mlp_c\nfrom sklearn.metrics import confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn import datasets","3f23b341":"Heart_data = pd.read_csv(\"..\/input\/heart.csv\")\nHeart_data.head()","f5d51f30":"X = (Heart_data.iloc[:,:-1]).as_matrix()\ny = (Heart_data.iloc[:,2]).as_matrix()","0d7c4a73":"X = (X - X.min(axis=0))\/(X.max(axis=0) - X.min(axis=0))\ny = y.reshape((-1,1))\/3","8c1709d1":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42)","5f026855":"regressor = mlp_r(\n    hidden_layer_sizes=(100,100),  activation='tanh', solver='adam', alpha=0.001, batch_size='auto',\n    learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n    random_state=0, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n    early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)","b1c2502a":"regressor.fit(X_train, y_train)","987601dd":"y_hats = regressor.predict(X_test)","8825548e":"plt.scatter(y_test, y_hats, c='k')\n\nplt.plot([0.1, 0.9], [0.1, 0.9], 'r')\nplt.xlabel('Real')\nplt.ylabel('Estimada')","dc283e22":"from sklearn.metrics import mean_squared_error\n\ncapa_1 = [5, 7, 9, 11, 13, 17,  19, 23, 29, 31]\ncapa_2 = [1, 5, 7, 9, 11, 13, 17,  19, 23, 29]\n\nmse_m = np.zeros((len(capa_1),len(capa_1)))\nmse_std = np.zeros((len(capa_1),len(capa_1)))\n\nfor j, n_1 in enumerate(capa_1):\n    for k, n_2 in enumerate(capa_2):\n        mse_temp = []\n    \n        for i in range(10):\n            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n            regressor = mlp_r(hidden_layer_sizes=(n_1,n_2),  activation='tanh', solver='adam', alpha=0.001, batch_size='auto',\n            learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n            random_state=0, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n            early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n            regressor.fit(X_train, y_train)\n            y_hats = regressor.predict(X_test)\n            mse_temp.append(mean_squared_error(y_test, y_hats))\n            \n        mse_m[j, k] = np.mean(mse_temp)\n        mse_std[j, k] = np.std(mse_temp)\n","75129929":"plt.imshow(mse_m)\nplt.colorbar()","50d7cbeb":"plt.imshow(mse_std)\nplt.colorbar()","b0187037":"mmntm=[0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1]\nmse_m = np.zeros(len(mmntm))\nfor m,n_3 in enumerate(mmntm):\n    for i in range(21):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n        regressor = mlp_r(hidden_layer_sizes=(29,31),  activation='tanh', solver='adam', alpha=0.001, batch_size='auto',\n        learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n        random_state=0, tol=0.0001, verbose=False, warm_start=False, momentum=n_3, nesterovs_momentum=True,\n        early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n        regressor.fit(X_train, y_train)\n        y_hats = regressor.predict(X_test)\n        mse_temp.append(mean_squared_error(y_test, y_hats))\n    mse_m[m] = np.mean(mse_temp)","80d09484":"plt.plot(mmntm, mse_m, '.')\nplt.xlabel('Momentum')\nplt.ylabel('Error cuadratico medio')","3479c55f":"X = (Heart_data.iloc[:,:-1]).as_matrix()\ny = (Heart_data.iloc[:,2]).as_matrix()","4b0ab689":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42)","9cb565af":"clf = mlp_c(solver='lbfgs', alpha=1e-5,\n                    hidden_layer_sizes=(1000, 500), random_state=1)\n\nclf.fit(X_train, y_train)\ny_hat = clf.predict(X_test)","7f510a01":"cm1 = confusion_matrix(y_test, y_hat)\ncm2 = confusion_matrix(y_test, y_hat)\ncm2 = cm2.astype('float') \/ cm2.sum(axis=1)[:, np.newaxis]\ncm1","4c83b810":"cm2","f1c80545":"capa_1=[100, 400, 800, 900, 1000]\ncapa_2=[20, 100, 500, 700, 800]\ncero=np.zeros((len(capa_1),len(capa_2)))\nuno=np.zeros((len(capa_1),len(capa_2)))\ndos=np.zeros((len(capa_1),len(capa_2)))\ntres=np.zeros((len(capa_1),len(capa_2)))\nsuma=np.zeros((len(capa_1),len(capa_2)))\nfor j, n_1 in enumerate(capa_1):\n    for k, n_2 in enumerate(capa_2):\n        X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.33, random_state=42)\n\n        clf = mlp_c(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(n_1, n_2), random_state=1)\n\n        clf.fit(X_train, y_train)\n        y_hat = clf.predict(X_test)\n        cm2 = confusion_matrix(y_test, y_hat)\n        cm2 = cm2.astype('float') \/ cm2.sum(axis=1)[:, np.newaxis]\n        suma[j,k]=cm2[0,0]+cm2[1,1]+cm2[2,2]+cm2[3,3]\n        cero[j,k]=cm2[0,0]\n        uno[j,k]=cm2[1,1]\n        dos[j,k]=cm2[2,2]\n        tres[j,k]=cm2[3,3]\n            \n","d9cdce9f":"plt.imshow(cero)\nplt.colorbar()","a35949c1":"plt.imshow(uno)\nplt.colorbar()","69b567bc":"plt.imshow(dos)\nplt.colorbar()","47c9027a":"plt.imshow(tres)\nplt.colorbar()","46882c65":"plt.imshow(suma\/4)\nplt.colorbar()","924d8453":"A medida que se aumenta el numero de neuronas de la capa 2, se obtienen mejores resultados, aunque el dato del cp categoria 3 nunca se puede predecir correctamente para este caso (en el unico caso que funciona, los dem\u00e1s fallan).","95d62d0a":"Se calcula la matriz de confusion sin normalizar","91c033cc":"Entrenamos","324459e8":"Creamos 2 vectores con posibles numeros de neuronas en cada capa (2 capas) y probamos todas las combinaciones para posteriormente medir el error cuadratico medio de cada uno","56f1536a":"Entrenamos","b71d605d":"Se puede ver que a medida que sube el momentum, el error cuadratico medio disminuye","be55a7a3":"Se dividen los datos para probar el modelo posteriormente","66be93c0":"# REGRESI\u00d3N\n\nSe crea una matriz X que contiene todas las filas de los datos y un vector Y que contiene la informaci\u00f3n del tipo de dolor de pecho (cp)","4ede50b2":"En cada imagen, se muestra la entrada diagonal de cada matriz de confusi\u00f3n normalizada, entre mas se aproxime a 1 mejor. Al final se tiene una grafica que cada entrada corresponde a un promedio de la diagonal.","a2b97e62":"Variando el momentum con uno de los mejores resultados","df620846":"# ****SOLUCI\u00d3N:****\n\nSe importan las librerias y se leen los datos escogidos: [Heart Disease UCI](https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci)","f1289470":"Se normalizan todos los datos en X segun el maximo y se hace lo mismo para Y (se divide entre 3 porque es el maximo de esos datos)","fd2f88a2":"# CLASIFICACI\u00d3N\nSe usan los mismos datos de [Heart Disease UCI](https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci) y se dividen los datos para el entrenamiento justo como la regresi\u00f3n","9fae2654":"A continuaci\u00f3n se grafican 2 mapas en donde se muestra el error cuadratico medio y la desviaci\u00f3n estandar de cada red neuronal calculada anteriormente","1903d029":"A continuaci\u00f3n, se crean varios modelos con diferentes numeros de neuronas en cada capa, y por medio de la herramienta de densidad de imagen, se establece el mejor.","a6acf5e5":"Para la grafica anterior, se puede ver que para el numero de neuronas en la capa 2 igual a 1 los datos contienen mucho error cuadratico medio, y que para 5 o mas neuronas en cada capa el error cuadratico medio es bajo y lo mismo ocurre para la desviaci\u00f3n estandar. En general se puede decir que el modelo corre bien para 5 o mas neuronas en la capa 1 siempre y cuando tenga 5 o mas neuronas en la capa 2\n","f12a58c0":"Para analizar la matriz, debemos fijarnos en la diagonal principal, para tener unos datos claros porcentualmente hablando, normalizamos."}}