{"cell_type":{"4194ae53":"code","02c76f24":"code","295b9f56":"code","9cbf9a37":"code","c5b12219":"code","f808cc43":"code","11b44882":"code","623649a5":"code","e6bf06cd":"code","fc904ca3":"code","ec91276c":"code","d48eb258":"code","7ff1aeab":"markdown","1d733f10":"markdown","25905700":"markdown","f7549b03":"markdown","cbdfc0d2":"markdown","49c80d2b":"markdown"},"source":{"4194ae53":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        #print(\"\")\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","02c76f24":"len(os.listdir('..\/input\/real-life-industrial-dataset-of-casting-product\/casting_data\/casting_data'))","295b9f56":"test_def=len(os.listdir('..\/input\/real-life-industrial-dataset-of-casting-product\/casting_data\/casting_data\/test\/def_front'))\ntest_ok=len(os.listdir('..\/input\/real-life-industrial-dataset-of-casting-product\/casting_data\/casting_data\/test\/ok_front'))\ntrain_ok=len(os.listdir('..\/input\/real-life-industrial-dataset-of-casting-product\/casting_data\/casting_data\/train\/ok_front'))\ntrain_def=len(os.listdir('..\/input\/real-life-industrial-dataset-of-casting-product\/casting_data\/casting_data\/train\/def_front'))","9cbf9a37":"import matplotlib.pyplot as plt\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nlangs = ['train_ok', 'train_def', 'test_ok', 'test_def']\nstudents = [train_ok,train_def,test_ok,test_def]\nax.bar(langs,students)\nplt.show()","c5b12219":"train_def\/train_ok","f808cc43":"class_weight = {0: 1.,\n                1:1.3 }","11b44882":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\n\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), input_shape=(224, 224,3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten()) \nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n","623649a5":"model.summary()","e6bf06cd":"from keras.preprocessing.image import ImageDataGenerator\n\nbatch_size = 16\n\n# this is the augmentation configuration we will use for training\ntrain_datagen = ImageDataGenerator(\n        rescale=1.\/255,\n        horizontal_flip=True)\n\n# only rescaling\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\n\ntrain_generator = train_datagen.flow_from_directory(\n        '..\/input\/real-life-industrial-dataset-of-casting-product\/casting_data\/casting_data\/train',  # this is the target directory\n        target_size=(224, 224),  \n        batch_size=batch_size,\n        class_mode='binary')  \n\n# this is a similar generator, for validation data\nvalidation_generator = test_datagen.flow_from_directory(\n        '..\/input\/real-life-industrial-dataset-of-casting-product\/casting_data\/casting_data\/test',\n        target_size=(224, 224),\n        batch_size=batch_size,\n        class_mode='binary')","fc904ca3":"hist=model.fit_generator(\n        train_generator,\n        steps_per_epoch=6633 \/\/ batch_size,\n        epochs=50,class_weight=class_weight,\n        validation_data=validation_generator,\n        validation_steps=715 \/\/ batch_size)","ec91276c":"history=hist","d48eb258":"# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","7ff1aeab":"# dataset exploration ","1d733f10":"# visualization ","25905700":"# data preparation","f7549b03":"# training","cbdfc0d2":"set the weights of the classes to solve the imbalnced dataset issue ","49c80d2b":"# building the model "}}