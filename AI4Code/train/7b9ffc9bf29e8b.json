{"cell_type":{"a950e93d":"code","728e7656":"code","2b08f32e":"code","b9e1dc67":"code","684d0782":"code","3b7b85fa":"code","de190249":"code","65d97c4a":"code","0f4be6a0":"code","802fa7be":"code","0252cca0":"code","26fd5476":"code","f1932072":"code","7b5c7ebe":"code","71ac1703":"code","00a21e08":"code","aa00ed43":"code","1870ecbd":"code","c311190f":"code","3ca3bf9f":"code","12a55633":"code","af858d6b":"code","d1406c93":"code","edc13d05":"code","144e1046":"code","9aab3233":"code","3d348f65":"code","cab6b505":"code","8229e708":"code","58af1f23":"code","9c6d9688":"code","605a8cad":"code","074aab63":"code","0dceb64b":"code","caa79350":"code","93abed7c":"code","417ac8c7":"code","84890379":"code","41b8d2bb":"code","eb801cb8":"code","3e2d870c":"code","ea470547":"code","16db670f":"code","2d43659c":"code","ee2c438b":"code","0131a477":"code","9b93d1dc":"code","5583f810":"code","c74dcabc":"code","6d061e5e":"code","bce30b86":"code","c6069b6e":"code","9b6e75aa":"code","50978c1c":"code","2d69da9e":"code","b53158b9":"code","17e795fc":"markdown","13535c70":"markdown","b2ebf38c":"markdown","af97bb46":"markdown","1d3f3f34":"markdown","f2cd99a1":"markdown","b34b5fbe":"markdown","9aa1c4cc":"markdown","85f6c267":"markdown","f01fe4dd":"markdown","5255c021":"markdown","dbd414ab":"markdown","ea4541b5":"markdown","25e226db":"markdown","3334f3d6":"markdown","86e7413b":"markdown","cb4adfd9":"markdown","9ffa265e":"markdown","c804411b":"markdown"},"source":{"a950e93d":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","728e7656":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nfrom warnings import filterwarnings\nimport pprint\nfrom sklearn.preprocessing import StandardScaler","2b08f32e":"filterwarnings(action='ignore')","b9e1dc67":"train_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain_df","684d0782":"train_df.describe()","3b7b85fa":"train_df.info()","de190249":"plt.figure(figsize=(12,10))\nsns.heatmap(train_df.corr(), cmap='Greys')\nplt.show()","65d97c4a":"df_corr = train_df.corr()\ncols = df_corr.nlargest(14, 'SalePrice')['SalePrice'].index","0f4be6a0":"plt.figure(figsize=(8,6))\nsns.heatmap(train_df[cols].corr(), cmap='Greys', annot=True)\nplt.show()","802fa7be":"df = train_df[cols].drop(['GarageYrBlt', 'GarageCars', '1stFlrSF', 'MasVnrArea'], axis=1)\ndf['Id'] = train_df['Id']\ndf","0252cca0":"df['SalePrice'].mean()","26fd5476":"df['SalePrice'].sort_values(ascending=False)","f1932072":"sns.boxplot(df.SalePrice)","7b5c7ebe":"def iqr(data):\n    q1, q3 = np.percentile(data,[25,75])\n    print(q1,q3)\n    iqr = q3 - q1\n    upper_out = q3 + 1.5*iqr\n    lower_out = q1 - 1.5*iqr\n    print(iqr, upper_out, lower_out)\n    return iqr, upper_out, lower_out","71ac1703":"iqr, upper_out, lower_out = iqr(df.SalePrice)","00a21e08":"rem_index = df[df.SalePrice > upper_out]['SalePrice'].index\nlen(rem_index)","aa00ed43":"df1 = df.copy()\ndf1","1870ecbd":"sns.scatterplot(x=df['YearBuilt'], y=df['SalePrice'], alpha=0.25)\nplt.show()","c311190f":"sns.scatterplot(x=df['GrLivArea'], y=df['SalePrice'], alpha=0.25)\nplt.show()","3ca3bf9f":"sns.scatterplot(x=df['TotalBsmtSF'], y=df['SalePrice'], alpha=0.25)\nplt.show()","12a55633":"qualification = df[['SalePrice', 'OverallQual']].groupby(\n    'OverallQual', as_index=False)['SalePrice'].mean()\nqualification","af858d6b":"sns.barplot(x=qualification['OverallQual'], y=qualification['SalePrice'])\nplt.show()","d1406c93":"# indexes came from sorting values of SalePrice\n\ndf.drop(df.index[rem_index], inplace=True)\ndf","edc13d05":"sns.boxplot(df['GrLivArea'])","144e1046":"i, u, l=iqr(df.GrLivArea)","9aab3233":"# i consider those as outliers\n\ndf.drop(df[df['GrLivArea']>u].index, inplace=True)","3d348f65":"sns.boxplot(df['TotalBsmtSF'])","cab6b505":"i, u, l = iqr(df['TotalBsmtSF'])","8229e708":"df.drop(df[df['TotalBsmtSF']>u].index, inplace=True)","58af1f23":"df","9c6d9688":"sns.boxplot(df['TotalBsmtSF'])","605a8cad":"sns.distplot(df['SalePrice'], fit=stats.norm)","074aab63":"df['SalePrice'] = np.log(df['SalePrice'])","0dceb64b":"sns.distplot(df['SalePrice'], fit=stats.norm)","caa79350":"# checking if there are any duplicated values\nprint(\"num of duplicates: \", df.duplicated().sum(), \"\\n-------------------------\")\n\n# checking if there are any NaNs\nprint(\"num of NaNs:\",'\\n' ,df.isna().sum(), \"\\n-------------------------\")","93abed7c":"X = df.drop(['SalePrice', 'Id'], axis=1)\ny = df['SalePrice']","417ac8c7":"X_train, X_val, y_train, y_val = train_test_split(\n    X, y, random_state=42, test_size=0.2)","84890379":"scaler = StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_val = scaler.transform(X_val)","41b8d2bb":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score, make_scorer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV","eb801cb8":"lr = LinearRegression()\nrf = RandomForestRegressor(random_state=42)\nlgb = LGBMRegressor(random_state=42, objective='regression')\nsvr = SVR()","3e2d870c":"ensemble_regressor = VotingRegressor(\n    [('lr', lr), ('rf', rf), ('lgb', lgb), ('svr', svr)])","ea470547":"for reg in (lr, rf, lgb, svr, ensemble_regressor):\n    reg.fit(X_train, y_train)\n    y_pred = reg.predict(X_val)\n    print(reg.__class__.__name__, mean_squared_error(y_val, y_pred))","16db670f":"lgb = LGBMRegressor(n_jobs=-1)\n#Define the parameters\nparameters = {'num_leaves':[20,40,60,80,100,200], 'min_child_samples':[5,10,15],'max_depth':[5,8,12,10,20],\n             'learning_rate':[0.05,0.1,0.2, 0.01, 0.02, 0.03],'reg_alpha':[0,0.01,0.03], 'num_iterations': [100,200,300,400,500]}\n\nmse = make_scorer(mean_squared_error,greater_is_better=False)\n\nreg = GridSearchCV(lgb,parameters, scoring=mse, n_jobs=4)\nreg.fit(X_train, y_train)\nprint(reg.best_params_)\ny_pred = reg.predict(X_val)\nprint('The result is:')\nprint(mean_squared_error(y_val, y_pred))","2d43659c":"reg.best_estimator_","ee2c438b":"lgb_best = reg.best_estimator_\nlgb_best","0131a477":"lgb_best.fit(X_train, y_train)","9b93d1dc":"y_pred = lgb_best.predict(X_val)\nprint('The result is:')\nprint(mean_squared_error(y_val, y_pred))","5583f810":"from joblib import dump\ndump(lgb_best, 'lgb.joblib')","c74dcabc":"cols = df.columns\ncols = cols.drop(['Id', 'SalePrice'])","6d061e5e":"submission = test_df['Id']\nX_test = test_df[cols]","bce30b86":"X_test.isna().sum()","c6069b6e":"#taking care of missing values by replacing them with mean\n\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nX_test[['GarageArea']] = imputer.fit(X_test[['GarageArea']]).transform(X_test[['GarageArea']])","9b6e75aa":"imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nX_test[['TotalBsmtSF']] = imputer.fit(X_test[['TotalBsmtSF']]).transform(X_test[['TotalBsmtSF']])","50978c1c":"X_test = scaler.transform(X_test)","2d69da9e":"y_pred = np.expm1(ensemble_regressor.predict(X_test))","b53158b9":"y_pred","17e795fc":"### Scatter plot for recognizing outliers","13535c70":"Standarization should be done after spliting the dataset. otherwise it may cause data leakage.","b2ebf38c":"### Heatmaps","af97bb46":"Looks like there are outliers above $340k.","1d3f3f34":"### Dropping outliers","f2cd99a1":"### Barplot","b34b5fbe":"Based on heatmap which is shown above, these columns are highly correlated with the dependent column which is SalePrice.\nThere are some other information that we can extract from this heatmap: <br>\nGarageArea and GarageCars(huh! makes sense), GarageYrBlt and MasVnrArea and Fireplaces, TotalBsmtSF and 1stFlrSf have the same effect on producing SalePrice; so we can drop one per each pair.","9aa1c4cc":"Based on what we just saw above, I decided to get rid of not only those two high sale prices, but also some of them which don't make sense.","85f6c267":"## Standard Scaler","f01fe4dd":"Some values are way different from the rest of values. The mean of SalePrice is something about 181000, but there are two values which are about 700000! we should find out the feature which is messing up the prices.","5255c021":"## Modeling","dbd414ab":"## Loading datasets and getting some info","ea4541b5":"## Preparing data to train regression model","25e226db":"Prices are not normally distributed, we transform them to be normal by using log transformation","3334f3d6":"## Predicting the test set","86e7413b":"OverallQual has a really good effect on SalePrice, nice chosen features so far.","cb4adfd9":"## Outliers","9ffa265e":"## Feature importance","c804411b":"## Normalizing data"}}