{"cell_type":{"1347ab14":"code","bab6faf4":"code","ec377296":"code","e1fa2442":"code","73037567":"code","2ba88177":"code","50e163f3":"code","b1104cae":"code","7063914d":"code","ea834b09":"code","9974197b":"code","01da477e":"code","ec99910f":"code","d1be52b1":"code","a8d8dba2":"code","b2db9eb1":"code","824b17cb":"code","17360920":"code","6fcd9dfe":"code","d6c0408c":"code","7f185d0f":"code","c5b61419":"code","013bac8e":"code","b3a031df":"code","b7ef2b67":"code","10004ab6":"code","bcb09e95":"code","4004812a":"code","e910a1cb":"code","197e8340":"code","f14d234f":"code","debde9dc":"code","4cbf5031":"code","04695ca0":"code","bee6f299":"code","b628c241":"code","479f3fa3":"code","3dc92190":"code","8c23e6bd":"code","f8d26f60":"code","846d84f3":"code","868166f5":"code","d3b00940":"code","51b47504":"code","dba4ad52":"code","6a45ac7c":"code","cf439335":"code","393d0ec4":"code","b6b5d2b5":"code","b245f2a3":"code","3ee0802e":"code","ddb538c2":"code","9c408090":"code","3ca48a72":"code","94cf659e":"code","852b4663":"code","5dcc1a94":"code","26c46af2":"code","f9e437e6":"code","45f89c31":"code","f168f737":"code","053f0643":"code","2eb5603e":"code","62f92f83":"code","397d6f42":"code","88f762ae":"code","173314dd":"code","359ec882":"code","5cc1c235":"code","2e862143":"code","a16cb224":"code","0f92712e":"code","48e13759":"code","02cb4947":"code","f5d31916":"code","67fd45df":"code","e10767d2":"code","aa7a0029":"code","7deed193":"code","352f5724":"code","bb996586":"code","c1978a80":"code","a1e1c410":"code","40a838c3":"code","44216282":"code","d146738d":"code","8f0fbb4d":"code","020484ae":"code","2dff2efc":"code","c3d3d13b":"code","0e08c28a":"code","0dff8cbb":"code","fa4b4fca":"code","f837e121":"code","718e6ca7":"code","35ad5537":"code","9129c600":"code","422671eb":"code","e5041844":"code","8bb87c78":"code","20074c1a":"code","6cfe299a":"code","5f8eee3e":"code","2625aec3":"code","e53d8730":"code","f00b61eb":"code","a3552b81":"code","e1b40f7c":"code","6ad8c91f":"code","e1b6d0e6":"code","4fca8440":"code","47e36175":"code","8611ed4c":"code","5c44843f":"code","21c8110e":"code","00403d6b":"code","e376914b":"code","0fcdb5b4":"code","f47f1d1c":"code","bff36bed":"code","a4328de8":"code","bb391509":"code","2ec9d89a":"code","7975752b":"code","00dc496a":"code","9c5e6261":"code","d33f1040":"code","372bc06b":"code","befe3564":"code","e3bf111a":"code","5e8f1d18":"code","6fb048be":"code","535478b4":"code","b66ae24d":"code","8bfa2af7":"code","e1cb91c2":"code","db517197":"markdown","b312e62b":"markdown","452724de":"markdown","60922da3":"markdown","cc44aabd":"markdown","8e120641":"markdown","df8d5aae":"markdown","dd461cc9":"markdown","f631cce5":"markdown","f3723b0c":"markdown","0c6fb434":"markdown","02b86c2d":"markdown","d584fb73":"markdown","c71cc812":"markdown","104cfd3c":"markdown","cd4a3ad2":"markdown","01a04b9c":"markdown","83e9fd68":"markdown","5cd58d43":"markdown","3c8127cf":"markdown","03c86d1e":"markdown","20cdf570":"markdown","8d308b9c":"markdown","891f4fcb":"markdown","b3d190a7":"markdown","10498326":"markdown","03ab6889":"markdown","7a30d80f":"markdown","5181a23e":"markdown","f983ce3d":"markdown","6f3cedfd":"markdown","cd9d3f80":"markdown","2ee786c9":"markdown","8b8ff4c2":"markdown","618d861a":"markdown","bb73e2d8":"markdown","bc20bf31":"markdown","d98546f5":"markdown","c37e507e":"markdown","cf97442c":"markdown","9f490326":"markdown","88618157":"markdown","a4fd4b31":"markdown","7c919d8b":"markdown","aaa58ff6":"markdown","e275f93c":"markdown","ad98043b":"markdown","ac1e0002":"markdown","948a944c":"markdown"},"source":{"1347ab14":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","bab6faf4":"from __future__ import print_function\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.ticker as ticker\nfrom matplotlib.pyplot import xticks\n%matplotlib inline\nsns.set(style=\"whitegrid\")\nsns.set(rc={'figure.figsize':(12,8)})\npd.set_option('display.max_rows', 50)\npd.set_option('display.max_columns', 40)\npd.set_option('display.max_colwidth', -1)\n\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets","ec377296":"#function for missing values in columns\ndef missing_coldata(df):\n    missin_col = pd.DataFrame(round(df.isnull().sum().sort_values(ascending=False)\/len(df.index)*100,1), columns=['% of missing value'])\n    missin_col['Count of Missing Values'] = df.isnull().sum()\n    return missin_col\n\n#function for missing values in rows\ndef missing_rowdata(df):\n    missin_row = pd.DataFrame(round(df.isnull().sum(axis=1).sort_values(ascending=False)\/len(df.columns)*100), columns=['% of missing value'])\n    missin_row['Count of Missing Values'] = df.isnull().sum(axis=1)\n    return missin_row","e1fa2442":"leadsdata = pd.read_csv('..\/input\/leads-dataset\/Leads.csv')\nleadsdata.head(5) ","73037567":"leadsdata.shape","2ba88177":"leadsdata.info()","50e163f3":"leadsdata.isnull().sum()","b1104cae":"dupcheck=leadsdata[leadsdata.duplicated([\"Prospect ID\"])]\ndupcheck","7063914d":"sum(leadsdata.duplicated('Prospect ID')) == 0","ea834b09":"sum(leadsdata.duplicated('Lead Number')) == 0","9974197b":"leadsdata.nunique()","01da477e":"Conversion_rate = (sum(leadsdata['Converted'])\/len(leadsdata['Converted'].index))*100\nprint(\"The conversion rate of leads is: \",Conversion_rate)","ec99910f":"# Divide the data into Numeric and categorical data  \nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n# NUMERIC\nnumdata=leadsdata[list(leadsdata.select_dtypes(numerics).columns)]\n# CATEGORICAL \ncatdata=leadsdata[list(leadsdata.select_dtypes(exclude=numerics).columns)]\ncatdata.columns","d1be52b1":"# Conversion rate for each categorical feature\n@interact\ndef counts(col =catdata.iloc[:,1:].columns):\n    sns.countplot(x=col,data=leadsdata,hue=\"Converted\",palette=\"husl\",hue_order=[0,1])\n    plt.xlabel(col)\n    plt.ylabel('Total count')\n    plt.legend(loc='upper center', bbox_to_anchor=(1, 0.8), ncol=1)\n    plt.xticks(rotation=65, horizontalalignment='right',fontweight='light')\n    convertcount=leadsdata.pivot_table(values='Lead Number',index=col,columns='Converted', aggfunc='count').fillna(0)\n    convertcount[\"Conversion(%)\"] =round(convertcount[1]\/(convertcount[0]+convertcount[1]),2)*100\n    return print(convertcount.sort_values(ascending=False,by=1),plt.show())","a8d8dba2":"@interact\ndef described(col=leadsdata.iloc[:,2:].columns):\n    return leadsdata[col].describe()","b2db9eb1":"#Choosing to drop the columns that have only 1 unique value\nleadsdata=leadsdata.drop([\"Receive More Updates About Our Courses\",\"Magazine\",\"Update me on Supply Chain Content\",\"Get updates on DM Content\",\"I agree to pay the amount through cheque\"],axis=1)","824b17cb":"leadsdata=leadsdata.drop([\"Newspaper\",\"X Education Forums\",\"Newspaper Article\",\n                          \"Through Recommendations\",\"Digital Advertisement\",\n                          \"What matters most to you in choosing a course\",\"Search\",\"Do Not Call\"],axis=1)","17360920":"missing_coldata(leadsdata)","6fcd9dfe":"leadsdata[\"Lead Source\"]=leadsdata[\"Lead Source\"].fillna(\"Google\")","d6c0408c":"# What is your current occupation\nleadsdata[\"What is your current occupation\"]=leadsdata[\"What is your current occupation\"].fillna(\"Unemployed\")","7f185d0f":"# Also the missing values can be imputed with Any_Other\nleadsdata[\"Specialization\"]=leadsdata[\"Specialization\"].replace(\"Select\",\"Any_Other\")\nleadsdata[\"Specialization\"]=leadsdata[\"Specialization\"].fillna(\"Any_Other\")","c5b61419":"leadsdata[\"How did you hear about X Education\"]=leadsdata[\"How did you hear about X Education\"].replace(\"Select\",\"Not_Mentioned\")\nleadsdata[\"How did you hear about X Education\"]=leadsdata[\"How did you hear about X Education\"].fillna(\"Not_Mentioned\")","013bac8e":"leadsdata[\"Lead Profile\"]=leadsdata[\"Lead Profile\"].replace(\"Select\",\"Any_Other\")\nleadsdata[\"Lead Profile\"]=leadsdata[\"Lead Profile\"].fillna(\"Any_other\")","b3a031df":"leadsdata[\"Lead Quality\"]=leadsdata[\"Lead Quality\"].replace(\"Select\",\"Might be\")\nleadsdata[\"Lead Quality\"]=leadsdata[\"Lead Quality\"].fillna(\"Might be\")","b7ef2b67":"# Tags\nleadsdata[\"Tags\"]=leadsdata[\"Tags\"].fillna(\"Will revert after reading the email\")","10004ab6":"leadsdata[\"Country\"]=leadsdata[\"Country\"].fillna(\"India\")","bcb09e95":"leadsdata[\"City\"]=leadsdata[\"City\"].fillna(\"Mumbai\")","4004812a":"leadsdata.shape","e910a1cb":"missing_coldata(leadsdata)","197e8340":"numdata.columns","f14d234f":"@interact\ndef density( y=numdata.iloc[:,2:].columns,tick_spacing = [100,50,25,10,5]):\n    ax=leadsdata[y].plot(kind=\"hist\",title=y,bins=50, rot=30)\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n    return","debde9dc":"@interact\ndef outliers_check( y=numdata.iloc[:,2:].columns):\n    return leadsdata.plot(kind='box',y=y,figsize=[6,5]) ","4cbf5031":"leadsdata.drop(['Asymmetrique Activity Score', 'Asymmetrique Profile Score','Asymmetrique Activity Index','Asymmetrique Profile Index'],axis=1,inplace=True)","04695ca0":"missing_coldata(leadsdata)","bee6f299":"leadsdata.dropna(inplace = True)","b628c241":"missing_coldata(leadsdata)","479f3fa3":"leadsdata.shape","3dc92190":"leadsdata['Tags'] = leadsdata['Tags'].replace(['In confusion whether part time or DLP', 'in touch with EINS','Diploma holder (Not Eligible)',\n                                     'Approached upfront','Graduation in progress','number not provided', 'opp hangup','Still Thinking',\n                                    'Lost to Others','Shall take in the next coming month','Lateral student','Interested in Next batch',\n                                    'Recognition issue (DEC approval)','Want to take admission but has financial problems',\n                                    'University not recognized'], 'Misc_Tags')","8c23e6bd":"leadsdata['Tags'] = leadsdata['Tags'].replace([\"Ringing\",\"Misc_Tags\",\"Interested in other courses\",\"switched off\",\"Already a student\",\n                                               \"Interested  in full time MBA\",\"Not doing further education\",\"invalid number\",\"wrong number given\"], 'Misc_Tags')                                  \n","f8d26f60":"leadsdata['Tags'] = leadsdata['Tags'].replace([\"Ringing\",\"Misc_Tags\",\"Interested in other courses\",\"switched off\",\"Already a student\",\n                                               \"Interested  in full time MBA\",\"Not doing further education\",\"invalid number\",\"wrong number given\"], 'Misc_Tags')                                  \n","846d84f3":"leadsdata[\"Last Notable Activity\"] = leadsdata[\"Last Notable Activity\"].replace(['Approached upfront',\n       'Resubscribed to emails', 'View in browser link Clicked',\n       'Form Submitted on Website', 'Email Received', 'Email Marked Spam'], 'Misc_Notable_Activity')","868166f5":"leadsdata['Lead Source'] = leadsdata['Lead Source'].replace(['Click2call', 'Live Chat', 'NC_EDM', 'Pay per Click Ads', 'Press_Release',\n  'Social Media', 'WeLearn', 'bing', 'blog', 'testone', 'welearnblog_Home', 'youtubechannel'], 'Miscellaneous')","d3b00940":"#There are two google in lead source which should be corrected to one\nleadsdata['Lead Source'] = leadsdata['Lead Source'].replace('google',\"Google\")\n","51b47504":"# As we can see that There are various categories in Last Activity which have very few records, thus combining all those to one category Miscellaneous\nleadsdata['Last Activity'] = leadsdata['Last Activity'].replace(['Had a Phone Conversation', 'View in browser link Clicked', \n                                                       'Visited Booth in Tradeshow', 'Approached upfront',\n                                                       'Resubscribed to emails','Email Received', 'Email Marked Spam'], 'Miscellaneous')","dba4ad52":"# Total Visit\n(leadsdata[\"TotalVisits\"]>=30).sum()","6a45ac7c":"# removed outliers with values greater than 30\n\nleadsdata=leadsdata[leadsdata[\"TotalVisits\"] < 30]","cf439335":"# Page view per visit\n# can easily remove these two outliers\n(leadsdata[\"Page Views Per Visit\"]>=15).sum()","393d0ec4":"#### removed outliers with values greater than 15\nleadsdata=leadsdata[leadsdata[\"Page Views Per Visit\"]<15]","b6b5d2b5":"# dataframe is sliced for more than one hour time spent on website\nleads1hrplus=leadsdata[leadsdata['Total Time Spent on Website']>=60]\nleads1hrplus[\"hours spent\"]=round(leads1hrplus[\"Total Time Spent on Website\"]\/60).astype(int)","b245f2a3":"time_spent_abv1hr=leads1hrplus.pivot_table(values='Lead Number',index=['hours spent'],columns='Converted', aggfunc='count').fillna(0)\ntime_spent_abv1hr[\"Conversion(%)\"] =round(time_spent_abv1hr[1]\/(time_spent_abv1hr[0]+time_spent_abv1hr[1]),2)*100\ntime_spent_abv1hr.sort_values(ascending=False,by=1)","3ee0802e":"time_spent_abv1hr.iloc[:,:-1].plot(kind='bar',title= \"Conversion count for the leads that spend at least 1 hour on website\",stacked=True,figsize=[8,6])","ddb538c2":"leadslessthan1hr=leadsdata[leadsdata['Total Time Spent on Website']<60]\nleadslessthan1hr[\"mins_spent\"]=leadslessthan1hr[\"Total Time Spent on Website\"].astype(int)","9c408090":"time_spent_upto1hr=leadslessthan1hr.pivot_table(values='Lead Number',index=['mins_spent'],columns='Converted', aggfunc='count').fillna(0)\ntime_spent_upto1hr[\"Conversion(%)\"] =round(time_spent_upto1hr[1]\/(time_spent_upto1hr[0]+time_spent_upto1hr[1]),2)*100\ntime_spent_upto1hr.sort_values(ascending=False,by=\"Conversion(%)\")","3ca48a72":"time_spent_upto1hr.iloc[:,:-1].plot(kind='bar',title=\"Conversion count for the leads that spend atmost 1 hour on website\",stacked=True,figsize=[10,8],log=True)","94cf659e":"@interact\ndef numcount(cols=['TotalVisits','Asymmetrique Activity Score', 'Asymmetrique Profile Score']):\n    numdfcount=round(leadsdata.pivot_table(values='Lead Number',index=cols,columns='Converted', aggfunc='count')).fillna(0)\n    numdfcount[\"Conversion(%)\"]=round((numdfcount[1]\/(numdfcount[0]+numdfcount[1]))*100)\n    cnplot=numdfcount.iloc[:,:-1].plot(kind=\"bar\",stacked=True, legend=\"upper right\", title=cols,figsize=[8,6])\n    return print(numdfcount, \"\\n\", cnplot)","852b4663":"pageview=leadsdata.pivot_table(values='Lead Number',index=['Page Views Per Visit'],columns='Converted', aggfunc='count')\npageview.reset_index(inplace=True)","5dcc1a94":"pageview.fillna(0,inplace=True)","26c46af2":"pageviews=pageview.round().groupby(\"Page Views Per Visit\").sum()\npageviews[\"Conversion(%)\"]=round((pageviews[1]\/(pageviews[0]+pageviews[1]))*100)","f9e437e6":"pageviews.iloc[:,:-1].plot(kind=\"bar\",legend=\"upper right\",stacked=True,figsize=[7,5])\npageviews","45f89c31":"# there are two unique keys for the data, hence dropping Prospect ID for now n keeping Lead Number.\nleadsdata=leadsdata.drop(\"Prospect ID\",axis=1)","f168f737":"leadsdata.nunique()","053f0643":"leadsdata.drop(['Country'],axis=1,inplace=True)","2eb5603e":"leadsdata.columns","62f92f83":"catdata=leadsdata[list(leadsdata.select_dtypes(exclude=numerics).columns)]\ncatdata.columns","397d6f42":"@interact\ndef counts(col =catdata.columns):\n    sns.countplot(x=col,data=leadsdata,hue=\"Converted\",palette=\"husl\",hue_order=[0,1])\n    plt.xlabel(col)\n    plt.ylabel('Total count')\n    plt.legend(loc='upper center', bbox_to_anchor=(1, 0.8), ncol=1)\n    plt.xticks(rotation=65, horizontalalignment='right',fontweight='light')\n    convertcount=leadsdata.pivot_table(values='Lead Number',index=col,columns='Converted', aggfunc='count').fillna(0)\n    convertcount[\"Conversion(%)\"] =round(convertcount[1]\/(convertcount[0]+convertcount[1]),2)*100\n    return print(convertcount.sort_values(ascending=False,by=1),plt.show())","88f762ae":"catdata.nunique()","173314dd":"df = pd.get_dummies(leadsdata[catdata.columns], drop_first=True)\ndf.head()","359ec882":"#Create a copy of leads data to add these dummies to the whole data\nleads_copy = leadsdata.copy(deep=True)","5cc1c235":"leads = leadsdata.drop(catdata.columns, axis = 1)","2e862143":"leads.columns","a16cb224":"leads = pd.concat([leads, df], axis=1)","0f92712e":"leads.shape","48e13759":"%matplotlib inline\nplt.figure(figsize = (10,6))\nsns.heatmap(leadsdata.corr(),annot = True)","02cb4947":"# Total Visits and Page Views Per Visit are significantly correlated, hence we drop one of those\nleads = leads.drop(\"Page Views Per Visit\", axis = 1)","f5d31916":"leads.columns","67fd45df":"from sklearn.preprocessing import MinMaxScaler\nscaler =  MinMaxScaler()\nleads[['TotalVisits','Total Time Spent on Website']] = scaler.fit_transform(leads[['TotalVisits','Total Time Spent on Website']])\nleads.head()","e10767d2":"from sklearn.model_selection import train_test_split\n# Creating target variable as y and remaining as X\nX = leads.drop([\"Lead Number\",'Converted'], axis=1)\ny = leads['Converted']\ndisplay(y.head(),X.head())","aa7a0029":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","7deed193":"numdata=X_train[list(X_train.select_dtypes(numerics).columns)]\nnumdata.columns","352f5724":"import statsmodels.api as sm\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","bb996586":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\n\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg, 24)             # running RFE with 15 variables as output\nrfe = rfe.fit(X_train, y_train)\nrfe.support_\nlist(zip(X_train.columns, rfe.support_, rfe.ranking_))","c1978a80":"vars=X_train.columns[rfe.support_]","a1e1c410":"X_train_sm = sm.add_constant(X_train[vars])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","40a838c3":"# VIF\nX_train_sm = X_train_sm.drop(['const'], axis=1)\n# Checking the  VIF of all the  features\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_sm\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","44216282":"vars=vars.drop(['How did you hear about X Education_SMS'],1)\n\nX_train_sm = sm.add_constant(X_train[vars])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","d146738d":"vars=vars.drop(['Specialization_Travel and Tourism'],1)\n\nX_train_sm = sm.add_constant(X_train[vars])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","8f0fbb4d":"X_train_sm = X_train_sm.drop(['const'], axis=1)\n# Checking the  VIF of all the  features\n\nvif = pd.DataFrame()\nX = X_train_sm\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","020484ae":"\nvars=vars.drop(['Last Activity_Miscellaneous'],1)\n\nX_train_sm = sm.add_constant(X_train[vars])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","2dff2efc":"vars=vars.drop(['Last Activity_SMS Sent'],1)\n\nX_train_sm = sm.add_constant(X_train[vars])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","c3d3d13b":"vars=vars.drop(['Lead Quality_Might be'],1)\n\nX_train_sm = sm.add_constant(X_train[vars])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","0e08c28a":"X_train_sm = X_train_sm.drop(['const'], axis=1)\n# Checking the  VIF of all the  features\n\nvif = pd.DataFrame()\nX = X_train_sm\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","0dff8cbb":"vars=vars.drop(['Tags_Misc_Tags'],1)\n\nX_train_sm = sm.add_constant(X_train[vars])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","fa4b4fca":"vars=vars.drop(['What is your current occupation_Unemployed'],1)\n\nX_train_sm = sm.add_constant(X_train[vars])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","f837e121":"X_train_sm = X_train_sm.drop(['const'], axis=1)\n# Checking the  VIF of all the  features\n\nvif = pd.DataFrame()\nX = X_train_sm\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","718e6ca7":"vars=vars.drop(['Lead Profile_Any_other'],1)\n\nX_train_sm = sm.add_constant(X_train[vars])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","35ad5537":"vars=vars.drop(['Lead Quality_Worst'],1)\n\nX_train_sm = sm.add_constant(X_train[vars])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","9129c600":"X_train_sm = X_train_sm.drop(['const'], axis=1)\n# Checking the  VIF of all the  features\n\nvif = pd.DataFrame()\nX = X_train_sm\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","422671eb":"X_train_sm = sm.add_constant(X_train[vars])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","e5041844":"print(\"The final variables selected by the logsitic regression model are \",\"\\n\",vars)","8bb87c78":"# Let's run the model using the selected variables\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlogsk = LogisticRegression(C=1e9)\nlogsk.fit(X_train[vars], y_train)","20074c1a":"# Predicted probabilities\ny_pred = logsk.predict_proba(X_train[vars])\n# Converting y_pred to a dataframe which is an array\ny_pred_df = pd.DataFrame(y_pred)\n# Converting to column dataframe\ny_pred_1 = y_pred_df.iloc[:,[1]]\n# Let's see the head\ny_pred_1.head()","6cfe299a":"# Converting y_train to dataframe\ny_train_df = pd.DataFrame(y_train)\ny_train_df.head()","5f8eee3e":"# Putting index to LeadID\ny_train_df['LeadID'] = y_train_df.index\ny_train_df.head()","2625aec3":"\n# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_train_df.reset_index(drop=True, inplace=True)\n# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_train_df,y_pred_1],axis=1)\n# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 1 : 'Conv_Prob'})\n# Rearranging the columns\ny_pred_final = y_pred_final.reindex(['LeadID','Converted','Conv_Prob'], axis=1)\n# Let's see the head of y_pred_final\ny_pred_final.head()","e53d8730":"# Creating new column 'predicted' with 1 if Conversion_Rate>0.5 else 0\ny_pred_final['predicted'] = y_pred_final.Conv_Prob.map( lambda x: 1 if x > 0.5 else 0)\n# Let's see the head\ny_pred_final.head()","f00b61eb":"# Creating new column \"Lead Score\" with 1to100 using conversion rates\ny_pred_final['Lead Score'] = y_pred_final.Conv_Prob.map( lambda x: round(x*100))\n# Let's see the head\ny_pred_final.head()","a3552b81":"from sklearn import metrics\n# Confusion matrix \nconfusion = metrics.confusion_matrix( y_pred_final.Converted, y_pred_final.predicted )\nconfusion","e1b40f7c":"#Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.Converted, y_pred_final.predicted)","6ad8c91f":"metrics.precision_score(y_pred_final.Converted, y_pred_final.predicted)","e1b6d0e6":"metrics.recall_score(y_pred_final.Converted, y_pred_final.predicted)","4fca8440":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(6, 6))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return fpr, tpr, thresholds","47e36175":"draw_roc(y_pred_final.Converted, y_pred_final.predicted)","8611ed4c":"#draw_roc(y_pred_final.Converted, y_pred_final.predicted)\n\"{:2.2f}\".format(metrics.roc_auc_score(y_pred_final.Converted, y_pred_final.Conv_Prob))","5c44843f":"from sklearn import metrics\n\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.predicted )\nprint(confusion)","21c8110e":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_pred_final[i]= y_pred_final.Conv_Prob.map(lambda x: 1 if x > i else 0)\ny_pred_final.head()","00403d6b":"# Calculating accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['probability','accuracy','sensitivity','specificity'])\nfrom sklearn.metrics import confusion_matrix\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    specificity = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensitivity = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensitivity,specificity]\nprint(cutoff_df)","e376914b":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='probability', y=['accuracy','sensitivity','specificity'])\nplt.show()","0fcdb5b4":"#  0.3 is the optimum point to take it as a cutoff probability tp predict the final probability\n\ny_pred_final['final_pred'] = y_pred_final.Conv_Prob.map( lambda x: 1 if x > 0.3 else 0)\n\ny_pred_final.head(10)","f47f1d1c":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.Converted, y_pred_final.final_pred)\n\ncm2 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.final_pred )\ncm2\nTP = cm2[1,1] \nTN = cm2[0,0] \nFP = cm2[0,1] \nFN = cm2[1,0] \nprint(\"SENSITIVITY of the logistic regression model is  \",TP \/ float(TP+FN))\n","bff36bed":"print(\"True negatives are \",TN \/ float(TN+FP))\nprint(\"False Positives are  \",FP\/ float(TN+FP))\nprint (\"True Positives are  \",TP \/ float(TP+FP))\nprint (TN \/ float(TN+ FN))","a4328de8":"from sklearn.metrics import precision_recall_curve\nprecision, recall, thresholds = precision_recall_curve(y_pred_final.Converted, y_pred_final.Conv_Prob)\nplt.plot(thresholds, precision[:-1], \"b\")\nplt.plot(thresholds, recall[:-1], \"g\")\nplt.show()","bb391509":"X_test[['TotalVisits','Total Time Spent on Website']] = scaler.fit_transform(X_test[['TotalVisits','Total Time Spent on Website']])\n","2ec9d89a":"X_test=X_test[vars]\nX_test.head()","7975752b":"X_test_sm = sm.add_constant(X_test)\ny_test_pred = res.predict(X_test_sm)\ny_test_pred.head()","00dc496a":"y_pred_1 = pd.DataFrame(y_test_pred)\ny_test_df = pd.DataFrame(y_test)\n# Putting CustID to index\ny_test_df['LeadID'] = y_test_df.index\n# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)\n# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)\ny_pred_final= y_pred_final.rename(columns={ 0 : 'Conv_Prob'})\ny_pred_final = y_pred_final.reindex(['LeadID','Converted','Conv_Prob'], axis=1)\ny_pred_final.head()","9c5e6261":"# Creating new column \"Lead Score\" with 1to100 using conversion rates\ny_pred_final['Lead_Score'] = y_pred_final.Conv_Prob.map( lambda x: round(x*100))\n# Let's see the head\ny_pred_final.head()","d33f1040":"y_pred_final['final_pred'] = y_pred_final.Conv_Prob.map(lambda x: 1 if x > 0.38 else 0)\ny_pred_final.head(10)","372bc06b":"print(\"Model Accuracy on Test data is \",metrics.accuracy_score(y_pred_final.Converted, y_pred_final.final_pred))","befe3564":"confusion2 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.final_pred )\nconfusion2\nTP = confusion2[1,1]  \nTN = confusion2[0,0] \nFP = confusion2[0,1] \nFN = confusion2[1,0] \n\nprint(\"Sensitivity of the model on test data is \",round(TP \/ float(TP+FN),2))","e3bf111a":"print(\"Specificity of the model on test data is \",TN \/ float(TN+FP))","5e8f1d18":"ydf=y_train_df.set_index(\"LeadID\")","6fb048be":"Xy_Traindf=pd.concat([ydf,X_train_sm.iloc[:,1:]],axis=1)","535478b4":"Xy_Traindf.corr()[\"Converted\"].sort_values()","b66ae24d":"Xy_Traindf.reset_index(inplace=True)","8bfa2af7":"Xy_Traindf=Xy_Traindf.rename(columns={\"index\":\"LeadID\"})","e1cb91c2":"@interact\ndef counts(col =['Lead Origin_Lead Add Form', 'Lead Source_Olark Chat',\n       'Lead Source_Welingak Website', 'Do Not Email_Yes',\n       'Tags_Closed by Horizzon', 'Tags_Lost to EINS',\n       'Tags_Will revert after reading the email', 'Lead Quality_Not Sure',\n       'Lead Profile_Other Leads', 'Lead Profile_Potential Lead',\n       'Last Notable Activity_Modified',\n       'Last Notable Activity_Olark Chat Conversation',\n       'Last Notable Activity_SMS Sent']):\n    sns.countplot(x=col,data=Xy_Traindf,hue=\"Converted\",palette=\"husl\")\n    plt.xlabel(col)\n    plt.ylabel('Total count')\n    plt.legend(loc='upper center', bbox_to_anchor=(1, 0.8), ncol=1)\n    plt.xticks(rotation=65, horizontalalignment='right',fontweight='light')\n    convertcount=Xy_Traindf.pivot_table(values='LeadID',index=col,columns='Converted', aggfunc='count').fillna(0)\n    convertcount[\"Conversion(%)\"] =round(convertcount[1]\/(convertcount[0]+convertcount[1]),2)*100\n    return print(convertcount.sort_values(ascending=False,by=1),plt.show())","db517197":"### DATA MODELLING","b312e62b":"#### Creating a dataframe for duplicate values if any","452724de":"#### Lead Quality\nThe highest frequency is of Select, which means we can not drop it and need to convert it to \"Any Other\"\/ \"Not Mentioned\"\n","60922da3":"### Taking 0.38 as the cutoff using precision recall tradeoff","cc44aabd":"### After cleaning the data we can visualise it again to see the conversion rates among various categories\n","8e120641":"### DATA PREPARATION\n#### Encoding categorical features ","df8d5aae":"### Goal\nThere are quite a few goals for this case study.\n\nBuild a logistic regression model to assign a lead score between 0 and 100 to each of the leads which can be used by the company to target potential leads. A higher score would mean that the lead is hot, i.e. is most likely to convert whereas a lower score would mean that the lead is cold and will mostly not get converted.\n\nThere are some more problems presented by the company which your model should be able to adjust to if the company's requirement changes in the future so you will need to handle these as well. These problems are provided in a separate doc file. Please fill it based on the logistic regression model you got in the first step. Also, make sure you include this in your final PPT where you'll make recommendations.","dd461cc9":"#### Lets see the shape of dataframe now","f631cce5":"### Handling Missing Data ","f3723b0c":"### DATA DICTIONARY\n\nProspect ID: A unique ID with which the customer is identified.\n\nLead Number: A lead number assigned to each lead procured.\n\nLead Origin: The origin identifier with which the customer was identified to be a lead. Includes API, Landing Page Submission, etc.\n\nLead Source: The source of the lead. Includes Google, Organic Search, Olark Chat, etc.\n\nDo Not Email: An indicator variable selected by the customer wherein they select whether of not they want to be emailed about the course or not.\n\nDo Not Call: An indicator variable selected by the customer wherein they select whether of not they want to be called about the course or not.\n\nConverted: The target variable. Indicates whether a lead has been successfully converted or not.\n\nTotalVisits: The total number of visits made by the customer on the website.\n\nTotal Time Spent on Website: The total time spent by the customer on the website.\n\nPage Views Per Visit: Average number of pages on the website viewed during the visits.\n\nLast Activity: Last activity performed by the customer. Includes Email Opened, Olark Chat Conversation, etc.\nCountry\tThe country of the customer.\n\nSpecialization:\tThe industry domain in which the customer worked before. Includes the level 'Select Specialization' which means the customer had not selected this option while filling the form.\n\nHow did you hear about X Education:\tThe source from which the customer heard about X Education.\n\nWhat is your current occupation: Indicates whether the customer is a student, umemployed or employed.\n\nWhat matters most to you in choosing this course: An option selected by the customer indicating what is their main motto behind doing this course.\n\nSearch\/Magazine\/Newspaper Article\/X Education Forums\/Newspaper\/Digital Advertisement:\tIndicating whether the customer had seen the ad in any of the listed items.\n\t\nThrough Recommendations: Indicates whether the customer came in through recommendations.\n\nReceive More Updates About Our Courses:\tIndicates whether the customer chose to receive more updates about the courses.\n\nTags: Tags assigned to customers indicating the current status of the lead.\n\nLead Quality: Indicates the quality of lead based on the data and intuition the the employee who has been assigned to the lead.\n\nUpdate me on Supply Chain Content: Indicates whether the customer wants updates on the Supply Chain Content.\n\nGet updates on DM Content: Indicates whether the customer wants updates on the DM Content.\n\nLead Profile: A lead level assigned to each customer based on their profile.\n\nCity: The city of the customer.\n\nAsymmetrique Activity Index \/Asymmetrique Profile Index\/Asymmetrique Activity Score\/Asymmetrique Profile Score: An index and score assigned to each customer based on their activity and their profile\n\t\nI agree to pay the amount through cheque: Indicates whether the customer has agreed to pay the amount through cheque or not.\n\na free copy of Mastering The Interview:\tIndicates whether the customer wants a free copy of 'Mastering the Interview' or not.\n\nLast Notable Activity:\tThe last notable acitivity performed by the student.\n","0c6fb434":"#### City\nmajority records have city as Mumbai or nearby, hence imputing the NaN with Mumbai is not inappropriate choice","02b86c2d":"#### Total Time Spent on Website\n\n##### As we can see the time spent is in minutes: we analyse it by dividing the data into two parts:- \n* #####  more than one hour spent \n","d584fb73":"#### Last Activity\n","c71cc812":"### Splitting the data","104cfd3c":"### Visualising the conversion rate of the most Impactful Features of Leads Data","cd4a3ad2":"#### Country\n95% of the data has country as India hence imputing with India","01a04b9c":"##### It can easily be considered to remove these features from the model as they play no role in conversion of leads","83e9fd68":"### Importing the data","5cd58d43":"### A Beginner's Guide to very Simple EDA \nThis Notebook can be used as a basic guide to understand how to explore the data and build a simple regression model\nusing step by step feature elimination and making use of RFE as well for feature selection.\nIf you have any doubts, please comment to ask and if there is something you find should be done in a better way, do mention that \nas well.\nLet us now see what the Problem of Leads Score is!!!\n\n","3c8127cf":"* ### Conversion rate for other numeric features of the leads ","03c86d1e":"### Model Evaluation\n","20cdf570":"#### Optimal Cutoff","8d308b9c":"### Outliers Check of numeric data\n","891f4fcb":"#### Specialization\nThe highest frequency is of Select, which means we can not drop it and need to convert it to \"Any Other\"\/ \"Not on List\"\nAlso if manually entering the specialization is not provided on website then this should be taken care of in the form design.\n","b3d190a7":"##### Analyzing the categories, we can reduce them as per the number of leads, the categories which have least number of records, almost negligible can be combined to form one category like Miscellaneous","10498326":"### Problem Statement\n\nAn education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses. \n\n \n\nThe company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is around 30%. \n\n \n\nNow, although X Education gets a lot of leads, its lead conversion rate is very poor. For example, if, say, they acquire 100 leads in a day, only about 30 of them are converted. To make this process more efficient, the company wishes to identify the most potential leads, also known as \u2018Hot Leads\u2019. If they successfully identify this set of leads, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone.\n\nX Education has appointed you to help them select the most promising leads, i.e. the leads that are most likely to convert into paying customers. The company requires you to build a model wherein you need to assign a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance. The CEO, in particular, has given a ballpark of the target lead conversion rate to be around 80%.\n\n#### Data\nYou have been provided with a leads dataset from the past with around 9000 data points. This dataset consists of various attributes such as Lead Source, Total Time Spent on Website, Total Visits, Last Activity, etc. which may or may not be useful in ultimately deciding whether a lead will be converted or not. The target variable, in this case, is the column \u2018Converted\u2019 which tells whether a past lead was converted or not wherein 1 means it was converted and 0 means it wasn\u2019t converted. You can learn more about the dataset from the data dictionary provided in the zip folder at the end of the page.\nAnother thing that you also need to check out for are the levels present in the categorical variables. Many of the categorical variables have a level called 'Select' which needs to be handled because it is as good as a null value (think why?). \n","03ab6889":"#### Lead Source : \nMissing values are much less than 1 percent, imputing it with most frequent value \"Google\"","7a30d80f":"#### 'Lead Number','Asymmetrique Activity Score', 'Asymmetrique Profile Score','Asymmetrique Activity Index','Asymmetrique Profile Index' \n45% missing values should not be imputed because this may effect the data's correctness, hence dropping these features.","5181a23e":"### Desciribing the data","f983ce3d":"### MAKING PREDICTIONS","6f3cedfd":"### Data Visualization and Cleaning\n","cd9d3f80":"##### Newspaper : \n##### X Education Forums : \n##### Newspaper Articles : \n##### Through Recommendations : \n##### Digital Advertisements : \nfor all these variables above mentioned , all the values are no hence it does not have any significant role in lead score, drop this column\n \n##### What matters most to you in choosing a course : \n99.9% of available values are \"Better career prospects\" and around 30 % are missing, hence it does not have any significant role in lead score, drop this column\n\n##### Search :\n99% values are no except a few yes and missing, hence it does not have any significant role in lead score, drop this column\n##### Do Not Call: \nAll the values are no except 2 values, hence there is no variance, doesnt indicate anything about leads and can easily be dropped","2ee786c9":"#### Analysing the training data over the Encoded variables which are finally selected ny the logistic regression model","8b8ff4c2":"#### TotalVisits, Page views per visit, Last Activity  : \nMissing values are approx. 1 percent\nhence drop all these rows","618d861a":"#### Last Notable Activity","bb73e2d8":"### Final Variables selected with RFE and Manual Elimination\n","bc20bf31":"#### Those features which have only one unique value are :\n* #### *-Magazine*\n* #### *-Recieve More updates about the course*\n* #### *-Update me on Supply chain content*\n* #### *-Get updates on DM content*\n* #### *-I agree to pay the amount through cheque*\nThese features show no variance and thus all the leads have chosen one option, \nthus this feature doesnt make any impact or difference on conversion of leads.","d98546f5":"#### Making Predictions on test set X_test","c37e507e":"#### Lead Source\nAs we can see that There are various lead source which have just 1 or 2 leads, thus combining all those to one category Miscellaneous\n","cf97442c":"##### Time spent on website\n* #####  less than one hour.","9f490326":"#### How did you hear about X Education\nThe highest frequency is of Select, which means we can not drop it and need to convert it to \"Any Other\"\/ \"Not Mentioned\"\nAs this is an important factor in order to plan the marketing of X Education, hence more information should be gathered about it through other tools.\n","88618157":"### Final Logistic regression model","a4fd4b31":"### Conversion rate","7c919d8b":"According to our final model Top 3 variables (using highest coefficient) are:\nThe Variable which give the highest correlation to  variable Converted are:\n1. Tags:\n    Tags_Lost to EINS:                           9.8\n    Tags_Closed by Horizzon :                    9.7\n    Tags_Will revert after reading the email:    4.9\n2. Total Time Spent on Website:                  4.6\n3. Lead Origin:\n    Lead Add Form:                               3.5\n","aaa58ff6":"#### Lead Profile\nThe highest frequency is of Select, which means we can not drop it and need to convert it to \"Any Other\"\/ \"Not Mentioned\"\n","e275f93c":"####  The above Visualisation shows that the variables chosen by out logistic regression model are appropriate and make the most impact in conversion rates of the leads to Hot leads\n\nMost focus should be given to \n\n1. Lead Source: WElingkak WEbsite,\n    Although the number of Users are less, but there is almost 100 percent conversion\n    Thus if it is focused more then a very good Conversion rate can be achieved.\n    The strategy should be to promote this source\n    \n2. Lead Origin: Add_Form : This origin has 93% conversion rate, this should not be neglected at al and infact if the origin is Add_Form then more pritoty should be given to the user as iit has higher chances to convert to HOT leads \n\n3. Lead Notable Activity: Olark Chat : this reveals that there are a very large number of users who are using Olark chat, and the conversion rate here is not so high, so keeping in mind the number of users, and their interest in online conversation, focus should be given to look for more potential leads, so that we don\u2019t miss a large number of enquiring users.\n","ad98043b":"### Evaluation of model on test data","ac1e0002":"### The features which have highest corrleation to y variable - Converted","948a944c":"### 0.38 is the tradeoff between Precision and Recall - \nthus we can safely choose to consider any Prospect Lead with Conversion Probability higher than 38 % to be a hot Lead"}}