{"cell_type":{"e5c4de9e":"code","86555f5d":"code","8e555979":"code","4e0c8e42":"code","0cc2e8bb":"code","cd68e9d7":"code","0767328b":"code","e898ce9b":"code","8b8b8e2d":"code","e29358c6":"code","10a06b48":"code","5fdf08cf":"code","be405df9":"code","81b2f37d":"code","665a18cb":"code","5e09240d":"code","d9be4cd7":"code","c1015c41":"code","32726171":"code","27223cc1":"code","6a55335a":"code","55ce6ded":"code","3cf63bdb":"code","e3a387f5":"code","bf4121a3":"code","c24cd54e":"code","0b9b3f42":"code","e66f6bf5":"code","036be6ea":"code","2c0e2e3e":"code","4ef82d5e":"code","fcc7f7a2":"code","91d7c281":"code","ccf0231a":"code","64107b11":"code","7a4170cb":"code","bf4ea2cc":"code","96f047e9":"code","6a958d03":"code","a80c8eca":"code","b8a8766e":"code","3e684b00":"code","23f49a54":"code","59826fa5":"markdown","2bd92cfd":"markdown","54837158":"markdown","e626960d":"markdown","72d4cdd8":"markdown","7579d4f8":"markdown","d2085ae4":"markdown","5829e957":"markdown","0ab36208":"markdown","29b090d4":"markdown","1c73599c":"markdown","1a961263":"markdown"},"source":{"e5c4de9e":"%%capture\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\ntrain = pd.read_csv('..\/input\/fakenewsvortexbsb\/train_df.csv', sep=';', error_bad_lines=False, quoting=3);","86555f5d":"train.head(4) #primeiros 4 registros","8e555979":"train.shape #numero de registros do dataset","4e0c8e42":"train.columns.values #colunas do dataset","0cc2e8bb":"train[\"manchete\"][80] #amostra do dataset","cd68e9d7":"train[\"Class\"][80] #amostra do dataset","0767328b":"train['Class'].unique()#tipos diferentes de labels ","e898ce9b":"from unidecode import unidecode\nexample = train[\"manchete\"][1]\nprint(unidecode(example)) #teste com amsotra do dataset","8b8b8e2d":"import re\n# aplicamos ambas bibliotecas propostas, unidecode aplica o unicode no texto\n# re retira as pontua\u00e7\u00f5es.\nletters_only=re.sub(\"[^a-zA-Z]\",\" \",unidecode(example)) \nprint(letters_only)","e29358c6":"lower_case=letters_only.lower() #aplica as letras minusculas para todas as letras nos textos\nwords=lower_case.split()","10a06b48":"from nltk.corpus import stopwords\n#lista de stop words da lingua portugu\u00easa\nprint (stopwords.words(\"portuguese\"))","5fdf08cf":"stop = stopwords.words(\"portuguese\") #criar um arry com as stop words do portugu\u00eas","be405df9":"# n\u00e3o \u00e9 possivel aplicar o unicode em uma lista, ent\u00e3o vamos percorrer o array aplciando emc ada registro\nlista_stop = [unidecode(x) for x in stop]  \nprint (lista_stop)","81b2f37d":"print(words)","665a18cb":"#Filtro das palavras n\u00e3o stop words presentes no texto\nwords=[w for w in words if not w in lista_stop] \nprint(words)","5e09240d":"# Fun\u00e7\u00e3o review_to_words realiza todas as transforma\u00e7\u00f5es que foram realizadas antes\ndef review_to_words(raw_review):\n    raw_review = unidecode(raw_review)\n    raw_review.lstrip('Jovem Pan')\n    letters_only=re.sub(\"[^a-zA-Z]\",\" \",raw_review)\n    words=letters_only.lower().split()\n    meaningful_words=[w for w in words if not w in lista_stop]\n    return(' '.join(meaningful_words))","d9be4cd7":"#testando a fun\u00e7\u00e3o\nclean_review=review_to_words(train['manchete'][1])\nprint(clean_review)","c1015c41":"# Pegando o valor da dimen\u00e7\u00e3o dos dados para passar no for logo abaixo\nnum_reviews=train['manchete'].size\nprint (num_reviews)","32726171":"# loop para aplicar as transforma\u00e7\u00f5es em cada registro da coluna manifestacao_clean do dataset\nclean_train_review=[]\nfor i in range(0,num_reviews):\n    clean_train_review.append(review_to_words(train['manchete'][i]))","27223cc1":"from sklearn.feature_extraction.text import CountVectorizer\n# configurar os parametros do WordtoVec\/Tokeninza\u00e7\u00e3o e criar o objeto\nvectorizer=CountVectorizer(analyzer='word',tokenizer=None,preprocessor = None, stop_words = None,max_features = 7000)\n# aplicar WordtoVec\/Tokeninza\u00e7\u00e3o\ntrain_data_features=vectorizer.fit_transform(clean_train_review)\n# aplicar a estrutura de dados numpy array\ntrain_data_features=train_data_features.toarray()","6a55335a":"train_data_features.shape","55ce6ded":"train_data_features[1]","3cf63bdb":"# vocabulario de todas das palavaras mais importantes de todas requisi\u00e7\u00f5es\nvcab=vectorizer.get_feature_names()\n#print(vcab)","e3a387f5":"train_y = train[\"Class\"]","bf4121a3":"from sklearn.model_selection import train_test_split\n# Split dos dados em treino e valida\u00e7\u00e3o\nX_train, X_test, y_train, y_test = train_test_split(train_data_features, train_y, test_size=0.25, random_state=42)","c24cd54e":"from sklearn.neighbors import KNeighborsClassifier\n# Cria\u00e7\u00e3o do objeto que corresponde o modelo com o hiperparametros especificados\nmodel = KNeighborsClassifier(n_neighbors=3)\n#Treinamento do modelo\n%time model = model.fit( X_train, y_train )","0b9b3f42":"# Prevendo os dados de teste com o modelo treinado\nresult = model.predict(X_test)","e66f6bf5":"from sklearn.metrics import accuracy_score, classification_report\n# Acurr\u00e1cia absoluta dos resultados\naccuracy_score(y_test, result)","036be6ea":"# Essa fun\u00e7\u00e3o realiza testes de recall e F1-score do modelo, importante n\u00e3o apenas se basear na acurr\u00e1cia e precis\u00e3o.\nprint (classification_report(y_test, result))","2c0e2e3e":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(result, y_test, labels=y_train.unique())","4ef82d5e":"import seaborn as sn\nimport matplotlib.pyplot as plt\narray = confusion_matrix(result, y_test, labels=y_train.unique())\narray = array.astype('float') \/ array.sum(axis=1)[:, np.newaxis] #normaliza\u00e7\u00e3o dos valores \ndf_cm = pd.DataFrame(array, index = y_train.unique(), #cria um data frame para base ao gr\u00e1fico\n                  columns = y_train.unique())\nplt.figure(figsize = (10,7)) \nsn.heatmap(df_cm, annot=True, cmap=sn.light_palette((210, 90, 60), input=\"husl\"))","fcc7f7a2":"def prevendo_noticias(string, model):\n    to_array=[]\n    to_array.append(review_to_words(string))\n    sample_final=vectorizer.transform(to_array)\n    sample_final=sample_final.toarray()\n    result = model.predict(sample_final)\n    if  result[0] == 1:\n        label = 'Fake News'\n    else:\n        label = 'Verdadeira'\n        \n    return label, string","91d7c281":"prevendo_noticias('Aras: decis\u00e3o do STF n\u00e3o deveria valer para casos conclu\u00eddos', model)","ccf0231a":"prevendo_noticias('Bolsonaro pessoalmente incend\u00eaia a amazonia e mata as girafas', model)","64107b11":"prevendo_noticias('Jornalista joga \u00e1gua benta em Temer e ele admite que impeachment foi golpe', model)","7a4170cb":"# Cria\u00e7\u00e3o do objeto que corresponde o modelo com o hiperparametros especificados\nmodel_final = KNeighborsClassifier(n_neighbors=3)\n#Treinamento do modelo\n%time model_final = model_final.fit( train_data_features, train_y )","bf4ea2cc":"# Importando o dados de teste\ntest = pd.read_csv('..\/input\/fakenewsvortexbsb\/sample_submission.csv', sep=';', error_bad_lines=False, quoting=3);","96f047e9":"test.head(5)","6a958d03":"#Filtro das palavras n\u00e3o stop words presentes no texto\nnum_reviews, = test['Manchete'].shape\nprint(num_reviews)","a80c8eca":"# loop para aplicar as transforma\u00e7\u00f5es em cada registro da coluna manifestacao_clean do dataset\nclean_test_review=[]\nfor i in range(0,num_reviews):\n    clean_test_review.append(review_to_words(test['Manchete'][i]))","b8a8766e":"# aplicar WordtoVec\/Tokeninza\u00e7\u00e3o\ntest_data_features = vectorizer.transform(clean_test_review)\ntest_data_features=test_data_features.toarray()","3e684b00":"# Prevendo os dados de teste com o modelo treinado\nresult_test = model_final.predict(test_data_features)","23f49a54":"# Criando um dataframe com os resultados obtidos para submiter na competi\u00e7\u00e3o\nminha_sub = pd.DataFrame({'index': test.index, 'Category': result_test})\n# Criando um arquivo csv com os resultados\nminha_sub.to_csv('submission.csv', index=False)","59826fa5":"## Teste seus modelos abaixo:","2bd92cfd":"## ARGUMENTA\u00c7\u00c3O DE DADOS","54837158":"## Modelo KNN (k-vizinhos ou K-nearest neighbours)","e626960d":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcTWTUa_WsfLC5Q-GnwjpF8CJzP-pVwvyXh9sgZ7ouavsOv-KzE_EA)","72d4cdd8":"## Utilizando o modelo","7579d4f8":"## CRIA\u00c7\u00c3O DOS MODELOS","d2085ae4":"## Criando uma submission ","5829e957":"# PR\u00c1TICA DETEC\u00c7\u00c3O DE FAKE NEWS","0ab36208":"## M\u00c9TODO BAG OF WORDS (BOW)","29b090d4":"## Valida\u00e7\u00e3o do Modelo","1c73599c":"### Modelo de Regress\u00e3o logist\u00edca \n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\n\n### Modelo de Random Forests\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\n\n### Modelo Arvore de decis\u00e3o\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html\n\n### Modelo Boosting Trees\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html\n\n### Modelo SVM\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html\n\n","1a961263":"**Como posso fazer um computador entender textos?** \n\nPrimeiramente temos que converter textos normais para um tipo de representa\u00e7\u00e3o num\u00e9rica para que Machine Learning possa processar, uma abordagem tradicional \u00e9 a utiliza\u00e7\u00e3o da t\u00e9cnica Bag of Words que consiste em usar o vocubulario de todos o documentos analisados (Corpus) e quebrar os textos ao ponto de lidar apenas com a frequ\u00eancia de palavras, como diz literalmente o nome da t\u00e9cninca traduzida do portugu\u00eas essas informa\u00e7\u00f5es viram um \"Saco de palavras\", \u00e9 levado em conta a frequencia utilizadas de certas palavras dentro di voculabulario. Segue exemplo em duas senten\u00e7as: \n\nFrase 1: \"O gato agarrou o cachorro\"\n\nFrase 2: \"O dono agarrou o cachorro e o gato\"\n\n\nPara as duas senten\u00e7as, o vocabulo segue:\n\n{ O, GATO, AGARROU, CACHORRO, DONO, E}\n\nPara conseguir o \"saco de palavras\", contamos o numero de vezes que a palavra ocorre em cada Frase. Na senten\u00e7a 1, \"O\" aparece duas vezes, tamb\u00e9m as palavras \"GATO\", \"AGARROU\" e \"CACHORRO\" aparece uma vez, ent\u00e3o o novo registro da sente\u00e7a 1 num\u00e9ricamente fica:\n\nVocab = { O, GATO, AGARROU, CACHORRO, DONO, E}\n\nFrase 1: { 2, 1, 1, 1, 1, 0, 0}\n\nFrase 2: { 3, 1, 1, 1, 1, 1, 1}\n\nPara n\u00e3o hiperdimencionalizar o vetor de colunas e causar riscos de performances no modelo, vamos escolher um espa\u00e7o amostal m\u00e1ximo de voc\u00e1bulos. Abaixo, utilizaremos as 5000 palavras mais frequentes presentes no corpus (lembrando que tiramos as stops words)."}}