{"cell_type":{"aae5d8ed":"code","92210522":"code","7b2e1fe5":"code","3dad41fd":"code","5723bb96":"code","82cfb790":"code","34d96e9a":"code","9306287f":"code","f6facbb1":"code","daefd686":"code","25727743":"code","c1d97898":"code","6e5d7dfc":"code","f5a84da8":"code","f0e88322":"code","32ebeaa7":"code","192ca33c":"code","fc5e817c":"code","2367e482":"code","5ca762d6":"code","c8ae84f7":"code","fec6832f":"code","17fbb824":"code","28399e82":"code","55f81fa5":"code","4dfb67a6":"code","3cf8ea17":"markdown","2e43ebc4":"markdown","ea955a2b":"markdown","50ff8305":"markdown","b8e4671b":"markdown","9c6ef0cc":"markdown","fbffea4b":"markdown","3336c63c":"markdown","6e20b301":"markdown","42ad0841":"markdown","09afedb5":"markdown"},"source":{"aae5d8ed":"import numpy as np                                 \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os                                         \nimport cv2 \nfrom random import shuffle \n\nimport tensorflow as tf                      \nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom skimage.util import img_as_float\n\nfrom zipfile import ZipFile \nfrom sklearn.metrics import classification_report \nimport warnings\nwarnings.filterwarnings('ignore')","92210522":"# Data directories and categories\ntrain_data_dir = \"..\/input\/chest-xray-pneumonia\/chest_xray\/train\"\ntest_data_dir = \"..\/input\/chest-xray-pneumonia\/chest_xray\/test\"\nCATEGORIES = [\"NORMAL\",\"PNEUMONIA\"]","7b2e1fe5":"# Visualizing an image from data\nfor category in CATEGORIES:\n    path = os.path.join(train_data_dir, category)\n    for img in os.listdir(path):\n        img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n        plt.imshow(img_array, cmap=\"gray\")\n        plt.show()\n        break\n    break","3dad41fd":"print(img_array.shape)","5723bb96":"IMG_SIZE = 150\n# Visualizing an image with its lowered resolution\nnew_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE ))\nplt.imshow(new_array, cmap = \"gray\")\nplt.show()","82cfb790":"# Creating training data\ntraining_data = []\n\ndef create_training_data():\n    for category in CATEGORIES:\n        path = os.path.join(train_data_dir, category)\n        class_num = CATEGORIES.index(category)\n        for img in os.listdir(path):\n            try:\n                img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)\n                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n                training_data.append([new_array, class_num])\n            except Exception as e:\n                pass\ncreate_training_data()","34d96e9a":"print(len(training_data))","9306287f":"# Creating test data\n\ntest_data = []\n\ndef create_test_data():\n    for category1 in CATEGORIES:\n        path1 = os.path.join(test_data_dir, category1)\n        class_num = CATEGORIES.index(category1)\n        for img in os.listdir(path1):\n            try:               \n                img_array1 = cv2.imread(os.path.join(path1,img), cv2.IMREAD_GRAYSCALE)\n                new_array1 = cv2.resize(img_array1, (IMG_SIZE, IMG_SIZE))\n                test_data.append([new_array1, class_num])\n            except Exception as e:\n                pass\ncreate_test_data()","f6facbb1":"print(len(test_data))","daefd686":"# Datasets are sorted by categories. We need to shuffle them. \nimport random \n\nrandom.shuffle(training_data)\nrandom.shuffle(test_data)","25727743":"# Checking data is imbalance.\n\nlist = []\nfor i in training_data:\n    if(i[1] == 0):\n        list.append(\"Pneumonia\")\n    else:\n        list.append(\"Normal\")\nsns.countplot(list)   ","c1d97898":"X_train = []\ny_train = []\nX_test = []\ny_test = []","6e5d7dfc":"for features, label in training_data:\n    X_train.append(features)\n    y_train.append(label)","f5a84da8":"for features, label in test_data:\n    X_test.append(features)\n    y_test.append(label)","f0e88322":"# Normalize the data\nX_train = np.array(X_train) \/ 255\nX_test = np.array(X_test) \/ 255","32ebeaa7":"# Reshape\nX_train = np.array(X_train).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\ny_train = np.array(y_train)\nX_test = np.array(X_test).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\ny_test = np.array(y_test)","192ca33c":"# Handling the imbalance\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.2, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip = True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(X_train)","fc5e817c":"# Creating the model\nmodel = Sequential()\nmodel.add(Conv2D(32 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (IMG_SIZE,IMG_SIZE,1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Conv2D(16 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(Dropout(0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Conv2D(16 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D((2,2) , strides = 2 , padding = 'same'))\nmodel.add(Flatten())\nmodel.add(Dense(units = 64 , activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units = 1 , activation = 'sigmoid'))\nmodel.compile(optimizer = \"rmsprop\" , loss = 'binary_crossentropy' , metrics = ['accuracy'])\nmodel.summary()","2367e482":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.3, min_lr=0.000001)","5ca762d6":"history = model.fit(datagen.flow(X_train,y_train, batch_size = 32) ,epochs = 12 , validation_data = datagen.flow(X_test, y_test) ,callbacks = [learning_rate_reduction])","c8ae84f7":"predictions = model.predict_classes(X_test)\npredictions = predictions.reshape(1,-1)[0]","fec6832f":"print(classification_report(y_test, predictions, target_names = ['Pneumonia','Normal']))","17fbb824":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,predictions)\ncm","28399e82":"f,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(cm, annot=True, linewidths=0.01,cmap=\"Blues\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","55f81fa5":"# Losses\nplt.subplots(figsize=(8,6))\nplt.plot(history.history[\"loss\"], label = \"train loss\")\nplt.plot(history.history[\"val_loss\"], label = \"test loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Losses of fit w LR reduction\")\nplt.legend()\nplt.savefig(\"Losses of fit w LR reduction\")\nplt.show()","4dfb67a6":"# Losses\nplt.subplots(figsize=(8,6))\nplt.plot(history.history[\"accuracy\"], label = \"train accuracy\")\nplt.plot(history.history[\"val_accuracy\"], label = \"test accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracies of fit w LR reduction\")\nplt.legend()\nplt.savefig(\"Accuracies of fit w LR reduction\")\nplt.show()","3cf8ea17":"## <a id='4.'> 4. Implementing CNN<\/a>","2e43ebc4":"## <a id='6.'> 6. Conclusion<\/a>","ea955a2b":"## Content:\n\n- <a href='#1.'> 1. Importing Libraries<\/a>\n- <a href='#2.'> 2. Loading and Checking Data<\/a>\n- <a href='#3.'> 3. Preprocessing<\/a>\n- <a href='#4.'> 4. Implementing CNN<\/a>\n- <a href='#5.'> 5. Evaluation<\/a>\n- <a href='#6.'> 6. Conclusion<\/a>\n- <a href='#7.'> 7. References<\/a>","50ff8305":"It is implemented CNN to predict the pneumonia from chest X-ray images. Our model predicted the results with accuracy of 0.88. We also have overfitting problem. I'll try to deal with overfitting in the next updates of notebook. \nNote: If you feedback me, I'll be greatful.","b8e4671b":"## <a id='7.'> 7. References<\/a> ","9c6ef0cc":"## <a id='2.'> 2. Loading and Checking Dataset<\/a> ","fbffea4b":"## <a id='3.'> 3. Preprocessing<\/a> ","3336c63c":"## Introduction:\nPneumonia is an inflammatory condition of the lung primarily affecting the small air sacs known as alveoli. Symptoms typically include some combination of productive or dry cough, chest pain, fever and difficulty breathing. The severity of the condition is variable. Pneumonia is usually caused by infection with viruses or bacteria, and less commonly by other microorganisms. Identifying the responsible pathogen can be difficult. \nPneumonia is typically diagnosed based on a combination of physical signs and a chest X-ray.\n\n![image.png](attachment:image.png)\n\nIn this notebook, It is tried to classify pneumonia using x-ray photographs in the data set with Convolutional Neural Network (CNN).","6e20b301":"## <a id='5.'> 5. Evaluation<\/a>","42ad0841":"## <a id='1.'> 1. Importing Libraries<\/a> ","09afedb5":"1. https:\/\/en.wikipedia.org\/wiki\/Pneumonia\n2. https:\/\/www.kaggle.com\/senolcomert\/convolutional-neural-network-cnn-tutorial\/edit\n3. https:\/\/pythonprogramming.net\/introduction-deep-learning-python-tensorflow-keras\/\n4. https:\/\/www.kaggle.com\/madz2000\/pneumonia-detection-using-cnn-92-6-accuracy"}}