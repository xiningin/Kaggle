{"cell_type":{"ecda2151":"code","779a3da9":"code","644c8e05":"code","7c739919":"code","a5e7b494":"code","8b2b695a":"code","a9e3cde6":"code","172021ec":"code","870964f1":"code","7d32cbae":"code","0d20b198":"code","859f97db":"code","a67955b6":"code","820bd634":"code","db26eb02":"code","5521c5bd":"markdown","674c664d":"markdown","acd876cc":"markdown","15af3fcb":"markdown","27de8089":"markdown","7cb77203":"markdown","3996e08f":"markdown","8d499fd2":"markdown"},"source":{"ecda2151":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","779a3da9":"df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv\")\ndf.head(5)","644c8e05":"df.info()","7c739919":"FEATURES = [ f\"f{i}\" for i in range(1,119)]\nTARGET = \"claim\"\nX = df[FEATURES]\nX = X.fillna(X.median()) # Basic data cleaning to avoid NaN\ny = df[[TARGET]]","a5e7b494":"import umap\nsubsample = 10000 # To limit computation time\nmapper = umap.UMAP()\nmapper.fit(X.head(subsample), y.head(subsample))","8b2b695a":"import umap.plot\numap.plot.output_notebook()\n\ny_array = y.head(subsample)['claim'].values\n\np = umap.plot.interactive(mapper, labels=y_array)\numap.plot.show(p)","a9e3cde6":"import datatable as dt\n\nframe = dt.fread(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv\")\nframe.head(5)","172021ec":"import optuna\nimport xgboost as xgb\nsubsamples = 1000\n\ndef objective(trial, X=X.head(subsamples), y=y.head(subsamples)):\n    params = {\n        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-2, 0.5),\n        'max_depth': trial.suggest_int('max_depth', 5, 30),\n    }\n    \n    scores = xgb.cv(\n        params=params, \n        dtrain=xgb.DMatrix(data=X,label=y), \n        nfold=2,\n        num_boost_round=10,\n        early_stopping_rounds=10, \n        metrics=\"auc\", # Receiver Operating Characteristic Area under the Curve\n        seed=123\n    )\n    return scores.loc[scores.index[-1], \"test-auc-mean\"]\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=10)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","870964f1":"optuna.visualization.plot_contour(study, \n    params=[\n    'max_depth',\n    'learning_rate'])","7d32cbae":"import shap  # pip install shap\nimport xgboost as xgb\n\nsubsamples = 10000\nmodel = xgb.XGBRegressor().fit(X.head(subsamples), y.head(subsamples))\n\nexplainer = shap.Explainer(model)\nshap_values = explainer(X.head(subsamples))\nprint(shap_values)","0d20b198":"shap.plots.beeswarm(shap_values)","859f97db":"shap.initjs()\nsample = 10\nshap.force_plot(\n    base_value=explainer.expected_value, \n    shap_values=shap_values.values[sample],\n    features=X.head(subsamples).iloc[sample]\n)","a67955b6":"# Now you understand why I placed this at the bottom: it is updating the version of many other packages!\n! pip install lazypredict","820bd634":"from lazypredict.Supervised import LazyClassifier, LazyRegressor\nfrom sklearn.model_selection import train_test_split\n\nsubsample = 2000 # To stay under 30 seconds\n\nX_train, X_test, y_train, y_test = train_test_split(X.head(subsample), y.head(subsample), test_size=0.2)\n\nreg = LazyClassifier(\n    ignore_warnings=True, \n    random_state=1121218, \n    verbose=False\n  )\nmodels, predictions = reg.fit(X_train, X_test, y_train, y_test)  # pass all sets","db26eb02":"models","5521c5bd":"## 5 - SHAP: feature importance","674c664d":"> I put #3 at the bottom, because it will need to install new libraries with some dependency mess!\n## 4 - Optuna: Hyperparameters optimization","acd876cc":"# Based on *''7 Cool Python Packages Kagglers Are Using Without Telling You''*****\nLink: https:\/\/towardsdatascience.com\/7-cool-python-packages-kagglers-are-using-without-telling-you-e83298781cf4\n\nNote: I try to improve the original articles, by fixing issues and implementing cooler stuff, if they are easy to find.\n\n## 0 - Basic data import using conventional names for the variables\n\n","15af3fcb":"## 1 - UMAP: 2D projection","27de8089":"## 7 - Automatic EDA libraries\nCheck https:\/\/www.kaggle.com\/andreshg\/automatic-eda-libraries-comparisson\/notebook#6.-%F0%9F%93%8A-D-Tale-%F0%9F%93%9A ","7cb77203":"## 2 - Datatable: faster than pandas","3996e08f":"## 6 - Rapids cuDF: GPU managed dataframes\nNo need here","8d499fd2":"## 3 - LazyPredict: models benchamark with one line of code"}}