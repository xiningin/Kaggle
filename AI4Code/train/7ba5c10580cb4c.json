{"cell_type":{"b6c2bb2e":"code","ce3c30d3":"code","78e620fc":"code","1dd07e92":"code","bfd37645":"code","66fbfb80":"code","3be3d6f9":"code","e555e026":"code","0b5d3c13":"code","5071a338":"code","adeb2d1c":"code","0abe1bf8":"code","1466f5be":"code","806ffd3d":"markdown","10c17534":"markdown","485626ae":"markdown","d5eef7fa":"markdown","b61abb36":"markdown","499938b2":"markdown","961bf06d":"markdown","a20e4e67":"markdown","451fb95a":"markdown","4b75790a":"markdown","0da77027":"markdown"},"source":{"b6c2bb2e":"import os\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D\nfrom tensorflow.keras.layers import Concatenate\nfrom tensorflow.keras.optimizers import Adam, SGD\nimport pandas as pd","ce3c30d3":"import glob\ndaisy = glob.glob('..\/input\/flowers\/Flowers\/daisy\/*.*')\ndandelion = glob.glob('..\/input\/flowers\/Flowers\/dandelion\/*.*')\nrose = glob.glob('..\/input\/flowers\/Flowers\/rose\/*.*')\nsunflower = glob.glob('..\/input\/flowers\/Flowers\/sunflower\/*.*')\ntulip = glob.glob('..\/input\/flowers\/Flowers\/tulip\/*.*')\n\ndata=[]\nlabels = []\n\nfor i in daisy:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', target_size= (224,224))\n    image=np.array(image)\n    data.append(image)\n    labels.append(0)\nfor i in dandelion:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', target_size= (224,224))\n    image=np.array(image)\n    data.append(image)\n    labels.append(1)\nfor i in rose:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', target_size= (224,224))\n    image=np.array(image)\n    data.append(image)\n    labels.append(2)\nfor i in sunflower:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', target_size= (224,224))\n    image=np.array(image)\n    data.append(image)\n    labels.append(3)\nfor i in tulip:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', target_size= (224,224))\n    image=np.array(image)\n    data.append(image)\n    labels.append(4)\n\n","78e620fc":"data = np.array(data)\nlabels = np.array(labels)","1dd07e92":"data.shape","bfd37645":"from sklearn.model_selection import train_test_split\n# Split into training set and testing set\nX_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.4,\n                                                random_state=42)\n","66fbfb80":"# Split into validation set and testing set\nX_valid,X_test,y_valid,y_test=train_test_split(X_test,y_test,test_size=0.5,random_state=42)\n\nX_train=X_train\/255\nX_test=X_test\/255\nX_valid=X_valid\/255\n","3be3d6f9":"print(\"Training set: \",X_train.shape)\nprint(\"Validation set: \",X_valid.shape)\nprint(\"Testing set: \",X_test.shape)\n\n","e555e026":"# create model\ndef inception(x, filters):\n    # 1x1\n    path1 = Conv2D(filters=filters[0], kernel_size=(1,1), strides=1, padding='same', activation='relu')(x)\n\n    # 1x1->3x3\n    path2 = Conv2D(filters=filters[1][0], kernel_size=(1,1), strides=1, padding='same', activation='relu')(x)\n    path2 = Conv2D(filters=filters[1][1], kernel_size=(3,3), strides=1, padding='same', activation='relu')(path2)\n    \n    # 1x1->5x5\n    path3 = Conv2D(filters=filters[2][0], kernel_size=(1,1), strides=1, padding='same', activation='relu')(x)\n    path3 = Conv2D(filters=filters[2][1], kernel_size=(5,5), strides=1, padding='same', activation='relu')(path3)\n\n    # 3x3->1x1\n    path4 = MaxPooling2D(pool_size=(3,3), strides=1, padding='same')(x)\n    path4 = Conv2D(filters=filters[3], kernel_size=(1,1), strides=1, padding='same', activation='relu')(path4)\n\n    return Concatenate(axis=-1)([path1,path2,path3,path4])\n\n\ndef auxiliary(x, name=None):\n    layer = AveragePooling2D(pool_size=(5,5), strides=3, padding='valid')(x)\n    layer = Conv2D(filters=128, kernel_size=(1,1), strides=1, padding='same', activation='relu')(layer)\n    layer = Flatten()(layer)\n    layer = Dense(units=256, activation='relu',kernel_regularizer=regularizers.l2(0.0001))(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(units=CLASS_NUM, activation='softmax', name=name)(layer)\n    return layer\n\n\ndef googlenet():\n    layer_in = Input(shape=IMAGE_SHAPE)\n    \n    # stage-1\n    layer = Conv2D(filters=64, kernel_size=(7,7), strides=2, padding='same', activation='relu')(layer_in)\n    layer = MaxPooling2D(pool_size=(3,3), strides=2, padding='same')(layer)\n    layer = BatchNormalization()(layer)\n\n    # stage-2\n    layer = Conv2D(filters=64, kernel_size=(1,1), strides=1, padding='same', activation='relu')(layer)\n    layer = Conv2D(filters=192, kernel_size=(3,3), strides=1, padding='same', activation='relu')(layer)\n    layer = BatchNormalization()(layer)\n    layer = MaxPooling2D(pool_size=(3,3), strides=2, padding='same')(layer)\n\n    # stage-3\n    layer = inception(layer, [ 64,  (96,128), (16,32), 32]) #3a\n    layer = inception(layer, [128, (128,192), (32,96), 64]) #3b\n    layer = MaxPooling2D(pool_size=(3,3), strides=2, padding='same')(layer)\n    \n    # stage-4\n    layer = inception(layer, [192,  (96,208),  (16,48),  64]) #4a\n    aux1  = auxiliary(layer, name='aux1')\n    layer = inception(layer, [160, (112,224),  (24,64),  64]) #4b\n    layer = inception(layer, [128, (128,256),  (24,64),  64]) #4c\n    layer = inception(layer, [112, (144,288),  (32,64),  64]) #4d\n    aux2  = auxiliary(layer, name='aux2')\n    layer = inception(layer, [256, (160,320), (32,128), 128]) #4e\n    layer = MaxPooling2D(pool_size=(3,3), strides=2, padding='same')(layer)\n    \n    # stage-5\n    layer = inception(layer, [256, (160,320), (32,128), 128]) #5a\n    layer = inception(layer, [384, (192,384), (48,128), 128]) #5b\n    layer = AveragePooling2D(pool_size=(7,7), strides=1, padding='valid')(layer)\n    \n    # stage-6\n    layer = Flatten()(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(units=256, activation='linear',kernel_regularizer=regularizers.l2(0.0001))(layer)\n    main = Dense(units=CLASS_NUM, activation='softmax', name='main')(layer)\n    \n    model = Model(inputs=layer_in, outputs=[main, aux1, aux2])\n    \n    return model","0b5d3c13":"CLASS_NUM = 5\nBATCH_SIZE = 16\nEPOCH_STEPS = int(X_train.shape[0]\/BATCH_SIZE)\nIMAGE_SHAPE = (224, 224, 3)\nMODEL_NAME = 'googlenet_flower.h5'","5071a338":"# train model\nmodel = googlenet()\nmodel.summary()\n#model.load_weights(MODEL_NAME)\ntf.keras.utils.plot_model(model, 'GoogLeNet.png')\n\noptimizer = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n#optimizer = SGD(lr=1 * 1e-1, momentum=0.9, nesterov=True)\n#model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\noptimizer = ['Adam', 'SGD', 'Adam', 'SGD']\nepochs = [20, 30, 20, 30]\nhistory_all = {}\n\nhistory = model.fit(X_train,y_train,epochs=20,steps_per_epoch=EPOCH_STEPS,validation_data=(X_valid,y_valid))\n\nmodel.save(MODEL_NAME)\n\n","adeb2d1c":"score = model.evaluate(X_test, y_test)\nprint('Score:', score[4])\n","0abe1bf8":"plt.plot(history.history['main_accuracy'])\nplt.plot(history.history['val_main_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n","1466f5be":"plt.plot(history.history['main_loss'])\nplt.plot(history.history['val_main_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","806ffd3d":"### Contents:\n\n1. [Import libraries and data](#import)\n\n2. [Split the data into training, testing and validation](#splitting)\n\n3. [Build the model](#model-building)\n\n4. [Model evaluation](#model-evaluation)\n","10c17534":"The data is splitted into training, testing and validation set. The ratio of training, testing and validation set is 6:2:2.","485626ae":"<a id='import'><\/a>\n## Import libraries and data","d5eef7fa":"<a id='model-evaluation'><\/a>\n## Model Evaluation","b61abb36":"# Flower Recogntion System using GoogleNet","499938b2":"Image-based object classification has always been a hot topic in AI filed, especially using different variation of CNN models to perform better to classifiy an image correctly to a category. In this notebook, one of the variation is used, which is GoogleNet. \n\nThe topic of flower recognition is chosen as flowers have several features of the same species. For example, daisy can have different petal length and width, colours, and radius of pistil, but as human, we know for a fact that it is a daisy. ","961bf06d":"The target size used cannot be smaller than (224,224) as it will affect the calculations in the GoogleNet model.","a20e4e67":"<a id='model-building'><\/a>\n## Build the model","451fb95a":"There are a total of 4934 images. Once appended, the size is (224,224,3). 3 refers to its RGB value.","4b75790a":"After splitting, all of the images in the training, testing and validation sets have to be normalized (divide by 255). Normalization helps to convert an input image to values that are more familiar to the senses.","0da77027":"<a id='splitting'><\/a>\n## Split the data into training, testing and validation"}}