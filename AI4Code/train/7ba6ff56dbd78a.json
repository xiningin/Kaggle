{"cell_type":{"2b5c5541":"code","32d4678e":"code","3da93e13":"code","b8ff12c9":"code","d54d93ce":"code","c0be2c26":"code","ad855348":"code","6ab9ed40":"code","867f21db":"code","2c1eb48e":"code","fdb9f89f":"code","ea18d415":"code","ec774b2e":"code","737ef58c":"code","b5f24c12":"code","d3057883":"code","85923d4d":"code","03c4795c":"code","24716aab":"code","f1720114":"code","01959e56":"code","2192ba6f":"code","85ecad24":"code","7fbe69b2":"code","2fc3668c":"code","ccc84835":"code","07dbe2fd":"code","a826d587":"markdown","2e5ca2fc":"markdown","2b71f1e3":"markdown","35ddf5d9":"markdown","1a83738e":"markdown"},"source":{"2b5c5541":"import tensorflow as tf\nfrom tensorflow import keras","32d4678e":"import numpy as np\nimport pandas as pd","3da93e13":"CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\nSPECIES = ['Setosa', 'Versicolor', 'Virginica']\n# Lets define some constants to help us later on","b8ff12c9":"train_path = tf.keras.utils.get_file(\n    \"iris_training.csv\", \"https:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/iris_training.csv\")\ntest_path = tf.keras.utils.get_file(\n    \"iris_test.csv\", \"https:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/iris_test.csv\")\n\ntrain = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\ntest = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n# Here we use keras (a module inside of TensorFlow) to grab our datasets and read them into a pandas dataframe","d54d93ce":"train.head()","c0be2c26":"y_train = train.pop('Species')\ny_test = test.pop('Species')\n\ntrain.head()","ad855348":"y_train","6ab9ed40":"train.isnull().sum()","867f21db":"# df_dict = dict.fromkeys(train.columns, \"\")\n# train.rename(columns = df_dict)\n# dict(train)\n# dx = tf.data.Dataset.from_tensor_slices((dict(train), y_train))","2c1eb48e":"def input_fn(features, labels, training=True, batch_size=256):\n    # Coverts the inputs into Dataset\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n    \n    # Shuffle and repeat if you are in training mode\n    if training:\n        dataset = dataset.shuffle(1000).repeat()\n    \n    return dataset.batch(batch_size=batch_size)","fdb9f89f":"feature_cols = []\nfor key in train.keys():\n    feature_cols.append(tf.feature_column.numeric_column(key=key))","ea18d415":"print(feature_cols)","ec774b2e":"# Build a DNN Classifier using 2 hidden layers\nclassifier = tf.estimator.DNNClassifier(\n    feature_columns=feature_cols, \n    hidden_units=[30, 10], \n    n_classes=3\n)","737ef58c":"classifier.train(\n    input_fn=lambda: input_fn(train, y_train, training=True), \n    steps=5000\n)\n# We inlcude a lambda to avoid creating an inner function previously","b5f24c12":"eval_result = classifier.evaluate(\n    input_fn=lambda: input_fn(test, y_test, training=False))\n\nprint('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))","d3057883":"val_ranges_max = {\n            'SepalLength': train['SepalLength'].max(),\n            'SepalWidth': train['SepalWidth'].max(), \n            'PetalLength': train['PetalLength'].max(),\n            'PetalWidth': train['PetalWidth'].max()\n            }\n\nval_ranges_min = {\n            'SepalLength': train['SepalLength'].min(),\n            'SepalWidth': train['SepalWidth'].min(), \n            'PetalLength': train['PetalLength'].min(),\n            'PetalWidth': train['PetalWidth'].min()\n            }\n\nval_ranges_max, val_ranges_min","85923d4d":"def input_fn(features, batch_size=256):\n    # Convert the inputs to a Dataset without labels.\n    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\n\nfeatures = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']\npredict = {}\n\nprint(\"Please type numeric values as prompted.\")\nfor feature in features:\n    val = input(feature + f\" (Range: {val_ranges_min[feature]} - {val_ranges_max[feature]}): \")\n    if not val.replace('.', '').isdigit(): \n        raise ValueError(\"Not a Valid Input! Please Enter a number\")\n\n    predict[feature] = [float(val)]\n\npredictions = classifier.predict(input_fn=lambda: input_fn(predict))\nfor pred_dict in predictions:\n    class_id = pred_dict['class_ids'][0]\n    probability = pred_dict['probabilities'][class_id]\n\n    print('Prediction is \"{}\" ({:.1f}%)'.format(\n        SPECIES[class_id], 100 * probability))","03c4795c":"txt = \"54\"\ntxt.replace(\".\", \"\").isdigit()\n","24716aab":"# Sample Inputs and Outputs\nexpected = ['Setosa', 'Versicolor', 'Virginica']\npredict_x = {\n    'SepalLength': [5.1, 5.9, 6.9],\n    'SepalWidth': [3.3, 3.0, 3.1],\n    'PetalLength': [1.7, 4.2, 5.4],\n    'PetalWidth': [0.5, 1.5, 2.1],\n}","f1720114":"CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\nSPECIES = ['Setosa', 'Versicolor', 'Virginica']\n# Lets define some constants to help us later on","01959e56":"train_path = tf.keras.utils.get_file(\n    \"iris_training.csv\", \"https:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/iris_training.csv\")\ntest_path = tf.keras.utils.get_file(\n    \"iris_test.csv\", \"https:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/iris_test.csv\")\n\ntrain = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\ntest = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n# Here we use keras (a module inside of TensorFlow) to grab our datasets and read them into a pandas dataframe","2192ba6f":"train.head()","85ecad24":"X_T = tf.convert_to_tensor(train.values)","7fbe69b2":"if isinstance(train.values, np.ndarray):\n    print(type(train.values))","2fc3668c":"# KMEANS CLUSTERING USING TENSORFLOW:\nimport time\nimport matplotlib.pyplot as plt\n\nclass Kmeans:\n    def __init__(self, k='infer', n_iters = 100):\n        self.clusters = k\n        self.n_iters = n_iters\n        self.centroids = None\n        self.points = None\n         \n    def clusterize(self, X, represent=False):\n        start = time.time_ns()\n        self.points = self.__make_input(X)\n        self.centroids = self.__initialize_centroids(self.points) \n        points_expanded = tf.expand_dims(self.points, 0)\n        centroids_expanded = tf.expand_dims(self.centroids, 1)\n        \n        distances = tf.reduce_sum(tf.square(tf.subtract(points_expanded, centroids_expanded)), 2)\n        assignments = tf.argmin(distances, 0)\n        \n        means = []\n        for c in range(self.clusters):\n            means.append(tf.reduce_mean(\n                tf.gather(X, \n                          tf.reshape(tf.where(\n                                        tf.equal(assignments, c)\n                                        ), [1, -1]\n                                     ),\n                        ), keepdims=True))\n\n        new_centroids = tf.concat(means, 0)\n        # update_centroids = tf.assign(self.centroids, new_centroids)\n        update_centroids = tf.Variable(new_centroids)\n        # init = tf.global_variables_initializer()\n        \n        for step in range(self.n_iters):\n            [_, centroid_values, points_values, assignment_values] = ([update_centroids, self.centroids, self.points, assignments])\n        \n        end = time.time_ns()\n        time_taken = end-start\n\n        if represent:\n            plt.scatter(points_values[:, 0], points_values[:, 1], c=assignment_values, s=50, alpha=0.5)\n            plt.plot(centroid_values[:, 0], centroid_values[:, 1], 'kx', markersize=15)\n            plt.show()\n     \n        return centroid_values, f\"Time Taken: {time_taken\/1e6} ms\"\n        \n    \n    def __make_input(self, X):\n        if isinstance(X, pd.core.frame.DataFrame):\n            return tf.convert_to_tensor(X.values)\n        elif isinstance(X, np.ndarray):\n            return tf.convert_to_tensor(X)\n        else: return X\n        \n    def __initialize_centroids(self, X):\n        return tf.Variable(tf.slice(tf.random.shuffle(X), [0, 0], [self.clusters, -1]))\n       \n","ccc84835":"clusterizer = Kmeans(k=3)\nresults = clusterizer.clusterize(train, True)","07dbe2fd":"results","a826d587":"So we see that our classifier has not performed very well on this dataset. Here we can use classical algorithm like SVM and Logistic Regression to get a more accurate classification or we canc onstruct a better DNN architecture ourselves to see the results we want. ","2e5ca2fc":"# Classification and Clustering using Tensorflow\nIn this notebook I use tensorflow to create a classification model and a Clustering model (Base: Kmeans) on the famous `iris-dataset`.\n### Dataset\nThis specific dataset seperates flowers into 3 different classes of species.\n- Setosa\n- Versicolor\n- Virginica\n\nThe information about each flower is the following.\n- sepal length\n- sepal width\n- petal length\n- petal width","2b71f1e3":"### Building the Model\nAnd now we are ready to choose a model. For classification tasks there are variety of different estimators\/models that we can pick from. Some options are listed below.\n- ```DNN Classifier``` (Deep Neural Network)\n- ```LinearClassifier```\n\nWe can choose either model but the DNN seems to be the best choice. This is because we may not be able to find a linear coorespondence in our data. \n\nSo let's build a model!","35ddf5d9":"### Training","1a83738e":"## Clustering\nClustering is a Machine Learning technique that involves the grouping of data points. In theory, data points that are in the same group should have similar properties and\/or features, while data points in different groups should have highly dissimilar properties and\/or features. (https:\/\/towardsdatascience.com\/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)\n\nUnfortunalty there are issues with the current version of TensorFlow and the implementation for KMeans. This means we cannot use KMeans without writing the algorithm from scratch. We aren't quite at that level yet, so we'll just explain the basics of clustering for now.\n\n### Basic Algorithm for K-Means.\n- Step 1: Randomly pick K points to place K centroids\n- Step 2: Assign all the data points to the centroids by distance. The closest centroid to a point is the one it is assigned to.\n- Step 3: Average all the points belonging to each centroid to find the middle of those clusters (center of mass). Place the corresponding centroids into that position.\n- Step 4: Reassign every point once again to the closest centroid.\n- Step 5: Repeat steps 3-4 until no point changes which centroid it belongs to."}}