{"cell_type":{"f2d8eecb":"code","efe1f6ea":"code","ec2b4737":"code","ffdb1dc6":"code","db3598f8":"code","5cff26b8":"code","953315ec":"code","b01e904b":"code","81d22d01":"code","5732d074":"code","c471ed78":"code","c12252b7":"code","b04357b8":"code","6676dca1":"code","4a690d99":"code","de152234":"code","f96d29ba":"code","1061612f":"code","a23fdd23":"code","198d8474":"code","e34b468d":"code","81cf5da6":"code","3a693ca9":"code","1a2cbf25":"code","3a08da5d":"code","5a8f0005":"code","3e219e5f":"code","e7422447":"code","f7f821ff":"code","9a257dbc":"code","62c439ab":"markdown","cbc6f5ac":"markdown","6857322b":"markdown","643a043f":"markdown","9438ab90":"markdown","b5cde7d9":"markdown","54e866cf":"markdown","51388b06":"markdown","b00266ab":"markdown","289684da":"markdown","f5b28717":"markdown","cf249f2e":"markdown","5dc4f120":"markdown","c6dae075":"markdown","230ef412":"markdown","2f0b0d96":"markdown","5069eab0":"markdown","89782c71":"markdown","cfacb1d4":"markdown","d46a5ed3":"markdown","79cbfd9a":"markdown","e5668dc1":"markdown","e08d349b":"markdown","f1b75c06":"markdown"},"source":{"f2d8eecb":"import numpy as np \nimport pandas as pd\nimport gc","efe1f6ea":"init_df = pd.read_csv('..\/input\/real-or-fake-fake-jobposting-prediction\/fake_job_postings.csv')\ninit_df.head()","ec2b4737":"init_df.nunique()","ffdb1dc6":"drop_columns = ['job_id', 'title', 'location', 'department', 'industry', 'function']\n\nproc_df = init_df.drop(drop_columns, axis=1)\n\ndel init_df\ngc.collect()","db3598f8":"proc_df.isnull().sum()","5cff26b8":"cat_columns = ['employment_type', 'required_experience', 'required_education']\n\nfor col in cat_columns:\n    proc_df[col].fillna(\"Unknown\", inplace=True)\n","953315ec":"text_columns = ['company_profile', 'description', 'requirements', 'benefits']\n\nproc_df = proc_df.dropna(subset=text_columns, how='all')\n\nfor col in text_columns:\n    proc_df[col].fillna(' ', inplace=True)\n    ","b01e904b":"unique_salary = proc_df['salary_range'].unique()\nprint(unique_salary[0:5])","81d22d01":"new = proc_df['salary_range'].str.split(\"-\", n = 1, expand = True) \n\nproc_df['salary_range_min']= new[0]\nproc_df['salary_range_max']= new[1]\n\nproc_df['salary_range_min'].fillna('-1', inplace=True)\nproc_df['salary_range_max'].fillna('-1', inplace=True)\n\ndef remove_string(x):\n    if not x.isnumeric(): \n        val = '-1'\n    else:\n        val = x\n    return val\n\nproc_df['salary_range_min'] = proc_df['salary_range_min'].apply(lambda x: remove_string(x))\nproc_df['salary_range_max'] = proc_df['salary_range_max'].apply(lambda x: remove_string(x))\n\nproc_df.drop('salary_range', axis=1, inplace = True) ","5732d074":"cat_eda_columns = ['telecommuting', 'has_company_logo', 'has_questions', 'employment_type', 'required_experience', 'required_education']\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.gridspec as gridspec \n\ngrid = gridspec.GridSpec(5, 2, wspace=0.5, hspace=0.5) \nplt.figure(figsize=(15,25)) \n\nfor n, col in enumerate(proc_df[cat_eda_columns]): \n    ax = plt.subplot(grid[n]) \n    sns.countplot(x=col, data=proc_df, hue='fraudulent', palette='Set2', order=proc_df[col].value_counts().iloc[:5].index) \n    ax.set_ylabel('Count', fontsize=12)\n    ax.set_title(f'{col} Distribution by Target', fontsize=15) \n    xlabels = ax.get_xticklabels() \n    ax.set_xticklabels(xlabels,  fontsize=10)\n    plt.legend(fontsize=8)\n    plt.xticks(rotation=30) \n    total = len(proc_df)\n    sizes=[] \n    for p in ax.patches: \n        height = p.get_height()\n        sizes.append(height)\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total*100),\n                ha=\"center\", fontsize=10) \n    \nplt.show()\n","c471ed78":"text_cols = ['company_profile', 'description', 'requirements', 'benefits']\n\nfor col in text_cols:\n    fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(10, 2.5), dpi=100)\n    num=proc_df[proc_df[\"fraudulent\"]==1][col].str.split().map(lambda x: len(x))\n    ax1.hist(num,bins = 20,color='orangered')\n    ax1.set_title('Fake Post')\n    num=proc_df[proc_df[\"fraudulent\"]==0][col].str.split().map(lambda x: len(x))\n    ax2.hist(num, bins = 20)\n    ax2.set_title('Real Post')\n    fig.suptitle(f'Words in {col}')\n    plt.show()","c12252b7":"text_cols = ['company_profile', 'description', 'requirements', 'benefits']\n\nproc_df['aggr_post'] = proc_df[text_cols].apply(lambda x: ' '.join(x), axis=1)\nproc_df.drop(text_cols, axis=1, inplace=True)\n\nproc_df.head()\n\nprint(proc_df.loc[0, 'aggr_post'])","b04357b8":"import langid\n\ndef detect_lang(x):\n    code,_ = langid.classify(x)\n    \n    return code\n\nproc_df = proc_df[proc_df['aggr_post'].apply(lambda x: detect_lang(x) == 'en')]\n\nproc_df.head()","6676dca1":"import re\nimport string\n\ndef clean_text(text):\n    text = text.lower()                                              # make the text lowercase\n    text = re.sub('\\[.*?\\]', '', text)                               # remove text in brackets\n    text = re.sub('http?:\/\/\\S+|www\\.\\S+', '', text)                  # remove links\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)                 # remove links\n    text = re.sub('<.*?>+', '', text)                                # remove HTML stuff\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  # get rid of punctuation\n    text = re.sub('\\n', '', text)                                    # remove line breaks\n    #text = re.sub('\\w*\\d\\w*', '', text)                             # remove anything with numbers, if you want\n    #text = re.sub(r'[^\\x00-\\x7F]+',' ', text)                       # remove unicode\n    return text\n\nproc_df['aggr_post'] = proc_df['aggr_post'].apply(lambda x: clean_text(x))\n\nproc_df.head()","4a690d99":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\nstop_words = stopwords.words('english')\n\ndef remove_stopwords(text):\n    words = [w for w in text if w not in stop_words]\n    return words\n\ndef combine_text(list_of_text):\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\nproc_df['aggr_post'] = proc_df['aggr_post'].apply(lambda x: tokenizer.tokenize(x))\nproc_df['aggr_post'] = proc_df['aggr_post'].apply(lambda x : remove_stopwords(x))\nproc_df['aggr_post'] = proc_df['aggr_post'].apply(lambda x : combine_text(x))\n\nproc_df.head()","de152234":"random_state = 42\n\nreal_df = proc_df[proc_df['fraudulent']==0].copy()\nfake_df = proc_df[proc_df['fraudulent']==1].copy()\n\nreal_sampled_df = real_df.sample(n=3000, random_state=random_state)\n\nfinal_df = pd.concat([real_sampled_df, fake_df], axis=0)","f96d29ba":"del proc_df\ndel real_df\ndel real_sampled_df\ndel fake_df\n\ngc.collect()","1061612f":"from sklearn.model_selection import train_test_split\n\nseed_state = 315\nrandom_state = 42\n\nreal_df = final_df[final_df['fraudulent']==0]\nfake_df = final_df[final_df['fraudulent']==1]\n\ny_real = real_df['fraudulent'].copy()\nx_real = real_df.drop(['fraudulent'], axis=1)\n\ny_fake = fake_df['fraudulent'].copy()\nx_fake = fake_df.drop(['fraudulent'], axis=1)\n\nx_real_tv, x_real_test, y_real_tv, y_real_test = train_test_split(x_real, y_real, test_size=0.3, random_state=seed_state)\nx_real_train, x_real_val, y_real_train, y_real_val = train_test_split(x_real_tv, y_real_tv, test_size=0.2, random_state=seed_state)\n\nx_fake_tv, x_fake_test, y_fake_tv, y_fake_test = train_test_split(x_fake, y_fake, test_size=0.3, random_state=seed_state)\nx_fake_train, x_fake_val, y_fake_train, y_fake_val = train_test_split(x_fake_tv, y_fake_tv, test_size=0.2, random_state=seed_state)\n\nx_train = pd.concat([x_real_train, x_fake_train])\ny_train = pd.concat([y_real_train, y_fake_train])\n\nx_val = pd.concat([x_real_val, x_fake_val])\ny_val = pd.concat([y_real_val, y_fake_val])\n\nx_test = pd.concat([x_real_test, x_fake_test])\ny_test = pd.concat([y_real_test, y_fake_test])","a23fdd23":"x_train_post = x_train['aggr_post'].copy()\nx_val_post = x_val['aggr_post'].copy()\nx_test_post = x_test['aggr_post'].copy()\n\nx_train_cat = x_train.drop(['aggr_post'], axis=1)\nx_val_cat = x_val.drop(['aggr_post'], axis=1)\nx_test_cat = x_test.drop(['aggr_post'], axis=1)","198d8474":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix, mean_absolute_error, make_scorer \n\n# Showing Confusion Matrix\n# Thanks to https:\/\/www.kaggle.com\/marcovasquez\/basic-nlp-with-tensorflow-and-wordcloud\ndef plot_cm(y_true, y_pred, title):\n    figsize=(14,14)\n    y_pred = y_pred.astype(int)\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","e34b468d":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, f1_score, mean_absolute_error, make_scorer \n\ncount_vectorizer = CountVectorizer()\nx_train_post_vec = count_vectorizer.fit_transform(x_train_post)\nx_val_post_vec = count_vectorizer.transform(x_val_post)\nx_test_post_vec = count_vectorizer.transform(x_test_post) \n\nlr_post = LogisticRegression(C=0.1, solver='lbfgs', max_iter=2000, verbose=0, n_jobs=-1)\nlr_post.fit(x_train_post_vec, y_train)","81cf5da6":"weights = lr_post.coef_\nabs_weights = np.abs(weights)","3a693ca9":"lr_post_val_preds = lr_post.predict(x_val_post_vec)\n\nf1_score(y_val, lr_post_val_preds, average = 'macro')\nplot_cm(y_val, lr_post_val_preds, 'Confusion Matrix: LR Validation Set Predictions ')","1a2cbf25":"lr_post_test_preds = lr_post.predict(x_test_post_vec)\n\nf1_score(y_test, lr_post_test_preds, average = 'macro')\nplot_cm(y_test, lr_post_test_preds, 'Confusion Matrix: LR Test Set Predictions ')","3a08da5d":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\nle_employment = LabelEncoder()\nle_experience = LabelEncoder()\nle_education  = LabelEncoder()\n\nx_train_cat['employment_type'] = le_employment.fit_transform(x_train_cat['employment_type'])\nx_val_cat['employment_type'] = le_employment.transform(x_val_cat['employment_type'])\nx_test_cat['employment_type'] = le_employment.transform(x_test_cat['employment_type'])\n\nx_train_cat['required_experience'] = le_experience.fit_transform(x_train_cat['required_experience'])\nx_val_cat['required_experience'] = le_experience.transform(x_val_cat['required_experience'])\nx_test_cat['required_experience'] = le_experience.transform(x_test_cat['required_experience'])\n\nx_train_cat['required_education'] = le_education.fit_transform(x_train_cat['required_education'])\nx_val_cat['required_education'] = le_education.transform(x_val_cat['required_education'])\nx_test_cat['required_education'] = le_education.transform(x_test_cat['required_education'])\n\nrf_cat = RandomForestClassifier(n_estimators=2000,bootstrap=True)\nrf_cat.fit(x_train_cat, y_train)","5a8f0005":"rf_cat_val_pred = rf_cat.predict(x_val_cat)\n\nf1_score(y_val, rf_cat_val_pred.round(), average = 'macro')\nplot_cm(y_val, rf_cat_val_pred.round(), 'Confusion Matrix: RF Validation Set Predictions ')","3e219e5f":"rf_cat_test_pred = rf_cat.predict(x_test_cat)\n\nf1_score(y_test, rf_cat_test_pred.round(), average = 'macro')\nplot_cm(y_test, rf_cat_test_pred.round(), 'Confusion Matrix: RF Test Set Predictions ')","e7422447":"aggregate_val = pd.DataFrame()\naggregate_val['post_preds'] = lr_post_val_preds\naggregate_val['cat_preds'] = rf_cat_val_pred\naggregate_val.head()","f7f821ff":"aggregate_test = pd.DataFrame()\naggregate_test['post_preds'] = lr_post_test_preds\naggregate_test['cat_preds'] = rf_cat_test_pred\naggregate_test.head()","9a257dbc":"lr_final = LogisticRegression(C=0.1, solver='lbfgs', max_iter=2000, verbose=0, n_jobs=-1)\nlr_final.fit(aggregate_val, y_val)\n\nlr_final_preds = lr_final.predict(aggregate_test)\n\nf1_score(y_test, lr_final_preds, average = 'macro')\nplot_cm(y_test, lr_final_preds, 'Confusion Matrix: Aggregate Model Final Predictions ')","62c439ab":"# Data Cleaning\n\nFirst, let's take a step in understanding and, subsequently, cleaning our dataset by taking a look at how many unique values are in each column.","cbc6f5ac":"Now that we have our final dataset, we get rid of our unused ones to keep memory free.","6857322b":"## Final Aggregate Model","643a043f":"We see that the *employment_type*, *required_experience* and *required_education* columns have a significant amout of missing values and only have 5, 7 and 13 unique object\/categorical values, respectively. As these are manageable amounts, we replace any missing values with a new object\/category value of *Unknown.*","9438ab90":"## Linear Regression Model 1 : Job Post Content\n","b5cde7d9":"To help visualize our results, I include a nifty confusion matrix function that I've been using in my recent notebooks.","54e866cf":"# Exploratory Data Analysis\n\nNow that we're finished with cleaning\/touching up our data, for now, we can move on to some EDA to get a better idea of data. First, we take a look at how many counts of real and fake posts there are, in relation to the top unique values of a feature.","51388b06":"## Random Forest Classifier : All Other Features","b00266ab":"Interestingly enough word counts of both posts match a similar distribution for all sections except the company profile one! We can guess intuitively that a fake job posting will try to entice the individual with the description (\"easy, fun hours!\"), requirements (\"none!\"), and benefits (\"full!\"), hoping that they will not look into the company, which doesn't exist and is using some boilerplate \"professional\" website.","289684da":"Not surprisingly, there are significantly fewer fraudulent posts per feature value for each value because there are significantly fewer fraudulent posts in the dataset (800 vs ~17,000). Even so, we get the idea that fraudulent posts match closely real posts. We see that:\n* Fraudulent posts are mostly not posted as telecommuting ones, like real posts.\n* Fraudulent posts mostly do not contain a company logo, unlike real posts.\n* Fraudulent posts have an equal mix of either having a questionnaire or not; like real posts.\n* Fraudulent posts are mostly for full-time \"positions,\" like real posts.\n* Fraudulent posts, like real posts, also do not specify the required experience and education necessary.\n\nAs we are realists, we can see that fake posts try to tick off all the main boxes of real posts to try to match them as closely as possible and entice unsuspecting people into their nefarious, insidious schemes. \n\nEven though some red flags pop up in this overview of determining a job posting's validity with the above markers, we feel they might not be enough especially when the dataset is so skewed.\n\nWe try to see if maybe there are some more significant markers for validity in the structure of the meat-and-potatoes of the job posting by looking at the word counts of each write-up section.","f5b28717":"# Data Preprocessing\n\nNow that we've got some EDA done and a better grasp of our data, we move on to some more preprocessing. \n\nAlthough we found that a fake post has significantly fewer words in the company profile section than a real post, we do not want to use the count of words to determine whether a post is real or not; We want to try to use the content (and maybe some metadata features) of the overall job post to make that determination! So, we elect to concatenate the *company_profile*, *description*, *requirements*, and *benefits* features into one feature to contain all of the text in all of the sections.\n\nWe do this and verify it below:","cf249f2e":"As the *company_profile*, *description*, *requirements* and *benefits* features contain string values of various lengths where the underlying information needs to be uncovered with text\/language analysis, we will fill any empty values with a blank space, ' ', instead of 'Unknown'. This is so that we do the text analysis, we only do it on what text is already there, not text we introduce, and blank space will just be ignored during said analysis.","5dc4f120":"We end up consolidating everything in the *aggr_post* feature and everything seems to look good! Now we are going to do two things:\n\n1) Remove any rows that contain job posts that aren't in english to keep things simple.\n\n2) *Normalize* our new feature, by cleaning it up by removing extraneous fluff like brackets, links, punctuation, capitalization, etc to turn it into essentially an excerpt from a Cormac McCarthy novel.","c6dae075":"# Introduction\n\nIn this exercise, we will explore a dataset, composed of 17,200 real and 800 fake jobpostings, try to see if we are able to use machine learning techniques to glean meaningful insight from it and construct models that are able to accurately predict whether a previously unseen job posting is real or not.\n\nFirst, let's read in the data and see what we are dealing with.","230ef412":"As we will be looking at the content of each job post, we will first vectorize our text and use that as our training input for our Linear Regression Model.","2f0b0d96":"We see that there are some columns that may be too difficult to deal with or reconcile. We drop the following columns for the following respective reasons:\n* The *job_id* feature because there is a unique value for every single entry and we can not get any useful information from this.\n* The *job_title* feature because it is probably irrelevant, especially when it is notorious that a lot of job titles are superfluous and fluff.\n* The *location* feature because we do not want our model to care about location and keep performance generalized for a job anywhere in the world.\n* The *department* because we feel it will be like *job_title,* where the department would vary too much by posting and not be meaningful enough to draw anything from.\n* The *industry* function for the same reasons as *job_title* and *department.*\n* The *function* function for the same reasons as *job_title* and *department.*","5069eab0":"Now that we have clean, uniform text in our new feature, we take the additional step of removing stopwords from our job posts. Essentially, when we do our text analysis and feed it to our model, we don't want our model to make a prediction based on how many words like \"and\", \"the\", \"or\", etc there are!\n\nSo first, we tokenize each *aggr_post* value, remove stopword tokens, join the stopword tokens back and repopulate our values!","89782c71":"To ensure we have enough samples of fraudulent job postings in our training, validation, testing sets, we do splits on the distinct values and rejoin them for our final sets.\n\nTo make our validation sets, we do a split on the overall sets to make a buffer set (along with a testing set) and then do another split on the buffer set (resulting in a training set and a validation set).","cfacb1d4":"# Training\/Validation\/Testing Set Preparation\n\nNow that we've cleaned up our dataset to the best of our ability, we are able to move on to splitting up our dataset into our necessary training, validation and testing sets for model building.\n\nAs we know that in this dataset, the number of real job posts greatly outnumber the number of fraudulent posts. To overcome any bias\/skew we may run into with this disparity, we create a final working dataframe with a downsampled number of real job posts (specifically 3,000).","d46a5ed3":"From a glance at the head of the dataset, we find the following features:\n* *job_id*             : Intuitively, this is just the unique ID for every single entry in the dataset. \n* *title*              : The job title or position. Most likely unique for each entry.\n* *location*           : The job's location in format: Country, State, City. \n* *department*         : What department in the organization, the job is part of. Most likely unique for each posting\n* *salary_range*       : The salary range for the position. From an initial glance of the head, we see its blank; However, in subsequent analysis, we see that it is in format MIN-MAX\n* *company_profile*    : An overview of the company.\n* *description*        : An overview of the job description.\n* *requirements*       : An overview of the preferred experience and requirements necessary to perform the job.\n* *benefits*           : An overview of what benefits this job\/company offers.\n* *telecommuting*      : Whether this job is telecommuting (1) or not (0).\n* *has_company_logo*   : Whether this job posting has the company logo or not.\n* *has_questions*      : Whether this job posting has a questionaire attached to it.\n* *employment_type*    : Whether this job is hourly, part-time, full-time, etc.\n* *required_education* : What degree is necessary: None, BS, MS, etc.\n* *industry*           : What industry this job is in: Fashion, IT, etc. Could possibly unique for each job.\n* *function*           : Keyword for the job. Could possible unique for each job.\n* *fraudulent*         : Target label. 0 if real job post. 1 if fake job post.\n\nAlthough we have a lot of information in this dataset, we may not be able to make use of all of it, especially when it comes to a certain feature's redundancy or irrelevance. For example,the job title of a post may most likely have nothing to do whether a job posting is fraudulent or not. We may know this, but our models may not and may end up viewing it as more significant than it actually is; So we may end up having to drop that feature.\n\nMore on that later. \n\n","79cbfd9a":"# Model Building\n\nAs we have a significant amount of information in our dataset with the metadata features and overall text of our job posts, we will take a two pronged approach. We will construct three models:\n* A linear regression model to make predictions solely on the text content of a job post.\n* A random forest model to make predictions solely on the metadata features of a job post.\n* A final linear regression model to use the predictions, of the first two models, together to make a final determination. We do this to see if we are able to improve our predictions.\n\nSo, we will split our training, validation, and testing sets into two more of each so that one group just has the text feature, and the other has the rest of the features.","e5668dc1":"When we reviewed the head of our dataframe, we also saw that a lot of different feature values were missing. Let's explore just how many missing values there are for each feature:","e08d349b":"Thankfully the *salary_range* feature contains meaningful and consistent features, in the format of **Min Range**-**Max Range**. From a review of all the unique values (not shown), we also see how alot of the ranges share some overlap, are not consistent and contain incorrect values (such as Dec, Oct, etc). To reconcile these inconsistencies, we choose to:\n\n* First split the 'Min-Max' string to get the discrete Min and Max values.\n* Store the Min and Max values in the new *salary_range_min* and *salary_range_max* features, respectively.\n* Replace any null and non-numeric values in new features with '-1'\n* Drop the original *salary_range* feature.","f1b75c06":"We also saw how in the header entries, the *salary_range* values were all empty. Before we reconcile the null values, we take a look at what the actual values are by looking at the first few unique possibilities."}}