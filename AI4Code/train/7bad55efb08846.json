{"cell_type":{"8aae607d":"code","9fd5b9d3":"code","45f3b26c":"code","547cb534":"code","647fe9d6":"code","a404f3eb":"code","41b3e837":"code","b6891b91":"code","e6045be0":"code","c313495a":"code","cccfaf81":"code","87a9f6b2":"code","d247f136":"code","08c8cac3":"code","66ff5feb":"code","87e4ffe7":"code","39ec67b9":"code","9fc4e578":"code","28e1be01":"code","4f088f16":"code","7ca4cbe9":"code","a1ddf516":"code","74a9e8de":"code","ab1b0429":"markdown","9c657e67":"markdown","d6a75d41":"markdown","e1a84858":"markdown","8fb344e2":"markdown","070a629e":"markdown","e7762bce":"markdown","062e3262":"markdown","8aeb0478":"markdown","a2cb211b":"markdown","ac609c2a":"markdown","a09563b3":"markdown","f73f6c3e":"markdown","a9aef0e2":"markdown","7ee6c188":"markdown","38dc3a23":"markdown","2d79e156":"markdown","b2ffaead":"markdown","106f1d83":"markdown","88a9805d":"markdown","6aa707a5":"markdown","f4792b59":"markdown","6b4d114c":"markdown"},"source":{"8aae607d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9fd5b9d3":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas_profiling\nfrom pandas_profiling import ProfileReport\n%matplotlib inline\n\n# Import label encoder:\nimport sklearn\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\n#Import libraries for model selection and building:\nimport sklearn.tree as tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score, confusion_matrix, roc_auc_score,roc_curve\nfrom sklearn.model_selection import learning_curve, GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.tree import export_graphviz\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#print(os.getcwd())","45f3b26c":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\").set_index('PassengerId')\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\").set_index('PassengerId')\nsurvive = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\").set_index('PassengerId')","547cb534":"print(\"The dimension of Train Dataset is:\", train.shape)\ntrain.head()","647fe9d6":"print(\"The dimension of Test Dataset is:\", test.shape)\ntest.head()","a404f3eb":"# embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n# 1 -> Survived, 0-> Did not survive\n\nprint(\"Train Dataset - Survival Number as per Sex, PClass, Embark\")\nsurvival_rate = pd.DataFrame(train.groupby([\"Sex\", \"Pclass\", \"Embarked\"])[\"Survived\"].count()).style.background_gradient(cmap=\"bone_r\")\nsurvival_rate","41b3e837":"ProfileReport(train)","b6891b91":"ProfileReport(test)","e6045be0":"# Drop the columns 'cabin' from training and test data as it contains more than 30% missing values\n\ncol_drop = ['Cabin']\n\ntrain.drop(col_drop, axis = 1, inplace = True)\ntest.drop(col_drop, axis = 1, inplace = True)","c313495a":"# Filling the missing Age variable with mean value \n\ntrain['Age'] =train['Age'].fillna(train['Age'].mean())\ntest['Age'] =test['Age'].fillna(test['Age'].mean())","cccfaf81":"# Filling the missing Emberked variable with 'S' as the largerst value is 'S' \n\ntrain['Embarked'] =train['Embarked'].fillna('S')\ntest['Embarked'] =test['Embarked'].fillna('S')","87a9f6b2":"# Filling the missing Age variable with mean value \ntest['Fare'] =test['Fare'].fillna(test['Fare'].mean())","d247f136":"# Seperating the title from the name\ntrain['Title'] = train.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ntest['Title'] = test.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\n\nprint(\"Train[Title] List of unquie values\", \"(\", train['Title'].nunique(), \") :\" , train['Title'].unique())\nprint(\" \")\nprint(\"Test[Title] List of unquie values\", \"(\", test['Title'].nunique(), \") :\" ,test['Title'].unique())","08c8cac3":"# Grouping the Title into 5 Category (Mr, Mrs, Miss, Master, Dr)\n\ntitle_replace_Mr = ['Don','Rev','Sir','Col','Capt','Jonkheer','Major']\ntitle_replace_Mrs = ['Mme','Lady','Mlle','the Countess']\ntrain['Title'] = train['Title'].replace('Ms','Miss')\ntrain['Title'] = train['Title'].replace(title_replace_Mr,'Mr')\ntrain['Title'] = train['Title'].replace(title_replace_Mrs,'Mrs')\n\ntest_title = ['Col','Rev']\ntest['Title'] = test['Title'].replace('Ms','Miss')\ntest['Title'] = test['Title'].replace('Dona','Mrs')\ntest['Title'] = test['Title'].replace(test_title,'Mr')","66ff5feb":"# Dropping unnecessary variable\ncols_to_drop = ['Name','Ticket']   \ntrain = train.drop(cols_to_drop, axis=1)   \ntest = test.drop(cols_to_drop, axis=1)","87e4ffe7":"# Title is as per age\n# 1 -> Survived, 0-> Did not survive\n\nprint(\"Train Dataset - Survival Numbers as per Pclass & Title\")\nsurvival_rate1 = pd.DataFrame(train.groupby([\"Pclass\", \"Title\"])[\"Survived\"].count()).style.background_gradient(cmap=\"bone_r\")\nsurvival_rate1","39ec67b9":"# label_encoder object knows how to understand word labels. \nle = LabelEncoder()\n  \n# Encode labels in column 'species'. \ntrain['Sex']= le.fit_transform(train.Sex)\ntrain['Embarked'] = le.fit_transform(train.Embarked)\ntrain['Title'] = le.fit_transform(train.Title)\n\ntest['Sex']= le.fit_transform(test.Sex)\ntest['Embarked'] = le.fit_transform(test.Embarked)\ntest['Title'] = le.fit_transform(test.Title)","9fc4e578":"X_train = train.drop('Survived', axis=1)\nY_train = train['Survived']\nX_test = test.copy()\nY_test = survive[\"Survived\"]\n\nprint(\"Dimension of X_Train:\", X_train.shape)\nprint(\"Dimension of X_Test:\", Y_train.shape)\nprint(\"Dimension of Y_Train:\", X_test.shape)\nprint(\"Dimension of Y_Test:\", Y_test.shape)","28e1be01":"DT = tree.DecisionTreeClassifier(max_depth=3, random_state=200)\nDT.fit(X_train, Y_train)\nDT_Y_Pred = DT.predict(X_test)","4f088f16":"print(\"Decsison Tree Model Results:\\n\")\nprint(\"Accuracy Score:\", round(accuracy_score(Y_test, DT_Y_Pred),2)*100,\"%\")\nprint(\"***************************************************\\n\")\nprint(\"Classification Report:\\n\", classification_report(Y_test, DT_Y_Pred))\nprint(\"***************************************************\\n\")\nprint(\"Confusion Matrix:\\n\", confusion_matrix(Y_test, DT_Y_Pred))\nprint(\"***************************************************\")","7ca4cbe9":"CM = pd.DataFrame(confusion_matrix(Y_test, DT_Y_Pred))\n\nplt.figure(figsize=(10,5))\nsns.heatmap(CM, annot=True,  fmt=\".0f\", annot_kws={\"size\": 20}, cmap=\"cividis_r\", linewidths=0.9)\nplt.title('Confusion matrix for Decision Tree Model', y=1.1, fontdict = {'fontsize': 20})\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","a1ddf516":"## ROC curve for RF:\nfpr, tpr, _ = metrics.roc_curve(Y_test, DT_Y_Pred)\nauc = metrics.roc_auc_score(Y_test, DT_Y_Pred)\n\nplt.figure(figsize=(10,5))\nplt.style.use('seaborn')\nplt.plot(fpr,tpr,label=\"AUC =\"+str(auc))\nplt.plot([0,1],[0,1],\"r--\")\nplt.title(\"ROC for Decision Tree model\", fontdict = {'fontsize': 20})\nplt.xlabel(\"True positive_rate\")\nplt.ylabel(\"False positive_rate\")\nplt.legend(loc= 4, fontsize = \"x-large\")","74a9e8de":"Final_Submission = pd.DataFrame()\nFinal_Submission['Survived'] = DT_Y_Pred\nFinal_Submission.to_csv('F:\\BA - Jigsaw\\Kaggle\\Titanic Machine Learning from Disaster\\Final_Submission.csv')","ab1b0429":"#Train dataset:\nmodel_selection = dabl.SimpleClassifier(random_state=0).fit(train, target_col=\"Survived\") ","9c657e67":"<div class=\"alert alert-block alert-info\">\n<h1>4 Exploratory data Analysis with dabl (Data Analysis Baseline Library) <\/h1><\/div>  <a class=\"anchor\" id=\"4\"><\/a>\n\n**dabl** has been created by [Andreas Mueller](https:\/\/amueller.github.io\/) and it tries to help make supervised machine learning more accessible for beginners, and reduce boiler plate for common tasks. Dabl takes inspirations from scikit-learn and auto-sklearn. Refer to the official [website](https:\/\/amueller.github.io\/dabl\/dev\/index.html) for more info.\n\n[Back to Table of Contents](#0.1)","d6a75d41":"<div class=\"alert alert-block alert-info\">\n<h1>9. Decision Tree Model<\/h1><\/div> <a class=\"anchor\" id=\"9\"><\/a>\n\n[Back to Table of Contents](#0.1)","e1a84858":"<div class=\"alert alert-block alert-info\">\n<h1>5. Feature Engineering<\/h1><\/div> <a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","8fb344e2":"### Note: dabl & pydotplus libraries were not loading in the karnel however the codes are mentioned in markdown.","070a629e":"<div class=\"alert alert-block alert-warning\">\n<h1>7.1 Observation<\/h1><\/div> <a class=\"anchor\" id=\"7.1\"><\/a>\n\n**Decision Tree Classifier** is the best model with <br>\n    accuracy: 0.828 <br> average_precision: 0.793 <br> roc_auc: 0.861 <br> recall_macro: 0.812<br> f1_macro: 0.816<br>\n    ","e7762bce":"<div class=\"alert alert-block alert-info\">\n<h1>10. Summary<\/h1><\/div> <a class=\"anchor\" id=\"10\"><\/a>\n\n[Back to Table of Contents](#0.1)","062e3262":"- Libraries not able to install in kaggle kernle <br>\n#!pip install dabl<br>\n#import dabl<br>\n\n- Plotting the decision tree:<br>\n#import pydotplus<br>\n\n- Displaying the image of the decision tree:<br>\n#from IPython.display import Image<br>","8aeb0478":"<div class=\"alert alert-block alert-info\">\n<h1>3. Exploratory Data Analysis with Pandas profiling<\/h1><\/div> <\/h1><\/div><a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","a2cb211b":"### Description: \nThe titanic Dataset is analysed and with machine learning technique, survival prediction is made to find out accurate survival rate.","ac609c2a":"<div class=\"alert alert-block alert-warning\">\n<h1>3.1 Observations from train dataset <\/h1><\/div> <\/h1><\/div><a class=\"anchor\" id=\"3.1\"><\/a>\n\n\n* Total passengers in the train set is 891.\n* 549 (61.6%) **did not survive** in the accident.\n* 491 (55.1%) were **3rd class passengers** while 216 (24.2%) were 1st class passengers.\n* 577 (64.8%) were **male passengers**.\n* Few passengers have **Age** range (0.42-0.92) - preferably outlier or error values. Also 19.9% of the data is missing\n* **Cabin** variable has 77.1% missing variables, will drop this variable.\n* **Fare** variable might have outlier.\n","a09563b3":"<div class=\"alert alert-block alert-info\">\n<h1> Table of Contents<\/h1><\/div>\n\n<a class=\"anchor\" id=\"0.1\"><\/a>\n1. [Importing Libararies](#1)\n2. [Loading Dataset](#2)\n3. [Exploratory Data Analysis with Pandas profiling](#3)<br>\n    3.1 [Observations](#3.1)\n4. [Exploratory Data Analysis with dabl library](#4)\n5. [Feature Engineering](#5)\n6. [Label Encoder](#6)\n7. [Initial Model Building with dabl Library](#7)<br>\n    7.1 [Observation](#7.1)\n8. [Spliting dataset for Model builidng](#8)\n9. [Decision Tree Model](#9) <br>\n    9.1 [Plotting the Decision Tree](#9.1)\n10. [Summary](#10)","f73f6c3e":"<div class=\"alert alert-block alert-info\">\n<h1>2. Loading Dataset <\/h1><\/div> <\/h1><\/div><a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","a9aef0e2":"___","7ee6c188":"dabl.plot(train, target_col=\"Survived\")","38dc3a23":"<div class=\"alert alert-block alert-info\">\n<h1>9.1 Plotting the Decision Tree<\/h1><\/div> <a class=\"anchor\" id=\"9.1\"><\/a>","2d79e156":"<div class=\"alert alert-block alert-info\">\n<h1>8. Spliting dataset for Model builidng<\/h1><\/div> <a class=\"anchor\" id=\"8\"><\/a>\n\n[Back to Table of Contents](#0.1)","b2ffaead":"<div class=\"alert alert-block alert-info\">\n<h1>1. Importing Libraries<\/h1><\/div><a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","106f1d83":"<div class=\"alert alert-block alert-info\">\n<h1>7. Initial Model Building with dabl Library<\/h1><\/div> <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","88a9805d":"<h1> Titanic - Sinking of the unsinkable <\/h1>\n\nThe RMS Titanic, a luxury steamship, sank in the early hours of April 15, 1912, off the coast of Newfoundland in the North Atlantic after sideswiping an iceberg during its maiden voyage. Of the 2,240 passengers and crew on board, more than 1,500 lost their lives in the disaster.","6aa707a5":"- Decision Tree classifier is choosen to predict the survival accuracy based dabl library for model selection.\n- Accuracy and AUC for DTC is 94%.\n- The best model for this dataset.","f4792b59":"os.environ[\"PATH\"] += os.pathsep + 'C:\/Program Files (x86)\/graphviz-2.38\/release\/bin\/'\nos.chdir('F:\\BA - Jigsaw\\Kaggle\\Titanic Machine Learning from Disaster')\n\ndot_data = tree.export_graphviz(DT, out_file=None, feature_names= test.columns, class_names= ['0','1'], filled= True, rounded=True, special_characters=True, proportion=True)\ngraph = pydotplus.graph_from_dot_data(dot_data)\n\nprint(\"**Decision Tree Classifier**\")\nImage(graph.create_png())","6b4d114c":"<div class=\"alert alert-block alert-info\">\n<h1>6. Label Encoder<\/h1><\/div> <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)"}}