{"cell_type":{"4aee9f7f":"code","fcdba5f6":"code","0b891504":"code","456591bb":"code","493d2844":"code","a1a01a10":"code","5bd05c73":"code","94f38f48":"code","b07b38d1":"code","7319b330":"code","e4ae96bd":"code","f6ebeda5":"code","737ecfe8":"code","fbb2d46a":"code","9fb5ecb9":"code","60d052a3":"markdown","93551d37":"markdown","57fffaab":"markdown","bd029353":"markdown","c611ffa5":"markdown","ffbb64aa":"markdown","a98bf2b9":"markdown","9646f9a7":"markdown","32269373":"markdown","2f491e2f":"markdown","8aa361e2":"markdown","ddda946c":"markdown","ea45dbaa":"markdown","68083587":"markdown","8bbfbb62":"markdown","4aabb0ed":"markdown","0b117028":"markdown","44c9898e":"markdown","59a69399":"markdown","617f7c06":"markdown","1830419b":"markdown","8c97870f":"markdown","cfbb5afe":"markdown","65543e02":"markdown","3e08f407":"markdown"},"source":{"4aee9f7f":"# Standard Imports\nimport os\nimport numpy as np # Linear Algebra\nimport matplotlib.pyplot as plt # Visualization\nimport matplotlib.image as implt # Visualization\nimport seaborn as sns # Visualization\n\n\n# For Network\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import InceptionV3\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout\nfrom tensorflow.keras import Model\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.optimizers.schedules import PolynomialDecay\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\nimport datetime\nfrom sklearn.metrics import confusion_matrix","fcdba5f6":"BATCH_SIZE = 64\nEPOCHS = 20\n\nTRAIN_PATH = '\/kaggle\/input\/labeled-chest-xray-images\/chest_xray\/train'\nTEST_PATH = '\/kaggle\/input\/labeled-chest-xray-images\/chest_xray\/test'\n\nTRAIN_NORMAL = os.listdir(TRAIN_PATH + '\/NORMAL')\nTRAIN_PNEUMONIA = os.listdir(TRAIN_PATH + '\/PNEUMONIA')\n\nTEST_NORMAL = os.listdir(TEST_PATH + '\/NORMAL')\nTEST_PNEUMONIA = os.listdir(TEST_PATH + '\/PNEUMONIA')\n\n\nTARGET_SIZE = (300, 300)\nGREY = (3,)\nINPUT_SIZE = TARGET_SIZE + GREY\n\nCLASSES = ['Normal','Pneumonia']","0b891504":"num_train_normal = len(TRAIN_NORMAL)\nnum_train_pneumonia = len(TRAIN_PNEUMONIA)\nnum_train = num_train_normal + num_train_pneumonia \n\nnum_test_normal = len(TEST_NORMAL)\nnum_test_pneumonia = len(TEST_PNEUMONIA)\nnum_test = num_test_normal + num_test_pneumonia\n\nnum_all = num_train + num_test\n\nprint('Normal images in train-set: ', num_train_normal) \nprint('Pneumonia images in train-set: ', num_train_pneumonia)\nprint('Total images in train-set: ', num_train,'--->', round(num_train\/num_all,3)*100, 'Percent')\n\nprint(\"\\nNormal images in test-set:\", num_test_normal)\nprint(\"Pneumonia images in test-set:\", num_test_pneumonia)\nprint('Total images in test-set: ', num_test, '--->', round(num_test\/num_all,3)*100, 'Percent')\n\nprint('\\nAll the images in Dataset: ', num_all)","456591bb":"# Data-Augmentation Parameters to ImageDataGenerator\ntrain_datagen = ImageDataGenerator(validation_split=0.2,\n                                   rescale=1.\/255,\n                                   rotation_range=5, # Randomly rotate my image between 0 and 5 degrees.\n                                   width_shift_range=0.1, # Horizontal shift.\n                                   height_shift_range=0.1, # Vertical shift.\n                                   zoom_range=0.1, # Zoom.\n                                   horizontal_flip=True, # Randomly flip inputs horizontally.\n                                   samplewise_center=True, # Set each sample mean to 0. \n                                   samplewise_std_normalization=True) # Divide each input by its std.\n                                   \ntrain_generator = train_datagen.flow_from_directory(directory=TRAIN_PATH, batch_size=BATCH_SIZE, class_mode='categorical', target_size=TARGET_SIZE, subset='training')\n\nvalidation_generator = train_datagen.flow_from_directory(directory=TRAIN_PATH, batch_size=BATCH_SIZE, class_mode='categorical', target_size=TARGET_SIZE, subset='validation')\n\nimage_train, label_train = train_generator.next()\n\n\n\n\n# Validation\\Test should not be augmented.\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntest_generator = test_datagen.flow_from_directory(directory=TEST_PATH, batch_size = BATCH_SIZE, class_mode='categorical', target_size=TARGET_SIZE, shuffle=True)\n\nimage_test, label_test = test_generator.next()","493d2844":"def show_tarin(batch_size, images, labels):\n    \n    images = images[0:batch_size,:,:,:]\n    labels = labels[0:batch_size,:]\n    \n    image_rows = 2\n    image_col = int(batch_size\/image_rows)\n    \n    _, axs = plt.subplots(image_rows, image_col, figsize=(32,8))\n    axs = axs.flatten()\n    \n    for i in range(batch_size):\n        img = images[i,:,:,:]\n        lab = labels[i,:]\n        axs[i].imshow(img)\n        axs[i].axis('off')\n        lab = np.argmax(lab)\n        axs[i].set_title(label = CLASSES[lab], fontsize=14)\n\n    plt.show()\n    \n    \nshow_tarin(10, image_train, label_train)","a1a01a10":"# InceptionV3 with Transfer Learning\ndef craete_model():\n    \n    # Load v3 model without the top layer\n    inception_v3_model = InceptionV3(input_shape=INPUT_SIZE, weights='imagenet', include_top=False)\n\n    # Freeze\n    inception_v3_model.trainable = False\n    \n    # New classifier layers\n    flatten = Flatten()(inception_v3_model.layers[-1].output) # Flatt last v3 layer (except the last).\n    fc1 = Dense(units=512, activation='relu')(flatten)\n    dropout = Dropout(0.05)(fc1)\n    output = Dense(2, activation='softmax')(dropout)\n   \n    # Define a new Model\n    model = Model(inputs=inception_v3_model.input, outputs=output)\n    \n    # Summary\n    model.summary()\n    \n    return model\n\n\nmodel = craete_model()","5bd05c73":"opt = optimizers.Nadam(learning_rate=0.00001)\nloss = CategoricalCrossentropy()\nmet = 'accuracy'\n\n# Compile the Model\nmodel.compile(optimizer=opt, loss=loss, metrics=[met])","94f38f48":"# Clear any logs from previous runs\n!rm -rf .\/logs\/ \n!mkdir .\/logs\/\n\n# From Github Gist: https:\/\/gist.github.com\/hantoine\/4e7c5bc6748861968e61e60bab89e9b0\nfrom urllib.request import urlopen\nfrom io import BytesIO\nfrom zipfile import ZipFile\nfrom subprocess import Popen\nfrom os import chmod\nfrom os.path import isfile\nimport json\nimport time\nimport psutil\n\ndef launch_tensorboard():\n    tb_process, ngrok_process = None, None\n    \n    # Launch TensorBoard\n    if not is_process_running('tensorboard'):\n        tb_command = 'tensorboard --logdir .\/logs\/ --host 0.0.0.0 --port 6006'\n        tb_process = run_cmd_async_unsafe(tb_command)\n    \n    # Install ngrok\n    if not isfile('.\/ngrok'):\n        ngrok_url = 'https:\/\/bin.equinox.io\/c\/4VmDzA7iaHb\/ngrok-stable-linux-amd64.zip'\n        download_and_unzip(ngrok_url)\n        chmod('.\/ngrok', 0o755)\n\n    # Create ngrok tunnel and print its public URL\n    if not is_process_running('ngrok'):\n        ngrok_process = run_cmd_async_unsafe('.\/ngrok http 6006')\n        time.sleep(1) # Waiting for ngrok to start the tunnel\n    ngrok_api_res = urlopen('http:\/\/127.0.0.1:4040\/api\/tunnels', timeout=10)\n    ngrok_api_res = json.load(ngrok_api_res)\n    assert len(ngrok_api_res['tunnels']) > 0, 'ngrok tunnel not found'\n    tb_public_url = ngrok_api_res['tunnels'][0]['public_url']\n    print(f'TensorBoard URL: {tb_public_url}')\n\n    return tb_process, ngrok_process\n\n\ndef download_and_unzip(url, extract_to='.'):\n    http_response = urlopen(url)\n    zipfile = ZipFile(BytesIO(http_response.read()))\n    zipfile.extractall(path=extract_to)\n\n\ndef run_cmd_async_unsafe(cmd):\n    return Popen(cmd, shell=True)\n\n\ndef is_process_running(process_name):\n    running_process_names = (proc.name() for proc in psutil.process_iter())\n    return process_name in running_process_names\n\n\ntb_process, ngrok_process = launch_tensorboard()","b07b38d1":"# For TensorBoard\nlog_dir = \"logs\/fit\/\" + datetime.datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n\nmy_callbacks = [EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=5, mode='auto',restore_best_weights=False, verbose=1),\n\n                ModelCheckpoint(filepath='my_model.h5', monitor='val_accuracy', save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch', verbose=1),\n\n                TensorBoard(log_dir=log_dir, histogram_freq=1) ]","7319b330":"history = model.fit(train_generator, # If x is a dataset -> y should not be specified (since targets will be obtained from x).\n                    epochs=EPOCHS, batch_size=BATCH_SIZE,\n                    validation_data=validation_generator,\n                    callbacks=[my_callbacks],\n                    verbose=1)\n\nprint('\\n*** Fit is over ***')\nmodel.save('my_model.h5')","e4ae96bd":"def graphs_learning_display(history):\n    \n    # Train and Test Loss\n    plt.subplot(2,1,1)\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.legend(loc='upper left')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss - Cross Entropy')\n    plt.title('Train and Validation Loss')\n    \n    # Train and Test Accuracy\n    plt.subplot(2,1,2)\n    plt.plot(history.history['accuracy'], label='Train Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.legend(loc='upper left')\n    plt.xlabel('Epoch'),\n    plt.ylabel('Accuracy')\n    plt.title('Train and Validation Accuracy')\n    \n    plt.show()\n    \n\ngraphs_learning_display(history)","f6ebeda5":"test_loss, test_accuracy = model.evaluate(test_generator, verbose=0)\ntest_loss = round(test_loss, 6)\ntest_accuracy = round(test_accuracy*100, 3)\n\nprint('Test Loss: ', test_loss)\nprint('Test Accuracy:', '\\033[0m', test_accuracy, '%\\033[0m')","737ecfe8":"def show(model, batch_size, images, labels):\n    \n    images = images[0:batch_size,:,:,:]\n    labels = labels[0:batch_size,:]\n    predict = np.round(model.predict(images))\n    \n    image_rows = 2\n    image_col = int(batch_size\/image_rows)\n    \n    _, axs = plt.subplots(image_rows, image_col, figsize=(32,8))\n    axs = axs.flatten()\n    \n    for i in range(batch_size):\n        img = images[i,:,:,:]\n        lab = labels[i,:]\n        axs[i].imshow(img)\n        pred = predict[i]\n        axs[i].axis('off')\n        lab, pred = np.argmax(lab), np.argmax(pred)\n        axs[i].set_title(label = f'True Label: {CLASSES[lab]}  |  Predicted: {CLASSES[pred]}', fontsize=14)\n\n    plt.show()\n    \n    \nshow(model, 10, image_test, label_test)","fbb2d46a":"y_pred = model.predict(test_generator).argmax(axis=-1)\n\nconf_mat = confusion_matrix(test_generator.classes, y_pred)\n\n\ngroup_names = ['TN','FP','FN','TP']\ngroup_counts = ['{0:0.0f}'.format(value) for value in conf_mat.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in conf_mat.flatten()\/np.sum(conf_mat)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names, group_counts, group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\n\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(conf_mat, xticklabels=CLASSES, yticklabels=CLASSES, ax=ax, annot=labels, fmt='', cbar=True, annot_kws={'size': 20}, cmap='Blues')\nax.set_xlabel('Predicted Labels', fontsize=20)\nax.set_ylabel('True Labels', fontsize=20)\nplt.title('Confusion Matrix', fontsize=25)","9fb5ecb9":"train_acc = round(np.max(history.history['accuracy'])*100, 2)\nvalid_acc = round(np.max(history.history['val_accuracy'])*100,2)\n\nprint('Training Accuracy:','\\033[0m', train_acc, '%\\033[0m')\nprint('Validation Accuracy:','\\033[0m', valid_acc, '%\\033[0m')\nprint('Test Accuracy:', '\\033[0m', test_accuracy,'%\\033[0m')","60d052a3":"# 16. Final Results","93551d37":"# Computer Vision: Pneumonia Detection with CNN using Transfer Learning with InceptionV3 of Google\n\n**1. Imports**\n\n**2. Global Variables**\n\n**3. Numerical information**\n\n**4. Data Augmentation** \n\n**5. Visual information (training data photos)**\n\n**6. Model Building -> InceptionV3 with Transfer Learning**\n\n**7. Define Optimizer, Loss & Metrics + Compile the Model**\n\n**8. TensorBoard In Kaggle**\n\n**9. Callbacks**\n\n**10. Training & Save The Model**\n\n**11. Plotting- Graphs**\n\n**12. TensorBoard - SCALARS, GRAPHS, DISTRIBUTIONS, HISTOGRAMS, TIME SERIES**\n\n**13. Model Evaluate**\n\n**14. Plotting- True Label & Predict of a particular Batch**\n\n**15. Confusion Matrix**\n\n**16. Final Results**","57fffaab":"# 14. True Label & Predict of a particular Batch","bd029353":"# TensorBoard - TIME SERIES\n![\u05ea\u05de\u05d5\u05e0\u05d4.png](attachment:859a4b12-7a41-41dc-8289-478bd96a9650.png)","c611ffa5":"# 5. Pull out some images to get visual information (After Data-Augmentation)","ffbb64aa":"# 8. TensorBoard in Kaggle\n**TensorBoard is a visualization tool that is available with tensorflow.\nBut the reason this is useful is that,\nit has special features such as viewing machine\\deep learning model as a conceptual graphical representation (computational graph) of nodes and edges connecting those nodes(data flows).\nFurther it also provides us the ability to evaluate the performance of our models in relation to desired metrics.**\n","a98bf2b9":"# TensorBoard - DISTRIBUTIONS ---> Display the distribution of the weight\n![\u05ea\u05de\u05d5\u05e0\u05d4.png](attachment:c2c60aa2-cca4-4a2d-8b9e-c0d986cb264f.png)","9646f9a7":"# 13. Model Evaluate\n**Evaluation is a process during development of the model to check whether the model is best fit for the given problem and corresponding data.**\n\n**Let us evaluate the model we have created.**","32269373":"# Context\n**Pneumonia is an infection that inflames the air sacs in one or both lungs.\nIt kills more children younger than 5 years old each year than any other infectious disease, such as HIV infection, malaria, or tuberculosis.\nDiagnosis is often based on symptoms and physical examination. Chest X-rays may help confirm the diagnosis.**\n\n# Content\n**This dataset contains 5,856 validated Chest X-Ray images.\nThe images are split into a training set and a testing set of independent patients.**","2f491e2f":"# 9. Callbacks - A set of functions to be applied at given stages of the training procedure\n\n**EarlyStoppingfunction - Stop training when a monitored metric has stopped improving.**\n\n**ModelCheckpoint - Automatic saving of the model- Keeps the network in memory so we can use it later.**\n\n**TensorBoard - Visualizations.**","8aa361e2":"# InceptionV3\n![image.png](attachment:image.png)","ddda946c":"# 1. First, let's import important libraries\n**In this project, I will use Keras- a high level API for neural networks in python.**","ea45dbaa":"# 4. Data Augmentation\n**Data augmentation can be used to address both the requirements,\nthe diversity of the training data, and the amount of data.\nBesides these two, augmented data can also be used to address the class imbalance problem in classification tasks.**\n\n**Assured since several case studies indicate that the performance improves and cautious since different augmentation techniques used affect the scale of improvement differently.**","68083587":"# 12.\n![image.png](attachment:image.png)","8bbfbb62":"# 15. Confusion Matrix - A summary of prediction results on a classification problem.\n\n**The number of correct and incorrect predictions are summarized with count values and broken down by each class.**\n**It gives us insight not only into the errors being made by our classifier but more importantly the types of errors that are being made.\n**It is this breakdown that overcomes the limitation of using classification accuracy alone.**","4aabb0ed":"# 10. Training & Save The Model\n**model.save('my_model.h5') ---> HDF5 is a high performance file format used for fast I\/O processing and storage of big data,\nmaking it suitable for machine & deep learning models.\nSaving a Keras model to an HDF5 preserves the architecture, optimizer state, and weights of the model,\nwhile loading a Keras model from an HDF5 file creates a new Keras model with the architecture, optimizer state, and weights stored in the model.**","0b117028":"# TensorBoard - HISTOGRAMS ---> Display weights with a histogram\n![\u05ea\u05de\u05d5\u05e0\u05d4.png](attachment:4f2ab244-2b10-4893-bf84-080fff93bd0f.png)","44c9898e":"# 3. Before starting, let's pull out some numerical information about the data","59a69399":"# 2. Import our data and define important variables","617f7c06":"# 6. Model Building \n\n**Using the Inception-v3 model in this project.\nTransfer Learning has become immensely popular because it considerably reduces training time, and requires a lot less data to train on to increase performance.\ngoing to use all the layers in the model except for the last fully connected layer as it is specific to the ImageNet competition.**","1830419b":"# TensorBoard - GRAPHS --->  Visualize the model\n![\u05ea\u05de\u05d5\u05e0\u05d4.png](attachment:fd54121c-0ff1-4b0c-a718-151f0fc016ba.png)","8c97870f":"# 11. Plotting Graphs\n**Visualizing data is one of the best ways to humanize data to make it easy to understand and get the relevant trends from it.\nThis activity can be crucial when the user is still trying to optimize the model and make it production ready.**\n\n**Train and Validation Loss - Plots the graph of the training loss vs. validation loss over the number of epochs.\nThis will help to make informed decisions about the architectural choices that need to be made.**\n\n**Train and Validation Accuracy - Plots the graph of training accuracy vs. validation accuracy over the number of epochs.**\n","cfbb5afe":"# As we can see, Transfer Learning has great applications in Computer Vision","65543e02":"# TensorBoard - SCALARS ---> Show different useful information during the model training\n![\u05ea\u05de\u05d5\u05e0\u05d4.png](attachment:9210f94c-e226-4da9-84b1-6ca124cc7f89.png)","3e08f407":"# 7. Define Optimizer, Loss & Metrics + Compile the Model\n**Optimizer is Nadam with a learning rate of 0.00001.**\n\n**CategoricalCrossentropy as the loss metric as we have 2 target classes (Normal or Pneumonia)**\n\n"}}