{"cell_type":{"ecadfe8c":"code","fea0f184":"code","e1ae21ea":"code","45d828fe":"code","6df7819e":"code","301869f7":"code","0985f7ad":"code","1019e49e":"code","c4dc2c61":"code","e81fc672":"code","83587a46":"code","34fb008c":"code","0be5243f":"code","ba77deb3":"code","43f1c711":"markdown","4b9ceea8":"markdown","4d73a143":"markdown","95b8af55":"markdown","de3f2da0":"markdown","60905ecb":"markdown","bd793b0f":"markdown","3c15bb0f":"markdown"},"source":{"ecadfe8c":"# Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","fea0f184":"# The dataset contains the information of 7042 Customers and their churn value.\ndata = pd.read_csv('..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndata = data.drop(['customerID'], axis=1)\ndata","e1ae21ea":"# list(data)\ncategorical = [#'customerID',\n 'gender',\n 'SeniorCitizen',\n 'Partner',\n 'Dependents',\n #'tenure',\n 'PhoneService',\n 'MultipleLines',\n 'InternetService',\n 'OnlineSecurity',\n 'OnlineBackup',\n 'DeviceProtection',\n 'TechSupport',\n 'StreamingTV',\n 'StreamingMovies',\n 'Contract',\n 'PaperlessBilling',\n 'PaymentMethod',\n #'MonthlyCharges',\n #'TotalCharges',\n #'Churn'\n]","45d828fe":"# Churn summary\nprint(data.groupby('Churn').Churn.count())","6df7819e":"data.SeniorCitizen = ['Yes' if sc == 1 else 'No' for sc in data.SeniorCitizen]\ndata.Churn = [1 if c == 'Yes' else 0 for c in data.Churn]","301869f7":"def perc(x):\n    return str(round(100*x,1))+'%'\nfig=plt.figure(figsize=(16,15))\nfig.suptitle('Churn rate by category',fontsize='x-large')\nfor i in range(0,len(categorical)):\n    category = categorical[i]\n    ax = fig.add_subplot(4,4,i+1)\n    group_churn = data.groupby(category).Churn.mean()\n    k = group_churn.keys()\n    v = group_churn.values\n    v2 = v\/np.sum(v)\n    left = np.cumsum(v2)\n    plt.barh([1],v2[0],height=0.2,label=k[0]+': '+perc(v[0]))\n    for j in range(1,len(v)):\n        plt.barh([1],v2[j],left=left[j-1],height=0.2,label=k[j]+': '+perc(v[j]))\n    plt.ylim([0.4,1.6])\n    plt.xlim(-0.1,1.1)\n    plt.axis('off')\n    plt.legend()\n    plt.title(category)\nfig=plt.figure(figsize=(16,15))\nfig.suptitle('Disparities in Churn rates for each category',fontsize='x-large')\nplt.show()","0985f7ad":"# Preprocessing and Dummy variables\n\ndata.TotalCharges = [0 if tc==' ' else float(tc) for tc in data.TotalCharges]\ndata = pd.get_dummies(data,columns=categorical, drop_first=True)\n\n# Dependent and Independent variables\nX = data.values\ny = data.Churn.values\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nX = scale.fit_transform(X)","1019e49e":"from sklearn.decomposition import KernelPCA\nkpca=KernelPCA(n_components = 2,kernel = 'rbf')\nXpca = kpca.fit_transform(X)\n\nfrom sklearn.cluster import KMeans\nclusters = 2\nkmeans = KMeans(n_clusters = clusters,init = 'k-means++',max_iter=300,n_init=10,random_state=0)\ny_kmeans = kmeans.fit_predict(X)\ndata['Clusters'] = y_kmeans\n\nmean_values = data.groupby('Clusters')['Churn'].mean().values\nrate = [str(round(100*mean_values.min(),1))+'%',str(round(100*mean_values.max(),1))+'%']\ncolors = ['skyblue','tomato']\nif mean_values[0]>mean_values[1]:\n    colors = colors[::-1]\n    rate = rate[::1]\n    \nplt.rcParams['axes.facecolor'] = 'whitesmoke'\nplt.figure(figsize=(12,6))\n\nplt.subplot(1,2,1)\nplt.scatter(Xpca[data.Clusters==0][:,0],Xpca[data.Clusters==0][:,1],marker='.',color=colors[0],label='Churn rate = '+rate[0])\nplt.scatter(Xpca[data.Clusters==1][:,0],Xpca[data.Clusters==1][:,1],marker='.',color=colors[1],label='Churn rate = '+rate[1])\nplt.legend()\nplt.title('Churn by Cluster')\nplt.xlabel('PCA component 1')\nplt.ylabel('PCA component 2')\nplt.axis('equal')\n\nplt.subplot(1,2,2)\nplt.title('Cluster Size')\nplt.pie(data.groupby('Clusters')['Clusters'].count().values,colors = colors,autopct='%1.1f%%')\nplt.axis('equal')\nplt.show()\n\nplt.show()","c4dc2c61":"clusters = 5\nkmeans = KMeans(n_clusters = clusters,init = 'k-means++',max_iter=300,n_init=10,random_state=0)\ny_kmeans = kmeans.fit_predict(X)\ndata['Clusters'] = y_kmeans\n\nX = scale.inverse_transform(X)","e81fc672":"plt.rcParams['axes.facecolor'] = 'silver'\nplt.figure(figsize=(12,6))\nplt.subplot(1,2,1)\n\ncolors = ['beige','palegoldenrod','orange','orangered','firebrick']\n\nmean_values = data.groupby('Clusters')['Churn'].mean().values\nrisk_sort = np.argsort(data.groupby('Clusters')['Churn'].mean().values)\n\nplt.bar(x=np.arange(0,clusters),height = (mean_values)[risk_sort],color= colors)\nplt.plot(np.arange(-1,clusters+1),np.full(clusters+2,data.Churn.mean()),'k--',label='Average Churn')\nplt.xlim([-.75,clusters-0.25])\nplt.tick_params(axis='x',which='both',bottom=False,top=False,labelbottom=False)\nplt.gca().set_yticklabels(['{:.0f}%'.format(x*100) for x in plt.gca().get_yticks()])\nplt.xlabel('Customer Clusters')\nplt.ylabel('Churn rate')\nplt.title('Churn by Cluster')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.title('Cluster Size')\nplt.pie(data.groupby('Clusters')['Clusters'].count().values[risk_sort],autopct='%1.1f%%',colors = colors)\nplt.axis('equal')\nplt.show()","83587a46":"print('Risk Cluster size:\\t\\t',str(round(100*data.groupby('Clusters')['Churn'].count().values.max()\/len(data),1))+'%')\nprint('Risk Cluster churn rate:\\t',str(round(100*mean_values.max(),1))+'%')","34fb008c":"# Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX = data.drop(columns=['Churn','Clusters']).values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n\n# RF Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier(n_estimators=200)\nrf_model.fit(X_train,y_train)\n\n# Prediciton\ny_pred = rf_model.predict(X_test)\n\n# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,y_pred)\n\nprint('Model:\\t\\t\\tRandom Forest Classification')\nprint('Test Set Accuracy:\\t' ,str(round(100*np.trace(cm)\/np.sum(cm),1))+'%')","0be5243f":"# XGBoost Classifier\nfrom xgboost import XGBClassifier\nxgb_model = XGBClassifier()\nxgb_model.fit(X_train,y_train)\n\n# Prediciton\ny_pred = xgb_model.predict(X_test)\n\n# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,y_pred)\n\nprint('Model:\\t\\t\\tXGBoost Classification')\nprint('Test Set Accuracy:\\t' ,str(round(100*np.trace(cm)\/np.sum(cm),1))+'%')","ba77deb3":"# Scaling\nX_train = scale.fit_transform(X_train)\nX_test = scale.transform(X_test)\n\n# Creating the ANN\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nann_model = Sequential()\nann_model.add(Dense(activation=\"relu\", input_dim=30, units=15, kernel_initializer=\"uniform\"))\nann_model.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\nann_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\nann_model.fit(X_train,y_train,batch_size=10,epochs=5)\n\n# Prediction\ny_pred = ann_model.predict(X_test) > 0.5\n\n# Confusion Matrix\ncm = confusion_matrix(y_test,y_pred)\n\nprint('\\nModel:\\t\\t\\tANN Classification')\nprint('Test Set Accuracy:\\t' ,str(round(100*np.trace(cm)\/np.sum(cm),1))+'%')","43f1c711":"__Preditive Models__\n\n__Random Forest Classifier__","4b9ceea8":"__KNN with 2 Clusters__\n\nThe relation between the information for each customer can be visualised more easily after applying PCA. Doing so shows clearly separable clusters, with markedly different churn rates. ","4d73a143":"__XGBoost Classifier__","95b8af55":"__Customer Churn using Telco Dataset__","de3f2da0":"__KNN with 5 Clusters__\n\nA better result can be achieved by breaking the customers into 5 groups, one of which has a significantly higher churn risk.","60905ecb":"The dataset contains the information of 7042 Customers and their churn value.","bd793b0f":"A first look into the disparity in churn rates for different categories in the data.","3c15bb0f":"__Artifical Neural Network Classifier__"}}