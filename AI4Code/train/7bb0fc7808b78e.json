{"cell_type":{"a03f443b":"code","683a929f":"code","7a1d2a64":"code","e68054c9":"code","dfb16047":"code","127e97ec":"code","14abbda2":"code","bbd7a79b":"code","1ec5100e":"code","39dd7496":"code","b612e974":"code","b746717b":"code","0d79196a":"code","de4925f9":"code","24b70c66":"code","ae299fde":"code","0c847146":"code","f863acc6":"code","995dd62e":"code","c92d487b":"code","175704e4":"code","d3cde17d":"code","40543fe1":"code","2d15112b":"code","c0ba321a":"code","f1e5eee1":"code","d8cd39eb":"code","6a5d12cd":"code","3cf3bcf7":"code","854d579f":"code","8366f183":"code","712273f3":"code","df909752":"code","965b0640":"code","a1e41a12":"code","98f77798":"code","9538ffd0":"code","394340fb":"markdown","b3cb8355":"markdown","1f3994ed":"markdown","7e9f3304":"markdown","11fbdb2e":"markdown","9a83afaf":"markdown","f26359a6":"markdown","e08bb9ce":"markdown","97f16596":"markdown","8a0d088e":"markdown","037d08af":"markdown","2e4b49e9":"markdown"},"source":{"a03f443b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","683a929f":"#### importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split","7a1d2a64":"# Loading train dataset\n\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")","e68054c9":"# Loading Test Dataset\n\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")","dfb16047":"train.head()","127e97ec":"train.shape","14abbda2":"# Checking Null Values\n\ntrain.isnull().sum().sum()","bbd7a79b":"##### Now we will trying to see the image from that 784 pixel\ndef show_image(pixel, label, index):\n    image2d = pixel.values.reshape(28,28)\n    plt.subplot('33%d' % (index))\n    plt.imshow(image2d, cmap=plt.cm.gray)\n    plt.title(label)\n\nplt.figure(figsize=(5,5))\nsample_image = train.sample(9).reset_index(drop=True)\n\nfor index, image in sample_image.iterrows():\n    label = image['label']\n    pixel = image.drop('label')\n    show_image(pixel, label, index)\n    \nplt.tight_layout()","1ec5100e":"# Creating training and test sets\n# Splitting the data into train and test\nX = train.iloc[:, 1:]\nY = train.iloc[:, 0]\n\n# Rescaling the features\nfrom sklearn.preprocessing import scale\nX = scale(X)","39dd7496":"# train test split with train_size=10% and test size=90%\nx_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.10, random_state=101)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","b612e974":"from sklearn import svm\nfrom sklearn import metrics\n\n# an initial SVM model with linear kernel   \nsvm_linear = svm.SVC(kernel='linear')\n\n# fit\nsvm_linear.fit(x_train, y_train)","b746717b":"# predict\npredictions = svm_linear.predict(x_test)\npredictions[:10]","0d79196a":"# evaluation: accuracy\n\naccuracy = metrics.accuracy_score(y_true = y_test, y_pred = predictions)\naccuracy","de4925f9":"# C(i, j) represents the number of points known to be in class i \n# but predicted to be in class j\nconfusion = metrics.confusion_matrix(y_true = y_test, y_pred = predictions)\nconfusion","24b70c66":"# run gc.collect() (garbage collect) to free up memory\n# else, since the dataset is large and SVM is computationally heavy,\n# it'll throw a memory error while training\n\nimport gc\ngc.collect()","ae299fde":"# rbf kernel with other hyperparameters kept to default \nsvm_rbf = svm.SVC(kernel='rbf')\nsvm_rbf.fit(x_train, y_train)","0c847146":"# predict\npredictions = svm_rbf.predict(x_test)\n\n# accuracy \nprint(metrics.accuracy_score(y_true=y_test, y_pred=predictions))","f863acc6":"# conduct (grid search) cross-validation to find the optimal values \n# of cost C and the choice of kernel\n\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = {'C':[1, 10, 100], 'gamma': [1e-2, 1e-3, 1e-4]}\n\n# instantiate a model \nsvc_grid_search = svm.SVC(kernel=\"rbf\")\n\n# create a classifier to perform grid search\nclf = GridSearchCV(svc_grid_search, param_grid=parameters, scoring='accuracy')\n\n# fit\nclf.fit(x_train, y_train)","995dd62e":"# results\ncv_results = pd.DataFrame(clf.cv_results_)\ncv_results","c92d487b":"# converting C to numeric type for plotting on x-axis\ncv_results['param_C'] = cv_results['param_C'].astype('int')\n\n# # plotting\nplt.figure(figsize=(16,6))\n\n# subplot 1\/3\nplt.subplot(131)\ngamma_01 = cv_results[cv_results['param_gamma']==0.01]\n\nplt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_test_score\"])\n#plt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.01\")\nplt.ylim([0.60, 1])\nplt.legend(['test accuracy', 'train accuracy'], loc='lower right')\nplt.xscale('log')\n\n# subplot 2\/3\nplt.subplot(132)\ngamma_001 = cv_results[cv_results['param_gamma']==0.001]\n\nplt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_test_score\"])\n#plt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.001\")\nplt.ylim([0.60, 1])\nplt.legend(['test accuracy', 'train accuracy'], loc='lower right')\nplt.xscale('log')\n\n\n# subplot 3\/3\nplt.subplot(133)\ngamma_0001 = cv_results[cv_results['param_gamma']==0.0001]\n\nplt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_test_score\"])\n#plt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.0001\")\nplt.ylim([0.60, 1])\nplt.legend(['test accuracy', 'train accuracy'], loc='lower right')\nplt.xscale('log')\n\nplt.show()","175704e4":"# optimal hyperparameters\nbest_C = 1\nbest_gamma = 0.001\n\n# model\nsvm_final = svm.SVC(kernel='rbf', C=best_C, gamma=best_gamma)\n\n# fit\nsvm_final.fit(x_train, y_train)","d3cde17d":"# predict\npredictions = svm_final.predict(x_test)","40543fe1":"confusion = metrics.confusion_matrix(y_true = y_test, y_pred = predictions)\n\n# measure accuracy\ntest_accuracy = metrics.accuracy_score(y_true=y_test, y_pred=predictions)\n\nprint(test_accuracy, \"\\n\")\nprint(confusion)","2d15112b":"#fit a Random Forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrandom=RandomForestClassifier()\nrandom.fit(x_train,y_train)","c0ba321a":"#predict value of label using Random Forest classifier\nrf_prediction = random.predict(x_test)","f1e5eee1":"metrics.accuracy_score(y_test,rf_prediction)","d8cd39eb":"from sklearn.svm import LinearSVC\nfrom sklearn.multiclass import OneVsRestClassifier","6a5d12cd":"#SVC Model\nmodel = LinearSVC()","3cf3bcf7":"# Make it an OvR classifier\novr_classifier = OneVsRestClassifier(model)","854d579f":"# Fit the data to the OvR classifier\novr_classifier = ovr_classifier.fit(x_train, y_train)","8366f183":"#predict value of label using 1vsRest classifier\novr_prediction = ovr_classifier.predict(x_test)","712273f3":"metrics.accuracy_score(y_test,ovr_prediction)","df909752":"from sklearn.ensemble import VotingClassifier","965b0640":"hard_voting_ensemble = VotingClassifier(estimators=[\n    (\"random_forest\", random),\n    (\"OnevsRest\", ovr_classifier),\n    (\"Kernel_SVM\", svm_rbf)\n], voting=\"hard\")","a1e41a12":"hard_voting_ensemble.fit(x_train, y_train)","98f77798":"hard_voting__prediction = hard_voting_ensemble.predict(x_test)","9538ffd0":"metrics.accuracy_score(y_test, hard_voting__prediction)","394340fb":"# 1 vs Rest Classifier","b3cb8355":"**Linear SVM**","1f3994ed":"**Grid Search Cross-Validation**","7e9f3304":"**Final Model**\n\nLet's now build the final model with chosen hyperparameters.","11fbdb2e":"The final accuracy on test data is approx. 92%. Note that this can be significantly increased by using the entire training data of 42,000 images (we have used just 10% of that!).","9a83afaf":"## Conclusion\nThe best accuracy we get on test data is approx. 93.0% using Random Forest Classifier. Note that this can be significantly increased by using the entire training data of 42,000 images (we have used just 10% of that!).","f26359a6":"The final accuracy on test data is approx. 93% using Random Forest Classifier. Note that this can be significantly increased by using the entire training data of 42,000 images (we have used just 10% of that!).","e08bb9ce":"From the plot above, we can observe that (from higher to lower gamma \/ left to right):\n\nAt very high gamma (0.01), the model is achieving 100% accuracy on the training data, though the test score is quite low (<75%). Thus, the model is overfitting.\n\nAt gamma=0.001, the training and test scores are comparable at around C=1, though the model starts to overfit at higher values of C\n\nAt gamma=0.0001, the model does not overfit till C=10 but starts showing signs at C=100. Also, the training and test scores are slightly lower than at gamma=0.001.\n\nThus, it seems that the best combination is gamma=0.001 and C=1 (the plot in the middle), which gives the highest test accuracy (~92%) while avoiding overfitting.\n\nLet's now build the final model and see the performance on test data.","97f16596":"# Random Forest","8a0d088e":"# Voting Classifier","037d08af":"**Non-Linear SVM**\n\nLet's now try a non-linear model with the RBF kernel.","2e4b49e9":"The accuracy achieved with a non-linear kernel is slightly higher than a linear one. Let's now do a grid search CV to tune the hyperparameters C and gamma."}}