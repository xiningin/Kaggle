{"cell_type":{"f6bc8faf":"code","0d36ec13":"code","4b48baf1":"code","0476e4e2":"code","2046b08c":"code","f4b5c1c9":"code","f34acc54":"code","10fdea52":"code","97e7ba67":"code","696def8d":"code","3aa10f25":"code","8ccc17de":"code","d479987c":"code","19673294":"markdown","04c1460a":"markdown","1ae5763c":"markdown","fcfa05fb":"markdown","2b57cd81":"markdown","ced02732":"markdown","251d17a2":"markdown","cca893e8":"markdown","b5c815fc":"markdown","02ed0990":"markdown","a6bb9be1":"markdown"},"source":{"f6bc8faf":"from __future__ import print_function, division\nfrom builtins import range\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, GlobalMaxPooling1D\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.models import Model\nfrom sklearn.metrics import roc_auc_score","0d36ec13":"# Download the data:\n# Download the word vectors:\n# http:\/\/nlp.stanford.edu\/data\/glove.6B.zip","4b48baf1":"MAX_SEQUENCE_LENGTH = 100\nMAX_VOCAB_SIZE = 20000\nEMBEDDING_DIM = 100\nVALIDATION_SPLIT = 0.2\nBATCH_SIZE = 128\nEPOCHS = 10","0476e4e2":"print('Loading word vectors...')\nword2vec = {}\nwith open(os.path.join('..\/input\/glove6b\/glove.6B.%sd.txt' % EMBEDDING_DIM)) as f:\n  # is just a space-separated text file in the format:\n  # word vec[0] vec[1] vec[2] ...\n  for line in f:\n    values = line.split()\n    word = values[0]\n    vec = np.asarray(values[1:], dtype='float32')\n    word2vec[word] = vec\nprint('Found %s word vectors.' % len(word2vec))","2046b08c":"# prepare text samples and their labels\nprint('Loading in comments...')\n\ntrain = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\nsentences = train[\"comment_text\"].fillna(\"DUMMY_VALUE\").values\npossible_labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ntargets = train[possible_labels].values","f4b5c1c9":"tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\ntokenizer.fit_on_texts(sentences)\nsequences = tokenizer.texts_to_sequences(sentences)\n# print(\"sequences:\", sequences); exit()\n\n\nprint(\"max sequence length:\", max(len(s) for s in sequences))\nprint(\"min sequence length:\", min(len(s) for s in sequences))\ns = sorted(len(s) for s in sequences)\nprint(\"median sequence length:\", s[len(s) \/\/ 2])\n\nprint(\"max word index:\", max(max(seq) for seq in sequences if len(seq) > 0))","f34acc54":"# get word -> integer mapping\nword2idx = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word2idx))\n\n# exit()\n\n\n# pad sequences so that we get a N x T matrix\ndata = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', data.shape)","10fdea52":"# prepare embedding matrix\nprint('Filling pre-trained embeddings...')\nnum_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word, i in word2idx.items():\n  if i < MAX_VOCAB_SIZE:\n    embedding_vector = word2vec.get(word)\n    if embedding_vector is not None:\n      # words not found in embedding index will be all zeros.\n      embedding_matrix[i] = embedding_vector","97e7ba67":"# note that we set trainable = False so as to keep the embeddings fixed\nembedding_layer = Embedding(\n  num_words,\n  EMBEDDING_DIM,\n  weights=[embedding_matrix],\n  input_length=MAX_SEQUENCE_LENGTH,\n  trainable=False\n)","696def8d":"print('Building model...')\n\ninput_ = Input(shape=(MAX_SEQUENCE_LENGTH,))\nx = embedding_layer(input_)\nx = Conv1D(128, 3, activation='relu')(x)\nx = MaxPooling1D(3)(x)\nx = Conv1D(128, 3, activation='relu')(x)\nx = MaxPooling1D(3)(x)\nx = Conv1D(128, 3, activation='relu')(x)\nx = GlobalMaxPooling1D()(x)\nx = Dense(128, activation='relu')(x)\noutput = Dense(len(possible_labels), activation='sigmoid')(x)\n\nmodel = Model(input_, output)\nmodel.compile(\n  loss='binary_crossentropy',\n  optimizer='rmsprop',\n  metrics=['accuracy']\n)\n\nprint('Training model...')\nr = model.fit(\n  data,\n  targets,\n  batch_size=BATCH_SIZE,\n  epochs=EPOCHS,\n  validation_split=VALIDATION_SPLIT\n)","3aa10f25":"plt.plot(r.history['loss'], label='loss')\nplt.plot(r.history['val_loss'], label='val_loss')\nplt.legend()\nplt.show()","8ccc17de":"plt.plot(r.history['accuracy'], label='acc')\nplt.plot(r.history['val_accuracy'], label='val_acc')\nplt.legend()\nplt.show()","d479987c":"p = model.predict(data)\naucs = []\nfor j in range(6):\n    auc = roc_auc_score(targets[:,j], p[:,j])\n    aucs.append(auc)\nprint(np.mean(aucs))","19673294":"# load in pre-trained word vectors","04c1460a":"# plot some data","1ae5763c":"# train a 1D convnet with global maxpooling","fcfa05fb":"# convert the sentences (strings) into integers","2b57cd81":"# Import library","ced02732":"# plot the mean AUC over each label","251d17a2":"# Introductoin","cca893e8":"\nThe Toxic Comment Classification Challenge Dataset are provided with a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are:\n\ntoxic\nsevere_toxic\nobscene\nthreat\ninsult\nidentity_hate\n\nOne important fact about this data is that it is not your typical single label classification data.Each comment can have zero or more labels, so a comment can be both toxic and a threat.\n\nFor example, in this situation, we can't use Softmax as our final activation function because Softmax only chooses the most probable class. It can't choose multiple classes.\n\nIn this scenario, we can think of each label as an individual label, either a comment is toxic or non-toxic, either it contains a threat or it does not contain a threat.\nSo the solution here is to treat this like six separate binary classification problems.\n\nOf course, that doesn't mean we need to create six separate binary classification.\nNeural networks instead will create one neural network with six separate binary logistic regressions at the end. At each output node, we apply a sigmoid and the total loss function is the average binary cross entropy\n","b5c815fc":"# load pre-trained word embeddings into an Embedding layer","02ed0990":"# some configuration","a6bb9be1":"# accuracies"}}