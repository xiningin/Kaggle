{"cell_type":{"491af75d":"code","0248b6fd":"code","3eeffed7":"code","710abf96":"code","51c20c70":"code","6e0d8cbc":"code","f9082f12":"code","43ddb426":"code","a4f77329":"code","1d386e5b":"code","11359685":"code","688a74c0":"code","e68b8be5":"code","ad0b3791":"code","dd6a381a":"code","e4a636b5":"code","af2cb785":"code","778b34f4":"code","51bb4f86":"code","7f3e1e97":"code","7ef08ae5":"code","35438dd5":"code","49e3d061":"code","c3cfa5e7":"code","e88e66d5":"code","ba1a0330":"code","16b6abbd":"code","47833bef":"code","32692e0f":"code","86fe37b3":"code","54cb8877":"code","53039135":"code","428f06ea":"code","1fa5121e":"code","7a1ef279":"code","acc9f74d":"code","c1396dbf":"code","f27d14fe":"code","aa58083e":"code","6920de56":"code","9ebb1a85":"code","93840253":"code","f6a6790c":"code","fc40c5be":"code","f4f6f86f":"code","72ae2fe2":"code","6ba34bb2":"code","2bf99c5c":"code","f24df525":"code","69282af6":"code","767e28c1":"code","c7fb4288":"code","0eb9d1b4":"code","96dcb84d":"code","05bc1e17":"code","3a8100c9":"code","c515ee04":"code","d49c7c6e":"code","f3cf3da7":"code","52de7ed7":"code","9b7f68d1":"code","fd92ba8a":"code","a2e8d83e":"code","508de9c9":"code","ab4969f4":"code","b35e0e63":"code","41c5ed74":"code","d06d8eae":"code","8294c19a":"code","05ef1644":"markdown","a4d8d890":"markdown","c3311ddf":"markdown","728bb0ec":"markdown","dd096a50":"markdown","22b85b6c":"markdown","22f51961":"markdown","73184a50":"markdown","c04d8d44":"markdown","35a5ba04":"markdown","9849566c":"markdown","c1c995a5":"markdown","09209b3d":"markdown","a65263a9":"markdown","d2f3886c":"markdown","735634f3":"markdown","68191db1":"markdown"},"source":{"491af75d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0248b6fd":"\nimport matplotlib as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV","3eeffed7":"path='..\/input\/'","710abf96":"train=pd.read_csv(path+'train.csv')\ntest=pd.read_csv(path+'test.csv')","51c20c70":"# save the passenger id for the final submission\npassengerId=test.PassengerId\n\n# merge train and test\ntitanic = train.append(test, ignore_index=True)\n\n## we use the ignore_index as in test data we have the labels columns which is not present in the train data.","6e0d8cbc":"train_id=len(train)\ntest_id=len(titanic)-len(test)","f9082f12":"train_id","43ddb426":"test_id","a4f77329":"len(titanic)","1d386e5b":"len(test)","11359685":"titanic.head()","688a74c0":"titanic.info()","e68b8be5":"titanic.drop(['PassengerId'],1,inplace=True)","ad0b3791":"titanic.head()","dd6a381a":"titanic['Title']=titanic.Name.apply(lambda name:name.split(',')[1].split('.')[0].strip() )","e4a636b5":"titanic.head()","af2cb785":"## title counts\n#print(\"There are {} unique title.\".format(titanic.Title.nunique))\nprint(\"There are {} unique titles.\".format(titanic.Title.nunique()))\nprint(\"\\n\", titanic.Title.unique())","778b34f4":"titanic.head()","51bb4f86":"# normalize the titles\nnormalized_titles = {\n    \"Capt\":       \"Officer\",\n    \"Col\":        \"Officer\",\n    \"Major\":      \"Officer\",\n    \"Jonkheer\":   \"Royalty\",\n    \"Don\":        \"Royalty\",\n    \"Sir\" :       \"Royalty\",\n    \"Dr\":         \"Officer\",\n    \"Rev\":        \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Dona\":       \"Royalty\",\n    \"Mme\":        \"Mrs\",\n    \"Mlle\":       \"Miss\",\n    \"Ms\":         \"Mrs\",\n    \"Mr\" :        \"Mr\",\n    \"Mrs\" :       \"Mrs\",\n    \"Miss\" :      \"Miss\",\n    \"Master\" :    \"Master\",\n    \"Lady\" :      \"Royalty\"\n}\n\ndef convert(val):\n    return normalized_titles[val]","7f3e1e97":"titanic.head()","7ef08ae5":"type(titanic.Title.values[0])","35438dd5":"# view value counts for the normalized titles\nprint(titanic.Title.value_counts())\n","49e3d061":"titanic.Title = titanic.Title.map(normalized_titles)\n","c3cfa5e7":"titanic.head()","e88e66d5":"# view value counts for the normalized titles\nprint(titanic.Title.value_counts())","ba1a0330":"#groupby sex,Pclass and Title\ngrouped=titanic.groupby(['Sex','Pclass','Title'])\ngrouped.Age.median()","16b6abbd":"## applying the grouped median age value\ntitanic.Age=grouped.Age.apply(lambda x:x.fillna(x.median()))\n\ntitanic.info()","47833bef":"titanic.head(10)","32692e0f":"titanic.Cabin=titanic.Cabin.fillna('NA')     ## NA-not available","86fe37b3":"titanic.head()","54cb8877":"titanic.Embarked.value_counts()","53039135":"most_embarked=titanic.Embarked.value_counts().index[0]","428f06ea":"most_embarked","1fa5121e":"titanic.Embarked=titanic.Embarked.fillna(most_embarked)","7a1ef279":"titanic.head()","acc9f74d":"titanic.info()","c1396dbf":"##only fare is left incomplete\ntitanic.Fare=titanic.Fare.fillna(titanic.Fare.median())\n\ntitanic.info()","f27d14fe":"##percentage of death vs percentage of survival\ntitanic.Survived.value_counts()","aa58083e":"titanic.Survived.value_counts(normalize=True)","6920de56":"## lets dig deeper and determine the survival rates based on the gender\ngroupbysex=titanic.groupby(['Sex'])\ngroupbysex.Survived.value_counts(normalize=True)","9ebb1a85":"##survival rates based on their sex\ngroupbysex.Survived.mean()","93840253":"## group by passenge Pclass and sex\ngroup_class_sex=titanic.groupby(['Pclass','Sex'])\ngroup_class_sex.Survived.mean()","f6a6790c":"##get stats on all other metrics\ntitanic.describe()","fc40c5be":"## size of the family including the passenger.\ntitanic['FamilySize']=titanic['Parch']+titanic['SibSp']+1","f4f6f86f":"## map the first letter of the cabin to the cabin.\ntitanic.Cabin=titanic.Cabin.map(lambda x:x[0])\n\n## view the normalized count\ntitanic.Cabin.value_counts(normalize=True)","72ae2fe2":"titanic.head()","6ba34bb2":"def handle_non_numeric_data(df):\n\tcolumns=df.columns.values\n\tfor column in columns:\n\t\ttext_digit_vals={}\n\t\tdef convert_to_int(val):\n\t\t\treturn text_digit_vals[val] \n\n\t\tif df[column].dtype!= np.int64 and df[column].dtype!= np.float64:\n\t\t\tcolumn_contents=df[column].values.tolist()\t\t#.values is used to get the values of a function\n\t\t\tunique_elements=set(column_contents)\t#converting to a set\n\t\t\tx=0\n\t\t\t\t\t\n\t\t\tfor unique in unique_elements:\n\t\t\t\tif unique not in text_digit_vals:\n\t\t\t\t\ttext_digit_vals[unique]=x\n\t\t\t\t\tx+=1\n\n\t\t\tdf[column]=list(map(convert_to_int,df[column]))\t\t#we are resetting the df column by mapping the function here to the value in the column\n\n\treturn df","2bf99c5c":"titanic=handle_non_numeric_data(titanic)","f24df525":"titanic.head()","69282af6":"train=titanic[:train_id]\ntest=titanic[test_id:]","767e28c1":"## convert the survived back to int\ntrain.Suvived=train.Survived.astype(int)","c7fb4288":"train.head()","0eb9d1b4":"# create X and y for data and target values\nX = train.drop('Survived', axis=1).values\ny = train.Survived.values","96dcb84d":"test.head()","05bc1e17":"X_test=test.drop('Survived',1).values","3a8100c9":"# The parameters that we are going to optimise\nparameters = dict(\n    C = np.logspace(-5, 10, 15),\n    penalty = ['l1', 'l2']\n    #solver =[\u2018newton-cg\u2019, \u2018lbfgs\u2019, \u2018liblinear\u2019, \u2018sag\u2019, \u2018saga\u2019]\n    \n)","c515ee04":"## instantiate the logistic regression\nclf=LogisticRegression()\n\n# Perform grid search using the parameters and f1_scorer as the scoring method\ngrid_search=GridSearchCV(estimator=clf,param_grid=parameters,cv=6,n_jobs=-1)\n# here cv is used for the cross-validation strategy.\n","d49c7c6e":"grid_search.fit(X,y)","f3cf3da7":"clf1=grid_search.best_estimator_        # get the best estimator(classifier)\nprint(clf1)","52de7ed7":"# Print the tuned parameters and score\nprint(\"Tuned Logistic Regression Parameters: {}\".format(grid_search.best_params_)) \nprint(\"Best score is {}\".format(grid_search.best_score_))","9b7f68d1":"## prediction on test set\npred=grid_search.predict(X_test)\nprint(pred)","fd92ba8a":"# create param grid object\nforrest_params = dict(\n    max_depth = [n for n in range(7, 14)],\n    min_samples_split = [n for n in range(4, 12)],\n    min_samples_leaf = [n for n in range(2, 6)],\n    n_estimators = [n for n in range(10, 60, 10)],\n)","a2e8d83e":"forest=RandomForestClassifier()","508de9c9":"# build and fit model\nforest_cv = GridSearchCV(estimator=forest, param_grid=forrest_params, cv=5)\nforest_cv.fit(X, y)","ab4969f4":"print(\"Best score: {}\".format(forest_cv.best_score_))\nprint(\"Optimal params: {}\".format(forest_cv.best_estimator_))","b35e0e63":"# random forrest prediction on test set\nforrest_pred = forest_cv.predict(X_test)","41c5ed74":"sub=pd.DataFrame({'PassengerId':passengerId,'Survived':forrest_pred})","d06d8eae":"sub.head()","8294c19a":"sub.to_csv('prediction.csv',index=False)   \n## we initialise the index as false as we donot need the index","05ef1644":" we can also generate info from the Cabin as cabins near the life boats will have higher chance of suvival compared to the others located elsewhere.So we extract the first letter from the cabin and generate features.","a4d8d890":"# Modelling","c3311ddf":"\nIt appears that 1st class females had an incredible 97% survival rate while 1st class males only still had a 37% chance of survival. Even though you only had a 37% chance of surviving as a 1st class male, you still were almost 3 times more likely to survive than a 3rd class male who had the lowest survival rate amongst sex and class at 13.5%.","728bb0ec":"The best score using logistic regression was ~82% which wasn't bad. But let's see how we can fare with a Random Forrest Classifier algorithm instead.","dd096a50":"As expected, those passengers with a title of \"Miss\" tend to be younger than those titled \"Mrs\". Also, it looks like we have some age variability amongst the different passenger classes as well as between the sexes, so this should help us more accurately estimate the missing ages for the observations that do not have an age recorded.","22b85b6c":"Random forest classifier has a better accuracy than the logistic regression as deduced above.","22f51961":"# Creating new features from the data","73184a50":"It looks like we have a few NaNs in the dataset across a few features. We will use the data to try and fill in the gaps. The info() method reveals that the Age, Cabin, Embarked, and Fare all have a few entries missing. Technically the Survived column also has entries missing, but this is actually correct since we merged the train and test together for future feature engineering and the test data doesn't have a Survived column.\n\nAdditionally, from looking at the features, it looks like we can just drop PassengerId from the dataset all together since it isn't really a helpful feature, but rather simply a row identifier.","c04d8d44":"The first model we will try is a Logistic Regression model which is a binary classifier algorithm. We will be using GridSearchCV to fit our model by specifying a few paramters and return the best possible combination of those parameters.","35a5ba04":"The first feature we will look at building is FamilySize. This is important to look at because we want to see if having a large or small family affected someone's chances of survival.\n\nThe relevant features that will help with this are Parch (number of parents\/children aboard) and SibSp (number of siblings\/spouses aboard). We combine the Parch and SibSp features and add 1 as well as we want to count the passenger for each observation.","9849566c":"# Random Forest Model","c1c995a5":"The social status gives us a pretty good idea about the survival chance.","09209b3d":"# For submission on kaggle","a65263a9":"For our next step, we are going to assume that their is a relationship between a person's age and their title since it makes sense that someone that is younger is more likely to be a titled a \"Miss\" vs a \"Mrs\".\n\nWith this in mind, we will group the data by Sex, Pclass, and Title and then view the median age for the grouped classes.","d2f3886c":"For those who have seen the fateful story of titanic we know that the women and children were given priority oven men.Even though it is very astounding that only 19% of the men survived compared the 75% women.","735634f3":"Now we create a title feature which extracts the honorifc from the Name feature.Simply put, an honorific is the title or rank of a given person such as \u201cMrs\u201d or \u201cMiss\u201d. The following code takes a value like \u201cBraund, Mr. Owen Harris\u201d from the Name column and extracts \u201cMr\u201d.","68191db1":"# Logistic Regression"}}