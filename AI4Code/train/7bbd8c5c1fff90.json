{"cell_type":{"4c2e41b4":"code","7aa71b58":"code","79fc7fb0":"code","0c3c1f6a":"code","90a99ffc":"code","b6f4a60c":"code","c86e4296":"code","e7bccdf0":"code","d54ac971":"code","10a2038f":"code","58756073":"code","c95926e4":"code","a9536be1":"code","3a0e3873":"code","e7c91a70":"code","d5304279":"code","b229629d":"code","cfa7a283":"code","625aae93":"code","56ca4ad0":"code","614a7e58":"code","fa34a016":"code","83c13638":"code","942661c0":"code","2d15e3da":"code","ca5fac63":"code","b87f7c15":"code","9e2c63b6":"code","a42805ce":"code","c963465e":"code","d0381f25":"code","9148ea01":"code","c0df3c4f":"code","1dde5d12":"markdown","f21d9253":"markdown","8d42d296":"markdown","f214182a":"markdown","f75f3970":"markdown","4c072f77":"markdown","d058ae8b":"markdown","d977e0e0":"markdown","37e6721e":"markdown","7737bc45":"markdown","00d13f15":"markdown","d742fbaf":"markdown","473e3302":"markdown","9930bf35":"markdown"},"source":{"4c2e41b4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud\n\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score,roc_auc_score\nfrom sklearn.decomposition import PCA","7aa71b58":"df1=pd.read_json(\"\/kaggle\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json\",lines = True)\ndf2 = pd.read_json(\"\/kaggle\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json\",lines=True)","79fc7fb0":"df1.head()","0c3c1f6a":"df2.head()","90a99ffc":"df1.shape, df2.shape","b6f4a60c":"# Concatinating both datasets\ndf=pd.concat([df1,df2])\ndf.reset_index(drop=True,inplace=True)","c86e4296":"df.head()","e7bccdf0":"# Extracting only useful data (article_link have no use for sarcasm detection)\n\ndata=df[[\"headline\",\"is_sarcastic\"]]\ndata.head()","d54ac971":"# Stopwords\nstops=stopwords.words(\"english\")","10a2038f":"# Generating Corpus\n\ncorpus=[]\n\nfor i in range(data.shape[0]):\n    headline = re.sub(\"[^a-zA-Z]\",\" \",data[\"headline\"][i] )\n    headline=headline.lower()\n    headline=headline.split()\n    ps=PorterStemmer()\n    headline=[ps.stem(word) for word in headline if not word in set(stops)]\n    headline='  '.join(headline)\n    corpus.append(headline)","58756073":"corpus","c95926e4":"# Generating WordCloud\n\ncorpus_str=str(corpus)\ncloud = WordCloud(relative_scaling = 1.0,background_color = \"white\").generate(corpus_str)\n\nplt.figure(figsize = (12,10))\nplt.imshow(cloud)\nplt.axis(\"off\")\nplt.show()","a9536be1":"# Tfidf Vectorizing\n# Taking most relevant 1500 words\nvec=TfidfVectorizer(max_features=1500)\nX = vec.fit_transform(corpus).toarray()\nX = pd.DataFrame(X,columns=vec.get_feature_names())\nt_var = data.iloc[:, -1]","3a0e3873":"X.head()","e7c91a70":"# Splitting the dataset into the Training set and Test set\n\nX_train, X_test, y_train, y_test = train_test_split(X, t_var, test_size = 0.20, random_state = 0)","d5304279":"# Fitting XGBoost\n\nxgb_c=XGBClassifier(tree_method='gpu_hist', gpu_id=0)\n\nxgb_c.fit(X_train,y_train)\n","b229629d":"# Predicting on test data\npredict_xgb_c = xgb_c.predict(X_test)","cfa7a283":"confusion_matrix(y_test, predict_xgb_c)\n\nprint(classification_report(y_test, predict_xgb_c))\n\nprint(\"Accuracy score for XGBoost\",accuracy_score(y_test, predict_xgb_c))\nprint(\"Roc_Auc score for XGBoost\",roc_auc_score(y_test, predict_xgb_c))\n","625aae93":"X.head()","56ca4ad0":"scaler= StandardScaler()\nX_scaled = scaler.fit_transform(X)\n","614a7e58":"pca=PCA()\nX_pca=pca.fit_transform(X_scaled)\n","fa34a016":"print(pca.explained_variance_,\"\\n\")\nprint(np.sum(pca.explained_variance_),\"\\n\")\nprint(pca.explained_variance_ratio_,\"\\n\")\nprint(pca.explained_variance_ratio_ * 100,\"\\n\") ","83c13638":"y = pca.explained_variance_ratio_ * 100\nx = np.arange(1,1501)\nplt.plot(x,y)\nplt.show()","942661c0":"cumsum = np.cumsum(pca.explained_variance_ratio_ * 100)\nx = np.arange(1,1501)\nplt.plot(x,cumsum)\nplt.show()\n","2d15e3da":"cumsum","ca5fac63":"(cumsum[cumsum<=90]).shape","b87f7c15":"col_list=[]\nfor i in np.arange(1,1501):\n    c=\"PC\"+str(i)\n    col_list.append(c)\n    \ncomponents = pd.DataFrame(X_pca,\n                 columns = col_list,\n                 index = X.index)\n","9e2c63b6":"components.head()","a42805ce":"X_final = components.iloc[:,:1263]\nX_final.head()","c963465e":"# Splitting the dataset into the Training set and Test set\n\nX_pca_train, X_pca_test, y_pca_train, y_pca_test = train_test_split(X_final, t_var, test_size = 0.20, random_state = 0)","d0381f25":"# Fitting XGBoost\n\nxgb_pca=XGBClassifier(tree_method='gpu_hist', gpu_id=0)\nxgb_pca.fit(X_pca_train,y_pca_train)","9148ea01":"# Predicting on test data\ny_pred_pca = xgb_pca.predict(X_pca_test)","c0df3c4f":"confusion_matrix(y_pca_test, y_pred_pca)\n\nprint(classification_report(y_pca_test, y_pred_pca))\n\nprint(\"Accuracy score for XGBoost\",accuracy_score(y_pca_test, y_pred_pca))\nprint(\"Roc_Auc score for XGBoost\",roc_auc_score(y_pca_test, y_pred_pca))\n","1dde5d12":"### XGBoost Classifier","f21d9253":"# TF-IDF Vectorization\nTF-IDF vectoriztion gives us the importance of each word in the data.","8d42d296":"We can see using PCA increased our accuracy to 90%","f214182a":"Now we will select 1263 pca components for our model","f75f3970":"* We can use 1263 components for upto 90% cumsum","4c072f77":"* Using XGBoost gives us 74% Accuracy.\n* Lets use PCA and see if our accuracy increases","d058ae8b":"# Preprocessing the Data\n* Remove punctuations and numbers\n* Lowercase\n* Stemming\n* Removing Stopwords","d977e0e0":"* We can use upto 90% of information content","37e6721e":"## Visualizing Dataset ","7737bc45":"## Fitting the model","00d13f15":"### XGBoost Classifier on PCA","d742fbaf":"## Inporting useful libraries","473e3302":"## Importing dataset","9930bf35":"### Using PCA"}}