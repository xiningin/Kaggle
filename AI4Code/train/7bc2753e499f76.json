{"cell_type":{"46a5c7d3":"code","c1faf480":"code","74ed2b00":"code","f6b28ceb":"code","949924b2":"code","006d8bc5":"code","ef5937ff":"code","a6a01aee":"code","21ab6925":"code","2e766a5c":"code","e97e2628":"code","32463f07":"code","0339a59f":"code","06e9346b":"code","8728fd82":"code","1ee7fc45":"code","8eafcd71":"code","ffdfe6f5":"code","2dbfdcf9":"code","24338450":"code","13027d24":"code","2b17524a":"code","36e82715":"markdown","ff200ee2":"markdown","96f9df61":"markdown"},"source":{"46a5c7d3":"import torch","c1faf480":"#Scalar\nx = torch.empty(1) \nprint(x)\n\n","74ed2b00":"# vector, 1D\nx = torch.empty(3) \nprint(x)","f6b28ceb":"# matrix, 2D\nx = torch.empty(2,3) \nprint(x)","949924b2":"# tensor, 3 dimensions\nx = torch.empty(2,2,3) \nprint(x)\n# tensor, 4 dimensions\ny = torch.empty(2,2,2,3) \nprint(y)","006d8bc5":"#creating random numbers using torch. we can use torch.rand(size)\nx = torch.rand(5, 3)\nprint(x)\n","ef5937ff":"# torch.zeros(size), fill with 0\n# torch.ones(size), fill with 1\nx = torch.zeros(5, 3)\nprint(x)\n\ny=torch.ones(5,3)\nprint(y)","a6a01aee":"# we can check size by using size keyword\nprint(x.size())","21ab6925":"# check data type\nprint(x.dtype)","2e766a5c":"# we can specify datatypes while creating tensors  float32 default\nx = torch.zeros(5, 3, dtype=torch.float16)\nprint(x)\n","e97e2628":"x = torch.tensor([5.5, 3])\nprint(x.size())","32463f07":"# Operations\ny = torch.rand(2, 2)\nx = torch.rand(2, 2)\n","0339a59f":"# elementwise addition\nz = x + y\n\n# Addition\nprint(torch.add(x,y))\n\n#Substarction\nprint(torch.sub(x, y))\n\n#multiplication\nprint(torch.mul(x,y))\n\n#division\n\nprint(torch.div(x,y))","06e9346b":"# Slicing\nx = torch.rand(5,3)\nprint(x)\nprint(x[:, 0]) # all rows, column 0\nprint(x[1, :]) # row 1, all columns\nprint(x[1,1]) # element at 1, 1","8728fd82":"# Get the actual value if only 1 element in your tensor\nprint(x[1,1].item())\n","1ee7fc45":"# Reshape with torch.view()\nx = torch.randn(4, 4)\ny = x.view(16)\nz = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n# if -1 it pytorch will automatically determine the necessary size\nprint(x.size(), y.size(), z.size())","8eafcd71":"# Numpy\n# Converting a Torch Tensor to a NumPy array and vice versa is very easy\na = torch.ones(5)\nprint(a)\n","ffdfe6f5":"# torch to numpy with .numpy()\nb = a.numpy()\nprint(b)\nprint(type(b))","2dbfdcf9":"# Carful: If the Tensor is on the CPU (not the GPU),\n# both objects will share the same memory location, so changing one\n# will also change the other\na.add_(1)\nprint(a)\nprint(b)","24338450":"# numpy to torch with .from_numpy(x)\nimport numpy as np\na = np.ones(5)\nb = torch.from_numpy(a)\nprint(a)\nprint(b)\n","13027d24":"# again be careful when modifying\na += 1\nprint(a)\nprint(b)\n","2b17524a":"# by default all tensors are created on the CPU,\n# but you can also move them to the GPU (only if it's available )\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")          # a CUDA device object\n    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n    z = x + y\n    # z = z.numpy() # not possible because numpy cannot handle GPU tenors\n    # move to CPU again\n    z.to(\"cpu\")       # ``.to`` can also change dtype together!\n    print(z = z.numpy())","36e82715":" Everything in pytorch is based on Tensor operations.\n A tensor can have different dimensions\n so it can be 1d, 2d, or even 3d and higher\n \n \n scalar, vector, matrix, tensor","ff200ee2":"#operations on tensors","96f9df61":"Credits: Python Engineer\n\nDon't hesitate to watch YouTube video. It has clear explanation\n\nhttps:\/\/youtu.be\/exaWOE8jvy8\n\nIf you like my notebook Please upvote\n\nI will going to create more notebooks on pytorch"}}