{"cell_type":{"a2847b52":"code","01503bc3":"code","22c7deab":"code","56e1fc15":"code","227cf836":"code","77ecc334":"code","2b4f3c5b":"code","ed28cb9c":"code","28636a0d":"code","dca39d5b":"code","f1d597d3":"code","6a0e61a5":"code","b39c0b27":"code","ca71ebf8":"code","4a5aeb80":"code","d5dc50bb":"code","e45344e3":"code","a54fe063":"code","a662fe74":"code","e8371e2c":"code","71ee2a99":"code","54b74004":"code","d03c72b3":"code","6e2f951a":"code","acc3950d":"code","15a25639":"code","e3af689c":"code","f6bb5fca":"code","b9513ba8":"code","f0b9b774":"code","720228ac":"code","ad245b4a":"code","e7e77e21":"code","b4f6abd8":"code","1d8a1626":"code","daeafd80":"code","caacdacc":"code","9b30abfd":"code","c5c73f13":"code","0b0ce5bf":"code","c36281c2":"code","76a12d20":"code","ec37baa7":"code","f8d66c36":"code","49b14d65":"code","c8c47aa8":"code","c13f5757":"code","488ff940":"code","c89d273e":"code","66c85c5e":"code","1d4b7414":"code","215dca28":"code","d5e9fdf4":"code","6ba989b2":"code","b035aab8":"code","1706efc0":"code","d5118df1":"code","37949ff8":"code","838ad2fb":"code","bdaf8e5a":"code","6a871007":"markdown"},"source":{"a2847b52":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\n# from sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n\n# from lightgbm import LGBMClassifier\n# from xgboost import XGBClassifier\n# from sklearn.linear_model import LogisticRegression\n\npd.set_option('display.max_columns', 100)","01503bc3":"import os\nprint(os.listdir(\"..\/input\"))","22c7deab":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","56e1fc15":"train.head()","227cf836":"data = []\nfor feature in train.columns:\n    if feature == 'id':\n        use = 'id'\n    elif feature == 'target':\n        use = 'target'\n    else:\n        use = 'input'\n    \n    if 'bin' in feature or feature == 'target':\n        type = 'binary'\n    elif 'cat' in feature or feature == 'id':\n        type = 'categorical'\n    elif train[feature].dtype == float or isinstance(train[feature].dtype,float):\n        type = 'real'\n    elif train[feature].dtype == int:\n        type = 'integer'\n        \n    preserve = True\n    if feature =='id':\n        preserve = False\n    \n    dtype = train[feature].dtype\n    \n    category = 'none'\n    if 'ind' in feature:\n        category = 'individual'\n    elif 'reg' in feature:\n        category = 'registration'\n    elif 'car' in feature:\n        category = 'car'\n    elif 'calc' in feature:\n        category = 'calculated'\n    \n    feature_dict = {\n        'var_name':feature,\n        'use':use,\n        'type':type,\n        'preserve':preserve,\n        'dtype':dtype,\n        'category':category\n    }\n    \n    data.append(feature_dict)\n\nmetadata = pd.DataFrame(data,columns=['var_name', 'use', 'type', 'preserve', 'dtype', 'category'])\nmetadata.set_index('var_name',inplace=True)","77ecc334":"metadata","2b4f3c5b":"metadata[(metadata['type']=='categorical') & (metadata.preserve)]","ed28cb9c":"pd.DataFrame({'count':metadata.groupby('category')['category'].size()}).reset_index()","28636a0d":"pd.DataFrame({'count':metadata.groupby(['use','type'])['use'].size()}).reset_index()","dca39d5b":"plt.figure()\nfig,ax = plt.subplots(figsize=[6,6])\nsns.countplot('target',data=train,ax=ax)\nplt.ylabel('Number of Values',fontsize=12)\nplt.xlabel('Target Value',fontsize=12)\nplt.tick_params(axis='both', which='major', labelsize=12)\nheight = [p.get_height() for p in ax.patches]","f1d597d3":"print('Percentage of Target 0 of the total {}'.format(height[0]\/sum(height)*100))\nprint('Percentage of Target 1 of the total {}'.format(height[1]\/sum(height)*100))","6a0e61a5":"train[metadata[(metadata.type=='real')&(metadata.preserve)].index].describe()","b39c0b27":"real_corr = train[metadata[(metadata.type=='real')&(metadata.preserve)].index].corr()","ca71ebf8":"fig = plt.figure()\nfig,ax = plt.subplots(figsize=[15,15])\nsns.heatmap(real_corr,annot=True,square=True,center=0,cmap=plt.cm.summer,ax=ax)","4a5aeb80":"\"\"\"\nps_car_12 and ps_car_13 (0.67) \nps_reg_01 and ps_reg_03 (0.64)\nps_car_13 and ps_car_15 (0.53)\nps_reg_02 and ps_reg_03 (0.52)\nps_reg_01 and ps_reg_02 (0.47)\n\"\"\"\ns = train.sample(frac=0.1)","d5dc50bb":"sns.lmplot(x='ps_car_12',y='ps_car_13',hue='target',data=s,palette='Set1',scatter_kws={'alpha':0.3})","e45344e3":"sns.lmplot(x='ps_car_12',y='ps_car_13',hue='target',data=s,palette='Set1',scatter_kws={'alpha':0.3})\nplt.suptitle('plot for ps_car_12 and ps_car_13')","a54fe063":"sns.lmplot(x='ps_reg_01',y='ps_reg_03',hue='target',data=s,palette='Set1',scatter_kws={'alpha':0.3})\nplt.suptitle('plot for ps_reg_01 and ps_reg_03')","a662fe74":"sns.lmplot(x='ps_car_13',y='ps_car_15',hue='target',data=s,palette='Set1',scatter_kws={'alpha':0.3})\nplt.suptitle('plot for ps_car_13 and ps_car_15')","e8371e2c":"sns.lmplot(x='ps_reg_02',y='ps_reg_03',hue='target',data=s,palette='Set1',scatter_kws={'alpha':0.3})\nplt.suptitle('plot for ps_reg_02 and ps_reg_03')","71ee2a99":"sns.lmplot(x='ps_reg_01',y='ps_reg_02',hue='target',data=s,palette='Set1',scatter_kws={'alpha':0.3})\nplt.suptitle('plot for ps_reg_01 and ps_reg_02')","54b74004":"train[metadata[(metadata['type'] == 'integer') & (metadata.preserve)].index].describe()","d03c72b3":"int_corr = train[metadata[(metadata['type'] == 'integer') & (metadata.preserve)].index].corr()\nfig = plt.figure()\nfig,ax = plt.subplots(figsize=[20,20])\nax = sns.heatmap(int_corr,cmap=plt.cm.summer,center=0,square=True,annot=True)","6e2f951a":"train[metadata[(metadata['type'] == 'binary') & (metadata.preserve)].index].describe()","acc3950d":"bin_col = [col for col in train.columns if '_bin' in col]\nzero_list = []\none_list = []\nfor col in bin_col:\n    zero_list.append((train[col]==0).sum()\/train.shape[0]*100)\n    one_list.append((train[col]==1).sum()\/train.shape[0]*100)\nfig = plt.figure()\nfig, ax = plt.subplots(figsize=[6,6])\np1 = sns.barplot(x=bin_col,y=zero_list,ax=ax,color='blue')\np1.set_xticklabels(p1.get_xticklabels(),rotation=90)\np2 = sns.barplot(x=bin_col,y=one_list,bottom=zero_list,ax=ax,color='red')\nplt.ylabel('Percent of one\/zero[%]')\nplt.xlabel('Binary Features')","15a25639":"var = [col for col in train.columns if '_bin' in col]\ni = 0\nt1 = train.loc[train.target==1]\nt0 = train.loc[train.target==0]\n\nfig = plt.figure()\nfig,ax = plt.subplots(6,3,figsize=[16,24])\n\nfor feature in var:\n    i+= 1\n    plt.subplot(6,3,i)\n    sns.kdeplot(t1[feature],bw=0.5,label='target=1')\n    sns.kdeplot(t0[feature],bw=0.5,label='target=0')\n    plt.ylabel('Density plot',fontsize=12)\n    plt.xlabel(feature,fontsize=12)","e3af689c":"vars = metadata[(metadata['type'] == 'categorical') & (metadata.preserve)].index\nfor col in vars:\n    fig,ax = plt.subplots(figsize=(6,6))\n    cat_perc = train[[col,'target']].groupby(col,as_index=False).mean()\n    cat_perc.sort_values(by='target',ascending=False,inplace=True)\n    sns.barplot(x=col,y='target',data=cat_perc,ax=ax,order=cat_perc[col])","f6bb5fca":"var = metadata[(metadata.type=='categorical') & (metadata.preserve)].index\ni = 0\nt1 = train.loc[train['target']==1] \nt0 = train.loc[train['target']==0]\nfig = plt.figure()\nfig,ax = plt.subplots(5,3,figsize=[16,20])\n\nfor col in var:\n    i+=1\n    plt.subplot(5,3,i)\n    sns.kdeplot(t1[col],bw=0.5,label='target = 1')\n    sns.kdeplot(t0[col],bw=0.5,label='target = 0')\n    plt.ylabel('Density plot', fontsize=12)\n    plt.xlabel(col, fontsize=12)","b9513ba8":"var = metadata[(metadata.category == 'registration')&(metadata.preserve)].index\nfig = plt.figure()\nfig,ax = plt.subplots(1,3,figsize=[12,4])\ni = 0\n\nfor col in var:\n    i+=1\n    plt.subplot(1,3,i)\n    sns.kdeplot(train[col],bw=0.5,label='train')\n    sns.kdeplot(test[col],bw=0.5,label='test')\n    plt.ylabel('Distribution')\n    plt.xlabel(col)","f0b9b774":"var = metadata[(metadata.category == 'individual')&(metadata.preserve)].index\nfig = plt.figure()\nfig,ax = plt.subplots(5,4,figsize=[18,20])\ni = 0\n\nfor col in var:\n    i+=1\n    plt.subplot(5,4,i)\n    sns.kdeplot(train[col],bw=0.5,label='train')\n    sns.kdeplot(test[col],bw=0.5,label='test')\n    plt.ylabel('Distribution')\n    plt.xlabel(col)","720228ac":"var = metadata[(metadata.category == 'car')&(metadata.preserve)].index\nfig = plt.figure()\nfig,ax = plt.subplots(4,4,figsize=[18,18])\ni = 0\n\nfor col in var:\n    i+=1\n    plt.subplot(4,4,i)\n    sns.kdeplot(train[col],bw=0.5,label='train')\n    sns.kdeplot(test[col],bw=0.5,label='test')\n    plt.ylabel('Distribution')\n    plt.xlabel(col)","ad245b4a":"var = metadata[(metadata.category == 'calculated')&(metadata.preserve)].index\nfig = plt.figure()\nfig,ax = plt.subplots(5,4,figsize=[18,20])\ni = 0\n\nfor col in var:\n    i+=1\n    plt.subplot(5,4,i)\n    sns.kdeplot(train[col],bw=0.5,label='train')\n    sns.kdeplot(test[col],bw=0.5,label='test')\n    plt.ylabel('Distribution')\n    plt.xlabel(col)","e7e77e21":"desired_apriori=0.10\n\nidx_0 = train[train.target == 0].index\nidx_1 = train[train.target == 1].index\n\nno_0 = len(train.loc[idx_0])\nno_1 = len(train.loc[idx_1])\n\nundersampling_rate = ((1-desired_apriori)*no_1)\/(desired_apriori*no_0)\nundersampled_no_0 = int(undersampling_rate*no_0)\nprint('Rate to undersample records with target=0: {}'.format(undersampling_rate))\nprint('Number of records with target=0 after undersampling: {}'.format(undersampled_no_0))\n\nundersampled_idx = shuffle(idx_0,random_state=242,n_samples=undersampled_no_0)\n\nidx_list = list(undersampled_idx)+list(idx_1)\n\ntrain = train.loc[idx_list].reset_index(drop=True)","b4f6abd8":"df_null = (train == -1).sum().sort_values(ascending=False).reset_index()\ndf_null.columns = ['features','count']\ndf_null = df_null[df_null['count'] > 0]\ndf_null","1d8a1626":"for col in df_null.features:\n    print('Variable {} has {} records among {} ({:.2%}) with missing values'.format(col,(train[col]==-1).sum(),len(train[col]),((train[col]==-1).sum()\/len(train[col]))))","daeafd80":"metadata.loc[col_to_drop,'preserve']","caacdacc":"col_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\nmetadata.loc[col_to_drop,'preserve'] = False\ntrain = train.drop(col_to_drop,axis=1)\ntest = test.drop(col_to_drop,axis=1)","9b30abfd":"vars_to_drop = ['ps_car_03_cat','ps_car_05_cat']\ntrain = train.drop(vars_to_drop,axis=1)\ntest = test.drop(vars_to_drop,axis=1)\nmetadata.loc[(vars_to_drop),'preserve'] = False","c5c73f13":"print('Train dataset(row,col):',train.shape,\"\\nTest dataset(row,col):\",test.shape)","0b0ce5bf":"dummy = metadata.loc[df_null.features,:]\ndummy[(dummy.type=='real')|(dummy.type=='integer')]","c36281c2":"mean_imputer = Imputer(missing_values=-1,strategy='mean',axis=0)\n\ndummy = metadata.loc[df_null.features,:]\n\nreal_int_cols = dummy[(dummy.type=='real')|(dummy.type=='integer')].index\nfor col in real_int_cols:\n    if metadata.loc[col].type =='real':\n        train[col] = mean_imputer.fit_transform(pd.DataFrame(train[col])).ravel()\n        train[col] = train[col].astype(float)\n        test[col] = mean_imputer.transform(pd.DataFrame(test[col])).ravel()\n        test[col] = test[col].astype(float)\n        \n    if metadata.loc[col].type == 'integer':\n        train[col] = mean_imputer.fit_transform(pd.DataFrame(train[col])).ravel()\n        train[col] = train[col].astype(float)\n        test[col] = mean_imputer.transform(pd.DataFrame(test[col])).ravel()\n        test[col] = test[col].astype(float)","76a12d20":"frequent_imputer = Imputer(missing_values=-1,strategy='most_frequent',axis=0)\n\ndroped_cats = dummy[dummy.type=='categorical'].index[~dummy[dummy.type=='categorical'].index.isin(['ps_car_03_cat','ps_car_05_cat'])]\n\nfor col in droped_cats:\n    train[col] = frequent_imputer.fit_transform(pd.DataFrame(train[col])).ravel()\n    train[col] = train[col].astype(int)\n\n    test[col] = frequent_imputer.fit_transform(pd.DataFrame(test[col])).ravel()\n    test[col] = test[col].astype(int)","ec37baa7":"for col in droped_cats:\n    print('Variable {} has {} records among {} ({:.2%}) with missing values'.format(col,(train[col]==-1).sum(),len(train[col]),((train[col]==-1).sum()\/len(train[col]))))\nfor col in real_int_cols:\n    print('Variable {} has {} records among {} ({:.2%}) with missing values'.format(col,(train[col]==-1).sum(),len(train[col]),((train[col]==-1).sum()\/len(train[col]))))","f8d66c36":"# Script by https:\/\/www.kaggle.com\/ogrellier\n# Code: https:\/\/www.kaggle.com\/ogrellier\/python-target-encoding-for-categorical-features\ndef add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None, \n                  tst_series=None, \n                  target=None, \n                  min_samples_leaf=1, \n                  smoothing=1,\n                  noise_level=0):\n    \"\"\"\n    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n    https:\/\/kaggle2.blob.core.windows.net\/forum-message-attachments\/225952\/7441\/high%20cardinality%20categoricals.pdf\n    trn_series : training categorical feature as a pd.Series\n    tst_series : test categorical feature as a pd.Series\n    target : target data as a pd.Series\n    min_samples_leaf (int) : minimum samples to take category average into account\n    smoothing (int) : smoothing effect to balance categorical average vs prior  \n    \"\"\" \n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean \n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 \/ (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) \/ smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","49b14d65":"train_encoded, test_encoded = target_encode(train['ps_car_11_cat'],\n                                           test['ps_car_11_cat'],\n                                           target=train.target,\n                                           min_samples_leaf=100,\n                                           smoothing=10,\n                                           noise_level=0.01)\ntrain['ps_car_11_cat_te'] = train_encoded\ntrain.drop('ps_car_11_cat',axis=1,inplace=True)\nmetadata.loc['ps_car_11_cat', 'preserve'] =False\n\ntemp_dict = metadata.loc['ps_car_11_cat',:].to_dict()\ntemp_dict['var_name'] = 'ps_car_11_cat_te'\ntemp_dict['type'] ='real'\ntemp_dict['preserve'] = True\ntemp_dict['dtype'] = train['ps_car_11_cat_te'].dtype\ntest['ps_car_11_cat_te'] = test_encoded\ntest.drop('ps_car_11_cat',axis=1,inplace=True)\n\nmetadata = pd.concat([metadata,pd.DataFrame(temp_dict,index=range(1)).set_index('var_name')])","c8c47aa8":"print('Train dataset(row,col):',train.shape,\"\\nTest dataset(row,col):\",test.shape)","c13f5757":"cat_features = metadata[(metadata.type=='categorical')&(metadata.preserve)].index\nfor col in cat_features:\n    temp = pd.get_dummies(pd.Series(train[col]),prefix=col)\n    train = pd.concat([train,temp],axis=1)\n    train.drop(col,inplace=True,axis=1)\n    \nfor col in cat_features:\n    temp = pd.get_dummies(pd.Series(test[col]),prefix=col)\n    test = pd.concat([test,temp],axis=1)\n    test.drop(col,inplace=True,axis=1)","488ff940":"print('Train dataset(row,col):',train.shape,\"\\nTest dataset(row,col):\",test.shape)","c89d273e":"real_cols = metadata[(metadata.type=='real')&(metadata.preserve)].index","66c85c5e":"poly = PolynomialFeatures(degree=2,interaction_only=True,include_bias=False)\n\nnew_poly_df_train = pd.DataFrame(poly.fit_transform(train[real_cols]),columns=poly.get_feature_names(real_cols))\nnew_poly_df_test = pd.DataFrame(poly.transform(train[real_cols]),columns=poly.get_feature_names(real_cols))\n\nnew_poly_df_train = new_poly_df_train.drop(real_cols,axis=1)\nnew_poly_df_test =new_poly_df_test.drop(real_cols,axis=1)\n\ntrain = pd.concat([train,new_poly_df_train],axis=1)\ntest = pd.concat([test,new_poly_df_train],axis=1)","1d4b7414":"print('Train dataset(row,col):',train.shape,\"\\nTest dataset(row,col):\",test.shape)","215dca28":"# id_test = test['id'].values\n# target_train = train['target'].values\n\n# train.drop(['id','target'],axis=1,inplace=True)\n# test.drop(['id'],axis=1,inplace=True)","d5e9fdf4":"X = train.drop(['id','target'],axis=1)\ntrain_labels = X.columns\ny = train['target']\n\nX_test = test.drop('id',axis=1)\ny_test = np.zeros(X_test.shape[0])\n\nsub = test['id'].to_frame()\nsub['target'] = 0","6ba989b2":"print('Train dataset(row,col):',X.shape,\"\\nTest dataset(row,col):\",X_test.shape)","b035aab8":"def gini(y, pred):\n    g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n    g = g[np.lexsort((g[:,2], -1*g[:,1]))]\n    gs = g[:,0].cumsum().sum() \/ g[:,0].sum()\n    gs -= (len(y) + 1) \/ 2.\n    return gs \/ len(y)\n\ndef gini_xgb(pred, y):\n    y = y.get_label()\n    return 'gini', gini(y, pred) \/ gini(y, y)","1706efc0":"import xgboost\nfrom xgboost import XGBClassifier\n\nXgbC = XGBClassifier( \n learning_rate =0.1,\n n_estimators=1000,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\nxgb_params = XgbC.get_xgb_params()\nxgtrain = xgboost.DMatrix(X.values,label=y.values)\ncvresult = xgboost.cv(xgb_params,xgtrain,num_boost_round=XgbC.get_params()['n_estimators'],nfold=5,metrics='auc',early_stopping_rounds=50)\n\nprint(cvresult.shape)\n\nfrom sklearn import metrics\n\nXgbC.set_params(n_estimators=cvresult.shape[0])\n\n#Fit the algorithm on the data\nXgbC.fit(X, y,eval_metric='auc')\n        \n#Predict training set:\ndtrain_predictions = XgbC.predict(X)\ndtrain_predprob = XgbC.predict_proba(X)[:,1]\n        \n#Print model report:\nprint (\"\\nModel Report\")\nprint (\"Accuracy : %.4g\" % metrics.accuracy_score(y, dtrain_predictions))\nprint (\"AUC Score (Train): %f\" % metrics.roc_auc_score(y, dtrain_predprob))","d5118df1":"indices = np.argsort(XgbC.feature_importances_)[::-1]\n\nnew_list = []\nfor f in range(X.shape[1]):\n    print(f+1,train_labels[indices[f]],XgbC.feature_importances_[indices[f]])\n    new_list.append(train_labels[indices[f]])","37949ff8":"feat_imp = pd.Series(XgbC.feature_importances_).sort_values(ascending=False)\n\nplt.figure(figsize=[12,8])\nfeat_imp.plot(kind='bar', title='Feature Importances')\nplt.ylabel('Feature Importance Score')\nplt.xticks(range(0,91),new_list,rotation=60)","838ad2fb":"XgbC = XGBClassifier( \n learning_rate =0.1,\n n_estimators=cvresult.shape[0],\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\n\nxgb_params = XgbC.get_xgb_params()\n\nfrom sklearn.model_selection import StratifiedKFold\n\nKFOLD = StratifiedKFold(n_splits=5,random_state=12)\n\nX = X.values\n\nfor i,(train_index,valid_index) in enumerate(KFOLD.split(X,y)):\n    \n    X_train, X_valid = X[train_index],X[valid_index]\n    y_train, y_valid = y[train_index], y[valid_index]\n    \n    d_train = xgboost.DMatrix(X_train,label=y_train)\n    d_valid = xgboost.DMatrix(X_valid,label=y_valid)\n    \n    watchlist = [(d_train,'train'),(d_valid,'valid')]\n    \n    xgboost_model = xgboost.train(xgb_params,d_train,cvresult.shape[0],watchlist,early_stopping_rounds=50,feval=gini_xgb,maximize=True,verbose_eval=100)\n    sub['target'] +=xgboost_model.predict(xgboost.DMatrix(X_test.values),ntree_limit=xgboost_model.best_ntree_limit+50) \/ 5","bdaf8e5a":"submission = pd.DataFrame()\nsubmission['id'] = sub['id']\nsubmission['target'] = sub['target']\nsubmission.to_csv('stacked.csv', index=False)","6a871007":"Now It's time to adapt my knowledge at porto by stacking\n==> \uc9c0\ub2c8\uacc4\uc218\uc5d0\uc11c \uc608\uce21\uac12\uacfc \ube44\uad50\ub418\ub294 \ub77c\ubca8\uc740 \uc2e4\uc81c\ub85c \uadf8\uc5d0 \ud574\ub2f9\ud558\ub294 target\uac12\uc744 \ub300\uc751\ud568"}}