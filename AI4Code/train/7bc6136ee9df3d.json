{"cell_type":{"3b478745":"code","68696176":"code","47d8a492":"code","156b93ac":"code","ccb7237d":"code","3c2dde6f":"code","cbad0cee":"code","c1d620fe":"code","ce268209":"code","da58d0b5":"code","e897c0cf":"code","66d5371e":"code","d1872a14":"code","b8c443d9":"code","6b7a1286":"code","658ba3c4":"code","9a89e80e":"code","37f5047c":"code","adeb688c":"code","dece8f63":"code","2d2ce48b":"code","6eee34b9":"code","4b20faf1":"code","beeaacbc":"code","1c3a6d8f":"code","c65058f5":"code","c156105c":"code","0f9be8e5":"markdown","7a6922c0":"markdown","fc505afb":"markdown","2c45b11c":"markdown","0bd334c1":"markdown","4c492d6d":"markdown","8a9ddea4":"markdown","ab863008":"markdown","4fee3637":"markdown","49184627":"markdown","61a69864":"markdown","b988710a":"markdown","53a33111":"markdown","3c0ece0c":"markdown","ce0d6e38":"markdown","540ac7eb":"markdown","1a765db4":"markdown"},"source":{"3b478745":"import pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RepeatedKFold\nimport numpy as np\nimport re","68696176":"df = pd.DataFrame(pd.read_csv('..\/input\/titanic\/train.csv'))\ndf_test = pd.DataFrame(pd.read_csv('..\/input\/titanic\/test.csv'))","47d8a492":"df.head()","156b93ac":"sns.lmplot(data = df, x = 'Age', y = 'Fare', hue = 'Survived', fit_reg = False, size = 4, aspect = 3)\nplt.show()","ccb7237d":"def countplot(var):\n    sns.countplot(data = df, x = var, hue = 'Survived', palette = ('R', 'b'))\n    plt.show()\ncountplot('Sex')","3c2dde6f":"countplot('Pclass')","cbad0cee":"countplot('Embarked')","c1d620fe":"df.isnull().sum()","ce268209":"print(df.groupby(['Pclass']).mean()['Age'])\nprint('\\n')\nprint(df.groupby(['Sex']).mean()['Age'])","da58d0b5":"# fill NaN Age values with the means, separating by Sex and Pclass\n\ndef age_nan(df):\n    for i in df.Sex.unique():\n        for j in df.Pclass.unique():\n            x = df.loc[((df.Sex == i) & (df.Pclass == j)), 'Age'].mean()\n            df.loc[((df.Sex == i) & (df.Pclass == j)), 'Age'] = df.loc[((df.Sex == i) & (df.Pclass == j)), 'Age'].fillna(x)\n\nage_nan(df)\nage_nan(df_test)","e897c0cf":"# fill NaN values of Embarked with 'S', because it's the most commom value for it\n\ndf['Embarked'] = df['Embarked'].fillna('S')\ndf_test['Embarked'] = df_test['Embarked'].fillna('S')","66d5371e":"#creating Cabin_NaN to test and train dataset and analysing it\n\ndf['Cabin_NaN'] = df['Cabin'].isnull().astype(int)\ndf_test['Cabin_NaN'] = df_test['Cabin'].isnull().astype(int)\ncountplot('Cabin_NaN')","d1872a14":"df_test.isnull().sum()","b8c443d9":"df_test.Fare = df_test.Fare.fillna(-1)","6b7a1286":"# logistic regression - cross validation function\n\ndef reg_cross_val(variables):\n    \n    X = df[variables]\n    y = df['Survived']\n    \n    rkfold = RepeatedKFold(n_splits = 2, n_repeats = 10, random_state = 10)\n    result = []\n    for treino, teste in rkfold.split(X):\n        X_train, X_test = X.iloc[treino], X.iloc[teste]\n        y_train, y_test = y.iloc[treino], y.iloc[teste]\n        \n        reg = LogisticRegression(max_iter = 500)\n        reg.fit(X_train, y_train)\n        result.append(reg.score(X_test, y_test))\n        \n    return np.mean(result)","658ba3c4":"#creating feature: Sex_bin\n\ndef is_female(x):\n    if x == 'female':\n        return 1\n    else:\n        return 0\n\ndf['Sex_bin'] = df['Sex'].map(is_female)\ndf_test['Sex_bin'] = df_test['Sex'].map(is_female)","9a89e80e":"# creating features: Embarked_S and Embarked_C\n\ndef embarked_s(x):\n    if x == 'S':\n        return 1\n    else:\n        return 0\n\ndf['Embarked_S'] = df['Embarked'].map(embarked_s)\ndf_test['Embarked_S'] = df_test['Embarked'].map(embarked_s)\n\ndef embarked_c(x):\n    if x == 'C':\n        return 1\n    else:\n        return 0\n    \ndf['Embarked_C'] = df['Embarked'].map(embarked_c)\ndf_test['Embarked_C'] = df_test['Embarked'].map(embarked_c)","37f5047c":"variables_before = ['Age', 'Pclass', 'Fare', 'SibSp', 'Parch']\nprint('Before the new features:', reg_cross_val(variables_before))\n\nvariables = ['Age', 'Sex_bin', 'Pclass', 'Fare', 'SibSp', 'Parch', 'Embarked_S',\\\n             'Embarked_C', 'Cabin_NaN']\n\nprint('With the new features:', reg_cross_val(variables))","adeb688c":"fig, ax =plt.subplots(1,2)\nsns.countplot(data = df, x = 'SibSp', hue = 'Survived', palette = ('R', 'b'), ax = ax[0])\nsns.countplot(data = df, x = 'Parch', hue = 'Survived', palette = ('R', 'b'), ax = ax[1])\nplt.show()","dece8f63":"# creating 'Family':\n\ndf['Family'] = df.SibSp + df.Parch\ndf_test['Family'] = df_test.SibSp + df_test.Parch","2d2ce48b":"variables = ['Age', 'Sex_bin', 'Pclass', 'Fare', 'Embarked_S',\\\n             'Embarked_C', 'Cabin_NaN', 'Family']\n\nreg_cross_val(variables)","6eee34b9":"text_ticket = ''\nfor i in df.Ticket:\n    text_ticket += i\n    \nlista = re.findall('[a-zA-Z]+', text_ticket)\nprint('Most repeated terms in Tickets: \\n')\nprint(pd.Series(lista).value_counts().head(10))","4b20faf1":"# creating features based on some commom words in Ticket feature\n\ndf['CA'] = df['Ticket'].str.contains('CA|C.A.').astype(int)\ndf['SOTON'] = df['Ticket'].str.contains('SOTON|STON').astype(int)\ndf['PC'] = df['Ticket'].str.contains('PC').astype(int)\ndf['SC'] = df['Ticket'].str.contains('SC|S.C').astype(int)\ndf['C'] = df['Ticket'].str.contains('C').astype(int)\n\n# same with the df_test\n\ndf_test['CA'] = df_test['Ticket'].str.contains('CA|C.A.').astype(int)\ndf_test['SOTON'] = df_test['Ticket'].str.contains('SOTON|STON').astype(int)\ndf_test['PC'] = df_test['Ticket'].str.contains('PC').astype(int)\ndf_test['SC'] = df_test['Ticket'].str.contains('SC|S.C').astype(int)\ndf_test['C'] = df_test['Ticket'].str.contains('C').astype(int)","beeaacbc":"text_name = ''\nfor i in df.Name:\n    text_name += i\n    \nlista = re.findall('[a-zA-Z]+', text_name)\nprint('Most repeated words in Name column: \\n')\nprint(pd.Series(lista).value_counts().head(10))","1c3a6d8f":"# creating features based on some commom words in Name feature\n\ndf['Master'] = df['Name'].str.contains('Master').astype(int)\ndf['Mr'] = df['Name'].str.contains('Mr').astype(int)\ndf['Miss'] = df['Name'].str.contains('Miss').astype(int)\ndf['Mrs'] = df['Name'].str.contains('Mrs').astype(int)\n\n#same with df_teste\n\ndf_test['Master'] = df_test['Name'].str.contains('Master').astype(int)\ndf_test['Mr'] = df_test['Name'].str.contains('Mr').astype(int)\ndf_test['Miss'] = df_test['Name'].str.contains('Miss').astype(int)\ndf_test['Mrs'] = df_test['Name'].str.contains('Mrs').astype(int)","c65058f5":"variables = ['Age', 'Sex_bin', 'Pclass', 'Fare', 'Embarked_S','Embarked_C',\\\n             'CA', 'SOTON', 'PC', 'SC','C', 'Mr', 'Miss', 'Master', 'Mrs', 'Family']\n\nprint(reg_cross_val(variables))","c156105c":"variables = ['Age', 'Sex_bin', 'Pclass', 'Fare','Family', 'Embarked_S','Embarked_C','Cabin_NaN',\\\n             'CA', 'SOTON', 'PC', 'SC', 'Master', 'Mr', 'Miss', 'C', 'Mrs']\n\nX = df[variables]\ny = df['Survived']\n\nreg = LogisticRegression(max_iter = 500)\nreg.fit(X,y)\nresp = reg.predict(df_test[variables])\n\nsubmit = pd.Series(resp, index=df_test['PassengerId'], name='Survived')\nsubmit.to_csv(\"model.csv\", header=True)","0f9be8e5":"## Thanks for your attention!\nComments and feedbacks are most welcome :)","7a6922c0":"The **Cabin** feature has a significant number of NaN values, and my first thought was to drop the column. But we can extract some interesting info from this, from separating who had a specified Cabin and who didn't. So we created the **Cabin_NaN** feature, that returns 1 for NaN values. ","fc505afb":"### Second try:","2c45b11c":"## Submiting","0bd334c1":"Okay, we had a bit of a improvement on our validation here!","4c492d6d":"Nice! It also seems reasonable to merge **SibSp** and **Parch** features into 1: **Family**. But it's yet more reasonable if we visualy check that before doing anything, so let's make some late exploratory data analysis here:","8a9ddea4":"Okay, we can observe some things:\n\n* the age-fare plot shows a distributed result, but some differences are evident on the extreme values.\n* being a man from 3rd class that embarked on 'S' may not be very good. \n* most of the passengers were men from 3rd class that embarked on 'S'.\n\nNow, let's take a look at the missing values and what to do with them.","ab863008":"Same thing with **Embarked**, but here we have 3 possible values, so features were created for S and C.\n\nNo, Q was not ignored. But it's logical that if someone didn't embark on S or C, surely did on Q.","4fee3637":"Now we should handle the test dataset missing values as well. We found only 1 NaN value for Fare, that was filled with -1.","49184627":"Logistic Regression must \"be fed\" with numbers, not strings. So a new feature **Sex_bin** was created based on **Sex** info. 0 for male and 1 for female.","61a69864":"## Missing Values\n\nLet's handle the **Age** missing values first. We could just fill with -1 or with the mean, but I think we can be more careful. \n\nWe will calculate the **Age** mean of each **Pclass** and **Sex** and fill the NaN values with them.","b988710a":"# Journey through *Titanic: Machine Learning from Disaster*\n\n### My first Kaggle Kernel\n\nYes, like most of you guys I started practicing data science with the Titanic competition. I learned a lot and realized that there is much to learn, and that's exciting! As these were my first experiences with Kaggle I would like to share them with you on my first Kaggle Kernel.","53a33111":"This function builds a Logistic Regression prediction model, make a cross validation and return the score. With it we can easily see, progressively, how the model improves as we create new features to feed the algorithm.","3c0ece0c":"There were just 2 NaN values on Embarked, so it's reasonable to fill them with Embarked's mode: **S**","ce0d6e38":"First we should do some initial exploratory data analysis:","540ac7eb":"Ticket and Name features may have some interesting information as well. The most commom words or terms in them may give us some ideas.","1a765db4":"## First try (with and without the created features):"}}