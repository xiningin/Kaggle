{"cell_type":{"d896932e":"code","70081190":"code","ee8f5f68":"code","17d8f36b":"code","1c650f03":"code","6bcac7c5":"code","84a650f8":"code","9cc24577":"code","4690fe19":"code","65ea9313":"code","aa341575":"code","b322e2fb":"code","2347145a":"code","0337be74":"code","b1765510":"code","128284bd":"code","1db69888":"markdown","54307c8f":"markdown","67fc0601":"markdown","6d043ab0":"markdown","921dbad8":"markdown","4c1a9107":"markdown","0294c181":"markdown","a0050cd2":"markdown","c6468f5b":"markdown","a26a0121":"markdown","1c1b6a53":"markdown","9a6ebed4":"markdown","d3d6616b":"markdown","20a25c60":"markdown","a5a87070":"markdown","5f7612bb":"markdown"},"source":{"d896932e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns #vizualization\nimport matplotlib.pyplot as plt #vizualization\nfrom matplotlib import cm\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","70081190":"iris = pd.read_csv(\"..\/input\/Iris.csv\")\niris.head(5)\niris.info()","ee8f5f68":"iris.columns","17d8f36b":"iris['Species'].unique()","1c650f03":"sns.pairplot(data=iris[iris.columns[1:6]], hue='Species')\nplt.show()","6bcac7c5":"fig = iris[iris.Species=='Iris-setosa'].plot(kind='scatter', x='PetalLengthCm', y='PetalWidthCm', color='red', label='Setosa')\niris[iris.Species=='Iris-versicolor'].plot(kind='scatter', x='PetalLengthCm', y='PetalWidthCm', color='green', label='Versicolor', ax=fig)\niris[iris.Species=='Iris-virginica'].plot(kind='scatter', x='PetalLengthCm', y='PetalWidthCm', color='yellow', label='Virginica', ax=fig)\nfig.set_xlabel(\"Petal Length\")\nfig.set_ylabel(\"Petal Width\")\nfig.set_title(\"Petal length depending on Width\")\nfig = plt.gcf()\nfig.set_size_inches(8,5)\nplt.show()","84a650f8":"plt.figure(figsize=(8,5)) \nsns.heatmap(iris.corr(),annot=True,cmap='cubehelix_r') \nplt.show()","9cc24577":"plt.subplots(figsize = (10,8))\nfrom pandas.tools import plotting\n\ncmap = cm.get_cmap('summer') \nplotting.andrews_curves(iris.drop(\"Id\", axis=1), \"Species\", colormap = cmap)\nplt.show()","4690fe19":"iris.drop('Id',axis=1, inplace=True) #\ndf_norm = iris[iris.columns[0:4]].apply(lambda x:(x - x.min())\/(x.max() - x.min()))\ndf_norm.sample(n=5)","65ea9313":"target = iris[['Species']].replace(iris['Species'].unique(), [0,1,2])\ndf = pd.concat([df_norm, target], axis=1)\ndf.sample(n=5)","aa341575":"import keras\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nfrom sklearn.preprocessing import StandardScaler, LabelBinarizer","b322e2fb":"X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]\ny = df['Species']\n\nX = StandardScaler().fit_transform(X)\ny = LabelBinarizer().fit_transform(y)","2347145a":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 101)\n","0337be74":"model = Sequential()\nmodel.add(Dense( 12, input_dim=4, activation = 'relu'))\nmodel.add(Dense( units = 15, activation= 'relu'))\nmodel.add(Dense( units = 8, activation= 'relu'))\nmodel.add(Dense( units = 10, activation= 'relu'))\nmodel.add(Dense( units = 3, activation= 'softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nhistory = model.fit(x_train, y_train, epochs = 120, validation_data = (x_test, y_test))","b1765510":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title(\"Accuracy\")\nplt.legend(['train', 'test'])\nplt.show()","128284bd":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss')\nplt.legend(['Train','Test'])\nplt.show()","1db69888":"We firstly drop out unecessary column, axis=1. Then process normalization for features vector values and check it with showing first 5 lines:","54307c8f":"The Petal Width and Length are highly correlated, what we can not say about Sepal Width and Length.\nNow let's build an Andrews curve:","67fc0601":"We start with loading essential libraries for data processing and visulization.","6d043ab0":"Dividing data into train set and test set. We will do it 80\/20.","921dbad8":"Lets build a heatmap with input as the correlation matrix calculted by * iris.corr()*","4c1a9107":"**Hello everyone. This is a detailed notebook with Iris dataset and deep learning. I hope it will be helpful and if yes, please vote up! Thank you :-)**","0294c181":"We are going to use very convinient tool to visualize data and dependencies between inputs, pairplot:","a0050cd2":"Creating a model, adding a layer after layer","c6468f5b":"Encoding Species labels for use as a target in Neural Network and concatenating feature vectors and target vector in one.","a26a0121":"Selecting the target as **y**, and feature vectors as **X** :","1c1b6a53":"The accuracy of our model is almost 97.5%. You can try to perform a better score by tunig the model.\n### Thank you for reading my notebook!","9a6ebed4":"There is no nulls so it is quite a clean dataset which can be processed.\nNow we are going to explore more information about columns and unique types of irises:","d3d6616b":"## Exploratory analysis - visualization","20a25c60":"Firstly let's load the dataset as a DateFrame, show the first 5 rows of the dataset and check the data in case of any nulls:","a5a87070":"We could not repeat this graph, but I think it would be usefull to notice that we see a clear correlation between Petal features and type of species. ","5f7612bb":"## Train the model"}}