{"cell_type":{"ad99adf5":"code","66204394":"code","1477994d":"code","52b7cd1e":"code","f4448f7f":"code","0e479b8b":"code","0e96b867":"code","a4a60f1b":"code","b7c7183c":"code","be8b1d27":"code","62e14a35":"code","a9f49d0c":"code","e4f43814":"code","49e2b818":"code","e2e3972c":"code","fcd97359":"code","ac1c33de":"code","7b11a6a5":"code","6dfbdc67":"code","341eee5b":"code","5f83c921":"code","a3865e7d":"code","df6406ca":"code","58bbe17c":"code","0b91ed9a":"code","b269a163":"code","4d736573":"code","8b997a6d":"code","d0f14089":"code","3e71f8ef":"code","1a54e29b":"code","d52c7ef8":"code","4710effe":"code","1d1fd9fc":"code","e9c9afb1":"code","ab1f10ad":"code","9cad0040":"code","e56eca40":"code","618ded06":"code","9c7d87ac":"code","756d44f5":"code","020954b4":"code","9ee52e9f":"code","ec12a60f":"code","0e2abfd7":"code","8b05815b":"code","4f6a8cac":"code","c4a22eb1":"code","052ac7dc":"code","515c6d12":"code","ed1f78ba":"code","56c8b807":"code","375cb2a9":"markdown","6ad68355":"markdown","8a85aeb5":"markdown","14cf0a6a":"markdown","ef2e99b4":"markdown","970d6ef5":"markdown","92a4544e":"markdown","35757de3":"markdown","7e7316de":"markdown","6b35c1da":"markdown","2a4842f3":"markdown","eb4cf6dd":"markdown","8571f7a1":"markdown","33e9d17f":"markdown","b923c7cd":"markdown","e0263651":"markdown","3d6ef269":"markdown","d7f45556":"markdown","8d56af3b":"markdown","0933648d":"markdown","6c072762":"markdown","ac1d53e3":"markdown","a2e294fd":"markdown","b9b6eb95":"markdown","f3fe72d4":"markdown","341b69ed":"markdown","786231fc":"markdown","7c6fa1aa":"markdown","c4f48653":"markdown","da707d75":"markdown","c9125f63":"markdown","e614206b":"markdown","69923d2e":"markdown","ea9bd675":"markdown","b98cf2b0":"markdown","5df8b221":"markdown","6f05f13e":"markdown","62a38aa4":"markdown"},"source":{"ad99adf5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","66204394":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re, nltk\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nimport spacy\nnlp = spacy.load('en_core_web_sm')\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nfrom tqdm import tqdm\nfrom gensim.models import Word2Vec\nfrom sklearn.manifold import TSNE\nimport gensim\nfrom gensim import corpora\nfrom gensim.models.coherencemodel import CoherenceModel\nimport matplotlib.colors as mcolors\nfrom collections import Counter\nfrom matplotlib.ticker import FuncFormatter\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook","1477994d":"data = pd.read_csv(\"\/kaggle\/input\/nips-papers\/papers.csv\")\ndata.head()","52b7cd1e":"print(data.columns)\nprint(len(data.columns))","f4448f7f":"data.shape","0e479b8b":"data.info()","0e96b867":"data.isnull().sum()","a4a60f1b":"sns.set_style(\"dark\")\ngraph = sns.catplot(data=data, x=\"year\", kind=\"count\", height = 4.5, aspect = 2.5, palette = \"hls\")\ngraph.set_xticklabels(rotation=90)\nplt.title(\"Frequency showing number of papers released in different years\", size = 20)","b7c7183c":"pd.set_option('display.max_colwidth', None)\ndata[\"paper_text\"].head()","be8b1d27":"data['Number_of_words'] = data['paper_text'].apply(lambda x:len(str(x).split()))","62e14a35":"plt.style.use('ggplot')\nplt.figure(figsize=(12,6))\nsns.distplot(data['Number_of_words'],kde = False,color=\"red\", bins = 100)\nplt.title(\"Frequency distribution of number of words for each text extracted\", size=20)","a9f49d0c":"plt.figure(figsize = (10,6))\ndata[\"Number_of_words\"].plot(kind=\"box\")\nplt.title(\"Word frequency distribution using Box plot\", size = 16)","e4f43814":"data.drop(data[data[\"Number_of_words\"]<200].index, inplace = True)","49e2b818":"data.shape","e2e3972c":"len(data[(data[\"Number_of_words\"]>200) & (data[\"Number_of_words\"]<500)])","fcd97359":"plt.style.use('ggplot')\nplt.figure(figsize=(12,6))\nsns.distplot(data['Number_of_words'],kde = False,color=\"blue\", bins = 100)\nplt.title(\"Frequency distribution of number of words for each text extracted after removing 3 columns\", size=18)","ac1c33de":"def cleaned_text(text):\n    clean = re.sub(\"\\n\",\" \",text)\n    clean=clean.lower()\n    clean=re.sub(r\"[~.,%\/:;?_&+*=!-]\",\" \",clean)\n    clean=re.sub(\"[^a-z]\",\" \",clean)\n    clean=clean.lstrip()\n    clean=re.sub(\"\\s{2,}\",\" \",clean)\n    return clean\ndata[\"cleaned_paper_text\"]=data[\"paper_text\"].apply(cleaned_text)","7b11a6a5":"data[\"cleaned_paper_text\"] = data[\"cleaned_paper_text\"].apply(lambda x: ' '.join([word for word in x.split() if len(word)>3]))","6dfbdc67":"data[\"cleaned_paper_text\"].head(10)","341eee5b":"cloud=WordCloud(colormap=\"winter\",width=600,height=400).generate(str(data[\"cleaned_paper_text\"]))\nfig=plt.figure(figsize=(13,18))\nplt.axis(\"off\")\nplt.imshow(cloud,interpolation='bilinear')","5f83c921":"stop=stopwords.words('english')\nstop.append(\"also\")\ndata[\"stop_removed_paper_text\"]=data[\"cleaned_paper_text\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))","a3865e7d":"data[\"tokenized\"]=data[\"stop_removed_paper_text\"].apply(lambda x: nltk.word_tokenize(x))","df6406ca":"def word_lemmatizer(text):\n    lem_text = [WordNetLemmatizer().lemmatize(i,pos='v') for i in text]\n    return lem_text\ndata[\"lemmatized\"]=data[\"tokenized\"].apply(lambda x: word_lemmatizer(x))\ndata[\"lemmatize_joined\"]=data[\"lemmatized\"].apply(lambda x: ' '.join(x))","58bbe17c":"data[\"lemmatize_joined\"].head()","0b91ed9a":"data['Number_of_words_for_cleaned'] = data['lemmatize_joined'].apply(lambda x:len(str(x).split()))","b269a163":"plt.style.use('ggplot')\nplt.figure(figsize=(12,6))\nsns.distplot(data['Number_of_words_for_cleaned'],kde = False, color= \"navy\", bins = 100)\nplt.title(\"Frequency distribution of number of words for each text extracted after removing stopwords and lemmatization\", size=16)","4d736573":"data.drop(data[data[\"Number_of_words_for_cleaned\"]>4500].index, inplace = True)","8b997a6d":"plt.style.use('ggplot')\nplt.figure(figsize=(12,6))\nsns.distplot(data['Number_of_words_for_cleaned'],kde = False, color= \"orangered\", bins = 100)\nplt.title(\"Frequency distribution of no of words in the documents after removing docs containing > 4500 words\", size=15)","d0f14089":"plt.style.use('ggplot')\nplt.figure(figsize=(14,6))\nfreq=pd.Series(\" \".join(data[\"lemmatize_joined\"]).split()).value_counts()[:30]\nfreq.plot(kind=\"bar\", color = \"orangered\")\nplt.title(\"30 most frequent words\",size=20)","3e71f8ef":"tokens = data[\"lemmatize_joined\"].apply(lambda x: nltk.word_tokenize(x))","1a54e29b":"w2v_model = Word2Vec(tokens,\n                     min_count=600,\n                     window=10,\n                     size=250,\n                     alpha=0.03, \n                     min_alpha=0.0007,\n                     workers = 4,\n                     seed = 42)","d52c7ef8":"v1 = w2v_model.wv['model']\nprint(v1)","4710effe":"sim_words = w2v_model.wv.most_similar('estimator')\nprint(sim_words)","1d1fd9fc":"sim_words_2 = w2v_model.wv.most_similar('synapse')\nprint(sim_words_2)","e9c9afb1":"sim_words_3 = w2v_model.wv.most_similar('connectivity')\nprint(sim_words_3)","ab1f10ad":"def tsne_plot(model):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=50, n_components=2, init='pca', n_iter=2000, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(15, 13)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()","9cad0040":"tsne_plot(w2v_model)","e56eca40":"w2v_model_2 = Word2Vec(tokens,\n                     min_count=1000,\n                     window=10,\n                     size=250,\n                     alpha=0.03, \n                     min_alpha=0.0007,\n                     workers = 4,\n                     seed=50)","618ded06":"tsne_plot(w2v_model_2)","9c7d87ac":"dictionary = corpora.Dictionary(data[\"lemmatized\"])\ndoc_term_matrix = [dictionary.doc2bow(rev) for rev in data[\"lemmatized\"]]","756d44f5":"LDA = gensim.models.ldamodel.LdaModel\n\n# Build LDA model\nlda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=4, random_state=100,\n                chunksize=200, passes=100)","020954b4":"lda_model.print_topics()","9ee52e9f":"coherence_model_lda = CoherenceModel(model=lda_model,\ntexts=data[\"lemmatized\"], dictionary=dictionary, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","ec12a60f":"cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n\ncloud = WordCloud(background_color='white',\n                  width=2500,\n                  height=1800,\n                  max_words=10,\n                  colormap='tab10',\n                  color_func=lambda *args, **kwargs: cols[i],\n                  prefer_horizontal=1.0)\n\ntopics = lda_model.show_topics(formatted=False)\n\nfig, axes = plt.subplots(2, 2, figsize=(15,15), sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):\n    fig.add_subplot(ax)\n    topic_words = dict(topics[i][1])\n    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n    plt.gca().imshow(cloud)\n    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n    plt.gca().axis('off')","0e2abfd7":"def format_topics_sentences(ldamodel=None, corpus=None, texts=data):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row_list in enumerate(ldamodel[corpus]):\n        row = row_list[0] if ldamodel.per_word_topics else row_list            \n        # print(row)\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=doc_term_matrix, texts=data[\"lemmatized\"])\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\ndf_dominant_topic.head(10)","8b05815b":"topics = lda_model.show_topics(formatted=False)\ndata_flat = [w for w_list in data[\"lemmatized\"] for w in w_list]\ncounter = Counter(data_flat)\n\nout = []\nfor i, topic in topics:\n    for word, weight in topic:\n        out.append([word, i , weight, counter[word]])\n\ndf = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n\n# Plot Word Count and Weights of Topic Keywords\nfig, axes = plt.subplots(2, 2, figsize=(10,7), sharey=True, dpi=100)\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\nfor i, ax in enumerate(axes.flatten()):\n    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n    ax_twin = ax.twinx()\n    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n    ax.set_ylabel('Word Count', color=cols[i])\n    ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n    ax.tick_params(axis='y', left=False)\n    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n\nfig.tight_layout(w_pad=2)    \nfig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \nplt.show()","4f6a8cac":"topic_counts = df_dominant_topic[\"Dominant_Topic\"].value_counts()\ntopic_counts.plot(kind = \"bar\", color = \"mediumseagreen\", figsize = (12,6))\nplt.title(\"how many documents belong to each topic\", size = 18)\nplt.xlabel(\"Topics\", size = 16)\nplt.ylabel(\"Number of documents\", size = 16)","c4a22eb1":"data[\"Dominant Topic\"] = df_dominant_topic[\"Dominant_Topic\"]","052ac7dc":"cloud=WordCloud(colormap=\"summer\",width=800,height=400).generate(str(data[data[\"Dominant Topic\"]==0.0][\"lemmatize_joined\"]))\nfig=plt.figure(figsize=(12,10))\nplt.axis(\"off\")\nplt.imshow(cloud,interpolation='bilinear')\nplt.title(\"WordCloud for Topic 0\", size = 24)","515c6d12":"cloud=WordCloud(colormap=\"Blues\",width=800,height=400).generate(str(data[data[\"Dominant Topic\"]==1.0][\"lemmatize_joined\"]))\nfig=plt.figure(figsize=(12,10))\nplt.axis(\"off\")\nplt.imshow(cloud,interpolation='bilinear')\nplt.title(\"WordCloud for Topic 1\", size = 24)","ed1f78ba":"cloud=WordCloud(colormap=\"Oranges\",width=800,height=400).generate(str(data[data[\"Dominant Topic\"]==2.0][\"lemmatize_joined\"]))\nfig=plt.figure(figsize=(12,10))\nplt.axis(\"off\")\nplt.imshow(cloud,interpolation='bilinear')\nplt.title(\"WordCloud for Topic 2\", size = 24)","56c8b807":"cloud=WordCloud(colormap=\"seismic\",width=800,height=400).generate(str(data[data[\"Dominant Topic\"]==3.0][\"lemmatize_joined\"]))\nfig=plt.figure(figsize=(12,10))\nplt.axis(\"off\")\nplt.imshow(cloud,interpolation='bilinear')\nplt.title(\"WordCloud for Topic 3\", size = 24)","375cb2a9":"Now let's create another model, and this time we will be using words with frequency more than 2000 and plot it using T-SNE, so that our plot looks a little more cleaner. But I believe this shouldn't be done, as words which are repeating 1000 times or more in the corpus, must have some importance, I am doing this just to show the plot a little cleaner, it is not recommended at all.","6ad68355":"Keyword above represents the most occuring words for a particular topic. Also there is weight of that topic in that document. This is so good!! Isn't it?? I am so thrilled!!","8a85aeb5":"# Dominant topic for each Document","14cf0a6a":"**Neural Information Processing System (NIPS) is one of the top machine learning conferences in the world. It covers topics ranging from deep learning and computer vision to cognitive science and reinforcement learning.** My intention is to extract as much information as possible from this data, and try all the text mining techniques on this dataset.","ef2e99b4":"Here I have limited the word count to 3500.\n\nNow let's see how many documents belong to each topic visually.","970d6ef5":"Now let's try to create a wordcloud from this cleaned text.","92a4544e":"# WordClouds for Topics' Keywords","35757de3":"So from the above we can see that most documents belong to topic 1, we will later label the topic names, based on the wordclouds, once we generate for each topic.","7e7316de":"![work_in_progress_anim.gif](attachment:work_in_progress_anim.gif)","6b35c1da":"![nips-logo-600px.png](attachment:nips-logo-600px.png)","2a4842f3":"So in the above we can see that there are still some stopwords present, but I want to keep them, as they are adding some meaning to our text. Now let's do vectorization, using ","eb4cf6dd":"Here we will work with only papers file.","8571f7a1":"Now we will do the next step cleaning process which is stop word removal and then we will lemmatize the words.","33e9d17f":"# Extracting informations from Text using Text Mining Techniques","b923c7cd":"So using the above code I have removed all the texts which are less than 200 words, basically there were only 3 rows, which contains data of this small size.","e0263651":"Basically 4 is not the optimum number of topics, we can find the optimum number by finding **coherence score** for different number of topics, and chosing topic with highest **coherence score**. Here I tried to save time, as I tried to plot coherence score for different topic numbers, but it was taking too much time, but very soon I will do that part for this dataset.\n\n**Now let's generate wordcloud for each topic.**","3d6ef269":"About T-SNE, it is mainly used for showing high dimensional figure to a lower dimensional figure. So here, as we know the words has a dimension of 250, and we are showing it in 2-D figure, just imagine! This is the job of T-SNE. So the concept says, similar words will have similar vectors, and so those similar words will be closer in the vector space, or sometimes may even overlap, if they are too similar.","d7f45556":"Now let's see how the word distribution has changed using a histogram after doing some text cleaning.","8d56af3b":"So it's a vector of 250 numbers, as described by us while creating the model, and each dimension represents some aspect of the word. Now let's see some similar words.","0933648d":"Hmmm... So our text size is quite big. Let's find out word-size for the texts using histogram. Let's just roughly get an idea before cleaning the text.","6c072762":"Now let's see the coherence score of this.","ac1d53e3":"Let's try for number of topics 5, we will keep on changing this, if we don't get better result.","a2e294fd":"So the above chart represents that in our dataset most papers got released for the year 2017. Now let's look at the text a little.","b9b6eb95":"# Vectorization using Word2vec","f3fe72d4":"# Topic Modelling using LDA","341b69ed":"# TSNE Plot for 2 models","786231fc":"So we can clearly see the shift after stopword removal. Now let's see the top 30 most frequent words in our whole text. Also let's remove documents with word size more than 6000, cause we don't want to deal with at the moment a document of this size.","7c6fa1aa":"# Word Clouds for each topic","c4f48653":"**Improvement needed**: I have to lemmatize it in a more robust way, as I can see some overlapping of the topics. Also I need to find the optimum number of topics, here I chose 4 randomly, I need to properly find the optimum number of topics using coherence score. \n\n**Work in progress.**","da707d75":"Using below piece of code we will try to find the dominant topic for each document. As we all know from the concept of LDA that each document is made up of different topics, for example in our case there are 4 topics in our case in total, and suppose for a document, it is made up of 70% of topic 1, and the rest consists of the rest topics distributed in some ways, so we are trying to find that dominant topic for each document using the below piece of code.","c9125f63":"**Work in progress.**","e614206b":"# Text Cleaning\n\nNow let's start the text cleaning process.","69923d2e":"Before begining, word2vec takes list of words as an input to train the neural network model. Word2vec is a neural network with 1 hidden layer. Input layer takes the vocabulary of words, and hidden layer's size is the vector size of the word that we want, so here we will be giving 250 as the hidden layer size or we want size of the vectors for each word to be 250.","ea9bd675":"So what's happening below is we are giving the list of words, and we are limiting some words, meaning, we are allowing those words in our vocabulary in the model, whose frequency is more than 600 using **min_count**, as we want a little cleaner t-sne plot, otherwise the plot will be a huge mess, as their are too much words in our data, so to make it a little cleaner, I just limited it to the words which has a frequency of above 600, also the vector size of a word here will be 250 and we defined it using **size**, and for **window** 10 mean, for a word, it will look 10 words right to it and 10 words to it's left. **Alpha** and **min_alpha** are the learning rates, so we will start with the learning rate of 0.03, and later near optimization, the learning rate will lower down to 0.0007, to find the optimized weights.","b98cf2b0":"Now let's see the vector representation of the word \"model\".","5df8b221":"The input will be in the form of document-term matrix, and we will convert that using the below piece of code.","6f05f13e":"Now we will try to see how many documents have words count between 200 and 500.","62a38aa4":"# Similar words using Word2vec"}}