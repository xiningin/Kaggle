{"cell_type":{"89d756ca":"code","a30951e6":"code","9fe4b0a6":"code","8dbe3af7":"code","aae9ed5b":"code","f4be1dbc":"code","7842392d":"code","3cda8af5":"code","5958c6fe":"code","2dc0658a":"code","678ae932":"markdown","8ee209d5":"markdown","f5862cb4":"markdown"},"source":{"89d756ca":"from shutil import copyfile\n\ncopyfile(src = \"..\/input\/quantitative-trading\/keras-rl\/requirements.txt\", dst = \"..\/working\/requirements.txt\")\ncopyfile(src = \"..\/input\/quantitative-trading\/keras-rl\/saved_models\/DQN_ep5.h5\", dst = \"..\/working\/DQN_ep5.h5\")\ncopyfile(src = \"..\/input\/quantitative-trading\/keras-rl\/saved_models\/DQN_ep10.h5\", dst = \"..\/working\/DQN_ep10.h5\")","a30951e6":"!pip install -r requirements.txt","9fe4b0a6":"import time\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom empyrical import sharpe_ratio\nfrom scipy.signal import argrelextrema\nfrom statsmodels.nonparametric.kernel_regression import KernelReg\nfrom collections import deque,defaultdict\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.optimizers import Adam\n\n%matplotlib inline","8dbe3af7":"class Portfolio:\n    def __init__(self, balance=50000):\n        self.initial_portfolio_value = balance\n        self.balance = balance\n        self.inventory = []\n        self.return_rates = []\n        self.portfolio_values = [balance]\n        self.buy_dates = []\n        self.sell_dates = []\n\n    def reset_portfolio(self):\n        self.balance = self.initial_portfolio_value\n        self.inventory = []\n        self.return_rates = []\n        self.portfolio_values = [self.initial_portfolio_value]\n\n        \ndef sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n\n\ndef softmax(x):\n    return np.exp(x) \/ np.sum(np.exp(x))\n\n\ndef stock_close_prices(key):\n    prices = []\n    lines = open(\"..\/input\/quantitative-trading\/\" + key + \".csv\", \"r\").read().splitlines()\n    for line in lines[1:]:\n        prices.append(float(line.split(\",\")[4]))\n    return prices\n\n\ndef generate_price_state(stock_prices, end_index, window_size):\n    start_index = end_index - window_size\n    if start_index >= 0:\n        period = stock_prices[start_index:end_index+1]\n    else:\n        # if end_index cannot suffice window_size, pad with prices on start_index\n        period = -start_index * [stock_prices[0]] + stock_prices[0:end_index+1]\n    return sigmoid(np.diff(period))\n\n\ndef generate_portfolio_state(stock_price, balance, num_holding):\n    return [np.log(stock_price), np.log(balance), np.log(num_holding + 1e-6)]\n\n\ndef generate_combined_state(end_index, window_size, stock_prices, balance, num_holding):\n    prince_state = generate_price_state(stock_prices, end_index, window_size)\n    portfolio_state = generate_portfolio_state(stock_prices[end_index], balance, num_holding)\n    return np.array([np.concatenate((prince_state, portfolio_state), axis=None)])\n\n\ndef treasury_bond_daily_return_rate():\n    r_year = 2.75 \/ 100  # approximate annual U.S. Treasury bond return rate\n    return (1 + r_year)**(1 \/ 365) - 1\n\n\ndef maximum_drawdown(portfolio_values):\n    end_index = np.argmax(np.maximum.accumulate(portfolio_values) - portfolio_values)\n    if end_index == 0:\n        return 0\n    beginning_iudex = np.argmax(portfolio_values[:end_index])\n    return (portfolio_values[end_index] - portfolio_values[beginning_iudex]) \/ portfolio_values[beginning_iudex]\n\n\ndef evaluate_portfolio_performance(agent):\n    portfolio_return = agent.portfolio_values[-1] - agent.initial_portfolio_value\n    print(\"--------------------------------\")\n    print('\u6295\u8cc7\u7d44\u5408\u7e3d\u50f9\u503c\uff1a         ${:.2f}'.format(agent.portfolio_values[-1]))\n    print('\u6295\u8cc7\u7d44\u5408\u73fe\u91d1\u9918\u984d\uff1a       ${:.2f}'.format(agent.balance))\n    print('\u6295\u8cc7\u7d44\u5408\u6301\u80a1\u6578\u91cf\uff1a        {}'.format(len(agent.inventory)))\n    print('\u7e3d\u6536\u76ca:                ${:.2f}'.format(portfolio_return))\n    print('\u5e73\u5747\/\u6bcf\u65e5\u6295\u62a5\u7387:         {:.3f}%'.format(np.mean(agent.return_rates) * 100))\n    print('\u8abf\u6574\u5f8c\u50f9\u683c\u590f\u666e\u6bd4\u7387\uff1a      {:.3f}'.format(sharpe_ratio(np.array(agent.return_rates)), risk_free=treasury_bond_daily_return_rate()))\n    print('\u6700\u5927\u56de\u64a4\uff1a              {:.3f}%'.format(maximum_drawdown(agent.portfolio_values) * 100))\n    print(\"--------------------------------\")\n    return portfolio_return\n\n\ndef plot_portfolio_transaction_history(stock_name, agent):\n\tportfolio_return = agent.portfolio_values[-1] - agent.initial_portfolio_value\n\tdf = pd.read_csv('..\/output\/{}.csv'.format(stock_name))\n\tbuy_prices = [df.iloc[t, 4] for t in agent.buy_dates]\n\tsell_prices = [df.iloc[t, 4] for t in agent.sell_dates]\n\tplt.figure(figsize=(15, 5), dpi=100)\n\tplt.title('{} \u7e3d\u56de\u5831\u7387 {}\uff1a ${:.2f}'.format(agent.model_type, stock_name, portfolio_return))\n\tplt.plot(df['Date'], df['Close'], color='black', label=stock_name)\n\tplt.scatter(agent.buy_dates, buy_prices, c='green', alpha=0.5, label='buy')\n\tplt.scatter(agent.sell_dates, sell_prices,c='red', alpha=0.5, label='sell')\n\tplt.xticks(np.linspace(0, len(df), 10))\n\tplt.ylabel('Price')\n\tplt.legend()\n\tplt.grid()\n\tplt.show()\n\n\ndef buy_and_hold_benchmark(stock_name, agent):\n    df = pd.read_csv('.\/data\/{}.csv'.format(stock_name))\n    dates = df['Date']\n    num_holding = agent.initial_portfolio_value \/\/ df.iloc[0, 4]\n    balance_left = agent.initial_portfolio_value % df.iloc[0, 4]\n    buy_and_hold_portfolio_values = df['Close']*num_holding + balance_left\n    buy_and_hold_return = buy_and_hold_portfolio_values.iloc[-1] - agent.initial_portfolio_value\n    return dates, buy_and_hold_portfolio_values, buy_and_hold_return\n\n\ndef plot_portfolio_performance_comparison(stock_name, agent):\n\tdates, buy_and_hold_portfolio_values, buy_and_hold_return = buy_and_hold_benchmark(stock_name, agent)\n\tagent_return = agent.portfolio_values[-1] - agent.initial_portfolio_value\n\tplt.figure(figsize=(15, 5), dpi=100)\n\tplt.title('{} vs. HODL'.format(agent.model_type))\n\tplt.plot(dates, agent.portfolio_values, color='green', label='{} \u7e3d\u6536\u76ca\uff1a ${:.2f}'.format(agent.model_type, agent_return))\n\tplt.plot(dates, buy_and_hold_portfolio_values, color='blue', label='{} HODL \u7e3d\u6536\u76ca\uff1a ${:.2f}'.format(stock_name, buy_and_hold_return))\n\t# compare with S&P 500 performance in 2018\n\tif '^GSPC' not in stock_name:\n\t\tdates, GSPC_buy_and_hold_portfolio_values, GSPC_buy_and_hold_return = buy_and_hold_benchmark('^GSPC_2018', agent)\n\t\tplt.plot(dates, GSPC_buy_and_hold_portfolio_values, color='red', label='S&P 500 2018 HODL \u7e3d\u6536\u76ca\uff1a ${:.2f}'.format(GSPC_buy_and_hold_return))\n\tplt.xticks(np.linspace(0, len(dates), 10))\n\tplt.ylabel('\u8cc7\u7522\u50f9\u503c ($)')\n\tplt.legend()\n\tplt.grid()\n\tplt.show()\n\n\ndef plot_all(stock_name, agent):\n    fig, ax = plt.subplots(2, 1, figsize=(16,8), dpi=100)\n\n    portfolio_return = agent.portfolio_values[-1] - agent.initial_portfolio_value\n    df = pd.read_csv('..\/output\/{}.csv'.format(stock_name))\n    buy_prices = [df.iloc[t, 4] for t in agent.buy_dates]\n    sell_prices = [df.iloc[t, 4] for t in agent.sell_dates]\n    ax[0].set_title('{} \u7e3d\u6536\u76ca {}: ${:.2f}'.format(agent.model_type, stock_name, portfolio_return))\n    ax[0].plot(df['Date'], df['Close'], color='black', label=stock_name)\n    ax[0].scatter(agent.buy_dates, buy_prices, c='green', alpha=0.5, label='buy')\n    ax[0].scatter(agent.sell_dates, sell_prices,c='red', alpha=0.5, label='sell')\n    ax[0].set_ylabel('Price')\n    ax[0].set_xticks(np.linspace(0, len(df), 10))\n    ax[0].legend()\n    ax[0].grid()\n\n    dates, buy_and_hold_portfolio_values, buy_and_hold_return = buy_and_hold_benchmark(stock_name, agent)\n    agent_return = agent.portfolio_values[-1] - agent.initial_portfolio_value\n    ax[1].set_title('{} vs. HODL'.format(agent.model_type))\n    ax[1].plot(dates, agent.portfolio_values, color='green', label='{} \u7e3d\u6536\u76ca\uff1a ${:.2f}'.format(agent.model_type, agent_return))\n    ax[1].plot(dates, buy_and_hold_portfolio_values, color='blue', label='{} HODL \u7e3d\u6536\u76ca\uff1a ${:.2f}'.format(stock_name, buy_and_hold_return))\n    # compare with S&P 500 performance in 2018 if stock is not S&P 500\n    if '^GSPC' not in stock_name:\n    \tdates, GSPC_buy_and_hold_portfolio_values, GSPC_buy_and_hold_return = buy_and_hold_benchmark('^GSPC_2018', agent)\n    \tax[1].plot(dates, GSPC_buy_and_hold_portfolio_values, color='red', label='S&P 500 2018 HODL \u7e3d\u6536\u76ca\uff1a ${:.2f}'.format(GSPC_buy_and_hold_return))\n    ax[1].set_ylabel('\u8cc7\u7522\u50f9\u503c ($)')\n    ax[1].set_xticks(np.linspace(0, len(df), 10))\n    ax[1].legend()\n    ax[1].grid()\n\n    plt.subplots_adjust(hspace=0.5)\n    plt.savefig('..\/output\/{}_trading_history.png'.format(stock_name))\n    plt.show()\n\n\ndef plot_portfolio_returns_across_epochs(model_name, returns_across_epochs):\n    len_epochs = len(returns_across_epochs)\n    plt.figure(figsize=(15, 5), dpi=100)\n    plt.title('\u6295\u8cc7\u7d44\u5408\u6536\u76ca')\n    plt.plot(returns_across_epochs, color='black')\n    plt.xlabel('Epoch')\n    plt.ylabel('Return Value')\n    plt.grid()\n    plt.savefig('..\/output\/{}_returns_ep{}.png'.format(model_name, len_epochs))\n    plt.show()\n","aae9ed5b":"class Agent(Portfolio):\n    def __init__(self, state_dim, balance, is_eval=False, model_name=\"\"):\n        super().__init__(balance=balance)\n        self.model_type = 'DQN'\n        self.state_dim = state_dim\n        self.action_dim = 3  # hold, buy, sell\n        self.memory = deque(maxlen=100)\n        self.buffer_size = 60\n\n        self.gamma = 0.95\n        self.epsilon = 1.0  # initial exploration rate\n        self.epsilon_min = 0.01  # minimum exploration rate\n        self.epsilon_decay = 0.995 # decrease exploration rate as the agent becomes good at trading\n        self.is_eval = is_eval\n        self.model = load_model('..\/{}.h5'.format(model_name)) if is_eval else self.model()\n\n        self.tensorboard = TensorBoard(log_dir='.\/logs\/DQN_tensorboard', update_freq=90)\n        self.tensorboard.set_model(self.model)\n\n    def model(self):\n        model = Sequential()\n        model.add(Dense(units=64, input_dim=self.state_dim, activation='relu'))\n        model.add(Dense(units=32, activation='relu'))\n        model.add(Dense(units=8, activation='relu'))\n        model.add(Dense(self.action_dim, activation='softmax'))\n        model.compile(loss='mse', optimizer=Adam(lr=0.01))\n        return model\n\n    def reset(self):\n        self.reset_portfolio()\n        self.epsilon = 1.0 # reset exploration rate\n\n    def remember(self, state, actions, reward, next_state, done):\n        self.memory.append((state, actions, reward, next_state, done))\n\n    def act(self, state):\n        if not self.is_eval and np.random.rand() <= self.epsilon:\n            return random.randrange(self.action_dim)\n        options = self.model.predict(state)\n        return np.argmax(options[0])\n\n    def experience_replay(self):\n        # retrieve recent buffer_size long memory\n        mini_batch = [self.memory[i] for i in range(len(self.memory) - self.buffer_size + 1, len(self.memory))]\n\n        for state, actions, reward, next_state, done in mini_batch:\n            if not done:\n                Q_target_value = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n            else:\n                Q_target_value = reward\n            next_actions = self.model.predict(state)\n            next_actions[0][np.argmax(actions)] = Q_target_value\n            history = self.model.fit(state, next_actions, epochs=1, verbose=0)\n\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n\n        return history.history['loss'][0]","f4be1dbc":"model_name = \"DQN\"\n# \u8cc7\u6599\u904e\u5927\u6703\u5931\u6e96\uff0c\u76e1\u91cf\u4ee5\u5e74\u5ea6\u3001\u5b63\u5ea6\u4f86\u62c6\u5206\u8cc7\u6599\u96c6\nstock_name = \"SPY_2018\"\nwindow_size = 10\nnum_epoch = 5\ninitial_balance = 50000","7842392d":"stock_prices = stock_close_prices(stock_name)\ntrading_period = len(stock_prices) - 1\nreturns_across_epochs = []\nnum_experience_replay = 0\naction_dict = {0: 'Hold', 1: 'Buy', 2: 'Sell'}","3cda8af5":"agent = Agent(state_dim=window_size + 3, balance=initial_balance)\n\ndef hold(actions):\n    # encourage selling for profit and liquidity\n    next_probable_action = np.argsort(actions)[1]\n    if next_probable_action == 2 and len(agent.inventory) > 0:\n        max_profit = stock_prices[t] - min(agent.inventory)\n        if max_profit > 0:\n            sell(t)\n            actions[next_probable_action] = 1 # reset this action's value to the highest\n            return 'HODL', actions\n\ndef buy(t):\n    if agent.balance > stock_prices[t]:\n        agent.balance -= stock_prices[t]\n        agent.inventory.append(stock_prices[t])\n        return '\u8cfc\u8cb7: ${:.2f}'.format(stock_prices[t])\n\ndef sell(t):\n    if len(agent.inventory) > 0:\n        agent.balance += stock_prices[t]\n        bought_price = agent.inventory.pop(0)\n        profit = stock_prices[t] - bought_price\n        global reward\n        reward = profit\n        return '\u8ce3\u51fa: ${:.2f} | \u7372\u5229\uff1a ${:.2f}'.format(stock_prices[t], profit)","5958c6fe":"print(f'\u4ea4\u6613\u5e02\u5834\uff1a     {stock_name}')\nprint(f'\u8cc7\u6599\u9577\u5ea6\uff1a     {trading_period} \u5929')\nprint(f'\u5747\u7dda\u7dad\u5ea6\uff1a     {window_size} \u5929')\nprint(f'\u8a13\u7df4\u6b21\u6578\uff1a     {num_epoch}')\nprint(f'\u4f7f\u7528\u6a21\u578b\uff1a     {model_name}')\nprint('\u521d\u59cb\u6301\u6709\u8cc7\u91d1\uff1a  ${:,}'.format(initial_balance))","2dc0658a":"start_time = time.time()\nfor e in range(1, num_epoch + 1):\n    print(f'\\n\u8a13\u7df4\u8fed\u4ee3\uff1a {e}\/{num_epoch}')\n\n    agent.reset() # reset to initial balance and hyperparameters\n    state = generate_combined_state(0, window_size, stock_prices, agent.balance, len(agent.inventory))\n\n    for t in range(1, trading_period + 1):\n        if t % 100 == 0:\n            print(f'\\n-------------------\u671f\u6578\uff1a {t}\/{trading_period}-------------------')\n\n        reward = 0\n        next_state = generate_combined_state(t, window_size, stock_prices, agent.balance, len(agent.inventory))\n        previous_portfolio_value = len(agent.inventory) * stock_prices[t] + agent.balance\n\n        actions = agent.model.predict(state)[0]\n        action = agent.act(state)\n        \n        # execute position\n        print('\u6279\u6b21\uff1a {}\\tHODL \u8a0a\u865f\uff1a {:.4} \\t\u8cb7\u5165 \u8a0a\u865f\uff1a {:.4} \\t\u8ce3\u51fa \u8a0a\u865f\uff1a {:.4}'.format(t, actions[0], actions[1], actions[2]))\n        if action != np.argmax(actions): print(f\"\\t\\t'{action_dict[action]}' is an exploration.\")\n        if action == 0: # hold\n            execution_result = hold(actions)\n        if action == 1: # buy\n            execution_result = buy(t)      \n        if action == 2: # sell\n            execution_result = sell(t)        \n        \n        # check execution result\n        if execution_result is None:\n            reward -= treasury_bond_daily_return_rate() * agent.balance  # missing opportunity\n        else:\n            if isinstance(execution_result, tuple): # if execution_result is 'Hold'\n                actions = execution_result[1]\n                execution_result = execution_result[0]   \n            print(execution_result)                \n\n        # calculate reward\n        current_portfolio_value = len(agent.inventory) * stock_prices[t] + agent.balance\n        unrealized_profit = current_portfolio_value - agent.initial_portfolio_value\n        reward += unrealized_profit\n\n        agent.portfolio_values.append(current_portfolio_value)\n        agent.return_rates.append((current_portfolio_value - previous_portfolio_value) \/ previous_portfolio_value)\n\n        done = True if t == trading_period else False\n        agent.remember(state, actions, reward, next_state, done)\n\n        # update state\n        state = next_state\n\n        # experience replay\n        if len(agent.memory) > agent.buffer_size:\n            num_experience_replay += 1\n            loss = agent.experience_replay()\n            print('\u8fed\u4ee3\uff1a {}\\t\u640d\u5931\uff1a {:.2f}\\t\u57f7\u884c\u52d5\u4f5c\uff1a {}\\t\u734e\u52f5\uff1a {:.2f}\\t\u73fe\u91d1\u9918\u984d\uff1a {:.2f}\\t\u6301\u6709\u80a1\u6578\uff1a {}'.format(e, loss, action_dict[action], reward, agent.balance, len(agent.inventory)))\n            agent.tensorboard.on_batch_end(num_experience_replay, {'loss': loss, 'portfolio value': current_portfolio_value})\n\n        if done:\n            portfolio_return = evaluate_portfolio_performance(agent)\n            returns_across_epochs.append(portfolio_return)\n\n    # save models periodically\n    if e % 5 == 0:\n        agent.model.save('..\/output\/saved_models\/DQN_ep' + str(e) + '.h5')\n        print('model saved')\n\nprint('\u7e3d\u8a13\u7df4\u6642\u9593\uff1a {0:.2f} \u5206\u9418'.format((time.time() - start_time)\/60))\nplot_portfolio_returns_across_epochs(model_name, returns_across_epochs)","678ae932":"### \u5b89\u88dd\u76f8\u4f9d\u5957\u4ef6","8ee209d5":"### \u6240\u6709\u6703\u7528\u5230\u7684\u5de5\u5177\uff0c\u4e4b\u5f8c\u89e3\u91cb","f5862cb4":"## \u4f7f\u7528 Tensorflow \u8a13\u7df4\u4ea4\u6613\u54e1\n\n\u672c\u7bc7\u7684\u539f\u7406\u662f\u900f\u904e\u65e2\u6709\u5e02\u5834\u8cc7\u6599\u8a13\u7df4\u51fa Agent \u57f7\u884c\u5c0d\u61c9\u7684\u7b56\u7565\u4e26\u505a\u5230\u6700\u4f73\u512a\u5316\uff0c\u8a13\u7df4\u5b8c\u6210\u7684\u6a21\u578b\u5c07\u53ef\u7528\u65bc\u4ea4\u6613\u8a55\u4f30\uff0c\u5fc5\u9808\u6ce8\u610f\u7684\u662f\u5e02\u5834\u4e2d\u5145\u6eff\u9ed1\u5929\u9d5d\uff0c\u4e0d\u540c\u5e02\u5834\u9593\u7121\u6cd5\u901a\u7528\uff0c\u6bd4\u5982\uff1a\u7576\u4f60\u4f7f\u7528 SPY \u8a13\u7df4\u7684 Agent \u7121\u6cd5\u61c9\u7528\u5728 FB\u3001AMZN\uff0c\u53cd\u4e4b\u4ea6\u6b64\uff0c\u5927\u5bb6\u53ef\u4ee5\u4e0b\u8f09\u5df2\u7d93\u8a13\u7df4\u597d\u7684 SPY Agent(DQN_ep10.h5) \u53bb\u57f7\u884c\n\n> \u76ee\u524d\u672c\u7b97\u6cd5\u53ea\u652f\u6301 [\u8cb7\u5165\u3001\u8ce3\u51fa\u3001\u6301\u5e73] \u4fe1\u865f\uff0c\u53ef\u4ee5 [\u505a\u591along\u3001\u505a\u7a7ashort] \u4ee5\u53ca\u6574\u5408 backtrade \u7684\u7b97\u6cd5\u9084\u9700\u8981\u4e00\u6bb5\u6642\u9593\n>\n> \u672c\u6b21\u7b97\u6cd5\u53c3\u8003\u4e86 [TA \u6307\u6a19](https:\/\/github.com\/bukosabino\/ta)\u3001[Agent](https:\/\/github.com\/Albert-Z-Guo\/Deep-Reinforcement-Stock-Trading\/blob\/master\/train.py)"}}