{"cell_type":{"dc94621d":"code","ce3b90d6":"code","8723eeed":"code","3386c257":"code","caa8de51":"code","d9f7394e":"code","48408dce":"code","c914f105":"code","c9093520":"code","a2c72e87":"code","0ceee8d2":"code","b926b409":"code","30f352bc":"code","9b3367c4":"code","a0fbf251":"code","b3f55b7f":"code","817210a9":"code","5c1721a7":"code","301e0acc":"code","f8e3ee42":"code","306467c0":"code","bfe770e4":"code","153ef783":"code","c863c3a0":"code","46a4821c":"code","75eef318":"code","aaac3d9b":"code","4513648a":"code","0a3cc54d":"code","a035947c":"code","5df3c83f":"code","778460c5":"code","a28b3bd4":"code","6960e14b":"code","4b7e4888":"code","e5218ff9":"code","d183cba7":"code","6448467f":"code","b925b8c6":"code","d7bc9d67":"code","1dbbc08d":"code","cfe0bdbd":"code","4a22134a":"code","2255b8fb":"code","8236aebd":"code","ba767f71":"code","895e2a3e":"code","461e8870":"markdown","6adab28a":"markdown","e063d3a1":"markdown","264975c5":"markdown","70d93f18":"markdown","23048b84":"markdown","9020f0eb":"markdown","cbd4a6f1":"markdown","f4df3a34":"markdown","e1c9b5f4":"markdown","02298f3b":"markdown","4de826f7":"markdown","62ffde4c":"markdown","0a26456c":"markdown","68a91d6a":"markdown","63b3b44b":"markdown","1d0f23b5":"markdown","f13a0133":"markdown","4c9283b3":"markdown","57a39687":"markdown","c85f2fa7":"markdown","3a9b3fc4":"markdown","0472cb7c":"markdown","d775456a":"markdown","e3a9f113":"markdown","784aeb2f":"markdown","06c26659":"markdown","3407e81a":"markdown","fcce77fb":"markdown"},"source":{"dc94621d":"import numpy as np\nimport tensorflow as tf\nimport random as rn\n\nimport os\nos.environ['PYTHONHASHSEED'] = \"0\"\n\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n                              inter_op_parallelism_threads=1)\n\nfrom keras import backend as K\n\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\nK.set_session(sess)","ce3b90d6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nimport pandas as pd\nimport seaborn as sb\nimport matplotlib.pyplot as plt\n\nfrom scipy import stats\nfrom pandas.api.types import CategoricalDtype\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.optimizers import Adam\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import OneHotEncoder\n\npd.set_option(\"display.max_columns\", 100)\npd.set_option(\"mode.chained_assignment\", None)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nprint(os.listdir(\"..\/input\"))","8723eeed":"df_train = pd.read_csv('..\/input\/train.csv')\nprint('Shape:', df_train.shape)\ndf_train.head(5)","3386c257":"target_column = 'SalePrice'","caa8de51":"def find_columns_with_missing_value(df):\n    missing_value_count = df.isnull().sum()\n    missing_value_percentage = missing_value_count \/ df.isnull().count()\n    missing_value_type = df.dtypes\n    missing_value = pd.DataFrame({\n        \"Total\": missing_value_count,\n        \"Percentage\": missing_value_percentage,\n        \"Type\": missing_value_type\n    })\n    return missing_value[missing_value['Total'] > 0].sort_values(by=\"Total\",ascending=False)","d9f7394e":"find_columns_with_missing_value(df_train)","48408dce":"def find_numeric_columns(df):\n    numeric_types = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    numeric_df = df.select_dtypes(include=numeric_types)\n    return numeric_df","c914f105":"def find_non_numeric_columns(df):\n    non_numeric_types = ['object']\n    non_numeric_df = df.select_dtypes(include=non_numeric_types)\n    return non_numeric_df","c9093520":"numeric_df_train = find_numeric_columns(df_train)\nnumeric_df_train = numeric_df_train.drop([target_column, 'Id'], axis=1)\n\nnon_numeric_df_train = find_non_numeric_columns(df_train)\n\nprint(\"Numeric columns:\", numeric_df_train.columns)\nprint(\"Non-numeric columns:\", non_numeric_df_train.columns)","a2c72e87":"def series_has_null_value(series):\n    return series.isnull().values.any()","0ceee8d2":"def replace_nan_with_zero(df):\n    for col in df.columns:\n        if series_has_null_value(df[col]):\n            df[col] = df[col].fillna(0)\n            \n    return df","b926b409":"numeric_df_train = replace_nan_with_zero(numeric_df_train)\nnumeric_df_train.head(5)","30f352bc":"def one_hot_encode(df, categories_dict={}):\n    missing_value_representation = 'None'\n    \n    # iteratively encode each column\n    for col in df.columns:\n        # replace missing value if any\n        if series_has_null_value(df[col]):\n            df[col] = df[col].fillna(missing_value_representation)\n        \n        # get numpy array from the series\n        X = df[col].values\n        # transform the array shape to satisfy sklearn OneHotEncoder format\n        X = X.reshape(-1,1)\n        \n        # get unique values for its column as categories\n        if col not in categories_dict:\n            unique_value = df[col].unique()\n            unique_value.sort()\n            categories_dict[col] = unique_value\n        \n        # create one hot encoder and feed our data\n        encoder = OneHotEncoder(categories=[categories_dict[col]], handle_unknown='ignore')\n        encoder.fit(X)\n        \n        # encode the value\n        encoded_data = encoder.transform(X).toarray()\n        \n        # get the columns name, it will produce column name with format \"x0_value\"\n        encoded_columns_name = encoder.get_feature_names()\n        # remove x0 and replace it with actual column name and remove column representing missing value\n        encoded_columns_name = [name.replace(\"x0\", col) \n                                for name in encoded_columns_name]\n        \n        # transform the encoded value to DataFrame format\n        encoded_df = pd.DataFrame(encoded_data, columns=encoded_columns_name)\n        \n        # omit missing value column\n        filtered_columns = [name for name in encoded_columns_name \n                            if name[name.find(\"_\")+1:] != missing_value_representation]\n        encoded_df = encoded_df[filtered_columns]\n        \n        # combine it to our result DataFrame\n        df = pd.concat([df, encoded_df], axis=1)\n        # remove the original column\n        df.drop([col], axis=1, inplace=True)\n    \n    return df, categories_dict","9b3367c4":"non_numeric_df_train, categories_dict = one_hot_encode(non_numeric_df_train)\nnon_numeric_df_train.head(5)","a0fbf251":"def get_input_and_target(numeric_df_train, non_numeric_df_train):\n    input_train = pd.concat([numeric_df_train, non_numeric_df_train], axis=1)\n    target_train = df_train[target_column]\n\n    X = input_train.values\n    Y = target_train.values\n    \n    return X, Y","b3f55b7f":"X, Y = get_input_and_target(numeric_df_train, non_numeric_df_train)","817210a9":"def build_model(input_shape, lr):\n    NN_classifier = Sequential()\n\n    NN_classifier.add(Dense(128, kernel_initializer='normal', activation='relu', input_dim=input_shape))\n    NN_classifier.add(Dense(256, kernel_initializer='normal', activation='relu'))\n    NN_classifier.add(Dense(256, kernel_initializer='normal', activation='relu'))\n    NN_classifier.add(Dense(256, kernel_initializer='normal', activation='relu'))\n\n    # The Output Layer :\n    NN_classifier.add(Dense(1, kernel_initializer='normal', activation='linear'))\n    \n    optimizer = Adam(lr=lr)\n    NN_classifier.compile(loss='mean_absolute_error', optimizer=optimizer)\n    \n    NN_classifier.summary()\n    \n    return NN_classifier","5c1721a7":"def cross_validation_score(X, Y, lr, epochs, batch_size, validation_portion, train_history_callback, n_fold=5):\n    np.random.seed(1234)\n    rn.seed(1234)\n    tf.set_random_seed(1234)\n    \n    kf = KFold(n_splits=n_fold)\n\n    results = []\n    histories = []\n    for train, test in kf.split(X, Y):\n        X_train, Y_train = X[train], Y[train]\n        X_test, Y_test = X[test], Y[test]\n\n        # Build classification model\n        model = build_model(X.shape[1], lr)\n        \n        # use ModelCheckpoint to automatically save the best model's weight into hdf5 file\n        model_path = \"model.hdf5\"\n        checkpoint = ModelCheckpoint(model_path, monitor='val_loss', save_best_only = True)\n        callbacks_list = [checkpoint]\n        \n        # train model\n        history = model.fit(\n            X_train, \n            Y_train, \n            epochs=epochs, \n            batch_size=batch_size, \n            validation_split=validation_portion, \n            callbacks=callbacks_list)\n        \n        # load the best model's weight\n        model.load_weights(model_path)\n        \n        result = model.evaluate(X_test, Y_test)\n        results.append(result)\n        histories.append(history)\n        \n    for index, history in enumerate(histories):\n        train_history_callback(history, index)\n        \n    results = np.array(results)\n    return results.mean()","301e0acc":"def train_history_plot(history, index=0):\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss - cross validation {0}'.format(index+1))\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'validation'], loc='upper left')\n    plt.show()","f8e3ee42":"def measure_model_performance(X, Y, lr=0.001, epochs=100, batch_size=32, validation_proportion=0.2):\n    score = cross_validation_score(X, Y, \n                                   lr=lr,\n                                   epochs=epochs, \n                                   batch_size=batch_size, \n                                   validation_portion=validation_proportion,\n                                   train_history_callback=train_history_plot)\n\n    print(\"Mean absolute error:\", score)","306467c0":"measure_model_performance(X, Y)","bfe770e4":"def plot_correlation(corr):\n    fig = plt.figure(figsize = (15,15))\n\n    sb.heatmap(corr, square = True)\n    plt.show()","153ef783":"numeric_df_train = find_numeric_columns(df_train)\nnumeric_df_train_corr = numeric_df_train.corr()\n\nplot_correlation(numeric_df_train_corr)","c863c3a0":"def filter_numeric_columns_by_correlation(corr, threshold):\n    indices = corr[target_column].map(lambda x: abs(x) >= threshold)\n    columns = corr[target_column][indices].index\n    columns = [x for x in columns if x != target_column]\n    return columns","46a4821c":"# these columns are excluded because these columns decreasing the model performance\nexcluded_columns = ['GarageYrBlt','2ndFlrSF','OpenPorchSF']\n\nnumeric_df_train_columns = filter_numeric_columns_by_correlation(numeric_df_train_corr, threshold=0.3)\nnumeric_df_train_columns = [col for col in numeric_df_train_columns if col not in excluded_columns]\n\nnumeric_df_train = numeric_df_train[numeric_df_train_columns]\nnumeric_df_train = replace_nan_with_zero(numeric_df_train)\n\nprint(numeric_df_train_columns)","75eef318":"X, Y = get_input_and_target(numeric_df_train, non_numeric_df_train)\nmeasure_model_performance(X, Y)","aaac3d9b":"numeric_df_train['BuildingAge'] = df_train['YrSold'] - df_train['YearRemodAdd']\nnumeric_df_train.drop('YearBuilt', axis=1, inplace=True)\n\nprint(numeric_df_train.columns)","4513648a":"X, Y = get_input_and_target(numeric_df_train, non_numeric_df_train)\nmeasure_model_performance(X, Y)","0a3cc54d":"def mean_normalization(df, normalization_dict={}):\n    for col in df.columns:\n        if col not in normalization_dict:\n            normalization_dict[col] = {\n                'min' : df[col].min(),\n                'max' : df[col].max(),\n                'mean' : df[col].mean()\n            }\n            \n        min_value = normalization_dict[col]['min']\n        max_value = normalization_dict[col]['max']\n        mean_value = normalization_dict[col]['mean']\n        df[col] = (df[col] - mean_value) \/ (max_value - min_value)\n    \n    return df, normalization_dict","a035947c":"numeric_df_train, normalization_dict = mean_normalization(numeric_df_train)\n        \nnumeric_df_train.head(5)","5df3c83f":"X, Y = get_input_and_target(numeric_df_train, non_numeric_df_train)\nmeasure_model_performance(X, Y)","778460c5":"def ANOVA_test(df, target_column, alpha_level=0.05):\n    columns = []\n    for col in df.columns:\n        if col != target_column:\n            values = df[col][df[col].isnull() == False].unique()\n            groups = [df[target_column][df[col] == group] for group in values]\n\n            df_between = len(values) - 1\n            df_within = sum([len(members) for members in groups]) - len(values)\n            f_crit = stats.f.ppf(q=1-alpha_level, dfn=df_between, dfd=df_within)\n\n            f_stat, p_value = stats.f_oneway(*groups)\n            if p_value <= alpha_level:\n                columns.append(col)\n            \n    return columns","a28b3bd4":"# These columns are excluded because contains a lot of missing values\nexcluded_columns = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'Utilities']\n\nnon_numeric_df_train = find_non_numeric_columns(df_train)\n\nnon_numeric_df_train_columns = non_numeric_df_train.columns\nnon_numeric_df_train_columns = [col for col in non_numeric_df_train_columns if col not in excluded_columns]\n\nnon_numeric_df_train = non_numeric_df_train[non_numeric_df_train_columns]\nnon_numeric_df_train = pd.concat([non_numeric_df_train, df_train[target_column]], axis=1)\n\nnon_numeric_df_train_columns = ANOVA_test(non_numeric_df_train, target_column, alpha_level=0.01)\n\nprint(non_numeric_df_train_columns)","6960e14b":"non_numeric_df_train = non_numeric_df_train[non_numeric_df_train_columns]\nnon_numeric_df_train, categories_dict = one_hot_encode(non_numeric_df_train)\n\nnon_numeric_df_train.head(5)","4b7e4888":"X, Y = get_input_and_target(numeric_df_train, non_numeric_df_train)\nmeasure_model_performance(X, Y)","e5218ff9":"def build_model(input_shape, lr):\n    NN_classifier = Sequential()\n\n    print(\"new build model function\")\n    NN_classifier.add(Dropout(0.2, seed=1234, input_shape=(input_shape,)))\n    NN_classifier.add(Dense(128, kernel_initializer='normal', activation='relu'))\n    NN_classifier.add(Dense(256, kernel_initializer='normal', activation='relu'))\n    NN_classifier.add(Dense(256, kernel_initializer='normal', activation='relu'))\n    NN_classifier.add(Dense(256, kernel_initializer='normal', activation='relu'))\n\n    # The Output Layer :\n    NN_classifier.add(Dense(1, kernel_initializer='normal', activation='linear'))\n    \n    optimizer = Adam(lr=lr)\n    NN_classifier.compile(loss='mean_absolute_error', optimizer=optimizer)\n    \n    NN_classifier.summary()\n    \n    return NN_classifier","d183cba7":"X, Y = get_input_and_target(numeric_df_train, non_numeric_df_train)\nmeasure_model_performance(X, Y, epochs=500, lr=0.0002)","6448467f":"np.random.seed(1234)\nrn.seed(1234)\ntf.set_random_seed(1234)\n\nX, Y = get_input_and_target(numeric_df_train, non_numeric_df_train)\nmodel = build_model(X.shape[1], lr=0.0002)\n\n# use ModelCheckpoint to automatically save the best model's weight into hdf5 file\nmodel_path = \"model.hdf5\"\ncheckpoint = ModelCheckpoint(model_path, monitor='val_loss', save_best_only = True)\ncallbacks_list = [checkpoint]\n\n# train model\nhistory = model.fit(\n    X, Y, \n    epochs=500, \n    batch_size=32, \n    validation_split=0.1, \n    callbacks=callbacks_list)\n\n# load the best model's weight\nmodel.load_weights(model_path)\n\ntrain_history_plot(history)","b925b8c6":"df_test = pd.read_csv('..\/input\/test.csv')\nprint(df_test.shape)\ndf_test.head(5)","d7bc9d67":"df_test['BuildingAge'] = df_test['YrSold'] - df_test['YearRemodAdd']","1dbbc08d":"numeric_columns = numeric_df_train.columns\nnon_numeric_columns = non_numeric_df_train_columns\n\nnumeric_df_test = df_test[numeric_columns]\nnon_numeric_df_test = df_test[non_numeric_columns]\n\nprint(\"Numeric columns:\", numeric_columns)\nprint(\"Non-numeric columns:\", non_numeric_columns)","cfe0bdbd":"find_columns_with_missing_value(numeric_df_test)","4a22134a":"find_columns_with_missing_value(non_numeric_df_test)","2255b8fb":"numeric_df_test = replace_nan_with_zero(numeric_df_test)\nnumeric_df_test, _ = mean_normalization(numeric_df_test, normalization_dict)\n\nnumeric_df_train.head(5)","8236aebd":"non_numeric_df_test, _ = one_hot_encode(non_numeric_df_test, categories_dict)\n\nnon_numeric_df_test.head(5)","ba767f71":"X_test = pd.concat([numeric_df_test, non_numeric_df_test], axis=1).values\n\npredicted = model.predict(X_test)\nprint(predicted.shape)","895e2a3e":"submission = pd.DataFrame({'Id':df_test.Id,'SalePrice':predicted[:,0]})\nsubmission.to_csv('{}.csv'.format(\"submission\"),index=False)\nprint('A submission file has been made')","461e8870":"## Prepare model for prediction\nOnce we have determine the best features, model architecture, and parameter, our next step is train the model for making prediction. In this step, we don't need to split the dataset into train and test. We need to feed more training data to give more knowledge to our model, so we will split the dataset with proportion **90% training** and **10% validation**.","6adab28a":"### Looking for numeric and non-numeric columns\nWe want to see which columns are numeric and which columns are non-numeric. Because we need to treat each type differently.","e063d3a1":"### Load test data\nWe will load the test data that provided in **csv** file using pandas.","264975c5":"### Model performance\nAfter we add the **dropout layer** and change some parameters, our model achieve **mean absolute error around 16150 (\u00b1 70)** and the learning process is more stable.","70d93f18":"### Set the target column name\nWhat is target column? This column is the one that we are trying to predict.","23048b84":"## What's next?\nBefore we doing any step to omit unused columns, it's better to train the model using all columns first and see the model performance. Why? we need to find baseline performance to compare, to make sure our next action didn't make it worse. Before we train the data into our model, we need to do some little things to make it works.","9020f0eb":"### Pre-processing\nWe need to pre-process the test data with the same step as in training process. For numeric columns, we will replace the **nan** values with **zero** and do **mean normalization**. For non-numeric columns, we will do **one hot encoding** for each column.","cbd4a6f1":"### Modeling\nLet's build our neural network model! Please keep in mind that the architecture (number of hidden layer, number of neuron, etc) and parameter (learning rate, batch size, etc) that I used for our model may not the best. You can freely design your neural network model that works best for your data based on experiments.","f4df3a34":"### Read training data\nWe need to load the training data from **csv** file and see what's inside. If you want to download the file, you can find it on the right panel. \n\nIn this tutorial, we used pandas to load data from **csv** file, the loaded data will have **DataFrame** type. If you are not familiar with pandas, you can learn about pandas [here](https:\/\/www.kaggle.com\/learn\/pandas).","e1c9b5f4":"### Adding new column\nBecause of we added **BuildingAge** column in train data, we also need to add that column in test data","02298f3b":"### Preprocess numeric columns\nPreviously, we have spot the missing values and some of them has numeric type. Before we use these numeric columns, we need to replace the missing values first, because the neural network cannot process missing values (*imagine how can you do arithmatic operation between a number and null*). Because of that, we will replace the missing values to 0 (zero). Why zero? Because we want to feed our data without any modification for our initial model. Zero value in neural network will be \"ignored\" because of everything that multiplied by zero will produce zero too.","4de826f7":"![Model Loss](https:\/\/i.imgur.com\/96HBKyF.png)","62ffde4c":"## Model enhancement\nYou are still able to improve your model performance, this can be done by experiments (choosing best number of hidden layers, neurons, and parameters). First thing that i tried is increase the number of epoch. But after looking at training history, at certain iteration the validation loss begins to rise while the training loss keep decreasing (I think it's overfitting). So i change my network architecture by adding **dropout layer** with drop fraction 0.2 (keep 0.8) and reducing the **learning rate** to 0.0002. You can read about **Dropout** [here](https:\/\/medium.com\/@amarbudhiraja\/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5). ","0a26456c":"### Model performance\nOur model now achieve **mean absolute error: 19224** after we eliminate some columns based on their correlation to the target column, let's keep improving.","68a91d6a":"## Overview\nIn this dataset, we are asked to create a model that able to predict the house sale price as best as we can from given data fields. From that task, we can clearly see that this is a regression problem because the target that we are trying to predict has continuous value. \n\nIn this tutorial, we going to use neural network as our model.\n\n## Let's Start!\nBefore start, we need import all packages that we need. We also need to set the random seed to get reproducible result. Because of neural network initialize it's weight randomly, setting the random seed will help us to get same result.","63b3b44b":"## Improving model performance\nTo make our model become smarter (has lower **mean absolute error**), we need to eliminate some features that less important or not contribute to predict the target value, adding useful features, feature preprocessing, etc. We will divide it into 2 parts, one for numeric columns and one for non-numeric columns.\n\n### Feature selection for numeric columns\nFor numeric columns, we will measure their importance using [pearson correlation coefficient](https:\/\/en.wikipedia.org\/wiki\/Pearson_correlation_coefficient). We will eliminate columns which has correlation score less than **threshold** value that we set. We want to measure correlation for each column without changing its distributions, so we should not replace the **nan** value to **zero**, because it can affect the correlation score. Note that the **threshold** value that we used here is determined based on my experiments.","1d0f23b5":"### Feature scaling\nWe will scale the input variables before passed into our neural network, in this case I used **mean normalization**. There are other normalization methods, like **min-max scaling**. But after tried both methods, **mean normalization** give better performance. Feature scaling can speed up **gradient descent** by having each of input values in roughly the same range. You can read about **feature scaling** [here](https:\/\/medium.com\/greyatom\/why-how-and-when-to-scale-your-features-4b30ab09db5e). Note that in our code below, we also return the normalization dict (consist of min, max, and mean value for each column), because of we need it to normalize the future test dataset.","f13a0133":"### Find out columns which have missing values\nWe need to know which columns that have missing values and deal with it later. Note that **Total** represents number of missing values and **Percentage** represents proportion of missing values. ","4c9283b3":"### Feature selection for non-numeric columns\nTo measuring correlation for non-numeric (categorical) columns, we cannot use **pearson correlation**, because it require both variables to have continuous value. So we will apply **ANOVA (Analysis of Variance)** test. **ANOVA (Analysis of variance)** is a statistical technique that is used to check if the means of two or more groups are significantly different from each other. You can read about **ANOVA**","57a39687":"## Predict test data\nFrom the given the test set, our goal is to predict the unlabeled data and submit it to the competition to get score.\n","c85f2fa7":"### Model performance\nAfter applying feature scaling, our model achieved **mean absolute error: 17466**. Keep improving!","3a9b3fc4":"## Feature engineering\nFeature engineering, also known as feature creation, is the process of constructing new features from existing data to train a machine learning model.\n\n### Adding new feature\nNew feature that i think might help for prediction is **BuildingAge**. We can subtract the **YrSold** by **YearRemodAdd** to calculate **BuildingAge**. I have tested to subtract **YrSold** by **YearBuilt**, but the better performance achieved by using **YearRemodAdd**. My assumption is because when the house has been remodeled, it's counted as a new house.","0472cb7c":"### Model performance\nOur model now achieve **mean absolute error: 16900** after we eliminate some columns based on ANOVA test.","d775456a":"# House Prices: Advanced Regression Techniques\nhttps:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data\n\n## File descriptions\n* **train.csv** - the training set\n* **test.csv** - the test set\n* **data_description.txt** - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here\n* **sample_submission.csv** - a benchmark submission from a linear regression on year and month of sale, lot square footage, and number of bedrooms\n\n## Data fields\nHere's a brief version of what you'll find in the data description file.\n\n* **SalePrice** - the property's sale price in dollars. This is the target variable that you're trying to predict.\n* **MSSubClass**: The building class\n* **MSZoning**: The general zoning classification\n* **LotFrontage**: Linear feet of street connected to property\n* **LotArea**: Lot size in square feet\n* **Street**: Type of road access\n* **Alley**: Type of alley access\n* **LotShape**: General shape of property\n* **LandContour**: Flatness of the property\n* **Utilities**: Type of utilities available\n* **LotConfig**: Lot configuration\n* **LandSlope**: Slope of property\n* **Neighborhood**: Physical locations within Ames city limits\n* **Condition1**: Proximity to main road or railroad\n* **Condition2**: Proximity to main road or railroad (if a second is present)\n* **BldgType**: Type of dwelling\n* **HouseStyle**: Style of dwelling\n* **OverallQual**: Overall material and finish quality\n* **OverallCond**: Overall condition rating\n* **YearBuilt**: Original construction date\n* **YearRemodAdd**: Remodel date\n* **RoofStyle**: Type of roof\n* **RoofMatl**: Roof material\n* **Exterior1st**: Exterior covering on house\n* **Exterior2nd**: Exterior covering on house (if more than one material)\n* **MasVnrType**: Masonry veneer type\n* **MasVnrArea**: Masonry veneer area in square feet\n* **ExterQual**: Exterior material quality\n* **ExterCond**: Present condition of the material on the exterior\n* **Foundation**: Type of foundation\n* **BsmtQual**: Height of the basement\n* **BsmtCond**: General condition of the basement\n* **BsmtExposure**: Walkout or garden level basement walls\n* **BsmtFinType1**: Quality of basement finished area\n* **BsmtFinSF1**: Type 1 finished square feet\n* **BsmtFinType2**: Quality of second finished area (if present)\n* **BsmtFinSF2**: Type 2 finished square feet\n* **BsmtUnfSF**: Unfinished square feet of basement area\n* **TotalBsmtSF**: Total square feet of basement area\n* **Heating**: Type of heating\n* **HeatingQC**: Heating quality and condition\n* **CentralAir**: Central air conditioning\n* **Electrical**: Electrical system\n* **1stFlrSF**: First Floor square feet\n* **2ndFlrSF**: Second floor square feet\n* **LowQualFinSF**: Low quality finished square feet (all floors)\n* **GrLivArea**: Above grade (ground) living area square feet\n* **BsmtFullBath**: Basement full bathrooms\n* **BsmtHalfBath**: Basement half bathrooms\n* **FullBath**: Full bathrooms above grade\n* **HalfBath**: Half baths above grade\n* **Bedroom**: Number of bedrooms above basement level\n* **Kitchen**: Number of kitchens\n* **KitchenQual**: Kitchen quality\n* **TotRmsAbvGrd**: Total rooms above grade (does not include bathrooms)\n* **Functional**: Home functionality rating\n* **Fireplaces**: Number of fireplaces\n* **FireplaceQu**: Fireplace quality\n* **GarageType**: Garage location\n* **GarageYrBlt**: Year garage was built\n* **GarageFinish**: Interior finish of the garage\n* **GarageCars**: Size of garage in car capacity\n* **GarageArea**: Size of garage in square feet\n* **GarageQual**: Garage quality\n* **GarageCond**: Garage condition\n* **PavedDrive**: Paved driveway\n* **WoodDeckSF**: Wood deck area in square feet\n* **OpenPorchSF**: Open porch area in square feet\n* **EnclosedPorch**: Enclosed porch area in square feet\n* **3SsnPorch**: Three season porch area in square feet\n* **ScreenPorch**: Screen porch area in square feet\n* **PoolArea**: Pool area in square feet\n* **PoolQC**: Pool quality\n* **Fence**: Fence quality\n* **MiscFeature**: Miscellaneous feature not covered in other categories\n* **MiscVal**: $Value of miscellaneous feature\n* **MoSold**: Month Sold\n* **YrSold**: Year Sold\n* **SaleType**: Type of sale\n* **SaleCondition**: Condition of sale\n","e3a9f113":"### Model performance\nAfter adding **BuildingAge** feature, our model achieved **mean absolute error : 18977**. You are still able to find other useful features to improve the performance.","784aeb2f":"### Prepare the input columns (X) and target column (Y)\nAfter numeric and non-numeric columns has been preprocessed, we need to merge them as input columns.","06c26659":"### Model performance\nOur model achieve **mean absolute error: 22474**, keep this score as our baseline performance and keep improving it.","3407e81a":"### Preprocess non-numeric columns\nWe need to transform our non-numeric (categorical) columns to numeric columns. Why? Because of neural network model require input variables to have numeric type. In order to do that, we will use **one hot encoding** to transform our categorical columns to numeric representation. If you never heard about **one hot encoding**, you can read about it [here](http:\/\/https:\/\/hackernoon.com\/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f). Note that in our code below, we also return the categories dict (consist of unique values for each column), because of we need it to encode the future test dataset.","fcce77fb":"### Measuring model performance\nIn this tutorial, we will use **Mean Absolute Error** to measure the model performance. It's simply taking mean of absolute difference between predicted value and actual value. To ensure that our model does not overfit, we will use 5-fold cross validation score. We also plot the training and validation loss during training to monitor our model training process."}}