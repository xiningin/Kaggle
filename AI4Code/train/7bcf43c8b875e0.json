{"cell_type":{"9fd8a577":"code","3681a4dd":"code","ef167b3a":"code","9dff1320":"code","980b54a9":"code","b91b0349":"code","58d867c6":"code","57c41c8f":"code","6901651a":"code","c2e6ae47":"code","a929a896":"code","085f1e8c":"code","3bf87458":"code","8978fb8b":"code","f329d0bf":"code","0c90b82c":"code","312c162a":"code","6b5c5b6c":"code","9dc965d3":"code","779c1a31":"code","fe28959f":"code","687aa2f3":"code","eef2d988":"code","de484025":"code","d6d033ee":"code","97eafd21":"code","059e6920":"code","de24aa7d":"code","3bc3bc1c":"code","31bc9345":"code","a0d489aa":"code","d6b18ecb":"code","af23690f":"code","6678c2f9":"code","7698b293":"code","f7f293b4":"code","b19602ed":"code","a2dcd162":"code","7204114a":"code","197c4303":"code","e22ed09f":"code","40ec4741":"code","446631ba":"code","f1caf91a":"code","b9995e8e":"code","5349d0e4":"code","99884be4":"code","b1657ad6":"code","59f5763e":"code","8ec9b549":"code","43b468cf":"code","c8f8256d":"code","e2ddb127":"code","c4a7b26a":"code","64e40aa2":"code","1e43aab9":"code","1248ca2f":"code","5d1a05d3":"code","e3682197":"markdown","ee5cb743":"markdown","28e6991b":"markdown","aef3fd7f":"markdown","e0e0d7f0":"markdown","e4bbebc9":"markdown","6eb2f496":"markdown","52603407":"markdown","b2a0677c":"markdown","65200ab7":"markdown","02e6a14c":"markdown","01f65a10":"markdown","1127e1de":"markdown","4ba953a5":"markdown","977f729b":"markdown","d42121a0":"markdown","5121be7d":"markdown","000ba88e":"markdown","b1a4a68d":"markdown","939d8eb7":"markdown","41e9ca11":"markdown","90088fd4":"markdown","75a8ae05":"markdown","24db4d6e":"markdown","a3322794":"markdown","19ad02d3":"markdown","bc8a079d":"markdown","f263a7ae":"markdown","39d07106":"markdown","c53edb11":"markdown","e03553a9":"markdown","abc84ed2":"markdown","429752c0":"markdown","5d2a4d02":"markdown","99f78046":"markdown","2cfbec0c":"markdown","5b3c3f34":"markdown","88271444":"markdown","14cda6fc":"markdown","6d75d7cf":"markdown","4a10e654":"markdown","fb35d0c4":"markdown","8e5456f7":"markdown","41b6d6b0":"markdown","4fa6db61":"markdown","48a78bba":"markdown","1d6a20ab":"markdown","b3fc6a27":"markdown","74029b53":"markdown","6763ceff":"markdown","20485ac1":"markdown","0c0a0b76":"markdown","6005a162":"markdown","771ff405":"markdown","05aa103a":"markdown","bd5fe893":"markdown","5179f627":"markdown","7de9fc7e":"markdown","ebc8f1ad":"markdown","100465da":"markdown","42573931":"markdown","043efe41":"markdown","65f91d02":"markdown","c954dea2":"markdown"},"source":{"9fd8a577":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\n\n# data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Machine Learning Algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n","3681a4dd":"#Upload CSV files\nfrom google.colab import files\nuploaded = files.upload()","ef167b3a":"train_df = pd.read_csv(\"train.csv\")\ntest_df = pd.read_csv(\"test.csv\")","9dff1320":"print(\"Dimensions of train: {}\".format(train_df.shape))\nprint(\"Dimensions of test: {}\".format(test_df.shape))","980b54a9":"#Data Exploration\ntrain_df.info()","b91b0349":"train_df.describe(include='all')","58d867c6":"train_df.head(10)","57c41c8f":"#From train dataset it is observed that it contains different types of data such as numeric data, categorical, continous, missing\/NaN values, which need to be taken care of\ntrain_df.columns.values ","6901651a":"sns.set(style=\"darkgrid\")\nplt.figure(figsize = (8, 5))\nax= sns.countplot(x='Survived', hue=\"Survived\", data=train_df)\n","c2e6ae47":"plt.figure(figsize = (8, 5))\nax= sns.countplot(x='Sex', hue=\"Survived\", data=train_df)\n","a929a896":"#By using heatmap\ngroup = train_df.groupby(['Pclass','Survived'])\npclass_survived = group.size().unstack()","085f1e8c":"# Heatmap - Color encoded 2D representation of data. \nsns.heatmap(pclass_survived, annot = True, fmt =\"d\")","3bf87458":"# Violinplot Displays distribution of data  \n# across all levels of a category. \nsns.violinplot(x =\"Sex\", y =\"Age\", hue =\"Survived\",  \ndata = train_df, split = True)","8978fb8b":"sns.catplot(x ='Embarked', hue ='Survived',  \nkind ='count', col ='Pclass', data = train_df) ","f329d0bf":"#Sibsp + Parch + Family, make them into Family COlumn\nall_df = [train_df, test_df]\n\nfor i in all_df:\n  i['Family'] = i['SibSp'] + i['Parch'] + 1\n","0c90b82c":"# Factorplot for Family_Size \nsns.factorplot(x ='Family', y ='Survived', data = train_df)","312c162a":"all_df = [train_df, test_df]\n\nfor data in all_df:\n  data['IsAlone'] = 0\n  data.loc[data['Family'] == 1, 'IsAlone'] = 1","6b5c5b6c":"# Factorplot for Alone \nsns.factorplot(x ='IsAlone', y ='Survived', data = train_df)\n","9dc965d3":"#So we will fill whose empty rows before doing computation.\n\nall_df = [train_df, test_df]\n\nfor data in all_df:\n  data['Fare'] = data['Fare'].fillna(data['Fare'].median())\n","779c1a31":"# Barplot - Shows approximate values based  \n# on the height of bars. \nsns.barplot(x ='category_fare', y ='Survived',  data = train_df) ","fe28959f":"print( train_df[[\"Sex\",\"Survived\"]].groupby([\"Sex\"], as_index = False).mean() )","687aa2f3":"print( train_df[[\"Pclass\",\"Survived\"]].groupby([\"Pclass\"], as_index = False).mean() )","eef2d988":"print( train_df[[\"Embarked\",\"Survived\"]].groupby([\"Embarked\"], as_index = False).mean() )","de484025":"print( train_df[[\"Family\",\"Survived\"]].groupby([\"Family\"], as_index = False).mean() )","d6d033ee":"train_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","97eafd21":"train_df['category_fare'] = pd.qcut(train_df['Fare'], 4)\nprint( train_df[[\"category_fare\",\"Survived\"]].groupby([\"category_fare\"], as_index = False).mean() )","059e6920":"train_df = train_df.drop(['Cabin'], axis = 1)\ntest_df = test_df.drop(['Cabin'], axis = 1 )","de24aa7d":"train_df = train_df.drop(['Ticket'], axis = 1)\ntest_df = test_df.drop(['Ticket'], axis = 1)","3bc3bc1c":"print(\"Southampton(S):\")\nsouthampton = train_df[train_df['Embarked'] == 'S'].shape[0]\nprint(southampton)\n\nprint(\"Cherbourg(C):\")\ncherbourg = train_df[train_df['Embarked'] == 'C'].shape[0]\nprint(cherbourg)\n\nprint(\"Queenstown(Q):\")\nqueenstown = train_df[train_df['Embarked'] == 'Q'].shape[0]\nprint(queenstown)","31bc9345":"train_df = train_df.fillna({\"Embarked\": \"S\"})","a0d489aa":"train_df.Embarked.isnull().sum()","d6b18ecb":"all_df = [train_df, test_df]\ntitles = {'Mr':1, 'Miss':2, 'Mrs':3, 'Master':4,'Rare':5}\n\nfor i in all_df:\n  i['Title'] = i.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n  i['Title'] = i['Title'].replace(['Lady','Countess','Capt','Col','Don','Dr','Major', 'Rev','Sir','Jonkheer','Dona'], 'Rare')\n  i['Title'] = i['Title'].replace('Mlle', 'Miss')\n  i['Title'] = i['Title'].replace('Ms', 'Miss')\n  i['Title'] = i['Title'].replace('Mme', 'Mrs')\n  # convert titles into numbers\n  i['Title'] = i['Title'].map(titles)\n  # filling NaN with 0, to get safe\n  i['Title'] = i['Title'].fillna(0)\n\n","af23690f":"train_df = train_df.drop(['Name'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)","6678c2f9":"train_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","7698b293":"\nall_df = [train_df, test_df]\n\nfor i in all_df:\n  i['Age'] = i['Age'].replace(np.NaN, i['Age'].mean())","f7f293b4":"train_df.Age.describe()","b19602ed":"all_df = [train_df, test_df]\n\nfor data in all_df:\n  data['Age'] = data['Age'].astype(int)\n  data.loc[data['Age'] <= 16, 'Age']                        = 0\n  data.loc[(data['Age'] > 16) & (data['Age'] <= 32), 'Age'] = 1\n  data.loc[(data['Age'] > 32) & (data['Age'] <= 48), 'Age'] = 2\n  data.loc[(data['Age'] > 48) & (data['Age'] <= 64), 'Age'] = 3\n  data.loc[data['Age'] > 64, 'Age']                        = 4","a2dcd162":"#Let us see its distribution\ntrain_df['Age'].value_counts()","7204114a":"#Mapping Fare\nall_df = [train_df, test_df]\n\nfor i in all_df:\n  i.loc[i['Fare'] <= 7.91, 'Fare'] = 0\n  i.loc[(i['Fare'] > 7.91) & (i['Fare'] <= 14.45), 'Fare'] = 1\n  i.loc[(i['Fare'] > 14.45) & (i['Fare'] <= 31), 'Fare'] = 2\n  i.loc[i['Fare'] > 31, 'Fare'] = 3\n  i['Fare'] = i['Fare'].astype(int)","197c4303":"train_df =train_df.drop(['category_fare'], axis =1)","e22ed09f":"#Convert Sex column to Numerics\n#Mapping Sex Column\nall_df = [train_df, test_df]\ngender = {\"male\":0, \"female\":1}\n\nfor i in all_df:\n  i[\"Sex\"] = i[\"Sex\"].map(gender).astype(int)","40ec4741":"#Convert all Port names to Numeric Form\n#Mapping Embarked\nall_df = [train_df, test_df]\nport = {'S':0, 'C':1, 'Q':2}\n\nfor i in all_df:\n  i['Embarked'] = i['Embarked'].map(port)","446631ba":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['Age_Class']= dataset['Age']* dataset['Pclass']","f1caf91a":"train_df = train_df.drop(['SibSp','Parch'], axis = 1)\ntest_df = test_df.drop(['SibSp','Parch'], axis = 1)","b9995e8e":"X_train = train_df.drop(['PassengerId', 'Survived'], axis= 1)\ny_train = train_df['Survived']","5349d0e4":"X_test = test_df.drop(['PassengerId'], axis = 1)","99884be4":"#Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, y_train) * 100, 2)\nprint(\"Logistic Regression Accuracy:\", acc_log)","b1657ad6":"#KNN\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\nprint(\"KNN Accuracy:\",acc_knn)","59f5763e":"#Decision Tree\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\nacc_dt = round(dt.score(X_train, y_train) * 100, 2)\nprint(\"Decision Tree Accuracy:\", acc_dt)","8ec9b549":"#Random Forest\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\nacc_rf = round(rf.score(X_train, y_train) * 100, 2)\nprint(\"Random Forest Accuracy:\", acc_rf)","43b468cf":"models = pd.DataFrame({\n    'Model' : ['Logistic Regression', 'KNN', 'Random Forest','Decision Tree'],\n    'Score' : [acc_log, acc_knn, acc_dt, acc_rf]\n})","c8f8256d":"models.sort_values(by = 'Score', ascending= False)","e2ddb127":"from sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rf, X_train, y_train, cv=10, scoring = 'accuracy')\n\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","c4a7b26a":"importances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(rf.feature_importances_,2)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances.head(15)","64e40aa2":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(rf, X_train, y_train, cv=3)\nconfusion_matrix(y_train, predictions)","1e43aab9":"from sklearn.metrics import precision_score, recall_score\n\nprint(\"Precision:\", precision_score(y_train, predictions))\nprint(\"Recall:\",recall_score(y_train, predictions))","1248ca2f":"from sklearn.metrics import f1_score\nf1_score(y_train, predictions)","5d1a05d3":"#Create csv file to save results\nsubmission = pd.DataFrame({\n    'PassengerId' : test_df['PassengerId'],\n    'Survived': y_pred\n})\n\nsubmission.to_csv('submission.csv', index = False)","e3682197":"### 4. Sex vs Age with respect to Survived Column","ee5cb743":"If the family size is greater than 5, chances of survival decreases considerably","28e6991b":"## Creating Submission File","aef3fd7f":"## Exploratory Data Analysis","e0e0d7f0":"Now, if we look at dataset I can observe some categorical columns which could be easily converted to numeric values. \n\nFor example, 'Sex' and 'Embarked'","e4bbebc9":"It can be observed that Random Forest performed excellently","6eb2f496":"### 3. Clean Age\n\nFrom Data Exploration Age had 177 missing entries so fare. So I have replaced it with average mean across all the rows. Also need to convert the Age column from float to integer","52603407":"This graph gives a summary of the age range of men, women and children who were saved. The survival rate is \u2013\n\n* Good for children.\n* High for women in the age range 20-50.\n* Less for men as the age increases.","b2a0677c":"As we can see, Cabin, Age , Embarked and Fare have missing data in them. We will find and fill missing values later in the notebook","65200ab7":" It can be concluded that if a passenger paid a higher fare, the survival rate is more.","02e6a14c":"* It can be observed that 38% survived with a mean age of 29 years. \n* Also that Age column is continuous ranging from 0.4 to 80.\n* Some Columns have missing values like Cabin, Ticket.","01f65a10":"**The Challenge**\n\n* The sinking of the Titanic is one of the most infamous shipwrecks in history.\n* On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\n* While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.","1127e1de":"It can be seen that, Age, Cabin and Embarked have some NaN values\nCabin has more than 75% missing values, so it would be wise to drop it.","4ba953a5":"#  Titanic Survival Prediction at Beginner Level","977f729b":"### 3. Pclass \n\nConsists of three classes according to socio-economic status\n1st = Upper\n2nd = Middle\n3rd = Lower\n","d42121a0":"* Which class had higher survival ?\n* Did the people of higher socio-economic make it alive or otherwise?\n","5121be7d":"### 2. Sex \n\nConsists of No of Female and Male passengers","000ba88e":"### 4. Handle Fare\n\nI had already created category_fare to bin Fare ranges together.  First we will convert it from float into integer.","b1a4a68d":"* Which port has maximum boarding?\n* Does this correlate with survival?\n","939d8eb7":"From the plot it is clear that survival chances of people on Titanic were quite low.","41e9ca11":"Upload necessary csv files.","90088fd4":"1. Import Libraries\n2. Explore Dataset\n4. Data Visualization\n5. Cleaning Data and Dealing with Missing Data\n6. Add New Features\n6. Building Models\n7. Choosing the best model\n8. Create Submission File","75a8ae05":"### 4. Random Forest","24db4d6e":"The training-set has 891 examples and 11 features + the target variable (survived). 2 of the features are floats, 5 are integers and 5 are objects","a3322794":"If a passenger is alone, the survival rate is less","19ad02d3":"Let us check first few rows in train.csv. ","bc8a079d":"Confusion Matrix\n\nA confusion matrix is a table that is often used to describe the performance of a classification model (or \u201cclassifier\u201d) on a set of test data for which the true values are known","f263a7ae":"### Age_Class ","39d07106":"we can also drop the Ticket feature since it's unlikely to yield any useful information","c53edb11":"A Broader explanation of all the column names and its meaning is being given [here](https:\/\/www.kaggle.com\/c\/titanic\/data) in tabular format.","e03553a9":"Assign it to respective DataFrame and check its dimensions","abc84ed2":"**Contents**\n\n","429752c0":"### 8. Fare\n\nAs we have seen from data exploration that Fare is a continous column and also has some missing values. Hence, we wil group Fare values in an assigned group and visualize it.","5d2a4d02":"### 3. Decision Tree","99f78046":"Numeric data is always easy to handle rather than string data. So I have converted the titles 'Mr, Miss, Rare' to integer data types for ease of use.","2cfbec0c":"### 1. Survived ","5b3c3f34":"A Kaggle Competition use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.","88271444":"Here I have grouped together Age ranges and assigned a number to it.","14cda6fc":"The heatmap tells us that Class 1 passengers have a higher survival chance compared to classes 2 and 3. It implies that Pclass contributes a lot to a passenger\u2019s survival rate.","6d75d7cf":"### 6. Family.\n\nIn our dataset we had SipSp and Parch columns which told us about various family relations. So I thought it would be better to combine them into a single column named Family and get insights.","4a10e654":"* What is the optimal Family size to survive the catastrophe?","fb35d0c4":"### 1. Logistic Regression","8e5456f7":"We can see that in train_df we are provided with the Survived Column, whereas test_df doesnt give us survival column","41b6d6b0":"### 2. KNN","4fa6db61":"## Cleaning the Data and Dealing with Missing Data","48a78bba":"Here it is seen that I was provided with 10 different cross validators which each gave different scores.\n\nMean represented the best out of 10 score which was 81.71%\n\nAlso the standard deviation said that the accuracy can be in the range of +4 or -4.","1d6a20ab":"## Data Visualization","b3fc6a27":"* How many People Survived on the RMS Titanic?\n\nFor this let us look at the 'Survived Column' in our dataset. Use countplot to give a count of total survived people.","74029b53":"Precision and Recall Scores\n\nPrecision refers to the percentage of your results which are relevant, recall refers to the percentage of total relevant results correctly classified by your algorithm.","6763ceff":"I have putforth a solution with guidance from various notebooks, blogs and ofcourse stack overflow website. Please feel free to glance over the analysis and do recommend any suggestions and queries.","20485ac1":"### 2. Get Title from Name\n\nLooking at the Name Column we can see many things. It contains the name of the passenger with its respective title.\n\nFor Example, Master title has Age mean of 5 years.\n\nSo let us extract the 'title' feature from the Name.","0c0a0b76":"F1 Score\n\nF1 score, is a measure of a model\u2019s accuracy on a dataset. It is used to evaluate binary classification systems, which classify examples into \u2018positive\u2019 or \u2018negative\u2019.","6005a162":"It can be observed that most people boarded from Southampton. \n\nReplacing the missing values in the Embarked feature with 'S'.","771ff405":"### 7. IsAlone \n\n It can be possible that some passengers might be travelling alone. So it is better to add IsAlone column.","05aa103a":"## Import Libraries","bd5fe893":"\n\n1. Numerical Features: Age (Continuous), Fare (Continuous), SibSp (Discrete), Parch (Discrete)\n2. Categorical Features: Survived, Sex, Embarked, Pclass \n3. \nAlphanumeric Features: Ticket, Cabin\n\n\n","5179f627":"This Dataframe holds all the importance of various columns in train.csv.\n\n'Sex' and 'Title' top the list whereas 'Age' and 'IsAlone' are not that necessary for evaluation.\n\nFurther step would be to drop 'IsAlone' and train the model again","7de9fc7e":"### K-Fold Cross Validation to validate a models performance","ebc8f1ad":"## Building Models\n\nBefore using any Machine Learning Algorithm it would better if I eliminate some columns which would not be that useful.","100465da":"### 5. Embarked\n\nC = Cherbourg, Q = Queenstown, S = Southampton","42573931":"### 1. Clean Embarked\n\nLet us check the counts of total passengers boarded from respective ports.","043efe41":"* Which Gender outlived the other?\n* Who had more chance of survival?","65f91d02":"It was observed that Females were mostly rescued than Male passengers. ","c954dea2":"## Add New Features\n\nAs seen earlier, I have created a 'Title' column extracted from Name column. Let me create one more column before moving on to model building."}}