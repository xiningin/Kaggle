{"cell_type":{"607e9194":"code","2e1889ae":"code","bd4da005":"markdown","080ad1f6":"markdown","17282013":"markdown","e925476b":"markdown","e777a0f5":"markdown"},"source":{"607e9194":"# Install versioned packages for compatability\n!pip install spacy==2.3.2\n\n# Install paperetl project\n!pip install paperetl\n\n# Install scispacy model\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.5\/en_core_sci_md-0.2.5.tar.gz","2e1889ae":"import os\nimport shutil\n\nfrom paperetl.cord19.execute import Execute as Etl\n\n# Copy study design models locally\nos.mkdir(\"cord19q\")\nshutil.copy(\"..\/input\/cord19-study-design\/attribute\", \"cord19q\")\nshutil.copy(\"..\/input\/cord19-study-design\/design\", \"cord19q\")\n\n# Copy previous articles database locally for predictable performance\nshutil.copy(\"..\/input\/cord-19-etl\/cord19q\/articles.sqlite\", \"\/tmp\")\n\n# Build SQLite database for metadata.csv and json full text files\nEtl.run(\"..\/input\/CORD-19-research-challenge\", \"cord19q\", \"cord19q\", \"..\/input\/cord-19-article-entry-dates\/entry-dates.csv\", False, \"\/tmp\/articles.sqlite\")","bd4da005":"Upon completion, a database named articles.sqlite will be stored in the output directory under a sub-folder named cord19q.","080ad1f6":"This notebook requires Internet connectivity to be enabled. If this notebook is copied, the GitHub project could also be forked for an edited notebook to modify the Python code. Would simply just need to update the pip install command above to the new repository location.","17282013":"# Install\n\n[paperetl](https:\/\/github.com\/neuml\/paperetl) can be installed directly from GitHub using pip as follows. This project also depends on scispacy which must be installed separately.","e925476b":"# Build SQLite articles database\n\nThe raw CORD-19 data is stored across a metadata.csv file and json files with the full text. This project uses [SQLite](https:\/\/www.sqlite.org\/index.html) to aggregate and store the merged content.\n\nThe ETL process transforms the csv\/json files into a SQLite database. The process iterates over each row in metadata.csv, extracts the column data and ensures it is not a pure duplicate (using the sha hash). This process will also load the full text if available. \n\n## Tagging\nArticles are tagged based on keyword matches. The only tag at this time is COVID-19 and articles are tagged with this if the article text contains any of the following regular expressions. \n\n>2019[\\-\\s]?n[\\-\\s]?cov, 2019 novel coronavirus, coronavirus 2(?:019)?, coronavirus disease (?:20)?19, covid(?:[\\-\\s]?(?:20)?19)?, n\\s?cov[\\-\\s]?2019, sars[\\-\\s]cov-?2, wuhan (?:coronavirus|cov|pneumonia)\n\nCredit to [@ajrwhite](https:\/\/www.kaggle.com\/ajrwhite) and his [notebook](https:\/\/www.kaggle.com\/ajrwhite\/covid-19-thematic-tagging-with-regular-expressions) in helping to develop this list.\n\n## Study Design\nAdditional metadata is parsed out of the article to derive information on the study design. The models referenced in this section are [available as a dataset](https:\/\/www.kaggle.com\/davidmezzetti\/cord19-study-design). \n\n### Design Type\nThe full text is analyzed to determine a design type for the backing study in the article using a machine learning model. The model has a pre-defined vocabulary and features are a count of each of these defined keywords. A Random Forest Classifier is then trained using the feature set and is used to predict study design labels. \n\nCredit to [@savannareid](https:\/\/www.kaggle.com\/savannareid) for developing the keywords to use with this method. The keywords can be found in this [domain dictionary](https:\/\/docs.google.com\/spreadsheets\/d\/1t2e3CHGxHJBiFgHeW0dfwtvCG4x0CDCzcTFX7yz9Z2E\/edit#gid=389064679). More details on deriving a study design can be found in [this discussion](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/discussion\/139355).\n\n### Attribute Type Detection and Extraction\nAdditionally, the full text is analyzed to identify study metadata for the backing study in the article also using a machine learning model. The model has a combination of features including a TF-IDF vector of the text elements and Natural Lanuage Processing (NLP) elements. The NLP features are built from entity, part of speech and dependency labels extracted with [scispacy](https:\/\/allenai.github.io\/scispacy\/). scispacy has been pretrained on medical articles and has good detection on articles in this dataset. A Logistic Regression Classifier is then trained using the feature set and is used to predict attribute labels.\n\nBased on the attribute type, further extraction is used via NLP. An example of this is with the sample size. Given a sentence \"34 patients were enrolled\", the logic will take the token patients and use dependency labels to extract the associated number (34) of patients to use as the sample size.\n\nAnother example of extraction is with risk factors with an example being the odds ratio of hypertension within a study. Using the detected entities, logic runs to find the matching statistic for a topic (such as hypertension) within a text section. The current process can currently only extract statistics, not calculate statistics from lower level data.\n\n## Grammar Labels\nThe title, abstract and full-text fields are tokenized into sentences. Linguistic rules are used to label each sentence to help identify concise, data-driven statements. \n\nFor the linguistic rules process, it has two basic rules right now.\n\n1. *QUESTION*: Sentence ending in a '?' mark\n2. *FRAGMENT*: Less informative\/incomplete statements. Acceptable sentences have the following structure.\n  - At least one nominal subject noun\/proper noun AND\n  - At least one action\/verb AND\n  - At least 5 words\n\nImportant source files to highlight\n- ETL Process -> [execute.py](https:\/\/github.com\/neuml\/paperetl\/blob\/master\/src\/python\/paperetl\/cord19\/execute.py)\n- Linguistic Rules -> [grammar.py](https:\/\/github.com\/neuml\/paperetl\/blob\/master\/src\/python\/paperetl\/grammar.py)\n- Study Design Model -> [design.py](https:\/\/github.com\/neuml\/paperetl\/blob\/master\/src\/python\/paperetl\/study\/design.py)\n- Attribute Model -> [attribute.py](https:\/\/github.com\/neuml\/paperetl\/blob\/master\/src\/python\/paperetl\/study\/attribute.py)\n- Sample Size Extraction -> [sample.py](https:\/\/github.com\/neuml\/paperetl\/blob\/master\/src\/python\/paperetl\/study\/sample.py)","e777a0f5":"# COVID-19 Open Research Dataset (CORD-19) ETL\n\n<p align=\"center\">\n    <img src=\"https:\/\/pages.semanticscholar.org\/hs-fs\/hubfs\/covid-image.png?width=300&name=covid-image.png\"\/>\n<\/p>\n\n***NOTE: There is a [Report Builder Notebook](https:\/\/www.kaggle.com\/davidmezzetti\/cord-19-report-builder) that runs on a prebuilt model. If you just want to try this out without a full build, this is the best choice.***\n\nCOVID-19 Open Research Dataset (CORD-19) is a free resource of scholarly articles, aggregated by a coalition of leading research groups, covering COVID-19 and the coronavirus family of viruses. The dataset can be found on [Semantic Scholar](https:\/\/pages.semanticscholar.org\/coronavirus-research) and [Kaggle](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge).\n\nThis notebook uses the [paperetl](https:\/\/github.com\/neuml\/paperetl) project to load the raw CORD-19 dataset into a SQLite database. paperetl also supports loading data into Elasticsearch and exporting output to JSON\/YAML files. [CORD-19 Analysis with Sentence Embeddings](https:\/\/www.kaggle.com\/davidmezzetti\/cord-19-analysis-with-sentence-embeddings) uses this notebook as the source data for analysis tasks."}}