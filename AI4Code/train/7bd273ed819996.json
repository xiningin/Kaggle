{"cell_type":{"e2e081d6":"code","4c1ef514":"code","07e7c36d":"code","4caf6b09":"code","29b08d9d":"code","71083562":"code","0cd7aa4c":"code","5cceac46":"code","c6f344f9":"code","345ee438":"code","3206ee4a":"code","8627f80d":"code","a7863c44":"code","120d690a":"markdown","d3531bd7":"markdown","f96acc40":"markdown"},"source":{"e2e081d6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4c1ef514":"import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Layer\n\nos.chdir('\/kaggle\/input\/facial-recognition-project-t7\/V3 Project Files\/LFW_support')\nfrom models_lfw.face_recognition.model_small import create_model\nfrom models_lfw.face_recognition.align import AlignDlib\nfrom triplet_loss import TripletLossLayer\nfrom coml_preprocessor import COMLDataGenerator","07e7c36d":"# Input for anchor, positive, and negative images\nin_a = Input(shape=(96, 96, 3), name=\"img_a\")\nin_p = Input(shape=(96, 96, 3), name=\"img_p\")\nin_n = Input(shape=(96, 96, 3), name=\"img_n\")\n\n# create the base model from model from model_small\nmodel_sm = create_model()\n\n# Output embedding vectors from anchor, positive, and negative images\n# The model weights are shared (Triplet network)\nemb_a = model_sm(in_a)\nemb_p = model_sm(in_p)\nemb_n = model_sm(in_n)\n\n# Layer that computes the triplet loss from anchor, positive, and negative embedding vectors\ntriplet_loss_layer = TripletLossLayer(alpha=0.2, name='triplet_loss_layer')([emb_a, emb_p, emb_n])\n\n# Model that can be trained with anchor, positive, and negative images\nmodel = Model([in_a, in_p, in_n], triplet_loss_layer)\n\nmodel.load_weights('lfw_basis_checkpoint.hdf5')\nmodel.summary()\n\nbase_model = model.layers[3]\nbase_model.summary()","4caf6b09":"class IdentityMetadata():\n    def __init__(self, base, name, file):\n        # dataset base directory\n        self.base = base\n        # identity name\n        self.name = name\n        # image file name\n        self.file = file\n\n    def __repr__(self):\n        return self.image_path()\n\n    def image_path(self):\n        return os.path.join(self.base, self.name, self.file)\n\n    def name(self):\n        return name \n    \ndef load_metadata(path, upper_limit):\n    metadata = []\n    count = 0\n    for i in sorted(os.listdir(path)):\n        if count == upper_limit: \n            break\n            \n        for f in sorted(os.listdir(os.path.join(path, i))):\n            if count == upper_limit: \n                break\n                \n            count += 1\n            # Check file extension. Allow only jpg\/jpeg' files.\n            ext = os.path.splitext(f)[1]\n            if ext == '.jpg' or ext == '.jpeg':\n                metadata.append(IdentityMetadata(path, i, f))\n    return np.array(metadata)\n\ndef load_image(path):\n    img = cv2.imread(path, 1)\n    # OpenCV loads images with color channels\n    # in BGR order. So we need to reverse them\n    return img[...,::-1]\n    \ndef align_image(img):\n        alignment = AlignDlib('\/kaggle\/input\/facial-recognition-project-t7\/V3 Project Files\/LFW_support\/models_lfw\/landmarks.dat')\n        bb = alignment.getLargestFaceBoundingBox(img)\n        if bb is None:\n            return cv2.resize(img, (96,96))\n        else:\n            return alignment.align(96, \n                                   img, \n                                   bb,\n                                   landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n\ndef create_dataset_embeddings(dataset):\n  embeddings = np.empty((metadata.shape[0], 128))\n\n  # use model to predict the embeddings from the dataset\n  for i, m in enumerate(dataset):\n      img = load_image(m.image_path())\n      img = align_image(img)\n      img = img.astype('float32')\n      img = img \/ 255.0\n      img = np.expand_dims(img, axis=0)\n      embeddings[i] = base_model.predict(img)\n\n      # for displaying the progress of creating embeddings\n      if i%1 == 0: print(i, end=\" \")\n      if i == len(metadata)-1: print('done')\n  return embeddings\n\ndef create_image_embeddings(images):\n  embeddings = np.empty((metadata.shape[0], 128))\n\n  # use model to predict the embeddings from the dataset\n  for i,img in enumerate(images):\n      img = align_image(np.asarray(img))\n      img = img.astype('float32')\n      img =img \/ 255.0\n      img = np.expand_dims(img, axis=0)\n      # person = str(i)\n      embeddings[i] = base_model.predict(img)\n\n      # for displaying the progress of creating embeddings\n      if i%1 == 0: print(i, end=\" \")\n      if i == len(metadata)-1: print('done')\n  return embeddings","29b08d9d":"# load all 144 identities\npath = '\/kaggle\/input\/facial-recognition-project-t7\/V3 Project Files\/fire_data'\nmetadata = load_metadata(path, 144) \n\n# initialize the OpenFace face alignment utility\nalignment = AlignDlib('\/kaggle\/input\/facial-recognition-project-t7\/V3 Project Files\/LFW_support\/models_lfw\/landmarks.dat')\n\n# load stored embeddings or create new ones\nstored_embeddings_path = '\/kaggle\/input\/facial-recognition-project-t7\/V3 Project Files\/LFW_support\/dataset_embeddings.npy'\nif os.path.exists(stored_embeddings_path):\n  dataset_embeddings = np.load(stored_embeddings_path)\nelse: \n  dataset_embeddings = create_dataset_embeddings(metadata)\n  np.save('\/kaggle\/input\/facial-recognition-project-t7\/V3 Project Files\/LFW_support\/dataset_embeddings.npy', dataset_embeddings)","71083562":"def distance(emb1, emb2):\n    return np.sum(np.square(emb1 - emb2))\n\ndef show_pair(embeddings, id1, id2):\n    plt.figure(figsize=(4,2))\n    plt.suptitle(f'Distance = {distance(embeddings[id1], embeddings[id2]):.2f}')\n    plt.subplot(1,2,1)\n    plt.imshow(align_image(load_image(metadata[id1].image_path())))\n    plt.subplot(1,2,2)\n    plt.imshow(align_image(load_image(metadata[id2].image_path())))","0cd7aa4c":"os.chdir('\/kaggle\/working')\n!pip install pycocotools","5cceac46":"import argparse\nimport random\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nimport torchvision.transforms as T\nimport matplotlib.pyplot as plt\nimport PIL.Image\n\nos.chdir('\/kaggle\/input\/facial-recognition-project-t7\/V3 Project Files\/DETR_support\/detr')\nimport util.misc as utils\nfrom models_detr import build_model\nfrom main import get_args_parser\n\n\nfrom pycocotools.coco import COCO\nimport skimage.io as io\nimport pylab\npylab.rcParams['figure.figsize'] = (8.0, 10.0)","c6f344f9":"parser = argparse.ArgumentParser(description='DETR args parser', parents=[get_args_parser()])\nargs = parser.parse_args(args=[])\nargs.resume = '\/kaggle\/input\/facial-recognition-project-t7\/V3 Project Files\/DETR_support\/checkpoint.pth'\nargs.device = 'cpu'\n\nif args.output_dir:\n  Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n\nargs.distributed = False\n\nprint(args)","345ee438":"model, criterion, postprocessors = build_model(args)\n\ndevice = torch.device(args.device)\nmodel.to(device)","3206ee4a":"output_dir = Path(args.output_dir)\nif args.resume:\n  # the model will download the weights and model state from the https link provided\n  if args.resume.startswith('https'):\n    checkpoint = torch.hub.load_state_dict_from_url(args.resume, map_location='cpu', check_hash=True)\n  else:\n    checkpoint = torch.load(args.resume, map_location='cpu')\n\n  # this loads the weights and model state into the model\n  model.load_state_dict(checkpoint['model'], strict=True)","8627f80d":"CLASSES = [\n   'N\/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n   'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N\/A',\n   'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n   'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N\/A', 'backpack',\n   'umbrella', 'N\/A', 'N\/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n   'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n   'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N\/A', 'wine glass',\n   'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n   'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n   'chair', 'couch', 'potted plant', 'bed', 'N\/A', 'dining table', 'N\/A',\n   'N\/A', 'toilet', 'N\/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n   'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N\/A',\n   'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n   'toothbrush'\n]\n\n# colors for visualization\nCOLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n\n# standard PyTorch mean-std input image normalization\ntransform = T.Compose([\n    T.ToTensor(),\n    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# for output bounding box post-processing\ndef box_cxcywh_to_xyxy(x):\n    x_c, y_c, w, h = x.unbind(1)\n    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n    return torch.stack(b, dim=1)\n\ndef rescale_bboxes(out_bbox, size):\n    img_w, img_h = size\n    b = box_cxcywh_to_xyxy(out_bbox)\n    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n    return b\n\ndef detect(im, model, transform):\n  # mean-std normalize the input image (batch-size: 1)\n  img = transform(im).unsqueeze(0)\n\n  assert img.shape[-2] <= 1600 and img.shape[-1] <= 1600, 'demo model only supports images up to 1600 pixels on each side'\n\n  # propagate through the model\n  outputs = model(img)\n\n  # keep only predictions with 0.7+ confidence\n  probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n  keep = probas.max(-1).values > 0.7\n\n  # convert boxes from [0; 1] to image scales\n  bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n  return probas[keep], bboxes_scaled\n\ndef get_bbox_points(pil_img, prob, boxes, classes):\n  bboxes = []\n  for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100):\n    cl = p.argmax()\n    if CLASSES[cl] not in classes: # only plot these classes\n      continue\n    bboxes.append([xmin, ymin, xmax, ymax])\n  return bboxes\n\ndef crop_images(pil_img, bboxes):\n  cropped = []\n  for bbox in bboxes:\n    cropped.append(pil_img.crop((bbox[0], bbox[1], bbox[2], bbox[3])))\n  return cropped\n\ndef process_input(pil_img, prob, boxes, classes, image_embeddings, filename):\n  # use global variable dataset_embeddings\n  plt.figure(figsize=(16,10))\n  plt.imshow(pil_img)\n  ax = plt.gca()\n  current_image = 0\n  used = []\n\n  for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100):\n    cl = p.argmax()\n    if CLASSES[cl] not in classes: # only plot these classes\n      continue\n    ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color=c, linewidth=3))\n\n    # determine which face from the dataset matches the current face closest\n    min_distance = .35\n    min_image = -1\n    for i in range(len(dataset_embeddings)):\n        current_distance = distance(image_embeddings[current_image], dataset_embeddings[i])\n        if current_distance < min_distance and metadata[i].name not in used:\n            min_distance = current_distance\n            min_image = i\n    current_image += 1\n    used.append(metadata[min_image].name)\n    name = metadata[min_image].name.replace(\"_\", \" \")\n    \n    # compare name with person mappings\n    id = filename + \"_\"\n    with open('\/kaggle\/input\/2021-spring-coml-face-recognition-competition\/person_id_name_mapping.csv', newline = '') as csvfile:\n        reader = csv.reader(csvfile)\n        for row in reader:\n            if row[1] == name:\n                id += row[0]\n            \n    # add determination to competition CSV    \n    if id != (filename + \"_\") and id in target_ids and id not in predicted_ids: # prevents entries like 'a_' or 'c_'\n        predicted_ids.append(id)\n        output_writer.writerow([id, int(xmin), int(xmax), int(ymin), int(ymax)])\n        \n    ax.text(xmin, ymin, name, fontsize=15, bbox=dict(facecolor='yellow', alpha=0.5))\n  plt.axis('off')\n  plt.show()\n","a7863c44":"import csv\ntarget_ids = []\npredicted_ids = []\nsample_csv = open('\/kaggle\/input\/2021-spring-coml-face-recognition-competition\/kaggle_sample_submission.csv', 'r')\nsample_reader = csv.reader(sample_csv)\nfor row in sample_reader:\n    if row[0] != 'id':\n        target_ids.append(row[0])\n\noutput_csv = open('\/kaggle\/working\/output.csv', 'a')\noutput_csv.truncate(0) # reset file contents if already exists\noutput_writer = csv.writer(output_csv)\n\noutput_writer.writerow(['id', 'xmin', 'xmax', 'ymin', 'ymax'])\n\n\n# Image paths\na = '\/kaggle\/input\/2021-spring-coml-face-recognition-competition\/a.jpg'\nb = '\/kaggle\/input\/2021-spring-coml-face-recognition-competition\/b.jpg'\nc = '\/kaggle\/input\/2021-spring-coml-face-recognition-competition\/c.jpg'\nd = '\/kaggle\/input\/2021-spring-coml-face-recognition-competition\/d.jpg'\n\n# predict and process a\nthe_image = PIL.Image.open(a).convert('RGB')\nscores, boxes = detect(the_image, model, transform)\nplot_classes = ['person']\n\nbboxes = get_bbox_points(the_image, scores, boxes, plot_classes)\ncropped = crop_images(the_image, bboxes)\nimage_embeddings = create_image_embeddings(cropped)\n\nprocess_input(the_image, scores, boxes, plot_classes, image_embeddings, 'a')\n\n# predict and process b\nthe_image = PIL.Image.open(b).convert('RGB')\nscores, boxes = detect(the_image, model, transform)\nplot_classes = ['person']\n\nbboxes = get_bbox_points(the_image, scores, boxes, plot_classes)\ncropped = crop_images(the_image, bboxes)\nimage_embeddings = create_image_embeddings(cropped)\n\nprocess_input(the_image, scores, boxes, plot_classes, image_embeddings, 'b')\n\n# predict and process c\nthe_image = PIL.Image.open(c).convert('RGB')\nscores, boxes = detect(the_image, model, transform)\nplot_classes = ['person']\n\nbboxes = get_bbox_points(the_image, scores, boxes, plot_classes)\ncropped = crop_images(the_image, bboxes)\nimage_embeddings = create_image_embeddings(cropped)\n\nprocess_input(the_image, scores, boxes, plot_classes, image_embeddings, 'c')\n\n# predict and process d\nthe_image = PIL.Image.open(d).convert('RGB')\nscores, boxes = detect(the_image, model, transform)\nplot_classes = ['person']\n\nbboxes = get_bbox_points(the_image, scores, boxes, plot_classes)\ncropped = crop_images(the_image, bboxes)\nimage_embeddings = create_image_embeddings(cropped)\n\nprocess_input(the_image, scores, boxes, plot_classes, image_embeddings, 'd')\n\n# fill unpredicted targets\nfor target in target_ids:\n    if target not in predicted_ids and target != 'id':\n        output_writer.writerow([target, 0, 0, 0, 0])","120d690a":"# DETR Face Detection Model Initialization","d3531bd7":"# LFW Face Recognition Model Initialization","f96acc40":"# Competition Data Processing"}}