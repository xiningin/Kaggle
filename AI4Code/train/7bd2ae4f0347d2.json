{"cell_type":{"d1ed3a82":"code","6c62c361":"code","70d0bb52":"code","67b518ac":"code","6477784f":"code","a1f88a31":"code","6033b241":"code","82c51327":"code","c8241d1c":"code","e2f4f2f0":"markdown","aa71a346":"markdown","92b83359":"markdown","11568f0e":"markdown","686390cc":"markdown","5cf61fa7":"markdown","2f66f00f":"markdown"},"source":{"d1ed3a82":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, Dropout, LSTM, GRU\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6c62c361":"KAGGLE_PATH = '\/kaggle\/input\/dl-urfu-yelp\/'\nx_train = pd.read_csv(KAGGLE_PATH + 'train.csv')\ny_train = pd.read_csv(KAGGLE_PATH + 'train_label.csv', index_col=0)\nx_test = pd.read_csv(KAGGLE_PATH + 'test.csv')\nx_train.shape, y_train.shape, x_test.shape, ","70d0bb52":"# \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u043c \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0441 \u0442\u0435\u043a\u0441\u0442\u043e\u043c\nx_train = x_train.Review\nx_test = x_test.Review","67b518ac":"# \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c \u0434\u043b\u0438\u043d\u0443 \u0441\u043b\u043e\u0432\u0430\u0440\u044f, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\nnum_words = 10000\n# \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c \u0434\u043b\u0438\u043d\u0443 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438, \u043a \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043c \u0432\u0441\u0435 \u0442\u0435\u043a\u0441\u0442\u044b\nmax_review_len = 50\n# \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u0438 \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0442\u043e\u0440 \u043d\u0430 \u0442\u0435\u0440\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\ntokenizer = Tokenizer(num_words=num_words)\ntokenizer.fit_on_texts(x_train)\n# \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0442\u043e\u0440 \u043a \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u043c \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u043c \u0434\u0430\u043d\u043d\u044b\u043c\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\n# \u043f\u0440\u0438\u0432\u043e\u0434\u0438\u043c \u0432\u0441\u0435 \u0432\u0435\u043a\u0442\u043e\u0440\u0430 \u0442\u0435\u043a\u0441\u0442\u043e\u0432 \u043a \u043e\u0431\u0449\u0435\u0439 \u0434\u043b\u0438\u043d\u0435\nx_train = pad_sequences(x_train, maxlen=max_review_len, padding='post')\nx_test = pad_sequences(x_test, maxlen=max_review_len, padding='post')","6477784f":"model = Sequential()\nmodel.add(Embedding(num_words, 32, input_length=max_review_len))\n# \u0412\u044b\u0431\u0435\u0440\u0438\u0442\u0435 \u043e\u0434\u0438\u043d \u0432\u0430\u0440\u0438\u0430\u043d\u0442 - LSTM \u0438\u043b\u0438 GRU\nmodel.add(LSTM(32))\n# model.add(GRU(32))\nmodel.add(Dense(1, activation='sigmoid'))","a1f88a31":"model.compile(optimizer='sgd', \n              loss='binary_crossentropy', \n              metrics=['accuracy'])","6033b241":"history = model.fit(x_train, \n                    y_train, \n                    epochs=5,\n                    batch_size=64,\n                    validation_split=0.1)","82c51327":"plt.plot(history.history['accuracy'], \n         label='\u0414\u043e\u043b\u044f \u0432\u0435\u0440\u043d\u044b\u0445 \u043e\u0442\u0432\u0435\u0442\u043e\u0432 \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u043c \u043d\u0430\u0431\u043e\u0440\u0435')\nplt.plot(history.history['val_accuracy'], \n         label='\u0414\u043e\u043b\u044f \u0432\u0435\u0440\u043d\u044b\u0445 \u043e\u0442\u0432\u0435\u0442\u043e\u0432 \u043d\u0430 \u043f\u0440\u043e\u0432\u0435\u0440\u043e\u0447\u043d\u043e\u043c \u043d\u0430\u0431\u043e\u0440\u0435')\nplt.xlabel('\u042d\u043f\u043e\u0445\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f')\nplt.ylabel('\u0414\u043e\u043b\u044f \u0432\u0435\u0440\u043d\u044b\u0445 \u043e\u0442\u0432\u0435\u0442\u043e\u0432')\nplt.legend()\nplt.show()","c8241d1c":"sample_submission = pd.read_csv(KAGGLE_PATH + 'sample_submission.csv', index_col='id')\nsample_submission.label = model.predict_classes(x_test)\nsample_submission.to_csv(\"sample_submission.csv\")","e2f4f2f0":"### \u0422\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u044f \u0442\u0435\u043a\u0441\u0442\u0430","aa71a346":"## \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u0443\u044e \u0441\u0435\u0442\u044c","92b83359":"## \u0413\u043e\u0442\u043e\u0432\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f LeaderBoard","11568f0e":"## \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438","686390cc":"\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435","5cf61fa7":"## \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f","2f66f00f":"## \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0442\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432 \u043e\u0442\u0437\u044b\u0432\u043e\u0432 \u043d\u0430 \u0441\u0430\u0439\u0442\u0435 [YELP](https:\/\/www.yelp.com\/dataset) \u0440\u0435\u043a\u043a\u0443\u0440\u0435\u043d\u0442\u043d\u043e\u0439 \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u044c\u044e\n\n\u0423\u0447\u0435\u0431\u043d\u044b\u0439 \u043a\u0443\u0440\u0441 \"[\u041f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0433\u043b\u0443\u0431\u043e\u043a\u0438\u0445 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u044b\u0445 \u0441\u0435\u0442\u0435\u0439 \u043d\u0430 Python](https:\/\/openedu.ru\/course\/urfu\/PYDNN\/)\".\n"}}