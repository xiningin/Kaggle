{"cell_type":{"547a446e":"code","ff4f7c02":"code","e007f2eb":"code","b30777cb":"code","7b67226c":"code","3d560ecd":"code","2fa9e331":"code","328326d5":"code","d6c12ab5":"code","5303fea4":"code","4dbe7c9c":"code","d2bafb9c":"code","e235828a":"code","c8bf3c5c":"code","e396cc5d":"code","6f6743fd":"code","7488dd8d":"code","e09a7523":"code","5934ab5e":"code","9c317641":"code","404989d7":"code","c509bca4":"code","c56ccfab":"code","19776d6b":"code","9bf9f5a4":"markdown","21294163":"markdown","35119650":"markdown","7350bb67":"markdown","3590878e":"markdown","b078437f":"markdown","f2ec5843":"markdown","526f64f0":"markdown","413bec08":"markdown","cc108a4c":"markdown","2ed4a024":"markdown","f2464e09":"markdown","409322a1":"markdown","8e7693ec":"markdown","1125a065":"markdown","e80dc4c3":"markdown"},"source":{"547a446e":"import numpy as np # linear algebra\nimport pandas as pd\nimport os\n\nimport random\n\n#deep learning imports\nimport keras\nfrom keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.callbacks import TensorBoard\nfrom keras.optimizers import Adam\nfrom keras import regularizers\nfrom keras.losses import categorical_crossentropy\nfrom keras import backend as K\nfrom keras.utils.vis_utils import plot_model\n\n#data visualization and plotting imports\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport time\n\n#word library import\nfrom nltk.corpus import words\n\n\nos.environ['KMP_DUPLICATE_LIB_OK']='True'","ff4f7c02":"#setting up global variables\nDATADIR = \"..\/input\/asl-alphabet\/asl_alphabet_train\/asl_alphabet_train\" #training data directory\nCATEGORIES = ['A', 'B' , 'C' , 'D' , 'del', 'E' , 'F' , 'G' , 'H', 'I', 'J', 'K', 'L' ,'M' , 'N', 'nothing', 'O', 'P' , 'Q' , 'R' , 'S' , 'space' , 'T' ,'U' , 'V', 'W', 'X' , 'Y' , 'Z']\ntest_dir = \"..\/input\/asl-alphabet\/asl_alphabet_test\/asl_alphabet_test\"\nown_dir = \"..\/input\/ishaan\/ishaan_pics\/ishaan_pics\"","e007f2eb":"def create_training_data(modeltype):\n    '''This function is run for each model in order to get the training data from the filepath \n    and convert it into array format'''\n    training_data = []\n    if(modeltype == 'cnn'):\n        for category in CATEGORIES:\n            path = os.path.join(DATADIR, category) #path to alphabets\n            class_num = CATEGORIES.index(category)\n            for img in os.listdir(path):\n                try:\n                    img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_COLOR)\n                    new_array = cv2.resize(img_array, (64, 64))\n                    final_img = cv2.cvtColor(new_array, cv2.COLOR_BGR2RGB)\n                    training_data.append([final_img, class_num])\n                except Exception as e:\n                    pass\n    else:\n         for category in CATEGORIES:\n            path = os.path.join(DATADIR, category) #path to alphabets\n            class_num = CATEGORIES.index(category)\n            for img in os.listdir(path):\n                try:\n                    img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)\n                    new_array = cv2.resize(img_array, (64, 64))\n                    training_data.append([new_array, class_num])\n                except Exception as e:\n                    pass\n    return training_data","b30777cb":"def make_data(modeltype, training_data):\n    '''This formats the training data into the proper format and passes it through an generator \n    so that it can be augmented(shifted left\/right, rotated, etc) and fed into the model '''\n    X=[]\n    y=[]\n    for features,label in training_data:\n        X.append(features)\n        y.append(label)\n    if(modeltype == \"cnn\"):\n        X = np.array(X).reshape(-1, 64, 64, 3)\n        X = X.astype('float32')\/255.0 #to normalize data\n        y = keras.utils.to_categorical(y) #one-hot encoding\n        y = np.array(y)\n        datagen = ImageDataGenerator(\n                                     validation_split = 0.1, \n                                     rotation_range=20,\n                                     width_shift_range=0.2,\n                                     height_shift_range=0.2,\n                                     horizontal_flip=True)\n        train_data = datagen.flow(X, y, batch_size = 64, shuffle=True, subset='training')\n        val_data = datagen.flow(X, y, batch_size = 64, shuffle=True, subset='validation')\n        return (train_data, val_data, X, y)\n    else:\n        X = np.array(X).flatten().reshape(-1, 4096)\n        X = X.astype('float32')\/255.0\n        y = keras.utils.to_categorical(y)\n        y = np.array(y)\n        return (X, y)\n   ","7b67226c":"def build_model(modeltype):\n    '''Builds the model based on the specified modeltype(either convolutional or fully_connected)'''\n    model = Sequential()\n    \n    if(modeltype == \"cnn\"):\n        model.add(Conv2D(64, kernel_size=4, strides=1, activation='relu', input_shape=(64,64,3)))\n        model.add(Conv2D(64, kernel_size=4, strides=2, activation='relu'))\n        model.add(Dropout(0.5))\n\n        model.add(Conv2D(128, kernel_size=4, strides=1, activation='relu'))\n        model.add(Conv2D(128, kernel_size=4, strides=2, activation='relu'))\n        model.add(Dropout(0.5))\n\n        model.add(Conv2D(256, kernel_size=4, strides=1, activation='relu'))\n        model.add(Conv2D(256, kernel_size=4, strides=2, activation='relu'))\n\n        model.add(BatchNormalization())\n\n        model.add(Flatten())\n        model.add(Dropout(0.5))\n        model.add(Dense(512, activation='relu', kernel_regularizer = regularizers.l2(0.001)))\n        model.add(Dense(29, activation='softmax'))\n        \n    else:\n        model.add(Dense(4096, activation = 'relu'))\n        model.add(Dense(4096, activation = 'relu'))\n        model.add(Dense(2000, activation = 'relu'))\n        model.add(Dense(29, activation = 'softmax'))\n    \n    model.compile(optimizer = Adam(lr=0.0005), loss = 'categorical_crossentropy', metrics = [\"accuracy\"]) #learning rate reduced to help problems with overfitting\n    return model\n        ","3d560ecd":"def fit_fully_connected_model(X, y, model):\n    '''fits the fully connected model'''\n    \n    filepath = \"weights2.best.h5\"\n    \n    # saving model weights with lowest validation loss to reduce overfitting\n    checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n    #tensorboard\n    tensorboard_callback = keras.callbacks.TensorBoard(\"logs\")\n    model.fit(X, y, epochs = 10, validation_split = 0.1, callbacks = [checkpoint, tensorboard_callback])","2fa9e331":"def fit_CNN_model(train_data, val_data, model):\n    '''fits the CNN model'''\n    \n    filepath = \"weights.best.h5\"\n    \n    # saving model weights with lowest validation loss to reduce overfitting\n    checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n    #tensorboard\n    tensorboard_callback = keras.callbacks.TensorBoard(\"logs\")\n    \n    #fitting model\n    model.fit_generator(train_data,epochs=10, steps_per_epoch = 1360, validation_data = val_data, validation_steps= len(val_data), callbacks = [checkpoint, tensorboard_callback])","328326d5":"def show_classification_report(X, y, input_shape, model):\n    '''This function prints a classification report for the validation data'''\n    start_time = time.time()\n    validation = [X[i] for i in range(int(0.1 * len(X)))]\n    validation_labels = [np.argmax(y[i]) for i in range(int(0.1 * len(y)))]\n    validation_preds = []\n    labels = [i for i in range(29)]\n    for img in validation:\n        img = img.reshape((1,) + input_shape)\n        pred = model.predict_classes(img)\n        validation_preds.append(pred[0])\n    print(classification_report(validation_labels, validation_preds,labels, target_names=CATEGORIES))\n    print(\"\\n Evaluating the model took {:.0f} seconds\".format(time.time()-start_time))\n    return (validation_labels, validation_preds)","d6c12ab5":"def plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    #classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    # print(cm)\n\n    fig, ax = plt.subplots(figsize=(20, 10))\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\nnp.set_printoptions(precision=2)","5303fea4":"def rotate_image(img):\n    '''This function will be applied to the given test data and my own test data\n    to see how rotating the data effects prediction accuracy.\n    It rotates it in a way such that no part of the image is lost'''\n    (h, w) = img.shape[:2]\n    \n    # calculate the center of the image\n    center = (w \/ 2, h \/ 2)\n\n    angle90 = 90\n    angle180 = 180\n    angle270 = 270\n\n    scale = 1.0\n\n    # Perform the counter clockwise rotation holding at the center\n    # 90 degrees\n    M = cv2.getRotationMatrix2D(center, angle90, scale)\n    rotated90 = cv2.warpAffine(img, M, (h, w))\n\n    # 180 degrees\n    M = cv2.getRotationMatrix2D(center, angle180, scale)\n    rotated180 = cv2.warpAffine(img, M, (w, h))\n\n    # 270 degrees\n    M = cv2.getRotationMatrix2D(center, angle270, scale)\n    rotated270 = cv2.warpAffine(img, M, (h, w))\n    \n    return (rotated90, rotated180, rotated270)","4dbe7c9c":"def create_testing_data(path, input_shape, modeltype):\n    '''This function will get and format both the testing data from the dataset and my own pictures.\n    It works in almost the exact same way as training_data except it returns image names to evaluate predictions'''\n    testing_data = []\n    names = []\n    for img in os.listdir(path):\n        if(modeltype == 'cnn'):\n            img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_COLOR)\n            rotated_90, rotated_180, rotated_270 = rotate_image(img_array) #in order to test predictions for rotated data\n            imgs = [img_array, rotated_90, rotated_180, rotated_270]\n            final_imgs = []\n            for image in imgs:\n                new_array = cv2.resize(image, (64, 64))\n                final_img = cv2.cvtColor(new_array, cv2.COLOR_BGR2RGB)\n                final_imgs.append(final_img)\n        else:\n            img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)\n            rotated_90, rotated_180, rotated_270 = rotate_image(img_array)\n            imgs = [img_array, rotated_90, rotated_180, rotated_270]\n            final_imgs = []\n            for image in imgs:\n                final_img = cv2.resize(image, (64, 64))\n                final_imgs.append(final_img)\n        # print(len(final_imgs))\n        for final_img in final_imgs:\n            testing_data.append(final_img) \n            names.append(img)\n    if modeltype == 'cnn':\n        new_testing_data = np.array(testing_data).reshape((-1,) + input_shape)\n    else:\n        new_testing_data = np.array(testing_data).flatten().reshape((-1,) + input_shape)\n    new_testing_data = new_testing_data.astype('float32')\/255.0\n    return (testing_data, new_testing_data, names)\n\ndef prediction_generator(testing_data, input_shape, model):\n    '''This function generates predictions for both sets of testing data'''\n    predictions=[]\n    for img in testing_data:\n        img = img.reshape((1,) + input_shape)\n        pred = model.predict_classes(img)\n        predictions.append(pred[0])\n    predictions = np.array(predictions)\n    return predictions","d2bafb9c":"def plot_predictions(testing_data, predictions, names):\n    '''This functions plots the testing data predictions along with the actual letter they represent so we can see the accuracy\n    of the model.'''\n    fig = plt.figure(figsize = (100, 100))\n    fig.subplots_adjust(hspace = 0.8, wspace = 0.5)\n    # fig.set_size_inches(np.array(fig.get_size_inches()) * (len(testing_data)\/10))\n    index = 0\n    for i in range(1, len(testing_data)):\n        y = fig.add_subplot(12 ,np.ceil(len(testing_data)\/float(12)),i)\n        \n        str_label = CATEGORIES[predictions[index]]\n        y.imshow(testing_data[index], cmap = 'gray')\n        if(index%4==0):\n            title = \"prediction = {}\\n {}\\n unrotated\".format(str_label,names[index])\n        else:\n            title = \"prediction = {}\\n {}\".format(str_label,names[index])\n        y.set_title(title,fontsize= 60)\n        y.axes.get_xaxis().set_visible(False)\n        y.axes.get_yaxis().set_visible(False)\n        index+=1\n        \ndef calculate_loss(names,predictions):\n    y_true = K.variable(np.array([CATEGORIES.index(name[0].upper()) for name in names]))\n    y_pred = K.variable(np.array(predictions))\n    print(y_true)\n    print(y_pred)\n    error = K.eval(categorical_crossentropy(y_true, y_pred))\n    print(error)","e235828a":"%load_ext tensorboard.notebook\n%tensorboard --logdir logs","c8bf3c5c":"modeltype = \"fully_connected\"\ninput_shape = 4096,\n\n#getting training data\ntraining_data = create_training_data(modeltype)\nrandom.shuffle(training_data)\n\n#building the model\nmodel = build_model(modeltype)\n\n#formatting data\nX, y = make_data(modeltype, training_data)\n\n#fitting model\nfit_fully_connected_model(X, y, model)\nmodel.load_weights(\"weights2.best.h5\")\ngraph = plot_model(model, to_file=\"my_model.png\", show_shapes=True)","e396cc5d":"#evaluating validation data\nvalidation_labels, validation_preds = show_classification_report(X, y, input_shape, model)","6f6743fd":"#confusion matrix for validation data\nplot_confusion_matrix(validation_labels, validation_preds, classes=CATEGORIES,\n                      title='Confusion matrix, without normalization')\nplt.show()","7488dd8d":"# database testing data and predictions\ntesting_data, new_testing_data, names = create_testing_data(test_dir, input_shape, modeltype)\npredictions = prediction_generator(new_testing_data, input_shape, model)\nplot_predictions(testing_data, predictions, names)\n# calculate_loss(names, predictions)","e09a7523":"#own testing data and predictions\nown_data, new_own_data, own_names = create_testing_data(own_dir, input_shape, modeltype)\nown_predictions = prediction_generator(new_own_data, input_shape, model)\nplot_predictions(own_data, own_predictions, own_names)","5934ab5e":"modeltype2 = \"cnn\"\ninput_shape2 = 64, 64, 3\n\n#getting training data\ntraining_data2 = create_training_data(modeltype2)\nrandom.shuffle(training_data2)\n\n#building model\nmodel2 = build_model(modeltype2)\n\n#formatting data\ntrain_data2, val_data2, X2, y2 = make_data(modeltype2, training_data2)\n\n#fitting model\nfit_CNN_model(train_data2, val_data2, model2)\nmodel2.load_weights(\"weights.best.h5\")\ngraph2 = plot_model(model2, to_file=\"my_model2.png\", show_shapes=True)","9c317641":"#evaluating validation data\nvalidation_labels2, validation_preds2 = show_classification_report(X2, y2, input_shape2, model2)","404989d7":"#confusion matrix for validation data\nplot_confusion_matrix(validation_labels2, validation_preds2, classes=CATEGORIES,\n                      title='Confusion matrix, without normalization')\nplt.show()","c509bca4":"#database testing data and predictions\ntest_dir = \"..\/input\/asl-alphabet\/asl_alphabet_test\/asl_alphabet_test\"\ntesting_data2, new_testing_data2, names2 = create_testing_data(test_dir, input_shape2, modeltype2)\npredictions2 = prediction_generator(new_testing_data2, input_shape2, model2)\nplot_predictions(testing_data2, predictions2, names2)\n# calculate_loss(names2, predictions2)\n","c56ccfab":"#own testing data and predictions\nown_dir = \"..\/input\/ishaan\/ishaan_pics\/ishaan_pics\"\nown_data2, new_own_data2, own_names2 = create_testing_data(own_dir, input_shape2, modeltype2)\nown_predictions2 = prediction_generator(new_own_data2, input_shape2, model2)\nplot_predictions(own_data2, own_predictions2, own_names2)","19776d6b":"word_list = words.words()\nfor i in range(5):\n    randNum = random.randint(0, len(word_list))\n    word = word_list[randNum]\n    letters = list(word)\n\n    letter_signs = []\n    for letter in letters:\n        img_name = \"{}_test.jpg\".format(letter.upper())\n        img_array = cv2.imread(os.path.join(test_dir,img_name), cv2.IMREAD_COLOR)\n        new_array = cv2.resize(img_array, (64, 64))\n        final_img = cv2.cvtColor(new_array, cv2.COLOR_BGR2RGB)\n        letter_signs.append(final_img)\n\n    processed_letter_signs = np.array(letter_signs).reshape((-1,) + input_shape2)\n    processed_letter_signs = processed_letter_signs.astype('float32')\/255.0\n    \n    letter_predictions = prediction_generator(processed_letter_signs, input_shape2, model2)\n    predicted_word = \"\"\n    for prediction in letter_predictions:\n        predicted_word += CATEGORIES[prediction]\n    \n    word_fig = plt.figure(figsize = (13, 13))\n    \n    for j in range(len(processed_letter_signs)):\n        y = word_fig.add_subplot(1,len(processed_letter_signs), (j+1))\n        y.imshow(letter_signs[j], cmap = 'gray')\n        title = letters[j]\n        y.set_title(title)\n        y.axes.get_xaxis().set_visible(False)\n        y.axes.get_yaxis().set_visible(False)\n    print(predicted_word)\n        ","9bf9f5a4":"**Testing data and predictions**","21294163":"**Conclusion**\n\nThe training data is not very varied, so while it performs well with the database testing data which is from the same hand, the model has learned to recognize features from the same hand in the same lighting conditions and so does not generalize well. Thus, it does not perform that well with my own pictures. Furthermore, since ASL alphabet images are based on orientation, and changing the orientation of an image can change it's letter representation the model does not perform well with rotated data.","35119650":"**The Models**\n\nAll the functions are run here.","7350bb67":"**The Models**\n\nThe fully connected model is a standard model consisting of dense layers.\n\nThe convolutional model is taken from [Running Kaggle Kernels with a GPU](https:\/\/www.kaggle.com\/dansbecker\/running-kaggle-kernels-with-a-gpu). I added a regularizer and BatchNormalization because, as you will see below, the model runs into problems with overfitting since all the training_data is from one hand and seems to be taken as a series of burst photos, which means that it doesn't do well with data from other people's hands. So I added them in an attempt to reduce overfitting.","3590878e":"The own_dir directory contains images of my hand, which are formatted to the same specifications as those in the test_data in an attempt to improve the prediction accuracy.","b078437f":"**Convolutional Neural Network**","f2ec5843":"The plot_confusion_matrix function was taken from the [scikit-learn documentation](https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html)","526f64f0":"**Getting Training Data**","413bec08":"**Fully Connected Model**","cc108a4c":"**TensorBoard**","2ed4a024":"The code is layed out in a way such that all the work is split up into functions which are called in order at the end of the code.\nIn this notebook i am running all the training data and test data through two models, one consisting of fully connected layers and the other being a convolutional neural network, so all the methods will be run twice(once for each model) and are written so that depending on the modeltype the data is processed and run through the model accordingly.\nThe fully connected model only returned a decent accuracy with grayscale data, so the fully connected model images are grayscale.","f2464e09":"**Importing Libraries**","409322a1":"**Data Visualization and Evaluation**","8e7693ec":"**Taking Multiple Images**\n\nThe following code generates 5 random words from the nltk word database which are converted to a series of asl alphabet images. These images are then fed into the model and through the predictions generated by the model the model predicts each individual letter and thus the word. All 5 word predictions are printed first followed by what the actual word was in ASL alphabet form(Each sign has its letter equivalent displayed above it). ","1125a065":"**Intro**\n\nThis notebook aims to use the ASL Alphabet kaggle dataset in order to predict images from the ASL Alphabet with a high level of accuracy. For further details on the project click on this [link](https:\/\/docs.google.com\/document\/d\/18ZJ-UoV0qIZ496GJwkGhrHfTzv3HM8L-J2h8ImQB33k\/edit?usp=sharing).\n","e80dc4c3":"**Pre-processing Training Data**"}}