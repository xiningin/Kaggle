{"cell_type":{"884a9aac":"code","c0a23117":"code","e3f9be0d":"code","72310f87":"code","f3d8ff58":"code","7982df8a":"code","d9e1949d":"code","7c6f4853":"markdown","d1699c9f":"markdown","ce117023":"markdown","d5bdc204":"markdown","4532d6a5":"markdown","52048e9e":"markdown","306bdec7":"markdown"},"source":{"884a9aac":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import io #scikit-image\n\n# read the image from a link\n# opencv doesn't directly read images from an url, so we will use\n# skimage module to read it first\nimg = io.imread(\"http:\/\/www.dphclub.com\/tutorials\/images\/war-time-1.jpg\", as_gray=True)\nplt.figure(figsize=(12,8))\nplt.imshow(img, cmap=\"gray\");","c0a23117":"'''\nNOTE: it is important to check the values of intensities before going on the analysis. \nHere, for instance, intensities are normalized, i.e. they range from 0 to 1 rather than 0 to 255,\nso we need to convert it back to the latter so openCV works properly. That's why I multiply \nthe entire image by 255 and convert it to np.uint8 (a format supported by OpenCV).\n'''\n# transform it to a numpy array \nimg_arr = (np.round(np.array(img)*255)).astype(np.uint8)\n\n# flatten\nimg_arr = img_arr.flatten()\n\n# plot histogram\nplt.hist(img_arr, bins = 256, range = [0,256])\nplt.title(\"Number of pixels in each intensity value\")\nplt.xlabel(\"Intensity\")\nplt.ylabel(\"Number of pixels\")\nplt.show()","e3f9be0d":"img_eq = cv2.equalizeHist((img*255).astype(np.uint8))\nplt.figure(figsize=(12,8))\nplt.imshow(img_eq, cmap = \"gray\");","72310f87":"#get the numpy array for the equalized image\nimg_eq_arr = np.array(img_eq)\n\n# flatten\nimg_eq_arr = img_eq_arr.flatten()\n\n# plot histogram\nplt.hist(img_eq_arr, bins = 256, range = [0,256])\nplt.title(\"Number of pixels in each intensity value\")\nplt.xlabel(\"Intensity\")\nplt.ylabel(\"Number of pixels\")\nplt.show()","f3d8ff58":"# read the image\nimg = io.imread(\"https:\/\/i.ibb.co\/PzYwtD9\/clahe-first.jpg\", as_gray=True)\n# use Histogram Equalization\nimg_eq = cv2.equalizeHist((img*255).astype(np.uint8))\n# plot\nplt.figure(figsize=(10,7))\nplt.imshow(img_eq, cmap = \"gray\");","7982df8a":"clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\nclahe_img = clahe.apply((img*255).astype(np.uint8))\nplt.figure(figsize=(10,7))\nplt.imshow(clahe_img, cmap=\"gray\");","d9e1949d":"from sklearn.datasets import load_files\nfrom keras.preprocessing.image import load_img\nfrom tqdm import tqdm # progress bar\n\nTRAIN_DIR = '..\/input\/fruits-360_dataset\/fruits-360\/Training'\nTEST_DIR = '..\/input\/fruits-360_dataset\/fruits-360\/Test'\n\ndef load_dataset(path):\n    '''\n    Gets the path to the directory where \n    the image paths and return features, \n    targets and labels\n    '''\n    data = load_files(path)\n    files = np.array(data['filenames'])\n    targets = np.array(data['target'])\n    target_labels = np.array(data['target_names'])\n    return files,targets,target_labels\n    \ndef convert_image_to_array(files):\n    '''\n    Take a file path as input and return the image in grayscale associated with that path.\n    Returns an image in grayscale of size (100,100,1)\n    '''\n    images_as_array=[]\n    for file in tqdm(files):\n        # Convert to Numpy Array\n        # np.expand_dims creates the channel dimension, that is\n        # original shape: (100,100) | Final shape: (100,100,1)\n        images_as_array.append(np.expand_dims(np.array((load_img(file, color_mode = \"grayscale\"))), axis=2))\n        \n        # uncomment the line below if you want to load colored images\n        # but be aware that we need to be more careful to use HE and CLAHE\n        # in colored images. See here for more detials: \n        # https:\/\/hypjudy.github.io\/2017\/03\/19\/dip-histogram-equalization\/\n#         images_as_array.append(np.array(load_img(file)))\n    return images_as_array\n\n# load train and test datasets\nx_train, y_train,target_labels = load_dataset(TRAIN_DIR)\nx_test, y_test,_ = load_dataset(TEST_DIR)\n\n# convert the paths to actual images to be used in training\nxtrain_img = np.array(convert_image_to_array(x_train))\nxtest_img = np.array(convert_image_to_array(x_test))","7c6f4853":"Although we can see all three people clearly, it doesn't have a good contrast, as the image as a whole is pretty dark. We can confirm this by looking at the image's histogram.","d1699c9f":"# Previous tutorial\n- [Colorspaces](https:\/\/www.kaggle.com\/hrmello\/intro-to-image-processing-colorspaces)\n\n# Introduction\n___\nIn the previous tutorial, we've seen what colorspaces are and how we can go from one to another using mathematical operations (or using OpenCV's functions as a blackbox to convert them). When we do something with a set of images, such as a classification or segmentation, it may -- and probably will -- happen that the images are not good enough to feed algorithms. We will need to preprocess them first. There are several preprocessing methods for images:\n- Contrast adjustment\n    - Histogram equalization\n    - Contrast Limited Adaptive Histogram Equalization (CLAHE)\n- Image filtering\n    - Linear filters\n    - Non-linear filters\n    - Edge detection\n    - Zooming\n- Noise removal\n    - Wiener filtering\n    - Gaussian Blur\n\nThese are only a few of them and in this tutorial, in particular, we will focus on the first bullet point. But first we will need to understand why adjust contrast in an image at all. \n\nContrast is the difference in visual properties that makes an object distinguishable from other objects and background. If we want to create an object detector, it seems reasonable to assume that we want objects in the scene as distringuishable as possible. \n\nFor instance, let's take a look at this picture:\n![](https:\/\/d1ro734fq21xhf.cloudfront.net\/attachments\/00V0B4-190321584.jpg)\n\nMost of the man's face is black and we can't really see much besides his nose and mouth. Therefore if we want to develop, say, a face recognition algorithm and this is one of the images we will use (in either training or testing), we will probably want to enhance its contrast so we are able to get better details. \n\n# Histogram Equalization\n___\n\nIn the context of image analysis, we can use histograms to understand the distribution of pixel intensities in the picture. In other words, its a graphical representation of the number of pixels for each possible intensity value -- usually ranging from 0 to 255. \n\nIn low-contrast images, all pixels are close in intensity and this is what make the picture looks more or less the same with few distinguishable features. Our objective then is to stretch the histogram to either ends. This usually improves contrast, as there are more different intensities on the same image. Here is what it looks like:\n\n![](https:\/\/docs.opencv.org\/3.1.0\/histogram_equalization.png)\n\nLet's see that in practice using the image of three people below.","ce117023":"The equalized histogram confirms what we just saw in the picture. Now we have pixels' intensities ranging from 0 to 256, so we are able to distinguish more features as their intensities are more different than before. \n\n# Contrast Limited Adaptive Histogram Equalization (CLAHE)\n___\n\nSometimes, however, using a simple Histogram Equalization (HE) may not provide a good outcome. This happens because it uses the same transformation derived from the image histogram to transform all pixels. This works well if the distribution of intensities is more or less the same across all the image, as it was in our test image above. However, there are situations in which this is not the case. For instance, take a look at the following image.\n![](https:\/\/i.ibb.co\/PzYwtD9\/clahe-first.jpg)\nThe background is very dark whereas the head statue on the foreground is way brighter. What happens if we use HE here?","d5bdc204":"Now it becomes very apparent why there is low contrast in the image: From the 256 possible values to take, most pixels are between 20 and 100. \n\nNow let's equalize the histogram and see how both the image and the histogram looks like after that. ","4532d6a5":"Now both the background and foreground objects are distinguishable and the head is more detailed than without any processing.\n\n# Try it yourself!\n- Take an image in the Fruits 360 dataset, load it in grayscale and see if HE or CLAHE improves it. \n- If you are already familiar with the basics of convolutional neural networks for classification, you may try using a somewhat weak classifier with and without HE to check if there is some improvement in the model's performance just by using this technique. \n\nI left some code below that loads the dataset's images in grayscale. \n\n___","52048e9e":"The background did became more detailed, but the foreground became excessively bright, which may cause some issues in our post-processing algorithms, such as a face recognition program. Here adaptive histogram equalization (AHE) can be used. In this, image is divided into small blocks called \"tiles\" (tileSize is 8x8 by default in OpenCV). Then each of these blocks are histogram equalized as usual.\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/8\/84\/AHE-neighbourhoods.svg\/800px-AHE-neighbourhoods.svg.png\" width=\"500\" height=\"500\">\nThis may turns out not so good as expected if there is noise in the image. If that's the case, it will only be amplified. To avoid this, contrast limiting should be applied -- and hence the full name of the technique. If any histogram bin is above the specified contrast limit (by default 40 in OpenCV), those pixels are clipped and distributed uniformly to other bins before applying histogram equalization. After equalization, to remove artifacts in tile borders, bilinear interpolation is applied.\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/5\/5f\/Clahe-redist.svg\/1920px-Clahe-redist.svg.png\" width=\"500\" height=\"500\">\n\nFor a more detailed and mathematical explanation on how the interpolation works, check [this wiki page](https:\/\/en.wikipedia.org\/wiki\/Bilinear_interpolation) and its references.\n\nSo let's apply OpenCV's CLAHE and see how it turns out.","306bdec7":"Well, that's an improvement! Details like the folds on their coats and the background are much clearer now. "}}