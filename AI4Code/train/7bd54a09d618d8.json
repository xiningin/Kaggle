{"cell_type":{"3d9178cc":"code","9186ffa4":"code","f4ad1967":"code","cd720f5c":"code","c7913cac":"code","1c1864d7":"code","e054b882":"code","b5eacbfa":"code","a9691a82":"code","8a3fecee":"code","72786603":"code","5766d327":"code","3ca24f32":"code","70e7e465":"code","e48dba1d":"code","833b142e":"code","3ac1cdf5":"code","cabbc875":"markdown","2bb87e53":"markdown","f6025b35":"markdown","0ee848e7":"markdown","7dc4a97e":"markdown","ab2d0e61":"markdown","4ca71b49":"markdown","147ea955":"markdown","4ceb7341":"markdown","6e460a40":"markdown","3bc5019e":"markdown","5019259b":"markdown"},"source":{"3d9178cc":"import sys\nsys.path.append(\"..\/input\/tez-lib\/\")\nsys.path.append(\"..\/input\/timmmaster\/\")\n\nimport tez\nimport albumentations\nimport pandas as pd\nimport cv2\nimport numpy as np\nimport timm\nimport torch.nn as nn\nfrom sklearn import metrics\nimport torch\nfrom tez.callbacks import EarlyStopping\nfrom tqdm import tqdm\nimport math\n\nclass args:\n    batch_size = 16\n    image_size = 384\n    \ndef sigmoid(x):\n    return 1 \/ (1 + math.exp(-x))","9186ffa4":"class PawpularDataset:\n    def __init__(self, image_paths, dense_features, targets, augmentations):\n        self.image_paths = image_paths\n        self.dense_features = dense_features\n        self.targets = targets\n        self.augmentations = augmentations\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, item):\n        image = cv2.imread(self.image_paths[item])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations is not None:\n            augmented = self.augmentations(image=image)\n            image = augmented[\"image\"]\n            \n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        \n        features = self.dense_features[item, :]\n        targets = self.targets[item]\n        \n        return {\n            \"image\": torch.tensor(image, dtype=torch.float),\n            \"features\": torch.tensor(features, dtype=torch.float),\n            \"targets\": torch.tensor(targets, dtype=torch.float),\n        }\n","f4ad1967":"    \nclass PawpularModel(tez.Model):\n    def __init__(self, model_name):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=False, in_chans=3)\n        self.model.head = nn.Linear(self.model.head.in_features, 128)\n        self.dropout = nn.Dropout(0.1)\n        self.dense1 = nn.Linear(140, 64)\n        self.dense2 = nn.Linear(64, 1)\n\n    def forward(self, image, features, targets=None):\n        x1 = self.model(image)\n        x = self.dropout(x1)\n        x = torch.cat([x, features], dim=1)\n        x = self.dense1(x)\n        x = self.dense2(x)\n        \n        x = torch.cat([x, x1, features], dim=1)\n        return x, 0, {}\n    \n\n","cd720f5c":"test_aug = albumentations.Compose(\n    [\n        albumentations.Resize(args.image_size, args.image_size, p=1),\n        albumentations.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n        albumentations.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n        albumentations.RandomBrightnessContrast(p=0.5),\n        \n        albumentations.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n            max_pixel_value=255.0,\n            p=1.0,\n        ),\n    ],\n    p=1.0,\n)","c7913cac":"import cuml, pickle\nfrom cuml.svm import SVR\nprint('RAPIDS version',cuml.__version__,'\\n')\n\nLOAD_SVR_FROM_PATH = '..\/input\/svr-models-10-folds\/'","1c1864d7":"df = pd.read_csv('..\/input\/same-old-creating-folds\/train_10folds.csv')\n","e054b882":"df.head()","b5eacbfa":"print('Train shape:', df.shape )","a9691a82":"super_final_predictions = []\nsuper_final_predictions2 = []\nsuper_final_oof_predictions = []\nsuper_final_oof_predictions2 = []\nsuper_final_oof_true = []\n\nfor fold_ in range(10):\n    print('#'*25)\n    print('### FOLD',fold_+1)\n    print('#'*25)\n    \n    model = PawpularModel(model_name=\"swin_large_patch4_window12_384\")\n    model.load(f\"..\/input\/paw-models\/model_f{fold_}.bin\", device=\"cuda\", weights_only=True)\n\n    df_test = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/test.csv\")\n    test_img_paths = [f\"..\/input\/petfinder-pawpularity-score\/test\/{x}.jpg\" for x in df_test[\"Id\"].values]\n        \n    df_valid = df[df.kfold == fold_].reset_index(drop=True)#.iloc[:160]\n    valid_img_paths = [f\"..\/input\/petfinder-pawpularity-score\/train\/{x}.jpg\" for x in df_valid[\"Id\"].values]\n\n    dense_features = [\n        'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n        'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'\n    ]\n    \n    name = f\"SVR_fold_{fold_}.pkl\" \n    if LOAD_SVR_FROM_PATH is None:\n       \n        # EXTRACT TRAIN EMBEDDINGS\n        \n        df_train = df[df.kfold != fold_].reset_index(drop=True)#.iloc[:320]\n        train_img_paths = [f\"..\/input\/petfinder-pawpularity-score\/train\/{x}.jpg\" for x in df_train[\"Id\"].values]\n        \n        train_dataset = PawpularDataset(\n            image_paths=train_img_paths,\n            dense_features=df_train[dense_features].values,\n            targets=df_train['Pawpularity'].values\/100.0,\n            augmentations=test_aug,\n        )\n        print('Extracting train embedding...')\n        train_predictions = model.predict(train_dataset, batch_size=2*args.batch_size, n_jobs=-1)\n    \n        embed = np.array([]).reshape((0,128+12))\n        for preds in train_predictions:\n            embed = np.concatenate([embed,preds[:,1:]],axis=0)\n        \n       \n        # FIT RAPIDS SVR\n        print('Fitting SVR...')\n        clf = SVR(C=20.0)\n        clf.fit(embed.astype('float32'), df_train.Pawpularity.values.astype('int32'))\n    \n       \n        # SAVE RAPIDS SVR \n        pickle.dump(clf, open(name, \"wb\"))\n        \n    else:\n       \n        # LOAD RAPIDS SVR \n        print('Loading SVR...',LOAD_SVR_FROM_PATH+name)\n        clf = pickle.load(open(LOAD_SVR_FROM_PATH+name, \"rb\"))\n\n    \n    # TEST PREDICTIONS\n    test_dataset = PawpularDataset(\n        image_paths=test_img_paths,\n        dense_features=df_test[dense_features].values,\n        targets=np.ones(len(test_img_paths)),\n        augmentations=test_aug,\n    )\n    print('Predicting test...')\n    test_predictions = model.predict(test_dataset, batch_size=2*args.batch_size, n_jobs=-1)\n\n    final_test_predictions = []\n    embed = np.array([]).reshape((0,128+12))\n    for preds in test_predictions: #tqdm\n        final_test_predictions.extend(preds[:,:1].ravel().tolist())\n        embed = np.concatenate([embed,preds[:,1:]],axis=0)\n\n    final_test_predictions = [sigmoid(x) * 100 for x in final_test_predictions]\n    final_test_predictions2 = clf.predict(embed)\n    super_final_predictions.append(final_test_predictions)\n    super_final_predictions2.append(final_test_predictions2)\n    \n    \n    \n    # OOF PREDICTIONS\n    valid_dataset = PawpularDataset(\n        image_paths=valid_img_paths,\n        dense_features=df_valid[dense_features].values,\n        targets=df_valid['Pawpularity'].values\/100.0,\n        augmentations=test_aug,\n    )\n    print('Predicting oof...')\n    valid_predictions = model.predict(valid_dataset, batch_size=2*args.batch_size, n_jobs=-1)\n\n    final_oof_predictions = []\n    embed = np.array([]).reshape((0,128+12))\n    for preds in valid_predictions:\n        final_oof_predictions.extend(preds[:,:1].ravel().tolist())\n        embed = np.concatenate([embed,preds[:,1:]],axis=0)\n\n    final_oof_predictions = [sigmoid(x) * 100 for x in final_oof_predictions]\n    final_oof_predictions2 = clf.predict(embed)    \n    super_final_oof_predictions.append(final_oof_predictions)\n    super_final_oof_predictions2.append(final_oof_predictions2)\n    \n    final_oof_true = df_valid['Pawpularity'].values\n    super_final_oof_true.append(final_oof_true)\n    \n    \n    \n    # COMPUTE RSME\n    rsme = np.sqrt( np.mean( (super_final_oof_true[-1] - np.array(super_final_oof_predictions[-1]))**2.0 ) )\n    print('NN RSME =',rsme,'\\n')\n    rsme = np.sqrt( np.mean( (super_final_oof_true[-1] - np.array(super_final_oof_predictions2[-1]))**2.0 ) )\n    print('SVR RSME =',rsme,'\\n')\n    \n    w = 0.5\n    oof2 = (1-w)*np.array(super_final_oof_predictions[-1]) + w*np.array(super_final_oof_predictions2[-1])\n    rsme = np.sqrt( np.mean( (super_final_oof_true[-1] - oof2)**2.0 ) )\n    print('Ensemble RSME =',rsme,'\\n')","8a3fecee":"true = np.hstack(super_final_oof_true)\n\noof = np.hstack(super_final_oof_predictions)\nrsme = np.sqrt( np.mean( (oof - true)**2.0 ))\nprint('Overall CV NN head RSME =',rsme)\n\noof2 = np.hstack(super_final_oof_predictions2)\nrsme = np.sqrt( np.mean( (oof2 - true)**2.0 ))\nprint('Overall CV SVR head RSME =',rsme)\n\noof3 = (1-w)*oof + w*oof2\nrsme = np.sqrt( np.mean( (oof3 - true)**2.0 ))\nprint('Overall CV Ensemble heads RSME with 50% NN and 50% SVR =',rsme)","72786603":"# Import Libs\nimport matplotlib.pyplot as plt","5766d327":"score = []\nfor ww in np.arange(0,1.05,0.05):\n    oof3 = (1-ww)*oof + ww*oof2\n    rsme = np.sqrt( np.mean( (oof3 - true)**2.0 ))\n    score.append(rsme)\nbest_w = np.argmin(score)*0.05\n\nplt.figure(figsize=(20,5))\nplt.plot(np.arange(21)\/20.0,score,'-o')\nplt.plot([best_w],np.min(score),'o',color='black',markersize=15)\nplt.title(f'Best Overall CV RSME={np.min(score):.4} with SVR Ensemble Weight={best_w:.2}',size=16)\nplt.ylabel('Overall Ensemble RSME',size=14)\nplt.xlabel('SVR Weight',size=14)\nplt.show()","3ca24f32":"# FORCE SVR WEIGHT TO LOWER VALUE TO HELP PUBLIC LB\nbest_w = 0.2","70e7e465":"super_final_predictions = np.mean(np.column_stack(super_final_predictions), axis=1)\nsuper_final_predictions2 = np.mean(np.column_stack(super_final_predictions2), axis=1)","e48dba1d":"df_test[\"Pawpularity\"] = (1-best_w)*super_final_predictions + best_w*super_final_predictions2\ndf_test = df_test[[\"Id\", \"Pawpularity\"]]","833b142e":"df_test.to_csv(\"submission.csv\", index=False)","3ac1cdf5":"df_test.head()","cabbc875":"# Now See Visualization","2bb87e53":"Thanks sir i inspired your notebook\n\n[1]: https:\/\/www.kaggle.com\/abhishek\/tez-pawpular-swin-ference\n[2]: https:\/\/www.kaggle.com\/abhishek\n[3]: https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/260800","f6025b35":"# Now Check The Head of the dataset","0ee848e7":"# Make Submission CSV\nWe make a submission csv using an ensemble of both heads. We use the optimal ensemble weights that we discovered above.","7dc4a97e":"# Compute CV Score\nBelow we compute the overall CV RSME scores of just the NN head, just the SVR head, and an ensemble of 50% NN and 50% SVR heads. Then we plot all ensemble weights to find the optimal weights for NN head and SVR heads.","ab2d0e61":"# How to Add RAPIDS SVR Head\nThere are 3 steps to building a double headed model. The first step is to train your Image NN backbone and head. This was done by Abhishek in his notebook [here][1] and achieves CV RSME 18.0. The next step is to train our RAPIDS SVR head with extracted embeddings from frozen Image NN backbone. This is done in version 1 of notebook you are reading [here][2] and achieves CV RSME 18.0. Lastly, we infer with both heads and average the predictions. This is done in the notebook you are reading and achieves CV RSME 17.8!\n\n![](https:\/\/raw.githubusercontent.com\/cdeotte\/Kaggle_Images\/main\/Oct-2021\/st1.png)\n![](https:\/\/raw.githubusercontent.com\/cdeotte\/Kaggle_Images\/main\/Oct-2021\/st2.png)\n![](https:\/\/raw.githubusercontent.com\/cdeotte\/Kaggle_Images\/main\/Oct-2021\/st3.png)\n\n[1]: https:\/\/www.kaggle.com\/abhishek\/tez-pawpular-swin-ference\n[2]: https:\/\/www.kaggle.com\/cdeotte\/rapids-svr-boost-17-8?scriptVersionId=76282086","4ca71b49":"# Trust CV or LB?\nAbove we see that using 50% NN head and 50% SVR head achieves the best overall CV score. However our RAPIDS SVR head isn't helping public LB much. We also notice that our RAPIDS SVR head helped folds `1, 2, 4, 5, 7, 8, 9, 10` but did not help folds `3, 6`. So is public test data just a \"bad fold\"? Will our RAPIDS SVR head help private LB? Below we force the weight of SVR head to be 10% in order to achieve a slight public LB boost. But maybe for final submission, we should use 50%??","147ea955":"# Now Check tha shape of the dataset","4ceb7341":"# Load Some Important Libraries for this notebook","6e460a40":"# Now Create Custom CLass","3bc5019e":"# Now Check The Head of Submission dataset","5019259b":"# Import RAPIDS"}}