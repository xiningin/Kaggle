{"cell_type":{"8527416c":"code","83e3e329":"code","fa070af5":"code","992ac65a":"code","d3e3df79":"code","139e3f26":"code","efe11d20":"code","a12377fa":"code","5b7d21b1":"code","8849e307":"code","8f899a3e":"code","dfabd060":"code","d36cd90a":"code","20436627":"code","9c7be81a":"code","0835ceeb":"code","df3a6484":"code","c7a3fbd0":"code","44abe9d8":"code","b26177a8":"code","5faa82c5":"code","a24c37e2":"code","d14fa5ac":"code","990aa75f":"code","b068fd58":"code","142696e4":"code","91cef0b6":"code","38bf3381":"code","69ad48fe":"code","49cd67b2":"code","6550e92c":"code","e1185708":"code","ef788a91":"code","9c576a94":"code","d35fa504":"code","2bdfc4ee":"code","6f0c6c47":"code","81f41bcc":"code","7dcb4801":"code","cd948ad2":"code","292e9c1d":"code","433feefd":"code","8c10994b":"code","8dd7a367":"code","07fa08c7":"code","d7fea107":"code","6936b0f3":"code","a6c2f4ab":"code","6fddb2a8":"code","430dcc40":"code","057a8199":"code","334291fa":"code","6eab8110":"code","2313e4fb":"code","92fbda62":"code","fc5b3444":"code","c45a96a1":"code","f662f803":"code","9c695419":"markdown","9ecced6e":"markdown","368e73ad":"markdown","2269040a":"markdown","862a7fdc":"markdown","cfd8aff4":"markdown","b922187c":"markdown","d9bfaaca":"markdown","9afd7339":"markdown","51d119b3":"markdown","ab26bdc0":"markdown","364b15f6":"markdown","0be21070":"markdown","6b19d0db":"markdown","0044f496":"markdown","94f75daf":"markdown","f12f93f0":"markdown","39527327":"markdown","3347395f":"markdown","0eeef8ee":"markdown","a910239b":"markdown","03c5253f":"markdown","23589722":"markdown","dc50b748":"markdown","96e74009":"markdown","b87f10e7":"markdown","933baa68":"markdown","63fd3b46":"markdown","29ee825e":"markdown","7a663c61":"markdown","4eecc85c":"markdown","3467cc8d":"markdown","97684718":"markdown","1274cf33":"markdown","1795d1c5":"markdown","0dc825a4":"markdown","d035c6d6":"markdown","f7d898fb":"markdown","b72ef8ce":"markdown","33fe9a4b":"markdown","883d17f7":"markdown","2f7d41b1":"markdown","81d7d09a":"markdown","2537b9fe":"markdown","346da646":"markdown","b56c6f3a":"markdown","0f1b706f":"markdown","b8c6c881":"markdown","abfdb322":"markdown","858cdfc4":"markdown","75fa1f9c":"markdown","425b7fd6":"markdown","e58c905b":"markdown","7ed16888":"markdown","d5bfb2da":"markdown","0052c721":"markdown"},"source":{"8527416c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport json\nfrom IPython.display import Image\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')","83e3e329":"root_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\nmetadata_path = f'{root_path}\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.head()","fa070af5":"meta_df.info()","992ac65a":"all_json = glob.glob(f'{root_path}\/**\/*.json', recursive=True)\nlen(all_json)","d3e3df79":"class FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            if not 'abstract' in content.keys():\n                self.abstract = ''\n                self.body_text= ''\n                return\n            for entry in content['abstract']:\n                self.abstract.append(entry['text'])\n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\nfirst_row = FileReader(all_json[0])\nprint(first_row)","139e3f26":"def get_breaks(content, length):\n    data = \"\"\n    words = content.split(' ')\n    total_chars = 0\n\n    # add break every length characters\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            data = data + \"<br>\" + words[i]\n            total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data","efe11d20":"dict_ = {'paper_id': [], 'doi':[], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) \/\/ 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    \n    try:\n        content = FileReader(entry)\n    except Exception as e:\n        continue  # invalid paper format, skip\n    \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    # no metadata, skip this paper\n    if len(meta_data) == 0:\n        continue\n    \n    dict_['abstract'].append(content.abstract)\n    dict_['paper_id'].append(content.paper_id)\n    dict_['body_text'].append(content.body_text)\n    \n    # also create a column for the summary of abstract to be used in a plot\n    if len(content.abstract) == 0: \n        # no abstract provided\n        dict_['abstract_summary'].append(\"Not provided.\")\n    elif len(content.abstract.split(' ')) > 100:\n        # abstract provided is too long for plot, take first 100 words append with ...\n        info = content.abstract.split(' ')[:100]\n        summary = get_breaks(' '.join(info), 40)\n        dict_['abstract_summary'].append(summary + \"...\")\n    else:\n        # abstract is short enough\n        summary = get_breaks(content.abstract, 40)\n        dict_['abstract_summary'].append(summary)\n        \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    \n    try:\n        # if more than one author\n        authors = meta_data['authors'].values[0].split(';')\n        if len(authors) > 2:\n            # if more than 2 authors, take them all with html tag breaks in between\n            dict_['authors'].append(get_breaks('. '.join(authors), 40))\n        else:\n            # authors will fit in plot\n            dict_['authors'].append(\". \".join(authors))\n    except Exception as e:\n        # if only one author - or Null valie\n        dict_['authors'].append(meta_data['authors'].values[0])\n    \n    # add the title information, add breaks when needed\n    try:\n        title = get_breaks(meta_data['title'].values[0], 40)\n        dict_['title'].append(title)\n    # if title was not provided\n    except Exception as e:\n        dict_['title'].append(meta_data['title'].values[0])\n    \n    # add the journal information\n    dict_['journal'].append(meta_data['journal'].values[0])\n    \n    # add doi\n    dict_['doi'].append(meta_data['doi'].values[0])\n    \ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'doi', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\ndf_covid.head()","a12377fa":"df_covid","5b7d21b1":"df_covid['abstract_word_count'] = df_covid['abstract'].apply(lambda x: len(x.strip().split()))  # word count in abstract\ndf_covid['body_word_count'] = df_covid['body_text'].apply(lambda x: len(x.strip().split()))  # word count in body\ndf_covid['body_unique_words']=df_covid['body_text'].apply(lambda x:len(set(str(x).split())))  # number of unique words in body\ndf_covid.head()","8849e307":"df_covid.info()","8f899a3e":"df_covid['abstract'].describe(include='all')","dfabd060":"\ndf_covid.drop_duplicates(['abstract', 'body_text'], inplace=True)\ndf_covid['abstract'].describe(include='all')","d36cd90a":"df_covid['body_text'].describe(include='all')","20436627":"df_covid.dropna(inplace=True)\n","9c7be81a":"df_covid.columns","0835ceeb":"df_covid.head()","df3a6484":"df_covid.describe()","c7a3fbd0":"df = df_covid","44abe9d8":"from tqdm import tqdm\nfrom langdetect import detect\nfrom langdetect import DetectorFactory\n\n# set seed\nDetectorFactory.seed = 0\n\n# hold label - language\nlanguages = []\n\n# go through each text\nfor ii in tqdm(range(0,len(df))):\n    # split by space into list, take the first x intex, join with space\n    text = df.iloc[ii]['body_text'].split(\" \")\n    \n    lang = \"en\"\n    try:\n        if len(text) > 50:\n            lang = detect(\" \".join(text[:50]))\n        elif len(text) > 0:\n            lang = detect(\" \".join(text[:len(text)]))\n    # ught... beginning of the document was not in a good format\n    except Exception as e:\n        all_words = set(text)\n        try:\n            lang = detect(\" \".join(all_words))\n        # what!! :( let's see if we can find any text in abstract...\n        except Exception as e:\n            \n            try:\n                # let's try to label it through the abstract then\n                lang = detect(df.iloc[ii]['abstract_summary'])\n            except Exception as e:\n                lang = \"unknown\"\n                pass\n    \n    # get the language    \n    languages.append(lang)","b26177a8":"from pprint import pprint\n\nlanguages_dict = {}\nfor lang in set(languages):\n    languages_dict[lang] = languages.count(lang)\n    \nprint(\"Total: {}\\n\".format(len(languages)))\npprint(languages_dict)","5faa82c5":"df['language'] = languages\nplt.bar(range(len(languages_dict)), list(languages_dict.values()), align='center')\nplt.xticks(range(len(languages_dict)), list(languages_dict.keys()))\nplt.title(\"Distribution of Languages in Dataset\")\nplt.show()","a24c37e2":"df = df[df['language'] == 'en'] \ndf.info()","d14fa5ac":"!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz\nimport en_core_sci_lg","990aa75f":"#NLP \nfrom IPython.utils import io\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\n","b068fd58":"import string\n\npunctuations = string.punctuation\nstopwords = list(STOP_WORDS)\nstopwords[:10]","142696e4":"custom_stop_words = [\n    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n    'al.', 'Elsevier', 'PMC', 'CZI', 'www'\n]\n\nfor w in custom_stop_words:\n    if w not in stopwords:\n        stopwords.append(w)","91cef0b6":"import re\n# Parser\nparser = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\nparser.max_length = 7000000\n\ndef spacy_tokenizer(sentence):\n    mytokens = parser(sentence)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n    mytokens = [ re.sub('[0-9%]','',word) for word in mytokens ]\n    mytokens = \" \".join([i for i in mytokens])\n    return mytokens","38bf3381":"from tqdm import tqdm\nimport numpy as np\nimport math","69ad48fe":"tqdm.pandas()\ndf[\"processed_text\"] = df[\"abstract\"].progress_apply(spacy_tokenizer)","49cd67b2":"df['abstract'].replace('', np.nan, inplace=True)\ndf.dropna(inplace=True)\ndf.info()","6550e92c":"df['processed_word_count'] = df[\"processed_text\"].apply(lambda x: len(x.strip().split()))\nsns.distplot(df['processed_word_count'])\n","e1185708":"df['abstract_word_count'].describe()","ef788a91":"from sklearn.feature_extraction.text import TfidfVectorizer\ntext = df['processed_text'].values\nvectorizer = TfidfVectorizer(max_features=4096)\nX = vectorizer.fit_transform(text) \nterms = vectorizer.get_feature_names()","9c576a94":"terms[0:20]","d35fa504":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=0.95, random_state=42)\nX_reduced= pca.fit_transform(X.toarray())\nX_reduced.shape","2bdfc4ee":"from sklearn.cluster import KMeans","6f0c6c47":"Image(filename='\/kaggle\/input\/kaggle-resources\/kmeans.PNG', width=800, height=800)","81f41bcc":"# from sklearn import metrics\n# from scipy.spatial.distance import cdist\n# %matplotlib inline\n# from matplotlib import pyplot as plt\n\n# # run kmeans with many different k\n# distortions = []\n# K = range(10, 35)\n# for k in tqdm(K):\n#     k_means = KMeans(n_clusters=k, random_state=42).fit(X_reduced)\n#     k_means.fit(X_reduced)\n#     distortions.append(sum(np.min(cdist(X_reduced, k_means.cluster_centers_, 'euclidean'), axis=1)) \/ X.shape[0])\n#     #print('Found distortion for {} clusters'.format(k))","7dcb4801":"# X_line = [K[0], K[-1]]\n# Y_line = [distortions[0], distortions[-1]]\n\n# # Plot the elbow\n# plt.plot(K, distortions, 'b-')\n# plt.plot(X_line, Y_line, 'r')\n# plt.xlabel('k')\n# plt.ylabel('Distortion')\n# plt.title('The Elbow Method showing the optimal k')\n# plt.show()","cd948ad2":"k = 20\nkmeans = KMeans(n_clusters=k, random_state=42)\ny_pred = kmeans.fit_predict(X_reduced)\ndf['y'] = y_pred","292e9c1d":"order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]","433feefd":"for i in range(20):\n    print('Cluster %d:' % i),\n    for ind in order_centroids[i, :20]:\n        print('%s' % terms[ind])","8c10994b":"# from sklearn.manifold import TSNE\n\n# tsne = TSNE(verbose=1, perplexity=100, random_state=42)\n# X_embedded = tsne.fit_transform(X.toarray())","8dd7a367":"# # sns settings\n# sns.set(rc={'figure.figsize':(15,15)})\n\n# # colors\n# palette = sns.color_palette(\"bright\", 1)\n\n# # plot\n# sns.scatterplot(X_embedded[:,0], X_embedded[:,1], palette=palette)\n# plt.title('t-SNE with no Labels')\n# plt.savefig(\"t-sne_covid19.png\")\n# plt.show()","07fa08c7":"# %matplotlib inline\n# from matplotlib import pyplot as plt\n# import seaborn as sns\n\n# # sns settings\n# sns.set(rc={'figure.figsize':(15,15)})\n\n# # colors\n# palette = sns.hls_palette(20, l=.4, s=.9)\n\n# # plot\n# sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y_pred, legend='full', palette=palette)\n# plt.title('t-SNE with Kmeans Labels')\n# plt.savefig(\"improved_cluster_tsne.png\")\n# plt.show()","d7fea107":"from sklearn.neighbors import KNeighborsClassifier","6936b0f3":"knn_model = KNeighborsClassifier(n_neighbors=10)\nknn_model.fit(X_reduced,df['y'].values )","a6c2f4ab":"def predict_nearest_neighbour(model,sentence):\n    print(\"Prediction\")\n    sentence = spacy_tokenizer(sentence)\n    print (sentence)\n    X = vectorizer.transform([sentence])\n    X = pca.transform(X.toarray())\n    predicted = model.kneighbors(X, 100)\n    return predicted","6fddb2a8":"sentence = \"What has been published about ethical and social science considerations?\"\nsentence_1 = 'What do we know about virus genetics, origin and evolution ?'\nsentence_2 = 'What is known about transmission, incubation, and environmental stability?'\nsentence_3 = 'Create summary tables that address risk factors related to COVID-19'\nsentence_4 = 'What do we know about COVID-19 risk factors?'\nsentence_5 = 'What has been published about medical care?'\nsentence_6 = 'What do we know about diagnostics and surveillance?'\nsentence_7 = 'What do we know about vaccines and therapeutics?'","430dcc40":"res = predict_nearest_neighbour(knn_model, sentence_1)\nprint (res)","057a8199":"inds =[]\nfor i,dist in enumerate(res[0][0]):\n    if dist > 1:\n        inds.append(res[1][0][i])\nprint (inds)\n        \n    ","334291fa":"for i in range(10):\n    index = inds[i]\n    print (df.iloc[index]['abstract'])\n    print (\"virus appeared\", df.iloc[index]['abstract'].count('virus'), 'times')\n    print (\"genetic appeared\", df.iloc[index]['abstract'].count('genetic'), 'times')\n    print (\"origin appeared\", df.iloc[index]['abstract'].count('origin'), 'times')\n    print (\"evolution appeared\", df.iloc[index]['abstract'].count('evolution'), 'times')","6eab8110":"for i in range(10):\n    index = inds[i]\n    print (df.iloc[index])","2313e4fb":"res = predict_nearest_neighbour(knn_model, sentence_4)\nprint (res)","92fbda62":"inds =[]\nfor i,dist in enumerate(res[0][0]):\n    if dist > 1:\n        inds.append(res[1][0][i])\nprint (inds)","fc5b3444":"for i in range(10):\n    index = inds[i]\n    print (df.iloc[index]['abstract'])\n    print (\"risk appeared\", df.iloc[index]['abstract'].count('risk'), 'times')\n    print (\"factor appeared\", df.iloc[index]['abstract'].count('factor'), 'times')\n    print (\"COVID-19\", df.iloc[index]['abstract'].count('covid-19'), 'times')","c45a96a1":"for i in range(10):\n    index = inds[i]\n    print (df.iloc[index])","f662f803":"import pickle\npickle.dump(kmeans, open(\"k-means_abstract.pkl\", \"wb\"))\npickle.dump(knn_model, open(\"knn_model_abstract.pkl\", \"wb\"))\n\n#kmeans = pickle.load(open(\"k-means_model.pkl\", \"rb\"))","9c695419":"Dimensionality Reduction with t-SNE\nUsing t-SNE we can reduce our high dimensional features vector to 2 dimensions. By using the 2 dimensions as x,y coordinates, the processed_text can be plotted.\n\nt-Distributed Stochastic Neighbor Embedding (t-SNE) reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. It is mostly used for visualization, in particular to visualize clusters of instances in high-dimensional space","9ecced6e":"### Helper Functions","368e73ad":"### Stopwords\n\nPart of the preprocessing will be finding and removing stopwords (common words that will act as noise in the clustering step).","2269040a":"# Goal\nGiven the large number of literature and the rapid spread of COVID-19, it is difficult for health professionals to keep up with new information on the virus. Can \nclustering similar research articles together to simplify the search for related publications? How can the content of the clusters be qualified? Is clustering only \nsufficient to get the related papers to the question\/s? or we need to use a classification algorithm to solve this problem.  \n\nBy using unsupervised clustering algorithm (K-means clustering) for labelling in combination with dimensionality reduction for visualization, the collection of \nliterature can be represented by a scatter plot. On this plot, publications of highly similar topic will share a label and will be plotted near each other.\n\nIn order to answer the question\/s with the most similar papers, we used the supervised classification algorithm k-nearest neighbors (KNN) as it takes the data \nlabels from the clustering stage.\n\nThis is a difficult time in which health care workers, sanitation staff, and many other essential personnel are out there keeping the world afloat. While adhering \nto quarantine protocol, the Kaggle CORD-19 competition has given us an opportunity to help in the best way we can as computer science students. It should be noted, \nhowever, that we are not epidemiologists, and it is not our place to gauge the importance of these papers. This tool was created to help make it easier for trained \nprofessionals to sift through many, many publications related to the virus, and find their own determinations.\n","862a7fdc":"[source](https:\/\/en.wikipedia.org\/wiki\/K-means_clustering)","cfd8aff4":"# Table of Contents\n1. Loading the data\n2. Pre-processing\n3. Vectorization\n4. PCA  & k_means Clustering\n5. Dimensionality Reduction with t-SNE\n6. Visualization Plot for clustering\n7. Classify using KNN\n8. Conclusion\n9. Citation\/Sources","b922187c":"## Take a Look at the Data:","d9bfaaca":"To separate the literature, k-means will be run on the vectorized text. Given the number of clusters, k, k-means will categorize each vector by taking the mean distance to a randomly initialized centroid. The centroids are updated iteratively.","9afd7339":"#### The documents' summary table","51d119b3":"#### let's see each cluster with its features","ab26bdc0":"### terms indicate the chosen features from the vectorizer , let's take a look on some of them","364b15f6":"### Using Knn to get the nearest  neighbours to the question","0be21070":"#### For question 1 ","6b19d0db":"### How many clusters? \n\nTo find the best k value for k-means we'll look at the distortion at different k values. Distortion computes the sum of squared distances from each point to its assigned center. When distortion is plotted against k there will be a k value after which decreases in distortion are minimal. This is the desired number of clusters.\n\n##### Uncomment those cells to watch the elbow , it takes some time","0044f496":"# Vectorization\n\nNow that we have pre-processed the data, it is time to convert it into a format that can be handled by our algorithms. For this purpose we will be using tf-idf. This will convert our string formatted data into a measure of how important each word is to the instance out of the literature as a whole.\n\nVectorize our data. We will be clustering based off the content of the processed text. The maximum number of features will be limited to 4096 features , eseentially acting as a noise filter. Additionally, more features cause painfully long runtimes.","94f75daf":"Get path to all JSON files:","f12f93f0":"# Conclusion","39527327":"This looks pretty bland. There are some clusters we can immediately detect, but the many instances closer to the center are harder to separate. t-SNE did a good job at reducing the dimensionality, but now we need some labels. Let's use the clusters found by k-means as labels. This will help visually separate different concentrations of topics.","3347395f":"Helper function adds break after every words when character length reach to certain amount. This is for the interactive plot so that hover tool fits the screen.","0eeef8ee":"It looks like we didn't have duplicates. Instead, it was articles without Abstracts.","a910239b":"#### Ten nearest douments related to the question","03c5253f":"# Pros:\n- Training a generalized model to responed faster to questions related to kaggle tasks.\n\n- Using unsubervised clustering technique to have labelled data that are used to train knn supervised classification technique which is simple and efficient.\n\n\n# Cons:\n- The results do not depend on the meaning of the question but search for the related papers according to words matching.\n- choosing abstarct data to train the model leads to dropping of miltiple documents which don't have an abstarct \n","23589722":"### Load the Data into DataFrame","dc50b748":"## Handle Possible Duplicates","96e74009":"In this plot we can see that the better k values are between 18-25. After that, the decrease in distortion is not as significant. For simplicity, we will use k=20","b87f10e7":"# Future thoughts to consider","933baa68":"# PCA  & Clustering\n\nLet's see how much we can reduce the dimensions while still keeping 95% variance. We will apply Principle Component Analysis (PCA) to our vectorized data. The reason for this is that by keeping a large number of dimensions with PCA, you don\u2019t destroy much of the information, but hopefully will remove some noise\/outliers from the data, and make the clustering problem easier for k-means. Note that X_reduced will only be used for k-means, t-SNE will still use the original feature vector X that was generated through tf-idf on the NLP processed text.\n","63fd3b46":"\nWe will be dropping any language that is not English. Attempting to translate foreign texts gave the following problems:\n\n1. API calls were limited\n\n2. Translating the language may not carry over the true semantic meaning of the text\n","29ee825e":"Let's load the metadata of the dateset. 'title' and 'journal' attributes may be useful later when we cluster the articles to see what kinds of articles cluster together.","7a663c61":"## Deploy the model to pickle file","4eecc85c":"So that step took a while! Let's take a look at what our data looks like when compressed to 2 dimensions.","3467cc8d":"### Loading Metadata","97684718":"### Use knn to get the nearest neighbours to different questions","1274cf33":"####  we need to clean-up the text to improve any clustering or classification efforts. First, let's drop Null vales","1795d1c5":"Now that we have an appropriate k value, we can run k-means on the PCA-processed feature vector (X_reduced). ","0dc825a4":"After loading the data we used **langdetect** to detect documents' languages to remove non english documents ,\nafter that we used spacy_tokenizer in our data pre-processing where it lemmitize the words , make them all small alphapetically and we removed numbers and null values  from them .\nWe used TF_IDF vectorizer for features abstraction , and used PCA for dimensionality reduction for more efficiency and  too many features take more running time .\n\nWe used k-means clustering  as an unsupervised approach as we don't have labels , after clustering the documents we used k-nearest neighbours to get the closest documents to our question.","d035c6d6":"# 1) Loading the Data\nLoad the data following the notebook by Ivan Ega Pratama, from Kaggle.\n#### Cite: [Dataset Parsing Code | Kaggle, COVID EDA: Initial Exploration Tool](https:\/\/www.kaggle.com\/ivanegapratama\/covid-eda-initial-exploration-tool)","f7d898fb":" File Reader Class","b72ef8ce":"# Table of Contents\n1. Loading the data\n2. Pre-processing\n3. Vectorization\n4. PCA  & Clustering\n5. Dimensionality Reduction with t-SNE\n6. Classification with KNN\n7. How to Use the Plot?\n8. Conclusion","33fe9a4b":"### Remove all samples with emtpy abstract","883d17f7":"When we look at the unique values above, we can see that there are duplicates. It may have caused because of author submiting the article to multiple journals. Let's remove the duplicats from our dataset:\n\n(Thank you Desmond Yeoh for recommending the below approach on Kaggle)","2f7d41b1":"## Open questions and literature ranking related to COVID-19 ","81d7d09a":"# Approach:\n\n- Parse the text from the body of each document using Natural Language Processing (NLP).\n- Turn each document instance $d_i$ into a feature vector $X_i$ using Term Frequency\u2013inverse Document Frequency (TF-IDF).\n- Apply Dimensionality Reduction to each feature vector $X_i$ using t-Distributed Stochastic Neighbor Embedding (t-SNE) to cluster similar research articles in the \ntwo dimensional plane $X$ embedding $Y_1$.\n- Use Principal Component Analysis (PCA) to project down the dimensions of $X$ to a number of dimensions that will keep .95 variance while removing noise and \noutliers in embedding $Y_2$.\n- Apply k-means clustering on $Y_2$, where $k$ is 28, to label each cluster on $Y_1$ (the suitable k was determined by the Elbow method).\n- Apply classification on the given question\/s to find the highest 10 related papers among the given dataset using KNN with the aid of clustering labels. \n","2537b9fe":"Lets take a look at the language distribution in the dataset","346da646":"# Data Pre-processing","b56c6f3a":"#### To focus on features using main needed words related to the topic , we chose to work with processed text of the abstract column\n       ","0f1b706f":"### Next lets create a function that will process the text data for us. \nFor this purpose we will be using the spacy library. This function will convert text to lower case, remove punctuation, and find and remove stopwords. For the parser, we will use en_core_sci_lg. This is a model for processing biomedical, scientific or clinical text.","b8c6c881":"## Some feature engineering\nAdding word count columns for both abstract and body_text can be useful parameters later:","abfdb322":"# Dimensionality Reduction with t-SNE","858cdfc4":"### Run k-means","75fa1f9c":"1) Topic modelling to discover the topic of each cluster ,  so after applying KNN algorithm we can vote for the major \n    label to indicate the topic related to this question so that results will be more accurate\n\n2) Using of alternative approach such as neural networks LSTM to classify based on words meanining instead of our\n    approach in TF-IDF that rely on the word count and its frequency inside the whole documents .","425b7fd6":"### Fetch All of JSON File Path","e58c905b":"### Handling multiple languages\nWe going to determine the language of each paper in the dataframe. Not all of the sources are English and the language needs to be identified so that we know how handle these instances","7ed16888":"### visualizing processed text","d5bfb2da":"### Trying question 4","0052c721":"Using the helper functions, let's read in the articles into a DataFrame that can be used easily:"}}