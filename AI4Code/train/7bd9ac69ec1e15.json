{"cell_type":{"8173c732":"code","c90a0d51":"code","f1b21632":"code","b9490e0b":"code","1a3e213f":"code","15cebea4":"code","4792ceae":"code","bf19f612":"code","825f235d":"code","9c96eb28":"code","a33e71df":"code","e3411fa8":"code","eddf5d5b":"code","0234b30f":"code","07e90b86":"code","7cb9b671":"code","0014f12a":"code","d5002525":"code","ec30e084":"code","247c8794":"code","5a11a58e":"code","3b63dab7":"code","196b96df":"code","5d9c8bbe":"code","d8e35486":"code","6d2bdbf6":"code","c44f5b50":"code","74f1a73f":"code","bbf2eb67":"code","8164d92b":"code","d59a60a6":"code","2974be3b":"code","195423c0":"code","f83fc2f9":"code","5e7e4e23":"code","53cccbe0":"code","436bc038":"code","97558836":"code","251e384e":"code","6ac58816":"code","4db4f527":"code","d5bdc8d8":"code","d3b80e4d":"code","251e8fff":"code","19a8a538":"code","016d1950":"code","d876060a":"code","0eea1fde":"code","00709078":"code","fbab3f70":"code","ddfcefda":"markdown","1f7c63f9":"markdown","1cee4896":"markdown","2c4fbdf0":"markdown","8075704f":"markdown","51790177":"markdown","d5668606":"markdown","031caf33":"markdown","7036209f":"markdown","0556a176":"markdown","b3df76df":"markdown","557d1b7b":"markdown","0f0e51c2":"markdown","f8035fb7":"markdown","93efb044":"markdown","b37f5e27":"markdown","d1146abf":"markdown","38a4174c":"markdown","6799f920":"markdown","e217ad25":"markdown","077fbb00":"markdown","7ab2b50b":"markdown","7d809822":"markdown","3a12af0c":"markdown","1c5b1fc7":"markdown","7fe0c89d":"markdown","de5c3f26":"markdown","ca42a0ec":"markdown","848d487b":"markdown","579976e2":"markdown","855011ed":"markdown","6b36f086":"markdown","ffd77670":"markdown","be522819":"markdown","7eb11f55":"markdown","f736ef4d":"markdown","4d8b0d15":"markdown"},"source":{"8173c732":"# Libraries\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\npd.set_option('max_columns', None)\nimport json\nimport ast\nimport time\nimport datetime\nimport os\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n# \uc815\uc6b0\uc77c\ub2d8\uc758 \uad00\ub828 \ube14\ub85c\uadf8 https:\/\/wooiljeong.github.io\/python\/python_plotly\/\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.feature_selection import GenericUnivariateSelect, SelectPercentile, SelectKBest, f_classif, mutual_info_classif, RFECV\n\n\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom sklearn import linear_model\nimport statsmodels.api as sm\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport shap\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n\nimport warnings\nwarnings.filterwarnings('ignore')","c90a0d51":"path = '\/kaggle\/input\/dont-overfit-ii'\ntrain = pd.read_csv(f'{path}\/train.csv')\ntest = pd.read_csv(f'{path}\/test.csv')\ntrain.shape","f1b21632":"test.shape","b9490e0b":"train.head()","1a3e213f":"train[train.columns[2:]].std().plot('hist');\nplt.title('Distribution of stds of all columns');","15cebea4":"train[train.columns[2:]].mean().plot('hist');\nplt.title('Distribution of means of all columns');","4792ceae":"# we have no missing values\ntrain.isnull().any().any()","bf19f612":"print('Distributions of first 28 columns')\nplt.figure(figsize=(26, 24))\nfor i, col in enumerate(list(train.columns)[2:30]):\n    plt.subplot(7, 4, i + 1)\n    plt.hist(train[col])\n    plt.title(col)","825f235d":"train['target'].value_counts()","9c96eb28":"corr = train[train.columns[2:]].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(25, 25))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5)","a33e71df":"corrs = train.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrs = corrs[corrs['level_0'] != corrs['level_1']]\ncorrs.tail(10)","e3411fa8":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nX_test = test.drop(['id'], axis=1)","eddf5d5b":"prediction = np.zeros(len(X_test))\nscores_train = []\nscores_valid = []\nfolds = StratifiedKFold(n_splits=20, shuffle=True, random_state=42)","0234b30f":"for fold_n, (train_index, valid_index) in enumerate(folds.split(X_train, y_train)):\n    X_train_fold, X_valid_fold = X_train.loc[train_index], X_train.loc[valid_index]\n    y_train_fold, y_valid_fold = y_train[train_index], y_train[valid_index]\n    break","07e90b86":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\nmodel.fit(X_train_fold, y_train_fold)\ny_pred_train = model.predict(X_train_fold).reshape(-1,)\ntrain_score = roc_auc_score(y_train_fold, y_pred_train)\n\ny_pred_valid = model.predict(X_valid_fold).reshape(-1,)\nvalid_score = roc_auc_score(y_valid_fold, y_pred_valid)\n","7cb9b671":"print(f'Train auc: {train_score:.4}. Valid auc: {valid_score:.4}')","0014f12a":"def train_model(X_train, y_train, X_test, folds=folds, model=None):\n    prediction = np.zeros(len(X_test))\n    scores_train = []\n    scores_valid = []\n    \n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X_train, y_train)):\n        X_train_fold, X_valid_fold = X_train[train_index], X_train[valid_index]\n        y_train_fold, y_valid_fold = y_train[train_index], y_train[valid_index]\n        \n        model.fit(X_train_fold, y_train_fold)\n        y_pred_train = model.predict(X_train_fold).reshape(-1,)\n        train_score = roc_auc_score(y_train_fold, y_pred_train)\n        scores_train.append(train_score)\n        \n        y_pred_valid = model.predict(X_valid_fold).reshape(-1,)\n        valid_score = roc_auc_score(y_valid_fold, y_pred_valid)\n        scores_valid.append(valid_score)\n\n        y_pred = model.predict_proba(X_test)[:, 1]\n        prediction += y_pred\n\n    prediction \/= folds.get_n_splits()\n    \n    print(f'Mean train auc: {np.mean(scores_train):.4f}, std: {np.std(scores_train):.4f}.')\n    print(f'Mean valid auc: {np.mean(scores_valid):.4f}, std: {np.std(scores_valid):.4f}.')\n    \n    return scores_valid, prediction\n","d5002525":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\nscores, prediction = train_model(X_train.values, y_train, X_test, folds=folds,  model=model)","ec30e084":"repeated_folds = RepeatedStratifiedKFold(n_splits=20, n_repeats=5, random_state=42)","247c8794":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\nscores, prediction = train_model(X_train.values, y_train, X_test, folds=repeated_folds, model=model)","5a11a58e":"eli5.show_weights(model, top=50)","3b63dab7":"(model.coef_ != 0).sum()","196b96df":"top_features = [i[1:] for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i]","5d9c8bbe":"for i, coef in enumerate(model.coef_[0]):\n    if coef != 0:\n        print(f'Feature {X_train.columns[i]} has coefficient {coef:.4f}')","d8e35486":"X_train_selected = train[top_features]\ny_train = train['target']\nX_test_selected = test[top_features]","6d2bdbf6":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\nscores, prediction = train_model(X_train_selected.values, y_train, X_test_selected, folds=repeated_folds, model=model)","c44f5b50":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nX_test = test.drop(['id'], axis=1)\nmodel = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\nscores, prediction = train_model(X_train.values, y_train, X_test, folds=repeated_folds,  model=model)","74f1a73f":"submission = pd.read_csv(f'{path}\/sample_submission.csv')\nsubmission['target'] = prediction\nsubmission.to_csv('submission_3.csv', index=False)","bbf2eb67":"perm = PermutationImportance(model, random_state=1).fit(X_train, y_train)\neli5.show_weights(perm, top=50)","8164d92b":"eli5.formatters.as_dataframe.explain_weights_df(perm).head()","d59a60a6":"eli5.formatters.as_dataframe.explain_weights_df(perm).loc[eli5.formatters.as_dataframe.explain_weights_df(perm)['weight'] != 0].shape","2974be3b":"selected_weights = eli5.formatters.as_dataframe.explain_weights_df(perm).loc[eli5.formatters.as_dataframe.explain_weights_df(perm)['weight'] != 0]","195423c0":"top_features = [i[1:] for i in selected_weights.feature if 'BIAS' not in i]\nX_train_selected = train[top_features]\ny_train = train['target']\nX_test_selected = test[top_features]","f83fc2f9":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\nscores, prediction = train_model(X_train_selected.values, y_train, X_test_selected, folds=repeated_folds, model=model)","5e7e4e23":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\nscores, prediction = train_model(X_train.values, y_train, X_test, folds=repeated_folds, model=model)","53cccbe0":"explainer = shap.LinearExplainer(model, X_train)\nshap_values = explainer.shap_values(X_train)\n\nshap.summary_plot(shap_values, X_train)","436bc038":"def train_model_with_feature_selection(X_train, y_train, X_test, folds=folds, model=None, feature_selector=None):\n    prediction = np.zeros(len(X_test))\n    scores_train = []\n    scores_valid = []\n    \n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X_train, y_train)):\n        X_train_fold, X_valid_fold = X_train[train_index], X_train[valid_index]\n        y_train_fold, y_valid_fold = y_train[train_index], y_train[valid_index]\n        # so that we don't transform the original test data\n        X_test_copy = X_test.copy()\n        \n        feature_selector.fit(X_train_fold, y_train_fold)\n        X_train_fold = feature_selector.transform(X_train_fold)\n        X_valid_fold = feature_selector.transform(X_valid_fold)\n        X_test_copy = feature_selector.transform(X_test_copy)\n        \n        model.fit(X_train_fold, y_train_fold)\n        y_pred_train = model.predict(X_train_fold).reshape(-1,)\n        train_score = roc_auc_score(y_train_fold, y_pred_train)\n        scores_train.append(train_score)\n        \n        y_pred_valid = model.predict(X_valid_fold).reshape(-1,)\n        valid_score = roc_auc_score(y_valid_fold, y_pred_valid)\n        scores_valid.append(valid_score)\n\n        y_pred = model.predict_proba(X_test_copy)[:, 1]\n        prediction += y_pred\n\n    prediction \/= folds.get_n_splits()\n    \n    print(f'Mean train auc: {np.mean(scores_train):.4f}, std: {np.std(scores_train):.4f}.')\n    print(f'Mean valid auc: {np.mean(scores_valid):.4f}, std: {np.std(scores_valid):.4f}.')\n    \n    return scores_valid, prediction\n","97558836":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\nfeature_selector = RFECV(model, min_features_to_select=10, scoring='roc_auc', step=0.1, verbose=0, cv=repeated_folds, n_jobs=-1)\nscores, prediction = train_model_with_feature_selection(X_train.values, y_train, X_test, folds=repeated_folds, model=model, feature_selector=feature_selector)","251e384e":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']","6ac58816":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\nprint('Default scores')\nscores, prediction = train_model(X_train.values, y_train, X_test, folds=folds, model=model)\nprint()\ntop_features = [i[1:] for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i]\nX_train_selected = train[top_features]\ny_train = train['target']\nX_test_selected = test[top_features]\n\nlr = linear_model.LogisticRegression(max_iter=1000)\n\nparameter_grid = {'class_weight' : ['balanced', None],\n                  'penalty' : ['l2', 'l1'],\n                  'C' : [0.001, 0.05, 0.08, 0.01, 0.1, 1.0, 10.0],\n                  'solver': ['liblinear']\n                 }\n\ngrid_search = GridSearchCV(lr, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train_selected, y_train)\nprint(f'Best score of GridSearchCV: {grid_search.best_score_}')\nprint(f'Best parameters: {grid_search.best_params_}')\n\nprint()\nscores_logreg, prediction = train_model(X_train_selected.values, y_train, X_test_selected, folds=repeated_folds, model=grid_search.best_estimator_)","4db4f527":"model = AdaBoostClassifier()\nprint('Default scores')\nscores, prediction = train_model(X_train.values, y_train, X_test, folds=folds, model=model)\nprint()\ntop_features = [i[1:] for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i]\nX_train_selected = train[top_features]\ny_train = train['target']\nX_test_selected = test[top_features]\n\n\nabc = AdaBoostClassifier()\n\nparameter_grid = {'n_estimators': [5, 10, 20, 50, 100],\n                  'learning_rate': [0.001, 0.01, 0.1, 1.0, 10.0]\n                 }\n\ngrid_search = GridSearchCV(abc, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train_selected, y_train)\nprint(f'Best score of GridSearchCV: {grid_search.best_score_}')\nprint(f'Best parameters: {grid_search.best_params_}')\n\nprint()\nscores_abc, prediction = train_model(X_train_selected.values, y_train, X_test_selected, folds=repeated_folds, model=grid_search.best_estimator_)","d5bdc8d8":"model = linear_model.SGDClassifier(eta0=1, max_iter=1000, tol=0.0001, loss='modified_huber')\nprint('Default scores')\nscores, prediction = train_model(X_train.values, y_train, X_test, folds=folds, model=model)\nprint()\ntop_features = [i[1:] for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i]\nX_train_selected = train[top_features]\ny_train = train['target']\nX_test_selected = test[top_features]\n\nsgd = linear_model.SGDClassifier(eta0=1, max_iter=1000, tol=0.0001)\n\nparameter_grid = {'loss': ['log', 'modified_huber'],\n                  'penalty': ['l1', 'l2', 'elasticnet'],\n                  'alpha': [0.001, 0.01, 0.1, 0.5],\n                  'l1_ratio': [0, 0.15, 0.5, 1.0],\n                  'learning_rate': ['optimal', 'invscaling', 'adaptive']\n                 }\n\ngrid_search = GridSearchCV(sgd, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train_selected, y_train)\nprint(f'Best score of GridSearchCV: {grid_search.best_score_}')\nprint(f'Best parameters: {grid_search.best_params_}')\n\nprint()\nscores_sgd, prediction = train_model(X_train_selected.values, y_train, X_test_selected, folds=repeated_folds, model=grid_search.best_estimator_)","d3b80e4d":"model = SVC(probability=True, gamma='scale')\nprint('Default scores')\nscores, prediction = train_model(X_train.values, y_train, X_test, folds=folds, model=model)\nprint()\nperm = PermutationImportance(model, random_state=1).fit(X_train, y_train)\nselected_weights = eli5.formatters.as_dataframe.explain_weights_df(perm).loc[eli5.formatters.as_dataframe.explain_weights_df(perm)['weight'] != 0]\ntop_features = [i[1:] for i in selected_weights.feature if 'BIAS' not in i]\nX_train_selected = train[top_features]\ny_train = train['target']\nX_test_selected = test[top_features]\n\nsvc = SVC(probability=True, gamma='scale')\n\nparameter_grid = {'C': [0.01, 0.1, 1.0, 10.0, 100.0],\n                  'kernel': ['linear', 'poly', 'rbf'],\n                 }\n\ngrid_search = GridSearchCV(svc, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train_selected, y_train)\nprint(f'Best score of GridSearchCV: {grid_search.best_score_}')\nprint(f'Best parameters: {grid_search.best_params_}')\n\nprint()\nscores_svc, prediction = train_model(X_train_selected.values, y_train, X_test_selected, folds=repeated_folds, model=grid_search.best_estimator_)","251e8fff":"plt.figure(figsize=(12, 8));\nscores_df = pd.DataFrame({'LogisticRegression': scores_logreg})\nscores_df['AdaBoostClassifier'] = scores_abc\nscores_df['SGDClassifier'] = scores_sgd\nscores_df['SVC'] = scores_svc\n\nsns.boxplot(data=scores_df);\nplt.xticks(rotation=45);","19a8a538":"X_train['mean'] = X_train.mean(axis=1)\nX_train['kurt'] = X_train.kurt(axis=1)\nX_train['mad'] = X_train.mad(axis=1)\nX_train['median'] = X_train.median(axis=1)\nX_train['max'] = X_train.max(axis=1)\nX_train['min'] = X_train.min(axis=1)\nX_train['skew'] = X_train.skew(axis=1)\nX_train['sem'] = X_train.sem(axis=1)\n\nX_test['mean'] = X_test.mean(axis=1)\nX_test['kurt'] = X_test.kurt(axis=1)\nX_test['mad'] = X_test.mad(axis=1)\nX_test['median'] = X_test.median(axis=1)\nX_test['max'] = X_test.max(axis=1)\nX_test['min'] = X_test.min(axis=1)\nX_test['skew'] = X_test.skew(axis=1)\nX_test['sem'] = X_test.sem(axis=1)","016d1950":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\nprint('Default scores')\nscores, prediction = train_model(X_train.values, y_train, X_test, folds=folds, model=model)\nprint()\ntop_features = itemgetter([int(i[1:]) for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i])(X_train.columns)\nX_train_selected = X_train[top_features]\ny_train = train['target']\nX_test_selected = X_test[top_features]\n\nlr = linear_model.LogisticRegression(max_iter=1000)\n\nparameter_grid = {'class_weight' : ['balanced', None],\n                  'penalty' : ['l2', 'l1'],\n                  'C' : [0.001, 0.05, 0.08, 0.01, 0.1, 1.0, 10.0],\n                  'solver': ['liblinear']\n                 }\n\ngrid_search = GridSearchCV(lr, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train_selected, y_train)\nprint(f'Best score of GridSearchCV: {grid_search.best_score_}')\nprint(f'Best parameters: {grid_search.best_params_}')\n\nprint()\nscores_logreg, prediction = train_model(X_train_selected.values, y_train, X_test_selected, folds=repeated_folds, model=grid_search.best_estimator_)","d876060a":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nX_test = test.drop(['id'], axis=1)","0eea1fde":"sc = StandardScaler()\ndata = StandardScaler().fit_transform(np.concatenate((X_train, X_test), axis=0))\nX_train.iloc[:, :] = data[:250]\nX_test.iloc[:, :] = data[250:]","00709078":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\nprint('Default scores')\nscores, prediction = train_model(X_train.values, y_train, X_test, folds=folds, model=model)\nprint()\ntop_features = itemgetter([int(i[1:]) for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i])(X_train.columns)\nX_train_selected = X_train[top_features]\ny_train = train['target']\nX_test_selected = X_test[top_features]\n\nlr = linear_model.LogisticRegression(max_iter=1000)\n\nparameter_grid = {'class_weight' : ['balanced', None],\n                  'penalty' : ['l2', 'l1'],\n                  'C' : [0.001, 0.05, 0.08, 0.01, 0.1, 1.0, 10.0],\n                  'solver': ['liblinear']\n                 }\n\ngrid_search = GridSearchCV(lr, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train_selected, y_train)\nprint(f'Best score of GridSearchCV: {grid_search.best_score_}')\nprint(f'Best parameters: {grid_search.best_params_}')\n\nprint()\nscores_logreg, prediction = train_model(X_train_selected.values, y_train, X_test_selected, folds=repeated_folds, model=grid_search.best_estimator_)","fbab3f70":"submission = pd.read_csv(f'{path}\/sample_submission.csv')\nsubmission['target'] = prediction\nsubmission.to_csv('submission.csv', index=False)","ddfcefda":"\uc810\uc218\uac00 \uc870\uae08 \uc99d\uac00\ud588\uc2b5\ub2c8\ub2e4!","1f7c63f9":"* \ubaa8\ub4e0 \uc5f4\uc758 \ud3c9\uade0 \uac12\uc774 -0.2\uc640 0.15 \uc0ac\uc774\uc784\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4\t\n* \ud45c\uc900 \ud3b8\ucc28\ub294 \ub9e4\uc6b0 \uc791\uc2b5\ub2c8\ub2e4\n* \uc6b0\ub9ac\ub294 \ud56d\ubaa9\uc774 \uc11c\ub85c \ub9e4\uc6b0 \ube44\uc2b7\ud558\ub2e4\uace0 \ub9d0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4","1cee4896":"\uac00\uc911\uce58\uac00 \ub9e4\uc6b0 \ub192\uc740 \ud56d\ubaa9\uacfc \uac00\uc911\uce58\uac00 \ub9c8\uc774\ub108\uc2a4\uc778 \ub354 \ub9ce\uc740 \ud56d\ubaa9\uc774 \uc788\uc74c\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4\t\n\n\uc2e4\uc81c\ub85c ELI5\uc5d0 \ub530\ub974\uba74 \uc911\uc694\ud55c \ud56d\ubaa9\uc740 32\uac1c\ub9cc \uc788\uc2b5\ub2c8\ub2e4\t\n\n\uc774 \ud56d\ubaa9\ub4e4\ub9cc \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 \uad6c\ucd95\ud574 \ubd05\uc2dc\ub2e4","2c4fbdf0":"## Approaches to feature selection\n\nfeature selection\uc774 \ubb34\uc5c7\uc774\uace0 \uc65c \uc911\uc694\ud55c\uc9c0 \uc124\uba85\ud558\uaca0\uc2b5\ub2c8\ub2e4","8075704f":"\ud56d\ubaa9 \uac04\uc758 \uc0c1\uad00 \uad00\uacc4\uac00 0.3\ubcf4\ub2e4 \ub0ae\uace0 target\uacfc \uac00\uc7a5 \uad00\ub828\uc774 \ub192\uc740 \ud53c\ucc98\uc758 \uc0c1\uad00 \uad00\uacc4\ub294 03.3\uc785\ub2c8\ub2e4\t\n\n\ub530\ub77c\uc11c \uc81c\uac70 \ud560 \uc218 \uc788\ub294 \uc0c1\uad00 \uad00\uacc4\uac00 \ub192\uc740 \ud56d\ubaa9\uc774 \uc5c6\uc73c\uba70\ttarget\uacfc \uc0c1\uad00 \uad00\uacc4\uac00 \uac70\uc758 \uc5c6\ub294 \uc77c\ubd80 \uc5f4\uc744 \uc0ad\uc81c\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4","51790177":"Important information about ELI5:\n\n\uc2e4\uc81c\ub85c \ub9e4\uc6b0 \uac04\ub2e8\ud558\uac8c \uc791\ub3d9\ub429\ub2c8\ub2e4\t\nlogistic regression\uc640 \uac19\uc740 \ubaa8\ub378\uc758 model coefficient\ub97c \ubcf4\uc5ec\uc8fc\uac70\ub098 \ub79c\ub364 \ud3ec\ub808\uc2a4\ud2b8\uc640 \uac19\uc740 \ubaa8\ub378\uc758 feature importance\ub97c \ubcf4\uc5ec \uc90d\ub2c8\ub2e4\t\n\nELI5\uc758 \uacb0\uacfc\ub97c model coefficient\uc640 \ube44\uad50\ud574 \ubd05\ub2c8\ub2e4","d5668606":"\n### SHAP\n\n\ub610 \ub2e4\ub978 \ud765\ubbf8\ub85c\uc6b4 \ub3c4\uad6c\ub294 SHAP\uc785\ub2c8\ub2e4\t\n\n\ub2e4\uc591\ud55c \ubaa8\ub378\uc5d0 \ub300\ud55c \uc124\uba85\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4","031caf33":"logistic regression\uac00 \ub300\ubd80\ubd84\uc758 \ub2e4\ub978 \ubaa8\ub378\ubcf4\ub2e4 \uc6b0\uc218\ud558\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4 \n\n\ub2e4\ub978 \ubaa8\ub378\uc740\uc774 \uc791\uc740 \ub370\uc774\ud130 \uc138\ud2b8\uc5d0\uc11c \uacfc\uc801\ud569\ud558\uac70\ub098 \uc791\ub3d9\ud558\uc9c0 \uc54a\ub294 \uac83 \uac19\uc2b5\ub2c8\ub2e4 ","7036209f":"<a id=\"eli5p\"><\/a>\n### Permutation importance\n\nELI5\ub97c \uc798 \uc774\uc6a9\ud558\ub294 \ub2e4\ub978 \ubc29\ubc95\uc774 \ud558\ub098 \ub354 \uc788\uc2b5\ub2c8\ub2e4\n\nPermutation Feature Importance\ub294 \ub370\uc774\ud130\uac00 \ud14c\uc774\ube14 \ud615\uc2dd\uc77c \ub54c \ud6c8\ub828\ub41c estimator\uc5d0 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \ubaa8\ub378 \uac80\uc0ac \uae30\uc220\uc785\ub2c8\ub2e4\t \n\nPermutation Importance\ub294 \ub2e4\uc74c\uacfc \uac19\uc740 \ubc29\uc2dd\uc73c\ub85c \uc791\ub3d9\ud569\ub2c8\ub2e4\t\n\n* \ubaa8\ub378\uc744 \ud6c8\ub828 \uc2dc\ud0b5\ub2c8\ub2e4\n* \ud558\ub098\uc758 \uc720\ud6a8\uc131 \uac80\uc0ac \ub370\uc774\ud130 \uc5f4\uc744 \ubb34\uc791\uc704\ub85c \uc11e\uace0 \uc810\uc218\ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4\n* \uc810\uc218\uac00 \ud06c\uac8c \ub5a8\uc5b4\uc9c0\uba74 \ud56d\ubaa9\uc774 \uc911\uc694\ud558\ub2e4\ub294 \uc758\ubbf8\uc785\ub2c8\ub2e4\n\n\ub9c1\ud06c\ub97c \ud074\ub9ad\ud558\uc2dc\uba74 \ucd94\uac00\uc801\uc73c\ub85c \ub0b4\uc6a9\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4: https:\/\/www.kaggle.com\/dansbecker\/permutation-importance","0556a176":"### Different ways of splitting data into folds\n\n\ub370\uc774\ud130\ub97c \ud3f4\ub4dc\ub85c \ub098\ub204\ub294 \ubc29\ubc95\uc5d0\ub294 \uc5ec\ub7ec \uac00\uc9c0\uac00 \uc788\uc2b5\ub2c8\ub2e4\n* \uac00\uc7a5 \uac04\ub2e8\ud55c \ubc29\ubc95\uc740 \ubb34\uc791\uc704\ub85c \ub098\ub204\ub294 \uac83\uc785\ub2c8\ub2e4: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.KFold.html\n* \uc77c\ubc18\uc801\uc73c\ub85c \ubd84\ub958\uc5d0\ub294 \ub354 \ub098\uc740 \ubc29\ubc95\uc774 \uc788\uae34 \ud569\ub2c8\ub2e4 - https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedKFold.html \n\nStratifiedKFold\ub294 \uacc4\uce35\ud654 \ub41c \ud3f4\ub4dc\ub97c \ubc18\ud658\ud558\ub294 k-\ud3f4\ub4dc\uc758 \ubcc0\ud615\uc785\ub2c8\ub2e4\n\n\uac01 \uc138\ud2b8\uc5d0\ub294 \uac01 \uc138\ud2b8\uc758 \uc0d8\ud50c\uc774 \uc804\uccb4 \uc138\ud2b8\uc640 \uac70\uc758 \uac19\uc740 \ube44\uc728\ub85c \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4\n\n* StratifiedKFold\uc640 \ube44\uc2b7\ud55c RepeatedStratifiedKFold\ub3c4 \uc788\ub294\ub370 https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RepeatedStratifiedKFold.html \uc774\ub294 \uadf8 \uc548\uc5d0\uc11c \uc5ec\ub7ec\ubc88 \ubc18\ubcf5\ub429\ub2c8\ub2e4\n\n\uc6b0\ub9ac\ub294 RepeatedStratifiedKFold\ub85c \ud655\uc778\ud558\uaca0\uc2b5\ub2c8\ub2e4","b3df76df":"## Prepare the data","557d1b7b":"\uacc4\uc218\ub098 \ud56d\ubaa9 \uc911\uc694\ub3c4 \ub4f1\uc774 \uc5c6\uae30 \ub54c\ubb38\uc5d0 SVC\uc5d0\uc11c\ub294 Permutation Importance\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4","0f0e51c2":"\ubaa8\ub378\uc744 \ucd5c\uc801\ud654\ud558\uba74 \uc2e4\uc81c\ub85c auc \uc810\uc218\uac00 \ud5a5\uc0c1\ub429\ub2c8\ub2e4!","f8035fb7":"\uc5f4\uc774 \ub108\ubb34 \ub9ce\uc544\uc11c \uc704\uc5d0\uc11c \ub3c4\uc800\ud788 \uc77d\uc744 \uc218\uac00 \uc5c6\uc2b5\ub2c8\ub2e4.\n\ntop correlated features\ub97c \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.","93efb044":"\uc5ec\uae30\uc5d0 \uc911\uc694\ud55c \uacb0\ub860\uc774 \uc788\uc2b5\ub2c8\ub2e4\n\n\ubaa8\ub378\uc5d0 \uacc4\uc218 \ub610\ub294 \ud56d\ubaa9 \uc911\uc694\ub3c4\uac00 \uc5c6\ub294 \uacbd\uc6b0 ELI5\uac00 \uc791\ub3d9\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4\t\n\nSVC\uac00 \uadf8\ub7f0 \uc608\uc785\ub2c8\ub2e4\t","b37f5e27":"### Recursive feature elimination\n\n","d1146abf":"\n### ELI5\n\nELI5\ub294 ML \ubaa8\ub378\uc5d0 \ub300\ud55c \uc124\uba85\uc744 \uc81c\uacf5\ud558\ub294 \ud328\ud0a4\uc9c0\uc785\ub2c8\ub2e4\t\n\n\uc120\ud615 \ubaa8\ub378\ubfd0\ub9cc \uc544\ub2c8\ub77c \ud2b8\ub9ac \uae30\ubc18 \uc54c\uace0\ub9ac\uc998\uc5d0 \ub300\ud574\uc11c\ub3c4 \uc774\ub97c \uc218\ud589 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4","38a4174c":"## Scaling the data\n\n\ub9c8\uc9c0\ub9c9 \uc694\ub839\uc740 \ub370\uc774\ud130\uc758 \ud06c\uae30\ub97c \uc870\uc815\ud558\ub294 \uac83\uc785\ub2c8\ub2e4\n\n\uc77c\ubc18\uc801\uc73c\ub85c \ub2e4\uc74c\uacfc \uac19\uc740 \uc811\uadfc \ubc29\uc2dd\uc774 \uc788\uc2b5\ub2c8\ub2e4\n\n* \uac01 \ud3f4\ub4dc\uc5d0\uc11c \ub370\uc774\ud130\ub97c \ud2b8\ub808\uc778\uacfc \uac80\uc99d\uc6a9\uc73c\ub85c \ub098\ub208 \ub2e4\uc74c \n* \ud2b8\ub808\uc778\ub370\uc774\ud130 \ubc0f \uac80\uc99d\ub370\uc774\ud130\uc5d0 \uc2a4\ucf00\uc77c\ub7ec\ub97c \uc801\uc6a9\ud55c \ud6c4 \n* \ub2e4\uc2dc \uac80\uc99d \ubc0f \ud14c\uc2a4\ud2b8\uc5d0 \uc801\uc6a9\ud558\ub294 \uac83\uc785\ub2c8\ub2e4\t\n* \uc55e\ucc98\ub7fc \ub2e4\uc2dc \uc9c4\ud589\ud574 \ubd05\ub2c8\ub2e4\t\n\n\uadf8\ub7ec\ub098 Kaggle\uc5d0\uc11c\ub294 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\ub97c \uc989\uc2dc \uc801\uc6a9\ud560 \uc218 \uc788\ub294 \ub3c5\ud2b9\ud55c \uc0c1\ud669\uc785\ub2c8\ub2e4\t\n\n\ub530\ub77c\uc11c \uc0ac\uc6a9 \uac00\ub2a5\ud55c \ubaa8\ub4e0 \ub370\uc774\ud130\uc5d0 \uc2a4\ucf00\uc77c\ub7ec\ub97c \uc801\uc6a9\ud574\ubcf4\uae30\ub3c4 \ud569\ub2c8\ub2e4\t\n\n\ub370\uc774\ud130\ub97c \ub2e4\uc2dc \uc900\ube44\ud558\uace0 stanard scaler\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4","6799f920":"## Feature engineering\n\n\ud56d\ubaa9 \uc0dd\uc131\uc5d0\ub294 \uc5ec\ub7ec \uc811\uadfc \ubc29\uc2dd\uc774 \uc788\uc2b5\ub2c8\ub2e4\t\n\n\uc775\uba85\ud654\ub418\uace0 \ube44\uc2b7\ud55c \ud56d\ubaa9\ub4e4\uc774 \uc788\uc73c\uba74 \ud589\uc744 \uae30\uc900\uc73c\ub85c \ud56d\ubaa9\uc744 \uacc4\uc0b0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4\n\n\uc608\ub97c \ub4e4\uc5b4 \ud589 \ubcc4 \ud3c9\uade0\uac12 \uac19\uc740 \uac83\uc744 \ub9d0\ud569\ub2c8\ub2e4","e217ad25":"\ud568\uc218\ub97c \ub9cc\ub4e4\uc5b4 \ubd05\ub2c8\ub2e4\n\n\uc774\ub294 \ud14c\uc2a4\ud2b8\ub370\uc774\ud130\uc5d0 \ub300\ud55c \uc608\uce21\ub3c4 \ub9cc\ub4e4 \uc218 \uc788\ub294 \uac83\uc785\ub2c8\ub2e4","077fbb00":"\uc774 \uac1c\uc694\uc5d0\uc11c \uc6b0\ub9ac\ub294 \ub2e4\uc74c\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4\n\n* \ub300\uc0c1\uc740 \uc774\uc9c4\uc774\uba70 \uc57d\uac04\uc758 \ubd88\uade0\ud615\uc774 \uc788\uc2b5\ub2c8\ub2e4\n* \uc0d8\ud50c\uc758 26.8 %\uac00 0 \ud074\ub798\uc2a4\uc5d0 \uc18d\ud569\ub2c8\ub2e4\n* \uc5f4\uc758 \uac12\uc740 \ub2e4\uc18c \ube44\uc2b7\ud569\ub2c8\ub2e4","7ab2b50b":"\uba3c\uc800 \uba87 \uac00\uc9c0 \uc0ac\ud56d\uc744 \uc815\uc758 \ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4\t\n\n* Prediction\uc740 \uc6b0\ub9ac\uc758 \uc608\uce21\uc774 \ub420 \uac83\uc785\ub2c8\ub2e4\n* scores_train, scores_valid\uc740 \uc810\uc218 \ub9ac\uc2a4\ud2b8\uc785\ub2c8\ub2e4\n* fold\ub780 \ub370\uc774\ud130\ub97c \ub098\ub204\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4","7d809822":"\uad6c\uae00 \ucf5c\ub7a9\uc5d0\uc11c \uc0ac\uc6a9\ud558\uc2e4 \ub54c\ub294 \ucef4\ud4e8\ud130\uc5d0 \ucca8\ubd80\ub41c \ud2b8\ub808\uc778 \ubc0f \ud14c\uc2a4\ud2b8 csv \ud30c\uc77c\uc744 \ucef4\ud4e8\ud130\uc5d0 \ub2e4\uc6b4\ub85c\ub4dc \ud55c \ud6c4 \uc544\ub798 \ucf54\ub4dc\ub97c \uc2e4\ud589\ud558\uc5ec \ub2e4\uc2dc \uadf8 \ud30c\uc77c\ub4e4\uc744 \ubd88\ub7ec\uc62c \uc218 \uc788\uac8c \ub429\ub2c8\ub2e4.\n\n    from google.colab import files\n    uploaded = files.upload()\n\n\uadf8\ub7f0 \ub2e4\uc74c \uc544\ub798 \ucf54\ub4dc\ub97c \ud1b5\ud574\uc11c csv\ub97c \ub370\uc774\ud130\ud504\ub808\uc784\uc73c\ub85c \ubc14\uafc0 \uc218 \uc788\uac8c \ub429\ub2c8\ub2e4.\n\n    import io\n    test = pd.read_csv(io.BytesIO(uploaded['test.csv']))\n    train = pd.read_csv(io.BytesIO(uploaded['train.csv']))","3a12af0c":"[](http:\/\/i.imgur.com\/QBuDOjs.jpg)","1c5b1fc7":"\n\uc774 \ud50c\ub86f\uc744 \ucc98\uc74c \ubcfc \ub54c \ud574\uc11d\ud558\uae30 \uc5b4\ub824\uc6b8 \uc218 \uc788\uc2b5\ub2c8\ub2e4\n\n\ud56d\ubaa9\uc774 \uc608\uce21\uc5d0 \ubbf8\uce58\ub294 \uc601\ud5a5\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4\t\n\n\uac01 \ud589\uc740 \uac01 \ud56d\ubaa9\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4\t\n\n\uc0c9\uc0c1\uc740 \uc2e4\uc81c \ud56d\ubaa9 \uac12\uc785\ub2c8\ub2e4\t\n\n\uc608\ub97c \ub4e4\uc5b4 \ud30c\ub780\uc0c9  \ud56d\ubaa9 18\uc758 \ub0ae\uc740 \uac12\uc740 \ubaa8\ud615 \uc608\uce21\uc5d0 \ubd80\uc815\uc801\uc778 \uc601\ud5a5\uc744 \ubbf8\uce69\ub2c8\ub2e4 (1\uc774\ub0d0 0\uc774\ub0d0\uc5d0\uc11c 0\uc774 \ub418\uaca0\uc9c0\uc694)\t\n\n\ube68\uac04\uc0c9\uc778 \ub192\uc740 \uac12\uc740 \uae0d\uc815\uc801\uc778 \uc601\ud5a5\uc744 \ubbf8\uce69\ub2c8\ub2e4 (1\uc774\ub0d0 0\uc774\ub0d0\uc5d0\uc11c 1\uc774 \ub418\uaca0\uc9c0\uc694)\n\n\ud56d\ubaa9 176\uc740 \ubc18\ub300 \uc601\ud5a5\uc774 \uc788\uc2b5\ub2c8\ub2e4 \t\n\n\ub0ae\uc740 \uac12\uc740 \uae0d\uc815\uc801\uc778 \uc601\ud5a5\uc744 \ubbf8\uce58\uba70 \ub192\uc740 \uac12\uc740 \ubd80\uc815\uc801\uc778 \uc601\ud5a5\uc744 \ubbf8\uce69\ub2c8\ub2e4\t\n\n\ubd88\ud589\ud788\ub3c4 \ud56d\ubaa9\uc744 \uc218\ub3d9\uc73c\ub85c \uc120\ud0dd\ud574\uc57c\ud569\ub2c8\ub2e4 \t\n\n\uadf8 \uc791\uc5c5\uc744 \ud574\uc8fc\ub294 \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c \uc0ac\uc6a9\ud558\uaca0\uc2b5\ub2c8\ub2e4\t","7fe0c89d":"\uc774\uc81c \ubaa8\ub378\uc744 \ud6c8\ub828\uc2dc\ud0a4\uace0 \uba54\ud2b8\ub9ad\uc744 \uacc4\uc0b0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4","de5c3f26":"\ubaa8\ub378\uc774 \ud6e8\uc52c \uc88b\uc544\uc84c\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4\n\n\uc911\uc694\ud55c \uad50\ud6c8\uc740 \ub54c\ub85c\ub294 \uc601\uc591\uac00 \uc5c6\ub294 \uac83\uc740 \uc5c6\ub294 \uac83\uc774 \uc88b\ub2e4\ub294 \uac83\uc774\uc8e0","ca42a0ec":"## Comparing models\n\n\ub2e4\ub978 \ubaa8\ub378\uc744 \ube44\uad50\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4 \n\n\uae30\ubcf8 \ub9e4\uac1c \ubcc0\uc218\uac00\uc788\ub294 \ubaa8\ub378\uc774 \uc81c\ub300\ub85c \uc791\ub3d9\ud558\uc9c0 \uc54a\uc744 \uc218 \uc788\uc73c\ubbc0\ub85c \ucd5c\uc801\ud654 \ub41c \ubaa8\ub378\uc744 \ube44\uad50\ud560 \uac00\uce58\uac00 \uc788\ub2e4\uace0 \uc0dd\uac01\ud569\ub2c8\ub2e4\t\n\n\ub2e4\uc74c\uacfc \uac19\uc774 \ud560 \uac83\uc785\ub2c8\ub2e4:\n\n* default parameter\ub85c \ubaa8\ub378\uc744 \ud559\uc2b5\ud558\uace0 \uae30\ubcf8 \uc810\uc218\ub97c \ud655\uc778\ud569\ub2c8\ub2e4\t \n* best feature\ub4e4\uc744 \uc120\ud0dd\ud569\ub2c8\ub2e4\t \n* grid search\ub97c \uc2e4\ud589\ud569\ub2c8\ub2e4 \n* best model\uc744 \ud6c8\ub828\uc2dc\ud0a4\uace0 \ub2e4\uc2dc \uc810\uc218\ub97c \ubd05\ub2c8\ub2e4\t\n\n\ub610\ud55c \uac01 \ubaa8\ub378\uc5d0 \ub300\ud55c feature selection\uc744 \ud574\ubd05\ub2c8\ub2e4  \n\n\uadf8\ub9ac\uace0 \ud6c8\ub828\uc744 \ub354 \ube60\ub974\uac8c \ud558\uae30 \uc704\ud574 \ubc18\ubcf5\ud558\uc9c0 \uc54a\ub294 \uac04\ub2e8\ud55c \ud3f4\ub4dc\ub97c \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4","848d487b":"* id \uc5f4, target \uc5f4 \ubc0f 300\uac1c\uc758 \ud56d\ubaa9\uc774 \uc788\uc74c\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4\n* \uc775\uba85\uc73c\ub85c \ucc98\ub9ac\ub418\uc5c8\uc73c\ubbc0\ub85c \uadf8 \uc758\ubbf8\ub97c \ubaa8\ub985\ub2c8\ub2e4\n* \uc5f4\ub4e4\uc5d0 \ub300\ud574 \uc774\ud574\ud558\ub824\uace0 \ub178\ub825\ud574\ubcf4\uc9c0\uc694","579976e2":"## Basic modelling\n\n\uae30\ubcf8 \ubaa8\ub378\ub9c1\uc744\ud569\ub2c8\ub2e4\n\n\ud6c8\ub828 \ubaa8\ub378\uacfc \ud3f4\ub4dc \uc608\uce21\uc5d0 \uc775\uc219\ud574\uc9c0\uae30\ub97c \ubc14\ub78d\ub2c8\ub2e4\t\n\n\uc774\uac83\uc774 \uc65c \uc720\uc6a9\ud560\uae4c\uc694?\n\nsklearn\uc758 cross_val_score\ub294 \ubaa8\ub378\uc758 \uc810\uc218\ub97c \uacc4\uc0b0\ud558\uae30\uc5d0 \ucda9\ubd84\ud558\uc9c0\ub9cc, \uc790\uc138\ud788 \ubc30\uc6b8\uc218\ub85d \ucda9\ubd84\ud558\uc9c0\ub294 \uc54a\ub2e4\ub294 \uac83\uc744 \uc54c\uac8c \ub420 \uac83\uc785\ub2c8\ub2e4\tbut as you learn more, you'll realize it isn't always enough\n\n\uc65c\ub0d0\ud558\uba74\t\n\n* \uc608\uce21\uc744 \uc81c\uacf5\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4\n* \ud3f4\ub4dc\ub85c\ubd80\ud130\uc758 \uc608\uce21\uc740 \uc81c\uacf5\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4\n* \ud2b9\uc815 \ubcc0\ud658\uc744 \uc801\uc6a9\ud560 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4\n* lgbm, catboost, xgboost\uc640 \uac19\uc740 \uadf8\ub798\ub514\uc5b8\ud2b8 \ubd80\uc2a4\ud305 \ubaa8\ub378\uc740 cross_val_score\ub85c \uc804\ub2ec\ud560 \uc218 \uc5c6\ub294 \ucd94\uac00 \ub9e4\uac1c \ubcc0\uc218\ub97c \ud544\uc694\ub85c \ud569\ub2c8\ub2e4\t\n\n\uc774 \uac83\uc740 \uc5b4\ub835\uc9c0 \uc54a\uace0 \ub17c\ub9ac\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4\n\n```\nfor fold in folds:\n    get train and validation data\n    apply some transformations (if necessary)\n    train model\n    predict on validation data\n    calculate train and validation metrics\n    predict on test data\n```\n\n\uac04\ub2e8\ud55c \ub85c\uc9c0\uc2a4\ud2f1 \ub9ac\uadf8\ub808\uc158\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub2e8\uacc4\ubcc4\ub85c \ucf54\ub4dc\ub97c \uc791\uc131\ud574 \ubd05\uc2dc\ub2e4","855011ed":"#### \uc774 \uac83\uc740 \uc548\ub4dc\ub808\uc774\ub8e9\uc57c\ub128\ucf54\uc758 \ub178\ud2b8\ubd81\uc744 \ud55c\uae00\ud654 \ud55c \uac83\uc785\ub2c8\ub2e4\n\n## General information\n\nDon't Overfit II \uc5d0\uc11c\ub294 \ubc14\uc774\ub108\ub9ac \ud074\ub798\uc2dc\ud53c\ucf00\uc774\uc158\uc744 \ud569\ub2c8\ub2e4\n\n300\uc5f4 250\ud589\uc758 \ud6c8\ub828 \uc0d8\ud50c\uc5d0 \ud14c\uc2a4\ud2b8 \uc0d8\ud50c\uc740 79\ubc30\ub098 \ub429\ub2c8\ub2e4\t\n\n\uc774\ub7f0 \uc0c1\ud669\uc5d0\uc11c \uc624\ubc84\ud54f\ud558\uae30 \uc26c\uc6b4\ub370 \ud6c8\ub828\uc138\ud2b8\uac00 \uc791\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4\t\n\n\uc624\ubc84\ud53c\ud305\uc758 \uc758\ubbf8\uac00 \uc0dd\uac01 \uc548 \ub098\uc2dc\uba74 \uac04\ub2e8\ud788 \ub9d0\ud574\uc11c \ud2b8\ub808\uc774\ub2dd \ub370\uc774\ud130\uc5d0\ub294 \uc798 \ub9de\uc73c\ub098 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc5d0\ub294 \uc798 \ub9de\uc9c0 \uc54a\ub294\ub2e4\ub294 \uc774\uc57c\uae30 \uc785\ub2c8\ub2e4\t\n\n\uc6b0\ub9ac\ub294 \uc624\ubc84\ud53c\ud305\ud558\uc9c0 \uc54a\ub294 \ubaa8\ub378\uc744 \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4\n\n\uc774 \ub178\ud2b8\uc5d0\uc11c\ub294 \ub2e4\uc74c\uc744 \ud569\ub2c8\ub2e4\n\n* \uc778\uc0ac\uc774\ud2b8\ub97c \uc5bb\uae30\uc704\ud574 \ud56d\ubaa9\uc5d0 \ub300\ud55c EDA\ub97c \ud569\ub2c8\ub2e4\n* \ud37c\ubba4\ud14c\uc774\uc158 \uc784\ud3ec\ud134\uc2a4\ub97c \uc0ac\uc6a9\ud558\uc5ec \uac00\uc7a5 \uc601\ud5a5\ub825 \uc788\ub294 \ud56d\ubaa9\uc744 \ucc3e\uc2b5\ub2c8\ub2e4\t\n* \uc5ec\ub7ec \ubaa8\ub378\uc744 \ube44\uad50\ud569\ub2c8\ub2e4 - \ubca0\uc774\uc2a4 \ud074\ub798\uc2dc\ud53c\ucf00\uc774\uc158, \ub9ac\ub2c8\uc5b4 \ubaa8\ub378, \ud2b8\ub9ac\uae30\ubc18 \ubaa8\ub378 \ub4f1\uc744 \ud574\ubd05\ub2c8\ub2e4\n* \uc5ec\ub7ec \uc885\ub958\uc758 feature selection \ubc29\ubc95\uc744 \ud574\ubd05\ub2c8\ub2e4 - ELI5 \ubc0f SHAP\uc744 \ud3ec\ud568\ud569\ub2c8\ub2e4\n* \ubaa8\ub378 \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130 \ucd5c\uc801\ud654\ub97c \ud574\ubd05\ub2c8\ub2e4\n* \ud56d\ubaa9 \uc0dd\uc131\uc744 \ud574\ubd05\ub2c8\ub2e4\n* \uadf8\ub9ac\uace0 \ub2e4\ub978 \uc5ec\ub7ec\uac00\uc9c0\ub97c \ud574\ubcf4\uc9c0\uc694\n\n![](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*vuZxFMi5fODz2OEcpG-S1g.png)","6b36f086":"mean auc\uac00 \uc99d\uac00\ud55c \uac83\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4","ffd77670":"\uc774\uc81c \ud3f4\ub4dc\ub97c \ud55c \ubc88 \uc0ac\uc6a9\ud558\uc5ec \ud2b8\ub808\uc778 \ub370\uc774\ud130\ub97c \ud6c8\ub828 \ubc0f \uac80\uc99d\uc73c\ub85c \ubd84\ud560\ud569\ub2c8\ub2e4","be522819":"<a id=\"de\"><\/a>\n## Data exploration","7eb11f55":"CV \uc810\uc218\ub97c \ud5a5\uc0c1\uc2dc\ud0a4\uc9c0 \uc54a\uc558\uc73c\ubbc0\ub85c \uc774\ub7ec\ud55c \ud56d\ubaa9\uc774 \ud06c\uac8c \ub3c4\uc6c0\uc774 \ub418\uc9c0 \uc54a\uc740 \uac83 \uac19\uc2b5\ub2c8\ub2e4","f736ef4d":"\uc774\uc81c \uc0c1\uad00 \uad00\uacc4\ub97c \uc0b4\ud3b4 \ubd05\uc2dc\ub2e4!","4d8b0d15":"\uc6b0\ub9ac \ubc84\uc804\uc758 \uc218\uc815\ub41c \ubc84\uc804\uc744 \uc791\uc131\ud574 \ubd05\ub2c8\ub2e4\t\n\n\uc5ec\uae30 \uad50\ucc28 \uad50\ucc28 \ub370\uc774\ud130 \ub0b4\uc5d0 RFECV\ub97c \ucd94\uac00\ud569\ub2c8\ub2e4"}}