{"cell_type":{"ac1ed113":"code","cef4c125":"code","a9f0be3d":"code","4c3b92ff":"code","6e429496":"code","b1fc68fd":"code","8d625d7a":"code","fdd21606":"code","49a4d36e":"code","04d8ee75":"code","669bfeeb":"code","632006aa":"code","1453a590":"code","b63a8dce":"code","e9f1ba7b":"code","e5bacdd4":"code","99ab6bcb":"code","61aa98ec":"code","ef3b9cab":"code","e2a993a1":"code","592ae111":"code","4242230f":"markdown","9020a9ed":"markdown","3760376f":"markdown","ca99e1d1":"markdown","4041f87c":"markdown","a1c7c260":"markdown","ac8f4cb7":"markdown","ee544c29":"markdown","d42ed2c6":"markdown","06691854":"markdown"},"source":{"ac1ed113":"#importing packages\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\n#importing style\nplt.style.use('seaborn')\n\n#setting up rc settings\nmpl.rcParams['axes.grid'] = False\nmpl.rcParams['lines.linewidth'] = 0.7\nmpl.rcParams['lines.markersize'] = 4\nmpl.rcParams['figure.figsize'] = 12.5, 7.5","cef4c125":"#creating training data\nX = np.linspace(-3, 2.5, 1000).reshape(1, 1000)\nX = np.random.permutation(X)\nY = (X+3)*(np.power((X-2),2))*(np.power((X+1), 3))\n\n#creating validation data\nX_Val = np.linspace(-4, 4, 100).reshape(1,100)\nY_Val = (X_Val+3)*(np.power((X_Val-2),2))*(np.power((X_Val+1), 3))","a9f0be3d":"#determining dimensions of data\nshape_X = X.shape\nshape_Y = Y.shape\nm = Y.shape[1]\n\nprint ('The shape of X is: ' + str(shape_X))\nprint ('The shape of Y is: ' + str(shape_Y))\nprint ('I have m =', m, 'training examples!')","4c3b92ff":"def sigmoid(Z):\n    \n    s = 1\/(1 + np.exp(-Z))\n    cache = Z\n    \n    return s, cache\n\n\n\ndef relu(Z):\n    \n\n    r = np.maximum(0, Z)\n    cache = Z\n    \n    return r, cache\n\n\n\ndef linear_function(Z):\n\n    l = Z\n    cache = Z\n    \n    return l, cache\n\n\n\ndef sigmoid_backward(dA, cache):\n    \n    Z = cache\n    s = 1\/(1 + np.exp(-Z))\n    \n    dZ = dA * s * (1-s)\n    \n    return dZ\n\n\n\ndef relu_backward(dA, cache):\n    \n    Z = cache\n    dZ = np.array(dA, copy = True)\n    dZ[Z <= 0] = 0\n    \n    return dZ\n\n\n\ndef linear_function_backward(dA, cache):\n    \n    Z = cache\n    dZ = dA\n    \n    return dZ","6e429496":"def layer_structure(X, Y, architecture):\n    \n    \"\"\"\n    Implements a function to intialize the structure of the network i.e. the number of layers and\n    corresponding number of units in each layer.\n    \n    Arguments:\n    X -- data, numpy array of shape (1, number of examples).\n    Y -- numpy array of shape (1, number of examples) containing corresponding y mappings\n    of the function f: x -> y.\n    architecture -- python list containing the configuration of the network.\n    For eg : [10, 5, 1] means network has 3 layers with 10, 5 and 1 units in corresponding layers.\n    \n    Returns:\n    all_layers_dims -- numpy array containing the configuration of the network (including input layer).\n    \"\"\"\n    \n    n_x = np.array([X.shape[0]])\n    n_y = np.array([Y.shape[0]])\n    \n    #dimensions of (hidden + output) layers\n    hidden_output_layers_dims = np.array(architecture)                                      \n    \n    assert hidden_output_layers_dims[-1] == n_y[0]\n    \n    hidden_layers_dims = hidden_output_layers_dims[:-1]                          \n    \n    all_layers_dims = np.concatenate((n_x, hidden_layers_dims, n_y))             \n    \n    global L_\n    L_ = len(all_layers_dims)                                                    \n    \n    #number of (hidden + output) layers\n    global L\n    L = len(hidden_layers_dims) + 1                                              \n\n    return all_layers_dims","b1fc68fd":"def initialize_parameters_deep(all_layers_dims):\n    \n    \"\"\"\n    Implements a function that initializes each layer of the network.\n    \n    Arguments:\n    all_layers_dims --  numpy array containing the configuration of the network (including input layer).\n    \n    Returns:\n    parameters -- initialized parameters (weights and biases) by the function.\n    \"\"\"\n    \n    parameters = {}\n    \n    for l in range(1, L_):\n        parameters['W' + str(l)] = np.random.randn(all_layers_dims[l], all_layers_dims[l-1])*0.01\n        parameters['b' + str(l)] = np.zeros((all_layers_dims[l], 1))\n        \n    return parameters","8d625d7a":"def activation_structure(activations):\n    \n    \"\"\"\n    Implements a function that pairs each layer of the network with its corresponding activation function.\n    \n    Arguments:\n    activations -- python list containing the names of activations for corresponding layers.\n    \n    Returns:\n    structure -- python list pairing each layer with its corresponding activation function.\n    \"\"\"\n    \n    #combined structure in form like ---[(1, \"relu\"),(2, \"sigmoid\"),(3, \"linear\")] for eg.\n    structure = []\n    \n    for l, activation in enumerate(activations, 1):\n        structure.append((l, activation))\n        \n    return structure  ","fdd21606":"def linear_forward(A, W, b):\n    \n    \"\"\"\n    Implement the linear part of a layer's forward propagation.\n\n    Arguments:\n    A -- activations from previous layer (or input data): (size of previous layer, number of examples).\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer).\n    b -- bias vector, numpy array of shape (size of the current layer, 1).\n\n    Returns:\n    Z -- the input of the activation function, also called pre-activation parameter.\n    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently.\n    \"\"\"\n    \n    Z = np.dot(W, A) + b\n    cache = (A, W, b)\n    \n    return Z, cache","49a4d36e":"def linear_activation_forward(A_prev, W, b, activation):\n    \n    \"\"\"\n    Implement the forward propagation for the LINEAR->ACTIVATION layer.\n\n    Arguments:\n    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples).\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer).\n    b -- bias vector, numpy array of shape (size of the current layer, 1).\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\" or \"linear\".\n\n    Returns:\n    A -- the output of the activation function, also called the post-activation value.\n    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n             stored for computing the backward pass efficiently.\n    \"\"\"\n    \n    if activation == \"linear\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = linear_function(Z)\n        \n    elif activation == \"sigmoid\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = sigmoid(Z)\n        \n    elif activation == \"relu\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = relu(Z)\n        \n    cache = (linear_cache, activation_cache)\n    \n    return A, cache","04d8ee75":"def L_model_forward(X, parameters, structure):\n    \n    \"\"\"\n    Implement forward propagation for the network.\n    \n    Arguments:\n    X -- data, numpy array of shape (1, number of examples).\n    parameters -- output of initialize_parameters_deep().\n    structure -- python list pairing each layer with its corresponding activation function.\n    \n    Returns:\n    AL -- last post-activation value.\n    caches -- list of caches containing:\n                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1).\n    \"\"\"\n    \n    caches = []\n    A = X\n    \n    for l, activation in (structure):\n\n        if l == L:\n            AL, cache = linear_activation_forward(A, parameters['W'+str(L)], parameters['b'+str(L)], activation = activation)\n            caches.append(cache)\n        \n        else:\n            A_prev = A\n            A, cache = linear_activation_forward(A_prev, parameters['W'+str(l)], parameters['b'+str(l)], activation = activation)\n            caches.append(cache)\n\n    \n    return AL, caches","669bfeeb":"def compute_cost(AL, Y):\n    \n    \"\"\"\n    Implements the cost function.\n\n    Arguments:\n    AL -- prediction vector corresponding to your y mappings, shape (1, number of examples).\n    Y -- true \"mapping\" vector (for example: containing y = 12 if x = 0 ), shape (1, number of examples).\n\n    Returns:\n    cost -- mean squared error.\n    \"\"\"\n    \n    #simple mean squared error\n    cost = 1\/(2*m) * np.sum(np.square(AL - Y))\n    \n    cost = np.squeeze(cost)\n    \n    return cost","632006aa":"def linear_backward(dZ, cache):\n    \n    \"\"\"\n    Implement the linear portion of backward propagation for a single layer (layer l).\n\n    Arguments:\n    dZ -- Gradient of the cost with respect to the linear output (of current layer l).\n    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer.\n\n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev.\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W.\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b.\n    \"\"\"\n\n    A_prev, W, b = cache\n    \n    dW = 1\/m*np.dot(dZ,A_prev.T)\n    db = 1\/m*np.sum(dZ, axis = 1, keepdims = True)\n    dA_prev = np.dot(W.T, dZ)\n    \n    return dA_prev, dW, db","1453a590":"def linear_activation_backward(dA, cache, activation):\n    \n    \"\"\"\n    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n    \n    Arguments:\n    dA -- post-activation gradient for current layer l.\n    cache -- tuple of values (linear_cache, activation_cache) we stored for computing backward propagation efficiently.\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\" or \"linear\".\n    \n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev.\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W.\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b.\n    \"\"\"\n    \n    linear_cache, activation_cache = cache\n    \n    if activation == \"linear\":\n        dZ = linear_function_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)    \n    \n    elif activation == \"sigmoid\":\n        dZ = sigmoid_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n        \n    elif activation == \"relu\":\n        dZ = relu_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n    \n    return dA_prev, dW, db","b63a8dce":"def L_model_backward(AL, Y, caches, structure):\n    \n    \"\"\"\n    Implement the backward propagation for the network.\n    \n    Arguments:\n    AL -- prediction vector, output of the forward propagation (L_model_forward()).\n    Y -- true \"mapping\" vector.\n    caches -- a python tuple containing \"linear_cache\" and \"activation_cache\".\n    structure -- python list pairing each layer with its corresponding activation function.\n    \n    Returns:\n    grads -- A dictionary with the gradients\n             grads[\"dA\" + str(l)] = ... \n             grads[\"dW\" + str(l)] = ...\n             grads[\"db\" + str(l)] = ... \n    \"\"\"\n    \n    grads = {}\n    \n    #intialisation of dAL = dL\/dA = 2(AL - Y)\n    dAL = 2*(AL - Y)\n    \n    for l, activation in reversed(structure):\n\n        if l == L:\n            #indexing of caches is from 0 to L-1 (L elements in total corresponding to each layer)\n            current_cache = caches[L-1]\n            dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, activation)\n            grads['dA' + str(L-1)], grads['dW' + str(L)], grads['db' + str(L)] = dA_prev_temp, dW_temp, db_temp \n    \n        else:\n            current_cache = caches[l-1]\n            dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads['dA' + str(l)], current_cache, activation)\n            grads['dA' + str(l-1)], grads['dW' + str(l)], grads['db' + str(l)] = dA_prev_temp, dW_temp, db_temp\n        \n        \n    return grads","e9f1ba7b":"def update_parameters(parameters, grads, learning_rate):\n    \n    \"\"\"\n    Implement the function to update parameters using gradient descent.\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters.\n    grads -- python dictionary containing your gradients, output of L_model_backward().\n    learning_rate -- learning rate of the gradient descent update rule.\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n                  parameters[\"W\" + str(l)] = ... \n                  parameters[\"b\" + str(l)] = ...\n    \"\"\"\n    \n    for l in range(L):\n        parameters['W'+str(l+1)] = parameters['W'+str(l+1)] - learning_rate*grads['dW'+str(l+1)]\n        parameters['b'+str(l+1)] = parameters['b'+str(l+1)] - learning_rate*grads['db'+str(l+1)]\n        \n    return parameters","e5bacdd4":"def dnn_model(X, Y, architecture, activations, learning_rate, num_iterations, print_cost):\n    \n    \"\"\"\n    Implements a L-layer neural network.\n    \n    Arguments:\n    X -- data, numpy array of shape (1, number of examples).\n    Y -- numpy array of shape (1, number of examples) containing corresponding y mappings\n    of the function f: x -> y.\n    architecture -- python list containing the configuration of the network.\n    activations -- python list containing the names of activations for corresponding layers.\n    learning_rate -- learning rate of the gradient descent update rule.\n    num_iterations -- number of iterations of the optimization loop.\n    print_cost -- if True, it prints the cost every 10000 steps.\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    structure -- python list pairing each layer with its corresponding activation function.\n    \"\"\"\n    \n    #registering cost after each 1000 iterations\n    costs = []\n    \n    all_layers_dims = layer_structure(X, Y, architecture)\n    parameters = initialize_parameters_deep(all_layers_dims)\n    structure = activation_structure(activations)\n    \n    for i in range(num_iterations):\n        \n        AL, caches = L_model_forward(X, parameters, structure)\n        \n        cost = compute_cost(AL, Y)\n        \n        grads = L_model_backward(AL, Y, caches, structure)\n        \n        parameters = update_parameters(parameters, grads, learning_rate)\n        \n        if print_cost and i % 10000 == 0:\n            print(\"Cost after iteration\" , i, \"is\", cost)\n            costs.append(cost)\n    \n    #plotting cost vs iterations\n    fig, ax = plt.subplots()\n    ax.plot(np.squeeze(costs))\n    ax.set_xlabel('number of iterations')\n    ax.set_ylabel('cost')\n    ax.set_yscale('log')\n    \n    return parameters, structure\n            ","99ab6bcb":"parameters, structure = dnn_model(X, Y, [10, 5, 1], [\"sigmoid\", \"relu\", \"linear\"], num_iterations = 100000, print_cost=True, learning_rate = 5e-3)","61aa98ec":"def predict(X_Val, parameters, structure):\n    \n    \"\"\"\n    Implements a function to predict on validation set.\n    \n    Arguments:\n    X_Val -- validation data, numpy array of shape (1, number of examples in validation set).\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    structure -- python list pairing each layer with its corresponding activation function.\n    \n    Returns:\n    predictions -- prediction vector, output of the forward propagation (L_model_forward()).\n    \"\"\"\n    \n    AL, _ = L_model_forward(X_Val, parameters, structure)\n    \n    predictions = AL\n    \n    return predictions","ef3b9cab":"def validation_model(X_Val, Y_Val, parameters, structure):\n    \n    \"\"\"\n    Implements a function to validate our predictions against validation set.\n    \n    Arguments:\n    X_Val -- validation data, numpy array of shape (1, number of examples in validation set).\n    Y_Val -- numpy array of shape (1, number of examples in validation set) containing corresponding\n    y mappings of the function f: x -> y from validation set.\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    structure -- python list pairing each layer with its corresponding activation function.\n    \n    Returns:\n    difference -- vector values, containing absolute differences between predicted and actual values.\n    \"\"\"\n    \n    AL = predict(X_Val, parameters, structure)\n    \n    #calculating difference against actual values of the polynomial function\n    difference = np.abs(AL - Y_Val)\n\n    return difference\n","e2a993a1":"predictions = predict(X_Val, parameters, structure)\ndifference = validation_model(X_Val, Y_Val, parameters, structure)","592ae111":"#rendering plot for comparison\nfig, ax = plt.subplots()\nax.scatter(X_Val, Y_Val, label = 'Validation Data')\nax.scatter(X_Val, predictions, label = 'Prediction')\nax.scatter(X_Val, difference, label = 'Difference')\nax.set_title('Comparisons')\nax.legend()\nax.axis([-5, 5, -70, 70])","4242230f":"<a id=\"training-model\"><\/a>\n### Training model\n\nFinally we will train our model by passing in the data and appropriate keyword arguments.\nWe will train for 100000 iterations of gradient descent. I have chosen 3 layers with sigmoid, relu, and linear activations respectively. Optimal learning rate was determined by hit and trial method.","9020a9ed":"<a id=\"plotting-comparisons\"><\/a>\n### Plotting comparisons \n\nFinally we will plot the comparison between the actual values from our validation set and the values predicted by the model.\nWe will plot all three - prediction, validation data, and the difference between the two on same axes to better visualise the comparisons.\nNote - If the validation data points are not visible that is because they have been overlapped by the prediction data points.","3760376f":"<a id=\"validating-model\"><\/a>\n### Validating model\n\nThen we will validate our model by using the validtaion data created earlier. First we will predict on validation data (X_Val), then we will compare the predicted values with the actual values (Y_Val) and determine the difference between the two.","ca99e1d1":"<a id=\"creating-data\"><\/a>\n### Creating data\n\nWe have created X to Y mappings for a fairly complex polynomial function $ y = (x+3)(x-2)^2(x+1)^3 $.\nWe have shuffled the data (X Values) so as improve the generalizability of neural network and to remove any bias.\nWe are using 1000 examples from domain [-3 , 2.5] to train and 100 examples from domain [-4, 4] to validate the model.  ","4041f87c":"<a id=\"observations\"><\/a>\n## Observations\n\n* The model perfomance is really good in the domain in which it was trained i.e. [-3, 2.5] but the performance deteriorates outside of that domain.\n* Hence the model is clearly overfitting.","a1c7c260":"<a id=\"creating-specific-functions\"><\/a>\n### Creating specific functions\n\nTo build the neural network we will implement several specific functions for their specific tasks as suggested by their names.\nThere are 11 specific functions in total which will be incorporated in the model function later to make it easier to implement and control the functionality of the neural network architecture.","ac8f4cb7":"# Handmade Neural Network\nImplementation of a configurable neural network from scratch exclusively in python.\n\n## Table of contents\n* [General](#general)\n* [Project Overview](#project-overview)\n* [Learning Task Overview](#learning-task-overview)\n* [Technologies](#technologies)\n* [Skeleton of the Code](#skeleton-of-the-code)\n* [Observations](#observations)\n\n<a id=\"general\"><\/a>\n## General\nThis project is a simple neural network mapping a polynomial function.\n\n<a id=\"project-overview\"><\/a>\n## Project Overview\nInspiration of this project lies in the fact that neural network, at its core, is simply a function estimator that, given enough data, can accurately map features (x) to target variables (y). So, here we have tried to map a fairly complex polynomial function so as to showcase the ability of a neural network to achieve that task using simple activation units like sigmoid, rectified linear and linear.\n\n Also, this project tries to explore and clarify what really goes on under the hood in a neural network. The choice of a polynomial function tries to mirror the functions a neural network tries to learn while performing tasks like image recognition or text classification. Clearly, we have chosen a much simpler task than the tasks mentioned above. This is done deliberately so as to serve the purpose of analysing the mechanics of neural network itself instead of dealing with the intricacies of a typical deep learning task.\n\nOnce cut open and clearly understood, we can explore numerous ways to experiment with the basic form of the network to create variations. The project is at its nascent stage and the code is largely inspired by Deep Learning Specialization from deeplearning.ai. Full specialization can be found out at - https:\/\/www.coursera.org\/specializations\/deep-learning.\n\n\n\n\n\n\n\n<a id=\"learning-task-overview\"><\/a>\n## Learning Task Overview\nHere we have chosen a fairly complex 2-dimensional polynomial function for the network to learn.\n\nThe chosen function is as follows:\n\n$y = (x+3)(x-2)^2(x+1)^3$\n\nAs training data we have given our network equidistant but randomly permutated 1000 x-values and corresponding y-values. The domain of x-values of the function for training is [-3, 2.5]. The network is trained for 100,000 iterations.\n\nWhereas, as validation data we use 100 equidistant x-values and corresponding y-values. The domain for validation data is [-4, 4].\n\nWe do not have test data because the purpose is to explore with the form of the network rather than to perform an actual learning task.\n\n\n<a id=\"technologies\"><\/a>\n## Technologies\nProject is created with:\n* Python version: 3.10.0\n* Numpy version: 1.21.4\n* Matplotlib version: 3.4.3\n","ee544c29":"<a id=\"creating-activation-functions\"><\/a>\n### Creating activation functions\n\nWe will implement three activation functions and their corresponding backward functions. These functions will then be used by our specific functions to aid them in the implementation. Three activation functions are sigmoid, relu, and linear.","d42ed2c6":"<a id=\"skeleton-of-the-code\"><\/a>\n## Skeleton of the Code\n\nThe following code will be presented in 7 distinct steps\n\n* [Creating data](#creating-data) - We have created X to Y mappings of a fairly complex polynomial function $ y = (x+3)(x-2)^2(x+1)^3 $.\n* [Creating activation functions](#creating-activation-functions) - We will implement three activation functions and their corresponding backward functions.\n* [Creating specific functions](#creating-specific-functions) - To build the neural network we will implement several specific functions for their specific tasks as suggested by their names.\n* [Creating model](#creating-model) - To build the neural network we will create a model function that will bring use of all the specific functions, implemented earlier, in a cohesive way.\n* [Training model](#training-model) - Finally we will train our model by passing in the data and appropriate keyword arguments.\n* [Validating model](#validating-model) - Then we will validate our model by using the validtaion data.\n* [Plotting comparisons](#plotting-comparisons) - Finally we will plot the comparison between the actual values from our validation set and the values predicted by the model.","06691854":"<a id=\"creating-model\"><\/a>\n### Creating model\n\nTo build the neural network we will create a model function that will bring use of all the specific functions, implemented earlier, in a cohesive way.\nWe will also render a plot to help visualise the value of the cost after every 10000 iterations of gradient descent. We plot on a log scale so as to visualise even very small changes in the cost in later iterations."}}