{"cell_type":{"6bb06ba8":"code","a7972bfd":"code","4373afd2":"code","edea7cb8":"code","b0ae06e2":"code","7dbf991c":"code","f8bfcba7":"code","484020f3":"code","1f2a0a6b":"code","09c833d3":"code","868cf5b9":"code","a14452b7":"code","d70ed971":"code","7f4b5c84":"code","031707e2":"code","ed373bef":"code","8a68e91a":"code","11b28be3":"code","4df8326c":"code","a6252930":"code","dcaeea1a":"code","c8d77f12":"code","cbcbfb57":"code","e5047196":"code","3cf58717":"code","852fbba3":"code","e8c218a7":"code","10fb4090":"code","15661973":"code","fb2d781d":"code","13fbe9df":"code","222e11ce":"code","b40b88e5":"code","f84bffae":"code","3485bd03":"code","c4d16913":"code","efba2cb9":"code","ed111c65":"markdown","ca9969c2":"markdown","e9953239":"markdown","5b537940":"markdown","06eb896f":"markdown","49a87c51":"markdown","e95d016d":"markdown","fefd1260":"markdown","a2195fec":"markdown","a216d4e5":"markdown","eb61e3d1":"markdown","68d70f5b":"markdown","1e04a487":"markdown","4232d3f5":"markdown","d461aeb2":"markdown","36b17b3e":"markdown","c1046e4c":"markdown","7d89d00e":"markdown","ad8ab87d":"markdown","d7e0fc4d":"markdown","00272a9f":"markdown","981e7b9c":"markdown","6e4e7264":"markdown","9e53921d":"markdown","182721d5":"markdown","fb74ea83":"markdown","ed7de99e":"markdown","94cfb5eb":"markdown","0d4aaefc":"markdown","a312409e":"markdown"},"source":{"6bb06ba8":"# Manipulation of vectors library\nimport numpy as np\n# Manipulation of data library\nimport pandas as pd\n#Ploting graphs library\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly_express as px\n# Manipulation of the system library\nimport os","a7972bfd":"! cd \/kaggle\/input\/house-prices-advanced-regression-techniques\/ && ls","4373afd2":"df_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","edea7cb8":"px.box(df_train[['OverallQual', 'SalePrice']].sort_values(by='OverallQual'), x='OverallQual', y='SalePrice', color='OverallQual')","b0ae06e2":"px.scatter(df_train, x='TotalBsmtSF', y='SalePrice', color='OverallQual')","7dbf991c":"px.scatter(df_train, x='1stFlrSF', y='SalePrice', color='OverallQual')","f8bfcba7":"px.scatter(df_train, x='GrLivArea', y='SalePrice', color='OverallQual')","484020f3":"px.box(df_train[['FullBath', 'SalePrice']].sort_values(by='FullBath'), x='FullBath', y='SalePrice', color='FullBath')","1f2a0a6b":"px.box(df_train[['GarageCars', 'SalePrice']].sort_values(by='GarageCars'), x='GarageCars', y='SalePrice', color='GarageCars')","09c833d3":"df_train.YearBuilt\npx.box(df_train, x='Fireplaces', y='SalePrice', color='Fireplaces')","868cf5b9":"plt.figure(figsize=(24,20))\nsns.heatmap(df_train.corr(), annot=True, square=True)\nplt.show()","a14452b7":"px.box(df_train, x='SaleCondition', y='SalePrice', color='SaleCondition').update_xaxes(categoryorder='mean ascending')","d70ed971":"px.box(df_train, x='RoofStyle', y='SalePrice', color='RoofStyle').update_xaxes(categoryorder='mean ascending')","7f4b5c84":"px.box(df_train, x='Foundation', y='SalePrice', color='Foundation').update_xaxes(categoryorder='mean ascending')","031707e2":"px.box(df_train, x='MSZoning', y='SalePrice', color='MSZoning').update_xaxes(categoryorder='mean ascending')","ed373bef":"px.box(df_train, x='Neighborhood', y='SalePrice', color='Neighborhood').update_xaxes(categoryorder='mean ascending')","8a68e91a":"print('Delete these columns:\\n',df_train.isna().sum()[df_train.isna().sum() > 100])\nprint('\\n\\n=================================================================================================\\n\\n')\nprint('Replace missing values using mean for numeric data and most frequent value for categorical data:\\n',df_train.isna().sum()[(df_train.isna().sum() > 0) & (df_train.isna().sum() < 100)])","11b28be3":"delete = list(df_train.isna().sum()[df_train.isna().sum()>100].index)\n\ndf_train = df_train.drop(delete, axis=1)\ndf_test = df_test.drop(delete, axis=1)","4df8326c":"df_train = df_train.fillna(df_train.mean())\ndf_test = df_test.fillna(df_test.mean())","a6252930":"df_train = df_train.fillna(df_train.mode().iloc[0])\ndf_test = df_test.fillna(df_test.mode().iloc[0])","dcaeea1a":"#df_train = pd.get_dummies(df_train, drop_first=True)\n#df_test = pd.get_dummies(df_test, drop_first=True)\n#len(list(df_train.columns)), len(list(df_test.columns))\n#id: 1460\n\n\ndata = pd.concat([df_train, df_test], axis=0, sort=False)\ndata.drop(['SalePrice'], axis=1, inplace=True)\ndata = pd.get_dummies(data, drop_first=True)\n\ndf_test = data.iloc[df_train['Id'].max():].copy()\n\ntrain_data = data.iloc[:df_train['Id'].max()].copy()\ntrain_data['SalePrice'] = df_train['SalePrice'].copy()\ndel df_train\ndf_train = train_data","c8d77f12":"X = df_train.drop(labels=['SalePrice'],axis=1)\ny = df_train['SalePrice']","cbcbfb57":"from sklearn.svm import SVR, LinearSVR\nfrom sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as MSE\nimport time","e5047196":"MLA = [\n    #SVM\n    ('SVR', SVR()),\n    ('LinearSVR', LinearSVR()),\n    \n    #Linear Model\n    ('LinearRegression', LinearRegression()),\n    ('BayesianRidge', BayesianRidge()),\n    \n    #Neighbors\n    ('KNeighborsRegressor', KNeighborsRegressor()),\n    \n    #Gaussian\n    ('GaussianProcessRegressor', GaussianProcessRegressor()),\n    \n    #Ensemble\n    ('AdaBoostRegressor', AdaBoostRegressor()), \n    ('GradientBoostingRegressor', GradientBoostingRegressor()),\n    \n    #XGBoost\n    ('XGBRegressor', XGBRegressor()),\n    \n    #LightGBM\n    ('LightGBM', LGBMRegressor()),\n    \n    #Catboost\n    ('CatBoost', CatBoostRegressor())\n    \n]","3cf58717":"t0 = time.time()\nshape = (10,10)\nresults = np.zeros((len(MLA), shape[0], shape[1]))\nfor count, algorithm in enumerate(MLA):\n    cv = RepeatedKFold(n_splits=10, n_repeats=10)\n    model = Pipeline([('scaler', MinMaxScaler()), algorithm])\n    scores = cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n    results[count] = scores.reshape(shape)\nprint(f'Total time {time.time() - t0} s')","852fbba3":"results = np.absolute(results)\nfor index, result in enumerate(results):\n    plt.title(MLA[index][0])\n    plt.boxplot(result, showmeans=True)\n    plt.axhline(np.mean(result), c='r')\n    plt.axhline(np.mean(result) + np.std(result), c='r', ls='--')\n    plt.axhline(np.mean(result) - np.std(result), c='r', ls='--')\n    plt.show()","e8c218a7":"best_mean = [np.mean(x) for x in results]\nindex_best_mean = best_mean.index(min(best_mean))\nprint(f'mean of best result ({MLA[index_best_mean][0]})')\nfor i in results[index_best_mean]:\n    print(f'{np.round(np.mean(i), decimals=3)} +\/- {np.round(np.std(i), decimals=3)}')","10fb4090":"from sklearn.model_selection import GridSearchCV\nt0 = time.time()\n\nn_estimators = [50, 100, 200, 400, 800]\nmax_depth = [4, 5, 6]\neta = [1, 0.1, 0.01]\nparam_grid = {'XGBRegressor__max_depth': max_depth, 'XGBRegressor__eta': eta, 'XGBRegressor__n_estimators': n_estimators}\nmodel = Pipeline([('scaler', MinMaxScaler()), ('XGBRegressor', XGBRegressor())])\nkfold = RepeatedKFold(n_splits=10)\ngrid_search = GridSearchCV(model, param_grid, scoring=\"neg_root_mean_squared_error\", n_jobs=-1, cv=kfold, verbose=1)\ngrid_result = grid_search.fit(X, y)\n    \nprint(f'Total time {time.time() - t0} s')","15661973":"print(f'Best score is {grid_result.best_score_} using {grid_result.best_params_} parameters')\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, std, param in zip(means, stds, params):\n    print(f'{np.round(mean,2)} +\/- {np.round(std,2)} with {param}')","fb2d781d":"train_X, test_X, train_y, test_y = train_test_split(X, y, \n                      test_size = 0.2)","13fbe9df":"scaler = MinMaxScaler()\nscaler.fit(train_X)\ntrain_X = scaler.transform(train_X)\ntest_X = scaler.transform(test_X)","222e11ce":"xgb_model = XGBRegressor(eta=.1, max_depth=5, n_estimators=400)\nxgb_model.fit(train_X, train_y)\npredict = xgb_model.predict(test_X)","b40b88e5":"rmse = np.sqrt(MSE(test_y, predict))\nprint(f'Root-mean-square error {np.round(rmse,3)}')","f84bffae":"scaler = MinMaxScaler()\nscaler.fit(X)\nX_train = scaler.transform(X)\ny_train = y\nX_test = scaler.transform(df_test)","3485bd03":"xgb_model = XGBRegressor(eta=.1, max_depth=5, n_estimators=400)\nxgb_model.fit(train_X, train_y)\npredict = xgb_model.predict(X_test)","c4d16913":"result = pd.DataFrame(predict, columns=['SalePrice'])\nresult['Id'] = df_test['Id']\nresult = result[['Id', 'SalePrice']]\nresult.head()","efba2cb9":"result.to_csv('submission.csv', index=False)","ed111c65":"Fill missing numerical data with the mean value of its column.","ca9969c2":"This, too, happens, as expected.","e9953239":"Realize that, OverallQual is directly proportional to the SalePrice, i.e., when OverallQual increases SalePrice too.","5b537940":"So far so good. Every missing data is out.","06eb896f":"The same thing happens with 1stFlrSF. There's a trend of growth.","49a87c51":"And so on, we see the same trend in the graph below.","e95d016d":"Now, let us see some categorical data in order to visualize other relationships.","fefd1260":"Delete the columns with more missing data.","a2195fec":"*OverallQual*","a216d4e5":"*GrLivArea*","eb61e3d1":"Applying get_dummies for categorical variables becomes indicator\/int.","68d70f5b":"Now, let us treat the missing data.\nTo treat this question, if data is numeric, should use the mean. On the other hand, if data is, categorical should use the most frequent data.\nMoreover, if there are more than 100 missing data, should delete the relative column, this will not pitch in the model.","1e04a487":"Now, predicting using all dataset to build up a better model.","4232d3f5":"*FullBath*","d461aeb2":"# Numerical Data","36b17b3e":"In order to confirm what the last figures show, let us plot the correlation graph below.","c1046e4c":"*GarageCars*","7d89d00e":"*TotalBsmtSF*\n\nLet's see the behavior of the SalePrice when TotalBsmtSF increases and too OverallQual.","ad8ab87d":"# Missing Data","d7e0fc4d":"Firstly, predict using a test data to calculate RMSE.","00272a9f":"Fill missing categorical data with the mode of its column.","981e7b9c":"*1stFlrSF*","6e4e7264":"Finally, the last one.","9e53921d":"Begin analyzing just numeric types. Let us list the data that seems important for a prior analysis about predicting a house price:\n\n\n*Overallqual: Rates the overall material and finish of the house;\n\n*TotalBsmtSF: Total square feet of basement area;\n\n*1stFlrSF: First Floor square feet;\n\n*GrLivArea: Above grade (ground) living area square feet;\n\n*FullBath: Full bathrooms above grade;\n\n*GarageCars: Size of garage in car capacity;\n\n*Fireplace: Number of fireplaces\n\n\nAfter this, we may analyze other types of data to figure others behaviors out.","182721d5":"## Baselines","fb74ea83":"*Fireplace*","ed7de99e":"# Categorical Data","94cfb5eb":"These figures seem not to be linear because houses with 4 garage cars are cheaper than houses with 3 garage cars. However, there are just a few data for houses with 4 cars, while in the other cases. Then, in my opinion, this question should be ignored because it will not affect the model.","0d4aaefc":"## Predict","a312409e":"Whoooa! Realize how the SalePrice growth with TotalBsmtSF and OverallQual"}}