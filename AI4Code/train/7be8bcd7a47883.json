{"cell_type":{"68972f29":"code","16c94c90":"code","0d2bbd3b":"code","6f9eb637":"code","cd15eeb1":"code","b83b3ee5":"code","a31eeea9":"code","ea0e29cb":"code","261ed77e":"code","0288d0b5":"code","b56ee2ee":"code","2e173329":"code","dbb92b28":"code","741c1629":"code","4e4ab02f":"code","88b5ae74":"code","0726d7d4":"code","2d1df453":"code","b20003aa":"code","487d8a18":"code","57f85fe4":"code","88ebddd8":"code","df86e1a5":"code","72c03f50":"code","3ea6b388":"code","5a7751ac":"code","43375895":"code","2d167683":"code","2ceba00e":"code","9ad1f074":"code","7ec531cc":"code","b414db3c":"markdown","21c27201":"markdown","ef291337":"markdown","9e095aaf":"markdown"},"source":{"68972f29":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom tqdm import tqdm\nimport seaborn as sns \nimport random\nimport cv2\nimport math\nfrom sklearn.model_selection import StratifiedKFold\nfrom lightgbm import LGBMRegressor\n\nimport tensorflow as tf\nfrom tensorflow import keras \nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import Input, Sequential, layers\nimport tensorflow.keras.backend as K\n\nimport sys\nsys.path.append('..\/input\/swintransformertf')\nfrom swintransformer import SwinTransformer","16c94c90":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f'Running on TPU {tpu.master()}')\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","0d2bbd3b":"# Tabular data \ntrain = pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\ntest = pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')\nsample_submission = pd.read_csv('..\/input\/petfinder-pawpularity-score\/sample_submission.csv')\n\ntrain_features = train.iloc[:,1:]\n\n# Photo data \ntrain_image = '..\/input\/petfinder-pawpularity-score\/train\/'\ntest_image = '..\/input\/petfinder-pawpularity-score\/test\/'\n\ntrain['file_path'] = train['Id'].apply(lambda x: '{}'.format(train_image)+f'{x}.jpg')\ntest['file_path'] = test['Id'].apply(lambda x: '{}'.format(test_image)+f'{x}.jpg')\n\nfeatures  = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', \n            'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', \n            'Info', 'Blur']\n\n# for working with tabular information\ntrain_features = train[features]\nY_features = train['Pawpularity'].astype(int)\ntest_features = test[features]","6f9eb637":"input_shape = (224,224, 3)","cd15eeb1":"OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=0.02)\nBATCH_SIZE = 32\nEPOCHS = 3\nFOLD = 5\nSEED = 42\nIMAGE_SIZE = [224,224]  # 512,512  224,224\n\npatch_size = (2, 2)  # 2-by-2 sized patches\ndropout_rate = 0.03  # Dropout rate\nnum_heads = 8  # Attention heads\nembed_dim = 64  # Embedding dimension\n\nnum_mlp = 256  # MLP layer size\n\nqkv_bias = True  # Convert embedded patches to query, key, and values with a learnable additive value\nwindow_size = 2  # Size of attention window\nshift_size = 2  # Size of shifting window\nimage_dimension = 256  # # \u0418\u0441\u0445\u043e\u0434\u043d\u044b\u0439 \u0440\u0430\u0437\u043c\u0435\u0440 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \/ \u0412\u0445\u043e\u0434\u043d\u043e\u0439 \u0440\u0430\u0437\u043c\u0435\u0440 \u043c\u043e\u0434\u0435\u043b\u0438 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0442\u043e\u0440\u0430. \n\nnum_patch_x = IMAGE_SIZE[0] \/\/ patch_size[0]\nnum_patch_y = IMAGE_SIZE[1] \/\/ patch_size[1]","b83b3ee5":"print(num_patch_x)\nprint(num_patch_y)","a31eeea9":"train['income_pawpularity'] = np.ceil(train['Pawpularity'] \/ 33)\ntrain['income_pawpularity'].where(train[\"income_pawpularity\"] < 4 , 4.0 ,inplace=True)\n\nfig = px.parallel_categories(train, train[['Subject Focus', 'Eyes', \n                                            'Face', 'Near', 'Action', \n                                            'Accessory', 'Group',\n                                            'Collage', 'Human', \n                                            'Occlusion', 'Info', \n                                            'Blur', 'income_pawpularity']].columns, \n                             color='income_pawpularity')\nfig.show()","ea0e29cb":"sns.displot(train, x=\"Pawpularity\", hue=\"income_pawpularity\", element=\"step\")","261ed77e":"def show_img(full_path, pawpularity_file):\n    plt.figure(figsize=(25, 30))\n   \n    for fpath in range(len(full_path)):\n        image = cv2.imread(full_path[fpath])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        plt.subplot(3, 3, fpath+1)\n        plt.title('pawpularity: ' + str(pawpularity_file[fpath]))\n        plt.imshow(image)\n        plt.axis(\"off\")\n        \nfor i_income in range(1, 5):\n    index = train[train['income_pawpularity']==i_income].index\n    id_file = train.iloc[index]['Id'] \n    full_path = id_file.apply(lambda x: '{}'.format(train_image)+f'{x}.jpg') \n    random_ind = random.sample(list(full_path.index), 9) \n    pawpularity_file = train.iloc[random_ind]['Pawpularity'] \n    full_path = full_path.loc[random_ind]\n    \n    show_img(list(full_path), list(pawpularity_file))","0288d0b5":"dups = {('b148cbea87c3dcc65a05b15f78910715','e09a818b7534422fb4c688f12566e38f'): 0.9921875,\n        ('43ab682adde9c14adb7c05435e5f2e0e', '9a0238499efb15551f06ad583a6fa951'): 1.0,\n        ('dbf25ce0b2a5d3cb43af95b2bd855718', 'e359704524fa26d6a3dcd8bfeeaedd2e'): 1.0,\n        ('5a642ecc14e9c57a05b8e010414011f2', 'c504568822c53675a4f425c8e5800a36'): 0.9921875,\n        ('08440f8c2c040cf2941687de6dc5462f', 'bf8501acaeeedc2a421bac3d9af58bb7'): 1.0,\n        ('01430d6ae02e79774b651175edd40842', '6dc1ae625a3bfb50571efedc0afc297c'): 1.0,\n        ('1feb99c2a4cac3f3c4f8a4510421d6f5', '264845a4236bc9b95123dde3fb809a88'): 0.9921875,\n        ('43bd09ca68b3bcdc2b0c549fd309d1ba', '6ae42b731c00756ddd291fa615c822a1'): 0.9921875,\n        ('13d215b4c71c3dc603cd13fc3ec80181', '373c763f5218610e9b3f82b12ada8ae5'): 1.0,\n        ('3877f2981e502fe1812af38d4f511fd2', '902786862cbae94e890a090e5700298b'): 1.0,\n        ('5ef7ba98fc97917aec56ded5d5c2b099', '67e97de8ec7ddcda59a58b027263cdcc'): 1.0,\n        ('871bb3cbdf48bd3bfd5a6779e752613e', '988b31dd48a1bc867dbc9e14d21b05f6'): 0.99609375,\n        ('68e55574e523cf1cdc17b60ce6cc2f60', '9b3267c1652691240d78b7b3d072baf3'): 0.99609375,\n        ('72b33c9c368d86648b756143ab19baeb', '763d66b9cf01069602a968e573feb334'): 0.99609375,\n        ('2b737750362ef6b31068c4a4194909ed', '41c85c2c974cc15ca77f5ababb652f84'): 0.98828125,\n        ('851c7427071afd2eaf38af0def360987', 'b49ad3aac4296376d7520445a27726de'): 1.0,\n        ('9f5a457ce7e22eecd0992f4ea17b6107', 'b967656eb7e648a524ca4ffbbc172c06'): 0.91796875,\n        ('5a5c229e1340c0da7798b26edf86d180', 'dd042410dc7f02e648162d7764b50900'): 0.9921875,\n        ('a9513f7f0c93e179b87c01be847b3e4c', 'b86589c3e85f784a5278e377b726a4d4'): 0.9921875,\n        ('1059231cf2948216fcc2ac6afb4f8db8', 'bca6811ee0a78bdcc41b659624608125'): 0.96875,\n        ('87c6a8f85af93b84594a36f8ffd5d6b8', 'd050e78384bd8b20e7291b3efedf6a5b'): 1.0,\n        ('5da97b511389a1b62ef7a55b0a19a532', '8ffde3ae7ab3726cff7ca28697687a42'): 1.0,\n        ('03d82e64d1b4d99f457259f03ebe604d', 'dbc47155644aeb3edd1bd39dba9b6953'): 0.98828125,\n        ('38426ba3cbf5484555f2b5e9504a6b03', '6cb18e0936faa730077732a25c3dfb94'): 1.0,\n        ('54563ff51aa70ea8c6a9325c15f55399', 'b956edfd0677dd6d95de6cb29a85db9c'): 0.984375,\n        ('0c4d454d8f09c90c655bd0e2af6eb2e5', 'fe47539e989df047507eaa60a16bc3fd'): 1.0,\n        ('78a02b3cb6ed38b2772215c0c0a7f78e', 'c25384f6d93ca6b802925da84dfa453e'): 0.99609375}","b56ee2ee":"index_id1, index_id2 = [], []\nfor (id1, id2), sim in dups.items():\n    index_id1.append(int(train[train['Id'] == id1]['Pawpularity'].index[0]))\n    index_id2.append(int(train[train['Id'] == id2]['Pawpularity'].index[0]))\n    pawp1 = train[train['Id'] == id1]['Pawpularity'].iloc[-1]\n    pawp2 = train[train['Id'] == id2]['Pawpularity'].iloc[-1]\n    mean_paw = int(np.mean([pawp1, pawp2]))\n    train.loc[(train['Id'] == id1), 'Pawpularity'] = mean_paw\n    \ntrain = train.drop(index_id2)\ntrain = train.reset_index()\ntrain.shape","2e173329":"skfolds = StratifiedKFold(n_splits=FOLD, \n                          random_state=SEED, \n                          shuffle = True)\n    \nfor num_fold, (train_index, val_index) in enumerate(skfolds.split(train, train.Pawpularity)):\n    train.loc[val_index, 'fold'] = int(num_fold)","dbb92b28":"class Dataset(Sequence):\n\n    def __init__(self, x_set, y_set=None, batch_size=32, image_transform=None,\n                       flip=None, rotate=None, RGB=None, comix=None):\n                 \n        self.x = x_set\n        self.y = y_set\n        self.batch_size = batch_size\n        self.image_transform = image_transform\n        self.flip = flip\n        self.rotate = rotate\n        self.RGB = RGB\n        self.comix = comix\n        \n    def __len__(self):\n        return math.ceil(len(self.x) \/ self.batch_size)\n    \n    def image_augmentation(self, image=None, flip=False, rotate=False, RGB=False):\n        \n        if flip:\n            # rotate the image\n            p_flip = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n\n            if p_flip >= .8:\n                image = tf.image.random_flip_left_right(image) \n            elif p_flip <= .8 and p_flip >= .5:   \n                image = tf.image.random_flip_up_down(image) \n            elif p_flip <= .5:\n                pass\n        \n        if rotate:\n            # Rotate the image (s) counterclockwise 90 degrees\n            p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n\n            if p_rotate > .25:\n                image = tf.image.rot90(image, k=1)\n            elif p_rotate < .20 and p_rotate > .15:\n                image = tf.image.rot90(image, k=4) \n            elif p_rotate < .15:\n                pass\n\n        if RGB:\n            # playing with RGB\n            p_pixel = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n\n            if p_pixel >= .8: \n                image = tf.image.random_saturation(image, lower=.7, upper=1.3)\n            elif p_pixel <= .8 and p_pixel >= .5: \n                image = tf.image.random_contrast(image, lower=.8, upper=1.2)\n            elif p_pixel <= .5: \n                pass\n            \n        return image\n    \n    def cut(self, bs_x, bs_y, prob=1.0):\n        imgs = []; labs = []\n        for j in range(self.batch_size):\n            P = tf.cast( tf.random.uniform([],0,1) <= prob, tf.int32)\n            k = tf.cast( tf.random.uniform([],0,24), tf.int32) # \u0437\u0434\u0435\u0441\u044c \u0438\u0437\u043c\u0435\u043d\u0438\u043b\u0438 \u043d\u0430 32\n            a = tf.random.uniform([], 0, 1)*tf.cast(P, tf.float32) \n \n            img1 = bs_x[j,]\n            img2 = bs_x[k,]\n            imgs.append((1-a)*img1 + a*img2) \n            \n            lab1 = bs_y[j,]\n            lab2 = bs_y[k,]\n            #lab1 = tf.cast(lab1, tf.float32)\n            #lab2 = tf.cast(lab2, tf.float32)\n            #labs.append((1-a)*lab1 + a*lab2)\n            labs.append(np.mean([lab1, lab2]))\n  \n        return imgs, labs\n        \n    def mix(self, bs_x, bs_y, prob=1.0):\n        imgs = []; labs = []\n        for j in range(self.batch_size):\n            P = tf.cast( tf.random.uniform([],0,1) <= prob, tf.int32)\n            k = tf.cast( tf.random.uniform([], 0, self.batch_size), tf.int32)\n            # CHOOSE RANDOM LOCATION\n            x = tf.cast( tf.random.uniform([],0, IMAGE_SIZE[0]),tf.int32)\n            y = tf.cast( tf.random.uniform([],0, IMAGE_SIZE[0]),tf.int32)\n\n            b = tf.random.uniform([],0,1) \n\n            width = tf.cast(IMAGE_SIZE[0] * tf.math.sqrt(1-b), tf.int32) * P\n            ya = tf.math.maximum(0, y-width\/\/2)\n            yb = tf.math.minimum(IMAGE_SIZE[0], y+width\/\/2)\n            xa = tf.math.maximum(0, x-width\/\/2)\n            xb = tf.math.minimum(IMAGE_SIZE[0], x+width\/\/2)\n\n            # MAKE CUTMIX IMAGE\n            one    = bs_x[j, ya:yb, 0:xa, :]\n            two    = bs_x[k, ya:yb, xa:xb, :]\n            three  = bs_x[j, ya:yb, xb:IMAGE_SIZE[0], :]\n            middle = tf.concat([one, two, three], axis=1)\n            img    = tf.concat([bs_x[j, 0:ya, :, :],\n                                middle,\n                                bs_x[j, yb:IMAGE_SIZE[0], :, :]], axis=0)\n            imgs.append(img)\n\n            # MAKE CUTMIX LABEL\n            a = tf.cast(width*width\/IMAGE_SIZE[0]\/IMAGE_SIZE[0], tf.float32)\n\n            lab1 = bs_y[j,]\n            lab2 = bs_y[k,]\n\n            #labs.append((1-a)*lab1 + a*lab2)\n            labs.append(np.mean([lab1, lab2]))\n        \n        return imgs, labs\n        \n    def cut_or_mix(self, bs_x, bs_y, p=0.5):\n        \n        p_cut_mix = tf.random.uniform([],0,2, dtype=tf.int32)\n        \n        if p_cut_mix:\n            return tf.cond( \n                tf.less(tf.random.uniform([], minval=0, maxval=1, \n                                          dtype=tf.float32), \n                        tf.cast(p, tf.float32)),\n                lambda: self.cut(bs_x, bs_y), # if tf.less returns True\n                lambda: self.mix(bs_x, bs_y)  # if tf.less returns False\n            )\n        else:\n            return bs_x, bs_y\n\n    \n    def image_refinement(self, path_image):\n        image = cv2.imread(path_image)\n        image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB) \n        image = cv2.resize(image, (IMAGE_SIZE[0], IMAGE_SIZE[1]))  \n        image = tf.cast(image, tf.float32)\n        # Normalize image\n        #image = (255*(image - np.min(image))\/np.ptp(image)) \n\n        if self.image_transform is not None:\n            image = self.image_augmentation(image=image, flip=self.flip, rotate=self.rotate, RGB=self.RGB)\n        \n        return image\n    \n    def normalize_y(self, batch_y):\n        batch_y = batch_y \/ 100 \n        return batch_y\n    \n    def __getitem__(self, idx):\n        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size] \n        \n        if self.y is not None:\n            batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size] \n            batch_y = self.normalize_y(batch_y)\n                \n        list_train = [self.image_refinement(path) for path in batch_x['file_path']] \n        \n        if self.comix is not False and batch_x.shape[0] == self.batch_size:\n            list_train, batch_y = self.cut_or_mix(np.array(list_train), np.array(batch_y))\n        \n        if self.y is not None:\n            return np.array(list_train), np.array(batch_y)\n        else:\n            return np.array(list_train)","741c1629":"def window_partition(x, window_size):\n    _, height, width, channels = x.shape\n    patch_num_y = height \/\/ window_size\n    patch_num_x = width \/\/ window_size\n    x = tf.reshape(\n        x, shape=(-1, patch_num_y, window_size, patch_num_x, window_size, channels)\n    )\n    x = tf.transpose(x, (0, 1, 3, 2, 4, 5))\n    windows = tf.reshape(x, shape=(-1, window_size, window_size, channels))\n    return windows\n\n\ndef window_reverse(windows, window_size, height, width, channels):\n    patch_num_y = height \/\/ window_size\n    patch_num_x = width \/\/ window_size\n    x = tf.reshape(\n        windows,\n        shape=(-1, patch_num_y, patch_num_x, window_size, window_size, channels),\n    )\n    x = tf.transpose(x, perm=(0, 1, 3, 2, 4, 5))\n    x = tf.reshape(x, shape=(-1, height, width, channels))\n    return x\n\n\nclass DropPath(layers.Layer):\n    def __init__(self, drop_prob=None, **kwargs):\n        super(DropPath, self).__init__(**kwargs)\n        self.drop_prob = drop_prob\n\n    def call(self, x):\n        input_shape = tf.shape(x)\n        batch_size = input_shape[0]\n        rank = x.shape.rank\n        shape = (batch_size,) + (1,) * (rank - 1)\n        random_tensor = (1 - self.drop_prob) + tf.random.uniform(shape, dtype=x.dtype)\n        path_mask = tf.floor(random_tensor)\n        output = tf.math.divide(x, 1 - self.drop_prob) * path_mask\n        return output","4e4ab02f":"class WindowAttention(layers.Layer):\n    def __init__(\n        self, dim, window_size, num_heads, qkv_bias=True, dropout_rate=0.0, **kwargs\n    ):\n        super(WindowAttention, self).__init__(**kwargs)\n        self.dim = dim\n        self.window_size = window_size\n        self.num_heads = num_heads\n        self.scale = (dim \/\/ num_heads) ** -0.5\n        self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias)\n        self.dropout = layers.Dropout(dropout_rate)\n        self.proj = layers.Dense(dim)\n\n    def build(self, input_shape):\n        num_window_elements = (2 * self.window_size[0] - 1) * (\n            2 * self.window_size[1] - 1\n        )\n        self.relative_position_bias_table = self.add_weight(\n            shape=(num_window_elements, self.num_heads),\n            initializer=tf.initializers.Zeros(),\n            trainable=True,\n        )\n        coords_h = np.arange(self.window_size[0])\n        coords_w = np.arange(self.window_size[1])\n        coords_matrix = np.meshgrid(coords_h, coords_w, indexing=\"ij\")\n        coords = np.stack(coords_matrix)\n        coords_flatten = coords.reshape(2, -1)\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n        relative_coords = relative_coords.transpose([1, 2, 0])\n        relative_coords[:, :, 0] += self.window_size[0] - 1\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)\n\n        self.relative_position_index = tf.Variable(\n            initial_value=tf.convert_to_tensor(relative_position_index), trainable=False\n        )\n\n    def call(self, x, mask=None):\n        _, size, channels = x.shape\n        head_dim = channels \/\/ self.num_heads\n        x_qkv = self.qkv(x)\n        x_qkv = tf.reshape(x_qkv, shape=(-1, size, 3, self.num_heads, head_dim))\n        x_qkv = tf.transpose(x_qkv, perm=(2, 0, 3, 1, 4))\n        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]\n        q = q * self.scale\n        k = tf.transpose(k, perm=(0, 1, 3, 2))\n        attn = q @ k\n\n        num_window_elements = self.window_size[0] * self.window_size[1]\n        relative_position_index_flat = tf.reshape(\n            self.relative_position_index, shape=(-1,)\n        )\n        relative_position_bias = tf.gather(\n            self.relative_position_bias_table, relative_position_index_flat\n        )\n        relative_position_bias = tf.reshape(\n            relative_position_bias, shape=(num_window_elements, num_window_elements, -1)\n        )\n        relative_position_bias = tf.transpose(relative_position_bias, perm=(2, 0, 1))\n        attn = attn + tf.expand_dims(relative_position_bias, axis=0)\n\n        if mask is not None:\n            nW = mask.get_shape()[0]\n            mask_float = tf.cast(\n                tf.expand_dims(tf.expand_dims(mask, axis=1), axis=0), tf.float32\n            )\n            attn = (\n                tf.reshape(attn, shape=(-1, nW, self.num_heads, size, size))\n                + mask_float\n            )\n            attn = tf.reshape(attn, shape=(-1, self.num_heads, size, size))\n            attn = keras.activations.softmax(attn, axis=-1)\n        else:\n            attn = keras.activations.softmax(attn, axis=-1)\n        attn = self.dropout(attn)\n\n        x_qkv = attn @ v\n        x_qkv = tf.transpose(x_qkv, perm=(0, 2, 1, 3))\n        x_qkv = tf.reshape(x_qkv, shape=(-1, size, channels))\n        x_qkv = self.proj(x_qkv)\n        x_qkv = self.dropout(x_qkv)\n        return x_qkv\n","88b5ae74":"class SwinTransformer(layers.Layer):\n    def __init__(\n        self,\n        dim,\n        num_patch,\n        num_heads,\n        window_size=7,\n        shift_size=0,\n        num_mlp=1024,\n        qkv_bias=True,\n        dropout_rate=0.0,\n        **kwargs,\n    ):\n        super(SwinTransformer, self).__init__(**kwargs)\n\n        self.dim = dim  # number of input dimensions\n        self.num_patch = num_patch  # number of embedded patches\n        self.num_heads = num_heads  # number of attention heads\n        self.window_size = window_size  # size of window\n        self.shift_size = shift_size  # size of window shift\n        self.num_mlp = num_mlp  # number of MLP nodes\n\n        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n        self.attn = WindowAttention(\n            dim,\n            window_size=(self.window_size, self.window_size),\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            dropout_rate=dropout_rate,\n        )\n        self.drop_path = DropPath(dropout_rate)\n        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n\n        self.mlp = keras.Sequential(\n            [\n                layers.Dense(num_mlp),\n                layers.Activation(keras.activations.gelu),\n                layers.Dropout(dropout_rate),\n                layers.Dense(dim),\n                layers.Dropout(dropout_rate),\n            ]\n        )\n\n        if min(self.num_patch) < self.window_size:\n            self.shift_size = 0\n            self.window_size = min(self.num_patch)\n\n    def build(self, input_shape):\n        if self.shift_size == 0:\n            self.attn_mask = None\n        else:\n            height, width = self.num_patch\n            h_slices = (\n                slice(0, -self.window_size),\n                slice(-self.window_size, -self.shift_size),\n                slice(-self.shift_size, None),\n            )\n            w_slices = (\n                slice(0, -self.window_size),\n                slice(-self.window_size, -self.shift_size),\n                slice(-self.shift_size, None),\n            )\n            mask_array = np.zeros((1, height, width, 1))\n            count = 0\n            for h in h_slices:\n                for w in w_slices:\n                    mask_array[:, h, w, :] = count\n                    count += 1\n            mask_array = tf.convert_to_tensor(mask_array)\n\n            # mask array to windows\n            mask_windows = window_partition(mask_array, self.window_size)\n            mask_windows = tf.reshape(\n                mask_windows, shape=[-1, self.window_size * self.window_size]\n            )\n            attn_mask = tf.expand_dims(mask_windows, axis=1) - tf.expand_dims(\n                mask_windows, axis=2\n            )\n            attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)\n            attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)\n            self.attn_mask = tf.Variable(initial_value=attn_mask, trainable=False)\n\n    def call(self, x):\n        height, width = self.num_patch\n        _, num_patches_before, channels = x.shape\n        x_skip = x\n        x = self.norm1(x)\n        x = tf.reshape(x, shape=(-1, height, width, channels))\n        if self.shift_size > 0:\n            shifted_x = tf.roll(\n                x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2]\n            )\n        else:\n            shifted_x = x\n\n        x_windows = window_partition(shifted_x, self.window_size)\n        x_windows = tf.reshape(\n            x_windows, shape=(-1, self.window_size * self.window_size, channels)\n        )\n        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n\n        attn_windows = tf.reshape(\n            attn_windows, shape=(-1, self.window_size, self.window_size, channels)\n        )\n        shifted_x = window_reverse(\n            attn_windows, self.window_size, height, width, channels\n        )\n        if self.shift_size > 0:\n            x = tf.roll(\n                shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2]\n            )\n        else:\n            x = shifted_x\n\n        x = tf.reshape(x, shape=(-1, height * width, channels))\n        x = self.drop_path(x)\n        x = x_skip + x\n        x_skip = x\n        x = self.norm2(x)\n        x = self.mlp(x)\n        x = self.drop_path(x)\n        x = x_skip + x\n        return x\n","0726d7d4":"# https:\/\/keras.io\/examples\/vision\/swin_transformers\/ \u0437\u0434\u0435\u0441\u044c \u0442\u0430\u043a \u0436\u0435 \u043e\u0431\u044a\u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u044d\u0442\u0438 \u0442\u0440\u0438 \u043a\u043b\u0430\u0441\u0441\u0430 \u043f\u0440\u043e\u0447\u0438\u0442\u0430\u0439 \u0441\u0442\u0430\u0442\u044c\u044e\n\n# \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0442\u044c \u043f\u0430\u0442\u0447\u0438 \u0438\u0437 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\n\nclass PatchExtract(layers.Layer): # \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0435\u0442 \u043d\u0430 \u0432\u0445\u043e\u0434 \u0441\u043b\u043e\u0439 \u0432\u044b\u0445\u043e\u0434\u0430 new_base\n    def __init__(self, patch_size, **kwargs):\n        super(PatchExtract, self).__init__(**kwargs)\n        self.patch_size_x = patch_size[0]\n        self.patch_size_y = patch_size[0]\n\n    def call(self, images):\n        batch_size = tf.shape(images)[0]\n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=(1, self.patch_size_x, self.patch_size_y, 1),\n            strides=(1, self.patch_size_x, self.patch_size_y, 1),\n            rates=(1, 1, 1, 1),\n            padding=\"VALID\",\n        )\n        patch_dim = patches.shape[-1]\n        patch_num = patches.shape[1]\n        return tf.reshape(patches, (batch_size, patch_num * patch_num, patch_dim))\n\n# \u0432\u0441\u0442\u0430\u0432\u043b\u044f\u0442\u044c \u043f\u0430\u0442\u0447\u0438 \u0438\u0437 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\n\nclass PatchEmbedding(layers.Layer):\n    def __init__(self, num_patch, embed_dim, **kwargs):\n        super(PatchEmbedding, self).__init__(**kwargs)\n        self.num_patch = num_patch\n        self.proj = layers.Dense(embed_dim)\n        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n\n    def call(self, patch):\n        pos = tf.range(start=0, limit=self.num_patch, delta=1)\n        return self.proj(patch) + self.pos_embed(pos)\n\n# \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0442\u044c \u043f\u0430\u0442\u0447\u0438 \u0438\u0437 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\n\nclass PatchMerging(tf.keras.layers.Layer):\n    def __init__(self, num_patch, embed_dim):\n        super(PatchMerging, self).__init__()\n        self.num_patch = num_patch\n        self.embed_dim = embed_dim\n        self.linear_trans = layers.Dense(2 * embed_dim, use_bias=False)\n\n    def call(self, x):\n        height, width = self.num_patch\n        _, _, C = x.get_shape().as_list()\n        x = tf.reshape(x, shape=(-1, height, width, C))\n        x0 = x[:, 0::2, 0::2, :]\n        x1 = x[:, 1::2, 0::2, :]\n        x2 = x[:, 0::2, 1::2, :]\n        x3 = x[:, 1::2, 1::2, :]\n        x = tf.concat((x0, x1, x2, x3), axis=-1)\n        x = tf.reshape(x, shape=(-1, (height \/\/ 2) * (width \/\/ 2), 4 * C))\n        return self.linear_trans(x)","2d1df453":"input = layers.Input(input_shape)\n#x = layers.experimental.preprocessing.RandomCrop(image_dimension, image_dimension)(input)\n#x = layers.experimental.preprocessing.RandomFlip(\"horizontal\")(x)\nx = layers.experimental.preprocessing.Rescaling(1.\/255)(input)\nx = PatchExtract(patch_size)(x)\nx = PatchEmbedding(num_patch_x * num_patch_y, embed_dim)(x)\nx = SwinTransformer(\n    dim=embed_dim,\n    num_patch=(num_patch_x, num_patch_y),\n    num_heads=num_heads,\n    window_size=window_size,\n    shift_size=0,\n    num_mlp=num_mlp,\n    qkv_bias=qkv_bias,\n    dropout_rate=dropout_rate,\n)(x)\nx = PatchMerging((num_patch_x, num_patch_y), embed_dim=embed_dim)(x)\nx = layers.GlobalAveragePooling1D()(x)\nx = layers.Dense(16, activation='relu')(x)\nx = layers.BatchNormalization()(x)\nx = layers.Dense(8, activation='elu')(x)\nx = layers.BatchNormalization()(x)\noutput = layers.Dense(1)(x)","b20003aa":"model = keras.Model(input, output)\n\nmodel.compile(\n    loss=tf.keras.losses.MeanSquaredError(),\n    optimizer=OPTIMIZER,\n    metrics=[\n        tf.keras.metrics.RootMeanSquaredError(name='rmse'),\n    ]\n)","487d8a18":"def scheduler(epoch, lr):\n    if epoch < 4:\n        return lr\n    else:\n        return lr * tf.math.exp(-0.1)","57f85fe4":"def train_model(model, FOLD, image_transform=None,flip=None, rotate=None, RGB=None, comix=None):\n    \n    for fold_n in range(FOLD): \n        print('Fold #{}'.format(fold_n+1))\n        \n        # # SAVE BEST MODEL EACH FOLD  \n        sv = tf.keras.callbacks.ModelCheckpoint(\n                                            '.\/fold-%i.h5'%fold_n, monitor='val_rmse', \n                                            verbose=0, save_best_only=True,\n                                            save_weights_only=False, mode='min', \n                                            save_freq='epoch'\n                                            ) \n        \n        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2)\n        scheduler_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n        \n        callbacks = [reduce_lr,scheduler_lr]\n        \n        train_data = train[train.fold != fold_n]        \n        val_data = train[train.fold == fold_n]          \n        \n        train_dataset = Dataset(train_data, train_data.Pawpularity, BATCH_SIZE, \n                                image_transform,flip,rotate,RGB,comix) \n        \n        test_dataset = Dataset(val_data, val_data.Pawpularity, BATCH_SIZE, \n                               image_transform=False,flip=False,rotate=False,RGB=False,comix=False)  \n        \n        model_fit = model.fit(train_dataset, validation_data = test_dataset, \n                              epochs=EPOCHS, callbacks = callbacks)\n        \n    return model_fit  ","88ebddd8":"history = train_model(model, FOLD, image_transform=True, flip=True, rotate=True, RGB=False, comix=True)  ","df86e1a5":"from array import array             ","72c03f50":"test","3ea6b388":"chunks = np.array_split(test, 7)","5a7751ac":"pred = np.array([])\nfor chunk in chunks:\n    test_dataset = Dataset(chunk, batch_size = BATCH_SIZE, \n                           image_transform=False,flip=False,rotate=False,RGB=False,comix=False)\n    pred_cnn = model.predict(test_dataset)\n    #print(pred_cnn)\n    #print(pred_cnn.shape)\n    #print(pred_cnn.reshape(-1))    \n    pred = np.concatenate((pred,pred_cnn.reshape(-1)),axis=0)\n    #print(pred)","43375895":"pred.shape","2d167683":"pred","2ceba00e":"pred * 100","9ad1f074":"sample_submission.Pawpularity = pred * 100\nsample_submission.to_csv(\"submission.csv\", index=False)","7ec531cc":"sample_submission","b414db3c":"# Model training and evaluation\n## Extract and embed patches","21c27201":"# Window based multi-head self-attention","ef291337":"# Modeling","9e095aaf":"# Swin-Transformer Stuff"}}