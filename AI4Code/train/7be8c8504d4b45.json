{"cell_type":{"48865446":"code","8424c8e8":"code","dbbdc4da":"code","3f94e69d":"code","56d1cc9b":"code","4c9510da":"code","6a35db8d":"code","9f3f0e80":"code","19242a07":"code","d89244fd":"code","1935859c":"code","7c571b4c":"code","cd442b67":"code","8edbb81b":"code","67bae9df":"code","59c57ada":"code","b582da10":"code","4d17dc8e":"code","50d664dc":"code","61938ac1":"code","44e41ea0":"code","4c600f26":"code","32899917":"code","1350f468":"code","3ecaa92b":"code","a4d80d0a":"code","4e827c6b":"code","3c031d1b":"code","2d33e099":"code","0cd5f878":"code","86ed0944":"code","c8c063be":"code","0dce4730":"markdown","63736ea8":"markdown","4631970e":"markdown","b00ea922":"markdown","2f4c9385":"markdown","eb2baa02":"markdown","18e880aa":"markdown","ca3416db":"markdown","bf816569":"markdown","ca82cdb8":"markdown","b4d64c44":"markdown","cbb685b8":"markdown","28c75369":"markdown","beb1c2a0":"markdown","28465088":"markdown","cdc018c6":"markdown","70a01fb5":"markdown","1329cdd8":"markdown","72caf7c9":"markdown","b57c39ba":"markdown","629260dd":"markdown","41de95fb":"markdown","6b293dc0":"markdown","42091ceb":"markdown","a156a246":"markdown","5cdc85ac":"markdown","f61838c4":"markdown","b20dbf70":"markdown","906faf31":"markdown","aaba2c49":"markdown"},"source":{"48865446":"import pandas as pd # package for high-performance, easy-to-use data structures and data analysis\nimport numpy as np # fundamental package for scientific computing with Python\nimport matplotlib\nimport os\nfrom PIL import Image, ImageDraw\nfrom ast import literal_eval\nimport matplotlib.pyplot as plt # for plotting\nimport seaborn as sns # for making plots with seaborn\ncolor = sns.color_palette()\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.offline as offline\noffline.init_notebook_mode()\nimport imageio\nfrom skimage import data, io, filters, img_as_ubyte","8424c8e8":"BASE_PATH = '..\/input\/global-wheat-detection'\nTRAIN_DIR = f'{BASE_PATH}\/train'\nTEST_DIR = f'{BASE_PATH}\/test'\n\ntrain = pd.read_csv(f'{BASE_PATH}\/train.csv')\nsubmission = pd.read_csv(f'{BASE_PATH}\/sample_submission.csv')","dbbdc4da":"train","3f94e69d":"submission","56d1cc9b":"print('Size of train data', train.shape)\nprint('Size of submission file', submission.shape)\n","4c9510da":"# display head of train data\ndisplay(train.head())","6a35db8d":"print(f'Number of unique images in train data is {len(list(np.unique(train.image_id)))}')","9f3f0e80":"# let's have a look at the describe function\ndisplay(train.describe())","19242a07":"display(submission.head())","d89244fd":"# checking missing data\ntotal = train.isnull().sum().sort_values(ascending = False)\npercent = (train.isnull().sum()\/train.isnull().count()*100).sort_values(ascending = False)\nmissing_train_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_train_data.head()","1935859c":"#Splitting the bboxes into x, y, w and h\nbboxs = np.stack(train['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\nfor i, column in enumerate(['x', 'y', 'w', 'h']):\n    train[column] = bboxs[:,i]\n\ntrain['x1'] = train['x'] + train['w']\ntrain['y1'] = train['y'] + train['h']","7c571b4c":"# Create a dataframe with all train images\nall_train_images = pd.DataFrame([i.split('\/')[-1][:-4] for i in train_dir])\nall_train_images.columns=['image_id']\n\n\n# Merge all train images with the bounding boxes dataframe\nall_train_images = all_train_images.merge(train, on='image_id', how='left')\n\n# replace nan values with zeros\nall_train_images['bbox'] = all_train_images.bbox.fillna('[0,0,0,0]')\n\n# split bbox column\nbbox_items = all_train_images.bbox.str.split(',', expand=True)\nall_train_images['bbox_xmin'] = bbox_items[0].str.strip('[ ').astype(float)\nall_train_images['bbox_ymin'] = bbox_items[1].str.strip(' ').astype(float)\nall_train_images['bbox_width'] = bbox_items[2].str.strip(' ').astype(float)\nall_train_images['bbox_height'] = bbox_items[3].str.strip(' ]').astype(float)","cd442b67":"nobboxes_images = all_train_images[~all_train_images.image_id.isin(train_df.image_id)]\nprint('Number of images with no bounding boxes:', len(nobboxes_images))","8edbb81b":"all_train_images['image_id'].value_counts().iplot(kind='hist',bins=30,color='blue',xTitle='No. of bboxes per Image Id',yTitle='No. of Images')","67bae9df":"def plot_count(df, feature, title='', size=2):\n    f, ax = plt.subplots(1,1, figsize=(4*size,3*size))\n    total = float(len(df))\n    sns.countplot(df[feature],order = df[feature].value_counts().index, palette='Set2')\n    plt.title(title)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n    plt.show()","59c57ada":"plot_count(df=train, feature='source', title = 'data source count and %age plot', size=3)","b582da10":"def display_images(images): \n    f, ax = plt.subplots(5,3, figsize=(18,22))\n    for i, image_id in enumerate(images):\n        image_path = os.path.join(TRAIN_DIR, f'{image_id}.jpg')\n        image = Image.open(image_path)\n        \n        # get all bboxes for given image in [xmin, ymin, width, height]\n        bboxes = [literal_eval(box) for box in train[train['image_id'] == image_id]['bbox']]\n        # draw rectangles on image\n        draw = ImageDraw.Draw(image)\n        for bbox in bboxes:    \n            draw.rectangle([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]], width=3)\n            \n        ax[i\/\/3, i%3].imshow(image) \n        image.close()       \n        ax[i\/\/3, i%3].axis('off')\n\n        source = train[train['image_id'] == image_id]['source'].values[0]\n        ax[i\/\/3, i%3].set_title(f\"image_id: {image_id}\\nSource: {source}\")\n        \n\n    plt.show() ","4d17dc8e":"     def rescale_image(image, bboxes, factor):\n                '''\\n\",\n            image : image_path that needs to be rescaled\\n\",\n            bboxes : list of bounding boxes for that image\\n\",\n            factor : rescaling factor, preferably an integer. We scale both dimensions equally\\n\",\n        '''\n        img = []\n        for i in images:\n            img.append(i)\n        x_size = im.shape[0]\n        y_size = im.shape[1]\n    \n        resized_image = cv2.resize(img, dsize=( int(x_size \/ factor), int(y_size \/ factor)), interpolation=cv2.INTER_CUBIC)\\n\",\n        \n        fig,ax = plt.subplots(1)\n        fig.set_size_inches(15, 10)\n        ax.imshow(resized_image)\n        \n        new_bboxes = []\\n\n        for xmin, ymin, width, height in bboxes:\n            rect = patches.Rectangle( (int(xmin\/factor) ,int(ymin\/factor)), int(width\/factor), int(height\/factor), linewidth=1, edgecolor='r', facecolor='none')\n            ax.add_patch(rect)\n            new_bboxes.append([int(xmin\/factor) ,int(ymin\/factor), int(width\/factor), int(height\/factor)])\n        return resized_image, new_bboxes\n        \n  ","50d664dc":"images = train.sample(n=15, random_state=42)['image_id'].values\ndisplay_images(images)\n","61938ac1":"images","44e41ea0":"def display_images_large(images): \n    f, ax = plt.subplots(5,2, figsize=(20, 50))\n    for i, image_id in enumerate(images):\n        image_path = os.path.join(TRAIN_DIR, f'{image_id}.jpg')\n        image = Image.open(image_path)        \n        bboxes = [literal_eval(box) for box in train[train['image_id'] == image_id]['bbox']]\n        # draw rectangles on image\n        draw = ImageDraw.Draw(image)\n        for bbox in bboxes:    \n            draw.rectangle([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]], width=3)\n            \n        ax[i\/\/2, i%2].imshow(image) \n        ax[i\/\/2, i%2].axis('off')\n        source = train[train['image_id'] == image_id]['source'].values[0]\n        ax[i\/\/2, i%2].set_title(f\"image_id: {image_id}\\nSource: {source}\")\n\n    plt.show() ","4c600f26":"images = train.sample(n=10, random_state=42)['image_id'].values\ndisplay_images_large(images)","32899917":"def display_test_images(images): \n    f, ax = plt.subplots(5,2, figsize=(20, 50))\n    for i, image_id in enumerate(images):\n        image_path = os.path.join(TEST_DIR, f'{image_id}.jpg')\n        image = Image.open(image_path)        \n            \n        ax[i\/\/2, i%2].imshow(image) \n        ax[i\/\/2, i%2].axis('off')\n        ax[i\/\/2, i%2].set_title(f\"image_id: {image_id}\")\n\n    plt.show()","1350f468":"# since we need to predict bounding boxes for test images, hence below images do not have any bounding boxes\ntest_images = submission.image_id.values\ndisplay_test_images(test_images)","3ecaa92b":"# Plots the images as per the annotations format and applied augmentations\ndef plot_image_list(annotations_list, subtitle_list, cols=2, title='Image Examples'):\n    fig, axs = plt.subplots(nrows=1, ncols=cols, figsize=(16,12), squeeze=False)\n    for i, (annotations, title) in enumerate(zip(annotations_list, subtitle_list)):\n        axs[i \/\/ cols][i % cols].imshow(annotations['image'])\n        axs[i \/\/ cols][i % cols].set_title(title, fontsize=14)\n        for bbox in annotations['bboxes']:\n            rect = patches.Rectangle((bbox[0],bbox[1]),bbox[2],bbox[3],linewidth=1,edgecolor='r',facecolor='none')\n            axs[i \/\/ cols][i % cols].add_patch(rect) \n    fig.suptitle(title, fontsize=18)\n    plt.tight_layout()","a4d80d0a":"def get_aug(aug, min_area=0., min_visibility=0.):\n    return A.Compose(aug, bbox_params=A.BboxParams(format='coco', min_area=min_area, \n                                               min_visibility=min_visibility, label_fields=['category_id']))","4e827c6b":"# Applying Vertical Flip on the Original Image\naug = get_aug([A.VerticalFlip(p=1)])\nverticalFlip = aug(**orig_annotations)\nannotations_list=[orig_annotations, verticalFlip]\nsubtitle_list=['Original Image', 'Vertically Flipped Image']\nplot_image_list(annotations_list, subtitle_list, title='Vertical Flip' )","3c031d1b":"# Applying Horizontal Flip on the Original Image\naug = get_aug([A.HorizontalFlip(p=1)])\nhorizontalFlip = aug(**orig_annotations)\nannotations_list=[orig_annotations, horizontalFlip]\nsubtitle_list=['Original Image', 'Horizontally Flipped Image']\nplot_image_list(annotations_list, subtitle_list, title='Horizontal Flip' )","2d33e099":"# Applying Center Cropping the Original Image\naug = get_aug([A.CenterCrop(p=1, height=512, width=512)])\nCropped = aug(**orig_annotations)\nannotations_list=[orig_annotations, Cropped]\nsubtitle_list=['Original Image', 'Centre Cropped Image']\nplot_image_list(annotations_list, subtitle_list, title='Image Cropping' )","0cd5f878":"# Changing the Brightness of the image randomly\naug = get_aug([A.RandomBrightness(p=0.4)])\nbrightness = aug(**orig_annotations)\nannotations_list=[orig_annotations, brightness]\nsubtitle_list=['Original Image', 'Random Brightness']\nplot_image_list(annotations_list, subtitle_list, title='Change in Brightness' )","86ed0944":"# Applying Random SunFlare\n\naug = get_aug([A.RandomSunFlare(p=1)])\nshadow = aug(**orig_annotations)\nannotations_list=[orig_annotations, shadow]\nsubtitle_list=['Original Image', 'Random Sun Flare']\nplot_image_list(annotations_list, subtitle_list, title='Random Sun Flare Results' )","c8c063be":"# webinar on how to improve wheat heads counting thanks to the Global Wheat Challenge ?\n\nfrom IPython.display import IFrame, YouTubeVideo\nYouTubeVideo('Wr44me5eyWY',width=600, height=400)\n","0dce4730":"<div align=\"center\"><img src=\"http:\/\/www.global-wheat.com\/wp-content\/uploads\/2020\/04\/ILLU_01_EN.jpg\" width=\"800\"\/><\/div>\n<span STYLE=\"text-decoration:underline\">\n\n\n","63736ea8":"In this competition, you\u2019ll detect wheat heads from outdoor images of wheat plants, including wheat datasets from around the globe. Using worldwide data, you will focus on a generalized solution to estimate the number and size of wheat heads. To better gauge the performance for unseen genotypes, environments, and observational conditions, the training dataset covers multiple regions. You will use more than 3,000 images from Europe (France, UK, Switzerland) and North America (Canada). The test data includes about 1,000 images from Australia, Japan, and China. ","4631970e":"## 2.2 Reading data","b00ea922":"# Possible Approaches for Prediction\n1. We can directly create an object detection neural network for this problem. But it would try to find wheat in every picture even if it doesn't have any. This might result in false positives.\n2. Another approach is to create a classification + object detection ensemble model which would first classify whether the image has any wheat or not. The images that are classified as having wheats will be passed to the object detector for bounding box detection. This would reduce the problem of false positives but might lead to some false negatives.","2f4c9385":"### 3.2.2 Visualizing test images\n\n","eb2baa02":"## References:\n\n* image visualization help taken from https:\/\/www.kaggle.com\/devvindan\/wheat-detection-eda\n* http:\/\/www.global-wheat.com\/2020-challenge\/","18e880aa":"### Q5. How dataset is prepared\n\nThe [Global Wheat Head Dataset](http:\/\/www.global-wheat.com\/2020-challenge\/) is led by nine research institutes from seven countries: the University of Tokyo, Institut national de recherche pour l\u2019agriculture, l\u2019alimentation et l\u2019environnement, Arvalis, ETHZ, University of Saskatchewan, University of Queensland, Nanjing Agricultural University, and Rothamsted Research. These institutions are joined by many in their pursuit of accurate wheat head detection, including the Global Institute for Food Security, DigitAg, Kubota, and Hiphen.\n\n","ca3416db":"## 1.2 Let's know more by answering few Questions","bf816569":"## 3. Exploratory Data Analysis (EDA)","ca82cdb8":"## 2.1 Loading Libraries","b4d64c44":"### Q3. What are the columns in the data\n\n* `image_id` - the unique image ID\n* `width` - the width of the images\n* `height` - the height of the images\n* `bbox` - a bounding box, formatted as a Python-style list of [xmin, ymin, width, height]\n* `source` - the source of the data","cbb685b8":"## 2.4 Table overview","28c75369":"### Q6 What is mAP(the metric used for evaluation)?\n\nThis competition is evaluated on the **mean average precision** at different intersection over union (IoU) thresholds.\n\n`MAP(mean average precision)`: **mAP (mean average precision)** is the average of AP. In some context, we compute the AP for each class and average them. But in some context, they mean the same thing. For example, under the COCO context, there is no difference between AP and mAP.\n\n\n![](https:\/\/i.stack.imgur.com\/JlHnn.jpg)\n\n> Important note: if there are no ground truth objects at all for a given image, ANY number of predictions (false positives) will result in the image receiving a score of zero, and being included in the mean average precision.\n\n\n\n\nPlease visit following links to know more about MAP\n* https:\/\/www.kaggle.com\/c\/global-wheat-detection\/overview\/evaluation\n* https:\/\/kharshit.github.io\/blog\/2019\/09\/20\/evaluation-metrics-for-object-detection-and-segmentation\n* https:\/\/towardsdatascience.com\/breaking-down-mean-average-precision-map-ae462f623a52\n* https:\/\/datascience.stackexchange.com\/questions\/25119\/how-to-calculate-map-for-detection-task-for-the-pascal-voc-challenge","beb1c2a0":"### Q1. Why are we solving this problem?\n\nFor several years, agricultural research has been using sensors to observe plants at key moments in their development. However, some important plant traits are still measured manually. One example of this is the manual counting of wheat ears from digital images \u2013 a long and tedious job. Factors that make it difficult to manually count wheat ears from digital images include the possibility of overlapping ears, variations in appearance according to maturity and genotype, the presence or absence of barbs, head orientation and even wind.  \n\nHowever, accurate wheat head detection in outdoor field images can be visually challenging. There is often overlap of dense wheat plants, and the wind can blur the photographs. Both make it difficult to identify single heads. Additionally, appearances vary due to maturity, color, genotype, and head orientation. Finally, because wheat is grown worldwide, different varieties, planting densities, patterns, and field conditions must be considered. Models developed for wheat phenotyping need to generalize between different growing environments. Current detection methods involve one- and two-stage detectors (Yolo-V3 and Faster-RCNN), but even when trained with a large dataset, a bias to the training region remains.\n\nThere is the need for a robust and accurate computer model that is capable of counting wheat ears from digital images. This model will benefit phenotyping research and help producers around the world assess ear density, health and maturity more effectively. Some work has already been done in deep learning, though it has resulted in too little data to have a generic model.  \n<br>\n","28465088":"### 3.2.1 train images","cdc018c6":"**inference**\n\n* `ethz_1` and `arvalis_1` are the 2 major data sources (contributing around 65% 0f total data).\n* Dataset is not balanced in terms of source provided.\n","70a01fb5":"###  Let's take a more closer look ","1329cdd8":"### number of unique images in train dataset","72caf7c9":"## 3.2 Visualizing images with bounding boxes","b57c39ba":"## Some insights about model selection and other tricks:\n* Till now, EfficentDet seems to outperform other model architectures.\n* Augmentation always helps improve accuracy.\n* Cutmix and mixup are specially useful types of augmentations.\n* 5 fold training with ensemble based on **WBF** seems to work great.\n* Training is a very very slow process and using kaggle for trainig is not a very good idea. Use colab with some tricks insted.","629260dd":"#### <p><span style=\"color:green\">This Kernel is work in progress, will update soon <\/br><\/span><\/p>\n\n### <p><span style=\"color:red\">Ending note: <br>Please upvote this kernel if you like it . It motivates me to produce more quality content :)<\/br><\/span><\/p>","41de95fb":"### train data","6b293dc0":"### 3.1 Checking for data `source` distribution\n","42091ceb":"## 2.3 Peek at Dataset\n\n* There are 147793 images in the train data\n* We need to predict bounding boxes around each wheat head in images that have them.","a156a246":"### Q4. What am I predicting?\n\nYou are attempting to predict bounding boxes around each wheat head in images that have them. If there are no wheat heads, you must predict no bounding boxes.","5cdc85ac":"### 2.5 Check for missing values\n\n\n","f61838c4":"# <span STYLE=\"text-decoration:underline\">Global Wheat Detection<\/span>","b20dbf70":"### submission file","906faf31":"# 2. Getting Data","aaba2c49":"### Q2. How the dataset looks like?\n\nThe dataset contains following 4 important files\/folders\n\n* `train.csv` - the training data\n* `sample_submission.csv` - a sample submission file in the correct format\n* `train.zip` - training images\n* `test.zip` - test images\n\n> **Note**: Most of the test set images are hidden. A small subset of test images has been included for your use in writing code.\n\n"}}