{"cell_type":{"2e325d69":"code","ba18ff6f":"code","c75236a9":"code","489ebbab":"code","e331f374":"code","2535503b":"code","028e1511":"code","e29a5777":"code","548881e4":"code","2c614aa6":"code","9a2d4c09":"code","8ae71d07":"code","52b44bac":"code","4b8938d4":"code","e20a9472":"code","50723e55":"code","87c3d741":"code","87b64e49":"code","4fa7729b":"code","ab65c68a":"code","7fd556e8":"code","dbf1113b":"code","968e6c8e":"code","2d3499c0":"code","2ee27450":"code","fd49ba5e":"code","9140e10e":"code","ca6b4697":"code","227552ef":"code","659fbd84":"code","1a9f3770":"code","0fdfa016":"markdown","fea20467":"markdown","452417e5":"markdown","78213ed6":"markdown","6d4fe58c":"markdown","60dd9255":"markdown","49779da9":"markdown","1708476a":"markdown","2396870d":"markdown","747bc215":"markdown","7d7b9ecb":"markdown","a7137da4":"markdown"},"source":{"2e325d69":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ba18ff6f":"import warnings\nimport seaborn as sns\nimport matplotlib.pylab as plt\nimport PIL\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nfrom keras import backend as K\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfrom keras.applications import *\n\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom keras import layers, models, optimizers","c75236a9":"warnings.filterwarnings('ignore')\nK.image_data_format()","489ebbab":"def get_base_model_name(base_model):\n    if(base_model is Xception):\n        return \"Xception\"\n    elif(base_model is VGG16):\n        return \"VGG16\"\n    elif(base_model is VGG19):\n        return \"VGG19\"\n    elif(base_model is ResNet50):\n        return \"ResNet50\"\n    elif(base_model is ResNet101):\n        return \"ResNet101\"\n    elif(base_model is ResNet152):\n        return \"ResNet152\"\n    elif(base_model is ResNet50V2):\n        return \"ResNet50V2\"\n    elif(base_model is ResNet101V2):\n        return \"ResNet101V2\"\n    elif(base_model is ResNet152V2):\n        return \"ResNet152V2\"\n    elif(base_model is ResNeXt50):\n        return \"ResNeXt50\"\n    elif(base_model is ResNeXt101):\n        return \"ResNeXt101\"\n    elif(base_model is InceptionV3):\n        return \"InceptionV3\"\n    elif(base_model is InceptionResNetV2):\n        return \"InceptionResNetV2\"\n    elif(base_model is MobileNet):\n        return \"MobileNet\"\n    elif(base_model is MobileNetV2):\n        return \"MobileNetV2\"\n    elif(base_model is DenseNet121):\n        return \"DenseNet121\"\n    elif(base_model is DenseNet16):\n        return \"DenseNet16\"\n    elif(base_model is DenseNet201):\n        return \"DenseNet201\"\n    elif(base_model is NASNetMobile):\n        return \"NASNetMobile\"\n    elif(base_model is NASNetLarge):\n        return \"NASNetLarge\"","e331f374":"def get_base_model_image_size(base_model):\n    if(base_model in [Xception, InceptionV3, InceptionResNetV2]):\n        return 299\n    elif(base_model in [VGG16, VGG19, ResNet50, ResNet101, ResNet152, ResNet50V2, ResNet101V2, ResNet152V2, ResNeXt50, ResNeXt101, MobileNet, MobileNetV2, DenseNet121, DenseNet16, DenseNet201, NASNetMobile]):\n        return 224\n    elif(base_model in [NASNetLarge]):\n        return 331","2535503b":"BATCH_SIZE = 32\nEPOCHS = 100\nk_folds = 5\nPATIENCE = 4\nSEED = 2019\nBASE_MODEL = Xception\nIMAGE_SIZE = get_base_model_image_size(BASE_MODEL)\n\nJUST_FOR_TESTING=True\n#for submission, change JUST_FOR_TRAINING to False","028e1511":"DATA_PATH = '..\/input'","e29a5777":"TRAIN_IMG_PATH = os.path.join(DATA_PATH, 'train')\nTEST_IMG_PATH = os.path.join(DATA_PATH, 'test')","548881e4":"df_train = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\ndf_test = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\ndf_class = pd.read_csv(os.path.join(DATA_PATH, 'class.csv'))","2c614aa6":"df_train.head()","9a2d4c09":"if(JUST_FOR_TESTING):\n    k_folds=2\n    PATIENCE=2\n    df_train=df_train[:2048]\n    df_test=df_train[:10]","8ae71d07":"plt.figure(figsize=(15,6))\nsns.countplot('class', data=df_train)\nplt.show()","52b44bac":"df_train['class'].value_counts()","4b8938d4":"df_train['class'].value_counts().mean()","e20a9472":"df_train['class'].value_counts().describe()","50723e55":"def crop_boxing_img(img_name, margin=0, size=(IMAGE_SIZE,IMAGE_SIZE)):\n    if img_name.split('_')[0] == 'train':\n        PATH = TRAIN_IMG_PATH\n        data = df_train\n    else:\n        PATH = TEST_IMG_PATH\n        data = df_test\n\n    img = PIL.Image.open(os.path.join(PATH, img_name))\n    pos = data.loc[data[\"img_file\"] == img_name, ['bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2']].values.reshape(-1)\n\n    width, height = img.size\n    x1 = max(0, pos[0] - margin)\n    y1 = max(0, pos[1] - margin)\n    x2 = min(pos[2] + margin, width)\n    y2 = min(pos[3] + margin, height)\n\n    return img.crop((x1, y1, x2, y2)).resize(size)","87c3d741":"TRAIN_CROPPED_PATH = '..\/cropped_train'\nTEST_CROPPED_PATH = '..\/cropped_test'","87b64e49":"if (os.path.isdir(TRAIN_CROPPED_PATH) == False):\n    os.mkdir(TRAIN_CROPPED_PATH)\n\nif (os.path.isdir(TEST_CROPPED_PATH) == False):\n    os.mkdir(TEST_CROPPED_PATH)\n\nfor i, row in df_train.iterrows():\n    cropped = crop_boxing_img(row['img_file'])\n    cropped.save(os.path.join(TRAIN_CROPPED_PATH, row['img_file']))\n\nfor i, row in df_test.iterrows():\n    cropped = crop_boxing_img(row['img_file'])\n    cropped.save(os.path.join(TEST_CROPPED_PATH, row['img_file']))","4fa7729b":"df_train['class'] = df_train['class'].astype('str')\ndf_train = df_train[['img_file', 'class']]\ndf_test = df_test[['img_file']]","ab65c68a":"model_path = '.\/'\nif not os.path.exists(model_path):\n    os.mkdir(model_path)","7fd556e8":"def get_callback(model_name, patient):\n    ES = EarlyStopping(\n        monitor='val_loss', \n        patience=patient, \n        mode='min', \n        verbose=1)\n    RR = ReduceLROnPlateau(\n        monitor = 'val_loss', \n        factor = 0.5, \n        patience = patient \/ 2, \n        min_lr=0.000001, \n        verbose=1, \n        mode='min')\n    MC = ModelCheckpoint(\n        filepath=model_name, \n        monitor='val_loss', \n        verbose=1, \n        save_best_only=True, \n        mode='min')\n\n    return [ES, RR, MC]","dbf1113b":"def get_model(model_name, iamge_size):\n    base_model = model_name(weights='imagenet', input_shape=(iamge_size,iamge_size,3), include_top=False)\n    #base_model.trainable = False\n    model = models.Sequential()\n    model.add(base_model)\n    model.add(layers.GlobalAveragePooling2D())\n    model.add(layers.Dense(2048, activation='relu', kernel_initializer='he_normal'))\n    model.add(layers.Dropout(0.15))\n \n    model.add(layers.Dense(196, activation='softmax', kernel_initializer='lecun_normal'))\n    model.summary()\n\n    optimizer = optimizers.Nadam(lr=0.0002)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n\n    return model","968e6c8e":"train_datagen = ImageDataGenerator(\n    rescale=1.\/255,\n    #featurewise_center= True,  # set input mean to 0 over the dataset\n    #samplewise_center=True,  # set each sample mean to 0\n    #featurewise_std_normalization= True,  # divide inputs by std of the dataset\n    #samplewise_std_normalization=True,  # divide each input by its std\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=False,\n    zoom_range=0.2,\n    #shear_range=0.2,\n    #brightness_range=(1, 1.2),\n    #fill_mode='nearest'\n    )\n\nvalid_datagen = ImageDataGenerator(\n    rescale=1.\/255,\n    #featurewise_center= True,  # set input mean to 0 over the dataset\n    #samplewise_center=True,  # set each sample mean to 0\n    #featurewise_std_normalization= True,  # divide inputs by std of the dataset\n    #samplewise_std_normalization=True  # divide each input by its std\n    )\ntest_datagen = ImageDataGenerator(\n    rescale=1.\/255,\n    #featurewise_center= True,  # set input mean to 0 over the dataset\n    #samplewise_center=True,  # set each sample mean to 0\n    #featurewise_std_normalization= True,  # divide inputs by std of the dataset\n    #samplewise_std_normalization=True,  # divide each input by its std\n    )","2d3499c0":"skf = StratifiedKFold(n_splits=k_folds, random_state=SEED)\n#skf = KFold(n_splits=k_folds, random_state=SEED)","2ee27450":"j = 1\nmodel_names = []\nfor (train_index, valid_index) in skf.split(\n    df_train['img_file'], \n    df_train['class']):\n\n    traindf = df_train.iloc[train_index, :].reset_index()\n    validdf = df_train.iloc[valid_index, :].reset_index()\n\n    print(\"=========================================\")\n    print(\"====== K Fold Validation step => %d\/%d =======\" % (j,k_folds))\n    print(\"=========================================\")\n    \n    train_generator = train_datagen.flow_from_dataframe(\n        dataframe=traindf,\n        directory=TRAIN_CROPPED_PATH,\n        x_col='img_file',\n        y_col='class',\n        target_size= (IMAGE_SIZE, IMAGE_SIZE),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=BATCH_SIZE,\n        seed=SEED,\n        shuffle=True\n        )\n    \n    valid_generator = valid_datagen.flow_from_dataframe(\n        dataframe=validdf,\n        directory=TRAIN_CROPPED_PATH,\n        x_col='img_file',\n        y_col='class',\n        target_size= (IMAGE_SIZE, IMAGE_SIZE),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=BATCH_SIZE,\n        seed=SEED,\n        shuffle=True\n        )\n    \n    model_name = model_path + str(j) + '_'+get_base_model_name(BASE_MODEL)+'.hdf5'\n    model_names.append(model_name)\n    \n    model = get_model(BASE_MODEL, IMAGE_SIZE)\n    \n    try:\n        model.load_weights(model_name)\n    except:\n        pass\n        \n    history = model.fit_generator(\n        train_generator,\n        steps_per_epoch=len(traindf.index) \/ BATCH_SIZE,\n        epochs=EPOCHS,\n        validation_data=valid_generator,\n        validation_steps=len(validdf.index) \/ BATCH_SIZE,\n        verbose=1,\n        shuffle=False,\n        callbacks = get_callback(model_name, PATIENCE)\n        )\n        \n    j+=1","fd49ba5e":"test_generator = test_datagen.flow_from_dataframe(\n    dataframe=df_test,\n    directory=TEST_CROPPED_PATH,\n    x_col='img_file',\n    y_col=None,\n    target_size= (IMAGE_SIZE, IMAGE_SIZE),\n    color_mode='rgb',\n    class_mode=None,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)","9140e10e":"prediction = []\nfor i, name in enumerate(model_names):\n    model = get_model(BASE_MODEL, IMAGE_SIZE)\n    model.load_weights(name)\n    \n    test_generator.reset()\n    pred = model.predict_generator(\n        generator=test_generator,\n        steps = len(df_test)\/BATCH_SIZE,\n        verbose=1\n    )\n    prediction.append(pred)\n\ny_pred = np.mean(prediction, axis=0)","ca6b4697":"preds_class_indices=np.argmax(y_pred, axis=1)","227552ef":"labels = (train_generator.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\nfinal_pred = [labels[k] for k in preds_class_indices]","659fbd84":"len(final_pred)\n","1a9f3770":"submission = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))\nif(JUST_FOR_TESTING):\n    submission=submission[:10]\nsubmission[\"class\"] = final_pred\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()\n#DO NOT SUBMIT RESULT if JUST_FOR_TESTING is TRUE","0fdfa016":"Cropping\ud55c \uc774\ubbf8\uc9c0 \uc800\uc7a5\ud558\uae30 \uc704\ud55c \ub514\ub809\ud1a0\ub9ac \uc0dd\uc131\ubc0f Cropping","fea20467":"5\uac1c\uc758 Fold\ub97c \ud6c8\ub828\ud558\uba74\uc11c \uac01 \ud3f4\ub4dc\ubcc4 Val loss\uac00 \uac00\uc7a5 \uc791\uc740 model weight\ub97c \uc800\uc7a5\ud569\ub2c8\ub2e4.\n\n\ud6c8\ub828\uc774 \ub05d\ub098\uba74 5\uac1c\uc758 Weight\ud30c\uc77c\uc774 \uc0dd\uc131\ub429\ub2c8\ub2e4.","452417e5":"5\uac1c\uc758 Fold\uc5d0\uc11c \ud6c8\ub828\ud55c \uacb0\uacfc\ub4e4(Weight)\ub97c \ubd88\ub7ec\ub4e4\ub824\uc11c 5\ubc88 \uc608\uce21\ud558\uace0 \ud3c9\uade0.","78213ed6":"\uac01 \ucc28\uc885\ubcc4 \ud3c9\uade0 \ubd84\ud3ec\uac1c\uc218\ub294 51\uac1c\uc784.","6d4fe58c":"crop_boxing\uc740 \n\n [Daehun Gwak\ub2d8 \ucee4\ub110](https:\/\/www.kaggle.com\/daehungwak\/keras-how-to-use-pretrained-model)\n\n [\ud5c8\ud0dc\uba85\ub2d8 \ucee4\ub110](https:\/\/www.kaggle.com\/tmheo74\/3rd-ml-month-car-image-cropping)\n \n \uc5d0\uc11c \uac00\uc838\uc654\uc2b5\ub2c8\ub2e4.","60dd9255":"\uc0ac\uc804 \ud6c8\ub828\ubaa8\ub378\uc5d0 \uce35\uc744 \ub354\ud574\uc11c \ud6c8\ub828 \uc2dc\ud0a4\ub294 \uad6c\uc870\ub85c \ubaa8\ub378\uc744 \uad6c\uc131\ud558\uc600\uc2b5\ub2c8\ub2e4.\nDropout\uc740 \uacfc\uc801\ud569\uc744 \ucd5c\ub300\ud55c \uc904\uc5ec\ubcfc \uc6a9\ub3c4\ub85c \ucd94\uac00\ud588\uc2b5\ub2c8\ub2e4.","49779da9":"\uc704\uc5d0\uc11c JUST_FOR_TESTING\uc740 \ube60\ub974\uac8c \ubaa8\ub378\uc774 \uc624\ub958\uc5c6\uc774 \ub3cc\uc544\uac00\ub294\uc9c0\ub9cc \ud655\uc778\ud558\uace0 \uc2f6\uc744 \ub54c\uc5d0 \uc0ac\uc6a9\ud558\ub294 \uac83\uc73c\ub85c, \uc544\ub798\uc5d0 \uc11c\uc220\ub41c(janged\ub2d8\uaed8\uc11c \uc6d0\ub798 \uc801\uc5b4\uc8fc\uc2e0) \uc218\uce58\ub294 \ubaa8\ub450 \uc774\uac83\uc774 False, \uc989 \uc804\uccb4 \ub370\uc774\ud130\uc14b\uc5d0 \ub300\ud55c \uc774\uc57c\uae30\uc785\ub2c8\ub2e4.","1708476a":"\uc870\ub9cc\uac04 F1\uc2a4\ucf54\uc5b4\ub3c4 \ubc18\uc601\ud574\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4!","2396870d":"119\ubc88 \uc720\ud615\uc774 84\uac1c\ub85c \uc81c\uc77c \ub9ce\uace0,\n\n136\ubc88 \uc720\ud615\uc774 30\uac1c\ub85c \uac00\uc7a5 \uc801\uc74c.","747bc215":"https:\/\/www.kaggle.com\/janged\/3rd-ml-month-xception-stratifiedkfold-ensemble\n\uc704 \ucee4\ub110\uc5d0\uc11c \uc57d\uac04?\uc758 \ubcc0\ud615\ub9cc \ud55c \uac83\uc785\ub2c8\ub2e4.\n\uadf8\ub7f0\uace0\ub85c \uc2e4\uc9c8\uc801\uc73c\ub85c \uc81c\uac00 \ud55c \uac74 (\uc81c \uae30\uc900\uc5d0\uc11c\uc9c0\ub9cc, \ub0a8\ub4e4\ub3c4 \uadf8\ub807\uae30\ub97c \ud76c\ub9dd\ud558\uba70)\uc880 \ub354 \ub2e4\uc591\ud558\uac8c \uc2e4\ud5d8\ud574\ubcf4\uae30 \uc26c\uc6b4 \ud615\ud0dc\ub85c \ubc14\uafbc \uac83\uc77c \ubfd0\uc785\ub2c8\ub2e4.","7d7b9ecb":"\uc704\uc758 \ub0b4\uc6a9\uc744 \uc694\uc57d\ud558\uc5ec \ucd9c\ub825\ud574 \ubcf4\uba74,\n\ucd5c\ub300\uac12 84, \ucd5c\uc18c\uac12 30, \ud3c9\uade051\ub85c \ud3b8\ucc28\uac00 \uc880 \uc788\ub2e4\uace0 \ubcfc \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.\n\uadf8\ub798\uc11c KFold\ub300\uc2e0 StratifiedKFold\ub97c \uc0ac\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4.","a7137da4":"* EarlyStopping\uc740 \uc778\uc790\uc778 Patient\uac00 \uc608\ub97c \ub4e4\uc5b4 10\uc73c\ub85c \ub4e4\uc5b4\uc628\ub2e4\uba74, Val_loss\uac00 10\ud68c \uc774\uc0c1 \uc88b\uc544\uc9c0\uc9c0 \uc54a\uc73c\uba74 Stop.\n* ReduceLROnPlateau\ub294 Patient\/2 \ud68c\ub9cc\ud07c Val_loss\uac00 \uc88b\uc544\uc9c0\uc9c0 \uc54a\uc73c\uba74 learing late\ub97c \ubc18\uc73c\ub85c \uc904\uc774\ub294 \uc6a9\ub3c4.\n* ModelCheckPoint\ub294 Val_loss\uac00 \uac00\uc7a5 \uc88b\uc740(\uac00\uc7a5\uc791\uc740) \uacbd\uc6b0\uc5d0 \ud574\ub2f9 \ud6c8\ub828 Weight \ud30c\uc77c\ub85c \uc800\uc7a5."}}