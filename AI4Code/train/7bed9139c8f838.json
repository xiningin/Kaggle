{"cell_type":{"d33150f0":"code","f7b20c0d":"code","543c04d7":"code","f9bde7f2":"code","70dc9628":"code","3041ad67":"code","59337fa0":"code","d8f99bbc":"code","11c6b524":"code","23141327":"code","1c2ec920":"code","8acd5815":"code","b5d2fa4d":"code","c530b845":"code","f4ab3d2b":"code","6afd5494":"code","b3425c02":"code","3d3cb171":"code","c56549fd":"code","1c357a7a":"code","27592a77":"code","1644c2b6":"code","bba4d2e3":"code","5a2e0782":"code","b672edf0":"code","91204734":"code","b0f35039":"code","372be412":"code","df598743":"code","7faef756":"code","fc6fe45d":"code","173ebf47":"code","20c78b75":"code","9a4f71ad":"code","aaf783c0":"code","b934553d":"code","ed69f849":"code","514f08c0":"code","79163c86":"code","2c29a8f5":"code","51df7e49":"code","54297a9a":"code","587cf0fe":"code","c76730e0":"code","ea240856":"code","78d84642":"code","81572351":"code","d63a0a67":"code","15774005":"code","6176f2c5":"code","6c3533bb":"code","7a27204b":"code","16f96ffd":"code","42fb5bed":"code","4cf9121a":"code","faa0e7b8":"code","b2bccee4":"code","37d4416e":"code","b16b43a2":"code","9ae65f7d":"code","da63bd68":"code","b8b6e06a":"code","d0df523b":"code","1e3af377":"code","6ead59d8":"code","733d1312":"code","88b7ee39":"code","abf09c06":"markdown","87467bfe":"markdown","e945ea48":"markdown","3257f0a3":"markdown","90699484":"markdown","c07f52a5":"markdown","c124ffae":"markdown","bb04aede":"markdown","526c229f":"markdown","d11856cc":"markdown","fc24423a":"markdown","9172e25e":"markdown","775da292":"markdown","f3957ec7":"markdown","e3e6b6cd":"markdown","8bff8c97":"markdown","3630813d":"markdown","f538c37f":"markdown","3b816180":"markdown","ddfc4e3c":"markdown","143d4fef":"markdown","172a1f1f":"markdown","de7f220b":"markdown","ab72861b":"markdown","01c405e6":"markdown","88bdbebe":"markdown","670c744a":"markdown","f45dcf6f":"markdown","733f0405":"markdown","825f31e6":"markdown","df3c9f64":"markdown","6a0f709c":"markdown","39bc9710":"markdown","ab427280":"markdown","020f0c94":"markdown","2900b4b0":"markdown","63242e0d":"markdown","bc57ed6d":"markdown","a5da0b69":"markdown","21179e20":"markdown","3a17c4a7":"markdown","70a89117":"markdown","b68899e4":"markdown","e42b4d8d":"markdown","65eeed70":"markdown","e30b46f2":"markdown","93d97f0c":"markdown","4503c72e":"markdown","c4d04f8c":"markdown","e477e376":"markdown","3557593f":"markdown","57b5c57d":"markdown","7e007682":"markdown","b0cb0ca3":"markdown","707ffa5f":"markdown","38828519":"markdown","42890bc2":"markdown","d7da02f7":"markdown","b8b2cbff":"markdown","6a521a4d":"markdown","0881bf84":"markdown","648f2cfc":"markdown","3e1fe744":"markdown","adf767a8":"markdown","be5d30e2":"markdown","201ec5e7":"markdown","07b84eae":"markdown","ba146b95":"markdown","2bc168ee":"markdown","11e28e7a":"markdown","8c244c6f":"markdown","3b1e4f8f":"markdown","24f84357":"markdown","4224ffcc":"markdown","8aed36be":"markdown","bee3c63c":"markdown","e44b9632":"markdown","5cba51b2":"markdown","6058ec5e":"markdown","1ce906f0":"markdown","e7af671d":"markdown","57b3bfdd":"markdown","6463540a":"markdown","ba8dc456":"markdown","a22c3a2e":"markdown","de4e53f9":"markdown","72a4e513":"markdown","6ac3195f":"markdown","0c03965b":"markdown"},"source":{"d33150f0":"import numpy\nimport scipy\nimport matplotlib\nimport sklearn\nimport xgboost\nimport imblearn","f7b20c0d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random as rnd\nfrom scipy import stats\nimport copy \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine-learning\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\nfrom sklearn.feature_selection import RFE, RFECV\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.ensemble import BaggingClassifier\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nflight = pd.read_csv('..\/input\/datavidia2019\/flight.csv')\ndf_test = pd.read_csv('..\/input\/datavidia2019\/test.csv')\nhotel = pd.read_csv('..\/input\/datavidia2019\/hotel.csv')","543c04d7":"# Melihat ukuran dari data\nprint('Ukuran data flight adalah', flight.shape)\nprint('Ukuran data test adalah', df_test.shape)","f9bde7f2":"print('Pada dataset flight terdapat beberapa feature, yaitu: ')\nprint('\\n')\nnum_features = flight.select_dtypes(['float64', 'int64']).columns.tolist()\ncat_features = flight.select_dtypes(['object']).columns.tolist()\nprint('{} numerical features:\\n{} \\n{} categorical features:\\n{}'.format(len(num_features), num_features, len(cat_features), cat_features))\nprint('\\n')\nprint('\\n')\nprint('Sedangkan, pada dataset test terdapat beberapa feature, yaitu: ')\nprint('\\n')\nnum_features = df_test.select_dtypes(['float64', 'int64']).columns.tolist()\ncat_features = df_test.select_dtypes(['object']).columns.tolist()\nprint('{} numerical features:\\n{} \\n{} categorical features:\\n{}'.format(len(num_features), num_features, len(cat_features), cat_features))","70dc9628":"flight.head()","3041ad67":"flight.tail()","59337fa0":"flight.describe()","d8f99bbc":"df_test.describe()","11c6b524":"for col in flight.columns:\n    print('Nilai unik pada feature', col, 'adalah')\n    print(flight[col].value_counts())\n    print('\\n')","23141327":"for col in df_test.columns:\n    print('Nilai unik pada feature', col, 'adalah')\n    print(df_test[col].value_counts())\n    print('\\n')","1c2ec920":"# Membuat feature cross_sell berdasarkan ada atau tidak adanya hotel id\nvar_target = []\nfor i in range(0, len(flight.hotel_id)):\n    val = flight.hotel_id[i]\n    if val != 'None':\n        var_target.append(1)\n    else:\n        var_target.append(0)\n\nflight['cross_sell'] = pd.Series(np.array(var_target))\nflight.drop(['hotel_id'], axis=1, inplace = True)","8acd5815":"used_library = [\n    numpy, pd, scipy, sns, matplotlib, sklearn, xgboost, imblearn\n]\nprint('Library yang digunakan pada kernel ini adalah sebagai berikut.\\n')\nfor library in used_library:\n    print('Library', library.__name__, 'dengan versi', library.__version__)","b5d2fa4d":"# Code untuk mengecek baris data mana yang terdapat missing value pada variabel gender\nmissing_index = []\nfor i in range(0, len(flight.gender)):\n    val = flight.gender[i]\n    if val != 'M' and val != 'F':\n        missing_index.append(i)\n        #print(i)\n    else:\n        continue\n\nmissing_index_no_cs = []\nmissing_index_with_cs = []\nfor i in missing_index:\n    if flight.iloc[i].cross_sell == 0:\n        missing_index_no_cs.append(i)\n        print('data flight index {} yang terdapat missing value tidak terjadi cross selling'.format(i))\n    else:\n        missing_index_with_cs.append(i)\n        print('data flight index {} yang terdapat missing value terjadi cross selling'.format(i))\n\n\n# Menghapus data yang missing value pada gender dan tidak terjadi cross selling\nflight = flight.drop(missing_index_no_cs, axis = 0)\nflight.reset_index(drop=True, inplace=True)\n\n# Memasukan nilai modus pada missing value gender yang terjadi cross selling. \n# Dilakukan secara manual karena hanya terdapat 2 baris yang memiliki kondisi tersebut.\nflight.gender[65919, 106169] = 'M'","c530b845":"def ubah_visited_city(dataset):\n    \"\"\" Docstring\n    Fungsi ini digunakan untuk mengubah feature visited_city menjadi bentuk yang lebih berguna.\n    Pada fungsi ini juga dilakukan pembuatan array yang mengembalikan nilai apakah pelanggan pernah\n    berkunjung di Semarang, Jogja, Surabaya, Aceh, atau Manado.\n    \n    Fungsi ini hanya berguna pada dataset yang digunakan dalam penyisihan Datavidia 2019, menerima \n    input parameter dataset dalam hal ini flight dan dataset test.\n    \"\"\"\n    visit_semarang = []\n    visit_jogja = []\n    visit_sby = []\n    visit_aceh = []\n    visit_manado = []\n    for i in range(0, len(dataset.visited_city)):\n        val = dataset.visited_city[i]\n        val = val.replace('[', '').replace(']', '').replace(\"'\", '').split(',')\n        if 'Semarang' in val:\n            visit_semarang.append(1)\n        else:\n            visit_semarang.append(0)\n        if 'Jogjakarta' in val:\n            visit_jogja.append(1)\n        else:\n            visit_jogja.append(0)\n        if 'Surabaya' in val:\n            visit_sby.append(1)\n        else:\n            visit_sby.append(0)\n        if 'Aceh' in val:\n            visit_aceh.append(1)\n        else:\n            visit_aceh.append(0)\n        if 'Manado' in val:\n            visit_manado.append(1)\n        else:\n            visit_manado.append(0)\n    return visit_semarang, visit_jogja, visit_sby, visit_aceh, visit_manado\n\ndef ubah_log_transaction(dataset):\n    \"\"\" Docstring\n    Fungsi ini digunakan untuk mengubah feature log_transaction menjadi bentuk yang lebih berguna.\n    Pada fungsi ini juga dilakukan pembuatan array yang mengembalikan nilai jumlah, total, dan rata-rata\n    transaksi. Selain itu, fungsi ini juga mengembalikan nilai binary apakah pelanggan telah \n    bertransaksi lebih dari 10 juta, 50 juta, atau bahkan 100 juta.\n    \n    Fungsi ini hanya berguna pada dataset yang digunakan dalam penyisihan Datavidia 2019, menerima \n    input parameter dataset dalam hal ini flight dan dataset test.\n    \"\"\"\n    var_jumlah = []\n    var_total = []\n    var_mean = []\n    var_have_spend_morethan_10m = []\n    var_have_spend_morethan_50m = []\n    var_have_spend_morethan_100m = []\n    for i in range(0, len(dataset.log_transaction)):\n        val = dataset.log_transaction[i]\n        val = val.replace('[', '').replace(']', '').replace(\"'\", '').split(',')\n        tot_transaksi = sum([float(x) for x in val])\n        mean = round(tot_transaksi\/len(val), 2)\n        if tot_transaksi >= 10000000:\n            var_have_spend_morethan_10m.append(1)\n        else:\n            var_have_spend_morethan_10m.append(0)\n        if tot_transaksi >= 50000000:\n            var_have_spend_morethan_50m.append(1)\n        else:\n            var_have_spend_morethan_50m.append(0)\n        if tot_transaksi >= 100000000:\n            var_have_spend_morethan_100m.append(1)\n        else:\n            var_have_spend_morethan_100m.append(0)\n        var_jumlah.append(len(val))\n        var_total.append(tot_transaksi)\n        var_mean.append(mean)\n        \n    return var_jumlah, var_total, var_mean, var_have_spend_morethan_10m, var_have_spend_morethan_50m, var_have_spend_morethan_100m","f4ab3d2b":"# Menggunakan fungsi ubah_visited_city untuk membuat feature baru.\nvisit_semarang, visit_jogja, visit_sby, visit_aceh, visit_manado = ubah_visited_city(flight)\nflight['have_visit_srg'] = pd.Series(np.array(visit_semarang))\nflight['have_visit_jogc'] = pd.Series(np.array(visit_jogja))\nflight['have_visit_sby'] = pd.Series(np.array(visit_sby))\nflight['have_visit_aceh'] = pd.Series(np.array(visit_aceh))\nflight['have_visit_mdc'] = pd.Series(np.array(visit_manado))\n\nvisit_semarang, visit_jogja, visit_sby, visit_aceh, visit_manado = ubah_visited_city(df_test)        \ndf_test['have_visit_srg'] = pd.Series(np.array(visit_semarang))\ndf_test['have_visit_jogc'] = pd.Series(np.array(visit_jogja))\ndf_test['have_visit_sby'] = pd.Series(np.array(visit_sby))\ndf_test['have_visit_aceh'] = pd.Series(np.array(visit_aceh))\ndf_test['have_visit_mdc'] = pd.Series(np.array(visit_manado))\n\n# Menggunakan fungsi ubah_log_transaction untuk membuat feature baru.\njumlah, total, mean, have_spend_morethan_10m, have_spend_morethan_50m, have_spend_morethan_100m = ubah_log_transaction(flight)\nflight['jumlah_transaksi'] = pd.Series(np.array(jumlah))\nflight['mean_transaksi'] = pd.Series(np.array(mean))\nflight['total_transaksi'] = pd.Series(np.array(total))\nflight['have_spend_10m'] = pd.Series(np.array(have_spend_morethan_10m))\nflight['have_spend_50m'] = pd.Series(np.array(have_spend_morethan_50m))\nflight['have_spend_100m'] = pd.Series(np.array(have_spend_morethan_100m))\n\njumlah, total, mean, have_spend_morethan_10m, have_spend_morethan_50m, have_spend_morethan_100m = ubah_log_transaction(df_test)\ndf_test['jumlah_transaksi'] = pd.Series(np.array(jumlah))\ndf_test['mean_transaksi'] = pd.Series(np.array(mean))\ndf_test['total_transaksi'] = pd.Series(np.array(total))\ndf_test['have_spend_10m'] = pd.Series(np.array(have_spend_morethan_10m))\ndf_test['have_spend_50m'] = pd.Series(np.array(have_spend_morethan_50m))\ndf_test['have_spend_100m'] = pd.Series(np.array(have_spend_morethan_100m))","6afd5494":"def categorical_to_numerical(dataset, feature):\n    \"\"\" Docstring\n    Fungsi ini digunakan untuk mengubah feature categorical menjadi numerik. Contohnya seperti feature service class \n    yang memiliki 2 nilai 'ECONOMY' dan 'BUSINESS' yang akan dirubah menjadi nilai 0 dan 1. Fungsi ini dibuat agar code \n    menjadi lebih bersih elegan.\n    \n    Parameter yang dibutuhkan ada 2, yaitu dataset (flight dan data test) dan feature (berupa categorical feature yang\n    ingin diubah menjadi numerik)\n    \"\"\"\n    dictionary = {}\n    for value in dataset[feature].unique():\n        index = np.where(dataset[feature].unique() == value)\n        dictionary[value] = index[0]\n    dataset[feature] = dataset[feature].map(dictionary).astype(int)","b3425c02":"# Menggunakan fungsi categorical_to_numerical untuk mengubah seluruh feature categorical menjadi numerik.\ndataset = [flight, df_test]\ncat_feature = ['gender', 'trip', 'service_class', 'is_tx_promo', 'airlines_name', 'visited_city']\nfor data in dataset:\n    for feature in cat_feature:\n        categorical_to_numerical(data, feature)","3d3cb171":"# Membuat fungsi untuk membantu dalam visualisasi countplot dan annotationnya\ndef count_plot_with_annotation(variabel, data, ax_X, ax_Y):\n    \"\"\" Docstring\n    Fungsi ini digunakan untuk melakukan plotting countplot dan memberikan annotasi berupa nilai count pada masing-masing bar\n    \n    Parameter yang dibutuhkan ada 4, yaitu variabel, data, dan nilai axes x dan y.\n    \"\"\"\n    ax = sns.countplot(variabel,data=data,ax=axes[ax_X, ax_Y])\n    for p in ax.patches:\n        ax.annotate(format(p.get_height()), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                       ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n        \ndef prob_box_plot_with_annotation(x, y, ax_X, ax_Y):\n    \"\"\" Docstring\n    Fungsi ini digunakan untuk melakukan plotting boxplot dan memberikan annotasi berupa nilai probability pada masing-masing bar\n    \n    Parameter yang dibutuhkan ada 4, yaitu x, y, dan nilai axes x dan y. x dan y merupakan input parameter barplot dari seaborn \n    yang dapat dibaca pada dokumentasinya.\n    \"\"\"\n    ax = sns.barplot(x, y, ax = axes[ax_X, ax_Y])\n    ax.set_title('Probabilitas terjadi cross selling berdasarkan {}'.format(x.name))\n    for p in ax.patches:\n        ax.annotate(np.round(p.get_height(),decimals= 3), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                       ha = 'center', va = 'center', xytext = (0, 25), textcoords = 'offset points')","c56549fd":"# Membuat subplot dan mengatur ukuran figur\nfig, axes = plt.subplots(2, 4, figsize=(18, 14))\n\n# Menggunakan fungsi yang telah dibuat untuk membuat countplot dengan mudah\ncount_plot_with_annotation('route', flight, 0, 0) # Countplot pada variabel route\ncount_plot_with_annotation('gender', flight, 0, 1) # Countplot pada variabel gender\ncount_plot_with_annotation('trip', flight, 0, 2) # Countplot pada variabel trip\ncount_plot_with_annotation('airlines_name', flight, 0, 3) # Countplot pada variabel airlines_name\ncount_plot_with_annotation('service_class', flight, 1, 0) # Countplot pada variabel service_class\ncount_plot_with_annotation('is_tx_promo', flight, 1, 1) # Countplot pada variabel is_tx_promo  \ncount_plot_with_annotation('visited_city', flight, 1, 2) # Countplot pada variabel visited_city\ncount_plot_with_annotation('cross_sell', flight, 1, 3) # Countplot pada variabel cross_sell","1c357a7a":"# Membuat subplot dan mengatur ukuran figur\nfig, axes = plt.subplots(2, 3, figsize=(18, 14))\n\n# Menggunakan fungsi yang telah dibuat untuk membuat boxplot dengan mudah\nprob_box_plot_with_annotation(flight['gender'], flight['cross_sell'], 0, 0)\nprob_box_plot_with_annotation(flight['trip'], flight['cross_sell'], 0, 1)\nprob_box_plot_with_annotation(flight['airlines_name'], flight['cross_sell'], 0, 2)\nprob_box_plot_with_annotation(flight['service_class'], flight['cross_sell'], 1, 0)\nprob_box_plot_with_annotation(flight['is_tx_promo'], flight['cross_sell'], 1, 1)\nprob_box_plot_with_annotation(flight['visited_city'], flight['cross_sell'], 1, 2)","27592a77":"sns.factorplot('gender', col = 'trip', data = flight, kind = 'count')","1644c2b6":"sns.factorplot('airlines_name', col = 'gender', data = flight, kind = 'count')","bba4d2e3":"sns.factorplot('gender', col = 'service_class', data = flight, kind = 'count')","5a2e0782":"sns.factorplot('gender', col = 'is_tx_promo', data = flight, kind = 'count')","b672edf0":"sns.factorplot('visited_city', col = 'gender', data = flight, kind = 'count')","91204734":"sns.factorplot('airlines_name', col = 'trip', data = flight, kind = 'count')","b0f35039":"sns.factorplot('trip', col = 'service_class', data = flight, kind = 'count')","372be412":"sns.factorplot('trip', col = 'is_tx_promo', data = flight, kind = 'count')","df598743":"sns.factorplot('visited_city', col = 'trip', data = flight, kind = 'count')","7faef756":"sns.factorplot('airlines_name', col = 'service_class', data = flight, kind = 'count')","fc6fe45d":"sns.factorplot('airlines_name', col = 'is_tx_promo', data = flight, kind = 'count')","173ebf47":"sns.factorplot('airlines_name', col = 'visited_city', data = flight, kind = 'count')","20c78b75":"sns.factorplot('service_class', col = 'is_tx_promo', data = flight, kind = 'count')","9a4f71ad":"sns.factorplot('visited_city', col = 'service_class', data = flight, kind = 'count')","aaf783c0":"sns.factorplot('visited_city', col = 'is_tx_promo', data = flight, kind = 'count')","b934553d":"figbi, axesbi = plt.subplots(1, 2, figsize=(16, 10))\nsns.boxplot(x=\"cross_sell\", y=\"member_duration_days\", data=flight,ax=axesbi[0])\nsns.boxplot(x=\"cross_sell\", y=\"price\", data=flight,ax=axesbi[1])","ed69f849":"ax = sns.countplot(x = 'no_of_seats',data = flight) # Countplot pada variabel no_of_seats\n\nfor p in ax.patches:\n    ax.annotate(format(p.get_height()), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n","514f08c0":"sns.boxplot(x=\"cross_sell\", y=\"no_of_seats\", data=flight)","79163c86":"sns.jointplot(\"price\", \"member_duration_days\", data=flight, kind=\"reg\")","2c29a8f5":"sns.jointplot(\"price\", \"no_of_seats\", data=flight, kind=\"reg\")","51df7e49":"sns.jointplot(\"no_of_seats\", \"member_duration_days\", data=flight, kind=\"reg\")","54297a9a":"# Mengubah bentuk dari member_duration_days kedalam bentuk tahunan (menggunakan standar 1 tahun = 365 hari)\nflight['member_duration_years'] = flight['member_duration_days'] \/\/ 365 \ndf_test['member_duration_years'] = df_test['member_duration_days'] \/\/ 365\n\n# Mengubah bentuk dari member_duration_days kedalam bentuk bulanan (menggunakan standar 1 tahun = 12 bulan)\nflight['member_duration_months'] = (flight['member_duration_days'] % 365) \/\/ 12\ndf_test['member_duration_months'] = (df_test['member_duration_days'] % 365) \/\/ 12\n\n# Menambahkan feature sisa hari, contohnya 1 tahun, 4 bulan, 20 hari. 20 hari tersebut menjadi nilai pada feature ini.\nflight['member_duration_days_left'] = (flight['member_duration_days'] % 365) % 12\ndf_test['member_duration_days_left'] = (df_test['member_duration_days'] % 365) % 12","587cf0fe":"# Mengubah bentuk price kedalam jutaan\nflight['price_in_million'] = flight['price'] \/\/ 1000000 \ndf_test['price_in_million'] = df_test['price'] \/\/ 1000000\n\n# Mendapatkan nilai price per jumlah seat yang dibeli\nflight['price_avg'] = flight['price'] \/ flight['no_of_seats']\ndf_test['price_avg'] = df_test['price'] \/ df_test['no_of_seats']","c76730e0":"# Code dibawah ini membuat feature baru, apakah pelanggan tersebut naik pesawat secara individu atau berkelompok. \n# Dibuktikan dengan nilai no_of_seats\nis_alone = []\n\nfor i in flight[\"no_of_seats\"]:\n    if i > 1:\n        is_alone.append(0)\n    else:\n        is_alone.append(1)\n\nflight[\"is_alone\"] = pd.Series(np.array(is_alone))\n\nis_alone = []\n\nfor i in df_test[\"no_of_seats\"]:\n    if i > 1:\n        is_alone.append(0)\n    else:\n        is_alone.append(1)\n\ndf_test[\"is_alone\"] = pd.Series(np.array(is_alone))","ea240856":"# Fungsi untuk mendapatkan index dari list 2 dimensi\ndef index_2d(myList, v):\n    for i, x in enumerate(myList):\n        if v in x:\n            return i","78d84642":"# Code dibawah ini untuk melabeli pelanggan menggunakan numerik.\n# acc_index = 74791 digunakan untuk data test karena nilai maskimal pada data flight adalah 74791.\n# Adapun terdapat akun yang sama pada data test dan data flight sehingga dapat meningkatkan keakuratan hasil prediksi.\n\n# flight['account'] = pd.factorize(flight.account_id)[0]\n# acc_index = 74791\n# account = []\n# for i in range(0, len(df_test.account_id)):\n#     val1 = df_test.account_id[i]\n#     index = index_2d(flight.account_id, val1)\n#     if index == None:\n#         acc_index += 1\n#         account.append(acc_index)\n#     else:\n#         account.append(flight.account[index])\n\n# df_test['account'] = pd.Series(np.array(account))\n\n\n# Code dibawah ini untuk membuat feature apakah pelanggan tersebut pernah melakukan cross selling sekaligus \n# mendapatkan berapa kali melakukan cross_selling dalam 2018.\n# have_cross = []\n# n_cross = []\n# for i in range(0, len(flight.account_id)):\n#     account = flight.account[i]\n#     if len(flight[flight.account == account][flight.cross_sell == 1]) == 0:\n#         have_cross.append(0)\n#         n_cross.append(0)\n#     else:\n#         have_cross.append(1)\n#         n_cross.append(len(flight[flight.account == account][flight.cross_sell == 1]))\n               \n# flight['have_cross'] = pd.Series(np.array(have_cross))\n# flight['n_cross'] = pd.Series(np.array(n_cross))\n\n# have_cross = []\n# for i in range(0, len(df_test.account_id)):\n#     account = df_test.account[i]\n#     if len(flight[flight.account == account][flight.cross_sell == 1]) == 0:\n#         have_cross.append(0)\n#         n_cross.append(0)\n#     else:\n#         have_cross.append(1)\n#         n_cross.append(len(flight[flight.account == account][flight.cross_sell == 1]))\n                \n# df_test['have_cross'] = pd.Series(np.array(have_cross))\n# df_test['n_cross'] = pd.Series(np.array(n_cross))","81572351":"# Code dibawah ini digunakan untuk menghapus feature yang tidak berguna berdasarkan hasil analisa pada tahap sebelumnya\n\n# flight = flight.drop(['order_id', 'account_id', 'route', 'log_transaction'], axis=1)\n\n# Menyimpan order_id dari data test (digunakan untuk submission)\n# df_test_order_id = df_test.order_id\n# df_test = df_test.drop(['order_id', 'account_id', 'route', 'log_transaction'], axis=1)","d63a0a67":"# Code dibawah ini digunakan untuk mengubah categorical menjadi dummy\/indikator.\n\n# cat_features = ['trip', 'airlines_name', 'visited_city']\n\n# for feature in cat_features:\n#     a = pd.get_dummies(flight[feature], prefix = feature)\n#     frames = [flight, a]\n#     flight = pd.concat(frames, axis = 1)\n    \n#     b = pd.get_dummies(df_test[feature], prefix = feature)\n#     frames = [df_test, b]\n#     df_test = pd.concat(frames, axis = 1)\n    \n# flight.drop(cat_features, axis = 1, inplace=True)\n# df_test.drop(cat_features, axis = 1, inplace=True)\n# flight","15774005":"# flight.corr()[\"cross_sell\"].sort_values(ascending = False)","6176f2c5":"# flight.to_csv('cleaned_flight.csv', index=False)\n# df_test.to_csv('cleaned_df_test.csv', index=False)","6c3533bb":"# flight = pd.read_csv('cleaned_flight.csv')\n# df_test = pd.read_csv('cleaned_df_test.csv')","7a27204b":"# Berikut adalah feature yang dipilih berdasarkan korelasi\n# columns = ['account', 'member_duration_days', 'member_duration_years',\n#        'total_transaksi', 'have_spend_10m', 'have_spend_100m', 'no_of_seats',\n#        'price', 'price_avg', 'have_visit_srg', 'have_cross',\n#        'n_cross', 'have_visit_jogc', 'have_visit_sby', 'have_visit_mdc',\n#        'gender', 'trip_0', 'is_alone', 'trip_1', 'trip_2', 'is_tx_promo',\n#        'airlines_name_1', 'airlines_name_2',\n#        'airlines_name_3', 'airlines_name_4', 'airlines_name_5',\n#        'visited_city_0', 'visited_city_1', 'visited_city_2', 'visited_city_3',\n#        'visited_city_4', 'visited_city_5', 'visited_city_7', 'cross_sell']\n\n# flight = flight[columns]\n# del columns[-1]\n# df_test = df_test[columns]","16f96ffd":"# Membuat variabel train dan variabel target(cross_sell)\n# train = flight.drop(\"cross_sell\", axis=1)\n# target = flight[\"cross_sell\"]\n\n# Melakukan metode oversampling untuk mengatasi imbalanced data\n# smote = SMOTE(sampling_strategy='minority')\n# train, target = smote.fit_sample(train, target)\n\n# Melakukan normalisasi pada data, hal ini dilakukan karena terdapat banyak data yang memiliki nilai tidak seimbang\n# seperti nilai price terhadap member_duration_days, dan juga kami berharap masalah outlier dapat teratasi dengan normalisasi\n# scaler = StandardScaler()\n# train = scaler.fit_transform(train)\n# scaled_df_test = scaler.fit_transform(df_test)\n\n# melakukan pembagian data menjadi data latih (80%) dan data validasi (20%)\n# X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.2, random_state = 123)","42fb5bed":"# print('Ukuran X_train = ', X_train.shape, \n#       '\\nUkuran X_test  = ', X_test.shape, \n#       '\\nUkuran y_train = ', y_train.shape,\n#       '\\nUkuran y_test  = ', y_test.shape\n#      )","4cf9121a":"# pd.DataFrame(X_train)","faa0e7b8":"# Code dibawah ini melakukan pelatihan pada data training dan validasi pada data test hasil split data flight\n# Metode yang digunakan pada tahapan ini adalah xgboost, knn, random forest, decision tree, dan gradient boosting.\n# parameter awal yang digunakan adalah parameter default\n\n# xgboost_model = XGBClassifier()\n# knn_model = KNeighborsClassifier() \n# rfc_model = RandomForestClassifier()\n# dtc_model = DecisionTreeClassifier()\n# gbc_model = GradientBoostingClassifier()\n\n# model_score = []\n# all_model = [xgboost_model, knn_model, rfc_model, dtc_model, gbc_model]\n# for model in all_model:\n#     model.fit(X_train, y_train)\n#     y_pred = model.predict(X_test)\n#     f_score = f1_score(y_test, y_pred, average='macro')\n#     accuracy = accuracy_score(y_test, y_pred)\n#     model_score.append([model.__class__.__name__, f_score, accuracy])\n#     print(\"Metode {}\".format(model.__class__.__name__))\n#     print('Score F1-Macro = ', f_score, ', Accuracy = ', accuracy)\n#     tn, fp, fn, tp = confusion_matrix(y_true=y_test, y_pred=y_pred).ravel()\n#     print('TN:', tn, '\\nFP:',fp, '\\nFN:', fn, '\\nTP:', tp)\n#     print('\\n')","b2bccee4":"# df_score = pd.DataFrame(model_score)\n# df_score.columns = ['metode', 'f1-score', 'akurasi']\n# df_score","37d4416e":"# param_grid = {'learning_rate': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n#               'n_estimators': [25, 50, 75, 100, 125, 150, 200],\n#               'max_depth' : [25, 50, 75, 100, 125, 150, 200]\n#              }\n# grid = RandomizedSearchCV(XGBClassifier(), param_grid,\n#                           cv=5, verbose=1, scoring='f1_macro', n_jobs = -1)\n\n# grid.fit(X_train, y_train)\n# grid.best_params_","b16b43a2":"# param_grid = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n# grid = RandomizedSearchCV(KNeighborsClassifier(), param_grid,\n#                           cv=5, verbose=1, scoring='f1_macro', n_jobs = -1)\n\n# grid.fit(X_train, y_train)\n# grid.best_params_","9ae65f7d":"# param_grid = {\n#               'n_estimators': [25, 50, 75, 100, 125, 150, 200],\n#               'max_depth' : [5, 10, 25, 50, 75, 100, 125, 150, 200]\n#              }\n# grid = RandomizedSearchCV(RandomForestClassifier(), param_grid,\n#                           cv=5, verbose=1, scoring='f1_macro', n_jobs = -1)\n\n# grid.fit(X_train, y_train)\n# grid.best_params_","da63bd68":"# param_grid = {\n#               'max_depth' : [5, 10, 25, 50, 75, 100, 125, 150, 200]\n#              }\n# grid = RandomizedSearchCV(DecisionTreeClassifier(), param_grid,\n#                           cv=5, verbose=1, scoring='f1_macro', n_jobs = -1)\n\n# grid.fit(X_train, y_train)\n# grid.best_params_","b8b6e06a":"# param_grid = {'learning_rate': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n#               'n_estimators': [25, 50, 75, 100, 125, 150, 200],\n#               'max_depth' : [25, 50, 75, 100, 125, 150, 200]\n#              }\n# grid = RandomizedSearchCV(GradientBoostingClassifier(), param_grid,\n#                           cv=5, verbose=1, scoring='f1_macro', n_jobs = -1)\n\n# grid.fit(X_train, y_train)\n# grid.best_params_","d0df523b":"# Code dibawah ini melakukan pelatihan pada data training dan validasi pada data test hasil split data flight\n# Metode yang digunakan pada tahapan ini adalah xgboost, knn, random forest, decision tree, dan gradient boosting.\n# parameter yang digunakan adalah parameter hasil optimasi tahap sebelumnya.\n\n# from sklearn.metrics import f1_score\n# xgboost_model = XGBClassifier(learning_rate= 0.2, n_estimators= 150, max_depth=50, n_jobs= -1)\n# knn_model = KNeighborsClassifier(n_neighbors=1) \n# rfc_model = RandomForestClassifier(random_state=0, n_estimators = 100, max_depth=50)\n# dtc_model = DecisionTreeClassifier(max_depth=50)\n# gbc_model = GradientBoostingClassifier(n_estimators=120, learning_rate=0.70, max_depth=50)\n\n# model_score = []\n# all_model = [xgboost_model, knn_model, rfc_model, dtc_model, gbc_model]\n# for model in all_model:\n#     print(\"Metode {}\".format(model.__class__.__name__))\n#     model.fit(X_train, y_train)\n#     y_pred = model.predict(X_test)\n#     f_score = f1_score(y_test, y_pred, average='macro')\n#     accuracy = accuracy_score(y_test, y_pred)\n#     model_score.append([model.__class__.__name__, f_score, accuracy]) \n#     print('Score F1-Macro = ', f_score, ', Accuracy = ', accuracy)\n#     tn, fp, fn, tp = confusion_matrix(y_true=y_test, y_pred=y_pred).ravel()\n#     print('TN:', tn, '\\nFP:',fp, '\\nFN:', fn, '\\nTP:', tp)\n#     print('\\n')","1e3af377":"# df_score = pd.DataFrame(model_score)\n# df_score.columns = ['metode', 'f1-score', 'akurasi']\n# df_score","6ead59d8":"# clf1 = RandomForestClassifier(random_state=0, n_estimators = 100, max_depth=50)\n# clf2 = XGBClassifier(learning_rate= 0.2, n_estimators= 150, max_depth=50, n_jobs= -1)\n# clf3 = KNeighborsClassifier(n_neighbors=1) \n# clf4 = DecisionTreeClassifier(max_depth=50)\n# clf5 = GradientBoostingClassifier(n_estimators=120, learning_rate=0.72, max_depth=50)\n# eclf = VotingClassifier(estimators=[('rf', clf1), ('xgboost', clf2), ('knn', clf3), \n#                                     ('dt', clf3), ('gbc', clf3)], \n#                         voting='hard')\n# eclf = eclf.fit(X_train, y_train)\n# y_pred = eclf.predict(X_test)\n# print(f1_score(y_test, y_pred, average='macro'), accuracy_score(y_test, y_pred))\n# tn, fp, fn, tp = confusion_matrix(y_true=y_test, y_pred=y_pred).ravel()\n# print('TN:', tn, '\\n FP:',fp, '\\n FN:', fn, '\\n TP:', tp)","733d1312":"# Code dibawah ini digunakan untuk melihat row mana saja terjadi kesalahan prediksi\n# Setelah dilakuakn analisa ini, apabila diperlukan pengerjaan dapat \n# kembali dilakukan pada tahap-tahap sebelumnya (praproses, eda, dan feature engineering) \n\n# for idx, prediction, label in zip(enumerate(X_test), y_pred, y_test):\n#     if prediction != label:\n#         print(\"Sample\", idx, ', has been classified as', prediction, 'and should be', label) ","88b7ee39":"# Berikut adalah code untuk membuat file submission\n# Tidak dijalankan pada kernel kaggle karena telah dilakukan pada saat code masih di local\n\n# Y_pred = knn_model.predict(cleaned_df_test)\n# prediction = []\n# for pred in Y_pred:\n#     if pred == 0:\n#         prediction.append(\"no\")\n#     elif pred == 1:\n#         prediction.append(\"yes\")\n# prediction = np.array(prediction)\n# submission = pd.DataFrame({\n#         \"order_id\": df_test_order_id,\n#         \"is_cross_sell\": prediction\n#     })\n# submission.to_csv(\"submission.csv\", index=False)","abf09c06":"**Feature no_of_seats**","87467bfe":"**Import dataset yang telah bersih**\n***","e945ea48":"**price vs member_duration_days**","3257f0a3":"**Gender vs is_tx_promo**\n<a id=\"3.1.3.4\"><\/a>","90699484":"### 4.1.3 Feature Baru dari no_of_seats\n<a id=\"4.1.3\"><\/a>\n***","c07f52a5":"### 5.3.1 Parameter Tunning pada XGBoost\n<a id=\"5.3.1\"><\/a>\n***","c124ffae":"### 5.3.4 Parameter Tunning pada Decision Tree\n<a id=\"5.3.4\"><\/a>\n***","bb04aede":"### 3.1.1 Countplot pada semua feature categorical\n<a id=\"3.1.1\"><\/a>\n***","526c229f":"### 3.2.2 Scatterplot antar feature\n<a id=\"3.2.2\"><\/a>\n***","d11856cc":"## 2.2 Mengubah Bentuk visited_city dan log_transaction\n<a id=\"2.2\"><\/a>\n***\nPada tahap ini, dilakukan perubahan bentuk dari visited_city dan log_transaction yang awalnya merupakan string. Tahap ini dilakukan agar kedua feature tersebut menjadi lebih bermanfaat. Code dibawah ini juga terdapat beberapa feature engineering, yang dianggap perlu untuk dimasukan pada sub-bab ini.","fc24423a":"# Referensi\n<a id=\"ref\"><\/a>\n***\n[1. Medium, How to handle imbalanced data](https:\/\/medium.com\/james-blogs\/handling-imbalanced-data-in-classification-problems-7de598c1059f) <br>\n[2. Dokumentasi Scikit-learn](https:\/\/scikit-learn.org\/)<br>\n[3. Dokumentasi Seaborn](https:\/\/seaborn.pydata.org\/)<br>\n[4. Dokumentasi Matplotlib](https:\/\/matplotlib.org\/)<br>\n[5. Dokumentasi XGBoost](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#)<br>\n[6. ANALISIS PREFERENSI KONSUMEN DALAM MENGGUNAKAN APLIKASI PENYEDIA TIKET PESAWAT & BOOKING HOTEL ONLINE oleh Agustin Wibisono dan Indrawati, Ph.D]()","9172e25e":"**Hasil analisa dari grafik diatas adalah sebagai berikut**\n***\nKeterangan numerik (0, 1, 2, ...) dapat dilihat pada [bagian ini](#keterangan).\n1. Promo tidak mempengaruhi penentuan tipe dari trip, terlihat dari grafik diatas bahwa frekuensinya kurang lebih sama antar tipe trip.","775da292":"**Gender vs Service_class**\n<a id=\"3.1.3.3\"><\/a>","f3957ec7":"### 3.1.2 Probabilitas terjadi cross sell berdasarkan feature yang ada\n<a id=\"3.1.2\"><\/a>\n***","e3e6b6cd":"## 2.1 Mengatasi Missing Value\n<a id=\"missing_value\"><\/a>\n***\nPada sub-bab 1.1.4, tim Red Line telah mengidentifikasi bahwa terdapat missing value pada feature gender dan tidak terdapat missing value pada feature lainnya. C\n\node dibawah ini digunakan untuk menghapus data yang missing value apabila tidak terjadi cross_selling dan jika data yang mengandung missing value terjadi cross_selling maka akan dilakukan pengisian berdasarkan modus dari gender yang terjadi cross_selling, dalam hal ini Male. Metode mengatasi missing value ini dilakukan karena dataset yang dimiliki memiliki jumlah class yang tidak seimbang (berdasarkan hipotesis awal bahwa pembelian tiket pesawat sekaligus dengan pemesanan hotel akan jauh lebih sedikit jumlahnya dibandingkan dengan pembelian tiket pesawat saja maupun hotel saja). Hipotesis tersebut akan dibuktikan pada sub-bab 3.1.1.","8bff8c97":"**Korelasi antar feature**\n***\nTerlihat bahwa feature yang dibuat pada saat feature engineering memiliki korelasi yang tinggi pada feature target","3630813d":"### 1.2.1 Versi library yang digunakan\n<a id=\"1.2.1\"><\/a>\n***\nBerikut adalah versi dari library-library yang digunakan.","f538c37f":"**Hasil analisa dari grafik diatas adalah sebagai berikut**\n***\nKeterangan numerik (0, 1, 2, ...) dapat dilihat pada [bagian ini](#keterangan).\n1. Kota yang dikunjungi antara pria maupun perempuan kurang lebih memiliki frekuensi yang sama.","3b816180":"**Gender vs visited_city**\n<a id=\"3.1.3.5\"><\/a>","ddfc4e3c":"**Hasil analisa dari grafik diatas adalah sebagai berikut**\n***\nKeterangan numerik (0, 1, 2, ...) dapat dilihat pada [bagian ini](#keterangan).\n1. Probabilitas terjadi cross selling pada masing-masing value tergolong sangat kecil. Dugaan kami hal ini dikarenakan kecilnya korelasi feature-feature diatas terhadap feature target, cross_sell.\n2. Pada feature airlines_name, 6 dan 7 memiliki nilai probabilitas 0, sehingga kemungkinan tidak akan digunakan dalam model prediksi.","143d4fef":"Parameter optimal dari masing-masing metode adalah sebagai berikut <br>\n\nXGBClassifier(learning_rate= 0.2, n_estimators= 150, max_depth=50, n_jobs= -1) <br>\nKNeighborsClassifier(n_neighbors=2) <br>\nRandomForestClassifier(random_state=0, n_estimators = 100, max_depth=50) <br>\nDecisionTreeClassifier(max_depth=50) <br>\nGradientBoostingClassifier(n_estimators=125, learning_rate=0.7, max_depth=50) <br>\n\n\nPada saat mengupload notebook pada kernel kaggle, tahapan parameter tuning tidak dijalankan karena terdapat masalah pada kernel kaggle yang menyebabkan lag dan sebagainya sehingga kami memutuskan untuk tidak memasukan output hasil running parameter tuning pada kaggle kernel. Namun hasil parameter diatas benar adanya merupakan output dari code diatas dengan metode pencarian parameter RandomSearchCV.","172a1f1f":"**Export dataset yang telah dibersihkan**\n***\nHal ini dilakukan untuk mempermudah pekerjaan, tanpa perlu menjalankan baris code yang telah dilakukan berkali-kali.","de7f220b":"Dataframe diatas merupakan data yang telah diproses namun belum dilakukan normalisasi.","ab72861b":"**Keterangan dalam membaca grafik**\n<a id=\"keterangan\"><\/a>\n***\n* **Feature gender** <br>\n0 = Male <br>\n1 = Female <br>\n* **Feature trip** <br>\n0 = Trip <br>\n1 = Round <br>\n2 = Roundtrip <br>\n* **Feature airlines_name** <br>\n0 = '33199710eb822fbcfd0dc793f4788d30' <br>\n1 = '0a102015e48c1f68e121acc99fca9a05' <br>\n2 = '6c483c0812c96f8ec43bb0ff76eaf716' <br>\n3 = 'ad5bef60d81ea077018f4d50b813153a' <br>\n4 = 'e35de6a36d385711a660c72c0286154a' <br>\n5 = '74c5549aa99d55280a896ea50068a211' <br>\n6 = '9855a1d3de1c46526dde37c5d6fb758c' <br>\n7 = '6872b49542519aea7ae146e23fab5c08' <br>\n* **Feature service_class** <br>\n0 = Economy <br>\n1 = Business <br>\n* **Feature is_tx_promo** <br>\n0 = No (tidak promo) <br>\n1 = Yes (promo) <br>\n* **Feature visited_city** <br>\n0 = [Semarang, Jakarta, Medan, Bali] <br>\n1 = [Jakarta, Medan, Bali] <br>\n2 = [Jogjakarta, Bali, Jakarta, Medan] <br>\n3 = [Jakarta, Bali, Medan, Jogjakarta, Semarang] <br>\n4 = [Bali, Jakarta, Medan] <br>\n5 = [Medan, Bali, Jakarta] <br>\n6 = [Manado, Medan, Bali, Jakarta] <br>\n7 = [Surabaya, Medan, Bali, Jakarta, Aceh] <br>\n* **Feature cross_sell** <br>\n0 = Tidak terjadi cross sell <br>\n1 = Terjadi cross sell\n\n\nUntuk mempermudah membaca kernel ini anda dapat kembali pada bagian terakhir grafik anda dengan cara klik link dibawah ini (agar anda tidak perlu scroll). \n1. [3.1.1 Countplot pada semua feature categorical](#3.1.1)\n2. [3.1.2 Probabilitas terjadi cross sell berdasarkan feature yang ada](#3.1.2)\n3. [3.1.3 Factorplot antar feature](#3.1.3)<br>\n    3.1 [Gender vs Trip](#3.1.3.1)<br>\n    3.2 [Gender vs airlines_name](#3.1.3.2)<br>\n    3.3 [Gender vs service_class](#3.1.3.3)<br>\n    3.4 [Gender vs is_tx_promo](#3.1.3.4)<br>\n    3.5 [Gender vs visited_city](#3.1.3.5)<br>\n    3.6 [Trip vs airlines_name](#3.1.3.6)<br>\n    3.7 [Trip vs service_class](#3.1.3.7)<br>\n    3.8 [Trip vs is_tx_promo](#3.1.3.8)<br>\n    3.9 [Trip vs visited_city](#3.1.3.9)<br>\n    3.10 [Airlines_name vs Trip](#3.1.3.10)<br>\n    3.11 [Airlines_name vs is_tx_promo](#3.1.3.11)<br>\n    3.12 [Airlines_name vs visited_city](#3.1.3.12)<br>\n    3.13 [service_class vs is_tx_promo](#3.1.3.13)<br>\n    3.14 [service_class vs visited_city](#3.1.3.14)<br>\n    3.15 [is_tx_promo vs visited_city](#3.1.3.15)<br>","01c405e6":"**Gender vs Trip**\n<a id=\"3.1.3.1\"><\/a>","88bdbebe":"**Service_class vs visited_city**\n<a id=\"3.1.3.14\"><\/a>","670c744a":"### 3.1.4 Feature route\n<a id=\"3.1.4\"><\/a>\n***\nTerlihat pada grafik sub-bab 3.1.1 bahwa route hanya memiliki 1 value yaitu CGK - DPS, sehingga tidak akan digunakan pada tahapan-tahapan selanjutnya","f45dcf6f":"## 4.3 Mengubah Variabel Kategorikal Menjadi Dummy\n<a id=\"4.3\"><\/a>\n***","733f0405":"## 1.2 Deskripsi Mengenai Library Yang Digunakan\n<a id=\"1.2\"><\/a>\n***","825f31e6":"## 5.3 Parameter Tunning\n<a id=\"5.3\"><\/a>\n***\nRandomizedSearchCV dipilih untuk mengoptimasi parameter karena lebih cepat dibandingkan dengan metode GridSearchCV.","df3c9f64":"### 1.1.2 Feature pada Dataset\n<a id=\"dataset_feature\"><\/a>\n***\nFeature yang terdapat pada dataset 'flight' dan 'test' kurang lebih sama, dimana terdapat 3 numerical feature dan 11 categorical feature.****","6a0f709c":"### 5.3.3 Parameter Tunning pada Random Forest\n<a id=\"5.3.3\"><\/a>\n***","39bc9710":"### 3.1.3 Factorplot antar feature\n<a id=\"3.1.3\"><\/a>\n***","ab427280":"### 1.1.1 Ukuran Dataset\n<a id=\"ukuran_dataset\"><\/a>\n***\nDiketahui bahwa pada dataset 'flight' terdapat 14 feature dan 117.946 baris data, sedangkat setiap kelompok harus memprediksi 10.000 baris data yang tardapat pada dataset 'test'","020f0c94":"**Trip vs is_tx_promo**\n<a id=\"3.1.3.8\"><\/a>","2900b4b0":"**Hasil analisa dari grafik diatas adalah sebagai berikut**\n***\nKeterangan numerik (0, 1, 2, ...) dapat dilihat pada [bagian ini](#keterangan).\n1. Maskapai penerbangan '33199710eb822fbcfd0dc793f4788d30' lebih dipilih ketika tidak menggunakan promo, tetapi ketika menggunakan promo, pelanggan cenderung lebih menggunakan maskapai '6c483c0812c96f8ec43bb0ff76eaf716'","63242e0d":"**Hasil analisa dari grafik diatas adalah sebagai berikut**\n***\nKeterangan numerik (0, 1, 2, ...) dapat dilihat pada [bagian ini](#keterangan).\n1. Dibandingkan pria, perempuan memiliki transaksi lebih banyak menggunakan promo.","bc57ed6d":"**is_tx_promo vs visited_city**\n<a id=\"3.1.3.15\"><\/a>","a5da0b69":"**Airlines_name vs is_tx_promo**\n<a id=\"3.1.3.11\"><\/a>","21179e20":"## 5.4 Voting Classifier\n<a id=\"5.4\"><\/a>\n***\nPada metode ini, digunakan semua metode yang telah dilakukan sebelumnya dengan menggunakan parameter hasil optimasi.","3a17c4a7":"**Trip vs Service_class**\n<a id=\"3.1.3.7\"><\/a>","70a89117":"### 1.2.2 Deskripsi singkat dari library yang digunakan\n<a id=\"1.2.2\"><\/a>\n***\n\n+ **Numpy** <br>\nDigunakan untuk operasi vektor dan matriks. Pada kernel ini, Numpy digunakan hampir pada semua tahapan.\n+ **Pandas** <br>\nDigunakan untuk import\/export dataset, membangun DataFrame\/Series, dll\n+ **Scipy** <br>\nDigunakan untuk menangani operasi matematika terutama statistika.\n+ **Seaborn** <br>\nDigunakan untuk membuat plot pada eksplorasi dan analisa data.\n+ **Matplotlib** <br>\nDigunakan untuk membuat plot pada eksplorasi dan analisa data.\n+ **Sklearn** <br>\nDigunakan untuk membuat model Decision Tree, Gradient Booster, k-NN, Random Forest, Voting Classifier. Juga digunakan untuk mempartisi dan menormalisasi data.\n+ **XGBoost** <br>\nDigunakan untuk membuat model Extreme Gradient Boosting.\n+ **Imblearn** <br>\nDigunakan untuk melakukan sampling data (mengatasi class yang unbalanced), dalam hal ini menggunakan metode SMOTE pada library tersebut.","b68899e4":"# 1. Pendahuluan\n<a id=\"pendahuluan\"><\/a>\n***\nDataset yang diberikan kepada peserta terdiri atas dataset 'flight' yang mana merupakan sample dari order tiket pesawat, dataset 'hotel' yang mana berisi tentang data hotel yang berhubungan dengan pembelian melalui platform Tiket.com, sedangkan dataset 'test' berisi data yang harus diprediksi apakah terjadi cross_sell atau tidak. Adapun pengertian cross_sell dalam studi kasus ini adalah kondisi dimana pelanggan memesan hotel bersamaan dengan transaksi pembelian tiket.\n\nPada kondisi ini, sesungguhnya dataset 'hotel' tidak dapat memberikan dampak apapun terhadap model untuk memprediksi cross_sell karena prediksi cross_sell dilakukan untuk memprediksi apakah pelanggan juga memesan hotel saat memesan tiket pesawat. Berikut adalah deskripsi dari dataset secara singkat","e42b4d8d":"Berikut merupakan tampilan dari X_train yang merupakan dataset yang telah dibersihkan, di normalisasi, dan disampling menggunakan metode SMOTE.","65eeed70":"Setelah dilakukan tahapan optimasi parameter, maka metode XGBoost adalah yang terbaik.","e30b46f2":"**Hasil analisa dari grafik diatas adalah sebagai berikut**\n***\nKeterangan numerik (0, 1, 2, ...) dapat dilihat pada [bagian ini](#keterangan).\n1. Kota yang telah dikungjungi oleh pelanggan tidak dipengaruhi oleh ada atau tidak adanya promo.","93d97f0c":"## 3.2 Eksplorasi Feature Numerik\n<a id=\"numeric\"><\/a>\n***\n","4503c72e":"# 3. EDA\n<a id=\"eda\"><\/a>\n***\nPada tahap ini, kelompok Red Line berusaha menggali informasi sebanyak mungkin dengan melakukan visualisasi dalam bentuk tulisan, grafik, dll. Eksplorasi yang dilakukan akan berdasarkan pada tipe data (kategorikal dan numerikal).","c4d04f8c":"### 5.3.5 Parameter Tunning pada Gradient Boosting\n<a id=\"5.3.5\"><\/a>\n***","e477e376":"## 1.1 Deskripsi Dataset\n<a id=\"1.1\"><\/a>\n***","3557593f":"**member_duration_days vs no_of_seats**","57b5c57d":"Berdasarkan hasil diatas, metode random forest merupakan metode terbaik dengan nilai f1-score dan akurasi sebesar 0.989.","7e007682":"**Trip vs visited_city**\n<a id=\"3.1.3.9\"><\/a>","b0cb0ca3":"## 3.1 Eksplorasi Feature Kategorikal\n<a id=\"category\"><\/a>\n***\nBerdasarkan sub-bab 1.1.2, categorical feature pada dataset flight adalah account_id, order_id, gender, trip, service_class, is_tx_promo, airlines_name, route, hotel_id, visited_city, log_transaction.\n\nNamun setelah diperhatikan, tidak semua dari feature tersebut akan dilakukan eksplorasi. Feature  yang akan dilakukan eksplorasi adalah gender, trip, service_class, is_tx_promo, airlines_name, route, dan visited_city","707ffa5f":"### 4.1.1 Feature Baru dari member_duration_days\n<a id=\"4.1.1\"><\/a>\n***","38828519":"**Trip vs Airlines_name**\n<a id=\"3.1.3.6\"><\/a>","42890bc2":"# Import Library Yang Dibutuhkan\n<a id=\"library\"><\/a>\n***\nPada tahap ini, library-library yang dibutuhkan dalam pengerjaan diimport. Selain itu, kami juga memasukan data yang dibutuhkan untuk analisa dan pelatihan.","d7da02f7":"### 1.1.3 Menampilkan Sebagian Data dari Dataset\n<a id=\"head_tail_dataset\"><\/a>\n***\n","b8b2cbff":"## 2.3 Mengubah Categorical Feature Menjadi Numerik\n<a id=\"2.3\"><\/a>\n***\nPada tahap ini, dilakukan perubahan bentuk categorical menjadi numerik. Contohnya pada feature gender terdapat 2 nilai unik, yaitu male dan female yang dapat dirubah kedalam bentuk numerik menjadi 0 dan 1. Hal ini dilakukan karena beberapa algoritma tidak dapat melakukan prediksi apabila masih dalam bentuk string.","6a521a4d":"## 4.4 Pemilihan Feature\n<a id=\"4.4\"><\/a>\n***\nPemilihan feature berikut didasarkan pada korelasi dengan target","0881bf84":"**Airlines_name vs visited_city**\n<a id=\"3.1.3.12\"><\/a>","648f2cfc":"# 2. Praproses Data\n<a id=\"praproses\"><\/a>\n***\nPada tahap ini, kelompok Red Line membersihkan dan mempersiapkan data yang masih mentah untuk dapat dimanfaatkan dalam pembangunan model prediksi. Tahapan-tahapan yang akan dilakukan dalam hal ini adalah mengatasi data yang hilang, mengatasi outlier dan lain-lain.","3e1fe744":"## 3.3 Kesimpulan Hasil Eksplorasi\n<a id=\"3.3\"><\/a>\n***\nKesimpulan penting yang dapat ditarik dari hasil eksplorasi diatas adalah\n1. Terdapat feature yang tidak terlalu berguna untuk dijadikan input model prediksi, yaitu feature route dan service_class.\n2. Terdapat sangat banyak outlier pada dataset flight maupun testing. Solusi yang dapat dilakukan adalah dengan melakukan normalisasi. Kelompok kami tidak melakukan penghapusan outlier tersebut karena pada data test juga terdapat banyak outlier.\n3. Terdapat beberapa feature baru yang dapat dibuat pada tahap selanjutnya (feature engineering), yaitu <br>\n    * Account pelanggan, hal ini dikarenakan terdapat banyak order yang dilakukan oleh 1 pelanggan dan account pelanggan tidaklah unique pada setiap baris.\n    * Feature yang menunjukan apakah pelanggan pernah melakukan cross_selling.\n    * Estimasi jumlah cross selling yang dilakukan oleh pelanggan selama 2018.\n    * Harga tiket\/seat\n    * Harga tiket dalam jutaan","adf767a8":"# Penyisihan Datavidia 2019\n***\n**Red Line (Yusnardo Tendio & Helena Angelita Depari) | 27\/12\/2019**\n\nPada kernel ini akan dibahas secara detail analisa, praproses, feature engineer, modeling, dan evaluasi (validasi) yang dilakukan.\n\n\n**Mohon maaf kepada dewan juri dan panitia, tahapan pada modelling, parameter tunning, dan validasi kami comment karena terdapat permasalahan pada saat melakukan commit di kaggle kernel. Setelah kami membaca pada forum tanya jawab kernel, ternyata banyak user yang juga mengalami hal tersebut. Sekali lagi kami mohon maaf**","be5d30e2":"**price vs no_of_seats**","201ec5e7":"## Daftar Isi\n***\n\n* [Import Library Yang Dibutuhkan](#library)\n* [1. Pendahuluan](#pendahuluan)\n    * [1.1 Deskripsi Dataset](#1.1)\n        + [1.1.1 Ukuran dari Masing-masing Dataset](#ukuran_dataset)\n        + [1.1.2 Feature pada Dataset](#dataset_feature)\n        + [1.1.3 Menampilkan Sebagian Data dari Dataset](#head_tail_dataset)\n        + [1.1.4 Menampilkan Nilai Unik Pada Masing-Masing Feature](#unique_value)\n        + [1.1.5 Membuat Feature cross_sell](#cross_sell)\n    * [1.2 Deskripsi Mengenai Library Yang Digunakan](#1.2)  \n        + [1.2.1 Versi dari library yang digunakan](#1.2.1)\n        + [1.2.2 Deskripsi singkat dari library yang digunakan](#1.2.2)\n* [2. Praproses Data](#praproses)\n    * [2.1 Mengatasi Missing Value](#missing_value)\n    * [2.2 Mengubah Bentuk visited_city dan log_transaction](#2.2)\n    * [2.3 Mengubah Categorical Feature Menjadi Numerik](#2.3)\n* [3. EDA](#eda)\n    * [3.1 Eksplorasi Feature Kategorikal](#category)\n        + [3.1.1 Countplot pada semua feature kategorikal](#3.1.1)\n        + [3.1.2 Probabilitas terjadi cross selling berdasarkan feature yang ada](#3.1.2)\n        + [3.1.3 Factorplot antar feature](#3.1.3)\n        + [3.1.4 Feature route](#3.1.4)\n    * [3.2 Eksplorasi Feature Numerik](#numeric)\n        + [3.2.1 Distribusi antar feature](#3.2.1)\n        + [3.2.2 Scatterplot antar feature](#3.2.2)\n    * [3.3 Kesimpulan Hasil Eksplorasi](#3.3)\n* [4. Feature Engineering](#4)\n    * [4.1 Membuat Feature Baru](#4.1)\n        + [4.1.1 Feature baru dari member_duration_days](#4.1.1)\n        + [4.1.2 Feature baru dari price](#4.1.2)\n        + [4.1.3 Feature baru dari no_of_seats](#4.1.3)\n        + [4.1.4 Feature baru dari account_id dan cross_sell](#4.1.4)\n    * [4.2 Menghapus Feature Yang Tidak Berguna](#4.2)\n    * [4.3 Mengubah Variabel Kategorikal Menjadi Dummy](#4.3)\n* [5. Modelling dan Validasi](#5)\n    * [5.1 Sampling, Normalisasi dan Split Data](#5.1)\n    * [5.2 Modeling dan Validasi Dengan Berbagai Metode](#5.2)\n    * [5.3 Parameter Tunning](#5.3)\n        + [5.3.1 Parameter tuning pada XGBoost](5.3.1)\n        + [5.3.2 Parameter tuning pada k-NN](5.3.2)\n        + [5.3.3 Parameter tuning pada Random Forest](5.3.3)\n        + [5.3.4 Parameter tuning pada Decision Tree](5.3.4)\n        + [5.3.5 Parameter tuning pada Gradient Boosting](5.3.5)\n        + [5.3.6 Modelling & Validasi menggunakan parameter optimal](5.3.6)\n    * [5.4 Voting Classifier](#5.4)\n* [References](#ref)","07b84eae":"### 5.3.6 Modelling & Validasi Menggunakan Parameter Optimal\n<a id=\"5.3.6\"><\/a>\n***","ba146b95":"**Hasil analisa dari grafik diatas adalah sebagai berikut**\n***\nKeterangan numerik (0, 1, 2, ...) dapat dilihat pada [bagian ini](#keterangan).\n1. Feature 'route' hanya memiliki satu value yaitu CGK-DPS, sehingga tidak akan berguna dalam model prediksi.\n2. Feature 'gender' yang telah dibersihkan dari missing value memiliki jumlah yang kurang lebih seimbang.\n3. Feature 'service_class' sangat tidak seimbang dimana tiket ekonomi jauh lebih banyak dibandingkan tiket business, sehingga feature ini tidak digunakan dalam model prediksi.\n4. Feature cross_sell tidak balanced, karena feature ini merupakan feature target maka akan dilakukan metode sampling dengan library imblearn dan metode SMOTE. \n","2bc168ee":"**Hasil analisa dari grafik diatas adalah sebagai berikut**\n***\nKeterangan numerik (0, 1, 2, ...) dapat dilihat pada [bagian ini](#keterangan).\n1. Data yang dimiliki terbanyak adalah order\/pembelian oleh pria. \n2. Pria memiliki pembelian terbanyak pada tiket 'trip' dan 'round', sedangkan pada tiket 'roundtrip' pembelian terbanyak dilakukan oleh perempuan.","11e28e7a":"**Hasil analisa dari grafik diatas adalah sebagai berikut**\n***\nKeterangan numerik (0, 1, 2, ...) dapat dilihat pada [bagian ini](#keterangan).\n1. Uniknya, keputusan pemilihan maskapai penerbangan antara pria dan perempuan sangat seimbang. Hipotesis kami pada kasus ini adalah jenis kelamin tidak terlalu berpengaruh terhadap penentuan maskapai penerbangan, penentuan maskapai penerbangan lebih condong kepada tingkat ekonomi pelanggan, pengalaman kenyamanan yang dirasakan pada masing-masing maskapai, dan tujuan penerbangan.","8c244c6f":"# 4. Feature Engineering\n<a id=\"4\"><\/a>\n***\nBeberapa langkah yang dilakukan pada tahap ini adalah berusaha mempersiapkan feature untuk dijadikan input model prediksi. Hal-hal yang dilakukan pada tahap ini, yaitu membuat feature baru, memilih feature yang akan digunakan, dll","3b1e4f8f":"### 5.3.2 Parameter Tunning pada k-NN\n<a id=\"5.3.2\"><\/a>\n***","24f84357":"## 5.5 Pemilihan Model\n<a id=\"5.5\"><\/a>\n***\nPemilihan model ini dilakukan berdasarkan perbandingan hasil score validasi pada sub-bab 5.2, 5.3 dan 5.4. Model yang terbaik adalah menggunakan k-NN dengan nilai k=1.","4224ffcc":"### 1.1.4 Menampilkan Nilai Unik Pada Masing-Masing Feature\n<a id=\"unique_value\"><\/a>\n***\nTerdapat hal menarik yang didapatkan pada tahap ini, yaitu\n1. Pada feature 'account_id' hanya terdapat 74.796 akun unik, sehingga terdapat akun yang melakukan pembelian berkali-kali pada platform Tiket.com di tahun 2018. Hal ini juga dapat dimanfaatkan untuk meningkatkan score model yang dihasilkan.\n2. Data yang dimiliki tidak terdapat nilai yang duplikat, hal ini dibuktikan dengan jumlah nilai unik pada feature order_id\n3. Terdapat 24 missing value pada feature gender\n4. Data service_class business sangatlah sedikit, sehingga kemungkinan tidak akan membantu dalam pembuatan model prediksi yang akurat.\n5. Feature route hanya terdapat 1 value yaitu CGK - DPS sehingga tidak berguna dalam pembuatan model prediksi.\n6. Tidak terdapat missing value pada data test","8aed36be":"### 1.1.5 Membuat Feature cross_sell\n<a id=\"cross_sell\"><\/a>\n***\nPada tahapan ini, tim Red Line akan mengekstrak feature hotel_id menjadi feature cross_sell sehingga mudah dalam menjalankan tahapan-tahapan selanjutnya. Adapun feature hotel_id akan langsung dihapus pada tahapan ini.","bee3c63c":"### 3.2.1 Distribusi antar feature\n<a id=\"3.2.1\"><\/a>\n***","e44b9632":"**Service_class vs is_tx_promo**\n<a id=\"3.1.3.13\"><\/a>","5cba51b2":"### 4.1.2 Feature Baru dari price\n<a id=\"4.1.2\"><\/a>\n***","6058ec5e":"**Hasil analisa dari grafik diatas adalah sebagai berikut**\n***\nDengan membuat scatterplot seperti grafik diatas, data outlier lebih terlihat. Distribusi dari setiap feature numerik juga bukan normal distribution melainkan high skewed distribution. Untuk mengurangi dampak dari permasalahan ini, data akan dilakukan normalisasi. Kelompok kami tidak menghapus data outlier tersebut karena melihat distribusi pada data test juga memiliki outlier dan memiliki distribusi high skewed.","1ce906f0":"## 5.1 Sampling, Normalisasi dan Split Data\n<a id=\"5.1\"><\/a>\n***\nDikarenakan dataset yang dimiliki mempunyai kelas\/target yang tidak balanced maka akan dilakukan metode sampling.\nMetode sampling ini menggunakan library imblearn dengan metode SMOTE (Synthetic Minority Over-sampling Technique) minority.\n\nAlasan tim kami memilih metode tersebut adalah\n1. SMOTE minority merupakan metode oversampling, dimana data dengan kelas minoritas (dalam hal ini terjadi cross_selling) dilakukan penggandaan sehingga seimbang dengan kelas mayoritas.\n2. Mempertimbangkan k-NN terdekat dari kelas minoritas, lalu membangun vektor ruang antar k.\n3. Oversampling dilakukan karena kami tidak ingin kehilangan banyak data jika menggunakan metode undersampling.","e7af671d":"## 4.2 Menghapus Feature Yang Tidak Berguna\n<a id=\"4.2\"><\/a>\n***","57b3bfdd":"## 5.2 Modelling-Validasi Dengan Berbagai Metode\n<a id=\"5.2\"><\/a>\n***","6463540a":"**Airlines_name vs Service_class**\n<a id=\"3.1.3.10\"><\/a>","ba8dc456":"# 5. Modelling dan Validasi\n<a id=\"5\"><\/a>\n***\nPada tahap ini dilakukan modelling terhadap data yang telah dibersihkan pada tahap sebelumnya.","a22c3a2e":"**Gender vs Airlines_name**\n<a id=\"3.1.3.2\"><\/a>","de4e53f9":"### 4.1.4 Feature Baru dari account_id dan cross_sell\n<a id=\"4.1.4\"><\/a>\n***","72a4e513":"## 4.1 Membuat Feature Baru\n<a id=\"4.1\"><\/a>\n***\nBerdasarkan hasil eksplorasi pada tahap-tahap sebelumnya maka beberapa feature dapat ditambahkan seperti dibawah ini. Adapun penambahan beberapa feature juga telah dilakukan pada sub-bab 2.2 untuk mengefisiensikan baris kode.","6ac3195f":"**Hasil analisa dari grafik diatas adalah sebagai berikut**\n***\n1. Dataset flight memiliki sangat banyak nilai ekstrim, terutama pada feature price dan no_of_seats. Hal ini dikarenakan feature price merupakan total pembayaran harga tiket berdasarkan jumlah tiket yang dibeli. Pembelian 10 seats secara langsung membuat price menjadi puluhan juta sedangkan pembelian 1 seats memiliki kisaran harga ratusan ribu hingga jutaan.\n\nSolusi dari permasalahan diatas dapat diatasi pada tahap feature engineering dengan menambahkan feature rata-rata harga tiket dengan formula price_avg = price\/no_of_seats","0c03965b":"Berikut merupakan shape\/ukuran dari X_train, X_test, y_train, dan y_test."}}