{"cell_type":{"3c878753":"code","2c4401a9":"code","a73f0d87":"code","3aa74ec7":"code","6e8b3011":"code","f3623538":"code","cb65012a":"code","a5d019b3":"code","d1344f20":"code","e2ed721d":"code","a96b6e4d":"code","33f413f9":"code","5c665fc0":"code","d24f0676":"code","c8aec8db":"code","8ac310b6":"code","05da4226":"code","583032ba":"code","0cd8919e":"code","0b94fd2a":"code","c3fd2b36":"code","f2c283bc":"code","a5f0ea91":"code","b01682d0":"code","a5e9764b":"code","e7e53899":"code","59d7f297":"code","e6828212":"code","29407e3c":"code","2eb11e05":"code","336b04f5":"code","51785a21":"code","c28b9a12":"code","0ddd09b2":"code","2df82fbb":"code","db9051a1":"code","d378fd28":"code","bf156adb":"code","37ecf3b0":"code","23a3e715":"code","56fa7299":"code","8f26ed77":"code","949c8cb6":"code","797ac2ca":"code","bf9dbb27":"code","5c6b94d4":"code","0c70463b":"code","efe7ced6":"code","1eacbb67":"code","9319784d":"code","1543cea5":"code","a7e298fd":"code","755e8116":"code","bf7ad014":"code","6f158579":"code","2c640dc1":"code","3f98f2b4":"code","cc0a995c":"code","c5701bc9":"code","b466d3bf":"code","e097f13a":"code","c0311482":"code","8e032147":"code","8f9bae5e":"markdown","e6c072d9":"markdown","89f86b2a":"markdown","fb9a84d0":"markdown","d5a0c28c":"markdown","b8ec5ac7":"markdown","5060f9ea":"markdown","2d76edcf":"markdown"},"source":{"3c878753":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"..\/input\"))\nimport matplotlib.pyplot as plt\nimport seaborn as se\nimport keras\nimport pickle \nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.models import save_model\nfrom IPython.display import Image\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier,ExtraTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score,roc_auc_score,f1_score\nfrom xgboost import XGBClassifier\nencoder = LabelEncoder()\nohe = OneHotEncoder()\nsc = StandardScaler()\nimport warnings\nwarnings.filterwarnings('ignore')\n# Any results you write to the current directory are saved as output.","2c4401a9":"df  = pd.read_csv('..\/input\/heart.csv')","a73f0d87":"df.head()","3aa74ec7":"df.shape","6e8b3011":"df.describe()","f3623538":"df.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',\n       'exercise_induced_angina', 'st_depression', 'st_slope', 'num_major_vessels', 'thalassemia', 'target']","cb65012a":"df['sex'][df['sex']==1] = 'male'\ndf['sex'][df['sex']==0] = 'female'","a5d019b3":"df['chest_pain_type'][df['chest_pain_type'] == 1] = 'typical angina'\ndf['chest_pain_type'][df['chest_pain_type'] == 2] = 'atypical angina'\ndf['chest_pain_type'][df['chest_pain_type'] == 3] = 'non-anginal pain'\ndf['chest_pain_type'][df['chest_pain_type'] == 4] = 'asymptomati'\ndf['fasting_blood_sugar'][df['fasting_blood_sugar'] == 0] = 'lower than 120mg\/ml'\ndf['fasting_blood_sugar'][df['fasting_blood_sugar'] == 1] = 'greater than 120mg\/ml'\n\ndf['rest_ecg'][df['rest_ecg'] == 0] = 'normal'\ndf['rest_ecg'][df['rest_ecg'] == 1] = 'ST-T wave abnormality'\ndf['rest_ecg'][df['rest_ecg'] == 2] = 'left ventricular hypertrophy'\n\ndf['exercise_induced_angina'][df['exercise_induced_angina'] == 0] = 'no'\ndf['exercise_induced_angina'][df['exercise_induced_angina'] == 1] = 'yes'\n\ndf['st_slope'][df['st_slope'] == 1] = 'upsloping'\ndf['st_slope'][df['st_slope'] == 2] = 'flat'\ndf['st_slope'][df['st_slope'] == 3] = 'downsloping'\n\ndf['thalassemia'][df['thalassemia'] == 1] = 'normal'\ndf['thalassemia'][df['thalassemia'] == 2] = 'fixed defect'\ndf['thalassemia'][df['thalassemia'] == 3] = 'reversable defect'","d1344f20":"df","e2ed721d":"se.countplot(data=df,x = 'sex')","a96b6e4d":"impaact_on_gender = pd.crosstab(df['target'],df['sex'])\nimpaact_on_gender\n","33f413f9":"impaact_on_gender.plot(kind = 'bar')","5c665fc0":"df[df.duplicated()  == True]","d24f0676":"df.drop_duplicates(inplace=True)","c8aec8db":"male_gender = len(df[df.sex == 1])\nfemale_gender = len(df[df.sex == 0])\n\nprint(\"In this dataset there exists {0} male subjects and {1} female subjects which computes to {2}% for males and {3}% for females.\".format(male_gender, female_gender, round((male_gender\/len(df.sex)), 2)*100, round((female_gender\/len(df.sex)), 2)*100))","8ac310b6":"impaact_on_gender","05da4226":"male =len(df[df['sex'] == 'male'])\nfemale = len(df[df['sex']== 'female'])\n\nplt.figure(figsize=(8,6))\n\n# Data to plot\nlabels = 'Male','Female'\nsizes = [male,female]\ncolors = ['skyblue', 'yellowgreen']\nexplode = (0, 0)  # explode 1st slice\n \n# Plot\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\nautopct='%1.1f%%', shadow=True, startangle=90)\n \nplt.axis('equal')\n","583032ba":"se.pairplot(df)","0cd8919e":"df['chest_pain_type'].value_counts()","0b94fd2a":"plt.figure(figsize=(8,6))\n\n# Data to plot\nlabels = 'Chest Pain Type:0','Chest Pain Type:1','Chest Pain Type:2','Chest Pain Type:3'\nsizes = [len(df[df['chest_pain_type'] == 0]),len(df[df['chest_pain_type'] == 'atypical angina']),\n         len(df[df['chest_pain_type'] == 'non-anginal pain']),\n         len(df[df['chest_pain_type'] == 'non-anginal pain'])]\ncolors = ['red', 'green','orange','gold']\nexplode = (0, 0,0,0)  # explode 1st slice\n \n# Plot\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\nautopct='%1.1f%%', shadow=True, startangle=180)\n \nplt.axis('equal')\nplt.show()","c3fd2b36":"df.head()","f2c283bc":"plt.figure(figsize=(8,6))\n\n# Data to plot\nlabels = 'greater than 120mg\/ml','greater than 120mg\/ml'\nsizes = [len(df[df['fasting_blood_sugar'] == 'greater than 120mg\/ml']),len(df[df['fasting_blood_sugar'] == 'lower than 120mg\/ml'])]\ncolors = ['red', 'green','orange','gold']\nexplode = (0,0)  # explode 1st slice\n \n# Plot\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,shadow=True, startangle=180)\n \nplt.axis('equal')\nplt.show()","a5f0ea91":"plt.figure(figsize=(8,6))\nse.countplot(x = df['rest_ecg'],data = df)","b01682d0":"plt.figure(figsize=(14,8))\nse.heatmap(df.corr(), annot = True, cmap='summer',linewidths=.1)\nplt.show()","a5e9764b":"df.info()","e7e53899":"df.hist(figsize=(30,30))\nplt.show()","59d7f297":"se.pairplot(df, hue=\"target\",diag_kind='kde',kind='scatter')","e6828212":"numerical = df.select_dtypes(exclude=['object'])\ncategorical = df.select_dtypes(include=['object'])\n","29407e3c":"numerical.columns","2eb11e05":"categorical.columns","336b04f5":"se.scatterplot(x = df['max_heart_rate_achieved'],y = df['resting_blood_pressure'],data= numerical,hue = df['age'])","51785a21":"se.scatterplot(x = df['st_depression'],y = df['resting_blood_pressure'],data= numerical,hue = df['age'])","c28b9a12":"numerical.plot(kind = 'hist')","0ddd09b2":"categorical.columns","2df82fbb":"df.head(100)","db9051a1":"df['chest_pain_type'].value_counts()\ndf['chest_pain_type'].replace(0,'atypical angina',inplace=True)","d378fd28":"df['st_slope'].value_counts()\ndf['st_slope'].replace(0,'flat',inplace=True)","bf156adb":"df['thalassemia'].value_counts()\ndf['thalassemia'].replace(0,'fixed defect',inplace=True)\n","37ecf3b0":"df['sex'] = encoder.fit_transform(df['sex'])\ndf['chest_pain_type'] = encoder.fit_transform(df['chest_pain_type'])\ndf['fasting_blood_sugar'] = encoder.fit_transform(df['fasting_blood_sugar'])\ndf['rest_ecg'] = encoder.fit_transform(df['chest_pain_type'])\ndf['st_slope'] = encoder.fit_transform(df['st_slope'])\ndf['thalassemia'] = encoder.fit_transform(df['thalassemia'])\n","23a3e715":"X = df.iloc[:,:-1].values\ny = df.iloc[:,-1].values","56fa7299":"onehotencoder = OneHotEncoder()\nX = onehotencoder.fit_transform(X).toarray()","8f26ed77":"X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=0,test_size=0.33)","949c8cb6":"X_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","797ac2ca":"models = []\nmodels.append(('Log Classifier', LogisticRegression()))\nmodels.append(('Support Vector Classifier', SVC()))\nmodels.append(('DT Classifier', DecisionTreeClassifier(random_state=0,max_depth = 10)))\nmodels.append(('Random Forest', RandomForestClassifier(random_state=0,n_estimators=100,min_samples_split=2,min_impurity_decrease=0.1)))\nmodels.append(('Extra Tree', ExtraTreeClassifier(random_state=0,min_samples_leaf=1,max_depth=10)))\nmodels.append(('GB Classifier Accuracy',GradientBoostingClassifier(random_state=0, n_estimators=500, learning_rate=1.0)))\nmodels.append(('XGB Classifier Accuracy',XGBClassifier()))\n\n\n#evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    model = model.fit(X_train,y_train.ravel())\n    predict = model.predict(X_test)\n    score = accuracy_score(y_test.ravel(), predict)\n    names.append(name)\n    msg = name,score\n    print(msg)\n    \ncolors = [\"purple\", \"green\", \"orange\", \"magenta\",\"#CFC60E\",\"#0FBBAE\"]\naccuracies = {'Linear Classifier':76,'Support Vector Classifier':76,'Random Forest':62,'Extra Tree':62,'GB Classifier Accuracy':77,\"XGB Classifier Accuracy\":76}\n\nse.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,100,10))\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Algorithms\")\nse.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)\nplt.show()","bf9dbb27":"input_dim = X_train.shape[1]\nregressor = Sequential()\nregressor.add(Dense(13,activation='relu',input_dim = input_dim,kernel_initializer = 'uniform'))\nregressor.add(Dense(13,activation='relu'))\nregressor.add(Dense(13,activation='relu'))\nregressor.add(Dense(13,activation='relu'))\nregressor.add(Dropout(0.5))\nregressor.add(Dense(1,activation='sigmoid',kernel_initializer='uniform'))\nregressor.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n\n","5c6b94d4":"history = regressor.fit(X_train,y_train,batch_size=64,epochs=700,validation_data=(X_test,y_test),verbose=2)\npredictions_ann=regressor.predict(X_test)\nprint(history.history.keys())\nregressor.evaluate(x = X_test,y = y_test)","0c70463b":"# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","efe7ced6":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_auc_score,roc_curve,auc\nfrom sklearn.model_selection import cross_val_score\nknn =KNeighborsClassifier()\nlr_model = LogisticRegression()\nsvc_model = SVC(probability=True)\ndt_model = DecisionTreeClassifier()\nrf_model = RandomForestClassifier()\ngb_model = GradientBoostingClassifier()\nparams_lg  = {'penalty':['l1','l2'],\n         'C':[0.01,0.1,1,10,100],\n         'class_weight':['balanced',None]}\nparams_knn = {'n_neighbors':[i for i in range(1,30,2)]}\nparams_svc = {'kernel':['linear','poly','rbf'],'gamma':[0.1, 1, 10, 100]}\nparams_dt  = {'max_features': ['auto', 'sqrt', 'log2'],\n          'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15], \n          'min_samples_leaf':[1,2,3,4,5,6,7,8,9,10,11]}\nparam_rf = {'n_estimators':[1, 2, 4, 8, 16, 32, 64, 100, 200],'max_depth':[int(x) for x in np.linspace(1, 32, 32, endpoint=True)]}\nparam_gb = {'learning_rate':[1, 0.5, 0.25, 0.1, 0.05, 0.01],'n_estimators':[1, 2, 4, 8, 16, 32, 64, 100, 200],'max_depth':[int(x) for x in np.linspace(1, 32, 32, endpoint=True)]}","1eacbb67":"knn_model = RandomizedSearchCV(estimator=knn,param_distributions=params_knn)\nknn_model.fit(X_train,y_train)\nknn_model.best_params_","9319784d":"print('Accuracy Score: ',accuracy_score(y_test,knn_model.predict(X_test)))\nprint('Using k-NN we get an accuracy score of: ',\n      round(accuracy_score(y_test,knn_model.predict(X_test)),5)*100,'%')","1543cea5":"predict_probabilities = knn_model.predict_proba(X_test)[:,1]\n#Create true and false positive rates\nfalse_positive_rate_knn,true_positive_rate_knn,threshold_knn = roc_curve(y_test,predict_probabilities)\n#Plot ROC Curve\nplt.figure(figsize=(10,6))\nplt.title('Revceiver Operating Characterstic')\nplt.plot(false_positive_rate_knn,true_positive_rate_knn)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()\nprint(\"AUC SCORE FOR KNN:\",roc_auc_score(y_test,predict_probabilities))\n","a7e298fd":"lr_model = RandomizedSearchCV(estimator=lr_model,param_distributions=params_lg)\nlr_model.fit(X_train,y_train)\nprint(lr_model.best_params_)\nprint('Accuracy Score: ',accuracy_score(y_test,lr_model.predict(X_test)))\nprint('Using Log Regression we get an accuracy score of: ',\n      round(accuracy_score(y_test,lr_model.predict(X_test)),5)*100,'%')","755e8116":"predict_probabilities = lr_model.predict_proba(X_test)[:,1]\n#Create true and false positive rates\nfalse_positive_rate_knn,true_positive_rate_knn,threshold_knn = roc_curve(y_test,predict_probabilities)\n#Plot ROC Curve\nplt.figure(figsize=(10,6))\nplt.title('Revceiver Operating Characterstic')\nplt.plot(false_positive_rate_knn,true_positive_rate_knn)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()\nprint(\"AUC SCORE FOR LR:\",roc_auc_score(y_test,predict_probabilities))\n","bf7ad014":"svc_model = RandomizedSearchCV(estimator=svc_model,param_distributions=params_svc)\nsvc_model.fit(X_train,y_train)\nprint(svc_model.best_params_)\nprint('Accuracy Score: ',accuracy_score(y_test,svc_model.predict(X_test)))\nprint('Using SVC we get an accuracy score of: ',\n      round(accuracy_score(y_test,svc_model.predict(X_test)),5)*100,'%')","6f158579":"predict_probabilities = svc_model.predict_proba(X_test)[:,1]\n#Create true and false positive rates\nfalse_positive_rate_knn,true_positive_rate_knn,threshold_knn = roc_curve(y_test,predict_probabilities)\n#Plot ROC Curve\nplt.figure(figsize=(10,6))\nplt.title('Revceiver Operating Characterstic')\nplt.plot(false_positive_rate_knn,true_positive_rate_knn)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()\nprint(\"AUC SCORE FOR SVC:\",roc_auc_score(y_test,predict_probabilities))\n","2c640dc1":"dt_model = RandomizedSearchCV(estimator=dt_model,param_distributions=params_dt)\ndt_model.fit(X_train,y_train)\nprint(dt_model.best_params_)\nprint('Accuracy Score: ',accuracy_score(y_test,dt_model.predict(X_test)))\nprint('Using Decision tree we get an accuracy score of: ',\n      round(accuracy_score(y_test,dt_model.predict(X_test)),5)*100,'%')","3f98f2b4":"predict_probabilities = dt_model.predict_proba(X_test)[:,1]\n#Create true and false positive rates\nfalse_positive_rate_knn,true_positive_rate_knn,threshold_knn = roc_curve(y_test,predict_probabilities)\n#Plot ROC Curve\nplt.figure(figsize=(10,6))\nplt.title('Revceiver Operating Characterstic')\nplt.plot(false_positive_rate_knn,true_positive_rate_knn)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()\nprint(\"AUC SCORE FOR DecisionTree:\",roc_auc_score(y_test,predict_probabilities))\n","cc0a995c":"rf_model = RandomizedSearchCV(estimator=rf_model,param_distributions=param_rf)\nrf_model.fit(X_train,y_train)\nprint(rf_model.best_params_)\nprint('Accuracy Score: ',accuracy_score(y_test,rf_model.predict(X_test)))\nprint('Using RF we get an accuracy score of: ',\n      round(accuracy_score(y_test,rf_model.predict(X_test)),5)*100,'%')","c5701bc9":"predict_probabilities = rf_model.predict_proba(X_test)[:,1]\n#Create true and false positive rates\nfalse_positive_rate_knn,true_positive_rate_knn,threshold_knn = roc_curve(y_test,predict_probabilities)\n#Plot ROC Curve\nplt.figure(figsize=(10,6))\nplt.title('Revceiver Operating Characterstic')\nplt.plot(false_positive_rate_knn,true_positive_rate_knn)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()\nprint(\"AUC SCORE FOR RandomForest:\",roc_auc_score(y_test,predict_probabilities))\n","b466d3bf":"gb_model = RandomizedSearchCV(estimator=gb_model,param_distributions=param_gb)\ngb_model.fit(X_train,y_train)\nprint(gb_model.best_params_)\nprint('Accuracy Score: ',accuracy_score(y_test,gb_model.predict(X_test)))\nprint('Using RF we get an accuracy score of: ',\n      round(accuracy_score(y_test,gb_model.predict(X_test)),5)*100,'%')","e097f13a":"predict_probabilities = gb_model.predict_proba(X_test)[:,1]\n#Create true and false positive rates\nfalse_positive_rate_knn,true_positive_rate_knn,threshold_knn = roc_curve(y_test,predict_probabilities)\n#Plot ROC Curve\nplt.figure(figsize=(10,6))\nplt.title('Revceiver Operating Characterstic')\nplt.plot(false_positive_rate_knn,true_positive_rate_knn)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()\nprint(\"AUC SCORE FOR Gradient boosting:\",roc_auc_score(y_test,predict_probabilities))\n","c0311482":"# Evaluate the model using 10-fold cross-validation\nclf=rf_model\n\nscores = cross_val_score(clf,X_train,y_train, scoring='accuracy', cv=10)\nprint (scores)\nprint (scores.mean())\n\nSEED=42\nmean_auc = 0.0\nn = 10  # repeat the CV procedure 10 times to get more precise results\nfor i in range(n):\n    # for each iteration, randomly hold out 20% of the data as CV set\n    X_train, X_cv, y_train, y_cv = train_test_split(\n    X_train,y_train, test_size=.20, random_state=i*SEED)\n\n    # train model and make predictions\n    clf.fit(X_train, y_train) \n    preds = clf.predict_proba(X_cv)[:, 1]\n\n    # compute AUC metric for this CV fold\n    fpr, tpr, thresholds = roc_curve(y_cv, preds)\n    roc_auc = auc(fpr, tpr)\n    print (\"AUC (fold %d\/%d): %f\" % (i + 1, n, roc_auc))\n    mean_auc += roc_auc\n\nprint (\"Mean AUC: %f\" % (mean_auc\/n)) ","8e032147":"# Save the trained model as a pickle string. \nmodel_pickle_path = 'rf_model.pkl'\nohe_model_path = 'ohe_model.pkl'\nscaling_model_path = 'scaled_model.pkl'\n \n# Create an variable to pickle and open it in write mode\nmodel_pickle = open(model_pickle_path, 'wb')\nohe_pickle = open(ohe_model_path, 'wb')\nscaled_pickle = open(scaling_model_path, 'wb')\n\nsaved_model = pickle.dump(rf_model, model_pickle)\nsaved_ohe = pickle.dump(onehotencoder, ohe_pickle)\nsaved_sc = pickle.dump(sc, scaled_pickle)\n\nmodel_pickle.close()\nohe_pickle.close()\nscaled_pickle.close()\n# Load the pickled model \nrf_from_pickle = open(model_pickle_path, 'rb')\nohe_from_pickle = open(ohe_model_path,'rb')\nsc_from_pickle = open(scaling_model_path,'rb')\n#decision_tree_model = pickle.load(decision_tree_model_pkl)\nrf_from_pickle = pickle.load(rf_from_pickle)\nohe_from_pickle = pickle.load(ohe_from_pickle)\nsc_from_pickle = pickle.load(sc_from_pickle)\n\nprint(rf_from_pickle)\nprint(ohe_from_pickle)\nprint(sc_from_pickle)","8f9bae5e":"#### Saving the model,OHE and Scaling object as pickle file","e6c072d9":"### Implementing RandomsearchCv** to select best parameters and applying ML Algorithm","89f86b2a":"##### After performing Randomized Search and analyzing Metrices we came into conclusion that Random Forest gave maximum accuracy and performed well on AUC ROC.Hence we will move for Cross validation of RF Model.","fb9a84d0":"### ANN using Three layers and one Dropout\n","d5a0c28c":"### Building Baseline Models","b8ec5ac7":"### Exploratory Data Analysis","5060f9ea":" - age: The person's age in years\n - sex: The person's sex (1 = male, 0 = female)\n - cp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n - trestbps: The person's resting blood pressure (mm Hg on admission to the hospital)\n - chol: The person's cholesterol measurement in mg\/dl\n - fbs: The person's fasting blood sugar (> 120 mg\/dl, 1 = true; 0 = false)\n - restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n - thalach: The person's maximum heart rate achieved\n - exang: Exercise induced angina (1 = yes; 0 = no)\n - oldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n - slope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n - ca: The number of major vessels (0-3)\n - thal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n - target: Heart disease (0 = no, 1 = yes)\n","2d76edcf":"* * 1. ## Feature Engineering "}}