{"cell_type":{"4f8d8f8b":"code","e403519b":"code","46514058":"code","a2c8e627":"code","64bd71ec":"code","46525bbd":"code","613e9abc":"code","cd74b94f":"code","0b89c79b":"code","562aac70":"code","72bab78b":"code","e0732ca3":"code","3866927d":"code","ce50cee9":"code","34c1c6f1":"code","33fe582d":"code","cc2b09a6":"code","fa686d52":"code","0a8bd186":"code","3b7d7b3e":"code","473581a4":"code","6f098ed3":"code","2dd8746c":"code","09fd1ca6":"code","d3f00279":"code","0403a770":"code","f47c2f3f":"code","269f0c6e":"code","6bea1cf0":"code","1ffdc84b":"code","5ca852e3":"code","71ba9b39":"code","c3aaf03d":"code","d7873ed4":"code","d2ad8e9f":"code","47c498a3":"code","4d0695d6":"code","0f80cdd4":"code","6b197399":"code","34000a39":"code","c8fabf2b":"code","f89ea25e":"code","cdbafe9d":"code","b2dabf45":"code","2201455b":"code","23b07244":"code","007bf053":"code","56eed33a":"code","1651543e":"code","66432f85":"markdown","8337509d":"markdown","a2b3a501":"markdown","e10477bb":"markdown","848e5b34":"markdown","f3e1c1b5":"markdown","c878249c":"markdown","d578c64d":"markdown","e74f858c":"markdown","1c95c58c":"markdown","4cc8a087":"markdown","48f3e6c3":"markdown","de535d27":"markdown","7312e24a":"markdown","a8914091":"markdown","66fd3e2d":"markdown","f8b1d3b6":"markdown","d845cb31":"markdown","2001443a":"markdown","b4b81d47":"markdown","b9bd7e87":"markdown","68c7c092":"markdown","172b01d5":"markdown","4f65d403":"markdown","1b5efae6":"markdown","2fb652d3":"markdown","aea0c8f6":"markdown","e96cba2e":"markdown","3a4510d7":"markdown","8ddf0915":"markdown","abf26ae1":"markdown","b6c74ddc":"markdown","686aa6d9":"markdown","1627ac27":"markdown","05117a97":"markdown","30358079":"markdown"},"source":{"4f8d8f8b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom numpy import mean\n\nimport os\nimport matplotlib.pyplot as plot\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.feature_selection import SelectKBest, chi2\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\n\nfrom imblearn.ensemble import BalancedRandomForestClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss \nfrom imblearn.under_sampling import RandomUnderSampler\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\nimport seaborn as sns # visualization\nimport matplotlib.pyplot as plt # visualization","e403519b":"#constants:\n\nseed = 7\ntestSize = 0.35","46514058":"#save accs score to compare later\nDT_scores = {}\nRF_scores = {}","a2c8e627":"def accuracyCheck(y_test, preds, printMe):\n    toRet={}\n    accuracy1 = accuracy_score(y_test, preds) * 100\n    \n    if printMe:\n        print('Accuracy accuracy_score:', round(accuracy1, 2), '%.')\n    \n#     avg_options = ['macro', 'weighted']\n    avg_options = ['weighted']\n\n    for i in range(0, len(avg_options)):\n        avg = avg_options[i]\n        accuracy2 = round(f1_score(y_test, preds, average=avg) * 100, 2)\n        \n        if printMe:\n            print(\"\\n(\", avg, \"avg):\")\n            print('Accuracy f1_score:', accuracy2, '%.')\n\n        name = avg + \"_f1\"\n        toRet[name] = accuracy2\n\n    return toRet\n","64bd71ec":"def fit_predict(model, X_train, y_train):\n    class_names = y_rta.unique()\n\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    accs = accuracyCheck(y_test, preds, True)\n    \n    fig, ax = plt.subplots(figsize=(5, 5))\n    plot_confusion_matrix(model, X_test, y_test, normalize='true', cmap=plt.cm.Blues,display_labels=class_names, ax=ax)\n    plt.show()\n    \n    return accs\n    \ndef Kbest_fit_predict(model, X_train, y_train, X_test, y_test, printMatrix):\n    class_names = y_rta.unique()\n\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    accs = accuracyCheck(y_test, preds, False)\n    \n    if printMatrix:\n        fig, ax = plt.subplots(figsize=(5, 5))\n        plot_confusion_matrix(model, X_test, y_test, normalize='true', cmap=plt.cm.Blues,display_labels=class_names, ax=ax)\n        plt.show()\n    \n    return accs\n\n\n#     print(confusion_matrix(y_test, preds))","46525bbd":"# choose model Random Forest:\ndef RF_fitPredict(X_train, y_train):\n    model = RandomForestClassifier()\n    return fit_predict(model, X_train, y_train)\n    \n# choose model Decision Tree:\ndef DT_fitPredict(X_train, y_train):\n    model = DecisionTreeClassifier()\n    return fit_predict(model, X_train, y_train)\n\n# choose model Balanced Decision Tree:\ndef BalancedDT_fitPredict(X_train, y_train):    \n    model = DecisionTreeClassifier(class_weight='balanced')\n    return fit_predict(model, X_train, y_train)\n\n# choose model Balanced Random Forest:\ndef BalancedRF_fitPredict(X_train, y_train):\n    model = BalancedRandomForestClassifier(max_depth=3, random_state=0, n_jobs=8)\n    return fit_predict(model, X_train, y_train)\n    \n# choose model Random Forest:\ndef Kbest_RF_fitPredict(X_train, y_train, X_test, y_test):\n    model = RandomForestClassifier()\n    return Kbest_fit_predict(model, X_train, y_train, X_test, y_test, False)\n    \n# choose model Decision Tree:\ndef Kbest_DT_fitPredict(X_train, y_train, X_test, y_test):\n    model = DecisionTreeClassifier()\n    return Kbest_fit_predict(model, X_train, y_train, X_test, y_test, False)","613e9abc":"# split to train and test:\n\ndef splitTrainTest(X, y):\n    return train_test_split(X, y, test_size=testSize, random_state=seed)\n\ndef showValueCount(col):\n    col.value_counts(dropna=False).plot.bar(figsize=(4,4))\n    \ndef crossValidation(model, X, y):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=seed)\n    scores = cross_val_score(model, X, y, cv=cv)\n    print('Mean ROC AUC: %.3f' % mean(scores))\n    \n    # scores = cross_val_score(model, X_new, y_new, cv=5)\n    # print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n    \n    \ndef DT_crossValidation(X, y):\n    model = DecisionTreeClassifier()\n    crossValidation(model, X, y)\n\ndef RF_crossValidation(X, y):\n    model = RandomForestClassifier()\n    crossValidation(model, X, y)","cd74b94f":"# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\nrta_data = pd.read_csv(\"\/kaggle\/input\/road-traffic-accidents\/RTA Dataset.csv\")\nrta_data.head()","0b89c79b":"bad_cols = ['Casualty_class', 'Sex_of_casualty', 'Age_band_of_casualty', 'Casualty_severity', 'Work_of_casuality', 'Fitness_of_casuality']\nnot_clear_cols = ['Defect_of_vehicle', 'Service_year_of_vehicle', 'Type_of_vehicle']\n\nto_remove_cols = bad_cols + not_clear_cols\n\nrta_afterDrop = rta_data.drop(columns=to_remove_cols)\n\n# rta_afterDrop = rta_afterDrop[rta_afterDrop.Accident_severity != 'Fatal injury']","562aac70":"rta_afterDrop.head()","72bab78b":"rta_afterDrop.info()","e0732ca3":"rta_normlize = rta_afterDrop.replace({'Driving_experience': {\"unknown\": \"Unknown\"}})\nrta_noNAN = rta_normlize.fillna(\"Unknown\")","3866927d":"rta_noNAN = rta_noNAN.replace(\"Unknown\", method='ffill')\nrta_fullVals = rta_noNAN.replace(\"Unknown\", method='bfill')\nrta_fullVals","ce50cee9":"rta_fullVals.isna().sum()","34c1c6f1":"rta_fullVals.info()","33fe582d":"def formatTimeCol(t):\n    t = t[:2]\n    if \":\" in t:\n        t = t[:1]\n        \n    return int(t)\n\ndef categorizeTimeCol(t):\n    if t >= 6 and t< 18:\n        return \"Day\"\n    else:\n        return \"Night\"\n\n\nrta_fullVals['Time'] = rta_fullVals['Time'].apply(lambda x: formatTimeCol(x))\nrta_fullVals['Time'] = rta_fullVals['Time'].apply(lambda x: categorizeTimeCol(x))\n\nrta_fullVals['Time'].value_counts(dropna=False)\n","cc2b09a6":"rta_mapProcess = rta_fullVals\n\nrta_mapProcess['Sex_of_driver'] = rta_mapProcess['Sex_of_driver'].map({'Female': 0, 'Male': 1})\nrta_mapProcess['Time'] = rta_mapProcess['Time'].map({'Day': 0, 'Night': 1})\n\nrta_mapProcess['Age_band_of_driver'] = rta_mapProcess['Age_band_of_driver'].map({'Under 18': 0, '18-30': 1, '31-50': 2, 'Over 51': 3})\nrta_mapProcess['Driving_experience'] = rta_mapProcess['Driving_experience'].map({'No Licence': 0, 'Below 1yr': 1, '1-2yr': 2, '2-5yr': 3, '5-10yr': 4, 'Above 10yr': 5})\nrta_mapProcess['Educational_level'] = rta_mapProcess['Educational_level'].map({'Illiterate': 0, 'Writing & reading': 1, 'Elementary school': 2, 'Junior high school': 3, 'High school': 4, 'Above high school': 5})\n\nrta_mapProcess['Accident_severity'] = rta_mapProcess['Accident_severity'].map({'Slight Injury': 0, 'Serious Injury': 1, 'Fatal injury': 2})\n\nmapped_cols = ['Driving_experience', 'Sex_of_driver', 'Age_band_of_driver', 'Educational_level', 'Time', 'Accident_severity']\nrta_afterMap = rta_mapProcess","fa686d52":"rta_afterMap.info()","0a8bd186":"columns = list(rta_afterMap.columns)\nint_cols = ['Number_of_vehicles_involved', 'Number_of_casualties']\nno_need_convert = int_cols + mapped_cols\n\ncolumns_needed = list(set(columns) - set(no_need_convert))\nprint(\"Cols:\", columns_needed)\n\nrta_cleanWdummies = pd.get_dummies(rta_afterMap, columns=columns_needed)\nrta_cleanWdummies","3b7d7b3e":"rta_cleanWdummies.info()","473581a4":"rta_cleanWdummies.head()","6f098ed3":"X_rta = rta_cleanWdummies.drop(columns=['Accident_severity'])\ny_rta = rta_cleanWdummies['Accident_severity']\n\nX_train, X_test, y_train, y_test = splitTrainTest(X_rta, y_rta)\n\nprint(X_train.shape)\nprint(X_test.shape)","2dd8746c":"DT_scores[\"Original fit\"] = DT_fitPredict(X_train, y_train)","09fd1ca6":"RF_scores[\"Original fit\"] = RF_fitPredict(X_train, y_train)","d3f00279":"showValueCount(rta_cleanWdummies['Accident_severity'])","0403a770":"DT_scores[\"BalancedDT fit\"] = BalancedDT_fitPredict(X_train, y_train)","f47c2f3f":"RF_scores[\"BalancedRF fit\"] = BalancedRF_fitPredict(X_train, y_train)","269f0c6e":"# transform the dataset\nundersample = NearMiss()\nX_train_US, y_train_US = undersample.fit_resample(X_train, y_train)\nshowValueCount(y_train_US)","6bea1cf0":"DT_scores[\"US DT fit\"] = DT_fitPredict(X_train_US, y_train_US)","1ffdc84b":"RF_scores[\"US RF fit\"] = RF_fitPredict(X_train_US, y_train_US)","5ca852e3":"# transform the dataset\noversample = SMOTE()\nX_train_OS, y_train_OS = oversample.fit_resample(X_train, y_train)\nshowValueCount(y_train_OS)","71ba9b39":"DT_scores[\"OS DT fit\"] = DT_fitPredict(X_train_OS, y_train_OS)","c3aaf03d":"RF_scores[\"OS RF fit\"] = RF_fitPredict(X_train_OS, y_train_OS)","d7873ed4":"rta_cleanWdummies.corr()['Accident_severity'].sort_values(ascending=False)","d2ad8e9f":"DT_K_best = {}\nRF_K_best = {}\n\nfor kNum in range(1, 120, 10):\n\n    selector = SelectKBest(chi2, k=kNum)\n    selector.fit(X_train, y_train)\n\n    X_train_k = selector.transform(X_train)\n    y_train_k = y_train\n\n    selected_feature_names = X_train.columns[selector.get_support(indices=True)].tolist()\n\n    X_test_k = X_test[selected_feature_names]\n    y_test_k = y_test\n\n    DT_K_best[str(kNum)] = Kbest_DT_fitPredict(X_train_k, y_train_k, X_test_k, y_test_k)\n    RF_K_best[str(kNum)] = Kbest_RF_fitPredict(X_train_k, y_train_k, X_test_k, y_test_k)\n","47c498a3":"DT_K_best","4d0695d6":"RF_K_best","0f80cdd4":"selector = SelectKBest(chi2, k=25)\nselector.fit(X_train, y_train)\n\nX_train_k = selector.transform(X_train)\ny_train_k = y_train\n\nselected_feature_names_k = X_train.columns[selector.get_support(indices=True)].tolist()\nprint(X_train_k.shape)\n\nX_test_k = X_test[selected_feature_names_k]\ny_test_k = y_test","6b197399":"DT_scores[\"Kbest DT fit\"] = Kbest_DT_fitPredict(X_train_k, y_train_k, X_test_k, y_test_k)","34000a39":"RF_scores[\"Kbest RF fit\"] = Kbest_RF_fitPredict(X_train_k, y_train_k, X_test_k, y_test_k)","c8fabf2b":"DT_K_best = {}\nRF_K_best = {}\n\nfor kNum in range(1, 120, 10):\n\n    selector = SelectKBest(chi2, k=kNum)\n    selector.fit(X_train_OS, y_train_OS)\n\n    X_train_k_os = selector.transform(X_train_OS)\n    y_train_k_os = y_train_OS\n\n    selected_feature_names = X_train_OS.columns[selector.get_support(indices=True)].tolist()\n\n    X_test_k_os = X_test[selected_feature_names]\n    y_test_k_os = y_test\n\n    DT_K_best[str(kNum)] = Kbest_DT_fitPredict(X_train_k_os, y_train_k_os, X_test_k_os, y_test_k_os)\n    RF_K_best[str(kNum)] = Kbest_RF_fitPredict(X_train_k_os, y_train_k_os, X_test_k_os, y_test_k_os)\n","f89ea25e":"DT_K_best","cdbafe9d":"RF_K_best","b2dabf45":"selector = SelectKBest(chi2, k=90)\nselector.fit(X_train_OS, y_train_OS)\n\nX_train_k_os = selector.transform(X_train_OS)\ny_train_k_os = y_train_OS\n\nselected_feature_names_os = X_train.columns[selector.get_support(indices=True)].tolist()\nprint(X_train_k_os.shape)\n\nX_test_k_os = X_test[selected_feature_names_os]\ny_test_k_os = y_test","2201455b":"DT_scores[\"Kbest+OS DT fit\"] = Kbest_DT_fitPredict(X_train_k_os, y_train_k_os, X_test_k_os, y_test_k_os)","23b07244":"RF_scores[\"Kbest+OS RF fit\"] = Kbest_RF_fitPredict(X_train_k_os, y_train_k_os, X_test_k_os, y_test_k_os)","007bf053":"DT_scores","56eed33a":"RF_scores","1651543e":"X_new = X_test[selected_feature_names_k]\ny_new = y_test\n\nDT_crossValidation(X_new, y_new)\nRF_crossValidation(X_new, y_new)","66432f85":"wrote few functions for convenience","8337509d":"It can be seen by the Matrix that there is very big mistake in prediction the sever accidents.","a2b3a501":"start with 'Time' column: \nconvert it from string <HH:MM:SS> to \"Day\"\/\"Night\"","e10477bb":"Train & fit with DT <a class=\"anchor\" id=\"DT\"><\/a>","848e5b34":"This project uses ML to help to identify the severity of a road accident.\n\nThe selected dataset contains extensive and rich information regarding road accidents that have occurred in Addis Ababa City for 3 years. \nThe dataset contains 32 features and more than 12K records. \nEach record represents road accident details, including driver\u2019s details\n\nThe target attribute is \u2018Accident_severity\u2019, which contains 3 categorical  possible values, the values indicate on an accident injury severity: Slight, Serious, Fatal. \n\n","f3e1c1b5":"# Drop non relvant columns <a class=\"anchor\" id=\"drop\"><\/a>","c878249c":"# Cross validation <a class=\"anchor\" id=\"CV\"><\/a>","d578c64d":"# Fill empty values <a class=\"anchor\" id=\"fill\"><\/a>","e74f858c":"# Read the data: <a class=\"anchor\" id=\"read\"><\/a>","1c95c58c":"Train & fit with RF <a class=\"anchor\" id=\"RF\"><\/a>","4cc8a087":" Data is clean, full and ready for process  :)","48f3e6c3":"Kbest is in the range of 21-31.\nSo I chose k=25","de535d27":"# my functions: <a class=\"anchor\" id=\"func\"><\/a>","7312e24a":"# Select K best: <a class=\"anchor\" id=\"kBest\"><\/a>","a8914091":"took the best option and preformed cross validation on it.","66fd3e2d":"In order to do that, examine few kbest option, and check accuracy for each.","f8b1d3b6":"# Handle imbalanced data <a class=\"anchor\" id=\"handle\"><\/a>","d845cb31":"make sure all the data is full, no missing values.","2001443a":"# Split data to train and test <a class=\"anchor\" id=\"split\"><\/a>","b4b81d47":"# Start to fit and predict with ML models: <a class=\"anchor\" id=\"models\"><\/a>","b9bd7e87":"Map logical categorical columns using dictionary","68c7c092":"* [Imports](#imports)\n* [Inner functions](#func)\n* [Read data](#read)\n* [Prepare data](#prepare)\n    * [drop columns](#drop)\n    * [fill empty values](#fill)\n    * [map to numric](#map)\n* [Split data to train and test](#split)\n* [ML model](#models)\n    * [DT](#DT)\n    * [RF](#RF)\n* [Handle Imbalance](#handle)\n    * [balanced models](#modelsBalanced)\n    * [Under Sample](#US)\n    * [Over Sample](#OS)\n* [Select K best](#kBest)\n* [Select K best with Over Sample](#kBestOS)\n* [Cross validation](#CV)\n","172b01d5":"Kbest is in the range of 91-101.\nSo I chose k=90","4f65d403":"preformed Kbest again, combined with OverSample: <a class=\"anchor\" id=\"kBestOS\"><\/a>","1b5efae6":"The accuracy_score isn't fit here since the target isn't balancd.\nI'll use f1_score with weighted avg.","2fb652d3":"# RTA project <a class=\"anchor\" id=\"UP\"><\/a>","aea0c8f6":"Map all the rest columns by using 'get_dummies' func","e96cba2e":"try to choose the features that most relvant to the target","3a4510d7":"So I tried using UnderSample and OverSample.\nThe US wasn't successful due to very shrinked data volume. <a class=\"anchor\" id=\"US\"><\/a>","8ddf0915":"# Convert types of columns to int <a class=\"anchor\" id=\"map\"><\/a>","abf26ae1":"The OS improved a bit the accuracy. <a class=\"anchor\" id=\"OS\"><\/a>","b6c74ddc":"Handling the unbalanced data was challenging and interesting. \nI tried few approaches during the process, and in the end, I chose the one with the best accuracy.\n\nThe approaches I tried included use of Balanced DT and Balanced RF which resulted not very good results.\nUse of Under Sample resulted bad results and use of Over Sample was better.\nUse of select K best (with ranges of k values to find the best), and use of select K best combined with Over Sample.\n\nFinally, I used cross validation to validate the highest approach results \n(The best accuracy approach: RF with selectKbest of k=25).\n","686aa6d9":"# Imports <a class=\"anchor\" id=\"imports\"><\/a>","1627ac27":" [go back up](#UP)\n","05117a97":"# Preprocess data: <a class=\"anchor\" id=\"prepare\"><\/a>","30358079":"I tried using Balanced DT and Balanced RF, which weren't very successful. <a class=\"anchor\" id=\"modelsBalanced\"><\/a>"}}