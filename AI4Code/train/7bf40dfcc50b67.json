{"cell_type":{"fa8e0c06":"code","d052ee01":"code","7eb1be29":"code","6679c790":"code","96f8dab7":"code","3d5795d1":"code","f43b0c92":"code","08672ac4":"code","de04b019":"code","9c001d95":"code","53d55735":"code","8202d9ac":"code","34067356":"code","220a41be":"code","4681c8bd":"code","988ef7a7":"code","73eb7612":"code","f6338d92":"code","ab8dc805":"code","f05cfbf5":"code","ee89dd5a":"code","3e53d7d8":"code","1d95e9f0":"code","5d7bb928":"code","75e5a5c7":"code","edd33ccc":"code","bee594ba":"code","e910d309":"code","b85b754e":"code","d7d03ce8":"code","64c9a09c":"code","1520b565":"code","ede8c348":"code","54f9c8fd":"code","09582417":"code","f7850aed":"code","812777a3":"code","f6e92724":"markdown","6d927db6":"markdown","092a4e0e":"markdown","2b632827":"markdown","c0d803b7":"markdown","5da51574":"markdown","bf1d5c81":"markdown","4d7f3f1d":"markdown","6c6551b7":"markdown","a1eb0916":"markdown"},"source":{"fa8e0c06":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport lightgbm as lgbm\nimport matplotlib.pyplot as plt\nimport shap\nimport datatable as dt\nfrom sklearn.metrics import mean_squared_error\nfrom hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt.pyll.base import scope\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d052ee01":"!rm -r \/opt\/conda\/lib\/python3.6\/site-packages\/lightgbm\n!git clone --recursive https:\/\/github.com\/Microsoft\/LightGBM\n!apt-get install -y -qq libboost-all-dev","7eb1be29":"%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=\/usr\/local\/cuda\/lib64\/libOpenCL.so -DOpenCL_INCLUDE_DIR=\/usr\/local\/cuda\/include\/ ..\nmake -j$(nproc)","6679c790":"!cd LightGBM\/python-package\/;python3 setup.py install --precompile\n!mkdir -p \/etc\/OpenCL\/vendors && echo \"libnvidia-opencl.so.1\" > \/etc\/OpenCL\/vendors\/nvidia.icd\n!rm -r LightGBM","96f8dab7":"import lightgbm as lgb","3d5795d1":"df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-feb-2021\/train.csv')","f43b0c92":"df.shape","08672ac4":"df.isnull().sum()","de04b019":"sns.histplot(df['target'])","9c001d95":"continuous = [x for x in df.columns if 'cont' in x]\ncat = [x for x in df.columns if 'cat' in x]\nfeatures = [x for x in df.columns if 'target' not in x]","53d55735":"fig, axs = plt.subplots(3, 4,figsize=(20,10))\ni, j = 0, 0\nfor c in cat:\n    sns.histplot(df[c],ax=axs[i, j])\n    i+=1\n    if i ==3:\n        i=0\n        j+=1","8202d9ac":"fig, axs = plt.subplots(3, 4,figsize=(20,10))\ni, j = 0, 0\nfor c in cat:\n    sns.violinplot(x=c, y='target',data=df,ax=axs[i, j])\n    i+=1\n    if i ==3:\n        i=0\n        j+=1","34067356":"fig, axs = plt.subplots(4, 4,figsize=(20,12))\ni, j = 0, 0\nfor c in continuous:\n    sns.histplot(df[c],ax=axs[i, j])\n    i+=1\n    if i ==4:\n        i=0\n        j+=1","220a41be":"fig, axs = plt.subplots(4, 4,figsize=(20,10))\ni, j = 0, 0\nfor c in continuous:\n    sns.scatterplot(x=c, y='target',data=df,ax=axs[i, j])\n    i+=1\n    if i ==4:\n        i=0\n        j+=1","4681c8bd":"from itertools import combinations","988ef7a7":"#Create a pairwise combination of all continuos variables (since they're only two this is pretty much OK)\ncont_comb = [x for x in combinations(continuous, 2)]","73eb7612":"gen_feat = []\nfor c in cont_comb:\n    df['{} mean {}'.format(c[0], c[1])] = (df[c[0]] + df[c[1]])\/2 #mean\n    df['{} by {}'.format(c[0], c[1])] = (df[c[0]] +0.001) \/ (df[c[1]]+0.001 ) #Dividing\n    \n    gen_feat.append('{} mean {}'.format(c[0], c[1]))\n    gen_feat.append('{} by {}'.format(c[0], c[1]))","f6338d92":"df.shape","ab8dc805":"def OHE(train, test, cat=None):\n    # This functions returns a ONE HOT ENCODE from the train set, then the test set's OHE to have the same columns\n    # Test's extra categories are ignored, missing categories are added anyway with a column of 0 in the test set\n \n    train = pd.get_dummies(train, cat) \n    test = pd.get_dummies(test, cat) \n    \n    return train.align(test, join='left', axis=1)","f05cfbf5":"features = features + gen_feat","ee89dd5a":"X = df[features]\ny = df['target']\nX = pd.get_dummies(X, cat) ","3e53d7d8":"kf = KFold(n_splits=5)","1d95e9f0":"def optimise(params):\n    \n    print(params)\n    p = {'learning_rate': params['learning_rate'],\n         'max_depth': params['max_depth'], \n         'gamma': params['gamma'], \n         'min_child_weight': params['min_child_weight'], \n         'subsample': params['subsample'], \n         'colsample_bytree': params['colsample_bytree'], \n         'verbosity': 0, \n         'objective': 'reg:squarederror',\n         'eval_metric': 'rmse', \n         'tree_method': 'gpu_hist', #MAKE SURE TO HAVE YOUR GPU ON, otherwise just remove this line\n         'random_state': 42,\n        }\n    \n    score_te=[]\n    \n    for tr, te in kf.split(X):\n        X_tr, X_te = X.loc[tr, :].values, X.loc[te, :].values\n        y_tr, y_te = y.loc[tr].values, y.loc[te].values\n        \n        d_tr = xgb.DMatrix(X_tr, y_tr)\n        d_val = xgb.DMatrix(X_te, y_te)\n        \n        clf = xgb.train(p, d_tr, params['n_round'], verbose_eval = False)\n        val_pred = clf.predict(d_val)\n        \n        score_te.append(mean_squared_error(y_te, val_pred))\n\n    return np.mean(score_te)\n\nparam_space = {'learning_rate': hp.uniform('learning_rate', 0.01, 0.3), \n               'max_depth': scope.int(hp.quniform('max_depth', 3, 8, 1)), \n               'gamma': hp.uniform('gamma', 0, 10), \n               'min_child_weight': hp.uniform('min_child_weight', 0, 10),\n               'lambda': hp.uniform('lambda', 0, 10),\n               'subsample': hp.uniform('subsample', 0.1, 1), \n               'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 0.8), \n               'n_round': scope.int(hp.quniform('n_round', 50, 150, 25))\n              }\n\ntrials = Trials()\n\nhopt = fmin(fn = optimise, \n            space = param_space, \n            algo = tpe.suggest, \n            max_evals = 60, \n            trials = trials, \n           )\nprint(hopt)","5d7bb928":"print(hopt)","75e5a5c7":"#{'colsample_bytree': 0.5873216071695012, 'gamma': 5.177112055270119, 'lambda': 7.735558992779168, 'learning_rate': 0.24208100159213855, 'max_depth': 3.0, 'min_child_weight': 2.1053503825354976, 'n_round': 150.0, 'subsample': 0.8655686441807873}","edd33ccc":"results={}\nparams = hopt\nparams['max_depth'] = int(params['max_depth'])\nparams['n_round'] = int(params['n_round'])\nparams['objective'] = 'reg:squarederror'\nparams['eval_metric'] = 'rmse'\nparams['tree_method'] = 'gpu_hist' #MAKE SURE TO HAVE YOUR GPU ON, otherwise just remove this line\n\nxgtr = xgb.DMatrix(X, y)\n\nclf = xgb.train(params, xgtr, params['n_round'], evals=[(xgtr, 'train')], evals_result=results, verbose_eval = 50)","bee594ba":"df['predicted'] = clf.predict(xgtr)\ndf['residuals'] = df['target'] - df['predicted']","e910d309":"plt.figure(figsize=(12,6))\nsns.scatterplot(x='target',y='residuals',data=df)","b85b754e":"plt.figure(figsize=(12,6))\nsns.scatterplot(x='target',y='predicted',data=df)","d7d03ce8":"def optimise(params):\n    \n    print(params)\n    p = {'learning_rate': params['learning_rate'],\n         'min_data_in_leaf': params['min_data_in_leaf'],\n         'num_leaves': params['num_leaves'],\n         'max_depth': params['max_depth'], \n\n         'bagging_freq': params['bagging_freq'], \n         'feature_fraction': params['feature_fraction'],\n         'lambda_l2': params['lambda_l2'],\n         'verbosity': 0, \n         'objective': 'regression',\n         'metric': 'l2', \n         'device': 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0, #MAKE SURE TO HAVE YOUR GPU ON, otherwise just remove this line\n         'random_state': 42\n        }\n    \n    score_te=[]\n    \n    for tr, te in kf.split(X):\n        X_tr, X_te = X.loc[tr, :].values, X.loc[te, :].values\n        y_tr, y_te = y.loc[tr].values, y.loc[te].values\n        d_tr = lgb.Dataset(X_tr,\n                           y_tr,\n                           feature_name=list(X.columns),\n                           free_raw_data = False\n                           )\n        \n        d_val = lgb.Dataset(X_te,\n                           label=y_te,\n                           feature_name=list(X.columns),\n                           free_raw_data = False\n                           )   \n        \n        clf = lgb.train(p, d_tr, params['num_iterations'], verbose_eval = False)\n        val_pred = clf.predict(X_te)\n        \n        score_te.append(mean_squared_error(y_te, val_pred))\n\n    return np.mean(score_te)\n\nparam_space = {\n    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n         'min_data_in_leaf': scope.int(hp.quniform('min_data_in_leaf', 10, 200, 20)),\n         'num_leaves': scope.int(hp.quniform('num_leaves', 10, 50, 2)),\n         'max_depth': scope.int(hp.quniform('max_depth', 3, 8, 1)), \n\n         'bagging_freq':scope.int(hp.uniform('bagging_freq', 0.3*len(features), 1*len(features))), \n         'feature_fraction': hp.uniform('feature_fraction', 0.3, 1),\n         'lambda_l2': hp.uniform('lambda_l2', 0.1, 10),\n         'num_iterations': scope.int(hp.quniform('num_iterations', 50, 200, 25))\n        }\n\ntrials = Trials()\n\nhopt = fmin(fn = optimise, \n            space = param_space, \n            algo = tpe.suggest, \n            max_evals = 60, \n            trials = trials, \n           )\nprint(hopt)","64c9a09c":"print(hopt)\n#{'bagging_freq': 95.11853984706136, 'feature_fraction': 0.34260532690897055, 'lambda_l2': 4.15465950834321, 'learning_rate': 0.10558892297096945, 'max_depth': 6.0, 'min_data_in_leaf': 140.0, 'num_iterations': 175.0, 'num_leaves': 28.0, 'verbosity': (0,), 'objective': ('regression',), 'device': ('gpu',), 'gpu_platform_id': (0,), 'gpu_device_id': (0,), 'random_state': 42}","1520b565":"results={}\nparams = hopt\nparams['verbosity'] = 0, \nparams['objective'] = 'regression'\nparams['num_iterations']=int(params['num_iterations'])\nparams['num_leaves']=int(params['num_leaves'])\nparams['max_depth']=int(params['max_depth'])\nparams['min_data_in_leaf']=int(params['min_data_in_leaf'])\nparams['device'] = 'gpu',\nparams['bagging_freq']=int(params['bagging_freq'])\n\nparams['gpu_platform_id'] = 0\nparams['gpu_device_id'] = 0\nparams['random_state'] = 42\n\nd_tr = lgb.Dataset(X,\n                           y,\n                           feature_name=list(X.columns),\n                           free_raw_data = False\n                           )\nclf = lgb.train(params, d_tr, int(params['num_iterations']), verbose_eval = False)\n\nprint(mean_squared_error(y, clf.predict(X)))","ede8c348":"df['predicted'] = clf.predict(X)\ndf['residuals'] = df['target'] - df['predicted']","54f9c8fd":"plt.figure(figsize=(12,6))\nsns.scatterplot(x='target',y='residuals',data=df)","09582417":"plt.figure(figsize=(12,6))\nsns.scatterplot(x='target',y='predicted',data=df)","f7850aed":"submission = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/test.csv\")\n\nfor c in cont_comb:\n    test['{} mean {}'.format(c[0], c[1])] = (test[c[0]] + test[c[1]])\/2 #Summing\n    test['{} by {}'.format(c[0], c[1])] = (test[c[0]] +0.001) \/ (test[c[1]]+0.001 ) #Dividing\n\nX_tr, X_sub = OHE(df[features], test[features], cat)\n\npred = clf.predict(X_sub)\nsubmission[\"target\"] = pred\nsubmission.to_csv(\"submission.csv\", index=False)","812777a3":"sns.histplot(submission[\"target\"]);","f6e92724":"### Converting to a matrix","6d927db6":"# Feature engineering\n\nLet's try to create some relation with our continuos variables","092a4e0e":"## Now let's explore target variable and our features","2b632827":"## Sampling\nInstead of train\/test we'll use CV","c0d803b7":"We won't need to think in a strategy to fill null values","5da51574":"# Basic Starter\n\nThis notebook is aimed to be an initial modeling of the problem, you will find:\n1. A basic EDA, with no in-depth exploring, using mainly SNS and PLT\n2. Building a XGB model\n3. Tuning its parameters with HOPT\n4. Evaluation regression results","bf1d5c81":"## Checking for nulls","4d7f3f1d":"# Let's Model\nFirst we'll need to sample our dataframe into train and test, since there's no information about time, we can simply randomly sample","6c6551b7":"### Optimizing with hyperOPT\n   It's pretty straight forward: \n   1. Define the function, which receives the parameters to be optimized and returns the metric that we want to minimize (in this example I defined RMSE)\n   2. Initiate the \"search space\" (which is the combination of our parameters and their range to be searched, I would advise to google and read the documentation of this functions)\n   3. Initiate a Trials object\n   4. Define fmin","a1eb0916":"## Summarizing EDA:\n* By LOOKING at the target distribution we can guess that it might be a bi-modal distribution centered around 6 and 8, this information might be usefull for optimizing our solution later.\n* Some categorical features has major unbalance problems, however when we look at the violin plot we see that this unbalance has little influence in the target data. Some categories has more targets around the value 6 or 8 exclusively, but the majority seems to not have any relation with target\n* The continuous features present multi-modal distributions, and they dont see to correlate with our target."}}