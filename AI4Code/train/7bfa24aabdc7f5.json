{"cell_type":{"73a9c85d":"code","9d501ac2":"code","eb34b0c9":"code","cbabe304":"code","a4e0ebc9":"code","39f74ed6":"code","b42a6c94":"code","4f634702":"code","37a6d4f7":"code","6fac39f1":"code","8f13d595":"code","da0b4f11":"code","8e277dec":"code","0c4cd215":"code","46926de4":"code","e0f95c4e":"code","fe52d4e6":"code","b5fc69b1":"code","e10e4181":"code","aa111273":"code","0abfe0f5":"code","007f9ba5":"code","ea0279ef":"code","35cd7991":"code","4f638d43":"code","06287240":"code","8dee3c32":"code","685b0a3e":"code","5ce74a8d":"code","d3f0d38f":"code","90932ad9":"code","bcc0bc02":"code","ba7d98ba":"code","9725160f":"code","5991d305":"code","3b6221ea":"code","ccf72fc2":"markdown","7f2f9010":"markdown","b3ad921c":"markdown","58972a9d":"markdown","dbd8146b":"markdown","13502119":"markdown","14cd2a9b":"markdown","93af06ee":"markdown","8cc2fe05":"markdown","445d0958":"markdown","472e6a56":"markdown","6d462ae1":"markdown","86b1a4ab":"markdown","73d28d3d":"markdown","54308f1e":"markdown"},"source":{"73a9c85d":"! pip install -q polyglot\n! pip install -q pycld2\n! pip install -q pyicu","9d501ac2":"import pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom polyglot.detect import Detector\nimport icu\nfrom string import punctuation\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom xgboost import XGBClassifier\n\nimport warnings\n\n\nwarnings.filterwarnings(\"ignore\")\nnltk.download('popular')","eb34b0c9":"df = pd.read_csv('..\/input\/quotes-from-goodread\/all_quotes.csv')\ndf.sample(5)","cbabe304":"df.info()","a4e0ebc9":"df.isnull().sum()","39f74ed6":"df['Main Tag'].value_counts()","b42a6c94":"df['Author'].value_counts().head(10).plot(kind='bar')","4f634702":"df['Quote'] = df['Quote'].apply(lambda x: re.sub(\"[\\\u201c\\\u201d]\", \"\", x))\ndf['Other Tags'] = df['Other Tags'].apply(lambda x: re.sub(\"[\\'\\[\\]]\", \"\", x))","37a6d4f7":"# Count Quote Word Freq\nquote_count = pd.Series(' '.join(df['Quote']).split()).value_counts()\nquote_count.head(10).plot(kind='bar')","6fac39f1":"# Count Other Tag Word Freq\nothertag_count = pd.Series(' '.join(df['Other Tags']).split(',')).value_counts()\nothertag_count.head(10).plot(kind='bar')","8f13d595":"# Count Number of Words\ndf['word_count'] = df['Quote'].apply(lambda x: len(str(x).split(\" \")))\ndf.sort_values('word_count', ascending=False).head()","da0b4f11":"# Count Number Of Character\ndf['char_count'] = df['Quote'].str.len() ## this also includes spaces\ndf.sort_values('char_count', ascending=False).head(5)","8e277dec":"# Count of Duplicated quote\ndf[df.duplicated('Quote')==True].groupby('Main Tag')['Quote'].count()","0c4cd215":"# Detect Text Language\nlangs = []\nfor text in df['Quote']:\n    try:\n        lang = Detector(text).language.code\n        langs.append(lang)\n    except:\n        lang = 'NaN'\n        langs.append(lang)\ndf['lang'] = langs","46926de4":"df['lang'].value_counts().head(10)","e0f95c4e":"# I will only use Eng Quote\ndf_eng = df[df['lang']=='en']\nprint(df_eng.shape)\ndf_eng['Main Tag'].value_counts()","fe52d4e6":"df_eng['CleanQuote'] = df_eng['Quote'].apply(lambda x: x.lower())\ndf_eng['CleanQuote'].sample(2)","b5fc69b1":"stop_nltk = stopwords.words(\"english\")\n\ndef drop_stop(input_tokens):\n    rempunc = re.sub(r'[^\\w\\s]','',input_tokens)\n    remstopword = \" \".join([word for word in str(rempunc).split() if word not in stop_nltk])\n    return remstopword\n\ndf_eng['CleanQuote'] = df_eng['CleanQuote'].apply(lambda x: drop_stop(x))\ndf_eng['CleanQuote'].sample(2)","e10e4181":"lemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\ndf_eng[\"CleanQuote\"] = df_eng[\"CleanQuote\"].apply(lambda x: lemmatize_words(x))\ndf_eng['CleanQuote'].sample(2)","aa111273":"X = df_eng['CleanQuote']\nY = df_eng['Main Tag']","0abfe0f5":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=48, stratify=Y)","007f9ba5":"print(f\"X_train : {X_train.shape}\\nX_test : {X_test.shape}\")","ea0279ef":"count_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(X_train)\n\ntfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)","35cd7991":"NB = MultinomialNB().fit(X_train_tfidf, Y_train)","4f638d43":"pipeline_nb = Pipeline([('vect', CountVectorizer()),\n                     ('tfidf', TfidfTransformer()),\n                     ('NB', MultinomialNB()),\n                    ])\n\nmodel_nb = pipeline_nb.fit(X_train, Y_train)","06287240":"predict_nb = model_nb.predict(X_test)","8dee3c32":"print(classification_report(predict_nb, Y_test))","685b0a3e":"pipeline_sgd = Pipeline([('vect', CountVectorizer()),\n                         ('tfidf', TfidfTransformer()),\n                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n                          alpha=1e-3, random_state=42)),\n                        ])\n\nmodel_sgd = pipeline_sgd.fit(X_train, Y_train)","5ce74a8d":"predict_sgd = model_sgd.predict(X_test)","d3f0d38f":"print(classification_report(predict_sgd, Y_test))","90932ad9":"pipeline_rf = Pipeline([('vect', CountVectorizer()),\n                         ('tfidf', TfidfTransformer()),\n                         ('rf', RandomForestClassifier()),\n                        ])\n\nmodel_rf = pipeline_rf.fit(X_train, Y_train)","bcc0bc02":"predict_rf = model_rf.predict(X_test)","ba7d98ba":"print(classification_report(predict_rf, Y_test))","9725160f":"pipeline_xgb = Pipeline([('vect', CountVectorizer()),\n                         ('tfidf', TfidfTransformer()),\n                         ('xgb', XGBClassifier()),\n                        ])\n\nmodel_xgb = pipeline_xgb.fit(X_train, Y_train)","5991d305":"predict_xgb = model_xgb.predict(X_test)","3b6221ea":"print(classification_report(predict_xgb, Y_test))","ccf72fc2":"## 2. Create SGDClassifier Pipeline","7f2f9010":"## Remove Punc & Stopword","b3ad921c":"# Preprocessing","58972a9d":"## Split Data to Train and Test","dbd8146b":"## 3. Create Random Forest Classifier Pipeline","13502119":"# Load Data","14cd2a9b":"## 4. Create XGBoost Classifier Pipeline","93af06ee":"# Modelling","8cc2fe05":"# Importing Library","445d0958":"## Lower Text","472e6a56":"## Lemmatization","6d462ae1":"# EDA","86b1a4ab":"## 1. Create Naive Baiyes Pipeline","73d28d3d":"## Specify Feature and Target","54308f1e":"## Extracting Feature"}}