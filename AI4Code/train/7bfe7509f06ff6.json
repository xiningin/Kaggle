{"cell_type":{"64d96be3":"code","c8d4a271":"code","128802f1":"code","7b4c25b9":"code","22029e8a":"code","dc3bd790":"code","416345ed":"code","4ed0e9e5":"code","bd6566f5":"code","997b70bd":"code","d04245e2":"code","9c793796":"code","69eabedf":"code","30f76c91":"code","09f8a677":"code","7b607f4f":"code","3b0bfef2":"code","c8406efc":"code","9efd305d":"code","0b071632":"code","2871b854":"code","c5b0d65c":"code","a2278b9d":"code","66ce06fe":"code","8268febc":"code","687c6d3c":"code","baec7fb1":"code","4ed83de9":"code","aa2b1013":"code","5cc6f977":"code","3d55a141":"code","6d589790":"code","dc60fb72":"code","c5ab72ac":"code","e2a88e6e":"code","230a0098":"code","521b7fb4":"code","a1a9fe97":"code","10de768f":"code","cc657f26":"code","c6e0c077":"code","1d92091b":"code","6e5d0a2c":"code","2c80536f":"code","560bcb97":"markdown","ad7e1f03":"markdown","a61094c2":"markdown","873fa417":"markdown","2710d2b9":"markdown","045b7743":"markdown","6147e871":"markdown","8177cb27":"markdown","df85cd67":"markdown","ec95393d":"markdown","416661dd":"markdown","555ca825":"markdown","afda5181":"markdown","baa7863b":"markdown","51246edc":"markdown","a95d9cb5":"markdown","c25e5b0b":"markdown","6127a8bf":"markdown","d8172e8c":"markdown","affa8b88":"markdown","f70bd4d2":"markdown","5a953a06":"markdown","3c158a8b":"markdown","08a8d373":"markdown","8b7e4971":"markdown","1b2ffbdc":"markdown","4b9a7418":"markdown","262b6749":"markdown","f4c90b53":"markdown","75837ab5":"markdown","1cd93800":"markdown","fa8c2d10":"markdown","3165c521":"markdown","ac18d4ac":"markdown"},"source":{"64d96be3":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom time import time,sleep\n\n\nimport nltk\nfrom nltk import tokenize\nfrom string import punctuation\nfrom nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\nfrom unidecode import unidecode\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score,f1_score \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate,KFold,GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import OrdinalEncoder,OneHotEncoder, LabelEncoder\nfrom sklearn.preprocessing import StandardScaler,Normalizer\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom scipy.stats import randint\nfrom numpy.random import uniform","c8d4a271":"# pandas options\npd.options.display.max_columns = 30\npd.options.display.float_format = '{:.2f}'.format\n\n# seaborn options\nsns.set(style=\"darkgrid\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42","128802f1":"def treat_words(df,\n      col,\n      language='english',\n      inplace=False,\n      tokenizer = tokenize.WordPunctTokenizer(),\n      decode = True,\n      stemmer = None,\n      lower = True,\n      remove_words = [],\n\n    ):\n    \"\"\"\n    Description:\n    ----------------\n        Receives a dataframe and the column name. Eliminates\n        stopwords for each row of that column and apply stemmer.\n        After that, it regroups and returns a list.\n        \n        tokenizer = tokenize.WordPunctTokenizer()\n                    tokenize.WhitespaceTokenizer()\n                    \n        stemmer =    PorterStemmer()\n                     SnowballStemmer()\n                     LancasterStemmer()\n                     nltk.RSLPStemmer() # in portuguese\n    \"\"\"\n    \n    \n    pnct = [string for string in punctuation] # from string import punctuation \n    wrds = nltk.corpus.stopwords.words(language)\n    unwanted_words = pnct + wrds + remove_words\n\n    processed_text = list()\n\n    for element in tqdm(df[col]):\n\n        # starts a new list\n        new_text = list()\n\n        # starts a list with the words of the non precessed text\n        text_old = tokenizer.tokenize(element)\n\n        # check each word\n        for wrd in text_old:\n\n            # if the word are not in the unwanted words list\n            # add to the new list\n            if wrd.lower() not in unwanted_words:\n                \n                new_wrd = wrd\n                \n                if decode: new_wrd = unidecode(new_wrd)\n                if stemmer: new_wrd = stemmer.stem(new_wrd)\n                if lower: new_wrd = new_wrd.lower()\n                    \n                if new_wrd not in remove_words:\n                    new_text.append(new_wrd)\n\n        processed_text.append(' '.join(new_text))\n\n    if inplace:\n        df[col] = processed_text\n    else:\n        return processed_text\n","7b4c25b9":"def list_words_of_class(df,\n                          col,\n                          language='english',\n                          inplace=False,\n                          tokenizer = tokenize.WordPunctTokenizer(),\n                          decode = True,\n                          stemmer = None,\n                          lower = True,\n                          remove_words = []\n                         ):\n    \"\"\"\n    Description:\n    ----------------\n    \n        Receives a dataframe and the column name. Eliminates\n        stopwords for each row of that column, apply stemmer\n        and returns a list of all the words.\n    \n    \"\"\"\n    \n    lista = treat_words(\n        df,col = col,language = language,\n        tokenizer=tokenizer,decode=decode,\n        stemmer=stemmer,lower=lower,\n        remove_words = remove_words\n        )\n    \n    words_list = []\n    for string in lista:\n        words_list += tokenizer.tokenize(string)\n        \n        \n    return words_list","22029e8a":"def get_frequency(df,\n                  col,\n                  language='english',\n                  inplace=False,\n                  tokenizer = tokenize.WordPunctTokenizer(),\n                  decode = True,\n                  stemmer = None,\n                  lower = True,\n                  remove_words = []\n                 ):\n    \n    list_of_words = list_words_of_class(\n              df,\n              col = col,\n              decode = decode,\n              stemmer = stemmer,\n              lower = lower,\n              remove_words = remove_words\n      )\n    \n    freq = nltk.FreqDist(list_of_words)\n    \n    df_freq = pd.DataFrame({\n        'word': list(freq.keys()),\n        'frequency': list(freq.values())\n    }).sort_values(by='frequency',ascending=False)\n\n    n_words = df_freq['frequency'].sum()\n\n    df_freq['prop'] = 100*df_freq['frequency']\/n_words\n\n    return df_freq","dc3bd790":"def common_best_words(df,col,n_common = 10,tol_frac = 0.8,n_jobs = 1):\n    list_to_remove = []\n\n    for i in range(0,n_jobs):\n        print('[info] Most common words in not survived')\n        sleep(0.5)\n        df_dead = get_frequency(\n                  df.query('Survived == 0'),\n                  col = col,\n                  decode = False,\n                  stemmer = False,\n                  lower = False,\n                  remove_words = list_to_remove )\n\n        print('[info] Most common words in survived')\n        sleep(0.5)\n        df_surv = get_frequency(\n                  df.query('Survived == 1'),\n                  col = col,\n                  decode = False,\n                  stemmer = False,\n                  lower = False,\n                  remove_words = list_to_remove )\n\n\n        words_dead = df_dead.nlargest(n_common, 'frequency')\n\n        list_dead = list(words_dead['word'].values)\n\n        words_surv = df_surv.nlargest(n_common, 'frequency')\n\n        list_surv = list(words_surv['word'].values)\n\n        for word in list(set(list_dead).intersection(list_surv)):\n            prop_dead = words_dead[words_dead['word'] == word]['prop'].values[0]\n            prop_surv = words_surv[words_surv['word'] == word]['prop'].values[0]\n            ratio = min([prop_dead,prop_surv])\/max([prop_dead,prop_surv])\n\n            if ratio > tol_frac:\n                list_to_remove.append(word)\n        \n        return list_to_remove","416345ed":"def just_keep_the_words(df,\n      col,\n      keep_words = [],\n      tokenizer = tokenize.WordPunctTokenizer()\n    ):\n    \"\"\"\n    Description:\n    ----------------\n        Removes all words that is not in `keep_words`\n    \"\"\"\n    \n    processed_text = list()\n\n    # para cada avalia\u00e7\u00e3o\n    for element in tqdm(df[col]):\n\n        # starts a new list\n        new_text = list()\n\n        # starts a list with the words of the non precessed text\n        text_old = tokenizer.tokenize(element)\n\n        for wrd in text_old:\n\n            if wrd in keep_words: new_text.append(wrd)\n\n        processed_text.append(' '.join(new_text))\n\n    return processed_text\n","4ed0e9e5":"class Classifier:\n    '''\n    Description\n    -----------------\n    \n    Class to approach classification algorithm\n    \n    \n    Example\n    -----------------\n        classifier = Classifier(\n                 algorithm = ChooseTheAlgorith,\n                 hyperparameters_range = {\n                    'hyperparameter_1': [1,2,3],\n                    'hyperparameter_2': [4,5,6],\n                    'hyperparameter_3': [7,8,9]\n                 }\n             )\n\n        # Looking for best model\n        classifier.grid_search_fit(X,y,n_splits=10)\n        #dt.grid_search_results.head(3)\n\n        # Prediction Form 1\n        par = classifier.best_model_params\n        dt.fit(X_trn,y_trn,params = par)\n        y_pred = classifier.predict(X_tst)\n        print(accuracy_score(y_tst, y_pred))\n\n        # Prediction Form 2\n        classifier.fit(X_trn,y_trn,params = 'best_model')\n        y_pred = classifier.predict(X_tst)\n        print(accuracy_score(y_tst, y_pred))\n\n        # Prediction Form 3\n        classifier.fit(X_trn,y_trn,min_samples_split = 5,max_depth=4)\n        y_pred = classifier.predict(X_tst)\n        print(accuracy_score(y_tst, y_pred))\n    '''\n    def __init__(self,algorithm, hyperparameters_range={},random_state=42):\n        \n        self.algorithm = algorithm\n        self.hyperparameters_range = hyperparameters_range\n        self.random_state = random_state\n        self.grid_search_cv = None\n        self.grid_search_results = None\n        self.hyperparameters = self.__get_hyperparameters()\n        self.best_model = None\n        self.best_model_params = None\n        self.fitted_model = None\n        \n    def grid_search_fit(self,X,y,verbose=0,n_splits=10,shuffle=True,scoring='accuracy'):\n        \n        self.grid_search_cv = GridSearchCV(\n            self.algorithm(),\n            self.hyperparameters_range,\n            cv = KFold(n_splits = n_splits, shuffle=shuffle, random_state=self.random_state),\n            scoring=scoring,\n            verbose=verbose\n        )\n        \n        self.grid_search_cv.fit(X, y)\n        \n        col = list(map(lambda par: 'param_'+str(par),self.hyperparameters))+[\n                'mean_fit_time',\n                'mean_test_score',\n                'std_test_score',\n                'params'\n              ]\n        \n        results = pd.DataFrame(self.grid_search_cv.cv_results_)\n        \n        self.grid_search_results = results[col].sort_values(\n                    ['mean_test_score','mean_fit_time'],\n                    ascending=[False,True]\n                ).reset_index(drop=True)\n        \n        self.best_model = self.grid_search_cv.best_estimator_\n        \n        self.best_model_params = self.best_model.get_params()\n    \n    def best_model_cv_score(self,X,y,parameter='test_score',verbose=0,n_splits=10,shuffle=True,scoring='accuracy'):\n        if self.best_model != None:\n            cv_results = cross_validate(\n                self.best_model,\n                X = X,\n                y = y,\n                cv=KFold(n_splits = 10,shuffle=True,random_state=self.random_state)\n            )\n            return {\n                parameter+'_mean': cv_results[parameter].mean(),\n                parameter+'_std': cv_results[parameter].std()\n            }\n        \n    def fit(self,X,y,params=None,**kwargs):\n        model = None\n        if len(kwargs) == 0 and params == 'best_model' and self.best_model != None:\n            model = self.best_model\n            \n        elif type(params) == dict and len(params) > 0:\n            model = self.algorithm(**params)\n            \n        elif len(kwargs) >= 0 and params==None:\n            model = self.algorithm(**kwargs)\n            \n        else:\n            print('[Error]')\n            \n        if model != None:\n            model.fit(X,y)\n            \n        self.fitted_model = model\n            \n    def predict(self,X):\n        if self.fitted_model != None:\n            return self.fitted_model.predict(X)\n        else:\n            print('[Error]')\n            return np.array([])\n            \n    def predict_score(self,X_tst,y_tst,score=accuracy_score):\n        if self.fitted_model != None:\n            y_pred = self.predict(X_tst)\n            return score(y_tst, y_pred)\n        else:\n            print('[Error]')\n            return np.array([])\n        \n    def hyperparameter_info(self,hyperpar):\n        str_ = 'param_'+hyperpar\n        return self.grid_search_results[\n                [str_,'mean_fit_time','mean_test_score']\n            ].groupby(str_).agg(['mean','std'])\n        \n    def __get_hyperparameters(self):\n        return [hp for hp in self.hyperparameters_range]","bd6566f5":"def cont_class_limits(lis_df,n_class):\n    ampl = lis_df.quantile(1.0)-lis_df.quantile(0.0)\n    ampl_class = ampl\/n_class \n    limits = [[i*ampl_class,(i+1)*ampl_class] for i in range(n_class)]\n    return limits\n\ndef cont_classification(lis_df,limits):\n    list_res = []\n    n_class = len(limits)\n    for elem in lis_df:\n        for ind in range(n_class-1):\n            if elem >= limits[ind][0] and elem < limits[ind][1]:\n                list_res.append(ind+1)\n            \n        if elem >= limits[-1][0]: list_res.append(n_class)\n            \n    return list_res\n","997b70bd":"df_trn = pd.read_csv('data\/train.csv')\ndf_tst = pd.read_csv('data\/test.csv')\n\ndf = pd.concat([df_trn,df_tst])\n\ndf_trn = df_trn.drop(columns=['PassengerId'])\ndf_tst = df_tst.drop(columns=['PassengerId'])","d04245e2":"df_tst.info()","9c793796":"sns.barplot(x='Pclass', y=\"Survived\", data=df_trn)","69eabedf":"treat_words(df_trn,col = 'Name',inplace=True)\ntreat_words(df_tst,col = 'Name',inplace=True)","30f76c91":"%matplotlib inline\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nall_words = ' '.join(list(df_trn['Name']))\n\nword_cloud = WordCloud().generate(all_words)\nplt.figure(figsize=(10,7))\nplt.imshow(word_cloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","09f8a677":"common_best_words(df_trn,col='Name',n_common = 10,tol_frac = 0.5,n_jobs = 1)","7b607f4f":"df_comm = get_frequency(df_trn,col = 'Name',remove_words=['(\"','\")','master', 'william']).reset_index(drop=True)\nsurv_prob = [ df_trn['Survived'][df_trn['Name'].str.contains(row['word'])].mean() for index, row in df_comm.iterrows()]\ndf_comm['survival_prob (%)'] = 100*np.array(surv_prob)\nprint('Survival Frequency related to words in Name')\ndf_comm.head(10)","3b0bfef2":"df_comm_surv = get_frequency(df_trn[df_trn['Survived']==1],col = 'Name',remove_words=['(\"','\")']).reset_index(drop=True)\nsleep(0.5)\nprint('Most frequent words within those who survived')\ndf_comm_surv.head(10)","c8406efc":"df_comm_dead = get_frequency(df_trn[df_trn['Survived']==0],col = 'Name',remove_words=['(\"','\")']).reset_index(drop=True)\nsleep(0.5)\nprint(\"Most frequent words within those that did not survive\")\ndf_comm_dead.head(10)","9efd305d":"min_occurrences = 2\ndf_comm = get_frequency(df,col = 'Name',\n                        remove_words=['(\"','\")','john','henry', 'william','h','j','jr']\n                       ).reset_index(drop=True)\nwords_to_keep = list(df_comm[df_comm['frequency'] > min_occurrences]['word'])\n\ndf_trn['Name'] = just_keep_the_words(df_trn,\n                    col = 'Name',\n                    keep_words = words_to_keep \n                   )\n\ndf_tst['Name'] = just_keep_the_words(df_tst,\n                    col = 'Name',\n                    keep_words = words_to_keep \n                   )","0b071632":"vectorize = CountVectorizer(lowercase=True,max_features = 4)\nvectorize.fit(df_trn['Name'])\nbag_of_words = vectorize.transform(df_trn['Name'])\n\nX = pd.DataFrame(vectorize.fit_transform(df_trn['Name']).toarray(),\n             columns=list(map(lambda word: 'Name_'+word,vectorize.get_feature_names()))\n            )\ny = df_trn['Survived']\n\nfrom sklearn.model_selection import train_test_split\nX_trn,X_tst,y_trn,y_tst = train_test_split(\n    X,\n    y,\n    test_size = 0.25,\n    random_state=42\n)\n\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(C=100)\nclassifier.fit(X_trn,y_trn)\naccuracy = classifier.score(X_tst,y_tst)\nprint('Accuracy = %.3f%%' % (100*accuracy))","2871b854":"df_trn = pd.concat([\n            df_trn\n            ,\n            pd.DataFrame(vectorize.fit_transform(df_trn['Name']).toarray(),\n                  columns=list(map(lambda word: 'Name_'+word,vectorize.get_feature_names()))\n                )\n        ],axis=1).drop(columns=['Name'])\n\ndf_tst = pd.concat([\n            df_tst\n            ,\n            pd.DataFrame(vectorize.fit_transform(df_tst['Name']).toarray(),\n                  columns=list(map(lambda word: 'Name_'+word,vectorize.get_feature_names()))\n                )\n        ],axis=1).drop(columns=['Name'])","c5b0d65c":"from sklearn.preprocessing import LabelEncoder\nSex_Encoder = LabelEncoder()\n\ndf_trn['Sex'] = Sex_Encoder.fit_transform(df_trn['Sex']).astype(int)\ndf_tst['Sex'] = Sex_Encoder.transform(df_tst['Sex']).astype(int)","a2278b9d":"mean_age = df['Age'][df['Age'].notna()].mean()\ndf_trn['Age'].fillna(mean_age,inplace=True)\ndf_tst['Age'].fillna(mean_age,inplace=True)\nage_limits = cont_class_limits(df['Age'],3)\ndf_trn['Age'] = cont_classification(df_trn['Age'],age_limits)\ndf_tst['Age'] = cont_classification(df_tst['Age'],age_limits)","66ce06fe":"# df_trn['FamilySize'] = df_trn['SibSp'] + df_trn['Parch'] + 1\n# df_tst['FamilySize'] = df_tst['SibSp'] + df_tst['Parch'] + 1\n\n# df_trn = df_trn.drop(columns = ['SibSp','Parch'])\n# df_tst = df_tst.drop(columns = ['SibSp','Parch'])","8268febc":"# df_trn['Cabin'] = df_trn['Cabin'].fillna('N000')\n\n# df_cab = df_trn[df_trn['Cabin'].notna()]\n\n# df_cab = pd.concat(\n#     [\n#         df_cab,\n#         df_cab['Cabin'].str.extract(\n#             '([A-Za-z]+)(\\d+\\.?\\d*)([A-Za-z]*)', \n#             expand = True).drop(columns=[2]).rename(\n#             columns={0: 'Cabin_Class', 1: 'Cabin_Number'}\n#         )\n#     ], axis=1)\n\n# df_trn = df_cab.drop(columns=['Cabin','Cabin_Number'])\n# df_trn = pd.concat([\n#             df_trn.drop(columns=['Cabin_Class']),\n#             pd.get_dummies(df_trn['Cabin_Class'],prefix='Cabin').drop(columns=['Cabin_N'])\n# #             pd.get_dummies(df_trn['Cabin_Class'],prefix='Cabin')\n#         ],axis=1)\n\n\n# df_tst['Cabin'] = df_tst['Cabin'].fillna('N000')\n\n# df_cab = df_tst[df_tst['Cabin'].notna()]\n\n# df_cab = pd.concat(\n#     [\n#         df_cab,\n#         df_cab['Cabin'].str.extract(\n#             '([A-Za-z]+)(\\d+\\.?\\d*)([A-Za-z]*)', \n#             expand = True).drop(columns=[2]).rename(\n#             columns={0: 'Cabin_Class', 1: 'Cabin_Number'}\n#         )\n#     ], axis=1)\n\n# df_tst = df_cab.drop(columns=['Cabin','Cabin_Number'])\n# df_tst = pd.concat([\n#             df_tst.drop(columns=['Cabin_Class']),\n#             pd.get_dummies(df_tst['Cabin_Class'],prefix='Cabin').drop(columns=['Cabin_N'])\n# #             pd.get_dummies(df_tst['Cabin_Class'],prefix='Cabin')\n#         ],axis=1)","687c6d3c":"df_trn = df_trn.drop(columns=['Ticket'])\ndf_tst = df_tst.drop(columns=['Ticket'])","baec7fb1":"mean_fare = df['Fare'][df['Fare'].notna()].mean()\ndf_trn['Fare'].fillna(mean_fare,inplace=True)\ndf_tst['Fare'].fillna(mean_fare,inplace=True)\n\nfare_limits = cont_class_limits(df['Fare'],3)\ndf_trn['Fare'] = cont_classification(df_trn['Fare'],fare_limits)\ndf_tst['Fare'] = cont_classification(df_tst['Fare'],fare_limits)","4ed83de9":"most_frequent_emb = df['Embarked'].value_counts()[:1].index.tolist()[0]\ndf_trn['Embarked'] = df_trn['Embarked'].fillna(most_frequent_emb)\ndf_tst['Embarked'] = df_tst['Embarked'].fillna(most_frequent_emb)","aa2b1013":"df_trn = pd.concat([\n            df_trn.drop(columns=['Embarked']),\n            pd.get_dummies(df_trn['Embarked'],prefix='Emb').drop(columns=['Emb_C'])\n#             pd.get_dummies(df_trn['Embarked'],prefix='Emb')\n        ],axis=1)\n\n\ndf_tst = pd.concat([\n            df_tst.drop(columns=['Embarked']),\n            pd.get_dummies(df_tst['Embarked'],prefix='Emb').drop(columns=['Emb_C'])\n#             pd.get_dummies(df_tst['Embarked'],prefix='Emb')\n        ],axis=1)","5cc6f977":"Model_Scores = {}","3d55a141":"Model_Scores = {}\n\ndef print_model_scores():\n    return pd.DataFrame([[\n        model,\n        Model_Scores[model]['test_accuracy_score'],\n        Model_Scores[model]['cv_score_mean'],\n        Model_Scores[model]['cv_score_std']\n    ] for model in Model_Scores.keys()],\n        columns=['model','test_accuracy_score','cv_score','cv_score_std']\n    ).sort_values(by='cv_score',ascending=False)\n\n\ndef OptimizeClassification(X,y,\n    model,\n    hyperparametric_space,\n    cv = KFold(n_splits = 10, shuffle=True,random_state=SEED),\n    model_description = 'classifier',\n    n_iter = 20,\n    test_size = 0.25\n):\n    \n    X_trn,X_tst,y_trn,y_tst = train_test_split(\n        X,\n        y,\n        test_size = test_size,\n        random_state=SEED\n    )\n    \n    start = time()\n\n    # Searching the best setting\n    print('[info] Searching for the best hyperparameter')\n    search_cv = RandomizedSearchCV(\n                        model,\n                        hyperparametric_space,\n                        n_iter = n_iter,\n                        cv = cv,\n                        random_state = SEED)\n\n    search_cv.fit(X, y)\n    results = pd.DataFrame(search_cv.cv_results_)\n\n    print('[info] Search Timing: %.2f seconds'%(time() - start))\n\n    # Evaluating Test Score For Best Estimator\n    start = time()\n    print('[info] Test Accuracy Score')\n    gb = search_cv.best_estimator_\n    gb.fit(X_trn, y_trn)\n    y_pred = gb.predict(X_tst)\n\n    # Evaluating K Folded Cross Validation\n    print('[info] KFolded Cross Validation')\n    cv_results = cross_validate(search_cv.best_estimator_,X,y,\n                    cv = cv )\n\n    print('[info] Cross Validation Timing: %.2f seconds'%(time() - start))\n    \n    Model_Scores[model_description] = {\n        'test_accuracy_score':gb.score(X_tst,y_tst),\n        'cv_score_mean':cv_results['test_score'].mean(),\n        'cv_score_std':cv_results['test_score'].std(),\n        'best_params':search_cv.best_estimator_.get_params()\n    }\n    \n    pd.options.display.float_format = '{:,.5f}'.format\n\n    print('\\t\\t test_accuracy_score: {:.3f}'.format(gb.score(X_tst,y_tst)))\n    print('\\t\\t cv_score: {:.3f}\u00b1{:.3f}'.format(\n        cv_results['test_score'].mean(),cv_results['test_score'].std()))\n\n\n    params_list = ['mean_test_score']+list(map(lambda var: 'param_'+var,search_cv.best_params_.keys()))+['mean_fit_time']\n    \n    return results[params_list].sort_values(\n        ['mean_test_score','mean_fit_time'],\n        ascending=[False,True]\n    )","6d589790":"scaler = StandardScaler()\n# caler = Normalizer()\nscaler.fit(df_trn.drop(columns=['Survived','Cabin']))\nX = scaler.transform(df_trn.drop(columns=['Survived','Cabin']))\ny = df_trn['Survived']","dc60fb72":"results = OptimizeClassification(X,y,\n        model = LogisticRegression(random_state=SEED),\n        hyperparametric_space = {\n            'solver' : ['newton-cg', 'lbfgs', 'liblinear'],# \n            'C' : uniform(0.075,0.125,200) #10**uniform(-2,2,200)\n        },\n        cv = KFold(n_splits = 50, shuffle=True,random_state=SEED),\n        model_description = 'LogisticRegression',\n        n_iter = 20\n    )\n\nresults.head(5)","c5ab72ac":"results = OptimizeClassification(X,y,\n         model = SVC(random_state=SEED),\n         hyperparametric_space = {\n                'kernel' : ['linear', 'poly','rbf','sigmoid'],\n                'C' : 10**uniform(-1,1,200),\n                'decision_function_shape' : ['ovo', 'ovr'],\n                'degree' : [1,2,3,4]\n            },\n        cv = KFold(n_splits = 50, shuffle=True,random_state=SEED),\n        model_description = 'SVC',\n        n_iter = 20\n    )\n\nresults.head(5)","e2a88e6e":"results = OptimizeClassification(X,y,\n         model = DecisionTreeClassifier(),\n         hyperparametric_space = {\n            'min_samples_split': randint(10,30),\n            'max_depth': randint(10,30),\n            'min_samples_leaf': randint(1,10)\n         },\n        cv = KFold(n_splits = 50, shuffle=True,random_state=SEED),\n        model_description = 'DecisionTree',\n        n_iter = 100\n    )\n\nresults.head(5)","230a0098":"print_model_scores()","521b7fb4":"results = OptimizeClassification(X,y,\n         model = RandomForestClassifier(random_state = SEED,oob_score=True),\n         hyperparametric_space = {\n            'n_estimators': randint(190,250),\n            'min_samples_split': randint(10,15),\n            'min_samples_leaf': randint(1,6)\n#             'max_depth': randint(1,100),\n#             ,\n#             'min_weight_fraction_leaf': uniform(0,1,100),\n#             'max_features': uniform(0,1,100),\n#             'max_leaf_nodes': randint(10,100),\n         },\n        cv = KFold(n_splits = 20, shuffle=True,random_state=SEED),\n        model_description = 'RandomForestClassifier',\n        n_iter = 20\n    )\n\nresults.head(5)","a1a9fe97":"print_model_scores()","10de768f":"results = OptimizeClassification(X,y,\n    model = GradientBoostingClassifier(),\n    hyperparametric_space = {\n            'loss':               ['exponential'], #'deviance', \n            'min_samples_split':  randint(130,170),\n            'max_depth':          randint(6,15),\n            'learning_rate':      uniform(0.05,0.15,100),\n            'random_state' :      randint(0,10),\n            'tol':                10**uniform(-5,-3,100)\n         },\n    cv = KFold(n_splits = 20, shuffle=True,random_state=SEED),\n    model_description = 'GradientBoostingClassifier',\n    n_iter = 20\n)\nresults.head(5)","cc657f26":"def random_layer(max_depth=4,max_layer=100):\n    res = list()\n    depth = np.random.randint(1,1+max_depth)\n    \n    for i in range(1,depth+1):\n        res.append(np.random.randint(2,max_layer))\n        \n    return tuple(res)\n\n\n\nresults = OptimizeClassification(X,y,\n    model = MLPClassifier(random_state=SEED),\n    hyperparametric_space  = {\n            'hidden_layer_sizes': [random_layer(max_depth=4,max_layer=40) for i in range(10)],\n            'solver' : ['lbfgs', 'sgd', 'adam'],\n            'learning_rate': ['adaptive'],\n            'activation' : ['identity', 'logistic', 'tanh', 'relu']\n    },\n    cv = KFold(n_splits = 20, shuffle=True,random_state=SEED),\n    model_description = 'MLPClassifier',\n    n_iter = 20\n)\nresults.head(5)","c6e0c077":"print_model_scores()","1d92091b":"model = GradientBoostingClassifier(**Model_Scores['GradientBoostingClassifier']['best_params'])\n\nX_trn,X_tst,y_trn,y_tst = train_test_split(\n    X,\n    y,\n    test_size = 0.25\n)\n\nmodel.fit(X_trn,y_trn)\n\ny_pred = model.predict(X_tst)\n\ncv_results = cross_validate(model,X,y,\n                    cv = KFold(n_splits = 20, shuffle=True)  )\n    \nprint('test_accuracy_score: {:.3f}'.format(model.score(X_tst,y_tst)))\nprint('cv_score: {:.3f}\u00b1{:.3f}'.format(\n    cv_results['test_score'].mean(),cv_results['test_score'].std()))\n","6e5d0a2c":"pass_id = pd.read_csv('data\/test.csv')['PassengerId']\nmodel = GradientBoostingClassifier(**Model_Scores['GradientBoostingClassifier']['best_params'])\nmodel.fit(X,y)\n\nX_sub = scaler.transform(df_tst.drop(columns=['Cabin']))\ny_pred = model.predict(X_sub)\n\nsub = pd.Series(y_pred,index=pass_id,name='Survived')\nsub.to_csv('gbc_model_2.csv',header=True)\n\ny_pred","2c80536f":"model","560bcb97":"## Ticket","ad7e1f03":"We can see that Master and William are words with equivalent proportion between both survived and not survived cases. So, they are not good descriptive words","a61094c2":"## Multi Layer Perceptron Classifier","873fa417":"<a id=\"0103\"><\/a>\n<h2>1.3 Settings <a href=\"#01\"\nstyle=\"\n    border-radius: 10px;\n    background-color: #f1f1f1;\n    border: none;\n    color: #37509b;\n    text-align: center;\n    text-decoration: none;\n    display: inline-block;\n    padding: 4px 4px;\n    font-size: 14px;\n\">\u21bb<\/a><\/h2>","2710d2b9":"Investigating if the class is related to the probability of survival","045b7743":"## Support Vector Classifier","6147e871":"## Pclass","8177cb27":"## Name","df85cd67":"## Fare","ec95393d":"## Cabin Feature","416661dd":"## Embarked","555ca825":"## Decision Tree Classifier","afda5181":"<a id=\"title\"><\/a>\n<a id=\"toc\"><\/a>\n![title](https:\/\/raw.githubusercontent.com\/emdemor\/PythonDataScienceWithPandas_Exercises\/master\/Titanico\/source\/header2.png)","baa7863b":"## Family Size","51246edc":"## Sex","a95d9cb5":"# Best Model","c25e5b0b":"## Gradient Boosting Classifier","6127a8bf":"## Age","d8172e8c":"## Logistic Regression","affa8b88":"## Random Forest Classifier","f70bd4d2":"### Features\n\n<table>\n<tbody>\n<tr><th><b>Variable<\/b><\/th><th><b>Definition<\/b><\/th><th><b>Key<\/b><\/th><\/tr>\n<tr>\n<td>survival<\/td>\n<td>Survival<\/td>\n<td>0 = No, 1 = Yes<\/td>\n<\/tr>\n<tr>\n<td>pclass<\/td>\n<td>Ticket class<\/td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd<\/td>\n<\/tr>\n<tr>\n<td>sex<\/td>\n<td>Sex<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>Age<\/td>\n<td>Age in years<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>sibsp<\/td>\n<td># of siblings \/ spouses aboard the Titanic<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>parch<\/td>\n<td># of parents \/ children aboard the Titanic<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>ticket<\/td>\n<td>Ticket number<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>fare<\/td>\n<td>Passenger fare<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>cabin<\/td>\n<td>Cabin number<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>embarked<\/td>\n<td>Port of Embarkation<\/td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton<\/td>\n<\/tr>\n<\/tbody>\n<\/table>","5a953a06":"<a id=\"02\" style=\"\n  background-color: #37509b;\n  border: none;\n  color: white;\n  padding: 2px 10px;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  font-size: 10px;\" href=\"#toc\">TOC \u21bb<\/a>\n  \n  \n<div  style=\"margin-top: 9px; background-color: #efefef; padding-top:10px; padding-bottom:10px;margin-bottom: 9px;box-shadow: 5px 5px 5px 0px rgba(87, 87, 87, 0.2);\">\n        <center>\n            <h1>2. Dataset: Cleaning and Exploration<\/h1>\n        <\/center>\n\n   \n   \n<ol type=\"i\">\n<!--     <li><a href=\"#0101\" style=\"color: #37509b;\">Inicializa\u00e7\u00e3o<\/a><\/li>\n    <li><a href=\"#0102\" style=\"color: #37509b;\">Pacotes<\/a><\/li>\n    <li><a href=\"#0103\" style=\"color: #37509b;\">Funcoes<\/a><\/li>\n    <li><a href=\"#0104\" style=\"color: #37509b;\">Dados de Indicadores Sociais<\/a><\/li>\n    <li><a href=\"#0105\" style=\"color: #37509b;\">Dados de COVID-19<\/a><\/li>\n -->\n<\/ol>\n\n\n\n<\/div>","3c158a8b":"<a id=\"01\" style=\"\n  background-color: #37509b;\n  border: none;\n  color: white;\n  padding: 2px 10px;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  font-size: 10px;\" href=\"#toc\">TOC \u21bb<\/a>\n  \n  \n<div  style=\"margin-top: 9px; background-color: #efefef; padding-top:10px; padding-bottom:10px;margin-bottom: 9px;box-shadow: 5px 5px 5px 0px rgba(87, 87, 87, 0.2);\">\n        <center>\n            <h1>1. Initialization<\/h1>\n        <\/center>\n\n   \n   \n<ol type=\"i\">\n<!--     <li><a href=\"#0101\" style=\"color: #37509b;\">Inicializa\u00e7\u00e3o<\/a><\/li>\n    <li><a href=\"#0102\" style=\"color: #37509b;\">Pacotes<\/a><\/li>\n    <li><a href=\"#0103\" style=\"color: #37509b;\">Funcoes<\/a><\/li>\n    <li><a href=\"#0104\" style=\"color: #37509b;\">Dados de Indicadores Sociais<\/a><\/li>\n    <li><a href=\"#0105\" style=\"color: #37509b;\">Dados de COVID-19<\/a><\/li>\n -->\n<\/ol>\n\n\n\n<\/div>","08a8d373":"<div  style=\"margin-top: 9px; background-color: #efefef; padding-top:10px; padding-bottom:10px;margin-bottom: 9px;box-shadow: 5px 5px 5px 0px rgba(87, 87, 87, 0.2);\">\n    <center>\n        <h2>Table of Contents<\/h2>\n    <\/center>\n\n   \n<ol>\n    <li><a href=\"#01\" style=\"color: #37509b;\">Initialization<\/a><\/li>\n    <li><a href=\"#02\" style=\"color: #37509b;\">Dataset: Cleaning and Exploration<\/a><\/li>\n    <li><a href=\"#03\" style=\"color: #37509b;\">Modelling<\/a><\/li>\n\n<\/ol>\n\n\n<\/div>","8b7e4971":"**Best Model (until now)**\n\n```\nGradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n                           learning_rate=0.10165006218060142,\n                           loss='exponential', max_depth=7, max_features=None,\n                           max_leaf_nodes=None, min_impurity_decrease=0.0,\n                           min_impurity_split=None, min_samples_leaf=1,\n                           min_samples_split=134, min_weight_fraction_leaf=0.0,\n                           n_estimators=100, n_iter_no_change=None,\n                           presort='deprecated', random_state=42, subsample=1.0,\n                           tol=0.0001, validation_fraction=0.1, verbose=0,\n                           warm_start=False)\n```","1b2ffbdc":"<a id=\"0102\"><\/a>\n<h2>1.2 Packages <a href=\"#01\"\nstyle=\"\n    border-radius: 10px;\n    background-color: #f1f1f1;\n    border: none;\n    color: #37509b;\n    text-align: center;\n    text-decoration: none;\n    display: inline-block;\n    padding: 4px 4px;\n    font-size: 14px;\n\">\u21bb<\/a><\/h2>","4b9a7418":"## Scaling DataSet","262b6749":"### Classification Approach","f4c90b53":"<a id=\"0101\"><\/a>\n<h2>2.1 Import Dataset <a href=\"#02\"\nstyle=\"\n    border-radius: 10px;\n    background-color: #f1f1f1;\n    border: none;\n    color: #37509b;\n    text-align: center;\n    text-decoration: none;\n    display: inline-block;\n    padding: 4px 4px;\n    font-size: 14px;\n\">\u21bb<\/a><\/h2>","75837ab5":"<a id=\"03\" style=\"\n  background-color: #37509b;\n  border: none;\n  color: white;\n  padding: 2px 10px;\n  text-align: center;\n  text-decoration: none;\n  display: inline-block;\n  font-size: 10px;\" href=\"#toc\">TOC \u21bb<\/a>\n  \n  \n<div  style=\"margin-top: 9px; background-color: #efefef; padding-top:10px; padding-bottom:10px;margin-bottom: 9px;box-shadow: 5px 5px 5px 0px rgba(87, 87, 87, 0.2);\">\n        <center>\n            <h1>3. Modelling<\/h1>\n        <\/center>\n\n   \n   \n<ol type=\"i\">\n<!--     <li><a href=\"#0101\" style=\"color: #37509b;\">Inicializa\u00e7\u00e3o<\/a><\/li>\n    <li><a href=\"#0102\" style=\"color: #37509b;\">Pacotes<\/a><\/li>\n    <li><a href=\"#0103\" style=\"color: #37509b;\">Funcoes<\/a><\/li>\n    <li><a href=\"#0104\" style=\"color: #37509b;\">Dados de Indicadores Sociais<\/a><\/li>\n    <li><a href=\"#0105\" style=\"color: #37509b;\">Dados de COVID-19<\/a><\/li>\n -->\n<\/ol>\n\n\n\n<\/div>","1cd93800":"<a id=\"0101\"><\/a>\n<h2>1.1 Description <a href=\"#01\"\nstyle=\"\n    border-radius: 10px;\n    background-color: #f1f1f1;\n    border: none;\n    color: #37509b;\n    text-align: center;\n    text-decoration: none;\n    display: inline-block;\n    padding: 4px 4px;\n    font-size: 14px;\n\">\u21bb<\/a><\/h2>","fa8c2d10":"### Feature Engineering","3165c521":"Dataset available in:\n\n  <a href=\"https:\/\/www.kaggle.com\/c\/titanic\/\" target=\"_blank\">https:\/\/www.kaggle.com\/c\/titanic\/<\/a>\n  ","ac18d4ac":"<a id=\"0104\"><\/a>\n<h2>1.4 Useful Functions <a href=\"#01\"\nstyle=\"\n    border-radius: 10px;\n    background-color: #f1f1f1;\n    border: none;\n    color: #37509b;\n    text-align: center;\n    text-decoration: none;\n    display: inline-block;\n    padding: 4px 4px;\n    font-size: 14px;\n\">\u21bb<\/a><\/h2>"}}