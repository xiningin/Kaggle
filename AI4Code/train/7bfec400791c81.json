{"cell_type":{"50c422ef":"code","721b2f74":"code","4ebe098b":"code","c84441bf":"code","8f458a36":"code","fef66919":"code","60b64b37":"code","305eb6a1":"code","e424be09":"code","ef1e4494":"code","ea398536":"code","f685797e":"code","f1354dc3":"code","d7379a3f":"code","43977c42":"code","d577d690":"code","1b797b6d":"code","88a316f9":"code","fd0ec3ea":"code","b045d960":"code","8fbb06ca":"code","c4d526c2":"code","66b37c05":"code","b04f066d":"code","427883f0":"code","b359a4e5":"code","93cc4494":"code","949b7ea9":"code","f66d716c":"code","d92fb4eb":"code","ab7b1a22":"code","1fd581f3":"code","f94066e3":"code","1c6da789":"code","4a8c0680":"code","55104d8c":"code","5cb740dc":"code","ee15e650":"code","796c0b2a":"code","41870021":"code","d635679a":"code","d8593c57":"code","b6f7c0be":"code","204703a9":"code","2ce3abdc":"code","0a9d4146":"code","ae520978":"code","c274836b":"code","28390b8d":"code","fa7f2e81":"markdown","dedc61ab":"markdown","621d4317":"markdown","a0548835":"markdown","30a8b270":"markdown","54ee0ff1":"markdown","6e8007b8":"markdown","0d348e04":"markdown","8140cb41":"markdown","24f6af62":"markdown","4a5aae6d":"markdown","956de38a":"markdown"},"source":{"50c422ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","721b2f74":"# Imports statements.\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\n#Data Preprocessing and Feature Engineering\nfrom wordcloud import WordCloud, STOPWORDS \nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n\n#Model Selection and Validation\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix, classification_report,accuracy_score\n%matplotlib inline","4ebe098b":"def display_all_details(dataframe):\n    print(('='*50)+'DATA'+('='*50))\n    print(('-'*50)+'SHAPE'+('-'*50))\n    print(dataframe.shape)\n    print(('-'*50)+'COLUMNS'+('-'*50))\n    print(dataframe.columns)\n    print(('-'*50)+'DESCRIBE'+('-'*50))\n    print(dataframe.describe())\n    print(('-'*50)+'INFO'+('-'*50))\n    print(dataframe.info())\n    print(('='*50)+'===='+('='*50))","c84441bf":"twitter_data = pd.read_csv('..\/input\/twitterdata\/finalSentimentdata2.csv')","8f458a36":"display_all_details(twitter_data)","fef66919":"twitter_data.head()","60b64b37":"twitter_data.tail()","305eb6a1":"# Checking missing values columns\ntwitter_data.isnull().sum()","e424be09":"print(twitter_data.sentiment.value_counts())\n","ef1e4494":"twitter_data.sentiment.value_counts().plot(kind = 'bar')","ea398536":"# Creating a mapping for sentiments\nmapping = {'fear':0,\n          'sad':1,\n          'anger':2,\n          'joy':3}","f685797e":"twitter_data['sentiment'] = twitter_data['sentiment'].map(mapping)","f1354dc3":"twitter_data.head()","d7379a3f":"for tweet in twitter_data.text.head(20):\n    print(tweet)","43977c42":"def clean_text_column(row):\n    text = row['text'].lower()\n    text = re.sub(r'[^(a-zA-Z\\s)]','',text)\n    text = re.sub(r'\\(','',text)\n    text = re.sub(r'\\)','',text)\n    text = text.replace('\\n',' ')\n    text = text.strip()\n    return text","d577d690":"twitter_data['cleaned_text'] = twitter_data.apply(clean_text_column,axis = 1)","1b797b6d":"twitter_data.head()","88a316f9":"# These are new stopwords which i add after several model runs and found out these are irrelevant words which are created which cleaning process.\nnew_additions=['aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'mustn', 're', 'shan', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn']","fd0ec3ea":"new_string = ''\nstopwords = set(list(STOPWORDS)+new_additions)\nfor val in twitter_data.cleaned_text: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    new_string += \" \".join(tokens)+\" \"","b045d960":"wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(new_string) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show()","8fbb06ca":"# Check for spaced entry which can be created due to cleaning step.\ntwitter_data.cleaned_text.str.isspace().sum()","c4d526c2":"filtered_sentences = []\nfor tweet in twitter_data.cleaned_text:\n    filtered_sentences.append(remove_stopwords(tweet))","66b37c05":"filter_sentence_df = pd.DataFrame(filtered_sentences,columns = ['filter_sentence'])","b04f066d":"new_twitter_data = pd.concat([twitter_data,filter_sentence_df],axis = 1)","427883f0":"new_twitter_data.head()","b359a4e5":"#Normalizing the words in tweets \ndef normalization(tweet):\n    lem = WordNetLemmatizer()\n    normalized_tweet = []\n    for word in tweet['filter_sentence'].split():\n        normalized_text = lem.lemmatize(word,'v')\n        normalized_tweet.append(normalized_text)\n    return normalized_tweet","93cc4494":"new_twitter_data['normalised_tweet'] = new_twitter_data.apply(normalization,axis = 1)","949b7ea9":"new_twitter_data.head()","f66d716c":"msg_train, msg_test, label_train, label_test = train_test_split(new_twitter_data['filter_sentence'],new_twitter_data['sentiment'], test_size=0.1,random_state = 2)","d92fb4eb":"pipeline = Pipeline([\n    ('tfidf', TfidfVectorizer(stop_words=stopwords)),\n    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w\/ Naive Bayes classifier\n    ])","ab7b1a22":"pipeline.fit(msg_train,label_train)\npredictions = pipeline.predict(msg_test)\nprint(classification_report(predictions,label_test))\nprint(confusion_matrix(predictions,label_test))\nprint(accuracy_score(predictions,label_test))","1fd581f3":"pipeline2 = Pipeline([\n    ('tfidf', TfidfVectorizer(stop_words=stopwords)),\n    ('classifier',LogisticRegression(solver='sag')),  # train on TF-IDF vectors w\/ Naive Bayes classifier\n    ])","f94066e3":"pipeline2.fit(msg_train,label_train)\npredictions2 = pipeline2.predict(msg_test)\nprint(classification_report(predictions2,label_test))\nprint(confusion_matrix(predictions2,label_test))\nprint(accuracy_score(predictions2,label_test))","1c6da789":"pipeline3 = Pipeline([\n                ('tfidf', TfidfVectorizer(stop_words=stopwords)),\n                ('clf', OneVsRestClassifier(SVC(), n_jobs=1)),\n            ])","4a8c0680":"pipeline3.fit(msg_train,label_train)\npredictions3 = pipeline3.predict(msg_test)\nprint(classification_report(predictions3,label_test))\nprint(confusion_matrix(predictions3,label_test))\nprint(accuracy_score(predictions3,label_test))","55104d8c":"from sklearn.ensemble import VotingClassifier","5cb740dc":"voting_classifier = VotingClassifier(estimators=[ ('nb', pipeline),('lr', pipeline2), ('svc', pipeline3)], voting='hard')","ee15e650":"voting_classifier.fit(msg_train,label_train)\npredictions4 = voting_classifier.predict(msg_test)\nprint(classification_report(predictions4,label_test))\nprint(confusion_matrix(predictions4,label_test))\nprint(accuracy_score(predictions4,label_test))","796c0b2a":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Dropout","41870021":"new_twitter_data.head()","d635679a":"# The maximum number of words to be used. (most frequent)\nMAX_NB_WORDS = 1000\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 250\n# This is fixed.\nEMBEDDING_DIM = 100\n\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-.\/:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(new_twitter_data.filter_sentence.values)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","d8593c57":"X = tokenizer.texts_to_sequences(new_twitter_data.filter_sentence.values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)","b6f7c0be":"Y = pd.get_dummies(new_twitter_data.sentiment).values\nprint('Shape of label tensor:', Y.shape)","204703a9":"X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 2)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","2ce3abdc":"model = Sequential()\nmodel.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\nmodel.add(SpatialDropout1D(0.1))\nmodel.add(LSTM(100, dropout=0.1, recurrent_dropout=0.1))\nmodel.add(Dense(4, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","0a9d4146":"epochs = 20\nbatch_size = 64\n# I am using EarlyStopping to monitor val_loss upto 3 patience level to prevent the model from overfitting.\nhistory = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])","ae520978":"accr = model.evaluate(X_test,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","c274836b":"plt.title('Loss')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()","28390b8d":"plt.title('Accuracy')\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","fa7f2e81":"## MultinomialNB Pipeline","dedc61ab":"## Support Vector Machine pipeline","621d4317":"# Results:\n### * Multinomial naive bayes Accuracy: 71.84%\n### * Logistic Regression Accuracy: 75.00%\n### * Support Vector Machine Accuracy: 74.75%\n### * Voting Classifier Accuracy: 75.00%\n### * LSTM Accuracy: 65.00%","a0548835":"### This seems good and gives a good picture of words frequency in dataset","30a8b270":"## Voting Classifier pipeline","54ee0ff1":"# Let's Try LSTM","6e8007b8":"## Creating a WordCloud to visualize most frequent words","0d348e04":"## Stopword removal","8140cb41":"If you find this notebook useful, please upvote it!","24f6af62":"## Logistic Regression pipeline","4a5aae6d":"## Getting Insight about the data","956de38a":"## Import Statements"}}