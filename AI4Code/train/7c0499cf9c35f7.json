{"cell_type":{"d9f2d62f":"code","091e735c":"code","36d22bd8":"code","413461ac":"code","40be6235":"code","09afe048":"code","1f1ad517":"code","b093a406":"code","5667f488":"code","dcb0cdeb":"code","76650029":"code","9228a8dc":"code","cfa7567e":"code","a39b3875":"code","b5b323ab":"code","9ddf4b6d":"code","5f780667":"code","3723639a":"code","456b5c7a":"code","7f6c4730":"code","25afc795":"code","cca23165":"code","ae308f7e":"code","5f6bc9d2":"code","b716d6e0":"code","6dd59008":"code","224945ad":"code","660120c5":"code","2abc425d":"code","cd039582":"code","b8955453":"code","03de3166":"code","3b1aa7bc":"code","80d40c9e":"code","f26d1c4a":"code","38b56e08":"code","88b32540":"code","87aa261a":"code","254c5c6f":"code","2b9526fd":"code","4ca88eb4":"code","64fcf0c2":"code","1ef6ec0b":"code","f3ce35ce":"code","f391b2fb":"code","27f15f67":"code","fffbf98b":"code","4efa8a27":"code","90f799ce":"code","0b29b01d":"code","24f5466a":"code","cabbc855":"code","1ddc9fca":"code","27551c5e":"code","012fce10":"code","6e210c31":"code","1d8ff96e":"code","441d5d32":"code","bbffc6a9":"code","1a70d644":"code","7a81006d":"code","69ca0d3d":"code","859bdb05":"code","072cd3c8":"code","218074b9":"markdown","89b9b0eb":"markdown","c943ca65":"markdown","8c4a99d9":"markdown","ba898925":"markdown","940e1d39":"markdown","85d1f34b":"markdown","124cc29b":"markdown","c486ae2d":"markdown","3df23279":"markdown","173c67fc":"markdown","70e6005e":"markdown","eb000bcd":"markdown","cbd454aa":"markdown","6f5e9dee":"markdown","a83b008b":"markdown","1c6477d0":"markdown","718a6798":"markdown","c6c3dccf":"markdown","9f816ef1":"markdown","2c9f9c89":"markdown","c338745a":"markdown","e4d597a1":"markdown","de6f0477":"markdown","f323bbb3":"markdown","228b7c6d":"markdown","61fb86ed":"markdown"},"source":{"d9f2d62f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport folium \n\n\n# import all libraries and dependencies for machine learning\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import IncrementalPCA\n\n\n# import all libraries and dependencies for clustering\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\n","091e735c":"# Reading the file on which analysis needs to be done\n\ndataset = pd.read_csv(\"\/kaggle\/input\/basecomparateurdeterritoires\/base-comparateur-de-territoires.csv\" , sep=\";\")\ndataset.head()","36d22bd8":"dataset.shape","413461ac":"dataset.describe()","40be6235":"dataset.info()","09afe048":"# Calculating the Missing Values Pourcentage in columns\ndf_null = dataset.isna().mean()*100\ndf_null","1f1ad517":"dataset[\"TP6014\"].head(20)","b093a406":"#droping TP6014 column beacuse it has minssing on 60-70 percent of it\ndataset = dataset.drop([\"TP6014\"], axis = 1)","5667f488":"# Segregation of Numerical and Categorical Variables\/Columns\ncat_col = dataset.select_dtypes(include = ['object']).columns\nnum_col = dataset.select_dtypes(exclude = ['object']).columns","dcb0cdeb":"# replace the Nan values with the mean\nfor column in num_col:\n    mean = int(dataset[column].mean(skipna=True))\n    dataset[column] = dataset[column].replace(np.NaN, mean)","76650029":"# Datatype check for the dataframe\ndataset.dtypes","9228a8dc":"# Duplicates check\n\ndataset.loc[dataset.duplicated()]","cfa7567e":"dataset.head()","a39b3875":"# Split \"geo_point_2d\" so we can use it later.\ndataset[['Latitude','Longitude']] = dataset['geo_point_2d'].str.split(',', 1, expand=True)\ndataset.head()","b5b323ab":"# Droping de columns that will be no need to give it to the model\ndataset_drop = dataset.drop([\"CODGEO\",\"geo_point_2d\",\"LIBGEO\",\"Geo Shape\",\"Latitude\",\"Longitude\"], axis = 1)","9ddf4b6d":"plt.figure(figsize=(25, 10))\nplt.boxplot(dataset_drop);","5f780667":"for col in dataset_drop.columns:\n  q25, q75 = np.percentile(dataset_drop[col].dropna(), 25), np.percentile(dataset_drop[col].dropna(), 75)\n  iqr = q75 - q25\n  upper_limit = q75 + 1.5 * iqr\n  lower_limit = q25 - 1.5 * iqr\n  dataset_drop[col] = np.where(\n      dataset_drop[col] < lower_limit,\n      lower_limit,\n      np.where(\n          dataset_drop[col] > upper_limit,\n          upper_limit,\n          dataset_drop[col]\n      )\n  )","3723639a":"plt.figure(figsize=(25, 10))\nplt.boxplot(dataset_drop);","456b5c7a":"# Heatmap to understand the attributes dependency\nplt.figure(figsize = (32,18))        \nax = sns.heatmap(dataset.corr(),annot = True, cmap=\"coolwarm\")\n# bottom, top = ax.get_ylim()\n# ax.set_ylim(bottom + 0.5, top - 0.5)","7f6c4730":"# Pairplot of all numeric columns\nsns.pairplot(dataset)","25afc795":"dataset_drop.head()","cca23165":"dataset_drop.shape","ae308f7e":"scaler = StandardScaler()\ndataset_scaled = scaler.fit_transform(dataset_drop)","5f6bc9d2":"dataset_scaled","b716d6e0":"pca = PCA(svd_solver='randomized', random_state=42)","6dd59008":"pca.fit(dataset_scaled)","224945ad":"# Variance Ratio bar plot for each PCA components.\n\nax = plt.bar(range(1,len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_)\nplt.xlabel(\"PCA Components\",fontweight = 'bold')\nplt.ylabel(\"Variance Ratio\",fontweight = 'bold')","660120c5":"# Scree plot to visualize the Cumulative variance against the Number of components\n\nfig = plt.figure(figsize = (16,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.vlines(x=3, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\nplt.hlines(y=0.93, xmax=8, xmin=0, colors=\"g\", linestyles=\"--\")\nplt.xlabel('Number of PCA components')\nplt.ylabel('Cumulative Explained Variance')","2abc425d":"# Checking which attributes are well explained by the pca components\n\ncommune_col = list(dataset_drop.columns)\nattributes_pca = pd.DataFrame({'Attribute':commune_col,'PC_1':pca.components_[0],'PC_2':pca.components_[1],'PC_3':pca.components_[2]})","cd039582":"attributes_pca","b8955453":"# Plotting the above dataframe for better visualization with PC1 and PC2\n\nsns.pairplot(data=attributes_pca, x_vars=[\"PC_1\"], y_vars=[\"PC_2\"], hue = \"Attribute\" ,height=8)\nplt.xlabel(\"Principal Component 1\",fontweight = 'bold')\nplt.ylabel(\"Principal Component 2\",fontweight = 'bold')\n\nfor i,txt in enumerate(attributes_pca.Attribute):\n    plt.annotate(txt, (attributes_pca.PC_1[i],attributes_pca.PC_2[i]))","03de3166":"# Plotting the above dataframe with PC1 and PC3 to understand the components which explains inflation.\n\nsns.pairplot(data=attributes_pca, x_vars=[\"PC_1\"], y_vars=[\"PC_3\"], hue = \"Attribute\" ,height=8)\nplt.xlabel(\"Principal Component 1\",fontweight = 'bold')\nplt.ylabel(\"Principal Component 3\",fontweight = 'bold')\n\nfor i,txt in enumerate(attributes_pca.Attribute):\n    plt.annotate(txt, (attributes_pca.PC_1[i],attributes_pca.PC_3[i]))","3b1aa7bc":"# Building the dataframe using Incremental PCA for better efficiency.\n\ninc_pca = IncrementalPCA(n_components=3)","80d40c9e":"# Fitting the scaled df on incremental pca\n\ndf_inc_pca = inc_pca.fit_transform(dataset_scaled)\nlen(df_inc_pca)","f26d1c4a":"# Creating new dataframe with Principal components\n\ndf_pca = pd.DataFrame(df_inc_pca, columns=[\"PC_1\", \"PC_2\",\"PC_3\"])\ndf_pca_final = pd.concat([dataset[\"LIBGEO\"], df_pca], axis=1)\ndf_pca_final.head()","38b56e08":"len(df_pca_final)","88b32540":"# Plotting Heatmap to check is there still dependency in the dataset.\n\nplt.figure(figsize = (8,6))        \nax = sns.heatmap(df_pca_final.corr(),annot = True)\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)","87aa261a":"# Scatter Plot to visualize the spread of data across PCA components\n\nplt.figure(figsize=(20, 8))\nplt.subplot(1,3,1)\nsns.scatterplot(data=df_pca_final, x='PC_1', y='PC_2')\nplt.subplot(1,3,2)\nsns.scatterplot(data=df_pca_final, x='PC_1', y='PC_3')\nplt.subplot(1,3,3)\nsns.scatterplot(data=df_pca_final, x='PC_3', y='PC_2')","254c5c6f":"# Scatter 3d Plot to visualize the spread of data across PCA components\n\nfig = plt.figure(figsize=(20,15))\nax = fig.add_subplot(projection='3d')\nax.scatter(df_pca_final[\"PC_1\"], df_pca_final[\"PC_2\"], df_pca_final[\"PC_3\"], marker='o' )\nax.set_xlabel('X-PC_1')\nax.set_ylabel('Y-PC_2')\nax.set_zlabel('Z-PC_3')","2b9526fd":"df_pca_final_data = df_pca\nlen(df_pca_final_data)","4ca88eb4":"# Elbow curve method to find the ideal number of clusters.\ninerties = []\nfor num_clusters in range(1,10):\n    model_clus = KMeans(n_clusters = num_clusters)\n    model_clus.fit_transform(df_pca_final_data)\n    inerties.append(model_clus.inertia_)\n\nplt.plot(range(1,10),inerties)\nplt.xlabel(\"Nombre de clusters\")\nplt.ylabel(\"Inertie\")\nplt.title('Inertie vs nombre de classes')","64fcf0c2":"# Silhouette score analysis to find the ideal number of clusters for K-means clustering\n\nrange_n_clusters = [3, 4, 5, 6, 7, 8]\n\nfor num_clusters in range_n_clusters:\n    \n    # intialise kmeans\n    kmeans = KMeans(n_clusters=num_clusters,random_state= 100)\n    kmeans.fit(df_pca_final_data)\n    y_pred = kmeans.predict(df_pca_final_data)\n    \n    # silhouette score\n    silhouette_avg = silhouette_score(df_pca_final_data, y_pred, metric='euclidean')\n    print(\"For n_clusters={0}, the silhouette score is {1}\".format(num_clusters, silhouette_avg))","1ef6ec0b":"#K-means with k=3 clusters\n\ncluster3 = KMeans(n_clusters=3, random_state= 100)\ncluster3.fit(df_pca_final_data)","f3ce35ce":"# Cluster labels\n\ncluster3.labels_","f391b2fb":"# Assign the label\n# df_pca_final_data=df_pca_final_data.drop(['Cluster_Id3'], axis=1)\ndf_pca_final_data['Cluster_Id3'] = cluster3.labels_\ndf_pca_final_data.head()","27f15f67":"# Number of region in each cluster\n\ndf_pca_final_data['Cluster_Id3'].value_counts()","fffbf98b":"#cluster centers\ncentres = cluster3.cluster_centers_","4efa8a27":"# Scatter plot on Principal components to visualize the spread of the data\n\nfig, axes = plt.subplots(1,2, figsize=(15,7))\n\nsns.scatterplot(x='PC_1',y='PC_2',hue='Cluster_Id3',legend='full',palette=\"Set1\",data=df_pca_final_data,ax=axes[0])\nsns.scatterplot(x='PC_1',y='PC_3',hue='Cluster_Id3',legend='full',palette=\"Set1\",data=df_pca_final_data,ax=axes[1])","90f799ce":"\nfig = plt.figure(figsize=(20,15))\nax = fig.add_subplot(projection='3d')\nax.scatter(df_pca_final[\"PC_1\"], df_pca_final[\"PC_2\"], df_pca_final[\"PC_3\"], c = cluster3.labels_,cmap=\"summer\",s=100, marker='o' )\nax.scatter(centres[:, 0], centres[:, 1], centres[:, 2],c='red', s=250, alpha=0.9 )\nax.set_xlabel('X-PC_1')\nax.set_ylabel('Y-PC_2')\nax.set_zlabel('Z-PC_3')","0b29b01d":"cha = AgglomerativeClustering().fit_predict(df_pca_final_data)","24f5466a":"cha","cabbc855":"fig = plt.figure(figsize=(20,15))\nax = fig.add_subplot(projection='3d')\nax.scatter(df_pca_final[\"PC_1\"], df_pca_final[\"PC_2\"], df_pca_final[\"PC_3\"], c = cha,cmap=\"summer\",s=100, marker='o' )\nax.set_xlabel('X-PC_1')\nax.set_ylabel('Y-PC_2')\nax.set_zlabel('Z-PC_3')","1ddc9fca":"# Single linkage\n\nmergings = linkage(df_pca_final_data, method='single',metric='euclidean')\ndendrogram(mergings)\nplt.show()","27551c5e":"# Complete Linkage\n\nmergings = linkage(df_pca_final_data, method='complete',metric='euclidean')\ndendrogram(mergings)\nplt.show()","012fce10":"# average Linkage\nZ = linkage(df_pca_final_data, 'average', metric='euclidean')\nplt.figure(figsize=(25, 10))\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('sample index')\nplt.ylabel('distance')\ndendrogram(\n    Z,\n    leaf_rotation=90.,  # rotates the x axis labels\n    leaf_font_size=8.,  # font size for the x axis labels\n)\nplt.show()","6e210c31":"cha = AgglomerativeClustering(n_clusters=3).fit_predict(df_pca_final_data)","1d8ff96e":"fig = plt.figure(figsize=(20,15))\nax = fig.add_subplot(projection='3d')\nax.scatter(df_pca_final_data[\"PC_1\"], df_pca_final_data[\"PC_2\"], df_pca_final_data[\"PC_3\"], c = cha ,cmap=\"summer\",s=100, marker='o' )\nax.set_xlabel('X-PC_1')\nax.set_ylabel('Y-PC_2')\nax.set_zlabel('Z-PC_3')","441d5d32":"silhouette_score(df_pca_final_data, cha)","bbffc6a9":"silhouette_score(df_pca_final_data, cluster3.labels_)","1a70d644":"communes = pd.DataFrame(\n    {\n      'LIBGEO': dataset['LIBGEO'],\n      'Latitude':dataset['Latitude'],\n      'Longitude': dataset['Longitude'],\n      'classe': cluster3.labels_,\n    }\n)\ncommunes ","7a81006d":"# Calculating the Missing Values Pourcentage in rows\nnullList=[]\nfor i in range(len(communes.index)) :\n    nullList.append((i,communes.iloc[i].isnull().sum()))\n    \n# max(nullList,key=lambda item:item[1])\nsorted(nullList,key=lambda item:item[1],reverse=True)[:15]","69ca0d3d":"communes = communes.drop([168,873,989],axis=0)","859bdb05":"# Create a map\ncolor = \"\"\nm = folium.Map(location=[48.7, 2.454], tiles='openstreetmap', zoom_start=8.5)\n\n# Add points to the map\nfor idx, row in communes.iterrows():\n    if row['classe'] == 0:\n        color =  'crimson'\n    elif row['classe'] == 1:\n        color = 'green'\n    else:\n        color = 'blue'\n    folium.Circle([row['Latitude'], row['Longitude']], popup=row['LIBGEO'] , color=color,fill=True,radius=240).add_to(m)\n\nm","072cd3c8":"# import folium\n\n# # Create a map\n# m = folium.Map(location=[48.7, 2.454], tiles='openstreetmap', zoom_start=8.5)\n\n# # Set up Choropleth map\n# folium.Choropleth(\n#         geo_data=communes,\n#         data=communes,\n#         columns=['CODGEO',\"classe\"],\n#         key_on=\"feature.properties.CODGEO\",\n#         fill_color='YlGnBu',\n#         fill_opacity=1,\n#         line_opacity=0.2,\n#         legend_name=\"classe\",\n#         smooth_factor=0,\n#         Highlight= True,\n#         line_color = \"#0000\",\n#         name = \"classe\",\n#         show=False,\n#         overlay=True,\n#         nan_fill_color = \"White\"\n# ).add_to(m)\n\n# m","218074b9":"\n# Step 6 : Model Building\n\n**K- means Clustering**\n<br>\nK-means clustering is one of the simplest and popular unsupervised machine learning algorithms.\n\n\n**Finding the Optimal Number of Clusters**\n\nElbow Curve to get the right number of Clusters\nA fundamental step for any unsupervised algorithm is to determine the optimal number of clusters into which the data may be clustered. The Elbow Method is one of the most popular methods to determine this optimal value of k.","89b9b0eb":"**Average linkage clustering**\n\nIn Average linkage clustering, the distance between two clusters is defined as the average of distances between all pairs of objects, where each pair is made up of one object from each group. ","c943ca65":"\n\n# Step 1: Reading and Understanding the Data","8c4a99d9":"#  Step 7 : Final Analysis","ba898925":"# Step 4 : Data Preparation","940e1d39":"**Inference:**\n* **p14_pop** and **p14_mean** are highly correlated with correlation of 0.99\n* **P14_CHOM1564** and **PIMP14** are highly correlated with correlation of -0.37\n* **DECE0914** and **P14_RP_PROP** are highly correlated with correlation of 0.96\n* **PIMP14** and **NAISD16** are highly correlated with correlation of -0.34","85d1f34b":"# <div class=\"alert\" style=\"background:#404241; color:white; padding:5px 10px; border-radius:10px; max-width: 100%;\"><h1 style='margin:5px 2px;font-size: 35px; text-align:center'> Communes of Ile-de-France <\/h1>\n<\/div>\n\n\n**Problem Statement**\n\n\n\nIn order to help the public authorities to better orient their decisions, you are asking to show whether there are classes of municipalities with similar characteristics. We will thus obtain a typology of the municipalities of \u00cele-de-France and we will be able to represent these groups visually. The data we use are data on the municipalities of \u00cele-de-France and their socio-demographic characteristics.\n\n\n**Business Goal**\n\n\nOur job is to categorise the region using some sociodemography\n\n**Below are the steps which we will be basically following:** <br>\n\n> **1. Step 1: Reading and Understanding the Data**\n> \n> **2. Step 2: Data Cleansing**\n> * Missing Value check\n> * Data type check\n> * Duplicate check\n> \n> **3. Step 3: Data Visualization**\n> * Outlier Analysis and Treatment\n> * Heatmap\n> * Pairplot\n> \n> **4. Step 4: Data Preparation**\n> * Rescaling\n> \n> **5. Step 5: PCA Application**\n> * Principal Components Selection\n> \n> **6. Step 6: Model Building**\n> * K-means Clustering\n> * Elbow Curve\n> * Silhouette Analysis\n> * Hierarchial Clustering\n> \n> **7. Step 7: Final Analysis**\n> * Visualization in a Map\n\n","124cc29b":"**Inference:**\nLooking at the above elbow curve it looks good to proceed with either 3 or 4 clusters.\n\n\n\n**Silhouette Analysis**\n \nThe value of the silhouette score range lies between -1 to 1.\n\nA score closer to 1 indicates that the data point is very similar to other data points in the cluster,\n\nA score closer to -1 indicates that the data point is not similar to the data points in its cluster.","c486ae2d":"**CAH with 3 clusters.**","3df23279":"**Corelation all numeric columns**","173c67fc":"**Inference:**\nIt is evident from the above Scree plot that more than 92% variance is explained by the first 3 principal components. Hence, we will use these components only going forward for Clustering process.","70e6005e":"**Incremental principal component analysis (IPCA)** is typically used as a replacement for principal component analysis (PCA) when the dataset to be decomposed is too large to fit in memory. ... It is still dependent on the input data features, but changing the batch size allows for control of memory usage.","eb000bcd":"**Inference:**\n*  PIMP14 and MED14 are is well explained by PC3.\n\nSince 92% variance is explained by 3 principal components, lets build the dataframe using those 3 components only","cbd454aa":"**Inference:**\nAs we can see from above heatmap that the correlation among the attributes is almost 0, we can proceed with this dataframe.","6f5e9dee":"**Complete Linkage Clustering**\n\nIn complete linkage hierarchical clustering, the distance between two clusters is defined as the longest distance between two points in each cluster.","a83b008b":"The dataset seems to be almost clean and hence no such cleansing activities are required.","1c6477d0":"# Step 2 : Data Cleansing","718a6798":"Looking at the silhouette_score it looks good to proceed with either 3 .","c6c3dccf":"# Step 3 : Data Visualization","9f816ef1":"**Inference:**\n* None of the columns have inconsistent datatype, hence no conversion is required.","2c9f9c89":"**Single Linkage Clustering**\n\nIn single linkage hierarchical clustering, the distance between two clusters is defined as the shortest distance between two points in each cluster. ","c338745a":"**Inference:**\n* With first component variance explained is almost 80%.\n* For second component variance explained is almost 8%.\n* For  component variance explained is almost 2%.","e4d597a1":"**Outlier Analysis and Treatment**\n<br>\nThere are 2 types of outliers and we will treat outliers as it can skew our dataset\u00b6\n\n* Statistical\n* Domain specific","de6f0477":"**Inference:**\n* ETPE15, ETOQ15, ETGZ15, P14_LOGVAC,P14_LOG and NAIS0914 well explained by PC1.\n* P14_RESECOCC well explained by both the components PC1 and PC2.\n* SUPERF and ETAZ15  are well explained by PC2.\n* PIMP14 and MED14 are neither explained by PC1 nor with PC2","f323bbb3":"**Rescaling the Features**\n<br>\nit's important to do standardisation\/normalisation. There are two common ways of rescaling:\n\n1. Min-Max scaling\n1. Standardisation (mean-0, sigma-1)\n\nHere, we will use Standardisation Scaling.","228b7c6d":"# Step 5 : PCA Application\nWe are doing PCA because we want to remove the redundancies in the data and find the most important directions where the data was aligned.\n\nPrincipal component analysis (PCA) is one of the most commonly used dimensionality reduction techniques in the industry. By converting large data sets into smaller ones containing fewer variables, it helps in improving model performance, visualising complex data sets, and in many more areas.\n\nLet's use PCA for dimensionality reduction as from the heatmap it is evident that correlation exists between the attributes.","61fb86ed":"**We have analyzed both K-means and Hierarchial clustering and found clusters formed are not really identical. The clusters formed in both the cases are not that great but its better in K-means as compared to Hierarchial.So, we will proceed with the clusters formed by K-means and based on the information provided by the final clusters we will visualize the data in the map**"}}