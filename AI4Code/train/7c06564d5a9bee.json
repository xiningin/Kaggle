{"cell_type":{"cafa0020":"code","78f1b9eb":"code","95c0a206":"code","5d689136":"code","4e1fec6b":"code","33e9664c":"code","af07f31a":"code","55e310e6":"markdown","1ab7fae0":"markdown","1f994e24":"markdown","d311ccc2":"markdown","a118fad7":"markdown","9690bed1":"markdown","b40f246e":"markdown","7885db95":"markdown"},"source":{"cafa0020":"import cudf \nimport numpy as np\nimport pandas as pd\n\n#@ Plotly import \nimport plotly.io as pio               \n#@ Not using Plotly express \nimport plotly.graph_objs as go      \n#@ Graph object has more customizations\nfrom plotly.offline import iplot\n#@ ggplot2 theme for plotly\npio.templates.default = \"ggplot2\"  \n\n#@ Importing environment\nimport janestreet\n#@ Initialize the environment\nenv = janestreet.make_env() \n#@ An iterator which loops over the test\niter_test = env.iter_test() \n\n#@ Classifier import\nimport xgboost as xgb\n\n#@ Clean progress bar\nfrom tqdm.notebook import tqdm\n\n#@ Numba is an open source JIT compiler that translates a subset of Python and NumPy code into fast machine code\nfrom numba import njit","78f1b9eb":"print(\"Reading dataset using CUDA dataframes ...\", end='')\n#@ Parsing the training dataset by using RAPIDSAI cudf library\ntrain_cudf = cudf.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv')\n#@ Converting to a pandas dataframe \ntrain_data = train_cudf.to_pandas()\n#@ Deleting training variable to save memory\ndel train_cudf\n\n#@ Parsing the meta-dataset by using RAPIDSAI cudf library\nmeta_cudf = cudf.read_csv('\/kaggle\/input\/jane-street-market-prediction\/features.csv')\n#@ Converting to a pandas dataframe \nmeta_data = meta_cudf.to_pandas()\n#@ Deleting meta-data variables to save memory\ndel meta_cudf\n\n#@ Parsing sample predictions\nsample_prediction_df = pd.read_csv('..\/input\/jane-street-market-prediction\/example_sample_submission.csv') \n\nprint('Finished.\\n')\n\n#@ Printing out training and meta-data shapes\nprint(f'Train shape: {format(train_data.shape)}')\nprint(f'Features meta shape: {format(meta_data.shape)}')","95c0a206":"print('Preprocessing data...', end='')\n\n#@ Storing columns with the word feature included\nfeatures = [c for c in train_data.columns if 'feature' in c]\n\n#@ Trades with weight=0 are not considered for scoring evaluation\ntrain_data = train_data[train_data['weight'] > 0].reset_index(drop = True)\n\n#@ Filling nan using ffill method\ntrain_data[features] = train_data[features].fillna(method = 'ffill').fillna(0)\n\n#@ Only considering the target column values > 0 \ntrain_data['action'] = (train_data['resp'].values > 0).astype(int)\n\nprint('Finished.')","5d689136":"x = train_data['action'].value_counts().index\ny = train_data['action'].value_counts().values\n\ntrace = go.Bar(x=x,\n               y=y,\n               marker=dict(\n               color=y,\n               colorscale='sunsetdark'))   \n    \ndata = [trace]\nlayout = go.Layout(showlegend=False,\n                   title='<b>Is the target balanced or Not?<\/b>',\n                   xaxis=dict(title='<b>Action<\/b>'),\n                   yaxis=dict(title='<b>Count<\/b>'))\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig)  \n\n#@ Deleting unnecessary variables to save memory\ndel(x, y)  ","4e1fec6b":"#@ XGBoost Classifier with GPU support \nprint('Creating XGBclassifier...\\n', end='')\n\n#@ Setting up hyper-parameters in a pretty formatted way\nparameters = {'max_depth': 8,\n              'learning_rate': 0.015,\n              'random_state': 42,\n              'tree_method': 'gpu_hist',\n              'min_child_weight': 0.30,\n              'subsample': 0.46,\n              'colsample_bytree': 0.99,\n              'eval_metric': 'auc',\n              'gamma': 9.8,\n              'objective': 'binary:logistic'}\n\n#@ Setting up training variables \nX_train = train_data.loc[train_data['date'] > 80, features].values\ny_train = train_data.loc[train_data['date'] > 80, 'action'].values\n\n#@ Loading numpy arrays into DMatrix\nd_train = xgb.DMatrix(X_train, y_train)\n#@ Fitting the classifier with hyper-parameters and training variables\n%time clf = xgb.train(parameters, d_train, 1175)   \n\nprint('Finished training the classifier.') ","33e9664c":"#@ Utitlity function for submittion using njit\n@njit\ndef fast_fillna(array, values):\n    if np.isnan(array.sum()):\n        array = np.where(np.isnan(array), values, array)\n    return array\n    \ntrain_data.loc[0, features[1:]] = fast_fillna(train_data.loc[0, features[1:]].values, 0) ","af07f31a":"tmp = np.zeros(len(features))\nfor (test_df, sample_prediction_df) in tqdm(iter_test):\n    if test_df['weight'].item() > 0:\n        X_test = test_df.loc[:, features].values\n        X_test[0, :] = fast_fillna(X_test[0, :], tmp)\n        tmp = X_test[0, :]\n        #@ Converting pandas df to DMatrix\n        d_test = xgb.DMatrix(X_test)\n        #@ Submitting xgb model predictions\n        y_preds = clf.predict(d_test) \n        sample_prediction_df.action = np.where(y_preds >= 0.5, 1, 0).astype(int)\n        \n    else:\n        sample_prediction_df.action = 0\n\n    env.predict(sample_prediction_df) ","55e310e6":"# **4. Data Visualization**\n\nWe'll now plot the target column distribution using [plotly](https:\/\/plotly.com\/python\/).\n\n","1ab7fae0":"# **3. Preprocessing the data**\n\nFirst, I'll be training the rows with weight > 0 (you can read the [data description](https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/data) for more details), store the mean of indivitual columns, fill the null values with indivitual column means, setup the training dataframes and variables, finally splitting the dataset for model evaluation.\n\n","1f994e24":"# **6. Submitting**","d311ccc2":"## If you liked this notebook, please make sure to upvote this kernel \u2b06\ufe0f. \ud83d\udcac Connect? Let\u2019s get social: http:\/\/myurls.co\/nakshatrasinghh.","a118fad7":"# **2. Reading the data**\n\nI'll be using the [cuDF library](https:\/\/github.com\/rapidsai\/cudf) by RAPIDSAI. cuDF provides a pandas-like API that will be familiar to data engineers & data scientists, so they can use it to easily accelerate their workflows without going into the details of CUDA programming. This library does a lot of heavy lifting for us.","9690bed1":"# **5. XGBoost Classifier** \n\nLet's start by using XGBoost as our first boosting classifier to build our Machine Learning Model. You can also try various **cross-validation techiniques** (like, [RandomizedSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html), [GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html), [StratifiedKFold](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedKFold.html), and many more..) to optimize your hyperparameters.\n\n{[XGBOOST - Official Documentation](https:\/\/xgboost.readthedocs.io\/en\/latest\/)}","b40f246e":"# **Jane Street Market Prediction using XGBoost Algorithm with GPU \ud83d\ude80\u26a1** \n\nHey everyone, this is my very first competition i'm taking part in. I want to thank Jane Street and Kaggle for making this competition available to all of us. \n\nThanks everyone, and good luck!\n\n**- By Nakshatra Singh**","7885db95":"# **1. Loading necessary libraries and dependencies**\n\nAll imports are delineated below for easy reference. Make sure you have selected the **`gpu`** accelerator instance for this notebook. Click on `+Add data` (by toggling the sidebar) and add the the [Jane Street Market dataset](https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/data). \n\nNow, you are all set-up to run this worksheet. \ud83e\udd17"}}