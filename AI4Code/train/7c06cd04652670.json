{"cell_type":{"9f4e9503":"code","5343277d":"code","3027b0a7":"code","be02f4c5":"code","70216746":"code","cf77e613":"code","c3932de9":"code","8805ca82":"code","875a3531":"code","3c808b0c":"code","60f811cd":"code","2b64bd36":"code","eb5f5e2d":"code","0562f609":"code","c9412872":"code","ccfc873f":"code","0b9e30dc":"code","2876f3cd":"code","f301fb3d":"code","38c57836":"code","82fd472f":"code","de0f54e3":"code","8157c4a1":"code","7a2b52f8":"markdown","7005be56":"markdown","ed9216a5":"markdown","8c31451a":"markdown","5ed8ed9b":"markdown","e3edcceb":"markdown","b997bd42":"markdown","831d3e79":"markdown","db51b848":"markdown","d83b372e":"markdown"},"source":{"9f4e9503":"#nltk.download()\n!pip install textmining3\n!pip install vaderSentiment","5343277d":"# import required \nimport pandas as pd\nimport csv\nfrom textblob import TextBlob\n#from textblob.sentiments import NaiveBayesAnalyzers\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nfrom nltk.corpus import stopwords\nimport string\nimport nltk\nimport textmining\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud,STOPWORDS\n# set the working dierectory\nimport os\nos.chdir(\"..\/input\")","3027b0a7":"# load the dataset\npost=pd.read_csv(\"Post.csv\")\npost.head(20)","be02f4c5":"# select few text\npost=post.iloc[:1000,]\npost.shape","70216746":"# define the stopwords and also import predefined ones\nstop=set(stopwords.words(\"english\"))\n# Extract punctuation marks\npunct_exclude=set(string.punctuation)","cf77e613":"# perform preoprocessing using function below\ndef clean(doc):\n    stop_free=\" \".join([i for i in doc.lower().split() if i not in stop])\n    punct_free=''.join(ch for ch in stop_free if ch not in punct_exclude)\n    num_free=''.join(i for i in punct_free if not i.isdigit())\n    return num_free\n\npost_corpus=[clean(post.iloc[i,1]) for i in range(0,post.shape[0])]","c3932de9":"print(type(post_corpus))\nprint(post_corpus[14])","8805ca82":"# create term document matrix\ntdm=textmining.TermDocumentMatrix() # use a function from textmining library\nfor i in post_corpus:\n    #print(i)\n    tdm.add_doc(i)# update the matrix with each variable conversion","875a3531":"type(tdm)","3c808b0c":"os.chdir(\"..\/working\")","60f811cd":"\n#write tdm into dataframe\ntdm.write_csv(\"TDM_DataFrame.csv\",cutoff= 1) #cutoff won't consider 1st line","2b64bd36":"def buildMatrix(self,document_list):\n        print(\"building matrix...\")\n        tdm = textmining.TermDocumentMatrix()\n        for doc in document_list:\n             tdm.add_doc(doc)\n        #write tdm into dataframe\n        tdm.write_csv(r'path\\matrix.csv', cutoff=1)","eb5f5e2d":"df=pd.read_csv(\"TDM_DataFrame.csv\")","0562f609":"df.head(20)","c9412872":"post.shape","ccfc873f":"df.shape","0b9e30dc":"#plot wordcloud\nwordcloud= WordCloud(width=1000,height=500, stopwords=STOPWORDS, background_color='white').generate(''.join(post['Post']))\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()\n","2876f3cd":"# Sentiment Analysis TextBlob implementation\n# create an empty dataframe to store the results\nFinalResult=pd.DataFrame()\nfor i in range(0,post.shape[0]):\n    #print(i)\n    blob=TextBlob(post.iloc[i,1])\n    temp=pd.DataFrame({\"Comments\":post.iloc[i,1],\"Polarity\":blob.sentiment.polarity},index=[0])\n    FinalResult=FinalResult.append(temp)","f301fb3d":"print(FinalResult.shape)\nFinalResult.head()","38c57836":"FinalResult.describe()","82fd472f":"# Sentiment Analysis using \"VADER\"\n# create an empty dataframe to store the results\nFinalResult_vader=pd.DataFrame()\n# initialize the engine\nanalyzer=SentimentIntensityAnalyzer()\nfor i in range(0,post.shape[0]):\n    #print(i)\n    snt=analyzer.polarity_scores(post.iloc[i,1]) # gives the output in dictionary form shown below\n    temp=pd.DataFrame({\"Comments\":post.iloc[i,1],\"Polarity\":snt.items()},index=[0])\n    FinalResult_vader=FinalResult_vader.append(temp)\n","de0f54e3":"snt","8157c4a1":"FinalResult_vader.head()","7a2b52f8":"## Text Mining Models:\n> **Sentiment analysis** is **the detection of attitude** of a speaker with respect to some topic or **overall contextual polarity of a document**.\nSentiment Analysis, or Opinion Mining, is a sub-field of Natural Language Processing (NLP) that tries to identify and extract opinions within a given text. The aim of sentiment analysis is to gauge the attitude, sentiments, evaluations, attitudes and emotions of a speaker\/writer based on the computational treatment of subjectivity in a text.\n>Sentiment analysis can be implemented in python using various libraries such as **textblob, vader, Naive Bayes analyser**,etc\n>> * VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. **It doesn\u2019t require any training data** \n> * The VADER algorithm outputs sentiment scores to 4 classes of sentiments:\n>>> * neg: Negative \n>>>* neu: Neutral  \n>>>* pos: Positive \n>>>* compound: Compound (i.e. aggregated score)(Note that compound score is not calculated based on above scores)\n>>>*  Typical threshold values are:\n       >>>> * positive sentiment: compound score >= 0.05\n       >>>> * neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n       >>>> * negative sentiment: compound score <= -0.05\n\n\n\n","7005be56":"If you go back and look at the the 15th sentence (ID=15), you will that the overall sentence is something like this \"then 9.4 then 7.2 NOW 6.2\" and that's why print(post_corpus[14]) is not showing any words","ed9216a5":"> ### Note:\nNow we have a kind of cleaned but unstructered data. To convert such unstructured data into a structured one, we use TDM(term document matrix).\n\n## Term Document Matrix\n> *  A collection of 'n' documents can be represented in the vector space model by a term-document matrix. \n* It creates a numerical representation of the documents in our corpus. It is easy to determine individual word counts for each document or for all documents. It is a fairly simple way to represent the documents as a numeric structure. Representing text as a numerical structure is a common starting point for text mining and analytics such as search and ranking, creating taxonomies, categorization, document similarity, and  text-based machine learning.\n * An entry in the matrix corresponds to the **\u201cweight\u201d** of a term in the document; zero means the term has no significance in the document or it simply doesn\u2019t exist in the document.\n * A typical weighting is tf-idf weighting:  **W = tf* idf**\n * **Term Frequency (TF)**: More frequent terms in a document are more important, i.e. more indicative of the topic. May want to normalize term frequency(tf) across the entire corpus:\n* **TF = (Number of times term t appears in a document) \/ (Total number of terms in the document)**\n* **Inverse Document Frequency (IDF)**: Terms that appear in many different documents are less indicative of overall topic.\n* **IDF = log10(Total number of documents \/ Number of documents with term t in it)**\n","8c31451a":"> ### 1) Punctuation Marks:\nRemoving all punctuation marks . There are different libraries used for removing punctuation marks.\n> ### 2) Numbers : \nRemove all the numbers or transform them into words from  the text data that includes timeline, dates, ip addresses, etc.\n> ### 3) Case folding: \nConverting all letters to lower-case.\n> ### 4) Stop Words (removing unnecessary words): \nIt generally includes removing high frequency words such as (a, an, the, all pronouns, etc). This step is highly depended on language. Stop words are stored in hash table according to the domain knowledge. Stop words are predefined in for English language in most of the libraries. But if any word is found not giving important information can also be labelled as stop word manually along with the predefined ones. There is no universal stopword list, but a standard English language stopwords list from nltk librsry can be used. Also, domain-specific stopwords can be added.\n      * E.g. SMART's common word list\n      * WordNet stopword list  http:\/\/www.ranks.nl\/resources\/stopwords.html\n> ### 5) White spaces: \nRemoving white spaces that may be present at the start or end of the sentences\/words or there can be extra spaces at anywhere in the sentence.\n> ### 6) Stemming :\nReducing the tokens to root forms to recognize morphological variations. It blindly strips off any prefixes or postfixes in iterative manner.\n> ### 7) Lemmatization: \nConverting variant forms to the base forms. This directly impacts vocabulary size. This prevents duplication of data by linking words to root word. For example, \u201cam\u201d , \u201care\u201d are linked to \u201cbe\u201d. To achieve this, we need list grammatical rules and a list of  irregular words. Practically , use Wordnet\u2019s \u201cmorphstr\u201d function.\n> ### 8) Synonym check: \nWords with the same form and multiple related meanings (e.g., bank: a financial institution and bank: to rely upon, as in\n&quot;You can bank on me&quot;) can't be treated as one entity. Words with same meaning can only be treated as one entity. \n> ### 9) Tokenization:\nThe process of breaking down a text paragraph into smaller chunks such as words or sentence is called Tokenization. Token is a single entity that is building blocks for sentence or paragraph. It's nothing but converting text into tokens. Splitting whole corpus into smaller parts(usually a word). (eg. Bag of words model, N-Gram model).Tokenizing can be simply achieved by splitting the text into white spaces.\n> * Input : collection of documents having text data.\n> * Output: Tokens which are to be added to the index. (No punctucation, no stop- words, stemmed)\n","5ed8ed9b":"**Thus there are 7393 unique words derived from all documents**","e3edcceb":"**Thus in TDM, each word gets converted into independent variables(column names) and each row represent a document.**","b997bd42":"# Exploration of Text Mining Library Textblob","831d3e79":"* NOTE:\n>* If polarity > 0 then it's a positively polarized comment\n>* If polarity > 0 then it's a negatively polarized  comment\n>* If polarity =0 then it's a neutrally polarized  comment","db51b848":"## Word Cloud: \n> * Word clouds are normally used to display the frequency of appearance of words in a particular document or speech\n* More frequently used words appear larger in the word cloud.\n* The frequency is assumed to reflect the importance of the term in the context of the document.\n* A Word Cloud or Tag Cloud is a visual representation of text data in the form of tags, which are typically single words whose importance is visualized by way of their size and color. As unstructured data in the form of text continues to see unprecedented growth, especially within the field of social media, there is an ever-increasing need to analyze the massive amounts of text generated from these systems. A Word Cloud is an excellent option to help visually interpret text and is useful in quickly gaining insight into the most prominent items in a given text, by visualizing the word frequency in the text as a weighted list\n","d83b372e":"# Text Mining:\n* It\u2019s the process of extracting non-trivial, high quality and interesting info from unstructured text.\n* Corpus: a collection of written texts, especially the entire works of a particular author or a body of writing on a particular subject. (group of docs, group of texts, group of tweets, etc)\n* It\u2019s framework is similar to ETL (Extract, Transform, Load)\n* Why used? : Well 80% of world\u2019s data stored is text data.\n* **Text Mining Process**:\n>Text mining process starts with pre-processing. That transforms input raw text in structured information. Text pre-processing input is unstructured data or semi structured data such as HTML pages. The data submitted to this process is cleaned and useful features are recovered. The output is stored in database or any other structured format. Text Mining techniques are applied to this structured form of data. The different data mining algorithms can be applicable to this data to model text data with applications such as information retrieval system.\n![text_mining_process](https:\/\/learning.maxtech4u.com\/wp-content\/uploads\/2017\/08\/Text-mining-process.png)\n* **Text Mining Areas**:\n> * Information Retrieval (IR):\nInformation retrieval is regarded as an extension to document retrieval where the documents that are returned are processed to condense or extract the particular information sought by the user. Thus document retrieval could be followed by a text summarization stage that focuses on the query posed by the user, or an information extraction stage using techniques. IR systems help to narrow down the set of documents that are relevant to a particular problem. As text mining involves applying very complex algorithms to large document collections, IR can speed up the analysis significantly by reducing the number of documents for analysis.\n> * Data Mining (DM):\nData mining can be simply described as looking for patterns in data. It can be more fully characterized as the extraction of hidden, previously unknown, and useful information from data. Data mining tools can predict behaviors and future trends, allowing businesses to make positive, knowledge based decisions. Data mining tools can answer business questions that have traditionally been too time consuming to resolve. They search databases for hidden and unknown patterns, finding critical information that experts may miss because it lies outside their expectations. The overall goal of the data mining process is to extract information from a data set and transform it into an understandable structure for further use.\n> * Natural Language Processing (NLP):\nNLP is one of the oldest, widest and most challenging problems in the field of artificial intelligence. It is the study of human language so that computers can understand natural languages as humans do. NLP research pursues the vague question of how we understand the meaning of a sentence or a document. The role of NLP in text mining is to deliver the system in the information extraction phase as an input. \n> * Information Extraction (IE): Information Extraction is the task of automatically extracting structured information from unstructured and\/or semi-structured machine-readable documents. In most of the cases, this activity includes processing human language texts by means of natural language processing (NLP). The recent activities in multimedia document processing like automatic annotation and mining information out of images\/audio\/video could be seen as information extraction and the best practical and live example of IE is Google Search Engine. It involves defining the general form of the information that we are interested in as one or more templates, which are used to guide the extraction process. IE systems greatly depend on the data generated by NLP systems.\n![text mining areas](https:\/\/pbs.twimg.com\/media\/DM9aRh6XUAAhTfX.jpg)\n### Text Pre-processing:\n> First import the required libraries for text procesing. If you get any error while installing \"textblob\" package, go through this link https:\/\/stackoverflow.com\/questions\/15717752\/python3-3-importerror-with-textmining-1-0\n> * (before doing that download the nltk modules using this command: nltk.download(), it will take a lot of time to dwonload everything, so just look for \"stopwords\",\"corpus\",\"\" packages )"}}