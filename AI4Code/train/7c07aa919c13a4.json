{"cell_type":{"f100e714":"code","a488dab1":"code","64b6845e":"code","f928c26b":"code","30308560":"code","03a6fe32":"code","9a7abb7d":"code","997218c7":"code","6f4753ac":"code","20600ac8":"code","b8282e16":"code","2cd80fa6":"code","cb74fb77":"code","d5f9dbd2":"code","c09c3e16":"code","76d8b8a2":"code","0c76a2ea":"code","b83814e1":"code","b0138195":"code","36db23cf":"code","8014c423":"code","3839c011":"code","17b42239":"code","8be015e9":"code","3321dc84":"code","7bfcc2fd":"code","2a9928d1":"code","f2db7970":"code","5604dc71":"code","e69c13dd":"code","8e71af7c":"markdown","78da89b5":"markdown","910d5d07":"markdown","f588d206":"markdown","0aba8b2e":"markdown","0571a3c7":"markdown","b24b00e0":"markdown","a77cfd39":"markdown","2d9ef423":"markdown","0186f121":"markdown","60f1befe":"markdown","4cff01a3":"markdown","e4159000":"markdown","caa5ee1e":"markdown","584d1cf9":"markdown","2b4e09cd":"markdown","f9e2d87c":"markdown","9810dc4b":"markdown","67cdfea2":"markdown","cbd3a04d":"markdown","1f8c364e":"markdown","d2d37a6e":"markdown","54acdad0":"markdown","5cf8df5b":"markdown","60ee20fe":"markdown","da5988d0":"markdown","6de086e3":"markdown","ad65567a":"markdown","7427aab4":"markdown","aed169cf":"markdown","e6ee6662":"markdown","99f681f8":"markdown"},"source":{"f100e714":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a488dab1":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport random\nfrom matplotlib import pyplot as plt\nfrom collections import Counter\nfrom sklearn.utils import resample\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers\nfrom keras.models import  Sequential\nfrom keras.layers.core import  Lambda , Dense, Flatten, Dropout\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import BatchNormalization, Convolution2D , MaxPooling2D\nfrom sklearn.decomposition import PCA\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\",category=plt.cbook.mplDeprecation)\nrandom.seed(54)","64b6845e":"train=pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')","f928c26b":"train.head()","30308560":"train.shape","03a6fe32":"train.isnull().sum()","9a7abb7d":"train.dtypes","997218c7":"fig=plt.figure(figsize=(16,16))\nfig = plt.subplot(531)\nsns.histplot(train['age'],kde=True,label=' Age',color='pink')\n\nfig = plt.subplot(532)\nx=train['sex'].value_counts()\nx.plot(kind='bar',label='sex',color='pink',xlabel='sex',ylabel='Density')\n\nfig = plt.subplot(533)\nx=train['cp'].value_counts()\nx.plot(kind='bar',label='cp',color='pink',xlabel='cp',ylabel='Density')\n\nfig = plt.subplot(534)\nsns.histplot(train['trtbps'],color='lightgreen',kde=True)\n\nfig = plt.subplot(535)\nsns.histplot(train['chol'],kde=True,label='Cholestrol',color='green')\n\nfig = plt.subplot(536)\nx=train['fbs'].value_counts()\nx.plot(kind='bar',label='fbs',xlabel='fbs',color='lightgreen',ylabel='Density')\n\nfig = plt.subplot(537)\nx=train['restecg'].value_counts()\nx.plot(kind='bar',label='restecg',xlabel='restecg',color='lightblue',ylabel='Density')\n\nfig = plt.subplot(538)\nsns.histplot(train['thalachh'],kde=True, label='thalachh')\n\n\nfig = plt.subplot(539)\nx=train['exng'].value_counts()\nx.plot(kind='bar',label='exng',xlabel='exng',ylabel='Density')\n\nfig = plt.subplot(5,3,10)\nsns.histplot(train['oldpeak'],color='pink', label='oldpeak')\n\nfig = plt.subplot(5,3,11)\nx=train['slp'].value_counts()\nx.plot(kind='bar',label='slp',xlabel='slp',color='pink',ylabel='Density')\n\nfig = plt.subplot(5,3,12)\nx=train['caa'].value_counts()\nx.plot(kind='bar',label='caa',xlabel='caa',color='pink',ylabel='Density')\n\n\nfig = plt.subplot(5,3,13)\nx=train['thall'].value_counts()\nx.plot(kind='bar',label='thall',xlabel='thall',color='green',ylabel='Density')\n\nfig = plt.subplot(5,3,14)\nx=train['output'].value_counts()\nx.plot(kind='bar',label='output',xlabel='output',color='green',ylabel='Density')\n\n\nplt.show()","6f4753ac":"fig=plt.figure(figsize=(10,10))\nsns.heatmap(train.corr(),cmap='coolwarm')","20600ac8":"fig=plt.figure(figsize=(16,16))\nsns.catplot(x=\"cp\",y='thalachh',hue='output',kind='box',data=train)","b8282e16":"fig=plt.figure(figsize=(5,5))\nsns.barplot(x=train['output'],y=train['thalachh'])","2cd80fa6":"def Resample(data):\n    label_0=data[data.output==0] #0\n    label_1=data[data.output==1] #1\n   \n    # upsample minority\n    label_0_upsampled = resample(label_0,\n                              replace=True, # sample with replacement\n                              n_samples=len(label_1), # match number in majority class\n                              random_state=27) # reproducible results\n    \n    # combine majority and upsampled minority\n    upsampled = pd.concat([label_1, label_0_upsampled])\n    return upsampled","cb74fb77":"train=Resample(train)","d5f9dbd2":"train.shape","c09c3e16":"x=train['output'].value_counts().values\nplot=sns.barplot([\"0\",\"1\"],x)\nplot.set(xlabel='Output', ylabel='Number of Data')\nplt.show()","76d8b8a2":"def normalize(x):\n\n  z=(x-x.mean())\/x.std()\n\n  return z","0c76a2ea":"labels=train['output']\ntrain=train.drop(['output'],axis=1)\ntrain=normalize(train)","b83814e1":"train.head()","b0138195":"x_train, x_test, y_train, y_test = train_test_split(train, labels, test_size=.2, random_state=1,stratify=labels)","36db23cf":"model=keras.Sequential([\n     layers.Dense(32,activation='relu',input_shape=[13]),\n     layers.Dense(64,activation='relu'),\n     layers.Dense(128,activation='relu'),\n     layers.Dense(128,activation='relu'),\n     layers.Dense(1)\n\n])","8014c423":"model.compile( \n    optimizer=keras.optimizers.Adam(lr=0.001),\n    loss='mae',\n    metrics=['accuracy']\n)","3839c011":"early_stopping= keras.callbacks.EarlyStopping(monitor='val_accuracy',mode='auto',\n                                    patience=20,restore_best_weights=True)","17b42239":"lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy',mode='auto',factor=0.5,patience=5)","8be015e9":"model.fit(x_train,y_train,validation_data=(x_test,y_test),batch_size=28,epochs=200,callbacks=[early_stopping,lr_scheduler])","3321dc84":"model.evaluate(x_test,y_test,verbose=2)","7bfcc2fd":"y_pred=model.predict(x_test).astype(\"int32\")","2a9928d1":"cf=confusion_matrix(y_test,y_pred)\nsns.heatmap(cf, annot=True).set(xlabel='Actual values',ylabel='Predict values')","f2db7970":"recall_score(y_test,y_pred,average='macro')","5604dc71":"f1_score(y_test,y_pred,average='macro')","e69c13dd":"precision_score(y_test,y_pred,average='macro')","8e71af7c":"# Neural Network Model","78da89b5":"Precision Result","910d5d07":"Fitting model","f588d206":"Importing important libraries for project","0aba8b2e":"Labels are unbalanced so we use the over- resampling method to solve this problem. Thus, each label has the same sample size.","0571a3c7":"Confusion Matrix","b24b00e0":"Output labels are unbalanced.","a77cfd39":"Show first 5 rows in data","2d9ef423":"Predicted label for performance metrics","0186f121":"Visualizations of data","60f1befe":"# Data Information","4cff01a3":"Data type of each column","e4159000":"Recal Result","caa5ee1e":"Compiling model using adam optimizer, loss function and accuracy metric","584d1cf9":"Data shape 303 rows and 14 columns","2b4e09cd":"F1 score Result","f9e2d87c":"Stop training when accuracy is not improved 20 consecutive times","9810dc4b":"Data split into 80% train and 20%","67cdfea2":"Total number of null values","cbd3a04d":"Call the resample function","1f8c364e":"Reducing learning rate when a metric has stopped improving 5 consecutive times","d2d37a6e":"Evaluating on the test data","54acdad0":"Using z-normalization in order to scaling the values","5cf8df5b":"Read csv file using pandas libray","60ee20fe":"Dropping output labels for normalization","da5988d0":"# Preprocessing","6de086e3":"Create sequential model","ad65567a":"# Cross Validation","7427aab4":"The new shape of data","aed169cf":"Heat map for attributes","e6ee6662":"We have seen clearly that chest pain type are a positive relationship with heart rate.","99f681f8":"# Import Libraries"}}