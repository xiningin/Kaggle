{"cell_type":{"f25ce731":"code","015f2a98":"code","ceef21f1":"code","265f06ac":"code","b4617cfb":"code","66996025":"code","e1e5e8cf":"code","8cb42bcb":"code","eecb52e1":"code","0bc38e07":"code","eb804033":"code","0632b96b":"code","2b15ded5":"code","71b505b9":"code","457c946e":"code","b41540c2":"code","23cf0d40":"code","e2e3a39e":"code","9bfd72fa":"code","89c80d5c":"code","6853ccc8":"code","507ac5e4":"code","8ecf37ca":"code","d7f5fdd3":"code","2428382c":"code","25ce7a19":"markdown","4c8667d5":"markdown"},"source":{"f25ce731":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","015f2a98":"#----DATA PREPARATION----\nraw = pd.read_csv('..\/input\/noaa-gsod-rain-prediction\/compiled_raw_y.csv', sep = ',')\nraw = raw.dropna()\n\ny = pd.get_dummies(raw['Y'], columns = ['Y'], prefix = ['Y'], drop_first=True)\ny1 = pd.get_dummies(raw['Y+1'], columns = ['Y+1'], prefix = ['Y+1'], drop_first=True)\ny2 = pd.get_dummies(raw['Y+2'], columns = ['Y+2'], prefix = ['Y+2'], drop_first=True)\ny3 = pd.get_dummies(raw['Y+3'], columns = ['Y+3'], prefix = ['Y+3'], drop_first=True)\ny4 = pd.get_dummies(raw['Y+4'], columns = ['Y+4'], prefix = ['Y+4'], drop_first=True)\ny5 = pd.get_dummies(raw['Y+5'], columns = ['Y+5'], prefix = ['Y+5'], drop_first=True)\ny6 = pd.get_dummies(raw['Y+6'], columns = ['Y+6'], prefix = ['Y+6'], drop_first=True)\ny7 = pd.get_dummies(raw['Y+7'], columns = ['Y+7'], prefix = ['Y+7'], drop_first=True)\n\nraw_loc = raw.iloc[:,1:-8]\nraw_loc = raw_loc.drop(columns='DATE', axis=1)\nraw_loc = raw_loc.drop(columns=\"LATITUDE\", axis=1)\nraw_loc = raw_loc.drop(columns='LONGITUDE',axis=1)\nraw_loc = raw_loc.drop(columns='FRSHTT',axis=1)\nraw_loc = raw_loc.drop(columns='PRCP',axis=1)\nraw_loc = raw_loc.drop(columns='WEEKDAY',axis=1)\n\nraw_loc.head()","ceef21f1":"rawCol = [y,y1,y2,y3,y4,y5,y6,y7]\nprint(rawCol)","265f06ac":"y1.head()","b4617cfb":"\n# Encode and Categorical boolean mask\ncategorical_feature_mask = raw_loc.dtypes==object\n\n# filter categorical columns using mask and turn it into a list\ncategorical_cols = raw_loc.columns[categorical_feature_mask].tolist()\ncategorical_cols\n","66996025":"for i in categorical_cols:\n    raw_dummie = pd.get_dummies(raw_loc[i])\n    raw_loc = raw_loc.drop(columns=i)","e1e5e8cf":"# raw_weekday = pd.get_dummies(raw_loc['WEEKDAY'])\n#Drop this line since weekday does not have a significant impact ont he weather\nraw_month = pd.get_dummies(raw_loc['MONTH'], prefix = 'Month')\nraw_loc = raw_loc.drop(columns='MONTH')","8cb42bcb":"raw_month.head()","eecb52e1":"raw_loc.head()","0bc38e07":"raw_dummie.shape","eb804033":"raw_loc.info()","0632b96b":"for i in raw_loc:\n    print(i)","2b15ded5":"raw_loc = raw_loc.drop(columns='GUST', axis=1)\nraw_loc = raw_loc.drop(columns='SNDP', axis=1)\nraw_loc = raw_loc.drop(columns='STP', axis=1)","71b505b9":"raw_loc.head()","457c946e":"#Outlier Checking\nfor i in raw_loc:\n    print('1\u00ba Quartile: ',i, raw_loc[i].quantile(q = 0.25))\n    print('2\u00ba Quartile: ',i, raw_loc[i].quantile(q = 0.50))\n    print('3\u00ba Quartile: ',i, raw_loc[i].quantile(q = 0.75))\n    print('4\u00ba Quartile: ',i, raw_loc[i].quantile(q = 1.00))\n    print('Mean: ',i, raw_loc[i].mean())","b41540c2":"#----DATA CLEANSING----\nraw_99 = raw_loc[raw_loc['DEWP'] != 9999.9].mean()\nDEWP_99 = raw_99['DEWP']\nraw_99 = raw_loc[raw_loc['MAX'] != 9999.9].mean()\nMAX_99 = raw_99['MAX']\nraw_99 = raw_loc[raw_loc['MIN'] != 9999.9].mean()\nMIN_99 = raw_99['MIN']\nraw_99 = raw_loc[raw_loc['MXSPD'] != 999.9].mean()\nMXSPD_99 = raw_99['MXSPD']\nraw_99 = raw_loc[raw_loc['SLP'] != 9999.9].mean()\nSLP_99 = raw_99['SLP']\nraw_99 = raw_loc[raw_loc['VISIB'] != 999.9].mean()\nVISIB_99 = raw_99['VISIB']\nraw_99 = raw_loc[raw_loc['WDSP'] != 999.9].mean()\nWDSP_99 = raw_99['WDSP']\nprint(WDSP_99)\n#new mean = 2.xx instead which mean the mean number is preserved","23cf0d40":"raw_99.head()","e2e3a39e":"raw_loc.loc[raw_loc['DEWP'] == 9999.9, 'DEWP'] = DEWP_99\nraw_loc.loc[raw_loc['MAX'] == 9999.9, 'MAX'] = MAX_99\nraw_loc.loc[raw_loc['MIN'] == 9999.9, 'MIN'] = MIN_99\nraw_loc.loc[raw_loc['MXSPD'] == 999.9, 'MXSPD'] = MXSPD_99\nraw_loc.loc[raw_loc['SLP'] == 9999.9, 'SLP'] = SLP_99\nraw_loc.loc[raw_loc['VISIB'] == 999.9, 'VISIB'] = VISIB_99\nraw_loc.loc[raw_loc['WDSP'] == 999.9, 'WDSP'] = WDSP_99\nraw_loc.loc[raw_loc['ELEVATION'] == -999.9, 'ELEVATION'] = 380\n# Elevation of the missing station is the factual geographical data, thus we impute 380","9bfd72fa":"for i in raw_loc:\n    print('1\u00ba Quartile: ',i, raw_loc[i].quantile(q = 0.25))\n    print('2\u00ba Quartile: ',i, raw_loc[i].quantile(q = 0.50))\n    print('3\u00ba Quartile: ',i, raw_loc[i].quantile(q = 0.75))\n    print('4\u00ba Quartile: ',i, raw_loc[i].quantile(q = 1.00))\n    print('Mean: ',i, raw_loc[i].mean())","89c80d5c":"rain_final = pd.concat([raw_dummie,raw_month,raw_loc],axis = 1)\nrain_final.shape","6853ccc8":"rain_final.head()","507ac5e4":"rain_final.columns","8ecf37ca":"#Use Logistic Regression to classify Rain and NoRain days\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.linear_model import LogisticRegression\n\nxTrain, xTest, yTrain, yTest = train_test_split(rain_final, y, test_size = 0.3, random_state = 1)\nk_fold = KFold(n_splits = 10, shuffle = True, random_state = 1)\n\nlogmodel = LogisticRegression() \nlogmodel.fit(xTrain,yTrain)\nlogpred = logmodel.predict(xTest)\n\nrn = 0\nprint(f'Confusion Matrix: for Y+{rn}\\n {confusion_matrix(yTest, logpred).ravel()}')\nprint(f'Precision of {round(precision_score(yTest, logpred),4)}')\nprint(f'Recall of {round(recall_score(yTest, logpred),4)}')\nprint(f'Accuracy of {round(accuracy_score(yTest, logpred, normalize=True),4)}')\nprint(f'F of {round(f1_score(yTest, logpred),4)}\\n')\nrn += 1","d7f5fdd3":"# roc curve and auc\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom matplotlib import pyplot\n\nrn = 0\nprecisionList = []\nrecallList = []\nfList = []\nROCList = []\nfor i in rawCol:\n    from sklearn.model_selection import train_test_split\n    from sklearn.model_selection import KFold\n    from sklearn.model_selection import cross_val_score\n    from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n    from sklearn.linear_model import LogisticRegression\n\n    xTrain, xTest, yTrain, yTest = train_test_split(rain_final, i, test_size = 0.3, random_state = 1)\n    k_fold = KFold(n_splits = 10, shuffle = True, random_state = 1)\n\n    logmodel = LogisticRegression() \n    logmodel.fit(xTrain,yTrain)\n    logpred = logmodel.predict(xTest)\n\n    print(f'\\nConfusion Matrix: for Y+{rn}\\n {confusion_matrix(yTest, logpred)}')\n    print(f'Precision of {round(precision_score(yTest, logpred),4)}')\n    print(f'Recall of {round(recall_score(yTest, logpred),4)}')\n    print(f'Accuracy of {round(accuracy_score(yTest, logpred, normalize=True),4)}')\n    print(f'F of {round(f1_score(yTest, logpred),4)}\\n')\n    rn += 1\n\n    precisionList.append(round(precision_score(yTest, logpred),4))\n    recallList.append(round(recall_score(yTest, logpred),4))\n    fList.append(round(f1_score(yTest, logpred),4))\n\n    # generate a no skill prediction (majority class)\n    ns_probs = [0 for _ in range(len(yTest))]\n    # predict probabilities\n    lr_probs = logmodel.predict_proba(xTest)\n    # keep probabilities for the positive outcome only\n    lr_probs = lr_probs[:, 1]\n    # calculate scores\n    ns_auc = roc_auc_score(yTest, ns_probs)\n    lr_auc = roc_auc_score(yTest, lr_probs)\n    # summarize scores\n    print('No Skill: ROC AUC=%.3f' % (ns_auc))\n    print('Logistic: ROC AUC=%.3f' % (lr_auc))\n    ROCList.append(round(lr_auc,4))\n\n    # calculate roc curves\n    ns_fpr, ns_tpr, _ = roc_curve(yTest, ns_probs)\n    lr_fpr, lr_tpr, _ = roc_curve(yTest, lr_probs)\n    # plot the roc curve for the model\n    pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n    pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n    # axis labels\n    pyplot.xlabel('False Positive Rate')\n    pyplot.ylabel('True Positive Rate')\n    # show the legend\n    pyplot.legend()\n    # show the plot\n    pyplot.show()\n    \n\nprint(f'The precision are \\t {precisionList}')\nprint(f'The recall are \\t\\t {recallList}')\nprint(f'The f-score are \\t {fList}')\nprint(f'The ROC are \\t {ROCList}')","2428382c":"#Checking Chi2 for the Significancy\n#Elevation is probably due to the high altitude having more forest, thus it's more predictable\n#Dew Point is caused by and causing the rain, it is linked directly to the humidity which is what the data lacks\n#Months are from the seasonality effect between each months\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n# X = data.iloc[:,0:20]  #independent columns\n# y = data.iloc[:,-1]    #target column i.e price range\n#apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2, k=10)\nchiScore = bestfeatures.fit(rain_final,y7)\ndfscores = round(pd.DataFrame(chiScore.scores_),1)\ndfcolumns = pd.DataFrame(rain_final.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10,'Score'))  #print 10 best features","25ce7a19":"# What I Learnt\n\n - This project will work better if there are\n     - higher data frequency eg. Hours, Minutes \n     - More Features eg. Humidity, Clouds Position, Wind Directions\n - Prediction on the same day of the data is more accurate because there is Inter-Causality; the linkage between independent variables\n     - Temperature does not only affect the rain, the reverse causality is that rain also affect temperature as well\n         - eg. rainy days tend to be cooler\n \n##### The prediction of the weather can be difficult, there are many factors that could contribution to the change\n##### Even the actual meteorology model the government has need to use more dataset at more resolution in terms of location and time","4c8667d5":"# What I Noticed\n\n - Prediction accuracy dropped significantly after the T+0 (prediction of the same day)\n - Elevation plays the large role in the prediction, followed by Dew Point and Month Numbers\n     - This is likely because in the higher elevation, there are more forest and it tends to rain more in the forest area.\n     - Dew Point is linked to the Humidity which is the major factor on the precipitation\n \n"}}