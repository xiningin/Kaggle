{"cell_type":{"3ca781b4":"code","0c2fcd2b":"code","f12fd573":"code","eccd899c":"code","7e85c065":"code","5d3358df":"code","23a52f97":"code","90fcc4b6":"code","baed83c0":"code","7e19a78e":"code","69b07aae":"markdown","6e95f9f7":"markdown","f79dc4bb":"markdown","79f8809b":"markdown","b088b6ba":"markdown","3699f439":"markdown"},"source":{"3ca781b4":"import pandas as pd\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Train file path\ntrain_file_path = '..\/input\/train.csv'\n\nhome_data = pd.read_csv(train_file_path)\nprint(\"Original # of columns: {0}\".format(len(home_data.columns)))\n\n# Create y\ny = home_data.SalePrice\n\n# Candidate X\ncols_with_missing = [col for col in home_data.columns \n                                 if home_data[col].isnull().any()]\n\nX_candidate = home_data.drop(['Id','SalePrice'] + cols_with_missing, axis=1)\n\nlow_cardinality_cols = [cname for cname in X_candidate.columns if \n                                X_candidate[cname].nunique() < 10 and\n                                X_candidate[cname].dtype == \"object\"]\nnumeric_cols = [cname for cname in X_candidate.columns if \n                                X_candidate[cname].dtype in ['int64', 'float64']]\n\nmy_cols = low_cardinality_cols + numeric_cols\n\n# One hot encoded\none_hot_encoded_X = pd.get_dummies(X_candidate[my_cols])\nprint(\"# of columns after one-hot encoding: {0}\".format(len(one_hot_encoded_X.columns)))","0c2fcd2b":"# path to file you will use for predictions\ntest_data_path = '..\/input\/test.csv'\n\n# read test data file\ntest_data = pd.read_csv(test_data_path)\ntest_candidate = test_data.drop(['Id'] + cols_with_missing, axis=1)\n\n# one hot encode\ntest_one_hot_encoded = pd.get_dummies(test_candidate[my_cols])\nprint(\"# of columns after one-hot encoding: {0}\".format(len(test_one_hot_encoded.columns)))","f12fd573":"X, test_X = one_hot_encoded_X.align(test_one_hot_encoded, join='left', axis=1)\nprint(np.shape(test_X))","eccd899c":"# Split into training and validation data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1, test_size=0.25)","7e85c065":"from xgboost import XGBRegressor\n\nprint(np.shape(train_X), np.shape(val_X))\n\nxg_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nxg_model.fit(train_X, train_y, early_stopping_rounds=5,\n             eval_set=[(val_X, val_y)], verbose=False)\n\nmax_estimators = len(xg_model.evals_result()['validation_0']['rmse'])\nprint(max_estimators)\npd.DataFrame(xg_model.evals_result()['validation_0']['rmse'], columns=['rmse']).plot()","5d3358df":"# make predictions\npredictions = xg_model.predict(val_X)\n\nfrom sklearn.metrics import mean_absolute_error\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, val_y)))","23a52f97":"print(np.shape(X), np.shape(y))\n\nxg_model = XGBRegressor(n_estimators=max_estimators, learning_rate=0.05)\nxg_model.fit(X, y, verbose=False)","90fcc4b6":"from matplotlib import pyplot as plt\nimport xgboost as xgb\n\nfig, ax = plt.subplots(1,1,figsize=(10,10))\nxgb.plot_importance(xg_model, max_num_features=10, ax=ax)","baed83c0":"xgb.to_graphviz(xg_model, num_trees=1)","7e19a78e":"print(np.shape(test_X))\n#test_X = missing_data(test_X)\n\n# make predictions which we will submit. \ntest_preds = xg_model.predict(test_X)\n\n# Submission format\noutput = pd.DataFrame({'Id': test_data.Id,\n                       'SalePrice': test_preds})\n\noutput.to_csv('submission.csv', index=False)","69b07aae":"## 3. Model training (Online)","6e95f9f7":"## 1. Feature Engineering","f79dc4bb":"## 2. XGBoost","79f8809b":"# Introduction\nMachine learning competitions are a great way to improve your data science skills and measure your progress. \n\nIn this notebook we will cover following steps:\n1. Feature engineering\n    * Remove columns with missing values\n    * Numerical features\n    * Categorical features with low cardinality - one hot encoded\n2. XGBoost model \n    * Model validation using Training - Validation split (Offline)\n    * Model performance metrics: Mean absolute error\n3. Model training (Online)\n4. Model Insights\n5. Submission for Kaggle competition","b088b6ba":"## 4. Model Insights","3699f439":"## 5. Submission"}}