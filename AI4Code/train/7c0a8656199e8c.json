{"cell_type":{"c50c0e42":"code","02377c35":"code","45c36b40":"code","3277fb0e":"code","d7ffb71a":"code","7b706cb7":"code","759900b3":"code","52ec214f":"code","e59a0344":"code","04a2903d":"code","07ccd0ab":"code","f05e8b7a":"code","4a603b43":"code","66aa34f4":"code","bbdfaa3f":"code","1e5b246b":"code","ff7feae5":"code","027318c8":"code","3d451da5":"code","9b4ded10":"code","bf6ff19c":"markdown","9695c4ad":"markdown","d20c0ca6":"markdown","4e4aff8d":"markdown","43cd6fd0":"markdown","4542bc70":"markdown","0908619d":"markdown","68eebb23":"markdown","47be6680":"markdown","d6931715":"markdown","40b86e90":"markdown","a714385a":"markdown","a1c5dd54":"markdown","4eb90161":"markdown","3a9d1578":"markdown","f4dd008b":"markdown","a07f5eee":"markdown","d10b3126":"markdown","f9a112e8":"markdown","94c19d11":"markdown","76efba66":"markdown","4a54b99b":"markdown","9ee3a7ce":"markdown"},"source":{"c50c0e42":"import nltk\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\nfrom wordcloud import WordCloud\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nfrom textblob import TextBlob","02377c35":"data=pd.read_csv(\"..\/input\/twitter-data-for-luciferseason5\/Twitter data for LuciferSeason5.csv\")","45c36b40":"data.head()","3277fb0e":"data2=pd.DataFrame(data=data[\"tweet\"],columns=[\"tweet\"])\npd.set_option('display.max_colwidth',None)\ndata2.head()","d7ffb71a":"def cleaned_text(text):\n    clean=re.sub(\"http\\S+\",\"\",text)\n    clean=re.sub(\"pic.twitter\\S+\",\"\",clean)\n    clean=re.sub(\"#\\S+\",\"\",clean)\n    clean=clean.lower()\n    clean=re.sub(\"@\\S+\",\"\",clean)\n    clean=re.sub(\"[^a-z]\",\" \",clean)\n    clean=re.sub(\"can t\",\"can not\",clean)\n    clean=re.sub(\"don t\",\"do not\",clean)\n    clean=re.sub(\"pleaseee\",\"please\",clean)\n    clean=re.sub(\"plss\",\"please\",clean)\n    clean=re.sub(\"haven t\",\"have not\",clean)\n    clean=re.sub(\"you re\",\"you are\",clean)\n    clean=re.sub(\"aren t\",\"are not\",clean)\n    clean=re.sub(\"there s\",\"there is\",clean)\n    clean=re.sub(\"isn t\",\"is not\",clean)\n    clean=re.sub(\"it s\",\"it is\",clean)\n    clean=re.sub(r\"\\s+[a-z]\\s+\",\" \",clean)\n    clean=re.sub(r\"\\s+[a-z]\\s+\",\" \",clean)\n    clean=re.sub(\"winzzzzzzz\",\"win\",clean)\n    clean=re.sub(\"pleaseeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\",\"please\",clean)\n    clean=re.sub(\"ishould\",\"should\",clean)\n    clean=re.sub(\"omg\",\"oh my god\",clean)\n    clean=re.sub(\"yessss\",\"yes\",clean)\n    clean=re.sub(\"gonna\",\"going to\",clean)\n    clean=re.sub(\"iseem\",\"seem\",clean)\n    clean=re.sub(\"maybe\",\"may be\",clean)\n    clean=re.sub(\"dont\",\"do not\",clean)\n    clean=re.sub(\"wouldnt\",\"would not\",clean)\n    clean=re.sub(\"imma\",\"i am going to\",clean)\n    clean=re.sub(\"btw\",\"by the way\",clean)\n    clean=re.sub(\"breath\",\"breathe\",clean)\n    clean=re.sub(\"fanfic\",\"fan fiction\",clean)\n    clean=re.sub(\"yall\",\"you all\",clean)\n    clean=re.sub(\"cannot\",\"can not\",clean)\n    clean=re.sub(\"eachother\",\"each other\",clean)\n    clean=re.sub(\"hubbys\",\"husbands\",clean)\n    clean=re.sub(\"frees\",\"free\",clean)\n    clean=re.sub(\"cking\",\"fucking\",clean)\n    clean=re.sub(\"dyiiiiiiiiiiing\",\"dying\",clean)\n    clean=re.sub(\"wheennn\",\"when\",clean)\n    clean=re.sub(\"pls\",\"please\",clean)\n    clean=re.sub(\"sofar\",\"so far\",clean)\n    clean=re.sub(\"soooon\",\"soon\",clean)\n    clean=re.sub(\"fufucking\",\"fucking\",clean)\n    clean=re.sub(\"sooooo\",\"so\",clean)\n    clean=re.sub(\"plz\",\"please\",clean)\n    clean=re.sub(\"pleeeaaaasssseeeee\",\"please\",clean)\n    clean=re.sub(\"fvfucking\",\"fucking\",clean)\n    clean=re.sub(\"soooooo\",\"so\",clean)\n    clean=re.sub(\"listenning\",\"listening\",clean)\n    clean=re.sub(\"yeeeeesss\",\"yes\",clean)\n    clean=re.sub(\"seaon\",\"season\",clean)\n    clean=re.sub(\"pleasee\",\"please\",clean)\n    clean=re.sub(\"awesomeeeeeeeee\",\"awesome\",clean)\n    clean=re.sub(\"waitttt\",\"wait\",clean)\n    clean=re.sub(\"fking\",\"fucking\",clean)\n    clean=re.sub(\"isoo\",\"so\",clean)\n    clean=re.sub(\"lmao\",\"laughing my ass off\",clean)\n    clean=re.sub(\"srsly\",\"seriously\",clean)\n    clean=re.sub(\"yaaaaaassss\",\"yes\",clean)\n    clean=re.sub(\"wanna\",\"want to\",clean)\n    clean=re.sub(\"f ck\",\"fuck\",clean)\n    clean=re.sub(\"guy\",\"\",clean)\n    clean=re.sub(\"freakin\",\"freaking\",clean)\n    clean=re.sub(\"tbh\",\"to be honest\",clean)\n    clean=re.sub(\"soooo\",\"so\",clean)\n    clean=re.sub(\"neighborhood\",\"neighbourhood\",clean)\n    clean=re.sub(\"needdd\",\"need\",clean)\n    clean=re.sub(\"cant\",\"cannot\",clean)\n    clean=re.sub(\"isad\",\"sad\",clean)\n    clean=re.sub(\"netflixxxx\",\"netflix\",clean)\n    clean=re.sub(\"ppl\",\"people\",clean)\n    clean=re.sub(\"sooo\",\"so\",clean)\n    clean=re.sub(\"bbq\",\"barbecue\",clean)\n    clean=re.sub(\"areally\",\"really\",clean)\n    clean=re.sub(\"frifucking\",\"freaking\",clean)\n    clean=clean.lstrip()\n    clean=re.sub(\"\\s{2,}\",\" \",clean)\n    return clean\ndata2[\"cleaned_tweets\"]=data2[\"tweet\"].apply(cleaned_text)","7b706cb7":"data2.head()","759900b3":"data2['Number_of_words'] = data2['cleaned_tweets'].apply(lambda x:len(str(x).split()))","52ec214f":"plt.style.use('ggplot')\nplt.figure(figsize=(14,6))\nsns.distplot(data2['Number_of_words'],kde = False,color=\"red\")\nplt.title(\"Frequency distribution of number of words for each tweet\", size=20)","e59a0344":"nltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\nstop=stopwords.words('english')\ndata2[\"cleaned_clean_tweets\"]=data2[\"cleaned_tweets\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))","04a2903d":"print(stop)","07ccd0ab":"data2[\"cleaned_clean_tweets\"]=data2[\"cleaned_clean_tweets\"].apply(lambda x: nltk.word_tokenize(x))","f05e8b7a":"def word_lemmatizer(text):\n    lem_text = [WordNetLemmatizer().lemmatize(i,pos='v') for i in text]\n    return lem_text\ndata2[\"cleaned_clean_tweets\"]=data2[\"cleaned_clean_tweets\"].apply(lambda x: word_lemmatizer(x))\ndata2[\"cleaned_clean_tweets\"]=data2[\"cleaned_clean_tweets\"].apply(lambda x: ' '.join(x))","4a603b43":"plt.style.use('ggplot')\nplt.figure(figsize=(14,6))\nfreq=pd.Series(\" \".join(data2[\"cleaned_clean_tweets\"]).split()).value_counts()[:20]\nfreq.plot(kind=\"bar\")\nplt.title(\"20 most frequent words\",size=20)","66aa34f4":"cloud=WordCloud(colormap=\"Dark2\").generate(str(data2[\"cleaned_clean_tweets\"]))\nfig=plt.figure(figsize=(12,18))\nplt.axis(\"off\")\nplt.imshow(cloud,interpolation='bilinear')","bbdfaa3f":"def analyze_sentiment(tweet):\n    analysis = TextBlob(tweet)\n    if analysis.sentiment.polarity > 0:     #### For positive sentiment\n        return 1\n    elif analysis.sentiment.polarity == 0:  ### Neutral\n        return 0\n    else:\n        return -1                           #### Negative sentiment\n\ndata2['SA'] = data2[\"cleaned_clean_tweets\"].apply(lambda x: analyze_sentiment(x))","1e5b246b":"descending_order=data2[\"SA\"].value_counts().sort_values(ascending=False).index\nsns.catplot(x=\"SA\",data=data2,kind=\"count\",height=5,aspect=2,order=descending_order)","ff7feae5":"another_method = data2.groupby('SA').count()['cleaned_clean_tweets'].reset_index().sort_values(by='cleaned_clean_tweets',ascending=False)\nanother_method.style.background_gradient(cmap='Purples')","027318c8":"print(len(data2[data2[\"SA\"]==1])\/len(data2[\"SA\"])*100)  ##positive\nprint(len(data2[data2[\"SA\"]==0])\/len(data2[\"SA\"])*100)  ##neutral\nprint(len(data2[data2[\"SA\"]==-1])\/len(data2[\"SA\"])*100) ##negative","3d451da5":"cloud=WordCloud(colormap=\"Dark2\").generate(str(data2[data2[\"SA\"]==1][\"cleaned_clean_tweets\"]))\nfig=plt.figure(figsize=(12,14))\nplt.axis(\"off\")\nplt.imshow(cloud,interpolation='bilinear')","9b4ded10":"cloud=WordCloud(colormap=\"Dark2\").generate(str(data2[data2[\"SA\"]==-1][\"cleaned_clean_tweets\"]))\nfig=plt.figure(figsize=(12,18))\nplt.axis(\"off\")\nplt.imshow(cloud,interpolation='bilinear')","bf6ff19c":"Now we will do tokenization and lemmatization.","9695c4ad":"First we will import all the necessary libraries.","d20c0ca6":"We will do this using another method.","4e4aff8d":"Now we will take a look at the first 5 row of the data to check whether things that we were trying to do, has been done or not.","43cd6fd0":"Now almost all the text pre-processing steps has been done, now we will move to some visualization and finally sentiments labelling. First we will start our visualization by plotting 20 most frequent words, and we will show it using bar graph.","4542bc70":"And finally we create wordcloud for negative tweets.","0908619d":"Now we will import the dataset.","68eebb23":"But here we will mainly deal with the tweets only, so we will ignore the other varibles, but one is free to do analysis of the rest of the variables also.","47be6680":"Now we will start our next step of cleaning which includes stopwords removal, tokenization and bringing back the words to it's base form using lemmatization. Basically there exists two ways by which we can bring the words to their base form, i.e., stemming and lemmatization, but particularly I am not a big fan of stemming, so we will use lemmatization here to achieve our that objective.","d6931715":"Also we will check percentages for +ve, -ve and neutral tweets.","40b86e90":"And first we will create wordcloud for poitive tweets.","a714385a":"So sentiment labelling has been done, now we will check how many of them are postive, negative and neutral, and we will do that using visualization.","a1c5dd54":"Now we will move to our final step of sentiment labelling using textblob library.","4eb90161":"Now we will check what includes stopwords.","3a9d1578":"So in the above two visualizations we can see that most of the tweets are postive, i.e., 4644 tweets are positive, 4056 are neutral and 1057 are negative, and it is obvious that most number of tweets are positive as fans like the show so much. Now we will see using wordcloud what are the frequent words used for both positive and negative tweets.","f4dd008b":"So now we will show the distribution of the number of words using a histogram.","a07f5eee":"And it seems we have been able to achieve our first goal, our text is much cleaner now. Now we will create another column which will be the count of number of words for each row.","d10b3126":"# **Sentiment Analysis of the tweets for the #LuciferSeason5.**","f9a112e8":"Now we will do word clouding.","94c19d11":"So we can see that the text needs lots of cleaning, as it contains lots of unwanted things, also this will be the first step of text cleaning, there will be another level of cleaning, which will be done in the following steps. In this step we will mainly convert all the text to lowercase, remove hashtags, usernames, hyperlinks, images, punctuations and also correct other things like spelling mistakes and word expansion.","76efba66":"This is the end of our analysis. Don't forget to upvote and comment if you like, or if you have any feedback. Also you guys are free to come up with your own analysis. Thank you!!!!!!!!!!","4a54b99b":"![Lucifer%20image.jpg](attachment:Lucifer%20image.jpg)","9ee3a7ce":"So in the above wordcloud we can see the words like wait, share, release, begin, long, etc., and these are coming because as the release date of the show has not been announced yet, so fans are desperately waiting for the release date of the show. So in this data, people are mostly talking about the release date of the show."}}