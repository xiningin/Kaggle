{"cell_type":{"7232bd2c":"code","6b75c0f4":"code","76f5180b":"code","ff89013b":"code","7c7f365c":"code","82d735b8":"code","6ef02df1":"code","d1eaee30":"code","abcbad98":"code","da4cb2d8":"code","657feead":"code","0e7b11fa":"code","56411177":"code","73a5dd48":"code","9b7ca4a8":"code","c68b6f1f":"code","6cea0973":"code","3f3d064a":"markdown","9d3d6f73":"markdown","9ab9ae98":"markdown","45332c72":"markdown","637450ab":"markdown","2b4abb7b":"markdown","036d0161":"markdown","cf1d8a6e":"markdown","f5b5610e":"markdown","bd69bdb0":"markdown","6d0a146b":"markdown","a4a4f52b":"markdown","2aa324c6":"markdown","5516b018":"markdown","24db1c31":"markdown","69710508":"markdown","968f041b":"markdown"},"source":{"7232bd2c":"import pandas as pd\nimport numpy as np\n\ntrain_data = pd.read_csv(\"\/kaggle\/input\/sign-language-mnist\/sign_mnist_train\/sign_mnist_train.csv\", sep=\",\")\ntrain_data.head()","6b75c0f4":"test_data = pd.read_csv(\"\/kaggle\/input\/sign-language-mnist\/sign_mnist_test\/sign_mnist_test.csv\", sep=\",\")\n\ntrain_data.append(test_data)\ndataset = train_data\ndataset.head()","76f5180b":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nmapping_values = {\n    0:\"A\", 1:\"B\", 2:\"C\", 3:\"D\", 4:\"E\", 5:\"F\",\n    6:\"G\", 7:\"H\", 8:\"I\", 10:\"K\", 11:\"L\", 12:\"M\", 13:\"N\",\n    14:\"O\", 15:\"P\", 16:\"Q\", 17:\"R\", 18:\"S\", 19:\"T\", 20:\"U\",\n    21:\"V\", 22:\"W\", 23:\"X\", 24:\"Y\"}\n\nplt.figure(figsize=(10, 18))\nfor i in range(26):\n    if i in (9, 25): \n        continue\n    plt.subplot(7, 4, i+1)\n    plt.imshow(np.array(dataset[dataset.label == i].iloc[0,1:].values.tolist()).reshape(28,28), cmap='Greys')\n    plt.xticks([])\n    plt.yticks([])\n    plt.title(mapping_values[i])\n\nplt.show()    ","ff89013b":"y = dataset.label\ndataset.drop(\"label\", axis=1, inplace=True)\nX = dataset","7c7f365c":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","82d735b8":"X_train = X_train.to_numpy().reshape(-1, 28,28, 1) # 28*28\nX_test = X_test.to_numpy().reshape(-1, 28,28, 1) # 28*28\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","6ef02df1":"X_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train = X_train \/ 255.\nX_test = X_test \/ 255.","d1eaee30":"X_train,X_valid,y_train,y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=13)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","abcbad98":"import keras\nimport tensorflow as tf\nfrom keras.models import Sequential,Input,Model\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.advanced_activations import LeakyReLU\n\nfrom tensorflow.python.client import device_lib \ndevice_lib.list_local_devices() # let's list all available computing devices","da4cb2d8":"batch_size = 128\nepochs = 15\nnum_classes = 26","657feead":"model = Sequential()\n\nmodel.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(28,28,1),padding='same'))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(MaxPooling2D((2, 2),padding='same'))\nmodel.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\nmodel.add(LeakyReLU(alpha=0.1))\nmodel.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\nmodel.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\nmodel.add(LeakyReLU(alpha=0.1))                  \nmodel.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='linear'))\nmodel.add(LeakyReLU(alpha=0.1))                  \nmodel.add(Dense(num_classes, activation='softmax'))","0e7b11fa":"model.compile(loss=keras.losses.sparse_categorical_crossentropy, \n              optimizer=keras.optimizers.Adam(),\n              metrics=['accuracy'])\nmodel.summary()","56411177":"with tf.device('\/GPU:0'):\n    model_train = model.fit(X_train, y_train, \n                            batch_size=batch_size,\n                            epochs=epochs,\n                            verbose=1,\n                            validation_data=(X_valid, y_valid))","73a5dd48":"plt.figure(figsize=(7, 7), dpi=80)\nplt.subplot(2,1,1)\nplt.title(\"Training History - Accuracy\")\nplt.plot(range(epochs), model_train.history[\"accuracy\"], label=\"accuracy\", color=\"red\")\nplt.scatter(range(epochs), model_train.history[\"val_accuracy\"], label=\"val_accuracy\")\nplt.xticks(range(0,epochs,1))\nmin_y = min(np.min(model_train.history[\"val_accuracy\"]), np.min(model_train.history[\"accuracy\"]))\nplt.yticks(np.linspace(min_y-0.1,1,11))\nplt.legend()\n\n\nplt.subplot(2,1,2)\nplt.title(\"Training History - Loss\")\nplt.plot(range(epochs), model_train.history[\"val_loss\"], label=\"val_loss\", color=\"red\")\nplt.scatter(range(epochs), model_train.history[\"loss\"], label=\"loss\")\nplt.xticks(range(0,epochs,1))\nmax_y = max(np.max(model_train.history[\"val_loss\"]), np.max(model_train.history[\"loss\"]))\nplt.yticks(np.linspace(0,max_y+0.1,11))\nplt.legend()\n\nplt.show()","9b7ca4a8":"test_eval = model.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', test_eval[0])\nprint('Test accuracy:', test_eval[1]*100, \"%\")","c68b6f1f":"from sklearn.metrics import classification_report\n\ny_pred = np.argmax(np.round(model.predict(X_test)), axis=1)\n\ntarget_names = [\"Class {}\".format(i) for i in range(num_classes)]\nprint(classification_report(y_test, y_pred))","6cea0973":"from sklearn.metrics import confusion_matrix\nimport seaborn as sn\n\nplt.figure(figsize=(9, 7))\ncf = confusion_matrix(y_test, y_pred)\nsn.heatmap(cf, annot=True)\nplt.title(\"Confusion Matrix\")\nplt.show()","3f3d064a":"## Defining Model","9d3d6f73":"## Importing Data and Preprocessing","9ab9ae98":"As we can see, there are **no errors** and **100%** accuracy is reached.","45332c72":"Let's define basic learning parameters","637450ab":"Let's define train and test dataset.","2b4abb7b":"Let's train the model. I've used GPU for training: each epoch takes about ~2s to complete, against ~35s while using CPU.","036d0161":"### Defining Training, Validation and Test set","cf1d8a6e":"### Visualizing Learning Process ","f5b5610e":"Here follows the model: a deep convolutionary neural network, with the following kind of layers:\n- **Conv2D**: convulutional layer. In the first layer the input is shaped (28, 28, 1): a 3D-object of 28x28x1, or simply a matrix 28x28;\n- **LeakyReLU**: this layer helps to manage non-linearity of data using a non linear activation function;\n- **MaxPooling2D**: used for reducing data dimensionality;\n- **Flatten**: number of units is proportional to input;\n- **Dense**: regular dense connected layer.","bd69bdb0":"### Image examples\n**Let's visualize one image for each class**\n\n*N.B.*: there is no image for label *9* and *25*, or *J* and *Z*, because their representation require motions.","6d0a146b":"Now let's evaluate the model on test data.","a4a4f52b":"# Sign Language MNIST\nThe purpose of this notebook is about images classification through a Convolutionary Neural Network.<br>\nIn particular, in this case has been obtained the 100% accuracy on test data.\n\nImages are about sign language, every 28x28 image represets a particular alphabet letter. ","2aa324c6":"### Model Evaluation","5516b018":"**Confusion Matrix**<br>","24db1c31":"Let's normalize values, making them inside \\\\([0,1] \\in \\mathbb{R}\\\\).<br>\n*N.B.:* each pixel value is expressed as grey depth in 8bit, so values in \\\\([0, 255] \\in \\mathbb{N}\\\\).","69710508":"### Training","968f041b":"Then, from train data let's extract a validation set for learning process."}}