{"cell_type":{"7685a86f":"code","fe15c528":"code","aa5de1cd":"code","0ad171cc":"code","2ae734a0":"code","e3fe05ec":"code","c317693f":"code","6f430768":"code","66ca092f":"code","23800c6b":"code","5765e9d0":"code","874b0760":"code","ae587469":"code","1421ca22":"code","2e5ad767":"code","924e48f0":"code","3ce64515":"code","70c6da2c":"code","87177b79":"code","58510e81":"code","388256c1":"code","7922bdfc":"code","373fccfe":"code","4cc0c6e0":"code","b0815c37":"markdown","d23378f9":"markdown","64c75cd6":"markdown","38a51c3f":"markdown","21a919cc":"markdown","222b5694":"markdown","1c4f65c6":"markdown","7c5b6955":"markdown","005981bc":"markdown","5be77a40":"markdown","25e31033":"markdown","eb606b88":"markdown","34a915c0":"markdown","70830764":"markdown","1d097573":"markdown","673ea1b3":"markdown","d67c3f1b":"markdown","557e38d1":"markdown","98fec25e":"markdown","5f0a42c8":"markdown","1a7c7c34":"markdown","4fcc0f62":"markdown","ae18ddaa":"markdown"},"source":{"7685a86f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fe15c528":"#!pip install kaggle\n#!kaggle datasets download -d mlg-ulb\/creditcardfraud","aa5de1cd":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import IsolationForest, RandomForestClassifier, GradientBoostingClassifier\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, matthews_corrcoef\nfrom sklearn.neural_network import MLPClassifier","0ad171cc":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","2ae734a0":"df.info()","e3fe05ec":"print(\"Mean amount of transactions for fraudulent ones: \", df[df.Class == 1]['Amount'].mean())\nprint(\"Mean amount of transactions for good ones: \", df[df.Class == 0]['Amount'].mean())","c317693f":"# Visualizing the distribution of data\n\nfor i, c in enumerate(df.columns):\n    sns.distplot(df[df.columns[i]])\n    plt.show()","6f430768":"# Splitting the data into testing and training sets\nX = df.drop('Class', axis=1)\ny = df['Class']\n\nX_data = X.values\ny_data = y.values\n\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, random_state=2)\n\n# Training the model\noutlier_fraction = len(df[df.Class == 1])\/len(df[df.Class == 0])\n\nmodel_if = IsolationForest(max_samples=len(X_train), contamination=outlier_fraction, random_state=2)\nmodel_if.fit(X_train)\n\nscores_pred = model_if.decision_function(X_train)\ny_pred = model_if.predict(X_test)","66ca092f":"# Cleaning the predicted values for proper display of good and fraudulent transaction.\n\ny_pred[y_pred == -1] = 1\ny_pred[y_pred == 1] = 0\n\n#number of errors by the model\nprint('Number of errors by the model: ', (y_pred != y_test).sum())","23800c6b":"conf_matrix = confusion_matrix(y_test, y_pred)\nLabels = ['Good', 'Fraud']\nsns.heatmap(conf_matrix, xticklabels=Labels, yticklabels=Labels, annot=True, fmt='d')\nplt.title('Confusion Matrix')\nplt.xlabel('True Class')\nplt.ylabel('Predicted Class')\nplt.show()","5765e9d0":"# Evaluation of the model\nprint('Model Evaluation metrics - Isolation Forest')\nprint('Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Precision: ', precision_score(y_test, y_pred))\nprint('F1: ', f1_score(y_test, y_pred))\nprint('Recall: ',recall_score(y_test, y_pred))\nprint('Matthews correlation coefficient', matthews_corrcoef(y_test, y_pred))","874b0760":"model_rf = RandomForestClassifier()\nmodel_rf.fit(X_train, y_train)\ny_pred = model_rf.predict(X_test)\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nLabels = ['Good', 'Fraud']\nsns.heatmap(conf_matrix, xticklabels=Labels, yticklabels=Labels, annot=True, fmt='d')\nplt.title('Confusion Matrix')\nplt.xlabel('True Class')\nplt.ylabel('Predicted Class')\nplt.show()","ae587469":"# Evaluation of the Ranfom Forest model\nprint('Model Evaluation metrics')\nprint('Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Precision: ', precision_score(y_test, y_pred))\nprint('F1: ', f1_score(y_test, y_pred))\nprint('Recall: ',recall_score(y_test, y_pred))\nprint('Matthews correlation coefficient', matthews_corrcoef(y_test, y_pred))","1421ca22":"model_mlp = MLPClassifier()\nmodel_mlp.fit(X_train, y_train)\ny_pred = model_mlp.predict(X_test)\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nLabels = ['Good', 'Fraud']\nsns.heatmap(conf_matrix, xticklabels=Labels, yticklabels=Labels, annot=True, fmt='d')\nplt.title('Confusion Matrix')\nplt.xlabel('True Class')\nplt.ylabel('Predicted Class')\nplt.show()","2e5ad767":"# Evaluation of the Ranfom Forest model\nprint('Model Evaluation metrics')\nprint('Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Precision: ', precision_score(y_test, y_pred))\nprint('F1: ', f1_score(y_test, y_pred))\nprint('Recall: ',recall_score(y_test, y_pred))\nprint('Matthews correlation coefficient', matthews_corrcoef(y_test, y_pred))","924e48f0":"model_GB = GradientBoostingClassifier()\nmodel_GB.fit(X_train, y_train)\ny_pred = model_GB.predict(X_test)\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nLabels = ['Good', 'Fraud']\nsns.heatmap(conf_matrix, xticklabels=Labels, yticklabels=Labels, annot=True, fmt='d')\nplt.title('Confusion Matrix')\nplt.xlabel('True Class')\nplt.ylabel('Predicted Class')\nplt.show()","3ce64515":"# Evaluation of the Ranfom Forest model\nprint('Model Evaluation metrics')\nprint('Accuracy: ', accuracy_score(y_test, y_pred))\nprint('Precision: ', precision_score(y_test, y_pred))\nprint('F1: ', f1_score(y_test, y_pred))\nprint('Recall: ',recall_score(y_test, y_pred))\nprint('Matthews correlation coefficient', matthews_corrcoef(y_test, y_pred))","70c6da2c":"import h2o\nh2o.init()","87177b79":"df_cc = h2o.import_file('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndf_cc.head()","58510e81":"train, test = df_cc.split_frame(ratios=[.8])","388256c1":"from h2o.automl import H2OAutoML\n\nx = df_cc.col_names[:-1]\n\nautoml = H2OAutoML(project_name='cc_fraud_detect', \n                  max_models=5,\n                  max_runtime_secs=500,\n                  sort_metric='MAE',\n                  exclude_algos=['StackedEnsemble'],\n                  seed=111)\n\nautoml.train(training_frame=train, y='Class', x=x)","7922bdfc":"automl.leaderboard","373fccfe":"automl.leader","4cc0c6e0":"automl.predict(test)","b0815c37":"### Generating a model to detect fraudulent transaction","d23378f9":"There are no null values. So nothing to worry about cleaning the dataset.","64c75cd6":"#### Exploratory Data Analysis:","38a51c3f":"### Context:\nIt is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.","21a919cc":"#### Importing necessary libraries for running the model","222b5694":"So this is how predictions look like in automl. Models are build and sorted in the leaderboard according to the sorting metric \"MAE\" Mean Absolute Error. The first model in the leader is Distributed Random Forest which reminds us that the same algorithm performed better for this data before when we implemented using scikit-learn library.","1c4f65c6":"It means that fraudulent transaction have transaction amount greater than the good ones on an average. So it is important to look at these transactions","7c5b6955":"Although Accuracy is good, the model compromised on other metrics. Hence it is not useful to classify the transactions. However, this is a good benchmark for improving on accuracy","005981bc":"#### Model Building: H2O AutoML","5be77a40":"#### Loading the dataset","25e31033":"It is understood that most the columns have values around zero and it is ok.","eb606b88":"#### Model Building - Isolation Forest","34a915c0":"Using Random forest classifier from sklearn library and evaluating the model from the results.","70830764":"The dataset can be downloaded by running the below. Please run the cell only once to download the data. Running multiple time will create duplicate copies of the same dataset.","1d097573":"Using multilayer perception model from the scikit learn library.","673ea1b3":"#### Model Building - Random Forest","d67c3f1b":"#### Model Building - Neural networks","557e38d1":"### Dataset Introduction: \n\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders.\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.","98fec25e":"From the results, it can be understood that the model is good for classifying the transactions inspite of much imbalance in the data.","5f0a42c8":"I appreciate the feedback and any upvotes I get. Feedback makes me only better.\n\n**Thank you for taking time to look into my notebook. Happy learning!**","1a7c7c34":"Using multilayer perceptron also generated very good results but the rank of Random Forest holds up still. Let's use the very famous Gradient Boosting algorithm. Using Gradient Boosting Classifier from scikit-learn library.","4fcc0f62":"Converting the dataset into training and testing data and see some prediction and evaluation results. The first approach to use Isolation Forest algorithm which is generally used for outlier detection. So, it will be used for setting the baseline performance of the model.","ae18ddaa":"#### Model Building: Gradient Boosting"}}