{"cell_type":{"23ea4582":"code","8ff758f7":"code","c06ee73c":"code","c39771d7":"code","d0f70a35":"code","0b593970":"markdown"},"source":{"23ea4582":"!pip install coremltools\nimport coremltools as ct\n\n!pip install -q efficientnet >> \/dev\/null\nimport efficientnet.tfkeras as efn\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom keras import models\nfrom IPython.display import clear_output\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# NUMBER OF FOLDS. USE 3, 5, OR 15 \nFOLDS = 5\n\n# WHICH IMAGE SIZES TO LOAD EACH FOLD\nIMG_SIZES = [384] * FOLDS\n\n# WHICH EFFICIENTNET\nEFF_NETS = [6] * FOLDS\n\n# Download models\nEFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3, \n        efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6, efn.EfficientNetB7]","8ff758f7":"def build_model(img_dims, ef):\n    inp = tf.keras.layers.Input(shape=(img_dims, img_dims, 3))\n    base = EFNS[ef](input_shape=(img_dims, img_dims, 3), weights='imagenet', include_top=False)\n    x = base(inp)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    model = tf.keras.Model(inputs=inp, outputs=x)\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05), \n                  metrics=['AUC', 'binary_accuracy'])\n    return model\n\nclass FixedDropout(tf.keras.layers.Dropout):\n    def _get_noise_shape(self, inputs):\n        if self.noise_shape is None:\n            return self.noise_shape\n        symbolic_shape = K.shape(inputs)\n        noise_shape = [symbolic_shape[axis] if shape is None else shape\n        for axis, shape in enumerate(self.noise_shape)]\n        return tuple(noise_shape)","c06ee73c":"for k in range(0, FOLDS):\n    print('Model ' + str(k+1) + '\/' + str(FOLDS) + ' conversion')\n    model = build_model(IMG_SIZES[k], EFF_NETS[k])\n    model.load_weights('..\/input\/melanoma-classification-models\/FOLD' + str(k+1) + '_model.h5')\n    \n    image_input = ct.ImageType(shape=(1, IMG_SIZES[k], IMG_SIZES[k], 3), scale=1.0\/255.0, bias=[0, 0, 0])\n    classifier_config = ct.ClassifierConfig(['melanoma'])\n    \n    modelCoreML = ct.convert(model, \n                             inputs=[image_input],\n                             classifier_config=classifier_config,\n                             add_custom_layers=True,\n                             custom_conversion_functions={'FixedDropout' : FixedDropout})\n    \n    modelCoreML.author = 'mnowak061'\n    modelCoreML.short_description = 'Model trained for the melanoma classification competition SIIM & ISIC 2020 with k-fold stratified cross validation.'\n    modelCoreML.license = 'Apache License. Version 2.0'\n    modelCoreML.version = '1.0.0'\n    modelCoreML.save('.\/FOLD' + str(k+1) + '_model.mlmodel')\n    clear_output(wait=True)","c39771d7":"for k in range(0, FOLDS):\n    print('FOLD ' + str(k+1))\n    \n    image = cv2.imread('..\/input\/isic2020-384x384-jpeg\/ISIC2020_384x384_jpeg\/test\/ISIC_0112420.jpg')\n    cv2.imwrite('.\/ISIC_0112420.jpg', image)\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n    image = np.array(image).reshape(1, IMG_SIZES[k], IMG_SIZES[k], 3)\n    image = image.astype(np.float32)\n    image = image \/ 255.0\n\n    modelKeras = build_model(IMG_SIZES[k], EFF_NETS[k])\n    modelKeras.load_weights('..\/input\/melanoma-classification-models\/FOLD' + str(k+1) + '_model.h5')\n    predictionKeras = modelKeras.predict(image)\n\n    print('prediction: ', predictionKeras[0][0])","d0f70a35":"plt.figure(figsize=(30,30))\nfor i in range(0, FOLDS):\n    image = cv2.imread('..\/input\/melanoma-classification-coremlmodels\/FOLD' + str(i+1) + '_test.png')\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n    plt.subplot(3, 3, i+1)\n    plt.imshow(np.squeeze(image), cmap=plt.cm.bone)","0b593970":"# Conversion test\nYou have to verify on MacOS\/iOS that the converted models give the same\/similar prediction value for the same photos. Small differences may be due to hardware accuracy"}}