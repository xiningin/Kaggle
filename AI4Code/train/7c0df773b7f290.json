{"cell_type":{"c043607c":"code","87d34237":"code","18de4143":"code","49ef5163":"code","f49dccf5":"code","ce67021e":"code","f58e443a":"code","6d866d74":"code","2b00ab4e":"code","d9dc38a8":"code","54638a75":"code","886a41df":"code","21c3502f":"code","f8a30b67":"code","2d514e12":"code","05557f62":"code","0e4fe1d7":"code","a164d77c":"code","5263b83e":"markdown","37af7aff":"markdown","5d02e67c":"markdown","98bab673":"markdown"},"source":{"c043607c":"import torch\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import TensorDataset\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntorch.cuda.is_available()","87d34237":"!nvidia-smi","18de4143":"train_ds=pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/train.csv')\ntrain_ds.describe()","49ef5163":"y_train=train_ds[['target_carbon_monoxide','target_benzene','target_nitrogen_oxides']]\ncols=['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']\nfor col in cols:\n    y_train.loc[:,col]=np.log(y_train.loc[:,col])\nx_train=train_ds.drop(['target_carbon_monoxide','target_benzene','target_nitrogen_oxides','date_time'],axis=1)\ny_train","f49dccf5":"try:\n    x_train=x_train.to_numpy()\nexcept:\n    print(x_train)\nprint()\nprint()\ntry:\n    y_train=y_train.to_numpy()\nexcept:\n    print(y_train)","ce67021e":"try:\n    x_train = torch.from_numpy(x_train)\n    y_train = torch.from_numpy(y_train)\n    x_train=x_train.float()\n    y_train=y_train.float()\n    train_ds=TensorDataset(x_train,y_train)\nexcept:\n    print()\n\ntrain_dl=DataLoader(train_ds,batch_size=128,shuffle=True)","f58e443a":"class MODEL(nn.Module):\n  def __init__(self,in_size,hidden_size1,hidden_size2,hidden_size3,out_size):\n    super().__init__()\n    self.linear1=nn.Linear(in_size,hidden_size1)\n    self.linear2=nn.Linear(hidden_size1,hidden_size2)\n    self.linear3=nn.Linear(hidden_size2,hidden_size3)\n    self.linear4=nn.Linear(hidden_size3,out_size)\n  \n  def forward(self,x):\n    out=self.linear1(x)\n    out=F.relu(out)\n    \n    out=self.linear2(out)\n    out=F.relu(out)\n    \n    out=self.linear3(out)\n    out=F.relu(out)\n    \n    out=self.linear4(out)\n\n    return(out)\nmodel=MODEL(x_train.size(1),32,64,128,y_train.size(1))\nif torch.cuda.is_available():\n    model=model.cuda()\nmodel","6d866d74":"import time\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n    \n    \ndef training(epochs,train_dl,loss_fn,max_lr,model,save_file_name,grad_clip=0,weight_decay=0,opt_func=torch.optim.Adam,max_epochs_stop=10):\n    start=time.time()\n    time_flag=0\n    optimizer=opt_func(model.parameters(),max_lr,weight_decay=weight_decay)\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n                                                steps_per_epoch=len(train_dl))\n    train_loss_min=np.Inf\n    # Actualy to be done with val_dataset\n    k=0\n    train_loss=np.zeros(epochs)\n    lrs=[]\n    for epoch in range(epochs):\n        for batch in train_dl:\n            x,y=batch\n            if torch.cuda.is_available():\n                x=x.cuda()\n                y=y.cuda()\n                model=model.cuda()\n            \n            #STEP-1: Forward\n            out=model(x)\n        \n            #STEP-2:Loss\n            loss=loss_fn(out,y)\n        \n            #Step-3:Cleaning the prev calculated gradients\n            model.zero_grad()\n        \n            #Step-4:Accumalate partial derivatives of Loss wrt to params\n            loss.backward()\n            \n            if grad_clip:\n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n        \n            #Step-5: Takes 5 steps to update\n            optimizer.step()\n\n            loss=loss.cpu()\n            lrs.append(get_lr(optimizer))\n            #STEP-6\n            sched.step()\n            \n        train_loss[k]=loss\n        \n            \n        k+=1\n        if(epoch==0):\n            print(\"{}\/{} Epochs | Train Loss={:.4f}  |lr={:.5f}\".format(epoch+1,epochs,loss,get_lr(optimizer)))\n            \n        if((epoch+1)%1==0):\n            print(\"{}\/{} Epochs    | Train Loss={:.4f}  |lr={:.5f}\".format(epoch+1,epochs,loss,get_lr(optimizer)))\n            \n            \n        if loss<train_loss_min:\n            f = open(\"best.pth\",\"w\",encoding='utf-8')\n            torch.save(model.state_dict(), save_file_name)\n            epochs_no_improve = 0\n            train_loss_min = loss\n            best_epoch = epoch\n            f.close()\n        else:\n            net_time=time.time()-start\n            time_flag=1\n            epochs_no_improve+=1\n            if epochs_no_improve>max_epochs_stop:\n                    print(\"Early Stopping! Total_epochs:\",epochs,\"Best epoch:\",best_epoch,\"with train loss:\",train_loss_min,\n                         \"Time elapsed:\",time)\n            \n            \n                    # Load the best state dict\n                    f = open(\"best.pth\",\"r\",encoding='utf-8')\n                    model.load_state_dict(torch.load(save_file_name))\n                    f.close()\n                    # Attach the optimizer\n                    model.optimizer = optimizer\n                    return model,train_loss,lrs\n                \n    if time_flag==0:       \n        net_time=time.time()-start\n    print(\"TIME Taken:{:.2f} sec \",net_time,\"avg time per epoch:{:.2f} sec\",((net_time)*1.0)\/epochs)\n    return model,train_loss,lrs\n\n\n\n\ndef plot_loss(loss_list):\n    sns.set_style('darkgrid')\n    matplotlib.rcParams['font.size'] = 18\n    matplotlib.rcParams['figure.figsize'] = (12,8)\n    plt.plot(loss_list,'g-')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Training loss\")\n    plt.show()\ndef plot_lr(lr_list):\n    sns.set_style('darkgrid')\n    matplotlib.rcParams['font.size'] = 18\n    matplotlib.rcParams['figure.figsize'] = (12,8)\n    plt.plot(lr_list,'b--')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Learning Rates\")\n    plt.show()    ","2b00ab4e":"#List of \"HYPERPARAMETERS\"\n\nloss_fn=F.mse_loss\nepochs=150\nmax_lr=1e-2\ngrad_clip=1\nweight_decay=0.01\nmax_epochs_stop=50","d9dc38a8":"model,loss_co,lrs=training(epochs,train_dl,loss_fn,max_lr,model,\n                           save_file_name=\"best.pth\",\n                           grad_clip=grad_clip,weight_decay=weight_decay,\n                           opt_func=torch.optim.RMSprop,\n                           max_epochs_stop=max_epochs_stop)","54638a75":"plot_loss(loss_co)","886a41df":"plot_lr(lrs)","21c3502f":"model.cpu()\nprediction=model(x_train)\nprediction","f8a30b67":"preds=prediction.detach().numpy()\nprint(\"loss:\",loss_fn(y_train,torch.from_numpy(preds)))\npreds=np.exp(preds)\ndf = pd.DataFrame(preds, columns = ['Column_A','Column_B','Column_C'])\n","2d514e12":"test_ds=pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/test.csv')\ntest_ds.drop('date_time',axis=1,inplace=True)\ntry:\n    test_ds=test_ds.values\nexcept:\n    print(test_ds.shape)\ntest_ds=torch.from_numpy(test_ds)\n\n\ntest_ds=test_ds.float()","05557f62":"model.cpu()\nprediction=model(test_ds)\npreds=prediction.detach().numpy()\npreds=np.exp(preds)\ndf = pd.DataFrame(preds, columns = ['Column_A','Column_B','Column_C'])\ndf","0e4fe1d7":"df_predict=pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/sample_submission.csv')\ndf_predict['target_carbon_monoxide']=df['Column_A']\ndf_predict['target_benzene']=df['Column_B']\ndf_predict['target_nitrogen_oxides']=df['Column_C']\ndf_predict.to_csv('Submission.csv',index=False)\ndf_predict","a164d77c":"df_predict.to_csv('submissions.csv',index=False)","5263b83e":"\n# MODEL\n","37af7aff":"## Training the model\n\nBefore we train the model, we're going to make a bunch of small but important improvements to our `fit` function:\n\n* **Learning rate scheduling**: Instead of using a fixed learning rate, we will use a learning rate scheduler, which will change the learning rate after every batch of training. There are many strategies for varying the learning rate during training, and the one we'll use is called the **\"One Cycle Learning Rate Policy\"**, which involves starting with a low learning rate, gradually increasing it batch-by-batch to a high learning rate for about 30% of epochs, then gradually decreasing it to a very low value for the remaining epochs. Learn more: https:\/\/sgugger.github.io\/the-1cycle-policy.html\n\n\n\n* **Weight decay**: We also use weight decay, which is yet another regularization technique which prevents the weights from becoming too large by adding an additional term to the loss function.Learn more: https:\/\/towardsdatascience.com\/this-thing-called-weight-decay-a7cd4bcfccab\n\n\n\n* **Gradient clipping**: Apart from the layer weights and outputs, it also helpful to limit the values of gradients to a small range to prevent undesirable changes in parameters due to large gradient values. This simple yet effective technique is called gradient clipping. Learn more: https:\/\/towardsdatascience.com\/what-is-gradient-clipping-b8e815cdfb48\n\n\nLet's define a `fit_one_cycle` function to incorporate these changes. We'll also record the learning rate used for each batch.","5d02e67c":"# This is code for Tabular Playground Series - Jul 2021 \n## Note: Beginer friendly\n\n\n## I have added\n### * Early stopping\n### * gradient clipping\n### * weights decay\n### * learning rate scheduler","98bab673":"### Since we have grad='true' we have to remove it.\n\n\n### We use `.detach().numpy()`"}}