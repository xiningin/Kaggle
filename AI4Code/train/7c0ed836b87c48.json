{"cell_type":{"ca122cc2":"code","570c4266":"code","6eaf81dc":"code","4ab9234c":"code","7b1ffa24":"code","65a07aa7":"code","3716acf5":"code","c88be43a":"code","936a1101":"code","368829f2":"code","bff5e27d":"code","a70cad73":"code","39c6f4b0":"code","ff29573c":"code","785399fa":"code","694d4cf4":"code","dfb52b4a":"code","2ef29cd3":"code","11b176f1":"code","e5430739":"code","1f069e60":"code","c3253b17":"code","d21489e2":"code","cd95d6ad":"code","b45ecabd":"code","7b359663":"code","e72301fb":"code","1814189e":"code","91005527":"code","fc02e995":"markdown","b4c03500":"markdown","a98b8650":"markdown","108ad9eb":"markdown","bbe588e8":"markdown"},"source":{"ca122cc2":"!pip install kaggle-environments -U","570c4266":"# run this if using kaggle notebooks\n!cp -r ..\/input\/lux-ai-2021\/* .","6eaf81dc":"from kaggle_environments import make\n# pick interesting seed shown in tutorial\n# https:\/\/www.kaggle.com\/stonet2000\/lux-ai-season-1-jupyter-notebook-tutorial\n# env = make(\"lux_ai_2021\", configuration={\"seed\": 562124210, \"loglevel\": 2}, debug=True)\nenv = make(\"lux_ai_2021\", configuration={\"seed\": 562_124_210, \"loglevel\": 0}, debug=True)","4ab9234c":"from lux.game import Game\nfrom lux.game_map import Cell, RESOURCE_TYPES, Position\nfrom lux.constants import Constants\nfrom lux.game_constants import GAME_CONSTANTS\nfrom lux import annotate\nimport math\nimport sys\nimport random","7b1ffa24":"import numpy as np\nfrom numpy.lib.shape_base import split\n\nclass Obs():\n    def __init__(self, observation):\n        self.observation = observation\n\n        self.wood_map = np.zeros((32,32))\n        self.coal_map = np.zeros((32,32))\n        self.uran_map = np.zeros((32,32))\n\n        self.worker_cooldown = np.zeros((2,32,32))\n        self.worker_capacity = np.zeros((2, 32, 32))\n\n        self.cart_cooldown = np.zeros((2, 32, 32))\n        self.cart_capacity = np.zeros((2, 32, 32))\n\n        self.city_tiles_cooldown = np.zeros((2, 32, 32))\n        self.city_tiles_fuel = np.zeros((2, 32, 32))\n\n        self.step = self.observation[0][\"observation\"][\"step\"]\n\n        pad_size_width = (32 - self.observation[0][\"observation\"][\"width\"]) \/\/ 2\n        pad_size_length = (32 - self.observation[0][\"observation\"][\"height\"]) \/\/ 2\n\n        self.worker_pos_dict = {}\n        self.ct_pos_dict = {}\n\n        for player in range(1):\n            ups = observation[player][\"observation\"][\"updates\"]\n            cities = {}\n            for row in ups:\n                splits = row.split(\" \")\n                if splits[0] == \"r\":\n                  if splits[1] == \"wood\":\n                    self.wood_map[(\n                      int(splits[2]) + pad_size_width,\n                      int(splits[3]) + pad_size_length)] = int(float(splits[4]))\n                  elif splits[1] == \"uranium\":\n                    self.uran_map[(\n                      int(splits[2]) + pad_size_width, \n                      int(splits[3]) + pad_size_length)] = int(float(splits[4]))\n                  elif splits[1] == \"coal\":\n                    self.coal_map[(\n                        int(splits[2]) + pad_size_width, \n                        int(splits[3]) + pad_size_length)] = int(float(splits[4]))\n                elif splits[0] == \"c\":\n                  cities[splits[2]] = int(splits[3])\n                elif splits[0] == \"u\":\n                  self.worker_capacity[(\n                    int(splits[2]),\n                    int(splits[4]) + pad_size_width,\n                    int(splits[5]) + pad_size_length\n                  )] = int(splits[7]) + int(splits[8]) + int(splits[9])\n                  self.worker_cooldown[(\n                    int(splits[2]),\n                    int(splits[4]) + pad_size_width,\n                    int(splits[5]) + pad_size_length\n                  )] = int(splits[6])\n                  self.worker_pos_dict[(\n                    int(splits[4]) + pad_size_width, \n                    int(splits[5]) + pad_size_length)] = splits[3] \n                elif splits[0] == \"ct\":\n                  city_fuel = cities.get( splits[2] )\n                  self.city_tiles_cooldown[(\n                    int(splits[1]), \n                    int(splits[3]) + pad_size_width, \n                    int(splits[4]) + pad_size_length)] = int(splits[5])\n                  self.city_tiles_fuel[(\n                    int(splits[1]), \n                    int(splits[3]) + pad_size_width, \n                    int(splits[4]) + pad_size_length)] = int(city_fuel)\n                  self.ct_pos_dict[(\n                    int(splits[3]) + pad_size_width, \n                    int(splits[4]) + pad_size_length)] = splits[2]\n                \n        self.wood_map = np.expand_dims(self.wood_map, axis=0)\n        self.uran_map= np.expand_dims(self.uran_map, axis=0)\n        self.coal_map = np.expand_dims(self.coal_map, axis=0)\n\n        self.state = np.concatenate((\n        self.wood_map \/ 1000, self.uran_map \/ 1000, self.coal_map \/ 1000, \n        self.worker_cooldown \/ 2, self.worker_capacity \/ 100, \n        self.city_tiles_fuel \/ 1000, self.city_tiles_cooldown \/ 10 ), axis=0)\n\n\ndef log_to_action(entity_action_prob, is_worker = True):\n    entity_action_dim = {\n        0: \"n\",\n        1: \"s\",\n        2: \"w\",\n        3: \"e\",\n        4: \"stay\",\n        5: \"bcity\",\n        6: \"bw\",\n        7: \"r\",\n        8: \"None\"\n    }\n\n    if is_worker:\n        ordered_actions = [(entity_action_dim[i], entity_action_prob[i]) for i in range(6)]\n    else:\n        ordered_actions = [(entity_action_dim[i], entity_action_prob[i]) for i in range(6, 9)]\n\n        ordered_actions = sorted(ordered_actions, key=lambda x: x[1], reverse=True)\n\n    return ordered_actions\n","65a07aa7":"observation = env.reset()","3716acf5":"obs = Obs(observation)","c88be43a":"obs.state.shape","936a1101":"def action_to_tensor(action_list, worker_pos_dict, ct_pos_dict):\n  action_dict = {}\n  for action in action_list:\n    splits = action.split(\" \")\n    #print(splits)\n    if splits[0] == \"m\":\n      action_dict[splits[1]] = splits[2]\n    elif splits[0] == \"bcity\":\n      action_dict[splits[1]] = splits[0]\n    elif splits[0] == \"bw\":\n      action_dict[(splits[1], splits[2])] = splits[0]\n    elif splits[0] == \"r\":\n      action_dict[(splits[1], splits[2])] = splits[0]\n\n  actions = {\n    \"n\": 0,\n    \"s\": 1,\n    \"w\": 2,\n    \"e\": 3,\n    \"stay\":4,\n    \"bcity\": 5,\n    \"bw\":6,\n    \"r\":7,\n    \"n\":8\n  }\n#   print(action_dict)\n\n  entity_action_tensor = np.zeros((9, 32, 32))\n  if len(worker_pos_dict) > 0:\n    for pos, id in worker_pos_dict.items():\n      if id not in action_dict:\n        entity_action_tensor[5, int(pos[0]), int(pos[1])] = 1\n      else:\n#         print(action_dict[id])\n#         print(actions[action_dict[id]])\n        entity_action_tensor[actions[action_dict[id]], pos[0], pos[1]] = 1\n    \n  if len(ct_pos_dict) > 0:\n    for pos, id in ct_pos_dict.items():\n      if id not in action_dict:\n        entity_action_tensor[6, int(pos[0]), int(pos[1])] = 1\n      else:\n        entity_action_tensor[actions[action_dict[(int(pos[0]), int(pos[1]))]], int(pos[0]), int(pos[1])] = 1\n  #print(entity_action_tensor.shape)\n  return entity_action_tensor","368829f2":"import torch \nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef single_conv5(in_channels, out_channels):\n  return nn.Sequential(\n    nn.Conv2d(in_channels, out_channels, 5),\n    nn.BatchNorm2d(out_channels, eps = 1e-5, momentum=0.1),\n    nn.Tanh()\n  )\n\ndef single_conv3(in_channels, out_channels):\n  return nn.Sequential(\n    nn.Conv2d(in_channels, out_channels, 3),\n    nn.BatchNorm2d(out_channels, eps = 1e-5, momentum=0.1),\n    nn.Tanh()\n  )\n\ndef single_conv2(in_channels, out_channels):\n  return nn.Sequential(\n    nn.Conv2d(in_channels, out_channels, 2),\n    nn.BatchNorm2d(out_channels, eps = 1e-5, momentum=0.1),\n    nn.Tanh()\n  )\n\nclass Actor(nn.Module):\n    def __init__(self, Cin, out_size, seed):\n\n        super(Actor, self).__init__()\n        self.seed = torch.manual_seed(seed)\n\n        self.maxpool = nn.MaxPool2d(2)\n\n        self.layer1 = single_conv3(Cin, 16)\n        self.layer2_1 = single_conv5(16, 32)\n        self.layer2_2 = single_conv5(32, 32)\n        self.layer2_3 = single_conv3(32, 32)\n        self.layer3_1 = single_conv3(32, 32)\n        self.layer3_2 = single_conv3(32, 64)\n        self.layer4 = single_conv5(64, 128)\n\n        self.fc1 = nn.Sequential(\n        nn.Linear(128*12*12, 64),\n        nn.ReLU(inplace=True)\n        )\n        self.fc2 = nn.Linear(64, out_size)\n\n    def forward(self, x1):\n        x1 = self.layer1(x1)\n\n        x1 = self.layer2_1(x1)\n        #print(f'conv1: {x1.data.cpu().numpy().shape}')\n\n        x1 = self.layer2_2(x1)\n        #print(f'conv2: {x1.data.cpu().numpy().shape}')\n\n        x1 = self.layer2_3(x1)\n        x1 = self.layer3_1(x1)\n        #print(f'conv3: {x1.data.cpu().numpy().shape}')\n        x1 = self.layer3_2(x1)\n        x1 = self.layer4(x1)\n        #print(f'conv4: {x1.data.cpu().numpy().shape}')\n\n        x1 = x1.view(-1, 128*12*12)\n\n        x = self.fc1(x1)\n        #print(f'lconv6: {x.data.cpu().numpy().shape}')\n        out = self.fc2(x)\n\n        return out\n    ","bff5e27d":"import numpy as np\nimport random\nimport copy\nfrom collections import namedtuple, deque, defaultdict\nimport os\n\nfrom torch._C import device\nfrom torch.random import seed\n\nimport torch\nimport torch.nn.functional as F \nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\n\n# BUFFER_SIZE = int(2e5)\nBATCH_SIZE = 32\nLR_ACTOR = 5e-5 # learning rate of the actor\nLR_DECAY = 0.9\nDISCOUNT = 0.99\nUPDATE_TARGET_EVERY = 5\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ndef calc_loss(pred, target, metrics):\n    criterion = nn.MSELoss()\n\n    loss = criterion(pred, target)\n\n    pred = torch.argmax(pred, dim=1)\n\n    acc = np.sum(pred.data.cpu().numpy() == target.data.cpu().numpy()) \/ len(target.data.cpu().numpy())\n\n    metrics['loss'] += loss.data.cpu().numpy()\n    #metrics['acc'] = acc\n    return loss\n\nclass ReplayBuffer:\n    def __init__(self, buffer_size, batch_size, seed):\n        self.memory = deque(maxlen=buffer_size)\n        self.batch_size = batch_size\n        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"new_state\", \"reward\", \"done\"])\n        self.seed = random.seed(seed)\n\n    def add(self, state, action, new_state, reward, done):\n        e = self.experience(state=state, action=action, new_state=new_state, reward=reward, done=done)\n        self.memory.append(e)\n\n    def sample(self):\n        experiences = random.sample(self.memory, k=self.batch_size)\n\n        states1 = torch.from_numpy(np.stack([e.state for e in experiences if e is not None], axis=0)).float().to(device)\n        actions = torch.from_numpy(np.stack([e.action for e in experiences if e is not None], axis=0)).float().to(device)\n        states2 = torch.from_numpy(np.stack([e.new_state for e in experiences if e is not None], axis=0)).float().to(device)\n        rewards = torch.from_numpy(np.stack([e.reward for e in experiences if e is not None], axis=0)).float().to(device)\n        dones = torch.from_numpy(np.stack([e.done for e in experiences if e is not None], axis=0)).float().to(device)\n\n        return (states1, actions, states2, rewards, dones)\n\n    def __len__(self):\n        return len(self.memory)\n\n\nclass ActorAgent():\n    def __init__(self, cin, out_worker, out_ctiles, random_seed):\n        self.cin = cin\n        self.out_worker = out_worker\n        self.out_ctiles = out_ctiles\n#         self.out_ctiles = out_ctiles\n        self.seed = random.seed(random_seed)\n\n        self.worker_model = Actor(Cin=cin, out_size=out_worker, seed=random_seed).to(device)\n        self.worker_model_optimizer = optim.Adam(self.worker_model.parameters(), lr=LR_ACTOR)\n        self.worker_model_scheduler = optim.lr_scheduler.ExponentialLR(self.worker_model_optimizer, gamma=LR_DECAY)\n        \n        self.target_worker_model = Actor(Cin=cin, out_size=out_worker, seed=random_seed).to(device)\n        self.target_worker_model_optimizer = optim.Adam(self.worker_model.parameters(), lr=LR_ACTOR)\n        self.target_worker_model_scheduler = optim.lr_scheduler.ExponentialLR(self.worker_model_optimizer, gamma=LR_DECAY)\n        \n        self.target_worker_model.load_state_dict(self.worker_model.state_dict())\n        \n        self.ctiles_model = Actor(Cin=cin, out_size=out_ctiles, seed=random_seed).to(device)\n        self.ctiles_model_optimizer = optim.Adam(self.ctiles_model.parameters(), lr=LR_ACTOR)\n        self.ctiles_model_scheduler = optim.lr_scheduler.ExponentialLR(self.ctiles_model_optimizer, gamma=LR_DECAY)\n\n        self.target_ctiles_model = Actor(Cin=cin, out_size=out_ctiles, seed=random_seed).to(device)\n        self.target_ctiles_model_optimizer = optim.Adam(self.ctiles_model.parameters(), lr=LR_ACTOR)\n        self.target_ctiles_model_scheduler = optim.lr_scheduler.ExponentialLR(self.ctiles_model_optimizer, gamma=LR_DECAY) \n        \n        self.target_ctiles_model.load_state_dict(self.ctiles_model.state_dict())\n        \n        self.worker_memory = ReplayBuffer(int(1e6), BATCH_SIZE, random_seed)\n        self.ctiles_memory = ReplayBuffer(int(1e4), BATCH_SIZE, random_seed)\n        \n        self.target_update_counter = 0\n        \n    def learnworker(self, states, target):\n        states = states.to(device)\n        target = target.to(device)\n        pred = self.worker_model(states)\n\n        #     print(pred.shape)\n        #     print(target.shape)\n        metrics = defaultdict(float)\n\n        loss = calc_loss(pred, target, metrics)\n\n        self.worker_model_optimizer.zero_grad()\n        loss.backward()\n        self.worker_model_optimizer.step()\n        \n        return metrics\n\n    def learnctiles(self, states, target):\n        states = states.to(device)\n        target = target.to(device)\n\n        pred = self.ctiles_model(states)\n\n        metrics = defaultdict(float)\n        loss = calc_loss(pred, target, metrics)\n\n        self.ctiles_model_optimizer.zero_grad()\n        loss.backward()\n        self.ctiles_model_optimizer.step()\n        return metrics\n    \n    def act(self, state, is_worker = True):\n        state = torch.from_numpy(state).float().to(device)\n        if is_worker:\n            self.worker_model.eval()\n            with torch.no_grad():\n                out = self.worker_model(state)\n                out = out.cpu().data.numpy()\n            self.worker_model.train()\n        else:\n            self.ctiles_model.eval()\n            with torch.no_grad():\n                out = self.ctiles_model(state)\n                out = out.cpu().data.numpy()\n            self.ctiles_model.train()\n        return out    \n\n    def add(self, state, action, new_state, reward, done, is_worker = True):\n        if is_worker:\n            self.worker_memory.add(state, action, new_state, reward, done)\n        else:\n            self.ctiles_memory.add(state, action, new_state, reward, done)\n\n    def lr_step(self):\n        self.worker_model_scheduler.step()\n        self.ctiles_model_scheduler.step()\n\n    def step(self):\n        worker_metric_mean = defaultdict(float)\n        ctiles_metric_mean = defaultdict(float)\n        if len(self.worker_memory) > 20_000:\n            for _ in range(100):\n                experiences = self.worker_memory.sample()\n                worker_metrics = self.learnworker(experiences)\n                for key, val in worker_metrics.items():\n                    worker_metric_mean[key] += val\n            for key, val in worker_metric_mean.items():\n                worker_metric_mean[key] = val \/ 1000\n        if len(self.ctiles_memory) > 20_000:\n            for _ in range(100):\n                experiences = self.ctiles_memory.sample()\n                ctiles_metrics = self.learnctiles(experiences)\n                for key, val in ctiles_metrics.items():\n                    ctiles_metric_mean[key] += val\n\n            for key, val in ctiles_metric_mean.items():\n                ctiles_metric_mean[key] = val\/ 100\n\n        return worker_metric_mean, ctiles_metric_mean \n    \n    def train(self, terminal_state):\n        if len(self.worker_memory) < 20_000 or len(self.ctiles_memory) < 10_000:\n            return\n        worker_minibatch = self.worker_memory.sample()\n        ctiles_minibatch = self.ctiles_memory.sample()\n\n        #print(ctiles_minibatch)\n        #  transition: (states1, actions, states2, rewards, dones)\n        current_worker_states = worker_minibatch[0].to(device)\n        current_worker_qs_list = self.worker_model(current_worker_states).to(device)\n        new_current_worker_state = worker_minibatch[2].to(device)\n        future_qs_worker_list = self.target_worker_model(new_current_worker_state).to(device)\n\n        current_ctiles_states = ctiles_minibatch[0].to(device)\n        current_ctiles_qs_list = self.ctiles_model(current_ctiles_states).to(device)\n        new_current_ctiles_state = ctiles_minibatch[2].to(device)\n        future_qs_ctiles_list = self.target_ctiles_model(new_current_ctiles_state).to(device)\n\n        X_worker = torch.tensor([]).to(device)\n        y_worker = torch.tensor([]).to(device)\n\n        for index in range(len(worker_minibatch[0])):\n            state = worker_minibatch[0][index]\n            action = int(worker_minibatch[1][index])\n            new_state = worker_minibatch[2][index]\n            reward = worker_minibatch[3][index]\n            done = worker_minibatch[4][index]\n\n            if not done:\n                max_future_worker_q = torch.max(future_qs_worker_list[index])\n                new_q_worker = reward + DISCOUNT * max_future_worker_q\n            else:\n                new_q_worker = reward\n            current_qs = current_worker_qs_list[index]\n            current_qs[action] = new_q_worker\n\n            X_worker = torch.cat((X_worker, torch.unsqueeze(state, 0)))\n            y_worker = torch.cat((y_worker, torch.unsqueeze(current_qs, 0)))\n\n        X_ctiles = torch.tensor([]).to(device)\n        y_ctiles = torch.tensor([]).to(device)\n\n        for index in range(len(ctiles_minibatch)):\n            state = ctiles_minibatch[0][index]\n            action = int(ctiles_minibatch[1][index])\n            new_state = ctiles_minibatch[2][index]\n            reward = ctiles_minibatch[3][index]\n            done = ctiles_minibatch[4][index]            \n\n            if not done:\n                max_future_ctiles_q = torch.max(future_qs_ctiles_list[index])\n                new_q_ctiles = reward + DISCOUNT * max_future_ctiles_q\n            else:\n                new_q_ctiles = reward\n            current_qs = current_ctiles_qs_list[index]\n            if action > 2: \n                action = 2\n            current_qs[action] = new_q_ctiles\n\n            X_ctiles = torch.cat((X_ctiles, torch.unsqueeze(state, 0)))\n            y_ctiles = torch.cat((y_ctiles, torch.unsqueeze(current_qs, 0)))\n\n        self.learnworker(X_worker, y_worker)\n        self.learnctiles(X_ctiles, y_ctiles)\n        if terminal_state:\n            self.target_update_counter += 1\n\n        if self.target_update_counter > UPDATE_TARGET_EVERY:\n            self.target_worker_model.load_state_dict(self.worker_model.state_dict())\n            self.target_ctiles_model.load_state_dict(self.ctiles_model.state_dict())\n            self.target_update_counter = 0","a70cad73":"def generate_offset_map(Hmap, row_c, col_c):\n    Hmap_copy = Hmap.copy()\n    v_shift = row_c - 16\n    h_shift = col_c - 16\n    h_index = (np.arange(32) + h_shift) % 32\n    v_index = (np.arange(32) + v_shift) % 32\n    temp = Hmap_copy[:, v_index]\n    \n    return temp[:, :, h_index]","39c6f4b0":"EPISODES = 1\nepsilon = 1\nEPSILON_DECAY = 0.9995\nMIN_EPSILON = 0.001\nSHOW_EVERY = 1000\nREPEAT = 300","ff29573c":"from lux.game import Game\nfrom lux.game_map import Cell, RESOURCE_TYPES, Position\nfrom lux.constants import Constants\nfrom lux.game_constants import GAME_CONSTANTS\nfrom lux import annotate\nimport math\nimport numpy as np\n\n### Define helper functions\n\n# this snippet finds all resources stored on the map and puts them into a list so we can search over them\ndef find_resources(game_state):\n    resource_tiles: list[Cell] = []\n    width, height = game_state.map_width, game_state.map_height\n    for y in range(height):\n        for x in range(width):\n            cell = game_state.map.get_cell(x, y)\n            if cell.has_resource():\n                resource_tiles.append(cell)\n    return resource_tiles\n\n# the next snippet finds the closest resources that we can mine given position on a map\ndef find_closest_resources(pos, player, resource_tiles):\n    closest_dist = math.inf\n    closest_resource_tile = None\n    for resource_tile in resource_tiles:\n        # we skip over resources that we can't mine due to not having researched them\n        if resource_tile.resource.type == Constants.RESOURCE_TYPES.COAL and not player.researched_coal(): continue\n        if resource_tile.resource.type == Constants.RESOURCE_TYPES.URANIUM and not player.researched_uranium(): continue\n        dist = resource_tile.pos.distance_to(pos)\n        if dist < closest_dist:\n            closest_dist = dist\n            closest_resource_tile = resource_tile\n    return closest_resource_tile, closest_dist\n\ndef find_closest_city_tile(pos, player):\n    closest_city_tile = None\n    closest_dist = math.inf\n    if len(player.cities) > 0:\n        # the cities are stored as a dictionary mapping city id to the city object, which has a citytiles field that\n        # contains the information of all citytiles in that city\n        for k, city in player.cities.items():\n            for city_tile in city.citytiles:\n                dist = city_tile.pos.distance_to(pos)\n                if dist < closest_dist:\n                    closest_dist = dist\n                    closest_city_tile = city_tile\n    return closest_city_tile, closest_dist\n\ngame_state = None\n\ndef get_random_step():\n    return np.random.choice(['s','n','w','e'])\n\n\ndef baseline_agent(observation, game_state, player, opponent):\n    actions = []\n    ### AI Code goes down here! ### \n    width, height = game_state.map.width, game_state.map.height\n\n    resource_tiles = find_resources(game_state)\n        \n    # max number of units available\n    units_cap = sum([len(x.citytiles) for x in player.cities.values()])\n    # current number of units\n    units  = len(player.units)\n    \n    cities = list(player.cities.values())\n    if len(cities) > 0:\n        city = cities[0]\n        created_worker = (units >= units_cap)\n        for city_tile in city.citytiles[::-1]:\n            if city_tile.can_act():\n                if created_worker:\n                    # let's do research\n                    action = city_tile.research()\n                    actions.append(action)\n                else:\n                    # let's create one more unit in the last created city tile if we can\n                    action = city_tile.build_worker()\n                    actions.append(action)\n                    created_worker = True\n    \n    \n    # we want to build new tiless only if we have a lot of fuel in all cities\n    can_build = True\n    night_steps_left = ((359 - observation[\"step\"]) \/\/ 40 + 1) * 10\n    for city in player.cities.values():            \n        if city.fuel \/ (city.get_light_upkeep() + 20) < min(night_steps_left, 20):\n            can_build = False\n       \n    steps_until_night = 30 - observation[\"step\"] % 40\n    \n    \n    # we will keet all tiles where any unit wants to move in this set to avoid collisions\n    taken_tiles = set()\n    for unit in player.units:\n        # it is too strict but we don't allow to go to the the currently occupied tile\n        taken_tiles.add((unit.pos.x, unit.pos.y))\n        \n    for city in opponent.cities.values():\n        for city_tile in city.citytiles:\n            taken_tiles.add((city_tile.pos.x, city_tile.pos.y))\n    \n    # we can collide in cities so we will use this tiles as exceptions\n    city_tiles = {(tile.pos.x, tile.pos.y) for city in player.cities.values() for tile in city.citytiles}\n    \n    \n    for unit in player.units:\n        if unit.can_act():\n            closest_resource_tile, closest_resource_dist = find_closest_resources(unit.pos, player, resource_tiles)\n            closest_city_tile, closest_city_dist = find_closest_city_tile(unit.pos, player)\n            \n            # we will keep possible actions in a priority order here\n            directions = []\n            \n            # if we can build and we are near the city let's do it\n            if unit.is_worker() and unit.can_build(game_state.map) and ((closest_city_dist == 1 and can_build) or (closest_city_dist is None)):\n                # build a new cityTile\n                action = unit.build_city()\n                actions.append(action)  \n                can_build = False\n                continue\n            \n            # base cooldown for different units types\n            base_cd = 2 if unit.is_worker() else 3\n            \n            # how many steps the unit needs to get back to the city before night (without roads)\n            steps_to_city = unit.cooldown + base_cd * closest_city_dist\n            \n            # if we are far from the city in the evening or just full let's go home\n            if (steps_to_city + 3 > steps_until_night or unit.get_cargo_space_left() == 0) and closest_city_tile is not None:\n                actions.append(annotate.line(unit.pos.x, unit.pos.y, closest_city_tile.pos.x, closest_city_tile.pos.y))\n                directions = [unit.pos.direction_to(closest_city_tile.pos)]\n            else:\n                # if there is no risks and we are not mining resources right now let's move toward resources\n                if closest_resource_dist != 0 and closest_resource_tile is not None:\n                    actions.append(annotate.line(unit.pos.x, unit.pos.y, closest_resource_tile.pos.x, closest_resource_tile.pos.y))\n                    directions = [unit.pos.direction_to(closest_resource_tile.pos)]\n                    # optionally we can add random steps\n                    for _ in range(2):\n                        directions.append(get_random_step())\n\n            moved = False\n            for next_step_direction in directions:\n                next_step_position = unit.pos.translate(next_step_direction, 1)\n                next_step_coordinates = (next_step_position.x, next_step_position.y)\n                # make only moves without collision\n                if next_step_coordinates not in taken_tiles or next_step_coordinates in city_tiles:\n                    action = unit.move(next_step_direction)\n                    actions.append(action)\n                    taken_tiles.add(next_step_coordinates)\n                    moved = True\n                    break\n            \n            if not moved:\n                # if we are not moving the tile is occupied\n                taken_tiles.add((unit.pos.x,unit.pos.y))\n    \n    return actions","785399fa":"agent = ActorAgent(cin = 11, out_worker=6, out_ctiles=3, random_seed=42)","694d4cf4":"direction_dict = {\n        0: Constants.DIRECTIONS.NORTH,\n        1: Constants.DIRECTIONS.SOUTH,\n        2: Constants.DIRECTIONS.WEST,\n        3: Constants.DIRECTIONS.EAST,\n}","dfb52b4a":"from tqdm import tqdm\n\nfor episode in tqdm(range(1, EPISODES + 1), ascii=True, unit=\"episode\"):\n    env = make(\"lux_ai_2021\", configuration={\"seed\": np.random.randint(100_000_000, 999_999_999), \"loglevel\": 0}, debug=True)\n    for re in range(REPEAT):\n        episode_reward = 0\n        step = 0\n        current_obs = env.reset()\n        parsed_obs = Obs(current_obs)\n        current_state = parsed_obs.state\n\n        observation = current_obs[0][\"observation\"]\n        game_state = Game()\n        game_state._initialize(observation[\"updates\"])\n        game_state._update(observation[\"updates\"][2:])\n        game_state.id = observation.player\n\n        player = game_state.players[observation.player]\n        opponent = game_state.players[(observation.player + 1) % 2]\n        #print(\"HERE START\")\n        done = False\n        while not done:\n\n                #print(\"HERE 1\")\n                base_line_actions = baseline_agent(observation, game_state, opponent, player)\n                actions = []\n                unit_actions = {}\n                ctiles_ation = {}\n                #print(\"HERE 2\")\n                for unit in player.units:\n                    if unit.can_act():\n                        if unit.is_worker():\n                            offset_state = generate_offset_map(current_state, unit.pos.x, unit.pos.y)\n                            offset_state_expand = np.expand_dims(offset_state, axis=0)\n                            if np.random.random() > epsilon:\n\n                                player_action = np.argmax( agent.act(offset_state_expand, is_worker=True) )\n\n                                unit_actions[(unit.pos.x, unit.pos.y)] = [offset_state, player_action]\n                            else:\n                                player_action = np.random.randint(0, 6)\n                                unit_actions[(unit.pos.x, unit.pos.y)] = [offset_state, player_action]\n\n                            action = None\n                            if player_action < 4:\n                                #try:\n                                    action = unit.move(direction_dict[player_action])\n                                #except:\n                                    #pass\n                            elif player_action == 5:\n                                #try:\n                                    action = unit.build_city()\n                                #except:\n                                    #pass\n                            if action is not None:\n                                actions.append(action)\n                #print(\"HERE 3\")\n                cities = list(player.cities.values())\n                if len(cities) > 0:\n                    for city in cities:\n                        for city_tile in city.citytiles[::-1]:\n                            if city_tile.can_act():\n                                offset_state = generate_offset_map(current_state, city_tile.pos.x, city_tile.pos.y)\n                                offset_state_expand = np.expand_dims(offset_state, axis=0)\n                                if np.random.random() > epsilon:\n                                    ctile_action = np.argmax( agent.act( offset_state_expand, is_worker=False ))\n\n                                    ctiles_ation[(city_tile.pos.x, city_tile.pos.y)] = [offset_state, ctile_action]\n                                else:\n                                    ctile_action = np.random.randint(0, 3)\n                                    ctiles_ation[(city_tile.pos.x, city_tile.pos.y)] = [offset_state, ctile_action]\n\n                                action = None\n                                if ctile_action == 0:\n                                    #try:\n                                        action = city_tile.build_worker()\n                                    #except:\n                                        #pass\n                                elif ctile_action == 1:\n                                    #try:\n                                        action = city_tile.research()\n                                    #except:\n                                        #pass\n                                if action is not None:\n                                    actions.append(action)\n                #print(\"HERE 4\")\n                new_observations = env.step([actions, base_line_actions])\n                new_obs = Obs(new_observations)\n                reward = new_observations[0][\"reward\"]\n                done = new_observations[0][\"status\"] == \"DONE\"\n                new_state = new_obs.state\n\n                episode_reward += reward\n\n                #print(\"HERE 5\")\n                for (pos_x,pos_y,)  in unit_actions: \n    #                 print(pos_x, pos_y)\n    #                 print(unit_actions[(pos_x, pos_y)])\n                    offset_state, status_action = unit_actions[(pos_x, pos_y)]\n                    \n                    new_offset_state = generate_offset_map(new_state, pos_x, pos_x)\n                    \n                    agent.add(offset_state, status_action, new_offset_state, reward, done, is_worker=True)\n                for (pos_x, pos_y) in ctiles_ation:\n                    offset_state, status_action = ctiles_ation[(pos_x, pos_y)]\n                    \n                    new_offset_state = generate_offset_map(new_state, pos_x, pos_y)\n                    \n                    agent.add(offset_state, status_action, new_offset_state, reward, done, is_worker=False)\n\n                #print(\"HERE 6\")\n                agent.train(done)\n\n                current_obs = new_observations\n                parsed_obs = new_obs\n                current_state = new_state\n                game_state._update(observation[\"updates\"])\n                    # Decay epsilon\n        if epsilon > MIN_EPSILON:\n            epsilon *= EPSILON_DECAY\n            epsilon = max(MIN_EPSILON, epsilon)\n        \n        if re % 10 == 0:\n            print(f\"reward: {episode_reward}\")\n            torch.save(agent.worker_model.state_dict(), f\".\/worker_model_v{re\/\/10}\")\n            torch.save(agent.ctiles_model.state_dict(), f\".\/ctiles_model_v{re\/\/10}\")","2ef29cd3":"agent.worker_model.load_state_dict(torch.load(\".\/worker_model_v29\"), strict=False)","11b176f1":"agent.ctiles_model.load_state_dict(torch.load(\".\/ctiles_model_v29\"), strict=False)","e5430739":"worker_state_dict = agent.worker_model.state_dict()","1f069e60":"file = open(\".\/worker_model_v29\")","c3253b17":"#worker_state_dict","d21489e2":"%%writefile agent.py\nfrom lux.game import Game\nfrom lux.game_map import Cell, RESOURCE_TYPES, Position\nfrom lux.constants import Constants\nfrom lux.game_constants import GAME_CONSTANTS\nfrom lux import annotate\nimport math\nimport numpy as np\n\n### Define helper functions\n\n# this snippet finds all resources stored on the map and puts them into a list so we can search over them\ndef find_resources(game_state):\n    resource_tiles: list[Cell] = []\n    width, height = game_state.map_width, game_state.map_height\n    for y in range(height):\n        for x in range(width):\n            cell = game_state.map.get_cell(x, y)\n            if cell.has_resource():\n                resource_tiles.append(cell)\n    return resource_tiles\n\n# the next snippet finds the closest resources that we can mine given position on a map\ndef find_closest_resources(pos, player, resource_tiles):\n    closest_dist = math.inf\n    closest_resource_tile = None\n    for resource_tile in resource_tiles:\n        # we skip over resources that we can't mine due to not having researched them\n        if resource_tile.resource.type == Constants.RESOURCE_TYPES.COAL and not player.researched_coal(): continue\n        if resource_tile.resource.type == Constants.RESOURCE_TYPES.URANIUM and not player.researched_uranium(): continue\n        dist = resource_tile.pos.distance_to(pos)\n        if dist < closest_dist:\n            closest_dist = dist\n            closest_resource_tile = resource_tile\n    return closest_resource_tile, closest_dist\n\ndef find_closest_city_tile(pos, player):\n    closest_city_tile = None\n    closest_dist = math.inf\n    if len(player.cities) > 0:\n        # the cities are stored as a dictionary mapping city id to the city object, which has a citytiles field that\n        # contains the information of all citytiles in that city\n        for k, city in player.cities.items():\n            for city_tile in city.citytiles:\n                dist = city_tile.pos.distance_to(pos)\n                if dist < closest_dist:\n                    closest_dist = dist\n                    closest_city_tile = city_tile\n    return closest_city_tile, closest_dist\n\ngame_state = None\n\ndef get_random_step():\n    return np.random.choice(['s','n','w','e'])\n\n\ndef agent(observation, configuration):\n    global game_state\n\n    ### Do not edit ###\n    if observation[\"step\"] == 0:\n        game_state = Game()\n        game_state._initialize(observation[\"updates\"])\n        game_state._update(observation[\"updates\"][2:])\n        game_state.id = observation.player\n    else:\n        game_state._update(observation[\"updates\"])\n    \n    actions = []\n\n    ### AI Code goes down here! ### \n    player = game_state.players[observation.player]\n    opponent = game_state.players[(observation.player + 1) % 2]\n    width, height = game_state.map.width, game_state.map.height\n\n    resource_tiles = find_resources(game_state)\n        \n    # max number of units available\n    units_cap = sum([len(x.citytiles) for x in player.cities.values()])\n    # current number of units\n    units  = len(player.units)\n    \n    cities = list(player.cities.values())\n    if len(cities) > 0:\n        city = cities[0]\n        created_worker = (units >= units_cap)\n        for city_tile in city.citytiles[::-1]:\n            if city_tile.can_act():\n                if created_worker:\n                    # let's do research\n                    action = city_tile.research()\n                    actions.append(action)\n                else:\n                    # let's create one more unit in the last created city tile if we can\n                    action = city_tile.build_worker()\n                    actions.append(action)\n                    created_worker = True\n    \n    \n    # we want to build new tiless only if we have a lot of fuel in all cities\n    can_build = True\n    night_steps_left = ((359 - observation[\"step\"]) \/\/ 40 + 1) * 10\n    for city in player.cities.values():            \n        if city.fuel \/ (city.get_light_upkeep() + 20) < min(night_steps_left, 30):\n            can_build = False\n       \n    steps_until_night = 30 - observation[\"step\"] % 40\n    \n    \n    # we will keet all tiles where any unit wants to move in this set to avoid collisions\n    taken_tiles = set()\n    for unit in player.units:\n        # it is too strict but we don't allow to go to the the currently occupied tile\n        taken_tiles.add((unit.pos.x, unit.pos.y))\n        \n    for city in opponent.cities.values():\n        for city_tile in city.citytiles:\n            taken_tiles.add((city_tile.pos.x, city_tile.pos.y))\n    \n    # we can collide in cities so we will use this tiles as exceptions\n    city_tiles = {(tile.pos.x, tile.pos.y) for city in player.cities.values() for tile in city.citytiles}\n    \n    \n    for unit in player.units:\n        if unit.can_act():\n            closest_resource_tile, closest_resource_dist = find_closest_resources(unit.pos, player, resource_tiles)\n            closest_city_tile, closest_city_dist = find_closest_city_tile(unit.pos, player)\n            \n            # we will keep possible actions in a priority order here\n            directions = []\n            \n            # if we can build and we are near the city let's do it\n            if unit.is_worker() and unit.can_build(game_state.map) and ((closest_city_dist == 1 and can_build) or (closest_city_dist is None)):\n                # build a new cityTile\n                action = unit.build_city()\n                actions.append(action)  \n                can_build = False\n                continue\n            \n            # base cooldown for different units types\n            base_cd = 2 if unit.is_worker() else 3\n            \n            # how many steps the unit needs to get back to the city before night (without roads)\n            steps_to_city = unit.cooldown + base_cd * closest_city_dist\n            \n            # if we are far from the city in the evening or just full let's go home\n            if (steps_to_city + 3 > steps_until_night or unit.get_cargo_space_left() == 0) and closest_city_tile is not None:\n                actions.append(annotate.line(unit.pos.x, unit.pos.y, closest_city_tile.pos.x, closest_city_tile.pos.y))\n                directions = [unit.pos.direction_to(closest_city_tile.pos)]\n            else:\n                # if there is no risks and we are not mining resources right now let's move toward resources\n                if closest_resource_dist != 0 and closest_resource_tile is not None:\n                    actions.append(annotate.line(unit.pos.x, unit.pos.y, closest_resource_tile.pos.x, closest_resource_tile.pos.y))\n                    directions = [unit.pos.direction_to(closest_resource_tile.pos)]\n                    # optionally we can add random steps\n                    for _ in range(2):\n                        directions.append(get_random_step())\n\n            moved = False\n            for next_step_direction in directions:\n                next_step_position = unit.pos.translate(next_step_direction, 1)\n                next_step_coordinates = (next_step_position.x, next_step_position.y)\n                # make only moves without collision\n                if next_step_coordinates not in taken_tiles or next_step_coordinates in city_tiles:\n                    action = unit.move(next_step_direction)\n                    actions.append(action)\n                    taken_tiles.add(next_step_coordinates)\n                    moved = True\n                    break\n            \n            if not moved:\n                # if we are not moving the tile is occupied\n                taken_tiles.add((unit.pos.x,unit.pos.y))\n    return actions","cd95d6ad":"%%writefile rl_agent.py\nfrom lux.game import Game\nfrom lux.game_map import Cell, RESOURCE_TYPES, Position\nfrom lux.constants import Constants\nfrom lux.game_constants import GAME_CONSTANTS\nfrom lux import annotate\nimport math\nimport sys\nimport random\n\nimport numpy as np\nfrom numpy.lib.shape_base import split\n\nclass Obs():\n    def __init__(self, observation):\n        self.observation = observation\n\n        self.wood_map = np.zeros((32,32))\n        self.coal_map = np.zeros((32,32))\n        self.uran_map = np.zeros((32,32))\n\n        self.worker_cooldown = np.zeros((2,32,32))\n        self.worker_capacity = np.zeros((2, 32, 32))\n\n        self.cart_cooldown = np.zeros((2, 32, 32))\n        self.cart_capacity = np.zeros((2, 32, 32))\n\n        self.city_tiles_cooldown = np.zeros((2, 32, 32))\n        self.city_tiles_fuel = np.zeros((2, 32, 32))\n\n        self.step = self.observation[\"step\"]\n\n        pad_size_width = (32 - self.observation[\"width\"]) \/\/ 2\n        pad_size_length = (32 - self.observation[\"height\"]) \/\/ 2\n\n        self.worker_pos_dict = {}\n        self.ct_pos_dict = {}\n\n        ups = observation[\"updates\"]\n        cities = {}\n        for row in ups:\n            splits = row.split(\" \")\n            if splits[0] == \"r\":\n              if splits[1] == \"wood\":\n                self.wood_map[(\n                  int(splits[2]) + pad_size_width,\n                  int(splits[3]) + pad_size_length)] = int(float(splits[4]))\n              elif splits[1] == \"uranium\":\n                self.uran_map[(\n                  int(splits[2]) + pad_size_width, \n                  int(splits[3]) + pad_size_length)] = int(float(splits[4]))\n              elif splits[1] == \"coal\":\n                self.coal_map[(\n                    int(splits[2]) + pad_size_width, \n                    int(splits[3]) + pad_size_length)] = int(float(splits[4]))\n            elif splits[0] == \"c\":\n              cities[splits[2]] = int(splits[3])\n            elif splits[0] == \"u\":\n              self.worker_capacity[(\n                int(splits[2]),\n                int(splits[4]) + pad_size_width,\n                int(splits[5]) + pad_size_length\n              )] = int(splits[7]) + int(splits[8]) + int(splits[9])\n              self.worker_cooldown[(\n                int(splits[2]),\n                int(splits[4]) + pad_size_width,\n                int(splits[5]) + pad_size_length\n              )] = int(splits[6])\n              self.worker_pos_dict[(\n                int(splits[4]) + pad_size_width, \n                int(splits[5]) + pad_size_length)] = splits[3] \n            elif splits[0] == \"ct\":\n              city_fuel = cities.get( splits[2] )\n              self.city_tiles_cooldown[(\n                int(splits[1]), \n                int(splits[3]) + pad_size_width, \n                int(splits[4]) + pad_size_length)] = int(splits[5])\n              self.city_tiles_fuel[(\n                int(splits[1]), \n                int(splits[3]) + pad_size_width, \n                int(splits[4]) + pad_size_length)] = int(city_fuel)\n              self.ct_pos_dict[(\n                int(splits[3]) + pad_size_width, \n                int(splits[4]) + pad_size_length)] = splits[2]\n                \n        self.wood_map = np.expand_dims(self.wood_map, axis=0)\n        self.uran_map= np.expand_dims(self.uran_map, axis=0)\n        self.coal_map = np.expand_dims(self.coal_map, axis=0)\n\n        self.state = np.concatenate((\n        self.wood_map \/ 1000, self.uran_map \/ 1000, self.coal_map \/ 1000, \n        self.worker_cooldown \/ 2, self.worker_capacity \/ 100, \n        self.city_tiles_fuel \/ 1000, self.city_tiles_cooldown \/ 10 ), axis=0)\n\n\ndef log_to_action(entity_action_prob, is_worker = True):\n    entity_action_dim = {\n        0: \"n\",\n        1: \"s\",\n        2: \"w\",\n        3: \"e\",\n        4: \"stay\",\n        5: \"bcity\",\n        6: \"bw\",\n        7: \"r\",\n        8: \"None\"\n    }\n\n    if is_worker:\n        ordered_actions = [(entity_action_dim[i], entity_action_prob[i]) for i in range(6)]\n    else:\n        ordered_actions = [(entity_action_dim[i], entity_action_prob[i]) for i in range(6, 9)]\n\n        ordered_actions = sorted(ordered_actions, key=lambda x: x[1], reverse=True)\n\n    return ordered_actions\n\ndef action_to_tensor(action_list, worker_pos_dict, ct_pos_dict):\n  action_dict = {}\n  for action in action_list:\n    splits = action.split(\" \")\n    #print(splits)\n    if splits[0] == \"m\":\n      action_dict[splits[1]] = splits[2]\n    elif splits[0] == \"bcity\":\n      action_dict[splits[1]] = splits[0]\n    elif splits[0] == \"bw\":\n      action_dict[(splits[1], splits[2])] = splits[0]\n    elif splits[0] == \"r\":\n      action_dict[(splits[1], splits[2])] = splits[0]\n\n  actions = {\n    \"n\": 0,\n    \"s\": 1,\n    \"w\": 2,\n    \"e\": 3,\n    \"stay\":4,\n    \"bcity\": 5,\n    \"bw\":6,\n    \"r\":7,\n    \"n\":8\n  }\n#   print(action_dict)\n\n  entity_action_tensor = np.zeros((9, 32, 32))\n  if len(worker_pos_dict) > 0:\n    for pos, id in worker_pos_dict.items():\n      if id not in action_dict:\n        entity_action_tensor[5, int(pos[0]), int(pos[1])] = 1\n      else:\n#         print(action_dict[id])\n#         print(actions[action_dict[id]])\n        entity_action_tensor[actions[action_dict[id]], pos[0], pos[1]] = 1\n    \n  if len(ct_pos_dict) > 0:\n    for pos, id in ct_pos_dict.items():\n      if id not in action_dict:\n        entity_action_tensor[6, int(pos[0]), int(pos[1])] = 1\n      else:\n        entity_action_tensor[actions[action_dict[(int(pos[0]), int(pos[1]))]], int(pos[0]), int(pos[1])] = 1\n  #print(entity_action_tensor.shape)\n  return entity_action_tensor\n\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef single_conv5(in_channels, out_channels):\n  return nn.Sequential(\n    nn.Conv2d(in_channels, out_channels, 5),\n    nn.BatchNorm2d(out_channels, eps = 1e-5, momentum=0.1),\n    nn.Tanh()\n  )\n\ndef single_conv3(in_channels, out_channels):\n  return nn.Sequential(\n    nn.Conv2d(in_channels, out_channels, 3),\n    nn.BatchNorm2d(out_channels, eps = 1e-5, momentum=0.1),\n    nn.Tanh()\n  )\n\ndef single_conv2(in_channels, out_channels):\n  return nn.Sequential(\n    nn.Conv2d(in_channels, out_channels, 2),\n    nn.BatchNorm2d(out_channels, eps = 1e-5, momentum=0.1),\n    nn.Tanh()\n  )\n\nclass Actor(nn.Module):\n    def __init__(self, Cin, out_size, seed):\n\n        super(Actor, self).__init__()\n        self.seed = torch.manual_seed(seed)\n\n        self.maxpool = nn.MaxPool2d(2)\n\n        self.layer1 = single_conv3(Cin, 16)\n        self.layer2_1 = single_conv5(16, 32)\n        self.layer2_2 = single_conv5(32, 32)\n        self.layer2_3 = single_conv3(32, 32)\n        self.layer3_1 = single_conv3(32, 32)\n        self.layer3_2 = single_conv3(32, 64)\n        self.layer4 = single_conv5(64, 128)\n\n        self.fc1 = nn.Sequential(\n        nn.Linear(128*12*12, 64),\n        nn.ReLU(inplace=True)\n        )\n        self.fc2 = nn.Linear(64, out_size)\n\n    def forward(self, x1):\n        x1 = self.layer1(x1)\n\n        x1 = self.layer2_1(x1)\n        #print(f'conv1: {x1.data.cpu().numpy().shape}')\n\n        x1 = self.layer2_2(x1)\n        #print(f'conv2: {x1.data.cpu().numpy().shape}')\n\n        x1 = self.layer2_3(x1)\n        x1 = self.layer3_1(x1)\n        #print(f'conv3: {x1.data.cpu().numpy().shape}')\n        x1 = self.layer3_2(x1)\n        x1 = self.layer4(x1)\n        #print(f'conv4: {x1.data.cpu().numpy().shape}')\n\n        x1 = x1.view(-1, 128*12*12)\n\n        x = self.fc1(x1)\n        #print(f'lconv6: {x.data.cpu().numpy().shape}')\n        out = self.fc2(x)\n\n        return out\nimport numpy as np\nimport random\nimport copy\nfrom collections import namedtuple, deque, defaultdict\nimport os\n\nfrom torch._C import device\nfrom torch.random import seed\n\nimport torch\nimport torch.nn.functional as F \nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\n\n# BUFFER_SIZE = int(2e5)\nBATCH_SIZE = 32\nLR_ACTOR = 5e-5 # learning rate of the actor\nLR_DECAY = 0.9\nDISCOUNT = 0.99\nUPDATE_TARGET_EVERY = 5\nEPISODES = 1\nepsilon = 1\nEPSILON_DECAY = 0.9995\nMIN_EPSILON = 0.001\nSHOW_EVERY = 1000\nREPEAT = 300\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ndef calc_loss(pred, target, metrics):\n    criterion = nn.MSELoss()\n\n    loss = criterion(pred, target)\n\n    pred = torch.argmax(pred, dim=1)\n\n    acc = np.sum(pred.data.cpu().numpy() == target.data.cpu().numpy()) \/ len(target.data.cpu().numpy())\n\n    metrics['loss'] += loss.data.cpu().numpy()\n    #metrics['acc'] = acc\n    return loss\n\nclass ReplayBuffer:\n    def __init__(self, buffer_size, batch_size, seed):\n        self.memory = deque(maxlen=buffer_size)\n        self.batch_size = batch_size\n        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"new_state\", \"reward\", \"done\"])\n        self.seed = random.seed(seed)\n\n    def add(self, state, action, new_state, reward, done):\n        e = self.experience(state=state, action=action, new_state=new_state, reward=reward, done=done)\n        self.memory.append(e)\n\n    def sample(self):\n        experiences = random.sample(self.memory, k=self.batch_size)\n\n        states1 = torch.from_numpy(np.stack([e.state for e in experiences if e is not None], axis=0)).float().to(device)\n        actions = torch.from_numpy(np.stack([e.action for e in experiences if e is not None], axis=0)).float().to(device)\n        states2 = torch.from_numpy(np.stack([e.new_state for e in experiences if e is not None], axis=0)).float().to(device)\n        rewards = torch.from_numpy(np.stack([e.reward for e in experiences if e is not None], axis=0)).float().to(device)\n        dones = torch.from_numpy(np.stack([e.done for e in experiences if e is not None], axis=0)).float().to(device)\n\n        return (states1, actions, states2, rewards, dones)\n\n    def __len__(self):\n        return len(self.memory)\n\n\nclass ActorAgent():\n    def __init__(self, cin, out_worker, out_ctiles, random_seed):\n        self.cin = cin\n        self.out_worker = out_worker\n        self.out_ctiles = out_ctiles\n#         self.out_ctiles = out_ctiles\n        self.seed = random.seed(random_seed)\n\n        self.worker_model = Actor(Cin=cin, out_size=out_worker, seed=random_seed).to(device)\n        self.worker_model_optimizer = optim.Adam(self.worker_model.parameters(), lr=LR_ACTOR)\n        self.worker_model_scheduler = optim.lr_scheduler.ExponentialLR(self.worker_model_optimizer, gamma=LR_DECAY)\n        \n        self.target_worker_model = Actor(Cin=cin, out_size=out_worker, seed=random_seed).to(device)\n        self.target_worker_model_optimizer = optim.Adam(self.worker_model.parameters(), lr=LR_ACTOR)\n        self.target_worker_model_scheduler = optim.lr_scheduler.ExponentialLR(self.worker_model_optimizer, gamma=LR_DECAY)\n        \n        self.target_worker_model.load_state_dict(self.worker_model.state_dict())\n        \n        self.ctiles_model = Actor(Cin=cin, out_size=out_ctiles, seed=random_seed).to(device)\n        self.ctiles_model_optimizer = optim.Adam(self.ctiles_model.parameters(), lr=LR_ACTOR)\n        self.ctiles_model_scheduler = optim.lr_scheduler.ExponentialLR(self.ctiles_model_optimizer, gamma=LR_DECAY)\n\n        self.target_ctiles_model = Actor(Cin=cin, out_size=out_ctiles, seed=random_seed).to(device)\n        self.target_ctiles_model_optimizer = optim.Adam(self.ctiles_model.parameters(), lr=LR_ACTOR)\n        self.target_ctiles_model_scheduler = optim.lr_scheduler.ExponentialLR(self.ctiles_model_optimizer, gamma=LR_DECAY) \n        \n        self.target_ctiles_model.load_state_dict(self.ctiles_model.state_dict())\n        \n        self.worker_memory = ReplayBuffer(int(1e6), BATCH_SIZE, random_seed)\n        self.ctiles_memory = ReplayBuffer(int(1e4), BATCH_SIZE, random_seed)\n        \n        self.target_update_counter = 0\n        \n    def learnworker(self, states, target):\n        states = states.to(device)\n        target = target.to(device)\n        pred = self.worker_model(states)\n\n        #     print(pred.shape)\n        #     print(target.shape)\n        metrics = defaultdict(float)\n\n        loss = calc_loss(pred, target, metrics)\n\n        self.worker_model_optimizer.zero_grad()\n        loss.backward()\n        self.worker_model_optimizer.step()\n        \n        return metrics\n\n    def learnctiles(self, states, target):\n        states = states.to(device)\n        target = target.to(device)\n\n        pred = self.ctiles_model(states)\n\n        metrics = defaultdict(float)\n        loss = calc_loss(pred, target, metrics)\n\n        self.ctiles_model_optimizer.zero_grad()\n        loss.backward()\n        self.ctiles_model_optimizer.step()\n        return metrics\n    \n    def act(self, state, is_worker = True):\n        state = torch.from_numpy(state).float().to(device)\n        if is_worker:\n            self.worker_model.eval()\n            with torch.no_grad():\n                out = self.worker_model(state)\n                out = out.cpu().data.numpy()\n            self.worker_model.train()\n        else:\n            self.ctiles_model.eval()\n            with torch.no_grad():\n                out = self.ctiles_model(state)\n                out = out.cpu().data.numpy()\n            self.ctiles_model.train()\n        return out    \n\n    def add(self, state, action, new_state, reward, done, is_worker = True):\n        if is_worker:\n            self.worker_memory.add(state, action, new_state, reward, done)\n        else:\n            self.ctiles_memory.add(state, action, new_state, reward, done)\n\n    def lr_step(self):\n        self.worker_model_scheduler.step()\n        self.ctiles_model_scheduler.step()\n\n    def step(self):\n        worker_metric_mean = defaultdict(float)\n        ctiles_metric_mean = defaultdict(float)\n        if len(self.worker_memory) > 20_000:\n            for _ in range(100):\n                experiences = self.worker_memory.sample()\n                worker_metrics = self.learnworker(experiences)\n                for key, val in worker_metrics.items():\n                    worker_metric_mean[key] += val\n            for key, val in worker_metric_mean.items():\n                worker_metric_mean[key] = val \/ 1000\n        if len(self.ctiles_memory) > 20_000:\n            for _ in range(100):\n                experiences = self.ctiles_memory.sample()\n                ctiles_metrics = self.learnctiles(experiences)\n                for key, val in ctiles_metrics.items():\n                    ctiles_metric_mean[key] += val\n\n            for key, val in ctiles_metric_mean.items():\n                ctiles_metric_mean[key] = val\/ 100\n\n        return worker_metric_mean, ctiles_metric_mean \n    \n    def train(self, terminal_state):\n        if len(self.worker_memory) < 20_000 or len(self.ctiles_memory) < 10_000:\n            return\n        worker_minibatch = self.worker_memory.sample()\n        ctiles_minibatch = self.ctiles_memory.sample()\n\n        #print(ctiles_minibatch)\n        #  transition: (states1, actions, states2, rewards, dones)\n        current_worker_states = worker_minibatch[0].to(device)\n        current_worker_qs_list = self.worker_model(current_worker_states).to(device)\n        new_current_worker_state = worker_minibatch[2].to(device)\n        future_qs_worker_list = self.target_worker_model(new_current_worker_state).to(device)\n\n        current_ctiles_states = ctiles_minibatch[0].to(device)\n        current_ctiles_qs_list = self.ctiles_model(current_ctiles_states).to(device)\n        new_current_ctiles_state = ctiles_minibatch[2].to(device)\n        future_qs_ctiles_list = self.target_ctiles_model(new_current_ctiles_state).to(device)\n\n        X_worker = torch.tensor([]).to(device)\n        y_worker = torch.tensor([]).to(device)\n\n        for index in range(len(worker_minibatch[0])):\n            state = worker_minibatch[0][index]\n            action = int(worker_minibatch[1][index])\n            new_state = worker_minibatch[2][index]\n            reward = worker_minibatch[3][index]\n            done = worker_minibatch[4][index]\n\n            if not done:\n                max_future_worker_q = torch.max(future_qs_worker_list[index])\n                new_q_worker = reward + DISCOUNT * max_future_worker_q\n            else:\n                new_q_worker = reward\n            current_qs = current_worker_qs_list[index]\n            current_qs[action] = new_q_worker\n\n            X_worker = torch.cat((X_worker, torch.unsqueeze(state, 0)))\n            y_worker = torch.cat((y_worker, torch.unsqueeze(current_qs, 0)))\n\n        X_ctiles = torch.tensor([]).to(device)\n        y_ctiles = torch.tensor([]).to(device)\n\n        for index in range(len(ctiles_minibatch)):\n            state = ctiles_minibatch[0][index]\n            action = int(ctiles_minibatch[1][index])\n            new_state = ctiles_minibatch[2][index]\n            reward = ctiles_minibatch[3][index]\n            done = ctiles_minibatch[4][index]            \n\n            if not done:\n                max_future_ctiles_q = torch.max(future_qs_ctiles_list[index])\n                new_q_ctiles = reward + DISCOUNT * max_future_ctiles_q\n            else:\n                new_q_ctiles = reward\n            current_qs = current_ctiles_qs_list[index]\n            if action > 2: \n                action = 2\n            current_qs[action] = new_q_ctiles\n\n            X_ctiles = torch.cat((X_ctiles, torch.unsqueeze(state, 0)))\n            y_ctiles = torch.cat((y_ctiles, torch.unsqueeze(current_qs, 0)))\n\n        self.learnworker(X_worker, y_worker)\n        self.learnctiles(X_ctiles, y_ctiles)\n        if terminal_state:\n            self.target_update_counter += 1\n\n        if self.target_update_counter > UPDATE_TARGET_EVERY:\n            self.target_worker_model.load_state_dict(self.worker_model.state_dict())\n            self.target_ctiles_model.load_state_dict(self.ctiles_model.state_dict())\n            self.target_update_counter = 0\n            \ndef generate_offset_map(Hmap, row_c, col_c):\n    Hmap_copy = Hmap.copy()\n    v_shift = row_c - 16\n    h_shift = col_c - 16\n    h_index = (np.arange(32) + h_shift) % 32\n    v_index = (np.arange(32) + v_shift) % 32\n    temp = Hmap_copy[:, v_index]\n    \n    return temp[:, :, h_index]\n\ndirection_dict = {\n        0: Constants.DIRECTIONS.NORTH,\n        1: Constants.DIRECTIONS.SOUTH,\n        2: Constants.DIRECTIONS.WEST,\n        3: Constants.DIRECTIONS.EAST,\n}\n\n\ndef agent(observation, configuration):\n    global game_state\n\n    ### Do not edit ###\n    if observation[\"step\"] == 0:\n        game_state = Game()\n        game_state._initialize(observation[\"updates\"])\n        game_state._update(observation[\"updates\"][2:])\n        game_state.id = observation.player\n    else:\n        game_state._update(observation[\"updates\"])\n    \n    agent = ActorAgent(cin = 11, out_worker=6, out_ctiles=3, random_seed=42)\n    agent.worker_model.load_state_dict(torch.load(\".\/\/worker_model_v29\", map_location=torch.device(\"cpu\")), strict=False, )\n    agent.ctiles_model.load_state_dict(torch.load(\".\/ctiles_model_v\", map_location=torch.device(\"cpu\")), strict=False)\n    ### AI Code goes down here! ### \n    \n    player = game_state.players[observation.player]\n    opponent = game_state.players[(observation.player + 1) % 2]\n    \n    obs = Obs(observation)\n    \n    actions = []\n    current_state = obs.state\n    unit_actions = {}\n    ctiles_ation = {}\n    #print(\"HERE 2\")\n    for unit in player.units:\n        if unit.can_act():\n            if unit.is_worker():\n                offset_state = generate_offset_map(current_state, unit.pos.x, unit.pos.y)\n                offset_state_expand = np.expand_dims(offset_state, axis=0)\n                if np.random.random() > epsilon:\n\n                    player_action = np.argmax( agent.act(offset_state_expand, is_worker=True) )\n\n                    unit_actions[(unit.pos.x, unit.pos.y)] = [offset_state, player_action]\n                else:\n                    player_action = np.random.randint(0, 6)\n                    unit_actions[(unit.pos.x, unit.pos.y)] = [offset_state, player_action]\n                #print(\"WORKER ACTION:\",player_action, \"STEP:\", obs.step)\n                action = None\n                if player_action < 4:\n                    #try:\n                        action = unit.move(direction_dict[player_action])\n              \n                    #except:\n                    #    pass\n                elif player_action == 5:\n                    #try:\n                        action = unit.build_city()\n                    #except:\n                    #    pass\n                if action is not None:\n                    actions.append(action)\n    #print(\"HERE 3\")\n    cities = list(player.cities.values())\n    if len(cities) > 0:\n        for city in cities:\n            for city_tile in city.citytiles[::-1]:\n                if city_tile.can_act():\n                    offset_state = generate_offset_map(current_state, city_tile.pos.x, city_tile.pos.y)\n                    offset_state_expand = np.expand_dims(offset_state, axis=0)\n                    if np.random.random() > epsilon:\n                        ctile_action = np.argmax( agent.act( offset_state_expand, is_worker=False ))\n\n                        ctiles_ation[(city_tile.pos.x, city_tile.pos.y)] = [offset_state, ctile_action]\n                    else:\n                        ctile_action = np.random.randint(0, 3)\n                        ctiles_ation[(city_tile.pos.x, city_tile.pos.y)] = [offset_state, ctile_action]\n                    \n                    #print(\"CTILE ACTION:\", ctile_action, \"STEP: \", obs.step)\n                    action = None\n                    if ctile_action == 0:\n                        #try:\n                            action = city_tile.build_worker()\n                        #except:\n                        #    pass\n                    elif ctile_action == 1:\n                        #try:\n                            action = city_tile.research()\n                        #except:\n                        #    pass\n                    if action is not None:\n                        actions.append(action)\n    #print(\"STEP: \",obs.step,\"ACTION:\", actions)\n    return actions","b45ecabd":"from kaggle_environments import make\nenv = make(\"lux_ai_2021\", debug=True)\nsteps = env.run([\"agent.py\", \"rl_agent.py\"])","7b359663":"env.render(mode=\"ipython\", width=900, height=600)","e72301fb":"obs = Obs( observation )\ncurrent_state = obs.state","1814189e":"agent.act(np.expand_dims(current_state, axis=0), is_worker=True) ","91005527":"!tar -czf submission.tar.gz *","fc02e995":"# Further improvements:\n- Adding a model for cart units\n- Adding more features to the model: such as day night cycle, closest resources...\n- Training for longer time. In this note book, i only train the model on a single map with 300 episodes","b4c03500":"## Create a submission\nNow we need to create a .tar.gz file with main.py (and agent.py) at the top level. We can then upload this!","a98b8650":"# Description\n\nI am trying to create a Reinforcement Learning model. The general idea is to train a model to beat the [baseline model](http:\/\/https:\/\/www.kaggle.com\/ilialar\/lux-ai-risk-averse-baseline) by \nIlia Larchenko.\n\nTo keep it simple, I use 2 separate model for worker and city tiles, using Deep Q Network (DQN) over the generic Q Learning approach.\n\nI hope this could be helpful :)","108ad9eb":"# RL Model","bbe588e8":"# Baseline Model"}}