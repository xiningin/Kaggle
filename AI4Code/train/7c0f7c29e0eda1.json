{"cell_type":{"191d4d79":"code","822cfdac":"code","9ce4c2d1":"code","c581c1ce":"code","75aa433d":"code","297bd769":"code","a1cf8e8e":"code","608f151c":"code","a0b437fa":"code","a3d3dc05":"code","5a1d8a5a":"code","731f0dd6":"code","bdefa584":"code","d049571d":"code","a07526db":"code","4815cb87":"code","53491cb6":"code","e8945248":"code","7b869d30":"markdown","6795827f":"markdown"},"source":{"191d4d79":"import pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier)\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_validate, KFold, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder","822cfdac":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\ntarget = train['Survived']\n\ndataset = train.append(test, sort=False)\n\nPassengerId = test['PassengerId']\n\nprint('train', train.shape)\nprint('test', test.shape)\n\ntrain.head()","9ce4c2d1":"print(train.info())\nprint(test.info())","c581c1ce":"label = LabelEncoder()\n\n# fill nan values\ndataset['Fare'].fillna(dataset['Fare'].median(), inplace=True)\ndataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace=True)\ndataset['Age'].fillna(dataset['Age'].median(), inplace=True)\n\n# place Fare and Age into buckets\ndataset['Fare'] = pd.qcut(dataset['Fare'], 4)\ndataset['Age'] = pd.cut(dataset['Age'].astype('int'), 5)\n\n# create new features from existing\ndataset['Title'] = dataset['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\ntitle_names = (dataset['Title'].value_counts() < 10)\ndataset['Title'] = dataset['Title'].apply(lambda x: 'Other' if title_names.loc[x] == True else x)\ndataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\ndataset['Alone'] = 0\ndataset['Alone'].loc[dataset['FamilySize'] > 1] = 1\ndataset['HasCabin'] = dataset['Cabin'].apply(lambda x: 0 if type(x) == float else 1)\ndataset['Name_length'] = dataset['Name'].apply(len)\n\n# transform non-numerical features \ndataset['Sex'] = label.fit_transform(dataset['Sex'])\ndataset['Title'] = label.fit_transform(dataset['Title'])\ndataset['Embarked'] = label.fit_transform(dataset['Embarked'])\ndataset['Fare'] = label.fit_transform(dataset['Fare'])\ndataset['Age'] = label.fit_transform(dataset['Age'])","75aa433d":"dataset['LastName'] = dataset['Name'].apply(lambda x: str.split(x, \",\")[0])\nDEFAULT_SURVIVAL_VALUE = 0.5\ndataset['FamilySurvival'] = DEFAULT_SURVIVAL_VALUE\nfor grp, grp_df in dataset.groupby(['LastName', 'Fare']):\n    if(len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                dataset.loc[dataset['PassengerId'] == passID, 'FamilySurvival'] = 1\n            elif (smin==0.0):\n                dataset.loc[dataset['PassengerId'] == passID, 'FamilySurvival'] = 0\nfor _, grp_df in dataset.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['FamilySurvival'] == 0) | (row['FamilySurvival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    dataset.loc[dataset['PassengerId'] == passID, 'FamilySurvival'] = 1\n                elif (smin==0.0):\n                    dataset.loc[dataset['PassengerId'] == passID, 'FamilySurvival'] = 0","297bd769":"# drop unnecessary features and scale features to have mean=0 and std=1\ndataset.drop(['Name', 'LastName', 'PassengerId', 'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1, inplace=True)\n\nss = StandardScaler()\nss.fit_transform(dataset)\n\ntrain = dataset[:891]\ntest = dataset[891:]","a1cf8e8e":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","608f151c":"print(train[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean())\nprint(train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean())\nprint(train[['Alone', 'Survived']].groupby(['Alone'], as_index=False).mean())\nprint(train[['HasCabin', 'Survived']].groupby(['HasCabin'], as_index=False).mean())\nprint(train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())\nprint(train[['Fare', 'Survived']].groupby(['Fare'], as_index=False).mean())\nprint(train[['Age', 'Survived']].groupby(['Age'], as_index=False).mean())\nprint(train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())","a0b437fa":"train_y = train['Survived']\n\ntrain.drop(['Survived'], axis=1, inplace=True)\ntest.drop(['Survived'], axis=1, inplace=True)\n\ntrain_X = train","a3d3dc05":"rf_parameters = {\n    'n_estimators': [300, 500],\n    'warm_start': [True],\n    'max_depth'   : [n for n in range(6, 12)],\n    'max_features': [n for n in range(5, 8)],\n    \"min_samples_split\": [n for n in range(8, 11)],\n    \"bootstrap\": [True, False]\n}\n\net_parameters = {\n    'n_estimators': [300, 500],\n    'max_depth'   : [n for n in range(2, 10)],\n    'max_features': [n for n in range(5, 8)],\n    \"min_samples_leaf\": [n for n in range(1, 6)],\n    \"bootstrap\": [True, False]\n}\n\nada_parameters = {\n    'n_estimators': [300, 500],\n    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.35, 0.5, 0.75, 1]\n}\n\ngb_parameters = {\n    'loss' : [\"deviance\",\"exponential\"],\n    'n_estimators' : [300, 500],\n    'learning_rate': [0.0025, 0.005, 0.0075, 0.01, 0.05, 0.1],\n    'max_depth':  [n for n in range(2, 6)],\n    'max_features': [n for n in range(2, 6)],\n    'min_samples_leaf': [n for n in range(1, 7)],\n}\n\nsvc_parameters = {\n    'kernel': ['rbf', 'linear'],\n    'C': [0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1],\n    'gamma': [0.001 ,0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1, 2.5, 5, 10],\n    'probability': [True]\n}","5a1d8a5a":"rfc_cv = GridSearchCV(RandomForestClassifier(), rf_parameters, cv=4, n_jobs=8).fit(train_X, train_y)\netc_cv = GridSearchCV(ExtraTreesClassifier(), et_parameters, cv=4, n_jobs=4).fit(train_X, train_y)\nada_cv = GridSearchCV(AdaBoostClassifier(), ada_parameters, cv=4, n_jobs=4).fit(train_X, train_y)\ngbc_cv = GridSearchCV(GradientBoostingClassifier(), gb_parameters, cv=4, n_jobs=4).fit(train_X, train_y)\nsvc_cv = GridSearchCV(SVC(), svc_parameters, cv=4, n_jobs=4).fit(train_X, train_y)","731f0dd6":"cols = train.columns.values\n\nfeature_dataframe = pd.DataFrame(\n    {\n        'Random Forest feature importances': rfc_cv.best_estimator_.feature_importances_,\n        'Extra Trees feature importances': etc_cv.best_estimator_.feature_importances_,\n        'AdaBoost feature importances': ada_cv.best_estimator_.feature_importances_,\n        'Gradient Boost feature importances': gbc_cv.best_estimator_.feature_importances_\n    }\n)","bdefa584":"trace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = cols,\n    mode='markers',\n    marker=dict(\n        sizemode='diameter',\n        sizeref=1,\n        size=25,\n        color=feature_dataframe['Random Forest feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = cols\n)\ndata = [trace]\n\nlayout = go.Layout(\n    autosize= True,\n    title='Random Forest Feature Importance',\n    hovermode='closest',\n    yaxis=dict(\n        title='Feature Importance',\n        ticklen=5,\n        gridwidth=2\n    ),\n    showlegend=False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n\ntrace = go.Scatter(\n    y=feature_dataframe['Extra Trees feature importances'].values,\n    x=cols,\n    mode='markers',\n    marker=dict(\n        sizemode='diameter',\n        sizeref=1,\n        size=25,\n        color=feature_dataframe['Extra Trees feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = cols\n)\ndata = [trace]\n\nlayout = go.Layout(\n    autosize=True,\n    title='Extra Trees Feature Importance',\n    hovermode='closest',\n    yaxis=dict(\n        title='Feature Importance',\n        ticklen=5,\n        gridwidth=2\n    ),\n    showlegend=False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n\ntrace = go.Scatter(\n    y=feature_dataframe['AdaBoost feature importances'].values,\n    x=cols,\n    mode='markers',\n    marker=dict(\n        sizemode='diameter',\n        sizeref=1,\n        size=25,\n        color=feature_dataframe['AdaBoost feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = cols\n)\ndata = [trace]\n\nlayout = go.Layout(\n    autosize=True,\n    title='AdaBoost Feature Importance',\n    hovermode='closest',\n    yaxis=dict(\n        title='Feature Importance',\n        ticklen=5,\n        gridwidth=2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='scatter2010')\n\ntrace = go.Scatter(\n    y = feature_dataframe['Gradient Boost feature importances'].values,\n    x = cols,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n        color = feature_dataframe['Gradient Boost feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = cols\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title='Gradient Boosting Feature Importance',\n    hovermode='closest',\n    yaxis=dict(\n        title='Feature Importance',\n        ticklen=5,\n        gridwidth=2\n    ),\n    showlegend=False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","d049571d":"feature_dataframe['mean'] = feature_dataframe.mean(axis=1)\nfeature_dataframe.head(3)","a07526db":"y = feature_dataframe['mean'].values\nx = cols\ndata = [go.Bar(\n            x=x,\n            y=y,\n            width=0.5,\n            marker=dict(\n                color = feature_dataframe['mean'].values,\n                colorscale='Portland',\n                showscale=True,\n                reversescale = False\n            ),\n            opacity=0.6\n)]\nlayout = go.Layout(\n    autosize=True,\n    title='Barplots of Mean Feature Importance',\n    hovermode='closest',\n    yaxis=dict(\n        title='Feature Importance',\n        ticklen=5,\n        gridwidth=2\n    ),\n    showlegend=False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='bar-direct-labels')","4815cb87":"vc = VotingClassifier(estimators=[\n    ('rfc', rfc_cv.best_estimator_), \n    ('etc', etc_cv.best_estimator_), \n    ('ada', ada_cv.best_estimator_),\n    ('gbc', gbc_cv.best_estimator_), \n    ('svm', svc_cv.best_estimator_)\n], voting='soft', n_jobs=4)\n\nvc = vc.fit(train_X, train_y)","53491cb6":"predictions = vc.predict(test)\n\nsubmission = pd.DataFrame({\n        \"PassengerId\": PassengerId,\n        \"Survived\": predictions\n    }, dtype='int64')\n\nsubmission.head(15)","e8945248":"submission.to_csv('titanic.csv', index=False)","7b869d30":"Good idea to extract FamilySurvival based on the Last Name from [this kernel](https:\/\/www.kaggle.com\/himaoka\/ensemble-rfc-etc-gbc-svm)","6795827f":"**Feature Engineering**"}}