{"cell_type":{"6b2192cb":"code","e575bf5c":"code","527a43e3":"code","dc6d030d":"code","8e8da4eb":"code","97eed378":"code","cebbc451":"code","daa44031":"code","957578ca":"code","93f80cd6":"code","184d6d62":"code","76a37cae":"code","e25f8dff":"code","a5734b6b":"code","1f5519b2":"code","98dc3ecb":"code","c6beb71e":"code","79bd4fdf":"code","399d62dd":"code","cf069ce3":"code","b410630c":"code","ca8aae13":"code","d06363de":"code","bb08d973":"code","acaa26d2":"markdown","f31658b1":"markdown","268b5efa":"markdown","2870b734":"markdown","2f504d22":"markdown","72bc80f6":"markdown","f324a821":"markdown","edba51f5":"markdown","c2762484":"markdown"},"source":{"6b2192cb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e575bf5c":"import pandas as pd\nimport numpy as np\nimport string\nimport os\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nimport gensim\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\n\nstring.punctuation\nstopword = nltk.corpus.stopwords.words('english')\n\nlemmatizer = WordNetLemmatizer()\nps = nltk.PorterStemmer()\ntokenizer = RegexpTokenizer(r'\\w+')","527a43e3":"maxlen = 80\nbatch_size = 32","dc6d030d":"data = pd.read_csv(\"\/kaggle\/input\/smsspamcollectiontsv\/SMSSpamCollection.tsv\", sep='\\t')\ndata.columns = ['label', 'body_text']\ndata.head()","8e8da4eb":"data['label'] = (data['label']=='spam').astype(int)\ndata.head()","97eed378":"data.shape","cebbc451":"def clean_text(text):\n    \n    ''' Text preprocessing '''\n\n    tokens = tokenizer.tokenize(text.lower())\n    \n    table = str.maketrans('', '', string.punctuation)\n    stripped = [w.translate(table) for w in tokens]\n    tokens = [word for word in stripped if word.isalpha()]\n\n    text = [lemmatizer.lemmatize(word) for word in tokens if word not in stopword]\n    return text","daa44031":"data['body_text'] = data['body_text'].apply(lambda x: clean_text(x))\nbody_text_data = data['body_text'].values.tolist()","957578ca":"print(len(body_text_data))\nbody_text_data[0]","93f80cd6":"# word embedding using word2vec\nmodel = gensim.models.Word2Vec(body_text_data, size=100, window=5, min_count=3)\nlen(model.wv.vocab)","184d6d62":"# similarity\nmodel.most_similar('customer')","76a37cae":"# save model\nmodel.wv.save_word2vec_format(\"spam_word2vec_model.txt\", binary=False)","e25f8dff":"# Load embeddings\n\nembeddings_index = {}\nfile = open(os.path.join('', 'spam_word2vec_model.txt'), encoding = \"utf-8\")\n\nfor record in file:\n    values = record.split()\n    word = values[0]\n    coefficient = np.asarray(values[1:])\n    embeddings_index[word] = coefficient\nfile.close()","a5734b6b":"len(embeddings_index)","1f5519b2":"embeddings_index['free']","98dc3ecb":"tokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(body_text_data)\nsequences = tokenizer_obj.texts_to_sequences(body_text_data)\n\nword_index = tokenizer_obj.word_index\nprint(\"Word index\", len(word_index))\n\nX = pad_sequences(sequences, maxlen=maxlen)\nprint(\"X shape:\", X.shape)\n\ny = data['label'].values\nprint(\"y shape:\", y.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=15, stratify=y)","c6beb71e":"word_index['free']","79bd4fdf":"X[0], y[0]","399d62dd":"# Create embedding matrix for words\n\nEMBEDDING_DIM = 100\n\nmax_features = len(word_index) + 1\nembedding_matrix = np.zeros((max_features, EMBEDDING_DIM))\n\nfor word, i in word_index.items():\n    if i > max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","cf069ce3":"# embeddings for word - 'free'\nembedding_matrix[9]","b410630c":"# Base model\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.initializers import Constant\n\nprint(\"Build model...\")\n\nmodel = Sequential()\n\nembedding_layer = Embedding(max_features, EMBEDDING_DIM, \n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=maxlen,\n                            trainable=False)\n\nmodel.add(embedding_layer)\nmodel.add(LSTM(units=64, dropout=0.2, recurrent_dropout=0.2, activation='relu'))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()","ca8aae13":"print(\"Train...\")\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=30, validation_data=(X_test, y_test), verbose=2)","d06363de":"score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\nprint('Test score:', score)\nprint('Test accuracy:', acc)","bb08d973":"import matplotlib.pyplot as plt\n\nplt.plot(model.history.history['loss'][5:])\nplt.plot(model.history.history['val_loss'][5:])\nplt.title('Loss over epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='best')\nplt.show()","acaa26d2":"***3 - Creating Neural Network***\n\n* Using word embeddings from word2vec in first layer\n* Building LSTM network with 64 units\n* Adding Dense layers","f31658b1":"***Creating embedding matrix***","268b5efa":"<img src=\"https:\/\/inttix.ai\/wp-content\/uploads\/2019\/10\/capture-1.jpg\" width=600 height=20 \/>\n","2870b734":"<font color='blue'><b><i>In this notbook, I am using 'sms spam collection' dataset which classifies the messages as spam\/ham. I have created a base model using the following techniques.<\/i><\/b><\/font>\n\n\n* NLP text preprocessing using NLTK\n* Created static word embeddings using word2vec from Gensim\n* Created LSTM network\n","2f504d22":"***Step 2 - Creating word embeddings on cleaned text data using word2vec and saving the model***\n","72bc80f6":"***Padding the data with 0s to have similar length.***","f324a821":"<font color='blue'><b><i>Here, I have built the base model which can be further improved. Please upvote if you found it helpful... :)<\/i><\/b><\/font>","edba51f5":"***Loading the word embeddings***","c2762484":"***Step 1 - Cleaning the text data.***\n\nRemoving punctuations, stopwords. Tokenizing the sentences and lemmatizing the words to their original form."}}