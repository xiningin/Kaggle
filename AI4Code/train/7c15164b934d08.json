{"cell_type":{"8a3dac41":"code","6ff9f4aa":"code","126cc037":"code","f464d083":"code","77cb7715":"code","b24863df":"code","9651d6e4":"code","133e0ab5":"code","4c293d1f":"code","a861a448":"code","0dd6f04f":"code","db4b2517":"code","e624361a":"code","72fc6967":"code","fdd4411a":"code","580ff02b":"code","5b70d9d6":"code","9cabb73c":"code","7c355fb5":"code","24c466e2":"code","866a75de":"code","2b0a92d1":"code","b97ce67c":"code","7310d910":"code","f57f3db4":"code","aad0b256":"code","66821b84":"code","5bb295fb":"code","03f151a3":"code","b23a8f06":"code","7156b6d8":"code","b03d2917":"code","51a329bd":"code","e32d3b9a":"code","1f84a791":"code","f92920a3":"code","c28eefa4":"code","a3c07003":"code","14e7cb99":"code","10fb6361":"code","1ff39d48":"code","2d63a01e":"code","e065f07c":"code","04b363ff":"code","f2f03d20":"code","ef80e332":"code","d7cfe202":"code","58aade2f":"code","36f529b4":"code","c10e201e":"code","d29cb168":"code","761bc7b3":"markdown","50c12fa5":"markdown","63ea5eac":"markdown","ab97da7f":"markdown","942decc3":"markdown","ec80c0bb":"markdown","92a14134":"markdown","75939a22":"markdown","0ac8570f":"markdown","4ac91c51":"markdown","f0a8add3":"markdown","629623ff":"markdown","76c32aa0":"markdown","61c77832":"markdown"},"source":{"8a3dac41":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","6ff9f4aa":"import math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nfrom PIL import Image\nimport random\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom tqdm.notebook import tqdm\n\nimport torch\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms, datasets\nfrom torch import nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torchvision.models as models\nfrom torchvision.utils import make_grid","126cc037":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","f464d083":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","77cb7715":"SEED = 17\nseed_everything(SEED)","b24863df":"data_dir = '..\/input\/cassava-leaf-disease-classification'\ntrain_dir = data_dir + '\/train_images'\ntrain_csv = data_dir + '\/train.csv'\ntest_dir = data_dir + '\/test_images'\nname_json = data_dir + '\/label_num_to_disease_map.json'\nsample_csv = data_dir + '\/sample_submission.csv'","9651d6e4":"train_df = pd.read_csv(train_csv)\ntrain_df.head()","133e0ab5":"train_df.label.value_counts()","4c293d1f":"sub_df = pd.read_csv(sample_csv)\nsub_df.head()","a861a448":"class CassavaDS(Dataset):\n    def __init__(self, df, data_dir, transforms=None):\n        super().__init__()\n        self.df_data = df.values\n        self.transforms = transforms\n        self.data_dir = data_dir\n\n    def __len__(self):\n        return len(self.df_data)\n\n    def __getitem__(self, index):\n        img_name, label = self.df_data[index]\n        img_path = os.path.join(self.data_dir, img_name)\n        img = Image.open(img_path).convert(\"RGB\")\n        if self.transforms is not None:\n            image = self.transforms(img)\n        return image, label","0dd6f04f":"X_train, X_valid = train_test_split(train_df, test_size=0.1, \n                                                    random_state=SEED,\n                                                    stratify=train_df.label.values)","db4b2517":"X_train.shape, X_valid.shape","e624361a":"normalize = transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))","72fc6967":"train_tf = transforms.Compose([\n    transforms.Pad(4, padding_mode='reflect'),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.RandomResizedCrop(224),\n    transforms.ToTensor(),\n    normalize\n])\n\nvalid_tf = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    normalize\n])","fdd4411a":"train_ds = CassavaDS(X_train, train_dir, train_tf)\nvalid_ds = CassavaDS(X_valid, train_dir, valid_tf)","580ff02b":"bs = 64","5b70d9d6":"train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\nvalid_loader = DataLoader(valid_ds, batch_size=bs, shuffle=True)","9cabb73c":"import json\n\nwith open(name_json, 'r') as f:\n    cat_to_name = json.load(f)","7c355fb5":"cat_to_name","24c466e2":"class UnNormalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        for t, m, s in zip(tensor, self.mean, self.std):\n            t.mul_(s).add_(m)\n        return tensor","866a75de":"unnorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))","2b0a92d1":"def display_img(img, label=None, unnorm_obj=None, invert=True, return_label=True):\n    if unnorm_obj != None:\n        img = unnorm_obj(img)\n\n    plt.imshow(img.permute(1, 2, 0))\n    \n    if label != None:\n        plt.title(cat_to_name[str(label)])","b97ce67c":"def display_batch(batch, unnorm_obj=None):    \n    imgs, labels = batch\n    \n    if unnorm_obj:\n        unnorm_imgs = []\n        for img in imgs:\n            unnorm_imgs.append(unnorm_obj(img))\n        imgs = unnorm_imgs\n    \n    ig, ax = plt.subplots(figsize=(16, 8))\n    ax.set_xticks([]); ax.set_yticks([])\n    ax.imshow(make_grid(imgs, nrow=16).permute(1, 2, 0))","7310d910":"img, label = train_ds[0]\ndisplay_img(img, label)","f57f3db4":"display_batch(next(iter(train_loader)))","aad0b256":"class AvgStats(object):\n    def __init__(self):\n        self.reset()\n        \n    def reset(self):\n        self.losses =[]\n        self.precs =[]\n        self.its = []\n        \n    def append(self, loss, prec, it):\n        self.losses.append(loss)\n        self.precs.append(prec)\n        self.its.append(it)","66821b84":"def save_checkpoint(model, is_best, filename='.\/checkpoint.pth'):\n    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n    if is_best:\n        torch.save(model.state_dict(), filename)  # save checkpoint\n    else:\n        print (\"=> Validation Accuracy did not improve\")","5bb295fb":"def load_checkpoint(model, filename = '.\/checkpoint.pth'):\n    sd = torch.load(filename, map_location=lambda storage, loc: storage)\n    names = set(model.state_dict().keys())\n    for n in list(sd.keys()): \n        if n not in names and n+'_raw' in names:\n            if n+'_raw' not in sd: sd[n+'_raw'] = sd[n]\n            del sd[n]\n    model.load_state_dict(sd)","03f151a3":"def train(loader, model, optimizer, device):\n    model.train()\n    correct, trn_loss, trn_time = 0., 0., 0\n    t = tqdm(loader, leave=False, total=len(loader))\n    bt_start = time.time()\n    for i, (ip, target) in enumerate(t):\n        ip, target = ip.to(device), target.to(device)                          \n        output = model(ip)\n        loss = criterion(output, target)\n        trn_loss += loss.item()\n        \n        # measure accuracy and record loss\n        _, pred = output.max(dim=1)\n        correct += torch.sum(pred == target.data)\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    trn_time = time.time() - bt_start\n    trn_acc = correct * 100 \/ len(loader.dataset)\n    trn_loss \/= len(loader)\n    return trn_acc, trn_loss, trn_time","b23a8f06":"def valid(loader, model, optimizer, device):\n    model.eval()\n    with torch.no_grad():\n        correct, val_loss, val_time = 0., 0., 0\n        t = tqdm(loader, leave=False, total=len(loader))\n        bt_start = time.time()\n        for i, (ip, target) in enumerate(t):\n            ip, target = ip.to(device), target.to(device)                          \n            output = model(ip)\n            loss = criterion(output, target)\n            val_loss += loss.item()\n\n            # measure accuracy and record loss\n            _, pred = output.max(dim=1)\n            correct += torch.sum(pred == target.data)\n\n        val_time = time.time() - bt_start\n        val_acc = correct * 100 \/ len(loader.dataset)\n        val_loss \/= len(loader)\n        return val_acc, val_loss, val_time","7156b6d8":"def fit(model, sched, optimizer, device, epoch):\n    print(\"Epoch\\tTrn_loss\\tVal_loss\\tTrn_acc\\t\\tVal_acc\")\n    best_acc = 0.\n    for j in range(epoch):\n        trn_acc, trn_loss, trn_time = train(train_loader, model, optimizer, device)\n        trn_stat.append(trn_loss, trn_acc, trn_time)\n        val_acc, val_loss, val_time = valid(valid_loader, model, optimizer, device)\n        val_stat.append(val_acc, val_loss, val_time)\n        if sched:\n            sched.step()\n        if val_acc > best_acc:\n            best_acc = val_acc\n            save_checkpoint(model, True, '.\/best_model.pth')\n        print(\"{}\\t{:06.8f}\\t{:06.8f}\\t{:06.8f}\\t{:06.8f}\"\n              .format(j+1, trn_loss, val_loss, trn_acc, val_acc))","b03d2917":"!cp ..\/input\/visiontransformerpretrainedimagenet1kweights\/vit.py .\n!cp ..\/input\/visiontransformerpretrainedimagenet1kweights\/vit_16_224_imagenet1000.pth .","51a329bd":"from vit import ViT","e32d3b9a":"def get_model(out_features=5):\n    model = ViT(224, 16, drop_rate=0.1)\n    load_checkpoint(model, '.\/vit_16_224_imagenet1000.pth')\n    model.out = nn.Linear(in_features=model.out.in_features, out_features=5)\n    for param in model.parameters():\n        param.require_grad = True\n    return model","1f84a791":"model = get_model()","f92920a3":"model = model.to(device)","c28eefa4":"trn_stat = AvgStats()\nval_stat = AvgStats()","a3c07003":"criterion = nn.CrossEntropyLoss()","14e7cb99":"optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)","10fb6361":"epochs = 20","1ff39d48":"sched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, 1e-3)","2d63a01e":"fit(model, sched, optimizer, device, epochs)","e065f07c":"test_tf = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    normalize\n])","04b363ff":"load_checkpoint(model, '.\/best_model.pth')","f2f03d20":"def predict(test_dir, model, device):\n    img_names = []\n    preds = []\n    for name in os.listdir(test_dir):\n        img_path = os.path.join(test_dir, name)\n        img = Image.open(img_path).convert(\"RGB\")\n        img = test_tf(img)\n        img = img.unsqueeze(0)\n        img = img.to(device)\n        op = model(img)\n        _, pred = op.max(dim=1)\n        img_names.append(name)\n        preds.append(pred.item())\n    return img_names, preds","ef80e332":"img_names, preds = predict(test_dir, model, device)","d7cfe202":"img_names, preds","58aade2f":"sub_df.head()","36f529b4":"sub_df['image_id'] = img_names\nsub_df['label'] = preds","c10e201e":"sub_df.head()","d29cb168":"sub_df.to_csv('submission.csv', index=False)","761bc7b3":"# Train and Test","50c12fa5":"pytorch tpu kernel available @ https:\/\/www.kaggle.com\/nachiket273\/pytorch-tpu-vision-transformer","63ea5eac":"# Vision Transformer","ab97da7f":"# Predict","942decc3":"# Data Folder","ec80c0bb":"# Dataset","92a14134":"Using weights from pretrained-model on Imagenet-1k <br>\nFile is available at https:\/\/www.kaggle.com\/nachiket273\/visiontransformerpretrainedimagenet1kweights","75939a22":"# Helper Functions","0ac8570f":"# Labels","4ac91c51":"Using implementation from https:\/\/github.com\/nachiket273\/Vision_transformer_pytorch.git","f0a8add3":"# Read CSV","629623ff":"# Seed","76c32aa0":"the dataset seems heavily unbalanced towards label 3.","61c77832":"# Plot Images"}}