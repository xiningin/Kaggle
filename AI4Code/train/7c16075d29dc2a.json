{"cell_type":{"6da98af9":"code","68d9f76d":"code","7bfe682b":"code","e3171364":"code","de136ba1":"code","dae7f7f2":"code","5b0e37ac":"code","c1796fa1":"code","2001fdc4":"code","af2f7725":"code","f5142f17":"code","48e4dc30":"code","17266052":"code","3fbdd998":"code","b854c514":"code","d5591a24":"markdown","7eb546ca":"markdown","d6771e01":"markdown","6cea15da":"markdown","a3a5552a":"markdown","601451d0":"markdown","22ad09f2":"markdown","31782ba2":"markdown","7f86c9fb":"markdown"},"source":{"6da98af9":"!pip install segmentation_models -q","68d9f76d":"import numpy as np\nimport pandas as pd\nfrom pylab import *\nimport os\nos.environ['SM_FRAMEWORK'] = 'tf.keras'\nimport sys\nimport segmentation_models as sm\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras.models import model_from_json\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TensorBoard\nimport tensorflow_addons as tfa\nfrom functools import partial\nimport json\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\n%matplotlib inline  ","7bfe682b":"DPATH = '..\/input\/fork-of-data-hubmap-image-2-tfrecords-256-512-10'\n# get parameters from the data creation notebook\n#with open(DPATH+'\/dparams.json') as json_file:\n#    dparams = json.load(json_file)\n#dparams","e3171364":"FILENAMES = tf.io.gfile.glob(DPATH+\"\/*-256.tfrecord\")\nK_SPLITS = 5 # number of folds","de136ba1":"# augmentation\ndef data_augment(image, mask):\n    p_rotation = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_cutout = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_shear = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    \n    # grayscale\n    if tf.random.uniform(()) > 0.9: \n        image = tf.image.rgb_to_grayscale(image)\n        image = tf.image.grayscale_to_rgb(image)\n\n    # x*90deg rotation\n    if p_rotate > .5:\n        image, mask = data_augment_rotate(image, mask)\n    \n    # flip\n    image, mask = data_augment_spatial(image, mask)\n    \n    # brightness\/contrast\n    if tf.random.uniform(()) > 0.5:\n        if tf.random.uniform(()) > 0.5:\n            image = tf.image.random_contrast(image, 0.8, 1.2)\n        else:\n            image = tf.image.random_brightness(image, 0.1)\n    \n    # hue\/saturation\n    if tf.random.uniform(()) > 0.5:\n        if tf.random.uniform(()) > 0.5:    \n            image = tf.image.random_saturation(image, 0.7, 1.3)\n        else:      \n            a = tf.random.uniform((), -0.1, 0.1)\n            image = tf.image.adjust_hue(image, a)\n            \n    # noise \n    if tf.random.uniform(()) > 0.5: \n        if tf.random.uniform(()) > 0.5:\n            gnoise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=0.1, dtype=tf.float32)\n            image = tf.add(image, gnoise)\n        else:\n            image = tf.image.random_jpeg_quality(image, 40, 80, seed=None)\n    \n    # black pixels\n    if tf.random.uniform(()) > 0.75: \n        r = tf.random.uniform((IMG_SIZE, IMG_SIZE,3), minval=0, maxval=1, dtype=tf.dtypes.float32)      \n        r = (r > tf.random.uniform([], 0., .2, dtype=tf.float32))\n        image = tf.math.multiply(image, tf.cast(r, dtype=tf.float32))\n        \n    # cutout    \n    if tf.random.uniform(()) > 0.75:\n        image = data_augment_cutout(image, 10, 10, 48)\n            \n    image = tf.clip_by_value(image, 0.0, 1.0)\n\n    \n    return image, mask\n\ndef one_cut(image, min_size, max_size):\n    image = tf.squeeze(tfa.image.random_cutout(tf.raw_ops.Pack(values=[image]),\n                                               (tf.random.uniform((), minval=min_size, maxval=max_size, dtype=tf.dtypes.int32),\n                                                tf.random.uniform((), minval=min_size, maxval=max_size, dtype=tf.dtypes.int32)),\n                                               #tf.random.uniform((), minval=min_size, maxval=max_size, dtype=tf.dtypes.int32),\n                                               constant_values=tf.random.uniform(())))\n    return image\n\ndef data_augment_cutout(image, max_cuts, min_size, max_size):\n    cuts = tf.random.uniform((), minval=1, maxval=max_cuts, dtype=tf.dtypes.int32)\n    image = one_cut(image, min_size, max_size)\n    if cuts > 1:\n        image = one_cut(image, min_size, max_size)\n    if cuts > 2:\n        image = one_cut(image, min_size, max_size)\n    if cuts > 3:\n        image = one_cut(image, min_size, max_size)\n    if cuts > 4:\n        image = one_cut(image, min_size, max_size)\n    if cuts > 5:\n        image = one_cut(image, min_size, max_size)\n    if cuts > 6:\n        image = one_cut(image, min_size, max_size)\n    if cuts > 7:\n        image = one_cut(image, min_size, max_size)\n    if cuts > 8:\n        image = one_cut(image, min_size, max_size)\n    if cuts > 9:\n        image = one_cut(image, min_size, max_size)\n    return image\n\ndef data_augment_spatial(image, mask):\n    if tf.random.uniform(()) > 0.5:\n        image = tf.image.flip_left_right(image)\n        mask = tf.image.flip_left_right(mask)\n    if tf.random.uniform(()) > 0.5:\n        image = tf.image.flip_up_down(image)\n        mask = tf.image.flip_up_down(mask)\n\n    return image, mask\n\ndef data_augment_rotate(image, mask):\n    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    \n    if p_rotate > .66:\n        image = tf.image.rot90(image, k=3) # rotate 270\u00ba\n        mask = tf.image.rot90(mask, k=3) # rotate 270\u00ba\n    elif p_rotate > .33:\n        image = tf.image.rot90(image, k=2) # rotate 180\u00ba\n        mask = tf.image.rot90(mask, k=2) # rotate 180\u00ba\n    else:\n        image = tf.image.rot90(image, k=1) # rotate 90\u00ba\n        mask = tf.image.rot90(mask, k=1) # rotate 90\u00ba\n\n    return image, mask\n\ndef data_augment_crop(image, mask):\n    p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    crop_size = tf.random.uniform([], int(IMG_SIZE*.8), IMG_SIZE, dtype=tf.int32)\n    \n    if p_crop > .5:\n        ox = tf.random.uniform([], 0, IMG_SIZE-crop_size, dtype=tf.int32)\n        oy = tf.random.uniform([], 0, IMG_SIZE-crop_size, dtype=tf.int32)\n        image = tf.image.crop_to_bounding_box(image, oy, ox, crop_size, crop_size)\n        mask = tf.image.crop_to_bounding_box(mask, oy, ox, crop_size, crop_size)\n    else:\n        if p_crop > .4:\n            image = tf.image.central_crop(image, central_fraction=.7)\n            mask = tf.image.central_crop(mask, central_fraction=.7)\n        elif p_crop > .2:\n            image = tf.image.central_crop(image, central_fraction=.8)\n            mask = tf.image.central_crop(mask, central_fraction=.8)\n        else:\n            image = tf.image.central_crop(image, central_fraction=.9)\n            mask = tf.image.central_crop(mask, central_fraction=.9)\n    \n    image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])\n    mask = tf.image.resize(mask, size=[IMG_SIZE, IMG_SIZE], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n    return image, mask","dae7f7f2":"IMG_SIZE = 256\nIMAGE_SIZE = [IMG_SIZE, IMG_SIZE]\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 16\n\n# hyperparameters saved for later use during inference\nhparams = {\n    \"IMG_SIZE\": IMG_SIZE,\n    \"SCALE_FACTOR\": 1024\/\/IMG_SIZE,  # TFRecords dataset uses tiles of 1024x1024\n    \"BATCH_SIZE\": BATCH_SIZE,\n    \"K_SPLITS\": K_SPLITS}\nwith open(\"hparams.json\", \"w\") as json_file:\n    json_file.write(json.dumps(hparams, indent = 4))\n\n# decode image or mask\ndef decode_image(image, isjpeg=True):\n    if isjpeg:\n        ch = 3\n        image = tf.image.decode_jpeg(image, channels=ch)\n    else:\n        ch = 1\n        image = tf.image.decode_png(image, channels=ch)\n        image = tf.expand_dims(image, -1)\n    image = tf.cast(image, tf.float32)\n    image = image \/255.\n    image = tf.reshape(image, [*IMAGE_SIZE, ch])\n    return image\n\n# read a single record \ndef read_tfrecord(example):\n    tfrecord_format = ( # only extract features we are interested in\n        {\n            \"image\/encoded\": tf.io.FixedLenFeature([], tf.string),\n            \"mask\/encoded\": tf.io.FixedLenFeature([], tf.string),\n        }\n    )\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example[\"image\/encoded\"], True) # jpeg format\n    mask = decode_image(example[\"mask\/encoded\"], False) # png format\n    return image, mask\n\ndef load_dataset(filenames, IsTrain=True):\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False  # disable order, increase speed\n    # automatically interleaves reads from multiple files\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTOTUNE)\n    # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.with_options(ignore_order)  \n    dataset = dataset.map(partial(read_tfrecord), num_parallel_calls=AUTOTUNE)\n    if IsTrain: # augmentation        \n        dataset = dataset.map(partial(data_augment), num_parallel_calls=AUTOTUNE)\n    # returns a dataset of (image, mask) pairs \n    return dataset\n\ndef get_dataset(filenames, IsTrain=True):\n    dataset = load_dataset(filenames, IsTrain)\n    dataset = dataset.shuffle(BATCH_SIZE*256)\n    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.repeat()\n    return dataset","5b0e37ac":"train_dataset = get_dataset(FILENAMES, True)\n\nimage_batch, mask_batch = next(iter(train_dataset))\n\ndef show_batch(image_batch, mask_batch):\n    plt.figure(figsize=(16, 16))\n    for n in range(min(BATCH_SIZE,16)):\n        ax = plt.subplot(4, 4, n + 1)\n        plt.imshow(image_batch[n])\n        plt.imshow(np.squeeze(mask_batch[n]), alpha=0.25)\n        plt.axis(\"off\")\n\nshow_batch(image_batch.numpy(), mask_batch.numpy())","c1796fa1":"# calculate number of images in train\/val sets for each fold\nkf = KFold(n_splits=K_SPLITS)\ndf = pd.read_pickle(DPATH+'\/record_stats.pkl')\ntcnt, vcnt = np.zeros(K_SPLITS, dtype=int), np.zeros(K_SPLITS, dtype=int)\nidx = 0\nfor train_index, test_index in kf.split(FILENAMES):\n    for i in train_index:\n        fname = FILENAMES[i].split('\/')[-1].split('\\\\')[-1]\n        tcnt[idx] += df[df.File == fname].ImgCount.iloc[0]\n    for i in test_index:\n        fname = FILENAMES[i].split('\/')[-1].split('\\\\')[-1]\n        vcnt[idx] += df[df.File == fname].ImgCount.iloc[0]\n    idx += 1\n\nprint('Train images: {}, Validation images: {}'.format(tcnt, vcnt))","2001fdc4":"dff = pd.read_pickle('..\/input\/hubmap-quick-reference\/image_stats.pkl')\ntot = dff[dff.dataset == 'train'].glomeruli.sum()\ndff.head(15)","af2f7725":"# metrics and loss functions\nsmooth = 1.\n\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1.-dice_coef(y_true, y_pred)\n\n\ndef iou(y_true, y_pred):\n    def f(y_true, y_pred):\n        intersection = (y_true * y_pred).sum()\n        union = y_true.sum() + y_pred.sum() - intersection\n        x = (intersection + 1e-15) \/ (union + 1e-15)\n        x = x.astype(np.float32)\n        return x\n    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n\ndef tversky(y_true, y_pred, smooth=1, alpha=0.7):\n    y_true_pos = K.flatten(y_true)\n    y_pred_pos = K.flatten(y_pred)\n    true_pos = K.sum(y_true_pos * y_pred_pos)\n    false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n    false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n    return (true_pos + smooth) \/ (true_pos + alpha * false_neg + (1 - alpha) * false_pos + smooth)\n\ndef tversky_loss(y_true, y_pred):\n    return 1 - tversky(y_true, y_pred)\n\ndef focal_tversky_loss(y_true, y_pred, gamma=0.75):\n    tv = tversky(y_true, y_pred)\n    return K.pow((1 - tv), gamma)","f5142f17":"MNAME = 'FPN-model43e'\n\ndef get_callbacks(idx):\n    mc = ModelCheckpoint(MNAME+\"-{}.h5\".format(idx), save_best_only=True)\n    rp = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n    #cl = CSVLogger(\"train_log-{}.csv\".format(idx))\n    #tb = TensorBoard()\n    es = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=False)\n    return [mc, rp, es]","48e4dc30":"#%%capture\nhistory = []\nloss, vloss, dice, vdice = np.zeros(K_SPLITS),np.zeros(K_SPLITS),np.zeros(K_SPLITS),np.zeros(K_SPLITS)\nidx = 0\nlr = 5e-4\nselected_folds = [4] # select a few folds only to reduce execution time\nfor train_index, test_index in kf.split(FILENAMES):\n    if idx in selected_folds: \n        train_dataset = get_dataset([FILENAMES[i] for i in train_index], True)\n        valid_dataset = get_dataset([FILENAMES[i] for i in test_index], False)\n        model = sm.FPN('efficientnetb4', classes=1, encoder_weights='imagenet', activation = 'sigmoid')\n        with open(MNAME+\"-{}.json\".format(idx), \"w\") as json_file:\n            json_file.write(model.to_json())    \n        opt = tf.keras.optimizers.Adam(lr)\n        metrics = [\"acc\", iou, dice_coef, tversky]\n        model.compile(loss=dice_coef_loss, optimizer=opt, metrics=metrics)\n        callbacks = get_callbacks(idx)\n        hist = model.fit(train_dataset, \n                            validation_data=valid_dataset,\n                            epochs=25,\n                            steps_per_epoch=1+tcnt[idx]\/\/BATCH_SIZE,\n                            validation_steps=1+vcnt[idx]\/\/BATCH_SIZE,\n                            callbacks=callbacks)\n        loss[idx] = hist.history['loss'][-1]\n        vloss[idx] = min(hist.history['val_loss'])\n        dice[idx] = hist.history['dice_coef'][-1]\n        vdice[idx] = max(hist.history['val_dice_coef'])\n        history.append(hist)\n    idx += 1","17266052":"pd.DataFrame({'loss': loss, 'min val.loss': vloss, 'dice coef.': dice, 'max val. dice coef.': vdice})","3fbdd998":"colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\nplt.figure(figsize=(16,10))\nfor i in range(len(selected_folds)):\n    plt.plot(history[i].history['loss'], linestyle='-', color=colors[i], label='Train loss fold #{}'.format(selected_folds[i]))\nfor i in range(len(selected_folds)):\n    plt.plot(history[i].history['val_loss'], linestyle='--', color=colors[i], label='Validation loss fold #{}'.format(selected_folds[i]))\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show();","b854c514":"colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\nplt.figure(figsize=(16,10))\nfor i in range(len(selected_folds)):\n    plt.plot(history[i].history['dice_coef'], linestyle='-', color=colors[i], label='Train dice coef. fold #{}'.format(selected_folds[i]))\nfor i in range(len(selected_folds)):\n    plt.plot(history[i].history['val_dice_coef'], linestyle='--', color=colors[i], label='Validation dice coef. fold #{}'.format(selected_folds[i]))\nplt.title('Model Dice Coef.')\nplt.ylabel('Dice coef.')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show();","d5591a24":"Plot a few images from the dataset to check that everything is OK (including augmentation):","7eb546ca":"Let's have a look at some statistics from the [quick reference](https:\/\/www.kaggle.com\/mistag\/hubmap-quick-reference):","d6771e01":"There is a Pandas pickle file accompanying the TFRecords files containing the number of images per TFRecord.","6cea15da":"## Compile & train model\nNote that we save the built models as a .json file for use during inference, one for each fold.","a3a5552a":"## Loss Functions\nThere are several loss functions to choose from, a few ones are defined below. The Focal Tversky loss is known to perform well on many segmentation tasks. We could also use the Dice coefficient loss or the Tversky loss (experiment to find the best one). ","601451d0":"## Dataset creation\nWe will use TFRecords with a resolution of 256x256 (downscaled from 1024x1024).","22ad09f2":"## Keras UFPN+EfficientNet training with TFRecords input\nIn this notebook we will train a U-Net architecture implemented in Keras\/TensorFlow. We already created TFRecords of the dataset in this notebook:\n  * [HuBMAP TIF 2 TFRecords](https:\/\/www.kaggle.com\/mistag\/hubmap-image-2-tfrecords-256-512-1024)  \n  \n","31782ba2":"Looks pretty good! We are now ready for the final step - [making predictions with the saved model](https:\/\/www.kaggle.com\/mistag\/inference-hubmap-fpn-single-model-ii). ","7f86c9fb":"## Inspect learning curves\nBelow we plot the loss for both training and validation along with the Dice coefficients for the last fold. It is important to keep an eye on these curves to verify that our model and training are setup correctly."}}