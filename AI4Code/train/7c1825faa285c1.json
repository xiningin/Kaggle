{"cell_type":{"3772060f":"code","380cd638":"code","6c0254e5":"code","8040bc57":"code","04aff83e":"code","8019e784":"code","17bd3f1c":"code","05ae09f2":"code","80abed0f":"code","af4dc735":"code","e18071c1":"code","ed87a9b5":"code","6619a899":"code","290527ec":"code","98841e40":"code","2a2a64bb":"code","0f97bc86":"code","70cb1b40":"code","c7b79b7a":"code","c85dd336":"code","dcfc0da7":"code","de85aebb":"code","185d881e":"code","5014f1f2":"code","4f2a7bb1":"code","a9f5c5cc":"code","5495b763":"code","110a0dca":"code","3596981f":"code","f4673936":"code","ed2e6e2c":"code","caf5c644":"code","7215d356":"code","3f38a5f6":"code","483a5525":"markdown","7f9f4570":"markdown","3592ca6e":"markdown","bc7ede77":"markdown","37fb02fb":"markdown","18b89623":"markdown","f8e31147":"markdown","55783c78":"markdown","43b767e9":"markdown","adc381b4":"markdown","8abde725":"markdown","621f32ca":"markdown","7a65b166":"markdown","b43f07bc":"markdown","8f56b83d":"markdown","5b5f9327":"markdown","060a7a1f":"markdown","631134fe":"markdown","6ebbc01b":"markdown","96c182f5":"markdown","43d2d4be":"markdown","829f2819":"markdown","3c2e163a":"markdown","d4eb886a":"markdown","726edcf6":"markdown","3dd1966a":"markdown","34890270":"markdown","2e4ac625":"markdown","e7ab9661":"markdown","5415bcae":"markdown","f6a04dd7":"markdown","470eadb1":"markdown","94c9aabf":"markdown"},"source":{"3772060f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","380cd638":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nimport warnings  \nwarnings.filterwarnings('ignore')","6c0254e5":"entre = pd.read_csv('..\/input\/entrepreneurial-competency-in-university-students\/data.csv')\nentre.head()","8040bc57":"#Features Understanding\nlist(entre.columns)","04aff83e":"#check for dtypes\nentre.dtypes\n#they are in correct dtypes","8019e784":"#check for nan\nentre.isnull().sum()","17bd3f1c":"entre.drop('ReasonsForLack', axis=1, inplace=True) #drop reason for lack\n#okay we are done with data wrangling","05ae09f2":"#first thing first lets classify the features to numerical and categorical type\n\nnumerical_features = ['Age']\n\ncategorical_features = [i for i in list(entre.columns) if i != 'Age' and i != 'y']\n\ntarget = entre['y']\n\n#only 1 categorical features\n#the rest is either categorical or ordinal discreet (still categorical but discreet and has clear order)","80abed0f":"entre[numerical_features].describe()","af4dc735":"#Age histogram\nentre[numerical_features].hist(bins=entre['Age'].nunique())\nplt.show()","e18071c1":"entre[numerical_features].boxplot()\n#ax.set_xticklabels(list(heart_attack[numerical_features].columns))\nplt.show()","ed87a9b5":"#Barplot of age\n#count the value inside numerical variable and plot\n#since our nuemrical feature is only one, we use value_counts instead of groupby\ndata = entre[numerical_features].value_counts().to_frame().rename(columns={0 : 'Counts'})\nindex = [i[0] for i in data.index]\nax = plt.bar(x=index, height=data['Counts'])\nfor bar in ax:\n    yval = bar.get_height()\n    plt.text(bar.get_x() + 0.12, yval + 0.7, yval) #parameter = x position, y position, and string\nplt.show()","6619a899":"entre = entre[entre['Age'] <= 22]\nentre[numerical_features].hist(bins=entre['Age'].nunique())\nplt.show() #better","290527ec":"#Categorical features \n#starts with this question, what is the dominant value of each cat features?\n\nfig = plt.figure(figsize=(15, 20))\nfor x in range(1,len(list(entre[categorical_features].columns))):\n    ax = plt.subplot(7, 2, x)\n    sns.countplot(entre[categorical_features[x]]) #, textprops={'fontsize': 20}\n    plt.title(categorical_features[x], fontsize=20)\n    plt.xlabel('')\n    plt.tight_layout()\n#fig.suptitle(year[x], fontsize=16, y=1.03)\nplt.show()","98841e40":"plt.figure(figsize=(10,7))\nsns.countplot(y = entre[categorical_features[0]])\nplt.show()","2a2a64bb":"plt.figure(figsize=(7,5))\nsns.countplot(target)\nplt.show()","0f97bc86":"#fig, ax = plt.subplots(figsize=(10, 8))\n\ntranslate_output = {'output' : {1: 'Become Entrepreneur',  0 : 'Not Entrepreneur'}}\n\n# Pie chart\nlabels = list(target.value_counts().reset_index()['index'])\nlabels = [translate_output['output'][i] for i in labels]\nsizes = list(target.value_counts().reset_index()['y'])\n\n#colors\ncolors = ['#ff9999','#66b3ff']\n#explsion\nexplode = (0.05,0.05)\nfig, ax = plt.subplots(figsize=(8, 5)) \nax.pie(sizes, colors = colors, labels=labels, autopct='%1.1f%%', startangle=90, pctdistance=0.85, explode = explode)\n#draw circle\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n# Equal aspect ratio ensures that pie is drawn as a circle\nax.axis('equal')  \nplt.tight_layout()\nplt.show() ","70cb1b40":"#try to use pairplot\nsns.pairplot(pd.concat([entre[numerical_features], target], axis=1), hue='y')\nplt.savefig('num_pairplot.png')\nplt.show()","c7b79b7a":"#heatmap\nfig, ax = plt.subplots(figsize=(5, 3))\nsns.heatmap(pd.concat([entre[numerical_features], target], axis=1).corr(),annot=True,cmap=\"Oranges\",ax=ax)\nplt.savefig('num_heatmap.png')\nplt.show()","c85dd336":"#try to use pairplot\nfig1 = plt.figure(figsize=(16,16))\nsns.pairplot(pd.concat([entre[categorical_features], target], axis=1), hue='y')\nplt.savefig('cat_pairplot.png')\nplt.show()","dcfc0da7":"#i'm not using sklearn label encoding, doing it manually instead\ncategorical_features_to_change = ['IndividualProject', 'City', 'Influenced', 'MentalDisorder']\n\n#we need to drop education sector later as it's not serve any purpose because it will make our dataset imbalance\nfor i in categorical_features_to_change:\n    entre[i] = entre[i].apply(lambda x: 1 if 'Yes' else 0)","de85aebb":"#for gender\nentre['Gender'] = entre['Gender'].apply(lambda x : 0 if 'Male' else 1)\n\n#perform manual label encoding, just like dummy variables in pandas instead of using sklearn\nentre = entre.replace({'KeyTraits' : {'Passion' : 0, 'Vision' : 1, 'Resilience' : 2, 'Positivity' : 3, 'Work Ethic' : 4}})\n\n#drop educationsector\nentre.drop(['EducationSector'], axis=1, inplace=True)","185d881e":"entre.dtypes #ok everything is in order","5014f1f2":"target = entre['y'] #get our output\nentre.drop(['y'], axis=1, inplace=True) #drop from entre df\ndef mi_scores(dataset, target):\n    mutual_class = mutual_info_classif(dataset, target, random_state=42)\n    mutual_class = pd.Series(mutual_class, name=\"mutual information scores\", index=dataset.columns)\n    mutual_class = mutual_class.sort_values(ascending=False)\n    return mutual_class\n\nmutual_info_score = mi_scores(entre, target)\nmutual_info_score # show a few features with their MI scores","4f2a7bb1":"#plot MI\nplt.figure(dpi=100, figsize=(8, 5))\nmutual_info_score = mutual_info_score.sort_values(ascending=True)\nwidth = np.arange(len(mutual_info_score))\nticks = list(mutual_info_score.index)\nplt.barh(width, mutual_info_score, color= '#ff9999')\nplt.yticks(width, ticks)\nplt.savefig('mutual_information_score.png')\nplt.title(\"Mutual Information Scores\")","a9f5c5cc":"#features to use\nfeatures = [i for i in entre.columns if i not in ['SelfReliance', 'Perseverance', 'Influenced', 'Age', 'Individual Project']]\n\nentre = entre[features]","5495b763":"entre.head()\n#as you can see all our features are already in either binary or label encoding form, \n#so no need to transform nor scaling them","110a0dca":"X_train, X_valid, y_train, y_valid = train_test_split(entre, target, stratify=target, train_size=0.80)","3596981f":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_shape=[len(list(entre.columns))]),\n    layers.Dense(256, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(256, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(1, activation='sigmoid')\n])","f4673936":"#optimizer\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'],\n)","ed2e6e2c":"#early stopping to prevent overfitting\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=10,\n    min_delta=0.001,\n    restore_best_weights=True,\n)","caf5c644":"#train the model\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=512,\n    epochs=200,\n    callbacks=[early_stopping],\n)","7215d356":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title=\"Accuracy\")\nplt.show()","3f38a5f6":"#Best validation loss and validation mae score\nprint((\"Best Validation Loss: {:0.2f}\" +\"\\nBest Validation accuracy: {:0.2f}\").format(history_df['val_loss'].min(), history_df['val_binary_accuracy'].max()))","483a5525":"### NAN\n    no nan except in ReasonsForLack features, 91 NAN and this is not good because we only have 219 samples\n    it means 41% of total samples\n    for the time being we should just remove this feature and see what happen with our predictive model later","7f9f4570":"### Check Dtypes","3592ca6e":"## Features Explanation\n\n### EducationSector\n    Engineering background or not\n\n### IndividualProject\n    If the student builds personal project\n\n### Age\n    Age of student\n\n### Gender\n    Sex of student\n\n### City\n    If the student stays in a city\n\n### Influenced\n    If the student is influenced by someone\n\n### Perseverance\n    Rating of a student based upon perseverance\n\n### DesireToTakeInitiative\n    Rating of a student based upon desire to take initiative\n\n### Competitiveness\n    Competitive rating\n\n### SelfReliance\n    Self reliance rating\n\n### StrongNeedToAchieve\n    Strong need to achieve a goal rating\n\n### SelfConfidence\n    Self confidence rating\n\n### GoodPhysicalHealth\n    Good physical health rating\n\n### MentalDisorder\n    If there is any mental disorder\n\n### KeyTraits\n    Key traits of the student\n\n### ReasonsForLack\n    Reason for lack of entrepreneurship culture\n\n### y\n    Whether the student seems to become a entrepreneur or not","bc7ede77":"## Univariate Analysis","37fb02fb":"### Pairplot","18b89623":"## Age Distribution\n### Histogram\n### Boxplot","f8e31147":"### Countplot","55783c78":"## Entrepreneur Competency in University Students","43b767e9":"## Check for data types","adc381b4":"## using keras deep learning model","8abde725":"### Check before proceed further","621f32ca":"### Check For NAN","7a65b166":"### Perform Binary encoding to categorical features with binary traits","b43f07bc":"### Descriptive Statistic\n### Numerical","8f56b83d":"## Mutual Information Score","5b5f9327":"### Outliers\n    I decide to drop outliers above age 22\n \n### Results below","060a7a1f":"### analysis\n    58.4% becomes entrepreneur vs 41.6% not entrepenreur\n    it is the fact from the sample\n    but this also means\n    Imbalance in the target or the dataset itself is imbalance\n    16.8% difference is huge. this will affect our model's accuracy","631134fe":"### Train Test Split\n### I'm not using Cross validation yet","6ebbc01b":"### Pairplot","96c182f5":"## Categorical","43d2d4be":"## Target\n### Balance Analysis\n","829f2819":"# Bivariate Analysis\n\n## Numerical ","3c2e163a":"## Categorical Univariate Analysis\n    1. most of respondents are from engineering science major\n    2. eventhough by small margin, more students have individual project than not\n    3. There are more Male students than female, is it because the majority is engineering science students? have to check later\n    4. more students stay in city than not\n    5. most students are influenced by someone \n\n## Below is categorical data from survey given to students\n    1. More students answer 3 or 4 in:\n    Perseverance, Desire to take initiative, Competitiveness, self reliance, self confidence, good physical health\n    2. Most Students feel the strong need to achieve something\n    4. 2 of the most strong key traits for students are positivity and passion","d4eb886a":"### Heatmap","726edcf6":"### Little bit unbalance data","3dd1966a":"## Building Machine Learning Model","34890270":"## Exploratory Data Analysis","2e4ac625":"## Data Wrangling","e7ab9661":"### Distribution Analysis\n    our histogram shows that age features is bell-shape with right-skewed and short tail\n    Turns out there are outliers in our dataset\n    probably because age values 17 and 22-26 is so little compare to other age values\n    lets see the bar chart","5415bcae":"#### Age\n    219 values\n    Minimum age is 17\n    maximum age is 26,  this is data  represent university student age\n    mean of age is 19.75\n    median is 20\n    Since median and mean is close, the age feature might not have outliers \n    but since median is still slightly higer than mean, the data distribution might be bell-shaped but right-skewed with short tail\n    standard deviation is 1.28, if our age feature follows normal distribution, then 68% of age will be around 18.47-21.03, lets say around 18-21 years old, and 95% of age will be around 17-22 years old\n    ","f6a04dd7":"## Loss and Accuracy Plot","470eadb1":"## Categorical Data","94c9aabf":"### Mutual information score analysis\n    Competitiveness has the highest features score\n    Self reliance, perseverance, influenced, age, and individual project has no impact\n    so we're only going to use 8 features"}}