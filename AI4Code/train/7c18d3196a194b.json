{"cell_type":{"b775c2bd":"code","a02192bd":"code","c847e389":"code","1fa0ff06":"code","3719dbf8":"code","77c69976":"code","53c1cc55":"code","4cd7fdc3":"code","d0147a42":"code","f5f3b083":"code","3ed0515d":"code","4ef1fc20":"code","9d03ce5d":"code","13c3d155":"code","6ad08ffa":"code","71155904":"code","737823ca":"code","07b64374":"code","008244a9":"code","dba95040":"code","d13d6674":"code","f31a7810":"code","6b335063":"code","56d2cfd0":"code","1ec661c4":"code","dfc7b9fe":"code","70cea555":"code","20953495":"code","b4eb5fea":"code","d267d95d":"code","58fc588f":"code","02f585d6":"code","267d22ba":"code","dd89946e":"code","696c9365":"code","1a2d8093":"code","286cfcbf":"code","f4ad714c":"code","4f333d6e":"code","99af51c2":"code","3a63bda5":"code","c786911a":"code","56541f7c":"markdown","10149258":"markdown","264c1a79":"markdown","f02deba6":"markdown","dc1d466e":"markdown","75bd964f":"markdown","89af0517":"markdown","cbdd40f9":"markdown","6fac1642":"markdown","c1027c77":"markdown","4f29a255":"markdown","978f1363":"markdown","5af8d22b":"markdown","721c187d":"markdown"},"source":{"b775c2bd":"!pip install torchcontrib","a02192bd":"import os\nimport math\nimport time\nimport torch\nimport random\nimport shutil\nimport datetime\nimport functools\nimport numpy as np\nimport pandas as pd\nimport torch.nn.functional as F\n\n\nfrom torch import nn\nfrom pathlib import Path\nfrom torchcontrib.optim import SWA\nfrom sklearn.cluster import KMeans\nfrom collections import OrderedDict\nfrom torch.utils.data import Dataset\nfrom sklearn.model_selection import *\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer","c847e389":"TARGETS = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\nSCORED_TARGETS = [0, 1, 3]\n\nNUM_TARGETS = len(TARGETS)\n\nSEQ_SCORED_PUBLIC = 68\nSEQ_SCORED_PRIVATE = 91\n\nSEQ_LEN_PUBLIC = 107\nSEQ_LEN_PRIVATE = 130","1fa0ff06":"BASE_PATH = \"..\/input\/stanford-covid-vaccine\/\"\nCP_PATH = \"\"\nPRETRAINED_PATH = \"..\/input\/covid-pretrained\/pretrained_model.pt\"\n\nDEVICE = torch.device('cuda')\nTODAY = str(datetime.date.today())","3719dbf8":"train_df = pd.read_json(str(Path(BASE_PATH) \/ 'train.json'), lines=True)\n\ntest_df = pd.read_json(str(Path(BASE_PATH) \/ 'test.json'), lines=True)\npublic_df = test_df[test_df[\"seq_length\"] == SEQ_LEN_PUBLIC].reset_index(drop=True)\nprivate_df = test_df[test_df[\"seq_length\"] == SEQ_LEN_PRIVATE].reset_index(drop=True)","77c69976":"target_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\ninput_cols = ['sequence', 'structure', 'predicted_loop_type']\nerror_cols = ['reactivity_error', 'deg_error_Mg_pH10', 'deg_error_Mg_50C', 'deg_error_pH10', 'deg_error_50C']\n\ntoken_dicts = {\n    \"sequence\": {x: i for i, x in enumerate(\"ACGU\")},\n    \"structure\": {x: i for i, x in enumerate('().')},\n    \"predicted_loop_type\": {x: i for i, x in enumerate(\"BEHIMSX\")}\n}","53c1cc55":"def set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    \n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True  # False\n    \nSEED = 1234\nset_seed(SEED)","4cd7fdc3":"token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\n\ndef preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n    return np.transpose(\n        np.array(\n            df[cols]\n            .applymap(lambda seq: [token2int[x] for x in seq])\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )","d0147a42":"X = preprocess_inputs(train_df, cols=['sequence'])[:, :, 0]\nkmeans = KMeans(n_clusters=200, random_state=110).fit(X)\ngroups = kmeans.labels_","f5f3b083":"aug_df = pd.read_csv('..\/input\/covid-data\/aug_data.csv')","3ed0515d":"def augment_data(df, concat=True):\n    df = df.copy()\n    target_df = df.copy()\n    new_df = aug_df[aug_df['id'].isin(target_df['id'])]\n                         \n    del target_df['structure']\n    del target_df['predicted_loop_type']\n    new_df = new_df.merge(target_df, on=['id','sequence'], how='left').sort_values('index')\n\n    df['cnt'] = df['id'].map(new_df[['id','cnt']].set_index('id').to_dict()['cnt'])\n    df['log_gamma'] = 100\n    df['score'] = 1.0\n        \n    if concat:\n        df = df.append(new_df[df.columns]).reset_index(drop=True)\n        return df\n    else:\n        return new_df[df.columns].reset_index(drop=True)","4ef1fc20":"PUBLIC_IDS = public_df['id'].values\n\ndef is_public(id_seqpos):\n    id_ = '_'.join(id_seqpos.split('_')[:2])\n    return id_ in PUBLIC_IDS","9d03ce5d":"PL_PATH = \"..\/input\/covid-pl\/\"","13c3d155":"PL_PUBLIC = np.load(PL_PATH + 'pl_public.npy')\nPL_PRIVATE = np.load(PL_PATH + 'pl_private.npy')","6ad08ffa":"for t, target in enumerate(TARGETS):\n    tgt = []\n    for i in range(len(public_df)):\n        tgt.append(list(PL_PUBLIC[i, :SEQ_SCORED_PUBLIC, t]))\n    \n    public_df[target] = tgt\n    \n    tgt = []\n    for i in range(len(private_df)):\n        tgt.append(list(PL_PRIVATE[i, :SEQ_SCORED_PRIVATE, t]))\n    private_df[target] = tgt\n    \npublic_df['signal_to_noise'] = 1\nprivate_df['signal_to_noise'] = 1\n\npublic_df = augment_data(public_df)\nprivate_df = augment_data(private_df)","71155904":"def save_model_weights(model, filename, verbose=1, cp_folder=\"\"):\n    \"\"\"\n    Saves the weights of a PyTorch model\n    \n    Arguments:\n        model {torch module} -- Model to save the weights of\n        filename {str} -- Name of the checkpoint\n    \n    Keyword Arguments:\n        verbose {int} -- Whether to display infos (default: {1})\n        cp_folder {str} -- Folder to save to (default: {''})\n    \"\"\"\n    if verbose:\n        print(f\"\\n -> Saving weights to {os.path.join(cp_folder, filename)}\\n\")\n    torch.save(model.state_dict(), os.path.join(cp_folder, filename))\n\n\ndef load_model_weights(model, filename, verbose=1, cp_folder=\"\"):\n    \"\"\"\n    Loads the weights of a PyTorch model. The exception handles cpu\/gpu incompatibilities\n    \n    Arguments:\n        model {torch module} -- Model to load the weights to\n        filename {str} -- Name of the checkpoint\n    \n    Keyword Arguments:\n        verbose {int} -- Whether to display infos (default: {1})\n        cp_folder {str} -- Folder to load from (default: {''})\n    \n    Returns:\n        torch module -- Model with loaded weights\n    \"\"\"\n    if verbose:\n        print(f\"\\n -> Loading weights from {os.path.join(cp_folder,filename)}\\n\")\n    try:\n        model.load_state_dict(os.path.join(cp_folder, filename), strict=strict)\n    except BaseException:\n        model.load_state_dict(\n            torch.load(os.path.join(cp_folder, filename), map_location=\"cpu\"),\n            strict=True,\n        )\n    return model","737823ca":"def preprocess_inputs(df, cols):\n    return np.concatenate([preprocess_feature_col(df, col) for col in cols], axis=2)\n\n\ndef preprocess_feature_col(df, col):\n    dic = token_dicts[col]\n    dic_len = len(dic)\n    seq_length = len(df[col][0])\n    ident = np.identity(dic_len)\n    # convert to one hot\n    arr = np.array(\n        df[[col]].applymap(lambda seq: [ident[dic[x]] for x in seq]).values.tolist()\n    ).squeeze(1)\n    # shape: data_size x seq_length x dic_length\n    assert arr.shape == (len(df), seq_length, dic_len)\n    return arr\n\n\ndef preprocess(base_data, is_test=False):\n    inputs = preprocess_inputs(base_data, input_cols)\n    if is_test:\n        labels = None\n    else:\n        labels = np.array(base_data[target_cols].values.tolist()).transpose((0, 2, 1))\n        assert labels.shape[2] == len(target_cols)\n    assert inputs.shape[2] == 14\n    return inputs, labels\n\n\ndef get_bpp_feature(bpp):\n    bpp_nb_mean = 0.077522  # mean of bpps_nb across all training data\n    bpp_nb_std = 0.08914  # std of bpps_nb across all training data\n    bpp_max = bpp.max(-1)[0]\n    bpp_sum = bpp.sum(-1)\n    bpp_nb = torch.true_divide((bpp > 0).sum(dim=1), bpp.shape[1])\n    bpp_nb = torch.true_divide(bpp_nb - bpp_nb_mean, bpp_nb_std)\n    return [bpp_max.unsqueeze(2), bpp_sum.unsqueeze(2), bpp_nb.unsqueeze(2)]\n\n\n@functools.lru_cache(5000)\ndef load_from_id(id_):\n    path = Path(BASE_PATH) \/ f\"bpps\/{id_}.npy\"\n    data = np.load(str(path))\n    return data\n\n\ndef get_distance_matrix(leng):\n    idx = np.arange(leng)\n    Ds = []\n    for i in range(len(idx)):\n        d = np.abs(idx[i] - idx)\n        Ds.append(d)\n\n    Ds = np.array(Ds) + 1\n    Ds = 1 \/ Ds\n    Ds = Ds[None, :, :]\n    Ds = np.repeat(Ds, 1, axis=0)\n\n    Dss = []\n    for i in [1, 2, 4]:\n        Dss.append(Ds ** i)\n    Ds = np.stack(Dss, axis=3)\n\n    return Ds\n\n\ndef get_structure_adj(df):\n    Ss = []\n    for i in range(len(df)):\n        seq_length = df[\"seq_length\"].iloc[i]\n        structure = df[\"structure\"].iloc[i]\n        sequence = df[\"sequence\"].iloc[i]\n\n        cue = []\n        a_structures = OrderedDict([\n            ((\"A\", \"U\"), np.zeros([seq_length, seq_length])),\n            ((\"C\", \"G\"), np.zeros([seq_length, seq_length])),\n            ((\"U\", \"G\"), np.zeros([seq_length, seq_length])),\n            ((\"U\", \"A\"), np.zeros([seq_length, seq_length])),\n            ((\"G\", \"C\"), np.zeros([seq_length, seq_length])),\n            ((\"G\", \"U\"), np.zeros([seq_length, seq_length])),\n        ])\n        for j in range(seq_length):\n            if structure[j] == \"(\":\n                cue.append(j)\n            elif structure[j] == \")\":\n                start = cue.pop()\n                a_structures[(sequence[start], sequence[j])][start, j] = 1\n                a_structures[(sequence[j], sequence[start])][j, start] = 1\n\n        a_strc = np.stack([a for a in a_structures.values()], axis=2)\n        a_strc = np.sum(a_strc, axis=2, keepdims=True)\n        Ss.append(a_strc)\n\n    Ss = np.array(Ss)\n    return Ss","07b64374":"def create_loader(df, batch_size=64, is_test=False, shuffle=True):\n    if is_test:\n        shuffle = False\n        \n    features, labels = preprocess(df, is_test)\n    features_tensor = torch.from_numpy(features)\n    \n    if labels is not None:\n        labels_tensor = torch.from_numpy(labels)\n        dataset = VacDataset(features_tensor, df, labels_tensor)\n        loader = torch.utils.data.DataLoader(dataset, batch_size, shuffle=shuffle)\n    else:\n        dataset = VacDataset(features_tensor, df, None)\n        loader = torch.utils.data.DataLoader(dataset, batch_size, shuffle=False)\n    return loader\n\n\nclass VacDataset(Dataset):\n    def __init__(self, features, df, labels=None):\n        self.features = features\n        self.labels = labels\n        self.test = labels is None\n        self.ids = df[\"id\"]\n        self.score = None\n        self.structure_adj = get_structure_adj(df)\n        self.distance_matrix = get_distance_matrix(self.structure_adj.shape[1])\n        if \"score\" in df.columns:\n            self.score = df[\"score\"]\n        else:\n            df[\"score\"] = 1.0\n            self.score = df[\"score\"]\n        self.signal_to_noise = None\n        if not self.test:\n            self.signal_to_noise = df[\"signal_to_noise\"]\n            assert self.features.shape[0] == self.labels.shape[0]\n        else:\n            assert self.ids is not None\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, index):\n        bpp = torch.from_numpy(load_from_id(self.ids[index]).copy()).float()\n        adj = self.structure_adj[index]\n        distance = self.distance_matrix[0]\n        bpp = np.concatenate([bpp[:, :, None], adj, distance], axis=2)\n        if self.test:\n            return dict(sequence=self.features[index].float(), bpp=bpp, ids=self.ids[index])\n        else:\n            return dict(sequence=self.features[index].float(), bpp=bpp,\n                        label=self.labels[index], ids=self.ids[index],\n                        signal_to_noise=self.signal_to_noise[index],\n                        score=self.score[index])","008244a9":"USE_FT = True\n\nCNN_DROP = 0.1\nENC_DROP = 0.1\nRNN_DROP = 0.3\nLOGIT_DROP = 0.25\n\nD = 256","dba95040":"class Conv1dStack(nn.Module):\n    def __init__(self, in_dim, out_dim, kernel_size=3, padding=1, dilation=1):\n        super(Conv1dStack, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv1d(in_dim, out_dim, kernel_size=kernel_size, padding=padding, dilation=dilation, bias=False),\n            nn.BatchNorm1d(out_dim),\n            nn.Dropout(CNN_DROP),\n            nn.LeakyReLU(),\n        )\n        self.res = nn.Sequential(\n            nn.Conv1d(out_dim, out_dim, kernel_size=kernel_size, padding=padding, dilation=dilation, bias=False),\n            nn.BatchNorm1d(out_dim),\n            nn.Dropout(CNN_DROP),\n            nn.LeakyReLU(),\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        h = self.res(x)\n        return x + h\n\n\nclass Conv2dStack(nn.Module):\n    def __init__(self, in_dim, out_dim, kernel_size=3, padding=1, dilation=1):\n        super(Conv2dStack, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_dim, out_dim, kernel_size=kernel_size, padding=padding, dilation=dilation, bias=False),\n            nn.BatchNorm2d(out_dim),\n            nn.Dropout(CNN_DROP),\n            nn.LeakyReLU(),\n        )\n        self.res = nn.Sequential(\n            nn.Conv2d(out_dim, out_dim, kernel_size=kernel_size, padding=padding, dilation=dilation, bias=False),\n            nn.BatchNorm2d(out_dim),\n            nn.Dropout(CNN_DROP),\n            nn.LeakyReLU(),\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        h = self.res(x)\n        return x + h\n\n\nclass SeqEncoder(nn.Module):\n    def __init__(self, in_dim: int, out_dim=256):\n        super(SeqEncoder, self).__init__()\n        self.conv0 = Conv1dStack(in_dim, out_dim \/\/ 2, 3, padding=1)\n        self.conv1 = Conv1dStack(out_dim \/\/ 2, out_dim \/\/ 4, 6, padding=5, dilation=2)\n        self.conv2 = Conv1dStack(out_dim \/\/ 4, out_dim \/\/ 8, 15, padding=7, dilation=1)\n        self.conv3 = Conv1dStack(out_dim \/\/ 8, out_dim \/\/ 8, 30, padding=29, dilation=2)\n\n    def forward(self, x):\n        x1 = self.conv0(x)\n        x2 = self.conv1(x1)\n        x3 = self.conv2(x2)\n        x4 = self.conv3(x3)\n        x = torch.cat([x1, x2, x3, x4], dim=1)\n        # x = x.permute(0, 2, 1).contiguous()\n        # BATCH x 256 x seq_length\n        return x\n\n\nclass BppAttn(nn.Module):\n    def __init__(self, in_channel: int, out_channel: int):\n        super(BppAttn, self).__init__()\n        self.conv0 = Conv1dStack(in_channel, out_channel, 3, padding=1)\n        self.bpp_conv = Conv2dStack(5, out_channel)\n\n    def forward(self, x, bpp):\n        x = self.conv0(x)\n        bpp = self.bpp_conv(bpp)\n        # BATCH x C x SEQ x SEQ\n        # BATCH x C x SEQ\n        x = torch.matmul(bpp, x.unsqueeze(-1))\n        return x.squeeze(-1)\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        if ENC_DROP:\n            self.dropout = nn.Dropout(p=ENC_DROP)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) \/ d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        if ENC_DROP:\n            x = self.dropout(x)\n        return x\n\n\nclass TransformerWrapper(nn.Module):\n    def __init__(self, dmodel=256, nhead=8, num_layers=2):\n        super(TransformerWrapper, self).__init__()\n        self.pos_encoder = PositionalEncoding(256)\n        encoder_layer = TransformerEncoderLayer(d_model=dmodel, nhead=nhead)\n        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers)\n        self.pos_emb = PositionalEncoding(dmodel)\n\n    def flatten_parameters(self):\n        pass\n\n    def forward(self, x):\n        x = x.permute((1, 0, 2)).contiguous()\n        x = self.pos_emb(x)\n        x = self.transformer_encoder(x)\n        x = x.permute((1, 0, 2)).contiguous()\n        return x, None\n\n\nclass RnnLayers(nn.Module):\n    def __init__(self, dmodel, transformer_layers: int = 2):\n        super(RnnLayers, self).__init__()\n        self.dropout = nn.Dropout(RNN_DROP)\n        self.rnn0 = TransformerWrapper(dmodel, nhead=8, num_layers=transformer_layers)\n        self.rnn1 = nn.LSTM(dmodel, dmodel \/\/ 2, batch_first=True, num_layers=1, bidirectional=True)\n        self.rnn2 = nn.GRU(dmodel, dmodel \/\/ 2, batch_first=True, num_layers=1, bidirectional=True)\n\n    def forward(self, x):\n        self.rnn0.flatten_parameters()\n        x, _ = self.rnn0(x)\n        if self.rnn1 is not None:\n            self.rnn1.flatten_parameters()\n            x = self.dropout(x)\n            x, _ = self.rnn1(x)\n        if self.rnn2 is not None:\n            self.rnn2.flatten_parameters()\n            x = self.dropout(x)\n            x, _ = self.rnn2(x)\n        return x\n\n    \nclass BaseAttnModel(nn.Module):\n    def __init__(self, transformer_layers=2, d=256):\n        super(BaseAttnModel, self).__init__()\n        \n        if USE_FT:\n            self.linear0 = nn.Linear(14 + 3, 1)\n            self.seq_encoder_x = SeqEncoder(in_dim=18, out_dim=d)\n        else:\n            self.linear0 = nn.Linear(14, 1)\n            self.seq_encoder_x = SeqEncoder(in_dim=15, out_dim=d)\n        \n        self.attn = BppAttn(d, d\/\/2)\n        self.seq_encoder_bpp = SeqEncoder(in_dim=d\/\/2, out_dim=d)\n        self.seq = RnnLayers(d * 2, transformer_layers=transformer_layers)\n\n    def forward(self, x, bpp):\n        bpp_features = get_bpp_feature(bpp[:, :, :, 0].float())\n        \n        if USE_FT:\n            x = torch.cat([x] + bpp_features, dim=-1)\n            \n        learned = self.linear0(x)\n        x = torch.cat([x, learned], dim=-1)\n        x = x.permute(0, 2, 1).contiguous().float()\n        # BATCH x 18 x seq_len\n        bpp = bpp.permute([0, 3, 1, 2]).contiguous().float()\n        # BATCH x 5 x seq_len x seq_len\n        x = self.seq_encoder_x(x)\n        # BATCH x d x seq_len\n        bpp = self.attn(x, bpp)\n        bpp = self.seq_encoder_bpp(bpp)\n        # BATCH x d x seq_len\n        x = x.permute(0, 2, 1).contiguous()\n        # BATCH x seq_len x d\n        bpp = bpp.permute(0, 2, 1).contiguous()\n        # BATCH x seq_len x d\n        x = torch.cat([x, bpp], dim=2)\n        # BATCH x seq_len x 2d\n        x = self.seq(x)\n        return x\n\n\nclass AEModel(nn.Module):\n    def __init__(self, transformer_layers=2):\n        super(AEModel, self).__init__()\n        self.seq = BaseAttnModel(transformer_layers=transformer_layers, d=D)\n        self.linear = nn.Sequential(\n            nn.Linear(D * 2, 14),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x, bpp):\n        x = self.seq(x, bpp)\n        x = F.dropout(x, p=0.3)\n        x = self.linear(x)\n        return x\n\n\nclass FromAeModel(nn.Module):\n    def __init__(self, seq):\n        super(FromAeModel, self).__init__()\n        self.seq = seq\n        \n        self.linear = nn.Sequential(\n            nn.Linear(D * 2, len(target_cols)),\n        )\n        \n#         self.linear = nn.Sequential(\n#             nn.Linear(D * 2, D),\n#             nn.ReLU(),\n#             nn.Dropout(LOGIT_DROP),\n#             nn.Linear(D, len(target_cols))\n#         )\n\n    def forward(self, x, bpp, pred_len=68):\n        x = self.seq(x, bpp)\n        x = self.linear(x)\n        x = x[:, :pred_len]\n        return x\n","d13d6674":"PRETRAIN = False","f31a7810":"features, _ = preprocess(train_df, True)\nfeatures_tensor = torch.from_numpy(features)\ndataset0 = VacDataset(features_tensor, train_df, None)\n\nfeatures, _ = preprocess(public_df, True)\nfeatures_tensor = torch.from_numpy(features)\ndataset1 = VacDataset(features_tensor, public_df, None)\n\nfeatures, _ = preprocess(private_df, True)\nfeatures_tensor = torch.from_numpy(features)\ndataset2 = VacDataset(features_tensor, private_df, None)","6b335063":"BATCH_SIZE = 64\n\nloader0 = torch.utils.data.DataLoader(dataset0, BATCH_SIZE, shuffle=True)\nloader1 = torch.utils.data.DataLoader(dataset1, BATCH_SIZE, shuffle=True)\nloader2 = torch.utils.data.DataLoader(dataset2, BATCH_SIZE, shuffle=True)","56d2cfd0":"def learn_from_batch_ae(model, data):\n    seq = data[\"sequence\"].clone()\n    seq[:, :, :14] = F.dropout2d(seq[:, :, :14], p=0.3)\n    target = data[\"sequence\"][:, :, :14]\n    out = model(seq.to(DEVICE), data[\"bpp\"].to(DEVICE))\n    loss = F.binary_cross_entropy(out, target.to(DEVICE))\n    return loss\n\n\ndef train_ae(model, train_data, optimizer, lr_scheduler, epochs=10, start_epoch=0, start_it=0, log_path=\".\/logs\"):\n\n    it = start_it\n    model_save_path = Path(MODEL_SAVE_PATH)\n    \n    end_epoch = start_epoch + epochs\n    min_loss = 10.0\n    min_loss_epoch = 0\n    \n    if not model_save_path.exists():\n        model_save_path.mkdir(parents=True)\n        \n    for epoch in range(start_epoch, end_epoch):\n        model.train()\n        losses = []\n        for i, data in enumerate(train_data):\n            optimizer.zero_grad()\n            \n            loss = learn_from_batch_ae(model, data)\n            loss.backward()\n            \n            nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n            optimizer.step()\n            \n            if lr_scheduler:\n                lr_scheduler.step()\n                \n            loss_v = loss.item()\n            losses.append(loss_v)\n            it += 1\n            \n        loss_m = np.mean(losses)\n        if loss_m < min_loss:\n            min_loss_epoch = epoch\n            min_loss = loss_m\n        \n        print(f'Epoch {epoch} \\t loss={loss_m:.4f}')\n        \n        torch.save(optimizer.state_dict(), str(model_save_path \/ \"optimizer.pt\"))\n        torch.save(model.state_dict(), str(model_save_path \/ f\"model-{epoch}.pt\"))\n    \n    return dict(end_epoch=end_epoch, it=it, min_loss_epoch=min_loss_epoch)\n","1ec661c4":"set_seed(SEED)","dfc7b9fe":"if PRETRAIN:\n    model = AEModel()\n    model = model.to(DEVICE)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    lr_scheduler = None\n\n    res = dict(end_epoch=0, it=0, min_loss_epoch=0)\n    epochs = [5, 5, 5, 5]\n\n\n    for e in epochs:\n        print(' -> Training with train data')\n        res = train_ae(model, loader0, optimizer, lr_scheduler, e, start_epoch=res[\"end_epoch\"], start_it=res[\"it\"])\n\n        print('\\n -> Training with public data')\n        res = train_ae(model, loader1, optimizer, lr_scheduler, e, start_epoch=res[\"end_epoch\"], start_it=res[\"it\"])\n\n        print('\\n -> Training with private data')\n        res = train_ae(model, loader2, optimizer, lr_scheduler, e, start_epoch=res[\"end_epoch\"], start_it=res[\"it\"])\n\n\n    save_model_weights(model, CP_PATH + f'pretrained_model_{TODAY}.pt')\n\n    print('\\n Done.')","70cea555":"CLASS_WEIGHT_5 = torch.from_numpy(np.array([1, 1, 1, 1, 1])).unsqueeze(0).cuda()\nCLASS_WEIGHT_3 = torch.from_numpy(np.array([1, 1, 0, 1, 0])).unsqueeze(0).cuda()","20953495":"def mcrmse(truth, pred, verbose=0, scored_targets=[0, 1, 3], filtered=None, reduce=True):\n    \"\"\"\n    Metric for the competition\n    \"\"\"\n\n    error = (truth - pred) ** 2\n    error = error[:, :, scored_targets]\n\n    if filtered is not None:\n        error = np.array([error[i] for i, kept in enumerate(filtered) if kept])\n\n    rmse = np.sqrt(error.mean(1))\n    \n    if verbose:\n        for t, score in zip(scored_targets, rmse.mean(0)):\n            print(f'Score for target \"{TARGETS[t]}\":\\t {score:.4f}')\n            \n    if reduce:\n        return rmse.mean()\n    else:\n        return rmse.mean(-1)\n\ndef MCRMSE(y_true, y_pred, class_weight):\n    colwise_mse = torch.mean(torch.square(y_true - y_pred), dim=1)\n    return torch.mean(torch.sqrt(colwise_mse) * class_weight, dim=1)\n\n\ndef sn_mcrmse_loss(predict, target, signal_to_noise, class_weight):\n    loss = MCRMSE(target, predict, class_weight)\n    weight = 0.5 * torch.log(signal_to_noise + 1.01)\n#     weight = torch.sqrt(signal_to_noise + 1.01)\n    weight = torch.clamp(weight, 0.01, 1000)\n    loss = (loss * weight).mean()\n    return loss","b4eb5fea":"def learn_from_batch(model, data, optimizer, lr_scheduler, class_weight, pred_len=68):\n    optimizer.zero_grad()\n    \n    out = model(\n        data[\"sequence\"].to(DEVICE), \n        data[\"bpp\"].to(DEVICE),\n        pred_len=pred_len,\n    )\n    \n    signal_to_noise = data[\"signal_to_noise\"] * data[\"score\"]\n    loss = sn_mcrmse_loss(\n        out, \n        data[\"label\"].to(DEVICE), \n        signal_to_noise.to(DEVICE),\n        class_weight,\n    )\n    loss.backward()\n    \n    nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n    \n    optimizer.step()\n    if lr_scheduler:\n        lr_scheduler.step()\n        \n    return out, loss\n\n\ndef evaluate(model, valid_data, class_weight):\n    model.eval()\n    loss_list = []\n    mcrmses = []\n    for i, data in enumerate(valid_data):\n        with torch.no_grad():\n            y = model(\n                data[\"sequence\"].to(DEVICE), \n                data[\"bpp\"].to(DEVICE),\n                pred_len=68,\n            )\n            \n            mcrmse_ = mcrmse(data[\"label\"].numpy(), y.cpu().numpy(), filtered=data[\"signal_to_noise\"] > 1, reduce=False)\n            mcrmses += list(mcrmse_)\n            \n            loss = sn_mcrmse_loss(\n                y, \n                data[\"label\"].to(DEVICE), \n                data[\"signal_to_noise\"].to(DEVICE),\n                class_weight\n            )\n            loss_list.append(loss.item())\n    model.train()\n    return dict(loss=np.mean(loss_list), mcrmse=np.mean(mcrmses))","d267d95d":"def predict(model, loader, pred_len=68):\n    \"\"\"\n    Usual torch predict function\n\n    Arguments:\n        model {torch model} -- Model to predict with\n        dataset {torch dataset} -- Dataset to predict with on\n\n    Keyword Arguments:\n        batch_size {int} -- Batch size (default: {32})\n        pred_len {int} -- Number of elements to keep for scoring (default: {68})\n\n    Returns:\n        numpy array -- Predictions\n    \"\"\"\n    model.eval()\n    preds = np.empty((0, pred_len, NUM_TARGETS))\n\n    with torch.no_grad():\n        for batch in loader:\n            y_pred = model(\n                batch[\"sequence\"].cuda(),\n                batch[\"bpp\"].cuda(),\n                pred_len=pred_len,\n            ).detach()\n            \n            preds = np.concatenate([preds, y_pred.cpu().numpy()])\n\n    return preds","58fc588f":"def train(model, train_data, valid_data, optimizer, lr_scheduler, epochs=10, swa_first_epoch=40, class_weight=None, pred_len=68):\n    it = 0\n\n    for epoch in range(epochs):\n        t0 = time.time()\n        print(f\"Epoch {epoch+1}\/{epochs}\", end='\\t')\n        model.train()\n        \n        # Training\n        losses = []\n        for i, data in enumerate(train_data):\n            _, loss = learn_from_batch(model, data, optimizer, lr_scheduler, class_weight, pred_len=pred_len)\n            loss_v = loss.item()\n            losses.append(loss_v)\n            it += 1\n        \n        # Evaluating\n        \n        if epoch + 1 >= swa_first_epoch:\n            optimizer.update_swa()\n            optimizer.swap_swa_sgd()\n            \n        eval_result = evaluate(model, valid_data, class_weight)\n        eval_loss = eval_result[\"loss\"]\n        \n        if epoch + 1 >= swa_first_epoch and epoch < epochs - 1:\n#             print(epoch, \"swap\")\n            optimizer.swap_swa_sgd()\n    \n        dt = time.time() - t0\n\n        lr = lr_scheduler.get_last_lr()[0] if lr_scheduler else 1e-3\n        print(f't={dt:.1f}s\\t lr={lr:.1e}\\tloss={np.mean(losses):.4f}', end='\\t')\n        print(f\"val_loss={eval_loss:.4f} \\t val_mcrmse={eval_result['mcrmse']:.4f}\")","02f585d6":"EPOCHS_1 = 30\nEPOCHS_2 = 10\nEPOCHS_3 = 10\nEPOCHS_4 = 5\n\nSWA_FIRST_EPOCH = 0\nWARMUP_PROP = 0.05\nK = 5 \nBATCH_SIZE = 32\nLR = 5e-4\n\nLOAD = True","267d22ba":"samples = train_df.copy().drop('score', axis=1)\nids = samples.reset_index()[\"id\"]\nset_seed(SEED)","dd89946e":"gkf = GroupKFold(n_splits=K)\nsplits = list(gkf.split(X=samples, groups=groups))","696c9365":"scores = []\npred_oof = np.zeros((len(samples) , 68, NUM_TARGETS))\n\nfor fold, (train_index, test_index) in enumerate(splits):\n    print(f\"\\n-------------  Fold {fold + 1}\/{K}  -------------\\n\")\n    set_seed(SEED)\n    \n    df_train = samples.loc[train_index].reset_index()\n    df_train = augment_data(df_train)\n    \n    df_val = samples.loc[test_index].reset_index()\n    df_val_aug = augment_data(df_val.copy(), concat=False)\n    \n    train_loader = create_loader(df_train, BATCH_SIZE)\n    public_loader = create_loader(public_df, BATCH_SIZE)\n    private_loader = create_loader(private_df, BATCH_SIZE)\n    \n    valid_loader = create_loader(df_val, BATCH_SIZE, shuffle=False)\n    valid_loader_aug = create_loader(df_val_aug, BATCH_SIZE, shuffle=False)\n\n    ae_model = AEModel()\n    \n    if LOAD:\n        load_model_weights(ae_model, PRETRAINED_PATH)\n    \n    model = FromAeModel(ae_model.seq)\n    model = model.to(DEVICE)\n    model.zero_grad()\n    model.train()\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n    if SWA_FIRST_EPOCH < EPOCHS_4:\n        optimizer = SWA(optimizer)\n    \n    s1 = EPOCHS_1 * len(train_loader)\n    s2 = EPOCHS_2 * len(public_loader)\n    s3 = EPOCHS_3 * len(private_loader)\n    s4 = EPOCHS_4 * len(train_loader)\n    num_training_steps = int(s1 + s2 + s3 + s4)\n    num_warmup_steps = int(EPOCHS_1 * len(train_loader) * WARMUP_PROP)\n    \n    lr_scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps, num_training_steps\n    )\n    \n    print('\\n -> Training with train data \\n')\n    train(\n        model, train_loader, valid_loader, optimizer, lr_scheduler, pred_len=68,\n        epochs=EPOCHS_1, swa_first_epoch=100, class_weight=CLASS_WEIGHT_5)\n    \n    print('\\n -> Training with public data \\n')\n    train(\n        model, public_loader, valid_loader, optimizer, lr_scheduler, pred_len=SEQ_SCORED_PUBLIC,\n        epochs=EPOCHS_2, swa_first_epoch=100, class_weight=CLASS_WEIGHT_3\n    )\n    \n    print('\\n -> Training with private data \\n')\n    train(\n        model, private_loader, valid_loader, optimizer, lr_scheduler, pred_len=SEQ_SCORED_PRIVATE,\n        epochs=EPOCHS_3, swa_first_epoch=100, class_weight=CLASS_WEIGHT_3\n    )\n    \n    print('\\n -> Retraining with train data \\n')\n    train(\n        model, train_loader, valid_loader, optimizer, lr_scheduler, pred_len=68,\n        epochs=EPOCHS_4, swa_first_epoch=SWA_FIRST_EPOCH, class_weight=CLASS_WEIGHT_3\n    )\n    \n    y = np.array(df_val[TARGETS].values.tolist()).transpose((0, 2, 1))\n    \n    pred_val_ = predict(model, valid_loader)\n    pred_val_aug = predict(model, valid_loader_aug)\n    pred_val = (0.5 * pred_val_ + 0.5 * pred_val_aug)\n    \n    pred_oof[test_index] = pred_val\n    \n    score = mcrmse(y, pred_val, verbose=0, scored_targets=[0, 1, 3], filtered=df_val[\"signal_to_noise\"] > 1)\n    scores.append(score)\n    \n    print(f\"\\n   -> Scored {score:.4f} with TTA\")\n\n    save_model_weights(model, f'model_{fold}.pt', verbose=0, cp_folder=CP_PATH)\n    \n    del model","1a2d8093":"BATCH_SIZE = 64\nTTA = True","286cfcbf":"test_df = pd.read_json(str(Path(BASE_PATH) \/ 'test.json'), lines=True)\n\npublic_df = test_df[test_df[\"seq_length\"] == SEQ_LEN_PUBLIC].reset_index(drop=True)\nprivate_df = test_df[test_df[\"seq_length\"] == SEQ_LEN_PRIVATE].reset_index(drop=True)\n\npub_loader = create_loader(public_df, BATCH_SIZE, is_test=True)\npri_loader = create_loader(private_df, BATCH_SIZE, is_test=True)\n\npublic_df_aug = augment_data(public_df.copy().drop('score', axis=1), concat=False)\npub_loader_aug = create_loader(public_df_aug, BATCH_SIZE, is_test=True)\n\nprivate_df_aug = augment_data(private_df.copy().drop('score', axis=1), concat=False)\npri_loader_aug = create_loader(private_df_aug, BATCH_SIZE, is_test=True)","f4ad714c":"pred_df_list = []\npred_public = np.zeros((len(public_df), 107, NUM_TARGETS))\npred_private = np.zeros((len(private_df), SEQ_LEN_PRIVATE, NUM_TARGETS))\n\nfor fold in range(K):\n    print(f\"\\n-------------  Fold {fold + 1}\/{K}  -------------\\n\")\n\n    model_load_path = CP_PATH + f\"model_{fold}.pt\"\n    \n    print(f' -> Loading weights from {model_load_path}\\n')\n    \n    ae_model0 = AEModel()\n    model_pub = FromAeModel(seq=ae_model0.seq)\n    model_pub = model_pub.to(DEVICE)\n    \n    ae_model1 = AEModel()\n    model_pri = FromAeModel(seq=ae_model1.seq)\n    model_pri = model_pri.to(DEVICE)\n    \n    state_dict = torch.load(model_load_path, map_location=DEVICE)\n    model_pub.load_state_dict(state_dict)\n    model_pri.load_state_dict(state_dict)\n    del state_dict\n\n    pred_public += predict(model_pub, pub_loader, pred_len=107) \/ K\n    pred_private += predict(model_pri, pri_loader, pred_len=130) \/ K\n\n    if TTA:\n        pred_public += predict(model_pub, pub_loader_aug, pred_len=107) \/ K\n        pred_private += predict(model_pri, pri_loader_aug, pred_len=130) \/ K\n              \nif TTA:\n    pred_public *= 0.5\n    pred_private *= 0.5","4f333d6e":"def pred_to_sub(df_test, pred_public, pred_private):\n    sub_public = df_test[df_test['seq_scored'] == SEQ_SCORED_PUBLIC][['id']].reset_index(drop=True)\n    sub_private = df_test[df_test['seq_scored'] == SEQ_SCORED_PRIVATE][['id']].reset_index(drop=True)\n    \n    test_preds = []\n\n    for sub, pred in [(sub_public, pred_public), (sub_private, pred_private)]:\n        for i, seq in enumerate(sub.id):\n            single_pred = pred[i]\n            single_df = pd.DataFrame(single_pred, columns=TARGETS)\n            single_df['id_seqpos'] = [f'{seq}_{x}' for x in range(single_df.shape[0])]\n            test_preds.append(single_df)\n\n    return pd.concat(test_preds)[['id_seqpos'] + TARGETS]","99af51c2":"sub = pred_to_sub(test_df, pred_public, pred_private)","3a63bda5":"score = np.mean(scores)\nscore","c786911a":"print(f'Saving submission to \"{TODAY}_{score:.4f}_pl.csv\"')\n\nsub.to_csv(f\"{TODAY}_{score:.4f}_pl.csv\", index=False)\nnp.save(f\"oof_{TODAY}_{score:.4f}_pl.npy\", pred_oof)\n\nsub.head()","56541f7c":"## Groups","10149258":"### Pretrain","264c1a79":"## Data\n","f02deba6":"## Model","dc1d466e":"## Prediction","75bd964f":"## Imports","89af0517":"## PL","cbdd40f9":"## Augmented data","6fac1642":"## Pretrain","c1027c77":"### Checkpointing","4f29a255":"## Training","978f1363":"## Initialization","5af8d22b":"## Loaders\n","721c187d":"### Create loaders"}}