{"cell_type":{"6c9723ce":"code","20edaec7":"code","649d181c":"code","29630854":"code","52e05075":"code","ee625b2e":"code","86bbf20a":"code","460854ef":"code","4427f44a":"code","53cd20e5":"code","4668b95a":"code","bacd2101":"code","c2ae8cca":"code","c14eab52":"code","722ec150":"code","734aafee":"code","f52dc271":"code","acd430e7":"code","a08cf04a":"code","c60a75d0":"code","3be24261":"markdown","dd452996":"markdown","076998bc":"markdown","8ab813af":"markdown","31f97c5e":"markdown","af7ee33b":"markdown","f8fe381c":"markdown","0416ad36":"markdown","2f9b2d44":"markdown","d863b9b9":"markdown","9e0467bf":"markdown","5da75cfc":"markdown","f8b9a416":"markdown","b29ed841":"markdown","946c99e6":"markdown","cde06e88":"markdown","28f54d78":"markdown","bd06cf56":"markdown","68428561":"markdown","72c8f375":"markdown","fc3a52ae":"markdown","89dd7e22":"markdown"},"source":{"6c9723ce":"import matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport scipy.special as sp\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport re\nimport timeit\n\n\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.corpus import stopwords\n\nLOAD_PARAMS = True\ntrain = False\nK = 10\nVI_ITERATIONS = 250\nEM_ITERATIONS = 15\nLOW_BOUND = 25\nUP_BOUND = 0.2\nb_scale = 0.8\na_scale = 0.5\nREDUCE_DATASET = False\nREDUTION_SIZE= 400\nmodel_path = \"..\/input\/lda-model\"\n\ndata = pd.read_csv('..\/input\/topic-modeling-for-research-articles\/train.csv')\ntrain_df = data[['ABSTRACT']]\n","20edaec7":"classes = ['Physics','Mathematics','Statistics','Computer Science','Quantitative Biology','Quantitative Finance']\nclass_count = []\nword_count = []\nfor c in classes:\n    class_count.append(data[c].value_counts()[[1]][1])\n    class_abstracts = data.where(data[c]==1)[['ABSTRACT']]\n    word_count.append(pd.Series(class_abstracts.values.flatten()).str.len().sum())\nprint(class_count)\n\n\nf, (ax1,ax2) = plt.subplots(1,2,figsize=(26,5))\n\nax1.bar(classes,class_count,align='center', alpha=0.9)\nax1.set_ylabel('Number of Documents')\nax2.bar(classes,word_count,align='center', alpha=0.5)\nax2.set_ylabel('Number of Words')\nax1.set_title('# of Research documents per Category')\nax2.set_title('# of Words per Category')\n\nplt.show()\nprint(\"Amount of documents:\", data.shape[0])\n","649d181c":"df = None\nif REDUCE_DATASET:\n    df = pd.DataFrame(columns=data.columns)\n    for i,c in enumerate(classes):\n        df = df.append(data.loc[data[c]==1][:REDUTION_SIZE]) #& ~df['ID'].isin(new_df['ID'])])\n    df = df.drop_duplicates(subset='ID')\n\n    class_count = []\n    word_count = []\n    for c in classes:\n        class_count.append(df[c].value_counts()[[1]][1])\n        class_abstracts = df.where(df[c]==1)[['ABSTRACT']]\n        word_count.append(pd.Series(class_abstracts.values.flatten()).str.len().sum())\n\n    f, (ax1,ax2) = plt.subplots(2,1,figsize=(30,10))\n    ax1.bar(classes,class_count,align='center', alpha=0.9)\n    ax1.set_ylabel('Number of Documents')\n    ax2.bar(classes,word_count,align='center', alpha=0.5)\n    ax2.set_ylabel('Number of Words')\n    ax1.set_title('# of Research documents per Category')\n    ax2.set_title('# of Words per Category')\n    plt.show()\n\n    print(\"Amount of documents:\", df.shape[0])\nelse:\n    df = data","29630854":"from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom gensim.utils import simple_preprocess\nfrom wordcloud import WordCloud\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\nstart = timeit.default_timer()\n\ncachedStopWords = set(stopwords.words('english'))\ndef pre_process(text):\n    tokens = simple_preprocess(text,deacc=True,min_len=3)\n    text = [lemmatizer.lemmatize(re.sub('[^A-Za-z]+', ' ', token)) for token in tokens if not token in cachedStopWords]\n    return text\n\ndf['BOW'] = df.apply(lambda row : pre_process(row['ABSTRACT']), axis = 1)\n\ncheckpoint = timeit.default_timer()\n\nprint(\"Pre-Processing finished in \", str(round(checkpoint-start,2)), \"seconds\")\nword_frequencies = pd.Series([y for x in df['BOW'].values.flatten() for y in x]).value_counts()\n","52e05075":"f, ax1 = plt.subplots(1,figsize=(30,10))\nax1.bar(word_frequencies.axes[0].tolist()[:15],word_frequencies.iloc[:15])\nax1.set_ylabel('Word Frequency')\nax1.set_title('Most Common (non-stopword) Terms in Dataset')\n#ax1.imshow(WordCloud().fit_words(word_frequencies))\nplt.show()\n","ee625b2e":"from gensim.corpora import Dictionary\ndictionary = Dictionary()\nif LOAD_PARAMS:\n    dictionary = dictionary.load(model_path+'\/dictionary')\n    print(\"Loaded Dictionary of Size:\",len(dictionary))\nelse:\n    start = timeit.default_timer()\n    dictionary = Dictionary(df['BOW'])\n    print(\"Dictionary size without filtering:\", len(dictionary))\n    dictionary.filter_extremes(no_below=LOW_BOUND,no_above=UP_BOUND)\n    end = timeit.default_timer()\n\n    print(\"Dictionary size after filtering:\", len(dictionary))\n\n    print(\"Constructing Filtered Dictionary took\", str(round(end-start,2)), \"Seconds\")\n    dictionary.save(\"dictionary\")\n\n    # Corpus is the set of documents where each word corresponds to an index in the dictionary\n\ncorpus = [np.array(list(filter(lambda a: a != -1, dictionary.doc2idx(doc)))) for doc in df['BOW']]\nV = len(dictionary)\n","86bbf20a":"# Phi initialized to row-uniform distribution\ndef initializeDoc(alpha, N, K):\n    phi = np.ones((N,K)) \/ K\n    gamma = alpha + N \/ K\n    return phi, gamma\n\ndef VI(alpha, beta, N, doc, K, numIterations, gammaPrev):\n    phi, gamma = initializeDoc(alpha, N, K)\n    t = 0\n    gammaPrev = gamma\n    phiPrev = phi\n    for t in range(numIterations):\n        words = doc[np.arange(N)]\n        expVec = np.exp(sp.digamma(gamma) - sp.digamma(np.sum(gamma)))\n        if len(words) > 0:\n            betaMat = beta[:, words]\n            phi = betaMat.T * expVec + 1e-100\n            phi = phi \/ np.expand_dims(np.sum(phi, axis=1), 1) # Normalize phi to avoid overflow since the update function is proportionate, not equals\n            gamma = alpha + np.sum(phi, axis=0)\n    return phi, gamma","460854ef":"import concurrent \nimport sys\nfrom multiprocessing import Pool, cpu_count\n\ndef hessian_inverse_gradient(alpha, M, K, gamma):\n    alphaSum = np.sum(alpha)\n    z = M * sp.polygamma(1, alphaSum)  # c\n\n    hessian = np.zeros(K)\n    gradient = np.zeros(K)\n\n    gammaSumDigamma = sp.digamma(np.sum(gamma, axis=1))\n    gammaSumDigamma = np.expand_dims(gammaSumDigamma, 1)  # (M, 1)\n    extectedTheta = sp.digamma(gamma) - gammaSumDigamma  # (M, K)\n    avgExtectedTheta = np.mean(extectedTheta, axis=0)  # (1, K)\n\n    if alphaSum == 0:\n        print(\"alpha:\", alpha)\n    for k in range(K):\n        hessian[k] = -M * sp.polygamma(1, alpha[k])\n        gradient[k] = M * (sp.digamma(alphaSum) - sp.digamma(alpha[k]) + avgExtectedTheta[k])\n    inv_hessian = np.reciprocal(hessian)\n    if z == 0:\n        print(\"z:\", z)\n\n    c = np.sum(gradient * inv_hessian)\n    c \/= (np.sum(inv_hessian) + (1 \/ z))\n\n    return (gradient - c) \/ hessian\n\ndef summing_over_beta(i,phi,beta_row,V,M):\n    for j in range(V):\n        betaSum = 0\n        for d in range(M):\n            idxs = np.where(corpus[d] == j)[0]\n            if len(idxs) > 0:\n                betaSum += np.sum(phi[d][:, i][idxs])\n        beta_row[j] = betaSum\n    return beta_row,i\n\ndef maximizationStep(corpus, V, alpha, beta, phi, K, gamma):\n    M = len(corpus)\n    alpha = alpha - hessian_inverse_gradient(alpha, M, K, gamma)\n    with concurrent.futures.ProcessPoolExecutor(K) as executor:\n        futures = [executor.submit(summing_over_beta,i,phi,beta[i],V,M) for i in range(K)]\n        for f in concurrent.futures.as_completed(futures):    \n            val,i = f.result()\n            beta[i] = val\n            \n    return alpha, beta\/beta.sum(axis=1, keepdims=True)\n","4427f44a":"def initAlpha(K,scale=a_scale):\n    return np.ones((K))*scale\n\ndef initBeta(K,V,scale=b_scale):\n    return np.random.dirichlet(np.ones(V)*scale,K)\n\ndef expectationMaximization(corpus, V,alpha,beta):\n    #alpha, beta =initAlphaBeta(V, K)\n    phi = None\n    gamma = []\n    phiPrev = None\n    gammaPrev = None\n\n    for i in range(EM_ITERATIONS):\n        print(\"EM iteration:\", i)\n        phi = []\n        gamma = [None for _ in range(len(corpus))]\n        # E: VI\n        timeTaken = timeit.default_timer()\n        for idx, doc in enumerate(corpus):\n            N = len(doc)\n            phiDoc, gammaDoc = VI(alpha, beta, N, doc, K, VI_ITERATIONS, gamma[idx])\n            phi.append(phiDoc)\n            gamma[idx] = gammaDoc\n\n        print(\"Time taken E:\", timeit.default_timer() - timeTaken)\n        timeTaken = timeit.default_timer()\n        alpha, beta = maximizationStep(corpus, V, alpha, beta, phi, K, gamma)\n        print(\"Time taken M:\", timeit.default_timer() - timeTaken)\n    return alpha,beta,phi,gamma\n\ndef estimateParams(vocab, corpus):\n    V = len(vocab)\n    alpha=initAlpha(K)\n    beta=initBeta(K,V)\n    a, b, phi, gamma = expectationMaximization(corpus, V,alpha,beta)\n    return a, b, phi, gamma\n        \n# only estimate params if this is false, otherwise load old params\n\nif LOAD_PARAMS:\n    print(\"Loading parameters!\")\n    alpha = np.load(model_path+\"\/a.npy\")\n    beta = np.load(model_path+\"\/b.npy\")\n    phi = np.load(model_path+\"\/phi.npy\", allow_pickle=True)\n    gamma = np.load(model_path+\"\/gamma.npy\")\n    K = beta.shape[0]\n    if train:\n        alpha,beta,phi,gamma = expectationMaximization(corpus, V,alpha,beta)\n        np.save(\"a\", alpha)\n        np.save(\"b\", beta)\n        np.save(\"phi\", phi)\n        np.save(\"gamma\", gamma)\n        dictionary.save(\"dictionary\")\nelse:\n    print(\"Estimating parameters!\")\n    alpha, beta, phi, gamma = estimateParams(dictionary, corpus)\n    np.save(\"a\", alpha)\n    np.save(\"b\",beta)\n    np.save(\"phi\", phi)\n    np.save(\"gamma\", gamma)","53cd20e5":"from termcolor import colored\ncolors = ['red','green','blue','yellow','magenta','cyan','brown','grey','yellow','olive']\navg_N = df['BOW'].apply(len).mean()\n#1. Choose N\nN = np.random.poisson(int(avg_N))\ntheta = np.random.dirichlet(alpha)\nZ = np.random.choice(K,size=N,p=theta)\nphis = np.array([np.random.dirichlet(phi) for phi in beta[Z]])\nwords = np.array([np.random.choice(V,size=1,p=phi)[0] for phi in phis])\nprint(' '.join([colored(\"Topic\"+ str(i),c) for i,c in enumerate(colors[0:K])]))\nprint()\nprint(' '.join([colored(dictionary[w],colors[np.argmax(beta[:,w])]) for i,w in enumerate(words)]))\n","4668b95a":"from math import floor\ntop_colors = ['Reds','Greens','Blues','Oranges','Purples', 'GnBu','copper','Greys','Wistia','summer']\n\ndef display_word_weighting(beta,dictionary,words_to_display=10,colors=top_colors):\n    topic_amt = beta.shape[0]\n    word_df = pd.DataFrame()\n    fig,ax = plt.subplots(int(topic_amt\/2),2,figsize=(30,30))\n    for k in range(topic_amt):\n        phi = np.random.dirichlet(beta[k])\n        indices = beta[k].argsort()[-words_to_display:][::-1]\n        words = []\n        word_dist = []\n        for i in indices:\n            words.append(dictionary[i])\n            word_dist.append(beta[k][i])\n        word_df.insert(k,\"Topic \"+str(k),words)\n        d = {w: f for w, f in\n         zip([v for i,v in dictionary.items()],beta[k])}\n        wordcloud = WordCloud(background_color='black', colormap=colors[k], prefer_horizontal=1)\n        wordcloud.generate_from_frequencies(frequencies=d)\n        ax[floor(k\/2)][k%2].imshow(wordcloud)\n        ax[floor(k\/2)][k%2].axis('off')\n        ax[floor(k\/2)][k%2].set_title(\"Word Cloud for Topic: \" + str(k))\n    display(word_df)\n    plt.show()\n\n","bacd2101":"display_word_weighting(beta,dictionary)","c2ae8cca":"topics = ['Astronomy', 'Computer Science', 'Magnetism', 'Mathematics', 'Neural Networks', 'Thermodynamics']\n#topics = [x for x in range(K)]","c14eab52":"#source : https:\/\/stackoverflow.com\/questions\/12301071\/multidimensional-confidence-intervals\nfrom matplotlib.patches import Ellipse\n\ndef plot_point_cov(points, nstd=2, ax=None, **kwargs):\n    pos = points.mean(axis=0)\n    cov = np.cov(points, rowvar=False)\n    return plot_cov_ellipse(cov, pos, nstd, ax, **kwargs)\n\ndef plot_cov_ellipse(cov, pos, nstd=2, ax=None, **kwargs):\n    def eigsorted(cov):\n        vals, vecs = np.linalg.eigh(cov)\n        order = vals.argsort()[::-1]\n        return vals[order], vecs[:,order]\n\n    if ax is None:\n        ax = plt.gca()\n\n    vals, vecs = eigsorted(cov)\n    theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n\n    # Width and height are \"full\" widths, not radius\n    width, height = 2 * nstd * np.sqrt(vals)\n    ellip = Ellipse(xy=pos, width=width, height=height, angle=theta, alpha=0.3,**kwargs)\n\n    ax.add_artist(ellip)\n    return ellip\n","722ec150":"from sklearn.manifold import TSNE\nimport matplotlib.patches as mpatches\nfrom matplotlib.pyplot import figure\nimport matplotlib.transforms as transforms\nfrom scipy.spatial import ConvexHull\nfrom scipy import interpolate\ndef run_cluster_plot(corpus,confidence_level=0.23,plot_ellipse=True):\n    colors = ['red','green','blue','orange','purple','cyan','brown','grey','yellow','olive']\n    doc_topics = np.ones((len(corpus)))*-1\n    encoded_documents = np.zeros((len(corpus),len(dictionary)))\n    indices = []\n    for i,d in enumerate(corpus):\n        try:\n            s = np.sum(beta[:,d],axis=1)\n            doc_topics[i] = np.argmax(s)\n            if np.max(s) > confidence_level:\n                indices.append(i)\n        except:\n            continue\n    corpus = np.array(corpus)\n    for v in range(len(dictionary)):\n        encoded_documents[:,v] = np.array([(corpus[d] == v).sum() for d in range(len(corpus))])\n    print(\"Number of high confidence documents:\",len(indices))\n\n    doc_colors = [colors[int(x)] for i,x in enumerate(doc_topics) if i in indices] \n\n    tsne = TSNE(n_components=2,perplexity=80,n_iter=2000,early_exaggeration=20,n_iter_without_progress=1000)\n    transformed = tsne.fit_transform(encoded_documents[indices])\n    fig, ax = plt.subplots(1, figsize=(10, 10))\n    legends = []\n    for i in range(K):\n        topic_indices = np.where(np.array(doc_colors)==colors[i])[0]\n        if plot_ellipse:\n            plot_point_cov(transformed[topic_indices], ax=ax, nstd=2, facecolor=colors[i], edgecolor=colors[i])\n        legends.append(mpatches.Patch(color=colors[i], label=topics[i]))    \n\n    ax.scatter(transformed[:, 0], transformed[:, 1], color=doc_colors)\n    plt.legend(handles=legends)\n    plt.show()","734aafee":"run_cluster_plot(corpus)","f52dc271":"doc_topic_prob = np.zeros((len(corpus),2))\ndocuments_to_display = 5\npd.set_option('display.max_colwidth', None)\n\nfor i,d in enumerate(corpus):\n    try:\n        s = np.sum(beta[:,d],axis=1)\n        doc_topic_prob[i,1] = np.argmax(s)\n        doc_topic_prob[i,0] = np.max(s)\n    except:\n        doc_topic_prob[i,1] = -1\n        doc_topic_prob[i,0] = 0\nfor k in range(6):\n    doc_list = []\n    indices = doc_topic_prob[doc_topic_prob[:,1]==k][:,0].argsort()[-documents_to_display:][::-1]\n    doc_list = df.iloc[indices]['TITLE']\n    init_df = {'Topic':[topics[int(i)] for i in doc_topic_prob[doc_topic_prob[:,1]==k][:,1][indices]],\n            'Probability':doc_topic_prob[doc_topic_prob[:,1]==k][:,0][indices],\n            'Title':doc_list\n           }\n    df_k = pd.DataFrame(init_df)\n    display(df_k)\n","acd430e7":"\nidx = np.random.choice(range(len(corpus)))\ndoc = dictionary.doc2idx(df['BOW'][idx])\nprint(' '.join([colored(topics[i],c) for i,c in enumerate(colors[0:K])]))\nprint()\nprint(' '.join([colored(dictionary[w],colors[np.argmax(beta[:,w])]) if w>=0 else colored(df['BOW'][idx][i],'grey') for i,w in enumerate(doc)]))\n","a08cf04a":"df = pd.DataFrame(columns=data.columns)\nfor i,c in enumerate(classes):\n    df = df.append(data.loc[data[c]==1][:REDUTION_SIZE]) #& ~df['ID'].isin(new_df['ID'])])\ndf = df.drop_duplicates(subset='ID')\n\nclass_count = []\nword_count = []\nfor c in classes:\n    class_count.append(df[c].value_counts()[[1]][1])\n    class_abstracts = df.where(df[c]==1)[['ABSTRACT']]\n    word_count.append(pd.Series(class_abstracts.values.flatten()).str.len().sum())\n\nf, (ax1,ax2) = plt.subplots(2,1,figsize=(30,10))\nax1.bar(classes,class_count,align='center', alpha=0.9)\nax1.set_ylabel('Number of Documents')\nax2.bar(classes,word_count,align='center', alpha=0.5)\nax2.set_ylabel('Number of Words')\nax1.set_title('# of Research documents per Category')\nax2.set_title('# of Words per Category')\nplt.show()\n\nprint(\"Amount of documents:\", df.shape[0])","c60a75d0":"beta10 = np.load(model_path+\"\/b_k10.npy\")\ndictionary10 = dictionary.load(model_path+'\/dictionary_k10')\ndisplay_word_weighting(beta10,dictionary10)","3be24261":"While the LDA model is a generative model, it doesn't really generate any useful documents (especially considering we removed stopwords and added lemmatization). The true power of the model lies in its topic modelling capabilities as it can identify underlying topics in the documents.\n\n","dd452996":" # Topic Modelling using LDA","076998bc":"**Maximization (Step 2):**\n\nThe Maximization step sets out to **maximize the lower bound** with respects to the Dirichlet parameters $\\alpha$ and $\\beta$ through ML-estimates.\n\n$\\beta_{i j} \\propto \\sum_{d=1}^{M} \\sum_{n=1}^{N_{d}} \\phi_{d n i}^{*} w_{d n^{*}}^{j}$\n\n$\\alpha_{\\text {new }}=\\alpha_{\\text {old }}-\\frac{g_{i}-c}{h_{i}}$\n\n\nThe parameter $\\alpha$ is maximized through a Newton-Rhapson method which makes use of the Hessian matrix. The $\\alpha$ update step is shown above. \n\nwhere\n\n$c=\\frac{\\sum_{j=1}^{k} g_{j} \/ h_{j}}{z^{-1}+\\sum_{j=1}^{k} h_{j}^{-1}}$,\n\n$z=M \\Psi^{\\prime}\\left(\\sum_{k} \\alpha_{k}\\right)$\n\n$g$ and $h$ are the gradient and Hessian matrix of the old $\\alpha$ and $\\Psi^{\\prime}$ is the trigamma function.\n\nBy using derivations presented by [Minka][1] in conjunction with those presented by [Blei][2] in the original paper, we can derive the following hessian and gradient equations:\n\n$g_{k}=M \\Psi\\left(\\sum_{k} \\alpha_{k}\\right)-M \\Psi\\left(\\alpha_{k}\\right)+M \\log \\bar{\\theta}_{k}$\n\n$h_k = -M \\Psi^{\\prime}\\left(\\alpha_{k}\\right)$\n\nwhere $\\bar{\\theta}$ is the average proportionality of topic $k$ over all sampled documents from $Dir(\\alpha)$. In other words, it's the expected value of the Dirichlet distribution for a specific topic $k$,\n\n$ \\log \\bar{\\theta}_{k}=\\frac{1}{M} \\sum_{i} \\log \\theta_{i k}$\n\n$\\log \\left({\\theta}_{k}\\right)=E\\left[\\log \\left(\\theta_{k}\\right)\\right]=\\Psi\\left(\\gamma_{k}\\right)-\\Psi\\left(\\sum_{i}^{M} \\gamma_{i}\\right)$\n\nBy plugging in all the values into the update functions, we can now calculate the the M-step.\n\nHere is another useful source for the M-step update by [J. Huang][3]\n\n[1]: [https:\/\/tminka.github.io\/papers\/dirichlet\/minka-dirichlet.pdf]\n[2]: [https:\/\/www.jmlr.org\/papers\/volume3\/blei03a\/blei03a.pdf]\n[3]: [http:\/\/jonathan-huang.org\/research\/dirichlet\/dirichlet.pdf]","8ab813af":"### Analysing and processing the data\n\nLet's start off by briefly analysing the data and the classes available to us.","31f97c5e":"#### Clustering on a balanced dataset\n\nLet's explore how things change when we run the model on a smaller and more balanced dataset. For the previous experiment, we found that two topics of the ground truth, finance and biology, were lost in the LDA model. For this experiment, we set **$K=10$**","af7ee33b":"## Testing and Results\n\n### Results\n\n\n### Generative Powers\n\nThe model assumes that each word is generated by a random topic where the topic has been chosen from a topic distribution. \n\nThe process of generating document using the model essentially goes like this:\n\n1. Choose length of the document (**$N$**).\n2. From **$Dir(\\alpha)$**, sample **$\\theta$**.\n3. Do $N$ times: \n    3. From **$\\theta$**, sample a topic **$z_n$**\n    4. From $Dir(\\beta_{z_n})$, sample a word **$w_n$.**\n    ","f8fe381c":"We can clearly see that the model manages to capture 6 distinct topics that make sense in the context of research papers. Topic 0 could be labeled as **Astronomy** as it is characterized by words such as star, mass, galaxy, ray and planets. Topic 1 could both fall under the topic of **Computer Science**, characterized by words such as algorithm, function, parameter and optimization. Topic 4 is fairly similar to topic 1, though the words are more centered around **Neural Networks**. Topic 2 is characterized by words such as state, spin, phase, field and magnetic, thus it would be appropriate to give it the label **Magnetism**. Topic 3 is centered around **Mathematics** as it is characterized with words such as graph, function and mathematical latex notations. However, some of the highly characterizing words are terms that mainly fall into the field of group theory and discrete mathematics (group, space, set) which is something to take note of. Finally, topic 5 is characterized by words relating to **Thermodynamics** such as energy, flow, particle, dynamic and wave with some mathematical terms such as solution and equation mixed in the distribution aswell.\n\nThere are some obvious discrepencies when comparing the LDA topics to the ground thruth. For one, the finance and biology topics have been completely dropped by the LDA model, most likely due to the fact that there were very few papers about finance and biology in the dataset. When running these tests with highly reduced dataset, the model did in fact find underlying topics characterized by financial and biological terms (we'll discuss this more later). Instead of the finance and biology topics, the model seems to have found several subfields of physics (magnetism, thermodynamics and astronomy) instead of only having one physics topic like there was in the ground truth. The model has also found two topics relating to computer science, one being the 'Computer Science' topic and the other being the 'Neural Network' topic. Mathematics seems to be the only topic where the LDA model agrees with the ground truth, though the LDA mathematics topic is fairly focused on discrete mathematics.\n\nWe can further illustrate how the topics are related by classifying all the documents in the dataset and vizualizing them on a 2D plane through [t-SNE][1]. By analyzing the clusters, we can find out if and how the topics overlap and which topics are related to eachother.\n\n[1]:[https:\/\/www.jmlr.org\/papers\/volume9\/vandermaaten08a\/vandermaaten08a.pdf]","0416ad36":"While most of the topics are well seperated, the mathematics cluster seems to overlap with several other topics. This reflects reality in a sense, as the fields of thermodynamics, magnetisism and computer science make heavy use of maths. The plot also shows that the computer science and neural network fields are fairly similar and that the astronomy topic is fairly independent, only being a little bit tied to magnetism. Finally, the model is not very confident when labeling documents by thermodynamics, showcased by the few amount of data points for said topic.\n\nFor a more qualitative idea on how documents look like for each generated topic, we could **list the documents that had the highest probability for each respective topic.**","2f9b2d44":"We know understand that documents are a mixture of topics, there are no cases where a topic makes up more than 50% of a document and in most cases, a singular topic does not even make up 23% of a topic as shown with in the t-SNE test. To further hit home this idea, one can shwocase the topic mixture for a document in the dataset by highlighting words corresponding to each topic.","d863b9b9":"The topics for this models are fairly similar to earlier with the exceptions of topic 1 and 2 representing finance and biology respectively. This shows that LDA can struggle identifying underrepresented topics in a large collection of documents.","9e0467bf":"The documents are classified by summing the probability of a word given a topic for each word in the document. The topic with the highest cumulative probability is declared the winner. \n\nThe process of classifying a document:\n\n$\\max_{k\\in K}P(d|z_k) = \\max_{k\\in K} \\sum^{N_d}_{n}P(w_n|z_k) = \\max_{k\\in K}\\sum^{N_d}_{n}\\beta_{z_k,w_n}$\n\nWe decide to only plot a data point if the probability $P(d|z_k)$ for the majority topic exceeds 23%, this way only the high confidence predictions are showcased. This gives us an idea of how the model seperates the topics from each other.  To further illustrate the the grouping, confidence ellipses were plotted surrounding each cluster.","5da75cfc":"The barplot and the word cloud visualize the most frequent non-stopwords throughout the dataset. The most frequent words are definitely relevant to the research classes shown earlier, meaning we can move on to the next step.\n\nNow it's time to create the dictionary using the bags of words. **The more words** included in the dictionary, **the longer training will take** for the LDA model to learn. For that reason, we should filted out words that are not especially relevant for many documents. This is done by **filtering out low frequency words**. Furthermore, words that are too frequent might also need to be filtered out as they are most likely relevant for all underlying topics, similarly to stopwords. By adding the filter, the amount of dictionary words were **reduced from 49380 down to 5310**. Reducing the size of the vocabulary speeds up the training process tremendously.","f8b9a416":"## What is Topic Modeling\n\nTopic modeling is the idea that a collection of text can be represented by latent topics. Representing documents as topics can be useful for text-mining, feature reduction, clustering and document generation. Running a 'Topic Model' on data returns a collection of topics, where each topic is an unlabeled cluster of words that characterize a set of documents. Topic modeling is an unsupervised approach, meaning the input data does not have to be labeled.\n\n\nMore about topic models: https:\/\/mimno.infosci.cornell.edu\/papers\/2017_fntir_tm_applications.pdf","b29ed841":"Listing the most popular documents for each topic and the confidence in the prediction from the model helps us understand the clustering a bit better. The first thing that stands out is that both mathematics and esepecially thermodynamics have low probabilities. A low probability essentially means that the model is not confident in labeling a document to a singular class. This was reflected in the t-SNE cluster plot earlier where the two clusters were somewhat intertwined. Interestingly enough, many machine learning and neural network terms appear in the titles of documents classified as non-neural network topics, especially for thermodynamics, mathematics and computer science. This also mirrors the t-SNE plot where those three clusters collided.","946c99e6":"The bar charts shows the distribution of classes over the documents and how many words all documents from a class has. It becomes clear that the groud truth of the data is quite unbalanced as the quantitative biology and finance classes are lacking in both words and documents compared to the rest of the classes. As is usually the case with LDA, the amount of latent topics that might fit best for inference might not necessarily reflect the ground truth. We will look into how the model performs on a balanced and unbalanced dataset when it comes to topic modeling. For some of the experiments we are going to run the code on a reduced dataset as the computation time scales heavily with feature size. Another important note is that some documents are a mixture of ground true topics.","cde06e88":"## Latent Dirichlet Allocation\n\n### An overview\n\nLDA, or 'Latent Dirichlet Allocation' is a **probabilistic generative model** often used in the context of text. It's a 3-stage **hierarchial bayesian model**, meaning the parameters of the posterior distribution can be estimated through **Bayesian inference**.\n\n\n![ldaModel.PNG](attachment:0a7ff8e5-45d8-464a-ba59-06953a69411f.PNG).\n\nThe figure above is the **graphical model representation of LDA**. The innermost plate (N) represents words and the outer plate (M) represents documents.\n\n+ $k$ - A predefined variable that decides the amount of latent topics.\n\n+ $M$ - The amount of documents in the collection.\n+ $N$ - The amount of words for a document.\n+ $V$ - Vocabulary size.\n\n\n+ **$\\alpha$** - The **Dirichlet prior ($Dir(\\alpha)$**) is parametrized by a vector $\\alpha$ of length $k$. **$\\alpha$** is known as the **concentration hyperparameter**.\n\n+ $\\theta_m$ - The **topic distribution** over $k$ topics for a document $m$. Sampled from $Dir(\\alpha)$\n\n+ **$z_{m,n}$** - a sampled topic from the multinomial topic distribution $\\theta_m$ for the  $n$:th word in a specific document $m$.\n\n+ **$w_{m,n}$** - The specific word in position $n$ in document $m$\n\n+ $\\beta$ - Parametrizes the **word distribution for each topic**. $\\beta$ is a $k \\times V$ matrix where each row of $\\beta$ parametrizes a **Dirichlet prior ($Dir(\\beta_k)$)**  modeling the word distribution **$\\phi_i$** over a topic $k$. In other words, an element $\\beta_{k,n} = P(w_{n}|z_k)$\n\n\n\nSource:\nhttps:\/\/ai.stanford.edu\/~ang\/papers\/jair03-lda.pdf","28f54d78":"### Training the Model\n\nThe training of the model is done through a **variational expectation-maximization** (EM) procedure. It's an iterative method where parameters of the statistical model are approximated. In this case, the focus is to maximize the family of lower bounds in the variational distribution of the model. We are looking to find the parameters that maximize the marginal log likelihood of the data, as shown below.\n\n$\\ell(\\alpha, \\beta)=\\sum_{d=1}^{M} \\log p\\left(\\mathbf{w}_{d} \\mid \\alpha, \\beta\\right)$\n\nEach iteration of the EM procedure is made up of two steps, an expectation (E) step and a maximization (M) step. The E step will provide a lower bound on the log likelihood through variational inference which we will then maximize in the M step. The conditional $p(w_d|\\alpha,\\beta)$ is intractable which is why we use variational inference to find a tractable lower bound which is can then be easily maximized.\n\n**Expectation (Step 1):**\n\nThe variational distribution is presented in [the original LDA paper][1] both graphically and as an equation as shown below,\n\n![image.png](attachment:0a17ed86-0fe6-43c2-93d3-24854c94e1ee.png)\n\n$q(\\theta, \\mathbf{z} \\mid \\gamma, \\phi)=q(\\theta \\mid \\gamma) \\prod_{n=1}^{N} q\\left(z_{n} \\mid \\phi_{n}\\right)$\n\nwhere $\\gamma$ is a Dirichlet parameter and each $\\phi_n$ is a free variational parameter.\n\nThe expectation step aims to optimize the values of the variational parameters for each document by minimizing the KL-divergence between the presented variational distribution and the true joint posterior of $\\theta$ and $z$. I'm going to present the update functions without going through the trouble of deriving them. If you're interested in how they were derived, take a look in the original LDA paper.\n\n$\\phi_{n i} \\propto \\beta_{i w_{n}} \\exp {\\Psi\\left(\\gamma_{i}\\right)-\\Psi\\left(\\sum_{j=1}^{k} \\gamma_{j}\\right)}$\n\n$\\gamma_{i}=\\alpha_{i}+\\sum_{n=1}^{N} \\phi_{n i}$\n\n\nnote that $\\Psi$ is the digamma function.\n\n\n\n\n\n\n[1]: https:\/\/ai.stanford.edu\/~ang\/papers\/jair03-lda.pdf","bd06cf56":"The dictionary is needed to **map word to indices** since we need numerical values when training.","68428561":"## Loading Data and Building a Dictionary\n\nThe documents used for the LDA model are represented by abstracts from various research papers provided from https:\/\/www.kaggle.com\/blessondensil294\/topic-modeling-for-research-articles. \n\nEach paper belongs to one of six different research fields.\n\nStep one of building the model is to build a word dictionary with relevant words for the underlying topics of the documents. We'll also transform the text data into a format more suitable as input for the model in the form of bags of words.","72c8f375":"**Now let's run the model!**","fc3a52ae":"### Clustering on the full dataset\n\nLets explore the clustering performance of the method. By analyzing the $\\beta$ matrix, we can identify the word distribution of each topic and analyze how the model classifies different documents into underlying topics. To find the words that characterize each topic, we extract the words corresponding to the highest probability in the $\\beta$ matrix for a specified topic $k$. The results can be visualized as a list of words corresponding to each topic or as word clouds where the words that characterize a topic the most are shown in a larger font.\n\nNote, the clustering experiments were conducted with the **number of topics $K = 6$ on the full dataset** in order to compare to the ground truth labels of the collection.","89dd7e22":"The text has to be transformed into a format that is suitable as input to the model. For this problem we choose the **Bag of Words** format where each term and it's respective term frequency is represented as a vector.\n\nThe text for each document is **tokenized** using the ```simple_preprocess``` function from the gensim library which also lowercases and deaccents each individual word. Tokens are much easier to work with compared to a long string of text. It's important that the words used for the model have meaning to the context of the topics. Therefore, words that are common among all documents are filtered out using the **stopwords** list provided by the nltk corpus. Examples of stopwords are 'it, this, or, and'. Finally, a regex is applied to remove any unessecary special characters."}}