{"cell_type":{"28f41cbd":"code","13140465":"code","f7815eb2":"code","c619be7a":"code","1b9ca3ce":"code","70def6d9":"code","c8f5c9c7":"code","1a76473f":"code","8d2c0ebb":"code","67e760fb":"code","759cd129":"code","2abb257c":"code","fac3cab7":"code","f5999f4e":"code","938b73ce":"code","411b1a32":"code","436e5d2a":"code","76c6e2fc":"code","cc53453e":"code","627751ea":"code","eab552cc":"code","0476d299":"code","c13b16fe":"code","f677676c":"code","a61a16b4":"code","d80ce8bd":"code","15642321":"code","f19e4d87":"code","51dbad1e":"code","09aa73fa":"markdown","b306a580":"markdown","35269380":"markdown","9814c61e":"markdown","daf869c7":"markdown","4e382785":"markdown","eb846463":"markdown","963491d7":"markdown"},"source":{"28f41cbd":"import numpy as np # linear algebra\nnp.random.seed(42)\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\ndataset = pd.read_csv(\"..\/input\/train.csv\")\n\ndataset.head()","13140465":"dataset.info()","f7815eb2":"dataset.describe()","c619be7a":"dataset.corr()","1b9ca3ce":"import seaborn as sns\nimport matplotlib.pyplot as plt","70def6d9":"sns.heatmap(dataset.corr(),annot=True)\nplt.show()","c8f5c9c7":"Sex_pct = pd.crosstab(\n    dataset['Sex'].astype('category'),\n    dataset['Survived'].astype('category'),\n    margins=True,\n#     normalize=True\n)\nSex_pct","1a76473f":"sns.barplot('Sex','Survived',data=dataset)\nplt.show()","8d2c0ebb":"dataset.Sex.value_counts()","67e760fb":"sns.countplot(dataset.Age.value_counts())\nplt.show()","759cd129":"sns.pairplot(dataset)\nplt.show()","2abb257c":"sns.countplot(dataset.Sex.value_counts())\nplt.show()","fac3cab7":"dataset.isnull().sum(axis=0)","f5999f4e":"dataset.dropna(inplace=True)","938b73ce":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndataset.Embarked = le.fit_transform(dataset.Embarked)\ndataset.Sex = le.fit_transform(dataset.Sex)\ndataset.head()","411b1a32":"X,y = dataset[['Pclass','Sex','Age','Embarked']],dataset['Survived']\n","436e5d2a":"X.head()","76c6e2fc":"y.head()","cc53453e":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\ntree_clf = DecisionTreeClassifier(random_state=42)\ntree_clf.fit(X_train, y_train)","627751ea":"from sklearn.metrics import accuracy_score\ny_pred = tree_clf.predict(X_test)\nprint(tree_clf.__class__.__name__, accuracy_score(y_test, y_pred))\nprint(f'Classification Report for {tree_clf.__class__.__name__}')\nprint(classification_report(y_test, y_pred))\nprint('*'*60)","eab552cc":"bag_clf = BaggingClassifier(DecisionTreeClassifier(random_state=42), \n                            n_estimators=500,\n                            bootstrap=True, n_jobs=-1)\nbag_clf.fit(X_train, y_train)","0476d299":"y_pred = bag_clf.predict(X_test)\nprint(bag_clf.__class__.__name__, accuracy_score(y_test, y_pred))\nprint(f'Classification Report for {bag_clf.__class__.__name__}')\nprint(classification_report(y_test, y_pred))\nprint('*'*60)","c13b16fe":"from sklearn.ensemble import RandomForestClassifier\n\nrnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\nrnd_clf.fit(X_train, y_train)","f677676c":"y_pred_rf = rnd_clf.predict(X_test)\n\naccuracy_score(y_test, y_pred_rf)","a61a16b4":"bag_clf = BaggingClassifier(\n    DecisionTreeClassifier(splitter='random', random_state=42),\n    n_estimators=500, n_jobs=-1\n)\nbag_clf.fit(X_train, y_train)","d80ce8bd":"y_pred = bag_clf.predict(X_test)\n\naccuracy_score(y_test, y_pred)","15642321":"np.sum(y_pred == y_pred_rf) \/ len(y_pred)  # almost identical predictions","f19e4d87":"output = pd.DataFrame(X_test)\noutput['y_pred'] = y_pred","51dbad1e":"output.head()","09aa73fa":"Find missing value and remove it","b306a580":"Make data normal or in numeric so model can learn better","35269380":"random forest and decision tree with bagging classifier predicition simmilarity","9814c61e":"![](http:\/\/)Bagging claasifier with DecisionTreeClassifier will improve the accuracy","daf869c7":"lets visualize correlation","4e382785":"The Titanic\nUsing information about passengers of the Titanic, we are interested in building a model based on a DecisionTreeClassifier and RandomForestClassifier to say something about the chances of surviving the disaster.\n\nThe training data provided contains 891 records with the following attributes:\n\nSurvived: 0= No; 1 = Yes\n\nPclass: Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n\nName: Passenger name\n\nSex: (female; male)\n\nAge: Passenger age\n\nSibSp: Number of Siblings\/Spouses Aboard\n\nParch: Number of Parents\/Children Aboard\n\nTicket:Ticket Number\n\nFare: Passenger Fare\n\nCabin: Cabin\n\nEmbarked: Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)","eb846463":"You are able to notice that random forest and decision tree with bagging classifier will give somewhere same accuracy.","963491d7":"****Random Forests****"}}