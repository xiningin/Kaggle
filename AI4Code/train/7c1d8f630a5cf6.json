{"cell_type":{"f3cf23c3":"code","860d8319":"code","23d8f23b":"code","2c860797":"code","94413b45":"code","84957f21":"code","de63013e":"code","bdb2aa30":"code","a1a90f73":"code","3305a245":"code","181969cd":"code","9b23a99f":"code","2f33abe5":"code","361342cd":"code","7d60e8a4":"code","88a8886f":"code","fbe05f02":"code","2529db17":"code","e9eda706":"code","5c86e645":"code","b899423d":"code","e27e9e42":"code","8c0daba2":"code","5bf8fd6a":"code","6f93a313":"code","1ac3e22f":"code","4c589f9a":"code","b2c5aa09":"code","cd0d3638":"code","b73c3176":"code","91419fa9":"code","fd7cfcbe":"code","f399cdf2":"code","ee0bc700":"code","94777aec":"code","48a9291b":"code","f9bebe55":"code","025a4c13":"code","c29141c6":"code","2391cf5d":"code","57d340dc":"code","07bfacb7":"code","07284221":"code","cfa6f3b3":"code","57ac137e":"code","0860f768":"code","bdef9990":"code","342e9ef0":"code","f4336edf":"code","6429d851":"code","f2e71c4b":"code","ba672668":"code","da9c8693":"code","ee4e1b79":"code","4f6bc2b1":"code","d9a9c12b":"code","e2cfa59b":"code","17493d1d":"code","b7277a49":"code","b889fcdd":"code","bbcc17de":"code","971af430":"code","8b479a2f":"code","1ff66993":"code","2fa87484":"code","edda4d06":"code","367dcd5b":"code","c411ef52":"code","50b17ee1":"code","50db7845":"code","2688fd91":"code","35dcb708":"code","76ea55d4":"code","09e2c590":"code","b448e7de":"code","7db098d8":"code","125afabe":"code","e39dc76f":"code","72383fd4":"code","b85362d9":"code","68695dab":"code","e32234de":"markdown","d57767e9":"markdown","55b59b63":"markdown","b2b44bb2":"markdown","f463de36":"markdown","e3ff487c":"markdown","44733237":"markdown","96745f83":"markdown","544d3b76":"markdown","2c926154":"markdown","6a62064e":"markdown","101b13de":"markdown","6b7c4266":"markdown","82e00c1c":"markdown","844aba6c":"markdown","44b1e696":"markdown","f7143e7c":"markdown","09309680":"markdown","0739f6c9":"markdown"},"source":{"f3cf23c3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","860d8319":"# An\u00e1lise e manipula\u00e7\u00e3o de dados\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# Visualiza\u00e7\u00e3o\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport missingno\n%matplotlib inline\n\n# Machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.model_selection import RandomizedSearchCV \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nimport xgboost as xgb\nfrom sklearn.ensemble import VotingClassifier","23d8f23b":"treino_dados = pd.read_csv('..\/input\/titanic\/train.csv')\nteste_dados = pd.read_csv('..\/input\/titanic\/test.csv')\ncombine = [treino_dados, teste_dados]","2c860797":"treino_dados.head()","94413b45":"treino_dados.describe()","84957f21":"treino_dados.info()","de63013e":"print(treino_dados.columns.values)","bdb2aa30":"treino_dados.describe(include=['O'])","a1a90f73":"teste_dados.head()","3305a245":"teste_dados.describe()","181969cd":"dados_numericos = treino_dados[['Age', 'SibSp', 'Parch', 'Fare']]\ndados_categoricos = treino_dados[['Survived', 'Pclass', 'Sex', 'Ticket', 'Cabin', 'Embarked']]","9b23a99f":"missingno.matrix(treino_dados, figsize = (30,10))","2f33abe5":"for i in dados_numericos.columns:\n    plt.hist(dados_numericos[i])\n    plt.title(i)\n    plt.show()","361342cd":"print(dados_numericos.corr())\nsns.heatmap(dados_numericos.corr())","7d60e8a4":"for i in dados_categoricos.columns:\n    sns.barplot(dados_categoricos[i].value_counts().index,dados_categoricos[i].value_counts()).set_title(i)\n    plt.show()","88a8886f":"treino_dados[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","fbe05f02":"treino_dados[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","2529db17":"treino_dados[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","e9eda706":"treino_dados[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","5c86e645":"g = sns.FacetGrid(treino_dados, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","b899423d":"grid = sns.FacetGrid(treino_dados, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","e27e9e42":"grid = sns.FacetGrid(treino_dados, row='Embarked', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","8c0daba2":"grid = sns.FacetGrid(treino_dados, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","5bf8fd6a":"treino_dados['cabin_multiple'] = treino_dados.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\nteste_dados['cabin_multiple'] = teste_dados.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n\ntreino_dados['cabin_adv'] = treino_dados.Cabin.apply(lambda x: str(x)[0])\nteste_dados['cabin_adv'] = teste_dados.Cabin.apply(lambda x: str(x)[0])","6f93a313":"treino_dados['numeric_ticket'] = treino_dados.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\nteste_dados['numeric_ticket'] = teste_dados.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\n\ntreino_dados['ticket_letters'] = treino_dados.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\nteste_dados['ticket_letters'] = teste_dados.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) >0 else 0)","1ac3e22f":"print(\"Before\", treino_dados.shape, teste_dados.shape, combine[0].shape, combine[1].shape)\n\ntreino_dados = treino_dados.drop(['Ticket', 'Cabin'], axis=1)\nteste_dados = teste_dados.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [treino_dados, teste_dados]\n\n\"After\", treino_dados.shape, teste_dados.shape, combine[0].shape, combine[1].shape","4c589f9a":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(treino_dados['Title'], treino_dados['Sex'])","b2c5aa09":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntreino_dados[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","cd0d3638":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntreino_dados.head()","b73c3176":"treino_dados = treino_dados.drop(['Name', 'PassengerId'], axis=1)\nteste_dados = teste_dados.drop(['Name'], axis=1)\ncombine = [treino_dados, teste_dados]\ntreino_dados.shape, teste_dados.shape","91419fa9":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntreino_dados.head()","fd7cfcbe":"grid = sns.FacetGrid(treino_dados, row='Pclass', col='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","f399cdf2":"guess_ages = np.zeros((2,3))\nguess_ages","ee0bc700":"for dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n            \n            age_guess = guess_df.median()\n            \n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntreino_dados.head()","94777aec":"treino_dados['AgeBand'] = pd.cut(treino_dados['Age'], 5)\ntreino_dados[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","48a9291b":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntreino_dados.head()","f9bebe55":"treino_dados = treino_dados.drop(['AgeBand'], axis=1)\ncombine = [treino_dados, teste_dados]\ntreino_dados.head()","025a4c13":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntreino_dados[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","c29141c6":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntreino_dados[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","2391cf5d":"treino_dados = treino_dados.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\nteste_dados = teste_dados.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [treino_dados, teste_dados]\n\ntreino_dados.head()","57d340dc":"for dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ntreino_dados.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)","07bfacb7":"freq_port = treino_dados.Embarked.dropna().mode()[0]\nfreq_port","07284221":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntreino_dados[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","cfa6f3b3":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntreino_dados.head()","57ac137e":"teste_dados['Fare'].fillna(teste_dados['Fare'].dropna().median(), inplace=True)\nteste_dados.head()","0860f768":"treino_dados['FareBand'] = pd.qcut(treino_dados['Fare'], 4)\ntreino_dados[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","bdef9990":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntreino_dados = treino_dados.drop(['FareBand'], axis=1)\ncombine = [treino_dados, teste_dados]\n    \ntreino_dados.head(10)","342e9ef0":"teste_dados.head(10)","f4336edf":"for dataset in combine:\n    dataset['cabin_adv'] = dataset['cabin_adv'].map( {'n': 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'T': 8} ).astype(int)\n\ntreino_dados.head()","6429d851":"for dataset in combine:\n    dataset['ticket_letters'] = pd.get_dummies(dataset[['ticket_letters']])\n    \ntreino_dados.head()","f2e71c4b":"X_train = treino_dados.drop(\"Survived\", axis=1)\nY_train = treino_dados[\"Survived\"]\nX_test  = teste_dados.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","ba672668":"gnb = GaussianNB()\ncv = cross_val_score(gnb,X_train,Y_train,cv=5)\nprint(cv)\nprint(cv.mean())","da9c8693":"lr = LogisticRegression(max_iter=2000)\ncv = cross_val_score(lr,X_train,Y_train,cv=5)\nprint(cv)\nprint(cv.mean())","ee4e1b79":"dt = DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt,X_train,Y_train,cv=5)\nprint(cv)\nprint(cv.mean())","4f6bc2b1":"knn = KNeighborsClassifier()\ncv = cross_val_score(knn,X_train,Y_train,cv=5)\nprint(cv)\nprint(cv.mean())","d9a9c12b":"rf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X_train,Y_train,cv=5)\nprint(cv)\nprint(cv.mean())","e2cfa59b":"svc = SVC(probability = True)\ncv = cross_val_score(svc,X_train,Y_train,cv=5)\nprint(cv)\nprint(cv.mean())","17493d1d":"xgb = XGBClassifier(random_state =1)\ncv = cross_val_score(xgb,X_train,Y_train,cv=5)\nprint(cv)\nprint(cv.mean())","b7277a49":"perceptron = Perceptron()\ncv = cross_val_score(perceptron, X_train, Y_train, cv=5)\nprint(cv)\nprint(cv.mean())","b889fcdd":"linear_svc = LinearSVC()\ncv = cross_val_score(linear_svc, X_train, Y_train, cv=5)\nprint(cv)\nprint(cv.mean())","bbcc17de":"sgd = SGDClassifier(loss='log')\ncv = cross_val_score(sgd, X_train, Y_train, cv=5)\nprint(cv)\nprint(cv.mean())","971af430":"voting_clf = VotingClassifier(estimators = [('lr',lr),('knn',knn),('rf',rf),('gnb',gnb),('svc',svc),('xgb',xgb),('sgd',sgd),('dt',dt)], voting = 'soft') ","8b479a2f":"cv = cross_val_score(voting_clf,X_train,Y_train,cv=5)\nprint(cv)\nprint(cv.mean())","1ff66993":"voting_clf.fit(X_train,Y_train)\ny_hat_base_vc = voting_clf.predict(X_test).astype(int)\nbasic_submission = {'PassengerId': teste_dados.PassengerId, 'Survived': y_hat_base_vc}\nbase_submission = pd.DataFrame(data=basic_submission)\nbase_submission.to_csv('base_submission.csv', index=False)","2fa87484":"def clf_performance(classifier, model_name):\n    print(model_name)\n    print('Best Score: ' + str(classifier.best_score_))\n    print('Best Parameters: ' + str(classifier.best_params_))","edda4d06":"lr = LogisticRegression()\nparam_grid = {'max_iter' : [2000],\n              'penalty' : ['l1', 'l2'],\n              'C' : np.logspace(-4, 4, 20),\n              'solver' : ['liblinear']}\n\nclf_lr = GridSearchCV(lr, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_lr = clf_lr.fit(X_train,Y_train)\nclf_performance(best_clf_lr,'Logistic Regression')","367dcd5b":"knn = KNeighborsClassifier()\nparam_grid = {'n_neighbors' : [3,5,7,9],\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree','kd_tree'],\n              'p' : [1,2]}\nclf_knn = GridSearchCV(knn, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_knn = clf_knn.fit(X_train,Y_train)\nclf_performance(best_clf_knn,'KNN')","c411ef52":"svc = SVC(probability = True)\nparam_grid = tuned_parameters = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10],\n                                  'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['poly'], 'degree' : [2,3,4,5], 'C': [.1, 1, 10, 100, 1000]}]\nclf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_svc = clf_svc.fit(X_train,Y_train)\nclf_performance(best_clf_svc,'SVC')","50b17ee1":"rf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [100,500,1000], \n                                  'bootstrap': [True,False],\n                                  'max_depth': [3,5,10,20,50,75,100,None],\n                                  'max_features': ['auto','sqrt'],\n                                  'min_samples_leaf': [1,2,4,10],\n                                  'min_samples_split': [2,5,10]}\n                                  \nclf_rf_rnd = RandomizedSearchCV(rf, param_distributions = param_grid, n_iter = 100, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_rf_rnd = clf_rf_rnd.fit(X_train,Y_train)\nclf_performance(best_clf_rf_rnd,'Random Forest')","50db7845":"rf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [400,450,500,550],\n               'criterion':['gini','entropy'],\n                                  'bootstrap': [True],\n                                  'max_depth': [15, 20, 25],\n                                  'max_features': ['auto','sqrt', 10],\n                                  'min_samples_leaf': [2,3],\n                                  'min_samples_split': [2,3]}\n                                  \nclf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_rf = clf_rf.fit(X_train,Y_train)\nclf_performance(best_clf_rf,'Random Forest')","2688fd91":"best_rf = best_clf_rf.best_estimator_.fit(X_train,Y_train)\nfeat_importances = pd.Series(best_rf.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","35dcb708":"xgb = XGBClassifier(random_state = 1)\n\nparam_grid = {\n    'n_estimators': [20, 50, 100, 250, 500,1000],\n    'colsample_bytree': [0.2, 0.5, 0.7, 0.8, 1],\n    'max_depth': [2, 5, 10, 15, 20, 25, None],\n    'reg_alpha': [0, 0.5, 1],\n    'reg_lambda': [1, 1.5, 2],\n    'subsample': [0.5,0.6,0.7, 0.8, 0.9],\n    'learning_rate':[.01,0.1,0.2,0.3,0.5, 0.7, 0.9],\n    'gamma':[0,.01,.1,1,10,100],\n    'min_child_weight':[0,.01,0.1,1,10,100],\n    'sampling_method': ['uniform', 'gradient_based']\n}\n\n\nclf_xgb_rnd = RandomizedSearchCV(xgb, param_distributions = param_grid, n_iter = 1000, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_xgb_rnd = clf_xgb_rnd.fit(X_train,Y_train)\nclf_performance(best_clf_xgb_rnd,'XGB')","76ea55d4":"xgb = XGBClassifier(random_state = 1)\n\nparam_grid = {\n    'n_estimators': [450,500,550],\n    'colsample_bytree': [0.75,0.8,0.85],\n    'max_depth': [None],\n    'reg_alpha': [1],\n    'reg_lambda': [2, 5, 10],\n    'subsample': [0.55, 0.6, .65],\n    'learning_rate':[0.5],\n    'gamma':[.5,1,2],\n    'min_child_weight':[0.01],\n    'sampling_method': ['uniform']\n}\n\nclf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_xgb = clf_xgb.fit(X_train,Y_train)\nclf_performance(best_clf_xgb,'XGB')","09e2c590":"y_hat_xgb = best_clf_xgb.best_estimator_.predict(X_test).astype(int)\nxgb_submission = {'PassengerId': teste_dados.PassengerId, 'Survived': y_hat_xgb}\nsubmission_xgb = pd.DataFrame(data=xgb_submission)\nsubmission_xgb.to_csv('xgb_submission3.csv', index=False)","b448e7de":"best_lr = best_clf_lr.best_estimator_\nbest_knn = best_clf_knn.best_estimator_\nbest_svc = best_clf_svc.best_estimator_\nbest_rf = best_clf_rf.best_estimator_\nbest_xgb = best_clf_xgb.best_estimator_\n\nvoting_clf_hard = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc)], voting = 'hard') \nvoting_clf_soft = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc)], voting = 'soft') \nvoting_clf_all = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc), ('lr', best_lr)], voting = 'soft') \nvoting_clf_xgb = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc), ('xgb', best_xgb),('lr', best_lr)], voting = 'soft')\n\nprint('voting_clf_hard :',cross_val_score(voting_clf_hard,X_train,Y_train,cv=5))\nprint('voting_clf_hard mean :',cross_val_score(voting_clf_hard,X_train,Y_train,cv=5).mean())\n\nprint('voting_clf_soft :',cross_val_score(voting_clf_soft,X_train,Y_train,cv=5))\nprint('voting_clf_soft mean :',cross_val_score(voting_clf_soft,X_train,Y_train,cv=5).mean())\n\nprint('voting_clf_all :',cross_val_score(voting_clf_all,X_train,Y_train,cv=5))\nprint('voting_clf_all mean :',cross_val_score(voting_clf_all,X_train,Y_train,cv=5).mean())\n\nprint('voting_clf_xgb :',cross_val_score(voting_clf_xgb,X_train,Y_train,cv=5))\nprint('voting_clf_xgb mean :',cross_val_score(voting_clf_xgb,X_train,Y_train,cv=5).mean())","7db098d8":"params = {'weights' : [[1,1,1],[1,2,1],[1,1,2],[2,1,1],[2,2,1],[1,2,2],[2,1,2]]}\n\nvote_weight = GridSearchCV(voting_clf_soft, param_grid = params, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_weight = vote_weight.fit(X_train,Y_train)\nclf_performance(best_clf_weight,'VC Weights')\nvoting_clf_sub = best_clf_weight.best_estimator_.predict(X_test)","125afabe":"voting_clf_hard.fit(X_train, Y_train)\nvoting_clf_soft.fit(X_train, Y_train)\nvoting_clf_all.fit(X_train, Y_train)\nvoting_clf_xgb.fit(X_train, Y_train)\n\nbest_rf.fit(X_train, Y_train)\ny_hat_vc_hard = voting_clf_hard.predict(X_test).astype(int)\ny_hat_rf = best_rf.predict(X_test).astype(int)\ny_hat_vc_soft =  voting_clf_soft.predict(X_test).astype(int)\ny_hat_vc_all = voting_clf_all.predict(X_test).astype(int)\ny_hat_vc_xgb = voting_clf_xgb.predict(X_test).astype(int)","e39dc76f":"final_data = {'PassengerId': teste_dados.PassengerId, 'Survived': y_hat_rf}\nsubmission = pd.DataFrame(data=final_data)\n\nfinal_data_2 = {'PassengerId': teste_dados.PassengerId, 'Survived': y_hat_vc_hard}\nsubmission_2 = pd.DataFrame(data=final_data_2)\n\nfinal_data_3 = {'PassengerId': teste_dados.PassengerId, 'Survived': y_hat_vc_soft}\nsubmission_3 = pd.DataFrame(data=final_data_3)\n\nfinal_data_4 = {'PassengerId': teste_dados.PassengerId, 'Survived': y_hat_vc_all}\nsubmission_4 = pd.DataFrame(data=final_data_4)\n\nfinal_data_5 = {'PassengerId': teste_dados.PassengerId, 'Survived': y_hat_vc_xgb}\nsubmission_5 = pd.DataFrame(data=final_data_5)\n\nfinal_data_comp = {'PassengerId': teste_dados.PassengerId, 'Survived_vc_hard': y_hat_vc_hard, 'Survived_rf': y_hat_rf, 'Survived_vc_soft' : y_hat_vc_soft, 'Survived_vc_all' : y_hat_vc_all,  'Survived_vc_xgb' : y_hat_vc_xgb}\ncomparison = pd.DataFrame(data=final_data_comp)","72383fd4":"comparison['difference_rf_vc_hard'] = comparison.apply(lambda x: 1 if x.Survived_vc_hard != x.Survived_rf else 0, axis =1)\ncomparison['difference_soft_hard'] = comparison.apply(lambda x: 1 if x.Survived_vc_hard != x.Survived_vc_soft else 0, axis =1)\ncomparison['difference_hard_all'] = comparison.apply(lambda x: 1 if x.Survived_vc_all != x.Survived_vc_hard else 0, axis =1)","b85362d9":"comparison.difference_hard_all.value_counts()","68695dab":"submission.to_csv('submission_rf.csv', index =False)\nsubmission_2.to_csv('submission_vc_hard.csv',index=False)\nsubmission_3.to_csv('submission_vc_soft.csv', index=False)\nsubmission_4.to_csv('submission_vc_all.csv', index=False)\nsubmission_5.to_csv('submission_vc_xgb2.csv', index=False)","e32234de":"# Preparando dados para treinamento","d57767e9":"**Cr\u00e9ditos**\n\nEste notebook foi baseado em tr\u00eas outros notebooks:\n* https:\/\/github.com\/mrdbourke\/your-first-kaggle-submission\/tree\/master\n* https:\/\/www.kaggle.com\/kenjee\/titanic-project-example\n* https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions","55b59b63":"# Calibrando os algoritmos para selecionar os melhores par\u00e2metros","b2b44bb2":"# Plotando gr\u00e1ficos de barra dos dados categ\u00f3ricos para observar as frequ\u00eancias\n\n* **De acordo com os gr\u00e1ficos, observamos que:**\n\n    *  O n\u00famero de sobreviventes \u00e9 menor do que o n\u00famero de n\u00e3o-sobreviventes\n    *  O maior n\u00famero de passageiros pertenciam a terceira classe\n    *  A maioria dos passageiros eram homens\n    *  A maior parte dos embarques foram feitos em Southampto\n    *  As vari\u00e1veis Cabin e Ticket possuem muitos valores diferentes","f463de36":"#  **Definindo e entendendo os dados**\n\n* Nome da vari\u00e1vel: **Survival**\n* Tipo: categ\u00f3rica\n* Defini\u00e7\u00e3o: Sobreviventes e n\u00e3o sobreviventes. 1 = sobreviveu, 0 = n\u00e3o sobreviveu\n\n\n* Nome da vari\u00e1vel: **Pclass**\n* Tipo: categ\u00f3rica\n* Defini\u00e7\u00e3o: Classe da cabine do passageiro. 1 = primeira classe, 2 = segunda classe, 3 = terceira classe\n\n\n* Nome da vari\u00e1vel: **Sex**\n* Tipo: categ\u00f3rica\n* Defini\u00e7\u00e3o: Sexo do passageiro. Male = homem, Female = mulher\n\n\n* Nome da vari\u00e1vel: **Sibsp**\n* Tipo: num\u00e9rica\n* Defini\u00e7\u00e3o: Indica a exist\u00eancia e quantidade de irm\u00e3os, irm\u00e3s, esposo ou esposa. \n\n\n* Nome da vari\u00e1vel: **Parch**\n* Tipo: num\u00e9rica\n* Defini\u00e7\u00e3o: Indica a exist\u00eancia e quantidade de pais, m\u00e3es, filhos ou filhas.\n\n\n* Nome da vari\u00e1vel: **Ticket**\n* Tipo: alfanum\u00e9rica\n* Defini\u00e7\u00e3o: combina\u00e7\u00e3o de letras e n\u00fameros representando o ingresso de cada passageiro.\n\n\n* Nome da vari\u00e1vel: **Fare**\n* Tipo: num\u00e9rica\n* Defini\u00e7\u00e3o: indica o gasto que cada passageiro teve dentro do navio.\n\n\n* Nome da vari\u00e1vel: **Cabin**\n* Tipo: alfanum\u00e9rica\n* Defini\u00e7\u00e3o: indica o c\u00f3digo da cabine de cada passageiro.\n\n\n* Nome da vari\u00e1vel: **Embarked**\n* Tipo: categ\u00f3rica\n* Defini\u00e7\u00e3o: define a cidade de embarque de cada passageiro. C = Cherbourg, Q = Queenstown e S = Southampton\n\n","e3ff487c":"# Analisando a rela\u00e7\u00e3o entre o gasto de cada passageiro e o n\u00famero de sobreviventes, de acordo com o local de embarque\n\n* Os homens e as mulheres que gastaram mais tiveram uma maior chance de sobreviver, tanto nos embarques feitos em Southampton quanto em Cherbourg.\n* Por outro lado, a chance de sobreviv\u00eancia dos passageiros que embarcaram em Queenstown foi pouco influenciada pelo gasto que tiveram. ","44733237":"# Analisando dados faltantes\n\nA fun\u00e7\u00e3o missingno.matrix() nos retorna um gr\u00e1fico onde os espa\u00e7os em branco representam os valores faltantes em cada vari\u00e1vel. Nesse caso, vemos que existem alguns valores faltantes na vari\u00e1vel de idade, e v\u00e1rios valores faltantes na vari\u00e1vel cabine.","96745f83":"# Plotando mapa de calor para identificar correla\u00e7\u00f5es entre os dados num\u00e9ricos\n\nPodemos observar que as vari\u00e1veis Parch e SibSp tem uma correla\u00e7\u00e3o consider\u00e1vel. ","544d3b76":"# Salvando os melhores resultados para submiss\u00e3o","2c926154":"# Engenharia de dados\n\nDe acordo com os insights tirados da an\u00e1lise explorat\u00f3ria dos dados, foram tomadas algumas decis\u00f5es em rela\u00e7\u00e3o a cria\u00e7\u00e3o e remo\u00e7\u00e3o de algumas vari\u00e1veis. \u00c9 importante lembrar que toda mudan\u00e7a feita no dataset de treino deve ser refletida no dataset de testes. Segue explica\u00e7\u00e3o:\n\n* **Preenchimento de valores faltantes:**\n    * Os valores faltantes da vari\u00e1vel \"**Age**\" ser\u00e3o preenchidos com a mediana das idades dos passageiros que est\u00e3o na mesma classe do passageiro em quest\u00e3o.\n    * Os valores faltantes da vari\u00e1vel \"**Embarked**\" ser\u00e3o preenchidos com a moda da vari\u00e1vel.\n    * Os valores faltantes da vari\u00e1vel \"**Fare**\" ser\u00e3o preenchidos com a mediana da vari\u00e1vel.\n\n\n* **Novas vari\u00e1veis:**\n    * Cria\u00e7\u00e3o da vari\u00e1vel \"**cabin_multiple**\", para representar os passageiros que possuem mais de uma cabine. Esse n\u00famero refor\u00e7a o poder aquisitivo do passageiro.\n    * Cria\u00e7\u00e3o da vari\u00e1vel \"**cabin_adv**\", que guarda a letra do c\u00f3digo de cada cabine. Provavelmente essa letra representa a localidade da cabine dentro do navio, impactando ent\u00e3o no resultado.\n    * Cria\u00e7\u00e3o da vari\u00e1vel \"**numeric_ticket**\", que representar\u00e1 os ingressos que possuem valores apenas num\u00e9ricos.\n    * Cria\u00e7\u00e3o da vari\u00e1vel \"**ticket_letters**\", que guarda as letras iniciais dos ingressos dos passageiros. \n    * Cria\u00e7\u00e3o da vari\u00e1vel \"**Title**\", onde guardaremos o t\u00edtulo (Ms., Mr., etc) de cada passageiro. Al\u00e9m disso, agruparemos os t\u00edtulos \"Mlle\" (Mademoiselle) e \"Ms\" junto com o t\u00edtulo \"Miss\", e o t\u00edtulo \"Mme\" (Madame) com o t\u00edtulo \"Mrs\". Os t\u00edtulos restantes ser\u00e3o agrupados como \"Rare\".\n    * Cria\u00e7\u00e3o da vari\u00e1vel \"**IsAlone**\", que representa se o passageiro estava sozinho ou se foi acompanhado de familiares. Para isso, antes criaremos a vari\u00e1vel \"FamilySize\", que soma as vari\u00e1veis \"SibSp\" e \"Parch\" + 1 para representar o tamanho das fam\u00edlias de cada passageiro. Feito isso, os passageiros que estiverem \"FamilySize\" igual a zero receber\u00e3o 1 na vari\u00e1vel \"IsAlone\", enquanto os passageiros que possuem familiares a bordo receber\u00e3o 0.\n    * Cria\u00e7\u00e3o da vari\u00e1vel \"**Age* Class**\", que multiplica a idade e a classe dos passageiros.\n    * Cria\u00e7\u00e3o da vari\u00e1vel \"**AgeBand**\", que separa os passageiros em 5 faixas et\u00e1rias.\n    * Cria\u00e7\u00e3o da vari\u00e1vel \"**FareBand**\", que separa os passageiros em 4 grupos em rela\u00e7\u00e3o aos gastos que tiveram.\n\n\n* **Convers\u00e3o de vari\u00e1veis:**\n    * A vari\u00e1vel \"**Title**\" ser\u00e1 representada de forma num\u00e9rica, seguindo a regra \"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5\n    * A vari\u00e1vel \"**Sex**\" ser\u00e1 representada de forma num\u00e9rica, seguindo a regra 'female': 1, 'male': 0\n    * A vari\u00e1vel \"**Embarked**\" ser\u00e1 representada de forma num\u00e9rica, seguindo a regra 'S': 0, 'C': 1, 'Q': 2\n    * A vari\u00e1vel \"**cabin_adv**\" ser\u00e1 representada de forma num\u00e9rica, seguindo a regra 'n': 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'T': 8\n    * A vari\u00e1vel \"**ticket_letters**\" ser\u00e1 representada de forma num\u00e9ria, utilizando a fun\u00e7\u00e3o do pandas .get_dummies()\n    * A vari\u00e1vel \"**Age**\" ser\u00e1 representada por n\u00fameros de 1 a 5, de acordo com a faixa et\u00e1ria estabelecida pela vari\u00e1vel \"AgeBand\"\n    * A vari\u00e1vel \"**Fare**\" ser\u00e1 representada por n\u00fameros de 1 a 4, de acordo com a faixa de gastos estabelecida pela vari\u00e1vel \"FareBand\"\n\n\n* **Remo\u00e7\u00e3o de vari\u00e1veis:**\n    * As vari\u00e1veis Ticket, Cabin, Name, AgeBand, Parch, SibSp, FamilySize e FareBand foram removidas.","6a62064e":"# Dividindo os dados em num\u00e9ricos e categ\u00f3ricos\n\nAs vari\u00e1veis com dados num\u00e9ricos e com dados categ\u00f3ricos ser\u00e3o tratadas de formas diferentes. ","101b13de":"# **Roteiro:**\n\n* **An\u00e1lise explorat\u00f3ria de dados (EDA)**\n\n    * Entender as vari\u00e1veis\n    * Analisar as rela\u00e7\u00f5es\n    * Ter insights sobre a cria\u00e7\u00e3o de novas vari\u00e1veis e o descarte de vari\u00e1veis j\u00e1 existentes\n    * Plotar alguns gr\u00e1ficos para facilitar a interpreta\u00e7\u00e3o dos dados\n    * Verificar dados faltantes e decidir como preench\u00ea-los\n    \n    \n* **Engenharia de dados (Feature Engineering)**\n\n    * Criar novas vari\u00e1veis \n    * Descartar vari\u00e1veis redundantes ou desnecess\u00e1rias\n    * Convers\u00e3o de vari\u00e1veis\n    \n    \n* **Treinamento e calibra\u00e7\u00e3o do modelo**\n\n    * Treinar os algoritmos\n    * Escolher os melhores par\u00e2metros","6b7c4266":"# Analisando a rela\u00e7\u00e3o entre idade e n\u00famero de sobreviventes\n\nAqui vemos que boa parte dos passageiros entre 15 e 45 anos n\u00e3o conseguiram sobreviver. Observamos tamb\u00e9m que um passageiro mais velho (pr\u00f3ximo aos 80 anos) sobreviveu, assim como a maior parte das crian\u00e7as entre 0 e 10 anos.","82e00c1c":"# Observando como os dados est\u00e3o dispostos\n\nAs fun\u00e7\u00f5es .head(), .describe() e .info() nos d\u00e3o uma boa amostra sobre a base de dados.","844aba6c":"# Analisando a rela\u00e7\u00e3o do n\u00famero de homens e mulheres sobreviventes, de acordo com a classe e o local de embarque\n\n* **Embarques em Southampton:**\n    * Quase todas as mulheres na primeira e segunda classe sobreviveram, por\u00e9m menos da metade das mulheres na terceira classe conseguiram sobreviver.\n    * Menos da metade dos homens conseguiram sobreviver, em todas as classes.\n    \n\n* **Embarques em Cherbourg:**\n    * A maioria dos homens na primeira e segunda classe conseguiram sobreviver. Da mesma forma, aproximadamente 75% dos homens na terceira classe tamb\u00e9m sobreviveram.\n    * O n\u00famero de mulheres sobreviventes fica entre 25 e 50%, em todas as classes.\n    \n\n* **Embarques em Queenstown:**\n    * A grande maioria das mulheres sobreviveram, em todas as classes.\n    * A grande maioria dos homens n\u00e3o sobreviveram, em todas as classes.","44b1e696":"# Aplicando os algoritmos","f7143e7c":"# Analisando a rela\u00e7\u00e3o entre classe e n\u00famero de sobreviventes, de acordo com a idade\n\n* A maior parte dos passageiros da primeira classe sobreviveram, independente da idade\n* Entre os passageiros da segunda classe, todas as crian\u00e7as foram salvas, mas boa parte das pessoas entre 17 - 40 anos n\u00e3o sobreviveram\n* A maioria dos passageiros da terceira classe n\u00e3o sobreviveram, sendo que a maior parte dos \u00f3bitos foram de passageiros entre 15-30 anos","09309680":"# Plotando tabelas para observar a rela\u00e7\u00e3o entre os dados das vari\u00e1veis com o resultado (se sobreviveu ou n\u00e3o)\n\n* **Novas observa\u00e7\u00f5es podem ser feitas:**\n    * ~63% dos passageiros da primeira classe sobreviveram, enquanto apenas 47% da segunda classe e 24% da terceira classe conseguiram sobreviver\n    * 74% das mulheres sobreviveram, enquanto apenas ~19% dos homens conseguiram sobreviver\n    * Quanto maior a quantidade de parentes que um passageiro tinha no navio (tanto SibSp quanto Parch), menores s\u00e3o as chances que esse passageiro tenha sobrevivido","0739f6c9":"# Plotando histogramas dos dados num\u00e9ricos para observar a distribui\u00e7\u00e3o\n\nO ideal \u00e9 que os dados num\u00e9ricos sigam uma distribui\u00e7\u00e3o normal (formato de sino). Podemos usar a fun\u00e7\u00e3o log para normalizar a distribui\u00e7\u00e3o de uma vari\u00e1vel, mas preferi analisar melhor cada caso antes de tomar uma decis\u00e3o final."}}