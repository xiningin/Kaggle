{"cell_type":{"a75fb51f":"code","cebc7d5f":"code","ffd02b4d":"code","550b5a0a":"code","f7000e8f":"code","4e335587":"code","a88c1cb9":"code","956bc330":"code","2f43333d":"code","2241761f":"code","082e293b":"code","ff1c849c":"code","bab339c5":"code","1fb26028":"code","67b94bc2":"code","adc5064c":"code","8969ec06":"code","4eb93c95":"code","c9820777":"code","2ca7290e":"code","6d154206":"code","d658bb7d":"code","e3e579a1":"code","85586f26":"code","5d523242":"code","42e99c74":"code","471867ad":"code","5e0a9b55":"code","0a47348f":"code","840d2bfb":"code","d87ba017":"code","0c11f3bf":"markdown","ce633c7c":"markdown","065fd8b9":"markdown","216307c3":"markdown"},"source":{"a75fb51f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cebc7d5f":"input_path = '\/kaggle\/input\/nlp-getting-started\/'\n\ntrain = pd.read_csv(os.path.join(input_path, 'train.csv'))\ntest = pd.read_csv(os.path.join(input_path, 'test.csv'))","ffd02b4d":"train.head()","550b5a0a":"print('Train: ', train.shape)\nprint('Test: ', test.shape)","f7000e8f":"# check the missing values for keyword and location\nlen(train['keyword'].isnull()), len(train['location'].isnull())","4e335587":"# non disaster tweet\ntrain[train['target'] == 0]['text'].values[0]","a88c1cb9":"# disaster tweet\ntrain[train['target'] == 1]['text'].values[0]","956bc330":"import re\nimport unicodedata\nimport spacy\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dropout, Dense\nfrom tensorflow.keras.optimizers import Adam","2f43333d":"# reference:~ https:\/\/github.com\/dipanjanS\/practical-machine-learning-with-python\/blob\/master\/bonus%20content\/nlp%20proven%20approach\/contractions.py\n\nCONTRACTION_MAP = {\n    \"ain't\": \"is not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"cannot\",\n    \"can't've\": \"cannot have\",\n    \"'cause\": \"because\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"couldn't've\": \"could not have\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"he would\",\n    \"he'd've\": \"he would have\",\n    \"he'll\": \"he will\",\n    \"he'll've\": \"he he will have\",\n    \"he's\": \"he is\",\n    \"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\",\n    \"how'll\": \"how will\",\n    \"how's\": \"how is\",\n    \"I'd\": \"I would\",\n    \"I'd've\": \"I would have\",\n    \"I'll\": \"I will\",\n    \"I'll've\": \"I will have\",\n    \"I'm\": \"I am\",\n    \"I've\": \"I have\",\n    \"i'd\": \"i would\",\n    \"i'd've\": \"i would have\",\n    \"i'll\": \"i will\",\n    \"i'll've\": \"i will have\",\n    \"i'm\": \"i am\",\n    \"i've\": \"i have\",\n    \"isn't\": \"is not\",\n    \"it'd\": \"it would\",\n    \"it'd've\": \"it would have\",\n    \"it'll\": \"it will\",\n    \"it'll've\": \"it will have\",\n    \"it's\": \"it is\",\n    \"let's\": \"let us\",\n    \"ma'am\": \"madam\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"shan't've\": \"shall not have\",\n    \"she'd\": \"she would\",\n    \"she'd've\": \"she would have\",\n    \"she'll\": \"she will\",\n    \"she'll've\": \"she will have\",\n    \"she's\": \"she is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\n    \"so's\": \"so as\",\n    \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\n    \"that's\": \"that is\",\n    \"there'd\": \"there would\",\n    \"there'd've\": \"there would have\",\n    \"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\n    \"they'll\": \"they will\",\n    \"they'll've\": \"they will have\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"to've\": \"to have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"we would\",\n    \"we'd've\": \"we would have\",\n    \"we'll\": \"we will\",\n    \"we'll've\": \"we will have\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"weren't\": \"were not\",\n    \"what'll\": \"what will\",\n    \"what'll've\": \"what will have\",\n    \"what're\": \"what are\",\n    \"what's\": \"what is\",\n    \"what've\": \"what have\",\n    \"when's\": \"when is\",\n    \"when've\": \"when have\",\n    \"where'd\": \"where did\",\n    \"where's\": \"where is\",\n    \"where've\": \"where have\",\n    \"who'll\": \"who will\",\n    \"who'll've\": \"who will have\",\n    \"who's\": \"who is\",\n    \"who've\": \"who have\",\n    \"why's\": \"why is\",\n    \"why've\": \"why have\",\n    \"will've\": \"will have\",\n    \"won't\": \"will not\",\n    \"won't've\": \"will not have\",\n    \"would've\": \"would have\",\n    \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\n    \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\",\n    \"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\n    \"you'd\": \"you would\",\n    \"you'd've\": \"you would have\",\n    \"you'll\": \"you will\",\n    \"you'll've\": \"you will have\",\n    \"you're\": \"you are\",\n    \"you've\": \"you have\"\n}","2241761f":"# loading the spacy's en_core_web_sm\nnlp = spacy.load('en_core_web_sm')\nnlp.pipe_names","082e293b":"# create and add sentencizer to the pipeline\nsent = nlp.create_pipe('sentencizer')\nnlp.add_pipe(sent, before='parser')\nnlp.pipe_names","ff1c849c":"def text_cleaning(text):\n    \"\"\"\n    Returns cleaned text (Accented Characters, Expand Contractions, Special Characters)\n    Parameters\n    ----------\n    text -> String\n    \"\"\"\n    # remove accented characters\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    \n    # remove emails\n    text = ' '.join([i for i in text.split() if '@' not in i])\n    \n    # remove urls\n    text = re.sub('http[s]?:\/\/\\S+', '', text)\n    \n    # expand contractions\n    for word in text.split():\n        if word.lower() in CONTRACTION_MAP:\n            text = text.replace(word[1:], CONTRACTION_MAP[word.lower()][1:])\n    \n    # remove special characters\n    pattern = r'[^a-zA-Z0-9\\s]'\n    text = re.sub(pattern, '', text)\n    \n    # remove extra white spaces\n    text = re.sub('\\s+', ' ', text)\n    \n    doc = nlp(text)\n    tokens = []\n    \n    for token in doc:\n        if token.lemma_ != '-PRON-':\n            tokens.append(token.lemma_.lower().strip())\n        else:\n            tokens.append(token.lower_)\n\n    return ' '.join(tokens)","bab339c5":"text_cleaning(\"I don't like this movie. The plot is   terrible :(\")","1fb26028":"# split the data into inputs and outputs\nX_train = train['text'].apply(text_cleaning)\ny_train = train['target']\nX_test = test['text'].apply(text_cleaning)","67b94bc2":"X_train.head()","adc5064c":"# split the train set into train and valid set\nsplit = 0.8\ntrain_size = int(len(X_train) * 0.8)\nidx = np.random.permutation(X_train.index)\ntrain_idx = idx[:train_size]\nvalid_idx = idx[train_size:]\ntrain_data = X_train.iloc[train_idx]\ntrain_labels = y_train.iloc[train_idx]\nvalid_data = X_train.iloc[valid_idx]\nvalid_labels = y_train.iloc[valid_idx]","8969ec06":"oov_token = '<unk>'\npadding_type = 'post'\ntrunc_type = 'post'\nembedding_dim = 100\nmax_len = max([len(x) for x in train_data])","4eb93c95":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_data)\nword_index = tokenizer.word_index\nvocab_size = len(word_index)","c9820777":"print('Vocab size : ', vocab_size)","2ca7290e":"train_seq = tokenizer.texts_to_sequences(train_data)\ntrain_pad = pad_sequences(train_seq, padding=padding_type, truncating=trunc_type, maxlen=max_len)\n\nvalid_seq = tokenizer.texts_to_sequences(valid_data)\nvalid_pad = pad_sequences(valid_seq, padding=padding_type, truncating=trunc_type, maxlen=max_len)","6d154206":"test_seq = tokenizer.texts_to_sequences(X_test)\ntest_pad = pad_sequences(test_seq, padding=padding_type, truncating=trunc_type, maxlen=max_len)","d658bb7d":"embeddings_idx = {}\nglove_path = '\/kaggle\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt'\nwith open(glove_path, 'r') as f:\n    for line in f:\n        data = line.split()\n        word = data[0]\n        values = np.asarray(data[1:], dtype=np.float32)\n        embeddings_idx[word] = values\n\nprint('Building Embedding Matrix...')\nembeddings_matrix = np.zeros((vocab_size + 1, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vec = embeddings_idx.get(word)\n    if embedding_vec is not None:\n        embeddings_matrix[i] = embedding_vec\nprint('Embedding Matrix Generating...')\nprint('Embedding Matrix Shape -> ', embeddings_matrix.shape)","e3e579a1":"# build a model\nmodel =  keras.models.Sequential([\n    Embedding(vocab_size + 1, embedding_dim, input_length=max_len, weights=[embeddings_matrix], trainable=False),\n    Dropout(0.2),\n    Bidirectional(LSTM(64, return_sequences=True)),\n    Bidirectional(LSTM(32)),\n    Dense(32, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\nmodel.summary()","85586f26":"optimizer = Adam(lr=3e-4)\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])","5d523242":"history = model.fit(train_pad, train_labels, epochs=20, validation_data=(valid_pad, valid_labels))","42e99c74":"# model analysis\nimport matplotlib.pyplot as plt\n\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_' + string])\n    plt.xlabel('# epochs')\n    plt.ylabel(string)\n    plt.legend([string, 'val_' + string])\n    plt.show()","471867ad":"plot_graphs(history, 'accuracy')","5e0a9b55":"plot_graphs(history, 'loss')","0a47348f":"# load the sample submission csv file\nsample_submission = pd.read_csv(os.path.join(input_path, 'sample_submission.csv'))","840d2bfb":"# predict on test data\nsample_submission['target'] = model.predict_classes(test_pad)","d87ba017":"# save the sample submission csv file\nsample_submission.to_csv('submission.csv', index=False)","0c11f3bf":"#### Load the spacy en_core library and add a sentencizer to pipeline","ce633c7c":"# NLP with Disaster Tweets using TensorFlow, Keras\n","065fd8b9":"### Build a Model\n","216307c3":"### Quick Look at Data\n"}}