{"cell_type":{"8fcb905f":"code","0b0fc7f4":"code","f30c1d49":"code","d55e6a3a":"code","7711ce0d":"markdown","9bd38e63":"markdown","32128176":"markdown","d8de4977":"markdown","44e7b0eb":"markdown","41722c89":"markdown","55732cdc":"markdown","df40ed25":"markdown","f11d0737":"markdown","5504120d":"markdown","f184de0e":"markdown"},"source":{"8fcb905f":"#Getting the basic libraries set up\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#Machine learning-specific\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom collections import Counter\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\ny_train = pd.read_csv('..\/input\/y_train.csv')\nx_train = pd.read_csv('..\/input\/X_train.csv')\nx_test = pd.read_csv('..\/input\/X_test.csv')\n\n# Any results you write to the current directory are saved as output.","0b0fc7f4":"#Function for setting up training & testing data\ndef processor(input,out_name,num_samples):\n    #Input should be an input csv turned into a data frame\n    #out_name is a string ending in .csvorientations = [\"or_x_mean\",\"or_x_std\",\"or_x_range\",\"or_y_mean\",\"or_y_std\",\"or_y_range\",\"or_z_mean\",\"or_z_std\",\"or_z_range\",\"or_w_mean\",\"or_w_std\",\"or_w_range\"]\n    orientations = [\"or_x_mean\",\"or_x_std\",\"or_x_range\",\"or_y_mean\",\"or_y_std\",\"or_y_range\",\"or_z_mean\",\"or_z_std\",\"or_z_range\",\"or_w_mean\",\"or_w_std\",\"or_w_range\"]\n    ang_vels = [\"an_x_mean\",\"an_x_std\",\"an_x_range\",\"an_y_mean\",\"an_y_std\",\"an_y_range\",\"an_z_mean\",\"an_z_std\",\"an_z_range\"]\n    accels = [\"acc_x_mean\",\"acc_x_std\",\"acc_x_range\",\"acc_y_mean\",\"acc_y_std\",\"acc_y_range\",\"acc_z_mean\",\"acc_z_std\",\"acc_z_range\"]\n    columns = orientations + ang_vels+accels\n    processed = pd.DataFrame(index = range(0,num_samples+1), columns = columns)\n    #What you are about to see is an affront to good code, but I'm not entire sure how to make this manageable\n    for i in range(0,num_samples+1):\n        curr = input.loc[input[\"series_id\"]==i]\n        \n        or_x_mean = np.mean(curr[\"orientation_X\"])\n        processed[\"or_x_mean\"][i] = or_x_mean\n        or_x_std = np.std(curr[\"orientation_X\"])\n        processed[\"or_x_std\"][i] = or_x_std\n        or_x_range = max(curr[\"orientation_X\"])-min(curr[\"orientation_X\"])\n        processed[\"or_x_range\"][i] = or_x_range\n        \n        \n        or_y_mean = np.mean(curr[\"orientation_Y\"])\n        processed[\"or_y_mean\"][i] = or_y_mean\n        or_y_std = np.std(curr[\"orientation_Y\"])\n        processed[\"or_y_std\"][i] = or_y_std\n        or_y_range = max(curr[\"orientation_Y\"])-min(curr[\"orientation_Y\"])\n        processed[\"or_y_range\"][i] = or_y_range\n        \n        or_z_mean = np.mean(curr[\"orientation_Z\"])\n        processed[\"or_z_mean\"][i] = or_z_mean\n        or_z_std = np.std(curr[\"orientation_Z\"])\n        processed[\"or_z_std\"][i] = or_z_std\n        or_z_range = max(curr[\"orientation_Z\"])-min(curr[\"orientation_Z\"])\n        processed[\"or_z_range\"][i] = or_z_range\n        \n        or_w_mean = np.mean(curr[\"orientation_W\"])\n        processed[\"or_w_mean\"][i] = or_z_mean\n        or_w_std = np.std(curr[\"orientation_W\"])\n        processed[\"or_w_std\"][i] = or_z_std\n        or_w_range = max(curr[\"orientation_W\"])-min(curr[\"orientation_W\"])\n        processed[\"or_w_range\"][i] = or_z_range\n        \n        an_x_mean = np.mean(curr[\"angular_velocity_X\"])\n        processed[\"an_x_mean\"][i] = an_x_mean\n        an_x_std = np.std(curr[\"angular_velocity_X\"])\n        processed[\"an_x_std\"][i] = an_x_std\n        an_x_range = max(curr[\"angular_velocity_X\"])-min(curr[\"angular_velocity_X\"])\n        processed[\"an_x_range\"][i] = an_x_range\n        \n        \n        an_y_mean = np.mean(curr[\"angular_velocity_Y\"])\n        processed[\"an_y_mean\"][i] = an_y_mean\n        an_y_std = np.std(curr[\"angular_velocity_Y\"])\n        processed[\"an_y_std\"][i] = an_y_std\n        an_y_range = max(curr[\"angular_velocity_Y\"])-min(curr[\"angular_velocity_Y\"])\n        processed[\"an_y_range\"][i] = an_y_range\n        \n        an_z_mean = np.mean(curr[\"angular_velocity_Z\"])\n        processed[\"an_z_mean\"][i] = an_z_mean\n        an_z_std = np.std(curr[\"angular_velocity_Z\"])\n        processed[\"an_z_std\"][i] = an_z_std\n        an_z_range = max(curr[\"angular_velocity_Z\"])-min(curr[\"angular_velocity_Z\"])\n        processed[\"an_z_range\"][i] = an_z_range\n        \n        acc_x_mean = np.mean(curr[\"linear_acceleration_X\"])\n        processed[\"acc_x_mean\"][i] = acc_x_mean\n        acc_x_std = np.std(curr[\"linear_acceleration_X\"])\n        processed[\"acc_x_std\"][i] = acc_x_std\n        acc_x_range = max(curr[\"linear_acceleration_X\"])-min(curr[\"linear_acceleration_X\"])\n        processed[\"acc_x_range\"][i] = acc_x_range\n        \n        acc_y_mean = np.mean(curr[\"linear_acceleration_Y\"])\n        processed[\"acc_y_mean\"][i] = acc_y_mean\n        acc_y_std = np.std(curr[\"linear_acceleration_Y\"])\n        processed[\"acc_y_std\"][i] = acc_y_std\n        acc_y_range = max(curr[\"linear_acceleration_Y\"])-min(curr[\"linear_acceleration_Y\"])\n        processed[\"acc_y_range\"][i] = acc_y_range\n        \n        acc_z_mean = np.mean(curr[\"linear_acceleration_Z\"])\n        processed[\"acc_z_mean\"][i] = acc_z_mean\n        acc_z_std = np.std(curr[\"linear_acceleration_Z\"])\n        processed[\"acc_z_std\"][i] = acc_z_std\n        acc_z_range = max(curr[\"linear_acceleration_Z\"])-min(curr[\"linear_acceleration_Z\"])\n        processed[\"acc_z_range\"][i] = acc_z_range\n        \n    processed.to_csv(out_name)\n    return processed","f30c1d49":"train = processor(x_train,'training_processed.csv',3809)\ntest = processor(x_test,'testing_processed.csv',3815)\n\nle = preprocessing.LabelEncoder()\ny_train = le.fit_transform(y_train['surface'])","d55e6a3a":"#You know what, lets try this XGBoost thing they've been talking about\nmodel = XGBClassifier(n_estimators = 2000)\nX = train.values\nmodel.fit(X,y_train,early_stopping_rounds=5,eval_set = [(X,y_train)]) #Too laxy to split r\/n,just seeing if this works\n\n\npredictions = model.predict(test.values)\npredictions = le.inverse_transform(predictions)\n\nindexes = range(0,3816)\ncolumns = [\"series_id\",\"surface\"]\nsubmission = pd.DataFrame(index = indexes, columns = columns)\nsubmission[\"series_id\"]= range(0,3816)\nsubmission[\"surface\"]=predictions\n\nsubmission.to_csv('predictions.csv', index = False)","7711ce0d":"#Random forest implementation\n\npredictions=clf.predict(test.values)\n\npredictions = le.inverse_transform(predictions)\n\nindexes = range(0,3816)\ncolumns = [\"series_id\",\"surface\"]\nsubmission = pd.DataFrame(index = indexes, columns = columns)\nsubmission[\"series_id\"]= range(0,3816)\nsubmission[\"surface\"]=predictions\n\nsubmission.to_csv('predictions.csv', index = False)","9bd38e63":"#Random forest implementation\n\npredictions=clf.predict(test.values)\n\npredictions = le.inverse_transform(predictions)\n\nindexes = range(0,3816)\ncolumns = [\"series_id\",\"surface\"]\nsubmission = pd.DataFrame(index = indexes, columns = columns)\nsubmission[\"series_id\"]= range(0,3816)\nsubmission[\"surface\"]=predictions\n\nsubmission.to_csv('predictions.csv', index = False)","32128176":"Now the first payoff- Building the trainining dataframes","d8de4977":"X = train.values\n\nmodel = keras.Sequential([\n\nkeras.layers.Dense(25, activation=tf.nn.relu),\nkeras.layers.BatchNormalization(), #Absolutely no idea if this will do anything, throwing data science at the wall and seeing what sticks\nkeras.layers.Dense(25, activation=tf.nn.relu),\nkeras.layers.Dense(9, activation=tf.nn.softmax)\n])\n\nstop = keras.callbacks.EarlyStopping(monitor='loss')\ncallbacks = [stop]\n\nmodel.compile(optimizer='adam', \nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\n\n\nmodel.fit(X, y_train, epochs=1000,batch_size = 32)\nmodel.save_weights('attempt.hd5')","44e7b0eb":"...But what if we did this another way? Time for me to find out what random forests are","41722c89":"First things first, let's get the inputs and outputs set up.\n\nFor outputs, this is a pretty simple use of label encoding.\n\nFor inputs, this implementation crafts a couple of features from each measurement series: mean, stdev, and range. I suspect that the feature selection is the first thing that needs to be improved to make an accurate model, but I'm not entirely sure what else to add","55732cdc":"X = train.values\n\nclf = RandomForestClassifier(n_estimators = 200)\n\nclf.fit(X,y_train)","df40ed25":"#This is the predictions using the NN implementation\npredictions = model.predict(test)\nindexes = range(0,3816)\ntrue_predictions = np.zeros((3816,)) #Ask Mike about this one\n\n#convert the number arrays to actual surfaces\nfor i in range(len(predictions)):\n    true_predictions[i]= np.argmax(predictions[i])\n\ntrue_predictions = true_predictions.astype(int)\ntrue_predictions = le.inverse_transform(true_predictions)  \n    \ncolumns = [\"series_id\",\"surface\"]\nsubmission = pd.DataFrame(index = indexes, columns = columns)\n\nsubmission[\"series_id\"]= range(0,3816)\nsubmission[\"surface\"]=true_predictions\n\nsubmission.to_csv('predictions.csv', index = False)","f11d0737":"With this taken care of, we can get to building the actual model\nThis is another place where I think improvements can be made in shaping the model. This is my first idea for doing this","5504120d":"Now we apply the model to the test set","f184de0e":"So what exactly is this assignment? This is a simple classification task with a minor hiccup: how exactly should the inputs be vectorized? \n\nThis will be a public kernel, so feel free to offer suggestions\/yell at me if you're reading this"}}