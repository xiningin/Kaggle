{"cell_type":{"3b9903ec":"code","fb5993e4":"code","a3db1fba":"code","63585b46":"code","cb7ff8aa":"code","34cb122e":"code","f9c85fcd":"code","d95efe3b":"code","5834a79e":"code","e1209adb":"code","6ec10770":"code","a307f139":"code","201466f9":"code","d976589f":"code","7d895a8a":"code","72d7ef4b":"code","288f0726":"code","9903fed5":"code","5fe5c48c":"code","cc5a53cc":"code","cbc28ff8":"code","6c568ed2":"code","bcf5db64":"code","b85afb37":"code","c7f953d4":"code","d54f9d5f":"code","10cd144d":"code","932e5f62":"code","d772d8a6":"code","f772fd76":"code","0618c478":"code","90c027eb":"code","ae78c65a":"code","1280a63c":"code","34351b9b":"code","f932fa7f":"code","c41dd1aa":"code","c5fcb38e":"code","e2f4f79b":"code","0e1ec48c":"code","4f2dc68f":"code","6906c814":"code","14b3d5d2":"code","9e473f4f":"code","acddc1e5":"code","e73fe1e9":"code","03b3277e":"code","f6d32fea":"code","5bfad0d0":"code","63e1b2b5":"code","40cab961":"code","508f4425":"markdown","0963ad93":"markdown","8dd931a4":"markdown","a0b27023":"markdown","e7b7e1a2":"markdown","d8e9fd56":"markdown","dd804804":"markdown","67b9cc7c":"markdown","5dba3c59":"markdown","24158e38":"markdown","e97af401":"markdown","4addcd2e":"markdown","fadd5935":"markdown","6936949a":"markdown"},"source":{"3b9903ec":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport nltk\nimport re\nimport string\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fb5993e4":"train_data = pd.read_csv(\"\/kaggle\/input\/spooky-author-identification\/train.zip\")\n# train_data.head()","a3db1fba":"train_data.head()","63585b46":"train_data = train_data.drop('id', axis='columns')\ntrain_data.head()","cb7ff8aa":"def clean_train_data(x):\n    text = x\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text) # remove square brackets\n    text = re.sub(r'[^\\w\\s]','',text) # remove punctuation\n    text = re.sub('\\w*\\d\\w*', '', text) # remove words containing numbers\n    text = re.sub('\\n', '', text)\n    return text","34cb122e":"clean_data = train_data.copy()","f9c85fcd":"clean_data['text'] = train_data.text.apply(lambda x : clean_train_data(x))\nclean_data.head()","d95efe3b":"eng_stopwords = nltk.corpus.stopwords.words(\"english\")","5834a79e":"def remove_eng_stopwords(text):\n    token_text = nltk.word_tokenize(text)\n    remove_stop = [word for word in token_text if word not in eng_stopwords]\n    join_text = ' '.join(remove_stop)\n    return join_text","e1209adb":"remove_stop_data = clean_data.copy()","6ec10770":"remove_stop_data['text'] = clean_data.text.apply(lambda x : remove_eng_stopwords(x))\nremove_stop_data.head()","a307f139":"print(\"Before remove stopwords\", len(clean_data['text'][0]))\nprint(\"After remove stopwords\", len(remove_stop_data['text'][0]))","201466f9":"from itertools import chain\nfrom collections import Counter","d976589f":"list_words = remove_stop_data['text'].str.split()\nlist_words_merge = list(chain(*list_words))\n\nd = Counter(list_words_merge)\ndf = pd.DataFrame(data=d, index=['count'])\ntop_common_words = df.T.sort_values(by=['count'], ascending=False).reset_index().head(50)\ntop_common_words.head()","7d895a8a":"plt.figure(figsize=(15,7))\nsns.set(style=\"darkgrid\")\nsns.barplot(x=\"index\", y='count', data=top_common_words)\nplt.xticks(rotation=90)","72d7ef4b":"common_words_value = top_common_words['index'].values\nremove_words = ['man', 'life', 'night', 'house', 'heart']\nnew_stop_words = [x for x in common_words_value if x not in remove_words]\nnew_stop_words","288f0726":"def remove_new_stopwords(text):\n    token_text = nltk.word_tokenize(text)\n    remove_stop = [word for word in token_text if word not in new_stop_words]\n    join_text = ' '.join(remove_stop)\n    return join_text","9903fed5":"new_stop_data = remove_stop_data.copy()","5fe5c48c":"new_stop_data['text'] = remove_stop_data.text.apply(lambda x : remove_new_stopwords(x))\nnew_stop_data.head()","cc5a53cc":"print(\"Before remove stopwords\", len(remove_stop_data['text'][4]))\nprint(\"After remove stopwords\", len(new_stop_data['text'][4]))","cbc28ff8":"plt.figure(figsize=(10,7))\nsns.set(style=\"darkgrid\")\nsns.countplot(x=\"author\", data=train_data)\nplt.title('Author text distribution')","6c568ed2":"all_words_after = train_data['text'].str.split()\nmerged = list(chain(*all_words_after))\nd = Counter(merged)\ndf = pd.DataFrame(data=d, index=['count'])\ntop_count_words = df.T.sort_values(by=['count'], ascending=False).reset_index().head(50)\ntop_count_words.head()","bcf5db64":"plt.figure(figsize=(15,7))\nsns.set(style=\"darkgrid\")\nsns.barplot(x=\"index\", y='count', data=top_count_words)\nplt.xticks(rotation=90)\n\nplt.title(\"Most common words before removing stop words\")","b85afb37":"all_words_before = new_stop_data['text'].str.split()\nmerged = list(chain(*all_words_before))\nd = Counter(merged)\ndf = pd.DataFrame(data=d, index=['count'])\nbefore_top_words = df.T.sort_values(by=['count'], ascending=False).reset_index().head(50)\nbefore_top_words.head()","c7f953d4":"plt.figure(figsize=(15,7))\nsns.set(style=\"darkgrid\")\nsns.barplot(x=\"index\", y='count', data=before_top_words)\nplt.xticks(rotation=90)\n\nplt.title(\"Most common words after removing stop words\")","d54f9d5f":"eap_cloud = train_data[train_data.author == 'EAP'].text.values\nhpl_cloud = train_data[train_data.author == 'HPL'].text.values\nmws_cloud = train_data[train_data.author == 'MWS'].text.values","10cd144d":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nwc = WordCloud(stopwords=STOPWORDS, background_color=\"white\", colormap=\"Dark2\",\n               max_font_size=150, random_state=42)\n","932e5f62":"plt.figure(figsize=(20, 15))\n\nplt.subplot(1, 3, 1)\nasd = \" \".join(eap_cloud)\nwc.generate(asd)\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('Edgar Allen Poe')\n\nplt.subplot(1, 3, 2)\nasd = \" \".join(hpl_cloud)\nwc.generate(asd)\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('HP Lovecraft')\n\nplt.subplot(1, 3, 3)\nasd = \" \".join(mws_cloud)\nwc.generate(asd)\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('Mary Shelley')\n\nplt.figtext(.5,.63,'All writers, word clouds before stop word removal', color='b', fontsize=25, ha='center')","d772d8a6":"eap_cloud_before = new_stop_data[new_stop_data.author == 'EAP'].text.values\nhpl_cloud_before = new_stop_data[new_stop_data.author == 'HPL'].text.values\nmws_cloud_before = new_stop_data[new_stop_data.author == 'MWS'].text.values","f772fd76":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nwc = WordCloud(stopwords=STOPWORDS, background_color=\"white\", colormap=\"Dark2\",\n               max_font_size=150, random_state=42)","0618c478":"plt.figure(figsize=(20, 15))\n\nplt.subplot(1, 3, 1)\nasd = \" \".join(eap_cloud_before)\nwc.generate(asd)\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('Edgar Allen Poe')\n\nplt.subplot(1, 3, 2)\nasd = \" \".join(hpl_cloud_before)\nwc.generate(asd)\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('HP Lovecraft')\n\nplt.subplot(1, 3, 3)\nasd = \" \".join(mws_cloud_before)\nwc.generate(asd)\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('Mary Shelley')\n\nplt.figtext(.5,.63,'All writers, word clouds After stop word removal', color='b', fontsize=25, ha='center')","90c027eb":"full_name = {'EAP': 'Edgar Allen Poe', 'MWS': 'Mary Shelley', 'HPL': 'HP Lovecraft'}\nwriter_name = ['EAP', 'MWS', 'HPL']\nwriter_count_obj = {'writer_full_name': [], 'total_words': [], 'unique_words': []}\nfor name in writer_name:\n    name_all_words = new_stop_data[new_stop_data.author == name].text.str.split()\n    name_merged = list(chain(*name_all_words))\n    name_total_len = len(name_merged)\n    myset = set(name_merged)\n    \n    writer_count_obj['writer_full_name'].append(full_name[name])\n    writer_count_obj['total_words'].append(name_total_len)\n    writer_count_obj['unique_words'].append(len(myset))","ae78c65a":"words_df = pd.DataFrame(writer_count_obj)\nwords_df","1280a63c":"tidy = words_df.melt(id_vars='writer_full_name').rename(columns=str.title)\ntidy","34351b9b":"fig, ax1 = plt.subplots(figsize=(15, 10))\ntidy = words_df.melt(id_vars='writer_full_name').rename(columns=str.title)\nsns.barplot(x='Writer_Full_Name', y='Value', hue='Variable', data=tidy, ax=ax1)\nsns.despine(fig)","f932fa7f":"from nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer","c41dd1aa":"lemm = WordNetLemmatizer()","c5fcb38e":"def word_lemmatizer(text):\n    token_text = nltk.word_tokenize(text)\n    remove_stop = [lemm.lemmatize(w) for w in token_text]\n    join_text = ' '.join(remove_stop)\n    return join_text","e2f4f79b":"lemmatize_data = new_stop_data.copy()\nlemmatize_data['text'] = new_stop_data.text.apply(lambda x : word_lemmatizer(x))\nlemmatize_data.head()","0e1ec48c":"from sklearn.feature_extraction.text import CountVectorizer\ncount_vec = CountVectorizer(stop_words='english')\ndata_count_vec = count_vec.fit_transform(lemmatize_data.text)\ndata_count_vec","4f2dc68f":"data_count_df = pd.DataFrame(data_count_vec.toarray(), columns=count_vec.get_feature_names())\ndata_count_df.index = lemmatize_data.author\ndata_count_df","6906c814":"from sklearn.decomposition import NMF, LatentDirichletAllocation","14b3d5d2":"lda_model = LatentDirichletAllocation(n_components=5, max_iter=5, learning_method = 'online', learning_offset = 50.,random_state = 0)","9e473f4f":"lda_model.fit(data_count_vec)","acddc1e5":"lda_model.components_","e73fe1e9":"print_words = 20\nget_feature_names = count_vec.get_feature_names()\nfor index, topic in enumerate(lda_model.components_):\n    words = \" \".join([get_feature_names[i] for i in topic.argsort()[:-print_words - 1 :-1]])\n    print(f\"Topic - {index}:\")\n    print(words)\n    print(\"-\"*100)\n    print('\\n')","03b3277e":"from gensim import matutils, models\nimport scipy.sparse","f6d32fea":"data_count_df.index.name = None\nnew_dtm_t_data = data_count_df.T","5bfad0d0":"spare_counts = scipy.sparse.csr_matrix(new_dtm_t_data)\nnew_corpus = matutils.Sparse2Corpus(spare_counts)\nnew_corpus","63e1b2b5":"cv = count_vec\nid2word = dict((v, k) for k, v in cv.vocabulary_.items())\n# id2word","40cab961":"gensim_lda_topic = models.LdaModel(corpus=new_corpus, id2word=id2word, num_topics=5, passes=10)\ngensim_lda_topic.print_topics()","508f4425":"## Way-1 Latent Dirichlet Allocation","0963ad93":"# Vectorizing","8dd931a4":"# Process\n* Data Cleaning\n* Stopword Removal\n* Find out common words\n* EDA\n* Lemmatization\n* Vectorizing\n* Tokenization\n* Topic Modelling","a0b27023":"\n\n<center><img style=\"width: 700px;\" src=\"https:\/\/miro.medium.com\/max\/638\/0*Sj65xR38wDwuxhtr.jpg\"><\/center>\n\n---\n<i>Source: Base image from Google<\/i>","e7b7e1a2":"## Topic List\n1. Topic 1: friend, gentleman\n1. Topic 2: life, love\n1. Topic 3: sea, night\n1. Topic 4: sea, water\n1. Topic 5: man, moon","d8e9fd56":"## Topic List\n1. Topic 1: sea, water\n1. Topic 2: house, room\n1. Topic 3: dream, star\n1. Topic 4: time, table\n1. Topic 5: friend, heart","dd804804":"## Way-2 LDA with gensim ","67b9cc7c":"# Stopword Removal","5dba3c59":"# Data Cleaning","24158e38":"---\n\n<h1 style=\"text-align: center;font-size: 30px; color: #013b86;\">NLP and Topic Modelling<\/h1>\n\n---\n\n<center><img style=\"width: 700px;\" src=\"https:\/\/miro.medium.com\/max\/2796\/1*jpytbqadO3FtdIyOjx2_yg.png\"><\/center>\n\n---\n<i>Source: Base image from Google<\/i>","e97af401":"# Lemmatization","4addcd2e":"## Find out common words","fadd5935":"# EDA","6936949a":"# Topic modelling"}}