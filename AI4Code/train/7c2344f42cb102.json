{"cell_type":{"984ae94d":"code","1d3655fe":"code","c095951a":"code","badbe1c8":"code","b4eb2ed1":"code","8dab7e0f":"code","895340f3":"code","a9749c98":"code","7ab79be2":"code","29daf5a6":"code","23849336":"code","f98c85bc":"code","554cb884":"code","c90b8f13":"code","2af7abe4":"code","334f6e5a":"code","3bb92ec7":"code","b2c0b074":"code","96038adc":"code","02cec100":"code","75ab201f":"code","96bde463":"code","62500c2c":"code","0d1d32d4":"code","bd83863d":"code","5d18791a":"code","6619551f":"code","6977422c":"code","7c476870":"code","b26b2e78":"code","3c3cb2ce":"code","36e87cf9":"code","3c9a6318":"code","d9d9866e":"code","8debe615":"code","0d743d9f":"code","fbbe840e":"code","6217ffcc":"code","f2b22c7b":"code","56ac8be4":"code","c8dc052e":"markdown","289e63c8":"markdown","9565ca17":"markdown","b79efcc4":"markdown","a7412d4a":"markdown","2105220d":"markdown","6f03c142":"markdown","11789498":"markdown","34f01848":"markdown","e43867bd":"markdown","0d00af3b":"markdown","9db2b02f":"markdown","488a3156":"markdown","31215bd7":"markdown","70264fcb":"markdown","03f461ff":"markdown","a91fe81c":"markdown","4db607b2":"markdown","d3426f37":"markdown","f33ed624":"markdown","b0ffbeaf":"markdown"},"source":{"984ae94d":"from fastai import *\nfrom fastai.text import *\nfrom fastai.callbacks import *\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport re\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nprint(\"Cuda available\" if torch.cuda.is_available() is True else \"CPU\")\nprint(\"PyTorch version: \", torch.__version__)","1d3655fe":"train_df = pd.read_csv('..\/input\/train.csv', nrows=10000)\ntest_df = pd.read_csv('..\/input\/test.csv', nrows=2000)","c095951a":"train_df.head()","badbe1c8":"test_df.head()","b4eb2ed1":"len(train_df), len(test_df)","8dab7e0f":"train_df['label'] = (train_df['target'] >= 0.4)\n","895340f3":"train_df['label'].value_counts()","a9749c98":"train_df[['target','comment_text','label']].sample(10)","7ab79be2":"train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['label'])","29daf5a6":"len(train_df), len(val_df)","23849336":"data_lm = TextLMDataBunch.from_df(\n    path='',\n    train_df=train_df,\n    valid_df=val_df,\n    test_df=test_df,\n    text_cols=['comment_text'],\n    label_cols=['label'],\n    #label_cols=['target_better'],\n    #classes=['target_better'],\n    min_freq=3\n)","f98c85bc":"data_lm.show_batch()","554cb884":"x,y = next(iter(data_lm.train_dl))\nexample = x[:15,:15].cpu()\ntexts = pd.DataFrame([data_lm.train_ds.vocab.textify(l).split(' ') for l in example])\ntexts","c90b8f13":"learn = language_model_learner(data_lm, arch=AWD_LSTM, drop_mult=0.6)","2af7abe4":"learn.lr_find(start_lr=1e-6, end_lr=1e2)\nlearn.recorder.plot()","334f6e5a":"learn.fit_one_cycle(cyc_len=3, max_lr=1e-01)","3bb92ec7":"learn.unfreeze()\nlearn.fit_one_cycle(cyc_len=5, max_lr=1e-3, moms=(0.8, 0.7))","b2c0b074":"learn.save_encoder('ft_enc')","96038adc":"learn.predict(\"Thank\",n_words=5)","02cec100":"data_class = TextClasDataBunch.from_df(\n    path='',\n    train_df=train_df,\n    valid_df=val_df,\n    test_df=test_df,\n    text_cols=['comment_text'],\n    label_cols=['label'],  \n    min_freq=3,\n    vocab=data_lm.train_ds.vocab,\n    #label_delim=' '\n)","75ab201f":"iter_dl = iter(data_class.train_dl)\n_ = next(iter_dl)\nx,y = next(iter_dl)\nx[-10:,:10]","96bde463":"learn = text_classifier_learner(data_class, arch=AWD_LSTM, drop_mult=0.6)\nlearn.load_encoder('ft_enc')\nlearn.freeze()","62500c2c":"learn.lr_find(start_lr=1e-8, end_lr=1e2)\nlearn.recorder.plot()","0d1d32d4":"learn.fit_one_cycle(cyc_len=2, max_lr=1e-2)\n#learn.fit_one_cycle(1, 1e-2)","bd83863d":"learn.freeze_to(-2)\nlearn.fit_one_cycle(3, slice(1e-4,1e-2))","5d18791a":"learn.unfreeze()","6619551f":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","6977422c":"\nlearn.fit_one_cycle(10, slice(1e-5,1e-3),callbacks=[SaveModelCallback(learn, name=\"best_lm\")])","7c476870":"learn.recorder.plot_losses()","b26b2e78":"preds = learn.get_preds(ds_type=DatasetType.Test, ordered=True)","3c3cb2ce":"\np = preds[0][:,1]\ntest_df['prediction'] = p\ntest_df.sort_values('prediction', inplace=True)\ntest_df.reset_index(drop=True, inplace=True)\nii = 100\nprint(test_df['comment_text'][ii])\nprint(test_df['prediction'][ii])\n","36e87cf9":"learn.show_results()","3c9a6318":"interp = ClassificationInterpretation.from_learner(learn)","d9d9866e":"interp.plot_confusion_matrix()","8debe615":"preds = learn.get_preds(ds_type=DatasetType.Valid, ordered=True)\np = preds[0][:,1]","0d743d9f":"preds,y, loss = learn.get_preds(with_loss=True)\n# get accuracy\nacc = accuracy(preds, y)\nprint('The accuracy is {0} %.'.format(acc))","fbbe840e":"from sklearn.metrics import roc_curve, auc\n# probs from log preds\nprobs = np.exp(preds[:,1])\n# Compute ROC curve\nfpr, tpr, thresholds = roc_curve(y, probs, pos_label=1)\n\n# Compute ROC area\nroc_auc = auc(fpr, tpr)\nprint('ROC area is {0}'.format(roc_auc))","6217ffcc":"plt.figure()\nplt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.xlim([-0.01, 1.0])\nplt.ylim([0.0, 1.01])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")","f2b22c7b":"interp2 = TextClassificationInterpretation.from_learner(learn) \ninterp2.show_top_losses(10)","56ac8be4":"print(interp2.show_intrinsic_attention(\"Thank you, for your comments\"))","c8dc052e":"ULMFiT is essentially a method to enable transfer learning for any NLP task and achieve great results. All this, without having to train models from scratch. \n\nULMFiT achieves state-of-the-art result using novel techniques like:\n\nDiscriminative fine-tuning\nSlanted triangular learning rates, and\nGradual unfreezing\n\nLink to paper -\nhttps:\/\/arxiv.org\/abs\/1801.06146","289e63c8":"Visualizing tokenization\nULMFit doesn't feed the different texts separately but concatenate them all together in a big array. To create the batches, it splits this array into bs chunks of continuous texts.","9565ca17":"Graph clearly shows high variance and overfitting. One reason could be data imbalance","b79efcc4":"## Data Preprocessing\n\nThough Jeremy discourages cleaning of input data (removing stop words, part of speech etc), we can do using NLTK libraries if the need arises. Although I have not seen much impact of data cleaning on the overall model accuracy after training.","a7412d4a":"data is padded with token 1 at beginning","2105220d":"Confusion matrix can help us understand the ratio of false negatives and positives and it's a fast way looking at our model's performance. This is a simple table that shows the counts in a way of actual label vs. predicted label. ","6f03c142":"As we can see from the figure that model is skewed in favor of the high freaquency class due to data imbalance. ","11789498":"1cycle policy\n\nWe will use the one cycle policy proposed by Leslie Smith, arXiv, April 2018. The policy brings more disciplined approach for selecting hyperparameters such as learning rate and weight decay. This can potentially save us a lot of time from training with suboptimal hyperparameters. In addititon, Fastai library has implemented a training function for one cycle policy that we can use with only a few lines of code.\n","34f01848":"Splitting data 20% for validation at each epoch","e43867bd":"## Training Text Classifier","0d00af3b":"### Classifier data\nsince the text don't all have the same length, LM can't easily collate them together in batches. To help with this fastai uses two different techniques:\n\n* padding: each text is padded with the PAD token to get all the ones picked to the same size. \n* sorting the texts (ish): to avoid having together a very long text with a very short one (which would then have a lot of PAD tokens), it regroups the texts by order of length.\n* \nThis is all done behind the scenes by fastai library","9db2b02f":"For simplicity I am only taking 10K rows, but eventually we will train on all data","488a3156":"We can see the contribution of each token to the classifier model","31215bd7":"### Fine Tuning Language Model","70264fcb":"### Language Model\n\n    ","03f461ff":"## Overview of ULMFiT","a91fe81c":"We create a language model based on the given vocabulary and the architecture AWD_LSTM. We only take into account the comment_text input. To quickly gain understanding of the dataset and the potential performance of our model, we will only take a subset of  sentences from our original dataset df_train. Eventually, we will train our model on the entire dataset.","4db607b2":"## Import Libraries","d3426f37":"Apart from AWD_LSTM, I will try with other available architectures like TransformerXL\n\nLink to AWD_LSTM paper\nhttps:\/\/arxiv.org\/pdf\/1708.02182","f33ed624":"New labels True, False. True if target value >0.4","b0ffbeaf":"This method involves fine-tuning a pre-trained language model (LM), trained on the Wikitext 1 Million token dataset to your own custom datset same as we do transfer learning in image processing."}}