{"cell_type":{"008ad1ea":"code","8fa88438":"code","9fb0957d":"code","139a3285":"code","3de4504c":"code","ce656116":"code","3081decc":"code","9b3de5e4":"code","8183c7d1":"code","b9a63fed":"code","5b0f9961":"code","b45ccfcb":"code","cc9d463c":"code","d40eb6c8":"code","6a6b1fc7":"code","69b8adf6":"code","d9915d87":"code","c40efca0":"code","4fd75555":"code","20f6ebea":"code","ac472e24":"code","050b748c":"code","9a8699d9":"code","225d5435":"code","c0a4505a":"code","90fa708d":"code","efc813b2":"code","9cfd50e9":"code","058a88ed":"code","f056373b":"code","b0b44b6b":"code","fe83e691":"code","d736cbce":"code","1e74b612":"code","8b3878d5":"markdown","d0b44a63":"markdown","ca5ba1d2":"markdown","bac12f7e":"markdown","e451926f":"markdown","f2c88188":"markdown","f3359b5d":"markdown","a8553618":"markdown","3d31d71a":"markdown","64901c85":"markdown","1f0c9ec1":"markdown","67be7e64":"markdown","1cabae3e":"markdown","60330ed1":"markdown","f12b157c":"markdown","ab86753e":"markdown","b4efb550":"markdown","11e5e2e9":"markdown","f1ae87ad":"markdown","37413227":"markdown","9ce75cbd":"markdown","1f86a608":"markdown","3c18a89d":"markdown","30557935":"markdown","a586969f":"markdown","7d61c989":"markdown","ca613bc8":"markdown","7109768b":"markdown","bb50c5d6":"markdown","0e72f917":"markdown","9a378400":"markdown","5ba6517f":"markdown"},"source":{"008ad1ea":"# import libraries \nimport pandas as pd # Import Pandas for data manipulation using dataframes\nimport numpy as np # Import Numpy for data statistical analysis \nimport matplotlib.pyplot as plt # Import matplotlib for data visualisation\nimport seaborn as sns # Statistical data visualization\n\n%matplotlib inline","8fa88438":"# Import Cancer data drom the Sklearn library\nfrom sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()","9fb0957d":"cancer","139a3285":"cancer.keys()","3de4504c":"print(cancer['DESCR'])\nprint(cancer['target_names'])","ce656116":"print(cancer['feature_names'])\nprint(cancer['data'])","3081decc":"cancer['data'].shape","9b3de5e4":"df_cancer = pd.DataFrame(np.c_[cancer['data'], cancer['target']], columns = np.append(cancer['feature_names'], ['target']))\n","8183c7d1":"df_cancer.head()","b9a63fed":"df_cancer.tail()","5b0f9961":"sns.pairplot(df_cancer, vars = ['mean radius', 'mean texture', 'mean area', 'mean perimeter', 'mean smoothness'] )","b45ccfcb":"sns.pairplot(df_cancer, hue = 'target', vars = ['mean radius', 'mean texture', 'mean area', 'mean perimeter', 'mean smoothness'] )","cc9d463c":"sns.countplot(df_cancer['target'], label = \"Count\") ","d40eb6c8":"sns.scatterplot(x = 'mean area', y = 'mean smoothness', hue = 'target', data = df_cancer)\n","6a6b1fc7":"# Let's check the correlation between the variables \n\nplt.figure(figsize=(20,10)) \nsns.heatmap(df_cancer.corr(), annot=True) ","69b8adf6":"\n# Let's drop the target label coloumns\nX = df_cancer.drop(['target'],axis=1)\n","d9915d87":"y = df_cancer['target']","c40efca0":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=5)","4fd75555":"print(X_train.shape)\n\nprint(X_test.shape)\n\nprint(y_train.shape)\n\nprint(y_test.shape)","20f6ebea":"from sklearn.svm import SVC \nfrom sklearn.metrics import classification_report, confusion_matrix\n\nsvc_model = SVC()\nsvc_model.fit(X_train, y_train)","ac472e24":"y_predict = svc_model.predict(X_test)\ncm = confusion_matrix(y_test, y_predict)","050b748c":"print(classification_report(y_test, y_predict))\n\nsns.heatmap(cm, annot=True)","9a8699d9":"sns.scatterplot(x= 'mean area', y = 'mean smoothness', hue = 'target', data = df_cancer)","225d5435":"min_train = X_train.min()\n\nrange_train = (X_train - min_train).max()","c0a4505a":"X_train_scaled = (X_train - min_train)\/range_train\n\nX_train_scaled","90fa708d":"sns.scatterplot(x = X_train_scaled['mean area'], y = X_train_scaled['mean smoothness'], hue = y_train)","efc813b2":"min_test = X_test.min()\nrange_test = (X_test - min_test).max()\nX_test_scaled = (X_test - min_test)\/range_test","9cfd50e9":"from sklearn.svm import SVC \nfrom sklearn.metrics import classification_report, confusion_matrix\n\nsvc_model = SVC()\nsvc_model.fit(X_train_scaled, y_train)","058a88ed":"y_predict = svc_model.predict(X_test_scaled)\ncm = confusion_matrix(y_test, y_predict)\n\nprint(classification_report(y_test,y_predict))\nsns.heatmap(cm,annot=True,fmt=\"d\")","f056373b":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf']} \n\ngrid = GridSearchCV(SVC(),param_grid,refit=True,verbose=4)\n\ngrid.fit(X_train_scaled,y_train)","b0b44b6b":"grid.best_params_","fe83e691":"grid.best_estimator_","d736cbce":"grid_predictions = grid.predict(X_test_scaled)","1e74b612":"cm = confusion_matrix(y_test, grid_predictions)\n\nprint(classification_report(y_test,grid_predictions))\nsns.heatmap(cm, annot=True)","8b3878d5":"Create scatter plot with different features.","d0b44a63":"Let's import the SVC class and fit the model.","ca5ba1d2":"Here, We have implemented one of the common Machine Learning classification algorithms.\n\nI hope this kernal is useful to you to learn machine learning from the scratch with Breast_Cancer dataset.\n\nIf you find this notebook helpful to you to learn, **Please Upvote**.\n\n*Thank You!!*","bac12f7e":"# Contents \n1. [Problem Statement ](#PS) \n2. [Importing Data](#ID)\n3. [Data Visualization](#DV)\n4. [Model Training](#MT)\n    1. [Support Vector Machine](#SVM)        \n5. [Evaluating the Model](#EM)\n6. [Improving the Model](#IM)\n    1. [Grid Search](#GS)","e451926f":"# STEP 6: Improving the Model <a id='IM'><\/a>","f2c88188":"**If** you are new to Data Science I am sure you will get something reading this.\n\n**Else** your feedback will definitely help me improving this kernel... :)","f3359b5d":"From above confusion matrix, out of 114 entries 48 pridictions (almmost 42%) are wrong.\n\nSo, need to improve the model.","a8553618":"As initially told, there are 30 features and 569 instances.","3d31d71a":"Above plots showing the relationship between different feature but not showing whether cancer is Malignant or Benign ie target variable.\n\nLet's create another set of scatter plots where Malignant or Benign Cancer is classified.","64901c85":"Let's explore the data stored in the cancer keys.","1f0c9ec1":"# STEP 2: Importing Data <a id='ID'><\/a>\n","67be7e64":"# STEP 4: Model Training (Finding a Training Solution) <a id='MT'><\/a>","1cabae3e":"Let's split the dataset into train and test dataset.","60330ed1":"# STEP 5: Evaluating the Model <a id='EM'><\/a>","f12b157c":"# STEP 1: Problem Statement <a id='PS'><\/a>","ab86753e":"Let's create scatter plots among few features.","b4efb550":"- Predicting if the cancer diagnosis is benign or malignant based on several observations\/features \n- 30 features are used, few of the examples are:\n        - radius (mean of distances from center to points on the perimeter)\n        - texture (standard deviation of gray-scale values)\n        - perimeter\n        - area\n        - smoothness (local variation in radius lengths)\n        - compactness (perimeter^2 \/ area - 1.0)\n        - concavity (severity of concave portions of the contour)\n        - concave points (number of concave portions of the contour)\n        - symmetry \n        - fractal dimension (\"coastline approximation\" - 1)\n\n- Datasets are linearly separable using all 30 input features\n- Number of Instances: 569\n- Class Distribution: 212 Malignant, 357 Benign\n- Target class:\n         - Malignant\n         - Benign\n\n\n[Data can be downloaded from here.](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+(Diagnostic))","11e5e2e9":"So, these are the best parameters used for the particular model and dataset.\n\nLet's make prediction based on these hyperparameters. ","f1ae87ad":"Again create a model using Support Vector Machine.","37413227":"## Support Vector Machine <a id='SVM'><\/a>\n \nSVM is a supervised machine learning algorithm which can be used for classification or regression problems. It uses a technique called the kernel trick to transform the data and then based on these transformations it finds an optimal boundary between the possible outputs. Simply put, it does some extremely complex data transformations, then figures out how to seperate data based on the labels or outputs that are defined.\n\n**So what makes it so great?**\n \nWell SVM, it is capable of doing both classification and regression. In this post we'll focus on using SVM for classification. In particular I'll be focusing on non-linear SVM, or SVM using a non-linear kernel. Non-linear SVM means that the boundary that the algorithm calculates doesn't have to be a straight line. The benefit is that we can capture much more complex relationships between datapoints without having to perform difficult transformations on our own. The downside is that the training time is much longer as it's much more computationally intensive.","9ce75cbd":"# STEP 3: Visualising the Data <a id='DV'><\/a>","1f86a608":"One of the reason for such error is the range of data sets. From above we can say that the range for *mean smoothness* is from *0.04 to 0.18* while for the *mean area* is *0 to 2500*.\n\nSo, the solution is to set the range from 0 to 1 or we can say to normalise the data(normalization)\n\n![image.png](attachment:image.png)","3c18a89d":"Let's explore the data..","30557935":"From above *confusion matrix*, we can say the error is further reduced ie 4% to 2.6% and these are the best hyperparameters that are selected for this particular model.","a586969f":"** Hello Kaggler!**\n\nHere is a detailed case study from scratch to recognise the Breast Cancer based on few input features using Support Vector Machine.","7d61c989":"Let's create a plot showing the number of incidences of target variable (already given but still lets create!)","ca613bc8":"# Improving the Model - Part 2","7109768b":"By observing the above *Confusion Matrix* we can say, the error reduced from *42% to 4%* only.","bb50c5d6":"Let's create a Data Frame of all cancer features.","0e72f917":"# Grid Search <a id='GS'><\/a>\n\nThis technique is used to find the optimal parameters to use with an algorithm. This is NOT the weights or the model, those are learned using the data. This is obviously quite confusing so I will distinguish between these parameters, by calling one hyper-parameters.\n\nHyper-parameters are like the *c* or *gamma* or *kernel* in *SVM*. SVM requires the user to select which neighbor to consider when calculating the distance for *hyperplane*. The algorithm then tunes a parameter, a threshold, to see if a novel example falls within the learned distribution, this is done with the data.\n\n\n**How does it work?**\n\nFirst we need to build a grid. This is essentially a set of possible values your hyper-parameter can take. For our case we can use for example *{'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf']}[1,2,3,...,10]*. Then we will train our model for each value in the grid. First it would take '*C*' and do for the remaining *gamma* and *kernel* and so on. For each iteration, we will get a performance score which will tell us how well the algorithm performed using that value for the hyper-parameter. After we have gone through the entire grid we will select the value that gave the best performance.\n\n","9a378400":"Strong correlation between the mean radius and mean perimeter, mean area and mean primeter","5ba6517f":"Similarily, Normalize the test data."}}