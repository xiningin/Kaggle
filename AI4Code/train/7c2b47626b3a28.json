{"cell_type":{"e0ecdd03":"code","1abf2e36":"code","e1589b14":"code","ef0f7600":"code","a0d0c3b8":"code","d8ecae75":"code","92df679d":"code","2fecb63a":"code","1c3a5201":"code","ff00bbf6":"code","2affb1f6":"code","276db69e":"code","327460b1":"code","d6bc8723":"code","e58335f6":"code","b6b202b6":"code","a15a5e13":"code","1b3a08af":"code","2372e14d":"code","fd5c6704":"code","cbbb40ef":"code","a67d71c9":"code","33224303":"code","8559f08a":"code","49f8fdef":"code","904fcedf":"code","1ac5e095":"code","b8b0b184":"code","a8229b4e":"code","3ac5cbf1":"code","0541bf4b":"code","84322876":"code","5ac3d752":"code","805f38f7":"markdown","f503e0f2":"markdown","beb1760d":"markdown","d3a732ff":"markdown","df7fda25":"markdown","e49bc422":"markdown","fe13e11c":"markdown","4d419527":"markdown","f609410b":"markdown","311881c3":"markdown","caa6920a":"markdown","42f7fbcb":"markdown","f11792f5":"markdown","da7d9cf5":"markdown","087692d4":"markdown","e68912c6":"markdown","d68b1613":"markdown","8bffa13a":"markdown","31c68d43":"markdown","c5fcaedd":"markdown","a8eda44e":"markdown","171ed8ff":"markdown","75cd0fe0":"markdown","d06f41d1":"markdown","2918980f":"markdown","d781d48b":"markdown","2ef8f405":"markdown","681cc222":"markdown","ec0f1fec":"markdown","f79ede6c":"markdown","5e3863b9":"markdown","1872e68f":"markdown","90de96cd":"markdown","c819e1f4":"markdown"},"source":{"e0ecdd03":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","1abf2e36":"\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly as py\n\nimport plotly.express as px\nimport cufflinks as cf\n\n%matplotlib inline\n\nfrom plotly.offline import iplot,plot, download_plotlyjs, init_notebook_mode\n\ninit_notebook_mode()\ncf.go_offline()\nimport plotly.graph_objects as go\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n","e1589b14":"init_notebook_mode()","ef0f7600":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","a0d0c3b8":"#Let's have a look of the data.\ndf.shape","d8ecae75":"df.info()","92df679d":"#First Let's have a look how unbalanced our dataset is.\nimport plotly.graph_objects as go\nfig = px.pie(df, names = 'Class',\n       title = 'Comparision between Fraud and Legitimate cases',\n       color_discrete_sequence =px.colors.sequential.RdBu)\n\nfig.update_traces(hoverinfo = 'label + percent',\n                  textfont_size = 10,\n                  textinfo = 'label+percent',\n                  pull = [.1,0],\n                  marker = dict( line = dict( color = 'Blue',width = 6) )\n                 )\nfig","2fecb63a":"Fraud = df[df['Class'] == 1]\nNormal =  df[df['Class'] == 0]\noutlier_fraction = (len(Fraud)\/len(Normal))\nprint(outlier_fraction)","1c3a5201":"print(Fraud.shape, Normal.shape)","ff00bbf6":"#For the computitional purposes working only with a fraction of the main dataset.But after making the model based on tis model we will test it by whole data.\ndf1 = df.sample(frac = .1, random_state = 1)","2affb1f6":"df1.shape","276db69e":"X = df1.drop(['Class'], axis = 1)\ny = df1.Class","327460b1":"#spliting the main data into two part to Checj  before and after effect. \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","d6bc8723":"X_train.shape","e58335f6":"from imblearn.under_sampling import NearMiss","b6b202b6":"nm = NearMiss(sampling_strategy=0.1)\nX_res, y_res = nm.fit_resample(X_train, y_train)\nfrom collections  import Counter\nprint('Original dataset shape{}'.format(Counter(y_train)))\nprint('Resampled dataset shape{}'.format(Counter(y_res)))\n","a15a5e13":"model = LogisticRegression()\nmodel.fit(X_res,y_res)\ny_pred = model.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score, classification_report\nprint(accuracy_score(y_test,y_pred))\nprint('Confussion_metrics:\\n')\nprint(pd.crosstab(y_test,y_pred))\nprint('\\nClassification_report:\\n')\nprint(classification_report(y_test, y_pred))","1b3a08af":"model = RandomForestClassifier()\nmodel.fit(X_res,y_res)\ny_pred = model.predict(X_test)\n\nprint(accuracy_score(y_test,y_pred))\nprint('Confussion_metrics:\\n')\nprint(pd.crosstab(y_test,y_pred))\nprint('\\nClassification_report:\\n')\nprint(classification_report(y_test, y_pred))","2372e14d":"model = DecisionTreeClassifier()\nmodel.fit(X_res,y_res)\ny_pred = model.predict(X_test)\n\nprint(accuracy_score(y_test,y_pred))\nprint('Confussion_metrics:\\n')\nprint(pd.crosstab(y_test,y_pred))\nprint('\\nClassification_report:\\n')\nprint(classification_report(y_test, y_pred))","fd5c6704":"model = KNeighborsClassifier()\nmodel.fit(X_res,y_res)\ny_pred = model.predict(X_test)\n\nprint(accuracy_score(y_test,y_pred))\nprint('Confussion_metrics:\\n')\nprint(pd.crosstab(y_test,y_pred))\nprint('\\nClassification_report:\\n')\nprint(classification_report(y_test, y_pred))","cbbb40ef":"model = RandomForestClassifier()\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\n\nprint(accuracy_score(y_test,y_pred))\nprint('Confussion_metrics:\\n')\nprint(pd.crosstab(y_test,y_pred))\nprint('\\nClassification_report:\\n')\nprint(classification_report(y_test, y_pred))","a67d71c9":"from imblearn.over_sampling import RandomOverSampler ","33224303":"ros = RandomOverSampler (sampling_strategy=0.5)\nX_res, y_res = ros.fit_resample(X_train, y_train)\n\nprint('Original dataset shape{}'.format(Counter(y_train)))\nprint('Resampled dataset shape{}'.format(Counter(y_res)))\n","8559f08a":"#Random forest classifier\nmodel = RandomForestClassifier()\nmodel.fit(X_res,y_res)\ny_pred = model.predict(X_test)\n\nprint(accuracy_score(y_test,y_pred))\nprint('Confussion_metrics:\\n')\nprint(pd.crosstab(y_test,y_pred))\nprint('\\nClassification_report:\\n')\nprint(classification_report(y_test, y_pred))","49f8fdef":"# import library\nfrom imblearn.under_sampling import TomekLinks\n\ntl = RandomOverSampler(sampling_strategy='majority')\n\n# fit predictor and target variable\nx_tl, y_tl = ros.fit_resample(X_train, y_train)\n\nprint('Original dataset shape', Counter(y))\nprint('Resample dataset shape', Counter(y_tl))","904fcedf":"#Random forest classifier\nmodel = RandomForestClassifier()\nmodel.fit(X_res,y_res)\ny_pred = model.predict(X_test)\n\nprint(accuracy_score(y_test,y_pred))\nprint('Confussion_metrics:\\n')\nprint(pd.crosstab(y_test,y_pred))\nprint('\\nClassification_report:\\n')\nprint(classification_report(y_test, y_pred))","1ac5e095":"# import library\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\n\n# fit predictor and target variable\nx_smote, y_smote = smote.fit_resample(X_train, y_train)\n\nprint('Original dataset shape', Counter(y_train))\nprint('Resample dataset shape', Counter(y_smote))","b8b0b184":"#Random forest classifier\nmodel = RandomForestClassifier()\nmodel.fit(X_res,y_res)\ny_pred = model.predict(X_test)\n\nprint(accuracy_score(y_test,y_pred))\nprint('Confussion_metrics:\\n')\nprint(pd.crosstab(y_test,y_pred))\nprint('\\nClassification_report:\\n')\nprint(classification_report(y_test, y_pred))","a8229b4e":"from sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor","3ac5cbf1":"classifier = {\n    \n    'Isolation Forest' : IsolationForest(n_estimators =  100, max_features  = 5,random_state= 3,\n                                          contamination = outlier_fraction) ,\n    \n    'Local Outlier Factor' : LocalOutlierFactor( contamination = outlier_fraction) \n}","0541bf4b":"for i, (clf_name, clf) in enumerate(classifier.items()):\n    if clf_name == 'Isolation Forest':\n        clf.fit(X_train)\n        scores_prediction = clf.decision_function(X_test)\n        y_pred = clf.predict(X_test)\n    else:\n        y_pred = clf.fit_predict(X_test)\n        scores_prediction = clf.negative_outlier_factor_\n        \n        \n    y_pred[y_pred == 1 ] = 0\n    y_pred[y_pred == -1 ] = 1\n    \n    n_errors = (y_pred!= y_test).sum()\n    print(clf_name, ':', n_errors)\n    print('Confussion_metrics:\\n')\n    print(pd.crosstab(y_test,y_pred))\n    print('Classifiation_report:')\n    print(classification_report(y_pred, y_test))\n        \n    ","84322876":"rf = LogisticRegression(class_weight=\"balanced\")\nrf.fit(X_train, y_train)\n\n# Predicting on the test data\ny_pred = rf.predict(X_test)\n\nprint(accuracy_score(y_test,y_pred))\nprint('Confussion_metrics:\\n')\nprint(pd.crosstab(y_test,y_pred))\nprint('\\nClassification_report:\\n')\nprint(classification_report(y_test, y_pred))","5ac3d752":"\nlr = LogisticRegression( class_weight={0: 0.1, 1: 0.9},max_iter = 1000)\nlr.fit(X_train, y_train)\n\n# Predicting on the test data\ny_pred = lr.predict(X_test)\n\nprint(accuracy_score(y_test,y_pred))\nprint('Confussion_metrics:\\n')\nprint(pd.crosstab(y_test,y_pred))\nprint('\\nClassification_report:\\n')\nprint(classification_report(y_test, y_pred))","805f38f7":"# KNeighborsClassifier","f503e0f2":"#  Logistic Regression (manual class weights):\nFinally, we are trying to find optimal weights with the highest score using grid search. We will search for weights between 0 to 1. The idea is, if we are giving n as the weight for the minority class, the majority class will get 1-n as the weights.\n\nHere, the magnitude of the weights is not very large but the ratio of weights between majority and minority class will be very high.\n\nExample:\n\nw1 = 0.95\n\nw0 = 1 \u2013 0.95 = 0.05\n\nw1:w0 = 19:1\n\nSo, the weights for the minority class will be 19 times higher than the majority class.","beb1760d":"# Under-sampling: Tomek links\n    \nTomek links are pairs of very close instances but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process.\n\nTomek\u2019s link exists if the two samples are the nearest neighbors of each other.\n\n![image.png](attachment:image.png)\n\n\n\n","d3a732ff":"# 1.Isolation Forest.\nReturn the anomaly score of each sample using the IsolationForest algorithm\n\nThe IsolationForest \u2018isolates\u2019 observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\n\nSince recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.\n\nThis path length, averaged over a forest of such random trees, is a measure of normality and our decision function.\n\nRandom partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.\n![image.png](attachment:image.png)\n","df7fda25":"Most of the machine learning models provide a parameter called class_weights. For example, in a random forest classifier using, class_weights we can specify a higher weight for the minority class using a dictionary.\n\nfrom sklearn.linear_model import LogisticRegressionclf = LogisticRegression(class_weight={0:1,1:10})\n\n\nBut what happens exactly in the background?\n\nIn logistic Regression, we calculate loss per example using binary cross-entropy:\n\nLoss = \u2212ylog(p) \u2212 (1\u2212y)log(1\u2212p)\n\n\nIn this particular form, we give equal weight to both the positive and the negative classes. When we set class_weight as class_weight = {0:1,1:20}, the classifier in the background tries to minimize:\n\nNewLoss = \u221220*ylog(p) \u2212 1*(1\u2212y)log(1\u2212p)\n\n\nSo what happens exactly here?\n\nIf our model gives a probability of 0.3 and we misclassify a positive example, the NewLoss acquires a value of -20log(0.3) = 10.45\nIf our model gives a probability of 0.7 and we misclassify a negative example, the NewLoss acquires a value of -log(0.3) = 0.52\nThat means we penalize our model around twenty times more when it misclassifies a positive minority example in this case.","e49bc422":"# Challenges of fraud detection model:","fe13e11c":"Oversampling can be defined as adding more copies to the minority class. Oversampling can be a good choice when you don\u2019t have a ton of data to work with.\n\nA con to consider when undersampling is that it can cause overfitting and poor generalization to your test set.\n![image.png](attachment:image.png)","4d419527":"Here we are seeing that this is the best technique that we have applied till now.","f609410b":"# Synthetic Minority Oversampling Technique (SMOTE)\nThis technique generates synthetic data for the minority class.\n\nSMOTE (Synthetic Minority Oversampling Technique) works by randomly picking a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors\n\n![image.png](attachment:image.png)\n\nSMOTE algorithm works in 4 simple steps:\n\n1.Choose a minority class as the input vector\n\n2.Find its k nearest neighbors (k_neighbors is specified as an argument in the SMOTE() function)\n\n3.Choose one of these neighbors and place a synthetic point anywhere on the line joining the point under consideration and its chosen neighbor\n\n4.Repeat the steps until data is balanced","311881c3":"# Credit card Fraud:\n\nCredit card fraud is when someone uses another person's credit card or account information to make unauthorized purchases or access funds through cash advances. Credit card fraud doesn\u2019t just happen online; it happens in brick-and-mortar stores, too. As a business owner, you can avoid serious headaches \u2013 and unwanted publicity \u2013 by recognizing potentially fraudulent use of credit cards in your payment environment.","caa6920a":"Here we are seeing that RandomForestClassifier giving us a good accuracy. And a decent percentage in  'precision,    recall,  f1-score'\ncomparing to others model.\n\nBut if we  compare Our RandomForestClassifier itself with afer and before applying under_sampling we are getting a good result from the model without applying the under_sampling.\n\n\nBut this is not good enough because at the time of undersampling we are lossing a lot's of data. So we cann't\nexpect a good figure by undersampling.","42f7fbcb":"Only 492 fraud cases are available.So if we do make a model With this will have model of more then 90% accuracy because more then 99% cases are legitimate.\nBut our objective is find fraud cases more as possible.","f11792f5":"# Random over sampling","da7d9cf5":"# Sampling Methods:\n\nAs our main goal is to handling the unbalanced data let's talk about some technique to do it.\n\n1.Random over-sampling:\n\nBy using this method we can increase the amount of minority data points creating duplicate.\n\n\n2.Random under-sampling:\n\n By using this method we can decrease the amount of majority data points by deleting them.\n \n3.Both:\n\nIn this method we combine the above two methods.","087692d4":"1.Unbalanced data:\n\nHere the main problem we are goingh to face is unbalanced data. Because According to the survey only .5% of\ntransactions are fraud, rest of them are legitamate.So here main task is to handle this unbalanced data.\n\n2.Operational effiency:\n\nLess then 8 sec to flag a transaction.Model have take decision so quick by seeing the process. So it's very hard to keep the efficiency.\n\n3.Incorrect flagging:\n\nAvoid harrasing the real customers.","e68912c6":"# Logistic_Regression:","d68b1613":"In this case it's not seem to be satisfying.","8bffa13a":"Undersampling can be defined as removing some observations of the majority class. This is done until the majority and minority class is balanced out.\n\nUndersampling can be a good choice when you have a ton of data -think millions of rows. But a drawback to undersampling is that we are removing information that may be valuable.\n\n![image.png](attachment:image.png)\n","31c68d43":"Here we are seeing how imbalanced  the dataset is. And if we train our model with this data there is strong possibility that model is going to biased towards majority lagitimate class. ","c5fcaedd":"Now let's come into the original part. Here first of all we will apply all the method on the data and then train our model by this \nnew data. Finally we will check how does all the tecnique effect in our model accuracy comparing to model fitted by original dataset. ","a8eda44e":"Now let's try some algorithm to handling the inbalanced data.","171ed8ff":"By reading the description we have come to known, that we have this dataset by placing PCA transformation into the real dataset, due top the confidensiality issue.\nSo we don't what actually specipying by the features from V1 to V28. Dimentionality reduction is already placed here.\nWe can have EDA here. But real challenges is anything else here. So let's deal with Challenges that we are going to face here.","75cd0fe0":"# Class weights in the models\n\n\n ![image.png](attachment:image.png)","d06f41d1":"# Work in progress....","2918980f":"# Logistic Regression (class_weight=\u2019balanced\u2019):\nWe have added the class_weight parameter to our logistic regression algorithm and the value we have passed is \u2018balanced\u2019.\n","d781d48b":"# Model selection:","2ef8f405":"# 2.Local Outlier Factor:\n![image.png](attachment:image.png)","681cc222":"## Check with original fractioned data","ec0f1fec":" Here we are seeing that by oversampling we are getting far more better result then under_sampling and original dataset. ","f79ede6c":"# Objective:\n Here our main objective is to helping in detecting the fraud while transaction through Credit Card.\n Before dig deep into task let's have us a domain knowledge.","5e3863b9":"# DecisionTreeClassifier","1872e68f":"![image.png](attachment:image.png)","90de96cd":"# Applying Sampling method:\nUnder_sampling:","c819e1f4":"# RandomForestClassifier"}}