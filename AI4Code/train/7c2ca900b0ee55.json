{"cell_type":{"ebb4af54":"code","82f21905":"code","a43f7146":"code","48428bb5":"code","87a51b60":"code","5d1af2a5":"code","6e052efe":"code","1767d5f4":"code","945a7384":"code","873fb8c0":"code","4d26e2e7":"code","c0116a1f":"code","57ad53be":"code","ad8d7a2b":"code","1bb06d34":"code","917dd4fd":"code","c309eeaa":"code","b92ba4d1":"code","674b4ef7":"code","3dfa13f1":"code","83c0a193":"code","ef3cd330":"code","e9fb53a5":"code","e6d40168":"code","3147efb5":"code","bfc6142c":"code","95cffb46":"code","f3d3e365":"code","38bbad05":"code","fc6c9e8e":"code","cb0664db":"code","cbe189eb":"code","d01bf7f1":"code","5f5d9c9c":"code","b0322dd5":"code","86e90f85":"code","54efb9b1":"code","47c703bb":"code","a1d5749f":"code","991fdb6e":"code","a65c7580":"code","b6b17551":"code","0d3028bc":"code","87744a71":"code","36c7c3c3":"code","d5581844":"code","c2a41051":"code","f8e9fe3c":"code","89094bc8":"code","431e837a":"code","2d39787d":"code","068e2c7f":"code","623cf822":"code","1f8e0069":"code","8e80af57":"code","bd441dc1":"code","acde93d1":"code","1840c7eb":"code","b1066b40":"code","b0d20069":"code","83fcf8ea":"code","fad4bad7":"code","23854ef7":"code","c134375c":"code","32536b1b":"code","dd55fd30":"code","fa4600d9":"code","180d2def":"code","2fed71e0":"code","76703de1":"code","b359ef74":"code","be89bce6":"code","9e3953b1":"markdown","fe61a388":"markdown","e5737325":"markdown","4095eabb":"markdown","3d1e682c":"markdown","e4c32145":"markdown","21fae715":"markdown","6833e6fb":"markdown","96f6c448":"markdown","2a7df841":"markdown","a0fff865":"markdown","f203b3f0":"markdown","6004bbe8":"markdown","12d44c2d":"markdown","fe69298f":"markdown","eefbb084":"markdown"},"source":{"ebb4af54":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","82f21905":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n#importing libraries to use the library functions","a43f7146":"df=pd.read_csv(\"..\/input\/cardataset\/data.csv\")\n#df contains the file information  which is in csv format","48428bb5":"df","87a51b60":"df.columns\n#listing the column names of the dataset\/dataframe","5d1af2a5":"df.dtypes\n#checking the datatypes of different columns of the dataframe","6e052efe":"df.info()\n#getting the information of dataframe such as no. of entries,data columns,non-null count,data types,etc","1767d5f4":"df.shape\n#shape of the dataframe ie no. of rows and columns","945a7384":"df.describe()\n#checking for statistical summary such as count,mean,etc. of numeric columns","873fb8c0":"df.drop(df[df['MSRP'] == 0].index,inplace=True)\n#dropping rows which have zero as a value for MSRP column as it is our dependent\/target variable.","4d26e2e7":"df.shape","c0116a1f":"df.drop(['Market Category'], axis=1, inplace=True)\n#dropping 'market category' column as MSRP is independent of it and hence not useful in predicting price of car. ","57ad53be":"df.shape","ad8d7a2b":"df=df.rename(columns={'Engine HP':'HP','Engine Cylinders':'Cylinders','Transmission Type':'Transmission','Driven_Wheels':'Drive Mode','highway MPG':'MPG-H','city mpg':'MPG-C','MSRP':'Price'})\n#renaming the column names as per mentioned in the steps of the problem statement","1bb06d34":"df","917dd4fd":"df.duplicated().sum()\n#checking for any duplicates in the data","c309eeaa":"df.drop_duplicates(keep=False,inplace=True)\n#removing the duplicates in the data","b92ba4d1":"df","674b4ef7":"df.isnull().sum()\n#checking for any null values in the data","3dfa13f1":"df.dropna(inplace=True,axis=0)\n#removing the null values in the data","83c0a193":"df.isnull().sum()\n#verfying for any null values","ef3cd330":"sns.boxplot(data=df,orient='h',palette='Set2')\n#checking for any outliers in the data","e9fb53a5":"df.drop(df[df['Price'] >= 500000].index,inplace=True)\n#removing the unnecessary data points from the dataset","e6d40168":"df","3147efb5":"sns.boxplot(x=df['Price'])","bfc6142c":"sns.boxplot(x=df['Cylinders'])","95cffb46":"sns.boxplot(x=df['HP'])","f3d3e365":"Q1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","38bbad05":"df = df[~((df < (Q1 - 1.5*IQR))|(df > (Q3 + 1.5*IQR))).any(axis = 1)]\ndf.shape","fc6c9e8e":"counts=df['Make'].value_counts()*100\/sum(df['Make'].value_counts())\n#calculating percentage of each brand","cb0664db":"popular_labels=counts.index[:10]\n\ncolors=['lightslategray',]*len(popular_labels)\ncolors[0]='crimson'\n\nfig=go.Figure(data=[go.Bar(x=counts[:10],y=popular_labels,marker_color=colors,orientation='h')])\nfig.update_layout(title_text='Most represented Car Brands in the Dataset',xaxis_title=\"Percentage\",yaxis_title=\"Car Brand\")\n#plotting the top 10 brands represented in the dataset","cbe189eb":"prices = df[['Make','Price']].loc[(df['Make'].isin(popular_labels))].groupby('Make').mean()\nprint(prices)\n#calculating the average price of top 10 brands represented in the dataset","d01bf7f1":"display_p=df[['Make','Year','Price']].loc[(df['Make'].isin(popular_labels))]\n\nfig=px.box(display_p,x=\"Make\",y=\"Price\")\nfig.update_layout(title_text='Average Price over 10 most represented Car Brands',xaxis_title=\"Make\",yaxis_title=\"Average Price\")","5f5d9c9c":"df.corr()","b0322dd5":"df_corr=df.corr()\nf,ax=plt.subplots(figsize=(12,7))\nsns.heatmap(df_corr,cmap='viridis',annot=True)\nplt.title(\"Correlation between features\",weight='bold',fontsize=18)\nplt.show()\n\n#plotting the heatmap for different features","86e90f85":"fig,ax = plt.subplots(figsize=(12,7))\nax.scatter(df['HP'],df['Price'])\nax.set_xlabel('HP')\nax.set_ylabel('Price')\nplt.show()","54efb9b1":"fig,ax = plt.subplots(figsize=(12,7))\nax.scatter(df['HP'],df['Price'])\nax.set_xlabel('HP')\nax.set_ylabel('Price')\nplt.show()","47c703bb":"fig,ax = plt.subplots(figsize=(12,7))\nax.scatter(df['HP'],df['Cylinders'])\nax.set_xlabel('HP')\nax.set_ylabel('Cylinders')\nplt.show()","a1d5749f":"#creating new column 'Price Range' for easy visualization\ndef getrange(Price):\n    if (Price >= 0 and Price < 25000):\n        return '0 - 25000'\n    if (Price >= 25000 and Price < 50000):\n        return '25000 - 50000'\n    if (Price >= 50000 and Price < 75000):\n        return '50000 - 75000'\n    if (Price >= 75000 and Price < 100000):\n        return '75000 - 100000'\n       \ndf['Price Range'] = df.apply(lambda x:getrange(x['Price']),axis = 1)\n\ndf['Price Range'].value_counts()","991fdb6e":"#distribution of number of cars over the years\ndic = {1990+i : sum(df['Year']==1990+i) for i in range(28)}\nx_dic = [1990 + i for i in range(28)]\ny_dic = [dic[1990 + i] for i in range(28)]\n\n# Plot\nfig = go.Figure([go.Bar(x=x_dic, y=y_dic)])\n\nfig.update_layout(title=\"Car year distribution\",\n                  xaxis_title=\"Year\",\n                  yaxis_title=\"Count Cars sold\")\n\n\nfig.show()","a65c7580":"plt.rcParams['figure.figsize'] = (15,9)\n\nx = pd.crosstab(df['Price Range'],df['Engine Fuel Type'])\ncolor = plt.cm.copper(np.linspace(0,1,9))\nx.div(x.sum(1).astype(float),axis = 0).plot(kind = 'bar',stacked = True ,color=color)\nplt.title(\"Price vs Engine Fuel Type\",fontweight = 30,fontsize = 20)\nplt.show()","b6b17551":"plt.rcParams['figure.figsize'] = (15,9)\nx = pd.crosstab(df['Price Range'],df['Drive Mode'])\nx.div(x.sum(1).astype(float),axis = 0).plot(kind = 'bar',stacked = False)\nplt.title('Price vs Drive Mode',fontweight = 30,fontsize = 20)\nplt.show()","0d3028bc":"plt.rcParams['figure.figsize'] = (15,9)\nx = pd.crosstab(df['Price Range'],df['Vehicle Size'])\nx.div(x.sum(1).astype(float),axis = 0).plot(kind = 'bar',stacked = False)\nplt.title('Price vs Size',fontweight = 30,fontsize = 20)\nplt.show()","87744a71":"plt.rcParams['figure.figsize'] = (15,9)\n\nx = pd.crosstab(df['Price Range'],df['Vehicle Style'])\nx.div(x.sum(1).astype(float),axis = 0).plot(kind = 'bar',stacked = True)\nplt.title(\"Price vs Vehicle Style\",fontweight = 30,fontsize = 20)\nplt.show()","36c7c3c3":"data_pie = df['Transmission'].value_counts()\n\nfig = go.Figure(data=[go.Pie(labels=data_pie.index, values=data_pie.tolist(), textinfo='label+percent',insidetextorientation='radial')])\n\nfig.update_traces(hole=.3, hoverinfo=\"label+percent+name\")","d5581844":"df.head()","c2a41051":"df.shape","f8e9fe3c":"# performing label encoding to the categorical columns\ncolumns_to_convert=['Make','Model','Engine Fuel Type','Transmission','Drive Mode','Vehicle Size','Vehicle Style','Price Range']\ndf[columns_to_convert] = df[columns_to_convert].astype('category')","89094bc8":"df.dtypes","431e837a":"from sklearn import preprocessing\n  \n# label_encoder object knows how to understand word labels.\nlabel_encoder = preprocessing.LabelEncoder()\n  \n# Encode labels in column 'species'.\nfor col in ['Make','Model','Engine Fuel Type','Transmission','Drive Mode','Vehicle Size','Vehicle Style','Price Range']: df[col] = label_encoder.fit_transform(df[col])","2d39787d":"df.head()","068e2c7f":"# splitting the dependent and independent variables\n\nx = df[['Popularity','Year','HP','Cylinders','MPG-H','MPG-C']].values\ny = df['Price'].values\n\nprint(x.shape)\nprint(y.shape)","623cf822":"#normalizing the data\nfrom sklearn.preprocessing import StandardScaler\nsc_x=StandardScaler()\nsc_y=StandardScaler()\n\nx=sc_x.fit_transform(x)\ny=sc_y.fit_transform(y.reshape(-1,1))","1f8e0069":"# splitting the dataset into training and test sets\n\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 0)\n\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","8e80af57":"from sklearn.linear_model import LinearRegression\n\nlr_model = LinearRegression()\nlr_model.fit(x_train,y_train)\n\n# calculating the accuracies\nprint(\"Training Accuracy :\",lr_model.score(x_train,y_train))\nprint(\"Testing Accuracy :\",lr_model.score(x_test,y_test))","bd441dc1":"y_pred = lr_model.predict(x_test)\ny_pred[0:5]","acde93d1":"plt.scatter(y_test,y_pred)\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predicted Values\")","1840c7eb":"sns.distplot((y_test-y_pred),bins=50)","b1066b40":"from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\nimport math\n\nprint(\"R2_Score : \", r2_score(y_test,y_pred))\nprint(\"Mean Squared Error : \", mean_squared_error(y_test,y_pred))\nprint(\"MAE : \",mean_absolute_error(y_test,y_pred))\nprint(\"RSME : \",math.sqrt(mean_squared_error(y_test,y_pred)))","b0d20069":"results_df = pd.DataFrame(data=[[\"Linear Regression\", lr_model.score(x_train,y_train),lr_model.score(x_test,y_test),r2_score(y_test,y_pred),mean_squared_error(y_test,y_pred),mean_absolute_error(y_test,y_pred),math.sqrt(mean_squared_error(y_test,y_pred))]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %','r2 score','MSE','MAE','RSME'])\n\nresults_df","83fcf8ea":"from sklearn.svm import SVR\nsvr_model=SVR(kernel = 'rbf')\nsvr_model.fit(x_train,y_train)\n\n# calculating the accuracies\nprint(\"Training Accuracy :\",svr_model.score(x_train,y_train))\nprint(\"Testing Accuracy :\",svr_model.score(x_test,y_test))","fad4bad7":"y_pred = svr_model.predict(x_test)\ny_pred[0:5]","23854ef7":"plt.scatter(y_test,y_pred)\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predicted Values\")","c134375c":"sns.distplot((y_test-y_pred),bins=50)","32536b1b":"print(\"R2_Score : \", r2_score(y_test,y_pred))\nprint(\"Mean Squared Error : \", mean_squared_error(y_test,y_pred))\nprint(\"MAE : \",mean_absolute_error(y_test,y_pred))\nprint(\"RSME : \",math.sqrt(mean_squared_error(y_test,y_pred)))","dd55fd30":"results_df_2 = pd.DataFrame(data=[[\"Support Vector Machine\", svr_model.score(x_train,y_train),svr_model.score(x_test,y_test),r2_score(y_test,y_pred),mean_squared_error(y_test,y_pred),mean_absolute_error(y_test,y_pred),math.sqrt(mean_squared_error(y_test,y_pred))]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %','r2 score','MSE','MAE','RSME'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","fa4600d9":"from sklearn.ensemble import RandomForestRegressor\n\nrfc_model=RandomForestRegressor(n_estimators=300,random_state=0)\nrfc_model.fit(x_train,y_train)\n\n# calculating the accuracies\nprint(\"Training Accuracy :\",rfc_model.score(x_train,y_train))\nprint(\"Testing Accuracy :\",rfc_model.score(x_test,y_test))","180d2def":"y_pred = rfc_model.predict(x_test)\ny_pred[0:5]","2fed71e0":"plt.scatter(y_test,y_pred)\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predicted Values\")","76703de1":"sns.distplot((y_test-y_pred),bins=50)","b359ef74":"print(\"R2_Score : \", r2_score(y_test,y_pred))\nprint(\"Mean Squared Error : \", mean_squared_error(y_test,y_pred))\nprint(\"MAE : \",mean_absolute_error(y_test,y_pred))\nprint(\"RSME : \",math.sqrt(mean_squared_error(y_test,y_pred)))","be89bce6":"results_df_2 = pd.DataFrame(data=[[\"Random Forest\", rfc_model.score(x_train,y_train),rfc_model.score(x_test,y_test),r2_score(y_test,y_pred),mean_squared_error(y_test,y_pred),mean_absolute_error(y_test,y_pred),math.sqrt(mean_squared_error(y_test,y_pred))]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %','r2 score','MSE','MAE','RSME'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","9e3953b1":"#### Splitting the dataset\nTypically,we separate a data set into a training set and testing set, most of the data is used for training, and a smaller portion of the data is used for testing. Analysis Services randomly samples the data to help ensure that the testing and training sets are similar.","fe61a388":"#### Predictive Modelling\nPredictive modeling is a powerful way to add intelligence to your application. It enables applications to predict outcomes against new data. The act of incorporating predictive analytics into your applications involves two major phases: model training and model deployment.","e5737325":"#### Boxplot\nBoxplots are a measure of how well distributed the data in a data set is. It divides the data set into three quartiles. This graph represents the minimum, maximum, median, first quartile and third quartile in the data set.","4095eabb":"## Data Cleaning\nData cleansing or data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data.","3d1e682c":"## EDA with Data Visualization\nExploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations.\n\nData visualization is the graphical representation of data in order to interactively and efficiently convey insights to clients, customers, and stakeholders in general.\n\n#### Box plot for outliers\nIn descriptive statistics, a box plot is a method for graphically depicting groups of numerical data through their quartiles.\nBox plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram. Outliers may be plotted as individual points.\n\n","e4c32145":"#### Support Vector Machine\n\u201cSupport Vector Machine\u201d (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However, it is mostly used in classification problems. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate.","21fae715":"### From the above heatmap ,we can conclude that :\n#### >> Price greatly depends upon features Horse Power(HP) and Year\n#### >> Also, the features HP and Cylinders are positively dependent on each other.\ni.e if no. of cylinders are increased, HP also increases.\n\n#### >> And features MPG-H,MPG-C are negatively dependent on Cylinders\ni.e if no. of cylinders are increased, MPG-H & MPG-C decreases.","6833e6fb":"#### Bar plot\nA bar chart or bar graph is a chart or graph that presents categorical data with rectangular bars with heights or lengths proportional to the values that they represent. The bars can be plotted vertically or horizontally. A bar graph shows comparisons among discrete categories.","96f6c448":"#### Heatmap\nA heatmap is a graphical representation of data in which data values are represented as colors. That is, it uses color in order to communicate a value to the reader. This is a great tool to assist the audience towards the areas that matter the most when you have a large volume of data.","2a7df841":"#### Linear Regression\nLinear regression is a basic and commonly used type of predictive analysis.The overall idea of regression is to examine two things:\n\n(1) does a set of predictor variables do a good job in predicting an outcome (dependent) variable?\n\n(2) Which variables in particular are significant predictors of the outcome variable, and in what way do they\u2013indicated by the magnitude and sign of the beta estimates\u2013impact the outcome variable?\n\nThese regression estimates are used to explain the relationship between one dependent variable and one or more independent variables.","a0fff865":"#### Scatterplot\nScatter plots are used to plot data points on horizontal and vertical axis in the attempt to show how much one variable is affected by another. Each row in the data table is represented by a marker the position depends on its values in the columns set on the X and Y axes.","f203b3f0":"## Modelling and Prediction\n#### Label Encoding\nLabel Encoding refers to converting the labels into numeric form so as to convert it into the machine-readable form. Machine learning algorithms can then decide in a better way on how those labels must be operated. It is an important pre-processing step for the structured dataset in supervised learning.","6004bbe8":"#### Pie Chart\nA pie chart is a type of data visualization that is used to illustrate numerical proportions in data. The python library \u2018matplotlib\u2019 provides many useful tools for creating beautiful visualizations, including pie charts.","12d44c2d":"#### Correlation matrix\nCorrelation coefficients quantify the association between variables or features of a dataset. These statistics are of high importance for science and technology, and Python has great tools that you can use to calculate them. SciPy, NumPy, and Pandas correlation methods are fast, comprehensive, and well-documented.\n\nThe correlation matrix can be used to estimate the linear historical relationship between the returns of multiple assets. You can use the built-in . corr() method on a pandas DataFrame to easily calculate the correlation matrix. Correlation ranges from -1 to 1.","fe69298f":"#### Random Forest\nRandom forest is like bootstrapping algorithm with Decision tree (CART) model. Say, we have 1000 observation in the complete population with 10 variables. Random forest tries to build multiple CART models with different samples and different initial variables. For instance, it will take a random sample of 100 observation and 5 randomly chosen initial variables to build a CART model. It will repeat the process (say) 10 times and then make a final prediction on each observation. Final prediction is a function of each prediction. This final prediction can simply be the mean of each prediction.","eefbb084":"### So, from the above table, the best suitable algorithm for the give dataset is \"Random Forest\" with an accuracy of \"93%\"."}}