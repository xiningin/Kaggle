{"cell_type":{"3c69bf07":"code","f112e355":"code","41ba5477":"code","c9dc2832":"code","329c7fa0":"code","5a5f2ce9":"code","a86b7ee2":"code","cb32ee41":"code","59b2508b":"code","de85499e":"code","dd3343ea":"code","732f3e87":"code","0478c1f8":"code","a25008d6":"code","bf657925":"code","b82f8b61":"code","ff217852":"code","d17f8598":"code","0671efef":"code","3d01e87b":"code","046098c2":"code","9477581e":"code","dae8a35a":"code","95498993":"code","62259c79":"code","448d5b27":"code","725fb7b6":"code","e3d20824":"code","3b9d88ce":"code","08991c8f":"code","dc16520d":"code","a2671034":"code","6be984c4":"code","d1a38691":"code","b1f49d22":"code","85bd5f63":"code","22ba3f30":"code","6e4d7e0b":"code","cd44dca7":"code","4819475a":"code","3e9e4b9b":"code","310b351d":"code","f245dfeb":"code","94d31582":"code","d224a3dc":"markdown","f3f71902":"markdown","80233700":"markdown","1f9fd46b":"markdown","1f1aec8b":"markdown","09f4893f":"markdown","d685527c":"markdown","b3615f19":"markdown","dbb00ff0":"markdown","7412fc75":"markdown","c2035e9c":"markdown","02600987":"markdown","8b0448d3":"markdown","cba43b57":"markdown","baffd6c5":"markdown","81648517":"markdown","ccc36f0a":"markdown","fc8a3519":"markdown","822fec62":"markdown","362b32d9":"markdown","f113599d":"markdown","52974454":"markdown","0a0e2bba":"markdown","25dc21ef":"markdown","4442f4aa":"markdown","73a08681":"markdown","f6179076":"markdown","5b4e41db":"markdown","4110bac7":"markdown","46a0f675":"markdown","54be7d4a":"markdown","0fec36cc":"markdown","aa610906":"markdown","77d6877d":"markdown","b7dd38fc":"markdown","a98b72d4":"markdown"},"source":{"3c69bf07":"#Import necessary packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport warnings\nwarnings.filterwarnings('ignore')","f112e355":"data=pd.read_csv('..\/input\/students-performance-in-exams\/StudentsPerformance.csv')\ndata.head()","41ba5477":"data.info()","c9dc2832":"data.shape","329c7fa0":"#To find if there are any null values in the dataset\ndata.isnull().sum()","5a5f2ce9":"#Renaming the columns for better accessibility\n\ndata.rename(columns={'race\/ethnicity': 'race', 'parental level of education': 'parent_education','test preparation course':'test_prep_course',\n                         'math score':'math_score','reading score':'reading_score','writing score':'writing_score'},inplace=True)","a86b7ee2":"#Displaying the new dataset\n\ndf=data\ndf.head()","cb32ee41":"#Basic Statistical Analysis of numerical columns in the dataset\n\ndf.describe()","59b2508b":"df['gender'].value_counts()","de85499e":"num=df[\"gender\"].value_counts()\nplt.figure(figsize=(10,6))\nplt.pie(num,labels=[\"Female\",\"Male\"],autopct='%1.0f%%')\nplt.show()","dd3343ea":"df['race'].value_counts()","732f3e87":"plt.figure(figsize=(10,6))\nsns.countplot(df[\"race\"])\nplt.title(\"COUNT OF RACE\/ETHNICITY\",fontsize=20)\nplt.xlabel(\"RACE\/ETHNICITY\",fontsize=15)\nplt.ylabel(\"COUNT\",fontsize=15)\nplt.show()","0478c1f8":"df['parent_education'].value_counts()","a25008d6":"plt.figure(figsize=(10,6))\nsns.countplot(df[\"parent_education\"])\nplt.title(\"COUNT OF PARENTAL LEVEL OF EDUCATION\",fontsize=20)\nplt.xlabel(\"EDUCATION\",fontsize=15)\nplt.ylabel(\"COUNT\",fontsize=15)\nplt.show()","bf657925":"df['lunch'].value_counts()","b82f8b61":"num=df[\"lunch\"].value_counts()\nplt.figure(figsize=(10,6))\nplt.pie(num,labels=[\"Standard\",\"Free\"],autopct='%1.1f%%')\nplt.show()","ff217852":"df['test_prep_course'].value_counts()","d17f8598":"num=df[\"test_prep_course\"].value_counts()\nplt.figure(figsize=(10,6))\nplt.pie(num,labels=[\"None\",\"Completed\"],autopct='%1.1f%%')\nplt.show()","0671efef":"fig1 = df.groupby('lunch').agg({'math_score' : 'mean','reading_score' : 'mean','writing_score' : 'mean'})\npx.bar(data_frame=fig1, barmode='group',title = \"<b>Chart Showing Relationship Between Lunch and Scores<\/b>\")","3d01e87b":"fig2 = df.groupby('test_prep_course').agg({'math_score' : 'mean','reading_score' : 'mean','writing_score' : 'mean'})\npx.bar(data_frame=fig2, barmode='group',title = \"<b>Chart Showing Relationship Between Test Preparation and Scores<\/b>\")","046098c2":"fig4 = df.groupby('parent_education').agg({'math_score' : 'mean','reading_score' : 'mean','writing_score' : 'mean'})\npx.bar(data_frame=fig4, barmode='group',title = \"<b>Chart Showing Relationship Between Parents' Education and Scores<\/b>\")","9477581e":"fig3 = df.groupby('gender').agg({'math_score' : 'mean','reading_score' : 'mean','writing_score' : 'mean'})\npx.bar(data_frame=fig3, barmode='group',title = \"<b>Chart Showing Relationship Between Gender and Scores<\/b>\")","dae8a35a":"# Defining regressand(Y) and regressors(X)\n\nX=df[['gender','race','parent_education','lunch','test_prep_course','reading_score','writing_score']]\nY=df['math_score']","95498993":"#Creating dummy variables for categorical variables using inbuilt function from Pandas\n\nX = pd.get_dummies(data=X, drop_first=True)\nX.head()","62259c79":"#Splitting the dataset for training(80%) and testing(20%)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=101)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","448d5b27":"#Creating Random Forest Regression model\n\nfrom sklearn.ensemble import RandomForestRegressor\nforest= RandomForestRegressor(n_estimators =40, random_state = 0)\nforest.fit(X_train,y_train)  \ny_pred = forest.predict(X_test)","725fb7b6":"#Accuracy score of the Random Forest Regression model\n\nprint(\"Random Forest Regression score : \",forest.score(X_test,y_test))","e3d20824":"#Creating linear regression model\n\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train,y_train)","3b9d88ce":"# Linearity Assumption\n\ndef calculate_residuals(model, features, label):\n    predictions = model.predict(features)\n    df_results = pd.DataFrame({'Actual': label, 'Predicted': predictions})\n    df_results['Residuals'] = abs(df_results['Actual']) - abs(df_results['Predicted'])\n    return df_results\n\ndef linear_assumption(model, features, label):\n    df_results = calculate_residuals(model, features, label)\n    sns.lmplot(x='Actual', y='Predicted', data=df_results, fit_reg=False, size=7)\n    line_coords = np.arange(df_results.min().min(), df_results.max().max())\n    plt.plot(line_coords, line_coords, color='darkorange', linestyle='--')\n    plt.title('Actual vs. Predicted Values')\n    plt.show()\n    \nlinear_assumption(model,X_train,y_train)","08991c8f":"# The error terms are normally distributed\n\ndef normal_errors_assumption(model, features, label, p_value_thresh=0.05):\n    from statsmodels.stats.diagnostic import normal_ad    \n    df_results = calculate_residuals(model, features, label)\n    print('Using the Anderson-Darling test for Normal Distribution')\n    p_value = normal_ad(df_results['Residuals'])[1]\n    print('p-value from the test: ', p_value)\n    if p_value < p_value_thresh:\n        print('Since p-value less than 0.05, Residuals are not normally distributed.')\n    else:\n        print('Since p-value greater than 0.05, Residuals are normally distributed.')\n    \n    # Plotting the residuals distribution\n    plt.subplots(figsize=(12, 6))\n    plt.title('Distribution of Residuals')\n    sns.distplot(df_results['Residuals'])\n    plt.show()\n        \nnormal_errors_assumption(model,X_train,y_train)","dc16520d":"# No Multicollinearity among Predictors\n\ndef multicollinearity_assumption(model, features, label, feature_names=None):        \n    \n    # Plotting the heatmap\n    plt.figure(figsize = (10,8))\n    sns.heatmap(pd.DataFrame(features, columns=feature_names).corr(), annot=True)\n    plt.title('Correlation of Variables')\n    plt.show()\n    \nmulticollinearity_assumption(model,X_train,y_train)","a2671034":"# No Autocorrelation of the Error Terms\n\nfrom statsmodels.stats.stattools import durbin_watson #Using Durbin-Watson test\ndf_results = calculate_residuals(model,X_train,y_train)\ndurbin_watson(df_results['Residuals'])","6be984c4":"# Homoscedasticity\n\ndf_results = calculate_residuals(model,X_train,y_train)\nplt.subplots(figsize=(12, 6))\nax = plt.subplot(111)\nplt.scatter(x=df_results.index, y=df_results.Residuals, alpha=0.5)\nplt.plot(np.repeat(0, df_results.index.max()), color='darkorange', linestyle='--')\nax.spines['right'].set_visible(False)  \nax.spines['top'].set_visible(False)  \nplt.title('Residuals')\nplt.show()","d1a38691":"#Creating linear regression model\n\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train,y_train)","b1f49d22":"#Display regression results\n\nimport statsmodels.api as sm\nX_train_Sm= sm.add_constant(X_train)\nX_train_Sm= sm.add_constant(X_train)\nls=sm.OLS(y_train,X_train_Sm).fit()\nprint(ls.summary())","85bd5f63":"#Accuracy score of the linear regression model\n\nprint(\"Linear Regression score : \",model.score(X_train,y_train))","22ba3f30":"#Mean Square Error\n\nfrom sklearn.metrics import mean_squared_error\npredictions = model.predict(X_test)\nprint(\"Mean Square Error: \",mean_squared_error(y_test,predictions))","6e4d7e0b":"#Linear Regression Line\n\npredictions = model.predict(X_test)\nsns.regplot(y_test,predictions)\nplt.title(\"Linear Regression Line\")\nplt.show()","cd44dca7":"#Creating Lasso Regression model\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\n\nls = Lasso()\nls.fit(X_train, y_train)\ny_pred = ls.predict(X_test)","4819475a":"#Accuracy score of the Lasso Regression model\n\nprint(\"Lasso regression score : \", ls.score(X_test, y_test))","3e9e4b9b":"#Mean Square Error\n\nprint(\"Mean Square Error: \",mean_squared_error(y_test,y_pred))","310b351d":"#Creating Ridge Regression model\n\nrd = Ridge()\nrd.fit(X_train,y_train)\ny_pred = rd.predict(X_test)","f245dfeb":"#Accuracy score of the Ridge Regression model\n\nprint(\"Ridge regression score : \", rd.score(X_test, y_test))","94d31582":"#Mean Square Error\n\nprint(\"Mean Square Error: \",mean_squared_error(y_test,y_pred))","d224a3dc":"## DATA TRANSFORMATION\nTo perform the regression, categorical variables in the data must be transformed into dummy variables.","f3f71902":"# 2. Data Collection\n\n__Dataset considered for the problem: Students Performance in Exams__\n\n\nThis data set contains data about each student's performance in Math, Reading and\nWriting tests. It is a fictional dataset, which includes scores from three exams and a\nvariety of personal, social, and economic factors that have interaction effects upon them.\nThe dataset consists of details of 1000 students based on 8 factors (Gender,\nRace\/Ethnicity, Parental level of education, Lunch, Test preparation course, Math score,\nReading score, Writing score).\n","80233700":"### Students whose parents have a bachelor's or master's degree have higher scores in all three subjects","1f9fd46b":"### 64.2% of students completed the Test Preparation Course and 35.8% students have not completed.","1f1aec8b":"### A scatter plot is used to plot the actual vs predicted values. From the plot, the points largely lie on and around the diagonal line. This shows that the variables have a linear relationship.","09f4893f":"### Residuals have been plotted. From the plot of residuals, residual terms appear to be randomly distributed. Hence, the residuals have constant variance, and are homoscedastic.","d685527c":"### The linear regression table is displayed. The table gives a summary of the regression results. The regression coefficients give the relationship between the dependent variable, math score, with each of the independent variables. Using the coefficient values, the regression line can be estimated.\n\n### From the adjusted R-squared, the obtained regression model is a 88% fit for the data, indicating the regression model is a good fit.","b3615f19":"### The scores of students who opted for standard lunch are higher in all three subjects as compared to free\/reduced lunch students.","dbb00ff0":"###  Race has 5 unique values; Group A, B, C, D and E. There are more students belonging to Group C. There are less students belonging to Group A.","7412fc75":"### 5. Homoscedasticity","c2035e9c":"# 3. Data Pre-Processing and Exploratory Data Analysis\n\nSince there are no null values in the dataset, no further treatment to improve the data was\nnecessary. So we can directly proceed to explore the data.\n","02600987":"\n# Linear Regression\n\nLinear regression is the standard algorithm for regression that assumes a linear\nrelationship between inputs and the target variable.\nAssumptions for Linear Regression:\nThe major assumptions of linear regression which have been tested for this problem are\nas follows:\n\n1. Linearity\nThis assumes that there is a linear relationship between the predictors and the response\nvariable. This is an important assumption whose violation will cause inaccurate\npredictions. A scatter plot is used to plot the actual and predicted values to test for\nlinearity.\n\n2. Normality of Error Terms\nThis assumes that the error terms of the regression model are normally distributed.\nViolation of this assumption will lead to inaccurate confidence intervals.\n\n3. No Multicollinearity among Regressors\nThis assumes that the predictors used in the regression are not correlated with each other.\nMulticollinearity affects interpretation of regression coefficients.\n\n4. No Autocorrelation of Error Terms\nThis assumes that the error terms are not highly correlated with each other. High\ncorrelation, either positive or negative, may impact model estimates.\n\n5. Homoscedasticity\nThis assumes that the error terms have constant variance.","8b0448d3":"### 1.Linearity","cba43b57":"### Students who have completed their test preparation have higher scores in all three subjects compared to students who have no test preparation","baffd6c5":"\n#  Random Forest Regressor\n\nRegression is a statistical technique which is used to determine the relationship between\none dependent variable and one or many independent variables. It can give inferences\nabout how the result varies for different factors. Regression is a modeling task that\ninvolves predicting a numeric value given any input.\n\nIt is an algorithm which can be used for both classification and regression problems. It\nuses the method of Ensemble Learning (bagging) for Regression. Ensemble learning\nmethod is a technique which combines the predictions from multiple machine learning\nalgorithms to make a more accurate prediction than an individual model.\nThe algorithm operates by constructing a number of decision trees in a random way and\nmerging them to obtain the aggregate of the results thus resulting in a stable and accurate\nprediction. Within a random forest, there is no interaction between the individual trees so\nthe more trees used, the more robust the prediction would be.\n\nDisadvantages, however, include: there is no interpretability, overfitting may easily occur,\nwe must choose the number of trees to include in the model.","81648517":"# Ridge Regression\n\nRidge Regression uses L2 regularization technique where a penalty is added which is\nequal to the square of the magnitude of the coefficient.\n\nMinimization Objective = RSS + \u03bb * (sum of square of coefficients)\n\nHigher the values of alpha, bigger is the penalty and therefore the magnitude of\ncoefficients is reduced.\n\nIf \u03bb = 0, then we will get the same coefficients as Linear Regression.\n\nIf \u03bb = \u221e, then the coefficients will be zero because infinite weightage on squares of\ncoefficients which is less than zero makes the objective infinite.\nIf 0 < \u03bb < \u221e then, the coefficients will be between 0 and 1 for Linear regression as the\nmagnitude of \u03bb decides the weightage given to the objective.\nAs \u03bb increases the variance decreases but the bias increases.","ccc36f0a":"### 3. No Multicollinearity","fc8a3519":"### Gender has two values; Male and Female, of which females constitute 52% and males 48%","822fec62":"# INTRODUCTION\n\nAs the saying goes, \u201cToday\u2019s children are Tomorrow\u2019s citizens\u201d, it is vital that every\nindividual receives quality education which plays an important role in the growth of their\ncareer as well as the development of a country. Data Science along with machine\nlearning algorithms is the most common method used to measure student\u2019s academic\nperformance and career prediction in the Education domain. The main objective of\nEducational Institutions is to provide high quality education to its students and one way\nto accomplish this is to predict student's academic performance early and thereby taking\nearly steps to improve it. Failing to do so can affect the individual\u2019s academic\nperformance and their career prospects.\n\n\nThe timely prediction of student performance enables the detection of low performing\nstudents, thus enabling the educators to take necessary steps during the learning process\nto help them. These steps can include special attention during classes, extra coaching\nclasses along with study material to help them understand the subjects well etc.\n\n\nPredicting student academic performance is a challenging task because academic\nperformance of students depends on several factors. Here, we are trying to observe how\nsome external factors such as parental level of education, the pre-test course and lunch\nstandards affect the performance of students in their exams. We have used some\nsupervised machine learning models such as Linear Regression and Random Tree\nRegression to predict the Math score of the students. With the help of these models, the\neducational institution can determine the performance of the student and take necessary\nsteps to improve it.","362b32d9":"\n\n## Inferences from EDA:\n\n\u25cf Gender has two values: Male and Female, of which females constitute 52% and\nmales 48%.\n\n\u25cf Race has 5 unique values: Group A, B, C, D and E. There are more students\nbelonging to Group C. There are less students belonging to Group A.\n\n\u25cf Parental level of Education has 6 unique values: some high school (a part of high\nschool), high school, some college, bachelor's degree, associate's degree, master's\ncollege. Most parents of students have attended college, while very few parents\nhave a master's degree.\n\n\u25cf 64.5% of students have opted for standard lunch, while 35.5% of students opted\nfor free lunch.\n\n\u25cf 64.2% of students completed the Test Preparation Course and 35.8% students have\nnot completed.\n\n\u25cf The scores of students who opted for standard lunch are higher in all three subjects\nas compared to free\/reduced lunch students.\n\n\u25cf Students who have completed their test preparation have higher scores in all three\nsubjects compared to students who have no test preparation.\n\n\u25cf Students whose parents have a bachelor's or master's degree have higher scores in\nall three subjects.\n\n\u25cf The scores of female students in reading and writing are higher than the scores of\nmales. However, the scores of male students in maths are higher than the scores of\nfemales.","f113599d":"### 2. Normality of the Error Terms","52974454":"\n\n\n\n\n\n### Inferences from Regression Models\n\n\u25cf Random Forest Regressor gave an accuracy score of: 82.55%\n\n\u25cf Linear Regression gave an accuracy score of: 88.06%\n\n\u25cf Lasso Regression gave an accuracy score of: 81.99%\n\n\u25cf Ridge Regression gave an accuracy score of: 85.47%\n","0a0e2bba":"Multicollinearity has been tested using the heatmap of correlations between the variables. From the map, nearly all the variables have very low correlation between each other. Only the reading and writing score are highly correlated between each other. Since both variables are important to the study, this correlation is permissible.","25dc21ef":"## LINEAR REGRESSION MODEL","4442f4aa":"### The histogram is plotted and the p-value from the Anderson-Darling is found. Since the p-value is greater than the level of significance, the assumption holds. From the histogram, the residuals follow a bell-shaped normal curve. Hence, the residuals follow a normal distribution.","73a08681":"### Inferences After Checking Assumptions\n\n1. From the scatter plot, the points largely lie on and around the diagonal line. This\nshows that the variables have a linear relationship.\n\n2. Since the Anderson-Darling p-value is greater than the level of significance, the no\nautocorrelation assumption holds. From the histogram, the residuals follow a\nbell-shaped normal curve. Hence, the residuals follow a normal distribution.\n\n3. Multicollinearity has been tested using the heatmap of correlations between the\nvariables. From the map, nearly all the variables have very low correlation\nbetween each other. Only the reading and writing score are highly correlated\nbetween each other. Since both variables are important to the study, this\ncorrelation is permissible.\n\n4. To test autocorrelation of error terms, the Durbin-Watson test is used. Since the\nDurbin-Watson test statistic lies between 1.5 and 2.5, there is little or no\nautocorrelation among the error terms.\n\n5. Residuals have been plotted. From the plot of residuals, residual terms appear to\nbe randomly distributed. Hence, the residuals have constant variance, and are\nhomoscedastic.","f6179076":"\n# Regularization\n\nLinear regression and logistic regression are the two frequently used terms when we talk\nabout regression. But there are other types of regression like Lasso regression, Ridge\nregression, Polynomial regression, Stepwise regression, and Elastic net regression.\n\nThe model fitting in linear regression involves a loss function known as the sum of\nsquares where the coefficients in the equation are chosen in a way so as to reduce the loss\nfunction to a minimum value. But sometimes, wrong coefficients can get selected if there\nis a lot of irrelevant data in the training set which will not work well for predictions in the\nfuture. To solve this, regularization can be used to regularize or shrink the wrongly\ncalculated coefficients.\n\n\nRegularization is executed by the addition of the \u201cpenalty\u201d term to the best-fit equation\nproduced by the trained data. This technique can be effectively used to minimize the\nnumber of variables to maintain them in the model. It also solves the problem of\noverfitting which causes low model accuracy. It happens when the model learns the data\nas well as the noises in the training set.\nThe two main terms used along with Lasso and Ridge regression are bias and variance.\nBias is the inability of a machine learning model to come up with a plot that\u2019s inline with\nthe samples which fails to capture the relationship.\nVariance is how the accuracy of a model changes with respect to different datasets.\n\n# Lasso Regression\n\nLASSO stands for Least Absolute Shrinkage and Selection Operation. It uses the\n\u201cshrinkage\u201d technique where the coefficients of determination are shrunk towards zero\nwhich enables the identification of variables strongly associated with the target variable.\nLasso Regression uses L1 regularization technique where a penalty is added which is\nequal to the absolute value of the magnitude of the coefficient. This regularization type\ncan result in sparse models with few coefficients as some coefficients might become zero\nand get eliminated from the model. This can result in feature reduction.\nWhile performing lasso regression, we add a penalizing factor to the least-squares. The\nbelow formula is used to reduce the loss function which helps in choosing a best model.\n\n\nD = least-squares + \u03bb* (sum of absolute values of the magnitude of the coefficients)\nwhere \u03bb denotes the amount of shrinkage which can be of any value between 0 and\ninfinity.\n\nIf \u03bb = 0 it implies that all features are considered and it is equivalent to the linear\nregression where only the residual sum of squares is considered to build a predictive\nmodel.\n\nIf \u03bb = \u221e it implies that no feature is considered i.e, as \u03bb closes to infinity it eliminates\nmore and more features.\n\n\nLasso penalizes the sum of absolute values of coefficients. As the lambda value increases,\ncoefficients decrease and eventually become zero. This way, lasso regression eliminates\ninsignificant variables from our model.\nSo, the idea of Lasso regression is to optimize the cost function by reducing the absolute\nvalues of the coefficients.\nThough the regularized model may have a slightly higher bias than the linear regression,\nit will have less variance for future predictions.","5b4e41db":"# CONCLUSION\n\nAmong the four regression models constructed, the Linear Regression model has the\nhighest accuracy score. Hence, this model can be considered to predict the math score of\nstudents with greatest precision. Using these values, the education institution can analyze\nthe factors highly affecting the education of students and take necessary action to\nimprove their scores.","4110bac7":"### To test autocorrelation of error terms, the Durbin-Watson test is used. Since the Durbin-Watson test statistic lies between 1.5 and 2.5, there is little or no autocorrelation among the error terms.","46a0f675":"# DATA SCIENCE PROCESS\n\n# 1. Problem Formulation\n\nOur primary objective is to build a model that can predict the Maths marks of a student\nbased on the information available from Students Performance dataset in Kaggle.\nOther objectives include:\n\n\u25cf To understand the influence of the parents educational background, test\npreparation, lunch budget etc on students performance.\n\n\u25cf We aim to compare the accuracies of these Regression models to understand\nwhich model can predict the marks most accurately.\n","54be7d4a":"### 4. No Autocorrelation","0fec36cc":"# EXPLORATORY DATA ANALYSIS (EDA)","aa610906":"### The scores of females in reading and writing are higher than the scores of males. However, the scores of males in maths are higher than the scores of females.","77d6877d":"### Parental level of Education has 6 unique values; some high school (a part of high school), high school, some college, bachelor's degree, associate's degree, master's college. Most parents of students have attended college, while very few parents have a master's degree.","b7dd38fc":"### 64.5% of students have opted for standard lunch, while 35.5% of students opted for free lunch.","a98b72d4":"# 4. Model building\n\nWe have used the following supervised machine learning models to predict the Math\nscore of students."}}