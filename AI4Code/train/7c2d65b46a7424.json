{"cell_type":{"a815dc2b":"code","87227bd2":"code","4e9e88fd":"code","d30826a6":"code","5002d4cd":"code","37a895cb":"code","95e5d44d":"code","2b868b16":"code","ec61246d":"code","5fcb4b8e":"code","e42527b0":"code","2f527d43":"code","cbc47003":"code","f0d4b985":"code","53efbfd6":"code","55ce17c8":"code","3b23aa84":"code","957aa7ad":"code","4354b25f":"code","51e0cce0":"code","1962c741":"markdown","0215b095":"markdown","71022191":"markdown","ca252c23":"markdown","60f74451":"markdown","af7ebf9e":"markdown","28536a65":"markdown","e720cf52":"markdown","56f3835c":"markdown","45209073":"markdown"},"source":{"a815dc2b":"#Importing the native packages\nimport pandas as pd\nimport numpy as np\nimport re","87227bd2":"#Importing the NLP packages\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom lda import guidedlda","4e9e88fd":"def get_wordnet_pos(word):\n    '''\n    Tags the parts of speech to tokens\n    Expects a string and outputs the string with its part of speech\n    '''\n\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n    return tag_dict.get(tag, wordnet.NOUN)#If its neither of the mentioned POS,returning it as noun","d30826a6":"def word_lemmatizer(text):\n    '''\n    lemamtizes the tokens based on their part of speech\n    Expects a lits of tokens and outputs a list of lemmatized tokens\n    '''\n\n    lemmatizer = WordNetLemmatizer()\n    text = lemmatizer.lemmatize(text, get_wordnet_pos(text))\n    return text","5002d4cd":"stop_words = stopwords.words('english')","37a895cb":"def reflection_tokenizer(text):\n    '''\n    Tokenizes a list of string, expects a list of strings and outputs a list of strings.\n    before tokenizing:\n    1)removes the non-alphanumeric charcaters like emojies\n    2)removes the numbers\n    3)lower cases the words\n    4)tokenizes the sentences\n    5)lemmatizes teh tokens\n    6)removes the tokens in stop words list\n     '''\n\n    text=re.sub(r'[\\W_]+', ' ', text) #Just alphnumeric characters\n    text=re.sub(r'\\d+', '', text) #Removes numbers\n    text = text.lower()\n    tokens = [word for word in word_tokenize(text)]\n    tokens = [word for word in tokens if len(word) >= 3]#removes words smaller than 3 characters\n    tokens = [word_lemmatizer(w) for w in tokens]\n    tokens = [s for s in tokens if s not in stop_words]\n    return tokens","95e5d44d":"df = pd.read_csv(r\"Unstructured\\Hackathon\\data.csv\")","2b868b16":"df['lemmatize_token'] = df['text'].apply(reflection_tokenizer)\ndf['cleaned']=df.apply(lambda x: ' '.join(x['lemmatize_token']),axis=1)","ec61246d":"#Convert a collection of text documents to a matrix of token counts, matrix of documents and tokens\ntoken_vectorizer = CountVectorizer(tokenizer = reflection_tokenizer, min_df=10, stop_words=get_stopwords(), ngram_range=(1, 3))                        ","5fcb4b8e":"#function maps the column of the dataframe to a matrix of documents in the rows and token counts as columns,\n# this is bag of words representation of the documents\nX_ngrams = token_vectorizer.fit_transform(df['cleaned']) ","e42527b0":"#list of terms (words or n_grams of words)\ntf_feature_names = token_vectorizer.get_feature_names()\n\n#dictionary of words frequency\nword2id = dict((v, idx) for idx, v in enumerate(tf_feature_names))","2f527d43":"%matplotlib inline\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nwordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                min_font_size = 10).generate(' '.join(df['cleaned']))\n# plot the WordCloud image                       \nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n \nplt.show()","cbc47003":"#Returns the top words\nwordcloud.words_","f0d4b985":"#keywords for seeded topics that I want GuidedLDA model to converge to \nseed_topic_list_5 = [['pro', 'con', 'company', 'employee','job','pay'],\n                   ['feature','price','tech','google','company','brand','technology','mobile'],\n                   ['home','house','business traveler','area','stay','solo adventurer'],\n                   ['team','game','season','win','point','score','league','club'],\n                   ['car','brake','drive','engine','article','vehicle','wheel','subject']]","53efbfd6":"#Instantiate the guidedlda as model with parameters like number of topics \nmodel = guidedlda.GuidedLDA(n_topics=5, n_iter=1000, random_state=7, refresh=10,alpha=0.01,eta=0.01)\n#seed_topics is the dictionary {word_id to topic_id}\nseed_topics = {}\nfor t_id, st in enumerate(seed_topic_list_5):\n    for word in st:\n        seed_topics[word2id[word]] = t_id\n\n#build the model on the dataset with 5 topics\n#tseed_confidence: how much extra boost should be given to a term between 0 to 1\nmodel.fit(X_ngrams, seed_topics=seed_topics, seed_confidence=0.3)","55ce17c8":"#Displaying the top 15 words from each topic after fitting our model\nn_top_words = 15\ntopic_word = model.topic_word_\nfor i, topic_dist in enumerate(topic_word):\n    topic_words = np.array(tf_feature_names)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n    print('Topic {}: {}'.format(i, ' '.join(topic_words)))","3b23aa84":"#Creating a dataframe of topics\ndoc_topic = model.transform(X_ngrams)\ncolumns_label = [i for i in range(5)]  # number of topics\ntopic_vector = pd.DataFrame(doc_topic, columns = columns_label)#dataframe of doc-topics\nfinal=topic_vector.round(2)","957aa7ad":"#Extracting the topics with most probability for each document\nfinal_data=final.eq(final.max(1), axis=0).dot(final.columns)","4354b25f":"#Modifying the output based on submission format\noutput=pd.DataFrame()\noutput['Id']=df['Id'].values\noutput['topic']=final_data.values\noutput.replace({\"0\": 'glassdoor_reviews','1':'tech_news','2':'room_rentals','3':'sports_news','4':'Automobiles'},inplace=True)","51e0cce0":"output.to_csv('Hackathon_submission.csv',index=False)","1962c741":"### Before doing the guidedlda import, There are a few workarounds that needs to be done and this can be implemented only in your local.\n\n### 1)Install lda Library \n\n### 2)Add the py files from the [workaround here](https:\/\/github.com\/dex314\/GuidedLDA_WorkAround) inside lda folder under site-packages\n\n### This whole process is clearly mentioned in the workaround link.","0215b095":"# **Loading Data and Preprocessing**","71022191":"### 1)Visualizing the words\n\n### This part becomes necessary because we have to identify the top words and provide these seeds for the distinguishing between topics.","ca252c23":"### Guided LDA is a semi-supervised learning algorithm. The idea is to set some seed words for topics that user believes are representative of the underlying topics in the corpus and guide the model to converge around those terms. ","60f74451":"### Now from the top words obtained from the Wordcloud, Identify the tokens that can clearly distinguish between the topics. For eg. \n\n### 1)The Glassdoor reviews always come with pros, cons, employee etc. which are hardly being used in our other topics \n\n### 2)Likewise Sports_news talk about the teams, wins, scores and stuff like that","af7ebf9e":"# **Preprocessing**","28536a65":"# **Intro**\n\n ### Having tried both LDA and NMF, I couldn't clearly distinguish the texts based on the given topics. Since both of these are Unsupervised techniques and our use case was more like a semi supervised one(known labels), I went ahead with the appoach of Guided LDA.","e720cf52":"# **Guided LDA implementation**","56f3835c":"### NOTE: Make sure you don't provide a lot of seeds for each topics as it might lead to overfitting our model if we were to predict for new test datasets. And also ensure that these seeds are unique to the provided topics.","45209073":"### This provides a dataframe containing the probabilities for each document corresponding to the topics\n![image.png](attachment:image.png)"}}