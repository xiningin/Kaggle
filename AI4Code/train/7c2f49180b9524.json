{"cell_type":{"66521840":"code","2b79c0cb":"code","08fe8deb":"code","a106259f":"code","ba4316c4":"code","2759bdab":"code","41f98784":"code","cc8ee139":"code","bd5d070a":"code","10bcec28":"code","1d277c38":"code","db00033f":"code","b3125d72":"code","aab3732d":"code","f14d1de8":"code","bc07bce3":"code","5eeb0313":"code","51d8863a":"code","071231e3":"code","bcd55d9f":"code","fda47d43":"code","d406537b":"code","b2c352aa":"code","3736e61d":"code","4d75c14d":"code","f98a995e":"code","e7b95a63":"code","1ab78e52":"code","e44ab6c0":"code","168000b5":"code","562bff9c":"code","258701b4":"code","2cf788e1":"code","ec646a17":"code","1ad1d345":"code","fa599555":"code","887efa7b":"code","296b7829":"code","a10a935b":"code","d3e002d9":"code","cffa3219":"code","0f902343":"code","2d81d29c":"code","70266632":"code","0e36af8f":"code","deeb240a":"code","a6ca658d":"code","e420aa2f":"code","5999e7d8":"code","aa4bfe15":"code","ef757917":"code","19272860":"code","0ab3f48e":"code","7c7da77f":"code","c6493890":"code","5713e2f9":"code","9b6eb74d":"code","6e0cf4bb":"code","f5caa097":"code","4362c113":"code","cb9da2cf":"code","036638e8":"code","aa15327f":"code","9d28ff6e":"code","d92c4263":"code","7f0de638":"code","5f897736":"code","8ac2879f":"code","f2043ab0":"code","d1d46baf":"code","0338f17c":"code","f38f227d":"code","b3fa2e01":"code","25c00400":"code","0ed410f0":"code","c7907f5c":"code","90c451e1":"code","4cabf493":"code","8ff18cf4":"code","b2f2edee":"code","bd616bd1":"code","c41d4c23":"code","f2161226":"code","ec9e4976":"code","a6f3ee7b":"markdown","a2e3e289":"markdown","8b69f528":"markdown","faabf800":"markdown","f6b78f22":"markdown","0bc5b5e7":"markdown","68691502":"markdown","de8f0e94":"markdown","7ef55692":"markdown","5003cbd3":"markdown","9510c7ad":"markdown","0fcd019d":"markdown","18a59194":"markdown","eda296c1":"markdown","0d30f2f3":"markdown","12748669":"markdown","10b9fc7b":"markdown","46153e46":"markdown","060cea69":"markdown","2036eddc":"markdown","fa0557e0":"markdown","4dc013c2":"markdown","e52a8b48":"markdown","afe0c717":"markdown","61853514":"markdown","bd22280e":"markdown","8cec5588":"markdown","a1691ca4":"markdown","410f7ba3":"markdown","6c5125a0":"markdown","f99599c3":"markdown","4d94609e":"markdown","150b34e2":"markdown","e5e8772f":"markdown","02d0bfd4":"markdown","6484893a":"markdown","109ef2b7":"markdown","96b53921":"markdown","3e97d553":"markdown","454e364c":"markdown","5f8ed993":"markdown","cd4ca4c7":"markdown","0d7eb080":"markdown","19a679c9":"markdown","d173ec8b":"markdown","ba3e36ac":"markdown","7adafc75":"markdown","22456d5a":"markdown","46464259":"markdown","44006caa":"markdown","125a0f03":"markdown","fd0368f8":"markdown"},"source":{"66521840":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\n\n# Standard plotly imports\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks\nimport cufflinks as cf\nimport plotly.figure_factory as ff\n\n# Using plotly + cufflinks in offline mode\ninit_notebook_mode(connected=True)\ncufflinks.go_offline(connected=True)\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport gc\ngc.enable()\n\nimport logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.DEBUG)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\nprint (\"Ready!\")","2b79c0cb":"# functions from: https:\/\/www.kaggle.com\/kabure\/baseline-fraud-detection-eda-interactive-views\n\ndef resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary\n\ndef plot_distribution(df, var_select=None, title=None, bins=1.0): \n    # Calculate the correlation coefficient between the new variable and the target\n    tmp_fraud = df[df['isFraud'] == 1]\n    tmp_no_fraud = df[df['isFraud'] == 0]    \n    corr = df['isFraud'].corr(df[var_select])\n    corr = np.round(corr,3)\n    tmp1 = tmp_fraud[var_select].dropna()\n    tmp2 = tmp_no_fraud[var_select].dropna()\n    hist_data = [tmp1, tmp2]\n    \n    group_labels = ['Fraud', 'No Fraud']\n    colors = ['seagreen','indianred', ]\n\n    fig = ff.create_distplot(hist_data,\n                             group_labels,\n                             colors = colors, \n                             show_hist = True,\n                             curve_type='kde', \n                             bin_size = bins\n                            )\n    \n    fig['layout'].update(title = title+' '+'(corr target ='+ str(corr)+')')\n\n    iplot(fig, filename = 'Density plot')\n    \ndef plot_dist_churn(df, col, binary=None):\n    tmp_churn = df[df[binary] == 1]\n    tmp_no_churn = df[df[binary] == 0]\n    tmp_attr = round(tmp_churn[col].value_counts().sort_index() \/ df[col].value_counts().sort_index(),2)*100\n    print(f'Distribution of {col}: ')\n    trace1 = go.Bar(\n        x=tmp_churn[col].value_counts().sort_index().index,\n        y=tmp_churn[col].value_counts().sort_index().values, \n        name='Fraud',opacity = 0.8, marker=dict(\n            color='seagreen',\n            line=dict(color='#000000',width=1)))\n\n    trace2 = go.Bar(\n        x=tmp_no_churn[col].value_counts().sort_index().index,\n        y=tmp_no_churn[col].value_counts().sort_index().values,\n        name='No Fraud', opacity = 0.8, \n        marker=dict(\n            color='indianred',\n            line=dict(color='#000000',\n                      width=1)\n        )\n    )\n\n    trace3 =  go.Scatter(   \n        x=tmp_attr.sort_index().index,\n        y=tmp_attr.sort_index().values,\n        yaxis = 'y2', \n        name='% Fraud', opacity = 0.6, \n        marker=dict(\n            color='black',\n            line=dict(color='#000000',\n                      width=2 )\n        )\n    )\n    \n    layout = dict(title =  f'Distribution of {str(col)} feature by %Fraud',\n              xaxis=dict(type='category'), \n              yaxis=dict(title= 'Count'), \n              yaxis2=dict(range= [0, 15], \n                          overlaying= 'y', \n                          anchor= 'x', \n                          side= 'right',\n                          zeroline=False,\n                          showgrid= False, \n                          title= 'Percentual Fraud Transactions'\n                         ))\n\n    fig = go.Figure(data=[trace1, trace2, trace3], layout=layout)\n    iplot(fig)","08fe8deb":"print('# File sizes')\nfor f in os.listdir('..\/input'):\n    if 'zip' not in f:\n        print(f.ljust(30) + str(round(os.path.getsize('..\/input\/' + f) \/ 1000000, 2)) + 'MB')","a106259f":"%%time\ntrain_transaction = pd.read_csv('..\/input\/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('..\/input\/test_transaction.csv', index_col='TransactionID')\ntrain_identity = pd.read_csv('..\/input\/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('..\/input\/test_identity.csv', index_col='TransactionID')\nprint (\"Data is loaded!\")","ba4316c4":"print('train_transaction shape is {}'.format(train_transaction.shape))\nprint('test_transaction shape is {}'.format(test_transaction.shape))\nprint('train_identity shape is {}'.format(train_identity.shape))\nprint('test_identity shape is {}'.format(test_identity.shape))","2759bdab":"train_transaction.head()","41f98784":"train_identity.head()","cc8ee139":"missing_values_count = train_transaction.isnull().sum()\nprint (missing_values_count[0:10])\ntotal_cells = np.product(train_transaction.shape)\ntotal_missing = missing_values_count.sum()\nprint (\"% of missing data = \",(total_missing\/total_cells) * 100)","bd5d070a":"missing_values_count = train_identity.isnull().sum()\nprint (missing_values_count[0:10])\ntotal_cells = np.product(train_identity.shape)\ntotal_missing = missing_values_count.sum()\nprint (\"% of missing data = \",(total_missing\/total_cells) * 100)","10bcec28":"del missing_values_count, total_cells, total_missing","1d277c38":"x = train_transaction['isFraud'].value_counts().index\ny = train_transaction['isFraud'].value_counts().values\n\ntrace2 = go.Bar(\n     x=x ,\n     y=y,\n     marker=dict(\n         color=y,\n         colorscale = 'Viridis',\n         reversescale = True\n     ),\n     name=\"Imbalance\",    \n )\nlayout = dict(\n     title=\"Data imbalance - isFraud\",\n     #width = 900, height = 500,\n     xaxis=go.layout.XAxis(\n     automargin=True),\n     yaxis=dict(\n         showgrid=False,\n         showline=False,\n         showticklabels=True,\n #         domain=[0, 0.85],\n     ), \n)\nfig1 = go.Figure(data=[trace2], layout=layout)\niplot(fig1)\n","db00033f":"del x,y\ngc.collect()","b3125d72":"# Here we confirm that all of the transactions in `train_identity`\nprint(np.sum(train_transaction.index.isin(train_identity.index.unique())))\nprint(np.sum(test_transaction.index.isin(test_identity.index.unique())))","aab3732d":"train_transaction['TransactionDT'].head()","f14d1de8":"train_transaction['TransactionDT'].shape[0] , train_transaction['TransactionDT'].nunique()","bc07bce3":"train_transaction['TransactionDT'].value_counts().head(10)","5eeb0313":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\ntime_val = train_transaction['TransactionDT'].values\n\nsns.distplot(time_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of TransactionDT', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\n\nsns.distplot(np.log(time_val), ax=ax[1], color='b')\nax[1].set_title('Distribution of LOG TransactionDT', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\nplt.show()","51d8863a":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\ntime_val = train_transaction.loc[train_transaction['isFraud'] == 1]['TransactionDT'].values\n\nsns.distplot(np.log(time_val), ax=ax[0], color='r')\nax[0].set_title('Distribution of LOG TransactionDT, isFraud=1', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\ntime_val = train_transaction.loc[train_transaction['isFraud'] == 0]['TransactionDT'].values\n\nsns.distplot(np.log(time_val), ax=ax[1], color='b')\nax[1].set_title('Distribution of LOG TransactionDT, isFraud=0', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\n\nplt.show()","071231e3":"train_transaction['TransactionDT'].plot(kind='hist',\n                                        figsize=(15, 5),\n                                        label='train',\n                                        bins=50,\n                                        title='Train vs Test TransactionDT distribution')\ntest_transaction['TransactionDT'].plot(kind='hist',\n                                       label='test',\n                                       bins=50)\nplt.legend()\nplt.show()","bcd55d9f":"train_transaction.head()","fda47d43":"i = 'isFraud'\ncor = np.corrcoef(train_transaction['TransactionDT'], train_transaction[i])[0,1]\ntrain_transaction.loc[train_transaction['isFraud'] == 0].set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3), label=\"isFraud=0\")\ntrain_transaction.loc[train_transaction['isFraud'] == 1].set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3), label=\"isFraud=1\")\n#test_transaction.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\nplt.legend()\nplt.show()","d406537b":"c_features = list(train_transaction.columns[16:30])\nfor i in c_features:\n    cor = np.corrcoef(train_transaction['TransactionDT'], train_transaction[i])[0,1]\n    train_transaction.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\n    test_transaction.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\n    plt.show()","b2c352aa":"c_features = list(train_transaction.columns[16:30])\nfor i in c_features:\n    cor = np.corrcoef(train_transaction['TransactionDT'], train_transaction[i])[0,1]\n    train_transaction.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\n    test_transaction.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\n    plt.show()","3736e61d":"del c_features\ngc.collect()","4d75c14d":"d_features = list(train_transaction.columns[30:45])\n\nfor i in d_features:\n    cor = np.corrcoef(train_transaction['TransactionDT'], train_transaction[i])[0,1]\n    train_transaction.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\n    test_transaction.set_index('TransactionDT')[i].plot(style='.', title=i+\" corr= \"+str(round(cor,3)), figsize=(15, 3))\n    plt.show()","f98a995e":"train_transaction[d_features].head()","e7b95a63":"# Click output to see the number of missing values in each column\nmissing_values_count = train_transaction[d_features].isnull().sum()\nmissing_values_count","1ab78e52":"# how many total missing values do we have?\ntotal_cells = np.product(train_transaction[d_features].shape)\ntotal_missing = missing_values_count.sum()\n# percent of data that is missing\n(total_missing\/total_cells) * 100","e44ab6c0":"for i in d_features:\n    cor_tr = np.corrcoef(train_transaction['TransactionDT'], train_transaction[i].fillna(-1))[0,1]\n    cor_te = np.corrcoef(test_transaction['TransactionDT'], test_transaction[i].fillna(-1))[0,1]\n    train_transaction.set_index('TransactionDT')[i].fillna(-1).plot(style='.', title=i+\" corr_tr= \"+str(round(cor_tr,3))+\" || corr_te= \"+str(round(cor_te,3)), figsize=(15, 3))\n    test_transaction.set_index('TransactionDT')[i].fillna(-1).plot(style='.', title=i+\" corr_tr= \"+str(round(cor_tr,3))+\"  || corr_te= \"+str(round(cor_te,3)), figsize=(15, 3))\n    plt.show()","168000b5":"del d_features, cor\ngc.collect()","562bff9c":"m_features = list(train_transaction.columns[45:54])\ntrain_transaction[m_features].head()","258701b4":"del m_features\ngc.collect()","2cf788e1":"i = \"V150\"\ncor_tr = np.corrcoef(train_transaction['TransactionDT'], train_transaction[i].fillna(-1))[0,1]\ncor_te = np.corrcoef(test_transaction['TransactionDT'], test_transaction[i].fillna(-1))[0,1]\ntrain_transaction.set_index('TransactionDT')[i].fillna(-1).plot(style='.', title=i+\" corr_tr= \"+str(round(cor_tr,3))+\" || corr_te= \"+str(round(cor_te,3)), figsize=(15, 3))\ntest_transaction.set_index('TransactionDT')[i].fillna(-1).plot(style='.', title=i+\" corr_tr= \"+str(round(cor_tr,3))+\"  || corr_te= \"+str(round(cor_te,3)), figsize=(15, 3))\nplt.show()","ec646a17":"train_transaction.loc[:,train_transaction.columns[train_transaction.columns.str.startswith('V')]].isnull().sum().head(20)","1ad1d345":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\ntime_val = train_transaction['TransactionAmt'].values\n\nsns.distplot(time_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of TransactionAmt', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\n\nsns.distplot(np.log(time_val), ax=ax[1], color='b')\nax[1].set_title('Distribution of LOG TransactionAmt', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\nplt.show()","fa599555":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\ntime_val = train_transaction.loc[train_transaction['isFraud'] == 1]['TransactionAmt'].values\n\nsns.distplot(np.log(time_val), ax=ax[0], color='r')\nax[0].set_title('Distribution of LOG TransactionAmt, isFraud=1', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\ntime_val = train_transaction.loc[train_transaction['isFraud'] == 0]['TransactionAmt'].values\n\nsns.distplot(np.log(time_val), ax=ax[1], color='b')\nax[1].set_title('Distribution of LOG TransactionAmt, isFraud=0', fontsize=14)\nax[1].set_xlim([min(np.log(time_val)), max(np.log(time_val))])\n\n\nplt.show()","887efa7b":"del time_val","296b7829":"tmp = train_transaction[['TransactionAmt', 'isFraud']][0:100000]\nplot_distribution(tmp[(tmp['TransactionAmt'] <= 800)], 'TransactionAmt', 'Transaction Amount Distribution', bins=10.0,)\ndel tmp","a10a935b":"plt.figure(figsize=(10, 7))\nd_features = list(train_transaction.columns[30:45])\nuniques = [len(train_transaction[col].unique()) for col in d_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(d_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature TRAIN')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","d3e002d9":"plt.figure(figsize=(10, 7))\nd_features = list(test_transaction.columns[30:45])\nuniques = [len(test_transaction[col].unique()) for col in d_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(d_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature TEST')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","cffa3219":"plt.figure(figsize=(10, 7))\nc_features = list(train_transaction.columns[16:30])\nuniques = [len(train_transaction[col].unique()) for col in c_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(c_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature TRAIN')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","0f902343":"plt.figure(figsize=(10, 7))\nc_features = list(test_transaction.columns[16:30])\nuniques = [len(test_transaction[col].unique()) for col in c_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(c_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature TEST')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","2d81d29c":"plt.figure(figsize=(35, 8))\nv_features = list(train_transaction.columns[54:120])\nuniques = [len(train_transaction[col].unique()) for col in v_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(v_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","70266632":"plt.figure(figsize=(35, 8))\nv_features = list(train_transaction.columns[120:170])\nuniques = [len(train_transaction[col].unique()) for col in v_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(v_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","0e36af8f":"plt.figure(figsize=(35, 8))\nv_features = list(train_transaction.columns[170:220])\nuniques = [len(train_transaction[col].unique()) for col in v_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(v_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","deeb240a":"plt.figure(figsize=(35, 8))\nv_features = list(train_transaction.columns[220:270])\nuniques = [len(train_transaction[col].unique()) for col in v_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(v_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","a6ca658d":"plt.figure(figsize=(35, 8))\nv_features = list(train_transaction.columns[270:320])\nuniques = [len(train_transaction[col].unique()) for col in v_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(v_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","e420aa2f":"plt.figure(figsize=(38, 8))\nv_features = list(train_transaction.columns[320:390])\nuniques = [len(train_transaction[col].unique()) for col in v_features]\nsns.set(font_scale=1.2)\nax = sns.barplot(v_features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","5999e7d8":"train_identity.head(2)","aa4bfe15":"plt.figure(figsize=(35, 8))\nfeatures = list(train_identity.columns[0:38])\nuniques = [len(train_identity[col].unique()) for col in features]\nsns.set(font_scale=1.2)\nax = sns.barplot(features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature TRAIN')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","ef757917":"plt.figure(figsize=(35, 8))\nfeatures = list(test_identity.columns[0:38])\nuniques = [len(test_identity[col].unique()) for col in features]\nsns.set(font_scale=1.2)\nax = sns.barplot(features, uniques, log=True)\nax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature TEST')\nfor p, uniq in zip(ax.patches, uniques):\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 10,\n            uniq,\n            ha=\"center\") ","19272860":"train_transaction.head(3)","0ab3f48e":"train_identity.head(3)","7c7da77f":"fig, ax = plt.subplots(1, 2, figsize=(20,5))\n\nsns.countplot(x=\"ProductCD\", ax=ax[0], hue = \"isFraud\", data=train_transaction)\nax[0].set_title('ProductCD train', fontsize=14)\nsns.countplot(x=\"ProductCD\", ax=ax[1], data=test_transaction)\nax[1].set_title('ProductCD test', fontsize=14)\nplt.show()","c6493890":"ax = sns.countplot(x=\"DeviceType\", data=train_identity)\nax.set_title('DeviceType', fontsize=14)\nplt.show()","5713e2f9":"print (\"Unique Devices = \",train_identity['DeviceInfo'].nunique())\ntrain_identity['DeviceInfo'].value_counts().head()","9b6eb74d":"cards = ['card1', 'card2', 'card3', 'card4', 'card5', 'card6']\nfor i in cards:\n    print (\"Unique \",i, \" = \",train_transaction[i].nunique())","6e0cf4bb":"fig, ax = plt.subplots(1, 4, figsize=(25,5))\n\nsns.countplot(x=\"card4\", ax=ax[0], data=train_transaction.loc[train_transaction['isFraud'] == 0])\nax[0].set_title('card4 isFraud=0', fontsize=14)\nsns.countplot(x=\"card4\", ax=ax[1], data=train_transaction.loc[train_transaction['isFraud'] == 1])\nax[1].set_title('card4 isFraud=1', fontsize=14)\nsns.countplot(x=\"card6\", ax=ax[2], data=train_transaction.loc[train_transaction['isFraud'] == 0])\nax[2].set_title('card6 isFraud=0', fontsize=14)\nsns.countplot(x=\"card6\", ax=ax[3], data=train_transaction.loc[train_transaction['isFraud'] == 1])\nax[3].set_title('card6 isFraud=1', fontsize=14)\nplt.show()","f5caa097":"cards = train_transaction.iloc[:,4:7].columns\n\nplt.figure(figsize=(18,8*4))\ngs = gridspec.GridSpec(8, 4)\nfor i, cn in enumerate(cards):\n    ax = plt.subplot(gs[i])\n    sns.distplot(train_transaction.loc[train_transaction['isFraud'] == 1][cn], bins=50)\n    sns.distplot(train_transaction.loc[train_transaction['isFraud'] == 0][cn], bins=50)\n    ax.set_xlabel('')\n    ax.set_title('feature: ' + str(cn))\nplt.show()\n","4362c113":"\"emaildomain\" in train_transaction.columns, \"emaildomain\" in train_identity.columns","cb9da2cf":"fig, ax = plt.subplots(1, 3, figsize=(32,10))\n\nsns.countplot(y=\"P_emaildomain\", ax=ax[0], data=train_transaction)\nax[0].set_title('P_emaildomain', fontsize=14)\nsns.countplot(y=\"P_emaildomain\", ax=ax[1], data=train_transaction.loc[train_transaction['isFraud'] == 1])\nax[1].set_title('P_emaildomain isFraud = 1', fontsize=14)\nsns.countplot(y=\"P_emaildomain\", ax=ax[2], data=train_transaction.loc[train_transaction['isFraud'] == 0])\nax[2].set_title('P_emaildomain isFraud = 0', fontsize=14)\nplt.show()","036638e8":"fig, ax = plt.subplots(1, 3, figsize=(32,10))\n\nsns.countplot(y=\"R_emaildomain\", ax=ax[0], data=train_transaction)\nax[0].set_title('R_emaildomain', fontsize=14)\nsns.countplot(y=\"R_emaildomain\", ax=ax[1], data=train_transaction.loc[train_transaction['isFraud'] == 1])\nax[1].set_title('R_emaildomain isFraud = 1', fontsize=14)\nsns.countplot(y=\"R_emaildomain\", ax=ax[2], data=train_transaction.loc[train_transaction['isFraud'] == 0])\nax[2].set_title('R_emaildomain isFraud = 0', fontsize=14)\nplt.show()","aa15327f":"%%time\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ny_train = train['isFraud'].astype(\"uint8\").copy()\nprint(\"Tain: \",train.shape)\ndel train_transaction, train_identity\n\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\nprint(\"Test: \",test.shape)\ndel test_transaction, test_identity\nprint (\"Merged!\")","9d28ff6e":"%%time\nX_train = train.drop('isFraud', axis=1)\nX_test = test.copy()\n\ndel train, test\ngc.collect()\n\n# Label Encoding\nfor f in X_train.columns:\n    if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        X_test[f] = lbl.transform(list(X_test[f].values))  ","d92c4263":"def reduce_mem_usage1(props):\n    start_mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            \n            # Print current column type\n            print(\"******************************\")\n            print(\"Column: \",col)\n            print(\"dtype before: \",props[col].dtype)\n            \n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(props[col]).all(): \n                NAlist.append(col)\n                props[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = props[col].fillna(0).astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n            \n            # Print new column type\n            print(\"dtype after: \",props[col].dtype)\n            print(\"******************************\")\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg\/start_mem_usg,\"% of the initial size\")\n    return props, NAlist","7f0de638":"%%time\n# From kernel https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage2(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","5f897736":"# %%time\n# props_train, NAlist_train = reduce_mem_usage1(X_train)\n# props_test, NAlist_test = reduce_mem_usage1(X_test)","8ac2879f":"%%time\nX_train = reduce_mem_usage2(X_train)\nX_test = reduce_mem_usage2(X_test)","f2043ab0":"X_train.head(3)","d1d46baf":"X_test.head(3)","0338f17c":"logging.debug(\"memory usage!\")","f38f227d":"#drop_col = ['TransactionDT', 'V300', 'V309', 'V111', 'C3', 'V124', 'V106', 'V125', 'V315', 'V134', 'V102', 'V123', 'V316', 'V113', 'V136', 'V305', 'V110', 'V299', 'V289', 'V286', 'V318', 'V103', 'V304', 'V116', 'V298', 'V284', 'V293', 'V137', 'V295', 'V301', 'V104', 'V311', 'V115', 'V109', 'V119', 'V321', 'V114', 'V133', 'V122', 'V319', 'V105', 'V112', 'V118', 'V117', 'V121', 'V108', 'V135', 'V320', 'V303', 'V297', 'V120']\ndrop_col = ['TransactionDT']\nX_train.drop(drop_col,axis=1, inplace=True)\nX_test.drop(drop_col, axis=1, inplace=True)\nX_train.head()","b3fa2e01":"X_train.fillna(-1,inplace=True)\nX_test.fillna(-1,inplace=True)\nX_train.head()","25c00400":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ntrain_scaled = scaler.fit_transform(X_train)         \nPCA_train_x = PCA(2).fit_transform(train_scaled)\nplt.scatter(PCA_train_x[:, 0], PCA_train_x[:, 1], c=y_train, cmap=\"copper_r\")\nplt.axis('off')\nplt.colorbar()\nplt.show()","0ed410f0":"from sklearn.decomposition import KernelPCA\n\nlin_pca = KernelPCA(n_components = 2, kernel=\"linear\", fit_inverse_transform=True)\nrbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\nsig_pca = KernelPCA(n_components = 2, kernel=\"sigmoid\", gamma=0.001, coef0=1, fit_inverse_transform=True)\n\n\nplt.figure(figsize=(11, 4))\nfor subplot, pca, title in ((131, lin_pca, \"Linear kernel\"), (132, rbf_pca, \"RBF kernel, $\\gamma=0.04$\"), \n                            (133, sig_pca, \"Sigmoid kernel, $\\gamma=10^{-3}, r=1$\")):\n       \n    PCA_train_x = PCA(2).fit_transform(train_scaled)\n    plt.subplot(subplot)\n    plt.title(title, fontsize=14)\n    plt.scatter(PCA_train_x[:, 0], PCA_train_x[:, 1], c=y_train, cmap=\"nipy_spectral_r\")\n    plt.xlabel(\"$z_1$\", fontsize=18)\n    if subplot == 131:\n        plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n    plt.grid(True)\n\nplt.show()","c7907f5c":"del train_scaled,PCA_train_x,scaler, lin_pca,rbf_pca, sig_pca\ngc.collect()","90c451e1":"xgb.XGBClassifier(\n        n_estimators=500,\n        max_depth=9,\n        learning_rate=0.05,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        gamma = 0.1,\n        alpha = 4,\n        missing = -1,\n        tree_method='gpu_hist'\n)","4cabf493":"%%time\nNFOLDS = 5\nkf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=123)\n\ny_preds = np.zeros(X_test.shape[0])\ny_oof = np.zeros(X_train.shape[0])\nscore = 0\n  \nfor fold, (tr_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n    clf = xgb.XGBClassifier(\n        n_estimators=500,\n        max_depth=9,\n        learning_rate=0.05,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        gamma = 0.2,\n        alpha = 4,\n        missing = -1,\n        tree_method='gpu_hist'\n    )\n    \n    X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n    y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n    clf.fit(X_tr, y_tr)\n    y_pred_train = clf.predict_proba(X_vl)[:,1]\n    y_oof[val_idx] = y_pred_train\n    print(\"FOLD: \",fold,' AUC {}'.format(roc_auc_score(y_vl, y_pred_train)))\n    score += roc_auc_score(y_vl, y_pred_train) \/ NFOLDS\n    y_preds+= clf.predict_proba(X_test)[:,1] \/ NFOLDS\n    \n    del X_tr, X_vl, y_tr, y_vl\n    gc.collect()\n    \n    \nprint(\"\\nMEAN AUC = {}\".format(score))\nprint(\"OOF AUC = {}\".format(roc_auc_score(y_train, y_oof)))","8ff18cf4":"# Get xgBoost importances\nimportance_dict = {}\nfor import_type in ['weight', 'gain', 'cover']:\n    importance_dict['xgBoost-'+import_type] = clf.get_booster().get_score(importance_type=import_type)\n    \n# MinMax scale all importances\nimportance_df = pd.DataFrame(importance_dict).fillna(0)\nimportance_df = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(importance_df),\n    columns=importance_df.columns,\n    index=importance_df.index\n)\n\n# Create mean column\nimportance_df['mean'] = importance_df.mean(axis=1)\n\n# Plot the feature importances\nimportance_df.sort_values('mean').head(40).plot(kind='bar', figsize=(30, 7))\n","b2f2edee":"del clf, importance_df\ngc.collect()","bd616bd1":"sub = pd.read_csv('..\/input\/sample_submission.csv', index_col='TransactionID')\nsub['isFraud'] = y_preds\nsub.to_csv('xgboost.csv')\nsub.head()","c41d4c23":"sub.loc[ sub['isFraud']>0.99 , 'isFraud'] = 1\nb = plt.hist(sub['isFraud'], bins=50)","f2161226":"print (\"Predicted {} frauds\".format(int(sub[sub['isFraud']==1].sum())))","ec9e4976":"del sub, X_train, X_test, importance_df\ngc.collect()","a6f3ee7b":"<br>\n# Groups","a2e3e289":"# Memory reduction","8b69f528":"<br>\n# TransactionAmt","faabf800":"**Drop some columns**\n> from: https:\/\/www.kaggle.com\/jazivxt\/safe-box\/notebook","f6b78f22":"# Unique Values","0bc5b5e7":"<br>\n# Models\n---\n","68691502":"### Importance PLOT\n> last FOLD","de8f0e94":"### To be continued ...\n**I'll keep updating almost every day :)**","7ef55692":"### 2nd Problem ...\n\nNotice how **imbalanced** is our original dataset! Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud. But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!\n\n**Imbalance** means that the number of data points available for different the classes is different\n\n<img src='https:\/\/www.datascience.com\/hs-fs\/hubfs\/imbdata.png?t=1542328336307&width=487&name=imbdata.png'>","5003cbd3":"**PCA 2 components**","9510c7ad":"<br>\n# Categorical Features\n\n- ProductCD\n- emaildomain\n- card1 - card6\n- addr1, addr2\n- P_emaildomain\n- R_emaildomain\n- M1 - M9\n- DeviceType\n- DeviceInfo\n- id_12 - id_38","0fcd019d":"**Important** Check the [XGB official documentation](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html) in order to know more about the parameters.\n\nAlso check this thread [CV vs Public LB](https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100255#latest-578503)","18a59194":"It seems that criminals prefer gmail","eda296c1":"### Now memory should be around 4 GB !","0d30f2f3":"# Time vs fe\n> **The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).**\n\n**Important ! read the post [The timespan of the dataset is 1 year ?\n](https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100071#latest-577632) by Suchith**\n\n```\nTrain: min = 86400 max = 15811131\nTest: min = 18403224 max = 34214345\n```\n\nThe difference train.min() and test.max() is ```x = 34214345 - 86400 = 34127945``` but we don't know is it in seconds,minutes or hours.\n\n```\nTime span of the total dataset is 394.9993634259259 days\nTime span of Train dataset is  181.99920138888888 days\nTime span of Test dataset is  182.99908564814814 days\nThe gap between train and test is 30.00107638888889 days\n```\n\nIf it is in seconds then dataset timespan will be ```x\/(3600*24*365) = 1.0821``` years which seems reasonable to me. So if the **transactionDT** is in **seconds** then\n\n```\nTime span of the total dataset is 394.9993634259259 days\nTime span of Train dataset is  181.99920138888888 days\nTime span of Test dataset is  182.99908564814814 days\nThe gap between train and test is 30.00107638888889 days\n```\n\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F2370491%2Fc9bf5af5e902595b737df5470adc193b%2Fdownload-1.png?generation=1563312982845419&alt=media)\n\n**source: [FChmiel](https:\/\/www.kaggle.com\/fchmiel)**\n<br>","12748669":"### Email Domain","10b9fc7b":"**Device information**","46153e46":"### Reduce Memory Usage\n> 2 options\n\n**Note** Using te option1 the missing values are encoded as -1, you have to update the XGBoost model and set ```missing=-1```","060cea69":"**train_transaction**","2036eddc":"### Card","fa0557e0":"### C features: C1, C2 ... C14","4dc013c2":"Also you should read this post by Rob [Plotting features over time shows something.... interesting\n](https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100167#latest-577688) he discovered a weird correlation between C and D features, and that's why I do the following plots :)","e52a8b48":"### D features: D1 ... D15","afe0c717":"**Merge transaction & identity + Label Encoder**","61853514":"# Submission","bd22280e":"### M features: M1 .. M9","8cec5588":"**Fill NaN**","a1691ca4":"**Load data**","410f7ba3":"### V features","6c5125a0":"<span style=\"font-family:Calibri; font-size:3em; color:blue\">IEEE Fraud Detection<\/span>\n\n<br>\n<img src=\"https:\/\/cdn.datafloq.com\/cache\/blog_pictures\/878x531\/fraud-analytics-protect-banking-sector.jpg\" width=\"500\" height=\"600\">\n<br>\n\n\n**Why fraud detection?**\n> Fraud is a billion-dollar business and it is increasing every year. The PwC global economic crime survey of 2018[1] found that half (49 percent) of the 7,200 companies they surveyed had experienced fraud of some kind. This is an increase from the PwC 2016 study in which slightly more than a third of organizations surveyed (36%) had experienced economic crime.\n\n\nThis competition is a **binary classification** problem - i.e. our target variable is a binary attribute (Is the user making the click fraudlent or not?) and our goal is to classify users into \"fraudlent\" or \"not fraudlent\" as well as possible.\n\nUnlike metrics such as ```LogLoss```, the **AUC score** only depends on how well you well you can separate the two classes. In practice, this means that only the order of your predictions matter, as a result of this, any rescaling done to your model's output probabilities will have no effect on your score. [click here to read more about AUC-ROC](https:\/\/stats.stackexchange.com\/questions\/132777\/what-does-auc-stand-for-and-what-is-it)\n\n<img src='https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/6b\/Roccurves.png' width=300 height=300>\n\n\n### Content\n\n- Data exploration\n- Missing Data.\n- Imbalanced problem.\n\n\n- Plots\n    - Distribution plots\n    - Count plots\n    - Unique values\n    - Groups\n    \n    \n- Memory reduction  \n\n- PCA\n\n\n- Models\n    - XGBoost Model.\n    - LGBM\n    \n**Remember the <span style=\"color:red\">upvote<\/span> button is next to the fork button, and it's free too! ;)**\n\n----\n\n### References:\n\n- https:\/\/www.kaggle.com\/artgor\/eda-and-models\/data\n- https:\/\/www.kaggle.com\/artkulak\/ieee-fraud-simple-baseline-0-9383-lb\n- https:\/\/www.kaggle.com\/robikscube\/ieee-fraud-detection-first-look-and-eda\n- https:\/\/www.kaggle.com\/mjbahmani\/reducing-memory-size-for-ieee\n\n<br>","f99599c3":"If we consider D features, de 58.15% are missing values ... Let's plot without missing values","4d94609e":"**Interactive Plots Utils**\n> from https:\/\/www.kaggle.com\/kabure\/baseline-fraud-detection-eda-interactive-views (more about Interactive plots there)","150b34e2":"As you can see it seems that train and test transaction dates don't overlap, so it would be prudent to use time-based split for validation. Rob discovered this here: https:\/\/www.kaggle.com\/robikscube\/ieee-fraud-detection-first-look-and-eda.\n\nAlso we can see the **30 days** gap between train and test.\n","e5e8772f":"As you can see, ``` Card 1``` column is given as Categorical but it is behaving like Continuous Data. Having '13553' unique Values.\n\n> **From organizer: ** This is a encoded categorical variable. \nThe dataset contains many high-cardinality variables, and it's challenge to model such variable. Meanwhile, it's worthy to see how you talented people deal with them.\n\nCheck this post: https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100340#latest-578626","02d0bfd4":"### ProductCD","6484893a":"Remove ```.head(20)``` and check the entire list.","109ef2b7":"### isFraud vs time","96b53921":"## V150","3e97d553":"OK, the problem here is that ```D``` features are mostly NaNs!","454e364c":"# Data\n\n\nIn this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target ```isFraud```.\n\nThe data is broken into two files **identity** and **transaction**, which are joined by ```TransactionID```. \n\n> Note: Not all transactions have corresponding identity information.\n\n**Categorical Features - Transaction**\n\n- ProductCD\n- emaildomain\n- card1 - card6\n- addr1, addr2\n- P_emaildomain\n- R_emaildomain\n- M1 - M9\n\n**Categorical Features - Identity**\n\n- DeviceType\n- DeviceInfo\n- id_12 - id_38\n\n**The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).**\n\n**Files**\n\n- train_{transaction, identity}.csv - the training set\n- test_{transaction, identity}.csv - the test set (**you must predict the isFraud value for these observations**)\n- sample_submission.csv - a sample submission file in the correct format\n","5f8ed993":"### 1st problem: NaN\n\nRemember\n> Not all transactions have corresponding identity information","cd4ca4c7":"```24.4%``` of TransactionIDs in train (144233 \/ 590540) have an associated train_identity.\n\n```28.0%``` of TransactionIDs in test (144233 \/ 590540) have an associated train_identity.","0d7eb080":"OK, there are a lot of **NaN** and **interesting columns**: \n\n- ``` C1, C2 ... D1, V300, V339 ... ``` \n- ``` id_01 ... id_38``` \n\nThe columns with those names don't look friendly.\nApparently we don't have **dates**.","19a679c9":"### id_code","d173ec8b":"this takes 6-7 mins. You can click and check the ``` output ```","ba3e36ac":"### D Features","7adafc75":"## XGBoost Model + FE Importance\n\n> This part is from [can_we_beat_it](https:\/\/www.kaggle.com\/konradb\/can-we-beat-it) by Konrad\n\n> Also check this kernel [IEEE Fraud Simple Baseline [0.9383 LB]](https:\/\/www.kaggle.com\/artkulak\/ieee-fraud-simple-baseline-0-9383-lb)","22456d5a":"### C features","46464259":"### Device Type & Device Info","44006caa":"**train_identity**","125a0f03":"# PCA","fd0368f8":"**TransactionDT** is not a timestamp, but somehow we use it to measure time."}}