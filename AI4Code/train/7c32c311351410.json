{"cell_type":{"fb87ea86":"code","cc8bf2c4":"code","4a102cf3":"code","7d2dae2d":"code","bef85d6a":"code","b79a260b":"code","98468949":"code","872615d1":"code","0d387f78":"code","a4203e10":"code","d42f7ba9":"code","c5084e79":"code","273eb1ce":"markdown","6554e4ec":"markdown","828d060f":"markdown","67b31262":"markdown","6e3acbf8":"markdown","1c84cb11":"markdown","3ab1d0ff":"markdown","8018a8e2":"markdown","fa5424af":"markdown","c6729bbf":"markdown","cec6716b":"markdown","070b0df5":"markdown","e800cc5c":"markdown","4ac93e32":"markdown"},"source":{"fb87ea86":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge, Lasso, LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler,LabelEncoder, label_binarize\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score,confusion_matrix, roc_curve, auc\nfrom keras.datasets import mnist\nimport warnings \nwarnings.filterwarnings('ignore')","cc8bf2c4":"train_set = pd.read_csv('..\/input\/income-predictions-dataset2-class-classification\/test_new.csv',names = ['one','two','three','four','five','six','seven','eight','nine','ten','eleven','twelve','thirteen','fourteen','fifteen'])\ntest_set = pd.read_csv('..\/input\/income-predictions-dataset2-class-classification\/train_new.csv',names = ['one','two','three','four','five','six','seven','eight','nine','ten','eleven','twelve','thirteen','fourteen','fifteen'])\n\n\ntotal_set = train_set.append(test_set)\ntotal_set.head()","4a102cf3":"\ny_train = total_set.iloc[0:30161,-1]\ny_test = total_set.iloc[30161:,-1]\n\ntotal_set.drop('fifteen',axis = 1,inplace = True)\n\ntotal_set = pd.get_dummies(total_set)\n\nx_train = total_set.iloc[0:30161,:]\nx_test = total_set.iloc[30161:,:]\n\n\nscaler = MinMaxScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.fit_transform(x_test)\n\nx_train  = np.c_[np.ones(len(x_train)),x_train]\nx_test  = np.c_[np.ones(len(x_test)),x_test]\n","7d2dae2d":"def sigmoid(z):\n    return 1 \/ (1 + np.exp(-z))","bef85d6a":"iter = 2000\ntheta = np.zeros(x_train.shape[1]).T  \nalpha = 0.1\nn = len(x_train)\n\ncost = np.empty(iter)\nfor i in range(iter):\n    z = x_train.dot(theta)\n    Y_pred = sigmoid(z)\n    cost[i] = (-1 \/ n) * (y_train.T.dot(np.log(Y_pred)) + (1 - y_train).T.dot(np.log(1 - Y_pred)) )\n    theta = theta - (alpha \/ n) * (x_train.T.dot(Y_pred - y_train)) \n    \nh_theta = x_test.dot(theta)\nh_theta[h_theta < 0.5] = 0\nh_theta[h_theta >= 0.5] = 1\n\nprint(\"Model accuracy is:\" , accuracy_score(y_test,h_theta) * 100)\n","b79a260b":"iter = 2000\ntheta = np.zeros(x_train.shape[1]).T  \nalpha = 0.1\nn = len(x_train)\nl1_param = 0.01\n\ncost = np.empty(iter)\naccuracy = np.empty(iter)\nfor i in range(iter):\n    z = x_train.dot(theta)\n    Y_pred = sigmoid(z)\n    cost[i] = (-1 \/ n) * (y_train.T.dot(np.log(Y_pred)) + (1 - y_train).T.dot(np.log(1 - Y_pred)) + (l1_param * (np.sum(np.abs(theta)))))  \n    theta = theta - (alpha \/ n) * (x_train.T.dot(Y_pred - y_train)) \n    h_theta = x_test.dot(theta)\n    h_theta[h_theta < 0.5] = 0\n    h_theta[h_theta >= 0.5] = 1\n    accuracy[i] = accuracy_score(y_test,h_theta)\n    \n\n\nprint(\"Model accuracy is:\" , accuracy_score(y_test,h_theta) * 100)\n\nplt.plot(np.arange(0,iter),cost,'r-',label = 'Cost')\nplt.plot(np.arange(0,iter),accuracy,'b-',label = 'Accuracy')\nplt.xlabel(\"No of iterations\")\nplt.ylabel(\"Percentage\")\nplt.title(\"Number of iterations v\/s Cost function and Accuracy\")\nplt.legend()\nplt.show()\n","98468949":"iter = 2000\ntheta = np.zeros(x_train.shape[1]).T  \nalpha = 0.1\nn = len(x_train)\nl2_param = 0.01\n\ncost = np.empty(iter)\naccuracy = np.empty(iter)\n\nfor i in range(iter):\n    z = x_train.dot(theta)\n    Y_pred = sigmoid(z)\n    cost[i] = (-1 \/ n) * (y_train.T.dot(np.log(Y_pred)) + (1 - y_train).T.dot(np.log(1 - Y_pred)) + (l2_param * (np.sum(theta**2))))  \n    theta = theta - (alpha \/ n) * (x_train.T.dot(Y_pred - y_train)) \n    h_theta = x_test.dot(theta)\n    h_theta[h_theta < 0.5] = 0\n    h_theta[h_theta >= 0.5] = 1\n    accuracy[i] = accuracy_score(y_test,h_theta)\n    \n\n\nprint(\"Model accuracy is:\" , accuracy_score(y_test,h_theta) * 100)\n\nplt.plot(np.arange(0,iter),cost,'r-',label = 'Cost')\nplt.plot(np.arange(0,iter),accuracy,'b-',label = 'Accuracy')\nplt.xlabel(\"No of iterations\")\nplt.ylabel(\"Percentage\")\nplt.title(\"Number of iterations v\/s Cost function and Accuracy\")\nplt.legend()\nplt.show()\n","872615d1":"(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nnsamples, nx, ny = x_train.shape\nx_train = x_train.reshape((nsamples,nx*ny))\n\nnsamples, nx, ny = x_test.shape\nx_test = x_test.reshape((nsamples,nx*ny))","0d387f78":"logreg_with_L1 = LogisticRegression(penalty='l1', multi_class='ovr', solver='liblinear')\nlogreg_with_L1.fit(x_train, y_train)\nprint('Training accuracy: ',logreg_with_L1.score(x_train, y_train) * 100)\nprint('Testing accuracy: ',logreg_with_L1.score(x_test, y_test) * 100)","a4203e10":"(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nnsamples, nx, ny = x_train.shape\nx_train = x_train.reshape((nsamples,nx*ny))\n\nnsamples, nx, ny = x_test.shape\nx_test = x_test.reshape((nsamples,nx*ny))","d42f7ba9":"logreg_with_L2 = LogisticRegression(penalty='l2', multi_class='ovr', solver='liblinear',max_iter = 1000)\nlogreg_with_L2.fit(x_test, y_test)\nprint('Training accuracy: ',logreg_with_L2.score(x_train, y_train) * 100)\nprint('Testing accuracy: ',logreg_with_L2.score(x_test, y_test) * 100)","c5084e79":"probabs = logreg_with_L2.predict_proba(x_train)\nclasses = range(10)\ny_test = label_binarize(y_train, classes)\nfor i in range(10):\n    preds = probabs[:,i]    \n    fpr, tpr, threshold = roc_curve(y_test[:, i], preds)\n    roc_auc = auc(fpr, tpr)\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\nplt.show()","273eb1ce":"## One vs All Logistic Regression with L2 regularization","6554e4ec":"## Data Preparation","828d060f":"## Logistic Regression without regularization as a classifier ","67b31262":"## Model Evaluation","6e3acbf8":"## Logistic Regression with L1 regularization","1c84cb11":"# Two Class Classification","3ab1d0ff":"## One vs All Logistic Regression with L1 regularization","8018a8e2":"# Imports","fa5424af":"## Data preparation","c6729bbf":"## Logistic Regression with L2 regularization ","cec6716b":"# Conclusion:\n> By reducing the sum of absolute values of the coefficients, what Lasso Regularization (L1 Norm) does is to reduce the number of features in the model altogether to predict the target variable.\n\n> On the other hand, by reducing the sum of square of coefficients, Ridge Regularization (L2 Norm) doesn\u2019t necessarily reduce the number of features, but rather reduces the magnitude\/impact that each features has on the model by reducing the coefficient value.\n\n> Both regularization does indeed prevent the model from overfitting, but I would like to think of Lasso Regularization as reducing the quantity of features while Ridge Regularization as reducing the quality of features.","070b0df5":">  Observation:\n>   Since the training accuracy and testing accuracy are almost the same with both L1 and L2 regularization, we can conclude that the model is a good fit.","e800cc5c":"# Things you will find in this notebook:\n\n1.Two class classification.\n* Prediction on whether a person's income is more than $50K or not using logistic regression implemented from scratch.\n* How L1 and L2 regularization affects the overall accuracy and which one is better!.\n* An error-iteration and accuracy-iteration graphs for both L1 and L2 regularized models.\n\n2.Multi Class classification.\n* We will be using the famous MNIST dataset for multiclass classification using logistic regression(L1 and L2 regularized).\n* You will be familiarized with one vs all approach.\n* Comment whether it is a good fit, over-fit or under-fit.\n* We will plot the ROC curve and use area under the curve as an evaluation metric.\n\n> Do upvote if you find this notebook useful. So let's get started.","4ac93e32":"# MultiClass Classification"}}