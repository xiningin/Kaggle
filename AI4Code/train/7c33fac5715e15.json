{"cell_type":{"c7081533":"code","5fcbda69":"code","a7b424e2":"code","a7522567":"code","15936fbb":"code","7e89f6f1":"code","1abd726a":"code","fa949a68":"code","70150299":"code","4e111d23":"code","4675f6c2":"code","5cc2b73f":"code","56226ec2":"code","8c10a777":"markdown","30d0181f":"markdown","3303879f":"markdown","c55a2e97":"markdown","418ae765":"markdown","16d66a70":"markdown","15780a41":"markdown","18339f68":"markdown","ec34e5e4":"markdown","6720d59b":"markdown","226f7e5b":"markdown","0d029f62":"markdown"},"source":{"c7081533":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport PIL.Image as Image\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\n\nimport torchvision\nfrom torchvision import transforms\ntorch.manual_seed(42)","5fcbda69":"#Download & install PyTorch\/XLA & Delete the files after installing.\n!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version 1.7 --apt-packages libomp5 libopenblas-dev\n!rm .\/torch_xla-1.7-cp37-cp37m-linux_x86_64.whl\n!rm .\/torch-1.7-cp37-cp37m-linux_x86_64.whl\n!rm .\/torchvision-1.7-cp37-cp37m-linux_x86_64.whl\n!rm .\/pytorch-xla-env-setup.py","a7b424e2":"import torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\nos.environ['XLA_USE_BF16'] = '1' #Setting this Environment variable allows TPU to use 'bfloat16'","a7522567":"path = '..\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba'\ndf = pd.read_csv('..\/input\/celeba-dataset\/list_attr_celeba.csv').iloc[:,0]","15936fbb":"class ImageDataset(Dataset):\n    \n    def __init__(self,path,df,image_transforms):\n        super().__init__()\n        self.path = path\n        self.df   = df\n        self.image_transforms = image_transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self,idx):\n        image = Image.open(os.path.join(self.path,self.df.iloc[idx]))\n        if self.image_transforms:\n            image = self.image_transforms(image)\n            \n        return image #We're only returning the image.Since this is a DCGAN,there is no need for labels.","7e89f6f1":"image_transforms = transforms.Compose([\n    transforms.Resize(64),\n    transforms.CenterCrop(64),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) #Normalizes the values b\/w -1 & 1.\n])","1abd726a":"#Generator outputs fake images given an input noise vector.\nclass Generator(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.main = nn.Sequential(\n                             \n                    nn.ConvTranspose2d(100,64*8,4,1,0,bias=False),\n                    nn.BatchNorm2d(64*8),\n                    nn.LeakyReLU(0.01,inplace=True),\n            \n                    nn.ConvTranspose2d(64*8,64*4,4,2,1,bias=False),\n                    nn.BatchNorm2d(64*4),\n                    nn.LeakyReLU(0.01,inplace=True),\n            \n                    nn.ConvTranspose2d(64*4,64*2,4,2,1,bias=False),\n                    nn.BatchNorm2d(64*2),\n                    nn.LeakyReLU(0.01,inplace=True),\n            \n                    nn.ConvTranspose2d(64*2,64,4,2,1,bias=False),\n                    nn.BatchNorm2d(64),\n                    nn.LeakyReLU(0.01,inplace=True),\n            \n                    nn.ConvTranspose2d(64,3,4,2,1,bias=False),\n                    nn.Tanh(),      \n                )\n    \n    def forward(self,input): #Input is the Noise vector.\n        return self.main(input)","fa949a68":"#Discriminator classifies whether the given input images are Real or Fake.\nclass Discriminator(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.main = nn.Sequential(\n                    \n                    nn.Conv2d(3,64,4,2,1,bias=False),\n                    nn.LeakyReLU(0.01,inplace=True),\n            \n                    nn.Conv2d(64,64*2,4,2,1,bias=False),\n                    nn.BatchNorm2d(64*2),\n                    nn.LeakyReLU(0.01,inplace=True),\n            \n                    nn.Conv2d(64*2,64*4,4,2,1,bias=False),\n                    nn.BatchNorm2d(64*4),\n                    nn.LeakyReLU(0.01,inplace=True),\n            \n                    nn.Conv2d(64*4,64*8,4,2,1,bias=False),\n                    nn.BatchNorm2d(64*8),\n                    nn.LeakyReLU(0.01,inplace=True),\n                     \n                    nn.Conv2d(64*8,1,4,1,0,bias=False),\n                    nn.Sigmoid()\n               )\n    \n    def forward(self,input): #Input is the images.\n        return self.main(input)","70150299":"def reduce(values):\n    '''    \n    Returns the average of the values.\n    Args:\n        values : list of any value which is calulated on each core \n    '''\n    return sum(values) \/ len(values)","4e111d23":"def train_one_epoch(dataloader,G,D,loss_fn,optimizer_G,optimizer_D,epoch_no,nb_epochs,fixed_noise,device):\n        '''\n        This function will train both the Generator and the Discriminator for one epoch.\n        Args :\n             dataloader - DataLoader object for iterating the data.\n             G - Generator.\n             D - Discriminator.\n             loss_fn - Loss to optimize.\n             optimizer_G - Optimizer for Generator.\n             optimizer_D - Optimizer for Discriminator.\n             epoch_no - Number for current epoch which is running.\n             nb_epochs - Total Number of epochs to run.\n             fixed_noise - Generator's input for outputting images for every 20 epochs.\n             device - which device to train on.\n        \n         Returns : Nothing.\n         '''\n        \n        for batch,imgs in enumerate(dataloader):\n            \n            real_imgs = imgs.to(device)\n            bs = len(imgs) #Batch Size.\n            real_labels = torch.ones((bs,),dtype=torch.float,device=device)\n            fake_labels = torch.zeros((bs,),dtype=torch.float,device=device)\n        \n            #Update the Discriminator.\n            noise = torch.rand(bs,100,1,1,device=device) #Noise for Generator's input.\n            fake_imgs = G(noise) #Generated images.\n        \n            real_loss = loss_fn(D(real_imgs).view(-1),real_labels)\n            fake_loss = loss_fn(D(fake_imgs.detach()).view(-1),fake_labels)\n            loss_D = (real_loss+fake_loss)\/2 #Discriminator loss.\n        \n            optimizer_D.zero_grad()\n            loss_D.backward()\n            xm.optimizer_step(optimizer_D)\n        \n            #Update the Generator.\n            loss_G = loss_fn(D(fake_imgs).view(-1),real_labels) #Generator loss.\n        \n            optimizer_G.zero_grad()\n            loss_G.backward()\n            xm.optimizer_step(optimizer_G)\n            \n            if ((batch+1)%50 == 0):\n                reduced_loss_D = xm.mesh_reduce('reduced_loss_D',loss_D,reduce)\n                reduced_loss_G = xm.mesh_reduce('reduced_loss_D',loss_G,reduce)\n                xm.master_print(f'Epoch[{epoch_no+1}\/{nb_epochs}] Batch[{(batch+1)}\/{len(dataloader)}] Discriminator Loss:{reduced_loss_D:.7f} Generator Loss:{reduced_loss_G}')\n        \n        if((epoch_no+1)%10 == 0):\n            with torch.no_grad():\n                torchvision.utils.save_image(G(fixed_noise),f'fake_images_after_{epoch_no+1}.jpg',8,normalize=True)","4675f6c2":"def _mp_fn(rank,flags):\n    '''\n    This function is executed on all the devices when it is spawned.\n    Args :\n        rank  - Index of the process.\n        flags - Arguments you need to pass to each process.\n    '''\n    device = xm.xla_device()\n    #Creates the (distributed) train sampler, which let this process only access its portion of the training data.\n    data_sampler = DistributedSampler(dataset=flags['DS'],\n                                      num_replicas=xm.xrt_world_size(),\n                                      rank=xm.get_ordinal(),\n                                      shuffle=True)\n    data_loader = DataLoader(dataset=flags['DS'],\n                             batch_size=flags['BS'],\n                             sampler=data_sampler,\n                             num_workers=0)\n    del data_sampler\n    gc.collect()\n    \n    G = flags['Generator'].to(device)\n    D = flags['Discriminator'].to(device)\n    fixed_noise = flags['fixed_noise'].to(device)\n    loss_fn = nn.BCELoss()\n    optimizer_G = torch.optim.Adam(G.parameters(),lr=flags['lr'],betas=[0.5,0.999]) #Optimizer for Generator.\n    optimizer_D = torch.optim.Adam(D.parameters(),lr=flags['lr'],betas=[0.5,0.999]) #Optimizer for Discriminator.\n    \n    xm.master_print('Training has started\\n')\n    for epoch in range(flags['nb_epochs']):\n        # Calling 'per_device_loader()' on it will return the data loader for the particular device.\n        parallel_loader = pl.ParallelLoader(data_loader,[device]).per_device_loader(device)\n        train_one_epoch(parallel_loader,G,D,loss_fn,optimizer_G,optimizer_D,epoch,flags['nb_epochs'],fixed_noise,device)\n        xm.master_print(f'Epoch[{epoch+1}\/{flags[\"nb_epochs\"]}] has completed\\n')\n        del parallel_loader\n        gc.collect()","5cc2b73f":"dataset = ImageDataset(path,df,image_transforms)\n\nG = Generator()\nG.float()\nD = Discriminator()\nD.float()\n\nfixed_noise = torch.rand((32, 100, 1, 1),dtype=torch.float)\nflags = {'DS':dataset,\n         'Generator': G,\n         'Discriminator':D,\n         'BS':128, #Batch size.\n         'lr':0.0002,\n         'nb_epochs':50,\n         'fixed_noise':fixed_noise\n        }\n\nxmp.spawn(fn=_mp_fn,args=(flags,),nprocs=8,start_method='fork')","56226ec2":"#Uncomment the below line of code to save the Generator's weights.\n#xm.save(G.state_dict(),'generator_weights.pth')","8c10a777":"# 2. <a id='2'>Import Packages<\/a>\n[Table of contents](#0.1)","30d0181f":"<img src=https:\/\/storage.googleapis.com\/kaggle-media\/tpu\/tpu_cores_and_chips.png >","3303879f":"# 1. <a id='1'>Introduction<\/a>\n[Table of contents](#0.1)","c55a2e97":"# 3. <a id='3'>Loading Data<\/a>\n[Table of contents](#0.1)","418ae765":"# 5. <a id='5'>Train the GAN<\/a>\n[Table of contents](#0.1)","16d66a70":"# 6. <a id='6'>References<\/a>\n[Table of contents](#0.1)","15780a41":"1. Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks - https:\/\/arxiv.org\/pdf\/1511.06434.pdf \n1. DCGAN Tutorial - https:\/\/pytorch.org\/tutorials\/beginner\/dcgan_faces_tutorial.html\n1. PyTorch Implementations of Generative Adversarial Networks - https:\/\/github.com\/eriklindernoren\/PyTorch-GAN\n1. [FoldTraining] PyTorch-TPU\ud83d\udd25-8-Cores - https:\/\/www.kaggle.com\/joshi98kishan\/foldtraining-pytorch-tpu-8-cores","18339f68":"# Table of contents <a id='0.1'><\/a>\n\n1. [Introduction](#1)\n2. [Import Packages](#2)\n3. [Loading Data](#3)\n4. [Build the GAN](#4)\n5. [Train the GAN](#5)\n6. [References](#6)","ec34e5e4":"Hey folks. This Notebook will show you how to Train a Generative Adversarial Network (GAN) on **all the 8 cores of the TPU v3**.\n\n\n> GANs are difficult to train.\n> \n> The reason they are difficult to train is that both the generator model and the discriminator model are trained simultaneously in a zero sum game. This means that improvements to one model come at the expense of the other model.\n> \n> The goal of training two models involves finding a point of equilibrium between the two competing concerns.\n> \n> It also means that every time the parameters of one of the models are updated, the nature of the optimization problem that is being solved is changed. This has the effect of creating a dynamic system. In neural network terms, the technical challenge of training two competing neural networks at the same time is that they can fail to converge.\n\n[source](https:\/\/machinelearningmastery.com\/practical-guide-to-gan-failure-modes\/)\n\n\nLets, Dive in.","6720d59b":"# <center>Training GAN on 8-cores of TPU\ud83d\udd25using PyTorch\/XLA<center>","226f7e5b":"# 4. <a id='4'>Build the GAN<\/a>\n[Table of contents](#0.1)","0d029f62":"At the Output section, you can look at the images generated for every 10 epochs.\nThough, We're using DCGAN. Our Generator can able to generate some decent images matching the Training data.\n\nIf you're looking to generate High-Resolution images, then I'll suggest you to take a look at the other variants of GAN like StyleGAN."}}