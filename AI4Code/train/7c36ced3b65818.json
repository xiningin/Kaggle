{"cell_type":{"5e72f6ec":"code","c06c1351":"code","7cb60137":"code","140c1d5c":"code","d08af507":"code","28f5b247":"code","79674b45":"code","5fbf2b4e":"code","45cab425":"code","d0c07117":"code","1c9292c4":"markdown","9d8784ed":"markdown","30fe3528":"markdown","28b1aeb8":"markdown","c32d36c7":"markdown","e357259b":"markdown","00a7e820":"markdown"},"source":{"5e72f6ec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport re\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c06c1351":"\"\"\"import the dataset in data library\"\"\"\ndata = pd.read_csv(\"..\/input\/fraud-call-india-dataset\/fraud_call.file\",sep='\\t',names=['label','content'])","7cb60137":"data.head()","140c1d5c":"data['label'].value_counts()","d08af507":"import seaborn as sns\nsns.countplot('label',data=data)","28f5b247":"\"\"\"perform text preprocessing\"\"\"\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nps = WordNetLemmatizer()\ncv = TfidfVectorizer(max_features=2000)","79674b45":"def remove_digit(data) :\n    corpos = []\n    for i in range(0, len(data)) :\n        review = re.sub('[^a-zA-Z]', ' ', data['content'][i])\n        review = review.lower()\n        review = review.split()\n        review = [ps.lemmatize(word) for word in review if word not in stopwords.words('english')]\n        review = ' '.join(review)\n        corpos.append(review)\n    return corpos","5fbf2b4e":"from sklearn.metrics import recall_score,classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix,accuracy_score\n\"\"\"creating\/training of model.\"\"\"\ndef detect_model(corpos, data):\n    x = cv.fit_transform(corpos).toarray()\n    y = pd.get_dummies(data['label'])\n    y = y.iloc[:, 1].values\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)\n    fraud_detect = MultinomialNB().fit(x_train, y_train)\n    print(\"model has trained.\")\n    y_ped = fraud_detect.predict(x_test)\n    cong_m = confusion_matrix(y_test, y_ped)\n    acc = accuracy_score(y_test, y_ped)\n    recall = recall_score(y_test,y_ped)\n    cl_r = classification_report(y_test,y_ped)\n    print(\"Confusion matrix:\", cong_m)\n    print(\"Accuracy_score:\", acc)\n    print(\"recall_score is:\",recall)\n    print(\"Classification report id:\",cl_r)\n    return fraud_detect","45cab425":"proper_list = remove_digit(data)\nmodel= detect_model(proper_list, data)","d0c07117":"\"\"\"when dataset is imbalanced dataset, then we should use precision,recall or f1-score metrics function because \naccuracy will be failed in unbalance dataset  for  classification. but some time it will work fine\"\"\"\n","1c9292c4":"visualize the label column **for count** the number of fraud data and normal data.","9d8784ed":"call **remove_digit()** for text preprocessing operation and **detect_model()** for creating classification model \nand train it by traing data as well as validate it by testing data and will return **confusion_metrics** and \n**accuracy_score**\"\"\"","30fe3528":"## \"\"\"if you all find out this notebook  helpful, please upvote\"\"\"","28b1aeb8":"I am going to count howmany fraud and normal data is avialble in dataset.","c32d36c7":"In **machine learning technique**, Every model will take input as integer\/numeric data type values,but in this dataset data are present in object type so before giving input to model we have to convert object type data to integer types.\n\nIn the below cell i have used **TfidVectorizer method** which is imported from nltk library for create a vector of words which is present dataset. **TfidVectorizer** is more better then **Bag of Words(BOW)** method.\n\n***Note:*** for categorical\/object type information, before performing encoding methods always remove stopwords from the data because  stopwords do not play important role for improve performance and accuracy of model.\n\nso in the below cell, for removing stopwords \"WordNetLemmatizer\" have used. it will remove stopwords from dataset.\n\nEx: for,in,you,at etc these are stopwords,\n**stopwords** imported from **nltk** library, this is a text Preprocessing library. \n\n","e357259b":"In  given below cell, **Naive Bayes** classification has used to classify whether it is fraud call or not.\nbefore it split data in to two parts.\n\n**1. training data\n2. test data**\n\nAn in-build **train_test_split** module used to split data in to training and testing part.","00a7e820":"This notebook is for **fraud call detection** dataset. the dataset contain two type of data \none is fraud and second is normal call data.\nTo detect fraud call, In this notebook i am going to used Naive Bayes classification.\n\nwhen ever we start creating a model in Machine Learning the first step is perform EDA ,in the EDA first step is import\/load the dataset."}}