{"cell_type":{"f91fe89b":"code","5594bec0":"code","f338e878":"code","a5d08602":"code","6c3716af":"code","555d7b83":"code","a73ee9d2":"code","b27af11f":"code","79fa7a14":"markdown","d64a5bd2":"markdown","b2de609b":"markdown","5ecfba50":"markdown","b43cf414":"markdown","af5bb1ed":"markdown","8b43ea33":"markdown"},"source":{"f91fe89b":"import pandas as pd \n  \ndata = [['Day1', 'Sun', 'Warm', 'High', 'Weak', 'No'], \n        ['Day2', 'Sun', 'Warm', 'High', 'Strong', 'No'],\n        ['Day3', 'Cloud', 'Warm', 'High', 'Weak', 'Yes'],\n        ['Day4', 'Rain', 'Soft', 'High', 'Weak', 'Yes'], \n        ['Day5', 'Rain', 'Fresh', 'Normal', 'Weak', 'Yes'],\n        ['Day6', 'Rain', 'Fresh', 'Normal', 'Strong', 'No'],\n        ['Day7', 'Cloud', 'Fresh', 'Normal', 'Weak', 'Yes'], \n        ['Day8', 'Sun', 'Soft', 'High', 'Weak', 'No'],\n        ['Day9', 'Sun', 'Fresh', 'Normal', 'Weak', 'Yes'],\n        ['Day10', 'Rain', 'Soft', 'Normal', 'Strong', 'Yes'],\n        ['Day11', 'Sun', 'Soft', 'Normal', 'Strong', 'Yes'], \n        ['Day12', 'Cloud', 'Soft', 'High', 'Strong', 'Yes'],\n        ['Day13', 'Cloud', 'Warm', 'Normal', 'Weak', 'Yes'],\n        ['Day14', 'Rain', 'Soft', 'High', 'Strong', 'No']]\n\n\ndf = pd.DataFrame(data, columns = ['Day', 'Aspect', 'Temperature', 'Humidity','Wind','Output']) \ndf.head(12)","5594bec0":"df_type_1 = df.select_dtypes(include=['int64']).copy()\ndf_type_2 = df.select_dtypes(include=['float64']).copy()\ndf_type_3 = pd.concat([df_type_2, df_type_1], axis=1, join_axes=[df_type_1.index])","f338e878":"from sklearn import preprocessing \nfrom sklearn.preprocessing import LabelEncoder\n\ncategorization = preprocessing.LabelEncoder()\ncategorization.fit(df[\"Aspect\"].astype(str))\nlist(categorization.classes_)\n\ndf_object = df.astype(str).apply(categorization.fit_transform)\ndf_formated = pd.concat([df_type_3, df_object], axis=1, join_axes=[df_type_3.index])\ndf_formated.head(12)","a5d08602":"import matplotlib.pyplot as plt\ndf_formated.hist(alpha=0.5, figsize=(15, 15), color='blue')\nplt.show()","6c3716af":"import pandas\nfrom pandas.plotting import scatter_matrix\n\nscatter_matrix(df_formated, alpha=0.5, figsize=(20, 20))\nplt.show()","555d7b83":"from sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\n\nX = df_formated.ix[:,'Day':'Wind':] \ny = df_formated.ix[:, 'Output':]","a73ee9d2":"tree_clf = DecisionTreeClassifier(max_depth=6)\ntree_clf.fit(X, y)","b27af11f":"from sklearn.tree import export_graphviz\nfrom IPython.display import Image\nfrom subprocess import call\nfrom graphviz import Digraph\n\nt = export_graphviz(tree_clf,out_file='tree_decision.dot',\n                    feature_names=['Day','Aspect','Temperature','Humidity','Wind'],\n                    class_names=['Yes', 'No'],\n                    rounded=True,filled=True)\n\ncall(['dot', '-Tpng', 'tree_decision.dot', '-o', 'tree.png', '-Gdpi=800'])\nImage(filename = 'tree.png')","79fa7a14":"# SImple Decision Tree on Classification\n\n\n## About this notebook\nThis notebook kernel was created to help you understand more about machine learning. I intend to create tutorials with several machine learning algorithms from basic to advanced. I hope I can help you with this data science trail. For any information, you can contact me through the link below.\n\nContact me here: https:\/\/www.kaggle.com\/vitorgamalemos\/machine-learning-01-decision-tree\n\n## What is a Decision Tree?\n\n**Another notebook on decision trees:** https:\/\/www.kaggle.com\/vitorgamalemos\/machine-learning-02-simple-decision-tree\n\n<p style=\"text-align: justify;\"> A decision tree is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements. In short, a decision tree is a function as a decision model. These decision trees are trained according to a training data set. The algorithms most used to the construction of these trees are IDE, C4.5, and ASSISTANT.<\/p>","d64a5bd2":"## Categorization (Transforming to numeric data)","b2de609b":"## An example dataset","5ecfba50":"## Entropy Calculation\nThe entropy of a set can be defined as the purity of the set. Given a set $S$, with instances belonging to class $i$, with probability $pi$, the entropy is defined as:\n\nTo build a decision tree, we need to calculate the entropy using the frequency table of only one attribute and the entropy of the two attributes.\n\n### Entropy Calculation only One Attribute:\n$$\nEntropy(S) = \\sum_{i=1}^{c}({{-P_i}} ({{log_2}} {{P_i}})\n$$\n\n### Entropy Calculation Two Attributes:\n$$\nEntropy(T, X) = \\sum_{c {E} x}^{c}({{P(C)}} {{Entropy(C)}})\n$$","b43cf414":"## Visualizing our decision tree","af5bb1ed":"## Categorization using Sklearn","8b43ea33":"## Training decision tree"}}