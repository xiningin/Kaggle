{"cell_type":{"130ef3fa":"code","8d773c22":"code","27476565":"code","dbd8e15e":"code","9f447dc3":"code","69f9d2ba":"code","87843932":"code","798309ba":"code","10a47873":"code","00139d50":"markdown","ccd992a1":"markdown","90d7e33c":"markdown","ac6ffc71":"markdown","d700c70b":"markdown","813868a3":"markdown","78ad86b4":"markdown","80fd21cb":"markdown","5a48b573":"markdown","bc4ffd14":"markdown","302c7b61":"markdown"},"source":{"130ef3fa":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-deep')\nimport seaborn as sns\n\n# ML imports\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8d773c22":"iris_df = pd.read_csv('..\/input\/iris\/Iris.csv', index_col = 'Id')\niris_df.head()","27476565":"sns.pairplot(iris_df, hue = 'Species', height = 3.5)\nplt.show()","dbd8e15e":"iris_df.hist(figsize = (12,8))\nplt.show()","9f447dc3":"fig, ax = plt.subplots(figsize = (12, 8))\nsns.heatmap(iris_df.corr(), annot=True, cmap = 'RdGy')\nplt.show()","69f9d2ba":"# A function for training the different classifiers on the data\ndef classifiers(X_train, X_test, y_train, y_test):\n\n    # Model names\n    names = [\n        'Logistic Regression:',\n        'Naive Bayes:',\n        'KNN:',\n        'Decision Tree:',\n        'Random Forest:',\n        'Support Vector Machine:'\n    ]\n\n    # Instantiating the estimators\n    clfs = [\n        LogisticRegression(solver = 'liblinear', multi_class = 'auto'),\n        GaussianNB(),\n        KNeighborsClassifier(),\n        DecisionTreeClassifier(),\n        RandomForestClassifier(n_estimators = 10, random_state = 1002),\n        SVC(kernel = 'linear',gamma = 'auto')\n    ]\n    print('Accuracies:')\n\n    # Building a model with each classifier and evaluating the accuracy\n    for name, clf in zip(names, clfs):\n        mdl = clf\n        mdl = mdl.fit(X_train, y_train)\n        preds = mdl.predict(X_test)\n        print(name, '{:.4f}'.format(accuracy_score(y_test, preds)))","87843932":"# First we split the data into training and testing sets in order to evaluate the classifiers\nX_train, X_test, y_train, y_test = train_test_split(iris_df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']], iris_df['Species'],\n                                                   random_state = 1002)\nclassifiers(X_train, X_test, y_train, y_test)","798309ba":"# Splitting into training and testing sets but using 3 features this time.\nX_train, X_test, y_train, y_test = train_test_split(iris_df[['SepalLengthCm', 'SepalWidthCm', 'PetalWidthCm']], iris_df['Species'],\n                                                   random_state = 1002)\n\nclassifiers(X_train, X_test, y_train, y_test)","10a47873":"splitter = StratifiedKFold(n_splits = 5, random_state = 1002)\n\nX = iris_df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]\nY = iris_df['Species']\n\n# Model names\nnames = [\n    'Logistic Regression:',\n    'Naive Bayes:',\n    'KNN:',\n    'Decision Tree:',\n    'Random Forest:',\n    'Support Vector Machine:'\n]\n\n# Instantiating the estimators\nclfs = [\n    LogisticRegression(solver = 'liblinear', multi_class = 'auto'),\n    GaussianNB(),\n    KNeighborsClassifier(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators = 10, random_state = 1002),\n    SVC(kernel = 'linear',gamma = 'auto')\n]\nprint('Accuracies using 5-fold cross validation:')\n\nfor name, clf in zip(names, clfs): # Iterating through classifiers\n    acc = [] # List to store accuracy for each fold\n    # Iterating through training and testing sets in each fold\n    for train_idx, test_idx in splitter.split(X, Y):\n        mdl = clf.fit(X.iloc[train_idx], Y.iloc[train_idx]) # Fitting model\n        preds = mdl.predict(X.iloc[test_idx]) # Making predictions\n        acc.append(accuracy_score(Y.iloc[test_idx], preds))\n    print(name, '{:.4f}'.format(np.mean(acc)), '\u00b1',  '{:.4f}'.format(np.std(acc))) # Calculating the mean and standard deviation of each model","00139d50":"# Supervised Machine Learning\nWe will now train our machine learning models on the dataset. I will use 6 different algorithms:  \n* Logistic Regression\n* Naive Bayes\n* K Nearest Neighbours\n* Decision Tree\n* Random Forest\n* Support Vector Machine","ccd992a1":"We can see that the average accuracy across the 5 folds is much improved over the accuracy from the single run with the best accuracy found with the SVM of 98%. The SVM also has the lowest standard deviation suggesting it achieved similar accuraices across all folds.","90d7e33c":"We see from this that some feature combinations will allow us to separate the classes almost linearly (Petal width and Petal length) and some that do not allow the classes to be cleanly separated (such as sepal length and sepal width).  \nI now want to check the distributions of the features. I am interested in seeing if the features are normally distributed since many ML models assume this sort of distribution.","ac6ffc71":"We see from this that the Sepal width and length are very roughly equivalent to a normal distribution. Petal length and width are not normally distributed however.  \nLooking at the pairplot above we can see that this is due to the petal width and length of the setosa class being significantly smaller than that of the other two classes. I will need to keep this in mind when fitting the ML models.  \nOne final thing to check is the correlation of the different features. Highly correlated features can cause problems in machine learning (for example multicolinearity can lead to unstable models). We would therefore like to exclude these correlated features.  \nRemoving features also reduces issues caused by the curse of dimensionality.  ","d700c70b":"# Data Exploration\nFirst we read in the data and print the first few lines to check the formatting is correct","813868a3":"We can see that `Sepal Length`, `Petal Length` and `Petal Width` are all highly correlated with each other. `Sepal Width` has fairly low correlation with the other features. We can maybe remove some of these features when we train the final model.  First though I will train the models using all features.","78ad86b4":"# Conclusion\nHere I have shown how to perform some simple analysis on a dataset before building models to classify instances into particular categories. We saw that the best performing model was the SVM which achieved an accuracy of 98%.","80fd21cb":"# Introduction\nIn this notebook I will go through some simple steps for analysing data using pandas before applying some machine learning for predicting flower types based on the Sepal and Petal features.","5a48b573":"I will then look at the relationships between the different features as well as how easily the different classes are to separate.","bc4ffd14":"We see here that the best performing models are Naive Bayes, Random Forests and SVM. The models I have used here have the basic settings (with the exception of the SVM where I specified the linear kernel). Parameter tuning could improve the model scores further but I will leave that for another time since this is a fairly simple problem and we want to avoid overfitting to the data.  \nEarlier on we saw how the different features were correlated and noted that the `Petal Length` was highly correlated with the other features. I will rerun the models without the petal length feature to see how that affects the scores.","302c7b61":"We see a slight reduction in all the models' accuracies apart from the Naive Bayes model.  \nThis suggests that although the `Petal Length` feature is highly correlated it is also an informative feature so should be retained in the model.  \n\nIt is important to note here that the choice of training and testing split will have an impact on the accuracy since if we choose easy to classify instances (those that fall far from the decision boundaries) then our accuracy score will be high and conversely if there are harder to classify instances in the test set then the accuracy will be lower. We ideally want a test set that represents the whole dataset. This is why we would normally perform k-fold cross validation in order to get a general accuracy figure for our model:"}}