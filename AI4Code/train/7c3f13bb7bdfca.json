{"cell_type":{"966a97c5":"code","6d5da06b":"code","0a908f5d":"code","ff32942e":"code","2fcbfd6e":"code","dd307beb":"code","2a8b4832":"code","273fc5c1":"code","590dbf9b":"code","de01afc9":"code","492303d3":"code","1f39efba":"code","9b96758c":"code","0fa49247":"code","28bb4271":"code","8859b985":"code","a5a7c021":"code","70e2d4c3":"code","97f44c10":"code","c2b07ba4":"code","bfb57a37":"code","ecabb9af":"code","e571f409":"code","d7c581fa":"code","82d01090":"code","4c6d4d82":"code","70d647f9":"code","f8cb3dae":"code","631d1c74":"code","488a6653":"code","5036dfb1":"code","e5841b36":"code","6cc6d7d1":"code","04b8380f":"code","8bf50271":"code","3b10df51":"code","4820b313":"code","5388ff23":"code","c0750038":"code","8e7b3d40":"code","70592d1e":"code","a47e65df":"code","d8238fbb":"code","65d864c5":"code","ab986705":"code","736652e4":"code","96bb48d7":"code","2632ac26":"code","ef3491d5":"code","741f121b":"code","a7e55737":"code","e3a8101a":"code","6e73e38f":"code","bb6dd533":"code","aca69a4d":"code","ff620a31":"code","6d97a6e3":"code","462fed0e":"code","c01b9153":"code","c2c6fb17":"code","714de925":"code","77862aee":"code","2a3fc452":"code","e9f99950":"code","a593333e":"code","21b0c15b":"code","df6891aa":"code","c277136a":"code","2604a000":"code","8b1655f4":"code","7955fbbf":"code","0fdd1e47":"code","64b9fd9f":"code","4b5a659a":"code","c0dfc984":"code","d0e088ce":"code","9b9073ce":"code","baecc10a":"code","62394ca3":"code","48755e09":"code","0a62f262":"code","92d9b7f8":"code","c0b49577":"code","eb3afe73":"markdown","15416ec1":"markdown","532734f1":"markdown","beee14ad":"markdown","b569bb3a":"markdown","b3bbe3ce":"markdown","76c91cd7":"markdown","7d7c627b":"markdown","754b1429":"markdown","15c581b1":"markdown","300d0c78":"markdown","df208f48":"markdown","11e2a631":"markdown","528bfceb":"markdown","7cdbb9f7":"markdown"},"source":{"966a97c5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6d5da06b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy as sp\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Standard plotly imports\n#import plotly.plotly as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\n#import cufflinks\n#import cufflinks as cf\nimport plotly.figure_factory as ff\n\n# Using plotly + cufflinks in offline mode\ninit_notebook_mode(connected=True)\n#cufflinks.go_offline(connected=True)\n\n# Preprocessing, modelling and evaluating\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n\n## Hyperopt modules\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\nfrom functools import partial\n\nimport os\nimport gc\n\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nplt.style.use('fivethirtyeight')\n\nplt.figure(figsize=(25,25))\n\nimport pandas_profiling as pp\n\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\n!pip install fastai==1.0.57\nimport fastai\n\nfrom fastai import *\nfrom fastai.tabular import *\nfrom fastai.collab import *\n\n# from torchvision.models import *\n# import pretrainedmodels\n\n# from utils import *\nimport sys\n\nfrom fastai.callbacks.hooks import *\n\nfrom fastai.callbacks.tracker import EarlyStoppingCallback\nfrom fastai.callbacks.tracker import SaveModelCallback\n\nimport pandas as pd \nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nfrom sklearn.manifold import TSNE\n\nfrom sklearn.experimental import enable_hist_gradient_boosting \nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nimport gc\nfrom datetime import datetime \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn import svm\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier  \nfrom xgboost.sklearn import XGBRegressor\n\nfrom scipy.special import erfinv\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.utils.data import *\nfrom torch.optim import *\nfrom fastai.tabular import *\nimport torch.utils.data as Data\nfrom fastai.basics import *\nfrom fastai.callbacks.hooks import *\nfrom tqdm import tqdm_notebook as tqdm\n\nfrom hyperopt import hp, tpe\nfrom hyperopt.fmin import fmin\nfrom hyperopt import STATUS_OK\n\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.metrics import make_scorer\nfrom surprise import Reader, Dataset, SVD\nfrom surprise.model_selection import cross_validate, train_test_split\nfrom sklearn.decomposition import PCA\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set_style('whitegrid')","0a908f5d":"df_mov = pd.read_csv('..\/input\/movielens-100k\/ml-latest-small\/movies.csv')\ndf_tag = pd.read_csv('..\/input\/movielens-100k\/ml-latest-small\/tags.csv', sep=',', parse_dates=['timestamp'])\ndf_rating = pd.read_csv('..\/input\/movielens-100k\/ml-latest-small\/ratings.csv', sep=',', parse_dates=['timestamp'])\n\ndf_mov.shape, df_tag.shape, df_rating.shape","ff32942e":"df_mov.head(2)","2fcbfd6e":"df_tag.head(2)","dd307beb":"df_rating.head(2)","2a8b4832":"df_rating.rating.value_counts(normalize=True)","273fc5c1":"df_mov.isnull().any()","590dbf9b":"df_rating.isnull().any()","de01afc9":"movies_with_ratings = df_mov.merge(df_rating, on='movieId', how='inner')\nmovies_with_ratings.shape","492303d3":"gc.collect()","1f39efba":"movies_with_ratings.head(10)","9b96758c":"import collections\n\ncollections.Counter(\" \".join(movies_with_ratings['genres']).split(\"|\")).most_common(10)","0fa49247":"collections.Counter(\" \".join(movies_with_ratings['genres']).split(\"|\")).most_common()[-10:]","28bb4271":"movie_stats = movies_with_ratings.groupby('title').agg({'rating': [np.size, np.mean]})\nmovie_stats.head()","8859b985":"atleast_100 = movie_stats['rating']['size'] >= 100\nmovie_stats[atleast_100].sort_values([('rating', 'mean')], ascending=False)[:15]","a5a7c021":"movie_stats[atleast_100].sort_values([('rating', 'mean')], ascending=True)[:15]","70e2d4c3":"movies_with_ratings.shape","97f44c10":"pivot = movies_with_ratings.pivot(index='userId', columns='movieId', values='rating')\npivot.replace(np.nan, 0, inplace=True, regex=True)\npivot.head(6)","c2b07ba4":"print(pivot.shape)\nfrom sklearn.preprocessing import StandardScaler \nX_std = StandardScaler().fit_transform(pivot)\ndel pivot","bfb57a37":"gc.collect()","ecabb9af":"cov_mat = np.cov(X_std.T)\nevals, evecs = np.linalg.eig(cov_mat)\ndel X_std","e571f409":"k = 10\nmovieId = 1 # Grab an id from movies.dat\ntop_n = 5\n\ndef top_cosine_similarity(data, movieId, top_n=10):\n    index = movieId - 1 # Movie id starts from 1\n    movie_row = data[index, :]\n    magnitude = np.sqrt(np.einsum('ij, ij -> i', data, data))\n    similarity = np.dot(movie_row, data.T) \/ (magnitude[index] * magnitude)\n    sort_indexes = np.argsort(-similarity)\n    return sort_indexes[:top_n]\n\n# Helper function to print top N similar movies\ndef print_similar_movies(movie_data, movieId, top_indexes):\n    print('Recommendations for {0}: \\n'.format(\n    movie_data[movies_with_ratings.movieId == movieId].title.values[0]))\n    for id in top_indexes + 1:\n        print(movie_data[movies_with_ratings.movieId == id].title.values[0])\n\nsliced = evecs[:, :k] # representative data\nindexes = top_cosine_similarity(sliced, movieId, top_n)\nprint_similar_movies(movies_with_ratings, movieId, indexes)","d7c581fa":"df_mov.head(10)","82d01090":"df_rating.head(10)","4c6d4d82":"R_df = df_rating.pivot(index = 'userId', columns='movieId', values='rating').fillna(0)\nR_df.head()","70d647f9":"R = R_df.as_matrix()\nuser_ratings_mean = np.mean(R, axis = 1)\nR_demeaned = R - user_ratings_mean.reshape(-1, 1)","f8cb3dae":"from scipy.sparse.linalg import svds\nU, sigma, Vt = svds(R_demeaned, k = 50)","631d1c74":"sigma = np.diag(sigma)","488a6653":"all_user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) + user_ratings_mean.reshape(-1, 1)\npreds_df = pd.DataFrame(all_user_predicted_ratings, columns = R_df.columns)","5036dfb1":"def recommend_movies(predictions_df, userId, movies_df, original_ratings_df, num_recommendations=5):\n    \n    # Get and sort the user's predictions\n    user_row_number = userId - 1 # UserID starts at 1, not 0\n    sorted_user_predictions = predictions_df.iloc[user_row_number].sort_values(ascending=False)\n    \n    # Get the user's data and merge in the movie information.\n    user_data = original_ratings_df[original_ratings_df.userId == (userId)]\n    user_full = (user_data.merge(df_mov, how = 'left', left_on = 'movieId', right_on = 'movieId').\n                     sort_values(['rating'], ascending=False)\n                 )\n\n    print ('User {0} has already rated {1} movies.'.format(userId, user_full.shape[0]))\n    print ('Recommending the highest {0} predicted ratings movies not already rated.'.format(num_recommendations))\n    \n    # Recommend the highest predicted rating movies that the user hasn't seen yet.\n    recommendations = (df_mov[~df_mov['movieId'].isin(user_full['movieId'])].\n         merge(pd.DataFrame(sorted_user_predictions).reset_index(), how = 'left',\n               left_on = 'movieId',\n               right_on = 'movieId').\n         rename(columns = {user_row_number: 'Predictions'}).\n         sort_values('Predictions', ascending = False).\n                       iloc[:num_recommendations, :-1]\n                      )\n\n    return user_full, recommendations\n\nalready_rated, predictions = recommend_movies(preds_df, 8, df_mov, df_rating, 5)","e5841b36":"already_rated.dropna().head(5)","6cc6d7d1":"predictions","04b8380f":"mov_with_rating = movies_with_ratings[['userId', 'title', 'rating']]\nreader = Reader(rating_scale=(1,5))\ndata = Dataset.load_from_df(mov_with_rating, reader)","8bf50271":"train, test = train_test_split(data, test_size=0.25)","3b10df51":"model = SVD(n_factors=200)\nmodel.fit(train)","4820b313":"item_to_row_idx: Dict[Any, int] = model.trainset._raw2inner_id_items\ntoy_story_row_idx : int = item_to_row_idx['Toy Story (1995)']","5388ff23":"model.qi[toy_story_row_idx]","c0750038":"movies_with_ratings.head(2)","8e7b3d40":"a_user = 196\na_product = 'Toy Story (1995)'\n\nmodel.predict(a_user, a_product)","70592d1e":"predictions = model.test(test)\n\nfrom surprise import accuracy\n\naccuracy.rmse(predictions)","a47e65df":"Mapping_file = dict(zip(movies_with_ratings.title.tolist(), movies_with_ratings.movieId.tolist()))","d8238fbb":"def pred_user_rating(ui):\n    if ui in movies_with_ratings.userId.unique():\n        ui_list = movies_with_ratings[movies_with_ratings.userId == ui].movieId.tolist()\n        d = {k: v for k,v in Mapping_file.items() if not v in ui_list}        \n        predictedL = []\n        for i, j in d.items():     \n            predicted = model.predict(ui, j)\n            predictedL.append((i, predicted[3])) \n        pdf = pd.DataFrame(predictedL, columns = ['movies', 'ratings'])\n        pdf.sort_values('ratings', ascending=False, inplace=True)  \n        pdf.set_index('movies', inplace=True)    \n        return pdf.head(10)        \n    else:\n        print(\"User Id does not exist in the list!\")\n        return None","65d864c5":"user_id = 87\npred_user_rating(user_id)","ab986705":"from collections import defaultdict\n\nfrom surprise import SVD\nfrom surprise import Dataset\n\n\ndef get_top_n(predictions, n=10):\n    '''Return the top-N recommendation for each user from a set of predictions.\n\n    Args:\n        predictions(list of Prediction objects): The list of predictions, as\n            returned by the test method of an algorithm.\n        n(int): The number of recommendation to output for each user. Default\n            is 10.\n\n    Returns:\n    A dict where keys are user (raw) ids and values are lists of tuples:\n        [(raw item id, rating estimation), ...] of size n.\n    '''\n\n    # First map the predictions to each user.\n    top_n = defaultdict(list)\n    for uid, iid, true_r, est, _ in predictions:\n        top_n[uid].append((iid, est))\n\n    # Then sort the predictions for each user and retrieve the k highest ones.\n    for uid, user_ratings in top_n.items():\n        user_ratings.sort(key=lambda x: x[1], reverse=True)\n        top_n[uid] = user_ratings[:n]\n\n    return top_n\n\n\n# First train an SVD algorithm on the movielens dataset.\ndata = data\ntrainset = data.build_full_trainset()\nalgo = SVD()\nalgo.fit(trainset)\n\n# Than predict ratings for all pairs (u, i) that are NOT in the training set.\ntestset = trainset.build_anti_testset()\npredictions = algo.test(testset)\n\ntop_n = get_top_n(predictions, n=3)\n\n# Print the recommended items for each user\nfor uid, user_ratings in top_n.items():\n    print(uid, [iid for (iid, _) in user_ratings])","736652e4":"movies_with_ratings.drop(['genres'], axis=1, inplace=True)","96bb48d7":"movies_with_ratings.head()","2632ac26":"data = CollabDataBunch.from_df(movies_with_ratings, seed=42, valid_pct=0.1, item_name='title', user_name='userId', \n                              rating_name='rating')","ef3491d5":"data.show_batch()","741f121b":"y_range = [0,5.5]\nlearn = collab_learner(data, n_factors=40, y_range=y_range, use_nn=True, layers=[256, 128])\nlearn.loss = torch.nn.SmoothL1Loss\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","a7e55737":"learn.fit_one_cycle(5, 1e-3, wd=[0.1])","e3a8101a":"learn.recorder.plot_losses()","6e73e38f":"movies_with_ratings.iloc[250]","bb6dd533":"learn.predict(movies_with_ratings.iloc[250])","aca69a4d":"learn.get_preds(ds_type=DatasetType.Valid)","ff620a31":"def pred_user_rating(ui):\n    if ui in movies_with_ratings.userId.unique():\n        ui_list = movies_with_ratings[movies_with_ratings.userId == ui].movieId.tolist()\n        d = {k: v for k,v in Mapping_file.items() if not v in ui_list}\n        \n        predictedL = []\n        for i, j in d.items():     \n            predicted = learn.predict(movies_with_ratings.iloc[ui])\n            predictedL.append((i, predicted[0])) \n        pdf = pd.DataFrame(predictedL, columns = ['movies', 'ratings'])\n        #pdf.sort_values('ratings', ascending=False, inplace=True)  \n        pdf.set_index('movies', inplace=True)    \n        return pdf.head(10)        \n    else:\n        print(\"User Id does not exist in the list!\")\n        return None","6d97a6e3":"predicted = learn.predict(movies_with_ratings.iloc[8])\npredicted[0]","462fed0e":"user_id = 9\npred_user_rating(user_id)","c01b9153":"from keras.layers import Input, Embedding, Flatten, Dot, Dense, Concatenate, Dropout\nfrom keras.models import Model","c2c6fb17":"mov_with_rating = movies_with_ratings[['userId', 'movieId', 'rating']]","714de925":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(mov_with_rating, test_size=0.2, random_state=42)","77862aee":"train.shape, test.shape","2a3fc452":"train.head()","e9f99950":"n_users = len(train.userId.unique())","a593333e":"n_movies = len(train.movieId.unique())","21b0c15b":"n_users, n_movies","df6891aa":"# Create Movie Embedding\nmovie_input = Input(shape=[1], name=\"Movie-Input\")\nmovie_embedding = Embedding(200000, 5, name='Movie-Embedding')(movie_input)\nmovie_vec = Flatten(name='Movie-Vector')(movie_embedding)","c277136a":"# Create User Embedding\nuser_input = Input(shape=[1], name=\"User-Input\")\nuser_embedding = Embedding(200000, 5, name='User-Embedding')(user_input)\nuser_vec = Flatten(name='User-Vector')(user_embedding)","2604a000":"# Concatenate two vectors\n\nconc = Concatenate()([user_vec, movie_vec])","8b1655f4":"# fully connected layers\n\nfrom keras.models import Sequential\nmodel = Sequential()\n\nfc1 = Dense(128, activation='relu')(conc)\nfc1 = Dropout(0.3)(fc1)\nfc2 = Dense(32, activation='relu')(fc1)\nfc2 = Dropout(0.3)(fc2)\nout = Dense(1)(fc2)\n\n# Create Model and Compile it\n\nmodel = Model([user_input, movie_input], out)\nmodel.compile('adam', 'mean_squared_error')","7955fbbf":"from keras.models import load_model\n\nif os.path.exists('regression_model.h5'):\n    model = load_model('regression_model.h5')\nelse:\n    history = model.fit([train.userId, train.movieId], train.rating, epochs=15, verbose=1)\n    model.save('regression_model.h5')\n    plt.plot(history.history['loss'])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Training Error\")","0fdd1e47":"model.evaluate([test.userId, test.movieId], test.rating)","64b9fd9f":"predictions = model.predict([test.userId.head(10), test.movieId.head(10)])\n\n[print(predictions[i], test.rating.iloc[i]) for i in range(0,10)]","4b5a659a":"# Extract embeddings\nmov_em = model.get_layer('Movie-Embedding')\nmov_em_weights = mov_em.get_weights()[0]","c0dfc984":"from sklearn.decomposition import PCA\nimport seaborn as sns\n\npca = PCA(n_components=2)\npca_result = pca.fit_transform(mov_em_weights)\nsns.scatterplot(x=pca_result[:,0], y=pca_result[:,1])","d0e088ce":"mov_em_weights.shape","9b9073ce":"movie_data = np.array(list(set(movies_with_ratings.movieId)))\nmovie_data[:5]","baecc10a":"user_data = np.array([1 for i in range(len(movie_data))])\nuser_data[:5]","62394ca3":"predictions = model.predict([user_data, movie_data])\npredictions = np.array([a[0] for a in predictions])\npredictions[:5]","48755e09":"recommended_movie_ids = (-predictions).argsort()[:5]\n\nrecommended_movie_ids","0a62f262":"predictions[recommended_movie_ids]","92d9b7f8":"movies_with_ratings[movies_with_ratings.movieId == 5877]","c0b49577":"movies_with_ratings[movies_with_ratings['movieId'].isin(recommended_movie_ids)]","eb3afe73":"# Predictions","15416ec1":"# A look at the Data","532734f1":"# Introduction","beee14ad":"# Fastai Collab","b569bb3a":"# Collaborative Filtering using SVD","b3bbe3ce":"# Keras Implementation","76c91cd7":"Lets see which Movie Genres are most popular","7d7c627b":"# Importing Libraries","754b1429":"# Collaborative filtering using Surprise","15c581b1":"# Visualizing Embeddings","300d0c78":"Lets see if there are any null values in our data","df208f48":"Lets merge Movies with Ratings","11e2a631":"## Top and Worse Movies","528bfceb":"Lets see Most Highly Rated movies which got at least 100 votes","7cdbb9f7":"## Top and Worse Movie Genres"}}