{"cell_type":{"14a8e7d3":"code","ce12b3ab":"code","29a2597c":"code","4df74dcb":"code","db576595":"code","f42c8f6a":"code","ed13fa88":"code","9ad03c39":"code","ec5fb951":"code","693d6f3f":"code","8358e945":"code","e9d5cc34":"code","2f13a16a":"code","bdb4e713":"code","5347079b":"code","c3a89922":"code","0a2c4d9e":"code","eb95be18":"code","0e2f4b74":"code","502eaf04":"code","c45609ce":"code","f41c01c4":"code","2d548035":"code","f8b0d072":"code","d3a38e1a":"code","220e5cf0":"code","815983db":"code","7f14f3a6":"code","9f14bd60":"code","a2213701":"code","a957e337":"code","3ec55152":"code","df0b80a9":"code","81c4c2c3":"code","f277a770":"code","e171f24c":"code","fa86c84c":"code","c31279c3":"code","b59b2546":"code","c810dab2":"code","df874c70":"code","fe034630":"code","82c8631f":"code","9ab376ee":"code","929c6b9c":"code","a992c12f":"code","96773112":"code","456377dc":"code","d3445c0a":"code","d81423a7":"code","8c3649f9":"code","8ddbf23c":"code","496c99a3":"code","68703353":"markdown","3fb6b346":"markdown","8465096b":"markdown","07e79999":"markdown","84966bf9":"markdown","4385dde4":"markdown","27cf2518":"markdown","e2ea8c25":"markdown","84de9d91":"markdown","10286cba":"markdown"},"source":{"14a8e7d3":"# import library \nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings \nwarnings.filterwarnings('ignore')\nnp.random.seed(43)","ce12b3ab":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","29a2597c":"print('Shape of training data : ',train.shape, '\\nShape of test data : ', test.shape)\n","4df74dcb":"data = pd.concat([train, test],axis = 0)\nprint('Shape of data : ',data.shape)","db576595":"data.head()","f42c8f6a":"# function for missing value \ndef missing_df(data):\n    missing_df = pd.DataFrame({'missing_values':data.isnull().sum(),\\\n                            'ratio': (data.isnull().sum()\/data.shape[0]).round(2),\\\n                            'missing_train_data' : data[(data.Survived.notnull())].isnull().sum(),\\\n                            'missing_test_data' : data[(data.Survived.isnull())].isnull().sum(),\\\n                              'feature_type': data.dtypes})   \n    return missing_df[missing_df.missing_values>0].sort_values('missing_values', ascending = False)","ed13fa88":"missing_df(data)","9ad03c39":"data.Cabin.unique()","ec5fb951":"data['Cabin'] = data['Cabin'].str[0]","693d6f3f":"data.Cabin.unique()","8358e945":"data.Cabin.fillna('O',inplace=True) # because there are lot of null values thats why we fill an imagine cabin","e9d5cc34":"data.Cabin.unique()","2f13a16a":"data.Age.fillna(data.Age.median(), inplace = True)\ndata.Embarked.fillna(data.Embarked.mode().values[0],inplace = True)\ndata.Fare.fillna(data.Fare.median(),inplace = True)","bdb4e713":"missing_df(data)","5347079b":"data = data.set_index(['PassengerId'])","c3a89922":"data= data.drop(columns= ['Name','Ticket'],axis= 1)","0a2c4d9e":"data.head()","eb95be18":"data.Pclass = data.Pclass.astype('str')","0e2f4b74":"cat_cols = data.loc[:,data.dtypes == 'object'].columns.tolist()\nnum_cols = data.loc[:,data.dtypes != 'object'].columns.tolist()","502eaf04":"def cat_visualization(data,col):\n    fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize= (15,5),constrained_layout = True )\n    sns.countplot(data[data.Survived.notnull()][col], ax= ax1).set_title('Train_data countplot',fontsize= 13,color = 'm')\n    sns.countplot(data[data.Survived.isnull()][col], ax= ax2).set_title('Test_data countplot',fontsize= 13,color = 'm')\n    sns.countplot(x=col, hue= 'Survived',data= data[data.Survived.notnull()],ax= ax3)\n    ax3.set_title('Train Data With Survived or not',fontsize= 13,color = 'm')\n    fig.suptitle('Variable : '+col,fontsize=18,color='#1b1f33')\n    for ax in (ax1,ax2,ax3):\n        for p in ax.patches:\n            ax.annotate('{}'.format(p.get_height()), (p.get_x()+0, p.get_height()+5))","c45609ce":"def num_visualization(data,col):\n    fig, (ax1,ax2,ax3,ax4) = plt.subplots(1,4,figsize= (20,4),constrained_layout = True)\n    sns.kdeplot(x= col,data= data[data.Survived.notnull()],fill= True, color='darkblue',label = 'Train',ax= ax1)\n    sns.kdeplot(x= col,data= data[data.Survived.isnull()], color='hotpink',alpha= 0.2,label = 'Test',ax= ax1)\n    ax1.set_title('Distribution',fontsize= 13,color='m')\n    sns.boxplot(y=col, data= data[data.Survived.notnull()],ax=ax2).set_title('Train data -- Boxplot',fontsize= 13,color='m')\n    sns.boxplot(y=col, data= data[data.Survived.isnull()],ax=ax3).set_title('Test data -- Boxplot',fontsize= 13,color='m')\n    sns.swarmplot(y=col,x='Sex',hue= 'Survived',data= data[data.Survived.notnull()],ax= ax4).set_title('Train data --Swarmplot',fontsize= 13,color='m')\n    fig.suptitle('Variable : '+col,fontsize=18,color='#1b1f33')\n    plt.grid()","f41c01c4":"for col in cat_cols:\n    cat_visualization(data,col)","2d548035":"sns.set_theme(style=\"whitegrid\")\n","f8b0d072":"for col in num_cols:\n    if col =='Survived':\n        continue\n    num_visualization(data,col)","d3a38e1a":"def outlier_hundle(data,col): \n    Q1 = data[col].quantile(0.25)\n    Q3 = data[col].quantile(0.75)\n    IQR = Q3 - Q1 \n    data[col] = data[col].apply( lambda x: Q3 + 1.5 * IQR if x > Q3 + 1.5 * IQR  else x)\n    data[col] = data[col].apply( lambda x: Q1 - 1.5 * IQR if x < Q1 - 1.5 * IQR  else x)","220e5cf0":"data[\"is_couple\"] = data['SibSp'].apply(lambda x: 0 if x==0 else 1)","815983db":"data['family'] = data['SibSp'] + data['Parch']\ndata.family.unique()","7f14f3a6":"data.family.value_counts()","9f14bd60":"data['Single_family'] = data['family'].apply(lambda x: 1 if x > 0 & x<4 else 0)\ndata['Joint_family'] = data['family'].apply(lambda x: 1 if x > 3 else 0)\ndata['Single'] = data['family'].apply(lambda x: 1 if x == 0 else 0)","a2213701":"cabin_int = (data.pivot_table('Survived',['Cabin']).values.reshape(1,9)*10).round()[0].tolist()\ncabin = data.Cabin.unique().tolist()","a957e337":"data['cabin_int'] = data['Cabin'].replace(cabin,cabin_int)","3ec55152":"for col in ['Age','Fare']:\n    outlier_hundle(data,col)","df0b80a9":"for col in ['Age','Fare']:\n    num_visualization(data,col)","81c4c2c3":"data.columns","f277a770":"data1 = data[['Survived', 'Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'is_couple', 'Single_family', 'Joint_family','Single', 'cabin_int']]\ndata1 = pd.get_dummies(data1)","e171f24c":"data1.head()","fa86c84c":"train_data = data1[data1.Survived.notnull()]\ntest_data = data1[data1.Survived.isnull()]","c31279c3":"training_set = train_data.drop('Survived',axis= 1)\ntest_set = test_data.drop('Survived',axis= 1)\ntarget= train_data['Survived'].values\nprint('Train shape : ',training_set.shape, '\\nTest shape : ',test_set.shape,'\\nTarget shape : ', target.shape)","b59b2546":"from sklearn.preprocessing import MinMaxScaler as Scaler\nscaler = Scaler()\nscaler.fit(training_set)\nscaled_train = scaler.transform(training_set)\nscaled_test = scaler.transform(test_set)","c810dab2":"from sklearn.linear_model import LogisticRegression as Lg\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier as Rfc , GradientBoostingClassifier as Gbc\nfrom xgboost import XGBClassifier as Xgb\nfrom sklearn.naive_bayes import GaussianNB as nb\nfrom sklearn.neighbors import KNearestNeighbors as knn\nfrom sklearn.trees import DecisionTreeClassifier as tree\n","df874c70":"from sklearn.model_selection import RandomizedSearchCV as CV\nfrom sklearn.model_selection import KFold\nkfold = KFold(n_splits = 10 ,shuffle = True, random_state = 43)","fe034630":"lg_params = {'solver':['newton-cg','lbfgs','liblinear','sag','saga'],\\\n             'penalty': ['none','l1','l2','elasticnet'],\\\n             'C': [100,10,1,0.1,0.01,0.01]}\nsvc_params = {'kernel': ['linear','rbf','poly','sigmoid'],\\\n              'C': [100,10,1,0.1,0.01,0.001]}\nforest_params= {'max_features': [i for i in range(2,20,2)],\\\n                'n_estimators': [i for i in range(80,500,80)],\\\n                'max_depth': [3,4,5,6,7,8,9],\\\n                'min_samples_split': [2,4,5,9],\\\n                'min_samples_leaf': [1,2,4]}\ngbc_params = {'max_features': [i for i in range(2,20,2)],\\\n              'n_estimators': [i for i in range(80,500,80)],\\\n              'max_depth': [3,4,5,6,7,8,9],\\\n              'learning_rate': [0.001,0.01,0.1,1],\\\n              'subsample': [0.1,0.3,0.5,0.7,0.9]}\nnb_params = {}\nparams = [lg_params,svc_params,forest_params,gbc_params, nb_params]\nmodels = [Lg(),SVC(),Rfc(),Gbc(),nb()]","82c8631f":"from sklearn.model_selection import train_test_split as tts \nx_train, x_test, y_train, y_test = tts(scaled_train,target)","9ab376ee":"dic = {}\npredict = pd.DataFrame()\npredictions = pd.DataFrame()\nfor model,param in zip(models,params):\n  \n    dic_2 = {}\n    cv = CV(model,param,n_jobs= -1,cv =kfold, verbose = 1)\n    cv.fit(x_train,y_train )\n    dic_2['best_score']  = cv.best_score_\n    dic_2['best_params'] = cv.best_params_\n    predict[str(model).split('(')[0]] = cv.predict(x_test)\n    predictions[str(model).split('(')[0]] = cv.predict(scaled_test)\n    dic[str(model).split('(')[0]] = dic_2\ndf = pd.DataFrame(dic)","929c6b9c":"df","a992c12f":"from sklearn.metrics import confusion_matrix as cm\n","96773112":"cm_df = {}\ndef evoluation_model(col):\n    matrix = cm(y_test,predict[col].values)\n    cm_df[col] = ((matrix.ravel()[0]+ matrix.ravel()[3])*100\/matrix.ravel().sum()).round(2)","456377dc":"for col in predict.columns:\n    evoluation_model(col)","d3445c0a":"cm_df","d81423a7":"predictions.head()","8c3649f9":"submission1 = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubmission1['Survived'] = predictions.RandomForestClassifier.values\nsubmission1.to_csv('kaggle1.csv',index= False)","8ddbf23c":"predictions['blend'] = predictions[['RandomForestClassifier','LogisticRegression','GradientBoostingClassifier']].sum()\npredictions['blend'] = predictions['blend'].apply(lambda x: 1 if x>1 else 0)","496c99a3":"submission1['Survived'] = predictions.blend.values\nsubmission1.to_csv('kaggle2.csv',index= False)","68703353":"# Load Data ","3fb6b346":"# Fill data","8465096b":"# Blend_prediction ","07e79999":"# Feature Engineering  \n-  For this we first visualize the data to understand the data ","84966bf9":"# Prepare data for Machine Learning Model ..","4385dde4":"# Prepare Model","27cf2518":"# A pro method to solve titanic project....","e2ea8c25":"# Prediction with highest accuracy ","84de9d91":"# We Follow the following steps in this Project...\n- Load Data\n- Fill Data\n- Feature Engineering \n- Prepair Data for Model\n- Prepair Model \n- Model Evoluation \n- Predict \n- Predict with blend models ","10286cba":"# Model Evoluation \n with confusion matrix"}}