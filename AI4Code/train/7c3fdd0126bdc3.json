{"cell_type":{"c63f8173":"code","cdcc9f90":"code","f219ae39":"code","faeeede4":"code","ff8037d1":"code","38c5a774":"code","0abde173":"code","2cbfe52c":"code","1e04bb1c":"code","31bd0243":"code","f81b96b1":"code","3d348f1b":"code","5b46ed2c":"code","c888c33a":"code","d95f9bff":"code","4e85eac3":"code","a756634f":"code","7384f1f9":"code","29a05db0":"code","104a5419":"code","20a96d97":"code","eb32e3c2":"code","98e5b536":"code","44db402f":"code","e6eb3655":"code","2e0784a5":"code","1a89798f":"code","10c4f631":"code","2a79cb69":"code","686a83db":"code","33a452c5":"code","7593b515":"code","2be42ec2":"code","997e8d7f":"code","41dc9e9a":"code","d4debe27":"code","c5dee078":"code","b5dabe64":"code","1bfd31f3":"code","206cc6fb":"code","56c3441a":"code","3508c689":"code","41c3668b":"code","931a1aa3":"code","51f3022c":"code","a4df510b":"code","cd2dde7d":"code","cb6ea753":"code","0f4d8930":"code","9ee15acd":"code","4764dc6d":"code","bd16d52a":"code","aa0e3136":"code","0bb26226":"code","502ff8c2":"code","7e89d71e":"code","34544fd0":"code","409f7c6e":"code","fcd0d9be":"code","2503a46b":"code","f28a4a14":"code","49093990":"code","19452eb5":"code","dad4575d":"code","85271b83":"code","15a6051a":"code","ee59df73":"markdown","bbd919f0":"markdown","5385c60d":"markdown","5c02f089":"markdown","001f1dd5":"markdown","c669e591":"markdown","6950eea4":"markdown","962f97e8":"markdown","31f1853b":"markdown","95aad7ab":"markdown","755424c3":"markdown","e54eb876":"markdown","0ff6bb98":"markdown","2f853bf8":"markdown","7a2950f4":"markdown","8b49fdad":"markdown","351dc9b3":"markdown","c3a7cfa8":"markdown","197316ff":"markdown","48a652a9":"markdown"},"source":{"c63f8173":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cdcc9f90":"import pandas as pd\n\ndf = pd.read_csv('\/kaggle\/input\/covid19-tweets\/covid19_tweets.csv')","f219ae39":"df.head()","faeeede4":"df.tail()","ff8037d1":"df.describe()","38c5a774":"print('Shape of tweets dataframe : {}'.format(df.shape))","0abde173":"df.info()","2cbfe52c":"\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef return_missing_values(data_frame):\n    missing_values = data_frame.isnull().sum()\/len(data_frame)\n    missing_values = missing_values[missing_values>0]\n    missing_values.sort_values(inplace=True)\n    return missing_values\n\ndef plot_missing_values(data_frame):\n    missing_values = return_missing_values(data_frame)\n    missing_values = missing_values.to_frame()\n    missing_values.columns = ['count']\n    missing_values.index.names = ['Name']\n    missing_values['Name'] = missing_values.index\n    sns.set(style='whitegrid', color_codes=True)\n    sns.barplot(x='Name', y='count', data=missing_values)\n    plt.xticks(rotation=90)\n    plt.show()\n     ","1e04bb1c":"\n\nreturn_missing_values(df)","31bd0243":"plot_missing_values(df)","f81b96b1":"# heatmap representation of missing values\n\n# plasma,visdir\n\nsns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='ocean')","3d348f1b":"def return_unique_values(data_frame):\n    unique_dataframe = pd.DataFrame()\n    unique_dataframe['Features'] = data_frame.columns\n    uniques = []\n    for col in data_frame.columns:\n        u = data_frame[col].nunique()\n        uniques.append(u)\n    unique_dataframe['Uniques'] = uniques\n    return unique_dataframe","5b46ed2c":"udf = return_unique_values(df)\nprint(udf)","c888c33a":"f, ax = plt.subplots(1,1, figsize=(10,5))#plt.figure(figsize=(10, 5))\n\nsns.barplot(x=udf['Features'], y=udf['Uniques'], alpha=0.8)\nplt.title('Bar plot for #unique values in each column')\nplt.ylabel('#Unique values', fontsize=12)\nplt.xlabel('Features', fontsize=12)\nplt.xticks(rotation=90)\nplt.show()","d95f9bff":"def plot_frequency_charts(df, feature, title, pallete):\n    freq_df = pd.DataFrame()\n    freq_df[feature] = df[feature]\n    \n    f, ax = plt.subplots(1,1, figsize=(16,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette=pallete)\n    g.set_title(\"Number and percentage of {}\".format(title))\n\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n\n    plt.title('Frequency of {} tweeting about Corona'.format(feature))\n    plt.ylabel('Frequency', fontsize=12)\n    plt.xlabel(title, fontsize=12)\n    plt.xticks(rotation=90)\n    plt.show()\n    ","4e85eac3":"plot_frequency_charts(df, 'user_name', 'User Names','Wistia')","a756634f":"plot_frequency_charts(df, 'user_location', 'User Locations', 'BuGn_r')","7384f1f9":"plot_frequency_charts(df, 'source','Source', 'vlag')","29a05db0":"from string import punctuation\nfrom nltk.corpus import stopwords\nprint(stopwords.words('english')[10:15])\n\ndef punctuation_stopwords_removal(sms):\n    # filters charecter-by-charecter : ['h', 'e', 'e', 'l', 'o', 'o', ' ', 'm', 'y', ' ', 'n', 'a', 'm', 'e', ' ', 'i', 's', ' ', 'p', 'u', 'r', 'v', 'a']\n    remove_punctuation = [ch for ch in sms if ch not in punctuation]\n    # convert them back to sentences and split into words\n    remove_punctuation = \"\".join(remove_punctuation).split()\n    filtered_sms = [word.lower() for word in remove_punctuation if word.lower() not in stopwords.words('english')]\n    return filtered_sms","104a5419":"df.head()","20a96d97":"from collections import Counter\n\ndef draw_bar_graph_for_text_visualization(df, location):\n    tweets_from_loc = df.loc[df.user_location==location]\n    tweets_from_loc.loc[:, 'text'] = tweets_from_loc['text'].apply(punctuation_stopwords_removal)\n    loc_tweets_curated = tweets_from_loc['text'].tolist()\n    loc_tweet_list = []\n    for sublist in loc_tweets_curated:\n        for word in sublist:\n            loc_tweet_list.append(word)\n    loc_tweet_count = Counter(loc_tweet_list)\n    loc_top_30_words = pd.DataFrame(loc_tweet_count.most_common(50), columns=['word', 'count'])\n    fig, ax = plt.subplots(figsize=(16, 6))\n    sns.barplot(x='word', y='count', \n                data=loc_top_30_words, ax=ax)\n    plt.title(\"Top 50 Prevelant Words in {}\".format(location))\n    plt.xticks(rotation='vertical');\n    ","eb32e3c2":"from wordcloud import WordCloud, STOPWORDS\n\n\n\ndef draw_word_cloud(df, location, title):\n    loc_df = df.loc[df.user_location==location]\n    loc_df.loc[:, 'text'] = loc_df['text'].apply(punctuation_stopwords_removal)\n    word_cloud = WordCloud(\n                    background_color='white',\n                    stopwords=set(STOPWORDS),\n                    max_words=50,\n                    max_font_size=40,\n                    scale=5,\n                    random_state=1).generate(str(loc_df['text']))\n    fig = plt.figure(1, figsize=(10,10))\n    plt.axis('off')\n    fig.suptitle(title, fontsize=20)\n    fig.subplots_adjust(top=2.3)\n    plt.imshow(word_cloud)\n    plt.show()\n    ","98e5b536":"draw_bar_graph_for_text_visualization(df, 'India')","44db402f":"draw_word_cloud(df, 'India', 'Word Cloud for top 50 prevelant words in India')","e6eb3655":"draw_bar_graph_for_text_visualization(df, 'United Kingdom')","2e0784a5":"draw_word_cloud(df, 'United Kingdom', 'Word Cloud for top 50 prevelant words in United Kingdom')","1a89798f":"draw_bar_graph_for_text_visualization(df, 'Canada')","10c4f631":"draw_word_cloud(df, 'Canada', 'Word Cloud for top 50 prevelant words in Canada')","2a79cb69":"draw_bar_graph_for_text_visualization(df, 'South Africa')","686a83db":"draw_word_cloud(df, 'South Africa', 'Word Cloud for top 50 prevelant words in South Africa')","33a452c5":"draw_bar_graph_for_text_visualization(df, 'Switzerland')","7593b515":"draw_word_cloud(df, 'Switzerland', 'Word Cloud for top 50 prevelant words in Switzerland')","2be42ec2":"draw_bar_graph_for_text_visualization(df, 'London')","997e8d7f":"draw_word_cloud(df, 'London', 'Word Cloud for top 50 prevelant words in London')","41dc9e9a":"sentiment_df = pd.read_csv('\/kaggle\/input\/twitterdata\/finalSentimentdata2.csv')","d4debe27":"sentiment_df.head()","c5dee078":"sentiment_df.columns","b5dabe64":"sentiment_df['sentiment'].nunique","1bfd31f3":"sentiment_df.loc[:, 'text'] = sentiment_df['text'].apply(punctuation_stopwords_removal)","206cc6fb":"reviews_split = []\nfor i, j in sentiment_df.iterrows():\n    reviews_split.append(j['text'])\n","56c3441a":"words = []\nfor review in reviews_split:\n    for word in review:\n        words.append(word)\n","3508c689":"print(words[:20])","41c3668b":"from collections import Counter\n\ncounts = Counter(words)\nvocab = sorted(counts, key=counts.get, reverse=True)\nvocab_to_int = {word:ii for ii, word in enumerate(vocab, 1)}","931a1aa3":"encoded_reviews = []\nfor review in reviews_split:\n    encoded_reviews.append([vocab_to_int[word] for word in review])\n","51f3022c":"print(len(vocab_to_int))\nprint(encoded_reviews[:10])","a4df510b":"labels_to_int = []\nfor i, j in sentiment_df.iterrows():\n    if j['sentiment']=='joy':\n        labels_to_int.append(1)\n    else:\n        labels_to_int.append(0)\n    ","cd2dde7d":"reviews_len = Counter([len(x) for x in encoded_reviews])\nprint(max(reviews_len))","cb6ea753":"print(len(encoded_reviews))","0f4d8930":"non_zero_idx = [ii for ii, review in enumerate(encoded_reviews) if len(encoded_reviews)!=0]\nencoded_reviews = [encoded_reviews[ii] for ii in non_zero_idx]\nencoded_labels = np.array([labels_to_int[ii] for ii in non_zero_idx])","9ee15acd":"print(len(encoded_reviews))\nprint(len(encoded_labels))","4764dc6d":"def pad_features(reviews_int, seq_length):\n    features = np.zeros((len(reviews_int), seq_length), dtype=int)\n    for i, row in enumerate(reviews_int):\n        if len(row)!=0:\n            features[i, -len(row):] = np.array(row)[:seq_length]\n    return features","bd16d52a":"seq_length = 50\npadded_features= pad_features(encoded_reviews, seq_length)\nprint(padded_features[:2])\n","aa0e3136":"split_frac = 0.8\nsplit_idx = int(len(padded_features)*split_frac)\n\ntraining_x, remaining_x = padded_features[:split_idx], padded_features[split_idx:]\ntraining_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n\ntest_idx = int(len(remaining_x)*0.5)\nval_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\nval_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n","0bb26226":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader","502ff8c2":"# torch.from_numpy creates a tensor data from n-d array\ntrain_data = TensorDataset(torch.from_numpy(training_x), torch.from_numpy(training_y))\ntest_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\nvalid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n\nbatch_size = 1\n\ntrain_loader = DataLoader(train_data, batch_size=batch_size)\ntest_loader = DataLoader(test_data, batch_size=batch_size)\nvalid_loader = DataLoader(valid_data, batch_size=batch_size)","7e89d71e":"gpu_available = torch.cuda.is_available\n\nif gpu_available:\n    print('Training on GPU')\nelse:\n    print('GPU not available')","34544fd0":"import torch.nn as nn\n\nclass CovidTweetSentimentAnalysis(nn.Module):\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.2):\n        super(CovidTweetSentimentAnalysis, self).__init__()\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        \n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sig = nn.Sigmoid()\n    \n    def forward(self, x, hidden):\n        # x : batch_size * seq_length * features\n        batch_size = x.size(0)\n        x = x.long()\n        embeds = self.embedding_layer(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        sig_out = self.sig(out)\n        \n        sig_out = sig_out.view(batch_size, -1)\n        sig_out = sig_out[:, -1]\n        \n        return sig_out, hidden\n    \n    def init_hidden(self, batch_size):\n        # initialize weights for lstm layer\n        weights = next(self.parameters()).data\n        \n        if gpu_available:\n            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero())\n        return hidden","409f7c6e":"vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\noutput_size = 1 # either happy or sad\nembedding_dim = 400\nhidden_dim = 256\nn_layers = 2","fcd0d9be":"net = CovidTweetSentimentAnalysis(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\nprint(net)","2503a46b":"lr = 0.001\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)","f28a4a14":"epochs = 4\ncount = 0\nprint_every = 100\nclip = 5 \nif gpu_available:\n    net.cuda()\n\nnet.train()\nfor e in range(epochs):\n    # initialize lstm's hidden layer \n    h = net.init_hidden(batch_size)\n    for inputs, labels in train_loader:\n        count += 1\n        if gpu_available:\n            inputs, labels = inputs.cuda(), labels.cuda()\n        h = tuple([each.data for each in h])\n        \n        # training process\n        net.zero_grad()\n        outputs, h = net(inputs, h)\n        loss = criterion(outputs.squeeze(), labels.float())\n        loss.backward()\n        nn.utils.clip_grad_norm(net.parameters(), clip)\n        optimizer.step()\n        \n        # print average training losses\n        if count % print_every == 0:\n            val_h = net.init_hidden(batch_size)\n            val_losses = []\n            net.eval()\n            for inputs, labels in valid_loader:\n                val_h = tuple([each.data for each in val_h])\n                if gpu_available:\n                    inputs, labels = inputs.cuda(), labels.cuda()\n            outputs, val_h = net(inputs, val_h)\n            val_loss = criterion(outputs.squeeze(), labels.float())\n            val_losses.append(val_loss.item())\n        \n            net.train()\n            print(\"Epoch: {}\/{}...\".format(e+1, epochs),\n                  \"Step: {}...\".format(count),\n                  \"Loss: {:.6f}...\".format(loss.item()),\n                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))","49093990":"test_losses = []\nnum_correct = 0\n\nh = net.init_hidden(batch_size)\nnet.eval()\n\nfor inputs, labels in test_loader:\n    h = tuple([each.data for each in h])\n    if gpu_available:\n        inputs, labels = inputs.cuda(), labels.cuda()\n    \n    outputs, h = net(inputs, h)\n    test_loss = criterion(outputs.squeeze(), labels.float())\n    test_losses.append(test_loss.item())\n    pred = torch.round(outputs.squeeze())\n    correct_tensor = pred.eq(labels.float().view_as(pred))\n    correct = np.squeeze(correct_tensor.numpy()) if not gpu_available else np.squeeze(correct_tensor.cpu().numpy())\n    num_correct += np.sum(correct)\n\n# printing average statistics\nprint(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n    \n# accuracy over all test data\ntest_acc = num_correct\/len(test_loader.dataset)\nprint(\"Test accuracy: {:.3f}\".format(test_acc))","19452eb5":"from string import punctuation\n\ndef tokenize_covid_tweet(tweet):\n    test_ints = []\n    test_ints.append([vocab_to_int[word] for word in tweet])\n    return test_ints","dad4575d":"def predict_covid_sentiment(net, test_tweet, seq_length=50):\n    print('Original Sentence :')\n    print(test_tweet)\n    \n    print('\\nAfter removing punctuations and stop-words :')\n    test_tweet = punctuation_stopwords_removal(test_tweet)\n    print(test_tweet)\n    \n    print('\\nAfter converting pre-processed tweet to tokens :')\n    tokenized_tweet = tokenize_covid_tweet(test_tweet)\n    print(tokenized_tweet)\n    \n    print('\\nAfter padding the tokens into fixed sequence lengths :')\n    padded_tweet = pad_features(tokenized_tweet, 50)\n    print(padded_tweet)\n    \n    feature_tensor = torch.from_numpy(padded_tweet)\n    batch_size = feature_tensor.size(0)\n    \n    if gpu_available:\n        feature_tensor = feature_tensor.cuda()\n    \n    h = net.init_hidden(batch_size)\n    output, h = net(feature_tensor, h)\n    \n    predicted_sentiment = torch.round(output.squeeze())\n    print('\\n==========Predicted Sentiment==========\\n')\n    if predicted_sentiment == 1:\n        print('Happy')\n    else:\n        print('Sad')\n    print('\\n==========Predicted Sentiment==========\\n')\n","85271b83":"test_sad_tweet = 'It is very sad to see the corona pandemic increasing at such an alarming rate'\npredict_covid_sentiment(net, test_sad_tweet)","15a6051a":"test_happy_tweet = 'It is amazing to see that New Zealand reaches 100 days without Covid transmission!'\npredict_covid_sentiment(net, test_happy_tweet)","ee59df73":"\n## Plot Missing Values","bbd919f0":"## Exploring Tweet Data\n\n* Sentiment Analysis on Covid19 Tweets\n    * Exploring tweet data\n    * Encoding tweets\n    * Encoding sentiments\n    * Detecting outlier reviews\n    * Training, testing and validating\n    * Dataloaders and batching\n    * Sentiment network with PyTorch\n    * Instantiate the netork\n    * Calculating model's accuracy\n    * Testing model on a random covid19 tweet.","5385c60d":"## Frequency of locations tweeting about Corona","5c02f089":"## Frequency of sources tweeting about Corona","001f1dd5":"## Testing model on random tweet\n\nSince for performing sentiment analysis on covid 19 tweets, I on-boarded a completely different dataset in this notebook. Now that the our model is trained,we can use this model to perform sentiment analysis on tweets related to covid19 on this notebook.","c669e591":"## Sentiment Network with PyTorch\nBelow are the various layers of our RNN that would perform sentiment analysis -<br>\n1. An *embedding layer* that converts our word tokens (integers) into embeddings of a specific size.\n2. A *LSTM layer* defined by a hidden_state size and number of layers\n3. A fully-connected output layer that maps the LSTM layer outputs to a desired output_size\n4. A sigmoid activation layer which turns all outputs into a value 0-1; return only the last sigmoid output as the output of this network.\"","6950eea4":"## Encoding Sentiments\n\nFor simplicity purposes, I am encoding positive sentiment such as joy as 1 and rest (anger, sad) as 0","962f97e8":"## Frequency of users tweeting about Corona","31f1853b":"## Detecting any outlier reviews\n\nThis step involves -<br>\n1. Getting rid of extremely long\/short reviews\n2. Padding\/truncating reaining data to maintain constant review length.","95aad7ab":"## Instantiate the network\nHere, I will define the model hyper-parameters -<br>\n\n1. `vocab_size` : Size of our vocabulary or the range of values for our input, word tokens.\n2. `output_size` : Size of our desired output; the number of class scores we want to output (pos\/neg).\n3. `embedding_dim` : Number of columns in the embedding lookup table; size of our embeddings.\n4. `hidden_dim` : Number of units in the hidden layers of our LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n5. `n_layers`: Number of LSTM layers in the network. Typically between 1-3","755424c3":"## Calculating model's accuracy\n\nThe `CovidTweetSentimentAnalysis` model achieved accuracy of 87.4 %","e54eb876":"### Acceptable color pallets in seaborn (i.e. we can experiment with `cmap` value below) : <br>\n\n```python\nAccent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2', 'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', 'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r', 'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix', 'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r', 'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', 'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean', 'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', 'rocket_r', 'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'twilight', 'twilight_r', 'twilight_shifted', 'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r'\n```\n","0ff6bb98":"## Visualizing top 30 words location wise","2f853bf8":"# EDA and Sentiment Analysis on COVID19 Tweets\n\nThis notebook is organized as follows:\n\n1. EDA on Covid19 tweets<br> \n    * Plot missing values.\n    * Plot unique values.\n    * Plot frequency of users tweeting about Corona\n    * Plot frequency of locations tweeting about Corona\n    * Plot frequency of sources tweeting about Corona\n    * Visualizing location-wise top 50 prevelant words\n2. Sentiment Analysis on Covid19 Tweets<br>\n    * Exploring tweet data\n    * Encoding tweets\n    * Encoding sentiments\n    * Detecting outlier reviews\n    * Training, testing and validating\n    * Dataloaders and batching\n    * Sentiment network with PyTorch\n    * Instantiate the netork\n    * Calculating model's accuracy\n    * Testing model on a random covid19 tweet.","7a2950f4":"## Training, Testing and Validating","8b49fdad":"# EDA on Covid19 Tweets","351dc9b3":"## Encoding Tweets\nCreate an array that contains integer encoded version of words in reviews. The word appearing the most should have least integer value. Example if the appeared the most in reviews, then assign 'the' : 1","c3a7cfa8":"## Plot Unique Values ","197316ff":"# Sentiment Analysis on Covid19 Tweets\n\nFor this part of the notebook I will be using [Covid 19 Indian Sentiments on covid19 and lockdown](https:\/\/www.kaggle.com\/surajkum1198\/twitterdata) dataset.","48a652a9":"## Dataloaders and Batching\n\nA neat way to create data-loaders and batch our training, validation and test Tensor datasets is as follows -<br>\n```python\ntrain_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\ntrain_loader = DataLoader(train_data, batch_size=batch_size)\n```\nThis is an alternative to creating a generator function for batching our data into full batches."}}