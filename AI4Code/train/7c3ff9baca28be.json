{"cell_type":{"9e18ad85":"code","c772f01d":"code","12471f41":"code","0c11b382":"code","c34484ee":"code","326959cf":"code","fe1f3bd4":"code","d27da067":"code","8639da19":"markdown","2940695f":"markdown","eb950310":"markdown","e1a64503":"markdown"},"source":{"9e18ad85":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\ner=1","c772f01d":"!pip install ..\/input\/sacremoses\/sacremoses-master\/ > \/dev\/null","12471f41":"import sys\nsys.path.insert(0, \"..\/input\/transformers\/transformers-master\/\")\nimport warnings\nwarnings.filterwarnings(action='once')\nimport transformers\nfrom collections import defaultdict\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport pickle\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline\nfrom functools import partial\nfrom sklearn.model_selection import KFold\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as D\nimport torch.nn.functional as F\nimport os\npd.set_option('max_columns', 1000)\nfrom tqdm import notebook\n","0c11b382":"def get_model_device(model):\n    if not torch.cuda.is_available():\n        return torch.device('cpu')\n    else:\n        device_num = next(model.parameters()).get_device()\n        if device_num<0:\n            return torch.device('cpu')\n        else:\n            return torch.device(\"cuda:{}\".format(device_num))\n\nclass FastTokenIter(D.Dataset):\n    def __init__(self, ds,max_len=512, batch_size=16,shuffle = False,return_order=False):\n        self.ds = ds\n        self.max_len=max_len\n        self.batch_size=batch_size\n        self.num_items = ds.__len__()\n        self.len=int(np.ceil(float(self.num_items)\/self.batch_size))\n        list_items=[ds.__getitem__(i) for i in notebook.tqdm(range(ds.__len__()) ,leave=False)]\n        self.items=[torch.cat([item[i][None] for item in list_items]) for i in range(len(list_items[0]))]\n        self.item_len=self.items[1].sum(1)\n        self.item_order = np.argsort(self.item_len.numpy())\n        self.reorder=np.argsort(self.item_order)\n        self.batch_order =np.arange(self.len)\n        self.len_tuple=len(self.items)\n        if shuffle:\n            np.rand.shuffle(self.batch_order)\n        self.return_order=return_order or shuffle\n        self.idx=0\n            \n    def __iter__(self):\n        self.idx = 0\n        return self\n    \n    def __next__(self):\n        if self.idx>=self.len:\n            raise StopIteration\n        sidx=self.batch_order[self.idx]\n        self.idx+=1\n        mlen=min(self.item_len[self.item_order[sidx*self.batch_size:(1+sidx)*self.batch_size]].max(),self.max_len)\n        ret =tuple([self.items[i][self.item_order[sidx*self.batch_size:(1+sidx)*self.batch_size]][:,:mlen] for i in range(self.len_tuple)])\n        return (self.item_order[sidx*self.batch_size:(1+sidx)*self.batch_size],)+ret if self.return_order else ret\n\ndef fetch_vectors_full(ds,model,batch_size=64,num_workers=8):\n    device = get_model_device(model)\n    fin_features=[]\n    dl = FastTokenIter(ds, batch_size=batch_size, shuffle=False)\n    _=model.eval()\n    with torch.no_grad():\n        for batch in notebook.tqdm(dl,total=dl.len,leave=False):\n            fin_features.append(model( input_ids=batch[0].to(device), attention_mask=batch[1].to(device))[0][:, 0, :].detach().cpu().numpy())\n    return np.vstack(fin_features)[dl.reorder]   \n\ndef fetch_vectors_full_slow(ds,model,batch_size=64,num_workers=8):\n    device = get_model_device(model)\n    fin_features=[]\n    dl = D.DataLoader(ds, batch_size=batch_size, shuffle=False)\n    _=model.eval()\n    with torch.no_grad():\n        for batch in notebook.tqdm(dl,leave=False):\n            fin_features.append(model( input_ids=batch[0].to(device), attention_mask=batch[1].to(device))[0][:, 0, :].detach().cpu().numpy())\n    return np.vstack(fin_features)  \n\nclass TextDataset(D.Dataset):\n    def __init__(self,text_list,tokenizer,max_len=512):\n        self.text_list=text_list\n        self.tokenizer = tokenizer\n        self.max_len=max_len\n    def __len__(self):\n        return len(self.text_list)\n    def __getitem__(self,idx):\n        token_ids=self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(self.text_list[idx]))[:self.max_len-2]\n        token_ids = [self.tokenizer.cls_token_id]+token_ids+[self.tokenizer.sep_token_id]\n        token_ids_tensor=torch.zeros(self.max_len,dtype=torch.long)\n        mask_tensor=torch.zeros(self.max_len,dtype=torch.long)\n        token_type_tensor=torch.zeros(self.max_len,dtype=torch.long)\n        token_ids[:len(token_ids)]=token_ids\n        mask_tensor[:len(token_ids)]=1\n        return tuple((token_type_tensor,mask_tensor,token_type_tensor))\n","c34484ee":"device='cuda'\n","326959cf":"test = pd.read_csv('..\/input\/google-quest-challenge\/test.csv').fillna(' ')\nsample_submission = pd.read_csv('..\/input\/google-quest-challenge\/sample_submission.csv')\ner=2*er+1","fe1f3bd4":"tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"..\/input\/distilbertbaseuncased\/\")\nmodel = transformers.DistilBertModel.from_pretrained(\"..\/input\/distilbertbaseuncased\/\")\nmodel.to(device)\n","d27da067":"question_ds= TextDataset(test.question_body.to_list(),tokenizer,512)\n%time question_features=fetch_vectors_full(question_ds,model,batch_size=16)\n%time question_features=fetch_vectors_full_slow(question_ds,model,batch_size=16)","8639da19":"This notebook shows how your inference can be faster.\n\nThe idea is to use the shortest max_len you can (instead of the standart 512)\n\nTo do it we re-order all sentences by their token length, each batch get the minimal max_len it can, and then sent to the model.\n\nLater we reorder the outputs to the original order.\n\nThis kernel is for demonstration only. It is not a full competition solution","2940695f":"## Banchmark ","eb950310":"The magic is in FastTokenIter it is an itaratore that is somewhat like DataLoader.\n\nJust remember to reorder everything at the end using .reorder attribute\n\n\"fetch_vectors_full\" is an example how to use it","e1a64503":"## How to make your inference faster"}}