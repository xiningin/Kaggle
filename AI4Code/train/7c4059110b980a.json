{"cell_type":{"6537f744":"code","96f4c650":"code","4818404d":"code","d2bcc20e":"code","1e3a386d":"code","ae83565f":"code","b90e2c8e":"code","ae99716a":"code","fda1ee82":"code","177ee736":"code","8380fe2d":"code","cae1727a":"code","859add2f":"code","5ce13ceb":"code","182c86a6":"code","3a800b5b":"code","09f5412f":"code","ed74605e":"code","e9593ae8":"code","62d57ee0":"code","32029468":"code","08f19a7f":"code","7bea32d1":"code","ae7c2898":"code","15e541a7":"markdown","e326d826":"markdown","c2a75c57":"markdown","1b227d6a":"markdown","5e263454":"markdown","cd6a9008":"markdown","9ed7d509":"markdown","de82466e":"markdown","335b9c8f":"markdown","8c222d55":"markdown","b36efa8f":"markdown","7ec1fcab":"markdown","96ab2f30":"markdown","25ec85e5":"markdown","495ead3f":"markdown","60b01e46":"markdown","f887a21b":"markdown"},"source":{"6537f744":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nfrom glob import glob\nimport os\nfrom PIL import Image","96f4c650":"train_metadata = pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\ntrain_metadata.head()","4818404d":"test_metadata = pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')\ntest_metadata.head()","d2bcc20e":"def show_images(folder_name, num_images = 9, shape = (3, 3)):\n  row = shape[0]\n  col = shape[1]\n  assert num_images == row*col,\"Total image number is not matching with the size...\"\n  fig, ax = plt.subplots(row, col, figsize = (20, 6))\n  plt.suptitle(f\"Images : {folder_name.split('\/')[-2]}\")\n  for index in range(num_images):\n    plt.subplot(row, col, index + 1)\n    img = load_image(glob(f\"{folder_name}\/*jpg\")[index])\n    plt.imshow(img)\n  plt.show()\ndef load_image(source):\n  img = cv2.imread(source)\n  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n  return img\n\nshow_images('..\/input\/petfinder-pawpularity-score\/train\/')\nshow_images('..\/input\/petfinder-pawpularity-score\/test\/', 4, (2, 2) )","1e3a386d":"train_features = train_metadata.iloc[:,1:-1]\ntrain_features.head()","ae83565f":"Y = train_metadata['Pawpularity']","b90e2c8e":"sns.boxplot(Y)","ae99716a":"train_features.describe()","fda1ee82":"train_features.shape","177ee736":"for column in train_features.columns:\n  print(f\"{column} : \\n{train_features[column].value_counts()}\")","8380fe2d":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom torchvision.transforms import transforms\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\nimport math\nimport time\nfrom sklearn.metrics import r2_score\nfrom termcolor import cprint\nimport warnings\nwarnings.filterwarnings('ignore')","cae1727a":"class PawDataset(Dataset):\n  def __init__(self, data_source, metadata, H = 128, W = 128, test_data = False):\n    super(PawDataset, self).__init__()\n    self.data_source = data_source\n    self.metadata = metadata\n    self.H = H\n    self.W = W\n    self.test_data = test_data\n    self.augment = self.transform()\n\n  def transform(self):\n    augmentation = transforms.Compose(\n        [\n            transforms.RandomHorizontalFlip(),\n        ]\n    )\n    return augmentation\n\n  def __len__(self):\n    return len(self.metadata)\n\n  def __getitem__(self, index):\n    # image_link\n    source = self.metadata['Id'][index]\n    source = os.path.join(f\"{self.data_source}{source}.jpg\")\n    # loading the image and tranforming it into a torh tensor\n    image = self.load_image(source)\n    # loading metadata\n    metadata = self.metadata.iloc[index, 1:13].astype('float32').to_numpy().reshape(1,-1)\n    if self.test_data == False:\n        # target output\n        image = self.augment(image)\n        image = transforms.ToTensor()(image)\n        target = self.metadata['Pawpularity'][index] \/ 100.0\n        return (image, metadata, target)\n    else:\n        image = transforms.ToTensor()(image)\n        return (image, metadata)\n  def load_image(self, source):\n    img = cv2.imread(source)\n    img = cv2.resize(img, (self.H, self.W))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = Image.fromarray(img)\n    return img","859add2f":"BATCH_SIZE = 64","5ce13ceb":"Train_ds = PawDataset('..\/input\/petfinder-pawpularity-score\/train\/', train_metadata)\nTrain_dl = DataLoader(Train_ds, batch_size = BATCH_SIZE, shuffle = True)\ntrain_size = int(0.8 * Train_ds.__len__())\nval_size = Train_ds.__len__() - train_size\ntrain_ds , val_ds = torch.utils.data.random_split(Train_ds, [train_size, val_size ])\ntrain_dl = DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle = True)\nval_dl = DataLoader(val_ds, batch_size = BATCH_SIZE, shuffle = True)\ntest_ds = PawDataset('..\/input\/petfinder-pawpularity-score\/test\/', test_metadata, test_data = True)\ntest_dl = DataLoader(test_ds, batch_size = BATCH_SIZE, shuffle = False)\nfor patch, metadata, target in train_dl:\n  print(patch.shape, metadata.shape, target.shape)\n  break\nfor patch, metadata, target in val_dl:\n  print(patch.shape, metadata.shape, target.shape)\n  break\nfor patch, target in test_dl:\n  print(patch.shape, target.shape)\n  break","182c86a6":"class cnn(nn.Module):\n  def __init__(self, in_channels, out_channels, kernel_size = 3, stride = 1, padding = 0):\n    super(cnn, self).__init__()\n    self.cnn = nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n        nn.ReLU(),\n        nn.BatchNorm2d(out_channels, momentum = 0.95)\n    )\n  def forward(self, x):\n    return self.cnn(x)\n\nclass Network(nn.Module):\n  def __init__(self):\n    super(Network, self).__init__()\n    self.fet_ext = nn.Sequential(\n        cnn(3,4),\n        cnn(4, 64),   # 128 -> 126\n        nn.MaxPool2d(2), # 126 -> 63\n        cnn(64, 32),   # 63 -> 61\n        nn.MaxPool2d(2),  # 61 -> 30\n        cnn(32, 16),  # 30 -> 28\n        nn.MaxPool2d(2),\n        nn.Dropout(0.15),\n        nn.Flatten(), # 14*14*16\n    )\n    self.fc1 = nn.Sequential(\n        nn.Linear(12, 64),\n    )\n    self.fc2 = nn.Sequential(\n        nn.Linear(14*14*16, 1024),\n        nn.Linear(1024, 64),\n    )\n    self.fc3 = nn.Sequential(\n        nn.Linear(128, 1),\n    )\n\n  def forward(self, im_patch, mt_patch):\n      mt_patch = mt_patch.squeeze(dim = 2)\n      mt_patch = mt_patch.squeeze(dim = 1)\n      cnn_op = self.fet_ext(im_patch)\n      fc1_op = self.fc1(mt_patch)\n      fc2_op = self.fc2(cnn_op)\n      linear_op = torch.cat([fc1_op, fc2_op], axis = 1)\n      output = self.fc3(linear_op)\n      return output","3a800b5b":"model = Network()\nmodel = model.cuda()\nmodel","09f5412f":"for name, param in model.named_parameters():\n  print(f\"{name} | {param.shape} | {param.dtype}\")","ed74605e":"EPOCHS = 10\ncriterion = nn.MSELoss()\noptm = optim.Adam(model.parameters(), lr = 1e-4)","e9593ae8":"train_step_loss, val_step_loss = [], []\ntrain_loss, val_loss = [], []\nval_best_loss = np.inf\nfor epoch in range(EPOCHS):\n  start_time = time.time()\n  print(f\"Epoch {epoch + 1} : \")\n  epoch_loss = 0.0\n  model.train()\n  for index, (patch, metadata, target) in enumerate(train_dl):\n    optm.zero_grad()\n    patch = patch.float().cuda()\n    metadata  = metadata.float().cuda()\n    target = target.float().cuda()\n    op = model(patch, metadata)\n    loss = criterion(op, target)\n    epoch_loss += loss.item() * patch.shape[0]\n    train_step_loss.append(loss.item())\n    if index % 10 == 9:\n      print(f\"step {index + 1} Loss: {'%.4f'%(loss.item())}\")\n    loss.backward()\n    optm.step()\n  epoch_loss \/= train_size\n  print(f\"training data --> loss : {'%.4f'%(epoch_loss)}\")\n  train_loss.append(epoch_loss)\n  model.eval()\n  val_ep_loss = 0.0\n  with torch.no_grad():\n      for index, (patch, metadata, target) in enumerate(val_dl):\n          patch = patch.float().cuda()\n          metadata  = metadata.float().cuda()\n          target = target.float().cuda()\n          op = model(patch, metadata)\n          loss = criterion(op, target)\n          val_step_loss.append(loss.item())\n          val_ep_loss += loss.item() * patch.shape[0]\n  val_ep_loss \/= val_size\n  print(f\"validation data --> loss : {'%.4f'%(val_ep_loss)}\")\n  val_loss.append(val_ep_loss)\n  if val_ep_loss < val_best_loss :\n    val_best_loss = val_ep_loss\n    cprint(\"Success...Model Updated...\", 'green')\n    torch.save(model, 'best_model.pth')\n  else:\n    cprint(\"Failed... Model haven't uploaded...\", 'red')\n  elapsed_time = time.time() - start_time\n  print(f\"Elapsed time : {'%.2f'%(elapsed_time)} seconds...\\n\")\ncprint(\"Training completed...\", 'blue')","62d57ee0":"best_model = torch.load('.\/best_model.pth')","32029468":"best_model = best_model.cuda()","08f19a7f":"with torch.no_grad():\n    for index, (patch, metadata) in enumerate(test_dl):\n        patch = patch.float().cuda()\n        metadata  = metadata.float().cuda()\n        op = best_model(patch, metadata)","7bea32d1":"sub_df = pd.DataFrame({'Id': test_metadata.Id, 'Pawpularity': op.squeeze(dim = 1).cpu().detach().numpy()})\nsub_df","ae7c2898":"sub_df.to_csv('submission.csv', index = False)","15e541a7":"Now, the model is testing on the output data and one can see the data is getting out is in form of tensors, so we have to process that to fit inside the dataframe.","e326d826":"Batch size is taken 64 , so that it can be more generalized as well as more specific towards the training data.","c2a75c57":"![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*jcZLpgh3gppeFFgcpFSP0w.jpeg)\n\n# End to End Pytorch DNN Walkthrough :\n\nThis notebook holds end to end Pytorch DNN walkthrough covering all the procedures which are needed to do for a competition.","1b227d6a":"You can find the model graph [here](https:\/\/github.com\/sagnik1511\/Deep-Learning-Competitions\/blob\/main\/Kaggle\/Petfinder%20Pawpularity\/assets\/petfinder_model_graph.png).","5e263454":"Taking basic hyerparameters (This has been taken after long nmber of experiments).","cd6a9008":"Now, we have to load the metadata and see whether that can be used for attributes or not.","9ed7d509":"# Importing supporting libraries : \n\nAt first we need to import basic libraries that'll help us to find the visualize the data.","de82466e":"Now, it's the most important process, we have to train the model, so that it can be well fitted as well as more generalized but overfitted.","335b9c8f":"Now, it's time to generate the Model class. This is the general purpose DNN generation process, and followed by every single researcher.","8c222d55":"As the training process can be slow using cpu , we are going to process the training inside the **GPU** itself.","b36efa8f":"After performing basic EDA on the data now it is time to create the Pytorch dataset that'll generate the dataloader to make batches of data while training.","7ec1fcab":"Now checking through the **Pawpularity score**  if the data holds any outliers or not.","96ab2f30":"# Content : \n\n## 1. Primary Visualization.\n## 2. Understanding the solution.\n## 3. Creating Dataset.\n## 4. Creating Deep Neural Net.\n## 5. Model Training.\n## 6. Saving best Model.\n## 7. Testing on testing data.\n## 8. Creating submission.","25ec85e5":"Now , we can see that the dataloaders are created and it is actually creating a batch of data.","495ead3f":"As in some cases the images are horizontally different we are using the **horizontal flip augmentation** so that the data can be more general.","60b01e46":"As the last saved model can have too much varinace with the validation data, we are going to use the best saved model so far.","f887a21b":"Now, the final dataframe has been prepared, and ready to be submitted."}}