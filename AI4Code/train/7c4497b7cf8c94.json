{"cell_type":{"cb3d77d4":"code","2c8605a4":"code","1bae2f95":"code","e8c808df":"code","3680c47c":"code","879eb264":"code","c307e4f4":"code","7bd0a027":"code","4440c8f9":"code","aa958cf0":"code","09119c42":"code","a46a4166":"code","afa326d9":"code","e2b2fd2d":"code","714eab22":"code","8d83b220":"code","2ec4d201":"code","10e2e80a":"code","86e70242":"code","a50ef14e":"code","9dbb0c3e":"code","b33d29d0":"code","1fdff3b0":"code","b46718ad":"code","2d8f62f9":"code","f321c796":"code","22c90fe3":"code","d0c6bfe8":"code","817728a5":"code","d4708386":"code","5e047692":"code","5dd58a9f":"code","57fe5e14":"code","62e96678":"code","eeb81350":"code","29e4a14c":"code","4b4cf506":"code","aa8713d5":"code","364ae059":"code","3dccba47":"code","9125cdef":"code","976fef06":"code","6f858bd9":"code","09b5ad7f":"code","b1405834":"code","c7fa6729":"code","97584f98":"code","66580316":"code","be64a116":"code","f07ea7d7":"code","19f465e0":"code","2bf31ef5":"code","0c8ce86e":"code","e9e7cf96":"code","1aaea261":"code","9424f654":"code","c3234057":"code","86d4e87a":"code","e97d04cb":"code","ea66b0ae":"code","955cd566":"code","ec22751f":"code","63ecb51c":"code","5defe5b2":"code","1f9fe840":"markdown","27ffb38d":"markdown","609cec4f":"markdown","7d8a8c3f":"markdown","6d2f476b":"markdown","7c0ca12e":"markdown","faf3b4a8":"markdown","7ab2ffab":"markdown","75f3491c":"markdown","a7a25443":"markdown","e9dc51ea":"markdown","c8050ea1":"markdown","05bf42d9":"markdown","42b14c12":"markdown","34fdb76f":"markdown","5939bf6f":"markdown","8295c058":"markdown","b3d6e6d8":"markdown","fc7445c9":"markdown","5dc91545":"markdown","a7adce3f":"markdown","07525339":"markdown","25083888":"markdown","3c36b256":"markdown","788ed89e":"markdown","70002f0e":"markdown","9e320e9c":"markdown","ade7c3b2":"markdown","4694e6fb":"markdown","ef2bd3c5":"markdown","c5b680ad":"markdown","63526921":"markdown","eafd4e1c":"markdown","7abec6aa":"markdown","fa886a2a":"markdown","10b69086":"markdown","14981f35":"markdown","c2b1735b":"markdown","08fd0207":"markdown","c96e7b83":"markdown"},"source":{"cb3d77d4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2c8605a4":"from sklearn.linear_model import LogisticRegression # LogisticRegression\nfrom sklearn.model_selection import train_test_split # to split\nfrom sklearn.model_selection import TimeSeriesSplit # to split\nfrom sklearn.metrics import roc_curve # ROC curve\nfrom sklearn.metrics import roc_auc_score # ROC score\nfrom sklearn.model_selection import cross_val_score # cross validation score\nfrom sklearn.preprocessing import StandardScaler # Standartization\nfrom sklearn.preprocessing import MinMaxScaler # Standartization\nfrom sklearn.model_selection import GridSearchCV # to find best parameters\nfrom sklearn.ensemble import RandomForestClassifier # Random Forest\nfrom sklearn.ensemble import GradientBoostingClassifier # Gradient Boosting\nfrom sklearn.ensemble import BaggingClassifier # Bagging\nfrom sklearn.ensemble import VotingClassifier # Ensemble models (voting and mean)\nfrom sklearn.metrics import confusion_matrix # Confusion Matrix\nfrom sklearn.svm import SVC # SVM\nfrom sklearn.svm import LinearSVC # SVM\nfrom sklearn.pipeline import make_pipeline # Pipepline\nfrom catboost import CatBoostClassifier, Pool # Yandex Gradient Boosting\nimport xgboost as xgb # Gradient Boosting\nfrom mlxtend.classifier import StackingClassifier # Stacking\nfrom mlxtend.classifier import StackingCVClassifier # Stacking\nimport lightgbm as lgb # Microsoft Gradient Boosting\nimport matplotlib.pyplot as plt # to plot\nplt.style.use('ggplot')\nimport seaborn as sns # to plot\nimport numpy as np # to count\nimport pandas as pd # DataFrames\nfrom scipy import stats # Statistics\nimport pickle # to read pickle files\nfrom IPython.display import display # Display tables\nimport time # time checking\nimport re # regular expressions\nimport eli5 # to check features importance\nfrom collections import Counter # to count\npd.set_option('display.max_columns', None) # to display max columns in DataFrames\npd.options.mode.chained_assignment = None  # don't lool at mistakes","1bae2f95":"def load_train_or_test():\n    i = input('train or test ')\n    if i == 'test' or i =='train':\n        name = f\"{i}_sessions.csv\"\n        df = pd.read_csv(f\"..\/input\/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2\/\" + name)\n        return df\n    else:\n        pass","e8c808df":"def norm(data):\n    return (data)\/(max(data)-min(data)) # to norm","3680c47c":"#df_train = load_train_or_test()\ndf_train = pd.read_csv(\"..\/input\/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2\/train_sessions.csv\")","879eb264":"df_train.head()","c307e4f4":"df_train.info()","7bd0a027":"def time_operations(df):\n    if 'target' in df.columns:\n        df['number_of_sites'] = ((df.notna().sum(axis=1)) \/ 2) - 1\n    else:\n        df['number_of_sites'] = ((df.notna().sum(axis=1)) \/ 2) - 0.5\n    for col in [f\"time{i}\" for i in range(1,11)]:\n        df[col] = pd.to_datetime(df[col])\n    df['session_strt'] = df[[f\"time{i}\" for i in range(1,11)]].min(axis=1)\n    df['session_end'] = df[[f\"time{i}\" for i in range(1,11)]].max(axis=1)\n    df['session_len'] = (df['session_end'] - df['session_strt']).dt.seconds\n    df['weekday'] = df['session_strt'].dt.dayofweek\n    df['weekday'] = df['weekday'].astype('category')\n    df['start_hour'] = df['session_strt'].dt.hour\n    df['start_hour'] = df['start_hour'].astype('category')\n    df['end_hour'] = df['session_end'].dt.hour\n    df['end_hour'] = df['end_hour'].astype('category')\n    df['day'] = df['session_strt'].dt.day\n    df['day'] = df['day'].astype('category')\n    df['minute'] = df['session_strt'].dt.minute\n    df['month'] = df['session_strt'].dt.month\n    df = df.sort_values('session_strt', ignore_index=True)\n    df = df.reset_index()\n    df = df.rename(columns={'index':'time_index'})\n    df = df.sort_values('session_id', ignore_index=True)\n    return df","4440c8f9":"df_train = time_operations(df_train)","aa958cf0":"df_train.head()","09119c42":"df_train.info()","a46a4166":"ax, fig = plt.subplots(figsize=(10,6))\nplt.hist(df_train.weekday, bins=7,rwidth=0.9,color='peru')\nplt.xticks(ticks=[i - 0.5 for i in np.linspace(1,6,7)], labels=['Mon','Tue','Wed','Thu','Fri','Sat','Sun'])\nplt.title('Main weekday schedule')\nplt.show()","afa326d9":"fig, ax = plt.subplots(1, 2, figsize=(10,6))\nax[0].hist(df_train[df_train.target==1].weekday, bins=7, color='teal', label='Alice', rwidth=0.9)\nax[1].hist(df_train[df_train.target==0].weekday, bins=7, color='maroon', label='Users', rwidth=0.9)\nax[0].set_xticks(ticks=[i - 0.5 for i in np.linspace(1,6,7)])\nax[1].set_xticks(ticks=[i - 0.5 for i in np.linspace(1,6,7)])\nax[0].set_xticklabels(labels=['Mon','Tue','Wed','Thu','Fri','Sat','Sun'])\nax[1].set_xticklabels(labels=['Mon','Tue','Wed','Thu','Fri','Sat','Sun'])\nax[0].title.set_text('Alice weekday schedule')\nax[1].title.set_text('Users weekday schedule')\nplt.tight_layout()\nplt.show()","e2b2fd2d":"ax, fig = plt.subplots(figsize=(10,6))\nplt.hist(df_train.start_hour, bins=17, rwidth=0.9,color='peru')\nplt.xticks(ticks=[i + 0.5 for i in np.linspace(7,22,17)],\n           labels=['7','8' ,'9','10','11','12','13','14','15','16','17',\n                   '18','19','20','21','22','23'])\nplt.title('Main day (per hour) schedule')\nplt.show()","714eab22":"fig, ax = plt.subplots(1, 2, figsize=(10,6))\nax[0].hist(df_train[df_train.target==1].start_hour, bins=17, color='teal', label='Alice', rwidth=0.9)\nax[1].hist(df_train[df_train.target==0].start_hour, bins=17, color='maroon', label='Users', rwidth=0.9)\nax[0].set_xticks(ticks=[i + 0.5 for i in np.linspace(7,22,17)])\nax[1].set_xticks(ticks=[i + 0.5 for i in np.linspace(7,22,17)])\nax[0].set_xticklabels(labels=['7','8' ,'9','10','11','12','13','14','15','16','17',\n                   '18','19','20','21','22','23'])\nax[1].set_xticklabels(labels=['7','8' ,'9','10','11','12','13','14','15','16','17',\n                   '18','19','20','21','22','23'])\nax[0].title.set_text('Alice day (per hour) schedule')\nax[1].title.set_text('Users day (per hour) schedule')\nplt.tight_layout()\nplt.show()","8d83b220":"pd.DataFrame(df_train[df_train.target==1].groupby('start_hour')['target'].mean())","2ec4d201":"pd.DataFrame(df_train.groupby('start_hour')['target'].mean())","10e2e80a":"ax, fig = plt.subplots(figsize=(10,6))\nplt.hist(np.log(df_train.session_len + 0.01),rwidth=0.9,color='peru', bins=30)\nplt.title('Main length of session (log)')\nplt.show()","86e70242":"fig, ax = plt.subplots(1, 2, figsize=(10,6))\nax[0].hist(np.log(df_train[df_train.target==1].session_len + 1), bins=30, color='teal', label='Alice', rwidth=0.9)\nax[1].hist(np.log(df_train[df_train.target==0].session_len + 1), bins=30, color='maroon', label='Users', rwidth=0.9)\nax[0].title.set_text('Alice length of session (log)')\nax[1].title.set_text('Users length of session (log)')\nplt.tight_layout()\nplt.show()","a50ef14e":"ax, fig = plt.subplots(figsize=(10,6))\nplt.hist(df_train.session_strt, bins=100, color='peru')\nplt.xticks(rotation=30)\nplt.title('Dates of session (train part)')\nplt.tight_layout()\nplt.show()","9dbb0c3e":"#df_test = load_train_or_test()\ndf_test = pd.read_csv(\"..\/input\/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2\/test_sessions.csv\")\ndf_test = time_operations(df_test)","b33d29d0":"ax, fig = plt.subplots(figsize=(10,6))\nplt.hist(df_test.session_strt, bins=100, color='rosybrown')\nplt.xticks(rotation=30)\nplt.title('Dates of session (test part)')\nplt.tight_layout()\nplt.show()","1fdff3b0":"df_train[df_train.target==1].session_strt.dt.month.unique()","b46718ad":"fig, ax = plt.subplots(1, 2, figsize=(10,6))\nax[0].hist(df_train[df_train.target==1].session_strt, bins=100, color='teal', label='Alice')\nax[1].hist(df_train[df_train.target==0].session_strt, bins=100, color='maroon', label='Users')\nax[0].title.set_text('Alice dates of session (train part)')\nax[1].title.set_text('Users dates of session (train part)')\nplt.setp( ax[0].xaxis.get_majorticklabels(), rotation=70 )\nplt.setp( ax[1].xaxis.get_majorticklabels(), rotation=70 )\nplt.tight_layout()\nplt.show()","2d8f62f9":"fig, ax = plt.subplots(1, 2, figsize=(10,6))\nax[0].hist(df_train[df_train.target==1].month, bins=12, color='teal', label='Alice', rwidth=0.9)\nax[1].hist(df_train[df_train.target==0].month, bins=12, color='maroon', label='Users', rwidth=0.9)\nax[0].title.set_text('Alice month schedule')\nax[1].title.set_text('Users month schedule')\nplt.tight_layout()\nplt.show()","f321c796":"ax, fig = plt.subplots(figsize=(10,6))\nplt.hist(df_train.number_of_sites,rwidth=0.9,bins=10,color='peru')\nplt.title('Main number of sites during 1 session')\nplt.show()","22c90fe3":"df_train.groupby('target')['number_of_sites'].mean()","d0c6bfe8":"ax, fig = plt.subplots(figsize=(10,6))\nplt.hist(df_train.day,rwidth=0.9,bins=31,color='peru')\nplt.title('Main day of month schedule (train part)')\nplt.show()","817728a5":"ax, fig = plt.subplots(figsize=(10,6))\nplt.hist(df_test.day,rwidth=0.9,bins=31,color='rosybrown')\nplt.title('Main day of month schedule (test part)')\nplt.show()","d4708386":"fig, ax = plt.subplots(1, 2, figsize=(10,6))\nax[0].hist(df_train[df_train.target==1].day, bins=31, color='teal', label='Alice', rwidth=0.9)\nax[1].hist(df_train[df_train.target==0].day, bins=31, color='maroon', label='Users', rwidth=0.9)\nax[0].title.set_text('Alice day of month schedule (train part)')\nax[1].title.set_text('Users day of month schedule (train part)')\nplt.tight_layout()\nplt.show()","5e047692":"ax, fig = plt.subplots(figsize=(10,6))\nplt.hist(df_train.minute,rwidth=0.9,bins=60,color='peru')\nplt.title('Main minute schedule')\nplt.show()","5dd58a9f":"fig, ax = plt.subplots(1, 2, figsize=(10,6))\nax[0].hist(df_train[df_train.target==1].minute, bins=60, color='teal', label='Alice', rwidth=0.9)\nax[1].hist(df_train[df_train.target==0].minute, bins=60, color='maroon', label='Users', rwidth=0.9)\nax[0].title.set_text('Alice minute schedule')\nax[1].title.set_text('Users minute schedule')\nplt.tight_layout()\nplt.show()","57fe5e14":"def time_treshold_save_zero(df, date='2014-03-01'):\n    df['day'] = df['session_strt'].dt.dayofyear\n    train_df = df[df.session_strt < date]\n    test_df = df[df.session_strt >= date]\n    return train_df, test_df","62e96678":"for j in range(1,11):\n  if j == 1:\n    df = df_train[['session_id','site{i}'.format(i=j),'target']]\n    df.columns = ['session_id','site', 'target']\n  if j<10:\n    df_2 = df_train[['session_id','site{i}'.format(i=j+1), 'target']]\n    df_2.columns = ['session_id','site', 'target']\n    df = pd.concat([df, df_2], ignore_index=True)\nsites = df.groupby('site')['target'].mean()","eeb81350":"pd.DataFrame(sites).sort_values('target')","29e4a14c":"len(sites[sites == 0])\/len(sites)","4b4cf506":"ax, fig = plt.subplots(figsize=(10,6))\nplt.hist(sites[sites > 0], bins=20,rwidth=0.9,color='peru')\nplt.title(\"Frequency of Alice's visits\" )\nplt.xlabel(\"Fraction of Alice's visits on the website\")\nplt.ylabel(\"Number of visits\")\nplt.show()","aa8713d5":"def alice_sites(train_df):\n    for j in range(1,11):\n      if j == 1:\n        df = train_df[['session_id','site{i}'.format(i=j),'target']]\n        df.columns = ['session_id','site', 'target']\n      if j<10:\n        df_2 = train_df[['session_id','site{i}'.format(i=j+1), 'target']]\n        df_2.columns = ['session_id','site', 'target']\n        df = pd.concat([df, df_2], ignore_index=True)\n    sites = df.groupby('site')['target'].mean()\n    return sites","364ae059":"def add_alice_sites(df, sites, alice_size = 0.06):\n    mask = sites[sites > alice_size].index\n    df['alice_site'] = 0\n    for col in [f\"site{i}\" for i in range(1,11)]:\n        alice_mask = df[col].isin(mask)\n        df.alice_site[alice_mask] = 1\n    return df","3dccba47":"def add_more_alice_sites(df, sites, a_s_2 = 0.3, a_s_3 = 0.01, a_s_4 = 0.5):\n    count = 1\n    for j in [a_s_2, a_s_3, a_s_4]:\n        count += 1\n        mask = sites[sites > j].index\n        df[f'alice_site_{count}'] = 0\n        for col in [f\"site{i}\" for i in range(1,11)]:\n            alice_mask = df[col].isin(mask)\n            df.loc[alice_mask, f'alice_site_{count}'] = 1\n    return df","9125cdef":"train_df, test_df = time_treshold_save_zero(df_train)","976fef06":"sites = alice_sites(train_df)","6f858bd9":"train_df = add_alice_sites(train_df, sites, alice_size = 0.06)","09b5ad7f":"train_df = add_more_alice_sites(train_df, sites)","b1405834":"train_df.head()","c7fa6729":"work_df = train_df.groupby('day')['alice_site'].sum()\nwork_df","97584f98":"good_days = work_df[(work_df > 70) == True].index\ngood_days","66580316":"bad_days = work_df[(work_df <= 70) == True].index\nbad_days","be64a116":"df_train[df_train.target==1].start_hour.value_counts()","f07ea7d7":"df_train[(df_train.target==1) & (df_train.start_hour==11)]","19f465e0":"df_train[(df_train.target==1) & (df_train.start_hour==14)]","2bf31ef5":"df_train[(df_train.target==1) & (df_train.start_hour==9)].sort_values('minute')","0c8ce86e":"df_train[(df_train.target==1) & (df_train.start_hour==15)].sort_values('minute')","e9e7cf96":"df_train[(df_train.target==1) & (df_train.start_hour==18)].sort_values('minute')","1aaea261":"def remove_days(df, border=70):\n    \n    df = df.set_index('session_id')\n    \n    work_df = df.groupby('day')['alice_site'].sum()\n    good_days = work_df[(work_df > border) == True].index\n    bad_days = work_df[(work_df <= border) == True].index\n    good_session_id = df[df.day.isin(good_days)].index\n    bad_session_id = df[df.day.isin(bad_days)].index\n    \n    bad_hours_array = np.array([7,8,10,11,14,19,20,21,22,23])\n    good_hours_array = np.array([12,13,16,17])\n    good_hour_session_id = df[df.start_hour.isin(good_hours_array)].index\n    bad_hour_session_id = df[df.start_hour.isin(bad_hours_array)].index\n    \n    rare_hours_array = np.array([9,18,15])\n    \n    nine_oclock_bad_minutes = np.arange(30,60)\n    fiften_oclock_bad_minutes = np.arange(20,50)\n    six_oclock_bad_minutes = np.arange(25,60)\n    \n    nine_oclock_good_minutes = np.arange(0,30)\n    fifteen_oclock_good_minutes = np.append(np.arange(0,20), np.arange(50,60))\n    six_oclock_good_minutes = np.arange(0,25)\n    \n    bad_nine_oclock_session_id = df[(df.start_hour == 9) & (df.minute.isin(nine_oclock_bad_minutes))].index\n    bad_fifteen_oclock_session_id = df[(df.start_hour == 15) & (df.minute.isin(fiften_oclock_bad_minutes))].index\n    bad_six_oclock_session_id = df[(df.start_hour == 18) & (df.minute.isin(six_oclock_bad_minutes))].index\n    \n    good_nine_oclock_session_id = df[(df.start_hour == 9) & (df.day.isin(good_days)) & (df.minute.isin(nine_oclock_good_minutes))].index\n    good_fifteen_oclock_session_id = df[(df.start_hour == 15) & (df.day.isin(good_days)) & (df.minute.isin(fifteen_oclock_good_minutes))].index\n    good_six_oclock_session_id = df[(df.start_hour == 18) & (df.day.isin(good_days)) & (df.minute.isin(six_oclock_good_minutes))].index\n\n    good_session_id = np.intersect1d(good_session_id, good_hour_session_id)\n    \n    good_minutes_session_id = np.concatenate((good_nine_oclock_session_id, \n                                              good_fifteen_oclock_session_id, good_six_oclock_session_id), axis=None)\n    \n    bad_minutes_session_id = np.concatenate((bad_nine_oclock_session_id,\n                                              bad_fifteen_oclock_session_id, bad_six_oclock_session_id), axis=None)\n    \n    good_session_id = np.unique(np.concatenate((good_session_id, good_minutes_session_id), axis=None))\n    \n    bad_session_id = np.unique(np.concatenate((bad_session_id, bad_hour_session_id), axis=None))\n    bad_session_id = np.unique(np.concatenate((bad_session_id, bad_minutes_session_id), axis=None))\n\n    \n    df = df.drop('minute', axis=1)\n    df = df.loc[good_session_id]\n    df = df.reset_index()\n    \n    return df, good_session_id, bad_session_id","9424f654":"def get_dummies(train_df, more_features):\n    l1 = more_features\n    l2 = ['number_of_sites','session_len','weekday','start_hour', 'end_hour', 'time_index', 'session_id']\n    col_names = l1 + l2\n    train_df = train_df[col_names]\n    train_df = train_df.set_index('session_id')\n    train_df = pd.get_dummies(train_df, columns=['weekday','start_hour', 'end_hour', 'number_of_sites'],\n                           drop_first=True)\n    train_df = train_df.dropna()\n    return train_df","c3234057":"train_df_short, good_session_id_train, bad_session_id_train = remove_days(train_df, border=70)\n\ntest_df = add_alice_sites(test_df, sites, alice_size = 0.06)\ntest_df = add_more_alice_sites(test_df, sites)\n\ntest_df_short, good_session_id_test, bad_session_id_test = remove_days(test_df, border=70)","86d4e87a":"more_features = ['alice_site', 'alice_site_2', 'alice_site_3', 'alice_site_4', 'target']\n\n\ntrain_df_short = get_dummies(train_df_short, more_features)\ntest_df_short = get_dummies(test_df_short, more_features)\n","e97d04cb":"train_df_short.head()","ea66b0ae":"X_train = train_df_short.drop('target', axis=1)\nX_train = X_train.drop(['start_hour_8', 'end_hour_8','start_hour_10','end_hour_10','start_hour_11','end_hour_11','start_hour_12','end_hour_12',\n                         'start_hour_14','end_hour_14','start_hour_19','end_hour_19','start_hour_20','end_hour_20','start_hour_21','end_hour_21',\n                        'start_hour_22','end_hour_22','start_hour_23','end_hour_23','time_index', 'alice_site_4'], axis=1)\n\ny_train = train_df_short.target\n\ny_train_valid = train_df.target\n\nX_test = test_df_short.drop('target', axis=1)\nX_test = X_test.drop(['start_hour_8', 'end_hour_8','start_hour_10','end_hour_10','start_hour_11','end_hour_11','start_hour_12','end_hour_12',\n                         'start_hour_14','end_hour_14','start_hour_19','end_hour_19','start_hour_20','end_hour_20','start_hour_21','end_hour_21',\n                        'start_hour_22','end_hour_22','start_hour_23','end_hour_23','time_index', 'alice_site_4'], axis=1)\ny_test = test_df.target\n\n\n\nlog = LogisticRegression(C=4, multi_class='ovr', penalty='l1', solver='liblinear', random_state=126)\nlog.fit(X_train, y_train)\n\n# TEST PART\n# predict data with Alice (DataFrame with good_sessions_id)\ny_pred_prob_lg = log.predict_proba(X_test)[:,1]\nfinal_df_test_start = pd.DataFrame(y_pred_prob_lg, columns = ['target'], index=good_session_id_test)\n\n# prepare data without Alice (DataFrame with bad_sessions_id filled by zeros)\nempty_result = np.zeros((len(bad_session_id_test),), dtype=int)\nempty_df = pd.DataFrame(empty_result, columns = ['target'], index=bad_session_id_test)\n\n# concat two DataFrames\nfinal_df_test = pd.concat([final_df_test_start, empty_df], axis=0)\ny_pred = np.array(final_df_test.sort_index().target)\n\n# plot the ROC Curve\nfpr, tpr, trhesholds = roc_curve(y_test, y_pred)\nplt.plot([0, 1], [0, 1],'k--')\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Logistic Regression ROC Curve')\nplt.legend()\n\n# TRAIN PART\n# predict data with Alice (DataFrame with good_sessions_id)\ny_pred_prob_lg_train = log.predict_proba(X_train)[:,1]\nfinal_df_train_start = pd.DataFrame(y_pred_prob_lg_train, columns = ['target'], index=good_session_id_train)\n\n# prepare data without Alice (DataFrame with bad_sessions_id filled by zeros)\nempty_result = np.zeros((len(bad_session_id_train),), dtype=int)\nempty_df = pd.DataFrame(empty_result, columns = ['target'], index=bad_session_id_train)\n\n# concat two DataFrames\nfinal_df_train = pd.concat([final_df_train_start, empty_df], axis=0)\ny_pred_train = np.array(final_df_train.sort_index().target)\n\n# plot the ROC Curve\nfpr, tpr, trhesholds = roc_curve(y_train_valid, y_pred_train)\nplt.plot([0, 1], [0, 1],'k--')\nplt.plot(fpr, tpr, label='Logistic Regression Train')\nplt.legend()\n\nplt.show()\n\nprint(roc_auc_score(y_test, y_pred))\nprint(roc_auc_score(y_train_valid, y_pred_train))","955cd566":"# We are going to use DataFrame with good sessions. But we have to change the column name\nfinal_df_train_start = final_df_train_start.rename(columns=({'target':'pred'}))\n\n# We concatenate it with our train DataFrame (without bad sessions)\nnew_train_df_short = pd.concat([train_df_short, final_df_train_start], axis=1)\n\n# We choose time indexes with the prediction result over 0.1\ntime_of_alice_1 = np.array(new_train_df_short[new_train_df_short.pred > 0.1].time_index)\n\n# Mark their neighbors\ntime_of_alice_2 = time_of_alice_1 + 1\ntime_of_alice_3 = time_of_alice_1 - 1\n\n# Concatenate all those indexes\ntime_of_alice = np.unique(np.concatenate((time_of_alice_1, time_of_alice_2, time_of_alice_3), axis=None))\n\n# Create new feature for the strengthening of neighbors \nnew_train_df_short['neigh_time'] = 0\n\n# We choose neighbors with positive 'alice site 2'\ndf_for_mask = new_train_df_short[new_train_df_short.alice_site_2 == 1]\n\n# Apply the condition\ntime_of_alice = np.intersect1d(time_of_alice, np.array(df_for_mask['time_index']))\n\n# Add prediction result as a feature 'neigh time' with multiplicator\nneigh_time_mask = df_for_mask['time_index'].isin(time_of_alice)\nneigh_time_mask = neigh_time_mask[neigh_time_mask == True]\nnew_train_df_short.neigh_time[neigh_time_mask.index] = new_train_df_short.loc[neigh_time_mask.index, 'pred'] * 2.8\n\n# Drop the column with predictions \nnew_train_df_short = new_train_df_short.drop('pred', axis=1)\n\n# Apply the same operations to the test part\nfinal_df_test_start = final_df_test_start.rename(columns=({'target':'pred'}))\nnew_test_df_short = pd.concat([test_df_short, final_df_test_start], axis=1)\ntime_of_alice_1 = np.array(new_test_df_short[new_test_df_short.pred > 0.1].time_index)\ntime_of_alice_2 = time_of_alice_1 + 1\ntime_of_alice_3 = time_of_alice_1 - 1\ntime_of_alice = np.unique(np.concatenate((time_of_alice_1, time_of_alice_2, time_of_alice_3), axis=None))\nnew_test_df_short['neigh_time'] = 0\ndf_for_mask = new_test_df_short[new_test_df_short.alice_site_2 == 1]\ntime_of_alice = np.intersect1d(time_of_alice, np.array(df_for_mask['time_index']))\nneigh_time_mask = df_for_mask['time_index'].isin(time_of_alice)\nneigh_time_mask = neigh_time_mask[neigh_time_mask == True]\nnew_test_df_short.neigh_time[neigh_time_mask.index] = new_test_df_short.loc[neigh_time_mask.index, 'pred'] * 2.8\nnew_test_df_short = new_test_df_short.drop('pred', axis=1)\n\n# Repeat prediction\ntrain_df_short = new_train_df_short\ntest_df_short = new_test_df_short\n\nX_train = train_df_short.drop('target', axis=1)\nX_train = X_train.drop(['start_hour_8', 'end_hour_8','start_hour_10','end_hour_10','start_hour_11','end_hour_11','start_hour_12','end_hour_12',\n                         'start_hour_14','end_hour_14','start_hour_19','end_hour_19','start_hour_20','end_hour_20','start_hour_21','end_hour_21',\n                        'start_hour_22','end_hour_22','start_hour_23','end_hour_23','time_index', 'alice_site_4'], axis=1)\n\ny_train = train_df_short.target\n\ny_train_valid = train_df.target\n\nX_test = test_df_short.drop('target', axis=1)\nX_test = X_test.drop(['start_hour_8', 'end_hour_8','start_hour_10','end_hour_10','start_hour_11','end_hour_11','start_hour_12','end_hour_12',\n                         'start_hour_14','end_hour_14','start_hour_19','end_hour_19','start_hour_20','end_hour_20','start_hour_21','end_hour_21',\n                        'start_hour_22','end_hour_22','start_hour_23','end_hour_23','time_index', 'alice_site_4'], axis=1)\ny_test = test_df.target\n\n\nlog = LogisticRegression(C=4, multi_class='ovr', penalty='l1', solver='liblinear', random_state=126)\nlog.fit(X_train, y_train)\n\ny_pred_prob_lg = log.predict_proba(X_test)[:,1]\nfinal_df_test_start = pd.DataFrame(y_pred_prob_lg, columns = ['target'], index=good_session_id_test)\nempty_result = np.zeros((len(bad_session_id_test),), dtype=int)\nempty_df = pd.DataFrame(empty_result, columns = ['target'], index=bad_session_id_test)\nfinal_df_test = pd.concat([final_df_test_start, empty_df], axis=0)\ny_pred = np.array(final_df_test.sort_index().target)\n\n\ny_pred_prob_lg_train = log.predict_proba(X_train)[:,1]\nfinal_df_train_start = pd.DataFrame(y_pred_prob_lg_train, columns = ['target'], index=good_session_id_train)\nempty_result = np.zeros((len(bad_session_id_train),), dtype=int)\nempty_df = pd.DataFrame(empty_result, columns = ['target'], index=bad_session_id_train)\nfinal_df_train = pd.concat([final_df_train_start, empty_df], axis=0)\ny_pred_train = np.array(final_df_train.sort_index().target)\n\n\nprint(roc_auc_score(y_test, y_pred))\nprint(roc_auc_score(y_train_valid, y_pred_train))","ec22751f":"final_df_train_start = final_df_train_start.rename(columns=({'target':'pred'}))\nnew_train_df_short = pd.concat([train_df_short, final_df_train_start], axis=1)\ntime_of_alice_1 = np.array(new_train_df_short[new_train_df_short.pred > 0.8].time_index)\ntime_of_alice_2 = time_of_alice_1 + 1\ntime_of_alice_3 = time_of_alice_1 - 1\ntime_of_alice_4 = time_of_alice_1 + 2\ntime_of_alice_5 = time_of_alice_1 - 2\ntime_of_alice_6 = time_of_alice_1 + 3\ntime_of_alice_7 = time_of_alice_1 - 3\ntime_of_alice = np.unique(np.concatenate((time_of_alice_1, time_of_alice_2, time_of_alice_3,\n                                         time_of_alice_4, time_of_alice_5, time_of_alice_6,\n                                          time_of_alice_7), axis=None))\nnew_train_df_short['neigh_time_2'] = 0\ndf_for_mask = new_train_df_short[new_train_df_short.alice_site_4 == 1]\ntime_of_alice = np.intersect1d(time_of_alice, np.array(df_for_mask['time_index']))\nneigh_time_mask = df_for_mask['time_index'].isin(time_of_alice)\nneigh_time_mask = neigh_time_mask[neigh_time_mask == True]\nnew_train_df_short.neigh_time_2[neigh_time_mask.index] = new_train_df_short.loc[neigh_time_mask.index, 'pred'] * 3\nnew_train_df_short = new_train_df_short.drop('pred', axis=1)\n\nfinal_df_test_start = final_df_test_start.rename(columns=({'target':'pred'}))\nnew_test_df_short = pd.concat([test_df_short, final_df_test_start], axis=1)\ntime_of_alice_1 = np.array(new_test_df_short[new_test_df_short.pred > 0.4].time_index) \ntime_of_alice_2 = time_of_alice_1 + 1\ntime_of_alice_3 = time_of_alice_1 - 1\ntime_of_alice_4 = time_of_alice_1 + 2\ntime_of_alice_5 = time_of_alice_1 - 2\ntime_of_alice_6 = time_of_alice_1 + 3\ntime_of_alice_7 = time_of_alice_1 - 3\ntime_of_alice = np.unique(np.concatenate((time_of_alice_1, time_of_alice_2, time_of_alice_3,\n                                         time_of_alice_4, time_of_alice_5, time_of_alice_6,\n                                          time_of_alice_7), axis=None))\nnew_test_df_short['neigh_time_2'] = 0\ndf_for_mask = new_test_df_short[new_test_df_short.alice_site_4 == 1]\ntime_of_alice = np.intersect1d(time_of_alice, np.array(df_for_mask['time_index']))\nneigh_time_mask = df_for_mask['time_index'].isin(time_of_alice)\nneigh_time_mask = neigh_time_mask[neigh_time_mask == True]\nnew_test_df_short.neigh_time_2[neigh_time_mask.index] = new_test_df_short.loc[neigh_time_mask.index, 'pred'] * 2\nnew_test_df_short = new_test_df_short.drop('pred', axis=1)\n\ntrain_df_short = new_train_df_short\ntest_df_short = new_test_df_short\n\nX_train = train_df_short.drop('target', axis=1)\nX_train = X_train.drop(['start_hour_8', 'end_hour_8','start_hour_10','end_hour_10','start_hour_11','end_hour_11','start_hour_12','end_hour_12',\n                         'start_hour_14','end_hour_14','start_hour_19','end_hour_19','start_hour_20','end_hour_20','start_hour_21','end_hour_21',\n                        'start_hour_22','end_hour_22','start_hour_23','end_hour_23','time_index', 'alice_site_4'], axis=1)\n\ny_train = train_df_short.target\n\ny_train_valid = train_df.target\n\nX_test = test_df_short.drop('target', axis=1)\nX_test = X_test.drop(['start_hour_8', 'end_hour_8','start_hour_10','end_hour_10','start_hour_11','end_hour_11','start_hour_12','end_hour_12',\n                         'start_hour_14','end_hour_14','start_hour_19','end_hour_19','start_hour_20','end_hour_20','start_hour_21','end_hour_21',\n                        'start_hour_22','end_hour_22','start_hour_23','end_hour_23','time_index', 'alice_site_4'], axis=1)\ny_test = test_df.target\n\n\nlog = LogisticRegression(C=4, multi_class='ovr', penalty='l1', solver='liblinear', random_state=126)\nlog.fit(X_train, y_train)\n\ny_pred_prob_lg = log.predict_proba(X_test)[:,1]\nfinal_df_test_start = pd.DataFrame(y_pred_prob_lg, columns = ['target'], index=good_session_id_test)\nempty_result = np.zeros((len(bad_session_id_test),), dtype=int)\nempty_df = pd.DataFrame(empty_result, columns = ['target'], index=bad_session_id_test)\nfinal_df_test = pd.concat([final_df_test_start, empty_df], axis=0)\ny_pred = np.array(final_df_test.sort_index().target)\n\n\ny_pred_prob_lg_train = log.predict_proba(X_train)[:,1]\nfinal_df_train_start = pd.DataFrame(y_pred_prob_lg_train, columns = ['target'], index=good_session_id_train)\nempty_result = np.zeros((len(bad_session_id_train),), dtype=int)\nempty_df = pd.DataFrame(empty_result, columns = ['target'], index=bad_session_id_train)\nfinal_df_train = pd.concat([final_df_train_start, empty_df], axis=0)\ny_pred_train = np.array(final_df_train.sort_index().target)\n\n\nprint(roc_auc_score(y_test, y_pred))\nprint(roc_auc_score(y_train_valid, y_pred_train))","63ecb51c":"final_df_train_start = final_df_train_start.rename(columns=({'target':'pred'}))\nnew_train_df_short = pd.concat([train_df_short, final_df_train_start], axis=1)\ntime_of_alice_1 = np.array(new_train_df_short[new_train_df_short.pred > 0.3].time_index)\ntime_of_alice_2 = time_of_alice_1 + 1\ntime_of_alice_3 = time_of_alice_1 - 1\ntime_of_alice_4 = time_of_alice_1 + 2\ntime_of_alice_5 = time_of_alice_1 - 2\ntime_of_alice_6 = time_of_alice_1 + 3\ntime_of_alice_7 = time_of_alice_1 - 3\ntime_of_alice_8 = time_of_alice_1 + 4\ntime_of_alice_9 = time_of_alice_1 - 4\ntime_of_alice = np.unique(np.concatenate((time_of_alice_1, time_of_alice_2, time_of_alice_3,\n                                         time_of_alice_4, time_of_alice_5, time_of_alice_6,\n                                          time_of_alice_7, time_of_alice_8, time_of_alice_9), axis=None))\nnew_train_df_short['neigh_time_3'] = 0\ndf_for_mask = new_train_df_short[new_train_df_short.alice_site == 1]\ntime_of_alice = np.intersect1d(time_of_alice, np.array(df_for_mask['time_index']))\nneigh_time_mask = df_for_mask['time_index'].isin(time_of_alice)\nneigh_time_mask = neigh_time_mask[neigh_time_mask == True]\nnew_train_df_short.neigh_time_3[neigh_time_mask.index] = new_train_df_short.loc[neigh_time_mask.index, 'pred'] * 0.3\nnew_train_df_short = new_train_df_short.drop('pred', axis=1)\n\nfinal_df_test_start = final_df_test_start.rename(columns=({'target':'pred'}))\nnew_test_df_short = pd.concat([test_df_short, final_df_test_start], axis=1)\ntime_of_alice_1 = np.array(new_test_df_short[new_test_df_short.pred > 0.3].time_index)\ntime_of_alice_2 = time_of_alice_1 + 1\ntime_of_alice_3 = time_of_alice_1 - 1\ntime_of_alice_4 = time_of_alice_1 + 2\ntime_of_alice_5 = time_of_alice_1 - 2\ntime_of_alice_6 = time_of_alice_1 + 3\ntime_of_alice_7 = time_of_alice_1 - 3\ntime_of_alice_8 = time_of_alice_1 + 4\ntime_of_alice_9 = time_of_alice_1 - 4\ntime_of_alice = np.unique(np.concatenate((time_of_alice_1, time_of_alice_2, time_of_alice_3,\n                                         time_of_alice_4, time_of_alice_5, time_of_alice_6,\n                                          time_of_alice_7, time_of_alice_8, time_of_alice_9), axis=None))\nnew_test_df_short['neigh_time_3'] = 0\ndf_for_mask = new_test_df_short[new_test_df_short.alice_site == 1]\ntime_of_alice = np.intersect1d(time_of_alice, np.array(df_for_mask['time_index']))\nneigh_time_mask = df_for_mask['time_index'].isin(time_of_alice)\nneigh_time_mask = neigh_time_mask[neigh_time_mask == True]\nnew_test_df_short.neigh_time_3[neigh_time_mask.index] = new_test_df_short.loc[neigh_time_mask.index, 'pred'] * 3\nnew_test_df_short = new_test_df_short.drop('pred', axis=1)\n\ntrain_df_short = new_train_df_short\ntest_df_short = new_test_df_short\n\nX_train = train_df_short.drop('target', axis=1)\nX_train = X_train.drop(['start_hour_8', 'end_hour_8','start_hour_10','end_hour_10','start_hour_11','end_hour_11','start_hour_12','end_hour_12',\n                         'start_hour_14','end_hour_14','start_hour_19','end_hour_19','start_hour_20','end_hour_20','start_hour_21','end_hour_21',\n                        'start_hour_22','end_hour_22','start_hour_23','end_hour_23','time_index', 'alice_site_4'], axis=1)\n\ny_train = train_df_short.target\n\ny_train_valid = train_df.target\n\nX_test = test_df_short.drop('target', axis=1)\nX_test = X_test.drop(['start_hour_8', 'end_hour_8','start_hour_10','end_hour_10','start_hour_11','end_hour_11','start_hour_12','end_hour_12',\n                         'start_hour_14','end_hour_14','start_hour_19','end_hour_19','start_hour_20','end_hour_20','start_hour_21','end_hour_21',\n                        'start_hour_22','end_hour_22','start_hour_23','end_hour_23','time_index', 'alice_site_4'], axis=1)\ny_test = test_df.target\n\n\nlog = LogisticRegression(C=4, multi_class='ovr', penalty='l1', solver='liblinear', random_state=126)\nlog.fit(X_train, y_train)\n\ny_pred_prob_lg = log.predict_proba(X_test)[:,1]\nfinal_df_test_start = pd.DataFrame(y_pred_prob_lg, columns = ['target'], index=good_session_id_test)\nempty_result = np.zeros((len(bad_session_id_test),), dtype=int)\nempty_df = pd.DataFrame(empty_result, columns = ['target'], index=bad_session_id_test)\nfinal_df_test = pd.concat([final_df_test_start, empty_df], axis=0)\ny_pred = np.array(final_df_test.sort_index().target)\n\n\ny_pred_prob_lg_train = log.predict_proba(X_train)[:,1]\nfinal_df_train_start = pd.DataFrame(y_pred_prob_lg_train, columns = ['target'], index=good_session_id_train)\nempty_result = np.zeros((len(bad_session_id_train),), dtype=int)\nempty_df = pd.DataFrame(empty_result, columns = ['target'], index=bad_session_id_train)\nfinal_df_train = pd.concat([final_df_train_start, empty_df], axis=0)\ny_pred_train = np.array(final_df_train.sort_index().target)\n\n\nprint(roc_auc_score(y_test, y_pred))\nprint(roc_auc_score(y_train_valid, y_pred_train))","5defe5b2":"# TRAIN part\n\ndf = pd.read_csv(\"..\/input\/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2\/train_sessions.csv\")\ndf = time_operations(df)\ntrain_df, test_df = time_treshold_save_zero(df, date='2020-03-01')\ntrain_df = add_alice_sites(train_df, sites, alice_size = 0.06)\ntrain_df = add_more_alice_sites(train_df, sites)\ntrain_df_short, good_session_id_train, bad_session_id_train = remove_days(train_df, border=70)\nmore_features = ['alice_site', 'alice_site_2', 'alice_site_3', 'alice_site_4', 'target']\ntrain_df_short = get_dummies(train_df_short, more_features)\nX_train = train_df_short.drop('target', axis=1)\nX_train = X_train.drop(['start_hour_8', 'end_hour_8','start_hour_10','end_hour_10','start_hour_11','end_hour_11','start_hour_12','end_hour_12',\n                         'start_hour_14','end_hour_14','start_hour_19','end_hour_19','start_hour_20','end_hour_20','start_hour_21','end_hour_21',\n                        'start_hour_22','end_hour_22','start_hour_23','end_hour_23','time_index', 'alice_site_4'], axis=1)\ny_train = train_df_short.target\ny_train_valid = train_df.target\n\nlog1 = LogisticRegression(C=4, multi_class='ovr', penalty='l1', solver='liblinear', random_state=126)\nlog1.fit(X_train, y_train)\n\ny_pred_prob_lg_train = log1.predict_proba(X_train)[:,1]\nfinal_df_train_start = pd.DataFrame(y_pred_prob_lg_train, columns = ['target'], index=good_session_id_train)\nfinal_df_train_start = final_df_train_start.rename(columns=({'target':'pred'}))\nnew_train_df_short = pd.concat([train_df_short, final_df_train_start], axis=1)\ntime_of_alice_1 = np.array(new_train_df_short[new_train_df_short.pred > 0.1].time_index)\ntime_of_alice_2 = time_of_alice_1 + 1\ntime_of_alice_3 = time_of_alice_1 - 1\ntime_of_alice = np.unique(np.concatenate((time_of_alice_1, time_of_alice_2, time_of_alice_3), axis=None))\nnew_train_df_short['neigh_time'] = 0\ndf_for_mask = new_train_df_short[new_train_df_short.alice_site_2 == 1]\ntime_of_alice = np.intersect1d(time_of_alice, np.array(df_for_mask['time_index']))\nneigh_time_mask = df_for_mask['time_index'].isin(time_of_alice)\nneigh_time_mask = neigh_time_mask[neigh_time_mask == True]\nnew_train_df_short.neigh_time[neigh_time_mask.index] = new_train_df_short.loc[neigh_time_mask.index, 'pred'] * 2.8\nnew_train_df_short = new_train_df_short.drop('pred', axis=1)\n\ntrain_df_short = new_train_df_short\nX_train = new_train_df_short.drop('target', axis=1)\nX_train = X_train.drop(['start_hour_8', 'end_hour_8','start_hour_10','end_hour_10','start_hour_11','end_hour_11','start_hour_12','end_hour_12',\n                         'start_hour_14','end_hour_14','start_hour_19','end_hour_19','start_hour_20','end_hour_20','start_hour_21','end_hour_21',\n                        'start_hour_22','end_hour_22','start_hour_23','end_hour_23','time_index', 'alice_site_4'], axis=1)\ny_train = train_df_short.target\nlog2 = LogisticRegression(C=4, multi_class='ovr', penalty='l1', solver='liblinear', random_state=126)\nlog2.fit(X_train, y_train)\n\ny_pred_prob_lg = log2.predict_proba(X_train)[:,1]\nfinal_df_train_start = pd.DataFrame(y_pred_prob_lg, columns = ['target'], index=good_session_id_train)\nfinal_df_train_start = final_df_train_start.rename(columns=({'target':'pred'}))\nnew_train_df_short = pd.concat([train_df_short, final_df_train_start], axis=1)\ntime_of_alice_1 = np.array(new_train_df_short[new_train_df_short.pred > 0.8].time_index)\ntime_of_alice_2 = time_of_alice_1 + 1\ntime_of_alice_3 = time_of_alice_1 - 1\ntime_of_alice_4 = time_of_alice_1 + 2\ntime_of_alice_5 = time_of_alice_1 - 2\ntime_of_alice_6 = time_of_alice_1 + 3\ntime_of_alice_7 = time_of_alice_1 - 3\ntime_of_alice = np.unique(np.concatenate((time_of_alice_1, time_of_alice_2, time_of_alice_3,\n                                         time_of_alice_4, time_of_alice_5, time_of_alice_6,\n                                          time_of_alice_7), axis=None))\nnew_train_df_short['neigh_time_2'] = 0\ndf_for_mask = new_train_df_short[new_train_df_short.alice_site_2 == 1]\ntime_of_alice = np.intersect1d(time_of_alice, np.array(df_for_mask['time_index']))\nneigh_time_mask = df_for_mask['time_index'].isin(time_of_alice)\nneigh_time_mask = neigh_time_mask[neigh_time_mask == True]\nnew_train_df_short.neigh_time_2[neigh_time_mask.index] = new_train_df_short.loc[neigh_time_mask.index, 'pred'] * 3\nnew_train_df_short = new_train_df_short.drop('pred', axis=1)\n\ntrain_df_short = new_train_df_short\nX_train = new_train_df_short.drop('target', axis=1)\nX_train = X_train.drop(['start_hour_8', 'end_hour_8','start_hour_10','end_hour_10','start_hour_11','end_hour_11','start_hour_12','end_hour_12',\n                         'start_hour_14','end_hour_14','start_hour_19','end_hour_19','start_hour_20','end_hour_20','start_hour_21','end_hour_21',\n                        'start_hour_22','end_hour_22','start_hour_23','end_hour_23','time_index', 'alice_site_4'], axis=1)\ny_train = train_df_short.target\nlog3 = LogisticRegression(C=4, multi_class='ovr', penalty='l1', solver='liblinear', random_state=126)\nlog3.fit(X_train, y_train)\n\ny_pred_prob_lg = log3.predict_proba(X_train)[:,1]\nfinal_df_test_train = pd.DataFrame(y_pred_prob_lg, columns = ['target'], index=good_session_id_train)\nfinal_df_train_start = final_df_train_start.rename(columns=({'target':'pred'}))\nnew_train_df_short = pd.concat([train_df_short, final_df_train_start], axis=1)\ntime_of_alice_1 = np.array(new_train_df_short[new_train_df_short.pred > 0.3].time_index)\ntime_of_alice_2 = time_of_alice_1 + 1\ntime_of_alice_3 = time_of_alice_1 - 1\ntime_of_alice_4 = time_of_alice_1 + 2\ntime_of_alice_5 = time_of_alice_1 - 2\ntime_of_alice_6 = time_of_alice_1 + 3\ntime_of_alice_7 = time_of_alice_1 - 3\ntime_of_alice_8 = time_of_alice_1 + 4\ntime_of_alice_9 = time_of_alice_1 - 4\ntime_of_alice = np.unique(np.concatenate((time_of_alice_1, time_of_alice_2, time_of_alice_3,\n                                         time_of_alice_4, time_of_alice_5, time_of_alice_6,\n                                          time_of_alice_7, time_of_alice_8, time_of_alice_9), axis=None))\nnew_train_df_short['neigh_time_3'] = 0\ndf_for_mask = new_train_df_short[new_train_df_short.alice_site == 1]\ntime_of_alice = np.intersect1d(time_of_alice, np.array(df_for_mask['time_index']))\nneigh_time_mask = df_for_mask['time_index'].isin(time_of_alice)\nneigh_time_mask = neigh_time_mask[neigh_time_mask == True]\nnew_train_df_short.neigh_time_3[neigh_time_mask.index] = new_train_df_short.loc[neigh_time_mask.index, 'pred'] * 0.3\nnew_train_df_short = new_train_df_short.drop('pred', axis=1)\n\ntrain_df_short = new_train_df_short\nX_train = new_train_df_short.drop('target', axis=1)\nX_train = X_train.drop(['start_hour_8', 'end_hour_8','start_hour_10','end_hour_10','start_hour_11','end_hour_11','start_hour_12','end_hour_12',\n                         'start_hour_14','end_hour_14','start_hour_19','end_hour_19','start_hour_20','end_hour_20','start_hour_21','end_hour_21',\n                        'start_hour_22','end_hour_22','start_hour_23','end_hour_23','time_index', 'alice_site_4'], axis=1)\ny_train = train_df_short.target\nlog4 = LogisticRegression(C=4, multi_class='ovr', penalty='l1', solver='liblinear', random_state=126)\nlog4.fit(X_train, y_train)\n\n# TEST part\n\ndf = pd.read_csv(\"..\/input\/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2\/test_sessions.csv\")\ndf = time_operations(df)\ntrain_df, test_df = time_treshold_save_zero(df, date='2020-03-01')\ntrain_df = add_alice_sites(train_df, sites, alice_size = 0.06)\ntrain_df = add_more_alice_sites(train_df, sites)\ntrain_df_short, good_session_id_train, bad_session_id_train = remove_days(train_df, border=70)\nmore_features = ['alice_site', 'alice_site_2', 'alice_site_3','alice_site_4']\ntrain_df_short = get_dummies(train_df_short, more_features)\nX_train = train_df_short\nX_train = X_train.drop(['start_hour_8', 'end_hour_8','start_hour_10','end_hour_10','start_hour_11','end_hour_11','start_hour_12','end_hour_12',\n                         'start_hour_14','end_hour_14','start_hour_19','end_hour_19','start_hour_20','end_hour_20','start_hour_21','end_hour_21',\n                        'start_hour_22','end_hour_22','start_hour_23','end_hour_23','time_index', 'alice_site_4'], axis=1)\n\ny_pred_prob_lg_train = log1.predict_proba(X_train)[:,1]\nfinal_df_train_start = pd.DataFrame(y_pred_prob_lg_train, columns = ['target'], index=good_session_id_train)\nfinal_df_train_start = final_df_train_start.rename(columns=({'target':'pred'}))\nnew_train_df_short = pd.concat([train_df_short, final_df_train_start], axis=1)\ntime_of_alice_1 = np.array(new_train_df_short[new_train_df_short.pred > 0.1].time_index) #0.6\ntime_of_alice_2 = time_of_alice_1 + 1\ntime_of_alice_3 = time_of_alice_1 - 1\ntime_of_alice = np.unique(np.concatenate((time_of_alice_1, time_of_alice_2, time_of_alice_3), axis=None))\nnew_train_df_short['neigh_time'] = 0\ndf_for_mask = new_train_df_short[new_train_df_short.alice_site_2 == 1]\ntime_of_alice = np.intersect1d(time_of_alice, np.array(df_for_mask['time_index']))\nneigh_time_mask = df_for_mask['time_index'].isin(time_of_alice)\nneigh_time_mask = neigh_time_mask[neigh_time_mask == True]\nnew_train_df_short.neigh_time[neigh_time_mask.index] = new_train_df_short.loc[neigh_time_mask.index, 'pred'] * 2.8\nnew_train_df_short = new_train_df_short.drop('pred', axis=1)\n\ntrain_df_short = new_train_df_short\nX_train = new_train_df_short\nX_train = X_train.drop(['start_hour_8', 'end_hour_8','start_hour_10','end_hour_10','start_hour_11','end_hour_11','start_hour_12','end_hour_12',\n                         'start_hour_14','end_hour_14','start_hour_19','end_hour_19','start_hour_20','end_hour_20','start_hour_21','end_hour_21',\n                        'start_hour_22','end_hour_22','start_hour_23','end_hour_23','time_index', 'alice_site_4'], axis=1)\n\ny_pred_prob_lg = log2.predict_proba(X_train)[:,1]\nfinal_df_train_start = pd.DataFrame(y_pred_prob_lg, columns = ['target'], index=good_session_id_train)\nfinal_df_train_start = final_df_train_start.rename(columns=({'target':'pred'}))\nnew_train_df_short = pd.concat([train_df_short, final_df_train_start], axis=1)\ntime_of_alice_1 = np.array(new_train_df_short[new_train_df_short.pred > 0.8].time_index)\ntime_of_alice_2 = time_of_alice_1 + 1\ntime_of_alice_3 = time_of_alice_1 - 1\ntime_of_alice_4 = time_of_alice_1 + 2\ntime_of_alice_5 = time_of_alice_1 - 2\ntime_of_alice_6 = time_of_alice_1 + 3\ntime_of_alice_7 = time_of_alice_1 - 3\ntime_of_alice = np.unique(np.concatenate((time_of_alice_1, time_of_alice_2, time_of_alice_3,\n                                         time_of_alice_4, time_of_alice_5, time_of_alice_6,\n                                          time_of_alice_7), axis=None))\nnew_train_df_short['neigh_time_2'] = 0\ndf_for_mask = new_train_df_short[new_train_df_short.alice_site_2 == 1]\ntime_of_alice = np.intersect1d(time_of_alice, np.array(df_for_mask['time_index']))\nneigh_time_mask = df_for_mask['time_index'].isin(time_of_alice)\nneigh_time_mask = neigh_time_mask[neigh_time_mask == True]\nnew_train_df_short.neigh_time_2[neigh_time_mask.index] = new_train_df_short.loc[neigh_time_mask.index, 'pred'] * 3\nnew_train_df_short = new_train_df_short.drop('pred', axis=1)\n\ntrain_df_short = new_train_df_short\nX_train = new_train_df_short\nX_train = X_train.drop(['start_hour_8', 'end_hour_8','start_hour_10','end_hour_10','start_hour_11','end_hour_11','start_hour_12','end_hour_12',\n                         'start_hour_14','end_hour_14','start_hour_19','end_hour_19','start_hour_20','end_hour_20','start_hour_21','end_hour_21',\n                        'start_hour_22','end_hour_22','start_hour_23','end_hour_23','time_index', 'alice_site_4'], axis=1)\n\ny_pred_prob_lg = log3.predict_proba(X_train)[:,1]\nfinal_df_train_start = pd.DataFrame(y_pred_prob_lg, columns = ['target'], index=good_session_id_train)\nfinal_df_train_start = final_df_train_start.rename(columns=({'target':'pred'}))\nnew_train_df_short = pd.concat([train_df_short, final_df_train_start], axis=1)\ntime_of_alice_1 = np.array(new_train_df_short[new_train_df_short.pred > 0.3].time_index)\ntime_of_alice_2 = time_of_alice_1 + 1\ntime_of_alice_3 = time_of_alice_1 - 1\ntime_of_alice_4 = time_of_alice_1 + 2\ntime_of_alice_5 = time_of_alice_1 - 2\ntime_of_alice_6 = time_of_alice_1 + 3\ntime_of_alice_7 = time_of_alice_1 - 3\ntime_of_alice_8 = time_of_alice_1 + 4\ntime_of_alice_9 = time_of_alice_1 - 4\ntime_of_alice = np.unique(np.concatenate((time_of_alice_1, time_of_alice_2, time_of_alice_3,\n                                         time_of_alice_4, time_of_alice_5, time_of_alice_6,\n                                          time_of_alice_7, time_of_alice_8, time_of_alice_9), axis=None))\nnew_train_df_short['neigh_time_3'] = 0\ndf_for_mask = new_train_df_short[new_train_df_short.alice_site == 1]\ntime_of_alice = np.intersect1d(time_of_alice, np.array(df_for_mask['time_index']))\nneigh_time_mask = df_for_mask['time_index'].isin(time_of_alice)\nneigh_time_mask = neigh_time_mask[neigh_time_mask == True]\nnew_train_df_short.neigh_time_3[neigh_time_mask.index] = new_train_df_short.loc[neigh_time_mask.index, 'pred'] * 0.3\nnew_train_df_short = new_train_df_short.drop('pred', axis=1)\n\ntrain_df_short = new_train_df_short\nX_train = new_train_df_short\nX_train = X_train.drop(['start_hour_8', 'end_hour_8','start_hour_10','end_hour_10','start_hour_11','end_hour_11','start_hour_12','end_hour_12',\n                         'start_hour_14','end_hour_14','start_hour_19','end_hour_19','start_hour_20','end_hour_20','start_hour_21','end_hour_21',\n                        'start_hour_22','end_hour_22','start_hour_23','end_hour_23','time_index', 'alice_site_4'], axis=1)\n\n\n# predict data with Alice (DataFrame with good_sessions_id)\ny_pred_prob_lg_train = log4.predict_proba(X_train)[:,1]\nfinal_df_train_start = pd.DataFrame(y_pred_prob_lg_train, columns = ['target'], index=good_session_id_train)\n\n# prepare data without Alice (DataFrame with bad_sessions_id filled by zeros)\nempty_result = np.zeros((len(bad_session_id_train),), dtype=int)\nempty_df = pd.DataFrame(empty_result, columns = ['target'], index=bad_session_id_train)\n\n# concat two DataFrames\nfinal_df_train = pd.concat([final_df_train_start, empty_df], axis=0)\ny_pred_train = np.array(final_df_train.sort_index().target)\n\nfinal_df = pd.DataFrame(y_pred_train, columns = ['target'])\nfinal_df = final_df.reset_index()\nfinal_df.columns\nfinal_df = final_df.rename(columns={\"index\": \"session_id\"})\nfinal_df.loc[:,['session_id']] += 1\nfinal_df.to_csv('catch-me_sub_iy.csv', index=False)","1f9fe840":"There are two main factors in the Alice dataset. The first one is an uneven distribution. It plays against us. The second one is the repeatability of Alice's actions. It's our best friend","27ffb38d":"There are two possibilities for how to apply 'sites' features: with or without the data from the pickle file.\n\nThis information could help to discover some dependencies between different URL names. So you can highlight some sites that are very popular with Alice, and use them as features. For this purpose, I used regex with and without an English dictionary (to gain English words instead of meaningless phrases). It didn't give me an advantage and even make the submission's result a bit worse.\n\nProbably there is a good way to use the pickle file that I didn't find. If you did write me in the comments, please!\n\nSo now I show my way to interact with sites. You need to create a long DataFrame to collect information about sites in separate rows. And then - group by. To receive a mean target value for each site.","609cec4f":"And it's possible to add more dummies: something like upper and lower bounds","7d8a8c3f":"Let's explore the other 2.5%. We can see different fractions. A hypothesis is if the user during the session visited one or more of these sites that means the user is Alice. But we need to find a good weight: to put in the feature popular site and to save the influence of the feature at the same time.\n\nAfter some tests, I chose a value of 0.06. I created the dummy feature \"Alice sites\". It's 'one', if during the session user visited one or more sites with a mean target over 0.06. If he (she) didn't, the value is 'zero'.","6d2f476b":"... and after 18.25.","7c0ca12e":"It works. We can repeat it twice with the big number of neighbors","faf3b4a8":"First of all. I would like to say thank you to all the guys and gals who shared their codes here. As a novice, I was appreciated. These notebooks helped me a lot.\n\nBut while working on my own code I found that some features became very popular among the authors. And I'm not sure that they are helpful ones. So I decided to tell you about my view of features.\n\nSo in the very begging, I would like to show you three lists:\n\n### BAD Features:\n- times of day (morning, evening, night, etc). It's a really bad idea, and I tell you why\n- working days and weekends as dummies\n- months. I can understand the idea, but it doesn't work\n\n### GOOD Features (new ones):\n- hourly, daily, and weekly observations\n- frequency sites' visits\n- neighbors\n\n### DOUBTFUL Features:\n- URL names\n- English words inside URL names\n- number of sites during the session\n\nI'll try to explain you all. Let's start!","7ab2ffab":"Just some important operations before predictions. Do you remember what we found about Alice's schedule? It differs from all the other users do. So we can be pretty sure that during some periods Alice didn't use her browser. So it's better to hide these periods from the estimator. We know that the target is 'zero'.\n\nIt's easy to remove excess days from the train part. But it's useless for prediction. So we will apply our \"Alice site\" feature. We are not interested in days with a low sum value. Let's count them.","75f3491c":"There is a way to improve result a little bit. Some Alice's sessions are followed one by one (you have to look at the time index, not at the session id). What if the estimator is sure about some of them, but doubts about the others? We can try to add some point to the Alice's neighbors.","a7a25443":"## The summary\n\nIf you like my code don't forget to vote and say \"thank you\". It's important for me to know that somebody read it :) It would be great if write in the comments something about you features that you discovered, about your technics and, of course, about my mistakes. Because I'm sure there is a lot of things which are still hidden from me.","e9dc51ea":"# Know basics about your data","c8050ea1":"But it's not the end!","05bf42d9":"The situation with the \"number of sites\" feature is the same as \"session length\". It is a very weak feature, but I decided to save it.","42b14c12":"The histogram plots with the dates from both datasets (train and test) demonstrate the uneven distribution that I mentioned. It's harder to build a good pattern when you don't have full information. But as you can see Alice's dates differ from other users' dates. It could be useful too.","34fdb76f":"There is a big amount of bad (excess) days. We can hope that the same rules work in the test part also.","5939bf6f":"There are only three visits between 11 and 12 AM. We can exclude the full period or only the main part (session started before 11.50). It's up to you","8295c058":"I loaded an almost full list of the libraries that I used for analysis. You can exclude many of them if you want to build a baseline model. But I think it would be interesting to try different estimators, for example. Or combinations of them. I prefer LogisticRegression, but with some features, lightgbm performed better.","b3d6e6d8":"It's time to show some plot with useless features (\"day of month\" and \"minute\"). The distribution has nothing to show, but sometimes it better to plot all :) And we are going to use one of these features in the next section","fc7445c9":"# Prediction","5dc91545":"# Sites features","a7adce3f":"And finally... Prediction!","07525339":"# Libraries","25083888":"# Remove dates without Alice","3c36b256":"It's better to exclude all between 2 and 3 PM. There are only two days (four sessions) in the sample and they look spontaneous.","788ed89e":"Now we can create our train and test sets.","70002f0e":"Now is the top feature. Hours. It shows us that Alice is a great fan of a clear schedule. The distributions of hours are really different. We observe the periods when Alice doesn't use the internet at all. We will use this fact.","9e320e9c":"She doesn't browse between 15.20 and 15.50...","ade7c3b2":"## Neighbors","4694e6fb":"I used logarithm to demonstrate the distribution of sessions' length better. We observe the difference, but it's not critical. Anyway, I saved this feature.","ef2bd3c5":"We create a function to separate good session ids (we are going to use them for prediction) from bad session ids (we are definitely sure that the target should be 'zero').","c5b680ad":"Alice doesn't use the internet after 9.30. We can exclude this period using the 'minute' feature.","63526921":"97.5% of them have 'zero' value. It means that Alice never visited them","eafd4e1c":"Let's split our dataset into train and validate parts. The threshold is '2014-03-01'.","7abec6aa":"A bit additional portions of information about Alice's hours.","fa886a2a":"And as I said, now it's a very important thing - hours. Do you remember histograms? All users browse websites during seventeen hours per day, Alice - nine. Maybe even seven! So we exclude eight hours (with prediction 'zero') and explore all the others.","10b69086":"Standard preparatory operations:\n- creating future features: number of sites, length of session, weekday, start hour, end hour (not necessary, but it helps)\n- creating technical features for EDA: session start and session end, minute, day of the month, time index","14981f35":"Creating dummies variables","c2b1735b":"If you plot the distributions of weekdays (weekday schedule) you could see that Alice's day differs from all the others. We have very weak Wednesday, Saturday, and Sunday. And a very strong Monday. That's the reason why I think it should be better to leave weekday as dummy features and don't group them.","08fd0207":"## Submit","c96e7b83":"The distribution of months looks very intriguing. It's easy to create a hypothesis that Alice doesn't use the internet during summer (she likes schedules!). But a big part of the test set is completed with summer sessions. So it's implausible that we have to predict 'zero' for all those dates. But you can try (I've tried): hypotheses need to be tested."}}