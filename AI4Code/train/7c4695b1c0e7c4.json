{"cell_type":{"572a5b32":"code","305864eb":"code","0c88d3b3":"code","31cabba0":"code","d38b574e":"code","6cd62f53":"code","80630dbc":"code","1b7d6df7":"markdown","a2fda948":"markdown","068003d6":"markdown","da5763c8":"markdown","6a799ee0":"markdown","9db869df":"markdown","d0c0d69b":"markdown","4ff2dcc5":"markdown","ddc1f27a":"markdown"},"source":{"572a5b32":"import os, sys\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModel","305864eb":"train_data = pd.read_csv('\/kaggle\/input\/zenify-tweet-train-folds\/train_folds.csv')\ntrain_data.head()","0c88d3b3":"model_name = \"roberta-base\"\nsanitycheck_model_names = ['bert-base-uncased','bert-large-uncased','bert-base-cased',\n                    'bert-large-cased','bert-base-multilingual-uncased',\n                    'bert-base-multilingual-cased','roberta-base','roberta-large',\n                    'albert-base-v1','albert-large-v1','albert-xlarge-v1']","31cabba0":"tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n\nfor train_index in range(5):\n    question = train_data.sentiment[train_index]\n    answer = train_data.text[train_index]\n    encoded_input = tokenizer.encode_plus(question, answer, add_special_tokens=True, return_offsets_mapping=True)\n    try:\n        print(\"\\nQuestion: \" + question + ', Answer: ' + answer)\n        print(\"Encoded Input: \" + str(encoded_input['input_ids']))\n        print(\"Attention Mask: \" + str(encoded_input['attention_mask']))\n        print(\"Offset: \" + str(encoded_input['offset_mapping']))\n        print(\"Token Type Ids: \" + str(encoded_input['token_type_ids']))\n    except:\n        pass\n    ","d38b574e":"for model_ in sanitycheck_model_names:\n    print(\"\\nModel: \" + model_)\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(model_, use_fast=True)\n        question = train_data.sentiment[0]\n        answer = train_data.text[0]\n    \n        if model_ in [\"albert-base-v1\", \"albert-large-v1\", \"albert-xlarge-v1\"]:\n            encoded_input = tokenizer.encode_plus(question, answer, add_special_tokens=True, return_offsets_mapping=False)        \n        else:\n            encoded_input = tokenizer.encode_plus(question, answer, add_special_tokens=True, return_offsets_mapping=True)\n            \n        print(\"Question: \" + question + ', Answer: ' + answer)\n        print(\"Encoded Input: \" + str(encoded_input['input_ids']))\n        print(\"Attention Mask: \" + str(encoded_input['attention_mask']))\n        print(\"Offset: \" + str(encoded_input['offset_mapping']))\n        print(\"Token Type Ids: \" + str(encoded_input['token_type_ids']))\n    except:\n        pass","6cd62f53":"model = AutoModel.from_pretrained(model_name)","80630dbc":"for model_ in sanitycheck_model_names:\n    print(\"\\nModel: \" + model_)\n    try:\n        model = AutoModel.from_pretrained(model_)\n        print(\"Model successfully loaded!\")\n        del model\n    except:\n        pass","1b7d6df7":"Some excellent examples are available here: https:\/\/huggingface.co\/transformers\/usage.html\n\nIf you think I missed something, please let me know in the comment so that I can add in the future.\n\n# NEXT: Do your own transfer learning! Thank you!\n","a2fda948":"## Import Packages","068003d6":"## <span style=\"color:green\">Define Tokenizer<\/span> \n```AutoTokenizer```\n\n---\n\n***Note***: \n\n1. To find how to estimate \"offset\" in auto settings, I had to dig a lot. It was recently added as a part of PreTrainedTokenizerFirst (https:\/\/github.com\/huggingface\/transformers\/pull\/2674). Please look at this thread for more details.\n\n2. Offset estimation is not implemented in ```albert``` models. Therefore, you have to set ```return_offsets_mapping=False``` for ```albert``` models\n\n3. RoBERTa does not have a ```token_type_ids```. Remember from their paper that ```RoBERTa``` dropped second sentence prediction from the architecture, so just set all token ids to ```1```!!","da5763c8":"<center> ![image.png](attachment:image.png)","6a799ee0":"## <span style=\"color:green\">Global Variables<\/span>\n\n---\n\n### Available Models\n\n**Disclaimer**: Most popular models are compatible, but not all. Just change the ```model_name``` to check if it is.\n\n```\nbert-base-uncased\nbert-large-uncased\nbert-base-cased\nbert-large-cased\nbert-base-multilingual-uncased\nbert-base-multilingual-cased\nbert-base-chinese\nbert-base-german-cased\nbert-large-uncased-whole-word-masking\nbert-large-cased-whole-word-masking\nbert-large-uncased-whole-word-masking-finetuned-squad\nbert-large-cased-whole-word-masking-finetuned-squad\nbert-base-cased-finetuned-mrpc\nbert-base-german-dbmdz-cased\nbert-base-german-dbmdz-uncased\nbert-base-japanese\nbert-base-japanese-whole-word-masking\nbert-base-japanese-char\nbert-base-japanese-char-whole-word-masking\nbert-base-finnish-cased-v1\nbert-base-finnish-uncased-v1\nbert-base-dutch-cased\nopenai-gpt\ngpt2\ngpt2-medium\ngpt2-large\ngpt2-xl\ntransfo-xl-wt103\nxlnet-base-cased\nxlnet-large-cased\nxlm-mlm-en-2048\nxlm-mlm-ende-1024\nxlm-mlm-enfr-1024\nxlm-mlm-enro-1024\nxlm-mlm-xnli15-1024\nxlm-mlm-tlm-xnli15-1024\nxlm-clm-enfr-1024\nxlm-clm-ende-1024\nxlm-mlm-17-1280\nxlm-mlm-100-1280\nroberta-base\nroberta-large\nroberta-large-mnli\ndistilroberta-base\nroberta-base-openai-detector\nroberta-large-openai-detector\ndistilbert-base-uncased\ndistilbert-base-uncased-distilled-squad\ndistilbert-base-cased\ndistilbert-base-cased-distilled-squad\ndistilgpt2\ndistilbert-base-german-cased\ndistilbert-base-multilingual-cased\nctrl\ncamembert-base\nalbert-base-v1\nalbert-large-v1\nalbert-xlarge-v1\nalbert-xxlarge-v1\nalbert-base-v2\nalbert-large-v2\nalbert-xlarge-v2\nalbert-xxlarge-v2\nt5-small\nt5-base\nt5-large\nt5-3B\nt5-11B\nxlm-roberta-base\nxlm-roberta-large\nflaubert-small-cased\nflaubert-base-uncased\nflaubert-base-cased\nflaubert-large-cased\nbart-large\nbart-large-mnli\nbart-large-cnn\nmbart-large-en-ro\nDialoGPT-small\nDialoGPT-medium\nDialoGPT-large\n\n```\n\n","9db869df":"## Define Model\n\n```AutoModel```\n\n---","d0c0d69b":"## Get Data","4ff2dcc5":"### Sanity Check Tokenizer","ddc1f27a":"# <span style=\"color:orange\"> <center> Hugging Face in AUTO Mode To-The-Rescue! <\/span>\n\n\n\n#### This Notebook shows how we can use Hugging Face auto commands as much as we can to reduce hustle.\n\n### <span style=\"color:green\">Advantages:<\/span>\n* No need to hand coding sequences differently to satisfy the need of tokenizers of different BERT models\n\n* BERT models can be changed just by changing a global ```model_name``` variable and Hugging Face API will take care of the rest\n\n### If this notebook is useful to you please upvote, thank you! \n\nHugging Face Auto modules are actively under development, if something new is added to their library, that I missed, please point it out!"}}