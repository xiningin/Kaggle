{"cell_type":{"8f826281":"code","4e96bd85":"code","5a8251a9":"code","38e9672c":"code","42b59198":"code","30daf252":"code","c188ff3e":"code","a395485f":"code","d55e31f8":"code","20567b7d":"code","f9265a4d":"code","51f30b4d":"code","3daf2f0c":"code","5f5c58e2":"code","624cfcee":"code","330d9a21":"code","d880ac83":"code","6d8488d9":"code","12ce939f":"code","9a7b660f":"code","88fb7e4b":"code","76c30c98":"code","c171038b":"code","ab55ceef":"code","3fb33331":"code","9fc432de":"code","88bf7ae3":"code","54e80eb0":"code","ac8977d0":"code","ccfebb85":"code","8cd57578":"markdown","ddd3d69b":"markdown","330d2267":"markdown","fc022b47":"markdown","8e8bd76a":"markdown","f506b0cd":"markdown","7e0a825d":"markdown","18de43a0":"markdown","daad7ef8":"markdown","28911543":"markdown","60017104":"markdown","c7bc7d00":"markdown","ce207a77":"markdown","6bcdb607":"markdown","c5e85787":"markdown","d1672c8b":"markdown","5304ea72":"markdown","dc6ed1ed":"markdown","75c594c3":"markdown","9d60ac56":"markdown","2aa8058c":"markdown","b2805724":"markdown"},"source":{"8f826281":"import numpy as np\nimport pandas as pd\npd.options.display.max_rows = 200\npd.options.display.max_columns = 200\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.cluster import KMeans\nimport lightgbm as lgb\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nSEED = 42","4e96bd85":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_train.name = 'Training Set'\ndf_test = pd.read_csv('..\/input\/test.csv')\ndf_test.name = 'Test Set'\n\nprint('Number of Training Examples = {}'.format(df_train.shape[0]))\nprint('Number of Test Examples = {}'.format(df_test.shape[0]))\nprint('Training X Shape = {}'.format(df_train.shape))\nprint('Training y Shape = {}'.format(df_train['target'].shape[0]))\nprint('Test X Shape = {}'.format(df_test.shape))\nprint('Test y Shape = {}'.format(df_test.shape[0]))\nprint(df_train.columns)\nprint(df_test.columns)","5a8251a9":"print(df_train.info())\ndf_train.sample(5)","38e9672c":"print(df_test.info())\ndf_test.sample(5)","42b59198":"ones = df_train['target'].value_counts()[1]\nzeros = df_train['target'].value_counts()[0]\nones_per = ones \/ df_train.shape[0] * 100\nzeros_per = zeros \/ df_train.shape[0] * 100\n\nprint('{} out of {} rows are Class 1 and it is the {:.2f}% of the dataset.'.format(ones, df_train.shape[0], ones_per))\nprint('{} out of {} rows are Class 0 and it is the {:.2f}% of the dataset.'.format(zeros, df_train.shape[0], zeros_per))\n\nplt.figure(figsize=(8, 6))\nsns.countplot(df_train['target'])\n\nplt.xlabel('Target')\nplt.xticks((0, 1), ['Class 0 ({0:.2f}%)'.format(ones_per), 'Class 1 ({0:.2f}%)'.format(zeros_per)])\nplt.ylabel('Count')\nplt.title('Training Set Target Distribution')\n\nplt.show()","30daf252":"df_train_corr = df_train.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ndf_train_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_train_corr.drop(df_train_corr.iloc[1::2].index, inplace=True)\ndf_train_corr_nd = df_train_corr.drop(df_train_corr[df_train_corr['Correlation Coefficient'] == 1.0].index)\n\ndf_test_corr = df_test.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ndf_test_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_test_corr.drop(df_test_corr.iloc[1::2].index, inplace=True)\ndf_test_corr_nd = df_test_corr.drop(df_test_corr[df_test_corr['Correlation Coefficient'] == 1.0].index)","c188ff3e":"# Top 5 Highest Correlations in the Training Set\ndf_train_corr_nd.tail()","a395485f":"# Top 5 Highest Correlations between variables in the Training Set\ndf_train_corr_nd[np.logical_and(df_train_corr_nd['Feature 1'] != 'target', df_train_corr_nd['Feature 2'] != 'target')].tail()","d55e31f8":"# Top 5 Highest Correlations in the Test Set\ndf_test_corr_nd.tail()","20567b7d":"df_train_unique = df_train.agg(['nunique']).transpose().sort_values(by='nunique')\ndf_test_unique = df_test.agg(['nunique']).transpose().sort_values(by='nunique')\ndf_uniques = df_train_unique.drop('target').reset_index().merge(df_test_unique.reset_index(), how='left', right_index=True, left_index=True)\ndf_uniques.drop(columns=['index_y'], inplace=True)\ndf_uniques.columns = ['Feature', 'Training Set Unique Count', 'Test Set Unique Count']","f9265a4d":"fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(24, 12))\n\nsns.barplot(x=df_train_unique.index[1:6], y=\"nunique\", data=df_train_unique[1:].head(), ax=axs[0][0])\nsns.barplot(x=df_test_unique.index[:5], y=\"nunique\", data=df_test_unique.head(), ax=axs[0][1])\nsns.barplot(x=df_train_unique.index[-6:-1], y=\"nunique\", data=df_train_unique[-6:-1].tail(), ax=axs[1][0])\nsns.barplot(x=df_test_unique.index[-6:-1], y=\"nunique\", data=df_test_unique[-6:-1].tail(), ax=axs[1][1])\n\nfor i in range(2):\n    for j in range(2):        \n        axs[i][j].set(xlabel='Features', ylabel='Unique Count')\n        \naxs[0][0].set_title('Training Set Features with Least Unique Values')\naxs[0][1].set_title('Test Set Features with Least Unique Values')\naxs[1][0].set_title('Training Set Features with Most Unique Values')\naxs[1][1].set_title('Test Set Features with Most Unique Values')\n\nplt.show()","51f30b4d":"df_qdist = pd.DataFrame(np.zeros((200, 9)), columns=['Quartile 1 Positives', 'Quartile 2 Positives', 'Quartile 3 Positives', 'Quartile 4 Positives',\n                                                     'Quartile 1 Positive Percentage', 'Quartile 2 Positive Percentage', 'Quartile 3 Positive Percentage', 'Quartile 4 Positive Percentage',\n                                                     'Quartile Order'])\nfeatures = [col for col in df_train.columns.values.tolist() if col.startswith('var')]\nquartiles = np.arange(0, 1, 0.25)\ndf_qdist.index = features\n\nfor i, feature in enumerate(features):\n    for j, quartile in enumerate(quartiles):\n        target_counts = df_train[np.logical_and(df_train[feature] >= df_train[feature].quantile(q=quartile), \n                                                df_train[feature] < df_train[feature].quantile(q=quartile + 0.25))].target.value_counts()\n        \n        ones_per = target_counts[1] \/ (target_counts[0] + target_counts[1]) * 100\n        df_qdist.iloc[i, j] = target_counts[1]\n        df_qdist.iloc[i, j + 4] = ones_per\n\npers = df_qdist.columns.tolist()[4:-1]         \n        \nfor i, index in enumerate(df_qdist.index):\n    order = df_qdist[pers].iloc[[i]].sort_values(by=index, ascending=False, axis=1).columns\n    order_str = ''.join([col[9] for col in order])\n    df_qdist.iloc[i, 8] = order_str        \n                \ndf_qdist = df_qdist.round(2)\ndf_qdist.head(10)","3daf2f0c":"# 5 features that doesn't have highest positive target percentage in 1st and 4th quartiles\ndf_qdist[np.logical_or(df_qdist['Quartile Order'].str.startswith('2'), df_qdist['Quartile Order'].str.startswith('3'))] ","5f5c58e2":"for i, col in enumerate(pers):    \n    print('There are {} features that have the highest positive target percentage in Quartile {}'.format(df_qdist[df_qdist['Quartile Order'].str.startswith(str(i + 1))].count()[0],\n                                                                                                            i + 1))\n    print('Quartile {} max positive target percentage = {}% ({})'.format(i + 1, df_qdist[col].max(), df_qdist[col].argmax()))\n    print('Quartile {} min positive target percentage = {}% ({})\\n'.format(i + 1, df_qdist[col].min(), df_qdist[col].argmin()))","624cfcee":"features = [col for col in df_train.columns.tolist() if col.startswith('var')]\n\nnrows = 50\nfig, axs = plt.subplots(nrows=50, ncols=4, figsize=(24, nrows * 5))\n\nfor i, feature in enumerate(features, 1):\n    plt.subplot(50, 4, i)\n    sns.kdeplot(df_train[feature], bw='silverman', label='Training Set', shade=True)\n    sns.kdeplot(df_test[feature], bw='silverman', label='Test Set', shade=True)\n    \n    plt.tick_params(axis='x', which='major', labelsize=8)\n    plt.tick_params(axis='y', which='major', labelsize=8)\n    \n    plt.legend(loc='upper right')\n    plt.title('Distribution of {} in Training and Test Set'.format(feature))\n    \nplt.show()","330d9a21":"features = [col for col in df_train.columns.tolist() if col.startswith('var')]\n\nnrows = 50\nfig, axs = plt.subplots(nrows=50, ncols=4, figsize=(24, nrows * 5))\n\nfor i, feature in enumerate(features, 1):\n    plt.subplot(50, 4, i)\n    \n    sns.distplot(StandardScaler().fit_transform(df_train[df_train['target'] == 0][feature].values.reshape(-1, 1)), label='Target=0', hist=True, color='#e74c3c')\n    sns.distplot(StandardScaler().fit_transform(df_train[df_train['target'] == 1][feature].values.reshape(-1, 1)), label='Target=1', hist=True, color='#2ecc71')\n    \n    plt.tick_params(axis='x', which='major', labelsize=8)\n    plt.tick_params(axis='y', which='major', labelsize=8)\n    \n    plt.legend(loc='upper right')\n    plt.xlabel('')\n    plt.title('Distribution of Target in {}'.format(feature))\n    \nplt.show()","d880ac83":"test = df_test.drop(['ID_code'], axis=1).values\n\nunique_count = np.zeros_like(test)\n\nfor feature in range(test.shape[1]):\n    _, index, count = np.unique(test[:, feature], return_counts=True, return_index=True)\n    unique_count[index[count == 1], feature] += 1\n    \nreal_samples = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\nsynth_samples = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n\nprint('Number of real samples in test set is {}'.format(len(real_samples)))\nprint('Number of synthetic samples in test set is {}'.format(len(synth_samples)))","6d8488d9":"features = [col for col in df_train.columns if col.startswith('var')]\ndf_all = pd.concat([df_train, df_test.ix[real_samples]])\n\nfor feature in features:\n    temp = df_all[feature].value_counts(dropna=True)\n\n    df_train[feature + 'vc'] = df_train[feature].map(temp).map(lambda x: min(10, x)).astype(np.uint8)\n    df_test[feature + 'vc'] = df_test[feature].map(temp).map(lambda x: min(10, x)).astype(np.uint8)\n\n    df_train[feature + 'sum'] = ((df_train[feature] - df_all[feature].mean()) * df_train[feature + 'vc'].map(lambda x: int(x > 1))).astype(np.float32)\n    df_test[feature + 'sum'] = ((df_test[feature] - df_all[feature].mean()) * df_test[feature + 'vc'].map(lambda x: int(x > 1))).astype(np.float32) \n\n    df_train[feature + 'sum2'] = ((df_train[feature]) * df_train[feature + 'vc'].map(lambda x: int(x > 2))).astype(np.float32)\n    df_test[feature + 'sum2'] = ((df_test[feature]) * df_test[feature + 'vc'].map(lambda x: int(x > 2))).astype(np.float32)\n\n    df_train[feature + 'sum3'] = ((df_train[feature]) * df_train[feature + 'vc'].map(lambda x: int(x > 4))).astype(np.float32) \n    df_test[feature + 'sum3'] = ((df_test[feature]) * df_test[feature + 'vc'].map(lambda x: int(x > 4))).astype(np.float32)\n    \nprint('Training set shape after creating magic features: {}'.format(df_train.shape))\nprint('Test set shape after creating magic features: {}'.format(df_test.shape))","12ce939f":"def augment(x, y, t=2):\n    \n    xs, xn = [], []\n    \n    for i in range(t \/\/ 2):\n        mask = y == 0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        featnum = x1.shape[1] \/\/ 200 - 1\n\n        for c in range(200):\n            np.random.shuffle(ids)\n            x1[:, [c] + [200 + featnum * c + idc for idc in range(featnum)]] = x1[ids][:, [c] + [200 + featnum * c + idc for idc in range(featnum)]]\n        xn.append(x1)\n    \n    for i in range(t):\n        mask = y > 0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        featnum = x1.shape[1] \/\/ 200 - 1\n        \n        for c in range(200):\n            np.random.shuffle(ids)\n            x1[:, [c] + [200 + featnum * c + idc for idc in range(1)]] = x1[ids][:, [c] + [200 + featnum * c + idc for idc in range(1)]]\n        xs.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x, xs, xn])\n    y = np.concatenate([y, ys, yn])\n    \n    return x, y","9a7b660f":"\"\"\"\ndef get_quartile_mask(df, feature, q):\n    \n    assert feature in df.columns\n    \n    # Returns a boolean mask of the given features' quartile\n    if q==1:\n        return np.logical_and(df[feature] >= df[feature].quantile(q=0), df[feature] < df[feature].quantile(q=0.25))\n    elif q==2:\n        return np.logical_and(df[feature] >= df[feature].quantile(q=0.25), df[feature] < df[feature].quantile(q=0.5))\n    elif q==3:\n        return np.logical_and(df[feature] >= df[feature].quantile(q=0.5), df[feature] < df[feature].quantile(q=0.75))\n    elif q==4:\n        return np.logical_and(df[feature] >= df[feature].quantile(q=0.75), df[feature] <= df[feature].quantile(q=1))\n    else:\n        return -1      \n    \nfor df in [df_train, df_test]:\n    df['quartile_rank'] = 0\n\n# Ranking every cell by their quartile\nfor df in [df_train, df_test]:\n    for col in variables:\n        col_rank = df_qdist.loc[col, 'order']\n        for i in range(1, 5):\n            q_ind = df[get_quartile_mask(df, col, i)].index\n            df.loc[q_ind, 'quartile_rank'] += col_rank[::-1].find(str(i)) + 1      \n            \ndf_train['quartile_rank'] = MinMaxScaler().fit_transform(df_train['quartile_rank'].values.reshape(-1, 1))\ndf_test['quartile_rank'] = MinMaxScaler().fit_transform(df_test['quartile_rank'].values.reshape(-1, 1))\n\"\"\"","88fb7e4b":"def smooth_mean(df, by, on, weight):\n    \n    global_mean = df[on].mean()\n    \n    agg = df.groupby(by)[on].agg(['count', 'mean'])\n    counts = agg['count']\n    means = agg['mean']\n\n    smooth = (counts * means + weight * global_mean) \/ (counts + weight)\n\n    return df[by].map(smooth)\n\n\"\"\"\nfor df in [df_train, df_test]:\n    df['var_68'] = smooth_mean(df, 'var_68', 'var_81', 10)\n\"\"\"","76c30c98":"class KMeansFeaturizer:\n\n    def __init__(self, k, target_scale=5.0, random_state=None):\n        self.k = k\n        self.target_scale = target_scale\n        self.random_state = random_state\n        self.encoder = OneHotEncoder(categories='auto').fit(np.array(range(k)).reshape(-1, 1))\n\n    def fit(self, X, y=None):\n        if y is None:\n            kmeans = KMeans(n_clusters=self.k, n_init=20, random_state=self.random_state)\n            kmeans.fit(X)\n            self.kmeans = kmeans\n            self.cluster_centers_ = kmeans.cluster_centers_\n        else:\n            Xy = np.hstack((X, y[:, np.newaxis] * self.target_scale))\n            kmeans_pretrain = KMeans(n_clusters=self.k, n_init=20, random_state=self.random_state)\n            kmeans_pretrain.fit(Xy)\n\n            kmeans = KMeans(n_clusters=self.k, init=kmeans_pretrain.cluster_centers_[:, :2], n_init=1, max_iter=1)\n            kmeans.fit(X)\n\n            self.kmeans = kmeans\n            self.cluster_centers_ = km_model.cluster_centers_\n            \n        return self\n\n    def transform(self, X, y=None):\n        clusters = self.kmeans.predict(X)\n        return self.encoder.transform(clusters.reshape(-1, 1))\n\n    def fit_transform(self, X, y=None):\n        self.fit(X, y)\n        return self.transform(X, y)\n    \n\"\"\"\ntrain = df_train_orig.drop(columns=['ID_code', 'target'])\ntest = df_test_orig.drop(columns=['ID_code'])\nall = pd.concat([train, test])\n\nks = [2, 5, 10, 25, 50]\nfor k in ks:\n    kmf = KMeansFeaturizer(k=k, random_state=SEED)\n    k_features = kmf.fit_transform(all)\n    \n    k_features_df = pd.DataFrame(k_features.toarray())\n    k_features_df = k_features_df.add_prefix('k{}_group'.format(k))\n    all = pd.concat([all, k_features_df], axis=1)\n\"\"\"","c171038b":"def transform_feature(df, feature, transformation, **transform_params):\n    \n    fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(16, 12))  \n    plt.subplots_adjust(right=1.5)\n       \n    sns.distplot(df[df['target'] == 0][feature].values, label='Target=0', color='blue', ax=axs[0][0])\n    sns.distplot(df[df['target'] == 1][feature].values, label='Target=1', color='red', ax=axs[0][0])\n    axs[0][0].set_title('{} Target Distribution in Training Set'.format(feature))\n    axs[0][0].legend()\n    \n    sns.distplot(transformation(df[df['target'] == 0][feature].values, **transform_params), label='Target=0', color='blue', ax=axs[0][1])\n    sns.distplot(transformation(df[df['target'] == 1][feature].values, **transform_params), label='Target=1', color='red', ax=axs[0][1])\n    axs[0][1].set_title('{} Target Distribution After Applying {} Function '.format(feature, transformation.__name__))\n    axs[0][1].legend()\n    \n    sns.distplot(df[feature].values, label='Training Set', hist=False, color='grey', ax=axs[1][0])\n    sns.distplot(df_test[feature].values, label='Test Set', hist=False, color='magenta', ax=axs[1][0])\n    axs[1][0].set_title('{} Distribution in Training and Test Set'.format(feature))\n    axs[1][0].legend()\n    \n    sns.distplot(transformation(df[feature].values, **transform_params), label='Training Set', hist=False, color='grey', ax=axs[1][1])\n    sns.distplot(transformation(df_test[feature].values, **transform_params), label='Test Set', hist=False, color='magenta', ax=axs[1][1])\n    axs[1][1].set_title('{} Distribution in Training and Test Set After Applying {} Function'.format(feature, transformation.__name__))\n    axs[1][1].legend()\n    \n    plt.show()","ab55ceef":"transform_feature(df=df_train, feature='var_108', transformation=np.round, decimals=2)","3fb33331":"gbdt_param = {\n    # Core Parameters\n    'objective': 'binary',\n    'boosting': 'gbdt',\n    'learning_rate': 0.01,\n    'num_leaves': 15,\n    'tree_learner': 'serial',\n    'num_threads': 8,\n    'seed': SEED,\n    \n    # Learning Control Parameters\n    'max_depth': -1,\n    'min_data_in_leaf': 50,\n    'min_sum_hessian_in_leaf': 10,  \n    'bagging_fraction': 0.6,\n    'bagging_freq': 5,\n    'feature_fraction': 0.05,\n    'lambda_l1': 1.,\n    'bagging_seed': SEED,\n    \n    # Others\n    'verbosity ': 1,\n    'boost_from_average': False,\n    'metric': 'auc',\n}","9fc432de":"predictors = df_train.columns.tolist()[2:]\nX_test = df_test[predictors]\n\nn_splits = 5\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n\noof = df_train[['ID_code', 'target']]\noof['predict'] = 0\npredictions = df_test[['ID_code']]\nval_aucs = []\nfeature_importance_df = pd.DataFrame()","88bf7ae3":"for fold, (train_ind, val_ind) in enumerate(skf.split(df_train, df_train.target.values)):\n    \n    X_train, y_train = df_train.iloc[train_ind][predictors], df_train.iloc[train_ind]['target']\n    X_valid, y_valid = df_train.iloc[val_ind][predictors], df_train.iloc[val_ind]['target']\n\n    N = 1\n    p_valid, yp = 0, 0\n        \n    for i in range(N):\n        print('\\nFold {} - N {}'.format(fold + 1, i + 1))\n        \n        X_t, y_t = augment(X_train.values, y_train.values)\n        weights = np.array([0.8] * X_t.shape[0])\n        weights[:X_train.shape[0]] = 1.0\n        print('Shape of X_train after augment: {}\\nShape of y_train after augment: {}'.format(X_t.shape, y_t.shape))\n        \n        X_t = pd.DataFrame(X_t)\n        X_t = X_t.add_prefix('var_')\n    \n        trn_data = lgb.Dataset(X_t, label=y_t, weight=weights)\n        val_data = lgb.Dataset(X_valid, label=y_valid)\n        evals_result = {}\n        \n        lgb_clf = lgb.train(gbdt_param, trn_data, 100000, valid_sets=[trn_data, val_data], early_stopping_rounds=5000, verbose_eval=1000, evals_result=evals_result)\n        p_valid += lgb_clf.predict(X_valid)\n        yp += lgb_clf.predict(X_test)\n        \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = predictors\n    fold_importance_df[\"importance\"] = lgb_clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    oof['predict'][val_ind] = p_valid \/ N\n    val_score = roc_auc_score(y_valid, p_valid)\n    val_aucs.append(val_score)\n    \n    predictions['fold{}'.format(fold + 1)] = yp \/ N\n","54e80eb0":"mean_auc = np.mean(val_aucs)\nstd_auc = np.std(val_aucs)\nall_auc = roc_auc_score(oof['target'], oof['predict'])\n\nprint('Mean AUC: {}, std: {}.\\nAll AUC: {}.'.format(mean_auc, std_auc, all_auc))","ac8977d0":"cols = (feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:1000].index)\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(15, 150))\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\nplt.title('LightGBM Features (Averaged over Folds)')\nplt.show()","ccfebb85":"predictions['target'] = np.mean(predictions[[col for col in predictions.columns if col not in ['ID_code', 'target']]].values, axis=1)\npredictions.to_csv('predictions.csv', index=None)\nsub_df = pd.DataFrame({\"ID_code\":df_test[\"ID_code\"].values})\nsub_df[\"target\"] = predictions['target']\nsub_df.to_csv(\"lgb_submission.csv\", index=False)\noof.to_csv('lgb_oof.csv', index=False)","8cd57578":"### **2.4 Target Encoding (Not Used)**\nThis function is for averaging the target value by feature. It computes the number of values and mean of each group. After that, the smooth mean is computed and replaced with the feature. Target encoding should be used in the folds otherwise it leaks data.","ddd3d69b":"## **0. Introduction**","330d2267":"### **1.5 Target Distribution in Quartiles**\nClass 1 `target` distribution in feature quartiles are quite similar for each feature. Most of the class 1 `target` rows are either in the **1st** quartile or in the **4th** quartile of the features because of the winsorization. Winsorization clips the extreme values, so they are grouped up in the spikes inside **1st** quartile and **4th** quartile.\n* **94** features have highest class 1 `target` percentage in **1st** quartile\n* **101** features have highest class 1 `target` percentage in **4th** quartile\n* Only **5** features have highest class 1 `target` percetange in **2nd** and **3rd** quartile, and those features are `var_17`, `var_30`, `var_100`, `var_101`, `var_105`\n\nMaximum class 1 `target` percentage for **1st** quartile is **14.35%** (**85.65%** class 0), and for **4th** quartile is **13.43%** (**86.57%** class 0). Maximum class 1 `target` percentage for **2nd** quartile is **10.34%** (**89.66%** class 0), and for **3rd** quartile is **10.05%** (**89.95%** class 0 `target`). To conclude, values in **1st** and **4th** quartiles have higher chance (**3-4%**) to be class 1 than values in **2nd** and **3rd** quartile for 195 features.","fc022b47":"## **1. Exploratory Data Analysis**","8e8bd76a":"### **1.4 Unique Value Count**\nThe lowest unique value count belongs to `var_68` which has only **451** unique values in training set and **428** unique values in test set. **451** and **428** unique values in **200000** rows are too less that `var_68` could even be a categorical feature. The highest unique value count belongs to`var_45` which has **169968** unique values in the training set and **92058** unique values in the test set. Every feature in training set have higher unique value counts compared to features in test set.\n\nThe lowest unique value count difference is in the `var_68` feature (Training Set Unique Count **451**, Test Set Unique Count **428**). The highest unique value count difference is in the `var_45` feature (Training Set Unique Count **169968**, Test Set Unique Count **92058**). When the unique value count of a feature increases, the difference between training set unique value count and test set unique value count also increases. The explanation of this situation is probably the synthetic records in the test set. ","f506b0cd":"### **2.1 Separating Real\/Synthetic Test Data and Magic Features**\nUsing unique value count in a row to identify synthetic samples. If a row has at least one unique value in a feature, then it is real, otherwise it is synthetic. This technique is shared by [YaG320](https:\/\/www.kaggle.com\/yag320) in this kernel [List of Fake Samples and Public\/Private LB split](https:\/\/www.kaggle.com\/yag320\/list-of-fake-samples-and-public-private-lb-split) and it successfuly identifies synthetic samples in entire test set. This way the unusual bumps on the distribution peaks of test set features are captured. The magic features are extracted from the combination of training set and real samples in the test set. ","7e0a825d":"## **3. Model**","18de43a0":"### **3.2 ROC-AUC Score**","daad7ef8":"## **2. Feature Engineering and Data Augmentation**","28911543":"### **1.8 Conclusion**\nData imbalance is very common in customer datasets like this. Oversampling **Class 1** or undersampling **Class 0** are suitable solutions for this dataset because of its large size. Since the dataset is big enough, resampling would not introduce underfitting.\n\nTraining set has more unique values than test set so some part of test set is most likely synthetic. Rows with more frequent values are less reliable because test set has bumps over distribution peaks. This is also related to synthetic data in test set.\n\nFeatures are not correlated with each other or not dependent to each other. However, `target` feature has the highest correlation with `var_81` (**0.08**). This relationship can bu used to make other features more informative. If a feature is target encoded on `var_81`, it could give information about `target`.\n\nValues in **1st** and **4th** quartiles have higher chance to be **Class 1** than values in **2nd** and **3rd** quartile for almost every feature because of winsorization.","60017104":"### **2.3 Quartile Rank (Not Used)**\nThis code ranks every value by their quartile. Ranking is done according to the features' **Class 1** distribution percentage in a quartile. In order to do that, every features' quartiles are sorted by **Class 1** percentage. After that, the ranks **(4, 3, 2, 1)** are mapped to the sorted quartiles. This way, the quartile with the highest **Class 1** distribution in a feature gets the highest rank. After every value in a row are ranked,the ranks are summed and scaled. This way the mean rank of a row is calculated. The problems with this feature are:\n* The distributions are already captured by decision trees, so this feature is not very useful in LightGBM\n* If this feature is computed outside the folds, it leaks data","c7bc7d00":"### **2.2 Data Augmentation**\nOversampling the data increases CV and LB score significantly since the data is imbalanced. This oversampling technique is shared by [Jiwei Liu](https:\/\/www.kaggle.com\/jiweiliu) in this kernel [LGB 2 leaves + augment](https:\/\/www.kaggle.com\/jiweiliu\/lgb-2-leaves-augment).","ce207a77":"### **1.1 Overview**\n* Both training set and test set have **200000** rows\n* Training set have **202** features and test set have **201** features\n* One extra feature in the training set is `target` feature, which is the class of a row\n* `target` feature is binary (**0** or **1**), **1 = transaction** and **0 = no transaction**\n* `ID_code` feature is the unique id of the row and it doesn't have any effect on target\n* The other features are anonymized and labeled from `var_0` to `var_199`\n* There are no missing values in both training set and test set because the dataset is already processed","6bcdb607":"### **3.1 LightGBM**","c5e85787":"### **1.6 Feature Distributions in Training and Test Set**\nTraining and test set distributions of features are not perfectly identical. There are bumps on the distribution peaks of test set because the unique value counts are lesser than training set. Distribution tails are smoother than peaks and spikes are present in both training and test set.","d1672c8b":"### **2.5 KMeansFeaturizer (Not Used)**\n`KMeansFeaturizer` is a pipeline of scikit-learn `KMeans` and `OneHotEncoder`. First, the records are grouped into **k** groups by `KMeans` with or without `target`. A return object of an $m * n$ matrix is $m * k$ group matrix which can be added to the previous matrix as features. This can be used to add likelihood features.\n* In order to make these features reliable, `KMeans` should be initialized with different seeds with many times and then blended\n* The information gain from this approach doesn't worth it because it adds lot of new features to the dataset and takes too much time","5304ea72":"### **1.2 Target Distribution**\n* **10.05%** (20098\/200000) of the training set is **Class 1**\n* **89.95%** (179902\/200000) of the training set is **Class 0**","dc6ed1ed":"### **1.3 Correlations**\nFeatures from `var_0` to `var_199` have extremely low correlation between each other in both training set and test set. The lowest correlation between variables is **2.7e-8** and it is in the training set (between `var_191` and `var_75`). The highest correlation between variables is **0.00986** and it is in the test set (between `var_139` and `var_75`). `target` has slightly higher correlations with other features. The highest correlation between a feature and `target` is **0.08** (between `var_81` and `target`).","75c594c3":"### **3.3 Feature Importance**","9d60ac56":"### **1.7 Target Distributions in Features**\nMajority of the features have good split points and huge spikes. This explains why a simple LightGBM model can achieve 0.90 AUC. Distribution difference is bigger in tails because of winsorization.","2aa8058c":"### **3.4 Submission**","b2805724":"### **2.6 Feature Transformation (Not Used)**\nThis function is for simulating feature transformations. The transformation objective is to increase information gain by decreasing the overlapping area in the target distribution. By decreasing the overlapping area, LightGBM decision trees are able to make better splits. A transformed feature can be added to the data set as a new feature or it can replace the old one depending on the model's performance. A new feature can also be combinations of transformations and interactions between other features."}}