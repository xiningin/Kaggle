{"cell_type":{"9bf480e8":"code","1b8ac26b":"code","44db9051":"code","c50a0636":"code","87759c30":"code","3c80d4ba":"code","5def8c55":"code","441bae0a":"code","5afb17bd":"markdown","5e46b79f":"markdown","33ac5646":"markdown","a89b154b":"markdown","bb43f9ec":"markdown","17630bea":"markdown","c5d51aa2":"markdown","02c02096":"markdown"},"source":{"9bf480e8":"# task 1: What do we know about COVID-19 risk factors?\n# Task details: What do we know about COVID-19 risk factors? What have we learned from epidemiological studies?\n# https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/tasks?taskId=558\n\nquestions = [{'question':\"Is smoking a risk factor?\",'keyword':None},\n             {'question':\"Is a pre-existing pulmonary disease a risk factor?\",'keyword':None},\n             {'question':\"Do co-existing conditions make the virus more transmissible?\",'keyword':None},\n             {'question':\"Is being a pregnant woman a risk factor?\",'keyword':'pregnant'},\n             {'question':\"Is being a neonate a risk factor?\",'keyword':'neonate'},\n             {'question':\"Are there differences in risk factors associated to socio-economic factors?\",'keyword':None},\n             {'question':\"How does the transmission happen?\",'keyword':'transmission'},\n             {'question':\"What is the reproductive rate?\",'keyword':None},\n             {'question':\"What is the incubation period?\",'keyword':None},\n             {'question':\"What are the modes of transmission?\",'keyword':None},\n             {'question':\"What are the enviromental factors?\",'keyword':None},\n             {'question':\"How long is the serial interval?\",'keyword':None},\n             {'question':\"What is the severity of disease among high risk groups and patients?\",'keyword':None},\n             {'question':\"What is the risk of death among high risk groups and patients?\",'keyword':None},\n             {'question':\"What is the susceptibility of populations?\",'keyword':None},\n             {'question':\"What are the public health mitigation measures that could be effective for control?\",'keyword':None}]","1b8ac26b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","44db9051":"!pip install transformers\nimport torch\nimport pandas as pd\nfrom transformers import BertForQuestionAnswering\nfrom transformers import BertTokenizer\n#device_available = torch.cuda.is_available()\ndevice_available = False\nfrom IPython.core.display import display, HTML\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Use plot styling from seaborn.\nsns.set(style='darkgrid')\n\n# Increase the plot size and font size.\n#sns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (20,8)\n\nmodel = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\nif device_available:\n    model.cuda()\n\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')","c50a0636":"# this is just to load the files needed instead of running the model each time.\n\nimport pickle\n\ndef load_or_run_answer_question_dict(question,keyword):\n    pickle_name = question.replace(' ','_').replace('?','_')\n    path_to_file = F\"\/kaggle\/input\/{pickle_name}.pickle\"\n    print(path_to_file)\n    try:\n      df = pickle.load(open(path_to_file, \"rb\"))\n    except (OSError, IOError) as e:\n        df = answer_question_dict(question, keyword)\n        pickle.dump(df, open(path_to_file, \"wb\"))\n    return df\n# print(os.listdir(\"..\/input\"))\n# print(os.listdir(\"..\/input\/datacompetition\"))","87759c30":"import textwrap\n\ndef get_dataset(csv_path):\n    corpus = []\n    csv_df = pd.read_csv(csv_path).dropna(subset=['authors', 'abstract']).drop_duplicates(subset='abstract')\n    csv_df = csv_df[csv_df['abstract']!='Unknown']\n    for ix,row in csv_df.iterrows():\n        if row['abstract'] and not pd.isna(row['abstract']):\n            temp_dict = dict()\n            temp_dict['abstract'] = row['abstract']\n            temp_dict['title'] = row['title']\n            temp_dict['authors'] = row['authors']\n            temp_dict['url'] = row['doi']\n            temp_dict['publish_time'] = row['publish_time']\n            corpus.append(temp_dict)\n    return corpus\n\nwrapper = textwrap.TextWrapper(width=80) \n\ncorpus = get_dataset('https:\/\/ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com\/2020-03-27\/metadata.csv')","3c80d4ba":"def answer_question_dict(question, keyword=None, show_visualization=False):\n\n    '''\n    Takes a `question` string and an `answer_text` string (which contains the\n    answer), and identifies the words within the `answer_text` that are the\n    answer. Prints them out.\n    '''\n    # select corpus\n    answer_text = corpus\n\n    # Initializing answers list\n    answers = {}\n    min_score = 0\n    counter = 0 # for stopping iterations earlier\n    \n    for answer_option in answer_text:\n      if keyword and keyword not in answer_option['abstract']:\n        continue\n\n      # ======== Tokenize ========\n      # Apply the tokenizer to the input text, treating them as a text-pair.\n      input_ids = tokenizer.encode(question, answer_option['abstract'],max_length=512)\n\n      # Report how long the input sequence is.\n      #print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n\n      # ======== Set Segment IDs ========\n      # Search the input_ids for the first instance of the `[SEP]` token.\n      sep_index = input_ids.index(tokenizer.sep_token_id)\n\n      # The number of segment A tokens includes the [SEP] token istelf.\n      num_seg_a = sep_index + 1\n\n      # The remainder are segment B.\n      num_seg_b = len(input_ids) - num_seg_a\n\n      # Construct the list of 0s and 1s.\n      segment_ids = [0]*num_seg_a + [1]*num_seg_b\n    \n\n      # There should be a segment_id for every input token.\n      assert len(segment_ids) == len(input_ids)\n      \n\n      # ======== Evaluate ========\n      # Run our example question through the model.\n        \n      input_ids_tensor = torch.tensor([input_ids])\n      segment_ids_tensor = torch.tensor([segment_ids])\n      if device_available:\n         input_ids_tensor = input_ids_tensor.to('cuda:0')\n         segment_ids_tensor = segment_ids_tensor.to('cuda:0')\n\n      start_scores, end_scores = model(input_ids_tensor, # The tokens representing our input text.\n                                  token_type_ids=segment_ids_tensor) # The segment IDs to differentiate question from answer_text\n    \n      # only review answers with score above threshold\n      score = round(torch.max(start_scores).item(), 3)\n\n      if score>min_score and score>0:\n\n        # ======== Reconstruct Answer ========\n        \n        # Find the tokens with the highest `start` and `end` scores.\n        answer_start = torch.argmax(start_scores)\n        answer_end = torch.argmax(end_scores)\n\n\n        # Get the string versions of the input tokens.\n        tokens = tokenizer.convert_ids_to_tokens(input_ids)\n\n        # Start with the first token.\n        answer = tokens[answer_start]\n\n        # Select the remaining answer tokens and join them with whitespace.\n        for i in range(answer_start + 1, answer_end + 1):\n            \n            # If it's a subword token, then recombine it with the previous token.\n            if tokens[i][0:2] == '##':\n                answer += tokens[i][2:]\n            \n            # Otherwise, add a space then the token.\n            else:\n                answer += ' ' + tokens[i]\n\n        # ======== Add Answer to best answers list ========\n\n        if len(answers)>4:\n          min_score = min([d for d in answers.keys()])\n          \n        if len(answers)==10:\n          answers.pop(min_score)\n        answers[score] = [answer, score, '<a href=\"https:\/\/doi.org\/'+str(answer_option['url'])+'\" target=\"_blank\">' + str(answer_option['title']) +'<\/a>', answer_option['abstract'], answer_option['publish_time']]\n        \n        visualization_start = max(answer_start-20,0)\n        visualization_end = min((answer_end+1)+20,len(tokens))\n        # Variables needed for graphs\n        s_scores = start_scores.cpu().detach().numpy().flatten()\n        e_scores = end_scores.cpu().detach().numpy().flatten()\n        \n        # We'll use the tokens as the x-axis labels. In order to do that, they all need\n        # to be unique, so we'll add the token index to the end of each one.\n        token_labels = []\n        for (i, token) in enumerate(tokens):\n            token_labels.append('{:} - {:>2}'.format(token, i))\n        answers[score] = [answer, score, '<a href=\"https:\/\/doi.org\/'+str(answer_option['url'])+'\" target=\"_blank\">' + str(answer_option['title']) +'<\/a>', answer_option['abstract'], answer_option['publish_time'], s_scores, e_scores, token_labels, visualization_start, visualization_end]\n        \n    # Return dataframe with relevant data\n    df_columns = ['Answer', 'Confidence', 'Title', 'Abstract', 'Published', 's_scores', 'e_scores', 'token_labels', 'visualization_start', 'visualization_end']\n    df = pd.DataFrame.from_dict(answers, orient='index',columns = df_columns)\n    df.sort_values(by=['Confidence'], inplace=True, ascending=False)\n    return df","5def8c55":"for question in questions:\n    print(\"======================\")\n    print(question)\n    df = load_or_run_answer_question_dict(question['question'], question['keyword'])\n    display(HTML(df[['Answer', 'Confidence', 'Title', 'Abstract', 'Published']].to_html(render_links=True, escape=False, index=False)))\n    print(\"======================\")","441bae0a":"def start_word_plot(token_labels, s_scores):\n  ax = sns.barplot(x=token_labels, y=s_scores, ci=None)\n  ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n  ax.grid(True)\n  plt.title('Start Word Scores [for first answer]')\n  plt.show()\n\ndef end_word_plot(token_labels,e_scores):\n  ax = sns.barplot(x=token_labels, y=e_scores, ci=None)\n  ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n  ax.grid(True)\n  plt.title('End Word Scores [for first answer]')\n  plt.show()\n\ndf = load_or_run_answer_question_dict(questions[0]['question'], question['keyword'])\nstart_word_plot(df.iloc[0]['token_labels'],df.iloc[0]['s_scores'])\nend_word_plot(df.iloc[0]['token_labels'],df.iloc[0]['e_scores'])","5afb17bd":"### 3.2 Discussion\n\nWe believe our method is a novel approach to this challenge and it allowed to answer to the posed questions in an accurate and clear manner. The code is simple, the results are surfaced and can be used easily even by non-experts without too much effort.\n\nThere are a few gaps in the answers - in particular to susceptible population - this is an interesting point about data gaps because we do know there are publications out there addressing this questions (e.g. https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0896841120300469 ) but they are not in the database and therefore the algorithm did not surface them. Also, a lot of the data is unstructured and not parsed - in particular in the full text form.\n\nAlso, the database is very heterogenous and the results are often relative to old research or other viruses or outbreaks - rhinovirus, Sars etc - presenting the risk of oversimplification and generalization of the findings that the careful reader can identify when looking at the best set of results, but the models (not ours, all of those that we checked) will struggle to grasp. This could lead to really dangerous results and therefore we do think this tools should be used just to rank the findings and surface the possible answers and then - as demonstrated in the Summary above - it'd be very simple for a reader to pick the best answer. This is often based on the full abstract, the title and the year of publication, so we included those elements (as well as a link to the full text) in our results output.\n\nIn terms of models parameters - different scoring methods affect the results of the model. Therefore, we assessed the scores and compared results using two different scoring methods - first using only the starting token score, and then using a composite score based on both start and end tokens. An exemplificative set of tokens scores for the first question is showed in the figures below - we found that by taking the highest scores in a 40 tokens range around the start (green spike in the first figure below) and end (green spike in the second figure below) token gives the best answers, so we implemented that in the question\/answer model.","5e46b79f":"# 3. Results & Discussion\n\n### 3.1 Results in full\n\nHere the questions defined in the sections above are answered and results are presented in full. All answers in the results above come from these results.","33ac5646":"### 3.3 Pros and cons and future work\n\nWe think that our model is accurate and can surface the best answers. The mode is however not fast on a chaep machine, and needs some pre-training on faster machines. Also, it can be largely be improved - our plans for the next submission is to proceed with a split of the answers by outbreak and viruses, as it seems that many papers related to rhinovirus, SARS-CoV-2 or other respiratory pathogen; also we plan to extend the answers model to the full text of the papers, as well as provide more guidance to the doctors on how to pick the best answers in the surfaced set.\n\n# 4. REUSABILITY\n---\nThe code needed to train and run the models is all in this file, as well as the results. To add a question, simply add it to the list of questions in section 2.1, and re-run the remaining of the code. \n\n\n# 5. REFERENCES\n---\n\nScoring and other notes we used: \n\nhttps:\/\/docs.google.com\/spreadsheets\/d\/1PKQVCSBK2Xsuvh_oLbQkejBDI0Jve23hJCeDMkYWu80\/edit?ts=5e7a9121#gid=164381334\n\n\nOther references that we used:\n\nhttps:\/\/arxiv.org\/pdf\/1908.10084.pdf\n\nhttps:\/\/static.googleusercontent.com\/media\/research.google.com\/en\/\/pubs\/archive\/46808.pdf\n\nhttps:\/\/search.carrot2.org\/#\/pubmed\n\nhttps:\/\/public.ukp.informatik.tu-darmstadt.de\/reimers\/sentence-transformers\/v0.2\/\n\nhttps:\/\/arxiv.org\/pdf\/1512.01337.pdf\n\nhttps:\/\/www.aclweb.org\/anthology\/D14-1070.pdf\n\nhttps:\/\/arxiv.org\/abs\/1903.10676\n\nhttps:\/\/arxiv.org\/pdf\/1706.03762.pdf\n\nhttps:\/\/rajpurkar.github.io\/SQuAD-explorer\/\n\nhttps:\/\/arxiv.org\/pdf\/1810.04805.pdf","a89b154b":"## Set up question-answer algorithm\n\nHere we define our main answering function. As you can see, we are scoring every token in every abstract and not just a pre-selected few papers or abstracts like in the other submissions we saw. This allows us to obtain the full database scored, which we then rank and return only the top results. ","bb43f9ec":"### 2.2 Model selection\n\nThen, we looked at various models trained on different datasets - including SBERT [https:\/\/arxiv.org\/pdf\/1908.10084.pdf] and SCIBERT model [https:\/\/github.com\/allenai\/scibert] - and we compared results across the questions proposed. We compared results by manually veriying the top papers picked by each method and scoring them from 1 to 5. Eventually we set for a BERT model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of the hidden-states output to compute span start logits and span end logits). \n\n\n### 2.2 Detailed Methodology (Code)\n\nThe detailed methodology follows, including the code for each step.","17630bea":"# 1. Summary\n\n### 1.1 THE CHALLENGE\n\nThe challenge of choice was \"What do we know about COVID-19 risk factors? What have we learned from epidemiological studies?\". A set of questions were raised in the challenge outline and we set out to find the asnwers to those questions using data analysis. We did not want to lose the focus on what was asked and decided for a clean simple yet very effective approach.\n\n### 1.2 THE APPROACH\n\nOur approach is based on selective models benchmarked in reference to the provided dataset in order to maximise the answering power and keep the results tied to the scope of the challenge. After reviewing a series of references referring to cuttin-edge methods in the field, we compared benchmarks in various papers and decided to find the best BERT model for this purpose. We tried several models, found the best results using a specific BERT model fine tuned for questions-answers.\n\nOther submissions picked this model, but the peculiarity that we found extremely effective was that the other solutions did not apply extensively the model, in particular decided to first filte the papers database by searching keywords and strings, and only apply BERT to find the answer in the reulting filtered data. Our approach flips this logic upside down, since we first identify the best answers across all papers database, and only after filter by keyword where needed.\n\n### 1.3 THE RESULTS\nOur results provide satisfying answers for 7 out of 8 questions. The time taken to extract an answer in is reduced to a less than 30 seconds per question on a faster machine but we decided to pre-calculate them because of the limited power available on Kaggle notebooks. The summary of the results - which can be found in extended form in the relative section - follow:\n\n<table width=\"100%\">\n    <thead>\n    <tr>\n        <td width=\"30%\"  style=\"text-align:left\"> Question <\/td>\n        <td  style=\"text-align:left\"> Answer <\/td>\n    <\/tr>\n    <\/thead>\n    <tbody>\n    <tr>\n        <td style=\"text-align:left\"> Risk factors: Smoking, pre-existing pulmonary disease. <\/td> \n        <td style=\"text-align:left\"> Very likely but not demonstrated. It was demonstrated that smoking has no clinical or preventive significance for risk of influenza in the elderly. However, asthma is to be considered a risk factor for rhinovirus, which in turn often exaplains more underlying pulmonary illness. <\/td>\n    <\/tr>\n    <tr>\n        <td style=\"text-align:left\"> Risk factors: Co-infections (determine whether co-existing respiratory\/viral infections make the virus more transmissible or virulent) and other co-morbidities. <\/td>\n        <td style=\"text-align:left\"> Likely not. It was observed that very often infected individuals with The COVID-19 coronavirus but without any symptoms could still transfer the virus to others . However, for pneumonia in relation to viruses similar to Covid-19, i.e. HCoV-NL63 and influenza A\/H1N1, it was observed that co-infections caused significantly higher rates of breathing difficulties, cough, and sore throat than those of single infections. <\/td>\n    <\/tr>\n    <tr>\n        <td style=\"text-align:left\"> Neonates and pregnant women. <\/td>\n        <td style=\"text-align:left\"> Likely. While multiple studies demonstrated that pregnant women are more likely to develop severe illness after infection with Covid-19 and other influenza viruses, there is no study demonstrating that Covid-19 poses higher risk for neonates. However, neonates are thought to be susceptible to the virus, because their immune system is not well developed. <\/td>\n    <\/tr>\n    <tr>\n        <td style=\"text-align:left\"> Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences. <\/td>\n        <td style=\"text-align:left\"> Likely but not demonstrated. While no study looked directly at socio-economic and behavioral factors in relation to Covid-19, previous studies focused on risk factors for population from poorer areas of the World, identifying different health risks when compared to local population - such as other infectious diseases, inadequate immunity to vaccine-preventable illnesses, higher likelihood of having malnutrition and developmental delay and psychological trauma. <\/td>\n    <\/tr> \n    <tr>\n        <td style=\"text-align:left\"> Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors. <\/td>\n        <td style=\"text-align:left\"> Covid-19 is spread by human-to-human transmission via direct contact, droplets and aerosol transmissions, and infection has been estimated to have mean incubation period of 6.4 days and a basic reproduction number of 2.24\u20133.58. With regards to enviromental factors, tests on virus surrogaes for Covid demonstrated that low air temperature and low humidity are likely to increase virus survival on surfaces up to 28 days, with studies going as far as suggesting hospitals to keep high temperatures at relatively high RH to reduce the survival of airborne influenza virus. It is also demonstrated that an enviroment that allows for substantial undocumented infection facilitates the rapid dissemination of COVID-19, while after travel restrictions and control measures are imposed the reproductive number number falls considerably. Also, testing strategies that do not produce repeated false-negatives as the existing test do are deemed beneficial for the containment of the virus. <\/td>\n    <\/tr> \n    <tr>\n        <td style=\"text-align:left\"> Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups. <\/td>\n        <td style=\"text-align:left\"> The risk of fatality among infected individuals is 0.3% to 0.6%, but among hospitalised cases this increases to 14%. Based on deaths reported, high risk groups are pregnant women, older than the general population (mean age 45),and people in hospices. <\/td>\n    <\/tr> \n    <tr>\n        <td style=\"text-align:left\"> Susceptibility of populations. <\/td>\n        <td style=\"text-align:left\"> Unkonwn <\/td>\n    <\/tr> \n        <tr>\n        <td style=\"text-align:left\"> Public health mitigation measures that could be effective for control. <\/td>\n        <td style=\"text-align:left\"> Although little data is available for Covid-19 control measures, based on a forecasting model on Covid-19 data, it has been showed to be effective closin borders, schools, and suspending community services and commuters. More in general, in a number of recent studies, it has been shown how different intervention measures like travel restrictions, school closures, treatment and prophylaxis might allow to control outbreaks of diseases, such as SARS, pandemic influenza and others. It is also worth noting that because the elevated death risk estimates from Covid-19 are likely associated with a breakdown of the medical\/health system, enhanced public health interventions including social distancing and movement restrictions should be effectively implemented to bring the epidemic under control.  <\/td>\n    <\/tr> \n       \n    <\/tbody>\n<\/table>\n\n\n# 2. Methodology\n\n### 2.1 Data preparation\n\nWe first explored the dataset as well as clustering techniques, taking inspiration from Carrot [https:\/\/search.carrot2.org\/#\/pubmed]. However we decided to focus on actually providing the answers to the very specific questions that the challenge requested. Therefore, we first broke the challenge questions into a more specific set of questions that we could use to compare models and obtain overall better results.","c5d51aa2":"## Get data","02c02096":"## Initialise environment\n\nWe first intall the packages needed, get the papers corpus and load the local files, including the models that we run on another machine to save time to execution on the Kaggle machine. If the files for the models not in the folder, this same code will generate the files."}}