{"cell_type":{"d79a833c":"code","649cc03d":"code","eac7108b":"code","29313843":"code","2a706bd1":"code","4311bfab":"code","9171724c":"code","8a28eb12":"code","4f3cf94a":"code","9a9d5656":"code","dffdfeb1":"code","f59d995b":"code","59e4eb19":"code","6011863a":"code","44ba2f90":"code","8e8fae69":"code","870eda87":"code","32f51da4":"code","dd411714":"code","502e1828":"code","6669f02b":"code","f98568f7":"code","6ef61d16":"code","48b14d81":"code","0e48299f":"code","8d8769ef":"code","cf5edb00":"code","72477630":"code","bbd28066":"code","a51ecb11":"code","4c7560ef":"code","95e957eb":"code","a6fc7071":"code","bf240556":"code","53381908":"code","97db337d":"code","0ae9e1b8":"code","3eec701a":"code","c3b9e402":"code","dd0e1856":"code","294c9c15":"code","9bcae1de":"code","c47311da":"code","eb5d06b3":"code","873744e9":"code","c8e8f80d":"code","fab42774":"code","2793d97a":"code","fe053caf":"code","93c2d045":"code","098169cb":"code","3bb5f3f1":"code","4671d7dd":"code","0bff7cc0":"code","2ebe57ae":"code","cdb6d6fd":"code","6f2f6d13":"code","9c12a82d":"code","58d294ae":"code","7c52ed77":"code","f8ff08c7":"code","c71fa5c1":"code","cd41c2c7":"code","fe4b0e65":"code","e3b257e8":"code","97c2cee3":"code","354c6319":"code","90840c58":"markdown","a00e1459":"markdown","2dbf38c3":"markdown","8a5dcb51":"markdown","227f51bd":"markdown","9978bf2b":"markdown","74e684ae":"markdown","f9218b43":"markdown","0844eaee":"markdown","7bbb0c10":"markdown","789b6bea":"markdown","72950dba":"markdown","245ed7fd":"markdown","55c88bf1":"markdown","d7dd9d9f":"markdown","882d5e68":"markdown","0c63a9d9":"markdown","3b84781b":"markdown","668511c6":"markdown","5ce94ea2":"markdown","3d207cf3":"markdown","6f2ec87a":"markdown","bf23af0b":"markdown"},"source":{"d79a833c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","649cc03d":"#Importing the necessary libraries \n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n!pip install -U sentence-transformers\n!pip install ethnicolr\nfrom ethnicolr import census_ln, pred_census_ln\nimport scipy\n\n! pip install geotext\n! pip install geopy\nimport geopandas as gpd\n\nfrom urllib import request\nfrom geotext import GeoText\n\nfrom geopy.geocoders import Nominatim\nfrom geopy.exc import GeocoderTimedOut\n\nfrom shapely.geometry import Point, Polygon\nimport descartes","eac7108b":"#Loading the Titanic Datasets \n\ndf = pd.read_csv('..\/input\/hackerearth-love-in-the-time-of-screens\/data.csv')\nsample_df = pd.read_csv('..\/input\/hackerearth-love-in-the-time-of-screens\/sample dataset.csv')\nsample_sub = pd.read_csv('..\/input\/hackerearth-love-in-the-time-of-screens\/sample submission.csv')\nib = pd.read_csv('..\/input\/interest-bert-score\/Interest_relation.csv').drop(['Unnamed: 0'],axis=1)\njb = pd.read_csv('..\/input\/jobbertscore\/Job_relation.csv').drop(['Unnamed: 0'],axis=1)\n\ndf.head()","29313843":"df.info()","2a706bd1":"df[['age','height']].hist()","4311bfab":"from sentence_transformers import SentenceTransformer\n\nsbert_model = SentenceTransformer('bert-base-nli-mean-tokens')","9171724c":"def bert(d1,d2):\n  s1 = d1.bio_model.iloc[0]\n  s2 = d2.bio_model.iloc[0]\n  \n  cosine = scipy.spatial.distance.cosine(s1,s2)\n\n  return round((1-cosine)*100,2)","8a28eb12":"def bert_transform(data):\n    data['bio_model'] = data.bio.apply(lambda x: sbert_model.encode(x))\n    return data\n\nsample_df = bert_transform(sample_df)\nsample_df.bio_model","4f3cf94a":"bert(sample_df.iloc[[0]],sample_df.iloc[[2]])","9a9d5656":"df.cities = df.location.apply(lambda x: x.split(', ')[0])\ndf.cities.head()","dffdfeb1":"geolocator = Nominatim(timeout=2,user_agent=\"smy-application\")\nlat_lon = []\nk=0\nfor city in df.cities: \n    try:\n        location = geolocator.geocode(city)\n        if location:\n            #print(location.latitude, location.longitude)\n            lat_lon.append(location)\n    except GeocoderTimedOut as e:\n        print(\"Error: geocode failed on input %s with message %s\"(city, e))\n    if k%50==0:\n        print(\"No. of iteration {}\".format(k))\n    k+=1\nlat_lon[0:5]","f59d995b":"lat_lon = [[i.latitude,i.longitude] for i in lat_lon]","59e4eb19":"se = pd.Series(lat_lon)\ndf['lat_Lon'] = se.values","6011863a":"def loc_transform(data):\n  LocationList = str(data['location']).split(\",\")\n  data['City'] = LocationList[0]\n  data['State'] = LocationList[1]\n  return data\n\nsample_df = loc_transform(sample_df)\nse = pd.Series(lat_lon[0:5])\nsample_df['lat_Lon'] = se.values","44ba2f90":"from math import sin, cos, sqrt, atan2, radians\ndef distance(LL1,LL2):\n  # approximate radius of earth in km\n  R = 6373.0\n\n  lat1 = radians(LL1[0])\n  lon1 = radians(LL1[1])\n  lat2 = radians(LL2[0])\n  lon2 = radians(LL2[1])\n\n  dlon = lon2 - lon1\n  dlat = lat2 - lat1\n\n  a = sin(dlat \/ 2)**2 + cos(lat1) * cos(lat2) * sin(dlon \/ 2)**2\n  c = 2 * atan2(sqrt(a), sqrt(1 - a))\n\n  distance = R * c\n  return distance\n\n\ndef loc_score(d1,d2):\n  score=0\n  dis = distance(d1.lat_Lon.iloc[0],d2.lat_Lon.iloc[0])\n  if(d1.location_preference.iloc[0] == 'same city' and dis == 0):\n    score = 100\n  elif(d1.location_preference.iloc[0] == 'same city' and 0< dis <= 10 ) :\n    score = 90\n  elif(d1.location_preference.iloc[0] == 'same city' and 10< dis <= 20 ) :\n    score = 80\n  elif(d1.location_preference.iloc[0] == 'same city' and 20< dis <= 30 ) :\n    score = 70\n  elif(d1.location_preference.iloc[0] == 'same city' and 30< dis <= 40 ) :\n    score = 60\n  elif(d1.location_preference.iloc[0] == 'same city' and 0< dis <= 50 ) :\n    score = 50\n  elif(d1.location_preference.iloc[0] == 'same city' and 50 <dis <= 100 ) :\n    score = 30\n  elif(d1.location_preference.iloc[0] == 'same city' and 100 <dis <= 200 ) :\n    score = 10\n  elif(d1.State.iloc[0] == d2.State.iloc[0] and d1.location_preference.iloc[0] == 'same state' ):\n    score = 100\n  elif(d1.location_preference.iloc[0] == 'anywhere' and dis <= 10):\n    score = 100\n  elif(d1.location_preference.iloc[0] == 'anywhere' and 10 < dis <= 100 ):\n    score = 75\n  elif(d1.location_preference.iloc[0] == 'anywhere' and 100 < dis <= 200 ):\n    score = 50\n  elif(d1.location_preference.iloc[0] == 'anywhere' and 200 < dis <= 500 ):\n    score = 25 \n  return score","8e8fae69":"loc_score(sample_df.iloc[[0]],sample_df.iloc[[4]])","870eda87":"df.iloc[[0]][['interests','other_interests']].values.tolist()","32f51da4":"def interest_trans(data):\n    data['interests'] = data[['interests','other_interests']].values.tolist()\n    return data\n   \nsample_df = interest_trans(sample_df)\n\nsample_df.interests.head(1)","dd411714":"df = interest_trans(df)\nList = df.interests.tolist()\nflat_list = [item for sublist in List for item in sublist]\nflat_list[0:4]","502e1828":" from collections import Counter\n len(Counter(flat_list))","6669f02b":"#This is the method by which I have created the Interest_relation.csv\n\nfrom sentence_transformers import SentenceTransformer\nsbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n\nfirst=[]\nsecond=[]\nsimilarity=[]\nfor i in set(flat_list):\n  for j in set(flat_list):\n    document_embeddings1 = sbert_model.encode(i)\n    document_embeddings2 = sbert_model.encode(j)\n    cosine = scipy.spatial.distance.cosine(document_embeddings1,document_embeddings2)\n\n    first.append(i)\n    second.append(j)\n    similarity.append(round((1-cosine)*100,2))\n    \n    \nd = {'first':first,'second':second,'similarity':similarity}\nib = pd.DataFrame(d)\n","f98568f7":"ib.head()","6ef61d16":"df.interests[0:5]","48b14d81":"def int_score(d1,d2):\n  l1 = d1.interests.iloc[0]\n  l2 = d2.interests.iloc[0]\n  bert_score=0\n  if l1 and l2:\n    for i in l1:\n      for j in l2:\n        bert_score+= ib[(ib['first']==i) & (ib['second']==j) ].similarity.iloc[0]\n  \n  return bert_score\/4","0e48299f":"int_score(df.iloc[[4]],df.iloc[[5]])","8d8769ef":"df.job.value_counts()","cf5edb00":"def job_list(string):\n  l = string.split('\/')\n  l = [word.strip() for word in l]\n  l = [word for word in l if word not in ['rather not say','unemployed','other']]\n  return l\n\nsample_df['job_list'] = sample_df['job'].apply(lambda x: job_list(x))\nsample_df.job_list","72477630":"jb['firstname'] = jb.name.apply(lambda x : x.split(' and ')[0].strip().replace('[','').replace(']','').replace(\"'\",\"\"))\njb['second'] = jb.name.apply(lambda x : x.split(' and ')[1].strip().split(' are ')[0].replace('[','').replace(']','').replace(\"'\",\"\"))\njb['similarity'] = jb.name.apply(lambda x : float(x.split(' and ')[1].strip().split(' are ')[1].split('to ')[1].replace('%','')))\njb.head()","bbd28066":"def trans_list(l):\n  return str(l).replace(\"[\",\"\").replace(']','').replace(\"'\",'')\n\n#def trans_string(string):\n#  return string.replace('[','').replace(']','').replace(\"'\",\"\").split(', ')\n\ndef job_bert(d1,d2):\n  l1 = d1.job_list.iloc[0]\n  l2 = d2.job_list.iloc[0]\n  if l1 and l2:\n    bert_score = jb[(jb['firstname']==trans_list(l1)) & (jb['second']==trans_list(l2)) ].similarity.iloc[0]\n  else:\n    bert_score=0\n  return bert_score","a51ecb11":"job_bert(sample_df.iloc[[0]],sample_df.iloc[[3]])","4c7560ef":"df.pets.value_counts()","95e957eb":"def pets_trans(data):\n  data['likes_dogs'] = data.pets.apply(lambda x: 1 if 'likes dogs' in x else (1 if 'has dogs' in x else 0 ) )\n  data['has_dogs'] = data.pets.apply(lambda x: 1 if 'has dogs' in x else 0 )\n  data['likes_cats'] = data.pets.apply(lambda x: 1 if 'likes cats' in x else (1 if 'has cats' in x else 0 ) )\n  data['has_cats'] = data.pets.apply(lambda x: 1 if 'has cats' in x else 0 )\n  data['dislikes_dogs'] = data.pets.apply(lambda x: 1 if 'dislikes dogs' in x else 0 )\n  data['dislikes_cats'] = data.pets.apply(lambda x: 1 if 'dislikes cats' in x else 0 )\n  return data\n\nsample_df = pets_trans(sample_df)\nsample_df.head(1)","a6fc7071":"def pets_score(d1,d2):\n  score = 0\n  if (d1.likes_dogs.iloc[0] == 1 and d2.likes_dogs.iloc[0] == 1):\n    score+=100\n  if (d1.likes_dogs.iloc[0] == 1 and d2.has_dogs.iloc[0] == 1):\n    score+=100\n  if (d1.has_dogs.iloc[0] == 1 and d2.likes_dogs.iloc[0] == 1):\n    score+=100\n  if (d1.dislikes_dogs.iloc[0] == 1 and d2.dislikes_dogs.iloc[0] == 1):\n    score+=100\n\n  if (d1.likes_cats.iloc[0] == 1 and d2.likes_cats.iloc[0] == 1):\n    score+=100\n  if (d1.likes_cats.iloc[0] == 1 and d2.has_cats.iloc[0] == 1):\n    score+=100\n  if (d1.has_cats.iloc[0] == 1 and d2.likes_cats.iloc[0] == 1):\n    score+=100\n  if (d1.dislikes_cats.iloc[0] == 1 and d2.dislikes_cats.iloc[0] == 1):\n    score+=100\n\n  if (d1.likes_dogs.iloc[0] == 1 and d2.dislikes_dogs.iloc[0] == 1):\n    score-=50\n  if (d1.dislikes_dogs.iloc[0] == 1 and d2.likes_dogs.iloc[0] == 1):\n    score-=50\n  if (d1.has_dogs.iloc[0] == 1 and d2.dislikes_dogs.iloc[0] == 1):\n    score-=100\n  if (d1.dislikes_dogs.iloc[0] == 1 and d2.has_dogs.iloc[0] == 1):\n    score-=100\n\n  if (d1.likes_cats.iloc[0] == 1 and d2.dislikes_cats.iloc[0] == 1):\n    score-=50\n  if (d1.dislikes_cats.iloc[0] == 1 and d2.likes_cats.iloc[0] == 1):\n    score-=50\n  if (d1.has_cats.iloc[0] == 1 and d2.dislikes_cats.iloc[0] == 1):\n    score-=100\n  if (d1.dislikes_cats.iloc[0] == 1 and d2.has_cats.iloc[0] == 1):\n    score-=100\n\n  \n\n  return score\/4","bf240556":"pets_score(sample_df.iloc[[0]],sample_df.iloc[[4]])","53381908":"def lang_list(string):\n  l =string.split(',')\n  l = [word.strip()for word in l]\n  m=[]\n  n=[]\n  for word in l:\n    if word.startswith('eng'):\n      pass\n    else:\n      m.append(word)\n  for word in m:\n    n.append(word.split(' ')[0])\n  return n","97db337d":"def language(data):\n  data['know_english'] = data['language'].apply(lambda x: 0 if x.startswith('english (poorly)') else 1 )\n  data['other_lang'] = data['language'].apply(lambda x: lang_list(x))\n  return data","0ae9e1b8":"sample_df = language(sample_df)\nsample_df.head(1)","3eec701a":"def lang_score(d1,d2):\n  score = 100 if d1.know_english.iloc[0] else 0\n  c = sum(el in d1.other_lang.iloc[0] for el in d2.other_lang.iloc[0])\n  score+= (100*c)\n  if c ==0 and d1.new_languages.iloc[0]=='interested':\n    score+=50\n  if c ==0 and d2.new_languages.iloc[0]=='interested':\n    score+=50\n  return score\/3","c3b9e402":"lang_score(sample_df.iloc[[0]],sample_df.iloc[[4]])","dd0e1856":"def ethnicity(data):\n  data['last_name'] = data['username'].apply(lambda x : x.split(' ')[1])\n  data['race'] = pred_census_ln(data[['last_name']], 'last_name', year=2010)['race']\n  return data","294c9c15":"ethnicity(sample_df)\nsample_df.head(1)","9bcae1de":"def race_score(d1,d2):\n  score=0\n  if d1.race.iloc[0] == d2.race.iloc[0]:\n    score+=100\n  return score","c47311da":"race_score(sample_df.iloc[[0]],sample_df.iloc[[4]])","eb5d06b3":"def euclidean_distance(x, y):   \n    return np.sqrt(np.sum((x - y) ** 2))\n\n\ndef normalisation(OldValue,OldMax,OldMin):\n  NewMax=100\n  NewMin=0\n  #OldValue=score\n\n  OldRange = (OldMax - OldMin)  \n  NewRange = (NewMax - NewMin)  \n  NewValue = (((OldValue - OldMin) * NewRange) \/ OldRange) + NewMin\n  return NewValue\n\ndef age_score(d1,d2):\n  score = euclidean_distance(d1.age.iloc[0],d2.age.iloc[0])\n  return normalisation( round(100-score,2),100,49)\n","873744e9":"age_score(sample_df.iloc[[0]],sample_df.iloc[[2]])","c8e8f80d":"a = ['average', 'fit', 'athletic','jacked']\nb = ['curvy', 'a little extra','full figured', 'overweight']\nc = ['thin','skinny']","fab42774":"def body_score(d1,d2):\n  score=0\n  if d1.body_profile.iloc[0] in a and d2.body_profile.iloc[0] in a:\n    score+=100\n  if d1.body_profile.iloc[0] in b and d2.body_profile.iloc[0] in b:\n    score+=100\n  if d1.body_profile.iloc[0] in c and d2.body_profile.iloc[0] in c:\n    score+=100\n    \n  return score","2793d97a":"body_score(sample_df.iloc[[0]],sample_df.iloc[[3]])","fe053caf":"print(df.drinks.value_counts().index)\nprint('-'*50)\n\nprint(df.smokes.value_counts().index)\nprint('-'*50)\n\nprint(df.drugs.value_counts().index)\nprint('-'*50)","93c2d045":"drinks_yes = ['very often','desperately','often']\ndrinks_rare = ['socially', 'rarely']\ndrinks_no = ['not at all']\n\nsmokes_yes = ['sometimes', 'yes', 'when drinking', 'trying to quit']\nsmokes_no = ['no']\n\ndrugs_yes = ['sometimes','often']\ndrugs_no = ['never']","098169cb":"def dds_score(d1,d2):\n  score = 0\n  if (d1.drinks.iloc[0] in drinks_yes and d2.drinks.iloc[0] in drinks_yes):\n    score +=100\n  elif (d1.drinks.iloc[0] in drinks_yes and d2.drinks.iloc[0] in drinks_rare):\n    score +=50\n  elif (d1.drinks.iloc[0] in drinks_rare and d2.drinks.iloc[0] in drinks_yes):\n    score +=50\n  elif (d1.drinks.iloc[0] in drinks_rare and d2.drinks.iloc[0] in drinks_rare):\n    score+=100\n  elif (d1.drinks.iloc[0] in drinks_no and d2.drinks.iloc[0] in drinks_no):\n    score +=100\n  elif (d1.drinks.iloc[0] in drinks_rare and d2.drinks.iloc[0] in drinks_no):\n    score +=50\n  elif (d1.drinks.iloc[0] in drinks_no and d2.drinks.iloc[0] in drinks_rare):\n    score +=50\n\n  if (d1.smokes.iloc[0] in smokes_yes and d2.smokes.iloc[0] in smokes_yes):\n    score +=100 \n  elif (d1.smokes.iloc[0] in smokes_no and d2.smokes.iloc[0] in smokes_no):\n    score +=100\n\n  if (d1.drugs.iloc[0] in drugs_yes and d2.drugs.iloc[0] in drugs_yes):\n    score +=100 \n  elif (d1.drugs.iloc[0] in drugs_no and d2.drugs.iloc[0] in drugs_no):\n    score +=100\n\n  return score\/3","3bb5f3f1":"dds_score(sample_df.iloc[[0]],sample_df.iloc[[3]])","4671d7dd":"def edu_score(d1,d2):\n  score=0\n  if (d2.education_level.iloc[0] == d1.education_level.iloc[0]):\n    score+=100\n  if (d2.education_level.iloc[0] == d1.education_level.iloc[0] - 1):\n    score+=50\n  if (d2.education_level.iloc[0] == d1.education_level.iloc[0] + 1):\n    score+=50\n  if (d2.education_level.iloc[0] == d1.education_level.iloc[0] - 2):\n    score+=25\n  if (d2.education_level.iloc[0] == d1.education_level.iloc[0] + 2):\n    score+=25\n  if (d1.dropped_out.iloc[0] == d2.dropped_out.iloc[0]):\n    if score == 100 :\n      score += 100\n    else:\n      score +=50\n       \n  return score\/2","0bff7cc0":"edu_score(sample_df.iloc[[0]],sample_df.iloc[[3]])","2ebe57ae":"df.status.value_counts().index","cdb6d6fd":"a = ['single', 'available']\nb = ['seeing someone', 'married']\n\ndef status_score(d1,d2):\n  score=0\n  if (d1.status.iloc[0] in a and d2.status.iloc[0] in a):\n    score +=100\n  return score","6f2f6d13":"status_score(sample_df.iloc[[0]],sample_df.iloc[[3]])","9c12a82d":"df.groupby([\"orientation\",\"sex\"]).count()['user_id']","58d294ae":"def ori_score(d1,d2):\n  score = 0\n  if (d1.orientation.iloc[0] == 'straight' and d2.orientation.iloc[0] == 'straight'):\n    score += 100\n  elif (d1.orientation.iloc[0] == 'gay' and d2.orientation.iloc[0] == 'gay'):\n    score += 100 \n  elif (d1.orientation.iloc[0] == 'bisexual' and d2.orientation.iloc[0] == 'bisexual'):\n    score += 100 \n  \n  return score","7c52ed77":"ori_score(sample_df.iloc[[1]],sample_df.iloc[[0]])","f8ff08c7":"def fin_trans(data):\n\n  data = bert_transform(data) #bio\n  data = loc_transform(data) #location\n# data = interest_trans(data) #interest\n  data['job_list'] = data['job'].apply(lambda x: job_list(x)) #job\n  data = pets_trans(data) #pets\n  data = language(data)   #language\n  data = ethnicity(data)\n  return data\n","c71fa5c1":"df = fin_trans(df)\ndf.columns","cd41c2c7":"weights = {\"bio\":0.15,\"job\":0.05,\"interest\":0.05,\"race\":0.05,\"age\":0.15,\"location\":0.15\n           ,\"drinks_drugs_smokes\":0.05,'language' :0.05 ,\n           \"pets\":0.05,\"education_level\":0.05,\"status\":0.05,\"body_profile\":0.05,\"orientation\":0.10}","fe4b0e65":"round(sum(weights.values()),3)","e3b257e8":"def score_fin(d1,d2):\n    \n  # age score\n  age_sc = age_score(d1,d2)* weights['age']\n \n  # location score\n  loca_sc = loc_score(d1,d2) * weights['location']\n\n  # drinks_drugs_smokes score\n  dds_sc = dds_score(d1,d2) * weights['drinks_drugs_smokes']\n\n  # job score\n  job_sc = job_bert(d1,d2) * weights['job']\n\n  # pet score\n  pet_sc = pets_score(d1,d2) * weights['pets']\n\n  #language score\n  lan_sc = lang_score(d1,d2) * weights['language']\n  \n  #body_profile score\n  bp_sc = body_score(d1,d2) * weights['body_profile']\n\n  #education level\n  edu_sc = edu_score(d1,d2) * weights['education_level']\n\n  #bio score\n  bio_sc = bert(d1,d2) * weights['bio']\n\n  #interest score\n  int_sc = int_score(d1,d2) * weights['interests']\n\n  #race score\n  race_sc = race_score(d1,d2) * weights['race']\n\n  #status score\n  status_sc = status_score(d1,d2) * weights['status']\n\n  #orientation score\n  ori_sc = ori_score(d1,d2) * weights['orientation']\n\n  fin_score = (age_sc  + loca_sc + dds_sc + job_sc + edu_sc  + pet_sc + bp_sc + lan_sc \n               + bio_sc + int_sc + race_sc +status_sc + ori_sc)\n\n  return round(fin_score,2)","97c2cee3":"def match_fin(data1,data2):\n  \n  result = pd.DataFrame(data = np.zeros((data1.user_id.nunique(), data2.user_id.nunique())),  \n                        index = data1.user_id.tolist(),  \n                        columns = data2.user_id.tolist())\n  \n  result.index.name = 'user_id'\n\n  k=1\n  for i in data1.user_id:\n    for j in data2.user_id:\n      d1 = data1[data1.user_id==i] \n      d2 = data2[data2.user_id==j]\n\n      \n\n      #Straight people\n\n      if (d1.orientation.iloc[0] == 'straight' and d2.orientation.iloc[0] == 'straight'):\n        if (d1.sex.iloc[0] == d2.sex.iloc[0]) :\n          result.loc[i,j] = 0                                                   #SM-SM\n        else:\n          result.loc[i,j] = score_fin(d1,d2)                                    #SM-SF\n      \n      if (d1.orientation.iloc[0] == 'straight' and d2.orientation.iloc[0] == 'gay'):\n        result.loc[i,j] = 0                                                     #SM-GF SM-GM\n      \n      if (d1.orientation.iloc[0] == 'straight' and d2.orientation.iloc[0] == 'bisexual'):\n        if (d1.sex.iloc[0] == d2.sex.iloc[0]) :\n          result.loc[i,j] = 0                                                   #SM-BM\n        else:\n          result.loc[i,j] = score_fin(d1,d2)                                    #SM-BF\n\n      #gay people\n      \n      if (d1.orientation.iloc[0] == 'gay' and d2.orientation.iloc[0] == 'gay'):\n       if (d1.sex.iloc[0] == d2.sex.iloc[0]):\n        result.loc[i,j] = score_fin(d1,d2)                                      #GM-GM\n       else:\n        result.loc[i,j] = 0                                                     #GM-GF\n\n      if (d1.orientation.iloc[0] == 'gay' and d2.orientation.iloc[0] == 'straight'):\n        result.loc[i,j] = 0                                                     #GM-SF, GM-SM\n\n      if (d1.orientation.iloc[0] == 'gay' and d2.orientation.iloc[0] == 'bisexual'):\n       if (d1.sex.iloc[0] == d2.sex.iloc[0]):\n        result.loc[i,j] = score_fin(d1,d2)                                      #GM-BM\n       else:\n        result.loc[i,j] = 0                                                     #GM-BF\n\n    #bisexual people\n\n      if (d1.orientation.iloc[0] == 'bisexual' and d2.orientation.iloc[0] == 'bisexual'):\n        if (d1.sex.iloc[0] == d2.sex.iloc[0]):\n          result.loc[i,j] = (score_fin(d1,d2)                                   #BM-BM\n        else:\n          result.loc[i,j] = 0                                                   #BM-BF\n\n      if (d1.orientation.iloc[0] == 'bisexual' and d2.orientation.iloc[0] == 'straight'):\n        result.loc[i,j] = score_fin(d1,d2)                                      #BM-SM , BM-SF\n\n      if (d1.orientation.iloc[0] == 'bisexual' and d2.orientation.iloc[0] == 'gay'):\n        if (d1.sex.iloc[0] == d2.sex.iloc[0]):\n          result.loc[i,j] = score_fin(d1,d2)                                    #BM-GM\n        else:\n          result.loc[i,j] = 0                                                   #BM-GF\n\n      if (d1.user_id.iloc[0] == d2.user_id.iloc[0]):\n        result.loc[i,j] = 0\n\n  \n\n      \n\n\n    if k%100==0:\n      print('{} iteration done'.format(k))\n    k+=1\n\n  print('Process Done')\n  result.to_csv(Data_dir+'result.csv',index=False)\n\n\n  return result.head()","354c6319":"import time\nstart = time.time()\n\nresult_fin = match_fin(df,df)\n\ntime.sleep(1)\nend = time.time()\nprint(f\"Runtime of the program is {end - start} sec\")\n\nresult_fin","90840c58":"# Pets score","a00e1459":"# Final Model","2dbf38c3":"# Orientation Score","8a5dcb51":"# Bio Score","227f51bd":"# Job score","9978bf2b":"*English is very common among all the datapoints so I have created a separate attribute 'Know_english'. Apart from english , the other languages were stored in a list, through the lang_list() function.*","74e684ae":"*I have already found out the similarity score between different set of Interests and Jobs using BERT and stored them in the Interest_relation.csv and Job_relation.csv. So beside loading the data , I have also loaded those two tables under the alias ib and jb respectively.Donot worry , you will soon see their use in Interest score and Language score.* ","f9218b43":"Using Geoparsing method I have extracted the latitude and longitude from the city names.\n\nCredit - https:\/\/towardsdatascience.com\/geoparsing-with-python-c8f4c9f78940","0844eaee":"# Education Score","7bbb0c10":"*We have done this way because encoding(making words into array) and calculating BERT similarity during the running of Main_Function takes lots of time.*","789b6bea":"*We have found out the euclidean distance between two participants age and normalise it to fall in between (0,100).*","72950dba":"*I have taken an approach to give score based on each attribute\/feature and then finally take the cumulative weighted score.Everything else will be explained as we continue, no worries.*","245ed7fd":"# Language Score","55c88bf1":"# Location Score","d7dd9d9f":"# Body Profile Score","882d5e68":"# DDS Score","0c63a9d9":"# Status Score","3b84781b":"*The bio_model column contains the BERT encodings for the respective bio.BERT is a pretrained model which convert a sentence to an array so that machine can read them.*","668511c6":"# Ethnicity score from username","5ce94ea2":"*This is done by taking the help of  a pre created python package which determines the race based on last_name of observants. Although it is debatable to use race in algorithm but thinking of human approach of matching, we have included this section.*","3d207cf3":"# Interest Score","6f2ec87a":"***evaluation metric : score = max(0, 100 - root\\_mean\\_squared\\_error(actual, predicted))***\n\n> # This have given us a score of 97.9823","bf23af0b":"# Age Score"}}