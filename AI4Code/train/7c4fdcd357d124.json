{"cell_type":{"b52c2bfa":"code","7a03d492":"code","15086dc6":"code","43d758a7":"code","afd05d0e":"code","464f4f7d":"code","d98b9e7b":"code","06038f25":"code","41ea342e":"code","8bf6fc99":"code","fe4e5e5f":"code","06671e37":"code","b7ca49f4":"code","de8bf0bf":"code","4c365ac4":"code","5c2e393c":"code","34a72aba":"code","7a3ff72b":"code","d2e042e7":"code","738c8f83":"code","496608c9":"code","40747b20":"code","5dc5d77d":"code","66599f32":"code","4bb44deb":"code","b566872c":"code","58f9d564":"code","fdf0e4cc":"code","d68973c4":"code","5f94bf68":"code","5f8695e7":"code","1765c4de":"code","9187f6ee":"code","e79f1aee":"code","93d6549f":"code","fcd6916c":"code","6babe488":"markdown","a6617a3b":"markdown","b55f240d":"markdown","33014936":"markdown","328aa113":"markdown","f0149493":"markdown","38b25b39":"markdown","f4f66760":"markdown"},"source":{"b52c2bfa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7a03d492":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\ndata_train = pd.read_csv('..\/input\/train.csv')\ndata_test = pd.read_csv('..\/input\/test.csv')\n","15086dc6":"#Lets see data sample\ndata_train.sample(10)\n\n","43d758a7":"# Lets check Df shape\ndata_train.shape\n\n# there are 128 features.","afd05d0e":"data_test.shape","464f4f7d":"data_train.dtypes\ndata_train.dtypes.unique()\n#No string data type - all are numerical values which is good.","d98b9e7b":"data_train.isnull().sum()[data_train.isnull().sum() !=0]\n#Below listed columns have missing values in the combined (Train+test) dataset. ","06038f25":"# Lets draw a bar graph to visualize percentage of missing features in train set\nmissing= data_train.isnull().sum()[data_train.isnull().sum() !=0]\nmissing=pd.DataFrame(missing.reset_index())\nmissing.rename(columns={'index':'features',0:'missing_count'},inplace=True)\nmissing['missing_count_percentage']=((missing['missing_count'])\/59381)*100\nplt.figure(figsize=(20,8))\nsns.barplot(y=missing['features'],x=missing['missing_count_percentage'])\n\n#Looking at below bar grah- \n#Medical_Hist_32\/24\/15\/10 , Family_hist_5 are top five features with huge amount of missing data ( imputaion to these might not be fruitful - I will drop these features)\n","41ea342e":"# Lets see spread of data before we impute missing values\nplt.plot(figsize=(15,10))\nsns.boxplot(data_train['Employment_Info_1'])\n# Employment_Info_1 seems to have lots of outliers - Median should be right to impute missing values","8bf6fc99":"data_train['Employment_Info_1'].isna().sum()","fe4e5e5f":"data_train['Employment_Info_1'].fillna(data_train['Employment_Info_1'].median(),inplace=True) \n# imputing with Meadian , as there are lots of Outliers \ndata_test['Employment_Info_1'].fillna(data_test['Employment_Info_1'].median(),inplace=True) ","06671e37":"data_train['Employment_Info_1'].isna().sum()","b7ca49f4":"#Outlier Treatment -\ndata_train['Employment_Info_1'].describe()\n","de8bf0bf":"sns.boxplot(data_train['Employment_Info_4'])\n# ['Employment_Info_4'] is has most of the values centered close to zero , also huge presence of outliers \n","4c365ac4":"data_train['Employment_Info_4'].fillna(data_train['Employment_Info_4'].median(),inplace=True)\ndata_test['Employment_Info_4'].fillna(data_test['Employment_Info_4'].median(),inplace=True)","5c2e393c":"sns.boxplot(data_train['Employment_Info_6'])\n#No outlieers - mean should be rigth candidate to impute missing values","34a72aba":"data_train['Employment_Info_6'].fillna(data_train['Employment_Info_6'].mean(),inplace=True)\ndata_test['Employment_Info_6'].fillna(data_test['Employment_Info_6'].mean(),inplace=True)","7a3ff72b":"sns.boxplot(y=data_train['Medical_History_1'])","d2e042e7":"data_train['Medical_History_1'].fillna(data_train['Medical_History_1'].median(),inplace=True)\ndata_test['Medical_History_1'].fillna(data_test['Medical_History_1'].median(),inplace=True)","738c8f83":"#lets drop features with high number of missing values \ndata_train.drop(['Medical_History_10','Medical_History_15','Medical_History_24','Medical_History_32','Family_Hist_3','Family_Hist_5','Family_Hist_2','Family_Hist_4'],axis=1,inplace=True)\n\n","496608c9":"data_test.drop(['Medical_History_10','Medical_History_15','Medical_History_24','Medical_History_32','Family_Hist_3','Family_Hist_5','Family_Hist_2','Family_Hist_4'],axis=1,inplace=True)","40747b20":"data_train.isnull().sum()[data_train.isnull().sum()!=0]","5dc5d77d":"#imputing with median \ndata_train['Insurance_History_5'].fillna(data_train['Insurance_History_5'].median(),inplace=True)\ndata_test['Insurance_History_5'].fillna(data_test['Insurance_History_5'].median(),inplace=True)\n","66599f32":"data_train.isnull().sum()\n#All missing NA values has been treated\n","4bb44deb":"data_train.head()\n#Product_info_2 seems to be the only feature where we should map string values with numeric categorical values","b566872c":"data_train['Product_Info_2'].unique()","58f9d564":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ndata_train['Product_Info_2']=le.fit_transform(data_train['Product_Info_2'])\ndata_test['Product_Info_2']=le.transform(data_test['Product_Info_2'])\n\n#data_train.dtypes\n#Employment_Info_1-4-6  Insurance_History_5\n# I faced an error stating dta types of train columns are not float\/numeric ill apply encoder on all column and see what happens\n","fdf0e4cc":"data_train.head()","d68973c4":"# feature meatrix and response vector seperation\nX_train=data_train.iloc[:,0:-1]\ny_train=data_train['Response']\nX_train.drop('Id',axis=1,inplace=True)","5f94bf68":"X_train.head()","5f8695e7":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X_train,y_train)","1765c4de":"y_train.unique()\n#there are 8 labels\/class in dataset","9187f6ee":"\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nfrom sklearn.model_selection import cross_val_score,GridSearchCV\nfrom sklearn.multiclass import OneVsRestClassifier\n","e79f1aee":"# Using a Decision Tree classifier \nfrom sklearn.tree import DecisionTreeClassifier\nparam_grid={'max_depth':range(1,20,2)}\nDT=DecisionTreeClassifier()\nclf_DT=GridSearchCV(DT,param_grid,cv=10,scoring='accuracy',n_jobs=-1).fit(X_train,y_train)\ny_pred=clf_DT.predict(X_test)\nprint(accuracy_score(y_test,y_pred))\n\n","93d6549f":"#Using a Random Forest tree classifier\nfrom sklearn.ensemble import RandomForestClassifier\nparam_grid={'max_depth':range(1,20,2)}\nRF=RandomForestClassifier()\nclf_rf=GridSearchCV(RF,param_grid,cv=10,scoring='accuracy',n_jobs=-1).fit(X_train,y_train)\ny_pred=clf_rf.predict(X_test)\naccuracy_score(y_test,y_pred)","fcd6916c":"ids = data_test['Id']\npredictions = clf_DT.predict(data_test.drop('Id', axis=1))\n\n\noutput = pd.DataFrame({ 'Id' : ids, 'Response': predictions })\noutput.to_csv('\/Users\/adityaprakash\/Downloads\/predictions.csv', index = False)\noutput.head()","6babe488":" \n \n Employment_Info_1_4_6 Insurance_History_5 Family_Hist_2-3-4-5 are continous features . \n\nThe following variables are discrete:\nMedical_History_1, Medical_History_10, Medical_History_15, Medical_History_24, Medical_History_32\n\n1. remove rows with missing values and see model performance \n2. impute missing values with mean and median or may be mode.\n\n","a6617a3b":"We will check for missing values . \n\nIf a categorical feature has missing values - if required will impute it with median \n\nif a continous feature has missing values - if required will impute it with mean ","b55f240d":"Machine Learning Model fitting and prediction","33014936":"For now i'll use Decison tree for summission ,  i'll work on to improve my predictions, any suggestion\/feedback is appreciated.\n","328aa113":"#Now that we have imputed Missing values - we can move to next step to convert string type feature data into numric data","f0149493":"Import Necessary Libraries & Import .csv files into pandas DataFrames \n","38b25b39":"\n#Missing Value imputation ","f4f66760":"**Feature details posted in data overview section - **\nThe following variables are all categorical (nominal):\n\nProduct_Info_1, Product_Info_2, Product_Info_3, Product_Info_5, Product_Info_6, Product_Info_7, Employment_Info_2, Employment_Info_3, Employment_Info_5, InsuredInfo_1, InsuredInfo_2, InsuredInfo_3, InsuredInfo_4, InsuredInfo_5, InsuredInfo_6, InsuredInfo_7, Insurance_History_1, Insurance_History_2, Insurance_History_3, Insurance_History_4, Insurance_History_7, Insurance_History_8, Insurance_History_9, Family_Hist_1, Medical_History_2, Medical_History_3, Medical_History_4, Medical_History_5, Medical_History_6, Medical_History_7, Medical_History_8, Medical_History_9, Medical_History_11, Medical_History_12, Medical_History_13, Medical_History_14, Medical_History_16, Medical_History_17, Medical_History_18, Medical_History_19, Medical_History_20, Medical_History_21, Medical_History_22, Medical_History_23, Medical_History_25, Medical_History_26, Medical_History_27, Medical_History_28, Medical_History_29, Medical_History_30, Medical_History_31, Medical_History_33, Medical_History_34, Medical_History_35, Medical_History_36, Medical_History_37, Medical_History_38, Medical_History_39, Medical_History_40, Medical_History_41\n\nThe following variables are continuous:\n\nProduct_Info_4, Ins_Age, Ht, Wt, BMI, Employment_Info_1, Employment_Info_4, Employment_Info_6, Insurance_History_5, Family_Hist_2, Family_Hist_3, Family_Hist_4, Family_Hist_5\n\nThe following variables are discrete:\n\nMedical_History_1, Medical_History_10, Medical_History_15, Medical_History_24, Medical_History_32\n\nMedical_Keyword_1-48 are dummy variables."}}