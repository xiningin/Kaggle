{"cell_type":{"77f168c8":"code","202a69cd":"code","8163aee9":"code","dfebcf1e":"code","637080a6":"code","020ad03d":"code","f794ddf9":"code","9b518a4a":"code","dd88beb8":"code","a0c33ed7":"code","5eda217c":"code","91270f1b":"code","062e19ce":"code","4e8b45ff":"code","1e99d3e6":"code","b87e46e1":"code","de259396":"code","7846a5ff":"code","779ce504":"code","ea71abf4":"code","3030594e":"code","0e6fda9c":"code","52b0afb8":"code","27d0cc32":"code","df255e98":"code","385fcb4e":"code","00a9ab87":"code","8e8fccae":"code","bdf018bf":"code","491c93db":"code","4aa5ad4e":"code","f7e47a0b":"code","9a50bbeb":"code","ebf2b56a":"markdown","74197d80":"markdown","7faf4748":"markdown","bdc5bedc":"markdown","347b9d1b":"markdown","edc33504":"markdown","a0d8caa0":"markdown","aadf6548":"markdown","ff09d2a8":"markdown","8e0afeae":"markdown","cbdaeffb":"markdown","65d04b8f":"markdown","dedef19d":"markdown","43166ec6":"markdown","ce53a2e6":"markdown","3dd2a417":"markdown","1829dbec":"markdown","94e81bd1":"markdown","8f46c2fc":"markdown","e38a690c":"markdown","e31f6a8b":"markdown","b37fbaca":"markdown","ac5e0684":"markdown","365bb242":"markdown","0d44ace6":"markdown","ad2303e8":"markdown"},"source":{"77f168c8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# data vizualization\nfrom matplotlib import gridspec\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 15, 10\n\n# data preprocessing\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.manifold import TSNE\n\n# machine learning models\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\n\n# accuracy metrics\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_recall_curve, roc_auc_score\n\n# machine learning explainibility \nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom pdpbox import pdp, get_dataset, info_plots\nimport shap \n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","202a69cd":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","8163aee9":"df.describe()","dfebcf1e":"df.isna().sum()","637080a6":"plt.subplot(121)\nplt.pie(x = df.groupby(['Class']).Class.count().to_list(),\n        labels = [\"Left\", \"Right\"], autopct='%1.2f%%', explode = (0, 0.2))\n\nplt.subplot(122)\nsns.countplot(data = df, x = 'Class')\nfraudnt, fraud = df.Class.value_counts()\nplt.text(0, fraudnt\/\/2, fraudnt, fontsize = 20,  horizontalalignment='center')\nplt.text(1, fraud\/\/2, fraud, fontsize = 20, horizontalalignment='center')","020ad03d":"features = df.iloc[:,0:29].columns\nplt.figure(figsize=(15,29*4))\ngs = gridspec.GridSpec(29, 1)\nfor i, c in enumerate(df[features]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(df[c][df.Class == 1], bins=50,label='Fraud')\n    sns.distplot(df[c][df.Class == 0], bins=50,label=\"Valid\")\n    ax.legend()\n    ax.set_xlabel('')\n    ax.set_title('histogram of feature: ' + str(c))\nplt.show()","f794ddf9":"plt.subplot(211)\nsns.distplot(df.Amount.values, kde = True)\nplt.xlabel('Amount')\nplt.subplot(212)\nsns.histplot(df.Time.values, kde = True)\nplt.xlabel('Time')","9b518a4a":"plt.subplot(221)\nsns.distplot(df[df.Class == 0].Amount.values, kde = True)\nplt.subplot(222)\nsns.histplot(df[df.Class == 1].Amount.values, kde = True)\nplt.subplot(223)\nsns.histplot(df[df.Class == 0].Time.values, kde = True)\nplt.subplot(224)\nsns.histplot(df[df.Class == 1].Time.values, kde = True)","dd88beb8":"dft = df[[\"Amount\",\"Class\"]].copy()\ndft['Digits'] = dft.Amount.astype(str).str[:1].astype(int)\n\nplt.subplot(211)\nsns.countplot(dft[dft.Class == 0].Digits)\n\nplt.subplot(212)\nsns.countplot(dft[dft.Class == 1].Digits)","a0c33ed7":"rob_scaler = RobustScaler()\n\n#Normalizing the dataframe\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\ndf.drop(['Time','Amount'], axis=1, inplace=True)","5eda217c":"sns.heatmap(df.corr(), cmap='coolwarm_r', vmin = -1, vmax = 1, center = 0)","91270f1b":"fraud_df = df.loc[df['Class'] == 1]\nnon_fraud_df = df.loc[df['Class'] == 0][:492]\nsub_df = pd.concat([fraud_df, non_fraud_df]).sample(frac=1, random_state=420).reset_index(drop=True)\nsns.heatmap(sub_df.corr(), cmap='coolwarm_r', vmin = -1, vmax = 1, center = 0)","062e19ce":"# Making a subset to plot points clearly\nX_sub = sub_df.copy().drop('Class', axis=1)\ny_sub = sub_df.copy()['Class']\nX_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X_sub.values)\nplt.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y_sub == 0), cmap='coolwarm_r', label='No Fraud')\nplt.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y_sub == 1), cmap='coolwarm_r', label='Fraud')","4e8b45ff":"X = df.copy().drop('Class', axis=1)\ny = df.copy().Class","1e99d3e6":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify  = y, random_state = 42)\nX_sub_train, X_sub_test, y_sub_train, y_sub_test = train_test_split(X_sub, y_sub, test_size = 0.2, stratify  = y_sub, random_state = 42)","b87e46e1":"sm = SMOTE(random_state = 666)\n\n# New training dataset with smote applied to it\nX_train_s, y_train_s = sm.fit_resample(X_train, y_train.ravel())","de259396":"plt.subplot(121)\nplt.pie(x = [len(y_train_s)- sum(y_train_s), sum(y_train_s)],\n        labels = [\"Left\", \"Right\"], autopct='%1.2f%%', explode = (0, 0.01))\n\nplt.subplot(122)\nsns.countplot(x = y_train_s)\nfraudnt_s, fraud_s = len(y_train_s)- sum(y_train_s), sum(y_train_s)\nplt.text(0, fraudnt_s\/\/2, fraudnt_s, fontsize = 20,  horizontalalignment='center')\nplt.text(1, fraud_s\/\/2, fraud_s, fontsize = 20, horizontalalignment='center')","7846a5ff":"def model_eval(y_actual, predicted):\n    \"\"\"\n    Evaluates a model's accuracy using classification report and confusion matrix\n\n        Parameters\n        ----------\n        y_actual : pandas.core.series.Series\n            target pandas series\n        \n        predicted : pandas.core.series.Series\n            pandas series of predictions\n\n        Returns\n        -------\n        classification report followed by confusion matrix for the arrays given\n    \"\"\"\n    print(classification_report(y_actual, predicted, target_names = ['Not Fraud', 'Fraud']))\n    sns.heatmap(data = confusion_matrix(y_test, predicted), annot = True,  cmap = \"coolwarm_r\",center = 0)","779ce504":"lr_clf = LogisticRegression(max_iter = 1000)\n\nlr_clf.fit(X_train, y_train)\nprediction_lr_clf = lr_clf.predict(X_test)\nmodel_eval(y_test, prediction_lr_clf)\ntype(y_test)","ea71abf4":"lr_clf_sub = LogisticRegression(max_iter = 1000)\n\nlr_clf_sub.fit(X_sub_train, y_sub_train)\nprediction_lr_clf_sub = lr_clf.predict(X_test)\nprint(classification_report(y_test, prediction_lr_clf_sub, target_names = ['Not Fraud', 'Fraud']))\nsns.heatmap(data = confusion_matrix(y_test, prediction_lr_clf_sub), annot = True,  cmap = \"coolwarm_r\",center = 0)","3030594e":"lr_clf_smote = LogisticRegression(max_iter = 1000)\nlr_clf_smote.fit(X_train_s, y_train_s)\n\nprediction_lr_clf_smote = lr_clf_smote.predict(X_test)\nmodel_eval(y_test, prediction_lr_clf_smote)","0e6fda9c":"scale_val = fraudnt\/\/fraud # inverse of the ratio of number of each class present \nprint(f'The scale value is {scale_val}')","52b0afb8":"xgb_clf = xgb.XGBClassifier(n_estimators = 1000, verbosity = 1, scale_pos_weight = scale_val,\n                            max_depth = 5, gamma = 0.2, colsamplebytree = 0.8, regalpha = 0.1, \n                            subsample = 0.2, learning_rate = 0.3)","27d0cc32":"xgb_clf.fit(X_train, y_train)\nprediction_xgb_clf = xgb_clf.predict(X_test)\nmodel_eval(y_test, prediction_xgb_clf)","df255e98":"xgb.plot_importance(xgb_clf)","385fcb4e":"def prc_with_model(X_test, y_test, model):\n    \"\"\"\n    Plots the precision-recall curve for a given model along with the legend\n\n        Parameters\n        ----------\n        X_test : pandas.DataFrame\n            input pandas dataframe for validation\n        \n        y_test : pandas.core.series.Series\n            target pandas series for validation\n            \n        model : str\n            name of the model to be evaluated as a string\n\n        Returns\n        -------\n        classification report followed by confusion matrix for the arrays given\n    \"\"\"    \n    y_prob = eval(model).predict_proba(X_test)[:,1]\n    precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n    plt.plot(precision, recall, label = model)\n\n    \ndef auc_score(y_test, model):\n    \"\"\"\n    Evaluates a model's AUC score and prints it in a formatted string\n\n        Parameters\n        ----------\n        y_test : pandas.core.series.Series\n            target pandas series for validation\n            \n        model : str\n            name of the model to be evaluated as a string\n\n        Returns\n        -------\n        Prints the AUC score of the model after formatting it in a string\n    \"\"\"\n    print(f\"AUC score for {model} is: \", roc_auc_score(y_test, eval(f'prediction_{model}')))\n    \n    \nmodels = ['lr_clf', 'lr_clf_sub', 'lr_clf_smote', 'xgb_clf']\nfor model in models:\n    prc_with_model(X_test, y_test, model)\n    auc_score(y_test, model)\nplt.legend()","00a9ab87":"perm = PermutationImportance(xgb_clf, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","8e8fccae":"pdp_dist = pdp.pdp_isolate(model=xgb_clf, dataset=X_test, model_features= X_test.columns, \n                           feature='scaled_time')\npdp.pdp_plot(pdp_dist, 'scaled_time')\nplt.show()","bdf018bf":"features_to_plot = ['scaled_time', 'scaled_amount']\ninter1  =  pdp.pdp_interact(model=xgb_clf, dataset=X_test, model_features= X_test.columns, features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\nplt.show()","491c93db":"data_for_prediction = X.iloc[X_test[y_test == 1].index[1]]\nexplainer = shap.TreeExplainer(xgb_clf)\ndata_for_prediction_array = data_for_prediction.values.reshape(1, -1)\nshap_values = explainer.shap_values(data_for_prediction_array)\nprint(xgb_clf.predict_proba(data_for_prediction_array))\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values, data_for_prediction_array)","4aa5ad4e":"data_for_prediction = X_test.iloc[5]\nexplainer = shap.TreeExplainer(xgb_clf)\ndata_for_prediction_array = data_for_prediction.values.reshape(1, -1)\nshap_values = explainer.shap_values(data_for_prediction_array)\nprint(xgb_clf.predict_proba(data_for_prediction_array))\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values, data_for_prediction_array)","f7e47a0b":"explainer = shap.TreeExplainer(xgb_clf)\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values, X_test)","9a50bbeb":"explainer = shap.TreeExplainer(xgb_clf)\nshap_values = explainer.shap_values(X)\n\nshap.dependence_plot('scaled_time', shap_values, X, interaction_index=\"scaled_amount\",  x_jitter=1, dot_size=10)","ebf2b56a":"The recall in this model is excellent! 92% of the frauds are caught, but the precision takes a hit. A lot of normal transactions are flagged, this means many unhappy customers.","74197d80":"XGBoost has a parameter which gives higher priority to the minority class making the model fairly unbiased as the weights for both the categories remains the same.","7faf4748":"Not only do fraudulent transactions cash out larger sums of money from banks, they also fail to follow the seasonal nature of the time as well.","bdc5bedc":"# Feature Overview","347b9d1b":"Even though the recall value is lower, there is higher precision here giving us a better F1 score, making XGBoost a better all round model.","edc33504":"The accuracy maybe high but the recall is sub-par at best. We could only catch 62% of the frauds.","a0d8caa0":"To mitigate the imbalance in the dataset, we can either use a subset of the non fraud cases to match the number of fraud cases, i.e. undersampling. Alternatively we can increase the number of fraud cases using SMOTE, i.e. oversampling.","aadf6548":"# Data Preprocessing","ff09d2a8":"Even though Logistic Regression has a higher AUC than XGBoost, I will be choosing XGBoost for further analysis as it is compatible with the libraries being used for machine learning explainibility.","8e0afeae":"Perfectly balanced!","cbdaeffb":"> Benford\u2019s Law, also known as the Law of First Digits or the Phenomenon of Significant Digits, is the finding that the first digits (or numerals to be exact) of the numbers found in series of records of the most varied sources do not display a uniform distribution, but rather are arranged in such a way that the digit \u201c1\u201d is the most frequent, followed by \u201c2\u201d, \u201c3\u201d, and so in a successively decreasing manner down to \u201c9\u201d.\n\nWe can see that the fraudulent transactions fail to follow this law, hence we can infer that something is wrong with those transactions.","65d04b8f":"Using such an imbalanced dataset for machine learning can lead to very poor performance in real life. We will see some ways that we can mitigate this issue later in this notebook.","dedef19d":"**<font size = 6>If you liked this notebook, don't forget to upvote it!<\/font>**","43166ec6":"t-SNE helps us visualize the data points in a lower dimension that can be understood by us. Here, we can observe that these two classes can be separated by using a simple straight line with little to no overlapping, hence Logistic regression can work well on this dataset.","ce53a2e6":"## XGBoost with scale_pos_weight","3dd2a417":"Looks like no feature is correlated, but this is the result of the highly imbalanced nature of the dataset, taking a subset of the dataset will give us a clearer picture.","1829dbec":"We can see clear differences between the densities of Fraud and Valid classes. Valid cases are usually concentrated together and hence have sharp density plots whereas Fraud transactions are more spread out.","94e81bd1":"No Null values, lesser work to do!","8f46c2fc":"## Logistic Regression on sub-sample","e38a690c":"The Class column in looking highly skewed. We will see how much imbalance there really is.","e31f6a8b":"# Machine Learning","b37fbaca":"This is much better at telling us the true correlation between the features.","ac5e0684":"## Logistic Regression on original dataset ","365bb242":"Using a subset gave us some minor improvement in recall but it still isn't enough, lets see what oversampling does to the model.","0d44ace6":"## Logistic Regression with SMOTE","ad2303e8":"The amount variable is highly left skewed as most people take part in low volume banking transactions but the time variable has visible seasonality indicating peak times and breaks that are part of a human's daily routine."}}