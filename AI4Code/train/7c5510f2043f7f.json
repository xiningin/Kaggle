{"cell_type":{"7236188c":"code","cc79b7e2":"code","269aeee7":"code","c5b94e1b":"code","63ac9b4e":"code","eb30aaba":"code","9e8e92bf":"code","f38626fe":"code","681c88ab":"code","c8a02226":"code","7db576a7":"code","f4a250b7":"code","ae5baf17":"code","17429584":"code","3ccf4d5c":"code","53b5c0ea":"code","02df6672":"code","7a57a1de":"markdown","272283b1":"markdown","fa4aaad5":"markdown"},"source":{"7236188c":"import warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\nimport os\nimport re\nimport json\nimport glob\nfrom collections import defaultdict, Counter\nfrom textblob import TextBlob\nfrom functools import partial\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\n\nimport nltk\nimport spacy\nnlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\nnlp.max_length = 4000000\nfrom nltk.probability import FreqDist\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom tqdm.autonotebook import tqdm\nimport string\n\n%matplotlib inline\n\nos.listdir('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/')","cc79b7e2":"train_df = pd.read_csv('..\/input\/show-us-the-datanew\/new_train.csv')\nsample_sub = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')\ntrain_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\ntest_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/test'\ntrain_df.head()","269aeee7":"sample_sub.head()","c5b94e1b":"existing_set = Counter(list(train_df['cleaned_label']))\nsorted_set = sorted(existing_set.items(), key=lambda x: x[1], reverse=True)\ntotal_set = 0\nfor name, num in sorted_set:\n#     print(\"{}: {}\".format(name, num))\n    total_set += num\nprint(total_set)\nmostcommon_set = sorted_set[:10]\nx, y = zip(*mostcommon_set)\nplt.figure(figsize=(50,30))\nplt.margins(0.02)\nplt.bar(x, y)\nplt.xlabel('Datasets', fontsize=50)\nplt.ylabel('Frequency of Datasets', fontsize=50)\nplt.yticks(fontsize=40)\nplt.xticks(rotation=60, fontsize=40)\nplt.tight_layout(pad=0)\nplt.title('Freq of 10 Most Common Datasets in cleaned_label', fontsize=60)\nplt.show()\n# existing_set = set(train_df['cleaned_label'])\n# for item in existing_set:\n#     print(\"the {} has found {}\".format(item, list(train_df['cleaned_label']).count(item)))\nexisting_labels = train_df['cleaned_label'].unique()\nprint(len(existing_labels))\n# print(existing_labels)","63ac9b4e":"def clean_text(txt, nlp=nlp):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()","eb30aaba":"def clean_text_advanced(txt, nlp=nlp):\n    target_size = 5\n    curr_size = 0\n    lemma_sentence = []\n    cleaned_txt = re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n    word_list = list(set(cleaned_txt.split()))\n    word_set = Counter(word_list)\n    sorted_set = sorted(word_set.items(), key=lambda x: x[1], reverse=True)\n    for name, num in sorted_set:\n        name = nlp(name)\n        if name[0].is_stop or name[0].is_digit:\n            continue\n        else:\n            lemma_sentence.append(name[0].lemma_)\n            curr_size += 1\n        if curr_size == target_size:\n            return ' '.join(lemma_sentence)\n    return ' '.join(lemma_sentence)\n#     word_dict = dict()\n    # print(word_set)\n#     for name, num in word_set.items():\n#         name = nlp(name)\n#         if not name[0].is_stop:\n#             name = name[0].lemma_\n#         else:\n#             continue\n#         if word_dict.get(name) is None:\n#             word_dict[name] = num\n#         else:\n#             word_dict[name] += num\n#     sorted_set = sorted(word_dict.items(), key=lambda x: x[1], reverse=True)\n#     return sorted_set","9e8e92bf":"# print(clean_text_advanced(\"I am a boy. You are a Girl dataset datasets has have.\"))\n# print(clean_text(\"I am a boy. You are a Girl dataset.\"))","f38626fe":"def read_append_return(filename, train_files_path=train_files_path, output='text'):\n    \"\"\"\n    Function to read json file and then return the text data from them and append to the dataframe\n    \"\"\"\n    json_path = os.path.join(train_files_path, (filename+'.json'))\n    headings = [ ]\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","681c88ab":"# %%time\n# tqdm.pandas()\n# train_df['text'] = train_df['Id'].progress_apply(read_append_return)\n# train_df['most_common'] = train_df['text'].progress_apply(clean_text_advanced)\n\n# train_df.to_csv('new_train.csv', index=False)","c8a02226":"train_df.head()","7db576a7":"%%time\ntqdm.pandas()\nsample_sub['text'] = sample_sub['Id'].progress_apply(partial(read_append_return, train_files_path=test_files_path))\nsample_sub['most_common'] = sample_sub['text'].progress_apply(clean_text_advanced)","f4a250b7":"sample_sub.head()","ae5baf17":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","17429584":"temp_1 = [x.lower() for x in train_df['dataset_label'].unique()]\ntemp_2 = [x.lower() for x in train_df['dataset_title'].unique()]\ntemp_3 = [x.lower() for x in train_df['cleaned_label'].unique()]\n\nexisting_labels = set(temp_1 + temp_2 + temp_3)\nprint(len(existing_labels))\nid_list = []\nlables_list = []\nfor index, row in tqdm(sample_sub.iterrows()):\n    sample_text = row['text']\n    sample_most_common = row['most_common']\n    row_id = row['Id']\n    # print(train_df['text'] == text_cleaning(sample_text))\n    sample_text = clean_text(sample_text)\n    cleaned_labels = []\n    temp_df = train_df[train_df['most_common'].progress_apply(partial(jaccard, str2=sample_most_common)) > 0.1]\n    cleaned_labels = temp_df['cleaned_label'].to_list()\n    print(len(set(cleaned_labels)))\n    print(set(cleaned_labels))\n#     for known_label in existing_labels:\n#         if known_label in sample_text.lower():\n#             print(\"matching: {}\".format(known_label))\n#             cleaned_labels.append(clean_text(known_label))\n#     print(set(cleaned_labels))\n    cleaned_labels = set(cleaned_labels)\n    lables_list.append('|'.join(cleaned_labels))\n    id_list.append(row_id)","3ccf4d5c":"submission = pd.DataFrame()\nsubmission['Id'] = id_list\nsubmission['PredictionString'] = lables_list","53b5c0ea":"# pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\nsubmission.head()","02df6672":"submission.to_csv('submission.csv', index=False)","7a57a1de":"# Example of clean_text effect","272283b1":"# 2. How many kinds of cleaned_labels?\n\n130 cleaned_labels","fa4aaad5":"# 1. Load Data"}}