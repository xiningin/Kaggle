{"cell_type":{"6f6164de":"code","15bb6fbb":"code","528d653a":"code","d60a0268":"code","0550a6bb":"code","fb621f89":"code","330297d2":"code","8bcaef9e":"code","80855f7d":"code","16f6f939":"code","bd140dac":"markdown","f1fef225":"markdown"},"source":{"6f6164de":"!pip install ProgressBar","15bb6fbb":"import numpy as np \nimport pandas as pd \nfrom progressbar import ProgressBar\nimport glob\nimport os","528d653a":"frag = glob.glob(\"..\/input\/predict-volcanic-eruptions-ingv-oe\/train\/*\")","d60a0268":"def preprocessing_train(name):\n    train_df = pd.read_csv('..\/input\/predict-volcanic-eruptions-ingv-oe\/train.csv')\n    dataframe = pd.DataFrame()\n    pbar = ProgressBar()\n    index=[]\n    for i in range(0,len(frag[0:5])):\n        index = np.append(index,os.path.splitext(frag[i].split('{}\/'.format(name))[1])[0])\n    index = index.astype('int')\n\n    for i in pbar(index):\n        df = pd.read_csv('..\/input\/predict-volcanic-eruptions-ingv-oe\/{}\/{}.csv'.format(name,i))\n        value = (train_df['time_to_eruption'][train_df['segment_id']==i])\n        value = np.array(value)\n        valuelist = np.arange(value,value+60001,1)\n        df['time_to_eruption']=valuelist\n        dataframe = pd.concat([dataframe,df],axis=0)\n        del df\n    return(dataframe)\n\ndef preprocessing_test(name):\n    index = []\n    frag = glob.glob(\"..\/input\/predict-volcanic-eruptions-ingv-oe\/{}\/*\".format(name))\n    df=pd.DataFrame()\n\n    pbar = ProgressBar()\n    for i in pbar(frag):\n        df = np.append(df,pd.read_csv(i).mean())\n    \n    df = pd.DataFrame(df.reshape(len(frag),10))  \n\n    for i in range(0,len(frag)):\n        index = np.append(index,os.path.splitext(frag[i].split('{}\/'.format(name))[1])[0])\n        \n    df['segment_id']=index\n    df['segment_id']=df['segment_id'].astype(int)\n    df.columns= ['sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_9', 'sensor_10','segment_id']\n    return(df)\n\ndef fill_na(data):\n    for i in data.columns:\n        data[i] = data[i].fillna(np.mean(data[i]))","0550a6bb":"%%time\ndata_x = preprocessing_train('train')\ndata_y= preprocessing_test('test')","fb621f89":"for i in range(0,60001):\n    print(i,data_x.iloc[i,-1])","330297d2":"fill_na(data_x)\nfill_na(data_y)","8bcaef9e":"y = data_x['time_to_eruption']\nx= data_x.iloc[:,0:-1]\n\ndel data_x","80855f7d":"%%time \n\nfrom  sklearn.model_selection import train_test_split\nimport lightgbm as lgb\n\nXt, Xv, Yt, Yv = train_test_split(x, y, test_size =0.2, shuffle=False)\n\nparams = {\n        'objective': 'regression', #specify how is the dependet variable, binary can be used for logistic regression or log loss classification\n        'max_bin': 600, #max number of bins that features values will be bucketed in. Small number may reduce training accuracy but may increase general power\n        'learning_rate': 0.02, #learning_rate refers to the step size at each interation while moving toward an optimal point\n        'num_leaves': 80, # maximum number of leaves in a tree, where a leave is a final termination of a tree\n        'metric' : 'mae'\n}\n\n\nlgb_train = lgb.Dataset(Xt, Yt)\nlgb_eval = lgb.Dataset(Xv, Yv, reference=lgb_train)\n        #lightgbm need to take as argument lightgbm dataset, it is required to make this trasformation\n\nmodel = lgb.train(\n            params, lgb_train, #it is required to insert the parameters, then the train set\n            valid_sets=[lgb_train, lgb_eval],\n            verbose_eval=100,\n            num_boost_round=1500, # number of boosting iterations \n            early_stopping_rounds=15 # will stop training if one metric of one validation data doesn\u2019t improve in last early_stopping_round rounds, so if \n            #  for ten 'epochs' the model will stop, in this way the num_boost_round is a maximum value.  \n)  \n\ny_pred = model.predict(Xv)\ny_true = np.array(Yv)\nprint('mean absolute error:',mae(y_true, y_pred))\n\nprediction = model.predict(data_y.iloc[:,0:-1])","16f6f939":"sub_df = pd.DataFrame(data_y['segment_id'])\nsub_df = pd.concat([sub_df,pd.Series(prediction)],axis=1)\nsample_submission=pd.read_csv('..\/input\/predict-volcanic-eruptions-ingv-oe\/sample_submission.csv')\n        \nsample_submission = pd.merge(sample_submission,sub_df, on =['segment_id'])\nsample_submission = sample_submission.drop(columns=['time_to_eruption'])\nsample_submission.columns = ['segment_id', 'time_to_eruption']\nsample_submission.to_csv('sample_submission.csv', header=True, index=False)\nprint('saved :)')","bd140dac":"I used just 500 out of the 4000 available dataset cause otherwise it crashs. ","f1fef225":"In this notebook I've tried a new strategy, I specified for each row the time, starting from the initial time_to_eruption time associated with each dataset in order to perform a regression model on each row. I'm open to suggetion about this new strategy :)"}}