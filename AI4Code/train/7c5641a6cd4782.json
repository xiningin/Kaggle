{"cell_type":{"811ea0c0":"code","ddb84981":"code","1dd95729":"code","e62f45bb":"code","f854e6f5":"code","02ca9cbc":"code","abd1d8ec":"code","b6d7470c":"code","b45415f4":"code","99349a02":"code","844b7a69":"code","eaf58243":"code","bc92fa01":"code","ebc29fa8":"code","c58d4ba6":"code","f4ae7c62":"code","fc7a4f25":"code","aef20c8c":"code","97f32078":"code","efc96260":"code","e4512a91":"code","411c1124":"code","2f11fc30":"code","f21506e0":"code","2caf7fd3":"code","85affb2e":"code","c6c0e39c":"code","3c4c48e9":"code","421d31b3":"code","849c1813":"code","70b4dbde":"code","732164cd":"code","9df2e95d":"code","121598f1":"markdown","59f10d83":"markdown","2b27fc38":"markdown","dc7eafda":"markdown","74039713":"markdown","cd37d866":"markdown","ea85bcce":"markdown","051ec465":"markdown","67540b4c":"markdown","7f0916f9":"markdown","e0d05aa2":"markdown","f23840f9":"markdown"},"source":{"811ea0c0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\npd.options.display.float_format = '{:20,.2f}'.format\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom  warnings import simplefilter\nfrom sklearn.exceptions import ConvergenceWarning\nsimplefilter(\"ignore\", category=ConvergenceWarning)\n# Any results you write to the current directory are saved as output.","ddb84981":"train = pd.read_csv('..\/input\/covid19-global-forecasting-week-1\/train.csv')\ntest = pd.read_csv('..\/input\/covid19-global-forecasting-week-1\/test.csv')\nsubmission = pd.read_csv('..\/input\/covid19-global-forecasting-week-1\/submission.csv')\n","1dd95729":"test['Date']=test.Date.astype('datetime64[ns]')","e62f45bb":"train['key']=train['Province\/State'].astype('str')+ \" \" + train['Country\/Region'].astype('str')+ \" \" +train['Lat'].astype('str')+ \" \"  +train['Long'].astype('str')\n\ntest['key']=test['Province\/State'].astype('str')+ \" \" + test['Country\/Region'].astype('str')+ \" \" +test['Lat'].astype('str')+ \" \"  +test['Long'].astype('str')\n\ntrain.describe()","f854e6f5":"train.columns","02ca9cbc":"daily_analysis=train.groupby(['Date']).sum()","abd1d8ec":"daily_analysis[['ConfirmedCases','Fatalities']].plot()","b6d7470c":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n\ndef RMSLE(pred,actual):\n    return np.sqrt(np.mean(np.power((np.log(pred+1)-np.log(actual+1)),2)))","b45415f4":"train_lag_1=train.groupby(['key']).shift(periods=1)\ntrain_lag_2=train.groupby(['key']).shift(periods=2)\ntrain_lag_3=train.groupby(['key']).shift(periods=3)\ntrain['lag_1_ConfirmedCases']=train_lag_1['ConfirmedCases']\ntrain['lag_1_Fatalities']=train_lag_1['Fatalities']\ntrain['lag_2_ConfirmedCases']=train_lag_2['ConfirmedCases']\ntrain['lag_2_Fatalities']=train_lag_2['Fatalities']\ntrain['lag_3_ConfirmedCases']=train_lag_3['ConfirmedCases']\ntrain['lag_3_Fatalities']=train_lag_3['Fatalities']\ntrain","99349a02":"\ndef pred_ets(fcastperiod,fcastperiod1,actual,ffcast,type_ck='ConfirmedCases',verbose=False):\n    \n    actual=actual[actual[type_ck]>0]\n    index=pd.date_range(start=ffcast.index[0], end=ffcast.index[-1], freq='D')\n    data=ffcast[type_ck].values\n    ffcast1 = pd.Series(data, index)\n    index=pd.date_range(start=actual.index[0], end=actual.index[-1], freq='D')\n    data=actual[type_ck].values\n    daily_analysis_dat = pd.Series(data, index)\n    livestock2=daily_analysis_dat\n    fit=[]\n    fcast=[]\n    fname=[]\n    try:\n        fit1 = SimpleExpSmoothing(livestock2).fit()\n        fcast1 = fit1.forecast(fcastperiod1).rename(\"SES\")\n        fit.append(fit1)\n        fcast.append(fcast1)\n        fname.append('SES')\n    except:\n        1==1\n    try:\n        fit2 = Holt(livestock2).fit()\n        fcast2 = fit2.forecast(fcastperiod1).rename(\"Holt\")\n        fit.append(fit2)\n        fcast.append(fcast2)\n        fname.append('Holt')\n    except:\n        1==1\n    try:\n        fit3 = Holt(livestock2, exponential=True).fit()\n        fcast3 = fit3.forecast(fcastperiod1).rename(\"Exponential\")\n        fit.append(fit3)\n        fcast.append(fcast3)\n        fname.append('Exponential')\n    except:\n        1==1\n    try:\n        fit4 = Holt(livestock2, damped=True).fit(damping_slope=0.98)\n        fcast4 = fit4.forecast(fcastperiod1).rename(\"AdditiveDamped\")\n        fit.append(fit4)\n        fcast.append(fcast4)\n        fname.append('AdditiveDamped')\n    except:\n        1==1\n    try:\n        fit5 = Holt(livestock2, exponential=True, damped=True).fit()\n        fcast5 = fit5.forecast(fcastperiod1).rename(\"MultiplicativeDamped\")\n        fit.append(fit5)\n        fcast.append(fcast5)\n        fname.append('MultiplicativeDamped')\n    except:\n        1==1\n    try:\n        fit6 = Holt(livestock2, damped=True).fit()\n        fcast6 = fit6.forecast(fcastperiod1).rename(\"AdditiveDampedC\")\n        fit.append(fit6)\n        fcast.append(fcast6)\n        fname.append('AdditiveDampedC')\n    except:\n        1==1\n\n\n    def RMSLE(pred,actual):\n        return np.sqrt(np.mean(np.power((np.log(pred+1)-np.log(actual+1)),2)))\n    \n    pred_all_result=pd.concat([pd.DataFrame(k.fittedvalues) for k in fit],axis=1)\n    pred_all_result.columns=fname\n    all_result=pd.concat([pd.DataFrame(k) for k in fcast],axis=1)\n    col_chk=[]\n    vvvl=ffcast[type_ck].values.shape[0]\n    for k in all_result.columns:\n        if verbose: print(\"actual value for method %s  is = %s\" % (k,RMSLE(all_result[k].values,ffcast[type_ck].values)))\n        if RMSLE(all_result[k].values[:vvvl],ffcast[type_ck].values) is not np.nan:\n            col_chk.append(k)\n    col_chk_f=[]\n    min_acc=-1\n    for k in col_chk:\n        acc=RMSLE(pred_all_result[k].values,actual[type_ck].values)\n        #if k =='Exponential' and acc>0.01:\n                #acc=acc-0.01\n        if verbose: print(\"pred value for method %s  is = %s\" % (k,acc))\n        if acc is not np.nan:\n            col_chk_f.append(k)\n            if min_acc==-1:\n                min_acc=acc\n                model_select=k\n            elif acc<min_acc:\n                min_acc=acc\n                model_select=k\n    all_result=all_result.append(pred_all_result,sort=False)\n\n    all_result['best_model']=model_select\n    all_result['best_pred']=all_result[model_select]\n    return all_result\n    #return pred_all_result,all_result\n","844b7a69":"import sys\norig_stdout = sys.stdout\n\nFatalities_all_result_final=pd.DataFrame()\nConfirmedCases_all_result_Final=pd.DataFrame()\nfor keys in train['key'].unique():\n    chk=train[train['key']==keys]\n    chk.index=chk.Date\n    fcastperiod=0\n    fcastperiod1=35\n    actual=chk[:chk.shape[0]-fcastperiod]\n    ffcast=chk[chk.shape[0]-fcastperiod-1:]\n    ffcast\n    try:\n        Fatalities_all_result_1=pred_ets(fcastperiod,fcastperiod1,actual,ffcast,'Fatalities').reset_index()\n        \n        \n    except:\n        Fatalities_all_result_1=pd.DataFrame(pd.date_range(start=chk.Date.min(), periods=60+fcastperiod1+1, freq='D')[1:])\n        Fatalities_all_result_1.columns=['index']\n        Fatalities_all_result_1['best_model']='naive'\n        Fatalities_all_result_1['best_pred']=0\n        \n    Fatalities_all_result_1['key']=keys\n    Fatalities_all_result_final=Fatalities_all_result_final.append(Fatalities_all_result_1,sort=True)\n    try:\n        ConfirmedCases_all_result_1=pred_ets(fcastperiod,fcastperiod1,actual,ffcast,'ConfirmedCases').reset_index()\n\n        \n    except:\n        ConfirmedCases_all_result_1=pd.DataFrame(pd.date_range(start=train.Date.min(), periods=60+fcastperiod1+1, freq='D')[1:])\n        ConfirmedCases_all_result_1.columns=['index']\n        ConfirmedCases_all_result_1['best_model']='naive'\n        ConfirmedCases_all_result_1['best_pred']=1\n    \n    ConfirmedCases_all_result_1['key']=keys\n    ConfirmedCases_all_result_Final=ConfirmedCases_all_result_Final.append(ConfirmedCases_all_result_1,sort=True)\n    print( ' done for %s' % keys)\nsys.stdout = orig_stdout","eaf58243":"ConfirmedCases_all_result_Final.rename(columns={'index':'Date'},inplace=True)\nFatalities_all_result_final.rename(columns={'index':'Date'},inplace=True)\nConfirmedCases_all_result_Final['best_pred']=np.where(ConfirmedCases_all_result_Final['best_pred'] is np.nan , 0,\n                                                       ConfirmedCases_all_result_Final['best_pred'] )\nFatalities_all_result_final['best_pred']=np.where(Fatalities_all_result_final['best_pred'] is np.nan , 0 ,\n                                                       Fatalities_all_result_final['best_pred'] )\nConfirmedCases_all_result_Final['best_pred']=np.where(ConfirmedCases_all_result_Final['best_pred'] <0 , 0,\n                                                       ConfirmedCases_all_result_Final['best_pred'] )\nFatalities_all_result_final['best_pred']=np.where(Fatalities_all_result_final['best_pred'] <0 , 0 ,\n                                                       Fatalities_all_result_final['best_pred'] )\nConfirmedCases_all_result_Final['best_pred_1']=np.where(ConfirmedCases_all_result_Final['AdditiveDamped'] is np.nan , ConfirmedCases_all_result_Final['best_pred'] ,\n                                                       ConfirmedCases_all_result_Final['AdditiveDamped'] )\nFatalities_all_result_final['best_pred_1']=np.where(Fatalities_all_result_final['AdditiveDamped'] is np.nan , Fatalities_all_result_final['best_pred'] ,\n                                                       Fatalities_all_result_final['AdditiveDamped'] )\nConfirmedCases_all_result_Final['best_pred_1']=np.where(ConfirmedCases_all_result_Final['best_pred'] is np.nan , 0,\n                                                       ConfirmedCases_all_result_Final['best_pred'] )\nFatalities_all_result_final['best_pred_1']=np.where(Fatalities_all_result_final['best_pred'] is np.nan , 0 ,\n                                                       Fatalities_all_result_final['best_pred'] )\nConfirmedCases_all_result_Final['best_pred_1']=np.where(ConfirmedCases_all_result_Final['best_pred'] <0 , 0,\n                                                       ConfirmedCases_all_result_Final['best_pred'] )\nFatalities_all_result_final['best_pred_1']=np.where(Fatalities_all_result_final['best_pred'] <0 , 0 ,\n                                                       Fatalities_all_result_final['best_pred'] )","bc92fa01":"test['Date']=test.Date.astype('datetime64[ns]')","ebc29fa8":"\neval1 = ConfirmedCases_all_result_Final[['key','Date','best_pred','best_pred_1']].merge(test, how='right', on=['key','Date'])\neval1.rename(columns={'best_pred':'ConfirmedCases'},inplace=True)\neval1['ConfirmedCases']=eval1['ConfirmedCases'].fillna(0)\neval1","c58d4ba6":"eval2 = Fatalities_all_result_final[['key','Date','best_pred','best_pred_1']].merge(test, how='right', on=['key','Date'])\n\neval2.rename(columns={'best_pred':'Fatalities'},inplace=True)\neval2['Fatalities']=eval2['Fatalities'].fillna(0)\neval2","f4ae7c62":"sub_prep = eval1[['ForecastId','ConfirmedCases','key']].merge(eval2[['ForecastId','Fatalities']], on=['ForecastId'],  how='left')\nsub_prep","fc7a4f25":"sub = sub_prep.merge(submission['ForecastId'], on=['ForecastId'],  how='right')\nsub","aef20c8c":"sub=sub[['ForecastId','ConfirmedCases','Fatalities']]\nsub=sub.sort_values('ForecastId')\nsub","97f32078":"sub.to_csv('submission.csv',header=['ForecastId','ConfirmedCases','Fatalities'],index=False)","efc96260":"#sub.to_csv('submission.csv')\nsub","e4512a91":"train['Date']=train.Date.astype('datetime64[ns]')\nverify=train[['key','Date','ConfirmedCases','Fatalities']].merge(test[['key','Date','ForecastId']], how='inner', on=['key','Date'])\npred=verify[['ForecastId']].merge(sub, how='inner', on=['ForecastId'])","411c1124":"RMSLE(pred['Fatalities'].values,verify['Fatalities'].values)","2f11fc30":"RMSLE(pred['ConfirmedCases'].values,verify['ConfirmedCases'].values)","f21506e0":"ConfirmedCases_all_result_Final","2caf7fd3":"best_model_key=ConfirmedCases_all_result_Final[['key','best_model']].drop_duplicates()\nmax_number_current=train.groupby('key').max()[['ConfirmedCases','Fatalities']].reset_index()\nbest_model_key=best_model_key.merge(max_number_current,on='key',how='left')\nbest_model_key=best_model_key.sort_values('ConfirmedCases',ascending=False).reset_index(drop=True)","85affb2e":"for j in best_model_key.best_model.unique():\n    print('Top Countries\/District currently under %s growth rate' % str (j))\n    print(best_model_key[best_model_key['best_model']==j].head())\n    print('-'*30)\n    print('-'*30)","c6c0e39c":"best_model_key.key.unique()","3c4c48e9":"train","421d31b3":"train_ck=train.groupby(['Country\/Region','Date']).sum().reset_index()\ntrain_ck['key']=train_ck['Country\/Region']","849c1813":"\nFatalities_all_result_final=pd.DataFrame()\nConfirmedCases_all_result_Final=pd.DataFrame()\nfor keys in train_ck['key'].unique():\n    chk=train_ck[train_ck['key']==keys]\n    chk.index=chk.Date\n    fcastperiod=0\n    fcastperiod1=35\n    actual=chk[:chk.shape[0]-fcastperiod]\n    ffcast=chk[chk.shape[0]-fcastperiod-1:]\n    ffcast\n    try:\n        Fatalities_all_result_1=pred_ets(fcastperiod,fcastperiod1,actual,ffcast,'Fatalities').reset_index()\n        \n        \n    except:\n        Fatalities_all_result_1=pd.DataFrame(pd.date_range(start=train.Date.min(), periods=60+fcastperiod1+1, freq='D')[1:])\n        Fatalities_all_result_1.columns=['index']\n        Fatalities_all_result_1['best_model']='naive'\n        Fatalities_all_result_1['best_pred']=0\n        \n    Fatalities_all_result_1['key']=keys\n    Fatalities_all_result_final=Fatalities_all_result_final.append(Fatalities_all_result_1,sort=True)\n    try:\n        ConfirmedCases_all_result_1=pred_ets(fcastperiod,fcastperiod1,actual,ffcast,'ConfirmedCases').reset_index()\n\n        \n    except:\n        ConfirmedCases_all_result_1=pd.DataFrame(pd.date_range(start=train.Date.min(), periods=60+fcastperiod1+1, freq='D')[1:])\n        ConfirmedCases_all_result_1.columns=['index']\n        ConfirmedCases_all_result_1['best_model']='naive'\n        ConfirmedCases_all_result_1['best_pred']=1\n    \n    ConfirmedCases_all_result_1['key']=keys\n    ConfirmedCases_all_result_Final=ConfirmedCases_all_result_Final.append(ConfirmedCases_all_result_1,sort=True)\n    print( ' done for %s' % keys)","70b4dbde":"ConfirmedCases_all_result_Final.rename(columns={'index':'Date'},inplace=True)\nFatalities_all_result_final.rename(columns={'index':'Date'},inplace=True)\nConfirmedCases_all_result_Final['best_pred']=np.where(ConfirmedCases_all_result_Final['best_pred'] is np.nan , 0,\n                                                       ConfirmedCases_all_result_Final['best_pred'] )\nFatalities_all_result_final['best_pred']=np.where(Fatalities_all_result_final['best_pred'] is np.nan , 0 ,\n                                                       Fatalities_all_result_final['best_pred'] )\nConfirmedCases_all_result_Final['best_pred']=np.where(ConfirmedCases_all_result_Final['best_pred'] <0 , 0,\n                                                       ConfirmedCases_all_result_Final['best_pred'] )\nFatalities_all_result_final['best_pred']=np.where(Fatalities_all_result_final['best_pred'] <0 , 0 ,\n                                                       Fatalities_all_result_final['best_pred'] )\nConfirmedCases_all_result_Final['best_pred_1']=np.where(ConfirmedCases_all_result_Final['AdditiveDamped'] is np.nan , ConfirmedCases_all_result_Final['best_pred'] ,\n                                                       ConfirmedCases_all_result_Final['AdditiveDamped'] )\nFatalities_all_result_final['best_pred_1']=np.where(Fatalities_all_result_final['AdditiveDamped'] is np.nan , Fatalities_all_result_final['best_pred'] ,\n                                                       Fatalities_all_result_final['AdditiveDamped'] )\nConfirmedCases_all_result_Final['best_pred_1']=np.where(ConfirmedCases_all_result_Final['best_pred'] is np.nan , 0,\n                                                       ConfirmedCases_all_result_Final['best_pred'] )\nFatalities_all_result_final['best_pred_1']=np.where(Fatalities_all_result_final['best_pred'] is np.nan , 0 ,\n                                                       Fatalities_all_result_final['best_pred'] )\nConfirmedCases_all_result_Final['best_pred_1']=np.where(ConfirmedCases_all_result_Final['best_pred'] <0 , 0,\n                                                       ConfirmedCases_all_result_Final['best_pred'] )\nFatalities_all_result_final['best_pred_1']=np.where(Fatalities_all_result_final['best_pred'] <0 , 0 ,\n                                                       Fatalities_all_result_final['best_pred'] )","732164cd":"best_model_key=ConfirmedCases_all_result_Final[['key','best_model']].drop_duplicates()\nmax_number_current=train_ck.groupby('key').max()[['ConfirmedCases','Fatalities']].reset_index()\nbest_model_key=best_model_key.merge(max_number_current,on='key',how='left')\nbest_model_key=best_model_key.sort_values('ConfirmedCases',ascending=False).reset_index(drop=True)","9df2e95d":"best_model_key=best_model_key[~(best_model_key['key']=='China')].reset_index()\nfor j in best_model_key.best_model.unique():\n    print('Top Countries\/District currently under %s growth rate' % str (j))\n    print(best_model_key[best_model_key['best_model']==j].head(10))\n    print('-'*30)\n    print('\\n \\n')\n    print('-'*30)","121598f1":"# Skip warnings","59f10d83":"# Top country except china model state ","2b27fc38":"Creating ky for easy joins ","dc7eafda":"# Global Total Numbers","74039713":"# Run prediction","cd37d866":"Converting Date stored as object to datetime dtype","ea85bcce":"# Using Exponential and holt winter methods\n## Series only starts after first case is reported\nCurrently ConfirmedCases and Fatalities are sparately forecasted \n\n[Link to python ets](https:\/\/www.statsmodels.org\/stable\/examples\/notebooks\/generated\/exponential_smoothing.html) \n\n[Details about ets method used ](https:\/\/robjhyndman.com\/uwafiles\/3-ExponentialSmoothing.pdf)","051ec465":"# Country level","67540b4c":"# Accuracy function ","7f0916f9":"# Next Steps\nRun the models at weekly level starting mid feb and see the best ETS model picked for each country. \nWe can understand at which week of breakout a country is growing by how much","e0d05aa2":"# Forecast country level","f23840f9":"# Skip for error"}}