{"cell_type":{"3d89fd76":"code","e4d6cbf8":"code","e251e84a":"code","b643ffba":"code","40083fb7":"code","c0d6a908":"code","a4b79bc6":"code","ebf90083":"code","d8e9557d":"code","98df7560":"markdown","4870451c":"markdown","59e59f6f":"markdown","42a99fb8":"markdown","77bd6b75":"markdown","7d9a7eee":"markdown","8335d4f7":"markdown","37e771a2":"markdown","c4e61a0e":"markdown"},"source":{"3d89fd76":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, add, ReLU\nfrom tensorflow.keras import Model\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n\ntrain_data = pd.read_csv(\"..\/input\/heartbeat\/mitbih_train.csv\", header=None)\ntest_data = pd.read_csv(\"..\/input\/heartbeat\/mitbih_test.csv\", header=None)\n\n","e4d6cbf8":"train_information = train_data.iloc[:, :-1].to_numpy()\ntrain_label = train_data.iloc[:, -1].to_numpy()\n\ntest_information = test_data.iloc[:, :-1].to_numpy()\ntest_label = test_data.iloc[:, -1].to_numpy()\n\nprint(test_label)","e251e84a":"gaussian_flag = 0\n\ndef add_gaussian_noise(signal):\n    noise=np.random.normal(0,0.05,187)\n    return (signal+noise)\n\ntempo=train_information[20]\nprint(train_label[0])\nbruiter=add_gaussian_noise(tempo)\n\nplt.subplot(2,1,1)\nplt.plot(tempo)\n\nplt.subplot(2,1,2)\nplt.plot(bruiter)\n\nplt.show()\n\ntrain_information = add_gaussian_noise(train_information)\ngaussian_flag = not gaussian_flag","b643ffba":"train_information = train_information[..., tf.newaxis]\ntest_information = test_information[..., tf.newaxis]\n\nprint(train_information.shape)","40083fb7":"BATCH_SIZE = 64\n\ntrain_ds = tf.data.Dataset.from_tensor_slices(\n    (train_information, train_label)).shuffle(10000).batch(BATCH_SIZE )\ntest_ds = tf.data.Dataset.from_tensor_slices(\n    (test_information, test_label)).batch(BATCH_SIZE )","c0d6a908":"class MyModel(Model):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.conv_in = Conv1D(32, input_shape=(None, 187, 1), kernel_size=(5), strides=1,\n                              padding='same', activation='relu')\n        self.conv_relu = Conv1D(32, kernel_size=(5), strides=1,\n                           padding='same', activation='relu')\n        self.conv_raw = Conv1D(32, kernel_size=(5), strides=1,\n                              padding='same')\n        self.maxpool = MaxPooling1D(pool_size=5, strides=2)\n        self.flatten = Flatten()\n        self.relu = ReLU()\n        self.dense = Dense(32, activation='relu')\n        self.dense_out = Dense(5, activation='softmax')\n\n    def routine(self, x):\n        input_param = x\n        x = self.conv_relu(x)\n        x = self.conv_raw(x)\n        x = add([input_param, x])\n        x = self.relu(x)\n        return self.maxpool(x)\n\n    def call(self, x):\n        x = self.conv_in(x)\n        for _ in range(5):\n            x = self.routine(x)\n\n        x = self.flatten(x)\n\n        x = self.dense(x)\n        return self.dense_out(x)","a4b79bc6":"model = MyModel()\n\n# Loss Func. SCC\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n\n# Opimizer Adam\noptimizer = tf.keras.optimizers.Adam()\n\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n\ntest_loss = tf.keras.metrics.Mean(name='test_loss')\ntest_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n\n\n@tf.function\ndef train_step(images, labels):\n    with tf.GradientTape() as tape:\n        predictions = model(images)\n        loss = loss_object(labels, predictions)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n    train_loss(loss)\n    train_accuracy(labels, predictions)\n\n\n@tf.function\ndef test_step(images, labels):\n    predictions = model(images)\n    t_loss = loss_object(labels, predictions)\n\n    test_loss(t_loss)\n    test_accuracy(labels, predictions)","ebf90083":"EPOCHS = 75\n\ntrain_loss_results = []\ntrain_accuracy_results = []\n\ntest_loss_results = []\ntest_accuracy_results = []\n\n\nfor epoch in range(EPOCHS):\n    for images, labels in train_ds:\n        train_step(images, labels)\n\n    for test_images, test_labels in test_ds:\n        test_step(test_images, test_labels)\n\n    train_loss_results.append(train_loss.result())\n    train_accuracy_results.append(train_accuracy.result())\n\n    test_loss_results.append(test_loss.result())\n    test_accuracy_results.append(test_accuracy.result())\n\n    template = 'EPOCH: {}, LOSS: {}, ACCURACY: {}, TEST LOSS: {}, TEST ACCURACY: {}'\n    if (epoch + 1) % 5 == 0:\n        print(template.format(epoch + 1,\n                              train_loss.result(),\n                              train_accuracy.result() * 100,\n                              test_loss.result(),\n                              test_accuracy.result() * 100))","d8e9557d":"COLOR = 'white'\nmatplotlib.rcParams['text.color'] = COLOR\nmatplotlib.rcParams['axes.labelcolor'] = COLOR\nmatplotlib.rcParams['xtick.color'] = COLOR\nmatplotlib.rcParams['ytick.color'] = COLOR\n\nfig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\nfig.suptitle('Training Results', fontsize=20)\n\naxes[0].set_ylabel(\"Loss\", fontsize=14)\naxes[0].plot(train_loss_results)\naxes[0].grid()\n\naxes[1].set_xlabel(\"Epoch\", fontsize=14)\naxes[1].set_ylabel(\"Accuracy\", fontsize=14)\naxes[1].plot(train_accuracy_results)\naxes[1].grid()\n\n\nfig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\nfig.suptitle('Test Results', fontsize=20)\n\naxes[0].set_ylabel(\"Loss\", fontsize=14)\naxes[0].plot(test_loss_results)\naxes[0].grid()\n\naxes[1].set_xlabel(\"Epoch\", fontsize=14)\naxes[1].set_ylabel(\"Accuracy\", fontsize=14)\naxes[1].plot(test_accuracy_results)\naxes[1].grid()\n\nplt.show()","98df7560":"**Add Gaussian noise to the dataset** It prevents dataset from overfitting","4870451c":"Processing Pandas dataframe to Numpy array","59e59f6f":"Visual results.","42a99fb8":"Add a channel for convolutional layer","77bd6b75":"Set Loss function & Optimizer and training & test sequence.","7d9a7eee":"Batch the dataset to enhance performance","8335d4f7":"Construct the model by using Keras subclass API. This model is from the paper 'ArXiv 1805.00794'","37e771a2":"Refer to the 'ArXiv 1805.00794'","c4e61a0e":"Let's get it started."}}