{"cell_type":{"802164b8":"code","8e406d9d":"code","642a747f":"code","82305358":"code","d9cc0fc5":"code","247b0546":"code","1a889441":"code","56787207":"code","3c821fa4":"code","477cb4ee":"code","64501c31":"code","87dc4b42":"code","7d669e95":"code","f1c9a65f":"code","7611e4c8":"code","51a36d29":"code","08306154":"code","9547c672":"markdown","df3b2d0c":"markdown","bbd465c0":"markdown"},"source":{"802164b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom  tensorflow.keras  import layers\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8e406d9d":"data =keras.datasets.imdb","642a747f":"max_word=10000","82305358":"(x_train, y_train) , (x_test, y_test) = data.load_data(num_words=max_word) ","d9cc0fc5":"len(x_train[0]), x_train.shape,y_train.shape , x_test.shape, y_test.shape ","247b0546":"#[len(x) for x in x_train]\nx_train=keras.preprocessing.sequence.pad_sequences(x_train,300)\nx_test=keras.preprocessing.sequence.pad_sequences(x_test,300)\n","1a889441":"len(x_train[0]), x_train.shape,y_train.shape , x_test.shape, y_test.shape ","56787207":"#test = 'i am a student'","3c821fa4":" #dict( (word , test.split().index(word) )  for word in  test.split())","477cb4ee":"model =keras.models.Sequential()# LSTM is better for time-seq question\nmodel.add(layers.Embedding(10000 , 50 ,input_length=300) )  #\uff0825000  300  50 \uff09\nmodel.add(layers.GlobalAveragePooling1D()) \nmodel.add(layers.Dense(32   , activation ='relu'))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(1   , activation ='sigmoid'))","64501c31":"model.summary()","87dc4b42":"model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001) ,\n              loss= 'binary_crossentropy' , \n              metrics =['acc']\n             )","7d669e95":"history   = model.fit(x_train , y_train , epochs =15 , batch_size=256 , validation_data=(x_test , y_test)) ","f1c9a65f":"history.history.keys()","7611e4c8":"import matplotlib.pyplot as plt\n%matplotlib inline","51a36d29":"plt.plot(history.epoch , history.history['loss'] ,'r')\nplt.plot(history.epoch , history.history['val_loss'] ,'b')\n","08306154":"plt.plot(history.epoch , history.history['acc'] ,'r')\nplt.plot(history.epoch , history.history['val_acc'] ,'b')","9547c672":"k-hot","df3b2d0c":"# 25000 sentences\n# max different words 10000\n# 300 words per sentence\n# 50 dim per word","bbd465c0":"embed \u5bc6\u96c6\u5411\u91cf"}}