{"cell_type":{"bfecc095":"code","64279795":"code","2b402890":"code","9d363b27":"code","b554b3e4":"code","983599c6":"code","74e49c78":"code","15ecf901":"code","0d11fad3":"code","e89eb458":"code","7fccc411":"code","1cb0bf35":"code","e6d30576":"code","359f129d":"code","437bbc60":"code","51e7ac41":"code","47c6bf9d":"code","55d4bca7":"markdown","827a3c5b":"markdown","69eef987":"markdown","8c0c2f99":"markdown","92e5600b":"markdown","5f93ad69":"markdown","5ab44305":"markdown","0c59b922":"markdown","bd9287e7":"markdown","53af47c8":"markdown","bae79d6f":"markdown","1b69ab85":"markdown","d3c43e37":"markdown","37f9944e":"markdown","f8e4bb8a":"markdown","707b19a5":"markdown","eb7f584f":"markdown","01a1e69b":"markdown","b0cfb49f":"markdown","78efab8d":"markdown","7f80dc51":"markdown","8f45c478":"markdown","cdf23499":"markdown","c985e227":"markdown","d33ede34":"markdown"},"source":{"bfecc095":"import numpy as np \nimport pandas as pd\nimport glob\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras import Model,layers\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.optimizers import RMSprop","64279795":"paths=glob.glob('..\/input\/intel-image-classification\/seg_train\/seg_train\/*')\nl=len('..\/input\/intel-image-classification\/seg_train\/seg_train\/')\nlabels=[]\nfor path in paths:\n    labels.append(path[l:])\nprint(labels)","2b402890":"def prepare_dataset(path,label):\n    x_train=[]\n    y_train=[]\n    all_images_path=glob.glob(path+'\/*.jpg')\n    for img_path in all_images_path :\n            img=load_img(img_path, target_size=(150,150))\n            img=img_to_array(img)\n            img=img\/255.0\n            x_train.append(img)\n            y_train.append(label)\n    return np.array(x_train),np.array(y_train)","9d363b27":"trainX_building, trainY_building  = prepare_dataset(\"..\/input\/intel-image-classification\/seg_train\/seg_train\/buildings\/\",0)\ntrainX_forest,trainY_forest  = prepare_dataset(\"..\/input\/intel-image-classification\/seg_train\/seg_train\/forest\/\",1)\ntrainX_glacier,trainY_glacier  = prepare_dataset(\"..\/input\/intel-image-classification\/seg_train\/seg_train\/glacier\/\",2)\ntrainX_mount,trainY_mount  = prepare_dataset(\"..\/input\/intel-image-classification\/seg_train\/seg_train\/mountain\/\",3)\ntrainX_sea,trainY_sea  = prepare_dataset(\"..\/input\/intel-image-classification\/seg_train\/seg_train\/sea\/\",4)\ntrainX_street,trainY_street  = prepare_dataset(\"..\/input\/intel-image-classification\/seg_train\/seg_train\/street\/\",5)\n\nprint('train building shape ', trainX_building.shape, trainY_building.shape) \nprint('train forest', trainX_forest.shape ,trainY_forest.shape)\nprint('train glacier', trainX_glacier.shape,trainY_glacier.shape)\nprint('train mountain', trainX_mount.shape, trainY_mount.shape)\nprint('train sea',     trainX_sea.shape, trainY_sea.shape)\nprint('train street', trainX_street.shape ,trainY_street.shape)","b554b3e4":"x_train=np.concatenate((trainX_building,trainX_forest,trainX_glacier,trainX_mount,trainX_sea,trainX_street),axis=0)\ny_train=np.concatenate((trainY_building,trainY_forest,trainY_glacier,trainY_mount,trainY_sea,trainY_street),axis=0)\nprint(x_train.shape)\nprint(y_train.shape)","983599c6":"testX_building, testY_building  = prepare_dataset(\"..\/input\/intel-image-classification\/seg_test\/seg_test\/buildings\/\",0)\ntestX_forest,testY_forest  = prepare_dataset(\"..\/input\/intel-image-classification\/seg_test\/seg_test\/forest\/\",1)\ntestX_glacier,testY_glacier  = prepare_dataset(\"..\/input\/intel-image-classification\/seg_test\/seg_test\/glacier\/\",2)\ntestX_mount,testY_mount  = prepare_dataset(\"..\/input\/intel-image-classification\/seg_test\/seg_test\/mountain\/\",3)\ntestX_sea,testY_sea  = prepare_dataset(\"..\/input\/intel-image-classification\/seg_test\/seg_test\/sea\/\",4)\ntestX_street,testY_street  = prepare_dataset(\"..\/input\/intel-image-classification\/seg_test\/seg_test\/street\/\",5)\n\nx_test=np.concatenate((testX_building,testX_forest,testX_glacier,testX_mount,testX_sea,testX_street),axis=0)\ny_test=np.concatenate((testY_building,testY_forest,testY_glacier,testY_mount,testY_sea,testY_street),axis=0)","74e49c78":"local_weights_file = '\/kaggle\/input\/inceptionv3\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\npre_trained_model = InceptionV3(input_shape = (150, 150, 3), \n                                include_top = False, \n                                weights = None)\n\npre_trained_model.load_weights(local_weights_file)\n\nfor layer in pre_trained_model.layers:\n     layer.trainable = False\n        \npre_trained_model.summary()","15ecf901":"last_layer = pre_trained_model.get_layer('mixed7')\nprint('last layer output shape: ', last_layer.output_shape)\nlast_output = last_layer.output\n\n\nx = layers.Flatten()(last_output)\nx = layers.Dense(1024, activation='relu')(x)\nx = layers.Dropout(0.2)(x)                  \nx = layers.Dense(6, activation='softmax')(x)           \n\nmodel = Model(pre_trained_model.input, x)","0d11fad3":"model.compile(optimizer = RMSprop(lr=0.0001), \n              loss = 'sparse_categorical_crossentropy', \n              metrics = ['acc'])","e89eb458":"history=model.fit(x_train,y_train,epochs=1,validation_data=(x_test,y_test))","7fccc411":"from tensorflow.keras.applications import ResNet50\n\npretrained_model=ResNet50( input_shape=(150,150,3),\n                                  include_top=False,\n                                  weights='imagenet'\n                                   )\n\nfor layer in pretrained_model.layers:\n     layer.trainable = False\n\npretrained_model.summary()","1cb0bf35":"last_layer = pretrained_model.get_layer('conv5_block3_out')\nprint('last layer of vgg : output shape: ', last_layer.output_shape)\nlast_output = last_layer.output\n\nx = layers.Flatten()(last_output)\nx = layers.Dense(1024, activation='relu')(x)\nx = layers.Dropout(0.2)(x)                  \nx = layers.Dense(6, activation='softmax')(x)\n\nmodel_resnet = Model(pretrained_model.input, x) ","e6d30576":"model_resnet.compile(optimizer = RMSprop(lr=0.0001), \n              loss = 'sparse_categorical_crossentropy', \n              metrics = ['acc'])","359f129d":"model_resnet.fit(x_train,y_train,epochs=1,validation_data=(x_test,y_test))","437bbc60":"from tensorflow.keras.applications import VGG16\nfile='\/kaggle\/input\/vgg16\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\npretrained_model=VGG16(input_shape = (150, 150, 3), \n                        include_top = False, \n                        weights =None)\n\npretrained_model.load_weights(file)\n\nfor layer in pretrained_model.layers:\n     layer.trainable = False","51e7ac41":"last_layer = pretrained_model.get_layer('block5_pool')\nprint('last layer of vgg : output shape: ', last_layer.output_shape)\nlast_output = last_layer.output\n\nx = layers.Flatten()(last_output)\nx = layers.Dense(1024, activation='relu')(x)\nx = layers.Dropout(0.2)(x)                  \nx = layers.Dense(6, activation='softmax')(x)           \n\nmodel_vgg = Model(pretrained_model.input, x) ","47c6bf9d":"model_vgg.compile(optimizer = RMSprop(lr=0.0001), \n              loss = 'sparse_categorical_crossentropy', \n              metrics = ['acc'])\n\n#model_vgg.fit(x_train,y_train,epochs=1,validation_data=(x_test,y_test))","55d4bca7":"<font size=\"+2\" color=\"indigo\"><b>5.3 VGG 16<\/b><\/font><br><a id=\"5.3\"><\/a>","827a3c5b":"<font size=\"+2\" color=\"indigo\"><b>6.3 Implementing VGG 16 <\/b><\/font><br><a id=\"6.3\"><\/a>","69eef987":"![image.png](attachment:image.png)","8c0c2f99":"![image.png](attachment:image.png)","92e5600b":"<font size=\"+3\" color=\"blue\"><b>2. Importing The Required Libraries<\/b><\/font><br><a id=\"2\"><\/a>","5f93ad69":"The aim of this kernel is to provide a comprehensive guide on how to use and implement pre-trained architectures along with little fine tuning.\n\n<font size=\"+1\"><i>Readers,This kernel gives a direct practical approach on pre-trained architecture implementation,If you want to an in-depth explanation of the architectures used,you can have a look at my Explaining architecture series of Notebooks.(Link at End of the kernel).<\/i><\/font><br><br>\n\n<font size=\"+1\" color=green ><b>Please appreciate me through your Upvote.<\/b><\/font>","5ab44305":"<font size=\"+3\" color=\"blue\"><b>1. Objective<\/b><\/font><br><a id=\"1\"><\/a>","0c59b922":"I am creating a function to load the dataset alongwith small preprocessing.","bd9287e7":"<font size=\"+3\" color=blue ><b> <center><u>Transfer Learning Tutorial for Beginner to Intermediate level CV enthusaists<\/u><\/center><\/b><\/font>","53af47c8":"Inception-v3 is a convolutional neural network that is 48 layers deep. You can load a pretrained version of the network trained on more than a million images from the ImageNet database. The pretrained network can classify images into 1000 object categories, such as keyboard, mouse, pencil, and many animals. As a result, the network has learned rich feature representations for a wide range of images. The network has an image input size of 299-by-299.","bae79d6f":"<font size=\"+3\" color=\"blue\"><b>3. Importing The Dataset <\/b><\/font><br> <a id=\"3\"><\/a>\nI will be using data from [Intel Image Classification](https:\/\/www.kaggle.com\/puneet6060\/intel-image-classification) competiton","1b69ab85":"<a id='top'><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Table of content<\/h3>\n    \n* [1. Objective](#1)  \n* [2. Importing the Required Libraries](#2)\n* [3. Importing The Dataset ](#3)\n* [4. Preparing the Train and Test set](#4)\n* [5. Explaining the Architectures using in Brief](#5)\n    - [5.1 Inception V3](#5.1)\n    - [5.2 ResNet 50](#5.2)\n    - [5.3 VGG 16 ](#5.3)\n* [6. Implementing the Architectures with little Fine tuning ](#6)\n    - [6.1 Inception V3](#6.1)\n    - [6.2 ResNet 50](#6.2)\n    - [6.3 VGG 16 ](#6.3)\n* [7. Acknowledgement and References for the Kernels ](#7)    ","d3c43e37":"<font size=\"+2\" color=\"indigo\"><b>6.1 Implementing ResNet 50 (using ImageNet) <\/b><\/font><br><a id=\"6.2\"><\/a>","37f9944e":"<font size=\"+2\" color=\"indigo\"><b>5.1 Inception V3<\/b><\/font><br><a id=\"5.1\"><\/a>","f8e4bb8a":"<font size=\"+3\" color=\"blue\"><b>5. Explaining The Architectures In Brief <\/b><\/font><br> <a id=\"5\"><\/a>","707b19a5":"<font size=\"+2\" color=\"chocolate\"><b>My Other Kernels<\/b><\/font><br>\n\n#### Click on the button to view kernels...\n\n\n<a href=\"https:\/\/www.kaggle.com\/darthmanav\/explaining-resnet-model-fine-tuning-pca-t-sne\" class=\"btn btn-primary\" style=\"color:white;\">Explaining ResNet architecture<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/darthmanav\/explaining-alexnet-model-tutorial-fine-tuning-pca\" class=\"btn btn-primary\" style=\"color:white;\">Explaining AlexNet architecture<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/darthmanav\/multilayer-perceptron-fine-tuning-pca-t-sne-mnist\" class=\"btn btn-primary\" style=\"color:white;\">Explaining Multi layer Perceptron architecture<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/darthmanav\/explaining-vgg-model-fine-tuning-pca-t-sne\" class=\"btn btn-primary\" style=\"color:white;\">Explaining VGG architecture<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/darthmanav\/predicting-wine-quality-using-svm-knn-with-eda\" class=\"btn btn-primary\" style=\"color:white;\">Wine Quality using SVM,KNN<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/darthmanav\/glove-lstm-sentiment-analysis-for-beginners\" class=\"btn btn-primary\" style=\"color:white;\">NLP tutorial on Glove and LSTM<\/a>\n\n<a href=\"https:\/\/www.kaggle.com\/darthmanav\/horse-vs-human-classification-by-resnet50-beginner\" class=\"btn btn-primary\" style=\"color:white;\">Horse Vs Humans<\/a>\n\n<br>\n\n###  If these kernels impress you,give them an <font size=\"+2\" color=\"red\"><b>Upvote<\/b><\/font>.<br>\n<a href=\"#top\" class=\"btn btn-success btn-lg active\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOP<\/a>","eb7f584f":"<font size=\"+3\" color=\"blue\"><b>6. Implementing the Architectures with little Fine tuning <\/b><\/font><br> <a id=\"6\"><\/a>","01a1e69b":"<font size=\"+3\" color=\"blue\"><b>4. Preparing the Train and Test Set <\/b><\/font><br> <a id=\"4\"><\/a>","b0cfb49f":"ResNet, short for Residual Networks is a classic neural network used as a backbone for many computer vision tasks. This model was the winner of ImageNet challenge in 2015. The fundamental breakthrough with ResNet was it allowed us to train extremely deep neural networks with 150+layers successfully. Prior to ResNet training very deep neural networks was difficult due to the problem of vanishing gradients.\nAlexNet, the winner of ImageNet 2012 and the model that apparently kick started the focus on deep learning had only 8 convolutional layers, the VGG network had 19 and Inception or GoogleNet had 22 layers and ResNet 152 had 152 layers. In this blog we will code a ResNet-50 that is a smaller version of ResNet 152 and frequently used as a starting point for transfer learning.","78efab8d":"The input to conv1 layer is of fixed size 224 x 224 RGB image. The image is passed through a stack of convolutional (conv.) layers, where the filters were used with a very small receptive field: 3\u00d73 (which is the smallest size to capture the notion of left\/right, up\/down, center). In one of the configurations, it also utilizes 1\u00d71 convolution filters, which can be seen as a linear transformation of the input channels (followed by non-linearity). The convolution stride is fixed to 1 pixel; the spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1-pixel for 3\u00d73 conv. layers. Spatial pooling is carried out by five max-pooling layers, which follow some of the conv.  layers (not all the conv. layers are followed by max-pooling). Max-pooling is performed over a 2\u00d72 pixel window, with stride 2.\n\nThree Fully-Connected (FC) layers follow a stack of convolutional layers (which has a different depth in different architectures): the first two have 4096 channels each, the third performs 1000-way ILSVRC classification and thus contains 1000 channels (one for each class). The final layer is the soft-max layer. The configuration of the fully connected layers is the same in all networks.\n\nAll hidden layers are equipped with the rectification (ReLU) non-linearity. It is also noted that none of the networks (except for one) contain Local Response Normalisation (LRN), such normalization does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time.","7f80dc51":"<font size=\"+2\" color=\"blue\"><b>7. Acknowledgement and Sources of Inspiration for the Kernels <\/b><\/font><br><a id=\"7\"><\/a>","8f45c478":"<font size=\"+2\" color=\"indigo\"><b>6.1 Implementing Inception V3<\/b><\/font><br><a id=\"6.1\"><\/a>","cdf23499":"A big Thankyou to @raenish for making such an awesome theme for the notebooks.\n- https:\/\/www.kaggle.com\/janvichokshi\/transfer-learning-cnn-resnet-vgg16-iceptionv3\n- https:\/\/www.kaggle.com\/raenish\/cheatsheet-text-helper-functions\n- https:\/\/www.kaggle.com\/keras\/inceptionv3\n- https:\/\/www.kaggle.com\/keras\/resnet50\n- https:\/\/www.kaggle.com\/keras\/vgg16","c985e227":"<font size=\"+2\" color=\"indigo\"><b>5.2 ResNet 50<\/b><\/font><br><a id=\"5.2\"><\/a>","d33ede34":"![image.png](attachment:image.png)"}}