{"cell_type":{"38b4068c":"code","2fa593db":"code","712edb91":"code","2e863a64":"code","ea0b9619":"code","2ebf0478":"code","09117085":"code","4bce8bf1":"code","82385a61":"code","d11ec384":"code","dc70665f":"code","9850740d":"code","78433e53":"code","a8a24513":"markdown","b7719cad":"markdown","ba47f848":"markdown","d5f0cd14":"markdown","8df8a4a8":"markdown","cd3c6e5f":"markdown","05662f0e":"markdown"},"source":{"38b4068c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2fa593db":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,accuracy_score,recall_score,classification_report \n\nimport warnings\nwarnings.filterwarnings('ignore')\ndf = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\ndf.head()\ndf.describe()","712edb91":"print(\"Total record count\",len(df))\nprint(\"percentage of non fraud record count\",len(df.loc[df['Class']==0])\/len(df) * 100)\nprint(\"percentage of fraud record count\",len(df.loc[df['Class']==1])\/len(df) * 100)\nprint(\"Total fraud record count\",len(df.loc[df['Class']==1]))\nsns.countplot(df['Class'])","2e863a64":"#Check distribution of each attributes with Boxplot\nfig,ax=plt.subplots(7,2,figsize=(15,25))\nsns.boxplot(data=df,x='Class',y='V1',ax=ax[0,0])\nsns.boxplot(data=df,x='Class',y='V2',ax=ax[0,1])\nsns.boxplot(data=df,x='Class',y='V3',ax=ax[1,0])\nsns.boxplot(data=df,x='Class',y='V4',ax=ax[1,1])\nsns.boxplot(data=df,x='Class',y='V5',ax=ax[2,0])\nsns.boxplot(data=df,x='Class',y='V6',ax=ax[2,1])\nsns.boxplot(data=df,x='Class',y='V7',ax=ax[3,0])\nsns.boxplot(data=df,x='Class',y='V8',ax=ax[3,1])\nsns.boxplot(data=df,x='Class',y='V9',ax=ax[4,0])\nsns.boxplot(data=df,x='Class',y='V10',ax=ax[4,1])\nsns.boxplot(data=df,x='Class',y='V11',ax=ax[5,0])\nsns.boxplot(data=df,x='Class',y='V12',ax=ax[5,1])\nsns.boxplot(data=df,x='Class',y='V13',ax=ax[6,0])\nsns.boxplot(data=df,x='Class',y='V14',ax=ax[6,1])","ea0b9619":"sns.boxplot(data=df,x='Class',y='Amount')","2ebf0478":"fig,ax=plt.subplots(7,2,figsize=(15,25))\nsns.boxplot(data=df,x='Class',y='V15',ax=ax[0,0])\nsns.boxplot(data=df,x='Class',y='V16',ax=ax[0,1])\nsns.boxplot(data=df,x='Class',y='V17',ax=ax[1,0])\nsns.boxplot(data=df,x='Class',y='V18',ax=ax[1,1])\nsns.boxplot(data=df,x='Class',y='V19',ax=ax[2,0])\nsns.boxplot(data=df,x='Class',y='V20',ax=ax[2,1])\nsns.boxplot(data=df,x='Class',y='V21',ax=ax[3,0])\nsns.boxplot(data=df,x='Class',y='V22',ax=ax[3,1])\nsns.boxplot(data=df,x='Class',y='V23',ax=ax[4,0])\nsns.boxplot(data=df,x='Class',y='V24',ax=ax[4,1])\nsns.boxplot(data=df,x='Class',y='V25',ax=ax[5,0])\nsns.boxplot(data=df,x='Class',y='V26',ax=ax[5,1])\nsns.boxplot(data=df,x='Class',y='V27',ax=ax[6,0])\nsns.boxplot(data=df,x='Class',y='V28',ax=ax[6,1])","09117085":"# Define function for Confusion matrix visualization\n\nimport itertools\ndef plot_confusion_matrix(cm, classes=[0,1],\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        #print(\"Normalized confusion matrix\")\n    else:\n        1#print('Confusion matrix, without normalization')\n\n    #print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","4bce8bf1":"# Check the Distribution of the labels\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nimport sklearn.preprocessing as preprocessing\n\nx1 = df.drop(['Class','Time'],axis=1)\nx2 = x1.values #returns a numpy array\nmin_max_scaler = preprocessing.StandardScaler()\nx_scaled = min_max_scaler.fit_transform(x2)\nX = pd.DataFrame(x_scaled)\ny = df['Class']\ndff = pd.concat([X,y],axis=1)\nprint(dff.shape)\nsss = StratifiedKFold(n_splits=5, random_state=1, shuffle=False)\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n\n# Turn into an array\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytrain = original_ytrain.reshape(-1,1)\noriginal_ytest = original_ytest.values\noriginal_ytest = original_ytest.reshape(-1,1)\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)\nprint('Label Distributions: \\n')\nprint(train_counts_label\/ len(original_ytrain))\nprint(test_counts_label\/ len(original_ytest))\nprint(original_Xtrain.shape)\nprint(original_ytrain.shape)","82385a61":"import tensorflow as tf\nimport tensorflow.contrib.learn as tflearn\nimport tensorflow.contrib.layers as tflayers\nimport tensorflow.contrib.metrics as metrics\nfrom sklearn.metrics import mean_squared_error\noriginal_ytrain = original_ytrain.reshape(-1,1)\noriginal_ytest = original_ytest.reshape(-1,1)\n\ntraining_epochs = 4000\nlearning_rate = 0.0002\nhidden_layers1=30\nhidden_layers2=24\n\n\ncost_history=np.empty(shape=[1],dtype=float)\n#model\nX=tf.placeholder(tf.float32,[None,29])\nY=tf.placeholder(tf.float32,[None,1])\nis_training=tf.Variable(True,dtype=tf.bool)\ninitializer = tflayers.xavier_initializer(seed=1)\nh0=tf.layers.dense(X,hidden_layers1,activation=tf.nn.leaky_relu,kernel_initializer=initializer,use_bias=True,\n                   bias_initializer=tf.ones_initializer())\ndropout0 = tf.layers.dropout(h0,rate=0.6)\nh1=tf.layers.dense(dropout0,hidden_layers2,activation=tf.nn.leaky_relu,kernel_initializer=initializer,use_bias=True,\n                   bias_initializer=tf.ones_initializer())\ndropout1 = tf.layers.dropout(h1,rate=0.5)\nh2=tf.layers.dense(dropout1,1,activation=None)\n\n#cross_entropy=tf.nn.sigmoid_cross_entropy_with_logits(labels=Y,logits=h3)\ncross_entropy=tf.nn.weighted_cross_entropy_with_logits(labels=Y,logits=h2,pos_weight=5)\ncost=tf.reduce_mean(cross_entropy)\noptimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n#optimizer=tf.train.ProximalGradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n#optimizer=tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\npredicted=tf.nn.sigmoid(h2)\ncorrect_pred=tf.equal(tf.round(predicted),Y)\naccuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n\n#session\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    \n    for step in range(training_epochs + 1):\n        sess.run(optimizer,feed_dict={X: original_Xtrain,Y: original_ytrain})\n        loss, _,acc=sess.run([cost,optimizer,accuracy],feed_dict={X: original_Xtrain,Y: original_ytrain})\n        cost_history=np.append(cost_history,acc)\n        if step % 500 == 0:\n            print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step,loss,acc))\n    test_predict_result=sess.run(tf.cast(tf.round(predicted),tf.int32),feed_dict={X:original_Xtest})\nprint(classification_report(original_ytest,test_predict_result))\nprint(\"AUC:\",\"{:.2f}\".format(roc_auc_score(original_ytest,test_predict_result)))\nprint(\"Accuracy\",(accuracy_score(original_ytest,test_predict_result)))\ncm2=confusion_matrix(original_ytest,test_predict_result)\nprint(cm2)\nplt.figure(figsize=(5,5))\nplot_confusion_matrix(cm2)","d11ec384":"# Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes.\n\n# Lets shuffle the data before creating the subsamples\n\ndf2 = df.sample(frac=1)\n\n# amount of fraud classes 492 rows.\ndf_1 = df2.loc[df2['Class'] == 1]\ndf_0 = df2.loc[df2['Class'] == 0][:492]\n\nnormal_distributed_df = pd.concat([df_1, df_0],axis=0).reset_index()\nnormal_distributed_df = normal_distributed_df.drop('index',axis=1)\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=1)\n\n#print(new_df.head())\n\n# Check the Distribution of the labels\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nimport sklearn.preprocessing as preprocessing\n\nx = new_df.drop(['Class','Time'],axis=1)\nx2 = x.values #returns a numpy array\nmin_max_scaler = preprocessing.StandardScaler()\nx_scaled = min_max_scaler.fit_transform(x2)\nX = pd.DataFrame(x_scaled)\n\ny = new_df['Class']\n\ndff = pd.concat([X,y],axis=1)\nprint(dff.shape)\nsss = StratifiedKFold(n_splits=5, random_state=1, shuffle=False)\n\nfor train_index, test_index in sss.split(X, y):\n    #print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n\n# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.\n\n# Turn into an array\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)\n\nprint('Label Distributions: \\n')\nprint(train_counts_label\/ len(original_ytrain))\nprint(test_counts_label\/ len(original_ytest))\nprint(original_Xtrain.shape)\nprint(original_ytrain.shape)","dc70665f":"import tensorflow as tf\nimport tensorflow.contrib.learn as tflearn\nimport tensorflow.contrib.layers as tflayers\nimport tensorflow.contrib.metrics as metrics\nfrom sklearn.metrics import mean_squared_error\noriginal_ytrain = original_ytrain.reshape(-1,1)\noriginal_ytest = original_ytest.reshape(-1,1)\n\ntraining_epochs = 6000\nlearning_rate = 0.00001\nhidden_layers1=30\nhidden_layers2=24\n\n\ncost_history=np.empty(shape=[1],dtype=float)\n#model\nX=tf.placeholder(tf.float32,[None,29])\nY=tf.placeholder(tf.float32,[None,1])\nis_training=tf.Variable(True,dtype=tf.bool)\ninitializer = tflayers.xavier_initializer(seed=1)\nh0=tf.layers.dense(X,hidden_layers1,activation=tf.nn.leaky_relu,kernel_initializer=initializer,use_bias=True,bias_initializer=tf.ones_initializer())\ndropout0 = tf.layers.dropout(h0,rate=0.6)\nh1=tf.layers.dense(dropout0,hidden_layers2,activation=tf.nn.leaky_relu,kernel_initializer=initializer,use_bias=True,bias_initializer=tf.ones_initializer())\ndropout1 = tf.layers.dropout(h1,rate=0.5)\nh2=tf.layers.dense(dropout1,1,activation=None)\n\ncross_entropy=tf.nn.sigmoid_cross_entropy_with_logits(labels=Y,logits=h2)\ncost=tf.reduce_mean(cross_entropy)\noptimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n#optimizer=tf.train.ProximalGradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n#optimizer=tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\npredicted=tf.nn.sigmoid(h2)\ncorrect_pred=tf.equal(tf.round(predicted),Y)\naccuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n\n#session\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    \n    for step in range(training_epochs + 1):\n        sess.run(optimizer,feed_dict={X: original_Xtrain,Y: original_ytrain})\n        loss, _,acc=sess.run([cost,optimizer,accuracy],feed_dict={X: original_Xtrain,Y: original_ytrain})\n        cost_history=np.append(cost_history,acc)\n        if step % 500 == 0:\n            print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step,loss,acc))\n    test_predict_result=sess.run(tf.cast(tf.round(predicted),tf.int32),feed_dict={X:original_Xtest})\nprint(classification_report(original_ytest,test_predict_result))\nprint(\"AUC:\",\"{:.2f}\".format(roc_auc_score(original_ytest,test_predict_result)))\nprint(\"Accuracy\",(accuracy_score(original_ytest,test_predict_result)))\ncm2=confusion_matrix(original_ytest,test_predict_result)\nprint(cm2)\nplt.figure(figsize=(5,5))\nplot_confusion_matrix(cm2)","9850740d":"from imblearn.over_sampling import SMOTE\n# Check the Distribution of the labels\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nimport sklearn.preprocessing as preprocessing\n\nx = df.drop(['Class','Time'],axis=1)\nx2 = x.values #returns a numpy array\nmin_max_scaler = preprocessing.StandardScaler()\nx_scaled = min_max_scaler.fit_transform(x2)\nX = pd.DataFrame(x_scaled)\n\ny = df['Class']\n\nX_resampled, y_resampled = SMOTE(sampling_strategy='minority',random_state=1).fit_resample(X, y)\n\nX=pd.DataFrame(X_resampled)\ny=pd.DataFrame(y_resampled)\nsss = StratifiedKFold(n_splits=5, random_state=1, shuffle=False)\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n\n# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.\n\n# Turn into an array\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)\n\nprint('Label Distributions: \\n')\nprint(train_counts_label\/ len(original_ytrain))\nprint(test_counts_label\/ len(original_ytest))\nprint(original_Xtrain.shape)\nprint(original_ytrain.shape)\nprint(original_Xtest.shape)\nprint(original_ytest.shape)","78433e53":"import tensorflow as tf\nimport tensorflow.contrib.learn as tflearn\nimport tensorflow.contrib.layers as tflayers\nimport tensorflow.contrib.metrics as metrics\nfrom sklearn.metrics import mean_squared_error\n\noriginal_ytrain = original_ytrain.reshape(-1,1)\noriginal_ytest = original_ytest.reshape(-1,1)\n\ntraining_epochs = 2000\nlearning_rate = 0.008\nhidden_layers1=30\nhidden_layers2=24\n\n\ncost_history=np.empty(shape=[1],dtype=float)\n#model\nX=tf.placeholder(tf.float32,[None,29])\nY=tf.placeholder(tf.float32,[None,1])\nis_training=tf.Variable(True,dtype=tf.bool)\ninitializer = tflayers.xavier_initializer(seed=1)\nh0=tf.layers.dense(X,hidden_layers1,activation=tf.nn.leaky_relu,kernel_initializer=initializer,use_bias=True,\n                   bias_initializer=tf.ones_initializer())\ndropout0 = tf.layers.dropout(h0,rate=0.6)\nh1=tf.layers.dense(dropout0,hidden_layers2,activation=tf.nn.leaky_relu,kernel_initializer=initializer,use_bias=True,\n                  bias_initializer=tf.ones_initializer())\ndropout1 = tf.layers.dropout(h1,rate=0.5)\nh2=tf.layers.dense(dropout1,1,activation=None)\n\n#cross_entropy=tf.nn.sigmoid_cross_entropy_with_logits(labels=Y,logits=h2)\ncross_entropy=tf.nn.weighted_cross_entropy_with_logits(labels=Y,logits=h2,pos_weight=0.1)\ncost=tf.reduce_mean(cross_entropy)\noptimizer=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n#optimizer=tf.train.ProximalGradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n#optimizer=tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\npredicted=tf.nn.sigmoid(h2)\ncorrect_pred=tf.equal(tf.round(predicted),Y)\naccuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n\n#session\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    \n    for step in range(training_epochs + 1):\n        sess.run(optimizer,feed_dict={X: original_Xtrain,Y: original_ytrain})\n        loss, _,acc=sess.run([cost,optimizer,accuracy],feed_dict={X: original_Xtrain,Y: original_ytrain})\n        cost_history=np.append(cost_history,acc)\n        if step % 500 == 0:\n            print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step,loss,acc))\n    test_predict_result=sess.run(tf.cast(tf.round(predicted),tf.int32),feed_dict={X:original_Xtest})\nprint(classification_report(original_ytest,test_predict_result))\nprint(\"AUC:\",\"{:.2f}\".format(roc_auc_score(original_ytest,test_predict_result)))\nprint(\"Accuracy\",(accuracy_score(original_ytest,test_predict_result)))\ncm2=confusion_matrix(original_ytest,test_predict_result)\nprint(cm2)\nplt.figure(figsize=(5,5))\nplot_confusion_matrix(cm2)","a8a24513":"**Leaky ReLU Activation:**\nThe ReLU function suffers from what is called the \u201cdying ReLU\u201d problem. Since the slope of the ReLU function on the negative side is zero, \na neuron stuck on that side is unlikely to recover from it. This causes the neuron to output zero for every input, thus rendering it useless.\nA solution to this problem is to use Leaky ReLU which has a small slope on the negative side.\n\n**Handling Imbalance data with weighted_cross_entropy_with_logits:**\nThis is like sigmoid_cross_entropy_with_logits() except that pos_weight, allows one to trade off recall and precision by up- or \ndown-weighting the cost of a positive error relative to a negative error. The usual cross-entropy cost is defined as: \nlabels * -log(sigmoid(logits)) + (1 - labels) * -log(1 - sigmoid(logits)) A value pos_weights > 1 decreases the false negative count, \nhence increasing the recall. Conversely setting pos_weights < 1 decreases the false positive count and increases the precision","b7719cad":"**Over Sampling method**","ba47f848":"**Under Sampling Method:**","d5f0cd14":"****Tensor Flow:\nTensorFlow is an open source software library released in 2015 by Google to make it easier for developers to design, build, and train deep learning models. \nAt a high level, TensorFlow is a Python library that allows users to express arbitrary computation as a graph of data flows. Nodes in this graph represent mathematical operations, whereas edges represent data that is communicated from one node to another. Data in TensorFlow are represented as tensors, which are multidimensional arrays. Although this framework for thinking about computation is valuable in many different fields, TensorFlow is primarily used for deep learning in practice and research.\n\n**TensorFlow Core Walkthrough**\nYou might think of TensorFlow Core programs as consisting of two discrete sections:\n\nBuilding the computational graph (a tf.Graph).\nRunning the computational graph (using a tf.Session).\n\n**Graph**\nA computational graph is a series of TensorFlow operations arranged into a graph. The graph is composed of two types of objects.\n\n**tf.Operation:** The nodes of the graph. Operations describe calculations that consume and produce tensors.\n**tf.Tensors:** The edges in the graph. These represent the values that will flow through the graph. Most TensorFlow functions return tf.Tensors.\n\n**tf.placeholder:**\nIt is simply a variable that we will assgin data to at a later date.It allows\nus to create our operations and build our computation graph, without needing the data.\n","8df8a4a8":"###### **Exploratory Data Analysis**","cd3c6e5f":"*Deep Learning - Tensor Flow*","05662f0e":"**Conclusion:**\nDeep learning(Tensorflow) process is not required the Exploratory data analysis and features selection engineering.It has capable to learn any kind of data patterns means nonlinear shapes.\n"}}