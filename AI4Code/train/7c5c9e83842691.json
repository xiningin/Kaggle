{"cell_type":{"ba77eb18":"code","87012f2c":"code","345ed333":"code","c50eb162":"code","2f1a88fa":"code","a1f5a843":"code","580f9830":"code","31890dbb":"code","e263f963":"code","e44bd795":"code","8c04700f":"code","a3af2686":"code","278d0a27":"code","03d8b798":"code","a04e6e7e":"code","efb1d2a7":"code","6be1a3ef":"code","284111d0":"code","a0fa64a2":"code","5d26b454":"code","7c34eb95":"code","276f77ab":"code","917e058d":"code","71ce5c04":"code","b140bc9c":"code","7856adee":"code","31533641":"code","8da21695":"code","2b1f3743":"code","82d9e4c8":"code","105985e8":"code","1c993416":"code","5b2b8843":"code","c1ce8b33":"code","6cdcace5":"code","f3c4608a":"code","b3eee2b8":"code","578813a9":"code","3d433543":"code","e6812611":"code","31067616":"code","1f372013":"code","3cdea958":"code","eb31a67e":"code","dcf204d3":"code","9664d9d5":"code","ddb5a0f2":"code","1719a476":"code","7d8cf16a":"code","13798ad8":"code","8d0941da":"code","160d22a0":"code","6a0adb9d":"code","b4d24ca8":"code","69e46b80":"code","711c0da0":"code","9a7c7c42":"code","143b2447":"code","70c3b1b1":"code","e56e7f17":"code","b2a83bb8":"code","44c86d55":"code","49b566e4":"code","33f5dd54":"code","f01cfbae":"code","11fdf6b6":"code","15aeb92a":"code","7a8367d7":"code","907ab8ea":"code","bded09a6":"code","cadbe4bb":"code","0a2b4303":"code","ec01fe7c":"code","3f44044c":"code","bf3e4c41":"code","0201a74f":"code","3e19dc8c":"code","92f4198c":"code","ebcfd05f":"code","5cc83a7f":"code","8c17050f":"code","4c1fd0d1":"code","157e572b":"code","e63dc2dd":"code","017dcef1":"code","f6b94a0e":"code","e13677ca":"code","e8844134":"code","ff6f3e2c":"code","d0fde023":"code","4712364b":"code","2762db63":"code","ea4f374b":"code","52a3e540":"code","88d48b26":"code","273609c1":"code","127a601b":"code","8341f902":"code","a3bbb41c":"code","5dbdf77d":"code","f80a311f":"code","0083a3f2":"code","c613af2d":"code","4daaa139":"code","e894eaef":"code","46ab951e":"code","b4f51f1c":"code","5edd1ffc":"code","ddd32590":"code","17a710ad":"code","da918cf4":"code","81e40116":"code","e9ff72c5":"code","e424310c":"code","d89e8a21":"code","f06c37b7":"markdown","ced0c90e":"markdown","7d224cf9":"markdown","46bbf0a3":"markdown","3da421e6":"markdown","b888e8bc":"markdown","bf99448e":"markdown","a3a6523c":"markdown","575bb4cb":"markdown","a0ccffaa":"markdown","a3457af7":"markdown","01808674":"markdown","ffbdce97":"markdown","68f95685":"markdown","22012274":"markdown","443008c0":"markdown","59e6f98d":"markdown","0a91b82f":"markdown","9c199571":"markdown","05c42d82":"markdown","a7f493db":"markdown","0b09ffd1":"markdown","cb7d54aa":"markdown"},"source":{"ba77eb18":"from google.colab import drive\ndrive.mount('\/content\/drive')","87012f2c":"class HyperParams:\n  n_fft= 1200\n  num_freq= 601 # n_fft\/\/2 + 1\n  sample_rate= 16000\n  hop_length= 160\n  win_length= 400\n  min_level_db= -100.0\n  ref_level_db= 20.0\n  preemphasis= 0.97\n  power= 0.30\n  embedder_window= 80\n  data_audio_len= 3.0\n  embedder_num_mels= 40\n  embedder_lstm_hidden = 768\n  embedder_emb_dim = 256\n  embedder_lstm_layers = 3\n  embedder_window = 80\n  embedder_stride = 40\n  model_lstm_dim = 400\n  model_fc1_dim = 600\n  model_fc2_dim = 601 # num_freq","345ed333":"import librosa\nimport numpy as np  ","c50eb162":"class Audio:\n  def __init__(self,hyper_params):\n    self.hyper_params = hyper_params\n    self.mel_basis_matrix = librosa.filters.mel(sr=hyper_params.sample_rate,\n                                             n_fft=hyper_params.n_fft,\n                                             n_mels=hyper_params.embedder_num_mels);\n\n  def get_mel_spec(self,wave):\n    spec = librosa.core.stft(y=wave, n_fft=self.hyper_params.n_fft,\n                              hop_length=self.hyper_params.hop_length,\n                              win_length=self.hyper_params.win_length,\n                              window='hann')\n    power_spec = np.abs(spec) ** 2\n    mel_spec = np.log10(np.dot(self.mel_basis_matrix,power_spec)+1e-6)\n    return mel_spec  \n  def wave2spec(self,wave): \n    spec = librosa.core.stft(y=wave, n_fft=self.hyper_params.n_fft,\n                            hop_length=self.hyper_params.hop_length,\n                            win_length=self.hyper_params.win_length)\n    phase = np.angle(spec)\n    spec_db = self.amp2db(np.abs(spec))\n    spec_db_norm = self.normalize(spec_db)\n    spec_db_norm = spec_db_norm.T   # Taking transpose here\n    phase = phase.T # Taking transpose here\n    return spec_db_norm, phase\n  def spec2wave(self,spec_db_norm,phase):\n    spec_db_norm, phase = spec_db_norm.T, phase.T\n    spec_db = self.denormalize(spec_db_norm)\n    spec_amp = self.db2amp(spec_db)\n    spec = spec_amp * np.exp(1j*phase)\n    wave = librosa.core.istft(spec,\n                             hop_length=self.hyper_params.hop_length,\n                             win_length=self.hyper_params.win_length)\n    return wave\n  def amp2db(self,mat):\n    return 20.0 * np.log10(np.maximum(1e-5,mat)) - self.hyper_params.ref_level_db\n  def db2amp(self,mat):\n    return np.power(10.0, (mat+self.hyper_params.ref_level_db)*0.05)\n  def normalize(self,mat):\n    return np.clip((mat-self.hyper_params.min_level_db)\/-self.hyper_params.min_level_db, 0.0 , 1.0)\n  def denormalize(self, mat):\n    return np.clip(mat,0.0,1.0)*(-self.hyper_params.min_level_db)+self.hyper_params.min_level_db","2f1a88fa":"hyper_params = HyperParams()\naudio = Audio(hyper_params)","a1f5a843":"import os","580f9830":"dataset_path = os.path.join('drive','My Drive','Speech Separation Dataset');\npath = {}\npath['dev'] = os.path.join(dataset_path,'Dev Dataset')\npath['test'] = os.path.join(dataset_path,'Test Dataset')\npath['train'] = os.path.join(dataset_path ,'Train Dataset')","31890dbb":"# create directories to store dataset\nfor dataset in ('dev','test','train'):\n  os.makedirs(os.path.join(path[dataset],'input_spec'),exist_ok=True)\n  os.makedirs(os.path.join(path[dataset],'output_spec'),exist_ok=True)\n  os.makedirs(os.path.join(path[dataset],'input_phase'),exist_ok=True)\n  os.makedirs(os.path.join(path[dataset],'output_phase'),exist_ok=True)\n  os.makedirs(os.path.join(path[dataset],'dvector'),exist_ok=True)","e263f963":"# create 8 separate directories for training dataset to avoid issues with gdrive\ndef create_folders(i):\n  os.makedirs(os.path.join(path['train'],'input_spec_'+i),exist_ok=True)\n  os.makedirs(os.path.join(path['train'],'output_spec_'+i),exist_ok=True)\n  os.makedirs(os.path.join(path['train'],'input_phase_'+i),exist_ok=True)\n  os.makedirs(os.path.join(path['train'],'output_phase_'+i),exist_ok=True)\n  os.makedirs(os.path.join(path['train'],'dvector_'+i),exist_ok=True)","e44bd795":"for i in range(8):\n  create_folders(str(i))","8c04700f":"import shutil","a3af2686":"shutil.unpack_archive('drive\/My Drive\/LibriSpeech Dataset\/dev-clean.tar.gz','drive\/My Drive\/LibriSpeech Dataset\/')\n# Rename the extracted folder LibriSpeech to Dev Dataset","278d0a27":"shutil.unpack_archive('drive\/My Drive\/LibriSpeech Dataset\/test-clean.tar.gz','drive\/My Drive\/LibriSpeech Dataset\/')\n# Rename the extracted folder LibriSpeech to Test Dataset","03d8b798":"shutil.unpack_archive('drive\/My Drive\/LibriSpeech Dataset\/train-clean-100.tar.gz','drive\/My Drive\/LibriSpeech Dataset\/')\n# Rename the extracted folder LibriSpeech to Train Dataset","a04e6e7e":"import glob\nimport pickle","efb1d2a7":"#### Run this cell only the first time ####\ndev_base_path = os.path.join(path['dev'],'dev-clean')\ntest_base_path = os.path.join(path['test'],'test-clean')\ntrain_base_path = os.path.join(path['train'],'train-clean-100')","6be1a3ef":"#### Run this cell only the first time ####\ndev_spks = os.listdir(dev_base_path)\n# list of all speaker folders\ndev_speeches = [glob.glob(os.path.join(dev_base_path,spk,'*','*.flac'),recursive=True) for spk in dev_spks]\ndev_speeches = [speeches for speeches in dev_speeches if len(speeches)>=2]\n# list of lists containing speeches of a speaker\ntest_spks = os.listdir(test_base_path)\n# list of all speaker folders\ntest_speeches = [glob.glob(os.path.join(test_base_path,spk,'*','*.flac'),recursive=True) for spk in test_spks]\ntest_speeches = [speeches for speeches in test_speeches if len(speeches)>=2]\n# list of lists containing speeches of a speaker\ntrain_spks = os.listdir(train_base_path)\n# list of all speaker folders\ntrain_speeches = [glob.glob(os.path.join(train_base_path,spk,'*','*.flac'),recursive=True) for spk in train_spks]\ntrain_speeches = [speeches for speeches in train_speeches if len(speeches)>=2]\n# list of lists containing speeches of a speaker","284111d0":"##### Run this cell only the first time #####\nwith open(os.path.join(path['dev'],'dev_speeches.data'),'wb') as f:\n  pickle.dump(dev_speeches,f)\nwith open(os.path.join(path['test'],'test_speeches.data'),'wb') as f:\n  pickle.dump(test_speeches,f)\nwith open(os.path.join(path['train'],'train_speeches.data'),'wb') as f:\n  pickle.dump(train_speeches,f)","a0fa64a2":"with open(os.path.join(path['dev'],'dev_speeches.data'),'rb') as f:\n  dev_speeches = pickle.load(f)\nwith open(os.path.join(path['test'],'test_speeches.data'),'rb') as f:\n  test_speeches = pickle.load(f)\nwith open(os.path.join(path['train'],'train_speeches.data'),'rb') as f:\n  train_speeches = pickle.load(f)","5d26b454":"import torch\nimport torch.nn as nn\n\nclass LinearNorm(nn.Module):\n    def __init__(self, hp):\n        super(LinearNorm, self).__init__()\n        self.linear_layer = nn.Linear(hp.embedder_lstm_hidden, hp.embedder_emb_dim)\n\n    def forward(self, x):\n        return self.linear_layer(x)\n\n\nclass SpeechEmbedder(nn.Module):\n    def __init__(self, hp):\n        super(SpeechEmbedder, self).__init__()\n        self.lstm = nn.LSTM(hp.embedder_num_mels,\n                            hp.embedder_lstm_hidden,\n                            num_layers=hp.embedder_lstm_layers,\n                            batch_first=True)\n        self.proj = LinearNorm(hp)\n        self.hp = hp\n\n    def forward(self, mel):\n        # (num_mels, T)\n        mels = mel.unfold(1, self.hp.embedder_window, self.hp.embedder_stride) # (num_mels, T', window)\n        mels = mels.permute(1, 2, 0) # (T', window, num_mels)\n        x, _ = self.lstm(mels) # (T', window, lstm_hidden)\n        x = x[:, -1, :] # (T', lstm_hidden), use last frame only\n        x = self.proj(x) # (T', emb_dim)\n        x = x \/ torch.norm(x, p=2, dim=1, keepdim=True) # (T', emb_dim)\n        x = x.sum(0) \/ x.size(0) # (emb_dim), average pooling over time frames\n        return x","7c34eb95":"embedder_path = os.path.join(dataset_path,\"embedder.pt\")\nembedder_pt = torch.load(embedder_path,map_location=torch.device('cpu'))\nembedder = SpeechEmbedder(hyper_params)\nembedder.load_state_dict(embedder_pt)\nembedder.eval()","276f77ab":"import random\nimport pandas as pd","917e058d":"# returns dvec for the input wave using pre trained embedder model\ndef get_dvector(wave):\n  mel_spec = audio.get_mel_spec(wave)\n  dvec = embedder(torch.from_numpy(mel_spec).float())\n  dvec = dvec.detach().numpy()\n  return dvec","71ce5c04":"# pre process waves and store spectrogram, phase and dvector in their respective folders\ndef create_example(target_dir, hyper_params, idx, ref_speech, pri_speech, sec_speech):\n  sample_rate = hyper_params.sample_rate\n  ref_wave, _ = librosa.load(ref_speech,sr=sample_rate) #load the audio file\n  pri_wave, _ = librosa.load(pri_speech, sr = sample_rate)\n  sec_wave,_ = librosa.load(sec_speech, sr = sample_rate)\n  assert len(ref_wave.shape)==len(pri_wave.shape)==len(sec_wave.shape)==1,\\\n  'wave files must be mono and not stereo'\n  ref_wave,_ = librosa.effects.trim(ref_wave, top_db = 20) # clip silent portion on both ends\n  pri_wave,_ = librosa.effects.trim(pri_wave, top_db = 20)\n  sec_wave,_ = librosa.effects.trim(sec_wave, top_db = 20)\n  \n  if ref_wave.shape[0] < 1.1 * hyper_params.embedder_window * hyper_params.hop_length :\n    return\n  length_wave = int(sample_rate * hyper_params.data_audio_len)\n  if pri_wave.shape[0]<length_wave or sec_wave.shape[0]<length_wave:\n    return\n  pri_wave, sec_wave = pri_wave[:length_wave], sec_wave[:length_wave] # clip wave to fixed length\n  mix_wave = pri_wave + sec_wave\n  norm = np.max(np.abs(mix_wave)) * 1.1\n  pri_wave, mix_wave = pri_wave\/norm , mix_wave\/norm  # normalize wave by 1.1*max(absolute amplitude)\n  pri_spec, pri_phase = audio.wave2spec(pri_wave)  # convert wave to spec\n  mix_spec, mix_phase = audio.wave2spec(mix_wave)\n  dvec = get_dvector(ref_wave)\n\n  # paths for storing data\n  pri_spec_path = os.path.join(target_dir,'output_spec','%06d.npy'%idx)\n  pri_phase_path = os.path.join(target_dir,'output_phase','%06d.npy'%idx)\n  mix_spec_path = os.path.join(target_dir, 'input_spec','%06d.npy'%idx)\n  mix_phase_path = os.path.join(target_dir,'input_phase','%06d.npy'%idx)\n  dvec_path = os.path.join(target_dir,'dvec','%06d.npy'%idx)\n  # store data on paths above\n  np.save(pri_spec_path,pri_spec)\n  np.save(mix_spec_path,mix_spec)\n  np.save(mix_phase_path, mix_phase)\n  np.save(pri_phase_path, pri_phase)\n  np.save(dvec_path,dvec)\n\n  #print(idx)\n  return [idx, ref_speech, pri_speech, sec_speech,  mix_spec_path, pri_spec_path, mix_phase_path, pri_phase_path, dvec_path]","b140bc9c":"columns=['key','ref_speech','pri_speech','sec_speech','input_spec_path','output_spec_path','input_phase_path','output_phase_path','dvector_path']","7856adee":"#### to be run just once ####\nsample_data_frame = pd.DataFrame(data = [], columns=columns)\nfor dataset in ('train','dev','test'):\n  sample_data_frame.to_csv(os.path.join(path[dataset],'data_frame.csv'),index=False);","31533641":"def create_dataset(i):\n  batch = []\n  array = range(i+1,n+1)\n  if parity == 1:\n    array = range(1,i)\n  for j in array:\n    first = min(i,j)\n    sec = max(i,j)\n    if (sec-first)%2 == parity:\n      first, sec = sec, first\n    n1 = len(speeches[first-1]) # -1 accounts for zero based indexing\n    n2 = len(speeches[sec-1]) # -1 accounts for zero based indexing\n    sum = first+sec-1 # -1 accounts for zero based indexing\n    diff = first-sec-1 # -1 accounts for zero based indexing\n    diff_mod = (abs(diff))%n1\n    if diff < 0 and diff_mod > 0:\n      diff_mod = n1 - diff_mod\n    ref_speech = speeches[first-1][diff_mod]\n    pri_speech = speeches[first-1][sum%n1]\n    sec_speech = speeches[sec-1][first%n2]\n    row = create_example( path[dataset], hyper_params , n*(i-1) + j, ref_speech, pri_speech, sec_speech)\n    if row is not None:\n      batch.append(row)\n  print(i)\n  return batch","8da21695":"def save_batch(dataset,data):\n  df_path = os.path.join(path[dataset],'data_frame.csv')\n  df = pd.read_csv(df_path)\n  df_batch = pd.DataFrame(data = data, columns = columns)\n  df = df.append(df_batch)\n  df.to_csv(df_path,index=False)","2b1f3743":"import os\nimport time\nfrom multiprocessing import Pool\ncpu_num = len(os.sched_getaffinity(0))","82d9e4c8":"print(\"Number of cpu available : \",cpu_num)","105985e8":"dataset = 'dev' # important global variable\nspeeches = dev_speeches # important global variable\nn = len(dev_speeches) # important global variable\nprint(\"number of speakers(dev set) : \",n)\nfor i in range(n):\n  random.shuffle(dev_speeches[i])  # shuffle the speeches of all speakers\narr = list(range(1,n+1))  # create a list for all speakers","1c993416":"data = []\nparity = 0 # important global variable\nx = time.time()\nwith Pool(cpu_num) as p:\n  res = p.map(create_dataset, arr , 4)\nfor batch in res:\n  if len(batch) > 0:\n    data.extend(batch)\ny = time.time()\nprint(y-x)\nsave_batch('dev',data)","5b2b8843":"data = []\nparity = 1 # important global variable\nx = time.time()\nwith Pool(cpu_num) as p:\n  res = p.map(create_dataset, arr , 4)\nfor batch in res:\n  if len(batch) > 0:\n    data.extend(batch)\ny = time.time()\nprint(y-x)\nsave_batch('dev',data)","c1ce8b33":"dataset = 'test' # important global variable\nspeeches = test_speeches # important global variable\nn = len(test_speeches) # important global variable\nprint(\"number of speakers(test set) : \",n)\nfor i in range(n):\n  random.shuffle(test_speeches[i])  # shuffle the speeches of all speakers\narr = list(range(1,n+1))  # create a list for all speakers","6cdcace5":"data = []\nparity = 0 # important global variable\nx = time.time()\nwith Pool(cpu_num) as p:\n  res = p.map(create_dataset, arr , 4)\nfor batch in res:\n  if len(batch) > 0:\n    data.extend(batch)\ny = time.time()\nprint(y-x)\nsave_batch('test',data)","f3c4608a":"data = []\nparity = 1 # important global variable\nx = time.time()\nwith Pool(cpu_num) as p:\n  res = p.map(create_dataset, arr , 4)\nfor batch in res:\n  if len(batch) > 0:\n    data.extend(batch)\ny = time.time()\nprint(y-x)\nsave_batch('test',data)","b3eee2b8":"dataset = 'train' # important global variable\nspeeches = train_speeches # important global variable\nn = len(train_speeches) # important global variable\nprint(\"number of speakers(train set) : \",n)\nfor i in range(n):\n  random.shuffle(train_speeches[i])  # shuffle the speeches of all speakers\narr = list(range(1,n+1))  # create a list for all speakers","578813a9":"data = []\nparity = 0 # important global variable\nx = time.time()\nwith Pool(cpu_num) as p:\n  res = p.map(create_dataset, arr[0:25] , 4)\nfor batch in res:\n  if len(batch) > 0:\n    data.extend(batch)\ny = time.time()\nprint(y-x)\nsave_batch('train',data)","3d433543":"data = []\nparity = 0 # important global variable\nx = time.time()\nwith Pool(cpu_num) as p:\n  res = p.map(create_dataset, arr[25:50] , 4)\nfor batch in res:\n  if len(batch) > 0:\n    data.extend(batch)\ny = time.time()\nprint(y-x)\nsave_batch('train',data)","e6812611":"data = []\nparity = 0 # important global variable\nx = time.time()\nwith Pool(cpu_num) as p:\n  res = p.map(create_dataset, arr[50:75] , 4)\nfor batch in res:\n  if len(batch) > 0:\n    data.extend(batch)\ny = time.time()\nprint(y-x)\nsave_batch('train',data)","31067616":"data = []\nparity = 0 # important global variable\nx = time.time()\nwith Pool(cpu_num) as p:\n  res = p.map(create_dataset, arr[75:100] , 4)\nfor batch in res:\n  if len(batch) > 0:\n    data.extend(batch)\ny = time.time()\nprint(y-x)\nsave_batch('train',data)","1f372013":"data = []\nparity = 0 # important global variable\nx = time.time()\nwith Pool(cpu_num) as p:\n  res = p.map(create_dataset, arr[75:100] , 4)\nfor batch in res:\n  if len(batch) > 0:\n    data.extend(batch)\ny = time.time()\nprint(y-x)\nsave_batch('train',data)","3cdea958":"data = []\nparity = 0 # important global variable\nx = time.time()\nwith Pool(cpu_num) as p:\n  res = p.map(create_dataset, arr[100:125] , 4)\nfor batch in res:\n  if len(batch) > 0:\n    data.extend(batch)\ny = time.time()\nprint(y-x)\nsave_batch('train',data)","eb31a67e":"data = []\nparity = 0 # important global variable\nx = time.time()\nwith Pool(cpu_num) as p:\n  res = p.map(create_dataset, arr[125:150] , 4)\nfor batch in res:\n  if len(batch) > 0:\n    data.extend(batch)\ny = time.time()\nprint(y-x)\nsave_batch('train',data)","dcf204d3":"data = []\nparity = 0 # important global variable\nx = time.time()\nwith Pool(cpu_num) as p:\n  res = p.map(create_dataset, arr[150:175] , 4)\nfor batch in res:\n  if len(batch) > 0:\n    data.extend(batch)\ny = time.time()\nprint(y-x)\nsave_batch('train',data)","9664d9d5":"data = []\nparity = 0 # important global variable\nx = time.time()\nwith Pool(cpu_num) as p:\n  res = p.map(create_dataset, arr[175:200] , 4)\nfor batch in res:\n  if len(batch) > 0:\n    data.extend(batch)\ny = time.time()\nprint(y-x)\nsave_batch('train',data)","ddb5a0f2":"data = []\nparity = 0 # important global variable\nx = time.time()\nwith Pool(cpu_num) as p:\n  res = p.map(create_dataset, arr[200:225] , 4)\nfor batch in res:\n  if len(batch) > 0:\n    data.extend(batch)\ny = time.time()\nprint(y-x)\nsave_batch('train',data)","1719a476":"data = []\nparity = 0 # important global variable\nx = time.time()\nwith Pool(cpu_num) as p:\n  res = p.map(create_dataset, arr[225:251] , 4)\nfor batch in res:\n  if len(batch) > 0:\n    data.extend(batch)\ny = time.time()\nprint(y-x)\nsave_batch('train',data)","7d8cf16a":"import shutil\nimport os\nimport pandas as pd","13798ad8":"# move files in the dataframe from start_index to end_index to folder for fraction j\ndef move_files(start_index,end_index,j):\n  df_train = pd.read_csv(os.path.join(path['train'],'data_frame.csv'))\n  for i in range(start_index,end_index):\n    for col in ['dvector_path','input_phase_path','input_spec_path','output_phase_path','output_spec_path']:\n      old_path = df_train[col][i]\n      new_path = os.path.join(path['train'],col.rsplit('_',1)[0]+\"_\"+str(j),old_path.split('\/')[-1])\n      #print(old_path,\"  \",new_path)\n      shutil.move(old_path,new_path)\n      df_train.loc[i,col]=new_path  \n  df_train.to_csv(os.path.join(path['train'],'data_frame.csv'),index=False)","8d0941da":"num_fractions = 8\nfraction_sizes = num_fractions * [ num_samples\/\/num_fractions ]\nfor i in range(num_samples%num_fractions):\n  fraction_sizes[i]+=1\nprint(fraction_sizes)","160d22a0":"start_pos = 0\nfor i in range(num_fractions):\n  end_pos = start_pos + fraction_sizes[i]\n  move_files(start_pos,end_pos,i)\n  start_pos = end_pos","6a0adb9d":"import pandas as pd","b4d24ca8":"def print_stats(dataset):\n  df = pd.read_csv(os.path.join(path[dataset],'data_frame.csv'))\n  num_samples = df.shape[0]\n  cnt=0 # cnt of the number of times primary speech is same as the reference speech\n  pairs = {} # cnt of all ordered pairs of speakers\n  cnt_pri ={} # maps spk to the number of times the spk appears as a primary speaker\n  cnt_sec = {} # maps spk to the number of times the spk appears as a sec speaker\n  waves=[]\n  for i in range(num_samples):\n    ref = df['ref_speech'][i]\n    pri = df['pri_speech'][i]\n    sec = df['sec_speech'][i]\n    ref_wave = ref.split('\/')[-1]\n    pri_wave = pri.split('\/')[-1]\n    sec_wave = sec.split('\/')[-1]\n    waves.append(ref_wave)\n    waves.append(pri_wave)\n    waves.append(sec_wave)\n    pri_spk = pri.split('\/')[-3]\n    sec_spk = sec.split('\/')[-3]\n    if (pri_spk,sec_spk) in pairs:\n      pairs[(pri_spk,sec_spk)]+=1\n    else :\n      pairs[(pri_spk,sec_spk)]=1\n    if pri_wave == ref_wave:\n      cnt += 1\n    if pri_spk  in cnt_pri :\n      cnt_pri[pri_spk] += 1\n    else:\n      cnt_pri[pri_spk] = 1\n    if sec_spk in cnt_sec:\n      cnt_sec[sec_spk] += 1\n    else:\n      cnt_sec[sec_spk] =1 \n  waves = len(list(set(waves)))\n  if dataset == 'train':\n    speeches = train_speeches\n  elif dataset == 'dev':\n    speeches = dev_speeches\n  else :\n    speeches = test_speeches\n  total_speeches = sum([len(spk) for spk in speeches])\n  print(\"====================\",dataset,\"dataset statistics ====================\")\n  print(\"Total no. of unique speeches available in LibriSpeech\",dataset,\"dataset :\",total_speeches)\n  print(\"No. of unique speeches used :\",waves)\n  print(\"Percentage of total speeches used : {:.2f} %\".format((waves\/total_speeches)*100))\n  print(\"------------------------------------------------------------\")\n  print(\"Total no. of samples prepared :\",num_samples)\n  print(\"No. of samples with same primary and reference speech :\",cnt)\n  print(\"Fraction of such samples as a part of the entire dataset : {:.2f} %\".format((cnt\/num_samples)*100))\n  print(\"-------------------------------------------------------------\")\n  if all(val == 1 for val in pairs.values()):\n    print(\"Note: All ordered pairs of primary and secondary speakers are unique\")\n    print(\"-------------------------------------------------------------\")\n  print(\"Min no. of times a speaker appears as a primary speaker :\",min(cnt_pri.values()))\n  print(\"Max no. of times a speaker appears as a primary speaker :\",max(cnt_pri.values()))\n  print(\"Min no. of times a speaker appears as a secondary speaker :\",min(cnt_sec.values()))\n  print(\"Max no. of times a speaker appears as a secondary speaker :\",max(cnt_sec.values()))","69e46b80":"print_stats('train')","711c0da0":"print_stats('dev')","9a7c7c42":"print_stats('test')","143b2447":"#input dims for model [ T_dim, num_freq ]\nT_dim = 301 \nnum_freq =  hyper_params.num_freq\nemb_dim = hyper_params.embedder_emb_dim\nlstm_dim =  hyper_params.model_lstm_dim\nfc1_dim = hyper_params.model_fc1_dim\nfc2_dim = hyper_params.model_fc2_dim #num_freq\nbatch_size = 8","70c3b1b1":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Multiply, ZeroPadding2D, concatenate, Conv2D, Input, Dense, Reshape, BatchNormalization, Activation, LSTM, Bidirectional, Lambda","e56e7f17":"def get_model():\n  dvec_inp = Input(shape=[emb_dim],name=\"dvec\")\n  input_spec = Input(shape=[T_dim,num_freq],name=\"input_spec\")\n  x = Reshape((T_dim,num_freq,1))(input_spec)\n \n  # cnn\n \n  #cnn1\n  x = ZeroPadding2D(((0,0), (3,3)))(x)\n  x = Conv2D(filters=64, kernel_size=[1,7], dilation_rate=[1, 1])(x)\n  x = BatchNormalization()(x)\n  x = Activation('relu')(x)\n  #cnn2\n  x = ZeroPadding2D(((3,3), (0,0)))(x)\n  x = Conv2D(filters=64, kernel_size=[7,1], dilation_rate=[1, 1])(x)\n  x = BatchNormalization()(x)\n  x = Activation('relu')(x)\n  #cnn3\n  x = ZeroPadding2D(((2,2), (2,2)))(x)\n  x = Conv2D(filters=64, kernel_size=[5,5], dilation_rate=[1, 1])(x)\n  x = BatchNormalization()(x)\n  x = Activation('relu')(x)\n  #cnn4\n  x = ZeroPadding2D(((4,4), (2,2)))(x)\n  x = Conv2D(filters=64, kernel_size=[5,5], dilation_rate=[2, 1])(x)\n  x = BatchNormalization()(x)\n  x = Activation('relu')(x)\n  #cnn5\n  x = ZeroPadding2D(((8,8), (2,2)))(x)\n  x = Conv2D(filters=64, kernel_size=[5,5], dilation_rate=[4, 1])(x)\n  x = BatchNormalization()(x)\n  x = Activation('relu')(x)\n  #cnn6\n  x = ZeroPadding2D(((16,16), (2,2)))(x)\n  x = Conv2D(filters=64, kernel_size=[5,5], dilation_rate=[8, 1])(x)\n  x = BatchNormalization()(x)\n  x = Activation('relu')(x)\n  #cnn7\n  x = ZeroPadding2D(((32,32), (2,2)))(x)\n  x = Conv2D(filters=64, kernel_size=[5,5], dilation_rate=[16, 1])(x)\n  x = BatchNormalization()(x)\n  x = Activation('relu')(x)\n  #cnn8\n  x = Conv2D(filters=8, kernel_size=[1,1], dilation_rate=[1, 1])(x)\n  x = BatchNormalization()(x)\n  x = Activation('relu')(x)\n   \n  x = Reshape((x.shape[1],x.shape[2]*x.shape[3]))(x) #else use -1 as last arg\n  #x = tf.reshape(x, [x.shape[0],x.shape[1],-1])\n \n  dvec = Lambda(lambda a : tf.expand_dims(a,1))(dvec_inp)\n  dvec = Lambda(lambda a : tf.repeat(a,repeats =x.shape[1],axis =1))(dvec)\n  #dvec= tf.expand_dims(dvec_inp,1)\n  #dvec= tf.repeat(dvec,repeats =x.shape[1],axis =1)\n  \n  x = concatenate([x,dvec],-1)\n  #x= tf.concat([x,dvec],-1)\n  \n  #lstm\n  x = Bidirectional(LSTM(lstm_dim,return_sequences=True))(x)\n  \n  #fc1\n  x = Dense(fc1_dim,activation =\"relu\")(x)\n  #fc2\n  mask = Dense(fc2_dim,activation =\"sigmoid\",name=\"mask\")(x) #soft mask\n  \n  #element-wise\n  output = Multiply()([input_spec,mask])\n\n  model = Model(inputs=[input_spec,dvec_inp], outputs=output)\n \n  return model","b2a83bb8":"model = get_model()\n#model_val = get_model()\nmodel.summary()","44c86d55":"model.compile(optimizer='adam', loss='mse')\ntf.keras.utils.plot_model(model, show_shapes=True, dpi=64)","49b566e4":"### execute just once ###\nmodel.save_weights(os.path.join(dataset_path,'Model weights','weights_epoch0000.h5'))","33f5dd54":"!pip install mir_eval","f01cfbae":"import os\nimport pandas as pd\nimport numpy as np\nfrom tensorflow.keras.utils import Sequence","11fdf6b6":"# loads the numpy arrays whose path is stored in the column \"col\" of \"data_frame\"\ndef load_col_data(data_frame ,indices, start_pos, end_pos, col):\n  col_list = []\n  for i in range(start_pos,end_pos):\n    idx = indices[i]\n    col_list.append(np.load(data_frame[col][idx]))\n    print(idx)\n  return np.array(col_list)","15aeb92a":"## preload training data into 8 separate folders to avoid issues with gdrive\ndef preload_training_data(cur_fraction, start_pos, end_pos):\n  input_spec = load_col_data(df_train, list(range(num_samples)) , start_pos , end_pos ,'input_spec_path' )\n  np.save(os.path.join(dataset_path,'PreLoad Training Dataset','fraction_'+str(cur_fraction),'input_spec'),input_spec)\n  output_spec = load_col_data(df_train, list(range(num_samples)), start_pos, end_pos, 'output_spec_path')\n  np.save(os.path.join(dataset_path,'PreLoad Training Dataset','fraction_'+str(cur_fraction),'output_spec'), output_spec)\n  dvec = load_col_data(df_train, list(range(num_samples)), start_pos, end_pos, 'dvector_path')\n  np.save(os.path.join(dataset_path,'PreLoad Training Dataset','fraction_'+str(cur_fraction),'dvec'),dvec)","7a8367d7":"dataset_train = 'train'\ndf_train = pd.read_csv(os.path.join(path[dataset_train],'data_frame.csv'))\nnum_samples = df_train.shape[0]","907ab8ea":"### execute just once ###\n# preload training data\nfor cur_fraction in range(num_fractions):\n  start_pos = 0\n  for i in range(cur_fraction):\n    start_pos+=fraction_sizes[i]\n  end_pos = start_pos + fraction_sizes[cur_fraction]\n  preload_training_data(cur_fraction, start_pos, end_pos)\n  start_pos, end_pos","bded09a6":"num_fractions = 8\nfraction_sizes = num_fractions * [ num_samples\/\/num_fractions ]\nfor i in range(num_samples%num_fractions):\n  fraction_sizes[i]+=1\nprint(fraction_sizes)\n\nsteps_per_epoch = 0\nfor i in range(num_fractions):\n  steps_per_epoch += (fraction_sizes[i]+batch_size-1)\/\/batch_size\nprint(steps_per_epoch)","cadbe4bb":"from tensorflow.keras.utils import Sequence","0a2b4303":"class data_generator(Sequence):\n  def __init__(self, fraction_sizes , batch_size):\n    self.batch_size = batch_size\n    self.fraction_sizes = fraction_sizes\n    self.num_fractions = len(fraction_sizes)\n    self.num_samples = sum(self.fraction_sizes)\n    self.pos = 0\n    self.cur_fraction = 0\n    self.input_spec=None\n    self.output_spec=None\n    self.dvec=None\n  def __len__(self):\n    self.num_batches = 0\n    for i in range(self.num_fractions):\n      self.num_batches += (self.fraction_sizes[i]+self.batch_size-1)\/\/self.batch_size\n    #print(\"len \",self.num_batches)\n    return self.num_batches\n  def __getitem__(self,batch_index):\n    if batch_index == 0:\n      self.pos = 0\n    start_pos = self.pos\n    end_pos = start_pos + self.batch_size\n    if end_pos > self.fraction_sizes[self.cur_fraction]:\n      end_pos = self.fraction_sizes[self.cur_fraction]\n    if start_pos == 0 :\n      print(\"Loading new data\")\n      if self.input_spec is not None:\n        print(\"de-allocating memory space of old data\")\n        del self.input_spec\n        del self.output_spec\n        del self.dvec\n        print(\"de-allocation complete\")\n      indices = np.random.permutation(self.fraction_sizes[self.cur_fraction])\n      self.input_spec = np.load(os.path.join(dataset_path,'PreLoad Training Dataset','fraction_'+str(self.cur_fraction),'input_spec.npy'))[indices]\n      self.output_spec = np.load(os.path.join(dataset_path,'PreLoad Training Dataset','fraction_'+str(self.cur_fraction),'output_spec.npy'))[indices]\n      self.dvec = np.load(os.path.join(dataset_path,'PreLoad Training Dataset','fraction_'+str(self.cur_fraction),'dvec.npy'))[indices]\n      del indices\n      print(\"New data loaded\")\n    if end_pos == self.fraction_sizes[self.cur_fraction]:\n      self.cur_fraction += 1\n      if self.cur_fraction == self.num_fractions:\n        self.cur_fraction = 0\n      self.pos = 0\n    else :\n      self.pos = end_pos\n    input_spec_batch = np.copy(self.input_spec[start_pos:end_pos])  # creating a copy so that input_spec, output_spec, dvec can be deleted when loading subsequent batches\n    dvector_batch = np.copy(self.dvec[start_pos:end_pos])\n    output_spec_batch = np.copy(self.output_spec[start_pos:end_pos])\n    return ({'input_spec':input_spec_batch, 'dvec': dvector_batch}, output_spec_batch)\n  def on_epoch_end(self):\n    self.pos = 0\n    self.cur_fraction = 0","ec01fe7c":"gen_train = data_generator(fraction_sizes, batch_size)","3f44044c":"from tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tqdm.notebook import tqdm","bf3e4c41":"model_checkpoint_callback = ModelCheckpoint(\n    os.path.join(dataset_path,'Model weights','weights_epoch{epoch:04d}.h5'),save_weights_only=True)","0201a74f":"### config ###\ninitial_epoch = 0\nepochs = 25\nbatch_size = 8","3e19dc8c":"#model.load_weights(os.path.join(dataset_path,'Model weights','weights_epoch0015.h5'))","92f4198c":"hist = model.fit(gen_train, epochs=epochs, verbose=1, steps_per_epoch = steps_per_epoch, shuffle=False,\n                 initial_epoch = initial_epoch,\n          callbacks=[ model_checkpoint_callback ] )","ebcfd05f":"loss = hist.history['loss']\nnp.save(os.path.join(dataset_path,'training_loss_total_epochs%04d'%total_epochs),np.array(loss))","5cc83a7f":"!pip install mir_eval","8c17050f":"import pandas as pd\nimport numpy as np\nfrom mir_eval.separation import bss_eval_sources\nfrom tqdm import tqdm","4c1fd0d1":"def load_col_data(data_frame ,indices, start_pos, end_pos, col):\n  col_list = []\n  for i in range(start_pos,end_pos):\n    idx = indices[i]\n    col_list.append(np.load(data_frame[col][idx]))\n    print(idx)\n  return np.array(col_list)","157e572b":"dataset = 'dev'","e63dc2dd":"### To be executed just once ###\n# preload all data into disk\ndf = pd.read_csv(os.path.join(path[dataset],'data_frame.csv'))\noutput_spec = load_col_data(df, list(range(df.shape[0])), 0 , df.shape[0] ,'output_spec_path')\noutput_phase = load_col_data(df,list(range(df.shape[0])), 0 , df.shape[0] ,'output_phase_path')\ninput_spec = load_col_data(df, list(range(df.shape[0])), 0 , df.shape[0] ,'input_spec_path')\ninput_phase = load_col_data(df,list(range(df.shape[0])), 0 , df.shape[0] ,'input_phase_path')\ndvec = load_col_data(df,list(range(df.shape[0])), 0 , df.shape[0] ,'dvector_path')\nnp.save(os.path.join(path[dataset],'output_spec.npy'),output_spec)\nnp.save(os.path.join(path[dataset],'output_phase.npy'),output_phase)\nnp.save(os.path.join(path[dataset],'input_spec.npy'),input_spec)\nnp.save(os.path.join(path[dataset],'input_phase.npy'),input_phase)\nnp.save(os.path.join(path[dataset],'dvec.npy'),dvec)","017dcef1":"# load the dev dataset\ninput_spec = np.load(os.path.join(path[dataset],'input_spec.npy'))\ninput_phase = np.load(os.path.join(path[dataset],'input_phase.npy'))\noutput_spec = np.load(os.path.join(path[dataset],'output_spec.npy'))\noutput_phase = np.load(os.path.join(path[dataset],'output_phase.npy'))\ndvec = np.load(os.path.join(path[dataset],'dvec.npy'))","f6b94a0e":"# re construct target waves using output spectrogram and output phase\ntarget_waves = []\nfor i in tqdm(range(output_spec.shape[0])):\n  target_waves.append(audio.spec2wave(output_spec[i], output_phase[i]))\nval_loss = []\nval_sdr = []","e13677ca":"def compute_loss_sdr(weights_path):\n  model.load_weights(weights_path)\n  predict_spec = model.predict(x={'input_spec':input_spec,'dvec':dvec} , batch_size = batch_size, verbose = 1)\n  val_loss.append(np.mean(np.square(output_spec - predict_spec)))\n  sdr=[]\n  for i in tqdm(range(predict_spec.shape[0])):\n    predict_wave = audio.spec2wave(predict_spec[i], input_phase[i])\n    sdr.append(bss_eval_sources(target_waves[i], predict_wave, False)[0][0])\n  val_sdr.append(np.median(np.array(sdr)))","e8844134":"### config ###\nstart_epochs = 21\nend_epochs = 26","ff6f3e2c":"for i in range(start_epochs,end_epochs):\n  weights_path = os.path.join(dataset_path,'Model weights','weights_epoch%04d.h5'%i)\n  compute_loss_sdr(weights_path)\nprint()\nprint(val_loss)\nprint(val_sdr)\nnp.save(os.path.join(dataset_path,'val_loss_total_epochs%04d'%(end_epochs-1)),np.array(val_loss))\nnp.save(os.path.join(dataset_path,'val_sdr_total_epochs%04d'%(end_epochs-1)),np.array(val_sdr))","d0fde023":"import matplotlib.pyplot as plt\n%matplotlib inline","4712364b":"loss = np.load(os.path.join(path[dataset],'training_loss_total_epochs%04d.npy'%(end_epochs-1)))","2762db63":"# plot loss on train and dev set\nfig , axis = plt.subplots(1,1,figsize = (9,6))\naxis.plot(np.array(range(1,len(loss)+1)),loss,label=\"training loss\")\naxis.plot(np.array(range(1,len(val_loss)+1)),val_loss,label=\"val loss\")\naxis.legend()\n#plt.xticks(np.array(range(len(loss)+1)))\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.title('Loss on dev set')\nplt.show()\nfig.savefig('loss.png')","ea4f374b":"# plot sdr on dev set\nfig , axis = plt.subplots(1,1,figsize = (6,6))\naxis.plot(np.array(range(len(val_sdr))), val_sdr, label = \"median sdr on val set\")\nplt.xlabel('epoch')\nplt.ylabel('median sdr')\nplt.title('SDR on dev set')\nplt.show()\nfig.savefig('sdr.png')","52a3e540":"dataset = 'test'","88d48b26":"# load the entire dataset\ninput_spec = np.load(os.path.join(path[dataset],'input_spec.npy'))\ninput_phase = np.load(os.path.join(path[dataset],'input_phase.npy'))\noutput_spec = np.load(os.path.join(path[dataset],'output_spec.npy'))\noutput_phase = np.load(os.path.join(path[dataset],'output_phase.npy'))\ndvec = np.load(os.path.join(path[dataset],'dvec.npy'))","273609c1":"# re construct target waves using output spectrogram and output phase\ntarget_waves = []\nfor i in range(output_spec.shape[0]):\n  target_waves.append(audio.spec2wave(output_spec[i], output_phase[i]))","127a601b":"sdr = []","8341f902":"def compute_loss_sdr_test(weights_path):\n  model.load_weights(weights_path)\n  predict_spec = model.predict(x={'input_spec':input_spec,'dvec':dvec} , batch_size = batch_size, verbose = 1)\n  test_loss = np.mean(np.square(output_spec - predict_spec))\n  for i in range(predict_spec.shape[0]):\n    predict_wave = audio.spec2wave(predict_spec[i], input_phase[i])\n    sdr.append(bss_eval_sources(target_waves[i], predict_wave, False)[0][0])\n  test_sdr = np.median(np.array(sdr))\n  return test_loss, test_sdr","a3bbb41c":"weights_path = os.path.join(dataset_path,'Model weights','weights_epoch0029.h5')\nprint(\"\")\nprint(\"loss, sdr : \",end=\"\")\nprint(compute_loss_sdr_test(weights_path))","5dbdf77d":"sdr = np.array(sdr)\nindices = np.argsort(sdr)\nnum = sdr.shape[0]\nprint(indices[:6])\nprint(indices[num\/\/2-3:num\/\/2+3])\nprint(indices[-6:])","f80a311f":"model.load_weights(os.path.join(dataset_path,'Model weights','weights_epoch0029.h5'))","0083a3f2":"## preprocess audio in the same way as training audio input\ndef preprocess_audio(audio_path):\n  wave,_ = librosa.load(audio_path,sr=hyper_params.sample_rate) \n  wave,_ = librosa.effects.trim(wave, top_db=20)  #trim silent portions\n  length_wave = int(hyper_params.sample_rate * hyper_params.data_audio_len) \n  if(wave.shape[0]<length_wave) :\n    return\n  wave = wave[:length_wave] #clip wave to a fixed wavelength\n  norm = np.max(np.abs(wave)) * 1.1 #normalize\n  wave = wave\/norm\n  return wave","c613af2d":"## pass input spec and dvec thru model and re construct wave after combining with input phase\ndef get_filtered_wave(input_spec_path, phase_path, dvec_path):\n  input_spec = np.load(input_spec_path)\n  phase = np.load(phase_path)\n  dvec = np.load(dvec_path)\n  dvec = np.expand_dims(dvec,axis=0)\n  input_spec = np.expand_dims(input_spec,axis=0)\n  clean_spec = model.predict(x={ 'input_spec' : input_spec, 'dvec' : dvec}, verbose=1)\n  wave = audio.spec2wave(clean_spec[0],phase)\n  return np.asarray(wave)","4daaa139":"from IPython.display import Audio as ipythonAudio\nimport pandas as pd\nimport numpy as np\nimport librosa\nimport random","e894eaef":"#workflow to test model on random audio speech\n#hyper_params = HyperParams()\n#audio = Audio(hyper_params)\n#wave = pre_process_audio(<audio path>)\n#spec = None\n#phase = None\n#if wave is not None: \n#  spec, phase = audio.wave2spec(wave)\n#clean_wave = store_audio_from_spec(spec , phase, <dvector path>)\n#if clean_wave is not None:\n#  librosa.output.write_wav('<some file name>.flac', clean_wave , sr=hyper_params.sample_rate)","46ab951e":"dataset_audio = 'test'\ndf = pd.read_csv(os.path.join(path[dataset_audio],'data_frame.csv'))\nprint(df)","b4f51f1c":"#idx = random.choice(range(df.shape[0]))\nidx = 943\nprint(idx)","5edd1ffc":"noisy_spec_path = df['input_spec_path'][idx]\noutput_spec_path = df['output_spec_path'][idx]\ninput_phase_path = df['input_phase_path'][idx]\noutput_phase_path = df['output_phase_path'][idx]\ndvector_path = df['dvector_path'][idx]\nnoisy_spec = np.load(noisy_spec_path)\ninput_phase = np.load(input_phase_path)","ddd32590":"# mixed audio input\nnoisy_wave = audio.spec2wave( noisy_spec, input_phase )\nipythonAudio(noisy_wave, rate = hyper_params.sample_rate)","17a710ad":"# filtered audio from model\nfiltered_wave = get_filtered_wave( noisy_spec_path, input_phase_path, dvector_path )\nipythonAudio(filtered_wave, rate = hyper_params.sample_rate)","da918cf4":"# ideal target wave\ntarget_wave = audio.spec2wave( np.load(output_spec_path), np.load(output_phase_path))\nipythonAudio(target_wave, rate = hyper_params.sample_rate)","81e40116":"librosa.output.write_wav('noise'+str(idx)+'.flac', noisy_wave , sr=hyper_params.sample_rate)\nlibrosa.output.write_wav('clean'+str(idx)+'.flac', filtered_wave , sr=hyper_params.sample_rate)\nlibrosa.output.write_wav('target'+str(idx)+'.flac', target_wave , sr=hyper_params.sample_rate)","e9ff72c5":"# ref speech used for d vector\nref_wave,_ = librosa.load(df['ref_speech'][idx],sr=hyper_params.sample_rate)\nprint(df['ref_speech'][idx])\nipythonAudio(ref_wave, rate = hyper_params.sample_rate)","e424310c":"# primary speech\npri_wave,_ = librosa.load(df['pri_speech'][idx],sr=hyper_params.sample_rate)\nprint(df['pri_speech'][idx])\nipythonAudio(pri_wave, rate = hyper_params.sample_rate)","d89e8a21":"# secondary speech\nsec_wave,_ = librosa.load(df['sec_speech'][idx],sr=hyper_params.sample_rate)\nprint(df['sec_speech'][idx])\nipythonAudio(sec_wave, rate = hyper_params.sample_rate)","f06c37b7":"### Data Analysis","ced0c90e":"#### Define paths and create folders","7d224cf9":"##### 200-251","46bbf0a3":"#### Hyper Parameters","3da421e6":"### Visualize model training","b888e8bc":"#### Grouping data to avoid gdrive timeout","bf99448e":"#### Audio related helper functions","a3a6523c":"### Test set metrics","575bb4cb":"### Use pre trained model to obtain Embedding","a0ccffaa":"##### 100-200","a3457af7":"### Create and store speech collection","01808674":"For more information on this notebook, checkout the following github repo :\n\nhttps:\/\/github.com\/jain-abhinav02\/VoiceFilter\n\nThis is an unofficial Tensorflow\/Keras implementation of Google AI VoiceFilter.\n\nOur work is inspired from the the academic paper : https:\/\/arxiv.org\/abs\/1810.04826\n\nThe implementation is based on the work : https:\/\/github.com\/mindslab-ai\/voicefilter","ffbdce97":"### Prepare dataset","68f95685":"#### Train set","22012274":"##### 0-100","443008c0":"### Mount drive","59e6f98d":"#### Dev set","0a91b82f":"### Unzip LibriSpeech dataset ( Execute just once )","9c199571":"### Load Data and Train\n","05c42d82":"### Basic necessary definitions","a7f493db":"#### Test set","0b09ffd1":"### Test audio samples","cb7d54aa":"### Create Model"}}