{"cell_type":{"777e80d4":"code","f7de3c4f":"code","02e420a9":"code","5b642c7f":"code","32daa6bb":"code","18da014d":"code","b4f1a94c":"code","b5339f44":"code","b9197a2d":"code","ef7749f5":"code","0392ead8":"code","628e8aa9":"code","54577afa":"code","ba55344e":"code","9f8a6290":"code","60f9b3d7":"code","3af334f6":"code","a8cfda09":"code","8f3ba901":"code","9972d50f":"code","dfb7d316":"code","bcd38b39":"code","3dc89e47":"code","eee0c2a9":"code","fb99519e":"code","fc76893d":"code","45977a47":"code","0736c64a":"code","fc5a7378":"code","ccbf92ed":"code","344e1d18":"code","6b2b9975":"code","71a78df4":"code","bc82ba75":"code","fbe04574":"code","4acce8e4":"code","37bbeef1":"code","53d48e16":"code","f636e638":"code","76279775":"code","8c44882c":"code","03d5dd17":"code","4064650e":"code","a15819e1":"code","e98e1d36":"code","13bafb77":"code","6e1a36db":"code","06003e8f":"code","87703803":"code","a80d939b":"code","56408aae":"code","be5e60c5":"code","adbc58fc":"code","f3df7d1a":"code","b8d52a03":"code","71269e61":"markdown","5b1ec75c":"markdown","76a26689":"markdown","a7501039":"markdown","afcce7d2":"markdown"},"source":{"777e80d4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f7de3c4f":"import pandas as pd \nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#import missingno as msno\n\n\n%matplotlib inline","02e420a9":"df = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')","5b642c7f":"df.head(10)","32daa6bb":"df.info()","18da014d":"df.describe()","b4f1a94c":"corrmat = df.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True,cmap=\"YlGnBu\",annot =True);","b5339f44":"df.columns","b9197a2d":"agee  = df.age","ef7749f5":"agee.value_counts().sort_index()","0392ead8":"df.sex.value_counts().sort_index()","628e8aa9":"df.trestbps.value_counts().sort_index()","54577afa":"df.chol.value_counts().sort_index()","ba55344e":"df.thalach.value_counts().sort_index()","9f8a6290":"pd.crosstab(df.age,df.target).plot(kind=\"bar\",figsize=(20,6))\nplt.title('Heart Disease Frequency for Ages')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.savefig('heartDiseaseAndAges.png')\nplt.show()","60f9b3d7":"sns.kdeplot(df.age)","3af334f6":"sns.set(rc={'figure.figsize':(35,20)})\nsns.violinplot(x='age',y='trestbps',data=df,inner=None,hue='target')\n\nplt.show()","a8cfda09":"#scatterplot\nsns.set()\ncols = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'target']\nsns.pairplot(df[cols], height = 2.5)\nplt.show();","8f3ba901":"a = pd.get_dummies(df['cp'],prefix='cp')\nb = pd.get_dummies(df['thal'],prefix='thal')\nc = pd.get_dummies(df['slope'],prefix='slope')","9972d50f":"df = pd.concat([df,a,b,c],axis=1)\ndf.head()","dfb7d316":"df.drop(columns=['cp','thal','slope'],inplace=True)\ndf.head()","bcd38b39":"from sklearn.preprocessing import StandardScaler\n\ns_sc = StandardScaler()\ncol_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\ndf[col_to_scale] = s_sc.fit_transform(df[col_to_scale])","3dc89e47":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\ndef print_score(clf,X_train,y_train,X_test,y_test,train=True):\n    if train:\n        pred = clf.predict(X_train)\n        clf_report = pd.DataFrame(classification_report(y_train,pred,output_dict=True))\n        print(\"Train Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n        \n    elif train==False:\n        pred = clf.predict(X_test)\n        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n        print(\"Test Result:\\n================================================\")        \n        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")","eee0c2a9":"from sklearn.model_selection import train_test_split\n\nX = df.drop('target', axis=1)\ny = df.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","fb99519e":"len(X_train), len(y_train)\n","fc76893d":"from sklearn.linear_model import LogisticRegression\n\nlr_clf = LogisticRegression(solver='liblinear')\nlr_clf.fit(X_train, y_train)\n\nprint_score(lr_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(lr_clf, X_train, y_train, X_test, y_test, train=False)","45977a47":"test_score = accuracy_score(y_test, lr_clf.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, lr_clf.predict(X_train)) * 100\n\nresults_df = pd.DataFrame(data=[[\"Logistic Regression\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df","0736c64a":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_clf = KNeighborsClassifier()\nknn_clf.fit(X_train, y_train)\n\nprint_score(knn_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(knn_clf, X_train, y_train, X_test, y_test, train=False)","fc5a7378":"test_score = accuracy_score(y_test, knn_clf.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, knn_clf.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"K-nearest neighbors\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","ccbf92ed":"from sklearn.svm import SVC\n\n\nsvm_clf = SVC(kernel='rbf', gamma=0.1, C=1.0)\nsvm_clf.fit(X_train, y_train)\n\nprint_score(svm_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(svm_clf, X_train, y_train, X_test, y_test, train=False)","344e1d18":"test_score = accuracy_score(y_test, svm_clf.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, svm_clf.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"Support Vector Machine\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","6b2b9975":"from sklearn.tree import DecisionTreeClassifier\n\n\ntree_clf = DecisionTreeClassifier(random_state=42)\ntree_clf.fit(X_train, y_train)\n\nprint_score(tree_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(tree_clf, X_train, y_train, X_test, y_test, train=False)","71a78df4":"test_score = accuracy_score(y_test, tree_clf.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, tree_clf.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"Decision Tree Classifier\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","bc82ba75":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrf_clf = RandomForestClassifier(n_estimators=1000, random_state=42)\nrf_clf.fit(X_train, y_train)\n\nprint_score(rf_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(rf_clf, X_train, y_train, X_test, y_test, train=False)","fbe04574":"test_score = accuracy_score(y_test, rf_clf.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, rf_clf.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"Random Forest Classifier\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","4acce8e4":"from xgboost import XGBClassifier\n\nxgb_clf = XGBClassifier()\nxgb_clf.fit(X_train, y_train)\n\nprint_score(xgb_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(xgb_clf, X_train, y_train, X_test, y_test, train=False)","37bbeef1":"test_score = accuracy_score(y_test, xgb_clf.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, xgb_clf.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"XGBoost Classifier\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","53d48e16":"from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n\nparams = {\"C\": np.logspace(-4, 4, 20),\n          \"solver\": [\"liblinear\"]}\n\nlr_clf = LogisticRegression()\n\nlr_cv = GridSearchCV(lr_clf, params, scoring=\"accuracy\", n_jobs=-1, verbose=1, cv=5, iid=True)\nlr_cv.fit(X_train, y_train)\nbest_params = lr_cv.best_params_\nprint(f\"Best parameters: {best_params}\")\nlr_clf = LogisticRegression(**best_params)\n\nlr_clf.fit(X_train, y_train)\n\nprint_score(lr_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(lr_clf, X_train, y_train, X_test, y_test, train=False)","f636e638":"test_score = accuracy_score(y_test, lr_clf.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, lr_clf.predict(X_train)) * 100\n\ntuning_results_df = pd.DataFrame(data=[[\"Tuned Logistic Regression\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\ntuning_results_df","76279775":"train_score = []\ntest_score = []\nneighbors = range(1, 30)\n\nfor k in neighbors:\n    model = KNeighborsClassifier(n_neighbors=k)\n    model.fit(X_train, y_train)\n    train_score.append(accuracy_score(y_train, model.predict(X_train)))\n#     test_score.append(accuracy_score(y_test, model.predict(X_test)))","8c44882c":"plt.figure(figsize=(12, 8))\n\nplt.plot(neighbors, train_score, label=\"Train score\")\n# plt.plot(neighbors, test_score, label=\"Test score\")\nplt.xticks(np.arange(1, 21, 1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\nprint(f\"Maximum KNN score on the test data: {max(train_score)*100:.2f}%\")","03d5dd17":"knn_clf = KNeighborsClassifier(n_neighbors=27)\nknn_clf.fit(X_train, y_train)\n\nprint_score(knn_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(knn_clf, X_train, y_train, X_test, y_test, train=False)","4064650e":"test_score = accuracy_score(y_test, knn_clf.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, knn_clf.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"Tuned K-nearest neighbors\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\ntuning_results_df = tuning_results_df.append(results_df_2, ignore_index=True)\ntuning_results_df\n","a15819e1":"svm_clf = SVC(kernel='rbf', gamma=0.1, C=1.0)\n\nparams = {\"C\":(0.1, 0.5, 1, 2, 5, 10, 20), \n          \"gamma\":(0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1), \n          \"kernel\":('linear', 'poly', 'rbf')}\n\nsvm_cv = GridSearchCV(svm_clf, params, n_jobs=-1, cv=5, verbose=1, scoring=\"accuracy\")\nsvm_cv.fit(X_train, y_train)\nbest_params = svm_cv.best_params_\nprint(f\"Best params: {best_params}\")\n\nsvm_clf = SVC(**best_params)\nsvm_clf.fit(X_train, y_train)\n\nprint_score(svm_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(svm_clf, X_train, y_train, X_test, y_test, train=False)","e98e1d36":"test_score = accuracy_score(y_test, svm_clf.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, svm_clf.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"Tuned Support Vector Machine\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\ntuning_results_df = tuning_results_df.append(results_df_2, ignore_index=True)\ntuning_results_df","13bafb77":"params = {\"criterion\":(\"gini\", \"entropy\"), \n          \"splitter\":(\"best\", \"random\"), \n          \"max_depth\":(list(range(1, 20))), \n          \"min_samples_split\":[2, 3, 4], \n          \"min_samples_leaf\":list(range(1, 20))\n          }\n\ntree_clf = DecisionTreeClassifier(random_state=42)\ntree_cv = GridSearchCV(tree_clf, params, scoring=\"accuracy\", n_jobs=-1, verbose=1, cv=3, iid=True)\ntree_cv.fit(X_train, y_train)\nbest_params = tree_cv.best_params_\nprint(f'Best_params: {best_params}')\n\ntree_clf = DecisionTreeClassifier(**best_params)\ntree_clf.fit(X_train, y_train)\n\nprint_score(tree_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(tree_clf, X_train, y_train, X_test, y_test, train=False)","6e1a36db":"test_score = accuracy_score(y_test, tree_clf.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, tree_clf.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"Tuned Decision Tree Classifier\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\ntuning_results_df = tuning_results_df.append(results_df_2, ignore_index=True)\ntuning_results_df","06003e8f":"n_estimators = [int(x) for x in np.linspace(start=200, stop=1000, num=10)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(10, 110, num=11)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\n\nparams_grid = {'n_estimators': n_estimators, 'max_features': max_features,\n               'max_depth': max_depth, 'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}\n\nrf_clf = RandomForestClassifier(random_state=42)\n\nrf_cv = RandomizedSearchCV(rf_clf, params_grid, scoring=\"accuracy\", cv=3, verbose=2, n_jobs=-1)\n\n\nrf_cv.fit(X_train, y_train)\nbest_params = rf_cv.best_params_\nprint(f\"Best parameters: {best_params}\")\n\nrf_clf = RandomForestClassifier(**best_params)\nrf_clf.fit(X_train, y_train)\n\nprint_score(rf_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(rf_clf, X_train, y_train, X_test, y_test, train=False)","87703803":"test_score = accuracy_score(y_test, rf_clf.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, rf_clf.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"Tuned Random Forest Classifier\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\ntuning_results_df = tuning_results_df.append(results_df_2, ignore_index=True)\ntuning_results_df","a80d939b":"\n\n\ngbm_param_grid = {\n    'subsample': np.arange(0.05, 1, 0.05),\n    'max_depth': np.arange(3, 20, 1),\n    'colsample_bytree': np.arange(0.1, 1.05, .05)}                      \n\nxgb_clf = XGBClassifier()\n\nxgb_cv = RandomizedSearchCV(xgb_clf, gbm_param_grid, cv=3, scoring = 'accuracy', verbose=1)\n\n\nxgb_cv.fit(X_train, y_train)\nbest_params = xgb_cv.best_params_\nprint(f\"Best paramters: {best_params}\")\n\nxgb_clf = XGBClassifier(**best_params)\nxgb_clf.fit(X_train, y_train)\n\nprint_score(xgb_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(xgb_clf, X_train, y_train, X_test, y_test, train=False)","56408aae":"test_score = accuracy_score(y_test, xgb_clf.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, xgb_clf.predict(X_train)) * 100\n\nresults_df_2 = pd.DataFrame(data=[[\"Tuned XGBoost Classifier\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\ntuning_results_df = tuning_results_df.append(results_df_2, ignore_index=True)\ntuning_results_df","be5e60c5":"results_df","adbc58fc":"def feature_imp(df, model):\n    fi = pd.DataFrame()\n    fi[\"feature\"] = df.columns\n    fi[\"importance\"] = model.feature_importances_\n    return fi.sort_values(by=\"importance\", ascending=False)","f3df7d1a":"feature_imp(X, rf_clf).plot(kind='barh', figsize=(12,7), legend=False)","b8d52a03":"feature_imp(X, rf_clf).plot(kind='barh', figsize=(12,7), legend=False)","71269e61":"# Data verification \ncheck data is in limits","5b1ec75c":"# Hyper-Parameter Tuning","76a26689":"# Important Features ","a7501039":"# Data cleaning","afcce7d2":"# Data Exploration"}}