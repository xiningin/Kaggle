{"cell_type":{"d16945e1":"code","453634f6":"code","b7616fbc":"code","87d229f5":"code","81c5df6e":"code","0f6b6064":"code","80c729e9":"code","59f71210":"code","b2c2ed17":"code","6823d8ef":"code","33d1e326":"code","cfdbc377":"code","5f86d90c":"markdown","31312df8":"markdown","76eb2fb4":"markdown","7186b06d":"markdown","0c4788b3":"markdown"},"source":{"d16945e1":"import pickle\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import f1_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom functools import partial\n\nimport tensorflow as tf\n\nAUTO = tf.data.experimental.AUTOTUNE","453634f6":"class Params:\n\n    LEARNING_RATE = 0.001\n    BATCH_SIZE = 32\n    EPOCHS = 5\n\n    #parameters for dataset\n    CLASSES = [\n        'Atelectasis',\n        'Cardiomegaly',\n        'Consolidation',\n        'Edema',\n        'Effusion',\n        'Emphysema',\n        'Fibrosis',\n        'Hernia',\n        'Infiltration',\n        'Mass',\n        # original label 'No Finding'\n        # we don't include it in our labels\n        # 'No_Finding',\n        'Nodule',\n        'Pleural_Thickening',\n        'Pneumonia',\n        'Pneumothorax'\n    ]\n    NO_FINDING = 'No_Finding'\n    PNEUMONIA = 'Pneumonia'\n    NOT_PNEUMONIA = 'not ' + PNEUMONIA\n    TRAIN_SHUFFLE_BUFFER = 2048\n\n    # parameters for the model\n    IMAGE_SIZE = [224, 224]\n    CHANNELS = 3\n\n    # parameters for the trainer\n    TRAIN_SPLIT = .8\n    VAL_SPLIT = .1\n    TEST_SPLIT = .1\n\n    SEED = 42\n\n\nparams = Params()\n\n\nclass ChexnetUtils:\n\n    @staticmethod\n    def plot_sample_from_dataset(dataset, is_prep=True, is_pneumonia=True):\n        dataset = dataset.take(9)\n        plt.figure(figsize=(8, 8))\n        subplot = 331\n        for i, (image, label) in enumerate(dataset):\n            plt.subplot(subplot)\n            plt.axis('off')\n\n            image = image.numpy()\n            if is_prep: image = image \/ 2 + .5\n            plt.imshow(image)\n\n            label = label.numpy()\n            if is_pneumonia: label = ChexnetUtils.get_label_pneumonia(label)\n            else: label = ChexnetUtils.get_label_full(label)\n            plt.title(label, fontsize=12)\n\n            subplot += 1\n            if i == 8: break\n        # plt.tight_layout()\n        plt.subplots_adjust(wspace=0.1, hspace=0.1)\n\n    @staticmethod\n    def get_label_pneumonia(label):\n        return params.PNEUMONIA if label else params.NOT_PNEUMONIA\n\n    @staticmethod\n    def get_label_full(labels_arr):\n        labels = [params.CLASSES[i] for i, j in enumerate(labels_arr) if j]\n        if len(labels) == 0:\n            return params.NO_FINDING\n        return '|'.join(labels)\n\n    @staticmethod\n    def plot_sample_from_batch(batched_dataset, is_pneumonia=True):\n        dataset = batched_dataset.take(1).unbatch()\n        ChexnetUtils.plot_sample_from_dataset(dataset, is_pneumonia=is_pneumonia)\n\n    @staticmethod\n    def save_history(history, trainer):\n        filename = trainer.history_file\n        with open(filename, 'wb') as f:\n            pickle.dump(history.history, f)\n\n    @staticmethod\n    def load_history(trainer):\n        filename = trainer.history_file\n        with open(filename, \"rb\") as f:\n            history = pickle.load(f)\n        return history\n\n    @staticmethod\n    def plot_metric(metric, trainer):\n        history = ChexnetUtils.load_history(trainer)\n        plt.plot(history[metric], label=metric)\n        val_metric = f'val_{metric}'\n        plt.plot(history[val_metric], label=val_metric)\n        plt.legend()\n        plt.title(metric)\n\n    # @staticmethod\n    # # TODO correct plotting of history with continued epochs\n    # # TODO 2 history and weight files\n    # def train_with_fine_tuning(metric='accuracy'):\n    #     ft = FlowerTrainer()\n    #     ft.train()\n    #     FlowerUtils.plot_metric(metric=metric, trainer=ft)\n    #     params.LEARNING_RATE \/= 10\n    #     params.EPOCHS = 20\n    #     ft = FlowerTrainer(is_fine_tune=True)\n    #     ft.train()\n    #     FlowerUtils.plot_metric(metric=metric, trainer=ft)\n    #     ft.evaluate()\n\n\nclass CheXNetDataset:\n    \"\"\"\n    This class writes TFRecord files for NIH\n    \"\"\"\n\n    def __init__(self, is_pneumonia=True):\n\n        # directories and files\n        self.DATA_DIR = Path('\/kaggle\/input\/data')\n        self.WORKING_DIR = Path('\/kaggle\/working')\n        self.DF_LABELS_PATH = self.DATA_DIR \/ 'Data_Entry_2017.csv'\n        self.DF_FILES_PATH = self.WORKING_DIR \/ 'df_files.csv'\n\n        # dataframes columns\n        self.FILE_PATH = 'file_path'\n        self.PATIENT_ID = 'patient_id'\n        self.SPLIT = 'split'\n        self.LABELS = 'labels'\n        self.LABELS_ARRAY = 'labels_array'\n        self.LABEL_PNEUMONIA = 'label_pneumonia'\n        self.PNEUMONIA = 'Pneumonia'\n\n        # main dataframes and dictionaries\n        self.df_original = None\n        self.labels_dict = None\n        self._get_labels_dict()\n        self.df_files = None\n        self._get_df_files()\n\n        # dataframes for splits\n        self.SPLIT_TRAIN = 'train'\n        self.SPLIT_VAl = 'val'\n        self.SPLIT_TEST = 'test'\n        self.df_train = None\n        self.df_val = None\n        self.df_test = None\n        self._get_patient_splits()\n\n        # datasets\n        self.is_pneumonia = is_pneumonia\n        self.ds_train = None\n        self.ds_val = None\n        self.ds_test = None\n        self._get_datasets()\n\n    def _get_labels_dict(self):\n        print(f'===> getting label_dict ... ')\n        self.df_original = pd.read_csv(self.DF_LABELS_PATH)\n        # change 'No Finding' to 'No_Finding'\n        self.df_original .loc[self.df_original['Finding Labels'] == 'No Finding', 'Finding Labels'] = 'No_Finding'\n        # get mapping of image name to its labels\n        self.labels_dict = pd.Series(self.df_original['Finding Labels'].values,\n                                     index=self.df_original['Image Index']).to_dict()\n        print(f'===> getting label_dict DONE ')\n\n    def _get_df_files(self):\n\n        print(f'===> getting df_files ... ')\n\n        # get list of paths for all image files\n        if self.DF_FILES_PATH.is_file():\n            files_str = list(pd.read_csv('df_files.csv').file_path.values)\n        else:\n            files_str = [str(fp) for fp in self.DATA_DIR.glob('**\/**\/*.png')]\n\n        # create df that contain all image files\n        self.df_files = pd.DataFrame(list(files_str), columns=[self.FILE_PATH], dtype='string')\n\n        # add column with patient_id\n        def get_patient(file_path): return str(file_path).split('\/')[-1][:-8]\n        self.df_files[self.PATIENT_ID] = self.df_files[self.FILE_PATH].apply(get_patient).astype('string')\n\n        # add column with labels from original df provided in dataset\n        def get_label(file_path): return self.labels_dict[file_path.split('\/')[-1]]\n        self.df_files[self.LABELS] = self.df_files[self.FILE_PATH].apply(get_label).astype('string')\n\n        # add labels as array for full 14 disease problem\n        def get_labels_as_array(labels):\n            labels = labels.split('|')\n            labels_arr = np.zeros(len(params.CLASSES))\n            for label in labels:\n                if label != params.NO_FINDING:\n                    index = params.CLASSES.index(label)\n                    labels_arr[index] = 1\n            return labels_arr\n        self.df_files[self.LABELS_ARRAY] = self.df_files[self.LABELS].apply(get_labels_as_array)\n\n        # add label for pneumonia\n        self.df_files[self.LABEL_PNEUMONIA] = self.df_files[self.LABELS].str.contains(self.PNEUMONIA).astype(int)\n\n        # safe list of files to disc\n        if not self.DF_FILES_PATH.is_file():\n            self.df_files.file_path.to_csv(self.DF_FILES_PATH, index=False)\n\n        print(f'===> getting df_files DONE ')\n\n    def _get_patient_splits(self):\n\n        print(f'===> getting patient_splits ... ')\n\n        # get unique patient ids\n        patients = self.df_files[self.PATIENT_ID].unique()\n\n        # split patients using sklearn\n        np.random.seed(params.SEED)\n        patients_train, patients_val = train_test_split(list(patients),\n                                                        train_size=params.TRAIN_SPLIT,\n                                                        test_size=1-params.TRAIN_SPLIT,\n                                                        random_state=params.SEED)\n        patients_val, patients_test = train_test_split(patients_val,\n                                                       train_size=.5,\n                                                       test_size=.5,\n                                                       random_state=params.SEED)\n        patients_train, patients_val, patients_test = set(patients_train), set(patients_val), set(patients_test)\n\n        # add split to main dataframe\n        def get_split(patient_id):\n            if patient_id in patients_train: return self.SPLIT_TRAIN\n            elif patient_id in patients_val: return self.SPLIT_VAl\n            elif patient_id in patients_test: return self.SPLIT_TEST\n            else: raise ValueError('wrong patient_id')\n\n        self.df_files[self.SPLIT] = self.df_files[self.PATIENT_ID].apply(get_split).astype('string')\n\n        # get split dataframes\n        self.df_train = self.df_files[self.df_files[self.SPLIT] == self.SPLIT_TRAIN]\n        self.df_val = self.df_files[self.df_files[self.SPLIT] == self.SPLIT_VAl]\n        self.df_test = self.df_files[self.df_files[self.SPLIT] == self.SPLIT_TEST]\n\n        print(f'===> getting patient_splits DONE ')\n\n    def _get_dataset(self, df, is_train=False):\n\n        def _prepocess(file_path, label, _is_train=False):\n            image = _decode_image(file_path)\n            if _is_train:\n                image = tf.image.random_flip_left_right(image)\n            image = tf.cast(image, tf.float32)\n            image = tf.keras.applications.xception.preprocess_input(image)\n            return image, label\n\n        def _decode_image(file_path):\n            image_bytes = tf.io.read_file(file_path)\n            image = tf.image.decode_jpeg(image_bytes, channels=3)\n            image = tf.image.resize(image, params.IMAGE_SIZE)\n            return image\n\n        # load slices of dataframe into dataset\n        if self.is_pneumonia: labels = df[self.LABEL_PNEUMONIA].values\n        else: labels = np.stack(df[self.LABELS_ARRAY].values, axis=0)\n        files = df[self.FILE_PATH].values\n        list_ds = tf.data.Dataset.from_tensor_slices((files, labels))\n\n        # preprocess dataset\n        dataset = list_ds.map(partial(_prepocess, _is_train=is_train))\n\n        if is_train:\n            dataset = dataset.shuffle(params.TRAIN_SHUFFLE_BUFFER).repeat()\n\n        dataset = dataset.batch(params.BATCH_SIZE).prefetch(AUTO)\n\n        return dataset\n\n    def _get_datasets(self):\n        self.ds_train = self._get_dataset(df=self.df_train, is_train=True)\n        self.ds_val = self._get_dataset(df=self.df_val)\n        self.ds_test = self._get_dataset(df=self.df_test)","b7616fbc":"cds = CheXNetDataset()","87d229f5":"cds.df_files.head()","81c5df6e":"cds.df_files.split.value_counts()","0f6b6064":"np.sum(cds.df_files.split.value_counts())","80c729e9":"cds.df_train.shape, cds.df_val.shape, cds.df_test.shape","59f71210":"for image_batch, label_batch in cds.ds_train.take(1):\n    print(image_batch.shape, label_batch.shape)","b2c2ed17":"ChexnetUtils.plot_sample_from_batch(cds.ds_train)","6823d8ef":"cds = CheXNetDataset(is_pneumonia=False)","33d1e326":"for image_batch, label_batch in cds.ds_train.take(1):\n    print(image_batch.shape, label_batch.shape, label_batch[0])","cfdbc377":"ChexnetUtils.plot_sample_from_batch(cds.ds_train, is_pneumonia=False)","5f86d90c":"## 1 - dataframes","31312df8":"If someone prefres to work with `tf.data.Dataset` instead of `keras` generators here's the code to get these datasets. To create datasets for all splits just create an instance of `CheXNetDataset`. To get a dataset call `cds.ds_train` for train dataset and so on.\n\nWe may create datasets with 2 types of labels: for detection of `Pneumonia` or all 14 diseases. See more detailed information below.","76eb2fb4":"## 3 - dataset: 14 diseases","7186b06d":"To get splits and labels we build the `dataframe`. We first split patients and then use it to split files. It's necessary to prevent overlapping between splits. We also add columns for 2 cases: detection of pneumonia and detection of 14 diseases.","0c4788b3":"## 2 - dataset: pneumonia"}}