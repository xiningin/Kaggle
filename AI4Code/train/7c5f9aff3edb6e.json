{"cell_type":{"64df53aa":"code","4775b5c2":"code","2f4a7a54":"code","e6ef3aa9":"code","6500c4fa":"code","cfb55843":"code","0b32046a":"code","cff3ea99":"code","beba5dc6":"code","dd8a5f04":"markdown","4a230777":"markdown","ec9261aa":"markdown","a508bd5c":"markdown","503df4ee":"markdown","e5935e26":"markdown"},"source":{"64df53aa":"\n\n\n\n#importeert libraries voor neural network\n#numpy voor de calculus\n#pandas voor het lezen van de data\n#matplotlib voor visualiseren van voorspellingen\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\n#met deze functie laadt pandas de MNIST database in\ndata = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ndata.head().T\n","4775b5c2":"#maakt een matrix van de data, dit is handig voor de latere vectorvermenigvuldiging\ndata = np.array(data)\n\n#m is het aantal rijen en correspondeert naar het aantal plaatjes in de dataset \n#n is het aantal kolommen en correspondeert naar het aantal pixels van het plaatje (784)\nm, n = data.shape\n\n#Het husselen van de plaatjes door elkaar\nnp.random.shuffle(data)\n\n#Eerste duizend plaatjes als controleset\n#Transpose functie draait rijen en kolommen om, opdat elke kolom uit 784 pixels bestaat\ndata_dev = data[0:1000].T\n#Een rij van alle labels (welke getal het plaatje is)\nY_dev = data_dev[0]\n#Alle pixels (784)\nX_dev = data_dev[1:n]\n\n#Zelfde wordt gedaan voor de training data, alle plaatjes behalve de eerste duizend\n#X_dev = X_dev \/225; de waarde van elke pixel is van 0-225 (waar 0 pikzwart is 255 fel wit)\n#Een neural network gedijdt beter met waardes van dezelfde orde, vandaar delen door 255\nX_dev = X_dev \/ 255.\ndata_train = data[1000:m].T\nY_train = data_train[0]\nX_train = data_train[1:n]\nX_train = X_train \/ 255.\n#shape functie geeft de grootte van x_train aan\n_,m_train = X_train.shape\n","2f4a7a54":"Y_train","e6ef3aa9":"#De init_params functie initialiseert de waardes van de weights en biases als willekeurige getallen (Normale verdeling)\n#W1 zijn de weights van de input layer --> de hidden layer\n#b1 zijn de biases van de input layer --> de hidden layer\n#W2 zijn de weights van de hidden layer --> de output layer\n#b2 zijn de biases van de hidden layer --> de output layer\ndef init_params():\n    W1 = np.random.rand(10, 784) - 0.5\n    b1 = np.random.rand(10, 1) - 0.5\n    W2 = np.random.rand(10, 10) - 0.5\n    b2 = np.random.rand(10, 1) - 0.5\n    return W1, b1, W2, b2\n\n#De ReLU functie (alle negatieve getallen worden nul, alle positieve getallen blijven hetzelfde)\ndef ReLU(Z):\n    return np.maximum(Z, 0)\n\n#De bekende sigmoid functie: transformeert weighted sum naar getallen tussen 0 en 1 voor output layer\ndef sigmoid(Z):\n    B = 1 \/ (1 + np.exp(-Z))\n    return B\n#De forward_prop functie waar je een plaatje door het neurale netwerk laat gaan\n#De variabele X is de representatie van de waarden alle 784 pixels\n#Z1 vermenigvuldigt de weights * X + bias. Dit was de weighted sum\n#A1 is de ReLU van Z1\n#Z1 is wederom een weighted sum, nu van de hidden layer --> output layer.\n#A2 is de softmax van Z2\ndef forward_prop(W1, b1, W2, b2, X):\n    Z1 = W1.dot(X) + b1\n    A1 = ReLU(Z1)\n    Z2 = W2.dot(A1) + b2\n    A2 = sigmoid(Z2)\n    return Z1, A1, Z2, A2\n#Afgeleide van de ReLU, is gelijk aan 1 of nul. \n#Afgeleide is wederom gelijk aan nul als de input negatief was, 1 als de input positief was.\ndef ReLU_deriv(Z):\n    return Z > 0\n#De one_hot functie stelt een soort antwoordenschema op. We kunnen daarmee kijken of de uitkomst van de output layer overeenkomt met de werkelijke labels. \n#Eerst wordt er een matrix met tien rijen gemaakt gevuld met nullen.\n#Vervolgens wordt er gekeken welk label corespondeert aan de correcte plaats van de kolom, dan wordt dat getal naar 1 gezet\n#Stel het netwerk heeft input een plaatje met getal drie en dus label drie, dan zet de one_hot functie de derde kolom naar 1.\n#Transpose functie draaide kolommen en rijen om, dus de derde rij wordt 1 (rest van de rijen is nul)\ndef one_hot(Y):\n    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n    one_hot_Y[np.arange(Y.size), Y] = 1\n    one_hot_Y = one_hot_Y.T\n    return one_hot_Y\n\n#De functie backward_prop berekent hoe \"fout\" de weights en biases zaten.\n#Eerst wordt de zojuist uitgelegde one_hot(Y) gecalled\n#dZ2 (delta Z2) is het verschil tussen het juiste antwoord (one_hot(y)) en het daadwerkelijke antwoord van het netwerk. Het is optimaal wanneer dit nul nadert\n#dW2 geeft aan hoe \"fout\" de weights van de hidden layer --> de output layer gemiddeld zaten.\n#Wiskundig bereken je hier de gradient, wat dus aangeeft welke richting de weights moeten gaan om dZ1 te vergroten (we willen verkleinen dus later wordt de negatieve gradient gehanteerd)\n#db2: zelfde als dW2 maar dan voor de biases\n#dZ1: hier komt de term backpropagation terug, er wordt terug gerekend\n#Dit berekent wat de weighted sum in de hidden layer had moeten zijn\n#Vanuit daar kunnen we de gemiddelde juiste richting waarop weights (dw1) en biases (db1) moeten gaan, van input layer --> hidden layer, berekenen\n#Met deze informatie kunnen we de weights en biases aanpassen\ndef backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n    one_hot_Y = one_hot(Y)\n    dZ2 = (A2 - one_hot_Y)\n    dW2 = 1 \/ m * dZ2.dot(A1.T)\n    db2 = 1 \/ m * np.sum(dZ2)\n    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n    dW1 = 1 \/ m * dZ1.dot(X.T)\n    db1 = 1 \/ m * np.sum(dZ1)\n    return dW1, db1, dW2, db2\n#In de functie update_params passen wij de weights en biases aan met de informatie uit backward_prop\n#Alpha is de learning rate, de grootte van de stappen bij het bepalen van de juiste weights en biases\n#W1 zijn de onveranderde weights - de richting waarheen de weights heen moeten om een kleinere cost te creeren * de stapgrootte\n#b1; wederom hetzelfde voor de biases\n#W2 en b2 hetzelfde wordt gedaan voor de weights en biases in de layers; hidden --> output\ndef update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n    W1 = W1 - alpha * dW1\n    b1 = b1 - alpha * db1    \n    W2 = W2 - alpha * dW2  \n    b2 = b2 - alpha * db2    \n    return W1, b1, W2, b2","6500c4fa":"#De get_predictions functie geeft de voorspellingen van de AI in de output layer\ndef get_predictions(A2):\n    return np.argmax(A2, 0)\n#In de get_accuracy functie wordt berekend hoeveel procent van de antwoorden van het netwerk goed waren\ndef get_accuracy(predictions, Y):\n    print(predictions, Y)\n    return np.sum(predictions == Y) \/ Y.size\n#De gradient descent is het daadwerkelijke leermechanisme van het neurale netwerk\n#Eerst wordt de init_params functie ingeroepen, deze maakt de weights en biases\n#Vervolgens is er een loop die als volgende loopt;\n#Eerst forward propagation\n#Dan backward propagation\n#Aanpassen weights en biases\n#En de loop begint opnieuw\n#En bij elke tiende herhaling wordt de \"accuracy\" van het netwerk geprint\ndef gradient_descent(X, Y, alpha, iterations):\n    W1, b1, W2, b2 = init_params()\n    for i in range(iterations):\n        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n        if i % 10 == 0:\n            print(\"Iteration: \", i)\n            predictions = get_predictions(A2)\n            print(get_accuracy(predictions, Y))\n    return W1, b1, W2, b2","cfb55843":"W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 700)","0b32046a":"#Deze functie voorspelt het juiste getal met het nu getrainde neurale netwerk\n#De forward_prop functie wordt ingeroepen en de geeft de matrix van de output layer\n#Vervolgens wordt de get_predictions functie van eerder gebruikt welke het grootste getal (het dichts bij 1) uit die matrix geeft\ndef make_predictions(X, W1, b1, W2, b2):\n    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n    predictions = get_predictions(A2)\n    return predictions\n#De test_prediction functie kijkt of het daadwerkelijke plaatje overeenkomt met het antwoord van het algoritme\n#De current image \n#De prediction variabele gebruikt de make_predictions variabele welke als uitkomst de plek (index) van het antwoord geeft\n#Heeft het algoritme bijvoorbeeld een plaatje met een drie als input, dan is de output het getal dat op de derde plek (van links naar rechts) het dichts bij 1 staat\n#Vervolgens print het algoritme zijn voorspelling en print hij de label, ook wordt het bijbehorende plaatje getoond\ndef test_prediction(index, W1, b1, W2, b2):\n    current_image = X_train[:, index, None]\n    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2)\n    label = Y_train[index]\n    print(\"Prediction: \", prediction)\n    print(\"Label: \", label)\n    \n    current_image = current_image.reshape((28, 28)) * 255\n    plt.gray()\n    plt.imshow(current_image, interpolation='nearest')\n    plt.show()","cff3ea99":"test_prediction(0, W1, b1, W2, b2)\ntest_prediction(1, W1, b1, W2, b2)\ntest_prediction(2, W1, b1, W2, b2)\ntest_prediction(3, W1, b1, W2, b2)","beba5dc6":"dev_predictions = make_predictions(X_dev, W1, b1, W2, b2)\nget_accuracy(dev_predictions, Y_dev)","dd8a5f04":"Dit neurale netwerk kan handgeschreven cijfers van 0 tot 9 classificeren. Deze cijfers zijn afkomstig van de MNIST database. In dit notebook wordt uiteengezet hoe de code werkt en wordt de werking van het netwerk getoond. De code is grotendeels niet van ons, maar is afkomstig van deze tutorial https:\/\/www.youtube.com\/watch?v=w8yWXqWQYmU&t=428s. Shout out naar Samson dus.  \n\n\n\n**Paar belangrijke notaties:\n-De Z variabele staat voor de weighted sum-matrix, (Z want dat lijkt op dat sommatie teken)\n-De W variabele staat voor de weight-matrix\n-De b variabele staat voor bias-matrix\n-De A variabele staat voor uitkomst van de activeringsfunctie als functie van Z**\n","4a230777":"Still 84% accuracy, so our model generalized from the training data pretty well.","ec9261aa":"Our NN will have a simple two-layer architecture. Input layer $a^{[0]}$ will have 784 units corresponding to the 784 pixels in each 28x28 input image. A hidden layer $a^{[1]}$ will have 10 units with ReLU activation, and finally our output layer $a^{[2]}$ will have 10 units corresponding to the ten digit classes with softmax activation.\n\n**Forward propagation**\n\n$$Z^{[1]} = W^{[1]} X + b^{[1]}$$\n$$A^{[1]} = g_{\\text{ReLU}}(Z^{[1]}))$$\n$$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$$\n$$A^{[2]} = g_{\\text{softmax}}(Z^{[2]})$$\n\n**Backward propagation**\n\n$$dZ^{[2]} = A^{[2]} - Y$$\n$$dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T}$$\n$$dB^{[2]} = \\frac{1}{m} \\Sigma {dZ^{[2]}}$$\n$$dZ^{[1]} = W^{[2]T} dZ^{[2]} .* g^{[1]\\prime} (z^{[1]})$$\n$$dW^{[1]} = \\frac{1}{m} dZ^{[1]} A^{[0]T}$$\n$$dB^{[1]} = \\frac{1}{m} \\Sigma {dZ^{[1]}}$$\n\n**Parameter updates**\n\n$$W^{[2]} := W^{[2]} - \\alpha dW^{[2]}$$\n$$b^{[2]} := b^{[2]} - \\alpha db^{[2]}$$\n$$W^{[1]} := W^{[1]} - \\alpha dW^{[1]}$$\n$$b^{[1]} := b^{[1]} - \\alpha db^{[1]}$$\n\n**Vars and shapes**\n\nForward prop\n\n- $A^{[0]} = X$: 784 x m\n- $Z^{[1]} \\sim A^{[1]}$: 10 x m\n- $W^{[1]}$: 10 x 784 (as $W^{[1]} A^{[0]} \\sim Z^{[1]}$)\n- $B^{[1]}$: 10 x 1\n- $Z^{[2]} \\sim A^{[2]}$: 10 x m\n- $W^{[1]}$: 10 x 10 (as $W^{[2]} A^{[1]} \\sim Z^{[2]}$)\n- $B^{[2]}$: 10 x 1\n\nBackprop\n\n- $dZ^{[2]}$: 10 x m ($~A^{[2]}$)\n- $dW^{[2]}$: 10 x 10\n- $dB^{[2]}$: 10 x 1\n- $dZ^{[1]}$: 10 x m ($~A^{[1]}$)\n- $dW^{[1]}$: 10 x 10\n- $dB^{[1]}$: 10 x 1","a508bd5c":"Finally, let's find the accuracy on the dev set:","503df4ee":"~86% accuracy op de training set.","e5935e26":"Let's look at a couple of examples:"}}