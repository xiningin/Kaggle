{"cell_type":{"73eb267d":"code","93eef91c":"code","83e65c0a":"code","3994db78":"code","73e01e84":"code","2650b646":"code","6a90330b":"code","d9edbc7f":"code","ecca987f":"code","fb67345c":"code","0e58ec26":"code","768afa09":"code","7a5d8e2d":"code","7862e89a":"markdown","80720860":"markdown","8b97e0d8":"markdown","082da4ce":"markdown","0bbf5365":"markdown","42e3fc47":"markdown","bbaf697b":"markdown","242135c1":"markdown","24877f88":"markdown","0692bc24":"markdown","ed72d0ef":"markdown","2624483c":"markdown","3e9f1cb1":"markdown","610d9e37":"markdown","3f1a11f0":"markdown"},"source":{"73eb267d":"import matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nsns.set_style(\"darkgrid\")\nsns.set_context(\"notebook\")\n\nclass Clustering_Visualization:\n    \n    def __init__(self, dataframe): \n        # dataframe = pandas dataframe de onde ser\u00e3o extra\u00eddos os dados\n        \n        self.data = dataframe\n        \n    def plot2D(self, xlabel, ylabel, clusters):\n        # xlabel = string, nome da coluna do dataframe que formar\u00e1 o eixo x\n        # ylabel = string, nome da coluna do dataframe que formar\u00e1 o eixo y\n        # clusters = array, quantidade de clusters em cada subgr\u00e1fico\n\n        n = len(clusters)\n        \n        plt.subplots(1, n, figsize=(n*5, 4))\n\n        for i, c in enumerate(clusters):\n            model = KMeans(n_clusters=c)\n            model.fit(self.data[[xlabel,ylabel]])\n            labels = model.labels_\n            centroids = model.cluster_centers_\n\n            plt.subplot(1, n, i+1)\n\n            sns.scatterplot(x=xlabel, y=ylabel, data=self.data, hue=labels, palette=sns.color_palette(\"tab10\", c), legend=False).set(title='K-Means %d clusters' %c)\n            sns.scatterplot(x=centroids[:,0], y=centroids[:,1], **{'marker':'o', 's':150, 'color':'white', 'edgecolor':'black'})\n            sns.scatterplot(x=centroids[:,0], y=centroids[:,1], **{'marker':'x', 's':40, 'color':'red', 'linewidth':2})\n\n        plt.show()\n        \n    def elbow(self, labels, max_clusters):\n        # label = array of string, nomes das colunas do dataframe a considerar\n        # max_clusters = positive int, n\u00famero m\u00e1ximo de clusters a iterar\n        \n        inertia = list()\n        \n        for c in range(1, max_clusters+1):\n            model = KMeans(n_clusters=c)\n            model.fit(self.data[labels])\n            inertia.append(model.inertia_)\n        \n        sns.lineplot(x=range(1, max_clusters+1), y=inertia, **{'marker':'o'}).set(xticks=range(1, max_clusters+1), \n                                                                                  xlabel='N\u00famero de clusters k', \n                                                                                  ylabel='Soma total das dist\u00e2ncias',\n                                                                                  title='M\u00e9todo do Cotovelo')\n        plt.show()","93eef91c":"iris = sns.load_dataset(\"iris\")\niris = iris.drop(columns='species')\niris.head()","83e65c0a":"iris.describe()","3994db78":"iris.iloc[:,:] = MinMaxScaler().fit_transform(iris)\niris.head()","73e01e84":"sns.pairplot(iris).set(xlim=(-.1,1.1), ylim=(-.1,1.1))\nplt.show()","2650b646":"iris.corr()","6a90330b":"clusterview = Clustering_Visualization(iris)","d9edbc7f":"clusterview.plot2D('sepal_width', 'sepal_length', [2,3,4])","ecca987f":"clusterview.elbow(['sepal_width', 'sepal_length'], 10)","fb67345c":"clusterview.plot2D('petal_width', 'petal_length', [2,3,4])","0e58ec26":"clusterview.elbow(['petal_width', 'petal_length'], 10)","768afa09":"clusterview.elbow(iris.columns, 10)","7a5d8e2d":"model = KMeans(n_clusters=3)\nmodel.fit(iris)\n\niris['pred_target'] = model.labels_\nsns.pairplot(iris, hue='pred_target', palette=sns.color_palette(\"tab10\", 3)).set(xlim=(-.1,1.1), ylim=(-.1,1.1))\nplt.show()","7862e89a":"Mesmo com a visualiza\u00e7\u00e3o dos centr\u00f3ides e das inst\u00e2ncias englobadas por eles, ainda \u00e9 dif\u00edcil escolher o melhor valor para o par\u00e2metro k do K-means. Para isso podemos usar outras abordagens, como por exemplo o m\u00e9todo do cotovelo (Elbow Method). Este m\u00e9todo consiste em, para cada k, calcular a dist\u00e2ncia de todas as inst\u00e2ncias ao seu centr\u00f3ide mais pr\u00f3ximo e ent\u00e3o som\u00e1-las. O n\u00famero de clusters k \u00f3timo ser\u00e1 igual ao ponto no eixo x em que a curva da soma das dist\u00e2ncias passa a ter decaimento aproximadamente linear.","80720860":"#### Elbow Method","8b97e0d8":"* Como escolher as features para a execu\u00e7\u00e3o do k-means? \n* Utilizar todas as features do dataset \u00e9 uma boa abordagem? \n* E se as features n\u00e3o forem vari\u00e1veis cont\u00ednuas?","082da4ce":"Como o KMeans \u00e9 uma m\u00e1quina de aprendizado que se baseia no calculo de dist\u00e2ncias, os dados devem ser normalizados entre 0 e 1 para que seja dada igual import\u00e2ncia a cada uma das features.","0bbf5365":"## K-means com o par (sepal_width,sepal_length)","42e3fc47":"Utilizando todas as 4 features do dataset, verifica-se que o n\u00famero de clusters \u00f3timo \u00e9 igual a 3. Por fim, vamos visualizar os grupos formados pelo algoritmo k-means.","bbaf697b":"Por se tratar do estudo de uma m\u00e1quina de aprendizado n\u00e3o-supervionado, a coluna com os r\u00f3tulos de classe ser\u00e1 removida.","242135c1":"## K-means com todo o dataset","24877f88":"*  Quando h\u00e1 apenas 2 centr\u00f3ides, j\u00e1 pode-se ver o reconhecimento dos dois agrupamentos discutidos anteriormente. Muito devido a grande dist\u00e2ncia entre suas fronteiras. \n* Vemos ao longo dos gr\u00e1ficos que as sucessivas adi\u00e7\u00f5es de centr\u00f3ides particionam cada vez mais o grupo onde h\u00e1 maior espalhamento.","0692bc24":"## Analisando as features par a par","ed72d0ef":"Analisando a curva gerada pelo m\u00e9todo do cotovelo para o par (sepal_width,sepal_length), vemos que \u00e9 dif\u00edcil dizer com exatid\u00e3o a partir de que ponto a curva passa a ser aproximadamente linear. O decaimento \u00e9 suave, mas o cotovelo parece acontecer entre k igual a 3 ou 4. Muito dessa suaviza\u00e7\u00e3o se deve ao grande espalhamento das inst\u00e2ncias, e da exist\u00eancia de fronteiras pouco definidas.","2624483c":"Atrav\u00e9s da visualiza\u00e7\u00e3o acima, \u00e9 poss\u00edvel observar que:\n* Todos os pares revelam dois agrupamentos disjuntos. Um grupo menor e unido, outro maior e disperso disperso. Dando ind\u00edcios da presen\u00e7a de no m\u00ednimo duas classes;\n* O par (sepal_width,sepal_length) \u00e9 o que apresenta maior grau de espalhamento, al\u00e9m de correla\u00e7\u00e3o linear pr\u00f3xima a zero;\n* O par (petal_width,petal_lenght) \u00e9 o que apresenta menor grau de espalhamento, e alt\u00edssima correla\u00e7\u00e3o linear positiva. ","3e9f1cb1":"O m\u00e9todo do cotovelo para o par (petal_width,petal_length) revela uma curva de decaimento brusco antes de k=3, e um decaimento linear ap\u00f3s este ponto. Neste caso \u00e9 f\u00e1cil identificar o n\u00famero de clusters \u00f3timo devido ao pouco espalhamento das inst\u00e2ncias, e tamb\u00e9m \u00e0 presen\u00e7a de grupos isolados.","610d9e37":"## K-means com o par (petal_width,petal_length)","3f1a11f0":"* Vemos que com 2 centr\u00f3ides n\u00e3o h\u00e1 uma separa\u00e7\u00e3o ideal dos dois grupos identificados anteriormente durante a an\u00e1lise par a par.\n* Usando 3 centr\u00f3ides, a separa\u00e7\u00e3o destes grupos enfim acontece. E as inst\u00e2ncias restantes s\u00e3o divididas em 2 outros agrupamentos.\n* E ent\u00e3o, com 4 centr\u00f3ides, v\u00ea-se a cria\u00e7\u00e3o de um grupo mais ao sul onde grande parte das inst\u00e2ncias est\u00e3o distantes do centr\u00f3ide. "}}