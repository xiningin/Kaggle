{"cell_type":{"cc203534":"code","a73531f6":"code","f0712fbf":"code","d4ed4713":"code","880f9935":"code","7940efe9":"code","74fca48c":"code","62047ed0":"code","87751ee3":"code","7bc6d560":"code","3fd7e5f1":"code","a303476b":"code","4d9ca331":"code","9141ee3c":"code","dbb8aed4":"code","97d0c00a":"code","7bce5942":"code","3601083a":"code","37bd8bd0":"code","f4601945":"code","c4d49437":"code","4a25a332":"code","1a49b7d4":"code","49a513f0":"code","07761b9e":"code","a7021bd9":"code","dee35d78":"code","4c255f66":"code","bc72156c":"code","095d29f0":"code","2f275731":"code","be9f84ca":"code","febed0ca":"code","5fdcb9c1":"code","b2f8ec0f":"code","9ddf9067":"code","548a9ec2":"code","6248ba50":"code","1f03bf71":"code","6d3bf090":"code","71f6e946":"code","3f1649ca":"code","3956eb9e":"code","7fa70a25":"code","ce270e07":"code","c02ef56c":"code","f9d99773":"code","d322fcf8":"code","446d226c":"code","d8bd9820":"code","c382c782":"code","c3ecddf7":"code","083ccb4a":"code","7af1ada8":"code","6b89b5f5":"code","15d3eb7e":"code","78b6cd61":"code","f4e0171d":"code","3218e815":"markdown","52af9ab2":"markdown","7f8a6dbc":"markdown","c94e4a8b":"markdown","76dfbef5":"markdown","300634cc":"markdown","381dfdfa":"markdown","55187970":"markdown","5721dbfe":"markdown","c0761d0f":"markdown","47ffce04":"markdown","0e19b57b":"markdown","50ccaf1a":"markdown","9bda62c9":"markdown","d1dd8b74":"markdown","30e4fc49":"markdown","a9fe468e":"markdown","fbbd8152":"markdown","78d6270d":"markdown","61a45331":"markdown","6e8891a0":"markdown","8ba136a8":"markdown","debfdc6c":"markdown","e248aacc":"markdown","555ded10":"markdown","3055ba52":"markdown","81afeb8d":"markdown","6be6b8b2":"markdown","53b6e016":"markdown","e4ae9164":"markdown","a1e2fe80":"markdown","dcb876e2":"markdown","243201dc":"markdown","b3ceb233":"markdown","10b7624c":"markdown","a2183544":"markdown","ef290b92":"markdown","56677f64":"markdown","382955a5":"markdown","02ee1f1b":"markdown","90174911":"markdown"},"source":{"cc203534":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a73531f6":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","f0712fbf":"data = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ntest = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')","d4ed4713":"data.head(5)","880f9935":"test.head(3)","7940efe9":"data.dtypes","74fca48c":"data.isnull().sum()","62047ed0":"data.corr()","87751ee3":"#Frequency of each category separated by label\nplt.figure(figsize=[15,18])\nfeafures = ['gender','relevent_experience','enrolled_university','education_level', 'major_discipline',\n       'experience','company_size','company_type','last_new_job']\nn=1\nfor f in feafures:\n    plt.subplot(5,2,n)\n    sns.countplot(x=f, hue='target', alpha=0.7, data=data)\n    plt.title(\"Countplot of {}  by target\".format(f))\n    n=n+1\nplt.tight_layout()\nplt.show()","7bc6d560":"#Churn vs. normal \ncounts = data.target.value_counts()\nnot_change = counts[0]\nchange = counts[1]\nperc_not_change = (not_change\/(not_change+change))*100\nperc_change = (change\/(not_change+change))*100\nprint('There were {} nonot_change ({:.3f}%) and {} change ({:.3f}%).'.format(not_change, perc_not_change, change, perc_change))","3fd7e5f1":"np.array(data.columns[data.dtypes != object])","a303476b":"import copy\ndf_train=copy.deepcopy(data)\ndf_test=copy.deepcopy(test)\n\ncols=np.array(data.columns[data.dtypes != object])\nfor i in df_train.columns:\n    if i not in cols:\n        df_train[i]=df_train[i].map(str)\n        df_test[i]=df_test[i].map(str)\ndf_train.drop(columns=cols,inplace=True)\ndf_test.drop(columns=np.delete(cols,len(cols)-1),inplace=True)","4d9ca331":"df_train.columns","9141ee3c":"from sklearn.preprocessing import LabelEncoder\nfrom collections import defaultdict\n\n# build dictionary function\ncols=np.array(data.columns[data.dtypes != object])\nd = defaultdict(LabelEncoder)\n\n# only for categorical columns apply dictionary by calling fit_transform \ndf_train = df_train.apply(lambda x: d[x.name].fit_transform(x))\ndf_test=df_test.apply(lambda x: d[x.name].transform(x))\ndf_train[cols]=data[cols]\ndf_test[np.delete(cols,len(cols)-1)]=test[np.delete(cols,len(cols)-1)]","dbb8aed4":"df_train.dtypes","97d0c00a":"df_test.columns","7bce5942":"import matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nstyle.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (12,7))\n## Plotting heatmap. # Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(df_train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(df_train.corr(), cmap=sns.diverging_palette(20, 220, n=200), annot=True, mask=mask, center = 0, );\nplt.title(\"Heatmap of all the Features of Train data set\", fontsize = 25);","3601083a":"import matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nstyle.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (12,7))\n## Plotting heatmap. # Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(df_train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\ncorr = df_train.corr()\nsns.heatmap(corr[(corr.abs() > 0.01)], cmap=sns.diverging_palette(20, 220, n=200), annot=True, mask=mask, center = 0, );\nplt.title(\"Heatmap of all the Features of Train data set\", fontsize = 25);","37bd8bd0":"#visualizing the features whigh positive and negative correlation\nf, axes = plt.subplots(nrows=3, ncols=3, figsize=(20,15))\n\nf.suptitle('Features With High Positive & Negative Correlation', size=35)\nsns.boxplot(x=\"target\", y=\"city\", data=df_train, ax=axes[0,0])\nsns.boxplot(x=\"target\", y=\"gender\", data=df_train, ax=axes[0,1])\nsns.boxplot(x=\"target\", y='relevent_experience', data=df_train, ax=axes[0,2])\nsns.boxplot(x=\"target\", y='enrolled_university', data=df_train, ax=axes[1,0])\nsns.boxplot(x=\"target\", y='education_level', data=df_train, ax=axes[1,1])\nsns.boxplot(x=\"target\", y='company_size', data=df_train, ax=axes[1,2])\nsns.boxplot(x=\"target\", y='company_type', data=df_train, ax=axes[2,0])\nsns.boxplot(x=\"target\", y='enrollee_id', data=df_train, ax=axes[2,1])\nsns.boxplot(x=\"target\", y='training_hours', data=df_train, ax=axes[2,2])","f4601945":"import matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nstyle.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (12,7))\n## Plotting heatmap. # Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(df_test.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\ncorr = df_test.corr()\nsns.heatmap(corr[(corr.abs() > 0.01)], cmap=sns.diverging_palette(20, 220, n=200), annot=True, mask=mask, center = 0, );\nplt.title(\"Heatmap of all the Features of Test data set\", fontsize = 25);","c4d49437":"df_train['target'].value_counts()","4a25a332":"df_train.columns","1a49b7d4":"ftrain = ['city','gender', 'relevent_experience','enrolled_university','education_level','company_size','company_type','city_development_index', 'training_hours', 'target']\nftest = ['city','gender', 'relevent_experience','enrolled_university','education_level','company_size','company_type','city_development_index', 'training_hours']\n\ndef Definedata():\n    # define dataset\n    data2=df_train[ftrain]\n    X=data2.drop(columns=['target']).values\n    y=data2['target'].values\n    return X, y","49a513f0":"def SMOTE():\n    # borderline-SMOTE for imbalanced dataset\n    from collections import Counter\n    from sklearn.model_selection import train_test_split\n    from sklearn.datasets import make_classification\n    from imblearn.over_sampling import SMOTE\n    from matplotlib import pyplot\n    from numpy import where\n    \n    X, y = Definedata()\n\n# summarize class distribution\n    counter = Counter(y)\n    print(counter)\n# transform the dataset\n    smt = SMOTE(random_state=0)\n    X, y = smt.fit_sample(X, y) \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=2)\n# summarize the new class distribution\n    counter = Counter(y)\n    print(counter)\n# scatter plot of examples by class label\n    for label, _ in counter.items():\n        row_ix = where(y == label)[0]\n        pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n    pyplot.legend()\n    pyplot.show()\n    return X_train, X_test, y_train, y_test","07761b9e":"def BSMOTE():\n    from collections import Counter\n    from sklearn.model_selection import train_test_split\n    from imblearn.over_sampling import BorderlineSMOTE\n    from matplotlib import pyplot\n    from numpy import where\n    \n    X, y = Definedata()\n    \n# summarize class distribution\n    counter = Counter(y)\n    print(counter)\n# transform the dataset\n    X, y = BorderlineSMOTE().fit_resample(X, y)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=2)\n# summarize the new class distribution\n    counter = Counter(y)\n    print(counter)\n# scatter plot of examples by class label\n    for label, _ in counter.items():\n        row_ix = where(y == label)[0]\n        pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n    pyplot.legend()\n    pyplot.show()\n    return X_train, X_test, y_train, y_test","a7021bd9":"def SMOTESVM():\n    from collections import Counter\n    from sklearn.model_selection import train_test_split\n    from imblearn.over_sampling import SVMSMOTE\n    from matplotlib import pyplot\n    from numpy import where\n\n    X, y = Definedata()\n\n# summarize class distribution\n    counter = Counter(y)\n    print(counter)\n# transform the dataset\n    X, y = SVMSMOTE().fit_resample(X, y)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=2)\n# summarize the new class distribution\n    counter = Counter(y)\n    print(counter)\n# scatter plot of examples by class label\n    for label, _ in counter.items():\n        row_ix = where(y == label)[0]\n        pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n    pyplot.legend()\n    pyplot.show()\n    return X_train, X_test, y_train, y_test","dee35d78":"def ADASYN():\n    from collections import Counter\n    from sklearn.model_selection import train_test_split\n    from imblearn.over_sampling import ADASYN\n    from matplotlib import pyplot\n    from numpy import where\n\n    X, y = Definedata()\n\n# summarize class distribution\n    counter = Counter(y)\n    print(counter)\n# transform the dataset\n    X, y = ADASYN().fit_resample(X, y)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=2)\n# summarize the new class distribution\n    counter = Counter(y)\n    print(counter)\n# scatter plot of examples by class label\n    for label, _ in counter.items():\n        row_ix = where(y == label)[0]\n        pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n    pyplot.legend()\n    pyplot.show()\n    return X_train, X_test, y_train, y_test","4c255f66":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import plot_confusion_matrix\n\nimport seaborn \nimport matplotlib.pyplot as plt\n\ndef Models(models, X_train, X_test, y_train, y_test, title):\n    model = models\n    model.fit(X_train,y_train)\n    \n    X, y = Definedata()\n    train_matrix = pd.crosstab(y_train, model.predict(X_train), rownames=['Actual'], colnames=['Predicted'])    \n    test_matrix = pd.crosstab(y_test, model.predict(X_test), rownames=['Actual'], colnames=['Predicted'])\n    matrix = pd.crosstab(y, model.predict(X), rownames=['Actual'], colnames=['Predicted'])\n    \n    f,(ax1,ax2,ax3) = plt.subplots(1,3,sharey=True, figsize=(20, 3))\n    g1 = sns.heatmap(train_matrix, annot=True, fmt=\".1f\", cbar=False,annot_kws={\"size\": 18},ax=ax1)\n    g1.set_title(title)\n    g1.set_ylabel('Total career swith = {}'.format(y_train.sum()), fontsize=14, rotation=90)\n    g1.set_xlabel('Accuracy score Trainset: {}'.format(accuracy_score(model.predict(X_train), y_train)))\n    g2 = sns.heatmap(test_matrix, annot=True, fmt=\".1f\",cbar=False,annot_kws={\"size\": 18},ax=ax2)\n    g2.set_ylabel('Total career swith = {}'.format(y_test.sum()), fontsize=14, rotation=90)\n    g2.set_xlabel('Accuracy score Testingset: {}'.format(accuracy_score(model.predict(X_test), y_test)))\n    g3 = sns.heatmap(matrix, annot=True, fmt=\".1f\",cbar=False,annot_kws={\"size\": 18},ax=ax3)\n    g3.set_ylabel('Total career swith = {}'.format(y.sum()), fontsize=14, rotation=90)\n    g3.set_xlabel('Accuracy score Totalset: {}'.format(accuracy_score(model.predict(X), y)))\n    \n    plt.show()\n    return y, model.predict(X)\n\ndef Featureimportances(models):\n    model = models\n    model.fit(X_train1,y_train1)\n    importances = model.feature_importances_\n    features = df_test.columns[:9]\n    imp = pd.DataFrame({'Features': ftest, 'Importance': importances})\n    imp['Sum Importance'] = imp['Importance'].cumsum()\n    imp = imp.sort_values(by = 'Importance')\n    return imp","bc72156c":"X_train1, X_test1, y_train1, y_test1 = SMOTE()\nX_train2, X_test2, y_train2, y_test2 = BSMOTE()\nX_train3, X_test3, y_train3, y_test3 = SMOTESVM()\nX_train4, X_test4, y_train4, y_test4 = ADASYN()","095d29f0":"title = \"LogisticRegression\/ SMOTE\" \nModels(LogisticRegression(),X_train1, X_test1, y_train1, y_test1, title)","2f275731":"title = \"LogisticRegression\/BSMOTE\"\nModels(LogisticRegression(),X_train2, X_test2, y_train2, y_test2, title)","be9f84ca":"title = \"LogisticRegression\/SMOTESVM\"\nModels(LogisticRegression(),X_train3, X_test3, y_train3, y_test3, title)","febed0ca":"title = \"LogisticRegression\/ADASYN\"\nModels(LogisticRegression(),X_train4, X_test4, y_train4, y_test4, title)","5fdcb9c1":"title = \"GaussianNB\/SMOTESVM\"\nModels(GaussianNB(),X_train3, X_test3, y_train3, y_test3, title)","b2f8ec0f":"title = \"KNeighborsClassifier\/BSMOTE\"\nModels(KNeighborsClassifier(n_neighbors=1),X_train2, X_test2, y_train2, y_test2, title)","9ddf9067":"title = \"DecisionTreeClassifier\/ADASYN\"\nModels(DecisionTreeClassifier(max_depth=14),X_train4, X_test4, y_train4, y_test4, title)","548a9ec2":"Featureimportances(DecisionTreeClassifier(max_depth=14))","6248ba50":"title = \"RandomForestClassifier\/SMOTE\"\nModels(RandomForestClassifier(n_estimators=2000,max_depth=200,max_features=7),X_train1, X_test1, y_train1, y_test1, title)","1f03bf71":"title = \"RandomForestClassifier\/SMOTESVM\"\nModels(RandomForestClassifier(),X_train3, X_test3, y_train3, y_test3, title)","6d3bf090":"title = \"RandomForestClassifier\/ADASYN\"\nModels(RandomForestClassifier(),X_train4, X_test4, y_train4, y_test4, title)","71f6e946":"Featureimportances(RandomForestClassifier())","3f1649ca":"title = \"Extratreesclassifier\/ADASYN\"\nModels(ExtraTreesClassifier(),X_train4, X_test4, y_train4, y_test4, title)","3956eb9e":"from sklearn.ensemble import GradientBoostingClassifier\n# Now we can try setting different learning rates, so that we can compare the performance of the classifier's \n#performance at different learning rates. Let's see what the performance was for different learning rates:\n\nlr_list = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]\n\nfor learning_rate in lr_list:\n    gb_clf = GradientBoostingClassifier(n_estimators=50, learning_rate=learning_rate, max_features=2, max_depth=2, random_state=0)\n    gb_clf.fit(X_train1, y_train1)\n\n    print(\"Learning rate: \", learning_rate)\n    print(\"Accuracy score (training): {0:.3f}\".format(gb_clf.score(X_train1, y_train1)))\n    print(\"Accuracy score (validation): {0:.3f}\".format(gb_clf.score(X_test1, y_test1)))","7fa70a25":"title = \"GradientBoostingClassifier\/SMOTESVM\"\nModels(GradientBoostingClassifier(n_estimators=500, learning_rate=1, max_features=2, max_depth=2, random_state=0),X_train3, X_test3, y_train3, y_test3, title)","ce270e07":"Featureimportances(GradientBoostingClassifier(n_estimators=500, learning_rate=1, max_features=2, max_depth=2, random_state=0))","c02ef56c":"title = \"XGBClassifier\/BSMOTE\"\nModels(XGBClassifier(),X_train2, X_test2, y_train2, y_test2, title)","f9d99773":"Featureimportances(XGBClassifier())","d322fcf8":"title = \"LGBMClassifier\/SMOTE\"\nModels(LGBMClassifier(),X_train1, X_test1, y_train1, y_test1, title)","446d226c":"title = \"LGBMClassifier\/BSMOTE\"\nModels(LGBMClassifier(),X_train2, X_test2, y_train2, y_test2, title)","d8bd9820":"title = \"LGBMClassifier\/SMOTESVM\"\nModels(LGBMClassifier(),X_train3, X_test3, y_train3, y_test3, title)","c382c782":"title = \"LGBMClassifier\/ADASYN\"\nModels(LGBMClassifier(),X_train4, X_test4, y_train4, y_test4, title)","c3ecddf7":"Featureimportances(LGBMClassifier())","083ccb4a":"from sklearn.metrics import confusion_matrix,auc,roc_curve\n\ntitle = 'RandomForestClassifier using the SMOTE'\ny, ypred =  Models(RandomForestClassifier(),X_train1, X_test1, y_train1, y_test1, title)\n\nfpr, tpr, thresholds = roc_curve(y, ypred)\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b',label='AUC = %0.3f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.0])\nplt.ylim([-0.1,1.01])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","7af1ada8":"from sklearn.inspection import permutation_importance\nfrom sklearn.metrics import confusion_matrix,auc,roc_curve\nfrom sklearn.model_selection import train_test_split\n\nX=df_train.drop(columns=['target'])\ny=df_train['target'].values\nmodel = RandomForestClassifier()\n    \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 25)\nmodel.fit(X_train,y_train)    \nresultmymodel = permutation_importance(model, X_test, y_test, n_repeats=10,random_state=42, n_jobs=2)\nsorted_idx = resultmymodel.importances_mean.argsort()\n\nfig, ax = plt.subplots(figsize=(10,7))\nax.boxplot(resultmymodel.importances[sorted_idx].T,vert=False, labels=X_test.columns[sorted_idx])\nax.set_title(\"Permutation Importances (test set)\")\nfig.tight_layout()\nplt.show()","6b89b5f5":"resultmymodel = permutation_importance(model, X_train, y_train, n_repeats=10,random_state=42, n_jobs=2)\nsorted_idx = resultmymodel.importances_mean.argsort()\n\nfig, ax = plt.subplots(figsize=(10,7))\nax.boxplot(resultmymodel.importances[sorted_idx].T,vert=False, labels=X_train.columns[sorted_idx])\nax.set_title(\"Permutation Importances (train set)\")\nfig.tight_layout()\nplt.show()","15d3eb7e":"ftest0 = ['enrollee_id','city','gender', 'relevent_experience','enrolled_university','education_level','company_size','company_type','city_development_index', 'training_hours']\ntest = df_test[ftest0].copy()\ndf_test = df_test[ftest].copy()\n\nX_train, X_test, y_train, y_test = SMOTESVM()\n\nmodel = RandomForestClassifier()\nmodel.fit(X_train,y_train)\npredictions=model.predict(df_test[ftest].values)\nsubmission = pd.DataFrame({'enrollee_id':test['enrollee_id'],'target':predictions})\nsubmission['enrollee_id']=test['enrollee_id']\n\nsubmission['target']=model.predict(df_test.values)\n\nprint(submission.target.value_counts())","78b6cd61":"submission.head(20)","f4e0171d":"#This is saved in the same directory as your notebook\nfilename = 'submission.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","3218e815":"# II. Model fitting, prediction, recommendation and submission.\n\nGood to know is that correlation can\u2019t be calculated for categorical variables so dealing with them before was important. \n\nWe will start processing the categorical data by removing numerical columns and using map (str).","52af9ab2":"in fact, with a result of 81.6% this is the level of accuracy expected for a simple, fast method.","7f8a6dbc":"# 1. Data treatment","c94e4a8b":"Adaptive Synthetic Sampling (ADASYN)\nAnother approach involves generating synthetic samples inversely proportional to the density of the examples in the minority class.\n\nThat is, generate more synthetic examples in regions of the feature space where the density of minority examples is low, and fewer or none where the density is high.\n\nThis modification to SMOTE is referred to as the Adaptive Synthetic Sampling Method, or ADASYN, and was proposed to Haibo He, et al. in their 2008 paper named for the method titled \u201cADASYN: Adaptive Synthetic Sampling Approach For Imbalanced Learning.\u201d","76dfbef5":"<h1 align=\"center\"> HR Analytics: Job change prediction <\/h1>\n<img align = \"center\" src=\"https:\/\/i.ibb.co\/wynpxvD\/career-change.jpg\" alt=\"career-change\" border=\"0\">","300634cc":"As with any algorithm, there are advantages and disadvantages to using it. \n\nThe random forest algorithm is not biased, since, there are multiple trees and each tree is trained on a subset of data. Basically, the random forest algorithm relies on the power of \"the crowd\"; therefore the overall biasedness of the algorithm is reduced.\n\nThis algorithm is very stable. Even if a new data point is introduced in the dataset the overall algorithm is not affected much since new data may impact one tree, but it is very hard for it to impact all the trees. The random forest algorithm works well when you have both categorical and numerical features. The random forest algorithm also works well when data has missing values or it has not been scaled well.\n\nHowever, a major disadvantage of random forests lies in their complexity. They required much more computational resources, owing to the large number of decision trees joined together. Due to their complexity, they require much more time to train than other comparable algorithms.","381dfdfa":"# E. Random Forest\n\nRandom Forest is a trademark term for an ensemble of decision trees. In Random Forest, we\u2019ve collection of decision trees (so known as \u201cForest\u201d). To classify a new object based on attributes, each tree gives a classification and we say the tree \u201cvotes\u201d for that class. The forest chooses the classification having the most votes (over all the trees in the forest).\n\nRandom forest is a type of supervised machine learning algorithm based on ensemble learning. Ensemble learning is a type of learning where you join different types of algorithms or same algorithm multiple times to form a more powerful prediction model. The random forest algorithm combines multiple algorithm of the same type i.e. multiple decision trees, resulting in a forest of trees, hence the name \"Random Forest\". The random forest algorithm can be used for both regression and classification tasks.","55187970":"... and try to check the type of the database.","5721dbfe":"We will assign each categorical variable value a number, so let\u2019s say [A, B, A, F] named values will map to [1, 2, 1, 3]. To do that we will use LabelEncoder from sklearn.preprocessing package, as following.","c0761d0f":"... and a significant amount of NaN data ...","47ffce04":"# Notebook outline:\n\n1. Data cleaning\n2. Models fitting, prediction, recommendation and submission.\n3. Conclusion","0e19b57b":"Thus, the correlation between the variables in the Dataset and Testdata is nearly the same, good predictive results could be expected if a good train result is obtained.","50ccaf1a":"# I. Data cleaning\n\nI will load the dataset by using pandas the standard python approach for dealing with data.","9bda62c9":"Now examine the results, considering the correlation between \"pseudo categorical variables\" and the \"target\" objective function.","d1dd8b74":"The results are not bad, but try to limit some of variables with low correlation results by setting limits, for example, we choose the correlation only if it\u2019s above 0.01 or below -0.01","30e4fc49":"Thank you for reading here, look forward to receiving comments on this area so that I can learn and improve my skills.","a9fe468e":"Borderline-SMOTE: A New Over-Sampling Method in Imbalanced Data Sets Learning, 2005.\n\nWe can implement the BorderlineSMOTE class from imbalanced-learn as following.","fbbd8152":"# I. LightGBM\n\nLightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n\nFaster training speed and higher efficiency\nLower memory usage\nBetter accuracy\nParallel and GPU learning supported\nCapable of handling large-scale data\nThe framework is a fast and high-performance gradient boosting one based on decision tree algorithms, used for ranking, classification and many other machine learning tasks. It was developed under the Distributed Machine Learning Toolkit Project of Microsoft.\n\nSince the LightGBM is based on decision tree algorithms, it splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms.\n\nAlso, it is surprisingly very fast, hence the word \u2018Light\u2019.","78d6270d":"From these histogram charts, it can be seen, there is no special correlation between the variables with the target function to distinguish the value of the target. Furthermore, categorical variables cannot determine the correlation factor between these variables and the target function.","61a45331":"From this we can clearly see that the target 0 is in majority which will effect our model so we will use SMOTE (Synthetic Minority Over-sampling Technique) which will help us to create more synthetic data for the minority class 1 :)\n    ","6e8891a0":"# G. Gradient Boosting Algorithms\n\nGBM is a boosting algorithm used when we deal with plenty of data to make a prediction with high prediction power. Boosting is actually an ensemble of learning algorithms which combines the prediction of several base estimators in order to improve robustness over a single estimator. It combines multiple weak or average predictors to a build strong predictor. These boosting algorithms always work well in data science competitions like Kaggle, AV Hackathon, CrowdAnalytix.\n\nGradient boosting models are powerful algorithms which can be used for both classification and regression tasks. Gradient boosting models can perform incredibly well on very complex datasets, but they are also prone to overfitting, which can be combated with several of the methods described above. Gradient boosting classifiers are also easy to implement in Scikit-Learn.\n\nGradient boosting models are becoming popular because of their effectiveness at classifying complex datasets, and have recently been used to win many Kaggle data science competitions.","8ba136a8":"# Data information:\n\n+ enrollee_id : Unique ID for enrollee\n+ city: City code\n+ citydevelopmentindex: Developement index of the city (scaled)\n+ gender: Gender of enrolee\n+ relevent_experience: Relevent experience of enrolee\n+ enrolled_university: Type of University course enrolled if any\n+ education_level: Education level of enrolee\n+ major_discipline :Education major discipline of enrolee\n+ experience: Enrolee total experience in years\n+ company_size: No of employees in current employer's company\n+ company_type : Type of current employer\n+ lastnewjob: Difference in years between previous job and current job\n+ training_hours: training hours completed\n+ target: 0 \u2013 Not looking for job change, 1 \u2013 Looking for a job change","debfdc6c":"# B. Naive Bayes - Gaussian Naive Bayes\n\nIt is a classification technique based on Bayes\u2019 theorem with an assumption of independence between predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, a naive Bayes classifier would consider all of these properties to independently contribute to the probability that this fruit is an apple.\n\nNaive Bayesian model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.\n\nNaive Bayes models are a group of extremely fast and simple classification algorithms that are often suitable for very high-dimensional datasets. Because they are so fast and have so few tunable parameters, they end up being very useful as a quick-and-dirty baseline for a classification problem. That said, they have several advantages.","e248aacc":"From the above heatmap we can clearly observe that the target has a high dependance on the city_development_index which means candidates from city with higher amount of development index tends to move towards the field of data science.","555ded10":"# F. Extra Trees Algorithm\n\nExtremely Randomized Trees, or Extra Trees for short, is an ensemble machine learning algorithm.\n\nSpecifically, it is an ensemble of decision trees and is related to other ensembles of decision trees algorithms such as bootstrap aggregation (bagging) and random forest.\n\nThe Extra Trees algorithm works by creating a large number of unpruned decision trees from the training dataset. Predictions are made by averaging the prediction of the decision trees in the case of regression or using majority voting in the case of classification.","3055ba52":"# 2. Deal with Imbalanced Data using SMOTE","81afeb8d":"# D. Decision Tree\n\nThis is a type of supervised learning algorithm that is mostly used for classification problems. Surprisingly, it works for both categorical and continuous dependent variables. In this algorithm, we split the population into two or more homogeneous sets. This is done based on most significant attributes\/ independent variables to make as distinct groups as possible. \nA decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in recursively manner call recursive partitioning. This flowchart-like structure helps you in decision making. It's visualization like a flowchart diagram which easily mimics the human level thinking. That is why decision trees are easy to understand and interpret.\n\nDecision Tree is a white box type of ML algorithm. It shares internal decision-making logic, which is not available in the black box type of algorithms such as Neural Network. Its training time is faster compared to the neural network algorithm. The time complexity of decision trees is a function of the number of records and number of attributes in the given data. The decision tree is a distribution-free or non-parametric method, which does not depend upon probability distribution assumptions. Decision trees can handle high dimensional data with good accuracy.","6be6b8b2":"Check the dataset balance","53b6e016":"So, sampling approaches also influence on the accuracy of the model.","e4ae9164":"We're mainly interested in the classifier's accuracy on the validation set, but it looks like a learning rate of 0.5 gives us the best performance on the validation set and good performance on the training set.\n\nNow we can evaluate the classifier by checking its accuracy and creating a confusion matrix. Let's create a new classifier and specify the best learning rate we discovered.","a1e2fe80":"Next, let's look at the frequency of each category separated the histogram charts to check if there is any special information to distinguish whether the result of the \"target\" is 0 - Not looking for job change, OR, 1 - Looking for a job change.","dcb876e2":"# But, \"Categorical variables\"\n\nYes, first I need to deal with categorical variables so columns which have values different than numbers. \n\nA simple way of selecting all categorical columns is by checking their type.\n\nThus, In the database, only 4 columns are of numerical-data, and up to 10 columns are Categorical variables type.","243201dc":"# H. XGBoost\n\nA classic gradient boosting algorithm that\u2019s known to be the decisive choice between winning and losing in some Kaggle competitions.\n\nXGBoost is one of the most popular machine learning algorithm these days. Regardless of the type of prediction task at hand; regression or classification. The XGBoost has an immensely high predictive power which makes it the best choice for accuracy in events as it possesses both linear model and the tree learning algorithm, making the algorithm almost 10x faster than existing gradient booster techniques. The support includes various objective functions, including regression, classification and ranking.\n\nOne of the most interesting things about the XGBoost is that it is also called a regularized boosting technique. This helps to reduce overfit modelling and has a massive support for a range of languages such as Scala, Java, R, Python, Julia and C++.\n\nXGBoost is well known to provide better solutions than other machine learning algorithms. In fact, since its inception, it has become the \"state-of-the-art\u201d machine learning algorithm to deal with structured data.","b3ceb233":"Broadly, there are 3 types of Machine Learning Algorithms\n\n(A) Supervised Learning\n\nThis algorithm consist of a target \/ outcome variable (or dependent variable) which is to be predicted from a given set of predictors (independent variables). Using these set of variables, we generate a function that map inputs to desired outputs. The training process continues until the model achieves a desired level of accuracy on the training data. Examples of Supervised Learning: Regression, Decision Tree, Random Forest, KNN, Logistic Regression etc.\n\n\n(B) Unsupervised Learning\n\nIn this algorithm, we do not have any target or outcome variable to predict \/ estimate. It is used for clustering population in different groups, which is widely used for segmenting customers in different groups for specific intervention. Examples of Unsupervised Learning are Apriori algorithm, K-means, ...\n\n(C) Reinforcement Learning:\n\nUsing this algorithm, the machine is trained to make specific decisions. It works this way: the machine is exposed to an environment where it trains itself continually using trial and error. This machine learns from past experience and tries to capture the best possible knowledge to make accurate business decisions. Example of Reinforcement Learning such as Markov Decision Process.\n\nIn this Notebook, we will apply some commonly used machine learning algorithms, these algorithms can be applied to almost any data problem.","10b7624c":"# A. Logistic Regression\n\nIt is a classification not a regression algorithm. It is used to estimate discrete values (Binary values like 0\/1, yes\/no, true\/false) based on given set of independent variable(s). In simple words, it predicts the probability of occurrence of an event by fitting data to a logit function. Hence, it is also known as logit regression. Since, it predicts the probability, its output values lies between 0 and 1 (as expected).\n\nLogistic regression is a fundamental classification technique. It belongs to the group of linear classifiers and is somewhat similar to polynomial and linear regression. Logistic regression is fast and relatively uncomplicated, and it\u2019s convenient for you to interpret the results. Although it\u2019s essentially a method for binary classification, it can also be applied to multiclass problems.","a2183544":"# C. kNN (k- Nearest Neighbors)\n\nIt can be used for both classification and regression problems. However, it is more widely used in classification problems in the industry. K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases by a majority vote of its k neighbors. The case being assigned to the class is most common amongst its K nearest neighbors measured by a distance function.\n\nK Nearest Neighbor(KNN) is a very simple, easy to understand, versatile and one of the topmost machine learning algorithms. KNN used in the variety of applications such as finance, healthcare, political science, handwriting detection, image recognition and video recognition. In Credit ratings, financial institutes will predict the credit rating of customers. In loan disbursement, banking institutes will predict whether the loan is safe or risky. In political science, classifying potential voters in two classes will vote or won\u2019t vote. KNN algorithm used for both classification and regression problems. KNN algorithm based on feature similarity approach.\n\nHowever, KNN is computationally expensive. Variables should be normalized else higher range variables can bias it. For better results, normalizing data on the same scale is highly recommended. Generally, the normalization range considered between 0 and 1. KNN is not suitable for the large dimensional data. In such cases, dimension needs to reduce to improve the performance. Also, handling missing values will help us in improving results.","ef290b92":"# Abstract\n\nThe title \"data scientist\" has only been around for a few years, but thousands of professionals have worked as data scientists at companies of all sizes. Dubbed \"the sexiest job of the 21st century\" by Harvard Business Review and \"the best job in the US\" by Glassdoor, the data scientist's role is in high demand as organizations are in great demand. need experts who can organize incredible amounts of data.\n\nData science is attracting a growing number of professionals in a managerial or analytical role, especially those interested in directing their careers to the new digital age.\n\nA company in Big Data and Data Science wants to hire data scientists among those who have successfully passed some of the courses conducted by the company or have had certain experience or some special background. So the question is, is it possible to predict whether an employee can change his\/ her careers path? Especially in difficult times, economic crisis, job crisis or pandemic crisis like Covid-19 nowaday. What parameters that affect career switch decision?\n\nFrom a human resource management perspective, the company wants to know which of these candidates really want to work for the company or are looking for new jobs as it helps to reduce costs and time as well as the quality of training or planning for course and candidate classification. Important information may pertain to demographics, education, and experience on hand from registered and enrolled candidates.\n\nTherefore, this Notebook attempted to understand what factors that lead a person to work for the company or switch his\/ her career path to Data Scientist. In the next step, different models using existing credentials \/ demographics \/ experience data to predict the probability of candidates looking for a new job or will work for the Company. This Notebook also synthesizes models that can be used to predict an employee's possibility to switch their careers to a \"data scientist\"?","56677f64":"# 3. Machine Learning Algorithms","382955a5":"Borderline-SMOTE SVM\n\nHien Nguyen, et al. suggest using an alternative of Borderline-SMOTE where an SVM algorithm is used instead of a KNN to identify misclassified examples on the decision boundary.\n\nTheir approach is summarized in the 2009 paper titled \u201cBorderline Over-sampling For Imbalanced Data Classification.\u201d An SVM is used to locate the decision boundary defined by the support vectors and examples in the minority class that close to the support vectors become the focus for generating synthetic examples.","02ee1f1b":"# Conclusion: \n\nExtraTreesClassifier is the recommended model for this topic.\n\nModel | Training set | Testing set | Overal |\n\nLogisticRegression: | 70.5% | 70.7% | 68.9% |\n\nGaussianNB: | 72.1% | 70.7% | 70.9% |\n\nKNeighborsClassifier: | 99.1% | 80.2% | 90.5% |  ===> THIS IS THE SECOND ONE\n\nDecisionTreeClassifier: | 89.5% | 81.6% | 83.0% |\n\nRandomForestClassifier: | 99.1% | 84.5% | 91.9% |   ===> THIS IS THE BEST ONE\n\nExtraTreesClassifier: | 99.1% | 82.4% | 91.1% |\n\nGradientBoostingClassifier: | 87.5% | 85.5% | 80.8% |\n\nXGBClassifier: | 89.9% | 85.5% | 83.1% |\n\nLGBMClassifier: | 87.7% | 86.1% | 81.4% |","90174911":"Some basic libraries ..."}}