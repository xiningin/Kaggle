{"cell_type":{"4609da9d":"code","8aa80681":"code","7ea5808c":"code","dcbb4934":"code","ab5a70c0":"code","66ddb145":"code","eaa99ae5":"code","9c624213":"code","98cf9edc":"code","60fbeb81":"code","c7de4f6c":"code","94cabbcc":"code","76a2b2ab":"code","a324da82":"code","34aee903":"code","1319a10a":"code","5dc02c35":"code","c9512721":"code","d48858aa":"code","86fcfc51":"code","4b599f18":"code","1b9209bb":"code","d84650c5":"code","96db24df":"markdown","9e4b6574":"markdown","e2b12769":"markdown","99072437":"markdown","dfaa90e5":"markdown"},"source":{"4609da9d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8aa80681":"import zipfile\nzip_ref = zipfile.ZipFile('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip', 'r')\nzip_ref.extractall('\/kaggle\/temp')\nzip_ref.close()\n\nos.listdir('\/kaggle\/temp\/')","7ea5808c":"df = pd.read_csv('\/kaggle\/temp\/train.csv')\ndf.info()","dcbb4934":"df.head()","ab5a70c0":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef summarize_data(corpus):\n    \"\"\"\n    print statements and visualizations to summarize the corpus\n    \"\"\"\n    \n    # get the documents size\n    df_doc_size = pd.Series([len(str(doc).split(\" \")) for doc in corpus])\n    \n    # get the tokens in the corpus\n    df_tokens = pd.Series([token for doc in corpus for token in str(doc).split(\" \")])\n    \n    print(\"---------------------------\")\n    print(\"num docs\", len(corpus))\n    print(\"median tokens\", df_doc_size.median())\n    print(\"num tokens\", len(df_tokens))\n    print(\"unique tokens\", len(df_tokens.value_counts()))\n    print(\"---------------------------\")\n    \n    # make plots\n    fig = plt.figure(figsize=(14,6))\n    ax1 = fig.add_subplot(121)\n    ax2 = fig.add_subplot(122)\n    \n    df_doc_size.plot.hist(ax=ax1, title='Document Sizes')\n    df_tokens.value_counts().plot.hist(ax=ax2, title='Tokens Counts')\n    \nsummarize_data(df.comment_text.values.tolist())","66ddb145":"df[df.drop(['id','comment_text'], axis=1).sum(axis=1)>1]","eaa99ae5":"fig = plt.figure(figsize=(8,6))\ndf.drop(['id','comment_text'], axis=1).sum().sort_values(ascending=False).plot.bar(title='Classes Counts')\nplt.show()","9c624213":"from sklearn.model_selection import train_test_split\n\ntrain_set, valid_set = train_test_split(df, test_size=0.2, random_state=42)\n\nprint(train_set.shape)\nprint(valid_set.shape)","98cf9edc":"fig = plt.figure(figsize=(14,6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\ntrain_set.drop(['id','comment_text'], axis=1).sum().sort_values(ascending=False).plot.bar(title='Classes Counts | Train', ax=ax1)\nvalid_set.drop(['id','comment_text'], axis=1).sum().sort_values(ascending=False).plot.bar(title='Classes Counts | Valid', ax=ax2)","60fbeb81":"train_path = \"\/kaggle\/temp\/train_set.csv\"\nvalid_path = \"\/kaggle\/temp\/valid_set.csv\" \n\ntrain_set.to_csv(train_path, index=False)\nvalid_set.to_csv(valid_path, index=False)\n\nos.listdir('\/kaggle\/temp\/')","c7de4f6c":"import tensorflow as tf\n\nvocab_size = 100000\nsequence_length = 150\n\ntrain_sentences = train_set.comment_text.values.tolist()\nvalid_sentences = valid_set.comment_text.values.tolist()\n\nvectorize_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size, output_mode='int', output_sequence_length=sequence_length)\nvectorize_layer.adapt(train_sentences)\n\nvectorizer = tf.keras.models.Sequential()\nvectorizer.add(tf.keras.Input(shape=(1,), dtype=tf.string))\nvectorizer.add(vectorize_layer)\n\ntrain_sequences = vectorizer.predict(train_sentences)\nvalid_sequences = vectorizer.predict(valid_sentences)","94cabbcc":"print(train_sentences[:3])\nprint(train_sequences[:3])","76a2b2ab":"print(len(vectorize_layer.get_vocabulary()))\nprint(vectorize_layer.get_vocabulary()[:10])","a324da82":"labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\n# input layer\ninputs = tf.keras.layers.Input(shape=(sequence_length,))\n\n# embeddings\nembed = tf.keras.layers.Embedding(vocab_size, 100, input_length=sequence_length, mask_zero=True)(inputs)\n\n# lstm layers\nz = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(embed)\nz = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(z)\n\n# output block\nclass OutputBlock(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.dense = tf.keras.layers.Dense(32, activation='relu')\n        self.dropout = tf.keras.layers.Dropout(0.5)\n        self.out = tf.keras.layers.Dense(1, activation='sigmoid')\n        \n    def call(self, inputs):\n        Z = inputs\n        Z = self.dense(Z)\n        Z = self.dropout(Z)\n        return self.out(Z)\n    \noutput_blocks = [OutputBlock(name=label) for label in labels]\noutputs = []\nfor block in output_blocks:\n    outputs.append(block(z))\n\nmodel = tf.keras.models.Model(inputs=[inputs], outputs=outputs)\nmodel.summary()","34aee903":"y_train = [train_set[label].values for label in labels]\ny_valid = [valid_set[label].values for label in labels]","1319a10a":"K = tf.keras.backend\nK.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nmodel.compile(loss={label:\"binary_crossentropy\" for label in labels},\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics={label:tf.keras.metrics.AUC(name='auc') for label in labels})\n\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5)\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"nlp.h5\", save_best_only=True)\n\nhistory = model.fit(train_sequences, y_train, epochs=30, validation_data=(valid_sequences, y_valid), batch_size=128,\n                    callbacks=[early_stopping_cb, checkpoint_cb])","5dc02c35":"loss=history.history['loss']\nval_loss=history.history['val_loss']\nepochs=range(len(loss)) # Get number of epochs\n\n# Plot training and validation loss per epoch\nplt.plot(epochs, loss, 'r', label=\"Training Loss\")\nplt.plot(epochs, val_loss, 'b', label=\"Validation Loss\")\nplt.legend()\nplt.show()","c9512721":"model = tf.keras.models.load_model('nlp.h5', custom_objects={'OutputBlock':OutputBlock}) # rollback to the best model\nmodel.evaluate(valid_sequences, y_valid)","d48858aa":"zip_ref = zipfile.ZipFile(\"\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip\", 'r')\nzip_ref.extractall('\/kaggle\/temp')\n\nzip_ref = zipfile.ZipFile(\"\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/sample_submission.csv.zip\", 'r')\nzip_ref.extractall('\/kaggle\/temp')\n\nzip_ref = zipfile.ZipFile(\"\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/test_labels.csv.zip\", 'r')\nzip_ref.extractall('\/kaggle\/temp')\n\nzip_ref.close()\n\nos.listdir('\/kaggle\/temp')","86fcfc51":"pd.read_csv('\/kaggle\/temp\/sample_submission.csv').head()","4b599f18":"test_set = pd.read_csv('\/kaggle\/temp\/test.csv')\ntest_sentences = test_set.comment_text.values.tolist()\ntest_sequences = vectorizer.predict(test_sentences)\npredictions = model.predict(test_sequences)","1b9209bb":"for label, y_pred in zip(labels, predictions):\n    test_set[label] = y_pred\n    \ntest_set.head()","d84650c5":"test_set.drop(['comment_text'], axis=1).to_csv('submission.csv', index=False)\npd.read_csv('submission.csv').head()","96db24df":"# Create Classification model","9e4b6574":"# Explore the Target","e2b12769":"# keras Text preprocessing with TextVectorization","99072437":"# Dataset Exploration","dfaa90e5":"# Make Predictions"}}