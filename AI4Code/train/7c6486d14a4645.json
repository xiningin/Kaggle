{"cell_type":{"da8c0deb":"code","2f2377a5":"code","a0ae10c2":"code","0b6be6a5":"code","726f07d2":"code","c7689503":"code","8ed898dd":"code","b4fe6e79":"code","dafe996b":"code","c3048237":"code","a1158cc3":"code","53b9d7d6":"code","ad01609d":"code","79a2a80f":"code","39971588":"code","8d7e9ccb":"code","b19d8cc1":"code","02da590a":"code","aa433ac1":"code","51fd2c1c":"code","2f44cbfe":"code","a34df231":"code","8b100970":"code","7580c0a1":"code","a7ea3b20":"code","f341fb4b":"markdown","08b495db":"markdown","979724f8":"markdown","4ba61b08":"markdown","51da00a1":"markdown","eb6fd75d":"markdown","9036ee46":"markdown","e0750817":"markdown","5098648e":"markdown","a6be7346":"markdown","5748104d":"markdown","e6921587":"markdown","05f6dd53":"markdown","cbb0aadf":"markdown","d34825be":"markdown","51054036":"markdown","d770c6b1":"markdown","dd1979ff":"markdown"},"source":{"da8c0deb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n#import numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2f2377a5":"import json\nimport requests\n\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense, Dropout, LSTM\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import mean_absolute_error\n\nsns.set_palette('Set2')\n%matplotlib inline","a0ae10c2":"endpoint = 'https:\/\/min-api.cryptocompare.com\/data\/histoday'\nres = requests.get(endpoint + '?fsym=BTC&tsym=USD&limit=2000')\ndata = json.loads(res.content)['Data']\nfor element in data:\n    element.pop('conversionType', None)\n    element.pop('conversionSymbol', None)\n\nhist = pd.DataFrame(data)\nhist = hist.set_index('time')\nhist.index = pd.to_datetime(hist.index, unit='s')\nhist.head(15)","0b6be6a5":"hist.tail(15)","726f07d2":"target_col = 'close'","c7689503":"def train_test_split(df, test_size=0.1):\n    split_row = len(df) - int(test_size * len(df))\n    train_data = df.iloc[:split_row]\n    test_data = df.iloc[split_row:]\n    return train_data, test_data","8ed898dd":"train, test = train_test_split(hist, test_size=0.1)","b4fe6e79":"def line_plot(line1, line2, label1=None, label2=None, title='', lw=2):\n    fig, ax = plt.subplots(1, figsize=(16, 9))\n    ax.plot(line1, label=label1, linewidth=lw)\n    ax.plot(line2, label=label2, linewidth=lw)\n    ax.set_ylabel('price [USD]', fontsize=14)\n    ax.set_title(title, fontsize=18)\n    ax.legend(loc='best', fontsize=18);","dafe996b":"line_plot(train[target_col], test[target_col], 'training', 'test', title='BTC')","c3048237":"def normalise_zero_base(df):\n    \"\"\" Normalise dataframe column-wise to reflect changes with respect to first entry. \"\"\"\n    return df \/ df.iloc[0] - 1\n\ndef normalise_min_max(df):\n    \"\"\" Normalise dataframe column-wise min\/max. \"\"\"\n    return (df - df.min()) \/ (data.max() - df.min())","a1158cc3":"def extract_window_data(df, window_len=10, zero_base=True):\n    \"\"\" Convert dataframe to overlapping sequences\/windows of len `window_data`.\n    \n        :param window_len: Size of window\n        :param zero_base: If True, the data in each window is normalised to reflect changes\n            with respect to the first entry in the window (which is then always 0)\n    \"\"\"\n    window_data = []\n    for idx in range(len(df) - window_len):\n        tmp = df[idx: (idx + window_len)].copy()\n        if zero_base:\n            tmp = normalise_zero_base(tmp)\n        window_data.append(tmp.values)\n    return np.array(window_data)","53b9d7d6":"\ndef prepare_data(df, target_col, window_len=10, zero_base=True, test_size=0.2):\n    \"\"\" Prepare data for LSTM. \"\"\"\n    # train test split\n    train_data, test_data = train_test_split(df, test_size=test_size)\n    \n    # extract window data\n    X_train = extract_window_data(train_data, window_len, zero_base)\n    X_test = extract_window_data(test_data, window_len, zero_base)\n    \n    # extract targets\n    y_train = train_data[target_col][window_len:].values\n    y_test = test_data[target_col][window_len:].values\n    if zero_base:\n        y_train = y_train \/ train_data[target_col][:-window_len].values - 1\n        y_test = y_test \/ test_data[target_col][:-window_len].values - 1\n\n    return train_data, test_data, X_train, X_test, y_train, y_test","ad01609d":"def build_lstm_model(input_data, output_size, neurons=20, activ_func='linear',\n                     dropout=0.25, loss='mae', optimizer='adam'):\n    model = Sequential()\n\n    model.add(LSTM(neurons, input_shape=(input_data.shape[1], input_data.shape[2])))\n    model.add(Dropout(dropout))\n    model.add(Dense(units=output_size))\n    model.add(Activation(activ_func))\n\n    model.compile(loss=loss, optimizer=optimizer)\n    return model","79a2a80f":"np.random.seed(42)\n\n# data params\nwindow_len = 7\ntest_size = 0.1\nzero_base = True\n\n# model params\nlstm_neurons = 20\nepochs = 50\nbatch_size = 4\nloss = 'mae'\ndropout = 0.25\noptimizer = 'adam'","39971588":"train, test, X_train, X_test, y_train, y_test = prepare_data(hist, target_col, window_len=window_len, zero_base=zero_base, test_size=test_size)","8d7e9ccb":"model = build_lstm_model(\n    X_train, output_size=1, neurons=lstm_neurons, dropout=dropout, loss=loss,\n    optimizer=optimizer)\nhistory = model.fit(\n    X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=True)","b19d8cc1":"targets = test[target_col][window_len:]\npreds = model.predict(X_test).squeeze()","02da590a":"mean_absolute_error(preds, y_test)","aa433ac1":"preds = test[target_col].values[:-window_len] * (preds + 1)\npreds = pd.Series(index=targets.index, data=preds)\n\nline_plot(targets, preds, 'actual', 'prediction', lw=3)","51fd2c1c":"n_points = 30\n\nline_plot(targets[-n_points:], preds[-n_points:], 'actual', 'prediction', lw=3)","2f44cbfe":"line_plot(targets[-n_points:][:-1], preds[-n_points:].shift(-1), 'actual', 'prediction', lw=3)","a34df231":"actual_returns = targets.pct_change()[1:]\npredicted_returns = preds.pct_change()[1:]","8b100970":"#dual_line_plot(actual_returns[-n_points:], predicted_returns[-n_points:], actual_returns[-n_points:][:-1], predicted_returns[-n_points:].shift(-1),'actual returns', 'predicted returns', lw=3)","7580c0a1":"line_plot(actual_returns[-n_points:][:-1], predicted_returns[-n_points:].shift(-1),\n           'actual returns', 'predicted returns', lw=3)","a7ea3b20":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 9))\n\n# actual correlation\ncorr = np.corrcoef(actual_returns, predicted_returns)[0][1]\nax1.scatter(actual_returns, predicted_returns, color='k', marker='o', alpha=0.5, s=100)\nax1.set_title('r = {:.2f}'.format(corr), fontsize=18)\n\n# shifted correlation\nshifted_actual = actual_returns[:-1]\nshifted_predicted = predicted_returns.shift(-1).dropna()\ncorr = np.corrcoef(shifted_actual, shifted_predicted)[0][1]\nax2.scatter(shifted_actual, shifted_predicted, color='k', marker='o', alpha=0.5, s=100)\nax2.set_title('r = {:.2f}'.format(corr), fontsize=18);","f341fb4b":"Then, we split the data into a training and a test set.<br> \nWe used the last 10% of the data for testing, which splits the data on the 2017\u201309\u201314. All data before this date was used for training, all data from this date on was used to test the trained model. <br>\nBelow, I plotted the close column of our DataFrame, which is the daily closing price I intended to predict.","08b495db":"<b> Long Short Term Memory (LSTM) neural network was used to predict the price of Bitcoin that yields the prediction results you saw above. <\/b><br>\n<b>LSTMs are a special kind of Recurrent Neural Networks (RNN), that are particularly suitable for time series problems.<\/b>","979724f8":"we observe an almost perfect match between actual data and predictions, indicating that the model is essentially learning the price at the previous day.","4ba61b08":"For training the LSTM, the data was split into windows of 7 days and within each window we normalised the data to zero base, <br>\ni.e. the first entry of each window is 0 and all other values represent the change with respect to the first value. <br>\nHence, I am predicting price changes, rather than absolute price.","51da00a1":"We used a simple neural network with a single LSTM layer consisting of 20 neurons, <br>\na dropout factor of 0.25, and a Dense layer with a single linear activation function.\n<br> In addition, I used Mean Absolute Error (MAE) as loss function.\n\n","eb6fd75d":"As we can see from the plots above, actual and predicted returns are uncorrelated. <br>\nOnly after applying the 1-day-shift on the predictions we obtain highly correlated returns that resemble the returns of the actual bitcoin data.","9036ee46":"Looking at the actual and predicted returns, both in their original form as well as with the 1-day-shift applied to them, we obtain the same observation.","e0750817":"# Plot Data","5098648e":"***First, we fetched historic Bitcoin price data. <br>\nTo do so I used the API from cryptocompare:***","a6be7346":"# Compare Returns","5748104d":"# Plot Prediction","e6921587":"### split Data into Testing & Training","05f6dd53":"<b> The fundamental flaw with this model is that for the prediction of a particular day, it is mostly using the value of the previous day.\n<br>\nThe prediction line doesn\u2019t seem to be much more than a shifted version of the actual price.\n<br>\nIn fact, if we adjust the predictions and shift them by a day, this observation becomes even more obvious. <\/b>","cbb0aadf":"Using the trained model to predict on the left-out test set.","d34825be":"***Let\u2019s take a closer look and zoom into the last 30 days of the plot***","51054036":"def dual_line_plot(line1, line2, line3, line4, label1=None, label2=None, title='', lw=2):\n    import matplotlib.dates as mdates\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(21, 9))\n    ax1.plot(line1, label=label1, linewidth=lw)\n    ax1.plot(line2, label=label2, linewidth=lw)\n    ax2.plot(line3, label=label1, linewidth=lw)\n    ax2.plot(line4, label=label2, linewidth=lw)\n    ax2.set_xticks(ax1.get_xticks())\n    ax2.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n    ax1.set_ylabel('daily returns', fontsize=14)\n    ax2.legend(loc='best', fontsize=18);","d770c6b1":"# LSTM - Long short term memory","dd1979ff":"# Bitcoin Prediction System"}}