{"cell_type":{"b94585a8":"code","02aa39ba":"code","e529ad50":"code","442ff7c6":"code","047e0ca9":"code","21622675":"code","056b4bb9":"code","599807b2":"code","3cf9f094":"code","a35a4e94":"code","27317708":"code","f6d4cc7c":"code","72025019":"code","a726827c":"code","1547cf4a":"code","7750f18a":"code","4ea0212a":"code","f8e469ca":"code","26eae97b":"code","9d885a28":"code","e68facbb":"code","98be0240":"code","8a797cc7":"code","4f1d9025":"code","8b9c51ab":"code","a9f6cdb2":"code","28287935":"code","5f69564e":"code","381cc674":"code","c77bf3b6":"code","133e9336":"code","cf1bd7a9":"code","c1703edf":"code","d0a7775a":"code","01349fe4":"code","84e96c64":"code","7feb40dd":"code","e35fa4e5":"code","dd4a4f42":"code","4c65aed3":"code","61dea188":"code","401405e8":"code","914a1062":"code","d414c401":"code","974af836":"code","c84c215b":"code","e078aebe":"code","62c0f17a":"code","7a924835":"code","19cb33da":"code","bb25a3e4":"code","23879295":"code","6f321cc6":"code","a216b29a":"code","1c7c0057":"code","00126e94":"code","a0314163":"code","4730783f":"code","deb36325":"code","058ec1f4":"code","3105b43a":"code","8c0fc664":"code","1aaf5c8d":"code","078ae20a":"code","a6382178":"code","ecac3d2c":"code","736be071":"code","49609cea":"code","d6a87be6":"code","99416725":"code","43e13df6":"code","5096efaf":"code","8bb6e12c":"code","80c192a1":"code","e99c89d6":"code","0b190c54":"code","3be60079":"code","b5e752dd":"code","c7ce6ab0":"markdown","d978abe6":"markdown","d8f035d7":"markdown","4c7d158a":"markdown","4ab57460":"markdown","f03a1448":"markdown","495ab74d":"markdown","fc18a18a":"markdown","edc290fd":"markdown","10e25feb":"markdown","7e5a2cc8":"markdown","70f3a537":"markdown","2b1ace13":"markdown","933c3124":"markdown"},"source":{"b94585a8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# for visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pylab import rcParams\n\n# for modeling \nimport sklearn\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import plot_precision_recall_curve, precision_recall_curve\nfrom sklearn.metrics import plot_confusion_matrix, classification_report, confusion_matrix\nfrom sklearn.metrics import roc_curve, roc_auc_score, f1_score, recall_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets, metrics\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\n\nimport imblearn\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\n\nimport missingno as msno\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.warn(\"this will not show\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","02aa39ba":"d = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')\ndf = d.copy().sample(10000)\ndf","e529ad50":"df.info()","442ff7c6":"pd.DataFrame({\"No. of unique values\": list(df.nunique())}, index=df.columns)","047e0ca9":"df.duplicated().sum()","21622675":"y = len(df[df['RainToday'] == 'Yes'])\nn = len(df[df['RainToday'] == 'No'])\nprint(y,n)","056b4bb9":"df.describe().T","599807b2":"df.corr() #I can see from here which variables I can put into the model\n          #For example, the variables can be removed by looking at the order of importance.","3cf9f094":"#VISUALIZATION OF NAN  VALUES\nmsno.matrix(df)","a35a4e94":"#drop missing values in the RainToday and RainTomorrow\ndf.dropna(subset=['RainToday', 'RainTomorrow'],axis=0,inplace=True)","27317708":"ax = df['RainTomorrow'].value_counts(normalize=True).plot.bar(color=[\"blue\", \"red\"])\ndef labels(ax):\n    for p in ax.patches:\n        ax.annotate(f\"%{p.get_height()*100:.2f}\", (p.get_x() + 0.15, p.get_height() * 1.005),size=11)\nlabels(ax)","f6d4cc7c":"def summary(df):\n    Types = df.dtypes\n    Counts = df.apply(lambda x: x.count())\n    Min = df.min()\n    Max = df.max()\n    Uniques = df.apply(lambda x: x.unique().shape[0]) # .shape[0] yazilmaz ise unique olan degerlerin listelerini getirir.\n    Nulls = df.apply(lambda x: x.isnull().sum())\n\n    cols = ['Types', 'Counts', 'Uniques', 'Nulls', 'Min', 'Max']\n    str = pd.concat([Types, Counts, Uniques, Nulls, Min, Max], axis = 1, sort=True)\n\n    str.columns = cols\n    display(str.sort_values(by='Nulls', ascending=False))\n    print('__________Data Types__________\\n')\n    print(str.Types.value_counts())\nsummary(df)","72025019":"#label encoding for univariate variables\nfrom sklearn.preprocessing import LabelEncoder\n\nlbe = LabelEncoder()\ndf[\"RainToday_label\"] = lbe.fit_transform(df[\"RainToday\"])\ndf[\"RainTomorrow_label\"] = lbe.fit_transform(df[\"RainTomorrow\"])","a726827c":"# one-hot encoding for variables with more than 2 categories\n\n#drop variables with so many countries for the sake of time and memory consumption\ndf.drop(['Date','Location','WindDir9am','WindDir3pm','WindGustDir'], axis=1, inplace=True) ","1547cf4a":"summary(df)","7750f18a":"# DecisionTreeRegressor\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.tree import DecisionTreeRegressor\n\n#drop unnecessary columns and date columns\ndf_imputation = df.drop(['RainToday','RainTomorrow'], axis=1) \n\n#define variables to keep the index and the columns\nindex = df_imputation.index\ncolumns = df_imputation.columns\n\n#imputation steps\nimp_tree = IterativeImputer(random_state=0, estimator=DecisionTreeRegressor())\nimp_tree.fit(df_imputation)\ndf_imputed = imp_tree.transform(df_imputation)\n\n#transform imputed data in array format to dataframe\ndf_imputed_tree = pd.DataFrame(df_imputed, index=index, columns=columns)\n\ndf_imputed_tree.isnull().sum()","4ea0212a":"df_imputed_tree.info()","f8e469ca":"df2 = df_imputed_tree.copy()\nx_dat = df2.drop(['RainTomorrow_label'],axis=1)\ny = df2[\"RainTomorrow_label\"].values","26eae97b":"X=(x_dat-np.min(x_dat))\/(np.max(x_dat)-np.min(x_dat)).values","9d885a28":"from sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.metrics import accuracy_score","e68facbb":"dtc = DecisionTreeClassifier()","98be0240":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state =42)\n\ncart_model = DecisionTreeClassifier(random_state=42).fit(X_train,y_train)\ny_pred = cart_model.predict(X_test)\ny_pred","8a797cc7":"dtc_score=accuracy_score(y_test,y_pred)\ndtc_score","4f1d9025":"c_dtc=confusion_matrix(y_test,y_pred)\nc_dtc","8b9c51ab":"print(classification_report(y_test,y_pred))","a9f6cdb2":"tree_grid = {\"max_depth\": range(1,10),\n            \"min_samples_split\" : list(range(2,50)) }","28287935":"tree1 = DecisionTreeClassifier()\ntree_cv = GridSearchCV(tree1, tree_grid, cv = 10, n_jobs = -1, verbose = 2)\ntree_cv_model = tree_cv.fit(X_train, y_train)","5f69564e":"print(\"Best Parameters: \" + str(tree_cv_model.best_params_))","381cc674":"tree1 = DecisionTreeClassifier(max_depth = 5, min_samples_split = 23)\ntree_tuned1 = tree1.fit(X_train, y_train)","c77bf3b6":"y_pred = tree_tuned1.predict(X_test)\ndtc_finalscore=accuracy_score(y_test, y_pred)\ndtc_finalscore","133e9336":"c_dtc2=confusion_matrix(y_test,y_pred)\nc_dtc2","cf1bd7a9":"print(classification_report(y_test,y_pred))","c1703edf":"from sklearn.ensemble import RandomForestClassifier","d0a7775a":"rf_model=RandomForestClassifier()\nrf_model.fit(X_train,y_train)","01349fe4":"y_pred = rf_model.predict(X_test)","84e96c64":"rf_score=accuracy_score(y_test,y_pred)\nrf_score","7feb40dd":"c_rf=confusion_matrix(y_test,y_pred)\nc_rf","e35fa4e5":"print(classification_report(y_test,y_pred))","dd4a4f42":"rf_model.predict(X_test)[0:10] # ilk 10 datatestdeki tahminler.","4c65aed3":"rf_model.predict_proba(X_test)[0:10] #1.si 0 olma 2.si 1 olma olasiligi oranlari.","61dea188":"from sklearn.ensemble import RandomForestClassifier \nscore_list=[]\nfor each in range(1,75):\n    rf2=RandomForestClassifier(n_estimators=each, random_state=42)\n    rf2.fit(X_train, y_train)\n    score_list.append(100*rf2.score(X_test, y_test))\n    print(\"n_estimators=\", each, \"--> Accuracy:\", 100*rf2.score(X_test, y_test), \"%\")\n\nplt.plot([*range(1,75)], score_list)\nplt.xlabel(\"n_estimators Value\")\nplt.ylabel(\"Accuracy %\")\nplt.show()","401405e8":"Importance = pd.DataFrame({\"Importance\": rf_model.feature_importances_*100},\n                         index = X_train.columns)\nImportance.sort_values(by = \"Importance\", \n                       axis = 0, \n                       ascending = True).plot(kind =\"barh\", color = \"b\")\n\nplt.xlabel(\"Variable Importance Levels\");","914a1062":"y_pred = rf2.predict(X_test)\nrf_finalscore=accuracy_score(y_test, y_pred)\nrf_finalscore","d414c401":"from lightgbm import LGBMClassifier","974af836":"lgbm_model = LGBMClassifier().fit(X_train, y_train)","c84c215b":"y_pred = lgbm_model.predict(X_test)","e078aebe":"accuracy_score(y_test, y_pred)","62c0f17a":"lgbm_params = {\n        'n_estimators': [100, 500, 1000, 2000],\n        'subsample': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5,6],\n        'learning_rate': [0.1,0.01,0.02,0.05],\n        \"min_child_samples\": [5,10,20]}","7a924835":"lgbm = LGBMClassifier(learning_rate = 0.1, \n                       max_depth = 4,\n                       subsample = 0.6,\n                       n_estimators = 500,\n                       min_child_samples = 10)\nlgbm_tuned = lgbm.fit(X_train,y_train)","19cb33da":"y_pred = lgbm_tuned.predict(X_test)\nlgbm_finalscore=accuracy_score(y_test, y_pred)\nlgbm_finalscore","bb25a3e4":"c_lgbm=confusion_matrix(y_test,y_pred)\nc_lgbm","23879295":"print(classification_report(y_test,y_pred))","6f321cc6":"from sklearn.ensemble import GradientBoostingClassifier","a216b29a":"gbm_model = GradientBoostingClassifier().fit(X_train, y_train)","1c7c0057":"y_pred = gbm_model.predict(X_test)","00126e94":"accuracy_score(y_test, y_pred)","a0314163":"gbm = GradientBoostingClassifier(learning_rate = 0.05, \n                                 max_depth = 10,\n                                min_samples_split = 10,\n                                n_estimators = 100)","4730783f":"gbm_tuned =  gbm.fit(X_train,y_train)","deb36325":"y_pred = gbm_tuned.predict(X_test)\ngbm_finalscore=accuracy_score(y_test,y_pred)\ngbm_finalscore","058ec1f4":"c_gbm=confusion_matrix(y_test,y_pred)\nc_gbm","3105b43a":"print(classification_report(y_test,y_pred))","8c0fc664":"!pip install xgboost","1aaf5c8d":"from xgboost import XGBClassifier","078ae20a":"xgb_model = XGBClassifier().fit(X_train, y_train)","a6382178":"y_pred = xgb_model.predict(X_test)","ecac3d2c":"accuracy_score(y_test, y_pred)","736be071":"xgb = XGBClassifier(learning_rate = 0.01, \n                    max_depth = 6,\n                    n_estimators = 100,\n                    subsample = 0.8)\nxgb_tuned =  xgb.fit(X_train,y_train)\ny_pred = xgb_tuned.predict(X_test)\nXGBoost_finalscore=accuracy_score(y_test, y_pred)\nXGBoost_finalscore","49609cea":"c_xgb=confusion_matrix(y_test,y_pred)\nc_xgb","d6a87be6":"print(classification_report(y_test,y_pred))","99416725":"!pip install catboost","43e13df6":"from catboost import CatBoostClassifier, Pool","5096efaf":"cat = CatBoostClassifier()","8bb6e12c":"cat.fit(X_train, y_train)\ny_pred = cat.predict(X_test)","80c192a1":"cat_finalscore = accuracy_score(y_test, y_pred)\ncat_finalscore","e99c89d6":"c_cat=confusion_matrix(y_test,y_pred)\nc_cat","0b190c54":"print(classification_report(y_test,y_pred))","3be60079":"print(dtc_finalscore,rf_finalscore,lgbm_finalscore, gbm_finalscore, XGBoost_finalscore, cat_finalscore)","b5e752dd":"idx = [\"DTM\",\"RFM\",\"LGBM\", \"GBM\", \"XGBM\", \"CATBM\"]\nregressions = [rf_finalscore,dtc_finalscore, gbm_finalscore, XGBoost_finalscore, lgbm_finalscore,cat_finalscore,]\n\nplt.figure(figsize=(6,4))\nsns.barplot(x=idx,y=regressions)\nplt.xticks()\nplt.title('Model Comparision',color = 'orange',fontsize=20);","c7ce6ab0":"### ----> Best Model is CatBOOST <----","d978abe6":"## Model Tuning of LightGBM","d8f035d7":"## Model Tuning of GBM","4c7d158a":"## SOME OF VISUALIZATION","4ab57460":"## 6-Catboost","f03a1448":"## 5-XGBOOST","495ab74d":"## Decision Tree Model tuning","fc18a18a":"## 4-Gradient Boosting Classifier","edc290fd":"We want you to set the 'RainToday' variable as the dependent variable and set up a model that predicts whether there will be rain or not. \nThis model will be made using;\n\n- 'Desicion Tree',\n- 'Random Forest',\n- 'LightGBM',\n- 'GBM',\n- 'XGBoost',\n- 'CatBoost' models.","10e25feb":"## DATA READING AND EXPLORING","7e5a2cc8":"## 3-LightGBM","70f3a537":"## 1-Decision Tree Classifier","2b1ace13":"## 2-Random Forest Classifier","933c3124":"## Model Tuning of XGBoost"}}