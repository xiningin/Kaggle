{"cell_type":{"170a897e":"code","72f56c4f":"code","0156ce5c":"code","d18da316":"code","7b63cb7f":"code","a5b99323":"code","8a4a510a":"code","96396a81":"code","368c276e":"code","4a6a42ab":"code","c5487adf":"code","8ae985d6":"code","ac500eeb":"code","300451e1":"code","d72c3e5b":"code","d710ea03":"code","33d36d70":"code","fa68de4a":"code","bb7ec7c2":"code","6ce7ea79":"code","62f0ff6b":"code","41ec5e17":"code","9092e5af":"code","83eff0f6":"code","15323460":"code","64285fa4":"code","75317f82":"code","a2e6ba69":"code","1d651783":"code","7f66a770":"code","62348a6a":"code","07642fd3":"code","c472bba7":"code","84b10068":"code","8c0e5312":"code","012bccbb":"code","a0e80bfa":"code","27ee368e":"markdown","e35842f3":"markdown","c52e8562":"markdown","36854489":"markdown","8f51b083":"markdown","bb64793b":"markdown","973ebe32":"markdown","18d3e6c5":"markdown","0d403890":"markdown","70a725f5":"markdown"},"source":{"170a897e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVC, SVC\nfrom sklearn.metrics import accuracy_score, classification_report,confusion_matrix\npd.options.display.precision = 15\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\n\nimport json\nimport ast\nimport eli5\nimport shap\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, train_test_split\nfrom sklearn.linear_model import Ridge, RidgeCV\nimport gc\nfrom catboost import CatBoostClassifier\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport altair as alt\nfrom IPython.display import HTML\nfrom sklearn.linear_model import LinearRegression\n\nfrom scipy.signal import hilbert\nfrom scipy.signal import hann\nfrom scipy.signal import convolve\nfrom scipy import stats\nfrom sklearn.kernel_ridge import KernelRidge\n\nfrom bayes_opt import BayesianOptimization\n\nimport os\n","72f56c4f":"IS_LOCAL = False\nif(IS_LOCAL):\n    PATH=\"..\/input\/Santander\/\"\nelse:\n    PATH=\"..\/input\/\"\nos.listdir(PATH)","0156ce5c":"%%time\ndf = pd.read_csv(PATH+\"online_shoppers_intention.csv\")","d18da316":"df.shape","7b63cb7f":"df.head()","a5b99323":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","8a4a510a":"%%time\nmissing_data(df)","96396a81":"df = df.dropna()","368c276e":"%%time\ndf.describe()","4a6a42ab":"df.dtypes","c5487adf":"# categorical feature\ndf['SpecialDay'] = df['SpecialDay'].astype('object')\ndf['OperatingSystems'] = df['OperatingSystems'].astype('object')\ndf['Browser'] = df['Browser'].astype('object')\ndf['Region'] = df['Region'].astype('object')\ndf['TrafficType'] = df['TrafficType'].astype('object')","8ae985d6":"sns.countplot(df['Revenue'], palette='Set3')","ac500eeb":"print(\"There are {}% target values with 1\".format(100 * df[\"Revenue\"].value_counts()[1]\/df.shape[0]))","300451e1":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(3,3,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(3,3,i)\n        sns.distplot(df1[feature], hist=False,label=label1)\n        sns.distplot(df2[feature], hist=False,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();","d72c3e5b":"t0 = df.loc[df['Revenue'] == True]\nt1 = df.loc[df['Revenue'] == False]\nfeatures = df.columns.values[0:9]\nplot_feature_distribution(t0, t1, 'True', 'False', features)","d710ea03":"sns.countplot(df['SpecialDay'], palette='Set3')","33d36d70":"sns.countplot(df['OperatingSystems'], palette='Set3')","fa68de4a":"sns.countplot(df['Browser'], palette='Set3')","bb7ec7c2":"sns.countplot(df['Region'], palette='Set3')","6ce7ea79":"sns.countplot(df['TrafficType'], palette='Set3')","62f0ff6b":"sns.countplot(df['Weekend'], palette='Set3')","41ec5e17":"df = pd.get_dummies(df,columns=['SpecialDay', 'Month', 'OperatingSystems','Browser','Region','TrafficType','VisitorType','Weekend'])","9092e5af":"df.columns","83eff0f6":"target = df['Revenue']\ndf = df.drop(columns=['Revenue'])","15323460":"n_fold = 20\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=11)","64285fa4":"X_train,X_test,y_train,y_test = train_test_split(df,target,random_state=0)","75317f82":"X_train.shape","a2e6ba69":"X_test.shape","1d651783":"def lgbm_evaluate(**params):\n    warnings.simplefilter('ignore')\n    \n    params['num_leaves'] = int(params['num_leaves'])\n    params['max_depth'] = int(params['max_depth'])\n        \n    clf = lgb.LGBMClassifier(**params, n_estimators=20000, nthread=-1)\n\n    test_pred_proba = np.zeros((X_train.shape[0], 2))\n    \n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X_train, y_train)):\n        X_train_bo, X_valid = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n        y_train_bo, y_valid = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n        \n        model = lgb.LGBMClassifier(**params, n_estimators = 10000, n_jobs = -1)\n        model.fit(X_train_bo, y_train_bo, \n                eval_set=[(X_train_bo, y_train_bo), (X_valid, y_valid)], eval_metric='binary_logloss',\n                verbose=False, early_stopping_rounds=200)\n\n        y_pred_valid = model.predict_proba(X_valid)\n\n        test_pred_proba[valid_idx] = y_pred_valid\n\n    return accuracy_score(y_valid, y_pred_valid.argmax(1))","7f66a770":"params = {'colsample_bytree': (0.6, 1),\n     'learning_rate': (.001, .08), \n      'num_leaves': (8, 124), \n      'subsample': (0.6, 1), \n      'max_depth': (3, 25), \n      'reg_alpha': (.05, 15.0), \n      'reg_lambda': (.05, 15.0), \n      'min_split_gain': (.001, .03),\n      'min_child_weight': (12, 80)}\n\nbo = BayesianOptimization(lgbm_evaluate, params)\nbo.maximize(init_points=5, n_iter=20)","62348a6a":"def eval_acc(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'acc', accuracy_score(labels, preds.argmax(1)), True\n\ndef train_model(X, X_test, y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n\n    oof = np.zeros((len(X), 2))\n    prediction = np.zeros((len(X_test), 2))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        \n        if model_type == 'lgb':\n            model = lgb.LGBMClassifier(**params, n_estimators = 10000, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='multi_logloss',\n                    verbose=5000, early_stopping_rounds=200)\n            \n            y_pred_valid = model.predict_proba(X_valid)\n            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict_proba(X_valid)\n            score = accuracy_score(y_valid, y_pred_valid.argmax(1))\n            print(f'Fold {fold_n}. Accuracy: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict_proba(X_test)\n        \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=20000,  eval_metric='MAE', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid\n        scores.append(accuracy_score(y_valid, y_pred_valid.argmax(1)))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction \/= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] \/= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction\n    \n    else:\n        return oof, prediction","07642fd3":"params = {'num_leaves': int(bo.max['params']['num_leaves']),\n          'min_data_in_leaf': int(bo.max['params']['min_child_weight']),\n          'min_split_gain': bo.max['params']['min_split_gain'],\n          'objective': 'binary',\n          'max_depth': int(bo.max['params']['max_depth']),\n          'learning_rate': bo.max['params']['learning_rate'],\n          \"boosting\": \"gbdt\",\n          \"bagging_freq\": 5,\n          \"bagging_fraction\": bo.max['params']['subsample'],\n          \"bagging_seed\": 11,\n          \"verbosity\": -1,\n          'reg_alpha': bo.max['params']['reg_alpha'],\n          'reg_lambda': bo.max['params']['reg_lambda'],\n          \"num_class\": 1,\n          'nthread': -1\n         }","c472bba7":"oof_lgb, prediction_lgb, feature_importance = train_model(X=X_train, X_test=X_test, y=y_train, params=params, model_type='lgb', plot_feature_importance=True)","84b10068":"print(\"Test score: \",accuracy_score(y_test,prediction_lgb.argmax(1)))","8c0e5312":"import itertools\n\ndef plot_confusion_matrix(truth, pred, classes, normalize=False, title=''):\n    cm = confusion_matrix(truth, pred)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=(10, 10))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion matrix', size=15)\n    plt.colorbar(fraction=0.046, pad=0.04)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.tight_layout()","012bccbb":"plot_confusion_matrix(y_test, prediction_lgb.argmax(1), ['False','True'])","a0e80bfa":"print(classification_report(y_test, prediction_lgb.argmax(1)))","27ee368e":"# Data Visualization","e35842f3":"# Train","c52e8562":"# Load Data","36854489":"## Please upvote if you like it ;)","8f51b083":"## Please upvote if you like it ;)","bb64793b":"# Import Modules","973ebe32":"# Check Result","18d3e6c5":"# Prepare Train Data","0d403890":"# Columns \nAdministrative : Administrative Value  \nAdministrative_Duration : Duration in Administrative Page  \nInformational : Informational Value  \nInformational_Duration : Duration in Informational Page  \nProductRelated : Product Related Value  \nProductRelated_Duration : Duration in Product Related Page  \nBounceRates : Bounce Rates of a web page  \nExitRates : Exit rate of a web page  \nPageValues : Page values of each web page  \nSpecialDay : Special days like valentine etc  \nMonth : Month of the year  \nOperatingSystems : Operating system used  \nBrowser : Browser used  \nRegion : Region of the user  \nTrafficType : Traffic Type  \nVisitorType : Types of Visitor  \nWeekend : Weekend or not  \nRevenue : Revenue will be generated or not  ","70a725f5":"# Bayesian Optimization"}}