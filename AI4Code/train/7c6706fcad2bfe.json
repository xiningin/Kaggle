{"cell_type":{"17e2117b":"code","60a3d00a":"code","1e781e1b":"code","3c606e7a":"code","85716441":"code","15c25954":"code","a3477614":"code","10f5232f":"code","a70121fb":"code","ba6666b8":"code","55f8a9b3":"code","baf39900":"code","bc1ab6cc":"code","3fac9fde":"code","67602eca":"code","5bd240a4":"code","1fc9555c":"code","927cd41a":"code","9b188910":"code","ac9e7dfb":"code","156973d5":"code","a9e9401e":"code","7b798429":"code","35d33705":"code","4885aa82":"code","2c77c040":"code","233589eb":"code","8770ee12":"code","b7b083ac":"code","faf2dc27":"code","de029605":"code","da9bb947":"code","d129b93c":"code","1bc01cc5":"code","f55bebe3":"code","080eeff4":"code","f9a1bd44":"code","72858bb9":"code","1891bb60":"code","be0bfad1":"markdown","5b779397":"markdown","d629f3d8":"markdown","a19069c2":"markdown","5dda935c":"markdown"},"source":{"17e2117b":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","60a3d00a":"from fastai.vision import *\nfrom fastai.metrics import error_rate\nfrom fastai.callbacks import SaveModelCallback\nfrom sklearn.metrics import roc_auc_score","1e781e1b":"bs = 48\nimg_size=320 #actual size of all pics is 500,500","3c606e7a":"PATH = Path('..\/input\/flower-recognition-he\/he_challenge_data\/data\/')","85716441":"train_df=pd.read_csv(PATH\/'train.csv')\ntrain_df.head()","15c25954":"tfms = get_transforms(do_flip=True, flip_vert=True, max_rotate=360.0, max_zoom=1.35, max_warp=0.2, max_lighting=.4,)","a3477614":"src = (\n    ImageList.from_df(train_df,PATH,folder='train',suffix='.jpg')\n        .split_by_rand_pct(0.2, seed=42)\n        .label_from_df(cols='category')    \n    )\ndata = (\n    src.transform(tfms=tfms,size=img_size)\n    .databunch(bs=bs)\n    .normalize(imagenet_stats)\n)","10f5232f":"#data.show_batch(rows=3, figsize=(7,6))","a70121fb":"#print(data.classes)\n#len(data.classes),data.c","ba6666b8":"class FocalLoss(nn.Module):\n    def __init__(self, alpha=1., gamma=1.):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, inputs, targets, **kwargs):\n        CE_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n        pt = torch.exp(-CE_loss)\n        F_loss = self.alpha * ((1-pt)**self.gamma) * CE_loss\n        return F_loss.mean()\n\n'''\n\ndef roc_score(inp, target): #defined for binary classification only not multiclass\n    _, indices = inp.max(1)\n    return torch.Tensor([roc_auc_score(target, indices)])[0]\n\n\nfrom torch import nn\nimport torch.nn.functional as F\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, logits=False, reduction='elementwise_mean'):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.logits = logits\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        if self.logits:\n            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n        else:\n            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n\n        if self.reduction is None:\n            return F_loss\n        else:\n            return torch.mean(F_loss)\n'''","55f8a9b3":" #Making pretrained weights work without needing to find the default filename\nif not os.path.exists('\/tmp\/.cache\/torch\/checkpoints\/'):\n        os.makedirs('\/tmp\/.cache\/torch\/checkpoints\/')\n!cp '..\/input\/resnet50\/resnet50.pth' '\/tmp\/.cache\/torch\/checkpoints\/resnet50-19c8e357.pth'\n#!cp '..\/input\/resnet34\/resnet34.pth' '\/tmp\/.cache\/torch\/checkpoints\/resnet34-333f7ec4.pth'","baf39900":"#loss_func = FocalLoss(gamma=1.)\nlearn = (cnn_learner(data,\n                     models.resnet50,\n                     metrics=[accuracy, error_rate],\n                     loss_func= LabelSmoothingCrossEntropy(),\n                     model_dir= '..\/..\/..\/..\/working\/'))","bc1ab6cc":"#learn.lr_find() \n#learn.recorder.plot()","3fac9fde":"learn.load('..\/input\/flower-weights2\/bestmodel');","67602eca":"lr=1e-02","5bd240a4":"learn.fit_one_cycle(15, lr,callbacks=[SaveModelCallback(learn, every='improvement', monitor='accuracy')])","1fc9555c":"learn.save('stage-1')\n!cp \/kaggle\/working\/bestmodel.pth \/kaggle\/working\/bestmodel_stg1.pth ","927cd41a":"learn.load('bestmodel');\nlearn.fit_one_cycle(10, lr,callbacks=[SaveModelCallback(learn, every='improvement', monitor='accuracy')])","9b188910":"!cp \/kaggle\/working\/bestmodel.pth \/kaggle\/working\/bestmodel_stg1_2.pth ","ac9e7dfb":"learn.unfreeze()","156973d5":"lr=1e-05\nlearn.fit_one_cycle(15, slice(lr\/100,lr\/10),callbacks=[SaveModelCallback(learn, every='improvement', monitor='accuracy')])","a9e9401e":"learn.save('stage-2')\n!cp \/kaggle\/working\/bestmodel.pth \/kaggle\/working\/bestmodel_stg2.pth","7b798429":"bs = 24\nimg_size=399 #actual size of all pics is 500,500\ndata = (\n    src.transform(tfms=get_transforms(),size=img_size)\n    .databunch(bs=bs)\n    .normalize(imagenet_stats)\n)","35d33705":"#learn.data=data","4885aa82":"#learn.freeze()","2c77c040":"learn.load('bestmodel');","233589eb":"#learn.fit_one_cycle(5, max_lr=slice(1e-6,1e-4),callbacks=[SaveModelCallback(learn, every='improvement', monitor='valid_loss')])","8770ee12":"learn.save('stage-1-299');","b7b083ac":"#learn.unfreeze()\nlearn.load('bestmodel');\n#learn.fit_one_cycle(5, max_lr=slice(1e-6,1e-4),callbacks=[SaveModelCallback(learn, every='improvement', monitor='valid_loss')])","faf2dc27":"learn.save('stage-2-299')\nlearn.load('bestmodel');","de029605":"sample_df=pd.read_csv(PATH\/'sample_submission.csv')","da9bb947":"learn.data.add_test(ImageList.from_df(sample_df,PATH,folder='test',suffix='.jpg'))","d129b93c":"preds,_ = learn.TTA(ds_type=DatasetType.Test)\n","1bc01cc5":"preds=np.argmax(preds,axis=1)\npreds[:5]\n","f55bebe3":"fnames = [f.split('.')[2].split('\/')[-1] for f in learn.data.test_ds.items]\nfnames[:3]","080eeff4":"labelled_preds = [learn.data.classes[pred] for pred in preds]\nlabelled_preds[:10]","f9a1bd44":"df = pd.DataFrame({'image_id':fnames,'category':labelled_preds}, columns=['image_id','category'])\ndf.head()","72858bb9":"df = df.sort_values(by = ['image_id'], ascending = [True])\ndf.to_csv('resnet34.csv', index=False)","1891bb60":"#without comminting download sub file :)\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"resnet50.csv\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\ncreate_download_link(df)","be0bfad1":"## Predictions","5b779397":"valid_names=trn_df.sample(n=350, random_state=1) \nvalidList = ImageList.from_df(df=valid_names,path='..\/input\/aptos2019-blindness-detection\/train_images\/',cols='id_code',suffix='.png') \nvalidList\n\nvalid_idx=valid_names.index \nnp.save('..\/working\/valid_idx',valid_idx) \nnp.savetxt('..\/working\/val_idx1',valid_idx)\n\ntrn_df.drop(index=valid_names.index,axis=0,inplace=True) \ntrn_df.shape\n\ntrn_idx=trn_df.index \nnp.save('..\/working\/trn_idx',trn_idx) \nnp.savetxt('..\/working\/trn_idx1',trn_idx)\n\nbase_image_dir = os.path.join('..', 'input\/aptos2019-blindness-detection\/') \ntrain_dir = os.path.join(base_image_dir,'train_images\/') \ntrn_df['path'] = trn_df['id_code'].map(lambda x: os.path.join(train_dir,'{}.png'.format(x))) \ntrn_df = trn_df.drop(columns=['id_code']) \ntrn_df.shape","d629f3d8":"learn.lr_find() \nlearn.recorder.plot()","a19069c2":"That's a pretty accurate model!","5dda935c":"Since our model is working as we expect it to, we will *unfreeze* our model and train some more."}}