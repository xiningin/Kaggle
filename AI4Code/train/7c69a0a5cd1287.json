{"cell_type":{"220f1707":"code","2ae504a5":"code","b4875fe4":"code","2b777f6b":"code","5f79b47a":"code","e031076e":"code","8d842e2e":"code","ed5fc35a":"code","5e66b1a1":"code","2dc151ce":"code","ce57ad72":"code","7ea4403f":"code","b3403c76":"code","79d1d409":"code","e1f696cf":"code","1c70d4f6":"code","cb4f95ad":"code","826a19c8":"code","75c6cb69":"code","95da5aaa":"code","4ecef989":"code","0c5d0739":"code","0f413837":"code","c2fd0fe7":"code","7e8d5781":"code","bad4bea3":"code","22e91e2e":"code","d0f3e7a6":"code","3263cefc":"code","90c3ec37":"code","705649d1":"code","18501cc2":"code","5665dce0":"code","ef5211bd":"code","f547a0a7":"code","9e435291":"code","420bc21d":"code","253979bc":"code","d39455be":"code","86647512":"code","bf089b92":"code","054d7788":"code","cbdbfbb2":"code","557a3f72":"code","b6edbc5e":"code","c2377fb4":"code","67c6d5f0":"code","11acb0f1":"code","0cf1149d":"code","bf929070":"code","cf316ca9":"code","17f8ca55":"code","24d64f33":"code","4c109634":"code","b41b0f62":"code","a1bacee0":"code","6a4177c5":"code","2ed33e1b":"code","005adb0e":"code","1ad166b9":"code","f58932c1":"code","d24f7c9e":"code","d81285cc":"code","21d1c42a":"code","4f542765":"code","ce096d80":"code","24a7b34a":"code","ef1d5012":"code","a37c9839":"code","f5f4e5e0":"code","b45eaa80":"code","3e0700d6":"code","fd08896a":"code","72c84497":"code","d6f5cd86":"code","a3fd2e9d":"code","9c99b18d":"code","2379b6ac":"code","b0e9507f":"code","7cf4a2ba":"code","787dcf1b":"code","0d0aceba":"code","4ceafbd5":"code","b95b4a5b":"code","d2749054":"code","bd14f734":"code","ba8e30e3":"code","814a00e8":"code","43733c6f":"code","f4e2f196":"markdown","9772cdec":"markdown","9590c7fa":"markdown","e5275f3b":"markdown","f946cd41":"markdown","d00f7740":"markdown","c96665c9":"markdown","7ea3dc09":"markdown","46de7425":"markdown","488b7c54":"markdown","ba0db1b3":"markdown","76e39dc9":"markdown","c87ea5d4":"markdown"},"source":{"220f1707":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2ae504a5":"# Importing the Necessary Libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set()","b4875fe4":"train_data = pd.read_excel(r\"\/kaggle\/input\/flight-fare-prediction-mh\/Data_Train.xlsx\")","2b777f6b":"pd.set_option('display.max_columns', None)","5f79b47a":"train_data.head()","e031076e":"train_data.shape","8d842e2e":"train_data.info()","ed5fc35a":"train_data.describe()","5e66b1a1":"train_data['Duration'].nunique()","2dc151ce":"train_data['Duration'].value_counts()","ce57ad72":"# Let's check whether there is any null value\ntrain_data.isnull().sum()","7ea4403f":"train_data.dropna(inplace = True)","b3403c76":"# Now again checking whethere there is any missing values\n\n\ntrain_data.isnull().sum()","79d1d409":"train_data[\"Journey_day\"] = pd.to_datetime(train_data[\"Date_of_Journey\"], format=\"%d\/%m\/%Y\").dt.day","e1f696cf":"train_data[\"Journey_month\"] = pd.to_datetime(train_data[\"Date_of_Journey\"], format=\"%d\/%m\/%Y\").dt.month","1c70d4f6":"train_data.head()","cb4f95ad":"# Droping the Date_of_Journey Column\n\ntrain_data.drop([\"Date_of_Journey\"], axis = 1, inplace = True)","826a19c8":"# Extracting Hours\n\ntrain_data[\"Dep_Hour\"] = pd.to_datetime(train_data[\"Dep_Time\"]).dt.hour\n\ntrain_data[\"Dep_min\"] = pd.to_datetime(train_data[\"Dep_Time\"]).dt.minute\n\n# Dropping the Dep_Time column from the data\n\ntrain_data.drop([\"Dep_Time\"], axis = 1, inplace=True)","75c6cb69":"train_data.head()","95da5aaa":"train_data[\"Arrival_Hour\"] = pd.to_datetime(train_data[\"Arrival_Time\"]).dt.hour\n\ntrain_data[\"Arrival_min\"] = pd.to_datetime(train_data[\"Arrival_Time\"]).dt.minute\n\n# Dropping the Arrival_Time column\n\ntrain_data.drop([\"Arrival_Time\"], axis = 1, inplace=True)","4ecef989":"train_data.head()","0c5d0739":"# Shape of the dataset after addition of the new features \ntrain_data.shape","0f413837":"# Assigning and converting Duration Column into list\nduration = list(train_data[\"Duration\"])\nduration","c2fd0fe7":"for i in range(len(duration)):\n    if len(duration[i].split()) != 2:   # to check whether it contains the both hour and minutes\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"  # If duration is having hour only adding the minute value\n        else:\n            duration[i] = \"0h \" + duration[i]     # If duration is having minute only adding the hour value\n            \nduration_hours = []\nduration_mins = []\n\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep=\"h\")[0]))\n    duration_mins.append(int(duration[i].split(sep=\"m\")[0].split()[-1]))","7e8d5781":"# Adding the Duration Hours and Duration Mins Columns in the Datasets\n\ntrain_data[\"Duration_hours\"] = duration_hours\ntrain_data[\"Duration_mins\"] = duration_mins","bad4bea3":"train_data.head()","22e91e2e":"train_data.drop([\"Duration\"], axis = 1, inplace = True)","d0f3e7a6":"print(train_data[\"Airline\"].nunique())\nprint(train_data[\"Airline\"].unique())","3263cefc":"# Counting the number of of flights from each of the Airlines\ntrain_data[\"Airline\"].value_counts()","90c3ec37":"sns.catplot(y=\"Price\", x=\"Airline\", data=train_data.sort_values(\"Price\", ascending=False), kind=\"boxen\", height=6, aspect=3)\nplt.show()","705649d1":"# Since Airline is Nominal Categorical data we will perform OneHotEncoding\n\nAirline = train_data[[\"Airline\"]]\n\nAirline = pd.get_dummies(Airline, drop_first=True)\n\nAirline.head()","18501cc2":"print(train_data[\"Source\"].nunique())\nprint(train_data[\"Source\"].unique())","5665dce0":"train_data[\"Source\"].value_counts()","ef5211bd":"# Plotting the Source vs Price\n\n\nsns.catplot(y=\"Price\", x=\"Source\", data=train_data.sort_values(\"Price\", ascending=False), kind=\"boxen\", height=7, aspect = 4 )","f547a0a7":"# Since Source is also Numerical we will do OneHotEncodding\n\nSource = train_data[[\"Source\"]]\n\nSource = pd.get_dummies(Source, drop_first = True)\n\nSource.head()","9e435291":"# Printing and Counting the number of unique values in the Destination Columns\nprint(train_data[\"Destination\"].nunique())\nprint(train_data[\"Destination\"].unique())","420bc21d":"train_data[\"Destination\"].value_counts()","253979bc":"Destination = train_data[[\"Destination\"]]\n\nDestination = pd.get_dummies(Destination, drop_first=True)\n\nDestination.head()","d39455be":"# Route Column\n\ntrain_data[\"Route\"]","86647512":"train_data[\"Additional_Info\"].nunique()","bf089b92":"train_data[\"Additional_Info\"].value_counts()","054d7788":"# Getting the percentage of \"No Info\" values in the \"Additional_Info\" column\n(train_data[\"Additional_Info\"] == \"No info\").value_counts()\/(train_data.shape[0])","cbdbfbb2":"train_data[\"Total_Stops\"].value_counts()","557a3f72":"# Additional Info contains almost 80% \"No info\"\n# Route and Total_Stops are related to each other\n\n# So dropping the \"Route and Additional_Info\" columns\n\ntrain_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)","b6edbc5e":"# Since the \"Total_Stops\" column is ordinal we will use LabelEncoder\n\ntrain_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)","c2377fb4":"train_data.head()","67c6d5f0":"data_train = pd.concat([train_data, Airline, Source, Destination], axis = 1)","11acb0f1":"data_train.head()","0cf1149d":"data_train.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)","bf929070":"data_train.head()","cf316ca9":"data_train.shape","17f8ca55":"test_data = pd.read_excel(r\"\/kaggle\/input\/flight-fare-prediction-mh\/Test_set.xlsx\")","24d64f33":"test_data.head()","4c109634":"# # Preprocessing\n\n# print(\"test data Info\")\n# print(\"_\"*75)\n# print(test_data.info())\n\n# print()\n# print()\n\n# train_data.dropna(inplace = True)\n\n\n\n# train_data[\"Journey_day\"] = pd.to_datetime(train_data[\"Date_of_Journey\"], format=\"%d\/%m\/%Y\").dt.day\n# train_data[\"Journey_month\"] = pd.to_datetime(train_data[\"Date_of_Journey\"], format=\"%d\/%m\/%Y\").dt.month\n# # Droping the Date_of_Journey Column\n\n# train_data.drop([\"Date_of_Journey\"], axis = 1, inplace = True)\n\n# # Extracting Hours\n\n# train_data[\"Dep_Hour\"] = pd.to_datetime(train_data[\"Dep_Time\"]).dt.hour\n\n# train_data[\"Dep_min\"] = pd.to_datetime(train_data[\"Dep_Time\"]).dt.minute\n\n# # Dropping the Dep_Time column from the data\n\n# train_data.drop([\"Dep_Time\"], axis = 1, inplace=True)\n\n# train_data[\"Arrival_Hour\"] = pd.to_datetime(train_data[\"Arrival_Time\"]).dt.hour\n\n# train_data[\"Arrival_min\"] = pd.to_datetime(train_data[\"Arrival_Time\"]).dt.minute\n\n# # Dropping the Arrival_Time column\n\n# train_data.drop([\"Arrival_Time\"], axis = 1, inplace=True)\n\n# # Assigning and converting Duration Column into list\n# duration = list(train_data[\"Duration\"])\n\n# for i in range(len(duration)):\n#     if len(duration[i].split()) != 2:   # to check whether it contains the both hour and minutes\n#         if \"h\" in duration[i]:\n#             duration[i] = duration[i].strip() + \" 0m\"  # If duration is having hour only adding the minute value\n#         else:\n#             duration[i] = \"0h \" + duration[i]     # If duration is having minute only adding the hour value\n            \n# duration_hours = []\n# duration_mins = []\n\n# for i in range(len(duration)):\n#     duration_hours.append(int(duration[i].split(sep=\"h\")[0]))\n#     duration_mins.append(int(duration[i].split(sep=\"m\")[0].split()[-1]))\n    \n# # Adding the Duration Hours and Duration Mins Columns in the Datasets\n\n# train_data[\"Duration_hours\"] = duration_hours\n# train_data[\"Duration_mins\"] = duration_mins\n\n\n# train_data.drop([\"Duration\"], axis = 1, inplace = True)\n\n# sns.catplot(y=\"Price\", x=\"Airline\", data=train_data.sort_values(\"Price\", ascending=False), kind=\"boxen\", height=6, aspect=3)\n# plt.show()\n\n# # Since Airline is Nominal Categorical data we will perform OneHotEncoding\n\n# Airline = train_data[[\"Airline\"]]\n\n# Airline = pd.get_dummies(Airline, drop_first=True)\n\n# Airline.head()\n\n# # Plotting the Source vs Price\n\n\n# sns.catplot(y=\"Price\", x=\"Source\", data=train_data.sort_values(\"Price\", ascending=False), kind=\"boxen\", height=7, aspect = 4 )\n\n# # Since Source is also Numerical we will do OneHotEncodding\n\n# Source = train_data[[\"Source\"]]\n\n# Source = pd.get_dummies(Source, drop_first = True)\n\n# Source.head()\n\n# Destination = train_data[[\"Destination\"]]\n\n# Destination = pd.get_dummies(Destination, drop_first=True)\n\n# Destination.head()\n\n# # Additional Info contains almost 80% \"No info\"\n# # Route and Total_Stops are related to each other\n\n# # So dropping the \"Route and Additional_Info\" columns\n\n# train_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)\n\n# # Since the \"Total_Stops\" column is ordinal we will use LabelEncoder\n\n# train_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\n\n# data_train = pd.concat([train_data, Airline, Source, Destination], axis = 1)\n\n# data_train.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)","b41b0f62":"# Preprocessing\n\nprint(\"Test data Info\")\nprint(\"-\"*75)\nprint(test_data.info())\n\nprint()\nprint()\n\nprint(\"Null values :\")\nprint(\"-\"*75)\ntest_data.dropna(inplace = True)\nprint(test_data.isnull().sum())\n\n# EDA\n\n# Date_of_Journey\ntest_data[\"Journey_day\"] = pd.to_datetime(test_data.Date_of_Journey, format=\"%d\/%m\/%Y\").dt.day\ntest_data[\"Journey_month\"] = pd.to_datetime(test_data[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.month\ntest_data.drop([\"Date_of_Journey\"], axis = 1, inplace = True)\n\n# Dep_Time\ntest_data[\"Dep_hour\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.hour\ntest_data[\"Dep_min\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.minute\ntest_data.drop([\"Dep_Time\"], axis = 1, inplace = True)\n\n# Arrival_Time\ntest_data[\"Arrival_hour\"] = pd.to_datetime(test_data.Arrival_Time).dt.hour\ntest_data[\"Arrival_min\"] = pd.to_datetime(test_data.Arrival_Time).dt.minute\ntest_data.drop([\"Arrival_Time\"], axis = 1, inplace = True)\n\n# Duration\nduration = list(test_data[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration\n\n# Adding Duration column to test set\ntest_data[\"Duration_hours\"] = duration_hours\ntest_data[\"Duration_mins\"] = duration_mins\ntest_data.drop([\"Duration\"], axis = 1, inplace = True)\n\n\n# Categorical data\n\nprint(\"Airline\")\nprint(\"-\"*75)\nprint(test_data[\"Airline\"].value_counts())\nAirline = pd.get_dummies(test_data[\"Airline\"], drop_first= True)\n\nprint()\n\nprint(\"Source\")\nprint(\"-\"*75)\nprint(test_data[\"Source\"].value_counts())\nSource = pd.get_dummies(test_data[\"Source\"], drop_first= True)\n\nprint()\n\nprint(\"Destination\")\nprint(\"-\"*75)\nprint(test_data[\"Destination\"].value_counts())\nDestination = pd.get_dummies(test_data[\"Destination\"], drop_first = True)\n\n# Additional_Info contains\n\n#almost 80% no_info\n# Route and Total_Stops are related to each other\ntest_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)\n\n# Replacing Total_Stops\ntest_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\n\n# Concatenate dataframe --> test_data + Airline + Source + Destination\ndata_test = pd.concat([test_data, Airline, Source, Destination], axis = 1)\n\ndata_test.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)\n\nprint()\nprint()\n\nprint(\"Shape of test data : \", data_test.shape)","a1bacee0":"data_test.head()","6a4177c5":"data_train.shape","2ed33e1b":"data_train.head()","005adb0e":"data_train.columns","1ad166b9":"X = data_train.loc[:, ['Total_Stops', 'Price', 'Journey_day', 'Journey_month', 'Dep_Hour',\n       'Dep_min', 'Arrival_Hour', 'Arrival_min', 'Duration_hours',\n       'Duration_mins', 'Airline_Air India', 'Airline_GoAir', 'Airline_IndiGo',\n       'Airline_Jet Airways', 'Airline_Jet Airways Business',\n       'Airline_Multiple carriers',\n       'Airline_Multiple carriers Premium economy', 'Airline_SpiceJet',\n       'Airline_Trujet', 'Airline_Vistara', 'Airline_Vistara Premium economy',\n       'Source_Chennai', 'Source_Delhi', 'Source_Kolkata', 'Source_Mumbai',\n       'Destination_Cochin', 'Destination_Delhi', 'Destination_Hyderabad',\n       'Destination_Kolkata', 'Destination_New Delhi']]\n\nX.head()","f58932c1":"y = data_train.iloc[:, 1]\ny.head()","d24f7c9e":"# Heatmap\n\nplt.figure(figsize=(18,18))\n\nsns.heatmap(train_data.corr(), annot = True, cmap=\"RdYlGn\")\n\nplt.show()","d81285cc":"# Important feature using ExtraTreeRegressor\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nselection = ExtraTreesRegressor()\nselection.fit(X,y)","21d1c42a":"print(selection.feature_importances_)","4f542765":"#plot graph of feature importances for better visualization\n\nplt.figure(figsize=(12,8))\nfeat_importances = pd.Series(selection.feature_importances_, index = X.columns)\n\nfeat_importances.nlargest(20).plot(kind=\"barh\")\n\nplt.show()","ce096d80":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","24a7b34a":"from sklearn.ensemble import RandomForestRegressor\nreg_rf = RandomForestRegressor()\nreg_rf.fit(X_train, y_train)","ef1d5012":"y_pred = reg_rf.predict(X_test)","a37c9839":"reg_rf.score(X_train, y_train)","f5f4e5e0":"reg_rf.score(X_test, y_test)","b45eaa80":"sns.distplot(y_test - y_pred)\nplt.show()","3e0700d6":"plt.scatter(y_test, y_pred, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","fd08896a":"from sklearn import metrics","72c84497":"print(\"MAE:\", metrics.mean_absolute_error(y_test, y_pred))\nprint(\"MSE:\", metrics.mean_squared_error(y_test, y_pred))\nprint(\"RMSE:\", np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","d6f5cd86":"metrics.r2_score(y_test, y_pred)","a3fd2e9d":"from sklearn.model_selection import RandomizedSearchCV","9c99b18d":"#Randomized Search CV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","2379b6ac":"# Create the random grid\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}","b0e9507f":"rf_random = RandomizedSearchCV(estimator = reg_rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)","7cf4a2ba":"rf_random.fit(X_train,y_train)","787dcf1b":"rf_random.best_params_","0d0aceba":"prediction = rf_random.predict(X_test)","4ceafbd5":"plt.figure(figsize = (8,8))\nsns.distplot(y_test-prediction)\nplt.show()","b95b4a5b":"plt.figure(figsize=(8,8))\nplt.scatter(y_test, prediction, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","d2749054":"print(\"MAE:\", metrics.mean_absolute_error(y_test, prediction))\nprint(\"MSE:\", metrics.mean_squared_error(y_test, prediction))\nprint(\"RMSE:\", np.sqrt(metrics.mean_squared_error(y_test, prediction)))","bd14f734":"import pickle\n\n# Open a file, where you want to store the data\nfile = open('flight_rf.pkl', 'wb')\n\n# pickle.dump(reg_rf, file)\n\npickle.dump(rf_random, file)","ba8e30e3":"model = open('flight_rf.pkl', 'rb')\nforest = pickle.load(model)","814a00e8":"y_prediction = forest.predict(X_test)\n","43733c6f":"metrics.r2_score(y_test, y_prediction)","f4e2f196":"From above we can see that there are 12 functional airlines","9772cdec":"There is 5 unique values Source in the Dataset","9590c7fa":"# Save the model ","e5275f3b":"From the above we can see that there is no null values\n","f946cd41":"# Hypertuning","d00f7740":"There are 368 unique values in the Duration Column","c96665c9":"Price is only numerical here and its dependent column and remaining other columns is independent","7ea3dc09":"# Exploratory Data Analysis","46de7425":"From above we see that Jet Airways and IndiGo have the larget share of the filghts\n\nWhile Trujet, Vistara Premium economy and Jet Airways Business have very less or neglegible share","488b7c54":"# Handling Categorical Data","ba0db1b3":"# Fitting Model Using Random Forest","76e39dc9":"There is only missing values in the Route and Additional_Info columns","c87ea5d4":"# Feature Selection"}}