{"cell_type":{"309326a9":"code","5f12aaa0":"code","477662d9":"code","7e58ed5c":"code","f5fb1674":"code","e85bf82d":"code","dd90d1f4":"code","2c21ab23":"code","1b8e2948":"code","0253c4cd":"code","b89fdd03":"code","e945550d":"code","7f0cf6e4":"code","54d57c1c":"code","01eaea2f":"code","a6db0baa":"code","aa36dc06":"code","26464e5d":"code","25f4f9ec":"code","e2770d1b":"code","d0119ce3":"code","a22ff0de":"code","03d10c7d":"code","3c051147":"code","829cb40d":"code","f9d770fd":"code","41bf1cf4":"code","84659af7":"code","7c8d4213":"code","007cdfa6":"code","5ae03b88":"code","5382ef86":"code","8d8be067":"code","d42890c9":"code","99ed00e1":"code","db59ffa0":"code","cfeb8da7":"markdown","997dab48":"markdown"},"source":{"309326a9":"import matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom textblob import TextBlob\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nimport math\nimport seaborn as sns\nfrom sklearn.feature_selection import SelectKBest, f_regression,f_classif,chi2\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.tree import DecisionTreeClassifier\nimport string\nfrom sklearn.impute import SimpleImputer, KNNImputer\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom sklearn.compose import ColumnTransformer\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler,PowerTransformer,QuantileTransformer,Normalizer,RobustScaler,StandardScaler,MaxAbsScaler\nfrom keras.preprocessing.text import Tokenizer\nimport numpy as np \nimport pandas as pd \nimport os\nfrom sklearn.utils import shuffle\nimport string\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.layers import Activation, Dense, Embedding\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers","5f12aaa0":"import tensorflow as tf\ntf.__version__ # newest version","477662d9":"data=pd.read_csv(\"..\/input\/iba-ml1-final-project\/train.csv\")\n\ndata=data.drop(['Id'], axis=1)\ndata2=pd.read_csv(\"..\/input\/iba-ml1-final-project\/test.csv\")\nidcon=data2['Id']\ndata2=data2.drop(['Id'], axis=1)","7e58ed5c":"data['Review_Title']=data['Review_Title'].replace(np.nan, 'Gorgeous', regex=True)\ndata['Review']=data['Review'].replace(np.nan, 'Love this top! the quality is magnificent', regex=True)\ndata2['Review_Title']=data2['Review_Title'].replace(np.nan, 'Gorgeous', regex=True)\ndata2['Review']=data2['Review'].replace(np.nan, 'Love this top! the quality is magnificent', regex=True)","f5fb1674":"!pip install contractions\nimport contractions\ndata[\"Review\"]=data[\"Review\"].apply(lambda x: contractions.fix(x))\ndata[\"Review_Title\"]=data[\"Review_Title\"].apply(lambda x: contractions.fix(x))\ndata2[\"Review\"]=data2[\"Review\"].apply(lambda x: contractions.fix(x))\ndata2[\"Review_Title\"]=data2[\"Review_Title\"].apply(lambda x: contractions.fix(x))","e85bf82d":"# from nltk.corpus import stopwords\n# stop = stopwords.words('english')\n\n# data['Review'] = data['Review'].str.replace('[{}]'.format(string.punctuation), '',regex=True)\n# data['Review_Title'] = data['Review_Title'].str.replace('[{}]'.format(string.punctuation), '',regex=True)\n# data['Review'] = data['Review'].str.lower()\n# data['Review_Title'] = data['Review_Title'].str.lower()\n# data['Review'] = data['Review'].str.replace('[^\\w\\s]','',regex=True)\n# data['Review_Title'] = data['Review_Title'].str.replace('[^\\w\\s]','',regex=True)\n# data['Review'] =data['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n# data['Review_Title'] =data['Review_Title'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n# data['Review'] = data['Review'].str.replace('\\d+', '',regex=True)\n# data['Review_Title'] = data['Review_Title'].str.replace('\\d+', '',regex=True)","dd90d1f4":"# from nltk.corpus import stopwords\n# stop = stopwords.words('english')\n\n# data2['Review'] = data2['Review'].str.replace('[{}]'.format(string.punctuation), '',regex=True)\n# data2['Review_Title'] = data2['Review_Title'].str.replace('[{}]'.format(string.punctuation), '',regex=True)\n# data2['Review'] = data2['Review'].str.lower()\n# data2['Review_Title'] = data2['Review_Title'].str.lower()\n# data2['Review'] = data2['Review'].str.replace('[^\\w\\s]','',regex=True)\n# data2['Review_Title'] = data2['Review_Title'].str.replace('[^\\w\\s]','',regex=True)\n# data2['Review'] =data2['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n# data2['Review_Title'] =data2['Review_Title'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n# data2['Review'] = data2['Review'].str.replace('\\d+', '',regex=True)\n# data2['Review_Title'] = data2['Review_Title'].str.replace('\\d+', '',regex=True)","2c21ab23":"def get_average_word_len(x):\n    words=x.split()\n    word_len=0\n    if len(x.split())==0:\n        return 0\n        \n    for word in words:\n        word_len=word_len+len(word)\n       \n    return word_len\/len(words)\n","1b8e2948":"data['polarity']= data[\"Review\"].apply(lambda x: (TextBlob(x).sentiment.polarity) )\ndata['rewiew_lenght']=data[\"Review\"].apply(lambda x:len(x))\ndata['word_count']=data[\"Review\"].apply(lambda x:len(x.split()))\ndata['avr_word_len']=data[\"Review\"].apply(lambda x: get_average_word_len(x))","0253c4cd":"data2['polarity']= data2[\"Review\"].apply(lambda x: (TextBlob(x).sentiment.polarity) )\ndata2['rewiew_lenght']=data2[\"Review\"].apply(lambda x:len(x))\ndata2['word_count']=data2[\"Review\"].apply(lambda x:len(x.split()))\ndata2['avr_word_len']=data2[\"Review\"].apply(lambda x: get_average_word_len(x))","b89fdd03":"X=data.drop(['Rating', 'Recommended'], axis=1)\ny=data.drop(['Age', 'Review_Title','Review','Pos_Feedback_Cnt','Division','Department','Product_Category','polarity','rewiew_lenght','word_count','avr_word_len'], axis=1)","e945550d":"y_train100 = np.zeros((y.shape[0], 5))\ny_train100[np.arange(y_train100.shape[0]), y['Rating']-1] = 1\ny_train100 = pd.DataFrame(y_train100,columns=['1','2','3','4','5'],dtype='int64')\ny_train200=y.drop(['Rating'], axis=1)","7f0cf6e4":"y_train100","54d57c1c":"y_train200","01eaea2f":"X_train, X_test, y_train, y_test = train_test_split(X, y ,shuffle=True,stratify=y,test_size=0.33,random_state=43)","a6db0baa":"numeric_preprocessing1 = Pipeline(steps=[\n        ('imputation', KNNImputer(n_neighbors=9)),\n        ('scaling',  RobustScaler())\n    ])\nnumeric_preprocessing2 = Pipeline(steps=[\n        ('impute', SimpleImputer(strategy='most_frequent')),\n        ('one_hot_encoding', OneHotEncoder(handle_unknown='ignore'))\n    \n   \n    ])\npreprocessing = ColumnTransformer(transformers=[\n     \n        ('numeric', numeric_preprocessing1, [\n 'Age',\n 'Pos_Feedback_Cnt','polarity','rewiew_lenght','word_count','avr_word_len']),\n    \n            ('cat', numeric_preprocessing2, ['Division','Department','Product_Category'])\n    ])\n\nmodel_pipeline4 = Pipeline(steps=[\n        ('preprocessing', preprocessing)\n    ])","aa36dc06":"L=model_pipeline4.fit_transform(X_train)\nL=pd.DataFrame.sparse.from_spmatrix(L)\nL2=model_pipeline4.transform(X_test)\nL2=pd.DataFrame.sparse.from_spmatrix(L2)","26464e5d":"\ny_train2 = np.zeros((y_train.shape[0], 5))\ny_train2[np.arange(y_train.shape[0]), y_train['Rating']-1] = 1\ny_test2 = np.zeros((y_test.shape[0], 5))\ny_test2[np.arange(y_test.shape[0]), y_test['Rating']-1] = 1\ny_train2 = pd.DataFrame(y_train2,columns=['1','2','3','4','5'],dtype='int64')\ny_test2 = pd.DataFrame(y_test2,columns=['1','2','3','4','5'],dtype='int64')\ny_train=y_train.drop(['Rating'], axis=1)\ny_test=y_test.drop(['Rating'], axis=1)","25f4f9ec":"max_features = 50000\ntokenizer3 = Tokenizer(num_words=max_features)\ntokenizer3.fit_on_texts(data['Review_Title']+data['Review'])\ntokenized_train3 = tokenizer3.texts_to_sequences(data['Review_Title']+data['Review'])\nvocab3 = pad_sequences(tokenized_train3, maxlen=250)\n","e2770d1b":"initializer = tf.keras.initializers.GlorotNormal()","d0119ce3":"\n\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 128 ))\nmodel.add(Bidirectional(LSTM(16)))\nmodel.add(Dense(32, activation=\"relu\"))\nmodel.add(Dense(5, activation=\"softmax\"))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n\nmodel.fit(vocab3,y_train100,  epochs=4, validation_split=0.2)","a22ff0de":"\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 128 ))\nmodel.add(Bidirectional(LSTM(16)))\nmodel.add(Dense(32, activation=\"relu\"))\nmodel.add(Dense(1, activation=\"sigmoid\"))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n\nmodel.fit(vocab3,y_train200,  epochs=4, validation_split=0.2)","03d10c7d":"vocab_data=(data['Review_Title']+ ' ' +data['Review']).to_string()\n\nvocab_data2=(data['Review_Title']).to_string()\n\nvocab_data3=(data['Review']).to_string()","3c051147":"from nltk.tokenize import TweetTokenizer\ntknzr = TweetTokenizer()\nvocab_data=tknzr.tokenize(vocab_data)\nvocab_data2=tknzr.tokenize(vocab_data2)\nvocab_data3=tknzr.tokenize(vocab_data3)\nVOCAB_SIZE = 50000\nVOCAB_SIZE2 = 10000\nVOCAB_SIZE3 = 45000\nencoder = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=VOCAB_SIZE)\nencoder2 = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=VOCAB_SIZE2)\nencoder3 = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=VOCAB_SIZE3)\nencoder.adapt(\n    vocab_data, reset_state=True\n)\nencoder2.adapt(\n    vocab_data2, reset_state=True\n)\nencoder3.adapt(\n    vocab_data3, reset_state=True\n)","829cb40d":"X_train222=X_train['Review_Title']\nX_test222=X_test['Review_Title']\nX_train333=X_train['Review']\nX_test333=X_test['Review']\nX_train=X_train['Review_Title']+' '+X_train['Review']\nX_test=X_test['Review_Title']+' '+X_test['Review']","f9d770fd":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n# image_input1 = keras.Input(shape=(None,L.shape[1]))\n\n# x5 = layers.Dense(L.shape[1], activation=layers.LeakyReLU(),kernel_initializer=initializer)(image_input1)\n# x5 = layers.Dense(L.shape[1]*2, activation=layers.LeakyReLU(),kernel_initializer=initializer)(x5)\n# x5 = layers.Dense(L.shape[1], activation=layers.LeakyReLU(),kernel_initializer=initializer)(x5)\nimage_input = keras.Input(shape=(),dtype=tf.string)\nimage_input2 = keras.Input(shape=(),dtype=tf.string)\nimage_input3 = keras.Input(shape=(),dtype=tf.string)\n\n\nemb=encoder(image_input)\nemb = layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()), \n        output_dim=64,\n        mask_zero=True\n    )(emb)\nx1 = layers.Bidirectional(LSTM(32,kernel_initializer=initializer))(emb)\nx1 = layers.Dense(64, activation=layers.LeakyReLU(),kernel_initializer=initializer)(x1)\nx11 = layers.GRU(256)(emb)\nx11 = layers.Dense(64, activation=layers.LeakyReLU(),kernel_initializer=initializer)(x11)\n\nemb2=encoder2(image_input2)\nemb2 = layers.Embedding(\n        input_dim=len(encoder2.get_vocabulary()), \n        output_dim=64,\n        mask_zero=True\n    )(emb2)\nx2 = layers.Bidirectional(LSTM(32,kernel_initializer=initializer))(emb2)\nx2 = layers.Dense(16, activation=layers.LeakyReLU(),kernel_initializer=initializer)(x2)\nx22 = layers.GRU(256)(emb2)\nx22 = layers.Dense(64, activation=layers.LeakyReLU(),kernel_initializer=initializer)(x22)\nemb3=encoder3(image_input3)\nemb3 = layers.Embedding(\n        input_dim=len(encoder3.get_vocabulary()), \n        output_dim=64,\n        mask_zero=True\n    )(emb3)\nx3 = layers.Bidirectional(LSTM(32,kernel_initializer=initializer))(emb3)\nx3 = layers.Dense(64, activation=layers.LeakyReLU(),kernel_initializer=initializer)(x3)\nx33 = layers.GRU(256)(emb3)\nx33 = layers.Dense(64, activation=layers.LeakyReLU(),kernel_initializer=initializer)(x33)\n\n\n#x5=tf.reshape(x5, [ -1 ,L.shape[1]])\n\nx = layers.concatenate([x1,x11,x2,x22,x3,x33])\n\n\nscore_output = layers.Dense(1,activation='sigmoid', name=\"score_output\",kernel_initializer=initializer)(x)\nclass_output = layers.Dense(5,activation='softmax', name=\"class_output\",kernel_initializer=initializer)(x)\n\nmodel = keras.Model(\n    inputs=[image_input,image_input2,image_input3], outputs=[score_output,class_output]\n)","41bf1cf4":"model.compile(\n    optimizer=tf.keras.optimizers.Adam(\n    learning_rate=0.00016, beta_1=0.8, beta_2=0.888\n),\n    loss=['binary_crossentropy','categorical_crossentropy'],\n    metrics=['accuracy'])","84659af7":"early_stopping = EarlyStopping(patience=3,restore_best_weights=True,monitor='val_class_output_accuracy')\nmy_callbacks = [\n    early_stopping\n]\ntrain_history = model.fit([X_train,X_train222,X_train333], [y_train,y_train2], epochs=20, validation_data=([X_test,X_test222,X_test333], [y_test,y_test2]),callbacks=[my_callbacks])","7c8d4213":"model.evaluate([X_test,X_test222,X_test333], [y_test,y_test2])","007cdfa6":"a=data2['Review_Title']+data2['Review']\nb=data2['Review_Title']\nc=data2['Review']\nm=model.predict([a,b,c])","5ae03b88":"keras.utils.plot_model(model,show_shapes=True)","5382ef86":"plt.figure(figsize=(12, 6))\nplt.plot(train_history.history['score_output_loss'], label='Train loss')\nplt.plot(train_history.history['val_score_output_loss'], label='Validation loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","8d8be067":"plt.figure(figsize=(12, 6))\nplt.plot(train_history.history['class_output_loss'], label='Train loss')\nplt.plot(train_history.history['val_class_output_loss'], label='Validation loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","d42890c9":"plt.figure(figsize=(12, 6))\nplt.plot(train_history.history['class_output_accuracy'], label='Train Accuracy')\nplt.plot(train_history.history['val_class_output_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","99ed00e1":"plt.figure(figsize=(12, 6))\nplt.plot(train_history.history['score_output_accuracy'], label='Train Accuracy')\nplt.plot(train_history.history['val_score_output_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","db59ffa0":"z=m[0]\nr=m[1]\ndata2=pd.read_csv(\"..\/input\/iba-ml1-final-project\/test.csv\")\nidcon=data2['Id']\n\nr2=np.zeros((r.shape[0],1),dtype='int64')\nfor i in range(r.shape[0]):\n    r2[i]=r[i].argmax()+1\nz=z.round()\nz=z.astype('int64')\ndf = pd.DataFrame(r2,columns=['Rating'], index=idcon,dtype='int64')\n\n\n\ndf['Recommended'] = z\ndf.to_csv('submission\u2116777.csv')","cfeb8da7":"## My first main model that consists of word2vec embedding separetaly on review, review_title and their addition, I tried to use other columns of dataset using my pipeline but results were better without it so this part is commented this model had score of 69.866 on public leaderboard","997dab48":"## Bag of words separetaly for reccomended and rating "}}