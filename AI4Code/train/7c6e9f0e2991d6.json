{"cell_type":{"b2b2fac9":"code","690d6ef8":"code","8583b2f8":"code","1936b651":"code","df4a10a8":"code","8d33a686":"code","35620427":"code","1765d720":"code","1be9d9b7":"code","6723430e":"code","d411fa60":"code","e12a2940":"code","2de02403":"code","59f8b8cf":"code","2e913655":"code","86b1661d":"code","6daef57d":"code","4a1eab13":"code","17a89191":"code","aea2e2e1":"code","04bafcd2":"code","0af7711f":"code","106e0d52":"code","8231bc16":"code","1955d2c7":"code","7b43bfde":"code","99ba3895":"code","51ad26a3":"code","3955446d":"code","547ff153":"code","a5aeedb7":"code","2f8919ac":"code","a180eeb3":"code","75b11ecc":"code","4b3f089e":"code","e08ffdb7":"markdown","89407cde":"markdown","ab142198":"markdown","2568c781":"markdown","85049cf5":"markdown","0f442043":"markdown","050a87cd":"markdown","6fab2651":"markdown","f4dd8875":"markdown","5fb638d2":"markdown","b692f514":"markdown","3af5cc5e":"markdown"},"source":{"b2b2fac9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","690d6ef8":"# import all libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport re\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import scale\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\n\n\nimport warnings # supress warnings\nwarnings.filterwarnings('ignore')","8583b2f8":"# import train.csv\ndf = pd.read_csv(\"..\/input\/train.csv\")","1936b651":"#display the first ten columns of the dataset to get a feel of the dataset.\n\ndf.head(10)","df4a10a8":"# summary of the dataset: 1460 rows, 81 columns, some have null values\nprint(df.info())","8d33a686":"df.describe()","35620427":"# plotting correlations on a heatmap\n\n# figure size\nplt.figure(figsize=(25,15))\n\n# heatmap\n\ncorr= df.corr()\nsns.heatmap(corr, cmap=\"YlGnBu\", annot=True)\nplt.show()","1765d720":"# paiwise scatter plot\n\nplt.figure(figsize=(20, 10))\nsns.pairplot(df)\nplt.show()","1be9d9b7":"# scatter plot\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(df[cols])\nplt.show()","6723430e":"# dependent variable is  salesprice so separating dependent and independent variable\ndependent_variable = df['SalePrice']\ndependent_variable.head()","d411fa60":"# check the distribution of target variable\nsns.distplot(dependent_variable,hist=True)","e12a2940":"# visualise area-price relationship\nsns.regplot(x=\"OverallQual\", y=\"SalePrice\", data=df, fit_reg=False)","2de02403":"sns.regplot(x=\"GrLivArea\", y=\"SalePrice\", data=df, fit_reg=False)","59f8b8cf":"sns.regplot(x=\"GarageArea\", y=\"SalePrice\", data=df, fit_reg=False)","2e913655":"sns.regplot(x=\"TotalBsmtSF\", y=\"SalePrice\", data=df, fit_reg=False)","86b1661d":"plt.subplots(figsize=(16,10))\nplt.xticks(rotation='90')\nsns.boxplot(x=df['YearBuilt'], y=df['SalePrice'])","6daef57d":"df= df.fillna(\"NO\")","4a1eab13":"# all numeric (float and int) variables in the dataset\ndf_numeric = df.select_dtypes(include=['float64', 'int64'])\ndf_numeric.head()","17a89191":"# dropping MSSSubClass and ID \ndf_numeric = df_numeric.drop(['MSSubClass','Id','OverallQual','OverallCond'], axis=1)\ndf_numeric.head()","aea2e2e1":"# variable formats\ndf.info()","04bafcd2":"# converting those variables categorical\ndf['MSSubClass'] = df['MSSubClass'].astype('object')\ndf['OverallQual'] = df['OverallQual'].astype('object')\ndf['OverallCond'] = df['OverallCond'].astype('object')\ndf.info()","0af7711f":"df = df.drop('Id', axis=1)","106e0d52":"df.isnull().sum().sort_values(ascending = False).head(100)\n# checking NA\n# there are no missing values in the dataset\ndf.isnull().values.any()\n#df.columns","8231bc16":"# split into X and y\nX= df.iloc[ : , :-1]\n\ny = df['SalePrice']","1955d2c7":"# creating dummy variables for categorical variables\n\n# subset all categorical variables\ndf_categorical = X.select_dtypes(include=['object'])\ndf_categorical.head()","7b43bfde":"# convert into dummies\ndf_dummies = pd.get_dummies(df_categorical, drop_first=True)\ndf_dummies.head()","99ba3895":"# drop categorical variables \nX = X.drop(list(df_categorical.columns), axis=1)","51ad26a3":"# concat dummy variables with X\nX = pd.concat([X, df_dummies], axis=1)","3955446d":"# scaling the features\nfrom sklearn.preprocessing import scale\n\n# storing column names in cols, since column names are (annoyingly) lost after \n# scaling (the df is converted to a numpy array)\ncols = X.columns\nX = pd.DataFrame(scale(X))\nX.columns = cols\nX.columns","547ff153":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    train_size=0.7,\n                                                    test_size = 0.3, random_state=100)","a5aeedb7":"from sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nimport os\n\n\n\n# list of alphas to tune\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000,2000,5000,8000]}\n\n\nridge = Ridge()\n\n# cross validation\nfolds = 5\nmodel_cv = GridSearchCV(estimator = ridge, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel_cv.fit(X_train, y_train) ","2f8919ac":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results = cv_results[cv_results['param_alpha']<=8000]\ncv_results","a180eeb3":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","75b11ecc":"alpha = 500\nridge = Ridge(alpha=alpha)\n\nridge.fit(X_train, y_train)\nridge.coef_","4b3f089e":"# model with optimal alpha\n# lasso regression\nlm = Ridge(alpha=500)\nlm.fit(X_train, y_train)\n\n# predict\ny_train_pred = lm.predict(X_train)\nprint(metrics.r2_score(y_true=y_train, y_pred=y_train_pred))\ny_test_pred = lm.predict(X_test)\nprint(metrics.r2_score(y_true=y_test, y_pred=y_test_pred))","e08ffdb7":"This is a plot of the Target variable against the TotalBsmtSF Total square feet of basement area. From the plot we can tell that 'SalePrice' and 'TotalBsmtSF' have a strong linear relationship\n","89407cde":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a plot of the Target variable against the GarageArea Total square feet of Garage area. From the plot we can tell that 'SalePrice' and 'GarageArea' have a strong linear relationship\n","ab142198":"some predictors are strongly correlated to each other thus causing multicollinearity. \"OverallQual\",'GrLivArea','TotalBsmtSF', and '1stFlrSF' and the Garage variablesare strongly correlated to the target variable.\n","2568c781":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere we are plotting the Target varible against the GrLivArea(ground) living area in square feet. From this we can understand that \"GrLivArea\" and \"SalePrice\" has a linear relationship.\n","85049cf5":"\n**4. Model Building and Evaluation**\n\nRidge and Lasso Regression\n\nLet's now try predicting car prices, a dataset used in simple linear regression, to perform ridge and lasso regression.\n\nRidge Regression","0f442043":"\n\n\nThis plot is of the 'YearBuilt' feature against the Sales Price.we can say that 'SalePrice' is more to spend money in new stuff than old.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet us plot the correlation matrix using heatmap to better understand the data.\n","050a87cd":"We can say that Overall quality increases WRT the the house price.","6fab2651":"# US Revenue - House Prices Solution\n\nThe solution is divided into the following sections: \n\u2022Data understanding and exploration\n\u2022Data cleaning\n\u2022Data preparation\n\u2022Model building and evaluation","f4dd8875":"2. Data Cleaning\n\nLet's now conduct some data cleaning steps. \n\nWe've seen that there are no missing values in the dataset. We've also seen that variables are in the correct format, except MSSubClass,OverallQual, OverallCond which should rather be a categorical variable (so that dummy variable are created for the categories).\n\nNote that it can be used in the model as a numeric variable also. \n","5fb638d2":"**3. Data Preparation**\n\nData Preparation\n\nLet's now prepare the data and build the model.\n","b692f514":"**Exploratory Data Analysis**\nTo perform linear regression, the (numeric) target variable should be linearly related to at least one another numeric variable. Let's see whether that's true in this case.\n\nWe'll first subset the list of all (independent) numeric variables, and then make a pairwise plot.\n","3af5cc5e":"\n\n\n\n\n\n **Data Understanding and Exploration**\n\nLet's first have a look at the dataset and understand the size, attribute names etc\n"}}