{"cell_type":{"bb125ce8":"code","2acb9246":"code","8ef7652d":"code","07045b21":"code","8b597a75":"code","484934e7":"code","e5f26fc3":"code","26000188":"code","d5650551":"code","1fa78155":"code","68b25266":"code","2b6ae20e":"code","8520b1ac":"code","a55cb27e":"code","ea28fb13":"code","7b82bbf7":"code","026fbd03":"code","7b77431a":"code","e43c66d8":"code","7ca3048d":"code","d8c5f588":"code","3c71b03a":"code","a2c2ce59":"code","e1ac649b":"code","c059def7":"code","33896dc2":"code","f596dcfc":"code","1d05e005":"code","89d50d4e":"code","45a322b2":"code","66e7cf8e":"code","26a3458d":"code","a28bc309":"code","82d06a3f":"code","6306dd64":"code","b6c54649":"code","c4ec06fb":"code","bde3ad25":"code","f4eb6d8e":"code","aa0adb1c":"code","a8b9234b":"code","61d24797":"code","22020cc0":"code","ebfbf3aa":"code","2457fcb3":"code","41251386":"code","88f07e6c":"code","3d26d9c9":"code","7b06d64b":"code","07e61cdc":"code","13d52be0":"code","d9279af9":"code","ec225d0f":"code","bab06137":"code","8a753386":"code","b01ab61e":"markdown","853c6668":"markdown","836cb9af":"markdown","bed82d56":"markdown","4a168982":"markdown","869d75bb":"markdown","6ee0b96a":"markdown","b71e11db":"markdown","2b83aef0":"markdown","9a3356f7":"markdown","8233d9bb":"markdown","51fec81e":"markdown","b18c8237":"markdown","98fa7087":"markdown","e5f52aa4":"markdown","e73149bd":"markdown","354fab81":"markdown","e9757c1f":"markdown","cfb341fc":"markdown","8c535c76":"markdown","1b9cf5a8":"markdown","2683ad5e":"markdown","0bb66593":"markdown","10c4f42b":"markdown"},"source":{"bb125ce8":"import numpy as np\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom fastai.vision.all import *","2acb9246":"from tensorflow.keras.preprocessing.image import ImageDataGenerator","8ef7652d":"images_folder = ['CBB', 'CBSD', 'CGM', 'CMD', 'Healthy']\n\ntrain_folder_path = '.\/train_images'\nval_folder_path = '.\/val_images'\n\n#Create some folders\nfor img_f in images_folder:\n    os.makedirs(train_folder_path + '\/'+ img_f, exist_ok=True)\n    os.makedirs(val_folder_path + '\/'+ img_f, exist_ok=True)\n    \nprint(\"Train folder:\", os.listdir(train_folder_path))\nprint(\"Validation folder:\", os.listdir(val_folder_path))","07045b21":"dataset_path = Path('..\/input\/cassava-leaf-disease-classification')","8b597a75":"train_df = pd.read_csv(dataset_path\/'train.csv')\ntrain_df.head()","484934e7":"n_picture = []\nfor l in range(5):    \n    n_picture.append(len(train_df[train_df['label']==l]))\nprint(\"Number of pictures:\", n_picture)\nprint(\"Total:\", sum(n_picture))","e5f26fc3":"plt.bar(images_folder, n_picture)","26000188":"# Put the images you want to exclude in the list\navoid_list = []\n\n# train_test_split\ntotal_size = sum(n_picture)\nratio = 0.8\ntotal_train = int(total_size*ratio)\ntotal_val = int(total_size-total_train)\n\nfor l in range(5):\n    file_count = 0\n    i = 0                    \n    file_count = 0\n    \n    # validation data\n    while i < int(n_picture[l]*ratio):\n        file_name = train_df[train_df['label']==l].loc[train_df[train_df['label']==l].index[i]]['image_id']\n        in_path = dataset_path\/'train_images'\/file_name\n        out_path = val_folder_path + '\/' + images_folder[l] + '\/' + file_name\n        shutil.copyfile(in_path, out_path)\n        _, _, files = next(os.walk(val_folder_path + '\/' + images_folder[l]))\n        file_count = len(files)\n        i += 1\n        \n    # train data\n    while i < n_picture[l]:\n        file_name = train_df[train_df['label']==l].loc[train_df[train_df['label']==l].index[i]]['image_id']\n        #Use avoid_list only for train data\n        if not file_name in avoid_list:\n            in_path = dataset_path\/'train_images'\/file_name\n            out_path = train_folder_path + '\/' + images_folder[l] + '\/' + file_name\n            shutil.copyfile(in_path, out_path)\n            _, _, files = next(os.walk(train_folder_path + '\/' + images_folder[l]))\n            file_count = len(files)\n        i += 1","d5650551":"train_image_generator = ImageDataGenerator(rescale=1.\/255,\n                                        rotation_range=360,\n                                        width_shift_range=0.1,\n                                        height_shift_range=0.1,\n                                        horizontal_flip=True,\n                                        vertical_flip=True,\n                                        zoom_range=0.5,\n                                        shear_range=0.2,\n                                        brightness_range=[0.5,1.0],\n                                        channel_shift_range=100,\n                                        fill_mode = 'nearest')","1fa78155":"validation_image_generator = ImageDataGenerator(rescale=1.\/255) # \u691c\u8a3c\u30c7\u30fc\u30bf\u306e\u30b8\u30a7\u30cd\u30ec\u30fc\u30bf","68b25266":"BATCH_SIZE = 8\nepochs = 20\n\nIMG_HEIGHT = 512\nIMG_WIDTH = 512","2b6ae20e":"train_data_gen = train_image_generator.flow_from_directory(batch_size=BATCH_SIZE,\n                                                           directory=train_folder_path,\n                                                           shuffle=True,\n                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n                                                           class_mode='categorical',\n                                                           classes = images_folder)","8520b1ac":"val_data_gen = validation_image_generator.flow_from_directory(batch_size=BATCH_SIZE,\n                                                              directory=val_folder_path,\n                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n                                                              class_mode='categorical',\n                                                              classes = images_folder)","a55cb27e":"print(\"Train generator's label:\\n\", train_data_gen.class_indices)\nprint(\"Validation generator's label:\\n\", val_data_gen.class_indices)","ea28fb13":"# https:\/\/www.tensorflow.org\/tutorials\/images\/classification?hl=ja\ndef plotImages(images_arr):\n    fig, axes = plt.subplots(1, 7, figsize=(20,20))\n    axes = axes.flatten()\n    for img, ax in zip( images_arr, axes):\n        ax.imshow(img)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()","7b82bbf7":"augmented_images = [train_data_gen[0][0][0] for i in range(7)]\nplotImages(augmented_images)\naugmented_images = [train_data_gen[1][0][0] for i in range(7)]\nplotImages(augmented_images)\naugmented_images = [train_data_gen[2][0][0] for i in range(7)]\nplotImages(augmented_images)\naugmented_images = [train_data_gen[3][0][0] for i in range(7)]\nplotImages(augmented_images)\naugmented_images = [train_data_gen[4][0][0] for i in range(7)]\nplotImages(augmented_images)","026fbd03":"import tensorflow as tf\nimport keras\nfrom tensorflow.keras.applications import ResNet152, EfficientNetB3\nfrom tensorflow.keras import models, layers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom keras.optimizers import Adamax\nfrom keras.losses import CategoricalCrossentropy\n","7b77431a":"def modelResNet152():\n    \n    model = models.Sequential()\n    model.add(ResNet152(include_top = False, \n                      weights='imagenet',\n                      #weights = \"..\/input\/resnet152traind\/resnet152_weights_tf_dim_ordering_tf_kernels_notop.h5\",\n                      input_shape=(IMG_HEIGHT,IMG_WIDTH, 3)))\n    \n    model.add(Dropout(0.5))\n    model.add(layers.GlobalAveragePooling2D())\n    \n    #additional\n    model.add(layers.Dense(1024, activation = 'relu'))\n    model.add(Dropout(0.25))\n    \n    model.add(layers.Dense(5, activation = \"softmax\"))\n    \n    return model \n    ","e43c66d8":"def modelEfficientNetB3():\n\n    model = models.Sequential()\n    model.add(EfficientNetB3(include_top = False, \n                            weights = 'imagenet',\n                            #weights = \"..\/input\/effib3trained\/efficientnetb3_notop.h5\",\n                            input_shape=(IMG_HEIGHT,IMG_WIDTH, 3)))\n    \n    model.add(Dropout(0.8))\n    model.add(layers.GlobalAveragePooling2D())\n    \n    #additional\n    model.add(layers.Dense(512, activation = 'relu'))\n    model.add(Dropout(0.4))\n    model.add(layers.Dense(256, activation = 'relu'))\n    model.add(Dropout(0.5))\n \n    model.add(layers.Dense(5, activation = \"softmax\"))\n    \n    return model ","7ca3048d":"model_res152 = modelResNet152()","d8c5f588":"model_effiB3 = modelEfficientNetB3()","3c71b03a":"model_res152.summary()","a2c2ce59":"model_effiB3.summary()","e1ac649b":"model_checkpoint_resnet = ModelCheckpoint(\n                            \".\/checkpoint_resnet.h5\",\n                            monitor = \"val_loss\",\n                            verbose = 1,\n                            save_best_only = True,\n                            save_weights_only = False,\n                            mode = \"min\")","c059def7":"model_checkpoint_effinet = ModelCheckpoint(\n                            \".\/checkpoint_effinet.h5\",\n                            monitor = \"val_loss\",\n                            verbose = 1,\n                            save_best_only = True,\n                            save_weights_only = False,\n                            mode = \"min\")","33896dc2":"early_stop = EarlyStopping(\n                            monitor = \"val_loss\",\n                            min_delta=0.001,\n                            patience=7,\n                            verbose=1,\n                            mode=\"min\",\n                            restore_best_weights=False)","f596dcfc":"reduce_lr = ReduceLROnPlateau(\n                            monitor=\"val_loss\",\n                            factor=0.1,\n                            patience=2,\n                            verbose=1,\n                            mode=\"min\",\n                            min_delta=0.0001)","1d05e005":"model_res152.compile(\n            optimizer=tf.keras.optimizers.Adamax(learning_rate=0.001),\n            loss = CategoricalCrossentropy(label_smoothing=0.3,reduction=\"auto\",name=\"categorical_crossentropy\"),\n            metrics = [\"accuracy\"])","89d50d4e":"model_effiB3.compile(\n            optimizer=tf.keras.optimizers.Adamax(learning_rate=0.001),\n            loss = CategoricalCrossentropy(label_smoothing=0.3,reduction=\"auto\",name=\"categorical_crossentropy\"),\n            metrics = [\"accuracy\"])","45a322b2":"epochs = 15","66e7cf8e":"history_res152 = model_res152.fit_generator(\n                    train_data_gen,\n                    steps_per_epoch=None,\n                    epochs=epochs,\n                    validation_data=val_data_gen,\n                    validation_steps=None,\n                    callbacks = [model_checkpoint_resnet,early_stop,reduce_lr]\n)","26a3458d":"epochs = 10","a28bc309":"history_effiB3 = model_effiB3.fit_generator(\n                    train_data_gen,\n                    steps_per_epoch=None,\n                    epochs=epochs,\n                    validation_data=val_data_gen,\n                    validation_steps=None,\n                    callbacks = [model_checkpoint_effinet,early_stop,reduce_lr]\n)","82d06a3f":"#saving the models\nmodel_res152.save('saved_model_resnet.h5')\nmodel_effiB3.save('saved_model_effinet.h5')","6306dd64":"acc = history_res152.history['accuracy']\nval_acc = history_res152.history['val_accuracy']\n\nloss = history_res152.history['loss']\nval_loss = history_res152.history['val_loss']\n\nepochs_range = range(len(acc))\n\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\nplt.show()\n\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","b6c54649":"acc = history_effiB3.history['accuracy']\nval_acc = history_effiB3.history['val_accuracy']\n\nloss = history_effiB3.history['loss']\nval_loss = history_effiB3.history['val_loss']\n\nepochs_range = range(len(acc))\n\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\nplt.show()\n\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","c4ec06fb":"import numpy as np\nimport os\nimport pandas as pd\nfrom fastai.vision.all import *","bde3ad25":"import tensorflow as tf\nfrom keras.models import load_model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n","f4eb6d8e":"#Choose either\nmodel_res_pred = tf.keras.models.load_model('.\/saved_model_resnet.h5')\n#model_res_pred = tf.keras.models.load_model('.\/checkpoint_resnet.h5')\n\n#Check the architecture\nmodel_res_pred.summary()","aa0adb1c":"#Choose either\nmodel_effi_pred = tf.keras.models.load_model('.\/saved_model_effinet.h5')\n#model_effi_pred = tf.keras.models.load_model('.\/checkpoint_effinet.h5')\n\n#Check the architecture\nmodel_effi_pred.summary()","a8b9234b":"dataset_path = Path('..\/input\/cassava-leaf-disease-classification')","61d24797":"sample_df = pd.read_csv(dataset_path\/'sample_submission.csv')\nsample_df","22020cc0":"test_folder_path = '.\/test_images'\n\nif not os.path.exists(test_folder_path):\n    os.mkdir(test_folder_path) ","ebfbf3aa":"test_ds_path = '..\/input\/cassava-leaf-disease-classification\/test_images'\ntest_dir_path = '.\/test_images\/all_classes'\n\nif not os.path.exists(test_dir_path):\n    shutil.copytree(test_ds_path, test_dir_path)","2457fcb3":"_, _, files = next(os.walk(test_dir_path))\nfile_count = len(files)\n\nprint(\"Number of pictures: \", file_count)\nprint(\"Picture name: \", files)","41251386":"test_image_generator = ImageDataGenerator(rescale=1.\/255)","88f07e6c":"test_folder_path = '.\/test_images'","3d26d9c9":"#IMG_HEIGHT = 512\n#IMG_WIDTH = 512","7b06d64b":"test_generator = test_image_generator.flow_from_directory(\n                                        directory=test_folder_path,\n                                        target_size=(IMG_HEIGHT, IMG_WIDTH),\n                                        class_mode=None,\n                                        shuffle=False\n                                        )","07e61cdc":"pred_res = model_res_pred.predict_generator(test_generator, verbose=1)\nprint(pred_res)","13d52be0":"pred_effi = model_effi_pred.predict_generator(test_generator, verbose=1)\nprint(pred_effi)","d9279af9":"total_pred = pred_res*0.5 + pred_effi*0.5\nprint(\"Ensemble predict:\", total_pred)","ec225d0f":"predicted_class_indices = np.argmax(total_pred, axis=1)\nprint(\"Predicted class indices:\", predicted_class_indices)\n\nlabels_dict = ({'CBB': 0, 'CBSD': 1, 'CGM': 2, 'CMD': 3, 'Healthy': 4})\nlabels = dict((v,k) for k,v in labels_dict.items())\npredictions = [labels[k] for k in predicted_class_indices]\nprint(\"Predicted class:\", predictions)","bab06137":"# Submission dataframe\nsubmit_ID = sample_df.loc[:]['image_id']\nsubmit_TARGET = pd.DataFrame(predicted_class_indices)\n\nsubmission_df = pd.concat([submit_ID, submit_TARGET], axis=1)\nsubmission_df.columns = ['image_id','label']\n\nsubmission_df","8a753386":"submission_df.to_csv('submission.csv',index=False)","b01ab61e":"### Training\nIt takes 6 hours on the GPU.","853c6668":"### Create some folders for ImageDataGenerator\nImageDataGenerator needs to allocate images to each class folder in advance, so we need to create a folder for each class.","836cb9af":"## Cassava Leaf Disease Classification","bed82d56":"### Compile","4a168982":"### Check the number of pictures for each class","869d75bb":"# Preprocessing","6ee0b96a":"### Check the picture","b71e11db":"### Submission","2b83aef0":"### Models\nreference  \nhttps:\/\/www.kaggle.com\/bununtadiresmenmor\/starter-keras-efficientnet","9a3356f7":"# Predict","8233d9bb":"I built an ensemble model using Keras's ResNet152 and EfficientNetB3.  \nThe result was 0.8777, which was not very high, but I will publish the notebook for information sharing.","51fec81e":"### Copy image files to each folder\nDivide the image files into the created folders at a ratio of 8:2 for use with ImageDataGenerator.  \nIf you have some images you don't want to use, add the filename to \u2018avoid_list\u2019.  \nExample: avoid_list = ['1000015157.jpg', '1000201771.jpg']","b18c8237":"Creat a folder for test files","98fa7087":"Check the sample submission file","e5f52aa4":"# Training","e73149bd":"### Load the saved model","354fab81":"### Ensemble\nCombine the output results of ResNet and EfficientNet.","e9757c1f":"### Data augmentation\n","cfb341fc":"I submitted the following content as a Predict Notebook separately from the Training Notebook.  \nIf you want to use the model saved in the training notebook, first upload the saved model from \"+ Add Data\".  \nThen, specify the path of the file in the argument of load_model.","8c535c76":"### Check \"train.csv\"","1b9cf5a8":"Copy the test file together with the folder ","2683ad5e":"### Creating a submit file","0bb66593":"### Callbacks\nI used the following three items for the callback.\n- ModelCheckpoint  \n    If the minimum value of val_loss is updated, it will save the model for each epoch.\n- EarlyStopping  \n     If val_loss cannot be updated for 7 consecutive epochs, training will end.\n- ReduceLROnPlateau  \n     If val_loss cannot be updated for 2 consecutive epochs, the learning rate will be multiplied by 0.1.","10c4f42b":"Thank you."}}