{"cell_type":{"a6b9fb51":"code","4de4d3d6":"code","1006847d":"code","bfc00a1e":"code","d8c73666":"code","49eb294a":"code","2576c91a":"code","5157fa79":"code","248da33b":"code","f8d0c931":"code","f31ec5f4":"code","3ea0bcf3":"code","912e6f17":"code","3fec9586":"markdown","190184ba":"markdown"},"source":{"a6b9fb51":"!pip install meteocalc\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\nfrom tqdm import tqdm_notebook as tqdm\nimport datetime\nfrom meteocalc import feels_like, Temp\nfrom sklearn import metrics\nimport gc\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4de4d3d6":"# Original code from https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage by @gemartin\n\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\n\ndef reduce_mem_usage(df, use_float16=False):\n    \"\"\"\n    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n    \"\"\"\n    \n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype(\"category\")\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","1006847d":"train_df = pd.read_csv('\/kaggle\/input\/ashrae-energy-prediction\/train.csv')\nbuilding_df = pd.read_csv('\/kaggle\/input\/ashrae-energy-prediction\/building_metadata.csv')\nweather_df = pd.read_csv('\/kaggle\/input\/ashrae-energy-prediction\/weather_train.csv')","bfc00a1e":"# Ref 1\n#train_df = train_df [ train_df['building_id'] != 1099 ]\n# Ref 2\n#train_df = train_df.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')","d8c73666":"# eliminate bad rows\nbad_rows = pd.read_csv('\/kaggle\/input\/rows-to-drop\/rows_to_drop.csv')\ntrain_df.drop(bad_rows.loc[:, '0'], inplace = True)\ntrain_df.reset_index(drop = True, inplace = True)","49eb294a":"# Original code from https:\/\/www.kaggle.com\/aitude\/ashrae-missing-weather-data-handling by @aitude\ndef fill_weather_dataset(weather_df):\n    \n    # Find Missing Dates\n    time_format = \"%Y-%m-%d %H:%M:%S\"\n    start_date = datetime.datetime.strptime(weather_df['timestamp'].min(),time_format)\n    end_date = datetime.datetime.strptime(weather_df['timestamp'].max(),time_format)\n    total_hours = int(((end_date - start_date).total_seconds() + 3600) \/ 3600)\n    hours_list = [(end_date - datetime.timedelta(hours=x)).strftime(time_format) for x in range(total_hours)]\n\n    missing_hours = []\n    for site_id in range(16):\n        site_hours = np.array(weather_df[weather_df['site_id'] == site_id]['timestamp'])\n        new_rows = pd.DataFrame(np.setdiff1d(hours_list,site_hours),columns=['timestamp'])\n        new_rows['site_id'] = site_id\n        weather_df = pd.concat([weather_df,new_rows])\n\n        weather_df = weather_df.reset_index(drop=True)           \n\n    # Add new Features\n    weather_df[\"datetime\"] = pd.to_datetime(weather_df[\"timestamp\"])\n    weather_df[\"day\"] = weather_df[\"datetime\"].dt.day\n    weather_df[\"week\"] = weather_df[\"datetime\"].dt.week\n    weather_df[\"month\"] = weather_df[\"datetime\"].dt.month\n    \n    # Reset Index for Fast Update\n    weather_df = weather_df.set_index(['site_id','day','month'])\n\n    air_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['air_temperature'].mean(),columns=[\"air_temperature\"])\n    weather_df.update(air_temperature_filler,overwrite=False)\n\n    # Step 1\n    cloud_coverage_filler = weather_df.groupby(['site_id','day','month'])['cloud_coverage'].mean()\n    # Step 2\n    cloud_coverage_filler = pd.DataFrame(cloud_coverage_filler.fillna(method='ffill'),columns=[\"cloud_coverage\"])\n\n    weather_df.update(cloud_coverage_filler,overwrite=False)\n\n    due_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['dew_temperature'].mean(),columns=[\"dew_temperature\"])\n    weather_df.update(due_temperature_filler,overwrite=False)\n\n    # Step 1\n    sea_level_filler = weather_df.groupby(['site_id','day','month'])['sea_level_pressure'].mean()\n    # Step 2\n    sea_level_filler = pd.DataFrame(sea_level_filler.fillna(method='ffill'),columns=['sea_level_pressure'])\n\n    weather_df.update(sea_level_filler,overwrite=False)\n\n    wind_direction_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_direction'].mean(),columns=['wind_direction'])\n    weather_df.update(wind_direction_filler,overwrite=False)\n\n    wind_speed_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_speed'].mean(),columns=['wind_speed'])\n    weather_df.update(wind_speed_filler,overwrite=False)\n\n    # Step 1\n    precip_depth_filler = weather_df.groupby(['site_id','day','month'])['precip_depth_1_hr'].mean()\n    # Step 2\n    precip_depth_filler = pd.DataFrame(precip_depth_filler.fillna(method='ffill'),columns=['precip_depth_1_hr'])\n\n    weather_df.update(precip_depth_filler,overwrite=False)\n\n    weather_df = weather_df.reset_index()\n    weather_df = weather_df.drop(['datetime','day','week','month'],axis=1)\n    \n    def get_meteorological_features(data):\n        def calculate_rh(df):\n            df['relative_humidity'] = 100 * (np.exp((17.625 * df['dew_temperature']) \/ (243.04 + df['dew_temperature'])) \/ np.exp((17.625 * df['air_temperature'])\/(243.04 + df['air_temperature'])))\n        def calculate_fl(df):\n            flike_final = []\n            flike = []\n            # calculate Feels Like temperature\n            for i in range(len(df)):\n                at = df['air_temperature'][i]\n                rh = df['relative_humidity'][i]\n                ws = df['wind_speed'][i]\n                flike.append(feels_like(Temp(at, unit = 'C'), rh, ws))\n            for i in range(len(flike)):\n                flike_final.append(flike[i].f)\n            df['feels_like'] = flike_final\n            del flike_final, flike, at, rh, ws\n        calculate_rh(data)\n        calculate_fl(data)\n        return data\n\n    weather_df = get_meteorological_features(weather_df)\n    return weather_df","2576c91a":"def features_engineering(df):\n    \n    # Sort by timestamp\n    df.sort_values(\"timestamp\")\n    df.reset_index(drop=True)\n    \n    # Add more features\n    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"],format=\"%Y-%m-%d %H:%M:%S\")\n    df[\"hour\"] = df[\"timestamp\"].dt.hour\n    df[\"dayofweek\"] = df[\"timestamp\"].dt.dayofweek\n    \n    df['month'] = df['timestamp'].dt.month\n    df['month'].replace((1, 2, 3, 4), 1, inplace = True)\n    df['month'].replace((5, 6, 7, 8), 2, inplace = True)\n    df['month'].replace((9, 10, 11, 12), 3, inplace = True)\n  \n    df['square_feet'] =  np.log1p(df['square_feet'])\n    \n    # Remove Unused Columns\n    drop = [\"timestamp\"]\n    df = df.drop(drop, axis=1)\n    gc.collect()\n    \n    # Encode Categorical Data\n    le = LabelEncoder()\n    df[\"primary_use\"] = le.fit_transform(df[\"primary_use\"])\n    \n    return df","5157fa79":"# weather manipulation\nweather_df = fill_weather_dataset(weather_df)\n\n# memory reduction\ntrain_df = reduce_mem_usage(train_df,use_float16=True)\nbuilding_df = reduce_mem_usage(building_df,use_float16=True)\nweather_df = reduce_mem_usage(weather_df,use_float16=True)\n\n# merge data\ntrain_df = train_df.merge(building_df, left_on='building_id',right_on='building_id',how='left')\ntrain_df = train_df.merge(weather_df,how='left',left_on=['site_id','timestamp'],right_on=['site_id','timestamp'])\ndel weather_df\ngc.collect()","248da33b":"# feature engineering\ntrain_df = features_engineering(train_df)\n\n# transform target variable\ntrain_df['meter_reading'] = np.log1p(train_df[\"meter_reading\"])","f8d0c931":"# declare target, categorical and numeric columns\ntarget = 'meter_reading'\ncategorical = ['building_id', 'site_id', 'primary_use', 'meter','dayofweek']\nnumeric_cols = [col for col in train_df.columns if col not in categorical + [target, 'timestamp', 'month']]\nfeatures = categorical + numeric_cols","f31ec5f4":"import seaborn as sns\n\ndef run_lgbm(train, cat_features = categorical, num_rounds = 20000, folds = 5):\n    kf = StratifiedKFold(n_splits=folds, shuffle=False, random_state=2319)\n    models = []\n    feature_importance_df = pd.DataFrame()\n\n    param =  {'num_leaves': 3160,\n             'objective': 'regression',\n             'learning_rate': 0.03,\n             'boosting': 'gbdt',\n             'subsample': 0.5,\n             'feature_fraction': 0.7,\n             'n_jobs': -1,\n             'seed': 50,\n             'metric': 'rmse'\n              }\n    \n    oof = np.zeros(len(train))\n  \n    for tr_idx, val_idx in tqdm(kf.split(train_df,train_df['month']), total = folds):\n        tr_x, tr_y = train[features].iloc[tr_idx], train[target].iloc[tr_idx]\n        vl_x, vl_y = train[features].iloc[val_idx], train[target].iloc[val_idx]\n        tr_data = lgb.Dataset(tr_x, label = tr_y,  categorical_feature = categorical)\n        vl_data = lgb.Dataset(vl_x, label = vl_y,  categorical_feature = categorical)\n        clf = lgb.train(param, tr_data, num_rounds, valid_sets = [tr_data, vl_data], verbose_eval = 25, \n                        early_stopping_rounds = 50)\n        \n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = features\n        fold_importance_df[\"importance\"] = clf.feature_importance()\n        \n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        models.append(clf)\n        oof[val_idx] = clf.predict(vl_x)\n        gc.collect()\n    score = np.sqrt(metrics.mean_squared_error(train[target], np.clip(oof, a_min=0, a_max=None)))\n    print('Our oof cv is :', score)\n    \n    cols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:20].index)\n    best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\n    plt.figure(figsize=(14,26))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n    plt.title('LightGBM Features (averaged over folds)')\n    plt.tight_layout()\n    plt.savefig('lgbm_importances.png')\n\n    return models\nmodels = run_lgbm(train_df)","3ea0bcf3":"# read test\ntest_df = pd.read_csv('\/kaggle\/input\/ashrae-energy-prediction\/test.csv')\nrow_ids = test_df[\"row_id\"]\ntest_df.drop(\"row_id\", axis=1, inplace=True)\ntest_df = reduce_mem_usage(test_df)\n\n# merge with building info\ntest_df = test_df.merge(building_df,left_on='building_id',right_on='building_id',how='left')\ndel building_df\ngc.collect()\n\n# fill test weather data\nweather_df = pd.read_csv('\/kaggle\/input\/ashrae-energy-prediction\/weather_test.csv')\nweather_df = fill_weather_dataset(weather_df)\nweather_df = reduce_mem_usage(weather_df)\n\n# merge weather data\ntest_df = test_df.merge(weather_df,how='left',on=['timestamp','site_id'])\ndel weather_df\ngc.collect()\n\n# feature engineering\ntest_df = features_engineering(test_df)","912e6f17":"def predictions(models, iterations = 120):\n    # split test data into batches\n    set_size = len(test_df)\n    batch_size = set_size \/\/ iterations\n    meter_reading = []\n    for i in tqdm(range(iterations)):\n        pos = i*batch_size\n        fold_preds = [np.expm1(model.predict(test_df[features].iloc[pos : pos+batch_size])) for model in models]\n        meter_reading.extend(np.mean(fold_preds, axis=0))\n\n    print(len(meter_reading))\n    assert len(meter_reading) == set_size\n    submission = pd.read_csv('\/kaggle\/input\/ashrae-energy-prediction\/sample_submission.csv')\n    submission['meter_reading'] = np.clip(meter_reading, a_min=0, a_max=None) # clip min at zero\n    submission.to_csv('fe2_lgbm.csv', index=False)\n    print('We are done!')\npredictions(models)","3fec9586":"Referring to the following discussions. We have filtered the data in the next step\n\n* Ref1. https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/discussion\/114830#latest-680086\n* Ref2. https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/discussion\/113054#656588 ","190184ba":"# Load Data"}}