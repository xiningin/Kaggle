{"cell_type":{"b27208b0":"code","27754af2":"code","abb6514f":"code","6e70e70f":"code","a3d8bb4d":"code","03524f30":"code","4fecba00":"code","ed6ca477":"code","89bc28ad":"code","87701c0e":"code","bf360754":"markdown","58969f6b":"markdown","08c28ed3":"markdown","d5d40b4c":"markdown","15aa06f7":"markdown","6db0e6d6":"markdown","b11c0cc2":"markdown","997271e0":"markdown","25a24efd":"markdown","65f9cbba":"markdown","611e186a":"markdown","849d705b":"markdown","a7a7cddb":"markdown","aae89f19":"markdown","2d5fb5bd":"markdown","c6265825":"markdown","154455ce":"markdown","997a6cf4":"markdown","a2b58de9":"markdown","0d44272e":"markdown","16184a71":"markdown"},"source":{"b27208b0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport os\nimport copy\nprint(os.listdir(\"..\/input\"))","27754af2":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","abb6514f":"class TorchFM(nn.Module):\n    def __init__(self, n=None, k=None):\n        super().__init__()\n        # Initially we fill V with random values sampled from Gaussian distribution\n        # NB: use nn.Parameter to compute gradients\n        self.V = nn.Parameter(torch.randn(n, k),requires_grad=True)\n        self.lin = nn.Linear(n, 1)\n\n        \n    def forward(self, x):\n        out_1 = torch.matmul(x, self.V).pow(2).sum(1, keepdim=True) #S_1^2\n        out_2 = torch.matmul(x.pow(2), self.V.pow(2)).sum(1, keepdim=True) # S_2\n        \n        out_inter = 0.5*(out_1 - out_2)\n        out_lin = self.lin(x)\n        out = out_inter + out_lin\n        \n        return out","6e70e70f":"# load train data\ntrain_df = pd.read_csv('..\/input\/dota-heroes-binary\/dota_train_binary_heroes.csv', index_col='match_id_hash')\ntest_df = pd.read_csv('..\/input\/dota-heroes-binary\/dota_train_binary_heroes.csv', index_col='match_id_hash')\ntarget = pd.read_csv('..\/input\/dota-heroes-binary\/train_targets.csv', index_col='match_id_hash')\ny = target['radiant_win'].values.astype(np.float32)\ny = y.reshape(-1,1)","a3d8bb4d":"# convert to 32-bit numbers to send to GPU \nX_train = train_df.values.astype(np.float32)\nX_test = test_df.values.astype(np.float32)","03524f30":"# To compute probalities\ndef sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))","4fecba00":"# for reproducibility\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()","ed6ca477":"# main training function\ndef train_mlp(X, X_test, y, folds, model_class=None, model_params=None, batch_size=128, epochs=1,\n              criterion=None, optimizer_class=None, opt_params=None,\n#               clr=cyclical_lr(10000),\n              device=None):\n    \n    seed_everything()\n    models = []\n    scores = []\n    train_preds = np.zeros(y.shape)\n    test_preds = np.zeros((X_test.shape[0], 1))\n    \n    X_tensor, X_test, y_tensor = torch.from_numpy(X).to(device), torch.from_numpy(X_test).to(device), torch.from_numpy(y).to(device)\n    for n_fold, (train_ind, valid_ind) in enumerate(folds.split(X, y)):\n        \n        print(f'fold {n_fold+1}')\n        \n        train_set = TensorDataset(X_tensor[train_ind], y_tensor[train_ind])\n        valid_set = TensorDataset(X_tensor[valid_ind], y_tensor[valid_ind])\n        \n        loaders = {'train': DataLoader(train_set, batch_size=batch_size, shuffle=True),\n                   'valid': DataLoader(valid_set, batch_size=batch_size, shuffle=False)}\n        \n        model = model_class(**model_params)\n        model.to(device)\n        best_model_wts = copy.deepcopy(model.state_dict())\n        \n        optimizer = optimizer_class(model.parameters(), **opt_params)\n#         scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, [clr])\n        \n        # training cycle\n        best_score = 0.\n        for epoch in range(epochs):\n            losses = {'train': 0., 'valid': 0}\n            \n            for phase in ['train', 'valid']:\n               \n                if phase == 'train':\n                    model.train()\n                else:\n                    model.eval()\n                \n                for batch_x, batch_y in loaders[phase]:\n                    optimizer.zero_grad()\n                    out = model(batch_x)\n                    loss = criterion(out, batch_y)\n                    losses[phase] += loss.item()*batch_x.size(0)\n                    \n                    with torch.set_grad_enabled(phase == 'train'):\n                        if phase == 'train':\n                            loss.backward()\n#                             scheduler.step()\n                            optimizer.step()\n\n                losses[phase] \/= len(loaders[phase].dataset)\n            \n            # after each epoch check if we improved roc auc and if yes - save model\n            with torch.no_grad():\n                model.eval()\n                valid_preds = sigmoid(model(X_tensor[valid_ind]).cpu().numpy())\n                epoch_score = roc_auc_score(y[valid_ind], valid_preds)\n                if epoch_score > best_score:\n                    best_model_wts = copy.deepcopy(model.state_dict())\n                    best_score = epoch_score\n            \n            if ((epoch+1) % 30) == 0:\n                print(f'epoch {epoch+1} train loss: {losses[\"train\"]:.3f} valid loss {losses[\"valid\"]:.3f} valid roc auc {epoch_score:.3f}')\n        \n        # prediction on valid set\n        with torch.no_grad():\n            model.load_state_dict(best_model_wts)\n            model.eval()\n            \n            train_preds[valid_ind] = sigmoid(model(X_tensor[valid_ind]).cpu().numpy())\n            fold_score = roc_auc_score(y[valid_ind], train_preds[valid_ind])\n            scores.append(fold_score)\n            print(f'Best ROC AUC score {fold_score}')\n            models.append(model)\n\n            test_preds += sigmoid(model(X_test).cpu().numpy())\n    \n    print('CV AUC ROC', np.mean(scores), np.std(scores))\n    \n    test_preds \/= folds.n_splits\n    \n    return models, train_preds, test_preds","89bc28ad":"folds = KFold(n_splits=5, random_state=17)","87701c0e":"%%time\nMS, train_preds, test_preds = train_mlp(X_train, X_test, y, folds, \n                            model_class=TorchFM, \n                            model_params={'n': X_train.shape[1], 'k': 5}, \n                            batch_size=1024,\n                            epochs=300,\n                            criterion=nn.BCEWithLogitsLoss(),\n                            optimizer_class=torch.optim.SGD, \n                            opt_params={'lr': 0.01, 'momentum': 0.9},\n                            device=DEVICE\n                            )","bf360754":"The first two terms is just a linear model (or, in the Deep Learning lingo, a linear layer). The last term can be expressed as:\n$$\\sum_{i=1}^{n} \\sum_{j=i+1}^{n} \\langle \\textbf v_i, \\textbf v_j \\rangle x_i x_j = \n\\frac{1}{2} \\sum_{f=1}^{k} \\Big( \\big(\\sum_{i=1}^{n} v_f^{(i)} x_i \\big)^2 - \\sum_{i=1}^{n}v_f^{(i) 2} x_i^2 \\Big) = \n\\frac{1}{2} \\sum_{f=1}^{} \\Big( S_{1,f}^2 - S_{2,f} \\Big) =\n\\frac{1}{2} \\Big( S_{1}^2 - S_{2} \\Big),\n$$","58969f6b":"The factorization machine with pairwise interactions is defines as:\n$$\\hat{y}(x) = w_0 + \\sum_{i=1}^{n}w_i x_i + \\sum_{i=1}^{n} \\sum_{j=i+1}^{n} \\langle \\textbf v_i, \\textbf v_j \\rangle x_i x_j$$","08c28ed3":"The original context for creation of factorization machines was recommender system: for instance, recommend a movie to a customer based on his ratings to other movies. However, I want to take a look on this algorithm from another point view: Factorization Machines can be considered as an extension of linear model that additionally incorporate information about features interactions (in an efficient way). In linear models we just compute a weighted sum of predictors and do not take into account interactions among features e.g. $x_i^{(k)} x_j^{(k)}$ for two features, where $i, j$ - feature indices and $k$ is an index of object in the trainset. However, the number of pairwise interactions scales quadratically with the number of features, so for 1000 features we get 1000000 interactions. Needless to say, it is computationally inefficient to compute weights for each interactions, moreover a model with a large number of parameters is prone to overfitting. Factorization Machines use an elegant trick: find vectors for each feature and compute interaction weight as a dot product of two those features i.e. we **factorize** interactions weight matrix $W \\in \\mathbb{R}^{n \\times n}$ as a product $VV^{^T}$, where $V \\in \\mathbb{R}^{n \\times k}$.","d5d40b4c":"Hi! In this tutotial I want to discuss libFFM algorithm and share my implementation of this algorithm in PyTorch. This tutorial should be considered as an extension to already published tutorial [\"Factorization Machines\" (Russian)](https:\/\/nbviewer.jupyter.org\/github\/Yorko\/mlcourse_open\/blob\/master\/jupyter_russian\/tutorials\/factorization_machines_sygorbatyuk.ipynb), however, my goal is different: to show that given a paper with mathematical description of a model and PyTorch we can easily implement the model all by ourselves.","15aa06f7":"We will test our model on data from recently ended (not really ended, but you know what I mean) [mlcourse.ai: Dota 2 Winner Prediction](https:\/\/www.kaggle.com\/c\/mlcourse-dota2-win-prediction). We won't use all features, only binary indicators of hero_ids for each team. We will try to find if there's any \"synergy\" among pairs of heroes.","6db0e6d6":"## PyTorch model","b11c0cc2":"## Conclusion","997271e0":"## Sources","25a24efd":"We have to do two things: add a linear layer and define all matrix operations from the expression above. Addition of a linear layer in straightforward. As to factorization part, we shouldn't forget to register $V$ as a parameter of our model with `nn.Parameter` (otherwise, we won't be able to learn optimal $V$ with gradient descent). What is good about PyTorch that we don't have to bother with finding a derivative of our expression, PyTorch will do that for us!","65f9cbba":"In this tutorial we made our own Factorization Machine with a pinch of linear algebra and autograd magic of PyTorch.\n\nGood luck with your own experiments!","611e186a":"1. Original paper by Rendle https:\/\/www.csie.ntu.edu.tw\/~b97053\/paper\/Rendle2010FM.pdf","849d705b":"where I used $S_1$ and $S_2$ for clarity. [](http:\/\/)Suppose we have $M$ training objects, $n$ features and we want to factorize feature interaction with vectors of size $k$ i.e. dimensionality of $v_i$. Let us denote our trainset as $X \\in \\mathbb{R}^{M \\times n}$ , and matrix of $v_i$ (the ith row is $v_i$) as  $V \\in \\mathbb{R}^{n \\times k}$. Also let's denote feature vector for the jth object as $\\textbf x_j$. So:<br><br>\n$$\nX = \\begin{bmatrix}\nx_1^{(1)} & \\dots & x_n^{(1)}\\\\\n \\vdots \\ & \\ddots \\ & \\vdots \\\\ \nx_1^{(M)} & \\dots & x_n^{(M)} \\\\\n\\end{bmatrix}\n$$\n<br><br>\n$$\nV = \\begin{bmatrix}\nv_1^{(1)} & \\dots & v_k^{(1)}\\\\\n \\vdots \\ & \\ddots \\ & \\vdots \\\\ \nv_1^{(n)} & \\dots & v_k^{(n)} \\\\\n\\end{bmatrix}\n$$\n<br>\nThe number in brackets indicate the index of the sample for $x$ and the index of feature for $v$.","a7a7cddb":"Let us rewrite the formula above in matrix form. Our final result should be the matrix of size $M \\times 1$. We clearly see $S_1 = \\sum_{i=1}^{n} v_f^{(i)} x_i $ is a dot product of feature vector  $\\textbf x_j$ (a row of $X$) and the ith column of $V$. If we multiply $X$ and $V$, we get: <br><br>\n$$\nXV = \\begin{bmatrix}\n\\sum_{i=1}^{n} v_f^{(1)} x_i^{(1)}  & \\dots &  \\sum_{i=1}^{n} v_f^{(k)} x_i\\\\\n \\vdots \\ & \\ddots \\ & \\vdots \\\\ \n\\sum_{i=1}^{n} v_f^{(1)} x_i^{(M)} & \\dots & \\sum_{i=1}^{n} v_f^{(k)} x_i^{(M)} \\\\\n\\end{bmatrix} = \n\\begin{bmatrix}\nS_{1,1}^{(1)}  & \\dots &  S_{1,k}^{(1)}\\\\\n \\vdots \\ & \\ddots \\ & \\vdots \\\\ \nS_{1,1}^{(M)}  & \\dots & S_{1,k}^{(M)} \\\\\n\\end{bmatrix}\n$$","aae89f19":"In order to train our model we have to define several functions:","2d5fb5bd":"Let's translate it into PyTorch model!","c6265825":"# Factorization machine implemented in PyTorch","154455ce":"## The FM model definition","997a6cf4":"Hm, looks pretty good. So if square $XV$ element-wise and then find sum of each row, we obtain vector of $S_1^2$ terms for each training sample. Also, if we first square $X$ and $V$ element-wise, then multiply them and finally sum by rows,  we'll get $S_2$ term for each training object. So, conceptually, we can express the final term like this:\n$$\n\\hat{\\textbf{y}}(X) = \\frac{1}{2} ( square(XV) - (square(X) \\times square(V) )).sum(rowwise),\n$$","a2b58de9":"I certainly wouldn't call it a good model, but at least it works. We see that there's indeed some link between teams composition and winning in the match.","0d44272e":"You see it's not that much of code. Let's try our model.","16184a71":"Since our kernel is just a proof of concept, we won't optimize hyperparameters and set high learning rate."}}