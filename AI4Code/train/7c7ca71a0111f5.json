{"cell_type":{"e011ebb5":"code","e2b34b57":"code","0f7c4463":"code","e4845ee9":"code","00ce5c17":"code","6c2cfee1":"code","c65006fa":"code","2ba35dd0":"code","0c90e607":"code","26949067":"code","67d74055":"code","4f2e4d09":"code","b73d088e":"code","623f5eb2":"code","d5d18978":"code","98c58d63":"code","a436e1ab":"code","443c84f8":"code","54237114":"code","19533d1f":"code","bbd86022":"code","9333a56c":"code","dd5c5912":"markdown"},"source":{"e011ebb5":"# Having a play with NER method as seen on Youtube - https:\/\/www.youtube.com\/watch?v=4SoLoo0fdH0\n# Thanks to Rachael Tatman","e2b34b57":"import pandas as pd\nimport spacy\nimport networkx as nx\nfrom itertools import combinations\nfrom collections import defaultdict\nimport operator\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom math import log","0f7c4463":"data.claim[0]","e4845ee9":"# check out the data\ndata = pd.read_csv(\"..\/input\/snopes.csv\")\ndata.head()\n\n# remove examples\ndata = data.replace({'Example\\(s\\)': ''}, regex=True)\ndata = data.replace({'\\s+': ' '}, regex=True)","00ce5c17":"# Method where you pass in a list of lists of ents, outputting coocur dict\ndef coocurrence(*inputs):\n    com = defaultdict(int)\n    \n    for common_entities in inputs:\n        # Build co-occurrence matrix\n        for w1, w2 in combinations(sorted(common_entities), 2):\n            com[w1, w2] += 1\n\n    result = defaultdict(dict)\n    for (w1, w2), count in com.items():\n        if w1 != w2:\n            result[w1][w2] = {'weight': count}    \n    return result","6c2cfee1":"# test coocurrence\n# Why is d not a key here? Intended? Saving on redundancy? \n\ncoocurrence('abcddc', 'bddad', 'cdda')","c65006fa":"# remove duplicate claims\nclaims = data.claim.unique()\n\n# make sure it's all strings \n# added lower and whitespace strip just in case\n# claims = [str(claim).lower().strip() for claim in claims]\n# Turns out this ruins it... and reduced most docs to few claims for some reason\n\n# NER list we'll use - Maybe this needs looking at?\nnlp = spacy.load('en_core_web_sm')\n\n# intialize claim counter & lists for our entities\ncoocur_edges = {}\n\nprint('Number of claims: ', len(claims))","2ba35dd0":"# Working much better without the str casting now. \n# Redoing whole sheet from here\n\nfor doc in nlp.pipe(claims[:10]):\n    print(doc)\n    print(list(doc.ents))","0c90e607":"# Looking at number of times each ent appears in the total corpus\n# nb. ents all appear as Spacy tokens, hence needing to cast as str for dict\n\n# Spacy seems to have error at 3k doc mark? \n# Related to this maybe? https:\/\/github.com\/explosion\/spaCy\/issues\/1927\n# Continuing on with the first 3000 of 3122 for now\n\nall_ents = defaultdict(int)\n\nfor i, doc in enumerate(nlp.pipe(claims[:3000])):\n    #print(i,doc)\n    for ent in doc.ents:\n        all_ents[str(ent)] += 1\n        \nprint('Number of distinct entities: ', len(all_ents))","26949067":"# Most popular ents\nsorted_ents = sorted(all_ents.items(), key=operator.itemgetter(1), reverse=True)\nsorted_ents[:20]","67d74055":"# Number of ents that appear at least twice\n\nmulti_ents = [x for x in sorted_ents if x[1] > 1]\n\nprint('Number of ents that appear at least twice: ', len(multi_ents))","4f2e4d09":"# How many ents appear per claim?\n# Blank strings (non breaking spaces?) popular?\n\nents_in_claim = [len(doc.ents) for doc in nlp.pipe(claims[:3000])]\n\nplt.hist(ents_in_claim, \n         rwidth=0.9, \n         bins=np.arange(max(ents_in_claim)+2)-0.5)  \n# Futzing with bins to fix column alignment\nplt.title('Entities per claim')\nplt.show()","b73d088e":"# Exploring\/Demonstrating with small subset, then performing calcs with whole dataset afterwards","623f5eb2":"# Listing claims as a list of their entities\n\nclaim_ents = []\nfor i, doc in enumerate(nlp.pipe(claims[:5])):\n    string_ents = list(map(str, doc.ents))\n    claim_ents.append(string_ents)\n    # Doubling some up to fake\/force coocurrence\n    if i%2==0:\n        claim_ents.append(string_ents)  \nclaim_ents\n\n# Could do as a one line list comprehension, though maybe not as readable:\n# claim_ents = [list(map(str, doc.ents)) for doc in nlp.pipe(claims[:5])]","d5d18978":"# Can filter out claims with only 1 ent (nothing to coocur with)\n\nmulti_ent_claims = [c for c in claim_ents if len(c)>1]\n# single_ent_claims = [c for c in claim_ents if len(c)==1]\n# no_ent_claims = [c for c in claim_ents if len(c)==0]\n\nmulti_ent_claims","98c58d63":"# Generating coocurrence dict of dicts\n# Something funny with that \/xa0 non breaking space... at least the method seems to be working?\n\ncoocur_edges = coocurrence(*multi_ent_claims)\ncoocur_edges","a436e1ab":"# Filter out ents with <2 weight\n# Could also use: del coocur_edges[k1][k2] rather than make new dict\ncoocur_edges_filtered = defaultdict()\n\nfor k1, e in coocur_edges.items():\n    ents_over_2_weight = {k2: v for k2, v in e.items() if v['weight'] > 1}\n    if ents_over_2_weight:  # ie. Not empty\n        coocur_edges_filtered[k1] = ents_over_2_weight\n\ncoocur_edges_filtered","443c84f8":"# Summing all coocurrences in order to see most coocurring edges\n\ncoocur_sum = defaultdict(int)\nfor k1, e in coocur_edges_filtered.items():\n    for k2, v in e.items():\n        coocur_sum[k1] += v['weight']\ncoocur_sum","54237114":"# Now to retry with whole dataset. Here goes nothin...","19533d1f":"# Making the list of claims\nclaim_ents = []\nfor doc in nlp.pipe(claims[:3000]):\n    string_ents = list(map(str, doc.ents))\n    claim_ents.append(string_ents)\n      \n# Keeping only claims with multiple entities\nmulti_ent_claims = [c for c in claim_ents if len(c)>1]\n# single_ent_claims = [c for c in claim_ents if len(c)==1]\n# no_ent_claims = [c for c in claim_ents if len(c)==0]\n\n# Creating the coocurrance dict\ncoocur_edges = coocurrence(*multi_ent_claims)","bbd86022":"# Filter out ents with < x weight - change this for graph clarity?\ncoocur_edges_filtered = defaultdict()\nfor k1, e in coocur_edges.items():\n    ents_over_x_weight = {k2: v for k2, v in e.items() if v['weight'] > 3}\n    if ents_over_x_weight:  # ie. Not empty\n        coocur_edges_filtered[k1] = ents_over_x_weight\n        \n# Looking at the most coocurring edges\ncoocur_sum = defaultdict(int)\nfor k1, e in coocur_edges_filtered.items():\n    for k2, v in e.items():\n        coocur_sum[k1] += v['weight']\n\nsorted_coocur = sorted(coocur_sum.items(), key=operator.itemgetter(1), reverse=True)\nprint('Most frequent CO-ocurring entity:')\ntop_cooccur = sorted_coocur[:20]\ntop_cooccur","9333a56c":"# Getting the data - top10, excl. that space occurring 3k times..\ntop_cooccur_no_space = [x[0] for x in top_cooccur[:50]]  \ngraph_edges = {k:coocur_edges_filtered[k] for k in top_cooccur_no_space}\n\n# Attempting to graph these top 10 coocurrances\nG = nx.from_dict_of_dicts(graph_edges)\npos = nx.spring_layout(G)\n\n# Normalise, then scale the line weights\nweights = [G[u][v]['weight'] for u, v in G.edges() if u != v]\nweights = list(map(lambda x: (x - min(weights)) \/ (max(weights) - min(weights)), weights))\nweights = list(map(lambda x: (x * 6) + 1, weights))\n\n# Scale node weights on log scale \nsum_weights = [coocur_sum[n] if coocur_sum[n]>0 else 1 for n in G.nodes]\nsum_weights = list(map(lambda x: 10*x, sum_weights))\n# sum_weights = list(map(lambda x: 100*log(x), sum_weights))\n# '\\xa0Example(s' with 0 weight throwing off scaling?\n\n\nplt.figure(figsize=(20,10))\n\n# nx.draw(G, pos)\nnx.draw_networkx_edges(G, pos, alpha=0.2, width=weights)\nnx.draw_networkx_nodes(G, pos, alpha=0.2, node_size=sum_weights)\nnx.draw_networkx_labels(G, pos)\n\nplt.xticks([])\nplt.yticks([])\n\nplt.title('Top coocurrances of named entities in Snopes claims')\nplt.show()","dd5c5912":"Function to return a count of co-occurances as a dictionary of dicationaries, by StackOverflow user [Patrick Maupin](https:\/\/stackoverflow.com\/questions\/32534655\/python-creating-undirected-weighted-graph-from-a-co-occurrence-matrix)."}}