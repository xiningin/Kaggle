{"cell_type":{"ae1315db":"code","07704133":"code","619761b9":"code","90aec561":"code","8f89b072":"code","79e6bb43":"code","7beaf9fc":"code","6b489ce9":"code","4e65d402":"code","1d0ea7c4":"code","1249f098":"code","9bf66a92":"code","252a3aba":"code","cb4ccc08":"code","21a4144e":"code","25e2d69c":"code","8c054949":"code","b470d039":"code","4e1428d9":"markdown","aba0ad7c":"markdown","27c3502e":"markdown","2604617f":"markdown","b54cb276":"markdown","8d64297d":"markdown","f33b035e":"markdown"},"source":{"ae1315db":"import os\nimport gc\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom joblib import Parallel, delayed\nfrom multiprocessing import Pool\nfrom tqdm import tqdm\n\nfrom numba import njit\nfrom numba import jit\n\nfrom sklearn.model_selection import KFold\n\nimport lightgbm as lgb\n\npd.options.display.max_columns = 300","07704133":"def rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","619761b9":"train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntrain['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ntest = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\ntest['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)","90aec561":"book_train_path = \"..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id=\"\ntrade_train_path = '..\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\/stock_id='\n\nbook_test_path = \"..\/input\/optiver-realized-volatility-prediction\/book_test.parquet\/stock_id=\"\ntrade_test_path = '..\/input\/optiver-realized-volatility-prediction\/trade_test.parquet\/stock_id='","8f89b072":"train_stock_ids = train['stock_id'].unique()\ntest_stock_ids = test['stock_id'].unique()","79e6bb43":"n_timeids = []\nfor i in train_stock_ids:\n    n_timeids.append(train[train['stock_id'] == i]['time_id'].nunique())\nstock_timeids = pd.DataFrame(zip(train_stock_ids, n_timeids), columns = ['stock', 'unique_time_ids'])\nstock_timeids.sort_values(by='unique_time_ids', inplace=True)\nstock_timeids[stock_timeids['unique_time_ids'] < 3830]","7beaf9fc":"n_timeids_book = []\ntime_ids_book = []\nn_timeids_trade = []\ntime_ids_trade = []\nfor i in tqdm(train_stock_ids):\n    book_train = pd.read_parquet(book_train_path + str(i))\n    trade_train = pd.read_parquet(trade_train_path + str(i))\n    book_time_ids  = book_train['time_id'].unique()\n    trade_time_ids  = trade_train['time_id'].unique()\n    \n    n_timeids_book.append(len(book_time_ids))\n    time_ids_book.append(book_time_ids)\n    n_timeids_trade.append(len(trade_time_ids))\n    time_ids_trade.append(trade_time_ids)\n    \nbook_trade_timeids = pd.DataFrame(zip(train_stock_ids, n_timeids_book, time_ids_book, n_timeids_trade, time_ids_trade), \n                           columns = ['stock_id', 'n_timeids_book', 'time_ids_book', 'n_timeids_trade', 'time_ids_trade'])\n\nunequal_time_ids = book_trade_timeids[~(book_trade_timeids['n_timeids_book'] == book_trade_timeids['n_timeids_trade'])]\n\nunequal_time_ids","6b489ce9":"for i, j in zip(unequal_time_ids['time_ids_book'], unequal_time_ids['time_ids_trade']):\n    print(np.setdiff1d(np.array(i), np.array(j)))","4e65d402":"original_cols = ['time_id', 'seconds_in_bucket', 'bid_price1', 'ask_price1',\n       'bid_price2', 'ask_price2', 'bid_size1', 'ask_size1', 'bid_size2',\n       'ask_size2', 'price', 'size', 'order_count']\n        \ncols = ['hbuy1', 'lsell1', 'mid1', \n            'hbuy2','lsell2', 'mid2', \n            'mid1_sub_bp1', 'mid1_sub_ap1',\n            'mid2_sub_bp2', 'mid2_sub_ap2', \n            'hbuy1_lsell1_spread', 'hbuy2_lsell2_spread', \n            'wap_high_low1', 'wap_high_low2',\n            'log_wap_high_low1_std', 'log_wap_high_low2_std'\n            ]","1d0ea7c4":"@njit\ndef calculate_features(np_arr, indices):\n    zeros = np.zeros((np_arr.shape[0], 16)) # change dimension as you have more features\n    for ind in range(len(indices) - 1):\n        start, end = indices[ind], indices[ind + 1] # will give data upto the point where trade occurs\n        arr = np_arr[start:end]\n        arr = arr.transpose() # for easy selection\n\n        hbuy1 = arr[2].max() # highest buy 1\n        lsell1 = arr[3].min() # lowest sell 1\n        mid1 = (hbuy1 + lsell1) \/ 2 # average of highest buy 1 and lowest sell 1\n        zeros[start:end, 0] = hbuy1 \n        zeros[start:end, 1] = lsell1\n        zeros[start:end, 2] = mid1\n        \n        hbuy2 = arr[4].max() # highest buy 2\n        lsell2 = arr[5].min() # lowest sell 2\n        mid2 = (hbuy2 + lsell2) \/ 2 # average of highest buy 2 and lowest sell 2\n        zeros[start:end, 3] = hbuy2 \n        zeros[start:end, 4] = lsell2\n        zeros[start:end, 5] = mid2\n        \n        # mid1 - bidprice1 # mid1 - askprice1\n        mid1_sub_bp1 = mid1 - arr[2]\n        mid1_sub_ap1 = mid1 - arr[3]\n        zeros[start:end, 6] = mid1_sub_bp1\n        zeros[start:end, 7] = mid1_sub_ap1\n\n        # mid2 - bidprice2 # mid2 - askprice2\n        mid2_sub_bp2 = mid2 - arr[4]\n        mid2_sub_ap2 = mid2 - arr[5]\n        zeros[start:end, 8] = mid2_sub_bp2\n        zeros[start:end, 9] = mid2_sub_ap2\n        \n        zeros[start:end, 10] = hbuy1 - lsell1 # high buy low sell spread 1\n        zeros[start:end, 11] = hbuy2 - lsell2 # high buy low sell spread 2\n         \n        wap_high_low1 = (hbuy1*arr[7] + lsell1*arr[6])\/ (arr[6]+arr[7]) # high buy 1 low sell 1\n        wap_high_low2 = (hbuy2*arr[7] + lsell2*arr[6])\/ (arr[6]+arr[7]) # high buy 2 low sell 2\n        log_wap_high_low1 = np.append(np.array(0), np.diff(np.log(wap_high_low1)))\n        log_wap_high_low2 = np.append(np.array(0),np.diff(np.log(wap_high_low2)))\n        zeros[start:end, 12] = wap_high_low1\n        zeros[start:end, 13] = wap_high_low2\n        zeros[start:end, 14] = log_wap_high_low1.std()\n        zeros[start:end, 15] = log_wap_high_low2.std()\n\n#         zeros[start:end, 4] = (arr[2]*arr[7] + arr[3]*arr[6])\/ (arr[6]+arr[7]) # wap1\n#         zeros[start:end, 5] = (arr[4]*arr[9] + arr[5]*arr[8])\/ (arr[8]+arr[9]) # wap2\n        \n#         zeros[start:end, 6] = (arr[4]*arr[9] + arr[5]*add[8])\/ (arr[8]+arr[9]) \n        \n    cols = ['hbuy1', 'lsell1', 'mid1', \n            'hbuy2','lsell2', 'mid2', \n            'mid1_sub_bp1', 'mid1_sub_ap1',\n            'mid2_sub_bp2', 'mid2_sub_ap2', \n            'hbuy1_lsell1_spread', 'hbuy2_lsell2_spread', \n            'wap_high_low1', 'wap_high_low2',\n            'log_wap_high_low1_std', 'log_wap_high_low2_std'\n            ]\n    \n    \n    trade_book_arr = np.append(np_arr, zeros, axis=1)\n    return trade_book_arr","1249f098":"%%time\ndef get_calculated_features_trade(stock, train=True):\n    if train:\n        trade_path = trade_train_path\n        book_path = book_train_path\n    else:\n        trade_path = trade_test_path\n        book_path = book_test_path\n        \n    trade_train = pd.read_parquet(trade_path+str(stock))\n    book_train = pd.read_parquet(book_path+str(stock))\n    \n    # time ids where there is no trade transaction \n    time_ids_not_in_trade = np.setdiff1d(book_train['time_id'].unique(), trade_train['time_id'].unique())\n\n    merged_df = book_train.merge(trade_train, on=['time_id', 'seconds_in_bucket'], how='left')\n    \n    # selection of rows upto whare a trade happens\n    no_trade_index = np.array(merged_df[~merged_df['price'].isna()].index + 1)\n    no_trade_index = np.insert(no_trade_index, 0, 0) # insert 0 at the beginning\n    \n    # Inserting the len of book data so that data is captured upto the last book order data\n    if merged_df.shape[0] != no_trade_index[-1]:\n        no_trade_index = np.insert(no_trade_index, len(no_trade_index), merged_df.shape[0])\n\n    # features calculation\n    features_arr = calculate_features(merged_df.to_numpy(), no_trade_index)\n\n    #  ********************************************** Version 1 WRONG CODE\n    # Return only the data where trade occurs \n    # trade_ind = no_trade_index.copy()\n    # trade_ind[-1] = trade_ind[-1] - 1 \n    # trade_ind = np.unique(trade_ind)\n    #  ********************************************** Version 1 WRONG CODE\n    \n    trade_ind = np.argwhere(~np.isnan(features_arr[:, 10]))[:, 0] # RIGHT APPROACH , 10 is price column , select where price ie trade occurs\n    if len(trade_ind) <= 1:\n        trade_arr = features_arr\n    else:\n        trade_arr = features_arr[trade_ind]\n    # filling the trade data with book order data so that all time ids are present for prediction\n    if len(time_ids_not_in_trade) > 0:\n        for i in time_ids_not_in_trade:\n            trade_arr = np.concatenate([trade_arr, features_arr[features_arr[:, 0] == i]])\n\n    df = pd.DataFrame(trade_arr, columns = original_cols + cols)\n    df['time_id'] = df['time_id'].astype('int64')\n\n    features = [f for f in df.columns if f not in ['time_id', 'seconds_in_bucket']]\n    grouped_df = df.groupby('time_id')[features].agg(['mean', \n                                            'std', \n                                            'median',\n                                            'var', \n                                            'sem', \n                                           ])\n    grouped_df.columns = ['_'.join(col) for col in grouped_df.columns]\n\n    # extract the last transaction of book order data for which trade occured\n    last_book = df.groupby('time_id')[features].agg(['last'])\n    last_book.columns = [k for k, v in last_book.columns]\n\n    # Open high low close for trade data only\n    ohlc_trade = df.groupby('time_id')[['price']].agg(['first', 'max', 'min', 'last'])\n    ohlc_trade.columns = ['_'.join(col) for col in ohlc_trade.columns]\n\n    df_all = pd.concat([last_book, ohlc_trade, grouped_df], axis=1)\n    df_all.insert(0, 'stock_id', stock)\n    df_all = df_all.reset_index()\n    df_all['row_id'] = df_all['stock_id'].astype(str) + \"-\" + df_all['time_id'].astype(str)\n    return df_all\n\ntrain_trade_book = Parallel(n_jobs=-1, verbose=1)(delayed(get_calculated_features_trade)(ind) for ind in train_stock_ids)\n\ntest_trade_book = Parallel(n_jobs=-1, verbose=1)(delayed(get_calculated_features_trade)(ind, train=False) for ind in test_stock_ids)","9bf66a92":"df_train = pd.concat(train_trade_book)\ndf_test = pd.concat(test_trade_book)","252a3aba":"df_train.head()","cb4ccc08":"df_test.head()","21a4144e":"features = [f for f in df_train.columns if f not in ['row_id', 'time_id']]\ny = train['target'].values","25e2d69c":"seed=60\nparams = {\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': -1,\n    'categorical_column':[0],\n    'seed':seed,\n    'feature_fraction_seed': seed,\n    'bagging_seed': seed,\n    'drop_seed': seed,\n    'data_random_seed': seed,\n    'n_jobs':-1,\n    'verbose': -1}\n\noof_predictions = np.zeros(df_train.shape[0])\n\ntest_preds = np.zeros(df_test.shape[0])\n\nkfold = KFold(n_splits = 5, random_state = 60, shuffle = True)\nfor fold, (trn_ind, val_ind) in enumerate(kfold.split(df_train)):\n    print(f'Training fold {fold + 1}')\n    x_train, x_val = df_train[features].iloc[trn_ind], df_train[features].iloc[val_ind]\n    y_train, y_val = y[trn_ind], y[val_ind]\n\n    train_weights = 1 \/ np.square(y_train)\n    val_weights = 1 \/ np.square(y_val)\n\n    train_dataset = lgb.Dataset(x_train[features], y_train, \n                                    weight = train_weights\n                               )\n    val_dataset = lgb.Dataset(x_val[features], y_val, \n                                  weight = val_weights\n                             )\n    model = lgb.train(params = params,\n                          num_boost_round=1200,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          verbose_eval = 250,\n                          early_stopping_rounds=30,\n                          feval = feval_rmspe)\n\n    oof_predictions[val_ind] = model.predict(x_val[features])\n    oof_rmspe = rmspe(y_val, oof_predictions[val_ind])\n    print(f'Fold {fold} RMSPE is {oof_rmspe}')\n    lgb.plot_importance(model, figsize=(5, 2.5), max_num_features=10)\n    plt.show()\n    \n    test_preds += model.predict(df_test[features])\n\ntest_preds = test_preds \/ kfold.n_splits","8c054949":"print(f\"RMSPE on overall oof predictions : {rmspe(y, oof_predictions)}\")\nres = pd.concat([pd.DataFrame(y, columns=['actual']), pd.DataFrame(oof_predictions, columns=['predicted'])], axis=1)\nres.plot(figsize=(15, 6))\nplt.show()","b470d039":"\ntest_preds = pd.DataFrame(test_preds, columns=['target'])\ntest_preds = pd.concat([test, test_preds], axis=1).fillna(0)\ntest_preds[['row_id', 'target']].to_csv('submission.csv', index=False)\ntest_preds.head()","4e1428d9":"# Submission","aba0ad7c":"> Mistake in version 1 notebook \n> * Trade index selection was wrong, now rectified","27c3502e":"# Train LGBM ","2604617f":"# Idea\nCreate features from book data upto where real trade occurs. This will reduce the book order data up to the tune of trade data for each stock and after that aggregate trade data on time ids.","b54cb276":"# Load train and test data","8d64297d":"# Some time ids not present in some stocks\nSome stocks dont have trades in specific time ids for which volatility is to be predicted.\n\nBelow stock 31 dont have trades in time ids [ 985 3987 5539 5629 6197 8753 8840 9208 12011 13377 13663 15010 20017 22498 28186 32174 ] for which volatility is to be predicted .\n\n**For Aggregations on trade data , the above time ids has to be incorporated**","f33b035e":"# Feature Creation Numpy"}}