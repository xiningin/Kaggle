{"cell_type":{"2cc133ae":"code","74267e24":"code","f6e8e6c6":"code","99682e57":"code","5b484d65":"code","b9fb1278":"code","7968254b":"code","bbda7f17":"code","c452600b":"code","814120f9":"code","a2820910":"code","740340cf":"code","b0b64d43":"code","102515d6":"code","cbae8b85":"code","53e3c278":"code","0b0bd307":"code","4ac75aad":"code","7443e8dc":"code","ed07a56f":"markdown","3eb06a75":"markdown"},"source":{"2cc133ae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","74267e24":"# Import necessory libraries\nimport pandas as pd\nimport numpy as np \nimport warnings\nwarnings.filterwarnings('ignore')","f6e8e6c6":"# Explore\ndf_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv',sep=',')\ndf_train.head()","99682e57":"# Shape of the dataset\ndf_train.shape","5b484d65":"# Any nulls?\ndf_train.isnull().sum()","b9fb1278":"def titanic(df):\n# STEP1  After analysing Columns such as 'PassengerId','Cabin','Ticket','Name','Fare','Embarked' do not add values, so we are dropping.    \n    df.drop(columns=['PassengerId','Cabin','Ticket','Name','Fare','Embarked'],axis=1,inplace=True)\n\n# STEP 2   Converting Categorical into Numerical      \n    df['Sex'].replace('male', 1,inplace=True)\n    df['Sex'].replace('female', 0,inplace=True)\n    \n# STEP 3  Since Age is important factor and taking average age in each class w.r.t Age for further imputation    \n    class_one_male = df.Age[(df['Pclass'] == 1) & (df['Sex'] == 1 )].mean()\n    class_one_female = df.Age[(df['Pclass'] == 1) & (df['Sex'] == 0)].mean()\n    class_two_male = df.Age[(df['Pclass'] == 2) & (df['Sex'] == 1)].mean()\n    class_two_female = df.Age[(df['Pclass'] == 2) & (df['Sex'] == 0)].mean()\n    class_three_male = df.Age[(df['Pclass'] == 3) & (df['Sex'] == 1)].mean()\n    class_three_female = df.Age[(df['Pclass'] == 3) & (df['Sex'] == 0)].mean()\n\n    \n# STEP 4  We are seperating passenger of each class w.r.t gender so that we can impute missing age accordingly     \n    df_male_o = df[(df['Pclass'] == 1) & (df['Sex'] == 1)]\n    df_female_o= df[(df['Pclass'] == 1) & (df['Sex'] == 0)]\n    df_male_t = df[(df['Pclass'] == 2) & (df['Sex'] == 1)]\n    df_female_t= df[(df['Pclass'] == 2) & (df['Sex'] == 0)]\n    df_male_th = df[(df['Pclass'] == 3) & (df['Sex'] == 1)]\n    df_female_th= df[(df['Pclass'] == 3) & (df['Sex'] == 0)]\n\n# STEP 5 Null Imputation for Age\n    df_male_o['Age'].fillna(value=class_one_male,inplace=True)\n    df_female_o['Age'].fillna(value=class_one_female,inplace=True)\n    df_male_t['Age'].fillna(value=class_two_male,inplace=True)\n    df_female_t['Age'].fillna(value=class_two_female,inplace=True)\n    df_male_th['Age'].fillna(value=class_three_male,inplace=True)\n    df_female_th['Age'].fillna(value=class_three_female,inplace=True)\n\n# STEP 6 Cancating above datasets   \n    dataframes = [df_male_o,df_female_o,df_male_t,df_female_t,df_male_th,df_female_th]\n    df_structured = pd.concat(dataframes)\n    return df_structured","7968254b":"# Calling function\ndf = titanic(df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv',sep=','))\n\n# Check if any null exists\nprint (df.isnull().sum())","bbda7f17":"# Check for any data loss\nprint ('Final Shape = >',df.shape)\n\n# training data sets\nX_train,y_train = df.drop('Survived',axis=1),df.Survived","c452600b":"# scoring metrics\nfrom sklearn.metrics import scorer","814120f9":"# Logistic model for training\nfrom sklearn.linear_model import LogisticRegression\nlog = LogisticRegression()\nlog.fit(X_train,y_train)","a2820910":"# Importing test set for prediction\nx_test= titanic(pd.read_csv('\/kaggle\/input\/titanic\/test.csv',sep=','))\nlog_pred = log.predict(x_test)\nlog_score = round(log.score(X_train,y_train)* 100,2)\nprint ('Logistic training Score ==>', log_score)","740340cf":"# Knn Classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train,y_train)\nknn_pred = knn.predict(x_test)\nknn_score = round(knn.score(X_train,y_train)* 100,2)\nprint ('Knn training Score ==>', knn_score)","b0b64d43":"# Support Vector Machine\nfrom sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(X_train, y_train)\nsvc_pred = svc.predict(x_test)\nsvc_score = round(svc.score(X_train, y_train) * 100, 2)\nprint ('SVC training Score ==>',svc_score)","102515d6":"# Sthocastic Gradiant Descent Classifier\nfrom sklearn.linear_model import SGDClassifier\nsgd = SGDClassifier()\nsgd.fit(X_train, y_train)\nsgd_pred = sgd.predict(x_test)\nsgd_score = round(sgd.score(X_train, y_train) * 100, 2)\nprint ('SGDC training Score ==>',sgd_score)","cbae8b85":"# DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier(criterion='entropy')\ndecision_tree.fit(X_train, y_train)\ndt_pred = decision_tree.predict(x_test)\ndt_score = round(decision_tree.score(X_train, y_train) * 100, 2)\nprint ('Dic Tree training Score ==>',dt_score)","53e3c278":"# RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier(n_estimators=50,criterion='entropy')\nrandom_forest.fit(X_train, y_train)\nrm_pred = random_forest.predict(x_test)\nrandom_forest.score(X_train, y_train)\nrm_score = round(random_forest.score(X_train, y_train) * 100, 2)\nprint ('Random Forest training Score ==>',rm_score)","0b0bd307":"models = pd.DataFrame({'Classifier':['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Stochastic Gradient Decent', 'Decision Tree'],\n                      'Score':[svc_score,knn_score,log_score,rm_score,sgd_score,dt_score]})\nmodels.sort_values(by='Score',ascending=False)","4ac75aad":"# Decision tree and Random forest seems to be a best classifier, so will be submitting the top results.\ntitanic_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv',sep=',')\nsubmission = pd.DataFrame({'PassengerId': titanic_test['PassengerId'],\n                          'Survived': dt_score})","7443e8dc":"#submission.to_csv('\/kaggle\/input\/titanic\/gender_submission.csv',index=False)","ed07a56f":"In the below function, I am taking care of filling null values based on class and gender, removing unwanted columns.","3eb06a75":"In this kernel I am not going to do any visualization or playing around with unnecessarily columns.\nBased on my assumptions and general idea, I am directly jumping into data preprocessing and building models.\n\nALSO,\nPlease NOTE : I am not tuning parameter in this as it is base model predection. I am going to work on param tuning in the next kernal"}}