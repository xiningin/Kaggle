{"cell_type":{"4821594c":"code","c63f11a0":"code","358993cb":"code","d5be4c03":"code","2d8b8fc8":"code","e7a05778":"code","c718a595":"code","73c88ded":"code","60afc474":"code","6df22f7a":"code","d85b3b1b":"code","34e071d7":"code","a574f75d":"code","b36a1310":"code","4b20167e":"code","f9a50e89":"code","b17c53a0":"code","76bbd980":"code","e802f38f":"code","b714198d":"code","6d37e4bb":"code","19f30d47":"code","830580fe":"code","8f5d0c86":"code","5dbd3473":"code","a27cc51f":"code","05c968e2":"code","56265363":"code","debdb32d":"code","fec5750c":"code","d55727c1":"code","b7e74878":"code","2a8ac712":"code","7518b426":"code","392e678e":"code","1ba3eb88":"code","8f2cd368":"code","845dec83":"code","3169834f":"code","753d71c8":"code","c5b6cff1":"code","8e8f68d2":"code","34453f0b":"code","44f1c04a":"code","84131e70":"code","29ce3aae":"code","7668b1d9":"code","05f448f9":"code","5d8e78c7":"code","0f743c90":"code","55f224c5":"code","cbb11f37":"markdown","2ce7d56e":"markdown","e7345ecd":"markdown","a903caa9":"markdown","98fb2195":"markdown","3bfecd30":"markdown","13495fc2":"markdown","31532fd3":"markdown","5b162a52":"markdown","ea57f836":"markdown","4c4c3ff4":"markdown","1ca3accb":"markdown","342eb729":"markdown","5547ebea":"markdown","d46d7b51":"markdown","71cb762c":"markdown","eb896f5a":"markdown","f92ff795":"markdown","e45b928d":"markdown","889b546b":"markdown","5172f98f":"markdown","b64ce56c":"markdown","433f58fe":"markdown","5f8990c2":"markdown","d9de7c83":"markdown","17da03e7":"markdown","4b9a48dd":"markdown","adeecf66":"markdown","2bfc49af":"markdown"},"source":{"4821594c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c63f11a0":"#load data set\ndata = pd.read_csv(\"..\/input\/marketing-data\/marketing_data.csv\")\nprint(data.info())\ndata.head()","358993cb":"#clean up column names that contain whitespace\ndata.columns = data.columns.str.replace(' ', '',regex=True)\n#data.info()\n\n#trasform income column to a numerical\ndata['Income'] = data['Income'].str.replace('$', '',regex=True)\ndata['Income'] = data['Income'].str.replace(',','').astype('float')\n#data.info()","d5be4c03":"data.head()","2d8b8fc8":"#identify null values \ndata.isnull().sum().sort_values(ascending=False)","e7a05778":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(8,4))\nsns.distplot(data['Income'], kde=True, hist=True)\nplt.title('Income distribution', size=16)\nplt.ylabel('count');\n","c718a595":"data['Income'].describe()","73c88ded":"data['Income'].plot(kind='box', figsize=(3,4), patch_artist=True)","60afc474":"#select columns to plot\ndata_to_plot = data.drop(columns = ['ID', 'AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response', 'Complain']).select_dtypes(include=np.number)","6df22f7a":"data_to_plot.info()","d85b3b1b":"#subplots\ndata_to_plot.plot(subplots=True, layout=(4,4), kind='box', figsize=(12,14), patch_artist=True)\nplt.subplots_adjust(wspace=0.5);","34e071d7":"data = data[data['Year_Birth']>1900].reset_index(drop=True)\n\nplt.figure(figsize=(3,4))\ndata['Year_Birth'].plot(kind='box', patch_artist=True)","a574f75d":"data.info()\ndata.head()","b36a1310":"data['Dt_Customer'] = pd.to_datetime(data['Dt_Customer'])\n\n#data['Dt_Customer'].head()","4b20167e":"list(data.columns)","f9a50e89":"#Dependents\ndata['Dependents'] = data['Kidhome'] + data['Teenhome']\n\n#Year becoming  a Customer\ndata['Year_Customer'] = pd.DatetimeIndex(data['Dt_Customer']).year\n\n#Total amount spent\nmnt_cols = [col for col in data.columns if 'Mnt' in col]\n\ndata['TotalMnt'] = data[mnt_cols].sum(axis=1)\n\n#totoal Purchases\npurchases_cols = [col for col in data.columns if  'Purchases' in col]\ndata['TotalPurchases'] = data[purchases_cols].sum(axis=1)\n\n# Total Campaigns Accepted\ncampaigns_cols = [col for col in data.columns if 'Cmp' in col] + ['Response']\ndata['TotalCampaignsAcc'] = data[campaigns_cols].sum(axis=1)\n\n# view new features, by customer ID\ndata[['ID', 'Dependents', 'Year_Customer', 'TotalMnt', 'TotalPurchases', 'TotalCampaignsAcc']].head()","b17c53a0":"#calculation correlation matrix \n## using non-parametric test of correlation by using kendall method since feature are binary\ncorrs = data.drop(columns='ID').select_dtypes(include=np.number).corr(method='kendall')\n\n#plot clustered headmap of correlations\nsns.clustermap(corrs, cbar_pos=(-0.05, 0.8, 0.05, 0.18), cmap='coolwarm', center=0);","76bbd980":"sns.lmplot(x='Income', y='TotalMnt', data=data[data['Income']<200000]);","e802f38f":"plt.figure(figsize=(4,4))\nsns.boxplot(x='Dependents', y='TotalMnt', data=data);\n#look there no correlation beatween them","b714198d":"plt.figure(figsize=(4,4))\nsns.boxplot(x='Dependents', y='NumDealsPurchases', data=data);\n#more dependents more number purchased","6d37e4bb":"plt.figure(figsize=(4,4))\nsns.boxplot(x='TotalCampaignsAcc', y='Income', data=data[data['Income'] < 200000])","19f30d47":"plt.figure(figsize=(4,4))\nsns.boxplot(x='TotalCampaignsAcc', y='Dependents', data=data)","830580fe":"sns.lmplot(x='NumWebVisitsMonth',y='NumWebPurchases', data=data)","8f5d0c86":"sns.lmplot(x='NumWebVisitsMonth',y='NumDealsPurchases',data=data)","5dbd3473":"plt.figure(figsize=(8,3))\ndata['NumStorePurchases'].hist(bins=12,grid=False)');\nplt.title('NumStorePurchases distribution', size=16)\nplt.ylabel('count","a27cc51f":"# drop unique ID\ndata.drop(columns=['ID','Dt_Customer'], inplace=True)","05c968e2":"# one-hot encoding of categorical features\nfrom sklearn.preprocessing import OneHotEncoder\n\n# get categorical features and review number of unique values\ncat = data.select_dtypes(exclude=np.number)\nprint(\"Number of unique values per categorical feature:\\n\", cat.nunique())\n\n# use one hot encoder\nenc = OneHotEncoder(sparse=False).fit(cat)\ncat_encoded = pd.DataFrame(enc.transform(cat))\ncat_encoded.columns = enc.get_feature_names(cat.columns)\n\n# merge with numeric data\nnum = data.drop(columns=cat.columns)\ndata2 = pd.concat([cat_encoded, num], axis=1)\ndata2.head()","56265363":"np.isnan(data2.any()) #and gets False\n","debdb32d":"np.all(np.isfinite(data2))","fec5750c":"np.any(np.isnan(data2))","d55727c1":"df2 = data2.apply (pd.to_numeric, errors='coerce')\ndf2 = data2.dropna()\n\nprint (df2)","b7e74878":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# isolate X and y variables, and perform train-test split\nX = df2.drop(columns='NumStorePurchases')\ny = df2['NumStorePurchases']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\n# linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# predictions\npreds = model.predict(X_test)\n\n# evaluate model using RMSE\nprint(\"Linear regression model RMSE: \", np.sqrt(mean_squared_error(y_test, preds)))\nprint(\"Median value of target variable: \", y.median())","2a8ac712":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(model, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist(), top=5)","7518b426":"import shap\n\n#calculate shap values\nex = shap.Explainer(model, X_train)\nshap_values = ex(X_test)\n\n#plot\nplt.title('SHAP summary for NumStorePurchases', size=16)\nshap.plots.beeswarm(shap_values, max_display=5)","392e678e":"plt.figure(figsize=(5,4))\ndata.groupby('Country')['TotalPurchases'].sum().sort_values(ascending=False).plot(kind='bar')\nplt.title('Total Number of Purchases by Country', size=16)\nplt.ylabel('Number of Purchases')","1ba3eb88":"sns.lmplot(x='MntGoldProds',y='NumStorePurchases', data = data);","8f2cd368":"from scipy.stats import kendalltau\n\nkendall_corr = kendalltau(x=data['MntGoldProds'], y=data['NumStorePurchases'])\n\n#print results\nprint('Kendall correlation (tau): ', kendall_corr.correlation)\nprint('Kendall p-value:', kendall_corr.pvalue)","845dec83":"df2['Marital_Status_Married'].describe()","3169834f":"# sum the marital status and phd dummy variables - the Married+PhD group will have value of 2\ndf2['Married_PhD'] = df2['Marital_Status_Married'] + df2['Education_PhD']\ndf2['Married_PhD'] = df2['Married_PhD'].replace({2:'Married-PhD', 1:'Other', 0:'Other'})\n\n#plot MntFistProducts between Married-phD and others\nplt.figure(figsize=(2.5,4))\nsns.boxplot(x='Married_PhD', y='MntFishProducts', data=df2);","753d71c8":"#independent t-test p-value\nfrom scipy.stats import ttest_ind\npval = ttest_ind(df2[df2['Married_PhD'] == 'Married-PhD']['MntFishProducts'],  df2[df2['Married_PhD'] == 'Other']['MntFishProducts']).pvalue\nprint(\"t-test p-value: \", round(pval,3))","c5b6cff1":"plt.figure(figsize=(8,3))\n#sns.distplot(data['MntFishProducts'], kde=False, hist=True, bins=12)\ndata['MntFishProducts'].hist(bins=12,grid=False)\nplt.title('MntFishProducts distribution', size=16)\nplt.ylabel('count');","8e8f68d2":"df2.info()","34453f0b":"# isolate X and y variables, and perform train-test split\nX = df2.drop(columns=['MntFishProducts','Married_PhD'])\ny = df2['MntFishProducts']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\n# linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# predictions\npreds = model.predict(X_test)\n\n# evaluate model using RMSE\nprint(\"Linear regression model RMSE: \", np.sqrt(mean_squared_error(y_test, preds)))\nprint(\"Median value of target variable: \", y.median())","44f1c04a":"perm = PermutationImportance(model, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist(), top=7)","84131e70":"import shap\n\n#calculate shap value\nex = shap.Explainer(model, X_train)\nshap_values = ex(X_test)\n\n#plot\nplt.title('SHAP summary for MntFishProducts', size=16)\nshap.plots.beeswarm(shap_values, max_display=7)","29ce3aae":"import statsmodels.formula.api as smf\nimport statsmodels as sm\nfrom scipy import stats\n\n## get the data of interest for glm\ndf_cam_wide = data[['Country', 'AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response']]\n\n## to store statistics results\nstat_results = []\n\n## perform glm\nfor col in df_cam_wide.drop(columns='Country').columns:\n    this_data = df_cam_wide[['Country', col]]\n    \n     # define formula\n    formula = col+'~Country'\n    \n    # logistic regression (family=binomial)\n    model = smf.glm(formula = formula, data=this_data, family=sm.genmod.families.Binomial())\n    result = model.fit()\n    \n        # get chisquare value for overall model (CampaignAccepted ~ Country) and calculate p-value\n    chisq = result.pearson_chi2\n    pval = stats.distributions.chi2.sf(chisq , 7) # Df Model = 7 degrees of freedom when you run result.summary()\n     \n    # append to stat_results\n    stat_results.append(pval)\n    \n    # print stat summary for entire model\n    print(result.summary())\n    \n## check results\nprint(\"\\nChisq p-values: \", stat_results)","7668b1d9":"# calculate success rate (percent accepted)\ncam_success = pd.DataFrame(data[['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', \n                               'AcceptedCmp4', 'AcceptedCmp5', \n                               'Response']].mean()*100,\n                           columns=['Percent']).reset_index()\n                           \n#plot\nsns.barplot(x='Percent', y='index', data=cam_success.sort_values('Percent'), palette = 'Blues')\nplt.xlabel('Accepted (%)')\nplt.ylabel('Campaign')\nplt.title('Marketing campaign success rate', size=16);","05f448f9":"# list of cols with binary responses\nbinary_cols = [col for col in data.columns if 'Accepted' in col] + ['Response', 'Complain']\n\n# list of cols for spending \nmnt_cols = [col for col in data.columns if 'Mnt' in col]\n\n# list of cols for channels\nchannel_cols = [col for col in data.columns if 'Num' in col] + ['TotalPurchases', 'TotalCampaignsAcc']","5d8e78c7":"# average customer demographics\ndemographics = pd.DataFrame(round(data.drop(columns=binary_cols+mnt_cols+channel_cols).mean(), 1), columns=['Average']).reindex([\n    'Year_Birth', 'Year_Customer', 'Income', 'Dependents', 'Kidhome', 'Teenhome', 'Recency'])\n\ndemographics","0f743c90":"spending = pd.DataFrame(round(data[mnt_cols].mean(), 1), columns=['Average']).sort_values(by='Average').reset_index()\n\n# plot\nax = sns.barplot(x='Average', y='index', data=spending, palette='Blues')\nplt.ylabel('Amount spent on...')\n\n## add text labels for each bar's value\nfor p,q in zip(ax.patches, spending['Average']):\n    ax.text(x=q+40,\n           y=p.get_y()+0.5,\n            s=q,\n            ha=\"center\");","55f224c5":"channels = pd.DataFrame(round(data[channel_cols].mean(), 1), columns=['Average']).sort_values(by='Average').reset_index()\n\n# plot\nax = sns.barplot(x='Average', y='index', data=channels, palette='Blues')\nplt.ylabel('Number of...')\n\n## add text labels for each bar's value\nfor p,q in zip(ax.patches, channels['Average']):\n    ax.text(x=q+0.8,\n            y=p.get_y()+0.5,\n            s=q,\n            ha=\"center\") ;","cbb11f37":"* Explore the directionality of these effects, using SHAP values:\n* Findings:\nThe amount spent on fish increases with higher total amount spent ('TotalMnt')\nThe amount spent on fish decreases with higher amounts spent on wine, meat, gold, fruit, or sweets ('MntWines', 'MntMeatProducts', 'MntGoldProds', 'MntSweetProducts', 'MntFruits')\n* Interpretation:\nCustomers who spend the most on fish are those who spend less on other products (wine, meat, gold, fruit, and sweets)","2ce7d56e":"Plots illustrating the positive effect of income and negative effect of having kids & teens on advertising campaign acceptance:\nFor the purposes of the following plot, limiting income to < 200000 to remove outlier","e7345ecd":"# Your supervisor insists that people who buy gold are more conservative. Therefore, people who spent an above average amount on gold in the last 2 years would have more in store purchases. Justify or refute this statement using an appropriate statistical test\n* Plot relationship between amount spent on gold in the last 2 years (MntGoldProds) and number of in store purchases (NumStorePurchases):\n* Findings: There is a positive relationship, but is it statistically significant?","a903caa9":"Fit linear regression model to training data (70% of dataset)\nEvaluate predictions on test data (30% of dataset) using RMSE:\n*     Findings: The RMSE is exceedingly small compared to the median value of the target variable, indicating good model predictions","98fb2195":"# Fish has Omega 3 fatty acids which are good for the brain. Accordingly, do \"Married PhD candidates\" have a significant relation with amount spent on fish?\n* We will compare MntFishProducts between Married PhD candidates and all other customers:\n    Findings: Married PhD candidates spend significantly less on fish products compared to other customers.","3bfecd30":"Statistical summary of regional effects on campaign success:\n\n    * Methodology: Performed logistic regression for Campaign Accepted by Country, reporting Chisq p-value for overall model.\n    \n    * Findings: The regional differences in advertising campaign success are statistically significant.","13495fc2":"Perform Kendall correlation analysis (non-parametric test since MntGoldProducts is not normally distributed and contains outliers):\n* Findings: There is significant positive correlation between MntGoldProds and NumStorePurchases","31532fd3":"Plot illustrating negative effect of having dependents (kids & teens) on spending:","5b162a52":"Plot illustrating positive effect of having dependents (kids & teens) on number of deals purchased:","ea57f836":"Indentify features container outliers\nlikely indicate data entry errors are Year_Birth <= 1900","4c4c3ff4":"Drop uninformative features\n    * ID is unique to each customer\n    * Dt_Customer will be dropped in favor of using engineered         variable Year_Customer\nPerform one-hot encoding of categorical features, encoded data shown below:","1ca3accb":"manipulation feature\nReview a list of the feature names below, from which we can engineer:\n* The total number of dependents in the home ('Dependents') can be engineered from the sum of 'Kidhome' and 'Teenhome'\n* The year of becoming a customer ('Year_Customer') can be engineered from 'Dt_Customer'\n* The total amount spent ('TotalMnt') can be engineered from the sum of all features containing the keyword 'Mnt'\n* The total purchases ('TotalPurchases') can be engineered from the sum of all features containing the keyword 'Purchases'\n* The total number of campains accepted ('TotalCampaignsAcc') can be engineered from the sum of all features containing the keywords 'Cmp' and 'Response' (the latest campaign)","342eb729":"Investigate anomaly:\nNumber of web visits in the last month is not positively correlated with number of web purchases\nInstead, it is positively correlated with the number of deals purchased, suggesting that deals are an effective way of stimulating purchases on the website","5547ebea":"# What other factors are significantly related to amount spent on fish?\nLike with the analysis of NumStorePurchases above, we will use use a linear regression model with MntFishProducts as the target variable, and then use machine learning explainability techniques to get insights about which features predict the amount spent on fish\nBegin by plotting the target variable:","d46d7b51":"To identify patterns, we will first identify feature correlations. Positive correlations between features appear red, negative correlations appear blue, and no correlation appears grey in the clustered heatmap below.","71cb762c":"Exploratory Data Analysis","eb896f5a":"# # Section 03: Data Visualization\nWhich marketing campaign is most successful?\u00b6\n\n* Plot marketing campaign overall acceptance rates:\n\n* Findings: The most successful campaign is the most recent (column name: Response)","f92ff795":"# What does the average customer look like for this company?\nBasic demographics: The average customer is...\n*     Born in 1969\n*     Became a customer in 2013\n*     Has an income of roughly $52,000 per year\n*     Has 1 dependent (roughly equally split between kids or teens)\n*     Made a purchase from our company in the last 49 days","e45b928d":"The Dt_Customer column should be transformed to datetime format","889b546b":"Plot illustrating the effect of high income on spending:\nFor the purposes of this plot, limiting income to < 200000 to remove outlier","5172f98f":"* Fit linear regression model to training data (70% of dataset)\n* Evaluate predictions on test data (30% of dataset) using RMSE:\n     * Findings: The RMSE is exceedingly small compared to the median value of the target variable, indicating good model predictions","b64ce56c":"* Identify features that significantly affect the amount spent on fish, using permutation importance:\n* Significant features:\n    'TotalMnt', 'MntWines', 'MntMeatProducts', 'MntGoldProds', 'MntSweetProducts', 'MntFruits'\n    All other features are not significant","433f58fe":"# Which products are performing best?\nThe average customer spent...\n\n* $25-50 on Fruits, Sweets, Fish, or Gold products\n* Over $160 on Meat products\n* Over $300 on Wines\n* Over $600 total\n\nProducts performing best:\n\n* Wines\n* Followed by meats","5f8990c2":"# Does US fare significantly better than the Rest of the World in terms of total purchases?\n* Plot total number of purchases by country:\n*     Findings:\n        Spain (SP) has the highest number of purchases\n        US is second to last, therefore the US does not fare better than the rest of the world in terms of the total number of purchases","d9de7c83":"Explore the directionality of these effects, using SHAP values:\n* Findings:\n    *     The number of store purchases increases with higher number of total purchases ('TotalPurchases')\n    *     The number of store purchases decreases with higher number of catalog, web, or deals purchases ('NumCatalogPurchases', 'NumWebPurchases', 'NumDealsPurchases')\n* Interpretation:\n    Customers who shop the most in stores are those who shop less via the catalog, website, or special deals","17da03e7":"Identify features that significantly affect the number of store purchases, using permutation importance:\nSignificant features:\n'TotalPurchases', 'NumCatalogPurchases', 'NumWebPurchases', 'NumDealsPurchases'\nAll other features are not significant","4b9a48dd":"# Section 02: Statistical Analysis\n**What factors are significantly related to the number of store purchases?**\n\nWe will use use a linear regression model with NumStorePurchases as the target variable, and then use machine learning explainability techniques to get insights about which features predict the number of store purchases\nBegin by plotting the target variable:","adeecf66":"are there any variables that warrant transformations?","2bfc49af":"Remove rows where Year_Birth <= 1900:"}}