{"cell_type":{"053a71c2":"code","14ec7093":"code","1cf8a5b8":"code","90027f43":"code","99abb44b":"code","fb6d7060":"code","32ea08f0":"code","ffd336ca":"code","e4d078d5":"code","47199e41":"code","8f3e16b5":"code","cc0cb9d1":"code","e9cd8751":"code","b2e7fd0a":"code","8d0016dc":"code","b2716c46":"code","eae749f3":"code","2002a357":"code","9f2e198d":"code","3638fb25":"code","71d38641":"code","5decf82d":"code","3aafdf9c":"code","1fbf3f7e":"code","1192a653":"code","93683f89":"code","5c4e6307":"code","2d85ba6f":"code","6e97c0b3":"code","42b03b0f":"code","858973b7":"code","09bf7f37":"code","d2a982b1":"code","d40462cd":"code","85282dcd":"code","f8e5e700":"code","c8151a8d":"code","85f3c519":"code","d4900468":"code","d93aff46":"code","50c5ad1d":"code","28db62f8":"code","8cb66d6e":"code","6c5f5a7a":"code","ff6259b4":"code","9ff963cc":"code","4e719529":"code","f4e72525":"code","4ddb4e7e":"code","bd11da1d":"code","b6b28df2":"code","70e8d514":"code","ab97e212":"code","44177a1f":"code","064de95a":"code","a561e496":"code","79a57210":"code","ba2a5e02":"code","2e76b1d8":"code","f3647fd0":"code","a45d78a2":"code","81d2d48a":"code","7a509c1f":"code","5d765f71":"code","31cde73f":"code","118e84c4":"code","20bf8c83":"code","6f505ecd":"code","7dec2394":"code","f6192917":"code","511062d8":"code","fb6c82c3":"markdown","8cd75bf3":"markdown","223a797e":"markdown","080c5e56":"markdown","8f72058b":"markdown","87577d90":"markdown","64350d71":"markdown","65260c4e":"markdown","4fe57a57":"markdown","3d293a43":"markdown","3693b88c":"markdown","3d3d456f":"markdown","9c09a926":"markdown","da90754b":"markdown","229c598b":"markdown","f82d5910":"markdown","20d0a2d2":"markdown","503c0d5e":"markdown","d5b8d802":"markdown","6e8dd89c":"markdown","77d03b16":"markdown","085cc438":"markdown","f73e4407":"markdown","ec6abf13":"markdown","2bbd1048":"markdown","7b88cdba":"markdown","699c39e6":"markdown","8121841e":"markdown","a376e44c":"markdown","31965108":"markdown","1010241b":"markdown","a70528ca":"markdown","1a31d151":"markdown"},"source":{"053a71c2":"from IPython.display import Image\nImage(url= \"https:\/\/static1.squarespace.com\/static\/5006453fe4b09ef2252ba068\/5095eabce4b06cb305058603\/5095eabce4b02d37bef4c24c\/1352002236895\/100_anniversary_titanic_sinking_by_esai8mellows-d4xbme8.jpg\")","14ec7093":"import warnings\nwarnings.filterwarnings('ignore')","1cf8a5b8":"# Importing the Required Libraries\nimport numpy as np\nimport pandas as pd\nimport missingno as msno\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\npd.set_option('display.max_columns',100)\npd.set_option('display.max_row',100)\npd.options.display.max_columns=100\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report,confusion_matrix","90027f43":"train=pd.read_csv('..\/input\/titanic\/train.csv')\ntest=pd.read_csv('..\/input\/titanic\/test.csv')","99abb44b":"train.head()","fb6d7060":"train.info()","32ea08f0":"test.head()","ffd336ca":"test.info()","e4d078d5":"print(\"Train Dimensions:\",train.shape)\nprint(\"Test Dimensions:\",test.shape)","47199e41":"train.describe(percentiles=[.25,.5,.75,.90,.95,.99])","8f3e16b5":"test.describe(percentiles=[.25,.5,.75,.90,.95,.99])","cc0cb9d1":"# Train Data Set Missing Value\nmsno.bar(train)\n","e9cd8751":"# Test Data Set Missing Value\n\nmsno.bar(test)","b2e7fd0a":"missing=pd.concat([train.isnull().sum()\/train.shape[0]*100,test.isnull().sum()\/train.shape[0]*100],axis=1,keys=['Train_Datset_Percentage','Test_Dataset_Percentage'])\nprint(missing)    ","8d0016dc":"# Class vs Survived\nprint(train[['Pclass','Survived']].groupby(['Pclass'],as_index=False).mean())","b2716c46":"# Gender vs Survived\nprint(train[['Sex','Survived']].groupby(['Sex'],as_index=False).mean())","eae749f3":"# Sibsp vs Survived\nprint(train[['SibSp','Survived']].groupby(['SibSp'],as_index=False).mean())","2002a357":"# Parch vs Survived\nprint(train[['Parch','Survived']].groupby(['Parch'],as_index=False).mean())","9f2e198d":"# We can change the Missing Value with Median or we can check the Average age by the Passenger Class and then impute the Missing Value","3638fb25":"plt.figure(figsize=(12,7))\nsns.boxplot(x='Pclass',y='Age',data=train,palette='winter')\nplt.show()","71d38641":"#train['Age']=train['Age'].fillna(train['Age'].median())\n#test['Age']=test['Age'].fillna(test['Age'].median())\n\ndef age_impute(cols):\n    Age=cols[0]\n    global Pclass\n    Pclass=cols[1]\n   \n    if pd.isnull(Age):\n        if Pclass == 1:\n            return 37\n        if Pclass == 2:\n            return 29\n        else:\n            return 24\n        \n    else:\n        return Age","5decf82d":"# Applying the Age_Impute Function\ntrain['Age']=train[['Age','Pclass']].apply(age_impute,axis=1)\ntest['Age']=train[['Age','Pclass']].apply(age_impute,axis=1)","3aafdf9c":"# Checking for any Missing Value in the Age Column\ndisplay(train.Age.isnull().sum())\ndisplay(test.Age.isnull().sum())","1fbf3f7e":"#Imputing the Missing Value with Mode as it is a Categorical Variable\ntrain['Embarked']=train.Embarked.fillna(train.Embarked.mode()[0])\n","1192a653":"# Imputing the Missing Fare Value with median\ntest['Fare']=test.Fare.fillna(test.Fare.median())","93683f89":"plt.figure(figsize=(8,5))\nsns.countplot('Survived',hue='Sex',data=train,palette='dark')\nplt.show()","5c4e6307":"plt.figure(figsize=(20,6))\nsns.violinplot(x='Sex',y='Age', hue='Survived',data=train, split=True,palette={0:'r',1:'g'})\nplt.show()","2d85ba6f":"plt.figure(figsize=(9,6))\nsns.barplot(y=\"Survived\", x=\"Sex\", hue=\"Pclass\", data=train, palette=\"gist_rainbow_r\")\nplt.show()","6e97c0b3":"plt.figure(figsize=(8,5))\nsns.violinplot(x=\"Survived\", y = \"Age\",data = train,palette='cool',size=6)\nplt.show()","42b03b0f":"plt.figure(figsize=(8,5))\nsns.barplot('Embarked','Survived',data=train,palette='ocean')\nplt.show()","858973b7":"fig = plt.figure(figsize=(25, 7))\nsns.violinplot(x='Embarked', y='Fare', hue='Survived', data=train, split=True, palette={0: \"r\", 1: \"g\"})\nplt.show()","09bf7f37":"sns.pairplot(train)\nplt.show()","d2a982b1":"# introducing other features based on the family size\ntrain['FamilySize'] = train['Parch'] + train['SibSp'] + 1\ntest['FamilySize'] = test['Parch'] + test['SibSp'] + 1\n    \n\ntrain['Singleton'] = train['FamilySize'].map(lambda s: 1 if s == 1 else 0)\ntrain['SmallFamily'] = train['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)\ntrain['LargeFamily']=train['FamilySize'].map(lambda s: 1 if 5 <= s else 0)\n    \n\ntest['Singleton'] = test['FamilySize'].map(lambda s: 1 if s == 1 else 0)\ntest['SmallFamily'] = test['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)\ntest['LargeFamily']=test['FamilySize'].map(lambda s: 1 if 5 <= s else 0)\n    \n    ","d40462cd":"plt.figure(figsize=(9,6))\nsns.boxplot(y=\"Age\",x=\"FamilySize\",hue=\"Survived\", data=train,palette='spring_r')\nplt.show()","85282dcd":"train_test_data=[train,test]\nembarked_mapping = {\"S\": 0, \"C\": 1, \"Q\": 2}\nfor dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].map(embarked_mapping)\n \n# Here In dataset all the values is mapped in train and test data set","f8e5e700":"train_test_data=[train,test]\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n","c8151a8d":"train['Title'].value_counts()\n","85f3c519":"test['Title'].value_counts()","d4900468":"# Mapping the Tit\nTitle_Dictionary = {\n    \"Capt\": \"Officer\",\n    \"Col\": \"Officer\",\n    \"Major\": \"Officer\",\n    \"Jonkheer\": \"Royalty\",\n    \"Don\": \"Royalty\",\n    \"Sir\" : \"Royalty\",\n    \"Dr\": \"Officer\",\n    \"Rev\": \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Mme\": \"Mrs\",\n    \"Mlle\": \"Miss\",\n    \"Ms\": \"Mrs\",\n    \"Mr\" : \"Mr\",\n    \"Mrs\" : \"Mrs\",\n    \"Miss\" : \"Miss\",\n    \"Master\" : \"Royalty\",\n    \"Lady\" : \"Royalty\"\n}\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Title'].map(Title_Dictionary)","d93aff46":"# encoding in dummy variable\ntitles_dummies = pd.get_dummies(train['Title'], prefix='Title')\ntrain = pd.concat([train, titles_dummies], axis=1)\n    \ntitles_dummies = pd.get_dummies(test['Title'], prefix='Title')\ntest = pd.concat([test, titles_dummies], axis=1)","50c5ad1d":"map_item={'male':0,'female':1}\ntrain['Sex']=train['Sex'].map(map_item)\ntest['Sex']=test['Sex'].map(map_item)","28db62f8":"# Removing the Unnecssary Columns\ntrain=train.drop(['Name','Ticket','Cabin','Title','SibSp','Parch','PassengerId'],axis=1)\ntest=test.drop(['Name','Ticket','Cabin','Title','SibSp','Parch'],axis=1)\nPasseengerID_test=test.pop('PassengerId')\n","8cb66d6e":"train.head()","6c5f5a7a":"train.columns","ff6259b4":"train.info()","9ff963cc":"scaler = StandardScaler()\n\ntrain[['Pclass','Age','Fare','FamilySize']] = scaler.fit_transform(train[['Pclass','Age','Fare','FamilySize']])\ntrain.head()","4e719529":"test[['Pclass','Age','Fare','FamilySize']] = scaler.transform(test[['Pclass','Age','Fare','FamilySize']])","f4e72525":"Y_train=train.pop('Survived')\ntrain.shape, test.shape,Y_train.shape","4ddb4e7e":"from sklearn.linear_model import LogisticRegression\nLR=LogisticRegression()\n# Selecting the Significant Features using RFE\nfrom sklearn.feature_selection import RFE\nrfe = RFE(LR, 11)             # running RFE with 11 variables as output\nrfe = rfe.fit(train, Y_train)","bd11da1d":"list(zip(train.columns,rfe.support_,rfe.ranking_))","b6b28df2":"col = train.columns[rfe.support_]\ncol","70e8d514":"from sklearn.linear_model import LogisticRegression\nLR=LogisticRegression()","ab97e212":"# Fitting the model on the Train dataset.\nLR.fit(train,Y_train)","44177a1f":"# Making Predictions on the Test Data set\ny_pred = LR.predict(test)","064de95a":"# Calculating the Accuracy of the model.\n\nprint(\"Accuracy:\",round(LR.score(train, Y_train)*100,2))","a561e496":"\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)\nclf = SVC()\n\nscoring = 'accuracy'\nscore = cross_val_score(clf, train[col], Y_train, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\nround(np.mean(score)*100, 2)","79a57210":"\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nscoring = 'accuracy'\nscore = cross_val_score(clf, train[col], Y_train, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\n# Random Forest Score\nround(np.mean(score)*100, 2)","ba2a5e02":"# Using SVM to predict the test set as we are getting Better result from there\n# Hypertuning the Parameter\n\nfrom sklearn.model_selection import GridSearchCV \n  \n# defining parameter range \nparam_grid = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf']}  \n  \ngrid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3) \n  \n# fitting the model for grid search \ngrid.fit(train[col], Y_train)","2e76b1d8":"# print best parameter after tuning \nprint(grid.best_params_) \n# print how our model looks after hyper-parameter tuning \nprint(grid.best_estimator_) ","f3647fd0":"clf = SVC(C= 10, gamma= 0.01, kernel= 'rbf')\n","a45d78a2":"clf.fit(train[col], Y_train)","81d2d48a":"\n# Making Predictions on the Test Data set\ny_pred = clf.predict(test[col])","7a509c1f":"submission = pd.DataFrame({\n        \"PassengerId\": PasseengerID_test,\n        \"Survived\": y_pred\n    })\n\nfilename = 'Titanic Predictions.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","5d765f71":"from IPython.display import Image\nImage(url= \"https:\/\/images.unsplash.com\/photo-1543118141-8598f6bfbc0a?ixlib=rb-1.2.1&auto=format&fit=crop&w=500&q=60\")","31cde73f":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout\n\nclassifier = Sequential()\nclassifier.add(Dense(activation=\"relu\", input_dim=11, units=11, kernel_initializer=\"uniform\"))\nclassifier.add(Dense(activation=\"relu\", units=9, kernel_initializer=\"uniform\"))\nclassifier.add(Dropout(0.5))\nclassifier.add(Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\"))\nclassifier.add(Dropout(0.5))\nclassifier.add(Dense(activation=\"relu\", units=3, kernel_initializer=\"uniform\"))\nclassifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nclassifier.summary()\n","118e84c4":"train.shape","20bf8c83":"Y_train.values","6f505ecd":"features=train[col].values","7dec2394":"history=classifier.fit(features, Y_train.values, batch_size = 10, epochs = 100,\n    validation_split=0.1,verbose = 1,shuffle=True)","f6192917":"Y_pred = classifier.predict(test[col])\nY_pred=Y_pred.round()","511062d8":"submission = pd.DataFrame({\n        \"PassengerId\": PasseengerID_test,\n        \"Survived\": y_pred\n    })\n\nfilename = 'Titanic Predictions_1_Deep_Learning.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","fb6c82c3":"# Making Prediction","8cd75bf3":"### Observation:\n1. We can see the Cabin Column has more than 77% Missing Value so Imputing the missing Cabin Values will make no Sense\n2. We will Impute the Missing values for Age, Embarked and Fare Column","223a797e":"## 2) Embarked","080c5e56":"## 1) Age\n    ","8f72058b":"## Trying with Deep Learning Approach","87577d90":"**Insights:From the Bar plot we can see High number of missing Value in Cabin and Age Column for both Train and Test Data Frame**","64350d71":"## Visualization","65260c4e":"## Thank you for your Time","4fe57a57":"#### ****Insights****: We can see the wealtheir Passengers is in the higher Class Tends to be be Older which is making some Sense. We'll use the average age values to impute the missing value based on PClass for Age","3d293a43":"### 3) Sex","3693b88c":"## 3) Fare","3d3d456f":"## Titanic Disaster ","9c09a926":"## Modeling and Feature Selection","da90754b":"## Feature Engineering","229c598b":"## Analyzing the Data","f82d5910":"### Importing the Libraries","20d0a2d2":"**Reading and Understanding the Data**","503c0d5e":"### Handling Missing Values","d5b8d802":"### Observation: \n1. It seems that Embarkation C have a wider Range of Fare Tickets and therefore Passengers who pay highest Prices are those who Survive.\n2. We can also see this happening in embarkation S and less in Embarkation Q.","6e8dd89c":"### 3.Name","77d03b16":"* **Numerical Features**: PassengerId,PClass,Age,Fare,SibSp,Parch\n* **Categorical Features**: PClass,Name,Sex,Embarked,Survived","085cc438":"### Data Dictionary:\n\n* survival - Survival (0 = No; 1 = Yes)\n* class - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n* name - Name\n* sex - Sex\n* age - Age\n* sibsp - Number of Siblings\/Spouses Aboard\n* parch - Number of Parents\/Children Aboard\n* ticket - Ticket Number\n* fare - Passenger Fare\n* cabin - Cabin\n* embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n","f73e4407":"### 1. Family Size","ec6abf13":"## Checking with SVM\n","2bbd1048":"### 1.Logistic Regression: \nLogisitic Regression is a statistical model that use sigmoid function to estimate the probability of an event occuring having been given some Previous[](http:\/\/) Data.","7b88cdba":"### Feature Scaling ","699c39e6":"## Checking with Random Forest","8121841e":"### Observation: \n1. It Clearly shows that Female Passenger from all Class Resuced First.\n2. P class 1 have higher Survial Rate in both Male and Female","a376e44c":"* **Numerical Features**: PassengerId,PClass,Age,Fare,SibSp,Parch\n* **Categorical Features**: PClass,Name,Sex,Embarked","31965108":"### Observation : \nYoung Passsengers have high survival Rate","1010241b":"### 2. Embarked","a70528ca":"**Observation**: People with Small Families tend to survive more as comapred to people with more than 3 members","1a31d151":"### Observations:\n1. Women Survive more than men as depicated by the larger brown Histogram\n2. The Age Conditions for the Survival male passengers are :\n   Younger males tends to survive more and large number of Passengers between 20 and 40 succumb.\n3. Violin Plot confirms that the sailors and caption follow the same old code of Conduct in threatening situation \" Women and Children First\""}}