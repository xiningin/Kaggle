{"cell_type":{"b647f1e6":"code","f4411b31":"code","9acfa48a":"code","25eab032":"code","fc755de1":"code","1a1d4bed":"code","c460e15a":"code","8eaf6c19":"code","bd4e7ba4":"code","8d332acc":"code","6c2f76e8":"code","b58982d5":"code","ff6d941a":"code","6609db6e":"code","9aafd9ff":"code","8c420cdf":"code","d913e470":"code","770bdb18":"code","cebc3e06":"code","9f213b50":"code","c68fd17e":"code","edd02b42":"code","f91a8f5a":"code","749da857":"code","34651d7b":"code","30a86ad9":"code","d7018573":"code","ca06fe90":"code","d7835c40":"code","02edddae":"code","de70544b":"code","131fad18":"code","78e1f22e":"code","6ba75390":"code","621b89a8":"code","49933791":"code","bf460028":"markdown","0eeb39a9":"markdown","3aace210":"markdown","5d795e48":"markdown","cb0ffbe9":"markdown","70c6a03f":"markdown","c1c8d9e7":"markdown","947c3d58":"markdown","523d5f30":"markdown","5f82b990":"markdown","a252994f":"markdown","6d0c17d3":"markdown","2b023741":"markdown","531a53f6":"markdown","050b14c5":"markdown","f99c35e5":"markdown","42ba8638":"markdown","1a5b8268":"markdown","8fb10f93":"markdown","eb5686f6":"markdown","418a76e1":"markdown","49a38538":"markdown","24fa05b6":"markdown","1064c752":"markdown","bbc9d871":"markdown","df5e2706":"markdown","e79b2f07":"markdown","e41ddc35":"markdown","24dc8ddc":"markdown","3b97078b":"markdown","9204c60f":"markdown","59ed11fb":"markdown","5228fe69":"markdown","50b36ca4":"markdown","4b681feb":"markdown","9359df53":"markdown","630b5292":"markdown","74960fde":"markdown","545937ef":"markdown","dcce3213":"markdown","05137feb":"markdown","d3de8e1f":"markdown","0f68242e":"markdown","8f4cdf35":"markdown","e2584d0a":"markdown","27047a0d":"markdown","23e5bbd4":"markdown","ae333076":"markdown","9be82911":"markdown","2b562a2e":"markdown","1ed60c93":"markdown","514f7199":"markdown"},"source":{"b647f1e6":"import sys\nimport math\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport scipy\n\nfrom sklearn.metrics import confusion_matrix,auc,roc_auc_score,roc_curve,recall_score, precision_score, accuracy_score, f1_score\nfrom sklearn.preprocessing import StandardScaler,normalize\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\n\nimport itertools\n\nplt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\ndataset = pd.read_csv(\"..\/input\/creditcard.csv\")\ndf = pd.read_csv(\"..\/input\/creditcard.csv\")\n# Any results you write to the current directory are saved as output.","f4411b31":"dataset.head()","9acfa48a":"dataset.describe()","25eab032":"print(\"Procent ca\u0142kowitych transakcji oszuka\u0144czych\")\nprint(str(dataset[\"Class\"].mean()*100) + '%')","fc755de1":"print(\"Straty spowodowane oszustwem:\")\nprint(\"Ca\u0142kowita kwota utracona w wyniku oszustwa\")\nprint(dataset.Amount[dataset.Class == 1].sum())\nprint(\"\u015arednia kwota za transakcj\u0119 oszuka\u0144cz\u0105\")\nprint(dataset.Amount[dataset.Class == 1].mean())\nprint(\"Por\u00f3wnaj z normalnymi transakcjami:\")\nprint(\"Ca\u0142kowita kwota z normalnych transakcji\")\nprint(dataset.Amount[dataset.Class == 0].sum())\nprint(\"\u015arednia kwota za zwyk\u0142e transakcje\")\nprint(dataset.Amount[dataset.Class == 0].mean())","1a1d4bed":"\nf, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n\nbins = 40\n\nax1.hist(dataset.Amount[dataset.Class == 1], bins = bins, normed = True, alpha = 0.75, color = 'red')\nax1.set_title('Fraud')\n\nax2.hist(dataset.Amount[dataset.Class == 0], bins = bins, normed = True, alpha = 0.5, color = 'blue')\nax2.set_title('Not Fraud')\n\nplt.xlabel('Amount')\nplt.ylabel('% of Transactions')\nplt.yscale('log')\nplt.show()","c460e15a":"bins = 75\nplt.hist(dataset.Time[dataset.Class == 1], bins = bins, normed = True, alpha = 0.75, label = 'Fraud', color = 'red')\nplt.hist(dataset.Time[dataset.Class == 0], bins = bins, normed = True, alpha = 0.5, label = 'Not Fraud', color = 'blue')\nplt.legend(loc='upper right')\nplt.xlabel('Time (seconds)')\nplt.ylabel('% of ')\nplt.title('Transactions over Time')\nplt.show()","8eaf6c19":"Vfeatures = dataset.iloc[:,1:29].columns\nprint(Vfeatures)","bd4e7ba4":"import matplotlib.gridspec as gridspec\nimport seaborn as sns\nbins = 50\nplt.figure(figsize=(12,28*4))\ngs = gridspec.GridSpec(28, 1)\nfor i, V in enumerate(dataset[Vfeatures]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(dataset[V][dataset.Class == 1], bins = bins, norm_hist = True, color = 'red')\n    sns.distplot(dataset[V][dataset.Class == 0], bins = bins, norm_hist = True, color = 'blue')\n    ax.set_xlabel('')\n    ax.set_title('distributions (w.r.t fraud vs. non-fraud) of feature: ' + str(V))\nplt.show()","8d332acc":"# heat map of correlation of features\ncorrelation_matrix = df.corr()\nfig = plt.figure(figsize=(12,9))\nsns.heatmap(correlation_matrix,vmax=0.8,square = True)\nplt.show()","6c2f76e8":"df.isnull().sum()","b58982d5":"df = pd.read_csv(\"..\/input\/creditcard.csv\")\n\ny = np.array(df.Class.tolist())     #classes: 1..fraud, 0..no fraud\ndf = df.drop('Class', 1)\ndf = df.drop('Time', 1)     # optional\ndf['Amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1,1))    #optionally rescale non-normalized column\nX = np.array(df.as_matrix())   # features\nprint(\"Fraction of frauds: {:.5f}\".format(np.sum(y)\/len(y)))\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n#    else:\n#        print('Confusion matrix, without normalization')\n\n#    print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n\ndef show_data(cm, print_res = 0):\n    tp = cm[1,1]\n    fn = cm[1,0]\n    fp = cm[0,1]\n    tn = cm[0,0]\n    if print_res == 1:\n        print('Precision =     {:.3f}'.format(tp\/(tp+fp)))\n        print('Recall (TPR) =  {:.3f}'.format(tp\/(tp+fn)))\n        print('Fallout (FPR) = {:.3e}'.format(fp\/(fp+tn)))\n    return tp\/(tp+fp), tp\/(tp+fn), fp\/(fp+tn)","ff6d941a":"lrn = LogisticRegression()\n\nskf = StratifiedKFold(n_splits = 5, shuffle = True)\nfor train_index, test_index in skf.split(X, y):\n    X_train, y_train = X[train_index], y[train_index]\n    X_test, y_test = X[test_index], y[test_index]\n    break\n\nlrn.fit(X_train, y_train)\ny_pred = lrn.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nif lrn.classes_[0] == 1:\n    cm = np.array([[cm[1,1], cm[1,0]], [cm[0,1], cm[0,0]]])\n\nplot_confusion_matrix(cm, ['0', '1'], )\npr, tpr, fpr = show_data(cm, print_res = 1);","6609db6e":"def ROC(X, y, c, r):\n#makes cross_validation for given parameters c,r. Returns FPR, TPR (averaged)\n    dic_weight = {1:len(y)\/(r*np.sum(y)), 0:len(y)\/(len(y)-r*np.sum(y))} \n    lrn = LogisticRegression(penalty = 'l2', C = c, class_weight = dic_weight)\n    \n    N = 5      #how much k-fold\n    N_iter = 3    #repeat how often (taking the mean)\n    mean_tpr = 0.0\n    mean_thresh = 0.0\n    mean_fpr = np.linspace(0, 1, 50000)\n    \n\n    for it in range(N_iter):\n        skf = StratifiedKFold(n_splits = N, shuffle = True)\n        for train_index, test_index in skf.split(X, y):\n            X_train, y_train = X[train_index], y[train_index]\n            X_test, y_test = X[test_index], y[test_index]\n         \n            lrn.fit(X_train, y_train)\n            y_prob = lrn.predict_proba(X_test)[:,lrn.classes_[1]]\n            \n            fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n            mean_tpr += np.interp(mean_fpr, fpr, tpr)\n            mean_thresh += np.interp(mean_fpr, fpr, thresholds)\n            mean_tpr[0] = 0.0\n\n    mean_tpr \/= (N*N_iter)\n    mean_thresh \/= (N*N_iter)\n    mean_tpr[-1] = 1.0\n    return mean_fpr, mean_tpr, roc_auc_score(y_test, y_prob), mean_thresh\n","9aafd9ff":"def plot_roc(X,y, list_par_1, par_1 = 'C', par_2 = 1):\n\n    f = plt.figure(figsize = (12,8));\n    for p in list_par_1:\n        if par_1 == 'C':\n            c = p\n            r = par_2\n        else:\n            r = p\n            c = par_2\n        list_FP, list_TP, AUC, mean_thresh = ROC(X, y, c, r)      \n        plt.plot(list_FP, list_TP, label = 'C = {}, r = {}, TPR(3e-4) = {:.4f}, AUC = {:.4f}'.format(c,r,list_TP[10],AUC));\n    plt.legend(title = 'values', loc='lower right')\n    plt.xlim(0, 0.001)   #we are only interested in small values of FPR\n    plt.ylim(0.5, 0.9)\n    plt.xlabel('FPR')\n    plt.ylabel('TPR')\n    plt.title('ROC detail')\n    plt.axvline(3e-4, color='b', linestyle='dashed', linewidth=2)\n    plt.show()\n    plt.close()","8c420cdf":"#plot_roc(X,y, [1], 'r', 1)","d913e470":"#plot_roc(X,y, [0.001, 0.01, 0.1, 1, 100], 'C', 1) ","770bdb18":"'''\nN = np.arange(10,80,2)     # will define threshold\ncm = {}     #will store the confusion matrix for different thresholds\nfor n in N:\n    cm[n] = 0.0\nlrn = LogisticRegression(penalty = 'l2', C = 1, class_weight = 'balanced')   #'balanced' corresponds to the case r=1\nN_Kfold = 5      #how much k-fold\nN_iter = 3   #repeat how often (taking the mean)\nfor it in range(N_iter):\n    skf = StratifiedKFold(n_splits = N_Kfold, shuffle = True)\n    for train_index, test_index in skf.split(X, y):\n        X_train, y_train = X[train_index], y[train_index]\n        X_test, y_test = X[test_index], y[test_index]\n        lrn.fit(X_train, y_train)\n\n        y_prob = lrn.predict_proba(X_test)[:,lrn.classes_[1]]\n        \n        for n in N:\n            \n            thresh = 1 - np.power(10.,-(n\/10))  #we want thresholds very close to 1\n            # generate the prediction from the probabilities y_prob:\n            y_pred = np.zeros(len(y_prob))\n            for j in range(len(y_prob)):\n                if y_prob[j] > thresh:\n                    y_pred[j] = 1\n    \n            B = confusion_matrix(y_test, y_pred)\n            #if the classes are mixed up, remedy that:\n            if lrn.classes_[0] == 1:\n                B = np.array([[B[1,1], B[1,0]], [B[0,1], B[0,0]]])\n            cm[n]+=B\n            '''","cebc3e06":"'''\nPR = []      #precision\nTPR = []\nFPR = []\nTHRESH = N\nfor n in N:\n    pr, tpr, fpr = show_data(cm[n])\n    PR.append(pr)\n    TPR.append(tpr)\n    FPR.append(-np.log(fpr)\/10)\n\ng  = plt.figure(figsize = (12,8))   \nplt.plot(THRESH, PR, label = 'Precision')\nplt.plot(THRESH, TPR, label = 'Recall (TPR)')\nplt.plot(THRESH, FPR, label = '-log(FPR)\/10')\nplt.axhline(-np.log(3e-4)\/10, color='b', linestyle='dashed', linewidth=2)\nplt.title('Evaluation of the classifier')\nplt.legend( loc='lower right')\nplt.xlabel('-log(1-thresh)\/log(10)')\nplt.ylim(0.55,0.9)\nplt.show()\n'''","9f213b50":"'''\ni = 0\nwhile FPR[i] < -np.log(3e-4)\/10:\n    i+=1\nA = cm[THRESH[i]].astype(int)\nplot_confusion_matrix(A, ['0', '1'])\nshow_data(A, print_res = 1);\n'''","c68fd17e":"'''\ni = 0\nwhile FPR[i] < -np.log(2e-3)\/10:\n    i+=1\nA = cm[THRESH[i]].astype(int)\nplot_confusion_matrix(A, ['0', '1'])\nshow_data(A, print_res = 1);\n'''","edd02b42":"'''\ni = 0\nwhile FPR[i] < -np.log(3e-4)\/10:\n    i+=1\nA = cm[THRESH[i]].astype(int)\nplot_confusion_matrix(A, ['0', '1'])\nshow_data(A, print_res = 1);\n'''","f91a8f5a":"def split_data(dataset,ratio):\n    sample=np.random.rand(len(dataset))<ratio\n    return(dataset[sample],dataset[~sample])","749da857":"credit_data = pd.read_csv(\"..\/input\/creditcard.csv\")\ncol=list(credit_data.columns.values)","34651d7b":"def NB_Classify(ratio,drop_var):\n    print('Dropped:',drop_var)\n    pred_acc=[]\n    for i in range(10):\n        train,test=split_data(credit_data,ratio)\n        clf=GaussianNB()\n        clf.fit(train.drop(drop_var,axis=1),train['Class'])\n        pred=clf.predict(test.drop(drop_var,axis=1))\n        pred_acc.append([pd.crosstab(test['Class'],pred).iloc[1,1]\/(pd.crosstab(test['Class'],pred).iloc[1,0]+pd.crosstab(test['Class'],pred).iloc[1,1])])\n    \n    print(np.mean(pred_acc))","30a86ad9":"for var in col:\n    NB_Classify(0.6,['Class',var])","d7018573":"NB_Classify(0.6,['Class','Time'])","ca06fe90":"df = pd.read_csv('..\/input\/creditcard.csv')\n\ndf = df.drop(['Time','Amount'], axis = 1)\ndf = df.sample(frac=1)\n\nfrauds = df[df['Class'] == 1]\nnon_frauds = df[df['Class'] == 0][:492]\n\nnew_df = pd.concat([non_frauds, frauds])\n# Shuffle dataframe rows\nnew_df = new_df.sample(frac=1, random_state=42)","d7835c40":"labels = ['non frauds','fraud']\nclasses = pd.value_counts(new_df['Class'], sort = True)\nclasses.plot(kind = 'bar', rot=0)\nplt.title(\"Dystrybucja klasy transakcji\")\nplt.xticks(range(2), labels)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Cz\u0119stotliwo\u015b\u0107\")","02edddae":"features = new_df.drop(['Class'], axis = 1)\nlabels = pd.DataFrame(new_df['Class'])\n\nfeature_array = features.values\nlabel_array = labels.values","de70544b":"X_train,X_test,y_train,y_test = train_test_split(feature_array,label_array,test_size=0.20)\n\n# normalize: skaluje wektory wej\u015bciowe indywidualnie do normy jednostki (d\u0142ugo\u015b\u0107 wektora).\nX_train = normalize(X_train)\nX_test=normalize(X_test)","131fad18":"neighbours = np.arange(1,25)\ntrain_accuracy =np.empty(len(neighbours))\ntest_accuracy = np.empty(len(neighbours))\n\nfor i,k in enumerate(neighbours):\n    #Skonfigurujemy klasyfikator knn z k s\u0105siadami\n    knn=KNeighborsClassifier(n_neighbors=k,algorithm=\"kd_tree\",n_jobs=-1)\n    \n    #Doposowanie modelu\n    knn.fit(X_train,y_train.ravel())\n    \n    #Oblicz dok\u0142adno\u015b\u0107 na zestawie treningowym\n    train_accuracy[i] = knn.score(X_train, y_train.ravel())\n    \n    #Oblicz dok\u0142adno\u015b\u0107 na zestawie testowym\n    test_accuracy[i] = knn.score(X_test, y_test.ravel()) ","78e1f22e":"plt.title('k-NN  Liczba s\u0105siad\u00f3w')\nplt.plot(neighbours, test_accuracy, label='Testing Accuracy')\nplt.plot(neighbours, train_accuracy, label='Training accuracy')\nplt.legend()\nplt.xlabel('Number of neighbors')\nplt.ylabel('Accuracy')\nplt.show()","6ba75390":"idx = np.where(test_accuracy == max(test_accuracy))\nx = neighbours[idx]\n\nknn=KNeighborsClassifier(n_neighbors=x[0],algorithm=\"kd_tree\",n_jobs=-1)\nknn.fit(X_train,y_train.ravel())","621b89a8":"knn_predicted_test_labels=knn.predict(X_test)\n\nknn_accuracy_score  = accuracy_score(y_test,knn_predicted_test_labels)\nknn_precison_score  = precision_score(y_test,knn_predicted_test_labels)\nknn_recall_score    = recall_score(y_test,knn_predicted_test_labels)\n\n\nprint(\"\")\nprint(\"K-Nearest Neighbours\")\nprint(\"Scores\")\nprint(\"Accuracy -->\",knn_accuracy_score)\nprint(\"Precison -->\",knn_precison_score)\nprint(\"Recall -->\",knn_recall_score)\n\nprint(classification_report(y_test,knn_predicted_test_labels))","49933791":"import seaborn as sns\nLABELS = ['Normal', 'Fraud']\nconf_matrix = confusion_matrix(y_test, knn_predicted_test_labels)\nplt.figure(figsize=(12, 12))\nsns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","bf460028":" # **Wykrywanie nadu\u017cy\u0107 kart kredytowych** \n ## **Semen Avramenko Alfred Balcewicz**","0eeb39a9":"Parametry, kt\u00f3re chc\u0119 zoptymalizowa\u0107, to C w regresji logistycznej, waga klasy i pr\u00f3g prawdopodobie\u0144stwa: regresja logistyczna zwraca prawdopodobie\u0144stwa dla przewidywanych klas, a przewidywanie jest zwykle oparte na tym, czy prawdopodobie\u0144stwo jest powy\u017cej pewnego progu, czy nie. Zmieniaj\u0105c pr\u00f3g, mo\u017cemy sprawi\u0107, \u017ce nasz model b\u0119dzie bardziej wra\u017cliwy (ale r\u00f3wnie\u017c zwi\u0119kszy liczb\u0119 b\u0142\u0119dnych wykry\u0107) lub mniej wra\u017cliwy (nie zauwa\u017cy wi\u0119cej oszustw).","3aace210":"Liczby te zapewniaj\u0105 bardziej namacaln\u0105 charakterystyk\u0119 klasyfikatora. Podczas strojenia parametr\u00f3w klasyfikatora musimy zada\u0107 sobie pytanie, czego oczekujemy od \u201edobrego\u201d klasyfikatora.","5d795e48":"Klasyfikator najbli\u017cszych s\u0105siad\u00f3w to jedna z wa\u017cniejszych nieparametrycznych metod klasyfikacji. W tej metodzie klasyfikowany obiekt przydzielamy do tej klasy, do kt\u00f3rej nale\u017cy wi\u0119kszo\u015b\u0107 z jego:math:k s\u0105siad\u00f3w.","cb0ffbe9":"Najpierw podzielimy dane na zestawy.\n\n","70c6a03f":"Poniewa\u017c klasy s\u0105 wysoce niezr\u00f3wnowa\u017cone, musimy rozwa\u017cy\u0107 odpowiedni\u0105 miar\u0119 jako\u015bci metody wykrywania oszustw. Gdyby\u015bmy naiwnie zliczyli u\u0142amek klas, kt\u00f3re otrzymali\u015bmy poprawnie (tj. Prawid\u0142owo przewidziany 0 lub 1), ju\u017c prosty model, kt\u00f3ry zawsze przewiduje 0 (nie-oszustwo), osi\u0105gn\u0105\u0142by \u201edok\u0142adno\u015b\u0107\u201d 99,827%, poniewa\u017c tylko oszustwa nie s\u0105 rozpoznawane poprawnie i s\u0105 oczywi\u015bcie rzadkie. Jednak taki model jest nonsensem, poniewa\u017c nie mo\u017ce nam powiedzie\u0107, czy dana transakcja jest oszustwem, czy nie. Niezwykle wysoka dok\u0142adno\u015b\u0107 jest myl\u0105ca i nie m\u00f3wi nam nic o rzeczywistej jako\u015bci przewidywania. Dlatego potrzebujemy bardziej wyrafinowanego podej\u015bcia do oceny prognozy. ","c1c8d9e7":"Ten histogram pokazuje procent transakcji dokonanych w danym okresie. Widzimy, \u017ce wi\u0119cej nieuczciwych dzia\u0142a\u0144 zazwyczaj ma miejsce, gdy wyst\u0119puj\u0105 og\u00f3lne przestoje w transakcjach. Je\u015bli za\u0142o\u017cymy, \u017ce dane s\u0105 zbierane od Dnia 0 12:01 do Dnia 2 23:59, poniewa\u017c jest to opisywane jako gromadzone w ci\u0105gu \u201edw\u00f3ch dni\u201d, widzimy, \u017ce oszuka\u0144cza dzia\u0142alno\u015b\u0107 ma miejsce bardzo wcze\u015bnie rano. Nie wykorzystujemy Time jako funkcji w naszym modelu predykcyjnym, poniewa\u017c dane s\u0105 tylko za dwa dni. Gdyby dane by\u0142yby zbierane przynajmniej przez miesi\u0105c, by\u0142oby to z pewno\u015bci\u0105 przydatne, je\u015bli zobaczymy podobny wz\u00f3r w d\u0142u\u017cszym okresie czasu.","947c3d58":"U\u015brednili\u015bmy macierze zamieszania dla r\u00f3\u017cnych prog\u00f3w, przechowywane w cm. Wyodr\u0119bniamy TPR i FPR z macierzy i wykre\u015blamy je:","523d5f30":"Na podstawie funkcji \"Klasa\" wyliczamy fa\u0142szywe transakcje, stanowi\u0105 jedynie ~ 0,17% wszystkich transakcji. Oznacza to, \u017ce d\u0105\u017cymy do przewidywania nietypowych zdarze\u0144. Na podstawie przeanalizowanych danych, okre\u015blamy jaka kwota zosta\u0142a utracona i r\u00f3wnujemy z normalnymi transakcjami.","5f82b990":"Szczeg\u00f3lnie bardzo ma\u0142y C = 0,001 wyr\u00f3\u017cnia si\u0119 znacznie gorsz\u0105 wydajno\u015bci\u0105. Dla wszystkich wi\u0119kszych warto\u015bci model dzia\u0142a podobnie dobrze i wybieramy C = 1 dla naszego ostatecznego modelu.","a252994f":"### 5.2 Grupowanie metod\u0105 k-\u015brednich: dob\u00f3r parametr\u00f3w","6d0c17d3":"### 5.3 Grupowanie metod\u0105 k-\u015brednich: Zako\u0144czenie","2b023741":"Z wykres\u00f3w wynika, \u017ce dla Fallouta mniejszego ni\u017c 4e-4 i C = 1 model dzia\u0142a najlepiej dla ma\u0142ych warto\u015bci r. Poniewa\u017c r = 1 jest naturalnym wyborem, na razie naprawimy t\u0119 warto\u015b\u0107.","531a53f6":"Je\u015bli chcemy Fallouta <3e-4, interesuj\u0105 nas tylko dane dotycz\u0105ce przeci\u0119cia linii przerywanej i krzywej FPR. Obliczamy Recall, Fallout i Precision dla odpowiedniego progu:","050b14c5":"### 5.3 Grupowanie metod\u0105 k-\u015brednich: ostatecznego modelu","f99c35e5":"Jak zobaczymy wkr\u00f3tce, jest to pr\u00f3g prawdopodobie\u0144stwa bardzo bliski 1, kt\u00f3ry daje po\u017c\u0105dany wynik. Powt\u00f3rz\u0119 teraz StratifiedKFold dla ostatecznego modelu i wykre\u015blimy TPR i FPR jako funkcje progu, aby\u015bmy mogli wybra\u0107 odpowiedni pr\u00f3g.","42ba8638":"W danym rozdziale analizujemy i przygotowujemy dane do analizy. Najpierw sprawdzamy ile obiekt\u00f3w posiada warto\u015b\u0107 NULL. W przypadku gdyby macierz danych posiada\u0142a tak\u0105 warto\u015b\u0107, trzeba by\u0142oby zastosowa\u0107 warto\u015b\u0107 \"not a number\" kt\u00f3ra znajduje si\u0119 w pakiecie *numpy* do reprezentacji brakuj\u0105cej danej. Aczkolwiek po analizie naszego zbioru danych, widzimy, \u017ce \u017caden z obiekt\u00f3w nie posiada takiej warto\u015bci.","1a5b8268":" ## 1. Wst\u0119p ","8fb10f93":"W ko\u0144cu my\u015bl\u0119, \u017ce op\u00f3r mi\u0119dzy 2e-4 a 3e-4 jest uzasadniony. Oczekiwane wycofanie wynosi wtedy od 75% do 80%.","eb5686f6":"Klasyfikator Naive Bayes jest w stanie wykry\u0107 ponad 80% transakcji oszustwa.","418a76e1":"Jak wida\u0107 powy\u017cej, w tym przypadku czas nie jest dobrym predyktorem, co wyra\u017anie potwierdza nasze wnioski z histogram\u00f3w, \u017ce czas nie mia\u0142 nic wsp\u00f3lnego z transakcjami oszustwa. Wi\u0119c na usuwanie czasu z listy predyktor\u00f3w","49a38538":"Podzielili\u015bmy dane na trening i zestaw test\u00f3w w stosunku 4: 1, wyszkolili\u015bmy Regres logistyczny na zestawie treningowym i przewidzieli\u015bmy wynik na zestawie testowym. Wynik tej prognozy jest przedstawiony w macierzy zamieszania. Widzimy, \u017ce prawie wszystkie transakcje nieuczciwe s\u0105 r\u00f3wnie\u017c uznawane za takie. Wykryto oko\u0142o 2\/3 wszystkich oszustw, ale wiele z nich nie jest rozpoznawanych. Macierz zamieszania jest wygodnym sposobem zilustrowania zachowania klasyfikatora","24fa05b6":"- Precision  powinna by\u0107 du\u017ca (bliska 1). Precyzja bliska 0 oznacza, \u017ce ostrze\u017cenie o oszustwie oka\u017ce si\u0119 b\u0142\u0119dem w wi\u0119kszo\u015bci przypadk\u00f3w.\n- Recall  powinno by\u0107 bliskie 1. Chcemy wykry\u0107 oszustwa z du\u017cym prawdopodobie\u0144stwem. Przypomnienie o ~ 60%, jak w przypadku klasyfikatora powy\u017cej, z pewno\u015bci\u0105 nie jest wystarczaj\u0105co dobre - wiele oszustw pozostanie niewykrytych. Zdecydowanie d\u0105\u017cymy do czego\u015b wy\u017cszego, 80% by\u0142oby ca\u0142kiem dobre.\n- Fallout powinien by\u0107 bardzo niski. ","1064c752":"Naiwny klasyfikator bayesowski \u2013 prosty klasyfikator probabilistyczny. Naiwne klasyfikatory bayesowskie s\u0105 oparte na za\u0142o\u017ceniu o wzajemnej niezale\u017cno\u015bci predyktor\u00f3w (zmiennych niezale\u017cnych). Cz\u0119sto nie maj\u0105 one \u017cadnego zwi\u0105zku z rzeczywisto\u015bci\u0105 i w\u0142a\u015bnie z tego powodu nazywa si\u0119 je naiwnymi. Bardziej opisowe jest okre\u015blenie \u2013 \u201emodel cech niezale\u017cnych\u201d. Ponadto model prawdopodobie\u0144stwa mo\u017cna wyprowadzi\u0107 korzystaj\u0105c z twierdzenia Bayesa.","bbc9d871":"## 3. Og\u00f3lny opis danych","df5e2706":"Zestaw analizowanych danych zawiera tylko numeryczne zmienne wej\u015bciowe, kt\u00f3re s\u0105 wynikiem transformacji PCA. Transformacja PCA jest algorytmem analizy danych. Wykorzystuje w tym celu informacje dotycz\u0105ce powi\u0105za\u0144 pomi\u0119dzy danymi wej\u015bciowymi. Umo\u017cliwia to dokonanie selekcji i \"kompresji\" danych bez utraty istotnych informacji pierwotnego zestawu danych. Cechy V1, V2, ... V28 s\u0105 g\u0142\u00f3wnymi sk\u0142adnikami uzyskanymi z PCA, jedynymi cechami, kt\u00f3re nie zosta\u0142y przekszta\u0142cone za pomoc\u0105 PCA, s\u0105 \u201eCzas\u201d i \u201eKwota\u201d. Funkcja \u201eCzas\u201d zawiera sekundy, kt\u00f3re up\u0142yn\u0119\u0142y mi\u0119dzy ka\u017cd\u0105 transakcj\u0105 a pierwsz\u0105 transakcj\u0105 w zbiorze danych. Funkcja \u201eKwota\u201d jest kwot\u0105 transakcji, ta funkcja mo\u017ce by\u0107 wykorzystywana na przyk\u0142ad w zale\u017cno\u015bci od uczenia si\u0119 zale\u017cnego od koszt\u00f3w. Funkcja \u201eKlasa\u201d jest zmienn\u0105 odpowiedzi i przyjmuje warto\u015b\u0107 1 w przypadku oszustwa, a 0 w przeciwnym razie. Tak\u017ce w analizowanym zbiorze danych nie wyst\u0119puj\u0105 \u017cadne braki w macierzach.","e79b2f07":"Interesuj\u0105ce jest to, \u017ce podczas gdy oszuka\u0144cze transakcje stanowi\u0105 niewielk\u0105 cz\u0119\u015b\u0107 zestawu danych, maj\u0105 wy\u017csz\u0105 \u015bredni\u0105 kwot\u0119 na transakcj\u0119. Przydatne mo\u017ce by\u0107 wypr\u00f3bowanie modelu z Amount jako funkcj\u0105.","e41ddc35":"## 7. Podsumowanie","24dc8ddc":"Podzielimy dane na zestaw treningowy i zestaw testowy. StratifiedKFold bierze pod uwag\u0119 stosunek oszustw \/ nieuczciwo\u015bci, co jest wa\u017cne dla takiego niezr\u00f3wnowa\u017conego zestawu.","3b97078b":"W tym przypadku u\u017cywamy modelu K Nearest Neighbours, dlatego potrzebujemy optymalnego K, aby uzyska\u0107 z niego jak najwi\u0119cej.","9204c60f":"### 5.1 Grupowanie metod\u0105 k-\u015brednich","59ed11fb":"Tematem projektu by\u0142o rozwi\u0105zanie problemu zwi\u0105zanego z nadu\u017cyciem kart kredyowych. Na co dzie\u0144 jesy wykonywane setki fa\u0142szywych transakcji i dla firm obs\u0142uguj\u0105cych karty kredytowe, jest istotne m\u00f3c rozpoznawa\u0107 fa\u0142szywe transakcje, aby klienci nie byli obci\u0105\u017cani dodatkowymi kosztami. Do analizy zosta\u0142 u\u017cyty zestaw danych zawieraj\u0105ce transakcje dokonane we wrze\u015bniu 2013 roku. Ten zestaw danych przedstawia transakcje, kt\u00f3re mia\u0142y miejsce w ci\u0105gu dw\u00f3ch dni, w kt\u00f3rych mamy 492 oszustwa z 284 807 transakcji. Zbi\u00f3r danych jest wysoce niezr\u00f3wnowa\u017cony, klasa dodatnia (oszustwa) stanowi 0,172% wszystkich transakcji. Do wykrycycia nadu\u017cy\u0107 zosta\u0142y wykorzystane metodyki i narz\u0119dzia analizy, wizualizacji, grupowania oraz klasyfikacji danych. \n","5228fe69":"## 4. Przygotowanie danych do analizy","50b36ca4":"## 5. Uczenie nienadzorowane","4b681feb":"Metoda k-\u015brednich jest metod\u0105 nale\u017cac\u0105 do grupy algorytm\u00f3w analizy skupie\u0144 tj. analizy polegaj\u0105cej na szukaniu i wyodr\u0119bnianiu grup obiekt\u00f3w podobnych (skupie\u0144) . Reprezentuje ona grup\u0119 algorytm\u00f3w niehierarchicznych. G\u0142\u00f3wn\u0105 r\u00f3\u017cnic\u0105 pomi\u0119dzy niehierarchicznymi i hierarchicznymi algorytmami jest konieczno\u015b\u0107 wcze\u015bniejszego podania ilo\u015bci skupie\u0144.","9359df53":"Dok\u0142adno\u015b\u0107 nie jest w tym przypadku w\u0142a\u015bciw\u0105 miar\u0105. Powody:\n\n* Istnieje mniej transakcji oszustwa. Ka\u017cdy model dowie si\u0119 o transakcjach bez nadu\u017cy\u0107 finansowych i przewidzi z dok\u0142adno\u015bci\u0105 wi\u0119ksz\u0105 ni\u017c 90%.\n* W bankowo\u015bci wa\u017cniejsze jest, aby nie przegapi\u0107 transakcji oszustwa. Dlatego nale\u017cy skoncentrowa\u0107 si\u0119 na tym pomiarze. Tak wi\u0119c dok\u0142adno\u015b\u0107 by\u0142aby (wykryte prawdziwe oszustwa) \/ (ca\u0142kowite oszustwa przewidywane przez model).\n","630b5292":"Do tego zadania u\u017cyjmy  krzywe ROC, kt\u00f3ra jest narz\u0119dziem do oceny poprawno\u015bci klasyfikatora, zape\u0142nia ona \u0142\u0105czny opis jego czu\u0142o\u015bci i specyficzno\u015bci.","74960fde":"Przygotowanie danych","545937ef":"Zaczniemy analiz\u0119 danych z opisu struktur i analiz g\u0142\u00f3wnych cech. Importujemy wszystkie potrzebne dla nas biblioteki oraz wczytatujemy plik z danymi. Opisujemy podstawowe cechy wykres\u00f3w.","dcce3213":"Przeprowadzimy teraz wyszukiwanie siatki, aby znale\u017a\u0107 optymalne parametry C i r. Powiedzmy, \u017ce chcemy Fallouta <3e-4.","05137feb":"Regresja logistyczna mo\u017ce zosta\u0107 uwzgl\u0119dniona w klasach niezbalansowanych przy u\u017cyciu opcji class_weight. Ten parametr daje wag\u0119 niedoreprezentowanej klasie 1: r = 1 daje klas\u0119 1, kt\u00f3ra jest rzeczywi\u015bcie zalecana. Im wi\u0119kszy r jest wybrany, tym mniejszy ci\u0119\u017car przypisuje si\u0119 klasie 1.","d3de8e1f":"### 6.2 Klasyfikacja najbli\u017cszych s\u0105siad\u00f3w\n","0f68242e":"Funkcja do klasyfikacji na podstawie Naive Bayes. Algorytm dzia\u0142a 10 razy i podaje \u015bredni\u0105 z przewidywana dok\u0142adno\u015b\u0107 za ka\u017cdym razem. I tak\u017ce okre\u015bli\u0107, kt\u00f3r\u0105 zmienn\u0105 usun\u0105\u0142em z ca\u0142kowitej zmiennej lista, dzi\u0119ki kt\u00f3rej dowiem si\u0119, kt\u00f3re z nich nale\u017cy usun\u0105\u0107.","8f4cdf35":"Poniewa\u017c dane s\u0105 bardzo niezr\u00f3wnowa\u017cone, bior\u0119 tylko 492 wiersze z transakcji bez oszustw.","e2584d0a":"Odkryli\u015bmy, \u017ce dla Fallouta o warto\u015bci 0,03% mo\u017cemy uzyska\u0107 Recall oko\u0142o 80%. Ka\u017cda znacz\u0105ca poprawa przywo\u0142ania wi\u0105\u017ce si\u0119 z bardzo wysokimi kosztami Fallouta. Podobnie, ka\u017cda znacz\u0105ca poprawa Fallouta drastycznie zmniejszy Fallouta.","27047a0d":"## 6. Uczenie nadzorowane","23e5bbd4":"Pobieranie listy wszystkich kolumn w zestawie danych kredyt\u00f3w. Na pocz\u0105tek dodam pocz\u0105tkowo wszystkie zmienne do modelu i usun\u0119 jedn\u0105 po drugiej zmienn\u0105 i zobacz\u0119, kt\u00f3re z nich najbardziej zwi\u0119kszaj\u0105 dok\u0142adno\u015b\u0107.","ae333076":"### 6.2 Klasyfikator Bayesa","9be82911":"Kontynuujemy teraz z r = 1 i szukamy dobrego wyboru C:","2b562a2e":"## 2. Problemy ktory zosta\u0142y ju\u017c rozwi\u0105zane dla wybranego zbioru","1ed60c93":"\n- Precyzja oznacza prawdopodobie\u0144stwo, \u017ce transakcja sklasyfikowana jako oszustwo jest naprawd\u0119 oszustwem.\n- Recall  to prawdopodobie\u0144stwo, \u017ce klasyfikator rozpoznaje prawdziwe oszustwo.\n- Fallout  jest prawdopodobie\u0144stwem, \u017ce nieuczciwo\u015b\u0107 jest b\u0142\u0119dnie klasyfikowana jako oszustwo.\n","514f7199":"Pokazuje to r\u00f3\u017cnice w rozk\u0142adzie funkcji podczas por\u00f3wnywania fa\u0142szywych transakcji z normalnymi transakcjami."}}