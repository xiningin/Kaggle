{"cell_type":{"c07dae62":"code","3dcb1cf2":"code","6fd9bda5":"code","01b5b488":"code","8385c27e":"code","1c399b1c":"code","54d296e9":"code","3e811d55":"code","3391ec18":"code","404ac2ca":"code","9983d261":"code","f0ee69b3":"code","35f114f9":"code","d04171ee":"code","ebf939cf":"code","0c34ac65":"code","bf91cab4":"code","592bb1b9":"code","30cb1f3e":"code","7767ef4d":"code","4990499d":"code","84b747f0":"code","22aae1c1":"code","a341caae":"code","c21a9998":"code","58940c57":"code","426de935":"code","8bfceabc":"code","fca7a14b":"code","672f26c7":"code","5b3e4d2b":"code","5313f885":"code","9ccaa5f0":"code","e30b8138":"code","ae90c7d8":"code","179231e6":"code","c4a4706c":"code","0ec11fd8":"code","fc7d2574":"code","3c0e23e3":"code","384240d7":"code","280af29d":"code","692a41bf":"code","ba1d5944":"code","f01e68da":"code","a7f7939a":"code","cc2e2457":"code","6257d251":"code","8e64a04a":"code","de057a39":"code","0c11e3e5":"code","986f06a5":"code","e623cad1":"code","059a2edf":"code","912bd9f8":"code","818452e9":"code","013cfe6e":"code","f0e00c14":"code","e4a9add5":"code","496113e8":"code","5d11ae6f":"code","514a7e89":"code","cabc2ab0":"code","0a7e8c0e":"code","93cd97d6":"code","39b7863f":"code","146cee85":"code","5c91f551":"code","ac7e9388":"code","35c8d949":"code","c532c04f":"code","89dcaa76":"code","851ab61a":"code","3698a087":"code","292f3a86":"code","c0ac40ba":"code","35359528":"code","9b999ef2":"code","1d02c77f":"code","43dc915f":"code","11c2ab01":"code","d86c6f2b":"code","1b167c1a":"code","d4c626f8":"code","00ed39dd":"code","16c32f2b":"code","30a3f542":"code","e3f1416f":"code","0a245848":"code","f1d1d21a":"code","4f982bff":"code","0edd3415":"code","0b93f32d":"code","d549bc52":"code","4bba43c7":"code","39d37cc8":"code","19acd5c6":"code","0dcc9663":"code","572c2d80":"code","1f31f77d":"code","0ad70716":"code","ab92867b":"code","b398ca57":"code","0a1a0637":"code","d0b89b58":"code","9e7765fc":"code","aa28f024":"code","fa29b650":"code","4e919e2e":"markdown","c12046c9":"markdown","8f1c45fb":"markdown","a4807fb8":"markdown","8c4f65d8":"markdown","0a9a7a39":"markdown","4db79769":"markdown","8ead7e32":"markdown","a7d29f79":"markdown","8e9560fb":"markdown","a7f3dcde":"markdown","f1de521f":"markdown","77f7b40f":"markdown","86fefb22":"markdown","e46f38c9":"markdown","14c5b7ed":"markdown","2b1ad662":"markdown"},"source":{"c07dae62":"import math\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nparameters = {'axes.grid': True}\nplt.rcParams.update(parameters)\n\nimport xgboost as xgb\nimport lightgbm as lgb\nimport missingno as msno\nimport eli5\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, learning_curve, KFold\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_absolute_error\nfrom scipy import stats\nfrom scipy.stats import uniform, randint\nfrom tqdm import tqdm","3dcb1cf2":"df_test = pd.read_csv('..\/input\/titanic\/test.csv')\ndf_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_train.head()","6fd9bda5":"plt.figure(figsize = (15,10))\nplt.subplot(2,2,1)\nplt.pie(x = df_train.groupby(by = 'Survived').count()['PassengerId'], \n        labels = ['Not Survived', 'Survived'], autopct = '%1.2f%%', explode = [0.05, 0], startangle = 90)\nplt.title(\"Distribution of Titanic's passengers in terms of survival\")\nplt.grid()\nplt.subplot(2,2,2)\nplt.pie(x = df_train.groupby(by = 'Sex').count()['PassengerId'], \n        labels = ['Female', 'Male'], autopct = '%1.2f%%', explode = [0.05, 0], startangle = 90)\nplt.title(\"Distribution of Titanic's passengers in terms of sex\")\nplt.grid()\nplt.subplot(2,2,3)\nplt.pie(x = df_train.groupby(by = 'Pclass').count()['PassengerId'], \n        labels = ['First class', 'Second class', 'Third class'], autopct = '%1.2f%%', explode = [0.03, 0.03, 0.03], startangle = 90)\nplt.title(\"Distribution of Titanic's passengers in terms of class\")\nplt.grid()\nplt.subplot(2,2,4)\nplt.pie(x = df_train.groupby(by = 'Embarked').count()['PassengerId'], \n        labels = ['C', 'Q', 'S'], autopct = '%1.2f%%', explode = [0.03, 0.03, 0.03], startangle = 90)\nplt.title(\"Distribution of Titanic's passengers in terms of embarked\")\nplt.grid()","01b5b488":"plt.figure(figsize = (8,5))\nsns.countplot(x = df_train['Sex'], hue = df_train['Survived'], palette = 'Set1')\nplt.title(\"Number of Titanic's passengers in terms of sex and survival\")","8385c27e":"num_of_female = df_train[(df_train['Sex'] == 'female')]['PassengerId'].count()\nnum_of_surv_female = df_train[(df_train['Sex'] == 'female') & (df_train['Survived'] == 1)]['PassengerId'].count()\n\nnum_of_male = df_train[(df_train['Sex'] == 'male')]['PassengerId'].count()\nnum_of_surv_male = df_train[(df_train['Sex'] == 'male') & (df_train['Survived'] == 1)]['PassengerId'].count()\n\nper_of_surv_female = num_of_surv_female \/ num_of_female * 100\nper_of_surv_male = num_of_surv_male \/ num_of_male * 100\n\nprint(f\"% of female survived:   {per_of_surv_female:.3f}\")\nprint(f\"% of male survived:     {per_of_surv_male:.3f}\")","1c399b1c":"plt.figure(figsize = (15,5))\nsns.swarmplot(x = df_train['Age'], y = df_train['Sex'], hue = df_train['Survived'], palette = 'Set1')\nplt.title(\"Distribution of Titanic's passengers in terms of age, sex and survival\")","54d296e9":"for i in [i for i in range(0, 80, 10)]:\n    num_of_surv = df_train[(df_train['Age'] > i) & (df_train['Age'] <= i+10) & (df_train['Survived'] == 1)]['PassengerId'].count()\n    num = df_train[(df_train['Age'] > i) & (df_train['Age'] <= i+10)]['PassengerId'].count()\n    per_of_surv = num_of_surv \/ num * 100\n    print(f\"% of survived in age {i} - {i+10}:     {per_of_surv:0.3f}\")","3e811d55":"plt.figure(figsize = (15,12))\n\nplt.subplot(2,2,1)\nsns.scatterplot(x = df_train[df_train['Pclass'] == 1]['PassengerId'], y = df_train[df_train['Pclass'] == 1]['Age'], \n                hue = df_train['Survived'], palette = 'Set1')\nplt.title(\"First class Titanic's passengers in terms of age and survival\")\n\nplt.subplot(2,2,2)\nsns.scatterplot(x = df_train[df_train['Pclass'] == 2]['PassengerId'], y = df_train[df_train['Pclass'] == 2]['Age'], \n                hue = df_train['Survived'], palette = 'Set1')\nplt.title(\"Second class Titanic's passengers in terms of age and survival\")\n\nplt.subplot(2,2,3)\nsns.scatterplot(x = df_train[df_train['Pclass'] == 3]['PassengerId'], y = df_train[df_train['Pclass'] == 3]['Age'], \n                hue = df_train['Survived'], palette = 'Set1')\nplt.title(\"Third class Titanic's passengers in terms of age and survival\")","3391ec18":"for i in [1, 2, 3]:\n    \n    num = df_train[(df_train['Pclass'] == i)]['PassengerId'].count()\n    num_of_surv = df_train[(df_train['Pclass'] == i) & (df_train['Survived'] == 1)]['PassengerId'].count()\n    \n    num_of_female = df_train[(df_train['Pclass'] == i) & (df_train['Sex'] == 'female')]['PassengerId'].count()\n    num_of_surv_female = df_train[(df_train['Pclass'] == i) & (df_train['Survived'] == 1) & (df_train['Sex'] == 'female')]['PassengerId'].count()\n    \n    num_of_male = df_train[(df_train['Pclass'] == i) & (df_train['Sex'] == 'male')]['PassengerId'].count()\n    num_of_surv_male = df_train[(df_train['Pclass'] == i) & (df_train['Survived'] == 1) & (df_train['Sex'] == 'male')]['PassengerId'].count()\n    \n    per_of_surv = num_of_surv \/ num * 100\n    per_of_surv_female = num_of_surv_female \/ num_of_female * 100\n    per_of_surv_male = num_of_surv_male \/ num_of_male * 100\n    \n    print(f\"-------------------------------\")\n    print(f\"class #{i}\")\n    print(f\"% of survived:           {per_of_surv:0.3f}\")\n    print(f\"% of female survived:    {per_of_surv_female:0.3f}\")\n    print(f\"% of male survived:      {per_of_surv_male:0.3f}\")","404ac2ca":"plt.figure(figsize = (10,5))\nsns.countplot(y = df_train['SibSp'], hue = df_train['Survived'], palette = 'Set1')\nplt.title(\"Number of Titanic's passengers in terms of siblings\/spouse and survival\")","9983d261":"for i in df_train['SibSp'].sort_values().unique():\n    \n    num_of_surv = df_train[(df_train['Survived'] == 1) & (df_train['SibSp'] == i)]['PassengerId'].count()\n    num = df_train[df_train['SibSp'] == i]['PassengerId'].count()\n    \n    num_of_female = df_train[(df_train['SibSp'] == i) & (df_train['Sex'] == 'female')]['PassengerId'].count()\n    num_of_surv_female = df_train[(df_train['SibSp'] == i) & (df_train['Survived'] == 1) & (df_train['Sex'] == 'female')]['PassengerId'].count()\n    \n    num_of_male = df_train[(df_train['SibSp'] == i) & (df_train['Sex'] == 'male')]['PassengerId'].count()\n    num_of_surv_male = df_train[(df_train['SibSp'] == i) & (df_train['Survived'] == 1) & (df_train['Sex'] == 'male')]['PassengerId'].count()\n    \n    per_of_surv = num_of_surv \/ num * 100\n    per_of_surv_female = num_of_surv_female \/ num_of_female * 100\n    per_of_surv_male = num_of_surv_male \/ num_of_male * 100\n    \n    \n    print(f\"-------------------------------\")\n    print(f\"Siblings\/spouse: {i}\")\n    print(f\"% of survived:           {per_of_surv:0.3f}\")\n    print(f\"% of female survived:    {per_of_surv_female:0.3f}\")\n    print(f\"% of male survived:      {per_of_surv_male:0.3f}\")","f0ee69b3":"plt.figure(figsize = (10,5))\nsns.countplot(y = df_train['Parch'], hue = df_train['Survived'], palette = 'Set1')\nplt.title(\"Number of Titanic's passengers in terms of parents\/children and survival\")","35f114f9":"for i in df_train['Parch'].sort_values().unique():\n    \n    num_of_surv = df_train[(df_train['Survived'] == 1) & (df_train['Parch'] == i)]['PassengerId'].count()\n    num = df_train[df_train['Parch'] == i]['PassengerId'].count()\n    \n    num_of_female = df_train[(df_train['Parch'] == i) & (df_train['Sex'] == 'female')]['PassengerId'].count()\n    num_of_surv_female = df_train[(df_train['Parch'] == i) & (df_train['Survived'] == 1) & (df_train['Sex'] == 'female')]['PassengerId'].count()\n    \n    num_of_male = df_train[(df_train['Parch'] == i) & (df_train['Sex'] == 'male')]['PassengerId'].count()\n    num_of_surv_male = df_train[(df_train['Parch'] == i) & (df_train['Survived'] == 1) & (df_train['Sex'] == 'male')]['PassengerId'].count()\n    \n    per_of_surv = num_of_surv \/ num * 100\n    per_of_surv_female = num_of_surv_female \/ num_of_female * 100\n    per_of_surv_male = num_of_surv_male \/ num_of_male * 100\n    \n    \n    print(f\"-------------------------------\")\n    print(f\"Parents\/children: {i}\")\n    print(f\"% of survived:           {per_of_surv:0.3f}\")\n    print(f\"% of female survived:    {per_of_surv_female:0.3f}\")\n    print(f\"% of male survived:      {per_of_surv_male:0.3f}\")","d04171ee":"plt.figure(figsize = (10,5))\nsns.countplot(y = df_train['Embarked'].sort_values(), hue = df_train['Survived'], palette = 'Set1')\nplt.title(\"Number of Titanic's passengers in terms of embarkation and survival\")","ebf939cf":"for i in ['C', 'Q', 'S']:\n    \n    num_of_surv = df_train[(df_train['Survived'] == 1) & (df_train['Embarked'] == i)]['PassengerId'].count()\n    num = df_train[df_train['Embarked'] == i]['PassengerId'].count()\n    \n    num_of_female = df_train[(df_train['Embarked'] == i) & (df_train['Sex'] == 'female')]['PassengerId'].count()\n    num_of_surv_female = df_train[(df_train['Embarked'] == i) & (df_train['Survived'] == 1) & (df_train['Sex'] == 'female')]['PassengerId'].count()\n    \n    num_of_male = df_train[(df_train['Embarked'] == i) & (df_train['Sex'] == 'male')]['PassengerId'].count()\n    num_of_surv_male = df_train[(df_train['Embarked'] == i) & (df_train['Survived'] == 1) & (df_train['Sex'] == 'male')]['PassengerId'].count()\n    \n    per_of_surv = num_of_surv \/ num * 100\n    per_of_surv_female = num_of_surv_female \/ num_of_female * 100\n    per_of_surv_male = num_of_surv_male \/ num_of_male * 100\n    \n    \n    print(f\"-------------------------------\")\n    print(f\"Embarkation: {i}\")\n    print(f\"% of survived:           {per_of_surv:0.3f}\")\n    print(f\"% of female survived:    {per_of_surv_female:0.3f}\")\n    print(f\"% of male survived:      {per_of_surv_male:0.3f}\")","0c34ac65":"df_train_2 = df_train.copy()\ndf_train_2['Sex'] = df_train_2['Sex'].map({'female': 0, 'male': 1})\ndf_train_2['Embarked'] = df_train_2['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\nsns.pairplot(data = df_train_2, hue = 'Survived', palette = 'Set1')","bf91cab4":"df = pd.concat([df_train, df_test], ignore_index = True)\ndf.head()","592bb1b9":"df.isnull().sum()","30cb1f3e":"df['Embarked'].value_counts()","7767ef4d":"df['Embarked'].fillna('S', inplace = True)","4990499d":"df['Cabin'].value_counts()","84b747f0":"def extract_letter_from_cabin(cabin):\n    if pd.isna(cabin) == True:\n        return 'unknown'\n    else:\n        return cabin[0]","22aae1c1":"df['deck'] = df['Cabin'].map(extract_letter_from_cabin)","a341caae":"sns.countplot(x = df['deck'])","c21a9998":"def group_deck(deck):\n    if deck in ['A', 'B', 'C', 'T']:\n        return 'ABC'\n    if deck in ['D', 'E']:\n        return 'DE'\n    if deck in ['F', 'G']:\n        return 'FG'\n    else:\n        return 'unknown'","58940c57":"df['deck'] = df['deck'].map(group_deck)","426de935":"sns.countplot(x = df['deck'])","8bfceabc":"df[df['Fare'].isnull() == True]","fca7a14b":"median = df[(df['Pclass'] == 3) & (df['Sex'] == 'male') & (df['Embarked'] == 'S')]['Fare'].median()\nmedian","672f26c7":"df['Fare'].fillna(median, inplace = True)","5b3e4d2b":"df['Fare'].describe()","5313f885":"df['fare_bin'] = pd.cut(x = df['Fare'], bins=[0.0, 7.8958, 14.4542, 31.275, 512.3292], labels=[0, 1, 2, 3], include_lowest = True)\ndf['fare_bin'] = df['fare_bin'].astype(int)","9ccaa5f0":"df.head()","e30b8138":"df['Age'] = df.groupby(by = ['Sex', 'Pclass', 'Embarked'])['Age'].apply(lambda x: x.fillna(x.median()))","ae90c7d8":"df['Age'].describe()","179231e6":"df['age_bin'] = pd.cut(x = df['Age'], bins=[0.0, 21, 28, 39, 80], labels=[0, 1, 2, 3], include_lowest = True)\ndf['age_bin'] = df['age_bin'].astype(int)","c4a4706c":"df['Name'].value_counts()","0ec11fd8":"def extract_title_from_name(name):\n    return name.split(',')[1].split('.')[0].strip()","fc7d2574":"df['title'] = df['Name'].map(extract_title_from_name)","3c0e23e3":"df['title'].value_counts()","384240d7":"df['title'].replace(['Don', 'Major', 'Jonkheer','Sir'], 'Mr', inplace = True)\ndf['title'].replace(['the Countess', 'Mme', 'Lady', 'Dona'], 'Mrs', inplace = True)\ndf['title'].replace(['Mlle', 'Ms'], 'Miss', inplace = True)\n\nfor i in ['Dr', 'Rev', 'Col', 'Capt']:\n    df.loc[(df['Sex'] == 'female') & (df['title'] == i), 'title'] = 'Mrs'\n    df.loc[(df['Sex'] == 'male') & (df['title'] == i), 'title'] = 'Mr'","280af29d":"df['title'].value_counts()","692a41bf":"df.head()","ba1d5944":"def extract_from_ticket(ticket):\n    ticket = ticket.split(' ')[0]\n    if ticket.isdigit() == True:\n        return '-1'\n    else:\n        return ticket","f01e68da":"df['ticket_letter'] = df['Ticket'].map(extract_from_ticket)","a7f7939a":"df['ticket_letter'].value_counts()","cc2e2457":"for i in [i for i in df['ticket_letter'].unique() if ('-1' not in i) & ('PC' not in i) & ('C.A.' not in i)] + ['C.A.\/SOTON']:\n    df.loc[df['ticket_letter'] == i, 'ticket_letter'] = 'C.A.'","6257d251":"sns.countplot(x = df['ticket_letter'])","8e64a04a":"df['family_members'] = df['SibSp'] + df['Parch']","de057a39":"def alone_or_not(x):\n    if x == 0:\n        return 1\n    else:\n        return 0\n    \ndf['alone'] = df['family_members'].map(alone_or_not)","0c11e3e5":"for i in ['Sex', 'Embarked', 'deck', 'title', 'ticket_letter']:\n    df[f\"{i}_cat\"] = df[i].factorize()[0]","986f06a5":"df.head()","e623cad1":"df_final = df.drop(['PassengerId', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', \n                    'title', 'Cabin', 'deck', 'Embarked', 'ticket_letter'], axis = 1)\ndf_final.head()","059a2edf":"df_test_final = df_final[df_final['Survived'].isnull()].copy()\ndf_train_final = df_final[~ df_final['Survived'].isnull()].copy()","912bd9f8":"df_train_final","818452e9":"df_train_final['Survived'] = df_train_final['Survived'].astype(int)","013cfe6e":"plt.figure(figsize = (15, 10))\ncorr = stats.spearmanr(df_train_final)\ncoef = np.around(corr[0], 4)\nsns.heatmap(coef, annot = True, xticklabels = df_train_final.columns, yticklabels = df_train_final.columns, center = 0.9)\nplt.title('Heatmap of correlation of features')","f0e00c14":"X = df_train_final.drop('Survived', axis = 1)\ny = df_train_final['Survived']","e4a9add5":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 10)","496113e8":"fit_params = dict(early_stopping_rounds = 50, eval_set = [(X_test, y_test)])\n\nrs_params = dict(learning_rate = np.logspace(-2, 0, 100), reg_lambda = [0, 50, 100],\n                 n_estimators = [1000], max_depth = [5, 10, 50], gamma = [0, 5, 10])\n\n\nxgb_model = xgb.XGBClassifier()\nxgb_rs = RandomizedSearchCV(estimator = xgb_model, param_distributions = rs_params, n_jobs = -1,\n                                        scoring = 'accuracy', verbose = 5, cv = 10, n_iter = 200, random_state = 34)\nxgb_rs.fit(X_train, y_train, **fit_params)\npredictions_xgb = xgb_rs.predict(X_test)","5d11ae6f":"pd.DataFrame(xgb_rs.cv_results_).nlargest(5, 'mean_test_score')","514a7e89":"xgb_rs.best_params_","cabc2ab0":"cm = metrics.confusion_matrix(y_test, predictions_xgb)\naccuracy = metrics.accuracy_score(y_test, predictions_xgb)\nerror_rate = 1 - accuracy\nrecall = metrics.recall_score(y_test, predictions_xgb)\nprecision = metrics.precision_score(y_test, predictions_xgb)\nspecificity = cm[0][0] \/ (cm[0][0] + cm[0][1])\nfpr = 1 - specificity\nf_1 = metrics.f1_score(y_test, predictions_xgb)\naucroc = metrics.roc_auc_score(y_test, predictions_xgb)\n\nprint(f\"Accuracy:            {accuracy:0.3%}\")\nprint(f\"Error Rate:          {error_rate:0.3%}\")\nprint(f\"Recall (TPR):        {recall:0.3%}\")\nprint(f\"Precision:           {precision:0.3%}\")\nprint(f\"Specificity (TNR):   {specificity:0.3%}\")\nprint(f\"FPR:                 {fpr:0.3%}\")\nprint(f\"F1:                  {f_1:0.3%}\")\nprint(f\"AUROC:               {aucroc:0.3%}\")","0a7e8c0e":"print(classification_report(y_test, predictions_xgb))","93cd97d6":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (15,5))\n\ndesc = np.asarray([['TN', 'FP'], ['FN', 'TP']])\nlabels = (np.asarray([f\"{cm}\\n{desc}\" for cm, desc in zip(cm.flatten(), desc.flatten())])).reshape(2,2)\nsns.heatmap(cm, annot = labels, linewidths = 0.5, square = True, cmap = 'Blues_r', annot_kws = {'size': 18}, fmt = '', ax = ax1)\nax1.set_title('Confusion Matrix')\n\nmetrics.plot_precision_recall_curve(xgb_rs, X_test, y_test, ax = ax2)\n\nmetrics.plot_roc_curve(xgb_rs, X_test, y_test, label = 'ROC', ax = ax3)\nx_lim = (0,1)\ny_lim = (0,1)\nsns.lineplot(x = x_lim, y = y_lim, ax = ax3)","39b7863f":"cv = cross_val_score(xgb_rs.best_estimator_, X, y, scoring = \"accuracy\", cv = 10, n_jobs = -1)\nmean_cv = np.mean(cv)\nstd_cv = np.std(cv)\n\nprint(f\"10-Cross Validation Mean Explained Variance Score:    {mean_cv:0.3f}\")\nprint(f\"Standard Deviation of Validation:                     {std_cv:0.3f}\")\nprint(f\"Train Score:                                          {xgb_rs.score(X_train, y_train):0.3f}\")\nprint(f\"Test Score:                                           {xgb_rs.score(X_test, y_test):0.3f}\")","146cee85":"def plot_learning_curve(estimator, title, X, y, scoring, ylim = None, cv = None, \n                        n_jobs = -1, train_sizes = np.linspace(.1, 1.0, 5)):\n\n    plt.figure(figsize = (15,5))\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv = cv, n_jobs = n_jobs,\n                                                            train_sizes = train_sizes, scoring = scoring)\n    train_scores_mean = np.mean(train_scores, axis = 1)\n    train_scores_std = np.std(train_scores, axis = 1)\n    test_scores_mean = np.mean(test_scores, axis = 1)\n    test_scores_std = np.std(test_scores, axis = 1)\n    \n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha = 0.1, color = \"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha = 0.1, color = \"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color = \"r\", label = \"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color = \"g\", label = \"Cross-validation score\")\n    plt.legend(loc = \"best\")\n    \n    return plt","5c91f551":"plot_learning_curve(xgb_rs.best_estimator_, \"Learning Curve\", X, y, scoring = 'accuracy', cv = 10, n_jobs = -1)","ac7e9388":"preds_xgb = xgb_rs.best_estimator_.predict(df_test_final.drop('Survived', axis = 1))\npreds_xgb = preds_xgb.astype(int)","35c8d949":"fit_params = dict(early_stopping_rounds = 50, eval_set = [(X_test, y_test)])\n\nrs_params = dict(learning_rate = uniform(loc = 0, scale = 1), boosting_type = ['gbdt', 'dart', 'goss', 'rf'],\n                 n_estimators = [1000], max_depth = [5, 10, 50])\n\n\nlgb_model = lgb.LGBMClassifier()\nlgb_rs = RandomizedSearchCV(estimator = lgb_model, param_distributions = rs_params, n_jobs = -1,\n                                        scoring = 'accuracy', verbose = 5, cv = 10, n_iter = 200, random_state = 34)\nlgb_rs.fit(X_train, y_train, **fit_params)\npredictions_lgb = lgb_rs.predict(X_test)","c532c04f":"pd.DataFrame(lgb_rs.cv_results_).nlargest(5, 'mean_test_score')","89dcaa76":"lgb_rs.best_params_","851ab61a":"cm = metrics.confusion_matrix(y_test, predictions_lgb)\naccuracy = metrics.accuracy_score(y_test, predictions_lgb)\nerror_rate = 1 - accuracy\nrecall = metrics.recall_score(y_test, predictions_lgb)\nprecision = metrics.precision_score(y_test, predictions_lgb)\nspecificity = cm[0][0] \/ (cm[0][0] + cm[0][1])\nfpr = 1 - specificity\nf_1 = metrics.f1_score(y_test, predictions_lgb)\naucroc = metrics.roc_auc_score(y_test, predictions_lgb)\n\nprint(f\"Accuracy:            {accuracy:0.3%}\")\nprint(f\"Error Rate:          {error_rate:0.3%}\")\nprint(f\"Recall (TPR):        {recall:0.3%}\")\nprint(f\"Precision:           {precision:0.3%}\")\nprint(f\"Specificity (TNR):   {specificity:0.3%}\")\nprint(f\"FPR:                 {fpr:0.3%}\")\nprint(f\"F1:                  {f_1:0.3%}\")\nprint(f\"AUROC:               {aucroc:0.3%}\")","3698a087":"print(classification_report(y_test, predictions_lgb))","292f3a86":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (15,5))\n\ndesc = np.asarray([['TN', 'FP'], ['FN', 'TP']])\nlabels = (np.asarray([f\"{cm}\\n{desc}\" for cm, desc in zip(cm.flatten(), desc.flatten())])).reshape(2,2)\nsns.heatmap(cm, annot = labels, linewidths = 0.5, square = True, cmap = 'Blues_r', annot_kws = {'size': 18}, fmt = '', ax = ax1)\nax1.set_title('Confusion Matrix')\n\nmetrics.plot_precision_recall_curve(lgb_rs, X_test, y_test, ax = ax2)\n\nmetrics.plot_roc_curve(lgb_rs, X_test, y_test, label = 'ROC', ax = ax3)\nx_lim = (0,1)\ny_lim = (0,1)\nsns.lineplot(x = x_lim, y = y_lim, ax = ax3)","c0ac40ba":"cv = cross_val_score(lgb_rs.best_estimator_, X, y, scoring = \"accuracy\", cv = 10, n_jobs = -1)\nmean_cv = np.mean(cv)\nstd_cv = np.std(cv)\n\nprint(f\"10-Cross Validation Mean Explained Variance Score:    {mean_cv:0.3f}\")\nprint(f\"Standard Deviation of Validation:                     {std_cv:0.3f}\")\nprint(f\"Train Score:                                          {lgb_rs.score(X_train, y_train):0.3f}\")\nprint(f\"Test Score:                                           {lgb_rs.score(X_test, y_test):0.3f}\")","35359528":"plot_learning_curve(lgb_rs.best_estimator_, \"Learning Curve\", X, y, scoring = 'accuracy', cv = 10, n_jobs = -1)","9b999ef2":"preds_lgb = lgb_rs.best_estimator_.predict(df_test_final.drop('Survived', axis = 1))\npreds_lgb = preds_lgb.astype(int)","1d02c77f":"rs_params = dict(learning_rate = uniform(loc = 0, scale = 1),\n                 n_estimators = [1000])\n\nada_model = AdaBoostClassifier()\nada_rs = RandomizedSearchCV(estimator = ada_model, param_distributions = rs_params, n_jobs = -1,\n                                        scoring = 'accuracy', verbose = 5, cv = 10, n_iter = 50, random_state = 34)\nada_rs.fit(X_train, y_train)\npredictions_ada = ada_rs.predict(X_test)","43dc915f":"pd.DataFrame(ada_rs.cv_results_).nlargest(5, 'mean_test_score')","11c2ab01":"ada_rs.best_params_","d86c6f2b":"cm = metrics.confusion_matrix(y_test, predictions_ada)\naccuracy = metrics.accuracy_score(y_test, predictions_ada)\nerror_rate = 1 - accuracy\nrecall = metrics.recall_score(y_test, predictions_ada)\nprecision = metrics.precision_score(y_test, predictions_ada)\nspecificity = cm[0][0] \/ (cm[0][0] + cm[0][1])\nfpr = 1 - specificity\nf_1 = metrics.f1_score(y_test, predictions_ada)\naucroc = metrics.roc_auc_score(y_test, predictions_ada)\n\nprint(f\"Accuracy:            {accuracy:0.3%}\")\nprint(f\"Error Rate:          {error_rate:0.3%}\")\nprint(f\"Recall (TPR):        {recall:0.3%}\")\nprint(f\"Precision:           {precision:0.3%}\")\nprint(f\"Specificity (TNR):   {specificity:0.3%}\")\nprint(f\"FPR:                 {fpr:0.3%}\")\nprint(f\"F1:                  {f_1:0.3%}\")\nprint(f\"AUROC:               {aucroc:0.3%}\")","1b167c1a":"print(classification_report(y_test, predictions_ada))","d4c626f8":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (15,5))\n\ndesc = np.asarray([['TN', 'FP'], ['FN', 'TP']])\nlabels = (np.asarray([f\"{cm}\\n{desc}\" for cm, desc in zip(cm.flatten(), desc.flatten())])).reshape(2,2)\nsns.heatmap(cm, annot = labels, linewidths = 0.5, square = True, cmap = 'Blues_r', annot_kws = {'size': 18}, fmt = '', ax = ax1)\nax1.set_title('Confusion Matrix')\n\nmetrics.plot_precision_recall_curve(ada_rs, X_test, y_test, ax = ax2)\n\nmetrics.plot_roc_curve(ada_rs, X_test, y_test, label = 'ROC', ax = ax3)\nx_lim = (0,1)\ny_lim = (0,1)\nsns.lineplot(x = x_lim, y = y_lim, ax = ax3)","00ed39dd":"cv = cross_val_score(ada_rs.best_estimator_, X, y, scoring = \"accuracy\", cv = 10, n_jobs = -1)\nmean_cv = np.mean(cv)\nstd_cv = np.std(cv)\n\nprint(f\"10-Cross Validation Mean Explained Variance Score:    {mean_cv:0.3f}\")\nprint(f\"Standard Deviation of Validation:                     {std_cv:0.3f}\")\nprint(f\"Train Score:                                          {ada_rs.score(X_train, y_train):0.3f}\")\nprint(f\"Test Score:                                           {ada_rs.score(X_test, y_test):0.3f}\")","16c32f2b":"plot_learning_curve(ada_rs.best_estimator_, \"Learning Curve\", X, y, scoring = 'accuracy', cv = 10, n_jobs = -1)","30a3f542":"preds_ada = ada_rs.best_estimator_.predict(df_test_final.drop('Survived', axis = 1))\npreds_ada = preds_ada.astype(int)","e3f1416f":"rs_params = dict(C = np.logspace(-4, 4, 1000), solver = ['liblinear', 'saga', 'newton-cg'], max_iter = [1000])\n\nlr_model = LogisticRegression()\nlr_rs = RandomizedSearchCV(estimator = lr_model, param_distributions = rs_params, n_jobs = -1,\n                                        scoring = 'accuracy', verbose = 5, cv = 10, n_iter = 3000, random_state = 34)\nlr_rs.fit(X_train, y_train)\npredictions_lr = lr_rs.predict(X_test)","0a245848":"pd.DataFrame(lr_rs.cv_results_).nlargest(5, 'mean_test_score')","f1d1d21a":"lr_rs.best_params_","4f982bff":"cm = metrics.confusion_matrix(y_test, predictions_lr)\naccuracy = metrics.accuracy_score(y_test, predictions_lr)\nerror_rate = 1 - accuracy\nrecall = metrics.recall_score(y_test, predictions_lr)\nprecision = metrics.precision_score(y_test, predictions_lr)\nspecificity = cm[0][0] \/ (cm[0][0] + cm[0][1])\nfpr = 1 - specificity\nf_1 = metrics.f1_score(y_test, predictions_lr)\naucroc = metrics.roc_auc_score(y_test, predictions_lr)\n\nprint(f\"Accuracy:            {accuracy:0.3%}\")\nprint(f\"Error Rate:          {error_rate:0.3%}\")\nprint(f\"Recall (TPR):        {recall:0.3%}\")\nprint(f\"Precision:           {precision:0.3%}\")\nprint(f\"Specificity (TNR):   {specificity:0.3%}\")\nprint(f\"FPR:                 {fpr:0.3%}\")\nprint(f\"F1:                  {f_1:0.3%}\")\nprint(f\"AUROC:               {aucroc:0.3%}\")","0edd3415":"print(classification_report(y_test, predictions_lr))","0b93f32d":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (15,5))\n\ndesc = np.asarray([['TN', 'FP'], ['FN', 'TP']])\nlabels = (np.asarray([f\"{cm}\\n{desc}\" for cm, desc in zip(cm.flatten(), desc.flatten())])).reshape(2,2)\nsns.heatmap(cm, annot = labels, linewidths = 0.5, square = True, cmap = 'Blues_r', annot_kws = {'size': 18}, fmt = '', ax = ax1)\nax1.set_title('Confusion Matrix')\n\nmetrics.plot_precision_recall_curve(lr_rs, X_test, y_test, ax = ax2)\n\nmetrics.plot_roc_curve(lr_rs, X_test, y_test, label = 'ROC', ax = ax3)\nx_lim = (0,1)\ny_lim = (0,1)\nsns.lineplot(x = x_lim, y = y_lim, ax = ax3)","d549bc52":"cv = cross_val_score(lr_rs.best_estimator_, X, y, scoring = \"accuracy\", cv = 10, n_jobs = -1)\nmean_cv = np.mean(cv)\nstd_cv = np.std(cv)\n\nprint(f\"10-Cross Validation Mean Explained Variance Score:    {mean_cv:0.3f}\")\nprint(f\"Standard Deviation of Validation:                     {std_cv:0.3f}\")\nprint(f\"Train Score:                                          {lr_rs.score(X_train, y_train):0.3f}\")\nprint(f\"Test Score:                                           {lr_rs.score(X_test, y_test):0.3f}\")","4bba43c7":"plot_learning_curve(lr_rs.best_estimator_, \"Learning Curve\", X, y, scoring = 'accuracy', cv = 10, n_jobs = -1)","39d37cc8":"preds_lr = lr_rs.best_estimator_.predict(df_test_final.drop('Survived', axis = 1))\npreds_lr = preds_lr.astype(int)","19acd5c6":"rs_params = dict(C = np.logspace(-4, 2, 300), kernel = ['linear', 'poly', 'rbf'])\n\nsvc_model = SVC()\nsvc_rs = RandomizedSearchCV(estimator = svc_model, param_distributions = rs_params, n_jobs = -1,\n                                        scoring = 'accuracy', verbose = 5, cv = 10, n_iter = 900, random_state = 34)\nsvc_rs.fit(X_train, y_train)\npredictions_svc = svc_rs.predict(X_test)","0dcc9663":"pd.DataFrame(svc_rs.cv_results_).nlargest(5, 'mean_test_score')","572c2d80":"svc_rs.best_params_","1f31f77d":"cm = metrics.confusion_matrix(y_test, predictions_svc)\naccuracy = metrics.accuracy_score(y_test, predictions_svc)\nerror_rate = 1 - accuracy\nrecall = metrics.recall_score(y_test, predictions_svc)\nprecision = metrics.precision_score(y_test, predictions_svc)\nspecificity = cm[0][0] \/ (cm[0][0] + cm[0][1])\nfpr = 1 - specificity\nf_1 = metrics.f1_score(y_test, predictions_svc)\naucroc = metrics.roc_auc_score(y_test, predictions_svc)\n\nprint(f\"Accuracy:            {accuracy:0.3%}\")\nprint(f\"Error Rate:          {error_rate:0.3%}\")\nprint(f\"Recall (TPR):        {recall:0.3%}\")\nprint(f\"Precision:           {precision:0.3%}\")\nprint(f\"Specificity (TNR):   {specificity:0.3%}\")\nprint(f\"FPR:                 {fpr:0.3%}\")\nprint(f\"F1:                  {f_1:0.3%}\")\nprint(f\"AUROC:               {aucroc:0.3%}\")","0ad70716":"print(classification_report(y_test, predictions_svc))","ab92867b":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (15,5))\n\ndesc = np.asarray([['TN', 'FP'], ['FN', 'TP']])\nlabels = (np.asarray([f\"{cm}\\n{desc}\" for cm, desc in zip(cm.flatten(), desc.flatten())])).reshape(2,2)\nsns.heatmap(cm, annot = labels, linewidths = 0.5, square = True, cmap = 'Blues_r', annot_kws = {'size': 18}, fmt = '', ax = ax1)\nax1.set_title('Confusion Matrix')\n\nmetrics.plot_precision_recall_curve(svc_rs, X_test, y_test, ax = ax2)\n\nmetrics.plot_roc_curve(svc_rs, X_test, y_test, label = 'ROC', ax = ax3)\nx_lim = (0,1)\ny_lim = (0,1)\nsns.lineplot(x = x_lim, y = y_lim, ax = ax3)","b398ca57":"cv = cross_val_score(svc_rs.best_estimator_, X, y, scoring = \"accuracy\", cv = 10, n_jobs = -1)\nmean_cv = np.mean(cv)\nstd_cv = np.std(cv)\n\nprint(f\"10-Cross Validation Mean Explained Variance Score:    {mean_cv:0.3f}\")\nprint(f\"Standard Deviation of Validation:                     {std_cv:0.3f}\")\nprint(f\"Train Score:                                          {svc_rs.score(X_train, y_train):0.3f}\")\nprint(f\"Test Score:                                           {svc_rs.score(X_test, y_test):0.3f}\")","0a1a0637":"plot_learning_curve(svc_rs.best_estimator_, \"Learning Curve\", X, y, scoring = 'accuracy', cv = 10, n_jobs = -1)","d0b89b58":"preds_svc = svc_rs.best_estimator_.predict(df_test_final.drop('Survived', axis = 1))\npreds_svc = preds_svc.astype(int)","9e7765fc":"sub = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsub = sub.drop('Survived', axis = 1)\n\nsub['preds_xgb'] = preds_xgb\nsub['preds_lgb'] = preds_lgb\nsub['preds_ada'] = preds_ada\nsub['preds_lr'] = preds_lr\nsub['preds_svc'] = preds_svc\nsub['sum_preds'] = sub['preds_xgb'] + sub['preds_lgb'] + sub['preds_ada'] + sub['preds_lr'] + sub['preds_svc']\n\nsub['Survived'] = sub['sum_preds'].map({0: 0, 1: 0, 2: 0, 3: 0, 4: 1, 5: 1})","aa28f024":"sub.sample(5)","fa29b650":"sub[['PassengerId', 'Survived']].to_csv('major_vote.csv', index = False)","4e919e2e":"## 3. Linear correlation","c12046c9":"#### AdaBoostClassifier","8f1c45fb":"## 5. Submission","a4807fb8":"#### XGBClassifier","8c4f65d8":"#### Name","0a9a7a39":"#### Embarked","4db79769":"## 2. Feature engineering","8ead7e32":"#### Age","a7d29f79":"#### Cabin","8e9560fb":"#### SVC","a7f3dcde":"#### LGBMClassifier","f1de521f":"## 1. EDA","77f7b40f":"#### Fare","86fefb22":"#### SibSp\/Parch","e46f38c9":"#### Ticket","14c5b7ed":"#### LogisticRegression","2b1ad662":"## 4. Models"}}