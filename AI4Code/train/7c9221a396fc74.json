{"cell_type":{"6f583e00":"code","c34a2c80":"code","c1db66c3":"code","8d0a83a5":"code","26bf2f58":"code","b7c1bcf5":"code","3deaa417":"code","0fda964c":"code","296b9041":"code","7950605f":"code","34c31de2":"code","c8f141a4":"code","6b32284b":"code","b6528829":"code","bca644f3":"code","ffb3aea0":"code","336e8086":"code","9bff4d8a":"code","52ac105f":"code","d8b710de":"code","2d1ce1b3":"markdown","b03ce0ae":"markdown","bc594738":"markdown","e9a69dd7":"markdown","b193fa5b":"markdown","864588cd":"markdown","730a4748":"markdown","9e5b4575":"markdown","f815c54d":"markdown","ab8e7169":"markdown","0453ce27":"markdown","3f8dba20":"markdown","c8df0fdc":"markdown"},"source":{"6f583e00":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","c34a2c80":"df = pd.read_csv('..\/input\/avocado.csv')","c1db66c3":"df.head(10)","8d0a83a5":"df.describe()","26bf2f58":"df = df.drop(['Unnamed: 0', 'Date'], axis = 1)\ndf.info()","b7c1bcf5":"f, ax = plt.subplots(1, 1, figsize=(10,8))\ncorr = df.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax)\nax.set_title(\"Correlation Matrix\", fontsize=14)\nplt.show()","3deaa417":"sns.jointplot(x='Small Bags',y='Total Bags',data=df, color='red')","0fda964c":"fig, ax = plt.subplots(1, 1, figsize=(10,6))\nsns.boxplot(x='year',y='AveragePrice',data=df,color='red')","296b9041":"fig, ax = plt.subplots(1, 1, figsize=(10,6))\nprice_val = df['AveragePrice'].values\nsns.distplot(price_val, color='r')\nax.set_title('Distribution of Average Price', fontsize=14)\nax.set_xlim([min(price_val), max(price_val)])","7950605f":"X = df.drop(['AveragePrice'], axis = 1).values\ny = df['AveragePrice'].values\n\n# Encoding categorical data\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X_1 = LabelEncoder()\nX[:, 8] = labelencoder_X_1.fit_transform(X[:, 9])\nlabelencoder_X_2 = LabelEncoder()\nX[:, 9] = labelencoder_X_2.fit_transform(X[:, 10])\nlabelencoder_X_3 = LabelEncoder()\nX[:, 10] = labelencoder_X_3.fit_transform(X[:, 10])","34c31de2":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(df.drop(['AveragePrice', 'type', 'year', 'region'],axis=1))\nscaled_features = scaler.transform(df.drop(['AveragePrice', 'type', 'year', 'region'],axis=1))\ndf_feat = pd.DataFrame(scaled_features,columns=df.columns[1:9])\ndf_feat.head()","c8f141a4":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","6b32284b":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\nfrom sklearn import preprocessing\nfrom sklearn import utils\nlab_enc = preprocessing.LabelEncoder()\ny_train = lab_enc.fit_transform(y_train)\ny_test = lab_enc.fit_transform(y_test)","b6528829":"from sklearn.multiclass import OneVsRestClassifier\n\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg = OneVsRestClassifier(logreg, n_jobs=1)\nlogreg.fit(X_train, y_train)\npred_logreg = logreg.predict(X_test)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\nknn = OneVsRestClassifier(knn, n_jobs=1)\nknn.fit(X_train,y_train)\npred_knn = knn.predict(X_test)\n\nfrom sklearn.svm import SVC\nsvc = SVC()\nsvc = OneVsRestClassifier(svc, n_jobs=1)\nsvc.fit(X_train, y_train)\npred_svc = svc.predict(X_test)\n\nfrom sklearn.tree import DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier()\ndecision_tree = OneVsRestClassifier(decision_tree, n_jobs=1)\ndecision_tree.fit(X_train, y_train)\npred_tree = decision_tree.predict(X_test)","bca644f3":"sns.jointplot(x=y_test, y=pred_logreg, color= 'g')\nsns.jointplot(x=y_test, y=pred_knn, color= 'g')\nsns.jointplot(x=y_test, y=pred_svc, color= 'g')\nsns.jointplot(x=y_test, y=pred_tree, color= 'g')\n\nplt.show()","ffb3aea0":"error_rate = []\nfor i in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))\n    \nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","336e8086":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn = OneVsRestClassifier(knn, n_jobs=1)\nknn.fit(X_train,y_train)\npred_knn = knn.predict(X_test)\nsns.jointplot(x=y_test, y=pred_knn, color= 'g')\nplt.plot()","9bff4d8a":"print(X_train.shape[1])\nprint(X_train.shape[0])\nprint(len(np.unique(y_train)))\nprint((len(np.unique(y_train)) + X_train.shape[0]) \/2)","52ac105f":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.utils import np_utils\nfrom numpy import argmax\n\nBATCH_SIZE = 1000\nEPOCHS = 30\nVALIDATION_SPLIT = 0.1\nfile_path=\"weights_base3.best.hdf5\"\ncheckpoint = ModelCheckpoint(file_path, monitor='val_loss', save_best_only=True, mode='min')\nearly = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\ncallbacks_list = [checkpoint, early]\n\ndef get_model():\n    model = Sequential()\n    model.add(Dense(7428, input_dim=X_train.shape[1]))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(7428))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(256, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', \n                  optimizer='adam', \n                  metrics=['accuracy']\n                 )\n    model.summary()\n    return model\nmodel_nn = get_model()\nmodel_nn.fit(X_train, np_utils.to_categorical(y_train),\n                  batch_size=BATCH_SIZE, \n                  epochs=EPOCHS,\n                  callbacks=callbacks_list,\n                  validation_split=VALIDATION_SPLIT\n             )\nmodel_nn.load_weights(file_path)\npred_ann = argmax(model_nn.predict(X_test), axis = 1)","d8b710de":"sns.jointplot(x=y_test, y=pred_ann, color= 'g')\nplt.plot()","2d1ce1b3":"We choose k = 3.","b03ce0ae":"### ML models implementation","bc594738":"Indeed, the smaller bags, the higher number of them  are taken","e9a69dd7":"Average Price distribution shows that for most cases price of avocado is between 1.1, 1.4.","b193fa5b":"# Avocados - ML models, keras ANN, seaborn plots\n## Introduction\nI like avocados.\n\n## Import libraries and data","864588cd":"Surprisingly or not, price doesn't change among the years'","730a4748":"## Keras neural network will appear soon. Stay tuned :)\nAnd it is. Here i present the best model for that moment. If you have any advices, please let me know :). For now it is not working the best, as SVC model. I tried a model with a few number of units, various batch size, drop out or without and chose the best one. Maybe different number of hidden layers would help.","9e5b4575":"## Visualizations\nTotal volume is higly correlated with small bags and total bags which are correleted to each other too.","f815c54d":"### Confusion matrixes","ab8e7169":"### Standardize the variables","0453ce27":"### Train test split, label encoding","3f8dba20":"As we see, KNN method works the best here, logistic regression and decision tree completely do not work. We could tune k value.\n","c8df0fdc":"##  Implementing machine learning models\n### Data prepearing and encoding categorical variables\n"}}