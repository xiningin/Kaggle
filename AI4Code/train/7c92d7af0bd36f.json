{"cell_type":{"08e15e5b":"code","98271985":"code","7e3b0868":"code","efd141ff":"code","2940fcff":"code","1dc5229a":"code","4e77604e":"code","b3266886":"code","a3aafe4e":"code","90c13010":"code","683b4284":"code","0c0da892":"code","a73bb147":"code","9b50b0fd":"code","74a707b4":"code","6b02e2b8":"code","c09ee99e":"code","7e411843":"code","4d613494":"code","0f1639ef":"code","784a5421":"code","95982d12":"code","39a263bf":"markdown","d4242ac3":"markdown","4a3194da":"markdown","d475e783":"markdown","c791d433":"markdown","14dfd4fc":"markdown","99bd28ad":"markdown","4d843f67":"markdown","c8390c97":"markdown","afbf1d7a":"markdown","134e96ef":"markdown","ebd6a638":"markdown","1d287ee0":"markdown"},"source":{"08e15e5b":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom pandas_profiling import ProfileReport\nfrom hpsklearn import HyperoptEstimator\nfrom hpsklearn import any_regressor\nfrom hpsklearn import any_preprocessing\nfrom hyperopt import tpe\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer, SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.compose import ColumnTransformer\n\nimport matplotlib\nmatplotlib.use('TkAgg')\nimport seaborn as sns\nfrom matplotlib import pyplot as plt","98271985":"# implement SMOTER\n# see paper: https:\/\/core.ac.uk\/download\/pdf\/29202178.pdf\n\ndef get_synth_cases(D, target, o=200, k=3, categorical_col = []):\n    '''\n    Function to generate the new cases.\n    INPUT:\n        D - pd.DataFrame with the initial data\n        target - string name of the target column in the dataset\n        o - oversampling rate\n        k - number of nearest neighbors to use for the generation\n        categorical_col - list of categorical column names\n    OUTPUT:\n        new_cases - pd.DataFrame containing new generated cases\n    '''\n    new_cases = pd.DataFrame(columns = D.columns) # initialize the list of new cases \n    ng = o \/\/ 100 # the number of new cases to generate\n    for index, case in D.iterrows():\n        # find k nearest neighbors of the case\n        knn = KNeighborsRegressor(n_neighbors = k+1) # k+1 because the case is the nearest neighbor to itself\n        knn.fit(D.drop(columns = [target]).values, D[[target]])\n        neighbors = knn.kneighbors(case.drop(labels = [target]).values.reshape(1, -1), return_distance=False).reshape(-1)\n        neighbors = np.delete(neighbors, np.where(neighbors == index))\n        for i in range(0, ng):\n            # randomly choose one of the neighbors\n            x = D.iloc[neighbors[np.random.randint(k)]]\n            attr = {}          \n            for a in D.columns:\n                # skip target column\n                if a == target:\n                    continue;\n                if a in categorical_col:\n                    # if categorical then choose randomly one of values\n                    if np.random.randint(2) == 0:\n                        attr[a] = case[a]\n                    else:\n                        attr[a] = x[a]\n                else:\n                    # if continious column\n                    diff = case[a] - x[a]\n                    attr[a] = case[a] + np.random.randint(2) * diff\n            # decide the target column\n            new = np.array(list(attr.values()))\n            d1 = cosine_similarity(new.reshape(1, -1), case.drop(labels = [target]).values.reshape(1, -1))[0][0]\n            d2 = cosine_similarity(new.reshape(1, -1), x.drop(labels = [target]).values.reshape(1, -1))[0][0]\n            attr[target] = (d2 * case[target] + d1 * x[target]) \/ (d1 + d2)\n            \n            # append the result\n            new_cases = new_cases.append(attr,ignore_index = True)\n                    \n    return new_cases\n\ndef SmoteR(D, target, th = 0.999, o = 200, u = 100, k = 3, categorical_col = []):\n    '''\n    The implementation of SmoteR algorithm:\n    https:\/\/core.ac.uk\/download\/pdf\/29202178.pdf\n    INPUT:\n        D - pd.DataFrame - the initial dataset\n        target - the name of the target column in the dataset\n        th - relevance threshold\n        o - oversampling rate\n        u - undersampling rate\n        k - the number of nearest neighbors\n    OUTPUT:\n        new_D - the resulting new dataset\n    '''\n    # median of the target variable\n    y_bar = D[target].median()\n    \n    # find rare cases where target less than median\n    rareL = D[(relevance(D[target]) > th) & (D[target] > y_bar)]  \n    # generate rare cases for rareL\n    new_casesL = get_synth_cases(rareL, target, o, k , categorical_col)\n    \n    # find rare cases where target greater than median\n    rareH = D[(relevance(D[target]) > th) & (D[target] < y_bar)]\n    # generate rare cases for rareH\n    new_casesH = get_synth_cases(rareH, target, o, k , categorical_col)\n    \n    new_cases = pd.concat([new_casesL, new_casesH], axis=0)\n    \n    # undersample norm cases\n    norm_cases = D[relevance(D[target]) <= th]\n    # get the number of norm cases\n    nr_norm = int(len(norm_cases) * u \/ 100)\n    \n    norm_cases = norm_cases.sample(min(len(D[relevance(D[target]) <= th]), nr_norm))\n    \n    # get the resulting dataset\n    new_D = pd.concat([new_cases, norm_cases], axis=0)\n    \n    return new_D","7e3b0868":"def sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n\ndef relevance(x):\n    x = np.array(x)\n    return sigmoid(x - 0.3)","efd141ff":"categorical_columns = ['categoryA', 'categoryB', 'categoryC', 'categoryD', 'categoryE', 'categoryF', 'unit']","2940fcff":"#Reset the index as the 'id' variable \n\ndf = pd.read_csv(\"train.csv\")\ndf = df.set_index('id')","1dc5229a":"plt.boxplot(df['result'], vert=False)\nplt.title('Area Distribution')\nplt.xlabel('area')\nplt.show()","4e77604e":"#nulls = df[df.isnull().any(axis=1)]\n#df = SmoteR(df[~df.isnull().any(axis=1)], target='result', th = 0.9, o = 50, u = 200, k = 20, categorical_col = categorical_columns)\n#df = pd.concat([df, nulls])","b3266886":"X= df.iloc[:,0:-1]\ny= df['result'].values\n\nenc = OneHotEncoder(sparse=False, handle_unknown='ignore')\ncategorical_data = enc.fit_transform(X[categorical_columns])\n\nimp = SimpleImputer(strategy=\"median\")\nnumerical_data = X[X.columns[~X.columns.isin(categorical_columns)]].values\nnumerical_data = imp.fit_transform(numerical_data)\n\ncategorical_data = pd.DataFrame(categorical_data, index=X.index)\nnumerical_data = pd.DataFrame(numerical_data, index=X.index)\nX = pd.concat([numerical_data, categorical_data], axis=1)\nX = X.values","a3aafe4e":"#Double check to make sure there is no missing data\nnp.any(np.isnan(X))","90c13010":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","683b4284":"model = HyperoptEstimator(regressor=any_regressor('reg'), preprocessing=any_preprocessing('pre'), \n                          loss_fn=mean_squared_error, algo=tpe.suggest, max_evals=30, trial_timeout=30)","0c0da892":"model.fit(X_train, y_train)","a73bb147":"print(\"Test log MSE: {}\".format(np.log(model.score(X_test, y_test))))\nprint(\"Train log MSE: {}\".format(np.log(model.score(X_train, y_train))))","9b50b0fd":"print(model.best_model())","74a707b4":"final_model = Pipeline([('preprocess', model.best_model()['preprocs'][0]),\n                        ('reg', model.best_model()['learner'])\n                      ])\nfinal_model.fit(X, y)","6b02e2b8":"submission_df = pd.read_csv(\"test.csv\")\nX_sub = submission_df.iloc[:,1:]\n\ncategorical_columns = ['categoryA', 'categoryB', 'categoryC', 'categoryD', 'categoryE', 'categoryF', 'unit']\ncategorical_data = enc.transform(X_sub [categorical_columns])\n\nnumerical_data = X_sub[X_sub.columns[~X_sub.columns.isin(categorical_columns)]].values\nnumerical_data = imp.transform(numerical_data)\n\ncategorical_data = pd.DataFrame(categorical_data, index=X_sub.index)\nnumerical_data = pd.DataFrame(numerical_data, index=X_sub.index)\nX_sub = pd.concat([numerical_data, categorical_data], axis=1)\nX_sub = X_sub.values","c09ee99e":"np.any(np.isnan(X_sub))","7e411843":"y_sub = final_model.predict(X_sub)\nsubmission = pd.DataFrame(y_sub, index=submission_df['id'], columns=['result'])","4d613494":"submission.head()","0f1639ef":"submission.to_csv(\"submission.csv\")","784a5421":"y_preds = final_model.predict(X)\ny_preds[y_preds <0]= 0","95982d12":"sns.distplot(y)\nsns.distplot(y_preds)\nplt.show()","39a263bf":"For the actual machine learning portion, I tried using an AutoML system - in this case, Hyperopt. I basically used what I found in this guide: https:\/\/machinelearningmastery.com\/hyperopt-for-automated-machine-learning-with-scikit-learn\/. The process was a bit slow, so I ran a few of these in parallel. In hindsight, I don't think this really would have done any better than just trying XGBoost or CatBoost.","d4242ac3":"Attempting to use SMOTE to correct for the data imbalance did not actually improve performance","4a3194da":"Heart of the actual code begins here","d475e783":"Refit the final model on all of the training data","c791d433":"Add any helper functions below here. during the competition, I tried addressing an imbalance in the target variable with an oversampling strategy for regression that I found, SMOTER. It generally did not help","14dfd4fc":"In this section I tried see if I could evaluate where these errors are happening. Generally, I found my models actually did alright at the higher numbers, but sturggled a little with the really low target values (of which, there are a lot of them). It tended to inflate the low numbers a little bit.","99bd28ad":"Plot of the target variable so the I could oberve the distribution of it.","4d843f67":"At this point, I did one hot encoding for the categorical variables. I then also realized that the numerical columns occasionally had missing data. This problem was also present in the test data, so I couldn't just drop out missing data rows - I have to deal with them in the test data. So, I tried both the Simple Imputer and Iterative Imputer from Sklean. I really did not notice much of a difference between the two. After one-hot encoding and filling in missing numerical data, I then stapled the data back into one data set.","c8390c97":"Check for any missing data after doing the dtaa transformations","afbf1d7a":"After going back to the competition to do the post mortem, I realize I did not actually calculate the evaluation metric correctly :\/. However, this was a good proxy that mirrored eactly the actual results I got back","134e96ef":"Overall, not a bad start, but I think I probably could have been more creative with some feature engineering and with doing something with the target variable, like transforming it into a log space like of the other competitors did.\n\nI can't wait for the next one!!","ebd6a638":"Apply the same numerical and categorical data transformations to the test data","1d287ee0":"## Proposed Solution for Regression Challenge\nOverall, for this challenge I attempted to do imputation to adress data quality issues and find a model by an autoML system. __Note__: I ran this locally on my computer and not on Kaggle, as I cannot seem to get Hyperopt's hpsklearn to work on Kaggle."}}