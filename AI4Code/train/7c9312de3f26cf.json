{"cell_type":{"622cdbd6":"code","f1af795a":"code","8c7a20e9":"code","77928ca9":"code","55802ca3":"code","8e295088":"code","5ccfbb21":"code","23c8652f":"code","5b03c758":"code","832dc14c":"markdown"},"source":{"622cdbd6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/animeface-character-dataset\/animeface-character-dataset\/data\/\"))\n\n# Any results you write to the current directory are saved as output.","f1af795a":"import numpy as np \nimport pandas as pd \nimport os\nimport time\nimport tensorflow as tf\nimport numpy as np\nimport glob\nfrom glob import glob\nimport datetime\nimport random\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport keras\nfrom keras.layers import Input, Dense, Reshape, Flatten, Dropout\nfrom keras.layers import BatchNormalization, Activation, ZeroPadding2D\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.convolutional import UpSampling2D, Conv2D , Conv2DTranspose\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Impoort all packages and ensure that they are all installed within the library","8c7a20e9":"# Hyperparameters\n\n# The Hyperparameters are to ensure the training of the model. I found these to be fit best to ensure the modal doesnt collapse. You may experiment with these values.\nIMAGE_SIZE = 64\nNOISE_SIZE = 100\nLR_D = 0.00004\nLR_G = 0.0004\nBATCH_SIZE = 32\nEPOCHS = 50 # For better results increase this value\nBETA1 = 0.5\nWEIGHT_INIT_STDDEV = 0.02\nEPSILON = 0.00005\nSAMPLES_TO_SHOW = 8\n","77928ca9":"# Create Generator\ndef get_generator(z=(NOISE_SIZE,)):\n    # 4 x 4 x 512\n    input_layer = Input(z)\n    hid = Dense(4*4*512, activation='relu', name =\"Dense\")(input_layer)    \n    hid = LeakyReLU(alpha=0.2)(hid)\n    hid = Reshape((4, 4, 512))(hid)\n\n    # 4x4x512 -> 8x8x512\n    hid = Conv2DTranspose(512, kernel_size=[5,5],\n                               strides=[2,2],\n                               padding=\"same\",\n                               kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"trans_conv1\")(hid)\n    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_trans_conv1\")(hid)\n    hid = LeakyReLU(alpha=0.2,name =\"trans_conv1_out\")(hid)\n    \n\n    # 8x8x512 -> 16x16x256\n    hid = Conv2DTranspose(256, kernel_size=[5,5],\n                               strides=[2,2],\n                               padding=\"same\",\n                               kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"trans_conv2\")(hid)\n    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_trans_conv2\")(hid)\n    hid = LeakyReLU(alpha=0.2,name =\"trans_conv2_out\")(hid)\n    \n    \n    # 16x16x256 -> 32x32x128\n    hid = Conv2DTranspose(128, kernel_size=[5,5],\n                               strides=[2,2],\n                               padding=\"same\",\n                               kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"trans_conv3\")(hid)\n    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_trans_conv3\")(hid)\n    hid = LeakyReLU(alpha=0.2,name =\"trans_conv3_out\")(hid)    \n    \n\n    # 32x32x128 -> 64x64x64\n    hid = Conv2DTranspose(64, kernel_size=[5,5],\n                               strides=[2,2],\n                               padding=\"same\",\n                               kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"trans_conv4\")(hid)\n    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_trans_conv4\")(hid)\n    hid = LeakyReLU(alpha=0.2,name =\"trans_conv4_out\")(hid)        \n    \n\n    # 64x64x64 -> 64x64x3\n    hid = Conv2DTranspose(3, kernel_size=[5,5],\n                               strides=[1,1],\n                               padding=\"same\",\n                               kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),name =\"logits\")(hid)\n  \n    out = Activation(\"tanh\", name =\"out\")(hid)\n    \n    model = Model(inputs=input_layer, outputs=out)\n    model.summary()\n  \n    return model\n\n# Create Discriminator\n\ndef get_discriminator(input_shape=(IMAGE_SIZE, IMAGE_SIZE,3)):\n    # 64x64x3 -> 32x32x32\n    input_layer = Input(input_shape)\n    hid = Conv2D(filters=32,\n                    kernel_size=[5,5],\n                    strides=[2,2],\n                    padding=\"same\",\n                    kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),\n                    name = \"conv1\")(input_layer)\n    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_norm1\")(hid)\n    hid = LeakyReLU(alpha=0.2, name=\"conv1_out\")(hid)    \n    \n    # 32x32x32-> 16x16x64 \n    hid = Conv2D(filters=64,\n                        kernel_size=[5,5],\n                        strides=[2,2],\n                        padding=\"same\",\n                        kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),\n                        name = \"conv2\")(hid)\n    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_norm2\")(hid)\n    hid = LeakyReLU(alpha=0.2, name=\"conv2_out\")(hid) \n    \n    # 16x16x64  -> 8x8x128  \n    hid = Conv2D(filters=128,\n                    kernel_size=[5,5],\n                    strides=[2,2],\n                    padding=\"same\",\n                    kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),\n                    name = \"conv3\")(hid)\n    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_norm3\")(hid)\n    hid = LeakyReLU(alpha=0.2, name=\"conv3_out\")(hid)\n    \n    # 8x8x128 -> 8x8x256\n    hid = Conv2D(filters=256,\n                    kernel_size=[5,5],\n                    strides=[1,1],\n                    padding=\"same\",\n                    kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),\n                    name = \"conv4\")(hid)\n    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_norm4\")(hid)\n    hid = LeakyReLU(alpha=0.2, name=\"conv4_out\")(hid)\n    \n\n    # 8x8x256 -> 4x4x512\n    hid = Conv2D(filters=512,\n                    kernel_size=[5,5],\n                    strides=[2,2],\n                    padding=\"same\",\n                    kernel_initializer= keras.initializers.TruncatedNormal(stddev=WEIGHT_INIT_STDDEV),\n                    name = \"conv5\")(hid)  \n    hid = BatchNormalization(momentum=0.9, epsilon=EPSILON, name=\"batch_norm5\")(hid)\n    hid = LeakyReLU(alpha=0.2, name=\"conv5_out\")(hid)\n    \n    hid = Flatten(name = \"flatten\")(hid)\n\n    out = Dense(1, activation='sigmoid', name = \"ligit\")(hid)\n    model = Model(inputs= input_layer, outputs=out)\n\n    model.summary()\n\n    return model","55802ca3":"#Discriminator\ndiscriminator = get_discriminator((IMAGE_SIZE, IMAGE_SIZE,3))\ndiscriminator.compile(loss='binary_crossentropy',optimizer=Adam(lr=LR_D, beta_1=BETA1),metrics=['accuracy'])\n\n# For the combined model we will only train the generator\ndiscriminator.trainable = False\n\n#Generator\ngenerator = get_generator((NOISE_SIZE,))\n\n#GAN\n# The discriminator takes generated images as input and determines validity\ngan_input = Input(shape=(NOISE_SIZE,))\nx = generator(gan_input)\ngan_out = discriminator(x)\ngan = Model(gan_input, gan_out)\ngan.summary()\n\ngan.compile(loss='binary_crossentropy',optimizer=Adam(lr=LR_G, beta_1=BETA1))","8e295088":"def show_samples(sample_images, name, epoch):\n    figure, axes = plt.subplots(1, len(sample_images), figsize = (IMAGE_SIZE, IMAGE_SIZE))\n    for index, axis in enumerate(axes):\n        axis.axis('off')\n        image_array = sample_images[index]\n        axis.imshow(image_array)\n        image = Image.fromarray(image_array)\n        #image.save(name+\"_\"+str(epoch)+\"_\"+str(index)+\".png\") \n    #plt.savefig(name+\"_\"+str(epoch)+\".png\", bbox_inches='tight', pad_inches=0)\n    plt.show()\n    plt.close()\n    \ndef test(input_z, epoch):\n    samples = generator.predict(input_z[:SAMPLES_TO_SHOW])\n    sample_images = [((sample + 1.0) * 127.5).astype(np.uint8) for sample in samples]\n    show_samples(sample_images, OUTPUT_DIR + \"samples\", epoch)\n    \ndef summarize_epoch(d_losses, g_losses , data_shape, epoch, duration, input_z):\n    minibatch_size = int(data_shape[0]\/\/BATCH_SIZE)\n    print(\"Epoch {}\/{}\".format(epoch, EPOCHS),\n          \"\\nDuration: {:.5f}\".format(duration),\n          \"\\nD Loss: {:.5f}\".format(np.mean(d_losses[-minibatch_size:])),\n          \"\\nG Loss: {:.5f}\".format(np.mean(g_losses[-minibatch_size:])))\n    fig, ax = plt.subplots()\n    plt.plot(d_losses, label='Discriminator', alpha=0.6)\n    plt.plot(g_losses, label='Generator', alpha=0.6)\n    plt.title(\"Losses\")\n    plt.legend()\n    plt.savefig(OUTPUT_DIR + \"losses_\" + str(epoch) + \".png\")\n    plt.show()\n    plt.close()\n    test(input_z, epoch)\n    \ndef get_batches(data):\n    batches = []\n    for i in range(int(data.shape[0]\/\/BATCH_SIZE)):\n        batch = data[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n        augmented_images = []\n        for img in batch:\n            image = Image.fromarray(img)\n            if random.choice([True, False]):\n                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n            augmented_images.append(np.asarray(image))\n        batch = np.asarray(augmented_images)\n        normalized_batch = (batch \/ 127.5) - 1.0\n        batches.append(normalized_batch)\n    return np.array(batches)","5ccfbb21":"# Paths\nINPUT_DATA_DIR = \"..\/input\/animeface-character-dataset\/animeface-character-dataset\/data\/\" # Path to the folder with input images. \n#OUTPUT_DIR = '.\/{date:%Y-%m-%d_%H:%M:%S}\/'.format(date=datetime.datetime.now())\n#if not os.path.exists(OUTPUT_DIR):\n #   os.makedirs(OUTPUT_DIR)\nOUTPUT_DIR =\"\"","23c8652f":"# Import Data\nfrom PIL import Image\nimport re\n# Care has been taken to rid of images that are too similar\/repeated. You can do so using this code.\nexclude_img = []\n\nexclude_img = [s + \".png\" for s in exclude_img]\n\nprint(\"Image Samples\")\n#input_images = np.asarray([np.asarray(Image.open(file).resize((IMAGE_SIZE, IMAGE_SIZE))) for file in glob(INPUT_DATA_DIR + '*')])\ninput_images = np.asarray([np.asarray(Image.open(file).resize((IMAGE_SIZE, IMAGE_SIZE))) for file in glob(INPUT_DATA_DIR + '*') if file not in exclude_img])\n\nprint (\"Input: \" + str(input_images.shape))\n\nnp.random.shuffle(input_images)\n\nsample_images = random.sample(list(input_images), SAMPLES_TO_SHOW)\nshow_samples(sample_images, OUTPUT_DIR + \"inputs\", 0)","5b03c758":"#Training\nprint(\"Training Starts!\")\n\nwarnings.filterwarnings(\"ignore\")\n\nd_losses = []\ng_losses = []\ncum_d_loss = 0\ncum_g_loss = 0\n\n#EPOCHS = 10\nfor epoch in range(EPOCHS):\n    epoch += 1\n    start_time = time.time()\n    \n    for batch_images in get_batches(input_images):\n        \n        noise_data = np.random.normal(0, 1, size=(BATCH_SIZE, NOISE_SIZE))\n        # We use same labels for generated images as in the real training batch\n        generated_images = generator.predict(noise_data)\n        \n        noise_prop = 0.05 # Randomly flip 5% of targets\n        real_labels = np.zeros((BATCH_SIZE, 1)) + np.random.uniform(low=0.0, high=0.1, size=(BATCH_SIZE, 1))\n        flipped_idx = np.random.choice(np.arange(len(real_labels)), size=int(noise_prop*len(real_labels)))\n        real_labels[flipped_idx] = 1 - real_labels[flipped_idx]\n        \n        # Train discriminator on real data\n        d_loss_real = discriminator.train_on_batch(batch_images, real_labels)\n\n\n        # Prepare labels for generated data\n        fake_labels = np.ones((BATCH_SIZE, 1)) - np.random.uniform(low=0.0, high=0.1, size=(BATCH_SIZE, 1))\n        flipped_idx = np.random.choice(np.arange(len(fake_labels)), size=int(noise_prop*len(fake_labels)))\n        fake_labels[flipped_idx] = 1 - fake_labels[flipped_idx]\n        \n        # Train discriminator on generated data\n        d_loss_fake = discriminator.train_on_batch(generated_images, fake_labels)\n        \n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n        cum_d_loss += d_loss\n        d_losses.append(d_loss[0])\n        \n        \n        # Train generator\n        noise_data = np.random.normal(0, 1, size=(BATCH_SIZE, NOISE_SIZE))\n        g_loss = gan.train_on_batch(noise_data, np.zeros((BATCH_SIZE, 1)))\n        cum_g_loss += g_loss\n        g_losses.append(g_loss)\n        \n    if epoch > 0 and epoch % 20 == 0 :\n        print(\"saving model\")\n        discriminator.save_weights(\"desc-simposon-model.h5-\" + str(epoch))\n        gan.save_weights(\"gan-simposon-model.h5-\" + str(epoch))\n        \n    # Plot the progress\n    summarize_epoch(d_losses, g_losses, input_images.shape, epoch, time.time()-start_time, noise_data)","832dc14c":"The Purpose of this project is to be able to use DCGANs for anime character generation. This project will take a dataset of anime faces that are cropped and attempt to create similar to what the dataset holds. I trained this for 50 epochs on this platform for demonstration purposes. Train more for better results."}}