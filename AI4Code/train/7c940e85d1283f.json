{"cell_type":{"372cc0f5":"code","7f683057":"code","eab12e8d":"code","273499c2":"code","86b2a050":"code","5b69a7e6":"code","5a184a0a":"code","71f6f482":"code","c5916942":"code","8bfd10fe":"code","ad8f6435":"code","d1730cf1":"code","347eef4f":"code","aea141ed":"code","0a7e2365":"code","fad64d79":"code","d1dc7561":"code","b91794ec":"code","b966468a":"markdown","e8c94cc1":"markdown","42d35040":"markdown"},"source":{"372cc0f5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#import pytz as tz\n#from datetime import datetime\nimport sklearn\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import cm\nfrom collections import OrderedDict","7f683057":"df=pd.read_csv(\"...\/snapptrip.csv\")","eab12e8d":"print(df.info())\nprint(df.shape)\nprint(df.isnull().sum())\ndf.head()","273499c2":"def parse_datetime(s):\n    tzone = tz.timezone(\"America\/New_York\") #parse_datetime\n    utc = datetime.strptime(s, '%Y-%m-%dT%H:%M:%SZ')\n    return tz.utc.localize(utc).astimezone(tzone)\ndf['ts'] = df['departure_date'].apply(lambda x: parse_datetime(x))\ndf['departure_date'] = df['departure_date'].drop('departure_date',axis=1,errors='ignore')\n\n#local date and time\ndf['departure_date']  = df['departure_date'].astype(object).apply(lambda x : x.date())\ndf['departure_date']  = df['departure_date'].astype(object).apply(lambda x : x.time())","86b2a050":"#Observing correlation\ncorr = df.corr()\ncorr.style.background_gradient(cmap='coolwarm')","5b69a7e6":"#The most flights in 2 years 2019-2018\nplt.figure(figsize=(8,5))\nsns.countplot(df['departure-Year'] , color='#c9090F')\nplt.show()\nplt.figure(figsize=(8,5))\nsns.countplot(df['paid-Year'])","5a184a0a":"#Which airlines has the most flights\nplt.figure(figsize=(7,8))\ndf['airline'].value_counts()[:5].plot(kind='pie',autopct='%1.1f%%',shadow=True,legend = True)\nplt.show()","71f6f482":"group = data.groupby(['paid-Year','airline']).sum()\ntotal_sold = group[\"Tickets sold\"].groupby(level=0, group_keys=False)\nprint(total_sold.nlargest(5))\ngroup = data.groupby(['paid-Year','paid-Month']).sum()\ntotal_sold = group[\"Tickets sold\"].groupby(level=0, group_keys=False)\nprint(total_sold.nlargest(12))","c5916942":"# Plots of Total price by Hour, Year and month\nsns.pairplot(data.dropna(),\n             hue='Tickets sold',\n             x_vars=['paid-Hour','paid-Month','paid-Year'],\n             y_vars='Total Price (after discount)',\n             height=5,\n             plot_kws={'alpha':0.1, 'linewidth':0}\n            )\nplt.suptitle('Total price by Hour, Year and month of Year')\nplt.show()\ntotal_sold.plot(figsize=(16,4),legend=True)\nplt.title('\u060cTicket Sales Yy Years - Months')\nplt.show()\ngroup2 = data.groupby(['paid-Year','paid-Month']).sum()\ntotal_sold_per_hour = group2['Total Price (after discount)'].groupby(level=0, group_keys=False)\ntotal_sold_per_hour.plot(figsize=(15,4),legend=True)\nplt.title('Price Volatility By Year - Month')\nplt.show()","8bfd10fe":"# Feature Selection\nX_df = data.drop(columns='Original Price')\ny = data['Original Price'].values\n#One Hot Encoding\nencoder = OneHotEncoder()\nX = encoder.fit_transform(X_df.values)\nprint('type of X is :',X.dtype)\nfor category in encoder.categories_:\n    print(category[:5])","ad8f6435":"print(df.groupby(['airline']).mean())\nprint(df.groupby(['airline']).std())\n#df.groupby(['airline']).count()","d1730cf1":"data=df.drop(['paid_date','departure_date','DepDate','PDate','paidTime','departureTime','paidYear','departureYear'] , axis=1)","347eef4f":"#Total number of tickets sold by airline-year and month-year\ngroup = data.groupby(['paid-Year','airline']).sum()\ntotal_sold = group[\"Tickets sold\"].groupby(level=0, group_keys=False)\nprint(total_sold.nlargest(5))\ngroup = data.groupby(['paid-Year','paid-Month']).sum()\ntotal_sold = group[\"Tickets sold\"].groupby(level=0, group_keys=False)\nprint(total_sold.nlargest(12))","aea141ed":"# Plots of Total price by Hour, Year and month\nsns.pairplot(data.dropna(),\n             hue='Tickets sold',\n             x_vars=['paid-Hour','paid-Month','paid-Year'],\n             y_vars='Total Price (after discount)',\n             height=5,\n             plot_kws={'alpha':0.1, 'linewidth':0}\n            )\nplt.suptitle('Total price by Hour, Year and month of Year')\nplt.show()\ntotal_sold.plot(figsize=(16,4),legend=True)\nplt.title('\u060cTicket Sales Yy Years - Months')\nplt.show()\ngroup2 = data.groupby(['paid-Year','paid-Month']).sum()\ntotal_sold_per_hour = group2['Total Price (after discount)'].groupby(level=0, group_keys=False)\ntotal_sold_per_hour.plot(figsize=(15,4),legend=True)\nplt.title('Price Volatility By Year - Month')\nplt.show()","0a7e2365":"pd.plotting.scatter_matrix (data.loc[1:10,:],figsize = (10,10))\nplt.show()\npd.plotting.scatter_matrix(data, alpha=0.2, figsize=(10, 10), diagonal='kde')\nplt.show()","fad64d79":"# Choosing Influential Variables by ExtraTreesClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nX_df = data.drop(columns='Original Price')\narray=X_df.values\nX=array[:,1:15]\ny = data['Original Price'].array\nY=y.astype('int')\n\n# feature extraction\nforest = ExtraTreesClassifier()\nforest.fit(X, Y)\nimportances = forest.feature_importances_\nprint(importances)\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n       color='gold', yerr=std[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), indices)\nplt.xlim([-1, X.shape[1]])\nplt.show()","d1dc7561":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom matplotlib import cm\nfrom colorspacious import cspace_converter\nfrom collections import OrderedDict\nnew_df['airline'] = new_df['airline'].astype(str)\nlabel_encoder = LabelEncoder()\nnew_df['airline'] = label_encoder.fit_transform(new_df['airline'])\ncolormap = plt.cm.spring\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1, size=15)\nsns.heatmap(new_df.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","b91794ec":"#we have multicollinearity in our variable, there are 2 ways for handling : apply pca or using ridge regression\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import scale\n\narray = new_df.values\nXdf = new_df.drop(columns='log_price')\narray=Xdf.values\nX=array[:,0:4]\n\ny = new_df['log_price'].array\n# feature extraction\nX=scale(Xdf)\npca = PCA(n_components=2)\nfit = pca.fit(X)\n\n# summarize components\nprint(\"Explained Variance: %s\",fit.explained_variance_)\nprint(\"Explained Variance Ration: %s\",fit.explained_variance_ratio_)\nprint(\"Explained Variance cumulative: %s\",fit.explained_variance_ratio_.cumsum())\nprint(\"Singular values\",fit.singular_values_) \nprint(fit.components_)","b966468a":"# Total number of tickets sold by airline-year and month-year","e8c94cc1":"\tairline\tpaid_date\tdeparture_date\tOriginal Price\tTotal Price (after discount)\tTickets sold\tDepDate\tPDate\tdepartureYear\tpaidYear\tdepartureTime\tpaidTime\tdeparture-Year\tdeparture-Month\tdeparture-Hour\tpaid-Year\tpaid-Month\tpaid-Hour\tdeparture-day\tpaid-day\n0\tA\t2019-01-02T11:00:00.000+03:30\t2019-01-02T14:00:00.000+03:30\t687.80\t615.40\t1\t2019-01-02 14:00:00.000+03:30\t2019-01-02 11:00:00.000+03:30\t2019-01-02\t2019-01-02\t14:00:00.000+03:30\t11:00:00.000+03:30\t2019\t01\t14\t2019\t01\t11\t02\t02\n1\tA\t2019-01-20T10:00:00.000+03:30\t2019-01-20T14:00:00.000+03:30\t410.88\t367.62\t1\t2019-01-20 14:00:00.000+03:30\t2019-01-20 10:00:00.000+03:30\t2019-01-20\t2019-01-20\t14:00:00.000+03:30\t10:00:00.000+03:30\t2019\t01\t14\t2019\t01\t10\t20\t20\n2\tA\t2019-02-17T12:00:00.000+03:30\t2019-02-17T15:00:00.000+03:30\t687.80\t615.40\t1\t2019-02-17 15:00:00.000+03:30\t2019-02-17 12:00:00.000+03:30\t2019-02-17\t2019-02-17\t15:00:00.000+03:30\t12:00:00.000+03:30\t2019\t02\t15\t2019\t02\t12\t17\t17\n3\tA\t2019-02-24T10:00:00.000+03:30\t2019-02-24T15:00:00.000+03:30\t1174.20\t945.54\t2\t2019-02-24 15:00:00.000+03:30\t2019-02-24 10:00:00.000+03:30\t2019-02-24\t2019-02-24\t15:00:00.000+03:30\t10:00:00.000+03:30\t2019\t02\t15\t2019\t02\t10\t24\t24\n4\tA\t2019-03-02T10:00:00.000+03:30\t2019-03-02T14:00:00.000+03:30\t821.75\t661.72\t2\t2019-03-02 14:00:00.000+03:30\t2019-03-02 10:00:00.000+03:30\t2019-03-02\t2019-03-02\t14:00:00.000+03:30\t10:00:00.000+03:30\t2019\t03\t14\t2019\t03\t10\t02\t02","42d35040":"Business and Analytics Goals\n\nFor our project we determined two overarching business goals that we would focus on, which we will then expand upon in the analytics goals and general model. Firstly, our central business goal is to create a dynamic pricing tool that determines a price. By determining price at the certain period, we want to create a model that was as flexible as possible. \nOur first analytics goal is to find the function of price in terms of demand via k-means clustering. Our secondly business goal is to present the Recommendation system, that designed by certain function. \n"}}