{"cell_type":{"4e53f923":"code","e4d732db":"code","0373cd4f":"code","532cc457":"code","7be429e5":"code","fca0c0c8":"code","5bd06719":"code","ce31e6be":"code","f0a62d38":"code","20748f62":"code","f772b49d":"code","a8b7fd8e":"code","2a37e258":"code","b1afbf53":"code","679fa50b":"code","858498da":"code","da1524c1":"code","4b8d61ef":"code","2c07029a":"code","c306db85":"code","3359b998":"code","2ac0f587":"code","45a60f3c":"code","9eb0f440":"code","ee112f02":"code","14d52cf6":"code","d394303d":"code","8ccde3f5":"code","39dea075":"markdown","bc6617d6":"markdown","04fefa7c":"markdown","1bbaa642":"markdown","63b8e85d":"markdown","81f1ef32":"markdown","39b6e630":"markdown","355d2335":"markdown","e204a4ef":"markdown","6c7d8afc":"markdown"},"source":{"4e53f923":"!pip install icevision[all] > \/dev\/null","e4d732db":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n%matplotlib inline\nfrom PIL import Image\nimport glob\nfrom tqdm.notebook import tqdm\nfrom icevision.all import *\nimport ast","0373cd4f":"WIDTH = 1280\nHEIGHT= 720\n\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes","532cc457":"PATH = \"..\/input\/tensorflow-great-barrier-reef\/train_images\/**\"\nimages = glob.glob(PATH + '\/*.jpg')\ndataset = pd.read_csv('..\/input\/tensorflow-great-barrier-reef\/train.csv')\ndataset[\"num_bbox\"] = dataset['annotations'].apply(lambda x: str.count(x, 'x'))\ndataset = dataset[dataset[\"num_bbox\"]>0]\ndataset['annotations'] = dataset['annotations'].apply(lambda x: ast.literal_eval(x))\ndataset['bboxes'] = dataset.annotations.apply(get_bbox)\ndataset['video_frame'] = dataset['video_frame'].apply(lambda x: str(x)+ '.jpg')\ndataset[\"video_id\"] = dataset['video_id'].apply(lambda x: 'video_' + str(x))\ndataset['paths'] = '..\/input\/tensorflow-great-barrier-reef\/train_images\/'+ dataset['video_id'] + '\/' + dataset['video_frame'] \ndataset['label'] = 'Fish'","7be429e5":"def show_sample_image(n=2):\n    img_index = list(np.round((np.random.random(n))*len(dataset)))\n    fig,ax = plt.subplots(1,n,figsize=(10,10))\n    i = 0\n    for idx in img_index:\n        img = dataset.paths.values[int(idx)]\n        img = Image.open(img)\n        if n == 1:ax.imshow(img)\n        else:ax[i].imshow(img)\n            \n        annot = dataset.bboxes.values[int(idx)]\n        for a in range(len(annot)):\n            xmin = annot[a][0]\n            ymin = annot[a][1]\n            width = annot[a][2]\n            height = annot[a][3]\n            rect = patches.Rectangle((xmin, ymin), width, height, linewidth=3, edgecolor='r', facecolor='none')\n            if n==1:ax.add_patch(rect)\n            else:ax[i].add_patch(rect)\n        i = i+1\n    plt.show()\n    return None","fca0c0c8":"show_sample_image(n=1)","5bd06719":"training_df = pd.DataFrame(columns = ['img','xmin','ymin','label','width','height'])\n\nfor i in tqdm(range(len(dataset))):\n    annot = dataset.iloc[i].bboxes\n    annot_array = np.array(annot)\n    label = np.array([dataset.iloc[i].label] * annot_array.shape[0])\n    xmin = annot_array[:,0]\n    ymin = annot_array[:,1]\n    width = annot_array[:,2]\n    height = annot_array[:,3]\n    img = str(dataset.iloc[i].paths)\n    inter_df = pd.DataFrame({'img':img,'label':label,'xmin':xmin,'ymin':ymin,'width':width,'height':height})\n    training_df = training_df.append(inter_df)","ce31e6be":"template_record = ObjectDetectionRecord()","f0a62d38":"class MyParser(Parser):\n    def __init__(self, template_record):\n        super().__init__(template_record=template_record)\n\n        self.df = training_df\n        self.class_map = ClassMap(list(self.df['label'].unique()))\n\n    def __iter__(self) -> Any:\n        for o in self.df.itertuples():\n            yield o\n\n    def __len__(self) -> int:\n        return len(self.df)\n\n    def record_id(self, o) -> Hashable:\n        return o.img\n\n    def parse_fields(self, o, record, is_new):\n        if is_new:\n            record.set_filepath(o.img)\n            record.set_img_size(ImgSize(width=WIDTH, height=HEIGHT))\n            record.detection.set_class_map(self.class_map)\n            \n        record.detection.add_bboxes([BBox.from_xyxy(o.xmin, o.ymin, o.xmin + o.width, o.ymin + o.height)])\n        record.detection.add_labels([o.label])","20748f62":"parser = MyParser(template_record)","f772b49d":"train_records, valid_records = parser.parse()","a8b7fd8e":"parser.class_map","2a37e258":"train_records[10]","b1afbf53":"show_record(train_records[1], display_label=True, figsize=(14, 10))","679fa50b":"# Transforms\n# size is set to 384 because EfficientDet requires its inputs to be divisible by 128\nimage_size = 384\ntrain_tfms = tfms.A.Adapter([*tfms.A.aug_tfms(size=image_size, presize=512), tfms.A.Normalize()])\nvalid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(image_size), tfms.A.Normalize()])","858498da":"train_ds = Dataset(train_records, train_tfms)\nvalid_ds = Dataset(valid_records, valid_tfms)","da1524c1":"samples = [train_ds[0] for _ in range(3)]\nshow_samples(samples, ncols=3)","4b8d61ef":"extra_args = {}\nmodel_type = models.ross.efficientdet\nbackbone = model_type.backbones.tf_lite0\n# The efficientdet model requires an img_size parameter\nextra_args['img_size'] = image_size","2c07029a":"model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map), **extra_args) ","c306db85":"train_dl = model_type.train_dl(train_ds, batch_size=32, shuffle=True)\nvalid_dl = model_type.valid_dl(valid_ds, batch_size=32, shuffle=False)","3359b998":"learn = model_type.fastai.learner(dls=[train_dl,valid_dl], model=model)","2ac0f587":"learn.lr_find()","45a60f3c":"learn.fine_tune(10, 1e-2, freeze_epochs=1)","9eb0f440":"infer_dl = model_type.infer_dl(valid_ds, batch_size=4, shuffle=False)\npreds = model_type.predict_from_dl(model, infer_dl, keep_images=True,detection_threshold=0.2)","ee112f02":"img_number = 10\npred_bboxes_list, gt_bboxes_list = [], []\npred_bboxes = preds[img_number].pred.as_dict()['detection']['bboxes']\nground_truth_bboxes = preds[img_number].ground_truth.as_dict()['detection']['bboxes']\npreds_df = pd.DataFrame(columns=['x','y','width','height'])\ngt_df = pd.DataFrame(columns=['x','y','width','height'])\n\nif len(pred_bboxes) > 0:\n    for box in pred_bboxes:\n        pred_bboxes_list.append([box.xmin,box.ymin,box.xmax - box.xmin,box.ymax - box.ymin])\n    preds_df[['x','y','width','height']] = pred_bboxes_list\n    preds_df = preds_df.astype(int)\n    \nif len(ground_truth_bboxes) > 0:\n    for box in ground_truth_bboxes:\n        gt_bboxes_list.append([box.xmin,box.ymin,box.xmax - box.xmin,box.ymax - box.ymin])\n    gt_df[['x','y','width','height']] = gt_bboxes_list\n    gt_df = gt_df.astype(int)    ","14d52cf6":"print(f'predicted bboxes of image-{img_number}')\npreds_df","d394303d":"print(f'ground truth bboxes of image-{img_number}')\ngt_df","8ccde3f5":"show_preds(preds=preds[10:11],figsize=(15,15))","39dea075":"# \ud83e\uddca Loading icevision","bc6617d6":"## kaggle kernels\n\n1- https:\/\/www.kaggle.com\/aninda\/icevision\/data\n\n2- https:\/\/www.kaggle.com\/nyanswanaung\/cots-coco-stratifiedk-5-folds-4919-imgs\n\n## Ice vision official website\n \nhttps:\/\/airctic.com\/0.11.0\/getting_started_object_detection\/","04fefa7c":"# \ud83d\udc68\u200d\ud83c\udfeb model training","1bbaa642":"# \ud83c\udfd7\ufe0f creating your model","63b8e85d":"# \ud83d\udcda Importing Libraries","81f1ef32":"## augmentation","39b6e630":"![icevision-logo-slogan.png](attachment:a0b2546f-e0e5-4c7b-830e-cf28d8763345.png)","355d2335":"# \u2699\ufe0f Parse the dataset","e204a4ef":"# \ud83d\udd0e references","6c7d8afc":"# \u2728 model inference "}}