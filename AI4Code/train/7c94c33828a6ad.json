{"cell_type":{"a19ceccd":"code","10692c1e":"code","59fad96c":"code","4a33f0ec":"code","a717a14e":"code","90497852":"code","19a519b0":"code","15286558":"code","72a627af":"code","ef93d110":"code","475c99b1":"code","e4dd68f4":"code","54312c9b":"code","7c600114":"code","dbf1d58d":"code","96860d65":"code","32d4e14f":"code","d2df5f1e":"code","52373a08":"code","ef7e03e8":"code","9e7a86be":"code","0aa470e4":"code","ad00558c":"code","09b28a13":"code","f48f9bdd":"code","ad0b9ecc":"markdown","ea8062d9":"markdown","24438d31":"markdown","2127e7ea":"markdown","3cb74b98":"markdown","b0d43745":"markdown","a0341bb5":"markdown","f61537f1":"markdown","7db06d72":"markdown"},"source":{"a19ceccd":"import numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n%matplotlib inline \nstopwords = set(STOPWORDS)","10692c1e":"DATA_FOLDER = \"\/kaggle\/input\/tweet-sentiment-extraction\/\"\ntrain_df = pd.read_csv(os.path.join(DATA_FOLDER, \"train.csv\"))\ntest_df = pd.read_csv(os.path.join(DATA_FOLDER, \"test.csv\"))","59fad96c":"print(f\"train: {train_df.shape}  test: {test_df.shape}\")","4a33f0ec":"train_df.head()","a717a14e":"test_df.head()","90497852":"def show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=40, \n        scale=5,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(10,8))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=14)\n        fig.subplots_adjust(top=2.3)\n    plt.imshow(wordcloud)\n    plt.show()","19a519b0":"show_wordcloud(train_df['text'], 'train: text')","15286558":"show_wordcloud(train_df['selected_text'], 'train: selected text')","72a627af":"show_wordcloud(test_df['text'], 'test: text')","ef93d110":"def plot_sentiment_count(data_df, title):\n    plt.figure(figsize=(8,6))\n    sns.countplot(data_df['sentiment'])\n    plt.title(title)\n    plt.show()","475c99b1":"plot_sentiment_count(train_df, \"Sentiment distribution: train\")","e4dd68f4":"plot_sentiment_count(test_df, \"Sentiment distribution: test\")","54312c9b":"for sentiment in train_df.sentiment.unique():\n    show_wordcloud(train_df.loc[train_df['sentiment']==sentiment, 'text'], f'train data - (sentiment: {sentiment}): text')","7c600114":"for sentiment in train_df.sentiment.unique():\n    show_wordcloud(train_df.loc[train_df['sentiment']==sentiment, 'selected_text'], f'train data - (sentiment: {sentiment}): selected text')","dbf1d58d":"for sentiment in test_df.sentiment.unique():\n    show_wordcloud(test_df.loc[test_df['sentiment']==sentiment, 'text'], f'test data - (sentiment: {sentiment}): text')","96860d65":"punct_mapping = \"\/-'?!.,#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~\" + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'\npunct_mapping += '\u00a9^\u00ae` <\u2192\u00b0\u20ac\u2122\u203a \u2665\u2190\u00d7\u00a7\u2033\u2032\u00c2\u2588\u00bd\u00e0\u2026\u201c\u2605\u201d\u2013\u25cf\u00e2\u25ba\u2212\u00a2\u00b2\u00ac\u2591\u00b6\u2191\u00b1\u00bf\u25be\u2550\u00a6\u2551\u2015\u00a5\u2593\u2014\u2039\u2500\u2592\uff1a\u00bc\u2295\u25bc\u25aa\u2020\u25a0\u2019\u2580\u00a8\u2584\u266b\u2606\u00e9\u00af\u2666\u00a4\u25b2\u00e8\u00b8\u00be\u00c3\u22c5\u2018\u221e\u2219\uff09\u2193\u3001\u2502\uff08\u00bb\uff0c\u266a\u2569\u255a\u00b3\u30fb\u2566\u2563\u2554\u2557\u25ac\u2764\u00ef\u00d8\u00b9\u2264\u2021\u221a'\n\npuncts = {\"\u2018\": \"'\", \"\u00b4\": \"'\", \"\u00b0\": \"\", \"\u20ac\": \"e\", \"\u2014\": \"-\", \"\u2013\": \"-\", \"\u2019\": \"'\", \"_\": \"-\", \"`\": \"'\", '\u201c': '\"', '\u201d': '\"', '\u201c': '\"', \"\u00a3\": \"e\", '\u221e': 'infinity', '\u03b8': 'theta', '\u00f7': '\/', '\u03b1': 'alpha', '\u2022': '.', '\u00e0': 'a', '\u2212': '-', '\u03b2': 'beta', '\u2205': '', '\u00b3': '3', '\u03c0': 'pi', '\u2026': ' '}\n\ndef clean_special_chars(text, punct, mapping):\n    '''\n    credits to: https:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-when-using-embeddings \n    credits to: https:\/\/www.kaggle.com\/anebzt\/quora-preprocessing-model\n    input: current text, punctuations, punctuation mapping\n    output: cleaned text\n    '''\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    for p in punct:\n        text = text.replace(p, f' {p} ') \n    return text","32d4e14f":"train_df['text'] = train_df['text'].fillna(\"\")\ntrain_df['selected_text'] = train_df['selected_text'].fillna(\"\")\ntest_df['text'] = test_df['text'].fillna(\"\")","d2df5f1e":"train_df['cleaned_text'] = train_df['text'].apply(lambda x: clean_special_chars(x, punct_mapping, puncts))\ntest_df['cleaned_text'] = test_df['text'].apply(lambda x: clean_special_chars(x, punct_mapping, puncts))\ntrain_df['cleaned_selected_text'] = train_df['selected_text'].apply(lambda x: clean_special_chars(x, punct_mapping, puncts))","52373a08":"import gensim\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n            result.append(token)\n    return result","ef7e03e8":"train_df['preproc_text'] = train_df['cleaned_text'].apply(lambda x: preprocess(x))\ntest_df['preproc_text'] = test_df['cleaned_text'].apply(lambda x: preprocess(x))\ntrain_df['preproc_selected_text'] = train_df['cleaned_selected_text'].apply(lambda x: preprocess(x))","9e7a86be":"train_df['cnt_text'] = train_df['preproc_text'].apply(lambda x: len(x))\ntest_df['cnt_text'] = test_df['preproc_text'].apply(lambda x: len(x))\ntrain_df['cnt_selected_text'] = train_df['preproc_selected_text'].apply(lambda x: len(x))","0aa470e4":"def plot_feature_density(data_df, feature='cnt_text', title=''):\n    plt.figure(figsize=(8,6))\n    for sentiment in data_df.sentiment.unique():\n        sns.distplot(data_df.loc[data_df['sentiment']==sentiment, feature], kde=True, hist=False, label=sentiment)\n    plt.title(title)\n    plt.show()","ad00558c":"plot_feature_density(train_df, 'cnt_text', 'Word count distribution - text - train')","09b28a13":"plot_feature_density(test_df, 'cnt_text', 'Word count distribution - text - test')","f48f9bdd":"plot_feature_density(train_df, 'cnt_selected_text', 'Word count distribution - selected text - train')","ad0b9ecc":"The sentiment data is not balanced and, more that this, it is not balanced with respect to train and test data.  \n\nLet's look as well to the train\/test words distribution, grouped on sentiment.  \n","ea8062d9":"# Load data","24438d31":"# Load packages","2127e7ea":"# Pre-process text\n\nWe will perform the following text transformations:\n* Clean special characters;  \n* Clean punctuations;  \n* Eliminate stopwords;  \n* Convert to lovercase;","3cb74b98":"<h1>Tweet Sentiment Extraction - Exploratory Data Analysis<\/h1>\n\n\nThe objective of this competition is to extract a selected text from entire text that corresponds to a given sentiment, for each data sample. \n\nThe train data contains as well the extracted text, while test data only contains the text and sentiment.  ","b0d43745":"Let's count now the tokens.  \nWe will visualize the distribution of number of tokens \/ each sentiment.","a0341bb5":"# Data exploration\n\nLet's explore the train and test data.\n\n\nWe start by looking to the text distribution in train and test, as well as selected text in train data.","f61537f1":"## Glimpse the data","7db06d72":"Let's look to the distribution of sentiments in the train and test data."}}