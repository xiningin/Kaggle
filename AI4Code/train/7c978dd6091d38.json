{"cell_type":{"a38c683e":"code","1d20d156":"code","b7516c25":"code","c362331a":"code","9d22a46c":"code","8da6415b":"code","929a45cc":"code","22089bb0":"code","9c0e258c":"code","1e2fc0ec":"code","95473e4a":"code","872c4df3":"code","2227e55a":"code","756fccdd":"code","f3a605c8":"code","117dc737":"code","16be9e8a":"code","0856cc83":"code","63e5ce6f":"code","e0cb35f7":"code","d817422c":"code","c24ed3b9":"code","37b9c31d":"code","0a54a5f1":"code","ccc53e69":"code","6f4369b3":"code","a17dbc3e":"code","b601de36":"code","fe331b31":"code","b43431bd":"code","2f75cf93":"code","2dc47f86":"code","815394a2":"code","e39eddae":"code","93604fbe":"code","fbbd4df6":"code","afdd8389":"markdown","83cafdb8":"markdown","d239fc78":"markdown","99286430":"markdown","59db3a01":"markdown","375fb89c":"markdown","c0cc71b2":"markdown","01e09369":"markdown","ecf97dec":"markdown","8b47c920":"markdown","459fb285":"markdown","3bec0df5":"markdown"},"source":{"a38c683e":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1d20d156":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","b7516c25":"dataset = pd.DataFrame(pd.read_csv(\"\/kaggle\/input\/amdgoogle\/AMD.csv\"))","c362331a":"dataset.shape","9d22a46c":"dataset.head()","8da6415b":"dataset.tail()","929a45cc":"# check for any correlation\nplt.figure(figsize = (10,10))\nsns.heatmap(dataset.corr(), annot = True, fmt = \".1g\", vmin = -1, vmax = 1, center = 0, linewidth = 3,\n           linecolor = \"black\", square = True)","22089bb0":"dataset.info()","9c0e258c":"plt.figure(figsize = (20, 12))\nx = np.arange(0, dataset.shape[0], 1)\nplt.subplot(2,1,1)\nplt.plot(x, dataset.Open.values, color = \"red\", label = \"Open AMD Price\")\nplt.plot(x, dataset.Close.values, color = \"blue\", label = \"Close AMD Price\")\nplt.title(\"AMD Stock Prices 2009-2018\", fontsize = 18)\nplt.xlabel(\"Days\", fontsize = 18)\nplt.ylabel(\"Stock Prices in US Dollar\", fontsize = 18)\nplt.legend(loc = \"best\")\nplt.grid(which = \"major\", axis = \"both\")\n\nplt.subplot(2,1,2)\nplt.plot(x, dataset.Volume.values, color = \"green\", label = \"Stock Volume Available\")\nplt.title(\"Stock Volume of AMD b\/w 2009-2018\", fontsize = 18)\nplt.xlabel(\"Days\", fontsize = 18)\nplt.ylabel(\"Volume\", fontsize = 18)\nplt.legend(loc = \"best\")\nplt.grid(which = \"major\", axis = \"both\")\nplt.show()","1e2fc0ec":"TIME_STEP = 7\nDAYS = 20 # number of days at the end for which we have to predict. These will be in our validation set.","95473e4a":"dataset = pd.DataFrame(pd.read_csv(\"\/kaggle\/input\/amdgoogle\/AMD.csv\"))","872c4df3":"def dataset_split(dataset) : \n    train = dataset[0: len(dataset) - DAYS]\n    val = dataset[len(dataset) - DAYS - TIME_STEP : len(dataset)]\n    return train, val","2227e55a":"dataset.drop([\"Date\",\"High\", \"Low\", \"Close\", \"Volume\", \"Adj Close\"], axis = 1, inplace = True)\ndataset = dataset.values","756fccdd":"import sklearn\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range = (0,1))\ndataset_scaled = scaler.fit_transform(dataset)","f3a605c8":"train, val = dataset_split(dataset_scaled)","117dc737":"train.shape, val.shape","16be9e8a":"train_x, train_y = [], []\nfor i in range(TIME_STEP, train.shape[0]) : \n    train_x.append(train[i - TIME_STEP : i, 0])\n    train_y.append(train[i, 0])\ntrain_x, train_y = np.array(train_x), np.array(train_y)","0856cc83":"val_x, val_y = [], []\nfor i in range(TIME_STEP, val.shape[0]) : \n    val_x.append(val[i - TIME_STEP : i, 0])\n    val_y.append(val[i, 0])\nval_x, val_y = np.array(val_x), np.array(val_y)","63e5ce6f":"train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], 1))\nval_x = np.reshape(val_x, (val_x.shape[0], val_x.shape[1], 1))\nprint(\"Reshaped train_x = \", train_x.shape)\nprint(\"Shape of train_y = \", train_y.shape)\n\nprint(\"Reshaped val_x = \", val_x.shape)\nprint(\"Shape of val_y = \", val_y.shape)","e0cb35f7":"import tensorflow as tf","d817422c":"os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\ngpus = tf.config.list_physical_devices(\"GPU\")\nprint(gpus)\nif len(gpus) == 1 : \n    strategy = tf.distribute.OneDeviceStrategy(device = \"\/gpu:0\")\nelse:\n    strategy = tf.distribute.MirroredStrategy()","c24ed3b9":"tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\" : True})\nprint(\"Mixed precision enabled\")","37b9c31d":"reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor= \"loss\", factor = 0.5, patience = 10,\n                                                 min_lr = 0.000001, verbose = 1)\nmonitor_es = tf.keras.callbacks.EarlyStopping(monitor= \"loss\", patience = 25, restore_best_weights= False, verbose = True)","0a54a5f1":"model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.LSTM(units = 128, return_sequences = True, input_shape = (train_x.shape[1], 1)))\nmodel.add(tf.keras.layers.Dropout(0.4))\nmodel.add(tf.keras.layers.LSTM(units = 128, return_sequences = True))\nmodel.add(tf.keras.layers.Dropout(0.4))\nmodel.add(tf.keras.layers.LSTM(units = 128, return_sequences = False))\nmodel.add(tf.keras.layers.Dropout(0.4))\n\nmodel.add(tf.keras.layers.Dense(units = 10, activation = \"relu\"))\nmodel.add(tf.keras.layers.Dense(units = 1, activation = \"relu\"))","ccc53e69":"model.compile(tf.keras.optimizers.Adam(lr = 0.001), loss = \"mean_squared_error\")","6f4369b3":"model.summary()","a17dbc3e":"with tf.device(\"\/device:GPU:0\"):\n    history = model.fit(train_x, train_y, epochs = 300, batch_size = 16, callbacks = [reduce_lr, monitor_es])","b601de36":"plt.figure(figsize = (12, 4))\nplt.plot(history.history[\"loss\"], label = \"Training loss\")\nplt.title(\"Loss analysis\", fontsize = 18)\nplt.xlabel(\"Epoch\", fontsize = 18)\nplt.ylabel(\"Loss\", fontsize = 18)\nplt.legend([\"Train\"])\nplt.grid(\"both\")","fe331b31":"model_json = model.to_json()\nwith open(\"AMD_open_1.json\", \"w\") as json_file:\n  json_file.write(model_json)\n\nmodel.save_weights(\"AMD_open_1.h5\")","b43431bd":"# get model\nfrom keras.models import model_from_json\njson_file = open('AMD_open_1.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\nloaded_model.load_weights(\"AMD_open_1.h5\")\nprint(\"Loaded model from disk\")\nloaded_model.compile(loss='mean_squared_error', optimizer='adam')","2f75cf93":"real_prices = val[TIME_STEP:]\nreal_prices = scaler.inverse_transform(real_prices)","2dc47f86":"predicted_prices = loaded_model.predict(val_x)\npredicted_prices = scaler.inverse_transform(predicted_prices)","815394a2":"plt.figure(figsize= (16, 5))\nplt.subplot(1,1,1)\n\nx = np.arange(0, DAYS, 1)\n\nplt.plot(x, real_prices, color = \"red\", label = \"Real AMD Prices\")\nplt.plot(x, predicted_prices, color = \"blue\", label = \"Predicted AMD Prices\")\nplt.title(\"AMD Open Stock Prices\", fontsize = 18)\nplt.xlabel(\"Time In Days\", fontsize = 18)\nplt.ylabel(\"Stock Prices in US Dollars\", fontsize = 18)\nplt.legend()\nplt.grid(\"both\")","e39eddae":"original_training_prices = scaler.inverse_transform(train)\noriginal_training_prices","93604fbe":"x1 = np.arange(0,len(original_training_prices),1)\nx2 = np.arange(len(original_training_prices), len(dataset), 1)\nprint(len(x1), len(x2))","fbbd4df6":"plt.figure(figsize= (16,8))\nplt.subplot(1,1,1)\n\nX = len(dataset)\nx1 = np.arange(0,len(original_training_prices),1)\nx2 = np.arange(len(original_training_prices), len(dataset), 1)\n\nplt.plot(x1, original_training_prices, color = \"green\")\nplt.plot(x2, real_prices, color = \"red\", label = \"Real AMD Prices\")\nplt.plot(x2, predicted_prices, color = \"blue\", label = \"Predicted AMD Prices\")\nplt.title(\"AMD Open Stock Prices\", fontsize = 18)\nplt.xlabel(\"Time In Days\", fontsize = 18)\nplt.ylabel(\"Stock Prices in US Dollars\", fontsize = 18)\nplt.legend()\nplt.grid(\"both\")","afdd8389":"Save the model","83cafdb8":"We successfully trained a deep learning architecture based on state of the art LSTM networks in order to predict the prices based on historical understanding of our data.\n\nThank you!!\nUpvote if you find it insightful :-)","d239fc78":"# Scaling :\n\nIt refers to putting the values in the same range or same scale so that no variable is dominated by the other.\n\nMost of the times, our dataset contains features highly varying in magnitudes, units and range. But since, most of the machine learning algorithms use Euclidean distance between two data points in their computations, this poses to be a problem. If left alone, these algorithms only take in the magnitude of features neglecting the units. The results would vary greatly between different units, 5kg and 5000gms *for illustration. The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes. To suppress this effect, we need to bring all features to the same level of magnitudes. This can be achieved by scaling.*\n\n![image.png](attachment:image.png)\n\nIn a nutshell, scaling helps our optimization algorithm converge faster on our data. **In the figure we can see the skeweness of the data distribution decreases a lot after scaling, as a result of which gradient descent(optimization algorithm) converges faster.**\n\nFor scaling we will import the scikit-learn Python3 machine learning library where **we use MinMaxScaler to scale all the price values beteen 0 and 1, that is the feature range we provided in the code.**","99286430":"# Exploratory Data Analysis(EDA)\n\nWhen we\u2019re getting started with a machine learning (ML) project, one critical principle to keep in mind is that data is everything. It is often said that if ML is the rocket engine, then the fuel is the (high-quality) data fed to ML algorithms. However, deriving truth and insight from a pile of data can be a complicated and error-prone job. To have a solid start for our ML project, it always helps to analyze the data up front.\n\nDuring EDA, it\u2019s important that we get a deep understanding of:\n\n* The **properties of the data**, such as schema and statistical properties;\n* The **quality of the data**, like missing values and inconsistent data types;\n* The **predictive power of the data**, such as correlation of features against target.\n\nThis project didn't require profound EDA as the data was time-series. Only thing to enusure in the dataset of AMD were the case missing values. Fortunately, that didn't turn out to be true.\n\nUsing pandas info() function of the dataframe structure we found that all rows of the Open prices were filled.","59db3a01":"This heat map could be used in order to understand the available stock volume's correlation with other prices(open, close, max, min) for future applications. However, in this project, we keep ourselves to open stock prices prediction based on historical data.","375fb89c":"There is no missing value. We have full entry.","c0cc71b2":"![image.png](attachment:image.png)","01e09369":"# Long Short Term Memory - LSTM :\n\nHumans don\u2019t start their thinking from scratch every second. As we read this paragraph, we understand each word based on our understanding of previous words. We don\u2019t throw everything away and start thinking from scratch again. Our thoughts have persistence. Traditional neural networks can\u2019t do this, and it seems like a major shortcoming.\n\nRecurrent Neural Networks(RNNs) address this issue. They are networks with loops in them, allowing information to persist.\n\nLong Short Term Memory networks \u2013 usually just called \u201cLSTMs\u201d \u2013 are a special kind of RNN, capable of learning long-term dependencies.\n\n![image.png](attachment:image.png) \n\nThe entire process of the working behind a RNN is beautifully illustrated at : https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/\n\nUsing Keras API of Tensorflow a model was prepared having layers of LSTM cells stacked onto each other followed by a general Artificial Neural Network(ANN).\n\n* ReLU activation function was used in all the layers with dropout ranging from 0.2-0.4.\n* Adam Optimizer and MSE(Mean Squared Error) loss function was used.","ecf97dec":"# Hyperparameters :\n\nOur machine learning model was based on two hyperparameters which were :\n\n* `Time Step` : Number of days in the past our model looked at in order to predict the price on the asked day. For illustration, if we set time_step = 7 then for predicting the price on **n th day**, our model analyzed all the prices from **n-1** to **n-7 days**. This approach is relatively more accurate than using a traditional machine learning algorithm - such as polynomial linear regression - as we had considered only the recently reported prices rather than the whole dataset at once.\n\n* `Days` : Number of days in the end for which we have to predict the prices for. These were placed in our validation\/test set.","8b47c920":"![image.png](attachment:image.png) \n\nStock market prediction is the act of trying to determine the future value of a company stock or other financial instrument traded on an exchange. The successful prediction of a stock's future price could yield significant profit. The efficient market hypothesis posits that stock prices are a function of information and rational expectations, and that newly revealed information about a company's prospects is almost immediately reflected in the current stock price. Predicting how the stock market will perform is one of the most difficult things to do. There are so many factors involved in the prediction \u2013 physical factors vs. physhological, rational and irrational behaviour, etc. All these aspects combine to make share prices volatile and very difficult to predict with a high degree of accuracy.\n\nIn this endeavor we worked with historical data of the stock prices of few publicly listed companies and implemented a machine learning model based on Long Short Term Memory(LSTM) in order to predict the future prices.","459fb285":"# Configuring The Dataset For Deep Learning :\n\nSince we had planned to use an LSTM model for time-series prediction, the conversion of dataset's shape from 1-D to 3-D tensor became mandatory. For this we grouped the values from the past **time_step** days into one and stacked such units one behind the other.\n\n![image.png](attachment:image.png)","3bec0df5":"Load the model"}}