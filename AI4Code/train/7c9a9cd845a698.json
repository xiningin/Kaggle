{"cell_type":{"d0b983a7":"code","9a3a25a2":"code","437b0318":"code","30ddf54a":"code","5db8b1fc":"code","7385f585":"code","61a9901d":"code","0a3b02aa":"code","9da2567b":"code","37a3d36c":"code","360529e0":"code","8a685a02":"code","9484328d":"code","e2fb68e6":"code","49234fd1":"code","1f59ecea":"code","b13860ce":"code","865d8fd6":"code","ea21ad37":"code","20becf4c":"code","9bda4ff4":"code","1548aec1":"code","84d1011b":"markdown","ecef85c6":"markdown","9fd827b9":"markdown","00d30adb":"markdown","75c1cfa0":"markdown","58aa9373":"markdown","09a8f061":"markdown","704e3247":"markdown","320e5cf7":"markdown","c9f87fb5":"markdown","ade2a64b":"markdown","82faf8ee":"markdown","12683401":"markdown","721bb1a1":"markdown","0c805509":"markdown","f364a1eb":"markdown","98894f34":"markdown","82c46d62":"markdown","c00376c8":"markdown","ce3829ea":"markdown","9930a21f":"markdown"},"source":{"d0b983a7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n#For ignoring warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nprint(os.listdir(\"..\/input\"))","9a3a25a2":"data = pd.read_csv(\"..\/input\/Dataset_spine.csv\")","437b0318":"data.sample(5)","30ddf54a":"data[\"Class_att\"].unique()","5db8b1fc":"data.dropna(axis = 1, inplace = True)\ndata.sample()","7385f585":"#correlation map\nimport seaborn as sns\nf, ax = plt.subplots(figsize = (18,18))\nsns.heatmap(data.corr(), annot = True, fmt = '.2f', ax = ax)\nplt.show()","61a9901d":"data.Col2.plot(label = \"Col2\")\ndata.Col1.plot(label = \"Col1\")\nplt.xlabel(\"index\", color = \"red\")\nplt.ylabel(\"values\", color = \"red\")\nplt.legend()\nplt.title(\"Col1 and Col2\")\nplt.show()","0a3b02aa":"color_list = ['red' if i=='Abnormal' else 'green' for i in data.loc[:,'Class_att']]\npd.plotting.scatter_matrix(data.loc[:, data.columns != 'Class_att'],\n                                       c=color_list,\n                                       figsize= [15,15],\n                                       diagonal='hist',\n                                       alpha=0.5,\n                                       s = 200,\n                                       marker = '.',\n                                       edgecolor= \"black\")\nplt.show()","9da2567b":"sns.countplot(data = data, x = \"Class_att\")\nplt.show()\ndata.loc[:,\"Class_att\"].value_counts()","37a3d36c":"#x, y Split and Normalization\nx_data = data.iloc[:, 0:12].values\nx = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data))#Normalization\ny = data.iloc[:, 12]\n\n#Train, Test Split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 1)\n\n#Grid Search\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.grid_search import GridSearchCV\ngrid = {\"n_neighbors\":np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, grid, cv = 10)#cv = How many data split do we want\nknn_cv.fit(x_train, y_train)\nprint(\"Best number of neighbors is {}\".format(knn_cv.best_params_[\"n_neighbors\"]))\nprint(\"Best score is {}\".format(round(knn_cv.best_score_,2)))\n\n#Grid Search Visualization\nscore = []\nfor i in range(1,50):\n    knn2 = KNeighborsClassifier(n_neighbors = i)\n    knn2.fit(x_train, y_train)\n    score.append(knn2.score(x_test, y_test))\nplt.plot(np.arange(1,50), score)\nplt.xlabel(\"Number of neighbors\", color = \"red\", fontsize = 14)\nplt.ylabel(\"Score\", color = \"red\", fontsize = 14)\nplt.show()","360529e0":"#Clone our data\ndata_pca = data.copy()\ndata_pca[\"Class_att\"] = [1 if i == \"Abnormal\" else 0 for i in data_pca[\"Class_att\"]]\n#Then put it in PCA\nfrom sklearn.decomposition import PCA\npca_model = PCA(n_components = 6)\npca_model.fit(data_pca)\ndata_pca = pca_model.transform(data_pca)","8a685a02":"# PCA Variance\nplt.bar(range(pca_model.n_components_), pca_model.explained_variance_ratio_*100 )\nplt.xlabel('PCA n_components',size=12,color='red')\nplt.ylabel('Variance Ratio(%)',size=12,color='red')\nplt.show()","9484328d":"#In this scenario, I want my data in 2 dimension.\npca_model = PCA(n_components = 2)\npca_model.fit(data_pca)\ndata_pca = pca_model.transform(data_pca)","e2fb68e6":"print(\"My old shape:\", data.shape)\nprint(\"My new shape:\", data_pca.shape)","49234fd1":"x_pca = data_pca[:,0]\ny_pca = data_pca[:,1]\nplt.scatter(x_pca, y_pca, c = [\"red\",\"green\"])\nplt.show()\n","1f59ecea":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_normal = KNeighborsClassifier()\nknn_normal.fit(x_train, y_train)\nprint(\"KNN without PCA score :\", knn_normal.score(x_test, y_test))\n\n\n## KNN With PCA\n#Train, Test Split\nx_pca = x_pca.reshape(-1,1)\ny_pca = y_pca.reshape(-1,1)\ny_pca_edit = [round(float(i),0) for i in y_pca]\ny_pca_edit = [\"Abnormal\" if i>0 else \"Normal\" for i in y_pca_edit]\ny_pca_edit = np.array(y_pca_edit)\nfrom sklearn.model_selection import train_test_split\nx_pca_train, x_pca_test, y_pca_train, y_pca_test = train_test_split(x_pca, y_pca_edit, test_size = 0.1, random_state = 1)\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_pca = KNeighborsClassifier()\nknn_pca.fit(x_pca_train, y_pca_train)\nprint(\"KNN with PCA score    :\", knn_pca.score(x_pca_test, y_pca_test))","b13860ce":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda = LinearDiscriminantAnalysis(n_components = 2)# 13 ----> 2\nx_train_lda = lda.fit_transform(x_train, y_train)\nx_test_lda = lda.transform(x_test)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_lda = KNeighborsClassifier()\nknn_lda.fit(x_train_lda, y_train)\n\nprint(\"KNN score :\", knn_normal.score(x_test, y_test))\nprint(\"KNN with PCA score    :\", knn_pca.score(x_pca_test, y_pca_test))\nprint(\"KNN with LDA score    :\", knn_lda.score(x_test_lda, y_test))","865d8fd6":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(max_iter = 100)#max_iter is for forward and backward propogation\nlr.fit(x_train, y_train)\nlr.score(x_test, y_test)","ea21ad37":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(random_state = 1)\ndtc.fit(x_train, y_train)\n\n#K-fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\ncvs_scores = cross_val_score(knn_normal, x_pca_test, y_pca_test, cv=5) #cv=5 means, we will split our data into 5 pieces\nprint(\"Cross Validation score is\", cvs_scores.mean())\n\n#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, dtc.predict(x_test))\nprint(\"Confusion Matrix \\n\",cm)","20becf4c":"#x, y Split and Normalization\nx_data = data.iloc[:, 0:12].values\nx = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data))#Normalization\ny = data.iloc[:, 12]\n\n#Train, Test Split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 1)\n\ny_train=y_train.values.reshape(-1,1)\ny_test=y_test.values.reshape(-1,1)\n\n#Naive bayes\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ny_pred = gnb.fit(x_train, y_train)\n\nprint(\"Navie Bayes score is\", gnb.score(x_test, y_test))","9bda4ff4":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(max_depth=2, random_state=0, n_estimators=100)\n#max_depth = The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure\n#n_estimators = The number of trees in the forest.\nrfc.fit(x_train, y_train)\n\nprint(\"Random Forest score is\", rfc.score(x_test, y_test))","1548aec1":"from sklearn import svm\nmodel = svm.SVC() \nmodel.fit(x_train, y_train)\nprint(\"Support Vector Machine score is\", model.score(x_test, y_test))","84d1011b":"<h1>Conclusion<\/h1>\nThere are a few of supervised machine learning methods in this paper and wrote this to remember all of these methods.\n\n<h1>References<\/h1>\n* scikit-learn.org\n* machinelearningmastery.com\/k-fold-cross-validation\/\n* dataschool.io\/simple-guide-to-confusion-matrix-terminology\/\n* analyticsvidhya.com\/blog\/2017\/09\/understaing-support-vector-machine-example-code\/\n* towardsdatascience.com\/the-random-forest-algorithm-d457d499ffcd\n* www.techopedia.com\/definition\/32335\/naive-bayes\nanalyticsvidhya.com\/blog\/2017\/09\/naive-bayes-explained\/\n\n<br>See you later in Deep Learning!","ecef85c6":"There is a difference between \"*with and without KNN with PCA*\" score, **so PCA is really helpful when we need less data shape. Also if you have a less varience then you should use PCA.**","9fd827b9":"As you can see, **we have converted (310, 13) to (310, 2) and vizualized**\n* But wait a minute, what about scores?\n<br>Let's find out them!","00d30adb":"**Support Vector Machine Classifier**\n<br> In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiate the two classes very well.\n![svm.PNG](attachment:svm.PNG)","75c1cfa0":"It is time to find relevances between our data values.\n* To do this, I will use Seaborn library.","58aa9373":"**Random Forest Classifier**\n<br>Random forest builds multiple decision trees and merges them together to get a more accurate and stable prediction.\n![random%20forest.PNG](attachment:random%20forest.PNG)","09a8f061":"**KNN Classifier with Linear Discriminant Analysis (LDA)**\n1. LDA is similar to PCA but **in LDA, we try to find best dimension that separates columns perfectly.**\n1. Vice versa in PCA, we try to **separate all values from each other.**\n\nSo let's apply LDA to our data.\n\n","704e3247":"**Naive Bayes Classifier**\nNaive Bayes classifier uses probability theory to classify data. Naive Bayes classifier algorithms make use of Bayes' theorem. The key insight of Bayes' theorem is that the probability of an event can be adjusted as new data is introduced.","320e5cf7":"**Logistic Regression Classifier**\n<br>Logistic regression is named for the function used at the core of the method, the logistic function. Logistic regression is also a simple neural network because it has weight, bias and learning rate. In short, it has an iteration that tries to find bias and weights.\n<br>Input values (x) are combined linearly using weights or coefficient values to predict an output value (y). If the output value is higher than 0.5 then it predicts 1, else it predicts 0. \n![Logistic-Function.png](attachment:Logistic-Function.png)\n\n<br>Each column in your input data has an associated w and b coefficients that must be **learned** from your training data\n<br>\n<br>Let's implement Logistic Regression to our data.","c9f87fb5":"As you can see below, if we increase the number of components, our data variance is **sharply decreasing,**\n<br>which is good because this feature provides us **less complex data shape and less calculating time.**\n* Now let's  learn **KNN with PCA.**","ade2a64b":"Hi everyone,\n1. Today I will analyze \"Lower Back Pain Symptoms\" dataset.\n1. Then I will try some supervised machine learning algorithms.\n\nLet's start with including some necessary libraries.","82faf8ee":"Our data looks separable.\n* Let's look some relevant values graph.","12683401":"**KNN Classifier with Principal Component Analysis (PCA)**\n1. PCA is basically lowers dimension of the data.\n1. PCA helps us when we try to vizualize higher dimensioned data.\n\nSo let's apply PCA to our data.\n\n","721bb1a1":"As you can see above, we have 2 types of data whose are Abnormal and normal.\n\n<br>***We have learned enough about our data so now we can apply some supervised learning algorithms.***\n<h2> Supervised Machine Learning Topics:<\/h2>\n\n1. KNN Classifier (with Grid Search)\n1. KNN Classifier with Principal Component Analysis (PCA)\n1. KNN Classifier with Linear Discriminant Analysis (LDA)\n1. Logistic Regression Classifier\n1. Decision Tree Classifier With K-fold Cross Validation And Confusion Matrix\n1. Naive Bayes Classifier\n1. Random Forest Classifier\n1. Support Vector Machine Classifier\n\n<br>Let's get started!","0c805509":"Let's find out how many class types we have.","f364a1eb":"**KNN Classifier**\n<br>The idea is to find a predefined number of training samples closest in distance to the new point, and predict the label from these.\n![0_Sk18h9op6uK9EpT8_.png](attachment:0_Sk18h9op6uK9EpT8_.png)\n1. First thing we need to do is seperating our data to x and y.\n1. Then split data to train and test\n1. And apply KNN with Grid Search","98894f34":"So we have 23 true predictions and 8 false predictions","82c46d62":"I think it is time to look our dataset.","c00376c8":"As you can see above, we have columns named like  \"Col1, Col2, ..\". \n1. We have a column named\" Class_att\". We need to learn its unique values.\n1. And then we need to get rid of the \"Unnamed : 13\" column, also we need to delete rows which has NaN value.\n\nLet's learn our unique classes.","ce3829ea":"As you can see, there is a significant discrimination.\n* If you want more detailed correlation graphic for each value:","9930a21f":"<h1>**Decision Tree Classifier With K-fold Cross Validation And Confusion Matrix**<\/h1>\n\n<br>**Confusion Matrix** is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. \n![confusion_matrix_simple2.png](attachment:confusion_matrix_simple2.png)\n<br>**K-fold Cross Validation** is a *resampling procedure used to evaluate machine learning models on a limited data sample*. It has a single parameter called **k that refers to the number of groups that a given data sample is to be split into.** If k is given k=10 then this means our given data will be split into 10 pieces and also this means we have 10-fold cross-validation.\n![K-fold_cross_validation_EN.jpg](attachment:K-fold_cross_validation_EN.jpg)\n<br>**Decision Tree Classifier,** repetitively divides the working area(plot) into sub part by identifying lines.\n![1_1CchuZc1nLM3B60zS7A1yw.png](attachment:1_1CchuZc1nLM3B60zS7A1yw.png)\n<br>Let's implement Decision Tree Classifier to our data."}}