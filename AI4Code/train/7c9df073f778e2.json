{"cell_type":{"bebfd145":"code","34f1f61f":"code","903b8d0a":"code","7edf8713":"code","f545a77a":"code","4ff825d7":"code","ea4597e0":"code","8323dbe9":"code","e91e97ab":"code","7f14a9d8":"code","e93ae05b":"code","0036533f":"code","179c5cc5":"code","6b0d5e39":"code","4359875d":"code","7fae4971":"code","f4b4c70d":"code","dba1a55b":"code","e0046254":"code","821d0785":"code","e0508b46":"code","b2125688":"code","4def4f8f":"code","93a398d0":"code","33b38d57":"code","4248fe57":"code","e4623073":"code","0b47cdbe":"code","a2660302":"code","d219924b":"code","c17d5930":"code","52687da6":"code","9083d32c":"code","7c7a7662":"code","dcdeae22":"code","4a074bd7":"code","7210cac0":"code","2dc43047":"code","3ee96465":"code","27737d21":"code","a4454eb7":"code","0ed3cdaf":"code","3712d003":"code","cf0c51fe":"code","849fe47d":"code","4d96cf41":"code","57a66f9a":"markdown","5951b2b8":"markdown","01025e83":"markdown","2b0b03a9":"markdown","42cb56d0":"markdown","c6902b61":"markdown","e80a57e5":"markdown","d9e8c065":"markdown","ab8182ce":"markdown","55f3c500":"markdown","e0be833b":"markdown"},"source":{"bebfd145":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","34f1f61f":"df= pd.read_csv('..\/input\/titanic\/train.csv')","903b8d0a":"df.shape","7edf8713":"df.head()","f545a77a":"df.info()","4ff825d7":"df['Survived'].value_counts()","ea4597e0":"y1 = [df['Sex'].value_counts()['male'], df['Sex'].value_counts()['female']]\nx1 = ['male', 'female']\n\nmale_survived = 100*df[['Sex', 'Survived']].value_counts()['male'][1]\/(df[['Sex', 'Survived']].value_counts()['male'][0]+df[['Sex', 'Survived']].value_counts()['male'][1])\ny2 = [male_survived, 100-male_survived]\nx2 = ['male_survived', 'male_drowned']\n\nfemale_survived = 100*df[['Sex', 'Survived']].value_counts()['female'][1]\/(df[['Sex', 'Survived']].value_counts()['female'][0]+df[['Sex', 'Survived']].value_counts()['female'][1])\ny3 = [female_survived, 100-female_survived]\nx3 = ['female_survived', 'female_drowned']\n\nfig1 = plt.figure()\nax1 = fig1.add_axes([1, 1, 1, 1])\nax2 = fig1.add_axes([1, -0.15, 1, 1])\nax3 = fig1.add_axes([0, -0.15, 1, 1])\nax4 = fig1.add_axes([0, 1, 1, 1])\n\nax4.set_title('chart-1').set_size(20)\nax1.set_title('chart-2').set_size(20)\nax2.set_title('chart-3').set_size(20)\nax3.set_title('chart-4').set_size(20)\n\nsns.barplot(data=df, x=x1, y=y1, ax=ax4)\nax1.pie(y1, labels = x1, autopct='%1.2f', shadow=True)\nax2.pie(y2, labels = x2, autopct='%1.2f', shadow=True)\nax3.pie(y3, labels = x3, autopct='%1.2f', shadow=True)\n\nplt.show()","8323dbe9":"100*(df.isnull().sum()\/len(df))","e91e97ab":"#we dont need to cabin, PassengerId, Ticket and Name. because this data cannot help us in prediction.\n\ndf = df.drop(['Cabin', 'PassengerId', 'Ticket', 'Name'], axis = 1)","7f14a9d8":"df","e93ae05b":"df['Age'].fillna(df['Age'].mean(), inplace = True)","0036533f":"100*(df.isnull().sum()\/len(df))","179c5cc5":"df['Embarked'].fillna('None', inplace = True)","6b0d5e39":"100*(df.isnull().sum()\/len(df))","4359875d":"df.info()","7fae4971":"df['Pclass']= df['Pclass'].apply(str)","f4b4c70d":"#in this step we remove the data of columns Age that is far from another data.\n\nsns.scatterplot(data=df, y='Survived', x='Age')","dba1a55b":"index_drop=df[(df['Age'] > 70) & (df['Survived'] == 1)].index\ndf=df.drop(index_drop, axis=0)","e0046254":"sns.scatterplot(data=df, y='Survived', x='Fare')","821d0785":"index_drop=df[(df['Fare'] > 200)].index\ndf=df.drop(index_drop, axis=0)","e0508b46":"sns.scatterplot(data=df, y='Survived', x='Fare')","b2125688":"df.info()","4def4f8f":"df_num = df.select_dtypes(exclude='object')\ndf_obj = df.select_dtypes(include='object')","93a398d0":"df_obj = pd.get_dummies(df_obj, drop_first=True)","33b38d57":"df_obj","4248fe57":"final_df = pd.concat([df_num, df_obj], axis=1)","e4623073":"final_df.shape","0b47cdbe":"final_df.info()","a2660302":"sns.boxplot(data=df, x='Sex', y='Age', hue='Survived')","d219924b":"sns.boxplot(data=df, x='Survived', y='Age')","c17d5930":"sns.boxplot(data=df, x='Survived', y='Fare')","52687da6":"X = final_df.drop(['Survived'],axis=1)\ny = final_df['Survived']","9083d32c":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","7c7a7662":"from sklearn.preprocessing import StandardScaler","dcdeae22":"scaler = StandardScaler()\n\nscaler.fit(X_train)\n\nscaled_X_train = scaler.transform(X_train)\nscaled_X_test = scaler.transform(X_test)","4a074bd7":"from sklearn.linear_model import LogisticRegression","7210cac0":"log_model = LogisticRegression()\n\nlog_model.fit(scaled_X_train, y_train)","2dc43047":"y_pred = log_model.predict(scaled_X_test)","3ee96465":"df['Survived'].value_counts()","27737d21":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, plot_confusion_matrix","a4454eb7":"accuracy_score(y_test, y_pred)","0ed3cdaf":"confusion_matrix(y_test, y_pred)\nplot_confusion_matrix(log_model, scaled_X_test, y_test)","3712d003":"print(classification_report(y_test, y_pred))","cf0c51fe":"from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve, plot_roc_curve","849fe47d":"plot_roc_curve(log_model, scaled_X_test, y_test)","4d96cf41":"plot_precision_recall_curve(log_model, scaled_X_test, y_test)","57a66f9a":"#### Note: this data set needs to prepare and then we can use logistic regression. for more information about data preparation you can see [my previous notebook](https:\/\/www.kaggle.com\/rezanematollahi\/data-preparation).","5951b2b8":" # Scaling the Features:","01025e83":"based on above numbers, our data is almost balanced. so we can check the accuracy score of the model.","2b0b03a9":"# Train the Model:","42cb56d0":"#  Logistic Regression:\n#### What is logistic regression used for?\n>Logistic regression is used to obtain odds ratio in the presence of more than one explanatory variable. The procedure is quite similar to multiple linear regression, with the exception that the response variable is binomial. The result is the impact of each variable on the odds ratio of the observed event of interest.\n#### in this notebook i worked on titanic data set and i used logistic regression.","c6902b61":"#### What is a Confusion Matrix?\n>A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model. This gives us a holistic view of how well our classification model is performing and what kinds of errors it is making.\nFor a binary classification problem, we would have a 2 x 2 matrix.\n\n#### if you want to know more about confusion matrix, recall, precision or f1-score, read this [article](https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/confusion-matrix-machine-learning\/).","e80a57e5":"based on data set, __543__ passengers drowned and __327__ survived.\n\nfor better understand i displayed four charts. in chart-1 we can see the number of male and female passengers.\nin chart-2 we can see the percentage of male and female. in chart-3 and chart-4 we can see the percentage of survived and drowned passengers.","d9e8c065":"# Evaluating the Model:\n","ab8182ce":"# Data Overview:","55f3c500":"#### How do you interpret a ROC curve in logistic regression?\n>The Area Under the ROC curve (AUC) is an aggregated metric that evaluates how well a logistic regression model classifies positive and negative outcomes at all possible cutoffs. It can range from 0.5 to 1, and the larger it is the better.","e0be833b":"# Split the Dataset to Tain and Test set:"}}