{"cell_type":{"b22bf542":"code","310241bd":"code","cdea0986":"code","dbe00172":"code","93486046":"code","c3b637e7":"code","b8fc9cdf":"code","b73a3558":"code","7c3ad828":"code","a07164b3":"code","3008efc1":"code","13210fa0":"code","9d174a51":"code","b25ef8c2":"markdown","1add3a5b":"markdown","1078756f":"markdown","bbbc137b":"markdown","47832e0c":"markdown","b979e664":"markdown","71c8c702":"markdown","1770ba86":"markdown","4939737e":"markdown","bdecdf2d":"markdown","19ceb21f":"markdown","45bc1c73":"markdown"},"source":{"b22bf542":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom keras.layers import Dense, Dropout\nfrom keras.preprocessing.image import load_img, img_to_array\nimport matplotlib.pyplot as plt\n\nimport os\nlist_img = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/stanford-dogs-dataset\/images\/Images'):\n    \n    for filename in filenames:\n        list_img.append(os.path.join(dirname, filename))\n        \ntrue_dog=pd.Series(list_img)\nprint(true_dog.shape)\n# Any results you write to the current directory are saved as output.","310241bd":"d=27#image size\ndef load_preprocess(path, dim=(d, d)):\n    \"\"\"load and preprocess an image\"\"\"\n    img = load_img(path, target_size=dim)  # Charger l'image \n    img = img_to_array(img)  # Convertir en tableau numpy\n    #print(img.shape)\n    #img = img.reshape((img.shape[0], img.shape[1], img.shape[2]))  # Cr\u00e9er la collection d'images (un seul \u00e9chantillon)\n    #img = preprocess_input(img)\n    return img*1.\/255\n    #return minmax_scale(img)","cdea0986":"test_img = true_dog.sample(3).apply(load_preprocess)\nimtest = test_img.values\nplt.figure(figsize=(17,4))\nfor i in range(3):\n    plt.subplot(1,3,i+1)\n    plt.imshow(imtest[i])\nplt.show()\n","dbe00172":"\"\"\"True image for discriminator\"\"\"\nimg_true = tf.placeholder(tf.float32, shape=(None, d, d, 3))\n\n\"\"\"Generator input\"\"\"\ndr=9\nx_rand =tf.placeholder(tf.float32, shape=(None, dr))\n\n\"\"\"other parameter\"\"\"\nis_test = tf.placeholder(tf.bool)\nrate = tf.cond(is_test, true_fn=lambda :0.0, false_fn=lambda:0.30)","93486046":"def batchnorm(Ylogits, Offset, Scale,conv=False):\n    \n    if conv:\n        mean, variance = tf.nn.moments(Ylogits, [0,1,2])\n    else:\n        mean, variance = tf.nn.moments(Ylogits, [0])\n    \n    Ybn = tf.nn.batch_normalization(Ylogits,mean,variance, Offset, Scale, variance_epsilon=1e-5)\n    return Ybn","c3b637e7":"def cross_entropy(y_true, y_pred):\n    return -tf.reduce_sum((y_true*tf.log(y_pred)+(1-y_true)*tf.log(1-y_pred)))","b8fc9cdf":"#Discriminant\n#K=22*22*64\nK=5*5*124\nM=1000\nn=1\n\n\n\n\nwith tf.name_scope('discrim'):\n    \"\"\"Varible_D\"\"\"\n    Wconv = tf.Variable(tf.truncated_normal([3,3,3,32],mean=0, stddev=0.1))\n    bconv=tf.Variable(tf.zeros([32]))\n    Wconv2 = tf.Variable(tf.truncated_normal([3,3,32,64],mean=0, stddev=0.1))\n    bconv2 = tf.Variable(tf.zeros([64]))\n    Wconv3 = tf.Variable(tf.truncated_normal([3,3,64,64],mean=0, stddev=0.1))\n    bconv3 = tf.Variable(tf.zeros([64]))\n    Wconv4 = tf.Variable(tf.truncated_normal([3,3,64,124],mean=0, stddev=0.1))\n    bconv4 = tf.Variable(tf.zeros([124]))#124\n\n    \"\"\"Variables FullyConnected\"\"\"\n    W1=tf.Variable(tf.truncated_normal([K,M], mean=0, stddev=0.1))\n    b1=tf.Variable(tf.zeros([M]))\n\n    W2=tf.Variable(tf.truncated_normal([M,M], mean=0, stddev=0.1))\n    b2=tf.Variable(tf.zeros([M]))\n\n    W3=tf.Variable(tf.truncated_normal([M,M], mean=0, stddev=0.1))\n    b3=tf.Variable(tf.zeros([M]))\n\n    W4=tf.Variable(tf.truncated_normal([M,n],mean=0, stddev=0.1))\n    b4=tf.Variable(tf.truncated_normal([n],mean=0, stddev=0.1), name=\"b4\")\n    #print(X.shape[0], b2, Wconv, Wconv3)","b73a3558":"def discriminant(Img):\n    \"\"\"convolutions :\"\"\"\n    Xconv_lin=tf.nn.conv2d(Img, filter=Wconv, strides = [1,1],padding='VALID')\n    Xconv_lin = batchnorm(Xconv_lin, bconv, 1, conv=True)\n    Xconv = tf.nn.relu(Xconv_lin)\n    #Xconv = tf.nn.dropout(Xconv, rate=rate)\n\n    Xconv_lin2=tf.nn.conv2d(Xconv, filter=Wconv2, strides = [1,1],padding='SAME')\n    Xconv_lin2 = batchnorm(Xconv_lin2, bconv2, 1, conv=True)\n    Xconv2 = tf.nn.relu(Xconv_lin2)\n    #Xconv2 = tf.nn.dropout(Xconv2, rate=rate)\n\n    Xpool=tf.nn.pool(Xconv2, [5,5], 'MAX', strides=[5,5], padding='SAME')\n\n    Xconv_lin3=tf.nn.conv2d(Xconv2, filter=Wconv3, strides = [1,1],padding='SAME')\n    Xconv_lin3 = batchnorm(Xconv_lin3, bconv3, 1, conv=True)\n    Xconv3 = tf.nn.relu(Xconv_lin3)\n    #Xconv3 = tf.nn.dropout(Xconv3, rate=rate)\n\n    Xconv_lin4=tf.nn.conv2d(Xconv3, filter=Wconv4, strides =1 ,padding='SAME')\n    Xconv_lin4 = batchnorm(Xconv_lin4, bconv4, 1, conv=True)\n    Xconv4 = tf.nn.relu(Xconv_lin4)\n    #Xconv4 = tf.nn.dropout(Xconv4, rate=rate)\n\n    Xconv=tf.nn.pool(Xconv4, [5,5], 'MAX', strides=[5,5], padding='VALID')\n    #Xconv=tf.nn.dropout(Xconv, rate=rate)\n    print(Xpool, Xconv)\n    \n\n    \"\"\"Fully Connected\"\"\"\n\n    X1_lin=tf.matmul(tf.reshape(Xconv, (-1, Xconv.shape[1]*Xconv.shape[2]*Xconv.shape[3])), W1)\n    print(X1_lin)\n    X1_lin = batchnorm(X1_lin, Offset=b1, Scale=1, conv=False)\n\n    X1=tf.nn.relu(X1_lin)\n    X1 = tf.nn.dropout(X1, rate=rate)\n    #X1=tf.nn.elu(tf.matmul(tf.reshape(Xconv, (-1, Xconv.shape[1]*Xconv.shape[2]*Xconv.shape[3])), W1)+b1)\n    #Y = tf.nn.softmax(tf.matmul(X1, W2)+b2, axis=2)\n\n\n    X2_lin=tf.matmul(X1, W2)\n    X2_lin = batchnorm(X2_lin, Offset=b2, Scale=1, conv=False)\n    X2 = tf.nn.relu(X2_lin)\n    X2 = tf.nn.dropout(X2, rate=rate)\n\n    X3_lin=tf.matmul(X2, W3)\n    X3_lin= batchnorm(X3_lin, Offset=b2, Scale=1, conv=False)\n    X3=tf.nn.relu(X3_lin)\n    X3 = tf.nn.dropout(X3, rate=rate)\n\n\n    Y = tf.nn.sigmoid(tf.matmul(X1, W4)+b4)\n    return Y","7c3ad828":"\nwith tf.name_scope(\"gen\"): \n    gen1 = tf.Variable(tf.truncated_normal([dr,9*dr**2], mean=0, stddev=0.1))\n    genb1=tf.Variable(tf.zeros([9*dr**2]))\n    print(gen1)\n\n    g_conv1 = tf.Variable(tf.truncated_normal([3,3,3, 64], mean=0, stddev=0.1))\n    gc1_b = tf.Variable(tf.zeros([64]))\n    print(g_conv1)\n\n    g_conv2=tf.Variable(tf.truncated_normal([3,3,64,64], mean=0, stddev=0.1))\n    gc2_b = tf.Variable(tf.zeros([64]))\n    print(g_conv2)\n\n    g_conv3=tf.Variable(tf.truncated_normal([dr,dr,32,64], mean=0, stddev=0.1))\n    gc3_b = tf.Variable(tf.zeros([32]))\n\n    g_conv4=tf.Variable(tf.truncated_normal([3,3,3,32], mean=0, stddev=0.1))\n    gc4_b = tf.Variable(tf.zeros([3]))\n\n    g_conv5=tf.Variable(tf.truncated_normal([3,3,3,3], mean=0, stddev=0.1))\n    gc5_b = tf.Variable(tf.zeros([3]))\n    gc5_a = tf.Variable(tf.truncated_normal([3], mean=1, stddev=0.1))\n    ","a07164b3":"def generator(x_rand=x_rand):\n    lin1=tf.matmul(x_rand, gen1)+genb1\n    lin1=batchnorm(lin1, Offset=genb1, Scale=1)\n    dense1 = tf.nn.relu(lin1)\n    \n    gen_conv1=tf.nn.conv2d(tf.reshape(dense1, (-1,dr,3*dr,3)),g_conv1, strides=3, padding='SAME')\n    gen_conv1=batchnorm(gen_conv1, Offset=gc1_b, Scale=1, conv=True)\n    gen_conv1=tf.nn.leaky_relu(gen_conv1)\n    #print(gen_conv1)\n    bs=tf.shape(gen_conv1)[0]\n    #in_ = tf.constant(0.1, shape=[2,9,9,64])\n    #in_conv = tf.nn.conv2d(in_, g_conv2, strides=[3,1], padding='SAME')\n    #print(in_conv)\n    out2=tf.stack([bs, 9,9,64])\n    gen_conv2_t=tf.nn.conv2d_transpose(gen_conv1, g_conv2, output_shape=out2, strides=[3,1], padding='SAME')\n    gen_conv2=batchnorm(gen_conv2_t, Offset=gc2_b, Scale=1, conv=True)\n    gen_conv2=tf.nn.leaky_relu(gen_conv2)\n    #print(gen_conv2)\n\n    out3=tf.stack([bs, 27,27,32])\n    gen_conv3=tf.nn.conv2d_transpose(gen_conv2, g_conv3, output_shape=out3, strides=3, padding='SAME')#[1,3,3,2]\n    gen_conv3=batchnorm(gen_conv3, Offset=gc3_b, Scale=1, conv=True)\n    gen_conv3=tf.nn.leaky_relu(gen_conv3)\n    #print(gen_conv3)\n    out4=tf.stack([bs, 27,27,3])\n    gen_conv4=tf.nn.conv2d_transpose(gen_conv3, g_conv4, output_shape=out4, strides=1, padding='SAME')\n    gen_conv4=batchnorm(gen_conv4, Offset=gc4_b, Scale=1, conv=True)\n    gen_conv4=tf.nn.leaky_relu(gen_conv4)\n    \n    gen_conv5=tf.nn.conv2d_transpose(gen_conv4, g_conv5, output_shape=(bs,d,d,3), strides=1, padding='SAME')\n    gen_conv5=batchnorm(gen_conv5, Offset=gc5_b, Scale=gc5_a, conv=True)\n    gen_conv5=tf.nn.sigmoid(gen_conv5)\n    \n    return gen_conv5","3008efc1":"optimizer_gen = tf.train.AdamOptimizer(learning_rate=0.003, beta1=0.9, beta2=0.999)\nimg_fake=generator(x_rand)\nY_fake_pred = discriminant(img_fake)\nloss_gen =cross_entropy(tf.ones_like(Y_fake_pred), Y_fake_pred)\ntrain_gen = optimizer_gen.minimize(loss_gen, var_list=tf.trainable_variables(scope='gen'))#\n","13210fa0":"\n\"\"\"Our discriminator target\"\"\"\nY_fake = tf.zeros((tf.shape(img_fake)[0], 1))\nY_true = tf.ones((tf.shape(img_true)[0], 1))\n\"\"\"steps\"\"\"\nY_real_pred = discriminant(img_true)\nloss_discr = cross_entropy(Y_true, Y_real_pred)+cross_entropy(Y_fake, Y_fake_pred)\noptimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999)\ntrain_step=optimizer.minimize(loss_discr, var_list=tf.trainable_variables(scope='discrim'))\n\n\ninit = tf.global_variables_initializer()","9d174a51":"nb_true = 30#per batch\nnb_fake = 30\n\nY_ref = np.vstack((np.ones((nb_true,1)), np.zeros((nb_fake,1))))                \nloop=60\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n    #saver.restore(sess, '.\/model_gan2.h5')\n    for e in range(loop):\n        loss_generator =[]\n        loss_discreminant=[]\n        for i in range(50):\n            true = true_dog.sample(nb_true).apply(load_preprocess)\n            X_rand = np.random.random([nb_fake,9])\n            #print(X_rand.shape)\n            #gen_feed = {x_rand:X_rand, is_test:False}\n            #X_fake = sess.run(img_fake, feed_dict=gen_feed)\n            #plt.figure()\n            #plt.imshow(X_fake[0])\n            #plt.show()\n            X_true = np.array(list(true))\n            feed_dict = {x_rand:X_rand, img_true:X_true, is_test:False}# Y_:Y_ref\n            sess.run(train_step, feed_dict=feed_dict)\n            loss= sess.run([loss_discr], feed_dict=feed_dict)#gen_conv2_tcross_entropy\n            sess.run(train_gen, feed_dict=feed_dict)\n            loss_g = sess.run([loss_gen], feed_dict=feed_dict)\n            loss_generator.append(loss_g)\n            loss_discreminant.append(loss)     \n        print('loss_G :',np.mean(loss_generator))\n        print('loss_D: ',np.mean(loss_discreminant))\n        gen_feed = {x_rand:X_rand, is_test:True}\n        X_fake = sess.run(img_fake, feed_dict=gen_feed)\n        plt.figure(figsize=(9,9))\n        plt.imshow(X_fake[0])\n        plt.axis('off')\n        plt.show()\n        if np.isnan(loss):\n            break","b25ef8c2":"# 1 Images from real world","1add3a5b":"### Other functions :","1078756f":"## Discriminator layers","bbbc137b":"# 2 the model\n## Input and parmeters","47832e0c":"## Visualisation of True Images:","b979e664":"for discriminant :","71c8c702":"This is my first **gan network**. It can be improved but I wanted to share these small paintings. It's interesting to see how the **number 3** is omnipresent in each composition...<br>I hope you'll enjoy it :)<br><br>\n# Finally, i did a **BiGAN from scratch 60 * 60** on **github** : https:\/\/github.com\/kossowski-pierre\/LittleBiGAN ","1770ba86":"## Generator Layers","4939737e":"# 3 His life : (Learning by painting)","bdecdf2d":"# 2.2 Generator\n## Variables for generator :","19ceb21f":"# 2.3 Training step\nfor generator :","45bc1c73":"# 2.1 Discriminator\n## Variables for the discriminator model"}}