{"cell_type":{"d9e227ed":"code","8aba1e24":"code","e2133ceb":"code","4a67cb15":"code","a0163930":"code","b727156b":"code","f56a6185":"code","46132f8d":"code","856a6bdb":"code","5dd6fc2b":"code","e3beed21":"code","c2a98b10":"code","3d2bed71":"code","4b1f5e19":"code","8aaf5a2f":"code","1ae764fd":"code","7adbe55d":"code","fbf57f3e":"code","9d194483":"code","489b2fb7":"code","b39ada6a":"code","94f9632b":"code","fe98c5b8":"code","1b4ac2d9":"code","2f9ac1a9":"code","a95277c2":"code","d8953d07":"code","65eca463":"code","b6f76cda":"code","ac4bfba5":"code","64c5fb46":"code","c6cf605d":"code","8d9137d2":"code","a5d5e2a3":"code","ad676aab":"code","3cecb0c1":"markdown","98b6d91c":"markdown","c23be450":"markdown","0f05a691":"markdown","37434c54":"markdown","6cd4f99a":"markdown","b64bc2b3":"markdown","224d2a9d":"markdown","6d1503ed":"markdown","d30b0b79":"markdown"},"source":{"d9e227ed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8aba1e24":"train = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")","e2133ceb":"train.info()","4a67cb15":"train.head()","a0163930":"train.describe().T","b727156b":"train.isnull().values.any()","f56a6185":"train[\"Class\"].value_counts()","46132f8d":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","856a6bdb":"labels = [\"Normal\", \"Fraud\"]\nsns.countplot(\"Class\",data=train)\nplt.title(\"Class Distribution\")\nplt.xticks(range(2), labels)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")","5dd6fc2b":"plt.figure()\nplt.scatter(train['Amount'], train['Class'])\nplt.show()","e3beed21":"plt.subplots(figsize =(14,14))\ncorr = train.corr()\nsns.heatmap(corr)","c2a98b10":"from sklearn.utils import shuffle\n#get the fraud data and concatenate with ewual size non-fraud data\nunder_fraud = train[train[\"Class\"] ==1]\nunder_non_fraud = shuffle(train[train[\"Class\"] == 0], n_samples =492,random_state =42)","3d2bed71":"under_non_fraud","4b1f5e19":"under_sample = shuffle(pd.concat([under_fraud,under_non_fraud]),random_state =42)","8aaf5a2f":"under_sample.shape","1ae764fd":"under_sample.head()","7adbe55d":"plt.subplots(figsize =(14,14))\ncorr = under_sample.corr()\nsns.heatmap(corr)","fbf57f3e":"from sklearn.preprocessing import StandardScaler, RobustScaler\n\nstd_scaler = StandardScaler()\nrob_scaler = RobustScaler()\nunder_sample[\"Amount\"] =  rob_scaler.fit_transform(under_sample['Amount'].values.reshape(-1,1))\nunder_sample['Time'] = std_scaler.fit_transform(under_sample['Time'].values.reshape(-1,1))","9d194483":"from sklearn.model_selection import train_test_split as tts\nunder_y = under_sample['Class']\nunder_X = under_sample.drop('Class', axis = 1)","489b2fb7":"X_train, X_test, y_train, y_test = tts(under_X, under_y, test_size=0.2, random_state=42)","b39ada6a":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"RandomForestClassifier\": RandomForestClassifier()\n}","94f9632b":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    y_pred = cross_val_predict(classifier, X_train, y_train, cv=5)\n    cf = confusion_matrix(y_train, y_pred)\n    precision, recall, threshold = precision_recall_curve(y_train, y_pred)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")\n    print(\"Confusion matrix:\\n\",cf)\n    print('Area under ROC curve score: ', roc_auc_score(y_train, y_pred))\n    print('Overfitting: \\n')\n    print('Recall Score: {:.2f}'.format(recall_score(y_train, y_pred)))\n    print('Precision Score: {:.2f}'.format(precision_score(y_train, y_pred)))\n    print('F1 Score: {:.2f}'.format(f1_score(y_train, y_pred)))\n    print('Accuracy Score: {:.2f}'.format(accuracy_score(y_train, y_pred)))\n    print('---' * 45)","fe98c5b8":"log_reg = LogisticRegression()\nknears_neighbors = KNeighborsClassifier()\nsvc = SVC()\ntree_clf = DecisionTreeClassifier()\nrf_clf = RandomForestClassifier()","1b4ac2d9":"#https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    # First Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    \n    # Second Estimator \n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax2.set_title(\"Knears Neighbors Learning Curve\", fontsize=14)\n    ax2.set_xlabel('Training size (m)')\n    ax2.set_ylabel('Score')\n    ax2.grid(True)\n    ax2.legend(loc=\"best\")\n    \n    # Third Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax3.set_title(\"Support Vector Classifier \\n Learning Curve\", fontsize=14)\n    ax3.set_xlabel('Training size (m)')\n    ax3.set_ylabel('Score')\n    ax3.grid(True)\n    ax3.legend(loc=\"best\")\n    \n    # Fourth Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax4.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax4.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax4.set_title(\"Decision Tree Classifier \\n Learning Curve\", fontsize=14)\n    ax4.set_xlabel('Training size (m)')\n    ax4.set_ylabel('Score')\n    ax4.grid(True)\n    ax4.legend(loc=\"best\")\n    return plt","2f9ac1a9":"cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(log_reg, knears_neighbors, svc, tree_clf, X_train, y_train, (0.87, 1.01), cv=cv, n_jobs=4)","a95277c2":"from imblearn.over_sampling import RandomOverSampler","d8953d07":"from collections import Counter\nover_y = train['Class']\nover_X = train.drop('Class', axis = 1)\nros = RandomOverSampler(random_state=42)\nX_over, y_over = ros.fit_resample(over_X,over_y)\n\n","65eca463":"from collections import Counter\nprint(sorted(Counter(y_over).items())) ","b6f76cda":"plt.subplots(figsize =(14,14))\ncorr = pd.DataFrame(X_over).corr()\nsns.heatmap(corr)","ac4bfba5":"X_train, X_test, y_train, y_test = tts(X_over, y_over, test_size=0.2, random_state=42)","64c5fb46":"classifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n}","c6cf605d":"for key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    y_pred = cross_val_predict(classifier, X_train, y_train, cv=5)\n    cf = confusion_matrix(y_train, y_pred)\n    precision, recall, threshold = precision_recall_curve(y_train, y_pred)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")\n    print(\"Confusion matrix:\\n\",cf)\n    print('Area under ROC curve score: ', roc_auc_score(y_train, y_pred))\n    print('Overfitting: \\n')\n    print('Recall Score: {:.2f}'.format(recall_score(y_train, y_pred)))\n    print('Precision Score: {:.2f}'.format(precision_score(y_train, y_pred)))\n    print('F1 Score: {:.2f}'.format(f1_score(y_train, y_pred)))\n    print('Accuracy Score: {:.2f}'.format(accuracy_score(y_train, y_pred)))\n    print('---' * 45)","8d9137d2":"from imblearn.over_sampling import SMOTE\n\nX_smote, y_smote = SMOTE().fit_resample(over_X,over_y)\nprint(sorted(Counter(y_smote).items()))","a5d5e2a3":"X_train, X_test, y_train, y_test = tts(X_smote, y_smote, test_size=0.2, random_state=42)","ad676aab":"for key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    y_pred = cross_val_predict(classifier, X_train, y_train, cv=5)\n    cf = confusion_matrix(y_train, y_pred)\n    precision, recall, threshold = precision_recall_curve(y_train, y_pred)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")\n    print(\"Confusion matrix:\\n\",cf)\n    print('Area under ROC curve score: ', roc_auc_score(y_train, y_pred))\n    print('Overfitting: \\n')\n    print('Recall Score: {:.2f}'.format(recall_score(y_train, y_pred)))\n    print('Precision Score: {:.2f}'.format(precision_score(y_train, y_pred)))\n    print('F1 Score: {:.2f}'.format(f1_score(y_train, y_pred)))\n    print('Accuracy Score: {:.2f}'.format(accuracy_score(y_train, y_pred)))\n    print('---' * 45)","3cecb0c1":"**Another Over- Sampling Method (SMOTE)**","98b6d91c":"With the same number of fraud and non-fraud data we can see the negative and positive correlation of data. but will this help us give the right prediction?","c23be450":"**Lets compare with Over-sampling method(RandomOverSampler)**","0f05a691":"At first glance, we can sea count are all the same means there may be no missing values. lets double check. ","37434c54":"We have 31 columns: Time, Amount, V1-V28 which are the anonymized data, and the predicting variable Class. ","6cd4f99a":"The anonymized data is already scaled and normalized, which means we may scale and normalize other variables that are not scaled","b64bc2b3":"**Fast check data.**","224d2a9d":"Because of extreme unbalanced data.. We cant get much from correlation matrix. So we will to do under-sampling method and later implement over-sampling method to see which method will give good results.","6d1503ed":"**Under-sampling Method**\n\nWe have 492 fraudulent cases, so to balance the dataset we need to get 492 non-fraudulent data. .\n\n\n","d30b0b79":"Credit Card Fraud Detection\n\nWe will make a ML model that will predict which credit card transactions are fraudulent."}}