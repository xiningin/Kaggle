{"cell_type":{"ba7ba476":"code","4fb05adb":"code","0e11d5af":"code","4edc13bb":"code","8489a573":"code","a06d4695":"code","cbd4eb71":"code","e328b74f":"markdown","fe5d5078":"markdown","3620b445":"markdown","e45aee70":"markdown","e8dd776f":"markdown"},"source":{"ba7ba476":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport optuna\nfrom sklearn.model_selection import cross_val_score","4fb05adb":"#study.best_params","0e11d5af":"#10 folds cross validation\ndf_train = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ndf_train[\"kfold\"] = -1\nkf = model_selection.KFold(n_splits=10, shuffle=True, random_state=5)\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(X=df_train)):\n    df_train.loc[valid_indicies, \"kfold\"] = fold\ndf_train.to_csv(\"train_folds.csv\", index=False)\ndf = pd.read_csv(\".\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")","4edc13bb":"useful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\ndf_test = df_test[useful_features]","8489a573":"xgb_params = {'n_estimators': 10000,\n              'learning_rate': 0.25,\n              'subsample': 0.926,\n              'colsample_bytree': 0.84,\n              'max_depth': 2,\n              'booster': 'gbtree', \n              'reg_lambda': 35.1,\n              'reg_alpha': 34.9,\n              'random_state': 5,\n              'tree_method' : 'gpu_hist'}","a06d4695":"final_predictions = []\ntotal_mean_rmse = 0\nfor fold in range(10):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    model = XGBRegressor(**xgb_params)\n    model.fit(xtrain, ytrain,verbose=False,\n              # These three parameters will stop training before a model starts overfitting \n              eval_set=[(xtrain, ytrain), (xvalid, yvalid)],\n              eval_metric=\"rmse\",\n              early_stopping_rounds=100,)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    fold_rmse=mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, mean_squared_error(yvalid, preds_valid, squared=False))\n    total_mean_rmse += fold_rmse\n","cbd4eb71":"print(f\"\\nOverall RMSE: {total_mean_rmse\/10}\")\npreds = np.mean(np.column_stack(final_predictions), axis=1)\nprint(preds)\nsample_submission.target = preds\nsample_submission.to_csv(\"submission.csv\", index=False)               ","e328b74f":"xgb_params = {'n_estimators': 969,\n 'max_depth': 4,\n 'learning_rate': 0.09962281481785025,\n 'gamma': 0.17,\n 'min_child_weight': 1,\n 'subsample': 0.8,\n 'colsample_bytree': 0.6,\n 'reg_alpha': 1.0,\n 'reg_lambda': 0.30000000000000004,\n    'random_state': 42,\n    'n_jobs': 5,\n    'tree_method' : 'gpu_hist'}","fe5d5078":"#without optuna\n#xgb_params = {'n_estimators': 10000,\n             # 'learning_rate': 0.35,\n            #  'subsample': 0.926,\n             # 'colsample_bytree': 0.84,\n            #  'max_depth': 2,\n            #  'booster': 'gbtree', \n            #  'reg_lambda': 35.1,\n            #  'reg_alpha': 34.9,\n            #  'random_state': 42,\n            #  'n_jobs': 10,\n            #  'tree_method' : 'gpu_hist' }","3620b445":"import optuna\nfrom sklearn.model_selection import cross_val_score\n\ndef objective(trial):\n    \n    n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000)\n    max_depth = trial.suggest_int(\"max_depth\", 1, 10)\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-1, log=True)\n    gamma = trial.suggest_float(\"gamma\", 0.1, 1.0, step=0.01)\n    min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 7, step=2)\n    subsample = trial.suggest_float(\"subsample\", 0.5, 1.0, step=0.1)\n    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0, step=0.1)\n    reg_alpha = trial.suggest_float(\"reg_alpha\", 0., 1.0, step=0.1)\n    reg_lambda = trial.suggest_float(\"reg_lambda\", 0., 1.0, step=0.1)\n    \n    \n    model = XGBRegressor(n_estimators=n_estimators,\n                         max_depth=max_depth,\n                         learning_rate=learning_rate,\n                         gamma=gamma,\n                         min_child_weight=min_child_weight,\n                         colsample_bytree=colsample_bytree,\n                         subsample=subsample,\n                         reg_alpha=reg_alpha,\n                         reg_lambda=reg_lambda,\n                         n_jobs=4, \n                         tree_method='gpu_hist', \n                         gpu_id=0)\n    \n    model.fit(X_train, y_train, verbose=False)\n    \n    y_hat = model.predict(X_valid)\n    \n    return mean_squared_error(y_valid, y_hat, squared=False)\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=900)","e45aee70":"#optuna best params\nxgb_params = {'n_estimators': 4429,\n                 'max_depth': 2,\n                 'learning_rate': 0.08633086082281931,\n                 'gamma': 0.7000000000000001,\n                 'min_child_weight': 5,\n                 'subsample': 0.8,\n                 'colsample_bytree': 0.6,\n                 'reg_alpha': 0.6000000000000001,\n                 'reg_lambda': 1.0,\n                  'random_state': 42,\n                  'n_jobs': 5,\n                  'tree_method' : 'gpu_hist' } \n\n","e8dd776f":"train = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\ntrain.drop(\"id\", axis=1, inplace=True)\ntest.drop(\"id\", axis=1, inplace=True)\nfeature_cols = [col for col in train.columns if col != \"target\"]\ncategory_cols = [col for col in train.columns if \"cat\" in col]\ncont_cols = [col for col in train.columns if \"cont\" in col]\nX = train[feature_cols]\ny = train[\"target\"]\n\nX_final = test[feature_cols]\n\n\ntransformers = ColumnTransformer(\n    [(\"ordinary_encoder\", OrdinalEncoder(), category_cols),\n     (\"standardize\", StandardScaler(), cont_cols)],\n    remainder=\"passthrough\"\n)\n\nX = transformers.fit_transform(X)\nX_final = transformers.transform(X_final)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=42, test_size=0.1)\n\n"}}