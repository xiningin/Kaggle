{"cell_type":{"f96a10d7":"code","1f9fd5de":"code","084a26d0":"code","5e66524b":"code","31241a41":"code","e4f10545":"code","e9cc6274":"code","fd558fd0":"code","3ee95af9":"code","4c542645":"code","a1a4fc66":"code","406bffbf":"code","e80b14a3":"code","f1d613af":"code","245ece52":"code","8151385d":"code","9fffd545":"code","3049caa2":"code","bd33b741":"code","0c2f58cf":"code","976095c2":"code","4ef92b9e":"code","1318a161":"code","a79a11d7":"code","c0872686":"code","79645ef8":"code","f6a5137b":"code","42de7853":"code","e0c62e03":"code","dd33a662":"code","5afdea06":"code","40096caf":"code","081ceb74":"code","04ed6efe":"code","970f398e":"code","0c36d7f3":"code","53626df2":"code","52758f48":"code","cf5734d7":"code","7f734237":"code","19f9c548":"code","b3994688":"code","b6e3f327":"markdown","0dd0e105":"markdown","c617d29f":"markdown","a9ce7147":"markdown","3fd69cb7":"markdown","861de3d6":"markdown","9822f815":"markdown","8465fbb2":"markdown","de365751":"markdown","97ccc291":"markdown","31dc8671":"markdown","5d089a07":"markdown","8fe7ebee":"markdown"},"source":{"f96a10d7":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nimport re\nimport numpy as np\nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport string\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nimport nltk\nfrom gensim.utils import simple_preprocess\nfrom nltk.corpus import stopwords\nimport gensim\nfrom sklearn.model_selection import train_test_split\nimport spacy\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nimport pyLDAvis\nimport pyLDAvis.sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom pprint import pprint\nfrom time import time\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport xgboost as xgb\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score,roc_auc_score\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom datetime import datetime\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport keras\nimport tensorflow as tf\nprint('Done')","1f9fd5de":"os.listdir('\/kaggle\/input')","084a26d0":"dataset = pd.read_excel('..\/input\/depression-and-anxiety-comments\/Depression  Anxiety Facebook page Comments Text.xlsx')\ndataset.head()","5e66524b":"dataset.shape","31241a41":"dataset.isnull().sum()","e4f10545":"#Removing URLs with a regular expression\n\ndef remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\nfor i in range(len(dataset)):\n  dataset.at[i,'Comments Text'] = remove_urls(dataset.iloc[i]['Comments Text'])\ndataset.head()","e9cc6274":"# Convert to list\ndata = dataset['Comments Text'].values.tolist()\n\n# Remove Emails\ndata = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n\n# Remove new line characters\ndata = [re.sub('\\s+', ' ', sent) for sent in data]\n\n# Remove distracting single quotes\ndata = [re.sub(\"\\'\", \"\", sent) for sent in data]\n\nprint(data[:1])","fd558fd0":"def sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\ndata_words = list(sent_to_words(data))\n\nprint(data_words[:1])","3ee95af9":"# Build the bigram and trigram models\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n\n# Faster way to get a sentence clubbed as a trigram\/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\n# See trigram example\nprint(trigram_mod[bigram_mod[data_words[0]]])","4c542645":"# Define functions for stopwords, bigrams, trigrams and lemmatization\n\nstop_words = set(stopwords.words(\"english\"))\n\n\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","a1a4fc66":"# Remove Stop Words\ndata_words_nostops = remove_stopwords(data_words)\n\n# Form Bigrams\ndata_words_bigrams = make_bigrams(data_words_nostops)\n\n# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n# python3 -m spacy download en\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n# Do lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized[:1])","406bffbf":"dataset = []\nfor i in range(len(data_lemmatized)):\n    dataset.append(\" \".join(data_lemmatized[i]))\ndataset = pd.Series(dataset)","e80b14a3":"no_features = 15000\n\n# NMF is able to use tf-idf\ntfidf_vectorizer = TfidfVectorizer(ngram_range=(1,3), max_features=no_features)\ntfidf = tfidf_vectorizer.fit_transform(dataset)\ntfidf_feature_names = tfidf_vectorizer.get_feature_names()\n\n# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\ntf_vectorizer = CountVectorizer(min_df=0.05,max_features=no_features)\ntf = tf_vectorizer.fit_transform(dataset)\ntf_feature_names = tf_vectorizer.get_feature_names()","f1d613af":"no_topics = 2\n\n# Run NMF\nnmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5,max_iter=10000).fit(tfidf)\n\n# Run LDA\nlda = LatentDirichletAllocation(n_components=no_topics, max_iter=10, learning_method='online', learning_offset=50.,random_state=0).fit(tf)","245ece52":"def display_topics(model, feature_names, no_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"Topic %d:\" % (topic_idx))\n        print(\", \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n\nno_top_words = 25\nprint('NMF')\ndisplay_topics(nmf, tfidf_feature_names, no_top_words)\nprint('LDA')\ndisplay_topics(lda, tf_feature_names, no_top_words)","8151385d":"# Create Document \u2014 Topic Matrix\nlda_output = lda.transform(tf)\n# column names\ntopicnames = ['Topic' + str(i) for i in range(lda.n_components)]\n# index names\ndocnames = ['Doc' + str(i) for i in range(len(dataset))]\n# Make the pandas dataframe\ndf_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n# Get dominant topic for each document\ndominant_topic = np.argmax(df_document_topic.values, axis=1)\ndf_document_topic['dominant_topic'] = dominant_topic\n\ndf_document_topics = df_document_topic\ndataset2 = pd.read_excel('..\/input\/depression-and-anxiety-comments\/Depression  Anxiety Facebook page Comments Text.xlsx')\ndf_document_topics.reset_index(inplace=True,drop=True)\ndataset2['label'] = df_document_topics['dominant_topic']","9fffd545":"dataset2.head()","3049caa2":"# Create Document \u2014 Topic Matrix\nnmf_output = nmf.transform(tfidf)\n# column names\ntopicnames = ['Topic' + str(i) for i in range(nmf.n_components)]\n# index names\ndocnames = ['Doc' + str(i) for i in range(len(dataset))]\n# Make the pandas dataframe\ndf_document_topic = pd.DataFrame(np.round(nmf_output, 2), columns=topicnames, index=docnames)\n# Get dominant topic for each document\ndominant_topic = np.argmax(df_document_topic.values, axis=1)\ndf_document_topic['dominant_topic'] = dominant_topic\n\ndf_document_topics = df_document_topic\ndataset1 = pd.read_excel('..\/input\/depression-and-anxiety-comments\/Depression  Anxiety Facebook page Comments Text.xlsx')\ndf_document_topics.reset_index(inplace=True,drop=True)\ndataset1['label'] = df_document_topics['dominant_topic']","bd33b741":"dataset1.head()","0c2f58cf":"dataset1[dataset1['label']==1]","976095c2":"for i in range(20):\n    print(dataset1[dataset1['label']==1].iloc[i][0])\n    print('\\n')","4ef92b9e":"dataset2[dataset2['label']==1]","1318a161":"for i in range(20):\n    print(dataset2[dataset2['label']==1].iloc[i][0])\n    print('\\n')","a79a11d7":"dataset2","c0872686":"for i in range(len(dataset2)):\n  dataset2.at[i,'Comments Text'] = remove_urls(dataset2.iloc[i]['Comments Text'])\ndataset2.head()","79645ef8":"# Convert to list\ndata = dataset2['Comments Text'].values.tolist()\n\n# Remove Emails\ndata = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n\n# Remove new line characters\ndata = [re.sub('\\s+', ' ', sent) for sent in data]\n\n# Remove distracting single quotes\ndata = [re.sub(\"\\'\", \"\", sent) for sent in data]\n\n# Remove distracting commas\ndata = [re.sub(\",\", \"\", sent) for sent in data]\n\n# Remove distracting commas\ndata = [sent.lower() for sent in data]\n\n# Remove distracting dots\ndata = [sent.replace('.', '') for sent in data]\n\nprint(data[:1])","f6a5137b":"tweets = np.array(data)\nlabels = np.array(dataset2['label'])","42de7853":"tweets","e0c62e03":"len(tweets)","dd33a662":"from keras.models import Sequential\nfrom keras import layers\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import regularizers\nmax_words = 25000\nmax_len = 200\n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(tweets)\nsequences = tokenizer.texts_to_sequences(tweets)\ntweets = pad_sequences(sequences, maxlen=max_len)\nprint(tweets)","5afdea06":"X_train, X_test, y_train, y_test = train_test_split(tweets,labels, random_state=0)\nprint (len(X_train),len(X_test),len(y_train),len(y_test))","40096caf":"# Detect and init the TPU\n#tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n#tf.config.experimental_connect_to_cluster(tpu)\n#tf.tpu.experimental.initialize_tpu_system(tpu)","081ceb74":"# Instantiate a distribution strategy\n#tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","04ed6efe":"# instantiating the model in the strategy scope creates the model on the TPU\n#with tpu_strategy.scope():\n\n#    model = Sequential()\n#    model.add(layers.Embedding(max_words, 36))\n#    model.add(layers.Bidirectional(layers.LSTM(36,dropout=0.5,recurrent_dropout=0.5, return_sequences=True)))\n#    model.add(layers.Bidirectional(layers.LSTM(36,dropout=0.5,recurrent_dropout=0.5,)))\n#    model.add(layers.Dense(1,activation='sigmoid'))\n\n#    model.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['accuracy'])\n\n#history = model.fit(X_train, y_train, epochs=7,validation_data=(X_test, y_test))","970f398e":"model = Sequential()\nmodel.add(layers.Embedding(max_words, 36))\nmodel.add(layers.Bidirectional(layers.LSTM(36,dropout=0.5,recurrent_dropout=0.5, return_sequences=True)))\nmodel.add(layers.Bidirectional(layers.LSTM(36,dropout=0.5,recurrent_dropout=0.5,)))\nmodel.add(layers.Dense(1,activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train, epochs=7,validation_data=(X_test, y_test))","0c36d7f3":"test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\nprint('Model accuracy: ',test_acc)","53626df2":"y_pred = model.predict(X_test)","52758f48":"len(y_pred)","cf5734d7":"np.around(y_pred, decimals=0)","7f734237":"from sklearn.metrics import confusion_matrix\nmatrix = confusion_matrix(y_test, np.around(y_pred, decimals=0))","19f9c548":"import seaborn as sns\nconf_matrix = pd.DataFrame(matrix, index = ['Not Depression\/Anxiety','Anxiety\/Depression'],columns = ['Not Depression\/Anxiety','Anxiety\/Depression'])\n#Normalizing\nconf_matrix = conf_matrix.astype('float') \/ conf_matrix.sum(axis=1)[:, np.newaxis]\nplt.figure(figsize = (15,15))\nsns.heatmap(conf_matrix, annot=True, annot_kws={\"size\": 15})","b3994688":"#This model is still needing to be adjusted.","b6e3f327":"# Dominant Topics' Extraction\n\nEssentially what we'll do is to attach every document to its respective label.","0dd0e105":"# Topics' Comparison\n\nLet's see how coherent the topics are for each model. Remember we're looking for specifity, otherwise the classifier would get wrong results.","c617d29f":"# Data preprocessing for Topic Modeling\n\nWe need to apply some NLP tasks to prepare the data to be labeled. These are the main tasks:\n\n* Remove URLs\n* Remove emails\n* Lowercase all text\n* Remove punctuation signs\n* Remove stop words\n* Lemmatize text","a9ce7147":"# Data preprocessing for Topic Classification\n\nAs you could see before, our text processing pipeline did very well its task, so we'll use it again to feed and fit our classifier.","3fd69cb7":"# Model training\n\nWe'll explore how LDA and NMF can create the topics and depending on the outcomes we'll select the proper one for this project. Essentially we're looking for focused topics, otherwise the purpose of this project won't be reached.\n\nThe crucial difference between both models is that LDA adds a Dirichlet prior on top of the data generating process, meaning NMF qualitatively leads to worse mixtures, which could affect our dataset's topic quality. \n\nRegarding the library we'll be using: Scikit-Learn - the reasons are more than obvious, even when Gensim has more capabilities, it's also more complex and much more slower - we're looking to keep the things as simpler as possible and get results as quick as possible.\n\nThe outcome of this stage will be the original dataframe with its labels: 1 for depression\/anxiety comments and 0 for other type of comments.","861de3d6":"# Imports","9822f815":"As you can see, NMF delivers better term mixtures for the topic 1 which is the one that cares us the most. Actually, if you read carefully the topic 1 you'll find out that it's what we were looking for since the beginning. Just because NMF is able to work with ngrams is the reason why we get better results and delivers depressive\/anxious actual mixtures and not just terms about it.\n\nLet's go more in depth and get all #1 labels for both models and see which one is more coherent.","8465fbb2":"# The Project\n\nIn this opportunity we'll go through a very particular topic. We all know the lockdown during the COVID-19 is affecting all of us in different ways, but the most frequents are depression and anxiety which is an expected outcome - the natural responses to confinement are precisely these, and most of the people don't even know it. It's been a hard time, people are afraid of uncertainty, of losing their jobs as many people have already done, The conditions are met for a major emotional imbalance.\n\nExperts recommend to stay away from social media because it accelerates the depression process, and who is depressed already will be even more, however people expressions on it are a key instrument to determine how a population is feeling. Most of the social media active people express how they feel in tweets, facebook posts, comments and even Instagram captions. So, starting from there, **can we determine if the depression and anxiety have increased during the lockdown implementing Machine Learning?**\n\nThe next project will cover several steps, but a 10000 feet overview would be:\n\n* **Topic Modeling** - where we'll be looking for two labels: 1 - Depression & anxiety comments, 0 - Other\n* **Topic Classification**\n\nTo achieve both tasks, we'll go through:\n\n* **Data collection** - Getting data from different sources to accomplish the main objective.\n* **Data cleaning** - We'll have to take all the data which is already in different formats and clean it up to then be able to use it.\n* **Natural Language Processing for Topic Modeling** - We'll need to transform the text data into a type that can be interpreted by ML models.\n* **Unsupervised Learning tasks for Topic Modeling** - This is crucial, because most of the data we can find out there is unlabeled, so we first need to identify patterns in it.\n* **Supervised Learning tasks for Topic Classification**- Once the data is labeled, we'll go through several ML algorithms to finally select the one that delivers the best perfomance.\n* Predict depression and anxiety in unseen tweets before and after lockdown\n* Results' charting and conclusions\n\n\n**The Depression & Anxiety Facebook comments dataset was obtained at https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC6111060\/ - for future references.**\n\n**The COVID-19 tweets were acquired by myself implementing tweets scrapping. The dataset is composed by several tweets distribuited in the first week of the US lockdown**","de365751":"# Data Vectorization for Topic Modeling\n\nThis step is crucial, otherwise our models that can only interpret numerical data won't be able to process our text, and we'll do it in a very particular way: we'll implement TFIDF vectorizer and CountVectorizer. The reason why we do this is to then compare NMF and LDA models' topics.","97ccc291":"# Data exploration for Topic Modeling\n\nEssentially what we'll do is to explore how the unlabeled data is to then preprocess it.","31dc8671":"Let's see all depressive\/anxious comments labeled by NMF.","5d089a07":"Our first dataset contains 7145 entries without null values","8fe7ebee":"As you can see, NMF delivers better results because it's more specific and really determines depressive\/anxious comments meanwhile LDA labels all entries that contain depression\/anxiety related words and it makes the labeling more general. **Let's keep NMF's results.**\n\nAs you can see above, now the dataset is properly labeled; the label 1 - Depression & Anxiety-  is pretty coherent. Even when label 0 still contains text with relevant words, if you deep dive into it you'll notice the entries don't seem to be comments related to Depression & Anxiety. Said that, let's move on: It's time now to classify tweets."}}