{"cell_type":{"a4962f74":"code","48923666":"code","ad41c905":"code","c08a8c43":"code","16268644":"code","eb3e00bc":"code","bcaf71b3":"code","0dab5330":"code","1e9fddeb":"code","3297ba90":"code","7ec4dbfd":"code","c4868e90":"code","a2a028df":"code","2fabadb8":"code","62042c95":"code","0d389887":"code","aa0d498a":"code","63829896":"code","c6b04709":"code","c6f01365":"code","fab5521b":"code","1b584996":"code","b26cba9d":"code","eba8f894":"code","06e05c41":"code","5445482f":"markdown","7393577a":"markdown","06feed50":"markdown","d2d3e4a0":"markdown","e023b10e":"markdown","2889715f":"markdown","f3bf618f":"markdown","764819ca":"markdown","71d11dfa":"markdown"},"source":{"a4962f74":"import numpy as np\nimport pandas as pd\nimport matplotlib as plt\nimport seaborn as sns","48923666":"F = pd. read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv')\nT = pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv')","ad41c905":"F['text'].apply(len).mean() \n# Fake News Text length mean.. how long is the text of the news","c08a8c43":"T['text'].apply(len).mean()\n# True News Text length mean.. how long is the text of the news","16268644":"F['text'].apply(len).std() ","eb3e00bc":"T['text'].apply(len).mean() ","bcaf71b3":"F['title'].apply(len).mean()\n# Fake News Title length mean.. how long is the title of the news","0dab5330":"T['title'].apply(len).mean()\n# True News Title length mean.. how long is the title of the news\n# it looks like the title of real news is short.. ","1e9fddeb":"F['title'].apply(len).std()\n# trying to figure out how much do they deviate.. ","3297ba90":"T['title'].apply(len).std()\n# There seems to be an overlap, so cannot move forward with using lengths as a measure of Fake News","7ec4dbfd":"F.groupby('subject').count()\n#Trying to figure out if the subject matters to determine fake or real","c4868e90":"T.groupby('subject').count()\n#we can observe a difference","a2a028df":"T['labels'] = 1\nF['labels'] = 0\ndf = pd.concat([T,F])\n# We will add labels to both the news types to concatanate them and then train them ","2fabadb8":"df.count() # looking at total rows and columns\n# we will be working with only the titles. ","62042c95":"import re # for data cleaning\nimport nltk# general.. for stop words and stem porter\nnltk.download('stopwords')# stop words part 1\nfrom nltk.corpus import stopwords # stopwords part 2\nfrom nltk.stem.porter import PorterStemmer # love and loved will be characterized as one word.. \n#so we need this","0d389887":"import string # we will try to remove punctuation by this method\nnopunc = [c for c in df['title'] if c not in string.punctuation]\n# if we directly try to enter df['title'], it gives a value error. ","aa0d498a":"corpus = []\nfor i in range(0,44898):\n    title = re.sub('[^a-zA-Z]', ' ', nopunc[i]) # over here, cannot directly take df['title']\n    #this also will remove punctuations and brackets and everything else\n    title = title.lower()#lower the uppercase\n    title = title.split()#will split each word\n    ps = PorterStemmer()\n    title = [ps.stem(words) for words in title if not words in set(stopwords.words('english'))]\n    #stopwords will be removed after splitting\n    title = ' '.join(title)#we join the remaining words back\n    corpus.append(title)#whatever is left we put it in the corpus\n    \n#This happens for 44898 times for every news.","63829896":"from sklearn.model_selection import train_test_split# trainign and testing \nX_train, X_test, y_train, y_test = train_test_split(corpus, df['labels'], test_size = 0.20, random_state = 0)","c6b04709":"from sklearn.pipeline import Pipeline \nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\npipeline = Pipeline([('bow', CountVectorizer()), ('classifier', MultinomialNB())])\npipeline.fit(X_train, y_train)\nprediction = pipeline.predict(X_test)","c6f01365":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, prediction))","fab5521b":"cv = CountVectorizer() # max_features = 13000 ---> we can add this argument(optional)\nX=cv.fit_transform(corpus).toarray()\ny= df['labels'].values","1b584996":"len(X[0])","b26cba9d":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.20, random_state = 0)","eba8f894":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\nprediction2 = classifier.predict(X_test)","06e05c41":"print(classification_report(y_test, prediction2))","5445482f":"* # Found difference of 200 words\n* # trying to see if this can be used as an indicator","7393577a":"# Another prediction using GaussianNB from Naive Bayes","06feed50":"* # We have found some difference in the title length. \n* # Lets further explore it ","d2d3e4a0":"* We notice that the title of news is different in length for T\/F News\n* We can use the titles for our analysis purpose rather than text.\n* First I will reate 2 labels: 1 for True, 0 for False\n* Then I will concat the data from both files into one","e023b10e":"# Comparing Texts length of the data","2889715f":" # *Natural Language Processing Modelling*","f3bf618f":"# Comparing Title lengths","764819ca":"* # Grouping the news by subject to spot difference","71d11dfa":"* # Clearly MultiNomial is much better\n* # Upvote if you find this helpfula and useful :)"}}