{"cell_type":{"40bf89dc":"code","6b19d10b":"code","a177889a":"code","e788d9a8":"code","b68983d1":"code","351ad5a2":"code","20abd7b5":"code","b77c793d":"code","b16ce054":"code","d10bd5d7":"code","7e9e60c3":"code","3247d8c8":"code","92d82e2e":"code","b19908a5":"code","fb8d906a":"code","dbad59d5":"code","7084d47b":"code","963f4a92":"code","3576bcc4":"code","27ccbe03":"code","d73c48eb":"code","bdd52d3d":"code","29a06ec9":"code","4816add4":"code","9709d405":"code","44c2361c":"code","e4f879ee":"code","a24a9d8c":"code","f07d4890":"code","531e831a":"code","e3ffcfdc":"code","7660f2c2":"code","3ffce6c1":"code","33b7295e":"code","bd0af3d0":"code","e5fb7970":"code","be55afc1":"code","b1508e96":"code","b15d8167":"code","e20cdc8e":"code","cb56202e":"code","e9452e5d":"code","5b0bb7ec":"code","da67375e":"code","afaed227":"code","4c66dfc4":"markdown","def74fe6":"markdown","31c1b93f":"markdown","50226c13":"markdown","71aa85df":"markdown","d00d7aee":"markdown","059aff5c":"markdown","0eed47a1":"markdown","ceb0f092":"markdown","9f324009":"markdown","ffb2f6f2":"markdown","b983dceb":"markdown","dbab8ca0":"markdown"},"source":{"40bf89dc":"import pandas as pd\nimport numpy as np\nfrom keras.layers import Dense\nfrom keras.models import Sequential ","6b19d10b":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a177889a":"predictors = np.loadtxt(r'\/kaggle\/input\/edu-numpy-data\/student_data.csv', delimiter=',')","e788d9a8":"predictors","b68983d1":"n_cols = predictors.shape[1]","351ad5a2":"model = Sequential()","20abd7b5":"model.add(Dense(100.9, activation='relu'))","b77c793d":"model.add(Dense(1))","b16ce054":"input_data = np.array([2,3])","d10bd5d7":"weights = { 'node0': np.array([1,1]),\n            'node1': np.array([-1,1]),\n            'output': np.array([2,-1])}","7e9e60c3":"node_0_value = (input_data * weights['node0']).sum()","3247d8c8":"node_1_value = (input_data * weights['node1']).sum()","92d82e2e":"hidden_layer_values = np.array([node_0_value, node_1_value])\nprint(hidden_layer_values)","b19908a5":"output = (hidden_layer_values * weights['output']).sum()\nprint(output)","fb8d906a":"input_data1 = np.array([-1,2])","dbad59d5":"weights1 = { 'node0': np.array([3,3]),\n            'node1': np.array([1,5]),\n            'output': np.array([2,-1])}","7084d47b":"node_0_input = (input_data1 * weights1['node0']).sum()","963f4a92":"node_0_output = np.tanh(node_0_input)","3576bcc4":"node_1_input = (input_data1 * weights1['node1']).sum()","27ccbe03":"node_1_output = np.tanh(node_1_input)","d73c48eb":"hidden_layer_outputs = np.array([node_0_output, node_1_output])\nprint(hidden_layer_outputs)","bdd52d3d":"output = (hidden_layer_outputs * weights1['output']).sum()\nprint(output)","29a06ec9":"def relu(input):\n    '''Define your relu activation function here'''\n    # Calculate the value for the output of the relu function: output\n    output = max(0, input)\n    \n    # Return the value just calculated\n    return(output)\n\n# Calculate node 0 value: node_0_output\nnode_00_input = (input_data * weights['node0']).sum()\nnode_00_output = relu(node_0_input)\n\n# Calculate node 1 value: node_1_output\nnode_11_input = (input_data * weights['node1']).sum()\nnode_11_output = relu(node_1_input)\n\n# Put node values into array: hidden_layer_outputs\nhidden_layer_outputs = np.array([node_0_output, node_1_output])\n\n# Calculate model output (do not apply relu)\nmodel_output = (hidden_layer_outputs * weights['output']).sum()\n\n# Print model output\nprint(model_output)","4816add4":"# Define predict_with_network()\ndef predict_with_network(input_data_row, weights):\n\n    # Calculate node 0 value\n    node_0_input = (input_data_row * weights['node0']).sum()\n    node_0_output = relu(node_0_input)\n\n    # Calculate node 1 value\n    node_1_input = (input_data_row * weights['node1']).sum()\n    node_1_output = relu(node_1_input)\n\n    # Put node values into array: hidden_layer_outputs\n    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n    \n    # Calculate model output\n    input_to_final_layer = (hidden_layer_outputs * weights['output']).sum()\n    model_output = relu(input_to_final_layer)\n    \n    # Return model output\n    return(model_output)\n\n\n# Create empty list to store prediction results\nresults = []\nfor input_data_row in input_data:\n    # Append prediction to results\n    results.append(predict_with_network(input_data_row, weights))\n\n# Print results\nprint(results)","9709d405":"# i DIDNT CREATE MORE NODES HERE THATS WHY THE PREDICTION IS COMING ZERO\ndef predict_with_network(input_data):\n    # Calculate node 0 in the first hidden layer\n    node_0_0_input = (input_data * weights['node0']).sum()\n    node_0_0_output = relu(node_0_0_input)\n\n    # Calculate node 1 in the first hidden layer\n    node_0_1_input = (input_data * weights['node1']).sum()\n    node_0_1_output = relu(node_0_1_input)\n\n    # Put node values into array: hidden_0_outputs\n    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n    \n    # Calculate node 0 in the second hidden layer\n    node_1_0_input = (hidden_0_outputs * weights['node1']).sum()\n    node_1_0_output = relu(node_1_0_input)\n\n    # Calculate node 1 in the second hidden layer\n    node_1_1_input = (hidden_0_outputs * weights['node1']).sum()\n    node_1_1_output =  relu(node_1_1_input)\n\n    # Put node values into array: hidden_1_outputs\n    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n\n    # Calculate model output: model_output\n    model_output = (hidden_1_outputs * weights['output']).sum()\n    \n    # Return model_output\n    return(model_output)\n\noutput = predict_with_network(input_data)\nprint(output)","44c2361c":"def predict_with_network1(input_data, weights):\n    # Calculate node 0 in the first hidden layer\n    node_0_0_input = (input_data * weights['node_0']).sum()\n    node_0_0_output = relu(node_0_0_input)\n\n    # Calculate node 1 in the first hidden layer\n    node_0_1_input = (input_data * weights['node_1']).sum()\n    node_0_1_output = relu(node_0_1_input)\n\n    # Put node values into array: hidden_0_outputs\n    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n    \n    # Calculate node 0 in the second hidden layer\n    node_1_0_input = (hidden_0_outputs * weights['node_1']).sum()\n    node_1_0_output = relu(node_1_0_input)\n\n    # Calculate node 1 in the second hidden layer\n    node_1_1_input = (hidden_0_outputs * weights['node_1']).sum()\n    node_1_1_output =  relu(node_1_1_input)\n\n    # Put node values into array: hidden_1_outputs\n    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n\n    # Calculate model output: model_output\n    model_output = (hidden_1_outputs * weights['output']).sum()\n    \n    # Return model_output\n    return(model_output)\n# The data point you will make a prediction for\ninput_data = np.array([0, 3])\n\n# Sample weights\nweights_0 = {'node_0': [2, 1],\n             'node_1': [1, 2],\n             'output': [1, 1]\n            }\n\n# The actual target value, used to calculate the error\ntarget_actual = 3\n\n# Make prediction using original weights\nmodel_output_0 = predict_with_network1(input_data, weights_0)\n\n# Calculate error: error_0\nerror_0 = model_output_0 - target_actual\n\n# Create weights that cause the network to make perfect prediction (3): weights_1\nweights_1 = {'node_0': [2, 1],\n             'node_1': [1, 2],\n             'output': [1, 0]\n            }\n\n# Make prediction using new weights: model_output_1\nmodel_output_1 = predict_with_network1(input_data, weights_1)\n\n# Calculate error: error_1\nerror_1 = model_output_1 - target_actual\n\n# Print error_0 and error_1\nprint(error_0)\nprint(error_1)","e4f879ee":"weights = np.array([1,2])\ninput_data =  np.array([3,4])\ntarget = 6\nlearning_rate = 0.01\npreds = (weights * input_data).sum()\nerror =  preds - target\nprint(error)","a24a9d8c":"gradient =  2*input_data*error\ngradient","f07d4890":"weights_updated = weights - learning_rate*gradient\npreds_updated = (weights_updated * input_data).sum()\nerrors_updated = preds_updated - target\nprint(errors_updated)","531e831a":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nn_updates = 20\nmse_hist = []\nweights = np.array([2,3])\n# Iterate over the number of updates\nfor i in range(n_updates):\n    # Calculate the slope: slope\n    slope = gradient*input_data*target\n    \n    # not correct\n    # Update the weights: weights\n    weights = weights - 0.01 * slope\n    \n    # Calculate mse with new weights: mse\n    mse = input_data*target*weights\n    \n    # Append the mse to mse_hist\n    mse_hist.append(mse)\n\n# Plot the mse history\nplt.plot(mse_hist)\nplt.xlabel('Iterations')\nplt.ylabel('Mean Squared Error')\nplt.show()","e3ffcfdc":"# Creating A Keras Model\npredictors1 = np.loadtxt(r'\/kaggle\/input\/edu-numpy-data\/student_data.csv', delimiter=',')\n\ntarget = 1.\ntarget_o = np.array(target)\n# if data is not numericals\npredictors = np.asarray(predictors1).astype('float32')                                                                    \nn_cols = predictors1.shape[1]\nmodel1 = Sequential()\nmodel1.add(Dense(100.9, activation='relu'))\nmodel1.add(Dense(100.9, activation='relu'))\nmodel1.add(Dense(1))\n# Adam is a good optimizder choice\nmodel1.compile(optimizer='Adam', loss='mean_squared_error')\n# we can then fit a model","7660f2c2":"data = pd.read_csv(r'\/kaggle\/input\/student-data-2\/student_data.csv')","3ffce6c1":"data.dtypes","33b7295e":"data.dtypes","bd0af3d0":"# Classification Models\nfrom keras.utils.np_utils import to_categorical\npredictors1 = data.drop(['admit'], axis=1)\npredictors = np.asarray(predictors1) \ntarget = data['gpa']\nn_cols = predictors.shape[1]\nmodel1 = Sequential()\nmodel1.add(Dense(100, activation='relu'))\nmodel1.add(Dense(100, activation='relu'))\nmodel1.add(Dense(100, activation='relu'))\nmodel1.add(Dense(2, activation='softmax'))\n# Adam is a good optimizder choice\nmodel1.compile(optimizer='adam', loss='categorical_crossentropy', \n               metrics=['accuracy'])\n# we can then fit a model","e5fb7970":"data.isnull().sum()","be55afc1":"data.describe()","b1508e96":"# from keras.models import load_model\n# model.save('model_file.h5')\n# my_model = load_ model('my_model.h5')\n# pred = my_model.predict(data to predict with)\n# prob_true = pred[:,1]","b15d8167":"def get_new_model(input_shape):\n    model = Sequential()\n    model = Sequential()\n    model.add(Dense(100, activation='relu', input_shape=input_shape))\n    model.add(Dense(100, activation='relu'))\n    model.add(Dense(2, activation='softmax'))\n    return(model)","e20cdc8e":"from tensorflow.keras.optimizers import SGD\nlr_to_test = [.000001, 0.01, 1]\n# loop over learning rates\n# for lr in lr_to_test:\n    # model = get_new_model(input_shape=input_shape)\n    #my_optimizer = SGD(lr=lr)\n    #model.compile(optimizer = my_optimizer, loss = 'categorical_crossentropy')  \n    # we can then fit the model","cb56202e":"from keras.callbacks import EarlyStopping\nearly_stopping_monitor = EarlyStopping(patience=2)","e9452e5d":"# Fit the model\n# Model.compile(predictors, target, validation_split=0.3, nb_epoch=20,call_backs =[early_stopping_monitor])","5b0bb7ec":"# Experimenting with wider networks","da67375e":"# Define early_stopping_monitor\n##early_stopping_monitor = EarlyStopping(patience=2)\n\n# Create the new model: model_2\n##model_2 = Sequential()\n\n# Add the first and second layers\n##model_2.add(Dense(100, activation='relu', input_shape=input_shape))\n##model_2.add(Dense(100, activation='relu'))\n\n# Add the output layer\n##model_2.add(Dense(2, activation='softmax'))\n\n# Compile model_2\n##model_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Fit model_1\n##model_1_training = model_1.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n\n# Fit model_2\n##model_2_training = model_2.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n\n# Create the plot\n##plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\n##plt.xlabel('Epochs')\n##plt.ylabel('Validation score')\n##plt.show()","afaed227":"# Stepping Up to Images \n# Building your own Digit recognition Model\n# Create the model: model\n## model = Sequential()\n\n# Add the first hidden layer\n##model.add(Dense(50, activation='relu', input_shape=(784,)))\n\n# Add the second hidden layer\n##model.add(Dense(50, activation='relu'))\n\n# Add the output layer\n##model.add(Dense(10, activation='softmax'))\n\n# Compile the model\n##model.compile(optimizer='adam',\n              ##loss='categorical_crossentropy',\n              ##metrics=['accuracy'])\n\n# Fit the model\n##model.fit(X, y, validation_split=0.3)\n","4c66dfc4":"# Multi-layer Networks","def74fe6":"# Saving, Reloading and Using Model","31c1b93f":"# Build and Tune Deep Learning Models uusing Keras ","50226c13":"# Activiation Function","71aa85df":"# Model Validation","d00d7aee":"# Creating A Keras Model","059aff5c":"# compile and fitting a model","0eed47a1":"# Stochastic Gradient Descent","ceb0f092":"# How to calculate slope and update weights","9f324009":"# The Need for Optimization","ffb2f6f2":"# Forward Propogation code","b983dceb":"# Early Stopping Monitors","dbab8ca0":"# Calculate with a Relu Function"}}