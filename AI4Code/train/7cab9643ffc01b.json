{"cell_type":{"8b124533":"code","593d78be":"code","7a817325":"code","ccf6d360":"code","8f69b2fc":"code","8d3a913b":"code","e5bd0ecd":"code","c412ccca":"code","f3d08b31":"code","e6ab3019":"code","dfdd75e6":"code","916cc9b4":"code","e7ef16d1":"code","4b8dc394":"code","63770946":"code","7c2c13a9":"code","b6b1c7e7":"code","8bd25462":"code","3ae54f3e":"code","451883c9":"code","5af72bb6":"code","d8970c29":"code","6c499a9b":"code","d73ecaba":"code","13b3ff9d":"code","56b7083b":"code","bfa03158":"code","d4fe7f6d":"code","afa38d10":"code","0f04b413":"code","c69c2aa9":"code","72a693fe":"code","1552b92f":"code","b305e802":"code","0231304e":"markdown","43b63bf6":"markdown","5b8e62b0":"markdown","a2937319":"markdown","093349b8":"markdown","d1672e76":"markdown","6943ec18":"markdown","85f1df07":"markdown","d00160af":"markdown","c776331b":"markdown","5ba8819a":"markdown","058ec803":"markdown","878a0eec":"markdown","a5b3610f":"markdown","3a1be5e1":"markdown","c52a78b1":"markdown","a9e201cf":"markdown"},"source":{"8b124533":"import torch\ntorch.__version__","593d78be":"#Refer official document, https:\/\/pytorch.org\/blog\/pytorch-1.6-now-includes-stochastic-weight-averaging\/\nif False:\n    from torch.optim.swa_utils import AveragedModel, SWALR\n    from torch.optim.lr_scheduler import CosineAnnealingLR\n\n    loader, optimizer, model, loss_fn = ...\n    swa_model = AveragedModel(model)\n    scheduler = CosineAnnealingLR(optimizer, T_max=100)\n    swa_start = 5\n    swa_scheduler = SWALR(optimizer, swa_lr=0.05)\n\n    for epoch in range(100):\n        for input, target in loader:\n            optimizer.zero_grad()\n            loss_fn(model(input), target).backward()\n            optimizer.step()\n        if epoch > swa_start:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n        else:\n            scheduler.step()\n\n    # Update bn statistics for the swa_model at the end\n    torch.optim.swa_utils.update_bn(loader, swa_model)\n    # Use swa_model to make predictions on test data \n    preds = swa_model(test_input)","7a817325":"!ls ..\/input\/iterative-stratification","ccf6d360":"import sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","8f69b2fc":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.swa_utils import AveragedModel, SWALR\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nimport warnings\nwarnings.filterwarnings('ignore')","8d3a913b":"os.listdir('..\/input\/lish-moa')","e5bd0ecd":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","c412ccca":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","f3d08b31":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","e6ab3019":"train_targets_scored.sum()[1:].sort_values()","dfdd75e6":"train_features['cp_type'].unique()","916cc9b4":"# GENES\nn_comp = 50\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","e7ef16d1":"#CELLS\nn_comp = 15\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","4b8dc394":"from sklearn.feature_selection import VarianceThreshold\n\n\nvar_thresh = VarianceThreshold(threshold=0.5)\ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\ntrain_features\n","63770946":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntarget = train[train_targets_scored.columns]","7c2c13a9":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","b6b1c7e7":"train","8bd25462":"folds = train.copy()\n\nmskf = MultilabelStratifiedKFold(n_splits=5)\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)\nfolds","3ae54f3e":"print(train.shape)\nprint(folds.shape)\nprint(test.shape)\nprint(target.shape)\nprint(sample_submission.shape)","451883c9":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        x = torch.tensor(self.features[idx, :], dtype=torch.float)\n        y = torch.tensor(self.targets[idx, :], dtype=torch.float)\n        return x, y\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        x = torch.tensor(self.features[idx, :], dtype=torch.float)\n        return x\n    ","5af72bb6":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.Linear(num_features, hidden_size)\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.5)\n        self.dense2 = nn.Linear(hidden_size, hidden_size)\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.5)\n        self.dense3 = nn.Linear(hidden_size, num_targets)\n        \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","d8970c29":"def process_data(data):\n    \n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n#     data.loc[:, 'cp_time'] = data.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})\n#     data.loc[:, 'cp_dose'] = data.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n\n# --------------------- Normalize ---------------------\n#     for col in GENES:\n#         data[col] = (data[col]-np.mean(data[col])) \/ (np.std(data[col]))\n    \n#     for col in CELLS:\n#         data[col] = (data[col]-np.mean(data[col])) \/ (np.std(data[col]))\n    \n#--------------------- Removing Skewness ---------------------\n#     for col in GENES + CELLS:\n#         if(abs(data[col].skew()) > 0.75):\n            \n#             if(data[col].skew() < 0): # neg-skewness\n#                 data[col] = data[col].max() - data[col] + 1\n#                 data[col] = np.sqrt(data[col])\n            \n#             else:\n#                 data[col] = np.sqrt(data[col])\n    \n    return data","6c499a9b":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()","d73ecaba":"feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)","13b3ff9d":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 45\nSWA_START_EPOCH = 30\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 5\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=1024\n","56b7083b":"def train_fn(swa_model, model, optimizer, scheduler, swa_scheduler, epoch, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        data_x, data_y = data\n        inputs, targets = data_x.to(device), data_y.to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        \n        if epoch > SWA_START_EPOCH:\n            swa_model.update_parameters(model)\n            swa_scheduler.step()\n        else:\n            scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        data_x, data_y = data\n        inputs, targets = data_x.to(device), data_y.to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data.to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds","bfa03158":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    swa_model = AveragedModel(model)\n    \n    model = model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, \n                                              div_factor=1e3, max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    swa_scheduler = SWALR(optimizer, swa_lr=LEARNING_RATE, anneal_strategy=\"cos\", anneal_epochs=10)\n\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(swa_model, model, optimizer,scheduler, swa_scheduler, epoch, loss_fn, trainloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                oof[val_idx] = valid_preds\n                torch.optim.swa_utils.update_bn(trainloader, swa_model)\n                torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n                break\n                \n    oof[val_idx] = valid_preds\n    torch.optim.swa_utils.update_bn(trainloader, swa_model)\n    torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n            \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","d4fe7f6d":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ \/ NFOLDS\n        oof += oof_\n        \n    return oof, predictions","afa38d10":"SEED = 777\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n    \noof_, predictions_ = run_k_fold(NFOLDS, SEED)\n\noof  += oof_\npredictions += predictions_\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions","0f04b413":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)","c69c2aa9":"y_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values","72a693fe":"score = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ \/ target.shape[1]\n    \nprint(\"CV log_loss: \", score)\n    ","1552b92f":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","b305e802":"sub.shape","0231304e":"## Preparation","43b63bf6":"## feature Selection using Variance Encoding","5b8e62b0":"## Single fold training","a2937319":"The CV score of refered notebook is 0.014650792660668535. There is some differences of implementation, but it seems to be performing a little too poorly. If you know good improvement, please share!","093349b8":"## CV folds","d1672e76":"## PCA features + Existing features","6943ec18":"## Dataset Classes","85f1df07":"<a id=\"3\"><\/a> <br>\n# <div class=\"alert alert-block alert-info\">Usage of Pytorch module<\/div>\n\nUsing torch.optim.swa_utils in pytorch 1.6, we can use SWA with no extra modules.\n\nNow, we can use pytorch 1.6 in kaggle notebook.","d00160af":"I want you to see the official documentation and github implementation for details, but I'll summarize usages.\n\n- We can get AveragedModel instance by pass our model to AveragedModel. This instance accumulates the averages of the weights.\n\n- We can update the parameters of the averaged model by update_parameters(model)\n\n- Using SWALR, we can schedule  learning rate to anneal to a fixed value, and then keeps it constant. \n\n- update_bn is utility function to update SWA batch normalization statistics at the end of training.","c776331b":"<a id=\"4\"><\/a> <br>\n# <div class=\"alert alert-block alert-success\">Application<\/div>\n\nI'll use torch.optim.swa_utils and check its' power.\n\nFor training and inference piplin, I refer the great notebook,\n\nhttps:\/\/www.kaggle.com\/namanj27\/new-baseline-pytorch-moa\n\nBut with orginal notebook, torch.optim.swa_utils don't work, so I changed where necessary.","5ba8819a":"<a id=\"1\"><\/a> <br>\n# <div class=\"alert alert-block alert-success\">Introduction<\/div>\n\nNow, PyTorch 1.6 includes Stochastic Weight Averaging.\n\nUntil now, to use SWA, we had to turn on internet and install torch-contrib. A recent competition often required internet off, which was a bit difficult to use.\n\nAs you can see from my CV score, I'm not fully proficient in using SWA. I would like to publish this notebook to encourage more people to use SWA and discuss how to use it effectively.","058ec803":"## Reference\n\nGood documents to learn SWA. Especially, [1] is reference of figures (orginal article of SWA). \n\n[1] https:\/\/arxiv.org\/abs\/1803.05407\n\n[2] https:\/\/github.com\/timgaripov\/swa\n\n[3] https:\/\/towardsdatascience.com\/stochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a\n\n[4] https:\/\/pytorch.org\/blog\/pytorch-1.6-now-includes-stochastic-weight-averaging\/\n\n[5] https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/torch\/optim\/swa_utils.py","878a0eec":"### <div class=\"alert alert-block alert-warning\">\u2193Note implementation of __getitem__. To use update_bn, dataset should return tensor or list of tensor.<\/div>","a5b3610f":"## Preprocessing steps","3a1be5e1":"# Pytorch with SWA\n\n## **Content**\n1. [Introduction](#1)\n1. [About SWA (Stochastic Weight Averaging)](#2)\n1. [Usage of Pytorch module](#3)\n1. [Application](#4)\n","c52a78b1":"<a id=\"2\"><\/a> <br>\n# <div class=\"alert alert-block alert-info\">About SWA (Stochastic Weight Averaging)<\/div>\n\nSimply put, it would be an ensemble with the weight of the cyclic learning process going on.\n\nIt is normal for the solution by training to be somewhat out of sync with the general solution you originally wanted, and SWA can be often used to get the more universal solution.\n\nThe image used in article [1] is here, \n\n<img src=\"https:\/\/github.com\/tasotasoso\/kaggle_media\/blob\/main\/MoA\/Averaging%20Weights%20Leads_to_Wider_Optima_and_Better_Generalization_fig.png?raw=true\" width=\"1000\">\n\nThe image on the right shows that the train loss is minimized when SWA is not used. But on the left and in the middle, you can see that the TEST error is smaller when SWA is used.\n\nThe weight update argorism is here, in simply, \n\n$$\nw_{SWA} = \\frac{w_{SWA}*n_{models} + w}{n_{models} + 1} \n$$\n\nHere, w_SWA is gotten weight by SWA and n_models is number of models we calculated ensemble.\n\nFor implementation, article [1] has more specific representation, \n\n<img src=\"https:\/\/github.com\/tasotasoso\/kaggle_media\/blob\/main\/MoA\/Averaging%20Weights%20Leads_to_Wider_Optima_and_Better_Generalization_argorism.png?raw=true\" width=\"350\">","a9e201cf":"## Model"}}