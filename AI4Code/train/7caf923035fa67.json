{"cell_type":{"f913b407":"code","c9b17855":"code","a5ad1f15":"code","90aabc75":"code","b2bf7628":"code","ba7031c6":"code","19216b27":"code","7e0ddf2c":"code","c175f066":"code","ba1a13cc":"code","a77940f7":"code","3de0c74b":"code","56f48719":"code","4417f0c2":"code","2768c858":"code","6de94dfb":"code","990d35eb":"code","a95fd0dc":"code","f680c07a":"code","c73c77e8":"code","bfc19bb2":"code","0a273c66":"code","d9bff586":"code","376ccc39":"code","79b078e8":"code","6d5e55a6":"markdown","98f825af":"markdown"},"source":{"f913b407":"# Importing important libraries\nfrom keras.models import Model, Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation, GlobalAveragePooling2D, add\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.layers import Concatenate\nfrom keras.optimizers import Adam, SGD\nfrom keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import load_model\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import np_utils\nfrom keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\nfrom keras.models import load_model\nfrom keras.callbacks import *\nfrom keras.regularizers import l2\nfrom keras import backend as k\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nimport imgaug as ia\nfrom imgaug import augmenters as iaa\n\n# import wget\n\nimport glob\nimport pathlib\nimport cv2\nimport zipfile\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport itertools\nimport six\n\nnum_classes = 200\nnum_train = 90000\n# input image dimensions\nimg_height, img_width = 64,64\n# The images are RGB.\nimg_channels = 3\nnum_validation = 10000\nbatch_size = 60","c9b17855":"\"\"\" Mount gdrive on colab\"\"\"\nfrom google.colab import drive\ndrive.mount('\/content\/drive')","a5ad1f15":"\"\"\"\n      upload the zip file in gdrive. Mount it on colab and unzip it using the \n      following command\n\"\"\"\n!unzip '.\/drive\/My Drive\/image-detect.zip' -d '.\/drive\/My Drive\/'","90aabc75":"\"\"\"\n      Architeture for VGG16\n\"\"\"\n\n_input_ = Input(shape=(img_height, img_width, img_channels)) \n\nconv1  = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\" , kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(_input_)\nconv2  = Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\" , kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(conv1)\npool1  = MaxPooling2D((2, 2))(conv2)\n\nconv3  = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\" , kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(pool1)\nconv4  = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\" , kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(conv3)\npool2  = MaxPooling2D((2, 2))(conv4)\n\nconv5  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\" , kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(pool2)\nconv6  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\" , kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(conv5)\nconv7  = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(conv6)\npool3  = MaxPooling2D((2, 2))(conv7)\n\nconv8  = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\" , kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(pool3)\nconv9  = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\" , kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(conv8)\nconv10 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\" , kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(conv9)\npool4  = MaxPooling2D((2, 2))(conv10)\n\nconv11 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\" , kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(pool4)\nconv12 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\" , kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(conv11)\nconv13 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\" , kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(conv12)\npool5  = MaxPooling2D((2, 2))(conv13)\n\nflat   = Flatten()(pool5)\ndense1 = Dense(4096, activation=\"relu\")(flat)\ndense2 = Dense(4096, activation=\"relu\")(dense1)\noutput = Dense(200, activation=\"softmax\")(dense2)\n\nvgg16_model  = Model(inputs=_input_, outputs=output)\nmodel = vgg16_model\n\nmodel.summary()\ntf.keras.utils.plot_model(model, show_shapes=True)","b2bf7628":"\"\"\"Architecture for ResNet-34\n\"\"\"\n\n# # Model building\nip = Input(shape=(img_height, img_width, img_channels))\n\n# Block 1\nlayer0 = Conv2D(32, (3,3), padding='same', kernel_initializer=\"VarianceScaling\"\n                    ,kernel_regularizer=tf.keras.regularizers.l2(2e-4))(ip)\nlayer0 = BatchNormalization()(layer0)\nlayer0 = Activation('relu')(layer0)\n\nskip_connection_1 = layer0\n\n# # Block 2\n\nlayer1 = Conv2D(128, (3,3), padding='same', kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer0)\nlayer1 = BatchNormalization()(layer1)\nlayer1 = Activation('relu')(layer1)\n\nlayer2 = Conv2D(128, (3,3), padding='same', kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer1)\nlayer2 = BatchNormalization()(layer2)\nlayer2 = Activation('relu')(layer2)\n\nlayer3 = Conv2D(128, (3,3), padding='same', kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer2)\nlayer3 = BatchNormalization()(layer3)\nlayer3 = Activation('relu')(layer3)\n\nlayer4 = Conv2D(128, (3,3), padding='same', kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer3)\nlayer4 = BatchNormalization()(layer4)\nlayer4 = Activation('relu')(layer4)\n\nlayer5 = concatenate([skip_connection_1, layer4])\nlayer5 = BatchNormalization()(layer5)\nlayer5 = Activation('relu')(layer5)\nlayer5 = MaxPooling2D(pool_size=(2, 2))(layer5)\n\nskip_connection_2 = layer5\n\n# # Block 3\n\nlayer6 = Conv2D(256, (3,3), padding='same', kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer5)\nlayer6 = BatchNormalization()(layer6)\nlayer6 = Activation('relu')(layer6)\n\nlayer7 = Conv2D(256, (3,3), padding='same', kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer6)\nlayer7 = BatchNormalization()(layer7)\nlayer7 = Activation('relu')(layer7)\n\nlayer8 = Conv2D(256, (3,3), padding='same', kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer7)\nlayer8 = BatchNormalization()(layer8)\nlayer8 = Activation('relu')(layer8)\n\nlayer9 = Conv2D(256, (3,3), padding='same', kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer8)\nlayer9 = BatchNormalization()(layer9)\nlayer9 = Activation('relu')(layer9)\n\nlayer10 = concatenate([skip_connection_2, layer9])\nlayer10 = BatchNormalization()(layer10)\nlayer10 = Activation('relu')(layer10)\nlayer10 = MaxPooling2D(pool_size=(2, 2))(layer10)\n\nskip_connection_3 = layer10\n\n\n# # Block 4\n\nlayer11 = Conv2D(512, (3,3), padding='same', kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer10)\nlayer11 = BatchNormalization()(layer11)\nlayer11 = Activation('relu')(layer11)\n\nlayer12 = Conv2D(512, (3,3), padding='same', kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer11)\nlayer12 = BatchNormalization()(layer12)\nlayer12 = Activation('relu')(layer12)\n\nlayer13 = Conv2D(512, (3,3), padding='same',kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer12)\nlayer13 = BatchNormalization()(layer13)\nlayer13 = Activation('relu')(layer13)\n\nlayer14 = Conv2D(512, (3,3), padding='same',kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer13)\nlayer14 = BatchNormalization()(layer14)\nlayer14 = Activation('relu')(layer14)\n\nlayer15 = concatenate([skip_connection_3, layer14])\nlayer15 = BatchNormalization()(layer15)\nlayer15 = Activation('relu')(layer15)\nlayer15 = MaxPooling2D(pool_size=(2, 2))(layer15)\n\n\n# #Layer 16\nlayer16 = Conv2D(num_classes, (1,1), padding='same',kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer15)\nlayer16 = GlobalAveragePooling2D()(layer16)\n\n# # #Output Layer\noutput = Activation('softmax')(layer16)\n","ba7031c6":"\"\"\" Architecture for DenseNet-50\n\"\"\"\n\ndef space_to_depth_x2(x):\n    return tf.nn.space_to_depth(x, block_size=2)\n\nip = Input(shape=(None, None, img_channels))\n\n# Block 1\n\nlayer1 = Conv2D(32, (3,3), strides=(1,1), padding='same', name='conv_1' , kernel_initializer=\"VarianceScaling\" , kernel_regularizer=tf.keras.regularizers.l2(2e-4))(ip)\nlayer1 = BatchNormalization(name='norm_1')(layer1)\nlayer1 = Activation(\"relu\")(layer1)\n\nlayer2 = Conv2D(64, (3,3), strides=(1,1), padding='same', name='conv_2' , kernel_initializer=\"VarianceScaling\" , kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer1)\nlayer2 = BatchNormalization(name='norm_2')(layer2)\nlayer2 = Activation(\"relu\")(layer2)\n\nlayer3 = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_3' , kernel_initializer=\"VarianceScaling\" , kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer2)\nlayer3 = BatchNormalization(name='norm_3')(layer3)\nlayer3 = Activation(\"relu\")(layer3)\n\nlayer4 = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_4' , kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer3)\nlayer4 = BatchNormalization(name='norm_4')(layer4)\nlayer4 = Activation(\"relu\")(layer4)\n\nlayer5 = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_5' , kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer4)\nlayer5 = BatchNormalization(name='norm_5')(layer5)\nlayer5 = Activation(\"relu\")(layer5)\n\nlayer6 = MaxPooling2D(pool_size=(2, 2))(layer5)\n\nskip_connection_1 = layer6\n\n# Block 2\n\nlayer7 = Conv2D(64, (3,3), strides=(1,1), padding='same', name='conv_7' , kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer6)\nlayer7 = BatchNormalization(name='norm_7')(layer7)\nlayer7 = Activation(\"relu\")(layer7)\n\nlayer8 = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_8' , kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer7)\nlayer8 = BatchNormalization(name='norm_8')(layer8)\nlayer8 = Activation(\"relu\")(layer8)\n\nlayer9 = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_9' , kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer8)\nlayer9 = BatchNormalization(name='norm_9')(layer9)\nlayer9 = Activation(\"relu\")(layer9)\n\nlayer10 = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_10' , kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer9)\nlayer10 = BatchNormalization(name='norm_10')(layer10)\nlayer10 = Activation(\"relu\")(layer10)\n\nlayer11 = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_11' , kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer10)\nlayer11 = BatchNormalization(name='norm_11')(layer11)\nlayer11 = Activation(\"relu\")(layer11)\n\nlayer12 = MaxPooling2D(pool_size=(2, 2))(layer11)\n\nskip_connection_1 = Lambda(space_to_depth_x2)(skip_connection_1)\n\nlayer13 = concatenate([skip_connection_1, layer12])\n\nskip_connection_2 = layer13\n\n# Block 3\n\nlayer14 = Conv2D(32, (3,3), strides=(1,1), padding='same', name='conv_14' , kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer13)\nlayer14 = BatchNormalization(name='norm_14')(layer14)\nlayer14 = Activation(\"relu\")(layer14)\n\nlayer15 = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_15' , kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer14)\nlayer15 = BatchNormalization(name='norm_15')(layer15)\nlayer15 = Activation(\"relu\")(layer15)\n\nlayer16 = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_16' , kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer15)\nlayer16 = BatchNormalization(name='norm_16')(layer16)\nlayer16 = Activation(\"relu\")(layer16)\n\nlayer17 = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_17' , kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer16)\nlayer17 = BatchNormalization(name='norm_17')(layer17)\nlayer17 = Activation(\"relu\")(layer17)\n\nlayer18 = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_18' , kernel_initializer=\"VarianceScaling\",kernel_regularizer=tf.keras.regularizers.l2(2e-4))(layer17)\nlayer18 = BatchNormalization(name='norm_18')(layer18)\nlayer18 = Activation(\"relu\")(layer18)\n\nlayer19 = MaxPooling2D(pool_size=(2, 2))(layer18)\n\nskip_connection_2 = Lambda(space_to_depth_x2)(skip_connection_2)\n\nlayer20 = concatenate([skip_connection_2, layer19])\n\nlayer21 = Conv2D(num_classes, (1,1), name='conv_21')(layer20)\nlayer21 = BatchNormalization(name='norm_21')(layer21)\n\nlayer22 = GlobalAveragePooling2D(data_format=None)(layer21)\n\nlayer23 = Activation('softmax')(layer22)\n\noutput = layer23\n","19216b27":"\"\"\"\n    A simple convnet architecture\n\"\"\"\n\"\"\"Block 1\"\"\"\nmodel.add(Conv2D(128, (3, 3), strides=(1,1), padding='same', input_shape=(img_height,img_width,img_channels)))\nprint(model.layers[-1].output_shape)\nmodel.add(BatchNormalization())\nprint(model.layers[-1].output_shape)\nmodel.add(Conv2D(128, (3, 3), strides=(1,1), padding='same'))\nprint(model.layers[-1].output_shape)\nmodel.add(BatchNormalization())\nprint(model.layers[-1].output_shape)\nmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'))\nprint(model.layers[-1].output_shape)\nmodel.add(Activation('relu'))\nprint(model.layers[-1].output_shape)\n\n\"\"\"Block 2\"\"\"\nmodel.add(Conv2D(128, (3, 3), strides=(1,1), padding='same'))\nprint(model.layers[-1].output_shape)\nmodel.add(BatchNormalization())\nprint(model.layers[-1].output_shape)\nmodel.add(Conv2D(128, (3, 3), strides=(1,1), padding='same'))\nprint(model.layers[-1].output_shape)\nmodel.add(BatchNormalization())\nprint(model.layers[-1].output_shape)\nmodel.add(Activation('relu'))\nprint(model.layers[-1].output_shape)\nmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'))\nprint(model.layers[-1].output_shape)\n\n\"\"\"Block 3\"\"\"\nmodel.add(Conv2D(128, (3, 3), strides=(1,1), padding='same'))\nprint(model.layers[-1].output_shape)\nmodel.add(BatchNormalization())\nprint(model.layers[-1].output_shape)\nmodel.add(Conv2D(128, (3, 3), strides=(1,1), padding='same'))\nprint(model.layers[-1].output_shape)\nmodel.add(BatchNormalization())\nprint(model.layers[-1].output_shape)\nmodel.add(Activation('relu'))\nprint(model.layers[-1].output_shape)\nmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'))\nprint(model.layers[-1].output_shape)\n\n\"\"\"Block 4\"\"\"\nmodel.add(Conv2D(256, (3, 3), strides=(1,1), padding='same'))\nprint(model.layers[-1].output_shape)\nmodel.add(BatchNormalization())\nprint(model.layers[-1].output_shape)\nmodel.add(Conv2D(512, (3, 3), strides=(1,1), padding='same'))\nprint(model.layers[-1].output_shape)\nmodel.add(BatchNormalization())\nprint(model.layers[-1].output_shape)\nmodel.add(Activation('relu'))\nprint(model.layers[-1].output_shape)\nmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'))\nprint(model.layers[-1].output_shape)\n\n\"\"\"Block 5\"\"\"\nmodel.add(Flatten())\nprint(model.layers[-1].output_shape)\nmodel.add(Dense(4096))\nprint(model.layers[-1].output_shape)\nmodel.add(BatchNormalization())\nprint(model.layers[-1].output_shape)\nmodel.add(Activation('relu'))\nprint(model.layers[-1].output_shape)\n\n\"\"\"Block Test\"\"\"\nmodel.add(Dense(1024))\nprint(model.layers[-1].output_shape)\nmodel.add(BatchNormalization())\nprint(model.layers[-1].output_shape)\nmodel.add(Activation('relu'))\nprint(model.layers[-1].output_shape)\n\n\"\"\"Output Layer\"\"\"\nmodel.add(Dense(num_classes))\nprint(model.layers[-1].output_shape)\n\n\"\"\"Loss Layer\"\"\"\nmodel.add(Activation('softmax'))\nprint(model.layers[-1].output_shape)","7e0ddf2c":"val_data = pd.read_csv('.\/drive\/My Drive\/val\/val_annotations.txt', sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\nval_data.drop(['X', 'Y', 'H', 'W'], axis=1, inplace=True) \nval_data.head()","c175f066":"\"\"\"\n      For data augmentation \n\"\"\"\ndef CustomImageDataGen(input_img):\n  \"\"\"\n  Sometimes(0.5, ...) applies the given augmenter in 50% of all cases,\n  e.g. Sometimes(0.5, GaussianBlur(0.3)) would blur roughly every second\n  image.\n  \"\"\"\n  sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n  \n  seq = iaa.Sequential([\n      iaa.Fliplr(0.5), # horizontal flips\n      iaa.Flipud(0.2), # vertical flips\n      \n      # Small gaussian blur with random sigma between 0 and 0.5.\n      # But we only blur about 50% of all images.\n      sometimes(iaa.GaussianBlur(sigma=(0, 2.0))),\n      \n      # crop images by -10% to 20% of their height\/width\n      sometimes(iaa.CropAndPad(\n          percent=(-0.1, 0.2),\n          pad_mode=ia.ALL,\n          pad_cval=(0, 255)\n        )),\n      \n      # Apply affine transformations to some of the images\n      # - scale to 80-120% of image height\/width (each axis independently)\n      # - translate by -20 to +20 relative to height\/width (per axis)\n      # - rotate by -45 to +45 degrees\n      # - shear by -16 to +16 degrees\n      # - order: use nearest neighbour or bilinear interpolation (fast)\n      # - mode: use any available mode to fill newly created pixels\n      #         see API or scikit-image for which modes are available\n      # - cval: if the mode is constant, then use a random brightness\n      #         for the newly created pixels (e.g. sometimes black,\n      #         sometimes white)\n      sometimes(iaa.Affine(\n          scale={\"x\": (0.8, 1.5), \"y\": (0.8, 1.5)},\n          translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n          rotate=(-45, 45),\n          shear=(-16, 16),\n          order=[0, 1],\n          cval=(0, 255),\n          mode=ia.ALL\n      )),\n      \n      #drop 2-5% percent of the original size, leading to large dropped\n      # rectangles.\n      sometimes(iaa.CoarseDropout(\n                        (0.03, 0.15), size_percent=(0.02, 0.05),\n                        per_channel=0.2\n                    )),\n                \n      # Make some images brighter and some darker.\n      # In 20% of all cases, we sample the multiplier once per channel,\n      # which can end up changing the color of the images.\n      sometimes(iaa.Multiply((0.8, 1.2), per_channel=0.2)),\n      \n      #Improve or worsen the contrast of images.\n      #Comment it out after third model run (extreme saturation)\n      sometimes(iaa.ContrastNormalization((0.75, 1.5), per_channel=0.5)), \n     ],\n     # do all of the above augmentations in random order\n     random_order = True) # apply augmenters in random order\n  \n  output_img = seq.augment_image(input_img)\n  return output_img\n\n\"\"\"\n      rescale is a value by which we will multiply the data before any other \n      processing. Our original images consist in RGB coefficients in the 0-255, \n      but such values would be too high for our models to process \n      (given a typical learning rate), so we target values between 0 and 1 \n      instead by scaling with a 1\/255. factor.\n\"\"\"\ntrain_datagen = ImageDataGenerator(rescale=1\/255., preprocessing_function = CustomImageDataGen)\nvalid_datagen = ImageDataGenerator(rescale=1\/255.)","ba1a13cc":"train_generator = train_datagen.flow_from_directory( r'.\/drive\/My Drive\/train\/', \n                                                    target_size=(img_width, img_height), \n                                                    batch_size=batch_size, \n                                                    class_mode='categorical', \n                                                    shuffle=True, seed=101)","a77940f7":"validation_generator = valid_datagen.flow_from_dataframe(val_data, directory='.\/drive\/My Drive\/val\/images\/', \n                                                         x_col='File', y_col='Class', \n                                                         target_size=(img_width, img_height),\n                                                         class_mode='categorical', \n                                                         batch_size=batch_size, \n                                                         shuffle=False, seed=101)","3de0c74b":"\"\"\"\n      For visualizing the augmented images \n\"\"\"\nx_batch, y_batch = next(train_generator)\n\nfig = plt.figure(figsize=(10, 10))\n\nfor i in range(8):\n    sub = fig.add_subplot(2, 4, i + 1)\n    sub.imshow(x_batch[i,:,:], interpolation='bicubic')","56f48719":"model = Model(inputs=[ip], outputs=[output])\nmodel.summary()\n\ntf.keras.utils.plot_model(model, show_shapes=True)","4417f0c2":"\"\"\"for architecture 1 and VGG_16\n\"\"\"\n# Compile the Model\nmodel.compile(loss='categorical_crossentropy',\n              # optimizer= RMSprop(lr= 0.0001, epsilon=1e-08),\n              # optimizer= Adam(lr= 0.0001, epsilon=1e-08),\n              optimizer = SGD( learning_rate=0.001, momentum=0.9, nesterov=True),\n              metrics=['acc'])\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=10, verbose=1, min_lr=0.001) #on plateaus\n\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=20,\n                          verbose=1, mode='auto')\n\ncheckpoint = ModelCheckpoint(filepath=\".\/drive\/My Drive\/Network1_colab.hdf5\", verbose=1, save_best_only = False, monitor=\"val_loss\")\ncallbacks_list = [earlystop,reduce_lr,checkpoint]\n\nmodel = load_model(\".\/drive\/My Drive\/Network1_colab.hdf5\")\nmodel.fit_generator(train_generator,\n                    epochs = 75,\n                    steps_per_epoch= num_train \/\/ batch_size,\n                    validation_steps= num_validation \/\/ batch_size,\n                    validation_data=validation_generator,\n                    verbose=1, callbacks=callbacks_list\n                   )","2768c858":"\"\"\"for Architecture 2 and architecture 3\n\"\"\"\nreduce_lr = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10,\n                          verbose=1, mode='auto')\n\ncheckpoint = ModelCheckpoint(filepath=\".\/drive\/My Drive\/Network2_colab.hdf5\", verbose=1, save_best_only=False, monitor=\"val_loss\")\n\nmodel = load_model(\".\/drive\/My Drive\/Network2_colab.hdf5\" , custom_objects={'tf': tf})\nmodel.compile(loss='categorical_crossentropy',\n              # optimizer= Adam(lr= 0.001, epsilon=1e-08),\n              optimizer = SGD(learning_rate=0.001, momentum=0.9, nesterov=True),\n              metrics=['accuracy'])\ncallbacks_list = [earlystop,reduce_lr,checkpoint]\n\nmodel.fit_generator(train_generator, epochs=75, validation_data=validation_generator, verbose=1, callbacks=callbacks_list)","6de94dfb":"model = load_model(\".\/drive\/My Drive\/Network2_colab.hdf5\" , custom_objects={'tf': tf})","990d35eb":"model.fit_generator(train_generator, epochs=75, validation_data=validation_generator, verbose=1, callbacks=callbacks_list) # steps_per_epoch=3000, validation_steps=3000,","a95fd0dc":"\"\"\"Evaluating the trained model on validation data\"\"\"\nvalidation_steps_per_epoch = np.math.ceil(validation_generator.samples \/ validation_generator.batch_size)\nscore = model.evaluate_generator(validation_generator, verbose=1, steps=validation_steps_per_epoch)\nprint('Validation loss:', score[0])\nprint('Validation accuracy:', score[1])","f680c07a":"#Prediction based on validation data\npred=model.predict_generator(validation_generator, steps= np.ceil(num_validation\/batch_size), verbose=1)\npredicted_class_indices=np.argmax(pred,axis=1)\n# Predicted class indices of 1st 10 val images\npredicted_class_indices[:10]","c73c77e8":"# True class indices of 1st 10 validation images\nvalidation_generator.classes[:10]","bfc19bb2":"# Predicted classes from their indices\nlabels = (validation_generator.class_indices)\nlabels = dict((v,x) for x,v in labels.items())\npredictions = [labels[x] for x in predicted_class_indices]\npredictions[:10] #first 10 of them\n","0a273c66":"# Validation class names from words.txt\nclass_to_name = dict()\nfile = open('.\/drive\/My Drive\/words.txt','r')\ndata= file.readlines()\nfor line in data:\n  words = line.strip('\\n').split('\\t')\n  class_to_name[words[0]] = words[1].split(',')[0]\nfile.close()","d9bff586":"# Asserting Validation Class names from words.txt\nvalidation_class_names={}\nfor _class in validation_generator.class_indices.keys():\n  validation_class_names.update({_class : class_to_name[_class]})\n  \n# Classification Report of val classes\nprint(classification_report(validation_generator.classes, predicted_class_indices,\n                            #target_names=validation_generator.class_indices.keys(),\n                            target_names=validation_class_names.values(),\n                            digits=4))","376ccc39":"\"\"\" A list of all test images\"\"\"\nclasses=[]\ndata_dir = pathlib.Path('.\/drive\/My Drive\/train\/')\nclasses = sorted([item.name for item in data_dir.glob('*')])","79b078e8":"z=[]\nfile1 = open(r\".\/drive\/My Drive\/prediction_network2.txt\",\"w+\")\nfor file in glob.glob(\".\/drive\/My Drive\/test\/images\/*.JPEG\"):\n    images = [cv2.imread(file)]\n    processed_image=np.divide(images[:],255.0)\n    prediction_probs = model.predict(processed_image)\n    prediction = int(np.argmax(prediction_probs.reshape(-1)))\n    file1.write('{},{}\\n'.format(file[29:], classes[prediction]))\n    z.append(classes[prediction])\nfile1.close()","6d5e55a6":"Reading the contents of the file val_annotations.txt where entries are \nseparated by '\\t'. There are no column names but annotated using \n\n\n**names**=['File', 'Class', 'X', 'Y', 'H', 'W'] where File is the file name,\n\n**Class** is the true class label and 'X', 'Y', 'H', 'W' is for the four\nnumeric values","98f825af":"**Directory**: Directory where the data is located. If labels is \"inferred\",\nit should contain subdirectories, each containing images for a class. \n\n\n**Target_size**: Size to resize images to after they are read from disk.\n\n\n**Batch_size**: Size of the batches of data taken into account for an epoch.\n\n\n**Class_mode**: One of \"categorical\", \"binary\", \"sparse\", \"input\", or None. \n\n\n**Default**: \"categorical\". Determines the type of label arrays that are \nreturned: - \"categorical\" will be one-hot encoded labels \n\n**shuffle**: Whether to shuffle the data. Default: True. If set to False, sorts the data in alphanumeric order.\n\n**seed**: Optional random seed for shuffling and transformations."}}