{"cell_type":{"173aa9b0":"code","c95c9ce3":"code","14843b12":"code","764bee01":"code","ccc96356":"markdown","e9fbbd68":"markdown","3284e264":"markdown"},"source":{"173aa9b0":"#\u00a0Not necessary but you can get some logs.\n!wandb login 22f6b417e27a2c1fd23e5d45d687926b3f9e3b85","c95c9ce3":"import pandas as pd\n\n\ntrain_df = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/test.csv\")\n\ndf = pd.concat([test_df, train_df])\ndf.to_csv(\"train_test.csv\", index=False)","14843b12":"# If you want TPU.\n\"\"\"\n!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n!export XLA_USE_BF16=1\n\"\"\"","764bee01":"%%time\n!python ..\/input\/pytorchtransformers\/examples\/language-modeling\/run_language_modeling.py \\\n--output_dir=fine_tuned_roberta_update                                                     \\\n--model_type=roberta                                                                 \\\n--model_name_or_path=roberta-base                                                     \\\n--do_train                                                                             \\\n--train_data_file=train_test.csv                                                             \\\n--mlm \\\n--num_train_epochs 5","ccc96356":"Here, I start by loading the train and test, concatenating them and saving so that the file is available in a write folder. \nThis is necssary for the CLI to work since accessing from the input folder alone fails (it is read-only). As I said, running on the SquaD dataset (or using the squad roberta base https:\/\/huggingface.co\/deepset\/roberta-base-squad2) would have been better I guess. ","e9fbbd68":"So you need mainly need five things:\n    \n- where to store the fine-tuned model via the `output_dir` flag\n- The model type via the `model_type` flag: here `roberta`\n- The model base via the `model_name_or_path` flag: here `roberta-base` \n- The nature of the task via `--mlm` since it is masked language model here.\n- The train input via `train_data_file` flag: here `train_test.csv`","3284e264":"A short notebook where I show how to fine-tune the RoBERTa base model on the competition dataset using [transformers](https:\/\/huggingface.co\/transformers\/examples.html) CLI script. \nThis would have been better on the [SquaD](https:\/\/rajpurkar.github.io\/SQuAD-explorer\/) dataset but due to lack of time, didn't try it. Also, notice that one of the versions is running using TPU. \nEnjoy!"}}