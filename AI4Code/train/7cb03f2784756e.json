{"cell_type":{"d01cdf82":"code","39f25769":"code","cc33de86":"code","cdcfa529":"code","c2b5185b":"code","958c552e":"code","06ff15b5":"code","e3985e74":"code","bb2422a2":"code","1a16c92b":"code","076bcbc5":"code","73f272e5":"code","db4c82e0":"code","0f9ff331":"code","bdff4420":"code","2ecfa6a6":"code","3948f78c":"code","be7bc620":"code","4bd719f5":"code","3c428181":"code","64113f61":"code","28058cef":"code","89a38648":"code","0289bbd0":"code","1a3bf3ce":"code","0ac9cc02":"code","3b344701":"code","ac03849d":"code","f3c1fe77":"code","d2764982":"code","fa40dbe1":"code","50ac0888":"code","a901f398":"code","edcbb6d9":"code","e1389b59":"code","32acbcd7":"code","c30920ee":"markdown","b1f1d039":"markdown","58fc13cc":"markdown","40b81406":"markdown","b4d4f401":"markdown","061f9189":"markdown","d69e791b":"markdown","32596147":"markdown","07565b78":"markdown","4bc7b125":"markdown","e0bd08f3":"markdown","94a71c63":"markdown","15af821b":"markdown","9eb3e0f6":"markdown","c244326a":"markdown","eefaab26":"markdown","0e35b08b":"markdown","cf6cc760":"markdown","b13f88b8":"markdown","9cf28806":"markdown","de3550ea":"markdown","0b696be5":"markdown","f0bb0f32":"markdown","4c434b46":"markdown","03c25a1f":"markdown","955f9a46":"markdown","adb72597":"markdown","f9c4c280":"markdown","91855ef6":"markdown","0ba88100":"markdown"},"source":{"d01cdf82":"import random\nimport copy\nimport time\nimport pandas as pd\nimport numpy as np\nimport gc\nimport re\nimport torch\nfrom torchtext import data\nfrom tqdm import tqdm\nfrom tqdm import tqdm_notebook, tnrange\n\ntqdm.pandas(desc='Progress')\n\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom sklearn.metrics import f1_score\nimport torchtext\nimport os\n\nfrom multiprocessing import  Pool\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.optim.optimizer import Optimizer\n\nfrom functools import partial\nimport numpy as np","39f25769":"embed_size = 300 # Word vector dimension number\nmax_features = 120000 # Number of unique tokens to use\nmaxlen = 80 # maximum sequence length for LSTM\/GRU\nbatch_size = 512\nn_epochs = 5\nn_splits = 5 # K-Folds\n\ndebug = 0\n# debug = 0 runs full model on GPU, debug = 1 runs a part of it on CPU. Because Weekly GPU quota.\n\nnum_embeddings = 2 # number of embedding files used","cc33de86":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', \n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', \n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', \n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\n","cdcfa529":"mispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}","c2b5185b":"first_word_mispell_dict = {\n                'whta': 'what', 'howdo': 'how do', 'Whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', \n                'howmany': 'how many', 'whydo': 'why do', 'doi': 'do i', 'howdoes': 'how does', \"whst\": 'what', \n                'shoupd': 'should', 'whats': 'what is', \"im\": \"i am\", \"whatis\": \"what is\", \"iam\": \"i am\", \"wat\": \"what\",\n                \"wht\": \"what\",\"whts\": \"what is\", \"whtwh\": \"what\", \"whtat\": \"what\", \"whtlat\": \"what\", \"dueto to\": \"due to\",\n                \"dose\": \"does\", \"wha\": \"what\", 'hw': \"how\", \"its\": \"it is\", \"whay\": \"what\", \"ho\": \"how\", \"whart\": \"what\", \n                \"woe\": \"wow\", \"wt\": \"what\", \"ive\": \"i have\",\"wha\": \"what\", \"wich\": \"which\", \"whic\": \"which\", \"whys\": \"why\", \n                \"doe\": \"does\", \"wjy\": \"why\", \"wgat\": \"what\", \"hiw\": \"how\",\"howto\": \"how to\", \"lets\": \"let us\", \"haw\": \"how\", \n                \"witch\": \"which\", \"wy\": \"why\", \"girlfriend\": \"girl friend\", \"hows\": \"how is\",\"whyis\": \"why is\", \"whois\": \"who is\",\n                \"dont\": \"do not\", \"hat\": \"what\", \"whos\": \"who is\", \"whydoes\": \"why does\", \"whic\": \"which\",\"hy\": \"why\", \"w? hy\": \"why\",\n                \"ehat\": \"what\", \"whate\": \"what\", \"whai\": \"what\", \"whichis\": \"which is\", \"whi\": \"which\", \"isit\": \"is it\",\"ca\": \"can\", \n                \"wwhat\": \"what\", \"wil\": \"will\", \"wath\": \"what\", \"plz\": \"please\", \"ww\": \"how\", \"hou\": \"how\", \"whch\": \"which\",\n                \"ihave\": \"i have\", \"cn\": \"can\", \"doesnt\": \"does not\", \"shoul\": \"should\", \"whatdo\": \"what do\", \"isnt\": \"is not\", \n                \"whare\": \"what are\",\"whick\": \"which\", \"whatdoes\": \"what does\", \"hwo\": \"how\", \"howdid\": \"how did\", \"why dose\": \"why does\"\n}","958c552e":"def clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        if punct in x:\n            x = x.replace(punct, f' {punct} ')\n    return x\n\n\ndef clean_numbers(x):\n    if bool(re.search(r'\\d', x)):\n        x = re.sub('[0-9]{5,}', '#####', x)\n        x = re.sub('[0-9]{4}', '####', x)\n        x = re.sub('[0-9]{3}', '###', x)\n        x = re.sub('[0-9]{2}', '##', x)\n    return x\n","06ff15b5":"def _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n    return mispellings_re.sub(replace, text)\n\ndef correct_first_word(x):\n    for key in first_word_mispell_dict.keys():\n        if x.startswith(key + \" \"):\n            x = x.replace(key + \" \", first_word_mispell_dict[key] + \" \")\n            break\n    return x","e3985e74":"def add_features(df):\n    df['question_text'] = df['question_text'].progress_apply(lambda x:str(x))\n    df[\"lower_question_text\"] = df[\"question_text\"].apply(lambda x: x.lower())\n    df['total_length'] = df['question_text'].progress_apply(len)\n    df['capitals'] = df['question_text'].progress_apply(lambda comment: sum(1 for c in comment if c.isupper()))\n    df['caps_vs_length'] = df.progress_apply(lambda row: float(row['capitals'])\/float(row['total_length']),\n                                axis=1)\n    df['num_words'] = df.question_text.str.count('\\S+')\n    df['num_unique_words'] = df['question_text'].progress_apply(lambda comment: len(set(w for w in comment.split())))\n    df['words_vs_unique'] = df['num_unique_words'] \/ df['num_words'] \n    return df","bb2422a2":"def parallelize_apply(df,func,colname,num_process,newcolnames):\n    pool =Pool(processes=num_process)\n    arraydata = pool.map(func,tqdm(df[colname].values))\n    pool.close()\n    \n    newdf = pd.DataFrame(arraydata,columns = newcolnames)\n    df = pd.concat([df,newdf],axis=1)\n    return df\n\ndef parallelize_dataframe(df, func):\n    df_split = np.array_split(df, 4)\n    pool = Pool(4)\n    df = pd.concat(pool.map(func, df_split))\n    pool.close()\n    pool.join()\n    return df","1a16c92b":"if debug:\n    train_df = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/train.csv\")[:800]\n    test_df = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/test.csv\")[:200]\nelse:\n    train_df = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/train.csv\")\n    test_df = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/test.csv\")\nprint(\"Train shape : \",train_df.shape)\nprint(\"Test shape : \",test_df.shape)","076bcbc5":"train = parallelize_dataframe(train_df, add_features)\ntest = parallelize_dataframe(test_df, add_features)","73f272e5":"train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: x.lower())\ntest_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: x.lower())","db4c82e0":"train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_text(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: clean_text(x))","0f9ff331":"train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: clean_numbers(x))","bdff4420":"train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))","2ecfa6a6":"train_X = train_df[\"question_text\"].fillna(\"_##_\").values\ntest_X = test_df[\"question_text\"].fillna(\"_##_\").values","3948f78c":"def tokenize_and_split(train_X,test_X):\n    tokenizer = Tokenizer(num_words=max_features,oov_token = 'xxunk',filters='')\n    tokenizer.fit_on_texts(list(train_X))\n    train_X = tokenizer.texts_to_sequences(train_X)\n    test_X = tokenizer.texts_to_sequences(test_X)\n\n    train_X = pad_sequences(train_X, maxlen=maxlen)\n    test_X = pad_sequences(test_X, maxlen=maxlen)\n\n    train_y = train_df['target'].values\n \n    trn_idx = np.random.permutation(len(train_X))\n\n    train_X = train_X[trn_idx]\n    train_y = train_y[trn_idx]\n    \n    features, test_features = make_stat_features()\n    train_features = features[trn_idx]\n    \n    return train_X, test_X, train_y, train_features, test_features ,tokenizer.word_index","be7bc620":"def make_stat_features():\n    \n    train_features = train[['num_unique_words','words_vs_unique','total_length','capitals',\n                      'caps_vs_length','num_words']].fillna(0)\n    test_features = test[['num_unique_words','words_vs_unique','total_length','capitals',\n                      'caps_vs_length','num_words']].fillna(0)\n    \n    ss = StandardScaler()\n    ss.fit(np.vstack((train_features, test_features)))\n    train_features = ss.transform(train_features)\n    test_features = ss.transform(test_features)\n    \n    return train_features, test_features","4bd719f5":"x_train, x_test, y_train, train_features, test_features , word_index = tokenize_and_split(train_X,test_X) ","3c428181":"def load_embedding(path, word_index,emb_mean, emb_std):\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(path, encoding=\"utf8\", errors='ignore') if len(o)>100)\n\n    all_embs = np.stack(embeddings_index.values())\n    embed_size = all_embs.shape[1]\n\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n    \n    return embedding_matrix","64113f61":"if debug:\n    paragram_embeddings = np.random.randn(120000,300)\n    glove_embeddings = np.random.randn(120000,300)\nelse:\n    glove_embeddings = load_embedding('..\/input\/quora-insincere-questions-classification\/embeddings\/glove.840B.300d\/glove.840B.300d.txt',word_index,-0.005838499,0.48782197)    \n    paragram_embeddings = load_embedding('..\/input\/quora-insincere-questions-classification\/embeddings\/paragram_300_sl999\/paragram_300_sl999.txt',word_index,-0.0053247833,0.49346462)\n\nembedding_matrix = np.concatenate((glove_embeddings, paragram_embeddings), axis=1)          ","28058cef":"class AttentionBlock(nn.Module):\n    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n        super(AttentionBlock, self).__init__(**kwargs)\n        self.supports_masking = True\n\n        self.bias = bias\n        self.feature_dim = feature_dim\n        self.step_dim = step_dim\n        self.features_dim = 0\n        \n        weight = torch.zeros(feature_dim, 1)\n        nn.init.xavier_uniform_(weight)\n        self.weight = nn.Parameter(weight)\n        \n        if bias:\n            self.b = nn.Parameter(torch.zeros(step_dim))\n        \n    def forward(self, x, mask=None):\n        feature_dim = self.feature_dim\n        step_dim = self.step_dim\n\n        eij = torch.mm(\n            x.contiguous().view(-1, feature_dim), \n            self.weight\n        ).view(-1, step_dim)\n        \n        if self.bias:\n            eij = eij + self.b\n            \n        eij = torch.tanh(eij)\n        a = torch.exp(eij)\n        \n        if mask is not None:\n            a = a * mask\n\n        a = a \/ torch.sum(a, 1, keepdim=True) + 1e-10\n\n        weighted_input = x * torch.unsqueeze(a, -1)\n        return torch.sum(weighted_input, 1)","89a38648":"class EmbeddingDropout(nn.Module):\n    def __init__(self, embedding_matrix, max_features = 120000, embedding_size = 300):\n        super(EmbeddingDropout,self).__init__()\n        self.embedding = nn.Embedding(max_features, embedding_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        \n        self.embedding_drop = nn.Dropout2d(0.1)\n    \n    def forward(self,x):\n        h_embeddings = self.embedding(x)\n        h_embeddings = h_embeddings.unsqueeze(0)\n        h_embeddings = h_embeddings.permute(1,3,0,2)\n        h_embeddings = self.embedding_drop(h_embeddings) \n        h_embeddings = h_embeddings.permute(2,0,3,1)\n        h_embeddings = h_embeddings.squeeze(0)\n\n        return h_embeddings","0289bbd0":"class Body(nn.Module):\n    def __init__ (self, embedding_size= 300, hidden_size= 128):\n        super(Body,self).__init__()\n        self.lstm = nn.LSTM(embedding_size*num_embeddings, hidden_size, bidirectional=True, batch_first=True)\n        self.gru = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True)\n        self.hidden= hidden_size\n        \n        for name, param in self.lstm.named_parameters():\n            if 'bias' in name:\n                nn.init.constant_(param, 0.0)\n            elif 'weight_ih' in name:\n                nn.init.kaiming_normal_(param)\n            elif 'weight_hh' in name:\n                nn.init.orthogonal_(param)\n        \n        for name, param in self.gru.named_parameters():\n            if 'bias' in name:\n                nn.init.constant_(param, 0.0)\n            elif 'weight_ih' in name:\n                nn.init.kaiming_normal_(param)\n            elif 'weight_hh' in name:\n                nn.init.orthogonal_(param)\n        \n        self.lstm_attention = AttentionBlock(hidden_size*2, maxlen)\n        self.gru_attention = AttentionBlock(hidden_size*2, maxlen)\n   \n    def forward(self,x):\n        h_lstm, _ = self.lstm(x)\n        h_gru, _ = self.gru(h_lstm)\n        \n        h_lstm_att = self.lstm_attention(h_lstm)\n        h_gru_att = self.gru_attention(h_gru)\n\n        return h_lstm_att, h_gru_att, h_lstm, h_gru","1a3bf3ce":"class Extractor(nn.Module):\n    def __init__(self,maxlen= 70,hidden_size= 128,out= 64,len_feats= 6):\n        super(Extractor,self).__init__()\n        self.conv = nn.Conv1d(maxlen,out,kernel_size= 1,stride= 2)\n        self.stat = nn.Linear(len_feats,hidden_size)\n        \n    def forward(self,h_lstm,h_gru,stat_features):\n        conv_out = self.conv(h_lstm)\n        l_maxpool, _ = torch.max(conv_out,1)\n        g_avgpool = torch.mean(h_gru,1)\n        statistical_features = self.stat(stat_features)\n        \n        return l_maxpool, g_avgpool, statistical_features   ","0ac9cc02":"class Head(nn.Module):\n    def __init__(self,embedding_size= 300,intermediate_layer=64,maxlen=70,hidden_size=128):\n        super(Head,self).__init__()\n        self.linear = nn.Linear(hidden_size*8,intermediate_layer)\n        self.dropout = nn.Dropout(0.15)\n        self.bn = nn.BatchNorm1d(intermediate_layer)\n        self.output = nn.Linear(intermediate_layer, 1)\n        \n    def forward(self,x):\n        return self.output(self.bn(self.dropout(self.linear(x))))","3b344701":"class NeuralNet(nn.Module):\n    def __init__(self, embedding_matrix,max_features=120000,embedding_size=300,\n                 maxlen=70, hidden_size=128, len_feats = 6):\n        super(NeuralNet,self).__init__()\n        self.embedding = EmbeddingDropout(embedding_matrix,max_features= max_features,embedding_size= embedding_size)\n        self.stem = Body(embedding_size,hidden_size)\n        self.extractor = Extractor(maxlen= maxlen,hidden_size= hidden_size, out= 64, len_feats= len_feats)\n        self.head = Head(embedding_size= embedding_size,maxlen= maxlen,hidden_size = hidden_size)\n    \n    def forward(self,x,stat_features):\n        embedding_output = self.embedding(x)\n        h_lstm_att, h_gru_att, h_lstm, h_gru = self.stem(embedding_output)\n        l_maxpool, g_avgpool, extracted_feats = self.extractor(h_lstm, h_gru, stat_features)\n        features = torch.cat((h_lstm_att, h_gru_att, l_maxpool, g_avgpool, extracted_feats),1)\n        out = self.head(features)\n        \n        return out","ac03849d":"loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum')\nstep_size = 300","f3c1fe77":"class MyDataset(Dataset):\n    def __init__(self,dataset):\n        self.dataset = dataset\n    def __getitem__(self,index):\n        data,target = self.dataset[index]\n        return data,target,index\n    def __len__(self):\n        return len(self.dataset)","d2764982":"def sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\nif debug :\n    x_test_cuda = torch.tensor(x_test, dtype=torch.long)\nelse :\n    x_test_cuda = torch.tensor(x_test, dtype=torch.long).cuda()\n\ntest = torch.utils.data.TensorDataset(x_test_cuda)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)","fa40dbe1":"def kfold_train(x_train,y_train,x_test, model_obj, train_features, test_features,clip = True):\n    avg_losses_f = []\n    avg_val_losses_f = []\n    train_preds = np.zeros((len(x_train)))\n    test_preds = np.zeros((len(x_test)))\n\n    splits = list(StratifiedKFold(n_splits=n_splits, shuffle=True).split(x_train, y_train))\n\n    for i, (train_idx, valid_idx) in tqdm(enumerate(splits)):\n        x_train = np.array(x_train)\n        y_train = np.array(y_train)\n\n        x_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.long).cuda()\n        y_train_fold = torch.tensor(y_train[train_idx.astype(int), np.newaxis], dtype=torch.float32).cuda()\n        x_val_fold = torch.tensor(x_train[valid_idx.astype(int)], dtype=torch.long).cuda()\n        y_val_fold = torch.tensor(y_train[valid_idx.astype(int), np.newaxis], dtype=torch.float32).cuda()\n        \n        feature_train_fold = train_features[train_idx.astype(int)]\n        feature_valid_fold = train_features[valid_idx.astype(int)]\n        \n        model = copy.deepcopy(model_obj)\n        model.cuda()\n\n        train_ds = MyDataset(torch.utils.data.TensorDataset(x_train_fold, y_train_fold))\n        valid_ds = MyDataset(torch.utils.data.TensorDataset(x_val_fold, y_val_fold))\n        \n        train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n        valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=batch_size, shuffle=False)\n\n        optimizer = torch.optim.Adam(model.parameters())\n        scheduler = optim.lr_scheduler.ExponentialLR(optimizer,0.01)\n\n        print('Fold %d'%i)\n        for epoch in tqdm(range(n_epochs)):\n            start_time = time.time()\n            avg_loss = 0.\n            model.train()\n\n            for i, (x_batch, y_batch, index) in enumerate(train_dl):\n                y_pred = model(x_batch,torch.FloatTensor(feature_train_fold[index]).cuda())\n\n                loss = loss_fn(y_pred, y_batch)\n                optimizer.zero_grad()\n                loss.backward()\n\n                if clip:\n                    nn.utils.clip_grad_norm_(model.parameters(),1)\n                optimizer.step()\n                avg_loss += loss.item() \/ len(train_dl)\n                \n            model.eval()\n            \n            valid_preds_fold = np.zeros((x_val_fold.size(0)))\n            test_preds_fold = np.zeros((len(x_test)))\n            \n            avg_val_loss = 0.\n            for i, (x_batch, y_batch,index) in enumerate(valid_dl):\n\n                y_pred = model(x_batch,torch.FloatTensor(feature_valid_fold[index]).cuda()).detach()\n                \n                avg_val_loss += loss_fn(y_pred, y_batch).item() \/ len(valid_dl)\n                valid_preds_fold[index] = sigmoid(y_pred.cpu().numpy())[:, 0]\n            \n            elapsed_time = time.time() - start_time \n\n            print('Epoch {}\/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'\n                  .format(epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))\n        \n        avg_losses_f.append(avg_loss)\n        avg_val_losses_f.append(avg_val_loss) \n\n        for i, (x_batch,) in enumerate(test_loader):\n            y_pred = model(x_batch,torch.FloatTensor(test_features[i * batch_size:(i+1) * batch_size]).cuda())\n            test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.detach().cpu().numpy())[:, 0] \n        \n        train_preds[valid_idx] = valid_preds_fold\n        test_preds += test_preds_fold \/ len(splits)\n\n    print('All \\t loss={:.4f} \\t val_loss={:.4f} \\t '.format(np.average(avg_losses_f),np.average(avg_val_losses_f)))\n    return train_preds, test_preds","50ac0888":"def cpu_kfold_train(x_train,y_train,x_test, model_obj, train_features, test_features,clip = True):\n    avg_losses_f = []\n    avg_val_losses_f = []\n\n    train_preds = np.zeros((len(x_train)))\n    test_preds = np.zeros((len(x_test)))\n\n    splits = list(StratifiedKFold(n_splits=n_splits, shuffle=True).split(x_train, y_train))\n\n    for i, (train_idx, valid_idx) in tqdm(enumerate(splits)):\n        x_train = np.array(x_train)\n        y_train = np.array(y_train)\n\n        x_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.long)\n        y_train_fold = torch.tensor(y_train[train_idx.astype(int), np.newaxis], dtype=torch.float32)\n        x_val_fold = torch.tensor(x_train[valid_idx.astype(int)], dtype=torch.long)\n        y_val_fold = torch.tensor(y_train[valid_idx.astype(int), np.newaxis], dtype=torch.float32)\n        \n        feature_train_fold = train_features[train_idx.astype(int)]\n        feature_valid_fold = train_features[valid_idx.astype(int)]\n        \n        model = copy.deepcopy(model_obj)\n\n        train_ds = MyDataset(torch.utils.data.TensorDataset(x_train_fold, y_train_fold))\n        valid_ds = MyDataset(torch.utils.data.TensorDataset(x_val_fold, y_val_fold))\n        \n        train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n        valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=batch_size, shuffle=False)\n\n        optimizer = torch.optim.Adam(model.parameters())\n        scheduler = optim.lr_scheduler.ExponentialLR(optimizer,0.01)\n\n        print(f'Fold {i + 1}')\n        for epoch in tqdm(range(n_epochs)):\n            start_time = time.time()\n            avg_loss = 0.\n            model.train()\n\n            for i, (x_batch, y_batch, index) in tqdm(enumerate(train_dl)):\n                y_pred = model(x_batch,torch.FloatTensor(feature_train_fold[index]))\n\n                loss = loss_fn(y_pred, y_batch)\n                optimizer.zero_grad()\n                loss.backward()\n\n                if clip:\n                    nn.utils.clip_grad_norm_(model.parameters(),1)\n                optimizer.step()\n                avg_loss += loss.item() \/ len(train_dl)\n                \n            model.eval()\n            \n            valid_preds_fold = np.zeros((x_val_fold.size(0)))\n            test_preds_fold = np.zeros((len(x_test)))\n            \n            avg_val_loss = 0.\n            for i, (x_batch, y_batch,index) in tqdm(enumerate(valid_dl)):\n\n                y_pred = model(x_batch,torch.FloatTensor(feature_valid_fold[index])).detach()\n                \n                avg_val_loss += loss_fn(y_pred, y_batch).item() \/ len(valid_dl)\n                valid_preds_fold[index] = sigmoid(y_pred.detach().numpy())[:, 0]\n            \n            elapsed_time = time.time() - start_time \n\n            print('Epoch {}\/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))\n        \n        avg_losses_f.append(avg_loss)\n        avg_val_losses_f.append(avg_val_loss) \n\n        for i, (x_batch,) in tqdm(enumerate(test_loader)):\n            y_pred = model(x_batch,torch.FloatTensor(test_features[i * batch_size:(i+1) * batch_size]))\n            test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.detach().numpy())[:, 0] \n        \n        train_preds[valid_idx] = valid_preds_fold\n        test_preds += test_preds_fold \/ len(splits)\n\n    print('All \\t loss={:.4f} \\t val_loss={:.4f} \\t '.format(np.average(avg_losses_f),np.average(avg_val_losses_f)))\n    return train_preds, test_preds","a901f398":"model = NeuralNet(embedding_matrix,maxlen=maxlen,max_features=len(embedding_matrix))","edcbb6d9":"if debug:\n    train_preds, test_preds = cpu_kfold_train(x_train,y_train,x_test,model,train_features,test_features)\nelse :\n    train_preds, test_preds = kfold_train(x_train,y_train,x_test,model,train_features,test_features)","e1389b59":"def findthreshold (y_train, train_preds):\n    best_f1= 0\n    best_thresh= 0\n    for thresh in tqdm(np.arange(0.1,0.501,0.01)):\n        f1 = f1_score(y_train,(train_preds>thresh))\n        if f1 > best_f1:\n            best_thresh = thresh\n            best_f1 = f1\n    print('best threshold is {:.4f} with F1 score: {:.4f}'.format(best_thresh, best_f1))\n    return best_thresh, best_f1\n\nthreshold, score = findthreshold(y_train,train_preds)","32acbcd7":"if debug:\n    df_test = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/test.csv\")[:200]\nelse:\n    df_test = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/test.csv\")\nsubmission = df_test[['qid']].copy()\nsubmission['prediction'] = (test_preds > threshold).astype(int)\nsubmission.to_csv('submission.csv', index=False)","c30920ee":"## Tokenization\n\nI have used keras tokenizer here. All out of vocabulary words will be replaced with special 'xxunk' token.\nTokenizer is fit on the train data and used to make sequence of texts. Then the sequences are padded so all of them have the same length. Longer sequences are truncated and shorter ones are padded.\n\nLSTM\/GRU do not like uneven sequence lengths.","b1f1d039":"## Basic Parameters","58fc13cc":"### Custom dataset\n\nA custom dataset had to be used to keep track of the indices.","40b81406":"#### GPU Loop","b4d4f401":"### Helper Functions","061f9189":"### Complete Model\n\n1. First input is sent through embedding layer, returning feature vectors.\n2. Embedding output is then passed to the Body of the model.\n3. Extractor layer is used to find features from the sequential output of the model backbone.\n4. Attention infused lstm+gru outputs and extracted features are concatenated.\n5. Concatenated features are sent to the output layer.","d69e791b":"### Training Loop\n\nUsing Stratified K fold Cross Validation. Works as an ensemble.","32596147":"#### CPU Loop\n\nFor debugging without GPU","07565b78":"## Submit","4bc7b125":"## Imports","e0bd08f3":"#### Cleaning Text and Numbers","94a71c63":"### Punctuation and Misspelled Word Dictionary","15af821b":"### Choosing the best threshold for classification","9eb3e0f6":"### Output Layer\n\nThis layer consists of a dropout, a relu and a batchnorm layer, sandwiched between two linear layers.","c244326a":"## Training","eefaab26":"## Loading Embeddings\n\n1st place solution uses **0.7 \\* glove + 0.3 \\* paragram** matrix for its embedding layer.\n\nIn my reasoning, as these word embeddings were not trained in exactly the same system, some useful information may be lost if a weighted average is used. That is why, I have concatenated them into a **num_words \\* (embedding_dim \\* num_embedding)** matrix.","0e35b08b":"### Embedding and Dropout Layer\n\nMany public kernels have wrongly applied spatial droput on the batch dimension as discussed [here](https:\/\/www.kaggle.com\/c\/quora-insincere-questions-classification\/discussion\/79911).\n\nSome reshaping was done in order to apply correct dropout on the correct dimension, i.e. feature.","cf6cc760":"#### Parallel processing code for speedup","b13f88b8":"## Model Definition","9cf28806":"Some submissions use seeding methods for derterministic results, essentially treating them as hyperparameters, which is a bad practise. \n\nAs Psi quoted :\n> Seeing people setting the seed as a hyperparameter is weird.\n\nI decided not to use any seeds based on his recommendation.","de3550ea":"#### Engineering Statistical features\nI have extracted these statistical features from the train data.\n1. Total length \n2. Capitals\n3. Ratio of capitals and length\t\n4. Number of words\t\n5. Number of unique words\t\n6. Ratio of unique words and total words","0b696be5":"### Attention Module\n\nApplying attention on LSTM and GRU outputs should boost model performance.","f0bb0f32":"### Loading Text and Applying preprocessing","4c434b46":"### Extractor Layer\nThis layer serves as the feature extractor for lstm, gru outputs and statistical features.\n\nConv1d is applied on lstm output, which goes into a maxpool.\nGRU output is sent through avgpool.\nStatistical Features are put through a Linear layer","03c25a1f":"#### Fixing misspellings","955f9a46":"### Scaling Statistical Features\n\nCalculate Statistical Features will have to be scaled before putting through a neuralnet.\n\nAs because these features have different ranges for values, some features may produce vanishing gradients. For example number of words will be, on average, a way larger value than number of unique words. So, these feaures are scaled using a standard scaler.","adb72597":"## Overview\n\nThe following Figure illustrates my final model.\n![QuoraDiagram%20%281%29.jpeg](attachment:QuoraDiagram%20%281%29.jpeg)","f9c4c280":"## Preface\nThis kernel contains my final submission for the Quora Insincere Questions Classification Competition.\nI came up with this model from discussions and public kernels. The model itself (loosely) resembles the 1st place solution from this discussion [thread](https:\/\/www.kaggle.com\/c\/quora-insincere-questions-classification\/discussion\/80568) by Psi. I took the liberty to integrate Attention and GRU in an attempt to enhance the model.\n\nCurrently this kernel stands at:\n* Private LB : 0.7056 (ish)\n* Public LB : 0.699 (ish)\n\nMy initial plan was to train an off-the-shelf ULMFiT language model from FastAI and then fine tuning it for this competition. But, I found a couple of flaws in the plan:\n1. ULMFiT training was taking longer than the kernel limit.\n2. It was giving abysmal results. A simple BiLSTM with embeddings outperformed ULMFiT without embeddings.\n3. It needed major changes to the architecture to use word embeddings provided by the competition.\n4. Apparently very deep networks do not perform well in this competition.\n\nSo, I believe it is wiser to design a model from scratch.","91855ef6":"### Backbone of the model\n\nThe main body of the model consists of LSTM and GRU (128).\n\nThe body returns LSTM+GRU outputs and Attention infused  LSTM+GRU outputs.","0ba88100":"## Text Preprocessing and Loading\nFirst, I needed to learn pre-processing for using word embeddings. Dieters [kernel](https:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-when-using-embeddings) was a godsend for me.\n\nI also borrowed some helper functions from this [kernel](https:\/\/www.kaggle.com\/theoviel\/improve-your-score-with-some-text-preprocessing)."}}