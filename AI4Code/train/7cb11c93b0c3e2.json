{"cell_type":{"fb0df1cd":"code","d8b7a3c4":"code","28c46d2c":"code","7733d50e":"code","7a8095db":"code","605fc5df":"code","a9896fbe":"code","5826559c":"code","eee6ef8a":"code","cdf2ee80":"code","aa295b05":"code","a497559e":"code","f99d86a3":"code","b1896e2d":"code","419b0142":"code","bb985abf":"code","8ff7b03a":"code","2a1a1136":"code","46c76275":"code","56f98538":"code","892884b1":"code","505ea58f":"code","5b03fdaf":"code","8c239811":"code","6d8c0252":"markdown","206b2079":"markdown","321ba11c":"markdown","db5583be":"markdown","bdc8732b":"markdown"},"source":{"fb0df1cd":"import pandas as pd\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport matplotlib.pyplot as plt\n","d8b7a3c4":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\nprint('Shape of dataset:',str(df.shape))\ndf.head()","28c46d2c":"df.describe().T","7733d50e":"#Since the features from V1~V28 are generated from PCA dimensionality so they're already scaled\n# we have to scale both Time and Amount\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nstd_scaler = StandardScaler()\nrob_scaler = RobustScaler()\n\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\ndf.drop(['Time','Amount'], axis=1, inplace=True)\n\n#Inserting them at start of data\nscaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\n\ndf.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)\ndf.insert(1, 'scaled_time', scaled_time)","7a8095db":"df.head()","605fc5df":"# 1--> 492 , 0-->283415\n# Very Imbalanced data\nprint(df['Class'].value_counts())\nsns.countplot('Class',data=df)","a9896fbe":"# we will use downsampling method. \n# i will split the data into smaller parts (=492 rows)\nclass_0_df = df[df['Class']==0]\nclass_1_df = df[df['Class']==1]\n\nclass_1_df.shape","5826559c":"#Now i will take the class_0_df and split it randomly in small parts each equal to 492 rows\n# we first shuffle it\nclass_0_df = class_0_df.sample(frac = 1, random_state = 42)\nclass_0_df_subsample = class_0_df.loc[:][:492]\n\nclass_0_df_subsample.shape","eee6ef8a":"final_df = pd.concat([class_0_df_subsample,class_1_df])\nfinal_df = final_df.sample(frac = 1, random_state = 42)","cdf2ee80":"sns.countplot('Class',data=final_df)","aa295b05":"from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, plot_roc_curve, classification_report\nfrom  sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nX = final_df.drop(columns=['Class'])\nY = final_df['Class']\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)","a497559e":"model_lr = LogisticRegression(max_iter=120,random_state=0, n_jobs=20, solver='liblinear')\nmodel_lr.fit(X_train, y_train)","f99d86a3":"trn_lr_pred = model_lr.predict(X_train)\ntrn_lr_acc = accuracy_score(trn_lr_pred, y_train)\nprint(round(trn_lr_acc*100, 2))","b1896e2d":"tst_lr_pred = model_lr.predict(X_test)\ntst_lr_acc = accuracy_score(tst_lr_pred, y_test)\nprint(round(tst_lr_acc*100, 2))","419b0142":"#Plotting ROC\nfig, ax = plt.subplots(figsize=(6, 4))\nplot_roc_curve(model_lr, X_test, y_test, color='blue', ax=ax)","bb985abf":"from imblearn.over_sampling import SMOTE\n","8ff7b03a":"X_for_sm = df.drop(columns='Class', axis=1)\ny_for_sm = df['Class']\nX_for_sm.shape, y_for_sm.shape","2a1a1136":"sm = SMOTE(sampling_strategy='auto', random_state=42, k_neighbors=4)\nX_oversampled , y_oversampled = sm.fit_resample(X_for_sm, y_for_sm)\nX_oversampled.shape, y_oversampled.shape","46c76275":"sns.countplot(y_oversampled)","56f98538":"X_train_sm, X_test_sm, y_train_sm, y_test_sm = train_test_split(X_oversampled, y_oversampled, test_size=0.33, random_state=42)\nmodel_lr_smt = LogisticRegression(solver='liblinear')\nmodel_lr_smt.fit(X_train_sm, y_train_sm)","892884b1":"trn_lr_smt_pred = model_lr.predict(X_train_sm)\ntrn_lr_smt_acc = accuracy_score(trn_lr_smt_pred, y_train_sm)\nprint(round(trn_lr_smt_acc*100, 2))","505ea58f":"tst_lr_smt_pred = model_lr.predict(X_test_sm)\ntst_lr_smt_acc = accuracy_score(tst_lr_smt_pred, y_test_sm)\nprint(round(tst_lr_smt_acc*100, 2))","5b03fdaf":"fig, ax = plt.subplots(figsize=(6, 4))\nplot_roc_curve(model_lr_smt, X_test_sm, y_test_sm, color='darkgreen', ax=ax)","8c239811":"fig, ax = plt.subplots(figsize=(6, 4))\nplot_roc_curve(model_lr_smt, X_test_sm, y_test_sm, color='darkgreen', ax=ax,label='SMOTE')\nplot_roc_curve(model_lr, X_test, y_test, color='blue', ax=ax,label='Down-Sampling')","6d8c0252":"## Using SMOTE for imbalanced Data","206b2079":"## Final Comparison between Downsampling and SMOTE","321ba11c":"***So we have: ***\n* Time Column --> Number of seconds passed after the first transaction occurs\n* V1 ~ V28 \n* Amount --> amount of transaction\n* Class --> 1 for fraud, 0 otherwise","db5583be":"Finally, we can see that using SMOTE will give a better results","bdc8732b":"## Using Downsampling Method"}}