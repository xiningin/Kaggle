{"cell_type":{"3cfe5f0a":"code","4e16a928":"code","46f7fad0":"code","a8544f9a":"code","071831af":"code","2b1d2527":"code","8bbd7d3e":"code","541ee67a":"code","30395940":"code","7157dcfd":"code","c0a14e79":"code","7fd731ad":"code","1e9151d7":"code","3000160f":"code","9845414e":"code","3756ecfc":"code","5a3c1533":"code","2eae3905":"code","627a1adb":"code","9a905937":"code","a95bddaf":"code","fafa1e9b":"code","fe500581":"code","435b40fd":"code","4f9aacaf":"code","834f020e":"code","a07992b7":"code","2dc9969d":"code","dee82a48":"code","f6be903a":"code","d380a52b":"code","317a0274":"code","6f80b85d":"code","a39152be":"code","5bd927c3":"code","0e4a73be":"code","4d28233b":"code","182672de":"code","a17b7989":"code","de21a695":"code","b1b5fb82":"code","545de679":"code","8e47c0a1":"code","ef28bad6":"code","624c8f42":"markdown","77293463":"markdown","7e907dcb":"markdown","5935e42b":"markdown","27805c08":"markdown","b94922d2":"markdown","ed7285ed":"markdown","5a628247":"markdown","7d2a6528":"markdown","af3a86f7":"markdown","b44de5d8":"markdown","afe692a2":"markdown","068a47ab":"markdown","7ea1c75b":"markdown","4a7b6e54":"markdown","820f942c":"markdown","6bb7f37f":"markdown"},"source":{"3cfe5f0a":"import pandas as pd","4e16a928":"data = pd.read_csv('..\/input\/sel_hn_stories.csv', header = None, parse_dates=[0])","46f7fad0":"data.head()","a8544f9a":"columns = [\"submission_time\", \"upvotes\", \"url\", \"headline\"]","071831af":"data.columns = columns","2b1d2527":"data.head()","8bbd7d3e":"data.shape","541ee67a":"data.isnull().sum()","30395940":"data.info()","7157dcfd":"data.dropna(inplace=True)","c0a14e79":"data.shape","7fd731ad":"token_data = []","1e9151d7":"#let us split the strings on the space \n\nfor words in data['headline']:\n  token_data.append(words.split(\" \"))\n","3000160f":"token_data[0:5]","9845414e":"punctuation = [\",\", \":\", \";\", \".\", \"'\", '\"', \"\u2019\", \"?\", \"\/\", \"-\", \"+\", \"&\", \"(\", \")\"]\nclean_strings = []","3756ecfc":"for words in token_data:\n  cleaned = [] \n  for word in words:\n    word = word.lower()\n    for punc in punctuation:\n      word = word.replace(punc,\"\")\n    cleaned.append(word)\n  clean_strings.append(cleaned)","5a3c1533":"clean_strings[0:5]","2eae3905":"from collections import Counter\nimport numpy as np\n\nunique_words = [] \nsingle_words = []\n\n#let us combine all the words in the lists into one single list\n\nfinal_list = []\n\nfor words in clean_strings: \n  for word in words:\n    final_list.append(word)","627a1adb":"final_list[0:15]","9a905937":"word_counter = Counter(final_list)\nprint(word_counter['the'])\nprint(word_counter['google'])","a95bddaf":"for count in word_counter: \n  if word_counter[count] > 1: \n    unique_words.append(count)\n  else:\n    single_words.append(count)","fafa1e9b":"unique_words[0:5]","fe500581":"single_words[0:5]","435b40fd":"#creating the data frame \n#for now we will fill the df with 0\n#be mindful when indexing. The index should be cleaned list of strings and not the individual words\n\ncounts = pd.DataFrame(data = 0, index = np.arange(len(clean_strings)), columns = unique_words)","4f9aacaf":"counts.head()","834f020e":"#we will use the enumerate function. Remember each list in the clean_strings corresponds to the row. \n\nfor index, words in enumerate(clean_strings): \n  for word in words:\n    if word in unique_words:\n      counts.iloc[index][word] += 1","a07992b7":"counts.head(10)","2dc9969d":"counts.shape","dee82a48":"column_filtering = counts.sum(axis = 0)","f6be903a":"column_filtering.head()","d380a52b":"columns_to_drop = column_filtering[(column_filtering < 5) | (column_filtering > 100)]","317a0274":"columns_to_drop.head()","6f80b85d":"counts.drop(columns = columns_to_drop.index, inplace = True)","a39152be":"counts.shape","5bd927c3":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n\nX_train, X_test, y_train, y_test = train_test_split(counts, data['upvotes'], test_size = 0.2, random_state = 1)","0e4a73be":"X_train.head()","4d28233b":"y_train.head()","182672de":"lr_model = LinearRegression()","a17b7989":"lr_model.fit(X_train, y_train)\nlr_predictions = lr_model.predict(X_test)","de21a695":"lr_mse = mean_squared_error(y_test, lr_predictions)\nprint(lr_mse)\nlr_rmse = __import__('math').sqrt(lr_mse)\nprint(lr_rmse)","b1b5fb82":"data['upvotes'].mean()","545de679":"data['upvotes'].std()","8e47c0a1":"from sklearn.model_selection import cross_val_score\n","ef28bad6":"lr2 = LinearRegression()\nscore = cross_val_score(lr2, counts, data['upvotes'], scoring = 'neg_mean_squared_error', cv = 10)\nlr2_score = __import__('numpy').mean(score)\nprint(abs(lr2_score))","624c8f42":"Now that we have the disconnected words list, our next step is to convert them into numerical vectors. We can break this conversion into steps: \n\n- First let us get all the unique words in all of our headlines. We will focus on words that occur more than once, since words that occur only once provide no real value towards the model\n\n- Then we can create a dataframe with each unique word as the column header \n\n- Then for each corresponding row in the headline column, we will assign a value for every unique word that occurs in that string. \n\nYou might find some parts of this excercise are very similar to creating a vocabulary when performing Naive Bayes. ","77293463":"In this notebook we will look at a simple working of Natural Language Processing (NLP for short). NLP is basically a way of enabling your computer to understand human language. The concept is pretty self explanatory and there is no better way to understand it than by practice.","7e907dcb":"We can see that our model accuracy increased. We can continue to experiment. We can use a decision tree model to predict upvotes. Decision trees tend to overfit less. We could also add features such as headline length to see how it impacts model performance. ","5935e42b":"Now our main focus in on the headlines columns. The rest of columns such submission date and url won't be much useful when we are looking to predict the number of upvotes. We don't want to filter our data too much since we only have almost 2800 rows to work with. \n\nSo let us look at how tokenization really works.","27805c08":"**The Data:**","b94922d2":"We can see that this model wasn't that accurate considering the std is almost 39 and our error value is 49. There are multiple ways we can increase accuracy. Let us experiment with a few. \n\n","ed7285ed":"**Objective:**\n\nSo our objective is to predict, the number of upvotes an article recieved based on their headlines. Choosing a model to predict is the rather easy. The challenge is to make the column headline as a feature. \n\nThere are a couple of ways we can go about it, but the model I learned on is 'Bag of Words'. The bag of words model is in essence a process of converting the text into numerical vectors. One of the main steps in bag of words model is tokenization. Tokenization is converting our string into disconnected words. \n\nNow I undersand I am no expert in NLP and I am sure a domain expert would be able to explain it a lot better. Let us try to understand the model one step at a time and coding it as we go along. ","5a628247":"**Tokenization:**","7d2a6528":"You can see the concept of numerical vector taking shape and how tokenization was performed. The final step is to populate the values in the column.","af3a86f7":"You might notice that words are in mixed case and we know that python is case sensitive. Also words contain punctuation marks. Let us convert all words to lower case and remove any punctuation marks. In technical jargon, let us make sure our data is in a uniform format. ","b44de5d8":"**Exploring and cleaning the data:**\n\nBefore we go about doing anything, let us explore the data first and of course, the darling of all us data analysts,  data cleaning.","afe692a2":"Let us add columns to the dataset to make it more clearer what each reperesent. ","068a47ab":"If you recall our objective was to predict the number of the upvotes a post will get based on the headline. Since we are trying to predict a numerical variable the first model that comes to mind is the linear regression model. \n\nBefore we go about choosing a model though let us take a moment and look at our dataset. Each column in the counts column will act as a feature in our model. We have almost 2300 columns. Thats 2300 features. We can say for sure that our model will be overfitted. \n\nSo what can we do to reduce overfitting. Well if you remember previuously we choose words that had a counter of more than 1. We can further refine our data by setting a threshold. Features that have very few counts let's say below 5 will not provide much value to the model and will lead to overfitting. \n\nAlso words such as 'and', 'the' aka stopwords provide no concrete information.\n\nSo we can set a threshold and choose only those words that have counts more than 5 and less than 100. ","7ea1c75b":"Of course we will have missing values. Now there is no way for us to impute them so the natural course of action is to drop them. ","4a7b6e54":"**The dataset contains Hacker News Posts from the year 2006 to 2015. The original owner of this dataset is [Developer Arnaud Drizard](https:\/\/github.com\/arnauddri\/hn). The edited version that we will be using in this notebook was obtained from [Dataquest](https:\/\/www.dataquest.io\/).**\n\n**Here is the data dictionary:** \n\nsubmission_time - When the article was submitted\n\nupvotes - The number of upvotes the article received\n\nurl - The base URL of the article\n\nheadline - The article's headline\n\n___\n___","820f942c":"**Building the model:**","6bb7f37f":"We now have 661 columns remaining. Let us start building our model."}}