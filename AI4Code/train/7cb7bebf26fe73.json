{"cell_type":{"3eb83b4d":"code","458f137c":"code","bc4bb0e4":"code","fe6bd821":"code","5c7d73b5":"code","cd5a71be":"code","1c1517a4":"code","22022bde":"code","1e726597":"code","3b09b648":"code","a83e62e3":"code","d14d1a5f":"code","8d9b80f2":"code","1bad7d5e":"code","c7296b34":"code","d71830b7":"code","2ad68594":"code","b64a7a8b":"code","cf89abdb":"code","bb7493ec":"code","b557eb9a":"code","90a9222e":"code","0aec8a94":"code","18866e04":"code","304b5fa1":"markdown","a4d756f3":"markdown","2e15b131":"markdown","7ad5ba95":"markdown","b6da7ce1":"markdown","11157f73":"markdown","71b6ae4d":"markdown","dc0b86e6":"markdown","5a7cd59d":"markdown"},"source":{"3eb83b4d":"import warnings\nimport os, gc, io, requests, math, time\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport dask.dataframe as dd\nimport matplotlib.pyplot as plt     \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy\nfrom sklearn.impute import SimpleImputer\nfrom IPython.display import clear_output , Markdown\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.manifold import Isomap\nfrom sklearn.manifold import LocallyLinearEmbedding\nfrom sklearn.manifold import TSNE\n\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.metrics import r2_score\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import FastICA\nfrom sklearn.decomposition import TruncatedSVD\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import preprocessing\n\n!pip install pmdarima\n!pip install ivis\nfrom ivis import Ivis\nfrom pmdarima.preprocessing import BoxCoxEndogTransformer\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom tqdm.notebook import tqdm\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nplt.style.use('bmh')\nplt.rcParams['figure.figsize'] = [20, 5]  # width, height\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\n\nclear_output()","458f137c":"%%time\ntrain = dd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv' ).compute().sort_values('ts_id')\nfeatures = [c for c in train.columns if \"feature\" in c]\nnon_features = [c for c in train.columns if \"feature\" not in c]\n\ntrain['weight_x_resp'] = train['weight'] * train['resp']\ntrain['resp_mean'] = (train['resp_1']+train['resp_2']+train['resp_3']+train['resp_4'])\/4\n\n#Getting Target Variables\ntrain['resp_target'] = (train['resp'] > 0).astype(np.int8)\ntrain['resp_mean_target'] = (train['resp_mean'] > 0).astype(np.int8)","bc4bb0e4":"%%time\n# Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype.name\n\n        if col_type not in ['object', 'category', 'datetime64[ns, UTC]']:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df\ntrain = reduce_mem_usage(train)","fe6bd821":"Markdown('<li>As JointPoint Suggest there is no Statistical Relation Between Resp and Weight.\\\n          <li>Also Resp Column is <b>Pre-Standardized with Mean : {} and STD : {}<\/b>.\\\n          <li>The Data is Lighly Skewed with Skewness : {} \\\n          <li>Kurtosis Of Series Show its a Leptokurtic Distribution(Kurtosis > 3) With Kurtosis : {}\\n'\\\n         .format(train['resp'].mean() , train['resp'].std() , scipy.stats.skew(train['resp'].values) ,  scipy.stats.kurtosis(train['resp'].values)))","5c7d73b5":"%%time\ndef get_stats(series , i , outlier_limit = 1.5 ) :\n    describe = {}\n    time_describe = {}\n    describe['Column'] = i\n    s_time = time.time()\n    start_time = s_time\n    \n    describe['Mean'] = series.mean();time_describe['Mean'] = time.time() - start_time;start_time = time.time() #Mean of Series    \n    describe['Median'] = series.median();time_describe['Median'] = time.time() - start_time;start_time = time.time() #Median of Series\n    describe['STD'] = series.std();time_describe['STD'] = time.time() - start_time;start_time = time.time() #Standard Deviation of Series\n    describe['Minimum'] = series.min();time_describe['Minimum'] = time.time() - start_time;start_time = time.time() #Minimum of Series\n    describe['Maximum'] = series.max();time_describe['Maximum'] = time.time() - start_time;start_time = time.time() #Maximum of Series\n    \n    \n    describe['Q1'] , describe['Q3'] = series.quantile([0.25 , 0.75]) # Quantile 1 and 3 of Series   \n    describe['InterQuantile'] = describe['Q3'] - describe['Q1'] # Interquantile Range\n    # Calculating Number of Outliers Take lot of time\n#     lower_bound = describe['Q1'] - (outlier_limit * describe['InterQuantile']) \n#     upper_bound = describe['Q3'] + (outlier_limit * describe['InterQuantile'])\n#     describe['Number of Outliers'] = len([x for x in series.values if x <= lower_bound or x >= upper_bound]) #Number of Outliers\n    time_describe['Quantiles'] = time.time() - start_time;start_time = time.time()\n    \n    describe['Nulls'] = series.isnull().sum();time_describe['Nulls'] = time.time() - start_time;start_time = time.time() #Number of Null Values\n    describe['Skewness'] = scipy.stats.skew(series.values);time_describe['Skewness'] = time.time() - start_time;start_time = time.time() # Skewness of Series\n    describe['Kurtosis'] = scipy.stats.kurtosis(series.values);time_describe['Kurtosis'] = time.time() - start_time;start_time = time.time() #Kurtosis (Peakness in Distributed Data)\n    describe['Variation'] = scipy.stats.skew(series.values);time_describe['Variation'] = time.time() - start_time;start_time = time.time() #Ratio of the biased standard deviation to the mean.\n    describe['Entroy'] = scipy.stats.entropy(series.values);time_describe['Entroy'] = time.time() - start_time;start_time = time.time() #Entropy of Series\n    \n    describe['Total_Time'] = time.time() - s_time\n    return [describe , time_describe]\n\ndef Custom_Describe(data , columns , sample = 100000):\n    r_data = []\n    t_data = []\n    sample_data = data.sample(sample)\n    for i in tqdm(columns):\n        return_data = get_stats(sample_data[i] , i)\n        r_data.append(return_data[0])\n        t_data.append(return_data[1])\n    r_data = pd.DataFrame(r_data)\n    t_data = pd.DataFrame(t_data)\n    return r_data , t_data\n\nr_data  , t_data = Custom_Describe(train , non_features[1:-1]+features + ['resp_mean','weight_x_resp'] )\nr_data.index = r_data.Column\nr_data.drop(columns = ['Column'] , inplace = True)\nr_data.to_csv('Train_Details.csv')\ndisplay(r_data.head(10))","cd5a71be":"r_data.loc[[x  for x in r_data.index.values if 'resp' in x]][['InterQuantile' , 'Skewness' , 'Kurtosis' , 'Variation' , 'Mean' , 'STD']].sort_values(['InterQuantile'])","1c1517a4":"print('Percentage of Weight is equal to 0 is ',train[train['weight'] == 0.0].shape[0]*100\/train.shape[0])","22022bde":"%%time\nsns.jointplot(x = 'resp' ,y= 'weight' , data = train , kind = 'scatter')","1e726597":"fig, axs = plt.subplots(1, 2, figsize=(16, 6))\nsns.distplot(train['resp'], ax=axs[0])\nsns.distplot(train['weight'], ax=axs[1])","3b09b648":"correlations = train.sample(100000).corr(method='pearson')\nfig, axs = plt.subplots(figsize=(16, 16))\nsns.heatmap(correlations)","a83e62e3":"(1+train.groupby('date')['resp'].mean()).plot(label = 'Resp Mean')\n(1+train.groupby('date')['weight_x_resp'].mean()).cumprod().plot(label = 'Cummulative Weight*Resp')\n(1+train.groupby('date')['resp'].mean()).cumprod().plot(label = 'Cummulative Resp')\n(1+train.groupby('date')['resp'].std()).plot(label = 'Resp Standard Deviation')\nplt.legend()","d14d1a5f":"(1+train.groupby('date')['weight_x_resp'].mean()).plot(label = 'Mean Weight*Resp')\n(1+train.groupby('date')['resp'].mean()).plot(label = 'Resp Mean')\nplt.legend()","8d9b80f2":"(1+train.groupby('date')['resp'].mean()).cumprod().plot()\n(1+train.groupby('date')['weight_x_resp'].mean()).cumprod().plot()\n(1+train.groupby('date')['resp_mean'].mean()).cumprod().plot()\nplt.legend()","1bad7d5e":"(1+train.groupby('date')['resp_1'].mean()).cumprod().plot()\n(1+train.groupby('date')['resp_2'].mean()).cumprod().plot()\n(1+train.groupby('date')['resp_3'].mean()).cumprod().plot()\n(1+train.groupby('date')['resp_4'].mean()).cumprod().plot()\nplt.legend()","c7296b34":"Season_Period = 7\nModel_Behaviour = \"add\"\n\ndata = train.groupby('date')['resp'].mean()\n\nif Model_Behaviour == 'mul':\n    data = data + 1\nseas_d=sm.tsa.seasonal_decompose(data ,model=Model_Behaviour,period=Season_Period , extrapolate_trend='freq')\nclear_output()\n\nPlt_Col = 2\nPlt_Row = 1\nplt.figure(figsize=(20,16\/Plt_Col))\n\nplt.subplot(Plt_Row,Plt_Col , 1)\nplt.plot(seas_d.seasonal.values , label = 'Estimated Seasonal Component')\n# plt.fill_between(list(range(seas_d.nobs[0])) ,0, seas_d.seasonal.values , alpha =0.4)\nplt.legend()\n\nplt.subplot(Plt_Row,Plt_Col , 2)\nplt.plot(seas_d.resid.values , label = 'Estimated Residual Component')\nplt.fill_between(list(range(seas_d.nobs[0])) ,0, seas_d.seasonal.values , alpha =0.4 , label = 'Estimated Seasonal Components')\nplt.legend()\n\nplt.plot()\n\nplt.figure(figsize=(20,16\/Plt_Col))\nplt.plot(seas_d.observed.values , label = 'Observed Component')\nplt.plot(seas_d.trend.values , label = 'Estimated Trend Component')\nplt.legend()\nplt.plot()\n","d71830b7":"data = 1 + train.groupby('date')['weight'].mean()\nPlt_Col = 4\nfreq = []\n\nfreq.append([data.values.reshape(-1)  , 'Original Data'])\nfreq.append([data.diff().values.reshape(-1)  , 'Difference of Original Data'])\n# freq.append([data.cumprod().values.reshape(-1)  , 'Cummulative Product Plot'])\nfreq.append([data.cumsum().values.reshape(-1)  , 'Cummulative Sum Plot'])\nfreq.append([np.log1p(data).values.reshape(-1) , 'Log1p Fuction'])\nfreq.append([np.sqrt(data).values.reshape(-1) , 'Square Root Function'])\nfreq.append([np.cbrt(data).values.reshape(-1) , 'Cube Root Function'])\nfreq.append([np.sqrt(np.sqrt(data)).values.reshape(-1) , 'Quad Root Function'])\nfreq.append([np.square(data).values.reshape(-1) , 'Square Function'])\nfreq.append([preprocessing.MinMaxScaler().fit_transform(data.values.reshape(-1,1)).reshape(-1) , 'Min Max Scaler Function'])\nfreq.append([preprocessing.RobustScaler().fit_transform(data.values.reshape(-1,1)).reshape(-1) , 'Robust Scaler Function'])\nfreq.append([preprocessing.StandardScaler().fit_transform(data.values.reshape(-1,1)).reshape(-1) , 'Standard Scaler Function'])\nfreq.append([preprocessing.PowerTransformer().fit_transform(data.values.reshape(-1,1)).reshape(-1) , 'Power Transformer Scaler'])\nfreq.append([sm.tsa.filters.bkfilter(data).values.reshape(-1) , 'BKFilter Band Pass Filter'])\nfreq.append([sm.tsa.filters.hpfilter(data)[1].values.reshape(-1) , 'HPFilter Smoothing Filter'])\nfreq.append([sm.tsa.filters.hpfilter(data.cumprod())[1].values.reshape(-1) , 'HPFilter  on CumPord'])\nfreq.append([BoxCoxEndogTransformer().fit_transform(data+0.00000001)[0], 'BoxCoxEndogTransformer Filter'])\n\nPlt_Row = math.ceil(len(freq)\/Plt_Col)\nplt.figure(figsize=(16,12*Plt_Row\/Plt_Col))\n\nfor i,j in enumerate(freq):\n    plt.subplot(Plt_Row , Plt_Col , i+1)\n    plt.plot(j[0], label = j[1] , linewidth = .8)\n  \n    plt.legend(loc = 2)","2ad68594":"null_percent = []\nfor i in train.columns:\n    null_percent.append([i , train[i].isnull().sum()])\nnull_percent = pd.DataFrame(null_percent , columns = ['Column', 'Missings'])\nnull_percent['P_Missings'] = (null_percent['Missings']\/train.shape[0])*100\nnull_percent.sort_values(by =['Missings'], ascending = False, inplace = True)\nfig, axs = plt.subplots(1, 1, figsize=(16, 6))\nnull_percent.reset_index(drop = True)['P_Missings'].plot(ax = axs)\nplt.vlines(x = len(null_percent[null_percent['Missings'] != 0]), ymin = 0, ymax = null_percent['P_Missings'].max(),\n           colors = 'purple', \n           label = 'Zero Values') \n  \nplt.xlabel('Columns')\nplt.ylabel('Percentage of Missing %')\nplt.legend()\nnull_percent.index = null_percent.Column\nnull_percent.drop(columns = ['Column'] , inplace = True)","b64a7a8b":"null_percent.sort_index().loc[features]['Missings'].plot()","cf89abdb":"null_percent.head(10)","bb7493ec":"day = 54\n(train[train['date'] == day]['resp']*100).plot()\n(train[train['date'] == day]['feature_28']+40).plot()\n(train[train['date'] == day]['feature_27']+20).plot()\n(train[train['date'] == day]['feature_18']+60).plot()\n(train[train['date'] == day]['feature_7']+80).plot()\n(train[train['date'] == day]['feature_8']+100).plot()\nplt.legend()","b557eb9a":"(train[train['date'] == 0]['feature_28'].reset_index(drop = True)+0 ).plot(label = 'Day 0')\n(train[train['date'] == 1]['feature_28'].reset_index(drop = True)+10).plot(label = 'Day 1')\n(train[train['date'] == 2]['feature_28'].reset_index(drop = True)+10).plot(label = 'Day 2')\n(train[train['date'] == 3]['feature_28'].reset_index(drop = True)+20).plot(label = 'Day 3')\n(train[train['date'] == 4]['feature_28'].reset_index(drop = True)+30).plot(label = 'Day 4')\n(train[train['date'] == 5]['feature_28'].reset_index(drop = True)+40).plot(label = 'Day 5')\n(train[train['date'] == 6]['feature_28'].reset_index(drop = True)+50).plot(label = 'Day 6')\n(train[train['date'] == 7]['feature_28'].reset_index(drop = True)+60).plot(label = 'Day 7')\n(train[train['date'] == 8]['feature_28'].reset_index(drop = True)+70).plot(label = 'Day 8')\nplt.legend()","90a9222e":"### Highest count Days\n(train[train['date'] == 44]['feature_28'].reset_index(drop = True)+0 ).plot(label = 'Day 44')\n(train[train['date'] == 45]['feature_28'].reset_index(drop = True)+20).plot(label = 'Day 45')\n(train[train['date'] == 459]['feature_28'].reset_index(drop = True)+40).plot(label = 'Day 459')\n(train[train['date'] == 38]['feature_28'].reset_index(drop = True)+60).plot(label = 'Day 38')\n(train[train['date'] == 85]['feature_28'].reset_index(drop = True)+80).plot(label = 'Day 85')\nplt.legend()","0aec8a94":"### Least Count Days\n(train[train['date'] == 2]['feature_28'].reset_index(drop = True)).plot(label = 'Day 2')\n(train[train['date'] == 36]['feature_28'].reset_index(drop = True)+20).plot(label = 'Day 36')\n(train[train['date'] == 294]['feature_28'].reset_index(drop = True)+10).plot(label = 'Day 294')\n(train[train['date'] == 270]['feature_28'].reset_index(drop = True)+30).plot(label = 'Day 270')\nplt.legend()","18866e04":"count_data = train.groupby(['date'])['resp'].count()\nplt.axhline(y = count_data.mean() , linestyle = 'dashed' , label = 'Count Mean', color = 'm' , linewidth = 1.5)\nplt.axhline(y = count_data.median() , linestyle = 'dashed' , label = 'Count Median', color = 'c' , linewidth = 1)\nsm.tsa.filters.hpfilter(count_data , lamb = 50)[1].plot(label = 'Count Per Day (Filtered)' , color = '#0072B2',  linewidth = 1.3)\ncount_data.plot(label = 'Counts Per Day' , linewidth = 0.85 , color = '#0072B2')\nplt.legend()","304b5fa1":"# About Missing Values","a4d756f3":"# Loading Data and Preprocessing","2e15b131":"As JointPlot Suggest The Resp data is standardized ","7ad5ba95":"## Understanding Missing Values for Top Missed Columns","b6da7ce1":"## Understanding consecutive Days","11157f73":"# Count of Each Day","71b6ae4d":"The Cummulative Product of Resp Show Increment which Show the Profits are Showing Up with Days.","dc0b86e6":"# Visualizations","5a7cd59d":"* There is Missing Part every Day at beginning and Middle.\n* That must be The Gap of Some Sort that Inflences the Trade Happening (Except of Day  2 & 294)"}}