{"cell_type":{"2002afbe":"code","576fee17":"code","8d89b553":"code","ed395c2c":"code","b9befb6b":"code","238301ab":"code","e5b2311e":"code","2580a851":"code","c6d8420e":"code","5fe3a8cf":"code","988778b1":"code","19431891":"code","1567c7e3":"code","8b9b865c":"code","f2d91332":"code","1ebcb0ad":"code","48ab6f8d":"code","2f8aeec3":"code","3437d45a":"code","e24cd91f":"code","86605a36":"code","94fa5f0d":"code","d0086d72":"code","0e7e7566":"code","887143bb":"code","b4461dd1":"code","4e6f396e":"code","d69e47f1":"code","ebd3d1f9":"code","96469ac6":"code","87ef51d2":"code","9735620d":"code","a1d2ec40":"code","2190eb91":"code","6191a37a":"code","0ec979bc":"code","ab4a7617":"code","2b7d6cb3":"code","9bd60cf3":"code","5f018765":"code","fd81ca31":"code","a474c9f5":"code","03dd841d":"code","3913a414":"code","a57d1db4":"code","19f75e46":"code","d90bb23f":"markdown","00a25370":"markdown","a750441f":"markdown","e58659bd":"markdown","4335861f":"markdown","23acb78a":"markdown","7ce3058e":"markdown","263e74bf":"markdown","050783af":"markdown","92316b49":"markdown","42d46770":"markdown","7694b80a":"markdown","f71f9244":"markdown","a53176f8":"markdown","edbcd371":"markdown","9fe73342":"markdown","6fd455e5":"markdown","ddb01aee":"markdown","2b0dabc5":"markdown","d0d95794":"markdown","d5ec45fb":"markdown","c3b98f44":"markdown","a9b85ed0":"markdown","6164214b":"markdown","a0cdf518":"markdown","e4468cc6":"markdown","ac75715d":"markdown","f64d5293":"markdown","c15a7f6c":"markdown","8390a0bb":"markdown","5a9f8e3c":"markdown","82769eb1":"markdown","0a307847":"markdown","f38c064b":"markdown","bb232710":"markdown","94b89cd2":"markdown","e04e5f8f":"markdown"},"source":{"2002afbe":"# data processing\nimport pandas as pd\n\n## linear algebra\nimport numpy as np\n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Algorithms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\n \nfrom sklearn.metrics import accuracy_score  #for accuracy_score\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.metrics import confusion_matrix #for confusion matrix","576fee17":"titanic = pd.read_csv('..\/input\/titanic\/train.csv')\ntitanic_test = pd.read_csv('..\/input\/titanic\/test.csv')\n# \u62ff\u5230id\u5148\u8fdb\u884c\u5b58\u50a8\uff0c\u5728outputs\u65f6\u7528\u5230\ntitanic_test_id = titanic_test['PassengerId']\nprint(titanic.head(5))\nprint(titanic_test.head(5))","8d89b553":"print(\"shape : \",titanic.shape)\n\n# statistical information\nprint(titanic.describe())\n\nprint(\"information -----\")\nprint(titanic.info())","ed395c2c":"dataset = [titanic,titanic_test]","b9befb6b":"def missing_data(df):\n    total = df.isnull().sum().sort_values(ascending = False)\n    percent = round(df.isnull().sum().sort_values(ascending = False) * 100\/len(df) , 2)\n    # \u7ad6\u76f4\u7ea7\u8054\n    return pd.concat([total,percent],axis = 1, keys = ['Total','Percent'])\nmissing_data(titanic)","238301ab":"missing_data(titanic_test)","e5b2311e":"# \u5220\u9664\u5217 axis = 1;  inplace, \u8be5\u64cd\u4f5c\u662f\u5426\u5bf9\u5143\u6570\u636e\u751f\u6548\ndrop_column = ['Cabin']\ntitanic.drop(drop_column,axis = 1, inplace = True)\ntitanic_test.drop(drop_column,axis = 1, inplace = True)\n\nfor data in dataset:\n    data['Age'].fillna(data['Age'].median(),inplace = True)\n    data['Fare'].fillna(data['Fare'].median(),inplace = True)    \n    data['Embarked'].fillna(data['Embarked'].mode()[0],inplace = True) \n\nmissing_data(titanic)","2580a851":"def draw(graph):\n    for p in graph.patches:\n        height = p.get_height()\n        graph.text(p.get_x() + p.get_width()\/2.,height +5,height , ha =\"center\")","c6d8420e":"# sns.countplot \u4f7f\u7528\u56fe\u5f62\u67f1\u72b6\u56fe\u663e\u793a\u6bcf\u4e2a\u7c7b\u522b\u7684 \u6570\u91cf\u7edf\u8ba1 \uff1b \u4ee5\u6bcf\u4e2avalue\u503c\u4f5c\u4e3ax\u8f74\u4e0a\u7684\u5206\u7c7b\u7c7b\u578b  value count for a single or two variable\n# x x\u6807\u7b7e \uff1b hue \u5728x,y\u6807\u7b7e\u5212\u5206\u7684\u540c\u65f6\uff0c\u518d\u4ee5hue\u6807\u7b7e\u7edf\u8ba1\n# plt.figure(figsize) \u8868\u793a figure \u7684\u957f\u548c\u5bbd\nsns.set(style = \"darkgrid\")\nplt.figure(figsize = (8,5))\ngraph = sns.countplot(x = 'Survived',hue = 'Survived',data = titanic)\n# draw(graph)","5fe3a8cf":"plt.figure(figsize=(8,5))\ngraph = sns.countplot(x= \"Sex\",hue = \"Survived\",data = titanic)","988778b1":"# plt.subplots()\u628a\u56fe\u5f62\u5206\u6210 nrows * ncols \u4e2a\u5b50\u56fe\uff0c \u628a\u5b50\u56fe\u8d4b\u503c\u7ed9 ax\uff0cax[0]\u4e3a\u7b2c\u4e00\u4e2a\u5b50\u56fe\uff0cax[1]\u7b2c\u4e8c\u4e2a...\n# sns.countplot(data ,ax) ,\u5c06\u6570\u636e\u7ed8\u5236\u5230\u5b50\u56fe\u4e0a\nfig,ax = plt.subplots(nrows = 1, ncols = 2,figsize = (10,4))\nx = sns.countplot(titanic['Pclass'], ax = ax[0])\ny = sns.countplot(titanic['Embarked'], ax = ax[1])\ndraw(x)\ndraw(y)\nfig.show()","19431891":"# FacetGrid() \u7528\u4e8e \u5b50\u96c6\u4e2d\u5206\u522b\u53ef\u89c6\u5316\u53d8\u91cf\u7684\u5206\u5e03\uff0c \u6216\u8005\u591a\u4e2a\u53d8\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\u65f6\uff0c\u975e\u5e38\u6709\u7528\n# .FacetGrid() \u753b\u51fa\u8f6e\u5ed3 \uff1b .map()\u586b\u5145\u6570\u636e\n# .FacetGrid()  col,row,hue \u5b9a\u4e49\u5b50\u96c6\u7684\u53d8\u91cf\uff0c\u5728\u7f51\u683c\u7684\u4e0d\u540c\u65b9\u9762\u7ed8\u5236\uff1b\u653e\u5728\u9876\u90e8\uff0c\u5c55\u793a\u5b50\u96c6\u6570\u636e  \uff1b aspect \u6bcf\u4e2a\u5c0f\u56fe\u7684\u6a2a\u8f74\u548c\u7eb5\u8f74\u7684\u6bd4\u4f8b\n# FacetGrid .add_legend() \u7ed8\u5236\u4e00\u4e2a\u56fe\u4f8b\n# FacetGrid .map(func , *args, *kwargs) \u5c06\u7ed8\u56fe\u5e94\u7528\u4e8e\u6bcf\u4e2a\u65b9\u9762\u7684\u6570\u636e\u5b50\u96c6\n# sns.pointplot \u70b9\u56fe\uff1b \nFacetGrid = sns.FacetGrid(titanic , col = 'Pclass' , height = 4,aspect = 1)\nFacetGrid.map(sns.pointplot, 'Embarked','Survived','Sex',order = None, hue_order = None)\nFacetGrid.add_legend()","1567c7e3":"# .drop() \u5220\u9664\u884c\u6216\u5217 \uff1b \u9ed8\u8ba4\u5220\u9664\u67d0\u884c\uff0caxis = 1\uff0c\u5220\u9664\u67d0\u5217\uff1b inplace = True \u5728\u539f\u6570\u636e\u4e0a\u6539\u53d8\ndrop_column = ['Embarked']\ntitanic.drop(drop_column,axis = 1 ,inplace = True)\ntitanic_test.drop(drop_column,axis = 1 ,inplace = True)","8b9b865c":"plt.figure(figsize = (8,5))\npclass = sns.countplot(x = 'Pclass' , hue = 'Survived',data = titanic)","f2d91332":"plt.figure(figsize = (8,5))\nsns.barplot(x = 'Pclass', y = 'Survived',data = titanic)","1ebcb0ad":"all_data = [titanic,titanic_test]\nfor dataset in all_data:\n    dataset['Family'] = dataset['SibSp'] + dataset['Parch']+1","48ab6f8d":"# sns.factorplot \u7ed8\u5236\u591a\u4e2a\u7ef4\u5ea6\u7684\u53d8\u91cf factorplot(x,y,data,aspect)   \naxes = sns.factorplot('Family','Survived',data = titanic ,aspect = 2.5)","2f8aeec3":"axes = sns.factorplot('Family','Age','Survived',data = titanic , aspect = 2.5,)","3437d45a":"# pd.cut() \u5206\u7bb1\u64cd\u4f5c bins:\u6807\u91cf\u5e8f\u5217\u6216\u95f4\u9694\u7d22\u5f15\uff0c\u662f\u5206\u7bb1\u7684\u4f9d\u636e \uff1b labels\uff1a\u5206\u7bb1\u7684\u6807\u7b7e\nfor dataset in all_data:\n    dataset['Age_bin'] = pd.cut(dataset['Age'], bins = [0,12,20,40,120],labels = ['children','teens','adult','elder'])\nplt.figure(figsize = (8,5))\nsns.barplot(x = 'Age_bin' , y = 'Survived',data = titanic)","e24cd91f":"plt.figure(figsize = (8,5))\nage = sns.countplot(x = 'Age_bin',hue='Survived',data = titanic)","86605a36":"AAS = titanic[['Sex','Age_bin','Survived']].groupby(['Sex','Age_bin'],as_index = False).mean()\nsns.factorplot('Age_bin', 'Survived','Sex',data = AAS,aspect = 3,kind='bar')\nplt.suptitle('Age,Sex vs Survived')","94fa5f0d":"for dataset in all_data:\n    dataset['Fare_bin'] = pd.cut(dataset['Fare'],bins = [0,10,50,100,550],labels = ['low_fare','mid_fare','ave_fare','high_fare'])\nplt.figure(figsize = (8,5))\nsns.countplot(x = 'Pclass',hue = 'Fare_bin', data = titanic)","d0086d72":"sns.barplot(x = 'Fare_bin', y ='Survived', data = titanic)","0e7e7566":"# abs() \u7edd\u5bf9\u503c    \n# corr() \u76f8\u5173\u7cfb\u6570\u77e9\u9635\uff0c\u7ed9\u51fa\u4e86\u4efb\u610f\u4e24\u4e2a\u53d8\u91cf\u4e4b\u95f4\u7684\u76f8\u5173\u7cfb\u6570 corr()[A]\u53ea\u663e\u793aA\u548c\u5176\u4ed6\u53d8\u91cf\u4e4b\u95f4\u7684\u76f8\u5173\u7cfb\u6570\n# \u4e0a\u4e09\u89d2\u548c\u4e0b\u4e09\u89d2\u5806\u6210\npd.DataFrame(abs(titanic.corr()['Survived']).sort_values(ascending = False))","887143bb":"# np.zero_like\uff08W\uff0cdtype = np.bool\uff09 \u751f\u6210\u4e00\u4e2a \u521d\u59cb\u5316\u4e3a False \u7684bool\u578b\u77e9\u9635\uff0c\u7ef4\u5ea6\u548c w \u4e00\u81f4\n# np.triu_indices_from \u8fd4\u56de\u6570\u7ec4\u4e0a\u4e09\u89d2\u7684\u7d22\u5f15\n# mask[np.triu_indices_from(mask)] = True \u5c06\u4e0a\u4e09\u89d2\u533a\u57df\u7f6e\u4e3aTrue\n# \u4ee5\u4e0a \u751f\u6210\u4e86\u4e00\u4e2a\u63a9\u7801\n# sns.heatmap() \u751f\u6210\u70ed\u529b\u56fe \n# sns.heatmap(data , vmax\/vmin = \u8bbe\u7f6e\u989c\u8272\u5e26\u7684\u6700\u5927\u6700\u5c0f\u503c\uff0ccmap = \u8bbe\u7f6e\u989c\u8272\u5e26\u7684\u8272\u7cfb\uff0c \n#  center = \u8bbe\u7f6e\u989c\u8272\u5e26\u7684\u5206\u754c\u7ebf\uff0cannot = \u662f\u5426\u663e\u793a\u6570\u503c\u6ce8\u91ca\uff0cfmt = \u6570\u503c\u7684\u683c\u5f0f\u5316\u5f62\u5f0f\uff0c\n#  linewidths\/linecolor = \u6bcf\u4e2a\u5c0f\u65b9\u683c\u4e4b\u95f4\u7684\u95f4\u8ddd\u548c\u989c\u8272\uff0cmap = \u4f20\u5165\u5e03\u5c14\u578b\u77e9\u9635\uff0c\u4e3atrue\u7684\u5730\u65b9\uff0c\u70ed\u529b\u56fe\u76f8\u5e94\u7684\u533a\u57df\u4f1a\u88ab\u5c4f\u853d)\ncorr = titanic.corr()\n\nmask = np.zeros_like(corr, dtype = np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.subplots(figsize=(12,8))\nsns.heatmap(corr,\n            annot=True,\n            mask = mask,\n            cmap = 'RdBu',\n            linewidths = .9,\n            linecolor = 'white',\n            vmax = 0.3,\n            fmt = '.2f',\n            center = 0,\n            square = True)\nplt.title('Correlations Metrix', y = 1, fontsize = 20, pad = 20)","b4461dd1":"titanic.info()","4e6f396e":"gender = {\"male\":0 , \"female\":1}\nfor data in all_data:\n    data['Sex'] = data['Sex'].map(gender)\ntitanic['Sex'].value_counts()","d69e47f1":"for dataset in all_data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[dataset['Age']<=15,'Age'] = 0\n    dataset.loc[(dataset['Age']>15) & (dataset['Age']<=20),'Age'] = 1\n    dataset.loc[(dataset['Age']>20) & (dataset['Age']<=26),'Age'] = 2    \n    dataset.loc[(dataset['Age']>26) & (dataset['Age']<=28),'Age'] = 3     \n    dataset.loc[(dataset['Age']>28) & (dataset['Age']<=35),'Age'] = 4     \n    dataset.loc[(dataset['Age']>35) & (dataset['Age']<=45),'Age'] = 5  \n    dataset.loc[dataset['Age']>45,'Age'] = 6\ntitanic['Age'].value_counts()","ebd3d1f9":"# for dataset in [titanic]:\n#     drop_column = ['Age_bin','Fare','Name','Ticket','PassengerId','SibSp','Parch','Fare_bin']\n# #     drop_column = ['Age_bin','Fare','Name','Ticket','SibSp','Parch','Fare_bin']\n#     dataset.drop(drop_column ,axis = 1,inplace = True)\n    \n# for dataset in [titanic_test]:\n# #     drop_column = ['Age_bin','Fare','Name','Ticket','PassengerId','SibSp','Parch','Fare_bin']\n#     drop_column = ['Age_bin','Fare','Name','Ticket','SibSp','Parch','Fare_bin']\n#     dataset.drop(drop_column ,axis = 1,inplace = True)\n    \nfor dataset in all_data:\n    drop_column = ['Age_bin','Fare','Name','Ticket','PassengerId','SibSp','Parch','Fare_bin']\n#     drop_column = ['Age_bin','Fare','Name','Ticket','SibSp','Parch','Fare_bin']\n    dataset.drop(drop_column ,axis = 1,inplace = True)\n    \n    \n# titanic   DataFrame\n# [titanic].dtype()  list","96469ac6":"titanic.info()","87ef51d2":"# train_test_split( train_data , train_target , test_size , random ) \u968f\u673a\u5212\u5206\u6837\u672c\u4e3a\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\n# train_data \u5f85\u5212\u5206\u7684\u6837\u672c\u6570\u636e\uff0c  train_target \u5f85\u5212\u5206\u6837\u672c\u6570\u636e\u7684\u7ed3\u679c\u3010\u6807\u7b7e\u3011\n# test_size \u6d4b\u8bd5\u6570\u636e\u5360\u6837\u672c\u6570\u636e\u7684\u6bd4\u4f8b\uff0c\u82e5\u4e3a\u6574\u6570\u5219\u4ee3\u8868\u6837\u672c\u6570\u91cf\u3002     random \u968f\u673a\u6570\u79cd\u5b50\uff0c\u4fdd\u8bc1\u6bcf\u6b21\u90fd\u662f\u540c\u4e00\u4e2a\u968f\u673a\u6570\n# X_train,y_train \u5f97\u5230\u7684\u8bad\u7ec3\u6570\u636e  X_test,y_test \u5f97\u5230\u7684\u6d4b\u8bd5\u6570\u636e  \nall_features = titanic.drop('Survived',axis = 1)\nTarget = titanic['Survived']\nX_train,X_test,y_train,y_test = train_test_split(all_features,Target,test_size = 0.3 , random_state = 0)\nX_train.shape , X_test.shape , y_train.shape , y_test.shape","9735620d":"# \u903b\u8f91\u56de\u5f52\u3002 \u56de\u5f52\u6a21\u578b\u5206\u4e3a\u7ebf\u6027\u56de\u5f52\uff0c\u5904\u7406 \u56e0\u53d8\u91cf\u662f\u8fde\u7eed\u53d8\u91cf\u7684\u95ee\u9898\u3002 \u903b\u8f91\u56de\u5f52 \u5904\u7406 \u56e0\u53d8\u91cf\u662f\u5206\u7c7b\u53d8\u91cf [\u4e24\u70b9\uff080-1\uff09\u5206\u5e03\u53d8\u91cf]\n# sklearn\u903b\u8f91\u56de\u5f52\u7b97\u6cd5\uff1aLogisticRegression() \u5bfc\u5165\u6a21\u578b \uff1b fit(x,y) \u8bad\u7ec3\u6a21\u578b \uff1b predict() \u9884\u6d4b\uff0c\u7528\u8bad\u7ec3\u5f97\u5230\u7684\u6a21\u578b\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u9884\u6d4b\uff0c\u8fd4\u56de\u9884\u6d4b\u7ed3\u679c\n\n# accuracy_score(y_true , y_pred , normalize \u9ed8\u8ba4True\u8fd4\u56de\u6b63\u786e\u5206\u7c7b\u7684\u6bd4\u4f8b-\u4e3aFalse\u8fd4\u56de\u6b63\u786e\u5206\u7c7b\u7684\u6837\u672c\u6570)\n# \u8bc4\u4f30\u65b9\u6cd5\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u5206\u65f6 \uff1b \u6240\u6709\u5206\u7c7b\u6b63\u786e\u7684\u767e\u5206\u6bd4 \n# round() \u683c\u5f0f\u5316\u6570\u636e\uff0c\u6309\u7167\u6307\u5b9a\u7684\u5c0f\u6570\u4f4d\u6570\u56db\u820d\u4e94\u5165\n# KFold() K\u6298\u4ea4\u53c9\u9a8c\u8bc1   n_splits \u6700\u5c11\u5212\u5206\u4e3a\u51e0\u5757 \uff1b random_state \u968f\u673a\u79cd\u5b50\u6570\n#   \u5c06\u8bad\u7ec3\u96c6\u5212\u5206\u4e3an_splits\u5757\u4e92\u65a5\u5b50\u96c6\uff0c\u6bcf\u6b21\u7528\u5176\u4e2d\u4e00\u4e2a\u5b50\u96c6\u4f5c\u4e3a\u9a8c\u8bc1\u96c6\uff0c\u5176\u4ed6\u5b50\u96c6\u4f5c\u4e3a\u8bad\u7ec3\u96c6\uff0c\u8fdb\u884cn_splits\u6b21\u8bad\u7ec3\uff0c\u5f97\u5230n_splits\u4e2a\u7ed3\u679c\n# cross_val_score() sklearn \u63d0\u4f9b\u7684\u4ea4\u53c9\u9a8c\u8bc1\u65b9\u6cd5\uff0cKFold \u5212\u5206\u6570\u636e\u96c6\uff0ccross_val_score \u6839\u636e\u6a21\u578b\u8fdb\u884c\u8ba1\u7b97\uff0c\u5373\u8c03\u7528\u4e86KFold\u8fdb\u884c\u6570\u636e\u96c6\u5212\u5206\nmodel = LogisticRegression()\nmodel.fit(X_train , y_train)\nprediction_lr = model.predict(X_test)\nLog_acc = round(accuracy_score(prediction_lr,y_test)*100,2)\n\n# kfold = KFold(n_splits = 10 , random_state = 22)   # \u6709\u70b9\u8ff7\u60d1\u4e3a\u5565\u8c03\u7528KFold, cross_val_score\u9ed8\u8ba4\u8c03\u7528\u4e86KFold\u5212\u5206\u6570\u636e\u96c6\u4e86\uff0c\u6320\u5934\nlog_cv_acc = cross_val_score(model , all_features , Target , cv = 10 , scoring = 'accuracy')\n\nprint('The accuracy of the Logistic Regression is ',Log_acc)\nprint('The cross validated score for Logistic Regression is ',round(log_cv_acc.mean()*100,2))","a1d2ec40":"# KNN K\u4e34\u8fd1\u7b97\u6cd5 \uff1b  \u4e34\u8fd1\u7b97\u6cd5\uff1a\u5c06\u6d4b\u8bd5\u56fe\u7247\u548c \u8bad\u7ec3\u96c6\u56fe\u7247\u4e00\u4e00\u8ba1\u7b97\u76f8\u4f3c\u5ea6\uff0c\u76f8\u4f3c\u5ea6\u6700\u9ad8\u56fe\u7247\u7684\u6807\u7b7e\uff0c\u5373\u662f\u6d4b\u8bd5\u56fe\u7247\u7684\u6807\u7b7e\n# K \u4e34\u8fd1\u7b97\u6cd5 \u53ef\u4ee5\u627e k \u4e2a\u6700\u76f8\u8fd1\u7684\u56fe\u7247\uff0c\u518d\u7528k\u5f20\u56fe\u4e2d\u6570\u91cf\u6700\u591a\u7684\u6807\u7b7e\uff0c\u4f5c\u4e3a\u6d4b\u8bd5\u56fe\u7247\u7684\u6807\u7b7e\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train,y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train,y_train)*100 , 2)\n\n# kflod = KFold(n_splits = 10, random_state = 22)\nresult_knn = cross_val_score(model , all_features , Target , cv = 10 , scoring = 'accuracy')\n\nprint('The accuracy of the K Nearst Neighbors Classifier is ',acc_knn)\nprint('The cross validated score for K Nearst Neighbors Classifier is ',round(result_knn.mean()*100,2))","2190eb91":"'''\n\u9ad8\u65af\u6734\u7d20\u8d1d\u53f6\u65af\n\u6734\u7d20\u8d1d\u53f6\u65af\uff1a \u57fa\u4e8e\u6982\u7387\u7406\u8bba\uff0c\u5047\u5b9a \u5c5e\u6027\u76f8\u4e92\u6761\u4ef6\u72ec\u7acb\u3002 p(\u7c7b\u522b\/\u7279\u5f81) = p(\u7c7b\u522b,\u7279\u5f81)\/p(\u7c7b\u522b) \uff1b \u5728\u5c5e\u6027\u4e2a\u6570\u591a\uff0c\u6216\u8005\u5c5e\u6027\u4e4b\u95f4\u76f8\u5173\u6027\u8f83\u5927\u65f6\uff0c\u5206\u7c7b\u6548\u679c\u4e0d\u597d\n\u9ad8\u65af\u6734\u7d20\u8d1d\u53f6\u65af\uff1a \u5047\u8bbe\u6bcf\u4e2a\u5206\u7c7b\u76f8\u5173\u7684\u8fde\u7eed\u503c\u662f\u6309\u7167\u9ad8\u65af\u5206\u5e03\u7684\n'''\nfrom sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\nmodel.fit(X_train,y_train)\npred_gnb = model.predict(X_test)\ngnb_acc = round(accuracy_score(pred_gnb , y_test)*100,2)\n\nresult_gnb = cross_val_score(model , all_features , cv = 12 , scoring = 'accuracy')\n\nprint('The accuracy of the Gaussian Naive Bayes is ',gnb_acc)\nprint('The cross validated score for Gaussian Naive Bayes is ',round(result_gnb.mean()*100,2))","6191a37a":"'''\n\u7ebf\u6027\u652f\u6301\u5411\u91cf\u673a\n\u5411\u91cf\u673a SVM\uff0c\/\/TODO\n'''\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train,y_train)\npred_svc = linear_svc.predict(X_test)\nsvc_acc = round(linear_svc.score(X_train,y_train)*100,2)\nresult_svc = cross_val_score(model,all_features,Target,cv = 10 , scoring = 'accuracy')\n\nprint('The accuracy of the Linear Support Vector Machine is ',svc_acc)\nprint('The cross validated score for Linear Support Vector Machine is ',round(result_svc.mean()*100,2))","0ec979bc":"random_forest = RandomForestClassifier(n_estimators = 100)\nrandom_forest.fit(X_train , y_train)\npred_rf1 = random_forest.predict(X_test)\nacc_rf = round(random_forest.score(X_train,y_train)*100 , 2)\n\nresult_rf = cross_val_score(model,all_features , Target,cv = 10,scoring = 'accuracy')\n\nprint('The accuracy of the Random Forest is ',acc_rf)\nprint('The cross validated score for Random Forest is ',round(result_rf.mean()*100,2))","ab4a7617":"'''\n\u51b3\u7b56\u6811\uff0c\u662f\u6811\u5f62\u6a21\u578b\uff0c\u81ea\u9876\u5411\u4e0b\u9012\u5f52\uff0c\u4ee5\u4fe1\u606f\u71b5\u4e3a\u5ea6\u91cf\u6784\u9020\u4e00\u68f5\u71b5\u503c\u4e0b\u964d\u6700\u5feb\u7684\u6811\n\u548c\u968f\u673a\u68ee\u6797\u7684\u533a\u522b\uff1a\u968f\u673a\u68ee\u6797\u662f\u68ee\u6797\uff0c\u51b3\u7b56\u6811\u662f\u6811\uff1b \u51b3\u7b56\u6811\u8fdb\u884c\u526a\u679d \uff1b \n'''\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train,y_train)\npred_dt = decision_tree.predict(X_test)\nacc_dt = round(decision_tree.score(X_train,y_train)*100 , 2)\n\nresult_dt = cross_val_score(model,all_features,Target , cv = 10,scoring = 'accuracy')\n\nprint('The accuracy of the Decision Tree is ',acc_dt)\nprint('The cross validated score for Decision Tree is ',round(result_dt.mean()*100,2))","2b7d6cb3":"result = pd.DataFrame({\n    'Model' : ['Support Vector Machine','KNN','LogisticRegression',\n              'Random Forest','Gaussian Naive Bayes', 'Decision Tree'],\n    'Score' : [svc_acc, acc_knn,Log_acc,acc_rf,gnb_acc,acc_dt]\n})\nresult_df = result.sort_values(by = 'Score',ascending = False)\nresult_df = result_df.set_index('Model')\nresult_df.head(9)","9bd60cf3":"'''\n\u8f93\u51fa\u7ed3\u679c\u5e76\u4e0d\u56fa\u5b9a\ncross_val_score \u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5bf9\u6570\u636e\u96c6\u8fdb\u884ck\u6b21\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u5e76\u4e3a\u6bcf\u6b21\u9a8c\u8bc1\u7ed3\u679c\u8bc4\u6d4b\uff0c\u8fd4\u56de\u6a21\u578b\u7684\u8bc4\u6d4b\u7ed3\u679c\ncross_val_predict \u8fd4\u56de\u5206\u7c7b\u7ed3\u679c\nconfusion_matrix \u6df7\u6dc6\u77e9\u9635\uff0c\u603b\u7ed3\u5206\u7c7b\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u7684\u5206\u6790\u8868\n\u7cbe\u786e\u7387Precision=TP\/(TP+FP),\u53ec\u56de\u7387recall=TP\/(TP+FN),\u51c6\u786e\u7387accuracy=(TP+FN+FP+TN)\nTN  FP\nFN  TP\n'''\npred_rf = cross_val_predict(random_forest , X_train , y_train , cv = 3)\nconfusion_matrix(y_train,pred_rf)","5f018765":"'''\n\u7cbe\u786e\u5ea6  P = TP\/TP+FP   \u5206\u7c7b\u5668\u5224\u5b9a\u7684\u6b63\u4f8b\u4e2d\uff0c\u6b63\u6837\u672c\u5360\u7684\u6bd4\u4f8b  \n\u53ec\u56de\u7387  R = TP\/TP+FN   \u9884\u6d4b\u4e3a\u6b63\u7684\u6bd4\u4f8b\uff0c\u5360\u6b63\u4f8b\u603b\u6570\u7684\u6bd4\u4f8b\n'''\nfrom sklearn.metrics import precision_score,recall_score\nprint(\"Precision : \",precision_score(y_train,pred_rf))\nprint(\"Recall : \",recall_score(y_train,pred_rf))","fd81ca31":"'''\nf-score \u662f\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u7684\u8c03\u548c\u5e73\u5747\u6570\nf1 = ((\u7cbe\u786e\u7387*\u53ec\u56de\u7387)\/\u7cbe\u786e\u7387+\u53ec\u56de\u7387\uff09*2  \n'''\nfrom sklearn.metrics import f1_score\nf1_score(y_train, pred_rf)","a474c9f5":"from sklearn.metrics import roc_curve\ny_scores = random_forest.predict_proba(X_train)\ny_scores = y_scores[:,1]\nFPR, TPR , thresholds = roc_curve(y_train,y_scores)\ndef plot_roc_curve(FPR,TPR,label = None):\n    plt.plot(FPR,TPR,linewidth = 2 , label = label)\n    plt.plot([0,1],[0,1],'r',linewidth = 4)\n    plt.axis([-0.05,1.05,-0.05,1.05])\n    plt.xlabel('FPR',fontsize = 16)\n    plt.ylabel('TPR',fontsize = 16)    \n\nplt.figure(figsize=(14,7))\nplot_roc_curve(FPR,TPR)\nplt.show()","03dd841d":"titanic_test.info()","3913a414":"# \u6a21\u578b\u5df2\u8bad\u7ec3\u597d-\u76f4\u63a5\u9884\u6d4b\u5373\u53ef\nfeatures = ['Pclass','Sex','Age','Family']\n\ntest_show_data=pd.get_dummies(titanic_test[features])\n# test_show_data=pd.get_dummies(titanic_test,columns = ['Pclass','Sex','Age','Family'])\npre_test = random_forest.predict(test_show_data)\noutput = pd.DataFrame({'PassengerId': titanic_test_id, 'Survived': pre_test})\noutput.to_csv('titanic_predict2.csv', index=False)\nprint(\"Your submission was successfully saved!\")","a57d1db4":"titanic_test.columns","19f75e46":"pd.get_dummies(titanic_test,columns = ['Pclass','Sex','Age','Family'])","d90bb23f":"drop extra append columns","00a25370":"Embarked , from which you gon on board doesn't really matter. so we drop it","a750441f":"here we see , Random Forest give the best result.","e58659bd":"So we can see , fare is related to pclass \n\nthen let's see the Survived relation with fare_bin and compared the result with pclass","4335861f":"# Data Exploration\n\n## 1. Survived Count","23acb78a":"## Which is the best Model?","7ce3058e":"# Feature engineering\nFeature engineering is art of  transfer raw data into useful features.","263e74bf":"# Predictive Modeling\nuse some great Classification Algorithms , to predict whether the Passenger will survive or not","050783af":"## 3.Embarked & PClass vs Survived\nEmbarked : From which location passenger go on board \n\nPclass : Passenger belongs to which class\n* 1st = Upper\n* 2nd = Middle\n* 3rd = Lower","92316b49":"Age,Fare and Cabin  have missing values.\n\nCabin has more than 75% of missing values, so we drop the columns","42d46770":"## 5.SibSp & Parch vs Survived\nSibSp and Parch may have more sense. For parent would't let children die,and they will help family.","7694b80a":"Here we see, Pclass is relate to chance of survival","f71f9244":"### 6.Decisian tree ","a53176f8":"* children under 12 have more chance of survival\n\nlet's see the count number of each label","edbcd371":"# Conclusion\n\nI am a really new recruit of ml and kaggle, before I enter kaggle ,I only familiar with RF, Haha.\n\nThis is my first competion, and all copy from author 'Nirav Prajapati', I really learn a lot from his article, Thanks !\n\nI will end my article, and start to learn models in the article, bye~","9fe73342":"### 2.K Nearest Neighbor","6fd455e5":"Hmm...It works ok on train data ,but it doesn't work well on test data, I don't know why... ,\n\nso upset...\n\ngo and study, maybe oneday I can solve it","ddb01aee":"first row , TN , 329 passengers correctly classified as not survive ; FP  52 passengers wrongly classfied as not survive\n\n2nd row , FN , 78 passengers wrongly classified as survive ; TP  164 passengers correctly classfied as survive","2b0dabc5":"### Logistic Regression","d0d95794":"they are co-related with each other\n\nso drop fare , after we do the orrection\n\n# Corrextion Martix\nNote that , only numeri features can be compared.\n\n**POSITIVE CORRELATION: ** \nif an increase in feature A, leads to increase in feature B,value \"1\" , means they are positively correclate\n\n**NEGATIVE CORRELATION: ** \nif an increase in feature A, leads to decrease in feature B,value \"-1\" , means they are negativly correclate","d5ec45fb":"### 4.Linear Support Vector Machine","c3b98f44":"### 3. Gaussian Naive Bayes","a9b85ed0":"* if you are near 30, you have more chance of survival\n* if you are alone, and near 30, survival chance is 50%\n* if all family member near 30, you have the highest chance. but it's not usual\n\nthen we will see only age vs survived","6164214b":"# 4.Pclass VS Survived","a0cdf518":"-----------------------------\n\n-----------------------------\n\n-----------------------------\n\n-----------------------------\n\n-----------------------------\n\n-----------------------------\n\n-----------------------------\n\n-----------------------------\n\n\n# Here is my outputs","e4468cc6":"### F-score","ac75715d":"In all age band, Female have more chance to survive.\n\n## 7.Fare VS Survived\nFare is the fee paid to buy ticket.\n\nSo Fare is logiclly related with Pclass","f64d5293":"## 2. Sex vs Survived","c15a7f6c":"### check missing data","8390a0bb":"### 5.Random Forest","5a9f8e3c":"Age is really important features.\n\nLet's check the relation of age and sex vs survived.","82769eb1":"# \u4e8c\u3001Data Analysis","0a307847":"deal with the Dtype 'object' and 'float64'","f38c064b":"# Further Evalution\n### Confusion Matrix [\u6df7\u6dc6\u77e9\u9635]","bb232710":"### ROC AUC Curve\nAnother way to evaluate binary classfier\n\nthis curve plots (recall(TP rate)) \/ (FP rate)","94b89cd2":"# \u4e00\u3001Getting data","e04e5f8f":"* if you are alone , survival chances is around 25%\n* if you have family of 4, survival chance is highest around 70%\n* if you have more than 4 family members, survival chance will reduce\n\n## 6. Age vs Survived\nlet's check the Combination of Family and Age for survival"}}