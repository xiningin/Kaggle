{"cell_type":{"18b5f9fe":"code","d6966a41":"code","1cd7e106":"code","31484aad":"code","4a674212":"code","a41c11a0":"code","b25a0ef3":"code","a254e230":"code","73961429":"code","d9f4f11a":"markdown","35d12b8f":"markdown","3d59a937":"markdown","9108e35c":"markdown","e8964bfb":"markdown","75da53f3":"markdown","0e88e746":"markdown","c52e08a9":"markdown","51cf926c":"markdown"},"source":{"18b5f9fe":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom scipy import stats\n%matplotlib inline \npd.options.display.max_columns = 400\nimport statsmodels.api as sm\n","d6966a41":"filname='..\/input\/hsng.csv'\ndf = pd.read_csv(filname)\ndf.head(2)","1cd7e106":"model1_df=df[['OverallQual','BsmtQual','FullBath','TotRmsAbvGrd']]\nmodel2_df=df[['OverallQual','GarageCars','FullBath','TotRmsAbvGrd','MSZoning','Neighborhood','Condition1','Condition2',\n              'HouseStyle','ExterQual','BsmtQual','BsmtCond','KitchenQual','GarageType','GarageFinish','PoolQC',\n              'GrLivArea','GarageArea','TotalBsmtSF']]\nmodel3_df=df[['2ndFlrSF','OpenPorchSF','LotArea','BsmtUnfSF']]","31484aad":"model1_fit = sm.OLS(df['SalePrice'],model1_df,missing='drop').fit()\nmodel1_prediction=model1_fit.predict(model1_df) \n\nmodel2_fit = sm.OLS(df['SalePrice'],model2_df,missing='drop').fit()\nmodel2_prediction=model2_fit.predict(model2_df) \n\nmodel3_fit = sm.OLS(df['SalePrice'],model3_df,missing='drop').fit()\nmodel3_prediction=model3_fit.predict(model3_df) \n\nplt.figure(figsize=(13, 10))\n\nax1 = sns.distplot(df['SalePrice'], hist=False, color=\"r\", label=\"Actual Value\")\nsns.distplot(model1_prediction, hist=False, color=\"b\", label=\"Predicted Values Model 1\" , ax=ax1)\nsns.distplot(model2_prediction, hist=False, color=\"g\", label=\"Predicted Values Model 2\" , ax=ax1)\nsns.distplot(model3_prediction, hist=False, color=\"k\", label=\"Predicted Values Model 3\" , ax=ax1)\n\nplt.title('Actual vs Predicted Values for Price')\nplt.xlabel('SalePrice')\nplt.ylabel('parameter of House')\nplt.show()\nplt.close()","4a674212":"print(\"Model_1_AIC: \",model1_fit.aic)\nprint(\"Model_2_AIC: \",model2_fit.aic)\nprint(\"Model_3_AIC: \",model3_fit.aic)","a41c11a0":"print(\"Model_1_BIC: \",model1_fit.aic)\nprint(\"Model_2_BIC: \",model2_fit.bic)\nprint(\"Model_3_BIC: \",model3_fit.bic)","b25a0ef3":"print(\"R_Sqaured_Model_1: \",model1_fit.rsquared)\nprint(\"R_Sqaured_Model_2: \",model2_fit.rsquared)\nprint(\"R_Sqaured_Model_3: \",model3_fit.rsquared)","a254e230":"print(\"Adj_R_Sqaured_Model_1_BIC: \",model1_fit.rsquared_adj)\nprint(\"Adj_R_Sqaured_Model_2_BIC: \",model2_fit.rsquared_adj)\nprint(\"Adj_R_Sqaured_Model_3_BIC: \",model3_fit.rsquared_adj)","73961429":"print(\"MSE_Model_1: \",mean_squared_error(df['SalePrice'],model1_prediction))\nprint(\"MSE_Model_2: \",mean_squared_error(df['SalePrice'],model2_prediction))\nprint(\"MSE_Model_3: \",mean_squared_error(df['SalePrice'],model3_prediction))","d9f4f11a":"# AIC (Akaike\u2019s Information Criteria)\n* AIC score gives you a way to measure the goodness-of-fit of your model, while at the same time penalizing the model for over-fitting the data.\n* **AIC = -2(log-likelihood) + 2K + (2K(K+1)\/(n-K-1))**\n* n = sample size, (len(df[\u2018SalePrice\u2019])\n* K= number of model parameters, (len(model1_fit.params))\n* Log-likelihood ( mean_squared_error(df[\u2018SalePrice\u2019],model1_prediction) )\n* A lower AIC score indicates superior goodness-of-fit and a lesser tendency to over-fit.","35d12b8f":"# **# Data Science Model Evaluation.**","3d59a937":"# **Summary**\n\n![image.png](attachment:image.png)","9108e35c":"# Adjusted R-SquarR\u00b2\n* Adjusted R\u00b2, also measure to indicate how close the data is to the fitted regression line. However, it penalizes you for adding variables that do not improve your existing model.\n* The adjusted R\u00b2 tells you the percentage of variation explained by only the independent variable that affects the dependent variable.\n* For the linear model, it is always suggested to use Adjusted R-square.\n* **The model with the highest Adjusted R\u00b2 value is a better fit for the data.**","e8964bfb":"**The below graph represents 3 OLS models created using 3 different dataframe.**","75da53f3":"# **Overfitting and Underfitting:**\n**Overfitted:** A statistical model is said to be overfitted when it starts learning from the noise and inaccurate data entries in our data set. This means the model does not categorize the data correctly, because of too much of details and noise. In the above graph, model1 is overfitted.\n**Underfitted:** A statistical model is said to be underfitted when it cannot capture the underlying trend of the data. Accuracy of the underfitted model will be very poor and we need to train the model with more data. In the above, graph model3 is underfitted.","0e88e746":"# BIC (Bayesian Information Criteria):\n* When fitting the model we may end up with an overfitted model by adding more parameters(Model1). So BIC resolves this problem by introducing a penalty term for the number of parameters in the model.\n* The BIC has a heavier penalty term to penalize the model than AIC which always its value larger than the value of the AIC.\n* **BIC = -2 * (log-likelihood) + log(N) * k**\n* n = sample size, (len(df[\u2018SalePrice\u2019])\n* k= number of model parameters, (len(model1_fit.params))\n* Log-likelihood ( mean_squared_error(df[\u2018SalePrice\u2019],model1_prediction) )\n* **A lower BIC score indicates superior model.**","c52e08a9":"# R-Squared\/R\u00b2:\n* R squared, also known as the coefficient of determination, is a measure to indicate how close the data is to the fitted regression line.\n* R2 assumes that every single variable explains the variation in the dependent variable.\n* R-squared = 1 \u2014 (First Sum of Errors \/ Second Sum of Errors)\n* **The model with the highest R-squared value is a better fit for the data.**","51cf926c":"# MSE (Mean Squared Error)\n* The Mean Squared Error measures the average of the squares of errors, that is, the difference between actual value(Y) and the estimated\/Predicted value(\u0177).\n* **The model with the smallest MSE value is a better fit for the data.**"}}