{"cell_type":{"723bb712":"code","f6a9fb0c":"code","97f27f18":"code","8b76e66a":"code","ce6a0de0":"code","8242a399":"code","942ea295":"code","f128dc4f":"code","a682e1c9":"markdown","7498025d":"markdown","5febcd20":"markdown","adf39d12":"markdown","9b10b821":"markdown","af5c3042":"markdown","36605e46":"markdown","ec141a83":"markdown","ba5a6ccb":"markdown","3bf69715":"markdown","e706766f":"markdown","45e09231":"markdown"},"source":{"723bb712":"# for clustering\nfrom scipy.cluster.hierarchy import ward, dendrogram, fcluster, single, complete\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics import silhouette_score\n\n# feature extraction\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk\nfrom nltk.stem import WordNetLemmatizer \n\n# data\nimport pandas as pd\nimport numpy as np\nimport os\nimport json\n\n# viz\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# other\nfrom tqdm import tqdm\nfrom copy import deepcopy\nimport time\n\nINPUT_DIR = '\/kaggle\/input\/CORD-19-research-challenge\/'\nMAX_TITLE_LEN = 70\n\nnltk.download('wordnet')","f6a9fb0c":"# from xhlulu's kernel\n\ndef format_name(author):\n    middle_name = \" \".join(author['middle'])\n    \n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n    \n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n    \n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:\n            name_ls.append(name)\n    \n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n    \n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'], \n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n\n    return \"; \".join(formatted)\n\ndef load_files(dirname):\n    filenames = os.listdir(dirname)\n    raw_files = []\n\n    for filename in tqdm(filenames):\n        filename = dirname + filename\n        file = json.load(open(filename, 'rb'))\n        raw_files.append(file)\n    \n    return raw_files\n\ndef generate_clean_df(all_files):\n    cleaned_files = []\n    \n    for file in tqdm(all_files):\n        features = [\n            file['paper_id'],\n            file['metadata']['title'],\n            format_authors(file['metadata']['authors']),\n            format_authors(file['metadata']['authors'], \n                           with_affiliation=True),\n            format_body(file['abstract']),\n            format_body(file['body_text']),\n            format_bib(file['bib_entries']),\n            file['metadata']['authors'],\n            file['bib_entries']\n        ]\n\n        cleaned_files.append(features)\n\n    col_names = ['paper_id', 'title', 'authors',\n                 'affiliations', 'abstract', 'text', \n                 'bibliography','raw_authors','raw_bibliography']\n\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    clean_df.head()\n    \n    return clean_df","97f27f18":"data_path = INPUT_DIR + \"2020-03-13\/pmc_custom_license\/pmc_custom_license\/\"\nfiles = load_files(data_path)\nprint(\"Loaded {} files\".format(len(files)))\ndf = generate_clean_df(files)\ndf.head()","8b76e66a":"stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\nlemmatizer = WordNetLemmatizer() \ntranslator = str.maketrans('', '', \"!?;(),[]\")\n\ndef tokenize_and_lemmatize_and_stem(text):\n    \"\"\"\n        @param text: document text. Can be a synopsis, full text, or any multi-sentence text.\n        \n        This returns a list of lemmatized, stemmed, lowercase (if applicable) word-level tokens given a \n        multi-sentence text.\n    \"\"\"\n    text = text.translate(translator)\n    tokens = [stemmer.stem(lemmatizer.lemmatize(word.lower())) for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n    return tokens\n\ndef tokenize(text):\n    \"\"\"\n        @param text: document text. Can be a synopsis, full text, or any multi-sentence text.\n        \n        This returns a list of lowercase (if applicable) word-level tokens given a \n        multi-sentence text.\n    \"\"\"\n    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n    return tokens","ce6a0de0":"tfidf_vectorizer = TfidfVectorizer(ngram_range=(3,4), max_df=0.9, min_df=0.005, sublinear_tf=True, tokenizer=tokenize_and_lemmatize_and_stem)\ndata = df\n\ntfidf_matrix = tfidf_vectorizer.fit_transform(data.text)\ndist = 1 - cosine_similarity(tfidf_matrix)\ndist = dist - dist.min() # get rid of some pesky floating point errors that give neg. distance\nlinkage_matrix = ward(dist) # replace with complete, single, or other scipy.cluster.hierarchical algorithms","8242a399":"MAX_COPHENETIC_DIST = max(linkage_matrix[:,2]) * 0.39 # max distance between points to be considered together. can be tuned.\n\nfig, ax = plt.subplots(figsize=(15, 80)) # set size\nax = dendrogram(linkage_matrix, orientation=\"right\", color_threshold=MAX_COPHENETIC_DIST, leaf_font_size=4,\n                labels=data.title.apply(lambda x: x if len(x) < MAX_TITLE_LEN else x[:MAX_TITLE_LEN  - 3] + \"...\").tolist())\n\nplt.tick_params(axis= 'x', which='both',  bottom='off', top='off',labelbottom='off')\n\nplt.tight_layout() #show plot with tight layout\nplt.savefig('ward_clusters_all.png', dpi=300)","942ea295":"def silhouette_k(distance_matrix, linkage_matrix, max_k=20):\n    scores = []\n    for i in range(2, max_k+1):\n        clusters = fcluster(linkage_matrix, i, criterion='maxclust')\n        score = silhouette_score(distance_matrix, clusters, metric='precomputed')\n        print(\"Silhouette score with {} clusters:\".format(i), score)\n        scores.append(score)\n    plt.title(\"Silhouette score vs. number of clusters\")\n    plt.xlabel(\"# of clusters\")\n    plt.ylabel(\"Score (higher is better)\")\n    plt.plot(np.arange(2, max_k+1), scores)\n    plt.show()\n    return scores\n    ","f128dc4f":"_ = silhouette_k(dist, linkage_matrix)","a682e1c9":"## Visualization\n\nDendrogram visualization. We visualize the titles of each paper next to the corresponding leaf to qualitatively evaluate the clustering.\n\nChanging the `color_threshold` kwarg in the call to `dendrogram()` will affect how the clusters are displayed by increasing or decreasing the \nminimum distance necessary for `dendrogram()` to color a cluster differently.","7498025d":"# Limitations\/Assumptions\n\nWhile agglomerative document clustering achieves a somewhat interpretible clustering, and can also partition data into a number of clusters, there are a few key limitations to consider.\n\n* **The how-many-clusters problem.** This mode of clustering gives little insight into whether the clusters discovered actually constitute semantically meaningul topic differences. That is; it is unclear 1) how a human would divide this dataset into topics, and 2) if our discovered  clusters map onto human \"concepts\" well.\n* **Single, hard assignment.** As opposed to other EM-style distribution estimation algorithms or LDA, which yield a \"soft\" assignment by assigning a distribution over a set of latent variables, this method only assigns each document unequivocally to a single cluster.\n* **Distance metric.** Tf-idf has been shown to be successful on various tasks. However, the distance metric here is the cosine-distance between the tf-idf vectors of two documents. Let's call shingles (n-grams)  with high tf-idf scores \"important.\" Here I am assuming 1) that tf-idf ideally captures \"important\" n-grams in the corpus, and that 2) two closely related articles will have similarly co-occuring \"important\" n-grams.\n* **Space.** Calculating a pairwise distance matrix will invariably take O(n^2) memory. This will likely not scale as the set of documents grows.\n* **Feature Extraction.** This is very close to a CBOW (continuous bag of words) style model. The document-based representations here account for very little context (accounted for in the n-gram feature extraction only). To see why this might be a problem, one paper might use a particular set of words in an endorsing context; another paper might use that same turn of phrase in a critical context (e.g. counterargument (?), related work, etc.). This is left out of our feature extraction. ","5febcd20":"# Acknowledgements\n\nSpecial thanks to xhulu for their preprocessing code, and Brandon Rose for [this excellent resource on implementing hierarchical document clustering with sklearn](http:\/\/brandonrose.org\/clustering#Hierarchical-document-clustering). Further analysis of clustering algorithms in Mining of Massive Datasets (Ullman, Rajaravan, and Leskovec 2010) assisted with the creation of this as well.","adf39d12":"# Future Work\n\nA list of future tasks to improve on this work:\n* Tuning parameters (shingle size, TF-IDF thresholds, etc.)\n* Alternative distance metrics for alternative clustering methods (shingles->Jaccard sim., embedding->Euclidean clustering)\n* Alternative features (text vs. abstracts-only, etc.)\n* Cluster quality evaluation (both with and without ground-truth categories)\n\nI am most interested in exploring cluster quality evaluation at this time. Specifically, I am exploring ways to address the limitations above with and without expert knowledge, since I lack the domain knowledge to meaningfully label the articles myself.","9b10b821":"Silhouette score sharply drops off after k=5 on complete clustering and k=10 on Ward clustering. I ultimately opted for Ward clustering because of its intra-cluster variance minimization objective. I then retuned the `MAX_COPHENETIC_DIST` parameter such that the visualization would show 10 clusters. A qualitative high level description of the clusters from top to bottom follows. Please feel free to examine the image yourself.\n\n* **Cluster 1: Zoonotic diseases (magenta).** Mostly concerns diseases originally found in other animals.\n* **Cluster 2: Respiratory viruses, e.g. bocavirus, rhinovirus (cyan).** Concerns viruses that affect the respiratory tract.\n* **Cluster 3: MERS (red).** Self-explanatory.\n* **Cluster 4: indetereminate (green).** Unable to determine a topic based on this cluster.\n* **Cluster 5: indetereminate (black).**\n* **Cluster 6: policy\/institutional considerations (yellow).** Mostly about preparedness, policy evaluation, etc.\n* **Cluster 7: Treatments (magenta).**\n* **Cluster 8: Treatments (cyan).**\n* **Cluster 9: Indeterminate, possibly cell biology-related (red).** Has a lot of cell biology-sounding words that I don't know. I'm unable to describe this cluster qualitatively without domain knoweldge.\n* **Cluster 10: DNA-related (green).** Mostly pertains to DNA transcription, sequencing, and related topics.","af5c3042":"# Clustering\n\nDistance metric: Cosine distance, using (1,3)-shingles generated via TF-IDF feature extraction. Shingles are generated in lowercase, with options to stem or not stem the tokens.","36605e46":"# A hierarchical agglomerative clustering approach for CORD-19 article categorization\n\nMany other kernels published in the last few days have focused on LDA for topic detection and made significant advances in that area. This is a method that presents easily recoverable clusters and an intuitive visualization. Unsupervised clustering techniques have been used effectively to categorize and\/or classify other data.\n\nWe choose a hierarchical approach because documents are non-lattice, non-real valued data that do not live in Euclidean space. Conventional clustering methods would require some Euclidean embedding for documents, which would require even more validation. Furthermore, this method accounts for the overlapping\/concentric structure of topic clusters in document-based data. In this notebook, we extract TF-IDF features to get a representation of the document at a high level instead, and then apply a hierarchical clustering algorithm using the Ward objective function.\n\n","ec141a83":"# Analysis\n\nWe now take the raw clusters achieved and use the silhouette score and elbow method to evaluate.","ba5a6ccb":"## Hierarchical clustering\n\nWe experiment with multiple hierarchical clustering criteria:\n* Ward (maximize the decrease in intra-cluster variance with respect to all clusters)\n* Single (move point to cluster of nearest neighbor)\n* Complete (move point to the cluster with minimum distance to farthest point)\n\nWard clustering yielded reasonable results, as did complete clustering. Many cases Using the single criterion results in a degenerate case.","3bf69715":"We use a TF-IDF Vectorizer to generate (1,3)-shingles and create a TF-IDF matrix. The current threshold includes only terms with document frequencies in the interval (0.005,0.8), though this can be tuned. The idea is that if a term has document frequency outside that interval, it is likely to be extremely obscure and therefore irrelevant, or ubiquitous and also irrelevant (think stopwords). We use the sublinear tf scaling technique to \"normalize\" term frequencies on a log-scale. Using the TF-IDF matrix, we then use the cosine distance metric (1 - cosine similarity) to create a sparse pairwise distance matrix. We pass this into the hierarchical clustering algorithm to receive a linkage matrix for plotting.","e706766f":"# Setup and Utility functions","45e09231":"# Loading Data\n\nWe load data on the selected portion of the dataset."}}