{"cell_type":{"a995f52d":"code","2553d0ce":"code","b965e525":"code","25103664":"code","ed796dfd":"code","a6c4d9c2":"code","287c5ca7":"code","6f405fda":"code","a7cc8e95":"code","d6eddbc3":"code","4fb50304":"code","962afc0d":"code","9ae4ff77":"code","7ef6c713":"code","ee014810":"code","aa08a581":"code","a1565ef8":"code","1ba2503c":"code","623c7669":"code","8bb2b211":"code","745630c5":"code","4f7772e6":"code","28076a20":"code","a90cebfe":"code","d75d6770":"code","52f55258":"code","e0d026ad":"code","7bdd8d2d":"code","94308e4c":"code","f1d86380":"code","5fbdac14":"code","8d65fa5b":"code","2229d234":"code","26d4f571":"code","4274e105":"code","587281cd":"code","8536d9ee":"code","71b8ddf7":"code","2cc28ae7":"code","614d9608":"code","6a1aadba":"code","d96c134d":"code","5dcc463d":"code","e90e9d2f":"code","d10e2102":"code","48b3c878":"code","2103a478":"code","f2f76183":"code","029ad1f1":"code","08c9d919":"code","1306252d":"code","dd15fe95":"code","6ba3c3ab":"code","22bc9f3a":"code","657ade32":"code","50186f5c":"code","4a2c7864":"code","285d77c1":"markdown","4f570ea5":"markdown","bf5b5eea":"markdown","98d38b5d":"markdown","8f7b83ab":"markdown","fa177fc1":"markdown","343a2eca":"markdown","4abeab1e":"markdown","5c49e32b":"markdown","ee7af252":"markdown","02e11a5c":"markdown","4738e4be":"markdown","75ae96d1":"markdown","76cc1200":"markdown","9b59785c":"markdown","09f45457":"markdown","408713c7":"markdown","eaafd516":"markdown","73b7ce50":"markdown","2cf8d9ed":"markdown","859972cf":"markdown","e7a93db0":"markdown","095b4687":"markdown","601a16c6":"markdown","b866d4f5":"markdown","b5fcd7ec":"markdown"},"source":{"a995f52d":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set() # setting seaborn default for plots\n\n# Input data files are available in the read-only \"..\/input\/\" directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2553d0ce":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n","b965e525":"print(\"train data shape : \", train_data.shape)\nprint(\"test data shape : \", test_data.shape)\ntrain_data.head()","25103664":"train_data.info()","ed796dfd":"test_data.info()","a6c4d9c2":"train_data.isnull().sum()","287c5ca7":"test_data.isnull().sum()","6f405fda":"def bar_chart(feature):\n    survived = train_data[train_data['Survived']==1][feature].value_counts()\n    dead = train_data[train_data['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived,dead])\n    df.index = ['Survived','Dead']\n    df.plot(kind='bar',stacked=True, figsize=(10,5))\n","a7cc8e95":"bar_chart('Sex')","d6eddbc3":"bar_chart('Pclass')","4fb50304":"bar_chart('SibSp')","962afc0d":"bar_chart('Parch')","9ae4ff77":"bar_chart('Embarked')","7ef6c713":"# Combining train and test data to do feature engineering\n\ntrain_test = [train_data, test_data]\n\nfor data in train_test:\n    data['Title'] = data['Name'].str.split(', ').str[1].str.split('.').str[0]","ee014810":"train_data['Title'].value_counts()","aa08a581":"# We mainly want the first three titles for our analysis as they are in majority\n\nfor dataset in train_test:\n    top_3 = [x for x in dataset.Title.value_counts().sort_values(ascending=False).head(3).index]\n    for label in top_3:\n        dataset[label] = np.where(dataset['Title']==label,1,0)","a1565ef8":"train_data.head()","1ba2503c":"# Function to delete unnecessary feature from dataset\n\ndef drop_columns(df, col):\n    df.drop(col, axis=1, inplace=True)","623c7669":"for dataset in train_test:\n    drop_columns(dataset, 'Name')\n    drop_columns(dataset, 'Title')","8bb2b211":"train_data.head()","745630c5":"#Converting and concatenating sex to binary using one hot encoding\n\ntrain_data = pd.concat([train_data, pd.get_dummies(train_data['Sex'], prefix='gender')],axis=1)\ntest_data = pd.concat([test_data, pd.get_dummies(test_data['Sex'], prefix='gender')],axis=1)\n","4f7772e6":"train_test = [train_data, test_data]\nfor dataset in train_test:\n    drop_columns(dataset, 'Sex')\n    drop_columns(dataset, 'gender_male')","28076a20":"test_data.head()","a90cebfe":"# Replacing missing values with the median age grouped by title : 177, 86\n\ntrain_data['Age'].fillna(train_data.groupby(\"Mr\")[\"Age\"].transform(\"median\"), inplace=True)\ntrain_data['Age'].fillna(train_data.groupby(\"Mrs\")[\"Age\"].transform(\"median\"), inplace=True)\ntrain_data['Age'].fillna(train_data.groupby(\"Miss\")[\"Age\"].transform(\"median\"), inplace=True)\n\ntest_data['Age'].fillna(test_data.groupby(\"Mr\")[\"Age\"].transform(\"median\"), inplace=True)\ntest_data['Age'].fillna(test_data.groupby(\"Mrs\")[\"Age\"].transform(\"median\"), inplace=True)\ntest_data['Age'].fillna(test_data.groupby(\"Miss\")[\"Age\"].transform(\"median\"), inplace=True)\n","d75d6770":"Pclass1 = train_data[train_data['Pclass']==1]['Embarked'].value_counts()\nPclass2 = train_data[train_data['Pclass']==2]['Embarked'].value_counts()\nPclass3 = train_data[train_data['Pclass']==3]['Embarked'].value_counts()\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class','2nd class', '3rd class']\ndf.plot(kind='bar',stacked=True, figsize=(10,5))\n","52f55258":"# Based on above observation , we can conveniently replace the missing values of embarked with S.\n\ntrain_data['Embarked'] = train_data['Embarked'].fillna('S')","e0d026ad":"#Converting and concatenating embarked to binary using one hot encoding\n\ntrain_data = pd.concat([train_data, pd.get_dummies(train_data['Embarked'], prefix='em')],axis=1)\ntest_data = pd.concat([test_data, pd.get_dummies(test_data['Embarked'], prefix='em')],axis=1)\n\ndrop_columns(train_data, 'em_Q')\ndrop_columns(test_data, 'em_Q')\ndrop_columns(train_data, 'Embarked')\ndrop_columns(test_data, 'Embarked')","7bdd8d2d":"test_data.head()","94308e4c":"# replacing missing Fare with median fare for each Pclass\ntest_data[\"Fare\"].fillna(test_data.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)","f1d86380":"train_data.Cabin.value_counts()","5fbdac14":"#Getting the first alphabet of each cabin\ntrain_test = [train_data, test_data]\nfor dataset in train_test:\n    dataset['Cabin'] = dataset['Cabin'].str[:1]","8d65fa5b":"Pclass1 = train_data[train_data['Pclass']==1]['Cabin'].value_counts()\nPclass2 = train_data[train_data['Pclass']==2]['Cabin'].value_counts()\nPclass3 = train_data[train_data['Pclass']==3]['Cabin'].value_counts()\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class','2nd class', '3rd class']\ndf.plot(kind='bar',stacked=True, figsize=(10,5))","2229d234":"train_data['Cabin'].value_counts()","26d4f571":"#Dropping cabin. Need to decide how to replace NaN values\n\ndrop_columns(train_data, 'Cabin')\ndrop_columns(test_data, 'Cabin')","4274e105":"#Converting and concatenating Pclass to binary using one hot encoding\n\ntrain_data = pd.concat([train_data, pd.get_dummies(train_data['Pclass'], prefix='class')],axis=1)\ntest_data = pd.concat([test_data, pd.get_dummies(test_data['Pclass'], prefix='class')],axis=1)","587281cd":"drop_columns(train_data, 'Pclass')\ndrop_columns(test_data, 'Pclass')","8536d9ee":"train_data.head()","71b8ddf7":"#Adding all the parents, children, spouse and siblings to count the no of members in the family on board\n\ntrain_data[\"FamilySize\"] = train_data[\"SibSp\"] + train_data[\"Parch\"] + 1\ntest_data[\"FamilySize\"] = test_data[\"SibSp\"] + test_data[\"Parch\"] + 1\n","2cc28ae7":"#Dropping the unnecessary features\n\nfeatures_drop = ['Ticket', 'SibSp', 'Parch']\ntrain_data = train_data.drop(features_drop, axis=1)\ntest_data = test_data.drop(features_drop, axis=1)\ntrain_data = train_data.drop('PassengerId', axis=1)","614d9608":"train_data.head()","6a1aadba":"#Checking corelation matrix\n\ntrain_data.corr()","d96c134d":"#Segregating features and label\n\ny = train_data['Survived']\ntrain_data = train_data.drop('Survived', axis=1)","5dcc463d":"train_data.head()","e90e9d2f":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import plot_precision_recall_curve\nimport matplotlib.pyplot as plt\n\n\n","d10e2102":"#Baseline Model : Probability of not surviving which is the majority class\n\nSurvival_prob = (y==0).sum() \/ len(train_data)\nSurvival_prob","48b3c878":"k_fold = KFold(n_splits=10, shuffle=True, random_state=0)","2103a478":"model = GaussianNB()\nscore = cross_val_score(model, train_data, y, cv=k_fold, n_jobs=1, scoring='accuracy')\nprint(score)\nround(np.mean(score)*100, 2)","f2f76183":"model = tree.DecisionTreeClassifier(random_state=0)\nscore = cross_val_score(model, train_data, y, cv=k_fold, n_jobs=1, scoring='accuracy')\nprint(score)\nround(np.mean(score)*100, 2)","029ad1f1":"model = RandomForestClassifier(n_estimators=20, max_depth=8, random_state=0)\nscore = cross_val_score(model, train_data, y, cv=k_fold, n_jobs=1, scoring='accuracy')\nprint(score)\nround(np.mean(score)*100, 2)","08c9d919":"model = KNeighborsClassifier(n_neighbors = 13)\nscore = cross_val_score(model, train_data, y, cv=k_fold, n_jobs=1, scoring='accuracy')\nprint(score)\nround(np.mean(score)*100, 2)","1306252d":"model = svm.SVC(kernel='linear', random_state=0)\nscore = cross_val_score(model, train_data, y, cv=k_fold, n_jobs=1, scoring='accuracy')\nprint(score)\nround(np.mean(score)*100, 2)","dd15fe95":"model = LogisticRegression(random_state=0)\nscore = cross_val_score(model, train_data, y, cv=k_fold, n_jobs=1, scoring='accuracy')\nprint(score)\nround(np.mean(score)*100, 2)","6ba3c3ab":"print(train_data.shape)\nprint(test.shape)\nprint(test_data.shape)","22bc9f3a":"model = RandomForestClassifier(n_estimators=20, max_depth=8, random_state=0)\nmodel.fit(train_data, y)\n\ntest = test_data.drop(\"PassengerId\", axis=1).copy()\n\nprediction = model.predict(test)","657ade32":"print(test)\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_data[\"PassengerId\"],\n        \"Survived\": prediction\n    })\n\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","50186f5c":"submission = pd.read_csv('submission.csv')\nsubmission.head()","4a2c7864":"#Rough\n\nconf_mat = confusion_matrix(y, y_pred)\nprint(conf_mat)\ntrain_accuracy = accuracy_score(y.to_numpy(), y_pred)\nprint(train_accuracy)\n\nprecision = precision_score(y, y_pred, average='weighted')\nrecall = recall_score(y, y_pred, average='weighted')\nprint(precision)\nprint(recall)\n\n\n#average_precision = average_precision_score(y, y_score)\n#disp = plot_precision_recall_curve(model, X, y)\n#disp.ax_.set_title('Precision-Recall curve: ''AP={0:0.2f}'.format(average_precision))\n\n\n","285d77c1":"### Fields Description:\n\nSurvived -> 1:Yes, 0:No      \nPclass(Passenger Class) -> 1:1st class, 2:2nd class, 3:3rd class     \nSibSp(Siblings\/Spouse no) -> in range of 0-5    \nParch (Parents\/Children no) -> in range of 0-6    \nTicket (ticket no)\nCabin(Cabin no)      \nEmbarked(Port of Embarkation) -> C:Cherbourg, Q:Queenstown, S:Southampton      \n\n### Train data missing values : \nAge : 177      \nCabin : 687      \nEmbarked : 2    \n\n\n### Test data missing values : \nAge : 86     \nCabin : 327     \nFare : 1     \n","4f570ea5":"### 7. Logistic Regression","bf5b5eea":"### 6. SVM","98d38b5d":"### 3. Age","8f7b83ab":"### 6. Pclass","fa177fc1":"### 4. Random Forest","343a2eca":"## About Data","4abeab1e":"Confirms that first class passengers are more likely to survive","5c49e32b":"### 4. Fare","ee7af252":"### 7. Family size","02e11a5c":"Shows that a person with no sibling\/ spouse is more likely to survive","4738e4be":"Similar observation as that of SibSp. People having no parent or child is more likely to survive.","75ae96d1":"### Bar Chart for Categorical Features\n* Sex      \n* Pclass        \n* SibSp     \n* Parch     \n* Embarked      \n* Cabin      \n\n","76cc1200":"### 5. Cabin","9b59785c":"## Modelling","09f45457":"### 1. k-fold Cross Validation","408713c7":"### 3. Embarked","eaafd516":"### 2. Naive Bayes","73b7ce50":"### 5. kNN","2cf8d9ed":"### 3. Decision Trees","859972cf":"## Feature Engineering\n\n### 1. Name","e7a93db0":"It shows that females are more likely to survive than males","095b4687":"## Data Exploration and Visualization","601a16c6":"### 2. Sex","b866d4f5":"People embarked from S is more likely to survive, followed by C.","b5fcd7ec":"## Testing"}}