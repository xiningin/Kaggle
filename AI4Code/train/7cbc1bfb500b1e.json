{"cell_type":{"4fec7bd5":"code","4e858430":"code","148a8366":"code","efa92d7b":"code","162e3839":"code","4a99bac9":"code","f7fd7f48":"code","aa632ffe":"code","fcecbb01":"code","93311451":"code","3b4e7457":"code","57869693":"code","363a556c":"code","6cace91e":"code","bea6a890":"code","37b03e52":"code","b715a6a0":"code","08ef766e":"code","8ee32fed":"code","4419e443":"code","9efe711a":"code","800b159f":"code","7c8af0c9":"code","112a4f0b":"code","7c71d00a":"code","4edca543":"code","159fee93":"code","898d5b7a":"code","b7c34e18":"code","a59bb61e":"code","75f41033":"code","d9eea316":"code","8b926cb8":"code","ba009c50":"code","4efb6181":"code","f98795a6":"code","69baf0da":"code","c9cb7a82":"code","15f309eb":"code","73ccc858":"code","ebb2da32":"code","ebd7fe86":"code","41ce63e5":"code","bce625f3":"code","85896426":"code","a5458de2":"code","ebb1df80":"code","291a2236":"code","3170c504":"code","ee67ac2b":"code","274ca4fe":"code","0d0b39c4":"code","1b7a9da0":"code","683fd277":"code","6972f016":"code","d401b39b":"code","4870453c":"code","b1afb28e":"code","b4e3ec9d":"code","767f4f24":"markdown","b63e1ad3":"markdown","3d104004":"markdown","c604a385":"markdown","f30dffb9":"markdown","b39ada70":"markdown","53aed466":"markdown","eab5d95c":"markdown"},"source":{"4fec7bd5":"#### Link to Dataset on Kaggle: https:\/\/www.kaggle.com\/amitranjan01\/edmunds-data-analysis-cross-continent-review\n    \n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport os # accessing directory structure\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n#path for kaggle notebook '\/kaggle\/input'\nimport os\nfor dirname, _, filenames in os.walk('..\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n# List files available Only run on my local NoteBook\nprint(os.listdir(\"..\/input\"))","4e858430":"#Analyze following three Cars from three different region.\n#\/kaggle\/input\/edmundsconsumer-car-ratings-and-reviews\/Scrapped_Car_Review_Chevrolet.csv\n#\/kaggle\/input\/edmundsconsumer-car-ratings-and-reviews\/Scraped_Car_Review_mercedes-benz.csv\n#\/kaggle\/input\/edmundsconsumer-car-ratings-and-reviews\/Scrapped_Car_Reviews_Toyota.csv\n\n# specify 'None' if want to read whole file\nnRowsRead = 10000 \n\n# Scrapped_Car_Reviews_Toyota.csv\ndf_J = pd.read_csv('\/kaggle\/input\/edmundsconsumer-car-ratings-and-reviews\/Scrapped_Car_Reviews_Toyota.csv', delimiter=',', nrows = nRowsRead)\n#df_J = pd.read_csv('Scrapped_Car_Reviews_Toyota.csv', delimiter=',', nrows = nRowsRead)\ndf_J.dataframeName = 'Scrapped_Car_Reviews_Toyota.csv'\nnRow, nCol = df_J.shape\ndf_J_orig = df_J.copy(deep=True)\nprint(f'There are {nRow} rows and {nCol} columns for Toyota')\n\n# Scrapped_Car_Reviews_Toyota.csv\ndf_E = pd.read_csv('\/kaggle\/input\/edmundsconsumer-car-ratings-and-reviews\/Scraped_Car_Review_mercedes-benz.csv', delimiter=',', nrows = nRowsRead)\n#df_E = pd.read_csv('Scraped_Car_Review_mercedes-benz.csv', delimiter=',', nrows = nRowsRead)\ndf_E.dataframeName = 'Scraped_Car_Review_mercedes-benz.csv'\ndf_E_orig = df_E.copy(deep=True)\nnRow1, nCol1 = df_E.shape\nprint(f'There are {nRow1} rows and {nCol1} columns for Mercedes')\n\n# Scrapped_Car_Reviews_Toyota.csv\ndf_A = pd.read_csv('\/kaggle\/input\/edmundsconsumer-car-ratings-and-reviews\/Scrapped_Car_Review_Chevrolet.csv', delimiter=',', nrows = nRowsRead)\n#df_A = pd.read_csv('Scrapped_Car_Review_Chevrolet.csv', delimiter=',', nrows = nRowsRead)\ndf_A.dataframeName = 'Scrapped_Car_Review_Chevrolet.csv'\ndf_A_orig = df_A.copy(deep=True)\nnRow2, nCol2 = df_A.shape\nprint(f'There are {nRow2} rows and {nCol2} columns for Chevrolet')","148a8366":"#Analyze the Columns for the datset\n\nprint(\"Columns for Japense Car: \", df_J.columns)\nprint(\"Columns for European Car: \", df_E.columns)\nprint(\"Columns for American Car: \", df_A.columns)","efa92d7b":"#Analyze the Columns for the datset\n\nprint(\"Info for Japense Car: \", df_J.info())\nprint(\"Info for European Car: \", df_E.info())\nprint(\"Info for American Car: \", df_A.info())","162e3839":"#Print head to verify the data\nprint(\"Head for Japense Car: \", df_J.head())\nprint(\"Head for European Car: \", df_E.head())\nprint(\"Head for American Car: \", df_A.head())","4a99bac9":"print(\"Data Types for Japense Car: \", df_J.dtypes)\nprint(\"Data Types for European Car: \", df_E.dtypes)\nprint(\"Data Types for American Car: \", df_A.dtypes)","f7fd7f48":"#Extract the Vechile Make Year from the Vechile Title\n\ndf_J['Make_Year'] = df_J['Vehicle_Title'].str[:4]\ndf_J['Make_Year'] = df_J['Make_Year'].fillna(method ='ffill')\nprint(df_J['Make_Year'])  \n\ndf_E['Make_Year'] = df_E['Vehicle_Title'].str[:4]\ndf_E['Make_Year'] = df_E['Make_Year'].fillna(method ='ffill')\n\nprint(df_E['Make_Year'])  \n\ndf_A['Make_Year'] = df_A['Vehicle_Title'].str[:4]\ndf_A['Make_Year'] = df_A['Make_Year'].fillna(method ='ffill')\n\nprint(df_A['Make_Year'])  ","aa632ffe":"# Extract the Review Date from Review\ndf_J['Review_Date_D'] = df_J['Review_Date'].str[4:12]\ndf_J['Review_Date_D'] = df_J['Review_Date_D'].fillna(method ='ffill')\ndf_J['Review_Date_D'] = pd.to_datetime(df_J['Review_Date_D'], errors='coerce')\nprint(df_J['Review_Date_D'])  \n\ndf_E['Review_Date_D'] = df_E['Review_Date'].str[4:13]\ndf_E['Review_Date_D'] = df_E['Review_Date_D'].fillna(method ='ffill')\ndf_E['Review_Date_D'] = pd.to_datetime(df_E['Review_Date_D'],  errors='coerce')\nprint(df_E['Review_Date_D']) \n\ndf_A['Review_Date_D'] = df_A['Review_Date'].str[4:13]\ndf_A['Review_Date_D'] = df_A['Review_Date_D'].fillna(method ='ffill')\ndf_A['Review_Date_D'] = pd.to_datetime(df_A['Review_Date_D'], errors='coerce')\nprint(df_A['Review_Date_D']) ","fcecbb01":"#Drop data not required for my analysis. In Place update to exisiting Dataframes\ndf_E.drop(['Unnamed: 0', 'Review_Date', 'Vehicle_Title', 'Review_Title', 'Review'], axis=1, inplace=True)\ndf_A.drop(['Unnamed: 0', 'Review_Date', 'Vehicle_Title', 'Review_Title', 'Review'], axis=1, inplace=True)\ndf_J.drop(['Unnamed: 0', 'Review_Date', 'Vehicle_Title', 'Review_Title', 'Review'], axis=1, inplace=True)","93311451":"print(df_E.info())\nprint(\"99th %tile: \", df_E[\"Rating\"].quantile(0.99))\nprint(df_E.describe())","3b4e7457":"#Replace the missig entries in Rating column with Mean of Rating\ndf_E['Rating'].fillna(df_E['Rating'].mean(), inplace=True)\ndf_A['Rating'].fillna(df_E['Rating'].mean(), inplace=True)\ndf_J['Rating'].fillna(df_E['Rating'].mean(), inplace=True)","57869693":"#Replace the missing authoer with anonymous as author\ndf_E['Author_Name'].fillna('anonymous', inplace=True)\ndf_A['Author_Name'].fillna('anonymous', inplace=True)\ndf_J['Author_Name'].fillna('anonymous', inplace=True)","363a556c":"#Add a column representing region where the car belongs. Required to know which data came from which dataframe\ndf_E['Origin_Region'] = \"North America\"\ndf_A['Origin_Region'] = \"Eurpoe\"\ndf_J['Origin_Region'] = \"Japan\"","6cace91e":"#After replacing the mean,make sure that the rating is not impacted, we can see 99th %tile:  4.875 is unchanged.\nprint(df_E.info())\nprint(\"99th %tile: \", df_E[\"Rating\"].quantile(0.99))\nprint(df_E.describe())","bea6a890":"print(df_A.info())\nprint(\"99th %tile: \", df_A[\"Rating\"].quantile(0.99))\nprint(df_A.describe())","37b03e52":"print(df_J.info())\nprint(\"99th %tile: \", df_J[\"Rating\"].quantile(0.99))\nprint(df_J.describe())","b715a6a0":"#Append all three dataset\ndf = df_E.append(df_A).append(df_J)\nprint(df.shape)\nprint(df.head(10))\nprint(df.info())","08ef766e":"# convert the floating Rating to Integer\ndf['Rating'] = df['Rating'].round(0)\ndf.head()","8ee32fed":"#Plot the Rating to see how the data is distributed\ndf['Rating'].plot(kind = 'hist', bins = 100)\nplt.show()\n#Below Data shows that 5 rating is so frequent that other ratings are not vsisble on graph, \n#so chaging the scale of the graph","4419e443":"#Plot the Rating to see how the data is distributed - Using Log Scale\ndf['Rating'].plot(kind = 'hist', bins = 100)\nplt.yscale('log')\nplt.show()","9efe711a":"df[df['Origin_Region'] == \"North America\"]['Rating'].plot(kind = 'hist')\nplt.yscale('log')\nplt.show()","800b159f":"df[df['Origin_Region'] == \"Eurpoe\"]['Rating'].plot(kind = 'hist')\nplt.yscale('log')\nplt.show()","7c8af0c9":"df[df['Origin_Region'] == \"Japan\"]['Rating'].plot(kind = 'hist')\nplt.yscale('log')\nplt.show()","112a4f0b":"plt.figure(figsize=(20,10))\nplt.scatter(df['Make_Year'], df['Rating'] )\nplt.show()\n#Scatter plot doesn't relveal any specific data aspect.","7c71d00a":"df['Rating'].groupby(df['Author_Name']).count().nlargest(10)\n#Anomalies found, top three revewer has reviewed more than 2000 reviews each. \n#Where as the avarage review per reviewe is just 4\n#If we drop top reviewes, we will be left with very small set of data. So we will analyze the data in different section.","4edca543":"#Create a Backup copy of data so that we can process it and analyze it later.\ndf_copy = df.copy(deep=True)","159fee93":"df.drop(df.loc[df['Author_Name']=='anonymous'].index, inplace=True)\ndf.drop(df.loc[df['Author_Name']=='HD mike'].index, inplace=True)\ndf.drop(df.loc[df['Author_Name']=='Dave761'].index, inplace=True)\ndf.drop(df.loc[df['Author_Name']=='Avalon Driver'].index, inplace=True)","898d5b7a":"#Total dataset is reduced form 30,000 to 348\ndf.shape","b7c34e18":"df['Rating'].groupby(df['Author_Name']).count().nlargest(10)","a59bb61e":"q = df[\"Rating\"].quantile(0.99)\nprint(q)\ndf[df[\"Rating\"] < q].count()\n#Total of 84 rating are other than 5. Which is about 20% of total left data","75f41033":"df['Rating'].groupby(df['Author_Name']).describe()","d9eea316":"df_copy['Rating'].groupby(df_copy['Author_Name']).count().nlargest(10)","8b926cb8":"#Plot the Rating to see how the data is distributed - Using Log Scale\ndf['Rating'].plot(kind = 'hist', bins = 100)\nplt.yscale('log')\nplt.show()\n# The new rating distribution seems very similar to what was in original dataframe.","ba009c50":"#Now lets analyze the top reviewers rating pattern\nprint(df_copy[df_copy['Author_Name'] == \"Avalon Driver \"]['Origin_Region'].count())\ndf_copy[df_copy['Author_Name'] == \"Avalon Driver \"]['Origin_Region'].value_counts()\n#Seems like Avalon Driver has only reviewd Japanese car and have given all rating of 5","4efb6181":"#Now lets analyze the top reviewers rating pattern\nprint(df_copy[df_copy['Author_Name'] == \"Avalon Driver \"]['Rating'].count())\ndf_copy[df_copy['Author_Name'] == \"Avalon Driver \"]['Rating'].value_counts()\n#Seems like Avalon Driver has only reviewd Japanese car and have given all rating of 5","f98795a6":"#Now lets analyze the top reviewers rating pattern\nprint(df_copy[df_copy['Author_Name'] == \"Avalon Driver \"]['Make_Year'].count())\ndf_copy[df_copy['Author_Name'] == \"Avalon Driver \"]['Make_Year'].value_counts()\n#Seems like Avalon Driver has given all reviews in one year whcih is 2002","69baf0da":"#Now lets analyze the top reviewers rating pattern\nprint(df_copy[df_copy['Author_Name'] == \"Dave761 \"]['Origin_Region'].count())\ndf_copy[df_copy['Author_Name'] == \"Dave761 \"]['Origin_Region'].value_counts()\n#Seems like Avalon Driver has only reviewd Japanese car and have given all rating of 5","c9cb7a82":"#Now lets analyze the top reviewers rating pattern\nprint(df_copy[df_copy['Author_Name'] == \"Dave761 \"]['Rating'].count())\ndf_copy[df_copy['Author_Name'] == \"Dave761 \"]['Rating'].value_counts()\n#Seems like Avalon Driver has only reviewd Japanese car and have given all rating of 5","15f309eb":"#Now lets analyze the top reviewers rating pattern\nprint(df_copy[df_copy['Author_Name'] == \"Dave761 \"]['Make_Year'].count())\ndf_copy[df_copy['Author_Name'] == \"Dave761 \"]['Make_Year'].value_counts()\n#Seems like Avalon Driver has given all reviews in one year whcih is 2002","73ccc858":"#ax = df_copy.plot(x=\"Make_Year\", y=\"Rating\", kind=\"bar\")\n#df_copy.plot(x=\"Make_Year\", y=\"Rating\", kind=\"bar\", ax=ax, color=\"C2\")\n#plt.show()","ebb2da32":"#Load Original Data (unmodified)\nprint(df_A_orig.info())\nprint(df_J_orig.info())\nprint(df_E_orig.info())","ebd7fe86":"df_A_orig1 = df_A_orig.copy(deep=True)\ndf_A_orig1.dropna(subset = ['Rating'], inplace=True)\nprint(df_A_orig1.head())\nprint(df_A_orig1.shape)\ndf_A_orig1.info()\n#If I drop all NA, then I am only left with 51 rows of data out of 10,000 rows. So that is not best option. \n#Lets take another approach","41ce63e5":"df_E_orig1 = df_E_orig.copy(deep=True)\ndf_E_orig1.dropna(subset = ['Rating'], inplace=True)\nprint(df_E_orig1.head())\nprint(df_E_orig1.shape)\ndf_E_orig1.info()\n#If I drop all NA, then I am only left with 51 rows of data out of 10,000 rows. So that is not best option. \n#Lets take another approach","bce625f3":"df_J_orig1 = df_J_orig.copy(deep=True)\ndf_J_orig1.dropna(subset = ['Rating'], inplace=True)\nprint(df_J_orig1.head())\nprint(df_J_orig1.shape)\ndf_J_orig1.info()\n#If I drop all NA, then I am only left with 51 rows of data out of 10,000 rows. So that is not best option. \n#Lets take another approach","85896426":"#Create new column for Country Of Origin\n#Add a column representing region where the car belongs. Required to know which data came from which dataframe\ndf_E_orig1['Origin_Region'] = \"North America\"\ndf_A_orig1['Origin_Region'] = \"Eurpoe\"\ndf_J_orig1['Origin_Region'] = \"Japan\"","a5458de2":"# Extract the Review Date from Review\ndf_J_orig1['Review_Date_D'] = df_J_orig1['Review_Date'].str[4:12]\n#df_J_orig1['Review_Date_D'] = df_J_orig1['Review_Date_D'].fillna(method ='ffill')\ndf_J_orig1['Review_Date_D'] = pd.to_datetime(df_J_orig1['Review_Date_D'], errors='coerce')\nprint(df_J_orig1['Review_Date_D'])  \n\ndf_E_orig1['Review_Date_D'] = df_E_orig1['Review_Date'].str[4:13]\n#df_E_orig1['Review_Date_D'] = df_E_orig1['Review_Date_D'].fillna(method ='ffill')\ndf_E_orig1['Review_Date_D'] = pd.to_datetime(df_E_orig1['Review_Date_D'],  errors='coerce')\nprint(df_E_orig1['Review_Date_D']) \n\ndf_A_orig1['Review_Date_D'] = df_A_orig1['Review_Date'].str[4:13]\n#df_A_orig1['Review_Date_D'] = df_A_orig1['Review_Date_D'].fillna(method ='ffill')\ndf_A_orig1['Review_Date_D'] = pd.to_datetime(df_A_orig1['Review_Date_D'], errors='coerce')\nprint(df_A_orig1['Review_Date_D']) ","ebb1df80":"#Extract the Vechile Make Year from the Vechile Title\n\ndf_J_orig1['Make_Year'] = df_J_orig1['Vehicle_Title'].str[:4]\n#df_J_orig1['Make_Year'] = df_J_orig1['Make_Year'].fillna(method ='ffill')\nprint(df_J_orig1['Make_Year'])  \n\ndf_E_orig1['Make_Year'] = df_E_orig1['Vehicle_Title'].str[:4]\n#df_E_orig1['Make_Year'] = df_E_orig1['Make_Year'].fillna(method ='ffill')\nprint(df_E_orig1['Make_Year'])  \n\ndf_A_orig1['Make_Year'] = df_A_orig1['Vehicle_Title'].str[:4]\n#df_A_orig1['Make_Year'] = df_A_orig1['Make_Year'].fillna(method ='ffill')\n\nprint(df_A_orig1['Make_Year'])  ","291a2236":"df_tot = df_J_orig1.append(df_A_orig1).append(df_E_orig1)\ndf_tot.info()","3170c504":"#Plot the Rating to see how the data is distributed - Using Log Scale\ndf_tot['Rating'].plot(kind = 'hist', bins = 15)\nplt.yscale('log')\nplt.show()\n# The new rating distribution seems more like a continous distribution.","ee67ac2b":"df_tot['Rating'].groupby(df_tot['Author_Name']).count().nlargest(10)\n#even authors have good reviews distribution with more even review frqquency","274ca4fe":"df_tot.info()","0d0b39c4":"df_tot_new = df_tot[['Review_Date_D','Author_Name','Rating','Make_Year','Origin_Region']]","1b7a9da0":"df_tot_new.info()","683fd277":"grp = df_tot_new.groupby(['Author_Name','Origin_Region','Make_Year'])['Author_Name','Make_Year','Origin_Region','Rating','Review_Date_D']","6972f016":"grp.describe(include='all')","d401b39b":"df_tot_new['Make_Year'] =  pd.to_datetime(df_tot_new['Make_Year'], errors='coerce')","4870453c":"import numpy as np\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8,12))\n#df_tot_new = df_tot[['Review_Date_D','Author_Name','Rating','Make_Year','Origin_Region']]\n\ntw = df_tot_new[df_tot_new['Origin_Region'] == 'Japan']['Make_Year']\ntw.sort_values()\ntm = df_tot_new[df_tot_new['Origin_Region'] == 'Japan']['Rating']\n\nsw = df_tot_new[df_tot_new['Origin_Region'] == 'Eurpoe']['Make_Year']\nsm = df_tot_new[df_tot_new['Origin_Region'] == 'Eurpoe']['Rating']\nsw.sort_values()\n\nkw = df_tot_new[df_tot_new['Origin_Region'] == 'North America']['Make_Year']\nkm = df_tot_new[df_tot_new['Origin_Region'] == 'North America']['Rating']\nkw.sort_values()\n\nplt.subplot(3, 1, 1)\nplt.scatter(tw, tm)\nplt.xlabel('Model Year')\nplt.ylabel('Rating')\nplt.title('Japan car rating over year')\nplt.grid(True)\n\n\nplt.subplot(3, 1, 2)\n\nplt.scatter(sw, sm)\nplt.xlabel('Model Year')\nplt.ylabel('Rating')\nplt.title('Eurpoe car rating over year')\nplt.grid(True)\n\nplt.subplot(3, 1, 3)\n\nplt.scatter(kw, km)\nplt.xlabel('Model Year')\nplt.ylabel('Rating')\nplt.title('United States car rating over year')\nplt.grid(True)\n\n\nplt.tight_layout()\nplt.show()","b1afb28e":"sm.plot()\nticks,labels = plt.xticks()","b4e3ec9d":"plt.figure(figsize=(14,8))\n\nX = tw\nX1 =sw \nX2 =kw \nY = tm\nY1 = sm\nY2 = km\nplt.scatter(X,Y,  marker = '^', color = 'Green', label ='Japan')\nplt.scatter(X1,Y1,  marker = '>', color = 'Red', label ='Eurpoe')\nplt.scatter(X2,Y2,  marker = '<', color = 'Blue', label ='North America',)\nplt.xlabel('Make Year')\nplt.ylabel('Rating')\nplt.legend(loc='best')\nplt.title('Relationship Between Car make Year and Rating for cars')\nplt.ylim(0,6)\nplt.show()","767f4f24":"Above Analysis shows that almost 21,000 reviews were submitted anonymously and top three revieweres submitted too many review.","b63e1ad3":"# Exploratory Analysis: \n\nThis is a dataset containing consumer's thought and the star rating of car manufacturer\/model\/type. Currently, this dataset has data of 62 major brands. I am going to select one major major brand from each major region (North America, Japan and Europe). \n\nI would be doing descriptive analysis of the data and explore if there are any major anomalies across Car Manufactoring year, Reviewer or Rating.\n\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made.\n\nI would be loading, cleaning and comparing following two analysis\n\n1) How a typical reviewer tends to review a American, European and Japanese car.\n2) How the review varies for cars by Year Model for American, European and Japanese car.","3d104004":"#Based on the data displayed below some analysis of data is\n1) Column: \"Unnamed: 0\": Is dulicate to index, so we can drop that column\n2) Column: Review_Date is of Object Type and has some extra character around date information, so it needs transformation\n3) Column Vehicle_Title: This column includes the car Model year, rest information is not important for my anlysis.\n4) Column: Rating is a float, for my analysis, I am intrested in distribution of the data, so will need o convert it to int.\n5) Column: Review_Title Not needed for my analysis\n6) Column: Review Not needed for my analysis\n\nAs next step we will analyze the data types of column.","c604a385":"Unnamed: Sequence number for the rows\nReview_Date: Datetime when review was given\nAuthor_Name: Author who gave review. User can give review unimaniously\nVehicle_Title:\nReview_Title: \nReview:\nRating:","f30dffb9":"This is a logical end to my descriptive analysis. A further extension of this analysis can be forensic analysis or creating a model which can predict the Rating of a given car based on Make year, Model \/ Country of origin and may be we can extract more information about car features like Engine capcity etc. So far what we have learnt about the dataset.\n\n1) Data has many columns, big chunk of the information in the dataset is in form of descriptive set. Which makes it a good candidates for sentiment analysis.\n2) There is enough data about 5% of total data where we have enough information to look into ratings and explore how rating were awarded, what things influenced the rating like Where car technology originated e.g. Asia (japan), Europe or USA.\n3) The Rating dataset (subset of data cleaned up for Rating analysis) has some anolmolies \n    a) The Rating is not evenly distributed (not a normal distribution). It's negatively skewed\n    b) Lot of review were provided anonymously, so making it difficult to identify reviewer pattern.\n    c) Most of reviewers have reviewed same car for multiple times (1-5), so we cannot predict the reviewer bias about a given car or any comparison for same reviewer reviewing different cars. Which kind of make sense that user in japan would not have multiple cars to use and provide review.\n    d) There were few reviewer anomalies where in one year few reviewers have reviewed 2000-3000 review for same car and all 5 rating.\n4) Rating density distribution increases from 4 - 5.\n5) Different cars across region has consistent rating across the decade. Average rating remained same.\n6) Japan cars have slightly higher 99th percentile (5.0) rating and European \/ American (4.5 - 4.8)\n","b39ada70":"Drop Following Columns: They are not Stattistical Important for my Analysis of Rating by Make year or Made in Country\n* Unnamed: 0\n* Make Year\n* Review_Date\n* Vehicle_Title\n* Review_Title\n* Review","53aed466":"Based on Analysis so far, we have only discovered few intresting facts about data. Now we are going back to original data set and take a different approach. This time, we will\n1) Not change rating from fraction to integer\n2) Not fill the missing year to Make Year\n3) See how much data is there after dropping and is it enough for analysis. We will avoid any filling.","eab5d95c":"Based on above analysis, we can conclude\n1) Most of teh Cars are rated 5 across all three regions. Eurpoe and American cars have 99.9% rating of 4.5-4.8 while Japense cars of 99.9 % have rating of 5.\n2) "}}