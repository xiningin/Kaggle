{"cell_type":{"a309e403":"code","8fac5667":"code","ff0c3747":"code","6d43a55f":"code","7a6e16f3":"code","b07dac0c":"code","3d4e344b":"code","cf7f16df":"code","046f12a3":"markdown"},"source":{"a309e403":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.optim import Adam\n\nfrom sklearn.datasets import load_iris\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython import display","8fac5667":"iris = load_iris()\n\nx = iris['data']\ny = iris['target']\n\nx = torch.FloatTensor(x)\ny = torch.tensor(y, dtype=torch.long)\n\nx, y = Variable(x), Variable(y)","ff0c3747":"class SingleHiddenNet(nn.Module):\n    # define nn\n    def __init__(self, n_features, n_hidden, n_output):\n        super(SingleHiddenNet, self).__init__()\n        self.hidden = nn.Linear(n_features, n_hidden)\n        self.logits = nn.Linear(n_hidden, n_output)\n\n    def forward(self, X):\n        X = F.sigmoid(self.hidden(X))\n        X = self.logits(X)\n        out = F.log_softmax(X, dim=1)\n\n        return out","6d43a55f":"iris = load_iris()\n\nx = iris['data']\ny = iris['target']\n\nx = torch.FloatTensor(x)\ny = torch.tensor(y, dtype=torch.long)\n\nx, y = Variable(x), Variable(y)\n\nnet = SingleHiddenNet(4, 5, 4)\n\nuse_cuda = torch.cuda.is_available()\nprint('use cuda: ', use_cuda)\n\nif use_cuda:\n    x = x.cuda()\n    y = y.cuda()\n    net = net.cuda()\n\noptimizer = Adam(net.parameters(), lr=0.1)\n\npx, py = [], []\nfor i in range(1000):\n    logits = net(x)\n    \n    # loss = F.cross_entropy(logits, y)\n    loss = F.nll_loss(logits, y)\n    \n    optimizer.zero_grad()\n    \n    loss.backward()\n    \n    optimizer.step()\n    \n    # if i % 100 == 0:\n    #     print(i, \" loss: \", loss.data)\n        \n    px.append(i)\n    py.append(loss.data)\n    \n    if i % 10 == 0:\n        plt.plot(px, py, 'r-', lw=1)\n        print(i, \" loss: \", loss.data)\n        display.clear_output(wait=True)\n        plt.show()\n        \nprint(\"Final loss: \", loss.data)","7a6e16f3":"iris = load_iris()\n\nx = iris['data']\ny = iris['target']\n\nx = torch.FloatTensor(x)\ny = torch.tensor(y, dtype=torch.long)\n\nx, y = Variable(x), Variable(y)\n\nnet = SingleHiddenNet(4, 5, 4)\n\nuse_cuda = torch.cuda.is_available()\nprint('use cuda: ', use_cuda)\n\nif use_cuda:\n    x = x.cuda()\n    y = y.cuda()\n    net = net.cuda()\n\noptimizer = Adam(net.parameters(), lr=0.9)\n\npx, py = [], []\nfor i in range(1000):\n    logits = net(x)\n    \n    # loss = F.cross_entropy(logits, y)\n    loss = F.nll_loss(logits, y)\n    \n    optimizer.zero_grad()\n    \n    loss.backward()\n    \n    optimizer.step()\n    \n    # if i % 100 == 0:\n    #     print(i, \" loss: \", loss.data)\n        \n    px.append(i)\n    py.append(loss.data)\n    \n    if i % 10 == 0:\n        plt.plot(px, py, 'r-', lw=1)\n        print(i, \" loss: \", loss.data)\n        display.clear_output(wait=True)\n        plt.show()\n        \nprint(\"Final loss: \", loss.data)","b07dac0c":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.optim import Adam\n\nfrom sklearn.datasets import load_iris\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython import display\n\nimport torch.utils.data as Data \n\nfrom sklearn.model_selection import train_test_split","3d4e344b":"class SingleHiddenNet(nn.Module):\n    # define nn\n    def __init__(self, n_features, n_hidden, n_output):\n        super(SingleHiddenNet, self).__init__()\n        self.hidden = nn.Linear(n_features, n_hidden)\n        self.logits = nn.Linear(n_hidden, n_output)\n\n    def forward(self, X):\n        X = F.sigmoid(self.hidden(X))\n        X = self.logits(X)\n        out = F.log_softmax(X, dim=1)\n\n        return out","cf7f16df":"net = SingleHiddenNet(4, 5, 4)\n\niris = load_iris()\nx = iris['data']\ny = iris['target']\n\nx_tr, x_te, y_tr, y_te = train_test_split(x, y, test_size=0.2)\n\ntr_x = torch.FloatTensor(x_tr)\ntr_y = torch.tensor(y_tr)\n\nte_x = torch.FloatTensor(x_te)\nte_y = torch.tensor(y_te)\n\ntr_x, tr_y = Variable(tr_x), Variable(tr_y)\n\nte_x, te_y = Variable(te_x), Variable(te_y)\n\nuse_cuda = torch.cuda.is_available()\nprint('use cuda: ', use_cuda)\n\nif use_cuda:\n    tr_x = tr_x.cuda()\n    tr_y = tr_y.cuda()\n    \n    net = net.cuda()\n    \n    te_x = te_x.cuda()\n    te_y = te_y.cuda()\n    \n    \ntrain_dataset = Data.TensorDataset(tr_x, tr_y)\ntrain_loader = Data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n\ntest_dataset = Data.TensorDataset(te_x, te_y)\ntest_loader = Data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n\noptimizer = Adam(net.parameters(), lr=0.1)\n\npx, py = [], []\ni = 0\nfor epoch in range(1000):\n    for step, (batch_x, batch_y) in enumerate(train_loader): \n        b_x = Variable(batch_x)\n        b_y = Variable(batch_y)\n        \n        logits = net(b_x)\n\n        loss = F.nll_loss(logits, b_y)\n\n        optimizer.zero_grad()\n\n        loss.backward()\n\n        optimizer.step()\n\n        px.append(i)\n        i += 1\n        py.append(loss.data)\n\n    plt.plot(px, py, 'r-', lw=1)\n    print(\"Epoch: %d, Loss: %.5f\" %(epoch, loss.data))\n    display.clear_output(wait=True)\n    plt.show()\n        \nprint(\"Final loss: \", loss.data)\n\n\ntest_loss = 0\ncorrect = 0\ndevice = torch.device('cuda') if use_cuda else torch.device('cpu')\nwith torch.no_grad():\n    for data, target in test_loader:\n        data, target = data.to(device), target.to(device)\n        output = net(data)\n        test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n        pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n        correct += pred.eq(target.view_as(pred)).sum().item()\n\ntest_loss \/= len(test_loader.dataset)\n\nprint('\\nTest set: Average loss: {:.4f}, Accuracy: {}\/{} ({:.0f}%)\\n'.format(\n    test_loss, correct, len(test_loader.dataset),\n    100. * correct \/ len(test_loader.dataset)))","046f12a3":"## Dataloader practice"}}