{"cell_type":{"b31556a8":"code","5803a44f":"code","5f078dfb":"code","4c1cfbea":"code","91fe1656":"code","8b4b4069":"code","50644b4c":"code","57dce5b4":"code","2d678dcc":"code","d56ea751":"code","5ab864b7":"code","1caf487f":"code","9f0aed89":"code","00ae8cfa":"code","3ebeef42":"code","200d8d99":"code","d21ea23b":"code","e3b7b288":"code","24a5b98f":"code","09f13297":"code","f56b3fd2":"code","b296187e":"code","a2417255":"code","d4d1cc56":"code","4de79803":"code","8291ef58":"code","fc38773f":"code","bebb7e9d":"code","4463fda1":"code","91843734":"code","2bd95860":"code","4fb85da0":"code","ab9e0846":"code","24fc4611":"code","00dc1e6e":"code","2d4d6c25":"code","e90ae4d8":"code","38526200":"code","a50e96fc":"code","74b1a124":"code","7facc2d4":"code","e94228d3":"code","21e07a9b":"code","bec59d2a":"code","c7704f60":"code","5be434a3":"code","1988e84b":"code","1d16f522":"code","1f6ea5f9":"code","51b1317e":"markdown","f4947a61":"markdown","066e20f0":"markdown","9b8210de":"markdown","2962caa8":"markdown","ffebc3d1":"markdown","da4f1c8a":"markdown","0d343b82":"markdown","df325a5e":"markdown","7e2281be":"markdown","440915cb":"markdown","f705c352":"markdown","adbaa1aa":"markdown","da85057c":"markdown","c64bdaaf":"markdown","a84b9eb4":"markdown","1b8231f6":"markdown","09a595c4":"markdown","9f746303":"markdown","91aa8057":"markdown","54fb6713":"markdown","57fead62":"markdown","1245c11d":"markdown","28ea85b9":"markdown","8fe62ec9":"markdown","3eaa415d":"markdown","0e6aadbb":"markdown"},"source":{"b31556a8":"import pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\npd.pandas.set_option('display.max_columns',None)\nimport numpy as np\nimport seaborn as sns\n\nfrom scipy import stats","5803a44f":"X_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\nX_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')","5f078dfb":"X_train.shape, X_test.shape","4c1cfbea":"X_train.head()","91fe1656":"X_test.head()","8b4b4069":"correlation_train=X_train.corr()\nsns.set(font_scale=1.2)\nmask = np.triu(correlation_train.corr())\nplt.figure(figsize = (20,20))\nax = sns.heatmap(correlation_train, annot=True,annot_kws={\"size\": 11},fmt='.1f', linewidths=.5, square=True, mask=mask)","50644b4c":"y = X_train.SalePrice.reset_index(drop=True)\nX_train.drop(['SalePrice'], axis=1, inplace=True)","57dce5b4":"plt.figure(figsize=(12,6))\nsns.distplot(y)","2d678dcc":"stats.probplot(y, plot=plt)\nprint(f\"Skewness: {y.skew():.3f}\")","d56ea751":"y = np.log1p(y)            \nplt.figure(figsize=(12,6))\nsns.distplot(y)","5ab864b7":"stats.probplot(y, plot=plt)\nprint(f\"Skewness: {y.skew():.3f}\")","1caf487f":"train_test = pd.concat([X_train, X_test], axis=0).reset_index(drop=True)\ntrain_test.shape","9f0aed89":"# Find Missing Ratio of Dataset\nmissing = (train_test.isnull().sum() \/ len(train_test)) * 100\nmissing = missing.drop(missing[missing == 0].index).sort_values(ascending=False)[:35]","00ae8cfa":"f, ax = plt.subplots(figsize=(12, 10))\nplt.xticks(rotation='90')\nsns.barplot(x=missing.index, y=missing)\nplt.xlabel('Features')\nplt.ylabel('%')\nplt.title('Percentage of missing values');","3ebeef42":"train_test['MSSubClass'] = train_test['MSSubClass'].astype(str)\ntrain_test['MoSold'] = train_test['MoSold'].astype(str)\ntrain_test['YrSold'] = train_test['YrSold'].astype(str)","200d8d99":"none = ['Alley', 'PoolQC', 'MiscFeature', 'Fence', 'GarageType','MasVnrType']\nfor col in none:\n    train_test[col].replace(np.nan, 'None', inplace=True)","d21ea23b":"train_test['MSZoning'] = train_test.groupby('MSSubClass')['MSZoning'].transform(\n    lambda x: x.fillna(x.mode()[0]))","e3b7b288":"freq_cols = [\n    'Electrical', 'Exterior1st', 'Exterior2nd',\n    'SaleType', 'Utilities'\n]\nfor col in freq_cols:\n    train_test[col].replace(np.nan, train_test[col].mode()[0], inplace=True)","24a5b98f":"qualcond = ['GarageQual', 'GarageCond', 'FireplaceQu', 'KitchenQual', 'HeatingQC', 'BsmtCond', 'BsmtQual', 'ExterCond', 'ExterQual']\nfor f in qualcond:\n    train_test[f] = train_test[f].replace({'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1, 'NA':0})\ntrain_test['BsmtExposure'] = train_test['BsmtExposure'].replace({'Gd':4, 'Av':3, 'Mn':2, 'No':1, 'NA':0})\ntrain_test['GarageFinish'] = train_test['GarageFinish'].replace({'Fin':3, 'RFn':2, 'Unf':1, 'NA':0})\nbasement = ['BsmtFinType1', 'BsmtFinType2']\nfor f in basement:\n    train_test[f] = train_test[f].replace({'GLQ':6, 'ALQ':5, 'BLQ':4, 'Rec':3, 'LwQ':2, 'Unf':1, 'NA':0})","09f13297":"functional = {'Typ': 3, 'Min1': 2.5, 'Min2': 2, 'Mod': 1.5, 'Maj1': 1, 'Maj2': 0.5, 'Sev': 0, 'Sal': 0}\ntrain_test['Functional'] = train_test['Functional'].replace(functional)\ntrain_test['CentralAir'] = train_test['CentralAir'].replace({'Y':1, 'N':0})","f56b3fd2":"train_test.isnull().sum().sort_values(ascending=False)[:22]","b296187e":"train_test = train_test.drop(['Utilities', 'Street', 'PoolQC', ], axis=1)","a2417255":"outliers = [ 30, 462, 523, 588, 632, 1298, 1324]\ntrain_test = train_test.drop(train_test.index[outliers])\nlinear_train_test = train_test.copy()\ny = y.drop(y.index[outliers])","d4d1cc56":"cat_cols = [cname for cname in train_test.columns if  train_test[cname].dtype == \"object\"]\ncat_cols\ntrain = train_test.iloc[:1453]\ntest = train_test.iloc[1453:]","4de79803":"from category_encoders import CatBoostEncoder\ncbe = CatBoostEncoder()\ntrain[cat_cols] = cbe.fit_transform(train[cat_cols], y)\ntest[cat_cols] = cbe.transform(test[cat_cols])","8291ef58":"train_test = pd.concat([train, test]).reset_index(drop=True)","fc38773f":"from sklearn.impute import KNNImputer\nimp = KNNImputer(n_neighbors=7, weights='distance', missing_values=np.nan)\nimp_train_test = imp.fit_transform(train_test)","bebb7e9d":"train_test = pd.DataFrame(imp_train_test, columns=train_test.columns)","4463fda1":"missing = ['GarageCars', 'BsmtFinSF1', 'GarageArea', 'BsmtUnfSF', 'KitchenQual',\n       'BsmtFinSF2', 'TotalBsmtSF', 'Functional', 'BsmtHalfBath',\n       'BsmtFullBath', 'MasVnrArea', 'BsmtFinType1', 'BsmtFinType2',\n       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'GarageQual', 'GarageFinish',\n       'GarageYrBlt', 'GarageCond', 'LotFrontage', 'FireplaceQu']","91843734":"train_test[missing] = train_test[missing].apply(lambda x: np.round(x))\ntrain_test","2bd95860":"train_test['YearsSinceBuilt'] = train_test['YrSold'].astype(int) - train_test['YearBuilt']\ntrain_test['YearsSinceRemod'] = train_test['YrSold'].astype(int) - train_test['YearRemodAdd']\ntrain_test['TotalSF'] = train_test['TotalBsmtSF'] + train_test['1stFlrSF'] + train_test['2ndFlrSF']\n\ntrain_test['Total_Bathrooms'] = (train_test['FullBath'] + (0.5 * train_test['HalfBath']) +\n                               train_test['BsmtFullBath'] + (0.5 * train_test['BsmtHalfBath']))\n\ntrain_test['TotalPorchArea'] = (train_test['OpenPorchSF'] + train_test['3SsnPorch'] +\n                              train_test['EnclosedPorch'] + train_test['ScreenPorch'] +\n                              train_test['WoodDeckSF'])\ntrain_test['TotalOccupiedArea'] = train_test['TotalSF'] + train_test['TotalPorchArea']\ntrain_test['OtherRooms'] = train_test['TotRmsAbvGrd'] - train_test['BedroomAbvGr'] - train_test['KitchenAbvGr']\ntrain_test['haspool'] = train_test['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['has2ndfloor'] = train_test['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['hasgarage'] = train_test['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['hasbsmt'] = train_test['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['hasfireplace'] = train_test['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","4fb85da0":"print(train_test.shape)\ntrain = train_test.iloc[:1453]\ntest = train_test.iloc[1453:]\nprint(train.shape, test.shape, y.shape)","ab9e0846":"ohe = pd.get_dummies(linear_train_test).reset_index(drop=True)\n\nimp = KNNImputer(n_neighbors=7, weights='distance', missing_values=np.nan)\nimp_linear_train_test = imp.fit_transform(ohe)\n\nlinear_train_test = pd.DataFrame(imp_linear_train_test, columns=ohe.columns)\n\nlinear_train_test[missing] = linear_train_test[missing].apply(lambda x: np.round(x))","24fc4611":"linear_train_test['TotalSF'] = linear_train_test['TotalBsmtSF'] + linear_train_test['1stFlrSF'] + linear_train_test['2ndFlrSF']\n\nlinear_train_test['Total_Bathrooms'] = (linear_train_test['FullBath'] + (0.5 * linear_train_test['HalfBath']) +\n                               linear_train_test['BsmtFullBath'] + (0.5 * linear_train_test['BsmtHalfBath']))\n\nlinear_train_test['TotalPorchArea'] = (linear_train_test['OpenPorchSF'] + linear_train_test['3SsnPorch'] +\n                              linear_train_test['EnclosedPorch'] + linear_train_test['ScreenPorch'] +\n                              linear_train_test['WoodDeckSF'])\nlinear_train_test['TotalOccupiedArea'] = linear_train_test['TotalSF'] + linear_train_test['TotalPorchArea']\nlinear_train_test['OtherRooms'] = linear_train_test['TotRmsAbvGrd'] - linear_train_test['BedroomAbvGr'] - linear_train_test['KitchenAbvGr']\nlinear_train_test['haspool'] = linear_train_test['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nlinear_train_test['has2ndfloor'] = linear_train_test['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nlinear_train_test['hasgarage'] = linear_train_test['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nlinear_train_test['hasbsmt'] = linear_train_test['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nlinear_train_test['hasfireplace'] = linear_train_test['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\nlinear_train_test","00dc1e6e":"from sklearn.preprocessing import StandardScaler\n\nlin_train = linear_train_test.iloc[:1453]\nlin_test = linear_train_test.iloc[1453:]\n\n\nScaler = StandardScaler()\nscaled_train = Scaler.fit_transform(lin_train)\nscaled_test = Scaler.transform(lin_test)\n\nscaled_train = pd.DataFrame(scaled_train, columns=linear_train_test.columns)\nscaled_test = pd.DataFrame(scaled_test, columns=linear_train_test.columns)","2d4d6c25":"print(scaled_train.shape, scaled_test.shape, y.shape)","e90ae4d8":"from catboost import CatBoostRegressor, Pool\nmodel = CatBoostRegressor(iterations=2500,\n                            learning_rate=0.03,\n                            depth=6,\n                            loss_function='RMSE',\n                            random_seed = 10,\n                            bootstrap_type='Bernoulli',\n                            subsample=0.66,\n                            rsm=0.7\n                         )","38526200":"model.fit(train, y, verbose=False, plot=False);","a50e96fc":"import shap\nshap.initjs()\n\nshap_values = model.get_feature_importance(Pool(train, y), type='ShapValues')\n\nexpected_value = shap_values[0,-1]\nshap_values = shap_values[:,:-1]\n\nshap.force_plot(expected_value, shap_values[0,:], train.iloc[0,:])","74b1a124":"shap.summary_plot(shap_values, train, max_display=88,  plot_type='bar')","7facc2d4":"shap_sum = np.abs(shap_values).mean(axis=0)\nimportance_df = pd.DataFrame([train.columns.tolist(), shap_sum.tolist()]).T\nimportance_df.columns = ['column_name', 'shap_importance']\nimportance_df = importance_df.sort_values('shap_importance', ascending=False)\nimportance_df.tail(35)","e94228d3":"drop = importance_df[importance_df['shap_importance'] < 1.5e-3].iloc[:,0].tolist()","21e07a9b":"train_drop = train.drop(drop, axis=1)\ntest_drop = test.drop(drop, axis=1)\ntrain_drop","bec59d2a":"from sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel\n\ndef selected_features_l1(X,y,alpha):\n    logistic = Lasso(random_state=20, alpha=alpha).fit(X,y)\n    model = SelectFromModel(logistic, prefit=True)\n    X_new = model.transform(X)\n    selected_features = pd.DataFrame(model.inverse_transform(X_new),\n                                    index=X.index,\n                                    columns=X.columns)\n    features = selected_features.columns[selected_features.var() != 0]\n    drop = selected_features.columns[selected_features.var() == 0]\n    return features, drop","c7704f60":"features, drop = selected_features_l1(scaled_train, y, 0.001)\nprint(drop)","5be434a3":"selected_train =  scaled_train.drop(drop, axis=1)\nselected_test = scaled_test.drop(drop, axis=1)","1988e84b":"import xgboost as xgb\n\nimport optuna\n\n\ndef objective(trial):\n    dtrain = xgb.DMatrix(train_drop, label=y)\n\n    param = {\n        'seed': 20,\n        'tree_method': 'gpu_hist',\n        'max_depth': trial.suggest_int(\"max_depth\", 3, 8),\n        'eta' : trial.suggest_uniform(\"eta\", 1e-3, 5e-2),\n        \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n        \"gamma\": trial.suggest_uniform(\"gamma\", 1e-8, 1e-4),\n        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 1.0),\n        \"subsample\": trial.suggest_uniform(\"subsample\", 0.3, 1.0),        \n    }\n    if param['grow_policy']==\"lossguide\":\n        param['max_leaves'] =  trial.suggest_int('max_leaves',2, 32)\n    bst = xgb.cv(param, dtrain, num_boost_round=5000, nfold=10, early_stopping_rounds=50,  metrics='rmse', seed=20)\n    score = bst['test-rmse-mean'].tail(1).values[0]\n    return score\n\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=200)\nprint(study.best_trial)","1d16f522":"from optuna.visualization import plot_parallel_coordinate\n\nplot_parallel_coordinate(study)","1f6ea5f9":"import optuna\n\nimportances = optuna.importance.get_param_importances(study)\nimportance_values = list(importances.values())\nparam_names = list(importances.keys())\nparams = pd.DataFrame([param_names, importance_values]).T\nparams.columns = ['param_name', 'importance']\nparams = params.sort_values('importance', ascending=False)\nsns.catplot(x='param_name', y='importance', data=params, kind='bar')\nplt.xticks(rotation='45');","51b1317e":"## EDA","f4947a61":"### Target log-transformation","066e20f0":"Dropping features with one value - Utilities, Street, PoolQC","9b8210de":"### Removing outliers","2962caa8":"### Encoding nominal categorical features and Imputing missing values (Tree-based)","ffebc3d1":"After optimization, our best result will look like this:\n\nFinished trial#110 with value: **0.1036024** with parameters:\n{'max_depth': 3, 'eta': 0.01423912926193527, 'grow_policy': 'lossguide', 'gamma': 2.804584764149306e-05, \n'colsample_bytree': 0.2403604834036041, 'subsample': 0.38141269740154965, 'max_leaves': 6}.\n\nWe can see our hyperparameter's history","da4f1c8a":"### Detecting missing values","0d343b82":"Feature Generation","df325a5e":"## Feature Selection (Non-Tree-based)","7e2281be":"## Feature Selection (Tree-based)","440915cb":"We'll drop features with less than **1.5e-3** importance(you can change this threshold)","f705c352":"### SHAP importance","adbaa1aa":"We'll use L1 Regularization with alpha 0.001","da85057c":"# House Prices: Advanced Regression Techniques","c64bdaaf":"### Encoding ordinal categorical features","a84b9eb4":"CatBoostEncoder replaces a categorical value with the average value of the target from the rows before it. It works well with XGBoost and LightGBM","1b8231f6":"## Hyperparameter optimization","09a595c4":"### Encoding nominal categorical features, Imputing missing values and Scaling (Non-Tree)","9f746303":"Each sample\u2019s missing values are imputed using the mean value from n_neighbors nearest neighbors found in the training set. Two samples are close if the features that neither is missing are close.","91aa8057":"Hello everyone!\n\nThe key objective is to use feature engineering to improve performance for tree-based and non-tree models.","54fb6713":"It was an example of one model optimization, and we can apply this steps to any model.","57fead62":"We'll build a CatBoost model and find best features with SHAP Values","1245c11d":"Looking at the charts above, we can conclude that we can change boundaries for hyperparameters.\n\nFor example: `eta: 7e-3, 2.2e-2`, `max_leaves: 2,16`, `gamma: 1e-6, 7e-5`, `colsample_bytree: 0.1, 0.4`, `subsample: 0.3, 0.6`\n\nAfter another optimization, our best result improved to **0.1031473**","28ea85b9":"## Load packages and data","8fe62ec9":"### Plot hyperparameter importance","3eaa415d":"We'll use Tree-structured Parzen Estimater (TPE), which is a form of Bayesian Optimization.\n\n![](https:\/\/miro.medium.com\/max\/700\/1*tYWqO5BwNDVaM3kP3w1IAg.png)\n\nWe'll define hyperparameters and ranges with `trial`, perform 10-fold CV, and set direction with number of trials for optimization `n_trial` to 200. We'll use tree_method `gpu_hist` for faster computation.","0e6aadbb":"### Imputing nominal categorical features"}}