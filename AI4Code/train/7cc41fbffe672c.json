{"cell_type":{"001bc843":"code","209aac32":"code","ba9a2f5c":"code","c82a386b":"code","347d1dcc":"code","4efcbdc6":"code","1234dc43":"code","be576f83":"code","fcb47064":"code","d1484d66":"code","8384c44d":"code","484c6d7f":"code","b3f8bf46":"code","bf0bfcc3":"code","94ac4556":"code","22a53034":"code","57f252a4":"code","c28a9f45":"code","0ad58c60":"code","7fe763cb":"code","167ee032":"code","6914e600":"code","ac2abfbe":"code","00e526fe":"code","8e8d8240":"code","e52a1967":"code","7ae9361f":"code","46942777":"code","c4d17775":"code","235ffcd1":"code","01b90936":"code","aa59bc12":"code","f9a7e83e":"code","9336c322":"code","43a1be2b":"code","f6ae6702":"code","7414ac9b":"code","824039bf":"code","c8caf11e":"code","0af21094":"code","a0dd6756":"code","0e22974a":"code","16446eb8":"code","bf1cf795":"code","0eeb522d":"code","532315ea":"code","80e4c91b":"code","b86341f3":"code","7bf72edf":"code","50fb79b9":"code","b99ac7c2":"code","b1339455":"code","49d63704":"code","b5dcaae2":"code","4cb577fc":"code","60c65a0d":"code","b31b7e5f":"code","423d81b5":"code","d10c4f94":"code","7ce74725":"code","5347682d":"code","b91994fb":"code","f5280052":"code","a7b34a6f":"code","ce51d214":"code","bffe6bb3":"code","c3255115":"code","e7d8c60a":"code","626e4921":"code","4487e5fa":"code","76dae0a6":"code","d359611b":"code","9e5723e7":"code","c3f55214":"code","bed0f9c2":"code","c301baef":"code","c7f013db":"code","14c8a1d6":"code","660c61aa":"code","d1495a95":"code","60da1e87":"markdown","49a60805":"markdown","0523bcd2":"markdown","2950d959":"markdown","2c4cf796":"markdown","378196b5":"markdown","b264bf92":"markdown","2c384a9b":"markdown","30bf2482":"markdown","a028ad38":"markdown","c160898d":"markdown","aa3a6ce4":"markdown","8416c921":"markdown","4eaba439":"markdown","ff7a0bbf":"markdown","db682484":"markdown","16542d7d":"markdown","d8d02242":"markdown","25a060f8":"markdown","0028dfd5":"markdown","137207d2":"markdown","a6cb9fc9":"markdown","84d9ec41":"markdown","ebdbe565":"markdown","bc6c2477":"markdown","261f574a":"markdown","6905d317":"markdown","5940e019":"markdown"},"source":{"001bc843":"# !pip install scispacy\n!pip install guidedlda\n# !pip install langdetect","209aac32":"import covid19_tools as cv19 # library generous released to public by Andy White (https:\/\/www.kaggle.com\/ajrwhite\/covid19-tools)\nimport pandas as pd\nimport re\nfrom IPython.core.display import display, HTML\nimport html\nimport numpy as np\nimport json\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport glob\n\nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n# import scispacy\nimport spacy\n# import en_core_sci_lg\n\n# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim  # don't skip this\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom scipy.spatial.distance import jensenshannon\n\nimport joblib\n\nfrom IPython.display import HTML, display\n\nfrom ipywidgets import interact, Layout, HBox, VBox, Box\nimport ipywidgets as widgets\nfrom IPython.display import clear_output\n\nfrom tqdm import tqdm\nfrom os.path import isfile\n\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")","ba9a2f5c":"METADATA_FILE = '..\/input\/CORD-19-research-challenge\/metadata.csv'\n\n# Load metadata\nmeta = cv19.load_metadata(METADATA_FILE)\n# print(meta.shape)\n# Add tags\nmeta, covid19_counts = cv19.add_tag_covid19(meta)\n\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)","c82a386b":"meta.info()","347d1dcc":"SOC_ETHIC_TERMS = ['privacy', 'prevention', 'fear', 'antiviral', 'behaviors', 'quarantine', 'bed shortage', 'SRH', 'misinformation', 'force', 'ICU bed shortage', 'tracking', 'politics', 'anti-asian', 'WHO', 'emergency', 'psychological health', 'freedom', 'liberty', 'isolation', 'social', 'rapid indentification', 'school closures', 'ethical', 'hospitals overloaded', 'distancing', 'social media', 'surgical masks', 'vaccination', 'burden', 'tracing', 'unemployment', 'movement', 'force measures', 'mask', 'secondary impacts', 'self-determination', 'internet', 'digital rights', 'control', 'assembly', 'compliance', 'individual rights', 'rumor', 'anxiety', 'restriction of movement', 'surveillance', 'restriction', 'human rights', 'lockdown', 'contact tracing', 'modification of health', 'ethical framework', 'fake news', 'local barriers', 'social science', 'ethic', 'disinformation', 'enablers', 'detention', 'global networks', 'censorship', 'social concern', 'physical health', 'ethical principles', 'immediate needs', 'gathering', 'movement restriction', 'ethical concern', 'medical treatment', 'education', 'rights', 'intervention', 'stigma', 'policy', 'public health', 'access', 'policies', 'oversight', 'discrimination', 'democracy']","4efcbdc6":"meta, soc_ethic_counts = cv19.count_and_tag(meta,\n                                               SOC_ETHIC_TERMS,\n                                               'soc_ethic')","1234dc43":"print('Loading full text for tag_disease_covid19')\n# pulling ~1000 research articles\nfull_text_repr = cv19.load_full_text(meta[meta.tag_disease_covid19 &\n                                          meta.tag_soc_ethic],\n                                     '..\/input\/CORD-19-research-challenge')\n\n#pulling ~5000 research articles (picked due to broader search term, which include SARS)\n# metadata_filter = meta[meta.tag_soc_ethic == True] \n# full_text_repr = cv19.load_full_text(metadata_filter,\n#                                      '..\/input\/CORD-19-research-challenge')","be576f83":"full_text_repr[0]","fcb47064":"meta.shape","d1484d66":"meta.head()","8384c44d":"# meta_rel = meta[meta.tag_disease_covid19 & meta.tag_soc_ethic]\n# include only soc and ethic terms\nmeta_rel = meta[meta.tag_soc_ethic]\nmeta_rel.shape","484c6d7f":"# (~(meta_rel['abstract'].isna()))","b3f8bf46":"meta_rel['abstract'].isna().sum()","bf0bfcc3":"meta_rel = meta_rel[(~(meta_rel['abstract'].isna()))]","94ac4556":"meta_rel.shape","22a53034":"# meta_rel['tag_soc_ethic']== False","57f252a4":"metadata_filter = meta[meta.tag_soc_ethic == True] ","c28a9f45":"#remove non related articles\nmeta_rel_drop = meta_rel.drop(meta_rel[meta_rel['tag_soc_ethic'] == False].index, inplace=True)","0ad58c60":"meta_rel.shape","7fe763cb":"meta_rel['abstract_word_count'] = meta_rel['abstract'].apply(lambda x: len(x.strip().split()))  # word count in abstract\nmeta_rel['abstract_unique_words']=meta_rel['abstract'].apply(lambda x:len(set(str(x).split())))  # number of unique words in body\nmeta_rel.head()","167ee032":"# meta_rel['abstract_word_count'] = meta_rel['abstract'].apply(lambda x: len(x.strip().split()))  # word count in abstract\n# meta_rel['body_word_count'] = meta_rel['body_text'].apply(lambda x: len(x.strip().split()))  # word count in body\n# meta_rel['body_unique_words']= meta_rel['body_text'].apply(lambda x:len(set(str(x).split())))  # number of unique words in body\n# meta_rel.head()","6914e600":"# two_terms = ['shelter in place','bed shortage','public health','public interest','human rights','digital rights','face mask','fake news','civil society',\n# 'medical treatment','community containment','mental health','suicide hotline','gig worker','medical worker','vulnerable population',\n# 'vulnerable community','social distancing','contact tracing','stay at home']","ac2abfbe":"# def replace_space(x):\n#     x.replace(\" \", \"_\")\n#     print (x)","00e526fe":"# replace_space(two_terms)","8e8d8240":"# from nltk.tree import *\n\n# # Tree manipulation\n\n# # Extract phrases from a parsed (chunked) tree\n# # Phrase = tag for the string phrase (sub-tree) to extract\n# # Returns: List of deep copies;  Recursive\n# def ExtractPhrases( myTree, phrase):\n#     myPhrases = []\n#     if (myTree.node == phrase):\n#         myPhrases.append( myTree.copy(True) )\n#     for child in myTree:\n#         if (type(child) is Tree):\n#             list_of_phrases = ExtractPhrases(child, phrase)\n#             if (len(list_of_phrases) > 0):\n#                 myPhrases.extend(list_of_phrases)\n#     return myPhrases\n\n# test = Tree.parse('(S (NP I) (VP (V enjoyed) (NP my cookies)))')\n# print (\"Input tree: \", test)\n\n# print (\"\\nNoun phrases:\")\n# list_of_noun_phrases = ExtractPhrases(test, 'NP')\n# for phrase in list_of_noun_phrases:\n#     print (\" \", phrase)","e52a1967":"# def sent_to_words(sentences):\n#     for sentence in sentences:\n#         yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\n# abstract_gram = list(sent_to_words(meta_rel['abstract']))\n\n# print(abstract_gram[:1])","7ae9361f":"# # Build the bigram and trigram models\n# bigram = gensim.models.Phrases(abstract_gram, min_count=5, threshold=100) # higher threshold fewer phrases.\n# trigram = gensim.models.Phrases(bigram[abstract_gram], threshold=100)  \n\n# # Faster way to get a sentence clubbed as a trigram\/bigram\n# bigram_mod = gensim.models.phrases.Phraser(bigram)\n# trigram_mod = gensim.models.phrases.Phraser(trigram)\n\n# # bigram and trigram example\n# # print(bigram_mod[abstract_gram[0]])\n# # print(trigram_mod[bigram_mod[abstract_gram[0]]])","46942777":"# abstract_gram","c4d17775":"# print(bigram_mod[abstract_gram[1]])\n# # print(trigram_mod[bigram_mod[abstract_gram[0]]])","235ffcd1":"# include multiple word in the tokenization\nimport spacy\nspacy.load('en')\nfrom spacy.lang.en import English\nparser = English()\ndef tokenize(text):\n    lda_tokens = []\n    tokens = parser(text)\n    for token in tokens:\n        if token.orth_.isspace():\n            continue\n        elif token.like_url:\n            lda_tokens.append('URL')\n        elif token.orth_.startswith('@'):\n            lda_tokens.append('SCREEN_NAME')\n        else:\n            lda_tokens.append(token.lower_)\n    return lda_tokens","01b90936":"import nltk\nnltk.download('wordnet')\nfrom nltk.corpus import wordnet as wn\ndef get_lemma(word):\n    lemma = wn.morphy(word)\n    if lemma is None:\n        return word\n    else:\n        return lemma\n    \nfrom nltk.stem.wordnet import WordNetLemmatizer\ndef get_lemma2(word):\n    return WordNetLemmatizer().lemmatize(word)","aa59bc12":"nltk.download('stopwords')\n# en_stop = set(nltk.corpus.stopwords.words('english'))","f9a7e83e":"en_stop = nltk.corpus.stopwords.words('english')\n#add new stopwords here\nen_stop.extend(['abstract', 'doi', 'preprint', 'copyright','https', 'et', 'al','figure','fig', 'fig.', \n                'al.','PMC', 'CZI','peer', 'reviewed', 'org','author','rights', 'reserved', 'permission', \n                'used', 'using', 'biorxiv', 'medrxiv', 'license','Elsevier','www'])\n\nen_stop = set(en_stop)","9336c322":"def prepare_text_for_lda(text):\n    tokens = tokenize(text)\n    tokens = [token for token in tokens if len(token) > 4]\n    tokens = [token for token in tokens if token not in en_stop]\n    tokens = [get_lemma(token) for token in tokens]\n    return tokens","43a1be2b":"meta_rel['tokens'] = meta_rel.apply(lambda x: prepare_text_for_lda(x['abstract']),axis=1)\ntext_data = list(meta_rel['tokens'])","f6ae6702":"import seaborn as sns\nsns.distplot(meta_rel['abstract_word_count'])\nmeta_rel['abstract_word_count'].describe()","7414ac9b":"sns.distplot(meta_rel['abstract_unique_words'])\nmeta_rel['abstract_unique_words'].describe()","824039bf":"'''Topic model included non SOE term'''\n# meta_rel['tag_soc_ethic']== False","c8caf11e":"len(text_data[1])","0af21094":"from gensim import corpora\n#data dictionary\ndictionary = corpora.Dictionary(text_data)\n#corpus and tf-idf\ncorpus = [dictionary.doc2bow(text) for text in text_data]\n\n\nimport pickle\npickle.dump(corpus, open('corpus.pkl', 'wb'))\ndictionary.save('dictionary.gensim')","a0dd6756":"# #readable format of corpus (tf-idf)\n# [[(dictionary[id], freq) for id, freq in cp] for cp in corpus[:1]]","0e22974a":"'''\npasses (int, optional) \u2013 Number of passes through the corpus during training.\niterations (int, optional) \u2013 Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\nOptimize Topic: 8\n'''\n#10 topics\nimport gensim\nNUM_TOPICS = 8\n# ldamodel = gensim.models.ldamodel.LdaModel(corpus, \n#                                            num_topics = NUM_TOPICS, \n#                                            id2word=dictionary, \n#                                            update_every=1,\n#                                            passes=25, \n#                                            random_state=7,\n#                                            alpha='auto') # TO-Do: Find optimal # topics, iterations, and passes\n\n# ldamodel = gensim.models.ldamodel.LdaModel(corpus, \n#                                            num_topics = NUM_TOPICS, \n#                                            id2word=dictionary, \n#                                            update_every=1,\n#                                            passes=100, \n#                                            iterations =100,\n#                                            random_state=7,\n#                                            alpha='auto') # TO-Do: Find optimal # topics, iterations, and passes\n\nldamodel = gensim.models.ldamodel.LdaModel(corpus, \n                                           num_topics = NUM_TOPICS, \n                                           id2word=dictionary, \n                                           update_every=1,\n                                           passes=25, \n                                           iterations =50,\n                                           random_state=7,\n                                           alpha='auto') # TO-Do: Find optimal # topics, iterations, and passes\n\n\nldamodel.save('model10.gensim')\n\ntopics = ldamodel.print_topics(num_words=10)\nfor topic in topics:\n    print(topic)","16446eb8":"def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n    coherence_values = []\n    model_list = []\n    for num_topics in range(start, limit, step):\n        model = gensim.models.ldamodel.LdaModel(corpus, \n                                           num_topics = NUM_TOPICS, \n                                           id2word=dictionary)\n        model_list.append(model)\n        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n        coherence_values.append(coherencemodel.get_coherence())\n\n    return model_list, coherence_values","bf1cf795":"model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=text_data, start=2, limit=40, step=6)","0eeb522d":"# Show graph\n'''    \nPick model (num topics) with the highest coherence score before flattening out.\n'''\nlimit=40; start=2; step=6;\nx = range(start, limit, step)\nplt.plot(x, coherence_values)\nplt.xlabel(\"Num Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend((\"coherence_values\"), loc='best')\nplt.show()","532315ea":"# Print the coherence scores\nfor m, cv in zip(x, coherence_values):\n    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))","80e4c91b":"# Select the model and print the topics\noptimal_model = model_list[2]\nmodel_topics = optimal_model.show_topics(formatted=False)\nprint(optimal_model.print_topics(num_words=10))","b86341f3":"def format_topics_sentences(ldamodel, corpus, texts):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row in enumerate(ldamodel[corpus]):\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=ldamodel, corpus=corpus, texts=text_data)\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n\n# Show\ndf_dominant_topic.head(10)","7bf72edf":"# Group top 5 sentences under each topic\nsent_topics_sorteddf_mallet = pd.DataFrame()\n\nsent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n\nfor i, grp in sent_topics_outdf_grpd:\n    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n                                            axis=0)\n\n# Reset Index    \nsent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n\n# Format\nsent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n\n# Show\nsent_topics_sorteddf_mallet","50fb79b9":"# Number of Documents for Each Topic\ntopic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n\n# Percentage of Documents for Each Topic\ntopic_contribution = round(topic_counts\/topic_counts.sum(), 4)\n\n# Topic Number and Keywords\ntopic_num_keywords = sent_topics_sorteddf_mallet[['Topic_Num', 'Keywords']]\n\n# Concatenate Column wise\ndf_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n\n# Change Column names\ndf_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n\n# Show\ndf_dominant_topics","b99ac7c2":"dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\ncorpus = pickle.load(open('corpus.pkl', 'rb'))\nlda = gensim.models.ldamodel.LdaModel.load('model10.gensim')\n\n# lda10 = gensim.models.ldamodel.LdaModel.load('model10.gensim')\n# lda_display10 = pyLDAvis.gensim.prepare(lda10, corpus, dictionary, sort_topics=False)\n# pyLDAvis.display(lda_display10)","b1339455":"'''Match research paper to one of the 7 LDA topics'''\n# #print topic cluster of research articles.\n# for i in ldamodel.get_document_topics(corpus)[:]:\n#     li = []\n#     for j in i:\n#         li.append(j[1])\n#         bz=li.index(max(li))\n#     print(i[bz][0])","49d63704":"#Model perplexity and topic coherence measure topic accuracy. \n# Compute Perplexity # a measure of how good the model is. A low score is good.\nprint('\\nPerplexity: ', ldamodel.log_perplexity(corpus))  \n\n# Compute Coherence Score (Want a hight value)\ncoherence_model_lda = CoherenceModel(model=ldamodel, texts=text_data, dictionary=dictionary, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","b5dcaae2":"# import pyLDAvis.gensim\n#visual graph\n# lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n# # lda matching by words vs matching by documents. Document may contains multiple topics and words. \n# pyLDAvis.display(lda_display)","4cb577fc":"import pyLDAvis.gensim\nlda10 = gensim.models.ldamodel.LdaModel.load('model10.gensim')\n#visual graph\nlda_display10 = pyLDAvis.gensim.prepare(lda10, corpus, dictionary, sort_topics=False)\n# lda matching by words vs matching by documents. Document may contains multiple topics and words. \npyLDAvis.display(lda_display10)","60c65a0d":"# for item in full_text_repr[0]['body_text']:\n#     print(item['text'])","b31b7e5f":"# text_data_full = []\n# for i, record in enumerate(full_text_repr):\n#     record_text = \"\".join([item['text'] for item in record['body_text']])\n#     tokens = prepare_text_for_lda(record_text)\n#     text_data_full.append(tokens)","423d81b5":"# text_data_full","d10c4f94":"# len(text_data_full[3])","7ce74725":"# from gensim import corpora\n# dictionary_full = corpora.Dictionary(prepare_text_for_lda)\n# corpus_full = [dictionary.doc2bow(text) for text in text_data_full]\n# import pickle\n# pickle.dump(corpus_full, open('corpus_fulltexts.pkl', 'wb'))\n# dictionary_full.save('dictionary_fulltexts.gensim')","5347682d":"# # 5 topics\n# import gensim\n# NUM_TOPICS = 5\n# ldamodel_full = gensim.models.ldamodel.LdaModel(corpus_full, num_topics = NUM_TOPICS, id2word=dictionary_full, passes=15)\n# ldamodel_full.save('model10_fulltexts.gensim')\n# topics = ldamodel.print_topics(num_words=25)\n# for topic in topics:\n#     print(topic)","b91994fb":"# #10 topics\n# import gensim\n# NUM_TOPICS = 10\n# ldamodel_full = gensim.models.ldamodel.LdaModel(corpus_full, num_topics = NUM_TOPICS, id2word=dictionary_full, passes=15)\n# ldamodel_full.save('model10_fulltexts.gensim')\n# topics = ldamodel_full.print_topics(num_words=10)\n# for topic in topics:\n#     print(topic)","f5280052":"# dictionary_full = gensim.corpora.Dictionary.load('dictionary_fulltexts.gensim')\n# corpus_full = pickle.load(open('corpus_fulltexts.pkl', 'rb'))\n# lda_full = gensim.models.ldamodel.LdaModel.load('model10_fulltexts.gensim')\n# import pyLDAvis.gensim","a7b34a6f":"# #visual\n# lda_display_full = pyLDAvis.gensim.prepare(lda_full, corpus_full, dictionary_full, sort_topics=False)","ce51d214":"# #visual\n# pyLDAvis.display(lda_display_full)","bffe6bb3":"# import guidedlda ","c3255115":"# seed_topic_list =[\n# ['disinformation','misinformation','news','tweet','media','censorship','war','viral','anti-asian','fake'],\n# ['police','law','enforcement','liberty','self-determination','force','politics','restriction','freedom','detention','lockdown'],\n# ['well-being','isolation','psychological','mental','health','vulnerable','elderly','wellness','trauma','suicide','hotline'],\n# ['privacy','surveillance','digital','human','rights','declaration','censorship','self-determination','democracy','discrimination','civil','society'],\n# ['economics','economy','gig','low-income','worker','curve','recession','business','jobs','loss'],\n# ['healthcare','nurse','doctor','front-line','seniors','caregiver','medical'],\n# ['policy','shelter-in-place','GDPR','distancing','contain','containment','suppress','suppression','quarantine','closure','replication','reprecussion','capacity','lockdown']    \n# ]     \n\n# seed_topic_list =[     \n# [\"severe\", \"symptom\", \"clinical\", \"disease\", \"study\", \"result\", \"case\", \"cov-2\", \"coronavirus\", \"covid-19\", \"cov-2\"],\n# [\"study\", \"viral\", \"control\", \"method\", \"intervention\"],\n# [\"intervention\", \"social\", \"method\" ],\n# [\"china\", \"wuhan\", \"country\", \"hubei\", \"province\", \"health\", \"cov-2\", \"coronavirus\", \"covid-19\", \"cov-2\"],\n# [\"Patient\", \"patient\", \"transmission\", \"epidemic\", \"measure\", \"respiratory\"],\n# [\"emergency\", \"Patient\", \"patient\", \"cov-2\", \"coronavirus\", \"covid-19\", \"cov-2\", \"public\", \"medical\", \"outbreak\", \"case\", \"number\", \"estimate\", \"infection\", \"health\", \"disease\", \"virus\", \" treatment\"],\n# [\"group\", \"china\", \"wuhan\", \"country\", \"hubei\", \"quarantine\", \"using\", \"disease\"],\n# ]","e7d8c60a":"# '''Make our own dataset and word2id'''\n\n# # print(X.shape)\n# # print(corpus[100])\n# word2id = {}\n# vocab = []\n# index = 0\n# for tx in text_data:\n#     for word in tx:\n#         if word not in word2id:\n#             vocab.append(word)\n#             word2id[word] = index\n#             index += 1\n\n# print(len(word2id))\n\n# ## transfer corpus to word_ids sentences\n# corpus_with_id = []\n# max_len = max([len(x) for x in corpus])\n# for line in corpus:\n#     doc = []\n#     for word, fre in line:\n#         doc.append(word)\n#     doc += [0 for _ in range(max_len - len(doc))]\n#     corpus_with_id.append(doc)\n\n# import numpy\n# corpus_with_id = numpy.array(corpus_with_id)\n# print(corpus_with_id.shape)\n\n# # seed_topics = {}\n# # for t_id, st in enumerate(seed_topic_list):\n# #     for word in st:\n# #         print(word)","626e4921":"# '''Check seed topics for seed_topic_list'''\n# seed_topics = {}\n# for t_id, st in enumerate(seed_topic_list):\n#     for word in st:\n#         if word in word2id:\n#             seed_topics[word2id[word]] = t_id\n","4487e5fa":"# '''model training'''\n# model = guidedlda.GuidedLDA(n_topics=7, n_iter=100, random_state=7, refresh=20)\n# model.fit(corpus_with_id, seed_topics=seed_topics, seed_confidence=0.15)","76dae0a6":"# '''Get guidedLDA output'''\n# n_top_words = 10\n# topic_word = model.topic_word_\n# for i, topic_dist in enumerate(topic_word):\n#     topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n#     print('Topic {}: {}'.format(i, ' '.join(topic_words)))","d359611b":"# '''Retreive the document-topic distributions'''\n# doc_topic = model.transform(corpus_with_id)\n# for i in range(7):\n#     print(\"top topic: {} Document: {}\".format(doc_topic[i].argmax(),\n#                                                   ', '.join(np.array(vocab)[list(reversed(corpus_with_id[i,:].argsort()))[0:5]])))\n","9e5723e7":"# with open('guidedlda_model.pickle', 'wb') as file_handle:\n#      pickle.dump(model, file_handle)\n# # load the model for prediction\n# # with open('guidedlda_model.pickle', 'rb') as file_handle:\n# #      model = pickle.load(file_handle)","c3f55214":"# X = guidedlda.datasets.load_data(guidedlda.datasets.NYT) # need to update to main list\n# vocab = guidedlda.datasets.load_vocab(guidedlda.datasets.NYT)\n# word2id = dict((v, idx) for idx, v in enumerate(vocab))","bed0f9c2":"# model = guidedlda.GuidedLDA(n_topics=7, n_iter=100, random_state=7, refresh=20)\n# seed_topics = {}\n# for t_id, st in enumerate(seed_topic_list):\n#     for word in st:\n#         seed_topics[word2id[word]] = t_id","c301baef":"# model.fit(X, seed_topics=seed_topics, seed_confidence=0.15)","c7f013db":"# n_top_words = 10\n# topic_word = model.topic_word_\n# for i, topic_dist in enumerate(topic_word):\n#     topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n#     print('Topic {}: {}'.format(i, ' '.join(topic_words)))","14c8a1d6":"# doc_topic = model.transform(X)\n# for i in range(9):\n#     print(\"top topic: {} Document: {}\".format(doc_topic[i].argmax(),\n#                                                   ', '.join(np.array(vocab)[list(reversed(X[i,:].argsort()))[0:5]])))\n","660c61aa":"# model.purge_extra_matrices()","d1495a95":"# from six.moves import cPickle as pickle\n# with open('guidedlda_model.pickle', 'wb') as file_handle:\n#      pickle.dump(model, file_handle)\n# # load the model for prediction\n# with open('guidedlda_model.pickle', 'rb') as file_handle:\n#      model = pickle.load(file_handle)\n# doc_topic = model.transform(X)","60da1e87":"# **Adding new stopwords**","49a60805":"# **Adding abstract and fulltext word count**","0523bcd2":"# **Text Cleaning: Tokenization**","2950d959":"# **Abstract word count**\nAbstract average word is 222.\nAbstract unique average word is 140.","2c4cf796":"# **Topic distribution across documents**","378196b5":"# **(removed, will include in the future) Guided LDA **\nGuidededLDA is used to separate topics which had smaller representation in teh corpus and guide the classification of documents. \n\nX: call on internal vector (represents the corpus)\nvocab: calls on the important high representation of words in the corpus\nword2id: calls internal library transforming word to id. \n\nSeed topic list: \"force\" topic model words output into defined topic (categories)\n\nhttps:\/\/github.com\/vi3k6i5\/guidedlda \nhttps:\/\/www.freecodecamp.org\/news\/how-we-changed-unsupervised-lda-to-semi-supervised-guidedlda-e36a95f3a164\/ \nhttps:\/\/medium.com\/analytics-vidhya\/how-i-tackled-a-real-world-problem-with-guidedlda-55ee803a6f0d**","b264bf92":"# **Finding optimal number of topics for LDA (highest coherence)**\n    Compute c_v coherence for various number of topics\n\n    Parameters:\n    ----------\n    dictionary : Gensim dictionary\n    corpus : Gensim corpus\n    texts : List of input texts\n    limit : Max num of topics\n\n    Returns:\n    -------\n    model_list : List of LDA topic models\n    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n","2c384a9b":"# **Dominant topics in each sentence**","30bf2482":"# **(removed, will include in the future) Include N-gram SOC Terms **","a028ad38":"# **Filter Research Articles by Social and Ethical Terms (Keyword search)**","c160898d":"'''# **Bigram and Trigram models**\nMultiple words occuring together'''","aa3a6ce4":"# **Added Abstract word count**","8416c921":"# **Remove missing abstracts**","4eaba439":"# **pyLDAvis visualize information from a fit to a corpus of text data, LDA topic model **\n* Saliency: a measure of topic term strength (salient keywords form the selected topics)\n* Relevance: Average word probability weight to a topic and the word given the topic normalized by the probability of the topic.\n* Bubbles: Represents a topic. The larger the bubbles the stronger the importance of the topic relative to the data.\n\nThings to look for: \n* Each bubble represent a topic. The size of the bubble represent the prevalency of the topic.\n* A good topic will have big, non-overlapping bubvles scattered throughout the chart instead of clusered in one quadrant. \n* Each topic is filled with salient keywords. ","ff7a0bbf":"# **(need to come back in the future removed covid-19 tag to increase dataset) Same process with full-texts**","db682484":"# **Visualizing 10 topics**","16542d7d":"#  **Latent Dirichlet Allocation (LDA)**\n1. Create data dictionary (Gensim create a unique id for each word [word_id,word_frequency)\n2. Convert into bag-of-words corpus and Term Document Frequency (tf-idf)\n4. Save dictionary and corpus","d8d02242":"# **Add (abstract) token into list**","25a060f8":"# **10 LDA topics**","0028dfd5":"# **Find the meanings of words, synonyms, antonyms, and more (Lemmatization)**","137207d2":"# **Define a function to prepare the text for topic modelling**","a6cb9fc9":"# **Compute Model Perplexity and Coherence Score**","84d9ec41":"# **Loading metadata**","ebdbe565":"# **Most representative document for each topic**","bc6c2477":"# **Stopword Removal**","261f574a":"# ** (removed) Model Evaulation with loglikehoods**\nLoglikehoods attribute on a fitted model can monitor convergence. The attribute is bound to a list which records the sequence of log likelihoods associated with the model at different iterations. \n\nDocumentation:https:\/\/lda.readthedocs.io\/en\/latest\/getting_started.html","6905d317":"# **Visualize full text**","5940e019":"About this Notebook:\nFor this notebook we used topic model: Latent Dirichlet Allocation (LDA) which considers each document as a collection of (keyword) topics that rearrange the topics distribution within the documents and keywords into different defined number of topics for keywords distribution.\n\nSteps:\n1. Preprocess document metadata text by using Natural Language Processing (NLP). \n    * Filtered research articles by social and ethical keywords, and covid-19 tagging. \n    * Filtered missing abstract. \n    * Validated word count and unique word count. \n    * Included bigram and trigram terms. \n    * Tokenize, lemmatize, customized stop word removal\n2. Applied LDA, topic model on abstract.\n    * Create data dictionary (unique id for each word [word_id, word_frequency]) with Genism.\n    * Convert into bag-of-words corpus and term document frequency (tf-idf)\n    * Validated passes, iterations, random state, and alpha\n    * Finding optimal number of topics for LDA with highest coherence value and low perplexity value\n    * Finding dominant topics in each sentence\n    * Finding most representative document for each topic\n    * Finding topic distribution across documents\n3. Calculate similarity between pages by nearest neighbors, sourced by the distance metric. \n4. Visualizing topic with pyLDAvis\n\n\nPros:\n1.\tUsed research article meta-data \n2.\tShows latent relationships between articles \n3.\tLDA is an effective visualization tool for topic modeling.\n4.\tEasy to visually understand group of clusters. \n5.\tHas a history of producing reliable results in multiple different domains.\n6.\tTransferable tool for new applications. \n7.\tGuided LDA nudge LDA topic to a semi-supervised model\n\nCons:\n1.\tThe current topic model focused on the abstract, not full text. Abstract summarize the research article. While, full text contains the article complete context.\n2.\tTopic model\u2019s accuracy depends on known number of topics before applying to the model. In this notebook we compared and choose the number of topics with the highest coherence value. However, it is still random.\n3.\tThe model coherence value is poor; needs more iteration and pass test in the future. \n4.\tLDA distribution can't capture correlations among topics. \n5.\tLDA is an unsupervised learning thus topics are hard to identify\n\nTopic modeling is an unsupervised machine learning statistical model to discover hidden semantic text structures by assuming each word in the document are related. It then tries to learn topic representations of papers in a corpus. Documents are probability distribution over latent topics. Topics are probability distributions over words. After determining the topics in the documents, it use the model to generate topic and word distributions over a corpus. The output is then used to identify similar documents within the corpus. \n\nTopics is a collection of dominant keywords defined by five main factors:\n1. Quality of text preprocessing\n2. Variety of topics\n3. Topic modeling algorithm choice\n4. Number of topics\n5. Tuning algorithm parameters."}}