{"cell_type":{"2db5f600":"code","772bf89d":"code","747939ed":"code","2992a91f":"code","9aff058f":"code","7eec5f63":"code","62247d88":"code","06178d42":"code","a9c7ca76":"code","6ce4503c":"code","7af52f36":"code","7f69842b":"code","52bfc43d":"markdown","9b8a6a01":"markdown","f963c63d":"markdown","7799a6a4":"markdown","7d5721e7":"markdown","93a904dd":"markdown","d15e5730":"markdown","8d623fbb":"markdown","14bc2436":"markdown","a9e4e598":"markdown","1d36002c":"markdown","c5a1025c":"markdown","8148526b":"markdown","0f9fe029":"markdown"},"source":{"2db5f600":"from sklearn.datasets import make_classification \nimport numpy as np\nimport pandas as pd\nnp.random.seed(2019)\n\n# generate dataset \ntrain, target = make_classification(512, 255, n_informative=np.random.randint(33, 47), n_redundant=0, flip_y=0.08)\ntrain = np.hstack((train, np.ones((len(train), 1))*0))\n\nfor i in range(1, 512):\n    X, y = make_classification(512, 255, n_informative=np.random.randint(33, 47), n_redundant=0, flip_y=0.08)\n    X = np.hstack((X, np.ones((len(X), 1))*i))\n    train = np.vstack((train, X))\n    target = np.concatenate((target, y))\n    ","772bf89d":"col_names = []\nkind_arr = ['muggy', 'dorky', 'slimy', 'snazzy', 'frumpy', 'stealthy', 'chummy', 'hazy', 'nerdy', 'leaky', 'ugly', 'shaggy', 'flaky','squirrely', 'freaky', 'lousy', 'bluesy', 'baggy', 'greasy',\n       'cranky', 'snippy', 'flabby', 'goopy', 'homey', 'homely', 'hasty','blurry', 'snoopy', 'stinky', 'bumpy', 'slaphappy', 'messy','geeky', 'crabby', 'beady', 'pasty', 'snappy', 'breezy', 'sunny',\n       'cheeky', 'wiggy', 'flimsy', 'lanky', 'scanty', 'grumpy', 'chewy','crappy', 'clammy', 'tasty', 'thirsty', 'gloppy', 'gamy', 'hilly','woozy', 'squeaky', 'lovely', 'paltry', 'smelly', 'pokey','skanky', 'zippy', 'sleazy', 'queasy', 'foggy', 'wheezy', 'droopy',\n       'cozy', 'skinny', 'seedy', 'stuffy', 'jumpy', 'trippy', 'woolly','gimpy', 'randy', 'silly', 'craggy', 'skimpy', 'nippy', 'whiny','boozy', 'pretty', 'sickly', 'shabby', 'surly']\ncolor_arr = ['smalt', 'peach', 'seashell', 'harlequin', 'beige', 'cream','emerald', 'indigo', 'amaranth', 'tangerine', 'silver','chocolate', 'tan', 'plum', 'rose', 'copper', 'scarlet','cinnamon', 'cardinal', 'auburn', 'sepia', 'brass', 'eggplant',\n       'ruby', 'blue', 'wisteria', 'maroon', 'tomato', 'mauve', 'pumpkin','teal', 'goldenrod', 'aquamarine', 'gamboge', 'persimmon','mustard', 'red', 'magnolia', 'chestnut', 'champagne', 'flax',\n       'viridian', 'amber', 'zucchini', 'myrtle', 'lemon', 'pear',\n       'xanthic', 'turquoise', 'lilac', 'amethyst', 'lime', 'pink',\n       'periwinkle', 'crimson', 'burgundy', 'purple', 'rust', 'cerise',\n       'khaki', 'malachite', 'violet', 'sangria', 'magenta', 'russet',\n       'apricot', 'cobalt', 'platinum', 'denim', 'yellow', 'sapphire',\n       'bronze', 'green', 'thistle', 'buff', 'razzmatazz', 'charcoal',\n       'ultramarine', 'puce', 'carmine', 'gold', 'asparagus', 'ivory',\n       'orange', 'vermilion', 'chartreuse', 'heliotrope', 'azure', 'grey',\n       'jade', 'olive', 'coral', 'brown', 'cinnabar', 'lavender', 'aqua',\n       'firebrick', 'corn', 'bistre', 'cyan', 'ochre', 'dandelion',\n       'white']\nanimal_arr = ['axolotl', 'sheepdog', 'cassowary', 'chicken', 'mau', 'pinscher',\n       'tarantula', 'cuttlefish', 'wolfhound', 'lizard', 'chihuahua',\n       'indri', 'beetle', 'sheep', 'angelfish', 'penguin', 'wallaby',\n       'oriole', 'hound', 'bonobo', 'dogfish', 'vole', 'coral', 'fowl',\n       'bombay', 'bulldog', 'oyster', 'blue', 'armadillo', 'ragdoll',\n       'wolverine', 'moorhen', 'otter', 'bat', 'affenpinscher', 'rat',\n       'caterpillar', 'newt', 'collie', 'weasel', 'guppy', 'bullfrog',\n       'alligator', 'sloth', 'moth', 'kudu', 'wasp', 'okapi', 'quoll',\n       'shrew', 'walrus', 'schnauzer', 'termite', 'dragonfly', 'kakapo',\n       'quetzal', 'capuchin', 'eel', 'iguana', 'zonkey', 'fousek',\n       'javanese', 'leopard', 'gorilla', 'malamute', 'birman', 'donkey',\n       'lionfish', 'llama', 'emu', 'koala', 'saola', 'neanderthal',\n       'horse', 'mammoth', 'duck', 'peccary', 'hippopotamus',\n       'grasshopper', 'dolphin', 'gharial', 'frog', 'ostrich', 'akbash',\n       'bison', 'hyrax', 'capybara', 'earwig', 'cuscus', 'chinook',\n       'jackal', 'hornet', 'monkey', 'bordeaux', 'reindeer', 'squid',\n       'maltese', 'buffalo', 'hedgehog', 'octopus', 'swan', 'husky',\n       'zebu', 'olm', 'retriever', 'lemur', 'dhole', 'rabbit', 'loon',\n       'cat', 'millipede', 'flamingo', 'opossum', 'turtle', 'eagle',\n       'eleuth', 'binturong', 'uguisu', 'whippet', 'tiger', 'lobster',\n       'macaque', 'scorpion', 'goat', 'tapir', 'audemer', 'fox', 'molly',\n       'discus', 'gopher', 'gerbil', 'civet', 'gecko', 'dog', 'squirt',\n       'insect', 'tarsier', 'whale', 'paradise', 'deer', 'urchin',\n       'serval', 'rhinoceros', 'numbat', 'frigatebird', 'catfish', 'kiwi',\n       'bee', 'seahorse', 'beagle', 'tzu', 'dodo', 'mayfly', 'impala',\n       'dachshund', 'budgerigar', 'moose', 'labradoodle', 'spider',\n       'flounder', 'woodpecker', 'bobcat', 'corgi', 'buzzard', 'clam',\n       'hamster', 'bandicoot', 'mandrill', 'lemming', 'snail', 'havanese',\n       'hyena', 'monster']\ngoal_arr = ['pembus', 'ordinal', 'goose', 'distraction', 'golden', 'entropy',\n       'unsorted', 'sorted', 'important', 'fimbus', 'grandmaster',\n       'sumble', 'noise', 'discard', 'dummy', 'fepid', 'contributor',\n       'learn', 'dataset', 'master', 'expert', 'kernel', 'hint', 'novice',\n       'gaussian']\nfor _ in range(255):\n    new_col_name = np.random.choice(kind_arr) + '-' + np.random.choice(color_arr) + '-' + np.random.choice(animal_arr) + '-' + np.random.choice(goal_arr)\n    col_names.append(new_col_name)\ncol_names.append('wheezy-copper-turtle-magic')\n\n\n# build the dataframe\ntrain = pd.DataFrame(train, columns=col_names)\ntrain['target'] = target","747939ed":"import hashlib\n\ndef generate_hashed_id(inp):\n    return hashlib.md5(bytes(f'{inp}train', encoding='utf-8')).hexdigest()\n    \ntrain['id'] = train.index\ntrain['id'] = train['id'].apply(generate_hashed_id)\n\n# re-arrange columns\ncols = [c for c in train.columns if c not in ['id', 'target']]\ntrain = train[['id', 'target']+cols]","2992a91f":"train.head()","9aff058f":"train.target.hist()","7eec5f63":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.distplot(train[train.columns[10]])\nplt.figure()\nsns.distplot(train[train.columns[210]])\n","62247d88":"train_corr = train.iloc[:,:50].drop([\"target\", 'id'], axis=1).corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(train_corr, vmin=-0.016, vmax=0.016, cmap=\"RdYlBu_r\");","06178d42":"columns = [x for x in train.columns if x not in [\"target\", 'id', 'wheezy-copper-turtle-magic']]\ntrain2 = train.loc[train['wheezy-copper-turtle-magic']==0, columns]\nplt.figure(figsize=(6,6))\nplt.plot(train2.std())","a9c7ca76":"sns.distplot(train2[train.columns[10]])\nplt.figure()\nsns.distplot(train2[train.columns[210]])","6ce4503c":"import numpy as np, pandas as pd, os\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\ncols = [c for c in train.columns if c not in ['id', 'target']]\noof = np.zeros(len(train))\nskf = StratifiedKFold(n_splits=5, random_state=42)\n   \nfor train_index, test_index in skf.split(train.iloc[:,1:-1], train['target']):\n    clf = LogisticRegression(solver='liblinear',penalty='l2',C=1.0)\n    clf.fit(train.loc[train_index][cols],train.loc[train_index]['target'])\n    oof[test_index] = clf.predict_proba(train.loc[test_index][cols])[:,1]\n    \nauc = roc_auc_score(train['target'],oof)\nprint('LR without interactions scores CV =',round(auc,5))","7af52f36":"# INITIALIZE VARIABLES\ncols.remove('wheezy-copper-turtle-magic')\ninteractions = np.zeros((512,255))\noof = np.zeros(len(train))\n\n# BUILD 512 SEPARATE MODELS\nfor i in range(512):\n    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index\n    train2.reset_index(drop=True,inplace=True)\n    \n    skf = StratifiedKFold(n_splits=25, random_state=42)\n    for train_index, test_index in skf.split(train2.iloc[:,1:-1], train2['target']):\n        # LOGISTIC REGRESSION MODEL\n        clf = LogisticRegression(solver='liblinear',penalty='l1',C=0.05)\n        clf.fit(train2.loc[train_index][cols],train2.loc[train_index]['target'])\n        oof[idx1[test_index]] = clf.predict_proba(train2.loc[test_index][cols])[:,1]\n\n        \n# PRINT CV AUC\nauc = roc_auc_score(train['target'],oof)\nprint('LR with interactions scores CV =',round(auc,5))","7f69842b":"# LOAD LIBRARIES\nfrom sklearn.svm import SVC\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n# INITIALIZE VARIABLES\noof = np.zeros(len(train))\ncols = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]\n\n# BUILD 512 SEPARATE NON-LINEAR MODELS\nfor i in range(512):\n    \n    # EXTRACT SUBSET OF DATASET WHERE WHEEZY-MAGIC EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index\n    train2.reset_index(drop=True,inplace=True)\n    \n    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n    sel = VarianceThreshold(threshold=1.5).fit(train2[cols])\n    train3 = sel.transform(train2[cols])\n        \n    # STRATIFIED K FOLD (Using splits=25 scores 0.002 better but is slower)\n    skf = StratifiedKFold(n_splits=11, random_state=42)\n    for train_index, test_index in skf.split(train3, train2['target']):\n        \n        # MODEL WITH SUPPORT VECTOR MACHINE\n        clf = SVC(probability=True,kernel='poly',degree=4,gamma='auto')\n        clf.fit(train3[train_index,:],train2.loc[train_index]['target'])\n        oof[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n        \n    #if i%10==0: print(i)\n        \n# PRINT VALIDATION CV AUC\nauc = roc_auc_score(train['target'],oof)\nprint('CV score =',round(auc,5))","52bfc43d":"Last but not least, let's filter `train.csv` by the magical feature `wheezy-copper-turtle-magic==0` and see what happens.","9b8a6a01":"Now that the dataset is ready let's take a look at it from different aspects to see if it's similar enough?\nFirst we're gonna look at the head of dataframe.","f963c63d":"Below is the code from Chris' kernel https:\/\/www.kaggle.com\/cdeotte\/support-vector-machine-0-925 which has CV score of 0.9262.\n\nLet's train it on this dataset and compare the CV score.","7799a6a4":"Oh cool! we have those useful\/useless columns in here too. Surprisingly enough, useless cols have std about 1.0 and useful columns have an STD of about 3.7. Thanks to Chris for his discussion https:\/\/www.kaggle.com\/c\/instant-gratification\/discussion\/92930#latest-538458\n\nwhat about the distributions now? do they look normal after filtering?","7d5721e7":"Below is the code from Chris' kernel https:\/\/www.kaggle.com\/cdeotte\/logistic-regression-0-800 which has CV score of 0.80549.\n\nLet's train it on this dataset and compare the CV score.","93a904dd":"What about the correlation plot? (it was taking a long time to run, so I restricted it to only the first 50 columns) Compare the results with Allunia's EDA at https:\/\/www.kaggle.com\/allunia\/instant-gratification-some-eda-to-go","d15e5730":"We have super balanced target classes in Kaggle's train.csv, Does sklearn generates balanced target classes? i.e. do we have super balanced target classes? Yes!","8d623fbb":"Yes they do. ","14bc2436":"EDA shows this dataset is very similar to competition's data. \nNow it's time to build some models on top of this dataset;\n\nBelow is the code from Chris' kernel https:\/\/www.kaggle.com\/cdeotte\/logistic-regression-0-800 which has CV score of 0.52994. \n\nLet's train it on this dataset and compare the CV score.","a9e4e598":"## Final notes\n* It is very likely that Kaggle used a similar way to generate this competition's dataset. Yet nothing's for sure and they might have tricked a few parts of it so keep searching.\n* I believe if you play with `flip_y` and `n_informative` in `make_classification` you can get closer CV scores than mine to those mentioned above.\n* I would like to express my gratitude to Chris, Bojan, and other kagglers who generously share their findings with us\n* dont forget to upvote all the kernels\/discussions linked here \n* please let me know if I used parts of your work here and forgot to link","1d36002c":"As of now, despite that only a few days have passed since this competition started, we know a lot about the structure of dataset. We know about ID's, splits, etc. etc.\nI tried to put everything we knew so far together and generate a synthetic `train.csv` dataset from scratch as similar as possible to Kaggle's `train.csv`.\nI then ran some EDA on it and also trained some of the public kernels on this synthetic dataset and compared the CV score to theirs. \n\nHope you enjoy this kernel. Don't forget to upvote and share your opinions.\n\n\n\nFirst, we generate the dataset from 512 different classification datasets by using sklearn's `make_calssification`. ","c5a1025c":"Silly column names abound;\n\nlet's add some names to our features. It must be silly and cryptic so let's have this format of `kind-color-animal-goal`. \n\nNote that if you want it to look more cryptic you can rename some of them and replace `goal` part with `animal` part or `animal` part with `color` part so that the competitors spend some time on that too. For example make something like `slimy-seashell-cassowary-goose` (this column actually exists in Kaggle's train.csv).\n\nYou can also add some probabilities in `np.random.choice()` to make them look as such that there is pattern.","8148526b":"Add mysterious IDs. \n\nAs Yirun found in https:\/\/www.kaggle.com\/c\/instant-gratification\/discussion\/92634#latest-537452, md5 hashing seems cool.","0f9fe029":"How about the histograms? Are they similar to Kaggle's train.csv? Compare the shapes of this dataset to Bojan's EDA at https:\/\/www.kaggle.com\/tunguz\/instant-eda"}}