{"cell_type":{"7e477b58":"code","273fb1d7":"code","07aa9344":"code","96c50c28":"code","a7265d8a":"code","74f17451":"code","88d4e5c2":"code","0f2876bb":"code","983dba3b":"code","15c8f7c1":"code","94b0f198":"code","923b340b":"code","aa1f9924":"code","b52af7d6":"code","6b70e013":"code","4767e4d0":"code","86bad10a":"code","60cc3d0e":"code","178e5cc1":"code","def5dc2c":"code","7e5698ef":"code","d2296e92":"code","bcaf12dc":"code","157a8cbe":"code","526ab269":"code","d0fdab8c":"code","01d7e507":"markdown","2a2ba6c4":"markdown","9aa37aa2":"markdown","f805da5e":"markdown","7ee034d6":"markdown","3891027d":"markdown","c3136d3b":"markdown","7883fa39":"markdown","bca6ce50":"markdown","978b9843":"markdown","c1453340":"markdown","8a4db841":"markdown","3d593438":"markdown","7accd309":"markdown","26a55555":"markdown","3463c77f":"markdown","e21b0de1":"markdown","b534145b":"markdown"},"source":{"7e477b58":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom collections import Counter\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","273fb1d7":"data = pd.read_csv(\"\/kaggle\/input\/pokemon\/Pokemon.csv\")","07aa9344":"data.head(10)","96c50c28":"data.info()","a7265d8a":"data.rename(columns={\"Type 1\":\"Type1\", \"Type 2\":\"Type2\", \"Sp. Atk\":\"Sp.Atk\", \"Sp. Def\":\"Sp.Def\"},inplace=True)\ndata.columns = data.columns.str.strip()","74f17451":"print(\"Types:\",data[\"Type1\"].unique().tolist())\nprint(\"Amount of types:\",len(data[\"Type1\"].unique().tolist()))","88d4e5c2":"data.drop(\"Type2\",axis = 1,inplace=True)","0f2876bb":"data[\"Type1\"].value_counts()","983dba3b":"# objects = (\"Water\",\"Normal\",\"Grass\",\"Bug\",\"Psychic\",\"Fire\",\"Rock\",\"Electric\",\"Dragon\",\"Ground\",\"Ghost\",\"Dark\",\"Poison\",\"Fighting\",\"Steel\",\"Ice\",\"Fairy\",\"Flying\")\n# y_pos = np.arange(len(objects))\n# performance = [112,98,70,69,57,52,44,44,32,32,32,31,28,27,27,24,17,4]\n# \n# plt.figure(figsize=(22,10))\n# plt.bar(y_pos, performance, align='center', alpha=0.6)\n# plt.xticks(y_pos, objects)\n# plt.xlabel(\"Types\")\n# plt.ylabel('Frequency')\n# plt.title('Pokemon Types')\n# plt.show()","15c8f7c1":"#CATEGOR\u0130CAL PLOTTING\nvar= data[\"Type1\"]\n#count number of categorical variable(value\/sample)\nvarValue = var.value_counts()\n\n#visualize\nplt.figure(figsize=(20,10))\nplt.bar(varValue.index,varValue)\nplt.xticks(varValue.index,varValue.index.values) #With this command we can limit the ticks  \nplt.ylabel(\"Frequency\")\nplt.title(\"Pokemon Types\")\nplt.show()","94b0f198":"def plot_bar(variable):\n    \n    var = data[variable]\n    varValue = var.value_counts()\n    \n    plt.figure(figsize=(15,10))\n    plt.bar(varValue.index,varValue)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distrubution with barplot\".format(variable))\n    plt.show()","923b340b":"numericVar = [\"HP\",\"Attack\",\"Total\",\"Defense\",\"Sp.Atk\",\"Sp.Def\",\"Speed\"]\nfor n in numericVar:\n    plot_bar(n)","aa1f9924":"data.drop([\"Name\"],axis=1,inplace=True)\ndata.drop([\"#\"],axis =1, inplace=True)","b52af7d6":"#List Comprehension \ndata[\"Type1\"] = [0 if i == \"Water\" else 1 if i == \"Normal\" else 2 if i== \"Grass\" else 3 if i==\"Bug\" else 4 if i==\"Fire\" or i==\"Psychic\" else 5 if i==\"Electric\" or i==\"Rock\"\n                 else 6 if i==\"Dragon\" or i==\"Ground\" else 7 if i==\"Ghost\" or i==\"Dark\" else 8 if i==\"Poison\" or i==\"Fighting\" else 9 if  i==\"Steel\" or i==\"Ice\" else 10 for i in data[\"Type1\"]]","6b70e013":"#CATEGOR\u0130CAL PLOTTING\nvar= data[\"Type1\"]\n#count number of categorical variable(value\/sample)\nvarValue = var.value_counts()\n\n#visualize\nplt.figure(figsize=(20,10))\nplt.bar(varValue.index,varValue)\nplt.xticks(varValue.index,varValue.index.values) #With this command we can limit the ticks  \nplt.ylabel(\"Frequency\")\nplt.title(\"Pokemon Types\")\nplt.show()","4767e4d0":"data.info()","86bad10a":"data.loc[:,\"Type1\"].value_counts()\ny = data.Type1.values\nx_data = data.iloc[:,2:]\nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data)) #Normalization\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=42)","60cc3d0e":"def initialize_weights_and_bias_NN(x_train,y_train):\n    parameters = {\"weight1\":np.random.randn(3,x_train.shape[0])*0.1, #3 x 600\n                  \"bias1\":np.zeros((3,1)),\n                  \"weight2\":np.random.randn(y_train.shape[0],3)*0.1, #600 x 3\n                  \"bias2\":np.zeros((y_train.shape[0],1))} \n        # We define 3x1 because z2 = w2*A1 + b2 then A1 is 3x1 and\n        # when we want to matris product w2 must be 1x3 \n    return parameters \n\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","178e5cc1":"def forward_propagation_NN(x_train,parameters):\n    Z1 = np.dot(parameters[\"weight1\"],x_train) + parameters[\"bias1\"]\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(parameters[\"weight2\"],A1) + parameters[\"bias2\"]\n    A2 = sigmoid(Z2)\n    \n    cache={\"Z1\":Z1,\n           \"A1\":A1,\n           \"Z2\":Z2,\n           \"A2\":A2}\n    return A2,cache","def5dc2c":"def loss_cost_NN(A2,y,parameters): #A2 is an output actually but we gonna use it input for cost function\n    logaritmicprobs = np.multiply(np.log(A2),y)\n    cost = -np.sum(logaritmicprobs)\/y.shape[1]\n    return cost","7e5698ef":"def backward_propagation_NN(cache,parameters,x,y): #Derivative Part\n    dZ2 = cache[\"A2\"]-y\n    dW2 = np.dot(dZ2,cache[\"A1\"].T)\/x.shape[1]\n    db2 = np.sum(dZ2,axis =1,keepdims=True)\/x.shape[1]\n    dZ1 = np.dot(parameters[\"weight2\"].T,dZ2)*(1 - np.power(cache[\"A1\"], 2))\n    dW1 = np.dot(dZ1,x.T)\/x.shape[1]\n    db1 = np.sum(dZ1,axis =1,keepdims=True)\/x.shape[1]\n    grads = {\"dweight1\": dW1,\n             \"dbias1\": db1,\n             \"dweight2\": dW2,\n             \"dbias2\": db2}\n    return grads","d2296e92":"def update_parameters_NN(parameters,grads,learning_rate = 0.01):\n    parameters = {\"weight1\": parameters[\"weight1\"]-learning_rate*grads[\"dweight1\"],\n                      \"bias1\": parameters[\"bias1\"]-learning_rate*grads[\"dbias1\"],\n                      \"weight2\": parameters[\"weight2\"]-learning_rate*grads[\"dweight2\"],\n                      \"bias2\": parameters[\"bias2\"]-learning_rate*grads[\"dbias2\"]}\n    \n    return parameters","bcaf12dc":"def prediction_NN(parameters,x_test): #Remember predictions always apply w\/test data\n    A2,cache = forward_propagation_NN(x_test,parameters)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5 our prediction is sign one (y_head=1),\n    # if z is equal or smaller than 0.5, our prediction is sign zero (y_head=0) >> Sigmoid func\n    \n    for i in range(A2.shape[1]):\n        if A2[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n            \n    return Y_prediction","157a8cbe":"def two_layer_NN(x_train,y_train,x_test,y_test,num_iterations):\n    cost_list = []\n    index_list = []\n    #initializing parameters and layer size\n    parameters = initialize_weights_and_bias_NN(x_train,y_train)\n    \n    for i in range(0,num_iterations):\n        #Forward propagation \n        A2,cache = forward_propagation_NN(x_train,parameters)\n        #Computing Cost \n        cost = loss_cost_NN(A2,y_train,parameters)\n        #Backward Propagation\n        grads = backward_propagation_NN(parameters,cache,x_train,y_train)\n        #Updating Parameters \n        parameters = update_parameters_NN(parameters,grads)\n        \n        if i % 50 == 0: #Per 50 iterations\n            cost_list.append(cost)\n            index_list.append(i)\n            print(\"Cost after Iteration %i: %f\" %(i,cost))\n            \n    plt.plot(index_list,cost_list)\n    plt.xticks(index_list,rotation = 45)\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    #Prediction \n    y_prediction_test = prediction_NN(parameters,x_test)\n    y_prediction_train  = prediction_NN(parameters,x_train)\n    \n    #Printing Train & Test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return parameters\n\nparameters = two_layer_NN(x_train,y_train,x_test,y_test,num_iterations=2500)","526ab269":"#reshape - keras requires tranpose of data\nx_train,x_test,y_train,y_test = x_train.T, x_test.T ,y_train.T, y_test.T","d0fdab8c":"# L-layer NN w\/Keras\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score #cross_val_score = 2 means 4 part train 1 part test\nfrom keras.models import Sequential # Initializing NN Library\nfrom keras.layers import Dense #Layer Builder\n\ndef build_classifier():\n    classifier = Sequential() #\u0130nitializing NN\n    classifier.add(Dense(units = 16, kernel_initializer = \"uniform\", activation =\"relu\", input_dim = x_train.shape[1])) # First Hidden layer, we defined the input dimension 600\n    classifier.add(Dense(units = 12, kernel_initializer = \"uniform\", activation =\"relu\"))\n    classifier.add(Dense(units = 8, kernel_initializer = \"uniform\", activation =\"relu\"))\n    classifier.add(Dense(units = 6, kernel_initializer = \"uniform\", activation =\"relu\"))\n    classifier.add(Dense(units = 4, kernel_initializer = \"uniform\", activation =\"relu\"))\n    classifier.add(Dense(units = 2, kernel_initializer = \"uniform\", activation =\"relu\"))\n    classifier.add(Dense(units = 1, kernel_initializer = \"uniform\", activation = \"sigmoid\")) #means 1 output (1 node) and if we want to create model after hidden layer to output activation = sigmoid\n    classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n    return classifier\n\n#lets call our classifier\nclassifier = KerasClassifier(build_fn = build_classifier, epochs = 500)\naccuracies = cross_val_score(estimator = classifier, X=x_train, y=y_train, cv = 3)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy Mean :\"+str(mean))\nprint(\"Accuracy Variance :\"+str(variance))","01d7e507":"## 1-) 2 Layer Neural Network ","2a2ba6c4":"### 1.5) Backward Propagation \n<img src=\"https:\/\/www.guru99.com\/images\/1\/030819_0937_BackPropaga1.png\" alt=\"9\" border=\"0\">","9aa37aa2":"<a href=\"http:\/\/ibb.co\/eF315x\"><img src=\"http:\/\/preview.ibb.co\/dajVyH\/9.jpg\" alt=\"9\" border=\"0\"><\/a>\n* Step by step we will learn this image.\n    * Our hyperparameters : Number of Hidden Layer , Node ( Number of element in hidden layer - in example there are 3), Learning Rate \n    * Input and output layers do not change. They are same like logistic regression.\n    * In image, there is a tanh function that is unknown for you. It is a activation function like sigmoid function. Tanh activation function is better than sigmoid for hidden units bacause mean of its output is closer to zero so it centers the data better for the next layer. Also tanh activation function increase non linearity that cause our model learning better.\n    * As you can see with purple color there are two parts. Both parts are like logistic regression. The only difference is activation function, inputs and outputs.\n        * In logistic regression: input => output\n        * In 2 layer neural network: input => hidden layer => output. You can think that hidden layer is output of part 1 and input of part 2.\n* Thats all. We will follow the same path like logistic regression for 2 layer neural network.","f805da5e":"# Artifical Neural Network (ANN)","7ee034d6":"- units = node amount in hidden layer\n- kernel_initializer = initialize weight\n- activation = relu,tanh,sigmoid >> relu limits the values 0 by n if input <0 output is 0 but if input >0 output is input for ex : -1 == 0 , 7 == 7\n\n- Ps: Keras requres the defined input and output dimension\n- input_dim = input dimension \n- loss = \"binary_crossentropy\" is cost func same in logistic regression \n- optimizer =\"adam\" adam means adaptive momentum and \"adam\" provides us adaptive learning rate, with adam optimizer we can run our model more faster and effectively \n- metrics is our criteria to rating model performance\n- cross_val_score is cross validation gives us many accuracies and get average of them\n- epochs = number of iteration \n- cv = 3 means model will do cross validation 3 times and get average of them\n\nThis formula similar to Gradient Descent \n\n$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$ \n","3891027d":"### 1.1) Train & Test Split","c3136d3b":"### 1.8) Creating Artificial Neural Network Model - 2 Layer","7883fa39":"### 1.2) Initialize Weight & Bias and Sigmoid Function \n<img src=\"https:\/\/miro.medium.com\/max\/4000\/1*JHWL_71qml0kP_Imyx4zBg.png\" alt=\"3\" border=\"0\">","bca6ce50":"### 1.4) Loss and Cost Function \n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*EJPT0utTkQ2qrHfjDID5RA.png\" alt=\"9\" border=\"0\">","978b9843":"## 2) L Layer Neural Networks w\/Keras Library \n<img src=\"https:\/\/pskp-95.github.io\/public\/images\/multiple_layers.png\" alt=\"9\" border=\"0\">\n* There are some hyperparameters we need to choose like learning rate, number of iterations, number of hidden layer, number of hidden units, type of activation functions. Woww it is too much :)\n* These hyperparameters can be chosen intiutively if you spend a lot of time in deep learning world.\n* However, if you do not spend too much time, the best way is to google it but it is not necessary. You need to try hyperparameters to find best one.\n* In this tutorial our model will have 2 hidden layer with 8 and4 nodes, respectively. Because when number of hidden layer and node increase, it takes too much time. \n* As a activation function we will use *elu(first hidden layer), relu(second hidden layer) and sigmoid(output layer) respectively.\n* Number of iteration will be 100.","c1453340":"TYPE 1 Categorization\n- 0 - Water \n- 1 - Normal\n- 2 - Grass \n- 3 - Bug\n- 4 - Fire + Psychic\n- 5 - Electric + Rock \n- 6 - Dragon + Ground\n- 7 - Ghost + Dark \n- 8 - Poison + Fighting \n- 9 - Steel + Ice \n- 10 - Fairy + Flying\n","8a4db841":"### 1.3) Forward Propagation \n<img src=\"http:\/\/preview.ibb.co\/dajVyH\/9.jpg\" alt=\"9\" border=\"0\">","3d593438":"### 1.7) Prediction Part ","7accd309":"Let's drop the Type2 column \n","26a55555":"Now, all of the columns are int64 okey let's start the modelling ","3463c77f":"Firstly we need get rid of the spaces between column names to be easier, you can do it like that or you can use split() method also","e21b0de1":"-  Neural Network is one of the provided version of Logistic Regression \n-  Remember!!! In the Logistic Regression there are input and output layers \n-  In ANN there must be minimum 1 hidden layer between input and output layer. When hidden layer amount increase model will be more complicated.\n-  When model getting more complicated our model would be more succesfull but more slow then we need find the optimum option \n-  Most of steps are same and there is just one difference that is Layer Term, we going to see the details (\"What is Layer?\") \n-  In other words ANN is applying Logistic Regression minimum 2 times (if you apply 2 times called as 2-Layer, more >> L-Layer ANN)\n-  Okay how will we choose the exact number of layer, what is the optimum layer amount for models ? \n-  Actually it depends your hardware. For instance in my device 5 - 6 hidden layer force the hardware. For commercial projects 120-130 hidden layer might optimum value\n-  When we say 2 layer network means 1 Hidden + 1 Output Layer >>(Input layer doesn't count as layer) \n- ## Let's Start ","b534145b":"### 1.6) Updating Parameters \n- Weight = Weight - (learning rate x Derivative Weight)\n- Bias = Bias - (learning rate x Derivative Bias)"}}