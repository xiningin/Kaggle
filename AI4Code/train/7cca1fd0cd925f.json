{"cell_type":{"902152fa":"code","9ef54e69":"code","293be1ea":"code","68b4a0a0":"code","6e3e22bf":"code","dc2e36df":"code","01cf02ff":"code","ca878d8c":"code","fe424130":"code","d11842a8":"code","d66bec73":"code","fbfb5996":"code","12a30aad":"code","f519d218":"code","0a693b69":"code","952e2127":"code","34f7bed9":"code","4eae886f":"code","f1ffcfda":"code","6149647b":"code","34a978d7":"code","bb8b8b40":"code","08fd1a3f":"code","ef30d386":"code","33306c1f":"code","ab47da20":"code","2d269f3e":"code","995e074b":"code","e46ca70a":"code","6ca3e35e":"code","032416a3":"code","eccfb6dd":"code","597a82fc":"code","c4c14cdc":"code","a053b486":"code","c280e536":"code","d0215b61":"code","381e3ac8":"code","58f27b1c":"code","5bfe9305":"code","e0843232":"code","0b41be2b":"code","b198971d":"code","30bdbc6f":"code","e5ed31dd":"code","3af4d5fc":"code","4e07b4f2":"code","84b0aacf":"code","86bff4d8":"code","277178bf":"code","6cbcd5aa":"code","a3132999":"code","d3523c27":"code","2666d8a0":"code","a09a3db8":"code","f591057b":"code","c582c659":"code","6f23e770":"markdown","0094003a":"markdown","5016748d":"markdown","60680fa6":"markdown","10cd3f0c":"markdown","9857b817":"markdown","24279553":"markdown","341860b1":"markdown","184bfe12":"markdown","54e2a6ac":"markdown","2f69390a":"markdown","c880d743":"markdown","ed61dc7d":"markdown","84489f96":"markdown","39c64d92":"markdown","8a67f892":"markdown","b21bdc41":"markdown","d3f42685":"markdown","1ff7d3ea":"markdown","ea63ebed":"markdown","a68dd3ef":"markdown","af3689cb":"markdown","fa8436d8":"markdown","256d8a70":"markdown","87bb830c":"markdown","f6d63fc2":"markdown","d061886d":"markdown","1fc01b7f":"markdown","2f0aa113":"markdown","9b803c01":"markdown","1eb070a6":"markdown","9149dee1":"markdown","caf56f06":"markdown","56c73ffd":"markdown","049c45e3":"markdown","6d24e6d5":"markdown","a742e859":"markdown","b8780866":"markdown","e37770a3":"markdown","bee2a7b6":"markdown","c0d0e6d3":"markdown","876499aa":"markdown","5b8d242a":"markdown","04ce304e":"markdown","fc30574b":"markdown","5ce8dd14":"markdown","f0b137d8":"markdown","8fcb5455":"markdown","0cf7e6f2":"markdown","51a09006":"markdown","4196fccc":"markdown","8138bd98":"markdown","a993b957":"markdown","571a9e3b":"markdown","ff8f5a42":"markdown","65b7752f":"markdown","3a14e511":"markdown","c37be69f":"markdown","312067b2":"markdown","9ede9d84":"markdown","36b179f6":"markdown","81be72f4":"markdown","ae368bf7":"markdown","f6914558":"markdown","1e1dd5c1":"markdown","72796341":"markdown","d9773286":"markdown","b4b45cb7":"markdown","aa894b4d":"markdown","5ca046d5":"markdown","9e12f5c2":"markdown","8a23c7b1":"markdown","dfaa0f4e":"markdown","79c6c9b1":"markdown","1c01bd0d":"markdown","d76d0fe2":"markdown","30a2d3bb":"markdown","e641f33c":"markdown","8c7086bf":"markdown","2e3b5741":"markdown","5be3aa00":"markdown","a44e1fbb":"markdown","bb244da9":"markdown","31e41770":"markdown","000c5f63":"markdown","f3858bae":"markdown","dbd3a441":"markdown","b56cb6b3":"markdown","07f7996b":"markdown","fab58fb5":"markdown","4e5ad562":"markdown"},"source":{"902152fa":"# general\nimport pandas as pd\nimport numpy as np\nimport re\nimport time\n\n# visualization tools\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\n\n# preprocessing\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom sklearn.pipeline import Pipeline\n\n# models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.linear_model import LogisticRegression \nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom skopt import BayesSearchCV\n\n# scores\nfrom sklearn.metrics import accuracy_score, precision_recall_curve, f1_score, roc_auc_score, roc_curve, confusion_matrix, average_precision_score","9ef54e69":"import sys\nimport warnings\nif not sys.warnoptions:\n       warnings.simplefilter(\"ignore\")","293be1ea":"try:\n    df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv', sep=',')\nexcept:\n    df = pd.read_csv('..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv', sep=',')","68b4a0a0":"# function to describe the data on one dataframe\ndef get_df_info(df):\n    print('getting the info for the dataframe:')\n    df.info();\n    print()\n    print('Here are the first and the last 20 rows to get some idea about the data:')\n    with pd.option_context('display.max_rows', 20, 'display.max_columns', None):\n        display(df.head(20))\n        display(df.tail(20))\n        display('The description of the data gives us a good information about the colums like min and max values:')\n        display(df.describe(include='all'))\n    \n    print ('Duplicate entries in the table:', df.duplicated().sum())\n\nget_df_info(df)\nprint()","6e3e22bf":"with pd.option_context('display.max_rows', 20, 'display.max_columns', None):\n    display(df.describe(include='object'))","dc2e36df":"df.info()","01cf02ff":"for col in ('Partner', 'Dependents', 'PhoneService', 'OnlineSecurity', 'OnlineBackup',\n            'DeviceProtection', 'TechSupport', 'StreamingTV',\n            'StreamingMovies', 'MultipleLines', 'PaperlessBilling', 'Churn'):\n    df[col] = df[col].apply(lambda x: 1 if x == 'Yes' else 0)\n    print(df[col].value_counts())\n    df[col] = df[col].astype('int32')","ca878d8c":"df.loc[df['gender']=='Male','gender']=1\ndf.loc[df['gender']=='Female','gender']=0\ndf['gender']=df['gender'].astype('int32')","fe424130":"df.loc[df['TotalCharges']==' ','TotalCharges']=np.nan\ndf = df.dropna()","d11842a8":"df['TotalCharges']=df['TotalCharges'].astype('float64')\nprint(df['TotalCharges'].describe())","d66bec73":"df.columns = df.columns.str.lower()\ndf.info()","fbfb5996":"plt.hist(x=df.query('churn==0')['tenure'],bins=24,label='No-Churn')\nplt.hist(x=df.query('churn==1')['tenure'],bins=24,label='Churn')\nplt.legend()\nplt.title(\"Distribution Tenure for No-Churn and Churn customers\")\nplt.xlabel('Tenure')\nplt.ylabel('count');","12a30aad":"df.monthlycharges.describe()","f519d218":"df.info() # change type to contract type with 0 1 2 also for paymentmethod and internetservice\nprint(df.contract.value_counts(normalize=True))\nprint(df.paymentmethod.value_counts(normalize=True))\ndf.internetservice.value_counts(normalize=True)","0a693b69":"df['contract_type'] = df.contract.apply(lambda x: 0 if \"Month-to-month\" in x else 1 if \"One year\" in x else 2)","952e2127":"print(df.contract.value_counts())","34f7bed9":"df.info()","4eae886f":"df['autopay'] = df.paymentmethod.apply(lambda x: 0 if \"(automatic)\" not in x else 1)","f1ffcfda":"print(df.paymentmethod.value_counts())\ndf.autopay.value_counts()","6149647b":"print(df.churn.value_counts(normalize=True))\ndf.churn.value_counts()","34a978d7":"fig = px.histogram(df.churn.astype('bool'), labels={'value':'Churn','color':'Internet service'},color=df.internetservice,title='Churn distribution colored with Internet service')\nfig.show()\n","bb8b8b40":"fig = px.histogram(df.churn.astype('bool'), labels={'value':'Churn','color':'Contract type'},color=df.contract,title='Churn distribution colored with Contract type')\nfig.show()\n","08fd1a3f":"df.hist(edgecolor='black', linewidth=1.2, figsize=(15,15));","ef30d386":"plt.figure(figsize=(15,7))\nplt.subplot(2,3,1)\nsns.violinplot(x='churn', y = 'monthlycharges', data=df)\nplt.subplot(2,3,2)\nsns.violinplot(x='churn', y = 'totalcharges', data=df)\nplt.subplot(2,3,3)\nsns.violinplot(x='churn', y = 'partner', data=df)\nplt.subplot(2,3,4)\nsns.violinplot(x='churn', y = 'contract_type', data=df)\nplt.subplot(2,3,5)\nsns.violinplot(x='churn', y = 'tenure', data=df);","33306c1f":"plt.hist(x=df.query('churn==0')['tenure'],bins=24,label='No-Churn')\nplt.hist(x=df.query('churn==1')['tenure'],bins=24,label='Churn')\nplt.legend()\nplt.title(\"Distribution of Tenure for No-Churn and Churn customers\")\nplt.xlabel('Tenure (months)')\nplt.ylabel('count of customers');","ab47da20":"plt.figure(figsize=(25,15))\nax = sns.heatmap(df.corr(), annot=True, cmap='cividis')\nplt.title(\"Correclation matrix of the numeric parameters\")\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5);\nplt.yticks(rotation=0);","2d269f3e":"df.info()","995e074b":"# dropping the columns that are not a real feature\ndata=df.drop(['customerid','contract'], axis=1)\nwith pd.option_context('display.max_rows', 20, 'display.max_columns', None):\n    display(data.sample(10))","e46ca70a":"# We will do OHE before Train\/Test split for simplicity\nohe_encoding = ['paymentmethod','internetservice']\ndata_ohe = pd.get_dummies(data[ohe_encoding], drop_first=True)\ndata_ohe.head(5)","6ca3e35e":"# We join back the OHE columns and drop the original ones\ndata = data.join(data_ohe).drop(ohe_encoding, axis=1)","032416a3":"data.info()","eccfb6dd":"features_corr=data.drop(['churn'], axis=1).apply(lambda x: abs(x.corr(df['churn'])))","597a82fc":"features_corr.sort_values()","c4c14cdc":"df_features= pd.DataFrame(features_corr).reset_index()\ndf_features.columns=['feature','corr']","a053b486":"df_features.query('corr>0.2').sort_values(ascending=False, by='corr')","c280e536":"# print the number of rows using len()\ndata_total=len(data)\nprint('Total Rows:', data_total)","d0215b61":"\nfeatures_train, features_test, target_train, target_test = train_test_split(\n        data.drop('churn', axis=1), data.churn, test_size=0.2, random_state=12345\n    )\nprint('The Train Set total rows for the features and the target:',len(features_train),len(target_train))\nprint('The Test Set total rows for the features and the target:',len(features_test),len(target_test))","381e3ac8":"features_train.head(5)\n# checking the test for the most important features by the correlation with the target\nfeatures_test[df_features.query('corr>0.2').feature].head(5)","58f27b1c":"def train_predict(features_train, target_train, features_test, target_test, parameters, name, model):\n    ''' Gets features_train, target_train, features_test, target_test, parameters, name, model\n          Returns a DataFrame of one row with the train and predict results of the model which are also printed on the screen\n    '''\n    \n    pipe = Pipeline([('scale', MinMaxScaler()), ('model', model)])\n    scoring = 'roc_auc'\n    grid = BayesSearchCV(pipe, parameters, scoring=scoring, random_state=12345, n_iter=16, cv=8)\n    start_time = time.time()\n    grid.fit(features_train, target_train)\n    fit_time = time.time() - start_time\n    print('Best Model is: ',grid.best_estimator_)\n    fit_score = grid.best_score_.round(2)\n    print('ROC AUC score on the fit set:', fit_score)\n    \n    predicted_test = grid.predict(features_test)\n\n    if hasattr(grid.best_estimator_, 'predict_proba'):\n        predicted_p1_test = grid.predict_proba(features_test)[:, 1]\n        pred_test_score= roc_auc_score(target_test,predicted_p1_test).round(2)\n    else:\n        pred_test_score= roc_auc_score(target_test,predicted_test).round(2)\n    \n    \n    predict_time = (time.time() - start_time) - fit_time\n    print('ROC AUC score on the prediction set:', pred_test_score)\n    fpr, tpr, thresholds = roc_curve(target_test, predicted_p1_test) \n    # < plot the graph >\n    #plt.figure(figsize=(6, 6))   \n    plt.figure()\n    plt.plot(fpr, tpr)\n    # ROC curve for random model (looks like a straight line)\n    plt.plot([0, 1], [0, 1], linestyle='--')\n    # < use the functions plt.xlim() and plt.ylim() to\n    #   set the boundary for the axes from 0 to 1 >\n    plt.ylim([0.0, 1.0])\n    plt.xlim([0.0, 1.0])\n    # < use the functions plt.xlabel() and plt.ylabel() to\n    #   name the axes \"False Positive Rate\" and \"True Positive Rate\" >\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    # < add the heading \"ROC curve\" with the function plt.title() >\n    plt.title('ROC curve')\n    plt.show();\n    print()\n    best_roc = 0\n    for threshold in np.arange(0.1, 0.6, 0.02):\n        predicted_test = predicted_p1_test > threshold\n        f1 = f1_score(target_test,predicted_test)\n        accuracy = accuracy_score(target_test,predicted_test).round(2)\n        roc_auc = roc_auc_score(target_test, predicted_test)\n        if roc_auc> best_roc: \n            best_roc = roc_auc\n            best_threshold = threshold.round(2)\n        print(\n                'Threshold = {:.2f} | accuracy = {:.3f} | roc_auc = {:.3f} | F1 = {:.3f}'.format(\n                    threshold, accuracy, roc_auc, f1\n                )\n            )\n    predicted_test = predicted_p1_test > best_threshold\n    pred_acc_score = accuracy_score(target_test,predicted_test).round(2)\n    print()\n    print('Total ROC AUC score on the prediction set:', pred_test_score,\n          'accuracy:',pred_acc_score, 'for threshold:', best_threshold)\n    \n    #eval_res = evaluate_model(grid.best_estimator_._final_estimator, features_train, target_train, features_test, target_test)\n    # view confusion-matrix\n    # Print the Confusion Matrix and slice it into four pieces\n    # https:\/\/www.kaggle.com\/prashant111\/lightgbm-classifier-in-python\n    cm = confusion_matrix(target_test,predicted_test)\n    print('\\nConfusion matrix\\n\\n', cm)\n    print('\\nTrue Positives(TP) = ', cm[0,0])\n    print('\\nTrue Negatives(TN) = ', cm[1,1])\n    print('\\nFalse Positives(FP) = ', cm[0,1])\n    print('\\nFalse Negatives(FN) = ', cm[1,0]) \n    \n    FNR = (cm[1,0]\/(cm[1,0]+cm[0,0])).round(3)\n    \n    # visualize confusion matrix with seaborn heatmap\n    cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive: Churn', 'Actual Negative: No-Churn'], \n                                     index=['Predict Positive: Churn', 'Predict Negative: No-Churn'])\n    sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n    plt.show();\n    print()\n    \n    try:\n        # get importance\n        importance = grid.best_estimator_._final_estimator.feature_importances_\n        # summarize feature importance\n        feature_importances=pd.DataFrame({'features':features_test.columns,'importance':importance})\n        print(\"Top 10 important features\")\n        display(feature_importances.sort_values('importance',ascending=False).head(10))\n        print(\"Least 10 important features\")\n        display(feature_importances.sort_values('importance').head(10))     \n        #feature_importances.sort_values(by='importance',ascending = True).plot(kind='barh' , figsize=(10,10)) \n    except:\n        try:\n            #importance = grid.best_estimator_._final_estimator.coef_\n            feature_importances=pd.DataFrame({'feature':list(features_test.columns),'importance':[abs(i) for i in grid.best_estimator_._final_estimator.coef_[0]]})\n            # summarize feature importance\n            #feature_importances=pd.DataFrame({'features':features_test.columns,'importance':importance})\n            display(feature_importances.sort_values('importance',ascending=False).head(10))\n            print(\"Less 10 important features\")\n            display(feature_importances.sort_values('importance').head(10))\n            #feature_importances.plot(kind='barh' , figsize=(10,10)) \n        except:\n            print('there is no feature_importances_ attribute on this model')\n\n    return(pd.DataFrame([[name, fit_score, fit_time, pred_test_score, pred_acc_score, FNR, predict_time, grid.best_estimator_]], columns=['Model','fit_score', 'fit_time','pred_roc_score','pred_acc_score', 'pred_FNR', 'predict_time','estimator']))","5bfe9305":"%%time\n\nparams  = {'model__min_samples_leaf':[3, 5, 10, 20], \n           'model__min_samples_split':[60, 90, 120], \n           'model__max_depth': [3, 5, 10]}\nmodel   = DecisionTreeClassifier(random_state=12345)\nname    ='Decision Tree with scaling'\nresults = train_predict(features_train, target_train, \n                        features_test, target_test, \n                        params, name, model)\nwith pd.option_context('display.max_colwidth', 1000):\n    display(results)","e0843232":"%%time\n\nparams= {'model__max_features':[2, 4, 6], \n         'model__min_samples_leaf':[5, 10, 20], \n         'model__n_estimators':[250, 500, 700], \n         'model__max_depth': [5, 10, 20, 30],\n         'model__criterion':['gini', 'entropy']}\n\nmodel = RandomForestClassifier(random_state=12345)\nname  ='Random Forest with scaling'\nrf = train_predict(features_train, target_train, \n                   features_test, target_test, params, name, model)\n#append the new results \nresults = results.append(rf)\nwith pd.option_context('display.max_colwidth', 1000):\n    display(results)","0b41be2b":"%%time\nparams = {'model__solver':['sag','saga','newton-cg'],#'lbfgs', 'liblinear',\n              'model__penalty': ['l2','none'],# 'elasticnet','l1',\n              'model__C':[100, 10, 1.0, 0.1, 0.01]}\nmodel = LogisticRegression(random_state=12345)\nname  ='Logistic Regression with scaling with all features'\nlr = train_predict(features_train, target_train, \n                   features_test, target_test, \n                   params, name, model)\n#append the new results \nresults = results.append(lr)\nwith pd.option_context('display.max_colwidth', 1000):\n    display(results)","b198971d":"%%time\nmodel = LGBMClassifier(random_state=12345)\n\nparams = {    \n    \"model__max_depth\" : [2, 5, 10],\n    \"model__num_leaves\" : [3, 5, 15, 30, 90],\n    \"model__n_estimators\" : [600, 800, 1000],\n    \"model__learning_rate\" : [0.1, 0.3, 0.5, 0.7]\n    }\n\n# # the rest of the hyper parameters in order we would like to add more later:\n# #     \"model__subsample_freq\": [5],\n# #     \"model__subsample\" : [0.3],\n# #     \"model__colsample_bytree\" : [0.05],\n# #     \"model__min_child_samples\": [100],\n# #     \"model__min_child_weight\": [10],\n# #     \"model__reg_alpha\" : [0.12],\n# #     \"model__reg_lambda\" : [15.5],    \"model__min_data_in_leaf\" : [10,100,500,1000]\n\n\nname  ='Light GBM with scaling'\nlgb = train_predict(features_train, target_train, \n                    features_test, target_test, params, name, model)\n#append the new results \nresults = results.append(lgb)\nwith pd.option_context('display.max_colwidth', 1000):\n    display(results)","30bdbc6f":"%%time\n# this step takes about 20 mins\nmodel = XGBClassifier(objective='binary:logistic', random_state=12345)\nparams = {    \n    \"model__max_depth\" : [3, 5, 10],\n    \"model__n_estimators\" : [1000,1200,1350],\n    \"model__learning_rate\" : [0.1,0.3,0.5,0.7],\n    'model__eval_metric': ['auc','aucpr']\n#    'model__min_child_weight': [1, 5, 10]\n#    'model__gamma': [0.01]\n#     'model__subsample': [0.6, 0.8, 1.0],\n#     'model__colsample_bytree': [0.6, 0.8, 1.0]\n    }\n\nname  ='XGBoost with scaling'\nxgbt = train_predict(features_train, target_train, \n                     features_test, target_test, \n                     params, name, model)\n\n#append the new results \nresults = results.append(xgbt)\nwith pd.option_context('display.max_colwidth', 1000):\n    display(results)","e5ed31dd":"%%time\n# this step takes about 20 mins\nmodel = KNeighborsClassifier()\nparams = { \"model__n_neighbors\" : [20, 50, 150, 250] }\n\nname  ='Knn with scaling'\nxgbt = train_predict(features_train, target_train, \n                     features_test, target_test, \n                     params, name, model)\n\n#append the new results \nresults = results.append(xgbt)\nwith pd.option_context('display.max_colwidth', 1000):\n    display(results)","3af4d5fc":"%%time\n\nparams  = {'model__min_samples_leaf':[3, 5, 10, 20], \n           'model__min_samples_split':[60, 90, 120], \n           'model__max_depth': [3, 5, 10]}\nmodel   = DecisionTreeClassifier(class_weight='balanced', random_state=12345)\nname    ='Decision Tree with scaling and class_weight balanced on all features'\ndt = train_predict(features_train, target_train, \n                        features_test, target_test, \n                        params, name, model)\n#append the new results \nresults = results.append(dt)\nwith pd.option_context('display.max_colwidth', 1000):\n    display(results)","4e07b4f2":"%%time\n\nparams= {'model__max_features':[2, 4, 6], \n         'model__min_samples_leaf':[5, 10, 20], \n         'model__n_estimators':[250, 500, 700], \n         'model__max_depth': [5, 10, 20, 30],\n         'model__criterion':['gini', 'entropy']}\n\nmodel = RandomForestClassifier(class_weight='balanced',random_state=12345)\nname  ='Random Forest with scaling and class_weight balanced on all features'\nrf = train_predict(features_train, target_train, \n                   features_test, target_test, params, name, model)\n#append the new results \nresults = results.append(rf)\nwith pd.option_context('display.max_colwidth', 1000):\n    display(results)","84b0aacf":"%%time\n\nparams= {'model__max_features':[2, 4, 6], \n         'model__min_samples_leaf':[5, 10, 20], \n         'model__n_estimators':[250, 500, 700], \n         'model__max_depth': [5, 10, 20, 30],\n         'model__criterion':['gini', 'entropy']}\n\ncolumns=['monthlycharges','totalcharges','tenure','contract_type','internetservice_Fiber optic',\n         'paymentmethod_Electronic check', 'internetservice_No', 'onlinesecurity', 'autopay',\n         'paperlessbilling']\nmodel = RandomForestClassifier(class_weight='balanced',random_state=12345)\nname  ='Random Forest with scaling and class_weight balanced on 10 top important features only'\nrf = train_predict(features_train[columns], target_train, features_test[columns], \n                   target_test, params, name, model)\n                    \n#append the new results \nresults = results.append(rf)\nwith pd.option_context('display.max_colwidth', 1000):\n    display(results)","86bff4d8":"%%time\nparams = {'model__solver':['sag','saga','newton-cg'],#'lbfgs', 'liblinear',\n              'model__penalty': ['l2','none'],# 'elasticnet','l1',\n              'model__C':[100, 10, 1.0, 0.1, 0.01]}\nmodel = LogisticRegression(class_weight='balanced', random_state=12345)\nname  ='Logistic Regression with scaling and class_weight balanced on all features'\nlr = train_predict(features_train, target_train, \n                   features_test, target_test, \n                   params, name, model)\n#append the new results \nresults = results.append(lr)\nwith pd.option_context('display.max_colwidth', 1000):\n    display(results)","277178bf":"%%time\nparams = {'model__solver':['sag','saga','newton-cg'],#'lbfgs', 'liblinear',\n              'model__penalty': ['l2','none'],# 'elasticnet','l1',\n              'model__C':[100, 10, 1.0, 0.1, 0.01]}\nmodel = LogisticRegression(class_weight='balanced', random_state=12345)\ncolumns=['monthlycharges','totalcharges','tenure','contract_type','internetservice_Fiber optic',\n         'paymentmethod_Electronic check', 'internetservice_No', 'streamingmovies', 'streamingtv',\n         'techsupport']\nname  ='Logistic Regression with scaling and class_weight balanced on 10 top important features only'\nlr = train_predict(features_train[columns], target_train, features_test[columns], target_test, params, name, model)\n      \n#append the new results \nresults = results.append(lr)\nwith pd.option_context('display.max_colwidth', 1000):\n    display(results)","6cbcd5aa":"%%time\nmodel = LGBMClassifier(class_weight='balanced',random_state=12345)\n\nparams = {                 \n            \"model__max_depth\" : [2, 5, 10],\n            \"model__num_leaves\" : [3, 5, 15, 30, 90],\n            \"model__n_estimators\" : [800, 1000, 1200],\n            \"model__learning_rate\" : [0.3, 0.5, 0.7, 0.8]\n         }\n\nname  ='Light GBM with scaling and class_weight balanced on all features'\nlgb = train_predict(features_train, target_train, features_test, target_test, params, name, model)\n#append the new results \nresults = results.append(lgb)\nwith pd.option_context('display.max_colwidth', 1000):\n    display(results)","a3132999":"%%time\nmodel = LGBMClassifier(class_weight='balanced',random_state=12345)\n\nparams = {                 \n            \"model__max_depth\" : [2, 5, 10],\n            \"model__num_leaves\" : [3, 5, 15, 30, 90],\n            \"model__n_estimators\" : [800, 1000, 1200],\n            \"model__learning_rate\" : [0.3, 0.5, 0.7, 0.8]\n         }\n\ncolumns=['monthlycharges','totalcharges','tenure','contract_type','gender',\n         'paymentmethod_Credit card (automatic)', 'deviceprotection', 'onlinesecurity', 'autopay',\n         'paymentmethod_Mailed check']\nname  ='Light GBM with scaling and class_weight balanced on 10 top important features only'\nlgb = train_predict(features_train[columns], target_train, features_test[columns], target_test, params, name, model)\n#append the new results \nresults = results.append(lgb)\nwith pd.option_context('display.max_colwidth', 1000):\n    display(results)","d3523c27":"%%time\n# this step takes about 1 hour\n\nmodel = XGBClassifier(random_state=12345,scale_pos_weight=2.76,objective='binary:logistic')\n\nparams = {  \"model__max_depth\" : [3, 5, 10],\n            \"model__n_estimators\" : [1000,1200,1350],\n            \"model__learning_rate\" : [0.1,0.3,0.5,0.7],\n            'model__eval_metric': ['auc','aucpr']\n            }\n\nname  ='XGBoost with scaling with scale_pos_weight'\nxgbt = train_predict(features_train, target_train, features_test, target_test, params, name, model)\n\n#append the new results \nresults = results.append(xgbt)\nwith pd.option_context('display.max_colwidth', 1000):\n    display(results)","2666d8a0":"%%time\n\nmodel = XGBClassifier(random_state=12345,scale_pos_weight=2.76,objective='binary:logistic')\nparams = {\n            \"model__max_depth\" : [3, 5, 10],\n            \"model__n_estimators\" : [1000,1200,1350],\n            \"model__learning_rate\" : [0.1,0.3,0.5,0.7],\n            'model__eval_metric': ['auc','aucpr']\n#             'model__gamma': [0, 0.01, 0.02, 0.03]\n         }\n\nname  ='XGBoost with scaling and scale_pos_weight on Top 10 important features only'\n\ncolumns= ['internetservice_Fiber optic', 'contract_type','internetservice_No','streamingmovies',\n          'paymentmethod_Electronic check', 'tenure', 'onlinesecurity', 'phoneservice', 'techsupport',\n          'onlinesecurity']\n\nxgbt = train_predict(features_train[columns], target_train, features_test[columns], target_test, params, name, model)\n\n#append the new results \nresults = results.append(xgbt)\nwith pd.option_context('display.max_colwidth', 1000):\n    display(results)","a09a3db8":"print(data.columns)\nprint(round(data.churn.value_counts()\/len(data),4))","f591057b":"baseline = pd.DataFrame(0, index=np.arange(len(target_test)), columns=['churn'])\nprint('The baseline roc_auc score:', roc_auc_score(target_test, baseline))\n\ncm = confusion_matrix(target_test,baseline)\nprint('\\nConfusion matrix\\n\\n', cm)\nprint('\\nTrue Positives(TP) = ', cm[0,0])\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\nprint('\\nFalse Positives(FP) = ', cm[0,1])\nprint('\\nFalse Negatives(FN) = ', cm[1,0]) \n\nFNR = (cm[1,0]\/(cm[1,0]+cm[0,0])).round(3)\nprint('FNR:', FNR)\n\n# visualize confusion matrix with seaborn heatmap\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive: Churn', 'Actual Negative: No-Churn'], \n                                 index=['Predict Positive: Churn', 'Predict Negative: No-Churn'])\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\nplt.show();\nprint()\n    ","c582c659":"with pd.option_context('display.max_colwidth', 1000):\n    display(results.query('pred_acc_score > 0.73').drop('estimator',axis=1).sort_values(by=['pred_roc_score','pred_acc_score'],ascending=False).reset_index())","6f23e770":"##### Random Forest Model","0094003a":"* DecisionTreeClassifier(max_depth=5, min_samples_leaf=20, min_samples_split=120, random_state=12345)\n* with score on the fit set: 0.84\n* ROC AUC score on the prediction set: 0.83 accuracy: 0.74 for threshold: 0.24\n* False Negatives(FN) =  64\n* Top important features as we saw as well by the correlation are contract_type and tenure.\n* Less important features are internetservice_No, paymentmethod_Mailed check.","5016748d":"### One-Hot Encoding","60680fa6":"<a id='section2'><\/a>\n## Train the model without taking into account the imbalance. ","10cd3f0c":"* The Top important features by the correlation with the target are:\n    * contract_type\t                 0.396150\n    * tenure\t                     0.354049\n    * internetservice_Fiber optic\t 0.307463\n    * paymentmethod_Electronic check 0.301455\n    * internetservice_No\t         0.227578\n    * autopay\t                     0.210420","9857b817":"* XGBClassifier(base_score=0.5, booster='gbtree',\n                               colsample_bylevel=1, colsample_bynode=1,\n                               colsample_bytree=1, enable_categorical=False,\n                               eval_metric='auc', gamma=0, gpu_id=-1,\n                               importance_type=None, interaction_constraints='',\n                               learning_rate=0.1, max_delta_step=0, max_depth=3,\n                               min_child_weight=1, missing=nan,\n                               monotone_constraints='()', n_estimators=1000,\n                               n_jobs=4, num_parallel_tree=1, predictor='auto',\n                               random_state=12345, reg_alpha=0, reg_lambda=1,\n                               scale_pos_weight=1, subsample=1,\n                               tree_method='exact', validate_parameters=1,\n                               verbosity=None)\n* with ROC AUC score on the fit set: 0.82\n* ROC AUC score on the prediction set: 0.83 \n* accuracy: 0.74 for threshold: 0.24\n* False Negatives(FN) =  87\n* Top important features as we saw as well by the correlation are internetservice_Fiber optic and contract_type the less important feature for LGBM is here the most important.\n* Less important features are monthlycharges and dependents. which is really weird because monthlycharges was in the top for the target correlation, but still we got the best scores.\n* The results are similar to the LGBM results and the time for the model training is very long: 20 minutes.","24279553":"Let's look at the distirubtions for each numeric variable in the dataset:","341860b1":"* changing the contract type to ordinal encoding where the `month-to-month` count as 0 commitment, 1 as 1 year commitment and 2 as 2 years commitment:","184bfe12":"**The constant model accuracy score is 0.734**","54e2a6ac":"* Replace the column names (make them lowercase):","2f69390a":"* RandomForestClassifier(criterion='entropy', max_depth=10,\n                                        max_features=4, min_samples_leaf=10,\n                                        n_estimators=500,\n                                        random_state=12345)\n* ROC AUC with score on the fit set: 0.85 \n* ROC AUC score on the prediction set: 0.85\n* Accuracy: 0.75 for threshold: 0.26\n* False Negatives(FN) =  68\n* Top important features as we saw as well by the correlation are contract_type and tenure.\n* Less important features are phoneservice, paymentmethod_Credit card (automatic).\n* The results are better than the DT.\n* The results are the best but the time for the model training is much longer: 6 minutes.","c880d743":"* Sanity check is when you compare score of your model with the score of the best constant model or any other model.\n* For classification task the best constant model is the model which predict the label of the most frequent class.\n\n* So first let find our best constant model:","ed61dc7d":"### How is the target distributed?","84489f96":"* in order to simplify the code its better to have the column names in lower case.","39c64d92":"### Fixing data types","8a67f892":"<a id='section3'><\/a>\n## Taking into account the imbalance of classes using Class weight adjustment\nTrain the best models that we had on the imbalace of classes and check their score with balance of classes.","b21bdc41":"<a id='winner'><\/a>\n* Training the model again with its top important features only:","d3f42685":"First dropping the columns that are not a real feature before the models: ","1ff7d3ea":"* LogisticRegression(C=0.01, penalty='none', random_state=12345, solver='saga')\n* with ROC AUC score on the fit set: 0.84\n* ROC AUC score on the prediction set: 0.85 \n* Accuracy: 0.77 for threshold: 0.3\n* False Negatives(FN) =  87\n* Top important features as we saw as well by the correlation are tenure and totalcharges.\n* Less important features are autopay and deviceprotection. \n* The results are better than the DT and RF on the accuracy but not on the FNR.","ea63ebed":"### LightGBM Model","a68dd3ef":"**Univariate EDA:**","af3689cb":"### Random Forest Model","fa8436d8":"<a id='section4'><\/a>\n## Sanity check for the model","256d8a70":"#### Simple Models - check which will be better tree, forest or logistic","87bb830c":"* On the graphs above we can find the distribution of each numeric parameter we have:\n    * The services columns with the 2 bars (0,1) are imbalanced except `gender` and `partner`\n    * We can see it as well on the `tenure` histogram where the left bar is much higher than the others which means that most of the leaving customers just got to the company lately.\n    * feature engineering can be also on the features like cont`contract_type` where the 0 commitment is as much as the 2 other of 1-2 years of commitments so maybe to create a feature that say commitment yes or no here, but this should be checked later on after we will have the first scores of the models.","f6d63fc2":"In order to investigate the quality of the logistic regression, we will try adjusting the following set of hyperparameters:\n\n\u2014 **solver** in [\u2018newton-cg\u2019, \u2018lbfgs\u2019, \u2018liblinear\u2019, \u2018sag\u2019, \u2018saga\u2019]\nRegularization (penalty) can sometimes be helpful. I took out \u2018lbfgs\u2019, \u2018liblinear\u2019 which made compatible issue with the penalty argument.\n\n\u2014 **penalty** in [\u2018none\u2019, \u2018l1\u2019, \u2018l2\u2019, \u2018elasticnet\u2019] Used to specify the norm used in the penalization.\nNote: not all solvers support all regularization terms.\nThe \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers support only l2 penalties. \n\u2018elasticnet\u2019 is only supported by the \u2018saga\u2019 solver. \nIf \u2018none\u2019 (not supported by the liblinear solver), no regularization is applied.\nI took out the 'elasticnet','l1' which made compatible issue with the solver.\n\n\u2014 The **C** parameter controls the penality strength, which can also be effective.\nC in [100, 10, 1.0, 0.1, 0.01]","d061886d":"Exploring each features separately with respect to the class it belongs to:","1fc01b7f":"* If we want to understand the problem of the customers that are exiting on a daily basic or understanding the acute problem that the company has in the first 5 months were most of the new customers left on the same year, how much money those new customers cost to the company for installing the services that they left only few months later.. I believe that a new service had hit the market and had high monthly charges without any commitment from the customer just to sell the service on a fiber internet connection. maybe this new service was not so good for the customers and thats why they were hurry to leave or maybe because of the monthly charges were expensive and a new competition company started because they didn't committed at all it was easy to steal the new customers.","2f0aa113":"### XGBoost Model","9b803c01":"Investigate the quality of the Random Forest, we will try adjusting the following set of hyperparameters:\n\n\u2014 **n_estimators**: number of trees in the foreset\n\n\u2014 **max_features**: max number of features considered for splitting a node\n\n\u2014 **max_depth**: max number of levels in each decision tree\n\n\u2014 **min_samples_leaf**: min number of data points allowed in a leaf node","1eb070a6":"##### Decision Tree\n\nInvestigate the quality of the decision tree classifier by changing some of its hyperparameters:\n\n\u2014 **max_depth:** the max depth of the tree.\n\n\u2014 **min_samples_split:** this prohibits creating nodes that don't contain enough observations from the training set. \n\n\u2014 **min_samples_leaf:** Leaves are the lowest nodes with the answers that do not split the data any further. This hyperparameter prevents the algorithm from adding leaf nodes that don't have enough observations from the training set.","9149dee1":"$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$\n\n$Precision = \\frac{TP}{TP+FP}$\n\n$Recall = \\frac{TP}{TP+FN}$\n\n$F1 = \\frac{2*Precision*Recall}{Precision+Recall} = \\frac{2*TP}{2*TP+FP+FN}$\n\n$TPR (True Positive Rate) = Recall = Sensitivity = \\frac{TP}{TP+FN}$ \n\nmiss rate or false negative rate (FNR):\n\n$FNR = 1 - TPR = \\frac{FN}{FN+TP}$\n\n$Specificity = \\frac{TN}{FP+TN}$ \n\n$ FPR = 1 - Specificity$\n\n* The ROC curve\n    \n    A ROC space is defined by FPR and TPR as x and y axes, respectively, which depicts relative trade-offs between true positive (benefits) and false positive (costs). Since TPR is equivalent to sensitivity and FPR is equal to 1 \u2212 specificity, the ROC graph is sometimes called the sensitivity vs (1 \u2212 specificity) plot. Each prediction result or instance of a confusion matrix represents one point in the ROC space.\n\nvery nice explained in [Wikipedia](#https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic)","caf56f06":"##### Knn Model","56c73ffd":"* checking the number of the unique values in each column, the top value and the frequency for balance check:","049c45e3":"##### LightGBM Model","6d24e6d5":"* roc_auc score for all the models was higher than the baseline roc_auc score: 0.5","a742e859":"* In the encoding section I will take care of all the payment types if needed and the internet service.","b8780866":"*Class weight adjustment*\nSome models have *class_weight* argument. By default, it is `None` \u2014 i.e., classes are equivalent:\n`class \"0\" weight = 1.0`\n`class \"1\" weight = 1.0`\n\nIf we specify `class_weight='balanced'`, the algorithm will calculate how many times class \"0\" occurs more often than class \"1\". We\u2019ll denote this number as N (an unknown number of times). The new class weights look like this:\n`class \"0\" weight = 1.0`\n`class \"1\" weight = N`\nThe rare class will have a higher weight.\n\nSimilar adjustment can be done by the parameter `scale_pos_weight` by default=1\nControl the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative instances) \/ sum(positive instances) -> In our case this is 2.76 calculated by the target distributed section details we had on 1.5.\n\nLets try it on our models using Class weight adjustment:","e37770a3":"### EDA","bee2a7b6":"**Tune Parameters for the Leaf-wise (Best-first) Tree**\nLightGBM uses the leaf-wise tree growth algorithm, while many other popular tools use depth-wise tree growth. Compared with depth-wise growth, the leaf-wise algorithm can converge much faster. However, the leaf-wise growth may be over-fitting if not used with the appropriate parameters.\n\nTo get good results using a leaf-wise tree, these are some important parameters:\n\n\u2014 **num_leaves** This is the main parameter to control the complexity of the tree model. Theoretically, we can set num_leaves = 2^(max_depth) to obtain the same number of leaves as depth-wise tree. However, this simple conversion is not good in practice. The reason is that a leaf-wise tree is typically much deeper than a depth-wise tree for a fixed number of leaves. Unconstrained depth can induce over-fitting. Thus, when trying to tune the num_leaves, we should let it be smaller than 2^(max_depth). For example, when the max_depth=7 the depth-wise tree can get good accuracy, but setting num_leaves to 127 may cause over-fitting, and setting it to 70 or 80 may get better accuracy than depth-wise.\n\n\u2014 **max_depth** You also can use max_depth to limit the tree depth explicitly.\n\n\u2014 **n_estimators** An important hyperparameter for the LightGBM ensemble algorithm is the number of decision trees used in the ensemble. Recall that decision trees are added to the model sequentially in an effort to correct and improve upon the predictions made by prior trees. As such, more trees are often better.\nThe number of trees can be set via the \u201cn_estimators\u201d argument and defaults to 100.\n\n\u2014 **learning_rate** last but not least the learning rate is totally impact on the score and should be tuned as well.","c0d0e6d3":"### Tuning hyperparameters\n* For this classification task, we will develop a model that will predict if the customer will leave the company by the access to behavior data about customers who have already left the company.\n* Our target is the `churn` column \u2014 tells if the \u0441ustomer has left (True - 1, False - 0)\n* Our features are all the rest of the columns that are numeric.","876499aa":"* 11 observation were dropped because they seem to be errors where the total chages had value of space instead of a number:","5b8d242a":"* On the graph above we can see how many customers left or stayed by the time of tenure in months. so we can see that most of the customers that started the contract few years ago stayed (the blue peak on the right) and most of the customers that left, left in the same year (the orange peak on the left).","04ce304e":"* We could see the balance is not really impacting the models the differences are smalls and the only change is for the threshold that we'll take. \n* The demanding of the project was on ROC AUC and accuracy only so in that case the winner is:\n* After training all the models with the balanced parameter and on only the top 10 important features we can move on to validate it on the next section.","fc30574b":"* Now lets plot the contract age for the exited customers and the staying customers:","5ce8dd14":"# Research on the telecom operator Interconnect \n","f0b137d8":"##### XGBoost Model","8fcb5455":"'paymentmethod','internetservice' columns have only few values 3-4 and can be included in the ohe:","0cf7e6f2":"<a id='winner'><\/a>\n* Training the model again with its top important features only:","51a09006":"**Bivariate EDA:**","4196fccc":"* getting the info for the data dictionary:","8138bd98":"* The total charges has a right tale distribution as we can see the mean 2283 is impacted from the high values and highher than the median 1397.","a993b957":"check the other columns for the features which are still in object type:","571a9e3b":"### Conclusions:\n* The tuning of the hyper parameter first started on the simple models of trees and logistic regression which got nice scores, still I moved on to the boosting models which got worse scores than RF and LR.\n* The best models on the imbalanced classes are Random Forest and Logistic Regression with Roc auc score higher than 0.85 on the test and accuracy 0.75\/7","ff8f5a42":"##### Logistic Regression Model","65b7752f":"* LGBMClassifier(boosting_type='gbdt', class_weight=None,\n    colsample_bytree=1.0, importance_type='split',\n    learning_rate=0.3, max_depth=2,\n    min_child_samples=20, min_child_weight=0.001,\n    min_split_gain=0.0, n_estimators=800, n_jobs=-1,\n    num_leaves=5, objective=None,\n    random_state=12345, reg_alpha=0.0,\n    reg_lambda=0.0, silent=True, subsample=1.0,\n    subsample_for_bin=200000, subsample_freq=0)\n* with ROC AUC score on the fit set: 0.83\n* ROC AUC score on the prediction set: 0.84 accuracy: 0.73 for threshold: 0.22\n* False Negatives(FN) =  69 \n* Top important features as we saw as well by the correlation are monthlycharges and totalcharges.\n* Less important features are internetservice_No and internetservice_Fiber optic. \n* looking into the thresholds check, we can see that the best F1 is not the same for the roc auc but more similar to the accuracy:\n  * For best roc_auc: Threshold = 0.22 | accuracy = 0.730 | roc_auc = 0.756 | F1 = 0.621\n  * For best Accuracy: 0.88 from 0.44\n  * For best F1: Threshold = 0.30 | accuracy = 0.760 | roc_auc = 0.753 | F1 = 0.627\n* The roc auc for the threshold is lower than the total because its calculated by the values and not by the propabilities on the specific threshold.\n","3a14e511":"# Description of the Project\n\nThe telecom operator Interconnect would like to be able to forecast their churn of clients.\n\nIf it's discovered that a user is planning to leave, they will be offered promotional codes and special plan options. \nInterconnect's marketing team has collected some of their clientele's personal data, including information about their plans and contracts.\n\nWe need to predict whether a customer will leave the Telecom company soon using the\ndata on clients\u2019 past behavior and termination of contracts.\n\nBuilding a model with the maximum possible AUC-ROC score.\nAdditionally, we will measure the accuracy metric and compare it with the AUC-ROC.","c37be69f":"**First Place**\n\n**Random Forest with scaling and class_weight balanced on 10 top important features only**\n\n**with ROC AUC 0.85 and accuracy of 0.79 on the test and 103 FN.**\n\nAlthough the Random Forest with scaling but without balance and on all the features had much better FN of 68 which is a difference of 3% of the churn customers which can be missed so my recommendation is to use in this simple way eventhough the accuracy is worse by 4%.","312067b2":"### Adding more features","9ede9d84":"*  Using cross check validation for tuning the hyper parameters so the train and the valid can be into 80% train and the left for 20% test, this is 1:4 ratio.\n* The dataset of 7032 observations was split to:\n  * 80% for the training set - 5625 observations\n  * 20% for the test sets -    1407 observations.","36b179f6":"* The payment method can be splited to 2 classes on automatic payment and not automatic payment.","81be72f4":"<a id='section1'><\/a>\n## Download and prepare the data\n### Open the data file and study the general information","ae368bf7":"* KNeighborsClassifier(n_neighbors=50)\n* with ROC AUC score on the fit set: 0.83\n* ROC AUC score on the prediction set: 0.82 \n* accuracy: 0.73 for threshold: 0.3\n* False Negatives(FN) =  86\n* This is the worst model from all, maybe I could tune it more, but the simple models are the winners so I will leave it. still the Knn is very fast trainer. ","f6914558":"<a id='section5'><\/a>\n## Overall conclusion","1e1dd5c1":"* **Preparring a general function to fit and predict a model and tuning its hyper parameters combines with pipelines for the scaling**","72796341":"* On the correlation matrix above we can see the correlation between the features and the target (exit):\n  * `churn` and `tenure` have a negative correlation of -0.35, which is reasonable if the old customers are loyal so the lower the tenure is the higher the chance for churn in case the customer didn't leave yet. \n  * `totalcharges` and `tenure` has strong positive correlation of 0.83 as long the contract stay on the more total charges got in the company\n  * `contract_type` and `tenure` has strong positive correlation of 0.68 as long the contract stay on the more total charges got in the company.\n  * there are some more strong positive correlations between the all internet and phone services to the monthlycharges and the totalcharges ao it means those services cost more and make the monthly charge increased and so the total gets increased as well which make sense because also between both of them there is a positive strong correlation.\n  * checking the unimportant features that we might find out later which we can see they have no correlation at all like `gender`.","d9773286":"* taking care of all the columns with No\/Yes values and convert them into 0\/1 values with integer type:","b4b45cb7":"* Training the model again with its top important features only:","aa894b4d":"### Conclusions\n","5ca046d5":"* The fit and predict function for the models hyper parameter tuning:\n  * The auc_roc is the leading metric for the BayesSearchCV in order to find the best model.\n  * the iteration for the BayesSearchCV are 16 only because of the execution time consumer and if I had stronger computing resources I would increase it to check like 32 iterations and so on. also for the hyper parameters the tuning here is very low because of the time consuming when using gamma or booster type for example it takes for ever so I even didnt put them next.\n  * For each model this function shows: \n    * The model with the best parameter\n    * ROC Curve graph\n    * Confusion matrix - specialy the False Negative Rate\n    * The importance of the features by the specific model\n  * The function returns a dataframe row with the result summery.","9e12f5c2":"#### Boosting Models - check which will be better LightGBM or XGBoost","8a23c7b1":"### Logistic Regression Model","dfaa0f4e":"* It looks like there are new customers that leave in a rush on the first months, its probably depends on the contract type which should be check in the next section.","79c6c9b1":"* If the score of the model is worse than the score of best constant model then the model is useless:\n  * In our case:\n    * The ROC AUC score is 0.85 so our model can be used for predictions.\n    * We need to filter the models and to take only the results with accuracy greater than 0.73.\n    * All the models got FN less than the baseline.\n","1c01bd0d":"* Training the model again with its top important features only:","d76d0fe2":"On the plot above you can find the distribution of the contract age for the customer who didnt exited on the blue color and for those who left on the orange bars. The conclusions we can get from this graph can be a huge help: it shows us that most of the leaving customers do this in their first months and while the contract age goes up the customers are more loyal and most of them have old contract of 70 months.","30a2d3bb":"On both of the Churn Distribution above we can see the count (y axis) of the Churn on the x axis true and of the Non-Churn on the False bar. both bars are colored, on the first graph its the intrenet service and on the second by the Contract type which you can see in the legend on the top right.\n\n* There are 1869 customers that had existed which is 26.58% of the total customers which leaves us with 73.42% that didn't exit yet. The classes of exited and non-exited are imbalanced, this is reasonable and better for the company that only few customers will exit and here its a huge part..\n* We can see that on the Exited class there are more Fiber internet than DSL\/No internet, and on the contract type Month-to-month. This is reasonable that uncommitted customers will leave, they dont have 1-2 years contract to hold into.","e641f33c":"# The main steps of the analysis\n* [Step 1](#section1). Download and prepare the data\n* [Step 2](#section2). Train the model without taking into account the imbalance\n* [Step 3](#section3). Taking into account the imbalance of classes using Class weight adjustment\n* [Step 4](#section4). Sanity check for the model\n* [Step 5](#section5). Overall conclusion","8c7086bf":"What we can find in the Violin plots above on all the interesting columns we found before is much more interesting when looking on the differences of exiting (the left blue violin is for the staying customers and the orange on the right is for the customers that exited already):\n   * On the `monthlycharges` distribution per churn on the left up corner we can see that the left blue violin of the staying customers with huge peak of amount on the low side ~20 and the churn customers were paying much more, also the white point for the median is higher than the other one.\n   * On the other side, on the total charges distribution the exiting customers most of them have very low total because has we can see before they probably didn't stayed much time in order to aggregate large charges in total, also the white point of the median is lower than the one in the blue.\n   * In addition on the right up corner we can find the `partner` white point of the median for the churn customers on orange Violin, it means that most of the leaving customer have no partner, and on the left the blue violin for the staying customers the median is on the partner so it means they are more committed.\n   * On the right bottom, its the `tenure` distribution which looks much similar to the total charges above it. and we can say by this staying customers that are loyal to the company the total charge didn't increase much as the tenure so they probably have low monthly charges.\n   * On the `contract_type` distribution there are 3 values 0,1,2 and we can see nice split for the tree on the staying customers bit on the customers that exited the  huge amount and the median and the all IQR is on the zero commitment.","2e3b5741":"* nothing to do with monthlycharges column has like a normal distribution by the mean (65) and median (70) values that are closed.","5be3aa00":"The portion of the data to be assigned to the validation set depends on the number of observations and features, as well as the data variation. Here are the two most common scenarios: \n\n1) The test set exists (or will exist in the near future), but is unavailable for the time being. The preferable ratio is 3:1. This means 75% for the training set and 25% for the validation set. \n\n2) The test set doesn't exist. In that case, the source data has to be split into three parts: training, validation, and test. The sizes of the validation set and the test set are usually equal. This scenario gives us a 3:1:1 ratio.\n\nIn our case we hit the second common scenario so the data will be split into : 60% for the training set and 20% each for the validation and test sets BUT Im using CV for tuning the hyper parameters so the train and the valid can be into 80% train and the left for 20% test.","a44e1fbb":"* `gender` Female\/Male values converted into 0\/1 values:","bb244da9":"**Let's check how our features correlate with each other and with the target:**","31e41770":"### Split the source data into a training set, a validation set, and a test set","000c5f63":"* There are no missing data or duplicated data.\n* The primary key is the customer id which is unique.\n* The table has 21 columns and has 7043 customers.\n    * `gender` Female\/Male values should be converted into 0\/1 values.\n    * `SeniorCitizen` has a very low part of 1 values as reasonable statistic population, maybe those observations should be dropped but this should be checked after the model training when we will have the importancy of this feature.\n    * `Partner` and `Dependents` No\/Yes values should be converted into 0\/1 values.\n    * `tenure` has numerical values between 0 and 72.\n    * `Contract`, `PaymentMethod` should be encoded into numeric column, hope to ordinal encoding possibility.\n    * `PaperlessBilling` No\/Yes values should be converted into 0\/1 values. \n    * `TotalCharges` should be converted to float type as `MonthlyCharges` does.    \n    * `InternetService` should be encoded into numeric column, hope to ordinal encoding possibility.\n    * more than half of the customers with internet service has Fiber optic.\n    * about half of the customers have multiple lines service. this parameter can be interesting as well.\n    * All the rest of the columns have No\/Yes values that should be converted into 0\/1 values ('No phone service' or 'No internet service' are equal to No as we have this indication on the `PhoneService` or on `InternetService` with No value I will validate it next)","f3858bae":"The telecom operator Interconnect would like to be able to forecast their churn of clients.\n\nIf it's discovered that a user is planning to leave, they will be offered promotional codes and special plan options. Interconnect's marketing team has collected some of their client's personal data, including information about their plans and contracts.\n\nWe need to predict whether a customer will leave the Telecom company soon using the data on clients\u2019 past behavior and termination of contracts.\n\nBuilding a model with the maximum possible AUC-ROC metric and compare it with the accuracy.\n\nPrimary metric: AUC-ROC.\n\nAdditional metric: Accuracy.\n\nAdditional rate: False Negative Rate\n* The FN is the important one here, more than the FP:\n      * Its better to start a saving process for a customer that the model predict as churn even though he stayed (FP) than missing a customer because the model predicted he would not leave by error (FN). Checking the scores formula how each score depends on minimize FN and maximize TN.","dbd3a441":"* Target\n  * `Churn` \u2014 \u0441ustomer has left:\n    * Most frequent class is `Non-Churn` = 0 for customers who didnt left the company\n    * Churn = 1","b56cb6b3":"### conclusions\n\n* The merged table has total 21 columns and 7043 customers. there are no missing values and no duplicated in it. all data types were fixed.\n* most of the leaving customers do this in their first months and while the tenure goes up the customers are more loyal and most of them have an old contract of 70 months.\n* There are 1869 customers that had existed which is 26.58% of the total customers which leaves us with 73.42% that didn't churn yet. The classes of churn and non-churn are imbalanced, this is reasonable and better for the company that only few customers will exit and here its a huge part..\n* We can see that on the churn class there are more Fiber internet than DSL\/No internet, and on the contract type Month-to-month. This is reasonable that uncommitted customers will leave, they dont have 1-2 years contract to hold into.\n* The exiting customers most of them have very low total charges because has we saw before they probably didn't stayed much time in order to aggregate large charges in total.\n* The `tenure` distribution which looks much similar to the total charges above it. and we can say by this staying customers that are loyal to the company the total charge didn't increase much as the age so they probably have low monthly charges.\n* On the `contract_type` distribution there are 3 values 0,1,2 and we can see nice split for the tree on the staying customers bit on the customers that exited the  huge amount and the median and the all IQR is on the zero commitment.\n* `churn` and `tenure` have a negative correlation of -0.35, which is reasonable if the old customers are loyal so the lower the tenure is the higher the chance for churn in case the customer didn't leave yet. \n  * `totalcharges` and `tenure` has strong positive correlation of 0.83 as long the contract stay on the more total charges got in the company\n  * `contract_type` and `tenure` has strong positive correlation of 0.68 as long the contract stay on the more total charges got in the company.\n  * there are some more strong positive correlations between the all internet and phone services to the monthlycharges and the totalcharges ao it means those services cost more and make the monthly charge increased and so the total gets increased as well which make sense because also between both of them there is a positive strong correlation.\n  * checking the unimportant features that we might find out later which we can see they have no correlation at all like `gender`.\n  \n* new column were added:\n  * `contract_type` - 0 for contract of month to month, means no commitment, 1 for 1 year contract and 2 for 2 years contract.\n  * `autopay` - The payment method was split to 2 classes on automatic payment and not automatic payment.","07f7996b":"### Decision Tree","fab58fb5":"* We need to filter the models and to take only the results with accuracy greater than 0.73 of the baseline:","4e5ad562":"### conclusion"}}