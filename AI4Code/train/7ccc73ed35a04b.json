{"cell_type":{"37f62c4b":"code","5f269b1f":"code","ecf43822":"code","8394a779":"code","4e1703f0":"code","d99c0f36":"code","811d410e":"code","b8859f3d":"code","daaf6f46":"code","62af0d83":"code","fb165e4d":"code","8d750b1b":"code","764b8eb8":"code","323e3002":"code","4b4c7164":"code","71d8eddb":"code","b64192ce":"code","4cdfbb52":"code","c1b1b429":"code","54396c76":"code","2af54419":"code","1397121d":"code","a3306505":"code","aee86ff2":"code","84c3dbba":"code","703be309":"code","36bfeca8":"markdown","bbab4144":"markdown","207ef259":"markdown","103b5a9a":"markdown","6ee31f29":"markdown","435b0e61":"markdown","b75e015d":"markdown"},"source":{"37f62c4b":"import numpy as np\nclass MyLogisticRegression():\n    \n    def __init__(self,learning_rate):\n        self.learning_rate = learning_rate\n        self.w = 0\n        self.bias = 0\n        self.cost = []\n    \n    def sigmoid_func(self,z):\n        a = 1 \/ (1 + np.exp(-z))\n        return a\n    \n    def initializer_w_and_bias(self,size):\n        w = np.random.uniform(0.5,-0.5,size = (size,1))\n        bias = 0\n        return w,bias\n        \n    def update_w_and_bias(self,x_train,y_train,a,w,b):\n        \n        deriative_w = (np.dot(x_train.T,(a-y_train))) \/ x_train.shape[1]\n        deriative_bias = np.sum(a-y_train)\/x_train.shape[1]\n        \n        w = w - (self.learning_rate * deriative_w)\n        b = b - (self.learning_rate * deriative_bias)\n        \n        return w,b\n    \n    def forward_and_backward_propagation(self,x_train,y_train,iterations):\n        \n        w,bias = self.initializer_w_and_bias(x_train.shape[1])\n        epsilon = 1e-5\n        for i in range(iterations):\n            z = np.dot(x_train,w) + bias\n            a = self.sigmoid_func(z)\n            loss = -y_train*np.log(a+epsilon) + (1 - y_train) * np.log(1-a+epsilon)\n            cost = np.sum(loss) \/ x_train.shape[1]\n            self.cost.append(cost)\n            w,bias = self.update_w_and_bias(x_train,y_train,a,w,bias)\n        return w,bias\n        \n    \n    def my_fit(self,x_train,y_train,iterations):\n        \n        w,bias = self.forward_and_backward_propagation(x_train,y_train,iterations)\n        self.w = w\n        self.bias = bias\n       \n    \n    def my_predict(self,array):\n        predict = np.dot(array,self.w) + self.bias\n        predict = self.sigmoid_func(predict)\n        return predict\n    \n    def history(self):\n        history = {\"weight\" : self.w,\n                   \"bias\" : self.bias,\n                   \"cost\" : self.cost}\n        return history","5f269b1f":"import seaborn as sns\nimport matplotlib.pyplot as plt","ecf43822":"iris = sns.load_dataset(\"iris\")\niris.head()","8394a779":"iris[\"species\"].value_counts()","4e1703f0":"new_iris = iris.iloc[:100,:]","d99c0f36":"new_iris[\"species\"].value_counts()","811d410e":"new_iris = new_iris.sample(frac=1).reset_index(drop = True)","b8859f3d":"Y = new_iris[\"species\"].values\nX = new_iris.drop([\"species\"],axis = 1).values","daaf6f46":"X.shape","62af0d83":"Y.shape","fb165e4d":"from sklearn.preprocessing import LabelEncoder","8d750b1b":"le = LabelEncoder()\nY = le.fit_transform(Y)","764b8eb8":"Y.shape","323e3002":"Y = Y.reshape(100,1)","4b4c7164":"from sklearn.model_selection import train_test_split","71d8eddb":"x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=2)","b64192ce":"LR_model = MyLogisticRegression(0.01)","4cdfbb52":"LR_model.my_fit(x_train,y_train,16)","c1b1b429":"LR_model.history()[\"weight\"]","54396c76":"LR_model.history()[\"cost\"]","2af54419":"pred_val = LR_model.my_predict(x_test)","1397121d":"for i,val in enumerate(pred_val):\n    if val > 0.5:\n        pred_val[i] = 1\n    else:\n        pred_val[i] = 0","a3306505":"from sklearn.metrics import confusion_matrix","aee86ff2":"matrix = confusion_matrix(pred_val,y_test)","84c3dbba":"matrix","703be309":"plt.plot(LR_model.history()[\"cost\"])","36bfeca8":"###  <font color='blue'> Let's this try <\/font>","bbab4144":"##  <font color='blue'> What is a Logistic Regression ?<\/font>","207ef259":"###  <font color='blue'> Let's code together <\/font>","103b5a9a":"Logistic Regression Computational Graph ![](2.png)","6ee31f29":"<ul>\n      <li>Logistic Regression is a regression method used to perform classification operation.<\/li>\n      <li>In this analytics approach, the dependent variable is finite or categorical.<\/li>\n      <li>This type of analysis can help you predict the likelihood of an event happening or a choice being made.<\/li>\n      <li>For example, predicting whether a person is sick\n <\/li>\n<\/ul>","435b0e61":"<ul>\n    <li> Firstly, our weights are multiplied by the inputs and add bias. (z = x1*w1 + x2*w2 + b ) (x1,x2 = inputs,w = weigth,b = bias) <\/li>\n    <li> What is a weight and bias ?<\/li>\n    <li> Here, these are the parameters our algorithm will learn <\/li>\n    <li> So, How will you learn ?<\/li>\n    <li> Firstly, Let's learn what is forward propagation and backwrd propagation<\/li>\n    <li> Forward Propagation : is to move from left to right in the above table, and we get an output<\/li>\n    <li> Let's open this, our weights are multiplied by the inputs and add bias (output = z) , this result is given to the function (sigmoid function, output = a)<\/li>\n    <li> Bacward Propagation : Now, the algorithm is starting to learn :D. The result of the sigmoid function and our actual value go into a cost function and our error rate is calculated. <\/li>\n\n<\/ul>","b75e015d":"##  <font color='blue'> How Does Work Logistic Regression ?<\/font>"}}