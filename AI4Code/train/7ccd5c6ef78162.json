{"cell_type":{"92758d58":"code","11bdf458":"code","7e77dc31":"code","8fd17dc7":"code","620f8a74":"code","63cb72f8":"code","5eb881c1":"code","5490bff7":"code","29cfdd03":"code","ba06761a":"code","2e0554ce":"code","5320f1fb":"code","bb54b370":"code","d4b7bfcb":"code","cb5c0de6":"code","9e09a831":"code","bc996d3a":"code","c764d678":"code","2ac35692":"code","d5bfbafd":"code","8fb8764e":"code","11e0ff65":"code","00fb7ec2":"code","6d114bec":"code","d5940dfe":"code","287ed0b7":"code","f2f938e8":"code","121081a0":"code","75dd78ba":"code","5590f923":"code","57d27e35":"code","16624cef":"code","4e2d9ea3":"code","2ba09724":"code","7ab0be7a":"code","02808457":"code","43ee82b6":"code","c2897922":"code","41e9bd81":"code","9269fbec":"code","59a6ab23":"code","d266019f":"code","4f14e331":"code","7119b3db":"code","ccb0a518":"code","e1f802bb":"code","37e42349":"code","1c6876d5":"code","6ee0356a":"code","c09cdbee":"code","3cbf0d47":"code","03d85fd5":"code","c906f14e":"code","82b3a098":"code","9570145d":"code","b74c5ea9":"code","02affa6e":"code","b635f6b4":"code","4652a89a":"code","bd4cb91c":"code","4cc13ab6":"code","f2cec8c7":"code","f9d5f179":"code","ab4e3485":"code","7f9f555d":"code","5789ed05":"code","da87589c":"code","a58df893":"code","b78b14e2":"code","92ae9cd8":"code","7d8066dd":"code","0cffe55d":"code","59016a3f":"code","91ee77ef":"code","4a951ff2":"code","a2f7f039":"code","6cab45f7":"markdown","19761cfa":"markdown","2a3f6aee":"markdown","d1dfd18c":"markdown","a1bffebe":"markdown","97f9ef8e":"markdown","53ec5391":"markdown","b8fc7899":"markdown","cf460471":"markdown","15f0cc5f":"markdown","dec30ea0":"markdown","dededb1d":"markdown","113aac66":"markdown","0c0aa1db":"markdown","f0e52f0c":"markdown","b2f91308":"markdown","0d4a9f6a":"markdown","83ca508c":"markdown"},"source":{"92758d58":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set()","11bdf458":"!pip install openpyxl\ntrain_data = pd.read_excel(r\"..\/input\/flight-fare-prediction-mh\/Data_Train.xlsx\")","7e77dc31":"pd.set_option('display.max_columns', None)","8fd17dc7":"train_data.head()","620f8a74":"train_data.info()","63cb72f8":"train_data[\"Duration\"].value_counts()","5eb881c1":"train_data.dropna(inplace = True)","5490bff7":"train_data.isnull().sum()","29cfdd03":"train_data[\"Journey_day\"] = pd.to_datetime(train_data.Date_of_Journey, format=\"%d\/%m\/%Y\").dt.day","ba06761a":"train_data[\"Journey_month\"] = pd.to_datetime(train_data[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.month","2e0554ce":"train_data.head()","5320f1fb":"# Since we have converted Date_of_Journey column into integers, Now we can drop as it is of no use.\n\ntrain_data.drop([\"Date_of_Journey\"], axis = 1, inplace = True)","bb54b370":"# Departure time is when a plane leaves the gate. \n# Similar to Date_of_Journey we can extract values from Dep_Time\n\n# Extracting Hours\ntrain_data[\"Dep_hour\"] = pd.to_datetime(train_data[\"Dep_Time\"]).dt.hour\n\n# Extracting Minutes\ntrain_data[\"Dep_min\"] = pd.to_datetime(train_data[\"Dep_Time\"]).dt.minute\n\n# Now we can drop Dep_Time as it is of no use\ntrain_data.drop([\"Dep_Time\"], axis = 1, inplace = True)","d4b7bfcb":"train_data.head()","cb5c0de6":"# Arrival time is when the plane pulls up to the gate.\n# Similar to Date_of_Journey we can extract values from Arrival_Time\n\n# Extracting Hours\ntrain_data[\"Arrival_hour\"] = pd.to_datetime(train_data.Arrival_Time).dt.hour\n\n# Extracting Minutes\ntrain_data[\"Arrival_min\"] = pd.to_datetime(train_data.Arrival_Time).dt.minute\n\n# Now we can drop Arrival_Time as it is of no use\ntrain_data.drop([\"Arrival_Time\"], axis = 1, inplace = True)","9e09a831":"train_data.head()","bc996d3a":"# Time taken by plane to reach destination is called Duration\n# It is the differnce betwwen Departure Time and Arrival time\n\n\n# Assigning and converting Duration column into list\nduration = list(train_data[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration","c764d678":"# Adding duration_hours and duration_mins list to train_data dataframe\n\ntrain_data[\"Duration_hours\"] = duration_hours\ntrain_data[\"Duration_mins\"] = duration_mins","2ac35692":"train_data.drop([\"Duration\"], axis = 1, inplace = True)","d5bfbafd":"train_data.head(10)","8fb8764e":"train_data['Source'].nunique()","11e0ff65":"train_data[\"Airline\"].value_counts()","00fb7ec2":"# From graph we can see that Jet Airways Business have the highest Price.\n# Apart from the first Airline almost all are having similar median\n\n# Airline vs Price\nsns.catplot(y = \"Price\", x = \"Airline\", data = train_data.sort_values(\"Price\", ascending = False), kind=\"boxen\", height = 6, aspect = 3)\nplt.show()","6d114bec":"# As Airline is Nominal Categorical data we will perform OneHotEncoding\n\nAirline = train_data[[\"Airline\"]]\n\nAirline = pd.get_dummies(Airline, drop_first= True)\n\nAirline.head()","d5940dfe":"train_data[\"Source\"].value_counts()","287ed0b7":"# Source vs Price\n\nsns.catplot(y = \"Price\", x = \"Source\", data = train_data.sort_values(\"Price\", ascending = False), kind=\"boxen\", height = 4, aspect = 3)\nplt.show()","f2f938e8":"# As Source is Nominal Categorical data we will perform OneHotEncoding\n\nSource = train_data[[\"Source\"]]\n\nSource = pd.get_dummies(Source, drop_first= True)\n\nSource.head()","121081a0":"train_data[\"Destination\"].value_counts()","75dd78ba":"# As Destination is Nominal Categorical data we will perform OneHotEncoding\n\nDestination = train_data[[\"Destination\"]]\n\nDestination = pd.get_dummies(Destination, drop_first = True)\n\nDestination.head()","5590f923":"train_data[\"Route\"]","57d27e35":"# Additional_Info contains almost 80% no_info\n# Route and Total_Stops are related to each other\n\ntrain_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)","16624cef":"train_data[\"Total_Stops\"].value_counts()","4e2d9ea3":"# As this is case of Ordinal Categorical type we perform LabelEncoder\n# Here Values are assigned with corresponding keys\n\ntrain_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)","2ba09724":"train_data.head()","7ab0be7a":"# Concatenate dataframe --> train_data + Airline + Source + Destination\n\ndata_train = pd.concat([train_data, Airline, Source, Destination], axis = 1)","02808457":"data_train.head()","43ee82b6":"data_train.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)","c2897922":"data_train.head()","41e9bd81":"data_train.shape","9269fbec":"test_data = pd.read_excel(r\"..\/input\/flight-fare-prediction-mh\/Test_set.xlsx\")","59a6ab23":"test_data.head()","d266019f":"# Preprocessing\n\nprint(\"Test data Info\")\nprint(\"-\"*75)\nprint(test_data.info())\n\nprint()\nprint()\n\nprint(\"Null values :\")\nprint(\"-\"*75)\ntest_data.dropna(inplace = True)\nprint(test_data.isnull().sum())\n\n# EDA\n\n# Date_of_Journey\ntest_data[\"Journey_day\"] = pd.to_datetime(test_data.Date_of_Journey, format=\"%d\/%m\/%Y\").dt.day\ntest_data[\"Journey_month\"] = pd.to_datetime(test_data[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.month\ntest_data.drop([\"Date_of_Journey\"], axis = 1, inplace = True)\n\n# Dep_Time\ntest_data[\"Dep_hour\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.hour\ntest_data[\"Dep_min\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.minute\ntest_data.drop([\"Dep_Time\"], axis = 1, inplace = True)\n\n# Arrival_Time\ntest_data[\"Arrival_hour\"] = pd.to_datetime(test_data.Arrival_Time).dt.hour\ntest_data[\"Arrival_min\"] = pd.to_datetime(test_data.Arrival_Time).dt.minute\ntest_data.drop([\"Arrival_Time\"], axis = 1, inplace = True)\n\n# Duration\nduration = list(test_data[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration\n\n# Adding Duration column to test set\ntest_data[\"Duration_hours\"] = duration_hours\ntest_data[\"Duration_mins\"] = duration_mins\ntest_data.drop([\"Duration\"], axis = 1, inplace = True)\n\n\n# Categorical data\n\nprint(\"Airline\")\nprint(\"-\"*75)\nprint(test_data[\"Airline\"].value_counts())\nAirline = pd.get_dummies(test_data[\"Airline\"], drop_first= True)\n\nprint()\n\nprint(\"Source\")\nprint(\"-\"*75)\nprint(test_data[\"Source\"].value_counts())\nSource = pd.get_dummies(test_data[\"Source\"], drop_first= True)\n\nprint()\n\nprint(\"Destination\")\nprint(\"-\"*75)\nprint(test_data[\"Destination\"].value_counts())\nDestination = pd.get_dummies(test_data[\"Destination\"], drop_first = True)\n\n# Additional_Info contains almost 80% no_info\n# Route and Total_Stops are related to each other\ntest_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)\n\n# Replacing Total_Stops\ntest_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\n\n# Concatenate dataframe --> test_data + Airline + Source + Destination\ndata_test = pd.concat([test_data, Airline, Source, Destination], axis = 1)\n\ndata_test.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)\n\nprint()\nprint()\n\nprint(\"Shape of test data : \", data_test.shape)\n\n","4f14e331":"data_test.head()","7119b3db":"data_train.shape","ccb0a518":"data_train.columns","e1f802bb":"X = data_train.loc[:, ['Total_Stops', 'Journey_day', 'Journey_month', 'Dep_hour',\n       'Dep_min', 'Arrival_hour', 'Arrival_min', 'Duration_hours',\n       'Duration_mins', 'Airline_Air India', 'Airline_GoAir', 'Airline_IndiGo',\n       'Airline_Jet Airways', 'Airline_Jet Airways Business',\n       'Airline_Multiple carriers',\n       'Airline_Multiple carriers Premium economy', 'Airline_SpiceJet',\n       'Airline_Trujet', 'Airline_Vistara', 'Airline_Vistara Premium economy',\n       'Source_Chennai', 'Source_Delhi', 'Source_Kolkata', 'Source_Mumbai',\n       'Destination_Cochin', 'Destination_Delhi', 'Destination_Hyderabad',\n       'Destination_Kolkata', 'Destination_New Delhi']]\nX.head()","37e42349":"y = data_train.iloc[:, 1]\ny.head()","1c6876d5":"# Finds correlation between Independent and dependent attributes\n\nplt.figure(figsize = (18,18))\nsns.heatmap(train_data.corr(), annot = True, cmap = \"RdYlGn\")\n\nplt.show()","6ee0356a":"# Important feature using ExtraTreesRegressor\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nselection = ExtraTreesRegressor()\nselection.fit(X, y)","c09cdbee":"print(selection.feature_importances_)","3cbf0d47":"#plot graph of feature importances for better visualization\n\nplt.figure(figsize = (12,8))\nfeat_importances = pd.Series(selection.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()\n","03d85fd5":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","c906f14e":"from sklearn.ensemble import RandomForestRegressor\nreg_rf = RandomForestRegressor()\nreg_rf.fit(X_train, y_train)","82b3a098":"y_pred = reg_rf.predict(X_test)","9570145d":"reg_rf.score(X_train, y_train)","b74c5ea9":"reg_rf.score(X_test, y_test)","02affa6e":"sns.distplot(y_test-y_pred)\nplt.show()","b635f6b4":"\nplt.scatter(y_test, y_pred, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","4652a89a":"from sklearn import metrics","bd4cb91c":"print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","4cc13ab6":"# RMSE\/(max(DV)-min(DV))\n\n2090.5509\/(max(y)-min(y))","f2cec8c7":"metrics.r2_score(y_test, y_pred)","f9d5f179":"from sklearn.model_selection import RandomizedSearchCV","ab4e3485":"#Randomized Search CV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","7f9f555d":"# Create the random grid\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}","5789ed05":"# Random search of parameters, using 5 fold cross validation, \n# search across 100 different combinations\nrf_random = RandomizedSearchCV(estimator = reg_rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = -1)","da87589c":"rf_random.fit(X_train,y_train)","a58df893":"rf_random.best_params_","b78b14e2":"prediction = rf_random.predict(X_test)","92ae9cd8":"plt.figure(figsize = (8,8))\nsns.distplot(y_test-prediction)\nplt.show()","7d8066dd":"plt.figure(figsize = (8,8))\nplt.scatter(y_test, prediction, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","0cffe55d":"print('MAE:', metrics.mean_absolute_error(y_test, prediction))\nprint('MSE:', metrics.mean_squared_error(y_test, prediction))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))","59016a3f":"import pickle\n# open a file, where you ant to store the data\nfile = open('flight_rf.pkl', 'wb')\n\n# dump information to that file\npickle.dump(reg_rf, file)","91ee77ef":"model = open('.\/flight_rf.pkl','rb')\nforest = pickle.load(model)","4a951ff2":"y_prediction = forest.predict(X_test)","a2f7f039":"metrics.r2_score(y_test, y_prediction)","6cab45f7":"---","19761cfa":"## Handling Categorical Data\n\nOne can find many ways to handle categorical data. Some of them categorical data are,\n1. <span style=\"color: blue;\">**Nominal data**<\/span> --> data are not in any order --> <span style=\"color: green;\">**OneHotEncoder**<\/span> is used in this case\n2. <span style=\"color: blue;\">**Ordinal data**<\/span> --> data are in order --> <span style=\"color: green;\">**LabelEncoder**<\/span> is used in this case","2a3f6aee":"---","d1dfd18c":"## EDA","a1bffebe":"---","97f9ef8e":"---","53ec5391":"## Save the model to reuse it again","b8fc7899":"---","cf460471":"## Fitting model using Random Forest\n\n1. Split dataset into train and test set in order to prediction w.r.t X_test\n2. If needed do scaling of data\n    * Scaling is not done in Random forest\n3. Import model\n4. Fit the data\n5. Predict w.r.t X_test\n6. In regression check **RSME** Score\n7. Plot graph","15f0cc5f":"---","dec30ea0":"## Hyperparameter Tuning\n\n\n* Choose following method for hyperparameter tuning\n    1. **RandomizedSearchCV** --> Fast\n    2. **GridSearchCV**\n* Assign hyperparameters in form of dictionery\n* Fit the model\n* Check best paramters and best score","dededb1d":"From description we can see that Date_of_Journey is a object data type,\\\nTherefore, we have to convert this datatype into timestamp so as to use this column properly for prediction\n\nFor this we require pandas **to_datetime** to convert object data type to datetime dtype.\n\n<span style=\"color: red;\">**.dt.day method will extract only day of that date**<\/span>\\\n<span style=\"color: red;\">**.dt.month method will extract only month of that date**<\/span>","113aac66":"# Flight Price Prediction\n---","0c0aa1db":"## Feature Selection\n\nFinding out the best feature which will contribute and have good relation with target variable.\nFollowing are some of the feature selection methods,\n\n\n1. <span style=\"color: purple;\">**heatmap**<\/span>\n2. <span style=\"color: purple;\">**feature_importance_**<\/span>\n3. <span style=\"color: purple;\">**SelectKBest**<\/span>","f0e52f0c":"## Importing dataset\n\n1. Since data is in form of excel file we have to use pandas read_excel to load the data\n2. After loading it is important to check the complete information of data as it can indication many of the hidden infomation such as null values in a column or a row\n3. Check whether any null values are there or not. if it is present then following can be done,\n    1. Imputing data using Imputation method in sklearn\n    2. Filling NaN values with mean, median and mode using fillna() method\n4. Describe data --> which can give statistical analysis","b2f91308":"## Test set","0d4a9f6a":"---","83ca508c":"---"}}