{"cell_type":{"7a67d095":"code","7ee27ff0":"code","62a316ef":"code","4cafaf47":"code","1b87e1fa":"code","1390e562":"code","83ccf17b":"code","960ebb4e":"code","02212fb2":"code","3b632eb2":"code","16761dda":"code","57e5caf1":"code","a9b80c01":"code","8e1a55ce":"code","7590006f":"code","8e2e7ef0":"code","e31d00e3":"code","91cb2286":"code","3a72bf25":"code","f6fa249b":"code","e10943de":"code","a16fdaf6":"code","690b062e":"code","517ca758":"code","9c20dd7c":"code","2a03e048":"code","e62b73ea":"code","e6ae4d03":"code","fc4b4256":"code","7a744b24":"code","be27b7c3":"code","59a4e2f9":"code","fd56bedf":"code","3d1c5b5f":"code","78270d0c":"code","87ad7d13":"code","7b12184b":"code","b248152a":"code","ab77f475":"code","f418fd77":"code","2d32f992":"code","646a090f":"code","e1d8777f":"code","b133c8b8":"code","2237a591":"code","5658a5b1":"code","54f80fe7":"code","eead6316":"code","27015dc5":"code","5584dcaa":"code","82291ab7":"code","eae6cc38":"code","c39eeafd":"code","2b94bc71":"code","0c371ae4":"code","8176d6c6":"code","ee19fda2":"code","1906696f":"code","af57f778":"code","aaf10d91":"code","16ddb8a4":"code","1376f951":"code","e93057ca":"code","f6262412":"code","b7b3e45c":"code","5332c9fe":"code","8d0cae4f":"markdown","24f65e2f":"markdown","065937a1":"markdown","08dd52d8":"markdown","30e9e71d":"markdown","f6cb65c9":"markdown","f52a30c4":"markdown","01269891":"markdown","5d0fa946":"markdown","d0da324e":"markdown","8f83beea":"markdown","425391c5":"markdown","a279ee30":"markdown","bf73de22":"markdown","47d89a52":"markdown","9c0da69f":"markdown","75290206":"markdown","7b896850":"markdown","2bcddb5f":"markdown","f5a3663b":"markdown","408e09d3":"markdown","52b0a473":"markdown","199cb19e":"markdown","3185675e":"markdown","fb03a7c3":"markdown","33506222":"markdown","ad9be296":"markdown","8bc6355f":"markdown","e74a93ca":"markdown","968d7ea0":"markdown","61a9f3e6":"markdown","730dae50":"markdown","7ef0bc06":"markdown"},"source":{"7a67d095":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7ee27ff0":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import IncrementalPCA\n\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\npd.set_option('display.max_columns',None)\npd.set_option('display.max_rows',None)","62a316ef":"numbers = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\nnumbers.head()","4cafaf47":"numbers.shape","1b87e1fa":"numbers.info()","1390e562":"numbers.describe(percentiles = [0.05,0.10,0.25,0.50,0.75,0.90,0.99])","83ccf17b":"#missing value check\nsum(numbers.isnull().sum(axis=0))","960ebb4e":"numbers['label'].value_counts()","02212fb2":"np.unique(numbers['label'])","3b632eb2":"sns.countplot(numbers['label'],palette = 'icefire')","16761dda":"#Checking average value of all pixels\n#round(numbers.drop('label', axis=1).mean(), 2).sort_values(ascending = False)\ny = pd.value_counts(numbers.values.ravel()).sort_index()","57e5caf1":"width = 0.9\nplt.figure(figsize=[8,8])\nplt.bar(range(len(y)),y,width,color=\"blue\")\nplt.title('Pixel Value Frequency (Log Scale)')\nplt.yscale('log')\nplt.xlabel('Pixel Value (0-255)')\nplt.ylabel('Frequency')","a9b80c01":"plt.figure(figsize=[15,15])\nplt.subplot(2,3,1)\nsns.distplot(numbers['pixel575'],kde=False)\nplt.subplot(2,3,2)\nsns.distplot(numbers['pixel624'],kde=False)\nplt.subplot(2,3,3)\nsns.distplot(numbers['pixel572'],kde=False)\nplt.subplot(2,3,4)\nsns.distplot(numbers['pixel407'],kde=False)\nplt.subplot(2,3,5)\nsns.distplot(numbers['pixel576'],kde=False)\nplt.subplot(2,3,6)\nsns.distplot(numbers['pixel580'],kde=False)\nplt.show()","8e1a55ce":"plt.figure(figsize=[15,12])\nplt.subplot(2,3,1)\nsns.barplot(x='label', y='pixel575', data=numbers)\nplt.subplot(2,3,2)\nsns.barplot(x='label', y='pixel624', data=numbers)\nplt.subplot(2,3,3)\nsns.barplot(x='label', y='pixel572', data=numbers)\nplt.subplot(2,3,4)\nsns.barplot(x='label', y='pixel683', data=numbers)\nplt.subplot(2,3,5)\nsns.barplot(x='label', y='pixel576', data=numbers)\nplt.subplot(2,3,6)\nsns.barplot(x='label', y='pixel580', data=numbers)\nplt.show()","7590006f":"numbers.loc[numbers['label']==1].head(10).index.values","8e2e7ef0":"plt.figure(figsize=[10,10])\nones_index = numbers.loc[numbers['label']==1].head(10).index.values\nfor i in range(0,10):\n    one = numbers.iloc[ones_index[i], 1:]\n    one = one.values.reshape(28,28)\n    plt.subplot(2,5,i+1)\n    plt.imshow(one)\n    plt.title(\"Digit 1\")","e31d00e3":"plt.figure(figsize=[10,10])\nthrees_index = numbers.loc[numbers['label']==3].head(10).index.values\nfor i in range(0,10):\n    one = numbers.iloc[threes_index[i], 1:]\n    one = one.values.reshape(28,28)\n    plt.subplot(2,5,i+1)\n    plt.imshow(one)\n    plt.title(\"Digit 3\")","91cb2286":"plt.figure(figsize=[10,10])\nfives_index = numbers.loc[numbers['label']==5].head(10).index.values\nfor i in range(0,10):\n    one = numbers.iloc[fives_index[i], 1:]\n    one = one.values.reshape(28,28)\n    plt.subplot(2,5,i+1)\n    plt.imshow(one)\n    plt.title(\"Digit 5\")","3a72bf25":"plt.figure(figsize=[10,10])\nfours_index = numbers.loc[numbers['label']==4].head(10).index.values\nfor i in range(0,10):\n    one = numbers.iloc[fours_index[i], 1:]\n    one = one.values.reshape(28,28)\n    plt.subplot(2,5,i+1)\n    plt.imshow(one)\n    plt.title(\"Digit 4\")","f6fa249b":"### Heatmap","e10943de":"y = numbers['label']\nX = numbers.drop('label',axis=1)\nX.head()","a16fdaf6":"y[:5]","690b062e":"#test train split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.2 ,test_size = 0.8, random_state=100)","517ca758":"scaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n#X_train_scaled = pd.DataFrame(X_train_scaled)\n#round(X_train_scaled.describe(),2)","9c20dd7c":"print('X_train shape:',X_train_scaled.shape)\nprint('y_train shape:',y_train.shape)\nprint('X_test shape:',X_test_scaled.shape)\nprint('y_test shape:',y_test.shape)","2a03e048":"pca = PCA(random_state=42)","e62b73ea":"pca.fit(X_train_scaled)","e6ae4d03":"pca.components_.shape","fc4b4256":"#pca.explained_variance_ratio_","7a744b24":"var_cummu = np.cumsum(pca.explained_variance_ratio_)","be27b7c3":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize=[12,8])\nplt.vlines(x=200, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\nplt.hlines(y=0.91, xmax=800, xmin=0, colors=\"g\", linestyles=\"--\")\nplt.plot(var_cummu)\nplt.ylabel(\"Cumulative variance explained\")\nplt.show()","59a4e2f9":"pca_final = IncrementalPCA(n_components = 200)","fd56bedf":"X_train_pca = pca_final.fit_transform(X_train_scaled)","3d1c5b5f":"X_train_pca.shape","78270d0c":"pca_final.components_.shape","87ad7d13":"df_train_pca = pd.DataFrame(X_train_pca)\ndf_train_pca.head()","7b12184b":"y_train_df = pd.DataFrame(y_train)\ny_train_df['label'].value_counts()","b248152a":"new_df = pd.concat([df_train_pca,y_train_df],axis=1)\nnew_df['label'].value_counts().sort_index()","ab77f475":"plt.figure(figsize=(10,10))\nsns.scatterplot(x=new_df[1],y=new_df[0],hue=new_df['label'],size=10,legend='full',palette='rainbow')","f418fd77":"sns.pairplot(data=new_df, x_vars=[0,1,2], y_vars=[0,1,2], hue = \"label\", size=5)","2d32f992":"pca_final.explained_variance_ratio_","646a090f":"X_test_pca = pca_final.transform(X_test_scaled)\nX_test_pca.shape","e1d8777f":"# linear model\nmodel_linear = SVC(kernel='linear')\nmodel_linear.fit(X_train_pca, y_train)\n\n# predict\ny_train_pred = model_linear.predict(X_train_pca)\ny_test_pred = model_linear.predict(X_test_pca)","b133c8b8":"train_accuracy = metrics.accuracy_score(y_train,y_train_pred)\nprint(\"Accuracy on training data: {}\".format(train_accuracy))\ntest_accuracy = metrics.accuracy_score(y_test,y_test_pred)\nprint(\"Accuracy on testing data: {}\".format(test_accuracy))\n\nprint(\"\\nClassification report on testing set \\n\")\nprint(metrics.classification_report(y_test, y_test_pred))\n\nprint(\"\\nConfusion metrics on testing set \\n\")\nprint(metrics.confusion_matrix(y_true=y_test, y_pred=y_test_pred))","2237a591":"# non-linear model\n# using poly kernel, C=1, default value of gamma\n\n# model\nnon_linear_model_poly = SVC(kernel='poly')\nnon_linear_model_poly.fit(X_train_pca, y_train)\n\n# predict\ny_train_pred = non_linear_model_poly.predict(X_train_pca)\ny_test_pred = non_linear_model_poly.predict(X_test_pca)","5658a5b1":"train_accuracy = metrics.accuracy_score(y_train,y_train_pred)\nprint(\"Accuracy on training data: {}\".format(train_accuracy))\ntest_accuracy = metrics.accuracy_score(y_test,y_test_pred)\nprint(\"Accuracy on testing data: {}\".format(test_accuracy))\n\nprint(\"\\nClassification report on testing set \\n\")\nprint(metrics.classification_report(y_test, y_test_pred))\nprint(\"\\nConfusion metrics on testing set \\n\")\nprint(metrics.confusion_matrix(y_true=y_test, y_pred=y_test_pred))","54f80fe7":"# non-linear model\n# using rbf kernel, C=1, default value of gamma\n\n# model\nnon_linear_model_poly = SVC(kernel='rbf')\nnon_linear_model_poly.fit(X_train_pca, y_train)\n\n# predict\ny_train_pred = non_linear_model_poly.predict(X_train_pca)\ny_test_pred = non_linear_model_poly.predict(X_test_pca)","eead6316":"train_accuracy = metrics.accuracy_score(y_train,y_train_pred)\nprint(\"Accuracy on training data: {}\".format(train_accuracy))\ntest_accuracy = metrics.accuracy_score(y_test,y_test_pred)\nprint(\"Accuracy on testing data: {}\".format(test_accuracy))\n\nprint(\"\\nClassification report on testing set \\n\")\nprint(metrics.classification_report(y_test, y_test_pred))\nprint(\"\\nConfusion metrics on testing set \\n\")\nprint(metrics.confusion_matrix(y_true=y_test, y_pred=y_test_pred))","27015dc5":"pipe_steps = [('scaler',StandardScaler()),('pca',PCA()),('SVM',SVC(kernel='poly'))]\ncheck_params = {\n    'pca__n_components' : [195,200],\n    'SVM__C':[1,10],\n    'SVM__gamma':[0.01,0.001]\n}\n\npipeline = Pipeline(pipe_steps)","5584dcaa":"folds = KFold(n_splits=3,shuffle=True,random_state=101)\n\n\n#setting up GridSearchCV()\nmodel_cv = GridSearchCV(estimator = pipeline,\n                       param_grid = check_params,\n                       scoring = 'accuracy',\n                       cv = folds,\n                       verbose = 3,\n                       return_train_score=True,\n                       n_jobs=-1)\n\n#fit the model\nmodel_cv.fit(X_train,y_train) # Considering our initial data as scaling will be handled by the pipeline.\nbest_score = model_cv.best_score_\nbest_hyperparams = model_cv.best_params_\n\nprint(\"The best test score is {0} corresponding to hyperparameters {1}\".format(best_score, best_hyperparams))","82291ab7":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","eae6cc38":"# converting C to numeric type for plotting on x-axis\ncv_results['param_SVM__C'] = cv_results['param_SVM__C'].astype('int')\n\n# # plotting\nplt.figure(figsize=(20,7))\n\n# subplot 1\/3\nplt.subplot(121)\ngamma_01 = cv_results[(cv_results['param_SVM__gamma']==0.01) & (cv_results['param_pca__n_components'] == 195)]\n\nplt.plot(gamma_01[\"param_SVM__C\"], gamma_01[\"mean_test_score\"])\nplt.plot(gamma_01[\"param_SVM__C\"], gamma_01[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.01\")\nplt.ylim([0.50, 1.1])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\n\n\n# subplot 2\/3\nplt.subplot(122)\ngamma_001 = cv_results[(cv_results['param_SVM__gamma']==0.001) & (cv_results['param_pca__n_components'] == 195)]\n\nplt.plot(gamma_001[\"param_SVM__C\"], gamma_001[\"mean_test_score\"])\nplt.plot(gamma_001[\"param_SVM__C\"], gamma_001[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.001\")\nplt.ylim([0.50, 1.1])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\n\n","c39eeafd":"# # plotting\nplt.figure(figsize=(20,7))\n\n# subplot 1\/3\nplt.subplot(121)\ngamma_01 = cv_results[(cv_results['param_SVM__gamma']==0.01) & (cv_results['param_pca__n_components'] == 200)]\n\nplt.plot(gamma_01[\"param_SVM__C\"], gamma_01[\"mean_test_score\"])\nplt.plot(gamma_01[\"param_SVM__C\"], gamma_01[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.01\")\nplt.ylim([0.50, 1.1])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\n\n\n# subplot 2\/3\nplt.subplot(122)\ngamma_001 = cv_results[(cv_results['param_SVM__gamma']==0.001) & (cv_results['param_pca__n_components'] == 200)]\n\nplt.plot(gamma_001[\"param_SVM__C\"], gamma_001[\"mean_test_score\"])\nplt.plot(gamma_001[\"param_SVM__C\"], gamma_001[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.001\")\nplt.ylim([0.50, 1.1])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')","2b94bc71":"pca_final = IncrementalPCA(n_components = 195)\n\nX_train_pca = pca_final.fit_transform(X_train_scaled)\nX_test_pca = pca_final.transform(X_test_scaled)\nprint(X_test_pca.shape)\nprint(X_train_pca.shape)","0c371ae4":"# model with optimal hyperparameters\n\n# model\nfinal_model = SVC(C=1, gamma=0.01, kernel=\"poly\")\n\nfinal_model.fit(X_train_pca, y_train)\n# predict\ny_train_pred = final_model.predict(X_train_pca)\ny_test_pred = final_model.predict(X_test_pca)","8176d6c6":"# metrics\ntrain_accuracy = metrics.accuracy_score(y_train,y_train_pred)\nprint(\"Accuracy on training data: {}\".format(train_accuracy))\ntest_accuracy = metrics.accuracy_score(y_test,y_test_pred)\nprint(\"Accuracy on testing data: {}\".format(test_accuracy))\n\nprint(\"\\nClassification report on testing set \\n\")\nprint(metrics.classification_report(y_test, y_test_pred))\n\nprint(\"\\nConfusion metrics on testing set \\n\")\nprint(metrics.confusion_matrix(y_true=y_test, y_pred=y_test_pred))","ee19fda2":"#import file and reading few lines\ntest_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\ntest_df.head(10)","1906696f":"test_df.shape","af57f778":"test_scaled = scaler.transform(test_df)","aaf10d91":"final_test_pca = pca_final.transform(test_scaled)\nfinal_test_pca.shape","16ddb8a4":"#model.predict\ntest_predict = final_model.predict(final_test_pca)","1376f951":"# Plotting the distribution of prediction\na = {'ImageId': np.arange(1,test_predict.shape[0]+1), 'Label': test_predict}\ndata_to_export = pd.DataFrame(a)\nsns.countplot(data_to_export['Label'], palette = 'icefire')","e93057ca":"# Let us visualize few of predicted test numbers\ndf = np.random.randint(1,test_predict.shape[0]+1,5)\n\nplt.figure(figsize=(16,4))\nfor i,j in enumerate(df):\n    plt.subplot(150+i+1)\n    d = test_scaled[j].reshape(28,28)\n    plt.title(f'Predicted Label: {test_predict[j]}')\n    plt.imshow(d)\nplt.show()","f6262412":"# Exporting the predicted values \ndata_to_export.to_csv(path_or_buf='submission.csv', index=False)","b7b3e45c":"submitted = pd.read_csv('submission.csv')","5332c9fe":"submitted.head()","8d0cae4f":"#### Let us first try linear model","24f65e2f":"#### No null values in the data. Therefore, we can proceed with the data exploration","065937a1":"### Reading and understanding Data","08dd52d8":"#### We just built our first SVM model using a `linear` kernel. Its time to evaluate our model using Accuracy as our evauation metric","30e9e71d":"#### Accuracy is descreased by using rbf model. Lets go forward with poly kernel model.","f6cb65c9":"### Using final model on unseen data (test.csv)","f52a30c4":"### Redcucing the dimensions of the test data","01269891":"## Dimensionality Reduction: \nWe see that number of pixels are a quite large. Let us first try to reduce the number of features with the help of PCA","5d0fa946":"#### Next, lets see how the pixel values are distributed","d0da324e":"### Lets visualize how a digit is written in different styles","8f83beea":"## Handwritten digit recognizer using SVM","425391c5":"#### Inferences:\n- Label 6 have an average value of 255 for pixel575 and pixel572\n- Label 2 have an average value of 100 for pixel580\n","a279ee30":"Trying `poly` kernel","bf73de22":"#### Trying `rbf` kernel","47d89a52":"#### There is no data imbalance and the target values are equally distributed","9c0da69f":"## Data Preparation\n- Splitting data into train and test\n- Scaling of the data","75290206":"### Performing PCA with 200 components","7b896850":"### Data Cleaning and Data exploration","2bcddb5f":"#### Using Pipeline for performing scaling, PCA and SVM on the Data","f5a3663b":"#### Till now we have converted our initial scaled data to pca transformed data\n\n    - X_train --> X_train_scaled --> X_train_pca\n    - X_test  --> X_test_scaled  --> X_test_pca","408e09d3":"Till now, after PCA transformation with 200 components, we tried SVM using 3 different kernels and using default hyperparamters (C and gamma)\nWe finialized that `PCA transformed` data along with `poly` SVM kernel gives us an accuracy of 96%.\n\nTherefore, to further fine tune our model we can consider following changes:\n- Trying different number of PCA components. We can connsider using 195 and 200 components.\n- Trying different values of hyperparameter C. The value of C tells us how much you want to avoid misclassifying on training data. We can consider C as 1, 10.\n- Trying different values of hyperparameter gamma. The hyperparameter gamma controls  the amount of non-linearity in the model - as gamma increases, the model becomes more non-linear, and thus model complexity increases. We can consider gamma as 0.1,0.01.\n    \nNow we want to know what should be the best combination of values of number PCA compnents, C and gamma for our model built using `poly` kernel of SVM.\n\nWe will be using grid search cross validation to find the best comnination of the hyperparamters.\n\nAlong with this, we will introduce the pipelining functionality of sklearn.\n\nTill now the steps we did include:\n\n1. Scale the initial data\n2. Perform PCA to reduce the dimensionality\n3. Build a model using SVM.\n\nA pipeline, will schedule all the above steps and create the final model in our gridSearch.","52b0a473":"A classifier to predict the label of a given digit using PCA and SVM\n\nThis kernel aims at building a classifier to predict the label of a handwritten digits using PCA and SVM\n\n<b>Dataset<\/b>\n\nThe dataset consists of images of handwritten numeric digits between 0-9. Each image is of 28 x 28 pixels, i.e. 28 pixels along both length and breadth of the image. Each pixel is an attribute with a numeric value (representing the intensity of the pixel), and thus, there are 784 attributes in the dataset.\n\nIt can be found at the kaggle link\nhttps:\/\/www.kaggle.com\/c\/digit-recognizer\/data\n\n<b>Problem Statement<\/b>\n\nThe task is to build a classifier that predicts the label of an image (a digit between 0-9) given the features. Thus, this is a 10-class classification problem.\nSince this dataset has a large number of features (784), we will use PCA to reduce the dimensionality, and then, build a model on the low-dimensional data.\n\n<b>Solution Overview:<\/b>\n\nThe following topics are covered in this tutorial\n\n<b>Understanding and Exploring the Data<\/b>\n \n<b>Dimensionality Reduction: <\/b>\nPCA will be used to reduce the dimensionality of the dataset.\n\n<b>Model building using SVM on PCA transformed data<\/b>\nLinear, Poly and rbf kernel will be used to build the model and the comparisons will be done.\n\n<b>Hyperparameter tuning and pipelining:<\/b>\nFinal model will be built with the help of hyperparameter tuning by cross validation grid search and pipeline functionality.\n","199cb19e":"Now, gridSearch will be used to find the best combbiantion of parameters. In the above pipeline, for every value of PCA number of components, the grid search will build model 4 models.\n\n![image.png](attachment:image.png)\n\nAs we are considering, 2 values for PCA components, therefore, GridSearch will build 8 different models. \n\nFor the 8 different models, we will be using KFold cross validation with 3 splits. Therefore, the training will be done on 8X3 = 24 different models fits.\n\nNow lets get our final parameters for the model.","3185675e":"#### Lets evaluate our model using Accuracy as our evauation metric","fb03a7c3":"#### Now we have the best values of hyper parameters. \n### Let us build the final model using these values and evaluate the results.","33506222":"### Model building using SVM on PCA transformed data\n\nSVM uses following three kernel to buld a model.\n\n<b>The linear kernel:<\/b> This gives the linear support vector classifier, or the hyperplane.\n\n<b>The polynomial kernel:<\/b> It is capable of creating nonlinear, polynomial decision boundaries \n\n<b>The radial basis function (RBF) kernel:<\/b> This is the most complex one, which is capable of transforming highly nonlinear feature spaces to linear ones. It is even capable of creating elliptical (i.e. enclosed) decision boundaries\n","ad9be296":"#### Checking distribution of pixel values with repect to labels","8bc6355f":"### The accuracy of the final model is 95.68%","e74a93ca":"We see that 90% of variance is explained by 200 Principal Components.","968d7ea0":"#### Appying PCA to reduce the number of  features","61a9f3e6":"#### The linear model gives approx. 90.6% testing accuracy. Whereas our training data shows an accuracy of 95%. \n\n#### Let's look at a non-linear model with randomly chosen hyperparameters.\n","730dae50":"#### The accuracy increased to 95.66, so obviously no point in going with linear. Let us try 'rbf'.","7ef0bc06":"<b>The above table gives the results for 24 different model fits that were done by the grid Search. The best parameteres are selected based on the `mean_train_score` and `mean_test_score` as shown in the above table.\n"}}