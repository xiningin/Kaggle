{"cell_type":{"24dbab44":"code","89dc4bbc":"code","6c439504":"code","2fb9e7ec":"code","4d386f1d":"code","c583f87f":"code","f1abe211":"code","db2ba39e":"code","6c78f08b":"code","f45d5dee":"code","6815ac49":"code","cedb09ef":"code","e5e70206":"code","c59dd739":"code","60ac881f":"code","bc5569be":"code","4d7cbdbb":"code","0a34cea8":"code","9ab65458":"markdown","27788c64":"markdown","b4f1a1fb":"markdown","b09c433c":"markdown","7f319464":"markdown","ca285e77":"markdown","1ca7d70f":"markdown","e92efb2a":"markdown","8cf1c2d6":"markdown","7d40ef63":"markdown","1495562c":"markdown","3a4ed988":"markdown","ace8d8d7":"markdown","13986e4f":"markdown","05fc5a9e":"markdown","a27de667":"markdown","667ae6dc":"markdown"},"source":{"24dbab44":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense, Dropout\nfrom keras.callbacks import EarlyStopping\n\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport nltk\nnltk.download(\"stopwords\")","89dc4bbc":"plt.rcParams['figure.figsize'] = (16,8)","6c439504":"df = pd.read_csv(\"..\/input\/winemag-data_first150k.csv\")\ndf.head()","2fb9e7ec":"print(df.shape)\ndf = df.drop(columns=[\"Unnamed: 0\", \"region_1\", \"region_2\"])\ndf = df.dropna()\ndf.head()\nprint(df.shape)","4d386f1d":"df.describe()","c583f87f":"sns.countplot(x=\"points\", data=df)","f1abe211":"# Hard to see so will limit the x axis\nsns.distplot(df.price, bins=500).set(xlim=(0, 250))","db2ba39e":"g = sns.countplot(x=\"province\", data=df,  order = df['province'].value_counts().iloc[:20].index)\nplt.setp(g.get_xticklabels(), rotation=45)\nplt.show()","6c78f08b":"g = sns.countplot(x=\"variety\", data=df,  order = df['variety'].value_counts().iloc[:20].index)\nplt.setp(g.get_xticklabels(), rotation=45)\nplt.show()","f45d5dee":"dfWinary = df.groupby('winery').mean()\ndfWinary.sort_values(by='points', ascending=False)['points'].iloc[:20].plot(kind='bar').set_title('Winery average point')\nplt.show()\ndfWinary.sort_values(by='price', ascending=False)['price'].iloc[:20].plot(kind='bar').set_title('Winery average price')\nplt.show()\ndfWinary['P\/P'] = dfWinary.points \/ dfWinary.price\ndfWinary.sort_values(by='P\/P', ascending=False)['P\/P'].iloc[:20].plot(kind='bar').set_title('Most bang for the buck')\nplt.show()","6815ac49":"df = pd.read_csv(\"..\/input\/winemag-data_first150k.csv\")\nprint(df.shape)\ndf = df[[\"description\",\"variety\"]]\ndf = df.dropna()\ndf = df.drop_duplicates(subset=\"description\")\nprint(df.shape)\ndf.head()","cedb09ef":"df.description = df.description.str.replace('[^A-Za-z\\s]+', '')\ndf.description = df.description.str.lower()","e5e70206":"ps = PorterStemmer()\nenglish_words = stopwords.words('english')\n\ndef stem_words(sentence):\n    sentence = sentence.split()\n    sentence = [word for word in sentence if not word in english_words]\n    sentence = [ps.stem(word) for word in sentence]\n    return ' '.join(sentence)\n\nprint(df['description'].head())\ndf['description'] = df['description'].apply(stem_words)\nprint(df['description'].head())","c59dd739":"vocab_size = 10000\ntokenize = Tokenizer(num_words=vocab_size)\n\ntokenize.fit_on_texts(df.description)\nprint(len(tokenize.word_index) + 1)\n\nX = tokenize.texts_to_matrix(df.description, mode='tfidf')","60ac881f":"encoder = LabelBinarizer()\nencoder.fit(df.variety)\nY = encoder.transform(df.variety)\n\ndel df","bc5569be":"x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n\ndel X\ndel Y","4d7cbdbb":"model = Sequential()\n\nmodel.add(Dense(128, input_dim=x_train.shape[1]))\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(256))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(y_train.shape[1]))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n              optimizer='adam')\n\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n\nmodel_output = model.fit(x_train, y_train,\n             batch_size=32,\n             epochs=100,\n             verbose=2,\n             callbacks=[es],\n             validation_split=0.2)","0a34cea8":"print('Training Accuracy : ' , np.mean(model_output.history[\"acc\"]))\nprint('Validation Accuracy : ' , np.mean(model_output.history[\"val_acc\"]))\n\nplt.plot(model_output.history['acc'])\nplt.plot(model_output.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(model_output.history['loss'])\nplt.plot(model_output.history['val_loss'])\nplt.title('model_output loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","9ab65458":"Building and training my model. Using categorical_crossentropy and softmax since it has multiple categories. Adding an earlyStopping to stop it from overfitting. ","27788c64":"Validating and testing my model with the saved validation data","b4f1a1fb":"Then we will transform the variety by using sklearn LabelBinarizer","b09c433c":"So the average around 88 points seem to be on the spot!\nLets see how price looks like:","7f319464":"Then we will use keras tokenizer and limiting the vocabulary to 10000 words. ","ca285e77":"So these wineries are the ones with best rating for the cheapest cost! ;)","1ca7d70f":"Hi,\n\nThis is my first kernel here on Kaggle. \nPlease be gentle and hope you enjoy it as much as I did creating it","e92efb2a":"Pinot Noir, Chardonnay, Red Blend (red blend is a wine that\u2019s not made from a specific grape variety) and Cabernet Sauvignon seems to be the mosed used grape variety. OIV\u2019s ranking of the main grape varieties planted worldwide in 2015 is as follows:\n1. Kyoh\u014d (dessert grape): 365,000 hectares.\n1. Cabernet-Sauvignon (wine grape): 340,000 ha.\n1. Sultanine (wine, dessert and raisin grape): 300,000 ha.\n1. Merlot (wine grape): 266,000 ha.\n1. Tempranillo (wine grape): 231,000 ha.\n1. Airen (wine and distillation grape):  218,000 ha.\n1. Chardonnay (wine grape): 211,000 ha.\n1. Syrah (wine grape): 190,000 ha.\n1. Grenache noir (or Garnacha tinta): 163,000 ha.\n1. Red Globe (dessert grape): 160,000 ha.\n1. Sauvignon blanc (wine grape): 121,000 ha.\n1. Pinot noir (or Blauerburgunder): 115,000 ha.\n1. Ugni blanc (or Trebbiano Toscano): 111,000 ha.\nSo having these for popular varieties in top is what we should expect ","8cf1c2d6":"First of, we need to clear the sentences! Start with removing the [stopwords](https:\/\/en.wikipedia.org\/wiki\/Stop_words) and then running it through [Porter Stemmer algorithm](http:\/\/www.nltk.org\/howto\/stem.html). ","7d40ef63":"Looking at the province we can see that the majority of the wine in the dataset comes from California, hopefully some good Zinfandel. Talking about Zinfandel, lets look at variety, there are more then 600 variety in the dataset.","1495562c":"Majority of the price seems to be located below 50, and lets be honest atleast Im not going to pay that much. Lets explore the proviance:","3a4ed988":"Not the best result and there are some more things to test out. Using more words in the tokenizer and also instead of tfidf try binary. Also trying out hyperas to make a better model instead of me guessing layers. Please share feedback, Im here to learn! Thanks for reading to the end, take care.","ace8d8d7":"Start with removing non characters and lowering all words","13986e4f":"**Now for extremly important EDA!**\nLet's find out what winery has the best mean points, price and cheapest points.\nOfcourse all in the name of... science","05fc5a9e":"Removing unwanted columns and NaN-values:","a27de667":"Now we got the input (X) and the output (Y), lets split them into training and validation sets. ","667ae6dc":"**Let's try to build a model for predictiong the variety by using the description**\nI'll reset the dataframe before cleaning it up"}}