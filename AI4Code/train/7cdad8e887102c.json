{"cell_type":{"451642f9":"code","81c33225":"code","da3f322f":"code","7e6e4dfb":"code","9448082e":"code","d754a557":"code","6c910214":"code","6219a08d":"code","7022608d":"code","3eec8126":"code","8d0cfa5c":"code","b73ad42d":"code","ddcd3745":"code","cce8af55":"code","3283b988":"code","06758a8f":"code","5557fe21":"code","3314b8e9":"code","ccf04061":"code","a7da2d4f":"code","3de8fe25":"code","ad21e9db":"code","95f4e797":"code","6681ae36":"code","7cf441c3":"code","ae4e5040":"code","98a8fe8e":"code","f9217ff0":"code","af4ca615":"code","a70e9cf1":"code","ffa483fa":"code","a694d733":"code","ae2b2cbe":"code","75e9fde2":"code","e912cf82":"code","34306b43":"code","7fea25d6":"code","000a99b8":"code","c6ab2bb4":"code","22f139f7":"code","4514ed88":"code","c4ebceda":"code","9fe47996":"code","b84d9ab9":"code","f165d7ba":"code","0f2b529f":"code","61a7f3e7":"code","a5ba6d3c":"markdown","9579afd0":"markdown","65095dbc":"markdown","16ed3194":"markdown","19474641":"markdown","9f57fbcf":"markdown","7e4c3a73":"markdown","932753fc":"markdown","73dd51e0":"markdown","940c3f2d":"markdown","03b5ab00":"markdown","00c41c9f":"markdown","81a4d8f4":"markdown","d5c03ed5":"markdown","6e86bc12":"markdown","f999b6ff":"markdown","39edd42d":"markdown","9e309412":"markdown","9ec86729":"markdown","f3b8d046":"markdown"},"source":{"451642f9":"#!python -m pip install detectron2==0.4 -f \\\n#  https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu110\/torch1.7\/index.html\n","81c33225":"!python -m pip install detectron2==0.4 -f \\\n  https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu102\/torch1.6\/index.html","da3f322f":"%pip freeze | grep torch","7e6e4dfb":"!mkdir -p models logs configs ","9448082e":"!nvidia-smi","d754a557":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom typing import List, Optional, Union\n\nimport torch\nimport torchvision\nimport albumentations as A\n\n\n\nimport detectron2\nfrom detectron2.data.transforms import Transform as T\nfrom detectron2.data.datasets import register_coco_instances\nfrom detectron2.data import transforms as T\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog, DatasetMapper, build_detection_test_loader , build_detection_train_loader\nfrom detectron2.engine import DefaultTrainer\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.data import detection_utils as utils\nfrom detectron2.config import configurable\nfrom detectron2.engine.hooks import EvalHook\nfrom detectron2.modeling import build_model\nfrom detectron2.checkpoint import DetectionCheckpointer\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\n\nfrom sklearn.model_selection import KFold,StratifiedKFold,StratifiedShuffleSplit,GroupKFold\nfrom sklearn.utils import check_random_state\nfrom collections import Counter, defaultdict\nimport matplotlib.pyplot as plt\nimport cv2\nimport json\nimport io\nimport os\nimport copy\nimport random\nfrom IPython.display import FileLink, FileLinks\nimport yaml\nfrom abc import ABC,ABCMeta, abstractmethod\nfrom yacs.config import CfgNode as CN","6c910214":"DATASET_PATH = \"..\/input\/tacotrashdataset\"\nLOGS_PATH = \"logs\"\nMODELS_PATH = \"models\"\nCONFIG_PATH = \"configs\"","6219a08d":"_C = CN()\n_C.general=CN()\n_C.general.seed = 42\n_C.general.n_folds = 5\n_C.general.tool = \"detectron2\"\n_C.general.experiment_id = \"26-04-2021\"\n_C.general.category = \"super_category\"\n_C.general.augmentations = True\n_C.general.TTA = False\n\n_C.preprocess=CN()\n_C.preprocess.height = 1500\n_C.preprocess.width = 1500\n_C.preprocess.longest_max_size = 1500\n_C.preprocess.smallest_max_size = 1000\n\n_C.model=CN()\n_C.model.base_lr = 0.0004\n_C.model.num_classes = 29 #29 if super category 60 if normal category \n_C.model.model_name = \"faster_rcnn_R_101_FPN_3x\"\n_C.model.batchsize_per_image = 1024\n#_C.model.images_per_batch = 4\n_C.model.images_per_batch = 4\n_C.model.epochs = 9","7022608d":"def get_cfg_defaults():\n    \"\"\"Get a yacs CfgNode object with default values for my_project.\"\"\"\n    # Return a clone so that the defaults will not be altered\n    # This is for the \"local variable\" use pattern\n    #return _C.clone()\n    return _C\n\ndef dump_cfg(config = get_cfg_defaults() , path = \"experiment.yaml\"):\n    \"\"\"Save a yacs CfgNode object in a yaml file in path.\"\"\"\n    stream = open(path, 'w')\n    stream.write(config.dump())\n    stream.close()\n\ndef inject_config(funct):\n    \"\"\"Inject a yacs CfgNode object in a function as first arg.\"\"\"\n    def function_wrapper(*args,**kwargs):\n        return funct(_C,*args,**kwargs)  \n    return function_wrapper\n\ndef dump_dict(config,path=\"config.yaml\"):\n        stream = open(path, 'w')\n        yaml.dump(config,stream)\n        stream.close()\n\nc=get_cfg_defaults()","3eec8126":"dump_cfg(path = os.path.join(LOGS_PATH , \"experiment.yaml\"))","8d0cfa5c":"@inject_config\ndef seed_all(config):\n    \"\"\"\n    seed my experiments to be able to reproduce\n    \"\"\"\n    seed_value=config.general[\"seed\"]\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False","b73ad42d":"annot=json.load(open(os.path.join(DATASET_PATH,\"data\/annotations.json\")))\nannot[\"annotations\"][308][\"id\"]=0\nannot[\"annotations\"][4039][\"id\"]=2197","ddcd3745":"annot_to_delete=[]\nfor idx,annotation in enumerate(annot[\"annotations\"]):\n    if (annotation[\"bbox\"][0]<0 or annotation[\"bbox\"][1]<0 or\n        annotation[\"bbox\"][2]<0 or annotation[\"bbox\"][3]<0):\n        annot_to_delete.append(idx)\nfor pos,idx in enumerate(annot_to_delete):\n    del annot[\"annotations\"][idx-pos]\n","cce8af55":"json.dump(annot,open(\"new_annotations.json\",\"w\"))","3283b988":"categories={ annotation[\"id\"] : annotation[\"name\"] for annotation in annot[\"categories\"]}\nsuper_categories={ annotation[\"id\"] : annotation[\"supercategory\"] for annotation in annot[\"categories\"]}","06758a8f":"annot_df=pd.DataFrame(annot[\"annotations\"])\nimages_df=pd.DataFrame(annot[\"images\"])","5557fe21":"images_df.describe()","3314b8e9":"annot_df[\"category\"]=annot_df[\"category_id\"].apply(lambda value : categories[value])\nannot_df[\"super_category\"]=annot_df[\"category_id\"].apply(lambda value : super_categories[value])\nsuper_category_to_index={value : key for key,value in enumerate(annot_df[\"super_category\"].unique())}\nannot_df[\"super_category_id\"]=annot_df[\"super_category\"].apply(lambda value : super_category_to_index[value])\nannot_df[\"normal_category_id\"]=annot_df[\"category_id\"]\nannot_df[\"normal_category\"]=annot_df[\"category\"]\nif c.general[\"category\"] != \"normal_category\":\n    annot_df[\"category_id\"]=annot_df[\"super_category_id\"]\n    annot_df[\"category\"]=annot_df[\"super_category\"]\n    annot_cat=annot_df.groupby(\"category_id\")[[\"category_id\",\"category\",\"super_category\"]].first()\n    annot_cat.columns=[\"id\",\"name\",\"supercategory\"]\n    annot[\"categories\"]=annot_cat.to_dict(\"records\")\n\n                       ","ccf04061":"categories = {}\nfor id , name in zip(annot_df[\"category_id\"],annot_df[\"category\"]):\n    categories[id]=name","a7da2d4f":"categories","3de8fe25":"annot_df.head()","ad21e9db":"annot_df[\"category\"].value_counts().head(10).plot(kind=\"bar\",title=\"annotations category distribution\")","95f4e797":"annot_df[\"super_category\"].value_counts().head(10).plot(kind=\"bar\",title=\"annotations super category distribution\")","6681ae36":"class RepeatedStratifiedGroupKFold():\n\n    def __init__(self, n_splits=5, n_repeats=1, random_state=None):\n        self.n_splits = n_splits\n        self.n_repeats = n_repeats\n        self.random_state = random_state\n        \n    def split(self, X, y=None, groups=None):\n        k = self.n_splits\n        def eval_y_counts_per_fold(y_counts, fold):\n            y_counts_per_fold[fold] += y_counts\n            std_per_label = []\n            for label in range(labels_num):\n                label_std = np.std(\n                    [y_counts_per_fold[i][label] \/ y_distr[label] for i in range(k)]\n                )\n                std_per_label.append(label_std)\n            y_counts_per_fold[fold] -= y_counts\n            return np.mean(std_per_label)\n            \n        rnd = check_random_state(self.random_state)\n        for repeat in range(self.n_repeats):\n            #print(np.max(y))\n            labels_num = np.max(y) + 1\n            y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n            y_distr = Counter()\n            for label, g in zip(y, groups):\n                y_counts_per_group[g][label] += 1\n                y_distr[label] += 1\n\n            y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n            groups_per_fold = defaultdict(set)\n        \n            groups_and_y_counts = list(y_counts_per_group.items())\n            rnd.shuffle(groups_and_y_counts)\n\n            for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n                best_fold = None\n                min_eval = None\n                for i in range(k):\n                    fold_eval = eval_y_counts_per_fold(y_counts, i)\n                    if min_eval is None or fold_eval < min_eval:\n                        min_eval = fold_eval\n                        best_fold = i\n                y_counts_per_fold[best_fold] += y_counts\n                groups_per_fold[best_fold].add(g)\n            \n            all_groups = set(groups)\n            for i in range(k):\n                train_groups = all_groups - groups_per_fold[i]\n                test_groups = groups_per_fold[i]\n\n                train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n                test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n\n                yield train_indices, test_indices\n","7cf441c3":"@inject_config\ndef kfold_split(config,df):\n    seed_all()\n    df[\"folds\"]=-1\n    #kf = GroupKFold(n_splits=config.general[\"n_folds\"])\n    kf = RepeatedStratifiedGroupKFold(n_splits=config.general[\"n_folds\"], random_state=config.general[\"seed\"])\n    #for fold, (_, val_index) in enumerate(kf.split(df,groups=df[\"image_id\"])):\n    for fold, (_, val_index) in enumerate(kf.split(df,df.category_id, df.image_id)):\n            df.loc[val_index, \"folds\"] = fold\n    return df","ae4e5040":"annot_df=kfold_split(annot_df)","98a8fe8e":"annot_df[annot_df[\"folds\"]==0][\"category\"].value_counts().head(10).plot(kind=\"bar\",title=\"Test annotations category distribution\")","f9217ff0":"annot_df[annot_df[\"folds\"]!=0][\"category\"].value_counts().head(10).plot(kind=\"bar\",title=\"Train annotations category distribution\")","af4ca615":"annot_df[annot_df[\"folds\"]==0][\"super_category\"].value_counts().head(10).plot(kind=\"bar\",title=\"Test annotations super category distribution\")","a70e9cf1":"annot_df[annot_df[\"folds\"]==0][\"super_category\"].value_counts().head(10).plot(kind=\"bar\",title=\"Train annotations super category distribution\")","ffa483fa":"@inject_config\ndef register_dataset(config,fold):\n    train_dataset_name=f\"my_dataset_train_{fold}\"\n    test_dataset_name=f\"my_dataset_test_{fold}\"\n    train_dataset_file=f\"my_dataset_train_{fold}.json\"\n    test_dataset_file=f\"my_dataset_test_{fold}.json\"\n    \n    train_annot_df=annot_df[annot_df[\"folds\"]!=fold]\n    test_annot_df=annot_df[annot_df[\"folds\"]==fold]\n    train_annot_df=train_annot_df.drop([\"normal_category\",\"normal_category_id\"],axis=1)\n    test_annot_df=test_annot_df.drop([\"normal_category\",\"normal_category_id\"],axis=1)\n\n    train_images_df=images_df[images_df[\"id\"].apply(lambda i:True if i in list(train_annot_df[\"image_id\"].unique()) else False)]\n    test_images_df=images_df[images_df[\"id\"].apply(lambda i:True if i in list(test_annot_df[\"image_id\"].unique()) else False)]\n    \n    train_annot=annot.copy()\n    test_annot=annot.copy()\n    \n    train_annot[\"annotations\"]=train_annot_df.reset_index(drop=True).to_dict(\"records\")\n    train_annot[\"images\"]=train_images_df.reset_index(drop=True).to_dict(\"records\")\n    test_annot[\"annotations\"]=test_annot_df.reset_index(drop=True).to_dict(\"records\")\n    test_annot[\"images\"]=test_images_df.reset_index(drop=True).to_dict(\"records\")\n    \n    json.dump(train_annot,open(train_dataset_file,\"w\"))\n    json.dump(test_annot,open(test_dataset_file,\"w\"))\n    \n    if train_dataset_name in DatasetCatalog.list():\n        DatasetCatalog.remove(train_dataset_name)\n        MetadataCatalog.remove(train_dataset_name)\n    if test_dataset_name in DatasetCatalog.list():\n        DatasetCatalog.remove(test_dataset_name)\n        MetadataCatalog.remove(test_dataset_name)\n        \n    register_coco_instances(train_dataset_name, {}, train_dataset_file, os.path.join(DATASET_PATH,\"data\"))\n    register_coco_instances(test_dataset_name, {}, test_dataset_file, os.path.join(DATASET_PATH,\"data\"))\n","a694d733":"@inject_config\ndef get_train_transforms(config):\n    return A.Compose(\n        [\n            A.OneOf([\n                A.HueSaturationValue(hue_shift_limit=0.1, sat_shift_limit= 0.1, \n                                     val_shift_limit=0.1, p=0.8),\n                A.RandomBrightnessContrast(brightness_limit=0.3, \n                                           contrast_limit=0.2, p=0.8),\n            ],p=0.7),\n            A.Rotate (limit=15, interpolation=1, border_mode=4, value=None, mask_value=None, p=0.8),\n            \n            \n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.RandomResizedCrop (config.preprocess.height, config.preprocess.width, scale=(0.8, 0.8), ratio=(0.75, 1.3333333333333333), interpolation=1, always_apply=False, p=0.1),\n            A.OneOf([\n            A.Resize(height=config.preprocess.height, width=config.preprocess.width, p=0.2),\n            A.LongestMaxSize(max_size=config.preprocess.longest_max_size, p=0.2),\n            A.SmallestMaxSize(max_size=config.preprocess.smallest_max_size, p=0.2),\n                \n            ], p=1),\n            A.CLAHE(clip_limit=[1,4],p=1),\n            \n        ], \n        p=1.0, \n        bbox_params=A.BboxParams(\n            format='coco',\n            min_area=0.5, \n            min_visibility=0.5,\n            label_fields=['category_id']\n        )\n    )\n\n@inject_config\ndef get_valid_transforms(config):\n    return A.Compose(\n        [\n            A.SmallestMaxSize(max_size=config.preprocess.smallest_max_size, p=1.0),\n            A.CLAHE(clip_limit=[3,3],p=1),   \n        ], \n        p=1.0, \n        bbox_params=A.BboxParams(\n            format='coco',\n            min_area=0.5, \n            min_visibility=0.5,\n            label_fields=['category_id']\n        )\n    )\n\ndef get_transforms(train=True):\n    if (train):\n        return get_train_transforms()\n    return get_valid_transforms()\nalbu_transformations=get_transforms(train=True)","ae2b2cbe":"class PersonalMapper (detectron2.data.DatasetMapper):\n    \"\"\"\n    Define a detectron2 personal mapper in order to be able to use albumentation augmentations\n    \"\"\"\n    def __call__(self, dataset_dict):\n        \"\"\"\n        Args:\n            dataset_dict (dict): Metadata of one image, in Detectron2 Dataset format.\n\n        Returns:\n            dict: a format that builtin models in detectron2 accept\n        \"\"\"\n        dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n        # USER: Write your own image loading if it's not from a file\n        image = utils.read_image(dataset_dict[\"file_name\"], format=self.image_format)\n        #utils.check_image_size(dataset_dict, image)\n\n        \n        \n        ##### ADDED PART\n\n        #print(\"dataset dict : \",dataset_dict)\n\n        annos = [\n            obj for obj in dataset_dict[\"annotations\"]\n        ]\n        annos_bbox = [\n            obj[\"bbox\"] for obj in dataset_dict[\"annotations\"]\n        ]\n        annos_categroy_id = [\n            obj[\"category_id\"] for obj in dataset_dict.pop(\"annotations\")\n        ]\n        \n        if albu_transformations is not None:\n            transform_list=get_transforms(self.is_train)\n            image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            transform_result=transform_list(image=image,bboxes=annos_bbox,category_id=annos_categroy_id)\n            image=cv2.cvtColor(transform_result[\"image\"], cv2.COLOR_RGB2BGR)\n            annos=[annos[i] for i in range(len(transform_result[\"bboxes\"]))]\n            for i in range(len(annos)):\n                annos[i][\"bbox\"]=list(transform_result[\"bboxes\"][i])\n                annos[i][\"category_id\"]=transform_result[\"category_id\"][i]\n        \n        dataset_dict[\"annotations\"]=annos\n        \n        \n        ##### ADDED PART\n        \n        # USER: Remove if you don't do semantic\/panoptic segmentation.\n        if \"sem_seg_file_name\" in dataset_dict:\n            sem_seg_gt = utils.read_image(dataset_dict.pop(\"sem_seg_file_name\"), \"L\").squeeze(2)\n        else:\n            sem_seg_gt = None\n\n        aug_input = T.AugInput(image, sem_seg=sem_seg_gt)\n        transforms = self.augmentations(aug_input)\n        image, sem_seg_gt = aug_input.image, aug_input.sem_seg\n\n        image_shape = image.shape[:2]  # h, w\n        # Pytorch's dataloader is efficient on torch.Tensor due to shared-memory,\n        # but not efficient on large generic data structures due to the use of pickle & mp.Queue.\n        # Therefore it's important to use torch.Tensor.\n        dataset_dict[\"image\"] = torch.as_tensor(np.ascontiguousarray(image.transpose(2, 0, 1)))\n        if sem_seg_gt is not None:\n            dataset_dict[\"sem_seg\"] = torch.as_tensor(sem_seg_gt.astype(\"long\"))\n\n        # USER: Remove if you don't use pre-computed proposals.\n        # Most users would not need this feature.\n        if self.proposal_topk is not None:\n            utils.transform_proposals(\n                dataset_dict, image_shape, transforms, proposal_topk=self.proposal_topk\n            )\n\n        if not self.is_train:\n            # USER: Modify this if you want to keep them for some reason.\n            dataset_dict.pop(\"annotations\", None)\n            dataset_dict.pop(\"sem_seg_file_name\", None)\n            return dataset_dict\n\n        if \"annotations\" in dataset_dict:\n            # USER: Modify this if you want to keep them for some reason.\n            for anno in dataset_dict[\"annotations\"]:\n                if not self.use_instance_mask:\n                    anno.pop(\"segmentation\", None)\n                if not self.use_keypoint:\n                    anno.pop(\"keypoints\", None)\n\n            # USER: Implement additional transformations if you have other types of data\n            annos = [\n                utils.transform_instance_annotations(\n                    obj, transforms, image_shape, keypoint_hflip_indices=self.keypoint_hflip_indices\n                )\n                for obj in dataset_dict.pop(\"annotations\")\n                if obj.get(\"iscrowd\", 0) == 0\n            ]\n            instances = utils.annotations_to_instances(\n                annos, image_shape, mask_format=self.instance_mask_format\n            )\n\n            # After transforms such as cropping are applied, the bounding box may no longer\n            # tightly bound the object. As an example, imagine a triangle object\n            # [(0,0), (2,0), (0,2)] cropped by a box [(1,0),(2,2)] (XYXY format). The tight\n            # bounding box of the cropped triangle should be [(1,0),(2,1)], which is not equal to\n            # the intersection of original bounding box and the cropping box.\n            if self.recompute_boxes:\n                instances.gt_boxes = instances.gt_masks.get_bounding_boxes()\n            dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n        return dataset_dict","75e9fde2":"class PersonalTrainer (detectron2.engine.defaults.DefaultTrainer):\n    \"\"\"\n    Personal trainer based on detectron2 DefaultTrainer to add some hooks and change data loaders\n    \"\"\"\n    \n    def __init__(self, cfg , config=c):\n        super().__init__(cfg)\n        self.metric=0\n        self.checkpointer.save_dir=MODELS_PATH\n\n        \n    def build_hooks(self):\n        hooks = super().build_hooks()\n        def save_best_model():\n            \n            metric=self.test(self.cfg, self.model)[\"bbox\"][\"AP50\"]\n            if(metric>self.metric):\n                self.metric=metric\n                self.checkpointer.save(\"best_model\") # it will add .pth alone\n                \n        steps_per_epoch=annot_df.shape[0]\/\/c.model[\"images_per_batch\"]\n        model_checkpointer=EvalHook(steps_per_epoch, save_best_model)\n        hooks.insert(-1,model_checkpointer)\n        return hooks\n    \n    @classmethod\n    def build_train_loader(cls, cfg):\n        \n        #return build_detection_train_loader(cfg,mapper=DatasetMapper(cfg,is_train=True,))\n        return build_detection_train_loader(cfg,mapper=PersonalMapper(cfg,is_train=True,augmentations=[]))\n    @classmethod\n    def build_test_loader(cls, cfg, dataset_name):\n        \n        #return build_detection_test_loader( cfg,dataset_name,mapper=DatasetMapper(cfg,is_train=False,))\n        return build_detection_test_loader( cfg,dataset_name,mapper=PersonalMapper(cfg,is_train=False,augmentations=[]))\n\n    \n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name):\n        return COCOEvaluator(dataset_name, (\"bbox\",), False, output_dir=None)","e912cf82":"@inject_config\ndef get_config(config,fold=0):\n    \"\"\"\n    Detectron2 config\n    \"\"\"\n    steps_per_epoch=annot_df.shape[0]\/\/config.model[\"images_per_batch\"]\n    train_dataset_name=f\"my_dataset_train_{fold}\"\n    test_dataset_name=f\"my_dataset_test_{fold}\"\n    cfg = get_cfg()\n    cfg.MODEL.DEVICE='cuda' if torch.cuda.is_available() else 'cpu'\n    cfg.merge_from_file(model_zoo.get_config_file(f\"COCO-Detection\/{config.model['model_name']}.yaml\"))\n    cfg.DATASETS.TRAIN = (train_dataset_name,)\n    cfg.DATASETS.TEST = (test_dataset_name,)\n    cfg.DATALOADER.NUM_WORKERS = 4\n    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(f\"COCO-Detection\/{config.model['model_name']}.yaml\")  # Let training initialize from model zoo\n    cfg.SOLVER.IMS_PER_BATCH = config.model[\"images_per_batch\"]\n    cfg.SOLVER.BASE_LR = config.model[\"base_lr\"]  # pick a good LR\n    cfg.SOLVER.MAX_ITER = steps_per_epoch*config.model[\"epochs\"]  # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n    cfg.SOLVER.STEPS = (steps_per_epoch*8,)\n    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = config.model[\"batchsize_per_image\"]   # faster, and good enough for this toy dataset (default: 512)\n    cfg.MODEL.ROI_HEADS.NUM_CLASSES = config.model[\"num_classes\"]  # only has one class (ballon). (see https:\/\/detectron2.readthedocs.io\/tutorials\/datasets.html#update-the-config-for-new-datasets)\n    cfg.TEST.EVAL_PERIOD=0\n    cfg.OUTPUT_DIR = LOGS_PATH\n    cfg.OUTPUT_DIR_BEST = LOGS_PATH\n    cfg.SOLVER.AMP.ENABLED = True\n    cfg.MODEL.WEIGHTS = \"..\/input\/trash-detection-part-1\/models\/best_model.pth\"\n\n    cfg.SEED = config.general[\"seed\"]\n\n    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n    os.makedirs(cfg.OUTPUT_DIR_BEST, exist_ok=True)\n    return cfg\n","34306b43":"cfg=get_config()","7fea25d6":"def train(fold):\n    \"\"\"\n    train function that help train on the dataset and validate on a certain fold\n    \"\"\"\n    seed_all()\n    register_dataset(fold)\n    cfg=get_config(fold)\n    #trainer = DefaultTrainer(cfg)\n    trainer = PersonalTrainer(cfg) \n    trainer.resume_or_load(resume=False)\n    trainer.evaluator = COCOEvaluator(f\"my_dataset_test_{fold}\", (\"bbox\",), False, output_dir=None)\n    trainer.train()\n    ","000a99b8":"train(0)","c6ab2bb4":"with open(os.path.join(CONFIG_PATH,\"detectron_config.yaml\"),\"w\") as f:\n    f.write(get_config().dump())","22f139f7":"%ls logs","4514ed88":"%ls models","c4ebceda":"%ls configs","9fe47996":"metrics={}","b84d9ab9":"cfg=get_config()\ncfg.MODEL.WEIGHTS = os.path.join(MODELS_PATH, \"best_model.pth\")  # path to the model we just trained\nmodel = build_model(cfg)\nm=DetectionCheckpointer(model).load(cfg.MODEL.WEIGHTS)","f165d7ba":"evaluator = COCOEvaluator(\"my_dataset_test_0\", (\"bbox\",), False, output_dir=LOGS_PATH)\nval_loader = build_detection_test_loader(cfg, \"my_dataset_test_0\")\ntrain_metric=inference_on_dataset(model, val_loader, evaluator)\nmetrics[\"train_metric\"]=train_metric","0f2b529f":"evaluator = COCOEvaluator(\"my_dataset_train_0\", (\"bbox\",), False, output_dir=LOGS_PATH)\nval_loader = build_detection_test_loader(cfg, \"my_dataset_train_0\")\nvalid_metric=inference_on_dataset(model, val_loader, evaluator)\nmetrics[\"valid_metric\"]=valid_metric","61a7f3e7":"dump_dict(metrics,os.path.join(LOGS_PATH,\"metrics.yaml\"))","a5ba6d3c":"# Preprocess and augmentations ","9579afd0":"# Test Dataset (without validation augmentation)","65095dbc":"## Preprocess","16ed3194":"## Categories and Super categories dict","19474641":"# Kfold","9f57fbcf":"## delete negative BBOX","7e4c3a73":"# ENV","932753fc":"# Prepare Output","73dd51e0":"### Choose between normal categories or super categories","940c3f2d":"# Annotation Preprocess and Visualization","03b5ab00":"# Build Trainer","00c41c9f":"# FIX annotation duplicated ids and negative bboxes\n\nrepeated annotations idx \n\n308 => 0\n\n4039  =>2197\n","81a4d8f4":"# Config","d5c03ed5":"# Prepare config params","6e86bc12":"## Visualization","f999b6ff":"# Imports and utils","39edd42d":"# Prepare trainer","9e309412":"# Register Dataset","9ec86729":"# SEED","f3b8d046":"# Personal mapper"}}