{"cell_type":{"d25bebdf":"code","f8c96fc4":"code","8c1dd8e9":"code","7c37fc27":"code","5c03f957":"code","48b50d55":"code","f243702a":"code","8e21824a":"code","3506619e":"code","30614bf4":"code","0fbb11b6":"code","524a6148":"code","28b7717f":"code","092f91d6":"code","0941e61b":"code","a8b2e09d":"code","a13f7489":"code","288c8274":"code","114d7007":"code","3d3434c4":"code","7a34bc43":"code","786166e2":"code","1b1d1c27":"code","1c31dadb":"code","78000f37":"code","f0c5b301":"code","3f123e66":"code","8a0ddf22":"code","576a0785":"code","e9d5cddd":"code","d6eee087":"code","83f51b4f":"code","e3b8aa33":"code","2277470d":"code","9369f6e5":"markdown","f1c18ff8":"markdown","bf4e5a8c":"markdown","d2a3a05e":"markdown","2ae9c153":"markdown","96168754":"markdown","a6bddd3e":"markdown","146a72c6":"markdown","5e44de0d":"markdown","04c713d7":"markdown","7e2c9223":"markdown","974e75ff":"markdown","a2e498af":"markdown","e824125a":"markdown","829561c3":"markdown","cb97e95b":"markdown","d3f46425":"markdown"},"source":{"d25bebdf":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\ntrain_df = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/train.csv\")\nlabels, frequencies = np.unique(train_df.language.values, return_counts=True)\nplt.figure(figsize = (10,10))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()","f8c96fc4":"from IPython import display\ndisplay.Image(\"..\/input\/figures\/data_augmentation_workflow.png\")","8c1dd8e9":"from sklearn.model_selection import train_test_split\n\n# Create train-val split data\nprint (\"Creating training and validation split csv files...\")\n# Stratify ensures that each sub-set contains approximately the same percentage of samples of each target class as the original set.\ntrain_df, validation_df = train_test_split(train_df, stratify=train_df.label.values, \n                                                      random_state=42, \n                                                      test_size=0.20, shuffle=True)\n\n\ntrain_df.reset_index(drop=True, inplace=True)\nvalidation_df.reset_index(drop=True, inplace=True)\n    \n# check the number of rows and columns in the subsets after split\nprint(\"Training data shape after split: {}\".format(train_df.shape))\nprint(\"Validation data shape after split: {}\".format(validation_df.shape))","7c37fc27":"# install the latest version of the following libraries\n!pip install --upgrade pip\n!pip install --upgrade allennlp\n!pip install transformers==4.3.0\n!pip install datasets #to load xnli dataset from huggingface library\n!pip install googletrans==3.1.0a0","5c03f957":"# import libraries\nimport os\nfrom transformers import AutoTokenizer, AutoConfig, TFAutoModel    \nfrom transformers import (XLMRobertaConfig, XLMRobertaTokenizer, TFXLMRobertaModel)            \nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport os.path\nfrom os import path\nfrom tensorflow.keras.layers import Input, Dropout, Dense\nfrom sklearn.model_selection import train_test_split\nfrom datasets import load_dataset, list_datasets\nfrom tqdm import tqdm\nfrom googletrans import Translator\nimport time\nimport glob\nimport seaborn as sns\n\nos.environ[\"WANDB_API_KEY\"] = \"0\" # to silence warning\n\nnp.random.seed(0)","48b50d55":"# configure tpu settings\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\nprint('Number of replicas:', strategy.num_replicas_in_sync)","f243702a":"# Configuration Settings\nEPOCHS = 4\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 120\nPATIENCE = 1\nLEARNING_RATE = 1e-5","8e21824a":"# set up the tokenizer\nPRETRAINED_MODEL_TYPES = {\n    'xlmroberta': (XLMRobertaConfig, TFXLMRobertaModel, XLMRobertaTokenizer, 'jplu\/tf-xlm-roberta-large')\n}\n\nconfig_class, model_class, tokenizer_class, model_name = PRETRAINED_MODEL_TYPES['xlmroberta']\n\n# Download vocabulary from huggingface.co and cache.\n# tokenizer = tokenizer_class.from_pretrained(model_name) \ntokenizer = AutoTokenizer.from_pretrained(model_name) #fast tokenizer\n\ntokenizer","3506619e":"# function to vectorize the input data\ndef encode(df, tokenizer, max_len=50):\n    \n    pairs = df[['premise','hypothesis']].values.tolist() #shape=[num_examples]\n    \n    print (\"Encoding...\")\n    encoded_dict = tokenizer.batch_encode_plus(pairs, max_length=max_len, padding=True, truncation=True, \n                                               add_special_tokens=True, return_attention_mask=True)\n    print (\"Complete\")\n    \n    input_word_ids = tf.convert_to_tensor(encoded_dict['input_ids'], dtype=tf.int32) #shape=[num_examples, max_len])\n    input_mask = tf.convert_to_tensor(encoded_dict['attention_mask'], dtype=tf.int32) #shape=[num_examples, max_len]\n    \n    inputs = {\n        'input_word_ids': input_word_ids,\n        'input_mask': input_mask}    \n    \n    return inputs","30614bf4":"# process input data\ntrain_input = encode(train_df, tokenizer=tokenizer, max_len=MAX_LEN)\nvalidation_input = encode(validation_df, tokenizer=tokenizer, max_len=MAX_LEN)","0fbb11b6":"# Instantiate a XLM-R model\ndef build_model(max_len=50):\n    \n    tf.random.set_seed(12345) # For reproducibility\n    \n    # The bare XLM-RoBERTa Model transformer outputting raw hidden-states without any specific head on top.\n    base_model = model_class.from_pretrained(model_name)\n#     base_model = TFAutoModel.from_pretrained(model_name)\n    \n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    \n    ### pooler_output contains the hidden representation of just the \u2018[CLS]\u2019 token after additionally being passed to a fully connected layer with tanh activation function.\n    output = base_model([input_word_ids, input_mask]) # output from xlmroberta model\n    sequence_output = output.pooler_output #shape: [batch_size, output_size]\n   \n    # Add a classification layer\n    dense = tf.keras.layers.Dense(3, activation=\"softmax\")(sequence_output)  \n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=dense)\n    model.compile(tf.keras.optimizers.Adam(lr=LEARNING_RATE), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith strategy.scope():\n    model = build_model(MAX_LEN)\n    model.summary()","524a6148":"# Fit the model to original train data\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\ncallbacks = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=PATIENCE)]\n\ntrain_history = model.fit(x=train_input, y=train_df.label.values, validation_data=(validation_input, validation_df.label.values), epochs=EPOCHS, verbose=1, batch_size=BATCH_SIZE, callbacks=callbacks)","28b7717f":"# save validation predictions to compare the results later\nval_predictions = [np.argmax(i) for i in model.predict(validation_input)]\nvalidation_df['prediction'] = val_predictions\nvalidation_df.to_csv(\"validation_predictions_original.csv\", index=False)","092f91d6":"del model #to free up space","0941e61b":"# Resets all state generated by Keras\nK.clear_session()","a8b2e09d":"# Hyper-parameter Settings\nLOAD_XNLI = True #use auxiliary XNLI data\nBACK_TRANSLATE = True #enable data augmentation\n# directory containing the back-translations of input training data\nBT_DIR = '..\/input\/backtranslations-for-data-augmentation-in-nlp'","a13f7489":"# function to back-translate data examples\ndef back_translate(train_df, target_lang='fr', sample=True, num_samples_per_lang=1000):\n    if sample: #sample input training data to back translate\n        train_df = train_df.groupby('language', group_keys=False).apply(lambda x: x.sample(min(len(x), num_samples_per_lang))).reset_index(drop=True)  \n\n    df_list = []\n    limit_before_timeout = 100\n    timeout = 5\n    \n    translator = Translator() \n    \n    # Add functions to back translate input sentences\n    def target_translate(x, target_lang):\n        translation = translator.translate(x, dest=target_lang)\n        return translation.text\n    def source_translate(x, source_lang):\n        translation = translator.translate(x, dest=source_lang) \n        return translation.text \n    \n    for i in tqdm(range(len(train_df))):\n        entry = train_df.loc[[i]]\n        source_lang = entry.lang_abv.values.tolist()[0]\n        if source_lang == 'zh':\n            #print(googletrans.LANGUAGES) \n            source_lang = 'zh-cn' #'zh' not in googletrans.LANGUAGES        \n        if (i!=0) and (i%limit_before_timeout == 0): #apply timeout after every 100 iterations \n            print('Iteration {} of {}'.format(i, len(train_df)))\n            time.sleep(timeout)      \n        # Back translate premise sentence\n        entry['premise'] = entry['premise'].apply(lambda x: target_translate(x, target_lang))\n#         time.sleep(0.2)\n        entry['premise'] = entry['premise'].apply(lambda x: source_translate(x, source_lang))\n#         time.sleep(0.2)       \n        # Back translate hypothesis sentence\n        entry['hypothesis'] = entry['hypothesis'].apply(lambda x: target_translate(x, target_lang))\n#         time.sleep(0.2)\n        entry['hypothesis'] = entry['hypothesis'].apply(lambda x: source_translate(x, source_lang))\n#         time.sleep(0.2)\n        df_list.append(entry)\n    \n    train_bt = pd.concat(df_list, ignore_index=True)\n    print(\"Shape of back-translated training data: {}\".format(train_bt.shape))\n    return train_bt","288c8274":"# function of process XNLI data\ndef process_xnli_data(all_keys=False, only_train=False): \n    if only_train:\n        print (\"Splitting by machine-translated train data only\")\n        split = 'train'\n    elif all_keys:\n        print (\"Splitting by all keys\")\n        split = 'validation+test+train[:5%]'\n    else:\n        print (\"Splitting by human-catered validation and test data\")\n        split = 'validation+test'\n    \n    print(\"Loading XNLI data...\")\n    print(\"Split: \", split)\n    \n    dataset = load_dataset('xnli', 'all_languages', split=split) #returns a Dataset object  \n    print(dataset)\n    \n    entries = []   \n    for entry in tqdm(dataset): \n        hypothesis_langs = entry['hypothesis']['language'] #list of 15 lang string values\n        hypothesis_values = entry['hypothesis']['translation'] #list of 15 hypothesis string values\n\n        premise_langs = list(entry['premise'].keys()) #list of 15 lang string values\n        premise_values = list(entry['premise'].values()) #list of 15 premise string values\n\n        labels = [entry['label']]*len(hypothesis_langs) #all 15 languages for the same example have same label \n\n        if premise_langs == hypothesis_langs: #the languages in premise and hypothesis are in same order\n            values = list(zip(premise_values, hypothesis_values, hypothesis_langs, hypothesis_langs, labels))\n            entries += values\n\n    xnli_df = pd.DataFrame(entries, columns=['premise', 'hypothesis', 'lang_abv', 'language', 'label']) #create dataframe for each key\n    \n    xnli_df['language'].replace({\"en\": \"English\", \"ar\": \"Arabic\", \"sw\": \"Swahili\", \"th\": \"Thai\", \"vi\": \"Vietnamese\", \"es\": \"Spanish\", \"bg\": \"Bulgarian\", \"zh\": \"Chinese\", \"ur\": \"Urdu\", \"ru\": \"Russian\", \"hi\": \"Hindi\", \"fr\": \"French\", \"tr\": \"Turkish\", \"el\": \"Greek\", \"de\": \"German\"}, inplace=True)\n\n    xnli_df['id'] = xnli_df.index + 1\n    column_names = ['id', 'premise', 'hypothesis', 'lang_abv', 'language', 'label']\n    xnli_df = xnli_df.reindex(columns=column_names)\n    \n    # Get the number of missing data points per column\n    missing_values_xnli = xnli_df.isnull().sum() \n\n    print(\"Number of missing data points per column in XNLI corpus:\")\n    print (missing_values_xnli)\n\n    # Drop the missing value rows\n    xnli_df.dropna(axis=0, inplace=True)\n#     print(\"Total number of data examples in XNLI corpus after dropping NA values: {}\".format(xnli_df.shape[0]))\n    \n    print(\"XNLI corpus shape: {}\".format(xnli_df.shape))\n    \n    del dataset #free up space\n    \n    return xnli_df","114d7007":"# function for adding augmented and auxiliary train data\ndef augment_data(train_df, use_xnli=True, use_bt=True, bt_dir=''):\n    df_list = []  \n    if use_bt: #use back-translation\n        if path.isdir(bt_dir):\n            files = glob.glob(bt_dir+'\/*.csv')\n            bt_list = []\n            for filename in files:\n                bt_list.append(pd.read_csv(filename))\n            bt_df = pd.concat(bt_list, ignore_index=True)\n#             bt_df.isnull().sum() # we get the number of missing values\n            bt_df.dropna(inplace=True) #remove missing values\n            bt_df.drop_duplicates(inplace=True) #drop duplicate rows  \n            bt_df = bt_df.sample(frac=1) #randomly select n examples as back-translated data \n            print(\"Shape of back-translated training data: {}\".format(bt_df.shape))\n            bt_df.to_csv('back_translation_all.csv', index=False)\n        else:\n            bt_df = back_translate(df)\n        df_list.append(bt_df)\n        del bt_df #free up space \n    if use_xnli: #use auxiliary data\n        xnli_df = process_xnli_data()\n        df_list.append(xnli_df)\n        del xnli_df #free up space \n        \n    if len(df_list) > 0: #augment data\n        augmented_df = pd.concat(df_list, ignore_index=True)\n        train_df = train_df.append(augmented_df, ignore_index=True) \n        \n    train_df = train_df.sample(frac=1).reset_index(drop=True) #shuffle data\n    train_df.drop_duplicates(inplace=True) #drop duplicate rows\n    return train_df\n\n\naugmented_df = augment_data(train_df, LOAD_XNLI, BACK_TRANSLATE, BT_DIR) \naugmented_df.head(100)","3d3434c4":"# check the number of rows and columns in the augmented train data\nprint(\"Augmented train data shape: {}\".format(augmented_df.shape))","7a34bc43":"orig_lang_dist = train_df.language.value_counts(normalize=True).sort_index()\naug_lang_dist = augmented_df.language.value_counts(normalize=True).sort_index()\n\n# fig = plt.figure() # Create matplotlib figure\nfig = plt.figure(figsize = (10,10))\nax = fig.add_subplot(111) # Create matplotlib axes\nwidth = 0.4\norig_lang_dist.plot(kind='bar', color='blue', ax=ax, width=width, position=1, label='Original Train Data')\naug_lang_dist.plot(kind='bar', color='green', ax=ax, width=width, position=0, label='Augmented Train Data')\nax.set_xlabel('Language')\nax.set_ylabel('Train Data Coverage')\nplt.legend(loc=\"upper right\")\nplt.tight_layout()\nplt.savefig('data_coverage_comparison_bt.png')\nplt.show()","786166e2":"# process the new augmented train data\ntrain_input = encode(augmented_df, tokenizer=tokenizer, max_len=MAX_LEN)\ntrain_input","1b1d1c27":"# instantiate a new XLM-R model\nwith strategy.scope():\n    model = build_model(MAX_LEN)\n    model.summary()","1c31dadb":"# fine-tune model with augmented train data\ncheckpoint_filepath='best_checkpoint.hdf5' #save the best checkpoint\ncallbacks = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=PATIENCE), ModelCheckpoint(filepath=checkpoint_filepath, save_best_only=True, save_weights_only=True, monitor='val_loss', mode='min', verbose=1)]\ntrain_history = model.fit(x=train_input, y=augmented_df.label.values, validation_data=(validation_input, validation_df.label.values), epochs=EPOCHS, verbose=1, batch_size=BATCH_SIZE, callbacks=callbacks)","78000f37":"# save validation predictions to compare results later\nval_predictions = [np.argmax(i) for i in model.predict(validation_input)]\nvalidation_df['prediction'] = val_predictions\nvalidation_df.to_csv(\"validation_predictions_augmented.csv\", index=False)","f0c5b301":"# function to calculate accuracy per language\ndef accuracy(x):\n    return round(float(x[2]\/x[1]), 2)*100\n\ndef calculate(file):\n    validation = pd.read_csv(file)  \n    # Calculate the total number of examples per language\n    lang_counts = validation.language.value_counts().sort_index()\n\n    # Calculate the number of correct predictions per language\n    tp_per_lang = validation[validation['label'] == validation['prediction']].groupby('language').agg({'language': ['count']}).sort_index()\n\n    lang_names = lang_counts.index.tolist()\n    lang_tuples = list(zip(lang_names, lang_counts.values.tolist(), tp_per_lang.iloc[:, 0].values.tolist()))\n    acc = map(accuracy, lang_tuples)\n    acc_list = []\n    lang_list = []\n    for i, score in enumerate(acc):\n        acc_list.append(score)\n        lang_list.append(lang_tuples[i][0])\n#         print (\"Accuracy of {} is {} \".format(lang_tuples[i][0], score))\n    df = pd.DataFrame({'language': lang_list, 'validation_accuracy': acc_list})\n    return df\n\n\nval_df_original = calculate(\".\/validation_predictions_original.csv\")\nval_df_augmented = calculate(\".\/validation_predictions_augmented.csv\")","3f123e66":"DF1 = val_df_original\nDF2 = val_df_augmented\n\nDF = pd.concat([DF1, DF2])\nDF['model'] = ['Baseline']*15 + ['Baseline+BT+XNLI']*15\n\n# g = sns.factorplot(data=DF, x='language', y='validation_accuracy', hue='model', kind=\"bar\")\ng = sns.factorplot(data=DF, x='language', y='validation_accuracy', hue='model')\ng.fig.set_size_inches(10, 10)\ng.set_xticklabels(DF2.language, rotation=40, ha=\"right\")\nplt.xlabel(\"Language\", size=12)\nplt.ylabel(\"Score\", size=12)\nplt.savefig('model_comparison_per_lang_barplot.png')\nplt.show()","8a0ddf22":"display.Image(\"..\/input\/figures\/language_family.png\")","576a0785":"DF = pd.concat([DF1, DF2])\nDF['Model'] = ['Baseline']*15 + ['Baseline+BT+XNLI']*15\n\n# create a list of our conditions to assign language families\nconditions = [\n    (DF['language'] == 'Arabic'),\n    (DF['language'] == 'Bulgarian') | (DF['language'] == 'Russian'),\n    (DF['language'] == 'German') | (DF['language'] == 'English'),\n    (DF['language'] == 'Greek'),\n    (DF['language'] == 'Spanish') | (DF['language'] == 'French'),\n    (DF['language'] == 'Hindi') | (DF['language'] == 'Urdu'),\n    (DF['language'] == 'Chinese'),\n    (DF['language'] == 'Swahili'),\n    (DF['language'] == 'Thai'),\n    (DF['language'] == 'Turkish'),\n    (DF['language'] == 'Vietnamese'),\n    ]\n\n# create a list of the values we want to assign for each condition\nvalues = ['Afro-Asiatic', 'Indo-European: Slavic', 'Indo-European: Germanic', 'Indo-European: Greek', 'Indo-European: Romance', 'Indo-European: Indo-Aryan', 'Sino-Tibetan', 'Niger-Congo', 'Tai-Kadai', 'Turkic', 'Austro-Asiatic']\n\n# create a new column and use np.select to assign values to it using our lists as arguments\nDF['language_family'] = np.select(conditions, values)\n\nx_labels = ['Afro-Asiatic', 'Indo-European: Slavic', 'Sino-Tibetan', 'Indo-European: Germanic', 'Indo-European: Romance', 'Indo-European: Greek', 'Indo-European: Indo-Aryan', 'Niger-Congo', 'Tai-Kadai', 'Turkic', 'Austro-Asiatic']\n\nf, ax = plt.subplots(figsize=(10, 10))\ng = sns.scatterplot(data=DF, x='language_family', y='validation_accuracy', hue='Model', style='Model', palette='dark', hue_order=['Baseline', 'Baseline+BT+XNLI'])\ng.set_xticklabels(x_labels, rotation=40, ha=\"right\")\nplt.xlabel(\"Language Family\", size=12)\nplt.ylabel(\"Score\", size=12)\n# place the legend outside the figure\/plot\nplt.legend(bbox_to_anchor=(1.01, 1),borderaxespad=0)\nplt.tight_layout()\nplt.savefig('model_comparison_valscore_per_lang_family.png')\nplt.show()","e9d5cddd":"# The model weights (that are considered the best) are loaded into the model.\nmodel.load_weights(checkpoint_filepath)","d6eee087":"#encode the test-input sequences\ntest_df = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/test.csv\")\ntest_input = encode(test_df, tokenizer=tokenizer, max_len=MAX_LEN)","83f51b4f":"predictions = [np.argmax(i) for i in model.predict(test_input)]","e3b8aa33":"submission = test_df.id.copy().to_frame()\nsubmission['prediction'] = predictions\n\nsubmission.head()","2277470d":"submission.to_csv(\"submission.csv\", index = False)","9369f6e5":"We also analyze the performance of the models based on different language families. The table below shows the 15 languages grouped by language families and the next figure plots the results. \n\nWe observe that some branches of the Indo-European language family like Romance, Germanic, and Slavic perform fairly well across all the models. Overall, the difference in performance is the highest between the Indo-European language families and low-resource language families like Niger-Congo and Tai-Kadai. The performance of these low-resource language families improves by a large margin once the original train data is extended by back-translations and the XNLI corpus and the final best model yields at least 90% accuracy for most of the under-represented languages.","f1c18ff8":"# Data Augmentation:\nThe amount of labeled data available to train a machine learning model might impact the model\u2019s performance. This is especially true in case of deep learning-based NLP models that generally benefit from larger amounts of annotated training examples to be able to distinguish between the different output classes. However it can be an expensive and time-consuming process to manually annotate additional data. To increase the number of training examples in low-resource languages, data augmentation, in the form of back-translations, is used to generate additional, synthetic data using the original train data. \n\nIn back-translation, the input text data is translated to some language and then translated back to the original language. This can help to generate textual data with different words while preserving the context of the input text data.\n","bf4e5a8c":"# Fine-tune the Model with Back-Translations and XNLI data","d2a3a05e":"We will firslty implement the baseline model using XLM-RoBERTa with the original train data. Then the train data will be augmented with back-translated data and the model will be fine-tuned on this new train data. ","2ae9c153":"# Generate Predictions on Test Data and Submit","96168754":"As majority of the languages are under represented in the original train data, this might affect the classification performance per language. To alleviate data scarcity in these languages, an augmented dataset is generated before training the models, concatenated with the original train subset, and later fed into data loaders to train the model. \n\nThe data augmentation process is depicted in the flowchart below. ","a6bddd3e":"# Problem\nIn the [Contradictory, My Dear Watson](https:\/\/www.kaggle.com\/c\/contradictory-my-dear-watson\/overview) competition, the task is to build a system that automatically classifies how pairs of sentences are related from texts in 15 different languages. \n\nLet's look at the distribution of examples across all the languages in the original train data.","146a72c6":"# Visualize and Compare Results","5e44de0d":"Now, we will generate back-translations of the original train data and re-train a XLM-R model with the augmented and auxiliary data. I already generated the back-translations with a Google Translate API and saved it in a csv file, since it takes some time to get the back-translations. You can access the dataset from [here](https:\/\/www.kaggle.com\/wchowdhu\/backtranslations-for-data-augmentation-in-nlp). If you want to generate new back-translations, set `BT_FILE = ''`.","04c713d7":"From the above plot we observe that more than half of the training examples are in English, as data resources are abundant in this language. Rest of the data is fairly shared between other 14 languages.\n\n","7e2c9223":"The goal of this notebook is to use data augmentation and auxiliary data to improve the textual entailment classification across resource constrained languages. \n\nFor a beginner's tutorial on implementing baseline model for textual entailment recognition, you can check this [notebook](https:\/\/www.kaggle.com\/wchowdhu\/hands-on-nli-w-transformers-m-bert-xlm-roberta).\n\nIf you are interested to explore more on this topic, you can look at my [Udacity capstone project](https:\/\/github.com\/wchowdhu\/udacity-capstone-project) in GitHub!","974e75ff":"# Implement Baseline","a2e498af":"That's it! The submission file has been created, for more information on how to submit to the competition, please visit the following [link](https:\/\/www.kaggle.com\/c\/contradictory-my-dear-watson\/overview\/evaluation).\n\n\n\n\n<span style=\"color:blue\">If you find this notebook helpful, please kindly upvote:-)<\/span>","e824125a":"We will be splitting the training dataset into two parts - the data we will train the model with and a validation set. We stratify data during train-valid split to preserve the original distribution of the target classes.","829561c3":"# Split the Training Data","cb97e95b":"To compare the performance of the two models across all the languages, we calculate the number of correct prediction for each language in the validation data. \n\nThe figure below gives an overview of the validation accuracy in percentage across all the languages for each model. Augmenting the original train data with back-translations increases the classification performance for majority of the languages and using an auxiliary XNLI corpus helps to notably reduce the gap in accuracy between the languages. We observe that the scores of the majority of the languages now cluster in a relatively small range. We can see significant gain in low-resource languages like Swahili, Urdu, and Thai. \n","d3f46425":"The chart below compares the distribution of languages in the train data before and after data augmentation. We can see from the chart that adding augmented and auxiliary data greatly reduces the overall train data coverage in English language from ~56% to ~10%, while increasing the number of input samples in all other languages. Hence adding more samples helps to keep a fair balance in the number of samples per language."}}