{"cell_type":{"39ddc10f":"code","bf884c88":"code","a7151a5c":"code","443b1ebc":"code","15c2efd8":"code","3baac9ad":"code","b99cbc25":"code","fa39503b":"code","fa5c9570":"code","9c0863c8":"code","3fbb6a8d":"code","ec982346":"code","a6034d59":"code","ebf1bfb6":"code","a4a6160c":"code","efd00062":"code","bfdbb670":"code","586befcb":"code","9994aebe":"markdown","b3ad6f01":"markdown","a3b45843":"markdown","b9aa8a54":"markdown","aaf47708":"markdown","2e00ff12":"markdown","68b20018":"markdown","44ecaf10":"markdown","0f628f86":"markdown","f43dc5d9":"markdown","060d95e0":"markdown","d39c385b":"markdown","dee2719e":"markdown","fad78ecc":"markdown","83b323be":"markdown"},"source":{"39ddc10f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bf884c88":"!git clone https:\/\/github.com\/Kaggle\/kaggle-environments.git\n!cd kaggle-environments && pip install .\n\n# GFootball environment.\n!apt-get update -y\n!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n\n# Make sure that the Branch in git clone and in wget call matches !!\n!git clone -b v2.8 https:\/\/github.com\/google-research\/football.git\n!mkdir -p football\/third_party\/gfootball_engine\/lib\n\n!wget https:\/\/storage.googleapis.com\/gfootball\/prebuilt_gameplayfootball_v2.8.so -O football\/third_party\/gfootball_engine\/lib\/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .","a7151a5c":"! pip install tensorflow\n! pip install tf-agents\n","443b1ebc":"from enum import Enum\n\nclass Action(Enum):\n    Idle = 0\n    Left = 1\n    TopLeft = 2\n    Top = 3\n    TopRight = 4\n    Right = 5\n    BottomRight = 6\n    Bottom = 7\n    BottomLeft = 8\n    LongPass= 9\n    HighPass = 10\n    ShortPass = 11\n    Shot = 12\n    Sprint = 13\n    ReleaseDirection = 14\n    ReleaseSprint = 15\n    Slide = 16\n    Dribble = 17\n    ReleaseDribble = 18\n    NotUsed = 55\n\n\nsticky_index_to_action = [\n    Action.Left,\n    Action.TopLeft,\n    Action.Top,\n    Action.TopRight,\n    Action.Right,\n    Action.BottomRight,\n    Action.Bottom,\n    Action.BottomLeft,\n    Action.Sprint,\n    Action.Dribble\n]\n\n\nclass PlayerRole(Enum):\n    GoalKeeper = 0\n    CenterBack = 1\n    LeftBack = 2\n    RightBack = 3\n    DefenceMidfield = 4\n    CentralMidfield = 5\n    LeftMidfield = 6\n    RIghtMidfield = 7\n    AttackMidfield = 8\n    CentralFront = 9\n\n\nclass GameMode(Enum):\n    Normal = 0\n    KickOff = 1\n    GoalKick = 2\n    FreeKick = 3\n    Corner = 4\n    ThrowIn = 5\n    Penalty = 6\n\ndef simple_human_policy(obs):\n    # Make sure player is running.\n    if Action.Sprint not in obs['sticky_actions']:\n        return Action.Sprint\n    # We always control left team (observations and actions\n    # are mirrored appropriately by the environment).\n    controlled_player_pos = obs['left_team'][obs['active']]\n    # Does the player we control have the ball?\n    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:\n        # Shot if we are 'close' to the goal (based on 'x' coordinate).\n        if controlled_player_pos[0] > 0.5:\n            return Action.Shot\n        # Run towards the goal otherwise.\n        return Action.Right\n    else:\n        # Run towards the ball.\n        if obs['ball'][0] > controlled_player_pos[0] + 0.05:\n            return Action.Right\n        if obs['ball'][0] < controlled_player_pos[0] - 0.05:\n            return Action.Left\n        if obs['ball'][1] > controlled_player_pos[1] + 0.05:\n            return Action.Bottom\n        if obs['ball'][1] < controlled_player_pos[1] - 0.05:\n            return Action.Top\n        # Try to take over the ball if close to the ball.\n        return Action.Slide\n    \n\ndef human_agent_wrapper(obs):\n    # Extract observations for the first (and only) player we control.\n    obs_hr = obs['players_raw'][0]\n    # Turn 'sticky_actions' into a set of active actions (strongly typed).\n    obs_hr['sticky_actions'] = { sticky_index_to_action[nr] for nr, action in enumerate(obs_hr['sticky_actions']) if action }\n    # Turn 'game_mode' into an enum.\n    obs_hr['game_mode'] = GameMode(obs_hr['game_mode'])\n    return obs_hr\n\n\n#def agent(obs):\n\n#    a =  simple_human_policy(human_agent_wrapper(obs)).value\n    \n#    return [a]\n\n","15c2efd8":"\"\"\"Policy implementation that generates random actions.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\n# Using Type Annotations.\nfrom __future__ import print_function\n\nfrom typing import Optional, Sequence\n\nimport numpy as np\nimport tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\nfrom tf_agents.distributions import masked\nfrom tf_agents.policies import py_policy\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import policy_step\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.typing import types\nfrom tf_agents.utils import nest_utils\nfrom tf_agents.environments import utils\n\n\nclass MyFootbalHumanPyPolicy(py_policy.PyPolicy):\n    \"\"\"Returns random samples of the given action_spec.\"\"\"\n    \n    \n    def __convert_tf_obs_to_numpy(self, obs):\n        \n        rp = {}\n        new_obs = {}\n        for name in obs.keys():\n            rp[name] = obs[name].numpy()[0] if len(obs[name].numpy()[0]) > 1 else obs[name].numpy()[0][0]\n\n        new_obs['players_raw'] = [rp]\n        return new_obs\n\n    \n    def __init__(self,\n               time_step_spec: ts.TimeStep,\n               action_spec: types.NestedArraySpec,\n               info_spec: types.NestedArraySpec = (),\n               seed: Optional[types.Seed] = None,\n               outer_dims: Optional[Sequence[int]] = None,\n               observation_and_action_constraint_splitter: Optional[\n                   types.Splitter] = None):\n        self._seed = seed\n        self._outer_dims = outer_dims\n        \n        self._rng = np.random.RandomState(seed)\n        if time_step_spec is None:\n            time_step_spec = ts.time_step_spec()\n\n        super(MyFootbalHumanPyPolicy, self).__init__(\n            time_step_spec=time_step_spec,\n            action_spec=action_spec,\n            info_spec=info_spec,\n            observation_and_action_constraint_splitter=(\n                observation_and_action_constraint_splitter))\n\n    def _action(self, time_step, policy_state):\n        \n        outer_dims = self._outer_dims\n        \n        if outer_dims is None:\n            if self.time_step_spec.observation:\n                outer_dims = nest_utils.get_outer_array_shape(\n                    time_step.observation, self.time_step_spec.observation)\n            else:\n                outer_dims = ()\n\n        random_action = np.array([simple_human_policy(human_agent_wrapper(self.__convert_tf_obs_to_numpy(time_step.observation))).value])\n\n        info = array_spec.sample_spec_nest(\n            self._info_spec, self._rng, outer_dims=outer_dims)\n\n        return policy_step.PolicyStep(random_action, policy_state, info)\n\n","3baac9ad":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport tensorflow as tf\nimport numpy as np\nfrom tf_agents.environments import py_environment\nfrom tf_agents.environments import tf_environment\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.environments import utils\nfrom tf_agents.specs import array_spec\nfrom tf_agents.environments import wrappers\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import common\n\nimport gfootball\nimport gfootball.env as football_env\n\ndata_dic = {\n    'ball_owned_team':array_spec.BoundedArraySpec(shape=(1,), dtype=np.int32, minimum=0, maximum=2, name='ball_owned_team'),\n    'steps_left':array_spec.BoundedArraySpec(shape=(1,), dtype=np.float32, minimum=0.0, name='steps_left'),\n    'ball_owned_player':array_spec.BoundedArraySpec(shape=(1,), dtype=np.int32, minimum=0, maximum=11, name='ball_owned_player'),\n    'game_mode':array_spec.BoundedArraySpec(shape=(1,), dtype=np.int32, minimum=0, maximum=6, name='game_mode'),\n    'designated':array_spec.BoundedArraySpec(shape=(1,), dtype=np.int32, minimum=-1, maximum=11, name='designated'),\n    'active':array_spec.BoundedArraySpec(shape=(1,), dtype=np.int32, minimum=-1, maximum=11, name='active'),\n    'left_team_active':array_spec.BoundedArraySpec(shape=(11,), dtype=np.int32, minimum=0, maximum=1, name='left_team_active'),\n    'left_team':array_spec.BoundedArraySpec(shape=(11,2), dtype=np.float32, minimum=-1.5, maximum=1.5, name='left_team'),\n    'right_team_active':array_spec.BoundedArraySpec(shape=(11,), dtype=np.int32, minimum=0, maximum=1, name='right_team_active'),\n    'ball_direction':array_spec.BoundedArraySpec(shape=(3,), dtype=np.float32, name='ball_direction'),\n    'ball': array_spec.BoundedArraySpec(shape=(3,), dtype=np.float32, name='ball'),\n    'left_team_tired_factor':array_spec.BoundedArraySpec(shape=(11,), dtype=np.float32, minimum=0.0, name='left_team_tired_factor'),\n    'left_team_direction':array_spec.BoundedArraySpec(shape=(11,2), dtype=np.float32, minimum=-1.5, maximum=1.5, name='left_team_direction'),\n    'score':array_spec.BoundedArraySpec(shape=(2,), dtype=np.float32, minimum=0.0, name='score'),\n    'left_team_roles':array_spec.BoundedArraySpec(shape=(1,), dtype=np.int32, minimum=0, maximum=10, name='left_team_roles'),\n    'right_team_tired_factor':array_spec.BoundedArraySpec(shape=(11,), dtype=np.float32, minimum=0.0, name='right_team_tired_factor'),\n    'right_team':array_spec.BoundedArraySpec(shape=(11,2), dtype=np.float32, minimum=-1.5, maximum=1.5, name='right_team'),\n    'right_team_direction':array_spec.BoundedArraySpec(shape=(11,2), dtype=np.float32, minimum=-1.5, maximum=1.5, name='right_team_direction'),\n    'right_team_yellow_card':array_spec.BoundedArraySpec(shape=(11,), dtype=np.int32, minimum=0, maximum=1, name='right_team_yellow_card'),\n    'right_team_roles':array_spec.BoundedArraySpec(shape=(1,), dtype=np.int32, minimum=0, maximum=10, name='right_team_roles'),\n    'ball_rotation':array_spec.BoundedArraySpec(shape=(3,), dtype=np.float32, name='ball_rotation'),\n    'left_team_yellow_card':array_spec.BoundedArraySpec(shape=(11,), dtype=np.int32, minimum=0, maximum=1, name='left_team_yellow_card'),\n    'sticky_actions':array_spec.BoundedArraySpec(shape=(10,), dtype=np.int32, minimum=0, maximum=1, name='sticky_actions'),\n    }\n\nclass MyFootballEnv(py_environment.PyEnvironment):\n\n    @staticmethod\n    def convert_observation_to_tf(obs, data_cfg, propertie_list = []):\n        \n        if(len(propertie_list) == 0):\n            propertie_list = ['right_team_direction', 'right_team_tired_factor', \n                               'left_team_roles', 'left_team_direction', 'ball_direction', \n                               'ball_owned_player', 'right_team_yellow_card', 'ball', \n                               'right_team', 'steps_left', 'ball_rotation', \n                               'ball_owned_team', 'game_mode', 'left_team_yellow_card', \n                               'left_team', 'right_team_roles', 'right_team_active', \n                               'left_team_active', 'left_team_tired_factor', 'score', \n                               'designated', 'active', 'sticky_actions']\n        rp = {}\n\n        for name in propertie_list:\n            \n            if(('ball_owned_player' == name) and (obs[0][name] == -1)):\n                obs[0][name] = 11\n            if(('ball_owned_team' == name) and (obs[0][name] == -1)):\n                obs[0][name] = 2\n            \n            if('right_team_roles' == name):\n                if((obs[0]['ball_owned_team'] == 1) and ((obs[0]['ball_owned_player'] != -1) or (obs[0]['ball_owned_player'] != 11))):\n                    obs[0][name] = obs[0][name][obs[0]['ball_owned_player']]\n                else:\n                    obs[0][name] = 10\n\n            if('left_team_roles' == name):\n                if((obs[0]['ball_owned_team'] == 0) and ((obs[0]['ball_owned_player'] != -1) or (obs[0]['ball_owned_player'] != 11))):\n                    obs[0][name] = obs[0][name][obs[0]['ball_owned_player']]\n                else:\n                    obs[0][name] = 10\n\n\n            if(data_cfg[name].shape == (1,)):\n                rp[name] = np.array([np.squeeze(obs[0][name])]).astype(data_cfg[name].dtype)\n            else:\n                a = np.zeros(data_cfg[name].shape, dtype=data_cfg[name].dtype)\n                a[:len(obs[0][name])] = obs[0][name]\n                obs[0][name] = a.copy()\n                rp[name] = np.array(obs[0][name]).astype(data_cfg[name].dtype)\n\n        return rp\n\n    @staticmethod\n    def construct_obs_spec(data_cfg, cfg):\n        rp = {}\n        \n        for name in cfg:\n            rp[name] = data_cfg[name]\n        \n        return rp\n        \n    \n    def __init__(self, propertie_list = [], scenario='11_vs_11_kaggle'):\n        super().__init__()\n        self._data_dic = data_dic\n        \n        if(len(propertie_list) == 0):\n            propertie_list = ['right_team_direction', 'right_team_tired_factor', \n                               'left_team_roles', 'left_team_direction', 'ball_direction', \n                               'ball_owned_player', 'right_team_yellow_card', 'ball', \n                               'right_team', 'steps_left', 'ball_rotation', \n                               'ball_owned_team', 'game_mode', 'left_team_yellow_card', \n                               'left_team', 'right_team_roles', 'right_team_active', \n                               'left_team_active', 'left_team_tired_factor', 'score', \n                               'designated', 'active', 'sticky_actions']\n\n        self.propertie_list = propertie_list\n        \n        self.env = football_env.create_environment(\n              env_name=scenario,  \n              stacked=False,\n              representation='raw',  \n              rewards='scoring, checkpoints',\n              write_goal_dumps=False,\n              write_full_episode_dumps=False,\n              render=False,\n              write_video=False,\n              dump_frequency=1,\n              logdir='.\/',\n              extra_players=None,\n              number_of_left_players_agent_controls=1,\n              number_of_right_players_agent_controls=0\n            )\n        \n        \n        self._state = self.convert_observation_to_tf(self.env.reset(), self._data_dic, self.propertie_list)\n\n        self._action_spec = array_spec.BoundedArraySpec(\n            shape=(), dtype=np.int32, minimum=0, maximum=18, name='action')\n        \n        \n        #representation of the enviroment: price + open position state\n        self._observation_spec = self.construct_obs_spec(self._data_dic, self.propertie_list)\n\n        #used for idndication of the end of episode        \n        self._episode_ended = False\n\n        pass\n       \n    def action_spec(self):\n        return self._action_spec\n\n    def observation_spec(self):\n        return self._observation_spec\n\n    def _reset(self):\n        self._episode_ended = False\n        self._state = self.convert_observation_to_tf(self.env.reset(), self._data_dic, self.propertie_list)\n        return ts.restart(self._state)\n\n\n    def _step(self, action):\n\n        if self._episode_ended:\n            # The last action ended the episode. Ignore the current action and start\n            # a new episode.\n            return self.reset()\n        \n        self._state, reward, self._episode_ended, info = self.env.step(action)\n        self._state = self.convert_observation_to_tf(self._state, self._data_dic, self.propertie_list)\n\n        if self._episode_ended:\n            return ts.termination(self._state, reward)\n        else:\n            return ts.transition(self._state, reward=reward, discount=1.0)\n\n","b99cbc25":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport tensorflow as tf\nimport numpy as np\n\nfrom tf_agents.environments import py_environment\nfrom tf_agents.environments import tf_environment\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.environments import utils\nfrom tf_agents.specs import array_spec\nfrom tf_agents.environments import wrappers\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.trajectories import time_step as ts\n\nfrom pathlib import Path\nimport os\n\nfrom tf_agents.drivers import dynamic_episode_driver\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import common\nfrom tf_agents.policies import policy_saver\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.networks import q_network\n\nfrom tf_agents.metrics import tf_metrics\n\nimport gym\nimport gfootball\n\ntf.compat.v1.enable_v2_behavior()\ntf.random.set_seed(12)\ntf.print(tf.config.list_physical_devices('GPU'))","fa39503b":"cfg = ['ball_owned_team', \n       'steps_left', \n       'ball_owned_player', \n       'game_mode', \n       #'designated', \n       'active', \n       #'left_team_active', \n       'left_team', \n       #'right_team_active', \n       'ball_direction', \n       'ball', \n       #'left_team_tired_factor', \n       'left_team_direction', \n       'score', \n       #'left_team_roles', \n       #'right_team_tired_factor', \n       'right_team', \n       'right_team_direction', \n       #'right_team_yellow_card', \n       #'right_team_roles', \n       'ball_rotation', \n       #'left_team_yellow_card', \n       'sticky_actions']","fa5c9570":"# create test and train environment\ntrain_env = tf_py_environment.TFPyEnvironment(MyFootballEnv(cfg, scenario='1_vs_1_easy'))\ntf_env_eval = tf_py_environment.TFPyEnvironment(MyFootballEnv(cfg, scenario='1_vs_1_easy'))","9c0863c8":"# helper class for learning agent\nimport time\nclass learningHelper:\n    def __init__(self, train_env, test_env, agent, global_step, chkpdir='.\/',\n        num_iterations=20000, collect_episodes=100, collect_steps_per_iteration=2,\n        replay_buffer_capacity=20000, batch_size=64, log_interval=500, \n        num_eval_episodes=10, eval_interval = 5000, IsAutoStoreCheckpoint = True, collect_policy = None):\n\n        if collect_policy is None:\n            self.collect_policy = self.agent.collect_policy\n            print('selected agent collect_policy')\n        else:\n            self.collect_policy = collect_policy\n            print('selected USER collect_policy')\n        \n        tf.compat.v1.enable_v2_behavior()\n        \n        self.IsAutoStoreCheckpoint = IsAutoStoreCheckpoint\n        self.num_iterations = num_iterations\n        self.collect_episodes = collect_episodes\n        self.collect_steps_per_iteration = collect_steps_per_iteration\n        self.replay_buffer_capacity = replay_buffer_capacity\n\n        self.batch_size = batch_size\n        self.log_interval = log_interval\n\n        self.num_eval_episodes = num_eval_episodes\n        self.eval_interval = eval_interval\n        \n        self.agent = agent\n\n        self.train_env = train_env\n        self.test_env = test_env\n\n        self.global_step = global_step\n\n        #create reply buffer for collection trajactories\n        self.replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n            self.agent.collect_data_spec,\n            batch_size=self.train_env.batch_size,\n            max_length=self.replay_buffer_capacity)\n\n        #Checkpointer\n        self.checkpoint_dir = os.path.join(chkpdir, 'checkpoint')\n        Path(self.checkpoint_dir).mkdir(parents=True, exist_ok=True)\n        self.policy_dir = os.path.join(chkpdir, 'policy')\n        Path(self.policy_dir).mkdir(parents=True, exist_ok=True)\n\n        self.train_checkpointer = common.Checkpointer(\n            ckpt_dir=self.checkpoint_dir,\n            max_to_keep=1,\n            agent=self.agent,\n            policy=self.agent.policy,\n            replay_buffer=self.replay_buffer,\n            global_step=self.global_step\n            )\n\n        self.tf_policy_saver = policy_saver.PolicySaver(self.agent.policy)\n        \n        self.local_step_counter = 0\n\n        pass\n\n    def evaluate_agent(self, n_episodes=100):\n        #define metrics\n        num_episodes = tf_metrics.NumberOfEpisodes()\n        env_steps = tf_metrics.EnvironmentSteps()\n        average_return = tf_metrics.AverageReturnMetric()\n        #rew = TFSumOfRewards()\n\n        #add reply buffer and metrict to the observer\n        observers = [num_episodes, env_steps, average_return ]\n\n        _driver = dynamic_episode_driver.DynamicEpisodeDriver(self.test_env, self.agent.policy, observers, num_episodes=n_episodes)\n\n        final_time_step, _ = _driver.run()\n\n        print('eval episodes = {0}: Average Return = {1}'.format(num_episodes.result().numpy(), average_return.result().numpy()))\n        return average_return.result().numpy()\n\n    def collect_training_data(self, verbose=0):\n\n        if(verbose > 0):\n            #define metrics\n            num_episodes = tf_metrics.NumberOfEpisodes()\n            env_steps = tf_metrics.EnvironmentSteps()\n            #add reply buffer and metrict to the observer\n            observers = [self.replay_buffer.add_batch, num_episodes, env_steps]\n        else:\n            observers = [self.replay_buffer.add_batch]\n\n        self.replay_buffer.clear()\n        #create a driver\n        #we can create a driver using e.g. random policy\n        driver = dynamic_episode_driver.DynamicEpisodeDriver(\n            self.train_env, self.collect_policy, observers, num_episodes=self.collect_episodes)\n\n        #collect_steps_per_iteration = 2\n        #driver = dynamic_step_driver.DynamicStepDriver(\n        #    train_env, tf_policy, observers, num_steps=collect_steps_per_iteration)\n\n        # Initial driver.run will reset the environment and initialize the policy.\n        final_time_step, policy_state = driver.run()\n        if(verbose > 0):\n            #print('final_time_step', final_time_step)\n            print('Number of Steps: ', env_steps.result().numpy())\n            print('Number of Episodes: ', num_episodes.result().numpy())\n\n        pass \n\n    def train_step(self, n_steps):\n        # Convert the replay buffer to a tf.data.Dataset \n        # Dataset generates trajectories with shape [Bx2x...]\n        AUTOTUNE = tf.data.experimental.AUTOTUNE\n        dataset = self.replay_buffer.as_dataset(\n            num_parallel_calls=AUTOTUNE, \n            sample_batch_size=self.batch_size, \n            num_steps=2).prefetch(AUTOTUNE)\n\n        iterator = iter(dataset)\n\n        train_loss = None\n        #experience = self.replay_buffer.gather_all()\n        #train_loss = self.agent.train(experience) \n        for _ in range(n_steps):\n            # Sample a batch of data from the buffer and update the agent's network.\n            experience, unused_info = next(iterator)\n            train_loss = self.agent.train(experience)            \n        \n        print('Global steps {}: Traning Loss {}'.format(self.global_step.numpy(), train_loss.loss))\n\n    def train_agent(self, n_epoch):\n        local_epoch_counter = 0\n        for i in range(n_epoch):\n            start_time = time.time()\n            self.collect_training_data(verbose=0)\n            #print('num_frames()', self.replay_buffer.num_frames().numpy())\n            #print('n_steps()', int(self.replay_buffer.num_frames().numpy()\/self.batch_size))\n            self.train_step(self.replay_buffer.num_frames().numpy())\n            \n            if(self.IsAutoStoreCheckpoint == True):\n                self.store_check_point()\n            epoch_train_time = time.time() - start_time\n            local_epoch_counter = local_epoch_counter + 1\n            print('Epoch: {}, epoch train time: {}'.format( local_epoch_counter, epoch_train_time ))\n        pass\n\n    def train_agent_with_avg_ret_condition(self, max_steps, min_avg_return, n_eval_steps=100):\n        for i in range(max_steps):\n            self.collect_training_data()\n            self.train_step(1000)\n            if(self.IsAutoStoreCheckpoint == True):\n                self.store_check_point()\n            \n            if ((i>0) and (i % self.eval_interval) == 0):\n                avg_ret = self.evaluate_agent(n_eval_steps)\n                if(avg_ret > min_avg_return):\n                    return\n        pass\n\n    def get_agent(self):\n        return self.agent\n\n    def store_check_point(self):\n        self.train_checkpointer.save(self.global_step)\n        pass\n    def restore_check_point(self):\n        self.train_checkpointer.initialize_or_restore()\n        self.global_step = tf.compat.v1.train.get_global_step()\n        pass\n    def save_policy(self):\n        self.tf_policy_saver.save(self.policy_dir)\n        pass","3fbb6a8d":"fc_layer_params = (100,50)\n\npreprocessing_layers_d = {\n    'left_team_yellow_card': tf.keras.layers.Dense(11),\n    'left_team_roles': tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=11),#tf.keras.models.Sequential([tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=10),\n                        #                           tf.keras.layers.Flatten()]),\n\n    'ball_direction': tf.keras.layers.Dense(3),\n    'left_team_tired_factor': tf.keras.layers.Dense(11),\n    'left_team_active': tf.keras.layers.Dense(11),\n    'right_team_tired_factor': tf.keras.layers.Dense(11),\n    'ball': tf.keras.layers.Dense(3),\n    'ball_owned_player': tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=12),#tf.keras.models.Sequential([tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=12),\n                                                   #tf.keras.layers.Flatten()]),\n    'ball_rotation': tf.keras.layers.Dense(1),\n    'right_team_active': tf.keras.layers.Dense(11), \n    'game_mode': tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=7),#tf.keras.models.Sequential([tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=7),\n                                                   #tf.keras.layers.Flatten()]),\n    'steps_left': tf.keras.layers.Dense(1),\n    'right_team': tf.keras.layers.Flatten(),\n    'right_team_yellow_card': tf.keras.layers.Dense(11),\n    'left_team': tf.keras.layers.Flatten(),\n    'ball_owned_team': tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=3),#tf.keras.models.Sequential([tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=3),\n                                                  #tf.keras.layers.Flatten()]),\n    'score': tf.keras.layers.Dense(2),\n    'right_team_roles': tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=11), #tf.keras.models.Sequential([tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=12),\n                                                   #tf.keras.layers.Flatten()]),\n    'right_team_direction': tf.keras.layers.Flatten(),\n    'left_team_direction': tf.keras.layers.Flatten(),\n    'designated': tf.keras.layers.Dense(1),\n    'active': tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=12),#tf.keras.models.Sequential([tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens=12),\n                                                   #tf.keras.layers.Flatten()]),\n    'sticky_actions': tf.keras.layers.Dense(10),\n    }\n\npreprocessing_layers = {}\nfor k in cfg:\n    preprocessing_layers[k] = preprocessing_layers_d[k]\n\npreprocessing_combiner = tf.keras.layers.Concatenate(axis=-1)\n\nq_net = q_network.QNetwork(\n    train_env.observation_spec(),\n    train_env.action_spec(),\n    preprocessing_layers=preprocessing_layers,\n    preprocessing_combiner=preprocessing_combiner,\n    fc_layer_params=fc_layer_params)","ec982346":"learning_rate = 1e-3  \n\n#create optimizer\noptimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n\n#create a global step coubter\n#train_step_counter = tf.Variable(0)\nglobal_step = tf.compat.v1.train.get_or_create_global_step()\n\n#create agent\nagent = dqn_agent.DqnAgent(\n    train_env.time_step_spec(),\n    train_env.action_spec(),\n    q_network=q_net,\n    optimizer=optimizer,\n    td_errors_loss_fn=common.element_wise_squared_loss,\n    train_step_counter=global_step,\n    target_update_period = 10\n)\n\nagent.initialize()\n\n# (Optional) Optimize by wrapping some of the code in a graph using TF function.\nagent.train = common.function(agent.train)\n","a6034d59":"a_spec = train_env.action_spec()\nt_spec = train_env.time_step_spec()\nmp = MyFootbalHumanPyPolicy(time_step_spec=t_spec, action_spec=a_spec)","ebf1bfb6":"magent = learningHelper(train_env=train_env, test_env=tf_env_eval, agent=agent, global_step=global_step, \n                        collect_episodes = 3,\n                        eval_interval=5,\n                        replay_buffer_capacity=3500,\n                        batch_size=500,\n                        collect_policy = mp\n)\nmagent.restore_check_point()","a4a6160c":"%%time\nmagent.train_agent(1)\nmagent.save_policy()","efd00062":"avg_ret = magent.evaluate_agent(1)\nprint(avg_ret)","bfdbb670":"%%writefile submission.py\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom kaggle_environments.envs.football.helpers import *\n\nimport tensorflow as tf\nimport numpy as np\nfrom tf_agents.trajectories import time_step as ts\n\nimport abc\nimport tensorflow as tf\nimport numpy as np\nfrom tf_agents.environments import py_environment\nfrom tf_agents.environments import tf_environment\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.environments import utils\nfrom tf_agents.specs import array_spec\nfrom tf_agents.environments import wrappers\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.utils import common\n\nimport gfootball\nimport gfootball.env as football_env\n\ndata_dic = {\n    'ball_owned_team':array_spec.BoundedArraySpec(shape=(1,), dtype=np.int32, minimum=0, maximum=2, name='ball_owned_team'),\n    'steps_left':array_spec.BoundedArraySpec(shape=(1,), dtype=np.float32, minimum=0.0, name='steps_left'),\n    'ball_owned_player':array_spec.BoundedArraySpec(shape=(1,), dtype=np.int32, minimum=0, maximum=11, name='ball_owned_player'),\n    'game_mode':array_spec.BoundedArraySpec(shape=(1,), dtype=np.int32, minimum=0, maximum=6, name='game_mode'),\n    'designated':array_spec.BoundedArraySpec(shape=(1,), dtype=np.int32, minimum=-1, maximum=11, name='designated'),\n    'active':array_spec.BoundedArraySpec(shape=(1,), dtype=np.int32, minimum=-1, maximum=11, name='active'),\n    'left_team_active':array_spec.BoundedArraySpec(shape=(11,), dtype=np.int32, minimum=0, maximum=1, name='left_team_active'),\n    'left_team':array_spec.BoundedArraySpec(shape=(11,2), dtype=np.float32, minimum=-1.5, maximum=1.5, name='left_team'),\n    'right_team_active':array_spec.BoundedArraySpec(shape=(11,), dtype=np.int32, minimum=0, maximum=1, name='right_team_active'),\n    'ball_direction':array_spec.BoundedArraySpec(shape=(3,), dtype=np.float32, name='ball_direction'),\n    'ball': array_spec.BoundedArraySpec(shape=(3,), dtype=np.float32, name='ball'),\n    'left_team_tired_factor':array_spec.BoundedArraySpec(shape=(11,), dtype=np.float32, minimum=0.0, name='left_team_tired_factor'),\n    'left_team_direction':array_spec.BoundedArraySpec(shape=(11,2), dtype=np.float32, minimum=-1.5, maximum=1.5, name='left_team_direction'),\n    'score':array_spec.BoundedArraySpec(shape=(2,), dtype=np.float32, minimum=0.0, name='score'),\n    'left_team_roles':array_spec.BoundedArraySpec(shape=(1,), dtype=np.int32, minimum=0, maximum=10, name='left_team_roles'),\n    'right_team_tired_factor':array_spec.BoundedArraySpec(shape=(11,), dtype=np.float32, minimum=0.0, name='right_team_tired_factor'),\n    'right_team':array_spec.BoundedArraySpec(shape=(11,2), dtype=np.float32, minimum=-1.5, maximum=1.5, name='right_team'),\n    'right_team_direction':array_spec.BoundedArraySpec(shape=(11,2), dtype=np.float32, minimum=-1.5, maximum=1.5, name='right_team_direction'),\n    'right_team_yellow_card':array_spec.BoundedArraySpec(shape=(11,), dtype=np.int32, minimum=0, maximum=1, name='right_team_yellow_card'),\n    'right_team_roles':array_spec.BoundedArraySpec(shape=(1,), dtype=np.int32, minimum=0, maximum=10, name='right_team_roles'),\n    'ball_rotation':array_spec.BoundedArraySpec(shape=(3,), dtype=np.float32, name='ball_rotation'),\n    'left_team_yellow_card':array_spec.BoundedArraySpec(shape=(11,), dtype=np.int32, minimum=0, maximum=1, name='left_team_yellow_card'),\n    'sticky_actions':array_spec.BoundedArraySpec(shape=(10,), dtype=np.int32, minimum=0, maximum=1, name='sticky_actions'),\n    }\n\nclass MyFootballEnv(py_environment.PyEnvironment):\n\n    @staticmethod\n    def convert_observation_to_tf(obs, data_cfg, propertie_list = []):\n        \n        if(len(propertie_list) == 0):\n            propertie_list = ['right_team_direction', 'right_team_tired_factor', \n                               'left_team_roles', 'left_team_direction', 'ball_direction', \n                               'ball_owned_player', 'right_team_yellow_card', 'ball', \n                               'right_team', 'steps_left', 'ball_rotation', \n                               'ball_owned_team', 'game_mode', 'left_team_yellow_card', \n                               'left_team', 'right_team_roles', 'right_team_active', \n                               'left_team_active', 'left_team_tired_factor', 'score', \n                               'designated', 'active', 'sticky_actions']\n        rp = {}\n\n        for name in propertie_list:\n            \n            if(('ball_owned_player' == name) and (obs[0][name] == -1)):\n                obs[0][name] = 11\n            if(('ball_owned_team' == name) and (obs[0][name] == -1)):\n                obs[0][name] = 2\n            \n            if('right_team_roles' == name):\n                if((obs[0]['ball_owned_team'] == 1) and ((obs[0]['ball_owned_player'] != -1) or (obs[0]['ball_owned_player'] != 11))):\n                    obs[0][name] = obs[0][name][obs[0]['ball_owned_player']]\n                else:\n                    obs[0][name] = 10\n\n            if('left_team_roles' == name):\n                if((obs[0]['ball_owned_team'] == 0) and ((obs[0]['ball_owned_player'] != -1) or (obs[0]['ball_owned_player'] != 11))):\n                    obs[0][name] = obs[0][name][obs[0]['ball_owned_player']]\n                else:\n                    obs[0][name] = 10\n\n\n            if(data_cfg[name].shape == (1,)):\n                rp[name] = np.array([np.squeeze(obs[0][name])]).astype(data_cfg[name].dtype)\n            else:\n                a = np.zeros(data_cfg[name].shape, dtype=data_cfg[name].dtype)\n                a[:len(obs[0][name])] = obs[0][name]\n                obs[0][name] = a.copy()\n                rp[name] = np.array(obs[0][name]).astype(data_cfg[name].dtype)\n\n        return rp\n\n    @staticmethod\n    def construct_obs_spec(data_cfg, cfg):\n        rp = {}\n        \n        for name in cfg:\n            rp[name] = data_cfg[name]\n        \n        return rp\n        \n    \n    def __init__(self, propertie_list = [], scenario='11_vs_11_kaggle'):\n        super().__init__()\n        self._data_dic = data_dic\n        \n        if(len(propertie_list) == 0):\n            propertie_list = ['right_team_direction', 'right_team_tired_factor', \n                               'left_team_roles', 'left_team_direction', 'ball_direction', \n                               'ball_owned_player', 'right_team_yellow_card', 'ball', \n                               'right_team', 'steps_left', 'ball_rotation', \n                               'ball_owned_team', 'game_mode', 'left_team_yellow_card', \n                               'left_team', 'right_team_roles', 'right_team_active', \n                               'left_team_active', 'left_team_tired_factor', 'score', \n                               'designated', 'active', 'sticky_actions']\n\n        self.propertie_list = propertie_list\n        \n        self.env = football_env.create_environment(\n              env_name=scenario,  \n              stacked=False,\n              representation='raw',  \n              rewards='scoring, checkpoints',\n              write_goal_dumps=False,\n              write_full_episode_dumps=False,\n              render=False,\n              write_video=False,\n              dump_frequency=1,\n              logdir='.\/',\n              extra_players=None,\n              number_of_left_players_agent_controls=1,\n              number_of_right_players_agent_controls=0\n            )\n        \n        \n        self._state = self.convert_observation_to_tf(self.env.reset(), self._data_dic, self.propertie_list)\n\n        self._action_spec = array_spec.BoundedArraySpec(\n            shape=(), dtype=np.int32, minimum=0, maximum=18, name='action')\n        \n        \n        #representation of the enviroment: price + open position state\n        self._observation_spec = self.construct_obs_spec(self._data_dic, self.propertie_list)\n\n        #used for idndication of the end of episode        \n        self._episode_ended = False\n\n        pass\n       \n    def action_spec(self):\n        return self._action_spec\n\n    def observation_spec(self):\n        return self._observation_spec\n\n    def _reset(self):\n        self._episode_ended = False\n        self._state = self.convert_observation_to_tf(self.env.reset(), self._data_dic, self.propertie_list)\n        return ts.restart(self._state)\n\n\n    def _step(self, action):\n\n        if self._episode_ended:\n            # The last action ended the episode. Ignore the current action and start\n            # a new episode.\n            return self.reset()\n        \n        self._state, reward, self._episode_ended, info = self.env.step(action)\n        self._state = self.convert_observation_to_tf(self._state, self._data_dic, self.propertie_list)\n\n        if self._episode_ended:\n            return ts.termination(self._state, reward)\n        else:\n            return ts.transition(self._state, reward=reward, discount=1.0)\n\n\n\n\ndef convert_obs_to_tf_env_obs(obs):\n    cfg = ['ball_owned_team', \n       'steps_left', \n       'ball_owned_player', \n       'game_mode', \n       #'designated', \n       'active', \n       #'left_team_active', \n       'left_team', \n       #'right_team_active', \n       'ball_direction', \n       'ball', \n       #'left_team_tired_factor', \n       'left_team_direction', \n       'score', \n       #'left_team_roles', \n       #'right_team_tired_factor', \n       'right_team', \n       'right_team_direction', \n       #'right_team_yellow_card', \n       #'right_team_roles', \n       'ball_rotation', \n       #'left_team_yellow_card', \n       'sticky_actions']\n    \n    o = MyFootballEnv.convert_observation_to_tf(obs['players_raw'], data_dic, cfg )\n    for key, value in o.items():\n        o[key] = tf.expand_dims(tf.convert_to_tensor(value), axis=0)\n\n    return o\n\npolicy_dir = '.\/policy\/'\nsaved_policy = tf.compat.v2.saved_model.load(policy_dir)\n\n\n\ndef agent(obs):\n    global saved_policy\n    \n    st = tf.constant(np.array(np.array([0], dtype=np.int32)))\n    rw = tf.constant(np.array(np.array([0], dtype=np.float32)))\n    ds = tf.constant(np.array(np.array([0], dtype=np.float32)))\n    ts_obs = {}\n    ts_obs = convert_obs_to_tf_env_obs(obs)\n    t = ts.TimeStep(st, rw, ds, ts_obs)\n    \n    a = saved_policy.action(t).action.numpy()[0]\n    \n    return [a]\n","586befcb":"# Set up the Environment.\nfrom kaggle_environments import make\nenv = make(\"football\", debug=True, configuration={\"save_video\": True, \"scenario_name\": \"1_vs_1_easy\", \n                                                  \"running_in_notebook\": True, \"episodeSteps\": 200})\noutput = env.run([\"do_nothing\",\"submission.py\"])[-1]\nprint('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))\nprint('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))\nenv.render(mode=\"human\", width=800, height=600)","9994aebe":"# Human agent\n\nHuman agent will be used for data collection","b3ad6f01":"# Evaluate agent\nEvaluation is done on one episode","a3b45843":"# Create policy for data collection","b9aa8a54":"# Create Environment\nI wrapped gfootball environmet. So we can tweak a rewards, if we wnat.","aaf47708":"Configuration of the input data","2e00ff12":"# Test if all above is working","68b20018":"# Train one cycle\nI'm using here rule based policy for data collection","44ecaf10":"# Create a submission file\n\nAs all code is in the kaggle notebook, I copied some code above into the submission file directly","0f628f86":"## Info\n\nAll codes you can find here: https:\/\/github.com\/CloseToAlgoTrading\/Kaggle_GFootball\n\nIn the repo you can find a Dockerfile, so you can start gfootball environment localy with Tensorflow and TF-agents\n\nI used here a simle DQN, but we can change it to another agent. Check points and policy will be stored, so we can continue the traning, and we can use the policy for the submission. \n\nP.s. I'm not an expert in the reinforcement learning and my python code is also not optimal :), but I hope this notebook can help to someone understand the main steps. \nOh.. if you find any issues with this implementation, please not be silent, because i'm learning too. Thanks.","f43dc5d9":"# TF-agent policy\n\nI'm not expert on the user defined policy, but here I just create a simple policy that will use our agent for generate actions.","060d95e0":"# Create and train agent","d39c385b":"# Create DQN agent","dee2719e":"# Agent learning object","fad78ecc":"# Install dependencies","83b323be":"# Create a Q_Network"}}