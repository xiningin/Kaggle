{"cell_type":{"6b0faa44":"code","bae0943c":"code","68a7c926":"code","f2526852":"code","98c53eb9":"code","d9a530bd":"code","aad7c6f5":"code","541a5a53":"markdown","8ba48574":"markdown","3f8d1a15":"markdown","37bfff65":"markdown","e59bd309":"markdown","198b7ffb":"markdown","bdb8ed17":"markdown","d2de033c":"markdown","52ee4986":"markdown","58d64089":"markdown","90811c06":"markdown","443f5163":"markdown","b9f1fb21":"markdown","e16ba9dc":"markdown","8616bc1e":"markdown","271d15fa":"markdown","e441d45b":"markdown"},"source":{"6b0faa44":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bae0943c":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\n%matplotlib inline\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data[:, :2] # we only take the first two features. We could\n # avoid this ugly slicing by using a two-dim dataset\ny = iris.target","68a7c926":"def plotSVC(title):\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    h = (x_max \/ x_min)\/100\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n    np.arange(y_min, y_max, h))\n    plt.subplot(1, 1, 1)\n    Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n    plt.xlim(xx.min(), xx.max())\n    plt.title(title)\n    plt.show()","f2526852":"kernels = ['linear', 'rbf', 'poly']\nfor kernel in kernels:\n    svc= svm.SVC(kernel=kernel).fit(X, y)\n    plotSVC('kernel=' + str(kernel))","98c53eb9":"gammas = [0.1, 1, 10, 100]\nfor gamma in gammas:\n    svc = svm.SVC(kernel='rbf', gamma=gamma).fit(X, y)\n    plotSVC('gamma=' + str(gamma))","d9a530bd":"cs = [0.1, 1, 10, 100, 1000]\nfor c in cs:\n    svc = svm.SVC(kernel='rbf', C=c).fit(X, y)\n    plotSVC('C=' + str(c))","aad7c6f5":"degrees = [0, 1, 2, 3, 4, 5, 6]\nfor degree in degrees:\n    svc = svm.SVC(kernel='poly', degree=degree).fit(X, y)\n    plotSVC('degree=' + str(degree))","541a5a53":"# WORKING","8ba48574":"> The basics of Support Vector Machines and how it works are best understood with a simple example. Let\u2019s imagine we have two tags: red and blue, and our data has two features: x and y. We want a classifier that, given a pair of (x,y) coordinates, outputs if it\u2019s either red _or _blue.","3f8d1a15":"increasing c may increase over fitting","37bfff65":"But, what exactly is the best hyperplane? For SVM, it\u2019s the one that maximizes the margins from both tags. In other words: the hyperplane (remember it's a line in this case) whose distance to the nearest element of each tag is the largest.\n\n","e59bd309":"# Support Vector Machine","198b7ffb":"\u201cSupport Vector Machine\u201d (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well (look at the below snapshot).","bdb8ed17":"Using degree=1 is the same as using a \u2018linear\u2019 kernel. Also, increasing this parameters leads to higher training times.","d2de033c":"We can see that increasing gamma leads to overfitting as the classifier tries to perfectly fit the training data","52ee4986":"A support vector machine takes these data points and outputs the hyperplane (which in two dimensions it\u2019s simply a line) that best separates the tags. This line is the decision boundary: anything that falls to one side of it we will classify as blue, and anything that falls to the other as red.","58d64089":"# How does SVM work","90811c06":"# gamma\ngamma is a parameter for non linear hyperplanes. The higher the gamma value it tries to exactly fit the training data set","443f5163":"# Different Paramteres in SVM","b9f1fb21":"# C\nC is the penalty parameter of the error term. It controls the trade off between smooth decision boundary and classifying the training points correctly.","e16ba9dc":"Hpefully You learnt something from it !","8616bc1e":"Creating a function to visualise different parameters of SVC","271d15fa":"# Degree\ndegree is a parameter used when kernel is set to \u2018poly\u2019. It\u2019s basically the degree of the polynomial used to find the hyperplane to split the data.","e441d45b":"1.Kernels : kernel parameters selects the type of hyperplane used to separate the data. Using \u2018linear\u2019 will use a linear hyperplane (a line in the case of 2D data). \u2018rbf\u2019 and \u2018poly\u2019 uses a non linear hyper-plane"}}