{"cell_type":{"5aa338ad":"code","e0148bd1":"code","05b5a7fa":"code","cf6e756b":"code","887f1e8a":"code","32903483":"code","9befe683":"code","953800c0":"code","fa951d66":"code","65b0b5fc":"code","0ec3efcb":"code","de8ba0ca":"code","d97bbee5":"code","aee3b270":"code","6a801c95":"code","e3128b54":"code","1cb825a2":"code","31542e6d":"code","c404798f":"code","343e10a2":"code","cdf35d7a":"code","40655d94":"code","3c4ff70e":"code","55330933":"code","8ff31b2a":"code","dc5c6955":"code","faf86eb0":"code","6e1b17f5":"code","b27802bd":"code","292b7ac4":"code","e01a3bca":"code","4fb6047b":"code","23a7f70c":"code","e6c088fb":"code","bde9f02d":"code","cd02050b":"code","42d1fec4":"code","cc645daf":"code","bf4431c6":"code","d6e7c347":"code","11b47ab0":"code","b67266f1":"code","f905a976":"code","52b18100":"code","0400ef31":"code","d3c42423":"code","d13aa2ee":"code","fbfc0bf6":"code","699f230e":"code","d13a589c":"code","7e051a71":"code","f5846236":"code","fab7993c":"code","3a0c42d2":"code","8b00c758":"code","1d5b18a5":"code","9b4d623d":"code","09008487":"markdown","f6c7dba2":"markdown","210d2608":"markdown","bd381d9e":"markdown","a6e40cfb":"markdown","f44d64ed":"markdown","3c5ab40e":"markdown","7c413c56":"markdown","94b311f4":"markdown","496b5ff4":"markdown","bbf29c63":"markdown","99f71fb6":"markdown","d239741e":"markdown","080b3543":"markdown","2162b11f":"markdown","bb39863c":"markdown","f9621721":"markdown","42a743ce":"markdown","dc4e1aaa":"markdown","33bcc077":"markdown","549c6e41":"markdown","f0896ac2":"markdown","94c952d6":"markdown","24e030a0":"markdown","cf0c58f0":"markdown","49020d8e":"markdown","7091f292":"markdown","6bf3d7ab":"markdown","c996949c":"markdown","2b3028b2":"markdown","1f23ed74":"markdown","997104fe":"markdown"},"source":{"5aa338ad":"# Importing Libraries\n\nimport numpy as np \nimport pandas as pd\nimport os\n\ndf = pd.read_csv(\"\/kaggle\/input\/london-bike-sharing-dataset\/london_merged.csv\")\ndf.head()","e0148bd1":"import pandas_profiling as pp \n\nprofile = pp.ProfileReport(df)\nprofile","05b5a7fa":"df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\ndf = df.set_index(\"timestamp\")\n\ndf.head()","cf6e756b":"df[\"hour\"] = df.index.hour\ndf[\"day_of_month\"] = df.index.day\ndf[\"day_of_week\"]  = df.index.dayofweek\ndf[\"month\"] = df.index.month\ndf.head()","887f1e8a":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","32903483":"plt.figure(figsize=(15, 7))\nax = sns.lineplot(x=df.index, y=df.cnt,data=df)\nax.set_title(\"Amount of bike shares vs date\", fontsize=25)\nax.set_xlabel(\"Date\", fontsize=20)\nax.set_ylabel('Amount of bike shares', fontsize=20)\nplt.show()","9befe683":"# Resample timeseries, for plotting timeseries month frequency\ndf_by_month = df.resample(\"M\").sum()\n\nplt.figure(figsize=(16,6))\nax = sns.lineplot(data=df_by_month,x=df_by_month.index,y=df_by_month.cnt)\nax.set_title(\"Amount of bike shares per month\", fontsize=25)\nax.set_xlabel(\"Month\", fontsize=20)\nax.set_ylabel('Amount of bike shares', fontsize=20)\nplt.show()","953800c0":"df_by_week = df.resample(\"D\").sum()\nplt.figure(figsize=(16,6))\nax = sns.lineplot(data=df_by_week,x=df_by_week.index,y=df_by_week.cnt)\nax.set_title(\"Amount of bike shares per day\", fontsize=25)\nax.set_xlabel(\"Time\", fontsize=20)\nax.set_ylabel('Amount of bike shares', fontsize=20)\nplt.show()","fa951d66":"plt.figure(figsize=(16,6))\nax = sns.pointplot(data=df,hue=df.season,y=df.cnt,x=df.month)\nax.set_title(\"Amount of bike shares per season\", fontsize=25)\nax.set_xlabel(\"Month\", fontsize=20)\nax.set_ylabel('Amount of bike shares', fontsize=20)\nplt.show()","65b0b5fc":"plt.figure(figsize=(16, 6))\nax = sns.pointplot(x='day_of_week', y='cnt',data=df)\nax.set_title(\"Amount of bike shares in a week\", fontsize=25)\nax.set_xlabel(\"Day of the week\", fontsize=20)\nax.set_ylabel('Amount of bike shares', fontsize=20)\nplt.show()","0ec3efcb":"plt.figure(figsize=(16, 6))\nax = sns.pointplot(x='hour', y='cnt',hue='is_holiday',data=df)\nax.set_title(\"Amount of bike shares per hour in a day\", fontsize=25)\nax.set_xlabel(\"Hour of they day\", fontsize=20)\nax.set_ylabel('Amount of bike shares', fontsize=20)\nplt.show()","de8ba0ca":"plt.figure(figsize=(16, 6))\nax = sns.pointplot(x='hour', y='cnt',data=df[df[\"is_weekend\"]==1])\nax.set_title(\"Amount of bike shares per hour on a weekend day\", fontsize=25)\nax.set_xlabel(\"Hour of they day\", fontsize=20)\nax.set_ylabel('Amount of bike shares', fontsize=20)\nplt.show()","d97bbee5":"plt.figure(figsize=(20,10))\n\nax = sns.pointplot(x='t1', y='cnt',data=df)\nax.set_title(\"Amount of bike shares vs real temperature\", fontsize=25)\nax.set_xlabel(\"Real temperature (\u00b0C)\", fontsize=20)\nax.set_ylabel('Amount of bike shares', fontsize=20)\nplt.locator_params(axis='x', nbins=10)","aee3b270":"plt.figure(figsize=(20,10))\n\nax = sns.pointplot(x='t2', y='cnt',data=df)\nax.set_title(\"Amount of bike shares vs feeling temperature\", fontsize=25)\nax.set_xlabel(\"Feeling temperature (\u00b0C)\", fontsize=20)\nax.set_ylabel('Amount of bike shares', fontsize=20)\nplt.locator_params(axis='x', nbins=10)\nplt.show()\n\n","6a801c95":"plt.figure(figsize=(20,10))\n\nax = sns.pointplot(x='hum', y='cnt',data=df)\nax.set_title(\"Amount of bike shares vs humidity\", fontsize=25)\nax.set_xlabel(\"Humidity (%)\", fontsize=20)\nax.set_ylabel('Amount of bike shares', fontsize=20)\nplt.locator_params(axis='x', nbins=10)\nplt.show()","e3128b54":"print(\"Temperature and humidity have a weak negative correlation:\")\ndf[\"t1\"].corr(df[\"hum\"], method = \"pearson\")","1cb825a2":"plt.figure(figsize=(20,10))\n\nax = sns.pointplot(x='wind_speed', y='cnt',data=df)\nax.set_title(\"Amount of bike shares vs windspeed\", fontsize=25)\nax.set_xlabel(\"Windspeed (km\/h)\", fontsize=20)\nax.set_ylabel('Amount of bike shares', fontsize=20)\nplt.locator_params(axis='x', nbins=10)\nplt.show()","31542e6d":"plt.figure(figsize=(16,6))\nax = sns.histplot(data=df,y=df.cnt,x=df.weather_code)\nax.set_title(\"Amount of bike shares vs the weather\", fontsize=25)\nax.set_xlabel(\"Weather code\", fontsize=20)\nax.set_ylabel('Amount of bike shares', fontsize=20)\nplt.show()","c404798f":"plt.figure(figsize=(16,6))\nsns.heatmap(df.corr(),cmap=\"YlGnBu\",square=True,linewidths=.5,center=0)","343e10a2":"data = pd.read_csv(\"..\/input\/london-bike-sharing-dataset\/london_merged.csv\")\ndata[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])\n\nmydata = data[['timestamp', 'cnt']].copy()\nmydata[\"timestamp\"] = pd.to_datetime(mydata[\"timestamp\"])\nmydata = mydata.set_index(\"timestamp\")\n\n# Daily resampling\ndaydata = mydata.resample(\"D\").sum()\n\ndaydata['timestamp'] = daydata.index\ndaydata.index = range(0,len(daydata['cnt'].to_numpy()))\n\n# No resampling for hourly prediction\nhourdata = mydata\nhourdata['timestamp'] = hourdata.index\nhourdata.index = range(0,len(hourdata['cnt'].to_numpy()))","cdf35d7a":"daydf = daydata[['timestamp','cnt']].copy()\ndaydf.columns = ['ds','y']\n\nhourdf = hourdata[['timestamp','cnt']].copy()\nhourdf.columns = ['ds', 'y']","40655d94":"from sklearn.metrics import mean_absolute_error, mean_squared_error\nimport math\nfrom fbprophet import Prophet\n\n# Train the models\ndaymodel = Prophet()\nhourmodel = Prophet()\n\n# Fit the model with train set\ndaymodel.fit(daydf) \nhourmodel.fit(hourdf)\n\ndayfuture = daymodel.make_future_dataframe(periods=365)\nhourfuture = hourmodel.make_future_dataframe(periods=365, freq='H')\n\ndaypred = daymodel.predict(dayfuture)\nhourpred = hourmodel.predict(hourfuture)","3c4ff70e":"# Plot the day forecast\nf, ax = plt.subplots(1)\nf.set_figheight(6)\nf.set_figwidth(15)\n\ndaymodel.plot(daypred, ax=ax)\n\nax.set_title('Bike share prediction per day', fontsize=14)\nax.set_xlabel(xlabel='Date', fontsize=14)\nax.set_ylabel(ylabel='Bikes shares', fontsize=14)\n\nplt.show()","55330933":"# Plot the hour forecast\nf, ax = plt.subplots(1)\nf.set_figheight(6)\nf.set_figwidth(15)\n\nhourmodel.plot(hourpred, ax=ax)\n\nax.set_title('Bike share prediction per hour', fontsize=14)\nax.set_xlabel(xlabel='Date', fontsize=14)\nax.set_ylabel(ylabel='Bikes shares', fontsize=14)\n\nplt.show()","8ff31b2a":"# Python\nfrom fbprophet.plot import plot_plotly, plot_components_plotly\n\n#plot_plotly(daymodel, daypred)","dc5c6955":"#plot_plotly(hourmodel, hourpred)","faf86eb0":"#plot_components_plotly(daymodel, daypred)\nfig = daymodel.plot_components(daypred)","6e1b17f5":"#plot_components_plotly(hourmodel, hourpred)\nfig = hourmodel.plot_components(hourpred)","b27802bd":"data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])\nfeature_columns = [\n    't1',\n    't2',\n    'hum',\n    'wind_speed',\n]\ntarget_column = ['cnt']\n\nmultidata = data[['timestamp'] + target_column + feature_columns].copy()\nmultidata.columns = ['ds', 'y'] + feature_columns","292b7ac4":"# Train the model\nmultimodel = Prophet()\nmultimodel.add_regressor('t1')\nmultimodel.add_regressor('t2')\nmultimodel.add_regressor('hum')\nmultimodel.add_regressor('wind_speed')\n\n# Fit the model with train set\nmultimodel.fit(multidata)\n\nmultifuture = multimodel.make_future_dataframe(periods=365, freq='H')\n\n# Predict on valid set\n# multipred = multimodel.predict(multifuture)","e01a3bca":"!pip install neuralprophet\nfrom neuralprophet import NeuralProphet","4fb6047b":"neuraldaymodel = NeuralProphet()\nmetrics = neuraldaymodel.fit(daydf, freq='D')","23a7f70c":"neuralhourmodel = NeuralProphet()\nhourmetrics = neuralhourmodel.fit(hourdf, freq='H')","e6c088fb":"neuralhourfuture = neuralhourmodel.make_future_dataframe(hourdf, periods=365)\nneuralhourforecast = neuralhourmodel.predict(neuralhourfuture)\n\nneuraldayfuture = neuraldaymodel.make_future_dataframe(daydf, periods=365)\nneuraldayforecast = neuraldaymodel.predict(neuraldayfuture)\n\ndayforecasts_plot = neuraldaymodel.plot(neuraldayforecast)\n","bde9f02d":"fig_comp = neuraldaymodel.plot_components(neuraldayforecast)","cd02050b":"forecasts_plot = neuralhourmodel.plot(neuralhourforecast)","42d1fec4":"fig_comp = neuralhourmodel.plot_components(neuralhourforecast)","cc645daf":"data","bf4431c6":"X = data.drop(['timestamp','cnt'],axis=1)\ny = data['cnt']\n#scaler_x = preprocessing.MinMaxScaler()\n#X =  pd.DataFrame(scaler_x.fit_transform(X), columns = X.columns)","d6e7c347":"def df_split(df,train_percent):\n    split_index = int(train_percent * len(df))\n    train = df.iloc[:split_index]\n    test = df.iloc[split_index:]\n    return train,test","11b47ab0":"X_train,X_test = df_split(X,0.7)\ny_train,y_test = df_split(y,0.7)","b67266f1":"from xgboost.sklearn import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error,r2_score, mean_squared_log_error,mean_squared_error, make_scorer","f905a976":"xgbmodel = XGBRegressor()\nxgbmodel.fit(X_train,y_train)\n\npreds = xgbmodel.predict(X_test)","52b18100":"rmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))","0400ef31":"y_test.values","d3c42423":"import matplotlib.pyplot as plt\nplt.figure(figsize=(16,6))\nplt.plot(y_test.values,marker=\".\",label=\"actual\")\nplt.plot(preds,marker=\".\",label=\"prediction\",color=\"r\")\n\nplt.title('Bike share prediction per hour', fontsize=22)\nplt.xlabel(xlabel='Date', fontsize=14)\nplt.ylabel(ylabel='Bikes shares', fontsize=14)\nplt.legend(['Ground truth', 'Predicted'])\n\nplt.show()","d13aa2ee":"df","fbfc0bf6":"import math\nfrom sklearn.preprocessing import RobustScaler\n\ntraining_data_len = math.ceil(len(df) *.9) # taking 90% of data to train and 10% of data to test\ntesting_data_len = len(df) - training_data_len\n\ntime_steps = 24\ntrain, test = df.iloc[0:training_data_len], df.iloc[(training_data_len-time_steps):len(df)]\nprint(df.shape, train.shape, test.shape)","699f230e":"# Scale the all of the data from columns ['t1', 't2', 'hum', 'wind_speed']\ntrain_trans = train[['t1', 't2', 'hum', 'wind_speed']].to_numpy()\ntest_trans = test[['t1', 't2', 'hum', 'wind_speed']].to_numpy()\n\nscaler = RobustScaler() # Handles outliers\ntrain.loc[:, ['t1', 't2', 'hum', 'wind_speed']]=scaler.fit_transform(train_trans)\ntest.loc[:, ['t1', 't2', 'hum', 'wind_speed']]=scaler.fit_transform(test_trans)\n\n#Scale the all of the data from columns ['cnt']\ntrain['cnt'] = scaler.fit_transform(train[['cnt']])\ntest['cnt'] = scaler.fit_transform(test[['cnt']])","d13a589c":"from tqdm import tqdm_notebook as tqdm\n\n#Split the data into x_train and y_train data sets\nx_train = []\ny_train = []\n\nfor i in tqdm(range(len(train) - time_steps)):\n    x_train.append(train.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n    y_train.append(train.loc[:,'cnt'].iloc[i + time_steps])\n\n#Convert x_train and y_train to numpy arrays\nx_train = np.array(x_train)\ny_train = np.array(y_train)","7e051a71":"#Create the x_test and y_test data sets\nx_test = []\ny_test = df.loc[:,'cnt'].iloc[training_data_len:len(df)]\n\nfor i in tqdm(range(len(test) - time_steps)):\n    x_test.append(test.drop(columns='cnt').iloc[i:i + time_steps].to_numpy())\n    # y_test.append(test.loc[:,'cnt'].iloc[i + time_steps])\n\n#Convert x_test and y_test to numpy arrays\nx_test = np.array(x_test)\ny_test = np.array(y_test)","f5846236":"# All 12 columns of the data\nprint('Train size:')\nprint(x_train.shape, y_train.shape)\nprint('Test size:')\nprint(x_test.shape, y_test.shape)","fab7993c":"from keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout , LSTM , Bidirectional \n\nmodel = Sequential()\nmodel.add(Bidirectional(LSTM(50,input_shape=(x_train.shape[1],x_train.shape[2]))))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=1))\n\nmodel.compile(optimizer=\"adam\",loss=\"mse\")\n\n# prepared_model = model.fit(X_train,y_train,batch_size=32,epochs=100,validation_data=[X_test,y_test])\n\nhistory = model.fit(x_train, y_train, epochs=150, batch_size=24, validation_split=0.1, shuffle=True)","3a0c42d2":"y_pred = model.predict(x_test)\ny_pred = scaler.inverse_transform(y_pred)#Undo scaling\ny_pred","8b00c758":"from sklearn.metrics import mean_squared_error, r2_score\nrmse_lstm = np.sqrt(mean_squared_error(y_test, y_pred))\nrmse_lstm","1d5b18a5":"plt.figure(figsize=(16, 8))\nplt.plot(y_test[1200:1500], label='true')\nplt.plot(y_pred[1200:1500], label='predicted')\nplt.legend()","9b4d623d":"plt.plot(history.history[\"loss\"],label=\"loss\")\nplt.plot(history.history[\"val_loss\"],label=\"val_loss\")\nplt.legend(loc=\"best\")\nplt.xlabel(\"No. Of Epochs\")\nplt.ylabel(\"mse score\")","09008487":"An increase in humidity results in a decrease of the bikes shared.\n\nSpontaniously i asked myself if humidity is related to the outside temperature. Let's call the correlation matrix.\n\n","f6c7dba2":"<a id=\"eda\"><\/a>\n## 2.2. Exploratory Data Analysis\n\nExploring the data is key to understanding it and finding some usable insights!\n\nOne of the most importants conclusions from this dataset is how many bike shares occured over time.","210d2608":"Like we tought it's clear that people rent more bikes when the temperature is high. Next to this we see a high correlation between the real and the feelings temperature.\n\n### 2.2.5. Amount of bike shares related to the humidity","bd381d9e":"It's clear that on a normal day most bikes are rented within the rush hour peaks. On a holiday this clearly changes, the rush hour peaks are not present and most bikes ar rented in the afternoon.\n\nIt can be concluded that's there a distinct difference in shares when it's not a working day. If this trend is true, we should also see it occuring in the weekend.","a6e40cfb":"Above you can see the **ProfileReport()**. When looking at the report it's clear that we have a clean dataset. \n\nOne thing I did notice is that the timestamp isn't of the type \"DateTime\". We're gonna change this and also set the timestamp as index.","f44d64ed":"# <p style=\"background-color:skyblue; font-family:calibri; font-size:150%; text-align:center\" id=\"LSTM\"> 7. LSTM Modelling<\/p>\n\nIn this chapter we'll be creating a bidirectional LSTM model. It's basically a LSTM model (RNN) which presents each training sequence forwards and backwards through the layer. Thanks to this it picks up the sequential information about all points before and after it. [Here's](https:\/\/machinelearningmastery.com\/develop-bidirectional-lstm-sequence-classification-python-keras\/) another example where Bi-LSTM is used to classify sequences.\n\nIf you're not familiar with recurrent networks I recommend you to read this [notebook](https:\/\/www.kaggle.com\/thebrownviking20\/intro-to-recurrent-neural-networks-lstm-gru).\n\nWe'll be using the Keras library to implement this model.","3c5ab40e":"The xgb model did a good job at capturing the trend. But there is still quite a big difference with the ground truth. Let's see if an LSTM neural network can do a better job at this forecasting.","7c413c56":"It's clear that when the weather is good (1,2,3,4,7) more bikes are rented then when the weather is bad (10,26)","94b311f4":"# <p style=\"background-color:skyblue; font-family:calibri; font-size:150%; text-align:center\" id=\"prophet\"> 3. Prophet<\/p>\n\nOpensource library for univariate timeseries forecasting, Luckely prophets also implements an additive time series forecasting model. More information about the Prophet library can be found [here](https:\/\/facebook.github.io\/prophet\/).\n\n\nIn the documentation of Prophet i found the following.\n\n> Prophet will by default fit weekly and yearly seasonalities, if the time series is more than two cycles long. It will also fit daily seasonality for a sub-daily time series. You can add other seasonalities (monthly, quarterly, hourly) using the add_seasonality method (Python) or function (R).\n([link Specific Custom Seasonalities](https:\/\/facebook.github.io\/prophet\/docs\/seasonality,_holiday_effects,_and_regressors.html))\n\nWhen you're using Prophet it's important to know that you need atleast two full periods (yearly) of your data so that the model can completely capture the trend. Although it is possible to add your own seasonalities and other trend manually to the model (.[add_seasonality](https:\/\/facebook.github.io\/prophet\/docs\/seasonality,_holiday_effects,_and_regressors.html#specifying-custom-seasonalities)) I won't be doing this for now.\n\nIf you're dealing with holidays, check this [link](https:\/\/facebook.github.io\/prophet\/docs\/seasonality,_holiday_effects,_and_regressors.html#modeling-holidays-and-special-events) to add these to your model.\nOther non daily events can be addressed with through this [method](https:\/\/facebook.github.io\/prophet\/docs\/non-daily_data.html#sub-daily-data). As you can see alot of customization can be done to a train model giving you full control of the trend created. That's exactly why I wanted to test this library! \n\n\n<a id=\"univariateprophet\"><\/a>\n\n## 3.1. Univariate Prophet\n\nWe we'll be try to predict both the daily and hourly sales. We'll have to use all of the data since we barely have two full yearly periods of bike shares. Unfortunately this means that evaluating the model with unseen data is not possible.","496b5ff4":"### 2.2.8. Feature correlations\n\nEnding this EDA with an overview of all feature correlations.","bbf29c63":"Unfortunately since we needed to use all hour training data it's not possible to evaluate the models we created. We'll have to evaluate them visually instead.","99f71fb6":"It seems like there is small peak when the windspeed is at around 25 km\/h.\n\n\n### 2.2.7. Amount of bike shares related to the weather ","d239741e":"The trend above confirms the difference between bike shares on normal working days versus holidays.\n\n### 2.2.4. Amount of bike shares related to temperature","080b3543":"The prophet library requires some specific transformations.","2162b11f":"# <p style=\"background-color:skyblue; font-family:calibri; font-size:150%; text-align:center\" id=\"xgb\"> 6. XGBoost Modelling<\/p>","bb39863c":"Above you can see that we were training a model with additional regressors. Unfortunately due to a lack of data we cant futher test this. The only solution would be to split the data and define our own function for seasonality, add this to the trained model and then test it again. But for now I'm considering this as \"out-of-scope\".","f9621721":"It's clear that our model did a good job predicting future bike shares. I'm certain it will be making reliable prediction in the future. Checking the model components (trend + cyclic component [yearly seasonality, weekly seasonality] and effects of the holidays at this stage.","42a743ce":"# <p style=\"background-color:skyblue; font-family:calibri; font-size:150%; text-align:center\" id=\"neuralprophet\"> 5. Neural Prophet Modelling<\/p>\n\n[Neural Prophet](https:\/\/neuralprophet.com\/model-overview\/#when-to-use-neuralprophet) is another time series forecasting tool based on the Facebook Prophet. It's developed in a fully modular architecture which makes it easily scalable! This library is pretty young and continuous to add more features to the library. \n\nMore information about how this model works can be found [here](https:\/\/neuralprophet.com\/model-overview\/#when-to-use-neuralprophet). One clear difference is that it now uses a neural network called [AR-Net](https:\/\/github.com\/ourownstory\/AR-Net) to handle the auto-regression part.\n","dc4e1aaa":"### 2.2.2. Amount of bike shares in a week\n\n\nSo we see a clear pattern when looking at the bike shares per month. What about bike shares on a day to day basis?","33bcc077":"It's clear that more bikes are rented in summer time then winter time. Could this be related to the temperature? (See chapter 2.2.4)","549c6e41":"Just so you know it has interactive plotting! (It slowed down this notebook so i added them as comments.","f0896ac2":"The plot above shows the bike shares across the time. Even though it shows alls the bike sales it's hard to gain insights from it. Let's see if we can use our own created attributes to find a conclusion about the shares!\n\n### 2.2.1. Amount of bike shares per month\n\n\nCan we see a distinct difference in the shares per month?","94c952d6":"# <p style=\"background-color:skyblue; font-family:calibri; font-size:150%; text-align:center\" id=\"data\"> 2. The Data<\/p>\n\n<a id=\"dataprep\"><\/a>\n## 2.1. Data preprocessing\n\nIn this chapter we'll check the data how \"clean\" the data is. We'll be looking for missing values, incorrect values, duplicates, outliers, ...\n\nTo quickly perform this analysis i'll be trying out the **ProfileReport()** from **pandas_profiling**.","24e030a0":"<a id=\"multivariateprophet\"><\/a>\n## 3.2. Multivariate prophet\n\nA multivariate model, is a timeseries model where the input are multiple features varying over time. Here we'll be focusing on the daily data since we can easily compare it to the model above.\n\nTo add these features we'll use the [.add_regressor()](https:\/\/facebook.github.io\/prophet\/docs\/seasonality,_holiday_effects,_and_regressors.html#additional-regressors) function.","cf0c58f0":"Yes again both models did a great job at capturing the trend. Even though the NeuralProphet library has less features then the regular Prophet library it has a powerfull underlying model which does a great maby even better job on capturing the trend.","49020d8e":"We can clearly notice a dip in shares around the winter period of each year. When the year progress bike shares are increasing towards the summer months (see underneed).","7091f292":"# <p style=\"background-color:skyblue; font-family:calibri; font-size:200%; text-align:center\"> London bike share prediction - Comparing Prophet, XGB, LSTM \ud83d\udeb4\u200d\u2640\ufe0f\ud83d\udcc8  <\/p>\n\n![](https:\/\/www.pertemps.co.uk\/media\/2042\/london_banner.png)\n\n**The goal of this notebook is to predict the amount of bikeshares.\nI'll be comparing models from the Prophet libaries against a XGBoost and LSTM-model.**\n\n\n# <p style=\"background-color:skyblue; font-family:calibri; font-size:150%; text-align:center\"> Table of contents<\/p>\n\n* [1.Introduction](#intro)\n    * [1.1.Goals](#goals)\n    * [1.2.Libraries](#libraries)\n* [2.The Data](#data)\n    * [2.1.Data preprocessing](#dataprep)\n    * [2.2.Exploratory Data Analysis](#eda)\n    * [2.3.Feature engineering](#feature)\n* [3.Prophet modelling](#prophet)\n    * [3.1.Univariate Prophet](#univariateprophet)\n    * [3.2.Multivariate Prophet](#multivariateprophet)\n* [4.Neural Prophet modelling](#neuralprophet)\n* [5.XGBoost modelling](#xgb)\n* [6.LSTM modelling](#LSTM)\n* [7.Conclusion](#conclusion)\n\n\n# <p style=\"background-color:skyblue; font-family:calibri; font-size:150%; text-align:center\" id=\"intro\"> 1. Introduction<\/p>\n\n\nThe dataset we'll be analyzing is the [\"Bike Share\" data](https:\/\/www.kaggle.com\/hmavrodiev\/london-bike-sharing-dataset). It shows the amount of bikes rented every hour in London and it's related parameters. \n\nThe metadata of the columns:\n\n* \"timestamp\" - timestamp field for grouping the data\n* \"cnt\" - the count of a new bike shares\n* \"t1\" - real temperature in C\n* \"t2\" - temperature in C \"feels like\"\n* \"hum\" - humidity in percentage\n* \"windspeed\" - wind speed in km\/h\n* \"weather_code\" - category of the weather\n* \"is_holiday\" - boolean field - 1 holiday \/ 0 non holiday\n* \"is_weekend\" - boolean field - 1 if the day is weekend\n* \"season\" - category field meteorological seasons: 0-spring ; 1-summer; 2-fall; 3-winter.\n\n* \"weather_code\" category description: 1 = Clear ; mostly clear but have some values with haze\/fog\/patches of fog\/ fog in vicinity 2 = scattered clouds \/ few clouds 3 = Broken clouds 4 = Cloudy 7 = Rain\/ light Rain shower\/ Light rain 10 = rain with thunderstorm 26 = snowfall 94 = Freezing Fog\n\nAs I mentioned in the title, the main goal of this notebook is to train multiple models coming from a number of libraries. I wanted to see which models performed the best and which where the easiest to use. In the end we'll put each one side by side and see which models does the best job at creating a forecast!\n\nLibraries used:\n* [FB Prophet](https:\/\/facebook.github.io\/prophet\/docs\/quick_start.html#python-api)\n* [Neural Prophet](https:\/\/neuralprophet.com\/)\n* [XGBoost](https:\/\/xgboost.readthedocs.io\/en\/latest\/)\n* [Keras](https:\/\/keras.io\/) (LSTM)","6bf3d7ab":"Two more features are left to analyse (wind_speed and weather_code). Let's check them out!\n\n\n### 2.2.6. Amount of bike shares related to the windspeed","c996949c":"We'll be creating these features since they could come in handy whilest training some future models.\n","2b3028b2":"# <p style=\"background-color:skyblue; font-family:calibri; font-size:150%; text-align:center\" id=\"conclusion\"> 8. Conclusion<\/p>\n\nIt's clear that there are multiple ways to perform forecasting. Both Prophet-libraries are very easy to use and have some great features! Unfortunately due to a lack of data we weren't capable of further testing the created models. The Prophet models are definitely not the final model, we could add more features like holidays to get an even better performance. I'm honestly really looking forward use these extra features in future projects!\n\nUnderneed you can see a summary of the trained models:\n\n| **Model**  | **RMSE**  | **Remark** |\n|---|---|---|\n| FB Prophet  | \/  | Not enough data |\n| Neural Prophet | \/ | Not enough data|\n| XGBoost | 1015 | \/ |\n| LSTM model | 414 | \/ | \n\n\nThe best performing models was as expected the LSTM model. It took me a little while to get the parameters right, but in the end the prediction looks pretty good and it was well worth the effort! \n\nOverall I found this a really fun and interesting project, testing out multiple libraries on the same dataset.\n\n### Thank you for reading this notebook! Please comment if you have any questions and an upvote would be appreciated!","1f23ed74":"Both forecasts are already doing pretty good without that much effort! Let's view their components!","997104fe":"If we look over a timespan from one week it's clear that less bikes are rented within the weekend.\n\n### 2.2.3. Amount of bike shares in a day\n\n\nWhat about a single day? Since there are two types of days (holiday\/normal day) let's compare these."}}