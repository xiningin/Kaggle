{"cell_type":{"cc06c7a1":"code","2f53af46":"code","c96c6de7":"code","eddb5f5a":"code","f417afd3":"code","a6406f50":"code","817b48f4":"code","9fd68e1f":"code","755bbede":"code","7613650d":"code","a5ab271e":"code","cf580193":"code","28c7be32":"code","d2f94a30":"code","459f5126":"code","3b0ced94":"code","b8928fc0":"code","d842cd46":"code","fac25460":"code","c18f6821":"code","de872a78":"code","da12c5f4":"code","7475bb01":"code","fe1ee7aa":"code","7fde1d94":"code","4fa2334b":"code","8eee8ab2":"code","87aba821":"code","76d4148d":"code","0a3d1e01":"markdown","e64d0719":"markdown","58ce42aa":"markdown","645e9add":"markdown","3d89a65f":"markdown","d80876fd":"markdown","86994b83":"markdown","e9b6bcb5":"markdown","e5cac690":"markdown","e39239ee":"markdown","7cebe81b":"markdown","110c43dc":"markdown","a0e14e11":"markdown","4dbdddc7":"markdown","02888eb4":"markdown","140135ff":"markdown","78ace8d9":"markdown","fe52f08d":"markdown","317bf5da":"markdown","9458ae96":"markdown","166feea1":"markdown","fc55c066":"markdown","6ff8dd36":"markdown","f1948eed":"markdown","30ea4979":"markdown","0505bb69":"markdown","0d44cb78":"markdown","18665576":"markdown","1cc1083a":"markdown","b8fce412":"markdown","74a9fa80":"markdown","12dc7ba8":"markdown","f07eb852":"markdown","f815b9fe":"markdown","a5a1752a":"markdown","4a85f341":"markdown"},"source":{"cc06c7a1":"def build_model(transformer, max_len=512):\n    \"\"\"\n    function for training the model\n    \"\"\"\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=3e-5), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n    # changed from 1e-5 to 3e-5\n    return model","2f53af46":"def roc_auc(predictions,target):\n    '''\n    This methods returns the AUC Score when given the Predictions\n    and Labels\n    '''\n    \n    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n    roc_auc = metrics.auc(fpr, tpr)\n    return roc_auc","c96c6de7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport os\nfrom sklearn import metrics\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\n\nfrom tokenizers import BertWordPieceTokenizer","eddb5f5a":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","f417afd3":"DATA_PATH =  \"..\/input\/jigsaw-multilingual-toxic-comment-classification\"","a6406f50":"wiki_toxic_comment_data = \"jigsaw-toxic-comment-train.csv\"\ntoxic_unintended_data = \"jigsaw-unintended-bias-train.csv\"\nvalid_data = \"validation.csv\"\ntest_data = \"test.csv\"\nsub_sample = \"sample_submission.csv\"\n\n\nwiki_toxic_comment = pd.read_csv(os.path.join(DATA_PATH, wiki_toxic_comment_data))\ntoxic_unintended = pd.read_csv(os.path.join(DATA_PATH, toxic_unintended_data))\nvalid = pd.read_csv(os.path.join(DATA_PATH, valid_data))\ntest = pd.read_csv(os.path.join(DATA_PATH, test_data))\nsub = pd.read_csv(os.path.join(DATA_PATH, sub_sample))\n","817b48f4":"# Round toxic values\ntoxic_unintended.toxic = toxic_unintended.toxic.round().astype(int) \n\ntoxic = len(toxic_unintended[['comment_text', 'toxic']].query('toxic==1'))\n# Combine wiki_toxic_comment with a subset of toxic_unintended\ntrain1 = pd.concat([\n    wiki_toxic_comment[['comment_text', 'toxic']],\n    toxic_unintended[['comment_text', 'toxic']].query('toxic==1'),\n    toxic_unintended[['comment_text', 'toxic']].query('toxic==0').sample(n = (toxic + (toxic \/\/ 3) ), random_state = 42)\n])\ntrain1.info()","9fd68e1f":"# Configuration\nEPOCHS = 2\nBATCH_SIZE_PER_CORE = 64\nBATCH_SIZE = BATCH_SIZE_PER_CORE * strategy.num_replicas_in_sync\nMAX_LEN = 200","755bbede":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    \"\"\"\n    Encoder for encoding the text into sequence of integers for BERT Input\n    \"\"\"\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","7613650d":"# First load the real tokenizer \ntokenizer = transformers.BertTokenizer.from_pretrained('bert-base-multilingual-cased') \n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\nfast_tokenizer","a5ab271e":"%%time\nx_train = fast_encode(train1.comment_text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\nx_valid = fast_encode(valid.comment_text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\nx_test = fast_encode(test.content.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n\ny_train = train1.toxic.values\ny_valid = valid.toxic.values","cf580193":"#for prefetching\nAUTO = tf.data.experimental.AUTOTUNE","28c7be32":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(len(x_train))\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(BATCH_SIZE)\n)","d2f94a30":"%%time\nwith strategy.scope():\n    transformer_layer = (\n        transformers.TFBertModel\n        .from_pretrained('bert-base-multilingual-cased')\n    )\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","459f5126":"n_steps = x_train.shape[0] \/\/ BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)","3b0ced94":"n_steps = x_valid.shape[0] \/\/ BATCH_SIZE\ntrain_history_2 = model.fit(\n    valid_dataset.repeat(),\n    steps_per_epoch=n_steps,\n    epochs=EPOCHS*2\n)","b8928fc0":"scores = model.predict(valid_dataset)\nprint(\"Auc: %.2f%%\" % (roc_auc(scores,y_valid)*100))","d842cd46":"sub['toxic'] = model.predict(test_dataset, verbose=1)\nsub.to_csv('submission.csv', index=False)","fac25460":"# Configuration\nEPOCHS = 2\nBATCH_SIZE_PER_CORE = 32\nBATCH_SIZE = BATCH_SIZE_PER_CORE * strategy.num_replicas_in_sync\nMAX_LEN = 200","c18f6821":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])","de872a78":"# Load the tokenizer\ntokenizer = transformers.AutoTokenizer.from_pretrained('jplu\/tf-xlm-roberta-large')","da12c5f4":"%%time \n\nx_train = regular_encode(train1.comment_text.values, tokenizer, maxlen=MAX_LEN)\nx_valid = regular_encode(valid.comment_text.values, tokenizer, maxlen=MAX_LEN)\nx_test = regular_encode(test.content.values, tokenizer, maxlen=MAX_LEN)\n\ny_train = train1.toxic.values\ny_valid = valid.toxic.values","7475bb01":"AUTO = tf.data.experimental.AUTOTUNE","fe1ee7aa":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(len(x_train))\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(BATCH_SIZE)\n)","7fde1d94":"%%time\nwith strategy.scope():\n    transformer_layer = (\n        transformers\n        .TFAutoModel.from_pretrained('jplu\/tf-xlm-roberta-large')\n    )\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","4fa2334b":"n_steps = x_train.shape[0] \/\/ BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)","8eee8ab2":"n_steps = x_valid.shape[0] \/\/ BATCH_SIZE\ntrain_history_2 = model.fit(\n    valid_dataset.repeat(),\n    steps_per_epoch=n_steps,\n    epochs=EPOCHS*2\n)","87aba821":"scores = model.predict(valid_dataset)\nprint(\"Auc: %.2f%%\" % (roc_auc(scores,y_valid)*100))","76d4148d":"sub['toxic'] = model.predict(test_dataset, verbose=1)\nsub.to_csv('submission.csv', index=False)","0a3d1e01":"## XLM-RoBERTa: Define the model <a id=\"14\"><\/a>","e64d0719":"XLM-R transformer language model\u2019s architecture has two modifications namely XLM-RoBERTa-Base and XLM-RoBERTa-Large which have been built with a different set of parameters. Initially, XLM-RoBERTa-Base has been assigned having approximately 270M parameters with 12 Transformer layers, 768 hidden units (in the context of Recurrent Neural Networks). In contrast, the XLM-RoBERTa-Large version has extended architecture with tuned on 550M parameters with 24 Transformer layers and 1024 hidden units. The vocabulary is much larger than mBERT's one, 250K tokens for Base and Large versions.\n\nIn comparing to mBERT, XLM-R shows significantly higher results on multilingual tasks with low-resource languages. In the Facebook AI team\u2019s research on the XNLI (Cross-lingual Natural Language Inference) dataset, XLM-R outperformed by 5.1% average accuracy, whereas on F1-measure (weighted harmonic mean of the test\u2019s precision and recall) has been improved by 2.42% on Named Entity Recognition task and [demonstrated](https:\/\/arxiv.org\/abs\/1911.02116) higher F1-score convergence on Cross-lingual Question Answering.","58ce42aa":"## XLM-RoBERTa: Build the data pipeline <a id=\"13\"><\/a>","645e9add":"Fine-tuning both models, mBERT and XLM-RoBERTa, requires a hardware accelerator. That goal will be best achieved by using free access to Kaggle's Tensor Processing Units [TPUs](https:\/\/www.kaggle.com\/docs\/tpu). This design also calls for using Keras API with the TensorFlow backend which implicitly solves the problem with choosing the Deep Learning Framework.\n\nThere is a variety of techniques and ready-to-use libraries for fine-tuning mBERT or XLM-RoBERTa. For mBERT, I tried libraries such as [keras-bert](https:\/\/github.com\/CyberZHG\/keras-bert), [tensorflow_hub](https:\/\/tfhub.dev\/tensorflow\/bert_multi_cased_L-12_H-768_A-12\/2) and [transformers](https:\/\/github.com\/huggingface\/transformers) by HuggingFace. The latter was used for XLM-RoBERTa too, though there are concise approaches which use the PyTorch backend, but I will pass that due to TensorFlow choosing. Therefore, I came up with the implementation based on transformers library for fine-tuning both models due to possibility of making a similar architecture with the same hyperparameters wherever possible. ","3d89a65f":"## mBERT: Train the model <a id=\"9\"><\/a>","d80876fd":"## XLM-RoBERTa: Train the model <a id=\"15\"><\/a>","86994b83":"# XLM-RoBERTa <a id=\"11\"><\/a>","e9b6bcb5":"## Transfer Learning <a id=\"2\"><\/a>","e5cac690":"## mBERT: Evaluation and submission <a id=\"10\"><\/a>","e39239ee":"- **Epochs**: We tried a different number of epochs and found that more than 2 epochs lead to the overfitting problem, thus 2 epochs are enough to prevent that.\n- **Batch size**: The following formula is used for setting the batch size: <br><br>\n\\begin{equation*}\nbatch\\_size = batchsize\\_per\\_core \\cdot NUMBER\\_OF\\_TPU\\_CORES\n\\end{equation*} <br>\nwhere  $batch\\_size\\_per\\_core$ is a hyperparameter which we have set to 64, while $NUMBER\\_OF\\_TPU\\_CORES$ is a constant which equals to 8 cores. The constant is need to distribute data evenly among cores. I tried increasing $batch\\_size\\_per\\_core$, as recommended in the official documentation, to reduce the training time by loading more the TPU, but got a resource exhausted error for more than 64 units, which means a lack of random-access memory.\n\n- **Sequence length**: The median comment length is 203 tokens, and because of that fact, we can minimize the truncation and padding during tokenizing. Also, we empirically found that 200 tokens are an optimal sequence length.\n \n- **Learning rate**: The learning rate is fixed to 3e-5, as recommended in [the original BERT paper](https:\/\/arxiv.org\/abs\/1810.04805).\n","7cebe81b":"## TPU detection <a id=\"3\"><\/a>","110c43dc":"# Multilingual Toxic Comment Classification (mBERT,XLM-RoBERTA, transformers)","a0e14e11":"# mBERT <a id=\"5\"><\/a>","4dbdddc7":"### Helper Functions","02888eb4":"We will describe in detail the internal mBERT techniques: from raw text to \u201cready-to-train\u201d form.\nAs for any pre-trained model, developers provide not only the weights matrix for the complex neural network architecture, but preprocessing methods and the network hyperparameters which were used during training. Most often the authors contribute the vocabulary and the tokenizer for NLP task-agnostic models such as mBERT or XLM-RoBERTa which can be fine-tuned for any related downstream tasks.\nCrucially, the preprocessing phase should be the same as for original mBERT pre-training data to avoid mismatch.\nThe mBERT tokenization contains three main steps:\n- **Normalization.** The raw text is transformed to lowercase, each whitespace is converted to one space, and accent markers are removed. The reduction of accent markers potentially changing word meaning for some languages. This is a deliberate step, the authors state that this step significantly decreases the vocabulary size. A trade-off has to be settled by the strong context-dependence of mBERT.\n<center><code>Example: \u201cMartin's glasses.\u201d $\\rightarrow$ \u201cmartin's\u00a0glasses.\u201d<\/code><\/center>\n\n- **Punctuation splitting.** Both-side splitting all punctuation symbols (adding whitespace on both sides). The punctuation symbol is Unicode punctuation characters or any non-number\/letter\/space ASCII characters.\n<center><code>Example: \u201cmartin's\u00a0glasses.\u201d -> \u201cmartin ' s\u00a0glasses . \u201d<\/code><\/center>\n\n- **WordPiece tokenization.** This step applies whitespace tokenization and for each token. [WordPiece](https:\/\/static.googleusercontent.com\/media\/research.google.com\/ja\/\/pubs\/archive\/37842.pdf) is a word segmentation algorithm and needed to handle Out-Of-Vocabulary (OOV) or rare words. The core idea for the word segmentation algorithm is to give unique ids for most frequent words, while others are broken down into subword units with the best meaning preserve. The WordPiece algorithm operates as follows: any OOV or rare word will be decomposed into characters which will be merged to create the most suitable subword units. The subword units are chosen based on the increasing log-likelihood of a unigram model trained on the training data. It is worth noting that the hashed subwords (e.g. \\#\\#aff) are already in mBERT vocabulary.\n<center><code>Example: \u201cunaffable\u201d~$\\rightarrow$~\u201cun\u201d, \u201c\\#\\#aff\u201d, \u201c\\#\\#able\u201d<\/code><\/center> \n\n\nThere are still practices that should be mentioned. The maximum possible sequence length for mBERT and XLM-RoBERTa inputs is 512 tokens. I tried different sequence lengths for text comments and found 200 tokens as an optimal to avoid lots of padding or truncation on the other side. <br> <br>\nThe mBERT model is designed to solve a variety of NLP tasks, for this reason for indicating the classification we add [CLS] and [SEP] tokens at the beginning and the end respectively. \n\n","140135ff":"We try to define models and choose similar hyperparameters for them. That's why the following processes are quite similar for both of them.","78ace8d9":"The classical models imply to work with one specific task and certain feature-space after training based on machine learning or deep learning algorithms. The models have to be retrained if we want to change the task or even the domain. \\textbf{Transfer learning} is a technique for leveraging the learned knowledge for one task to solve similar ones.\n\nIn general, the transfer learning workflow for the classification task is the following:\n\n**1. Load a pre-trained model** <br>\n**2. Freeze the layers to save the learned knowledge, patterns for the previous task**\n**3. Exclude the existed head (classifier) and add a new head with an appropriate number of hidden units and activation function** <br>\n**4. Train the model on a new dataset** <br>\n\nI use mBERT and XLM-RoBERTa as pre-trained models and they both are [Transformer](https:\/\/arxiv.org\/abs\/1706.03762) based language models .\n\n- **Transformer layer** The basic component for both models is the Transformer layer. The classical Transformer consists of encoder and decoder modules, and connections between them. These modules are a bunch of encoders and decoders respectively. Models like mBERT or XLM-R have only encoder module, and we focus on that. The Transformer layer (encoder) has two components:\n\n    - **Self-Attention** which answers a question: \"How relevant i-th word in the sentence to other words in the same sentence?\" For example, we have the following sentence:\n <center><code>\u201dThe cow can't cross the street because it is too tired\u201d.<\/code><\/center> <br> \n What does \u201cit\u201d in this sentence refer to? Is it referring to the cow or to the street? It\u2019s an elementary question to a human, but not as elementary to a machine.\nWhen the model is processing the word \u201cit\u201d, self-attention allows to associate \u201cit\u201d with \u201ccow\u201d. As the model processes each position in the sentence, self-attention is looking at other positions in the input sequence and identifying the relations between them. Thus, the output for each word is a vector which represents the contextual relationship between other words in the sentence.\n\n    - **Feed-Forward** is just a simple feed-forward neural network applied to each attention vector. This network is used in practice to transform the attention vector into a form that digestible by the next encoder in the module.\n  \n![encoder.jpg](attachment:encoder.jpg)\n\n","fe52f08d":"The Kaggle Notebook environment provides the TPU accelerator for training with 8 computational cores and only 4 CPU cores for supporting processes such as the data pipeline. This poses the TPU data starvation problem because the TPU core is a dramatically fast matrix computer and CPU basically can't provide a new batch of training data on time. To mitigate this issue and properly load the computing units we use the TensorFlow Dataset type which allows building a highly optimized, asynchronous data pipeline. \n\nWe want the TPU to be fully loaded and the CPU to process the batches immediately, for that we use a prefetching technique at the end of the pipeline. This will always prepare the necessary number of batches(buffer) for the next need. We use automatically calculated buffer size based on batch size. The technique is used for training and validation datasets.","317bf5da":"## mBERT: Build the data pipeline <a id=\"7\"><\/a>","9458ae96":"The first step is to detect and initialize the TPU cores in the Kaggle Notebook. For this purpose, we should flip the **Accelerator** switch to the TPU in a notebook and bear in mind the time limits for using them (up to 30 hours per week).","166feea1":"---","fc55c066":"The main evaluation metric is an area under the ROC curve (ROC AUC). ","6ff8dd36":"We will try to in-depth describe aspects regarding fine-tuning and training: the model architecture, the experiment setup, hyperparameters choosing and all appropriate specificities.","f1948eed":"The final training process compromises two stages. The first stage includes training on the English dataset and validating on the relatively small multilingual dataset which contains Spanish, Italian, and Turkish. The second stage consists of training with the same hyperparameters on the validation dataset to expand the understanding of the toxic comments domain by non-English languages.\n \nAt first, the models were trained only on the English dataset, but after a while, it was investigated that just due to using validation data for additional training, we could strongly increase the results for both models up to 0.06 ROC AUC score. \n\nAssuming the models were pre-trained the same way, we use the same hyperparameters for an honest comparison. It should be noted that hyperparameters were selected based on mBERT, but XLM-RoBERTa showed the best result on them every time.","30ea4979":"The dense layer is added on top of the transformer layer. For solving the binary classification task, it has only one output neuron with the sigmoid activation function: \n\n\\begin{equation*} \n\\hat{y_{}}(z) =  \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-\\theta^Tz} },\n\\end{equation*}\n\nwhere $\\theta$ is a weights matrix of the last layer and $z$ is an output of the previous layer.\n\nThe binary cross-entropy function is our objective for minimizing:\n\n\\begin{equation*} \nJ(y_{i},\\hat{y_{i}}) = -\\frac{1}{N}\\sum_{i=1}^{N} \\left [ y_{i}\\ln(\\hat{y_{i}}) + (1-y_{i})\\ln(1-\\hat{y_{i}}) \\right ],\n\\end{equation*} \n\nwhere $y_{i}$ and $\\hat{y_{i}}$ are real and predicted toxic class for a comment $i$ in the batch respectively, $N$ is a batch size.\n\nFor updating network weights, we use the [Adam optimization algorithm](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/optimizers\/Adam) as a recommendation from [the original paper](https:\/\/arxiv.org\/abs\/1810.04805) instead of the regular stochastic gradient descent.","0505bb69":"In this section, we focus on fine-tuning the model for the binary classification task. In general, the mBERT is designed to solve a variety of NLP tasks such as sentence pair or single sentence classification tasks, question answering tasks, etc. For each task we have a different structure of inputs and outputs, to make it clear, Figure below shows the mBERT configuration for the toxic classification. We can see that the first input token is a special [CLS] token which stands for the classification task and because of which the model outputs only the first position. It should be noted that for non-classification tasks, there will be more output positions. \n\n![mbert.jpg](attachment:mbert.jpg)\n\nFigure demonstrates the mBERT model which has twelve Transformer layers (encoders). Further, each output position is a vector of size 768 hidden units. The vocabulary size is extended in comparison with monolingual BERT from 30K to 110K tokens. A total number of parameters is 172M. The maximum sequence length is 512 tokens.","0d44cb78":"## Methodology <a id=\"1\"><\/a>","18665576":"## XLM-RoBERTa: Preprocessing <a id=\"12\"><\/a>","1cc1083a":"We define a single function for building the transformer model in order to design the same fine-tuning architecture for both cases. The transformer layers are created by loading the pre-trained models:\n- **bert-base-multilingual-cased** - mBERT\n- **jplu\/tf-xlm-roberta-large** - XLM-RoBERTa","b8fce412":"* [Methodology](#1)\n* [Transfer learning](#2)\n* [TPU detection](#3)\n* [Loading the data](#4)\n* [mBERT](#5)\n> * [Preprocessing](#6)\n> * [Build the data pipeline](#7)\n> * [Define the model](#8)\n> * [Train the model](#9)\n> * [Evaluation and submission](#10)\n* [XLM-RoBERTa](#11)\n> * [Preprocessing](#12)\n> * [Build the data pipeline](#13)\n> * [Define the model](#14)\n> * [Train the model](#15)\n> * [Evaluation and submission](#16)","74a9fa80":"## mBERT: Define the model <a id=\"8\"><\/a>","12dc7ba8":"Instead of WordPiece, XLM-RoBERTa uses [SentencePiece](https:\/\/github.com\/google\/sentencepiece) tokenizer.\n\nFor classification task we add $<s>$ and $<\/s>$ tokens at the beginning and the end respectively. ","f07eb852":"## mBERT: Preprocessing <a id=\"6\"><\/a>","f815b9fe":"## Loading the data <a id=\"4\"><\/a>","a5a1752a":"A $batch\\_size\\_per\\_core$ hyperparameter we set to 32. I tried increasing it, as recommended in the official Kaggle documentation, to reduce the training time by loading more the TPU, but got a resource exhausted error for more than 32 units, which means a lack of random-access memory.","4a85f341":"## XLM-RoBERTa: Evaluation and submission <a id=\"16\"><\/a>"}}