{"cell_type":{"5b8ee66d":"code","953ce55b":"code","a886712d":"code","fee31768":"code","8039c4a4":"code","6d0c1e11":"code","0fe6fb51":"code","d660363d":"code","c7a639d7":"code","fc19cd20":"code","6c0d0eec":"code","fe85dbeb":"code","5bd8938f":"code","43a3beaf":"code","c79bad79":"code","543c2dca":"code","8eea6830":"code","e7739db3":"code","36142569":"code","5307089d":"code","bd6edafc":"code","c46201ed":"code","44e7a9dc":"markdown","998b5ee7":"markdown","cbddb589":"markdown","d7fffdf4":"markdown","5f422490":"markdown","90ec06bc":"markdown","b0f157f8":"markdown","0494bfdc":"markdown","806bc884":"markdown","b3743a02":"markdown","f627739f":"markdown","c52c5a42":"markdown","7a3a4eb3":"markdown","3420a9a0":"markdown","d0dc7ec3":"markdown","4e55a55c":"markdown","0b266c89":"markdown","bda1dc61":"markdown","95e8a2be":"markdown","c011a9e6":"markdown"},"source":{"5b8ee66d":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting\nimport seaborn as sns # plotting\nfrom sklearn.metrics import mean_squared_error # MSE metric\nfrom sklearn.preprocessing import OrdinalEncoder # ordinal encoding categorical variables\nfrom sklearn.model_selection import KFold\nfrom xgboost import XGBRegressor\n\nSEED = 91 # random seed\n\npd.set_option('display.max_columns', 100)","953ce55b":"PATH = '\/kaggle\/input\/30-days-of-ml\/' # you can use your own local path\n\nprint('Files in directory:')\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print('  '+os.path.join(dirname, filename))\nprint()\n\n# Load the training data\ntry:\n    df_train = pd.read_csv(PATH+'train.csv', index_col=0)\n    df_test = pd.read_csv(PATH+'test.csv', index_col=0)\n    submission = pd.read_csv(PATH+'sample_submission.csv', index_col=0)\n    print('All of the data has been loaded successfully!')\nexcept Exception as err:\n    print(repr(err))\nprint()","a886712d":"full_lenght_data = len(df_train) + len(df_test)\nprint(f\"{len(df_train)} ({100*len(df_train)\/full_lenght_data}%)\")\nprint(f\"{len(df_test)} ({100*len(df_test)\/full_lenght_data}%)\")","fee31768":"df_train.head()","8039c4a4":"df_train.info()","6d0c1e11":"df_test.info()","0fe6fb51":"CAT_FEATURES = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9']\nNUM_FEATURES = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8',\n                'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\nALL_FEATURES = CAT_FEATURES+NUM_FEATURES","d660363d":"df_full = pd.concat([df_test[CAT_FEATURES], df_train[CAT_FEATURES]]).sort_index()\ndf_full['test'] = 'train'\ndf_full.loc[df_test.index,'test'] = 'test'\ndf_full.head()","c7a639d7":"plt.figure(figsize=(20,8))\nplt.subplots_adjust(hspace=0.4, wspace=0.25)\nsns.set_palette(\"Set1\")\nfor i, col in enumerate(CAT_FEATURES):\n    plt.subplot(2, 5, i+1)\n    df = (df_full[col]\n     .groupby(df_full['test'])\n     .value_counts(normalize=True)\n     .rename('proprotion %')\n     .reset_index())\n    df['proprotion %'] = df['proprotion %']*100\n    g = sns.barplot(x=col, y=\"proprotion %\", data=df, hue='test')\n    plt.title(col)\n    g.set(xlabel='values')\nplt.show()","fc19cd20":"df_train[NUM_FEATURES].describe()","6c0d0eec":"sns.set_palette(\"Blues_r\")\nsns.pairplot(df_train[[\n             *NUM_FEATURES[:7],\n             'target']]\n            )","fe85dbeb":"sns.pairplot(df_train[[\n             *NUM_FEATURES[7:],\n             'target']],\n            )","5bd8938f":"df_train['target'].describe()","43a3beaf":"f,ax=plt.subplots(2,1,figsize=(12,8))\nsns.histplot(df_train['target'], ax=ax[0])\nsns.boxplot(x=df_train['target'], color='lightblue', saturation=0.8, ax=ax[1])\nplt.axvline(np.percentile(df_train['target'],.1), label='1%', c='orange', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(df_train['target'],.5), label='5%', c='darkblue', linestyle=':', linewidth=3)\nplt.legend()\nplt.xticks(np.arange(0,10.8,.5))\nplt.show()","c79bad79":"print(f\"Skewness: {df_train['target'].skew():.6f}\")","543c2dca":"corrMatrix = df_train.corr(method='pearson', min_periods=1)\nplt.figure(figsize=(12,10))\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corrMatrix, dtype=bool), k=1)\nax = sns.heatmap(corrMatrix, square=True, vmax=.3, annot=True, mask=mask, cbar_kws={\"shrink\": .5})\nplt.show()","8eea6830":"X = df_train.drop(['target'], axis=1)\ny = df_train['target']\n\nX_test = df_test.copy()\n\n# Preview features\nX.head()","e7739db3":"ordinal_encoder = OrdinalEncoder()\nX[CAT_FEATURES] = ordinal_encoder.fit_transform(X[CAT_FEATURES])\nX_test[CAT_FEATURES] = ordinal_encoder.transform(X_test[CAT_FEATURES])\n\nX.head()","36142569":"N_FOLD = 7\n\nkf = KFold(n_splits=N_FOLD, shuffle=True, random_state=SEED)","5307089d":"xgb_params = {\n    'booster': 'gbtree',\n    'n_estimators': 10000,\n    'learning_rate': 0.04,\n    'reg_lambda': 5,\n    'reg_alpha': 25.6,\n    'subsample': 0.93,\n    'colsample_bytree': 0.119,\n    'max_depth': 3,\n    'random_state': SEED \n}","bd6edafc":"predictions = 0\nmodel_fi = 0\nmean_rmse = 0\n\nfor num, (train_idx, valid_idx) in enumerate(kf.split(X)):\n    # Split the train data into train and validation\n    X_train = X.iloc[train_idx][ALL_FEATURES]\n    X_valid = X.iloc[valid_idx][ALL_FEATURES]\n    y_train = y.iloc[train_idx]\n    y_valid = y.iloc[valid_idx]\n    \n    model = XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_valid, y_valid)],\n             eval_metric = \"rmse\",\n             early_stopping_rounds = 50)\n    \n    # Mean of the predictions\n    predictions += model.predict(X_test) \/ N_FOLD\n    \n    # Mean of feature importance\n    model_fi += model.feature_importances_ \/ N_FOLD \n    \n    # Out of Fold predictions\n    pred_valid = model.predict(X_valid)\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, pred_valid))\n    print(f\"Fold {num} | RMSE: {fold_rmse}\")\n    \n    mean_rmse += fold_rmse \/ N_FOLD\n    \nprint(f\"\\nOverall RMSE: {mean_rmse}\")","c46201ed":"# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': df_test.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)","44e7a9dc":"# 3. Data preproccesing","998b5ee7":"### Correlation matrix","cbddb589":"Separate target from features","d7fffdf4":"## Categorical features\nCompare distribution on train and test data","5f422490":"### Import libraries","90ec06bc":"#### There's no significance linear correlation.","b0f157f8":"#### Ordinal-encode categorical columns","0494bfdc":"# 5. Submit predictions","806bc884":"# 4. Train model","b3743a02":"#### No missing values in dataset","f627739f":"# 2. Exploratory Data Analysis (EDA)","c52c5a42":"Add prediction on every step","7a3a4eb3":"# 1. Load data and first look","3420a9a0":"#### Build XGBoost Regressor","d0dc7ec3":"#### We observe a certain boundary of target values. Target has a litle right skewed distribution, but we have about some amount of otliers on the left tail (from 0.746555 to 6.9)","4e55a55c":"#### Variables of categorical features on test and train data distributed very similar","0b266c89":"## <center>30 Days of ML competition<\/center>\n### <center>XGBoost regressor with tuned hyperparameters<\/center>\n\n#### Competition:\nThis is the final competetion of [30 Days of ML program](https:\/\/www.kaggle.com\/thirty-days-of-ml)\n\n#### Dataset:\nThe dataset is used for this competition is synthetic (and generated using a CTGAN), but based on a real dataset. The original dataset deals with predicting the amount of an insurance claim. \n* 'cat0' - 'cat9' categorical features\n* 'cont0' - 'cont13' continuous features\n* 'target' - continous target","bda1dc61":"## Continuous features","95e8a2be":"## Target","c011a9e6":"#### Split data into folds"}}