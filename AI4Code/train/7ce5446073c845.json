{"cell_type":{"7f90de70":"code","7135280f":"code","9b23f9cd":"code","b7dba671":"code","8d8bc05d":"code","a24bbfe4":"code","8f8e3c91":"code","dabf04be":"code","6eaa7cd1":"code","aa0ba67e":"code","5dde0a3c":"code","16811f91":"code","33673f38":"code","6b02db79":"code","137a3d82":"code","076e24d3":"code","8e8504e2":"code","80adac2f":"code","b264e7a4":"code","6cc8739a":"code","b8331dbc":"code","d2961cca":"code","952c99cb":"code","fd003661":"code","b262877c":"code","5d1e3957":"code","e888e55f":"code","d91c9ea2":"code","f5edae61":"code","68ec0600":"code","82c6d534":"code","656705e6":"code","a923d885":"code","dee45d63":"code","6ee7ae1a":"code","d5c3938f":"code","47ae2ef1":"code","f99f89f7":"code","f5fc8e12":"code","8a7e3be1":"code","f7c16b25":"code","e69fc01c":"code","fc2e4f02":"code","29d5224e":"code","8d1c3ace":"code","cfe7e521":"code","8c209d5d":"code","3c23fcc6":"code","3bd4444e":"code","339116cd":"code","2ef8e736":"code","d5463246":"code","02fd8e3f":"code","05d57a82":"code","1d32f0ca":"code","0392b83d":"code","b585fece":"code","e2ed98f4":"code","b62318a6":"code","499b4533":"code","46a41376":"code","e097af63":"code","a2252b67":"code","bdc59daf":"code","166f51c8":"code","e69b8382":"code","5f7f1230":"code","2ccea78e":"code","e4ba1c3d":"markdown","27e5a4b2":"markdown","1d89ccaa":"markdown","74d78b41":"markdown","e50a46b0":"markdown","073e49b4":"markdown","3efe1679":"markdown","58dd906e":"markdown","3bb27036":"markdown","274952b1":"markdown","e7323363":"markdown","51f37f3e":"markdown","3272b493":"markdown"},"source":{"7f90de70":"# Import packages\n# Basic packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport pickle\nfrom math import floor\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Evaluation and bayesian optimization\nfrom sklearn.metrics import make_scorer, mean_absolute_error\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom hyperopt import hp, fmin, tpe\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom bayes_opt import BayesianOptimization\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option(\"display.max_columns\", None)","7135280f":"# Make scorer: MSE\nmse = make_scorer(MSE, greater_is_better=False)","9b23f9cd":"# Load dataset\ntrainSet = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv').drop(columns=['Id'])\ntestSet = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv').drop(columns=['Id'])\nsubmitSet = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\n\ntrainSet.head()","b7dba671":"trainSet.info()","8d8bc05d":"# Drop columns with too much NA\ntrain = trainSet.drop(columns=['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis=0)\n\n# Fill missing value with median or most occurance value\ntrain['LotFrontage'] = train['LotFrontage'].fillna(train['LotFrontage'].median())\ntrain['MasVnrType'] = train['MasVnrType'].fillna(train['MasVnrType'].value_counts().idxmax())\ntrain['MasVnrArea'] = train['MasVnrArea'].fillna(train['MasVnrArea'].median())\ntrain['MasVnrType'] = train['MasVnrType'].fillna(train['MasVnrType'].value_counts().idxmax())\ntrain['BsmtQual'] = train['BsmtQual'].fillna(train['BsmtQual'].value_counts().idxmax())\ntrain['BsmtCond'] = train['BsmtCond'].fillna(train['BsmtCond'].value_counts().idxmax())\ntrain['BsmtExposure'] = train['BsmtExposure'].fillna(train['BsmtExposure'].value_counts().idxmax())\ntrain['BsmtFinType1'] = train['BsmtFinType1'].fillna(train['BsmtFinType1'].value_counts().idxmax())\ntrain['BsmtFinType2'] = train['BsmtFinType2'].fillna(train['BsmtFinType2'].value_counts().idxmax())\ntrain['Electrical'] = train['Electrical'].fillna(train['Electrical'].value_counts().idxmax())\ntrain['GarageType'] = train['GarageType'].fillna(train['GarageType'].value_counts().idxmax())\ntrain['GarageYrBlt'] = train['GarageYrBlt'].fillna(train['GarageYrBlt'].median())\ntrain['GarageFinish'] = train['GarageFinish'].fillna(train['GarageFinish'].value_counts().idxmax())\ntrain['GarageQual'] = train['GarageQual'].fillna(train['GarageQual'].value_counts().idxmax())\ntrain['GarageCond'] = train['GarageCond'].fillna(train['GarageCond'].value_counts().idxmax())\n\ntrain.info()","a24bbfe4":"# Encode categorical val\ntrain = pd.get_dummies(train)\ntrain.head()","8f8e3c91":"# train validation split\nX_train0, X_val0, y_train0, y_val0 = train_test_split(train.drop(columns=['SalePrice'], axis=0),\n                                                  train['SalePrice'],\n                                                  test_size=0.2, random_state=123)","dabf04be":"import xgboost as xgb\n\n# Feature selection with XGBoost\nselection =  xgb.XGBRegressor(random_state=123, nthread=-1)\nselection.fit(X_train0, y_train0)\nsele_pred = selection.predict(X_val0)\nprint('RMSE: ' + str(MSE(y_val0, sele_pred)**0.5))\nprint('SD: ' + str(train['SalePrice'].std()))","6eaa7cd1":"# Feature importances\nFeature_sel = pd.DataFrame({'feature':X_train0.columns,\n                            'importance':list(selection.feature_importances_)}).sort_values('importance').reset_index(drop=True)\nplt.figure(figsize=(16,4))\nsns.barplot(data=Feature_sel.iloc[-20:,], x='feature', y='importance')\nselected = Feature_sel.iloc[-20:,]['feature']\nplt.xticks(rotation=90)\nplt.show()","aa0ba67e":"selected = ['BsmtQual_Gd', 'SaleType_New', 'Fireplaces', 'BsmtExposure_Gd', '2ndFlrSF',\n            'BsmtFinType1_GLQ', 'BsmtFinSF1', 'LowQualFinSF', 'GarageType_Detchd',\n            'KitchenAbvGr', 'MSZoning_RM', 'TotalBsmtSF', 'CentralAir_N', 'GrLivArea',\n            'KitchenQual_Ex', 'Exterior2nd_CmentBd', 'GarageCars', 'LandSlope_Gtl',\n            'BsmtQual_Ex', 'OverallQual', 'SalePrice']","5dde0a3c":"train_sel = train.loc[:, selected]\n\n# train validation split\nX_train, X_val, y_train, y_val = train_test_split(train_sel.drop(columns=['SalePrice'], axis=0),\n                                                  train_sel['SalePrice'],\n                                                  test_size=0.2, random_state=123)","16811f91":"# Scaling\nscaler = MinMaxScaler()\nX_trainS = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n\nX_valS = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns)","33673f38":"from sklearn.linear_model import LinearRegression","6b02db79":"# Hyperparameter-tuning: Grid Search\nfit_intercept = [True, False]\nparam_line={'fit_intercept':fit_intercept}\nline_grid = GridSearchCV(estimator=LinearRegression(), param_grid=param_line,\n                         scoring=mse, cv=5)\n\nline_grid.fit(X_trainS, y_train)\n\nprint('Best score: ' + str(((line_grid.best_score_)*-1)**0.5))\nprint('Best parameter {}'.format(line_grid.best_params_))","137a3d82":"# Fit the training data\nline_hyp = LinearRegression(fit_intercept=line_grid.best_params_['fit_intercept'])\nline_hyp.fit(X_trainS, y_train)\n\n# Predict the validation data\npred_line = line_hyp.predict(X_valS)\n\n# Compute the RMSE\nrmse_line = (MSE(y_val, pred_line))**0.5\n\n# Scatter plot true and predicted values\nplt.scatter(pred_line, y_val, alpha=0.2)\nplt.xlabel('predicted')\nplt.ylabel('true value')\nplt.text(100000, 400000, 'RMSE: ' + str(round(rmse_line)))\nplt.text(100000, 350000, 'MAE: ' + str(round(mean_absolute_error(pred_line, y_val))))\nplt.text(100000, 300000, 'R: ' + str(round(np.corrcoef(pred_line, y_val)[0,1],2)))\nplt.show()","076e24d3":"# Features Coefficients\nFeature_line = pd.DataFrame({'features': X_train.columns, 'coefficient':[i for i in list(line_hyp.coef_)]}).sort_values('coefficient')\nplt.figure(figsize=(16,4))\nsns.barplot(data=Feature_line, x='features', y='coefficient')\nplt.xticks(rotation=90)\nplt.show()","8e8504e2":"from sklearn.linear_model import  Ridge","80adac2f":"# Hyperparameter-tuning: Bayesian Optimization, bayes_opt\ndef ridge_re_bo(alpha, fit_intercept, solver):\n    params_ridge = {}\n    \n    fit_interceptL = [True, False]\n    solverL = ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'saga']\n    \n    params_ridge['alpha'] = alpha\n    params_ridge['fit_intercept'] = fit_interceptL[round(fit_intercept)]\n    params_ridge['solver'] = solverL[floor(solver)]\n    \n    scores = cross_val_score(Ridge(random_state=123, **params_ridge),\n                             X_trainS, y_train, scoring='neg_mean_squared_error', cv=5).mean()\n    score = ((scores*-1)**0.5)*-1\n    return score\n\n# Set hyperparameters spaces\nparams_ridge ={\n    'alpha':(0.01, 10),\n    'fit_intercept':(0, 1),\n    'solver':(0, 7)\n}\n\n# Run Bayesian Optimization\nridge_bo = BayesianOptimization(ridge_re_bo, params_ridge, random_state=123)\nridge_bo.maximize(init_points=4, n_iter=25)","b264e7a4":"# Best hyperparameters\nparams_ridge = ridge_bo.max['params']\n\nfit_interceptL = [True, False]\nsolverL = ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n\nparams_ridge['fit_intercept'] = fit_interceptL[round(params_ridge['fit_intercept'])]\nparams_ridge['solver'] = solverL[round(params_ridge['solver'])]\nparams_ridge","6cc8739a":"# Fit the training data\nridge_hyp =  Ridge(**params_ridge, random_state=123)\nridge_hyp.fit(X_trainS, y_train)\n\n# Predict the validation data\npred_ridge = ridge_hyp.predict(X_valS)\n\n# Compute the RMSE\nrmse_ridge = MSE(y_val, pred_ridge)**0.5\n\n# Scatter plot true and predicted values\nplt.scatter(pred_ridge, y_val, alpha=0.2)\nplt.xlabel('predicted')\nplt.ylabel('true value')\nplt.text(100000, 400000, 'RMSE: ' + str(round(rmse_ridge)))\nplt.text(100000, 350000, 'MAE: ' + str(round(mean_absolute_error(pred_ridge, y_val))))\nplt.text(100000, 300000, 'R: ' + str(round(np.corrcoef(pred_ridge, y_val)[0,1],2)))\nplt.show()","b8331dbc":"# Features Coefficients\nFeature_ridge = pd.DataFrame({'features': X_train.columns, 'coefficient':[i for i in list(ridge_hyp.coef_)]}).sort_values('coefficient')\nplt.figure(figsize=(16,4))\nsns.barplot(data=Feature_ridge, x='features', y='coefficient')\nplt.xticks(rotation=90)\nplt.show()","d2961cca":"from sklearn.linear_model import Lasso","952c99cb":"# Hyperparameter-tuning: Bayesian Optimization, bayes_opt\ndef lasso_re_bo(alpha, fit_intercept, precompute, selection):\n    params_lasso = {}\n    \n    fit_interceptL = [True, False]\n    precomputeL = [True, False]\n    selectionL = ['cyclic', 'random']\n    \n    params_lasso['alpha'] = alpha\n    params_lasso['fit_intercept'] = fit_interceptL[round(fit_intercept)]\n    params_lasso['precompute'] = precomputeL[round(precompute)]\n    params_lasso['selection'] = selectionL[round(selection)]\n    \n    scores = cross_val_score(Lasso(random_state=123, **params_lasso),\n                             X_trainS, y_train, scoring='neg_mean_squared_error', cv=5).mean()\n    score = ((scores*-1)**0.5)*-1\n    return score\n\n# Set hyperparameters spaces\nparams_lasso ={\n    'alpha':(0.01, 10),\n    'fit_intercept':(0, 1),\n    'precompute':(0, 1),\n    'selection':(0, 1)\n}\n\n# Run Bayesian Optimization\nlasso_bo = BayesianOptimization(lasso_re_bo, params_lasso, random_state=123)\nlasso_bo.maximize(init_points=4, n_iter=25)","fd003661":"# Best hyperparameters\nparams_lasso = lasso_bo.max['params']\n\nfit_interceptL = [True, False]\nprecomputeL = [True, False]\nselectionL = ['cyclic', 'random']\n\nparams_lasso['fit_intercept'] = fit_interceptL[round(params_lasso['fit_intercept'])]\nparams_lasso['precompute'] = precomputeL[round(params_lasso['precompute'])]\nparams_lasso['selection'] = selectionL[round(params_lasso['selection'])]\nparams_lasso","b262877c":"# Fit the training data\nlasso_hyp =  Lasso(**params_lasso, random_state=123)\nlasso_hyp.fit(X_trainS, y_train)\n\n# Predict the validation data\npred_lasso = lasso_hyp.predict(X_valS)\n\n# Compute the RMSE\nrmse_lasso = MSE(y_val, pred_lasso)**0.5\n\n# Scatter plot true and predicted values\nplt.scatter(pred_lasso, y_val, alpha=0.2)\nplt.xlabel('predicted')\nplt.ylabel('true value')\nplt.text(100000, 400000, 'RMSE: ' + str(round(rmse_lasso)))\nplt.text(100000, 350000, 'MAE: ' + str(round(mean_absolute_error(pred_lasso, y_val))))\nplt.text(100000, 300000, 'R: ' + str(round(np.corrcoef(pred_lasso, y_val)[0,1],2)))\nplt.show()","5d1e3957":"# Features Coefficients\nFeature_lasso = pd.DataFrame({'features': X_train.columns, 'coefficient':[i for i in list(lasso_hyp.coef_)]}).sort_values('coefficient')\nplt.figure(figsize=(16,4))\nsns.barplot(data=Feature_lasso, x='features', y='coefficient')\nplt.xticks(rotation=90)\nplt.show()","e888e55f":"from sklearn.linear_model import ElasticNet","d91c9ea2":"# Hyperparameter-tuning: Bayesian Optimization, bayes_opt\ndef net_re_bo(alpha, l1_ratio, fit_intercept, precompute, selection):\n    params_net = {}\n    \n    fit_interceptL = [True, False]\n    precomputeL = [True, False]\n    selectionL = ['cyclic', 'random']\n    \n    params_net['alpha'] = alpha\n    params_net['l1_ratio'] = l1_ratio\n    params_net['fit_intercept'] = fit_interceptL[round(fit_intercept)]\n    params_net['precompute'] = precomputeL[round(precompute)]\n    params_net['selection'] = selectionL[round(selection)]\n    \n    scores = cross_val_score(ElasticNet(random_state=123, **params_net),\n                             X_trainS, y_train, scoring='neg_mean_squared_error', cv=5).mean()\n    score = ((scores*-1)**0.5)*-1\n    return score\n\n# Set hyperparameters spaces\nparams_net ={\n    'alpha':(0.01, 10),\n    'l1_ratio':(0,1),\n    'fit_intercept':(0, 1),\n    'precompute':(0, 1),\n    'selection':(0, 1)\n}\n\n# Run Bayesian Optimization\nnet_bo = BayesianOptimization(net_re_bo, params_net, random_state=123)\nnet_bo.maximize(init_points=4, n_iter=25)","f5edae61":"# Best hyperparameters\nparams_net = net_bo.max['params']\n\nfit_interceptL = [True, False]\nprecomputeL = [True, False]\nselectionL = ['cyclic', 'random']\n\nparams_net['fit_intercept'] = fit_interceptL[round(params_net['fit_intercept'])]\nparams_net['precompute'] = precomputeL[round(params_net['precompute'])]\nparams_net['selection'] = selectionL[round(params_net['selection'])]\nparams_net","68ec0600":"# Fit the training data\nnet_hyp =  ElasticNet(**params_net, random_state=123)\nnet_hyp.fit(X_trainS, y_train)\n\n# Predict the validation data\npred_net = net_hyp.predict(X_valS)\n\n# Compute the RMSE\nrmse_net = MSE(y_val, pred_net)**0.5\n\n# Scatter plot true and predicted values\nplt.scatter(pred_net, y_val, alpha=0.2)\nplt.xlabel('predicted')\nplt.ylabel('true value')\nplt.text(100000, 400000, 'RMSE: ' + str(round(rmse_net)))\nplt.text(100000, 350000, 'MAE: ' + str(round(mean_absolute_error(pred_net, y_val))))\nplt.text(100000, 300000, 'R: ' + str(round(np.corrcoef(pred_net, y_val)[0,1],2)))\nplt.show()","82c6d534":"# Features Coefficients\nFeature_net = pd.DataFrame({'features': X_train.columns, 'coefficient':[i for i in list(net_hyp.coef_)]}).sort_values('coefficient')\nplt.figure(figsize=(16,4))\nsns.barplot(data=Feature_net, x='features', y='coefficient')\nplt.xticks(rotation=90)\nplt.show()","656705e6":"from sklearn.neighbors import KNeighborsRegressor","a923d885":"# Hyperparameter-tuning: Bayesian Optimization, bayes_opt\ndef knn_re_bo(n_neighbors, weights, algorithm):\n    params_knn = {}\n    \n    weightsL = ['uniform', 'distance']\n    algorithmL = ['auto', 'ball_tree', 'kd_tree', 'brute', 'brute']\n    \n    params_knn['n_neighbors'] = round(n_neighbors)\n    params_knn['weights'] = weightsL[round(weights)]\n    params_knn['algorithm'] = algorithmL[round(algorithm)]\n    params_knn['n_jobs'] = -1\n    \n    scores = cross_val_score(KNeighborsRegressor(**params_knn),\n                             X_trainS, y_train, scoring='neg_mean_squared_error', cv=5).mean()\n    score = ((scores*-1)**0.5)*-1\n    return score\n\n# Set hyperparameters spaces\nparams_knn ={\n    'n_neighbors':(3, 10),\n    'weights':(0,1),\n    'algorithm':(0, 4)\n}\n\n# Run Bayesian Optimization\nknn_bo = BayesianOptimization(knn_re_bo, params_knn, random_state=123)\nknn_bo.maximize(init_points=4, n_iter=25)","dee45d63":"# Best hyperparameters\nparams_knn = knn_bo.max['params']\n\nweightsL = ['uniform', 'distance']\nalgorithmL = ['auto', 'ball_tree', 'kd_tree', 'brute', 'brute']\n\nparams_knn['n_neighbors'] = round(params_knn['n_neighbors'])\nparams_knn['weights'] = weightsL[round(params_knn['weights'])]\nparams_knn['algorithm'] = algorithmL[floor(params_knn['algorithm'])]\nparams_knn['n_jobs'] = -1\nparams_knn","6ee7ae1a":"# Fit the training data\nknn_hyp =  KNeighborsRegressor(**params_knn)\nknn_hyp.fit(X_trainS, y_train)\n\n# Predict the validation data\npred_knn = knn_hyp.predict(X_valS)\n\n# Compute the RMSE\nrmse_knn = MSE(y_val, pred_knn)**0.5\n\n# Scatter plot true and predicted values\nplt.scatter(pred_knn, y_val, alpha=0.2)\nplt.xlabel('predicted')\nplt.ylabel('true value')\nplt.text(100000, 400000, 'RMSE: ' + str(round(rmse_knn)))\nplt.text(100000, 350000, 'MAE: ' + str(round(mean_absolute_error(pred_knn, y_val))))\nplt.text(100000, 300000, 'R: ' + str(round(np.corrcoef(pred_knn, y_val)[0,1],2)))\nplt.show()","d5c3938f":"from sklearn.svm import SVR","47ae2ef1":"# Hyperparameter-tuning: Bayesian Optimization, bayes_opt\ndef svm_re_bo(C, epsilon):\n    params_svm = {}\n    params_svm['C'] = C\n    params_svm['epsilon'] = epsilon\n    \n    scores = cross_val_score(SVR(**params_svm),\n                             X_trainS, y_train, scoring='neg_mean_squared_error', cv=5).mean()\n    score = ((scores*-1)**0.5)*-1\n    return score\n\n# Set hyperparameters spaces\nparams_svm ={\n    #'kernel':(0, 5),\n    'C':(0.01, 10),\n    'epsilon':(0.01, 1)\n}\n\n# Run Bayesian Optimization\nsvm_bo = BayesianOptimization(svm_re_bo, params_svm)\nsvm_bo.maximize(init_points=4, n_iter=25)","f99f89f7":"# Best hyperparameters\nparams_svm = svm_bo.max['params']\nparams_svm","f5fc8e12":"# Fit the training data\nsvm_hyp =  SVR(**params_svm)\nsvm_hyp.fit(X_trainS, y_train)\n\n# Predict the validation data\npred_svm = svm_hyp.predict(X_valS)\n\n# Compute the RMSE\nrmse_svm = MSE(y_val, pred_svm)**0.5\n\n# Scatter plot true and predicted values\nplt.scatter(pred_svm, y_val, alpha=0.2)\nplt.xlabel('predicted')\nplt.ylabel('true value')\nplt.text(161000, 400000, 'RMSE: ' + str(round(rmse_svm)))\nplt.text(161000, 350000, 'MAE: ' + str(round(mean_absolute_error(pred_svm, y_val))))\nplt.text(161000, 300000, 'R: ' + str(round(np.corrcoef(pred_svm, y_val)[0,1],2)))\nplt.show()\n# SVM is far from accurate for this dataset","8a7e3be1":"from sklearn.tree import DecisionTreeRegressor","f7c16b25":"# Hyperparameter-tuning: Bayesian Optimization, bayes_opt\ndef dt_re_bo(splitter, max_depth, min_samples_split, min_samples_leaf, max_features):\n    params_dt = {}\n    \n    splitterL = ['best', 'random']\n    max_featuresL = [None, 'auto', 'sqrt', 'log2', 'log2']\n    \n    params_dt['splitter'] = splitterL[round(splitter)]\n    params_dt['max_depth'] = round(max_depth)\n    params_dt['min_samples_split'] = round(min_samples_split)\n    params_dt['min_samples_leaf'] = round(min_samples_leaf)\n    params_dt['max_features'] = max_featuresL[floor(max_features)]\n    \n    scores = cross_val_score(DecisionTreeRegressor(random_state=123, **params_dt),\n                             X_train, y_train, scoring=mse, cv=5).mean()\n    score = ((scores*-1)**0.5)*-1\n    return score\n\n# Set hyperparameters spaces\nparams_dt ={\n    'splitter':(0, 1),\n    'max_depth':(4, 20),\n    'min_samples_split':(1, 10),\n    'min_samples_leaf':(1, 10),\n    'max_features': (0, 5)\n}\n\n# Run Bayesian Optimization\ndt_bo = BayesianOptimization(dt_re_bo, params_dt, random_state=123)\ndt_bo.maximize(init_points=4, n_iter=25)","e69fc01c":"# Best hyperparameters\nparams_dt = dt_bo.max['params']\n\nsplitterL = ['best', 'random']\nmax_featuresL = [None, 'auto', 'sqrt', 'log2', 'log2']\n    \nparams_dt['splitter'] = splitterL[round(params_dt['splitter'])]\nparams_dt['max_depth'] = round(params_dt['max_depth'])\nparams_dt['min_samples_split'] = round(params_dt['min_samples_split'])\nparams_dt['min_samples_leaf'] = round(params_dt['min_samples_leaf'])\nparams_dt['max_features'] = max_featuresL[floor(params_dt['max_features'])]\nparams_dt","fc2e4f02":"# Fit the training data\ndt_hyp =  DecisionTreeRegressor(**params_dt, random_state=123)\ndt_hyp.fit(X_train, y_train)\n\n# Predict the validation data\npred_dt = dt_hyp.predict(X_val)\n\n# Compute the RMSE\nrmse_dt = MSE(y_val, pred_dt)**0.5\n\n# Scatter plot true and predicted values\nplt.scatter(pred_dt, y_val, alpha=0.2)\nplt.xlabel('predicted')\nplt.ylabel('true value')\nplt.text(100000, 400000, 'RMSE: ' + str(round(rmse_dt)))\nplt.text(100000, 350000, 'MAE: ' + str(round(mean_absolute_error(pred_dt, y_val))))\nplt.text(100000, 300000, 'R: ' + str(round(np.corrcoef(pred_dt, y_val)[0,1],2)))\nplt.show()","29d5224e":"# Feature importances\nFeature_dt = pd.DataFrame({'feature':X_train.columns, 'importance':list(dt_hyp.feature_importances_)}).sort_values('importance')\nplt.figure(figsize=(16,4))\nsns.barplot(data=Feature_dt, x='feature', y='importance')\nplt.xticks(rotation=90)\nplt.show()","8d1c3ace":"from sklearn.ensemble import RandomForestRegressor","cfe7e521":"# Hyperparameter-tuning: Bayesian Optimization, bayes_opt\ndef rf_re_bo(n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features):\n    params_rf = {}\n        \n    params_rf['n_estimators'] = round(n_estimators)\n    params_rf['max_depth'] = round(max_depth)\n    params_rf['min_samples_split'] = round(min_samples_split)\n    params_rf['min_samples_leaf'] = round(min_samples_leaf)\n    params_rf['max_features'] = float(max_features)\n    \n    scores = cross_val_score(RandomForestRegressor(random_state=123, **params_rf),\n                             X_train, y_train, scoring=mse, cv=5).mean()\n    score = ((scores*-1)**0.5)*-1\n    return score\n\n# Set hyperparameters spaces\nparams_rf ={\n    'n_estimators':(70, 150),\n    'max_depth':(5, 20),\n    'min_samples_split':(2, 10),\n    'min_samples_leaf': (2, 10),\n    'max_features': (0.8,1)\n}\n\n# Run Bayesian Optimization\nrf_bo = BayesianOptimization(rf_re_bo, params_rf, random_state=123)\nrf_bo.maximize(init_points=4, n_iter=25)","8c209d5d":"# Best hyperparameters\nparams_rf = rf_bo.max['params']\n\nparams_rf['n_estimators'] = round(params_rf['n_estimators'])\nparams_rf['max_depth'] = round(params_rf['max_depth'])\nparams_rf['min_samples_split'] = round(params_rf['min_samples_split'])\nparams_rf['min_samples_leaf'] = round(params_rf['min_samples_leaf'])\nparams_rf['max_features'] = float(params_rf['max_features'])\nparams_rf","3c23fcc6":"# Fit the training data\nrf_hyp =  RandomForestRegressor(**params_rf, random_state=123)\nrf_hyp.fit(X_train, y_train)\n\n# Predict the validation data\npred_rf = rf_hyp.predict(X_val)\n\n# Compute the RMSE\nrmse_rf = MSE(y_val, pred_rf)**0.5\n\n# Scatter plot true and predicted values\nplt.scatter(pred_rf, y_val, alpha=0.2)\nplt.xlabel('predicted')\nplt.ylabel('true value')\nplt.text(100000, 400000, 'RMSE: ' + str(round(rmse_rf)))\nplt.text(100000, 350000, 'MAE: ' + str(round(mean_absolute_error(pred_rf, y_val))))\nplt.text(100000, 300000, 'R: ' + str(round(np.corrcoef(pred_rf, y_val)[0,1],2)))\nplt.show()","3bd4444e":"# Feature importances\nFeature_rf = pd.DataFrame({'feature':X_train.columns, 'importance':list(rf_hyp.feature_importances_)}).sort_values('importance')\nplt.figure(figsize=(16,4))\nsns.barplot(data=Feature_rf, x='feature', y='importance')\nplt.xticks(rotation=90)\nplt.show()","339116cd":"from sklearn.ensemble import GradientBoostingRegressor","2ef8e736":"# Hyperparameter-tuning: Bayesian Optimization, bayes_opt\ndef gbm_re_bo(learning_rate, n_estimators, subsample, max_depth, max_features):\n    params_gbm = {}\n    params_gbm['learning_rate'] = learning_rate\n    params_gbm['n_estimators'] = round(n_estimators)\n    params_gbm['subsample'] = subsample\n    params_gbm['max_depth'] = round(max_depth)\n    params_gbm['max_features'] = float(max_features)\n    \n    scores = cross_val_score(GradientBoostingRegressor(random_state=123, **params_gbm),\n                             X_train, y_train, scoring=mse, cv=5).mean()\n    score = ((scores*-1)**0.5)*-1\n    return score\n\n# Set hyperparameters spaces\nparams_gbm ={\n    'learning_rate':(0.01, 1),\n    'n_estimators':(80, 150),\n    'subsample': (0.8, 1),\n    'max_depth':(5, 20),\n    'max_features':(0.8, 1)\n}\n\n# Run Bayesian Optimization\ngbm_bo = BayesianOptimization(gbm_re_bo, params_gbm, random_state=123)\ngbm_bo.maximize(init_points=4, n_iter=25)","d5463246":"# Best hyperparameters\nparams_gbm = gbm_bo.max['params']\nparams_gbm['max_depth'] = round(params_gbm['max_depth'])\nparams_gbm['n_estimators'] = round(params_gbm['n_estimators'])\nparams_gbm['max_features'] = float(params_gbm['max_features'])\nparams_gbm","02fd8e3f":"# Fit the training data\ngbm_hyp =  GradientBoostingRegressor(**params_gbm, random_state=123)\ngbm_hyp.fit(X_train, y_train)\n\n# Predict the validation data\npred_gbm = gbm_hyp.predict(X_val)\n\n# Compute the RMSE\nrmse_gbm = MSE(y_val, pred_gbm)**0.5\n\n# Scatter plot true and predicted values\nplt.scatter(pred_gbm, y_val, alpha=0.2)\nplt.xlabel('predicted')\nplt.ylabel('true value')\nplt.text(100000, 400000, 'RMSE: ' + str(round(rmse_gbm)))\nplt.text(100000, 350000, 'MAE: ' + str(round(mean_absolute_error(pred_gbm, y_val))))\nplt.text(100000, 300000, 'R: ' + str(round(np.corrcoef(pred_gbm, y_val)[0,1],2)))\nplt.show()","05d57a82":"# Feature importances\nFeature_gbm = pd.DataFrame({'feature':X_train.columns, 'importance':list(gbm_hyp.feature_importances_)}).sort_values('importance')\nplt.figure(figsize=(16,4))\nsns.barplot(data=Feature_gbm, x='feature', y='importance')\nplt.xticks(rotation=90)\nplt.show()","1d32f0ca":"from lightgbm import LGBMRegressor","0392b83d":"# Hyperparameter-tuning: Bayesian Optimization, bayes_opt\ndef lgbm_re_bo(num_leaves, max_depth, learning_rate, min_child_weight, subsample, colsample_bytree):\n    params_lgbm = {'objective': 'regression'}\n    params_lgbm['num_leaves'] = round((2**round(max_depth))*num_leaves)\n    params_lgbm['max_depth'] = round(max_depth)\n    params_lgbm['learning_rate'] = learning_rate\n    params_lgbm['min_child_weight'] = min_child_weight\n    params_lgbm['subsample'] = subsample\n    params_lgbm['colsample_bytree'] = colsample_bytree\n    \n    scores = cross_val_score(LGBMRegressor(random_state=123, **params_lgbm),\n                            X_train, y_train, scoring=mse, cv=5).mean()\n    score = ((scores*-1)**0.5)*-1\n    return score\n\n# Set parameters distribution\nparams_lgbm ={\n    'num_leaves':(0.5,0.9),\n    'max_depth': (3, 15),\n    'learning_rate': (0.01, 0.5),\n    'min_child_weight':(1e-5, 1e-1),\n    'subsample':(0.5, 1),\n    'colsample_bytree':(0.5, 1)\n}\n\n# Run Bayesian Optimization\nlgbm_bo = BayesianOptimization(lgbm_re_bo, params_lgbm, random_state=123)\nlgbm_bo.maximize(init_points=2, n_iter=25)","b585fece":"# Best hyperparameters\nparams_lgbm = lgbm_bo.max['params']\nparams_lgbm['max_depth'] = round(params_lgbm['max_depth'])\nparams_lgbm['num_leaves'] = round((2**round(params_lgbm['max_depth']))*params_lgbm['num_leaves'])\nparams_lgbm","e2ed98f4":"# Fit the training data\nlgbm_hyp =  LGBMRegressor(**params_lgbm, random_state=123)\nlgbm_hyp.fit(X_train, y_train)\n\n# Predict the validation data\npred_lgbm = lgbm_hyp.predict(X_val)\n\n# Compute the RMSE\nrmse_lgbm = MSE(y_val, pred_lgbm)**0.5\n\n# Scatter plot true and predicted values\nplt.scatter(pred_lgbm, y_val, alpha=0.2)\nplt.xlabel('predicted')\nplt.ylabel('true value')\nplt.text(100000, 400000, 'RMSE: ' + str(round(rmse_lgbm)))\nplt.text(100000, 350000, 'MAE: ' + str(round(mean_absolute_error(pred_lgbm, y_val))))\nplt.text(100000, 300000, 'R: ' + str(round(np.corrcoef(pred_lgbm, y_val)[0,1],2)))\nplt.show()","b62318a6":"# Feature importances\nFeature_lgbm = pd.DataFrame({'feature':X_train.columns, 'importance':list(lgbm_hyp.feature_importances_)}).sort_values('importance')\nplt.figure(figsize=(16,4))\nsns.barplot(data=Feature_lgbm, x='feature', y='importance')\nplt.xticks(rotation=90)\nplt.show()","499b4533":"from xgboost import XGBRegressor","46a41376":"# Hyperparameter tuning: Bayesian Optimization\ndef xgb_re_bo(learning_rate, gamma, max_depth, min_child_weight, subsample, colsample_bytree):\n    params_xgb = {\n    'objective': 'reg:squarederror',\n    'eval_metric':'rmse',\n    'nthread':-1\n     }\n    \n    params_xgb['learning_rate'] = learning_rate\n    params_xgb['gamma'] = gamma\n    params_xgb['max_depth'] = round(max_depth)\n    params_xgb['min_child_weight'] = round(min_child_weight)\n    params_xgb['subsample'] = subsample\n    params_xgb['colsample_bytree'] = colsample_bytree\n    \n    scores = cross_val_score(XGBRegressor(random_state=123, **params_xgb),\n                            X_train, y_train, scoring=mse, cv=5).mean()\n    score = ((scores*-1)**0.5)*-1\n    return score\n\n# Set parameters distribution\nparams_xgb ={\n    'learning_rate': (0.01, 0.5),\n    'gamma':(0, 10),\n    'max_depth': (3, 15),\n    'min_child_weight':(3, 10),\n    'subsample':(0.5, 1),\n    'colsample_bytree':(0.1, 1)\n}\n\n# Run Bayesian Optimization\nxgb_bo = BayesianOptimization(xgb_re_bo, params_xgb, random_state=123)\nxgb_bo.maximize(init_points=4, n_iter=25)","e097af63":"# Best hyperparameters\nparams_xgb = xgb_bo.max['params']\nparams_xgb['objective'] = 'reg:squarederror'\nparams_xgb['eval_metric'] = 'rmse'\nparams_xgb['n_jobs'] = -1\nparams_xgb['max_depth'] = round(params_xgb['max_depth'])\nparams_xgb['min_child_weight'] = round(params_xgb['min_child_weight'])\nxgb_bo.max['params']","a2252b67":"# Fit the training data\nxgb_hyp =  XGBRegressor(**params_xgb, random_state=123)\nxgb_hyp.fit(X_train, y_train)\n\n# Predict the validation data\npred_xgb = xgb_hyp.predict(X_val)\n\n# Compute the RMSE\nrmse_xgb = MSE(y_val, pred_xgb)**0.5\n\n# Scatter plot true and predicted values\nplt.scatter(pred_xgb, y_val, alpha=0.2)\nplt.xlabel('predicted')\nplt.ylabel('true value')\nplt.text(100000, 400000, 'RMSE: ' + str(round(rmse_xgb)))\nplt.text(100000, 350000, 'MAE: ' + str(round(mean_absolute_error(pred_xgb, y_val))))\nplt.text(100000, 300000, 'R: ' + str(round(np.corrcoef(pred_xgb, y_val)[0,1],2)))\nplt.show()","bdc59daf":"# Feature importances\nFeature_xgb = pd.DataFrame({'feature':X_train.columns, 'importance':list(xgb_hyp.feature_importances_)}).sort_values('importance')\nplt.figure(figsize=(16,4))\nsns.barplot(data=Feature_xgb, x='feature', y='importance')\nplt.xticks(rotation=90)\nplt.show()","166f51c8":"# Deep Learning packages\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization, Dropout\nfrom keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.layers import LeakyReLU\nLeakyReLU = LeakyReLU(alpha=0.1)","e69b8382":"# Hyperparameter-tuning: Bayesian Optimization, bayes_opt\ndef nn_re_bo(neurons, activation, optimizer, batch_size, epochs,\n             layers1, layers2, dropout, dropout_rate):\n    optimizerL = ['Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD', 'SGD']\n    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n                   'elu', 'exponential', LeakyReLU, LeakyReLU]\n        \n    neurons = round(neurons)\n    activation = activationL[floor(activation)]\n    optimizer = optimizerL[floor(optimizer)]\n    batch_size = round(batch_size)\n    epochs = round(epochs)\n    layers1 = round(layers1)\n    layers2 = round(layers2)\n        \n    def nn_re_fun():\n        nn = Sequential()\n        nn.add(Dense(neurons, input_dim=X_train.shape[1], activation=activation))\n        for i in range(layers1):\n            nn.add(Dense(neurons, activation=activation))\n        if dropout > 0.5:\n            nn.add(Dropout(dropout_rate, seed=123))\n        for i in range(layers2):\n            nn.add(Dense(neurons, activation=activation))\n        nn.add(Dense(1, activation='linear'))\n        nn.compile(loss='mse', optimizer=optimizer)\n        return nn\n    \n    es = EarlyStopping(monitor='mse', mode='min', verbose=0, patience=20)\n    nn = KerasRegressor(build_fn=nn_re_fun, epochs=epochs, batch_size=batch_size, verbose=0)\n    \n    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n    scores = cross_val_score(nn, X_train, y_train, scoring=mse, cv=kfold, fit_params={'callbacks':[es]}).mean()\n    score = ((scores*-1)**0.5)*-1\n    return score\n\n# Set hyperparameters spaces\nparams_nn ={\n    'neurons': (10, 100),\n    'activation':(0, 9),\n    'optimizer':(0,7),\n    'batch_size':(200, 500),\n    'epochs':(200, 1000),\n    'layers1':(1,3),\n    'layers2':(1,3),\n    'dropout':(0,1),\n    'dropout_rate':(0,0.3)\n}\n\n# Run Bayesian Optimization\nnn_bo = BayesianOptimization(nn_re_bo, params_nn, random_state=123)\nnn_bo.maximize(init_points=4, n_iter=10)","5f7f1230":"# Best hyperparameters\nparams_nn = nn_bo.max['params']\n\noptimizerL = ['Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD', 'SGD']\nactivationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n               'elu', 'exponential', LeakyReLU,LeakyReLU]\n\nparams_nn['neurons'] = round(params_nn['neurons'])\nparams_nn['activation'] = activationL[floor(params_nn['activation'])]\nparams_nn['optimizer'] = optimizerL[round(params_nn['optimizer'])]\nparams_nn['batch_size'] = round(params_nn['batch_size'])\nparams_nn['epochs'] = round(params_nn['epochs'])\nparams_nn['layers1'] = round(params_nn['layers1'])\nparams_nn['layers2'] = round(params_nn['layers2'])\nparams_nn","2ccea78e":"# Fitting the training data\ndef nn_re_fun():\n    nn = Sequential()\n    nn.add(Dense(params_nn['neurons'], input_dim=X_train.shape[1], activation=params_nn['activation']))\n    for i in range(params_nn['layers1']):\n        nn.add(Dense(params_nn['neurons'], activation=params_nn['activation']))\n    if params_nn['dropout'] > 0.5:\n        nn.add(Dropout(params_nn['dropout_rate'], seed=123))\n    for i in range(params_nn['layers2']):\n        nn.add(Dense(params_nn['neurons'], activation=params_nn['activation']))\n    nn.add(Dense(1, activation='relu'))\n    nn.compile(loss='mean_squared_error', optimizer=params_nn['optimizer'], metrics=['mse'])\n    return nn\n        \nes = EarlyStopping(monitor='mse', mode='min', verbose=0, patience=20)\nnn_hyp = KerasRegressor(build_fn=nn_re_fun, epochs=params_nn['epochs'], batch_size=params_nn['batch_size'],\n                         verbose=0)\n\nnn_hyp.fit(X_train, y_train, validation_data=(X_val, y_val), verbose=0)\n\n# Predict the validation data\npred_nn = nn_hyp.predict(X_val)\n\n# Compute RMSE\nrmse_nn = MSE(y_val, pred_nn)**0.5\n\n# Scatter plot true and predicted values\nplt.scatter(pred_nn, y_val, alpha=0.2)\nplt.xlabel('predicted')\nplt.ylabel('true value')\nplt.text(100000, 400000, 'RMSE: ' + str(round(rmse_nn)))\nplt.text(100000, 350000, 'MAE: ' + str(round(mean_absolute_error(pred_nn, y_val))))\nplt.text(100000, 300000, 'R: ' + str(round(np.corrcoef(pred_nn, y_val)[0,1],2)))\nplt.show()","e4ba1c3d":"# 6. Support Vector Machine","27e5a4b2":"# 9. Gradient Boosting Machine","1d89ccaa":"# 8 Random Forest","74d78b41":"# 4. ElasticNet","e50a46b0":"# 2. Ridge Regression","073e49b4":"# 10. LightGBM","3efe1679":"# \ud83c\udfe0Regression-RMSE-House Prices\n\nThis notebook provides commonly used Machine Learning algorithms for regression tasks. Feature generation or selection is just simply performed. The objective of this notebook is to serve as a cheat sheet.\n\nTwelve Machine Learning algorithms are developed to predict with RMSE as the scorer. All algorithms are applied with hyperparameter-tuning to search for the optimum model evaluation results. The hyperparameter-tuning methods consist of GridSearchCV and Bayesian Optimization with 5-fold cross-validation.\n\nThe optimum hyperparameters are then used to train the training dataset and predict the unseen validation dataset. The model is evaluated using RMSE, followed by MAE, correlation coefficient (R), and scatter plot visualization. Useful attributes of the models are also displayed, such as coefficients or feature importances.","58dd906e":"# 1. Linear Regression","3bb27036":"# 7. Decision Tree","274952b1":"# 3. Lasso Regression","e7323363":"# 11. XGBoost","51f37f3e":"# 5. K Nearest Neighbors","3272b493":"# 12. Neural Network"}}