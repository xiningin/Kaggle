{"cell_type":{"1a2102b8":"code","392e90f1":"code","d33f83d5":"code","d9532c89":"code","9caf1921":"code","caa35524":"code","a56ecadc":"code","bcac9669":"code","edbd39b5":"code","6eb9d130":"code","da62a330":"code","92210816":"code","0d0352ad":"code","092221d2":"code","60159977":"code","b283255a":"code","ddede390":"code","a1083fc5":"code","20894be4":"code","952313ed":"code","15fe3ce0":"code","64c8d00d":"code","14da953e":"code","ee813694":"code","201eff29":"code","ec4bba02":"markdown","d3aca988":"markdown","09343754":"markdown","6d4fac5d":"markdown","88ca11d1":"markdown","a3972568":"markdown"},"source":{"1a2102b8":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import Ridge, ElasticNet, Lasso\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom scipy.stats import skew\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","392e90f1":"#Importing both the trainand test files as pandas dataframes.\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nprint('assert the length of the dataframes:'+ str(len(test)) + str(len(train)))","d33f83d5":"#removing all GrLivArea bigger than 4000 (outliers). Recommended by the creator of the dataset.\n\ntrain = train[train.GrLivArea < 4000]\nprint('the lenght of train after removing >4000 sqf lots:'+ str(len(train)))\n\n#Create the y vector out of the SalePrice new log1plus distribution\ny = np.log1p(train.SalePrice)\n","d9532c89":"#Store the Id of the test df in a variable (testId) for later use. Concatenate the test and train dfs for feature engineering.\ntestId = test.Id\ndata= pd.concat((train, test), axis=0, sort= False).reset_index(drop=True )\nprint(data.shape)\nmtrain = train.shape[0]\nmtest = test.shape[0]\nprint(mtrain, mtest)","9caf1921":"# Check for duplicates\nidsUnique = len(set(data.Id))\nidsTotal = data.shape[0]\nidsDupli = idsTotal - idsUnique\nprint(\"There are \" + str(idsDupli) + \" duplicate IDs for \" + str(idsTotal) + \" total entries\")\n\n# Drop Id column\ndata.drop(\"Id\", axis = 1, inplace = True)\nprint('the shape of data:' + str(data.shape))\nprint(data.columns)","caa35524":"#handling missing values for categorical data. Pandas profiling helped check for missing values \n\n# Alley : data description says NA means \"no alley access\"\ndata.loc[:, \"Alley\"] = data.loc[:, \"Alley\"].fillna(\"None\")\n# BedroomAbvGr : NA most likely means 0\ndata.loc[:, \"BedroomAbvGr\"] = data.loc[:, \"BedroomAbvGr\"].fillna(0)\n# Bsmt : data description says NA for basement features is \"no basement\"\ndata.loc[:, \"BsmtQual\"] = data.loc[:, \"BsmtQual\"].fillna(\"No\")\ndata.loc[:, \"BsmtCond\"] = data.loc[:, \"BsmtCond\"].fillna(\"No\")\ndata.loc[:, \"BsmtExposure\"] = data.loc[:, \"BsmtExposure\"].fillna(\"No\")\ndata.loc[:, \"BsmtFinType1\"] = data.loc[:, \"BsmtFinType1\"].fillna(\"No\")\ndata.loc[:, \"BsmtFinType2\"] = data.loc[:, \"BsmtFinType2\"].fillna(\"No\")\ndata.loc[:, \"BsmtFullBath\"] = data.loc[:, \"BsmtFullBath\"].fillna(0)\ndata.loc[:, \"BsmtHalfBath\"] = data.loc[:, \"BsmtHalfBath\"].fillna(0)\ndata.loc[:, \"BsmtUnfSF\"] = data.loc[:, \"BsmtUnfSF\"].fillna(0)\n#no missing vals from centralair\n#no missing vals from conditio1 and condition2\n#no missing vals in Enclosed porch only 0s\n# Fence : data description says NA means \"no fence\"\ndata.loc[:, \"Fence\"] = data.loc[:, \"Fence\"].fillna(\"No\")\n#fireplace 0 means 0 squarespace, and fireplacequ NA means no fireplace\ndata.loc[:, \"FireplaceQu\"] = data.loc[:, \"FireplaceQu\"].fillna(\"No\")\n#nothing missing in functional\n#Garage missing values\ndata.loc[:, \"GarageCond\"] = data.loc[:, \"GarageCond\"].fillna(\"No\")\ndata.loc[:, \"GarageQual\"] = data.loc[:, \"GarageQual\"].fillna(\"No\")\ndata.loc[:, \"GarageFinish\"] = data.loc[:, \"GarageFinish\"].fillna(\"No\")\ndata.loc[:, \"GarageType\"] = data.loc[:, \"GarageType\"].fillna(\"No\")\n# LotFrontage : NA most likely means no lot frontage\ndata.loc[:, \"LotFrontage\"] = data.loc[:, \"LotFrontage\"].fillna(0)\ndata.loc[:, \"MiscFeature\"] = data.loc[:, \"MiscFeature\"].fillna(\"None\")\n# PoolQC : data description says NA means \"no pool\"\ndata.loc[:, \"PoolQC\"] = data.loc[:, \"PoolQC\"].fillna(\"No\")\n","a56ecadc":"# Some numerical features are actually really categories\ndata = data.replace({\"MSSubClass\" : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\", 45 : \"SC45\", \n                                       50 : \"SC50\", 60 : \"SC60\", 70 : \"SC70\", 75 : \"SC75\", \n                                       80 : \"SC80\", 85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\", \n                                       150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\", 190 : \"SC190\"},\n                       \"MoSold\" : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\",\n                                   7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"}\n                      })","bcac9669":"# Encode some categorical features as ordered numbers when there is information in the order\ndata = data.replace({\"Alley\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       \"BsmtCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"BsmtExposure\" : {\"No\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3},\n                       \"BsmtFinType1\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtFinType2\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"FireplaceQu\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \n                                       \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n                       \"GarageCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"GarageQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"LandSlope\" : {\"Sev\" : 1, \"Mod\" : 2, \"Gtl\" : 3},\n                       \"LotShape\" : {\"IR3\" : 1, \"IR2\" : 2, \"IR1\" : 3, \"Reg\" : 4},\n                       \"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n                       \"PoolQC\" : {\"No\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4},\n                       \"Street\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       \"Utilities\" : {\"ELO\" : 1, \"NoSeWa\" : 2, \"NoSewr\" : 3, \"AllPub\" : 4},\n                       \"MSZoning\": {\"A\" : 1, \"C\" : 2, \"FV\": 3, \"I\" : 4, \"RH\" : 5, \"RL\" : 6, \"RP\" : 7, \"RM\" : 8 }})\n","edbd39b5":"# Create new features\n# 1* Simplifications of existing features. Reducing human subjective grading.\ndata[\"SimplOverallQual\"] = data.OverallQual.replace({1 : 1, 2 : 1, 3 : 1, # bad\n                                                       4 : 2, 5 : 2, 6 : 2, # average\n                                                       7 : 3, 8 : 3, 9 : 3, 10 : 3 # good\n                                                      })\ndata[\"SimplOverallCond\"] = data.OverallCond.replace({1 : 1, 2 : 1, 3 : 1, # bad\n                                                       4 : 2, 5 : 2, 6 : 2, # average\n                                                       7 : 3, 8 : 3, 9 : 3, 10 : 3 # good\n                                                      })\ndata[\"SimplPoolQC\"] = data.PoolQC.replace({1 : 1, 2 : 1, # average\n                                             3 : 2, 4 : 2 # good\n                                            })\ndata[\"SimplGarageCond\"] = data.GarageCond.replace({1 : 1, # bad\n                                                     2 : 1, 3 : 1, # average\n                                                     4 : 2, 5 : 2 # good\n                                                    })\ndata[\"SimplGarageQual\"] = data.GarageQual.replace({1 : 1, # bad\n                                                     2 : 1, 3 : 1, # average\n                                                     4 : 2, 5 : 2 # good\n                                                    })\ndata[\"SimplFireplaceQu\"] = data.FireplaceQu.replace({1 : 1, # bad\n                                                       2 : 1, 3 : 1, # average\n                                                       4 : 2, 5 : 2 # good\n                                                      })\n\n                                                    \ndata[\"SimplFunctional\"] = data.Functional.replace({1 : 1, 2 : 1, # bad\n                                                     3 : 2, 4 : 2, # major\n                                                     5 : 3, 6 : 3, 7 : 3, # minor\n                                                     8 : 4 # typical\n                                                    })\ndata[\"SimplKitchenQual\"] = data.KitchenQual.replace({1 : 1, # bad\n                                                       2 : 1, 3 : 1, # average\n                                                       4 : 2, 5 : 2 # good\n                                                      })\ndata[\"SimplHeatingQC\"] = data.HeatingQC.replace({1 : 1, # bad\n                                                   2 : 1, 3 : 1, # average\n                                                   4 : 2, 5 : 2 # good\n                                                  })\ndata[\"SimplBsmtFinType1\"] = data.BsmtFinType1.replace({1 : 1, # unfinished\n                                                         2 : 1, 3 : 1, # rec room\n                                                         4 : 2, 5 : 2, 6 : 2 # living quarters\n                                                        })\ndata[\"SimplBsmtFinType2\"] = data.BsmtFinType2.replace({1 : 1, # unfinished\n                                                         2 : 1, 3 : 1, # rec room\n                                                         4 : 2, 5 : 2, 6 : 2 # living quarters\n                                                        })\ndata[\"SimplBsmtCond\"] = data.BsmtCond.replace({1 : 1, # bad\n                                                 2 : 1, 3 : 1, # average\n                                                 4 : 2, 5 : 2 # good\n                                                })\ndata[\"SimplBsmtQual\"] = data.BsmtQual.replace({1 : 1, # bad\n                                                 2 : 1, 3 : 1, # average\n                                                 4 : 2, 5 : 2 # good\n                                                })\ndata[\"SimplExterCond\"] = data.ExterCond.replace({1 : 1, # bad\n                                                   2 : 1, 3 : 1, # average\n                                                   4 : 2, 5 : 2 # good\n                                                  })\ndata[\"SimplExterQual\"] = data.ExterQual.replace({1 : 1, # bad\n                                                   2 : 1, 3 : 1, # average\n                                                   4 : 2, 5 : 2 # good\n                                                  })\n","6eb9d130":"# 2* Combinations of existing features\n# Overall quality of the house\ndata[\"OverallGrade\"] = data[\"SimplOverallQual\"] * data[\"SimplOverallCond\"] \n# Overall quality of the garage\ndata[\"GarageGrade\"] = data[\"SimplGarageQual\"]* data[\"SimplGarageCond\"]\n# Overall pool score\ndata[\"PoolScore\"] = data[\"PoolArea\"] * data[\"PoolQC\"]\n# Simplified overall quality of the exterior\ndata[\"SimplExterGrade\"] = data[\"SimplExterQual\"] * data[\"SimplExterCond\"]\n# Simplified overall pool score\ndata[\"SimplPoolScore\"] = data[\"PoolArea\"] * data[\"SimplPoolQC\"]\n# Simplified overall garage score\ndata[\"SimplGarageScore\"] = data[\"GarageArea\"] * data[\"SimplGarageQual\"]\n# Simplified overall fireplace score\ndata[\"SimplFireplaceScore\"] = data[\"Fireplaces\"] * data[\"SimplFireplaceQu\"]\n# Simplified overall kitchen score\ndata[\"SimplKitchenScore\"] = data[\"KitchenAbvGr\"] * data[\"SimplKitchenQual\"]\n# Total number of bathrooms\ndata[\"TotalBath\"] = data[\"BsmtFullBath\"] + data[\"BsmtHalfBath\"] + data[\"FullBath\"] + data[\"HalfBath\"]\n# Total SF for house (incl. basement)\ndata[\"AllSF\"] = data[\"GrLivArea\"] + data[\"TotalBsmtSF\"]\n# Total SF for 1st + 2nd floors\ndata[\"AllFlrsSF\"] = data[\"1stFlrSF\"] + data[\"2ndFlrSF\"]\n# Total SF for porch\ndata[\"AllPorchSF\"] = data[\"OpenPorchSF\"] + data[\"EnclosedPorch\"] + data[\"3SsnPorch\"] + data[\"ScreenPorch\"]\n# Has masonry veneer or not\ndata[\"HasMasVnr\"] = data.MasVnrType.replace({\"BrkCmn\" : 1, \"BrkFace\" : 1, \"CBlock\" : 1, \n                                               \"Stone\" : 1, \"None\" : 0})\n# House completed before sale or not\ndata[\"BoughtOffPlan\"] = data.SaleCondition.replace({\"Abnorml\" : 0, \"Alloca\" : 0, \"AdjLand\" : 0, \n                                                      \"Family\" : 0, \"Normal\" : 0, \"Partial\" : 1})","da62a330":"# Differentiate numerical features (minus the target) and categorical features\ncategorical_features = data.select_dtypes(include = [\"object\"]).columns\nnumerical_features = data.select_dtypes(exclude = [\"object\"]).columns\nnumerical_features = numerical_features.drop(\"SalePrice\")\nprint(\"Numerical features : \" + str(len(numerical_features)))\nprint(\"Categorical features : \" + str(len(categorical_features)))\ndata_num = data[numerical_features]\ndata_cat = data[categorical_features]\nprint(data_num.columns)","92210816":"# Log transform of the skewed numerical features to lessen impact of outliers\n# Inspired by Alexandru Papiu's script : https:\/\/www.kaggle.com\/apapiu\/house-prices-advanced-regression-techniques\/regularized-linear-models\n# As a general rule of thumb, a skewness with an absolute value > 0.5 is considered at least moderately skewed\nskewness = data_num.apply(lambda x: skew(x))\nskewness = skewness[abs(skewness) > 0.5]\nprint(str(skewness.shape[0]) + \" skewed numerical features to log transform\")\nskewed_features = skewness.index\ndata_num[skewed_features] = np.log1p(data_num[skewed_features])","0d0352ad":"#After tranforming the skewed numerical variables, we check for missing data and replace it with madian.\n#The mean could have also been used, but since the distribbutions are close to normal it dosen't make to much difference.\nprint(\"NAs for numerical features in data : \" + str(data_num.isnull().values.sum()))\ndata_num = data_num.fillna(data_num.median())\nprint(\"Remaining NAs for numerical features in data : \" + str(data_num.isnull().values.sum()))","092221d2":"# Create dummy features for categorical values via one-hot encoding\nprint(\"NAs for categorical features in train : \" + str(data_cat.isnull().values.sum()))\ndata_cat = pd.get_dummies(data_cat)\nprint(\"Remaining NAs for categorical features in train : \" + str(data_cat.isnull().values.sum()))","60159977":"# Join categorical and numerical features\ndata = pd.concat([data_num, data_cat], axis = 1)\nprint(\"New number of features : \" + str(data.shape[1]))\n","b283255a":"#creating matrices for sklearn:\ntrain_df = data[:mtrain] #then use it to train\ntest_df = data[mtrain:] # then use it to make prediction, and output as csv.\n","ddede390":"# Standardize numerical features\nstdSc = StandardScaler()\ntrain_df.loc[:, numerical_features] = stdSc.fit_transform(train_df.loc[:, numerical_features])\n#test_.loc[:, numerical_features] = stdSc.transform(X_test.loc[:, numerical_features])\n","a1083fc5":"#Creating the cross validation function.\n\nn_folds = 10\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train_df.values)\n    rmse= np.sqrt(-cross_val_score(model, train_df.values, y, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","20894be4":"ridge = Ridge(alpha = 13)\nscore = rmsle_cv(ridge)\nprint(\"Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","952313ed":"KRR = KernelRidge(alpha=1, kernel='polynomial', degree=3, coef0=2.5)\nscore = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","15fe3ce0":"lasso = Lasso(alpha =0.005, random_state=1, max_iter = 1000)\nscore = rmsle_cv(lasso)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","64c8d00d":"#Using  a very small l1 ratio indicate that the L2 regularization is the dominant.\nENet = ElasticNet(alpha=2, l1_ratio=.000000001, random_state=3)\nscore = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","14da953e":"LassoMd = lasso.fit(train_df.values,y)\nENetMd = ENet.fit(train_df.values,y)\nridge_s = ridge.fit(train_df.values, y)\nKRRMd = KRR.fit(train_df.values,y)\n","ee813694":"#test set numerical features standardization and prediction of test set SalePrice.\nstdSc = StandardScaler()\ntest_df.loc[:, numerical_features] = stdSc.fit_transform(test_df.loc[:, numerical_features])\npreds = np.expm1(KRRMd.predict(test_df.values))","201eff29":"#create the submission file\ntest_df = pd.concat([testId, test_df],  axis=1, sort= False).reset_index(drop=True )\nsub = pd.DataFrame()\nsub['Id'] = testId\nsub['SalePrice'] = preds\nsub.to_csv('submission.csv',index=False)","ec4bba02":"**MODELS**\n","d3aca988":"**Submission**","09343754":"*This code is for making a nice visual description of the data* [Pandas profiling](http:\/\/github.com\/pandas-profiling\/pandas-profiling)\n\n**profile = pandas_profiling.ProfileReport(train)**\n\n**profile.to_file(outputfile=\"output.html\")**","6d4fac5d":"**Let's start by the most important stuff, the kernels that helped me score in the  top 8%: [\njuliencs](http:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset) wich made my life easier in feature engineeing that used a lot of common sense. I didn't though add polynomial features to the mix as the there are already enough features for great performance.\n**Also I used [\nAlexandru Papiu](http:\/\/www.kaggle.com\/apapiu\/regularized-linear-models), [Vijay Gupta](http:\/\/www.kaggle.com\/vjgupta\/reach-top-10-with-simple-model-on-housing-prices), [\nPhilipBall](http:\/\/www.kaggle.com\/fiorenza2\/journey-to-the-top-10) kernels for some code snippets and model intuitions**\n\n*EDA*\nExploring the dataset for type, distribution and quality was done mainly by PandasProfiling. Tough not perfect but it  gives a very detailed description about the data, especially the ammount f missing data. Also it gives a great overview on data distribution.\n\n*Feature engineering*\n\nThe bulk of work on this task was borrowed from [\njuliencs](http:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset) kernel. I used some insights on the data from PandasProfiling library to discard or adapt some features to my specific models. For expample, in the combining engineered and existing features, no feature was given a weight based on the subjective view on it's value (e.g. 'HalfBath' against 'FullBath'). Also no polynomial features were added as there were enough from creating and combining new ones. Also skewd numerical features and the SalePrice (target variable) were transformed into a normal distribution, which tought to boost the performance (speed and accuracy) of the models.\nAnother point of interest is the inspection for coliniearities. The idea is to look for highly intercorrelated paires of feature and remove one them. This techhnique was not used in this project for the chosen models deal already with coliniearities to reduce overfitting and at the same time to avoid bias in the case of overengineering. \n\n*Modeling*\n\nKernelRidge, Ridge, Lasso and ElasticNets were used for this regression problem. Sklearn is the librabry from wich these models are borrowed. Please find the Links for the documentation below:\n\n[KernelRidge](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.kernel_ridge.KernelRidge.html).\n [Ridge](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Ridge.html).\n [Lasso](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Lasso.html). \n [ElasticNets](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.ElasticNet.html).\n \nThe Ridge model (L2 regularization) performed well on the cross-validation by reducing colinearities and preserving most of the features. This consolidates the idea that the choice of most of the features was very optimal.\n\nEvantually, the model that got the great cross-validation and the test set score was KernelRidge regression. the reason why in my opinion is that the kernelRidge model uses the L2 regularization (Ridge) which minimizes the effect of conliniearities while preserving most of the features and at the same time make use of the kernel trick to compute higher dimentional space features to reduce the mean squared error.\n\n\nThe Lasso model wich implements the the L1 regularization was not as good as KernelRidge, probably because of the total minimization  of the weights of some features that are important to predicting the SalePrice.\n\nThe Elastic net performed well only when it was directed towards L2 regularization.\n\n\n","88ca11d1":"**The models use the cross validation function to calculate the negative mean squared error and the standard deviation of the distribution of the cross validation mse**","a3972568":"**Data quality assessment and profiling**"}}