{"cell_type":{"21fc0671":"code","87d0d8cd":"code","4352230e":"code","e529aec4":"code","81d3d5ff":"code","b194d81b":"code","2c74d2ab":"code","d61b92ec":"code","d9eca25d":"code","bd563f96":"code","cae9265d":"code","e185ede3":"code","08361bd2":"code","4fadea39":"code","0e6b1b1e":"code","2d0d64db":"code","fbe9912a":"code","fc923c24":"code","3e0dbfdb":"code","666714d1":"code","6b10faab":"code","3c094218":"code","85f3695a":"code","2f8d051b":"code","7647225b":"code","f290fcda":"code","0e2f9f06":"code","d89b2ecf":"code","a74bf521":"code","7f698b48":"code","45009444":"code","f86c8b44":"code","0938095b":"code","870c24a7":"code","1bbc195f":"code","e24de311":"code","68ec010b":"code","a2810fa1":"code","0b786c7b":"code","91bd922d":"code","7b398000":"code","00c2068a":"code","cac11310":"code","752c0acf":"code","ce49906c":"code","3d691f9a":"code","90bc03de":"code","12bd802d":"code","e00ac3bd":"code","a8a37cce":"code","128fcd5b":"code","9a56e01a":"code","5a0e1c79":"code","a359aa32":"code","eb9e5558":"code","298e9c6d":"code","ffde7e77":"code","ca9add72":"code","6aa96f1a":"code","27f74bec":"code","0d7d862b":"code","1bb819aa":"code","757708e2":"code","39097f3e":"code","d7a0e6cc":"code","55a75668":"code","99b19720":"code","553bc17f":"code","2c45a526":"code","4de60739":"code","c096b03e":"code","ab88c726":"code","1d4dac8e":"code","6ab9e3b9":"code","637685e6":"code","42caf7a7":"code","4f7f18b9":"code","3ce1a91a":"code","265188e5":"code","23f6ffed":"code","7e6abd98":"code","e57537ca":"code","d3b26d31":"markdown","5c25cac0":"markdown","98853615":"markdown","18adec25":"markdown","81c6698b":"markdown","9d18886c":"markdown","f357ae39":"markdown","0fe03d72":"markdown","8b97b335":"markdown","bd40b921":"markdown","6d07b911":"markdown","e0cd1801":"markdown","8c5f73c9":"markdown","4764c7fe":"markdown","28b1f60e":"markdown","541c3931":"markdown","908793cf":"markdown","3ac830f8":"markdown","b200a093":"markdown"},"source":{"21fc0671":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom collections import OrderedDict\nfrom sklearn.preprocessing import Imputer\npd.set_option('display.max_columns',None)\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nfrom sklearn.model_selection import StratifiedKFold,cross_val_score,GridSearchCV,RandomizedSearchCV\n# Any results you write to the current directory are saved as output.","87d0d8cd":"color_dict = OrderedDict({1: 'red', 0: 'green'})\nloan_dict = OrderedDict({1: 'Cant repay',0:'Repaid'})\ndrop_cols = []","4352230e":"def plot_continuos(data,var):\n    plt.figure(figsize=(8,8))\n    for key,clr in color_dict.items():\n        sns.kdeplot(data[train.TARGET==key][var].dropna(),color=clr,label = loan_dict[key])\n    plt.xlabel(var)\n    plt.ylabel('Density')\n    \ndef plot_bar(data,x,y):\n    tempdf = data.groupby(x)[y].mean().reset_index()\n    sns.barplot(x = x,y = y,data = tempdf)\n    plt.xticks(rotation=90)\n    plt.show()\n    \ndef plot_count(data,x,hue):\n    sns.countplot(x = x,data = data,hue = hue)\n    plt.show()\n    \ndef create_missing_columns_report(data):    \n    prop = data.isnull().sum()\/len(data)\n    missing_df = pd.DataFrame(prop).reset_index().rename(columns = {'index':'columns',0:'%missing'})\n    missing_df.sort_index(by = '%missing',ascending=False,inplace=True)\n    \n    dict_column_datatype = data.dtypes.to_dict()\n    missing_df['datatype'] = missing_df['columns'].map(dict_column_datatype)\n    return missing_df\n\n\ndef convert_categoricals(data):\n    ohe_frame = pd.DataFrame()\n    ohe_cols= []\n    for col in categorical_cols:\n        if(data[col].nunique()==2):            \n            data.loc[:,col] = np.where(((data[col]=='no')|(data[col]=='N')|(data[col]=='F')),0,1)\n            data[col] = data[col].astype(np.int8)         \n    data = pd.get_dummies(data)       \n    return data\n\ndef drop_rows(data,column,value,**args):\n    \n    index = data.index[(data[data[column]==value])&(data[AMT_INCOME_TOTAL]>1000000)]\n    train_filter = train.drop(index,axis=0)\n\ndef check_column_consistency(df1, df2):\n        \"\"\" Check if columns of train and test data are in same order or not. Should be called after train, valid\n        and test has been transformed. If certain columns are missing or are not in order then they are added or ordered\n        accordingly\n        :param df1: train data frame\n        :param df2: test or valid data frame\n        :return: consistent data frames\n        \"\"\"\n        df1_columns = df1.columns.tolist()\n        df2_columns = df2.columns.tolist()\n\n        for df1_col in df1_columns:\n            if df1_col not in df2_columns:\n                df2[df1_col] = 0\n        df2 = df2[df1_columns]\n        df1 = df1[df1_columns]\n        return df1, df2\n    \n    \ndef days_age(data):\n\n    #cols_days = [x for x in data if x.startswith('DAYS_')]\n    #for col in cols_days:\n    data.loc[:,'Age(years)'] = data['DAYS_BIRTH']*-1\/365\n    return data\n\n\ndef days_employ_flag(data):\n    quart90 = np.percentile(data.DAYS_EMPLOYED, 90)\n    index = data[data.DAYS_EMPLOYED>=quart90].index\n    data.loc[:,'days_employ_flag'] = np.where(data.DAYS_EMPLOYED>=quart90,1,0)\n    days_mean = np.mean(data.loc[~(data.DAYS_EMPLOYED>=quart90),'DAYS_EMPLOYED'].values)\n    data.loc[index,'DAYS_EMPLOYED'] = days_mean\n    \n    return data\n\n\n\ndef train_eval(feature_train,feature_test,target_train,nfolds,test_ids,return_preds=False):\n      \n    sfold = StratifiedKFold(n_splits= nfolds,shuffle=True,random_state=100)\n    valid_scores_list = []\n    test_predictions_df = pd.DataFrame()\n    feature_columns = feature_train.columns\n    feature_importance = np.zeros(len(feature_columns))\n    featuresNames = []\n    featureImps =[]\n\n    feature_train_arr = feature_train.values\n    feature_test_arr = feature_test.values\n    target_train_arr = target_train.values\n    \n    clf_lgb=lgb.LGBMClassifier(  n_estimators=10000,\n                                 n_jobs = -1,\n                                 metric = 'None',\n                                 random_state=100,\n                                 class_weight='balanced')\n    for i, (train_index,valid_index) in enumerate(sfold.split(feature_train,target_train)):\n        fold_predictions_df = pd.DataFrame()        \n        # Training and validation data\n        X_train = feature_train_arr[train_index]\n        X_valid = feature_train_arr[valid_index]\n        y_train = target_train_arr[train_index]\n        y_valid = target_train_arr[valid_index]\n        \n        \n        fit_params={\"early_stopping_rounds\":100,\n            \"eval_metric\" : 'auc', \n            \"eval_set\" : [(X_train,y_train), (X_valid,y_valid)],\n            'eval_names': ['train', 'valid'],\n            'verbose': 100,\n            'categorical_feature': 'auto'}\n        \n        clf_lgb.fit(X_train,y_train,**fit_params)\n        best_iteration = clf_lgb.best_iteration_\n        valid_scores_list.append(clf_lgb.best_score_['valid']['auc'])\n        display(f'Fold {i + 1}, Validation Score: {round(valid_scores_list[i], 5)}, Estimators Trained: {clf_lgb.best_iteration_}')\n        fold_probabilitites = clf_lgb.predict_proba(feature_test_arr,num_iteration = best_iteration)[:,1]      \n        fold_predictions_df['Score'] = fold_probabilitites     \n        fold_predictions_df['SK_ID_CURR'] = test_ids\n        fold_predictions_df['fold'] = (i+1)\n        \n        test_predictions_df = test_predictions_df.append(fold_predictions_df)\n        valid_scores = np.array(valid_scores_list)\n        #print(test_predictions_df.shape)\n        fold_feature_importance = clf_lgb.feature_importances_\n        fold_feature_importance = 100.0 * (fold_feature_importance \/ fold_feature_importance.max())\n        feature_importance = (feature_importance+fold_feature_importance)\/nfolds\n        sorted_idx = np.argsort(feature_importance)\n        for item in sorted_idx[::-1][:]:\n            featuresNames.append(np.asarray(feature_columns)[item])\n            featureImps.append(feature_importance[item])\n            featureImportance = pd.DataFrame([featuresNames, featureImps]).transpose()\n            featureImportance.columns = ['FeatureName', 'Importance']\n        \n    \n    # Average the predictions over folds    \n    test_predictions_df = test_predictions_df.groupby('SK_ID_CURR', as_index = False).mean()\n    #test_predictions_df['Target'] = test_predictions_df[[0,1]].idxmax(axis = 1)\n    #test_predictions_df['Score'] = test_predictions_df[1]   \n    test_predictions_df.drop('fold',axis=1,inplace=True)   \n        \n    \n    return test_predictions_df,featureImportance,valid_scores","e529aec4":"train = pd.read_csv(\"..\/input\/application_train.csv\")\ntest = pd.read_csv(\"..\/input\/application_test.csv\")\nprev_appl = pd.read_csv(\"..\/input\/previous_application.csv\")\nbureau = pd.read_csv(\"..\/input\/bureau.csv\")\nbureau_bal = pd.read_csv(\"..\/input\/bureau_balance.csv\")\n#pos_cash = pd.read_csv(\"..\/input\/POS_CASH_balance.csv\")\ncredit_card_bal = pd.read_csv(\"..\/input\/credit_card_balance.csv\")","81d3d5ff":"print('Shape of train:{}'.format(train.shape))\ntrain.head(3)\n","b194d81b":"print('Shape of test:{}'.format(test.shape))\ntest.head(3)","2c74d2ab":"prev_appl.loc[:,'FLAG_LAST_APPL_PER_CONTRACT'] = prev_appl['FLAG_LAST_APPL_PER_CONTRACT'].map({'Y':1,'N':0})","d61b92ec":"prev_appl.head(3)","d9eca25d":"pos_cash.head(3)","bd563f96":"'''\npos_cash_agg = pos_cash.groupby(['SK_ID_PREV','SK_ID_CURR'],as_index=False)[['MONTHS_BALANCE','CNT_INSTALMENT','CNT_INSTALMENT_FUTURE','SK_DPD']].agg(['mean','sum','std'])\npos_cash_agg.columns = [' _'.join(col).strip() for col in pos_cash_agg.columns.values]\npos_cash_agg.reset_index(inplace=True)\npos_cash_agg.head(3)\n'''","cae9265d":"credit_card_bal.head(3)","e185ede3":"credit_card_bal.columns = ['Credit_Card_'+col for col in credit_card_bal.columns.values]\ncredit_card_bal.head(2)","08361bd2":"credit_agg = credit_card_bal.groupby(['Credit_Card_SK_ID_PREV','Credit_Card_SK_ID_CURR'],as_index=False)[['Credit_Card_MONTHS_BALANCE','Credit_Card_AMT_BALANCE','Credit_Card_AMT_PAYMENT_CURRENT','Credit_Card_CNT_DRAWINGS_CURRENT']].agg(['mean','sum','std'])\ncredit_agg.columns = [' _'.join(col).strip() for col in credit_agg.columns.values]\ncredit_agg.reset_index(inplace=True)\ncredit_agg.head(3)","4fadea39":"#del pos_cash\ndel credit_card_bal","0e6b1b1e":"agg_data_prev_appl1 = prev_appl.groupby(['SK_ID_PREV','SK_ID_CURR'],as_index=False)[['AMT_ANNUITY','AMT_APPLICATION','AMT_CREDIT','AMT_DOWN_PAYMENT','AMT_GOODS_PRICE',\\\n                                                                      'DAYS_DECISION','CNT_PAYMENT','DAYS_FIRST_DUE']].agg(['mean','sum','std'])\nagg_data_prev_appl2 = prev_appl.groupby(['SK_ID_PREV','SK_ID_CURR'],as_index=False)[['FLAG_LAST_APPL_PER_CONTRACT','NFLAG_LAST_APPL_IN_DAY']].agg(['sum'])\nagg_data_prev_appl3 = prev_appl.groupby(['SK_ID_PREV','SK_ID_CURR'],as_index=False).size().reset_index().rename(columns = {0:'Count'}).set_index(['SK_ID_PREV','SK_ID_CURR'])                                                               \nagg_data_prev_appl1.columns = [' _'.join(col).strip() for col in agg_data_prev_appl1.columns.values]\nagg_data_prev_appl2.columns = [' _'.join(col).strip() for col in agg_data_prev_appl2.columns.values]\n#print(agg_data_prev_appl.columns)\nagg_data_prev_appl = pd.concat([agg_data_prev_appl1,agg_data_prev_appl2,agg_data_prev_appl3],axis=1)                                                                     \nagg_data_prev_appl.reset_index(inplace=True)\nagg_data_prev_appl.columns = ['Previous_Appl_'+col for col in agg_data_prev_appl.columns.values]\n#feat_data_prev_appl = pd.merge(agg_data_prev_appl.copy(),pos_cash_agg,left_on  =['Previous_Appl_SK_ID_PREV','Previous_Appl_SK_ID_CURR'], right_on = ['SK_ID_PREV','SK_ID_CURR'],how='left')\nfeat_data_prev_appl = pd.merge(agg_data_prev_appl.copy(),credit_agg,left_on = ['Previous_Appl_SK_ID_PREV','Previous_Appl_SK_ID_CURR'],right_on = ['Credit_Card_SK_ID_PREV','Credit_Card_SK_ID_CURR'],how='left')\nfeat_data_prev_appl.drop('Previous_Appl_SK_ID_PREV',axis=1,inplace=True)\ndel prev_appl\ndel agg_data_prev_appl1\ndel agg_data_prev_appl2\ndel agg_data_prev_appl3\n\n#agg_data_prev_appl.rename(columns = {'Previous_Appl_SK_ID_CURR':'SK_ID_CURR'},inplace=True)","2d0d64db":"feat_data_prev_appl.head(3)","fbe9912a":"train = pd.merge(train,feat_data_prev_appl,left_on='SK_ID_CURR',right_on='Previous_Appl_SK_ID_CURR',how='left')\ntest = pd.merge(test,feat_data_prev_appl,left_on='SK_ID_CURR',right_on='Previous_Appl_SK_ID_CURR',how='left')\ndel feat_data_prev_appl\nprint(train.shape,test.shape)","fc923c24":"bureau.head(4)","3e0dbfdb":"agg_data_bureau1 = bureau.groupby('SK_ID_CURR',as_index=False)[['DAYS_CREDIT','DAYS_CREDIT_ENDDATE','CREDIT_DAY_OVERDUE','AMT_CREDIT_MAX_OVERDUE','AMT_CREDIT_SUM','AMT_CREDIT_SUM_DEBT','AMT_CREDIT_SUM_OVERDUE','DAYS_CREDIT_UPDATE','AMT_ANNUITY']].agg(['mean','sum','std'])\nagg_data_bureau2 = bureau.groupby('SK_ID_CURR',as_index=False)[['CNT_CREDIT_PROLONG']].agg(['sum'])\nagg_data_bureau3 = bureau.groupby('SK_ID_CURR',as_index=False).size().reset_index().rename(columns = {0:'Bureau_Count'}).set_index('SK_ID_CURR')                                                               \nagg_data_bureau1.columns = [' _'.join(col).strip() for col in agg_data_bureau1.columns.values]\nagg_data_bureau2.columns = [' _'.join(col).strip() for col in agg_data_bureau2.columns.values]\n#print(agg_data_prev_appl.columns)\nagg_data_bureau = pd.concat([agg_data_bureau1,agg_data_bureau2,agg_data_bureau3],axis=1)                                                                     \nagg_data_bureau.reset_index(inplace=True)\nagg_data_bureau.columns = ['Bureau_'+col for col in agg_data_bureau.columns.values]\ndel agg_data_bureau1\ndel agg_data_bureau2\ndel agg_data_bureau3\ndel bureau","666714d1":"train = pd.merge(train,agg_data_bureau,left_on='SK_ID_CURR',right_on='Bureau_SK_ID_CURR',how='left')\ntest = pd.merge(test,agg_data_bureau,left_on='SK_ID_CURR',right_on='Bureau_SK_ID_CURR',how='left')\ndel agg_data_bureau\nprint(train.shape,test.shape)","6b10faab":"train.info()","3c094218":"train.select_dtypes('object').nunique()","85f3695a":"#Show a head of object columns\ntrain[list(train.select_dtypes('object').columns)].head(2)","2f8d051b":"#Show a head of float columns\ntrain[list(train.select_dtypes(np.float64).columns)].head(2)","7647225b":"sns.countplot(train.TARGET)\nplt.xlabel('Target')\nplt.ylabel('Frequency')\ntrain.TARGET.value_counts(normalize=True)","f290fcda":"missing_df = create_missing_columns_report(train)\nmissing_df.head(20)","0e2f9f06":"missing_df[missing_df.datatype == 'object']","d89b2ecf":"train.describe(percentiles=[0.9,0.92,0.94,0.96,0.98,0.99,0.995])","a74bf521":"test.describe(percentiles=[0.9,0.92,0.94,0.96,0.98,0.99,0.995])","7f698b48":"train.query('AMT_INCOME_TOTAL>100000000')","45009444":"train.OCCUPATION_TYPE.unique()","f86c8b44":"plot_bar(train,'OCCUPATION_TYPE','AMT_INCOME_TOTAL')","0938095b":"train_lab = train.query('OCCUPATION_TYPE==\"Laborers\"')\ntrain_lab.describe(percentiles= [0.9,0.94,0.98,0.99,0.995,0.999])","870c24a7":"train_lab = train.query('OCCUPATION_TYPE==\"Laborers\"')\ntrain_lab.drop((train_lab.index[train_lab['AMT_INCOME_TOTAL']>1000000]),axis=0,inplace=True)\nplot_continuos(train_lab,'AMT_INCOME_TOTAL')","1bbc195f":"index = train.index[(train.OCCUPATION_TYPE==\"Laborers\")&(train['AMT_INCOME_TOTAL']>1000000)]\ntrain_filter = train.drop(index,axis=0)","e24de311":"train_filter.shape","68ec010b":"train_filter.describe(percentiles=[0.9,0.92,0.94,0.96,0.98,0.99,0.995])","a2810fa1":"plot_continuos(train_filter,'AMT_INCOME_TOTAL')","0b786c7b":"plot_continuos(train_filter,'DAYS_EMPLOYED')","91bd922d":"print('Number of samples:{}'.format(len(train_filter.query('DAYS_EMPLOYED==365243'))))\nprint('Percentage of samples:{}'.format(len(train_filter.query('DAYS_EMPLOYED==365243'))*100\/len(train_filter)))","7b398000":"train_filter2 = days_employ_flag(train_filter.copy())\nprint('Shape of data before handling DAYS_EMPLOYED:{}'.format(train_filter.shape))\nprint('Shape of data after handling DAYS_EMPLOYED:{}'.format(train_filter2.shape))\n#quart3 = np.percentile(train_filter.DAYS_EMPLOYED, 75)\n#iqr = quart3 - quart1\n\n#outlier = train_filter[train_filter['DAYS_EMPLOYED'] > quart3 + 1.5 * iqr].DAYS_EMPLOYED.max()","00c2068a":"plot_continuos(train_filter2,'DAYS_EMPLOYED')","cac11310":"\ntrain_filter3 = days_age(train_filter2.copy())\nprint('Shape of data before handling DAYS_BIRTH:{}'.format(train_filter2.shape))\nprint('Shape of data after handling DAYS_BIRTH:{}'.format(train_filter3.shape))\nplot_continuos(train_filter3,'Age(years)')","752c0acf":"drop_cols.append('DAYS_BIRTH')","ce49906c":"train_filter3[[col for col in train_filter3 if col.startswith('OBS')]].describe(percentiles=[0.9,0.92,0.94,0.96,0.98,0.99,0.995])","3d691f9a":"train_filter3.OBS_30_CNT_SOCIAL_CIRCLE.isnull().sum()","90bc03de":"outlier_30 = np.nanpercentile(train_filter3.OBS_30_CNT_SOCIAL_CIRCLE,100)\noutlier_60 = np.nanpercentile(train_filter3.OBS_60_CNT_SOCIAL_CIRCLE,100)\nSK_ID_CURR_30 = train_filter3[train_filter3['OBS_30_CNT_SOCIAL_CIRCLE']>=outlier_30]['SK_ID_CURR'].values[0]\nSK_ID_CURR_60=train_filter3[train_filter3['OBS_60_CNT_SOCIAL_CIRCLE']>=outlier_60]['SK_ID_CURR'].values[0]\n\nprint(SK_ID_CURR_30 ,',',SK_ID_CURR_60)","12bd802d":"train_filter3.drop(train_filter3.index[train_filter3.SK_ID_CURR==SK_ID_CURR_30],axis=0,inplace=True)\nprint('Shape of data after handling outlier row:{}'.format(train_filter3.shape))","e00ac3bd":"plot_continuos(train_filter3,'OBS_30_CNT_SOCIAL_CIRCLE')","a8a37cce":"plot_continuos(train_filter3,'OBS_60_CNT_SOCIAL_CIRCLE')","128fcd5b":"train_filter3.head(3)","9a56e01a":"plot_count(train_filter3,'FLAG_MOBIL',hue='TARGET')","5a0e1c79":"train_filter3[train_filter3.FLAG_MOBIL==0]","a359aa32":"plot_count(train_filter3,'FLAG_EMP_PHONE',hue='TARGET')","eb9e5558":"plot_count(train_filter3,'FLAG_WORK_PHONE',hue='TARGET')","298e9c6d":"plot_count(train_filter3,'FLAG_CONT_MOBILE',hue='TARGET')","ffde7e77":"plot_count(train_filter3,'FLAG_PHONE',hue='TARGET')","ca9add72":"train_filter3['flag_mob'] = train_filter3['FLAG_MOBIL']+train_filter3['FLAG_EMP_PHONE']+\\\n                            train_filter3['FLAG_WORK_PHONE']+train_filter3['FLAG_CONT_MOBILE']+train_filter3['FLAG_PHONE']","6aa96f1a":"train_filter3.head(3)","27f74bec":"plot_count(train_filter3,'flag_mob',hue='TARGET')","0d7d862b":"train_filter3['flag_mob'] = train_filter3['flag_mob'].astype('object')","1bb819aa":"drop_cols.extend(['FLAG_MOBIL','FLAG_EMP_PHONE','FLAG_WORK_PHONE','FLAG_CONT_MOBILE','FLAG_PHONE'])\ndrop_cols","757708e2":"print('Shape of data after handling flag_mobile:{}'.format(train_filter3.shape))","39097f3e":"train_filter3.head(3)","d7a0e6cc":"test_filter2 = days_employ_flag(test.copy())\nprint('Shape of data before handling DAYS_EMPLOYED:{}'.format(test.shape))\nprint('Shape of data after handling DAYS_EMPLOYED:{}'.format(test_filter2.shape))\n\n\ntest_filter3 = days_age(test_filter2.copy())\nprint('Shape of data before handling DAYS_BIRTH:{}'.format(test_filter2.shape))\nprint('Shape of data after handling DAYS_BIRTH:{}'.format(test_filter3.shape))\n\n\n\ntest_filter3['flag_mob'] = test_filter3['FLAG_MOBIL']+test_filter3['FLAG_EMP_PHONE']+\\\n                            test_filter3['FLAG_WORK_PHONE']+test_filter3['FLAG_CONT_MOBILE']+test_filter3['FLAG_PHONE']\n    \n#print('Shape of data before handling flag_mobile:{}'.format(test_filter2.shape))\nprint('Shape of data after handling flag_mobile:{}'.format(test_filter3.shape))","55a75668":"test_filter3['flag_mob'] = test_filter3['flag_mob'].astype('object')","99b19720":"\ncategorical_cols = list(train_filter3.select_dtypes('object').columns)\nunique_levels = train_filter3[categorical_cols].apply(lambda x: x.nunique())\nprint('Total levels in categorical columns:{}'.format(unique_levels.sum()))\nunique_levels","553bc17f":"train_copy = convert_categoricals(train_filter3.copy())\ntest_copy = convert_categoricals(test_filter3.copy())\n\nprint('Train shape:{},Test shape:{}'.format(train_copy.shape,test_copy.shape))","2c45a526":"train_copy.head(2)","4de60739":"test_copy.head(2)","c096b03e":"train_copy.info()","ab88c726":"test_copy.info()","1d4dac8e":"train_labels = train_copy['TARGET']\ntest_ids = test.SK_ID_CURR.values\ndrop_cols.append('SK_ID_CURR')\nprint(drop_cols)\ntrain_copy.drop(drop_cols+['TARGET'],axis=1,inplace=True)\ntest_copy.drop(drop_cols,axis=1,inplace=True)\ntrain_copy, test_copy= check_column_consistency(train_copy, test_copy)\n#train_copy,test_copy = train_copy.align(test_copy,axis=1,join='inner')\nprint('Train shape:{},Test shape:{}'.format(train_copy.shape,test_copy.shape))","6ab9e3b9":"train_copy.info()","637685e6":"train_copy.head(3)","42caf7a7":"test_copy.head(3)","4f7f18b9":"test_copy.info()","3ce1a91a":"\ntest_predictions_df,featureImportance,valid_scores = train_eval(train_copy,test_copy,train_labels,10,test_ids,return_preds=False)","265188e5":"test_predictions_df.head(2)","23f6ffed":"#submission = test_predictions_df[['SK_ID_CURR','Score']]\ntest_predictions_df.rename(columns = {'Score':'TARGET'},inplace=True)","7e6abd98":"test_predictions_df.head(2)","e57537ca":"test_predictions_df.to_csv('baseline_lgb.csv', index = False)","d3b26d31":"**Handling days birth**","5c25cac0":"1.  **EDA**","98853615":"**From the above stats and  plot  it is clear that the average  income of laborers is around 170k,but the customer with SK_ID_CURR:114967 has an income of 117 million which is clearly an outlier**\n**I will cap the  income column for laborers at 1 million  and in the second plot you can see the distribution after capping**","18adec25":"**Handling days employed**","81c6698b":"**Extracting features from Bureau**","9d18886c":"**From 90th percentile onwards the column has a constant value of 365243 which is close to 100 years. Thats weird.**\n\n**Close to 18% of the data have days employed  approx 100 years which is surprising and cant get rid of such a huge chunk of data.**\n\n**One way to handle this is to replace this value with the mean of the values of this column between 0 to 90th percentile.**\n\n**Secondly ,create an extra column which will act as a flag  with value 1 where the outlier value exists in the days_employed column.**","f357ae39":"`**So majority of float columns have the largest missing values and a few categorical columns have missing values**","0fe03d72":"**Modelling**","8b97b335":"**The train data has one extra column than test which is the target column**","bd40b921":"1. 2. **Missing values**","6d07b911":"1. 3. **How many levels in categorical columns?**","e0cd1801":"1. 4. **Encoding of categoricals**","8c5f73c9":"**Distribution of target variable is imbalanced as shown in the above plot which signifies that  number of instances where the loan was paid in time  outweighs significantly the number of instances where the loan payment was not done or done after due  dateb**","4764c7fe":"* 1. 1. **Distribution of target variable**","28b1f60e":"**From the above plot, it is clear that as the age decreases, the propensity to pay the credit decreases\nas shown by the  tilt of the red curve towards left**","541c3931":"**Extracting features from Previous application , POS balance and Credit card balance **","908793cf":"**Train and test data  columns are aligned**","3ac830f8":"**From the above stats, some columns have suspicious values**\n\n**1.AMT_INCOME_TOTAL:Till 99 percentile, it has values to the order of 5,but the maximum value is of the order 8.Should be an outlier.**\n\n**2.DAYS_EMPLOYED: Between 90 percentile and 100 perentile, days employed translates to 100 years.Something odd!!.**\n\n**3.DAYS_BIRTH: It is with reference to the loan application date,so it is a negative number. Working on years will be easier. So I will create another column 'Age' which will be a postive number in  years**\n\n**4.OBS_30_CNT_SOCIAL_CIRCLE &OBS_60_CNT_SOCIAL_CIRCLE:At 9 percentile, the value is 10, but the maximum value is close to 3o times the 99 percentile value.**   ","b200a093":"**Converting two levels of categorical columns  to values 0(no or N or F) and 1(yes or Y or M) and more than two levels to be one-hot encoded**"}}