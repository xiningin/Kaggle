{"cell_type":{"c8a4d09b":"code","afefdc1b":"code","29ecf800":"code","7e02c423":"code","7aa31200":"code","7f323de0":"code","8f4bae5e":"code","c0b482c2":"code","a04c7fd1":"code","cf62856f":"code","98de3013":"code","b35debee":"code","ec72d573":"code","4e9956e5":"code","4ed8e641":"code","73b76237":"code","b826dbf7":"code","7e4f88ea":"code","8518f769":"code","f77f7884":"code","98bbd4f0":"markdown","1774b50f":"markdown","a67c3895":"markdown","b8e8f5b2":"markdown","722349f6":"markdown","6db55585":"markdown","f614c8ac":"markdown","c625eb2c":"markdown","7e44e0bb":"markdown","db2212fb":"markdown","bf221d41":"markdown","18384fb1":"markdown","0db19646":"markdown","caac368f":"markdown","7bb29628":"markdown","64a53d9d":"markdown","5249683e":"markdown","975ab9ed":"markdown"},"source":{"c8a4d09b":"# Data Manipulation\nimport pandas as pd\nimport numpy as np\n\n# sklearn helper functions\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import ShuffleSplit, cross_validate, GridSearchCV, cross_val_score\n\n# sklearn ML algorithms\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, BaggingRegressor, VotingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\n\n# Visualization\nimport matplotlib.pyplot as plt\n\n# Notebook customization\npd.options.display.max_rows = 500\npd.options.display.max_columns = 100","afefdc1b":"# read data from csv files\ndf_train_raw = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf_test_raw = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","29ecf800":"# create a copy for exploration purposes\ndf1 = df_train_raw.copy()\n\n# display some basic information about the data\nprint(df_train_raw.info())\ndf_train_raw.sample(10)","7e02c423":"# check null value count in train and test sets\nprint(\"Null values in training set:\\n\", df1.isnull().sum())\nprint(\"-\"*50)\n\nprint(\"Null values in test set:\\n\", df_test_raw.isnull().sum())\nprint(\"-\"*50)\n\n# summary statistics for train set\ndf1.describe(include='all')","7aa31200":"# Delete attributes with high null count in training set\ndrop_columns_list = ['Id','Alley', 'PoolQC', 'Fence', 'MiscFeature']\ndf1.drop(columns=drop_columns_list, inplace=True)","7f323de0":"plt.scatter(df1['GrLivArea'], df1['SalePrice'], c = \"pink\", marker = \"s\")\nplt.title(\"Looking for outliers\")\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.grid(True)\nplt.show()","8f4bae5e":"df1 = df1[df1['GrLivArea'] < 4000]","c0b482c2":"# Taking natural log of 'SalePrice'\ndf1['SalePrice'] = np.log1p(df1['SalePrice'])","a04c7fd1":"# Remove target variable from training set and keep a copy of it\ntrain_labels = df1['SalePrice'].copy()\ndf1.drop('SalePrice', axis=1, inplace=True)","cf62856f":"# selecting features for different preprocessing methods\npreprocess_features1 = ['LotFrontage', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageYrBlt', 'GarageCars', 'GarageArea']\npreprocess_features2 = ['MSZoning', 'Utilities', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Electrical', 'BsmtFullBath', 'BsmtHalfBath', 'SaleType']\npreprocess_features3 = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\npreprocess_features4 = ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']\npreprocess_features5 = ['KitchenQual']\npreprocess_features6 = ['Functional']\npreprocess_features7 = ['FireplaceQu']\n\nremove_list = preprocess_features1 + preprocess_features2 + preprocess_features3 + preprocess_features4 + preprocess_features5 + preprocess_features6 + preprocess_features7\n\nall_numeric_columns = list(df1.select_dtypes(include=[np.number]).columns.values)\nall_object_columns = list(df1.select_dtypes(include=['object']).columns.values)\n\npreprocess_features8 = [i for i in all_numeric_columns if i not in remove_list]\npreprocess_features9 = [i for i in all_object_columns if i not in remove_list]\n\n# creating transformers\ntransformer1 = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\ntransformer2 = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder())\n])\n\ntransformer3 = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value=\"No Basement\")),\n    ('onehot', OneHotEncoder())\n])\n\ntransformer4 = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value=\"No Garage\")),\n    ('onehot', OneHotEncoder())\n])\n\ntransformer5 = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value=\"TA\")),\n    ('onehot', OneHotEncoder())\n])\n\ntransformer6 = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value=\"Typ\")),\n    ('onehot', OneHotEncoder())\n])\n\ntransformer7 = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value=\"No Fireplace\")),\n    ('onehot', OneHotEncoder())\n])\n\ntransformer8 = Pipeline(steps=[\n    ('scaler', StandardScaler())\n])\n\ntransformer9 = Pipeline(steps=[\n    ('onehot', OneHotEncoder())\n])\n\n\n# final transformer: ColumnTransformer will automatically apply transforms to specific columns\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('t1', transformer1, preprocess_features1),\n        ('t2', transformer2, preprocess_features2),\n        ('t3', transformer3, preprocess_features3),\n        ('t4', transformer4, preprocess_features4),\n        ('t5', transformer5, preprocess_features5),\n        ('t6', transformer6, preprocess_features6),\n        ('t7', transformer7, preprocess_features7),\n        ('t8', transformer8, preprocess_features8),\n        ('t9', transformer9, preprocess_features9)])","98de3013":"# apply full preprocessing pipeline on train data\ndf1 = preprocessor.fit_transform(df1)","b35debee":"# list of ML algorithms for cross-validation\nMLA = [\n    RandomForestRegressor(n_estimators=500, max_leaf_nodes=16, min_samples_leaf=4, n_jobs=-1),\n    \n    ExtraTreesRegressor(n_estimators=500, max_leaf_nodes=16, min_samples_leaf=4, n_jobs=-1),\n    \n    AdaBoostRegressor(DecisionTreeRegressor(max_depth=1), \n                       n_estimators=200, \n                       loss='square',\n                       learning_rate=0.5),\n    \n    XGBRegressor(learning_rate=0.05, max_depth=4, n_estimators=50, reg_alpha=0.01, reg_lambda=0.3, seed=0),\n    \n    BaggingRegressor(base_estimator=DecisionTreeRegressor(), \n                      n_estimators=500,\n                      max_samples=50,\n                      bootstrap=True,\n                      n_jobs=-1),    # Bagging\n    \n    BaggingRegressor(base_estimator=DecisionTreeRegressor(), \n                      n_estimators=500,\n                      max_samples=50,\n                      bootstrap=False,\n                      n_jobs=-1),    # Pasting\n    \n    BaggingRegressor(base_estimator=DecisionTreeRegressor(), \n                      n_estimators=500,\n                      max_samples=100,\n                      bootstrap=True,\n                      max_features=5,\n                      bootstrap_features=True,\n                      n_jobs=-1),    # Random Patches Method\n    \n    BaggingRegressor(base_estimator=DecisionTreeRegressor(), \n                      n_estimators=500,\n                      bootstrap=False,\n                      max_features=5,\n                      bootstrap_features=True,\n                      n_jobs=-1),    # Random Subspaces Method\n    \n    LinearRegression(),\n    \n    Ridge(alpha=1, solver='auto'),\n    \n    Lasso(alpha=0.1),\n    \n    ElasticNet(alpha=0.1, l1_ratio=1),\n    \n    SGDRegressor(early_stopping=True, n_iter_no_change=10, learning_rate='constant', eta0=0.002),\n    \n    SVR(kernel='linear', epsilon=0.1),\n    \n    SVR(kernel='poly', degree=2, C=100, epsilon=0.1),\n    \n    DecisionTreeRegressor(min_samples_leaf=10)\n]","ec72d573":"cv_split = ShuffleSplit(n_splits=10, test_size=0.3, train_size=0.6, random_state=0)\n\nMLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Train Score Mean', 'MLA Test Score Mean' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\nMLA_predict = train_labels.copy()\n\nrow_index = 0\nfor alg in MLA:\n\n    # set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    # cross validation\n    cv_results = cross_validate(alg, df1, train_labels, cv=cv_split, scoring='neg_root_mean_squared_error', return_train_score=True)\n    \n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Score Mean'] = -(cv_results['train_score'].mean())\n    MLA_compare.loc[row_index, 'MLA Test Score Mean'] = -(cv_results['test_score'].mean())\n\n    # save MLA predictions\n    alg.fit(df1, train_labels)\n    MLA_predict[MLA_name] = alg.predict(df1)\n    \n    row_index+=1\n    \n# print and sort table\nMLA_compare.sort_values(by = ['MLA Test Score Mean'], ascending = True, inplace = True)\nMLA_compare","4e9956e5":"shortlisted_model = Ridge()\n\nparam_grid = {\n    'alpha': [1, 2, 3, 10, 20, 30, 50, 100, 300, 400]\n}\n\ngrid_search = GridSearchCV(shortlisted_model, param_grid=param_grid, scoring='neg_root_mean_squared_error', cv=cv_split, return_train_score=True)\ngrid_search.fit(df1, train_labels)\n\nprint('AFTER GridSearch Parameters: ', grid_search.best_params_)\nprint(\"AFTER GridSearch Training RMSE mean: {:.2f}\". format(-max(grid_search.cv_results_['mean_train_score'])))\nprint(\"AFTER GridSearch Test RMSE mean: {:.2f}\". format(-max(grid_search.cv_results_['mean_test_score'])))\nprint('-'*10)","4ed8e641":"cvres = grid_search.cv_results_\nprint(\"Train Score Mean\\t\", \"Test Score Mean\\t\", \"Parameters\")\nprint(\"-\"*70)\nfor mean_train_score, mean_test_score, params in zip(cvres['mean_train_score'], cvres['mean_test_score'], cvres['params']):\n    print(-mean_train_score,\"\\t\", -mean_test_score,\"\\t\", params)","73b76237":"# predictors for voting regressor\npredictor1 = Ridge(alpha=30, solver='auto')\n    \npredictor2 = Lasso(alpha=0.1)\n    \npredictor3 = ElasticNet(alpha=0.1, l1_ratio=1)\n    \npredictor4 = SVR(kernel='linear', epsilon=0.1)\n    \npredictor5 = SVR(kernel='poly', degree=2, C=100, epsilon=0.1)\n    \npredictor6 = RandomForestRegressor(n_estimators=500, max_leaf_nodes=16, min_samples_leaf=4, n_jobs=-1)","b826dbf7":"voting_reg = VotingRegressor(\n    estimators=[('pred1', predictor1), ('pred2', predictor2), ('pred3', predictor3), ('pred4', predictor4), ('pred5', predictor5), ('pred6', predictor6)]\n)\n\n-(cross_val_score(voting_reg, df1, train_labels, cv=cv_split, scoring='neg_root_mean_squared_error').mean())","7e4f88ea":"# copy test set 'Id'\nsubmission_Ids = df_test_raw['Id'].copy()\n\n# remove features with high null value count\ndf_test_raw.drop(columns=drop_columns_list, inplace=True)\n\n# apply preprocessing pipeline to test set\ndf_test_prepared = preprocessor.transform(df_test_raw)","8518f769":"# FINAL MODEL\nfinal_model = Ridge(alpha=30)\nfinal_model.fit(df1, train_labels)\n\n# submission predictions\nfinal_pred = final_model.predict(df_test_prepared)\nfinal_pred = np.expm1(final_pred)","f77f7884":"# create submission file\nmy_submission = pd.DataFrame({'Id': submission_Ids, 'SalePrice': final_pred})\nmy_submission.to_csv(\"submission.csv\", index=False)","98bbd4f0":"We can also check scores for each variation of parameters.","1774b50f":"### 3.2 First look at the data","a67c3895":"# 1. Define the problem\n\nPredict the Selling price of houses.","b8e8f5b2":"Now we can simply make the Final Model as we know the best parameters for it.","722349f6":"### 4.1 Cross-Validation","6db55585":"Look for Outliers and remove them. If you don't want to remove data, you can also use ML models that are robust to outliers.","f614c8ac":"Dataset author recommends removing any houses with more than 4000 square feet from the dataset.\n<br>Reference: [http:\/\/jse.amstat.org\/v19n3\/decock.pdf](http:\/\/jse.amstat.org\/v19n3\/decock.pdf)","c625eb2c":"### 3.3 Data Cleaning (Correcting, Completing, Creating, Converting)","7e44e0bb":"### 3.1 Import libraries","db2212fb":"<font color=\"red\">CAUTION:<\/font> The cell below does a very CPU intensive job and might take a lot of time for you. If you want to run the below 2 cells just select entire code(press ctrl+a) in the cells and un-comment it(press ctrl+\/).","bf221d41":"# 3. Prepare data for consumption","18384fb1":"#### 4.2.1 Voting Regressor","0db19646":"According to this competition page submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)","caac368f":"# 5. Validate and Implement","7bb29628":"# 2. Gather the data\n\nData is given to us in seperate Training and Test sets along with attributes description in a seperate text file.","64a53d9d":"### 4.2 Tune Model with Hyper-Parameters","5249683e":"# 4. Model Data","975ab9ed":"Voting does not do better than our best individual classifier."}}