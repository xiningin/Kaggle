{"cell_type":{"8f551499":"code","42a9c297":"code","9254082b":"code","ba1f3d0c":"code","475d274b":"code","9b52ab0c":"code","737d3f02":"code","06613fc7":"code","899c1481":"code","dd301d02":"code","37902be0":"code","425b470a":"code","8f37461e":"code","e7ec1db4":"code","ca6a6d53":"code","a7932951":"code","0c9a4f8b":"code","6dcd6bbe":"code","712549c5":"code","bacc6d79":"code","a63540e6":"code","ddb27f56":"code","3f410280":"code","0aa1d222":"code","85de2de9":"code","faef8959":"code","7fc114d8":"code","44f6f988":"code","5c272cfd":"code","d4b35444":"code","44e59c5a":"code","941ecf46":"markdown","d323cf39":"markdown","9a895703":"markdown","e6cba97c":"markdown","a55ab8c8":"markdown","257d9e88":"markdown","b6d0582a":"markdown","7e2b8784":"markdown","d611fb3f":"markdown","c06eb60b":"markdown","177c5926":"markdown","a89b04e1":"markdown","1e1e5e41":"markdown","a541a2e9":"markdown","f8b5d085":"markdown","67b558d4":"markdown","2ba70aca":"markdown","1d58f247":"markdown","bc721294":"markdown","d98f94ba":"markdown","87f25de5":"markdown","da64fae6":"markdown","14b458ae":"markdown","68ea6b80":"markdown","5c0fa4ad":"markdown","2876ea02":"markdown","f44a4950":"markdown","b17349f2":"markdown","04ff6f9d":"markdown","96b45d21":"markdown","d18d81e0":"markdown","60b4be8f":"markdown","b1dfd83a":"markdown","baad5570":"markdown","51777323":"markdown","e9589d8b":"markdown","697f9dec":"markdown"},"source":{"8f551499":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# %matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\nimport catboost as cb\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import GridSearchCV\n\nsns.set_theme()\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","42a9c297":"#Reading the dataset\ndata = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndata.head()","9254082b":"#Printing out some information about the data\ndef eda(data):\n    print(\"----------Top-5- Record----------\")\n    print(data.head(5))\n    print(\"-----------Information-----------\")\n    print(data.info())\n    print(\"-----------Data Types-----------\")\n    print(data.dtypes)\n    print(\"----------Missing value-----------\")\n    print(data.isnull().sum())\n    print(\"----------Null value-----------\")\n    print(data.isna().sum())\n    print(\"----------Shape of Data----------\")\n    print(data.shape)\neda(data)","ba1f3d0c":"#Lets have a look at the statistical info about our data\ndata.describe()","475d274b":"# replace the 0 values of the impacted columns with the mean values\n\ncols = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']\nfor i in cols:\n   data[i].replace(0,data[i].mean(),inplace=True)\n\ndata.head()","9b52ab0c":"group_Outcome= data.groupby('Outcome')['Pregnancies'].count().reset_index()\ngroup_Outcome.rename(columns={'Pregnancies':'Count'}, inplace=True)\ngroup_Outcome['Percentages'] = round(group_Outcome['Count']\/sum(group_Outcome['Count'])*100,2)\n\n# fig\nfig = plt.figure(figsize=(12,4))\n\n# axes\naxes = fig.add_axes([0,0,1,1])\n\n# barh\naxes.barh(width=group_Outcome['Percentages'][0]+group_Outcome['Percentages'][1], y=0, color='silver')\naxes.barh(width=group_Outcome['Percentages'][0], y=0, color='steelblue')\n\n# annotation\naxes.text(group_Outcome['Percentages'][0]\/2.5, 0, f\"{group_Outcome['Percentages'][0]}%\", color='black', fontsize=30, fontweight='bold')\naxes.text(group_Outcome['Percentages'][0]\/2.5, -0.1, f\"({group_Outcome['Count'][0]})\", color='black', fontsize=30, fontweight='bold')\naxes.text((group_Outcome['Percentages'][0]+group_Outcome['Percentages'][1])\/1.3, 0, f\"{group_Outcome['Percentages'][1]}%\", color='black', fontsize=30, fontweight='bold')\naxes.text((group_Outcome['Percentages'][0]+group_Outcome['Percentages'][1])\/1.3, -0.1, f\"({group_Outcome['Count'][1]})\", color='black', fontsize=30, fontweight='bold')\n\n# title\naxes.text(group_Outcome['Percentages'][0]\/2.2, 0.5, 'No ', color='Black', fontsize=30, fontweight='bold')\naxes.text((group_Outcome['Percentages'][0]+group_Outcome['Percentages'][1])\/1.27, 0.5, 'Yes', color='Black', fontsize=30, fontweight='bold')\n\n# conclusion\naxes.text(110, 0.3, 'We observe an unbalanced number of target.', fontsize=16, fontweight='bold', color='black', alpha=0.6)\naxes.text(110, 0.19, '''The number of people without diabetes significantly \nexceeds the number of people with diabetes.''', fontsize=16, fontweight='bold', color='black', alpha=0.6)\n\n# axis\naxes.axis('off')\n\nfig.show()","737d3f02":"plt.style.use('ggplot') \n\nf, ax = plt.subplots(figsize=(11, 15))\n\nax.set_facecolor('#B1DEFD')\nax.set(xlim=(-.05, 200))\nplt.ylabel('Variables')\nplt.title(\"Overview\")\nax = sns.boxplot(data = data, \n  orient = 'h', \n  palette = 'Set2',)","06613fc7":"def Remove_Outlier (col):\n    Q1,Q3 = np.percentile (col,[25,75])\n    \n    IQR= Q3-Q1\n    \n    upper_range =  Q3+(IQR*1.5)\n    \n    lower_range =  Q1-(IQR*1.5)\n    \n    return upper_range,lower_range\n\n# print(\"Shape Of The Before Ouliers: \", data.shape)\n\nfor i in data[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age']]:\n    ur,lr = Remove_Outlier(data[i])\n    data[i]= np.where(data[i]>ur,ur,data[i])\n    data[i]= np.where(data[i]<lr,lr,data[i])\n\n# print(\"Shape Of The After Ouliers: \", data.shape)\n","899c1481":"# Add all column names to a list except for the target variable (outcome)\ncolumns=data.columns\ncolumns=list(columns)\ncolumns.pop()\nprint(\"Column names except for the target column are :\",columns)\n\n#Graphs to be plotted with these colors\ncolours=['b','c','g','k','m','r','y','b']\nsns.set(rc={'figure.figsize':(15,17)})\nsns.set_style(style='white')\nfor i in range(len(columns)):\n    \n    plt.subplot(4,2,i+1)\n    sns.distplot(data[columns[i]], hist=True, rug=True, color=colours[i])","dd301d02":"#Pregnencies vs Outcome\nfig = px.histogram(data, x = data['Pregnancies'], color = 'Outcome')\nfig.show()\nfig2 = px.box(data, x = data['Pregnancies'], color = 'Outcome')\nfig2.show()","37902be0":"#Glucose vs Outcome\nfig = px.histogram(data, x = data['Glucose'], color = 'Outcome')\nfig.show()\nfig2 = px.box(data, x = data['Glucose'], color = 'Outcome')\nfig2.show()","425b470a":"#BloodPressure vs Outcome\nfig = px.histogram(data, x = data['BloodPressure'], color = 'Outcome')\nfig.show()\nfig2 = px.box(data, x = data['BloodPressure'], color = 'Outcome')\nfig2.show()","8f37461e":"#SkinThickness vs Outcome\nfig = px.histogram(data, x = data['SkinThickness'], color = 'Outcome')\nfig.show()\nfig2 = px.box(data, x = data['SkinThickness'], color = 'Outcome')\nfig2.show()","e7ec1db4":"#Insulin vs Outcome\nfig = px.histogram(data, x = data['Insulin'], color = 'Outcome')\nfig.show()\nfig2 = px.box(data, x = data['Insulin'], color = 'Outcome')\nfig2.show()","ca6a6d53":"#BMI vs Outcome\nfig = px.histogram(data, x = data['BMI'], color = 'Outcome')\nfig.show()\nfig2 = px.box(data, x = data['BMI'], color = 'Outcome')\nfig2.show()","a7932951":"# DiabetesPedigreeFunction andd Outcome\nfig = px.histogram(data, x = data['DiabetesPedigreeFunction'], color = 'Outcome')\nfig.show()\nfig2 = px.box(data, x = data['DiabetesPedigreeFunction'], color = 'Outcome')\nfig2.show()","0c9a4f8b":"# Age andd Outcome\nfig = px.histogram(data, x = data['Age'], color = 'Outcome')\nfig.show()\nfig2 = px.box(data, x = data['Age'], color = 'Outcome')\nfig2.show()","6dcd6bbe":"from sklearn.preprocessing import StandardScaler\n# scaler\nscaler = StandardScaler()\nnorm = scaler.fit_transform(data[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']])\ndf_norm = pd.DataFrame({'Pregnancies': norm[ :, 0], 'Glucose' : norm[ :, 1], 'BloodPressure' : norm[ :, 2], 'SkinThickness' : norm[ :, 3],\n                       'Insulin' : norm[ :, 4], 'BMI' : norm[ :, 5], 'DiabetesPedigreeFunction' : norm[ :, 5], 'Age' : norm[ :, 6]}, \n                       columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'])\ndf_norm['Outcome'] = data['Outcome']\n","712549c5":"# split\nx = df_norm.drop(['Outcome'], axis=1)\ny = df_norm['Outcome']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n","bacc6d79":"# over sampling\nos = SMOTE(random_state=42)\ncolumns = x_train.columns\nos_data_x,os_data_y = os.fit_resample(x_train, y_train.ravel())","a63540e6":"# logistic regression\nlog_params = {'penalty':['l1', 'l2'], \n              'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 100], \n              'solver':['liblinear', 'saga']} \nlog_model = GridSearchCV(LogisticRegression(), log_params, cv=5) #Tuning the hyper-parameters\nlog_model.fit(os_data_x, os_data_y)\nlog_predict = log_model.predict(x_test)\nlog_score = log_model.best_score_","ddb27f56":"# knn\nknn_params = {'n_neighbors': list(range(3, 20, 2)),\n          'weights':['uniform', 'distance'],\n          'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],\n          'metric':['euclidean', 'manhattan', 'chebyshev', 'minkowski']}\nknn_model = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5) #Tuning the hyper-parameters\nknn_model.fit(os_data_x, os_data_y)\nknn_predict = knn_model.predict(x_test)\nknn_score = knn_model.best_score_","3f410280":"# svc\nsvc_params = {'C': [0.001, 0.01, 0.1, 1],\n              'kernel': [ 'linear' , 'poly' , 'rbf' , 'sigmoid' ]}\nsvc_model = GridSearchCV(SVC(), svc_params, cv=5) #Tuning the hyper-parameters\nsvc_model.fit(os_data_x, os_data_y)\nsvc_predict = svc_model.predict(x_test)\nsvc_score = svc_model.best_score_\n","0aa1d222":"# decsion tree\ndt_params = {'criterion' : ['gini', 'entropy'],\n              'splitter': ['random', 'best'], \n              'max_depth': [3, 5, 7, 9, 11, 13]}\ndt_model = GridSearchCV(DecisionTreeClassifier(), dt_params, cv=5) #Tuning the hyper-parameters\ndt_model.fit(os_data_x, os_data_y)\ndt_predict = dt_model.predict(x_test)\ndt_score = dt_model.best_score_","85de2de9":"# rf\nrf_params = {'criterion' : ['gini', 'entropy'],\n             'n_estimators': list(range(5, 26, 5)),\n             'max_depth': list(range(3, 20, 2))}\nrf_model = GridSearchCV(RandomForestClassifier(), rf_params, cv=5) #Tuning the hyper-parameters\nrf_model.fit(os_data_x, os_data_y)\nrf_predict = rf_model.predict(x_test)\nrf_score = rf_model.best_score_","faef8959":"# sgd\nsgd_params = {'loss' : ['hinge', 'log', 'squared_hinge', 'modified_huber'],\n              'alpha' : [0.0001, 0.001, 0.01, 0.1, 1, 10],\n              'penalty' : ['l2', 'l1', 'none']}\nsgd_model = GridSearchCV(SGDClassifier(max_iter=10000), sgd_params, cv=5) #Tuning the hyper-parameters\nsgd_model.fit(os_data_x, os_data_y)\nsgd_predict = sgd_model.predict(x_test)\nsgd_score = sgd_model.best_score_","7fc114d8":"# lgb\nlgb_params = {'n_estimators': [5, 10, 15, 20, 25, 50, 100],\n                   'learning_rate': [0.01, 0.05, 0.1],\n                   'num_leaves': [7, 15, 31],\n                  }\nlgb_model = GridSearchCV(LGBMClassifier(), lgb_params, cv=5) #Tuning the hyper-parameters\nlgb_model.fit(os_data_x, os_data_y)\nlgb_predict = lgb_model.predict(x_test)\nlgb_score = lgb_model.best_score_","44f6f988":"# xgb\nxgb_params = {'max_depth': [3, 5, 7, 9],\n              'n_estimators': [5, 10, 15, 20, 25, 50, 100],\n              'learning_rate': [0.01, 0.05, 0.1]}\nxgb_model = GridSearchCV(xgb.XGBClassifier(eval_metric='logloss'), xgb_params, cv=5) #Tuning the hyper-parameters\nxgb_model.fit(os_data_x, os_data_y)\nxgb_predict = xgb_model.predict(x_test)\nxgb_score = xgb_model.best_score_","5c272cfd":"# cb\ncb_params = {'learning_rate': [0.01, 0.05, 0.1],\n             'depth': [3, 5, 7, 9]}\ncb_model = GridSearchCV(cb.CatBoostClassifier(verbose=False), cb_params, cv=5) #Tuning the hyper-parameters\ncb_model.fit(os_data_x, os_data_y)\ncb_predict = cb_model.predict(x_test)\ncb_score = cb_model.best_score_","d4b35444":"models = ['LogisticRegression', 'KNeighborsClassifier', 'SVC', 'DecisionTreeClassifier', \n          'RandomForestClassifier', 'SGDClassifier', 'LGBMClassifier', 'XGBClassifier', 'CatBoostClassifier']\nscores = [log_score, knn_score, svc_score, dt_score, rf_score, sgd_score, lgb_score, xgb_score, cb_score]\nscore_table = pd.DataFrame({'Model':models, 'Score':scores})\nscore_table.sort_values(by='Score', axis=0, ascending=False)\nprint(score_table.sort_values(by='Score', ascending=False))\nsns.barplot(x = score_table['Score'], y = score_table['Model'], palette='viridis');\n","44e59c5a":"#Printing classification report for catboost claassifier\nfrom sklearn import metrics\nprint('Classification Report_test','\\n',metrics.classification_report(y_test, cb_predict))","941ecf46":"\n<p style=\"font-family: Arials; line-height: 1.3; font-size: 23px; font-weight: bold; letter-spacing: 2px; text-align: left; color: #23527c\">Let's look at how glucose levels affect the chances if being Diabetic<\/p>","d323cf39":"\n<p style=\"font-family: Arials; line-height: 1.3; font-size: 23px; font-weight: bold; letter-spacing: 2px; text-align: left; color: #23527c\">Let's look at how the BMI affect the chances if being Diabetic<\/p>","9a895703":"<p style=\"font-family: Arials; line-height: 1.3; font-size: 23px; font-weight: bold; letter-spacing: 2px; text-align: left; color: #23527c\">As we can see, there are no missing and null values in our dataset. Well, that's a relief!<\/p>","e6cba97c":"---\n\n<p style=\"font-family: Arials; line-height: 1.3; font-size: 27px; font-weight: bold; letter-spacing: 2px; text-align: center; color: #23527c\">Exploratory data analysis<\/p>","a55ab8c8":"<p style=\"font-family: Arials; line-height: 1.3; font-size: 23px; font-weight: bold; letter-spacing: 2px; text-align: left; color: #23527c\">Let's look at how number of pregnancies affect the chances if being Diabetic<\/p>","257d9e88":"- This feature needs further analysis","b6d0582a":"# <span style=\"font-family: Arials; font-size: 25px; font-style: bold; font-weight: bold; letter-spacing: 2px; color: #23527c\">1. INTRODUCTION<\/span>\n<hr style=\"height: 0.5px; border: 0; background-color: 'Black'\">\n\nThe objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage. ","7e2b8784":"\n<p style=\"font-family: Arials; line-height: 1.3; font-size: 23px; font-weight: bold; letter-spacing: 2px; text-align: left; color: #23527c\">Let's have a look at the distribution of the data<\/p>","d611fb3f":"<p style=\"font-family: Arials; line-height: 1.3; font-size: 27px; font-weight: bold; letter-spacing: 2px; text-align: center; color: #23527c\">Reading the dataset<\/p>","c06eb60b":" ### we observe that there is less chance of diabetes among young people and more chances for the people above the Age of years\n---","177c5926":"---\n\n\n<p style=\"font-family: Arials; line-height: 1.3; font-size: 27px; font-weight: bold; letter-spacing: 2px; text-align: center; color: #23527c\">Evaluation<\/p>\n","a89b04e1":"\n---\n\n<p style=\"font-family: Arials; line-height: 1.3; font-size: 23px; font-weight: bold; letter-spacing: 2px; text-align: left; color: #23527c\">Thank You!<\/p>\n","1e1e5e41":"  ### that higher the Insulin level more the chances of diabetes.\u00b6","a541a2e9":"Now that the 0 values are accounted for, we can proceed further with the rest of the data exploratory analysis","f8b5d085":"\n<p style=\"font-family: Arials; line-height: 1.3; font-size: 23px; font-weight: bold; letter-spacing: 2px; text-align: left; color: #23527c\">Let's look at how the Insulin level affect the chances if being Diabetic<\/p>","67b558d4":"\n\n<p style=\"font-family: Arials; line-height: 1.3; font-size: 23px; font-weight: bold; letter-spacing: 2px; text-align: left; color: #23527c\">Let's look at how the SkinThickness affect the chances if being Diabetic<\/p>","2ba70aca":"\nBased on the understanding of the parameters, it seems highly unlikely that glucose, bloodpressure, skinthickness, insulin and bmi levels are 0.\nI will hence replace the 0 values with the mean of each parameter.","1d58f247":" ### The plots show that Glucose, Blood Pressure, BMI are normally distributed.Pregnancies, Insulin, Age, DiabetesPedigreeFunction are rightly skewed.","bc721294":"### Looking at both plots we can seee that higher the number of pragnancies, more is the risks of diabetes","d98f94ba":"\n<p style=\"font-family: Arials; line-height: 1.3; font-size: 27px; font-weight: bold; letter-spacing: 2px; text-align: center; color: #23527c\">Splitting the Data into training and testing sets<\/p>","87f25de5":" ### We observe that higher the BMI more the chances of diabetes.","da64fae6":"\n\n<p style=\"font-family: Arials; line-height: 1.3; font-size: 23px; font-weight: bold; letter-spacing: 2px; text-align: left; color: #23527c\">Let's look at how Age affect the chances if being Diabetic<\/p>","14b458ae":"<p style=\"font-family: Arials; line-height: 1.3; font-size: 30px; font-weight: bold; letter-spacing: 2px; text-align: center; color: #23527c\">Exploring the PIMA Indian Diabetes dataset<\/p>\n","68ea6b80":"<p style=\"font-family: Arials; line-height: 1.3; font-size: 22px; font-weight: bold; letter-spacing: 2px; text-align: left; color: #23527c\">An overview of the columns in the dataset<\/p>\n\n- **Pregnancies**: Number of times pregnant\n- **Glucose**: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n- **BloodPressure**: Diastolic blood pressure (mm Hg)\n- **SkinThickness**: Triceps skin fold thickness (mm)\n- **Insulin**: 2-Hour serum insulin (mu U\/ml)\n- **BMI**: Body mass index (weight in kg\/(height in m)^2)\n- **DiabetesPedigreeFunction**: Diabetes pedigree function\n- **Age**: Age (years)\n- **Cabin** : Cabin Number\n- **Outcome**: Class variable (0 or 1)","5c0fa4ad":"\n<p style=\"font-family: Arials; line-height: 1.3; font-size: 27px; font-weight: bold; letter-spacing: 2px; text-align: center; color: #23527c\">Scaling the Data<\/p>\n","2876ea02":"\n<p style=\"font-family: Arials; line-height: 1.3; font-size: 23px; font-weight: bold; letter-spacing: 2px; text-align: left; color: #23527c\">We can see that CatBoost classifier is the Best performing model with a score of .829599<\/p>","f44a4950":"### we can seee that the probabilty of diabetes is higher when Blood pressure is high.","b17349f2":"### Higher Glucose level leads to more chances of Diabetes!","04ff6f9d":" ### We observe that diabetic people have higher DiabetesPedigreeFunction value i,e genetic influence plays some role in the Diabetes among patients.\n","96b45d21":"\n<p style=\"font-family: Arials; line-height: 1.3; font-size: 23px; font-weight: bold; letter-spacing: 2px; text-align: left; color: #23527c\">Let's look at how blood pressure affect the chances if being Diabetic<\/p>","d18d81e0":"![](https:\/\/www.medicoverhospitals.in\/wp-content\/uploads\/2020\/11\/Diabetes-1200x438.jpg)","60b4be8f":"---\n\n<p style=\"font-family: Arials; line-height: 1.3; font-size: 27px; font-weight: bold; letter-spacing: 2px; text-align: center; color: #23527c\">Modelling<\/p>\n","b1dfd83a":"<p style=\"font-family: Arials; line-height: 1.3; font-size: 23px; font-weight: bold; letter-spacing: 2px; text-align: left; color: #23527c\">Let's try to get an idea about the outliers in our dataset<\/p>","baad5570":"### We can clearly see outliers are present in the data. So now we wil remove the outliers","51777323":"\n<p style=\"font-family: Arials; line-height: 1.3; font-size: 23px; font-weight: bold; letter-spacing: 2px; text-align: left; color: #23527c\">Let's look at how DiabetesPedigreeFunction affect the chances if being Diabetic<\/p>","e9589d8b":"\n<p style=\"font-family: Arials; line-height: 1.3; font-size: 27px; font-weight: bold; letter-spacing: 2px; text-align: center; color: #23527c\">Oversampling the data using SMOTE to deal with imbalance in dataset<\/p>\n","697f9dec":"### Moving ahead let's have a look at the distribution of our dependent variable - **Outcome**"}}