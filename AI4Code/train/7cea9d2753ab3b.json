{"cell_type":{"51c6ac4b":"code","de69ae45":"code","d95a6555":"code","33c22b64":"code","984db825":"code","a27045d1":"code","6011fa9c":"code","a513b9ef":"code","de207689":"code","57f72e68":"code","565e7e7b":"code","6461abc6":"code","bc9e5c07":"code","2245237c":"code","1e836ba3":"code","d646ea4a":"code","5a12df09":"code","15507162":"code","b3870bd5":"code","0872a370":"code","302a6ac9":"code","fe8aab62":"code","65b2385b":"code","ed1fe56d":"code","6f708e74":"code","674d2797":"code","eceea035":"code","fd8406d5":"code","5efe3dfe":"code","937d4c80":"code","f7a1b454":"code","42c65bbd":"code","1f505281":"code","17bd3740":"code","2f30dbd1":"code","f1defb03":"code","1ca529fc":"code","813a3169":"code","16e395da":"code","f4e8832a":"markdown","fa871843":"markdown","73effc55":"markdown","5c8c1fd0":"markdown","3dfc38b8":"markdown","3404974f":"markdown","32b0378a":"markdown","43bb491d":"markdown","04fe9d3f":"markdown"},"source":{"51c6ac4b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","de69ae45":"df = pd.read_csv('..\/input\/US_Heart_Patients.csv')\ndf.head()","d95a6555":"df.shape","33c22b64":"df.isnull().sum()","984db825":"df = df.fillna(method='ffill')","a27045d1":"df.info()","6011fa9c":"from sklearn.model_selection import train_test_split\n\nX = df.drop('TenYearCHD', axis=1)\ny = df['TenYearCHD']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)","a513b9ef":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(max_depth=3, min_samples_leaf=10 )\ndt.fit(X, y)","de207689":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\n\ny_pred_train = dt.predict(X_train)\ny_pred = dt.predict(X_test)\ny_prob = dt.predict_proba(X_test)","57f72e68":"from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n\nprint('Accuracy of Decision Tree-Train: ', accuracy_score(y_pred_train, y_train))\nprint('Accuracy of Decision Tree-Test: ', accuracy_score(y_pred, y_test))","565e7e7b":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV","6461abc6":"dt = DecisionTreeClassifier()\n\nparams = {'max_depth' : [2,3,4,5,6,7,8],\n        'min_samples_split': [2,3,4,5,6,7,8,9,10],\n        'min_samples_leaf': [1,2,3,4,5,6,7,8,9,10]}\n\ngsearch = GridSearchCV(dt, param_grid=params, cv=3)\n\ngsearch.fit(X,y)\n\ngsearch.best_params_","bc9e5c07":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n                                    X, y, test_size=0.3, random_state=1)","2245237c":"dt = DecisionTreeClassifier(**gsearch.best_params_)\n\ndt.fit(X_train, y_train)\n\ny_pred_train = dt.predict(X_train)\ny_prob_train = dt.predict_proba(X_train)[:,1]\n\ny_pred = dt.predict(X_test)\ny_prob = dt.predict_proba(X_test)[:,1]\n\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n\nprint('Accuracy of Decision Tree-Train: ', accuracy_score(y_pred_train, y_train))\nprint('Accuracy of Decision Tree-Test: ', accuracy_score(y_pred, y_test))","1e836ba3":"print('AUC of Decision Tree-Train: ', roc_auc_score(y_train, y_prob_train))\nprint('AUC of Decision Tree-Test: ', roc_auc_score(y_test, y_prob))","d646ea4a":"from scipy.stats import randint as sp_randint\n\ndt = DecisionTreeClassifier(random_state=1)\n\nparams = {'max_depth' : sp_randint(2,10),\n        'min_samples_split': sp_randint(2,50),\n        'min_samples_leaf': sp_randint(1,20),\n         'criterion':['gini', 'entropy']}\n\nrand_search = RandomizedSearchCV(dt, param_distributions=params, cv=3, \n                                 random_state=1)\n\nrand_search.fit(X, y)\nprint(rand_search.best_params_)","5a12df09":"dt = DecisionTreeClassifier(**rand_search.best_params_)\n\ndt.fit(X_train, y_train)\n\ny_pred_train = dt.predict(X_train)\ny_prob_train = dt.predict_proba(X_train)[:,1]\n\ny_pred = dt.predict(X_test)\ny_prob = dt.predict_proba(X_test)[:,1]\n\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n\nprint('Accuracy of Decision Tree-Train: ', accuracy_score(y_pred_train, y_train))\nprint('Accuracy of Decision Tree-Test: ', accuracy_score(y_pred, y_test))","15507162":"print('AUC of Decision Tree-Train: ', roc_auc_score(y_train, y_prob_train))\nprint('AUC of Decision Tree-Test: ', roc_auc_score(y_test, y_prob))","b3870bd5":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators=10, random_state=1)\n\nrfc.fit(X_train, y_train)\n\ny_pred_train = rfc.predict(X_train)\ny_prob_train = rfc.predict_proba(X_train)[:,1]\n\ny_pred = rfc.predict(X_test)\ny_prob = rfc.predict_proba(X_test)[:,1]\n\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n\nprint('Accuracy of Random Forest-Train: ', accuracy_score(y_pred_train, y_train))\nprint('Accuracy of Random Forest-Test: ', accuracy_score(y_pred, y_test))\n","0872a370":"print('AUC of Random Forest-Train: ', roc_auc_score(y_train, y_prob_train))\nprint('AUC of Random Forest-Test: ', roc_auc_score(y_test, y_prob))","302a6ac9":"from scipy.stats import randint as sp_randint\n\nrfc = RandomForestClassifier(random_state=1)\n\nparams = {'n_estimators': sp_randint(5,25),\n    'criterion': ['gini', 'entropy'],\n    'max_depth': sp_randint(2, 10),\n    'min_samples_split': sp_randint(2,20),\n    'min_samples_leaf': sp_randint(1, 20),\n    'max_features': sp_randint(2,15)}\n\nrand_search_rfc = RandomizedSearchCV(rfc, param_distributions=params,\n                                 cv=3, random_state=1)\n\nrand_search_rfc.fit(X, y)\nprint(rand_search_rfc.best_params_)","fe8aab62":"rfc = RandomForestClassifier(**rand_search_rfc.best_params_)\n\nrfc.fit(X_train, y_train)\n\ny_pred_train = rfc.predict(X_train)\ny_prob_train = rfc.predict_proba(X_train)[:,1]\n\ny_pred = rfc.predict(X_test)\ny_prob = rfc.predict_proba(X_test)[:,1]\n\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n\nprint('Accuracy of Random Forest-Train: ', accuracy_score(y_pred_train, y_train))\nprint('Accuracy of Random Forest-Test: ', accuracy_score(y_pred, y_test))","65b2385b":"print('AUC of Random Forest-Train: ', roc_auc_score(y_train, y_prob_train))\nprint('AUC of Random Forest-Test: ', roc_auc_score(y_test, y_prob))","ed1fe56d":"fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n\nplt.plot(fpr, tpr)\nplt.plot(fpr, fpr, 'r-')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.show()","6f708e74":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()","674d2797":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\n\nXs = ss.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\nX_trains = ss.fit_transform(X_train)\nX_tests = ss.transform(X_test)","eceea035":"knn.fit(X_trains, y_train)\n\ny_pred_train = knn.predict(X_trains)\ny_prob_train = knn.predict_proba(X_trains)[:,1]\n\ny_pred = knn.predict(X_tests)\ny_prob = knn.predict_proba(X_tests)[:,1]\n\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n\nprint('Accuracy of kNN-Train: ', accuracy_score(y_pred_train, y_train))\nprint('Accuracy of kNN-Test: ', accuracy_score(y_pred, y_test))","fd8406d5":"print('AUC of kNN-Train: ', roc_auc_score(y_train, y_prob_train))\nprint('AUC of kNN-Test: ', roc_auc_score(y_test, y_prob))","5efe3dfe":"knn = KNeighborsClassifier()\n\nparams = {'n_neighbors': sp_randint(1,20),\n        'p': sp_randint(1,5)}\n\nrand_search_knn = RandomizedSearchCV(knn, param_distributions=params,\n                                 cv=3, random_state=1)\nrand_search_knn.fit(Xs, y)\nprint(rand_search_knn.best_params_)","937d4c80":"knn = KNeighborsClassifier(**rand_search_knn.best_params_)\n\nknn.fit(X_trains, y_train)\n\ny_pred_train = knn.predict(X_trains)\ny_prob_train = knn.predict_proba(X_trains)[:,1]\n\ny_pred = knn.predict(X_tests)\ny_prob = knn.predict_proba(X_tests)[:,1]\n\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n\nprint('Accuracy of Random Forest-Train: ', accuracy_score(y_pred_train, y_train))\nprint('Accuracy of Random Forest-Test: ', accuracy_score(y_pred, y_test))","f7a1b454":"print('AUC of kNN-Train: ', roc_auc_score(y_train, y_prob_train))\nprint('AUC of kNN-Test: ', roc_auc_score(y_test, y_prob))","42c65bbd":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression","1f505281":"lr = LogisticRegression(solver='liblinear')\nrfc = RandomForestClassifier(**rand_search_rfc.best_params_)\nknn = KNeighborsClassifier(**rand_search_knn.best_params_)","17bd3740":"clf = VotingClassifier(estimators=[('lr',lr), ('rfc',rfc), ('knn',knn)], \n                       voting='soft')\nclf.fit(X_train, y_train)\ny_pred_train = clf.predict(X_train)\ny_prob_train = clf.predict_proba(X_train)[:,1]\n\ny_pred = clf.predict(X_test)\ny_prob = clf.predict_proba(X_test)[:,1]","2f30dbd1":"print('Accuracy of Stacked Algos-Train: ', accuracy_score(y_pred_train, y_train))\nprint('Accuracy of Stacked Algos-Test: ', accuracy_score(y_pred, y_test))","f1defb03":"print('AUC of Stacked Algos: ', roc_auc_score(y_train, y_prob_train))\nprint('AUC of Stacked Algos: ', roc_auc_score(y_test, y_prob))","1ca529fc":"clf = VotingClassifier(estimators=[('lr',lr), ('rfc',rfc), ('knn',knn)], \n                       voting='soft', weights=[2,3,1])\nclf.fit(X_train, y_train)\ny_pred_train = clf.predict(X_train)\ny_prob_train = clf.predict_proba(X_train)[:,1]\n\ny_pred = clf.predict(X_test)\ny_prob = clf.predict_proba(X_test)[:,1]","813a3169":"print('Accuracy of Stacked Algos-Train: ', accuracy_score(y_pred_train, y_train))\nprint('Accuracy of Stacked Algos-Test: ', accuracy_score(y_pred, y_test))","16e395da":"print('AUC of Stacked Algos: ', roc_auc_score(y_train, y_prob_train))\nprint('AUC of Stacked Algos: ', roc_auc_score(y_test, y_prob))","f4e8832a":"## Hyperparameter Tuning\n### Grid Search","fa871843":"## Random Forest Classifier","73effc55":"### Hyperparameter Tuning of Random Forest","5c8c1fd0":"## Stacking Algorithms","3dfc38b8":"### Hyperparameter Tuning - Random Search","3404974f":"For Grid search we considered max_depth between 2 to 8, min_samples_split between 2 to 10 and min_samples_leaf between 1 to 10. The best parameter for the decision was found to be 2, 5 and 2.\n\nNow we will build decision tree using these hyperparameters.\nAlso we will Select the best model using the AUC curve.\nThe area covered by the curve is the area between the orange line (ROC) and the axis. This area covered is AUC. The bigger the area covered, the better the machine learning models is at distinguishing the given classes. Ideal value for AUC is 1.","32b0378a":"We noticed that there is large difference between the model performance Train and Test. Hence, we will tune the Hyperparamter and check the results.","43bb491d":"We can see that there are few null values. Hence we can fill it by using forward fill as the null values are small.","04fe9d3f":"## k-NN Classifier"}}