{"cell_type":{"813ca8e9":"code","e3fe6fe8":"code","c2099af2":"code","e0eb2057":"code","1ee94ccd":"code","713ce935":"code","26e3ec94":"code","080d0bc4":"code","6ba15451":"code","a7f59501":"code","c24e4722":"code","d147dd9c":"code","b57b58a9":"code","8bdbb9f0":"code","6115d62b":"code","4983688d":"code","0a12c3a5":"code","5a3fc1de":"code","54dee343":"code","b100c7e5":"code","319e10d8":"code","ec61dcd5":"code","b2473e72":"markdown","a902b65a":"markdown","de7d013f":"markdown","333b62d2":"markdown","ff7e9cfe":"markdown","9c4b4a1b":"markdown","c955ace6":"markdown","23348f5c":"markdown","1e1a19fe":"markdown","527610c9":"markdown","4de80dd5":"markdown","48f50d91":"markdown","72dd2a5e":"markdown","935578d2":"markdown","90e9c84d":"markdown","eaec23f6":"markdown","8a45bcab":"markdown","9aacbf34":"markdown","4bc6b06a":"markdown"},"source":{"813ca8e9":"# Fixing a problem with Skopt (see https:\/\/github.com\/scikit-optimize\/scikit-optimize\/issues\/981)\n!conda install scipy=='1.5.3' --y","e3fe6fe8":"!conda install scipy=='1.5.3' scikit-learn=='0.23.2' --y\n","c2099af2":"# Importing core libraries\nimport numpy as np\nimport pandas as pd\nfrom time import time\nimport pprint\nimport joblib\nfrom functools import partial\n\n# Suppressing warnings because of skopt verbosity\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Classifier\/Regressor\nfrom xgboost import XGBRegressor, DMatrix\n\n# Model selection\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import make_scorer\n\n# Skopt functions\nfrom skopt import BayesSearchCV\nfrom skopt.callbacks import DeadlineStopper, DeltaYStopper\nfrom skopt.space import Real, Categorical, Integer\n\n# Data processing\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer","e0eb2057":"# Loading data \nX_train = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\nX_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")","1ee94ccd":"# Preparing data as a tabular matrix\ny_train = X_train.target\nX_train = X_train.set_index('id').drop('target', axis='columns')\nX_test = X_test.set_index('id')","713ce935":"# Stratifying the target\ny_stratified = pd.cut(y_train.rank(method='first'), bins=10, labels=False)","26e3ec94":"# Winsorizing lower bounds\nfrom scipy.stats.mstats import winsorize\ny_train = np.array(winsorize(y_train, [0.002, 0.0]))","080d0bc4":"# Pointing out categorical features\ncategoricals = [item for item in X_train.columns if 'cat' in item]","6ba15451":"# Dealing with categorical data using get_dummies\ndummies = pd.get_dummies(X_train.append(X_test)[categoricals])\nX_train[dummies.columns] = dummies.iloc[:len(X_train), :]\nX_test[dummies.columns] = dummies.iloc[len(X_train): , :]\ndel(dummies)","a7f59501":"# Dealing with categorical data using OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\nX_train[categoricals] = ordinal_encoder.fit_transform(X_train[categoricals])\nX_test[categoricals] = ordinal_encoder.transform(X_test[categoricals])","c24e4722":"# Feature selection (https:\/\/www.kaggle.com\/lucamassaron\/tutorial-feature-selection-with-boruta-shap)\nimportant_features = ['cat8_E', 'cont0', 'cont5', 'cont7', 'cont8', 'cat1_A', 'cont2', 'cont13', \n                      'cont3', 'cont10', 'cont1', 'cont9', 'cont11', 'cat1', 'cat8_C', 'cont6', \n                      'cont12', 'cat5', 'cat3_C', 'cont4', 'cat8']\n\nX_train = X_train[important_features]\nX_test = X_test[important_features]","d147dd9c":"# Reporting util for different optimizers\ndef report_perf(optimizer, X, y, title=\"model\", callbacks=None):\n    \"\"\"\n    A wrapper for measuring time and performances of different optmizers\n    \n    optimizer = a sklearn or a skopt optimizer\n    X = the training set \n    y = our target\n    title = a string label for the experiment\n    \"\"\"\n    start = time()\n    \n    if callbacks is not None:\n        optimizer.fit(X, y, callback=callbacks)\n    else:\n        optimizer.fit(X, y)\n        \n    d=pd.DataFrame(optimizer.cv_results_)\n    best_score = optimizer.best_score_\n    best_score_std = d.iloc[optimizer.best_index_].std_test_score\n    best_params = optimizer.best_params_\n    \n    print((title + \" took %.2f seconds,  candidates checked: %d, best CV score: %.3f \"\n           + u\"\\u00B1\"+\" %.3f\") % (time() - start, \n                                   len(optimizer.cv_results_['params']),\n                                   best_score,\n                                   best_score_std))    \n    print('Best parameters:')\n    pprint.pprint(best_params)\n    print()\n    return best_params","b57b58a9":"# Setting the scoring function\nscoring = make_scorer(partial(mean_squared_error, squared=False), \n                      greater_is_better=False)","8bdbb9f0":"# Setting the validation strategy\nskf = StratifiedKFold(n_splits=7,\n                      shuffle=True, \n                      random_state=0)\n\ncv_strategy = list(skf.split(X_train, y_stratified))","6115d62b":"# Setting the basic regressor\nreg = XGBRegressor(random_state=0, booster='gbtree', objective='reg:squarederror', tree_method='gpu_hist')","4983688d":"# Setting the search space\nsearch_spaces = {'learning_rate': Real(0.01, 1.0, 'uniform'),\n                 'max_depth': Integer(2, 12),\n                 'subsample': Real(0.1, 1.0, 'uniform'),\n                 'colsample_bytree': Real(0.1, 1.0, 'uniform'), # subsample ratio of columns by tree\n                 'reg_lambda': Real(1e-9, 100., 'uniform'), # L2 regularization\n                 'reg_alpha': Real(1e-9, 100., 'uniform'), # L1 regularization\n                 'n_estimators': Integer(50, 5000)\n   }","0a12c3a5":"# Wrapping everything up into the Bayesian optimizer\nopt = BayesSearchCV(estimator=reg,                                    \n                    search_spaces=search_spaces,                      \n                    scoring=scoring,                                  \n                    cv=cv_strategy,                                           \n                    n_iter=120,                                       # max number of trials\n                    n_points=1,                                       # number of hyperparameter sets evaluated at the same time\n                    n_jobs=1,                                         # number of jobs\n                    iid=False,                                        # if not iid it optimizes on the cv score\n                    return_train_score=False,                         \n                    refit=False,                                      \n                    optimizer_kwargs={'base_estimator': 'GP'},        # optmizer parameters: we use Gaussian Process (GP)\n                    random_state=0)                                   # random state for replicability","5a3fc1de":"# Running the optimizer\noverdone_control = DeltaYStopper(delta=0.0001)                    # We stop if the gain of the optimization becomes too small\ntime_limit_control = DeadlineStopper(total_time=60*60*4)          # We impose a time limit (7 hours)\n\nbest_params = report_perf(opt, X_train, y_train,'XGBoost_regression', \n                          callbacks=[overdone_control, time_limit_control])","54dee343":"# Transferring the best parameters to our basic regressor\nreg = XGBRegressor(random_state=0, booster='gbtree', objective='reg:squarederror', tree_method='gpu_hist', **best_params)","b100c7e5":"# Cross-validation prediction\nfolds = 10\nskf = StratifiedKFold(n_splits=folds,\n                      shuffle=True, \n                      random_state=0)\n\npredictions = np.zeros(len(X_test))\nrmse = list()\n\nfor k, (train_idx, val_idx) in enumerate(skf.split(X_train, y_stratified)):\n    reg.fit(X_train.iloc[train_idx, :], y_train[train_idx])\n    val_preds = reg.predict(X_train.iloc[val_idx, :])\n    val_rmse = mean_squared_error(y_true=y_train[val_idx], y_pred=val_preds, squared=False)\n    print(f\"Fold {k} RMSE: {val_rmse:0.5f}\")\n    rmse.append(val_rmse)\n    predictions += reg.predict(X_test).ravel()\n    \npredictions \/= folds\nprint(f\"repeated CV RMSE: {np.mean(rmse):0.5f} (std={np.std(rmse):0.5f})\")","319e10d8":"# Preparing the submission\nsubmission = pd.DataFrame({'id':X_test.index, \n                           'target': predictions})\n\nsubmission.to_csv(\"submission.csv\", index = False)","ec61dcd5":"submission","b2473e72":"# Setting up optimization","a902b65a":"We set up a  7-fold cross validation","de7d013f":"We then define the evaluation metric, using the Scikit-learn function make_scorer allows us to convert the optimization into a minimization problem, as required by Scikit-optimize. We set squared=False by means of a partial function to obtain the root mean squared error (RMSE) as evaluation.","333b62d2":"In optimizing your model for this competition, you may wonder if there is a way to:\n\n * Leverage the information that you get as you explore the hyper-parameter space\n\n * Not necessarily become be an expert of a specific ML algorithm\n\n * Quickly find an optimization","ff7e9cfe":"The key idea behind Bayesian optimization is that we optimize a proxy function (the surrogate function) instead than the true objective function (what actually grid search and random search both do). This holds if testing the true objective function is costly (if it is not, then we simply go for random search.\n\nBayesian search balances exploration against exploitation. At start it randomly explores, doing so it builds up a surrogate function of the objective. Based on that surrogate function it exploits an initial approximate knowledge of how the predictor works in order to sample more useful examples and minimize the cost function at a global level, not a local one.\n\nBayesian Optimization uses an acquisition function to tell us how promising an observation will be. In fact, to rule the tradeoff between exploration and exploitation, the algorithm defines an acquisition function that provides a single measure of how useful it would be to try any given point.","9c4b4a1b":"**Machine learning algorithms have lots of knobs, and success often comes from twiddling them a lot.**\n\n*(DOMINGOS, Pedro. A few useful things to know about machine learning.\u00a0Communications of the ACM, 2012, 55.10: 78-87.)*","c955ace6":"As first steps:\n\nwe load the train and test data \nwe separate the target from the training data\nwe separate the ids from the data\nwe convert integer variables to categories (thus our machine learning algorithm can pick them as categorical variables and not standard numeric one)\n\nYou can add further processing, for instance by feature engineering, in order to succeed in this competition","23348f5c":"We set up a generic XGBoost regressor.","1e1a19fe":"First, we create a wrapper function to deal with running the optimizer and reporting back its best results.","527610c9":"![image.png](attachment:f8d4cbb2-09ef-4c55-a3f9-a93e6ddd8033.png)","4de80dd5":"In this step by ste tutorial, you will deal Bayesian optimization using XGBoost in few clear steps:\n\n1. Prepare your data, especially your categorical\n2. Define your cross-validation strategy\n3. Define your evaluation metric\n4. Define your base model\n5. Define your hyper-parameter search space\n6. Run optimization for a while","48f50d91":"Finally we runt the optimizer and wait for the results. We have set some limits to its operations: we required it to stop if it cannot get consistent improvements from the search (DeltaYStopper) and time dealine set in seconds (we decided for 6 hours).","72dd2a5e":"# 1. Data preparation","935578d2":"# Prediction on test data","90e9c84d":"# Kaggle\u2019s 30 days of machine learning:\n## Scikit-optimize for XGBoost (regression tutorial)\n\n![image.png](attachment:49860e47-cfb0-4ad8-8edd-9bbc7e6d1b0c.png)","eaec23f6":"The answer is:\n\n**Bayesian Optimization** (*SNOEK, Jasper; LAROCHELLE, Hugo; ADAMS, Ryan P. Practical bayesian optimization of machine learning algorithms. In: Advances in neural information processing systems. 2012. p. 2951-2959*)","8a45bcab":"We then define the Bayesian optimization engine, providing to it our XGBoost, the search spaces, the evaluation metric, the cross-validation. We set a large number of possible experiments and some parallelism in the search operations.","9aacbf34":"We define a search space, expliciting the key hyper-parameters to optimize and the range where to look for the best values.\n","4bc6b06a":"Having got the best hyperparameters for the data at hand, we instantiate a XGBoost using such values and train our model on all the available examples.\n\nAfter having trained the model, we predict on the test set and we save the results on a csv file."}}