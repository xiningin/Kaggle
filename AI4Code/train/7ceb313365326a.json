{"cell_type":{"fb41a580":"code","aa717b5f":"code","c3d809a2":"code","19ab0608":"code","b462ada6":"code","9e57fe19":"code","b7dda0f1":"code","4e0896ac":"code","7f642cd9":"code","33d3a529":"code","8e23fe8c":"code","cf4a74f4":"code","e895d8ca":"code","acfa9b8f":"code","fa045541":"code","5afa3400":"code","89325d21":"code","662c7aae":"code","92048507":"code","4af536b0":"code","c9114748":"code","d5eed2a7":"code","d1446015":"code","7190bb52":"code","f538874a":"code","9c3447ce":"code","b8fc6826":"code","a7fe58bf":"code","261c6721":"code","35ec49da":"code","052ec07d":"code","7ed6a0ac":"code","1ff8752f":"code","e7337062":"code","e205bb47":"code","c120d125":"code","637b9b31":"code","292191e5":"code","97de01ac":"code","d324cfa5":"code","a5c0d3bf":"code","055449b4":"code","9cd22fa1":"code","cb367947":"code","d1852f0b":"code","fd860019":"code","e96297a7":"code","f89307c0":"code","da9eac09":"markdown","c60f5333":"markdown","76d9e68e":"markdown","afc54a49":"markdown","9bee756d":"markdown","a9e6903f":"markdown","c5dc818d":"markdown","371dbfbe":"markdown","2675b552":"markdown","10287029":"markdown","391dc3f7":"markdown","2850a1c5":"markdown","f8603216":"markdown","7897d884":"markdown","cd5213cc":"markdown","b0fe34ce":"markdown","4aa8ad17":"markdown","6cbef00a":"markdown","00c8ad20":"markdown","9ff3e27a":"markdown","b8722454":"markdown","e4834ec1":"markdown","4f24e850":"markdown","219b528b":"markdown","0c411de7":"markdown","e708495d":"markdown","8c59ffbe":"markdown","7fa1bc08":"markdown","019491f4":"markdown"},"source":{"fb41a580":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","aa717b5f":"import math\nimport random\nimport pickle\nimport itertools\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, label_ranking_average_precision_score, label_ranking_loss, coverage_error \n\nfrom sklearn.utils import shuffle\n\nfrom scipy.signal import resample\n\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nimport pickle\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Conv1D, MaxPooling1D, Softmax, Add, Flatten, Activation# , Dropout\nfrom keras import backend as K\nfrom keras.optimizers import Adam\nfrom keras.callbacks import LearningRateScheduler, ModelCheckpoint\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nimport math\nimport random\nimport pickle\nimport itertools\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nnp.random.seed(42)\nimport tensorflow as tf\nimport tensorflow.keras as keras\n","c3d809a2":"print(os.getcwd())","19ab0608":"mit_test_data = pd.read_csv(\"..\/input\/heartbeat\/mitbih_test.csv\", header=None)\nmit_train_data = pd.read_csv(\"..\/input\/heartbeat\/mitbih_train.csv\", header=None)","b462ada6":"# There is a huge difference in the balanced of the classes.\n# Better choose the resample technique more than the class weights for the algorithms.\nfrom sklearn.utils import resample\n\ndf_1=mit_train_data[mit_train_data[187]==1]\ndf_2=mit_train_data[mit_train_data[187]==2]\ndf_3=mit_train_data[mit_train_data[187]==3]\ndf_4=mit_train_data[mit_train_data[187]==4]\ndf_0=(mit_train_data[mit_train_data[187]==0]).sample(n=20000,random_state=42)\n\ndf_1_upsample=resample(df_1,replace=True,n_samples=20000,random_state=123)\ndf_2_upsample=resample(df_2,replace=True,n_samples=20000,random_state=124)\ndf_3_upsample=resample(df_3,replace=True,n_samples=20000,random_state=125)\ndf_4_upsample=resample(df_4,replace=True,n_samples=20000,random_state=126)\n\ntrain_df=pd.concat([df_0,df_1_upsample,df_2_upsample,df_3_upsample,df_4_upsample])\n\n\ndf_11=mit_test_data[mit_train_data[187]==1]\ndf_22=mit_test_data[mit_train_data[187]==2]\ndf_33=mit_test_data[mit_train_data[187]==3]\ndf_44=mit_test_data[mit_train_data[187]==4]\ndf_00=(mit_test_data[mit_train_data[187]==0]).sample(n=20000,random_state=42)\n\ndf_11_upsample=resample(df_1,replace=True,n_samples=20000,random_state=123)\ndf_22_upsample=resample(df_2,replace=True,n_samples=20000,random_state=124)\ndf_33_upsample=resample(df_3,replace=True,n_samples=20000,random_state=125)\ndf_44_upsample=resample(df_4,replace=True,n_samples=20000,random_state=126)\n\ntest_df=pd.concat([df_00,df_11_upsample,df_22_upsample,df_33_upsample,df_44_upsample])\n\n\nequilibre=train_df[187].value_counts()\nprint(equilibre)","9e57fe19":"print(\"ALL Train data\")\nprint(\"Type\\tCount\")\nprint((mit_train_data[187]).value_counts())\nprint(\"-------------------------\")\nprint(\"ALL Test data\")\nprint(\"Type\\tCount\")\nprint((mit_test_data[187]).value_counts())\n\nprint(\"ALL Balanced Train data\")\nprint(\"Type\\tCount\")\nprint((train_df[187]).value_counts())\nprint(\"-------------------------\")\nprint(\"ALL Balanced Test data\")\nprint(\"Type\\tCount\")\nprint((train_df[187]).value_counts())","b7dda0f1":"#One hot encoding for categorical target\n#Since we will be using neural networks for our classification model, \n#our output classes need to be turned into a numerical representation. We use one hot encoding (from sklearn package) to do this.\n\n\n\n#train_target = mit_train_data[187]\n#train_target = train_target.values.reshape(87554,1)\ntrain_target = train_df[187]\ntrain_target = train_target.values.reshape(100000,1)\n\n\n\n\n#one hot encode train_target\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import preprocessing\n# TODO: create a OneHotEncoder object, and fit it to all of X\n\n# 1. INSTANTIATE\nenc = preprocessing.OneHotEncoder()\n\n# 2. FIT\nenc.fit(train_target)\n\n# 3. Transform\nonehotlabels = enc.transform(train_target).toarray()\nonehotlabels.shape\n\ntarget = onehotlabels","4e0896ac":"#remove ground truth labels from training df\n#train\/test split\n\n\nfrom sklearn.model_selection import train_test_split\n\n#X = mit_train_data\nX = train_df\nX = X.drop(axis=1,columns=187)\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(X,target, test_size = 0.25, random_state = 36)\nX_train = np.asarray(X_train)\nX_valid = np.asarray(X_valid)\nY_train = np.asarray(Y_train)\nY_valid = np.asarray(Y_valid)\n\n#X_train.reshape((1, 2403, 187))\nX_train = np.expand_dims(X_train, axis=2)\nX_valid = np.expand_dims(X_valid, axis=2)\nprint(X_train.shape)\nprint(Y_train.shape)\n# 2,403 training heartbeats and 802 validation heartbeats \n# for a 75:25 train-test split. ","7f642cd9":"# MODEL 1 https:\/\/www.kaggle.com\/freddycoder\/heartbeat-categorization\n# Separate features and targets\n\nfrom keras.utils import to_categorical\n\nprint(\"--- X ---\")\n# X = mit_train_data.loc[:, mit_train_data.columns != 187]\nX = train_df.loc[:, mit_train_data.columns != 187]\nprint(X.head())\nprint(X.info())\n\nprint(\"--- Y ---\")\n# y = mit_train_data.loc[:, mit_train_data.columns == 187]\ny = train_df.loc[:, mit_train_data.columns == 187]\ny = to_categorical(y)\n\nprint(\"--- testX ---\")\n#testX = mit_test_data.loc[:, mit_test_data.columns != 187]\ntestX = test_df.loc[:, mit_test_data.columns != 187]\nprint(testX.head())\nprint(testX.info())\n\nprint(\"--- testy ---\")\n#testy = mit_test_data.loc[:, mit_test_data.columns == 187]\ntesty = test_df.loc[:, mit_test_data.columns == 187]\ntesty = to_categorical(testy)","33d3a529":"# Keras model to make prediction\n\n#The number of epochs is a hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset.\n#The batch size is a hyperparameter that defines the number of samples to work through before updating the internal model parameters.\n# softmax is used to categorize \n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\n\nmodel = Sequential()\n\nmodel.add(Dense(50, activation='relu', input_shape=(187,)))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(5, activation='softmax'))\n\nmodel.compile(optimizer='Adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(X, y, epochs=10)\n\nprint(\"Evaluation: \")\nmse, acc = model.evaluate(testX, testy)\nprint('mean_squared_error :', mse)\nprint('accuracy:', acc)","8e23fe8c":"test= pd.read_csv(\"..\/input\/arduino3a\/AS3.txt\", header=None)\ntest\n","cf4a74f4":"plt.plot(test.iloc[100,:])","e895d8ca":"# NORMALIZING TEST DATA AMPLITUDE\nfrom sklearn.preprocessing import MinMaxScaler\n# load the dataset and print the first 5 rows\n# prepare data for normalization\nvalues = test.values\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler = scaler.fit(values)\nnormalized = scaler.transform(values)\n\ndfnormalized = pd.DataFrame(normalized)\ndfnormalized.index = [x for x in range(1, len(dfnormalized.values)+1)]\nplt.plot(dfnormalized.iloc[30,:])","acfa9b8f":"# category= model.predict_classes(test) #not Normalized\ncategory= model.predict_classes(dfnormalized) #Normalized\nplt.plot(category)","fa045541":"np.mean(category)","5afa3400":"test = pd.read_csv(\"..\/input\/arduinorow1a\/test44.txt\", header=None)\n#test = pd.read_csv(\"..\/input\/arduinorow2a\/marnelakis.txt\", header=None)\n#test = test.iloc[0,0:len(test.T)-1] # Remove last line cause it might be a Nan\ntest = pd.DataFrame(test)\ntest=test.T\n","89325d21":"# NORMALIZING TEST DATA AMPLITUDE\nfrom sklearn.preprocessing import MinMaxScaler\n# load the dataset and print the first 5 rows\n# prepare data for normalization\nvalues = test.values\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler = scaler.fit(values)\nnormalized = scaler.transform(values)\nnormalized = pd.DataFrame(normalized) \nnormalized","662c7aae":"## TESTING FOR ONE X\n#x=0\n#normalized = pd.DataFrame(normalized.T) ## CAUTION!!! needs to run only once \n#normtest=normalized.iloc[0, 0+x:187+x] \n#normtest=pd.DataFrame(normtest)\n#category = model.predict_classes(normtest.T)\n#category","92048507":"category= pd.DataFrame()\ncategory=category.dropna()\nlst_seq = np.arange(0,len(normalized.T)-190)\nfor x in lst_seq:\n    normtest=normalized.iloc[0, 0+x:187+x] \n    normtest=pd.DataFrame(normtest)\n    category[x] = model.predict_classes(normtest)\ncategory","4af536b0":"category","c9114748":"np.mean(category.T)","d5eed2a7":"plt.plot(category.T)","d1446015":"category = pd.DataFrame(category)\ntemp1= category.iloc[0,:].value_counts()\nprint(\"Categories vs Value Count\")\nprint(temp1)\nprint(\"Categories vs Frequency\")\nprint(temp1\/(len(category.T)))","7190bb52":"target_train=train_df[187]\ntarget_test=test_df[187]\ny_train=to_categorical(target_train)\ny_test=to_categorical(target_test)","f538874a":"X_train=train_df.iloc[:,:186].values\nX_test=test_df.iloc[:,:186].values\n#for i in range(len(X_train)):\n#    X_train[i,:186]= add_gaussian_noise(X_train[i,:186])\nX_train = X_train.reshape(len(X_train), X_train.shape[1],1)\nX_test = X_test.reshape(len(X_test), X_test.shape[1],1)","9c3447ce":"def network(X_train,y_train,X_test,y_test):\n    \n\n    im_shape=(X_train.shape[1],1)\n    inputs_cnn=Input(shape=(im_shape), name='inputs_cnn')\n    conv1_1=Convolution1D(64, (6), activation='relu', input_shape=im_shape)(inputs_cnn)\n    conv1_1=BatchNormalization()(conv1_1)\n    pool1=MaxPool1D(pool_size=(3), strides=(2), padding=\"same\")(conv1_1)\n    conv2_1=Convolution1D(64, (3), activation='relu', input_shape=im_shape)(pool1)\n    conv2_1=BatchNormalization()(conv2_1)\n    pool2=MaxPool1D(pool_size=(2), strides=(2), padding=\"same\")(conv2_1)\n    conv3_1=Convolution1D(64, (3), activation='relu', input_shape=im_shape)(pool2)\n    conv3_1=BatchNormalization()(conv3_1)\n    pool3=MaxPool1D(pool_size=(2), strides=(2), padding=\"same\")(conv3_1)\n    flatten=Flatten()(pool3)\n    dense_end1 = Dense(64, activation='relu')(flatten)\n    dense_end2 = Dense(32, activation='relu')(dense_end1)\n    main_output = Dense(5, activation='softmax', name='main_output')(dense_end2)\n    \n    \n    model = Model(inputs= inputs_cnn, outputs=main_output)\n    model.compile(optimizer='adam', loss='categorical_crossentropy',metrics = ['accuracy'])\n    \n    \n    callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n\n    history=model.fit(X_train, y_train,epochs=30,callbacks=callbacks, batch_size=32,validation_data=(X_test,y_test))\n    model.load_weights('best_model.h5')\n    return(model,history)","b8fc6826":"def evaluate_model(history,X_test,y_test,model):\n    scores = model.evaluate((X_test),y_test, verbose=0)\n    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n    \n    print(history)\n    fig1, ax_acc = plt.subplots()\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Model - Accuracy')\n    plt.legend(['Training', 'Validation'], loc='lower right')\n    plt.show()\n    \n    fig2, ax_loss = plt.subplots()\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Model- Loss')\n    plt.legend(['Training', 'Validation'], loc='upper right')\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.show()\n    target_names=['0','1','2','3','4']\n    \n    y_true=[]\n    for element in y_test:\n        y_true.append(np.argmax(element))\n    prediction_proba=model.predict(X_test)\n    prediction=np.argmax(prediction_proba,axis=1)\n    cnf_matrix = confusion_matrix(y_true, prediction)","a7fe58bf":"from keras.layers import Dense, Convolution1D, MaxPool1D, Flatten, Dropout\nfrom keras.layers import Input\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization\nimport keras\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nmodel,history=network(X_train,y_train,X_test,y_test)","261c6721":"evaluate_model(history,X_test,y_test,model)\ny_pred=model.predict(X_test)","35ec49da":"df1 = pd.DataFrame()\ncategory= pd.DataFrame()\ncategory=category.dropna()\nlst_seq = np.arange(0,len(normalized.T)-190)\nfor x in lst_seq:\n    temp=normalized.iloc[0,0+x:186+x]\n    temp=pd.DataFrame(temp) \n    temp=temp.values\n    temp=temp.reshape(1,186,1)\n    category=pd.DataFrame(model.predict(temp))\n    df = pd.DataFrame(category)\n    df1=df1.append(df)\n    \ncategory=df1","052ec07d":"category=pd.DataFrame(category)\ncategory","7ed6a0ac":"cat1=category[0].mean()\ncat2=category[1].mean()\ncat3=category[2].mean()\ncat4=category[3].mean()\ncat5=category[4].mean()","1ff8752f":"cat1\n","e7337062":"cat2","e205bb47":"cat3","c120d125":"cat4","637b9b31":"cat5","292191e5":"test = pd.read_csv(\"..\/input\/arduinorow1a\/test44.txt\", header=None)\ntest = test.iloc[0,0:len(test.T)-1] # Remove last line cause it might be a Nan\ntest = pd.DataFrame(test)\n# NORMALIZING TEST DATA AMPLITUDE\nfrom sklearn.preprocessing import MinMaxScaler\n# load the dataset and print the first 5 rows\n# prepare data for normalization\nvalues = test.values\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler = scaler.fit(values)\nnormalized = scaler.transform(values)\nnormalized = pd.DataFrame(normalized)\nnormalized","97de01ac":"from keras.datasets import imdb\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Bidirectional\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\n# fix random seed for reproducibility\nnp.random.seed(7)","d324cfa5":"# MODEL 1 https:\/\/www.kaggle.com\/freddycoder\/heartbeat-categorization\n# Separate features and targets\n\nfrom keras.utils import to_categorical\n\nprint(\"--- X ---\")\n# X = mit_train_data.loc[:, mit_train_data.columns != 187]\nX = train_df.loc[:, mit_train_data.columns != 187]\nprint(X.head())\nprint(X.info())\n\nprint(\"--- Y ---\")\n# y = mit_train_data.loc[:, mit_train_data.columns == 187]\ny = train_df.loc[:, mit_train_data.columns == 187]\ny = to_categorical(y)\n\nprint(\"--- testX ---\")\n#testX = mit_test_data.loc[:, mit_test_data.columns != 187]\ntestX = test_df.loc[:, mit_test_data.columns != 187]\nprint(testX.head())\nprint(testX.info())\n\nprint(\"--- testy ---\")\n#testy = mit_test_data.loc[:, mit_test_data.columns == 187]\ntesty = test_df.loc[:, mit_test_data.columns == 187]\ntesty = to_categorical(testy)","a5c0d3bf":"# create the model.\nfrom keras.callbacks import History \nhistory = History()\nembedding_vecor_length = 187\nmodel = Sequential()\n#model = Bidirectional(model)\n\nmodel.add(Embedding(100000, embedding_vecor_length, input_length=187))\nmodel.add(LSTM(187))\nmodel.add(Dense(5, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\nhistory = model.fit(X, y, validation_data=(testX, testy), epochs=3, batch_size=8)\n\n\n#Dropout is a powerful technique for combating overfitting in your LSTM models \n#model = Sequential()\n#model.add(Embedding(1000, embedding_vecor_length, input_length=187))\n#model.add(LSTM(50, dropout=0.001, recurrent_dropout=0.001))\n#model.add(Dense(5, activation='softmax'))\n#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n#print(model.summary())\n#history = model.fit(X, y, validation_data=(testX, testy), epochs=50, batch_size=128)\n\n\n\n## SAVE MODEL ##\n# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"1model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"1model.h5\")\nprint(\"Saved model to disk\")","055449b4":"# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n# The history for the validation dataset is labeled test by convention as it is indeed a test dataset for the model.\n#The plots can provide an indication of useful things about the training of the model, such as:\n#*It\u2019s speed of convergence over epochs (slope).\n#*Whether the model may have already converged (plateau of the line).\n#*Whether the mode may be over-learning the training data (inflection for validation line)","9cd22fa1":"y_pred = model.predict(testX, batch_size=1000)\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, label_ranking_average_precision_score, label_ranking_loss, coverage_error \n\nprint(classification_report(testy.argmax(axis=1), y_pred.argmax(axis=1)))","cb367947":"category= pd.DataFrame()\ncategory=category.dropna()\nlst_seq = np.arange(0,len(normalized.T)-190)\nfor x in lst_seq:\n    normtest=normalized.iloc[0, 0+x:187+x] \n    normtest=pd.DataFrame(normtest)\n    category[x] = model.predict_classes(normtest.T)\ncategory","d1852f0b":"np.mean(category.T)","fd860019":"plt.plot(category.T)","e96297a7":"category = pd.DataFrame(category)\ntemp1= category.iloc[0,:].value_counts()\nprint(\"Categories vs Value Count\")\nprint(temp1)\nprint(\"Categories vs Frequency\")\nprint(temp1\/(len(category.T)))","f89307c0":"json_file = open(\"..\/working\/model.json\", 'r')\nmodel_json = json_file.read() \njson_file.close()\n\nfrom keras.models import model_from_json\nmodel = model_from_json(model_json)\nmodel.load_weights(\"..\/working\/model.h5\")\n\n#model.compile(loss='binary_crossentropy', optimizer='adam')\n#prediction = model.predict(x_test, batch_size=2048)[0].flatten()","da9eac09":"# ONE HOT Encoding *","c60f5333":"mse, acc = model.evaluate(testX, testy)\nprint('mean_squared_error :', mse)\nprint('accuracy:', acc)","76d9e68e":"## Evaluate Model","afc54a49":"# PRODUCE BALANCED DATASET train_df , test_df *","9bee756d":"# 1 MODEL NN","a9e6903f":"## 2. NEW MODEL CNN","c5dc818d":"## https:\/\/machinelearningmastery.com\/sequence-classification-lstm-recurrent-neural-networks-python-keras\/\n## https:\/\/www.hindawi.com\/journals\/jhe\/2019\/6320651\/\n## https:\/\/www.mathworks.com\/help\/signal\/examples\/classify-ecg-signals-using-long-short-term-memory-networks.html\n##","371dbfbe":"## Normalize samples","2675b552":"# USE IF SAMPLES ARE IN A ROW","10287029":"## \u0391ccuracy and prediction scores","391dc3f7":"# ARDUINO SAMPLES","2850a1c5":"## Predicing Category","f8603216":"## Mean of Category","7897d884":"# 3.1 USE IF SAMPLES ARE IN A ROW","cd5213cc":"## PLOT OF CATEGORIES","b0fe34ce":"## Predict category of Arduino sample","4aa8ad17":"## MEAN OF CATEGORIES","6cbef00a":"## Display frequency of each predicted category as evaluated by model","00c8ad20":"# 3. MODEL RNN LSTM GRU","9ff3e27a":"## MEAN OF CATEGORIES","b8722454":"## Normalizing Arduino Samples","e4834ec1":"## https:\/\/www.kaggle.com\/gregoiredc\/arrhythmia-on-ecg-classification-using-cnn","4f24e850":"1.  # DATA ACQUISITION *","219b528b":"## Mean of each Catecory","0c411de7":"## PLOT OF CATEGORIES","e708495d":"# LOAD MODEL ","8c59ffbe":"# MODEL LSTM RNN","7fa1bc08":"# USE IF SAMPLES ARE IN A MATRIX FORM","019491f4":"## Display frequency of each predicted category as evaluated by model"}}