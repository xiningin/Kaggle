{"cell_type":{"daa54b36":"code","6c89f5d6":"code","6371df43":"code","f9ab94c7":"code","0a389eb3":"code","f1c99cd7":"code","3a8c1b9d":"code","321f6b8b":"code","4126517c":"code","0c456cd9":"code","90a53ae1":"code","acaebe43":"code","4361000b":"code","20def1b2":"code","101d1a9a":"code","d1c1925e":"code","cd9f7154":"code","a42f2862":"code","18f8976d":"code","3560d921":"code","046b484d":"code","3712b3be":"code","e6404c47":"code","7bd1a5e5":"code","4a345d40":"code","052794be":"markdown","d40bd936":"markdown","cb476ac5":"markdown","9b945495":"markdown","c6ac5c5b":"markdown","55493d99":"markdown","704bfab7":"markdown","fae56c60":"markdown","c0128754":"markdown","52cca64e":"markdown","ff6018a9":"markdown","18e36670":"markdown","dd59b6e1":"markdown","c6b4838d":"markdown","c8b84d2e":"markdown","965cbdca":"markdown","fdb202ee":"markdown","7b238eca":"markdown","214c861b":"markdown","ae7489e0":"markdown","6cda08f9":"markdown","a9e220ee":"markdown","b2764f00":"markdown","9442f3ff":"markdown","450e6008":"markdown"},"source":{"daa54b36":"import re\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom IPython import display\nimport PIL\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n    \nprint(tf.__version__)","6c89f5d6":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nGCS_PATH = KaggleDatasets().get_gcs_path()\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nIMAGE_SIZE = [112, 112]","6371df43":"filenames = tf.io.gfile.glob(str(GCS_PATH + '\/dataset\/train\/romanticism\/*'))\nfilenames.extend(tf.io.gfile.glob(str(GCS_PATH + '\/dataset\/test\/romanticism\/*')))","f9ab94c7":"IMG_COUNT = len(filenames)\nprint(\"Image count for training: \" + str(IMG_COUNT))","0a389eb3":"train_ds = tf.data.Dataset.from_tensor_slices(filenames)\n\nfor f in train_ds.take(5):\n    print(f.numpy())","f1c99cd7":"# normalizing the images to [-1, 1]\ndef normalize(image):\n  image = tf.cast(image, tf.float32)\n  image = (image \/ 127.5) - 1\n  return image","3a8c1b9d":"def decode_img(img):\n  # convert the compressed string to a 3D uint8 tensor\n  img = tf.image.decode_jpeg(img, channels=3)\n  # Use `convert_image_dtype` to convert to floats in the [-1,1] range.\n  img = normalize(img)\n  # resize the image to the desired size.\n  return tf.image.resize(img, IMAGE_SIZE)\n\ndef process_path(file_path):\n    img = tf.io.read_file(file_path)\n    img = decode_img(img)\n    # convert the image to grayscale\n    return tf.expand_dims(img[:, :, 0], axis=2)","321f6b8b":"train_ds = train_ds.map(process_path, num_parallel_calls=AUTOTUNE).cache().batch(256)","4126517c":"image_batch = next(iter(train_ds))\nimage_batch.shape","0c456cd9":"def show_batch(image_batch):\n    plt.figure(figsize=(10,10))\n    for n in range(25):\n        ax = plt.subplot(5,5,n+1)\n        plt.imshow(image_batch[n, :, :, 0], cmap='gray')\n        plt.axis(\"off\")","90a53ae1":"show_batch(image_batch)","acaebe43":"EPOCHS = 25000\nnoise_dim = 100\nnum_examples_to_generate = 16\n\n# We will reuse this seed overtime (so it's easier)\n# to visualize progress in the animated GIF)\nseed = tf.random.normal([num_examples_to_generate, noise_dim])","4361000b":"def make_generator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(7*7*1024, use_bias=False, input_shape=(noise_dim,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Reshape((7, 7, 1024)))\n    assert model.output_shape == (None, 7, 7, 1024) # Note: None is the batch size\n\n    model.add(layers.Conv2DTranspose(512, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n    assert model.output_shape == (None, 7, 7, 512)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    assert model.output_shape == (None, 14, 14, 256)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    assert model.output_shape == (None, 28, 28, 128)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    assert model.output_shape == (None, 56, 56, 64)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n    assert model.output_shape == (None, 112, 112, 1)\n\n    return model","20def1b2":"generator = make_generator_model()\n\nnoise = tf.random.normal([1, noise_dim])\ngenerated_image = generator(noise, training=False)\n\nplt.imshow(generated_image[0, :, :, 0], cmap='gray')","101d1a9a":"def make_discriminator_model():\n    model = tf.keras.Sequential()\n    \n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n                                     input_shape=[112, 112, 1]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Conv2D(512, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n\n    return model","d1c1925e":"discriminator = make_discriminator_model()\ndecision = discriminator(generated_image)\nprint (decision)","cd9f7154":"# helper function\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)","a42f2862":"def discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss","18f8976d":"def generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)","3560d921":"generator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)","046b484d":"checkpoint_dir = '.\/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)","3712b3be":"# Notice the use of `tf.function`\n# This annotation causes the function to be \"compiled\".\n@tf.function\ndef train_step(images):\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n      generated_images = generator(noise, training=True)\n\n      real_output = discriminator(images, training=True)\n      fake_output = discriminator(generated_images, training=True)\n\n      gen_loss = generator_loss(fake_output)\n      disc_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))","e6404c47":"def train(dataset, epochs):\n  for epoch in range(epochs):\n    start = time.time()\n\n    for image_batch in dataset:\n      train_step(image_batch)\n\n    # Produce images for the GIF as we go\n    display.clear_output(wait=True)\n    generate_and_save_images(generator,\n                             epoch + 1,\n                             seed)\n\n    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n\n  # Generate after the final epoch\n  display.clear_output(wait=True)\n  generate_and_save_images(generator,\n                           epochs,\n                           seed)","7bd1a5e5":"def generate_and_save_images(model, epoch, test_input):\n  # Notice `training` is set to False.\n  # This is so all layers run in inference mode (batchnorm).\n  predictions = model(test_input, training=False)\n\n  fig = plt.figure(figsize=(4,4))\n\n  for i in range(predictions.shape[0]):\n      plt.subplot(4, 4, i+1)\n      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n      plt.axis('off')\n\n  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n  plt.show()","4a345d40":"train(train_ds, EPOCHS)","052794be":"We'll define some variables now so our code will be easier to read.","d40bd936":"## What is a DCGAN?\n\nDCGAN is short for deep convolutional generative adversarial networks. GANs are a pretty recent development in machine learning and computer vision. It works by training two different models in an adversarial, or a competitive, process. The generator is the model that, as the name suggests, generates or creates images, and the discriminator is the model that determines which images are real and fake (generated).\n\nThese models are in opposition because the generator is trying to trick the discriminator and the discriminator doesn't want to be tricked. The models continue training until the point when the discriminator cannot distinguish between real and generated images.","cb476ac5":"# Preprocess data\n\nImages are originally in the form of 3-stacked matrices, RGB, with values from 0 to 255. GANs generally expect smaller numbers. Therefore, we have to normalize our images so that values will fall between -1 and 1.\n\nLet's define the normalizing function.","9b945495":"We are building a DCGAN, a deep convolutional GAN. As the name suggests, we have many convolution layers in our model. Our generative model starts with taking in some noise. The noise is our seed.\n\nThe seed is fed through a Dense layer and Reshaped into a 2D array. This 2D is upsampled using Conv2DTranspose layers until we reach a generated image that is 112 x 112. ","c6ac5c5b":"Since we only have 320 images, we don't have to batch the images. Generally, batching is a good idea so we will batch this dataset as well.\n\nIn this batch, we have 256 images of the size 112 x 112.","55493d99":"# Load romantic paintings\n\nIn order to create romantic-style art, we first have to load in existing romantic-style art to train our model to understand what the romantic-style is. In this Kaggle dataset, there is also cubist art and expressionist art. Try loading in different styles of art to generate art of different styles.\n\nGANs typically don't need a separate training and validation dataset, unlike image classifiers. Therefore, we are going to combine the training dataset and the testing dataset to create a larger training dataset.","704bfab7":"## Define optimizers\n\nSince we have two different models, we need to instantiate two different optimizers.","fae56c60":"# Generated art\n\nThis is our generated art. Running it again will result in a new generation. We can see that our model did pick up some key features of romantic art. For example, from our initial image visualization, we see that many romantic paintings are portraits. Thus, we can see that many of our generated images resemble portraits.","c0128754":"It seems that we have 320 images available for training.","52cca64e":"# Build our discriminator model\n\nOur discriminator model will be used to train our generative model. The discriminator mirrors the generator. Instead of building up a 1D tensor to a 2D image, the discriminator reduces a 2D image into a single node. The architecture is basically a CNN image classifier.\n\nThe purpose of the discriminator is to classify images as real or fake. A fake image is what we would consider to be a generated image.\n\nWe will first train our discriminator so that it will classify real vs. fake images. Then, we will use the discriminator to train our generator to produce higher quality images that will trick the discriminator into thinking they are real.","ff6018a9":"# Training the model","18e36670":"We're also going to simplify our images so that our input images will be grayscale instead of colored. Grayscale images are simpler than colored images since grayscale images are only 2-dimensional instead of 3-dimensional. GANs are rather complicated and require a lot from our model. Simplifying the image to a 2D shape will help our model pinpoint the more important style characteristics and will not have to exert energy figuring out how the different color channels related to one another.\n\nLet's define the functions to decode our images into a tensor that can be read by the model and convert it to grayscale.","dd59b6e1":"Let's first install the necessary packages.","c6b4838d":"## Define the training loop\n\nBecause we are using a model to train a model, we cannot use `Model.fit()`. Instead, we will create a custom training loop that trains the generator based on the output of the discriminator.\n\nEach training loop started with the generator receiving some random noise and outputing a generated image. The discriminator classifies real images (from the training dataset) and the generated images. The losses, as defined above, are calculated for the models, and the gradient updates the generator and discriminator based on the values from these losses.","c8b84d2e":"We can also create an output directory that saves checkpoints, for example, after every 15 epochs. We won't be actually saving checkpoints in this kernel because of limited disk space, but if that is not an issue, it is generally good practice to save checkpoints.","965cbdca":"Format our dataset by processing filenames into actual images.","fdb202ee":"Right now, our generator model is not trained. Therefore, our generated image is all noise.","7b238eca":"# Introduction + Set-up\n\nComputer vision models can be used for more than just classification. In this tutorial, we will go over generative adversarial networks (GANs) and learn how to use DCGANs to create art from scratch in the style of the art of the Romantic Period. We can easily build our model using TensorFlow and Keras.","214c861b":"The loss for the generator is calculated by comparing the predictions on generated images with an arrays of 1s instead of an array of 0s. We want to optimize our model so that it can trick the discriminator.","ae7489e0":"# Build our generator model\n\nThe first aspect of a GAN is a generator model. This part of the model is the part that actually does the generating. Let's delve into the architecture of the this model.\n\nLet's first define some constants. We want to run this model for 30000 epochs and have the model generate 16 images at the end of each epoch. Our seed (noise) we define to be of 100 dimensions.","6cda08f9":"## Defining loss functions\n\nLoss functions are used to train your model. It is important to define a good loss function because this is the function that gets minimized by the optimizer when your model is being trained.","a9e220ee":"Right now, our discriminator model is not trained. But once it is trained, it will output positive values for real images and negative values for fake or images.","b2764f00":"# Visualize romantic art\n\nBefore we start generating romantic art, we first have to ask ourselves: how does romantic art look like?","9442f3ff":"At the end of each epoch, we will show the images generated. That way, we can see the progression of the model after each epoch.","450e6008":"The loss for the discriminator is calculated by comparing the discriminator's predictions on real images with an array of 1s and comparing the predictions on generated images with an array of 0s. This works because the discriminator outputs a positive value for \"real\" predictions and a negative value for \"fake\" predictions."}}