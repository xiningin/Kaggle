{"cell_type":{"e11b0f30":"code","39965354":"code","d9a47588":"code","5a8befaf":"code","a2b5b4af":"code","b5a82d7c":"code","b3b17b44":"code","8db683e9":"code","9b0f765c":"code","2b73682b":"code","78aa5ab1":"code","5537be5d":"code","9245d309":"code","63b8516d":"code","03a87f72":"code","0b3ef685":"code","2838d894":"code","0f557422":"code","75ea162d":"code","678ba539":"code","5ed2e86c":"code","0bf935b4":"code","ef8c7807":"code","22f1abe3":"code","4749bb59":"code","e331386d":"markdown","fc705af6":"markdown","ffa320b0":"markdown","755d2156":"markdown","39e8b04b":"markdown","34800592":"markdown","0280f0b7":"markdown","a7edb8ed":"markdown","30bc7854":"markdown","532a2ba7":"markdown","d0866b35":"markdown","2cea7081":"markdown","244ff62e":"markdown","1088507c":"markdown","bf9e8283":"markdown"},"source":{"e11b0f30":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', 500)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","39965354":"data = pd.read_csv('\/kaggle\/input\/covid19-in-limeiraspbrazil\/covid-limeira-daily.csv')\ndata['Date'] = pd.to_datetime(data['Date'])\ndata.head(10)","d9a47588":"print(data['Date'].diff().value_counts())","5a8befaf":"# keep sequential dates only\ndata = data[(data['Date'] - data['Date'].shift(5)) == pd.Timedelta(5, 'D')]\nprint(len(data))\n#data.head(10)","a2b5b4af":"data['y'] = data['NewCases'].shift(-1)\n\ndef train_test_split(data):\n    train = data.iloc[:-15,:]\n    train.reset_index(drop=True, inplace=True)\n    test = data.tail(15).copy()\n    test.reset_index(drop=True, inplace=True)    \n    return train, test\n\ntrain, test = train_test_split(data)\ntrain, valid = train_test_split(train)","b5a82d7c":"def sep_feature_target(data):\n    y = data['y']\n    x = data.drop('y', axis=1)\n    return x, y\n\nx_train, y_train = sep_feature_target(train)\nx_valid, y_valid = sep_feature_target(valid)\nx_test, y_test   = sep_feature_target(test)","b3b17b44":"# drop last row of test_set because there is no target for it\nx_test.drop(x_test.tail(1).index,inplace=True)\ny_test.drop(y_test.tail(1).index,inplace=True)\ny_test","8db683e9":"print('Checking sizes of train\/dev\/test sets:')\nprint(len(x_train), len(y_train))\nprint(len(x_valid), len(y_valid))\nprint(len(x_test), len(y_test))","9b0f765c":"from sklearn.metrics import mean_squared_log_error\n\ndef calc_error(valid, predict):\n    return np.sqrt(mean_squared_log_error( valid, predict ))\n\nbaseline_valid = x_valid['NewCases'].shift(1)\nprint(\"baseline shift1 =\", calc_error( y_valid[y_valid.notnull()][1:], baseline_valid[baseline_valid.notnull()] ) * 100.0)\n\nbaseline_valid = x_valid['NewCases'].shift(1).rolling(5).mean()\nprint(\"baseline rollingavg5 =\", calc_error( y_valid[y_valid.notnull()][5:], baseline_valid[baseline_valid.notnull()] ) * 100.0)","2b73682b":"import datetime as dt\n\ndef feature_generation(data_features):\n    data_features['weekday'] = data_features['Date'].dt.weekday\n    data_features['weekofyear'] = data_features['Date'].dt.isocalendar().week\n    data_features['weekofyear'] = data_features['weekofyear'].astype('int32')\n    data_features['dayofyear'] = data_features['Date'].dt.dayofyear\n    data_features['diff_1'] = data_features['NewCases'].diff()\n    data_features['diff_2'] = data_features['NewCases'].shift(1).diff()\n    data_features['diff_3'] = data_features['NewCases'].shift(2).diff()\n    data_features['ra3'] = data_features['NewCases'].rolling(3).mean()#.reset_index(level=0, drop=True)\n    data_features['ra7'] = data_features['NewCases'].rolling(7).mean()#.reset_index(level=0, drop=True)\n    data_features['ra10'] = data_features['NewCases'].rolling(10).mean()#.reset_index(level=0, drop=True)\n    data_features['ra14'] = data_features['NewCases'].rolling(14).mean()#.reset_index(level=0, drop=True)\n    data_features = data_features.fillna(0)\n    #data_features['Date'] = data_features['Date'].map(dt.datetime.toordinal)\n    data_features['Date'] = data_features['Date'].values.astype(float)\n    return data_features","78aa5ab1":"from sklearn.ensemble import RandomForestRegressor\n\nmodel1 = RandomForestRegressor(n_jobs=-1, random_state=11, min_samples_leaf=2)\nmodel1.fit(feature_generation(x_train), y_train)\n\nt = model1.predict(feature_generation(x_train))\nv = model1.predict(feature_generation(x_valid))\n\nprint('Checking the model1 errors:')\nprint(\"model1 (train set error)=\", calc_error(y_train, t))\nprint(\"model1 (dev set error)=\", calc_error(y_valid, v))","5537be5d":"import matplotlib.pyplot as plt\n\ndef plot_results(valid,predict):\n    results = pd.DataFrame()\n    results['target'] = valid\n    results['predict'] = predict\n    results.plot(figsize=(10,5))","9245d309":"plot_results(y_train, t)\nplt.show()","63b8516d":"plot_results(y_valid, v)","03a87f72":"import lightgbm as lgb\n\nmodel2 = lgb.LGBMRegressor(n_jobs=-1, random_state=11, learning_rate=.005, n_estimators=700);\nmodel2.fit(feature_generation(x_train), y_train)\n\nt = model2.predict(feature_generation(x_train))\nv2 = model2.predict(feature_generation(x_valid))\n\nprint('Checking the model2 errors:')\nprint(\"model2 (train set error)=\", calc_error(y_train, t))\nprint(\"model2 (dev set error)=\", calc_error(y_valid, v2))","0b3ef685":"plot_results(y_train, t)","2838d894":"plot_results(y_valid, v2)","0f557422":"def predict_ensemble(features, w=0.7):\n    s1 = model1.predict(feature_generation(features))\n    s2 = model2.predict(feature_generation(features))    \n    return s1*(1. - w) + s2*w","75ea162d":"ensemble = predict_ensemble(x_valid)\nplot_results(y_valid, ensemble)\ncalc_error(y_valid, ensemble)","678ba539":"ensemble = predict_ensemble(x_test)\nplot_results(y_test, ensemble)\ncalc_error(y_test, ensemble)\n\n# 0.40791120181955176 | 0.1 , 0.7","5ed2e86c":"def predict_last_rec():\n    last_rec = data.tail(1).copy()\n    last_rec.drop('y', axis=1, inplace=True)\n    next_day = (data.tail(1)['Date'].iloc[0] + dt.timedelta(1))\n    print(\"Predicted new confirmed cases for\", next_day.strftime('%m\/%d\/%Y') , \":\", int(predict_ensemble(last_rec)))    ","0bf935b4":"predict_last_rec()","ef8c7807":"last_rec = data.tail(1).copy()\nlast_rec.drop('y', axis=1, inplace=True)    \n\ndata_estimated = data.copy()\n\n#preds = []\n#next_dates = []\n#preds.columns = data.columns\n\nfor _ in range(1,14):\n    next_day = (last_rec['Date'].iloc[0] + dt.timedelta(1))\n    #next_dates.append(next_day)\n    next_newcases = int(predict_ensemble(last_rec))    \n    \n    print(\"Predicted new confirmed cases for\", next_day.strftime('%m\/%d\/%Y') , \":\", int(predict_ensemble(last_rec)))    \n    \n    last_rec['Date'] = next_day\n    last_rec['NewCases'] = next_newcases\n    last_rec['Confirmed'] = last_rec['Confirmed'] + next_newcases\n    last_rec['Notifications'] = last_rec['Notifications'] + next_newcases\n    \n    # estimate other fields\n    newdeaths = int(data['NewDeaths'].tail(7).mean())\n    last_rec['NewDeaths'] = newdeaths\n    \n    newurc = data['URCOccupancy'].tail(7).mean()\n    last_rec['URCOccupancy'] = newurc\n    \n    data_estimated = data_estimated.append(last_rec)\n    \n    #preds.append(next_newcases)","22f1abe3":"data_estimated.tail(15)","4749bb59":"last_dataset_date = data.tail(1).iloc[0, 0]\n\nax = data_estimated[data_estimated['Date'] <= last_dataset_date].plot(x='Date',\n                                                                            y='NewCases',\n                                                                            figsize=(15,10),\n                                                                            ylabel='Confirmed Cases',\n                                                                            title='COVID Confirmed Cases - Actual vs Predictions')\ndata_estimated[data_estimated['Date'] >= last_dataset_date].plot(x='Date', y='NewCases', ax=ax)\nax.legend(['Actual', 'Predictions'])\nplt.show()","e331386d":"The baseline model is a simple model created to get a simple and gross error value, that must be beated for the more complex models that will be built next.\n\nTwo baseline models will be used:\n* baseline shift1: this model considers that the new cases will be the same of the day before.\n* baseline rollingavg5: this model considers that new cases will be the rolling average of the last 5 days.","fc705af6":"## Time Series References\n\nThe references bellow was used to build this model:\n\n* [S\u00e9ries temporais \u2014 defini\u00e7\u00f5es e caracter\u00edsticas](https:\/\/medium.com\/data-sprints\/s%C3%A9ries-temporais-defini%C3%A7%C3%B5es-e-caracter%C3%ADsticas-698d85f4b353)\n* [Time Series #1 - Como Criar o Ambiente de Desenvolvimento para Data Science](https:\/\/www.youtube.com\/watch?v=lYLGaLEvWto)\n* [Time Series #2 - TUDO o Que Voc\u00ea Precisa Saber Para Criar Seu Modelo de Machine Learning](https:\/\/www.youtube.com\/watch?v=8UTNg4bzWlE)\n* [Time Series #3 - 5 Dicas para Melhorar Seus Modelos de Machine Learning](https:\/\/www.youtube.com\/watch?v=_xa1Yx6ZQo4)","ffa320b0":"Using the estimated values generated for the model for new cases and considering the rolling average for the other columns, here is the estimations for the next 2 weeks.","755d2156":"## Baseline","39e8b04b":"For better view and understanding, the expected and predicted data will be ploted bellow.","34800592":"## Conclusions\n\n* The model captured well the pattern of the time series, where working days always have more cases then weekends. This happens because the laboratories that process the results doesn't work on weekends.\n* The estimations of new cases are strongly influenced by the deaths and weekday. Therefore, another strategy has to be taken, using direct or native methods.","0280f0b7":"## First Model (RandomForestRegressor)\n\nFor this first model, will be used a `RandomForestRegressor`. The hyperparameters was tuned after a few executions of the model fit.","a7edb8ed":"This is an another try, using a more robust machine learning algorithm (LightGBM).","30bc7854":"## Ensembling Models\n\nIn order to minimize possible overfitting, these two models will be ensembled.","532a2ba7":"## Predictions\n\nUsing the models generated before, follows the prediction of the number of cases for the next day (considering the last day of the dataset).","d0866b35":"For better capturing the time series, the `feature_generation` function will add more features to the datasets, like the week day and week number, lags (difference of new cases between the days before) and rolling averages.","2cea7081":"The test set will be the last 15 days of data. From train set, will be extracted a validation set, with the last 15 records of the train set.","244ff62e":"## Second Model (LightGBM)","1088507c":"The goal of this notebook is to build a simple machine learning model with the COVID data to predict the number of cases of the next day. First of all, the data will be loaded and treated for build the model.","bf9e8283":"Not all records are in sequence, but we need a time series that have no gaps. The maximum gap size is 5 days. The gaps are inthe first records, when data started to being published and doesn't have a very high precision. Therefore, these records will be dropped."}}