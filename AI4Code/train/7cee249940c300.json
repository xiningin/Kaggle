{"cell_type":{"84fe7b08":"code","58ebca70":"code","77489de0":"code","ad69d144":"code","41236577":"code","efbedeea":"code","5263474b":"code","975cdd98":"code","62e2fd9b":"code","8ea85e5f":"code","411dcc94":"code","e0bbe532":"code","b10a05bf":"code","960e30e8":"code","c4e8c101":"code","ecf8af81":"code","fb92b573":"code","582bff0b":"code","2937653a":"code","74fd9998":"code","35f735a5":"code","b8c5f190":"code","1aea24b7":"code","f9eaedb3":"code","98c05cb3":"markdown","84402457":"markdown","c0b0a31d":"markdown","3515ab69":"markdown","7c305b4f":"markdown","9e457f69":"markdown","8892fc31":"markdown","96ea9bb6":"markdown","935711b5":"markdown","4916c4ff":"markdown","5f0e079f":"markdown","1916d260":"markdown","5834171d":"markdown","d814f143":"markdown","b8d43432":"markdown","8807ee53":"markdown","918bea91":"markdown","349d69b1":"markdown","0231f032":"markdown","f5401276":"markdown","8a54811f":"markdown","00f7a14d":"markdown","ef7d71d1":"markdown","702a6fb2":"markdown","c4ce61af":"markdown","4cad9803":"markdown","bee5a64b":"markdown","3056465c":"markdown","ccdd1fa3":"markdown","6d925b76":"markdown","f75593a0":"markdown","6c1a2ed9":"markdown","4c6bf26c":"markdown","ca02ce6a":"markdown","1410dc18":"markdown","0cb25b38":"markdown","d70a8f19":"markdown","bded7942":"markdown","6546bbb7":"markdown"},"source":{"84fe7b08":"# all imports we will need\nfrom wand.image import Image as WImage\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import RandomSampler, SequentialSampler, DataLoader\nfrom tabulate import tabulate\nfrom graphviz import Digraph\nfrom torch.autograd import Variable\nfrom torch.optim import Adam\nfrom sklearn.metrics import accuracy_score\n","58ebca70":"array = np.array([[[1, 2, 3, 4],\n                   [5, 6, 7, 8],\n                   [0, 0, 0, 1]],\n\n                  [[4, 3, 2, 1],\n                   [8, 7, 6, 5],\n                   [1, 0, 0, 0]]\n                  ])\n\n# bridge from np to torch\ntensor = torch.tensor(array)\n# bridge from torch to np\narray = tensor.numpy()\n\nprint(tensor, \"\\n\\n\")\nprint(\"size =\", tensor.size())\nprint(\"\\ntype:\\n\", type(tensor.numpy()))","77489de0":"tensor = torch.rand(size=(3, 4, 2))\nprint(\"original tensor:\\n\", tensor)\n\nnew_size = [4, 6]\ntensor_reshaped = tensor.reshape(new_size)\nprint(\"reshaped:\\n\", tensor_reshaped)","ad69d144":"tensor = torch.rand(size=(3, 3))\n\n# change dtype of a tensor\ntensor_long = tensor.long()\nprint(\"dtype of a long tensor:\", tensor_long.dtype)\n\ntensor_float = tensor.float()\nprint(\"dtype of a float tensor:\", tensor_float.dtype)","41236577":"tensor_a = torch.tensor([[1, 2, 3],\n                         [4, 5, 6]])\n\ntensor_b = torch.tensor([7, 8, 9])\n\ntensor_a + tensor_b","efbedeea":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\n# move them to the gpu (if one is available),\ntensor_a.to(device)\ntensor_b.to(device)\n\ntensor_a + tensor_b","5263474b":"class CustomDataset(Dataset):\n    \"\"\"\n    contains all the mandatory methods\n    to load it using a pytorch dataloader\n    \"\"\"\n\n    def __init__(self, data):\n        super().__init__()\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, item):\n        obs = self.data.iloc[item, :]\n        x = torch.tensor(obs.drop(\"label\").values).float()\n        y = torch.tensor(obs[\"label\"]).long()\n        return x, y","975cdd98":"def create_loader(df: pd.DataFrame, sampler_class, batch_size: int):\n    dataset = CustomDataset(data=df)\n    sampler = sampler_class(data_source=dataset)\n    return DataLoader(dataset=dataset, \n                      sampler=sampler, \n                      batch_size=batch_size)","62e2fd9b":"def get_demo_batch():\n    # parameters\n    batch_size = 16\n\n    # read data\n    demo_df = pd.read_csv(\"..\/input\/mnist-in-csv\/mnist_train.csv\")\n\n    # create loader\n    demo_loader = create_loader(df=demo_df,\n                                sampler_class=RandomSampler,\n                                batch_size=batch_size)\n\n    # draw first batch from loader\n    demo_x_batch, demo_y_batch = next(iter(demo_loader))\n    return demo_x_batch, demo_y_batch","8ea85e5f":"def demo_dataloaders():\n    demo_x_batch, demo_y_batch = get_demo_batch()\n    print(\"each batch is stored in a list.\")\n    print(\"\\nthe first entry is of type\", type(demo_x_batch))\n    print(\"and has a size of\", demo_x_batch.size())\n    print(\"\\nthe second entry is of type\", type(demo_y_batch))\n    print(\"and has a size of\", demo_y_batch.size())\n\n\ndemo_dataloaders()","411dcc94":"class CustomLinear(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        A = torch.randn(size=(out_features, in_features))\n        self.weight = nn.Parameter(data=A, requires_grad=True)\n        b = torch.randn(size=(out_features,))\n        self.bias = nn.Parameter(data=b, requires_grad=True) \n\n    def forward(self, x):\n        return x @ self.weight.t() + self.bias","e0bbe532":"class ExampleFNN(nn.Module):\n    def __init__(self, num_feats, num_classes):\n        super(ExampleFNN, self).__init__()\n\n        # hidden layer 1\n        self.linear1 = nn.Linear(in_features=num_feats, \n                                 out_features=256)\n        self.relu1 = nn.ReLU()\n\n        # output layer \n        self.linear2 = CustomLinear(in_features=256, \n                                    out_features=num_classes)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu1(x)\n        return self.linear2(x)","b10a05bf":"def show_weights(module):\n    if (type(module) == nn.Linear) or (type(module) == CustomLinear):\n        print(module)\n        print(type(module))\n        print(module.weight)\n        print(\"\\n\")","960e30e8":"def summary(module, x):\n    \"\"\"\n    Iterates over all inner module childs,\n    calculates number of parameters per child,\n\n    :param module: module to summarize\n    :param x: demo module input\n    \"\"\"\n    print_list = []\n    total_params = 0\n\n    # iterate over all inner module childs\n    for child in module.named_children():\n        x = child[1](x)\n        param_string = \"\"\n        child_params = 0\n        for param in child[1].named_parameters():\n            shape = list(param[1].size())\n            params = 1\n            for ax in shape:\n                params *= ax\n            child_params += params\n            param_string = param_string+f\"'{param[0]}'\"+\" shape: \"+str(shape)+\" \"\n        total_params += child_params\n        print_list.append([child[0], list(x.size()), param_string, child_params])\n\n    print(f\"Using a Batch Size of {x.size(0)}:\\n\")\n    headers = [\"Name\", \"Out Shape\", \"Weights\", \"Trainable Parameters\"]\n    print(tabulate(print_list, headers=headers))\n    print(\"\\nTrainable Model Parameters:\", total_params)","c4e8c101":"# source; https:\/\/gist.github.com\/wangg12\/f11258583ffcc4728eb71adc0f38e832\ndef make_dot(var, params=None):\n    if params is not None:\n        assert isinstance(params.values()[0], Variable)\n        param_map = {id(v): k for k, v in params.items()}\n\n    node_attr = dict(style=\"filled\", \n                     shape=\"box\", \n                     align=\"left\", \n                     fontsize=\"12\", \n                     ranksep=\"0.1\", \n                     height=\"0.2\")\n    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n    seen = set()\n\n    def size_to_str(size):\n        return \"(\" + (\", \").join([\"%d\" % v for v in size]) + \")\"\n\n    def add_nodes(var):\n        if var not in seen:\n            if torch.is_tensor(var):\n                dot.node(str(id(var)), \n                         size_to_str(var.size()), \n                         fillcolor=\"orange\")\n                dot.edge(str(id(var.grad_fn)), str(id(var)))\n                var = var.grad_fn\n            if hasattr(var, \"variable\"):\n                u = var.variable\n                name = param_map[id(u)] if params is not None else \"\"\n                node_name = \"%s\\n %s\" % (name, size_to_str(u.size()))\n                dot.node(str(id(var)), \n                         node_name, \n                         fillcolor=\"lightblue\")\n            else:\n                dot.node(str(id(var)), str(type(var).__name__))\n            seen.add(var)\n            if hasattr(var, \"next_functions\"):\n                for u in var.next_functions:\n                    if u[0] is not None:\n                        dot.edge(str(id(u[0])), str(id(var)))\n                        add_nodes(u[0])\n            if hasattr(var, \"saved_tensors\"):\n                for t in var.saved_tensors:\n                    dot.edge(str(id(t)), str(id(var)))\n                    add_nodes(t)\n\n    add_nodes(var)\n    return dot","ecf8af81":"def get_demo_model():\n    return ExampleFNN(num_feats=784, num_classes=10)","fb92b573":"def demo_fnn():\n    demo_model = get_demo_model()\n\n    demo_x_batch, _ = get_demo_batch()\n    demo_pred = demo_model(demo_x_batch)\n\n    # show weight initiation\n    demo_model.apply(show_weights)\n\n    # model summary\n    summary(module=demo_model, x=demo_x_batch)\n\n    # create computational graph\n    graph = make_dot(demo_pred)\n    graph.view()\n    img = WImage(filename='..\/input\/computational-graph\/Digraph.gv (2).pdf')\n    return img\n\n\ndemo_fnn()","582bff0b":"def f(x):\n    return x.t() @ x + 1\n\n\nx = torch.tensor([[2],\n                  [3]], dtype=torch.float32)\n\n# enable autograd for that tensor\n# underscore in the end denotes inplace operations\nx.requires_grad_()\n\ny = f(x)\n\nprint(y)\n\ny.backward()  # populate gradients\nx.grad","2937653a":"def demo_update_step():\n    # select a model to train\n    demo_model = get_demo_model()\n\n    # set the model into training mode if not done yet\n    demo_model.train()\n\n    # an elaborate variant of gradient descent\n    optimizer = Adam(demo_model.parameters())\n\n    # a loss function to determine the \"goodness\" of the model\n    loss_func = nn.CrossEntropyLoss()\n\n    # fetch a batch\n    x, y = get_demo_batch()\n\n    # forward\n    probas = demo_model(x)\n\n    # calculate loss\n    loss = loss_func(probas, y)\n\n    print(\"weight gradient before backward:\\n\", \n          demo_model.linear2.weight.grad)\n    print(\"x gradient before backward:\\n\", \n          x.grad)\n\n    # calculate gradient wrt all model parameters\n    loss.backward()\n\n    # gradient descent to update the model parameters\n    optimizer.step()\n\n    print(\"\\nweight gradient after backward:\\n\", \n          demo_model.linear2.weight.grad)\n    print(\"x gradient after backward:\\n\", \n          x.grad)\n\n    # clear gradient\n    optimizer.zero_grad()\n\n    print(\"\\nweight gradient after clearing:\\n\", \n          demo_model.linear2.weight.grad)\n    print(\"x gradient after clearing:\\n\", \n          x.grad)\n\n\ndemo_update_step()","74fd9998":"def calculate_loss(batch, model, loss_func, device):\n    x_batch = batch[0].to(device)\n    y_batch = batch[1].to(device)\n\n    optimizer.zero_grad()  # clear gradient\n    model.train()  # training mode  \n    out = model(x_batch)  # forward = make predictions\n    loss = loss_func(out, y_batch)  # calculate error\n    return loss\n\n\ndef train(batch, model, optimizer, loss_func, device):\n    loss = calculate_loss(batch=batch, \n                          model=model, \n                          loss_func=loss_func, \n                          device=device)\n    loss.backward()  # populate gradient wrt model parameters\n    optimizer.step()  # update model parameters\n    return loss\n\n\ndef train_loop(model, optimizer, loss_func, train_loader, device, n_loss_prints):\n    print_interval = int(len(train_loader) \/ n_loss_prints)\n    next_print_iter = print_interval\n    loss_summed = 0\n    for i, batch in enumerate(train_loader):\n        loss_summed += train(batch=batch,\n                             model=model,\n                             optimizer=optimizer,\n                             loss_func=loss_func,\n                             device=device)\n        if i == next_print_iter:\n            print(f\"iteration {i+1}\/{len(train_loader)}, loss:{loss_summed\/print_interval}\")\n            next_print_iter += print_interval\n            loss_summed = 0\n\n\ndef predict(model, sequential_loader, device):\n    model.eval()\n    y_proba = []\n    y_true = []\n    for batch in sequential_loader:\n        x_batch = batch[0].to(device)\n        y_batch = batch[1].to(device)\n        with torch.no_grad():\n            out = model(x_batch)\n            y_proba.append(out)\n            y_true.append(y_batch)\n    y_proba_tensor = torch.cat(y_proba)\n    y_true_tensor = torch.cat(y_true).cpu().numpy()\n    y_pred = np.argmax(y_proba_tensor.cpu().numpy(), axis=1)\n    return [y_pred, y_true_tensor]","35f735a5":"# parameters\ntotal_epochs = 2\nbatch_size = 16\n\n# create loaders\ntrain_df = pd.read_csv(\"..\/input\/mnist-in-csv\/mnist_train.csv\")\nval_df = pd.read_csv(\"..\/input\/mnist-in-csv\/mnist_test.csv\")\ntrain_sampler = RandomSampler\neval_sampler = SequentialSampler\ntrain_loader = create_loader(df=train_df,\n                             sampler_class=train_sampler,\n                             batch_size=batch_size)\ntrain_loader_eval = create_loader(df=train_df,\n                                  sampler_class=eval_sampler,\n                                  batch_size=batch_size)\nval_loader_eval = create_loader(df=val_df,\n                                sampler_class=eval_sampler,\n                                batch_size=batch_size)","b8c5f190":"model = ExampleFNN(num_feats=784, num_classes=10).to(device)\n\n# access all parameters\n# perform weight decay on selected ones\noptimizer = Adam([{\"params\": model.linear1.bias},\n                  {\"params\": model.linear2.bias},\n                  {\"params\": model.linear1.weight, \"weigth_decay\": 1},\n                  {\"params\": model.linear2.weight, \"weight_decay\": 1}], lr=0.0001)\nloss_func = nn.CrossEntropyLoss()\n\n# train\nfor epoch in range(1, total_epochs + 1):\n    print(f\"Epoch {epoch} \/ {total_epochs}:\")\n    train_loop(model=model,\n               optimizer=optimizer,\n               loss_func=loss_func,\n               train_loader=train_loader,\n               device=device,\n               n_loss_prints=10)\n    y_pred_train, y_true_train = predict(model=model,\n                                         sequential_loader=train_loader_eval,\n                                         device=device)\n\n    y_pred_val, y_true_val = predict(model=model,\n                                       sequential_loader=val_loader_eval,\n                                       device=device)\n    acc_train = accuracy_score(y_true=y_true_train, y_pred=y_pred_train)\n    acc_val = accuracy_score(y_true=y_true_val, y_pred=y_pred_val)\n    print(\"Train Accuracy:\", acc_train)\n    print(\"Validation Accuracy:\", acc_val)\n    print(\"\\n\")","1aea24b7":"def train_loop_ga(model, optimizer, loss_func, train_loader, device, accumulation):\n    loss_accumulated = 0\n    for i, batch in enumerate(train_loader):\n        loss_accumulated += calculate_loss(batch=batch, \n                              model=model, \n                              loss_func=loss_func, \n                              device=device)\n        if ((i+1)%accumulation==0):\n            loss_accumulated \/= accumulation\n            loss_accumulated.backward()\n            optimizer.step()\n            optimizer.zero_grad() \n            print(f\"iteration {i+1}\/{len(train_loader)}, loss:{loss_accumulated}\")\n            loss_accumulated = 0","f9eaedb3":"model = ExampleFNN(num_feats=784, num_classes=10).to(device)\noptimizer = Adam([{\"params\": model.linear1.bias},\n                  {\"params\": model.linear2.bias},\n                  {\"params\": model.linear1.weight, \"weigth_decay\": 50},\n                  {\"params\": model.linear2.weight, \"weight_decay\": 50}], lr=0.0001)\nloss_func = nn.CrossEntropyLoss()\n\n# train\nfor epoch in range(1, total_epochs + 1):\n    print(f\"Epoch {epoch} \/ {total_epochs}:\")\n    train_loop_ga(model=model,\n                  optimizer=optimizer,\n                  loss_func=loss_func,\n                  train_loader=train_loader,\n                  device=device,\n                  accumulation=150)\n    y_pred_train, y_true_train = predict(model=model,\n                                         sequential_loader=train_loader_eval,\n                                         device=device)\n\n    y_pred_val, y_true_val = predict(model=model,\n                                       sequential_loader=val_loader_eval,\n                                       device=device)\n    acc_train = accuracy_score(y_true=y_true_train, y_pred=y_pred_train)\n    acc_val = accuracy_score(y_true=y_true_val, y_pred=y_pred_val)\n    print(\"Train Accuracy:\", acc_train)\n    print(\"Validation Accuracy:\", acc_val)\n    print(\"\\n\")","98c05cb3":"As we can see, `loss.backward()` populates the gradient tensors of the parameters, not of the input.\nMoreover, we can see that we can clear them up (setting them to 0) manually using `optimizer.zero_grad()`. Otherwise the gradients would end up being added together (which has some benefits in more complex scenarios we will talk about later on).","84402457":"Note that we can use the apply method to iterate over the whole model. We can exploit this functionality to select certain module types and transform them. I.e., we can perform our own parameter initialization before training the model.\n\nThe last element contains the whole model.","c0b0a31d":"<a id=\"sec8\"><\/a>\n***\n<h1 style=\"background-color:SteelBlue; color:white\" >-> 8. PyTorch Best Practices And Learnings<\/h1>\nIn this chapter I collect some best practices I learned \"the hard way\"\nI will update this chapter when ever I have gone throgh some painful experiences.\nAs always: feel free to add your own experiences to the comment section or via LinkedIn so I can update my content. \nThis holds true especially for this chapter!\n\nNote that I might have ignored some of them in the chapters above, because this notebook is just educational content that isn't integrated in\na larger software project, doesn't need to maintained or refactored, and has to be as simple as possible to teach PyTorch beginners the fundamental concepts.\n\n### General Advices \n* write down an activity diagram of your data preprocessing and model forward pass to keep the big picture in mind while implementation\n* implement the loss function within the forward pass (like in the huggingface transformer models)\n\n### Debugging And Initial Hyperparameter Selection\n* try to overfit just one batch of your training data; if your model can't overfit one batch, your code probably contains bugs, or your initial hyperparameters are not feasible. **I recommend to do this for every hyperparameter configuration** to ensure that you don't waste any time training on unfeasible settings. cause time is the only thing that really matters in the end ;-) \n\n### Preprocessing\n* always have a backup of your raw data, no matter how unstructured it is\n* perform your preprocessing and store the results, so you don't have to perform that preprocessing again\n* perform as few (none if possible) processing in the `__getitem__()` method of your custom `torch.utils.data Dataset` to speed up your training and evaluation\n\n### CUDA Memory\n* if you run out of CUDA memory within training (after some epochs), you probably try to keep track of your results over many iterations \n* move from cpu to gpu or vice versa as few as possible to minimize runtime\n\n### Tracking\n* print out the current iteration,averaged loss, and averaged iteration runtime over the last iterations at least every few minutes (aka after a number of fixed iterations)\n* save a list of your training loss and runtimes on your hard drive for future decision making (like changing the learning rate, learning rate scheduling, runtime estimation, ...)\n* store your model after each epoch on your hard drive, so you have easy access to all your checkpoints for future analysis\n* store the training config\/hyperparameters (learning rate, used features, weight decay, dropout, ...) used per training instance on your harddrive and link them to the resulting model checkpoints and evaluation metricss\n\n### File Organization\n* `project` dir contains `code` dir, `checkpoints` dir, and `data` dir\n* `code` dir contains all `.py` files\n* `checkpoints` dir contains`<network_name>_config_<i>` dirs, which contain the model checkpoints along one training process, the utilized hyperparameter config files, and the visualization of the resulting evaluation metrics\n* `hyperparameter config files` are just dictionaries containing the parameter combinations written down into .txt files. \n* `data` dir contains the raw and the preprocessed data files\n* `data_preprocessing.py` contains the data preprocessing functionality. Writes the preprocessed data to a file\n* `datasets_custom.py` contains the custom torch dataset classes\n* `blocks.py` contains all custom network building block classes, i.e. all mmodules that are later combined to form your neural networks\n* `<network_name>.py` contains only the class, the hyperparameter configuration class (a class holding one dictionary having all the hyperparameters. save the default values of all hyperparameters (like default dam learning rate for example) as well), and training\/eval loop of the network named <network_name>\n* `<network_name>_training.py` performs the training of the network <network_name>\n* `<network_name>_evaluation.py` performs the evaluation of the network <network_name> and stores\n\n### Further Readings\nHelpful Videos and Blogs:\n* [Elliot Waite: Autograd](https:\/\/www.youtube.com\/watch?v=MswxJw-8PvE&t=75s)","3515ab69":"## Dropout\ntodo","7c305b4f":"## Reshaping Tensors\nWe can simply reshape tensors to change their size.\nHaving tensor of rank n, reshaping it recursively fills the desired shape beginning with the first element of the first tensor of rank n-1.","9e457f69":"The `CustomLinear` module is a simplified implementiation of the [nn.Linear](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.Linear.html) module.\nThe fundamental functionalities are the same.\nNote that this layer is nothing else but a combination of tensors with `requires_grad=True`, and tensor operations.\nNote that `A` and `b` are `nn.Parameters`, and thus are iteratively altered during the training process to improve the performance of the module.\nLater on we will see how this works in detail.\nI renamed `A` and `b` to `weight` and `bias` respectively. to match the pattern of the original linear layer.","8892fc31":"<a id=\"sec0\"><\/a>\n***\n<h1 style=\"background-color:SteelBlue; color:white\" >-> 0. Prerequisits<\/h1>\n\n\n1. **python fundamentals:** [This simple & free Kaggle Course](https:\/\/www.kaggle.com\/learn\/python) is already enough!\n2. **notebooks & numpy:** [Chapter 1&2 of this free book](https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/) is probably the best way to learn it! \n\nFrom now on I expect you all to be familiar with the concepts used in the named sources. I don't expect any further python skills.","96ea9bb6":"visualize the model as a computational graph:","935711b5":"## Overfitting And Underfitting\n* our goal is to fing **general patterns** in the population from which the training data is sampled (under iid)\n* **underfitting** occurs when our model is not complex enough to grasp the important general patterns and leads to an underperforming model\n* **detect underfitting** by comparing with other models. When both training- and validation error are high, chances are good that the model underfits\n* **overfitting** occurs when our model doesn't learn only the general patterns but subpatterns that occur in the training data but are not important in the whole population\n* **generalization error** is the error our model has over the whole population. It can be approximated using validation data. The generalization error can be minimized best by learning general patterns.\n* **identify overfitting:** model performs way better on the training data than on the validation data\n* make sure that you don't just overfit training and validation data by evaluating your model on a **test set** after training\n* using **cross validation** provides a quite robust estimation of the generalization error\n* **regularization techniques** are used to prevent overfitting","4916c4ff":"## Model Inspection\nLet's investigate our model a little. Therefore, we can either use a predefined model summarizer like [torchsummery](https:\/\/pypi.org\/project\/torch-summary\/https:\/\/pypi.org\/project\/torch-summary\/) or write our own summarizer from scratch. The latter provides some further insights into handling the model.\n\nLet's investigate how the model parameters are initialized:","5f0e079f":"This way, we can automatically calculate the gradient of a function (e.g. a loss function) with respect to all parameters, even if the computation of the function contains loops and conditionals.\nLet's exploit this for training a neural network!","1916d260":"<h1 style=\"background-color:SteelBlue; color:white\" >-> Content:<\/h1>\nHi all, this notebook covers the most important concepts of deep learning on structured (tabular) data. \n\nThis project is still work in progress. Feel free to leave a comment to suggest further improvements.\n\n## 0. [Prerequisits](#sec0)\n\n## 1. [Tensor Handling](#sec1)\n* Rank And Size\n* Reshaping Tensors\n* Dtypes\n* Tensor Broadcasting\n* Use Tensors On GPUs\n\n## 2. [Data Loading](#sec2)\n* Custom Datasets\n* Samplers And Data Loaders\n* Demonstration\n\n## 3. [Modules, Linear Layers And FNN](#sec3)\n* Linear Layers From Scratch\n* Create A Custom FNN\n* Model Inspection\n* Demonstration\n\n## 4. [Autograd and Training](#sec4)\n* Autograd\n* Training Neural Networks\n* Training Loop And Evaluation\n* Perform Training\n\n## 5. [Regularization](#sec5)\n* Overfitting And Underfitting\n* Vanishing And Exploding Gradients\n* Weight Decay\n* Dropout\n* Further Techniques\n\n## 6. [Gradient Accumulation](#sec6)\n\n## 7. [(Batch) Normalization](#sec7)\n\n## 8. [PyTorch Best Practices](#sec8)","5834171d":"## Training Loop And Evaluation","d814f143":"## Autograd\n\nWe can describe every tensor operation as a function. PyTorch Autograd allows us to calculate the Gradient of such a function with respect to each individual input. Let me give you an example. If you are not familiar with matrix calculus, you can always refer to [this](https:\/\/en.wikipedia.org\/wiki\/Matrix_calculus):\n\n$$f:\\mathbb{R}^2\\rightarrow \\mathbb{R}, f(x) = x^Tx+1 = x_1^2+x_2^2 +1$$\n\nHas the following partial derivatives:\n$$\\frac{\\partial f(x)}{\\partial x_1}=2x_1$$\n\n$$\\frac{\\partial f(x)}{\\partial x_2}=2x_2$$\n\nSo we would obtain at $x=\\left(\\begin{array}{c} 2 \\\\ 3 \\end{array}\\right)$:\n\n\n$$f(x)=2^2+3^2+1=4+9+1=14$$\n\n$$\\frac{\\partial f(x)}{\\partial x_1}=4$$\n\n$$\\frac{\\partial f(x)}{\\partial x_2}=6$$\n\nAs we can see, Pytorch autograd provides the same results:","b8d43432":"## Samplers And Data Loaders\n* `Samplers` define data drawing policies\n* `Samplers` are an iterable over torch dataset indices\n* DataLoaders are used to draw batches of data from Datasets using drawing policies","8807ee53":"<a id=\"sec3\"><\/a>\n***\n<h1 style=\"background-color:SteelBlue; color:white\" >-> 3. Linear Layers And FNN<\/h1>\n\n## Modules, Linear Layers From Scratch\n\nPyTorch combines tensors and tensor operations to modules. These modules are building blocks which can be used to construct neural networks. They can contain single layers or large neural networks. \nThe probably most simple module is the [nn.Linear](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.Linear.html) layer.\n\nGiven some input $x$, the linear layer computes its output $y$ via $y=xA^T+b$. Both, the weight matrix $A$ and the bias $b$ will be adapted during the training process so that the output $y$ becomes as close as possible to the ground truth.","918bea91":"<div class=\"alert alert-danger\" role=\"alert\">\n    <h3>Feel free to <span style=\"color:red\">comment<\/span> if you have any suggestions   |   motivate me with an <span style=\"color:red\">upvote<\/span> if you like this project.<\/h3>\n<\/div>","349d69b1":"the **size** of this tensor tells us that it is a tensor of rank 3 consisting of 2 tensors of rank 2, in which 3 tensors of rank 1 are stored. In each of these tensors of rank 1 are 4 tensors of rank 0.","0231f032":"The computational graph displays:\n\n* orange = model output is of size `batch_size x n_classes` (leaf of the forward pass tree, root of the backward pass tree)\n* blue = trainable model parameters (root of the forward pass tree, leaf of the backward pass tree)\n\nnote: the edges within the backward pass direct into the opposite directions. This results in the roots becoming leafs, and vice versa. Thus, we often say that the gradient can be calculated wrt the leafs, we actually refer to the model parameters.","f5401276":"## Tensor Broadcasting\n\nsimilar to numpy arrays, we can add, subtract, multiply ... 2 tensors. \n\nThe following example shows that tensor operations stick to the rules of **numpy broadcasting** as explained in [this chapter](https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/02.05-computation-on-arrays-broadcasting.html) of the book listed in the prerequisits:\n\nif we add a tensor **a of rank 2** to a tensor **b of rank 1**, b is will be added to each tensor of rank 1 stored in a","8a54811f":"<a id=\"sec7\"><\/a>\n***\n<h1 style=\"background-color:SteelBlue; color:white\" >-> 7. (Batch) Normalization<\/h1>\ntodo","00f7a14d":"<a id=\"sec4\"><\/a>\n***\n<h1 style=\"background-color:SteelBlue; color:white\" >-> 4. Autograd And Training<\/h1>","ef7d71d1":"<a id=\"sec6\"><\/a>\n***\n<h1 style=\"background-color:SteelBlue; color:white\" >-> 6. Gradient Accumulation<\/h1>\nGradient Accumulation might be beneficial:\n\n* The optimal batch size depends on both data and algorithm. \n\n* Too small values lead to less stable learning behavior and giving more importance to outliers. The calculated Gradient per batch is just a predicted gradient for the whole dataset.\n\n* GPU memory is limited, thus we might be forced to use batch sizes smaller than optimal.\n\n* GPU memory consumption mostly depends on the `model complexity` (#parameters), the `data size` (e.g. large images vs. small imagtes)\n\n* We can reduce the memory consumption depending on data size by using Gradient Accumulation\n\n* `Gradient Accumulation` sequentially creates gradients for pseudo batches that are larger than the actual batches.","702a6fb2":"As we can see, a larger batch size (aka a bigger accumulation range) doesn't automatically result in a better training behavior. ","c4ce61af":"## Rank And Size\nDescribing tensors we need some terminology. The most important property of a tensor is its **rank** and **size**.\n\nTensor of...\n\n* **rank 0**: a number \/ scalar\n* **rank 1**: an array of rank 0 tensors\n* **rank 2**: an array of rank 1 tensors\n* **rank 3**: an array of rank 2 tensors\n...\n\nThe **size** of a tensor describes how many tensors of smaller ranks are stored in it.","4cad9803":"## Weight Decay\n* weight decay (l2 regularization) is a regularization technique\n* weight decay flattens the distribution of weights within the model (parameters are more likely to have similar values)\n* model is less sensitive and focuses more on underlying patterns instead of noise\n* gradient flow is improved, which lowers the risk of vanishing\/exploding gradients (more on that later)\n\nLet's perform training with some weight decay:","bee5a64b":"<a id=\"sec2\"><\/a>\n***\n<h1 style=\"background-color:SteelBlue; color:white\" >-> 2. Data Loading<\/h1>\n\n## Custom Datasets\nEach entry of a TensorDataset contains the independend and the dependend variable(s) of one observation.\n\nThe following dataset is a \"map-style\" torch dataset, i.e.:\n* it implements `__getitem__()` and `__len__()` from `torch.utils.data.Dataset`\n* it maps indices to data samples\/observations","3056465c":"## Training Neural Networks\n\nOne update step is performed as follows:\n1. fetch a batch\n2. perform the forward pass\n3. calculate the loss\n4. calculate the gradient of the loss wrt all model parameters\n5. perform a variant of gradient descent to update the model parameters\n6. clear the gradient to perform further updates\n\nLet's perform such a step to get an idea of its mechanics!","ccdd1fa3":"<a id=\"sec5\"><\/a>\n***\n<h1 style=\"background-color:SteelBlue; color:white\" >-> 5. Regularization<\/h1>","6d925b76":"## Vanishing And Exploding Gradients\n* are underfitting problems \n* when calculating the gradient with respect to a parameter, we need to calculate the gradient of the loss function with respect to the parameter\n* the gradient is calculated using the chain rule, i.e. by calculating the derivative of each function wrapped around the parameter with respect to the next inner function wrapped around it until the parameter is reached\n* the chain rule results in a chain of multiplications. One multiplication for each inner function\n* the gradient with respect to the early parameters becomes a very long chain, especially for very **deep** neural networks\n* the linger this chain \/ the more factors lead to the final product, the higher is the probability to obtain either very large or very small products. \n* these products are later used to update the parameters, which leads to an imbalance between the parameters\n* imbalanced parameters (e.g. very small parameter values in the first layers, moderate parameter values in the later layers) lead to a worse learning ability of the model (and thus to udnerfitting)\n* **indicators:** underfitting or strong changes in the loss after each iteration, to check, see if the weights become very big\/small early\n* **counters:** [gradient clipping](https:\/\/stackoverflow.com\/questions\/54716377\/how-to-do-gradient-clipping-in-pytorch), i.e. binding the gradient to a certain size, if their norm exceeeds a certain threshhold; using weight decay fights exploding gradients ","f75593a0":"## Demonstration","6c1a2ed9":"This Project is still in progress =)","4c6bf26c":"Each layer $\\mathscr{l}$ of a FNN has $M_{\\mathscr{l}-1} \\cdot M_{\\mathscr{l}}$ many weights plus $M_{\\mathscr{l}}$ many bias terms. $M_{\\mathscr{l}}$ is the number of nodes (i.e. output size) of layer ${\\mathscr{l}}$.","ca02ce6a":"## Demonstration","1410dc18":"## Create A Custom FNN\nNote: The first linear layer is builtin, the last linear layer is our custom module\n\nAs we can see, the whole neural network is based on the module-abstraction as well.","0cb25b38":"<a id=\"sec1\"><\/a>\n***\n<h1 style=\"background-color:SteelBlue; color:white\" >-> 1. Tensor Handling<\/h1>\n\nThere are multiple ways of interpreting tensors. From a python perspective it's best to interpret them as regular arrays.\nWe can simply create them from existing lists or numpy arrays.","d70a8f19":"## Dtypes\nEach tensor stores values of a single type only.\nYou will see later on that some core functionalities of PyTorch expect tensors of a certain **dtype**.\n\nStandard **dtypes**:\n* **int64 aka long**: tensors storing integers only\n* **float32 aka float**: tensors storing at least one non-integer","bded7942":"## Further Techniques\n* using a less complex model, i.e. **reducing the number of parameters** (complexity of a model is sometimes called its capacity, flexibility, or degree of freedom)\n* collect **more training data** so the general patterns are easier to detect for the model\n* **smaller batch sizes** might lead to less overfitting (and thus lead to a smaller generalization error) according to Goodfellow as discussed in [this post](https:\/\/stats.stackexchange.com\/questions\/266368\/deep-learning-why-does-increase-batch-size-cause-overfitting-and-how-does-one-r); optimizing on smaller batches leads to more variability and magnitude in the update directions and thus lowers the probability of getting stuck in narrow local minima. broader (local) minima are more likely to correspond to actual patterns we want to learn \n* **bigger learning rates** can lead to getting out of narrow local minima as well\n* according to [this source](https:\/\/d2l.ai\/chapter_optimization\/lr-scheduler.html), **decreasing the learning rate iteratively** offers a regularization effect in practice, in spite of not beig backed by a theoretical proof.\n* changing the **parameter initialization** technique (i.e. using pretrained models)\n* create more meaningful features using **feature engineering**\n* **batch normalization** leads to some regularization. We will take a look at it in a later section of this notebook","6546bbb7":"## Use Tensors On GPUs\n\nOne of the most important benefits of tensors is that you can perform tensor operations on the GPU (if you have one).\n\nBoth tensors have to be on the same device if you want to perfom an operation on them."}}