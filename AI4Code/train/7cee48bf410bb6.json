{"cell_type":{"ebf5290b":"code","da8a0f47":"code","57c13ed6":"code","9699b769":"code","5425c980":"code","9bfb80da":"code","8d0a673c":"markdown","5b33577a":"markdown","a3f3eb3e":"markdown","4806da71":"markdown","b7ec7084":"markdown","a9b831c9":"markdown","d3abdb47":"markdown","3c8d504b":"markdown","08f618c1":"markdown","9f85038d":"markdown"},"source":{"ebf5290b":"import numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nfrom scipy.stats import norm, chi2\nfrom scipy.stats import t as t_dist\nfrom sklearn.datasets import load_wine\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split, KFold\n\n# Libs implementations\nfrom mlxtend.evaluate import mcnemar\nfrom mlxtend.evaluate import mcnemar_table\nfrom mlxtend.evaluate import paired_ttest_5x2cv\nfrom mlxtend.evaluate import proportion_difference\nfrom mlxtend.evaluate import paired_ttest_kfold_cv\nfrom mlxtend.evaluate import paired_ttest_resampled","da8a0f47":"def paired_t_test(p):\n    p_hat = np.mean(p)\n    n = len(p)\n    den = np.sqrt(sum([(diff - p_hat)**2 for diff in p]) \/ (n - 1))\n    t = (p_hat * (n**(1\/2))) \/ den\n    \n    p_value = t_dist.sf(t, n-1)*2\n    \n    return t, p_value\n\ndef mcnemar_test(y_true, y_1, y_2):\n    b = sum(np.logical_and((knn_y != y_test),(rf_y == y_test)))\n    c = sum(np.logical_and((knn_y == y_test),(rf_y != y_test)))\n    \n    c_ = (np.abs(b - c) - 1)**2 \/ (b + c)\n    \n    p_value = chi2.sf(c_, 1)\n    return c_, p_value\n\ndef z_test(pa, pb, n):\n    p = (pa + pb)\/2\n    z = (pa - pb) \/ (((2*p*(1-p))\/n)**(1\/2))\n    \n    p_value = norm.cdf(z)\n    \n    return z, p_value\n\ndef five_two_statistic(p1, p2):\n    p1 = np.array(p1)\n    p2 = np.array(p2)\n    p_hat = (p1 + p2) \/ 2\n    s = (p1 - p_hat)**2 + (p2 - p_hat)**2\n    t = p1[0] \/ np.sqrt(1\/5. * sum(s))\n    \n    p_value = t_dist.sf(t, 5)*2\n    \n    return t, p_value","57c13ed6":"X, y = load_wine(return_X_y = True)","9699b769":"rf = RandomForestClassifier(random_state=42)\nknn = KNeighborsClassifier(n_neighbors=1)","5425c980":"# For holdout cases\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\nrf.fit(X_train, y_train)\nknn.fit(X_train, y_train)\n\nrf_y = rf.predict(X_test)\nknn_y = knn.predict(X_test)\n\nacc1 = accuracy_score(y_test, rf_y)\nacc2 = accuracy_score(y_test, knn_y)\n\nprint(\"Proportions Z-Test\")\nz, p = proportion_difference(acc1, acc2, n_1=len(y_test))\nprint(f\"z statistic: {z}, p-value: {p}\\n\")\n\nprint(\"McNemar's test\")\nchi2_, p = mcnemar_test(y_test, rf_y, knn_y)\nprint(f\"chi\u00b2 statistic: {chi2_}, p-value: {p}\\n\")\n\nn_tests = 30\n\np_ = []\nrng = np.random.RandomState(42)\nfor i in range(n_tests):\n    randint = rng.randint(low=0, high=32767)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=randint)\n    rf.fit(X_train, y_train)\n    knn.fit(X_train, y_train)\n\n    acc1 = accuracy_score(y_test, rf.predict(X_test))\n    acc2 = accuracy_score(y_test, knn.predict(X_test))\n    p_.append(acc1 - acc2)\n    \nprint(\"Paired t-test Resampled\")\nt, p = paired_t_test(p_)\nprint(f\"t statistic: {t}, p-value: {p}\\n\")\n\np_ = []\n# For cross-validated cases\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\nfor train_index, test_index in kf.split(X):\n    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n    rf.fit(X_train, y_train)\n    knn.fit(X_train, y_train)\n\n    acc1 = accuracy_score(y_test, rf.predict(X_test))\n    acc2 = accuracy_score(y_test, knn.predict(X_test))\n    p_.append(acc1 - acc2)\n\nprint(\"Cross Validated Paired t-test\")\nt, p = paired_t_test(p_)\nprint(f\"t statistic: {t}, p-value: {p}\\n\")\n\np_1 = []\np_2 = []\n# For the 5x2cv\nrng = np.random.RandomState(42)\nfor i in range(5):\n    randint = rng.randint(low=0, high=32767)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=randint)\n\n    rf.fit(X_train, y_train)\n    knn.fit(X_train, y_train)\n    acc1 = accuracy_score(y_test, rf.predict(X_test))\n    acc2 = accuracy_score(y_test, knn.predict(X_test))\n    p_1.append(acc1 - acc2)\n\n    rf.fit(X_test, y_test)\n    knn.fit(X_test, y_test)\n    acc1 = accuracy_score(y_train, rf.predict(X_train))\n    acc2 = accuracy_score(y_train, knn.predict(X_train))\n    p_2.append(acc1 - acc2)\n\nprint(\"5x2 CV Paired t-test\")     \nt, p = five_two_statistic(p_1, p_2)\nprint(f\"t statistic: {t}, p-value: {p}\\n\")","9bfb80da":"print(\"Proportions Z-Test\")\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\nrf.fit(X_train, y_train)\nknn.fit(X_train, y_train)\n\nrf_y = rf.predict(X_test)\nknn_y = knn.predict(X_test)\n\nacc1 = accuracy_score(y_test, rf_y)\nacc2 = accuracy_score(y_test, knn_y)\n\nz, p = proportion_difference(acc1, acc2, n_1=len(y_test))\nprint(f\"z statistic: {z}, p-value: {p}\\n\")\n\nprint(\"McNemar's test\")\ntable = mcnemar_table(y_target=y_test, y_model1=rf_y, y_model2=knn_y)\nchi2_, p = mcnemar(ary=table, corrected=True)\nprint(f\"chi\u00b2 statistic: {chi2_}, p-value: {p}\\n\")\n\nprint(\"Paired t-test Resampled\")\nt, p = paired_ttest_resampled(estimator1=rf, estimator2=knn, X=X, y=y, random_seed=42, num_rounds=30, test_size=0.2)\nprint(f\"t statistic: {t}, p-value: {p}\\n\")\n\nprint(\"Cross Validated Paired t-test\")\nt, p = paired_ttest_kfold_cv(estimator1=rf, estimator2=knn, X=X, y=y, random_seed=42, shuffle=True, cv=10)\nprint(f\"t statistic: {t}, p-value: {p}\\n\")\n\nprint(\"5x2 CV Paired t-test\")\nt, p = paired_ttest_5x2cv(estimator1=rf, estimator2=knn, X=X, y=y, random_seed=42)\nprint(f\"t statistic: {t}, p-value: {p}\\n\")","8d0a673c":"### Manual Implementation","5b33577a":"## Implementation","a3f3eb3e":"#### Mlxtend","4806da71":"#### Data Splitting - Manual Verification","b7ec7084":"### Comparison With Existing Methods\n\nIn this section I will compare my implementation with the existing implementations from the Mlxtend library to be sure that my functions are correct.","a9b831c9":"# Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms","d3abdb47":"#### Defining the Classifiers","3c8d504b":"## Theoretical Introduction\n\n### Type I and Type II Error\n\nWe can define the Type I error as the error when we reject the null hypothesis when it is actually true. In this case, we are dealing with a false positive for the alternative.\n\nOn the other hand, the Type II error occours when we do not reject the null hypothesis when the alternative is actually true. In this case we have a false negative.\n\n![image.png](attachment:12a89e33-32e5-4df0-8ae6-abf196eab06e.png)\n\n### The Two Proportions Test\n\nThe two proportions tests compares the proportion of wrong classified examples (so it is basically 1 - accuracy). It has one assumption that is important:\n\n- It assumes that the samples are independent\n\nAs both proportions are calculated on the same test set, they are not, in fact, indepedent. Also, this does not take into account the variance that results may have given the variation of the test set and also does not account for the total dataset, but rather a smaller set used for training.\n\nThe null hypothesis is that the difference between the proportions (which is normally distributed since they are both binomial under the assumption that the proportions are independent) has mean zero (hence, the proportions are, on average, the same). The statistics for the test is given by:\n\n$$z = \\frac{p_A - p_B}{\\sqrt{2p(-1p)\/n}}$$\n\n### The Resampled Paired t-test\n\nOn this test we set a number of trials (e.g to 30) and measure the proportion of missclassifications for each algorithm on each trial. On this case, we use a holdout on each set. \n\nThen, we assume that $p^i = p_A^i - p_B^i$ is normally distributed, so we can apply the student's t test with the following statistic:\n\n$$t = \\frac{\\hat{p}\\sqrt{n}}{\\sqrt{\\frac{\\sum_{i=1}^n{(p^i-\\hat{p})^2}}{n-1}}}$$\n\nThe problems here are:\n\n- The proportions are not independent because they are made on the same test set\n- The trials are not independent because there is overlap between the tests sets and the training sets\n\n### The Cross-Validated Paired t-test\n\nThis is the same as the test above but we will use the k-fold cross validation technique and not a simple resample. With this test, the test sets do not overlap anymore, however, we still have the training overlap and the dependence betweeen the proportions.\n\n### The McNemar's test\n\nFor this test we will train two algorithms on a holdout set and create a contigency table as follows:\n\n![image.png](attachment:8f390941-8ba6-4726-94df-f1ca8ac4de3e.png)!\n\nUnder the null hypothesis that the two algorithms should have the same error rate, we can do a $\\chi^2$ test on the following statistics:\n\n$$\\chi^2 = \\frac{(|b - c| - 1)**2}{b+c}$$\n\nThe main drawbacks here are:\n\n- We do not take into account train set variation\n- We do not take into account internal algorithm variation\n- We use a smaller set than the original\n\nHowever, every test here suffers from the size problem. The advantage of the McNemar's test is that it only requires the algorithms to run once. The paper actually concludes that when the algorithms take too long to run, that this should be used.\n\n### The 2x5 Cross Validation\n\nThis method tries to improve on the poor mean estimation of the cross-validated paired t-test. In order to do so, a 2-fold cross validation is repeated 5 times, generating 10 different estimations of proportion difference.\n\nGiven that $p^1 = p_A^1 - p_B^1$, $p^2 = p_A^2 - p_B^2$, $\\hat{p} = p^1 + p^2 \/ 2$ and $s^2 = (p^1 - \\hat{p})^2 + (p^2 - \\hat{p})^2$, the statistic can be calculated as:\n\n$$t^~ = \\frac{p_1^1}{\\sqrt{\\frac{1}{5}\\sum^5_1 s_i^2}}$$\n\nThis still is not perfect. However, this is the paper suggestion if we can afford to train the models several times.","08f618c1":"This paper tries to analyze different approximate statistical tests methods to define which one should be used to compare two learning algorithms for a given problem. The original paper can be found on: https:\/\/direct.mit.edu\/neco\/article-abstract\/10\/7\/1895\/6224\/Approximate-Statistical-Tests-for-Comparing","9f85038d":"#### Defining the data"}}