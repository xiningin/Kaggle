{"cell_type":{"fd5fbfe7":"code","723ff213":"code","abf9bdaa":"code","3c90a4de":"code","1ecfdd1c":"code","add25890":"code","91f29c60":"code","a1629ce6":"code","d5df2ce4":"code","c8f4e5fd":"code","3748c778":"code","6eb5a667":"code","b9c55018":"code","2f522ae8":"code","4d0c53f5":"code","f644e973":"code","dc7ac61c":"code","79602502":"code","1f858f8e":"code","f1c3694a":"code","90bb332b":"code","4683dfcb":"code","3f43869c":"code","aaf5ad92":"code","604f9e91":"code","3eb3728e":"markdown","d6e0c1e3":"markdown","897d9bdf":"markdown","921ebcd3":"markdown","a243976b":"markdown","dc4c6436":"markdown","0a4a27af":"markdown","8b3acd40":"markdown","18b571f1":"markdown","0e9911db":"markdown","114d7d9a":"markdown","53a95863":"markdown","5b58af9a":"markdown","5be5facf":"markdown"},"source":{"fd5fbfe7":"import nltk\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\npd.set_option('display.max_colwidth', 10000)","723ff213":"data = pd.read_csv('..\/input\/apple-iphone-se-reviews-ratings\/APPLE_iPhone_SE.csv')","abf9bdaa":"data.head()","3c90a4de":"print(f'data length: {len(data)}\\n')\nprint(f'missing values by column:\\n{data.isnull().sum()}\\n')\nprint(f'data duplication(all columns): {data.duplicated().sum()}')\nprint(f'data duplication(Ratings+Comment): {data.duplicated(subset=[\"Ratings\",\"Comment\"]).sum()}')\nprint(f'data duplication(Ratings+Reviews): {data.duplicated(subset=[\"Ratings\",\"Reviews\"]).sum()}')","1ecfdd1c":"## visualization function\n\ndef value_counts_plot(df, column=None, top=None, fontsize=15):\n    if top is not None:\n        label_counts = df[column].value_counts().head(top)\n    else:\n        label_counts = df[column].value_counts()\n    \n    plt.figure(figsize=(15,10))\n    plt.style.use('seaborn')\n    if top is not None:\n        bar_colors = cm.rainbow(np.linspace(0,1,top))\n    else:\n        bar_colors = cm.rainbow(np.linspace(0,1,len(df[column].unique())))\n    plt.bar(label_counts.index.values,\n            label_counts.values,\n            color=bar_colors,\n            linewidth=0,\n            alpha=0.6)\n    plt.tick_params(labelsize=fontsize)\n    if top is not None:\n        plt.xticks(rotation=-45, ha='left')\n        plt.xlabel(f'{column} top {top}', fontsize=fontsize)\n    else:\n        plt.xlabel(column, fontsize=fontsize)\n    plt.ylabel(f'{column} value counts', fontsize=fontsize)\n    plt.show()\n    \n    \ndef text_length_plot(\n    df, \n    text_column=None, \n    label_column=None, \n    fontsize=15, \n    alpha=1.0\n):\n    df['text_len'] = df[text_column].apply(lambda x: len(x))\n    labels = df[label_column].unique().tolist()\n    bar_colors = cm.rainbow(np.linspace(0,1,len(labels))).tolist()\n    \n    plt.figure(figsize=(15,10))\n    plt.style.use('seaborn')\n    for label, bar_color in zip(labels, bar_colors):\n        df.query('{} == @label'.format(label_column))['text_len'].plot(\n            bins=50, \n            kind='hist', \n            color=bar_color, \n            label=label,\n            alpha=alpha\n        )\n    df.drop('text_len', axis=1, inplace=True)\n    plt.tick_params(labelsize=fontsize)\n    plt.legend()\n    plt.xlabel(f'{text_column} text length by {label_column}', fontsize=fontsize)\n    plt.ylabel('frequency', fontsize=fontsize)\n    plt.show()\n    \n    \ndef word_freq_plot(\n    df, \n    text_column=None, \n    apply_stopwords=False, \n    top=30, \n    fontsize=15\n):\n    all_words = df[text_column].str.split(expand=True).unstack()\n    if apply_stopwords == True:\n        stopwords = nltk.corpus.stopwords.words('english')\n        all_words = pd.Series([word for word in all_words if word not in stopwords])\n    count_words = all_words.value_counts().head(top)\n    \n    plt.figure(figsize=(15,10))\n    plt.style.use('seaborn')\n    bar_colors = cm.rainbow(np.linspace(0,1,top))\n    plt.bar(count_words.index.values,\n            count_words.values,\n            color=bar_colors,\n            linewidth=0)\n    plt.tick_params(labelsize=fontsize)\n    plt.xticks(rotation=-45, ha='left')\n    plt.title(f'Top {top} Word frequencies in the {text_column}', fontsize=fontsize)\n    plt.show()","add25890":"value_counts_plot(data, column='Ratings')","91f29c60":"value_counts_plot(data, column='Comment', top=10)","a1629ce6":"text_length_plot(data, text_column='Comment', label_column='Ratings', alpha=0.7)","d5df2ce4":"word_freq_plot(data, 'Comment', apply_stopwords=True)","c8f4e5fd":"text_length_plot(data, text_column='Reviews', label_column='Ratings', alpha=0.7)","3748c778":"word_freq_plot(data, 'Reviews', apply_stopwords=True)","6eb5a667":"import os\nimport random","b9c55018":"class Cfg:\n    seed = 42\n    n_folds = 5\n    test_size = 0.2","2f522ae8":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    \nseed_everything(Cfg.seed)","4d0c53f5":"import re\nimport unicodedata\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize, MWETokenizer\n\ndef text_normalize(text):\n    text = text.strip()\n    text = unicodedata.normalize('NFKC', text)\n    return text\n\ndef text_preprocess(text):\n    # remove this word as it commonly appears at the end of text\n    text = text.replace('...READ MORE', '')\n    text = text.replace('READ MORE', '')\n    # the numbers're deemed not to be important features and are removed \n    text = re.sub('\\d+', '', text)\n    # remove the debris generated by erasing the numbers\n    text = text.replace('hr', '')\n    # basic text cleansing\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n    return text\n\ndef text_tokenize(text):\n    tokenizer = MWETokenizer()\n    tokenized_text = tokenizer.tokenize(word_tokenize(text))\n    return tokenized_text\n\ndef text_lemmatization(text):\n    # apply stopwords\n    tokenized_text = text_tokenize(text)\n    stopwords = nltk.corpus.stopwords.words('english')\n    filtered_words = [word for word in tokenized_text if word not in stopwords]\n    # apply lemmatization\n    wordnet_lemmatizer = WordNetLemmatizer()\n    lemmatized_words = []\n    for word in filtered_words:\n        word = wordnet_lemmatizer.lemmatize(word, pos='n')\n        word = wordnet_lemmatizer.lemmatize(word, pos='v')\n        word = wordnet_lemmatizer.lemmatize(word, pos='a')\n        lemmatized_words.append(word)\n    text = ' '.join(lemmatized_words)\n    # remove the debris generated by lemmatization\n    text = text.replace('hr', '').replace('th', '')\n    return text\n    \ndef apply_preprocess(df):\n    new_df = df.copy()\n    new_df['Comment'] = new_df['Comment'].map(text_normalize)\n    new_df['Reviews'] = new_df['Reviews'].map(text_normalize)\n    new_df['Reviews'] = new_df['Reviews'].map(text_preprocess)\n    new_df['Reviews'] = new_df['Reviews'].map(text_lemmatization)\n    return new_df\n\nbefore_preprocess_data = data.copy()\ndata = apply_preprocess(data)","f644e973":"before_preprocess_data.head(3)","dc7ac61c":"data.head(3)","79602502":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer","1f858f8e":"def run_train(fold, data_df, tr_idx, val_idx, clf, config):\n    print(f'     ----- Fold: {fold} -----')\n    count_vect = CountVectorizer()\n    tfidf_transformer = TfidfTransformer()\n    \n    train, test = data_df.iloc[tr_idx], data_df.iloc[val_idx]\n    \n    X_train, y_train = np.array(train['Reviews']), np.array(train['Ratings'])\n    X_test, y_test = np.array(test['Reviews']), np.array(test['Ratings'])\n\n    X_train_counts = count_vect.fit_transform(X_train)\n    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n\n    clf.fit(X_train_tfidf, y_train)\n\n    X_test_counts = count_vect.transform(X_test)\n    pred = clf.predict(X_test_counts)\n    return y_test, pred","f1c3694a":"def run_all(data_df, clf, config):\n    sss = StratifiedShuffleSplit(\n        n_splits=config.n_folds, \n        test_size=config.test_size, \n        random_state=config.seed\n    )\n    oofs_acc = 0.0\n    oofs_qwk = 0.0\n    for i, (tr_idx, val_idx) in enumerate(sss.split(data_df, data_df['Ratings'])):\n        y_test, pred = run_train(i, data_df, tr_idx, val_idx, clf, config)\n        acc = accuracy_score(y_test, pred)\n        qwk = cohen_kappa_score(y_test, pred, weights='quadratic')\n        oofs_acc += acc\n        oofs_qwk += qwk\n        print(f'FOLD{i} Accuracy SCORE: {acc:.5f}')\n        print(f'FOLD{i} Quadratic Weighted Kappa SCORE: {qwk:.5f}')\n\n    print()\n    print(f'{config.n_folds}FOLDS Accuracy SCORE: {oofs_acc\/config.n_folds:.5f}')\n    print(f'{config.n_folds}FOLDS Quadratic Weighted Kappa SCORE: {oofs_qwk\/config.n_folds:.5f}')","90bb332b":"nb_clf = MultinomialNB(\n    alpha=0.4\n)\nrun_all(data, nb_clf, Cfg)","4683dfcb":"mlp_clf = MLPClassifier(\n    hidden_layer_sizes=(256), \n    max_iter=300,\n    random_state=Cfg.seed,\n    early_stopping=True\n)\n# calibrate the model preds\ncalib_mlp_clf = CalibratedClassifierCV(\n    mlp_clf, \n    method='isotonic', \n    cv=4\n)\nrun_all(data, calib_mlp_clf, Cfg)","3f43869c":"# without calibration\n# 5FOLDS Accuracy SCORE: 0.71024\n# 5FOLDS Quadratic Weighted Kappa SCORE: 0.60362","aaf5ad92":"sgd_clf = SGDClassifier(\n    penalty='elasticnet', \n    n_jobs=-1, \n    random_state=Cfg.seed\n)\n# calibrate the model preds\ncalib_sgd_clf = CalibratedClassifierCV(\n    sgd_clf, \n    method='isotonic', \n    cv=4\n)\nrun_all(data, calib_sgd_clf, Cfg)","604f9e91":"# without calibration\n# 5FOLDS Accuracy SCORE: 0.71498\n# 5FOLDS Quadratic Weighted Kappa SCORE: 0.63602","3eb3728e":"### SGDClassifier with Calibration","d6e0c1e3":"### Comment","897d9bdf":"### Model Training","921ebcd3":"## Thanks for reading!!","a243976b":"### Preprocessing\n\nAfter normalization, remove words that commonly occur in the text, unimportant features,\n\nand apply stopwords and lemmatization.\n\nLemmatization is similar to word stemming, but the algorithm is different.\n\nhttps:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/stemming-and-lemmatization-1.html","dc4c6436":"Metrics: `Quadratic Weighted Kappa`\n\nSince the objective variable is rating, we'll use it. \n\n<br>\n\nA great notebook that explains Quadratic Weighted Kappa.\n\nhttps:\/\/www.kaggle.com\/aroraaman\/quadratic-kappa-metric-explained-in-5-simple-steps\/notebook\n\n<br>\n\nCV: `StratifiedShuffleSplit`\n\nUnlike StratifiedKFold, the data will be shuffled for each fold.\n\nStratifiedKFold does'nt allow duplication of data per fold, \n\nbut StratifiedShuffleSplit may be duplicated.","0a4a27af":"### MLP Model with Calibration","8b3acd40":"## Ratings Prediction","18b571f1":"`Ratings`: Label (objective variables)\n\n`Reviews`: Input Data\n\n`Comment`: Not Use","0e9911db":"### Ratings","114d7d9a":"## Check The Dataset","53a95863":"# Apple iPhone SE Ratings Prediction\n\nThis notebook challenges the task of predicting user ratings using an interesting dataset published by [Kamal Das](https:\/\/www.kaggle.com\/kmldas).","5b58af9a":"### Reviews","5be5facf":"### Multinomial Naive Bayes Model"}}