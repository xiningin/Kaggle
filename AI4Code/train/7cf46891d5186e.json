{"cell_type":{"202ecb83":"code","cb72c9a8":"code","dfec3872":"code","1c07568f":"code","19e45aa8":"code","9bafa1ea":"code","1c21715b":"code","03252189":"code","8787974c":"code","58d0edaf":"code","591237ba":"code","6b778dc5":"code","2bf44a75":"code","44a22354":"code","b20454ba":"code","cb3e56b4":"code","0bb5e265":"code","8c2a99f2":"code","0bb37abb":"code","2975bb6b":"markdown","95595f2a":"markdown","7157954e":"markdown","ea879671":"markdown","d6d616bd":"markdown","d8979aae":"markdown"},"source":{"202ecb83":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nfrom __future__ import unicode_literals\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nfrom collections import Counter\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport nltk\n# NLTK Stop words\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nsid = SentimentIntensityAnalyzer()\n\nstop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n\n\n# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel, HdpModel\n\nimport spacy\ntry:\n    from spacymoji import Emoji\nexcept:\n    !pip install spacymoji\n    from spacymoji import Emoji\n\nnlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\nemoji = Emoji(nlp, merge_spans=False)\nnlp.add_pipe(emoji, first=True)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n#pd.options.plotting.backend = \"plotly\"\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim\npyLDAvis.enable_notebook()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cb72c9a8":"os.makedirs('.\/Models')\nos.makedirs('.\/Dictionary')","dfec3872":"## Time Features\n\ndef get_timefeatures(df: pd.DataFrame, col:str):\n    from pandas.api.types import is_datetime64_ns_dtype as is_datetime\n    from pandas.api.types import is_float_dtype as is_float\n    \n    if is_datetime(df[col]):\n        df[col+'_date'] = df[col].dt.date\n        df[col+'_doy'] = df[col].dt.day_of_year\n        df[col+'_dow'] = df[col].dt.day_of_week\n        df[col+'_time'] = df[col].dt.time\n        df[col+'_hour'] = df[col].dt.hour\n        df[col+'_week'] = df[col].dt.week\n        df[col+'_day'] = df[col+'_doy'] - df[col+'_doy'].min()\n        df[col+'_floorT'] = df[col].dt.floor('Min')\n        df[col+'_floorH'] = df[col].dt.floor('H')\n        return [col,col+'_date',col+'_doy', col+'_dow',\n                col+'_time',col+'_hour',col+'_week',col+'_day', col+'_floorT', col+'_floorH']\n    elif is_float(df[col]):\n        df[col+'_age'] = df[col].max() - df[col]\n        return [col, col+'_age']\n    \n## Feature Engineering\n\ndef remove_outliers(df: pd.DataFrame):\n    try:\n        return df[df.timestamp_doy<273]\n    except:\n        print('It is not possible to filter on day of year column.')\n        return df\ndef VaderSentiment(text: str):\n        d = sid.polarity_scores(text)\n        return d['pos'],d['neu'],d['neg'],d['compound']\ndef NumbersFromText(txt: str):\n    try:\n        doc = nlp(txt)\n        words = [token.text for token in doc if token.is_alpha]\n        emojis = [token.text for token in doc if token._.is_emoji]\n        return len(words), len(emojis)\n    except:\n        return 0,0\n\ndef get_TextFeatures(df: pd.DataFrame):\n    df['full_text'] = df['title'] + ' ' + df['body'].fillna('')\n    df['text_lenght'] = df['full_text'].apply(lambda x: len(x))\n    p = df.full_text.transform(VaderSentiment).to_list()\n    df[['VS_pos','VS_neu','VS_neg','compound']] = pd.DataFrame(p, columns=['VS_pos','VS_neu','VS_neg','compound'])\n    df['sentiment'] = df['compound'].apply(lambda c: 'pos' if c>0.1 else 'neg' if c<-0.1 else 'neu')\n    #df[['num_of_words','num_of_emojis']] = pd.DataFrame(df.title.apply(lambda x: NumbersFromText(x)).to_list(), columns=['num_of_words','num_of_emojis'])\n\ndef extract_emoji(sent):\n    doc = nlp(sent)\n    res = ' '.join([token.text for token in doc if token._.is_emoji])\n    return res.strip()\n\n## Text Preprocessing\n\ndef remove_url(text):\n    return re.sub(r'^https?:\\\/\\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\ndef remove_newline(text):\n    return re.sub('\\s+', ' ', text)\ndef remove_squote(text):\n    return re.sub(\"\\'\", \"\", text)\n\ndef sent_to_words(texts):\n    texts = [remove_url(text) for text in texts]\n    texts = [remove_newline(text) for text in texts]\n    texts = [remove_squote(text) for text in texts]\n    for sent in texts:\n        yield([token.lemma_.lower() for token in nlp(sent) if (token._.is_emoji)|(token.is_alpha)|(token.is_digit)])\n#    for sent in texts:\n#        yield(gensim.utils.simple_preprocess(str(sent), deacc=True))\n\ndef remove_stopwords(texts):\n    return [[word for word in text if word not in stop_words] for text in texts]\n#[[word for word in doc if word not in stop_words] for doc in texts]\n\n\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV','PROPN'], is_emoji=True):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_out = []\n    item = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if (token.pos_ in allowed_postags)|(is_emoji&token._.is_emoji)])\n    return texts_out\n\n# equivalent to build_corpus but optimized with spacy\ndef preprocess(texts: pd.Series, allowed_postags:list):\n    docs = list(nlp.pipe(texts))\n    data_words = [[token.lemma_.lower() for token in doc if (\n        (token._.is_emoji)|(token.is_alpha)|(token.is_digit)|(token.is_stop==False)\n    )&(token.pos_ in allowed_postags)]for doc in docs]\n    #data_words = remove_stopwords(data_words)\n    return data_words\n    \ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n    \n\ndef build_corpus(texts:pd.Series, dictionary, allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV','PROPN']):\n    docs = list(nlp.pipe(texts))\n    data_words = [[token.lemma_.lower() for token in doc if (\n        (token._.is_emoji)|(token.is_alpha)|(token.is_digit)|(token.is_stop==False)\n    )&(token.pos_ in allowed_postags)]for doc in docs]\n    \n    data_words = make_bigrams(data_words)\n    corpus = [id2word.doc2bow(text) for text in data_words]\n    return corpus\n\n# outdated function?\n# def build_corpus(texts:pd.Series, dictionary):\n#    data_words = list(sent_to_words(texts))\n#    data_words = remove_stopwords(data_words)\n#    data_words = make_bigrams(data_words)\n#    data_words = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n#    corpus = [id2word.doc2bow(text) for text in lemmas]\n#    return corpus","1c07568f":"def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n    \"\"\"\n    Compute c_v coherence for various number of topics\n\n    Parameters:\n    ----------\n    dictionary : Gensim dictionary\n    corpus : Gensim corpus\n    texts : List of input texts\n    limit : Max num of topics\n\n    Returns:\n    -------\n    model_list : List of LDA topic models\n    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n    \"\"\"\n    coherence_values = []\n    model_list = []\n    for num_topics in range(start, limit, step):\n        model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=num_topics,\n                                               alpha='auto',\n                                               eta='auto')#learn asymmetric priors\n        model_list.append(model)\n        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n        coherence_values.append(coherencemodel.get_coherence())\n\n    return model_list, coherence_values","19e45aa8":"path = '\/kaggle\/input\/reddit-wallstreetsbets-posts\/reddit_wsb.csv'\ndf = pd.read_csv(path, parse_dates = ['timestamp'], infer_datetime_format=True)","9bafa1ea":"%%time\nimport gc\n\ngc.collect()\n\nget_TextFeatures(df)\nget_timefeatures(df,'timestamp')\ndf = remove_outliers(df)","1c21715b":"%%time\n## Let's build text corpus\ntexts = df.full_text.sample(frac=1).dropna().unique()\ndata_words = preprocess(texts, ['PROPN','NOUN', 'ADJ', 'VERB', 'ADV'])\n## Build the bigram and trigram models\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n#\n## Faster way to get a sentence clubbed as a trigram\/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\ndata_words = make_bigrams(data_words)\n","03252189":"### Let's build text corpus\n#texts = df.full_text.sample(frac=1).dropna().unique()\n#data_words = list(sent_to_words(texts))\n#\n## Build the bigram and trigram models\n#bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n#trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n#\n## Faster way to get a sentence clubbed as a trigram\/bigram\n#bigram_mod = gensim.models.phrases.Phraser(bigram)\n#trigram_mod = gensim.models.phrases.Phraser(trigram)\n#\n## Remove Stop Words\n#data_words_nostops = remove_stopwords(data_words)\n#\n## Form Bigrams\n#data_words_bigrams = make_bigrams(data_words_nostops)\n#\n#data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['PROPN','NOUN', 'ADJ', 'VERB', 'ADV'])\n","8787974c":"%%time\ngc.collect()\n\n# Create Dictionary\nid2word = corpora.Dictionary(data_words)\nid2word.filter_extremes(no_below = 10,no_above=0.25)\n# Create Corpus\nlemmas = data_words\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in data_words]","58d0edaf":"%%time\nlimit=25; start=3; step=3;\nmodel_list, coherence_values = compute_coherence_values(dictionary=id2word, \n                                                        corpus=corpus, \n                                                        texts=data_words, \n                                                        start=start, limit=limit, step=step)\n# Show graph\nx = range(start, limit, step)\nplt.plot(x, coherence_values)\nplt.xlabel(\"Num Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend((\"coherence_values\"), loc='best')\nplt.show()\n\n# Print the coherence scores\nfor m, cv in zip(x, coherence_values):\n    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))","591237ba":"# Select the model and print the topics\noptimal_model = model_list[1]#np.argmax(coherence_values)]","6b778dc5":"def format_topics_sentences(ldamodel, corpus=corpus, texts=texts):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row in enumerate(ldamodel[corpus]):\n        row = sorted(row, key=lambda x: x[1], reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num, topn=7)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=texts)\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n\n# Show\ndf_dominant_topic.head(10)","2bf44a75":"# Group top 5 sentences under each topic\nsent_topics_sorteddf = pd.DataFrame()\n\nsent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n\nfor i, grp in sent_topics_outdf_grpd:\n    sent_topics_sorteddf = pd.concat([sent_topics_sorteddf,\n                                      grp.sort_values(['Perc_Contribution'], \n                                                      ascending=[0]).head(1)], \n                                            axis=0)\n\n# Reset Index    \nsent_topics_sorteddf.reset_index(drop=True, inplace=True)\n\n# Format\nsent_topics_sorteddf.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n\n# Show\nsent_topics_sorteddf","44a22354":"# Number of Documents for Each Topic\ntopic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n\n# Percentage of Documents for Each Topic\ntopic_contribution = round(topic_counts\/topic_counts.sum(), 4)\n\n# Topic Number and Keywords\ntopic_num_keywords = df_topic_sents_keywords[\n    ['Dominant_Topic', 'Topic_Keywords']\n].drop_duplicates()\n\n# Concatenate Column wise\ndf_dominant_topics = pd.concat([topic_num_keywords.sort_values(by='Dominant_Topic').reset_index() , \n                                topic_counts.sort_index()\n                                , topic_contribution.sort_index()\n                               ], axis=1).drop(columns='index')\n\n# Change Column names\ndf_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n\n# Show\ndf_dominant_topics.sort_values(by='Num_Documents', ascending=False)","b20454ba":"new_corpus = build_corpus(df.full_text, dictionary=id2word)\ntopics_docs= list(optimal_model.get_document_topics(new_corpus, minimum_probability=0.0))\nlda_topics = [sorted(topics, key=lambda x: x[1], reverse=True)[0] for topics in topics_docs]\ndf = pd.concat([df,pd.DataFrame(lda_topics, columns=['lda_topic','lda_topic_prob'])], axis = 1)\ndel topics_docs, lda_topics","cb3e56b4":"for num in range(optimal_model.num_topics):\n    print('\\n')\n    print('###### Topic no.',num)\n    print('\\n')\n    for i,row in df[(df.lda_topic==num)&(df.lda_topic_prob)>0.95].sort_values(by='lda_topic_prob').head(1).iterrows():\n        print('At: ', row.timestamp)\n        print('Text: ',row.full_text)\n        print('Url :', row.url)\n        print('DataFrame index: ', i)\n        print('\\n')","0bb5e265":"### Store dictionary and lda model\n\nfrom gensim.test.utils import datapath\n\noptimal_model.save('.\/Models\/lda_model')\n\nid2word.save('.\/Dictionary\/dictionary')","8c2a99f2":"df.to_csv('.\/processes_wsb_data.csv')","0bb37abb":"bigram_mod.save('.\/Models\/bigram_mod')\ntrigram_mod.save('.\/Models\/trigram_mod')\nbigram.save('.\/bigram')\ntrigram.save('.\/trigram')","2975bb6b":"### Apply LDA model to all texts","95595f2a":"## Topic Interpretability","7157954e":"# References\n\n## Other Notebooks by Radema \n* [YOLO - Explorative analysis on WallStreetBets, by Radema](https:\/\/www.kaggle.com\/radema\/yolo-explorative-analysis-on-wallstreetbets)\n\n## External Reference\n\n* [NLTK Documentation](https:\/\/www.nltk.org\/)\n* [Gensim Documentation](https:\/\/radimrehurek.com\/gensim\/auto_examples\/index.html)\n* [Spacy Usage - Linguistic Features](https:\/\/spacy.io\/usage\/linguistic-features)\n* [Universal POS Tags](https:\/\/universaldependencies.org\/docs\/u\/pos\/)\n* [*Topic Modeling with Gensim* by Selva Prabhakaran](https:\/\/www.machinelearningplus.com\/nlp\/topic-modeling-gensim-python\/)","ea879671":"## LDA Topic Modeling","d6d616bd":"The dataset has a total of 8 columns. From [an official page on GitHub of the API Wrapper](https:\/\/github.com\/reddit-archive\/reddit\/wiki\/JSON) we have the following:\n- **title**: the title of the link. may contain newlines for some reason\n- **score**:the net-score of the link. note: A submission's score is simply the number of upvotes minus the number of downvotes. If five users like the submission and three users don't it will have a score of 2. Please note that the vote numbers are not \"real\" numbers, they have been \"fuzzed\" to prevent spam bots etc. So taking the above example, if five users upvoted the submission, and three users downvote it, the upvote\/downvote numbers may say 23 upvotes and 21 downvotes, or 12 upvotes, and 10 downvotes. The points score is correct, but the vote totals are \"fuzzed\".\n- **id**: this item's identifier, e.g. \"8xwlg\"\n- **url**: the link of this post. the permalink if this is a self-post\n- **comms_num**: the number of comments that belong to this link. includes removed comments.\n- **created**: the time of creation in local epoch-second format\n- **body**:  the raw text. this is the unformatted text which includes the raw markup characters such as ** for bold. <, >, and & are escaped.\n- **timestamp**: datetime about the related activity ","d8979aae":"# Topic Modeling\n\nIn this notebook I host the topic modeling part previously coded [here](https:\/\/www.kaggle.com\/radema\/yolo-explorative-analysis-on-wallstreetbets). I decided to split the notebooks to improve readability and be able to separate the different subtask (topic modeling, feature engineering, data cleansing, etc. etc.). For a detailed theory about LDA and Topic Modeling I suggest the links in the Reference Section."}}