{"cell_type":{"470cd50c":"code","6bb7d1af":"code","3670993f":"code","f33d93b5":"code","ef20460c":"code","62c9fbc4":"code","1cb63d3c":"code","1d331a85":"code","c54b5595":"code","1556cc03":"code","32d602f8":"code","2b463cc2":"code","8f637cff":"code","fa3ba17e":"code","60fb336f":"code","adf4836b":"code","68711f9c":"code","3b9703f7":"code","16af55aa":"code","9931cf90":"code","bb1978de":"code","60063a07":"markdown","b3426aec":"markdown","aa2f9583":"markdown","df94b405":"markdown","3126c0b9":"markdown","0bc99db0":"markdown","8270b582":"markdown","287c28a2":"markdown","321e8664":"markdown","4db32b8a":"markdown","1d0f467f":"markdown","915356d7":"markdown","93874771":"markdown"},"source":{"470cd50c":"# Importa\u00e7\u00e3o das bibliotecas principais.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom pandas_profiling import ProfileReport\n\n# define o estilo de gr\u00e1fico para o matplotlib\nplt.style.use('ggplot')","6bb7d1af":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3670993f":"# Dados de treino e de teste oferecidos na atividade.\n\ntrain = pd.read_csv('\/kaggle\/input\/usp-pj01\/train_Iris.csv')\ntest = pd.read_csv('\/kaggle\/input\/usp-pj01\/test_Iris.csv')","f33d93b5":"# Observa\u00e7\u00e3o de uma amostra do conjunto de treino.\ntrain.sample(10)","ef20460c":"# Dimens\u00f5es do conjunto de treino\ntrain.shape","62c9fbc4":"from pandas_profiling import ProfileReport\n# Profile report das colunas relacionadas aos features e \u00e0 vari\u00e1vel avo categ\u00f3rica (esp\u00e9cie)\nprof = ProfileReport(train.iloc[:,1:5])\n#prof.to_file(output_file='output.html')\nprof","1cb63d3c":"plt.figure(figsize=(7, 7))\nsns.kdeplot(data=train, x = 'SepalLengthCm', hue = 'Species', palette = 'viridis')\nplt.title(\"Distribui\u00e7\u00e3o de frequ\u00eancias de SepalLengthCm\")\nplt.show(True)\n\nplt.figure(figsize=(7, 7))\nsns.kdeplot(data=train, x = 'SepalWidthCm', hue = 'Species', palette = 'viridis')\nplt.title(\"Distribui\u00e7\u00e3o de frequ\u00eancias de SepalWidthCm\")\nplt.show(True)\n\nplt.figure(figsize=(7, 7))\nsns.kdeplot(data=train, x = 'PetalLengthCm', hue = 'Species', palette = 'viridis')\nplt.title(\"Distribui\u00e7\u00e3o de frequ\u00eancias de PetalLegthCm\")\nplt.show(True)\n\nplt.figure(figsize=(7, 7))\nsns.kdeplot(data=train, x = 'PetalWidthCm', hue = 'Species', palette = 'viridis')\nplt.title(\"Distribui\u00e7\u00e3o de frequ\u00eancias de PetalWidthCm\")\nplt.show(True)","1d331a85":"plt.figure()\nsns.pairplot(train.iloc[:,1:6], hue=\"Species\", palette = \"viridis\")\nplt.show()","c54b5595":"# PCA de 2 componentes\n\npca = PCA(n_components=2)\n\ndf = train.copy(deep=True)\n\nX = np.matrix(df.iloc[:,1:5])\npca.fit(X)\n\nprint(np.round(pca.explained_variance_ratio_,10))\n\nPCA1 = pca.transform(X)[:,0]\nPCA2 = pca.transform(X)[:,1]\ndf[\"Component 1\"] = PCA1\ndf[\"Component 2\"] = PCA2\n\ndf.head()","1556cc03":"plt.figure(figsize=(10,7))\nsns.scatterplot(data=df, x=\"Component 1\", y=\"Component 2\", hue=\"Species\", palette = 'viridis')","32d602f8":"df.head()","2b463cc2":"train.head()","8f637cff":"# Dados de treino e de teste oferecidos na atividade.\n\ntrain = pd.read_csv('\/kaggle\/input\/usp-pj01\/train_Iris.csv')\ntest = pd.read_csv('\/kaggle\/input\/usp-pj01\/test_Iris.csv')\n\n# elimina\u00e7\u00e3o da coluna de identifica\u00e7\u00e3o para a classifica\u00e7\u00e3o.\ntrain = train.drop(train.columns[0], axis=1) \n\n# Nomes das classes e nome  dos features\nclasses = np.array(pd.unique(train[train.columns[-1]]), dtype=str)  #nome das classes \nfeatures = train.columns # features envolvidos\nprint(classes)\nprint(features)","fa3ba17e":"# transforma\u00e7\u00e3o para matriz numpy para facilitar a manipula\u00e7\u00e3o e aumentar performance\n\ntrain = train.to_numpy()\nnrow,ncol = train.shape\ny = train[:,-1] # vari\u00e1vel alvo (esp\u00e9cie)\nX = train[:,0:ncol-1] # vari\u00e1veis de treino","60fb336f":"# normaliza\u00e7\u00e3o dos dados para evitar efeitos de escala\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler().fit(X)\nX = scaler.transform(X)","adf4836b":"X","68711f9c":"# divis\u00e3o em conjuntos de dados de treino e de teste.\n\nfrom sklearn.model_selection import train_test_split\np = 0.25 # fracao de elementos no conjunto de teste\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = p, random_state = 19)","3b9703f7":"# treino de uma \u00e1rvore de decis\u00e3o pelo m\u00e9todo de gini para o problema.\n\nfrom sklearn import tree\n\nmodel = tree.DecisionTreeClassifier(criterion = 'gini', random_state = 2112)\nmodel.fit(x_train,y_train)\n\ny_pred = model.predict(x_test) ","16af55aa":"# avalia\u00e7\u00e3o da acur\u00e1cia\n\nfrom sklearn.metrics import accuracy_score\nscore = accuracy_score(y_pred, y_test)\nprint('Accuracy:', np.round(score,10))","9931cf90":"# Agora utilizemos o modelo Random forest (sem valida\u00e7\u00e3o cruzada ou gridsearch)\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel=RandomForestClassifier() # utiliando-se todos os atributos como default\nmodel.fit(x_train,y_train)\n\ny_pred = model.predict(x_test)\nscore = accuracy_score(y_pred, y_test)\nprint('Accuracy:', np.round(score,10))","bb1978de":"importancias = model.feature_importances_\nprint(importancias)\nindices = np.argsort(importancias)\nprint(indices)\naux = []\nfor i in indices:\n    aux.append(features[i])\n    \nprint(aux)    \n\nplt.figure(figsize=(12,8))\nplt.barh(range(len(indices)), importancias[indices], color='chocolate', align='center')\nplt.yticks(range(len(indices)), aux, fontsize=15)\nplt.xlabel('Import\u00e2ncia Relativa',fontsize=20)\nplt.xticks(color='k', size=20)\nplt.yticks(color='k', size=20)\nplt.show()","60063a07":"## Classifica\u00e7\u00e3o","b3426aec":"V\u00e1rias conclus\u00f5es interessantes podem ser tiradas desta simples an\u00e1lise explorat\u00f3ria. Mas alguns pontos espec\u00edficos s\u00e3o not\u00e1veis e merecem destaque:\n* Existe uma forte correla\u00e7\u00e3o positiva entre a ocorr\u00eancia dos valores de PetalLengthCm e PetalWidthCm, independente da esp\u00e9cie em quest\u00e3o. Tal fato foi destacado no Profiling realizado no in\u00edcio desta se\u00e7\u00e3o e tamb\u00e9m pode ser observado no pairplot. Isso indica que uma das features est\u00e1 explicando, em parte, a variabilidade de outra, fat que pode ser considerado caso seja necess\u00e1rio um estudo de features para classifica\u00e7\u00e3o mais robusto.\n* A esp\u00e9cie Iris-setosa muitas vezes se destaca nas an\u00e1lises de atributos. Isso \u00e9 not\u00f3rio principalmente na an\u00e1lise das vari\u00e1veis PetalLengthCm e PetalWidthCm, na qual a distribui\u00e7\u00e3o de frequ\u00eancias para setosa concentra-se em valores menores quando comparada com a dsitribui\u00e7\u00e3o dos mesmos features para a virginica e versicolor.\n* Em an\u00e1lises bivariadas, o cluster que compreende as observa\u00e7\u00f5es para a esp\u00e9cie setosa mostra-se relativamente afastado dos clusteres para as outras duas esp\u00e9cies, indicando uma mais f\u00e1cil identifica\u00e7\u00e3o de observa\u00e7\u00f5es da esp\u00e9cie setosa. J\u00e1 para as observa\u00e7\u00f5es das esp\u00e9cies versicolor e visginica, a separa\u00e7\u00e3o de dso clusteres j\u00e1 n\u00e3o \u00e9 t\u00e3o clara.","aa2f9583":"Agora que temos informa\u00e7\u00f5es quantitativas b\u00e1sicas de como o espa\u00e7o multivariado de observa\u00e7\u00f5es est\u00e1 disposto, \u00e9 interessante criar gr\u00e1ficos espec\u00edficos de compara\u00e7\u00e3o para ter-se ideia de como as vari\u00e1veis se comportam dependendo da esp\u00e9cie de Iris em quest\u00e3o.","df94b405":"Enfim, para complementar o estudo explorat\u00f3rio, fa\u00e7amos um gr\u00e1fico pairplot com os comportamentos bivariados dos 4 features que ser\u00e3o utilizados na classifica\u00e7\u00e3o.","3126c0b9":"No gr\u00e1fico bidimensional podemos perceber que existe uma forte separa\u00e7\u00e3o entre as observa\u00e7\u00f5es para cada esp\u00e9cie,originando clusteres relativamente bem definidos no espa\u00e7o. Essa observa\u00e7\u00e3o, conjuntamente com as an\u00e1lises anteriores, cria a suspeita de que um classificador simples, como uma \u00e1rvore de decis\u00e3o ou um classificador bayesiano, seja capaz de identificar as regi\u00f5es de separa\u00e7\u00e3o para este problema.","0bc99db0":"Aqui iremos testar 2 classificadores: uma \u00e1rvore de decis\u00e3o simples e um classificador random forest. Perceba que os dois m\u00e9todos s\u00e3o baseados em \u00e1rvores de decis\u00e3o, por\u00e9m, o random forest \u00e9 um ensemble de \u00e1rvores aleat\u00f3rias.","8270b582":"As duas primeiras componentes princiais explicam quase a totalidade da variabilidade dos dados (<97%), o que indica, novamente, correla\u00e7\u00e3o entre as vari\u00e1veis de interesse. Vejamos como as observa\u00e7\u00f5es st\u00e3o dispostas no espa\u00e7o bidimensional composto pelas componentes.","287c28a2":"\u00c9 not\u00f3rio que a dimens\u00e3o do conjunto de dados de treino \u00e9 extremamente pequena (120 observa\u00e7\u00f5es e 6 atributos, sendo 1 de identifica\u00e7\u00e3o, 4 de covari\u00e1veis e 1 vari\u00e1vel alvo). Assim sendo, podemos gerar um resumo explorat\u00f3rio utilizando-se do [pandas-profiling](https:\/\/towardsdatascience.com\/exploratory-data-analysis-with-pandas-profiling-de3aae2ddff3), o qual \u00e9 recomendado quando o dataframe de interesse n\u00e3o tem dimens\u00f5es muito elevadas.","321e8664":"O conjunto de dados [Iris de Fisher](https:\/\/pt.wikipedia.org\/wiki\/Conjunto_de_dados_flor_Iris) \u00e9 um dos conjuntos multivariados mais famosos para o aprendizado de an\u00e1lise explorat\u00f3ria com ferramentas computacionais, aplica\u00e7\u00e3o de t\u00e9cnicas multivariadas e de t\u00e9cnicas b\u00e1sicas de aprendizdo de m\u00e1quina para classifica\u00e7\u00e3o. Aqui, desenvolve-se uma an\u00e1lise simples seguida de uma proposta para um modelo de classifica\u00e7\u00e3o para os tipos difetentes de Iris. Tal exerc\u00edcio foi proposto no oferecimento da disciplina SME0829 - Aprendizado de M\u00e1quina no primeiro semestre de 2021, pelo Instituto de Ci\u00eancias Matem\u00e1ticas e de Computa\u00e7\u00e3o. A atividade pode ser consultada no [kaggle](https:\/\/www.kaggle.com\/c\/usp-pj01\/overview).","4db32b8a":"# Um classificador b\u00e1sico para o estudo do conjunto de dados Iris.","1d0f467f":"\u00c9 interessante notar como as vari\u00e1veis PetalLengthCm e PetalWidthCm foram os features com maior import\u00e2ncia relativa para a classifica\u00e7\u00e3o. Isso corrobora com as conclus\u00f5es da an\u00e1lise explorat\u00f3ria de dados, onde foi poss\u00edvel verificar que tais vari\u00e1veis possuem limites de separa\u00e7\u00e3o entre as esp\u00e9cies bem mais claros do que as outras duas features de classifica\u00e7\u00e3o. Inclusive, para o SepalWidthCm \u00e9 poss\u00edvel verificar que a distribui\u00e7\u00e3o de frequ\u00eancias de observa\u00e7\u00e3o para cada esp\u00e9cie se sobrep\u00f5em muito mais, corroborando para que este seja o feature que tem menos import\u00eancia relativa para a classifica\u00e7\u00e3o. \n<br>\nO classificador com maior acur\u00e1cia foi o baseado em Random Forest, mas vale lembrar que o conjunto de dados \u00e9 extremamnete pequeno e a amostra de teste, por consequ\u00eancia tamb\u00e9m, o que pode indicar um caso de de vi\u00e9s para o classificador.","915356d7":"Realizaemos uma redu\u00e7\u00e3o de dimensionalidade simples utilizando-se de An\u00e1lise de Componentes Principais para visualizar melhor o espa\u00e7o multivariado no qual os dados est\u00e3o dsipostos. Como existe correla\u00e7\u00e3o entre os features, \u00e9 esperado que consigamos explicar em grande parte a variabilidade dos dados com menos vari\u00e1veis.","93874771":"## An\u00e1lise Explorat\u00f3ria de Dados"}}