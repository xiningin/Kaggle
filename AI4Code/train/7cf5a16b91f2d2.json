{"cell_type":{"31d6a131":"code","3b023bce":"code","3077ee64":"code","37fa353b":"code","c1f8b593":"code","d3245a63":"code","85426815":"code","162ae6a7":"code","116d15cc":"code","45ec5061":"code","0ef779cd":"code","bd3a794d":"code","2db87b1b":"code","60386880":"code","8fbc50bf":"code","9be59504":"code","7ca3843c":"code","904a8d1f":"code","90d3e379":"code","bb1b2b76":"code","1102abe2":"code","991477f7":"code","f07a06a1":"code","63c1d892":"code","50d0c7a9":"code","914735be":"code","bb96dcd8":"code","2011dd2b":"code","acef5eca":"code","6eabf732":"code","d507344c":"code","f9d20acb":"code","4e64c696":"code","a6a68649":"code","a5735a7c":"code","b9639bf3":"code","7bd37d3e":"code","27f1b41f":"code","9782ecce":"code","d4f69a7e":"code","6e93f516":"code","cb68f548":"code","e0d6950b":"code","f0b80814":"code","051dfd3b":"code","b165c20a":"code","5d7ee9ab":"code","055b4bb4":"code","f040761c":"code","57f84c0f":"code","406f1e20":"code","8ed77539":"code","be3fd46a":"code","b11a556c":"code","3b0920f1":"code","8f4f353f":"code","df3dd808":"code","af091ae0":"code","89f8aed7":"code","abde4119":"code","a2e628cd":"code","ff7e6878":"code","97e90628":"code","dc408664":"code","7dade6b1":"code","e2cebbf9":"code","88bada66":"code","6bb0d911":"code","f87cade4":"code","92686ed8":"code","f9ef60ce":"code","f45d4a3b":"code","6a2377cf":"code","0ede96be":"code","f83dd0a5":"code","77927354":"code","dab322bf":"markdown","1d0f9bf5":"markdown","95396d66":"markdown","30e0196d":"markdown","99058b02":"markdown","847aa000":"markdown","aeeb7859":"markdown","9bf1efbd":"markdown","96fab6ce":"markdown","2c22d1a6":"markdown","5b02b66b":"markdown","8994d7e4":"markdown","35eb973f":"markdown","101a4f6a":"markdown","e04268a9":"markdown","6deb35fe":"markdown","3e04900e":"markdown","c58f0872":"markdown","9cb73e7a":"markdown","5793f61d":"markdown","339c8445":"markdown","141442bc":"markdown","9794049a":"markdown","d555a184":"markdown","0cea6a63":"markdown","a6149441":"markdown","02b85bec":"markdown","f47923c5":"markdown","977a75a0":"markdown","a48acc01":"markdown","18df9c93":"markdown","47cb115f":"markdown","0e2de7d2":"markdown","b892e0e1":"markdown","0aa728f4":"markdown","f6f28d2e":"markdown","1fe8d632":"markdown","ecce0e9b":"markdown","e3a9ee9a":"markdown","50dceb05":"markdown","06ba5c8d":"markdown","898ab160":"markdown","8698f96d":"markdown"},"source":{"31d6a131":"#Load libraries needed for the solution\n%matplotlib inline\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","3b023bce":"def metadata_summary(file_path):\n    ''' Function to summarise metadata file. The file has components of meta data seperated by _ in code and ; in description.\n    It returns unique combinations of code and description part. This will help in grouping and extracting only needed columns\n    from data.\n    Input - Metadata file path.\n    Output - Dataframe with code, desc and type unique combinations\n    '''\n    meta_df = pd.read_csv(file_path,names = ['code','desc'])\n    meta_code = meta_df['code'].str.split('_',expand = True)\n    meta_desc = meta_df['desc'].str.split(';',expand = True)\n    temp_df = pd.DataFrame(columns = ['code','desc','type'])\n    for cols in meta_code.columns:\n        t = pd.concat([meta_code[cols], meta_desc[cols]], axis=1)\n        t['type'] = 'type'+str(cols)\n        t.columns = ['code','desc','type']\n        temp_df = pd.concat([temp_df,t],axis=0)\n\n    temp_df.drop_duplicates(inplace = True)\n    temp_df.dropna(subset = ['code','desc'],inplace = True)\n    temp_df.reset_index(inplace = True, drop = True)\n    return(temp_df)","3077ee64":"def cleveland_plot(xmins, xmaxs, ylabs, chart_title, xmin_lab, xmax_lab,xtitle, ytitle, fsize=(10,10)):\n    ''' Plot a cleveland plot showing the xmin and xmax for a set of categorical variable'''\n    tot_recs = 10 if len(xmins) > 30 else len(xmins)\n    y_range = np.arange(1,tot_recs+1)\n\n    fig, ax = plt.subplots(figsize=fsize)\n    plt.hlines(y = y_range, xmin = xmins[:tot_recs], xmax = xmaxs[:tot_recs], \n               alpha=0.3, color='gray', linewidth = 2)\n    plt.scatter(x = xmins[:tot_recs],  y = y_range, color = 'green', label = xmin_lab, s =50, marker='o')\n    plt.scatter(x = xmaxs[:tot_recs],  y = y_range, color = 'red', label = xmax_lab, s = 50,marker = 'H')\n    plt.yticks(y_range, ylabs[:tot_recs],fontsize=12)\n    plt.ylabel(ytitle, fontsize = 14, color = 'gray')\n    plt.ylim(tot_recs+1,0)\n    plt.xticks(fontsize=12)\n    plt.xlabel(xtitle, fontsize = 14, color = 'gray')\n    plt.legend(fontsize = 12,loc='best')\n    plt.title(chart_title, fontsize = 18)\n    plt.grid(False)\n    ax.set_facecolor('white')\n    fig.set_facecolor('w')\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)","37fa353b":"base_path = '..\/input\/data-science-for-good\/cpe-data\/'\npolice_records_mn = pd.read_csv(base_path+'Dept_24-00098\/24-00098_Vehicle-Stops-data.csv',\n                            skiprows=[1])\npolice_records_mn.shape","c1f8b593":"police_records_mn.head()","d3245a63":"# Missing long\/lat will error out. Hence these are dropped.\n# create long\/lat point, set correct CRS it represented in then convert to common CRS used in this solution.\npolice_records_mn.dropna(subset = ['LOCATION_LONGITUDE','LOCATION_LATITUDE'],inplace = True)\npolice_records_mn['geometry'] = list(zip(police_records_mn['LOCATION_LONGITUDE'],police_records_mn['LOCATION_LATITUDE']))\npolice_records_mn['geometry'] = police_records_mn['geometry'].apply(Point)\npolice_records_mn = gpd.GeoDataFrame(police_records_mn,\n                                 geometry = 'geometry',\n                                 crs ={'init': 'epsg:4326'} )","85426815":"police_records_mn.info()","162ae6a7":"police_records_mn.crs","116d15cc":"police_records_mn.head()","45ec5061":"# to get total count by grid num\npr_summary_mn = pd.DataFrame(police_records_mn['LOCATION_DISTRICT'].value_counts()).reset_index()\npr_summary_mn.columns = ['gridnum', 'total_stops']","0ef779cd":"# get totals by gender of stoppages\nt = pd.DataFrame(police_records_mn.groupby(by = ['LOCATION_DISTRICT','SUBJECT_GENDER']).size()).reset_index()\nt.columns = ['gridnum','gender','stops']\nt = t.pivot_table(values = 'stops',index = 'gridnum', columns = 'gender', fill_value = 0).reset_index()\nt.rename(columns = {'No Data':'gender_missing', 'Female' : 'female_stops', 'Male':'male_stops'},inplace = True)\npr_summary_mn = pr_summary_mn.merge(t, how = 'left', on = 'gridnum')","bd3a794d":"# get totals by race of drivers\nt = pd.DataFrame(police_records_mn.groupby(by = ['LOCATION_DISTRICT','SUBJECT_RACE']).size()).reset_index()\nt.columns = ['gridnum','race','stops']\nt['race'] = t['race'].str.lower()\nt['race'] = t['race'].str.replace(' ','_')\nt = t.pivot_table(values = 'stops',index = 'gridnum', columns = 'race', fill_value = 0).reset_index()\nt['native_american'] = t['native_am'] + t['native_american']\nt['other'] = t['other'] + t['no_data']\nt.drop(columns = ['native_am','no_data'],inplace = True)\npr_summary_mn = pr_summary_mn.merge(t, how = 'left', on = 'gridnum')","2db87b1b":"# Calculate percentage columns\npr_summary_mn.sort_values(by = 'gridnum',inplace=True)\npr_summary_mn.reset_index(inplace = True,drop=True)\npr_summary_mn['female_stop_per'] = pr_summary_mn['female_stops']\/(pr_summary_mn['female_stops'] + pr_summary_mn['male_stops'])*100\npr_summary_mn['male_stop_per'] = pr_summary_mn['male_stops']\/(pr_summary_mn['female_stops'] + pr_summary_mn['male_stops'])*100\npr_summary_mn['asian_stop_per'] = pr_summary_mn['asian']\/pr_summary_mn['total_stops']*100\npr_summary_mn['black_stop_per'] = pr_summary_mn['black']\/pr_summary_mn['total_stops']*100\npr_summary_mn['latino_stop_per'] = pr_summary_mn['latino']\/pr_summary_mn['total_stops']*100\npr_summary_mn['native_american_stop_per'] = pr_summary_mn['native_american']\/pr_summary_mn['total_stops']*100\npr_summary_mn['other_stop_per'] = pr_summary_mn['other']\/pr_summary_mn['total_stops']*100\npr_summary_mn['white_stop_per'] = pr_summary_mn['white']\/pr_summary_mn['total_stops']*100\npr_summary_mn['gridnum'] = pr_summary_mn['gridnum'].astype(int)\npr_summary_mn.head(3)","60386880":"police_records_mn.INCIDENT_REASON.unique()","8fbc50bf":"stop_code = {'No Data':'no_reason_stop',\n             'Investigative Stop':'no_reason_stop',\n             'Moving Violation':'with_reason_stop', \n             'Equipment Violation':'with_reason_stop',\n             '911 Call \/ Citizen Reported':'with_reason_stop'}\npolice_records_mn['stop_category'] = police_records_mn['INCIDENT_REASON'].map(stop_code)\ntemp = pd.crosstab(police_records_mn['LOCATION_DISTRICT'],police_records_mn['stop_category'],normalize = 'index').reset_index()\ntemp['LOCATION_DISTRICT'] = temp['LOCATION_DISTRICT'].astype(int)\ntemp['no_reason_stop'] = temp['no_reason_stop']*100\npr_summary_mn = pr_summary_mn.merge(temp[['LOCATION_DISTRICT','no_reason_stop']], how = 'left', left_on = 'gridnum', right_on = 'LOCATION_DISTRICT')\npr_summary_mn.drop(columns = ['LOCATION_DISTRICT'],inplace = True)\npr_summary_mn.head()","9be59504":"pp_mn_geo = gpd.read_file(base_path + 'Dept_24-00098\/24-00098_Shapefiles\/StPaul_geo_export_6646246d-0f26-48c5-a924-f5a99bb51c47.shp')\npp_mn_geo.crs","7ca3843c":"pp_mn_geo.describe()","904a8d1f":"pp_mn_geo.head(3)","90d3e379":"ax = police_records_mn.plot(figsize=(10,10))\npp_mn_geo.plot(ax=ax,facecolor = 'cyan',edgecolor = 'black', alpha=0.2)\nax.set_title('MN police precinct and vehicle stop scatter plot', fontsize=18)\nax.set_axis_off()","bb1b2b76":"pp_mn_geo['center'] = pp_mn_geo['geometry'].centroid\npp_mn_geo['gridnum'] = pp_mn_geo['gridnum'].astype(int)\npp_pr_summary_mn = pd.merge(pp_mn_geo, pr_summary_mn, on = 'gridnum', how = 'inner')","1102abe2":"ax = pp_pr_summary_mn.plot(facecolor = 'None', edgecolor = 'grey',figsize = (16,16))\nplt.scatter(x=pp_pr_summary_mn['center'].apply(lambda x: Point(x).x),\n           y=pp_pr_summary_mn['center'].apply(lambda x: Point(x).y),\n           s = (pp_pr_summary_mn['total_stops']\/8),\n           alpha=0.4,\n           c='red')\nax.set_title('Fig - 4 Vehicle stops count in each police precinct', fontsize =18)\nfor i, pp in enumerate(pp_pr_summary_mn.gridnum):\n    ax.annotate(pp,(pp_pr_summary_mn.geometry[i].centroid.x, pp_pr_summary_mn.geometry[i].centroid.y))\nax.set_axis_off()","991477f7":"def choropleth_map(df, c, fsize = (6,6)):\n    ax = df.plot(column = c, legend=True, figsize=fsize, cmap = 'Oranges', scheme = 'quantiles')\n    ax.set_title('Map - ' +c, fontsize=16)\n    ax.get_legend().set_bbox_to_anchor((1.4,1.2))\n    ax.set_axis_off()","f07a06a1":"# plot the choropleth from per columns.\nfor cl in pp_pr_summary_mn.iloc[:,14:].columns:\n    choropleth_map(pp_pr_summary_mn, cl)","63c1d892":"def metadata_summary(file_path):\n    ''' Function to summarise metadata file. The file has components of meta data seperated by _ in code and ; in description.\n    It returns unique combinations of code and description part. This will help in grouping and extracting only needed columns\n    from data.\n    Input - Metadata file path.\n    Output - Dataframe with code, desc and type unique combinations\n    '''\n    meta_df = pd.read_csv(file_path,names = ['code','desc'])\n    meta_code = meta_df['code'].str.split('_',expand = True)\n    meta_desc = meta_df['desc'].str.split(';',expand = True)\n    temp_df = pd.DataFrame(columns = ['code','desc','type'])\n    for cols in meta_code.columns:\n        t = pd.concat([meta_code[cols], meta_desc[cols]], axis=1)\n        t['type'] = 'type'+str(cols)\n        t.columns = ['code','desc','type']\n        temp_df = pd.concat([temp_df,t],axis=0)\n\n    temp_df.drop_duplicates(inplace = True)\n    temp_df.dropna(subset = ['code','desc'],inplace = True)\n    temp_df.reset_index(inplace = True, drop = True)\n    return(temp_df)","50d0c7a9":"rsa_mn_path = base_path + 'Dept_24-00098\/24-00098_ACS_data\/24-00098_ACS_race-sex-age\/ACS_15_5YR_DP05_metadata.csv'\nrsa_meta_mn = metadata_summary(rsa_mn_path)\nrsa_meta_mn.head()","914735be":"rsa_meta_mn[rsa_meta_mn.desc.str.contains('RACE - One race')]","bb96dcd8":"# Read race_sex_age data\nrsa_mn_data = pd.read_csv(base_path + 'Dept_24-00098\/24-00098_ACS_data\/24-00098_ACS_race-sex-age\/ACS_15_5YR_DP05_with_ann.csv',\n           skiprows = [1], na_values = '-')\nrsa_mn_data.head(3)","2011dd2b":"acs_mn_data = rsa_mn_data.filter(items = ['GEO.id','HC01_VC03','HC01_VC04','HC01_VC05','HC01_VC49','HC01_VC50','HC01_VC56'])\nacs_mn_data.rename(columns = {'HC01_VC03':'tot_pop',\n                              'HC01_VC04':'male_pop',\n                              'HC01_VC05':'female_pop',\n                              'HC01_VC49':'race_white',\n                              'HC01_VC50':'race_black',\n                              'HC01_VC56':'race_asian'}, inplace = True)","acef5eca":"ea_mn_path = base_path + 'Dept_24-00098\/24-00098_ACS_data\/24-00098_ACS_education-attainment\/ACS_15_5YR_S1501_metadata.csv'\nea_meta_mn = metadata_summary(ea_mn_path)\nea_meta_mn.head()","6eabf732":"# Read education_attainment\nea_mn_data = pd.read_csv(base_path + 'Dept_24-00098\/24-00098_ACS_data\/24-00098_ACS_education-attainment\/ACS_15_5YR_S1501_with_ann.csv',\n           skiprows = [1], na_values = '-')\nea_mn_data = ea_mn_data.filter(items = ['GEO.id','HC01_EST_VC02', 'HC01_EST_VC08','HC01_EST_VC03' ,'HC01_EST_VC09', 'HC01_EST_VC10'])\nea_mn_data['edu_pop_tot'] = ea_mn_data['HC01_EST_VC02'] + ea_mn_data['HC01_EST_VC08']\nea_mn_data['edu_pop_less'] = ea_mn_data['HC01_EST_VC03'] + ea_mn_data['HC01_EST_VC09'] + ea_mn_data['HC01_EST_VC10']\nea_mn_data = ea_mn_data.filter(items = ['GEO.id', 'edu_pop_tot','edu_pop_less'])","d507344c":"acs_mn_data = acs_mn_data.merge(ea_mn_data, on = 'GEO.id', how = 'left')\nacs_mn_data.head()","f9d20acb":"p_mn_path = base_path + 'Dept_24-00098\/24-00098_ACS_data\/24-00098_ACS_poverty\/ACS_15_5YR_S1701_metadata.csv'\np_meta_mn = metadata_summary(p_mn_path)\np_meta_mn.head()","4e64c696":"# Poverty data\np_mn_data = pd.read_csv(base_path + 'Dept_24-00098\/24-00098_ACS_data\/24-00098_ACS_poverty\/ACS_15_5YR_S1701_with_ann.csv',\n           skiprows = [1], na_values = '-')\np_mn_data = p_mn_data.filter(items = ['GEO.id','HC01_EST_VC01','HC02_EST_VC01'])\np_mn_data.rename(columns = {'HC01_EST_VC01':'pov_pop_tot','HC02_EST_VC01':'pov_pop_below'},inplace = True)","a6a68649":"acs_mn_data = acs_mn_data.merge(p_mn_data, on = 'GEO.id', how = 'left')\nacs_mn_data.head(3)","a5735a7c":"emp_mn_path = base_path + 'Dept_24-00098\/24-00098_ACS_data\/24-00098_ACS_employment\/ACS_15_5YR_S2301_metadata.csv'\nemp_meta_mn = metadata_summary(emp_mn_path)\nemp_meta_mn.head()","b9639bf3":"# Employment data\nemp_mn_data = pd.read_csv(base_path + 'Dept_24-00098\/24-00098_ACS_data\/24-00098_ACS_employment\/ACS_15_5YR_S2301_with_ann.csv',\n           skiprows = [1], na_values = '-')\nemp_mn_data = emp_mn_data.filter(items = ['GEO.id','HC01_EST_VC01','HC04_EST_VC01'])\nemp_mn_data.rename(columns = {'HC01_EST_VC01':'pop_emp_tot','HC04_EST_VC01':'pop_emp_unemploy_rate'},inplace = True)","7bd37d3e":"acs_mn_data = acs_mn_data.merge(emp_mn_data, on = 'GEO.id', how = 'left')","27f1b41f":"ooh_mn_path = base_path + 'Dept_24-00098\/24-00098_ACS_data\/24-00098_ACS_owner-occupied-housing\/ACS_15_5YR_S2502_metadata.csv'\nooh_meta_mn = metadata_summary(ooh_mn_path)\nooh_meta_mn.head()","9782ecce":"# Employment data\nooh_mn_data = pd.read_csv(base_path + 'Dept_24-00098\/24-00098_ACS_data\/24-00098_ACS_owner-occupied-housing\/ACS_15_5YR_S2502_with_ann.csv',\n           skiprows = [1], na_values = '-')\nooh_mn_data = ooh_mn_data.filter(items = ['GEO.id','HC01_EST_VC01','HC02_EST_VC01'])\nooh_mn_data.rename(columns = {'HC01_EST_VC01':'occupied_units','HC02_EST_VC01':'owner_occupied_units'},inplace = True)","d4f69a7e":"acs_mn_data = acs_mn_data.merge(ooh_mn_data, on = 'GEO.id', how = 'left')\nacs_mn_data['pop_emp_unemployed_count'] = acs_mn_data['pop_emp_tot'] * acs_mn_data['pop_emp_unemploy_rate']\/100\nacs_mn_data.drop(columns = ['pop_emp_unemploy_rate'], inplace = True)\nacs_mn_data.head(3)","6e93f516":"# read the census track data and inspect first few rows\n#base_path = '..\/input\/cpe-data\/' for kaggle\ncensus_tracks_mn = gpd.read_file('..\/input\/mncensustracks\/cb_2017_27_tract_500k.shp')\ncensus_tracks_mn.head(2)","cb68f548":"census_tracks_mn.crs","e0d6950b":"census_tracks_mn.to_crs(pp_mn_geo.crs, inplace = True)","f0b80814":"acs_mn_data_geo = census_tracks_mn[['AFFGEOID','geometry']].merge(acs_mn_data,\n                                                                  right_on = 'GEO.id',\n                                                                  left_on = 'AFFGEOID', \n                                                                  how = 'right')","051dfd3b":"ax = acs_mn_data_geo.plot(color = 'cyan', alpha = 0.2, edgecolor = 'red', figsize=(10,10))\npp_mn_geo.plot(ax= ax, facecolor = 'None', edgecolor = 'black')\nax.set_title('Geospatial mapping of police precinct and census tracks',fontsize=16)\nax.set_axis_off()","b165c20a":"# area for geodf with units in degrees is very small number. We need to get % split to be applied by taking the ratio.\n# This can cause big rounding errors. So side effect of converting degrees to meters is sq.meters is much bigger number, \n# reducing the rounding error.\n\npp_mn_geo.to_crs({'init': 'epsg:32118', 'no_defs': True},inplace = True)\npp_mn_geo['area_pp'] = pp_mn_geo['geometry'].area\nacs_mn_data_geo.to_crs({'init': 'epsg:32118', 'no_defs': True},inplace = True)\nacs_mn_data_geo['area_acs'] = acs_mn_data_geo['geometry'].area","5d7ee9ab":"# overlay census polygons on police precincts. Overlap area \/ census track area will give split % to be applied to \n# acs data before rolling up.\n##########################################################################\n# pp_acs_mn = gpd.overlay(pp_mn_geo[['gridnum','geometry','area_pp']], \n#                           acs_mn_data_geo[['AFFGEOID','geometry','area_acs']], \n#                           how = 'intersection')\n#\n###################################################################\nfpath = '..\/input\/tempshapefile\/pp_acs_mn.shp'\npp_acs_mn = gpd.read_file(fpath)\npp_acs_mn.head()","055b4bb4":"pp_acs_mn['common_area'] = pp_acs_mn['geometry'].area\npp_acs_mn['overlap_per'] = pp_acs_mn['common_area']\/pp_acs_mn['area_acs']*100 \npp_acs_mn = pp_acs_mn.filter(items = ['gridnum','AFFGEOID','overlap_per'])\npp_acs_mn.head(3)","f040761c":"# acs data merged with pp and asc overlap dataframe.\n\npp_by_acs_data = pp_acs_mn.merge(acs_mn_data, left_on = 'AFFGEOID', right_on = 'GEO.id', how = 'left')","57f84c0f":"t = pp_by_acs_data.iloc[:,0:4]\nfor col in pp_by_acs_data.columns[4:]:\n    t[col] = pp_by_acs_data[col] * pp_by_acs_data['overlap_per']\n\nt.drop(columns = ['AFFGEOID','GEO.id','overlap_per'],inplace=True)\nt = t.groupby(by='gridnum').sum().reset_index()","406f1e20":"# calculate % for the populations rolled up by police precinct.\n\nt['pop_male_per_rollup'] = t['male_pop']\/t['tot_pop']*100\nt['pop_female_per_rollup'] = t['female_pop']\/t['tot_pop']*100\nt['race_white_rollup'] = t['race_white']\/t['tot_pop']*100\nt['race_black_rollup'] = t['race_black']\/t['tot_pop']*100\nt['race_asian_rollup'] = t['race_asian']\/t['tot_pop']*100\nt['edu_less_educated_rollup'] = t['edu_pop_less']\/t['edu_pop_tot']*100\nt['pov_below_poverty_rollup'] = t['pov_pop_below']\/t['pov_pop_tot']*100\nt['emp_unemployment_rollup'] = t['pop_emp_unemployed_count']\/t['pop_emp_tot']*100\nt['ooh_owneroccupied_rollup'] = t['owner_occupied_units']\/t['occupied_units']*100","8ed77539":"t.drop(columns = pp_by_acs_data.columns[4:],inplace=True)","be3fd46a":"acs_police_record_by_pp = pp_pr_summary_mn.merge(t, on = 'gridnum')","b11a556c":"bias_df = acs_police_record_by_pp[['gridnum','geometry','center']].copy()","3b0920f1":"# gender bias\nbias_df = pd.merge(bias_df,acs_police_record_by_pp[['gridnum','male_stop_per','pop_male_per_rollup']], \n                  on = 'gridnum')\nbias_df['male_bias'] = acs_police_record_by_pp['male_stop_per'] - acs_police_record_by_pp['pop_male_per_rollup']\n\nbias_df = pd.merge(bias_df,acs_police_record_by_pp[['gridnum','female_stop_per','pop_female_per_rollup']], \n                  on = 'gridnum')\nbias_df['female_bias'] = acs_police_record_by_pp['female_stop_per'] - acs_police_record_by_pp['pop_female_per_rollup']\n# race bias\nbias_df = pd.merge(bias_df,acs_police_record_by_pp[['gridnum','white_stop_per','race_white_rollup']], \n                  on = 'gridnum')\nbias_df['white_bias'] = acs_police_record_by_pp['white_stop_per'] - acs_police_record_by_pp['race_white_rollup']\nbias_df = pd.merge(bias_df,acs_police_record_by_pp[['gridnum','black_stop_per','race_black_rollup']], \n                  on = 'gridnum')\nbias_df['black_bias'] = acs_police_record_by_pp['black_stop_per'] - acs_police_record_by_pp['race_black_rollup']\nbias_df = pd.merge(bias_df,acs_police_record_by_pp[['gridnum','asian_stop_per','race_asian_rollup']], \n                  on = 'gridnum')\nbias_df['asian_bias'] = acs_police_record_by_pp['asian_stop_per'] - acs_police_record_by_pp['race_asian_rollup']\nbias_df['no_reason_stop'] = acs_police_record_by_pp['no_reason_stop']\n","8f4f353f":"for col in np.arange(5,len(bias_df.columns),3):\n    choropleth_map(bias_df,bias_df.columns[col],fsize = (8,8))\n    t = bias_df.iloc[:,[0,col-2,col-1,col]].sort_values(by = bias_df.columns[col],ascending = False)\n    cleveland_plot(xmins = t.iloc[:,2], \n               xmaxs=t.iloc[:,1], \n               ylabs =  t.gridnum, \n               chart_title =  'Top bias police precincts', \n               xmin_lab =  t.columns[2] + ' %', \n               xmax_lab =  t.columns[1] + ' %',\n               ytitle = 'Police Grid number',\n               xtitle = 'percent',\n               fsize=(6,6))   \n    t = bias_df.iloc[:,[0,col-2,col-1,col]].sort_values(by = bias_df.columns[col],ascending = True)\n    cleveland_plot(xmins = t.iloc[:,2], \n               xmaxs=t.iloc[:,1], \n               ylabs =  t.gridnum, \n               chart_title =  'Bottom bias police precincts', \n               xmin_lab =  t.columns[2] + ' %', \n               xmax_lab =  t.columns[1] + ' %',\n               ytitle = 'Police Grid number',\n               xtitle = 'percent',\n               fsize=(6,6))       ","df3dd808":"acs_police_record_by_pp.columns","af091ae0":"bias_df.head()","89f8aed7":"relation_df = acs_police_record_by_pp.filter(items = ['gridnum',\n                                                      'geometry',\n                                                      'center',\n                                                      'total_stops',\n                                                      'edu_less_educated_rollup',\n                                                      'pov_below_poverty_rollup',\n                                                     'emp_unemployment_rollup',\n                                                     'ooh_owneroccupied_rollup'])\nrelation_df.head(3)","abde4119":"relation_df = relation_df.merge(bias_df[['gridnum','male_bias','female_bias','white_bias','black_bias','asian_bias','no_reason_stop']],on = 'gridnum',how = 'left')","a2e628cd":"relation_df.iloc[:,3:].corr()","ff7e6878":"ax = relation_df.plot(edgecolor = 'gray', \n                      figsize = (15,15), \n                      column = 'edu_less_educated_rollup', \n                      scheme = 'quantiles',\n                     legend = True,\n                     cmap = 'Blues')\nplt.scatter(x = relation_df['center'].apply(lambda x: Point(x).x),\n           y = relation_df['center'].apply(lambda x: Point(x).y),\n           s = relation_df['total_stops']\/12,\n           edgecolor = 'red',\n           color = 'None')\nax.set_title('Less education choropleth with bubble plot of total plots', fontsize=16)\nax.set_axis_off()","97e90628":"ax = relation_df.plot(edgecolor = 'gray', \n                      figsize = (15,15), \n                      column = 'pov_below_poverty_rollup', \n                      scheme = 'quantiles',\n                     legend = True,\n                     cmap = 'Greens')\nplt.scatter(x = relation_df['center'].apply(lambda x: Point(x).x),\n           y = relation_df['center'].apply(lambda x: Point(x).y),\n           s = relation_df['total_stops']\/12,\n           edgecolor = 'red',\n           color = 'None')\nax.set_title('Below poverty level choropleth with bubble plot of total plots', fontsize=16)\nax.set_axis_off()","dc408664":"ax = relation_df.plot(edgecolor = 'gray', \n                      figsize = (15,15), \n                      column = 'emp_unemployment_rollup', \n                      scheme = 'quantiles',\n                     legend = True,\n                     cmap = 'Greys')\nplt.scatter(x = relation_df['center'].apply(lambda x: Point(x).x),\n           y = relation_df['center'].apply(lambda x: Point(x).y),\n           s = relation_df['total_stops']\/12,\n           edgecolor = 'red',\n           color = 'None')\nax.set_title('Unemployment choropleth with bubble plot of total plots', fontsize=16)\nax.set_axis_off()","7dade6b1":"ax = relation_df.plot(edgecolor = 'gray', \n                      figsize = (15,15), \n                      column = 'ooh_owneroccupied_rollup', \n                      scheme = 'quantiles',\n                     legend = True,\n                     cmap = 'Blues')\nplt.scatter(x = relation_df['center'].apply(lambda x: Point(x).x),\n           y = relation_df['center'].apply(lambda x: Point(x).y),\n           s = relation_df['total_stops']\/12,\n           edgecolor = 'red',\n           color = 'None')\nax.set_title('Owner occupied housing choropleth with bubble plot of total plots', fontsize=16)\nax.set_axis_off()","e2cebbf9":"ax = relation_df.plot(edgecolor = 'gray', \n                      figsize = (15,15), \n                      column = 'no_reason_stop', \n                      scheme = 'quantiles',\n                     legend = True,\n                     cmap = 'Blues')\nplt.scatter(x = relation_df['center'].apply(lambda x: Point(x).x),\n           y = relation_df['center'].apply(lambda x: Point(x).y),\n           s = relation_df['total_stops']\/12,\n           edgecolor = 'red',\n           color = 'None')\nax.set_title('No reason stops choropleth with bubble plot of total stops', fontsize=16)\nax.set_axis_off()","88bada66":"from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA","6bb0d911":"scale = MinMaxScaler()\nrelation_df_scaled = relation_df.iloc[:,3:8]\nrelation_df_scaled = scale.fit_transform(relation_df_scaled)\nranges = np.arange(2,10)\ninert = []\nfor k in ranges:\n    model = KMeans(n_clusters = k)\n    model.fit(relation_df_scaled)\n    inert.append(model.inertia_)\n\nplt.plot(ranges,inert,'-o')\nplt.xticks(ranges)\nplt.title('Elbow plot', fontsize = 16, loc = 'center')\nplt.show()","f87cade4":"good_model = KMeans(n_clusters =6)\ngood_model.fit(relation_df_scaled)\nrelation_df['cluster_scaled'] = good_model.predict(relation_df_scaled)","92686ed8":"pca = PCA(n_components = 2)\nt = pca.fit_transform(relation_df_scaled)\npca_df = pd.DataFrame(data = t, columns = ['PCA1','PCA2'])\npca_df = pd.concat([pca_df,relation_df['cluster_scaled']],axis=1)","f9ef60ce":"cluster_color = {0:'Clust-0',1:'Clust-1',2:'Clust-2',3:'Clust-3',4:'Clust-4',5:'Clust-5'}\npca_df['clust_color'] = pca_df.cluster_scaled.map(cluster_color)\nclset = set(zip( pca_df.cluster_scaled,pca_df.clust_color ))","f45d4a3b":"fig, ax = plt.subplots(figsize= (8,8))\nsc = plt.scatter(pca_df.PCA1,pca_df.PCA2,c = pca_df.cluster_scaled,s=60, alpha = 1,cmap = 'tab20_r')\nplt.title('Clustering of the police precincts', fontsize = 18)\nplt.xlabel('PCA 1', fontsize =16)\nplt.ylabel('PCA 2', fontsize =16)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nhandles = [plt.plot([],color=sc.get_cmap()(sc.norm(c)),ls=\"\", marker=\"o\")[0] for c,l in clset ]\nlabels = [l for c,l in clset]\nax.legend(handles, labels)\nax.get_legend().set_bbox_to_anchor((1.3,1))\nax.get_legend().set_title('cluster-color')","6a2377cf":"fig, ax = plt.subplots(figsize= (8,8))\nsc = plt.scatter(pca_df.PCA1,pca_df.PCA2,c = pca_df.cluster_scaled,s=60, alpha = 0.7,cmap = 'tab20_r')\nplt.title('Clustering of the police precincts', fontsize = 18)\nplt.xlabel('PCA 1', fontsize =16)\nplt.ylabel('PCA 2', fontsize =16)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nhandles = [plt.plot([],color=sc.get_cmap()(sc.norm(c)),ls=\"\", marker=\"o\")[0] for c,l in clset ]\nlabels = [l for c,l in clset]\nax.legend(handles, labels)\nax.get_legend().set_bbox_to_anchor((1.3,1))\nax.get_legend().set_title('cluster-color')\n\nfor i, pp in enumerate(relation_df.gridnum):\n    ax.annotate(pp,(pca_df.PCA1[i]+0.01,pca_df.PCA2[i]-0.01),fontsize=10)","0ede96be":"ax = relation_df.plot(column = 'cluster_scaled', figsize = (14,14),edgecolor = 'grey', legend = True, categorical = True, cmap = 'tab20_r')\nax.set_axis_off()\nax.set_title('Police Precincts clustered', fontsize = 18)\nax.get_legend().set_title('cluster-color')\nax.get_legend().set_bbox_to_anchor((1.1,1))\nfor i, pp in enumerate(relation_df.gridnum):\n    ax.annotate(pp,(relation_df.geometry[i].centroid.x, relation_df.geometry[i].centroid.y))","f83dd0a5":"plt.title('Correlation map for overall area', fontsize = 16)\nsns.heatmap(relation_df.iloc[:,3:-1].corr(),vmin=-1, vmax=1, annot = True,cmap = 'Blues',fmt = '.1g')","77927354":"for i in np.arange(6):\n    plt.figure()\n    plt.title('Corr plot Cluster ' + str(i), fontsize=14)\n    sns.heatmap(relation_df[relation_df.cluster_scaled == i].iloc[:,3:-1].corr(),vmin=-1, vmax=1, annot = True,cmap = 'Blues',fmt = '.1g')","dab322bf":"## Read police precinct shapefiles.","1d0f9bf5":"Create a summary of police records to get totals by gender, race to calculate percentage of vehicle stoppages by subjects' race and gender.","95396d66":"From above metadata summary,  EST is for estimate, (ignoring MOE margin of error). HC01 is total population and HC04 is unemplyment rate. VC01 is population 16 and above. So we need below 2 columns.  \n1. HC01_EST_VC01 - 'pop_emp_tot'\n2. HC04_EST_VC01 - 'pop_emp_unemploy_rate'","30e0196d":"## Are there any outliers or grouping??  \nLet's use unsupervised clustering methods.","99058b02":"'Violation' and 'Reported' are stops due to a reason. Others can be classified as pre-emptive\/preventive stops. ","847aa000":"HC01 is total, HC02 is below poverty level, EST is estimate and VC01 is total population. So we need 2 columns only.  \n1. HC01_EST_VC01 - Total population  \n2. HC02_EST_VC01 - Below poverty level population  ","aeeb7859":"CRS of census tracks to be converted to **epsg:4326** in order to be able to merge with police precinct geospatially.","9bf1efbd":"From the metadata summary, HC01 is total and EST is estimates. So columns starting with HC01_EST are needed.  \nVC02 + VC08 will give total population 18 + which is legal driver age.  \nWe will use High school and above and 'more educated' and less than high school educated as less educated.  \n1. HC01_EST_VC02 + HC01_EST_VC08 = 'edu_pop_tot'  \n2. HC01_EST_VC03 + HC01_EST_VC09 + HC01_EST_VC10 = 'edu_pop_less'  ","96fab6ce":"From above list, we need to extract below for one - race.  \n1. HC01_VC49 - white.  \n2. HC01_VC50 - black.  \n3. HC01_VC56 - asian.  ","2c22d1a6":"**Clustering visualization**","5b02b66b":"First folder choosen for solution is dept_24_00098 which is Ramsey County in MN.","8994d7e4":"## Income.  \nIt has median income per household for various categories as census track level. However we need to roll it up to police precinct level. We can't really use median values to rollup\/divide to police precinct level. So can't make use of this data.  \n","35eb973f":"Police precinct has 200 records, geometry in **epsg:4326** coordinate reference system.  \n\nNow overlay vehicle stops data on police precinct map.","101a4f6a":"**Principal Component Analysis is used for visualising of police precincts clustered.**","e04268a9":"### Exploring Data\nDataset consists of 19 departmentwise folder. Each department folder has.  \n1. One subfolder containing shapefiles of police departments.  \n2. Other subfolder has ACS socio-economic data.  \n3. A file with police records for incidents\/crimes.  \n\nQuick scan of files and as also specified in the problem statement, data is not consistent\/uniform accross folders. Also geospatial data(coordinates) does not seem to follow same standards even within a department data.  \n\nUsing geopandas, pandas, shapely and matplotlib libraries for geospatial and tabular data wrangling and basic matplotlib for plotting.","6deb35fe":"## Dept_24-00098 - Ramsey County Minnesota.","3e04900e":"## Preparing Police_records_mn for gender, race bias","c58f0872":"## Poverty data","9cb73e7a":"# Problem statement.\n\nKaggle competition @\nhttps:\/\/www.kaggle.com\/center-for-policing-equity\/data-science-for-good\n\nHow do you measure justice? And how do you solve the problem of racism in policing? We look for factors that drive racial disparities in policing by analyzing census and police department deployment data. The ultimate goal is to inform police agencies where they can make improvements by identifying deployment areas where racial disparities exist and are not explainable by crime rates and poverty levels.\n","5793f61d":"26, 46 153 seems to be outliers. Looking at clusters in PCA view does not seem clear 6 clusters. May be 4 is better??\nNow let's check how these clustering are spreadout geospatially.","339c8445":"Key observations from above choropleth maps are  \n1. Male driver stops are higher than female driver dtops. Range for Male driver stops is 50 to 90%. Female drivers are in the range of 10 to 50%. There is one police precinct in south where almost 100% stops are male drivers.  \n2. Vehicle stop by race has distinct clustering geospatially. Also they range is different accross the data set. Native american stops range up to 1.6%, asians range up to 20% and latinos range up to 25%. Black and white race vehicle stops range up to 80% and above.  \n3. Top % of vehicle stops by cluster geospatially. White almost except central part of the map. Latino stops are in south central region, black in central part of the map and asian north east part of the map.","141442bc":"**Let's investigate reason for these stops.**","9794049a":"## employment data.  ","d555a184":"This is bit unexpected chart!!  \nI was expecting vehicle stops to be spread accross geospatilly. Upon further study of data, it seems all long\/lat provided are 'center' of police precinct rather than actual long\/lat of incidents similar to police records in other departments data.  \nThis chart won't depict *hot zones* of vehicle stop activities.  \n\nLet's make use of **bubble chart.**  \n\nFor this we need center of police precincts and count of stops. So we need to *merge pp_mn_geo* with *pr_summary_mn*","0cea6a63":"census track and police presinct CRS unit is degrees in decimals. I am choosing a CRS with unit of measure in Meters. Rational further in the solution, there would be need for calculating area\/distance, so these measures in meters\/sq meters is sensible rather than degrees. Using EPSG:32118 CRS system.  \n\nMore details from https:\/\/epsg.io\/32118  \n\nUsing geopandas to_crs function to convert to same CRS.","a6149441":"## Occupied housing units.  ","02b85bec":"## Read ACS Data.  \nACS folder has subfolders for  \n1. education  \n2. education over 25  \n3. poverty  \n4. race-sex-age  \n5. income  \n6. employment  \n7. housing.  \n\nEach folder has metadata and the actual data. Meta data shows number of columns ranges in 700-800 count. Also they are not uniform accross different departments. There is need to systematically process the metadata.","f47923c5":"census.gov website has geospatial data for each of the states. \nMinnesota state census tracks shapefiles are downloaded from  \nhttps:\/\/www.census.gov\/geo\/maps-data\/data\/cbf\/cbf_tracts.html  ","977a75a0":"Total stops has positive correlation with less education, below poverty rate and unemployment rate while negative correlation with owner occupied housing. Also it is no surpise to see below poverty has strong positive correlation with less eduction, unemplyment rate and negative with owner occupied housing.","a48acc01":"**Correlations heat map**","18df9c93":"HC01 is estimate parameter. VC03, VC04 and VC05 are total population, male and female. So to answer gender bias below columns will be extracted.  \n1. HC01_VC03 - Total population\n2. HC01_VC04 - Total male\n3. HC01_VC05 - Total female  \n\nTo get Race data, let's filter out metadata by description containing RACE -  column.","47cb115f":"**Correlations of biases and other parameters with total stops.**","0e2de7d2":"From above metadata summary we will extract EST estimate, HC01 occupied housing units, HC02 owner occupied and VC01 for occupied housing units. So we will extract below 2 columns.  \n1. HC01_EST_VC01 - 'occupied_units'  \n2. HC02_EST_VC01 - 'owner_occupied_units'\n","b892e0e1":"**Race Sex Age folder.**","0aa728f4":"Police records have reported vehicle stops from 2001 onwards. More than 710K records.  \nsubject race, gender, Age(there seems many missing records for this) will dictate ACS data extraction subsequently for analysis. Geospatially it has long\/lat in decimal degrees.  \nTo make it geospatial data, long\/lat is used to create geometry which is of type 'Point'. Long\/Lat in decimal degrees in **epsg:4326.**  \nCRS needs to be set first before converting to common CRS as above using gpd inbuilt functions.","f6f28d2e":"## education data.  \n\nThere are 2 folders for education. Education and Education over 25. As the police records are only for vehicle stoppages, and drivers supposed to be 17 and above. I will use Education folder and select appropriate columns. Ignoring education over 25 folder.","1fe8d632":"## Adding geospatial data to ACS data.","ecce0e9b":"**Correlation graphs by cluster**","e3a9ee9a":"Top of the chart seems to have some outliers in cluster 5. To find out the police precincts, use annotation. though it will crowd the chart.","50dceb05":"Unfortunately geopandas.overlay and sjoin are erroring out in kaggle dockers. There are few issues being discussed on this topic. As a work around, output of overlay from local machine is saved and uploaded to kaggle in csv. It is read and converted back to GeoDataFrame to continue the workflow. Once the issue on kaggle is resolved, this intermediate step can be removed.","06ba5c8d":"**Conclusions. **  \n\n**Total number.**\n-\tTotal number of vehicles stops by police gridnum are clustered around center of the map. i.e. 88, 89 and 90 gridnums and 94, 74 and 75 grid nums. There are significantly less number of stops at peripheries of the map.\n**By gender of driver.**\n-\tStopped vehicle drivers are predominantly males. % of drivers stopped for males ranges from 47% to 98% by police precincts.\n-\tHigher female driver stoppages are clustered around south eastern part of the map.\n\n**By Race.**\n-\tHigher % of drivers by race are clustered geographically. \no\tAsians at north east of the map (overall % up to 20%)\no\tBlack drivers high % are clustered in central part.\no\tLatinos south central.\no\tWhites except central part of the chart.\nNo reason stop\/preventive.\n\tAll stops which don\u2019t have data and no violation (speed, equipment) and 911\/reports are categorized into \u2018No_reason_stops\u2019. It is significantly higher across the region ranging from 78% to 99% by police precincts.\n\n**Bias.** (Simple convention of population % - % of divers for that group.).\nMales \u2013 Across the region, male driver stoppage % is higher than male population %. It may be partly due to may be higher number of drivers are males. However, there is no data to cross check this assumption. Police precincts 175, 210 and 197 % of drivers stopped are 80% and more for males. While population % in these police precincts is around 50%. \nFemales \u2013 As the gender data is binary, female bias is inverse of Male bias.\n\nWhite bias \u2013 197,280 and 137 have higher white bias where as 206,86 and 27 have negative bias. i.e. % driver stopped white is less than % of population white.\n\nBlack bias \u2013 86, 54 pp have high black bias. 280 and  240 negative black bias.\nAsian \u2013 Positive bias range is very small. Negative bias is more in 32,11,31 pp.\nRelation with socio economic factors.\nOverall total stops have positive correlation with less education (high school or less), below poverty rate, unemployment rate and slightly negative correlation with owner occupied housing rates. Overall stops is positively correlated with black bias and negative with Asian bias.\n\n**Clustering.** The map shows similar police precincts color coded.\nCluster 1 coded in grey has a unique positive correlation of total stops with owner occupied housing while overall map it is negative correlation.\nCluster 2 in brown color is where white bias is strongly correlated with total stops.\n\n","898ab160":"**Any relation between education level, poverty to vehicle stops?**","8698f96d":"IT shows *hot zones* are clustered around center of the map. Peripheries seem to be less vehicle stops.  \n\nHow these precincts compare to different percentages calculated above. Use **choropleth** maps."}}