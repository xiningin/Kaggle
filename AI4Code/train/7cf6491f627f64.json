{"cell_type":{"2708a1db":"code","d53fe216":"code","7d431d20":"code","5fe71277":"code","33ff290b":"code","7a4b7a36":"code","2bda504b":"code","7ee04d92":"code","e12fa776":"code","7b6a7e3e":"code","71749db1":"code","68610135":"code","ec2068dc":"code","d2087306":"code","26384f23":"code","af6a5089":"code","db122bf3":"code","48ceb81a":"code","d2c1af22":"code","cebe84ea":"code","99d65db1":"code","fd5d34f4":"code","81751efe":"code","dbdd855d":"code","b6da5864":"code","6526d164":"code","7a4d0acf":"code","cba66d75":"code","210dea8a":"code","8c0f309d":"code","bbb8c522":"code","389654d2":"code","f8a3093e":"code","b39794b9":"code","e7a28ee0":"code","d9a5b378":"code","9d5d3902":"code","85d267fc":"code","5dcd62f7":"code","311bf559":"code","987c2489":"code","82c9e447":"code","8f54b108":"code","701e94d1":"code","6e2ad5c4":"code","0e85b32a":"code","b1ea7430":"code","0debb035":"code","193c3c39":"code","1e694809":"markdown","1b64ca4d":"markdown","9e677e18":"markdown","2e0b4930":"markdown","6d062e72":"markdown","a625c608":"markdown","8164e1c5":"markdown","25b8e35d":"markdown","5b8e4507":"markdown","f3e065ca":"markdown","8dc193af":"markdown","b757ae58":"markdown","7aa0b0df":"markdown","9144f269":"markdown","dbaa984d":"markdown","767b19ef":"markdown","05fdb2b2":"markdown","d5f95c4f":"markdown","36d999f9":"markdown","3d926fad":"markdown","984b67cf":"markdown","44af497b":"markdown","0e690ab5":"markdown","0c15e872":"markdown","69d958cf":"markdown","576bc3bb":"markdown","78fe65d3":"markdown","1c401abb":"markdown","46079d39":"markdown","8f4084d9":"markdown"},"source":{"2708a1db":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV, train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, plot_roc_curve, accuracy_score, roc_curve\n\nfrom matplotlib import rcParams\nfrom matplotlib.cm import rainbow\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","d53fe216":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","7d431d20":"df = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')","5fe71277":"df.head()","33ff290b":"df.shape","7a4b7a36":"df.info()","2bda504b":"df.describe()","7ee04d92":"#to check for null values\ndf.isnull().sum()","e12fa776":"df[\"target\"].value_counts()","7b6a7e3e":"sns.countplot(x = 'target', data = df, palette = 'rocket', saturation = 1)\nplt.show()","71749db1":"#sex wise distribuion of categorical value\nsns.set_style('darkgrid')\nsns.countplot(x = 'target', hue = 'sex', data = df, palette = 'rocket', saturation = 1)\nplt.title('Heart Disease Frequency : Sex Wise')\nplt.show()\n","68610135":"#It's always a good practice to work with the dataset where the target classes are of approximately equal size. Thus check for the same","ec2068dc":"#heart disease frequency according to sex\npd.crosstab(df.sex, df.target).plot(kind = \"bar\",figsize = (20, 6), color = ['salmon', 'deepskyblue'])\nplt.title('Heart Disease Frequency : Sex')\nplt.xlabel('Sex (0 = Female, 1 = Male)')\nplt.xticks(rotation=0)\nplt.legend([\"0\", \"1\"])\nplt.ylabel('Frequency')\nplt.show()","d2087306":"#heart disease frequency according to age\npd.crosstab(df.age,df.target).plot(kind=\"bar\", figsize=(20, 6), color = ['salmon', 'deepskyblue'])\nplt.title('Heart Disease Frequency : Age')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.savefig('heartDiseaseAndAges.png')\nplt.show()\n","26384f23":"#heart disease frequency according to Maximum Heart Rate and Age\nplt.figure(figsize=(10, 6))\nplt.scatter(x = df.age[df.target==1], y = df.thalach[(df.target==1)], c = \"salmon\")\nplt.scatter(x=df.age[df.target==0], y=df.thalach[(df.target==0)])\nplt.legend([\"Disease\", \"No Disease\"])\nplt.xlabel(\"Age\")\nplt.ylabel(\"Maximum Heart Rate\")\nplt.show()","af6a5089":"#heart disease frequency according to slope\npd.crosstab(df.slope, df.target).plot(kind=\"bar\",figsize=(20, 6),color=['salmon', 'deepskyblue'])\nplt.title('Heart Disease Frequency : Slope')\nplt.xlabel('The Slope of The Peak Exercise ST Segment ')\nplt.xticks(rotation = 0)\nplt.ylabel('Frequency')\nplt.show()\n","db122bf3":"#heart disease frequency according to FBS\npd.crosstab(df.fbs,df.target).plot(kind=\"bar\",figsize=(15,6),color=['salmon','deepskyblue' ])\nplt.title('Heart Disease Frequency : FBS')\nplt.xlabel('FBS - (Fasting Blood Sugar > 120 mg\/dl) (1 = true 0 = false)')\nplt.xticks(rotation = 0)\nplt.legend([\"No Disease\", \"Disease\"])\nplt.ylabel('Frequency of Disease or Not')\nplt.show()","48ceb81a":"#heart disease frequency according to Chest Pain Type\npd.crosstab(df.cp,df.target).plot(kind=\"bar\",figsize=(15,6),color=['salmon','deepskyblue' ])\nplt.title('Heart Disease Frequency : Chest Pain Type')\nplt.xlabel('Chest Pain Type')\nplt.xticks(rotation = 0)\nplt.ylabel('Frequency of Disease')\nplt.show()","d2c1af22":"#correlations study\ncorrmat = df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize = (15, 10))\ng = sns.heatmap(df[top_corr_features].corr(), annot = True, cmap = \"YlGnBu\")","cebe84ea":"df.hist(figsize = (20, 20))","99d65db1":"#Since 'sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca' and 'thal' are categorical variables we'll turn them into dummy variables.","fd5d34f4":"df = pd.get_dummies(df, columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'])","81751efe":"standardScaler = StandardScaler()\ncolumns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\ndf[columns_to_scale] = standardScaler.fit_transform(df[columns_to_scale])","dbdd855d":"df.head()\n","b6da5864":"#Split Data\nY = df['target']\nX = df.drop(['target'], axis = 1)","6526d164":"df.head()","7a4d0acf":"#train test split\nnp.random.seed(42)\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2) ","cba66d75":"#function to print accuracy score, classification report and confusion matrix\ndef print_score(clf, X_train, y_train, X_test, y_test, train = True):\n    #training performance\n    if train:\n        pred = clf.predict(X_train)\n        print(\"TRAIN RESULT \\n\")\n        print(\"Accuracy Score: {0:.4f}\\n\".format(accuracy_score(y_train, pred)))\n        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_train, pred)))\n        #print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_train, pred)))\n\n        res = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n        print(\"Average Accuracy: \\t {0:.4f}\".format(np.mean(res)))\n        print(\"Accuracy SD: \\t\\t {0:.4f}\".format(np.std(res)))\n        print(\"_______________________________________________________________________________________\")\n\n    #test performance    \n    elif train==False:\n        print(\"\\nTEST RESULT \\n\")        \n        print(\"Accuracy Score: {0:.4f}\\n\".format(accuracy_score(y_test, clf.predict(X_test))))\n        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_test, clf.predict(X_test))))\n        print(\"Confusion Matrix of Test Data Set: \\n {}\\n\".format(confusion_matrix(y_test, clf.predict(X_test))))","210dea8a":"#function to plot confusion matrix\ndef plot_cm(y_test, model):\n  cnf_matrix = confusion_matrix(y_test, model.predict(X_test))\n  class_names = [0,1]\n  fig,ax = plt.subplots()\n  tick_marks = np.arange(len(class_names))\n  plt.xticks(tick_marks,class_names)\n  plt.yticks(tick_marks,class_names)\n  #create a heat map\n  sns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'YlGnBu',\n            fmt = 'g')\n  ax.xaxis.set_label_position('top')\n  plt.tight_layout()\n  plt.title('Confusion Matrix ', y = 1.1)\n  plt.ylabel('Actual label')\n  plt.xlabel('Predicted label')\n  plt.show()","8c0f309d":"accuracy = {}","bbb8c522":"lr = LogisticRegression(solver='liblinear')\nlr.fit(X_train, y_train)\n\nprint_score(lr, X_train, y_train, X_test, y_test, train=True)\nprint_score(lr, X_train, y_train, X_test, y_test, train=False)\n\ntest_score = accuracy_score(y_test, lr.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, lr.predict(X_train)) * 100\n\naccuracy['Logistic Regression'] = test_score\n\nresults_df = pd.DataFrame(data=[[\"Logistic Regression\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nplot_cm(y_test, lr)","389654d2":"knn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\n\nprint_score(knn, X_train, y_train, X_test, y_test, train=True)\nprint_score(knn, X_train, y_train, X_test, y_test, train=False)\n\ntest_score = accuracy_score(y_test, knn.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, knn.predict(X_train)) * 100\n\naccuracy['KNN'] = test_score\n\n\nresults_df_2 = pd.DataFrame(data=[[\"K-nearest neighbors\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nplot_cm(y_test, knn)","f8a3093e":"dtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\n\nprint_score(dtc, X_train, y_train, X_test, y_test, train=True)\nprint_score(dtc, X_train, y_train, X_test, y_test, train=False)\n\ntest_score = accuracy_score(y_test, dtc.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, dtc.predict(X_train)) * 100\n\naccuracy['Decision Tree Classifier'] = test_score\n\n\nresults_df_2 = pd.DataFrame(data=[[\"Decision Tree Classifier\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nplot_cm(y_test, dtc)","b39794b9":"rf = RandomForestClassifier(n_estimators=1000, random_state=42)\nrf.fit(X_train, y_train)\n\nprint_score(rf, X_train, y_train, X_test, y_test, train=True)\nprint_score(rf, X_train, y_train, X_test, y_test, train=False)\n\ntest_score = accuracy_score(y_test, rf.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, rf.predict(X_train)) * 100\n\naccuracy['Random Forest Classifier'] = test_score\n\nresults_df_2 = pd.DataFrame(data=[[\"Random Forest Classifier\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nplot_cm(y_test, rf)","e7a28ee0":"svm = SVC(kernel='rbf', gamma=0.1, C=1.0)\nsvm.fit(X_train, y_train)\n\nprint_score(svm, X_train, y_train, X_test, y_test, train = True)\nprint_score(svm, X_train, y_train, X_test, y_test, train = False)\n\ntest_score = accuracy_score(y_test, svm.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, svm.predict(X_train)) * 100\n\naccuracy['SVM'] = test_score\n\nresults_df_2 = pd.DataFrame(data=[[\"Support Vector Machine\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df = results_df.append(results_df_2, ignore_index=True)\nplot_cm(y_test, svm)","d9a5b378":"results_df","9d5d3902":"sns.set_style(\"darkgrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,100,10))\nplt.ylabel(\"Accuracy\")\nsns.barplot(x = list(accuracy.keys()), y = list(accuracy.values()), palette = 'Paired')\nplt.show()","85d267fc":"accuracy_tuned = {}","5dcd62f7":"params = {\"C\": np.logspace(-4, 4, 20), \"solver\": [\"liblinear\"]}\nlr = LogisticRegression()\n\nlr_cv = GridSearchCV(lr, params, scoring=\"accuracy\", n_jobs=-1, verbose=1, cv=5, iid=True)\nlr_cv.fit(X_train, y_train)\nbest_params = lr_cv.best_params_\n#print(f\"Best parameters: {best_params}\")\nlr = LogisticRegression(**best_params)\n\nlr.fit(X_train, y_train)\n\nprint_score(lr, X_train, y_train, X_test, y_test, train=True)\nprint_score(lr, X_train, y_train, X_test, y_test, train=False)\n\n\ntest_score = accuracy_score(y_test, lr.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, lr.predict(X_train)) * 100\n\naccuracy_tuned['Logistic Regression'] = test_score\n\ntuning_results_df = pd.DataFrame(data=[[\"Tuned Logistic Regression\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\n#tuning_results_df\nplot_cm(y_test, lr)","311bf559":"train_score = []\ntest_score = []\nneighbors = range(1, 31)\n\nfor k in neighbors:\n    model = KNeighborsClassifier(n_neighbors=k)\n    model.fit(X_train, y_train)\n    train_score.append(accuracy_score(y_train, model.predict(X_train)))\n    test_score.append(accuracy_score(y_test, model.predict(X_test)))","987c2489":"plt.figure(figsize=(12, 8))\nplt.plot(neighbors, train_score, label=\"Train score\")\nplt.plot(neighbors, test_score, label=\"Test score\")\nplt.xticks(np.arange(1, 31, 1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\nprint(f\"Maximum KNN score on the test data: {max(test_score)*100:.2f}%\")","82c9e447":"knn = KNeighborsClassifier(n_neighbors=27)\nknn.fit(X_train, y_train)\n\nprint_score(knn, X_train, y_train, X_test, y_test, train=True)\nprint_score(knn, X_train, y_train, X_test, y_test, train=False)\n\ntest_score = accuracy_score(y_test, knn.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, knn.predict(X_train)) * 100\n\naccuracy_tuned['KNN'] = test_score\n\nresults_df_2 = pd.DataFrame(data=[[\"Tuned K-nearest neighbors\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\ntuning_results_df = tuning_results_df.append(results_df_2, ignore_index=True)\n#tuning_results_df\nplot_cm(y_test, knn)","8f54b108":"params = {\"criterion\":(\"gini\", \"entropy\"), \"splitter\":(\"best\", \"random\"), \"max_depth\":(list(range(1, 20))), \n          \"min_samples_split\":[2, 3, 4], \"min_samples_leaf\":list(range(1, 20))}\n\ntree = DecisionTreeClassifier(random_state=42)\ntree_cv = GridSearchCV(tree, params, scoring=\"accuracy\", n_jobs=-1, verbose=1, cv=3, iid=True)\ntree_cv.fit(X_train, y_train)\nbest_params = tree_cv.best_params_\nprint(f'Best_params: {best_params}')\n\ntree = DecisionTreeClassifier(**best_params)\ntree.fit(X_train, y_train)\n\nprint_score(tree, X_train, y_train, X_test, y_test, train=True)\nprint_score(tree, X_train, y_train, X_test, y_test, train=False)\n\ntest_score = accuracy_score(y_test, tree.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, tree.predict(X_train)) * 100\n\naccuracy_tuned['Desision Tree Classifier'] = test_score\n\nresults_df_2 = pd.DataFrame(data=[[\"Tuned Decision Tree Classifier\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\ntuning_results_df = tuning_results_df.append(results_df_2, ignore_index=True)\n#tuning_results_df\nplot_cm(y_test, knn)\n","701e94d1":"rf_grid = {'n_estimators': np.arange(10, 1000, 50), 'max_depth': [None, 3, 5, 10], \n           'min_samples_split': np.arange(2, 20, 2), 'min_samples_leaf': np.arange(1, 20, 2)}\nnp.random.seed(42)\n\nrf = RandomizedSearchCV(RandomForestClassifier(), param_distributions = rf_grid, cv=5, n_iter=20, verbose=True)\n\nrf.fit(X_train, y_train)\nrf.best_params_\n\ntest_score = accuracy_score(y_test, rf.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, rf.predict(X_train)) * 100\n\naccuracy_tuned['Random Forest Classifier'] = test_score\n\nresults_df_2 = pd.DataFrame(data=[[\"Tuned Random Forest Classifier\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\ntuning_results_df = tuning_results_df.append(results_df_2, ignore_index=True)\n#tuning_results_df\nplot_cm(y_test, rf)","6e2ad5c4":"svm = SVC(kernel='rbf', gamma=0.1, C=1.0)\n\nparams = {\"C\":(0.1, 0.5, 1, 2, 5, 10, 20), \n          \"gamma\":(0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1), \n          \"kernel\":('linear', 'poly', 'rbf')}\n\nsvm_cv = GridSearchCV(svm, params, n_jobs=-1, cv=5, verbose=1, scoring=\"accuracy\")\nsvm_cv.fit(X_train, y_train)\nbest_params = svm_cv.best_params_\nprint(f\"Best params: {best_params}\")\n\nsvm = SVC(**best_params)\nsvm.fit(X_train, y_train)\n\nprint_score(svm, X_train, y_train, X_test, y_test, train=True)\nprint_score(svm, X_train, y_train, X_test, y_test, train=False)\n\ntest_score = accuracy_score(y_test, svm.predict(X_test)) * 100\ntrain_score = accuracy_score(y_train, svm.predict(X_train)) * 100\n\naccuracy_tuned['SVM'] = test_score\n\nresults_df_2 = pd.DataFrame(data=[[\"Tuned Support Vector Machine\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\ntuning_results_df = tuning_results_df.append(results_df_2, ignore_index=True)\nplot_cm(y_test, svm)","0e85b32a":"tuning_results_df","b1ea7430":"sns.set_style(\"darkgrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,100,10))\nplt.ylabel(\"Accuracy\")\nsns.barplot(x = list(accuracy_tuned.keys()), y = list(accuracy_tuned.values()), palette = 'Paired')\nplt.show()","0debb035":"def feature_imp(df, model):\n    fi = pd.DataFrame()\n    fi[\"feature\"] = df.columns\n    fi[\"importance\"] = model.best_estimator_.feature_importances_\n    return fi.sort_values(by=\"importance\", ascending=False)","193c3c39":"feature_imp(X, rf).plot(kind='bar', figsize=(12,10), legend=False, colormap = 'seismic')","1e694809":"##6.2 KNN","1b64ca4d":"As seen from the table and graph, the highest accuracy is produced by KNN and SVM. ","9e677e18":"## Accuracy comparison between Different Models","2e0b4930":"Data has been imported successfully","6d062e72":"# 1.Import Libraries\n","a625c608":"#**Heart** **Disease** **Prediction**","8164e1c5":"#4.Feature Selection","25b8e35d":"It helps to undersand which features are relevant. ","5b8e4507":"Now the data is ready to be processed.\n\n5 Different Models are used.\n\n1.   Logistic Regression\n2.   K Nearest Neighbors\n3.   Decision Tree Classifier\n4.   Random Forest Classifier\n5.   Support Vector Machine\n\n\nFor all these 5 models, following data are evaluated : \n\n*   Accuracy Score\n*   Classification Report\n*   Confusion Matrix\n  \n\n\n\n\n\n","f3e065ca":"##6.1 Logistic Regression","8dc193af":"As seen from the table and graph, the highest testing accuracy is produced by Logistic Regression and highest training accuracy is produced by SVM.","b757ae58":"##7.4 Random Forest Classifier","7aa0b0df":"##7.2 KNN","9144f269":"##7.5 Support Vector Machine","dbaa984d":"##7.1 Logistic Regression","767b19ef":"#6.Models","05fdb2b2":"The accuracy can be improved by Hyperparameter Tuning. It involves choosing a range of optimal parameters for an algorithm. ","d5f95c4f":"##7.3 Decision Tree Classifier","36d999f9":"##6.4 Random Forest Classifier","3d926fad":"Heart Disease is one of the major concerns to be dealt with. It is very important to identify it and do the proper treatment.\nMachine learning proves to be effective in making decisions and predictions from the large quantity of data produced by the healthcare industry.\n\n\nHere, various ML models have been applied for classifying whether a person is suffering from Heart Disease or Not. The dataset is taken from [Cleveland Heart Disease dataset from the UCI Repository](https:\/\/archive.ics.uci.edu\/ml\/datasets\/heart+disease) and the same is also available at [Kaggle](https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci).\n\nThe dataset consists of 303 individuals data. There are 14 columns in the dataset, which are described below - \n1. Age: displays the age of the individual.\n2. Sex: displays the gender of the individual using the following format :\n- 1 = male\n- 0 = female\n3. Chest-pain type: displays the type of chest-pain experienced by the individual using the following format :\n- 1 = typical angina\n- 2 = atypical angina\n- 3 = non \u2014 anginal pain\n- 4 = asymptotic\n4. Resting Blood Pressure: displays the resting blood pressure value of an individual in mmHg (unit)\n5. Serum Cholestrol: displays the serum cholesterol in mg\/dl (unit)\n6. Fasting Blood Sugar: compares the fasting blood sugar value of an individual with 120mg\/dl.\nIf fasting blood sugar > 120mg\/dl then : 1 (true)\nelse : 0 (false)\n7. Resting ECG : displays resting electrocardiographic results\n- 0 = normal\n- 1 = having ST-T wave abnormality\n- 2 = left ventricular hyperthrophy\n8. Max heart rate achieved : displays the max heart rate achieved by an individual.\n9. Exercise induced angina :\n- 1 = yes\n- 0 = no\n10. ST depression induced by exercise relative to rest: displays the value which is an integer or float.\n11. Peak exercise ST segment :\n- 1 = upsloping\n- 2 = flat\n- 3 = downsloping\n12. Number of major vessels (0\u20133) colored by flourosopy : displays the value as integer or float.\n13. Thal : displays the thalassemia :\n- 3 = normal\n- 6 = fixed defect\n- 7 = reversible defect\n14. Diagnosis of heart disease : Displays whether the individual is suffering from heart disease or not :\n- 0 = absence\n- 1, 2, 3, 4 = present.\n\n","984b67cf":"##Accuracy comparison between different tuned models","44af497b":"##8.1 According to Random Forest Classifier","0e690ab5":"Function Definition to evaluate Accuracy Score, Classification Report and Confusion Matrix of Classifier","0c15e872":"#8.Feature Importance ","69d958cf":"#5.Data Processing and Train Test Split","576bc3bb":"#2.Import Dataset","78fe65d3":"##6.3 Decision Tree Classifier","1c401abb":"#3.Data Visualizaton","46079d39":"##6.5 Support Vector Machine","8f4084d9":"#7.Hyperparameter Tuning To Improve The Accuracy"}}