{"cell_type":{"95a1d2ef":"code","52961cbb":"code","a679bb74":"code","0aaa4c0f":"code","a47b7754":"code","10025869":"code","200222e7":"code","0dbf566b":"code","818e5274":"code","fee11d3f":"code","c3765043":"code","f7702147":"code","bc8fef0e":"code","42973608":"code","2d87f8c9":"code","01b49ab4":"code","8b248852":"code","2770bf61":"code","2905ab25":"code","dc8bb612":"code","fb759875":"code","908706c4":"markdown","5197cb6a":"markdown","2451bc92":"markdown","c320af05":"markdown"},"source":{"95a1d2ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","52961cbb":"import pandas as pd\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.width', 500)\npd.set_option('display.expand_frame_repr', False)\nfrom mlxtend.frequent_patterns import apriori, association_rules","a679bb74":"!pip install openpyxl","0aaa4c0f":"df_ = pd.read_excel('..\/input\/online-retail-ii-data-set-from-ml-repository\/online_retail_II.xlsx', sheet_name=\"Year 2010-2011\")\ndf = df_.copy()\ndf.info()\ndf.head()","a47b7754":"# The function is used to state the thresholds for the outlier values.\n\ndef outlier_thresholds(dataframe, variable):\n    quartile1 = dataframe[variable].quantile(0.01)\n    quartile3 = dataframe[variable].quantile(0.99)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n","10025869":"# The outlier values are replaced with the threshold values.\n\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n","200222e7":"# The NaN values are removed. In the Invoice column, the invoices starting with C shows the cancelled orders. Since the cancelled orders take place as \n# negative values in the Quantity and Price columns, the negative values are eliminated from these columns.   \n\ndef retail_data_prep(dataframe):\n    dataframe.dropna(inplace=True)\n    dataframe = dataframe[~dataframe[\"Invoice\"].str.contains(\"C\", na=False)]\n    dataframe = dataframe[dataframe[\"Quantity\"] > 0]\n    dataframe = dataframe[dataframe[\"Price\"] > 0]\n    replace_with_thresholds(dataframe, \"Quantity\")\n    replace_with_thresholds(dataframe, \"Price\")\n    return dataframe\n\n\ndf = retail_data_prep(df)","0dbf566b":"# The customer will be chosen from Germany. So a new dataset where only customers in Germany are involved is composed. \n\ndf_ger = df[df[\"Country\"] == \"Germany\"]\n\ndf_ger.head()\n","818e5274":"# The table the data chosen from should look like the table below. If a product is in the invoice, there should be write 1, otherwise 0. \n\n# Description   NINE DRAWER OFFICE TIDY   SET 2 TEA TOWELS I LOVE LONDON    SPACEBOY BABY GIFT SET...\n# Invoice\n# 536370                              0                                 1                       0..\n# 536852                              1                                 0                       1..\n# 536974                              0                                 0                       0..\n# 537065                              1                                 0                       0..\n# 537463                              0                                 0                       1..\n \n","fee11d3f":"# The number of products ordered at each order:\n\ndf_ger.groupby([\"Invoice\", \"Description\"]).agg({\"Quantity\": \"sum\"}).head(20)\n","c3765043":"# For having one invoice per row, unstack method is used. If a product is in the invoice, its amount will be on the table. Otherwise it will be NaN.\n# Only 5 rows and 5 columns are shown with iloc method.\n\ndf_ger.groupby([\"Invoice\", \"Description\"]).agg({\"Quantity\": \"sum\"}).unstack().iloc[0:5, 0:5]\n","f7702147":"# The NaN values are replaced with 0.\n\ndf_ger.groupby([\"Invoice\", \"Description\"]).agg({\"Quantity\": \"sum\"}).unstack().fillna(0).iloc[0:5, 0:5]","bc8fef0e":"# For the desired table, the product amounts are replaced with 1.\n\ndf_ger.groupby([\"Invoice\", \"Description\"]).agg({\"Quantity\": \"sum\"}).unstack().fillna(0).applymap(lambda x: 1 if x>0 else 0).iloc[0:5, 0:5]","42973608":"# The above code is valid if product id is not used. If the product id will be used, the function below will work.\n\ndef create_invoice_product_df(dataframe, id=False):\n    if id:\n        return dataframe.groupby(['Invoice', \"StockCode\"])['Quantity'].sum().unstack().fillna(0). \\\n            applymap(lambda x: 1 if x > 0 else 0)\n    else:\n        return dataframe.groupby(['Invoice', 'Description'])['Quantity'].sum().unstack().fillna(0). \\\n            applymap(lambda x: 1 if x > 0 else 0)\n","2d87f8c9":"# The new dataframe is composed.\n\nger_inv_pro_df = create_invoice_product_df(df_ger, id = True)\n\nger_inv_pro_df.head()","01b49ab4":"# The below function is used to identify the given id belongs to which item.\n\ndef check_id(dataframe, stock_code):\n    product_name = dataframe[dataframe[\"StockCode\"] == stock_code][[\"Description\"]].values[0].tolist()\n    print(product_name)","8b248852":"product_id = 21987\ncheck_id(df, product_id)","2770bf61":"# Apriori function calculates the support values of the given dataset. Since a minimum threshold is stated, no lower support values are taken into account. \n\n# These data are assigned to a new dataset and are sorted according to support values in descending order.\n\nfrequent_itemsets = apriori(ger_inv_pro_df, min_support=0.01, use_colnames=True, low_memory=True)\n\nfrequent_itemsets.sort_values(\"support\", ascending=False).head()","2905ab25":"# Association_rules function calculates confidence, lift and other metric values for the given dataset and the support values. \n\nrules = association_rules(frequent_itemsets, metric =\"support\", min_threshold=0.01)\n\nrules.sort_values(\"support\", ascending=False).head()","dc8bb612":"# This function brings the suggested item for the purchased item. \n\n# The dataset created is sorted according to the lift value. If the product id is in the antecedents column, the suggested item is the first value of the \n# consequents column. This value is added to the recommendation_list. \n\ndef arl_recommender(rules, product_id, rec_count):\n    sorted_rules = rules.sort_values(\"lift\", ascending=False)\n    recommendation_list = []\n\n    for i, product in sorted_rules[\"antecedents\"].items():\n        for j in list(product):\n            if j == product_id:\n                recommendation_list.append(list(sorted_rules.iloc[i][\"consequents\"]))\n\n    recommendation_list = list({item for item_list in recommendation_list for item in item_list})\n\n    return recommendation_list[: rec_count]\n","fb759875":"# With check_id function, we learn what the id of the suggested item belongs to.\n\ncheck_id(df, arl_recommender(rules, product_id, 1)[0])","908706c4":"#","5197cb6a":"This project aims to recommend products to the user when he\/she already has added one to cart. \n\nApriori algorithm will be used. It is an algorithm to detect frequent item sets. It works by first identifying individual items that satisfy a minimum occurrence threshold. It then extends the item set, by looking at all possible pairs that still satisfy the specified threshold. As a final step, we calculate the following three metrics that are central to this algorithm.\n\nsupport: This is the percentage of orders that contains the item or item set. For an individual item A, if it is seen in 3 of 5 orders:\n \nsupport(A) = 3\/5 or 60%\n\nIf there are 2 items A and B, support value is the percentage that A and B occur together:\n\nsupport(A, B) = Freq(A,B) \/ N where Freq(A,B) is the number that A and B are together and N is the overall order amount. \n\nconfidence: Confidence is the percentage that for an A item already purchased, B item was also purchased with A. \n\nconfidence(A->B) = Freq(A,B) \/ Freq(A) where Freq(A,B) is the number that A and B are purchased together and Freq(A) is the number where A is purchased.\n \nlift: Lift value indicates that for the A item purchased, how many times the percentage of B purchasing increased. \n\nlift = support(A,B) \/ (support(A) * support(B))\n\nlift > 1 implies that there is a positive relationship between A and B.\n\nThe dataset contains the purchasing data of a UK based online souvenir store between 12\/01\/2009 and 12\/09\/2011.\n\nTo narrow down, the user will be chosen among Germany customers that shopped between 2010 and 2011.","2451bc92":"# 2. Preparing the data for ARL","c320af05":"# 1. Data Preprocessing"}}