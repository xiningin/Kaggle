{"cell_type":{"f122c7ea":"code","47a4f97e":"code","d086e2c4":"code","bf0bbefa":"code","b922e94b":"code","4e95e23a":"code","89ed6639":"code","3d6de902":"code","d363ed98":"code","327790ad":"code","eb2b9f1e":"code","1d627c29":"code","6891e7e1":"code","e66f7167":"code","2a031da9":"code","b19290a7":"code","d0f6a293":"code","4a609104":"code","d24cb90e":"code","1eeaf70a":"code","a7a3b905":"code","eacfab0f":"code","cfa3ff5d":"code","bfcab991":"code","36ac4f5a":"code","32f98af1":"code","e35aa911":"code","d4368fa7":"code","f9bca213":"code","f28675d5":"code","6bf7f343":"code","ac4df03c":"code","f9e91a75":"code","fdb4e8cf":"code","2b72cc36":"code","9732d6fd":"code","e39afac6":"code","f9a0786a":"code","0c87451f":"code","09abb80e":"code","7d1f0edc":"code","54a1174c":"code","901219cb":"code","50ae6c5b":"code","0e63c6e4":"code","dc602081":"code","a9b82e88":"code","e83734d5":"code","8433b29b":"code","a5c9bc2f":"code","6a6960ca":"code","06ff1003":"code","7896bca2":"code","a2015eb5":"code","12870d6e":"markdown","61a30723":"markdown","132d5c49":"markdown","fee01edd":"markdown","04da3b5f":"markdown","f7fd4cbf":"markdown","f1feecc3":"markdown","4064dcfe":"markdown","aa769a1a":"markdown","64acb367":"markdown","738d8f2a":"markdown","a0ec4e24":"markdown","0541729b":"markdown","5b07971e":"markdown"},"source":{"f122c7ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.offline as py\nimport plotly.express as px\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","47a4f97e":"df = pd.read_excel('\/kaggle\/input\/covid19\/Kaggle_Sirio_Libanes_ICU_Prediction.xlsx')\ndf.head()","d086e2c4":"#Correlation map to see how features are correlated with each other and with SalePrice\ncorrmat = df.corr(method='kendall')\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","bf0bbefa":"# Numerical features\nNumerical_feat = [feature for feature in df.columns if df[feature].dtypes != 'O']\nprint('Total numerical features: ', len(Numerical_feat))\nprint('\\nNumerical Features: ', Numerical_feat)","b922e94b":"# Let's find the null values in data\n\ntotal = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","4e95e23a":"## Lets Find the realtionship between discrete features and ALBUMIN_MAX\n\n#plt.figure(figsize=(8,6))\n\nfor feature in Numerical_feat:\n    data=df.copy()\n    plt.figure(figsize=(8,6))\n    data.groupby(feature)['HEMATOCRITE_MAX'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('HEMATOCRITE_MAX')\n    plt.title(feature)\n    plt.show()","89ed6639":"#df[Numerical_feat].hist(bins=25)\n#plt.show()","3d6de902":"## let us now examine the relationship between continuous features and SalePrice\n## Before that lets find continous features that donot contain zero values\n\ncontinuous_nozero = [feature for feature in Numerical_feat if 0 not in data[feature].unique() and feature not in ['BE_VENOUS_DIFF', 'BIC_ARTERIAL_DIFF']]\n\nfor feature in continuous_nozero:\n    plt.figure(figsize=(8,6))\n    data = df.copy()\n    data[feature] = np.log(data[feature])\n    data['HEMATOCRITE_MAX'] = np.log(data['HEMATOCRITE_MAX'])\n    plt.scatter(data[feature], data['HEMATOCRITE_MAX'])\n    plt.xlabel(feature)\n    plt.ylabel('HEMATOCRITE_MAX')\n    plt.show()","d363ed98":"## Normality and distribution checking for continous features\nfor feature in continuous_nozero:\n    plt.figure(figsize=(6,6))\n    data = df.copy()\n    sns.distplot(data[feature])\n    plt.show()","327790ad":"# categorical features\ncategorical_feat = [feature for feature in df.columns if df[feature].dtypes=='O']\nprint('Total categorical features: ', len(categorical_feat))\nprint('\\n',categorical_feat)","eb2b9f1e":"# lets find unique values in each categorical features\nfor feature in categorical_feat:\n    print('{} has {} categories. They are:'.format(feature,len(df[feature].unique())))\n    print(df[feature].unique())\n    print('\\n')","1d627c29":"# let us find relationship of categorical with target variable\n\nfor feature in categorical_feat:\n    data=df.copy()\n    data.groupby(feature)['HEMATOCRITE_MAX'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('')\n    plt.title(feature)\n    plt.show()","6891e7e1":"# these are selected features from EDA section\nfeatures = ['HEMATOCRITE_MAX', 'LEUKOCYTES_MAX', 'HEMOGLOBIN_MAX', 'NEUTROPHILES_MAX', 'TGO_MAX', 'TGP_MAX', 'RESPIRATORY_RATE_DIFF_REL', 'TEMPERATURE_DIFF_REL']","e66f7167":"# plot bivariate distribution (above given features with saleprice(target feature))\nfor feature in features:\n    if feature!='HEMATOCRITE_MAX':\n        plt.scatter(df[feature], df['HEMATOCRITE_MAX'])\n        plt.xlabel(feature)\n        plt.ylabel('HEMATOCRITE_MAX')\n        plt.show()","2a031da9":"# Lets first handle numerical features with nan value\nnumerical_nan = [feature for feature in df.columns if df[feature].isna().sum()>1 and df[feature].dtypes!='O']\nnumerical_nan","b19290a7":"df[numerical_nan].isna().sum()","d0f6a293":"## Replacing the numerical Missing Values\n\nfor feature in numerical_nan:\n    ## We will replace by using median since there are outliers\n    median_value=df[feature].median()\n    \n    df[feature].fillna(median_value,inplace=True)\n    \ndf[numerical_nan].isnull().sum()","4a609104":"# categorical features with missing values\ncategorical_nan = [feature for feature in df.columns if df[feature].isna().sum()>1 and df[feature].dtypes=='O']\nprint(categorical_nan)","d24cb90e":"df[categorical_nan].isna().sum()","1eeaf70a":"# replacing missing values in categorical features\nfor feature in categorical_nan:\n    df[feature] = df[feature].fillna('None')","a7a3b905":"df[categorical_nan].isna().sum()","eacfab0f":"#Deleting outliers for LEUKOCYTES_MAX\ndf = df.drop(df[(df['LEUKOCYTES_MAX']>4000) & (df['HEMATOCRITE_MAX']<300000)].index)\n\nplt.scatter(df['LEUKOCYTES_MAX'], df['HEMATOCRITE_MAX'])\nplt.xlabel('LEUKOCYTES_MAX')\nplt.ylabel('HEMATOCRITE_MAX')\nplt.show()","cfa3ff5d":"#Deleting outliers for HEMOGLOBIN_MAX\ndf = df.drop(df[(df['HEMOGLOBIN_MAX']>4000) & (df['HEMATOCRITE_MAX']<300000)].index)\n\nplt.scatter(df['HEMOGLOBIN_MAX'], df['HEMATOCRITE_MAX'])\nplt.xlabel('HEMOGLOBIN_MAX')\nplt.ylabel('HEMATOCRITE_MAX')\nplt.show()","bfcab991":"#Deleting outliers for NEUTROPHILES_MAX \ndf = df.drop(df[(df['NEUTROPHILES_MAX']>4000) & (df['HEMATOCRITE_MAX']<300000)].index)\n\nplt.scatter(df['NEUTROPHILES_MAX'], df['HEMATOCRITE_MAX'])\nplt.xlabel('NEUTROPHILES_MAX')\nplt.ylabel('HEMATOCRITE_MAX')\nplt.show()","36ac4f5a":"#Deleting outliers for TGO_MAX\ndf = df.drop(df[(df['TGO_MAX']>4000) & (df['HEMATOCRITE_MAX']<300000)].index)\n\nplt.scatter(df['TGO_MAX'], df['HEMATOCRITE_MAX'])\nplt.xlabel('TGO_MAX')\nplt.ylabel('HEMATOCRITE_MAX')\nplt.show()","32f98af1":"#Deleting outliers for TGP_MAX \ndf = df.drop(df[(df['TGP_MAX']>4000) & (df['HEMATOCRITE_MAX']<300000)].index)\n\nplt.scatter(df['TGP_MAX'], df['HEMATOCRITE_MAX'])\nplt.xlabel('TGP_MAX')\nplt.ylabel('HEMATOCRITE_MAX')\nplt.show()","e35aa911":"#Deleting outliers for RESPIRATORY_RATE_DIFF_REL \ndf = df.drop(df[(df['RESPIRATORY_RATE_DIFF_REL']>4000) & (df['HEMATOCRITE_MAX']<300000)].index)\n\nplt.scatter(df['RESPIRATORY_RATE_DIFF_REL'], df['HEMATOCRITE_MAX'])\nplt.xlabel('RESPIRATORY_RATE_DIFF_REL')\nplt.ylabel('HEMATOCRITE_MAX')\nplt.show()","d4368fa7":"#Deleting outliers for TEMPERATURE_DIFF_REL\ndf = df.drop(df[(df['TEMPERATURE_DIFF_REL']>4000) & (df['HEMATOCRITE_MAX']<300000)].index)\n\nplt.scatter(df['TEMPERATURE_DIFF_REL'], df['HEMATOCRITE_MAX'])\nplt.xlabel('TEMPERATURE_DIFF_REL')\nplt.ylabel('HEMATOCRITE_MAX')\nplt.show()","f9bca213":"# these are selected features from EDA section\nfeatures = ['HEMATOCRITE_MAX', 'LEUKOCYTES_MAX', 'HEMOGLOBIN_MAX', 'NEUTROPHILES_MAX', 'TGO_MAX', 'TGP_MAX', 'RESPIRATORY_RATE_DIFF_REL', 'TEMPERATURE_DIFF_REL']\n\n# selecting continuous features from above\ncontinuous_features = ['HEMATOCRITE_MAX', 'LEUKOCYTES_MAX', 'HEMOGLOBIN_MAX', 'NEUTROPHILES_MAX', 'TGO_MAX', 'TGP_MAX', 'RESPIRATORY_RATE_DIFF_REL', 'TEMPERATURE_DIFF_REL']","f28675d5":"#Train = train_df.shape[0]\n#Test = test_df.shape[0]\n#target_feature = train_df.SalePrice.values\n#combined_data = pd.concat((train_df, test_df)).reset_index(drop=True)\n#combined_data.drop(['SalePrice','Id'], axis=1, inplace=True)\n#print(\"all_data size is : {}\".format(combined_data.shape))","6bf7f343":"#Since I have no train, test files, Id, I adapted the code above for just 1 line, so that I could plot the distplot.  \ncombined_data = pd.concat((df, df)).reset_index(drop=True)","ac4df03c":"# so let's label encode above ordinal features\nfrom sklearn.preprocessing import LabelEncoder\nfor feature in features:\n    encoder = LabelEncoder()\n    combined_data[feature] = encoder.fit_transform(list(combined_data[feature].values))","f9e91a75":"# Now lets see label encoded data\ncombined_data[features].head()","fdb4e8cf":"## One hot encoding or getting dummies \n\ndummy_ordinals = pd.get_dummies(features) \ndummy_ordinals.head()","2b72cc36":"# creating dummy variables\n\ncombined_data = pd.get_dummies(combined_data)\nprint(combined_data.shape)","9732d6fd":"combined_data.head()","e39afac6":"# checking distribution of continuous features(histogram plot)\nfor feature in continuous_features:\n    if feature!='HEMATOCRITE_MAX':\n        sns.distplot(combined_data[feature], fit=norm)\n        plt.show()\n    else:\n        sns.distplot(df['HEMATOCRITE_MAX'], fit=norm)\n        plt.show()","f9a0786a":"# let's first see descriptive stat info \ncombined_data.describe()","0c87451f":"## we willtake all features from combined_dummy_data \nfeatures_to_scale = [feature for feature in combined_data]\nprint(len(features_to_scale))","09abb80e":"## Now here is where we will scale our data using sklearn module.\n\nfrom sklearn.preprocessing import MinMaxScaler\n\ncols = combined_data.columns  # columns of combined_dummy_data\n\nscaler = MinMaxScaler()\ncombined_data = scaler.fit_transform(combined_data[features_to_scale])","7d1f0edc":"# after scaling combined_data it is now in ndarray datypes\n# so we will create DataFrame from it\ncombined_scaled_data = pd.DataFrame(combined_data, columns=[cols])","54a1174c":"combined_scaled_data.head() # this is the same combined_dummy_data in scaled form.","901219cb":"# lets see descriptive stat info \ncombined_scaled_data.describe()","50ae6c5b":"#That's the code. Though we don't have train nor test, then I adapted once more. \n#train_df.shape, test_df.shape, combined_scaled_data.shape, combined_data.shape","0e63c6e4":"df.shape, df.shape, combined_scaled_data.shape, combined_data.shape","dc602081":"# separate train data and test data \ntrain_data = combined_scaled_data.iloc[:504,:]\ntest_data = combined_scaled_data.iloc[504:,:]\n\ntrain_data.shape, test_data.shape","a9b82e88":"## lets add target feature to train_data\n#train_data['deaths_per_million_85_days_after_first_death']= train_data['deaths_per_million_85_days_after_first_death']  # This saleprice is normalized. Its very impportant","e83734d5":"train_data = train_data\ntrain_data.head(10)","8433b29b":"test_data = test_data.reset_index()\ntest_data.tail()","a5c9bc2f":"dataset = train_data.copy()  # copy train_data to dataset variable","6a6960ca":"dataset.head()","06ff1003":"dataset = dataset.dropna()","7896bca2":"## lets create dependent and target feature vectors\n\nX = dataset.drop(['HEMATOCRITE_MAX'],axis=1)\nY = dataset[['HEMATOCRITE_MAX']]\n\nX.shape, Y.shape","a2015eb5":"Y.head()","12870d6e":"#The clinical study on the relationship between serum albumin concentration and lymphocyte levels in patients with 2019-novel coronavirus pneumonia \n\nAuthors: LI, Ruoqing; TIAN, Jigang; YANG, Fang; YU, Jie; LV, Lei; SUN, Guangyan; WANG, Hongqun; LIU, Yinghong; CHEN, Xi; FANG, Qingyong; YANG, Xiaojuan. - Chinese Journal of Emergency Medicine ; 29(0):E012-E012, 2020.-Article | COVIDWHO | ID: covidwho-6154\n\nThe objective is to explore the relationship between different serum albumin and lymphocyte levels in patients with 2019-novel coronavirus (2019-nCoV) pneumonia (COVID-19) Methods A retrospective study was performed to identify the characteristics of the clinical data of 205 COVID-19 patients who were hospitalized in the Happy Street of Hanchuan People's Hospital, Xiaogan, Hubei Province from January 24 to February 12, 2020, including their general information, serum albumin (ALB) levels, lymphocyte counts (LYM), percentage of lymphocytes (LYM%) and other laboratory parameter levels Low ALB group and normal ALB group were demarcated by the concentration of 35g\/L, further to identify the differences of LYM and LYM% levels and the incidence of LYM and LYM% decline at different ALB levels between groups,as well as the correlation between ALB and LYM, LYM% levels in hypoalbuminemia conditions Results 17 5% of COVID-19 patients were associated with hypoalbuminemia The levels of LYM and LYM% in the low ALB group were significantly lower than those in the normal ALB group ( P <0 001) The incidence of LYM and LYM% decline in the low ALB group was significantly higher than those in the normal ALB group ( P <0 001) The levels of LYM and LYM% in the low ALB group were significantly positively correlated with serum ALB concentrations ( P <0 05) Conclusions The decrease of lymphocyte levels in COVID-19 patients may be correlated to hypoalbuminemia COVID-19 patients complicated by hypoalbuminemia should be actively intervened to maintain serum albumin in the normal range. https:\/\/search.bvsalud.org\/global-literature-on-novel-coronavirus-2019-ncov\/resource\/en\/covidwho-6154\n\n","61a30723":"#Feature Scaling","132d5c49":"#Feature Selection","fee01edd":"#Normalizing some numerical data","04da3b5f":"Initially, train data had 1932 observations but I had droped 7 (oo 3 )in outlier handling section so now I have ??? observations. I simply didn't understand that countability. The original said: \"Initially, train data had 1460 observations but we had droped 2 oo 3 in outlier handling section so now we have 4581 observations.\" And the numbers are: ((1458, 81), (1459, 80), (2919, 225), (2919, 225)). I'll read it later and try to get it.","f7fd4cbf":"#Categorical Features","f1feecc3":"#Dealing with Numerical features(handling missing data)\n\nThe codes below were suppose to deal with train, test and their combined data. Since we don't have, adapt and overcome.","4064dcfe":"#Categorical features(handling missing data)","aa769a1a":"#Codes from AmritGrg https:\/\/www.kaggle.com\/amritgrg\/high-accuracy-with-detailed-eda-feature-engineer","64acb367":"#Above, data range differs so much. We need to scale them to same range.","738d8f2a":"Above two dataframe tables I think that datas are now scaled. (or not?)","a0ec4e24":"#Label encoding, One-Hot-Encoding\/dummies","0541729b":"#Outliers\n\nWe can see a clear otliers in all the(7) features I choose. I mean it just doesn't make sense for larger values (all the 7) to have low value of 85 days after 1st death (the target I chose). There might be some reason for this but we'll consider them as outliers and drop them.","5b07971e":"Since I arrived in a dead end, that's all. Kaggle Notebook Runner: Mar\u00edlia Prata  @mpwolke"}}