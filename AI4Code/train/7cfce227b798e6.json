{"cell_type":{"f69c8b1b":"code","ac19a9da":"code","2dbd4527":"code","903304e8":"code","ccd3c9e4":"code","c2bb1309":"code","873189c1":"code","90f8536f":"code","7b2e11ff":"code","787a21cf":"code","5ca0c70b":"code","adf6ebd6":"code","9867a07e":"code","4b14bafa":"code","ce70832b":"code","d2254b19":"code","4583b125":"code","5f3694b7":"code","7050a5b7":"code","93c1fe95":"code","eac462d1":"code","03321a64":"code","d232e668":"code","05cd67f7":"code","aeca602e":"code","b001d183":"code","302065bc":"code","8b482ba3":"code","01e36e81":"code","1320f6ff":"code","ce52cb60":"code","0ba9a4af":"code","32d0423d":"code","ae082f64":"code","6d39a86c":"code","63b48a2a":"code","ca10ca89":"code","213b69f1":"code","09817257":"code","71473872":"code","711142fd":"code","132a94bd":"code","7ddebc06":"code","9bdeab0b":"code","069e6e49":"code","3882dbff":"markdown"},"source":{"f69c8b1b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ac19a9da":"import matplotlib.pyplot as plt \nimport seaborn as sns ","2dbd4527":"train = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ntest = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')","903304e8":"train.head(3)","ccd3c9e4":"train.isnull().sum()","c2bb1309":"train = train.dropna()","873189c1":"print(\"train shape:\", train.shape)\nprint(\"train len:\", len(train))\nprint(\"test shape:\", test.shape)\nprint(\"test len:\", len(test))\n","90f8536f":"train.dtypes","7b2e11ff":"test.dtypes","787a21cf":"import spacy\nfrom spacy.symbols import nsubj, VERB\nnlp = spacy.load('en_core_web_lg')","5ca0c70b":"train.head(3)","adf6ebd6":"sns.countplot(train['sentiment']);\nplt.title('Data: Target distribution');","9867a07e":"def text_entities(text):\n    doc = nlp(text)\n    for ent in doc.ents:\n        print(f'Entity: {ent}, Label: {ent.label_}, {spacy.explain(ent.label_)}')","4b14bafa":"text_entities(train['text'][9])","ce70832b":"one_sentence = train['text'][0]\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent',jupyter=True)","d2254b19":"one_sentence = train['text'][240]\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent',jupyter=True)","4583b125":"one_sentence = train['text'][300]\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent',jupyter=True)","5f3694b7":"one_sentence = train['text'][450]\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent',jupyter=True)","7050a5b7":"def redact_names(text):\n    doc = nlp(text)\n    redacted_sentence = []\n    for ent in doc.ents:\n        ent.merge()\n    for token in doc:\n        if token.ent_type_ == \"PERSON\":\n            redacted_sentence.append(\"[REDACTED]\")\n        else:\n            redacted_sentence.append(token.string)\n    return \"\".join(redacted_sentence)","93c1fe95":"one_sentence = train['text'][450]\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent',jupyter=True)\none_sentence = redact_names(train['text'][500])\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent',jupyter=True)\n","eac462d1":"text = train['text'][9]\ndoc = nlp(text)\nspacy.displacy.render(doc, style='ent', jupyter=True)\n\nfor idx, sentence in enumerate(doc.sents):\n    for noun in sentence.noun_chunks:\n        print(f\"sentence {idx+1} has noun chunk '{noun}'\")","03321a64":"one_sentence = train['text'][300]\ndoc = nlp(one_sentence)\nspacy.displacy.render(doc, style='ent', jupyter=True)\n\nfor token in doc:\n    print(token, token.pos_)","d232e668":"text = train['text'].str.cat(sep=' ')\n\nmax_length = 1000000-1\ntext = text[:max_length]\n\n# removing URLs and '&amp' substrings using regex\nimport re\nurl_reg  = r'[a-z]*[:.]+\\S+'\ntext   = re.sub(url_reg, '', text)\nnoise_reg = r'\\&amp'\ntext   = re.sub(noise_reg, '', text)","05cd67f7":"doc = nlp(text)\nitems_of_interest = list(doc.noun_chunks)\nitems_of_interest = [str(x) for x in items_of_interest]","aeca602e":"# pronoun in corona keyword  \ndf_nouns = pd.DataFrame(items_of_interest, columns=[\"Corona\"])\nplt.figure(figsize=(5,4))\nsns.countplot(y=\"Corona\",\n             data=df_nouns,\n             order=df_nouns[\"Corona\"].value_counts().iloc[:10].index)\nplt.show()","b001d183":"corona = []\nfor token in doc:\n    if (not token.is_stop) and (token.pos_ == \"NOUN\") and (len(str(token))>2):\n        corona.append(token)\n        \ncorona = [str(x) for x in corona]","302065bc":"df_nouns = pd.DataFrame(corona, columns=[\"Corona Topics\"])\ndf_nouns\nplt.figure(figsize=(5,4))\nsns.countplot(y=\"Corona Topics\",\n             data=df_nouns,\n             order=df_nouns[\"Corona Topics\"].value_counts().iloc[:10].index)\nplt.show()","8b482ba3":"# I wanna see how about trump noun keyword \ntrump_topics = []\nfor ent in doc.ents:\n    if ent.label_ not in [\"PERCENT\", \"CARDINAL\", \"DATE\"]:\n        trump_topics.append(ent.text.strip())","01e36e81":"df_ttopics = pd.DataFrame(trump_topics, columns=[\"Trump Nouns\"])\nplt.figure(figsize=(5,4))\nsns.countplot(y=\"Trump Nouns\",\n             data=df_ttopics,\n             order=df_ttopics[\"Trump Nouns\"].value_counts().iloc[1:11].index)\nplt.show()","1320f6ff":"from spacy.lang.en.stop_words import STOP_WORDS\nfrom wordcloud import WordCloud\nplt.figure(figsize=(10,5))\nwordcloud = WordCloud(background_color=\"white\",\n                      stopwords = STOP_WORDS,\n                      max_words=45,\n                      max_font_size=30,\n                      random_state=42\n                     ).generate(str(corona))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","ce52cb60":"from spacy.lang.en.stop_words import STOP_WORDS\nfrom wordcloud import WordCloud\nplt.figure(figsize=(10,5))\nwordcloud = WordCloud(background_color=\"white\",\n                      stopwords = STOP_WORDS,\n                      max_words=45,\n                      max_font_size=30,\n                      random_state=42\n                     ).generate(str(trump_topics))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","0ba9a4af":"text_ = train['text'][200]\ndoc = nlp(text_)","32d0423d":"options = {'compact': True, 'bg': '#09a3d5',\n           'color': 'white', 'font': 'Trebuchet MS'}\nspacy.displacy.render(doc, jupyter=True, style='dep', options=options)","ae082f64":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.sparse import hstack\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport eli5\nfrom IPython.display import Image","6d39a86c":"from sklearn.model_selection import train_test_split\ntrain_, test_ = train_test_split(train, test_size=0.2)\nprint(\"Train DF: \",train_.shape)\nprint(\"Test DF: \",test_.shape)","63b48a2a":"train_.head(3)","ca10ca89":"text_transformer = TfidfVectorizer(stop_words='english', \n                                   ngram_range=(1, 2), lowercase=True, max_features=150000)","213b69f1":"X_train_text = text_transformer.fit_transform(train_['text'])\nX_test_text = text_transformer.transform(test_['text'])","09817257":"X_train = X_train_text\nX_test = X_test_text\nprint(\"X Train DF: \",X_train.shape)\nprint(\"X Test DF: \", X_test.shape)","71473872":"logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='multinomial',\n                           random_state=17, n_jobs=4)","711142fd":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)\n","132a94bd":"cv_results = cross_val_score(logit, X_train, train_['sentiment'], cv=skf, scoring='f1_macro')","7ddebc06":"cv_results, cv_results.mean()","9bdeab0b":"logit.fit(X_train, train_['sentiment'])\n","069e6e49":"eli5.show_weights(estimator=logit, \n                  feature_names= text_transformer.get_feature_names(),top=(50, 5))","3882dbff":"# Modeling "}}