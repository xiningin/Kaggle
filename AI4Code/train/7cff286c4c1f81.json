{"cell_type":{"47797765":"code","8c06e50f":"code","f7c1a942":"code","83de936d":"code","b234f5a4":"code","864431b3":"code","8f363120":"code","8a2db979":"code","c833ddce":"code","3146af2c":"code","26dabd2d":"code","26036b3c":"code","d031340e":"code","84877451":"code","f05f735f":"code","41bfa544":"code","5a5acb64":"code","6c39b448":"code","7757b5ec":"code","a8f1e027":"code","56089bc1":"code","bd0f6971":"code","74ba2c8a":"code","14bfdcf4":"code","7056d4c5":"code","ef091eba":"code","624f1b5e":"code","c939ebfd":"code","f6107b11":"code","33300e21":"code","9778cda2":"code","d9a92848":"code","e8884599":"code","67213f95":"code","bc62d61c":"code","8c23dcf7":"code","9804b20c":"code","aabef461":"code","ab3914b7":"code","f905b657":"code","ea9f517d":"code","cc1e0ace":"code","5545e30d":"code","6167ede7":"code","eb44531d":"code","49138de3":"code","68353776":"code","1a07447e":"code","22917531":"code","2f4bb6c7":"code","1a8645c2":"code","b94699d6":"code","4edd1c65":"code","30e5ff30":"code","30a3831d":"code","58ecb261":"code","667fef2f":"code","ef23eb48":"code","33d1096b":"code","2213d445":"code","969cfe07":"code","ad22013b":"code","60229f3a":"code","5aad8c93":"markdown","b3602b60":"markdown","0606260b":"markdown","c9db5bea":"markdown","7c78b1d0":"markdown","c0770b3f":"markdown","291a5cf1":"markdown","eee702e4":"markdown","411b4b2b":"markdown","10edf45e":"markdown","8d368f77":"markdown","5569aa87":"markdown","8c28b5c8":"markdown","0327cbe4":"markdown","63ebe2da":"markdown","6464a11e":"markdown","11dd4244":"markdown","37782c6b":"markdown","2508e1b2":"markdown","75d31e0a":"markdown","b9e5a03b":"markdown","72ef01b1":"markdown","454ed6de":"markdown","9cf23853":"markdown","33f78dd1":"markdown","dae958e0":"markdown","72bf6693":"markdown","fc543fec":"markdown","bf4b0e23":"markdown","9dc50ba6":"markdown","aa35b875":"markdown","e64419d3":"markdown","ff23d5a5":"markdown","5334d8de":"markdown","47496164":"markdown","2c34b9a2":"markdown","98170dab":"markdown","28ad0110":"markdown","010c2421":"markdown","1269720f":"markdown","943f6943":"markdown","3ea31c98":"markdown","4eef372e":"markdown","035a289e":"markdown"},"source":{"47797765":"#Linear Algebra\nimport numpy as np\n\n#Data Preprocessing\nimport pandas as pd\nimport re\n\n#Data Visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Algorithms\nimport xgboost as xgb\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.model_selection import cross_val_score","8c06e50f":"train_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain_data.head()","f7c1a942":"print(\"Shape of train_data:\", train_data.shape)\nprint(\"Shape of test_data:\", test_data.shape)","83de936d":"train_data.info()","b234f5a4":"train_data.describe()","864431b3":"#missing values\nmissing_values = train_data.isnull().sum().sort_values(ascending = False)\npercentage = round(100*missing_values\/891,2)\nmissing_data = pd.concat([missing_values, percentage], axis=1, keys = [\"missing val\",\"%\"])\nmissing_data.head()","8f363120":"#class balance \/ imbalance\nsns.countplot(x='Survived', data=train_data)\nprint(\"Survived: \",train_data.Survived.sum()\/train_data.Survived.count())","8a2db979":"\ntrain_data[['Sex','Survived']].groupby('Sex').mean().plot.bar()\nsns.countplot('Sex',hue='Survived',data=train_data,)\nplt.show()\n","c833ddce":"train_data[['Pclass','Survived']].groupby(['Pclass']).mean().plot.bar()\nsns.countplot('Pclass',hue='Survived',data=train_data)\nplt.show()","3146af2c":"sns.factorplot('Pclass', 'Survived', hue='Sex', data=train_data)\nplt.show()","26dabd2d":"f,ax=plt.subplots(1,2,figsize=(14,5))\ntrain_data[train_data['Survived']==0].Age.plot.hist(ax=ax[0],bins=20,edgecolor='black')\nax[0].set_title('Survived = 0')\nx1=list(range(0,85,5))\nax[0].set_xticks(x1)\ntrain_data[train_data['Survived']==1].Age.plot.hist(ax=ax[1],bins=20,edgecolor='black',color='orange')\nx2=list(range(0,85,5))\nax[1].set_xticks(x2)\nax[1].set_title('Survived = 1')\nplt.show()","26036b3c":"f,ax=plt.subplots(1,2,figsize=(14,5))\nsns.violinplot('Pclass','Age',hue='Survived',data=train_data,split=True,ax=ax[0])\nax[0].set_title('PClass vs Age')\nax[0].set_yticks(range(0,110,10))\nsns.violinplot(\"Sex\",\"Age\", hue=\"Survived\", data=train_data,split=True,ax=ax[1])\nax[1].set_title('Sex vs Age')\nax[1].set_yticks(range(0,110,10))\nplt.show()","d031340e":"sns.factorplot('SibSp','Survived', data=train_data)\nplt.show()","84877451":"sns.factorplot('Parch','Survived', data=train_data)\nplt.show()","f05f735f":"train_data[['Embarked','Survived']].groupby(['Embarked']).mean().plot.bar()\nsns.countplot('Embarked',hue='Survived',data=train_data)\nplt.show()","41bfa544":"# Remove all NULLS in the Embarked column\ntrain_data['Embarked'] = train_data['Embarked'].fillna('S')","5a5acb64":"# Divide Fare into 5 bins \ntrain_data['Fare_Range'] = pd.qcut(train_data['Fare'], 5) \n  \n# Barplot - Shows approximate values based  \n# on the height of bars. \nfig, ax = plt.subplots(figsize=(10,5))\nsns.barplot(x ='Fare_Range', y ='Survived',  \ndata = train_data, ax=ax) ","6c39b448":"train_data['Family_Size'] = train_data['Parch'] + train_data['SibSp'] \nsns.factorplot(x ='Family_Size', y ='Survived', data = train_data) ","7757b5ec":"test_data['Family_Size'] = test_data['Parch'] + train_data['SibSp']","a8f1e027":"train_data['Alone'] = 0\ntrain_data.loc[train_data.Family_Size == 0, 'Alone'] = 1\n\ntrain_data[['Alone','Survived']].groupby(['Alone']).mean().plot.bar()\nsns.countplot('Alone',hue='Survived',data=train_data,)\nplt.show()\n","56089bc1":"test_data['Alone'] = 0\ntest_data.loc[test_data.Family_Size == 0, 'Alone'] = 1","bd0f6971":"def get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n# Create a new feature Title, containing the titles of passenger names\n\ntrain_data['Title'] = train_data['Name'].apply(get_title)\n# Group all non-common titles into one single grouping \"Rare\"\n\ntrain_data['Title'] = train_data['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\ntrain_data['Title'] = train_data['Title'].replace('Mlle', 'Miss')\ntrain_data['Title'] = train_data['Title'].replace('Ms', 'Miss')\ntrain_data['Title'] = train_data['Title'].replace('Mme', 'Mrs')\n\n    \n    # Mapping titles\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\ntrain_data['Title'] = train_data['Title'].map(title_mapping)\ntrain_data['Title'] = train_data['Title'].fillna(0)\n    ","74ba2c8a":"test_data['Title'] = test_data['Name'].apply(get_title)\n# Group all non-common titles into one single grouping \"Rare\"\n\ntest_data['Title'] = test_data['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\ntest_data['Title'] = test_data['Title'].replace('Mlle', 'Miss')\ntest_data['Title'] = test_data['Title'].replace('Ms', 'Miss')\ntest_data['Title'] = test_data['Title'].replace('Mme', 'Mrs')\n\n    \n    # Mapping titles\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\ntest_data['Title'] = test_data['Title'].map(title_mapping)\ntest_data['Title'] = test_data['Title'].fillna(0)","14bfdcf4":"age_train_data = train_data.groupby(['Sex','Pclass','Title'])\nmedian_data = age_train_data.median()\nmedian_data = median_data.reset_index()[['Sex','Pclass','Title','Age']]\nmedian_data.head()","7056d4c5":"def fill_age(row):\n    condition = (\n        (median_data['Sex']==row['Sex']) &\n        (median_data['Pclass']==row['Pclass']) \n    )\n    return median_data[condition]['Age'].values[0]\n\ndef process_age():\n    train_data['Age'] = train_data.apply(lambda row: fill_age(row) if np.isnan(row['Age']) else row['Age'], axis=1)","ef091eba":"process_age()","624f1b5e":"test_data['Age'] = test_data.apply(lambda row: fill_age(row) if np.isnan(row['Age']) else row['Age'], axis=1)","c939ebfd":"import re\ndeck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\ndata = [train_data, test_data]\n\nfor dataset in data:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(int)","f6107b11":"train_data['Sex'] = train_data['Sex'].map( {'female': 0, 'male': 1} ).astype(int)","33300e21":"test_data['Sex'] = test_data['Sex'].map( {'female': 0, 'male': 1} ).astype(int)","9778cda2":"train_data['Embarked'] = train_data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)","d9a92848":"test_data['Embarked'] = test_data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)","e8884599":"train_data.info()","67213f95":"train_final = train_data.drop(['Name','Cabin','Fare_Range','Ticket'], axis = 1)","bc62d61c":"train_final.info()","8c23dcf7":"plt.figure(figsize=(12,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train_final.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, linecolor='white', annot=True)","9804b20c":"features = ['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked','Family_Size','Alone','Title','Deck']\nX_train = train_data[features]\nX_train.head()","aabef461":"Y_train = train_data['Survived'].ravel()","ab3914b7":"#X = X_Train.values","f905b657":"gbm = xgb.XGBClassifier( n_estimators= 100,\n                        max_depth= 4,\n                        min_child_weight= 2,\n                        gamma=0.9,                        \n                        subsample=0.8,\n                        colsample_bytree=0.8,\n                        objective= 'binary:logistic',\n                        nthread= -1,\n                        scale_pos_weight=1)\n\n\nscores = cross_val_score(gbm, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\n\nxgb_score = scores.mean()","ea9f517d":"sgd = linear_model.SGDClassifier(max_iter=19, tol=None)\n\nscores = cross_val_score(sgd, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\n\nsgd_score = scores.mean()","cc1e0ace":"rf = RandomForestClassifier(n_estimators=100)\nrf.fit(X_train, Y_train)\n\nscores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\n\nrf_score = scores.mean()","5545e30d":"logr = LogisticRegression()\n\nscores = cross_val_score(logr, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\n\nlr_score = scores.mean()","6167ede7":"knn = KNeighborsClassifier(n_neighbors = 5) \n\nscores = cross_val_score(knn, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\n\nknn_score = scores.mean()","eb44531d":"gaussian = GaussianNB() \n\nscores = cross_val_score(gaussian, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\n\nnb_score = scores.mean()","49138de3":"perceptron = Perceptron(max_iter=4)\n\nscores = cross_val_score(perceptron, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\n\nperceptron_score = scores.mean()","68353776":"linear_svc = LinearSVC()\n\nscores = cross_val_score(linear_svc, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\n\nsvm_score = scores.mean()","1a07447e":"dt = DecisionTreeClassifier() \nscores = cross_val_score(dt, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\n\ndt_score = scores.mean()","22917531":"results = pd.DataFrame({\n    'Model': ['Stochastic Gradient Descent', 'Logistic Regression', 'KNN', 'XGBoost', 'Naive Bayes', 'Perceptron', \n              'Random Forest', 'SVM',\n              'Decision Tree'],\n    'Score': [sgd_score, lr_score, knn_score, xgb_score, nb_score, perceptron_score, rf_score, svm_score, dt_score]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df","2f4bb6c7":"importances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(rf.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances.plot.bar()","1a8645c2":"# Random Forest\nrandom_forest = RandomForestClassifier(criterion = \"entropy\", \n                                       min_samples_leaf = 1, \n                                       min_samples_split = 12,   \n                                       n_estimators=100, \n                                       max_features='auto', \n                                       oob_score=True, \n                                       random_state=1, \n                                       n_jobs=-1)\n\nrandom_forest.fit(X_train, Y_train)\n#Y_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\n\nprint(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")","b94699d6":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(random_forest, X_train, Y_train, cv=3)\nconfusion_matrix(Y_train, predictions)","4edd1c65":"from sklearn.metrics import precision_score, recall_score\n\nprint(\"Precision:\", precision_score(Y_train, predictions))\nprint(\"Recall:\",recall_score(Y_train, predictions))","30e5ff30":"from sklearn.metrics import f1_score\nf1_score(Y_train, predictions)","30a3831d":"y_scores = random_forest.predict_proba(X_train)\ny_scores = y_scores[:,1]","58ecb261":"from sklearn.metrics import roc_curve\n# compute true positive rate and false positive rate\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(Y_train, y_scores)\n# plotting them against each other\ndef plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n    plt.ylabel('True Positive Rate (TPR)', fontsize=16)\n\nplt.figure(figsize=(14, 7))\nplot_roc_curve(false_positive_rate, true_positive_rate)\nplt.show()","667fef2f":"from sklearn.metrics import roc_auc_score\nr_a_score = roc_auc_score(Y_train, y_scores)\nprint(\"ROC-AUC-Score:\", r_a_score)","ef23eb48":"mean = test_data['Fare'].mean()\ntest_data['Fare'] = test_data['Fare'].fillna(mean)","33d1096b":"test_data.info()","2213d445":"features = ['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked','Family_Size','Alone','Title','Deck']\nX_test = test_data[features]\nX_test.head()","969cfe07":"# Random Forest\nrandom_forest = RandomForestClassifier(criterion = \"entropy\", \n                                       min_samples_leaf = 1, \n                                       min_samples_split = 12,   \n                                       n_estimators=100, \n                                       max_features='auto', \n                                       oob_score=True, \n                                       random_state=1, \n                                       n_jobs=-1)\n\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_test)\n\nprint(random_forest.score(X_train, Y_train))\n\nprint(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")","ad22013b":"submission = pd.DataFrame({\n    'PassengerId': test_data['PassengerId'],\n    'Survived': Y_prediction})\nsubmission.head()","60229f3a":"submission.to_csv(path_or_buf='rf_titanic.csv', index=False)","5aad8c93":"## Preparing test data","b3602b60":"## XGBoost","0606260b":"### Hyperparameter tuning","c9db5bea":"### Observations:\n- Class plays an important role in determining survival.\n- Class 3 has highest percentage of dying for both genders.","7c78b1d0":"# Loading Data","c0770b3f":"## Alone","291a5cf1":"## Features selected","eee702e4":"### Observations:\n- Survival higher for <4 Paarch.","411b4b2b":"### Observations:\n- Class 3 has more younger people and kids while class 1 has more elederly.\n- Survival rate of children and young people is high.\n- Most deaths were for people of age range 30-40","10edf45e":"### Observations:\n- Women were more likely to survive than men.\n- Sex is an important feature here.","8d368f77":"## KNN","5569aa87":"## Paarch","8c28b5c8":"### Observations:\n- No significant imbalance.","0327cbe4":"### Observations:\n- Missing Values in columns Age, Cabin and Embarked\n- Data types of all columns are not numeric - have to be converted","63ebe2da":"## Gaussian Naive Bayes","6464a11e":"# Loading Libraries","11dd4244":"## Correlation Heatmap","37782c6b":"# Data Exploration","2508e1b2":"## Sex","75d31e0a":"## Title","b9e5a03b":"## Support Vector Machine (SVM)","72ef01b1":"## Evaluation","454ed6de":"# Model Evaluation","9cf23853":"clf.best_params_","33f78dd1":"## Perceptron","dae958e0":"## Family Size","72bf6693":"## Age","fc543fec":"## Embarked","bf4b0e23":"## Logistic Regression","9dc50ba6":"## Observations:\n- Survival rate decreases with increase in number of siblings\/spouse. \n- Exception to trend when travelling alone.","aa35b875":"## Comparision","e64419d3":"param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \n              \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \n              \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \n              \"n_estimators\": [100, 400, 700, 1000, 1500]}\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nrf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\nclf = GridSearchCV(estimator=rf, param_grid=param_grid, n_jobs=-1)\nclf.fit(X_train, Y_train)","ff23d5a5":"## Deck","5334d8de":"## Stochastic Gradient Descent (SGD)","47496164":"As can be seen, XGBoost followed by Random Forest gives the best result.","2c34b9a2":"## **Creating Features**","98170dab":"## SibSp","28ad0110":"## Feature Importance","010c2421":"### Observations:\n- S has highest frequency and Q has lowest.\n- For C, count of survived is greater than that of not survived.","1269720f":"## Processing missing values in Age","943f6943":"## Fare","3ea31c98":"## Decision Tree","4eef372e":"## Pclass","035a289e":"## Random Forest"}}