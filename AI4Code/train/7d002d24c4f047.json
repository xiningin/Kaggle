{"cell_type":{"cf09c2da":"code","026f16eb":"code","ae940633":"code","b3d66cf9":"code","e6771344":"code","7915b10e":"code","a42ec9d5":"code","c7fcae03":"code","00325f06":"markdown","deb68527":"markdown","92a2cea2":"markdown","e073c4c0":"markdown","b6c42816":"markdown","8de76f3a":"markdown","593d47ed":"markdown","065da7f3":"markdown"},"source":{"cf09c2da":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","026f16eb":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.pipeline import make_pipeline\nimport xgboost as xgb\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\n\nn_jobs = -2\ncv=4\n\ndf_1000 = pd.read_csv(\"\/kaggle\/input\/fem-simulations\/1000randoms.csv\")\n\ntest_size = 0.25\nrandom_state = 42\nfeatures = ['ecc', 'N', 'gammaG', 'Esoil', 'Econc', 'Dbot', 'H1', 'H2', 'H3']\nlabel = ['Mr_t', 'Mt_t', 'Mr_c', 'Mt_c']\n\nX = df_1000[features]\ny = df_1000[label]","ae940633":"# take only the first label ('Mr_t'), for demonstration\nX_train, X_test, y_train, y_test = train_test_split(\n        X, y.iloc[:,0],\n        test_size = test_size,\n        random_state = random_state\n    )","b3d66cf9":"\npipe = make_pipeline(\n    PowerTransformer(),\n    xgb.XGBRegressor()\n)\n\nparam_grid = {\n    pipe.steps[-1][0] + '__' + 'max_depth': np.arange(2,6),\n    pipe.steps[-1][0] + '__' + 'n_estimators': np.arange(100,1001,100)\n}","e6771344":"model = GridSearchCV(\n        pipe,\n        param_grid = param_grid,\n        n_jobs = n_jobs,\n        cv = cv,\n        verbose = 1\n    )\nmodel.fit(X_train, y_train)\nprint('--> best params:', model.best_params_)","7915b10e":"y_hat = model.predict(X_test)\n\nresults = pd.DataFrame(\n            {\n                'R-Squared TRAIN': [r2_score(y_train, model.predict(X_train))],\n                'R-squared TEST': [r2_score(y_test, y_hat)],\n                'MAE TEST': [mean_absolute_error(y_test, y_hat)],\n                'MSE TEST': [mean_squared_error(y_test, y_hat)],\n                'RMSE TEST': [np.sqrt(mean_squared_error(y_test, y_hat))]\n            },\n            index=[pipe.steps[-1][0]]\n        ).T\nresults","a42ec9d5":"df_5184 = pd.read_csv(\"\/kaggle\/input\/fem-simulations\/5184doe.csv\")\n\nX_additional = df_5184[features]\ny_additional = df_5184[label]\ny_additional = y_additional.iloc[:,0]\n\ny_hat_additional = model.predict(X_additional)\n\nresults = pd.DataFrame(\n            {\n                'R-squared': [r2_score(y_additional, y_hat_additional)],\n                'MAE': [mean_absolute_error(y_additional, y_hat_additional)],\n                'MSE': [mean_squared_error(y_additional, y_hat_additional)],\n                'RMSE': [np.sqrt(mean_squared_error(y_additional, y_hat_additional))]\n            },\n            index=['additional ' + pipe.steps[-1][0]]\n        ).T\nresults","c7fcae03":"span = max(y_additional) - min(y_additional)\nrmse = np.sqrt(mean_squared_error(y_additional, y_hat_additional))\nrmse_prop = 100 * (rmse \/ span)\n\nprint(\"span: \", span)\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_additional, y_hat_additional)))\nprint(\"therefore, the proportional RMSE is: {:.2f}%\".format(rmse_prop))","00325f06":"# Predict and Evaluate Performance","deb68527":"Compared to the previous they are significantly worse. But still, tey are good quite good. You can compare it to the span (6.28) of the values and determine the proportional loss (4%):","92a2cea2":"# Comment\nThe performance of the model is significantly good for a Machine Learning project.\nThis may result from the nature of the problem: The FEM-Simulation produces a model itself. So this Model also contains a artificial \"behaviour\" or \"status\" of the object of investigation. Therefore it might be very easy for the ML-method to imitate this FEM-model.\n\nBesides: I have just done something similar for my masters thesis and therefore have looked out for different solutions how this problem can be handled. This was of course not only about finding any ML-Method that performs good, because this will always be something mainstream like XGB ;) But I also searched for ways to express and handle uncertainty of these results. And also tried to find out how many samples will actually be necessary to produce stable results. This may become interesting depending on the number of objects one will have to create FEM-Models on. And since those can be comutational expensive, you might also want to reduce the number of FEM-simulations.","e073c4c0":"# Additional Performance Test\nWhile the Model was trained with cross validation and the holdout \/ splitting approach, there is also another possibility in this specific case. The thousand samples can be used for training and then test the model on the bigger (5000 samples). The assumption here is that all the samples are made with the same FEM-model. So that some of the dataset with 1000 samples might be included in the 5000 samples, but even though: using the 5000 samples dataset for testing will at least have 4000 unknown labels. So lets do that:","b6c42816":"# Data\nLoad Data and initialize environment","8de76f3a":"# Train and Optimize","593d47ed":"# Holdout\nSplit Data","065da7f3":"# Configuration\nConfiguration of Machine Learning Method and Optimization Methodology"}}