{"cell_type":{"6ea0ac0b":"code","d7615db9":"code","9f4e88af":"code","c50adab6":"code","a17110b7":"code","4af838d5":"code","f2d76e25":"code","39f2eb23":"code","126a9b83":"code","8068af9f":"code","81b2961b":"code","e0369043":"code","fee5670f":"code","0d805ea6":"code","df557836":"code","5116d489":"code","783496a8":"code","26580506":"markdown","54b16aa3":"markdown","c0e6a2a4":"markdown","0953cec7":"markdown","d01c208e":"markdown","cb98fefd":"markdown","04c58a26":"markdown","4bafcd58":"markdown","ca919f66":"markdown"},"source":{"6ea0ac0b":"# Basic\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Tools\nfrom category_encoders import TargetEncoder\n#import category_encoders as ce # You can import whole library and play around with that\n\n# Dataset\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d7615db9":"# Train\ntrain_df = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/train.csv')\ntrain_df.shape","9f4e88af":"# Test\ntest_df = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/test.csv')\ntest_df.shape","c50adab6":"# Have a look\ntrain_df.head()","a17110b7":"# Data types\ntrain_df.dtypes","4af838d5":"# Number of null values\ntrain_df.isnull().sum().sum()","f2d76e25":"# Differentiate\nfeatures = train_df.drop('target',axis=1)\ntarget = train_df.target","39f2eb23":"# Select categorical features\ncols = train_df.columns\nnums = train_df._get_numeric_data().columns\ncats = list(set(cols) - set(nums))","126a9b83":"# Define target encoder\nenc = TargetEncoder(cols=cats).fit(features, target)\n\n# Encode\ntrain_enc = enc.transform(features, target)\ntest_enc = enc.transform(test_df)","8068af9f":"# This will fail because of some bug in pandas library (fall 2019)\n'''# One Hot Encoding\ndf_ohe = pd.get_dummies(df, columns=cats, drop_first=True)\ndf_ohe.shape\n\nfrom scipy.sparse import csr_matrix\n\ndf_ohe = csr_matrix(df_ohe.values)\ndf_ohe.memory_usage().sum()'''","81b2961b":"# Get training features and the target\nX = train_enc\ny = target","e0369043":"# Make a traning and validation dataset\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8, random_state=42)","fee5670f":"# Define model\nfrom sklearn import linear_model\n\nlr = linear_model.LogisticRegression(\n    solver='lbfgs', \n    max_iter=5000, \n    fit_intercept=True,\n    random_state=42, \n    penalty='none', \n    verbose=0)\n\n# Train model\nlr.fit(X_train, y_train)","0d805ea6":"# Validate the model\nfrom sklearn.metrics import accuracy_score\n\ny_pre = lr.predict(X_test)\nprint('Accuracy : ',accuracy_score(y_test, y_pre))","df557836":"# Predict test values\npred = lr.predict(test_enc).astype(np.int)\nsub = pd.DataFrame({'id':test_df['id'], 'target':pred})","5116d489":"# Have a look\nsub.head()","783496a8":"# Make a submission file\nsub.to_csv('submission.csv',index=False)","26580506":"There're more ways how to approache the problem. *OHE will fail* in this case on Kaggle bacause of some bug in a recent update (as to fall 2019) which makes spare matrix output parameter inactive. We can *CatBoots* which would be probably the most straight-forward way how to get good results. But I wanted to take the oppurtinity here to show how simple *target (also known as mean or likely-hood) encoding* can be.\n\nAn interesting thing to do would be to use other encoding methods from *category_encoders* library to compare them in between.","54b16aa3":"**If that helps, please up-vote!**","c0e6a2a4":"# EDA","0953cec7":"# Libraries","d01c208e":"# Introduction","cb98fefd":"# Model","04c58a26":"# Encoding","4bafcd58":"# Load data","ca919f66":"# Submission"}}