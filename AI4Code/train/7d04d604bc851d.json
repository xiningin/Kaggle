{"cell_type":{"4552fa60":"code","9749a2d4":"code","c61357ec":"code","4e8880f6":"code","8918452c":"code","52bb3ef9":"code","60a36d1e":"code","cabe3b90":"code","499fffc8":"code","d28de2f7":"code","c1308535":"code","0535f76e":"code","1d603a69":"code","4b49b423":"code","7108163f":"code","d6612b46":"code","44087b43":"code","6bae81e9":"code","09be6a4f":"code","b414a914":"code","8fa0070b":"code","39c391af":"code","358c0dda":"code","56eccdcb":"code","17b7191f":"code","29185c79":"code","9713d2f6":"code","37c3f922":"code","92c6ec6e":"code","6f6a0607":"code","da64cf08":"code","3de7f0a4":"code","70135570":"code","5738e25a":"code","6331ea27":"code","cfb5f224":"code","b56a045c":"code","469cc34c":"code","847c7d28":"code","c92d78f3":"code","7c5ab287":"markdown","766836ea":"markdown","a2f505d1":"markdown","3dd0a570":"markdown","953d6027":"markdown","2ae361bb":"markdown"},"source":{"4552fa60":"### This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9749a2d4":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam","c61357ec":"tweet= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest= pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","4e8880f6":"tweet[tweet['target']==1]['text'].values[0]","8918452c":"tweet.head(10)","52bb3ef9":"test.head(10)","60a36d1e":"tweet.shape","cabe3b90":"x= tweet.target.value_counts()","499fffc8":"print(x)","d28de2f7":"sns.barplot(x.index,x)","c1308535":"y= tweet[tweet['target']==1]['text'].str.split()\nprint(y)","0535f76e":"word_len=tweet[tweet['target']==1]['text'].str.split().str.len()\nprint(word_len)","1d603a69":"plt.hist(word_len)","4b49b423":"def create_corpus(target):\n    corpus= []\n    \n    for l in y:\n        for q in l:\n            corpus.append(q)\n    return(corpus)","7108163f":"corpus= create_corpus(1)\n\ndic= defaultdict(int)\n\nfor i in corpus:\n    if i in stop:\n        dic[i]= dic[i]+1\ntop = sorted(dic.items(), key= lambda x: x[1], reverse= True)[:10]\nm,n =zip(*top)\nplt.bar(m,n)","d6612b46":"df= pd.concat([tweet, test])\ndf.shape","44087b43":"def sterilization(data):\n    \n    data = re.sub('https?:\/\/\\S+|www\\.\\S+', '', data)\n    data = re.sub('<.*?>', '', data)\n    emoj = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    emoj.sub(r'', data)\n    data = data.lower()\n    data = data.translate(str.maketrans('','', string.punctuation))\n    data = re.sub(r'\\[.*?\\]', '', data)\n    data = re.sub(r'\\w*\\d\\w*','', data)\n    \n\n    return data\n    \n    ","6bae81e9":"df['text']=df['text'].apply(lambda x : sterilization(x))\ndf.head(10)","09be6a4f":"from nltk.tokenize import RegexpTokenizer\n\ntokenizer = RegexpTokenizer(r'\\w+')\n\ndf[\"tokens\"] = df[\"text\"].apply(tokenizer.tokenize)\ndf.head()","b414a914":"word2vec_path = \"..\/input\/googles-trained-word2vec-model-in-python\/GoogleNews-vectors-negative300.bin\"\nword2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)","8fa0070b":"def filtered(text):\n    words= [w for w in text if w not in stop]\n    \n    return words","39c391af":"df['tokens']= df['tokens'].apply(lambda x: filtered(x))\ndf.head()","358c0dda":"def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n    if len(tokens_list)<1:\n        return np.zeros(k)\n    if generate_missing:\n        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n    else:\n        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n   \n    length = len(vectorized)\n    summed = np.sum(vectorized, axis=0)\n    averaged = np.divide(summed, length)\n    return averaged\n\ndef get_word2vec_embeddings(vectors, df, generate_missing=False):\n    embeddings = df['tokens'].apply(lambda x: get_average_word2vec(x, vectors, generate_missing=generate_missing))\n    return list(embeddings)","56eccdcb":"tweet.shape","17b7191f":"X_train= df[:7613]\ny_train= X_train['target']","29185c79":"X_test= df[7613:]","9713d2f6":"embeddings = get_word2vec_embeddings(word2vec, X_train)","37c3f922":"X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(embeddings, y_train, test_size= 0.2, random_state=40)","92c6ec6e":"from sklearn.linear_model import LogisticRegression\nclf_w2v = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n                         multi_class='multinomial', random_state=40)\nclf_w2v.fit(X_train_word2vec, y_train_word2vec)\ny_predicted_word2vec = clf_w2v.predict(X_test_word2vec)","6f6a0607":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n\ndef get_metrics(y_test, y_predicted):  \n    # true positives \/ (true positives+false positives)\n    precision = precision_score(y_test, y_predicted, pos_label=None,\n                                    average='weighted')             \n    # true positives \/ (true positives + false negatives)\n    recall = recall_score(y_test, y_predicted, pos_label=None,\n                              average='weighted')\n    \n    # harmonic mean of precision and recall\n    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n    \n    # true positives + true negatives\/ total\n    accuracy = accuracy_score(y_test, y_predicted)\n    return accuracy, precision, recall, f1\n\naccuracy_word2vec, precision_word2vec, recall_word2vec, f1_word2vec = get_metrics(y_test_word2vec, y_predicted_word2vec)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_word2vec, precision_word2vec, \n                                                                       recall_word2vec, f1_word2vec))","da64cf08":"test_embeds= get_word2vec_embeddings(word2vec, X_test)","3de7f0a4":"from xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline","70135570":"clf = XGBClassifier(colsample_bytree=0.7, learning_rate= 0.03, max_depth= 10,\n                    min_child_weight=11, missing= -999, n_estimators= 1200,\n                    nthread= 4, objective='binary:logistic', seed=1337, silent=1, subsample=0.8)","5738e25a":"XTa= np.array(X_train_word2vec)\nyTa= np.array(y_train_word2vec)","6331ea27":"clf.fit(XTa,yTa)","cfb5f224":"y_predicted_word2vec = clf.predict(X_test_word2vec)","b56a045c":"y_predicted_word2vec","469cc34c":"accuracy_word2vec, precision_word2vec, recall_word2vec, f1_word2vec = get_metrics(y_test_word2vec, y_predicted_word2vec)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_word2vec, precision_word2vec, \n                                                                       recall_word2vec, f1_word2vec))","847c7d28":"submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\n\nsubmission[\"target\"] = clf.predict(test_embeds)\n\nsubmission.to_csv(\"submission.csv\", index=False)","c92d78f3":"from IPython.display import FileLink\nFileLink(r'submission.csv')","7c5ab287":"## Download the generated CSV","766836ea":"## Logistic Regression","a2f505d1":"## XGBoost","3dd0a570":"**'filtered' function is for removing stop words**","953d6027":"## Taking average of all the vector weights of words present in a single tweet","2ae361bb":"**IMPORTING WORD2VEC pretrained model**"}}