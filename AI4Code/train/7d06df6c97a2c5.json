{"cell_type":{"8d5f1f19":"code","344b389c":"code","76c0c6d3":"code","64cd0f3d":"code","475c629a":"code","f226e3a7":"code","32aaeec6":"code","68cf6a61":"code","6e1e12d8":"code","ef848524":"code","8d7edcd8":"code","b511881d":"code","920f7639":"code","01b4a4e1":"code","6ecf0948":"code","c27cd8f7":"code","efd25ab6":"code","eb5076f9":"code","c998c4c9":"code","fcca8c1a":"code","8bc21348":"code","52306359":"markdown","9ddeefe4":"markdown","cedf7e8b":"markdown","2608f090":"markdown","25847664":"markdown","3bd4f0ef":"markdown","aa38a94f":"markdown","0c87b94a":"markdown","5f6c0366":"markdown","323c240c":"markdown","a1d6dcd3":"markdown","9108ff04":"markdown","67e91da7":"markdown","f7e78fa4":"markdown","ae655c0a":"markdown","5b2e7f95":"markdown","e108dcc0":"markdown","49320c99":"markdown"},"source":{"8d5f1f19":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #plotting\nimport seaborn as sns #plotting \nimport warnings\n\nfrom sklearn.model_selection import train_test_split #split training set\nfrom sklearn import metrics as ms  # MSE and accuracy\nfrom sklearn.preprocessing import StandardScaler #standardizing features\nfrom lightgbm import LGBMRegressor #Light Gradient Boost Regressor\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline\npd.set_option('display.max_columns', 200)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","344b389c":"df = pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv')","76c0c6d3":"df.shape","64cd0f3d":"nulls = pd.DataFrame(df.iloc[:, 8:].isnull().sum(), columns = ['missing_values'])\nnulls.sort_values(by='missing_values', ascending=False).T","475c629a":"df = df.drop(['feature_28', 'feature_17', 'feature_27', 'feature_18', 'feature_7',\n              'feature_8', 'feature_108', 'feature_114', 'feature_90', 'feature_96',\n              'feature_102', 'feature_78', 'feature_72', 'feature_84'], axis = 1)","f226e3a7":"df = df[df.weight != 0] # filtering our df\ndf['trade_return'] = df.weight * df.resp # trade_return column\ndf['action'] = df.resp.apply(lambda x: x > 0).astype(int) # this will be my \"second\" target","32aaeec6":"# I\u00b4ll drop ts_id column also, since I think that it\u00b4s useless for a first study\ndf.drop('ts_id', axis = 1, inplace = True)","68cf6a61":"df.shape # now wep\u00b4ve 1,9m samples and 125 features. ","6e1e12d8":"plt.figure(figsize=(15,15))\nsns.heatmap(df.drop(['action', 'date'], axis = 1).corr(), cmap='inferno');","ef848524":"X = df.iloc[:, 8:-2] # Select only feature columns\ny = df.iloc[:, -2:] # return per trade and action column","8d7edcd8":"# Action is based on resp, and resp*weight gives return. I\u00b4ll work with return target instead of action as said before\nXtrain, Xval, ytrain, yval = train_test_split(X, y.trade_return, test_size = .2, random_state=0)","b511881d":"Xtrain = Xtrain.fillna(value = Xtrain.median()) # Median is quite better than avg for this case\nXval = Xval.fillna(value = Xval.median()) # I\u00b4m avoiding information leakage here (from train to val)","920f7639":"Xtrain.shape","01b4a4e1":"# Creating a distplot for each of the variables \nplt.figure(figsize=(30, 70))\nfor i in range(1, len(Xtrain.columns)+1):\n\n  plt.subplot(58, 2, i)\n  sns.distplot(Xtrain.iloc[:5000, i-1], bins = 70, kde=True) # using only first 5000 rows\n\nplt.subplots_adjust(hspace=1.25, wspace=.2);","6ecf0948":"baseline = np.zeros_like(yval) # np.array with the same dimensions as yval\nbaseline += ytrain.mean() # broadcasting it with all elements equallying ytrain mean\n\n# Any model worst than RMSE baseline should be ignored\nprint ('Baseline RMSE:', np.sqrt(ms.mean_squared_error(yval, baseline))*100,'%')","c27cd8f7":"scaler = StandardScaler()\nscaler.fit(Xtrain)\n\nXtrain_std = scaler.transform(Xtrain)\nXval_std = scaler.transform(Xval)","efd25ab6":"lgb = LGBMRegressor(num_leaves=30, n_estimators=400, max_depth=10) # Without any hyperparameter opt.","eb5076f9":"lgb.fit(Xtrain_std, ytrain)\npredlgb = lgb.predict(Xval_std)\nprint ('Light GBM RMSE:', np.sqrt(ms.mean_squared_error(yval, predlgb))*100,'%')","c998c4c9":"predictions = pd.DataFrame() # blank dataframe\npredictions['pred_lgb'] = predlgb # creating a column that contain return predictions made before\n\ndf1 = pd.DataFrame(yval)\ndf1['action'] = [1 if p > 0 else 0 for p in df1.trade_return] # as resp defines action, whenever resp>0, action > 0.\ndf1.reset_index(inplace=True)\ndf1.drop('index', axis = 1, inplace = True)\n\npredictions = pd.concat([df1, predictions], axis = 1) # concatenating both dataframes","fcca8c1a":"predictions['lgb_action'] = [1 if p > 0 else 0 for p in predictions.pred_lgb]\npredictions['pred_baseline'] = 1 # cause the average return from ytrain is > 0\n\npredictions.head()","8bc21348":"print ('Baseline Accuracy:', ms.accuracy_score(predictions.action, predictions.pred_baseline)*100,'%')\nprint ('-'*50)\nprint ('Light Gradient Boosting Accuracy:', ms.accuracy_score(predictions.action, predictions.lgb_action)*100,'%')","52306359":"Now let\u00b4s create action predictions based on LGB return predictions and compare it to the baseline ","9ddeefe4":"I made a first data preprocessing when dropped some of the features above. But I did not impute any missing values yet. Now, I\u00b4ll do it using median, as I think that the remaining features won\u00b4t be impacted by imputing median","cedf7e8b":"# Standardizing data","2608f090":"# Now, classification  \nAs we\u00b4ve already done predictions of returns, we can try to classify action target based on trade_return criteria. If it\u00b4s greater than zero, action = 1, else, action = 0. ","25847664":"It\u00b4s a good idea to have a look at the correlations between features.","3bd4f0ef":"# A very brief EDA","aa38a94f":"# Regression instead of Classification  \nI start thinking about this problem mainly as a regression instead of classification. If we change the primary target to pursue a return? One should pay attention to the fact that, as an arbitrageur on high frequency trading, the most important thing is not the action of trade or not itself, but the return impact of each trade. \n\nBased on that predicted return per trade, it can be much easier to guide a decision about action. In this notebook my aim is to give a **brief** description of another way to look at the same problem. If you enjoy, upvote and comment below!","0c87b94a":"Let\u00b4s see the number of null values for each feature","5f6c0366":"Let\u00b4s get a visual clue about the features on the training set. Remember that we should do our EDA on train set and use validation set only after dealing with the training one.","323c240c":"We can see that the LGM model performed very good if considered that few EDA were done before and the hyperparameters weren\u00b4t optimized. Be creative, try another ways to deal with this problem!","a1d6dcd3":"Now, the complete train set is full of weight-zero rows. They do not change our return target, so, let\u00b4s get rid of them filtering df and after that, I\u00b4ll create a trade_return column that equals $response * weight$","9108ff04":"# Constructing a baseline","67e91da7":"# LightGBM Regressor  \nI think that a good start is LGBM, as it\u00b4s pretty fast and robust","f7e78fa4":"With a total of 2,4 million samples, I think that imputing the features with more than 10% missing values with median or something like will not improve that much our analysis. So, I decided to drop some of these features","ae655c0a":"Finally, let\u00b4s evaluate the accuracy of them. Remember that you can also try using different regression models, hyperparameter optimization on LGB, XGBoost or another, and even try to run many models and create a final ensemble of them, using stacking, average voting or mode voting!","5b2e7f95":"Any of the features alone cannot give good correlations with trade_return. That\u00b4s previously expected, as if arbitrage could be succesfully done with only one feature, everyone would succeed on investing.","e108dcc0":"# Splitting the train set","49320c99":"Almost all of the features seems to have a mean close to zero. But not all of them. Otherwise, some features have different range of values, and that may be a problem when doing regressions without standardizing them. Let\u00b4s do it after creating a baseline model"}}