{"cell_type":{"5557ee0d":"code","8df2c85f":"code","39e2a4bc":"code","95f79f29":"code","199ddfce":"code","b3b8bffa":"code","5978f762":"code","eca1fc18":"code","aeb8cae4":"code","9d09c2d1":"code","5e73ed7d":"code","a36b37b0":"code","fcb4da86":"markdown","c1cfbfb3":"markdown","11d69044":"markdown","a1381cbd":"markdown","652c07c2":"markdown","87daaa04":"markdown","8c47246f":"markdown","99ef7e85":"markdown","b0c5a23e":"markdown"},"source":{"5557ee0d":"import numpy as np\nimport pandas as pd\nimport torch\nimport torchvision\nfrom torchvision import transforms, datasets\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","8df2c85f":"def generate_sequence(n):\n    bit_seq = []\n    for i in range(n):\n        bit = np.random.randint(0,2)\n        bit_seq.append(bit)\n    return bit_seq\n\ndef parity(sequence):\n    k = 1\n    for bit in sequence:\n        if(bit == 0):\n            bit = -1\n        k*= bit\n    if(k == -1):\n        return 0\n    return 1\n\ndef generate_set(m,n):\n    #m = no of bit sequences\n    #n = length of bit sequences\n    dataset = []\n    for i in range(m):\n        dataset.append(generate_sequence(n))\n    return dataset\n\ndef cross_validate(model, X_test, y_test):\n    count = 0\n    correct = 0\n    for i in range(len(X_test)):\n        output = model(X_test[i])\n        if(y_test[i]==torch.round(output)):\n            correct +=1\n        count+=1\n    return correct\/count\n\ndef training_accuracy(X,y,model):\n    correct = 0\n    for i in range(len(X)):\n        output = model(X[i])\n        if(y[i]==torch.round(output)):\n            correct += 1\n    return correct\/len(X)","39e2a4bc":"dataset = generate_set(5,6)\n\nprint(\"Dataset: \\n\")\n\nfor seq in dataset:\n    print(\"Bit Sequence: \",seq,\"\\nParity:\",parity(seq),\"\\n\")","95f79f29":"class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(64,16)\n        \n        self.fc2 = nn.Linear(16,8)\n        \n        self.fc3 = nn.Linear(8,1)\n        \n    def forward(self,x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.sigmoid(self.fc3(x))\n        return x\n        \nnet = Net()\nnet.to(device)\nprint(net)","199ddfce":"class Deep_Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(64,32)\n        self.fc2 = nn.Linear(32,16)\n        self.fc3 = nn.Linear(16,8)\n        self.fc4 = nn.Linear(8,4)\n        self.fc5 = nn.Linear(4,2)        \n        self.fc6 = nn.Linear(2,1)\n        self.drop = nn.Dropout(0.3)\n        \n    def forward(self,x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.drop(self.fc3(x)))\n        x = F.relu(self.fc4(x))\n        x = F.relu(self.fc5(x))\n        x = F.sigmoid(self.fc6(x))\n        return x\n        \ndeep_net = Deep_Net()\ndeep_net.to(device)\nprint(deep_net)","b3b8bffa":"def train_and_cross_validate(model, X_train, y_train, epochs=150, alpha=1e-3):\n    \n    # m - size of the training set\n    optimizer = optim.Adam(model.parameters(), lr = alpha)\n    for epoch in range(epochs):\n        for i in range(len(X_train)):\n            X = X_train[i]\n            y = y_train[i]\n\n            model.zero_grad()\n\n            y = y.view(-1,1)\n            output = model(X.view(-1,64))\n\n            criterion = nn.BCELoss()\n\n            loss = criterion(output,y)\n\n            loss.backward()\n\n            optimizer.step()\n    \n        if((epoch+1) in [1,5,10,25,50,75,100,150]):\n            print(\"Epoch:\", epoch+1)\n            print(\"Training Loss:\",loss.item())\n            print(\"Training Accuracy:\", training_accuracy(X_train,y_train,model))\n            print(\"CrossVal Accuracy:\", cross_validate(model, X_test,y_test),\"\\n\")","5978f762":"    X_train = generate_set(2000,64)\n    y_train = [parity(x) for x in X_train]\n\n    X_train = torch.FloatTensor(X_train)\n    y_train = torch.FloatTensor(y_train)\n    \n    X_train, y_train = X_train.to(device), y_train.to(device)\n    \n    X_test = generate_set(100,64)\n    y_test = [parity(x) for x in X_test]\n    \n    X_test = torch.FloatTensor(X_test)\n    y_test = torch.FloatTensor(y_test)\n    \n    X_test, y_test = X_test.to(device), y_test.to(device)","eca1fc18":"train_and_cross_validate(net,X_train,y_train)","aeb8cae4":"train_and_cross_validate(deep_net,X_train,y_train)","9d09c2d1":"    X_train = generate_set(5000,64)\n    y_train = [parity(x) for x in X_train]\n\n    X_train = torch.FloatTensor(X_train)\n    y_train = torch.FloatTensor(y_train)\n    \n    X_train, y_train = X_train.to(device), y_train.to(device)\n    \n    X_test = generate_set(100,64)\n    y_test = [parity(x) for x in X_test]\n    \n    X_test = torch.FloatTensor(X_test)\n    y_test = torch.FloatTensor(y_test)\n    \n    X_test, y_test = X_test.to(device), y_test.to(device)","5e73ed7d":"train_and_cross_validate(net,X_train,y_train)","a36b37b0":"train_and_cross_validate(deep_net,X_train,y_train, alpha = 1e-4)","fcb4da86":"## Training Set of Size 2000","c1cfbfb3":"# Neural Network Architecture 2","11d69044":"# Training and Cross Validation","a1381cbd":"### Sanity Check - First 3 functions","652c07c2":"## Training Set of Size 5000","87daaa04":"# Defining Useful Functions\n\n1. generate_sequence(n) - Generating a bit sequence of length n\n2. parity(sequence) - Checks the parity of a bit sequence (Returns 1 if even, 0 if odd)\n3. generate_set(m,n) - Creates a dataset of m points which are bit sequences of length n\n4. cross_validate(m,n,net) - Checks the accuracy of \"net\" neural network on a test set of size m\n5. training_accuracy(X,y,net) - Checks the accuracy of \"net\" neural network on training set (X,y)","8c47246f":"# Neural Network Architecture 1","99ef7e85":"### Note\nMany combinations of neural network architectures were tried (with and without dropout regularization) but we show two examples (one 6 layered and another 3 layered) to illustrate the point as in Part 3 of the assignment (Manual Network for Parity Function), a 6 layered network should be sufficient to model a parity function using a neural network.","b0c5a23e":"We observed that even on wide range of network architectures and learning hyperparameters, the neural networks are overfitting to the training set of 2000 or 5000 and not improving the cross validation accuracy. This may be because the weights converge to a local minimum.\n\nThe parity function isn't properly captured by the neural network (due to 2^64 possibilities) and it's clear from the cross validation scores that the predictions are only as good as random guessing when it comes to an unseen test set  (Close to 50% accuracy)"}}