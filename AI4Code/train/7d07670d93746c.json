{"cell_type":{"cc91add6":"code","dad1cf65":"code","e050873f":"code","e351accb":"code","9a4b6d2d":"code","9f83a82a":"code","ab104002":"code","1edb1645":"code","0d8cef7c":"code","31f4e680":"code","5cea4211":"code","70a32b78":"code","565da7c3":"code","6693405c":"code","020b7cc5":"code","cdcb8b79":"markdown","fb7625ff":"markdown","1c4077ed":"markdown","d80cc826":"markdown","3a8037bb":"markdown","193922f1":"markdown","a33d5205":"markdown","138fc617":"markdown","e883fc01":"markdown","96de1357":"markdown","e3cf5482":"markdown","aacc6d4d":"markdown"},"source":{"cc91add6":"import pandas as pd\nfrom nltk.corpus import stopwords\nimport re\nimport os\nprint(os.listdir(\"..\/input\"))","dad1cf65":"df_train = pd.read_csv(\"..\/input\/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\ndf_train.head()","e050873f":"X = df_train.iloc[:, 2].values\ny = df_train.iloc[:, 1].values","e351accb":"df_test = pd.read_csv(\"..\/input\/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\nX_1 = df_test.iloc[:, 1].values","9a4b6d2d":"def review_to_words(raw_review):\n    letters_only = re.sub(\"[^a-zA-Z]\",\" \", raw_review)\n    lower_words = letters_only.lower().split()\n    stops = set(stopwords.words(\"english\"))\n    words = [word for word in lower_words if word.isalpha()] #removing special character and numbers\n    meaningful_words = [ w for w in words if not w in stops]\n    return(\" \".join(meaningful_words))","9f83a82a":"filtered_x = []\ntotal_reviews = X.size  #total number of reviews present or number of rows\nfor i in range(0,total_reviews):\n    filtered_x.append(review_to_words(X[i]))","ab104002":"from sklearn.model_selection import train_test_split\nX_train, X_val, Y_train, Y_val = train_test_split( filtered_x, y, test_size = 0.2, random_state = 0)\nx_test = df_test[\"review\"].map(review_to_words)","1edb1645":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","0d8cef7c":"tokenizer = Tokenizer(num_words=2000) #tokenised to 2000 most frequent words\ntokenizer.fit_on_texts(filtered_x)\n# padding sequence to the limit is 500 words so it will look 500 words back \ntrain_reviews_tokenized = tokenizer.texts_to_sequences(X_train)\nX_train = pad_sequences(train_reviews_tokenized, maxlen=400)\nval_review_tokenized = tokenizer.texts_to_sequences(X_val)\nX_val = pad_sequences(val_review_tokenized, maxlen=400)\ntest_review_tokenized = tokenizer.texts_to_sequences(x_test)\nx_test = pad_sequences(test_review_tokenized, maxlen=400)","31f4e680":"from keras.models import Sequential\nfrom keras.layers import Dense, Embedding\nfrom keras.layers import LSTM","5cea4211":"model = Sequential()\nmodel.add(Embedding(20000,128)) #20000 words and funneling them into 128 hidden neurons\nmodel.add(LSTM(128,dropout = 0.2, recurrent_dropout = 0.2))\nmodel.add(Dense(1, activation = \"sigmoid\"))\n#compiling model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","70a32b78":"model.fit(X_train, Y_train, batch_size = 32, epochs = 5, validation_data=[X_val, Y_val])","565da7c3":"prediction = model.predict(x_test)\ny_pred = (prediction > 0.5)","6693405c":"df_test[\"sentiment\"] = df_test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)\ny_test = df_test[\"sentiment\"]","020b7cc5":"from sklearn.metrics import f1_score, confusion_matrix\nprint('F1-score: {0}'.format(f1_score(y_pred, y_test)))\nprint('Confusion matrix:')\nconfusion_matrix(y_pred, y_test)","cdcb8b79":"Here we converted all our words to numbers so that our model can understand","fb7625ff":"It's time to build our RNN model we use sequential model and for layers we use Dense and Embedding and LSTM layers we can import all these from keras \nif you dont have all these then use pip install keras and you are good to go.","1c4077ed":"we import Tokenizer and pad_sequence from keras.preprocessig Tokenizer is used for text preprocessing.\nwe tokenize the words in numeric values here we can choose how many words we want to tokenize so we choose 2000 so most frequenty comming 2000 words will be tokenized.\n\n**how tokenizer works**\nThe Tokenizer stores everything in the word_index during fit_on_texts. Then, when calling the texts_to_sequences method, only the top num_words are considered.\n\nThen we pad the sequence by importing pad_sequence it is  used to ensure that all sequences in a list have the same length. here wh choose maxlen = 400 you can choose any other value ","d80cc826":"here we divided our training dataset into x and y where x is review and y is its curresponding sentiment size of x is (25000,1) and size of y is (25000,1) you can check size by x.shape command.","3a8037bb":"created a empty list filtered_x and stored the size of X which is out training dataset in total_review, then we apply a for loop to filtere all the reviews present in the training data. ","193922f1":"Importe train_test_split from sklearn.model_selection so that we can split our training and test data. here we choose the ratio is 80:20 80% training set and 20% validation set.\nif you don't have sklearn then use pip install sklearn and you are good to go. ","a33d5205":"Here we have created a function review_to_words to clean the review words from our review section we remove stop words then we remove all special characters and keep only words. line by line explanation\n\n**line1** ----  *\"re.sub(\"[^a-zA-Z]\",\" \", raw_review)\"* in this line we will keep all the alphabetical words which are present in the file name raw_review all special characters are replaced by a space. \n\n**line2** ---- * letters_only.lower().split()* convert the string into lowercase string then we use split() which will split the string and return a list of words.\n\n**line3** ----  *set(stopwords.words(\"english\"))* create a touple of stop words which are present in nltk stopword library\n\n**line4** ----  * [word for word in lower_words if word.isalpha()]* if any special character is left we will remove that by creating a  list comprehension and checking ever word.\n\n**line 4** ---- * [ w for w in words if not w in stops]*  here we keep only those words which are not  present in the stop word touple.\n\n**line 5** ----  *\" \".join(meaningful_words) * joining all the words back and making a string again.\n","138fc617":"we use pd.read_csv file to create to test and training data set then we use train.head() to take a look at our dataset so that we will know that which colunm contain what values.","e883fc01":"**Embedding** --- The weights of the Embedding layer are of the shape (vocabulary_size, embedding_dimension). For each training sample, its input are integers,so here our vocabulary size is 2000 and we choose 128 embedding_dimensions we can also call thses as hidden neurons.\n\n**LSTM dropout explanation:**\ndropout: Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs.\nrecurrent_dropout: Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state.\nin recurrent dropout the connections between the recurrent units will be dropped we we have choose 20% of the linear and recurrent connection will dropout every iteration.\n\nwe have used sigmoid activation because its a binary classification function.and loss is calculated by binary_crossentropy function and here we have used adam optimizer.it is one of the best optimizer present in keras to tackle classification problems.","96de1357":"Here read our test_data and store it in varaible df_test and we store the reviews of test data as X_1 ","e3cf5482":"First we import important packages like **pandas,nltk,re,os** we use pandas to handle our dataset it is used to take input of test and training data then we import stopwords to remove usnecessary words like is,are,names etc from the dataset we use re to keep only words i will explain this in details where we use re. then we import os for setting directory\n#if you dont have any of these files then you can download these files from command prompt by using pip install module name\nfor pandas --- pip install pandas\nfor nltk ---- pip install nltk then you have to download stopwords by going to python editor and import nltk then nltk.download() select all from gui or you can make custom download i suggest you to download all.\nRest are inbuilt in python(excluding keras i explained thoses below) just import and enjoy.","aacc6d4d":"now we fit our model with a batch size of 32 and number of epoch is 5 after this training will start."}}