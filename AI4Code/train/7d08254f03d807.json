{"cell_type":{"aacdf833":"code","74e87595":"code","1711bebd":"code","ae55dcec":"code","a8806378":"code","9b2c7d02":"code","f2cb86f8":"code","df99b1cc":"code","7d19b8ea":"code","568d9de3":"code","054d40fa":"code","6ec89309":"code","efe36277":"code","b4ce9321":"code","8c86ea02":"code","50dd187f":"code","b5c5fc96":"code","6e79ebbd":"code","b35d5497":"code","20f6140c":"code","ced2a721":"code","4d7521d4":"code","9319fbf8":"code","04e776b6":"code","5854c59a":"code","bd8b35ea":"code","3d1af494":"code","7ba372d8":"code","03dfa23f":"code","16a578b0":"code","adedf091":"code","ed437241":"code","80b08067":"code","e9599047":"code","ee4f61f4":"code","a516f2f2":"code","32a984b0":"code","acba9fae":"code","69533431":"code","e088da10":"code","f07cd28b":"code","5a859470":"code","5eb4525b":"code","e0305b40":"markdown","1228eeca":"markdown","278a2e98":"markdown","92f42972":"markdown","04599757":"markdown","99ed44b8":"markdown","681b238f":"markdown","fba5ccb0":"markdown","9fe0d773":"markdown","0f87fc29":"markdown","6a9bfea3":"markdown","c6e2e72a":"markdown","0596b168":"markdown","61311915":"markdown","9232a2b0":"markdown","58f7f3ee":"markdown","26a90f25":"markdown","f3d30e5b":"markdown","680f9564":"markdown","2829e245":"markdown","977547dc":"markdown","aa7a5c2a":"markdown","b6d2d281":"markdown","184fefb6":"markdown","9a02ed6e":"markdown","c7c9caeb":"markdown","30916733":"markdown","0c702873":"markdown"},"source":{"aacdf833":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\nimport os\nimport pathlib\nimport PIL\nimport time\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, Flatten\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\n\nfrom os import listdir\nfrom os.path import isfile, join\nfrom keras import backend as K\n\n","74e87595":"def getTime(startTime, endTime):\n    \n    \"\"\"Converts elapsed time in HH:MM:SS format\"\"\"\n    \n    elapsedTime = (endTime - startTime)\/3600\n    \n    hours = int(elapsedTime)\n    minutes = int((elapsedTime - hours)*60)\n    seconds = round((((elapsedTime - hours)*60) - minutes)*60)\n    print(\"{}:{}:{} --> format HH:MM:SS\".format(hours, minutes, seconds))\n    #return hours, minutes, seconds\n    \n\ndef plottingResults(modelName):\n    \n    print(\"\\n========================================\")\n    print(\"=====        Plotting {}         =====\".format(modelName.name))\n    print(\"========================================\")\n    \n    \n    history_dict = history.history\n    print(history_dict.keys())\n\n    acc      = history.history[     'accuracy']\n    val_acc  = history.history[ 'val_accuracy']\n    loss     = history.history[    'loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(1, len(acc)+ 1, 1)\n \n    \n    plt.figure()\n    plt.plot(epochs, acc, 'r--', label='Training acc')\n    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.ylabel('acc')\n    plt.xlabel('epochs')\n    plt.legend()\n\n    \n    plt.figure()    \n    plt.plot(epochs, loss, 'r--',label='Training loss')\n    plt.plot(epochs, val_loss, 'b',label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.ylabel('acc')\n    plt.xlabel('epochs')\n    #plt.yscale('log')\n    plt.legend()\n\n    plt.show()","1711bebd":"# Loading data uploaded in Google Drive\n\n#from google.colab import drive\n#drive.mount('\/content\/drive')","ae55dcec":"# Listing files in the Google Drive folder\n!ls","a8806378":"# Defining directories and subdirectories related to train, validation and test datasets\n\nbase_dir = '..\/input\/weatherdataset20200105'\n\ntrain_dir = os.path.join(base_dir, 'Training')\nvalidation_dir = os.path.join(base_dir, 'Validation')\ntest_dir = os.path.join(base_dir, 'Test')\n\n# Directory for training's dataset\ntrain_c1_dir = os.path.join(train_dir, 'DayCloudy')\ntrain_c2_dir = os.path.join(train_dir, 'DayRainy')\ntrain_c3_dir = os.path.join(train_dir, 'NightClear')\ntrain_c4_dir = os.path.join(train_dir, 'NightRainy')\n\n# Directory for validation's dataset\nvalidation_c1_dir = os.path.join(validation_dir, 'DayCloudy')\nvalidation_c2_dir = os.path.join(validation_dir, 'DayRainy')\nvalidation_c3_dir = os.path.join(validation_dir, 'NightClear')\nvalidation_c4_dir = os.path.join(validation_dir, 'NightRainy')\n\n# Directory for test' dataset\ntest_c1_dir = os.path.join(test_dir, 'DayCloudy')\ntest_c2_dir = os.path.join(test_dir, 'DayRainy')\ntest_c3_dir = os.path.join(test_dir, 'NightClear')\ntest_c4_dir = os.path.join(test_dir, 'NightRainy')\n\nprint(\"Finished!!!\")","9b2c7d02":"\"\"\"\n# Loading data uploaded in Kaggle\n\n# Dimension of the images\nimg_height = 331\nimg_width = 331\nchannels = 3\n\nbatch_size = 4\n\nimage_size = (img_height, img_width)\n\ndata_dir = \"..\/input\/weatherdatasettiny\/TrainingDataset\"\ntarget_path = \"..\/input\/weatherdatasettiny\/TestingDataset\/\"\n\n\ndata_dir = pathlib.Path(data_dir)\n\n\"\"\"","f2cb86f8":"\"\"\"\n# Creating a training dataset from the data\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    validation_split = 0.2,\n    subset = \"training\",\n    color_mode = 'rgb',\n    seed = 123,\n    image_size = (img_height, img_width),\n    batch_size = batch_size)\n\n\"\"\"","df99b1cc":"\"\"\"\n# Creating a validation dataset from the data\n\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    validation_split=0.2,\n    subset=\"validation\",\n    color_mode='rgb',\n    seed=123,\n    image_size=image_size,\n    batch_size=batch_size)\n\nclass_names = train_ds.class_names\nprint(class_names)\n\n\"\"\"","7d19b8ea":"# Checking some file names and number of images in the subdirectories\nprint(\"Some filenames in the training subdirectory\")\ntrain_c1_fnames = os.listdir(train_c1_dir)\nprint(train_c1_fnames[:5])\n\ntrain_c2_fnames = os.listdir(train_c2_dir)\nprint(train_c2_fnames[:5])\n\ntrain_c3_fnames = os.listdir(train_c3_dir)\nprint(train_c3_fnames[:5])\n\ntrain_c4_fnames = os.listdir(train_c4_dir)\nprint(train_c4_fnames[:5])\n\nprint(\"\\nSome filenames in the validation subdirectory\")\nvalidation_c1_fnames = os.listdir(validation_c1_dir)\nprint(validation_c1_fnames[:5])\n\nvalidation_c2_fnames = os.listdir(validation_c2_dir)\nprint(validation_c2_fnames[:5])\n\nprint(\"\\nSome filenames in the validation subdirectory\")\nvalidation_c3_fnames = os.listdir(validation_c3_dir)\nprint(validation_c3_fnames[:5])\n\nvalidation_c4_fnames = os.listdir(validation_c4_dir)\nprint(validation_c4_fnames[:5])\n\nprint(\"\\nSome filenames in the test subdirectory\")\ntest_c1_fnames = os.listdir(test_c1_dir)\nprint(test_c1_fnames[:5])\n\ntest_c2_fnames = os.listdir(test_c2_dir)\nprint(test_c2_fnames[:5])\n\ntest_c3_fnames = os.listdir(test_c3_dir)\nprint(test_c3_fnames[:5])\n\ntest_c4_fnames = os.listdir(test_c4_dir)\nprint(test_c4_fnames[:5])\n\nprint('\\n\\ntotal training c1 images:',\n     len(train_c1_fnames))\nprint('total training c2 images:',\n     len(train_c2_fnames))\nprint('total training c3 images:',\n     len(train_c3_fnames))\nprint('total training c4 images:',\n     len(train_c4_fnames))\n\nprint('\\ntotal training c1 images:',\n     len(validation_c1_fnames))\nprint('total training c2 images:',\n     len(validation_c2_fnames))\nprint('total training c3 images:',\n     len(validation_c3_fnames))\nprint('total training c4 images:',\n     len(validation_c4_fnames))\n\nprint('\\ntotal test c1 images:',\n     len(test_c1_fnames))\nprint('total test c2 images:',\n     len(test_c2_fnames))\nprint('total test c3 images:',\n     len(test_c3_fnames))\nprint('total test c4 images:',\n     len(test_c4_fnames))","568d9de3":"#plt.figure(figsize=(10, 10))\n#for images, labels in train_ds.take(1):\n#  for i in range(9):\n#    ax = plt.subplot(3, 3, i + 1)\n#    plt.imshow(images[i].numpy().astype(\"uint8\"))\n#    plt.title(class_names[labels[i]])\n#    plt.axis(\"off\")","054d40fa":"def print_pictures(dir, fnames):\n    # Displaying images in a matrix format (displaying 4 rows with 4 images in each row)\n    nrows = 4\n    ncols = 4\n    \n    pic_index = 0\n    \n    fig = plt.gcf()\n    fig.set_size_inches(ncols*4, nrows*4)\n    \n    pic_index+=8\n    \n    next_pix = [os.path.join(dir, fname)\n               for fname in fnames[pic_index - 8:pic_index]\n               ]\n    \n    for i, img_path in enumerate(next_pix):\n        sp = plt.subplot(nrows, ncols, i + 1)\n        img = mpimg.imread(img_path)\n        plt.imshow(img)\n    \n    plt.show()","6ec89309":"print(\"Figura DayCloudy\")\nprint_pictures(train_c1_dir, train_c1_fnames)\n\nprint(\"Figura DayRainy\")\nprint_pictures(train_c2_dir, train_c2_fnames)\n\nprint(\"Figura NightClear\")\nprint_pictures(train_c3_dir, train_c3_fnames)\n\nprint(\"Figura NightRainy\")\nprint_pictures(train_c4_dir, train_c4_fnames)\n\n#---------------------------------------------------\nprint(\"Figura DayCloudy\")\nprint_pictures(validation_c1_dir, validation_c1_fnames)\n\nprint(\"Figura DayRainy\")\nprint_pictures(validation_c2_dir, validation_c2_fnames)\n      \nprint(\"Figura NightClear\")\nprint_pictures(validation_c3_dir, validation_c3_fnames)\n\nprint(\"Figura NightRainy\")\nprint_pictures(validation_c4_dir, validation_c4_fnames)\n      \n#---------------------------------------------------\n\nprint(\"Figura DayCloudy\")\nprint_pictures(test_c1_dir, test_c1_fnames)\n\nprint(\"Figura DayRainy\")\nprint_pictures(test_c2_dir, test_c2_fnames)\n      \nprint(\"Figura NightClear\")\nprint_pictures(test_c3_dir, test_c3_fnames)\n\nprint(\"Figura NightRainy\")\nprint_pictures(test_c4_dir, test_c4_fnames)      ","efe36277":"\"\"\"\nfor image_batch, labels_batch in train_ds:\n  print(image_batch.shape)\n  print(labels_batch.shape)\n  break\n\"\"\"","b4ce9321":"#labels_batch","8c86ea02":"#AUTOTUNE = tf.data.experimental.AUTOTUNE\n\n#train_generator = train_generator.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n#val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)","50dd187f":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nclass_mode='sparse'\n\ntrain_datagen = ImageDataGenerator(rescale = 1.0\/255. )\nvalidation_datagen = ImageDataGenerator(rescale = 1.0\/255. )\ntest_datagen = ImageDataGenerator(rescale = 1.0\/255. )\n\ntrain_generator = train_datagen.flow_from_directory(train_dir,\n                                                   batch_size=20,\n                                                   class_mode=class_mode,\n                                                   target_size=(150,150))\n\nvalidation_generator = validation_datagen.flow_from_directory(validation_dir,\n                                                   batch_size=20,\n                                                   class_mode=class_mode,\n                                                   target_size=(150,150))\n\ntest_generator = test_datagen.flow_from_directory(test_dir,\n                                                   batch_size=20,\n                                                   class_mode=class_mode, \n                                                   target_size=(150,150))","b5c5fc96":"train_generator?","6e79ebbd":"\"\"\"\nnormalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1.\/255)\n\nnormalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n\nimage_batch, labels_batch = next(iter(normalized_ds))\n\nfirst_image = image_batch[0]\n\n# Notice the pixels values are now in `[0,1]`.\nprint(np.min(first_image), np.max(first_image))\n\"\"\"\n\n\n","b35d5497":"#print(image_batch)","20f6140c":"# Number of Classes, total 3 in this case for Cats, Dogs and Pandas\n#num_classes = len(class_names)","ced2a721":"# Dimension of the images\nimg_height = 150\nimg_width = 150\nchannels = 3\n\ninput_shape = (img_height, img_width, channels)\n\nnum_classes = 4","4d7521d4":"model_AlexNet = tf.keras.Sequential([\n                                     #C1\n                                     tf.keras.layers.Conv2D(filters=96,\n                                                            kernel_size=11,\n                                                            strides=4,\n                                                            activation='relu',\n                                                            input_shape=input_shape),\n                                     # Normalization layer\n                                     tf.keras.layers.BatchNormalization(),\n                                     \n                                     #S2\n                                     tf.keras.layers.MaxPool2D(pool_size=3, \n                                                               strides=2),\n                                     #C3\n                                     tf.keras.layers.Conv2D(filters=256,\n                                                            kernel_size=5,\n                                                            strides=1,\n                                                            padding='same',\n                                                            activation='relu'),\n                                     \n                                     # Normalization layer\n                                     tf.keras.layers.BatchNormalization(),\n\n                                     #S4\n                                     tf.keras.layers.MaxPool2D(pool_size=3,\n                                                               strides=2,\n                                                               padding='valid'),\n                                     #C5\n                                     tf.keras.layers.Conv2D(filters=384,\n                                                            kernel_size=3,\n                                                            strides=1,\n                                                            padding='same',\n                                                            activation='relu'),\n                                     # Normalization layer\n                                     tf.keras.layers.BatchNormalization(),\n\n                                     #C6\n                                     tf.keras.layers.Conv2D(filters=384,\n                                                            kernel_size=3,\n                                                            strides=1,\n                                                            padding='same',\n                                                            activation='relu'),\n                                     # Normalization layer\n                                     tf.keras.layers.BatchNormalization(),\n\n                                     #C7\n                                     tf.keras.layers.Conv2D(filters=256,\n                                                            kernel_size=3,\n                                                            strides = 1, \n                                                            padding='same',\n                                                            activation='relu'),\n                                     # Normalization layer\n                                     tf.keras.layers.BatchNormalization(),\n                                     \n                                     #S8\n                                     tf.keras.layers.MaxPool2D(pool_size=3,\n                                                               strides=2),\n                                     #Flattening previous layers\n                                     tf.keras.layers.Flatten(),\n\n                                     #FD9\n                                     tf.keras.layers.Dense(4096, activation='relu', kernel_regularizer =tf.keras.regularizers.l1( l=0.01)),\n                                     \n                                     #Dropout layer 1\n                                     tf.keras.layers.Dropout(0.2),\n\n                                     #FD10\n                                     tf.keras.layers.Dense(4096, activation='relu', kernel_regularizer =tf.keras.regularizers.l1( l=0.01)),\n                                     \n                                     #Dropout layer 2\n                                     tf.keras.layers.Dropout(0.5),\n\n                                     #Output Layer\n                                     tf.keras.layers.Dense(num_classes, activation='softmax')\n], name=\"AlexNet\")\n\nprint(\"Architecture Implemented\")","9319fbf8":"model_VGG16 = tf.keras.Sequential(name=\"VGG16\")\n\nmodel_VGG16.add(\n    tf.keras.applications.VGG16(\n        include_top = False,\n        weights = 'imagenet',\n        pooling = 'avg',\n        input_shape = input_shape)\n)\nmodel_VGG16.add(layers.Flatten())\nmodel_VGG16.add(layers.Dropout(0.5))\nmodel_VGG16.add(layers.Dense(num_classes, activation='softmax', kernel_regularizer =tf.keras.regularizers.l1( l=0.01)))\n\nprint(\"Architecture Implemented\")","04e776b6":"model_InceptionV3 = tf.keras.Sequential(name=\"InceptionV3\")\n\nmodel_InceptionV3.add(\n    tf.keras.applications.InceptionV3(\n        include_top = False,\n        weights = 'imagenet',\n        pooling = 'avg',\n        input_shape = input_shape)\n)\nmodel_InceptionV3.add(layers.Flatten())\nmodel_InceptionV3.add(layers.Dropout(0.5))\nmodel_InceptionV3.add(layers.Dense(num_classes, activation='softmax'))\n\nprint(\"Architecture Implemented\")","5854c59a":"model_ResNet50V2 = tf.keras.Sequential(name=\"ResNet-50V2\")\n\nmodel_ResNet50V2.add(\n    tf.keras.applications.ResNet50V2(\n        include_top = False,\n        weights = 'imagenet',\n        pooling = 'avg',\n        input_shape = input_shape)\n)\nmodel_ResNet50V2.add(layers.Flatten())\nmodel_ResNet50V2.add(layers.Dropout(0.5))\nmodel_ResNet50V2.add(layers.Dense(num_classes, activation='softmax'))\n\nprint(\"Architecture Implemented\")","bd8b35ea":"model_Xception = tf.keras.Sequential(name=\"Xception\")\n\nmodel_Xception.add(\n    tf.keras.applications.Xception(\n        include_top = False,\n        weights = 'imagenet',\n        pooling = 'avg',\n        input_shape = input_shape)\n)\nmodel_Xception.add(layers.Flatten())\nmodel_Xception.add(layers.Dropout(0.5))\nmodel_Xception.add(layers.Dense(num_classes, activation='softmax'))\n\nprint(\"Architecture Implemented\")","3d1af494":"model_InceptionResNetV2 = tf.keras.Sequential(name=\"InceptionResNetV2\")\n\nmodel_InceptionResNetV2.add(\n    tf.keras.applications.InceptionResNetV2(\n        include_top = False,\n        weights = 'imagenet',\n        pooling = 'avg',\n        input_shape = input_shape)\n)\nmodel_InceptionResNetV2.add(layers.Flatten())\nmodel_InceptionResNetV2.add(layers.Dropout(0.5))\nmodel_InceptionResNetV2.add(layers.Dense(num_classes, activation='softmax'))\n\nprint(\"Architecture Implemented\")","7ba372d8":"\"\"\"\nmodel_NASNetLarge = tf.keras.Sequential(name=\"NASNetLarge\")\n\nmodel_NASNetLarge.add(\n    tf.keras.applications.NASNetLarge(\n        include_top = False,\n        weights = 'imagenet',\n        pooling = 'avg',\n        input_shape = input_shape)\n)\nmodel_NASNetLarge.add(layers.Flatten())\nmodel_NASNetLarge.add(layers.Dropout(0.5))\nmodel_NASNetLarge.add(layers.Dense(num_classes, activation='softmax'))\n\nprint(\"Architecture Implemented\")\n\n\"\"\"\n\n","03dfa23f":"optimizerAlexNet = tf.keras.optimizers.Adam(epsilon = 0.1)\n\nmodel_AlexNet.compile(\n    optimizer = optimizerAlexNet,\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy']\n)\n\nmodel_AlexNet.summary()\n\nprint(\"Optimizer Implemented\")","16a578b0":"optimizerVGG16 = tf.keras.optimizers.Adam(epsilon = 0.1)\n\nmodel_VGG16.compile(\n    optimizer = optimizerVGG16,\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics = ['accuracy']\n)\n\nmodel_VGG16.summary()\n\nprint(\"Optimizer Implemented\")","adedf091":"optimizerInceptionV3 = tf.keras.optimizers.Adam(epsilon = 0.1)\n\nmodel_InceptionV3.compile(\n    optimizer = optimizerInceptionV3,\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics = ['accuracy']\n)\n\nmodel_InceptionV3.summary()\n\nprint(\"Optimizer Implemented\")","ed437241":"optimizerResNet50V2 = tf.keras.optimizers.Adam(epsilon = 0.1)\n\nmodel_ResNet50V2.compile(\n    optimizer = optimizerResNet50V2,\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics = ['accuracy']\n)\n\nmodel_ResNet50V2.summary()\n\nprint(\"Optimizer Implemented\")","80b08067":"optimizerXception = tf.keras.optimizers.Adam(epsilon = 0.1)\n\nmodel_Xception.compile(\n    optimizer = optimizerXception,\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics = ['accuracy']\n)\n\nmodel_Xception.summary()\n\nprint(\"Optimizer Implemented\")","e9599047":"optimizerInceptionResNetV2 = tf.keras.optimizers.Adam(epsilon = 0.1)\n\nmodel_InceptionResNetV2.compile(\n    optimizer = optimizerInceptionResNetV2,\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics = ['accuracy']\n)\n\nmodel_InceptionResNetV2.summary()\n\nprint(\"Optimizer Implemented\")","ee4f61f4":"\"\"\"\noptimizerNASNetLarge = tf.keras.optimizers.Adam(epsilon = 0.1)\n\nmodel_NASNetLarge.compile(\n    optimizer = optimizerNASNetLarge,\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics = ['accuracy']\n)\n\nmodel_NASNetLarge.summary()\n\nprint(\"Optimizer Implemented\")\n\"\"\"","a516f2f2":"models = [model_VGG16,\n         model_InceptionV3,\n         model_ResNet50V2,\n         model_Xception,\n         model_InceptionResNetV2\n]\n\n#epochs = 50\n#batch_size = 16\n\nbatch_size = 20\n\nsteps_per_epoch = train_generator.n \/\/ batch_size\nvalidation_steps = validation_generator.n \/\/ batch_size\n\nprint(\"Steps per epoch: \", steps_per_epoch)\nprint(\"Validation Steps: \", validation_steps)","32a984b0":"#early_stop = tf.keras.callbacks.EarlyStopping(\n#    monitor='val_loss', \n#    patience=10)\n\n\"\"\"\n#models = [model_AlexNet]\n\nfor model in models:\n    start = time.time()\n\n    history = model.fit(\n      train_ds,\n      validation_data = val_ds,\n      batch_size=batch_size,\n      epochs = epochs\n    )\n\n    end = time.time()\n    getTime(start, end)\n    plottingResults(model)\n    \n    K.clear_session()\n\n\"\"\"\nfor model in models:\n    start = time.time()\n    history = model.fit(\n        train_generator,\n        steps_per_epoch=steps_per_epoch,\n        epochs=20,\n        validation_data=validation_generator,\n        validation_steps=validation_steps,\n        verbose=1\n    )\n    end = time.time()\n    getTime(start, end)\n    plottingResults(model)\n    \n    K.clear_session()\n","acba9fae":"history_dict = history.history\nprint(history_dict.keys())\n\nacc      = history.history[     'acc']\nval_acc  = history.history[ 'val_acc']\nloss     = history.history[    'loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1, 1)\nplt.figure()\nplt.plot(epochs, acc, 'r--', label='Training acc')\nplt.plot(epochs, acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.ylabel('acc')\nplt.xlabel('epochs')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'r--', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.ylabel('loss')\nplt.xlabel('epochs')\nplt.legend()\n\nplt.show()\n","69533431":"model_Xception","e088da10":"test_loss, test_acc=model_InceptionV3.evaluate(test_generator)\nprint(\"Test Accuracy: \", test_acc)","f07cd28b":"onlyfiles = [f for f in listdir(target_path) if isfile(join(target_path, f))]\nonlyfiles.sort()\nprint(onlyfiles)\n","5a859470":"def predictions(modelName):\n    \n    for cat in onlyfiles:\n        path = target_path + cat\n        print(path)\n\n        img = tf.keras.preprocessing.image.load_img(\n            path,\n            target_size=(img_height, img_width)\n        )\n        \n        img_array = tf.keras.preprocessing.image.img_to_array(img)\n        img_array = tf.expand_dims(img_array, 0) # Create a batch\n\n        predictions = modelName.predict(img_array)\n        score = tf.nn.softmax(predictions[0])\n\n        print(\n            \"This image most likely belongs to {} with a {:.2f}% confidence.\\n\"\n            .format(class_names[np.argmax(score)], 100 * np.max(score))\n        )","5eb4525b":"for model in models:\n    \n    print(\"\\n==========================================================\")\n    print(\"=====        Plotting {}         \".format(model.name))\n    print(\"==========================================================\")\n    \n    predictions(model)\n    ","e0305b40":"#### 3.2.3 Inception-v3 (from Keras Applications) --> Optimizer","1228eeca":"# Basic Neural Networks\n\n## **Project:** Weather Detection Type and Time of Day on Videofrom Car\u2019s DVR\n\n<b>Done by  : <\/b> <br\/>\nEdwin Carre\u00f1o  <br\/>\nN. Sitdikova   <br\/>\nN. Rerikh   <br\/>\nR. Shakurov   <br\/><br\/>\n<b>Date:<\/b> 02.11.2020 <br\/>\n<b>**Institution: Kazan National Research Technical University** <\/b>\n","278a2e98":"#### 3.1.7 NASNetLarge(from Keras Application)","92f42972":"#### 3.1.3 Inception-v3","04599757":"### 3.2 Configuring Optimizers","99ed44b8":"#### 3.1.4 ResNet-50v2 (from Keras Application)","681b238f":"#### 3.2.1 AlexNet (from Scratch)  --> Optimizer","fba5ccb0":"## End of the Notebook!","9fe0d773":"### Visualizing some pictures in the dataset","0f87fc29":"# Machine Learning Pipeline","6a9bfea3":"## 4. Training ","c6e2e72a":"#### 3.1.5 Xception (from Keras Application)","0596b168":"#### 3.2.6 Inception ResNet (from Keras Application) --> Optimizer","61311915":"### 3.1 Building Convolutional Neural Networks Architectures\n* 1. (2000) AlexNet \n* 2. (2003) VGG16                 \n* 3. (2008) Inception-v3      \n* 4. (2011) ResNet-50         \n* 5. (2013) Xception   \n* 6. (2017) Inception ResNets \n* 7. (2019) NASNetLarge       ","9232a2b0":"### Importing Libraries","58f7f3ee":"Utility functions","26a90f25":"#### 3.2.2 VGG16 (from Keras Applications)  --> Optimizer","f3d30e5b":"#### 3.2.5 Xception (from Keras Application) --> Optimizer","680f9564":"#### 3.2.7 NASNetLarge(from Keras Application) --> Optimizer","2829e245":"### 3.3 Configuring Additional Parameters\n","977547dc":"## 2. Standarization","aa7a5c2a":"#### 3.1.6 Inception ResNet (from Keras Application)","b6d2d281":" ## 1. Loading Dataset","184fefb6":"#### 3.2.4 ResNet-50v2 (from Keras Application)  --> Optimizer","9a02ed6e":"## 3. Modeling","c7c9caeb":"#### 3.1.2 VGG16 (from Keras Application)","30916733":"#### 3.1.1 AlexNet (from scratch)","0c702873":"## 5. Prediction"}}