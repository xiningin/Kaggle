{"cell_type":{"bbade141":"code","c2692fd3":"code","af981cda":"code","1d511e96":"code","61da50bf":"code","8abdb81f":"code","87e17ab3":"code","bf4096d5":"code","2b15ffc7":"code","28b9f18a":"code","6ff5dd27":"code","fd490d82":"code","4ea0f6c7":"code","8caa8bf9":"code","5e40a9f8":"code","00908b7e":"code","b89fe09d":"code","9c2f9e01":"code","c76c9c5e":"markdown","b44f4755":"markdown","e17a9ce7":"markdown","6ec21f23":"markdown","0f2c3418":"markdown","f9e7b097":"markdown","fd890c33":"markdown","b0cbb05c":"markdown","fdd4d327":"markdown","cdb3b979":"markdown","6ccad9f2":"markdown","ef7cdc0b":"markdown","f63f6a1c":"markdown"},"source":{"bbade141":"import pandas as pd\nimport numpy as np\nfrom lda import guidedlda as glda\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport nltk\n\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\nimport string","c2692fd3":"df1=pd.read_csv(\"data.csv\")\ndf1.head()","af981cda":"df1['text'] =df1['text'].str.replace(\"[^a-zA-Z#]\", \" \")\n\nstopwords_list = stopwords.words('english')\npunctuations = list(set(string.punctuation))\n\nunwanted_list=punctuations+stopwords_list\n\ndef clean_text_initial(text):\n    text = ' '.join([x.lower() for x in word_tokenize(text) if x.lower() not in unwanted_list and len(x)>1])\n    text = ' '.join([x.lower() for x in word_tokenize(text) if nltk.pos_tag([x])[0][1].startswith(\"NN\") or nltk.pos_tag([x])[0][1].startswith(\"JJ\")])\n    return text.strip()\n\ndf1[\"clean_text\"]=df1.text.apply(lambda text:clean_text_initial(str(text)))\ndf1.head()","1d511e96":"corpus=df1.clean_text.tolist()\nvocab=list(set(word_tokenize(\" \".join(df1.clean_text))))\nvectorizer = CountVectorizer(ngram_range=(1,1),vocabulary=vocab)\nX = vectorizer.fit_transform(corpus)\nword2id=vectorizer.vocabulary_","61da50bf":"house_words=[\"bedroom\",\"room\",\"house\",\"home\",\"airbnb\",\"condo\",\"bed\",\"blocks\",\"comfy\",\"amenities\",\"pool\",\"gym\",\n             \"cottage\",\"min\",\"minutes\", \"away\",\"duplex\",\"kitchen\",\"stay\",\"short\",\"apartment\",\"residential\",\n             \"camping\",\"distance\",\"bungalow\",\"walking\",\"neighborhood\",\"cozy\",\"cabin\",\"cabins\",\"coziness\",\n             \"families\",\"property\",\"courtyard\",\"accommodate\",\"living\", \"area\",\"minutos\"]\n\nglassdoor_words=[\"pros\"]\n\nautomobile_words=[\"odometer\",\"automobile\",\"car\",\"engine\",\"automatic\", \"transmissions\",\"manual\", \"shift\",\n                      \"automotive\",\"chevrolet\",\"transmission\",\"accelerator\",\"toyota\",\"volvo\",\"nissan\",\n                     \"convertibles\",\"convertible\",\"drive\"]\n\nsports_words=[\"club\",\"manager\",\"player\",\"championship\",\"contract\",\"stadium\",\"players\",\"season\",\"score\",\"scorer\",\n              \"team\",\"teammate\",\"game\",\"liverpool\",\"football\",\"nfl\",\"victory\",\"ravens\",\"boxing\",\"cricket\",\"quarterback\",\n              \"middleweight\",\"arsenal\",\"barcelona\",\"welterweight\",\"icc\",\"ipl\",\"bowlers\",\"innings\",\"bowler\",\"lightweight\",\n              \"knicks\",\"match\",\"matches\",\"soccer\",\"football\",\"playoffs\",\"premier league\",\"drs\",\"tournament\",\"fan\",\n              \"sports\",\"boxer\",\"fielders\",\"ufc\",\"linebacker\",\"coach\",\"nba\",\"referee\",\"champion\",\"injury\",\"races\",\"points\",\n              \"golf\",\"arenas\",\"pitching\",\"receiver\",\"champ\",\"cornerback\",\"mvp\",\"jayhawks\",\"quarterfinal\",\"agent\",\"ball\",\n              \"comeback\",\"shot\",\"red sox\",\"agency\",\"wins\",\"winners\",\"warriors\",\"gonzaga\",\"race\"]\n\ntech_words=[\"snapchat\",\"facebook\",\"samsung\",\"phone\",\"smartphone\",\"iphone\",\"ai\",\"uber\",\"hewlettpackard\",\"technology\",\n            \"digitized\",\"google\",\"nintendo\",\"economy\",\"github\",\"aws\",\"tumblr\",\"entrepreneur\",\"business\",\"ncaa\",\n            \"tourney\",\"amazon\",\"startup\",\"apps\",\"android\",\"crypto\",\"develop\",\"headset\",\"intel\",\"mwc\",\"bitcoin\",\n            \"spacex\",\"processor\",\"software\",\"analytics\",\"twitter\",\"developer\",\"microsoft\",\"computer\",\"study\",\"research\",\n            \"recession\",\"plugin\",\"youtube\",\"netflix\",\"marvel\",\"logan\",\"siri\",\"electronic\",\"tesla\",\"programming\",\"snap\",\n            \"chromebooks\",\"fitbit\",\"insta\",\"instagram\",\"semiconductor\",\"vpn\",\"fintech\",\"industry\",\"system\",\"systems\",\n            \"att\",\"ceo\",\"alexa\",\"computing\",\"vr\",\"nasa\",\"technical\",\"companies\",\"hacker\",\"blog\",\"wireless\",\"speaker\",\n            \"screens\",\"organization\",\"photography\",\"article\",\"ebay\",\"pandora\",\"console\",\"printer\",\"pi\",\"spacesuit\",\n            \"movies\",\"novel\",\"bot\",\"robot\",\"nitrogen\",\"hoverboard\",\"conferences\",\"online\",\"data\",\"images\",\"biomimicry\",\n            \"apple\"]","8abdb81f":"house_words = [x for x in house_words if x in list(word2id.keys())]\nglassdoor_words = [x for x in glassdoor_words if x in list(word2id.keys())]\nautomobile_words = [x for x in automobile_words if x in list(word2id.keys())]\nsports_words = [x for x in sports_words if x in list(word2id.keys())]\ntech_words = [x for x in tech_words if x in list(word2id.keys())]","87e17ab3":"seed_topic_list = [\n    house_words,\n    glassdoor_words,\n    automobile_words,\n    sports_words,\n    tech_words\n]","bf4096d5":"model = glda.GuidedLDA(n_topics=5, n_iter=2000, random_state=7, refresh=20,alpha=0.01,eta=0.01)","2b15ffc7":"seed_topics = {}\nfor t_id, st in enumerate(seed_topic_list):\n    for word in st:\n        seed_topics[word2id[word]] = t_id","28b9f18a":"model.fit(X, seed_topics=seed_topics, seed_confidence=0.15)","6ff5dd27":"n_top_words = 10\ntopic_word = model.topic_word_\nfor i, topic_dist in enumerate(topic_word):\n    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n    print('Topic {}: {}'.format(i, ' '.join(topic_words)))","fd490d82":"topic_num_name = {\"Topic 0\":\"room_rentals\",\n                  \"Topic 1\":\"glassdoor_reviews\",\n                  \"Topic 2\":\"Automobiles\",\n                  \"Topic 3\":\"sports_news\",\n                  \"Topic 4\":\"tech_news\"}    ","4ea0f6c7":"def get_doc_topics(model_glda,X,num_topics,dataframe,col_name):\n    \"\"\"\n    A function which creates dataframe with documents, their dominant topic, along with their probabilities\n    \n    Parameters\n    -------------\n    model_glda - Guided LDA trained model\n    X - Document term frequency table\n    num_topics - Number of topics the model was trained for\n    dataframe - Dataframe consisting of cleaned text column\n    col_name - Column name in dataframe holding cleaned text\n    \n    Returns\n    -------------\n    A dataframe with document number, topic, probability of topic\n    \"\"\"\n    df_doc_top = pd.DataFrame()\n    final_list = []\n    for index in range(len(dataframe[col_name])):\n        word_id_dict = dict((x,y) for x,y in zip([x for x in range(num_topics)],np.round(model.transform(X[index])*100,1).tolist()[0]))\n        word_score_list = []\n        for index in range(num_topics):\n            try:\n                value = word_id_dict[index]\n            except:\n                value = 0\n            word_score_list.append(value)\n        final_list.append(word_score_list)\n\n    df_doc_top = pd.DataFrame(final_list)\n    df_doc_top.columns = ['Topic ' + str(i) for i in range(num_topics)]\n    df_doc_top.index = ['Document ' + str(i) for i in range(len(dataframe[col_name]))]\n\n    df_doc_top[\"Dominant_Topic\"] = df_doc_top.idxmax(axis=1).tolist()\n    df_doc_top[\"Topic_Probability\"] = df_doc_top.max(axis=1).tolist()\n    document_df = df_doc_top.reset_index().rename(columns={\"index\":\"Document\"})[[\"Document\",\"Dominant_Topic\",\"Topic_Probability\"]]\n\n    return document_df","8caa8bf9":"document_df=get_doc_topics(model,X,5,df1,\"clean_text\")","5e40a9f8":"submission=pd.concat([df1.Id,document_df.Dominant_Topic],axis=1)","00908b7e":"submission.Dominant_Topic=submission.Dominant_Topic.replace(topic_num_name)","b89fe09d":"submission=submission.set_index(\"Id\").rename(columns={\"Dominant_Topic\":\"topic\"})","9c2f9e01":"submission","c76c9c5e":"## Setting priors ","b44f4755":"## Importing Required packages ","e17a9ce7":"## Reading and processing the data","6ec21f23":"## Defining priors ","0f2c3418":"Getting error for lda package? <hr> Getting guided lda package that I am using is not a straightforward exercise as the original packages pip install is not working. I found the workaround for implementing it <a href=\"https:\/\/github.com\/dex314\/GuidedLDA_WorkAround\">here<\/a>.","f9e7b097":"# Introduction\n\nGuided LDA is a method in which the user gives some priors (words) for each or some topics, which is in a way used as a starting point by the algorithm to determine other words in the topics as needed. The package to implement this algorithm is GuidedLDA whos <a href=\"https:\/\/guidedlda.readthedocs.io\/en\/latest\/\">user guide<\/a> explains how to use it on a higher level. However, recently many people across the world are facing challenges in installing the package. Thankfully, someone found a workaround for using the implementation and it is given in detail in this <a href=\"https:\/\/github.com\/dex314\/GuidedLDA_WorkAround\">github<\/a> repo. if you want to install and try running the notebook for yourself, please do ensure that you follow steps given in the above github link before starting.","fd890c33":"## Defining model ","b0cbb05c":"## Tagging the topics to create id - topic file","fdd4d327":"### Creating list of word lists as needed ","cdb3b979":"## Training the model ","6ccad9f2":"## Creating objects required for model training ","ef7cdc0b":"### Removing prior words that are not part of vocabulary ","f63f6a1c":"### Seeing the model output topics and top 10 words per topic "}}