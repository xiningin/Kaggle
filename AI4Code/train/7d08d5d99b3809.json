{"cell_type":{"b4c9a0a2":"code","a8b064ef":"code","666b1c1c":"code","012e5153":"code","39af2a75":"code","19080697":"code","80f9b774":"code","cccf141c":"code","e31e18da":"code","347575d3":"code","355a4289":"code","220b97c9":"code","bac95f9f":"code","c4e869c7":"code","c9156773":"code","51136ed3":"code","1811a963":"code","43e144e6":"code","44d9dfdf":"code","9d815049":"code","affe73fc":"code","9bcabcdf":"code","93a9176e":"code","a1cb8b42":"code","64359a6f":"code","46020084":"code","ad0ef5ec":"code","53f7226a":"code","d91171dd":"code","97efd759":"code","928db235":"code","284a43fb":"code","15669089":"code","b2e45b32":"code","c5014ef5":"code","b91bfb83":"code","7c74fc17":"code","825eb07a":"code","809f6ae9":"code","992b2724":"code","3c631da4":"code","c728a916":"code","53d206d6":"code","d113a9cd":"code","fa7cc009":"code","6285be24":"code","839987b5":"code","a3314370":"code","80138712":"code","72222af3":"code","1b3c61c0":"code","ceb1a612":"code","ad45d6a6":"code","6f26d364":"code","fc38da5d":"code","ca363b8b":"code","87f05a1a":"code","6dd20379":"code","363bfbf4":"code","c207de97":"code","d1b9dc50":"code","b8cd02fc":"code","7ea565da":"code","20d176f8":"markdown","9129e87f":"markdown","c4b952a6":"markdown","532711a9":"markdown","24193462":"markdown","2abcdb91":"markdown","0c83db36":"markdown","fd7bf1bf":"markdown","b311a1fe":"markdown","b7f17889":"markdown","eaf2798f":"markdown","64838e7d":"markdown","50594bcb":"markdown","0c2eb31a":"markdown","9a6ec54a":"markdown","24be4f67":"markdown","c2a7d2a3":"markdown","d6cfecfc":"markdown","2d9caa41":"markdown","7118c489":"markdown","04db0f34":"markdown","51e0eb96":"markdown","33ae31b2":"markdown","e18b26fc":"markdown"},"source":{"b4c9a0a2":"# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n# data visualisation and manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\nimport missingno as msno\nimport pandas_profiling as pdp\n\n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\n\n# models\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, Perceptron, RidgeClassifier, SGDClassifier, LassoCV\nfrom sklearn.svm import SVC, LinearSVC, SVR\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier \nfrom sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn import metrics\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\n# feature selection\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel, SelectKBest, RFE, chi2\n\n#model selection\nfrom sklearn.model_selection import train_test_split,cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n# preprocessing\nimport sklearn\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, learning_curve, ShuffleSplit\nfrom sklearn.model_selection import cross_val_predict as cvp\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, accuracy_score, confusion_matrix, explained_variance_score\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel, SelectKBest, RFE, chi2\n#evaluation metrics\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error # for regression\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  # for classification\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\n\nimport pandas_profiling as pp\n\n","a8b064ef":"df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf.head()","666b1c1c":"df.shape","012e5153":"df = df.drop_duplicates()\ndf.shape","39af2a75":"df.info()","19080697":"df.dropna(inplace = True)","80f9b774":"for i in ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']:\n    print(df[i].unique())","cccf141c":"countplot_cols = ['heart_disease', 'hypertension', 'gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']","e31e18da":"boxplot_cols = ['age','avg_glucose_level', 'bmi']","347575d3":"for i, column in enumerate(countplot_cols):\n    sns.countplot(x=column, hue = 'stroke', data=df)\n    plt.show()","355a4289":"for i, column in enumerate(boxplot_cols):\n    sns.boxplot(x='stroke', y=column, data=df)\n    plt.show()","220b97c9":"#df = df.drop(df[df.smoking_status == 'Unknown'].index)\ndf = df.drop(df[df.gender == 'Other'].index)","bac95f9f":"le = LabelEncoder()\ndf['gender'] = le.fit_transform(df['gender'])\ndf['ever_married'] = le.fit_transform(df['ever_married'])\ndf['work_type'] = le.fit_transform(df['work_type'])\ndf['Residence_type'] = le.fit_transform(df['Residence_type'])\ndf['smoking_status'] = le.fit_transform(df['smoking_status'])\ndf = df.drop('id', axis = 1)\nprint('Encoding was successful ')","c4e869c7":"#replace_values = {'Unknown': 'never smoked','formerly smoked': 'smokes'}\n\n#df = df.replace({'smoking_status': replace_values})\n#print('Replace was successfully')","c9156773":"# Create new features\n\ndef feature_creation(df):\n    df['age1'] = np.log(df['age'])\n    df['age2'] = np.sqrt(df['age'])\n    df['age3'] = df['age']**3\n    df['bmi1'] = np.log(df['bmi'])\n    df['bmi2'] = np.sqrt(df['bmi'])\n    df['bmi3'] = df['bmi']**3\n    df['avg_glucose_level1'] = np.log(df['avg_glucose_level'])\n    df['avg_glucose_level2'] = np.sqrt(df['avg_glucose_level'])\n    df['avg_glucose_level3'] = df['avg_glucose_level']**3\n    for i in ['gender', 'age1', 'age2', 'age3', 'hypertension', 'heart_disease', 'ever_married', 'work_type']:\n        for j in ['Residence_type', 'avg_glucose_level1','avg_glucose_level2', 'avg_glucose_level3', 'bmi1', 'bmi2', 'bmi3','smoking_status']:\n            df[i+'_'+j] = df[i].astype('str')+'_'+df[j].astype('str')\n    return df\n\ndf = feature_creation(df)","51136ed3":"df.shape","1811a963":"# Determination categorical features\n\ncategorical_columns = []\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nfeatures = df.columns.values.tolist()\nfor col in features:\n    if df[col].dtype in numerics: continue\n    categorical_columns.append(col)\n\ncategorical_columns","43e144e6":"# Encoding categorical features\n\nfor col in categorical_columns:\n    if col in df.columns:\n        #le = LabelEncoder()\n        le.fit(list(df[col].astype(str).values))\n        df[col] = le.transform(list(df[col].astype(str).values))\nprint('Encoding was successfull')","44d9dfdf":"# the number of features that we need to choose as a result\n#num_features_opt = 25 \nnum_features_opt = 40 \n\n\n# the somewhat excessive number of features, which we will choose at each stage\nnum_features_max = 50   \n\nfeatures_best = []","9d815049":"X_train = df.drop('stroke', axis = 1).copy()\ny_train = df.stroke.copy()","affe73fc":"# Threshold for removing correlated variables\nthreshold = 0.9\n\ndef highlight(value):\n    if value > threshold:\n        style = 'background-color: pink'\n    else:\n        style = 'background-color: green'\n    return style\n\n# Absolute value correlation matrix\ncorr_matrix = df.corr().abs().round(2)\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper.style.format(\"{:.2f}\").applymap(highlight)","9bcabcdf":"# Select columns with correlations above threshold\ncollinear_features = [column for column in upper.columns if any(upper[column] > threshold)]\n\n# Drop features with correlation above the threshold\nfeatures_filtered = df.drop(columns = collinear_features)\nprint('The number of features that passed the collinearity threshold: ', features_filtered.shape[1])\n\n# Add filtered features\nfeatures_best.append(features_filtered.columns.tolist())","93a9176e":"lsvc = LinearSVC(C=0.1, penalty=\"l1\", dual=False).fit(X_train, y_train)\nmodel = SelectFromModel(lsvc, prefit=True)\nX_new = model.transform(X_train)\nX_selected_df = pd.DataFrame(X_new, columns=[X_train.columns[i] for i in range(len(X_train.columns)) if model.get_support()[i]])\n\n# add features\nfeatures_best.append(X_selected_df.columns.tolist())","a1cb8b42":"lasso = LassoCV(cv=3).fit(X_train, y_train)\nmodel = SelectFromModel(lasso, prefit=True)\nX_new = model.transform(X_train)\nX_selected_df = pd.DataFrame(X_new, columns=[X_train.columns[i] for i in range(len(X_train.columns)) if model.get_support()[i]])\n\n# add features\nfeatures_best.append(X_selected_df.columns.tolist())","64359a6f":"bestfeatures = SelectKBest(score_func=chi2, k='all')\nfit = bestfeatures.fit(abs(X_train), y_train)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X_train.columns)\n\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Feature','Score']  #naming the dataframe columns\n\n# add features\nfeatures_best.append(featureScores.nlargest(num_features_max,'Score')['Feature'].tolist())\nprint(featureScores.nlargest(len(dfcolumns),'Score')) ","46020084":"rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=num_features_max, step=10, verbose=5)\nrfe_selector.fit(X_train, y_train)\nrfe_support = rfe_selector.get_support()\nrfe_feature = X_train.loc[:,rfe_support].columns.tolist()\nprint(str(len(rfe_feature)), 'selected features')","ad0ef5ec":"# add features\nfeatures_best.append(rfe_feature)","53f7226a":"embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=200), threshold='1.25*median')\nembeded_rf_selector.fit(X_train, y_train)","d91171dd":"embeded_rf_support = embeded_rf_selector.get_support()\nembeded_rf_feature = X_train.loc[:,embeded_rf_support].columns.tolist()\nprint(str(len(embeded_rf_feature)), 'selected features')","97efd759":"# add features\nfeatures_best.append(embeded_rf_feature)","928db235":"# Check whether all features have a sufficiently different meaning\nselector = VarianceThreshold(threshold=10)\nnp.shape(selector.fit_transform(df))\nfeatures_best.append(list(np.array(df.columns)[selector.get_support(indices=False)]))","284a43fb":"# Show best features\nfeatures_best","15669089":"# The element is in at least one list of optimal features\nmain_cols_max = features_best[0]\nfor i in range(len(features_best)-1):\n    main_cols_max = list(set(main_cols_max) | set(features_best[i+1]))\nprint(main_cols_max)\n\nprint('Cols:', len(main_cols_max))","b2e45b32":"# Most common items in all lists of optimal features\nmain_cols = []\nmain_cols_opt = {feature_name : 0 for feature_name in df.columns.tolist()}\nfor i in range(len(features_best)):\n    for feature_name in features_best[i]:\n        main_cols_opt[feature_name] += 1\ndf_main_cols_opt = pd.DataFrame.from_dict(main_cols_opt, orient='index', columns=['Num'])\ndf_main_cols_opt.sort_values(by=['Num'], ascending=False).head(num_features_opt)","c5014ef5":"# Select only our best features that are included in num_features_opt\nmain_cols = df_main_cols_opt.nlargest(num_features_opt, 'Num').index.tolist()\nif not 'stroke' in main_cols:\n    main_cols.append('stroke')\nprint(main_cols)\n\nprint(\"Quantity:\", len(main_cols))","b91bfb83":"# show best features\n\ndf[main_cols].head()","7c74fc17":"X = df[main_cols].drop('stroke', axis = 1)\ny = df[main_cols].stroke","825eb07a":"#scaling\nrs = RobustScaler()\n\nX_rs = pd.DataFrame(rs.fit_transform(X), columns = X.columns)","809f6ae9":"#train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_rs, y, test_size=0.2, random_state=42)","992b2724":"from imblearn.over_sampling import SMOTE\nsmt = SMOTE()\nX_train_sm, y_train_sm = smt.fit_resample(X_train, y_train)\nprint(y_train_sm.value_counts())","3c631da4":"from imblearn.over_sampling import ADASYN\nada = ADASYN()\nX_train_ada, y_train_ada = ada.fit_resample(X_train, y_train)\nprint(y_train_ada.value_counts())","c728a916":"from imblearn.combine import SMOTETomek\nsmtom = SMOTETomek()\nX_train_smtom, y_train_smtom = smtom.fit_resample(X_train, y_train)\nprint(y_train_smtom.value_counts())","53d206d6":"from imblearn.combine import SMOTEENN\nsmenn = SMOTEENN()\nX_train_smenn, y_train_smenn = smenn.fit_resample(X_train, y_train)\nprint(y_train_smenn.value_counts())","d113a9cd":"from imblearn.under_sampling import RandomUnderSampler\nrus = RandomUnderSampler(random_state = 42)\nX_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\nprint(y_train_rus.value_counts())","fa7cc009":"from imblearn.under_sampling import NearMiss\nNMv1 = NearMiss(version = 1)\nX_train_NMv1, y_train_NMv1 = NMv1.fit_resample(X_train, y_train)\nprint(y_train_NMv1.value_counts())","6285be24":"NMv2 = NearMiss(version = 2)\nX_train_NMv2, y_train_NMv2 = NMv2.fit_resample(X_train, y_train)\nprint(y_train_NMv2.value_counts())","839987b5":"NMv3 = NearMiss(version = 3)\nX_train_NMv3, y_train_NMv3 = NMv3.fit_resample(X_train, y_train)\nprint(y_train_NMv3.value_counts())","a3314370":"from imblearn.under_sampling import CondensedNearestNeighbour\nCNN = CondensedNearestNeighbour()\nX_train_CNN, y_train_CNN = CNN.fit_resample(X_train, y_train)\nprint(y_train_CNN.value_counts())","80138712":"from imblearn.under_sampling import OneSidedSelection\nOSS = OneSidedSelection()\nX_train_OSS, y_train_OSS = OSS.fit_resample(X_train, y_train)\nprint(y_train_OSS.value_counts())","72222af3":"from imblearn.under_sampling import NeighbourhoodCleaningRule\nNCR = NeighbourhoodCleaningRule()\nX_train_NCR, y_train_NCR = NCR.fit_resample(X_train, y_train)\nprint(y_train_NCR.value_counts())","1b3c61c0":"from imblearn.under_sampling import EditedNearestNeighbours\nENN = EditedNearestNeighbours()\nX_train_ENN, y_train_ENN = ENN.fit_resample(X_train, y_train)\nprint(y_train_ENN.value_counts())","ceb1a612":"from imblearn.under_sampling import InstanceHardnessThreshold\nIHT = InstanceHardnessThreshold()\nX_train_IHT, y_train_IHT = IHT.fit_resample(X_train, y_train)\nprint(y_train_IHT.value_counts())","ad45d6a6":"from imblearn.under_sampling import RepeatedEditedNearestNeighbours\nRENN = RepeatedEditedNearestNeighbours()\nX_train_RENN, y_train_RENN = RENN.fit_resample(X_train, y_train)\nprint(y_train_RENN.value_counts())\n","6f26d364":"from imblearn.under_sampling import AllKNN\nALLKNN = AllKNN()\nX_train_ALLKNN, y_train_ALLKNN = ALLKNN.fit_resample(X_train, y_train)\nprint(y_train_ALLKNN.value_counts())\n","fc38da5d":"def evaluate_model(clf, X_train,X_test, y_train, y_test, model_name, sample_type):\n    print('--------------------------------------------')\n    print('Model ', model_name)\n    print('Data Type ', sample_type)\n    y_pred = clf.predict(X_test)\n    y_pred_train = clf.predict(X_train)\n    #accuracy_train = accuracy_score(y_train, y_pred_train)\n    accuracy_test = accuracy_score(y_test, y_pred)\n    #f1_train = f1_score(y_train, y_pred_train, average = 'binary')\n    #recall_train = recall_score(y_train, y_pred_train, average = 'binary')\n    #precision_train = precision_score(y_train, y_pred_train,  average = 'binary')\n    f1_test = f1_score(y_test, y_pred, average='binary')\n    recall_test = recall_score(y_test, y_pred, average='binary')\n    precision_test = precision_score(y_test, y_pred, average='binary')\n    print('TRAIN:', classification_report(y_train, y_pred_train))\n    #print('TRAIN Accuracy:', accuracy_score(y_train, y_pred_train))\n    #print(\"TRAIN: F1 Score \", f1_train)\n    #print(\"TRAIN: Recall \", recall_train)\n    #print(\"TRAIN: Precision \", precision_train)\n    print('==================================================================')\n    print('TEST:', classification_report(y_test, y_pred))\n    #print('TEST Accuracy:', accuracy_score(y_test, y_pred))\n    #print(\"TEST: F1 Score \", f1_test)\n    #print(\"TEST: Recall \", recall_test)\n    #print(\"TEST: Precision \", precision_test)\n    return [model_name, sample_type, \n            f1_test,\n            precision_test,\n            recall_test, \n            accuracy_test]","ca363b8b":"#17 models\n\nmodels = {\n    'Decision Trees': DecisionTreeClassifier(random_state=42),\n    'Random Forest':RandomForestClassifier(random_state=42),\n    'Linear SVC':LinearSVC(random_state=42),\n    'AdaBoost Classifier':AdaBoostClassifier(random_state=42),\n    'Stochastic Gradient Descent':SGDClassifier(random_state=42),\n    'XGBoost': xgb.XGBClassifier(random_state=42),\n    'LightGBM': lgb.LGBMClassifier(random_state=42),\n    'KNN': KNeighborsClassifier(),\n    'Logistic Regression': LogisticRegression(random_state=42),\n    'Support Vector Machines': SVC(random_state=42),\n    'MLP Classifier': MLPClassifier(random_state=42),\n    'Gradient Boosting Classifier': GradientBoostingClassifier(random_state=42),\n    'Ridge Classifier': RidgeClassifier(),\n    'Bagging Classifier': BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5),\n    'Extra Trees Classifier': ExtraTreesClassifier(random_state=42),\n    'Naive Bayes': GaussianNB(),\n    'Gaussian Process Classification': GaussianProcessClassifier(random_state=42)\n}","87f05a1a":"#15 techniques\n\nsampled_data = {\n    'ACTUAL':[X_train, y_train],\n    'SMOTE':[X_train_sm, y_train_sm],\n    'ADASYN':[X_train_ada, y_train_ada],\n    'SMOTE_TOMEK':[X_train_smtom, y_train_smtom],\n    'SMOTE_ENN':[X_train_smenn, y_train_smenn],\n    'Random Under Sampling': [X_train_rus, y_train_rus],\n    'Near Miss1': [X_train_NMv1, y_train_NMv1],\n    'Near Miss2': [X_train_NMv2, y_train_NMv2],\n    'Near Miss3': [X_train_NMv3, y_train_NMv3],\n    'Condensed Nearest Neighbour': [X_train_CNN, y_train_CNN],\n    'One Sided Selection': [X_train_OSS, y_train_OSS],\n    'Neighbourhood Cleaning Rule' : [X_train_NCR, y_train_NCR],\n    'Edited Nearest Neighbours': [X_train_ENN, y_train_ENN],\n    'Instance Hardness Threshold': [X_train_IHT, y_train_IHT],\n    'Repeated Edited Nearest Neighbours': [X_train_RENN, y_train_RENN],\n    'AllKNN': [X_train_ALLKNN, y_train_ALLKNN]\n   \n}","6dd20379":"%%time\n\noutput = []\nfor model_k, model_clf in models.items():\n    for data_type, data in sampled_data.items():\n        model_clf.fit(data[0], data[1])\n        output.append(evaluate_model(model_clf, X_train, X_test, y_train, y_test, model_k, data_type))\n","363bfbf4":"result = pd.DataFrame(output, columns=['Model', 'DataType',\n                                       'F1',\n                                       'Precision',\n                                       'Recall', \n                                       'Accuracy'])\npd.set_option('display.max_rows', None)\n\nresult = result[result['F1']!=0]\nresult.sort_values(by=\"F1\", ascending=False)","c207de97":"result.shape\n\n# We have 248 predicts :) ","d1b9dc50":"result.sort_values(by = 'F1', ascending=False).head(5)","b8cd02fc":"result.sort_values(by = 'Precision', ascending=False).head(5)","7ea565da":"result.sort_values(by = 'Recall', ascending=False).head(5)","20d176f8":"## Selection the best features","9129e87f":"# Modeling","c4b952a6":"### Top 5 Precision score","532711a9":"# Introduction\nIn this notebook you will see EDA, learn how to create new features and choose the best ones, how to prepare data for modeling, how to use Oversamplind and Undersampling Techniques, and see 100+ predicts.\n\n### If you like my work then please upvote and write your opinion.","24193462":"## Feature selection by the SelectKBest with Chi-2","2abcdb91":"# Result","0c83db36":"## Feature selection with Lasso","fd7bf1bf":"## Feature selection by the SelectFromModel with LinearSVC","b311a1fe":"# Write your opinion. Thanks!","b7f17889":"### Top 5 Recall score","eaf2798f":"![maxresdefault.jpeg](attachment:maxresdefault.jpeg)","64838e7d":"## Feature selection by the RFE with Logistic Regression","50594bcb":"### UPDATE: 30.03.2021","0c2eb31a":"# Import Libs","9a6ec54a":"# Imbalance Technique","24be4f67":"# EDA","c2a7d2a3":"## Feature Selection with Pearson correlation","d6cfecfc":"# Preparing to modeling","2d9caa41":"### Top 5 F1 score","7118c489":"## Feature selection by the RFE with Random Forest","04db0f34":"# Data loading and overview","51e0eb96":"## Feature selection by the VarianceThreshhold","33ae31b2":"**We looked at the imbalance techniques and predictions of 17 models.**\n\n**We have a very good Recall value, but a relatively low Precision value.**\n\n**What to do next? I think we need to do Grid Search CV for all imbalance of techniques and models and improve the Precision metric.**\n\n","e18b26fc":"# Conclusion"}}