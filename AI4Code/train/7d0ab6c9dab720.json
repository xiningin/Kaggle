{"cell_type":{"02823866":"code","ea7fa08b":"code","ee949cb3":"code","51925072":"code","732bc004":"code","d23a4a25":"code","bb82e3f9":"code","e5aa3451":"code","3b5a6b7f":"code","9a2ddf3b":"code","5ee07687":"code","2ea63ac3":"code","22772801":"code","18392997":"code","be85a3ef":"code","2cfed3fe":"code","56931ab1":"code","5cf54243":"code","1922c8d7":"code","d3438373":"code","bd51cdab":"code","79e72077":"code","aaf8c030":"code","a18b50f5":"code","de31727a":"code","919332ed":"code","095a18eb":"code","21effa32":"code","ecd555bc":"code","47561dba":"code","9030cbea":"code","efeb28c1":"code","c3606a92":"code","35212dc6":"markdown","24b54ff1":"markdown","67fa0548":"markdown","65a308f1":"markdown","3f7516ae":"markdown"},"source":{"02823866":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ea7fa08b":"import pandas as pd\ndata =  pd.read_csv(\"..\/input\/pump-sensor-data\/sensor.csv\")","ee949cb3":"data.shape","51925072":"data.columns","732bc004":"data.describe().transpose()","d23a4a25":"data.isnull().sum()","bb82e3f9":"data.drop(['Unnamed: 0', 'timestamp','sensor_00','sensor_15','sensor_50','sensor_51'],axis=1, inplace=True)","e5aa3451":"import matplotlib.pyplot as plt\nprint(data.plot(subplots =True, sharex = True, figsize = (20,50)))","3b5a6b7f":"data['machine_status'].value_counts()","9a2ddf3b":"import numpy as np\nconditions = [(data['machine_status'] =='NORMAL'), (data['machine_status'] =='BROKEN'), (data['machine_status'] =='RECOVERING')]\nchoices = [1, 0, 0.5]\ndata['Operation'] = np.select(conditions, choices, default=0)","5ee07687":"import matplotlib.pyplot as plt\ndata.plot(subplots =True, sharex = True, figsize = (20,50))","2ea63ac3":"data.columns","22772801":"df0 = pd.DataFrame(data, columns=['Operation','sensor_04', 'sensor_06', 'sensor_07', 'sensor_08', 'sensor_09'])","18392997":"df1 = pd.DataFrame(data, columns=['Operation','sensor_01', 'sensor_04', 'sensor_10', 'sensor_14', 'sensor_19', 'sensor_25'])","be85a3ef":"df2 = pd.DataFrame(data, columns = ['Operation','sensor_02', 'sensor_05', 'sensor_11', 'sensor_16', 'sensor_20', 'sensor_26'])","2cfed3fe":"df3 = pd.DataFrame(data, columns = ['Operation','sensor_03', 'sensor_06', 'sensor_12', 'sensor_17', 'sensor_21', 'sensor_28'])","56931ab1":"df0.plot(subplots =True, sharex = True, figsize = (20,20))","5cf54243":"df = df0\ndf.shape","1922c8d7":"def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    dff = pd.DataFrame(data)\n    cols, names = list(), list()\n    for i in range(n_in, 0, -1):\n        cols.append(dff.shift(-i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    for i in range(0, n_out):\n        cols.append(dff.shift(-i))\n        if i==0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1)) for j in range(n_vars)]        \n        agg = pd.concat(cols, axis=1)\n        agg.columns = names\n        if dropnan:\n            agg.dropna(inplace=True)\n        return agg","d3438373":"from sklearn.preprocessing import MinMaxScaler\n\nvalues = df.values\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\nreframed = series_to_supervised(scaled, 1, 1)\nr = list(range(df.shape[1]+1, 2*df.shape[1]))\nreframed.drop(reframed.columns[r], axis=1, inplace=True)\nreframed.head()\n\n# Data spliting into train and test data series.\nvalues = reframed.values\nn_train_time = 50000\ntrain = values[:n_train_time, :]\ntest = values[n_train_time:, :]\ntrain_x, train_y = train[:, :-1], train[:, -1]\ntest_x, test_y = test[:, :-1], test[:, -1]\ntrain_x = train_x.reshape((train_x.shape[0], 1, train_x.shape[1]))\ntest_x = test_x.reshape((test_x.shape[0], 1, test_x.shape[1]))","bd51cdab":"from keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import Dense\nfrom sklearn.metrics import mean_squared_error,r2_score\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nmodel = Sequential()\nmodel.add(LSTM(100, input_shape=(train_x.shape[1], train_x.shape[2])))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\n# Network fitting\nhistory = model.fit(train_x, train_y, epochs=50, batch_size=70, validation_data=(test_x, test_y), verbose=2, shuffle=False)\n\n# Loss history plot\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()\n\nsize = df.shape[1]\n\n# Prediction test\nyhat = model.predict(test_x)\ntest_x = test_x.reshape((test_x.shape[0], size))\n\n# invert scaling for prediction\ninv_yhat = np.concatenate((yhat, test_x[:, 1-size:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n\n# invert scaling for actual\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = np.concatenate((test_y, test_x[:, 1-size:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n\n# calculate RMSE\nrmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)","79e72077":"e = 100 - round(sum(abs(inv_y[:]-inv_yhat[:]))\/len(inv_y[:])*100,2)\naa=[x for x in range(160000)]\nplt.figure(figsize=(25,10)) \nplt.plot(aa, inv_y[:160000], marker='.', label=\"actual\")\nplt.plot(aa, inv_yhat[:160000], 'r', label=\"prediction with precision of {} %\".format(e))\nplt.ylabel(df.columns[0], size=15)\nplt.xlabel('Time', size=15)\nplt.legend(fontsize=15)\nplt.show()","aaf8c030":"df = df2\ndf.shape","a18b50f5":"df2.plot(subplots =True, sharex = True, figsize = (20,20))","de31727a":"def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    dff = pd.DataFrame(data)\n    cols, names = list(), list()\n    for i in range(n_in, 0, -1):\n        cols.append(dff.shift(-i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    for i in range(0, n_out):\n        cols.append(dff.shift(-i))\n        if i==0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1)) for j in range(n_vars)]        \n        agg = pd.concat(cols, axis=1)\n        agg.columns = names\n        if dropnan:\n            agg.dropna(inplace=True)\n        return agg","919332ed":"from sklearn.preprocessing import MinMaxScaler\n\nvalues = df.values\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\nreframed = series_to_supervised(scaled, 1, 1)\nr = list(range(df.shape[1]+1, 2*df.shape[1]))\nreframed.drop(reframed.columns[r], axis=1, inplace=True)\nreframed.head()\n\n# Data spliting into train and test data series.\nvalues = reframed.values\nn_train_time = 50000\ntrain = values[:n_train_time, :]\ntest = values[n_train_time:, :]\ntrain_x, train_y = train[:, :-1], train[:, -1]\ntest_x, test_y = test[:, :-1], test[:, -1]\ntrain_x = train_x.reshape((train_x.shape[0], 1, train_x.shape[1]))\ntest_x = test_x.reshape((test_x.shape[0], 1, test_x.shape[1]))","095a18eb":"from keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import Dense\nfrom sklearn.metrics import mean_squared_error,r2_score\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nmodel = Sequential()\nmodel.add(LSTM(100, input_shape=(train_x.shape[1], train_x.shape[2])))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\n# Network fitting\nhistory = model.fit(train_x, train_y, epochs=50, batch_size=70, validation_data=(test_x, test_y), verbose=2, shuffle=False)\n\n# Loss history plot\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()\n\nsize = df.shape[1]\n\n# Prediction test\nyhat = model.predict(test_x)\ntest_x = test_x.reshape((test_x.shape[0], size))\n\n# invert scaling for prediction\ninv_yhat = np.concatenate((yhat, test_x[:, 1-size:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n\n# invert scaling for actual\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = np.concatenate((test_y, test_x[:, 1-size:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n\n# calculate RMSE\nrmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)","21effa32":"e = 100 - round(sum(abs(inv_y[:]-inv_yhat[:]))\/len(inv_y[:])*100,2)\naa=[x for x in range(170000)]\nplt.figure(figsize=(25,10)) \nplt.plot(aa, inv_y[:170000], marker='.', label=\"actual\")\nplt.plot(aa, inv_yhat[:170000], 'r', label=\"prediction with precision of {} %\".format(e))\nplt.ylabel(df.columns[0], size=15)\nplt.xlabel('Time', size=15)\nplt.legend(fontsize=15)\nplt.show()","ecd555bc":"print(rmse)","47561dba":"corr = data.corr()\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\ncorr80 = corr[abs(corr)> 0.8]\nsns.heatmap(corr80)\n\n","9030cbea":"sns.pairplot(df)\nplt.show()","efeb28c1":"def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()","c3606a92":"plotPerColumnDistribution(df, 10, 5)","35212dc6":"In this database, we choose 50,000 data points with 2 broken points to train the model, the remaining 170,000 points with 5 broken states will be used to test the predictivity of the model.","24b54ff1":"Set 1: sensors 1, 4, 10, 14, 19, 25, 34, 38","67fa0548":"Set 3: sensors 3, 6, 12, 17, 21, 28, 40","65a308f1":"Set 0: sensors 4, 6, 7, 8, 9","3f7516ae":"Set 2: sensors 2, 5, 11, 16, 20, 26, 39"}}