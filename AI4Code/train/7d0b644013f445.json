{"cell_type":{"252cc14f":"code","f1137373":"code","51f0b0d9":"code","ce37a8e1":"code","d8db7544":"code","7abc4621":"code","87bafb09":"code","11e2a367":"code","a162f39b":"code","4baf123f":"code","dc8355e1":"code","cff1b81b":"code","d73c35f3":"code","1ae99e2a":"code","baa27889":"markdown","dd2d5078":"markdown","cb1e9abc":"markdown","06105f7e":"markdown","8761db2b":"markdown","c2c4f799":"markdown","4cd629ce":"markdown","7035fdea":"markdown"},"source":{"252cc14f":"import pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.metrics import r2_score\nimport glob\n\ndef ffill(data_df):\n    data_df=data_df.set_index(['time_id', 'seconds_in_bucket'])\n    data_df = data_df.reindex(pd.MultiIndex.from_product([data_df.index.levels[0], np.arange(0,600)], names = ['time_id', 'seconds_in_bucket']), method='ffill')\n    return data_df.reset_index()","f1137373":"# A function to calculate realized volatility for all time intervals in a single book file\ndef realized_volatility_single_stock(file_path, prediction_column_name):\n    \n    df_book_data = pd.read_parquet(file_path)\n\n    df_book_data = ffill(df_book_data)\n    \n    \n    stock_id = file_path.split('=')[1]\n    time_ids, bpr, bsz, apr, asz = (df_book_data[col].values for col in ['time_id', 'bid_price1','bid_size1','ask_price1','ask_size1' ])\n    wap = (bpr * asz +apr * bsz) \/ (asz + bsz)\n    log_wap = np.log(wap)\n    ids, index = np.unique(time_ids, return_index=True)\n\n    splits = np.split(log_wap, index[1:])\n    ret=[]\n    for time_id, x in zip(ids.tolist(), splits):\n        log_ret = np.diff(x)\n        volatility = np.sqrt((log_ret ** 2).sum())\n        ret.append((f'{stock_id}-{time_id}', volatility.item()))\n    return pd.DataFrame(ret, columns=['row_id', prediction_column_name])","51f0b0d9":"def realized_volatility_all(files_list, prediction_column_name):\n    return pd.concat( [realized_volatility_single_stock(file, prediction_column_name) for file in files_list])","ce37a8e1":"list_order_book_file_train = glob.glob('\/kaggle\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/*')","d8db7544":"%%time\ndf_past_realized_train = realized_volatility_all(list_order_book_file_train, 'pred')","7abc4621":"df_past_realized_train['stock_id'] = df_past_realized_train['row_id'].str.partition('-')[0].astype('int')\ndf_past_realized_train['time_id'] = df_past_realized_train['row_id'].str.partition('-')[2].astype('int')","87bafb09":"train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntrain['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)","11e2a367":"df_past_realized_train = df_past_realized_train[['pred','stock_id','time_id']]\ndf_train = train[['target','stock_id','time_id']]\n\ntable_train = pd.pivot_table(df_past_realized_train, values='pred', index=['time_id'],columns=['stock_id'], aggfunc=np.sum)\ntable_test = pd.pivot_table(df_train, values='target', index=['time_id'],columns=['stock_id'], aggfunc=np.sum)","a162f39b":"table_train['target'] = 0\ntable_test['target'] = 1","4baf123f":"whole_df = pd.concat([table_train,table_test])\n\nX = whole_df.drop(['target'], axis=1)\ny = whole_df['target']","dc8355e1":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)","cff1b81b":"import lightgbm\n\ntrain_data = lightgbm.Dataset(X_train, label=y_train)\ntest_data = lightgbm.Dataset( X_test, label=y_test)\n\n\nparameters = {\n    'application': 'binary',\n    'objective': 'binary',\n    'metric': 'auc',\n    'is_unbalance': 'true',\n    'boosting': 'gbdt',\n    'num_leaves': 31,\n    'feature_fraction': 0.5,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 20,\n    'learning_rate': 0.05,\n    'verbose': 1\n}\n\nmodel = lightgbm.train(parameters,\n                       train_data,\n                       valid_sets=test_data,\n                       num_boost_round=50000,\n                       early_stopping_rounds=200)","d73c35f3":"lightgbm.plot_importance(model,max_num_features = 10)","1ae99e2a":"from matplotlib import pyplot\n\nstock_number = 83\n\nh1 = whole_df[stock_number][whole_df['target']==0]\nh2 = whole_df[stock_number][whole_df['target']==1]\n\nbins = np.linspace(0, 0.08, 25)\n\npyplot.hist(h1, bins=bins, alpha=0.5)\npyplot.hist(h2, bins=bins, alpha=0.5)\npyplot.legend(loc='upper right')\npyplot.show()","baa27889":"# Fit a lgbm model to guess which data come from which data set","dd2d5078":"0.54 AUC not exactly random, but not far from it. Remember that for application in finance you don't really much more than 0.5 precision to make money. So that 0.54 AUC could be significant.","cb1e9abc":"The plot clearly shows some outliers that allow to identify a difference between the training and target realized vol.","06105f7e":"# Are the time selected at random around the day or are they selected around specific events ?\n\nIn my optionion, one of the biggest question we have here is : do we have the same distribution for input realized vol and target ? or maybe something specific happen between the two period ? \n\nOne idea to check for that is to agregate training and target realized volatilities, and try to see if you can guess which one is which. Doing so you end up with a binary classification problem, where you try to guess if volatility come from training or target data. This is usually used to check for time consitency between training and testing. \n\nBy calibrating a small model and checking common binary classification metrics we can know if both data set are different or not. I use a lgbm, which is fast, powerfull and rather explainable. \n\nThis notebook is relying on the amazingly fast notebook : https:\/\/www.kaggle.com\/slawekbiel\/naive-but-fast-submission","8761db2b":"Less than a minute !\n\n# Let's build the two data sets","c2c4f799":"## Run on the train set to sanity check","4cd629ce":"# We can look at the difference in distribution between the training and testing set for a given stock","7035fdea":"# Add the targets"}}