{"cell_type":{"ad002d31":"code","15119080":"code","52d16a28":"code","a30520d3":"code","1075bb44":"code","77044461":"code","93926324":"code","08ddc4ee":"code","5c1c6d2d":"code","1e66dfe4":"code","05d62980":"code","1adf53b4":"code","8f15e213":"code","a77e06c5":"code","a1d77e5c":"code","515c87a6":"code","82dbc57e":"code","ae17f610":"code","ed8e0d25":"code","4f9a09f2":"code","aa906e6c":"code","b59752b8":"code","395dd3b3":"code","b553dcd2":"code","900d6db9":"code","88fbeebe":"code","c08e2d56":"code","2145b597":"code","fa3c2f97":"code","f3703a08":"code","085753c3":"code","d6692a00":"code","bb1189fa":"markdown","45df3cdd":"markdown","bce6ebe6":"markdown","be2a70a6":"markdown","97a205a3":"markdown","69f36501":"markdown","d4c03b15":"markdown"},"source":{"ad002d31":"from __future__ import print_function\n\nimport sys\nimport os\nimport pandas as pd\nimport numpy as np\nimport re\nimport nltk\n\nfrom keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense, Bidirectional\nfrom keras.models import Model, load_model\n\nINPUT_LENGTH = 20\nOUTPUT_LENGTH = 20\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","15119080":"# Load the data\nlines = open('..\/input\/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\nconv_lines = open('..\/input\/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')","52d16a28":"# Create a dictionary to map each line's id with its text\nid2line = {}\nfor line in lines:\n    _line = line.split(' +++$+++ ')\n    if len(_line) == 5:\n        id2line[_line[0]] = _line[4]","a30520d3":"# Create a list of all of the conversations' lines' ids.\nconvs = []\nfor line in conv_lines[:-1]:\n    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n    convs.append(_line.split(','))","1075bb44":"#id and conversation sample\nfor k in convs[300]:\n    print (k, id2line[k])","77044461":"# Sort the sentences into questions (inputs) and answers (targets)\nquestions = []\nanswers = []\nfor conv in convs:\n    for i in range(len(conv)-1):\n        questions.append(id2line[conv[i]])\n        answers.append(id2line[conv[i+1]])\n        \n# Compare lengths of questions and answers\nprint(len(questions))\nprint(len(answers))","93926324":"def clean_text(text):\n    '''Clean text by removing unnecessary characters and altering the format of words.'''\n    text = text.lower()\n    text = re.sub(r\"i'm\", \"i am\", text)\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"she's\", \"she is\", text)\n    text = re.sub(r\"it's\", \"it is\", text)\n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"what's\", \"that is\", text)\n    text = re.sub(r\"where's\", \"where is\", text)\n    text = re.sub(r\"how's\", \"how is\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"can't\", \"cannot\", text)\n    text = re.sub(r\"n't\", \" not\", text)\n    text = re.sub(r\"n'\", \"ng\", text)\n    text = re.sub(r\"'bout\", \"about\", text)\n    text = re.sub(r\"'til\", \"until\", text)\n    text = re.sub(r\"[-()\\\"#\/@;:<>{}`+=~|]\", \"\", text)\n#     text = re.sub(r\"[-()\\\"#\/@;:<>{}`+=~|.!?,]\", \"\", text)\n    text = \" \".join(text.split())\n    return text","08ddc4ee":"# Clean the data\nclean_questions = []\nfor question in questions:\n    clean_questions.append(clean_text(question))\nclean_answers = []    \nfor answer in answers:\n    clean_answers.append(clean_text(answer))","5c1c6d2d":"# Find the length of sentences (not using nltk due to processing speed)\nlengths = []\n# lengths.append([len(nltk.word_tokenize(sent)) for sent in clean_questions]) #nltk approach\nfor question in clean_questions:\n    lengths.append(len(question.split()))\nfor answer in clean_answers:\n    lengths.append(len(answer.split()))\n# Create a dataframe so that the values can be inspected\nlengths = pd.DataFrame(lengths, columns=['counts'])\nprint(np.percentile(lengths, 80))\nprint(np.percentile(lengths, 85))\nprint(np.percentile(lengths, 90))\nprint(np.percentile(lengths, 95))","1e66dfe4":"# Remove questions and answers that are shorter than 1 word and longer than 20 words.\nmin_line_length = 2\nmax_line_length = 20\n\n# Filter out the questions that are too short\/long\nshort_questions_temp = []\nshort_answers_temp = []\n\nfor i, question in enumerate(clean_questions):\n    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n        short_questions_temp.append(question)\n        short_answers_temp.append(clean_answers[i])\n\n# Filter out the answers that are too short\/long\nshort_questions = []\nshort_answers = []\n\nfor i, answer in enumerate(short_answers_temp):\n    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n        short_answers.append(answer)\n        short_questions.append(short_questions_temp[i])\n        \nprint(len(short_questions))\nprint(len(short_answers))","05d62980":"r = np.random.randint(1,len(short_questions))\n\nfor i in range(r, r+3):\n    print(short_questions[i])\n    print(short_answers[i])\n    print()","1adf53b4":"#choosing number of samples\nnum_samples = 30000  # Number of samples to train on.\nshort_questions = short_questions[:num_samples]\nshort_answers = short_answers[:num_samples]\n#tokenizing the qns and answers\nshort_questions_tok = [nltk.word_tokenize(sent) for sent in short_questions]\nshort_answers_tok = [nltk.word_tokenize(sent) for sent in short_answers]","8f15e213":"#train-validation split\ndata_size = len(short_questions_tok)\n\n# We will use the first 0-80th %-tile (80%) of data for the training\ntraining_input  = short_questions_tok[:round(data_size*(80\/100))]\ntraining_input  = [tr_input[::-1] for tr_input in training_input] #reverseing input seq for better performance\ntraining_output = short_answers_tok[:round(data_size*(80\/100))]\n\n# We will use the remaining for validation\nvalidation_input = short_questions_tok[round(data_size*(80\/100)):]\nvalidation_input  = [val_input[::-1] for val_input in validation_input] #reverseing input seq for better performance\nvalidation_output = short_answers_tok[round(data_size*(80\/100)):]\n\nprint('training size', len(training_input))\nprint('validation size', len(validation_input))","a77e06c5":"# Create a dictionary for the frequency of the vocabulary\n# Create \nvocab = {}\nfor question in short_questions_tok:\n    for word in question:\n        if word not in vocab:\n            vocab[word] = 1\n        else:\n            vocab[word] += 1\n\nfor answer in short_answers_tok:\n    for word in answer:\n        if word not in vocab:\n            vocab[word] = 1\n        else:\n            vocab[word] += 1            ","a1d77e5c":"# Remove rare words from the vocabulary.\n# We will aim to replace fewer than 5% of words with <UNK>\n# You will see this ratio soon.\nthreshold = 15\ncount = 0\nfor k,v in vocab.items():\n    if v >= threshold:\n        count += 1","515c87a6":"print(\"Size of total vocab:\", len(vocab))\nprint(\"Size of vocab we will use:\", count)","82dbc57e":"#we will create dictionaries to provide a unique integer for each word.\nWORD_CODE_START = 1\nWORD_CODE_PADDING = 0\n\n\nword_num  = 2 #number 1 is left for WORD_CODE_START for model decoder later\nencoding = {}\ndecoding = {1: 'START'}\nfor word, count in vocab.items():\n    if count >= threshold: #get vocabularies that appear above threshold count\n        encoding[word] = word_num \n        decoding[word_num ] = word\n        word_num += 1\n\nprint(\"No. of vocab used:\", word_num)","ae17f610":"#include unknown token for words not in dictionary\ndecoding[len(encoding)+2] = '<UNK>'\nencoding['<UNK>'] = len(encoding)+2","ed8e0d25":"dict_size = word_num+1\ndict_size","4f9a09f2":"def transform(encoding, data, vector_size=20):\n    \"\"\"\n    :param encoding: encoding dict built by build_word_encoding()\n    :param data: list of strings\n    :param vector_size: size of each encoded vector\n    \"\"\"\n    transformed_data = np.zeros(shape=(len(data), vector_size))\n    for i in range(len(data)):\n        for j in range(min(len(data[i]), vector_size)):\n            try:\n                transformed_data[i][j] = encoding[data[i][j]]\n            except:\n                transformed_data[i][j] = encoding['<UNK>']\n    return transformed_data","aa906e6c":"#encoding training set\nencoded_training_input = transform(\n    encoding, training_input, vector_size=INPUT_LENGTH)\nencoded_training_output = transform(\n    encoding, training_output, vector_size=OUTPUT_LENGTH)\n\nprint('encoded_training_input', encoded_training_input.shape)\nprint('encoded_training_output', encoded_training_output.shape)","b59752b8":"#encoding validation set\nencoded_validation_input = transform(\n    encoding, validation_input, vector_size=INPUT_LENGTH)\nencoded_validation_output = transform(\n    encoding, validation_output, vector_size=OUTPUT_LENGTH)\n\nprint('encoded_validation_input', encoded_validation_input.shape)\nprint('encoded_validation_output', encoded_validation_output.shape)","395dd3b3":"import tensorflow as tf\ntf.keras.backend.clear_session()","b553dcd2":"INPUT_LENGTH = 20\nOUTPUT_LENGTH = 20\n\nencoder_input = Input(shape=(INPUT_LENGTH,))\ndecoder_input = Input(shape=(OUTPUT_LENGTH,))","900d6db9":"from keras.layers import SimpleRNN\n\nencoder = Embedding(dict_size, 128, input_length=INPUT_LENGTH, mask_zero=True)(encoder_input)\nencoder = LSTM(512, return_sequences=True, unroll=True)(encoder)\nencoder_last = encoder[:,-1,:]\n\nprint('encoder', encoder)\nprint('encoder_last', encoder_last)\n\ndecoder = Embedding(dict_size, 128, input_length=OUTPUT_LENGTH, mask_zero=True)(decoder_input)\ndecoder = LSTM(512, return_sequences=True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])\n\nprint('decoder', decoder)\n\n# For the plain Sequence-to-Sequence, we produced the output from directly from decoder\n# output = TimeDistributed(Dense(output_dict_size, activation=\"softmax\"))(decoder)","88fbeebe":"from keras.layers import Activation, dot, concatenate\n\n# Equation (7) with 'dot' score from Section 3.1 in the paper.\n# Note that we reuse Softmax-activation layer instead of writing tensor calculation\nattention = dot([decoder, encoder], axes=[2, 2])\nattention = Activation('softmax', name='attention')(attention)\nprint('attention', attention)\n\ncontext = dot([attention, encoder], axes=[2,1])\nprint('context', context)\n\ndecoder_combined_context = concatenate([context, decoder])\nprint('decoder_combined_context', decoder_combined_context)\n\n# Has another weight + tanh layer as described in equation (5) of the paper\noutput = TimeDistributed(Dense(512, activation=\"tanh\"))(decoder_combined_context)\noutput = TimeDistributed(Dense(dict_size, activation=\"softmax\"))(output)\nprint('output', output)","c08e2d56":"model = Model(inputs=[encoder_input, decoder_input], outputs=[output])\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\nmodel.summary()","2145b597":"training_encoder_input = encoded_training_input\ntraining_decoder_input = np.zeros_like(encoded_training_output)\ntraining_decoder_input[:, 1:] = encoded_training_output[:,:-1]\ntraining_decoder_input[:, 0] = WORD_CODE_START\ntraining_decoder_output = np.eye(dict_size)[encoded_training_output.astype('int')]\n\nvalidation_encoder_input = encoded_validation_input\nvalidation_decoder_input = np.zeros_like(encoded_validation_output)\nvalidation_decoder_input[:, 1:] = encoded_validation_output[:,:-1]\nvalidation_decoder_input[:, 0] = WORD_CODE_START\nvalidation_decoder_output = np.eye(dict_size)[encoded_validation_output.astype('int')]","fa3c2f97":"model.fit(x=[training_encoder_input, training_decoder_input], y=[training_decoder_output],\n          validation_data=([validation_encoder_input, validation_decoder_input], [validation_decoder_output]),\n          #validation_split=0.05,\n          batch_size=64, epochs=100)\n\nmodel.save('model_attention.h5')","f3703a08":"def prediction(raw_input):\n    clean_input = clean_text(raw_input)\n    input_tok = [nltk.word_tokenize(clean_input)]\n    input_tok = [input_tok[0][::-1]]  #reverseing input seq\n    encoder_input = transform(encoding, input_tok, 20)\n    decoder_input = np.zeros(shape=(len(encoder_input), OUTPUT_LENGTH))\n    decoder_input[:,0] = WORD_CODE_START\n    for i in range(1, OUTPUT_LENGTH):\n        output = model.predict([encoder_input, decoder_input]).argmax(axis=2)\n        decoder_input[:,i] = output[:,i]\n    return output\n\ndef decode(decoding, vector):\n    \"\"\"\n    :param decoding: decoding dict built by word encoding\n    :param vector: an encoded vector\n    \"\"\"\n    text = ''\n    for i in vector:\n        if i == 0:\n            break\n        text += ' '\n        text += decoding[i]\n    return text","085753c3":"for i in range(20):\n    seq_index = np.random.randint(1, len(short_questions))\n    output = prediction(short_questions[seq_index])\n    print ('Q:', short_questions[seq_index])\n    print ('A:', decode(decoding, output[0]))","d6692a00":"raw_input = input()\noutput = prediction(raw_input)\nprint (decode(decoding, output[0]))","bb1189fa":"## 3. Model testing","45df3cdd":"## 2  Model Building\n### 2.1  Sequence-to-Sequence in Keras","bce6ebe6":"### Resources:\nhttps:\/\/wanasit.github.io\/attention-based-sequence-to-sequence-in-keras.html","be2a70a6":"### 1.2  Word en\/decoding dictionaries","97a205a3":"### 1.3  Vectorizing dataset","69f36501":"### 1.1  Preprocessing for word based model","d4c03b15":"### 2.2  Attention Mechanism\nReference: Effective Approaches to Attention-based Neural Machine Translation's Global Attention with Dot-based scoring function (Section 3, 3.1) https:\/\/arxiv.org\/pdf\/1508.04025.pdf"}}