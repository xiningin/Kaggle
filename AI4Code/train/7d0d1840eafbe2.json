{"cell_type":{"8690442e":"code","c62b8e27":"code","04b52fa0":"code","7d18c96c":"code","84ef65ed":"code","490a103a":"code","06954ba6":"code","fe8a3798":"code","564b9fa1":"code","4500a49f":"code","bfec4f01":"code","5a6c6abd":"code","0d0dc442":"code","4d70abf2":"code","cff51e1a":"code","5b36b8ce":"code","cdc5f481":"code","2b312a52":"code","c1e6957d":"code","214fe587":"code","4c3aadd6":"code","e68ccac8":"code","31e2357e":"code","719f534b":"code","d48929aa":"code","d27c8647":"code","874fbc2e":"code","1dd3026b":"code","474b5e6b":"code","8f58b960":"code","05fd8376":"code","e8408621":"code","0edfda97":"markdown","037f4cde":"markdown","924d786f":"markdown","86cbf613":"markdown","053cce05":"markdown","349bdd4e":"markdown","117ee122":"markdown","f4923827":"markdown","fb1f7d74":"markdown","fa5f7ab5":"markdown","b90e953d":"markdown","2dee4c59":"markdown","bca2d55a":"markdown","95104716":"markdown","cf4d082b":"markdown","31d4d8cf":"markdown","50b61a5c":"markdown","d771d481":"markdown","3df4f9ee":"markdown","59bcbcc9":"markdown","fd4d75f1":"markdown","c1e8fc2f":"markdown","8fff710f":"markdown","8dc2fe51":"markdown","bf5190cf":"markdown","7c433f74":"markdown","f9da9e53":"markdown","bbeb3a2d":"markdown","d0172546":"markdown","1951c8d1":"markdown","fadbbff3":"markdown","92437d13":"markdown","4d56a06e":"markdown","b0e4e283":"markdown","6dff91b3":"markdown","b544c3d4":"markdown","fb6b0f3c":"markdown","29db92b7":"markdown"},"source":{"8690442e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c62b8e27":"# importing required libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport datetime as dt\nimport matplotlib.pyplot as plt\n%matplotlib inline","04b52fa0":"#Read in CSV\ngs_df = pd.read_csv('..\/input\/global-super-store-dataset\/Global_Superstore2.csv', encoding='latin1')\n\n#convert Column names\ngs_df.rename(columns={'Row ID': 'Row_ID', 'Order ID': 'Order_ID', 'Order Date': 'Order_Date', 'Ship Date': 'Ship_Date', 'Ship Mode': 'Ship_Mode',\n                     'Customer ID': 'Customer_ID', 'Customer Name': 'Customer_Name', 'Product ID': 'Product_ID', 'Sub-Category': 'Sub_category',\n                      'Product Name': 'Product_Name', 'Shipping Cost': 'Shipping_Cost', 'Order Priority': 'Order_Priority'}, inplace=True)\n\n#convert Order Date to datetime format\ngs_df['Order_Date'] = pd.to_datetime(gs_df['Order_Date'])\n\n#convert Ship Date to datetime format\ngs_df['Ship_Date'] = pd.to_datetime(gs_df['Ship_Date'])\n\n#sort columns by order date\ngs_df = gs_df.sort_values(by='Order_Date', ascending=True)\n\n","7d18c96c":"#confirm data import\ngs_df.head()","84ef65ed":"#check column datatypes and non-null counts\ngs_df.info()","490a103a":"gs_df.isnull().sum()","06954ba6":"gs_df.describe()","fe8a3798":"sns.boxplot(x=gs_df['Sales'])","564b9fa1":"gs_df.nunique()","4500a49f":"gs_df['Year'] = pd.DatetimeIndex(gs_df['Order_Date']).year","bfec4f01":"pronamagg_df = gs_df.groupby([\"Product_Name\", \"Year\"]).Sales.sum().reset_index()","5a6c6abd":"pronamagg_df.head()","0d0dc442":" propivot_df = pd.pivot_table(pronamagg_df, values='Sales', index=['Year',],\n                    columns=['Product_Name'])","4d70abf2":"propivot_df.head()","cff51e1a":"pctpivot_df = propivot_df.pct_change()","5b36b8ce":"pctpivot_df.head()","cdc5f481":"pctchg_df = pctpivot_df.stack().reset_index(name='Pct_Change')","2b312a52":"pctchg_df.head()","c1e6957d":"topproduct2014_df = pctchg_df[pctchg_df['Year'] == 2014].sort_values(by='Pct_Change', ascending=False, ignore_index=True).head(5)\ntopproduct2013_df = pctchg_df[pctchg_df['Year'] == 2013].sort_values(by='Pct_Change', ascending=False, ignore_index=True).head(5)\ntopproduct2012_df = pctchg_df[pctchg_df['Year'] == 2012].sort_values(by='Pct_Change', ascending=False, ignore_index=True).head(5)\ntopproduct2011_df = pctchg_df[pctchg_df['Year'] == 2011].sort_values(by='Pct_Change', ascending=False, ignore_index=True).head(5)","214fe587":"tpg_df = pd.concat([topproduct2014_df, topproduct2013_df,topproduct2012_df,topproduct2011_df]).reset_index(drop=True)\n","4c3aadd6":"\nax = sns.catplot(x=\"Product_Name\", y=\"Pct_Change\", hue=\"Year\", data=tpg_df, kind='bar',height=8, aspect=15\/8)\nplt.xticks(rotation=90)","e68ccac8":"gs_df[gs_df.Discount == 0.0].sort_values(by='Profit', ascending=False).head(10)\n","31e2357e":"gs_df['Month'] = pd.DatetimeIndex(gs_df['Order_Date']).month","719f534b":"gs_df.head()","d48929aa":"seasonal_df = gs_df.groupby([\"Product_Name\", \"Month\"]).Sales.sum().reset_index()","d27c8647":"seasonal_df.sort_values(by ='Sales', ascending=False)","874fbc2e":"seasonal_pivot = pd.pivot_table(seasonal_df, values='Sales', index=['Month',],\n                    columns=['Product_Name'])","1dd3026b":"seasonal_pivot","474b5e6b":"region_df = gs_df.groupby([\"Product_Name\", \"Region\"]).Sales.sum().reset_index()","8f58b960":"region_df.head()","05fd8376":"gs_df.columns","e8408621":"# purchase frequency = no of orders \/ no of unique customers (365 days)\n#gs_customerdf['Order ID'].groupby('Customer ID').count()","0edfda97":"#### *Missing 41,296 postal codes*","037f4cde":"As predicted, there is a $20,000+ sale skewing the statistics","924d786f":"Now I create a dataframe for each year with only the top 5 products according to sales growth of that year. ","86cbf613":"#### *And now we verify the data was imported properly*","053cce05":"First we need to extract the month from the order date to a new column.","349bdd4e":"## *Which Product Generates the Most Profit?*","117ee122":"### Product Analysis\n- Which products have increased sales over the years?\n- Which product generates the most profit?\n- Which top selling products are seasonal buys?\n- Do some products sell more in certain regions than others? If so which regions?","f4923827":"## Top 5 Products Each Year by Sales Growth","fb1f7d74":"#### *We need to check for univariate outliers in Sales*","fa5f7ab5":"We have 3,788 product names.\nNext let's extract the year from each order date to it's own column.","b90e953d":"Next we calculate the percentage of change from year to year. ","2dee4c59":"***","bca2d55a":"### Sales Analysis\n- What are the YTD sales Year over Year?\n- Which months have the highest sales?\n- Which country has the highest sales?\n- Which shipment mode has the highest sales?","95104716":"#### *Let's check our column types and non-null counts*","cf4d082b":"## *Do some products sell more in certain regions than others? If so which regions?*","31d4d8cf":"#### *First we will import our libraries*","50b61a5c":"***","d771d481":"I then reverse the pivot table, turning the product names back into rows.","3df4f9ee":"## *Which top selling products are seasonal buys?*","59bcbcc9":"**We can determine a few things right away:**\n- Max Sale was 22,638.48 vs minimum sale of .44\n- Minimum Profit is negative\n- The maximum discount given was 85%","fd4d75f1":"### Customer Analysis\n- Show the customer frequency of purchase\n- Who are the top 5 customers by profit each year?\n- Who are the top 5 customers by sales each year?\n- Show top 5 customer growth over years avalaible","c1e8fc2f":"Let's look at how many products total we are dealing with:","8fff710f":"Now I will combine those dataframes back into a single table for plotting","8dc2fe51":"We can pivot the table to show total sales for each month according to product name. ","bf5190cf":"Sale profit includes order discount. To find which products are the most profitable we must look at products with no discount applied. ","7c433f74":"## Product Analysis\n### *Which products have increased sales over the years?*","f9da9e53":"## We can see here that the *Canon imageCLASS 2200 Advanced Copier* takes the top 3 slots with most profit and no discount applied. \n\n***","bbeb3a2d":"Because 2011 is the first year of data we have, the values will all be NaN.","d0172546":"## *The table shows that most products appear to be seasonal buys. A deeper dive would be required to find out if these products are available to purchase year round or not.*\n\n***","1951c8d1":"The Global Superstore dataset has a great deal of info we can use to set up KPIs and a dashboard for stakeholders. In the following analysis we will attempt to answer the following questions:","fadbbff3":"# Exploratory Data Analysis for Global Superstore","92437d13":"First we need to determine sales by region.","4d56a06e":"### This tells us that in 2014, the products with the most sales growth were *Eaton Memo Slips-Premium,Tenex Door Stop-Duo Pack, Advantus Stacking Tray-Durable, 3.6 cubic foot Refrigerator, and Nokia Audio Dock-Cordless.*","b0e4e283":"#### *Next we will import our dataset, convert the dates to datetime format and  order the data by Order Date.*","6dff91b3":"Next I aggregate the sales of each item for each year and group according to product name and year","b544c3d4":"#### *Let's do a quick statistical analysis*","fb6b0f3c":"Now I create a pivot table, creating a column for each product name.","29db92b7":"Then we can aggregate sales and order by month."}}