{"cell_type":{"5e6cf5bb":"code","a96a819e":"code","1cf6d8d3":"code","674207b3":"code","e4d75676":"code","2130080c":"code","4d7954bc":"code","0ab5f37a":"code","5cb45ac1":"code","3d59b2e8":"code","a971ce67":"code","c842d9f0":"code","957b5677":"code","fd4133cd":"code","a3892d48":"code","98f0b6f2":"code","a80b6ec9":"code","0a5f12ab":"code","25ed0d32":"code","d4234720":"code","b6ae4e3d":"code","0ea69097":"code","eda14a78":"code","048192cc":"code","295f9783":"code","89e99fc8":"code","b6003127":"code","8197dca4":"markdown","3320b54e":"markdown","9b8bbc0a":"markdown","edf6aef2":"markdown","aed5b097":"markdown","47d6f3f1":"markdown","8b9fe96b":"markdown","3596e00b":"markdown","10fe729e":"markdown","26eb2d99":"markdown","4b39d108":"markdown"},"source":{"5e6cf5bb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a96a819e":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow import keras","1cf6d8d3":"train_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_df =  pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsubmission =pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\ntrain_df.head()","674207b3":"train_df.shape, test_df.shape","e4d75676":"labels = train_df['target']\ntrain_df = train_df.drop(['keyword','location','target'],axis = 1)\ntest_df = test_df.drop(['keyword','location'],axis = 1)","2130080c":"train_df.shape,test_df.shape","4d7954bc":"labels.value_counts().plot(kind = 'bar')","0ab5f37a":"train_length = train_df['text'].str.len()\ntest_length = test_df['text'].str.len()\nplt.figure(figsize = (8,3))\nplt.hist(train_length,bins = 30, label = \"train_texts\")\nplt.hist(test_length,bins = 30, label = \"test_texts\")\nplt.xlim(0,180)\nplt.legend()\nplt.show()","5cb45ac1":"combined_data = pd.concat([train_df,test_df],ignore_index = True)","3d59b2e8":"combined_data.shape","a971ce67":"stemmer = PorterStemmer()\n#lemmatizer = WordNetLemmatizer()\nclean = []\nfor i in range(0,len(combined_data)):\n    clean_text = re.sub('[^a-zA-Z]',' ',combined_data['text'][i])\n    clean_text = clean_text.lower()\n    clean_text = clean_text.split()\n    \n    #clean_text = [lemmatizer.lemmatize(word) for word in clean_text if word not in stopwords.words('english')]\n    clean_text = [stemmer.stem(word) for word in clean_text if word not in stopwords.words('english')]\n    clean_text = ' '.join(clean_text)\n    clean.append(clean_text)\n\ncombined_data['clean_text']= clean","c842d9f0":"type(combined_data['clean_text'])","957b5677":"unique = []\nmaximum = 0\nfor i in range(0,len(combined_data)):\n    words =str( combined_data['clean_text'][i]).split()\n    if len(words) > maximum:\n        maximum = len(words)\n    for word in words:\n        if word not in unique:\n            unique.append(word)\n            \nlen(unique),print(maximum)","fd4133cd":"vocab_size = 25000","a3892d48":"onehot_rep = [one_hot(words,vocab_size) for words in combined_data['clean_text']]","98f0b6f2":"sent_length = 30\nembedded_docs = pad_sequences(onehot_rep,padding = 'pre',maxlen = sent_length)\nembedded_docs","a80b6ec9":"embedding_feature_vectors = 60\nmodel = Sequential()\nmodel.add(Embedding(vocab_size,embedding_feature_vectors,input_length =sent_length))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(128,activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1,activation = 'sigmoid'))\nmodel.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics = ['accuracy'])\nprint(model.summary())","0a5f12ab":"train = embedded_docs[:7613]\ntest = embedded_docs[7613:]","25ed0d32":"train.shape","d4234720":"x_train,x_test,y_train,y_test = train_test_split(train,labels,test_size = 0.3,random_state = 48)","b6ae4e3d":"model.fit(x_train,y_train,validation_data = (x_test,y_test),epochs = 10,batch_size = 64)","0ea69097":"y_predict = model.predict_classes(x_test)","eda14a78":"x_test.shape,y_predict.shape","048192cc":"f1_score(y_test,y_predict)","295f9783":"predict = model.predict(test)","89e99fc8":"submission['target']= predict\n","b6003127":"submission.to_csv('submission.csv',index = False);","8197dca4":"one_hot() take two inputs, 1-> words int the text,2->vocab_size\n\nVocab_size can be any number. so I have set it to a range of unique words in my dataset.Basically what it does is \n","3320b54e":"# Model Training","9b8bbc0a":"# Submission file","edf6aef2":"Train set is not highly imbalanced so we can go on with dataset, Otherwise we would have over sampled the data\n\n","aed5b097":"# OneHot Representaion","47d6f3f1":"# Embedding Representation","8b9fe96b":"# Info about the Data.\nid -> a unique identifier for each tweet.\n\ntext -> the text of the tweet.\n\nlocation -> the location the tweet was sent from (may be blank).\n\nkeyword -> a particular keyword from the tweet (may be blank).\n\ntarget -> in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0).","3596e00b":"# Predicting ","10fe729e":"Since it is a NLP problem we will get rid of Location and keyword Column\n\n","26eb2d99":"cleaning text file and appending it to dataframe","4b39d108":"Distribution of lengths of tweets in test and train dataset\n"}}