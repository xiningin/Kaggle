{"cell_type":{"54b28abe":"code","d1fb2856":"code","5748531c":"code","89b99125":"code","1db1401f":"code","0d4dcad0":"code","f179be8f":"code","fe23584c":"code","d6461a42":"code","cecad0ca":"code","c632096b":"code","4857fb72":"code","d087749e":"code","c6aadb3c":"code","df4391a0":"code","5ad6050e":"code","e8e1ffb2":"code","a17c4a47":"code","a658ffcc":"code","d6bbf8f9":"code","d732a492":"code","3e1c0ce5":"code","dca49446":"code","9aec705b":"code","33376752":"markdown","d177fe35":"markdown","11ea3b01":"markdown","6fd44dd7":"markdown","005c86db":"markdown"},"source":{"54b28abe":"# data preprocessing for two classes: classes=['history', 'tech']\n!mkdir data && wget https:\/\/www.dropbox.com\/s\/7qimiz727clhj6s\/ua_wiki.zip && unzip ua_wiki.zip -d data","d1fb2856":"!rm -rf .\/ua_wiki\n!mkdir .\/ua_wiki\n!mv .\/data\/train .\/ua_wiki\/\n!mv .\/data\/test .\/ua_wiki\/\n!ls -GFlash --color .\/ua_wiki\/","5748531c":"!ls .\/ua_wiki\/train\/history|head","89b99125":"!ls .\/ua_wiki\/train\/tech|head","1db1401f":"!ls .\/ua_wiki\/test\/history|head","0d4dcad0":"!ls .\/ua_wiki\/test\/tech|head","f179be8f":"!pip install --upgrade tensorflow\n!pip install ktrain","fe23584c":"import ktrain\nfrom ktrain import text\nktrain.__version__","d6461a42":"#\n(x_train, y_train), (x_test, y_test), preproc = text.texts_from_folder('.\/ua_wiki\/', \n                                                                       maxlen=75, \n                                                                       max_features=10000,\n                                                                       preprocess_mode='bert',\n                                                                       train_test_names=['train', 'test'], \n                                                                       #val_pct=0.1,\n                                                                       classes=['history', 'tech'])","cecad0ca":"# Create a Model and Wrap in Learner Object\nmodel = text.text_classifier('bert', (x_train, y_train) , preproc=preproc)\nlearner = ktrain.get_learner(model, \n                             train_data=(x_train, y_train), \n                             val_data=(x_test, y_test), \n                             batch_size=32)","c632096b":"# STEP 3: Train the Model\nlearner.fit_onecycle(2e-5, 3, checkpoint_folder='..\/kaggle\/working\/saved_weights')","4857fb72":"learner.validate(val_data=(x_test, y_test))","d087749e":"# Inspecting Misclassifications\nlearner.view_top_losses(n=3, preproc=preproc)","c6aadb3c":"# Making Predictions on New Data\np = ktrain.get_predictor(learner.model, preproc)","df4391a0":"p.get_classes()","5ad6050e":"# Predicting label for the text\np.predict(\"\u041a\u043e\u0437\u0430\u0446\u044c\u043a\u0456 \u043f\u043e\u0432\u0441\u0442\u0430\u043d\u043d\u044f \u0431\u0443\u043b\u0438 \u0444\u043e\u0440\u043c\u043e\u044e \u0431\u043e\u0440\u043e\u0442\u044c\u0431\u0438 \u0437\u0430 \u0432\u043e\u043b\u044e \u0443\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u043e\u0433\u043e \u043d\u0430\u0440\u043e\u0434\u0443.\")","e8e1ffb2":"# Predicting label for the text\np.predict(\"\u0423 \u043c\u043e\u0454\u043c\u0443 \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u0456 \u0432\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u0435\u043d\u043e \u043f\u043e\u0442\u0443\u0436\u043d\u0438\u0439 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440.\")","a17c4a47":"p.save('..\/kaggle\/working\/ua_wiki_pred')","a658ffcc":"fin_bert_model = ktrain.load_predictor('..\/kaggle\/working\/ua_wiki_pred')","d6bbf8f9":"# still works\nfin_bert_model.predict('\u041f\u0440\u043e\u0433\u0440\u0430\u043c\u0443\u0432\u0430\u043d\u043d\u044f \u0454 \u0434\u043e\u0441\u0438\u0442\u044c \u0432\u0430\u0436\u043b\u0438\u0432\u043e\u044e \u0433\u0430\u043b\u0443\u0437\u0437\u044e \u0406\u0422 \u0456\u043d\u0434\u0443\u0441\u0442\u0440\u0456\u0457.')","d732a492":"# still works\nfin_bert_model.predict('\u041f\u043e\u043b\u044c\u0441\u044c\u043a\u0430 \u0448\u043b\u044f\u0445\u0442\u0430 \u0434\u043e\u0441\u0438\u0442\u044c \u0447\u0430\u0441\u0442\u043e \u043d\u0435 \u0432\u0440\u0430\u0445\u043e\u0432\u0443\u0432\u0430\u043b\u0430 \u0456\u043d\u0442\u0435\u0440\u0435\u0441\u0438 \u043a\u043e\u0437\u0430\u043a\u0456\u0432.')","3e1c0ce5":"# still works\nfin_bert_model.predict('\u0427\u0438 \u043c\u0430\u043b\u0430 \u0423\u043a\u0440\u0430\u0457\u043d\u0430 \u0448\u0430\u043d\u0441\u0438 \u043d\u0430 \u0443\u0441\u043f\u0456\u0445 \u0443 \u0432\u0438\u0437\u0432\u043e\u043b\u044c\u043d\u0456\u0439 \u0431\u043e\u0440\u043e\u0442\u044c\u0431\u0456?')","dca49446":"# still works\nfin_bert_model.predict('\u0424\u0443\u0442\u0431\u043e\u043b \u043d\u0435 \u0454 \u043d\u0430\u0448\u0438\u043c \u043d\u0430\u0446\u0456\u043e\u043d\u0430\u043b\u044c\u043d\u0438\u043c \u0441\u043f\u043e\u0440\u0442\u043e\u043c.')","9aec705b":"# still works\nfin_bert_model.predict(\"\u0427\u0438 \u043c\u043e\u0436\u0435\u043c\u0438 \u043c\u0438 \u0432\u0438\u0440\u043e\u0431\u043b\u044f\u0442\u0438 \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u0438.\")","33376752":"### STEP 4: Save Predictor for Later Deployment","d177fe35":"## STEP 1:  Load and Preprocess the Data\n\nFirst, we use the `texts_from_folder` function to load and preprocess the data.  We assume that the data is in the following form:\n```\n    \u251c\u2500\u2500 datadir\n    \u2502   \u251c\u2500\u2500 train\n    \u2502   \u2502   \u251c\u2500\u2500 class0       # folder containing documents of class 0\n    \u2502   \u2502   \u251c\u2500\u2500 class1       # folder containing documents of class 1\n    \u2502   \u2502   \u251c\u2500\u2500 class2       # folder containing documents of class 2\n    \u2502   \u2502   \u2514\u2500\u2500 classN       # folder containing documents of class N\n```\nWe set `val_pct` as 0.1, which will automatically sample 10% of the data for validation.  Since we will be using a pretrained BERT model for classification, we specifiy `preprocess_mode='bert'`.  If you are using any other model (e.g., `fasttext`), you should either omit this parameter or use `preprocess_mode='standard'`).\n\n**Notice that there is nothing speical or extra we need to do here for non-English text.**  *ktrain* automatically detects the language and character encoding and prepares the data and configures the model appropriately.\n","11ea3b01":"### STEP 3:  Predicting with fine-tuning Bert for ukrainian content","6fd44dd7":"THE END","005c86db":"## STEP 2:  Using Bert for ukrainian content"}}