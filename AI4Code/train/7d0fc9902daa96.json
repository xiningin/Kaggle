{"cell_type":{"8a2e750b":"code","8d9f7b10":"code","98ef18a4":"code","63411f69":"code","12abb072":"code","de5d7e26":"code","846fbeb4":"code","7d40075d":"code","efb6ebf3":"code","a1773804":"code","744c129a":"code","c87c7fdf":"code","838b395d":"code","9a7cd565":"code","7b05ca78":"code","c455e759":"code","afa121fb":"code","e36be747":"code","db163313":"code","da6831ed":"markdown","ce1928d8":"markdown","a2f65ed7":"markdown","6a762364":"markdown","b61280be":"markdown","9c225550":"markdown","f7d380bf":"markdown","b750bc06":"markdown","d24bc293":"markdown","1e7ec65b":"markdown","f7b6a3ce":"markdown","593de579":"markdown","ca9e8cc6":"markdown","c9d2afb5":"markdown","39d6b264":"markdown","dcde4686":"markdown"},"source":{"8a2e750b":"import random \nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.cluster import KMeans \nfrom sklearn.datasets import make_blobs \n%matplotlib inline\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","8d9f7b10":"np.random.seed(0)","98ef18a4":"X, y = make_blobs(n_samples=5000, centers=[[4,4], [-2, -1], [2, -3], [1, 1]], cluster_std=0.9)","63411f69":"plt.scatter(X[:, 0], X[:, 1], marker='.')","12abb072":"k_means = KMeans(init = \"k-means++\", n_clusters = 4, n_init = 12)\nk_means.fit(X)","de5d7e26":"k_means_labels = k_means.labels_\nk_means_labels","846fbeb4":"k_means_cluster_centers = k_means.cluster_centers_\nk_means_cluster_centers","7d40075d":"# Initialize the plot with the specified dimensions.\nfig = plt.figure(figsize=(6, 4))\n\n# Colors uses a color map, which will produce an array of colors based on\n# the number of labels there are. We use set(k_means_labels) to get the\n# unique labels.\ncolors = plt.cm.Spectral(np.linspace(0, 1, len(set(k_means_labels))))\n\n# Create a plot\nax = fig.add_subplot(1, 1, 1)\n\n# For loop that plots the data points and centroids.\n# k will range from 0-3, which will match the possible clusters that each\n# data point is in.\nfor k, col in zip(range(len([[4,4], [-2, -1], [2, -3], [1, 1]])), colors):\n\n    # Create a list of all data points, where the data poitns that are \n    # in the cluster (ex. cluster 0) are labeled as true, else they are\n    # labeled as false.\n    my_members = (k_means_labels == k)\n    \n    # Define the centroid, or cluster center.\n    cluster_center = k_means_cluster_centers[k]\n    \n    # Plots the datapoints with color col.\n    ax.plot(X[my_members, 0], X[my_members, 1], 'w', markerfacecolor=col, marker='.')\n    \n    # Plots the centroids with specified color, but with a darker outline\n    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,  markeredgecolor='k', markersize=6)\n\n# Title of the plot\nax.set_title('KMeans')\n\n# Remove x-axis ticks\nax.set_xticks(())\n\n# Remove y-axis ticks\nax.set_yticks(())\n\n# Show the plot\nplt.show()\n","efb6ebf3":"k_means2 = KMeans(init = \"k-means++\", n_clusters = 3, n_init = 12)\nk_means2.fit(X)","a1773804":"labels_2 = k_means2.labels_\ncentroids_2 = k_means2.cluster_centers_","744c129a":"fig = plt.figure(figsize=(6,4))\ncolors2 = plt.cm.Spectral(np.linspace(0,1,len(set(labels_2))))\nax = fig.add_subplot(1,1,1)\n\n\nfor k, col in zip(range(len(set(labels_2))), colors2):\n    my_members = (labels_2 == k)\n    \n    cluster_center = centroids_2[k]\n    \n    ax.plot(X[my_members,0], X[my_members,1], \"w\", markerfacecolor=col, marker = \".\")\n    \n    ax.plot(cluster_center[0], cluster_center[1], \"o\", markerfacecolor=col, markeredgecolor=\"k\", markersize=6)\n    \nax.set_title(\"KMeans\")\nax.set_xticks(())\nax.set_yticks(())\nplt.show()","c87c7fdf":"!wget -O Cust_Segmentation.csv https:\/\/cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud\/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork\/labs\/Module%204\/data\/Cust_Segmentation.csv\n\nimport pandas as pd\ncust_df = pd.read_csv(\"Cust_Segmentation.csv\")\ncust_df.head()","838b395d":"df = cust_df.drop('Address', axis=1)\ndf.head()","9a7cd565":"from sklearn.preprocessing import StandardScaler\nX = df.values[:,1:]\nX = np.nan_to_num(X)\nClus_dataSet = StandardScaler().fit_transform(X)\nClus_dataSet","7b05ca78":"clusterNum = 3\nk_means = KMeans(init = \"k-means++\", n_clusters = clusterNum, n_init = 12)\nk_means.fit(X)\nlabels = k_means.labels_\nprint(labels)","c455e759":"df[\"Clus_km\"] = labels\ndf.head(5)","afa121fb":"df.groupby('Clus_km').mean()","e36be747":"area = np.pi * ( X[:, 1])**2  \nplt.scatter(X[:, 0], X[:, 3], s=area, c=labels.astype(np.float), alpha=0.5)\nplt.xlabel('Age', fontsize=18)\nplt.ylabel('Income', fontsize=16)\n\nplt.show()\n","db163313":"from mpl_toolkits.mplot3d import Axes3D \nfig = plt.figure(1, figsize=(8, 6))\nplt.clf()\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n\nplt.cla()\n# plt.ylabel('Age', fontsize=18)\n# plt.xlabel('Income', fontsize=16)\n# plt.zlabel('Education', fontsize=16)\nax.set_xlabel('Education')\nax.set_ylabel('Age')\nax.set_zlabel('Income')\n\nax.scatter(X[:, 1], X[:, 0], X[:, 3], c= labels.astype(np.float))\n","da6831ed":"<h2 id=\"insights\">Insights<\/h2>\n\nWe assign the labels to each row in dataframe.\n","ce1928d8":"We can easily check the centroid values by averaging the features in each cluster.\n","a2f65ed7":"As you can see, **Address** in this dataset is a categorical variable. k-means algorithm isn't directly applicable to categorical variables because Euclidean distance function isn't really meaningful for discrete variables. So, lets drop this feature and run clustering.\n","6a762364":"#### Normalizing over the standard deviation\n\nNow let's normalize the dataset. But why do we need normalization in the first place? Normalization is a statistical method that helps mathematical-based algorithms to interpret features with different magnitudes and distributions equally. We use **StandardScaler()** to normalize our dataset.\n","b61280be":"k-means will partition your customers into mutually exclusive groups, for example, into 3 clusters. The customers in each cluster are similar to each other demographically.\nNow we can create a profile for each group, considering the common characteristics of each cluster. \nFor example, the 3 clusters can be:\n\n-   AFFLUENT, EDUCATED AND OLD AGED\n-   MIDDLE AGED AND MIDDLE INCOME\n-   YOUNG AND LOW INCOME\n","9c225550":"### Load Data From CSV File\n\nBefore you can work with the data, you must use the URL to get the Cust_Segmentation.csv.\n","f7d380bf":"<h1 id=\"customer_segmentation_K_means\">Customer Segmentation with K-Means<\/h1>\n\nImagine that you have a customer dataset, and you need to apply customer segmentation on this historical data.\nCustomer segmentation is the practice of partitioning a customer base into groups of individuals that have similar characteristics. It is a significant strategy as a business can target these specific groups of customers and effectively allocate marketing resources. For example, one group might contain customers who are high-profit and low-risk, that is, more likely to purchase products, or subscribe for a service. A business task is to retaining those customers. Another group might include customers from non-profit organizations. And so on.\n\nLets download the dataset. To download the data, we will use **`!wget`** to download it from IBM Object Storage.  ","b750bc06":"<h2 id=\"creating_visual_plot\">Creating the Visual Plot<\/h2>\n\nSo now that we have the random data generated and the KMeans model initialized, let's plot them and see what it looks like!\n","d24bc293":"## Modeling\n\nIn our example (if we didn't have access to the k-means algorithm), it would be the same as guessing that each customer group would have certain age, income, education, etc, with multiple tests and experiments. However, using the K-means clustering we can do all this process much easier.\n\nLets apply k-means on our dataset, and take look at cluster labels.\n","1e7ec65b":"<h2 id=\"pre_processing\">Pre-processing<\/h2\n","f7b6a3ce":"## Practice\n\nTry to cluster the above dataset into 3 clusters.  \nNotice: do not generate data again, use the same dataset as above.\n","593de579":"Now, lets look at the distribution of customers based on their age and income:\n","ca9e8cc6":"<h1 id=\"random_generated_dataset\">k-Means on a randomly generated dataset<\/h1>\n\nLets create our own dataset","c9d2afb5":"<h2 id=\"setting_up_K_means\">Setting up K-Means<\/h2>\nNow that we have our random data, let's set up our K-Means Clustering.\n\nThe KMeans class has many parameters that can be used, but we will be using these three:\n\n<ul>\n    <li> <b>init<\/b>: Initialization method of the centroids. <\/li>\n    <ul>\n        <li> Value will be: \"k-means++\" <\/li>\n        <li> k-means++: Selects initial cluster centers for k-mean clustering in a smart way to speed up convergence.<\/li>\n    <\/ul>\n    <li> <b>n_clusters<\/b>: The number of clusters to form as well as the number of centroids to generate. <\/li>\n    <ul> <li> Value will be: 4 (since we have 4 centers)<\/li> <\/ul>\n    <li> <b>n_init<\/b>: Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. <\/li>\n    <ul> <li> Value will be: 12 <\/li> <\/ul>\n<\/ul>\n\nInitialize KMeans with these parameters, where the output parameter is called <b>k_means<\/b>.\n","39d6b264":"First we need to set up a random seed.","dcde4686":"Next we will be making <i> random clusters <\/i> of points by using the <b> make_blobs <\/b> class. The <b> make_blobs <\/b> class can take in many inputs, but we will be using these specific ones. <br> <br>\n<b> <u> Input <\/u> <\/b>\n\n<ul>\n    <li> <b>n_samples<\/b>: The total number of points equally divided among clusters. <\/li>\n    <ul> <li> Value will be: 5000 <\/li> <\/ul>\n    <li> <b>centers<\/b>: The number of centers to generate, or the fixed center locations. <\/li>\n    <ul> <li> Value will be: [[4, 4], [-2, -1], [2, -3],[1,1]] <\/li> <\/ul>\n    <li> <b>cluster_std<\/b>: The standard deviation of the clusters. <\/li>\n    <ul> <li> Value will be: 0.9 <\/li> <\/ul>\n<\/ul>\n<br>\n<b> <u> Output <\/u> <\/b>\n<ul>\n    <li> <b>X<\/b>: Array of shape [n_samples, n_features]. (Feature Matrix)<\/li>\n    <ul> <li> The generated samples. <\/li> <\/ul> \n    <li> <b>y<\/b>: Array of shape [n_samples]. (Response Vector)<\/li>\n    <ul> <li> The integer labels for cluster membership of each sample. <\/li> <\/ul>\n<\/ul>\n"}}