{"cell_type":{"249a6d73":"code","de21cd13":"code","56d02b84":"code","51187ea5":"code","2430ae8a":"code","3dda2c1d":"code","ad58e905":"code","4902702d":"code","a8d92175":"code","b2abd0f4":"code","e4a67a87":"code","374b4078":"code","57e9e714":"code","c3701689":"code","a57265b9":"code","efba6795":"code","8f5c3f80":"code","82e0e72e":"code","a193a1b7":"code","8d3a42ab":"code","5eb6393d":"code","9468adb3":"code","adbbedb9":"code","cabb0b05":"code","4f03c66f":"code","e029311b":"code","9e735598":"code","faa32167":"code","80c9e432":"code","2e566544":"markdown","c805ab78":"markdown","c86e9913":"markdown","bfb87fd1":"markdown","47115515":"markdown","5b330907":"markdown","d22f9b2e":"markdown","cbde3429":"markdown","fcceebc6":"markdown","0df49ee1":"markdown","a6f09dd2":"markdown","ad7d9dce":"markdown","c1d0ef92":"markdown","90d00f87":"markdown"},"source":{"249a6d73":"import pandas as pd","de21cd13":"# Read the titanic train.csv and test.csv file into individual Pandas dataframes.\ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')","56d02b84":"# Show the top 5 rows of the train data.\ntrain_data.head()","51187ea5":"# Show the top 5 rows of the test data.\ntest_data.head()","2430ae8a":"# View number of rows and columns in test and train.\nprint(\"train_data shape: {}\".format(train_data.shape))\nprint(\"test_data shape: {}\".format(test_data.shape))","3dda2c1d":"# View columns data types and check for any null\/missing values in train.\ntrain_data.info()","ad58e905":"# View columns data types and check for any null\/missing values in test.\ntest_data.info()","4902702d":"# Show descriptive statistics of each numerical column in train.\ntrain_data.describe()","a8d92175":"# Show descriptive statistics of each numerical column in test.\ntest_data.describe()","b2abd0f4":"# Import visualisation packages and set plot theme.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")","e4a67a87":"# Show heatmap of both dataframes displaying missing\/NaN values.\nfig, ax = plt.subplots(1,2, figsize=(10,5))\nsns.heatmap(train_data.isnull(), cbar=False, ax=ax[0]).set_title(\"train_data\")\nsns.heatmap(test_data.isnull(), cbar=False, ax=ax[1]).set_title(\"test_data\")\nfig.tight_layout()\nfig.show()","374b4078":"# View the data, split by sex and class on various subplots.\nfig, ax = plt.subplots(2,3, figsize=(10,10))\nsns.countplot(data=train_data, x='Pclass', hue='Sex', ax=ax[0,0])\nsns.histplot(train_data, x='Age', hue='Sex', ax=ax[0,1], multiple=\"stack\")\nsns.countplot(data=train_data, x='Survived', hue='Sex', ax=ax[0,2])\nsns.countplot(data=train_data, x='Embarked', hue='Pclass', ax=ax[1,0])\nsns.violinplot(data=train_data, x='Pclass', y='Fare', ax=ax[1,1])\nsns.countplot(data=train_data, x='Survived', hue='Pclass', ax=ax[1,2])\nfig.tight_layout()\nfig.suptitle(\"Data split by Sex and Class\", y=1.03)\nfig.show()","57e9e714":"# Show missing ages by class and mean age in each class.\nno_age = train_data[train_data['Age'].isnull()]\nsns.countplot(data=no_age, x='Pclass')\nplt.title(\"Missing Ages by Class\")\nplt.show()\nprint(\"Mean age by class: {}\".format(train_data.groupby('Pclass')['Age'].mean()))","c3701689":"# Function to fill in missing ages.\ndef missing_ages(df):\n    for i in range(1,4):\n        df.loc[(df[\"Pclass\"] == i) & (df[\"Age\"].isnull()), 'Age'] = float(df[df['Pclass'] == i].groupby('Pclass')['Age'].mean())","a57265b9":"# Run the missing age function and see if there are anymore missing ages.\nmissing_ages(train_data)\n\nprint(\"Remaining missing ages: {}\".format(train_data['Age'].isna().sum()))","efba6795":"# Amend Cabin column to only contain the first letter i.e. Deck and show the data by Class.\ndef deck_count(df):\n    fig, ax = plt.subplots(figsize=(10,5))\n    df['Cabin'] = df['Cabin'].str.extract('(\\D+)', expand=False)\n    graph = sns.countplot(data=df, x='Cabin', \n                          hue='Pclass', \n                          order=['A', 'B', 'C', 'D', 'E', 'F', 'F E', 'F G', 'G', 'T'])\n\n    # Create a pivot table to show the number of people on each deck, including 0.\n    cabin_class_pivot = df.pivot_table(index='Cabin',\n                                       columns='Pclass', \n                                       values='PassengerId',\n                                       fill_value=0, \n                                       aggfunc='count').unstack()\n\n    # Show the counts at the top of each bar of the countplot, using the pivoted dataframe.\n    for i, num in enumerate(cabin_class_pivot):\n            graph.text(graph.patches[i].get_x() + graph.patches[i].get_width() \/ 2., \n                       graph.patches[i].get_height() + 0.2, \n                       num,\n                       ha=\"center\")\n        \n    plt.title(\"Number of Passengers on each Deck by Class\")\n    plt.xlabel(\"Deck\")\n    plt.show()\n\n    print(\"Passenger count by Class: {}\".format(train_data.groupby('Pclass')['PassengerId'].count()))","8f5c3f80":"deck_count(train_data)","82e0e72e":"# Import random, create a dictionary of decks and create a function to impute missing deck letters.\nimport random\n\nclass_decks = {'1':'A', '2':'B', '3':'C', '4':'D', '5':'E', '6':'F', '7':'F E', '8':'F G', '9':'G'}\n\ndef fill_cabins(df):\n    for i, row in df.iterrows():\n        if pd.isnull(df['Cabin'][i]) and df['Pclass'][i] == 1:\n            df.loc[i, 'Cabin'] = class_decks[str(random.randint(1,5))]\n        elif pd.isnull(df['Cabin'][i]) and df['Pclass'][i] == 2:\n            df.loc[i, 'Cabin'] = class_decks[str(random.randint(4,6))]\n        elif pd.isnull(df['Cabin'][i]) and df['Pclass'][i] == 3:\n            df.loc[i, 'Cabin'] = class_decks[str(random.randint(5,9))]\n        continue","a193a1b7":"# Impute missing decks and check the new count\nfill_cabins(train_data)\ndeck_count(train_data)\nprint(\"Number of missing values in the 'Cabin' column: {}\".format(train_data['Cabin'].isna().sum()))","8d3a42ab":"# Drop NaN rows and change data types.\ntrain_data.dropna(inplace=True)\ntrain_data = train_data.astype({'Pclass': 'category', 'Sex': 'category', 'Cabin': 'category', 'Embarked': 'category'})\ntrain_data.info()","5eb6393d":"# Drop \"Ticket\" column.\ntrain_data = train_data.drop(['Ticket'], axis=1)\n\n# See unique prefixes.\nprefix_list = list(train_data['Name'].str.findall('(?<=\\,)(?:.*)\\.'))\nprefix_list = [item for items in prefix_list for item in items]\nprefix_list = set([x.strip(' ') for x in prefix_list])\n\nprefix_list","9468adb3":"# Function to replace big strings with substrings (the prefixes).\ndef substrings_in_string(big_string, substrings):\n    for substring in substrings:\n        if str.find(big_string, substring) != -1:\n            return substring\n    print(big_string)\n    return np.nan\n\n# Replace names with prefixes.\ntrain_data['Name'] = train_data['Name'].map(lambda x: substrings_in_string(x, prefix_list))\n\n# Function to replace niche titles with more common titles.\ndef replace_titles(df):\n    prefix = df['Name']\n    if prefix in ['Don.', 'Major.', 'Capt.', 'Jonkheer.', 'Rev.', 'Col.', 'Sir.']:\n        return 'Mr.'\n    elif prefix in ['the Countess.', 'Mme.', 'Lady.', 'Mrs. Martin (Elizabeth L.', 'Dona.']:\n        return 'Mrs.'\n    elif prefix in ['Mlle.', 'Ms.']:\n        return 'Miss.'\n    elif prefix == 'Dr.':\n        if df['Sex'] == 'Male':\n            return 'Mr.'\n        else:\n            return 'Mrs.'\n    else:\n        return prefix\n\n# Replace niche titles with common titles, change \"Name\" to category and show remaining unique titles.\ntrain_data['Name'] = train_data.apply(replace_titles, axis=1)\ntrain_data = train_data.astype({'Name': 'category'})\ntrain_data['Name'].unique()","adbbedb9":"# Function to create family size column\ndef create_family_size(df):\n    df['FamilySize'] = df['SibSp'] + df['Parch']\n\n# Create \"FamilySize\" in train_data and show bottom 5 rows.\ncreate_family_size(train_data)\ntrain_data.tail()","cabb0b05":"# Import numpy to use numpy arrays and import sklearn for the MinMaxScaler (normalisation).\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Function to create X and y arrays, getting dummies and normalising.\ndef ml_data_prep(df):\n    y = np.array(df['Survived'])\n    df = df.drop(['Survived'], axis=1)\n    df = pd.get_dummies(df)\n    X = df[df.columns[2:]].values\n    X = MinMaxScaler().fit_transform(X)\n    return X, y\n\n# Assign X and y and show the function has worked.\nX, y = ml_data_prep(train_data)\nprint(X[0], y[0])\nprint(\"Number of rows in X: {}\".format(len(X)))\nprint(\"Number of rows in y: {}\".format(len(y)))","4f03c66f":"# Import models, selectors and scoring metric.\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, VotingClassifier, RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","e029311b":"# Dictionary containing models to use, including their potential parameters to be tuned.\nbest_non_ensembles = []\n\nmodels = {'non_ensemble':\n            {'DT': {'model': DecisionTreeClassifier(), \n                    'params':{'criterion':['gini','entropy'], \n                              'max_depth': [2,4,6,8,10,12,14,16,18,20]}},\n             'KNN': {'model': KNN(), \n                     'params':{'weights':['uniform', 'distance'], \n                               'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'], \n                               'leaf_size':[1,3,5,10,20,30,40], \n                               'p':[1,2]}}, \n             'LR': {'model': LogisticRegression(), \n                    'params':{'penalty':['l1', 'l2', 'elasticnet', 'none'], \n                              'C':[1,2,3,4,5,6], \n                              'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}},\n            }\n            , 'ensemble':\n                {'AB': {'model': AdaBoostClassifier(), \n                         'params':{'n_estimators':[50,55,60,65],\n                                   'learning_rate':[1,2,3,4,5],\n                                   'algorithm':['SAMME','SAMME.R']}},\n                'BC': {'model': BaggingClassifier(), \n                       'params':{'n_estimators':[10,15,20,25],\n                                 'max_samples':[1,2,3,4],\n                                 'max_features':[1,2,3,4]}}, \n                'VC': {'model': VotingClassifier(estimators=best_non_ensembles), \n                       'params':{'voting':['hard','soft']}},\n                'RF': {'model': RandomForestClassifier(), \n                       'params':{'n_estimators':[50,100,120,130,150],\n                                 'criterion':['gini','entropy'],\n                                 'min_samples_split':[2,3,4,5],\n                                 'min_samples_leaf':[1,2,3,4],\n                                 'max_features':['auto','sqrt','log2']}}\n                }\n         }","9e735598":"# Split the data into train and test, then run through all non-ensemble models to see how they perform.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nfor key, model in models['non_ensemble'].items():\n    \n    grid = GridSearchCV(estimator=model['model'],\n                        param_grid=model['params'],\n                        cv=5,\n                        verbose=0,\n                        n_jobs=-1)\n    \n    fit_model = grid.fit(X_train, y_train)\n    y_pred = grid.predict(X_test)\n    \n    best_non_ensembles.append((key, grid.best_estimator_))\n    print(\"Tuned Parameters: {}\".format(grid.best_estimator_)) \n    print(\"Best score is {}\".format(grid.best_score_))\n    print(\"Accuracy score on test data: {}\".format(accuracy_score(y_test, y_pred)))\n    print(\" \")","faa32167":"for key, model in models['ensemble'].items():\n    \n    grid = GridSearchCV(estimator=model['model'],\n                        param_grid=model['params'],\n                        cv=5,\n                        verbose=0,\n                        n_jobs=-1)\n    \n    fit_model = grid.fit(X_train, y_train)\n    y_pred = grid.predict(X_test)\n    \n    best_non_ensembles.append((key, grid.best_estimator_))\n    print(\"Tuned Parameters: {}\".format(grid.best_estimator_)) \n    print(\"Best score is {}\".format(grid.best_score_))\n    print(\"Accuracy score on test data: {}\".format(accuracy_score(y_test, y_pred)))\n    print(\" \")","80c9e432":"missing_ages(test_data)\ntest_data['Cabin'] = test_data['Cabin'].str.extract('(\\D+)', expand=False)\nfill_cabins(test_data)\ntest_data = test_data.astype({'Pclass': 'category', 'Sex': 'category', 'Cabin': 'category', 'Embarked': 'category'})\ntest_data = test_data.drop(['Ticket'], axis=1)\nprefix_list = list(test_data['Name'].str.findall('(?<=\\,)(?:.*)\\.'))\nprefix_list = [item for items in prefix_list for item in items]\nprefix_list = set([x.strip(' ') for x in prefix_list])\ntest_data['Name'] = test_data['Name'].map(lambda x: substrings_in_string(x, prefix_list))\ntest_data['Name'] = test_data.apply(replace_titles, axis=1)\ntest_data = test_data.astype({'Name': 'category'})\ncreate_family_size(test_data)\ntest_data.fillna(method='ffill', inplace=True)\ntest_data = pd.get_dummies(test_data)\nX_test = test_data[test_data.columns[1:]].values\nX_test = MinMaxScaler().fit_transform(X_test)\n\nfinal_fit_model = RandomForestClassifier(criterion='entropy', min_samples_leaf=2,\n                       min_samples_split=3).fit(X_train, y_train)\n\ny_pred = final_fit_model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': y_pred})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","2e566544":"<h3>Cabin<\/h3>\n<p>Let's see what's going on with the missing \"Cabin\" data. I suspect the first letter in the \"Cabin\" string is the deck label, which could be a useful feature to have.<\/p>","c805ab78":"<p>The above countplot shows:<\/p>\n<ul>\n    <li>81% of first class passengers have a cabin labeled, from A-E and a mystery T deck (<a href=\"https:\/\/www.encyclopedia-titanica.org\/titanic-deckplans\/boat-deck.html\">blueprints don't show T deck<\/a>)<\/li>\n    <li>9% of second class passengers have a cabin labeled, from D-F deck<\/li>\n    <li>2% of third class passengers have a cabin labeled, from E-G deck<\/li>\n<\/ul>\n<p>Perhaps distance from the top of the ship is a useful feature? The iceburg was hit late at night aferall, when we can assume most passengers were in their cabins. We can now create a function to fill in the missing cabin (now deck) data by placing a random placement for those without an assigned deck. E.g. A first class passenger will be assigned a deck between A and E, randomly.<\/p>","c86e9913":"<p>Random Forest offers a slightly better score; also 81% when rounded up. We will use this for our final model. If you have gotten this far, thank you for reading! Please leave comments and feedback, particularly on where I could have improved to get a better score! \ud83d\ude4f<\/p>","bfb87fd1":"<h2>EDA<\/h2>\n<p>What's in the data?<\/p>","47115515":"<p>\"Age\", \"Cabin\" and \"Embarked\" all have missing values in the training data set. Additionally, the test data set also has 1 missing \"Fare\" as well as missing values in \"Age\" and \"Cabin\". We cannot drop the rows with null values, because in some instances (\"Cabin\") this would result in the loss of 77% of the data! We will deal with these one-by-one and decide what to do.<\/p>\n\n<p>The \"Fare\" column shows some huge variation, skewed by the max value 513.33. As the violin plot above shows, this is due to an extortionate fare paid by a first class passenger. A quick Google search shows that high fares are to be expected with first class passengers; the maximum ticket price being $2,560, which is more than 61,000 today. (<a href=\"https:\/\/money.com\/titanic-most-expensive-ticket\/\">link<\/a>).\n\n<p>Some of the data types will also need to be changed. For example, \"Sex\" and \"Embarked\" are currently objects, when they should really be <a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/categorical.html\">categories<\/a>.<\/p>\n<h3>Age<\/h3>","5b330907":"<p><\/p>","d22f9b2e":"<p>We will use the mean age by class to fill in the missing ages by creating a function we can use later on the test data set.<\/p>","cbde3429":"<h2>Model Selection<\/h2>\n<p>Time to select which machine learning model performs the best at predicting if passengers survived the sinking. We will have a go with various classification models, including <a href=\"https:\/\/en.wikipedia.org\/wiki\/Ensemble_learning\">ensembles<\/a>. We will also use something called <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\">\"GridSearchCV\"<\/a> to help with tuning the model hyperparameters. The X and y data will be split into a train and test set (70:30)<\/p>","fcceebc6":"<h1>Titanic - My First Kaggle Competition<\/h1>\n<p>Hello and welcome to my first attempt at a Kaggle notebook! \ud83d\udc4b<\/p>\n<p>Firstly, some brief information about the Titanic which may prove useful when considering the data:<br> <a href=\"https:\/\/en.wikipedia.org\/wiki\/Titanic\">RMS Titanic<\/a> was a British passenger liner operated by the <a href=\"https:\/\/en.wikipedia.org\/wiki\/White_Star_Line\">White Star Line<\/a>, first launched on 31<sup>st<\/sup> May 1911. On her maiden voyage from Southampton (UK) to New York (USA), the Titanic stuck an iceburg in the north Atlantic Ocean at 23:40 on 14<sup>th<\/sup> April 1912 and sank 2 hours and 40 minutes later. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died. The liner was 46,328tonnes, 269.1m long and had 9 decks (A-G).<\/p>\n\n<p>The objective of this notebook is to create a model to predict if passengers survived the skinking of the Titanic. I think this will be the best way forward:<\/p>\n<ol>\n    <li>Exploratory Data Analysis (EDA)<\/li>\n    <li>Data Cleanse (if required)<\/li>\n    <li>Feature Engineering<\/li>\n    <li>Model Selection<\/li>\n    <li>Final Prediction<\/li>\n<\/ol>\n<p>Let's go!<\/p>","0df49ee1":"<p>That's the names taken care of. All passengers are now a member of 1 of 4 categories: Mr, Mrs, Miss and Master. Next we will create the family size column.<\/p>","a6f09dd2":"<h2>Feature Engineering<\/h2>\n<p>The \"Name\" and \"Ticket\" values don't appear very useful, because they act is identifiers rather than characteristics of the passengers. We will drop the \"Ticket\" column, however, the \"Name\" column may have something useful; the passengers prefix. The prefix may help identify if the passenger is married and how old they are. \"Master\", for example, is a prefix only given to young boys.<\/p>\n<p>We are also able to create new features. A good example would be to create a new column called \"FamilySize\", which combines the \"SibSp\" and \"Parch\" columns, totaling the numbers of siblings, spouses, parents and children.<\/p>\n<p>Lastly, we will need to replace the category columns with <a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.get_dummies.html\">dummy\/indicator variables<\/a>, so they can be used by machine learning algorithms, and numeric columns will need to be <a href=\"https:\/\/www.educative.io\/edpresso\/data-normalization-in-python\">normalised<\/a> to make model training less sensitive to the scale of features.<\/p>","ad7d9dce":"<p>The \"Cabin\" column now has no missing values, great!<\/p>\n<h3>Embarked<\/h3>\n<p>There are only 2 rows missing a value in \"Embarked\", we will drop these rows reducing the table size to 889 \ud83d\ude04 We will also change the data types of the columns to their appropriate formats whilst we're at it.<\/p>","c1d0ef92":"<p>Now we will finish with the features by creating dummy values for categories and normalising the numeric columns. We will end up with an X (feature) array and a y (target) array.<\/p>","90d00f87":"<p>Not bad... looks like logistic regression comes out on top with an 81% accuracy score on the training data. Let's see if the ensembles perform any better.<\/p>"}}