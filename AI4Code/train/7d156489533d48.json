{"cell_type":{"bcf0784e":"code","1934c5d8":"code","5187c8a0":"code","8bc89f48":"code","f88e7c8d":"code","6b41912d":"code","781400e6":"code","ba2fae41":"code","7c48cbc8":"code","5b04756e":"code","65720441":"code","88a47c6f":"markdown","51b446b5":"markdown","ed7f6be0":"markdown","3421fd8d":"markdown","c9329bee":"markdown","5453cef2":"markdown","10b93744":"markdown","018831ce":"markdown","9b42b24c":"markdown","875ec80f":"markdown","0c63ee2a":"markdown"},"source":{"bcf0784e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1934c5d8":"import numpy as np\n\nimport torch\nimport torch.nn as nn\ntorch.manual_seed(0)\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom scipy import stats\nimport pandas as pd\n\nimport wandb\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wand_key\")\n\nwandb.login(key=secret_value_0)\nwandb.init(project='titanic_kaggle_logistic_regression', save_code=True)","5187c8a0":"titanic_training_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntitanic_test_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntitanic_training_data.head()","8bc89f48":"def clean_titanic(df, train=True):\n    df[\"Cabin\"] = df[\"Cabin\"].apply(lambda x: pd.isna(x)).astype(bool)\n    df[\"Embarked\"] = df[\"Embarked\"].apply(lambda x: pd.isna(x)).astype(bool)\n    df[\"AgeNan\"] = df[\"Age\"].apply(lambda x: pd.isna(x)).astype(bool)\n    df = pd.concat([df, pd.get_dummies(df['Sex'], dtype='bool', prefix='sex_'), pd.get_dummies(df['Pclass'], dtype='bool', prefix='pclass_')], axis=1)\n    df = df.drop(['PassengerId', 'Name','Ticket','Sex','Pclass'], axis=1)\n    if train:\n        df = df.drop(['Survived'], axis=1)\n    numeric_features = df.dtypes[(df.dtypes != 'object') & (df.dtypes != 'bool')].index\n    df[numeric_features] = df[numeric_features].apply(lambda x: (x - x.mean()) \/ (x.std()))\n    df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].mean())\n    df[\"Fare\"] = df[\"Fare\"].fillna(df[\"Fare\"].mean())\n    return df\n\nlabels = torch.tensor(titanic_training_data[\"Survived\"].values, dtype=torch.float32)\ntitanic_training_data = clean_titanic(titanic_training_data)\ntitanic_training_data.head()","f88e7c8d":"titanic_data_tensor = torch.tensor(titanic_training_data.astype('float').values, dtype=torch.float32)\ntitanic_data_tensor.shape","6b41912d":"dataset = torch.utils.data.TensorDataset(titanic_data_tensor, labels)\ndataset","781400e6":"training_size = int(0.7 * len(dataset))\nvalidation_size = len(dataset) - training_size\ntrain, val = torch.utils.data.random_split(dataset, [training_size, validation_size], generator=torch.Generator().manual_seed(0))\ndata_loader_train = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True)\ndata_loader_val = torch.utils.data.DataLoader(val, batch_size=32, shuffle=True)\n\n","ba2fae41":"#TODO Xavier Uniform to the weight and set the bias to 0\ndef init_my_layer(m):\n    torch.nn.init.xavier_uniform_(m.weight)\n    torch.nn.init.constant_(m.bias,0)\n    return m        ","7c48cbc8":"class LinearModel(nn.Module):\n    #TODO One linear layer and Sigmoid applied to the ouput\n    #net = nn.Sequential(nn.Linear(4, 1))\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.linear = init_my_layer(nn.Linear(input_dim, output_dim))\n        self.activation = torch.nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.linear(x)\n        return self.activation(x)","5b04756e":"import torch.optim as optim\n\nnum_epochs = 400 # should be more than enought, but can be changed\nlr = 3e-3 # e.q to 0.003, you can change it if needed\ninput_dim = titanic_data_tensor.shape[1]\noutput_dim = 1\n\nnet = LinearModel(input_dim,output_dim)#TODO\n#net.apply(_weights_init)\n#net[0].weight.data\n\ncriterion = nn.MSELoss()#TODO\n\noptimizer = optim.Adam(net.parameters(), lr=lr)#TODO\n\nfor epoch in range(num_epochs):\n    training_loss = 0\n    #TODO TRAINING LOOP\n    for i, y in data_loader_train:\n        optimizer.zero_grad() # please don't forget!\n        output = net(i) # Forward pass\n        loss = criterion(output, y)\n        training_loss += loss \n        loss.backward() # remember: You need to tell wrt to what the gradient is computed\n        optimizer.step() # do a step in the gradient direction\n    with torch.no_grad():\n        validation_loss = 0#TODO VALIDATION LOOP\n        for z, j in data_loader_val:\n            loss2 = criterion(net(z),j)\n            validation_loss += loss2        \n    print(f'epoch {epoch + 1}, training_loss {training_loss.item()}, validation_loss {validation_loss.item()}')\n    wandb.log({'training_loss': training_loss, 'validation_loss': validation_loss})\n\n    # SAVE THE MODEL\n    torch.save(net.state_dict(), 'my_model.pt')","65720441":"titanic_test_data_cleaned = clean_titanic(titanic_test_data, train=False)\ntitanic_data_tensor = torch.tensor(titanic_test_data_cleaned.astype('float').values, dtype=torch.float32)\n\nwith torch.no_grad():\n    net.eval()\n    test_pred = torch.LongTensor()\n    for i, data in enumerate(titanic_data_tensor):\n        output = net(data)\n        predicted = torch.ge(output, 0.5)\n        test_pred = torch.cat((test_pred, predicted), dim=0)\n    out_df = pd.DataFrame(np.c_[titanic_test_data['PassengerId'].values, test_pred.numpy()], columns=['PassengerId', 'Survived'])\n    out_df.to_csv('submission.csv', index=False)","88a47c6f":"Import all the needed library and init Weights and Biases","51b446b5":"# Titanic - Machine Learning from Disaster\n\n\nKaggle link: https:\/\/www.kaggle.com\/c\/titanic","ed7f6be0":"This loop computes the prediction on the test dataset and creates a submission file\n\nYou then just have to click the submit button to get your score. Lucky you!","3421fd8d":"We then split between the training and validation set","c9329bee":"Layer initialization using Xavier Uniform on the weights and a constant 0 value on the bias","5453cef2":"Initialize the network (call it `net`, it would make things easier later), the loss, the optimizer, and write the training loop\n\nDon't forget to check the validation loss and save your model at the end of each epoch!","10b93744":"Dataframe needs to be cleaned. Knowing if some information are unknown can be very important to determine if someone survived","018831ce":"We then transform the data from numpy (pandas representation) into torch's `Tensor`","9b42b24c":"Create a `TensorDataset` to get tuple of data and label","875ec80f":"Create the LinearModel with one Linear layer and Sigmoid applied to the output","0c63ee2a":"We first need to read the datasets"}}