{"cell_type":{"6c65dfb6":"code","52c28a84":"code","c6849046":"code","9adb789a":"code","ef624fee":"code","2ff96424":"code","e9bb7608":"code","9e18482b":"code","a8141216":"code","ac946283":"code","bb24b8ca":"code","c7b256ae":"code","452204fc":"code","fe1de1e9":"code","68c086eb":"code","d7a480df":"code","569e1cdd":"code","d983eb81":"code","1622e331":"code","2249eab4":"code","305c2d75":"code","573fbde5":"code","7af5d951":"code","7b2df385":"code","8ed0f3c1":"code","d904ce07":"code","edf5a27d":"code","9672aa28":"code","88c07e02":"code","a49526b6":"code","4c89cde0":"code","f74a846c":"code","1fa163bd":"code","98a91be1":"code","0408d77d":"code","ef154a66":"code","8a753214":"code","e131af85":"code","ed8eba2f":"code","66921029":"code","23f0a187":"code","da0e2a2f":"markdown","fc8893e4":"markdown","b8808582":"markdown","bd717edd":"markdown","2491d8be":"markdown","dfab66d3":"markdown","cde8e39d":"markdown","b8d46688":"markdown","9d94e3bf":"markdown","da59ce1c":"markdown","8ef84da7":"markdown","80dfcd5a":"markdown","5ac5c1ef":"markdown"},"source":{"6c65dfb6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","52c28a84":"df1=pd.read_csv('\/kaggle\/input\/ml-club-nits\/train.csv')\ndf2=pd.read_csv('\/kaggle\/input\/ml-club-nits\/test.csv')","c6849046":"def clean_missing(df1,s1='mean',s2='most_frequent'):\n    import pandas as pd\n    \n    from sklearn.pipeline import Pipeline\n    from sklearn.impute import SimpleImputer\n    from sklearn.compose import ColumnTransformer\n    df1_missing_values = df1.isnull().sum()\n    df1_numeric_columns = df1.select_dtypes(include=[\"int64\",\"float64\"]).keys()\n    columns_numeric_missing = [var for var in df1_numeric_columns if df1_missing_values[var]>0]\n    \n    df1_categorical_columns = df1.select_dtypes(include=[\"object\"]).keys()\n    columns_categorical_missing = [var for var in df1_categorical_columns if df1_missing_values[var]>0]\n    \n    numeric_value_mean_imputer = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=s1))])\n    \n    if s2=='constant':\n        x=input(\"what do you want to fill in place of missing values\")\n        categorical_value_mode_imputer = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=s2,fill_value=x))])\n       # s2=strategy='constant',fill_value= x\n    else:     \n        categorical_value_mode_imputer = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=s2))])\n    \n    \n    preprocessing = ColumnTransformer(transformers=[(\"mean_imputer\", numeric_value_mean_imputer, columns_numeric_missing),\n                                                (\"mode_imputer\", categorical_value_mode_imputer, columns_categorical_missing)])\n    \n    #scale data\n    df1_clean_null_value = preprocessing.fit_transform(df1)\n\n    df1_missing_value_solve = pd.DataFrame(df1_clean_null_value, columns=columns_numeric_missing+columns_categorical_missing)\n\n    df1.update(df1_missing_value_solve)\n    return df1","9adb789a":"clean_missing(df1)\ndf1","ef624fee":"df1.head()","2ff96424":"df1.info()","e9bb7608":"df1.isnull()","9e18482b":"df1.isnull().sum()","a8141216":"y=df1['Dataset']\n\ndf1=df1.drop(['ID','Dataset'],axis=1)\n","ac946283":"cc=['Gender']\ndummy=pd.get_dummies(df1[cc])\ndummy.head()","bb24b8ca":"\ndf1=df1.drop(cc,axis=1)\n\nX=pd.concat([df1,dummy],axis=1)\n\n\nX.head()\n","c7b256ae":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,stratify=y,test_size=0.25)","452204fc":"from sklearn import tree\ndtc = tree.DecisionTreeClassifier(random_state=0)\ndtc.fit(X_train, y_train)","fe1de1e9":"dtc.score(X_test,y_test)","68c086eb":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,  stratify=y,  test_size=0.25)\ndtc = tree.DecisionTreeClassifier(random_state=0)\ndtc.fit(X_train, y_train)\ndtc.score(X_test,y_test)","d7a480df":"from sklearn.ensemble import RandomForestClassifier\nclf=RandomForestClassifier(n_estimators=30)\nclf.fit(X_train,y_train)\nclf.score(X_test,y_test)","569e1cdd":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,  stratify=y,  test_size=0.25)\nclf=RandomForestClassifier(n_estimators=30)\nclf.fit(X_train,y_train)\nclf.score(X_test,y_test)","d983eb81":"from sklearn.naive_bayes import GaussianNB\nnb=GaussianNB()\nnb.fit(X_train,y_train)\nnb.score(X_test,y_test)","1622e331":"from sklearn import svm\nsv=svm.SVC(kernel='linear')\nsv.fit(X_train,y_train)\nsv.score(X_test,y_test)","2249eab4":"from sklearn.neighbors import KNeighborsClassifier\nX_train, X_test, y_train, y_test = train_test_split(X, y,  stratify=y,  test_size=0.25)\nKNC=KNeighborsClassifier(n_neighbors=2)\nKNC.fit(X_train,y_train)\nKNC.score(X_test,y_test)","305c2d75":"from sklearn.datasets import make_classification\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier","573fbde5":"from sklearn.model_selection  import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.40,stratify=y,random_state = 6)","7af5d951":"X,y = make_classification(n_samples=200,n_features=12,n_informative=8,n_redundant=3,random_state=1)","7b2df385":"model_1 = tree.DecisionTreeClassifier(random_state=0)\nmodel_2 = RandomForestClassifier(n_estimators=30)\nmodel_3 = GaussianNB()\nmodel_4 = svm.SVC(kernel='linear')\nmodel_5 = KNeighborsClassifier(n_neighbors=2)","8ed0f3c1":"model_1.fit(X_train, y_train)\nmodel_2.fit(X_train, y_train)\nmodel_3.fit(X_train, y_train)\nmodel_4.fit(X_train, y_train)\nmodel_5.fit(X_train, y_train)","d904ce07":"estimators = [ ('m1', model_1),\n   ('m2', model_2),('m3',model_3 ),('m4',model_4)]","edf5a27d":"sc = StackingClassifier(   estimators=estimators, final_estimator=model_2 )","9672aa28":"sc.fit(X_train,y_train)\n\nsc.score(X_test,y_test)","88c07e02":"from sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import VotingClassifier","a49526b6":"X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, test_size=0.20)\nmodel = GradientBoostingRegressor()\nmodel.fit(X_train, y_train)","4c89cde0":"pred_final = model.predict(X_test)","f74a846c":"model.score(X_train, y_train)","1fa163bd":"level0=list()\nlevel0.append(('dtc',tree.DecisionTreeClassifier(random_state=0)))\nlevel0.append(('clf',RandomForestClassifier(n_estimators=30))) \nlevel0.append(('nb',GaussianNB()))   \nlevel0.append(('svm',svm.SVC(kernel='linear')))  \nlevel0.append(('KNC',KNeighborsClassifier(n_neighbors=2)))  ","98a91be1":"level1=GaussianNB()\nmodel=StackingClassifier(estimators=level0,final_estimator=level1,cv=5)\nmodel.fit(X,y)","0408d77d":"model_1 = tree.DecisionTreeClassifier(random_state=0)\nmodel_2 = RandomForestClassifier(n_estimators=30)\nmodel_3 = GaussianNB()\nmodel_4 = svm.SVC(kernel='linear')\nmodel_5 = KNeighborsClassifier(n_neighbors=2)","ef154a66":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import VotingClassifier\n","8a753214":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y,stratify=y, test_size=0.20)\n  \n\nboost = GradientBoostingRegressor()\nboost.fit(X_train, y_train)","e131af85":"pred_final = boost.predict(X_test)\nboost.score(X_train, y_train)","ed8eba2f":"VC = VotingClassifier(estimators=[ ('m1', model_1),\n   ('m2', model_2),('m3',model_3 ),('m4',model_4),('m5',model_5)], voting='hard')","66921029":"VC.fit(X_train, y_train)","23f0a187":"pred_final = VC.predict(X_test)\n\nVC.score(X_test,y_test)","da0e2a2f":"# BOOSTING","fc8893e4":"# NAIVE BAYES CLASSIFIER","b8808582":"# # STACKING CLASSIFIER","bd717edd":"# # DECISION TREE CLASSIFIER","2491d8be":"# DATA CLEANING FUNCTION","dfab66d3":"# SUPPORT VECTOR MACHINE","cde8e39d":"# K NEAREST NEIGBOR","b8d46688":"# # IMPORTING PACKAGES","9d94e3bf":"# GRADIENT BOOSTING","da59ce1c":"# DIVIDING INTO TEST AND TRAIN DATA","8ef84da7":"# VOTING","80dfcd5a":"# RANDOM FOREST","5ac5c1ef":"# IMPORTING PACKAGES"}}