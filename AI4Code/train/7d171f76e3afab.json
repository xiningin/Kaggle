{"cell_type":{"0cc044ac":"code","41756ee2":"code","ecb85d1f":"code","cfec64af":"code","d9682202":"code","57891ba7":"code","81c65c4a":"code","73e03415":"code","a1e2970d":"code","f5fa753a":"code","23b7a690":"code","990bbf8e":"code","de15bf5c":"code","b1e3f09e":"code","ccb18486":"code","214c671e":"code","f3238ee9":"code","1854fc6e":"code","62c6244e":"code","45a89c29":"code","c9bfb3ac":"code","cb5175ac":"code","750a5a37":"code","111fec5b":"code","078848a4":"code","53dbc2dd":"code","7988472b":"code","357419fc":"code","0afe101b":"code","199cda82":"code","6691a1a5":"code","a17421ad":"code","6fd80d2b":"code","8b9900ca":"code","895cd476":"code","4dd6790d":"code","0b3686bb":"code","d2fdeec6":"code","34569674":"code","1bdc4683":"code","0eecae81":"code","b4959697":"code","3e4fbb8a":"code","d5959bde":"code","b3dc55d7":"code","0597acda":"code","fb8c3a46":"code","2c860701":"markdown","fd11610f":"markdown","d22c1b6a":"markdown","6556d0df":"markdown"},"source":{"0cc044ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","41756ee2":"# Load the tarin data and test data\ntrain_df = pd.read_csv(r'\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv(r'\/kaggle\/input\/titanic\/test.csv')","ecb85d1f":"# Do a quick check on the data\ntrain_df.head()","cfec64af":"# Check if the data is skews or not\n\ntrain_df.Survived.value_counts()","d9682202":"train_df['Survived'].plot.hist()","57891ba7":"train_df['Age'].plot.hist()","81c65c4a":"# Check the data types of tarin data\ntrain_df.dtypes","73e03415":"# Get the columns\ntrain_df.columns","a1e2970d":"features =['PassengerId','Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\ntargets = ['Survived']","f5fa753a":"# Get names of columns with missing values\ncols_with_missing = [col for col in train_df.columns\n                     if train_df[col].isnull().any()]\n\ncols_with_missing","23b7a690":"categorical_columns = ['Pclass', 'Sex','Cabin', 'Embarked']\nnon_categorical_columns = ['Fare','Age', 'SibSp','Parch']","990bbf8e":"train_df_categorical_cols_imputed = train_df[categorical_columns].apply(lambda x: x.fillna(x.value_counts().index[0]))\ntest_df_categorical_cols_imputed = test_df[categorical_columns].apply(lambda x: x.fillna(x.value_counts().index[0]))\n\n# df_most_common_imputed","de15bf5c":"train_df_non_categorical_cols_imputes =train_df[non_categorical_columns].fillna(train_df[non_categorical_columns].mean())\ntest_df_non_categorical_cols_imputes =test_df[non_categorical_columns].fillna(test_df[non_categorical_columns].mean())","b1e3f09e":"X_train = pd.concat([train_df_categorical_cols_imputed, train_df_non_categorical_cols_imputes], axis=1)\nX_test = pd.concat([test_df_categorical_cols_imputed, test_df_non_categorical_cols_imputes], axis=1)","ccb18486":"cols_with_missing_train = [col for col in X_train.columns\n                     if X_train[col].isnull().any()]\n\ncols_with_missing_train ","214c671e":"cols_with_missing_test = [col for col in X_test.columns\n                     if X_test[col].isnull().any()]\n\ncols_with_missing_test","f3238ee9":"from sklearn.preprocessing import OneHotEncoder\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[categorical_columns]))\nOH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[categorical_columns]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_test.index = X_test.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(categorical_columns, axis=1)\nnum_X_test = X_test.drop(categorical_columns, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)","1854fc6e":"# num_val_samples = int(len(features) * 0.2)\ntrain_features = OH_X_train\ntrain_targets = train_df.Survived\ntest_features = OH_X_test\n# val_targets = targets[-num_val_samples:]\n\nprint(\"Number of training samples:\", len(train_features))\nprint(\"Number of validation samples:\", len(test_features))","62c6244e":"\ncounts =train_df.Survived.value_counts()\nprint(\n    \"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n        counts[1], 100 * float(counts[1]) \/ len(train_targets)\n    )\n)\n\nweight_for_0 = 1.0 \/ counts[0]\nweight_for_1 = 1.0 \/ counts[1]","45a89c29":"def display_scores(scores):\n    print(\"Scores: \",scores)\n    print(\"Mean:\",scores.mean())\n    print(\"Standard deviation:\",scores.std())","c9bfb3ac":"mean = np.mean(train_features, axis=0)\ntrain_features -= mean\ntest_features -= mean\nstd = np.std(train_features, axis=0)\ntrain_features \/= std\ntest_features \/= std","cb5175ac":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import balanced_accuracy_score,make_scorer\nbalance_accuracy = make_scorer(balanced_accuracy_score)","750a5a37":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n'''\nparam_grid = [{'class_weight':['balanced'],\n'kernel' : ['sigmoid','poly'], # 'linear', 'poly',\n'degree' : [4,5,6],\n'gamma' : ['auto'],\n'random_state':[1,2],\n}]\n\nsvc_clf= SVC()\n\ngrid_search = GridSearchCV(svc_clf,param_grid,cv=5,\n                          scoring=balance_accuracy,\n                          return_train_score=True)\n\ngrid_search.fit(train_features,train_targets)'''","111fec5b":"# grid_search.best_params_","078848a4":"# grid_search.best_estimator_","53dbc2dd":"svc_clf = SVC(class_weight='balanced', degree=4, gamma='auto', kernel='sigmoid',\n    random_state=1)\nsvc_clf.fit(train_features, train_targets)\nscores = cross_val_score(svc_clf, train_features, train_targets, scoring=balance_accuracy,cv=10)","7988472b":"display_scores(scores)","357419fc":"# from sklearn.model_selection import GridSearchCV\n\n# param_grid = [{'n_estimators':[3,10,30,100, 200],'max_features':[2,4,6,8],'max_leaf_nodes':[5,10,15,20,25]},\n#              {'bootstrap':[False],'n_estimators':[3,10],'max_features':[2,3,4]}]\n\n# forest_model = RandomForestClassifier(random_state=1)\n\n# grid_search = GridSearchCV(forest_model,param_grid,cv=5,\n#                           scoring=balance_accuracy,\n#                           return_train_score=True)\n\n# grid_search.fit(train_features,train_targets)","0afe101b":"# grid_search.best_params_","199cda82":"# grid_search.best_estimator_","6691a1a5":"# from sklearn.model_selection import GridSearchCV\n\n# param_grid = [{'n_estimators':[3,10,30,100, 200],'max_features':[2,4,6,8],'max_leaf_nodes':[100,200,300]},\n#              {'bootstrap':[False],'n_estimators':[3,10],'max_features':[2,3,4]}]\n\n# forest_model = RandomForestClassifier(random_state=1)\n\n# grid_search = GridSearchCV(forest_model,param_grid,cv=5,\n#                           scoring=balance_accuracy,\n#                           return_train_score=True)\n\n# grid_search.fit(train_features,train_targets)","a17421ad":"# grid_search.best_params_","6fd80d2b":"# grid_search.best_estimator_","8b9900ca":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_absolute_error\n\nforest_model = RandomForestClassifier(max_features=6, max_leaf_nodes=60, n_estimators=30,\n                       random_state=1)\nforest_model.fit(train_features, train_targets)\nrandom_forest_pred = forest_model.predict(test_features)","895cd476":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import balanced_accuracy_score,make_scorer\nbalance_accuracy = make_scorer(balanced_accuracy_score)\n\nscores = cross_val_score(forest_model, train_features, train_targets, scoring=balance_accuracy,cv=10)","4dd6790d":"display_scores(scores)","0b3686bb":"# random_forest_pred\npredictions = pd.DataFrame(random_forest_pred,columns=['Survived'])","d2fdeec6":"# predictions.head()\nfinal_submission = pd.concat([test_df['PassengerId'],predictions['Survived']],axis=1)\nfinal_submission.to_csv('gender_submission_forest_new3.csv',index=False)","34569674":"# from sklearn.model_selection import GridSearchCV\n# from xgboost import XGBClassifier\n\n# param_grid = [{'learning_rate':[0.5,0.2,0.3,0.2,0.1,0.01],'num_boost_round': [100,200,300,400,500,100,2000],'n_jobs':[1,2,3,4],\n# 'min_child_weight':[10,12,13,14,15],'colsample_bytree':[0.2,0.5,0.8,1],'max_depth':[10,12,16,18],'eta':[0.3,0.5]}]\n\n# # 'learning_rate':[0.5,0.2,0.3,0.2,0.1,0.01],'num_boost_round': [100,200,300,400,500,100,2000],'n_jobs':[1,2,3,4],\n# # 'min_child_weight':[10,12,13,14,15],colsample_bytree:[0.2,0.5,0.8,1],max_depth:[10,12,16,18],eta:0.3\n            \n\n# XGBoost_model = XGBClassifier(random_state=1)\n\n# grid_search = GridSearchCV(XGBoost_model,param_grid,cv=5,\n#                           scoring=balance_accuracy,\n#                           return_train_score=True)\n\n# grid_search.fit(train_features,train_targets)","1bdc4683":"import xgboost as xgb\n\nxgb_clf = xgb.XGBClassifier(colsample_bynode=1, colsample_bytree=0.8, eta=0.5, \n              gpu_id=-1, importance_type='gain', interaction_constraints='',\n              learning_rate=0.05, max_delta_step=0, max_depth=16,\n              min_child_weight=10, n_jobs=1,n_estimators=500,\n              num_parallel_tree=1, objective='binary:logistic', random_state=1,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\n\nxgb_clf = xgb_clf.fit(train_features,train_targets)\nprediction_xgb=xgb_clf.predict(test_features)\n#now we pass the testing data to the trained algorithm\npredictions = pd.DataFrame(prediction_xgb,columns=['Survived'])\n\n","0eecae81":"# predictions.head()\nfinal_submission = pd.concat([test_df['PassengerId'],predictions['Survived']],axis=1)\nfinal_submission.to_csv('gender_submission_new_xgb.csv',index=False)","b4959697":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import balanced_accuracy_score,make_scorer\nbalance_accuracy = make_scorer(balanced_accuracy_score)\n\nscores = cross_val_score(xgb_clf, train_features, train_targets, scoring=balance_accuracy,cv=10)","3e4fbb8a":"display_scores(scores)","d5959bde":"from sklearn.ensemble import VotingClassifier\n\nvoting_clf = VotingClassifier(\n    estimators = [('xg',xgb_clf),('rf',forest_model),('svc',svc_clf)],\n    voting='hard'\n)\nvoting_clf.fit(train_features,train_targets)","b3dc55d7":"scores = cross_val_score(voting_clf, train_features, train_targets, scoring=balance_accuracy,cv=10)","0597acda":"display_scores(scores)","fb8c3a46":"prediction_voting=voting_clf.predict(test_features)\nprediction_voting = pd.DataFrame(prediction_voting,columns=['Survived'])\nfinal_submission = pd.concat([test_df['PassengerId'],predictions['Survived']],axis=1)\nfinal_submission.to_csv('gender_submission_voting.csv',index=False)","2c860701":"# SVM Classifier","fd11610f":"# Handle missing values\n","d22c1b6a":"# One hot encode categorical variables\n[source](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables)","6556d0df":"# Xgboost"}}