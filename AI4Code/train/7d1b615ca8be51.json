{"cell_type":{"abaff233":"code","3b98da1e":"code","e8c38551":"code","70ac58e4":"code","42b5657f":"code","29605462":"code","45359713":"code","9a4e2409":"code","f0bae4d2":"code","ec8250bc":"code","828c13be":"code","b7502966":"code","1d496011":"code","15814155":"code","0098e64f":"code","a80f3aa2":"code","0932b3f3":"code","11729175":"code","794ba9ac":"code","03077be8":"code","d4685f3c":"code","f58a621f":"code","da43bb0d":"code","ac8787e2":"code","a8850116":"code","59f666d0":"code","a1e7c482":"code","f34def9d":"code","cbd3cd3b":"code","13246610":"code","de778c3c":"code","bf35ce7c":"markdown","07603581":"markdown","f80132b6":"markdown","eaa1ccc0":"markdown","de3c5d1d":"markdown","294c2c4d":"markdown","3c4486fd":"markdown","58a01ad1":"markdown","c7c8b9ac":"markdown","84681749":"markdown","214e272a":"markdown","7f01021b":"markdown","12845581":"markdown","6ccab181":"markdown","e7ec2f3d":"markdown","7eb2a012":"markdown","4dd02d24":"markdown","59ee3171":"markdown","c0d70e26":"markdown","b9e4ca00":"markdown","81879bae":"markdown","fc322471":"markdown","d59a296c":"markdown","240da952":"markdown","d273b079":"markdown","388af080":"markdown","ad3408be":"markdown","962e9287":"markdown","a1a32fb5":"markdown","4e89735a":"markdown","01eca6aa":"markdown","3f80ee8a":"markdown","beaa0478":"markdown","1aa3382c":"markdown","52a15ec5":"markdown","3c0d80d7":"markdown","c141e247":"markdown"},"source":{"abaff233":"# Import libraries and set desired options\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import csr_matrix, hstack\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set()","3b98da1e":"# Read the training and test data sets, change paths if needed\ntrain_df = pd.read_csv('..\/input\/websites_train_sessions.csv',\n                       index_col='session_id')\ntest_df = pd.read_csv('..\/input\/websites_test_sessions.csv',\n                      index_col='session_id')\n\n# Convert time1, ..., time10 columns to datetime type\ntimes = ['time%s' % i for i in range(1, 11)]\ntrain_df[times] = train_df[times].apply(pd.to_datetime)\ntest_df[times] = test_df[times].apply(pd.to_datetime)\n\n# Sort the data by time\ntrain_df = train_df.sort_values(by='time1')\n\n# Look at the first rows of the training set\ntrain_df.head()","e8c38551":"# Change site1, ..., site10 columns type to integer and fill NA-values with zeros\nsites = ['site%s' % i for i in range(1, 11)]\ntrain_df[sites] = train_df[sites].fillna(0).astype(np.uint16)\ntest_df[sites] = test_df[sites].fillna(0).astype(np.uint16)\n\n# Load websites dictionary\nwith open(r\"..\/input\/site_dic.pkl\", \"rb\") as input_file:\n    site_dict = pickle.load(input_file)\n\n# Create dataframe for the dictionary\nsites_dict = pd.DataFrame(list(site_dict.keys()), index=list(site_dict.values()), columns=['site'])\nprint(u'Websites total:', sites_dict.shape[0])\nsites_dict.head()","70ac58e4":"# Your code is here","42b5657f":"# Top websites in the training data set\ntop_sites = pd.Series(train_df[sites].values.flatten()\n                     ).value_counts().sort_values(ascending=False).head(5)\nprint(top_sites)\nsites_dict.loc[top_sites.drop(0).index]","29605462":"# Your code is here","45359713":"# Create a separate dataframe where we will work with timestamps\ntime_df = pd.DataFrame(index=train_df.index)\ntime_df['target'] = train_df['target']\n\n# Find sessions' starting and ending\ntime_df['min'] = train_df[times].min(axis=1)\ntime_df['max'] = train_df[times].max(axis=1)\n\n# Calculate sessions' duration in seconds\ntime_df['seconds'] = (time_df['max'] - time_df['min']) \/ np.timedelta64(1, 's')\n\ntime_df.head()","9a4e2409":"# Your code is here","f0bae4d2":"# Our target variable\ny_train = train_df['target']\n\n# United dataframe of the initial data \nfull_df = pd.concat([train_df.drop('target', axis=1), test_df])\n\n# Index to split the training and test data sets\nidx_split = train_df.shape[0]","ec8250bc":"# Dataframe with indices of visited websites in session\nfull_sites = full_df[sites]\nfull_sites.head()","828c13be":"# sequence of indices\nsites_flatten = full_sites.values.flatten()\n\n# and the matrix we are looking for \n# (make sure you understand which of the `csr_matrix` constructors is used here)\n# a further toy example will help you with it\nfull_sites_sparse = csr_matrix(([1] * sites_flatten.shape[0],\n                                sites_flatten,\n                                range(0, sites_flatten.shape[0]  + 10, 10)))[:, 1:]","b7502966":"full_sites_sparse.shape","1d496011":"# How much memory does a sparse matrix occupy?\nprint('{0} elements * {1} bytes = {2} bytes'.format(full_sites_sparse.count_nonzero(), 8, \n                                                    full_sites_sparse.count_nonzero() * 8))\n# Or just like this:\nprint('sparse_matrix_size = {0} bytes'.format(full_sites_sparse.data.nbytes))","15814155":"# data, create the list of ones, length of which equal to the number of elements in the initial dataframe (9)\n# By summing the number of ones in the cell, we get the frequency,\n# number of visits to a particular site per session\ndata = [1] * 9\n\n# To do this, you need to correctly distribute the ones in cells\n# Indices - website ids, i.e. columns of a new matrix. We will sum ones up grouping them by sessions (ids)\nindices = [1, 0, 0, 1, 3, 1, 2, 3, 4]\n\n# Indices for the division into rows (sessions)\n# For example, line 0 is the elements between the indices [0; 3) - the rightmost value is not included\n# Line 1 is the elements between the indices [3; 6)\n# Line 2 is the elements between the indices [6; 9) \nindptr = [0, 3, 6, 9]\n\n# Aggregate these three variables into a tuple and compose a matrix\n# To display this matrix on the screen transform it into the usual \"dense\" matrix\ncsr_matrix((data, indices, indptr)).todense()","0098e64f":"# Your code is here","a80f3aa2":"def get_auc_lr_valid(X, y, C=1.0, seed=17, ratio = 0.9):\n    # Split the data into the training and validation sets\n    idx = int(round(X.shape[0] * ratio))\n    # Classifier training\n    lr = LogisticRegression(C=C, random_state=seed, solver='liblinear').fit(X[:idx, :], y[:idx])\n    # Prediction for validation set\n    y_pred = lr.predict_proba(X[idx:, :])[:, 1]\n    # Calculate the quality\n    score = roc_auc_score(y[idx:], y_pred)\n    \n    return score","0932b3f3":"%%time\n# Select the training set from the united dataframe (where we have the answers)\nX_train = full_sites_sparse[:idx_split, :]\n\n# Calculate metric on the validation set\nprint(get_auc_lr_valid(X_train, y_train))","11729175":"# Function for writing predictions to a file\ndef write_to_submission_file(predicted_labels, out_file,\n                             target='target', index_label=\"session_id\"):\n    predicted_df = pd.DataFrame(predicted_labels,\n                                index = np.arange(1, predicted_labels.shape[0] + 1),\n                                columns=[target])\n    predicted_df.to_csv(out_file, index_label=index_label)","794ba9ac":"# Train the model on the whole training data set\n# Use random_state=17 for repeatability\n# Parameter C=1 by default, but here we set it explicitly\nlr = LogisticRegression(C=1.0, random_state=17, solver='liblinear').fit(X_train, y_train)\n\n# Make a prediction for test data set\nX_test = full_sites_sparse[idx_split:,:]\ny_test = lr.predict_proba(X_test)[:, 1]\n\n# Write it to the file which could be submitted\nwrite_to_submission_file(y_test, 'baseline_1.csv')","03077be8":"# Your code is here","d4685f3c":"# Dataframe for new features\nfull_new_feat = pd.DataFrame(index=full_df.index)\n\n# Add start_month feature\nfull_new_feat['start_month'] = full_df['time1'].apply(lambda ts: \n                                                      100 * ts.year + ts.month).astype('float64')","f58a621f":"# Your code is here","da43bb0d":"# Add the new feature to the sparse matrix\ntmp = full_new_feat[['start_month']].values\nX_train = csr_matrix(hstack([full_sites_sparse[:idx_split,:], tmp[:idx_split,:]]))\n\n# Compute the metric on the validation set\nprint(get_auc_lr_valid(X_train, y_train))","ac8787e2":"# Add the new standardized feature to the sparse matrix\ntmp = StandardScaler().fit_transform(full_new_feat[['start_month']])\nX_train = csr_matrix(hstack([full_sites_sparse[:idx_split,:], tmp[:idx_split,:]]))\n\n# Compute metric on the validation set\nprint(get_auc_lr_valid(X_train, y_train))","a8850116":"# Your code is here","59f666d0":"# Your code is here","a1e7c482":"# Compose the training set\ntmp_scaled = StandardScaler().fit_transform(full_new_feat[['start_month','start_hour','morning']])\nX_train = csr_matrix(hstack([full_sites_sparse[:idx_split,:], \n                             tmp_scaled[:idx_split,:]]))\n\n# Capture the quality with default parameters\nscore_C_1 = get_auc_lr_valid(X_train, y_train)\nprint(score_C_1)","f34def9d":"from tqdm import tqdm\n\n# List of possible C-values\nCs = np.logspace(-3, 1, 10)\nscores = []\nfor C in tqdm(Cs):\n    scores.append(get_auc_lr_valid(X_train, y_train, C=C))","cbd3cd3b":"plt.plot(Cs, scores, 'ro-')\nplt.xscale('log')\nplt.xlabel('C')\nplt.ylabel('AUC-ROC')\nplt.title('Regularization Parameter Tuning')\n# horizontal line -- model quality with default C value\nplt.axhline(y=score_C_1, linewidth=.5, color='b', linestyle='dashed') \nplt.show()","13246610":"# Your code is here","de778c3c":"# Prepare the training and test data\ntmp_scaled = StandardScaler().fit_transform(full_new_feat[['start_month', 'start_hour', \n                                                           'morning']])\nX_train = csr_matrix(hstack([full_sites_sparse[:idx_split,:], \n                             tmp_scaled[:idx_split,:]]))\nX_test = csr_matrix(hstack([full_sites_sparse[idx_split:,:], \n                            tmp_scaled[idx_split:,:]]))\n\n# Train the model on the whole training data set using optimal regularization parameter\nlr = LogisticRegression(C=C, random_state=17, solver='liblinear').fit(X_train, y_train)\n\n# Make a prediction for the test set\ny_test = lr.predict_proba(X_test)[:, 1]\n\n# Write it to the submission file\nwrite_to_submission_file(y_test, 'baseline_2.csv')","bf35ce7c":"The training data set contains the following features:\n\n- **site1** \u2013 id of the first visited website in the session\n- **time1** \u2013 visiting time for the first website in the session\n- ...\n- **site10** \u2013 id of the tenth visited website in the session\n- **time10** \u2013 visiting time for the tenth website in the session\n- **target** \u2013 target variable, 1 for Alice's sessions, and 0 for the other users' sessions\n    \nUser sessions are chosen in the way that they are shorter than 30 min. long and contain no more than 10 websites. I.e. a session is considered over either if a user has visited 10 websites or if a session has lasted over 30 minutes.\n\nThere are some empty values in the table, it means that some sessions contain less than ten websites. Replace empty values with 0 and change columns types to integer. Also load the websites dictionary and check how it looks like:\n\n-----------------------\n\nEl conjunto de datos de entrenamiento contiene las siguientes caracter\u00edsticas:\n\n- **site1** - ID del primer sitio web visitado en la sesi\u00f3n\n- **time1** - tiempo de visita para el primer sitio web en la sesi\u00f3n\n- ...\n- **site10** - id del d\u00e9cimo sitio web visitado en la sesi\u00f3n\n- **time10** - tiempo de visita para el d\u00e9cimo sitio web en la sesi\u00f3n\n- **target** - variable de destino, 1 para las sesiones de Alice y 0 para las sesiones de otros usuarios\n\u00a0\u00a0\u00a0\u00a0\nLas sesiones de usuario se eligen de forma que sean m\u00e1s cortas que 30 min. de largo y no contienen m\u00e1s de 10 sitios web. Es decir. una sesi\u00f3n se considera si un usuario ha visitado 10 sitios web o si una sesi\u00f3n ha durado m\u00e1s de 30 minutos.\n\nHay algunos valores vac\u00edos en la tabla, lo que significa que algunas sesiones contienen menos de diez sitios web. Reemplace los valores vac\u00edos con 0 y cambie los tipos de columnas a entero. Tambi\u00e9n cargue el diccionario de sitios web y verifique c\u00f3mo se ve:","07603581":"As you might have noticed, there are not four columns in the resulting matrix (corresponding to number of different websites) but five. A zero column has been added, which indicates if the session was shorter (in our mini example we took sessions of three). This column is excessive and should be removed from the dataframe (do that yourself).\n\n##### 4.4. What is the sparseness of the matrix in our small example?\n*For discussions, please stick to [ODS Slack](https:\/\/opendatascience.slack.com\/), channel #mlcourse_ai, pinned thread __#a4_q4__*\n\n-----\n\nComo puede haber notado, no hay cuatro columnas en la matriz resultante (que corresponden al n\u00famero de sitios web diferentes), sino cinco. Se agreg\u00f3 una columna cero, que indica si la sesi\u00f3n fue m\u00e1s corta (en nuestro mini ejemplo tomamos sesiones de tres). Esta columna es excesiva y debe eliminarse del marco de datos (h\u00e1galo usted mismo).\n\n##### 4.4. \u00bfCu\u00e1l es la dispersi\u00f3n de la matriz en nuestro peque\u00f1o ejemplo?\n* Para las discusiones, siga con [ODS Slack](https:\/\/opendatascience.slack.com\/), canal #mlcourse_ai, hilo anclado __# a4_q4 __*\n\n- 42%\n- 47%\n- 50% \n- 53%\n\n\n","f80132b6":"<img src='https:\/\/mlcourse.ai\/notebooks\/blob\/master\/img\/snowboard.jpg' width=70%>\n\n*Yorko in Sheregesh, the best palce in Russia for snowboarding and skiing.*\n\n*Yorko en Sheregesh, el mejor lugar de Rusia para practicar snowboard y esquiar.*","eaa1ccc0":"In order to perform the next task, generate descriptive statistics as you did in the first assignment.\n\n##### 4.3. Select all correct statements:\n*For discussions, please stick to [ODS Slack](https:\/\/opendatascience.slack.com\/), channel #mlcourse_ai, pinned thread __#a4_q3__*\n\n- on average, Alice's session is shorter than that of other users \n- more than 1% of all sessions in the dataset belong to Alice\n- minimum and maximum durations of Alice's and other users' sessions are approximately the same  \n- variation about the mean session duration for all users (including Alice) is approximately the same\n- less than a quarter of Alice's sessions are greater than or equal to 40 seconds  \n\n-----\n\nPara realizar la siguiente tarea, genere estad\u00edsticas descriptivas como lo hizo en la primera tarea.\n\n##### 4.3. Seleccione todas las afirmaciones correctas:\n*Para discusiones, siga con [ODS Slack](https:\/\/opendatascience.slack.com\/), canal #mlcourse_ai, hilo anclado __# a4_q3 __*\n\n- en promedio, la sesi\u00f3n de Alice es m\u00e1s corta que la de otros usuarios\n- m\u00e1s del 1% de todas las sesiones en el conjunto de datos pertenecen a Alice\n- La duraci\u00f3n m\u00ednima y m\u00e1xima de las sesiones de Alicia y de otros usuarios es aproximadamente la misma.\n- la variaci\u00f3n sobre la duraci\u00f3n media de la sesi\u00f3n para todos los usuarios (incluida Alice) es aproximadamente la misma\n- menos de un cuarto de las sesiones de Alicia son mayores o iguales a 40 segundos","de3c5d1d":"Let us explore how the matrix with the websites has been formed using a mini example. Suppose we have the following table with user sessions:\n\n| id | site1 | site2 | site3 |\n|---|---|---|---|\n| 1 | 1 | 0 | 0 |\n| 2 | 1 | 3 | 1 |\n| 3 | 2 | 3 | 4 |\n\nThere are 3 sessions, and no more than 3 websites in each. Users visited four different sites in total (there are numbers from 1 to 4 in the table cells). And let us assume that the mapping is:\n\n 1. vk.com\n 2. habrahabr.ru \n 3. yandex.ru\n 4. ods.ai\n\nIf the user has visited less than 3 websites during the session, the last few values will be zero. We want to convert the original dataframe in a way that each session has a corresponding row which shows the number of visits to each particular site. I.e. we want to transform the previous table into the following form:\n\n| id | vk.com | habrahabr.ru | yandex.ru | ods.ai |\n|---|---|---|---|---|\n| 1 | 1 | 0 | 0 | 0 |\n| 2 | 2 | 0 | 1 | 0 |\n| 3 | 0 | 1 | 1 | 1 |\n\n\nTo do this, use the constructor: `csr_matrix ((data, indices, indptr))` and create a frequency table (see examples, code and comments on the links above to see how it works). Here we set all the parameters explicitly for greater clarity:\n\n------\n\nExploremos c\u00f3mo se ha formado la matriz con los sitios web utilizando un mini ejemplo. Supongamos que tenemos la siguiente tabla con sesiones de usuario:\n\n| id | site1 | sitio2 | site3 |\n| --- | --- | --- | --- |\n| 1 | 1 | 0 | 0 |\n| 2 | 1 | 3 | 1 |\n| 3 | 2 | 3 | 4 |\n\nHay 3 sesiones, y no m\u00e1s de 3 sitios web en cada uno. Los usuarios visitaron cuatro sitios diferentes en total (hay n\u00fameros del 1 al 4 en las celdas de la tabla). Y supongamos que el mapeo es:\n\n\u00a01. vk.com\n\u00a02. habrahabr.ru\n\u00a03. yandex.ru\n\u00a04. ods.ai\n\nSi el usuario ha visitado menos de 3 sitios web durante la sesi\u00f3n, los \u00faltimos valores ser\u00e1n cero. Queremos convertir el marco de datos original de manera que cada sesi\u00f3n tenga una fila correspondiente que muestre el n\u00famero de visitas a cada sitio en particular. Es decir. Queremos transformar la tabla anterior en la siguiente forma:\n\n| id | vk.com | habrahabr.ru | yandex.ru | ods.ai |\n| --- | --- | --- | --- | --- |\n| 1 | 1 | 0 | 0 | 0 |\n| 2 | 2 | 0 | 1 | 0 |\n| 3 | 0 | 1 | 1 | 1 |\n\n\nPara hacer esto, use el constructor: `csr_matrix ((data, indices, indptr))` y cree una tabla de frecuencia (vea ejemplos, c\u00f3digo y comentarios en los enlaces anteriores para ver c\u00f3mo funciona). Aqu\u00ed establecemos todos los par\u00e1metros expl\u00edcitamente para mayor claridad:","294c2c4d":"The quality of the model has decreased significantly. We added a feature that definitely seemed useful to us, but its usage only worsened the model. Why did it happen?\n\n### Important detour #2: is it necessary to scale features?\n\nHere we give an intuitive reasoning (a rigorous mathematical justification for one or another aspect in linear models you can easily find on the internet). Consider the features more closely: those of them that correspond to the number of visits to a particular web-site per session vary from 0 to 10. The feature `start_month` has a completely different range: from 201301 to 201412, this means the contribution of this variable is significantly greater than the others. It would seem that problem can be avoided if we put less weight in a linear combination of attributes in this case, but in our case logistic regression with regularization is used (by default, this parameter is `C = 1`), which penalizes the model the stronger the greater its weights are. Therefore, for linear methods with regularization, it is recommended to convert features to the same scale (you can read more about the regularization, for example, [here](https:\/\/habrahabr.ru\/company\/ods\/blog\/322076\/)).\n\nOne way to do this is standardization: for each observation you need to subtract the average value of the feature and divide this difference by the standard deviation:\n\n$$ x^{*}_{i} = \\dfrac{x_{i} - \\mu_x}{\\sigma_x}$$\n\nThe following practical tips can be given:\n- It is recommended to scale features if they have essentially different ranges or different units of measurement (for example, the country's population is indicated in units, and the country's GNP in trillions)\n- Scale features if you do not have a reason\/expert opinion to give a greater weight to any of them\n- Scaling can be excessive if the ranges of some of your features differ from each other, but they are in the same system of units (for example, the proportion of middle-aged people and people over 80 among the entire population)\n- If you want to get an interpreted model, then build a model without regularization and scaling (most likely, its quality will be worse)\n- Binary features (which take only values of 0 or 1) are usually left without conversion, (but)\n- If the quality of the model is crucial, try different options and select one where the quality is better\n\nGetting back to `start_month`, let us rescale the new feature and train the model again. This time the quality has increased:\n\n------\n\nLa calidad del modelo ha disminuido significativamente. Agregamos una caracter\u00edstica que definitivamente nos pareci\u00f3 \u00fatil, pero su uso solo empeor\u00f3 el modelo. \u00bfPor qu\u00e9 sucedi\u00f3?\n\n### Desv\u00edo importante # 2: \u00bfes necesario escalar las caracter\u00edsticas?\n\nAqu\u00ed ofrecemos un razonamiento intuitivo (una justificaci\u00f3n matem\u00e1tica rigurosa para uno u otro aspecto en modelos lineales que puede encontrar f\u00e1cilmente en Internet). Considere las caracter\u00edsticas m\u00e1s de cerca: aquellas que corresponden al n\u00famero de visitas a un sitio web en particular por sesi\u00f3n var\u00edan de 0 a 10. La caracter\u00edstica `start_month` tiene un rango completamente diferente: de 201301 a 201412, esto significa la contribuci\u00f3n De esta variable es significativamente mayor que las otras. Parece que el problema se puede evitar si le damos menos importancia a una combinaci\u00f3n lineal de atributos en este caso, pero en nuestro caso se usa la regresi\u00f3n log\u00edstica con regularizaci\u00f3n (por defecto, este par\u00e1metro es `C = 1`), que penaliza la modelo cuanto m\u00e1s fuerte, mayor es su peso. Por lo tanto, para los m\u00e9todos lineales con regularizaci\u00f3n, se recomienda convertir las funciones a la misma escala (puede leer m\u00e1s sobre la regularizaci\u00f3n, por ejemplo, [aqu\u00ed](https:\/\/habrahabr.ru\/company\/ods\/blog\/322076\/ )).\n\nUna forma de hacer esto es la estandarizaci\u00f3n: para cada observaci\u00f3n, debe restar el valor promedio de la caracter\u00edstica y dividir esta diferencia por la desviaci\u00f3n est\u00e1ndar:\n\n$$ x^{*}_{i} = \\dfrac{x_{i} - \\mu_x}{\\sigma_x}$$\n\nSe pueden dar los siguientes consejos pr\u00e1cticos:\n- Se recomienda escalar las caracter\u00edsticas si tienen esencialmente diferentes rangos o diferentes unidades de medida (por ejemplo, la poblaci\u00f3n del pa\u00eds se indica en unidades y el PNB del pa\u00eds en trillones)\n- Caracter\u00edsticas de escala si no tiene una raz\u00f3n\/opini\u00f3n de un experto para dar mayor peso a cualquiera de ellas.\n- La escala puede ser excesiva si los rangos de algunas de sus caracter\u00edsticas difieren entre s\u00ed, pero est\u00e1n en el mismo sistema de unidades (por ejemplo, la proporci\u00f3n de personas de mediana edad y mayores de 80 en toda la poblaci\u00f3n)\n- Si desea obtener un modelo interpretado, cree un modelo sin regularizaci\u00f3n ni escala (lo m\u00e1s probable es que su calidad sea peor)\n- Las caracter\u00edsticas binarias (que toman solo valores de 0 o 1) generalmente se dejan sin conversi\u00f3n, (pero)\n- Si la calidad del modelo es crucial, pruebe diferentes opciones y seleccione una donde la calidad sea mejor\n\nVolviendo a `start_month`, volvamos a escalar la nueva funci\u00f3n y entrenamos el modelo nuevamente. Esta vez la calidad ha aumentado:","3c4486fd":"### 1. Data Downloading and Transformation\nRegister on [Kaggle](www.kaggle.com), if you have not done it before.\nGo to the competition [page](https:\/\/inclass.kaggle.com\/c\/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2) and download the data.\n\nFirst, read the training and test sets. Then we'll explore the data in hand and do a couple of simple exercises. \n\n-------\n\n### 1. Descarga y transformaci\u00f3n de datos\nReg\u00edstrese en [Kaggle] (www.kaggle.com), si no lo ha hecho antes.\nVaya a la [Pagina](https:\/\/inclass.kaggle.com\/c\/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2) de la competencia y descargue los datos.\n\nPrimero, lea los conjuntos de entrenamiento y prueba. Luego exploraremos los datos disponibles y haremos un par de ejercicios simples.","58a01ad1":"Now let us look at the timestamps and try to characterize sessions as timeframes:\n\nAhora veamos las marcas de tiempo y tratemos de caracterizar las sesiones como marcos de tiempo:","c7c8b9ac":"So, the new feature has slightly decreased the quality, so we will not use it. Nevertheless, do not rush to throw features out because they haven't performed well. They can be useful in a combination with other features (for example, when a new feature is a ratio or a product of two others).\n\n#####  4.8. Add two new features: start_hour and morning. Calculate the metric. Which of these features gives an improvement?\n\nThe `start_hour` feature is the hour at which the session started (from 0 to 23), and the binary feature `morning` is equal to 1 if the session started in the morning and 0 if the session started later (we assume that morning means `start_hour` is equal to 11 or less).\n\nWill you scale the new features? Make your assumptions and test them in practice.\n\n*For discussions, please stick to [ODS Slack](https:\/\/opendatascience.slack.com\/), channel #mlcourse_ai, pinned thread __#a4_q8__*\n\n- None of the features gave an improvement :(\n- `start_hour` feature gave an improvement, and `morning` did not\n- `morning` feature gave an improvement, and `start_hour` did not\n- Both features gave an improvement\n\n*Tip: find suitable functions for working with time series data in [documentation](http:\/\/pandas.pydata.org\/pandas-docs\/stable\/api.html). Do not forget to include the `start_month` feature.*\n\n-----\n\nEntonces, la nueva caracter\u00edstica ha disminuido ligeramente la calidad, por lo que no la usaremos. Sin embargo, no se apresure a lanzar caracter\u00edsticas porque no han tenido un buen desempe\u00f1o. Pueden ser \u00fatiles en combinaci\u00f3n con otras caracter\u00edsticas (por ejemplo, cuando una nueva caracter\u00edstica es una proporci\u00f3n o un producto de otras dos).\n\n##### 4.8. A\u00f1ade dos nuevas caracter\u00edsticas: start_hour y ma\u00f1ana. Calcula la m\u00e9trica. \u00bfCu\u00e1l de estas caracter\u00edsticas da una mejora?\n\nLa funci\u00f3n `start_hour` es la hora a la que comenz\u00f3 la sesi\u00f3n (de 0 a 23), y la funci\u00f3n binaria` morning` es igual a 1 si la sesi\u00f3n comenz\u00f3 por la ma\u00f1ana y 0 si la sesi\u00f3n comenz\u00f3 m\u00e1s tarde (suponemos que esa ma\u00f1ana significa `start_hour` es igual a 11 o menos).\n\n\u00bfVa a escalar las nuevas caracter\u00edsticas? Haz tus suposiciones y ponlas a prueba en la pr\u00e1ctica.\n\n*Para discusiones, siga con [ODS Slack](https:\/\/opendatascience.slack.com\/), canal #mlcourse_ai, hilo anclado __#a4_q8 __*\n\n- Ninguna de las caracter\u00edsticas dio una mejora :(\n- La caracter\u00edstica `start_hour` dio una mejora, y` morning` no lo hizo\n- La funci\u00f3n `morning` dio una mejora, y` start_hour` no lo hizo.\n- Ambas caracter\u00edsticas dieron una mejora.\n\n*Consejo: encuentre funciones adecuadas para trabajar con datos de series de tiempo en [documentaci\u00f3n](http:\/\/pandas.pydata.org\/pandas-docs\/stable\/api.html). No olvides incluir la caracter\u00edstica `start_month`. *","84681749":"If you understand what just happened here, then you can skip the next passage (perhaps, you can handle logistic regression too?), If not, then let us figure it out.\n\n### Important detour #1: Sparse Matrices\n\nLet us estimate how much memory it will require to store our data in the example above. Our united dataframe contains 336 thousand samples of 48 thousand integer features in each. It's easy to calculate the required amount of memory, roughly:\n\n$$336K * 48K * 8 bytes = 16M * 8 bytes = 128 GB,$$\n\n(that's the [exact](http:\/\/www.wolframalpha.com\/input\/?i=336358*48371*8+bytes) value). Obviously, ordinary mortals have no such volumes (strictly speaking, Python may allow you to create such a matrix, but it will not be easy to do anything with it). The interesting fact is that most of the elements of our matrix are zeros. If we count non-zero elements, then it will be about 1.8 million, i.\u0435. slightly more than 10% of all matrix elements. Such a matrix, where most elements are zeros, is called sparse, and the ratio between the number of zero elements and the total number of elements is called the sparseness of the matrix.\n\nFor the work with such matrices you can use `scipy.sparse` library, check [documentation](https:\/\/docs.scipy.org\/doc\/scipy-0.18.1\/reference\/sparse.html) to understand what possible types of sparse matrices are, how to work with them and in which cases their usage is most effective. You can learn how they are arranged, for example, in Wikipedia [article](https:\/\/en.wikipedia.org\/wiki\/Sparse_matrix).\nNote, that a sparse matrix contains only non-zero elements, and you can get the allocated memory size like this (significant memory savings are obvious):\n\n------\n\nSi entiendes lo que acaba de suceder aqu\u00ed, entonces puedes omitir el siguiente pasaje (quiz\u00e1s, \u00bftambi\u00e9n puedes manejar la regresi\u00f3n log\u00edstica?), Si no, entonces d\u00e9janos resolverlo.\n\n### Desv\u00edo importante # 1: Matrices dispersas\n\nEstimemos cu\u00e1nta memoria requerir\u00e1 para almacenar nuestros datos en el ejemplo anterior. Nuestro marco de datos unido contiene 336 mil muestras de 48 mil enteros en cada una. Es f\u00e1cil calcular la cantidad de memoria requerida, aproximadamente:\n\n$$336K * 48K * 8 bytes = 16M * 8 bytes = 128 GB,$$\n\n(ese es el valor [exacto](http:\/\/www.wolframalpha.com\/input\/?i=336358*48371*8+bytes)). Obviamente, los mortales comunes no tienen tales vol\u00famenes (hablando estrictamente, Python puede permitirte crear una matriz de este tipo, pero no ser\u00e1 f\u00e1cil hacer nada con ella). El hecho interesante es que la mayor\u00eda de los elementos de nuestra matriz son ceros. Si contamos los elementos que no son cero, entonces ser\u00e1n aproximadamente 1.8 millones, es decir, Un poco m\u00e1s del 10% de todos los elementos de la matriz. Dicha matriz, donde la mayor\u00eda de los elementos son ceros, se denomina dispersa, y la relaci\u00f3n entre el n\u00famero de elementos cero y el n\u00famero total de elementos se denomina dispersi\u00f3n de la matriz.\n\nPara el trabajo con dichas matrices puede usar la biblioteca `scipy.sparse`, verifique [documentaci\u00f3n](https:\/\/docs.scipy.org\/doc\/scipy-0.18.1\/reference\/sparse.html) para entender qu\u00e9 tipos posibles Las matrices dispersas son, c\u00f3mo trabajar con ellas y en qu\u00e9 casos su uso es m\u00e1s efectivo. Puede aprender c\u00f3mo est\u00e1n ordenados, por ejemplo, en Wikipedia [art\u00edculo](https:\/\/en.wikipedia.org\/wiki\/Sparse_matrix).\nTenga en cuenta que una matriz dispersa solo contiene elementos que no son cero, y puede obtener el tama\u00f1o de memoria asignado de esta manera (los ahorros significativos de memoria son obvios):","214e272a":"### 2. Brief Exploratory Data Analysis \/ Breve an\u00e1lisis de datos exploratorios","7f01021b":"Before we start training models, we have to perform Exploratory Data Analysis ([EDA](https:\/\/en.wikipedia.org\/wiki\/Exploratory_data_analysis)). Today, we are going to perform a shorter version, but we will use other techniques as we move forward. Let's check which websites in the training data set are the most visited. As you can see, they are Google services and a bioinformatics website (a website with 'zero'-index is our missed values, just ignore it):\n\n----\n\nAntes de comenzar con los modelos de capacitaci\u00f3n, debemos realizar un An\u00e1lisis de datos exploratorios ([EDA](https:\/\/en.wikipedia.org\/wiki\/Exploratory_data_analysis)). Hoy vamos a realizar una versi\u00f3n m\u00e1s corta, pero usaremos otras t\u00e9cnicas a medida que avancemos. Vamos a comprobar qu\u00e9 sitios web en el conjunto de datos de entrenamiento son los m\u00e1s visitados. Como puede ver, son servicios de Google y un sitio web de bioinform\u00e1tica (un sitio web con \u00edndice 'cero' son nuestros valores perdidos, simplemente ign\u00f3relo):","12845581":"Create a feature that will be a number in YYYYMM format from the date when the session was held, for example 201407 -- year 2014 and 7th month. Thus, we will take into account the monthly [linear trend](http:\/\/people.duke.edu\/~rnau\/411trend.htm) for the entire period of the data provided.\n\n----\n\nCree una funci\u00f3n que ser\u00e1 un n\u00famero en formato YYYYMM a partir de la fecha en que se celebr\u00f3 la sesi\u00f3n, por ejemplo, 201407 - a\u00f1o 2014 y s\u00e9ptimo mes. Por lo tanto, tendremos en cuenta la [tendencia lineal]mensual (http:\/\/people.duke.edu\/~rnau\/411trend.htm) para todo el per\u00edodo de los datos proporcionados.","6ccab181":"In this part, you'll need to beat the \"A4 baseline 3\" baseline. No more step-by-step instructions. But it'll be very helpful for you to study the Kernel \"[Correct time-aware cross-validation scheme](https:\/\/www.kaggle.com\/kashnitsky\/correct-time-aware-cross-validation-scheme)\".\n\nHere are a few tips for finding new features: think about what you can come up with using existing features, try multiplying or dividing two of them, justify or decline your hypotheses with plots, extract useful information from time series data (time1 ... time10), do not hesitate to convert an existing feature (for example, take a logarithm), etc. Checkout other [Kernels](https:\/\/www.kaggle.com\/c\/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2\/kernels). We encourage you to try new ideas and models throughout the course and participate in the competitions - it's fun!\n\nWhen you get into Kaggle and Xgboost, you'll feel like that, and it's OK :)\n\n-----\n\nEn esta parte, deber\u00e1 superar la l\u00ednea de base \"A4 basal 3\". No m\u00e1s instrucciones paso a paso. Pero ser\u00e1 muy \u00fatil para usted estudiar el Kernel \"[Esquema correcto de validaci\u00f3n cruzada consciente del tiempo](https:\/\/www.kaggle.com\/kashnitsky\/correct-time-aware-cross-validation-scheme)\" .\n\nAqu\u00ed hay algunos consejos para encontrar nuevas funciones: piense en lo que puede lograr con las funciones existentes, intente multiplicar o dividir dos de ellas, justifique o rechace sus hip\u00f3tesis con gr\u00e1ficos, extraiga informaci\u00f3n \u00fatil de datos de series de tiempo (tiempo1 ... time10), no dude en convertir una caracter\u00edstica existente (por ejemplo, tome un logaritmo), etc. Compruebe otros [Kernels](https:\/\/www.kaggle.com\/c\/catch-me-if-you-can- intruso-detecci\u00f3n-a-p\u00e1gina web-sesi\u00f3n-seguimiento2 \/ kernels). Lo invitamos a probar nuevas ideas y modelos a lo largo del curso y participar en las competiciones. \u00a1Es divertido!\n\nCuando entres en Kaggle y Xgboost, te sentir\u00e1s as\u00ed, y est\u00e1 bien :)\n\n<img src=https:\/\/mlcourse.ai\/notebooks\/blob\/master\/img\/xgboost_meme.jpg' width=50%>","e7ec2f3d":"Plot the graph of the quality metric (AUC-ROC) versus the value of the regularization parameter. The value of quality metric corresponding to the default value of C=1 is represented by a horizontal dotted line:\n\n-----\n\nGrafique la gr\u00e1fica de la m\u00e9trica de calidad (AUC-ROC) frente al valor del par\u00e1metro de regularizaci\u00f3n. El valor de la m\u00e9trica de calidad correspondiente al valor predeterminado de C = 1 se representa mediante una l\u00ednea de puntos horizontal:","7eb2a012":"#### 4.1. What are the dimensions of the training and test sets (in exactly this order)? \n*For discussions, please stick to [ODS Slack](https:\/\/opendatascience.slack.com\/), channel #mlcourse_ai, pinned thread __#a4_q1__*\n\n------\n\n#### 4.1. \u00bfCu\u00e1les son las dimensiones de los conjuntos de entrenamiento y prueba (exactamente en este orden)?\n*Para las discusiones, siga con [ODS Slack](https:\/\/opendatascience.slack.com\/), canal #mlcourse_ai, hilo anclado __# a4_q1 __*\n\n- (82797, 20) and (253561, 20)\n- (82797, 20) and (253561, 21)\n- (253561, 21) and (82797, 20) \n- (253561, 20) and (82797, 20)","4dd02d24":"The first model demonstrated the quality  of 0.92 on the validation set. Let's take it as the first baseline and starting point. To make a prediction on the test data set **we need to train the model again on the entire training data set** (until this moment, our model used only part of the data for training), which will increase its generalizing ability:\n\n-------\n\nEl primer modelo demostr\u00f3 la calidad de 0,92 en el conjunto de validaci\u00f3n. Tomemos como primera l\u00ednea de base y punto de partida. Para hacer una predicci\u00f3n sobre el conjunto de datos de prueba **necesitamos entrenar el modelo nuevamente en todo el conjunto de datos de entrenamiento** (hasta este momento, nuestro modelo us\u00f3 solo parte de los datos de entrenamiento), lo que aumentar\u00e1 su capacidad de generalizaci\u00f3n:","59ee3171":"In this part of the assignment, you have learned how to use sparse matrices, train logistic regression models, create new features and selected the best ones, learned why you need to scale features, and how to select hyperparameters. That's a lot!\n\n----\n\nEn esta parte de la tarea, aprendi\u00f3 a usar matrices dispersas, entrenar modelos de regresi\u00f3n log\u00edstica, crear nuevas funciones y seleccionar las mejores, aprender por qu\u00e9 necesita escalar las funciones y c\u00f3mo seleccionar los hiperpar\u00e1metros. \u00a1Eso es mucho!","c0d70e26":"##### 4.2. What kind of websites does Alice visit the most?\n*For discussions, please stick to [ODS Slack](https:\/\/opendatascience.slack.com\/), channel #mlcourse_ai, pinned thread __#a4_q2__*\n\n- videohostings \n- social networks\n- torrent trackers\n- news\n\n-----\n\n##### 4.2. \u00bfQu\u00e9 tipo de sitios web visita Alice m\u00e1s?\n*Para las discusiones, siga con [ODS Slack](https:\/\/opendatascience.slack.com\/), canal #mlcourse_ai, hilo anclado __# a4_q2 __*\n\n- Alojamiento de video\n- redes sociales\n- rastreadores de torrentes\n- Noticias","b9e4ca00":"# Part 2. Freeride  \/  Paseo libre","81879bae":"Another benefit of using sparse matrices is that there are special implementations of both matrix operations and machine learning algorithms for them, which sometimes allows to significantly accelerate operations due to the data structure peculiarities. This applies to logistic regression as well. Now everything is ready to build our first model.\n\n### 3. Training the first model\n\nSo, we have an algorithm and data for it. Let us build our first model, using [logistic regression](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html) implementation from ` Sklearn` with default parameters. We will use the first 90% of the data for training (the training data set is sorted by time), and the remaining 10% for validation. Let's write a simple function that returns the quality of the model and then train our first classifier:\n\n-----\n\nOtro beneficio del uso de matrices dispersas es que existen implementaciones especiales tanto de operaciones matriciales como de algoritmos de aprendizaje autom\u00e1tico, que a veces permiten acelerar significativamente las operaciones debido a las peculiaridades de la estructura de datos. Esto se aplica tambi\u00e9n a la regresi\u00f3n log\u00edstica. Ahora todo est\u00e1 listo para construir nuestro primer modelo.\n\n### 3. Entrenando el primer modelo\n\nEntonces, tenemos un algoritmo y datos para ello. Construyamos nuestro primer modelo, utilizando la [regresi\u00f3n log\u00edstica](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html) implementaci\u00f3n de `Sklearn` con par\u00e1metros predeterminados. Usaremos el primer 90% de los datos para la capacitaci\u00f3n (el conjunto de datos de la capacitaci\u00f3n est\u00e1 ordenado por tiempo) y el 10% restante para la validaci\u00f3n. Escribamos una funci\u00f3n simple que devuelva la calidad del modelo y luego entrenemos nuestro primer clasificador:","fc322471":"If you follow these steps and upload the answer to the competition [page](https:\/\/inclass.kaggle.com\/c\/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2), you will get `ROC AUC = 0.90812` on the public leaderboard (\"A4 baseline 1\").\n\n### 4. Model Improvement: Feature Engineering\n\nNow we are going to try to improve the quality of our model by adding new features to the data. But first, answer the following question:\n\n##### 4.5. What years are present in the training and test datasets, respectively?\n*For discussions, please stick to [ODS Slack](https:\/\/opendatascience.slack.com\/), channel #mlcourse_ai, pinned thread __#a4_q5__*\n\n-------\n\nSi sigue estos pasos y carga la respuesta a la competencia [p\u00e1gina](https:\/\/inclass.kaggle.com\/c\/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2) , obtendr\u00e1 `ROC AUC = 0.90812` en la tabla de clasificaci\u00f3n p\u00fablica (\" A4 baseline 1 \").\n\n### 4. Mejora del modelo: ingenier\u00eda de caracter\u00edsticas\n\nAhora vamos a tratar de mejorar la calidad de nuestro modelo agregando nuevas caracter\u00edsticas a los datos. Pero primero, responde la siguiente pregunta:\n\n##### 4.5. \u00bfQu\u00e9 a\u00f1os est\u00e1n presentes en los conjuntos de datos de entrenamiento y prueba, respectivamente?\n*Para las discusiones, siga con [ODS Slack](https:\/\/opendatascience.slack.com\/), canal #mlcourse_ai, hilo anclado __#a4_q5 __*\n\n- 13 and 14\n- 2012 and 2013\n- 2013 and 2014 \n- 2014 and 2015","d59a296c":"<img src='https:\/\/mlcourse.ai\/notebooks\/blob\/master\/img\/followme_alice.png' width=50%>\n\n*image credit [@muradosmann](https:\/\/www.instagram.com\/muradosmann\/?hl=en)*","240da952":"# <center> Asignacion #4\n## <center>  User Identification with Logistic Regression (beating baselines in the \"Alice\" competition)\n\nToday we are going to practice working with sparse matrices, training Logistic Regression models, and doing feature engineering. We will reproduce a couple of baselines in the [\"Catch Me If You Can: Intruder Detection through Webpage Session Tracking\"](https:\/\/www.kaggle.com\/c\/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2) (a.k.a. \"Alice\") Kaggle inclass competition. More credits will be given for beating a stronger baseline. \n\n**Your task:**\n 1. \"Follow me\". Complete the missing code and submit your answers via [the google-form](https:\/\/docs.google.com\/forms\/d\/1V4lHXkjZvpDDvHAcnH6RuEQJecBaLo8zooxDl1_aP60). 14 credit max. for this part\n 2. \"Freeride\". Come up with good features to beat the baseline \"A4 baseline 3\". You need to name your [team](https:\/\/www.kaggle.com\/c\/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2\/team) (out of 1 person) in full accordance with the course rating. You can think of it as a part of the assignment. 10 more credits for beating the mentioned baseline and correct team naming. \n \n ----\n \n ## <center> Identificaci\u00f3n del usuario con regresi\u00f3n log\u00edstica (superando las l\u00edneas de base en el concurso \"Alice\")\n\nHoy vamos a practicar trabajar con matrices dispersas, entrenar modelos de Regresi\u00f3n log\u00edstica y hacer ingenier\u00eda de caracter\u00edsticas. Reproduciremos un par de l\u00edneas de base en [\"Atr\u00e1pame si puedes: Detecci\u00f3n de intrusos a trav\u00e9s del seguimiento de sesi\u00f3n en la p\u00e1gina web\"](https:\/\/www.kaggle.com\/c\/catch-me-if-you-can-intruder- detecci\u00f3n a trav\u00e9s de la p\u00e1gina-sesi\u00f3n-seguimiento2) (tambi\u00e9n conocida como \"Alicia\") Kaggle en la competencia de clase. Se dar\u00e1n m\u00e1s cr\u00e9ditos por superar una l\u00ednea de base m\u00e1s fuerte.\n\n**Tu tarea:**\n\u00a01. \"S\u00edgueme\". Complete el c\u00f3digo que falta y env\u00ede sus respuestas a trav\u00e9s de [google-form](https:\/\/docs.google.com\/forms\/d\/1V4lHXkjZvpDDvHAcnH6RuEQJecBaLo8zooxDl1_aP60). 14 cr\u00e9ditos max. para esta parte\n\u00a02. \"Freeride\". Cree buenas caracter\u00edsticas para superar la l\u00ednea de base \"A4 baseline 3\". Debe nombrar a su [equipo](https:\/\/www.kaggle.com\/c\/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2\/team) (de 1 persona) ) en plena conformidad con la calificaci\u00f3n del curso. Puedes considerarlo como parte de la tarea. 10 cr\u00e9ditos m\u00e1s por superar la l\u00ednea de base mencionada y el nombramiento correcto del equipo.","d273b079":"##### Problem description\n\nIn this competition, we''ll analyze the sequence of websites consequently visited by a particular person and try to predict whether this person is Alice or someone else. As a metric we will use [ROC AUC](https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic).\n\n------------\n\n##### Descripci\u00f3n del problema\n\nEn esta competencia, analizaremos la secuencia de sitios web visitados por una persona en particular y trataremos de predecir si esta persona es Alice o alguien m\u00e1s. Como m\u00e9trica usaremos [ROC AUC](https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic) ..","388af080":"Sessions are sequences of website indices, and data in this representation is useless for machine learning method (just think, what happens if we switched all ids of all websites). \n\nAccording to our hypothesis (Alice has favorite websites), we need to transform this dataframe so each website has a corresponding feature (column) and its value is equal to number of this website visits in the session. It can be done in two lines:\n\n-----\n\nLas sesiones son secuencias de \u00edndices de sitios web, y los datos en esta representaci\u00f3n son in\u00fatiles para el m\u00e9todo de aprendizaje autom\u00e1tico (solo piense, qu\u00e9 pasa si cambiamos todas las ID de todos los sitios web).\n\nSeg\u00fan nuestra hip\u00f3tesis (Alice tiene sitios web favoritos), necesitamos transformar este marco de datos para que cada sitio web tenga una caracter\u00edstica correspondiente (columna) y su valor sea igual al n\u00famero de visitas de este sitio web en la sesi\u00f3n. Se puede hacer en dos l\u00edneas:","ad3408be":"# Part 1. Follow me  \/ Sigueme","962e9287":"### 5. Regularization and Parameter Tuning\n\nWe have introduced features that improve the quality of our model in comparison with the first baseline. Can we do even better? After we have changed the training and test sets, it almost always makes sense to search for the optimal hyperparameters - the parameters of the model that do not change during training.\n\nFor example, in week 3, you learned that, in decision trees, the depth of the tree is a hyperparameter, but the feature by which splitting occurs and its threshold is not. \n\nIn the logistic regression that we use, the weights of each feature are changing, and we find their optimal values during training; meanwhile, the regularization parameter remains constant. This is the hyperparameter that we are going to optimize now.\n\nCalculate the quality on a validation set with a regularization parameter, which is equal to 1 by default:\n\n------\n\n### 5. Regularizaci\u00f3n y ajuste de par\u00e1metros\n\nHemos introducido caracter\u00edsticas que mejoran la calidad de nuestro modelo en comparaci\u00f3n con la primera l\u00ednea de base. \u00bfPodemos hacerlo a\u00fan mejor? Despu\u00e9s de cambiar los conjuntos de entrenamiento y prueba, casi siempre tiene sentido buscar los hiperpar\u00e1metros \u00f3ptimos, los par\u00e1metros del modelo que no cambian durante el entrenamiento.\n\nPor ejemplo, en la semana 3, aprendi\u00f3 que, en los \u00e1rboles de decisi\u00f3n, la profundidad del \u00e1rbol es un hiperpar\u00e1metro, pero la caracter\u00edstica por la cual se produce la divisi\u00f3n y su umbral no lo es.\n\nEn la regresi\u00f3n log\u00edstica que usamos, los pesos de cada funci\u00f3n est\u00e1n cambiando y encontramos sus valores \u00f3ptimos durante el entrenamiento; Mientras tanto, el par\u00e1metro de regularizaci\u00f3n se mantiene constante. Este es el hiperpar\u00e1metro que vamos a optimizar ahora.\n\nCalcule la calidad en un conjunto de validaci\u00f3n con un par\u00e1metro de regularizaci\u00f3n, que es igual a 1 por defecto:","a1a32fb5":"##### 4.6. Plot the graph of the number of Alice sessions versus the new feature, start_month. Choose the correct statement:\n\n*For discussions, please stick to [ODS Slack](https:\/\/opendatascience.slack.com\/), channel #mlcourse_ai, pinned thread __#a4_q6__*\n\n- Alice wasn't online at all for the entire period\n- From the beginning of 2013 to mid-2014, the number of Alice's sessions per month decreased\n- The number of Alice's sessions per month is generally constant for the entire period\n- From the beginning of 2013 to mid-2014, the number of Alice's sessions per month increased\n\n*Hint: the graph will be more explicit if you treat `start_month` as a categorical ordinal variable*.\n\n----------\n\n##### 4.6. Dibuje la gr\u00e1fica del n\u00famero de sesiones de Alicia frente a la nueva funci\u00f3n, start_month. Elija la declaraci\u00f3n correcta:\n\n*Para discusiones, siga con [ODS Slack](https:\/\/opendatascience.slack.com\/), canal #mlcourse_ai, hilo anclado __#a4_q6 __*\n\n- Alice no estuvo en l\u00ednea en todo el per\u00edodo\n- Desde principios de 2013 hasta mediados de 2014, el n\u00famero de sesiones de Alice por mes disminuy\u00f3.\n- El n\u00famero de sesiones de Alicia por mes es generalmente constante durante todo el per\u00edodo.\n- Desde principios de 2013 hasta mediados de 2014, el n\u00famero de sesiones de Alice por mes aument\u00f3.\n\n* Sugerencia: la gr\u00e1fica ser\u00e1 m\u00e1s expl\u00edcita si trata a `start_month` como una variable ordinal categ\u00f3rica *.","4e89735a":"We will try to beat this result by optimizing the regularization parameter. We will take a list of possible values of C and calculate the quality metric on the validation set for each of C-values:\n\n-----\n\nIntentaremos mejorar este resultado optimizando el par\u00e1metro de regularizaci\u00f3n. Tomaremos una lista de posibles valores de C y calcularemos la m\u00e9trica de calidad en el conjunto de validaci\u00f3n para cada uno de los valores de C:","01eca6aa":"<center>\n<img src=\"https:\/\/mlcourse.ai\/notebooks\/blob\/master\/img\/ods_stickers.jpg\" \/>\n    \n## [mlcourse.ai](mlcourse.ai) \u2013 Open Machine Learning Course \nAuthors: Yury Isakov, [Yury Kashnitskiy](https:\/\/yorko.github.io) (@yorko). Edited by Anna Tarelina (@feuerengel). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/) license. Free use is permitted for any non-commercial purpose.","3f80ee8a":"In this way, we have an illustration and thoughts about the usefulness of the new feature, add it to the training sample and check the quality of the new model:\n\n----\n\nDe esta manera, tenemos una ilustraci\u00f3n y pensamientos sobre la utilidad de la nueva funci\u00f3n, la agregamos a la muestra de capacitaci\u00f3n y verificamos la calidad del nuevo modelo:","beaa0478":"For the last task in this assignment: train the model using the optimal regularization parameter you found (do not round up to two digits like in the last question). If you do everything correctly and submit your solution, you should see `ROC AUC = 0.92784` on the public leaderboard (\"A4 baseline 2\"):\n\n----\n\nPara la \u00faltima tarea en esta asignaci\u00f3n: entrene el modelo utilizando el par\u00e1metro de regularizaci\u00f3n \u00f3ptimo que encontr\u00f3 (no redondee hasta dos d\u00edgitos como en la \u00faltima pregunta). Si hace todo correctamente y env\u00eda su soluci\u00f3n, deber\u00eda ver `ROC AUC = 0.92784` en la tabla de clasificaci\u00f3n p\u00fablica (\" A4 baseline 2 \"):","1aa3382c":"##### 4.7. Add to the training set a new feature \"n_unique_sites\" \u2013 the number of the unique web-sites in a session. Calculate how the quality on the validation set has changed\n\n*For discussions, please stick to [ODS Slack](https:\/\/opendatascience.slack.com\/), channel #mlcourse_ai, pinned thread __#a4_q7__*\n\n- It has decreased. It is better not to add a new feature. \n- It has not changed\n- It has decreased. The new feature should be scaled.\n- I am confused, and I do not know if it's necessary to scale a new feature.\n\n*Tips: use the nunique() function from `pandas`. Do not forget to include the start_month in the set. Will you scale a new feature? Why?*\n\n-------------\n\n##### 4.7. Agregue al conjunto de capacitaci\u00f3n una nueva funci\u00f3n \"n_unique_sites\": el n\u00famero de sitios web \u00fanicos en una sesi\u00f3n. Calcule c\u00f3mo ha cambiado la calidad en el conjunto de validaci\u00f3n\n\n* Para las discusiones, siga con [ODS Slack](https:\/\/opendatascience.slack.com\/), canal #mlcourse_ai, hilo anclado __# a4_q7__*\n\n- Ha disminuido. Es mejor no agregar una nueva caracter\u00edstica.\n- No ha cambiado.\n- Ha disminuido. La nueva caracter\u00edstica debe ser escalada.\n- Estoy confundido, y no s\u00e9 si es necesario escalar una nueva funci\u00f3n.\n\n* Consejos: usa la funci\u00f3n nunique() de `pandas`. No olvides incluir el start_month en el set. \u00bfVa a escalar una nueva caracter\u00edstica? \u00bfPor qu\u00e9?*","52a15ec5":"In order to train our first model, we need to prepare the data. First of all, exclude the target variable from the training set. Now both training and test sets have the same number of columns, therefore aggregate them into one dataframe.  Thus, all transformations will be performed simultaneously on both training and test data sets. \n\nOn the one hand, it leads to the fact that both data sets have one feature space (you don't have to worry that you forgot to transform a feature in some data sets). On the other hand, processing time will increase. \nFor the enormously large sets it might turn out that it is impossible to transform both data sets simultaneously (and sometimes you have to split your transformations into several stages only for train\/test data set).\nIn our case, with this particular data set, we are going to perform all the transformations for the whole united dataframe at once, and before training the model or making predictions we will just take its appropriate part.\n\n---------------\n\nPara entrenar nuestro primer modelo, necesitamos preparar los datos. En primer lugar, excluya la variable objetivo del conjunto de entrenamiento. Ahora tanto los conjuntos de entrenamiento como los de prueba tienen el mismo n\u00famero de columnas, por lo tanto, agr\u00e9guelos en un solo marco de datos. Por lo tanto, todas las transformaciones se realizar\u00e1n simult\u00e1neamente en los conjuntos de datos de entrenamiento y de prueba.\n\nPor un lado, conduce al hecho de que ambos conjuntos de datos tienen un espacio de caracter\u00edsticas (no tiene que preocuparse de que se haya olvidado de transformar una caracter\u00edstica en algunos conjuntos de datos). Por otro lado, el tiempo de procesamiento aumentar\u00e1.\nPara los conjuntos enormemente grandes, podr\u00eda resultar imposible transformar ambos conjuntos de datos simult\u00e1neamente (y, a veces, tiene que dividir sus transformaciones en varias etapas solo para el conjunto de datos del tren\/prueba).\nEn nuestro caso, con este conjunto de datos en particular, vamos a realizar todas las transformaciones para todo el marco de datos unido de una vez, y antes de entrenar el modelo o hacer predicciones, tomaremos la parte apropiada.","3c0d80d7":"For the very basic model, we will use only the visited websites in the session (but we will not take into account timestamp features). The point behind this data selection is: *Alice has her favorite sites, and the more often you see these sites in the session, the higher probability that this is Alice's session, and vice versa.*\n\nLet us prepare the data, we will take only features `site1, site2, ... , site10` from the whole dataframe. Keep in mind that the missing values are replaced with zero. Here is how the first rows of the dataframe look like:\n\n-----\n\nPara el modelo muy b\u00e1sico, usaremos solo los sitios web visitados en la sesi\u00f3n (pero no tomaremos en cuenta las caracter\u00edsticas de la marca de tiempo). El punto detr\u00e1s de esta selecci\u00f3n de datos es: * Alice tiene sus sitios favoritos, y cuanto m\u00e1s a menudo vea estos sitios en la sesi\u00f3n, mayor ser\u00e1 la probabilidad de que esta sea la sesi\u00f3n de Alice y viceversa.*\n\nPreparemos los datos, tomaremos solo las caracter\u00edsticas `site1, site2, ..., site10` de todo el marco de datos. Tenga en cuenta que los valores que faltan se reemplazan con cero. Aqu\u00ed es c\u00f3mo se ven las primeras filas del marco de datos:","c141e247":"##### 4.9. What is the value of parameter C (if rounded to 2 decimals) that corresponds to the highest model quality?\n\n*For discussions, please stick to [ODS Slack](https:\/\/opendatascience.slack.com\/), channel #mlcourse_ai, pinned thread __#a4_q9__*\n\n- 0.17 \n- 0.46\n- 1.29\n- 3.14\n\n-----\n\n##### 4.9. \u00bfCu\u00e1l es el valor del par\u00e1metro C (si se redondea a 2 decimales) que corresponde a la calidad m\u00e1s alta del modelo?\n\n* Para las discusiones, siga con [ODS Slack](https:\/\/opendatascience.slack.com\/), canal #mlcourse_ai, hilo anclado __# a4_q9__*\n\n- 0.17\n- 0.46\n- 1.29\n- 3.14"}}