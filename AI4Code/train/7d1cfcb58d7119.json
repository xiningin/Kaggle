{"cell_type":{"f0a92be2":"code","c5059cea":"code","dd960532":"code","ea294812":"code","cc78b837":"code","d5432829":"code","67cecee2":"code","df2122e9":"code","ad4d8145":"code","a5462ebc":"code","f1b03ab7":"code","8aa4609d":"code","2478638f":"code","1f78d8e8":"code","e0cc3aa8":"code","69150331":"code","e82513c8":"code","f0f9702f":"code","61d194cf":"code","19186e0e":"code","e5df9f06":"code","1ec93611":"code","de7d3c4b":"code","5839de40":"code","b73e651b":"code","b00e460c":"code","9bc91398":"markdown","83706ab8":"markdown","3bbf6da3":"markdown","21e9f052":"markdown","1802a344":"markdown","4c489e7f":"markdown","d3a6aecb":"markdown","86e43f83":"markdown","4e2e5d57":"markdown","adafbdea":"markdown","db1ecede":"markdown","b5fb3ffd":"markdown","6c292c1e":"markdown","0dc6c3f2":"markdown","ee128ed3":"markdown","490f8b4d":"markdown","b929e75c":"markdown","0bb52382":"markdown","8922fd1c":"markdown"},"source":{"f0a92be2":"# source text\ndata = \"\"\" Jack and Jill went up the hill\\n\nTo fetch a pail of water\\n\nJack fell down and broke his crown\\n\nAnd Jill came tumbling after\\n \"\"\"","c5059cea":"from keras.preprocessing.text import Tokenizer\n# integer encode text\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts([data])\nencoded = tokenizer.texts_to_sequences([data])[0]","dd960532":"# determine the vocabulary size\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: %d' % vocab_size)","ea294812":"# create word -> word sequences\nsequences = []\nfor i in range(1, len(encoded)):\n    sequence = encoded[i-1:i+1]   \n    sequences.append(sequence)\nprint('Total Sequences: %d' % len(sequences))","cc78b837":"# split into X and y elements\nimport numpy as np\nimport pandas as pd\nsequences = np.array(sequences)\nX, y = sequences[:,0],sequences[:,1]","d5432829":"from keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\n# one hot encode outputs\ny = to_categorical(y, num_classes=vocab_size)","67cecee2":"# define the model\ndef define_model(vocab_size):\n    model = Sequential()\n    model.add(Embedding(vocab_size, 10, input_length=1))\n    model.add(LSTM(50))\n    model.add(Dense(vocab_size, activation='softmax'))\n    # compile network\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # summarize defined model\n    model.summary()\n    plot_model(model, to_file='model.png', show_shapes=True)\n    return model","df2122e9":"model = define_model(vocab_size) # build the model","ad4d8145":"model.fit(X, y, epochs=500, verbose=2) # fit the model","a5462ebc":"# evaluate\nin_text = 'Jack'\nprint(in_text)\nencoded = tokenizer.texts_to_sequences([in_text])[0]\nencoded = np.array(encoded)\nyhat = model.predict_classes(encoded, verbose=0)\nfor word, index in tokenizer.word_index.items():\n    if index == yhat:\n        print(word)","f1b03ab7":"# generate a sequence from the model\ndef generate_seq(model, tokenizer, seed_text, n_words):\n    in_text, result = seed_text, seed_text\n    # generate a fixed number of words\n    for _ in range(n_words):\n        # encode the text as integer\n        encoded = tokenizer.texts_to_sequences([in_text])[0]\n        encoded = np.array(encoded)\n        # predict a word in the vocabulary\n        yhat = model.predict_classes(encoded, verbose=0)\n        # map predicted word index to word\n        out_word = ''\n        for word, index in tokenizer.word_index.items():\n            if index == yhat:\n                out_word = word\n                break\n    # append to input\n        in_text, result = out_word, result + ' ' + out_word\n    return result","8aa4609d":"print(generate_seq(model, tokenizer, 'Jack', 6))","2478638f":"# create line-based sequences\nsequences = list()\nfor line in data.split('\\n'):\n    encoded = tokenizer.texts_to_sequences([line])[0]\n    for i in range(1, len(encoded)):\n        sequence = encoded[:i+1]\n        sequences.append(sequence)\nprint('Total Sequences: %d' % len(sequences))","1f78d8e8":"from keras.preprocessing.sequence import pad_sequences\n#pad input sequences\nmax_length = max([len(seq) for seq in sequences])\nsequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\nprint('Max Sequence Length: %d' % max_length)","e0cc3aa8":"# split into input and output elements\nsequences = np.array(sequences)\nX, y = sequences[:,:-1],sequences[:,-1]\ny = to_categorical(y, num_classes=vocab_size)","69150331":"# define the model\ndef define_model(vocab_size, max_length):\n    model = Sequential()\n    model.add(Embedding(vocab_size, 10, input_length=max_length-1))\n    model.add(LSTM(50))\n    model.add(Dense(vocab_size, activation='softmax'))\n    # compile network\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # summarize defined model\n    model.summary()\n    plot_model(model, to_file='model.png', show_shapes=True)\n    return model","e82513c8":"# generate a sequence from a language model\ndef generate_seq(model, tokenizer, max_length, seed_text, n_words):\n    in_text = seed_text\n    # generate a fixed number of words\n    for _ in range(n_words):\n        # encode the text as integer\n        encoded = tokenizer.texts_to_sequences([in_text])[0]\n        # pre-pad sequences to a fixed length\n        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n        # predict probabilities for each word\n        yhat = model.predict_classes(encoded, verbose=0)\n        # map predicted word index to word\n        out_word = ''\n        for word, index in tokenizer.word_index.items():\n            if index == yhat:\n                out_word = word\n                break\n        # append to input\n        in_text += ' ' + out_word\n    return in_text","f0f9702f":"# define model\nmodel = define_model(vocab_size, max_length)\n# fit network\nmodel.fit(X, y, epochs=500, verbose=2)","61d194cf":"# evaluate model\nprint(generate_seq(model, tokenizer, max_length-1, 'Jack', 4))\nprint(generate_seq(model, tokenizer, max_length-1, 'Jill', 4))","19186e0e":"# generate a sequence from a language model\ndef generate_seq(model, tokenizer, max_length, seed_text, n_words):\n    in_text = seed_text\n    # generate a fixed number of words\n    for _ in range(n_words):\n        # encode the text as integer\n        encoded = tokenizer.texts_to_sequences([in_text])[0]\n        # pre-pad sequences to a fixed length\n        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n        # predict probabilities for each word\n        yhat = model.predict_classes(encoded, verbose=0)\n        # map predicted word index to word\n        out_word = ''\n        for word, index in tokenizer.word_index.items():\n            if index == yhat:\n                out_word = word\n                break\n                # append to input\n        in_text += ' ' + out_word\n    return in_text","e5df9f06":"# tokenize the data\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts([data])\nencoded = tokenizer.texts_to_sequences([data])[0]","1ec93611":"sequences = list()\nfor i in range(2, len(encoded)):\n    sequence = encoded[i-2:i+1]\n    sequences.append(sequence)\nprint('Total Sequences: %d' % len(sequences))","de7d3c4b":"# pad sequences\nmax_length = max([len(seq) for seq in sequences])\nsequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\nprint('Max Sequence Length: %d' % max_length)","5839de40":"# split into input and output elements\nsequences = np.array(sequences)\nX, y = sequences[:,:-1],sequences[:,-1]\ny = to_categorical(y, num_classes=vocab_size)","b73e651b":"# define model\nmodel = define_model(vocab_size, max_length)\n# fit network\nmodel.fit(X, y, epochs=500, verbose=2)","b00e460c":"# evaluate model\nprint(generate_seq(model, tokenizer, max_length-1, 'Jack and', 5))\nprint(generate_seq(model, tokenizer, max_length-1, 'And Jill', 3))\nprint(generate_seq(model, tokenizer, max_length-1, 'fell down', 5))\nprint(generate_seq(model, tokenizer, max_length-1, 'pail of', 5))","9bc91398":"We will fit our model to predict a probability distribution across all words in the vocabulary. \n\nThat means that we need to turn the output element from a single integer into a `one hot encoding` with a 0 for every word in the vocabulary and a 1 for the actual word that the value.\n\nThis gives the network a ground truth to aim for from which we can calculate error and update the model. \n\nKeras provides the `to_categorical()` function that we can use to convert the integer to a `one hot encoding` while specifying the number of classes as the vocabulary size.","83706ab8":"Running this example, we can see that the size of the vocabulary is 21 words. \n\nWe add one,because we will need to specify the integer for the largest encoded word as an array index, e.g. words encoded 1 to 21 with array indicies 0 to 21 or 22 positions. \n\nNext, we need to create sequences of words to fit the model with one word as input and one word as output.","3bbf6da3":"Running this piece shows that we have a total of 24 input-output pairs to train the network.\n\nWe can then split the sequences into input (X) and output elements (y). This is straightforward as we only have two columns in the data.","21e9f052":"Running the example again gets a good fit on the source text at around 95% accuracy.\n\n`Note:` Given the stochastic nature of neural networks, your specific results may vary. Consider running the example a few times.\n\nThe first start of line case generated correctly, but the second did not. \n\nThe second case was an example from the 4th line, which is ambiguous with content from the first line.Perhaps a further expansion to 3 input words would be better. The two mid-line generation examples were generated correctly, matching the source text.\n\nWe can see that the choice of how the language model is framed and the requirements on how the model will be used must be compatible. That careful design is required when using language models in general, perhaps followed-up by spot testing with sequence generation to\nconfirm model requirements have been met.","1802a344":"# Model 2: Line-by-Line Sequence","4c489e7f":"We will need to know the `size of the vocabulary` later for both defining the `word embedding layer` in the model, and for encoding output words using a `one hot encoding.` \n\nThe size of the vocabulary can be retrieved from the trained Tokenizer by accessing the `word index` attribute.","d3a6aecb":"Next, we can `pad` the prepared sequences. We can do this using the `pad_sequences()` function provided in Keras. \n\nThis first involves finding the longest sequence, then using that as the length by which to pad-out all other sequences.","86e43f83":"We will use this same general network structure for each example in this tutorial, with minor changes to the learned embedding layer. \n\nWe can compile and fit the network on the encoded text data. \n\nTechnically, we are modeling a `multiclass classification` problem (predict the word in the vocabulary), therefore using the `categorical cross entropy` loss function. \n\nWe use the efficient`Adam` implementation of gradient descent and track accuracy at the end of each epoch.\n\nThe model is fit for 500 training epochs.\n\nAfter the model is fit, we test it by passing it a given word from the vocabulary and having the model predict the next word. \n\nHere we pass in \"Jack\" by encoding it and calling `model.predict classes()` to get the integer output for the predicted word. This is then looked up in the vocabulary mapping to give the associated word.","4e2e5d57":"# Model 3: Two-Words-In, One-Word-Out Sequence\n\nWe can use an intermediate between the `one-word-in` and the `whole-sentence-in` approaches and pass in a `sub-sequences` of words as input. This will provide a trade-off between the two framings allowing new lines to be generated and for generation to be picked up mid line. \n\nWe will use `3 words` as input to predict one word as output. The preparation of the sequences is much like the first example, except with different offsets in the source sequence arrays, as follows:","adafbdea":"This process could then be repeated a few times to build up a generated sequence of words.\n\nTo make this easier, we wrap up the behavior in a function that we can call by passing in our model and the seed word.","db1ecede":"# Word-Based Neural Language Model\n### Introduction\nLanguage modeling involves predicting the next word in a sequence given the sequence of words already present. \n\nA language model is a key element in many natural language processing models such as machine translation and speech recognition. The choice of how the language model is framed must match how the language model is intended to be used. \n\nIn this tutorial, we will discover how the framing of a language model affects the skill of the model when generating short sequences from a nursery rhyme.\n\nLanguage models both learn and predict one word at a time. The training of the network involves providing sequences of words as input that are processed one at a time where a prediction can be made and learned for each input sequence. Similarly, when making predictions, the process can be seeded with one or a few words, then predicted words can be gathered and presented as input on subsequent predictions in order to build up a generated output sequence\n\nTherefore, each model will involve splitting the source text into input and output sequences, such that the model can learn to predict words. \n\nThere are many ways to frame the sequences from a source text for language modeling. In this tutorial, we will explore 3 different ways of developing word-based language models in the Keras deep learning library. There is no single best approach, just different framings that may suit different applications.","b5fb3ffd":"This approach may allow the model to use the context of each line to help the model in those cases where a simple one-word-in-and-out model creates ambiguity. \n\nIn this case, this comes at the cost of predicting words across lines, which might be fine for now if we are only interested in modeling and generating lines of text. \n\nNote that in this representation, we will require a padding of sequences to ensure they meet a fixed length input. This is a requirement when using Keras. \n\nFirst, we can create the sequences of integers, line-by-line by using the Tokenizer already fit on the source text.","6c292c1e":"Next, we can split the sequences into input and output elements, much like before.","0dc6c3f2":"At the end of the run, we generate two sequences with different seed words: `Jack and Jill`.\n\nThe first generated line looks good, directly matching the source text. \n\nThe second is a bit strange. This makes sense, because the network only ever saw `Jill` within an input sequence,\nnot at the beginning of the sequence, so it has forced an output to use the word `Jill`, i.e. the last line of the rhyme.","ee128ed3":"Another approach is to split up the source text line-by-line, then break each line down into a series of words that build up. \n\nFor example:\n\n![image.png](attachment:image.png)","490f8b4d":"The model can then be defined as before, except the input sequences are now longer than a single word. \n\nSpecifically, they are max_length-1 in length, -1 because when we calculated the maximum length of sequences, they included the input and output elements.","b929e75c":"We can use the model to generate new sequences as before. \n\nThe `generate_seq()` function can be updated to build up an input sequence by adding predictions to the list of input words\neach iteration.","0bb52382":"# Model 1: One-Word-In, One-Word-Out Sequences\n\nWe can start with a very simple model. Given one word as input, the model will learn to predict\nthe next word in the sequence. For example:\n![image.png](attachment:image.png)\n\nThe first step is to encode the text as integers. Each lowercase word in the source text is assigned a unique integer and we can convert the sequences of words to sequences of integers. \n\nKeras provides the `Tokenizer` class that can be used to perform this encoding. \n\nFirst, the Tokenizer is fit on the source text to develop the mapping from words to unique integers. Then sequences of text can be converted to sequences of integers by calling the `texts to sequences()` function.","8922fd1c":"We are now ready to define the neural network model. \n\nThe model uses a 'learned word embedding' in the input layer. \n\nThis has one real-valued vector for each word in the vocabulary, where each word vector has a specified length. \n\nIn this case we will use a 10-dimensional projection. \n\nThe input sequence contains a single word, therefore the input length=1. \n\nThe model has a `single hidden LSTM layer with 50 units`. This is far more than is needed. \n\nThe output layer is comprised of one neuron for each word in the vocabulary and uses a `softmax activation` function to ensure the output is normalized to look like a probability."}}