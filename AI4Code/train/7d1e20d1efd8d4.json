{"cell_type":{"b7e02666":"code","c785bd27":"code","86187a86":"code","68b0ba78":"code","684d3a17":"code","a8b6b94d":"code","84761208":"code","e57bc444":"code","36d5cf68":"code","e6461869":"code","38705d60":"code","d252ba92":"code","89dc8f13":"code","77df66df":"code","96bb000c":"markdown","c0ec4d86":"markdown","801ed233":"markdown","156058d1":"markdown","698daf57":"markdown","65fb38c4":"markdown","5cbccf56":"markdown","d780b05d":"markdown","48cde259":"markdown","b7ddb596":"markdown","5fa29f4e":"markdown","682ef4a6":"markdown","42760205":"markdown","734e4d22":"markdown","1b5fc9bf":"markdown","9f262b8c":"markdown","4833bc41":"markdown","e3e5cc1f":"markdown","9bf93cb9":"markdown","7d3a1b9e":"markdown","b6d1467d":"markdown"},"source":{"b7e02666":"#IMPORTING REQUIRED MODULES\nimport pandas as pd\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\npd.set_option('display.float_format', lambda x: '%.6f' % x)\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nplt.style.use('ggplot')\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom IPython.display import Image","c785bd27":"df = pd.read_csv('..\/input\/train.csv')\ndf.columns = [x.lower() for x in df.columns]\n\n#MINI FEATURE ENGINEERING\n#transforming gender into numerical values\ndf['sex'].replace(['female','male'], [1,0],inplace=True)\n#replacing nan age values by mean age\ndf.loc[np.isnan(df['age']), 'age'] = df['age'].mean()","86187a86":"Image(url= \"http:\/\/www.tikalon.com\/blog\/2011\/sigmoid.gif\")","68b0ba78":"y = 1 - (1 \/ (1 + np.exp(0 + (1*2))))\nround(y,2)","684d3a17":"ind_vars = ['fare', 'age', 'sex']\nXtrain, Xtest, Ytrain, Ytest = train_test_split(df[ind_vars], df['survived'], test_size=0.33)","a8b6b94d":"logreg = LogisticRegression(solver='lbfgs')\nlogreg.fit(Xtrain[['fare']], Ytrain)\ncoef = logreg.coef_[0][0]\nintercept = logreg.intercept_[0]\n'intercept : ', round(intercept, 2), 'coef : ', round(coef, 2),","84761208":"x = Xtrain[['fare']].values[0][0]\npredicted_proba_y = 1- (1 \/ (1 + np.exp(intercept + (coef * x))))\n'fare :', x, 'probability of surviving :', round(predicted_proba_y, 2)","e57bc444":"Yprobas = logreg.predict_proba(Xtrain[['fare']])[:,1]\nXtrain['probas'] = Yprobas\nXtrain[['fare', 'probas']].head()","36d5cf68":"sns.scatterplot(Xtrain['fare'], Ytrain)\nsns.lineplot('fare', 'probas', data=Xtrain.sort_values('fare'));","e6461869":"# y = 1 \/ (1 + np.exp(intercept + (coef * x)))\n# 0.5 = 1 \/ (1 + np.exp(intercept + (coef * x)))\n# 1 + np.exp(intercept + (coef * x)) = 1 \/ 0.5 \n# np.exp(intercept + (coef * x)) = 1\n# intercept + (coef * x) = np.log(1)\n# coef * x = 0 - intercept\n# x = - intercept \/ coef\nx = - intercept \/ coef\nround(x, 2)","38705d60":"sns.scatterplot(Xtrain['fare'], Ytrain)\nsns.lineplot('fare', 'probas', data=Xtrain.sort_values('fare'));\nplt.axvline(x, 0, 1, color='black');","d252ba92":"Ypredicted = logreg.predict(Xtrain[['fare']])\nXtrain['prediction'] = Ypredicted\nXtrain[['fare', 'prediction']].head(20)","89dc8f13":"'accuracy', round(logreg.score(Xtrain[['fare']], Ytrain), 2)","77df66df":"ind_vars = ['sex', 'age', 'fare']\nlogreg.fit(Xtrain[ind_vars], Ytrain)\ncoefs = pd.DataFrame(logreg.coef_).transpose()\ncoefs.index = ind_vars\ncoefs.columns = ['coef']\ncoefs.abs().sort_values('coef', ascending=False)","96bb000c":"Here we see that compared to Age, Fare was a really bad predictor for Surviving.<br>\nBut for our defense, the tutorial would have been more confusing with a binary variable predicting another binary variable.","c0ec4d86":"Logistic Regression is a model that uses a special function (the sigmoid function) to predict a binary variable (variable that has only two outcomes, 0 or 1).<br>\nIn other words, it intents to predict the greatest number of correct Y values based on X values, given the constraint that the relationship between X and Y should look like this : <br>","801ed233":"The algorithm is telling us that given the Fare variable, the best way to predict whether a passenger survived or not is using the logistic equation with the above intercept and coefficient.<br>\nLet's try computing the probability of surviving with the first Fare value of the dataset.","156058d1":"Let's imagine a use case that we can apply to this theorical exemple.<br>\nWe pretend we work in a bank, and we kept record of all loans being accepted or not (dependant variable) given the amount of money on a bank account (independant variable).<br>\nFor exemple, one observation says that with -2$ on its account, one individual was refused a loan.<br>\nNow we'd like to know the probability, for each observation in the dataset, of obtaining a loan given the amount of money in the bank account.<br><br>\nWe therefore feed a Logistic Regression model with the dataset.<br>\nIt finds that the best way to correctly predict the outcome is to run the sigmoid equation with an intercept of 0 and coefficient of 1.<br>\nSo for instance, if you have 2 dollars on your account, the algorithm says you'll have 88% chance of obtaining a loan.","698daf57":"The goal of this brief tutorial is to understand the logic behind Logistic Regression.<br><br>\nWe'll first try to understand the theory before applying it to the Titanic dataset.<br>\nWe'll do mini feature-engineering, just enough to be able to treat the data.","65fb38c4":"Now that we have all predicted probabilities, we can plot our own sigmoid curve","5cbccf56":"If we run the equation for every value between -8 and 8, we'll come up with the above graph.<br>\nThe x-axis represents the amount of money on the bank account, the y-axis the probability of obtaining a loan.","d780b05d":"It doesn't look as good as the graph from the introduction, but it will do the job.","48cde259":"However, we intuitively observe that the algorithm is not doing that great of a job.<br>\nFor instance, a lot of points are on the top left part of the graph, meaning they correspond to surviving passengers, though the algorithm did think their life was going to end (Type I Error).<br>\nWe can actually get the accuracy of the algorithm, meaning the percentage of observations it correctly predicted : ","b7ddb596":"Now if the probabilty of a predicted value is over 50%, the regression predicts 1 as the outcome.<br>\nIf the probabilty of a predicted value is under 50%, the regression predicts 0 as the outcome.<br>\nSo basically, we need to find the x value for which y = 0.5 : all points on its left will predict the outcome of not surviving, all the ones on the right will predict the outcome of surviving.<br><br>\nLet's find this x value and draw a vertical line on it.<br>\nWe need to some basic maths to reverse the original equation :<br>","5fa29f4e":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Theory\" data-toc-modified-id=\"Theory-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Theory<\/a><\/span><\/li><li><span><a href=\"#Practice\" data-toc-modified-id=\"Practice-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Practice<\/a><\/span><\/li><\/ul><\/div>","682ef4a6":"Here it is : the algorithm predicts life for passengers whose ticket price were over 65.<br><br>\nBut there's also a function to get all the predicted outcomes for each observation at once :<br>","42760205":"# Theory","734e4d22":"So to summarize, when fitting a Logistic Regression model, we give the algorithm some known Y values and X values.<br>\nThen, the algorithm considers the X values and runs the sigmoid equation.<br>\nIt tunes the intercept and coefficients values, until it comes up with their optimum, meaning the values that will <b>correctly predict the most Y values<\/b>.<br>\nThe resulting sigmoid curve si designed so that there are no other curves out there capable of correctly predicting more Y values.","1b5fc9bf":"Now let's run the Logistic Regression on the train dataset, considering for now only the \"fare\" variable.<br>","9f262b8c":"Note that the equation of the above graph is the following : <br>\ny = 1 - (1 \/ (1 + exp(0 + (1 * x))))<br>\nIndeed, this is a theorical case, where intercept is of 0 and coefficient of 1 <br>","4833bc41":"# Practice","e3e5cc1f":"Of course, there's a built-in function to get all the predicted probabilites for each values :","9bf93cb9":"...or mathematically speaking, given the constraint that y should always be the result of this equation :<br>\ny = 1 - (1 \/ (1 + exp(intercept + (coef * x))))<br><br>","7d3a1b9e":"Also, in order to improve the accuracy of the regression, we can bring in more values. <br>\nThis means running a Multiple Logistic Regression.<br>\nNow the equation will have to compute a coefficient for each of the variable.<br>\nThe cool thing is that the coefficients computed will give us an idea of the influence of each variable on the independant variable.<br>\nThe more its absolute value is high, the more impact it has on the dependant variable.","b6d1467d":"Before we jump in, we first split the original dataframe into train and test. <br>(for more explanation about this, you can check out my [tutorial on Linear Regression]('https:\/\/www.kaggle.com\/duverj\/tutorial-linear-regression')), \"Train and Test\" section. <br>"}}