{"cell_type":{"3b1b3e0b":"code","f8010aff":"code","a33a8ef8":"code","9c13ce30":"code","6b955e70":"code","a544ca32":"code","04e96678":"code","2fdfab61":"code","f6ce6cba":"markdown","163cdbab":"markdown","ea749c8d":"markdown","e972b683":"markdown","4d0d6613":"markdown"},"source":{"3b1b3e0b":"import pandas as pd\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Train file path\ntrain_file_path = '..\/input\/train.csv'\n\nhome_data = pd.read_csv(train_file_path)\nprint(\"Original # of columns: {0}\".format(len(home_data.columns)))\n\n# Create y\ny = home_data.SalePrice\n\n# Candidate X\ncols_with_missing = [col for col in home_data.columns \n                                 if home_data[col].isnull().any()]\n\nX_candidate = home_data.drop(['Id','SalePrice'] + cols_with_missing, axis=1)\n\nlow_cardinality_cols = [cname for cname in X_candidate.columns if \n                                X_candidate[cname].nunique() < 10 and\n                                X_candidate[cname].dtype == \"object\"]\nnumeric_cols = [cname for cname in X_candidate.columns if \n                                X_candidate[cname].dtype in ['int64', 'float64']]\n\nmy_cols = low_cardinality_cols + numeric_cols\n\n# One hot encoded\none_hot_encoded_X = pd.get_dummies(X_candidate[my_cols])\nprint(\"# of columns after one-hot encoding: {0}\".format(len(one_hot_encoded_X.columns)))","f8010aff":"# path to file you will use for predictions\ntest_data_path = '..\/input\/test.csv'\n\n# read test data file\ntest_data = pd.read_csv(test_data_path)\ntest_candidate = test_data.drop(['Id'] + cols_with_missing, axis=1)\n\n# one hot encode\ntest_one_hot_encoded = pd.get_dummies(test_candidate[my_cols])\nprint(\"# of columns after one-hot encoding: {0}\".format(len(test_one_hot_encoded.columns)))","a33a8ef8":"X, test_X = one_hot_encoded_X.align(test_one_hot_encoded, join='left', axis=1)\nprint(np.shape(test_X))","9c13ce30":"# Cross Validation using GridSearchCV\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline \n\nxg_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n\nxgb_pipeline = Pipeline([('xgb', xg_model)])\n\nparameters = {'xgb__max_depth': range(2,5), \n              'xgb__min_child_weight': range(1,4)}\n\nclf = GridSearchCV(xgb_pipeline, parameters,\n                   scoring='neg_mean_absolute_error', cv=5)\nclf.fit(X, y)","6b955e70":"print(\"Best Score: {0}\".format(clf.best_score_ * -1))\nprint(clf.best_estimator_)\nprint(clf.best_params_)","a544ca32":"from matplotlib import pyplot as plt\nimport xgboost as xgb\n\nfig, ax = plt.subplots(1,1,figsize=(10,10))\nbest_estimator_model = clf.best_estimator_.named_steps[\"xgb\"]\nxgb.plot_importance(best_estimator_model, max_num_features=15, ax=ax)","04e96678":"xgb.to_graphviz(best_estimator_model, num_trees=1)","2fdfab61":"print(np.shape(test_X))\n#test_X = missing_data(test_X)\n\n# make predictions which we will submit. \ntest_preds = clf.predict(test_X)\n\n# Submission format\noutput = pd.DataFrame({'Id': test_data.Id,\n                       'SalePrice': test_preds})\n\noutput.to_csv('submission.csv', index=False)","f6ce6cba":"# Introduction\nMachine learning competitions are a great way to improve your data science skills and measure your progress. \n\nIn this notebook we will cover following steps:\n1. Feature engineering\n    * Remove columns with missing values\n    * Numerical features\n    * Categorical features with low cardinality - one hot encoded\n2. XGBoost model \n    * Model validation using Training - CV using GridSearchCV\n    * Model hyper parameter tuning using GridSearchCV\n    * Model performance metrics: Mean absolute error\n3. Model Insights\n4. Submission for Kaggle competition","163cdbab":"## 2. XGBoost","ea749c8d":"## 1. Feature Engineering","e972b683":"## 4. Submission","4d0d6613":"## 3. Model Insights"}}