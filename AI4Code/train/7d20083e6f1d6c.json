{"cell_type":{"53e9a022":"code","237eb7d4":"code","fb6e2f2e":"code","654e34a9":"code","4867a34b":"code","5ecb48c1":"code","141e7358":"code","d508df7f":"code","f3eabbf2":"code","d30e218d":"code","b333bde1":"code","5d69b465":"code","a7a1fce4":"code","6889c5e1":"code","656f523c":"code","f19463fa":"code","3d35150c":"code","f0e3b518":"code","8138c7ed":"code","0f147c6c":"code","7edaa989":"code","6635be78":"code","f2d3fa56":"code","559b6980":"code","0b1e9fc1":"markdown","e79e9dda":"markdown","33f3b31d":"markdown","2beb2575":"markdown","66662762":"markdown","ad04c2ea":"markdown","8b9edd0f":"markdown","74224b0a":"markdown","fde8efc8":"markdown","7485da7e":"markdown","f06eb743":"markdown","fab51960":"markdown","9ef061ed":"markdown","405f269f":"markdown","391fa19b":"markdown","ecd883c2":"markdown","ee8736dd":"markdown","9bf06a2c":"markdown","6b2b54bf":"markdown","a6dab461":"markdown","d0e3e796":"markdown","df458798":"markdown","97abf026":"markdown","648b0271":"markdown","0941773a":"markdown","5fda4ca3":"markdown","fb180447":"markdown","89ad32af":"markdown"},"source":{"53e9a022":"# import all the libraries required to read csv and make some modifications in the datset\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom matplotlib.cm import ScalarMappable\nfrom matplotlib import rcParams\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom iso3166 import countries\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\n\n\nrcParams['figure.figsize'] = 10,7.5\nrcParams['figure.dpi'] = 80\n\n","237eb7d4":"df = pd.read_csv('\/kaggle\/input\/covid19-tweets\/covid19_tweets.csv')\ndf.head(5)","fb6e2f2e":"df.info()","654e34a9":"# Let's view the percentage of NaN in each column\n\nmiss_nan = pd.DataFrame()\nmiss_nan['column'] = df.columns\n\nmiss_nan['percent'] = [round(100* df[col].isnull().sum()\/len(df), 2) for col in df.columns]\nmiss_nan = miss_nan.sort_values('percent', ascending = True)\nmiss_nan = miss_nan[miss_nan['percent']>0]\n\n\nsns.barplot(miss_nan['percent'], miss_nan['column'], palette = 'Blues')\nplt.show()","4867a34b":"number_tweets = df['user_name'].value_counts().reset_index()\nnumber_tweets.columns = ['user_name', 'tweets']\n\n\nsns.barplot(x = \"tweets\", y = \"user_name\", data = number_tweets.head(30), palette = 'Blues_r')\nplt.show()","5ecb48c1":"top_users = df.sort_values('user_followers', ascending =  False).drop_duplicates(subset = 'user_name', keep = 'first')\ntop_users = top_users[['user_name', 'user_followers']]\ntop_users = pd.merge(top_users, number_tweets, 'inner')\n\n\n#Normalize the scale to make the color bar on the right of the bar plot\nnorm = plt.Normalize(top_users['tweets'].min(), top_users['tweets'].max())\nsm = plt.cm.ScalarMappable(cmap=\"Blues_r\", norm=norm)\nsm.set_array([])\n#Show the barplot with color bar\nax = sns.barplot(x=\"user_followers\", y = \"user_name\", data = top_users.head(20), hue = 'tweets', dodge = False, palette = 'Blues_r')\nax.get_legend().remove()\nax.figure.colorbar(sm)\nplt.show()\n","141e7358":"device = df['source'].value_counts().reset_index()\ndevice.columns = ['source', 'count']\ndevice['percent_tweets'] = round(device['count']\/device['count'].sum()*100, 2)\n\n\nsns.barplot(x = \"percent_tweets\", y = \"source\", data = device.head(30), palette = 'Blues_r')\nplt.show()","d508df7f":"df['user_created'] = pd.to_datetime(df['user_created'])\nnew_users = df[['user_created', 'user_name']].drop_duplicates(subset = 'user_name', keep = 'first')\nnew_users['user_created']= new_users['user_created'].dt.year\ncount_year = new_users['user_created'].value_counts().reset_index()\ncount_year.columns = ['year', 'number']\ncount_year\n\n#sns.lineplot(x = 'year', y = 'number', data = count_year)\n#A first impression that we can see that some accounts were created in 1970 and obviously this is not real\ncount_year['year'] = count_year[count_year['year']>1990]\nsns.lineplot(x = 'year', y = 'number', data = count_year, marker = 'o')\nplt.xlabel('Year')\nplt.ylabel('Number of New Users')\nplt.show()\n","f3eabbf2":"df['hashtags'] = df['hashtags'].fillna('[]')\ndf['hashtags_count'] = df['hashtags'].apply(lambda x: len(x.split(',')))\ndf.loc[df['hashtags'] == '[]', 'hashtags_count'] = 0\n\n# let's see the number of hashtags used by users\nhashtag_per_user = df[['user_name','hashtags_count']].sort_values('hashtags_count', ascending =  False).drop_duplicates(subset = 'user_name', keep = 'first')\nsns.barplot(x=\"hashtags_count\", y = \"user_name\", data = hashtag_per_user.head(30), palette = 'Blues_r')\nplt.show()","d30e218d":"hashtag_per_user.describe()","b333bde1":"def hashtags_split(x):\n    return str(x).lower().replace('[','').replace(']','').replace(\"'\",'').replace(\" \", '').split(',')\n\nhashtag_tweets = df.copy()\nhashtag_tweets['hashtag'] = hashtag_tweets['hashtags'].apply(lambda row: hashtags_split(row))\nhashtag_tweets = hashtag_tweets.explode('hashtag')\nhashtag_tweets.loc[hashtag_tweets['hashtag'] == '', 'hashtag'] = 'No Hashtag'\nhashtag_tweets.head()","5d69b465":"hashtag_number = hashtag_tweets['hashtag'].value_counts().reset_index()\nhashtag_number.columns = ['hashtag', 'count']\n\nsns.barplot(x=\"count\", y = \"hashtag\", data = hashtag_number.head(10), palette = 'Blues_r')\nplt.show()","a7a1fce4":"text = \"\".join(tweet for tweet in df['text'])\nstopwords = set(STOPWORDS)\nstopwords.update(['https', 't','co', 'many', 's'])\n\nwordcloud = WordCloud(stopwords=stopwords, background_color='white').generate(text)\n\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('Prevalent words for all tweets')\nplt.show()","6889c5e1":"# In this part we gonna use another mask for the wordcloud and different colors\nimport os\nfrom PIL import Image\nfrom scipy.ndimage import gaussian_gradient_magnitude\nfrom wordcloud import ImageColorGenerator\n\nd = os.path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n# load image. This has been modified in gimp to be brighter and have more saturation.\ncovid_color = np.array(Image.open(os.path.join(d, \"..\/input\/images\/2019-nCoV-CDC-23312.jpg\")))\n# subsample by factor of 3. Very lossy but for a wordcloud we don't really care.\ncovid_color = covid_color[::3, ::3]\n\n# create mask  white is \"masked out\"\ncovid_mask = covid_color.copy()\ncovid_mask[covid_mask.sum(axis=2) == 0] = 255\n\nedges = np.mean([gaussian_gradient_magnitude(covid_color[:, :, i] \/ 255., 2) for i in range(3)], axis=0)\ncovid_mask[edges > .08] = 255\nhashtag = \" \".join(hashtag for hashtag in hashtag_tweets['hashtag'])\nstopwords = set(STOPWORDS)\nstopwords.update(['No', 'Hashtag'])\n\nwc = WordCloud(max_words=2000, mask=covid_mask, max_font_size=40, random_state=42, relative_scaling=0, stopwords=stopwords, background_color='white')\n\n# generate word cloud\nwc.generate(hashtag)\n\n\n\n# create coloring from image\nimage_colors = ImageColorGenerator(covid_color)\nwc.recolor(color_func=image_colors)\nplt.figure(figsize=(10, 10))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","656f523c":"vec = TfidfVectorizer(stop_words = 'english')\nvec.fit(df['text'].values)\nfeatures = vec.transform(df['text'].values)","f19463fa":"kmeans = KMeans(n_clusters = 2, random_state = 0)\nkmeans.fit(features)","3d35150c":"res = kmeans.predict(features)\ndf['Cluster'] = res\n","f0e3b518":"text_cluster_1 = \" \".join(tweet for tweet in df[df['Cluster'] == 0]['text'])\nstopwords = set(STOPWORDS)\nstopwords.update(['https', 't','co', 'many', 's'])\n\nwordcloud_1 = WordCloud(max_words = 100, stopwords=stopwords, background_color='white').generate(text_cluster_1)\n\nplt.imshow(wordcloud_1)\nplt.axis('off')\nplt.title('Group of words for the cluster N\u00ba0')\nplt.show()","8138c7ed":"text_cluster_2 = \" \".join(tweet for tweet in df[df['Cluster'] == 1]['text'])\nstopwords = set(STOPWORDS)\nstopwords.update(['https', 't','co', 'many', 's'])\n\nwordcloud_2 = WordCloud(max_words = 100, stopwords=stopwords, background_color='white').generate(text_cluster_2)\n\nplt.imshow(wordcloud_2)\nplt.axis('off')\nplt.title('Group of words for the cluster N\u00ba1')\nplt.show()","0f147c6c":"location = df['user_location'].value_counts().reset_index()\nlocation.columns = ['user_location', 'count']\nlocation = location[location['user_location'] != 'NA']\nlocation = location.sort_values(['count'], ascending = False)\n\nsns.barplot(x=\"count\", y = \"user_location\", data = location.head(30), palette = 'Blues_r')\nplt.show()","7edaa989":"!pip install geopandas\n","6635be78":"import pycountry\n\ndef alpha3code(column):\n    CODE = []\n    for country in column:\n        try:\n            code = pycountry.countries.get(name=country).alpha_3\n            CODE.append(code)\n        except:\n            CODE.append('None')\n    return CODE\n\n\nlocation['CODE'] = alpha3code(location['user_location'])\nlocation.head()","f2d3fa56":"import geopandas\nworld = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\nworld.columns = ['pop_est', 'continent', 'name', 'CODE', 'gdp_md_est', 'geometry']\nworld_merge = pd.merge(world, location, on='CODE')\n\nlocation_merge = pd.read_csv('..\/input\/latlong\/countries_latitude_longitude.csv')\nworld_merge = world_merge.merge(location_merge, on='name').sort_values(by='count', ascending=False).reset_index()\nworld_merge = world_merge[['user_location', 'count', 'latitude','longitude']]\nworld_merge.head()","559b6980":"import folium\nfrom folium import plugins\nfrom folium.plugins import HeatMap\n\nfolium_map = folium.Map(location=[50,0],\n                       zoom_start=3,\n                       tiles='CartoDB dark_matter')\n\nworld_merge['latitude']=world_merge['latitude'].fillna(0)\nworld_merge['longitude']=world_merge['longitude'].fillna(0)\n\nplugins.FastMarkerCluster(data=list(zip(world_merge['latitude'].values, world_merge['longitude'].values))).add_to(folium_map)\narr = world_merge[['latitude', 'longitude']].values\n\nHeatMap(arr, radius = 15).add_to(folium_map)\n\nfolium.LayerControl().add_to(folium_map)\nfolium_map","0b1e9fc1":"<p style=\"text-align:justify; font-size: 18px\">\nAs we can see in this graph, the sources or devices which were mostly used for tweeting, were \"Twitter Web App\", \"Twitter for Android\" and \"Twitter for iPhone\", these 3 sources represent approximately 74% of all the tweets posted; which correspond to the most traditional ways. Also, other sources were: \"Blood Donors India\", \"Zoho Social\", and others.\n    <\/p>\n<p style=\"text-align:justify; font-size: 18px\">\n    Now let's look, if this year the number of new users was increased.\n    <\/p>\n","e79e9dda":"<p style=\"text-align:justify; font-size: 18px\">\n    As we can see, the column \"hashtags\" have almost 30% of NaN values, and also we have \"use_lcoation\", and \"user_description\", this can be due some users didn't use the hashtag in their post and also some people don't have their profile complete, maybe doesn't use Twitter very often; however, the column \"source\" almost doesn't have NaN values. \n    <\/p>","33f3b31d":"<p style=\"text-align:justify; font-size: 18px\">\n    We have extracted some insights of the information collected in all tweets about COVID19, the next steps will be to aggregate some machine learning prediction, tune the clustering algorithm and finally optimize the extraction of latitude and longitude for each city or country. \n    <\/p>\n    \n<p style=\"text-align:justify; font-size: 18px\">\n    See You Soon!! :)\n    <\/p>","2beb2575":"<p style=\"text-align:justify; font-size: 18px\">\n    Now we gonna analyze the text information within hashtags that have been used in the tweets during this pandemic. As we know, in the before section, we have seen the column \"hashtag\" has a lot of NaN values, so the first step we gonna make converts these NaN values to something, and then we can extract some insights and create a wordcloud.\n    <\/p>","66662762":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#3E8FCE;\n           font-size:200%;\n           font-family:Verdana;\n           letter-spacing:1px\">\n\n<p style=\"padding: 40px;\n              color:white;\n          text-align:center\"> Navigation\n    <i class=\"fa fa-search icon\"><\/i>\n \n<\/p>\n<\/div>","ad04c2ea":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#3E8FCE;\n           font-size:200%;\n           font-family:Verdana;\n           letter-spacing:1px\">\n\n\n<p style=\"padding: 40px;\n              color:white;\n          text-align:center\"> \n    Dataset Overview\n    <i class=\"fa fa-database icon\"><\/i>\n   \n \n<\/p>\n<\/div>","8b9edd0f":"<p style=\"text-align:justify; font-size: 18px\">\n    In these two graphs, we can distinguish two sentimental groups, cluster 0 one is apparently is more negative because group all the texts with a topic about people who were positive in the test of COVID19, and the second group is more neutral information and talk about prevention, we can say is more optimistic.\n    <\/p>\n","74224b0a":"<p style=\"text-align:justify; font-size: 18px\">\n    As we can see, CNN as much as National Geographic has a big amount of followers but this account doesn't have published much about COVID19, as the difference of China Xinhua News which had posted a lot of tweets about this topic, and CGTN and Hindustan Times which are Asian accounts; all these accounts don't have a lot of followers in comparison of CNN and others. Furthermore, we gonna see the geospatial information.\n    <\/p>\n    <p style=\"text-align:justify; font-size: 18px\">\nNow let's take a look at which is the major \"source\" or device which have used to publish tweets.\n    <\/p>\n    ","fde8efc8":"<div class=\"knitr source\"><img src=\"https:\/\/seeklogo.com\/images\/U\/universidad-catolica-san-pablo-ucsp-logo-5309049584-seeklogo.com.gif\" align = 'right', style = 'position:absolute; top:0; right:0'>\n    <h1><p style=\"text-align:left; font-size: 24px\"> Exploratory Data Analysis (EDA) for tweets of COVID 19<\/p><\/h1>\n    <h2><p style=\"text-align:left; font-size: 20px\"> Universidad Catolica San Pablo<\/p><\/h2>\n  \n<\/div>","7485da7e":"<p style=\"text-align:justify; font-size: 18px\">\n    We gonna see the users by number of tweets\n    <\/p>\n    ","f06eb743":"<p style=\"text-align:justify; font-size: 18px\">\n    As we can see the prevalent word for all tweets is \"COVID19\", followed by words like \"mask\", \"help\", \"people\" and \"pandemic\".\n    <\/p>\n<p style=\"text-align:justify; font-size: 18px\">\n    On another hand, in the case of hashtags, we can see all hashtags follow the same topic as text, with \"Covid19\" or \"coronavirus\" as prevalent words, followed with \"wearmask\" this can be because of the policy which almost every country have adopted about the use of a mask for prevention and \"coronovirusupdate\" maybe this keeps update the number of positive cases and deaths, also we can see hashtags like \"healthcare\", \"trump\", \"lockdown\" and \"socialdistancing\".\n    <\/p>","fab51960":"<p style=\"text-align:justify; font-size: 18px\">\n    In this section we gonna analyze first of all the numerical data. So let's take a look and quick check of the dataset which will be treated\n<\/p>","9ef061ed":"    \n<p style=\"text-align:justify; font-size: 18px\">\n    References:\n        Kostiantyn Isaienkov (2020). Notebook\n    <\/p>","405f269f":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#3E8FCE;\n           font-size:200%;\n           font-family:Verdana;\n           letter-spacing:1px\">\n\n<p style=\"padding: 40px;\n              color:white;\n          text-align:center\"> Data Visualization\n    <i class=\"fa fa-bar-chart icon\"><\/i>\n \n<\/p>\n<\/div>","391fa19b":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#3E8FCE;\n           font-size:200%;\n           font-family:Verdana;\n           letter-spacing:1px\">\n\n<p style=\"padding: 40px;\n              color:white;\n          text-align:center\"> Geospacial Visualization\n    <i class=\"fa fa-rocket icon\"><\/i>\n \n<\/p>\n<\/div>","ecd883c2":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#3E8FCE;\n           font-size:200%;\n           font-family:Verdana;\n           letter-spacing:1px\">\n\n<p style=\"padding: 40px;\n              color:white;\n          text-align:center\"> Text analysis of tweets\n    <i class=\"fa fa-newspaper-o icon\"><\/i>\n \n<\/p>\n<\/div>","ee8736dd":"<img src=\"https:\/\/images.indianexpress.com\/2020\/04\/how-to-use-twitter-amid-covid-19-1.jpg\" align = 'center'>\n<p><i>Presented by: Joaqu\u00edn Antonio Casta\u00f1\u00f3n Vilca<\/i><\/p>\n","9bf06a2c":"<p style=\"text-align:justify; font-size: 18px\">\n    As we can see, India and the United States are countries that contributed to publishing more tweets than other countries.\n    <\/p>","6b2b54bf":"<p style=\"text-align:justify; font-size: 18px\">\n    Now we gonna make a wordcloud of the principal words and topics posted in the different tweets.\n    <\/p>","a6dab461":"<p style=\"text-align:justify; font-size: 18px\">\n    As we can see in this graph, the user called \"ROCAS THE PURPLEKING\" has used 17 different hashtags, in another hand, we can say, 75% of the users use 2 hashtags per tweet.\n    <\/p>\n<p style=\"text-align:justify; font-size: 18px\">\n        Now let's view which hashtag is the most used for the users within their tweets.\n    <\/p>\n","d0e3e796":"<p style=\"text-align:justify; font-size: 18px\">\nThis shows the number of tweets about COVID19 was made, but this doesn't mean that this user has a big amount of followers, so now we gonna extract all accounts with a lot of followers and how many tweets about COVID19 have done.\n    <\/p>","df458798":"<p style=\"text-align:justify; font-size: 18px\">\n    Finally, we gonna see, first of all, the distribution of tweets about COVID19 all over the world, and next this information will be display in some geospatial graph.\n    <\/p>","97abf026":"<p style=\"text-align:justify; font-size: 18px\">\n    <ul style=\"text-align:justify; font-size: 18px\">\n        <li>Dataset Overview<\/li>\n        <li>Data Visualization<\/li>\n        <li>Text analysis of tweets<\/li>\n        <li>Clustering of Sentiment<\/li>\n        <li>Geospacial Visualization<\/li>\n<\/ul>\n    <\/p>\n    ","648b0271":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#3E8FCE;\n           font-size:200%;\n           font-family:Verdana;\n           letter-spacing:1px\">\n\n<p style=\"padding: 40px;\n              color:white;\n          text-align:center\"> Clustering of Sentiment\n    <i class=\"fa fa-smile-o icon\"><\/i>\n \n<\/p>\n<\/div>","0941773a":"<p style=\"text-align:justify; font-size: 18px\">\n    Now, let's figure it how can cluster the text of tweets in two differents groups or sentimental groups.\n    <\/p>","5fda4ca3":"<p style=\"text-align:justify; font-size: 18px\">\n    As we saw, there are some NaN values; so let's view how many are.\n    <\/p>\n    ","fb180447":"<p style=\"text-align:justify; font-size: 18px\">This notebook tries to make an Exploratory Data Analysis of all tweets that have been publishing during this pandemic situation.\nThe objective of this notebook is to get visualization and some insights based on existing features of the data collected in the database called \"covid19-tweets\", also I will try to make a clustering for the words and some geospatial visualization. All this is only for educational purpose, in as much as to cover a diploma assessment of UCSP. So let's start!<\/p>\n\n<p style=\"text-align:justify; font-size: 18px\">\nYou can visit this notebook in Kaggle: <a href='https:\/\/www.kaggle.com\/jcastanonv\/ucsp-covid19-assessment'>https:\/\/www.kaggle.com\/jcastanonv\/ucsp-covid19-assessment<\/a>\n    <\/p>\n","89ad32af":"<p style=\"text-align:justify; font-size: 18px\">\n    As we can see in this graph, the number of new accounts increased this year, we can also see that in the years between 2020 and 2009 there is a valley for approximately 10 years where people have not created many accounts in comparison to 2009, in fact, the number of new accounts per year decreased in this lap of time of 10 years; the sudden increase this year maybe is due for the pandemic and the lockdown in many countries over the world.\n    <\/p>\n<p style=\"text-align:justify; font-size: 18px\">\n    In the next section, we gonna find insights based on text data\n    <\/p>"}}