{"cell_type":{"4b7ea10a":"code","663a5527":"code","cb07b0de":"code","8fcfbc00":"code","c2af0c60":"code","ae050d70":"code","bb629ecc":"code","66c0b6e8":"code","8e27371f":"code","708f057f":"code","5a7c6d9e":"code","0038302e":"code","2de24663":"code","b4a6c105":"code","8c2c3e9f":"code","1d147ab1":"code","c4acfabd":"code","c921f6eb":"code","c4d7adcc":"code","8ea54e82":"code","e5f5078b":"code","672b5a6d":"code","328e3337":"code","862f4a8d":"code","9e8f36db":"code","4747d9b6":"code","79646e76":"code","81045bd9":"code","8f16d330":"code","5bdda408":"code","ce14a890":"code","becd64dd":"code","6825e4ba":"code","20833af1":"code","c49341d2":"code","1066c0d9":"code","35109156":"code","97597116":"code","86b7cc71":"code","0ded0a86":"code","6522691d":"code","b261cd10":"code","8a37b338":"code","1c050717":"code","0798f4fe":"markdown","53567779":"markdown","95296bf7":"markdown","e465ac31":"markdown","2daa5916":"markdown","10f8af15":"markdown","f3c4ddb1":"markdown","463312f5":"markdown","8f7496aa":"markdown","4aa29264":"markdown","d1eb1a6f":"markdown","56601fbe":"markdown","8716f733":"markdown","d6d15a74":"markdown","25b94868":"markdown","e20b8d10":"markdown","a92693d0":"markdown","ebfb1c7b":"markdown","441263dd":"markdown","24c4d654":"markdown","93848e2e":"markdown","3ef9a51d":"markdown","955b992d":"markdown","80085885":"markdown","6c231e3e":"markdown","2f5652da":"markdown","62faacd6":"markdown","b0899cd5":"markdown","739c1665":"markdown","99aa9e62":"markdown","f068e78c":"markdown","9c11b7e5":"markdown","a7f732cf":"markdown","a5b6147e":"markdown","8199e792":"markdown","27965485":"markdown","348f0766":"markdown","c1719cda":"markdown","3d391c79":"markdown","8d8bf0ed":"markdown","8252e947":"markdown","379ff8c2":"markdown","09adbc9e":"markdown","8f67a5ee":"markdown","1f6b8c7a":"markdown","33938bd7":"markdown","9d66aa8a":"markdown","089469b5":"markdown","791b727f":"markdown","8e5f6f9a":"markdown","f77e7572":"markdown","a7f0e328":"markdown"},"source":{"4b7ea10a":"! pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html","663a5527":"import torchvision\nfrom torchvision import datasets\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as T\nimport os\nfrom tqdm.notebook import tqdm\nimport torch.optim as optim\nimport torch\nfrom PIL import Image","cb07b0de":"def create_cifar10_dataset(train_transforms, valid_transforms):\n    \"\"\" Creates CIFAR10 train dataset and a test dataset. \n    Args: \n    train_transforms: Transforms to be applied to train dataset.\n    test_transforms: Transforms to be applied to test dataset.\n    \"\"\"\n    # This code can be re-used for other torchvision Image Dataset too.\n    train_set = torchvision.datasets.CIFAR10(\n        \".\/data\", download=True, train=True, transform=train_transforms\n    )\n\n    valid_set = torchvision.datasets.CIFAR10(\n        \".\/data\", download=True, train=False, transform=valid_transforms\n    )\n\n    return train_set, valid_set\n","8fcfbc00":"# Train and validation Transforms which you would like\ntrain_transforms = T.Compose([T.ToTensor(), T.Normalize((0.5,), (0.5,))])\nvalid_transforms = T.Compose([T.ToTensor(), T.Normalize((0.5,), (0.5,))])","c2af0c60":"# For list of supported models use timm.list_models\nMODEL_NAME = \"resnet18\"\nNUM_CLASSES = 10\nIN_CHANNELS = 3\n\nUSE_TORCHVISION = False  # If you need to use timm models set to False.\n\n# USE_TORCHVISION = True # Should use Torchvision Models or timm models\nPRETRAINED = True  # If True -> Fine Tuning else Scratch Training\nEPOCHS = 3\nTRAIN_BATCH_SIZE = 512  # Training Batch Size\nVALID_BATCH_SIZE = 512  # Validation Batch Size\nNUM_WORKERS = 4  # Workers for training and validation\n\nEARLY_STOPPING = True  # If you need early stoppoing for validation loss\nSAVE_PATH = \"{}.pt\".format(MODEL_NAME)\n\nMOMENTUM = 0.8  # Use only for SGD\nLEARNING_RATE = 1e-3  # Learning Rate\nSEED = 42\n","ae050d70":"train_dataset, valid_dataset = create_cifar10_dataset(train_transforms, valid_transforms)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, TRAIN_BATCH_SIZE, shuffle=True, num_workers=4)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset, VALID_BATCH_SIZE, shuffle=False, num_workers=2)","bb629ecc":"# This simply instantiates the torchvision model which is pretrained.\n# This is normal PyTorch CNN model as you would create. Nothing new here, you can replace this with your own CNN too.\nmodel = torchvision.models.resnet18(pretrained=True)\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, NUM_CLASSES)","66c0b6e8":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.is_available():\n    print(\"Model Created. Moving it to CUDA\")\nelse:\n    print(\"Model Created. Training on CPU only\")\n\nmodel.to(device)\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\ncriterion = nn.CrossEntropyLoss()","8e27371f":"for epoch in tqdm(range(EPOCHS)):\n    model.train()\n    print(f\"Started epochs: {epoch}\")\n    for batch_idx, (inputs, target) in enumerate(train_loader):\n        inputs = inputs.to(device)\n        target = target.to(device)\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        output = model(inputs)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n    \n    print(f\"Finished epochs: {epoch}\")\n","708f057f":"from torch.cuda import amp","5a7c6d9e":"# Creates model and optimizer in default precision\nmodel = model.to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# Creates a GradScaler once at the beginning of training.\nscaler = amp.GradScaler()","0038302e":"for epoch in tqdm(range(EPOCHS)):\n    model.train()\n    print(f\"Started epochs: {epoch}\")\n    for batch_idx, (inputs, target) in enumerate(train_loader):\n        inputs = inputs.to(device)\n        target = target.to(device)\n        optimizer.zero_grad()\n        with amp.autocast():\n            output = model(inputs)\n            loss = criterion(output, target)\n            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n            # Backward passes under autocast are not recommended.\n            # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n            scaler.scale(loss).backward()\n\n            # scaler.step() first unscales the gradients of the optimizer's assigned params.\n            # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n            # otherwise, optimizer.step() is skipped.\n            scaler.step(optimizer)\n\n            # Updates the scale for next iteration.\n            scaler.update()\n\n    print(f\"Finished epochs: {epoch}\")\n","2de24663":"import torch\nimport torchvision.models as models\nimport torch.autograd.profiler as profiler","b4a6c105":"model = models.resnet18()\ninputs = torch.randn(5, 3, 224, 224)\nwith profiler.profile(profile_memory=True, record_shapes=True) as prof:\n    model(inputs)","8c2c3e9f":"# NOTE: some columns were removed for brevity\nprint(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))","1d147ab1":"# Some more statistics can be collected.\n# print(prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=10))","c4acfabd":"# print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))","c921f6eb":"model1 = torchvision.models.segmentation.fcn_resnet50(pretrained=True)\nmodel2 = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)","c4d7adcc":"from torchvision.models.resnet import resnet18\nfrom torch.quantization import get_default_qconfig, quantize_jit","8ea54e82":"model = torchvision.models.resnet18(pretrained=True)\n_ = model.eval()","e5f5078b":"ts_model = torch.jit.script(model).eval() # ts_model = torch.jit.trace(float_model, input)","672b5a6d":"qconfig = get_default_qconfig('fbgemm')\nqconfig_dict = {'': qconfig}","328e3337":"def calibrate(model, data_loader):\n    model.eval()\n    with torch.no_grad():\n        for image, target in data_loader:\n            model(image)","862f4a8d":"# I am using the same valid_loader we created for CIFAR10 example in first section\n# It's slightly big, so it can take some time\nquantized_model = quantize_jit(ts_model, {'': qconfig}, calibrate, [valid_loader], inplace=False, debug=False)","9e8f36db":"# print(quantized_model.graph)","4747d9b6":"# We can see that model is much smaller approximately 3x\ngraph_mode_model_file = 'resnet18_graph_mode_quantized.pt'\norignal_model = \"resnet18_unquantized.pt\"\ntorch.jit.save(quantized_model, graph_mode_model_file)\ntorch.jit.save(ts_model, orignal_model)","79646e76":"quantized_model = quantize_jit(ts_model, {'': qconfig}, calibrate, [valid_loader], inplace=False, debug=True)","81045bd9":"import torch.quantization\nimport torch.quantization._numeric_suite as ns\nfrom torch.quantization import default_eval_fn, default_qconfig, quantize","8f16d330":"# Quantize is false here\nfloat_model = torchvision.models.quantization.resnet18(pretrained=True, quantize=False)\n\nfloat_model.to('cpu')\nfloat_model.eval()\nfloat_model.fuse_model()\nfloat_model.qconfig = torch.quantization.default_qconfig","5bdda408":"img_data = [(torch.rand(2, 3, 10, 10, dtype=torch.float), torch.randint(0, 1, (2,), dtype=torch.long)) for _ in range(2)]","ce14a890":"# Here is the quantized model\nqmodel = quantize(float_model, default_eval_fn, img_data, inplace=False)","becd64dd":"wt_compare_dict = ns.compare_weights(float_model.state_dict(), qmodel.state_dict())","6825e4ba":"print('keys of wt_compare_dict:')\nprint(wt_compare_dict.keys())\n\nprint(\"\\nkeys of wt_compare_dict entry for conv1's weight:\")\nprint(wt_compare_dict['conv1.weight'].keys())\nprint(wt_compare_dict['conv1.weight']['float'].shape)\nprint(wt_compare_dict['conv1.weight']['quantized'].shape)","20833af1":"def compute_error(x, y):\n    Ps = torch.norm(x)\n    Pn = torch.norm(x-y)\n    return 20*torch.log10(Ps\/Pn)\n\nfor key in wt_compare_dict:\n    print(key, compute_error(wt_compare_dict[key]['float'], wt_compare_dict[key]['quantized'].dequantize()))","c49341d2":"import matplotlib.pyplot as plt","1066c0d9":"f = wt_compare_dict['conv1.weight']['float'].flatten()\nplt.hist(f, bins = 100)\nplt.title(\"Floating point model weights of conv1\")\nplt.show()\n\nq = wt_compare_dict['conv1.weight']['quantized'].flatten().dequantize()\nplt.hist(q, bins = 100)\nplt.title(\"Quantized model weights of conv1\")\nplt.show()","35109156":"import torchvision\nimport time","97597116":"net = torchvision.models.resnet18(pretrained=True)","86b7cc71":"# You can try torch.jit.trace() \nscripted_net = torch.jit.script(net)","0ded0a86":"# We Set it to eval mode and we freeze the net\nscripted_net.eval()\nfrozen_net = torch.jit.freeze(scripted_net)","6522691d":"def time_model(model, warmup=1, iter=100):\n    start = time.time()\n    inputs = torch.randn(1, 3, 224, 224) # Image Net single Image\n    _ = model(inputs)\n    end = time.time()\n    print(\"Warm up time: {0:7.4f}\".format(end-start))\n    \n    start = time.time()\n    for i in range(iter):\n        inputs = torch.randn(1, 3, 224, 224) # Image Net single Image\n        net(inputs)\n    end = time.time()\n    print(\"Inference time: {0:7.4f}\".format(end-start))\n","b261cd10":"# Let us compare all the three models\n\n# 1. Orignal resnet model\n\ntime_model(net)","8a37b338":"# 2. Torchscript model without freezing\ntime_model(scripted_net)","1c050717":"# 3. Frozen Torchscript model\ntime_model(frozen_net)","0798f4fe":"#### Freezing the model","53567779":"- We load two models. One is quantized resnet18 model and other is normal PyTorch Model\n\n- Nuemeric Suite will allow us to compare these models.","95296bf7":"## Torchvision New Segmenation models","e465ac31":"Freezing is the process of inlining Pytorch module parameters and attributes\nvalues into the TorchScript internal representation. \n\nParameter and attribute\nvalues are treated as final values and they cannot be modified in the resulting Frozen module.\n\n\nPart of this tutorial was taken from PyTorch repo. [Here](https:\/\/github.com\/pytorch\/tutorials\/blob\/master\/prototype_source\/torchscript_freezing.py) is the link for it.\n\n","2daa5916":"- Simply create a GradScaler from `torch.cuda.amp`\n- Change the losses and optimizer with `scaler()`\n\nThat's it it should support mixed precision training !!!","10f8af15":"#### Advantages of graph mode quantization are:\n\n- Simple quantization flow, minimal manual steps\n\n- Unlocks the possibility of doing higher level optimizations like automatic precision selection\n\n- Limitations of graph mode quantization is that quantization is configurable only at the level of module and the set of operators that are quantized is not configurable by user currently.\n","f3c4ddb1":"## Blog Posts about new version of PyTorch\n","463312f5":"- Earlier we required to do this with NVIDIA Apex.\n- Now it is native in PyTorch\n\nAMP allows users to easily enable automatic mixed precision training enabling higher performance and memory savings of up to 50% on Tensor Core GPUs. \n\nUsing the natively supported torch.cuda.amp API, AMP provides convenience methods for mixed precision, where some operations use the torch.float32 (float) datatype and other operations use torch.float16 (half). \n\nSome ops, like linear layers and convolutions, are much faster in float16. Other ops, like reductions, often require the dynamic range of float32. Mixed precision tries to match each op to its appropriate datatype.\n\nYou can find more details in PyTorch [blog post](https:\/\/pytorch.org\/blog\/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision\/) which compares the performances and benhmarks some algorithms\n","8f7496aa":"#### Motivation of Graph Mode Quantization\n\n\n Earlier PyTorch only has eager mode quantization: \n\n We can see there are multiple manual steps involved in the process, including:\n\n\n\n - Explicitly quantize and dequantize activations, this is time consuming when floating point and quantized operations are mixed in a model.\n\n - Explicitly fuse modules, this requires manually identifying the sequence of convolutions, batch norms and relus and other fusion patterns.\n\n - Special handling is needed for pytorch tensor operations (like add, concat etc.)\n\n - Functionals did not have first class support (functional.conv2d and functional.linear would not get quantized)\n\n\n\n Most of these required modifications comes from the underlying limitations of eager mode quantization. \n \n Eager mode works in module level since it can not inspect the code that is actually run (in the forward function), quantization is achieved by module swapping, and we don\u2019t know how the modules are used in forward function in eager mode, so it requires users to insert QuantStub and DeQuantStub manually to mark the points they want to quantize or dequantize. \n\nIn graph mode, we can inspect the actual code that\u2019s been executed in forward function (e.g. aten function calls) and quantization is achieved by module and graph manipulations. \n\nSince graph mode has full visibility of the code that is run, our tool is able to automatically figure out things like which modules to fuse and where to insert observer calls, quantize\/dequantize functions etc., we are able to automate the whole quantization process.\n\n\n","4aa29264":"## Install New Torch\n\n","d1eb1a6f":"PyTorch 1.6 is great. It has done really good additions.\n\n1. Mixed Precision training is made very simple. You don't need Apex now.\n2. Quantization has got attention and lot of new features here.\n3. Model profiling is nice suite that is being developed.\n4. TorchScript again continues to perform extremely well real time deployment enhancement. There are lot of JIT updates as well.\n5. There are lot of updates on Distributed Data Parallel and RPC Framework. I have skipped them here.\n6. Torchvision continues to get new models, new Segmentation models this time. ONNX support for faster RCNN, keypoint RCNN and mask RCNN too.\n7. Complex Number support. Useful for people doing signal \/ audio processing.\n\n","56601fbe":"### Usual Training Loop","8716f733":"### Create the model and optimizer","d6d15a74":"### Step 4\n- Define Calibration Function","25b94868":"### Changes in training loop to include Mixed Precision Training feature","e20b8d10":"## Torchvision Quantization Numeric Suite (Prototype)","a92693d0":"## What's new in PyTorch 1.6","ebfb1c7b":"- Torch now supports a good Profiling tool for models.\n- It is useful for debuggint the model time and other charactersistics.","441263dd":"- An example to show how this works for CNNs.\n- I am using torchvision models only.\n- Basically creating models for transfer learning.","24c4d654":"### Get CIFAR10 Data","93848e2e":"## Final Thoughts about PyTorch 1.6","3ef9a51d":"### Step 5\n- Quantize","955b992d":"The torch.autograd.profiler API now includes a memory profiler that lets you inspect the tensor memory cost of different operators inside your CPU and GPU models.","80085885":"### Step 1\n- Define the model\n- Optionally train it.\n- Here I am forgoing the training procedure, we will just use pre trained weights.","6c231e3e":"\n- Once get ``wt_compare_dict``, users can process this dictionary in whatever way they want. Here as an example we compute the quantization error of the weights of float and quantized models as following.\n\n- Compute the Signal-to-Quantization-Noise Ratio (SQNR) of the quantized tensor ``y``. The SQNR reflects the\n\n- relationship between the maximum nominal signal strength and the quantization error introduced in the quantization. Higher SQNR corresponds to lower quantization error.","2f5652da":"### Step 3\n\nQconfig is the configuration to be provided with torch quantization.\n\nWe have 2 default options here `fbgemm` and `qnnpack`. Let us use `fbgemm` for now.\n\nYou can know more about `qconfig` [here](https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/torch\/quantization\/qconfig.py)\n","62faacd6":"### Step 6\n\n 8. Debugging Quantized Model\n\n We can also use debug option:","b0899cd5":"Let me walk you through an example. This code was adpated from PyTorch repo. [Here]([here](https:\/\/github.com\/pytorch\/tutorials\/blob\/master\/prototype_source\/graph_mode_static_quantization_tutorial.py)) is the Link","739c1665":"- Create the Network","99aa9e62":"- You can have a look at docs of this feature [here](https:\/\/pytorch.org\/docs\/stable\/autograd.html#profiler)","f068e78c":"## Graph Mode Quantization (Prototype)","9c11b7e5":"You can find more details and code for this in over [here](https:\/\/github.com\/pytorch\/tutorials\/blob\/master\/prototype_source\/graph_mode_static_quantization_tutorial.py)\n\nThis is just a brief showcase of how useful this technique can be.\n","a7f732cf":"If not defualt on Kaggle","a5b6147e":"- Two new segmentation models were added `fcn_resnet50` and `deeplabv3_resnet50`\n\n- Earlier we had bigger models for segmentations. These smaller models are trained by PyTorch team and have following characteristics\n","8199e792":"### Step 2\n- Script\/Trace the model\n\nThe input for graph mode quantization is a TorchScript model, so we'll need to either script or trace the model first.\n\n","27965485":"- We can do lot of advanced stuff like Gradient Clipping, L2 Norm Penalty for Gradients.\n\nHave a look at these example [here](https:\/\/pytorch.org\/docs\/stable\/notes\/amp_examples.html)\n\nAlso you can view docs of mixed precision training [here](https:\/\/pytorch.org\/docs\/stable\/amp.html)\n","348f0766":"\n**1. FCN ResNet50**\n\nMetrics: -\n\nmean IoU \t60.5 \t\nglobal pixelwise acc  91.4\n\n\n**2. DeepLabV3 ResNet50**\n\nMetrics: -\n\nmean IoU\t66.4 \t\nglobal pixelwise acc 92.4","c1719cda":"## Automated Mixed Precision Training (Stable)","3d391c79":"## Model Profiling (Beta)","8d8bf0ed":"### Compare the weights of float and quantized models\n\nThe first thing we usually want to compare are the weights of quantized model and float model.\n\nWe can call ``compare_weights()`` from PyTorch Numeric Suite to get a dictionary ``wt_compare_dict`` with key corresponding to module names and each entry is a dictionary with two keys 'float' and 'quantized', containing the float and quantized weights.\n\n``compare_weights()`` takes in floating point and quantized state dict and returns a dict, with keys corresponding to the\n\nfloating point weights and values being a dictionary of floating point and quantized weights","8252e947":"- We can see ``aten::conv2d`` is changed to ``quantized::conv2d`` and the floating point weight has been quantized and packed into an attribute (``quantized._jit_pass_packed_weight_30``), so we don't need to quantize\/pack in runtime.\n\nAlso we can't access the weight attributes anymore after the debug option since they are frozen.","379ff8c2":"### Let's Time it","09adbc9e":"\n- Taken from PyTorch tutorial [here](https:\/\/github.com\/pytorch\/tutorials\/blob\/master\/prototype_source\/torchscript_freezing.py)\n\n- The frozen model is 50% faster than the scripted model. \n\n- On some more complex models, weobserved even higher speed up of warm up time. \n\n- freezing achieves this speed upbecause it is doing some the work TorchScript has to do when the first coupleruns are initiated.\n\n- Inference time measures inference execution time after the model is warmed up. Although we observed significant variation in execution time, thefrozen model is often about 15% faster than the scripted model. \n\n\n- When input is larger, we observe a smaller speed up because the execution is dominated by tensor operations.\n","8f67a5ee":"### Final word on Model Freezing. \n","1f6b8c7a":"\nHere are the Blog Posts that describe this new release.\n\n[PyTorch 1.6 released](https:\/\/pytorch.org\/blog\/pytorch-1.6-released\/)\n\n[Feature Classification in PyTorch](https:\/\/pytorch.org\/blog\/pytorch-feature-classification-changes\/)\n\n[Native Mixed Precision Training](https:\/\/pytorch.org\/blog\/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision\/)\n\n[Windows Support](https:\/\/pytorch.org\/blog\/microsoft-becomes-maintainer-of-the-windows-version-of-pytorch\/)\n","33938bd7":"Again this is just Introduction to how numeric Suite can be used. The code was adapted from [this](https:\/\/github.com\/pytorch\/tutorials\/blob\/master\/prototype_source\/numeric_suite_tutorial.py) tutorial.\n\nNumeric Suite is thus powerful to debug our quantized models\n","9d66aa8a":"PyTorch Numeric Suite currently supports models quantized through both static quantization and dynamic quantization with unified APIs.\n\nThe code was adapted from [this](https:\/\/github.com\/pytorch\/tutorials\/blob\/master\/prototype_source\/numeric_suite_tutorial.py) tutorial.\n\nI provide here a short summary of it to get started.\n","089469b5":"\n There are three things we do in ``quantize_jit``:\n\n\n\n 1. ``prepare_jit`` folds BatchNorm modules into previous Conv2d modules, and insert observers in appropriate places in the Torchscript model.\n\n 2. Run calibrate function on the provided sample dataset.\n\n 3. ``convert_jit`` takes a calibrated model and produces a quantized model.\n\n\n\n If ``debug`` is False (default option), ``convert_jit`` will:\n\n\n\n - Calculate quantization parameters using the observers in the model\n\n - Ifnsert quantization ops like ``aten::quantize_per_tensor`` and ``aten::dequantize`` to the model, and remove the observer modules after that.\n\n - Replace floating point ops with quantized ops\n\n - Freeze the model (remove constant attributes and make them as Constant node in the graph).\n\n - Fold the quantize and prepack ops like ``quantized::conv2d_prepack`` into an attribute, so we don't need to quantize and prepack the weight everytime we run the model.\n\n\n\n If ``debug`` is set to ``True``:","791b727f":"### Histogram to compare models\n\n-  As another example ``wt_compare_dict`` can also be used to plot the histogram of the weights of floating point and quantized models.","8e5f6f9a":"- PyTorch 1.6 was released a few days back.\n\n- I am excited to write this overview \/ tutorial to some of its features which are very useful and helpful.\n\n- I present the features with a working example and demo of how to use it.\n\n- I have adapted the code from various sources and written some. I will cite and direct you to the sources at places.","f77e7572":"## Model Freezing in TorchScipt (Prototype)","a7f0e328":"- You can use scripting or tracing to convert to TorchScript.\n- I will script it here."}}