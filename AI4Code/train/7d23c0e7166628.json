{"cell_type":{"2efd4036":"code","72e70c36":"code","3c0a0d2d":"code","5cdbfbc3":"code","2a88eb36":"code","a0c3271d":"code","84e36a51":"code","de6f0f7c":"code","d399b964":"code","807d1506":"code","5ef028f5":"code","3438d5bb":"code","402e0faf":"code","9d89c913":"code","22593634":"code","470e9fd5":"code","27f29b86":"code","e30e26af":"code","3c6ff99e":"code","58556e6d":"code","9bb3ea3c":"code","1aebe487":"code","143b2934":"code","aa7f7243":"code","781a7089":"code","dd276338":"code","63e65226":"code","fb01d29c":"code","a3862d93":"code","d0f00036":"code","def75cbe":"code","793eb691":"code","a7931857":"code","b7ac0d2d":"code","24525ba4":"code","c9956a79":"code","210098b8":"code","3276b720":"code","d1149f19":"code","c7e0fb39":"code","4366d927":"code","044499bf":"code","72b6ae84":"code","a33eee15":"code","31daf95f":"code","3f13a69a":"code","05fcd91e":"code","f256e377":"code","24443d1c":"code","074c35ac":"code","badb387b":"code","0d12dbc7":"code","497698dc":"markdown","8b0520ee":"markdown","d23a8611":"markdown","6747e190":"markdown"},"source":{"2efd4036":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\nimport seaborn as sns\nsns.set(style='dark')\nimport sklearn\nimport imblearn\nimport matplotlib.pyplot as plt\nimport time\nimport sklearn.metrics as m\nimport xgboost as xgb\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Settings\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\ndf1=pd.read_csv(\"\/kaggle\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\")#,nrows = 50000\ndf2=pd.read_csv(\"\/kaggle\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\")\ndf3=pd.read_csv(\"\/kaggle\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Friday-WorkingHours-Morning.pcap_ISCX.csv\")\n\ndf5=pd.read_csv(\"\/kaggle\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\")\ndf6=pd.read_csv(\"\/kaggle\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\")\n","72e70c36":"print (df1[' Label'].unique())\nprint (df2[' Label'].unique())\nprint (df3[' Label'].unique())\n\nprint (df5[' Label'].unique())\nprint (df6[' Label'].unique())\n","3c0a0d2d":"df = pd.concat([df1,df2])\ndel df1,df2\ndf = pd.concat([df,df3])\ndel df3\n\ndf = pd.concat([df,df5])\ndel df5\ndf = pd.concat([df,df6])\ndel df6\n","5cdbfbc3":"data = df.copy()","2a88eb36":"data.info()","a0c3271d":"data[\" Label\"].value_counts()","84e36a51":"# Check for missing data\nprint(f\"Missing values: {data.isnull().sum().sum()}\")\n\n# Check for infinite values, replace with NAN so it is easy to remove them\ndata.replace([np.inf, -np.inf], np.nan, inplace=True)\nprint(f\"Missing values: {data.isnull().sum().sum()}\")","de6f0f7c":"deleteCol = []\nfor column in data.columns:\n    if data[column].isnull().values.any():\n        deleteCol.append(column)\nfor column in deleteCol:\n    data.drop([column],axis=1,inplace=True)","d399b964":"deleteCol = []\nfor column in data.columns:\n    if column == ' Label':\n        continue\n    elif data[column].dtype==np.object:\n        deleteCol.append(column)\nfor column in deleteCol:\n    data.drop(column,axis=1,inplace=True)","807d1506":"data[' Flow Duration'].unique()","5ef028f5":"for column in data.columns:\n    if data[column].dtype == np.int64:\n        maxVal = data[column].max()\n        if maxVal < 120:\n            data[column] = data[column].astype(np.int8)\n        elif maxVal < 32767:\n            data[column] = data[column].astype(np.int16)\n        else:\n            data[column] = data[column].astype(np.int32)\n            \n    if data[column].dtype == np.float64:\n        maxVal = data[column].max()\n        minVal = data[data[column]>0][column]\n        if maxVal < 120 and minVal>0.01 :\n            data[column] = data[column].astype(np.float16)\n        else:\n            data[column] = data[column].astype(np.float32)\n            ","3438d5bb":"data.info()","402e0faf":"data[' Label'].value_counts()","9d89c913":"benign = data[data[' Label'] == 'BENIGN'].sample(frac=0.1).reset_index(drop=True)\nattack = data[data[' Label'] != 'BENIGN']\ndata = pd.concat([attack, benign])\ndata[' Label'].value_counts()","22593634":"ddos = data[data[' Label'] == 'DDoS'].sample(frac=0.32).reset_index(drop=True)\nattack = data[data[' Label'] != 'DDoS']\ndata = pd.concat([attack, ddos])\ndata[' Label'].value_counts()","470e9fd5":"PortScan = data[data[' Label'] == 'PortScan'].sample(frac=0.32).reset_index(drop=True)\nattack = data[data[' Label'] != 'PortScan']\ndata = pd.concat([attack, PortScan])\ndata[' Label'].value_counts()","27f29b86":"y = data[' Label']\nX = data.drop([' Label'],axis=1)","e30e26af":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nbestfeatures = SelectKBest(score_func=f_classif, k=10)\nfit = bestfeatures.fit(X,y)\n\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(30,'Score'))  #print 10 best features","3c6ff99e":"feature = pd.DataFrame()\nn = len(featureScores['Specs'])\nfor i in featureScores.nlargest(n\/\/2,'Score')['Specs']:\n        feature[i] = data[i]\nfeature[' Label'] = data[' Label']","58556e6d":"feature.info()","9bb3ea3c":"from matplotlib import pyplot as plt \nimport seaborn as sns\nfig= plt.figure(figsize=(40,40))\nsns.heatmap(feature.corr(), annot=True)","1aebe487":"feature.info()","143b2934":"feature.drop([' Bwd Packet Length Mean'],axis=1,inplace=True)\nfeature.drop([' Avg Bwd Segment Size'],axis=1,inplace=True)\nfeature.drop(['Bwd Packet Length Max'],axis=1,inplace=True)\nfeature.drop([' Packet Length Std'],axis=1,inplace=True)\nfeature.drop([' Average Packet Size'],axis=1,inplace=True)\nfeature.drop([' Packet Length Mean'],axis=1,inplace=True)\nfeature.drop([' Max Packet Length'],axis=1,inplace=True)\nfeature.drop([' Packet Length Variance'],axis=1,inplace=True)\n\n\nfeature.drop([' Idle Max'],axis=1,inplace=True)\nfeature.drop([' Fwd IAT Max'],axis=1,inplace=True)\nfeature.drop([' Flow IAT Std'],axis=1,inplace=True)\nfeature.drop([' Idle Std'],axis=1,inplace=True)\nfeature.drop(['Idle Mean'],axis=1,inplace=True)","aa7f7243":"fig= plt.figure(figsize=(40,40))\nsns.heatmap(feature.corr(), annot=True)","781a7089":"feature[' Label'].value_counts()","dd276338":"attackType = feature[' Label'].unique()\nfeature[' Label'] = feature[' Label'].astype('category')\nfeature[' Label'] = feature[' Label'].astype(\"category\").cat.codes","63e65226":"print (attackType)\nprint (feature[' Label'].value_counts())","fb01d29c":"feature.info()","a3862d93":"feature0 = feature[feature[' Label'] == 0]\nfeature0.info()","d0f00036":"y = feature[' Label']\nX = feature.drop([' Label'],axis=1)","def75cbe":"from imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler('majority')\nX_rus, y_rus = rus.fit_resample(X, y)\n\n","793eb691":"X_rus.info()","a7931857":"y_rus.value_counts()","b7ac0d2d":"from imblearn.combine import SMOTETomek\n\nsmt = SMOTETomek('not majority' ,n_jobs=-1)\nX_smt, y_smt = smt.fit_resample(X_rus, y_rus)","24525ba4":"X_smt.to_csv('\/kaggle\/working\/X_smt.csv')","c9956a79":"y_smt.to_csv('\/kaggle\/working\/y_smt.csv')","210098b8":"y_smt.value_counts()","3276b720":"#Split dataset on train and test\nfrom sklearn.model_selection import train_test_split\ntrain_X, test_X,train_y,test_y=train_test_split(X_smt,y_smt,test_size=0.3, random_state=10)\n\n#Exploratory Analysis\n# Descriptive statistics\n# print (train_X.describe())\n\n\n# Packet Attack Distribution\n# train[' Label'].value_counts()\n# test[' Label'].value_counts()","d1149f19":"#Scalling numerical attributes\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# extract numerical attributes and scale it to have zero mean and unit variance  \ncols = train_X.select_dtypes(include=['float32','float16','int32','int16','int8']).columns\nsc_train = scaler.fit_transform(train_X.select_dtypes(include=['float32','float16','int32','int16','int8']))\nsc_test = scaler.fit_transform(test_X.select_dtypes(include=['float32','float16','int32','int16','int8']))\n\n# turn the result back to a dataframe\ntrain_X = pd.DataFrame(sc_train, columns = cols)\ntest_X = pd.DataFrame(sc_test, columns = cols)","c7e0fb39":"train_X.head()","4366d927":"# #Feature Selection\n# from sklearn.ensemble import ExtraTreesClassifier\n# etc = ExtraTreesClassifier();\n\n# # fit random forest classifier on the training set\n# etc.fit(train_X, train_y);","044499bf":"# # extract important features\n# score = np.round(etc.feature_importances_,3)\n# importances = pd.DataFrame({'feature':train_X.columns,'importance':score})\n# importances = importances.sort_values('importance',ascending=False).set_index('feature')\n\n# # plot importances\n# plt.rcParams['figure.figsize'] = (11, 4)\n# importances.plot.bar();","72b6ae84":"#Dataset Partition\nX_train,X_test,Y_train,Y_test = train_test_split(train_X,train_y,train_size=0.70, random_state=2)\n\n#Fitting Models\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import BernoulliNB \nfrom sklearn import tree\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n# Train KNeighborsClassifier Model\n# KNN_Classifier = KNeighborsClassifier(n_jobs=-1)\n# KNN_Classifier.fit(X_train, Y_train); \n\n# Random Forest Model \n# RTC_Classifier = RandomForestClassifier(criterion='entropy')\n# RTC_Classifier.fit(X_train, Y_train)\n\n# Train LogisticRegression Model\n# LGR_Classifier = LogisticRegression(n_jobs=-1, random_state=0)\n# LGR_Classifier.fit(X_train, Y_train);\n\n# Train Gaussian Naive Baye Model\n# BNB_Classifier = BernoulliNB()\n# BNB_Classifier.fit(X_train, Y_train)\n\n# Train Decision Tree Model\n# DTC_Classifier = tree.DecisionTreeClassifier(criterion='entropy', random_state=0)\n# DTC_Classifier.fit(X_train, Y_train)\n# print ('DTC Classifier Trained')\n\n# XGB_Classifier = xgb.XGBClassifier(criterion='entropy', random_state=0)\n# XGB_Classifier.fit(X_train, Y_train)\n# print ('XGB Classifier Trained')\n\n\n\n\n# ADA_Classifier = AdaBoostClassifier(\n#     DTC_Classifier,\n#     n_estimators=100,\n#     learning_rate=1.5)\n\n# ADA_Classifier.fit(X_train, Y_train)\n# print ('ADA Classifier Trained')\n\n","a33eee15":"# Train KNeighborsClassifier Model\nKNN_Classifier = KNeighborsClassifier(n_jobs=-1)\nKNN_Classifier.fit(X_train, Y_train); ","31daf95f":"# Random Forest Model \nRTC_Classifier = RandomForestClassifier(criterion='entropy')\nRTC_Classifier.fit(X_train, Y_train)","3f13a69a":"# Train LogisticRegression Model\nLGR_Classifier = LogisticRegression(n_jobs=-1, random_state=0)\nLGR_Classifier.fit(X_train, Y_train);","05fcd91e":"# Train Gaussian Naive Baye Model\nBNB_Classifier = BernoulliNB()\nBNB_Classifier.fit(X_train, Y_train)","f256e377":"# Train Decision Tree Model\nDTC_Classifier = tree.DecisionTreeClassifier(criterion='entropy', random_state=0)\nDTC_Classifier.fit(X_train, Y_train)\n","24443d1c":"XGB_Classifier = xgb.XGBClassifier(criterion='entropy', random_state=0)\nXGB_Classifier.fit(X_train, Y_train)\nprint ('XGB Classifier Trained')","074c35ac":"ADA_Classifier = AdaBoostClassifier(\n    DTC_Classifier,\n    n_estimators=100,\n    learning_rate=1.5)\n\nADA_Classifier.fit(X_train, Y_train)","badb387b":"#Evaluate Models\nfrom sklearn import metrics\n\nmodels = []\n\nmodels.append(('Decision Tree Classifier', DTC_Classifier))\nmodels.append(('Random Forest Classifier', RTC_Classifier))\nmodels.append(('Gaussian Naive Baye Model', BNB_Classifier))\nmodels.append(('ADA_Classifier', ADA_Classifier))\nmodels.append(('XGB_Classifier', XGB_Classifier))\nmodels.append(('KNN_Classifier', KNN_Classifier))\nmodels.append(('Lgr_classifier', LGR_Classifier))\n\nfor i, v in models:\n    vpred = v.predict(X_train)\n    scores = cross_val_score(v, X_train, Y_train, cv=10)\n    accuracy = metrics.accuracy_score(Y_train, vpred)\n    confusion_matrix = metrics.confusion_matrix(Y_train, vpred)\n    classification = metrics.classification_report(Y_train, vpred)\n    print()\n    print('============================== {} Model Evaluation =============================='.format(i))\n    print()\n    print (\"Cross Validation Mean Score:\" \"\\n\", scores.mean())\n    print()\n    print (\"Model Accuracy:\" \"\\n\", accuracy)  \n    print()\n\n    \n    print(\"Confusion matrix:\" \"\\n\", confusion_matrix)\n    print()\n    print(\"Classification report:\" \"\\n\", classification) \n    print()","0d12dbc7":"#Validate Models\nfor i, v in models:\n    accuracy = metrics.accuracy_score(Y_test, v.predict(X_test))\n    confusion_matrix = metrics.confusion_matrix(Y_test, v.predict(X_test))\n    classification = metrics.classification_report(Y_test, v.predict(X_test))\n    print()\n    print('============================== {} Model Test Results =============================='.format(i))\n    print()\n    print (\"Model Accuracy:\" \"\\n\", accuracy)\n    print()\n    print(\"Confusion matrix:\" \"\\n\", confusion_matrix)\n    print()\n    print(\"Classification report:\" \"\\n\", classification) \n    print()        ","497698dc":"### Droping Null values","8b0520ee":"# Feature Selection\n\nFeature selection is a technique where we choose those features in our data that contribute most to the target variable.\n\n\n    1. Reduces Overfitting: Less redundant data means less possibility of making decisions based on redundant data\/noise.\n    2. Improves Accuracy: Less misleading data means modeling accuracy improves.\n    3. Reduces Training Time: Less data means that algorithms train faster.","d23a8611":"\n**Correlation**, statistical technique which determines how one variables moves\/changes in relation with the other variable.\nWhen we have highly correlated features in the dataset it increses the variance and unreliable.\n\nClearly there is an exellent correlation in -\n\n* Bwd Packet Length Std\n* Bwd Packet Length Mean\n* Avg Bwd Segment Size\n* Bwd Packet Length Max\n* Packet Length Std\n* Average Packet Size\n* Packet Length Mean\n* Max Packet Length\n* Packet Length Variance\n\nAlso,correlattion lies in -\n\n* Flow IAT Max\n* Idle Max\n* Fwd IAT Max\n* Flow IAT Std\n* Idle Std\n* Idle Mean\n                  ","6747e190":"SelectKBest\n\nThe SelectKBest class just scores the features using a function (in this case f_classif for classification) and then \"removes all but the k highest scoring features\""}}