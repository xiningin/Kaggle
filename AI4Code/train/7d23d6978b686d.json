{"cell_type":{"288a8608":"code","965fd080":"code","a28d80c8":"code","b7e46833":"code","190e5ab9":"code","62ec7538":"code","7ea8a916":"code","c9e5ba49":"code","4ff323d4":"code","ddac71f6":"code","520be831":"code","3e1765f0":"code","a6d333ab":"markdown","ae22b443":"markdown","4b9f36c0":"markdown"},"source":{"288a8608":"!pip install --upgrade pip\n!pip install pymap3d==2.1.0\n!pip install -U l5kit","965fd080":"import matplotlib.pyplot as plt\n\nimport numpy as np\n\nfrom l5kit.data import ChunkedDataset, LocalDataManager\nfrom l5kit.dataset import EgoDataset, AgentDataset\n\nfrom l5kit.rasterization import build_rasterizer\nfrom l5kit.configs import load_config_data\nfrom l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\nfrom l5kit.geometry import transform_points\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom l5kit.data import PERCEPTION_LABELS\nfrom prettytable import PrettyTable\n\nimport os\n\nfrom matplotlib import animation, rc\nfrom IPython.display import HTML\n\nrc('animation', html='jshtml')","a28d80c8":"# set env variable for data\nos.environ[\"L5KIT_DATA_FOLDER\"] = \"..\/input\/lyft-motion-prediction-autonomous-vehicles\"\n# get config\ncfg = load_config_data(\"..\/input\/lyft-config-files\/visualisation_config.yaml\")\nprint(cfg)","b7e46833":"print(f'current raster_param:\\n')\nfor k,v in cfg[\"raster_params\"].items():\n    print(f\"{k}:{v}\")","190e5ab9":"dm = LocalDataManager()\ndataset_path = dm.require(cfg[\"val_data_loader\"][\"key\"])\nzarr_dataset = ChunkedDataset(dataset_path)\nzarr_dataset.open()\nprint(zarr_dataset)","62ec7538":"frames = zarr_dataset.frames\ncoords = np.zeros((len(frames), 2))\nfor idx_coord, idx_data in enumerate(tqdm(range(len(frames)), desc=\"getting centroid to plot trajectory\")):\n    frame = zarr_dataset.frames[idx_data]\n    coords[idx_coord] = frame[\"ego_translation\"][:2]\n\n\nplt.scatter(coords[:, 0], coords[:, 1], marker='.')\naxes = plt.gca()\naxes.set_xlim([-2500, 1600])\naxes.set_ylim([-2500, 1600])","7ea8a916":"agents = zarr_dataset.agents\nprobabilities = agents[\"label_probabilities\"]\nlabels_indexes = np.argmax(probabilities, axis=1)\ncounts = []\nfor idx_label, label in enumerate(PERCEPTION_LABELS):\n    counts.append(np.sum(labels_indexes == idx_label))\n    \ntable = PrettyTable(field_names=[\"label\", \"counts\"])\nfor count, label in zip(counts, PERCEPTION_LABELS):\n    table.add_row([label, count])\nprint(table)","c9e5ba49":"rast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)","4ff323d4":"data = dataset[50]\n\nim = data[\"image\"].transpose(1, 2, 0)\nim = dataset.rasterizer.to_rgb(im)\ntarget_positions_pixels = transform_points(data[\"target_positions\"], data[\"raster_from_agent\"])\ndraw_trajectory(im, target_positions_pixels, TARGET_POINTS_COLOR, yaws=data[\"target_yaws\"])\n\nplt.imshow(im[::-1])\nplt.show()","ddac71f6":"cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)\ndata = dataset[50]\n\nim = data[\"image\"].transpose(1, 2, 0)\nim = dataset.rasterizer.to_rgb(im)\ntarget_positions_pixels = transform_points(data[\"target_positions\"], data[\"raster_from_agent\"])\ndraw_trajectory(im, target_positions_pixels, TARGET_POINTS_COLOR, yaws=data[\"target_yaws\"])\n\nplt.imshow(im[::-1])\nplt.show()","520be831":"dataset = AgentDataset(cfg, zarr_dataset, rast)\ndata = dataset[0]\n\nim = data[\"image\"].transpose(1, 2, 0)\nim = dataset.rasterizer.to_rgb(im)\ntarget_positions_pixels = transform_points(data[\"target_positions\"], data[\"raster_from_agent\"])\ndraw_trajectory(im, target_positions_pixels, TARGET_POINTS_COLOR, yaws=data[\"target_yaws\"])\n\nplt.imshow(im[::-1])\nplt.show()","3e1765f0":"from IPython.display import display, clear_output\nimport PIL\n \ncfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)\nscene_idx = 1\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"], data[\"raster_from_agent\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, TARGET_POINTS_COLOR, yaws=data[\"target_yaws\"])\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","a6d333ab":"# Imports","ae22b443":"# Scene Visualisation","4b9f36c0":"Thanks to Lyft for hosting this awesome competition!\n\nCode is taken from: \n\nhttps:\/\/github.com\/lyft\/l5kit\n\nhttps:\/\/github.com\/lyft\/l5kit\/blob\/master\/examples\/visualisation\/visualise_data.ipynb\n"}}