{"cell_type":{"0f2eaddf":"code","e4d81be3":"code","77fc8984":"code","31c47e73":"code","91b891f8":"code","aeea990d":"code","8585455a":"code","d45c3cab":"code","53076eab":"code","a3d15d35":"code","39f0832e":"code","34fee917":"code","5fe544ff":"code","88c84e04":"code","3ca4ff54":"code","62c127b9":"code","5fc37ae6":"code","4991b559":"code","3fb5ac79":"code","b645108f":"code","a89267be":"code","13d627de":"code","37ed9f87":"code","dfdda20b":"code","0acbb75c":"code","4516e68f":"markdown","b34d39ec":"markdown","aa00acb4":"markdown","92390e3c":"markdown","52b99aa8":"markdown","8213cc20":"markdown","62b18c07":"markdown","31b1ccfc":"markdown","06c7801a":"markdown","3cae2278":"markdown"},"source":{"0f2eaddf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n!pip install swifter\n!pip install Sastrawi\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e4d81be3":"# Dataset Import\nDATASET_COLUMNS  = [\"text\",\"kategori\"]\nDATASET_ENCODING = \"ISO-8859-1\"\ndataset = pd.read_csv('..\/input\/data-mining\/datasets.csv',\n                      encoding=DATASET_ENCODING , names=DATASET_COLUMNS)\n\n# Melakukan Pelabelan pada Dataset\ncode_label = {0: \"Negatif\", 1: \"Positif\" , 2:\"Netral\" }\ndef decode_sentiment(label):\n    return code_label [int(label)]\ndataset.kategori = dataset.kategori.apply(lambda x: decode_sentiment(x))\ndataset.head()","77fc8984":"# Melakukan Penghitungan data tiap Sentiment\nax = dataset['kategori'].value_counts().plot(kind='bar', figsize=(10,6),fontsize=13,color='#D2691E')\nax.set_title('Jumlah Data Pada Dataset', size=10, pad=30)\nax.set_ylabel('Level Angka', fontsize=14)\n\nfor i in ax.patches:\n    ax.text(i.get_x() + 0.19, i.get_height() + 8, str(round(i.get_height(), 6)), fontsize=15)","31c47e73":"# Library yang digunakan dalam langkah pre-processing\nimport nltk\nimport re\nimport string\nimport swifter\nimport re\nimport pickle\n\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\nfrom Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory","91b891f8":"factory = StemmerFactory()\nstemmer = factory.create_stemmer()","aeea990d":"# MelakukanTahap Case Folding Pada Data\ndef lowercase(text):\n    return text.lower()","8585455a":"# melakukan Tahap Tokenizing pada Data\ndef remove_unnecessary_char(text):\n    text = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+)|(http?:\/\/[^\\s]+))',' ',text) # Remove every URL\n    text = re.sub('\\n',' ',text) # Remove every '\\n'\n    text = re.sub('\\r',' ',text) # Remove every '\\r'\n    text = re.sub('@[^\\s]+[ \\t]','',text) # Remove every username\n    text = re.sub(r'\\brt\\b', '',text).strip() # Remove retweet symbol\n    text = re.sub('(?i)user','',text) # Remove every username\n    text = re.sub('(?i)url',' ',text) # Remove every url\n    text = re.sub(r'\\\\x..',' ',text) # Remove every emoji\n    text = re.sub('  +', ' ', text) # Remove extra spaces\n    text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text) #Remove characters repeating more than twice \n    return text\n\n# Haus Emoji Pada Data\ndef remove_emoji(string): # Remove emojis\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', string)\n\n# Hapus Numerik\ndef remove_nonaplhanumeric(text):\n    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n    return text","d45c3cab":"# Tahap Stopword Pada Data\ndef remove_stopword(text):\n    listStopword =  set(stopwords.words('indonesian'))\n    removed = []\n    for t in text:\n        if t not in listStopword:\n            removed.append(t)\n    return text\n\n# Tahap Stemming Pada Data\ndef stemming(text):\n    return stemmer.stem(text)","53076eab":"# dan Melakukan Tahap Preprocess\ndef preprocess(text):\n    text = remove_unnecessary_char(text)\n    text = lowercase(text)\n    text = remove_nonaplhanumeric(text)\n    text = stemming(text)\n    text = remove_emoji(text)\n    text = remove_stopword(text)\n    return text\ndataset['text'] = dataset['text'].apply(preprocess)","a3d15d35":"# Melakukan Pemanggilan Data\nprint(\"Shape: \", dataset.shape)\nprint (dataset)","39f0832e":"# Library yang digunakan\nfrom sklearn.model_selection import train_test_split, StratifiedKFold","34fee917":"# Melakukan Pembagian Data\nX_train, X_test, y_train, y_test = train_test_split(dataset['text'],dataset['kategori'],\n                                                    test_size = 0.05, random_state = 10)","5fe544ff":"# Library\nfrom sklearn.feature_extraction.text import TfidfVectorizer","88c84e04":"#Melakukan Vektorisasi data\nvectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=800000)\nvectoriser.fit(X_train)\nprint(f'Vectoriser fitted.')\nprint('No. of feature_words: ', len(vectoriser.get_feature_names()))","3ca4ff54":"# library\nfrom sklearn.metrics import confusion_matrix, classification_report","62c127b9":"X_train = vectoriser.transform(X_train)\nX_test  = vectoriser.transform(X_test)\n\n# Build Model\ndef model_Evaluate(model):\n    \n    # Predict values for Test dataset\n    y_pred = model.predict(X_test)\n\n    # Print the evaluation metrics for the dataset.\n    print(classification_report(y_test, y_pred))\n","5fc37ae6":"# Library\nfrom sklearn.ensemble import RandomForestClassifier","4991b559":"random_classifier= RandomForestClassifier()\nrandom_classifier.fit(X_train,y_train)","3fb5ac79":"y_pred= random_classifier.predict(X_test)\nprint(classification_report(y_test,y_pred))","b645108f":"# Library\nfrom sklearn.svm import SVC","a89267be":"svm_classifier= SVC(probability=True)\nsvm_classifier.fit(X_train,y_train)","13d627de":"#Predict\ny_pred_svm= svm_classifier.predict(X_test)\n#Classification Report\nprint(classification_report(y_test,y_pred_svm))","37ed9f87":"# LIbrary\nfrom sklearn.naive_bayes import BernoulliNB","dfdda20b":"BNBmodel = BernoulliNB(alpha = 2)\nBNBmodel.fit(X_train, y_train)","0acbb75c":"model_Evaluate(BNBmodel)","4516e68f":"Adapun dalam melakukan sentimen ini menggunakan beberapa model untuk menemukan hasil akurasi yang ada pada dataset, adapun hasilnya adalah sebagai berikut :\n\n* Random Forest Model ( 0,62 )\n* Suport Vector Machine Model ( 0,62 )\n* BernoulliNB Naive Bayes Model ( 0,50 )","b34d39ec":"**~~~~~ Random Forest Model**","aa00acb4":"# -----Pre-Processing-----","92390e3c":"# ----- Dataset To Matrik -----","52b99aa8":"# ----- Penerapan Model -----","8213cc20":"**~~~~~ BernaulliNB - Naive Bayes Model**","62b18c07":"**~~~~~ Support Vector Machine (SVM)**","31b1ccfc":"# ----- Vectoriser Data -----","06c7801a":"# ------- KESIMPULAN ------","3cae2278":"# ----- Split Data -----"}}