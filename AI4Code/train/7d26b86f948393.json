{"cell_type":{"f13dce66":"code","baaf11a6":"code","fd56cc47":"code","569148e1":"code","eb9e74b8":"code","e2d20307":"code","8629bdf0":"code","ee921246":"code","dff36776":"code","2fb75633":"code","0fce97ab":"code","a9976919":"code","678c8e3d":"code","2c199799":"code","363f87de":"code","9acf1ba3":"code","8f5f7fac":"code","1b3f6768":"code","fcac8390":"code","8287f12f":"code","d00de317":"code","53184e88":"code","a7ad2057":"code","107d66c6":"code","33889097":"code","6837dcd7":"code","67679560":"code","176c5c0c":"code","6f7a8258":"code","0c8cb290":"code","2d0a1f03":"code","8a77c7c6":"code","3e83a2fe":"code","f786b240":"code","810a4d02":"code","6db65bd1":"code","56d7bc08":"code","c64c2080":"code","240b7603":"code","264af541":"code","11b7de3c":"code","fafd77ba":"code","ba362c19":"code","ce387199":"code","f5a13463":"code","cfe7254e":"markdown","daa20de2":"markdown","eebd391c":"markdown","42b28772":"markdown","3c85765f":"markdown","1603f53d":"markdown","830a4a6f":"markdown"},"source":{"f13dce66":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os","baaf11a6":"#As the date is of object type we need to cnvert it to datetime format hence will reading the data:\nabc=pd.read_csv(\"..\/input\/airquality\/data.csv\", parse_dates=['date'])\nabc.info()","fd56cc47":"abc['type'].value_counts()","569148e1":"#as there are alot of duplicate types\n#cleaning type column and should have only four columns Industrial,Residential,Sensitive and RIRUO\n#Updating the changes to abc data frame\nabc.loc[(abc['type']==\"Residential, Rural and other Areas\"),'type']='Residential'\nabc.loc[(abc['type']==\"Residential and others\"),'type']='Residential'\nabc.loc[(abc['type']==\"Industrial Area\"),'type']='Industrial'\nabc.loc[(abc['type']==\"Industrial Areas\"),'type']='Industrial'\nabc.loc[(abc['type']==\"Sensitive Area\"),'type']='Sensitive'\nabc.loc[(abc['type']==\"Sensitive Areas\"),'type']='Sensitive'\nabc['type'].value_counts()","eb9e74b8":"#Filling missing values for rspm and spm hence grouping by location and type\ngrp_location=abc.groupby(['location','type'])\ndict_grp_location=dict(list(grp_location))\n# dict_grp_location","e2d20307":"print(abc['rspm'].isnull().sum())\nprint(abc['spm'].isnull().sum())","8629bdf0":"#Forward filling\ngrouped_location=pd.DataFrame()\nfor key in dict_grp_location:\n    df1=dict_grp_location[key].sort_values(by='date')\n    df1['rspm'].fillna(method='ffill',inplace=True)\n    df1['spm'].fillna(method='ffill',inplace=True)\n    grouped_location=pd.concat([grouped_location,df1])","ee921246":"print(grouped_location['rspm'].isnull().sum())\nprint(grouped_location['spm'].isnull().sum())\n","dff36776":"#Initially we have grouped by 'location' and 'type' and then did foward fill but some values were not filled hence backward fill\nbackwardfill=grouped_location.groupby(['location','type'])\nbackwardfill=dict(list(backwardfill))\nbackwardfill\ngrouped_location=pd.DataFrame()\nfor key in backwardfill:\n    df2=backwardfill[key].sort_values(by='date')\n    df2['rspm'].fillna(method='bfill',inplace=True)\n    df2['spm'].fillna(method='bfill',inplace=True)\n    grouped_location=pd.concat([grouped_location,df2])","2fb75633":"print(grouped_location['rspm'].isnull().sum())\nprint(grouped_location['spm'].isnull().sum())","0fce97ab":"#now we are grouping it on larger scale that is 'state' and thn by 'type' so as to fill null values\ndict_grouped_state=dict(list(grouped_location.groupby(['state','type'])))\n","a9976919":"grouped_state=pd.DataFrame()\nfor key in dict_grouped_state:\n    df3=dict_grouped_state[key]\n    df3['rspm'].fillna(df3['rspm'].median(),inplace=True)\n    df3['spm'].fillna(df3['spm'].median(),inplace=True)\n    grouped_state=pd.concat([grouped_state,df3])","678c8e3d":"print(grouped_state['spm'].isnull().sum())\nprint(grouped_state['rspm'].isnull().sum())","2c199799":"#Now we are grouping by 'type' and replacimg all remaining nan values\ngrouped_type=grouped_state.groupby('type').median()\ngrouped_type","363f87de":"dataframe=grouped_state","9acf1ba3":"dataframe.loc[(dataframe['type']=='Industrial') & (dataframe['rspm'].isnull()),'rspm']=grouped_type['rspm']['Industrial']\ndataframe.loc[(dataframe['type']=='RIRUO') & (dataframe['rspm'].isnull()),'rspm']=grouped_type['rspm']['RIRUO']\ndataframe.loc[(dataframe['type']=='Residential') & (dataframe['rspm'].isnull()),'rspm']=grouped_type['rspm']['Residential']\ndataframe.loc[(dataframe['type']=='Sensitive') & (dataframe['rspm'].isnull()),'rspm']=grouped_type['rspm']['Sensitive']\n\ndataframe.loc[(dataframe['type']=='Industrial') & (dataframe['spm'].isnull()),'spm']=grouped_type['spm']['Industrial']\ndataframe.loc[(dataframe['type']=='RIRUO') & (dataframe['spm'].isnull()),'spm']=grouped_type['spm']['RIRUO']\ndataframe.loc[(dataframe['type']=='Residential') & (dataframe['spm'].isnull()),'spm']=grouped_type['spm']['Residential']\ndataframe.loc[(dataframe['type']=='Sensitive') & (dataframe['spm'].isnull()),'spm']=grouped_type['spm']['Sensitive']","8f5f7fac":"print(dataframe['rspm'].isnull().sum())\nprint(dataframe['spm'].isnull().sum())","1b3f6768":"#adding a new 'year' column from 'date' column\ndataframe['year']=dataframe['date'].dt.year\nprint(dataframe['year'].isnull().sum())","fcac8390":"#filling null values in year by either doing forward fill or backwadr fill\ndataframe['year']=dataframe['year'].fillna(method='ffill')\nprint(dataframe['year'].isnull().sum())\ndataframe['year']=dataframe['year'].astype(int)","8287f12f":"#ploting states in descending order as per the level of spm\ndataframe","d00de317":"state=dataframe.groupby('state').median()\nstate=state[['rspm','spm']]\nstate=state.sort_values(by='spm',ascending=False)","53184e88":"state.plot(kind='bar',figsize=(15,10))","a7ad2057":"# potting a graph in  descending order as per the level of spm\nstate.sort_values(by='rspm',ascending=False).plot(kind='bar',figsize=(15,10))","107d66c6":"states=state.reset_index().head(5)\ntop_five_states=states['state']\nfor i in top_five_states:\n    print(i)","33889097":"group_by_state=dict(list(dataframe.groupby('state')))\nplot_five_states=pd.DataFrame()\nfor i in top_five_states:\n    df=group_by_state[i][['state','location','spm','rspm','type']]\n    plot_five_states=pd.concat([plot_five_states,df])\nplot_five_states","6837dcd7":"plot_five_states=plot_five_states.groupby(['state','location','type']).median()\nplot_five_states","67679560":"plt.figure(figsize = (20,20))\nplt.subplot(3,2,1)\nplt.title('Delhi')\na = sns.barplot(x = 'location',y = 'spm',hue = 'type',data = plot_five_states.loc['Delhi'].reset_index())\na.set(ylim = (0,600))\nplt.subplot(3,2,2)\nplt.title('Haryana')\nb = sns.barplot(x = 'location',y = 'spm',hue = 'type',data = plot_five_states.loc['Haryana'].reset_index())\nb.set(ylim = (0,600))\nplt.subplot(3,2,3)\nplt.title('Rajasthan')\nc = sns.barplot(x = 'location',y = 'spm',hue = 'type',data = plot_five_states.loc['Rajasthan'].reset_index())\nc.set(ylim = (0,600))\nplt.subplot(3,2,4)\nplt.title('Uttarakhand')\nd = sns.barplot(x = 'location',y = 'spm',hue = 'type',data = plot_five_states.loc['Uttarakhand'].reset_index())\nd.set(ylim = (0,600))\nplt.subplot(3,2,5)\nplt.title('Uttar Pradesh')\nplt.xticks(rotation = 90)\ng = sns.barplot(x = 'location',y = 'spm',hue = 'type',data = plot_five_states.loc['Uttar Pradesh'].reset_index())\ng.set(ylim = (0,600))","176c5c0c":"states_year=dataframe.groupby(['state','year']).median()['spm']","6f7a8258":"states_year","0c8cb290":"states_year=states_year.reset_index()\nstates_year['spm'].isnull().sum()","2d0a1f03":"pivot=pd.pivot_table(states_year,values='spm',index='state',columns='year')\npivot.fillna(0,inplace= True)","8a77c7c6":"pivot","3e83a2fe":"plt.figure(figsize=(20,20))\nsns.heatmap(data=pivot,annot=True)","f786b240":"#Finding the null values for so2 and no2 for karnataka state and the replacing them with median values.\n#1)SO2\nkarnataka=abc.groupby(['state','type'])\na=dict(list(karnataka))\nkar_ind=a[('Karnataka','Industrial')]\nprint(kar_ind['so2'].isnull().sum())\nkar_res=a[('Karnataka','Residential')]\nprint(kar_res['so2'].isnull().sum())\nkar_sensitive=a[('Karnataka','Sensitive')]\nprint(kar_sensitive['so2'].isnull().sum())\n# Karnataka has no RIRUO\n# kar_riruo=a[('Karnataka','RIRUO')]\n# kar_riruo['so2'].isnull().sum()","810a4d02":"#now replacing all these null values of So2  with median values  in copy_abc data frame\ncopy_abc=abc.copy()\n","6db65bd1":"copy_abc.loc[(copy_abc['state']=='Karnataka') & (copy_abc['type']=='Industrial') & (copy_abc['so2'].isnull()),'so2']=kar_ind.median()['so2']\ncopy_abc.loc[(copy_abc['state']=='Karnataka') & (copy_abc['type']=='Residential') & (copy_abc['so2'].isnull()),'so2']=kar_res.median()['so2']\ncopy_abc.loc[(copy_abc['state']=='Karnataka') & (copy_abc['type']=='Sensitive') & (copy_abc['so2'].isnull()),'so2']=kar_sensitive.median()['so2']\n","56d7bc08":"# Checking if null values are imputed S02 \nkarnataka=copy_abc.groupby(['state','type'])\na=dict(list(karnataka))\nprint(a[('Karnataka','Industrial')]['so2'].isnull().sum())\nprint(a[('Karnataka','Residential')]['so2'].isnull().sum())\nprint(a[('Karnataka','Sensitive')]['so2'].isnull().sum())","c64c2080":"#2)Replacing below null values for no2 state of karnataka with median in copy_abc\n#No2","240b7603":"kar_ind=a[('Karnataka','Industrial')]\nkar_ind['no2'].isnull().sum()\nkar_ind=a[('Karnataka','Residential')]\nkar_ind['no2'].isnull().sum()\nkar_sensitive=a[('Karnataka','Sensitive')]\nkar_sensitive['no2'].isnull().sum()\n# Karnataka has no RIRUO\n# kar_riruo=a[('Karnataka','RIRUO')]\n# kar_riruo['no2'].isnull().sum()","264af541":"copy_abc.loc[(copy_abc['state']=='Karnataka') & (copy_abc['type']=='Industrial') & (copy_abc['no2'].isnull()),'no2']=kar_ind.median()['no2']\ncopy_abc.loc[(copy_abc['state']=='Karnataka') & (copy_abc['type']=='Residential') & (copy_abc['no2'].isnull()),'no2']=kar_res.median()['no2']\ncopy_abc.loc[(copy_abc['state']=='Karnataka') & (copy_abc['type']=='Sensitive') & (copy_abc['no2'].isnull()),'no2']=kar_sensitive.median()['no2']\n","11b7de3c":"# Checking if null values are imputed No2\nkarnataka=copy_abc.groupby(['state','type'])\na=dict(list(karnataka))\nprint(a[('Karnataka','Industrial')]['no2'].isnull().sum())\nprint(a[('Karnataka','Residential')]['no2'].isnull().sum())\nprint(a[('Karnataka','Sensitive')]['no2'].isnull().sum())","fafd77ba":"#Now we are grouping the data by just Bangalore state as we have to draw graph of no2 and so2\nbangalore=copy_abc.groupby('location')\nbangalore=dict(list(bangalore))\nbangalore=bangalore['Bangalore'][['so2','no2','date']]\nbangalore['year']=bangalore['date'].dt.year\nbangalore=bangalore[['so2','no2','year']]\nbangalore['year'].isnull().sum()","ba362c19":"#as we have one null value for year field we have to do forward fill or backward fill.Performing 1st backward fill\nbangalore['year']=bangalore['year'].fillna(method='bfill')\nbangalore['year']=bangalore['year'].astype('int')\nbangalore['year'].isnull().sum()","ce387199":"bangalore=bangalore.groupby('year').median()\nbangalore=bangalore.reset_index()\nbangalore","f5a13463":"plt.figure(figsize=(15,5))\nplt.xticks(np.arange(1980,2016))\nplt.yticks(np.arange(5,55,5))\nsns.pointplot(bangalore['year'],bangalore['so2'],color='r')\nsns.pointplot(bangalore['year'],bangalore['no2'],color='g')\nplt.legend(['so2','no2'])","cfe7254e":"The 0 value represents for that year there is missing spm value\/no spm values are recorded for that year.\n","daa20de2":"# Which years have not recorded the spm value ","eebd391c":"# Showing Spm and Rspm level state wise","42b28772":"# So2 and No2 levels in bangalore over the years","3c85765f":"# Looking at top 5 states with high spm values","1603f53d":"The above graph shows the level of so2 and no2 level for Bangalore state from year 1987 10 2015.\nAmount of So2 in Bangalore city from the year 1988 to 2015:\n    In the year 1988 to 1992 the amount of So2 in the air had gradually decreased. \n    The amount of so2 gradually increased from the year 1995 to 1998.\n    The amount of so2 from the year 2000 has never increased beyond 25.\n    The highest amount of So2 is for the year 1998 and 1999.\nAmount of No2 in bangaolre city from the year 1988 to 2015:\n    The amount of no2 is gradually increasing till the year 2000.\n    The year 2004 marks the highest amount of no2 level whereas after 2004 the amount of no2 is gradually decreasing\n    \n    ","830a4a6f":"The above graphs are of top 5 states showing the spm levels as per their locations.\nIn almost all the states, the residential locations are contributing to spm level same as that of industrial locations.\nFrom the graph, its clear that in states like Delhi and Uttarakhand the spm level in residential areas is more \nthan that of industrial areas which is quite surprising."}}