{"cell_type":{"ac61ac66":"code","2aa98555":"code","fcc24596":"code","63b4f909":"code","146e18fa":"code","c88f6689":"code","cd40718f":"code","af56ddc2":"code","57fef71d":"code","1d87b6ce":"code","40d23dbc":"code","8233f98d":"code","a8eccdc2":"code","71b26b8e":"code","81724c7c":"code","41801488":"code","41094b20":"code","8499b147":"code","f1d610a9":"code","72ae4458":"code","81ada29d":"code","f05a54e4":"code","7a7d2908":"code","b0b7625b":"code","2642f661":"code","74dac2f9":"code","1c4bc619":"code","12decd71":"code","9061c0db":"code","7897514e":"code","ff70cf58":"code","b8fdc70e":"code","e8b39266":"code","03236259":"code","13308452":"code","7aaf4382":"code","c0e405ef":"markdown","6fdb3a32":"markdown","4765b3c7":"markdown","f3d2712b":"markdown","27fb9cdb":"markdown","45f6128b":"markdown","4ecfd83e":"markdown","4a4cc485":"markdown","0015f21f":"markdown","6fd5b95f":"markdown","4b2278e5":"markdown","d039c02b":"markdown","e9b9a226":"markdown","fcf4a46b":"markdown","f250beb4":"markdown","1afddf9c":"markdown","f05119a3":"markdown","a043b158":"markdown","0a8ab82f":"markdown","2a721bd9":"markdown"},"source":{"ac61ac66":"import numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nimport category_encoders as ce\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import model_selection\nimport xgboost as xgb\nimport optuna\nimport tqdm\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)","2aa98555":"# matplotlib setting\nmpl.rcParams['figure.dpi'] = 200\nmpl.rcParams['axes.spines.top'] = False\nmpl.rcParams['axes.spines.right'] = False","fcc24596":"train = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')","63b4f909":"print(f'Train Shape :  {train.shape}')\nprint(f'Test Shape :  {test.shape}')","146e18fa":"target = train['loss']\ntrain.drop(['id'], axis=1, inplace=True)\ntest.drop(['id'], axis=1, inplace=True)","c88f6689":"train.head(15)","cd40718f":"train.info()","af56ddc2":"print('Target column basic statistics:')\ntrain['loss'].describe()","57fef71d":"train['loss'].value_counts()","1d87b6ce":"train['loss'].quantile([0.25, 0.5 , 0.75,0.90])","40d23dbc":"fig, ax = plt.subplots(1, 1, figsize=(17, 8))\n\ntarget_cnt = train['loss'].value_counts().sort_index()\n\nax.bar(target_cnt.index, target_cnt,\n       width=0.55, \n       edgecolor='black', \n       linewidth=0.7)\nax.margins(0.02, 0.05)\n\nfor i in range(30):\n    ax.annotate(f'{target_cnt[i]\/len(train)*100:.3}', xy=(i, target_cnt[i]+1000),\n                   va='center', ha='center',)\nfig.tight_layout()\n\nplt.show()\n","8233f98d":"features = [feature for feature in train.columns if feature not in ['id', 'loss']]\nunique_values_train = np.zeros(2)\nfor feature in features:\n    temp = train[feature].unique()\n    unique_values_train = np.concatenate([unique_values_train, temp])\nunique_values_train = np.unique(unique_values_train)\n\nunique_values_test = np.zeros(2)\nfor feature in features:\n    temp = test[feature].unique()\n    unique_values_test = np.concatenate([unique_values_test, temp])\nunique_values_test = np.unique(unique_values_test)\n\nunique_value_feature_train = pd.DataFrame(train[features].nunique())\nunique_value_feature_train = unique_value_feature_train.reset_index(drop=False)\nunique_value_feature_train.columns = ['Features', 'Count']\nunique_value_feature_test = pd.DataFrame(test[features].nunique())\nunique_value_feature_test = unique_value_feature_test.reset_index(drop=False)\nunique_value_feature_test.columns = ['Features', 'Count']\n\n\n\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(4, 12), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace=0.4, hspace=0.1)\n\nbackground_color = \"#f6f5f5\"\n#sns.set_palette(['#ffd514']*75)\n\nax0 = fig.add_subplot(gs[0, 0])\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\nax0.set_facecolor(background_color)\nax0_sns = sns.barplot(ax=ax0, y=unique_value_feature_train['Features'], x=unique_value_feature_train['Count'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.set_xlabel(\"Unique Values\",fontsize=4, weight='bold')\nax0_sns.set_ylabel(\"Features\",fontsize=4, weight='bold')\nax0_sns.tick_params(labelsize=4, width=0.5, length=1.5)\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0.text(0, -1.9, 'Unique Values - Train Dataset', fontsize=6, ha='left', va='top', weight='bold')\nax0.text(0, -1.2, 'feature_1, feature55 , feature 86 have less unique values in test set', fontsize=4, ha='left', va='top')\n# data label\nfor p in ax0.patches:\n    value = f'{p.get_width():.0f}'\n    x = p.get_x() + p.get_width() + 7\n    y = p.get_y() + p.get_height() \/ 2 \n    ax0.text(x, y, value, ha='center', va='center', fontsize=4, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.3))\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#ff322d']*100)\n\nax1 = fig.add_subplot(gs[0, 1])\nfor s in [\"right\", \"top\"]:\n    ax1.spines[s].set_visible(False)\nax1.set_facecolor(background_color)\nax1_sns = sns.barplot(ax=ax1, y=unique_value_feature_test['Features'], x=unique_value_feature_test['Count'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax1_sns.set_xlabel(\"Unique Values\",fontsize=4, weight='bold')\nax1_sns.set_ylabel(\"Features\",fontsize=4, weight='bold')\nax1_sns.tick_params(labelsize=4, width=0.5, length=1.5)\nax1_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax1_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax1.text(0, -1.9, 'Unique Values - Test Dataset', fontsize=6, ha='left', va='top', weight='bold')\nax1.text(0, -1.2, 'feature_1, feature55 , feature 86 have less unique values in test set', fontsize=4, ha='left', va='top')\nfor p in ax1.patches:\n    value = f'{p.get_width():.0f}'\n    x = p.get_x() + p.get_width() + 7\n    y = p.get_y() + p.get_height() \/ 2 \n    ax1.text(x, y, value, ha='center', va='center', fontsize=4, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.3))\n\nplt.show()","a8eccdc2":"from sklearn.preprocessing import StandardScaler\n\nss = StandardScaler()\nfeatures = [f'f{i}' for i in range(100)]\ntrain[features] = ss.fit_transform(train[features])\ntest[features] = ss.transform(test[features])","71b26b8e":"fig, axes = plt.subplots(10,10,figsize=(12, 12))\naxes = axes.flatten()\n\nfor idx, ax in enumerate(axes):\n    sns.kdeplot(data=train, x=f'f{idx}', \n                fill=True, \n                ax=ax)\n    sns.kdeplot(data=test, x=f'f{idx}', \n                fill=True, \n                ax=ax)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel('')\n    ax.set_ylabel('')\n    ax.spines['left'].set_visible(False)\n    ax.set_title(f'f{idx}', loc='right', weight='bold', fontsize=10)\n\nfig.supxlabel('Average by class (by feature)', ha='center', fontweight='bold')\n\nfig.tight_layout()\nplt.show()","81724c7c":"background_color = \"#f6f5f5\"\n\nfig = plt.figure(figsize=(24, 15), facecolor=background_color)\nax0 = fig.add_subplot(gs[0, 0])\ncolors = [\"#2f5586\", \"#f6f5f5\",\"#2f5586\"]\ncolormap = mpl.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nax0.set_facecolor(background_color)\nax0.text(-0.2, -1, 'Features Correlation on Train Dataset', fontsize=12, fontweight='bold', fontfamily='serif')\nax0.text(-0, -0.4, 'Highest correlation in the dataset is 0.6', fontsize=8, fontweight='light', fontfamily='serif')\n\n'''ax1.set_facecolor(background_color)\nax1.text(-0.2, -1, 'Features Correlation on Test Dataset', fontsize=12, fontweight='bold', fontfamily='serif')\nax1.text(-0.3, -0.4, 'Features in test dataset resemble features in train dataset ', \n         fontsize=8, fontweight='light', fontfamily='serif')'''\n\ntrain_corr = train.drop(columns = 'loss').corr()\ntrain_mask = np.triu(np.ones_like(train_corr, dtype=bool))\n\nsns.heatmap(train_corr, \n            square=True, \n            linewidth=0.2,\n            cbar=False,\n            mask=train_mask,\n            annot=False,\n            center=0,\n            cmap=sns.diverging_palette(240, 10),\n            #ax=ax0\n           )\n\nfig = plt.figure(figsize=(24, 15), facecolor=background_color)\n\nax0.set_facecolor(background_color)\nax0.text(-0.2, -1, 'Features Correlation on Train Dataset', fontsize=12, fontweight='bold', fontfamily='serif')\nax0.text(-0, -0.4, 'Highest correlation in the dataset is 0.6', fontsize=8, fontweight='light', fontfamily='serif')\n\ntest_corr = test.corr()\ntest_mask = np.triu(np.ones_like(test_corr, dtype=bool))\n\nsns.heatmap(test_corr, \n            square=True, \n            linewidth=0.2,\n            cbar=False,\n            mask=test_mask,\n            annot=False,\n            center=0,\n            cmap=sns.diverging_palette(240, 10),\n            #ax=ax1\n           )\n\nplt.show()\n","41801488":"fig = plt.figure(figsize=(16, 16), facecolor='#f6f5f5')\n\ntrain_corr1 = train_corr[train_corr > 0.01]\nsns.heatmap(train_corr1, \n            square=True, \n            mask=train_mask,\n            annot=False,\n            cmap=sns.diverging_palette(240, 10),\n            #ax=ax0\n           )","41094b20":"train_corr = train.drop(columns = 'loss').corr()","8499b147":"cols = train_corr.compare(test_corr)","f1d610a9":"col_list1 = []\nfor x in range(0,100,4):\n    col_list1.append(cols.columns[x])\nfor x in range(1,100,4):\n    col_list1.append(cols.columns[x])\n    \ncol_list2 = []\nfor x in range(2,100,4):\n    col_list2.append(cols.columns[x])\nfor x in range(3,100,4):\n    col_list2.append(cols.columns[x])\n    \ntrain_corr.compare(test_corr).style.background_gradient(subset= col_list1, cmap='coolwarm')\\\n                                    .background_gradient(subset= col_list2, cmap='summer')","72ae4458":"# Comparing all features in test and train\n'''train_corr.compare(test_corr).style.bar( subset= col_list1, color=['red'])\\\n                                        .bar( subset= col_list2, color=['yellow'])'''\n'''\ncols\nnew_cols = pd.DataFrame()\ncol_list1 = []\nfor x in range(0,100,4):\n    new_cols[str(cols.columns[x])[:-9]] =  cols[cols.columns[x]] -  cols[cols.columns[x+1]] \n    x = x+2\n    new_cols[str(cols.columns[x])[:-9]] =  cols[cols.columns[x]] -  cols[cols.columns[x+1]] \n\n#new_cols = new_cols[new_cols > -0.01]\nnew_cols.style.bar(  color=['yellow', 'red'])'''","81ada29d":"fig = plt.figure(figsize=(16, 16), facecolor='#f6f5f5')\n\nnew_cols = new_cols[new_cols < 0.0001]\n#new_cols.style.bar(  color=['yellow', 'red'])\nsns.heatmap(new_cols)","f05a54e4":"sns.heatmap(train_corr.compare(test_corr))","7a7d2908":"train = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\n\n\n#from sklearn.model_selection import train_test_split\n#train , train_extra = train_test_split(train , test_size = 0.9)\n\ntrain = train.drop( columns = 'id')\ntrain\n\n\npred_col = 'loss'\n\ny_train = train[pred_col]\nx_train = train.drop(columns = pred_col)\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nss = StandardScaler()\nmm = MinMaxScaler()\n\nfeatures = [f'f{i}' for i in range(100)]\nmm_x_train = mm.fit_transform(x_train[features])\nmm_test = mm.transform(test[features])","b0b7625b":"mm_x_train = pd.DataFrame(mm_x_train , columns = [features])","2642f661":"log_features = ['f4' , 'f6' , 'f15' , 'f22' , 'f28' , 'f35' , 'f44']\n\nprint(x_train[log_features].describe())\n\nindex = 0\nplt.figure(figsize = (16,14))\n    \nfor z in log_features:\n    plt.subplot(4,4,index+1)\n    sns.histplot(x_train[z])\n    index += 1\nplt.xlabel('Before log scaling', ha='center', fontweight='bold')\n \n    \nindex = 0\nplt.figure(figsize = (16,14))\n    \nfor z in log_features:\n    plt.subplot(4,4,index+1)\n    sns.histplot(np.log(x_train[z]))\n    index += 1\nplt.xlabel('After log scaling', ha='center', fontweight='bold')","74dac2f9":"sns.heatmap(train[log_features + ['loss']].corr() , annot = True,\n                       cmap=sns.diverging_palette(240, 10),)","1c4bc619":"bell_features = ['f29' , 'f23' , 'f31' , 'f36']\n    \nindex = 0\nplt.figure(figsize = (16,14))\n    \nfor z in bell_features:\n    plt.subplot(4,4,index+1)\n    sns.histplot(x_train[z])\n    index += 1\nplt.xlabel('Bell shaped', ha='center', fontweight='bold')\n ","12decd71":"sns.heatmap(train[bell_features + ['loss']].corr() , annot = True,\n                       cmap=sns.diverging_palette(240, 10),)","9061c0db":"two_peak_features = ['f27' , 'f32' , 'f94' , 'f51']\n    \nindex = 0\nplt.figure(figsize = (16,14))\n    \nfor z in two_peak_features:\n    plt.subplot(4,4,index+1)\n    sns.histplot(x_train[z])\n    index += 1\nplt.xlabel('two_peak_features', ha='center', fontweight='bold')","7897514e":"sns.heatmap(mm_x_train[two_peak_features].corr() , annot = True,\n                       cmap=sns.diverging_palette(240, 10),)","ff70cf58":"x_train['cat_f27'] = x_train['f27']","b8fdc70e":"pd.cut(x_train['f27'] , bins = [20000 , 30000] , labels = ['cat2']) #.describe()","e8b39266":"#train , train_extra = train_test_split(train , test_size = 0.9)\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nss = StandardScaler()\nmm = MinMaxScaler()\n\nfeatures = [f'f{i}' for i in range(100)]\nx_train = mm.fit_transform(x_train[features])\ntest = mm.transform(test[features])\n","03236259":"import xgboost as xgb\nfrom xgboost import XGBRegressor ,XGBClassifier\n\nxgb_params = {'n_estimators': 1800,\n 'subsample': 0.8,\n 'colsample_bytree': 0.9,\n 'eta': 0.008353853073431708,\n 'reg_alpha': 24,\n 'reg_lambda': 74,\n 'max_depth': 11,\n 'min_child_weight': 9,\n 'tree_method': 'gpu_hist',\n 'random_state': 42}\n\nmodel_xgboost = XGBRegressor( **xgb_params) # define\n\nprint(model_xgboost.objective)\nprint(model_xgboost)\n\nmodel_xgboost.fit(x_train, y_train) #fit\n\npreds = model_xgboost.predict(test) #predict \nprint(preds.shape)\npreds\n\ndf_sub = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\ndf_sub['loss'] = preds\ndf_sub.to_csv('submission.csv' , index = False)\ndf_sub.head(10)","13308452":"print(model_xgboost.feature_importances_)","7aaf4382":"plt.figure(figsize = (15,3))\nplt.bar(range(len(model_xgboost.feature_importances_)), model_xgboost.feature_importances_ )\nplt.show()","c0e405ef":"# Feature correlation in Test and Train data","6fdb3a32":"## [TPS-AUG] Exploratory data analysis\n\n- Input - 100 features\n- Input type - Continous variables\n- Output - 1 feature\n- Output type - Continous\n\n- Type of problem - Regression","4765b3c7":"# Baseline model - Xgboost + Optuna","f3d2712b":"All features seem to be  Continous type.","27fb9cdb":"# Transforming features - MinMaxScaler","45f6128b":"# Looking at features that are most correlated in Train data","4ecfd83e":"# Comparing all features in test and train - least variance\n\nLets look at correlation matrix of Test and train data, then try to look at areas that have maximum and minimum difference.\n\nThis might be useful in choosing features that have lesser variance in test and train data. ","4a4cc485":"# Lets compare the correlation matrix for Train and test","0015f21f":"## Train data","6fd5b95f":"# Feature observation\n\n- Features with exponential decay curve f4,f6, f15, f22, f28 , f35, f44 , f49 , f52, f56, f59, f91, f89, f75 \n     - Log scaling can be used\n- Features with bell shaped curve - 29, f23, f31, f36, f68, f61 , f97\n- Several feature has 2 peaks - f27, f32, f94, f51 \n- Features with multiple peaks - f84, f32, f50","4b2278e5":"# Feature importance","d039c02b":"# Heatmap of the comparison","e9b9a226":"# If you find this helpful, please upvote","fcf4a46b":"## Now, lets compare Value counts in Train and test","f250beb4":"## Scaling & Visualization\n\nTaken from amazing notebook by @subinium - [link](https:\/\/www.kaggle.com\/subinium\/tps-aug-simple-eda)","1afddf9c":"# Converting two peaked features into categorical data","f05119a3":"These features also contain few negative values, so using log scaling might hurt that.","a043b158":"- total unique values for loss - 43 .\n- 90th quantile is 19.0 - Follows a shape of decay.","0a8ab82f":"# Work in progres","2a721bd9":"# Target columns"}}