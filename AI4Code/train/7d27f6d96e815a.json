{"cell_type":{"1d46777b":"code","ae5ffc0d":"code","30648b52":"code","77f0f6f6":"code","786c6677":"code","ffd57568":"code","c8b5e05b":"code","7c442d06":"code","56755a4d":"code","4c27a626":"code","c78c19a6":"code","8dd60713":"code","b271d07e":"code","266d18f8":"code","c73abfba":"code","5a066abd":"code","78fcc99f":"code","9a84b155":"code","603ad257":"code","7a4376ab":"code","f477fc20":"code","ea668229":"code","34d66a55":"code","3d04287f":"code","7c9bc352":"code","0afbfc7a":"code","9ac3ab18":"code","ef7b982b":"code","b8593d02":"code","1b639e30":"code","d9cb863d":"code","2e4a0388":"code","72cd9bce":"code","e30b2ab8":"code","656d04d5":"code","03b0fd42":"code","4c7eac4c":"code","12bff0aa":"code","08ac5450":"code","98beb687":"markdown","e849e886":"markdown","f9b7ea71":"markdown","4cfbdbe6":"markdown","69fa3023":"markdown","d957c13d":"markdown","8096d4f0":"markdown","77ba6e0a":"markdown","c8008643":"markdown","ab452590":"markdown","9de952de":"markdown","ddca8c07":"markdown","bef90e75":"markdown","90677530":"markdown","69cd1cc9":"markdown","53ea51e0":"markdown","69d035c3":"markdown","380dc170":"markdown","97c5b1da":"markdown","24ab887f":"markdown","4b67fbc7":"markdown","1a927b34":"markdown","00859e4d":"markdown","e48b7613":"markdown","efea82bd":"markdown","16811fa6":"markdown","54f1ce01":"markdown"},"source":{"1d46777b":"import pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n#import visual as vs","ae5ffc0d":"GTD = pd.read_csv('..\/input\/globalterrorismdb_0718dist.csv', engine = 'python')","30648b52":"GTD.shape","77f0f6f6":"GTD.head(5)","786c6677":"print(\"{}% of incidents occured at least one death.\".format(round(float(GTD[GTD.nkill > 0]['nkill'].count())\/GTD.nkill.notna().sum()*100,2)))","ffd57568":"plt.subplots(figsize=(18,6))\nax = sns.barplot(x=\"iyear\", y=\"nkill\", data=GTD, estimator = sum, palette='RdYlGn_r',edgecolor=sns.color_palette('dark',7)).set_title('Number of deaths by year')\nplt.xticks(rotation=90)","c8b5e05b":"print(\"GTD length before: %.2d \" % len(GTD))\nprint(\"Avoiding rows with missing values of deaths...\")\nGTD = GTD[GTD['nkill'].notnull()]\nGTD['death'] = np.where(GTD.nkill > 0, 1, 0)\nGTD = GTD.drop(['nkill'], axis = 1)\nprint(\"GTD length after: %.2d \" % len(GTD))","7c442d06":"plt.subplots(figsize=(18,8))\nax = sns.countplot(x='region_txt', hue='death', data=GTD)\nax.set_title('Count of death and not death occurrence by region')\nplt.xticks(rotation=90)","56755a4d":"plt.subplots(figsize=(18,8))\nax = sns.countplot(x='attacktype1_txt', hue='death', data=GTD)\nax.set_title('Count of death and not death occurrence by Attack type')\nplt.xticks(rotation=90)","4c27a626":"columns = GTD.columns\npercent_missing = GTD.isnull().sum() * 100 \/ len(GTD)\nunique = GTD.nunique()\ndtypes = GTD.dtypes\nmissing_value_data = pd.DataFrame({'column_name': columns,\n                                 'percent_missing': percent_missing,\n                                 'unique': unique,\n                                 'types': dtypes})\nmissing_value_data = missing_value_data[missing_value_data['percent_missing']>0]\nmissing_value_data=missing_value_data.sort_values(by=['percent_missing'], ascending=False)\nplt.subplots(figsize=(20,10))\nax = sns.barplot(x=\"column_name\", y=\"percent_missing\", hue='types', data=missing_value_data)\nax.axhline(50, ls='--', color = 'r')\nax.text(90,51,\"50% of missing values\", color = 'r')\nax.set_title(\"Percentage of Missing Values by column\")\nplt.xticks(rotation=90)","c78c19a6":"def missing_values(data,mis_min):\n    columns = data.columns\n    percent_missing = data.isnull().sum() * 100 \/ len(data)\n    unique = data.nunique()\n    missing_value_data = pd.DataFrame({'column_name': columns,\n                                 'percent_missing': percent_missing,\n                                 'unique': unique})\n    missing_drop = list(missing_value_data[missing_value_data.percent_missing>mis_min].column_name)\n    return(missing_drop)","8dd60713":"print(\"Number of features before dropping columns with >50%% of NAN: %.1d\" % GTD.shape[1])\nGTD['natlty1'].fillna(GTD['country'], inplace = True)\n\nmissing_drop = missing_values(GTD,50)\nGTD = GTD.drop(missing_drop, axis=1)\nGTD = GTD.drop(columns = ['nkillter'])\nprint(\"Number of features after dropping columns with >50%% of NAN: %.1d\" % GTD.shape[1])","b271d07e":"mode_fill = ['nwound','longitude','latitude','weapsubtype1','weapsubtype1_txt','targsubtype1','targsubtype1_txt','natlty1_txt','guncertain1','ishostkid', 'specificity','doubtterr','multiple', 'target1', 'city', 'provstate']\nfor col in mode_fill:\n    GTD[col].fillna(GTD[col].mode()[0], inplace=True)\n\nGTD['nperps'].fillna(GTD['nperps'].median(), inplace=True)\nGTD['nperpcap'].fillna(GTD['nperpcap'].median(), inplace=True)\nGTD['nwoundte'].fillna(GTD['nwoundte'].median(), inplace=True)\nGTD['nwoundus'].fillna(GTD['nwoundus'].median(), inplace=True)\nGTD['nkillus'].fillna(GTD['nkillus'].median(), inplace=True)\nGTD = GTD.drop(columns = ['weapdetail', 'scite1', 'summary', 'corp1'])\nGTD['claimed'].fillna(0, inplace=True)","266d18f8":"print((GTD.isnull().sum(axis=0)\/len(GTD)*100).sort_values(ascending=False).head(5))\nprint(\"Missing values successfully treated!!!\")","c73abfba":"GTD['suicide'] = GTD.suicide.astype('object')","5a066abd":"num_features = ['nperps','nkillus','nwound','nwoundus', 'nwoundte']\nmatplotlib.style.use('seaborn')\nGTD[num_features].hist(figsize=(16,10))","78fcc99f":"from scipy.stats import skew\nimport sys\n\nskewed_feat = GTD[num_features].apply(lambda x: skew(x))\nskewed_feat = skewed_feat[skewed_feat > 0.75]\nskewed_feat = skewed_feat.index\n\nGTD[skewed_feat] = np.log1p(GTD[skewed_feat])","9a84b155":"GTD[skewed_feat].hist(figsize=(16,10))","603ad257":"duplicated_columns = [col for col in GTD.columns if \"_txt\" in col]\nprint(\"There are %.2d duplicated columns to be removed\"%(len(duplicated_columns)))\nGTD = GTD.drop(duplicated_columns, axis=1)","7a4376ab":"cat_features = GTD.dtypes[GTD.dtypes == 'object'].index\n\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\nfor col in cat_features:\n    GTD[col] = le.fit_transform(GTD[col])","f477fc20":"corr_matrix = GTD.corr()\nabs(corr_matrix['death']).sort_values(ascending=False).head(10)\n\nmask = np.zeros_like(corr_matrix, dtype = np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.subplots(figsize=(25,10))\ncmap = sns.diverging_palette(0,240, as_cmap = True)\nsns.heatmap(corr_matrix, mask=mask, cmap=cmap, vmin = -.3, vmax=.3, center=0, square=True, linewidths=.3, cbar_kws={\"shrink\": .5})\n\nGTD[GTD == np.inf] = np.nan\nGTD.fillna(GTD.mean(), inplace = True)","ea668229":"from sklearn.model_selection import train_test_split\n\nincome = GTD['death']\n\n# Split the 'features' and 'income' data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(GTD.drop(['death'], axis=1),\n                                                    income,\n                                                    shuffle=True,\n                                                    test_size = 0.2,\n                                                    random_state = 43)\n\n# Show the results of the split\nprint(\"Training set has {} samples.\".format(X_train.shape[0]))\nprint(\"Testing set has {} samples.\".format(X_test.shape[0]))","34d66a55":"from sklearn.metrics import fbeta_score, accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nDTC = DecisionTreeClassifier(random_state = 43)\n\nDTC = DTC.fit(X_train, y_train)\nbench_acc = accuracy_score(y_test, DTC.predict(X_test))\nbench_fsc = fbeta_score(y_test, DTC.predict(X_test), 0.5)\n\nprint('-'*40)\nprint(\"Benchmark Model:\")\nprint(\"Accuracy: %.2f\" %round(bench_acc*100,2))\nprint(\"F_score: %.2f\" %round(bench_fsc*100,2))\nprint('-'*40)","3d04287f":"import time\n\ndef train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n    \n    results = {}\n\n    start = time.time()\n    learner = learner.fit(X_train[:sample_size], y_train[:sample_size])\n    end = time.time()\n    \n    # Training time\n    results['train_time'] = end - start\n        \n    # TODO: Get the predictions on the test set(X_test),\n    #       then get predictions on the first 300 training samples(X_train) using .predict()\n    start = time.time() # Get start time\n    predictions_test = learner.predict(X_test)\n    predictions_train = learner.predict(X_train[:300])\n    end = time.time() # Get end time\n    \n    # TODO: Calculate the total prediction time\n    results['pred_time'] = end - start\n            \n    # TODO: Compute accuracy on the first 300 training samples which is y_train[:300]\n    results['acc_train'] = accuracy_score(y_train[:300], predictions_train)\n        \n    # TODO: Compute accuracy on test set using accuracy_score()\n    results['acc_test'] = accuracy_score(y_test, predictions_test)\n    \n    # TODO: Compute F-score on the the first 300 training samples using fbeta_score()\n    results['f_train'] = fbeta_score(y_train[:300], predictions_train, beta = 0.5, average = 'weighted')\n        \n    # TODO: Compute F-score on the test set which is y_test\n    results['f_test'] = fbeta_score(y_test, predictions_test,  beta = 0.5, average = 'weighted')\n       \n    # Success\n    print(\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n        \n    # Return the results\n    return results","7c9bc352":"# TODO: Import the three supervised learning models from sklearn\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.svm import SVC\n\n# TODO: Initialize the three models\nclf_A = KNeighborsClassifier()\nclf_B = AdaBoostClassifier(random_state = 41)\nclf_C = RandomForestClassifier(random_state = 41)\n#clf_D = GaussianNB()\n\n# TODO: Calculate the number of samples for 1%, 10%, and 100% of the training data\n# HINT: samples_100 is the entire training set i.e. len(y_train)\n    # HINT: samples_10 is 10% of samples_100 (ensure to set the count of the values to be `int` and not `float`)\n# HINT: samples_1 is 1% of samples_100 (ensure to set the count of the values to be `int` and not `float`)\nsamples_100 = len(y_train)\nsamples_10 = int(samples_100\/10)\nsamples_1 = int(samples_100\/100)\n\n\n# Collect results on the learners\nresults = {}\nfor clf in [clf_A, clf_B, clf_C]:\n    clf_name = clf.__class__.__name__\n    results[clf_name] = {}\n    for i, samples in enumerate([samples_1, samples_10, samples_100]):\n        results[clf_name][i] = train_predict(clf, samples, X_train, y_train, X_test, y_test)\naccuracy = bench_acc\nfbeta = bench_fsc\n# Run metrics visualization for the three supervised learning models chosen\n#vs.evaluate(results, accuracy, fbeta)","0afbfc7a":"for k, v in results.items():\n    print('-'*40)\n    print(\"Model: %s\" %(k))\n    for i in v:\n        print(\"Accuraccy: %.4f \\n Fscore_test: %.4f\" %(v[i]['acc_test'], v[i]['f_test']))","9ac3ab18":"from sklearn.feature_selection import SelectFromModel\n\nfeat_labels = GTD.columns\nsfm = SelectFromModel(clf, threshold=0.02)\nsfm.fit(X_train, y_train)\nimportant_feat = [feat_labels[col] for col in sfm.get_support(indices=True)]\n#for feature_list_index in sfm.get_support(indices=True):\n#    print(feat_labels[feature_list_index])\nimportant_feat.append('death')\n\nX_important_test = sfm.transform(X_test)\nX_important_train = sfm.transform(X_train)\n\nprint('-'*40)\nprint('Feature Selection:')\nprint('before')\nprint(X_train.shape)\nprint('after')\nprint(X_important_train.shape)\nprint('-'*40)","ef7b982b":"rf = RandomForestClassifier(random_state = 41)\nrf.fit(X_important_train, y_train)\nimportant_pred = rf.predict(X_important_test)\n\nprint(\"Accuracy after Feature Selection: %.2f\" %round((accuracy_score(y_test, important_pred)*100),2))\nprint(\"F_score after Feature Selection: %.2f\" %round((fbeta_score(y_test, important_pred, 0.5)*100),2))","b8593d02":"X_intermediate, X_test, y_intermediate, y_test = train_test_split(GTD.drop(['death'], axis=1),\n                                                                  income,\n                                                                  shuffle=True,\n                                                                  test_size = 0.2,\n                                                                  random_state = 43)\n\nX_train, X_validation, y_train, y_validation = train_test_split(X_intermediate,\n                                                                y_intermediate,\n                                                                shuffle=False,\n                                                                test_size=0.2,\n                                                                random_state=43)\n# delete intermediate variables\ndel X_intermediate, y_intermediate\n\n# print proportions\nprint('train: {}% | validation: {}% | test {}%'.format(round(float(len(y_train))\/len(income),2),\n                                                       round(float(len(y_validation))\/len(income),2),\n                                                       round(float(len(y_test))\/len(income),2)))","1b639e30":"rf.fit(X_train, y_train)\n\nprint('-'*40)\nprint(\"Train:\")\nprint(\"Accuracy: %.2f\" %round((accuracy_score(y_train, rf.predict(X_train))*100),2))\nprint(\"F_score: %.2f\" %round((fbeta_score(y_train, rf.predict(X_train), 0.5)*100),2))\nprint('-'*40)\nprint(\"Validation:\")\nprint(\"Accuracy: %.2f\" %round((accuracy_score(y_validation, rf.predict(X_validation))*100),2))\nprint(\"F_score: %.2f\" %round((fbeta_score(y_validation, rf.predict(X_validation), 0.5)*100),2))\nprint('-'*40)\nprint(\"Test:\")\nprint(\"Accuracy: %.2f\" %round((accuracy_score(y_test, rf.predict(X_test))*100),2))\nprint(\"F_score: %.2f\" %round((fbeta_score(y_test, rf.predict(X_test), 0.5)*100),2))\nprint('-'*40)","d9cb863d":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 2, stop = 20, num = 5)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(1, 11, num = 5)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","2e4a0388":"\"\"\"rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n\n# Fit the random search model\nrf_random.fit(X_train, y_train)\nbest_random = rf_random.best_estimator_\nbest_random\"\"\"","72cd9bce":"best_random = RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=4, min_samples_split=5,\n            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=1,\n            oob_score=False, random_state=41, verbose=0, warm_start=False)","e30b2ab8":"new_rf = best_random\nnew_rf.fit(X_train, y_train)\n\nprint('-'*40)\nprint(\"Test:\")\nprint(\"Accuracy: %.2f\" %round((accuracy_score(y_test, new_rf.predict(X_test))*100),2))\nprint(\"F_score: %.2f\" %round((fbeta_score(y_test, new_rf.predict(X_test), 0.5)*100),2))\nprint('-'*40)","656d04d5":"from sklearn import metrics\n\nconf_matrix = metrics.confusion_matrix(y_test, new_rf.predict(X_test))\nTP = conf_matrix[1, 1]\nTN = conf_matrix[0, 0]\nFP = conf_matrix[0, 1]\nFN = conf_matrix[1, 0]\nconf_mat_df = pd.DataFrame(conf_matrix).astype(int)\nconf_mat_df = conf_mat_df.rename(index={0:\"no_death_true\"})\nconf_mat_df = conf_mat_df.rename(index={1:\"death_true\"})\nconf_mat_df = conf_mat_df.rename(columns={0:\"no_death_pred\"})\nconf_mat_df = conf_mat_df.rename(columns={1:\"death_pred\"})\nprint(\"Given death occurrance in attacks, the model mistakenly predicted no deathes in {}% cases.\".format(round(float(FN)\/(FN+TP)*100,2)))","03b0fd42":"grid_kws = {\"height_ratios\": (.8, .05), \"hspace\": .3}\nax = sns.heatmap(conf_mat_df, annot = True, fmt='d', cmap=\"YlGnBu\",  cbar_kws={\"orientation\": \"horizontal\"})\nax.set_title(\"Confusion Matrix\")","4c7eac4c":"from sklearn.datasets import load_digits\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","12bff0aa":"plt.style.use(['seaborn-whitegrid','seaborn-poster'])\ny_pred_proba = new_rf.predict_proba(X_test)[::,1]\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr, tpr,color='g', label = \"ROC curve (AUC = \"+str(round(auc,2))+')')\nplt.plot([0,1],[0,1], linestyle = '--', lw=2, color ='r', label='Chance', alpha=.8)\nplt.legend(loc=4)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC curve - Perfomance of the final classification model\")\nplt.show()","08ac5450":"# Feature Importance\n\nX = GTD.drop(['death'], axis=1)\ny = GTD['death']\n\nimportances = new_rf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in new_rf.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfeat_importances = [(feature, round(importance, 3)) for feature, importance in zip(GTD.columns, importances)]\nfeat_importances = sorted(feat_importances, key = lambda x: x[1], reverse = True)\nfeat_importances\n\n#for f in range(X.shape[1]):\n#    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), X.columns[indices], rotation = 90)\nplt.xlim([-1, X.shape[1]])\nplt.show()\n# display the relative importance of each attribute\n#print(model.feature_importances_)","98beb687":"Filling remaining columns with mode and median values","e849e886":"The column `nkill` records the number of deaths in an attack.  The cell bellow picks just this feature avoiding its empty values and creating another binary feature `death` which records whether occurred or not any death in the attack.","f9b7ea71":"#### Validation Set","4cfbdbe6":"# Install libraries and load the dataset","69fa3023":"### Treating Missing Values","d957c13d":"The highlated cell below can be used to find out the best hyperparemeters of the model to improve its accuraccy. This cell normally runs in 3 minutes,","8096d4f0":"**When happens deathes in attacks, how often the model predicts no deathes?**\n\nThis is the most critical case because the huge problem is whether the model predicts Negative for death occurrence, but the truth is Positive.","77ba6e0a":"### Training and Predicting Pipeline","c8008643":"Avoid features with more than 50% of missing values","ab452590":"# Exploring the Dataset","9de952de":"# Evaluation Model Perfomance","ddca8c07":"The entire dataset has 135 features and about 180k rows of data","bef90e75":"### Correlation and Most importante Features","90677530":"### Tuning the Random Forest Classifier","69cd1cc9":"### Label Encoder","53ea51e0":"### Shuffle and Split the data","69d035c3":"### Transform skewed continuos features and Normalizing Numerical Features","380dc170":"**Correlation Matrix**","97c5b1da":"Avoiding features with importance below 2%","24ab887f":"### Avoiding duplicated features","4b67fbc7":"### Creating the target variable","1a927b34":"Number of remaining Features","00859e4d":"Number of deaths by year from 1970 to 2017:","e48b7613":"#### Feature Importances","efea82bd":"The Global Terrorism Database (GTD) is maintained by the National Consortium for the Study of Terrorism and Responses to Terrorism (START). The current GTD is the product of several phases of data collection efforts, each relying on publicly available, unclassified source materials. These include media articles and electronic news archives, and to a lesser extent, existing data sets, secondary source materials such as books and journals, and legal documents.\n\n**GTD Definition of Terrorism**\n\nThe GTD defines a terrorist attack as the threatened or actual use of illegal force and violence by a non-state actor to attain a political, economic, religious, or social goal through fear, coercion, or intimidation. In practice this means in order to consider an incident for inclusion in the GTD, all three of the following attributes must be present:\n\n- The incident must be intentional;\n\n- The incident must entail some level of violence or immediate threat of violence;\n\n- The perpetrators of the incidents must be sub-national actors;","16811fa6":"Tunning the hyperparemeters of the model","54f1ce01":"### Benchmark Model"}}