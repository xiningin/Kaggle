{"cell_type":{"f1d22e23":"code","f498187d":"code","51b9eb7c":"code","584feb87":"code","7166bb9a":"code","1a98a876":"code","680e097b":"code","2f35ce31":"code","b131e510":"code","eaf23242":"code","523299a7":"code","28968e0f":"code","f8c7927b":"code","e8e31bcd":"code","2486491d":"code","0ae709a2":"markdown","2db1de03":"markdown","2a72d023":"markdown","e92b1c6c":"markdown","f6beb6dc":"markdown","4176a701":"markdown","37f263c4":"markdown","79b4343b":"markdown","6168adfc":"markdown"},"source":{"f1d22e23":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import ParameterGrid\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom lightgbm import LGBMRegressor\n\nimport optuna\n        \ninput_path = Path('\/kaggle\/input\/tabular-playground-series-feb-2021\/')","f498187d":"train = pd.read_csv(input_path \/ 'train.csv', index_col='id')\ntest = pd.read_csv(input_path \/ 'test.csv', index_col='id')\nsubmission = pd.read_csv(input_path \/ 'sample_submission.csv', index_col='id')\ntarget = train.pop('target')","51b9eb7c":"for c in train.columns:\n    if train[c].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values) + list(test[c].values))\n        train[c] = lbl.transform(train[c].values)\n        test[c] = lbl.transform(test[c].values)","584feb87":"display(train.head())","7166bb9a":"X_train, X_valid, y_train, y_valid = train_test_split(train, target, test_size=0.1, random_state=0)","1a98a876":"# Base model\n\nmodel = LGBMRegressor(random_state=0)\nmodel.fit(X_train, y_train)","680e097b":"y_pred = model.predict(X_valid)\nrmse = mean_squared_error(y_valid, y_pred, squared=False)\nprint('RMSE =', f'{rmse:0.5f}')","2f35ce31":"# Grid search\n\n#grid = {\n#    'metric': ['rmse'],\n#    'random_state': [0],\n#    'n_estimators': [200],\n#    'learning_rate': [0.001, 0.01, 0.1, 1.0],\n#    'reg_lambda': [0.001, 0.01, 0.1, 1.0, 10.0],\n#    'reg_alpha': [0.001, 0.01, 0.1, 1.0, 10.0],\n#    'colsample_bytree': [0.1, 0.2, 0.3, 0.4, 0.5],\n#    'min_child_samples': [10, 20, 40, 75, 100, 200, 300],\n#    'max_depth': [5, 10, 25, 50, 100],\n#    'num_leaves': [40, 50, 60, 80, 100],\n#}","b131e510":"#scores = []\n#for g in ParameterGrid(grid):\n#    model = LGBMRegressor()\n#    model.set_params(**g)\n#    model.fit(X_train, y_train)\n#    y_pred = model.predict(X_valid)\n#    score = mean_squared_error(y_valid, y_pred, squared=False)\n#    scores.append(score)\n#    print('RMSE =', f'{score:0.5f} ', 'Parameters:', g)\n#best_idx = np.argmin(scores)\n#print('Best score: ', scores[best_idx], ParameterGrid(grid)[best_idx])","eaf23242":"def objective(trial):\n    params = {\n        'metric': 'rmse',\n        'random_state': 0,\n        'n_estimators': trial.suggest_categorical('n_estimators', [10000]),\n        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.1, 0.2, 0.3, 0.4, 0.5]),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n        'max_depth': trial.suggest_int('max_depth', 6, 127),\n        'num_leaves': trial.suggest_int('num_leaves', 31, 128),\n        'cat_feature': [x for x in range(10)],\n        'cat_smooth': trial.suggest_int('cat_smooth', 10, 100),\n        'cat_l2': trial.suggest_int('cat_l2', 1, 20)\n        #'device':'gpu',        # comment this line if GPU is off\n        #'gpu_platform_id': 0,  # comment this line if GPU is off\n        #'gpu_device_id': 0,    # comment this line if GPU is off\n    }\n    model = LGBMRegressor(**params) \n    model.fit(X_train, y_train, eval_set=[(X_valid,y_valid)], early_stopping_rounds=1000, verbose=0)\n    y_pred = model.predict(X_valid)\n    rmse = mean_squared_error(y_valid, y_pred, squared=False)\n    \n    return rmse","523299a7":"%%time\nstudy = optuna.create_study(direction='minimize',sampler=optuna.samplers.TPESampler(seed=0))\nstudy.optimize(objective, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best parameters:', study.best_trial.params)\nprint('Best RMSE:', study.best_trial.value)","28968e0f":"optuna.visualization.plot_optimization_history(study)","f8c7927b":"params = study.best_params\nparams['random_state'] = 0\nparams['n_estimators'] = 10000","e8e31bcd":"n_folds = 10\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=0)\ny_pred = np.zeros(test.shape[0])\n\nfor fold, (train_index, valid_index) in enumerate(kf.split(train, target)):\n    print(\"Running Fold {}\".format(fold + 1))\n    X_train, X_valid = pd.DataFrame(train.iloc[train_index]), pd.DataFrame(train.iloc[valid_index])\n    y_train, y_valid = target.iloc[train_index], target.iloc[valid_index]\n    model = LGBMRegressor(**params)\n    model.fit(\n        X_train,\n        y_train,\n        eval_set=[(X_valid, y_valid)],\n        early_stopping_rounds=1000,\n        verbose=0,\n    )\n    y_pred += model.predict(test) \/ n_folds\n\nprint(\"Done!\")","2486491d":"submission['target'] = y_pred\nsubmission.to_csv('lgbm_optuna_num_enc.csv')","0ae709a2":"# Load libraries and data","2db1de03":"# Recover best parameters found and build final predictions","2a72d023":"### Please feel free to add comments and suggestions. Thanks! \ud83d\ude0a","e92b1c6c":"Initial parameter tuning with grid search. I ran each parameter individually to estimate bounds for the search with Optuna.","f6beb6dc":"# Set objective function for Optuna with parameters and their ranges\n\nIn order to search for the best parameter values, I created the following function with the parameters I wanted to be tuned. To set the ranges, I did some preliminary tests varying them individually. Finally, I put everything together as shown below.","4176a701":"# Encode categorical variables as integers","37f263c4":"# LightGBM with parameter tunning using Optuna\n\nThis notebook uses Optuna to tune LightGBM parameters. Here I use numerical encoding (instead of one-hot encoding) for the categorical variables.","79b4343b":"# Visualize optimization history","6168adfc":"# Preliminary tests to estimate ranges for the parameters"}}