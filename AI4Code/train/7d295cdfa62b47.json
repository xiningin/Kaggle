{"cell_type":{"2035b086":"code","3a729b5b":"code","b71a102a":"code","cf9a3188":"code","a524c8d1":"code","11339a28":"code","533ec9d6":"code","15f9ff64":"code","46f81ffa":"code","39b371ff":"code","92607e72":"code","e7376422":"code","c2a704c9":"code","79f19d7d":"markdown","b2b2c95d":"markdown","0521682f":"markdown","9439f79a":"markdown","f8b86c25":"markdown","ba081b50":"markdown","faa23d42":"markdown","afe0ed90":"markdown","a9d851d8":"markdown","05337a4d":"markdown","4c1fe92c":"markdown"},"source":{"2035b086":"# hide input\n\nimport os\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\n\nplt.rcParams.update({'font.size': 18, \"font.family\": \"Times\"})","3a729b5b":"from transformers import AutoTokenizer, pipeline\nfrom transformers import AutoModelForSeq2SeqLM\nfrom transformers import AutoModelForSequenceClassification\n\ndef zeroshot_classifier():\n    # Only works with English text\n    tokenizer = AutoTokenizer\\\n                    .from_pretrained(\"facebook\/bart-large-mnli\")\n    model = AutoModelForSequenceClassification\\\n                    .from_pretrained(\"facebook\/bart-large-mnli\")\n    return pipeline(task='zero-shot-classification',\n                    model=model, tokenizer=tokenizer)\n\n\n# we will need in the second example\ndef translator(src: str, dest: str):\n    src = src.lower()\n    dest = dest.lower()\n    tokenizer = AutoTokenizer\\\n                    .from_pretrained(f\"Helsinki-NLP\/opus-mt-{src}-{dest}\")\n    model = AutoModelForSeq2SeqLM\\\n                    .from_pretrained(f\"Helsinki-NLP\/opus-mt-{src}-{dest}\")\n    return pipeline(task='translation',\n                    model=model, tokenizer=tokenizer)\n","b71a102a":"pipe = zeroshot_classifier()\n\ncandidate_labels = ['causal reasoning', \n                    'evaluation reasoning', \n                    'personal impression']\n\ninput_texts = [\"Use of coal increases pollution\", \n              \"Use of wind power may not be reliable throughout the year\",\n              \"I prefer coal power\"]\n\nwith torch.no_grad():\n    predictions = pipe(input_texts, \n                       candidate_labels=candidate_labels)","cf9a3188":"# hide input\n\nfig, ax = plt.subplots(nrows=3, \n                       figsize=(6,10))\n\nfor i, p in enumerate(predictions):\n    sns.barplot(y='labels', x='scores', data=p, ax=ax[i], \n                order=candidate_labels)\n    ax[i].set_title(p['sequence'][:50])\n\nfig.tight_layout()","a524c8d1":"df = pd.read_csv('\/kaggle\/input\/gosuai-dota-2-game-chats\/dota2_chat_messages.csv', nrows=100)\ndf['text'] = df['text'].fillna('')\n\nprint('Mean length of text', df['text'].apply(lambda x: len(x)).mean())\nprint(df.head(15))","11339a28":"%%time\n\ndf_sample = df.sample(50)","533ec9d6":"translate = translator('ru', 'en')","15f9ff64":"%%time\n\nwith torch.no_grad():\n    translated_text = translate([t[:100] for t in list(df_sample['text'])])\ndf_sample['text_en'] = [t['translation_text'] for t in translated_text]","46f81ffa":"df_sample.head(10)","39b371ff":"candidate_labels = ['chitchat', \n                    'game features', \n                    'coordination', \n                    'toxic offense', \n                    'gender discrimination', \n                    'religious intolerance', \n                    'racism']\n\npipe = zeroshot_classifier()","92607e72":"%%time\n\nwith torch.no_grad():\n    predictions = pipe(list(df_sample['text_en']), \n                       candidate_labels=candidate_labels)\npredictions[0]","e7376422":"# hide input\n\nlabels = []\nfor p in predictions:\n    labels.append(p['labels'][np.argmax(p['scores'])])\n\ndf_sample['label'] = labels\ndf_sample","c2a704c9":"# hide input\n\nhow_many_to_plot = 10\n\nfig, ax = plt.subplots(nrows=how_many_to_plot, \n                       figsize=(6,30))\n\nfor i, p in enumerate(random.sample(predictions, how_many_to_plot)):\n    sns.barplot(y='labels', x='scores', data=p, ax=ax[i], \n                order=candidate_labels)\n    ax[i].set_title(p['sequence'][:50])\n\nfig.tight_layout()","79f19d7d":"## Imports and utilities","b2b2c95d":"# Introduction of zero-shot classification w\/ chats","0521682f":"### b. Translate to English","9439f79a":"* Original author: Adelson de Araujo\n* Last update: 08\/12\/2021","f8b86c25":"## Concluding remarks\n\nIn case you want to know more about zero-shot learning, I encourage you to go through the following material:\n\n* https:\/\/joeddav.github.io\/blog\/2020\/05\/29\/ZSL.html\n* https:\/\/arxiv.org\/abs\/1909.00161\n* https:\/\/www.deeplearningbook.org\/contents\/representation.html (Section 15.2)\n* https:\/\/www.aaai.org\/Papers\/AAAI\/2008\/AAAI08-132.pdf\n\nAs a takeaway, I think a well-designed zero-shot classifier (with suitable candidate labels) can be a game-changing tool for several AI projects. \n\nOne use case is, for example, \"expert systems,\" where you generate these output probabilities for classes that you understand as intermediary features that you provide to a rule-based decision-making mechanism. Then you can write things like \"if causal_reasoning is high, do the action A; if evaluation_reasoning is high, do the action B.\" There has been some research opportunity with zero-shot learning as a feature engineering step because of this kind of system. Interestingly, one can automate understandable features computed from black-box models. Also, if you build `candidate_labels` using some theory is great because you have more evidence to back up design choices.\n\nThe main issue I see with this is how we can evaluate if the zero-shot labels are suitable. Well, one way to measure is by letting the model compute several labels, and then you or another person can label manually. With your labels and the zero-shot ones, you can use Cohen's Kappa as your agreement level. That, however, is not scalable when you are testing several `candidate_labels`.\n\nIf you see interesting use cases for zero-shot learning or want to interact\/discuss this notebook, please comment below!!","ba081b50":"## Example #1: Reasoning or personal impression?\n\nThis is an example taken from [Fiacco & Ros\u00e9 (2018)](https:\/\/dl.acm.org\/doi\/pdf\/10.1145\/3231644.3231655).\n\nSuppose you want to classify a text as a ***causal reasoning***, a ***evaluation reasoning*** or a ***personal impression***. \n\nIf someone says \n* \"Use of coal increases pollution\", we expect the label to be ***causal reasoning***.\n* \"Use of wind power may not be reliable throughtout the year\", we expect the label to be ***evaluation reasoning***.\n* \"I prefer coal power\", we expect the label to be ***personal impression***.\n\nOf course, these are not completely exclusive classes and could be better conceived to be exclusive, but suppose these are exclusive.\n\nWe can use a pre-trained model that is able to perform zero-shot learning and generate labels without any previous training data:","faa23d42":"### d. Generate and explore predictions","afe0ed90":"## Example #2: DOTA-2 in-game chats\n\nIf you want to go through a second example, I will use some data from DOTA-2 chats to classify them as one of the following `candidate_labels = ['chitchat', 'game features', 'coordination', 'toxic offense', 'gender discrimination', 'religious intolerance', 'racism']`. This data is in Russian, so we have a translation step in between that we may loose some information. Also we are not carrying too much in preprocessing steps, but they are indeed required for more serious projects. Also, it seems that there are zero-shot models in a few other languages available out there, such as French, Spanish, German, even Russian. See a list from HuggingFace [models](https:\/\/huggingface.co\/models?pipeline_tag=zero-shot-classification&sort=downloads).\n\n### a. Load the data","a9d851d8":"### c. Load zero-shot classifier from Huggingface's transformers library","05337a4d":"## Okay, I want to know more. What is zero-shot classification?\n\nTo be more accurate, zero-shot classification refers to the inference part of zero-shot learning, which is currently a hot research topic in ML literature of transer learning.\n\nAs good overview of what this is, [Ian Goodfellow's wrote in Quora the following answer](https:\/\/qr.ae\/pGl1ss).\n\n```\nZero-shot learning is being able to solve a task despite not having received any training examples of that task.\n```\n\nHe answered the question quite some time ago, so this is not a new thing. In Wikipedia, you will see that this has been researched at least since 2008.\n\nFurther, Joe Davison wrote [a comprehensive blog post](https:\/\/joeddav.github.io\/blog\/2020\/05\/29\/ZSL.html) about zero-shot learning, where he greatly explains successes in the field of transfer learning that allowed models to perform surprisingly well in several classification tasks.\n\n## Why I have not hearded about this before?\n\nA more accessible use of zero-shot classification (inference) for NLP practitioners has been materialized more recently particularly by the HuggingFace's [transformers](https:\/\/huggingface.co\/docs\/transformers\/index) library and the [models](https:\/\/huggingface.co\/models) page. New models are being proposed given that more interesting, robust, and semantically transferable [datasets](https:\/\/huggingface.co\/datasets) are available. More people are using pre-trained models and engaging in transfer learning pipelines. \n\n\n## Okay, but labeling without training samples is really doable?\n\nZero-shot learning is a particular form of transfer learning. That there are different ways to do the job, and techniques vary in computer vision and NLP. Of course, I still want to see more studies discussing interrater reliability with these kind of tools in a diverse set of scenarios to have a stronger argument on using it \"in the wild\".\n\nIn this notebook, we will walk through how you can put your hands in some unlabeled text data and label it automatically using models available from HuggingFace's `transformers`. \n\n\n## Do you know exactly how\/why does this work?\n\nI guess this answer depends a bit on the model you are using. Below, we test the BART model trained by facebook, and I suggest you read the [description of the model](https:\/\/huggingface.co\/facebook\/bart-large-mnli) we are using here. They answer this question quite clearly, but you must have known what \"NLI\" is.\n\nIf you want to read my own (shorter) explanation of their explanation, here it is:\n\nNLI stands for Natural Language Inference, and it refers to a particular classification task. Suppose two pieces of texts, a premise, and a hypothesis. The option of labels are *entailment* (when the hypothesis confirms the premise), *contradiction* (when the hypothesis denies the premise), and *neutral*. Check some examples [here]. There are some NLI datasets available; for example, SNLI and [MultiNLI](https:\/\/cims.nyu.edu\/~sbowman\/multinli\/) are the most famous ones I know of. Some people also call NLI's task the entailment classification task.\n\nThe MultiNLI dataset is enormous and enables robust algorithm architectures to produce very good models that can be transferred to other semantically similar tasks.\n\n\n## But how does models trained on NLI works for zero-shot classification?\n\nWith the entailment, neutral, and contradiction classes to match two pieces of text (premise and hypothesis), a lot of other classification tasks can be adapted to be semantically similar. For example, you can take an arbitrary input text and consider it as the premise. For each `item` in `candidate_labels` you want to explore with zero-shot, you create a hypothesis \"This sentence is about `{item}`\". Suppose that your model does a great job at the entailment task. If the model predict that relation as an entailment, that indicates a good match between your input text and this label. Would that make sense?  \n\nLet's go through another example to see it in action again, but in a more challenging context.","4c1fe92c":"In this notebook, I would like to make a rapid introduction to zero-shot classification for practitioners. We will cover what it is and explore a use case. After being positively surprised with my toy examples, I prepared this notebook. I wanted to share what I see as a potentially game-changing technique to apply machine learning when ground-truth labels are unavailable or are costly to be collected. \n\nThis is not a comprehensive tutorial, nor does it discuss efficient approaches. Still, it's more like a taste of zero-shot classification in practice for those with no experience with it. In the conclusion, I mention some takeaways that I see on how zero-shot classification cal help in feature engineering pipelines, since you can use its output of intermediary variables to other systems. These intermediary variables are arguably more explainable since you chose the candidate labels, and you can use them in simpler models that other people can scrutinize.\n\n\nFor now, go to the example #1 below!\n"}}