{"cell_type":{"94daed3a":"code","55691f57":"code","6c42dd81":"code","951de04c":"code","f27ad5c2":"code","87aa8669":"code","2740fb24":"code","0a183148":"code","8c0049b1":"code","669d42d8":"code","8d325c42":"code","7bbebb6d":"code","b37a227b":"code","124b3e6c":"code","b55cbd46":"code","b5c289ff":"code","500e2966":"code","d68479c0":"code","9c8d18be":"code","753de8c0":"code","68ea2bea":"code","cf713fd9":"code","5a89e67e":"code","708b7499":"code","cd9812a1":"code","d154c020":"code","620a6326":"code","023b411d":"code","94dc0298":"code","7f9c73f1":"code","858256e9":"code","ba594a52":"code","67f1e6d3":"code","a608119d":"code","b7e965c2":"code","d6cbaabc":"code","3ab89654":"code","b25a4e5a":"code","6f6ce5e1":"code","c1332615":"markdown","0c94b75b":"markdown","af6c7f8d":"markdown","e423eef8":"markdown","aa42545a":"markdown","a465bbda":"markdown","779fa0cd":"markdown","4f0c2847":"markdown","abd3c05e":"markdown","55e2deb9":"markdown","5ef437d4":"markdown","6f0c5a03":"markdown","b870ad16":"markdown","fd254a73":"markdown","2f8dd1a3":"markdown","cb6515b5":"markdown","67755b2e":"markdown","bcc7f3f6":"markdown","6fe46898":"markdown","99b3152a":"markdown","dd7657c3":"markdown","5906bef3":"markdown","114517da":"markdown","ebfb4db1":"markdown","0b2b0dfb":"markdown","47f2bfd8":"markdown","d620447c":"markdown","6736700e":"markdown","74e2048e":"markdown","3e1bad46":"markdown","8022c643":"markdown","cebc45eb":"markdown","9626d0d4":"markdown","2ccca07c":"markdown","a37fcd0c":"markdown"},"source":{"94daed3a":"import pandas as pd\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings('ignore') \n\nheart = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\nheart.head()","55691f57":"# installing fonts\n!pip install proplot","6c42dd81":"print('Count of Features per Data Type:')\nheart.dtypes.value_counts()","951de04c":"# Do we have duplicates?\nprint('Number of Duplicates:', len(heart[heart.duplicated()]))\n\n# Do we have missing values?\nprint('Number of Missing Values:', heart.isnull().sum().sum())\n","f27ad5c2":"heart.describe()","87aa8669":"heart['FastingBS'] = heart['FastingBS'].astype(str)\nheart['HeartDisease'] = heart['HeartDisease'].astype(str)","2740fb24":"# Defining plot design\ndef plot_design():\n    plt.xlabel('')\n    plt.ylabel('')\n    plt.yticks(fontsize=15, color='black')\n    plt.xticks(fontsize=15, color='black')\n    plt.box(False)\n    plt.title(i[1], fontsize=24, color='black')\n    plt.tight_layout(pad=5.0)","0a183148":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib as mpl\nimport proplot as pplt\n\n# Select categorical variables\ncateg = heart.select_dtypes(include=object).columns\n\n# Visualize\nfig, ax = plt.subplots(figsize =(15, 15))\nfig.patch.set_facecolor('#CAD5E0')\nmpl.rcParams['font.family'] = 'TeX Gyre Heros'\n\n# Loop columns\nfor i in (enumerate(categ)):\n    plt.subplot(4, 2, i[0]+1)\n    sns.countplot(y = i[1], data = heart, order=heart[i[1]].value_counts().index, palette='Greens_r', edgecolor='black')\n    plot_design()\n    plt.suptitle('Categorical Variables', fontsize=40)","8c0049b1":"# Select numerical variables\nnumeric = heart.select_dtypes(exclude=object).columns\n\n# Visualize\nfig, ax = plt.subplots(figsize =(15, 15))\nfig.patch.set_facecolor('#CAD5E0')\nmpl.rcParams['font.family'] = 'TeX Gyre Heros'\n\n# Loop columns\nfor i in (enumerate(numeric)):\n    plt.subplot(3, 2, i[0]+1)\n    sns.histplot(x = i[1], data = heart, color='#ECACAE', edgecolor='black')\n    plot_design()\n    plt.suptitle('Numerical Variables', fontsize=40)\n","669d42d8":"# Scaling features\nfrom sklearn.preprocessing import MinMaxScaler\n\nfor col in numeric:\n    heart[col] = MinMaxScaler().fit_transform(heart[[col]])","8d325c42":"fig, ax = plt.subplots(figsize =(15,9))\nax.patch.set_facecolor('#CAD5E0')\nfig.patch.set_facecolor('#CAD5E0')\nmpl.rcParams['font.family'] = 'TeX Gyre Heros'\n\nsns.boxplot(data = heart, ax=ax, palette='husl', orient=\"h\", linewidth=4);\n\n# Colors\nfor i,artist in enumerate(ax.artists):\n    col = artist.get_facecolor()\n    artist.set_edgecolor(col)\n    artist.set_facecolor('None')\n    for j in range(i*6,i*6+6):\n        line = ax.lines[j]\n        line.set_color(col)\n        line.set_mfc(col)\n        line.set_mec(col)\n\n# Remove ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n\n# Remove axes splines\nfor i in ['top', 'bottom', 'left', 'right']:\n    ax.spines[i].set_visible(False)\n\n# Remove grid\nax.grid(False)\n\n# Change color axis\nplt.xticks(fontsize=16);\nplt.yticks(fontsize=16);\n\n# Title\nax.set_title('Outliers', fontsize=40, fontweight=\"bold\", pad=20);\n","7bbebb6d":"heart = heart.drop(heart[(heart['RestingBP'] == 0)].index)","b37a227b":"# Dealing with the outliers (RestingBP)\nq1 = heart['RestingBP'].quantile(0.25)\nq3 = heart['RestingBP'].quantile(0.75)\niqr = q3-q1\nLower_tail = q1 - 1.5 * iqr\nUpper_tail = q3 + 1.5 * iqr\n\nu = heart[(heart['RestingBP'] >= Upper_tail) | (heart['RestingBP'] <= Lower_tail)]  # | means \"or\"\nu = pd.DataFrame(u)\nprint('Outliers on RestingBP:')\nu.value_counts(u['HeartDisease'])","124b3e6c":"# Median imputation (RestingBP)\nmed = np.median(heart['RestingBP'])\nfor i in heart['RestingBP']:\n    if i > Upper_tail or i < Lower_tail:\n            heart['RestingBP'] = heart['RestingBP'].replace(i, med)","b55cbd46":"# Dealing with outliers (Cholesterol)\nq1 = heart['Cholesterol'].quantile(0.25)\nq3 = heart['Cholesterol'].quantile(0.75)\niqr = q3-q1\nLower_tail = q1 - 1.5 * iqr\nUpper_tail = q3 + 1.5 * iqr\n\nu = heart[(heart['Cholesterol'] >= Upper_tail) | (heart['Cholesterol'] <= Lower_tail)]  # | means \"or\"\nu = pd.DataFrame(u)\nprint('Outliers on Cholesterol:')\nu.value_counts(heart['HeartDisease'])","b5c289ff":"z = heart[heart['Cholesterol'] == 0]\nz = pd.DataFrame(z)\nprint('Outliers on Cholesterol equal to 0:')\nz.value_counts(heart['HeartDisease'])","500e2966":"# Median imputation (Cholesterol) just on upper tail\nmed = np.median(heart['Cholesterol'])\nfor i in heart['Cholesterol']:\n    if i > Upper_tail:\n            heart['Cholesterol'] = heart['Cholesterol'].replace(i, med)","d68479c0":"# Dealing with outliers (MaxHR)\nq1 = heart['MaxHR'].quantile(0.25)\nq3 = heart['MaxHR'].quantile(0.75)\niqr = q3-q1\nLower_tail = q1 - 1.5 * iqr\nUpper_tail = q3 + 1.5 * iqr\n\nu = heart[(heart['MaxHR'] >= Upper_tail) | (heart['MaxHR'] <= Lower_tail)]  # | means \"or\"\nu = pd.DataFrame(u)\nprint('Outliers on MaxHR:')\nu.value_counts(heart['HeartDisease'])","9c8d18be":"# Dealing with outliers (Oldpeak)\nq1 = heart['Oldpeak'].quantile(0.25)\nq3 = heart['Oldpeak'].quantile(0.75)\niqr = q3-q1\nLower_tail = q1 - 1.5 * iqr\nUpper_tail = q3 + 1.5 * iqr\n\nu = heart[(heart['Oldpeak'] >= Upper_tail) | (heart['Oldpeak'] <= Lower_tail)]  # | means \"or\"\nu = pd.DataFrame(u)\nu.value_counts(heart['HeartDisease'])","753de8c0":"# Changing back the dummies features to numeric \nheart['FastingBS'] = heart['FastingBS'].astype(int)\nheart['HeartDisease'] = heart['HeartDisease'].astype(int)","68ea2bea":"# Select categorical variables\ncateg = heart.select_dtypes(include=object).columns\n\n# One hot encoding\nheart = pd.get_dummies(heart, columns=categ, drop_first=True)  \nheart.head()","cf713fd9":"from sklearn.model_selection import train_test_split\n\n# Set up X and y variables\ny, X = heart['HeartDisease'], heart.drop(columns='HeartDisease')\n\n# Split the data into training and test samples\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","5a89e67e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Standard logistic regression\nlr = LogisticRegression(solver='liblinear').fit(X_train, y_train)\ny_pred = lr.predict(X_test)\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred), 4))\nprint('F1 Score: ', round(f1_score(y_test, y_pred), 4))","708b7499":"from sklearn.linear_model import LogisticRegressionCV\n\n# L1 regularized logistic regression\nlr_l1 = LogisticRegressionCV(Cs=10, cv=4, penalty='l1', solver='liblinear').fit(X_train, y_train)\ny_pred = lr_l1.predict(X_test)\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred), 4))\nprint('F1 Score: ', round(f1_score(y_test, y_pred), 4))","cd9812a1":"# L2 regularized logistic regression\nlr_l2 = LogisticRegressionCV(Cs=10, cv=4, penalty='l2', solver='liblinear').fit(X_train, y_train)\ny_pred = lr_l2.predict(X_test)\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred), 4))\nprint('F1 Score: ', round(f1_score(y_test, y_pred), 4))","d154c020":"from sklearn.neighbors import KNeighborsClassifier\n\n# First model\nknn = KNeighborsClassifier(n_neighbors=1)\nknn = knn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred), 4))\nprint('F1 Score: ', round(f1_score(y_test, y_pred), 4))","620a6326":"max_k = 20\nf1_scores = list()\nerror_rates = list() # 1-accuracy\n\nfor k in range(1, max_k):\n    \n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn = knn.fit(X_train, y_train)\n    \n    y_pred = knn.predict(X_test)\n    f1 = f1_score(y_pred, y_test)\n    f1_scores.append((k, round(f1_score(y_test, y_pred), 4)))\n    error = 1-round(accuracy_score(y_test, y_pred), 4)\n    error_rates.append((k, error))\n    \nf1_results = pd.DataFrame(f1_scores, columns=['K', 'F1 Score'])\nerror_results = pd.DataFrame(error_rates, columns=['K', 'Error Rate'])","023b411d":"mpl.rcParams['font.size'] = 12\nsns.set_style(\"whitegrid\", {'grid.linestyle': '--'})  # set grid\nmpl.rcParams['font.sans-serif'] = ['AppleGothic']\n\nfig, (ax_f1, ax_error) = plt.subplots(1, 2, figsize=(20, 7))\n\nfig.patch.set_facecolor('#F1F3F4')\nax_f1.patch.set_facecolor('#F1F3F4')\nax_error.patch.set_facecolor('#F1F3F4')\n\nsns.lineplot(f1_results['K'], f1_results['F1 Score'], color = '#236AB9', ax=ax_f1)\nsns.lineplot(error_results['K'], error_results['Error Rate'], color='#B85B14', ax=ax_error)\n\nax_f1.set_title('KNN F1 Score', color='#236AB9', fontsize= 25)\nax_error.set_title('KNN Elbow Curve', color='#B85B14', fontsize= 25)\n\n# Set xticks range\nax_f1.set_xticks(range(1,20))\nax_error.set_xticks(range(1,20))\n\n# Remove axes splines\nfor i in ['top', 'bottom', 'left', 'right']:\n    ax_f1.spines[i].set_visible(False)\n\nfor i in ['top', 'bottom', 'left', 'right']:\n    ax_error.spines[i].set_visible(False)","94dc0298":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=9)\nknn = knn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred), 4))\nprint('F1 Score: ', round(f1_score(y_test, y_pred), 4))","7f9c73f1":"from sklearn.svm import SVC\n\n# First model\nsvm=SVC(random_state=1)\nsvm.fit(X_train,y_train)\ny_pred = svm.predict(X_test)\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred), 4))\nprint('F1 Score: ', round(f1_score(y_test, y_pred), 4))","858256e9":"from sklearn.model_selection import GridSearchCV\n\n# declare parameters for hyperparameter tuning\nparameters = [ {'C':[1, 10, 100], 'kernel':['linear']},\n               {'C':[1, 10, 100], 'kernel':['rbf'], 'gamma':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]},\n               {'C':[1, 10, 100], 'kernel':['poly'], 'degree': [2,3] ,'gamma':[0.01,0.02,0.03,0.04,0.05]} \n              ]\n\ngrid_search = GridSearchCV(estimator = svm,  \n                           param_grid = parameters,\n                           scoring = 'f1',\n                           cv = 5,\n                           verbose=0)\n\ngrid_search.fit(X_train, y_train)\nprint(\"best score: \", grid_search.best_score_)\nprint(\"best param: \", grid_search.best_params_)","ba594a52":"svm=SVC(C=100, kernel='linear')\nsvm.fit(X_train,y_train)\ny_pred = svm.predict(X_test)\n\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred), 4))\nprint('F1 Score: ', round(f1_score(y_test, y_pred), 4))","67f1e6d3":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(random_state=42)\ndt = dt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\n\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred), 4))\nprint('F1 Score: ', round(f1_score(y_test, y_pred), 4))","a608119d":"from sklearn.ensemble import RandomForestClassifier\n\n# First model\nRF = RandomForestClassifier(random_state=42, n_estimators=100)\n                            \nRF = RF.fit(X_train, y_train)\ny_pred = RF.predict(X_test)\n\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred), 4))\nprint('F1 Score: ', round(f1_score(y_test, y_pred), 4))","b7e965c2":"n_estimators = [100, 150, 200]\nmax_depth = [15, 20, 25]\nmax_depth.append(None)\nmax_features = ['auto', 'sqrt']\nmin_samples_split = [5, 10, 15]\nmin_samples_leaf = [1, 2]\nbootstrap = [True, False]\n\nparams = {'n_estimators': n_estimators, 'max_features': max_features,\n          'max_depth': max_depth, 'min_samples_split': min_samples_split,\n          'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}\n\nRF = RandomForestClassifier(random_state=42)\n\ngrid_search = GridSearchCV(estimator = RF, \n                           param_grid = params,\n                           scoring = 'f1',\n                           cv = 5,\n                           verbose=0, \n                           n_jobs=-1)\n\ngrid_search.fit(X_train, y_train)\nprint(\"best score: \", grid_search.best_score_)\nprint(\"best param: \", grid_search.best_params_)","d6cbaabc":"best_params = grid_search.best_params_\nRF = RandomForestClassifier(random_state=42, **best_params)\n                            \nRF = RF.fit(X_train, y_train)\ny_pred = RF.predict(X_test)\n\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred), 4))\nprint('F1 Score: ', round(f1_score(y_test, y_pred), 4))","3ab89654":"### BEGIN SOLUTION\nfrom sklearn.ensemble import GradientBoostingClassifier\n    \n# The parameters to be fit\nparam_grid = {\n    'learning_rate': [0.01, 0.025, 0.05, 0.075, 0.1],\n    'n_estimators':[140, 150, 160], \n    'max_depth':[3, 4, 5]       \n     }\n\n# The grid search object\nGV_GBC = GridSearchCV(GradientBoostingClassifier(random_state=42), \n                      param_grid=param_grid, \n                      scoring='f1',\n                      cv = 5,\n                      verbose=0, \n                      n_jobs=-1)\n\n# Do the grid search\nGV_GBC = GV_GBC.fit(X_train, y_train)\nprint(\"best score: \", GV_GBC.best_score_)\nprint(\"best param: \", GV_GBC.best_params_)","b25a4e5a":"best_params = GV_GBC.best_params_\nGB = GradientBoostingClassifier(random_state=42, **best_params)\n                            \nGB = GB.fit(X_train, y_train)\ny_pred = GB.predict(X_test)\n\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred), 4))\nprint('F1 Score: ', round(f1_score(y_test, y_pred), 4))","6f6ce5e1":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nABC = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, random_state = 42))\n\nparam_grid = {'learning_rate': [0.01, 0.025, 0.05, 0.075, 0.1],\n              'n_estimators':[140, 150, 160]}\n\nGV_ABC = GridSearchCV(ABC,\n                      param_grid=param_grid, \n                      scoring='f1',\n                      n_jobs=-1)\n\nGV_ABC = GV_ABC.fit(X_train, y_train)\ny_pred = GV_ABC.predict(X_test)\n\nprint(\"best param: \", GV_ABC.best_params_)\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred), 4))\nprint('F1 Score: ', round(f1_score(y_test, y_pred), 4))","c1332615":"### Determining the optimal number of neighbours","0c94b75b":"## <center style=\"font-family:Arial\">Statistical Description of the Features<\/center>","af6c7f8d":"### MaxHR","e423eef8":"# <center style=\"font-family:Arial, font-size:Bold\">4. Feature Engineering<\/center>","aa42545a":"# <center style=\"font-family:Arial, font-size:Bold\">12. AdaBoost<\/center>\n","a465bbda":"# <center style=\"font-family:Arial, font-size:Bold\">8. Support Vector Machine<\/center>\n\n* <div style=\"font-size:120%; font-family:Arial\">The objective of the Support Vector Machine (SVM) algorithm is to find an optimal hyperplane in N-dimensional space (N refers to the number of features) that distinctly classifies the data points.<\/div>\n\n* <div style=\"font-size:120%; font-family:Arial\">The points closest to the hyperplane are called support vector points, and the distance of the vectors from the hyperplane are called margins.<\/div>\n\n* <div style=\"font-size:120%; font-family:Arial\">The SVM algorithm aims to maximize the margin between the support vector points and the hyperplane. <\/div>\n\n* <div style=\"font-size:120%; font-family:Arial\">There are different types of kernels: linear, polynomial and Gaussian.\n<\/div>\n\n* <div style=\"font-size:120%; font-family:Arial\">Radial basis function kernel (RBF)\/ Gaussian Kernel is a function whose value depends on the distance from the origin or some point. This kernel has two parameters: C (Inverse to the strength of regularization) and Gamma (to decrease underfits)<\/div>\n\n<center><img src='https:\/\/miro.medium.com\/max\/820\/0*luhI3gW7WnXfLnhA.jpg'><\/center>","779fa0cd":"## <center style=\"font-family:Arial\">Duplicates & Missing Values<\/center>","4f0c2847":"# <center style=\"font-family:Arial, font-size:Bold\">6. Logistic Regression<\/center>\n\n* <div style=\"font-size:120%; font-family:Arial\">The logistic regression is one of the most popular and straightforward models for classification.<\/div>\n\n* <div style=\"font-size:120%; font-family:Arial\">This model can be used for binary, multinominal or ordinal classification.<\/div>\n\n* <div style=\"font-size:120%; font-family:Arial\">It can be set a threshold to predict which class an observation relates.<\/div>\n\n* <div style=\"font-size:120%; font-family:Arial\">The main disadvantage of this model is that the interpretation may result complex.<\/div>\n\n<center><img src='https:\/\/magoosh.com\/statistics\/files\/2018\/03\/Peq2.jpg'><\/center>\n\n<center><img src='https:\/\/miro.medium.com\/max\/1400\/1*RqXFpiNGwdiKBWyLJc_E7g.png'><\/center>","abd3c05e":"### Cholesterol","55e2deb9":"<div style=\"font-size:120%; font-family:Arial\">Almost all the outliers from <code>Oldpeak<\/code> have heart disease. For this reason, I won't imputate them. They are telling us a lot of information!<\/div>","5ef437d4":"## <center style=\"font-family:Arial\">Scaling Numeric Features<\/center>","6f0c5a03":"## <center style=\"font-family:Arial\">Importing the Data<\/center>","b870ad16":"* <div style=\"font-size:120%; font-family:Arial\">Note that <code>HeartDisease<\/code>, the target variable of this dataset, is decently balanced.<\/div>","fd254a73":"## <center style=\"font-family:Arial\">Count of Features per Data Type <\/center>","2f8dd1a3":"# <center style=\"font-family:Arial, font-size:Bold\">1. Introduction<\/center>\n\n<div style=\"font-size:120%; font-family:Arial\">This notebook will present some of the most popular models for classification in Machine Learning like <b>Logistic Regression, K-Nearest Neighbor, Support Vector Machines, Decision Trees, Bagging, Boosting, <\/b>and many more. \n\nFor this purpose, I'll use a dataset containing different characteristics of patients to predict heart failures. If you learn to know more about it, you can click <a href=\"https:\/\/www.kaggle.com\/fedesoriano\/heart-failure-prediction\" target=\"_blank\"> here<\/a>.\n<\/div>\n\n<center><img src='https:\/\/i.pinimg.com\/originals\/87\/45\/76\/874576ebe081d0b3cc15c1a519dcb6c1.gif'><\/center>\n\n<hr style=\"height: 0.5px; border: 0; background-color: 'Black'\">","cb6515b5":"## <center style=\"font-family:Arial\">Outliers<\/center>","67755b2e":"### RestingBP\n\n<div style=\"font-size:120%; font-family:Arial\"><code>RestingBP<\/code> represents the blood pressure of the patient. It is impossible to have values equal to 0; that's why I'll remove the outlier with value 0.<\/div>","bcc7f3f6":"# <center style=\"font-family:Arial, font-size:Bold\">9. Decision Tree<\/center>\n\n* <div style=\"font-size:120%; font-family:Arial\"> The main benefit of this algorithm is its straightforward interpretation and visualization.<\/div>\n\n* <div style=\"font-size:120%; font-family:Arial\"> Impurity refers to the quality of a split in a determined decision. There are several ways to measure the impurity of the decision tree.<\/div>\n\n* <div style=\"font-size:120%; font-family:Arial\"> Decision Trees don't require any normalization or standardization.<\/div>\n\n* <div style=\"font-size:120%; font-family:Arial\"> This algorithm doesn't perform well (in general), but it helps introduce algorithms like the Random Forest.<\/div>\n\n<center><img src='https:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1545934190\/1_r5ikdb.png'><\/center>","6fe46898":"<div style=\"font-size:120%; font-family:Arial\">Because the outliers are not extremely imbalanced on <code>HeartDisease<\/code>, I'll use the median imputation to remove the outliers from this feature.<\/div>","99b3152a":"<p style=\"font-family: arial;font-size: 24px\"> \ud83d\udee0 Work in progress... If you liked it so far, please don't forget to comment and upvote. Thank you!<\/p>","dd7657c3":"<div style=\"font-size:120%; font-family:Arial\">We can see that <code>FastingBS<\/code> and <code>HeartDisease<\/code> are binary features [0, 1]. I am going to transform them into categorical variables just for the EDA.<\/div>","5906bef3":"# <center style=\"font-family:Arial, font-size:Bold\">7. K-Nearest Neighbor<\/center>\n\n* <div style=\"font-size:120%; font-family:Arial\">The KNN algorithm assumes that similar data points that are near each other will belong to the same class.<\/div>\n\n* <div style=\"font-size:120%; font-family:Arial\">K refers to the number of neigbors from each observation. <\/div>\n\n* <div style=\"font-size:120%; font-family:Arial\">The idea of similarity is known as distance or proximity. The goal is to find the optimal distance for the observations.<\/div>\n\n* <div style=\"font-size:120%; font-family:Arial\">It is also called a lazy learner algorithm because it does not learn from the training set immediately instead it stores the dataset and at the time of classification, it performs an action on the dataset.\n<\/div>\n\n* <div style=\"font-size:120%; font-family:Arial\">The following image nicely represents the idea of how the algorithm works.<\/div>\n\n<center><img src='http:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1531424125\/KNN_final1_ibdm8a.png'><\/center>","114517da":"## <center style=\"font-family:Arial\">Encoding Categorical Features<\/center>","ebfb4db1":"<div style=\"font-size:130%; font-family:cursive\"><blockquote>\"The main intuitive difference between the L1 and L2 regularization is that L1 regularization tries to estimate the median of the data while the L2 regularization tries to estimate the mean of the data to avoid overfitting.\"<\/blockquote><\/div>","0b2b0dfb":"# <center style=\"font-family:Arial, font-size:Bold\">5. Modelling<\/center>\n\n## <center style=\"font-family:Arial\">Splittig the Data into Train and Test<\/center>\n","47f2bfd8":"# <center style=\"font-family:Arial, font-size:Bold\">3. EDA<\/center>\n\n<div style=\"font-size:120%; font-family:Arial\"> For the EDA, I'm going to automate this process as much as possible by looping all the categorical and numerical variables simultaneously. This method will reduce time and effort for our analysis!<\/div>\n","d620447c":"# <center style=\"font-family:Arial, font-size:Bold\">2. Data Preprocessing<\/center>\n","6736700e":"### Hyperparameter Optimization","74e2048e":"<div style=\"font-size:120%; font-family:Arial\">We can see that almost all patients with <code>colesterol<\/code> equal 0 have a heart disease. For this reason, I'll not imputate the outliers equal to 0 in this feature.<\/div>","3e1bad46":"# <center style=\"font-family:Arial, font-size:Bold\">10. Random Forest<\/center>\n\n* <div style=\"font-size:120%; font-family:Arial\">Bagging is an ensemble algorithm that fits multiple models on different subsets of a training dataset, then combines the predictions from all models. Random forest is an extension of bagging that also randomly selects subsets of features used in each data sample.<\/div>\n\n* <div style=\"font-size:120%; font-family:Arial\">Random forest consist on a large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.<\/div>\n\n* <div style=\"font-size:120%; font-family:Arial\">The trees protect each other from their individual errors.<\/div>\n\n<center><img src='https:\/\/www.tibco.com\/sites\/tibco\/files\/media_entity\/2021-05\/random-forest-diagram.svg'><\/center>","8022c643":"| Feature | Description |\n| :- | :- |\n| Age | age of the patient [years]\n| Sex | sex of the patient [M: Male, F: Female]\n| ChestPainType | chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n| RestingBP | resting blood pressure [mm Hg]\n| Cholesterol | serum cholesterol [mm\/dl]\n| FastingBS | fasting blood sugar [1: if FastingBS > 120 mg\/dl, 0: otherwise]\n| RestingECG | resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n| MaxHR | maximum heart rate achieved [Numeric value between 60 and 202]\n| ExerciseAngina | exercise-induced angina [Y: Yes, N: No]\n| Oldpeak | oldpeak = ST [Numeric value measured in depression]\n| ST_Slope | the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n| HeartDisease | output class [1: heart disease, 0: Normal]","cebc45eb":"# <center style=\"font-family:Arial, font-size:Bold\">11. Gradient Boosting<\/center>\n\n* <div style=\"font-size:120%; font-family:Arial\">Boosting is a method of converting weak learners into strong learners.<\/div>\n\n* <div style=\"font-size:120%; font-family:Arial\">Gradient Boosting trains many models in a gradual, additive and sequential manner.<\/div>\n\n<center><img src='https:\/\/i.imgur.com\/e7MIgXk.png'><\/center>","9626d0d4":"<div style=\"font-size:120%; font-family:Arial\">On <code>MaxHR<\/code> there are only two outliers. I won't imputate any of them.<\/div>","2ccca07c":"## <center style=\"font-family:Arial\">Changing Back Dummies Variables to Numeric<\/center>","a37fcd0c":"### Oldpeak"}}