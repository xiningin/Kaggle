{"cell_type":{"5a246d0b":"code","38ef0c67":"code","99f66416":"code","0260c58f":"code","fb22cb51":"code","f7458a20":"code","4030e66e":"code","cac18123":"code","e999c52e":"code","4b60d261":"code","0454e8a4":"code","6ea3b1c4":"code","c9c54075":"code","a220dd1e":"code","58100e62":"code","82ffeae1":"code","c909877f":"code","05cf987b":"code","e277e23f":"code","5d9743d8":"code","23fd403b":"code","49f026ef":"code","86aeddc7":"code","4ed6e9ef":"code","74510330":"code","7c8cf066":"code","4fd4bf50":"code","b00fbf23":"code","376c5fcd":"code","9cd3e670":"code","c4da8789":"code","2219326f":"code","86a7c573":"code","af549bac":"code","a9b91636":"code","6e4bb964":"code","df9d0fc0":"code","ae55fde0":"code","75bce76a":"code","81136244":"code","47ebb0d9":"code","015c6b5f":"code","8e0c0907":"markdown","101736bb":"markdown","9bcafb56":"markdown","44efa86c":"markdown","807e9f6e":"markdown","60690ab1":"markdown","10c8bd84":"markdown","5cedd2e1":"markdown","1e88a9d1":"markdown","5e878913":"markdown","69812ee6":"markdown","073ad943":"markdown","f9e78b4a":"markdown","42a7a642":"markdown","be8419e6":"markdown","6480f16c":"markdown","fa44755a":"markdown","1fb12901":"markdown","7e934c72":"markdown","44064d19":"markdown","7f9e44cf":"markdown","085b45b6":"markdown","2652358c":"markdown","acc00d46":"markdown","85d21fdd":"markdown","f71b2f52":"markdown","5bd46d7b":"markdown","b529036d":"markdown","9948197f":"markdown","d69fc518":"markdown","dbae1680":"markdown","8d841017":"markdown","03163319":"markdown"},"source":{"5a246d0b":"import pathlib\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport os\nimport PIL\nimport glob\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.regularizers import l2","38ef0c67":"# Defining the path for train and test images\n\ndata_dir_train = pathlib.Path(\"..\/input\/skin-cancer9-classesisic\/Skin cancer ISIC The International Skin Imaging Collaboration\/Train\/\")\ndata_dir_test = pathlib.Path(\"..\/input\/skin-cancer9-classesisic\/Skin cancer ISIC The International Skin Imaging Collaboration\/Test\/\")","99f66416":"image_count_train = len(list(data_dir_train.glob('*\/*.jpg')))\nprint(image_count_train)\nimage_count_test = len(list(data_dir_test.glob('*\/*.jpg')))\nprint(image_count_test)","0260c58f":"batch_size = 32\nimg_height = 180\nimg_width = 180","fb22cb51":"## Write your train dataset here\n## Note use seed=123 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory\n## Note, make sure your resize your images to the size img_height*img_width, while writting the dataset\ntrain_ds = image_dataset_from_directory(data_dir_train, \n                                        seed = 123, \n                                        image_size=(img_height, img_width), \n                                        validation_split=0.2, \n                                        subset='training')","f7458a20":"## Write your validation dataset here\n## Note use seed=123 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory\n## Note, make sure your resize your images to the size img_height*img_width, while writting the dataset\nval_ds = image_dataset_from_directory(data_dir_train, \n                                      seed = 123, \n                                      image_size=(img_height, img_width), \n                                      validation_split=0.2, \n                                      subset='validation')","4030e66e":"# List out all the classes of skin cancer and store them in a list. \n# You can find the class names in the class_names attribute on these datasets. \n# These correspond to the directory names in alphabetical order.\nclass_names = train_ds.class_names\nprint(type(class_names), class_names)\n","cac18123":"### Visualizing the training data\n\nplt.figure(figsize=(15, 10))\n\nfor i, class_ in enumerate(list(class_names)):\n    plt.subplot(3, 3, i+1)\n    data_path = os.path.join(str(data_dir_train), class_)\n    file_path = glob.glob(os.path.join(data_path,'*.jpg'))[0]\n    img = PIL.Image.open(file_path)\n    plt.imshow(img)\n    plt.title(class_)\n    plt.axis(\"off\")\nplt.show()","e999c52e":"for image_batch, labels_batch in train_ds.take(1):\n    print(image_batch.shape)\n    print(labels_batch.shape)","4b60d261":"# Allow gpu usage\ngpus = tf.config.experimental.list_physical_devices('GPU')\nprint(gpus)\ntry:\n    tf.config.experimental.set_memory_growth = True\nexcept Exception as ex:\n    print(e)","0454e8a4":"data_augmentation = tf.keras.Sequential(\n  [\n    layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n                                                 input_shape=(img_height, \n                                                              img_width,\n                                                              3)),\n    layers.experimental.preprocessing.RandomRotation(0.1),\n    layers.experimental.preprocessing.RandomZoom(0.1),\n  ]\n)","6ea3b1c4":"AUTOTUNE = tf.data.experimental.AUTOTUNE\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)","c9c54075":"plt.figure(figsize=(15, 15))\nfor images, label in train_ds.take(1):\n    for i in range(15):\n        augmented_images = data_augmentation(images)\n        ax = plt.subplot(5, 3, i + 1)\n        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n        plt.title(class_names[label[i]])\n        plt.axis(\"off\")","a220dd1e":"num_classes = 9\n\n# Building model with data augmentation and drop out layers\n# Model building - Data augmentation -> Rescaling -> Conv2D -> MaxPooling2D -> Conv2D -> MaxPooling2D -> Conv2D -> MaxPooling2D -> Dropout -> Dense -> Dense\nmodel = data_augmentation\nmodel.add(layers.experimental.preprocessing.Rescaling(1.\/255, input_shape=(img_height, img_width, 3)))\nmodel.add(Conv2D(16, 3, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D())\n\nmodel.add(Conv2D(32, 3, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D())\n\nmodel.add(Conv2D(64, 3, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D())\nmodel.add(Dropout(0.15))\n\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(num_classes))","58100e62":"## Your code goes here\nmodel.compile(optimizer='adam',\n              loss=SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","82ffeae1":"model.summary()","c909877f":"epochs = 20\n\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","05cf987b":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","e277e23f":"class_size = {}\n\nfor name in class_names:\n    class_size[name] = len(list(data_dir_train.glob(name+'\/*.jpg')))\n\nclass_size","5d9743d8":"class_df = pd.DataFrame(class_size.items(),index=list(class_size), columns = ['ClassName', 'NumberOfSamples'])\nclass_df.drop(['ClassName'], axis = 1, inplace=True)\nclass_df","23fd403b":"class_df.plot.bar()\nplt.show()","49f026ef":"!pip install Augmentor","86aeddc7":"import Augmentor","4ed6e9ef":"path_to_training_dataset = '..\/input\/skin-cancer9-classesisic\/Skin cancer ISIC The International Skin Imaging Collaboration\/Train\/'\n\nfor i in class_names:\n    p = Augmentor.Pipeline(path_to_training_dataset + i, output_directory='\/kaggle\/working\/'+i+'\/output\/')\n    p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n    p.sample(1000) ## We are adding 500 samples per class to make sure that none of the classes are sparse.","74510330":"output_dir = pathlib.Path('\/kaggle\/working\/')","7c8cf066":"image_count_train = len(list(output_dir.glob('*\/output\/*.jpg')))\nprint(image_count_train)","4fd4bf50":"train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  output_dir,\n  seed=123,\n  validation_split = 0.2,\n  subset = 'training',\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","b00fbf23":"val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  output_dir,\n  seed=123,\n  validation_split = 0.2,\n  subset = 'validation',\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","376c5fcd":"AUTOTUNE = tf.data.experimental.AUTOTUNE\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)","9cd3e670":"num_classes = 9\n\n# Model - Rescaling -> Conv2D -> MaxPooling2D -> Dropout -> Conv2D -> MaxPooling2D -> Dropout -> Conv2D -> MaxPooling2D -> Dropout -> Dense -> Dense\nmodel = Sequential()\nmodel.add(layers.experimental.preprocessing.Rescaling(1.\/255, input_shape=(img_height, img_width, 3)))\nmodel.add(Conv2D(16, 3, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D())\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(32, 3, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D())\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, 3, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D())\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(num_classes))","c4da8789":"model.compile(optimizer='adam',\n              loss = SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","2219326f":"model.summary()","86a7c573":"epochs = 30\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","af549bac":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\n\nplt.show()","a9b91636":"data_augmentation = tf.keras.Sequential(\n  [\n    layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n                                                 input_shape=(img_height, \n                                                              img_width,\n                                                              3)),\n    layers.experimental.preprocessing.RandomRotation(0.1),\n    layers.experimental.preprocessing.RandomZoom(0.1),\n  ]\n)","6e4bb964":"num_classes = 9\n\n# Building model with data augmentation and drop out layers\n# Model building - Data augmentation -> Rescaling -> Conv2D -> MaxPooling2D -> Conv2D -> MaxPooling2D -> Conv2D -> MaxPooling2D -> Dropout -> Dense -> Dense\n\nmodel = data_augmentation\nmodel.add(layers.experimental.preprocessing.Rescaling(1.\/255, input_shape=(img_height, img_width, 3)))\nmodel.add(Conv2D(16, 3, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D())\n\nmodel.add(Conv2D(32, 3, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D())\n\nmodel.add(Conv2D(64, 3, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D())\nmodel.add(Dropout(0.20))\n\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(num_classes))","df9d0fc0":"model.compile(optimizer='adam',\n              loss = SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","ae55fde0":"model.summary()","75bce76a":"epochs = 50\n# Train the model\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","81136244":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\n\nplt.show()","47ebb0d9":"test_dataset = image_dataset_from_directory(data_dir_test, image_size=(img_height, img_width), \n                                           batch_size=batch_size)\ntest_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)","015c6b5f":"#Retrieve a batch of images from the test set\nimage_batch, label_batch = test_dataset.as_numpy_iterator().next()\npredictions = model.predict_on_batch(image_batch).flatten()\n\n# Apply a sigmoid since our model returns logits\npredictions = tf.nn.sigmoid(predictions)\npredictions = tf.where(predictions < 0.5, 0, 1)\n\nprint('Predictions:\\n', predictions.numpy())\nprint('Labels:\\n', label_batch)\n\nplt.figure(figsize=(10, 10))\nfor i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(image_batch[i].astype(\"uint8\"))\n    plt.title(class_names[predictions[i]] + \"\\n\" + class_names[label_batch[i]])\n    plt.axis(\"off\")","8e0c0907":"To use `Augmentor`, the following general procedure is followed:\n\n1. Instantiate a `Pipeline` object pointing to a directory containing your initial image data set.<br>\n2. Define a number of operations to perform on this data set using your `Pipeline` object.<br>\n3. Execute these operations by calling the `Pipeline\u2019s` `sample()` method.\n","101736bb":"####  Analyze your results here. Did you get rid of underfitting\/overfitting? Did class rebalance help?\n\n","9bcafb56":"# Load using keras.preprocessing\n\nLet's load these images off disk using the helpful image_dataset_from_directory utility.","44efa86c":"### Compiling the model","807e9f6e":"We still have a overfitting model but we have much better accuracy than previous models.At end we have around 10% accuuracy difference between training and validation data.\nClass rebalance helped us in getting better accuracy. \n\nWe can reduce the overfitting by having a deeper model or add a data augmentation layer before it.","60690ab1":"# Train the model on the data created using Augmentor","10c8bd84":"`Dataset.cache()` keeps the images in memory after they're loaded off disk during the first epoch.\n\n`Dataset.prefetch()` overlaps data preprocessing and model execution while training.","5cedd2e1":"The `image_batch` is a tensor of the shape `(32, 180, 180, 3)`. This is a batch of 32 images of shape `180x180x3` (the last dimension refers to color channels RGB). The `label_batch` is a tensor of the shape `(32,)`, these are corresponding labels to the 32 images.","1e88a9d1":"# Create the model, compile and train the model\n","5e878913":"# Using data augmentation","69812ee6":"# Visualizing the results","073ad943":"#### Train your model","f9e78b4a":"### Training the model","42a7a642":"Augmentor has stored the augmented images in the output sub-directory of each of the sub-directories of skin cancer types.. Lets take a look at total count of augmented images.","be8419e6":"<div style=\"text-align: center;\">\n<img src= \"https:\/\/github.com\/zyper26\/Skin-Cancer-ISIC\/blob\/main\/All_Images.png?raw=true\" title =\"Dataset Example\" style='width: 1000px;'>\n<\/div>","6480f16c":"# Model with data augmentation and using Augmentor dataset","fa44755a":"# Rectify the class imbalance\n#### Python package known as `Augmentor` (https:\/\/augmentor.readthedocs.io\/en\/master\/) to add more samples across all classes so that none of the classes have very few samples.","1fb12901":"#### Create your model (make sure to include normalization)","7e934c72":"# Importing all the important libraries","44064d19":"# Model evaluation","7f9e44cf":"### Visualize the data\n#### Code to visualize one instance of all the nine classes present in the dataset","085b45b6":"The dataset consists of 2357 images of malignant and benign oncological diseases, which were formed from the International Skin Imaging Collaboration (ISIC). All images were sorted according to the classification taken with ISIC, and all subsets were divided into the same number of images, with the exception of melanomas and moles, whose images are slightly dominant.","2652358c":"The data set contains the following diseases:\n\n    actinic keratosis\n    basal cell carcinoma\n    dermatofibroma\n    melanoma\n    nevus\n    pigmented benign keratosis\n    -seborrheic keratosis\n    squamous cell carcinoma\n    vascular lesion","acc00d46":"#### Compile your model (Choose optimizer and loss function appropriately)","85d21fdd":"Check the configuration of gpu and use gpu.","f71b2f52":"```layers.experimental.preprocessing.Rescaling``` to normalize pixel values between (0,1). The RGB channel values are in the `[0, 255]` range. This is not ideal for a neural network. Here, it is good to standardize values to be in the `[0, 1]`","5bd46d7b":"### Create a dataset\n\nDefine some parameters for the loader:","b529036d":"# Importing Skin Cancer Data","9948197f":"### Compile the model\nChoose an appropirate optimiser and loss function for model training ","d69fc518":"Use 80% of the images for training, and 20% for validation.","dbae1680":"#### Visualize the model results","8d841017":"<div style=\"text-align: center;text-size:20px;border:1px solid red;background:blue; font-size:30px\">\n    <span style='background :blue; color:white' align=\"center\"> <b>Skin Cancer Dataset<\/b> <\/span>\n<\/div>","03163319":"# Distribution of classes in the training dataset.\n#### Many times real life datasets can have class imbalance, one class can have proportionately higher number of samples compared to the others. Class imbalance can have a detrimental effect on the final model quality. Hence as a sanity check it becomes important to check what is the distribution of classes in the data."}}