{"cell_type":{"6dfbe395":"code","64dfe28e":"code","b6d8f6ae":"code","0a0afd74":"code","3b969c2e":"code","2f79fe7b":"code","1b51d1c3":"code","bac224d4":"code","7a386a0e":"code","ff6debb8":"code","735f9c84":"code","20e85ac4":"code","4dc42a67":"code","f13febf3":"code","6485995a":"code","37e2e4b7":"code","3f41d96e":"code","180a5032":"code","e6f8520e":"markdown","110093ca":"markdown","16db4605":"markdown"},"source":{"6dfbe395":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","64dfe28e":"!pip install matplotlib seaborn pandas sklearn","b6d8f6ae":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","0a0afd74":"from sklearn.datasets import load_boston\nboston = load_boston()\n\nprint(boston.DESCR)","3b969c2e":"# boston is numpy\nprint(boston)\n\ndf = pd.DataFrame(boston.data, columns=boston.feature_names)\ndf['TARGET'] = boston.target\ndf.tail()\n","2f79fe7b":"sns.pairplot(df)\nplt.show()","1b51d1c3":"# Check which attribute is regression shape with TARGET and selected attribute \ncols = ['TARGET', 'INDUS', 'RM', 'LSTAT', 'NOX', 'DIS']\ndf[cols].describe()","bac224d4":"sns.pairplot(df[cols])\nplt.show()","7a386a0e":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","ff6debb8":"# convert from Dataframe value to torch value\n\nprint(df[cols]) # Dataframe\nprint('-------------------')\nprint(df[cols].values) \nprint(type(df[cols].values)) # numpy \nprint(type(df[cols].values[0][0])) # numpy.float64 (double float)\n","735f9c84":"data = torch.from_numpy(df[cols].values).float() # from double to single float(=float32)","20e85ac4":"print(data.shape)\nprint(data)\nprint(data[0][0])\nprint(type(data[0][0].values))","4dc42a67":"# Split x and y\ny = data[:, :1] # TARGET\nx = data[:, 1:]\n\nprint(x.shape, y.shape)","f13febf3":"# Define model\nprint(x.size(), x.size(-1))\n\nmodel = nn.Linear(x.size(-1), y.size(-1)) # |model| = (5, 1)\nmodel","6485995a":"# Define configurations.\nn_epochs = 1000\nlearning_rate = 1e-3\nprint_interval = 100","37e2e4b7":"# Instead of implement gradient equation,\n# we can use <optim class> to update model parameters, automatically.\noptimizer = optim.SGD(model.parameters(),\n                     lr=learning_rate)","3f41d96e":"# Whole training samples are used in 1 epoch.\n# Thus, 'N epochs' means that model saw a sample N-times.\nfor i in range(n_epochs):\n    \n    ## Order is important\n    \n    # 1) y = f(x)\n    y_hat = model(x) # Theta w and b are determined by the dimensions of x and y\n    \n    # 2) Calculate MSE \n    loss = F.mse_loss(y_hat, y)\n    \n    # 3) Reset gradient\n    optimizer.zero_grad() \n    \n    # 4) differential of Loss\n    loss.backward() # w.grad, b.grad have value (w and b saved gradient value)\n    \n    # 5) Gradient descent with learning_rate, w grad, b grad\n    optimizer.step() # Gradientdescent\n    \n    if (i + 1) % print_interval == 0:\n        print('Epoch %d: loss=%.4e' % (i + 1, loss))","180a5032":"# To see the resulting graph at once, the top is y, the botton is y_hat\ndf = pd.DataFrame(torch.cat([y, y_hat], dim=1).detach_().numpy(),\n                  columns=['y', 'y_hat']) \nsns.pairplot(df, height=5)\nplt.show()","e6f8520e":"## **Load Dataset from sklearn**","110093ca":"## **Train Linear Model with PyTorch**","16db4605":"## Let's see the result!\n#### - detach_(): without gradient propagation"}}