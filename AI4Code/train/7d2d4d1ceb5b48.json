{"cell_type":{"56bdd09e":"code","0d0b4a97":"code","79b398b6":"code","bd5398ac":"code","2a85038f":"code","89685c9f":"code","03cdb7a6":"code","fb714fb0":"code","3a4843dd":"code","e11224c7":"code","1c458b4b":"code","092916d5":"code","45f4f44a":"code","28dd022e":"code","bb71844e":"code","0e1ac60f":"code","e6e7fa11":"code","ac4ecacb":"code","1976c867":"code","41a69145":"code","205e71e1":"code","a95b9d66":"code","c317dc47":"code","9838b2bb":"code","79bb6dad":"code","63803549":"code","738342cf":"code","fadcd836":"code","b0e599b5":"code","5c949add":"code","16eb25d3":"code","11220165":"code","27ae22a5":"markdown","1f117e81":"markdown","0e1c1428":"markdown","2a22ce8a":"markdown","84324d08":"markdown","22ae62fd":"markdown","b4341cc9":"markdown","56f9b4bd":"markdown","47ecd2e9":"markdown","00acbf4c":"markdown","f6fcfca0":"markdown","ec137f64":"markdown","1c14c9ac":"markdown","2789e709":"markdown","6f4c44e6":"markdown","8b7409fb":"markdown","126af78e":"markdown","a4811042":"markdown","eacfe34e":"markdown"},"source":{"56bdd09e":"!rm -rf images assets\n!pip install numpy==1.17.0\n!pip install tensorflow==1.15.2\n!pip install keras==2.1.0","0d0b4a97":"import os\nDATA_DIR = '\/kaggle\/working\/food-recognition-challenge'\n# Directory to save logs and trained model\nROOT_DIR = ''","79b398b6":"!git clone https:\/\/www.github.com\/matterport\/Mask_RCNN.git\nos.chdir('Mask_RCNN')\n!pip install -q -r requirements.txt\n!python setup.py -q install","bd5398ac":"!pip uninstall pycocotools -y\n!pip install -q git+https:\/\/github.com\/waleedka\/coco.git#subdirectory=PythonAPI","2a85038f":"import sys\nsys.path.append(os.path.join('.', 'Mask_RCNN'))  # To find local version of the library\nsys.path.append(ROOT_DIR)\nimport sys\nimport re\nimport random\nimport pandas as pd \nimport os \nimport numpy as np\nimport mrcnn.model as modellib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport matplotlib.lines as lines\nimport matplotlib\nimport math\nimport logging\nimport json\nimport itertools\nimport glob \nimport cv2\nfrom tqdm import tqdm\nfrom pycocotools.cocoeval import COCOeval\nfrom pycocotools.coco import COCO\nfrom pycocotools import mask as maskUtils\nfrom mrcnn.model import log\nfrom mrcnn.config import Config\nfrom mrcnn import visualize\nfrom mrcnn import utils\nfrom matplotlib.patches import Polygon\nfrom imgaug import augmenters as iaa\nfrom collections import defaultdict, Counter\nfrom collections import OrderedDict\nROOT_DIR = os.path.abspath(\".\")","89685c9f":"class FoodChallengeDataset(utils.Dataset):\n    def load_dataset(self, dataset_dir, load_small=False, return_coco=True):\n        \"\"\" Loads dataset released for the AICrowd Food Challenge\n            Params:\n                - dataset_dir : root directory of the dataset (can point to the train\/val folder)\n                - load_small : Boolean value which signals if the annotations for all the images need to be loaded into the memory,\n                               or if only a small subset of the same should be loaded into memory\n        \"\"\"\n        self.load_small = load_small\n        if self.load_small:\n            annotation_path = os.path.join(dataset_dir, \"annotation-small.json\")\n        else:\n            annotation_path = os.path.join(dataset_dir, \"annotations.json\")\n\n        image_dir = os.path.join(dataset_dir, \"images\")\n        print(\"Annotation Path \", annotation_path)\n        print(\"Image Dir \", image_dir)\n        assert os.path.exists(annotation_path) and os.path.exists(image_dir)\n\n        self.coco = COCO(annotation_path)\n        self.image_dir = image_dir\n\n        # Load all classes (Only Building in this version)\n        classIds = self.coco.getCatIds()\n\n        # Load all images\n        image_ids = list(self.coco.imgs.keys())\n\n        # register classes\n        for _class_id in classIds:\n            self.add_class(\"crowdai-food-challenge\", _class_id, self.coco.loadCats(_class_id)[0][\"name\"])\n\n        # Register Images\n        for _img_id in image_ids:\n            assert(os.path.exists(os.path.join(image_dir, self.coco.imgs[_img_id]['file_name'])))\n            self.add_image(\n                \"crowdai-food-challenge\", image_id=_img_id,\n                path=os.path.join(image_dir, self.coco.imgs[_img_id]['file_name']),\n                width=self.coco.imgs[_img_id][\"width\"],\n                height=self.coco.imgs[_img_id][\"height\"],\n                annotations=self.coco.loadAnns(self.coco.getAnnIds(\n                                            imgIds=[_img_id],\n                                            catIds=classIds,\n                                            iscrowd=None)))\n\n        if return_coco:\n            return self.coco\n\n    def load_mask(self, image_id):\n        \"\"\" Loads instance mask for a given image\n              This function converts mask from the coco format to a\n              a bitmap [height, width, instance]\n            Params:\n                - image_id : reference id for a given image\n\n            Returns:\n                masks : A bool array of shape [height, width, instances] with\n                    one mask per instance\n                class_ids : a 1D array of classIds of the corresponding instance masks\n                    (In this version of the challenge it will be of shape [instances] and always be filled with the class-id of the \"Building\" class.)\n        \"\"\"\n\n        image_info = self.image_info[image_id]\n        assert image_info[\"source\"] == \"crowdai-food-challenge\"\n\n        instance_masks = []\n        class_ids = []\n        annotations = self.image_info[image_id][\"annotations\"]\n        # Build mask of shape [height, width, instance_count] and list\n        # of class IDs that correspond to each channel of the mask.\n        for annotation in annotations:\n            class_id = self.map_source_class_id(\n                \"crowdai-food-challenge.{}\".format(annotation['category_id']))\n            if class_id:\n                m = self.annToMask(annotation,  image_info[\"height\"],\n                                                image_info[\"width\"])\n                # Some objects are so small that they're less than 1 pixel area\n                # and end up rounded out. Skip those objects.\n                if m.max() < 1:\n                    continue\n\n                # Ignore the notion of \"is_crowd\" as specified in the coco format\n                # as we donot have the said annotation in the current version of the dataset\n\n                instance_masks.append(m)\n                class_ids.append(class_id)\n        # Pack instance masks into an array\n        if class_ids:\n            mask = np.stack(instance_masks, axis=2)\n            class_ids = np.array(class_ids, dtype=np.int32)\n            return mask, class_ids\n        else:\n            # Call super class to return an empty mask\n            return super(FoodChallengeDataset, self).load_mask(image_id)\n\n\n    def image_reference(self, image_id):\n        \"\"\"Return a reference for a particular image\n\n            Ideally you this function is supposed to return a URL\n            but in this case, we will simply return the image_id\n        \"\"\"\n        return \"crowdai-food-challenge::{}\".format(image_id)\n    # The following two functions are from pycocotools with a few changes.\n\n    def annToRLE(self, ann, height, width):\n        \"\"\"\n        Convert annotation which can be polygons, uncompressed RLE to RLE.\n        :return: binary mask (numpy 2D array)\n        \"\"\"\n        segm = ann['segmentation']\n        if isinstance(segm, list):\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            rles = maskUtils.frPyObjects(segm, height, width)\n            rle = maskUtils.merge(rles)\n        elif isinstance(segm['counts'], list):\n            # uncompressed RLE\n            rle = maskUtils.frPyObjects(segm, height, width)\n        else:\n            # rle\n            rle = ann['segmentation']\n        return rle\n\n    def annToMask(self, ann, height, width):\n        \"\"\"\n        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n        :return: binary mask (numpy 2D array)\n        \"\"\"\n        rle = self.annToRLE(ann, height, width)\n        m = maskUtils.decode(rle)\n        return m","03cdb7a6":"from mrcnn.config import Config\nclass FoodChallengeConfig(Config):\n    \"\"\"Configuration for training on data in MS COCO format.\n    Derives from the base Config class and overrides values specific\n    to the COCO dataset.\n    \"\"\"\n    # Give the configuration a recognizable name\n    NAME = \"crowdai-food-challenge\"\n\n    # We use a GPU with 12GB memory, which can fit two images.\n    # Adjust down if you use a smaller GPU.\n    IMAGES_PER_GPU = 2\n\n    # Uncomment to train on 8 GPUs (default is 1)\n    GPU_COUNT = 1\n    BACKBONE = 'resnet50'\n    # Number of classes (including background)\n    NUM_CLASSES = 62  # 1 Background + 61 classes\n\n    STEPS_PER_EPOCH=10\n    VALIDATION_STEPS=10\n\n    LEARNING_RATE=0.001\n    IMAGE_MAX_DIM=256\n    IMAGE_MIN_DIM=256\nconfig = FoodChallengeConfig()\nconfig.display()","fb714fb0":"%cd ..\n!cp \/kaggle\/input\/food-recognition-challenge \/kaggle\/working -r\n!rm -rf images assets # to prevent displaying images at the bottom of a kernel","3a4843dd":"!cp \/kaggle\/working\/food-recognition-challenge\/train\/train\/annotations.json \/kaggle\/working\/food-recognition-challenge\/train\/train\/annotation.json","e11224c7":"#from mrcnn.dataset import FoodChallengeDataset\ndataset_train = FoodChallengeDataset()\ndataset_train.load_dataset(dataset_dir=os.path.join(\"\/kaggle\/working\/food-recognition-challenge\/train\", \"train\"), load_small=False)\n#dataset_train.load_dataset(dataset_dir=\"train\", load_small=False)\ndataset_train.prepare()\ndataset = dataset_train","1c458b4b":"from collections import Counter\nclass_counts = Counter()\nfor img_info in dataset_train.image_info:\n    ann = img_info['annotations']\n    for i in ann:\n        class_counts[i['category_id']] += 1\nclass_mapping = {i['id']: i['name'] for i in dataset_train.class_info}\n\nclass_counts = pd.DataFrame(class_counts.most_common(), columns=['class_name', 'count'])\nclass_counts['class_name'] = class_counts['class_name'].apply(lambda x: class_mapping[x])\nplt.figure(figsize=(12, 12))\nplt.barh(class_counts['class_name'], class_counts['count'])\nplt.title('Counts of classes of objects');","092916d5":"print(f'We have {class_counts.shape[0]} classes!')","45f4f44a":"# Load random image and mask.\nimage_id = random.choice(dataset.image_ids)\nimage = dataset.load_image(image_id)\nmask, class_ids = dataset.load_mask(image_id)\n# Compute Bounding box\nbbox = utils.extract_bboxes(mask)\n\n# Display image and additional stats\nprint(\"image_id \", image_id, dataset.image_reference(image_id))\nlog(\"image\", image)\nlog(\"mask\", mask)\nlog(\"class_ids\", class_ids)\nlog(\"bbox\", bbox)\n# Display image and instances\nvisualize.display_instances(image, bbox, mask, class_ids, dataset.class_names, figsize=(12, 12))","28dd022e":"dataset_train.image_info[0]","bb71844e":"class_images = defaultdict(list)\nfor ind, img_info in enumerate(dataset_train.image_info):\n    ann = img_info['annotations']\n    for i in ann:\n        class_images[i['category_id']].append(ind)","0e1ac60f":"image_ids = np.random.choice(dataset.image_ids, 4)\nfor class_id in np.random.choice(list(class_images.keys()), 10):\n    image_id = np.random.choice(class_images[class_id], 1)[0]\n    image = dataset.load_image(image_id)\n    mask, class_ids = dataset.load_mask(image_id)\n    visualize.display_top_masks(image, mask, class_ids, dataset.class_names)","e6e7fa11":"for idx, class_id in enumerate(np.random.choice(list(class_images.keys()), 10)):\n    image_id = np.random.choice(class_images[class_id], 1)[0]\n    image = dataset.load_image(image_id)\n    mask, class_ids = dataset.load_mask(image_id)\n    # Compute Bounding box\n    bbox = utils.extract_bboxes(mask)\n    visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names, figsize=(8, 8))","ac4ecacb":"# Generate Anchors\n\nbackbone_shapes = modellib.compute_backbone_shapes(config, config.IMAGE_SHAPE)\nanchors = utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES, \n                                          config.RPN_ANCHOR_RATIOS,\n                                          backbone_shapes,\n                                          config.BACKBONE_STRIDES, \n                                          config.RPN_ANCHOR_STRIDE)\n\n# Print summary of anchors\nnum_levels = len(backbone_shapes)\nanchors_per_cell = len(config.RPN_ANCHOR_RATIOS)\nprint(\"Count: \", anchors.shape[0])\nprint(\"Scales: \", config.RPN_ANCHOR_SCALES)\nprint(\"ratios: \", config.RPN_ANCHOR_RATIOS)\nprint(\"Anchors per Cell: \", anchors_per_cell)\nprint(\"Levels: \", num_levels)\nanchors_per_level = []\nfor l in range(num_levels):\n    num_cells = backbone_shapes[l][0] * backbone_shapes[l][1]\n    anchors_per_level.append(anchors_per_cell * num_cells \/\/ config.RPN_ANCHOR_STRIDE**2)\n    print(\"Anchors in Level {}: {}\".format(l, anchors_per_level[l]))","1976c867":"## Visualize anchors of one cell at the center of the feature map of a specific level\n\n# Load and draw random image\nimage_id = np.random.choice(dataset.image_ids, 1)[0]\nimage, image_meta, _, _, _ = modellib.load_image_gt(dataset, config, image_id)\nfig, ax = plt.subplots(1, figsize=(10, 10))\nax.imshow(image)\nlevels = len(backbone_shapes)\n\nfor level in range(levels):\n    colors = visualize.random_colors(levels)\n    # Compute the index of the anchors at the center of the image\n    level_start = sum(anchors_per_level[:level]) # sum of anchors of previous levels\n    level_anchors = anchors[level_start:level_start+anchors_per_level[level]]\n    print(\"Level {}. Anchors: {:6}  Feature map Shape: {}\".format(level, level_anchors.shape[0], \n                                                                  backbone_shapes[level]))\n    center_cell = backbone_shapes[level] \/\/ 2\n    center_cell_index = (center_cell[0] * backbone_shapes[level][1] + center_cell[1])\n    level_center = center_cell_index * anchors_per_cell \n    center_anchor = anchors_per_cell * (\n        (center_cell[0] * backbone_shapes[level][1] \/ config.RPN_ANCHOR_STRIDE**2) \\\n        + center_cell[1] \/ config.RPN_ANCHOR_STRIDE)\n    level_center = int(center_anchor)\n\n    # Draw anchors. Brightness show the order in the array, dark to bright.\n    for i, rect in enumerate(level_anchors[level_center:level_center+anchors_per_cell]):\n        y1, x1, y2, x2 = rect\n        p = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, facecolor='none',\n                              edgecolor=(i+1)*np.array(colors[level]) \/ anchors_per_cell)\n        ax.add_patch(p)","41a69145":"# Create data generator\nrandom_rois = 2000\ng = modellib.data_generator(\n    dataset, config, shuffle=True, random_rois=random_rois, \n    batch_size=4,\n    detection_targets=True)\n# Get Next Image\nif random_rois:\n    [normalized_images, image_meta, rpn_match, rpn_bbox, gt_class_ids, gt_boxes, gt_masks, rpn_rois, rois], \\\n    [mrcnn_class_ids, mrcnn_bbox, mrcnn_mask] = next(g)\n    \nelse:\n    [normalized_images, image_meta, rpn_match, rpn_bbox, gt_boxes, gt_masks], _ = next(g)\n    \nimage_id = modellib.parse_image_meta(image_meta)[\"image_id\"][0]\n\n# Remove the last dim in mrcnn_class_ids. It's only added\n# to satisfy Keras restriction on target shape.\nmrcnn_class_ids = mrcnn_class_ids[:,:,0]\n\nb = 0\n\n# Restore original image (reverse normalization)\nsample_image = modellib.unmold_image(normalized_images[b], config)\n\n# Compute anchor shifts.\nindices = np.where(rpn_match[b] == 1)[0]\nrefined_anchors = utils.apply_box_deltas(anchors[indices], rpn_bbox[b, :len(indices)] * config.RPN_BBOX_STD_DEV)\n\n# Get list of positive anchors\npositive_anchor_ids = np.where(rpn_match[b] == 1)[0]\nnegative_anchor_ids = np.where(rpn_match[b] == -1)[0]\nneutral_anchor_ids = np.where(rpn_match[b] == 0)[0]\n\n# ROI breakdown by class\nfor c, n in zip(dataset.class_names, np.bincount(mrcnn_class_ids[b].flatten())):\n    if n:\n        print(\"{:23}: {}\".format(c[:20], n))\n\n# Show positive anchors\nvisualize.draw_boxes(sample_image, boxes=anchors[positive_anchor_ids], \n                     refined_boxes=refined_anchors)","205e71e1":"if random_rois:\n    # Class aware bboxes\n    bbox_specific = mrcnn_bbox[b, np.arange(mrcnn_bbox.shape[1]), mrcnn_class_ids[b], :]\n\n    # Refined ROIs\n    refined_rois = utils.apply_box_deltas(rois[b].astype(np.float32), bbox_specific[:,:4] * config.BBOX_STD_DEV)\n\n    # Class aware masks\n    mask_specific = mrcnn_mask[b, np.arange(mrcnn_mask.shape[1]), :, :, mrcnn_class_ids[b]]\n\n    visualize.draw_rois(sample_image, rois[b], refined_rois, mask_specific, mrcnn_class_ids[b], dataset.class_names)\n    \n    # Any repeated ROIs?\n    rows = np.ascontiguousarray(rois[b]).view(np.dtype((np.void, rois.dtype.itemsize * rois.shape[-1])))\n    _, idx = np.unique(rows, return_index=True)\n    print(\"Unique ROIs: {} out of {}\".format(len(idx), rois.shape[1]))","a95b9d66":"!mkdir pretrained","c317dc47":"PRETRAINED_MODEL_PATH = os.path.join(\"pretrained\", \"mask_rcnn_coco.h5\")\nLOGS_DIRECTORY = os.path.join(ROOT_DIR, \"logs\")\nif not os.path.exists(PRETRAINED_MODEL_PATH):\n    utils.download_trained_weights(PRETRAINED_MODEL_PATH)","9838b2bb":"import keras.backend\nK = keras.backend.backend()\nif K=='tensorflow':\n    keras.backend.common.image_dim_ordering()\nmodel = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=LOGS_DIRECTORY)\nmodel_path = PRETRAINED_MODEL_PATH\nmodel.load_weights(model_path, by_name=True, exclude=[\n        \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n        \"mrcnn_bbox\", \"mrcnn_mask\"])","79bb6dad":"dataset_train = FoodChallengeDataset()\ndataset_train.load_dataset(os.path.join(\"\/kaggle\/working\/food-recognition-challenge\/train\", \"train\"), load_small=False)\ndataset_train.prepare()\ndataset_val = FoodChallengeDataset()\nval_coco = dataset_val.load_dataset(dataset_dir=os.path.join(\"\/kaggle\/working\/food-recognition-challenge\/val\", \"val\"), load_small=False, return_coco=True)\ndataset_val.prepare()","63803549":"class_names = dataset_train.class_names\nassert len(class_names)==62, \"Please check DatasetConfig\"","738342cf":"print(\"Training network\")\nmodel.train(dataset_train, dataset_val,\n            learning_rate=config.LEARNING_RATE,\n            epochs=1,\n            layers='heads')","fadcd836":"model_path = model.find_last()\nmodel_path","b0e599b5":"# I'll use my model trained locally\nmodel_path = '\/kaggle\/input\/food-model\/mask_rcnn_crowdai-food-challenge_0010.h5'","5c949add":"class InferenceConfig(FoodChallengeConfig):\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n    NUM_CLASSES = 62  # 1 Background + 61 classes\n    IMAGE_MAX_DIM=256\n    IMAGE_MIN_DIM=256\n    NAME = \"food\"\n    DETECTION_MIN_CONFIDENCE=0\n\ninference_config = InferenceConfig()\ninference_config.display()\n","16eb25d3":"# Recreate the model in inference mode\nmodel = modellib.MaskRCNN(mode='inference', \n                          config=inference_config,\n                          model_dir=ROOT_DIR)\n\n# Load trained weights (fill in path to trained weights here)\nassert model_path != \"\", \"Provide path to trained weights\"\nprint(\"Loading weights from \", model_path)\nmodel.load_weights(model_path, by_name=True)","11220165":"# Show few example of ground truth vs. predictions on the validation dataset \ndataset = dataset_val\nfig = plt.figure(figsize=(10, 30))\n\nfor i in range(4):\n\n    image_id = random.choice(dataset.image_ids)\n    \n    original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n        modellib.load_image_gt(dataset_val, inference_config, \n                               image_id, use_mini_mask=False)\n    \n    print(original_image.shape)\n    plt.subplot(6, 2, 2*i + 1)\n    visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n                                dataset.class_names, ax=fig.axes[-1])\n    \n    plt.subplot(6, 2, 2*i + 2)\n    results = model.detect([original_image]) #, verbose=1)\n    r = results[0]\n    visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n                                dataset.class_names, r['scores'], ax=fig.axes[-1])","27ae22a5":"Now let's use a function from the repo to see information about one random image","1f117e81":"Let's see what information we have about images:","0e1c1428":"* There is some meta information: path, height, width\n* for each object we have annotatioins: class, segmentation mask, total area, bbox coordinates.","2a22ce8a":"### Masks","84324d08":"We can see masks and boxes for different classes. Let's take 10 random classes","22ae62fd":"## General information\n\nIn this kernel I work with a dataset for the Food Recognition Challenge conducted on AIcrowd hosted here: https:\/\/www.aicrowd.com\/challenges\/food-recognition-challenge.\n```\nThis is a novel dataset of food images collected through the MyFoodRepo app where numerous volunteer Swiss users provide images of their daily food intake in the context of a digital cohort called Food & You. This growing data set has been annotated - or automatic annotations have been verified - with respect to segmentation, classification (mapping the individual food items onto an ontology of Swiss Food items), and weight \/ volume estimation.\n\nThis is an evolving dataset, where we will release more data as the dataset grows over time.\n```\n\n![](https:\/\/i.imgur.com\/Syv1Ycf.png)\n\nIn this kernel I'll show how to set up environment for this challenge, provide EDA and possible in future show baseline modelling.\n\nThe code is based on this notebook by organizers: https:\/\/colab.research.google.com\/drive\/1A5p9GX5X3n6OMtLjfhnH6Oeq13tWNtFO#scrollTo=lkjrKJfIVCM3","b4341cc9":"We can see that some masks are big, some are small. Some have a single area, some have multiple areas.","56f9b4bd":"## Data exploration","47ecd2e9":"### ROI\nRegion of interest pooling (also known as RoI pooling) is an operation widely used in object detection tasks using convolutional neural networks. For example, to detect multiple fruits and vegetables in a single image. Its purpose is to perform max pooling on inputs of nonuniform sizes to obtain fixed-size feature maps (e.g. 7\u00d77).\n\nThe result is that from a list of rectangles with different sizes we can quickly get a list of corresponding feature maps with a fixed size. Note that the dimension of the RoI pooling output doesn\u2019t actually depend on the size of the input feature map nor on the size of the region proposals. It\u2019s determined solely by the number of sections we divide the proposal into. What\u2019s the benefit of RoI pooling? One of them is processing speed. If there are multiple object proposals on the frame (and usually there\u2019ll be a lot of them), we can still use the same input feature map for all of them. Since computing the convolutions at early stages of processing is very expensive, this approach can save us a lot of time.","00acbf4c":"### import libraries","f6fcfca0":"## Modelling with Mask-RCNN\n\nThe code is based on the baseling by organizers:\nhttps:\/\/discourse.aicrowd.com\/t\/new-starter-notebook-paperspace\/2754\/1","ec137f64":"## Setting up environment\n\nThere are several steps which need to be done:\n* install certain versions of numpy, tensorflow, keras\n* clone the Mask_RCNN repo\n* install requirements and the repo itself\n* the utils reqires json with annotations to be called `annotation.json`, but we have `annotations.json`; so I copy the whole data and copy this file with a new name","1c14c9ac":"The most common is water - I suppose it is a background. Some vegetables and white bread are the most common.","2789e709":"Obviously bounding boxed have masks inside them. And if an object has several masks, then the bounding box will contain all the masks.","6f4c44e6":"### Anchors\n\nOne more important type of annotatioin is anchor. Anchors are a set of boxes with predefined locations and scales relative to images. These boxes are defined to capture the scale and aspect ratio of specific object classes you want to detect and are typically chosen based on object sizes in your training datasets.","8b7409fb":"### Defining dataset class and config","126af78e":"## Looking at the predictions","a4811042":"### Bounding Boxes","eacfe34e":"### Warning\n\nPlease, notice that in config values of `STEPS_PER_EPOCH` and `VALIDATION_STEPS` are quite low. I decreased them so that model would train fast, but the quality will be low. When you train the model, increase the values up to 50-200."}}