{"cell_type":{"0531dc5e":"code","542f36a6":"code","8d1c9599":"code","fe352d85":"code","210ea95e":"code","ceed5bd9":"code","0153b0c0":"code","14ab2e8b":"code","1b9a4dbb":"code","ec475f87":"code","b8993200":"code","cff84a1f":"code","5906d486":"code","d6b4f32a":"code","5e720732":"code","d8a86910":"code","51cd8663":"code","12ce8887":"code","b1e86df3":"code","b8cd3034":"code","07f89ae4":"code","353edfa6":"code","792464f2":"code","d77f0a00":"code","c586126e":"code","dbeddf3d":"code","f0b93620":"code","fdcd8306":"code","7cca1a29":"code","044ed655":"code","06013f4a":"code","4731b37f":"code","483015d0":"code","666530e0":"code","0f1fe673":"code","3d3b3cf5":"code","97a273fb":"code","ad04a608":"code","22f5023e":"code","7bc29797":"code","c7edbda9":"code","71bb79bd":"code","51bb59e3":"code","8a91ba6a":"code","7da524c9":"code","1609e5b4":"markdown","06744695":"markdown","a29b8c7f":"markdown","a31067dd":"markdown","02613b31":"markdown","962f42c5":"markdown","dc0341d3":"markdown","92d38fa2":"markdown","c2c14eda":"markdown","df438002":"markdown"},"source":{"0531dc5e":"import numpy as np\nimport tensorflow as tf\nprint(tf.__version__)\n\nimport matplotlib.pyplot as plt\nimport pandas as pd","542f36a6":"#!unzip \/content\/14.18_Genre_Classification_Dataset.zip\ndf = pd.read_csv('..\/input\/genre-classification-dataset-imdb\/Genre Classification Dataset\/train_data.txt', delimiter = \" ::: \", header=None, index_col=0)\ndf","8d1c9599":"df.info()","fe352d85":"df.groupby(2).count().sort_values(1, ascending=False).reset_index()[[2,1]]","210ea95e":"df[2] = pd.Categorical(df[2])\ndf['labels'] = df[2].cat.codes\ndf","ceed5bd9":"import nltk\nimport string\n\nnltk.download('punkt', download_dir='.')\n# \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u0430\nstop_words = [\n    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\",\n    'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers',\n    'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n    'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been',\n    'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n    'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',\n    'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out',\n    'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n    'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not',\n    'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'shold',\n    \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n    'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\",\n    'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n    'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"\n]\ndef tokenize_text(raw_text: str):\n    \"\"\"\u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u0438 \u0442\u0435\u043a\u0441\u0442\u0430\n    \n    :param raw_text: \u0438\u0441\u0445\u043e\u0434\u043d\u0430\u044f \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u0430\u044f \u0441\u0442\u0440\u043e\u043a\u0430\n    \"\"\"\n    tokenized_str = nltk.word_tokenize(raw_text)\n    tokens = [i.lower() for i in tokenized_str if ( i not in string.punctuation )]\n    filtered_tokens = [i for i in tokens if ( i not in stop_words )]\n    return filtered_tokens","0153b0c0":"from tqdm import tqdm\ntqdm.pandas()\n\n# \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0432 \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u0443 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0435\u0442\u043e\u0434\u0430 .apply()\ndf['tokenized']= df[3].progress_apply(tokenize_text)\ndf","14ab2e8b":"plt.hist(df['tokenized'].apply(len), bins=30, log=True)\nplt.title('Distribution of the number of tokens in sequences')\nplt.xlabel('Length of sequence')\nplt.show()","1b9a4dbb":"#from gensim.models import Word2Vec\n#import logging\n\n#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n#texts = df.tokenized.values\n#model_wv = Word2Vec(texts, vector_size =20, min_count=2, workers=8, epochs=10, sg=0) # , size=10, window=7","ec475f87":"#!cat \/opt\/conda\/lib\/python3.7\/site-packages\/gensim\/models\/word2vec.py","b8993200":"from gensim.models import Word2Vec # \u0440\u0435\u0448\u0438\u043b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c\nimport gensim.downloader as api\n\ninfo = api.info()  # show info about available models\/datasets\nmodel_wv2 = api.load(\"glove-twitter-25\")  # download the model and return as object ready for use\nmodel_wv2.most_similar(\"cat\")","cff84a1f":"def Text2Vec(lst):\n  \n  vecs = []\n  for tok in lst:\n    try:\n      vecs.append(model_wv2[tok])\n    except KeyError:\n      pass \n\n  return np.array(vecs[:256])\n\ndf['vecs'] = df['tokenized'].progress_apply(Text2Vec)\ndf","5906d486":"df.to_csv('drama.csv')\n!zip drama.zip drama.csv","d6b4f32a":"MAX_SEQ_LEN = 256 # \u0424\u0438\u043d\u0430\u043b\u044c\u043d\u0430\u044f \u0434\u043b\u0438\u043d\u0430 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438\nPAD = np.zeros(25)\n\ntrain_labels = df['labels'].values\n\ntrain_data = tf.keras.preprocessing.sequence.pad_sequences(\n    df['vecs'].values,\n    value= PAD,\n    padding='post', dtype='float32',\n    maxlen=MAX_SEQ_LEN)\n\nprint(\"Length examples: {}\".format([len(train_data[0]), len(train_data[1])]))\nprint('=====================================')\nprint(\"Entry example: {}\".format(train_data[0]))","5e720732":"BUFFER_SIZE = 1000\nBATCH_SIZE = 256\n\ndataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)","d8a86910":"x_val = train_data[:10000]\npartial_x_train = train_data[10000:]\n\ny_val = train_labels[:10000]\npartial_y_train = train_labels[10000:]","51cd8663":"#hidden_nodes = int(2\/3 * (word_vec_length * char_vec_length))","12ce8887":"model_lstm = tf.keras.Sequential([\n    \n    tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(32, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),\n    tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(27, activation=tf.nn.softmax)#\n])\n\noptimizer = tf.keras.optimizers.Adam()\nmodel_lstm.compile(optimizer=optimizer,\n              loss='SparseCategoricalCrossentropy',\n              metrics=['acc'])\n\nmodel_lstm.build(input_shape=(None, 256, 25))\nmodel_lstm.summary()","b1e86df3":"BATCH_SIZE = 256\nNUM_EPOCHS = 10\n\nhistory = model_lstm.fit(train_data,\n                    train_labels, validation_data=(x_val, y_val),\n                    epochs = NUM_EPOCHS,\n                    batch_size = BATCH_SIZE,\n                    verbose=1) #","b8cd3034":"plt.plot(history.history['acc'])","07f89ae4":"!zip -r mdl.zip \/content\/lstm.mdl","353edfa6":"optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)","792464f2":"BATCH_SIZE = 256\nNUM_EPOCHS = 5\n\nhistory = model_lstm.fit(train_data,\n                    train_labels,\n                    epochs = NUM_EPOCHS,\n                    batch_size=BATCH_SIZE,\n                    verbose=1) #validation_data=(x_val, y_val)","d77f0a00":"model_lstm.save('lstm.mdl')\nmodel_lstm.save_weights('lstm weights.dat')","c586126e":"model_lstm2 = tf.keras.Sequential([\n    tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 25]),\n    tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(32, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),\n    tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(32, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(27, activation=tf.nn.softmax)\n])\n\noptimizer = tf.keras.optimizers.Adam()\nmodel_lstm2.compile(optimizer=optimizer,\n              loss='SparseCategoricalCrossentropy',\n              metrics=['acc'])\n\nmodel_lstm2.summary()","dbeddf3d":"BATCH_SIZE = 256\nNUM_EPOCHS = 10\n\nhistory2 = model_lstm2.fit(partial_x_train,\n                    partial_y_train, validation_data=(x_val, y_val),\n                    epochs = NUM_EPOCHS,\n                    batch_size=BATCH_SIZE,\n                    verbose=1) ","f0b93620":"model_lstm2.save('lstm2.mdl')\n!zip -r mdl2.zip \/content\/lstm2.mdl","fdcd8306":"inputs = tf.keras.Input(shape=(256, 25))\nx = tf.keras.layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\",\n                      strides=1, padding=\"causal\")(inputs)\nx = tf.keras.layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\",\n                      strides=1, padding=\"causal\")(x)\nconv_output = tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(32, return_sequences=False, dropout=0.2, recurrent_dropout=0.2))(x)\n\nx = tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(32, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(inputs)\nx = tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(32, return_sequences=False, dropout=0.2, recurrent_dropout=0.2))(x)\n\nx = tf.keras.layers.Concatenate(axis=1)([x, conv_output])\n\nx = tf.keras.layers.Dense(64, activation='relu')(x)\nx = tf.keras.layers.Dense(32, activation='relu')(x)\nx = tf.keras.layers.Dense(32, activation='relu')(x)\noutputs = tf.keras.layers.Dense(27, activation=tf.nn.softmax)(x)\n\nmodel_lstm3 = tf.keras.Model(inputs, outputs, name=\"sw\")\nmodel_lstm3.summary()","7cca1a29":"optimizer = tf.keras.optimizers.Adam()\nmodel_lstm3.compile(optimizer=optimizer,\n              loss='SparseCategoricalCrossentropy',\n              metrics=['acc'])","044ed655":"NUM_EPOCHS = 10\n\nhistory = model_lstm3.fit(dataset,\n                    epochs = NUM_EPOCHS,\n                    verbose=1) ","06013f4a":"model_lstm3.save('lstm3.mdl')\n!zip -r mdl3.zip .\/lstm3.mdl","4731b37f":"model_lstm3 = tf.keras.models.load_model('..\/input\/trained-model\/lstm3.mdl')","483015d0":"import math\n\nNUM_EPOCHS = 10\ninitial_learning_rate = 0.0005\n\ndef lr_step_decay(epoch, lr):\n    drop_rate = 0.3\n    epochs_drop = 3.0\n    return initial_learning_rate * math.pow(drop_rate, math.floor(epoch\/epochs_drop))\n\nhistory = model_lstm3.fit(dataset,\n                    callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_step_decay, verbose=1)],\n                    epochs = NUM_EPOCHS,\n                    verbose=1)\n\nmodel_lstm3.save('lstm4.mdl')\n!zip -r mdl4.zip .\/lstm4.mdl","666530e0":"model_lstm3 = tf.keras.models.load_model('..\/input\/trained-model2\/lstm4.mdl')","0f1fe673":"df_test = pd.read_csv('..\/input\/genre-classification-dataset-imdb\/Genre Classification Dataset\/test_data_solution.txt', delimiter = \" ::: \", header=None, index_col=0)\ndf_test","3d3b3cf5":"df_test[2] = pd.Categorical(df_test[2])\ndf_test['labels'] = df_test[2].cat.codes\ndf[2].cat.categories","97a273fb":"df_test[2].cat.categories # \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u044b\u0435","ad04a608":"# \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0432 \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u0443 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0435\u0442\u043e\u0434\u0430 .apply()\ndf_test['tokenized']= df_test[3].progress_apply(tokenize_text)\ndf_test['vecs'] = df_test['tokenized'].progress_apply(Text2Vec)\ndf_test","22f5023e":"test_labels = df_test['labels'].values\n\ntest_data = tf.keras.preprocessing.sequence.pad_sequences(\n    df_test['vecs'].values,\n    value= PAD,\n    padding='post', dtype='float32',\n    maxlen=MAX_SEQ_LEN)\n\nprint(\"Length examples: {}\".format([len(train_data[0]), len(train_data[1])]))\nprint('=====================================')\nprint(\"Entry example: {}\".format(train_data[0]))","7bc29797":"test_data.shape","c7edbda9":"test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_labels))\ntest_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\ntest_dataset","71bb79bd":"results = model_lstm3.evaluate(test_dataset)\n\nprint('Test loss: {:.4f}'.format(results[0]))\nprint('Test accuracy: {:.2f} %'.format(results[1]*100))","51bb59e3":"epochs = range(1, len(history.history['acc']) + 1)\n\nplt.figure()\nplt.plot(epochs, history.history['loss'], 'bo', label='Training loss')\n#plt.plot(epochs, history.history['val_loss'], 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid()\n\nplt.figure()\nplt.plot(epochs, history.history['acc'], 'bo', label='Training acc')\n#plt.plot(epochs, history.history['val_acc'], 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid()","8a91ba6a":"text = 'It was a really good movie it was so fantastic'\n\nprint(tokenize_text(text))\nencoded = Text2Vec(tokenize_text(text))\n#print(encoded)\nencoded = np.array(encoded)[None, :]\n\nencoded_padded = tf.keras.preprocessing.sequence.pad_sequences(\n    encoded,\n    value= PAD,\n    padding='post', dtype='float32',\n    maxlen=MAX_SEQ_LEN)\n\nprediction = model_lstm3.predict(encoded_padded)[0,0]\n\nprint(prediction)\n#print('Positive' if prediction > 0.5 else 'Negative')","7da524c9":"df[2].cat.categories[int(prediction)]","1609e5b4":"### \u0413\u0440\u0430\u0444\u0438\u043a\u0438 \u043b\u043e\u0441\u0441\u0430 \u0438 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u043c \u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\u0445","06744695":"### \u041f\u0440\u0438\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u0432\u0441\u0435\u0445 \u0446\u0435\u043f\u043e\u0447\u0435\u043a \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u043a \u043e\u0434\u043d\u043e\u0439 \u0434\u043b\u0438\u043d\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043f\u0430\u0434\u0434\u0438\u043d\u0433\u0430\n\u041e\u0434\u043d\u0430 \u0446\u0435\u043f\u043e\u0447\u043a\u0430 \u044d\u0442\u043e \u043e\u0434\u0438\u043d \u043e\u0442\u0437\u044b\u0432. \u0410 \u043e\u0442\u0437\u044b\u0432\u044b \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u043e\u0439 \u0434\u043b\u0438\u043d\u044b. \n\n\u0422\u0430\u043a \u043a\u0430\u043a \u043d\u0430\u043c \u0431\u0443\u0434\u0435\u0442 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0441\u043e\u0431\u0438\u0440\u0430\u0442\u044c \u0438\u0437 \u043e\u0442\u0437\u044b\u0432\u043e\u0432 \u0431\u0430\u0442\u0447\u0438 (\u0433\u0440\u0443\u043f\u043f\u044b \u0446\u0435\u043f\u043e\u0447\u0435\u043a), \u0443\u0434\u043e\u0431\u043d\u043e \u0432\u0441\u0435 \u043e\u0442\u0437\u044b\u0432\u044b \u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438 \u043a \u043e\u0434\u043d\u043e\u0439 \u0434\u043b\u0438\u043d\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043f\u0430\u0434\u0438\u043d\u0433\u0430 \u0432 \u043a\u043e\u043d\u0446\u0435 (\u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435\u043c \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 `<PAD>`).","a29b8c7f":"# \u041c\u043e\u0434\u0435\u043b\u044c","a31067dd":"### \u041e\u0446\u0435\u043d\u043a\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435","02613b31":"#+## \u0422\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u043d\u043e\u0432\u043e\u0439 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438\n\n\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u0440\u0430\u0431\u043e\u0442\u0443 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u043c \u043e\u0442\u0437\u044b\u0432\u0435.\n\n\u041d\u0430\u043f\u0438\u0448\u0435\u043c \u0442\u0435\u043a\u0441\u0442 (\u043e\u0442\u0437\u044b\u0432), \u0437\u0430\u043a\u043e\u0434\u0438\u0440\u0443\u0435\u043c \u0435\u0433\u043e \u0432 \u0438\u043d\u0434\u0435\u043a\u0441\u044b \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e `encode_review` \u0438 \u043f\u043e5\u0430\u0434\u0438\u043c \u043d\u0430 \u0432\u0445\u043e\u0434 \u0432 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u0443\u044e \u0441\u0435\u0442\u044c, \u043f\u0440\u0438\u043a\u043b\u0435\u0438\u0432 \u0431\u0430\u0442\u0447-\u0438\u0437\u043c\u0435\u0440\u0435\u043d\u0438\u0435. \u0412\u044b\u0445\u043e\u0434 \u0438\u0437 \u0441\u0435\u0442\u0438 -- \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u0442\u043e\u0433\u043e, \u0447\u0442\u043e \u043e\u0442\u0437\u044b\u0432 \u043f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439.","962f42c5":"#\u0420\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f  \u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0430 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\n\n\u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0430 \u0442\u0435\u043a\u0441\u0442\u043e\u0432 \u043d\u0430 \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u043e\u0442\u0437\u044b\u0432\u043e\u0432 \u043d\u0430 \u0444\u0438\u043b\u044c\u043c\u044b (\u043f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0438\u043b\u0438 \u043e\u0442\u0440\u0438\u0446\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0439). \u0414\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 IMDB.","dc0341d3":"# \u041c\u043e\u0434\u0435\u043b\u044c 3","92d38fa2":"### \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 IMDB\n\u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 IMDB (\u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0443\u044e \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0438). \u0412\u043e \u0432\u0440\u0435\u043c\u044f \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u043c\u043e\u0436\u043d\u043e \u0443\u043a\u0430\u0437\u0430\u0442\u044c, \u043a\u0430\u043a\u043e\u0435 \u043f\u043e\u0434\u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u043e \u0441\u043b\u043e\u0432\u0430\u0440\u044f \u043c\u044b \u0445\u043e\u0442\u0438\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c, \u0443\u043a\u0430\u0437\u0430\u0432 `num_words=VOCAB_SIZE`, \u0442\u043e\u0433\u0434\u0430 \u043e\u0441\u0442\u0430\u043d\u0443\u0442\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u0441\u0430\u043c\u044b\u0435 \u0447\u0430\u0441\u0442\u044b\u0435 \u0441\u043b\u043e\u0432\u0430.\n\n\u0412 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 (\u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f) \u0443\u0436\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b \u0438\u043d\u0434\u0435\u043a\u0441\u0430\u043c\u0438 \u0441\u043b\u043e\u0432.","c2c14eda":"# \u041c\u043e\u0434\u0435\u043b\u044c #2","df438002":"### \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\nTensorFlow \u0434\u043e\u043b\u0436\u0435\u043d \u0438\u043c\u0435\u0442\u044c \u043a\u0430\u043a \u043c\u0438\u043d\u0438\u043c\u0443\u043c \u0432\u0435\u0440\u0441\u0438\u044e 2.0"}}