{"cell_type":{"9b10f3b2":"code","940200e7":"code","54893deb":"code","42ba9f13":"code","3d75f001":"code","5e6d3583":"code","6df49f82":"code","39541a41":"code","3513945b":"code","733aea8c":"code","de0f2a47":"code","b66bb535":"code","69dd33bf":"code","a6af37cf":"code","657f90fc":"code","f59b6369":"code","ddc33c66":"code","d2286951":"code","14ad45ef":"code","dbec7543":"code","ac5be9bd":"code","60802273":"code","e8422504":"code","8a99848b":"code","94ae7f11":"code","c861ae2d":"code","683791bb":"code","cafef50f":"code","798e401f":"code","3a741dd5":"code","0038719c":"code","441c9168":"code","80c965c3":"code","602c8bb2":"code","861abda7":"code","a830a915":"code","817242ea":"code","9fa10afa":"code","402ef4bc":"code","489d8b06":"code","da4174fc":"code","871962a9":"code","0cfb64d1":"code","043ed561":"code","3e715287":"code","6ce74faf":"code","83dc0d52":"code","5e58ca8f":"code","cb9bec1f":"code","7a9253c0":"markdown","571cfa03":"markdown","13bb744a":"markdown","fa1fa54d":"markdown","d31a10a0":"markdown","73d01bd6":"markdown"},"source":{"9b10f3b2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","940200e7":"import scipy\nfrom scipy.cluster import hierarchy as hc\nimport re, math, pathlib, numbers, functools, IPython, graphviz\nimport numpy as np \nimport pandas as pd\nimport altair as alt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport category_encoders as ce\nfrom pathlib import Path\nfrom pdpbox import pdp, get_dataset, info_plots\nfrom concurrent.futures import ProcessPoolExecutor\nfrom sklearn import metrics, ensemble, model_selection, tree\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_validate, cross_val_predict\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder, StandardScaler, FunctionTransformer\nfrom sklearn.impute import SimpleImputer, MissingIndicator\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.tree import export_graphviz","54893deb":"import datatable as dt  # pip install datatable\nimport pandas as pd","42ba9f13":"%%time\ntps_dt = dt.fread(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\").to_pandas()\ntps_dt","3d75f001":"%%time\ntrain = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain.head()","5e6d3583":"# Check for missing values in train & test sets.\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\ntrain.isnull().sum().to_frame(name='num_missing').query('num_missing>0').sort_values(by='num_missing', ascending=False).plot.barh(ax=ax1, figsize=(10,6));\ntest.isnull().sum().to_frame(name='num_missing').query('num_missing>0').sort_values(by='num_missing', ascending=False).plot.barh(ax=ax2, figsize=(10,6));\nplt.tight_layout()","6df49f82":"#For PoolQC, NaN is not missing data but means no pool, likewise for Fence, FireplaceQu etc.\ncols_fillna = ['PoolQC','MiscFeature','Alley','Fence','MasVnrType','FireplaceQu',\n               'GarageQual','GarageCond','GarageFinish','GarageType', 'Electrical',\n               'BsmtExposure','BsmtCond','BsmtQual','BsmtFinType1','BsmtFinType2']\nfor col in cols_fillna:\n    train[col].fillna('no',inplace=True)\n    test[col].fillna('no',inplace=True)","39541a41":"# List of features having null values\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","3513945b":"train.fillna(train.median(), inplace=True)\ntest.fillna(test.median(), inplace=True)","733aea8c":"#Null values in train and test set\nprint(train.isnull().sum().sum())\nprint(test.isnull().sum().sum())","de0f2a47":"#Checking for missing values\ntest.isnull().sum().sort_values(ascending=False)[:10]","b66bb535":"# Filling missing values in test\nfor col in test.columns:\n    if test[col].dtypes=='object' and test[col].isnull().sum()>0:\n        test[col].fillna(test[col].mode()[0],inplace=True)","69dd33bf":"test.isnull().sum().sum()","a6af37cf":"#Distribution of target value\ntrain['SalePrice'].hist(bins = 100)","657f90fc":"#Plotting correlation \nplt.figure(figsize=(12,10))\nsns.heatmap(train.corr(), cmap='Greys')","f59b6369":"#Correlation data\ntrain.corr()","ddc33c66":"#Finding top 15 corelation variables \ncorr_cols = train.corr()[\"SalePrice\"].nlargest(15).index\ncorr_cols\n","d2286951":"#Plotting the top 15 feature values with values\nplt.figure(figsize=(10, 6))\nsns.heatmap(train.loc[:, corr_cols].corr(), annot=True, cmap=\"gray\")","14ad45ef":"#Plotting target value with OverallQual\ntrain.plot(kind = 'scatter', x = 'OverallQual', y=\"SalePrice\", alpha=0.25)","dbec7543":"#Plotting target value with GrLivArea\ntrain.plot(kind = 'scatter', x = 'GrLivArea', y=\"SalePrice\", alpha=0.1)","ac5be9bd":"#Removing values that are scatttered\ntrain = train[train['GrLivArea']<4500]","60802273":"#Plotting target value with GarageArea\ntrain.plot(kind = 'scatter', x = 'GarageArea', y=\"SalePrice\", alpha=0.25)","e8422504":"#Removing values that are scatttered\ntrain = train[train['GarageArea']<1200]","8a99848b":"#Plotting target value with ID\ntrain.plot(kind = 'scatter', x = 'Id', y=\"SalePrice\", alpha=0.25)","94ae7f11":"# Exclude the two rows that are over 700k\ntrain = train[train[\"SalePrice\"] < 700000]","c861ae2d":"# Removing SalePrice and ID from train and test sets\ny = train.pop('SalePrice')\ntrain.pop('Id')\ntest_ids = test.pop('Id')","683791bb":"#Segregating categorical and numerical features\nnum_feats = [col for col in train.columns if train[col].dtype!='object']\n\n\ncat_feats = [col for col in train.columns if train[col].dtype=='object']","cafef50f":"# Encoding categorical features\nfrom sklearn.preprocessing import OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\ntrain[cat_feats] = ordinal_encoder.fit_transform(train[cat_feats])\ntest[cat_feats] = ordinal_encoder.fit_transform(test[cat_feats])","798e401f":"#Splitting the train data \nX_train, X_test, y_train, y_test = train_test_split(train, y, test_size = 0.2, random_state = 1)","3a741dd5":"#Scaling the features value\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","0038719c":"#Importing XGBoost and creating DMatrices for XGBoost\nimport xgboost as xgb\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\ndtest_1 = xgb.DMatrix(test)","441c9168":"from sklearn.metrics import mean_absolute_error\n# \"Learn\" the mean from the training data\nmean_train = np.mean(y_train)\n# Get predictions on the test set\nbaseline_predictions = np.ones(y_test.shape) * mean_train\n# Compute MAE\nmae_baseline = mean_absolute_error(y_test, baseline_predictions)\nprint(\"Baseline MAE is {:.2f}\".format(mae_baseline))","80c965c3":"#params dictionary\nparams = {\n    # Parameters that we are going to tune.\n    'max_depth':6,\n    'min_child_weight': 1,\n    'eta':.3,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    # Other parameters\n    'objective':'reg:linear',\n}","602c8bb2":"#add the evaluation metric\nparams['eval_metric'] = \"mae\"","861abda7":"#Introducing num_boost_round\nnum_boost_round = 999","a830a915":"#Finding the best number of boosting rounds\nmodel = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=10\n)\n\nprint(\"Best MAE: {:.2f} with {} rounds\".format(\n                 model.best_score,\n                 model.best_iteration+1))\n","817242ea":"cv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    seed=42,\n    nfold=5,\n    metrics={'mae'},\n    early_stopping_rounds=10\n)\ncv_results","9fa10afa":"#Finding best max_depth and min_child_weight\ngridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in range(9,12)\n    for min_child_weight in range(5,8)\n]","402ef4bc":"# Define initial best params and MAE\nmin_mae = float(\"Inf\")\nbest_params = None\nfor max_depth, min_child_weight in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}\".format(\n                             max_depth,\n                             min_child_weight))\n    # Update our parameters\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=42,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=10\n    )\n    # Update best MAE\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].argmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (max_depth,min_child_weight)\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))","489d8b06":"#Updating our params\nparams['max_depth'] = 9\nparams['min_child_weight'] = 5","da4174fc":"#Finding subsample and colsample_bytree\ngridsearch_params = [\n    (subsample, colsample)\n    for subsample in [i\/10. for i in range(7,11)]\n    for colsample in [i\/10. for i in range(7,11)]\n]","871962a9":"min_mae = float(\"Inf\")\nbest_params = None\n# We start by the largest values and go down to the smallest\nfor subsample, colsample in reversed(gridsearch_params):\n    print(\"CV with subsample={}, colsample={}\".format(\n                             subsample,\n                             colsample))\n    # We update our parameters\n    params['subsample'] = subsample\n    params['colsample_bytree'] = colsample\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=42,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=10\n    )\n    # Update best score\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].argmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (subsample,colsample)\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))","0cfb64d1":"#Updating the params\nparams['subsample'] = 1\nparams['colsample_bytree'] = 1","043ed561":"#Parameter ETA\n%time\n# This can take some time\u2026\nmin_mae = float(\"Inf\")\nbest_params = None\nfor eta in [.3, .2, .1, .05, .01, .005]:\n    print(\"CV with eta={}\".format(eta))\n    # We update our parameters\n    params['eta'] = eta\n    # Run and time CV\n    %time cv_results = xgb.cv(params,dtrain,num_boost_round=num_boost_round,seed=42,nfold=5,metrics=['mae'],early_stopping_rounds=100)\n            \n            \n    # Update best score\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].argmin()\n    print(\"\\tMAE {} for {} rounds\\n\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = eta\nprint(\"Best params: {}, MAE: {}\".format(best_params, min_mae))","3e715287":"#Updating EtA\nparams['eta'] = .01","6ce74faf":"#Final parameters\nparams","83dc0d52":"#train the model with above params\nmodel = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=10\n)\n\nprint(\"Best MAE: {:.2f} in {} rounds\".format(model.best_score, model.best_iteration+1))","5e58ca8f":"#Predicting our test set with above model\npred = model.predict(dtest_1)","cb9bec1f":"submission_1 = pd.DataFrame({\"id\": test_ids, \"SalePrice\": pred.reshape(-1)})\nsubmission_1.to_csv(\"submission_housing_4.csv\", index=False)","7a9253c0":"Filling null values","571cfa03":"### **Using XGBoost\u2019s CV**","13bb744a":"**Exploratory Data Analysis**","fa1fa54d":"# **XGB 1**","d31a10a0":"**Loading the dataset**","73d01bd6":"##  **Training and Tuning an XGBoost model**"}}