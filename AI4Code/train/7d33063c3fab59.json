{"cell_type":{"fada3219":"code","919b7691":"code","729238d4":"code","010f0fef":"code","08992b0d":"code","16000b96":"code","20b74942":"code","ffdc1820":"code","a1439d30":"code","cf7074f6":"code","6602b0ad":"code","e0887ba6":"code","b547b3ba":"code","946a1f12":"code","b8cfe5d4":"code","b3c73d71":"code","dd64ea75":"code","8688e856":"code","735f7964":"code","cd168005":"code","037a7bc1":"code","9e547431":"code","a470b378":"code","65f5cb89":"code","9f33b6ea":"code","72fa269f":"code","fc56c699":"code","8ad2a053":"code","8fea3814":"code","5d64deab":"code","f0802131":"markdown","a611bbd1":"markdown","1b6c7fc7":"markdown","4eac7f03":"markdown","b21ceb5c":"markdown","e10bf393":"markdown","d81fb5fc":"markdown","4664c470":"markdown","874214c4":"markdown","e841de9a":"markdown","b1e15c79":"markdown","fc4d979e":"markdown","a327a819":"markdown","3d1caef4":"markdown","543c28b1":"markdown","c6501c00":"markdown","15440c43":"markdown","2634ef13":"markdown","27dc757a":"markdown","db8f91bf":"markdown","55c3e0f2":"markdown","b7e4e207":"markdown","4a0c595a":"markdown","a1ecda1c":"markdown","652ee125":"markdown"},"source":{"fada3219":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline","919b7691":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df  = pd.read_csv(\"..\/input\/test.csv\")\n\n(len(train_df), len(test_df))","729238d4":"# Full dataset is needed for imputing missing values & also for pruning outliers\n\ntrain_len = len(train_df)\ntitanic_df = pd.concat([train_df, test_df], axis=0, ignore_index=True, sort=True)\ntitanic_df.info()","010f0fef":"# Impute \"Embarked\" missing values with the most common value 'S'\n\nsns.countplot(x='Embarked', data=titanic_df)","08992b0d":"titanic_df['Embarked'] = titanic_df['Embarked'].fillna(value='S')","16000b96":"# Extract Title from Name, store in column and plot barplot\n\nimport re\n\ntitanic_df['Title'] = titanic_df.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))\n\nsns.countplot(x='Title', data=titanic_df);\nplt.xticks(rotation=45);","20b74942":"# Replace rare Title with corresponding common Title\n\ntitanic_df['Title'] = titanic_df['Title'].replace({'Mlle': 'Miss', \n                                                   'Major': 'Mr', \n                                                   'Col': 'Mr', \n                                                   'Sir': 'Mr', \n                                                   'Don': 'Mr', \n                                                   'Mme': 'Miss', \n                                                   'Jonkheer': 'Mr', \n                                                   'Lady': 'Mrs', \n                                                   'Capt': 'Mr', \n                                                   'Countess': 'Mrs', \n                                                   'Ms': 'Miss', \n                                                   'Dona': 'Mrs'})\n\nsns.countplot(x='Title', data=titanic_df);\nplt.xticks(rotation=45);","ffdc1820":"# Impute \"Age\" by median of Age of Name's Title group\n\ntitles = ['Dr', 'Master', 'Miss', 'Mr', 'Mrs', 'Rev']\nfor title in titles:\n    age_to_impute = titanic_df.groupby('Title')['Age'].median()[titles.index(title)]\n    titanic_df.loc[(titanic_df['Age'].isnull()) & (titanic_df['Title'] == title), 'Age'] = age_to_impute","a1439d30":"titanic_df['Familial'] = (titanic_df['SibSp'] + titanic_df['Parch']) > 0","cf7074f6":"# Impute \"Fare\" missing value\n# Fare seem to be highly correlated to Pclass & the missing observation's Pclass is 3\n\nmedianFare = titanic_df[titanic_df['Pclass'] == 3]['Fare'].median()\ntitanic_df['Fare'] = titanic_df['Fare'].fillna(value = medianFare)","6602b0ad":"# Categorize continuous variables (Age into 16, i.e., bin width is 80\/16)\n\ncustom_bucket_array = np.linspace(0, 80, 17)\ntitanic_df['CatAge'] = pd.cut(titanic_df['Age'], custom_bucket_array)\nlabels, levels = pd.factorize(titanic_df['CatAge'])\ntitanic_df['CatAge'] = labels\ncustom_bucket_array","e0887ba6":"custom_bucket_array = np.linspace(0, 520, 53)\ntitanic_df['CatFare'] = pd.cut(titanic_df['Fare'], custom_bucket_array)\nlabels, levels = pd.factorize(titanic_df['CatFare'])\ntitanic_df['CatFare'] = labels\ncustom_bucket_array","b547b3ba":"titanic_df['SexBool'] = titanic_df['Sex'].map({'male': 0, 'female': 1})\ntitanic_df['EmbarkedInt'] = titanic_df['Embarked'].map({'S': 0, 'C': 1, 'Q':2})\ntitanic_df['TitleInt'] = titanic_df['Title'].map({'Mr':0, 'Mrs':1, 'Miss':2, 'Master':3, 'Rev':4, 'Dr':5})","946a1f12":"# Get back the features engineered train_df & test_df\n\ntrain_df = titanic_df.loc[titanic_df['PassengerId'] <= train_len]\ntest_df = titanic_df.loc[titanic_df['PassengerId'] > train_len].iloc[:, titanic_df.columns != 'Survived']\n\n(len(train_df), len(test_df))","b8cfe5d4":"# Heatmap to show Pearson Correlation of bivariate permutations\n\nplt.figure(figsize=(14,12))\nfoo = sns.heatmap(train_df.drop(['PassengerId', 'Name', 'Title', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'CatFare', 'Cabin', 'Embarked'],axis=1).corr(), vmax=0.6, square=True, annot=True)","b3c73d71":"fig, axs = plt.subplots(ncols=2, figsize=(15,5))\naxs[0].set_title('female')\nsns.countplot(x='Survived', hue='Pclass', data=titanic_df.loc[titanic_df['Sex'] == 'female'], ax=axs[0])\naxs[1].set_title('male')\nsns.countplot(x='Survived', hue='Pclass', data=titanic_df.loc[titanic_df['Sex'] == 'male'], ax=axs[1])","dd64ea75":"# The Puzzle\nsns.countplot(x='Survived', hue='Sex', data=titanic_df)","8688e856":"fig, axs = plt.subplots(ncols=2, figsize=(15,5))\naxs[0].set_title('female')\nsns.countplot(x='CatAge', hue='Survived', data=train_df.loc[train_df['Sex'] == 'female'], ax=axs[0])\naxs[1].set_title('male')\nsns.countplot(x='CatAge', hue='Survived', data=train_df.loc[train_df['Sex'] == 'male'], ax=axs[1])","735f7964":"fig, axs = plt.subplots(ncols=2, figsize=(15,5))\naxs[0].set_title('female')\nsns.countplot(x='Familial', hue='Survived', data=train_df.loc[train_df['Sex'] == 'female'], ax=axs[0])\naxs[1].set_title('male')\nsns.countplot(x='Familial', hue='Survived', data=train_df.loc[train_df['Sex'] == 'male'], ax=axs[1])","cd168005":"# Select feature column names and target variable we are going to use for training\n# Best score with ['Pclass', 'Fare', 'Sex_binary', 'AgeCategoryIndex', 'Alone']\n\nColumns = ['SexBool', 'Pclass', 'Fare', 'CatAge', 'Familial', 'EmbarkedInt', 'TitleInt']\nLabel = 'Survived'\n\ntrain_X = train_df.loc[:, train_df.columns != 'Survived']\ntrain_y = train_df['Survived']","037a7bc1":"# Instantiate XGB classifier - its hyperparameters are tuned through SkLearn Grid Search below\n\nmodel = XGBClassifier()","9e547431":"# Performing grid search for important hyperparameters of XGBoost\n# It has been observed that non-default value of only n_estimators is useful\n# Other hyerparameters default values are the best (learning_Rate as 0.1, max_depth as 3, alpha L1 regularizer as 0 & lambda L2 regularizer as 1)\n\nboth_scoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score), 'Loss':'neg_log_loss'}\nparams = {\n        'n_estimators': [100, 200, 500, 1000, 1500],\n        'learning_rate': [0.05, 0.1, 0.2]\n        #'max_depth':[3, 4, 5]\n        }","a470b378":"clf = GridSearchCV(model, params, cv=5, scoring=both_scoring, refit='AUC', return_train_score=True)\nclf.fit(train_X[Columns], train_y)","65f5cb89":"print((clf.best_score_, clf.best_params_))\nprint(\"=\"*30)\n\nprint(\"Grid scores on training data:\")\nmeans = clf.cv_results_['mean_test_AUC']\nstds = clf.cv_results_['std_test_AUC']\nlog_losses = clf.cv_results_['std_test_Loss']\n\nfor mean, std, log_loss, params in zip(means, stds, log_losses, clf.cv_results_['params']):\n    print(\"AUC Score: %0.3f (+\/-%0.03f); Log Loss: %0.3f for %r\" % (mean, std * 2, log_loss, params))","9f33b6ea":"# If grid params permutes across multiple hyperparameters, then below plot would have many lines (n1*n2*n3..) & may look cluttered\n# Observe the best AUC & Accuracy\n\nresults = clf.cv_results_\n\nplt.figure(figsize=(13, 13))\nplt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\", fontsize=16)\n\nplt.xlabel(\"n_estimators: no of boosted trees\")\nplt.ylabel(\"AUC Score\")\n\nax = plt.gca()\nax.set_xlim(80, 1020)\nax.set_ylim(0.7, 1)\n\nX_axis = np.array(results['param_n_estimators'].data, dtype=float)\n\nfor scorer, color in zip(sorted(both_scoring), ['g', 'k']):\n    for sample, style in (('train', '--'), ('test', '-')):\n        sample_score_mean = results['mean_%s_%s' % (sample, scorer)]\n        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n                        sample_score_mean + sample_score_std,\n                        alpha=0.1 if sample == 'test' else 0, color=color)\n        ax.plot(X_axis, sample_score_mean, style, color=color,\n                alpha=1 if sample == 'test' else 0.7,\n                label=\"%s (%s)\" % (scorer, sample))\n\n    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n    best_score = results['mean_test_%s' % scorer][best_index]\n\n    # Plot a dotted vertical line at the best score for that scorer marked by x\n    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n\n    # Annotate the best score for that scorer\n    ax.annotate(\"%0.2f\" % best_score,\n                (X_axis[best_index], best_score + 0.005))\n\nplt.legend(loc=\"best\")\nplt.grid('off')\nplt.show()","72fa269f":"#Make predictions using the features (Columns) from test_df\n\npredictions = clf.predict(test_df[Columns]).astype(int)\n\nsubmission = pd.DataFrame({'PassengerId':test_df['PassengerId'], 'Survived':predictions})","fc56c699":"# Fill submission csv file\nfilename = 'submit.csv'\nsubmission.to_csv(filename,index=False)","8ad2a053":"### EDA.x Extreme Fare which could possibly be outlier\n'''\n# 4 passengers with Fare > 512.0 of which 1 are from test_df (passenger id 1235)\n# All on same Ticket 'PC 17755' => hence pid 1235 can be predicted as SURVIVED\n\n# 17 passengers with Fare < 1.0 of which 2 are from test_df (passenger id 1158 on Ticket_112051 & 1264 on Ticket_112058)\n# Both these passengers can be predicted as DIED\n\n# TODO: Manual row append to 'submission' dataframe needs to be fixed\n#titanic_df = titanic_df.loc[(titanic_df['Fare'] > 1.0) & (titanic_df['Fare'] < 512.0)]\n#titanic_df = titanic_df.loc[titanic_df['Fare'] < 512.0]\n\n# manual row append needs to be fixed... If we prune Fare > 512.0 which consists of 4 observations (3 train & 1 test), then below prediction must be manually added\n#submission = submission.append({1235: 1}, ignore_index=True)\n#sideEntryPrediction = [1235, 1]\n#submission.loc[len(submission)] = sideEntryPrediction\n#submission = submission.astype(int)\n#submission.sort_values(by=['PassengerId','Survived'], ascending=True,inplace=True)\n'''","8fea3814":"### EDA.x: Though not useful... Survival trend among passengers on unique Ticket -vs- common\/group Ticket appeared to be quite visible, hence \n# tried to split full dataset into Grouped & Single\n#\n# test\/test_df (97 on group ticket -vs- 321 on single ticket)\n# train_df (344 on group ticket -vs- 547 on single ticket)\n\n# train_df has 344 passengers on Group Ticket\n#Survived  0.0  1.0\n#Sex               \n#female     47  133\n#male      118   46\n\n# trainf_df has 547 passengers with Single Ticket\n#Survived  0.0  1.0\n#Sex               \n#female     34  100\n#male      350   63\n\n'''\ntrainTktCount = train_df.groupby(\"Ticket\")[\"Ticket\"].transform(len)\nmaskGroupTrain = (trainTktCount > 1)\ntrainGrouped_df = train_df[maskGroupTrain]\n(len(trainGrouped_df), len(trainSingle_df), len(holdoutGrouped_df), len(holdoutSingle_df))\n'''","5d64deab":"### EDA.x: Checking if child & aged people were accompanied by relatives or were they vulnerable\n#Below 2 data tables clearly shows that there was NO IMPACT of Vulnerable on Suvived\n'''def is_vulnerable(passenger):\n    Age, SibSp, Parch = passenger\n    if (((Age < 18) or (Age > 60)) and (SibSp+Parch == 0)):\n        return 'vulnerable'\n    else:\n        return 'safe'\n\ntrain_df['Vulnerable'] = train_df[['Age', 'SibSp', 'Parch']].apply(is_vulnerable, axis=1)\n\ntab = pd.crosstab(train_df['Vulnerable'], train_df['AgeCategory'])\ntab.iloc[:,:]'''","f0802131":"### Categorizing continuous variables Age & Fare","a611bbd1":"## Feature Engineering","1b6c7fc7":"### References\n1. [[Approach to impute missing values in Age using Name's Title](http:\/\/https:\/\/www.kaggle.com\/jamesleslie\/titanic-random-forrest-use-title-to-impute-age)](http:\/\/)","4eac7f03":"### Identify Best Model, i.e., Hyperparameters","b21ceb5c":"## Import Py libs","e10bf393":"# Decision Classification using XGB and Hyperparameters Tuning through Grid Search","d81fb5fc":"## EDA & Features Importance Indentification, i.e., Features Selection\n\nEnsemble Learners like XGBoost can handle mutually correlated\/redundant features but at the cost of so much extra iterations (n_estimators & other hyperparameters), so one can afford to ignore features selection or dimentionality reduction and recover through larger grid search space.\n","4664c470":"### Categorizing string variables Sex, Ticket & Embarked","874214c4":"### EDA.x: Checking the survial distribution of male & female across Age bins\nNOTE that Age data availability is sparse (20% missing data), but available Age data shows a clear trend that male aged 15+ did not survive well. In fact, available Age data shows that majority of passengers were between 15-50.","e841de9a":"### Features Selection for Model Learning","b1e15c79":"Variable Name | Description\n------------------|-------------\nPassengerId | Passenger Id (unique)\nSurvived | Survived (1) or died (0)\nPclass | Passenger's class (1\/Upper, 2\/Middle, 3\/Lower)\nName | Passenger's Name (common Surname may be possible)\nSex | Passenger's sex (just 'male' & 'female')\nAge (20% missing values) | Passenger's age\nSibSp | Number of siblings\/spouses aboard\nParch | Number of parents\/children aboard\nTicket | Ticket number (many of the ticket seem to be common for a group of passengers, i.e., they are group ticket)\nFare (1 missing in 'test') | Fare\nCabin (77% missing in both 'train'& 'test') | Cabin\nEmbarked (2 missing values in 'train') | Port of embarkation","fc4d979e":"### EDA.x: Sex -vs- Survived\nGood correlation is observed between Sex & Survived. Obvious observation is that more females survived than males, but the root cause seems to Age + SibSp&Parch (which will become clear below)","a327a819":"### Imputing missing values in few variables\n\n Note that exact same imputation must be performed on both train_df & test_df using overall clues from full titanic_df  \n Variables suffering from missing observations are Age, Fare, Cabin & Embarked","3d1caef4":"## Appendix","543c28b1":"### Defining new feature Familial","c6501c00":"### Prepare Submission CSV File","15440c43":"### EDA.x: Pclass -vs- Survived\nPclass has the best correlation with Survived! The trend of survial across Pclass is different for male -vs- female Sex.\n=> Female survived even in lower Pclass(es)\n=> Male did not survive even in upper Pclass","2634ef13":"### Imputing Fare","27dc757a":"### Imputing Embarked","db8f91bf":"## Load dataset","55c3e0f2":"### Imputing Age","b7e4e207":"### EDA.x: Check the distribution of Survived across Familial\nThis data table shows that 60% were alone, i.e., without relatives and majority of them did not survive... 76% were male among all lone passengers\n\nSummary: Familial can be treated as boolean","4a0c595a":"## Titanic Passengers Survival with XGBoost & Tuning through Hyperparameters Grid Search\n\n### Suni Kumar","a1ecda1c":"### Done with Feature Engineering => Now extract train_df & test\/test_df","652ee125":"### Label & Features Correlation\n* Label -vs- features strongly correlated (closer to 1.0) or anti-correlated (closer to -1.0) are useful  \n* Feature_i -vs- Feature_j correlation reveals redundancy  \n\n* Notice that Age has so poor correlation with Survived!  \n* Embarked is neihter related to Survived nor to any feature!\n\n* Familial is relatively poorly correlated with Survived, but its cross correlation with Fare & Ticket is extremely strong, hence it is indirectly represented in Model by Fare and\/or Ticket... Ticket is further strongly correlated with Fare too\n\n* The most intuitive condition (Label-vs-features > 0.25) narrows down the features set to 'Pclass', 'Fare', and 'Sex_binary  \n* Observe that Age was originally sparse & its correlation with Label is very weak, but its cross correlation with Familial & Ticket  "}}