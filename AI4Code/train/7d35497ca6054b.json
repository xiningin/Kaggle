{"cell_type":{"27ed3619":"code","50c9a8dd":"code","d944cee2":"code","bcce18bf":"code","217a32e7":"code","7ed62e40":"code","b298ce12":"code","4dfb5ad7":"code","dfb1b5c0":"code","28dfef45":"code","a9dcbae4":"code","4145fc76":"code","1eec0b5c":"code","71c0c9f6":"code","d2ff1d27":"code","7df6c2a3":"code","c5de8e77":"code","1f201faf":"code","4602cc0d":"code","3c4cb760":"code","fe141865":"code","80580dea":"code","1fed45ae":"code","c9d2c3e6":"code","d9dcc2ee":"code","9ce79c58":"code","c6b55fdc":"code","74ea18b4":"code","bbef7fe2":"code","c2f6bc1f":"code","45a38285":"code","ee82a868":"code","d7e1c1e1":"code","fb09efc3":"code","4b1d38aa":"code","f243adfc":"code","48e069bf":"code","14835d83":"code","be94d171":"code","cf5eb056":"code","b21da88c":"code","6dfef7f9":"code","8ceedb06":"code","6b012472":"code","6fd446bd":"code","49564697":"code","88275821":"code","7013c617":"code","307b380a":"code","7ac593f4":"code","bbe97d46":"code","a3ea52eb":"code","beb524b8":"code","a1578741":"code","68117671":"code","87eecc26":"code","bf60054b":"code","87a3620e":"code","ffa9a498":"code","40150bf9":"code","1f0c34bb":"code","bfc04d46":"code","9dda59a2":"code","58ea71e9":"code","f0eab62f":"code","425dce72":"code","ae51a199":"code","5ec89cac":"markdown","c8c0927e":"markdown","a82d60a5":"markdown","1c2a7200":"markdown","97e45483":"markdown","cd336a32":"markdown","d07e9328":"markdown","a29de3f8":"markdown","e9a9ce0b":"markdown","cad7b2a2":"markdown","7b501c93":"markdown","fca2285c":"markdown","530d3073":"markdown","a65c0a55":"markdown","49aa3564":"markdown","861a6849":"markdown","af2e7848":"markdown","797f454b":"markdown","bc91768d":"markdown","e45f7848":"markdown","1a5f99a9":"markdown","de9d8b28":"markdown","3c2856cd":"markdown","42623894":"markdown","6d6e981a":"markdown","81ace771":"markdown","f3121880":"markdown","df08d0e9":"markdown","8315a26e":"markdown","b544b111":"markdown"},"source":{"27ed3619":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('whitegrid')\nplt.style.use('seaborn-deep')\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.serif'] = 'Ubuntu'\nplt.rcParams['font.monospace'] = 'Ubuntu Mono'\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.labelsize'] = 12\nplt.rcParams['axes.titlesize'] = 12\nplt.rcParams['xtick.labelsize'] = 8\nplt.rcParams['ytick.labelsize'] = 8\nplt.rcParams['legend.fontsize'] = 12\nplt.rcParams['figure.titlesize'] = 14\nplt.rcParams['figure.figsize'] = (12, 8)\n\npd.options.mode.chained_assignment = None\npd.options.display.float_format = '{:.2f}'.format\npd.set_option('display.max_columns', 200)\npd.set_option('display.width', 400)\nimport warnings\nwarnings.filterwarnings('ignore')\nimport sklearn.metrics as skm\nimport sklearn.model_selection as skms\nimport sklearn.preprocessing as skp\nimport random\nseed = 12\nnp.random.seed(seed)\n\nfrom datetime import date","50c9a8dd":"# important funtions\ndef datasetShape(df):\n    rows, cols = df.shape\n    print(\"The dataframe has\",rows,\"rows and\",cols,\"columns.\")\n    \n# select numerical and categorical features\ndef divideFeatures(df):\n    numerical_features = housing.select_dtypes(include=[np.number]).drop('SalePrice', axis=1)\n    categorical_features = housing.select_dtypes(include=[np.object])\n    return numerical_features, categorical_features","d944cee2":"data_file = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\"\ndata_file_test = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\"\ndata_file_submission = \"\/kaggle\/working\/submission.csv\"\nhousing = pd.read_csv(data_file)\nhousing_test = pd.read_csv(data_file_test)\nhousing.head()","bcce18bf":"# check dataset shape\ndatasetShape(housing)\n\n# check for duplicates\nif(len(housing) == len(housing.Id.unique())):\n    print(\"No duplicates found!!\")\nelse:\n    print(\"Duplicates occuring\")","217a32e7":"numerical_features, categorical_features = divideFeatures(housing)\nhousing.head()","7ed62e40":"# boxplots of numerical features for outlier detection\n\nfig = plt.figure(figsize=(16,30))\nfor i in range(len(numerical_features.columns)):\n    fig.add_subplot(9, 5, i+1)\n    sns.boxplot(y=numerical_features.iloc[:,i])\nplt.tight_layout()\nplt.show()","b298ce12":"# distplots for categorical data\n\nfig = plt.figure(figsize=(16,30))\nfor i in range(len(categorical_features.columns)):\n    fig.add_subplot(9, 5, i+1)\n    categorical_features.iloc[:,i].hist()\n    plt.xlabel(categorical_features.columns[i])\nplt.tight_layout()\nplt.show()","4dfb5ad7":"# plotting numerical features for bar plot patterns\n\ndiscrete_features=[feature for feature in list(numerical_features.columns) if numerical_features[feature].unique().shape[0]<25 and feature not in ['Id', 'YearBuilt', 'YearRemodAdd', 'GarageYrBlt']]\nfig = plt.figure(figsize=(16,30))\nfor i,feature in enumerate(discrete_features):\n    fig.add_subplot(6, 3, i+1)\n    data=housing.copy()\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\nplt.tight_layout()\nplt.show()","dfb1b5c0":"# plot missing values\n\ndef calc_missing(df):\n    missing = df.isna().sum().sort_values(ascending=False)\n    missing = missing[missing != 0]\n    missing_perc = missing\/df.shape[0]*100\n    return missing, missing_perc\n\nmissing, missing_perc = calc_missing(housing)\nmissing.plot(kind='bar',figsize=(16,6))\nplt.title('Missing Values')\nplt.show()","28dfef45":"import matplotlib.gridspec as gridspec\nfig = plt.figure(constrained_layout=True, figsize=(16,6))\ngrid = gridspec.GridSpec(ncols=3, nrows=1, figure=fig)\nax1 = fig.add_subplot(grid[0, :2])\nax1.set_title('Histogram')\nsns.distplot(housing.loc[:,'SalePrice'], norm_hist=True, ax = ax1)\nax3 = fig.add_subplot(grid[:, 2])\nax3.set_title('Box Plot')\nsns.boxplot(housing.loc[:,'SalePrice'], orient='v', ax = ax3)\nplt.show()","a9dcbae4":"# scatterplot for correlation analysis of features with SalePrice\n\nfig = plt.figure(figsize=(16,30))\nfor i in range(len(numerical_features.columns)):\n    fig.add_subplot(8, 5, i+1)\n    sns.regplot(x=housing['SalePrice'],y=numerical_features.iloc[:,i])\nplt.tight_layout()\nplt.show()","4145fc76":"# boxplot for distribution analysis of categorical features with SalePrice\n\nfig = plt.figure(figsize=(20,50))\nfor i in range(len(categorical_features.columns)):\n    fig.add_subplot(11, 4, i+1)\n    sns.boxplot(y=housing['SalePrice'],x=categorical_features.iloc[:,i])\nplt.tight_layout()\nplt.show()","1eec0b5c":"# correlation heatmap for all features\n\nplt.figure(figsize = (30,20))\nmask = np.zeros_like(housing.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(housing.corr(), cmap=sns.diverging_palette(20, 220, n=200), mask = mask, annot=True, center = 0)\nplt.show()","71c0c9f6":"# drop the Id\nhousing.drop('Id', axis=1, inplace=True)\n\n# drop the duplicate rows\nhousing.drop_duplicates(inplace=True)\ndatasetShape(housing)","d2ff1d27":"# remove all columns having no values\nhousing.dropna(axis=1, how=\"all\", inplace=True)\nhousing.dropna(axis=0, how=\"all\", inplace=True)\ndatasetShape(housing)","7df6c2a3":"# remove columns having null values more than 30%\nhousing.dropna(thresh=housing.shape[0]*0.7,how='all',axis=1, inplace=True)\ndatasetShape(housing)","c5de8e77":"print(\"Showing all features:\")\nprint(list(housing.columns))","1f201faf":"featuresToDrop = ['MiscVal', 'Exterior2nd']\nhousing.drop(featuresToDrop, axis=1, inplace=True)\ndatasetShape(housing)","4602cc0d":"# missing values with percentage\n\nmissing, missing_perc = calc_missing(housing)\npd.concat([missing, missing_perc], axis=1, keys=['Total','Percent'])","3c4cb760":"housing[missing.index].describe()","fe141865":"housing.loc[housing.LotFrontage.isna(), 'LotFrontage'] = housing.LotFrontage.median()\nprint(\"Missing values in LotFrontage:\",housing.LotFrontage.isna().sum())","80580dea":"housing_test.loc[housing_test.LotFrontage.isna(), 'LotFrontage'] = housing_test.LotFrontage.median()\nprint(\"Missing values in LotFrontage:\",housing_test.LotFrontage.isna().sum())","1fed45ae":"housing.loc[housing.GarageType.isnull(), 'GarageType'] = 'NoGarage'\nhousing.loc[housing.GarageFinish.isnull(), 'GarageFinish'] = 'NoGarage'\nhousing.loc[housing.GarageQual.isnull(), 'GarageQual'] = 'NoGarage'\nhousing.loc[housing.GarageCond.isnull(), 'GarageCond'] = 'NoGarage'","c9d2c3e6":"housing_test.loc[housing_test.GarageType.isnull(), 'GarageType'] = 'NoGarage'\nhousing_test.loc[housing_test.GarageFinish.isnull(), 'GarageFinish'] = 'NoGarage'\nhousing_test.loc[housing_test.GarageQual.isnull(), 'GarageQual'] = 'NoGarage'\nhousing_test.loc[housing_test.GarageCond.isnull(), 'GarageCond'] = 'NoGarage'","d9dcc2ee":"housing.loc[housing.GarageYrBlt.isna(), 'GarageYrBlt'] = housing.GarageYrBlt.median()\nprint(\"Missing values in GarageYrBlt:\",housing.GarageYrBlt.isna().sum())","9ce79c58":"housing_test.loc[housing_test.GarageYrBlt.isna(), 'GarageYrBlt'] = housing_test.GarageYrBlt.median()\nprint(\"Missing values in GarageYrBlt:\",housing_test.GarageYrBlt.isna().sum())","c6b55fdc":"housing.loc[housing.MasVnrType.isnull(), 'MasVnrType'] = 'None'\nhousing.loc[housing.MasVnrType == 'None', 'MasVnrArea'] = 0\nprint(\"Missing values in MasVnrType:\",housing.MasVnrType.isna().sum())\nprint(\"Missing values in MasVnrArea:\",housing.MasVnrArea.isna().sum())","74ea18b4":"housing_test.loc[housing_test.MasVnrType.isnull(), 'MasVnrType'] = 'None'\nhousing_test.loc[housing_test.MasVnrType == 'None', 'MasVnrArea'] = 0\nprint(\"Missing values in MasVnrType:\",housing_test.MasVnrType.isna().sum())\nprint(\"Missing values in MasVnrArea:\",housing_test.MasVnrArea.isna().sum())","bbef7fe2":"housing.loc[housing.BsmtQual.isnull(), 'BsmtQual'] = 'NoBsmt'\nhousing.loc[housing.BsmtCond.isnull(), 'BsmtCond'] = 'NoBsmt'\nhousing.loc[housing.BsmtExposure.isnull(), 'BsmtExposure'] = 'NoBsmt'\nhousing.loc[housing.BsmtFinType1.isnull(), 'BsmtFinType1'] = 'NoBsmt'\nhousing.loc[housing.BsmtFinType2.isnull(), 'BsmtFinType2'] = 'NoBsmt'","c2f6bc1f":"housing_test.loc[housing_test.BsmtQual.isnull(), 'BsmtQual'] = 'NoBsmt'\nhousing_test.loc[housing_test.BsmtCond.isnull(), 'BsmtCond'] = 'NoBsmt'\nhousing_test.loc[housing_test.BsmtExposure.isnull(), 'BsmtExposure'] = 'NoBsmt'\nhousing_test.loc[housing_test.BsmtFinType1.isnull(), 'BsmtFinType1'] = 'NoBsmt'\nhousing_test.loc[housing_test.BsmtFinType2.isnull(), 'BsmtFinType2'] = 'NoBsmt'","45a38285":"housing.loc[housing['Electrical'].isnull(), 'Electrical'] = 'SBrkr'\nmissing, missing_perc = calc_missing(housing)\nprint(\"Any Missing Values?\",missing.values)","ee82a868":"housing_test.loc[housing_test['Electrical'].isnull(), 'Electrical'] = 'SBrkr'","d7e1c1e1":"# plot sample skewed feature\nplt.figure(figsize=(10,4))\nsns.distplot(housing.LotFrontage)\nplt.show()","fb09efc3":"# extract all skewed features\ntemp_numerical_features, temp_categorical_features = divideFeatures(housing)\n# remove categorical features stored as int\ntemp_numerical_features.drop(['OverallCond', 'OverallQual'], axis=1, inplace=True)\nskewed_features = temp_numerical_features.apply(lambda x: x.skew()).sort_values(ascending=False)","4b1d38aa":"# transform skewed features\nfor feat in skewed_features.index:\n    if skewed_features.loc[feat] > 0.5:\n        housing[feat] = np.log1p(housing[feat])\n        housing_test[feat] = np.log1p(housing_test[feat])","f243adfc":"# plot sample treated feature\nplt.figure(figsize=(10,4))\nsns.distplot(housing.LotFrontage)\nplt.show()","48e069bf":"# outlier treatment for categorical features\ndef getCategoricalSkewed(categories, threshold):\n    tempSkewedFeatures = []\n    for feat in categories:\n        for featValuePerc in list(housing[feat].value_counts()\/housing.shape[0]):\n            if featValuePerc > threshold:\n                tempSkewedFeatures.append(feat)\n    return list(set(tempSkewedFeatures))\n\n# display all categorical skewed features which have value_counts > 90%\ncategoricalSkewed = getCategoricalSkewed(temp_categorical_features.columns, .90)\nfor feat in categoricalSkewed:\n    print(housing[feat].value_counts()\/len(housing))\n    print()","14835d83":"print(\"Before Removing:\")\ndatasetShape(housing)\n\n# removing skewed categorical data \nhousing.drop(categoricalSkewed, axis=1, inplace=True)\nprint(\"After Removing:\")\ndatasetShape(housing)","be94d171":"numerical_features, categorical_features = divideFeatures(housing)\ncategorical_features.columns","cf5eb056":"# housing overallqual in four bins\nhousing['OverallQual'].replace([1,2,3,4,5,6,7,8,9,10], ['Poor', 'Poor', 'Fair', 'Average', 'Average', 'Good', 'Good', 'Excellent', 'Excellent', 'Excellent'], inplace=True)\nhousing['OverallCond'].replace([1,2,3,4,5,6,7,8,9,10], ['Poor', 'Poor', 'Fair', 'Average', 'Average', 'Good', 'Good', 'Excellent', 'Excellent', 'Excellent'], inplace=True)","b21da88c":"housing_test['OverallQual'].replace([1,2,3,4,5,6,7,8,9,10], ['Poor', 'Poor', 'Fair', 'Average', 'Average', 'Good', 'Good', 'Excellent', 'Excellent', 'Excellent'], inplace=True)\nhousing_test['OverallCond'].replace([1,2,3,4,5,6,7,8,9,10], ['Poor', 'Poor', 'Fair', 'Average', 'Average', 'Good', 'Good', 'Excellent', 'Excellent', 'Excellent'], inplace=True)","6dfef7f9":"# feture engineering a new feature \"TotalFS\"\nhousing['TotalSF'] = (housing['TotalBsmtSF'] + housing['1stFlrSF'] + housing['2ndFlrSF'])\nhousing['Total_sqr_SF'] = (housing['BsmtFinSF1'] + housing['BsmtFinSF2'] + housing['1stFlrSF'] + housing['2ndFlrSF'])\nhousing['Total_Bathrooms'] = (housing['FullBath'] + (0.5 * housing['HalfBath']) + housing['BsmtFullBath'] + (0.5 * housing['BsmtHalfBath']))\nhousing['Total_porch_SF'] = (housing['OpenPorchSF'] + housing['3SsnPorch'] + housing['EnclosedPorch'] + housing['ScreenPorch'])","8ceedb06":"# feture engineering a new feature \"TotalFS\"\nhousing_test['TotalSF'] = (housing_test['TotalBsmtSF'] + housing_test['1stFlrSF'] + housing_test['2ndFlrSF'])\nhousing_test['Total_sqr_SF'] = (housing_test['BsmtFinSF1'] + housing_test['BsmtFinSF2'] + housing_test['1stFlrSF'] + housing_test['2ndFlrSF'])\nhousing_test['Total_Bathrooms'] = (housing_test['FullBath'] + (0.5 * housing_test['HalfBath']) + housing_test['BsmtFullBath'] + (0.5 * housing_test['BsmtHalfBath']))\nhousing_test['Total_porch_SF'] = (housing_test['OpenPorchSF'] + housing_test['3SsnPorch'] + housing_test['EnclosedPorch'] + housing_test['ScreenPorch'])","6b012472":"# extract number of years till date from year features\nyear_features = ['GarageYrBlt', 'YearBuilt', 'YearRemodAdd','YrSold']\nfor year in year_features:\n    housing[year] = int(date.today().year)-housing[year]\n    housing_test[year] = int(date.today().year)-housing_test[year]\ndatasetShape(housing)","6fd446bd":"# extract numerical and categorical for dummy and scaling later\nnumerical_features, categorical_features = divideFeatures(housing)\nfor feat in categorical_features.columns:\n    dummyVars = pd.get_dummies(housing[feat], drop_first=True, prefix=feat+\"_\")\n    housing = pd.concat([housing, dummyVars], axis=1)\n    housing.drop(feat, axis=1, inplace=True)\n\n    dummyVars_test = pd.get_dummies(housing_test[feat], drop_first=True, prefix=feat+\"_\")\n    housing_test = pd.concat([housing_test, dummyVars_test], axis=1)\n    housing_test.drop(feat, axis=1, inplace=True)\n\ndatasetShape(housing)","49564697":"# shuffle samples\ndf_shuffle = housing.sample(frac=1, random_state=seed).reset_index(drop=True)","88275821":"df_y = df_shuffle.pop('SalePrice')\ndf_X = df_shuffle\n\n# split into train dev and test\nX_train, X_test, y_train, y_test = skms.train_test_split(df_X, df_y, train_size=0.7, random_state=seed)\nprint(f\"Train set has {X_train.shape[0]} records out of {len(df_shuffle)} which is {round(X_train.shape[0]\/len(df_shuffle)*100)}%\")\nprint(f\"Test set has {X_test.shape[0]} records out of {len(df_shuffle)} which is {round(X_test.shape[0]\/len(df_shuffle)*100)}%\")","7013c617":"from sklearn.feature_selection import RFE\nimport sklearn.linear_model as sklm","307b380a":"scaler = skp.StandardScaler()\n\n# apply scaling to all numerical variables except dummy variables as they are already between 0 and 1\nX_train[numerical_features.columns] = scaler.fit_transform(X_train[numerical_features.columns])\n\n# scale test data with transform()\nX_test[numerical_features.columns] = scaler.transform(X_test[numerical_features.columns])\n\nhousing_test[numerical_features.columns] = scaler.transform(housing_test[numerical_features.columns])\n\n\n# view sample data\nX_train.describe()","7ac593f4":"# Running RFE to extract top 50 features\nlm = sklm.LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 50)\nrfe = rfe.fit(X_train, y_train)\nrfeCols = X_train.columns[rfe.support_]\nX_train_rfe = X_train[rfeCols]\nX_test_rfe = X_test[rfeCols]\nprint(\"Selected features by RFE are\",list(rfeCols))","bbe97d46":"# plotting mean test and train scoes with alpha \nimport operator\ndef plotCvResults(model_cv):\n    cv_results = pd.DataFrame(model_cv.cv_results_)\n    cv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\n    plt.plot(np.log1p(cv_results['param_alpha']), cv_results['mean_train_score'])\n    plt.plot(np.log1p(cv_results['param_alpha']), cv_results['mean_test_score'])\n    plt.xlabel('log1p(alpha)')\n    plt.ylabel('Negative Mean Absolute Error')\n    plt.title(\"Negative Mean Absolute Error and log1p(alpha)\")\n    plt.legend(['train score', 'test score'], loc='upper left')\n    plt.show()\n\n# display parameters\ndef bestParams(model):\n    print(\"Best Alpha for Regularized Regression:\",model.get_params()['alpha'])\n    model_parameters = [abs(x) for x in list(model.coef_)]\n    model_parameters.insert(0, model.intercept_)\n    model_parameters = [round(x, 3) for x in model_parameters]\n    cols = X_train_rfe.columns\n    cols = cols.insert(0, \"constant\")\n    model_coef = sorted(list(zip(cols, model_parameters)), key=operator.itemgetter(1), reverse=True)[:11]\n    print(\"Top 10 Model parameters (excluding constant) are:\")\n    for p,c in model_coef:\n        print(p)\n        \ndef modelR2AndSpread(model):\n    y_train_pred = model.predict(X_train_rfe)\n    print(\"Train r2:\",skm.r2_score(y_true=y_train, y_pred=y_train_pred))\n    y_test_pred = model.predict(X_test_rfe)\n    print(\"Test r2:\",skm.r2_score(y_true=y_test, y_pred=y_test_pred))\n    print('Root Mean Square Error train: ' + str(np.sqrt(skm.mean_squared_error(y_train, y_train_pred))))\n    print('Root Mean Square Error test: ' + str(np.sqrt(skm.mean_squared_error(y_test, y_test_pred)))) \n\n    fig = plt.figure(figsize=(16,10))\n    plt.suptitle(\"Linear Regression Assumptions\", fontsize = 16)\n\n    # plot error spread\n    fig.add_subplot(2, 2, 1)\n    sns.regplot(y_train, y_train_pred)\n    plt.title('y_train vs y_train_pred spread', fontsize = 14)\n    plt.xlabel('y_train', fontsize = 12)\n    plt.ylabel('y_train_pred', fontsize = 12)      \n\n    fig.add_subplot(2, 2, 2)\n    sns.regplot(y_test, y_test_pred)\n    plt.title('y_test vs y_test_pred spread', fontsize = 14)\n    plt.xlabel('y_test', fontsize = 12)\n    plt.ylabel('y_test_pred', fontsize = 12)      \n\n    # plot residuals for linear regression assumption\n    residuals_train = y_train - y_train_pred\n    fig = plt.figure(figsize=(16,6))\n    fig.add_subplot(2, 2, 3)\n    sns.distplot(residuals_train)\n    plt.title('residuals between y_train & y_train_pred', fontsize = 14)\n    plt.xlabel('residuals', fontsize = 12)\n\n    fig.add_subplot(2, 2, 4)\n    residuals_test = y_test - y_test_pred\n    sns.distplot(residuals_train)\n    plt.title('residuals between y_test & y_test_pred', fontsize = 14)\n    plt.xlabel('residuals', fontsize = 12)\n    plt.show()","a3ea52eb":"lmr = sklm.Ridge(alpha=0.001)\nlmr.fit(X_train_rfe, y_train)\n\n# predict\ny_train_pred = lmr.predict(X_train_rfe)\nprint(\"Train r2:\",skm.r2_score(y_true=y_train, y_pred=y_train_pred))\ny_test_pred = lmr.predict(X_test_rfe)\nprint(\"Test r2:\",skm.r2_score(y_true=y_test, y_pred=y_test_pred))","beb524b8":"# list of alphas to tune\nparams = {'alpha': [0.0001, 0.001, 0.005, 0.01, 0.03, 0.05, 0.1, 0.5, 1.0, 5.0, 10]}\nridge = sklm.Ridge()\n\n# cross validation\nmodel_cv_ridge = skms.GridSearchCV(estimator = ridge, n_jobs=-1, param_grid = params, \n                             scoring= 'neg_mean_squared_error', cv = 5, \n                             return_train_score=True, verbose = 3)            \nmodel_cv_ridge.fit(X_train_rfe, y_train)\nplotCvResults(model_cv_ridge)","a1578741":"# verify log1p value for best selected alpha by GridSearch\nprint(model_cv_ridge.best_params_['alpha'])\nprint(np.log1p(model_cv_ridge.best_params_['alpha']))","68117671":"lml = sklm.Lasso(alpha=0.01)\nlml.fit(X_train_rfe, y_train)\n\n# predict\ny_train_pred = lml.predict(X_train_rfe)\nprint(\"Train r2:\",skm.r2_score(y_true=y_train, y_pred=y_train_pred))\ny_test_pred = lml.predict(X_test_rfe)\nprint(\"Test r2:\",skm.r2_score(y_true=y_test, y_pred=y_test_pred))","87eecc26":"# list of alphas to tune\nparams = {'alpha': [0.00005, 0.0001, 0.0005, 0.0008, 0.001, 0.005, 0.01, 0.05, 0.1]}\nlasso = sklm.Lasso()\n\n# cross validation\nmodel_cv_lasso = skms.GridSearchCV(estimator = lasso, n_jobs=-1, param_grid = params, \n                             scoring= 'neg_mean_squared_error', cv = 5, \n                             return_train_score=True, verbose = 3)            \nmodel_cv_lasso.fit(X_train_rfe, y_train)\nplotCvResults(model_cv_lasso)","bf60054b":"# verify log1p value for best selected alpha by GridSearch\nprint(model_cv_lasso.best_params_['alpha'])\nprint(np.log1p(model_cv_lasso.best_params_['alpha']))","87a3620e":"alpha = model_cv_ridge.best_params_['alpha']\nridge_final = sklm.Ridge(alpha=alpha)\n\nridge_final.fit(X_train_rfe, y_train)\nbestParams(ridge_final)","ffa9a498":"# r2 score for selected model\nmodelR2AndSpread(ridge_final)","40150bf9":"alpha = model_cv_lasso.best_params_['alpha']\nlasso_final = sklm.Lasso(alpha=alpha)\n\nlasso_final.fit(X_train_rfe, y_train)\nbestParams(lasso_final)","1f0c34bb":"# r2 score for selected model\nmodelR2AndSpread(lasso_final)","bfc04d46":"# setting unavailable column\nhousing_test[\"GarageQual__Fa\"] = [0]*housing_test.shape[0]\nhousing_test[\"Exterior1st__ImStucc\"] = [0]*housing_test.shape[0]\nfinal_test = housing_test[X_train_rfe.columns]","9dda59a2":"missing_final_test, missing_perc_final_test = calc_missing(final_test)\npd.concat([missing_final_test, missing_perc_final_test], axis=1, keys=['Total','Percent'])","58ea71e9":"# final_test.loc[final_test[\"BsmtFullBath\"].isna(), \"BsmtFullBath\"] = final_test[\"BsmtFullBath\"].mean()\n# final_test.loc[final_test[\"Total_Bathrooms\"].isna(), \"Total_Bathrooms\"] = final_test[\"Total_Bathrooms\"].mean()\n# final_test.loc[final_test[\"BsmtFinSF1\"].isna(), \"BsmtFinSF1\"] = final_test[\"BsmtFinSF1\"].mean()\nfinal_test.loc[final_test[\"TotalBsmtSF\"].isna(), \"TotalBsmtSF\"] = final_test[\"TotalBsmtSF\"].mean()\n# final_test.loc[final_test[\"TotalSF\"].isna(), \"TotalSF\"] = final_test[\"TotalSF\"].mean()\n# final_test.loc[final_test[\"Total_sqr_SF\"].isna(), \"Total_sqr_SF\"] = final_test[\"Total_sqr_SF\"].mean()","f0eab62f":"# find predictions for test data\nfinal_test_pred = ridge_final.predict(final_test)\nfinal_test_pred[:10]","425dce72":"# make submission df and save\nsubmission_df = pd.DataFrame({\"Id\":housing_test[\"Id\"], \"SalePrice\": final_test_pred})\nsubmission_df.head()","ae51a199":"submission_df.to_csv(data_file_submission, index=False)","5ec89cac":"## Submission Data\n\nGenerating submission file after predicting for test data using ridge_final.predict().","c8c0927e":"# Step 5: Data Modelling\n\n### Split Train-Test Data","a82d60a5":"Correlation will be used for feature selection.\n\n# Step 3: Data Cleaning","1c2a7200":"### MultiVariate Analysis","97e45483":"Imputing MasVnrType, MasVnrArea with None=>0 value as half most of the values are None type.","cd336a32":"### Create Dummy Features","d07e9328":"# Conclusions\n\n=================================================================\n\n**Alpha for Ridge Regression: 1.0**\n\nTop 10 Ridge Model parameters (excluding constant) are:\n- KitchenQual__Fa\n- Exterior1st__BrkComm\n- MSZoning__FV\n- KitchenQual__TA\n- MSZoning__RL\n- OverallCond__Fair\n- GrLivArea\n- Neighborhood__Crawfor\n- Neighborhood__NoRidge\n- SaleCondition__AdjLand\n\n=================================================================\n\n**Alpha for Lasso Regression: 0.0005**\n\nTop 10 Lasso Model parameters (excluding constant) are:\n- Exterior1st__BrkComm\n- MSZoning__FV\n- KitchenQual__Fa\n- MSZoning__RL\n- BsmtQual__NoBsmt\n- Foundation__Stone\n- MSZoning__RH\n- BsmtCond__Po\n- KitchenQual__TA\n- GrLivArea\n\n=================================================================\n\n\n","a29de3f8":"In the above plot, we can see that after or before 0.0, the negative mean squared error is 0 and overlapping for both train and test. <br>\n`Hence, alpha for Lasso Regression is selected as 0.0005`\n\n# Step 6: Model Evaluation\n\n### Ridge Model Evaluation","e9a9ce0b":"### Missing Value Imputation","cad7b2a2":"Above features with missing values will be taken care in Data Cleaning step.\n\n**Target variable analysis for Linear Regression Assumption**","7b501c93":"We will remove all above cateogorical features which are highly skewed.","fca2285c":"### Feature Scaling","530d3073":"Imputing Electrical with highest mode value of the distribution.","a65c0a55":"All Missing values are treated.\n\n# Step 4: Data Preparation\n\n### Outlier Treatment\n\nTreating with the SalePrice target feature and other numerical features, which are skewed. We will take log of the feature values using np.log1p()","49aa3564":"### Lasso Model Evaluation","861a6849":"Date Features:","af2e7848":"# Step 2: EDA","797f454b":"### Derive New Features\n\nNew Features:","bc91768d":"### RFE Feature Selection","e45f7848":"In the above plot, we can see that after 0.7, the negative mean squared error started decreasing. <br>\n`Hence, alpha for Ridge Regression is selected as 1.0`\n\n### Lasso Regression","1a5f99a9":"- Our target variable, SalePrice is not normally distributed.\n- Our target variable is right-skewed.\n- There are multiple outliers in the variable.\n\nOutlier analysis will be done for SalePrice feature to make it as normally distributed.\n\n### Bivariate Analysis","de9d8b28":"Imputing GarageType, GarageFinish, GarageQual, GarageCond with NoGarage value as these values are NA.","3c2856cd":"Imputing BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2 with NoBsmt value as these values are NA.","42623894":"### Binning Features","6d6e981a":"Imputing LotFrontage with median value as its on the average of the distribution and also relevant.","81ace771":"# House Price Prediction using Advanced Linear Regression\n\nWe will be performing following tasks:\n- Feature Visualisation.\n- Which variables are significant in predicting the price of a house, and\n- How well those variables describe the price of a house.\n- The optimal value of lambda for ridge and lasso regression.\n\n# Step 1: Reading and Understanding the Data","f3121880":"Imputing GarageYrBlt with median value as its on the average of the distribution and also relevant.","df08d0e9":"### Ridge Regression","8315a26e":"### Drop Irrelevant Features","b544b111":"## Ridge & Lasso Regression\n\n#### Important Functions"}}