{"cell_type":{"c8309648":"code","43e2f49a":"code","e1ee6ea3":"code","b824ecce":"code","5c2f780b":"code","64029a65":"code","2b598311":"code","b62a409d":"code","7e4c6b93":"code","8219778a":"code","6e07a0cd":"code","65d21322":"code","254c4e82":"code","a291b900":"code","e0755736":"code","848e9a23":"code","35670291":"code","0a04297d":"code","03b56495":"code","9c0459da":"code","08283a4c":"code","c2de0d33":"code","c08a7883":"code","94e5c232":"code","cc902d36":"code","092ca94b":"code","8dadef64":"code","b33f5891":"code","924f3e20":"code","2d5f7fa7":"code","42fc2be4":"code","6a838ce5":"code","91f8f8f1":"code","854f7b69":"code","3e75931d":"code","97a93b5f":"code","a3c59f53":"code","8f6117f4":"code","5264b049":"code","e25a85d8":"code","c85c3676":"code","c7a595b8":"code","6543bce4":"code","27facf78":"code","8dc0b87d":"code","d5afbbbb":"code","9545ed86":"code","db5c2e68":"code","545680b1":"code","bf33ecc5":"code","c014e6ba":"code","89db47f9":"code","17620a28":"code","53807f12":"code","7c0b16d7":"code","41c7f5e6":"code","1d87fedd":"code","72817805":"code","cd702a09":"code","442a3290":"code","13c2bc87":"code","4d642d12":"code","655ed5f7":"code","25063706":"code","e00a5a1a":"code","22ce6a63":"code","fec1f830":"code","217903d8":"code","2f92242f":"code","b601b5c6":"code","23c6bef9":"code","08518b36":"code","35a80275":"code","0443b5d0":"code","e217587b":"code","d6a51301":"code","750d9882":"code","5f902131":"markdown","cc64ba9e":"markdown","e517d083":"markdown","936c7d1e":"markdown","3038eb4f":"markdown","90e300b9":"markdown","956f7904":"markdown","7182e100":"markdown","c60683ba":"markdown","43d4cecc":"markdown","5d1f3ccd":"markdown","e457d60c":"markdown","e9e42753":"markdown","7350c82e":"markdown","1be4ebb8":"markdown","bf8f259b":"markdown","195b89e5":"markdown","7426a222":"markdown","e272da9d":"markdown","98580694":"markdown","22d0eabd":"markdown","6109ecbb":"markdown","10c16b84":"markdown","5cd73846":"markdown","6d9a1638":"markdown","8a6d0b03":"markdown","9c7091ee":"markdown","5099ee2d":"markdown","47e7f8bc":"markdown","662854b7":"markdown","f43cf1b9":"markdown","d650b879":"markdown","6beb8b27":"markdown","e6ec6e86":"markdown","58c35d43":"markdown","1ea2dfd3":"markdown","75a6e8e0":"markdown","3043ea31":"markdown","08899f2a":"markdown","0acc8213":"markdown","29f4becd":"markdown","2d2dee52":"markdown","3ee1e483":"markdown","f597aaa1":"markdown","8f389e01":"markdown","0cd262c9":"markdown","d6c91fcb":"markdown","0bb44ea1":"markdown","9824e109":"markdown","47857d8d":"markdown","aef3d0fb":"markdown","5e71d5e7":"markdown","501e489d":"markdown","930e2592":"markdown","f10aedd2":"markdown","db2236c6":"markdown","617b57c1":"markdown","1e954df2":"markdown","eace06f4":"markdown","9040df97":"markdown","58379631":"markdown","7b83afad":"markdown","e682874c":"markdown","ecb3dd97":"markdown","d9112dda":"markdown","4b564e3a":"markdown","898b2696":"markdown","c64b97e7":"markdown","b407605a":"markdown","1e19ee5c":"markdown","cef5e409":"markdown","030b322c":"markdown","8437f554":"markdown"},"source":{"c8309648":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.feature_selection import RFECV","43e2f49a":"import warnings\nwarnings.filterwarnings('ignore')","e1ee6ea3":"initial_data = pd.read_csv('..\/input\/data.csv')","b824ecce":"initial_data.head()","5c2f780b":"initial_data.drop(initial_data.columns[0], axis=1, inplace=True)","64029a65":"for column in initial_data.columns:\n    if \"Unnamed\" in column:\n        initial_data.drop(column, axis = 1, inplace=True)","2b598311":"initial_data.head()","b62a409d":"initial_data['diagnosis']=initial_data['diagnosis'].map({'M':1,'B':0})","7e4c6b93":"initial_data.head()","8219778a":"X = initial_data[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean','fractal_dimension_mean',\n                 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se',\n                 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst','fractal_dimension_worst']]\n\ny = initial_data['diagnosis']","6e07a0cd":"col_labels = ['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave_points_mean', 'symmetry_mean','fractal_dimension_mean',\n              'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se',\n              'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst','fractal_dimension_worst'\n             ]\ninitial_data.columns = col_labels","65d21322":"for c in col_labels:\n    no_missing = initial_data[c].isnull().sum()\n    if no_missing > 0:\n        print(c)\n        print(no_missing)\n    else:\n        print(c)\n        print(\"No missing values\")\n        print(' ')","254c4e82":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.countplot(initial_data['diagnosis'],label=\"Sum\")\n\nplt.show()","a291b900":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.34)","e0755736":"LR = LogisticRegression()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(LR, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nLR_fit_time = scores['fit_time'].mean()\nLR_score_time = scores['score_time'].mean()\nLR_accuracy = scores['test_accuracy'].mean()\nLR_precision = scores['test_precision_macro'].mean()\nLR_recall = scores['test_recall_macro'].mean()\nLR_f1 = scores['test_f1_weighted'].mean()\nLR_roc = scores['test_roc_auc'].mean()","848e9a23":"decision_tree = DecisionTreeClassifier()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(decision_tree, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\ndtree_fit_time = scores['fit_time'].mean()\ndtree_score_time = scores['score_time'].mean()\ndtree_accuracy = scores['test_accuracy'].mean()\ndtree_precision = scores['test_precision_macro'].mean()\ndtree_recall = scores['test_recall_macro'].mean()\ndtree_f1 = scores['test_f1_weighted'].mean()\ndtree_roc = scores['test_roc_auc'].mean()","35670291":"SVM = SVC(probability = True)\n\nscoring = ['accuracy','precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(SVM, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nSVM_fit_time = scores['fit_time'].mean()\nSVM_score_time = scores['score_time'].mean()\nSVM_accuracy = scores['test_accuracy'].mean()\nSVM_precision = scores['test_precision_macro'].mean()\nSVM_recall = scores['test_recall_macro'].mean()\nSVM_f1 = scores['test_f1_weighted'].mean()\nSVM_roc = scores['test_roc_auc'].mean()","0a04297d":"LDA = LinearDiscriminantAnalysis()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(LDA, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nLDA_fit_time = scores['fit_time'].mean()\nLDA_score_time = scores['score_time'].mean()\nLDA_accuracy = scores['test_accuracy'].mean()\nLDA_precision = scores['test_precision_macro'].mean()\nLDA_recall = scores['test_recall_macro'].mean()\nLDA_f1 = scores['test_f1_weighted'].mean()\nLDA_roc = scores['test_roc_auc'].mean()","03b56495":"QDA = QuadraticDiscriminantAnalysis()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(QDA, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nQDA_fit_time = scores['fit_time'].mean()\nQDA_score_time = scores['score_time'].mean()\nQDA_accuracy = scores['test_accuracy'].mean()\nQDA_precision = scores['test_precision_macro'].mean()\nQDA_recall = scores['test_recall_macro'].mean()\nQDA_f1 = scores['test_f1_weighted'].mean()\nQDA_roc = scores['test_roc_auc'].mean()","9c0459da":"random_forest = RandomForestClassifier()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(random_forest, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nforest_fit_time = scores['fit_time'].mean()\nforest_score_time = scores['score_time'].mean()\nforest_accuracy = scores['test_accuracy'].mean()\nforest_precision = scores['test_precision_macro'].mean()\nforest_recall = scores['test_recall_macro'].mean()\nforest_f1 = scores['test_f1_weighted'].mean()\nforest_roc = scores['test_roc_auc'].mean()","08283a4c":"KNN = KNeighborsClassifier()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(KNN, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nKNN_fit_time = scores['fit_time'].mean()\nKNN_score_time = scores['score_time'].mean()\nKNN_accuracy = scores['test_accuracy'].mean()\nKNN_precision = scores['test_precision_macro'].mean()\nKNN_recall = scores['test_recall_macro'].mean()\nKNN_f1 = scores['test_f1_weighted'].mean()\nKNN_roc = scores['test_roc_auc'].mean()","c2de0d33":"bayes = GaussianNB()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(bayes, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nbayes_fit_time = scores['fit_time'].mean()\nbayes_score_time = scores['score_time'].mean()\nbayes_accuracy = scores['test_accuracy'].mean()\nbayes_precision = scores['test_precision_macro'].mean()\nbayes_recall = scores['test_recall_macro'].mean()\nbayes_f1 = scores['test_f1_weighted'].mean()\nbayes_roc = scores['test_roc_auc'].mean()","c08a7883":"models_initial = pd.DataFrame({\n    'Model'       : ['Logistic Regression', 'Decision Tree', 'Support Vector Machine', 'Linear Discriminant Analysis', 'Quadratic Discriminant Analysis', 'Random Forest', 'K-Nearest Neighbors', 'Bayes'],\n    'Fitting time': [LR_fit_time, dtree_fit_time, SVM_fit_time, LDA_fit_time, QDA_fit_time, forest_fit_time, KNN_fit_time, bayes_fit_time],\n    'Scoring time': [LR_score_time, dtree_score_time, SVM_score_time, LDA_score_time, QDA_score_time, forest_score_time, KNN_score_time, bayes_score_time],\n    'Accuracy'    : [LR_accuracy, dtree_accuracy, SVM_accuracy, LDA_accuracy, QDA_accuracy, forest_accuracy, KNN_accuracy, bayes_accuracy],\n    'Precision'   : [LR_precision, dtree_precision, SVM_precision, LDA_precision, QDA_precision, forest_precision, KNN_precision, bayes_precision],\n    'Recall'      : [LR_recall, dtree_recall, SVM_recall, LDA_recall, QDA_recall, forest_recall, KNN_recall, bayes_recall],\n    'F1_score'    : [LR_f1, dtree_f1, SVM_f1, LDA_f1, QDA_f1, forest_f1, KNN_f1, bayes_f1],\n    'AUC_ROC'     : [LR_roc, dtree_roc, SVM_roc, LDA_roc, QDA_roc, forest_roc, KNN_roc, bayes_roc],\n    }, columns = ['Model', 'Fitting time', 'Scoring time', 'Accuracy', 'Precision', 'Recall', 'F1_score', 'AUC_ROC'])\n\nmodels_initial.sort_values(by='Accuracy', ascending=False)","94e5c232":"correlation = initial_data.corr()\n\nmask = np.zeros_like(correlation, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nf, ax = plt.subplots(figsize=(20, 20))\n\ncmap = sns.diverging_palette(180, 20, as_cmap=True)\nsns.heatmap(correlation, mask=mask, cmap=cmap, vmax=1, vmin =-1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.show()","cc902d36":"X_corr = initial_data[['smoothness_mean', 'radius_se', 'texture_se', 'smoothness_se', 'symmetry_se', \n                       'fractal_dimension_se', 'texture_worst', 'symmetry_worst','fractal_dimension_worst']]\ny_corr = initial_data['diagnosis']","092ca94b":"correlation = X_corr.corr()\n\nmask = np.zeros_like(correlation, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nf, ax = plt.subplots(figsize=(5, 5))\n\ncmap = sns.diverging_palette(180, 20, as_cmap=True)\nsns.heatmap(correlation, mask=mask, cmap=cmap, vmax=1, vmin =-1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.show()","8dadef64":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.34)","b33f5891":"LR = LogisticRegression()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(LR, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nLR_fit_time = scores['fit_time'].mean()\nLR_score_time = scores['score_time'].mean()\nLR_accuracy = scores['test_accuracy'].mean()\nLR_precision = scores['test_precision_macro'].mean()\nLR_recall = scores['test_recall_macro'].mean()\nLR_f1 = scores['test_f1_weighted'].mean()\nLR_roc = scores['test_roc_auc'].mean()","924f3e20":"decision_tree = DecisionTreeClassifier()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(decision_tree, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\ndtree_fit_time = scores['fit_time'].mean()\ndtree_score_time = scores['score_time'].mean()\ndtree_accuracy = scores['test_accuracy'].mean()\ndtree_precision = scores['test_precision_macro'].mean()\ndtree_recall = scores['test_recall_macro'].mean()\ndtree_f1 = scores['test_f1_weighted'].mean()\ndtree_roc = scores['test_roc_auc'].mean()","2d5f7fa7":"SVM = SVC(probability = True)\n\nscoring = ['accuracy','precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(SVM, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nSVM_fit_time = scores['fit_time'].mean()\nSVM_score_time = scores['score_time'].mean()\nSVM_accuracy = scores['test_accuracy'].mean()\nSVM_precision = scores['test_precision_macro'].mean()\nSVM_recall = scores['test_recall_macro'].mean()\nSVM_f1 = scores['test_f1_weighted'].mean()\nSVM_roc = scores['test_roc_auc'].mean()","42fc2be4":"LDA = LinearDiscriminantAnalysis()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(LDA, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nLDA_fit_time = scores['fit_time'].mean()\nLDA_score_time = scores['score_time'].mean()\nLDA_accuracy = scores['test_accuracy'].mean()\nLDA_precision = scores['test_precision_macro'].mean()\nLDA_recall = scores['test_recall_macro'].mean()\nLDA_f1 = scores['test_f1_weighted'].mean()\nLDA_roc = scores['test_roc_auc'].mean()","6a838ce5":"QDA = QuadraticDiscriminantAnalysis()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(QDA, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nQDA_fit_time = scores['fit_time'].mean()\nQDA_score_time = scores['score_time'].mean()\nQDA_accuracy = scores['test_accuracy'].mean()\nQDA_precision = scores['test_precision_macro'].mean()\nQDA_recall = scores['test_recall_macro'].mean()\nQDA_f1 = scores['test_f1_weighted'].mean()\nQDA_roc = scores['test_roc_auc'].mean()","91f8f8f1":"random_forest = RandomForestClassifier()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(random_forest, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nforest_fit_time = scores['fit_time'].mean()\nforest_score_time = scores['score_time'].mean()\nforest_accuracy = scores['test_accuracy'].mean()\nforest_precision = scores['test_precision_macro'].mean()\nforest_recall = scores['test_recall_macro'].mean()\nforest_f1 = scores['test_f1_weighted'].mean()\nforest_roc = scores['test_roc_auc'].mean()","854f7b69":"KNN = KNeighborsClassifier()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(KNN, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nKNN_fit_time = scores['fit_time'].mean()\nKNN_score_time = scores['score_time'].mean()\nKNN_accuracy = scores['test_accuracy'].mean()\nKNN_precision = scores['test_precision_macro'].mean()\nKNN_recall = scores['test_recall_macro'].mean()\nKNN_f1 = scores['test_f1_weighted'].mean()\nKNN_roc = scores['test_roc_auc'].mean()","3e75931d":"bayes = GaussianNB()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(bayes, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nbayes_fit_time = scores['fit_time'].mean()\nbayes_score_time = scores['score_time'].mean()\nbayes_accuracy = scores['test_accuracy'].mean()\nbayes_precision = scores['test_precision_macro'].mean()\nbayes_recall = scores['test_recall_macro'].mean()\nbayes_f1 = scores['test_f1_weighted'].mean()\nbayes_roc = scores['test_roc_auc'].mean()","97a93b5f":"models_correlation = pd.DataFrame({\n    'Model'       : ['Logistic Regression', 'Decision Tree', 'Support Vector Machine', 'Linear Discriminant Analysis', 'Quadratic Discriminant Analysis', 'Random Forest', 'K-Nearest Neighbors', 'Bayes'],\n    'Fitting time': [LR_fit_time, dtree_fit_time, SVM_fit_time, LDA_fit_time, QDA_fit_time, forest_fit_time, KNN_fit_time, bayes_fit_time],\n    'Scoring time': [LR_score_time, dtree_score_time, SVM_score_time, LDA_score_time, QDA_score_time, forest_score_time, KNN_score_time, bayes_score_time],\n    'Accuracy'    : [LR_accuracy, dtree_accuracy, SVM_accuracy, LDA_accuracy, QDA_accuracy, forest_accuracy, KNN_accuracy, bayes_accuracy],\n    'Precision'   : [LR_precision, dtree_precision, SVM_precision, LDA_precision, QDA_precision, forest_precision, KNN_precision, bayes_precision],\n    'Recall'      : [LR_recall, dtree_recall, SVM_recall, LDA_recall, QDA_recall, forest_recall, KNN_recall, bayes_recall],\n    'F1_score'    : [LR_f1, dtree_f1, SVM_f1, LDA_f1, QDA_f1, forest_f1, KNN_f1, bayes_f1],\n    'AUC_ROC'     : [LR_roc, dtree_roc, SVM_roc, LDA_roc, QDA_roc, forest_roc, KNN_roc, bayes_roc],\n    }, columns = ['Model', 'Fitting time', 'Scoring time', 'Accuracy', 'Precision', 'Recall', 'F1_score', 'AUC_ROC'])\n\nmodels_correlation.sort_values(by='Accuracy', ascending=False)","a3c59f53":"models = [LogisticRegression(),\n         DecisionTreeClassifier(),\n         SVC(probability = True),\n         LinearDiscriminantAnalysis(),\n         QuadraticDiscriminantAnalysis(),\n         RandomForestClassifier(),\n         KNeighborsClassifier(),\n         GaussianNB()]\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']","8f6117f4":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.34)","5264b049":"for model in models:\n    scores = cross_validate(model, X_train, y_train, scoring=scoring, cv=20)\n    #print(model, scores['fit_time'].mean(), scores['score_time'].mean(), scores['test_accuracy'].mean(),\n          #scores['test_precision_macro'].mean(), scores['test_recall_macro'].mean(), \n          #scores['test_f1_weighted'].mean(), scores['test_roc_auc'].mean())","e25a85d8":"models_ens = list(zip(['LR', 'DT', 'SVM', 'LDA', 'QDA', 'RF', 'KNN', 'NB'], models))\n\nmodel_ens = VotingClassifier(estimators = models_ens, voting = 'hard')\nmodel_ens.fit(X_train, y_train)\npred = model_ens.predict(X_test)\n#prob = model_ens.predict_proba(X_test)[:,1]\n\nacc_hard = accuracy_score(y_test, pred)\nprec_hard = precision_score(y_test, pred)\nrecall_hard = recall_score(y_test, pred)\nf1_hard = f1_score(y_test, pred)\nroc_auc_hard = 'not applicable'","c85c3676":"model_ens = VotingClassifier(estimators = models_ens, voting = 'soft')\nmodel_ens.fit(X_train, y_train)\npred = model_ens.predict(X_test)\nprob = model_ens.predict_proba(X_test)[:,1]\n\nacc_soft = accuracy_score(y_test, pred)\nprec_soft = precision_score(y_test, pred)\nrecall_soft = recall_score(y_test, pred)\nf1_soft = f1_score(y_test, pred)\nroc_auc_soft = roc_auc_score(y_test, prob)","c7a595b8":"models_ensembling = pd.DataFrame({\n    'Model'       : ['Ensebling_hard', 'Ensembling_soft'],\n    'Accuracy'    : [acc_hard, acc_soft],\n    'Precision'   : [prec_hard, prec_soft],\n    'Recall'      : [recall_hard, recall_soft],\n    'F1_score'    : [f1_hard, f1_soft],\n    'AUC_ROC'     : [roc_auc_hard, roc_auc_soft],\n    }, columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1_score', 'AUC_ROC'])\n\nmodels_ensembling.sort_values(by='Accuracy', ascending=False)","6543bce4":"X.shape","27facf78":"lsvc = LinearSVC(C=0.05, penalty=\"l1\", dual=False).fit(X, y)\nmodel = SelectFromModel(lsvc, prefit=True)\nX_svc = model.transform(X)\nX_svc.shape #reduction from 30 to 10 features","8dc0b87d":"X_train, X_test, y_train, y_test = train_test_split(X_svc,y,test_size=0.34)","d5afbbbb":"LR = LogisticRegression()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(LR, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nLR_fit_time = scores['fit_time'].mean()\nLR_score_time = scores['score_time'].mean()\nLR_accuracy = scores['test_accuracy'].mean()\nLR_precision = scores['test_precision_macro'].mean()\nLR_recall = scores['test_recall_macro'].mean()\nLR_f1 = scores['test_f1_weighted'].mean()\nLR_roc = scores['test_roc_auc'].mean()","9545ed86":"decision_tree = DecisionTreeClassifier()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(decision_tree, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\ndtree_fit_time = scores['fit_time'].mean()\ndtree_score_time = scores['score_time'].mean()\ndtree_accuracy = scores['test_accuracy'].mean()\ndtree_precision = scores['test_precision_macro'].mean()\ndtree_recall = scores['test_recall_macro'].mean()\ndtree_f1 = scores['test_f1_weighted'].mean()\ndtree_roc = scores['test_roc_auc'].mean()","db5c2e68":"SVM = SVC(probability = True)\n\nscoring = ['accuracy','precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(SVM, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nSVM_fit_time = scores['fit_time'].mean()\nSVM_score_time = scores['score_time'].mean()\nSVM_accuracy = scores['test_accuracy'].mean()\nSVM_precision = scores['test_precision_macro'].mean()\nSVM_recall = scores['test_recall_macro'].mean()\nSVM_f1 = scores['test_f1_weighted'].mean()\nSVM_roc = scores['test_roc_auc'].mean()","545680b1":"LDA = LinearDiscriminantAnalysis()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(LDA, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nLDA_fit_time = scores['fit_time'].mean()\nLDA_score_time = scores['score_time'].mean()\nLDA_accuracy = scores['test_accuracy'].mean()\nLDA_precision = scores['test_precision_macro'].mean()\nLDA_recall = scores['test_recall_macro'].mean()\nLDA_f1 = scores['test_f1_weighted'].mean()\nLDA_roc = scores['test_roc_auc'].mean()","bf33ecc5":"QDA = QuadraticDiscriminantAnalysis()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(QDA, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nQDA_fit_time = scores['fit_time'].mean()\nQDA_score_time = scores['score_time'].mean()\nQDA_accuracy = scores['test_accuracy'].mean()\nQDA_precision = scores['test_precision_macro'].mean()\nQDA_recall = scores['test_recall_macro'].mean()\nQDA_f1 = scores['test_f1_weighted'].mean()\nQDA_roc = scores['test_roc_auc'].mean()","c014e6ba":"random_forest = RandomForestClassifier()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(random_forest, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nforest_fit_time = scores['fit_time'].mean()\nforest_score_time = scores['score_time'].mean()\nforest_accuracy = scores['test_accuracy'].mean()\nforest_precision = scores['test_precision_macro'].mean()\nforest_recall = scores['test_recall_macro'].mean()\nforest_f1 = scores['test_f1_weighted'].mean()\nforest_roc = scores['test_roc_auc'].mean()","89db47f9":"KNN = KNeighborsClassifier()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(KNN, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nKNN_fit_time = scores['fit_time'].mean()\nKNN_score_time = scores['score_time'].mean()\nKNN_accuracy = scores['test_accuracy'].mean()\nKNN_precision = scores['test_precision_macro'].mean()\nKNN_recall = scores['test_recall_macro'].mean()\nKNN_f1 = scores['test_f1_weighted'].mean()\nKNN_roc = scores['test_roc_auc'].mean()","17620a28":"bayes = GaussianNB()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(bayes, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nbayes_fit_time = scores['fit_time'].mean()\nbayes_score_time = scores['score_time'].mean()\nbayes_accuracy = scores['test_accuracy'].mean()\nbayes_precision = scores['test_precision_macro'].mean()\nbayes_recall = scores['test_recall_macro'].mean()\nbayes_f1 = scores['test_f1_weighted'].mean()\nbayes_roc = scores['test_roc_auc'].mean()","53807f12":"models_sfm = pd.DataFrame({\n    'Model'       : ['Logistic Regression', 'Decision Tree', 'Support Vector Machine', 'Linear Discriminant Analysis', 'Quadratic Discriminant Analysis', 'Random Forest', 'K-Nearest Neighbors', 'Bayes'],\n    'Fitting time': [LR_fit_time, dtree_fit_time, SVM_fit_time, LDA_fit_time, QDA_fit_time, forest_fit_time, KNN_fit_time, bayes_fit_time],\n    'Scoring time': [LR_score_time, dtree_score_time, SVM_score_time, LDA_score_time, QDA_score_time, forest_score_time, KNN_score_time, bayes_score_time],\n    'Accuracy'    : [LR_accuracy, dtree_accuracy, SVM_accuracy, LDA_accuracy, QDA_accuracy, forest_accuracy, KNN_accuracy, bayes_accuracy],\n    'Precision'   : [LR_precision, dtree_precision, SVM_precision, LDA_precision, QDA_precision, forest_precision, KNN_precision, bayes_precision],\n    'Recall'      : [LR_recall, dtree_recall, SVM_recall, LDA_recall, QDA_recall, forest_recall, KNN_recall, bayes_recall],\n    'F1_score'    : [LR_f1, dtree_f1, SVM_f1, LDA_f1, QDA_f1, forest_f1, KNN_f1, bayes_f1],\n    'AUC_ROC'     : [LR_roc, dtree_roc, SVM_roc, LDA_roc, QDA_roc, forest_roc, KNN_roc, bayes_roc],\n    }, columns = ['Model', 'Fitting time', 'Scoring time', 'Accuracy', 'Precision', 'Recall', 'F1_score', 'AUC_ROC'])\n\nmodels_sfm.sort_values(by='Accuracy', ascending=False)","7c0b16d7":"lsvc = LinearSVC(C=0.05, penalty=\"l1\", dual=False)\nmodel = RFECV(estimator=lsvc, step=1, cv=20).fit(X,y)\nX_rfecv = model.transform(X)\nX_rfecv.shape #reduction from 30","41c7f5e6":"X_train, X_test, y_train, y_test = train_test_split(X_rfecv,y,test_size=0.34)","1d87fedd":"LR = LogisticRegression()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(LR, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nLR_fit_time = scores['fit_time'].mean()\nLR_score_time = scores['score_time'].mean()\nLR_accuracy = scores['test_accuracy'].mean()\nLR_precision = scores['test_precision_macro'].mean()\nLR_recall = scores['test_recall_macro'].mean()\nLR_f1 = scores['test_f1_weighted'].mean()\nLR_roc = scores['test_roc_auc'].mean()","72817805":"decision_tree = DecisionTreeClassifier()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(decision_tree, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\ndtree_fit_time = scores['fit_time'].mean()\ndtree_score_time = scores['score_time'].mean()\ndtree_accuracy = scores['test_accuracy'].mean()\ndtree_precision = scores['test_precision_macro'].mean()\ndtree_recall = scores['test_recall_macro'].mean()\ndtree_f1 = scores['test_f1_weighted'].mean()\ndtree_roc = scores['test_roc_auc'].mean()","cd702a09":"SVM = SVC(probability = True)\n\nscoring = ['accuracy','precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(SVM, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nSVM_fit_time = scores['fit_time'].mean()\nSVM_score_time = scores['score_time'].mean()\nSVM_accuracy = scores['test_accuracy'].mean()\nSVM_precision = scores['test_precision_macro'].mean()\nSVM_recall = scores['test_recall_macro'].mean()\nSVM_f1 = scores['test_f1_weighted'].mean()\nSVM_roc = scores['test_roc_auc'].mean()","442a3290":"LDA = LinearDiscriminantAnalysis()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(LDA, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nLDA_fit_time = scores['fit_time'].mean()\nLDA_score_time = scores['score_time'].mean()\nLDA_accuracy = scores['test_accuracy'].mean()\nLDA_precision = scores['test_precision_macro'].mean()\nLDA_recall = scores['test_recall_macro'].mean()\nLDA_f1 = scores['test_f1_weighted'].mean()\nLDA_roc = scores['test_roc_auc'].mean()","13c2bc87":"QDA = QuadraticDiscriminantAnalysis()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(QDA, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nQDA_fit_time = scores['fit_time'].mean()\nQDA_score_time = scores['score_time'].mean()\nQDA_accuracy = scores['test_accuracy'].mean()\nQDA_precision = scores['test_precision_macro'].mean()\nQDA_recall = scores['test_recall_macro'].mean()\nQDA_f1 = scores['test_f1_weighted'].mean()\nQDA_roc = scores['test_roc_auc'].mean()","4d642d12":"random_forest = RandomForestClassifier()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(random_forest, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nforest_fit_time = scores['fit_time'].mean()\nforest_score_time = scores['score_time'].mean()\nforest_accuracy = scores['test_accuracy'].mean()\nforest_precision = scores['test_precision_macro'].mean()\nforest_recall = scores['test_recall_macro'].mean()\nforest_f1 = scores['test_f1_weighted'].mean()\nforest_roc = scores['test_roc_auc'].mean()","655ed5f7":"KNN = KNeighborsClassifier()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(KNN, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nKNN_fit_time = scores['fit_time'].mean()\nKNN_score_time = scores['score_time'].mean()\nKNN_accuracy = scores['test_accuracy'].mean()\nKNN_precision = scores['test_precision_macro'].mean()\nKNN_recall = scores['test_recall_macro'].mean()\nKNN_f1 = scores['test_f1_weighted'].mean()\nKNN_roc = scores['test_roc_auc'].mean()","25063706":"bayes = GaussianNB()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(bayes, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nbayes_fit_time = scores['fit_time'].mean()\nbayes_score_time = scores['score_time'].mean()\nbayes_accuracy = scores['test_accuracy'].mean()\nbayes_precision = scores['test_precision_macro'].mean()\nbayes_recall = scores['test_recall_macro'].mean()\nbayes_f1 = scores['test_f1_weighted'].mean()\nbayes_roc = scores['test_roc_auc'].mean()","e00a5a1a":"models_rfecv = pd.DataFrame({\n    'Model'       : ['Logistic Regression', 'Decision Tree', 'Support Vector Machine', 'Linear Discriminant Analysis', 'Quadratic Discriminant Analysis', 'Random Forest', 'K-Nearest Neighbors', 'Bayes'],\n    'Fitting time': [LR_fit_time, dtree_fit_time, SVM_fit_time, LDA_fit_time, QDA_fit_time, forest_fit_time, KNN_fit_time, bayes_fit_time],\n    'Scoring time': [LR_score_time, dtree_score_time, SVM_score_time, LDA_score_time, QDA_score_time, forest_score_time, KNN_score_time, bayes_score_time],\n    'Accuracy'    : [LR_accuracy, dtree_accuracy, SVM_accuracy, LDA_accuracy, QDA_accuracy, forest_accuracy, KNN_accuracy, bayes_accuracy],\n    'Precision'   : [LR_precision, dtree_precision, SVM_precision, LDA_precision, QDA_precision, forest_precision, KNN_precision, bayes_precision],\n    'Recall'      : [LR_recall, dtree_recall, SVM_recall, LDA_recall, QDA_recall, forest_recall, KNN_recall, bayes_recall],\n    'F1_score'    : [LR_f1, dtree_f1, SVM_f1, LDA_f1, QDA_f1, forest_f1, KNN_f1, bayes_f1],\n    'AUC_ROC'     : [LR_roc, dtree_roc, SVM_roc, LDA_roc, QDA_roc, forest_roc, KNN_roc, bayes_roc],\n    }, columns = ['Model', 'Fitting time', 'Scoring time', 'Accuracy', 'Precision', 'Recall', 'F1_score', 'AUC_ROC'])\n\nmodels_rfecv.sort_values(by='Accuracy', ascending=False)","22ce6a63":"lsvc = LinearSVC(C=0.05, penalty=\"l1\", dual=False).fit(X, y)\netc = ExtraTreesClassifier()\netc.fit(X, y)\n\nmodel = SelectFromModel(etc, prefit=True)\nX_etc = model.transform(X)\nX_etc.shape ","fec1f830":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.34)","217903d8":"LR = LogisticRegression()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(LR, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nLR_fit_time = scores['fit_time'].mean()\nLR_score_time = scores['score_time'].mean()\nLR_accuracy = scores['test_accuracy'].mean()\nLR_precision = scores['test_precision_macro'].mean()\nLR_recall = scores['test_recall_macro'].mean()\nLR_f1 = scores['test_f1_weighted'].mean()\nLR_roc = scores['test_roc_auc'].mean()","2f92242f":"decision_tree = DecisionTreeClassifier()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(decision_tree, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\ndtree_fit_time = scores['fit_time'].mean()\ndtree_score_time = scores['score_time'].mean()\ndtree_accuracy = scores['test_accuracy'].mean()\ndtree_precision = scores['test_precision_macro'].mean()\ndtree_recall = scores['test_recall_macro'].mean()\ndtree_f1 = scores['test_f1_weighted'].mean()\ndtree_roc = scores['test_roc_auc'].mean()","b601b5c6":"SVM = SVC(probability = True)\n\nscoring = ['accuracy','precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(SVM, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nSVM_fit_time = scores['fit_time'].mean()\nSVM_score_time = scores['score_time'].mean()\nSVM_accuracy = scores['test_accuracy'].mean()\nSVM_precision = scores['test_precision_macro'].mean()\nSVM_recall = scores['test_recall_macro'].mean()\nSVM_f1 = scores['test_f1_weighted'].mean()\nSVM_roc = scores['test_roc_auc'].mean()","23c6bef9":"LDA = LinearDiscriminantAnalysis()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(LDA, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nLDA_fit_time = scores['fit_time'].mean()\nLDA_score_time = scores['score_time'].mean()\nLDA_accuracy = scores['test_accuracy'].mean()\nLDA_precision = scores['test_precision_macro'].mean()\nLDA_recall = scores['test_recall_macro'].mean()\nLDA_f1 = scores['test_f1_weighted'].mean()\nLDA_roc = scores['test_roc_auc'].mean()","08518b36":"QDA = QuadraticDiscriminantAnalysis()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(QDA, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nQDA_fit_time = scores['fit_time'].mean()\nQDA_score_time = scores['score_time'].mean()\nQDA_accuracy = scores['test_accuracy'].mean()\nQDA_precision = scores['test_precision_macro'].mean()\nQDA_recall = scores['test_recall_macro'].mean()\nQDA_f1 = scores['test_f1_weighted'].mean()\nQDA_roc = scores['test_roc_auc'].mean()","35a80275":"random_forest = RandomForestClassifier()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(random_forest, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nforest_fit_time = scores['fit_time'].mean()\nforest_score_time = scores['score_time'].mean()\nforest_accuracy = scores['test_accuracy'].mean()\nforest_precision = scores['test_precision_macro'].mean()\nforest_recall = scores['test_recall_macro'].mean()\nforest_f1 = scores['test_f1_weighted'].mean()\nforest_roc = scores['test_roc_auc'].mean()","0443b5d0":"KNN = KNeighborsClassifier()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(KNN, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nKNN_fit_time = scores['fit_time'].mean()\nKNN_score_time = scores['score_time'].mean()\nKNN_accuracy = scores['test_accuracy'].mean()\nKNN_precision = scores['test_precision_macro'].mean()\nKNN_recall = scores['test_recall_macro'].mean()\nKNN_f1 = scores['test_f1_weighted'].mean()\nKNN_roc = scores['test_roc_auc'].mean()","e217587b":"bayes = GaussianNB()\n\nscoring = ['accuracy', 'precision_macro', 'recall_macro' , 'f1_weighted', 'roc_auc']\nscores = cross_validate(bayes, X_train, y_train, scoring=scoring, cv=20)\n\nsorted(scores.keys())\nbayes_fit_time = scores['fit_time'].mean()\nbayes_score_time = scores['score_time'].mean()\nbayes_accuracy = scores['test_accuracy'].mean()\nbayes_precision = scores['test_precision_macro'].mean()\nbayes_recall = scores['test_recall_macro'].mean()\nbayes_f1 = scores['test_f1_weighted'].mean()\nbayes_roc = scores['test_roc_auc'].mean()","d6a51301":"models_tree = pd.DataFrame({\n    'Model'       : ['Logistic Regression', 'Decision Tree', 'Support Vector Machine', 'Linear Discriminant Analysis', 'Quadratic Discriminant Analysis', 'Random Forest', 'K-Nearest Neighbors', 'Bayes'],\n    'Fitting time': [LR_fit_time, dtree_fit_time, SVM_fit_time, LDA_fit_time, QDA_fit_time, forest_fit_time, KNN_fit_time, bayes_fit_time],\n    'Scoring time': [LR_score_time, dtree_score_time, SVM_score_time, LDA_score_time, QDA_score_time, forest_score_time, KNN_score_time, bayes_score_time],\n    'Accuracy'    : [LR_accuracy, dtree_accuracy, SVM_accuracy, LDA_accuracy, QDA_accuracy, forest_accuracy, KNN_accuracy, bayes_accuracy],\n    'Precision'   : [LR_precision, dtree_precision, SVM_precision, LDA_precision, QDA_precision, forest_precision, KNN_precision, bayes_precision],\n    'Recall'      : [LR_recall, dtree_recall, SVM_recall, LDA_recall, QDA_recall, forest_recall, KNN_recall, bayes_recall],\n    'F1_score'    : [LR_f1, dtree_f1, SVM_f1, LDA_f1, QDA_f1, forest_f1, KNN_f1, bayes_f1],\n    'AUC_ROC'     : [LR_roc, dtree_roc, SVM_roc, LDA_roc, QDA_roc, forest_roc, KNN_roc, bayes_roc],\n    }, columns = ['Model', 'Fitting time', 'Scoring time', 'Accuracy', 'Precision', 'Recall', 'F1_score', 'AUC_ROC'])\n\nmodels_tree.sort_values(by='Accuracy', ascending=False)","750d9882":"model_general = pd.concat([models_initial['Model'], models_initial['Accuracy'], \n                           models_correlation['Model'],models_correlation['Accuracy'],\n                          models_sfm['Model'], models_sfm['Accuracy'],\n                          models_rfecv['Model'], models_rfecv['Accuracy'],\n                          models_tree['Model'], models_tree['Accuracy'],\n                          models_ensembling['Model'], models_ensembling['Accuracy']]\n                          , axis=1)\n\nmodel_general.columns = ['W\/out reduction', 'Accuracy', 'Correlation', 'Accuracy_corr',\n                        'Linear+SFM', 'Accuracy_sfm', 'Linear+RFECV', 'Accuracy_RFECV', 'Extra trees',\n                         'Accuracy_trees', 'Voting', 'Accuracy_voting']\n\nmodel_general.sort_values(by='Accuracy', ascending=False)","5f902131":"Binary diagnosis input <a class=\"anchor\" id=\"to_dummies\"><\/a>","cc64ba9e":"### Comparison <a class=\"anchor\" id=\"sum_1\"><\/a>","e517d083":"# Breast cancer Wisconsin diagnostic data set - Kaggle\n## Binary classification - mutiple method comparison\n\nhttps:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data\n\n------------------------------------------------------------\n\nThe main objective of this project is to present application of different classifiers used for binary classification. If you're looking for some background information on this topic or you're already familiar with the basics and want to dig deeper into the world of the binary classification metrics, check out this great artcile [\"F1 Score vs ROC AUC vs Accuracy vs PR AUC: Which Evaluation Metric Should You Choose?\"](https:\/\/neptune.ai\/blog\/f1-score-accuracy-roc-auc-pr-auc) by Jakub Czakon, Senior Data Scientist at neptune.ai. Written in an extremely reader-friendly way, the article will guide you throught the most commonly used metrics like F1 score, ROC AUC, PR AUC, and Accuracy, while comparing them using an example binary classification problem and explaining what should be considered when deciding to choose one metric over the other.\n\n------------------------------------------------------------\n","936c7d1e":"### Comparison <a class=\"anchor\" id=\"sum_3\"><\/a>","3038eb4f":"#### Support Vector Machine","90e300b9":"### [Data set without dimensionality reduction](#without_red)\n\n### Dimensionality reduction:\n\n### [Correlation coefficient score](#correlation)\n### [Voting classifier](#voting)\n### [Linear SVC + SelectFromModel](#linear_model)\n### [Linear SVC + RFECV](#linear_rfecv)\n### [Tree-based feature selection](#tree)\n#### Classifiers:\n\n* Logistic Regression\n* Decision Tree\n* Support Vector Machine\n* Linear Discriminant Analysis\n* Quadratic Discriminant Analysis\n* Random Forest\n* K-Nearest Neighbors\n* Naive Bayes\n\n#### Scoring:\n\n* precision score\n* recall score\n* F1 score\n* support score\n* accuracy score\n* AUC\/ROC","956f7904":"Missing values <a class=\"anchor\" id=\"missing values\"><\/a>","7182e100":"#### Decision Tree","c60683ba":"#### Decision Tree","43d4cecc":"#### Support Vector Machine","5d1f3ccd":"### Comparison <a class=\"anchor\" id=\"sum_6\"><\/a>","e457d60c":"#### Quadratic Discriminant Analysis","e9e42753":"### Soft <a class=\"anchor\" id=\"soft\"><\/a>","7350c82e":"#### Random Forest Classifier","1be4ebb8":"## Libraries <a class=\"anchor\" id=\"zero-bullet\"><\/a>","bf8f259b":"#### Support Vector Machine","195b89e5":"#### Naive Bayes","7426a222":"#### Linear Discriminant Analysis","e272da9d":"Note! Logistic regression, Lineard and Quadratic Discriminant Analysis, and Random Forest algorithms are themselves used for adjusting number of features. That means that applying additional reduction (corelation, SelectFromModel, REFCV) the dimensionality can be reduced more that once.","98580694":"#### Support Vector Machine","22d0eabd":"## Correlation <a class=\"anchor\" id=\"correlation\"><\/a>","6109ecbb":"#### Set is not perfectly balanced, however the differene in class distribution is not that significant to apply solutions dedicated for highly imbalanced cases.","10c16b84":"#### Random Forest Classifier","5cd73846":"#### Decision Tree","6d9a1638":"#### Decision Tree","8a6d0b03":"#### Linear Discriminant Analysis","9c7091ee":"#### Logistic Regression","5099ee2d":"#### Linear Discriminant Analysis","47e7f8bc":"#### Quadratic Discriminant Analysis","662854b7":"#### Decision Tree","f43cf1b9":"# Table of Contents:\n\n\n## 1. [Data set preparation](#first_bullet)\n- 1.1 [Read the data set](#read_data)\n- 1.2 [Remove ID column](#remove_id)\n- 1.3 [Binary 'diagnosis' input](#to_dummies)\n- 1.4 [Missing values](#missing_values)\n- 1.5 [Class distribution](#class_dist)\n\n## 2. [Testing algorithms](#second_bullet)\n- 2.1 [Withouth dimensionality reduction](#without_red)\n    > 2.1.1 [Comparison](#sum_1)\n- 2.2 [Correlation Coefficient Score](#correlation)\n    > 2.2.1 [Comparison](#sum_2)\n- 2.3 [Voting classifier](#voting)\n    > 2.3.1 [Voting hard](#hard)\n    > 2.3.2 [Voting soft](#soft)\n    >> 2.3.3 [Comparison](#sum_3)\n- 2.4 [Linear SVC + SelectFromModel](#linear_model)\n    > 2.4.1 [Comparison](#sum_4)\n- 2.5 [Linear SVC + RFECV](#linear_rfecv)\n    > 2.5.1 [Comparison](#sum_5)\n- 2.6 [Tree-based feature classifier](#tree)\n    > 2.6.1 [Comparison](#sum_6)\n\n## 3. [General comparison](#third_bullet)","d650b879":"#### K-Nearest Neighbors","6beb8b27":"Dividing the dataset into a separate training and test set:","e6ec6e86":"#### Random Forest Classifier","58c35d43":"Features to  be included: 'smoothness_mean', 'radius_se', 'texture_se', 'smoothness_se', 'symmetry_se', 'fractal_dimension_se', 'texture_worst', 'symmetry_worst', 'fractal_dimension_worst'.","1ea2dfd3":"#### Support Vector Machine","75a6e8e0":"## Withouth reduction <a class=\"anchor\" id=\"without_red\"><\/a>","3043ea31":"librimind.com: 'Data columns with very similar trends are also likely to carry very similar information. In this case, only one of them will suffice to feed the machine learning model. Here we calculate the correlation coefficient between numerical and nominal columns as the Coefficient and the Pearson\u2019s chi square value respectively. Pairs of columns with correlation coefficient higher than a threshold are reduced to only one.","08899f2a":"## Testing algorithms <a class=\"anchor\" id=\"second-bullet\"><\/a>","0acc8213":"#### Random Forest Classifier","29f4becd":"#### Quadratic Discriminant Analysis","2d2dee52":"#### K-Nearest Neighbors","3ee1e483":"## Voting classifier <a class=\"anchor\" id=\"voting\"><\/a>\n\n\nDocumentation: If \u2018hard\u2019, uses predicted class labels for majority rule voting. Else if \u2018soft\u2019, predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers.","f597aaa1":"Class distribution <a class=\"anchor\" id=\"class_dist\"><\/a>","8f389e01":"# Linear SVC + RFECV <a class=\"anchor\" id=\"linear_rfecv\"><\/a>\n\nDocumentation: Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. RFECV performs RFE in a cross-validation loop to find the optimal number of features.","0cd262c9":"#### Naive Bayes","d6c91fcb":"### Hard <a class=\"anchor\" id=\"hard\"><\/a>","0bb44ea1":"### Comparison <a class=\"anchor\" id=\"sum_5\"><\/a>","9824e109":"### Comparison <a class=\"anchor\" id=\"sum_2\"><\/a>","47857d8d":"Read the data set <a class=\"anchor\" id=\"read_data\"><\/a>","aef3d0fb":"#### Linear Discriminant Analysis","5e71d5e7":"On the data set:\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. n the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34]. \n\n\nAttribute Information:\n\n=>  Diagnosis (M = malignant, B = benign))\n\nTen real-valued features are computed for each cell nucleus:\n\n* radius (mean of distances from center to points on the perimeter) \n* texture (standard deviation of gray-scale values) \n* perimeter \n* area \n* smoothness (local variation in radius lengths) \n* compactness (perimeter^2 \/ area - 1.0) \n* concavity (severity of concave portions of the contour) \n* concave points (number of concave portions of the contour) \n* symmetry \n* fractal dimension (\"coastline approximation\" - 1)\n\n#### Limitations: super small data set!","501e489d":"#### K-Nearest Neighbors","930e2592":"# Linear SVC + SelectFromModel <a class=\"anchor\" id=\"linear_model\"><\/a>\n\n\nDocumentation: Linear models penalized with the L1 norm have sparse solutions: many of their estimated coefficients are zero. When the goal is to reduce the dimensionality of the data to use with another classifier, they can be used along with feature_selection.SelectFromModel to select the non-zero coefficients. In particular, sparse estimators useful for this purpose are the linear_model.Lasso for regression, and of linear_model.LogisticRegression and svm.LinearSVC for classification.","f10aedd2":"#### K-Nearest Neighbors","db2236c6":"## Data set cleaning <a class=\"anchor\" id=\"first-bullet\"><\/a>","617b57c1":"#### Logistic Regression","1e954df2":"#### Random Forest Classifier","eace06f4":"Removing 'ID' column <a class=\"anchor\" id=\"remove_id\"><\/a>","9040df97":"#### Naive Bayes","58379631":"#### Decision Tree","7b83afad":"#### Decision Tree","e682874c":"* One of the data set's hallmarks is relatively high correlation coefficient score - only score no higher than 0.6 will be considered acceptable.\n* Correlation not necceserily means causation, that is why features will not be exluded only for their low correlation with diagnosis (in the heatmap below diagnosis skipped).","ecb3dd97":"# Tree-based feature selection <a class=\"anchor\" id=\"tree\"><\/a>\n\n\nDocumentation: Tree-based estimators (see the sklearn.tree module and forest of trees in the sklearn.ensemble module) can be used to compute feature importances, which in turn can be used to discard irrelevant features (when coupled with the sklearn.feature_selection.SelectFromModel meta-transformer).","d9112dda":"#### Naive Bayes","4b564e3a":"#### K-Nearest Neighbors","898b2696":"# General comparison <a class=\"anchor\" id=\"third_bullet\"><\/a>","c64b97e7":"#### Naive Bayes","b407605a":"### Comparison <a class=\"anchor\" id=\"sum_4\"><\/a>","1e19ee5c":"#### Decision Tree","cef5e409":"#### Quadratic Discriminant Analysis","030b322c":"#### Quadratic Discriminant Analysis","8437f554":"#### Linear Discriminant Analysis"}}