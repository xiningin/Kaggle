{"cell_type":{"5aa50f76":"code","94271973":"code","ef23aadb":"code","5f65ee4b":"code","74255f1d":"code","9d93b5c7":"code","71771c96":"code","b9983478":"code","ea6e875c":"code","73feb253":"code","4971b5ce":"code","fc115b5b":"code","19337678":"code","7aa2c67a":"code","a407b0d2":"code","bef74c9b":"code","8130c80b":"code","97afc48d":"code","2d9945d2":"markdown","cbe3c230":"markdown","56569873":"markdown","7c287c5e":"markdown","bc327246":"markdown","9ea284fd":"markdown"},"source":{"5aa50f76":"import tensorflow as tf\nimport keras\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm","94271973":"EPOCHS = 2\nBATCH_SIZE = 128\n\nSEQ_LEN = 129","ef23aadb":"with open(\"\/kaggle\/input\/shakespeare-plays\/alllines.txt\") as f:\n    data = f.readlines()\ndata = \"\".join(data)\nprint(len(data))\nprint(data[:200])","5f65ee4b":"tokenizer = tf.keras.preprocessing.text.Tokenizer(\n    filters=\"\",\n    lower=False,\n    split=\"\",\n    char_level=True\n)\ntokenizer.fit_on_texts(data)\n\ntrain_len = int(0.9*len(data))\n\ntrain_tokens = tokenizer.texts_to_sequences(data[:train_len])\ntest_tokens = tokenizer.texts_to_sequences(data[train_len:])","74255f1d":"# the above comes out as [[letter1], [letter2],...], flatten this:\ntrain_tokens = [letter[0] for letter in train_tokens]\ntest_tokens = [letter[0] for letter in test_tokens]\n\n# batch the above\ntrain_tokens = np.array(\n    [train_tokens[i:i+SEQ_LEN] \n    for i in range(0, len(train_tokens), BATCH_SIZE)][:-1]\n)\ntest_tokens = np.array(\n    [test_tokens[i:i+SEQ_LEN] \n       for i in range(0, len(test_tokens), BATCH_SIZE)][:-1]\n)","9d93b5c7":"train_x = train_tokens[:,:-1] - 1\ntrain_y = train_tokens[:,1:] - 1\n\ntest_x = test_tokens[:,:-1] - 1\ntest_y = test_tokens[:,1:] - 1","71771c96":"class Seq2SeqModel(keras.Model):\n    def __init__(self, vocab_size, embedding_dim, h1):\n        super().__init__()\n        self.embedding = keras.layers.Embedding(\n                                vocab_size, \n                                embedding_dim,\n                                input_length=1,\n                                name=\"char_embedding\"\n        )\n        self.lstm = keras.layers.LSTM(h1, return_sequences=True)\n        self.dense = keras.layers.Dense(vocab_size, activation=\"softmax\")\n\n    def call(self, x):\n        char_embeds = self.embedding(x)\n        time_outputs = self.lstm(char_embeds)\n        char_probs = self.dense(time_outputs)\n        \n        return char_probs","b9983478":"model = Seq2SeqModel(len(tokenizer.index_word), 32, 64)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","ea6e875c":"model(train_x[:2])","73feb253":"model.fit(train_x, train_y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.1)","4971b5ce":"model.summary()","fc115b5b":"def generate_text(model, tokenizer, seed_text, next_chars):\n    letter_idx = [letter[0]-1 for letter in tokenizer.texts_to_sequences(seed_text)]\n    for _ in tqdm(range(next_chars)):\n        last_char_probabilities = model.predict([letter_idx])[0, -1]\n        last_idx = np.random.choice(len(tokenizer.index_word), p=last_char_probabilities)\n        letter_idx.append(last_idx)\n\n    print(\"\".join(tokenizer.sequences_to_texts([[letter+1] for letter in letter_idx])))\n    \nseed_text = \"To be, or not to be\"\nnext_chars = 500\ngenerate_text(model, tokenizer, seed_text, next_chars)","19337678":"model.fit(train_x, train_y, batch_size=BATCH_SIZE, epochs=5, validation_split=0.1)","7aa2c67a":"generate_text(model, tokenizer, seed_text, next_chars)","a407b0d2":"class Seq2SeqModel(keras.Model):\n    def __init__(self, vocab_size, embedding_dim, h1, rnn_stack):\n        super().__init__()\n        self.embedding = keras.layers.Embedding(\n                                vocab_size, \n                                embedding_dim,\n                                input_length=1,\n                                name=\"char_embedding\"\n        )\n        self.lstms = [keras.layers.LSTM(h1, return_sequences=True) for _ in range(rnn_stack)]\n        self.dense = keras.layers.Dense(vocab_size, activation=\"softmax\")\n\n    def call(self, x):\n        x = self.embedding(x) \n        for lstm in self.lstms:\n            x = lstm(x)\n        char_probs = self.dense(x)\n        \n        return char_probs","bef74c9b":"model = Seq2SeqModel(len(tokenizer.index_word), 32, 64, 3)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(train_x, train_y, batch_size=BATCH_SIZE, epochs=7, validation_split=0.1)","8130c80b":"generate_text(model, tokenizer, seed_text, next_chars)","97afc48d":"model.fit(train_x, train_y, batch_size=BATCH_SIZE, epochs=20, validation_split=0.1)\ngenerate_text(model, tokenizer, seed_text, next_chars)","2d9945d2":"## Model\nWe demo the keras Model API. This should be familiar to pytorch users.","cbe3c230":"We are trying to predict one character ahead. Therefore we need the following as X and Y:","56569873":"# Sequence to Sequence RNNs\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/a\/a2\/Shakespeare.jpg\/936px-Shakespeare.jpg\" alt=\"drawing\" width=\"200\"\/>\n\nThis is the first in a series of Sequence to Sequence LSTMs. This is inspired by Andrej Karpathy's [blog](http:\/\/karpathy.github.io\/2015\/05\/21\/rnn-effectiveness\/) in 2015 showing how to generate Shakespeare text, linux code etc.\n","7c287c5e":"Thanks to eager mode we can things like below:","bc327246":"## Stacked Model with Keras Model API","9ea284fd":"## Data"}}