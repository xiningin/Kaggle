{"cell_type":{"00286c5e":"code","ee00c802":"code","e140c0d6":"code","c6202a97":"code","efa5209f":"code","eac42a1d":"code","13ccdd96":"code","d657d980":"code","74591b85":"code","680120c8":"code","9937e7e4":"code","a081b185":"code","3ed9432a":"code","e8d4d7c9":"code","5890ee87":"code","be850fd1":"code","9485abfc":"code","9252366a":"code","0a3a3061":"code","e01dcfae":"code","16c5d30c":"code","1d4be727":"code","c9a9d23e":"code","2322e631":"code","978f1c5c":"code","33436696":"code","739d719b":"code","6b6c233c":"code","5b81d2bf":"code","fef86052":"code","4508dc30":"markdown","5fa12647":"markdown","3870cdf0":"markdown","1280c7a6":"markdown","32a8ebb1":"markdown","f5b4506e":"markdown","29e17e3e":"markdown","c0c0a4d4":"markdown","737b8b7e":"markdown","413bb88a":"markdown","0d232627":"markdown","ce0b9095":"markdown","9e036f53":"markdown","3493c834":"markdown","9af1b39b":"markdown"},"source":{"00286c5e":"import plotly.figure_factory as ff\nimport numpy as np\n# np.random.seed(1)\n\n#background = text\nbg_text = [['t', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l', 'a', 'y', 'g', 'r', 'o', 'u', 'n', 'd', 's', 'e', 'r', 'i', 'e', 's'],\n          ['m', 'a', 'y', '2', '0', '2', '1', 't', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l', 'a', 'y', 'g', 'r', 'o', 'u', 'n'],\n          ['d', 's', 'e', 'r', 'i', 'e', 's', 'm', 'a', 'y', '2', '0', '2', '1', 't', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l'],\n          ['a', 'y', 'g', 'r', 'o', 'u', 'n', 'd', 't', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l', 'a', 'y', 'g', 'r', 'o', 'u'],\n          ['n', 'd', 's', 'e', 't', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l', 'a', 'y', 'g', 'r', 'o', 'u', 'n', 'd', 's', 'e'],\n          ['r', 'i', 'e', 's', 'm', 'a', 'y', '2', '0', '2', '1', 't', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l', 'a', 'y', 'g'],          \n          ['r', 'o', 'u', 'n', 'd', 's', 'e', 'r', 'i', 'e', 's', 'm', 'a', 'y', '2', '0', '2', '1', 't', 'a', 'b', 'u', 'l'],\n          ['t', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l', 'a', 'y', 'g', 'r', 'o', 'u', 'n', 'd', 's', 'e', 'r', 'i', 'e', 's'],\n          ['m', 'a', 'y', '2', '0', '2', '1', 't', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l', 'a', 'y', 'g', 'r', 'o', 'u', 'n'],\n          ['d', 's', 'e', 'r', 'i', 'e', 's', 'm', 'a', 'y', '2', '0', '2', '1', 't', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l'],\n          ['a', 'y', 'g', 'r', 'o', 'u', 'n', 'd', 't', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l', 'a', 'y', 'g', 'r', 'o', 'u'],\n          ['n', 'd', 's', 'e', 't', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l', 'a', 'y', 'g', 'r', 'o', 'u', 'n', 'd', 's', 'e'],\n          ['r', 'i', 'e', 's', 'm', 'a', 'y', '2', '0', '2', '1', 't', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l', 'a', 'y', 'g'],          \n          ['r', 'o', 'u', 'n', 'd', 's', 'e', 'r', 'i', 'e', 's', 'm', 'a', 'y', '2', '0', '2', '1', 't', '@', 'd', 'e', 's'],]\n          \n    \ntext_1 = text_2 = bg_text\n\nz = [[.0, .5, 0, 0, 0, 0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .5, .0 ],\n     [.8, .7, 0, 0, 0,.0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .7, .8 ],\n     [.0, .0, 0, 0, 0,.8, .8, .8, .8, .8, .0, .8, .8, .8, .0, .8, .0, .8, .0, .0, .0, .0, .0 ],\n     [.0, .0, 0 ,0, 0,.8, .0, .8, .0, .8, .0, .8, .0, .8, .0, .8, .0, .8, .0, .0, .0, .0, .0 ],\n     [.0, .0, 0, 0, 0,.8, .0, .8, .0, .8, .0, .8, .8, .8, .0, .8, .8, .8, .0, .0, .0, .0, .0 ],\n     [.0, .0, 0, 0, 0,.8, .0, .8, .0, .8, .0, .8, .0, .8, .0, .0, .8, .0, .0, .0, .0, .0, .0 ],\n     [.0, .0, 0, 0, 0,.8, .0, .8, .0, .8, .0, .8, .0, .8, .0, .0, .8, .0, .0, .0, .0, .0, .0 ],\n     [.0, .0, 0, 0, 0,.0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0 ],\n     [.0, .0, 0, 0, 0,.5, .5, .5, .5, .5,  0, .5, .5, .5, .0, .5, .5, .5, .0, .0, .0, .0, .0,],\n     [.0, .0,.0,.0,.0,.0, .0, .5, .0, .0,  0, .5, .0, .5, .0, .5, .0, .0, .0, .0, .0, .0, .0 ],\n     [.0, .0, 0, 0, 0, 0, .0, .5, .0, .0,  0, .5, .5, .5, .0, .5, .5, .5, .0, .0, .0, .0, .0 ],\n     [.0, .0, 0,.0,.0,.0, .0, .5, .0, .0,  0, .5, .0, .0, .0, .0, .0, .5, .0, .0, .0, .0, .0 ],\n     [.8, .7, 0, 0, 0, 0, .0, .5, .0,  0, .0, .5, .0, .0, .0, .5, .5, .5, .0, .0, .0, .7, .8 ],\n     [.0, .5, 0, 0, 0, 0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .5, .0 ],\n     ]\n     \n    \n# Display something on hover\nhover=[]\nfor x in range(len(bg_text)):\n    hover.append([i + '<br>' + 'TPS: May 2021' + str(j)\n                      for i, j in zip(text_1[x], text_2[x])])\n\n# Invert Matrices\nbg_text = bg_text[::-1]\nhover =hover[::-1]\nz = z[::-1]\n\n# Set Colorscale\ncolorscale=[[0.0, '#d4d4d4'], [.2, 'lightsalmon'],\n            [.4, 'salmon'], [.6, 'lightseagreen'],\n            [.8, 'seagreen'],[1, 'rgba(255, 0, 0, 0.7)']]\n\n\n# Make Annotated Heatmap\nfig = ff.create_annotated_heatmap(z, annotation_text=bg_text, text=hover,\n                                 colorscale=colorscale, font_colors=['white'], hoverinfo='text')\nfig.update_layout(width=750,\n                  height=450,\n                 )                \n\nfig.show()","ee00c802":"import numpy as np\nimport pandas as pd\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\nfrom sklearn.preprocessing import LabelEncoder\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\npio.templates.default = \"none\"\n\nfrom umap import UMAP\nfrom sklearn.manifold import TSNE\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","e140c0d6":"subm = pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv', index_col='id')\ntrain = pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv',  index_col='id')\ntest = pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv',  index_col='id')","c6202a97":"print('Train data of shape {}'.format(train.shape))\ndisplay(train.head())\nprint('Test data of shape {}'.format(test.shape))\ndisplay(test.head())","efa5209f":"target = train.pop('target')","eac42a1d":"display(train.describe().T.head())","13ccdd96":"print('Features with negative values: Train data')\nfor i in train.columns:\n    if train[i].min() < 0:\n        print(i)\n","d657d980":"print('Features with negative values: Test data')\nfor i in test.columns:\n    if test[i].min() < 0:\n        print(i)\n","74591b85":"# train_data missing values\nunique_values_train = []\nfor col in train.columns:\n    c = train[col].nunique()\n    pc = np.round((100 * (c)\/len(train)), 2)            \n    dict1 ={\n        'Features' : col,\n        'unique_train (count)': c,\n        #'unique_trian (%)': '{}%'.format(pc)\n    }\n    unique_values_train.append(dict1)\nDF1 = pd.DataFrame(unique_values_train, index=None).sort_values(by='unique_train (count)',ascending=False)\n#print(DF1)\n\n\n# test_data missing values\nunique_values_test = []\nfor col in test.columns:\n    c = test[col].nunique()\n    pc = np.round((100 * (c)\/len(test)), 2)            \n    dict2 ={\n        'Features' : col,\n        'unique_test (count)': c,\n        #'unique_test (%)': '{}%'.format(pc)\n    }\n    unique_values_test.append(dict2)\nDF2 = pd.DataFrame(unique_values_test, index=None).sort_values(by='unique_test (count)',ascending=False)\n#print(DF2)\n\ndf = pd.concat([DF1, DF2], axis=1)\ndf.head()","680120c8":"fig = go.Figure(data=[go.Scatter(x=DF1['Features'],\n                             y=DF1[\"unique_train (count)\"], mode= 'markers',                             \n                             name='Train', marker_color='lightseagreen'),        \n\n                go.Scatter(x=DF2['Features'],\n                             y=DF2[\"unique_test (count)\"], mode= 'markers',\n                             name='Test', marker_color='lightsalmon')])\nfig.update_traces(marker_line_color='black', marker_line_width=1.5, opacity=1)\nfig.update_layout(title_text='Unique Values In Each Feature ', \n                  #template='plotly_dark',\n                  paper_bgcolor='rgb(230, 230, 230)',\n                  plot_bgcolor='rgb(230, 230, 230)',\n                  width=750, height=500,\n                  xaxis_title='Features', yaxis_title='Count',\n                  titlefont={'color':'black', 'size': 24, 'family': 'San-Serif'})\nfig.show()","9937e7e4":"targeT = target[0:1000]\nfig = px.histogram(targeT, x=\"target\", \n                   width=600, \n                   height=400,\n                   histnorm='percent',\n                   category_orders={\n                       \"target_class\": [\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\"],                      \n                   },\n                   \n                   template=\"simple_white\"\n                   )\ncolors = ['black',] * 4 \ncolors[3] = 'red' \ncolors[0] = 'seagreen' \n\n\nfig.update_traces(marker_color=colors, marker_line_color='seagreen',\n                  marker_line_width=2.5, opacity=0.5)\n\nfig.update_layout(title=\"<b>Target Class Distribution<b>\", \n                  font_family=\"San Serif\",\n                  titlefont={'size': 24},\n                  paper_bgcolor='lightgray',\n                  plot_bgcolor='lightgray',\n                  legend=dict(\n                  orientation=\"v\", y=1, yanchor=\"top\", x=1.0, xanchor=\"right\" )                 \n                 ).update_xaxes(categoryorder='total descending') # ordering the x-axis values\n\n\nfig.show()","a081b185":"for cat in train.columns:\n    new_cat_train = f'train_{cat}' \n    new_cat_train= list(pd.DataFrame(train[cat].value_counts()\/len(train[cat]))[:10].index)\n    new_cat_test = f'test{cat}' \n    new_cat_test = list(pd.DataFrame(test[cat].value_counts()\/len(test[cat]))[:10].index)\n    train[cat] = np.where(~train[cat].isin(new_cat_train), 'rest', train[cat])\n    test[cat] = np.where(~test[cat].isin(new_cat_test), 'rest', test[cat])","3ed9432a":"high_cardinal_cols= train.columns\na = len(train.columns)\nr = int(np.ceil(a\/2))\ntrain= train[0:10000]\n\ncolors = ['red','blue']\n\nfig = go.Figure()\nfig = make_subplots(\n    horizontal_spacing = 0.12,\n    vertical_spacing = 0.02,\n    rows= r, cols=2,\n    \n     subplot_titles=('feature_0', 'feature_25', 'feature_1', 'feature_26', 'feature_2', 'feature_27', 'feature_3', 'feature_28', 'feature_4', 'feature_29',\n           'feature_5', 'feature_30', 'feature_6', 'feature_31', 'feature_7', 'feature_32', 'feature_8', 'feature_33', 'feature_9', 'feature_34',\n           'feature_10', 'feature_35', 'feature_11', 'feature_36', 'feature_12','feature_37', 'feature_13', 'feature_38', 'feature_14', 'feature_39',\n         'feature_15', 'feature_40', 'feature_16', 'feature_41', 'feature_17',   'feature_42', 'feature_18', 'feature_43', 'feature_19', 'feature_44',\n         'feature_20', 'feature_45', 'feature_21', 'feature_46', 'feature_22',    'feature_47', 'feature_23', 'feature_48', 'feature_24', 'feature_49'))\n\nfor i, cat in enumerate(high_cardinal_cols[0:r]):\n    fig.add_trace(go.Histogram(x=train[cat], \n                          histnorm='percent',\n                          marker_color = 'salmon',showlegend=False, name='Train', legendgroup='group1' ),\n                  \n                  row=i+1, col=1,\n                 )\n    fig.add_trace(go.Histogram(x=test[cat], \n                          histnorm='percent',                                                         \n                          marker_color = 'lightseagreen',showlegend=False, name= 'Test', legendgroup='group2'    ),\n                  \n                  row=i+1, col=1,\n                 )\n\nfor j, cat in enumerate(high_cardinal_cols[r:a+1]):\n    fig.add_trace(go.Histogram(x=train[cat], \n                          histnorm='percent',\n                          marker_color = 'salmon',showlegend=False, name='Train', legendgroup='group1'  ),\n                  \n                  row=j+1, col=2\n                 )\n    fig.add_trace(go.Histogram(x=test[cat], \n                          histnorm='percent',                                                         \n                          marker_color = 'lightseagreen',showlegend=False, name= 'Test', legendgroup='group2'    ),\n                  \n                  row=j+1, col=2,\n                 )\nfig.update_traces(selector=dict(type='histogram'), \n                  marker_line_color='white',\n                 marker_line_width=1.5, \n                  opacity=0.65)\nfig.update_layout(title='<b>All Features: (red: train, green: test) <b>',\n                  bargap=0.1, \n                  paper_bgcolor=\"lightgray\",\n                  #plot_bgcolor=\"lightgray\",\n                  font=dict(color ='black', size=None),\n                  titlefont={'color': 'black', 'size': 28})\nfig.update_layout(template=\"ggplot2\", width=750, height=4600, showlegend=False)\nfig.show()","e8d4d7c9":"train = pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv',  index_col='id')\nclass_1 = train[train['target']=='Class_1']\nclass_2 = train[train['target']=='Class_2']\nclass_3 = train[train['target']=='Class_3']\nclass_4 = train[train['target']=='Class_4']\n\nfig = go.Figure()\nfig.add_trace(go.Violin(x=class_1, line_color='red', opacity=0.5, name='Class_1'))\nfig.add_trace(go.Violin(x=class_2, line_color='seagreen', opacity=0.5, name= 'Class_2'))\nfig.add_trace(go.Violin(x=class_3, line_color='gold', opacity=0.95, name='Class_3'))\nfig.add_trace(go.Violin(x=class_4, line_color='black', opacity=0.5, name= 'Class_4'))\n\nfig.update_traces(orientation='h', width=3, side='positive', meanline_visible=True)\nfig.update_layout(xaxis_showgrid=False, xaxis_zeroline=False)\n\nfig.update_layout(title='<b>Target class distn. <b>',\n                  titlefont={'size': 24},\n                  width=750,\n                  height=550,\n                  template=\"plotly_dark\",\n                  showlegend=True,\n                  paper_bgcolor=\"lightgray\",\n                  plot_bgcolor='lightgray', \n                  font=dict(\n                      color ='black',\n                      )\n                  )\nfig.show()","5890ee87":"features = train.iloc[:, 1:-1].columns.values\nsampled_train = train.sample(5000, random_state=521)\n\numap_2d = UMAP(n_components=2, random_state=521)\numap_3d = UMAP(n_components=3, random_state=521)\n\nproj_2d = umap_2d.fit_transform(sampled_train[features])\nproj_3d = umap_3d.fit_transform(sampled_train[features])\n\nfig_2d = px.scatter(\n    proj_2d, x=0, y=1, \n    labels={'color': 'target'},\n    color=sampled_train.target,\n    color_discrete_sequence=['red', 'seagreen', 'gold', 'white'],\n\n)\nfig_2d.update_layout(title='<b>2D - UMAP<b>',\n                  titlefont={'size': 24},\n                  width=650,\n                  height=650,\n                  template=\"plotly_dark\",\n                  showlegend=True,\n                  xaxis_showgrid=False,\n                  yaxis_showgrid=False,   \n                  paper_bgcolor=\"lightgray\",\n                  plot_bgcolor='lightgray', \n                  font=dict(\n                      color ='black',\n                      )\n                  )\nfig_2d.show()\nfig_3d = px.scatter_3d(\n    proj_3d, x=0, y=1, z=2,\n    labels={'color': 'target'},\n    color=sampled_train.target,\n    color_discrete_sequence=['red', 'seagreen', 'gold', 'white'],\n)\nfig_3d.update_layout(title='<b>3D - UMAP<b>',\n                  titlefont={'size': 24},\n                  width=650,\n                  height=650,\n                  template=\"plotly_dark\",\n                  showlegend=True,\n                  font=dict(\n                      color ='white',\n                      )\n                  )\nfig_3d.update_traces(marker_size=2)\nfig_3d.show()","be850fd1":"train = pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv',  index_col='id')#[0:1000]\nclass_encoder={\"Class_1\":0,\"Class_2\":1,\"Class_3\":2,\"Class_4\":3}\ntrain['target'].replace(class_encoder, inplace=True)\n\ncorr = train.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\ncorr = corr.mask(mask)\n\n\nfig = go.Figure(data= go.Heatmap(z=corr,\n                                 x=corr.index.values,\n                                 y=corr.columns.values,\n                                 colorscale='earth',                                  \n                                 )\n                )\nfig.update_layout(title_text='<b>Correlation Matrix<b>',\n                  title_x=0.5,\n                  titlefont={'size': 24},\n                  width=750, height=750,\n                  xaxis_showgrid=False,\n                  xaxis={'side': 'bottom'},\n                  yaxis_showgrid=False,\n                  yaxis_autorange='reversed',                   \n                  paper_bgcolor='lightgray',\n                  )\nfig.show()","9485abfc":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier","9252366a":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv', index_col='id')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv', index_col='id')\nsample_submission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv')","0a3a3061":"class_encoder={\"Class_1\":0,\"Class_2\":1,\"Class_3\":2,\"Class_4\":3}\ntrain['target'].replace(class_encoder, inplace=True)","e01dcfae":"y = train.pop('target')","16c5d30c":"folds = 7\nSEED = 521","1d4be727":"baseline_model = {'model' : ['catboost','lgbm', 'xgboost'],  'logloss': [1.092725, 1.096940, 1.103021]}\ndf_baseline = pd.DataFrame(data=baseline_model)\n\ndf_ = pd.DataFrame(df_baseline, index=None)\nfig = px.bar(df_, x=\"logloss\", y=\"model\", \n             title=\"<b> Baseline Models Logloss Comparison <b>\",\n            width=750,\n            height=500,)\nfig.update_layout(titlefont={'size':24},\n                  xaxis=dict(range=[1, 1.12])\n                 )\nfig.show()","c9a9d23e":"# ### the baseline model structure\n# ### three models, xbgoost, lgbm and catboost\n# ### uncomment the section to model \n\n# y_oof_pred = np.zeros((train.shape[0], 4))\n# y_test_pred_model = np.zeros((test.shape[0], 4))\n\n# kf = StratifiedKFold(n_splits = folds, shuffle=True, random_state=SEED)\n# for fold, (train_idx, val_idx) in enumerate(kf.split(train, y)):\n    \n#     X_train, X_val = train.iloc[train_idx], train.iloc[val_idx]\n#     y_train, y_val = y.iloc[train_idx], y.iloc[val_idx] \n    \n#     # models\n#     ###### xbgoost #######\n#     model = XGBClassifier(**parms)\n#     model.fit(X_train, y_train,\n#                  eval_set = [(X_train, y_train),(X_val, y_val)],\n#                  verbose = 200)\n#     ######################\n    \n#     ###### lgbm #######\n#     model = LGBMClassifier(random_state=SEED)\n\n#     model.fit(X_train, y_train,\n#                  eval_set = [(X_train, y_train),(X_val, y_val)],\n#                  verbose = 200, early_stopping_rounds=150)\n#     ######################\n    \n#     ###### catboost #####\n#     model = CatBoostClassifier(random_state=SEED)\n#     model.fit(X_train, y_train,\n#                  eval_set = [(X_train, y_train),(X_val, y_val)],\n#                  verbose = 200)\n#     ######################    \n\n#     y_val_pred = model.predict_proba(X_val)\n\n#     print(f\"Fold {fold + 1} Logloss: {log_loss(y_val, y_val_pred)}\")\n\n#     y_oof_pred[val_idx] = y_val_pred\n#     y_test_pred_model += model.predict_proba(test)\n\n\n# y_test_pred_model = y_test_pred_model \/ folds\n\n# print(f\"Overall OOF Logloss: {log_loss(y, y_oof_pred)}\")\n\n# #basic_model.append({'model': 'xgboost', 'logloss': log_loss(y, y_oof_pred)})","2322e631":"xgb_params2 = {'n_estimators': 750, \n'max_depth': 6,  \n'learning_rate': 0.1,\n'gamma': 0.5, \n'reg_lambda': 20, \n'reg_alpha': 10, \n'colsample_bytree': 0.1\n              }\n\nSEED = 2021\nfolds = 7\n\n\ny_oof_pred = np.zeros((train.shape[0], 4))\ny_test_pred_xgb2 = np.zeros((test.shape[0], 4))\n\nkf = StratifiedKFold(n_splits = folds, shuffle= True, random_state=SEED)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(train, y)):\n    \n    X_train, X_val = train.iloc[train_idx], train.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        \n    xgb2 = XGBClassifier(**xgb_params2)\n\n    xgb2.fit(X_train, y_train,\n                 eval_set = [(X_train, y_train),(X_val, y_val)],\n                 verbose = 500, early_stopping_rounds=150)\n\n    y_val_pred = xgb2.predict_proba(X_val)\n\n    print(f\"Fold {fold + 1} Logloss: {log_loss(y_val, y_val_pred)}\")\n\n    y_oof_pred[val_idx] = y_val_pred\n    y_test_pred_xgb2 += xgb2.predict_proba(test)\n\n\ny_test_pred_xgb2= y_test_pred_xgb2\/ folds\n\nprint(f\"-- Overall OOF Logloss: {log_loss(y, y_oof_pred)}\")","978f1c5c":"subm = pd.DataFrame(y_test_pred_xgb2)\nsubm.columns = ['Class_1', 'Class_2','Class_3','Class_4']\nsubm['id'] = sample_submission['id']\nsubm = subm[['id','Class_1', 'Class_2','Class_3','Class_4']]\n\nsubm.to_csv(\"submission_xgb2.csv\", index=False)","33436696":"lgbm_params5 = {'reg_alpha': 10, \n 'reg_lambda': 25, \n 'num_leaves': 72, \n #'learning_rate': 0.02, \n 'max_depth': 3, \n 'n_estimators': 20000, \n 'min_child_samples': 10, \n 'min_child_weight': 0.3, \n 'subsample': 0.5, \n 'colsample_bytree': 0.05}\n\nSEED = 2021\nfolds = 7\n\ny_oof_pred = np.zeros((train.shape[0], 4))\ny_test_pred_lgbm5 = np.zeros((test.shape[0], 4))\n\nkf = StratifiedKFold(n_splits = folds, shuffle= True, random_state=SEED)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(train, y)):\n    \n    X_train, X_val = train.iloc[train_idx], train.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        \n    lgbm5= LGBMClassifier(**lgbm_params5)\n\n    lgbm5.fit(X_train, y_train,\n                 eval_set = [(X_train, y_train),(X_val, y_val)],\n                 verbose = 250, early_stopping_rounds=150)\n\n    y_val_pred = lgbm5.predict_proba(X_val)\n\n    print(f\"Fold {fold + 1} Logloss: {log_loss(y_val, y_val_pred)}\")\n\n    y_oof_pred[val_idx] = y_val_pred\n    y_test_pred_lgbm5 += lgbm5.predict_proba(test)\n\n\ny_test_pred_lgbm5= y_test_pred_lgbm5\/ folds\n\nprint(f\"-- Overall OOF Logloss: {log_loss(y, y_oof_pred)}\")","739d719b":"subm = pd.DataFrame(y_test_pred_lgbm5)\nsubm.columns = ['Class_1', 'Class_2','Class_3','Class_4']\nsubm['id'] = sample_submission['id']\nsubm = subm[['id','Class_1', 'Class_2','Class_3','Class_4']]\n\nsubm.to_csv(\"submission_lgbm5.csv\", index=False)","6b6c233c":"subm = pd.DataFrame(0.5*y_test_pred_lgbm5 + 0.5*y_test_pred_xgb2)\nsubm.columns = ['Class_1', 'Class_2','Class_3','Class_4']\nsubm['id'] = sample_submission['id']\nsubm = subm[['id','Class_1', 'Class_2','Class_3','Class_4']]\n\nsubm.to_csv(\"submission_lgbm5_xgb2_5050.csv\", index=False)","5b81d2bf":"subm = pd.DataFrame(0.3*y_test_pred_lgbm5 + 0.7*y_test_pred_xgb2)\nsubm.columns = ['Class_1', 'Class_2','Class_3','Class_4']\nsubm['id'] = sample_submission['id']\nsubm = subm[['id','Class_1', 'Class_2','Class_3','Class_4']]\n\nsubm.to_csv(\"submission_lgbm5_xgb2_3070.csv\", index=False)","fef86052":"subm = pd.DataFrame(0.15*y_test_pred_lgbm5 + 0.85*y_test_pred_xgb2)\nsubm.columns = ['Class_1', 'Class_2','Class_3','Class_4']\nsubm['id'] = sample_submission['id']\nsubm = subm[['id','Class_1', 'Class_2','Class_3','Class_4']]\n\nsubm.to_csv(\"submission_lgbm5_xgb2_1585.csv\", index=False)","4508dc30":"## Unique values in features\n\n* The unique categories in each feature for train and test data aren't exactly the same. \n* The highest count is 65 for test and 71 for train (feature_38)\n* The fewest count is 4 (feature_13 and feature_36) for both train and test","5fa12647":"## Top 10 categories + the rest grouped together \n\n* In all features only few catagories dominate the feature (category zero stands out) \n* Train-test category split (%) seems to be fairly even (at least there is no huge imbalance)","3870cdf0":"# Introduction:\n\u200b\nStarting from January this year, the kaggle competition team is offering a month-long tabulary playground competitions. This series aims to bridge between inclass competition and featured competitions with a friendly and approachable datasets.\n\u200b\nFor May kaggle is offering a dataset which is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features.\n\nGoal of the competition: Predict **4 classes** give **50 features**.\n\nSubmissions are evaluated using multi-class logarithmic loss. Each row in the dataset has been labeled with one true Class. For each row, you must submit the predicted probabilities that the product belongs to each class label. The formula is:\n\n$ log loss = - \\frac{1}{N} \\sum_{i=1}^{N}\\sum_{j=1}^{M} y_{ij}log(p_{ij}) $.\n\nwhere **$N$** is the number of rows in the test set, **$M$** is the number of class labels, **$log$** is the natural logarithm, **$y_{ij}$** is 1 if observation  is in class  and 0 otherwise, and **$p_{ij}$** is the predicted probability that observation **$i$** belongs to class **$j$**.\n\nNote: The submitted probabilities for a given product are not required to sum to one; they are rescaled prior to being scored, each row is divided by the row sum. In order to avoid the extremes of the  function, predicted probabilities are replaced with.","1280c7a6":"# Part 2: Model\n\n* In this section I will try to train few models starting from simple basic model\n","32a8ebb1":"## Sumbission_lgmb","f5b4506e":"## Submission_xgb","29e17e3e":"## Features with negative values in them\n\n- We notice there are few features which have negative values\n- Train data: Feature_19, 30, 31, 32, 35, 38, 39, and 42 have negative values in them\n- Train data: **Feature_14, 19, 30, 31, 32, 38 and 39** have negative values in them\n> This information might be useful if we would like to change (assume) data type of features later for modeling\n","c0c0a4d4":"## Correlation\n\n* There seem to be little to no correlation between the features or with the target ","737b8b7e":"## Conclusions of EDA \n- Size of train and test data is 100,000 and 50,000 respectively\n- No missing values in both train and test datasets\n- Train and test data are similar in terms of feature distrubutions\n- Feature_38 has the highest unique values with 65 values for test and 71 for train\n- Feature_13 and Feature_36 have the lowest unique values with 4 each for both train and \n- Class_2 dominates the target variable with 57.5% \n- No strong correlation between features or with target variable","413bb88a":"## Target class dist\n\n* Class_2 is the dominant class with 57.5%\n* Class_1 with 8.5% is the least","0d232627":"## Dimensionality reduction as data-vis\n\n### UMAP in 2d and 3d","ce0b9095":"## Baseline model (xgboost, catboost and lgbm)\nI run three baseline models (xgboost, lgmb and catboost) with 5fold stratified split. Resuts are shown below.","9e036f53":"## Models with tuned hyperparameters (first trial)\n\n* First rounds of tuned hyperparameter models ","3493c834":"# Part 1: EDA\n## Load data and explore","9af1b39b":"## Submission_combi"}}