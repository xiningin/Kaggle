{"cell_type":{"77ddccb1":"code","0913eb4f":"code","857c6123":"code","61acdc9f":"code","a3de9e84":"code","b363faad":"code","e1e122cc":"code","90a6eb98":"code","04ce5b83":"code","80e23078":"code","45d35b62":"code","202a488c":"code","473fcfac":"code","cfb42f70":"code","99efdcaf":"code","596293e8":"code","34746cff":"code","f9e93b82":"code","499a50c6":"code","5762be9c":"code","46d27163":"code","716ee301":"code","2025ef42":"code","acfe53c8":"code","1bf036c3":"code","3fc3c56f":"code","d84beec9":"code","e19ce370":"code","aaa7a2ec":"code","7116b060":"code","3b474571":"code","9102194f":"code","ba669f92":"code","89df487a":"code","aaf5f8f7":"code","d8825017":"code","72ac8c38":"code","d08f00f3":"code","31680b54":"code","aa9cfee3":"code","c95bbaa7":"code","fca873c6":"code","52e3ceb3":"code","40464b0b":"code","585fe017":"code","b265ce02":"code","0497e525":"code","88d20bf1":"code","96ecf377":"code","7f416e56":"code","6197dc56":"code","4da51e6c":"code","0a7f6d08":"code","c1deae11":"code","4487515a":"code","1db5724f":"code","dba44e7b":"code","0273f454":"code","dc0912b1":"code","370678e8":"code","19074231":"code","0d1dbcec":"code","6eeeec6e":"code","70195b98":"code","7cf15bc3":"code","a2b8c41b":"code","2482a683":"code","21597441":"code","6360ec80":"code","fad67110":"code","dfd8adc8":"code","cc85dcc9":"code","b1a08b10":"code","423031dc":"code","1509a065":"code","922c159a":"code","ec7377a7":"code","264e4d8c":"code","d7c3c36d":"code","52793797":"code","6c242862":"code","16e3f5e4":"code","e78222bf":"code","608b5e27":"code","fe374e8a":"code","a599211e":"code","f5273711":"code","d386d675":"code","f9e0c18f":"code","11952564":"code","9900ecaf":"code","98b6b1f5":"code","b10c42e1":"code","f22fc985":"code","3536b9c6":"code","f108c5d0":"code","32a7bbf9":"code","cb2ad980":"code","a7265814":"code","0ff8a6f1":"code","6b9fd03d":"code","4ccfa3ab":"code","ab4bcdbe":"code","e1010cbe":"code","7f569b7c":"code","d1f50b9d":"code","f76aa254":"code","eaf87547":"code","9ff434bb":"code","b498eef1":"code","eed0a4a2":"markdown","ccaf4d96":"markdown","cb5c9593":"markdown","f13b0371":"markdown","0ce80fef":"markdown","417cb479":"markdown","cd0b36e1":"markdown","f06fbb5b":"markdown","9eebc6e0":"markdown","cf12762e":"markdown","9e754912":"markdown","cb0c8f65":"markdown","413568a3":"markdown","30c01c9b":"markdown","151b19d5":"markdown","e2afc41c":"markdown","02743cac":"markdown","f5da9771":"markdown","030b4391":"markdown","a4f2d80d":"markdown","0441455a":"markdown","b6f7907b":"markdown","4220933e":"markdown","80ffabca":"markdown","c3602206":"markdown","977f578f":"markdown","b9b7985c":"markdown","cc8eb770":"markdown","32bdf25c":"markdown","ba49de5a":"markdown","d851d963":"markdown","aa9ffd74":"markdown","741df365":"markdown","ae65660d":"markdown","6d1077e5":"markdown","ceb22232":"markdown","35867af5":"markdown","856887ab":"markdown","96995d57":"markdown","7d72a3d2":"markdown","5c1f1b4a":"markdown","306a9397":"markdown","17e007a2":"markdown","05372eaa":"markdown","6e7c4322":"markdown","704a9b0b":"markdown","95ae492a":"markdown","e2032d4b":"markdown","748f58ba":"markdown","7fee9305":"markdown","89485755":"markdown","8581b9c5":"markdown","a9cc14df":"markdown","f735f78e":"markdown","be668c87":"markdown","c9d23ffc":"markdown","bdce05c3":"markdown","66ff54c2":"markdown","b05ebb6f":"markdown","d99674fe":"markdown"},"source":{"77ddccb1":"# Importan Lib\nimport pandas as pd\n# numneric calculation\nimport numpy as np \n\n# Libraries for Visualization\nimport plotly.express as px\nimport matplotlib.pyplot as plt \n# data viz lib\nimport seaborn as sns # not used in this code\n# data viz lib\n\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n# Library for splitting the data in Train and Test\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import KFold, cross_val_score\n\n#Library for getting mutual info\nfrom sklearn.feature_selection import mutual_info_regression\n\n# Library for the metric required to evaluate the model\nfrom sklearn.metrics import r2_score,mean_squared_error, mean_absolute_error, mean_squared_log_error\n#from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix, roc_auc_score\n\n#Library for scaling the data\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n#from sklearn.preprocessing import MinMaxScaler\n\n%matplotlib inline \n# allow to plot the charts inline\n\nimport shap\n# Library required for the Regression Algorithm\nfrom sklearn.tree import DecisionTreeRegressor\nimport xgboost as xgb\nfrom catboost import Pool\nfrom sklearn.svm import SVR\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom mlxtend.regressor import StackingRegressor\nfrom sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nimport os\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")","0913eb4f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","857c6123":"#Reading the data\nos.getcwd()\nTrain_House_Data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\nTest_House_Data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\nTrain_House_Data.columns = Train_House_Data.columns.str.replace(' ', '') # Replacing the white spaces in columns' names\nTest_House_Data.columns = Test_House_Data.columns.str.replace(' ', '') # Replacing the white spaces in columns' names","61acdc9f":"# Checking Number of columns and Rows\nprint(\"train data: {}, test data: {}\".format(Train_House_Data.shape, Test_House_Data.shape))","a3de9e84":"# this is to display all the columns and rows using scroll bars. \npd.set_option('display.max_columns',None)\npd.set_option('display.max_rows',None)","b363faad":"# Getting the main parameters of the Normal Ditribution ()\n(mu, sigma) = norm.fit(Train_House_Data.SalePrice)\n\nplt.figure(figsize = (12,6))\nsns.distplot(Train_House_Data.SalePrice, kde = True, hist=True, fit = norm)\nplt.title('SalePrice distribution vs Normal Distribution', fontsize = 13)\nplt.xlabel(\"House's sale Price in $\", fontsize = 12)\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.show()","e1e122cc":"# Skew and kurt\n#Perform the Shapiro-Wilk test for normality.\n#The Shapiro-Wilk test tests the null hypothesis that the data was drawn from a normal distribution.\n\nfrom scipy import stats\n\nshapiro_test = stats.shapiro(Train_House_Data.SalePrice)\n\nprint(\"Skewness: %f\" % abs(Train_House_Data.SalePrice).skew())\nprint(\"Kurtosis: %f\" % abs(Train_House_Data.SalePrice).kurt())\nprint(\"Shapiro_Test Statistic: %f\" % shapiro_test.statistic)\nprint(\"Shapiro_Test P-Value: %f\" % shapiro_test.pvalue)","90a6eb98":"# Correlation Matrix\n\nf, ax = plt.subplots(figsize=(30, 25))\nmat = Train_House_Data.corr('pearson')\nmask = np.triu(np.ones_like(mat, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0, annot = True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","04ce5b83":"# OverallQuall - SalePrice [Pearson = 0.79]\n# As OverallQual is a categorical value using below plot to viz it\nfigure, ax = plt.subplots(1,3, figsize = (20,8))\nsns.stripplot(data=Train_House_Data, x = 'OverallQual', y='SalePrice', ax = ax[0])\nsns.violinplot(data=Train_House_Data, x = 'OverallQual', y='SalePrice', ax = ax[1])\nsns.boxplot(data=Train_House_Data, x = 'OverallQual', y='SalePrice', ax = ax[2])\nplt.show()","80e23078":"# GrLivArea vs SalePrice [corr = 0.71]\n# As GrLivArea is a continuos value using below plot to viz it\nPearson_GrLiv = 0.71\nplt.figure(figsize = (12,6))\nsns.regplot(data=Train_House_Data, x = 'GrLivArea', y='SalePrice', scatter_kws={'alpha':0.2})\nplt.title('GrLivArea vs SalePrice', fontsize = 12)\nplt.legend(['$Pearson=$ {:.2f}'.format(Pearson_GrLiv)], loc = 'best')\nplt.show()","45d35b62":"# GarageCars - SalePrice [Pearson = 0.64]\n# As GarageCars is a categorical value using below plot to viz it\nfigure, ax = plt.subplots(1,3, figsize = (20,8))\nsns.stripplot(data=Train_House_Data, x = 'GarageCars', y='SalePrice', ax = ax[0])\nsns.violinplot(data=Train_House_Data, x = 'GarageCars', y='SalePrice', ax = ax[1])\nsns.boxplot(data=Train_House_Data, x = 'GarageCars', y='SalePrice', ax = ax[2])\nplt.show()","202a488c":"# GarageArea vs SalePrice [corr = 0.62]\n# As GarageArea is a continuos value using below plot to viz it\nPearson_GrArea = 0.62\nplt.figure(figsize = (12,6))\nsns.regplot(data=Train_House_Data, x = 'GarageArea', y='SalePrice', scatter_kws={'alpha':0.2})\nplt.title('GarageArea vs SalePrice', fontsize = 12)\nplt.legend(['$Pearson=$ {:.2f}'.format(Pearson_GrArea)], loc = 'best')\nplt.show()","473fcfac":"# TotalBsmtSF vs SalePrice [corr = 0.61]\n# As TotalBsmtSF is a continuos value using below plot to viz it\nPearson_TotalBsmtSF = 0.61\nplt.figure(figsize = (12,6))\nsns.regplot(data=Train_House_Data, x = 'TotalBsmtSF', y='SalePrice', scatter_kws={'alpha':0.2})\nplt.title('TotalBsmtSF vs SalePrice', fontsize = 12)\nplt.legend(['$Pearson=$ {:.2f}'.format(Pearson_TotalBsmtSF)], loc = 'best')\nplt.show()","cfb42f70":"# 1stFlrSF vs SalePrice [corr = 0.61]\n# As 1stFlrSF is a continuos value using below plot to viz it\nPearson_TotalBsmtSF = 0.61\nplt.figure(figsize = (12,6))\nsns.regplot(data=Train_House_Data, x = '1stFlrSF', y='SalePrice', scatter_kws={'alpha':0.2})\nplt.title('1stFlrSF vs SalePrice', fontsize = 12)\nplt.legend(['$Pearson=$ {:.2f}'.format(Pearson_TotalBsmtSF)], loc = 'best')\nplt.show()","99efdcaf":"# FullBath - SalePrice [Pearson = 0.56]\n# As FullBath is a categorical value using below plot to viz it\nfigure, ax = plt.subplots(1,3, figsize = (20,8))\nsns.stripplot(data=Train_House_Data, x = 'FullBath', y='SalePrice', ax = ax[0])\nsns.violinplot(data=Train_House_Data, x = 'FullBath', y='SalePrice', ax = ax[1])\nsns.boxplot(data=Train_House_Data, x = 'FullBath', y='SalePrice', ax = ax[2])\nplt.show()","596293e8":"# TotRmsAbvGrd - SalePrice [Pearson = 0.53]\n# As TotRmsAbvGrd is a categorical value using below plot to viz it\nfigure, ax = plt.subplots(1,3, figsize = (20,8))\nsns.stripplot(data=Train_House_Data, x = 'TotRmsAbvGrd', y='SalePrice', ax = ax[0])\nsns.violinplot(data=Train_House_Data, x = 'TotRmsAbvGrd', y='SalePrice', ax = ax[1])\nsns.boxplot(data=Train_House_Data, x = 'TotRmsAbvGrd', y='SalePrice', ax = ax[2])\nplt.show()","34746cff":"# YearBuilt vs SalePrice\n\nPearson_YrBlt = 0.52\nplt.figure(figsize = (12,6))\nsns.regplot(data=Train_House_Data, x = 'YearBuilt', y='SalePrice', scatter_kws={'alpha':0.2})\nplt.title('YearBuilt vs SalePrice', fontsize = 12)\nplt.legend(['$Pearson=$ {:.2f}'.format(Pearson_YrBlt)], loc = 'best')\nplt.show()","f9e93b82":"# Median of Sale Price by Year wrt to year sold\n\nplt.figure(figsize = (10,5))\nsns.barplot(x='YrSold', y=\"SalePrice\", data = Train_House_Data, estimator = np.median)\nplt.title('Median of Sale Price by Year', fontsize = 13)\nplt.xlabel('Selling Year', fontsize = 12)\nplt.ylabel('Median of Price in $', fontsize = 12)\nplt.show()","499a50c6":"#Checking different type of data type in the data\nTrain_House_Data.dtypes.unique()","5762be9c":"# checking info about all columns with data type float64\nTrain_House_Data[[i for i in Train_House_Data.columns if Train_House_Data[i].dtype == 'float64']].info()","46d27163":"# checking info about all columns with data type int64\nTrain_House_Data[[i for i in Train_House_Data.columns if Train_House_Data[i].dtype == 'int64']].info()","716ee301":"# checking info about all columns with data type object\nTrain_House_Data[[i for i in Train_House_Data.columns if Train_House_Data[i].dtype == 'O']].info()","2025ef42":"# making copy of original data frame\nTrain_HD_Final = Train_House_Data.copy()\nTest_HD_Final = Test_House_Data.copy()","acfe53c8":"# Function of Checking for NaN, Missing values\ndef missing_values_table(df):\n    mis_val = df.isnull().sum()\n    mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n    mis_val_table_ren_columns = mis_val_table.rename(\n    columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n    mis_val_table_ren_columns = mis_val_table_ren_columns[\n        mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n    '% of Total Values', ascending=False).round(1)\n    return mis_val_table_ren_columns","1bf036c3":"# Checking missing value in Train data set\nmissing_df = missing_values_table(Train_HD_Final)\nmissing_df","3fc3c56f":"# Plotting Missing value\nplt.figure(figsize = (25,10))\nsns.barplot(x = missing_df.index, y = missing_df['% of Total Values'])\nplt.xticks(rotation=90)\nplt.title('Features containing Missing Values')\nplt.xlabel('Features')\nplt.ylabel('% of Missing Data')\nplt.show()","d84beec9":"# For Training Data Set\n# As per our data and data dictionary, if Bmst value is Not available -> No basement, Garage value is Not available -> No Garage, \n# Alley Value is NA -> No alley Access\nTrain_HD_Final['BsmtQual'] = np.where(Train_HD_Final['BsmtQual'].isnull() == True, \"No Basement\", Train_HD_Final['BsmtQual'])\nTrain_HD_Final['BsmtCond'] = np.where(Train_HD_Final['BsmtCond'].isnull() == True, \"No Basement\", Train_HD_Final['BsmtCond'])\nTrain_HD_Final['BsmtExposure'] = np.where(Train_HD_Final['BsmtExposure'].isnull() == True, \"No Basement\", Train_HD_Final['BsmtExposure'])\nTrain_HD_Final['BsmtFinType1'] = np.where(Train_HD_Final['BsmtFinType1'].isnull() == True, \"No Basement\", Train_HD_Final['BsmtFinType1'])\nTrain_HD_Final['BsmtFinType2'] = np.where(Train_HD_Final['BsmtFinType2'].isnull() == True, \"No Basement\", Train_HD_Final['BsmtFinType2'])\n#--------------------\nTrain_HD_Final['Alley'] = np.where(Train_HD_Final['Alley'].isnull() == True, \"No Alley Access\", Train_HD_Final['Alley'])\n#--------------------\nTrain_HD_Final['GarageType'] = np.where(Train_HD_Final['GarageType'].isnull() == True, \"No Garage\", Train_HD_Final['GarageType'])\nTrain_HD_Final['GarageYrBlt'] = np.where(Train_HD_Final['GarageYrBlt'].isnull() == True, \"No Garage\", Train_HD_Final['GarageYrBlt'])\nTrain_HD_Final['GarageFinish'] = np.where(Train_HD_Final['GarageFinish'].isnull() == True, \"No Garage\", Train_HD_Final['GarageFinish'])\nTrain_HD_Final['GarageQual'] = np.where(Train_HD_Final['GarageQual'].isnull() == True, \"No Garage\", Train_HD_Final['GarageQual'])\nTrain_HD_Final['GarageCond'] = np.where(Train_HD_Final['GarageCond'].isnull() == True, \"No Garage\", Train_HD_Final['GarageCond'])\n#No Fireplace\nTrain_HD_Final['FireplaceQu'] = np.where(Train_HD_Final['FireplaceQu'].isnull() == True, \"No Fireplace\", Train_HD_Final['FireplaceQu'])\n#No Fence\nTrain_HD_Final['Fence'] = np.where(Train_HD_Final['Fence'].isnull() == True, \"No Fence\", Train_HD_Final['Fence'])\n#No Pool\nTrain_HD_Final['PoolQC'] = np.where(Train_HD_Final['PoolQC'].isnull() == True, \"No Pool\", Train_HD_Final['PoolQC'])\n#MiscFeature: None\nTrain_HD_Final['MiscFeature'] = np.where(Train_HD_Final['MiscFeature'].isnull() == True, \"None\", Train_HD_Final['MiscFeature'])","e19ce370":"# As ID do not have any value in Prediction , we are removing it \nTrain_HD_Final = Train_HD_Final.drop(['Id'], axis =1)\nTrain_HD_Final.shape","aaa7a2ec":"# In case the no. of column is very high, just check for columns which have any null value\n# Checking missing value in Train data set\nmissing_values_table(Train_HD_Final)","7116b060":"#Checking the data Type of columns with  Null values for appropiate missing value treatment\n(Train_HD_Final[[i for i in Train_HD_Final.columns if Train_HD_Final[i].isnull().sum()>0]].info())","3b474571":"Train_HD_Final.Electrical.value_counts()\n# As SBrkr is the maximum by far, we will replace the null value with it. \nTrain_HD_Final.Electrical.replace(np.NaN, 'SBrkr', inplace=True)\nTrain_HD_Final.Electrical.value_counts()","9102194f":"# for MasVnrType (8 Missing Values) getting %\nprint(Train_HD_Final.MasVnrType.value_counts())\nround(Train_HD_Final.MasVnrType.value_counts()* 100\/Train_HD_Final.MasVnrType.notnull().sum())","ba669f92":"# Dividing the 8 missing values in same propertion\nprint (round(8 *.6))\nprint (round(8 *.31))\nprint (round(8 *.09))\nprint (round(8 *.01))","89df487a":"# So updating 5 Null values as None, 2 as BrkFace ans 1 as Stone\n#Getting the index for Null Value\nlist_index = Train_HD_Final[Train_HD_Final['MasVnrType'].isnull()].index.tolist()\n\nk = 0\nfor i in list_index:\n    k= k+1\n    if k < 6:\n        Train_HD_Final.MasVnrType[i] = 'None'\n    elif k < 8:\n        Train_HD_Final.MasVnrType[i] = 'BrkFace'\n    else:\n        Train_HD_Final.MasVnrType[i] = 'Stone'\n\nprint(Train_HD_Final.MasVnrType.value_counts())\nprint(round(Train_HD_Final.MasVnrType.value_counts()* 100\/Train_HD_Final.MasVnrType.notnull().sum()))","aaf5f8f7":"#as the missing value is very very less we can replace it by mean\nTrain_HD_Final.MasVnrArea = Train_HD_Final.MasVnrArea.fillna(Train_HD_Final.MasVnrArea.mean())","d8825017":"# Encoding all the columns with categorical string value for prediction model\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfor i in Train_HD_Final.columns:\n    if Train_HD_Final[i].dtype == 'O':        \n        Train_HD_Final[i] = le.fit_transform(Train_HD_Final[i].values)","72ac8c38":"# Checking if any of the column is too much or less correlated\ncols = ['Name', 'Corr']\nlst = []\nfor i in Train_HD_Final.columns:\n    lst.append([i, round(abs(Train_HD_Final['LotFrontage'].corr(Train_HD_Final[i])),3)])\n    \ndf1 = pd.DataFrame(lst, columns=cols)\ndf1 = df1.sort_values(ascending= False, by='Corr')\ndf1.head (20)\n#df1.tail(10)","d08f00f3":"#Dividing the data into train (with Lot Frontage data available only columns) and test ( columns where LF = null) datasets \ntrain_LF_main = Train_HD_Final[Train_HD_Final.LotFrontage.isnull() != True]\ntest_LF_main = Train_HD_Final[Train_HD_Final.LotFrontage.isnull() == True]\n# Checking Number of columns and Rows\nprint(\"train data of LotFrontage: {}, test data of LotFrontage: {}\".format(train_LF_main.shape, test_LF_main.shape))","31680b54":"#-------------------------\ntrain_LF_xtrain = train_LF_main.drop('LotFrontage', axis = 1)\ntrain_LF_ytrain = train_LF_main['LotFrontage']\n#-------------------------\ntest_LF = test_LF_main.drop('LotFrontage', axis = 1)\n#-------------------------\n#By using Random Forest Algorithm I replaced the missing values of LotFrontage with predicted values\nreg_rf = RandomForestRegressor(n_estimators=1000,min_samples_split=2,min_samples_leaf=1,max_features='sqrt',max_depth=25)\nreg_rf.fit(train_LF_xtrain, train_LF_ytrain)\ny_pred= reg_rf.predict(test_LF)\ntest_LF_main['LotFrontage'] = y_pred\nprint(\"Accuracy on Training set: \",reg_rf.score(train_LF_xtrain,train_LF_ytrain))","aa9cfee3":"Train_HD_Final = test_LF_main.append(train_LF_main).sort_index()\nprint(Train_HD_Final[[i for i in Train_HD_Final.columns if Train_HD_Final[i].isnull().sum()>0]].isnull().sum())\nTrain_HD_Final.head()","c95bbaa7":"Train_HD_Final[\"SqFtPerRoom\"] = Train_HD_Final[\"GrLivArea\"] \/ (Train_HD_Final[\"TotRmsAbvGrd\"] +\n                                                       Train_HD_Final[\"FullBath\"] +\n                                                       Train_HD_Final[\"HalfBath\"] +\n                                                       Train_HD_Final[\"KitchenAbvGr\"])\n\nTrain_HD_Final['Total_Home_Quality'] = Train_HD_Final['OverallQual'] + Train_HD_Final['OverallCond']\n\nTrain_HD_Final['Total_Bathrooms'] = (Train_HD_Final['FullBath'] + (0.5 * Train_HD_Final['HalfBath']) +\n                               Train_HD_Final['BsmtFullBath'] + (0.5 * Train_HD_Final['BsmtHalfBath']))\n\nTrain_HD_Final[\"HighQualSF\"] = Train_HD_Final[\"1stFlrSF\"] + Train_HD_Final[\"2ndFlrSF\"]\nTrain_HD_Final.shape","fca873c6":"# SalePrice before transformation\n\nfig, ax = plt.subplots(1,2, figsize= (15,5))\nfig.suptitle(\" qq-plot & distribution SalePrice \", fontsize= 15)\n\nsm.qqplot(Train_HD_Final.SalePrice, stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[0])\n\nsns.distplot(Train_HD_Final.SalePrice, kde = True, hist=True, fit = norm, ax = ax[1])\nplt.show()","52e3ceb3":"# SalePrice after transformation\n\nTrain_HD_Final.SalePrice = np.log1p(Train_HD_Final.SalePrice)\n\nfig, ax = plt.subplots(1,2, figsize= (15,5))\nfig.suptitle(\" qq-plot & distribution SalePrice \", fontsize= 15)\n\nsm.qqplot(Train_HD_Final.SalePrice, stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[0])\n\nsns.distplot(Train_HD_Final.SalePrice, kde = True, hist=True, fit = norm, ax = ax[1])\nplt.show()","40464b0b":"from scipy.stats import skew, norm\n# Checking skewed value of all the features and transforming them\nnumeric_features = Train_HD_Final.dtypes[Train_HD_Final.dtypes != object].index\nskewed_features = Train_HD_Final[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_features[skewed_features > 0.5]\nskew_index = high_skew.index\n\n# Normalize skewed features using log_transformation\n    \nfor i in skew_index:\n    Train_HD_Final[i] = np.log1p(Train_HD_Final[i])","585fe017":"#Checking the columns with Missing Value\nmissing_values_table(Test_HD_Final)","b265ce02":"# For Test Data Set\n# As per our data and data dictionary, if Bmst value is Not available -> No basement, Garage value is Not available -> No Garage, \n# Alley Value is NA -> No alley Access\nTest_HD_Final['BsmtQual'] = np.where(Test_HD_Final['BsmtQual'].isnull() == True, \"No Basement\", Test_HD_Final['BsmtQual'])\nTest_HD_Final['BsmtCond'] = np.where(Test_HD_Final['BsmtCond'].isnull() == True, \"No Basement\", Test_HD_Final['BsmtCond'])\nTest_HD_Final['BsmtExposure'] = np.where(Test_HD_Final['BsmtExposure'].isnull() == True, \"No Basement\", Test_HD_Final['BsmtExposure'])\nTest_HD_Final['BsmtFinType1'] = np.where(Test_HD_Final['BsmtFinType1'].isnull() == True, \"No Basement\", Test_HD_Final['BsmtFinType1'])\nTest_HD_Final['BsmtFinType2'] = np.where(Test_HD_Final['BsmtFinType2'].isnull() == True, \"No Basement\", Test_HD_Final['BsmtFinType2'])\nTest_HD_Final['BsmtFullBath'] = np.where(Test_HD_Final['BsmtFullBath'].isnull() == True, \"No Basement\", Test_HD_Final['BsmtFullBath'])\nTest_HD_Final['BsmtHalfBath'] = np.where(Test_HD_Final['BsmtHalfBath'].isnull() == True, \"No Basement\", Test_HD_Final['BsmtHalfBath'])\nTest_HD_Final['BsmtUnfSF'] = np.where(Test_HD_Final['BsmtUnfSF'].isnull() == True, \"No Basement\", Test_HD_Final['BsmtUnfSF'])\nTest_HD_Final['TotalBsmtSF'] = np.where(Test_HD_Final['TotalBsmtSF'].isnull() == True, \"No Basement\", Test_HD_Final['TotalBsmtSF'])\nTest_HD_Final['BsmtFinSF2'] = np.where(Test_HD_Final['BsmtFinSF2'].isnull() == True, \"No Basement\", Test_HD_Final['BsmtFinSF2'])\nTest_HD_Final['BsmtFinSF1'] = np.where(Test_HD_Final['BsmtFinSF1'].isnull() == True, \"No Basement\", Test_HD_Final['BsmtFinSF1'])\n#--------------------\nTest_HD_Final['Alley'] = np.where(Test_HD_Final['Alley'].isnull() == True, \"No Alley Access\", Test_HD_Final['Alley'])\n#--------------------\nTest_HD_Final['GarageType'] = np.where(Test_HD_Final['GarageType'].isnull() == True, \"No Garage\", Test_HD_Final['GarageType'])\nTest_HD_Final['GarageYrBlt'] = np.where(Test_HD_Final['GarageYrBlt'].isnull() == True, \"No Garage\", Test_HD_Final['GarageYrBlt'])\nTest_HD_Final['GarageFinish'] = np.where(Test_HD_Final['GarageFinish'].isnull() == True, \"No Garage\", Test_HD_Final['GarageFinish'])\nTest_HD_Final['GarageQual'] = np.where(Test_HD_Final['GarageQual'].isnull() == True, \"No Garage\", Test_HD_Final['GarageQual'])\nTest_HD_Final['GarageCond'] = np.where(Test_HD_Final['GarageCond'].isnull() == True, \"No Garage\", Test_HD_Final['GarageCond'])\nTest_HD_Final['GarageArea'] = np.where(Test_HD_Final['GarageArea'].isnull() == True, \"No Garage\", Test_HD_Final['GarageArea'])\nTest_HD_Final['GarageCars'] = np.where(Test_HD_Final['GarageCars'].isnull() == True, \"No Garage\", Test_HD_Final['GarageCars'])\n#No Fireplace\nTest_HD_Final['FireplaceQu'] = np.where(Test_HD_Final['FireplaceQu'].isnull() == True, \"No Fireplace\", Test_HD_Final['FireplaceQu'])\n#No Fence\nTest_HD_Final['Fence'] = np.where(Test_HD_Final['Fence'].isnull() == True, \"No Fence\", Test_HD_Final['Fence'])\n#No Pool\nTest_HD_Final['PoolQC'] = np.where(Test_HD_Final['PoolQC'].isnull() == True, \"No Pool\", Test_HD_Final['PoolQC'])\n#MiscFeature: None\nTest_HD_Final['MiscFeature'] = np.where(Test_HD_Final['MiscFeature'].isnull() == True, \"None\", Test_HD_Final['MiscFeature'])","0497e525":"#Checking the remaining columns with Missing Value\nmissing_values_table(Test_HD_Final)","88d20bf1":"#to remove same col from test data \/ # As ID do not have any value in Prediction , we are removing it \nTest_HD_Final = Test_HD_Final.drop(['Id'], axis =1)\nTest_HD_Final.shape","96ecf377":"#Checking the data Type of columns with  Null values for appropiate missing value treatment\n(Test_HD_Final[[i for i in Test_HD_Final.columns if Test_HD_Final[i].isnull().sum()>0]].info())","7f416e56":"# for MasVnrType (8 Missing Values) getting %\nprint(Test_HD_Final.MasVnrType.value_counts())\nround(Test_HD_Final.MasVnrType.value_counts()* 100\/Test_HD_Final.MasVnrType.notnull().sum())","6197dc56":"# Dividing the 16 missing values in same propertion\nprint (round(16 *.61))\nprint (round(16 *.3))\nprint (round(16 *.09))\nprint (round(16 *.01))","4da51e6c":"# So updating 10 Null values as None, 5 as BrkFace ans 1 as Stone\n#Getting the index for Null Value\nlist_index = Test_HD_Final[Test_HD_Final['MasVnrType'].isnull()].index.tolist()\n\nk = 0\nfor i in list_index:\n    k= k+1\n    if k < 11:\n        Test_HD_Final.MasVnrType[i] = 'None'\n    elif k < 16:\n        Test_HD_Final.MasVnrType[i] = 'BrkFace'\n    else:\n        Test_HD_Final.MasVnrType[i] = 'Stone'\n\nprint(Test_HD_Final.MasVnrType.value_counts())\nprint(round(Test_HD_Final.MasVnrType.value_counts()* 100\/Test_HD_Final.MasVnrType.notnull().sum()))","0a7f6d08":"#as the missing value is very very less we can replace it by mean\nTest_HD_Final.MasVnrArea = Test_HD_Final.MasVnrArea.fillna(Test_HD_Final.MasVnrArea.mean())","c1deae11":"# for MSZoning (4 Missing Values) getting %\nprint(Test_HD_Final.MSZoning.value_counts())\nround(Test_HD_Final.MSZoning.value_counts()* 100\/Test_HD_Final.MSZoning.notnull().sum())","4487515a":"# As RL is by far the maximum, so replacing it for null value. (this will not change the distribuion)\nTest_HD_Final['MSZoning'] = np.where(Test_HD_Final['MSZoning'].isnull() == True, \n                                     \"RL\", Test_HD_Final['MSZoning'])","1db5724f":"# for Utilities (2 Missing Values) getting %\nprint(Test_HD_Final.Utilities.value_counts())\nround(Test_HD_Final.Utilities.value_counts()* 100\/Test_HD_Final.Utilities.notnull().sum())","dba44e7b":"# As all the value are AllPub, so replacing it for null value. (this will not change the distribuion)\nTest_HD_Final['Utilities'] = np.where(Test_HD_Final['Utilities'].isnull() == True, \n                                     \"AllPub\", Test_HD_Final['Utilities'])","0273f454":"# for Functional (2 Missing Values) getting %\nprint(Test_HD_Final.Functional.value_counts())\nround(Test_HD_Final.Functional.value_counts()* 100\/Test_HD_Final.Functional.notnull().sum())","dc0912b1":"# As Typ is by far the maximum, so replacing it for null value. (this will not change the distribuion)\nTest_HD_Final['Functional'] = np.where(Test_HD_Final['Functional'].isnull() == True, \n                                     \"Typ\", Test_HD_Final['Functional'])","370678e8":"# for Exterior1st (1 Missing Values) getting %\nprint(Test_HD_Final.Exterior1st.value_counts())\nround(Test_HD_Final.Exterior1st.value_counts()* 100\/Test_HD_Final.Exterior1st.notnull().sum())","19074231":"# As VinylSd is by far the maximum and only 1 missing value in this, so replacing it for null value.\n# (this will not change the distribuion)\nTest_HD_Final['Exterior1st'] = np.where(Test_HD_Final['Exterior1st'].isnull() == True, \n                                     \"VinylSd\", Test_HD_Final['Exterior1st'])","0d1dbcec":"# for Exterior2nd (1 Missing Values) getting %\nprint(Test_HD_Final.Exterior2nd.value_counts())\nround(Test_HD_Final.Exterior2nd.value_counts()* 100\/Test_HD_Final.Exterior2nd.notnull().sum())","6eeeec6e":"# As VinylSd is by far the maximum and only 1 missing value in this, so replacing it for null value.\n# (this will not change the distribuion)\nTest_HD_Final['Exterior2nd'] = np.where(Test_HD_Final['Exterior2nd'].isnull() == True, \n                                     \"VinylSd\", Test_HD_Final['Exterior2nd'])","70195b98":"# for KitchenQual (1 Missing Values) getting %\nprint(Test_HD_Final.KitchenQual.value_counts())\nround(Test_HD_Final.KitchenQual.value_counts()* 100\/Test_HD_Final.KitchenQual.notnull().sum())","7cf15bc3":"# As TA is by far the maximum and only 1 missing value in this, so replacing it for null value.\n# (this will not change the distribuion)\nTest_HD_Final['KitchenQual'] = np.where(Test_HD_Final['KitchenQual'].isnull() == True, \n                                     \"TA\", Test_HD_Final['KitchenQual'])","a2b8c41b":"# for SaleType (1 Missing Values) getting %\nprint(Test_HD_Final.SaleType.value_counts())\nround(Test_HD_Final.SaleType.value_counts()* 100\/Test_HD_Final.SaleType.notnull().sum())","2482a683":"# As WD is by far the maximum and only 1 missing value in this, so replacing it for null value.\n# (this will not change the distribuion)\nTest_HD_Final['SaleType'] = np.where(Test_HD_Final['SaleType'].isnull() == True, \n                                     \"WD\", Test_HD_Final['SaleType'])","21597441":"# Encoding all the columns with categorical string value for prediction model\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfor i in Test_HD_Final.columns:\n    if Test_HD_Final[i].dtype == 'O':        \n        Test_HD_Final[i] = le.fit_transform(Test_HD_Final[i].values)","6360ec80":"train_LF_main_test_data = Test_HD_Final[Test_HD_Final.LotFrontage.isnull() != True]\ntest_LF_main_test_data = Test_HD_Final[Test_HD_Final.LotFrontage.isnull() == True]\n# Checking Number of columns and Rows\nprint(\"train data of Test DataSet for LotFrontage: {}, test data of Test DataSet for LotFrontage: {}\"\n      .format(train_LF_main.shape, test_LF_main.shape))","fad67110":"#--------------------\ntrain_LF_xtrain_test_data = train_LF_main_test_data.drop('LotFrontage', axis = 1)\ntrain_LF_ytrain_test_data = train_LF_main_test_data['LotFrontage']\n#--------------------\ntest_LF_test_data = test_LF_main_test_data.drop('LotFrontage', axis = 1)\n#-------------------------\n#By using Random Forest Algorithm I replaced the missing values of LotFrontage with predicted values\nreg_rf_test_data = RandomForestRegressor(n_estimators=1000,min_samples_split=2,\n                                         min_samples_leaf=1,max_features='sqrt',max_depth=25)\nreg_rf_test_data.fit(train_LF_xtrain_test_data, train_LF_ytrain_test_data)\ny_pred_test_data= reg_rf_test_data.predict(test_LF_test_data)\ntest_LF_main_test_data['LotFrontage'] = y_pred_test_data\nprint(\"Accuracy on Training set: \",reg_rf_test_data.score(train_LF_xtrain_test_data,train_LF_ytrain_test_data))","dfd8adc8":"Test_HD_Final = test_LF_main_test_data.append(train_LF_main_test_data).sort_index()","cc85dcc9":"print(Test_HD_Final[[i for i in Test_HD_Final.columns if Test_HD_Final[i].isnull().sum()>0]].isnull().sum())\nTest_HD_Final.head(3)","b1a08b10":"Test_HD_Final[\"SqFtPerRoom\"] = Test_HD_Final[\"GrLivArea\"] \/ (Test_HD_Final[\"TotRmsAbvGrd\"] +\n                                                       Test_HD_Final[\"FullBath\"] +\n                                                       Test_HD_Final[\"HalfBath\"] +\n                                                       Test_HD_Final[\"KitchenAbvGr\"])\n\nTest_HD_Final['Total_Home_Quality'] = Test_HD_Final['OverallQual'] + Test_HD_Final['OverallCond']\n\nTest_HD_Final['Total_Bathrooms'] = (Test_HD_Final['FullBath'] + (0.5 * Test_HD_Final['HalfBath']) +\n                               Test_HD_Final['BsmtFullBath'] + (0.5 * Test_HD_Final['BsmtHalfBath']))\n\nTest_HD_Final[\"HighQualSF\"] = Test_HD_Final[\"1stFlrSF\"] + Test_HD_Final[\"2ndFlrSF\"]\nTest_HD_Final.shape","423031dc":"# Checking skewed value of all the features and transforming them in test data too\nnumeric_features = Test_HD_Final.dtypes[Test_HD_Final.dtypes != object].index\nskewed_features = Test_HD_Final[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_features[skewed_features > 0.5]\nskew_index = high_skew.index\n\n# Normalize skewed features using log_transformation\n    \nfor i in skew_index:\n    Test_HD_Final[i] = np.log1p(Test_HD_Final[i])","1509a065":"#checking duplicate indexes\n# to check if any duplicate rows are present in the DF. \nTrain_HD_Final.duplicated().sum()","922c159a":"# Checking for infinites only at end after converting all to int\nnp.isinf(Train_HD_Final).values.sum()","ec7377a7":"X = Train_HD_Final.drop('SalePrice', axis = 1)\nY = Train_HD_Final['SalePrice']\n# Divide the data into test and train data set\nX_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.20,random_state=1)\n# Taking the whole data to train the model\n#X_train = X\n#y_train = Y","264e4d8c":"# Checking Number of columns and Rows\nprint(\"Train X data: {}, Train Y data : {}\".format(X_train.shape, y_train.shape))\nprint('--------------------------------------------------')\nprint(\"Test X data: {}, Test Y data : {}\".format(X_test.shape, y_test.shape))","d7c3c36d":"Standardscaler = StandardScaler()\nX_train_column = X_train.columns\nX_train_SS = pd.DataFrame(Standardscaler.fit_transform(X_train),columns = X_train_column )\nX_train_SS.head(2)","52793797":"mutual_info = mutual_info_regression(X_train_SS.fillna(0), y_train)\nmutual_info = pd.Series(mutual_info)\nmutual_info.index = X_train_SS.columns\nmutual_info.sort_values(ascending=False)","6c242862":"#Considering the columns for training the model which are atleast 10% of information shared with dependent variable\/feature\nReq_Columns = list(mutual_info[mutual_info>0].index)\nReq_Columns","16e3f5e4":"Train_SS = X_train_SS[Req_Columns]\nTrain_SS.head(3)","e78222bf":"#Applying Scaling technique to Test Dataset\nStandardscaler = StandardScaler()\nX_test_columns = X_test.columns\nX_test_SS = pd.DataFrame(Standardscaler.fit_transform(X_test),columns = X_test_columns )\n#--------------------\n#Creating the Testing ADS with selected columns\nTest_SS = X_test_SS[Req_Columns]\nprint(Test_SS.shape)\nTest_SS.head(3)","608b5e27":"def rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n#cross validation dataset check\ndef cv_rmse(model):\n    rmse = np.sqrt(-cross_val_score(model, Train_SS, y_train, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)\n\ndef calc_r2Score(model):\n    model = model.fit(Train_SS, y_train)\n    # Predict the data\n    y_pred = model.predict(Test_SS)\n    return (r2_score(y_test, y_pred))","fe374e8a":"# 10 Fold Cross validation\n\nkf = KFold(n_splits=10, random_state=42, shuffle=True)\n\ncv_scores = []\ncv_std = []\nr2_scr = []\nbaseline_models = ['Decision Tree Regressor','Linear Regression','KNeighbors Regressor','RandomForest Regressor',\n                   'XGBoost Regressor','Support Vector Regressor', 'Bayesian Ridge Regression',\n                   'Light Gradient Boost Regressor','Gradient Boosting Regressor','Cat Boost Regressor','Stacked Regressor']","a599211e":"np.random.seed(42)\ndecTree_reg = DecisionTreeRegressor()\n\nscore_dtr = cv_rmse(decTree_reg)\ncv_scores.append(round(score_dtr.mean(),4))\ncv_std.append(round(score_dtr.std(),4))\n\nr2_score_dtr = round(calc_r2Score(decTree_reg)*100,2)\nr2_scr.append(r2_score_dtr)\nprint(\"Accuracy on Training set: \",decTree_reg.score(Train_SS,y_train))\nprint(\"Accuracy on Testing set: \",decTree_reg.score(Test_SS,y_test))\nprint(\"Mean root mean sqr error: \",round(score_dtr.mean(),4))\nprint(\"std deviation root mean sqr error: \",round(score_dtr.std(),4))\nprint(\"R2 score\", r2_score_dtr)","f5273711":"np.random.seed(42)\nlinear_reg = LinearRegression()\n\nscore_LnR = cv_rmse(linear_reg)\ncv_scores.append(round(score_LnR.mean(),4))\ncv_std.append(round(score_LnR.std(),4))\n\nr2_score_LnR = round(calc_r2Score(linear_reg)*100,2)\nr2_scr.append(r2_score_LnR)\nprint(\"Accuracy on Training set: \",linear_reg.score(Train_SS,y_train))\nprint(\"Accuracy on Testing set: \",linear_reg.score(Test_SS,y_test))\nprint(\"Mean root mean sqr error: \",round(score_LnR.mean(),4))\nprint(\"std deviation root mean sqr error: \",round(score_LnR.std(),4))\nprint(\"R2 score\", r2_score_LnR)","d386d675":"np.random.seed(42)\nKneigh_reg = KNeighborsRegressor(n_neighbors=2)\nscore_KnR = cv_rmse(Kneigh_reg)\ncv_scores.append(round(score_KnR.mean(),4))\ncv_std.append(round(score_KnR.std(),4))\n\nr2_score_KnR = round(calc_r2Score(Kneigh_reg)*100,2)\nr2_scr.append(r2_score_KnR)\nprint(\"Accuracy on Training set: \",Kneigh_reg.score(Train_SS,y_train))\nprint(\"Accuracy on Testing set: \",Kneigh_reg.score(Test_SS,y_test))\nprint(\"Mean root mean sqr error: \",round(score_KnR.mean(),4))\nprint(\"std deviation root mean sqr error: \",round(score_KnR.std(),4))\nprint(\"R2 score\", r2_score_KnR)","f9e0c18f":"np.random.seed(42)\nreg_rfr = RandomForestRegressor()\nscore_rfr = cv_rmse(reg_rfr)\ncv_scores.append(round(score_rfr.mean(),4))\ncv_std.append(round(score_rfr.std(),4))\n\nr2_score_rfr = round(calc_r2Score(reg_rfr)*100,2)\nr2_scr.append(r2_score_rfr)\nprint(\"Accuracy on Training set: \",reg_rfr.score(Train_SS,y_train))\nprint(\"Accuracy on Testing set: \",reg_rfr.score(Test_SS,y_test))\nprint(\"Mean root mean sqr error: \",round(score_rfr.mean(),4))\nprint(\"std deviation root mean sqr error: \",round(score_rfr.std(),4))\nprint(\"R2 score\", r2_score_rfr)","11952564":"np.random.seed(42)\nxgb_model = xgb.XGBRegressor()\nscore_xgb = cv_rmse(xgb_model)\ncv_scores.append(round(score_xgb.mean(),4))\ncv_std.append(round(score_xgb.std(),4))\n\nr2_score_xgb = round(calc_r2Score(xgb_model)*100,2)\nr2_scr.append(r2_score_xgb)\nprint(\"Accuracy on Training set: \",xgb_model.score(Train_SS,y_train))\nprint(\"Accuracy on Testing set: \",xgb_model.score(Test_SS,y_test))\nprint(\"Mean root mean sqr error: \",round(score_xgb.mean(),4))\nprint(\"std deviation root mean sqr error: \",round(score_xgb.std(),4))\nprint(\"R2 score\", r2_score_xgb)","9900ecaf":"np.random.seed(42)\nSVR_model=SVR()\nscore_SVR = cv_rmse(SVR_model)\ncv_scores.append(round(score_SVR.mean(),4))\ncv_std.append(round(score_SVR.std(),4))\n\nr2_score_SVR = round(calc_r2Score(SVR_model)*100,2)\nr2_scr.append(r2_score_SVR)\nprint(\"Accuracy on Training set: \",SVR_model.score(Train_SS,y_train))\nprint(\"Accuracy on Testing set: \",SVR_model.score(Test_SS,y_test))\nprint(\"Mean root mean sqr error: \",round(score_SVR.mean(),4))\nprint(\"std deviation root mean sqr error: \",round(score_SVR.std(),4))\nprint(\"R2 score\", r2_score_SVR)","98b6b1f5":"np.random.seed(42)\nBRR_model=BayesianRidge(compute_score=True)\nscore_BRR = cv_rmse(BRR_model)\ncv_scores.append(round(score_BRR.mean(),4))\ncv_std.append(round(score_BRR.std(),4))\n\nr2_score_BRR = round(calc_r2Score(BRR_model)*100,2)\nr2_scr.append(r2_score_BRR)\nprint(\"Accuracy on Training set: \",BRR_model.score(Train_SS,y_train))\nprint(\"Accuracy on Testing set: \",BRR_model.score(Test_SS,y_test))\nprint(\"Mean root mean sqr error: \",round(score_BRR.mean(),4))\nprint(\"std deviation root mean sqr error: \",round(score_BRR.std(),4))\nprint(\"R2 score\", r2_score_BRR)","b10c42e1":"np.random.seed(42)\nLGBMR_model=LGBMRegressor(objective='regression')\nscore_LGBMR = cv_rmse(LGBMR_model)\ncv_scores.append(round(score_LGBMR.mean(),4))\ncv_std.append(round(score_LGBMR.std(),4))\n\nr2_score_LGBMR = round(calc_r2Score(LGBMR_model)*100,2)\nr2_scr.append(r2_score_LGBMR)\nprint(\"Accuracy on Training set: \",LGBMR_model.score(Train_SS,y_train))\nprint(\"Accuracy on Testing set: \",LGBMR_model.score(Test_SS,y_test))\nprint(\"Mean root mean sqr error: \",round(score_LGBMR.mean(),4))\nprint(\"std deviation root mean sqr error: \",round(score_LGBMR.std(),4))\nprint(\"R2 score\", r2_score_LGBMR)","f22fc985":"np.random.seed(42)\nGBR_model=GradientBoostingRegressor()\nscore_GBR = cv_rmse(GBR_model)\ncv_scores.append(round(score_GBR.mean(),4))\ncv_std.append(round(score_GBR.std(),4))\n\nr2_score_GBR = round(calc_r2Score(GBR_model)*100,2)\nr2_scr.append(r2_score_GBR)\nprint(\"Accuracy on Training set: \",GBR_model.score(Train_SS,y_train))\nprint(\"Accuracy on Testing set: \",GBR_model.score(Test_SS,y_test))\nprint(\"Mean root mean sqr error: \",round(score_GBR.mean(),4))\nprint(\"std deviation root mean sqr error: \",round(score_GBR.std(),4))\nprint(\"R2 score\", r2_score_GBR)","3536b9c6":"np.random.seed(42)\nCBR_model=CatBoostRegressor()\nscore_CBR = cv_rmse(CBR_model)\ncv_scores.append(round(score_CBR.mean(),4))\ncv_std.append(round(score_CBR.std(),4))\n\nr2_score_CBR = round(calc_r2Score(CBR_model)*100,2)\nr2_scr.append(r2_score_CBR)\nprint(\"Accuracy on Training set: \",CBR_model.score(Train_SS,y_train))\nprint(\"Accuracy on Testing set: \",CBR_model.score(Test_SS,y_test))\nprint(\"Mean root mean sqr error: \",round(score_CBR.mean(),4))\nprint(\"std deviation root mean sqr error: \",round(score_CBR.std(),4))\nprint(\"R2 score\", r2_score_CBR)","f108c5d0":"np.random.seed(42)\nSR_model = StackingRegressor(regressors=(CatBoostRegressor(),\n                                          LinearRegression(),\n                                          BayesianRidge(),\n                                          GradientBoostingRegressor()),\n                              meta_regressor = CatBoostRegressor(),\n                              use_features_in_secondary = True)\nscore_SR = cv_rmse(SR_model)\ncv_scores.append(round(score_SR.mean(),4))\ncv_std.append(round(score_SR.std(),4))\n\nr2_score_SR = round(calc_r2Score(SR_model)*100,2)\nr2_scr.append(r2_score_SR)\nprint(\"Accuracy on Training set: \",SR_model.score(Train_SS,y_train))\nprint(\"Accuracy on Testing set: \",SR_model.score(Test_SS,y_test))\nprint(\"Mean root mean sqr error: \",round(score_SR.mean(),4))\nprint(\"std deviation root mean sqr error: \",round(score_SR.std(),4))\nprint(\"R2 score\", r2_score_SR)","32a7bbf9":"# Creating final dataframe\nfinal_cv_score = pd.DataFrame(baseline_models, columns = ['Regressors'])\nfinal_cv_score['RMSE_mean'] = cv_scores\nfinal_cv_score['RMSE_std'] = cv_std\nfinal_cv_score['R2_Score'] =r2_scr\nfinal_cv_score = final_cv_score.sort_values(by = 'RMSE_mean')\nfinal_cv_score","cb2ad980":"plt.figure(figsize = (12,8))\nax1 = sns.set_style(style=None, rc=None )\nsns.barplot(final_cv_score['Regressors'],final_cv_score['RMSE_mean'])\nsns.lineplot(data = final_cv_score['RMSE_std'], marker='o', sort = True)\nplt.xlabel('Regressors', fontsize = 12)\nplt.ylabel('CV_Mean_RMSE', fontsize = 12)\nplt.xticks(rotation=90)\nplt.show()","a7265814":"# Cat Boost Regressor\n\ncat = CatBoostRegressor()\ncat_model = cat.fit(Train_SS,y_train,\n                     eval_set = (Test_SS,y_test),\n                     plot=True,\n                     verbose = 0)","0ff8a6f1":"cat_pred = cat_model.predict(Test_SS)\ncat_score = rmse(y_test, cat_pred)\ncat_score","6b9fd03d":"# Features' importance of our model\n\nfeat_imp = cat_model.get_feature_importance(prettified=True)\nfeat_imp","4ccfa3ab":"# Plotting top 20 features' importance\n\nplt.figure(figsize = (12,8))\nsns.barplot(feat_imp['Importances'][:20],feat_imp['Feature Id'][:20], orient = 'h')\nplt.show()","ab4bcdbe":"# Catboost default paramters\n\ncat_model.get_all_params()","e1010cbe":"# Preforming a Random Grid Search to find the best combination of parameters\n\ngrid = {'iterations': [1000,6000],\n        'learning_rate': [0.05, 0.005, 0.0005],\n        'depth': [4, 6, 10],\n        'l2_leaf_reg': [1, 3, 5, 9]}\n\nfinal_model = CatBoostRegressor()\nrandomized_search_result = final_model.randomized_search(grid,\n                                                   X = X_train,\n                                                   y= y_train,\n                                                   verbose = False,\n                                                   plot=True)","7f569b7c":"# Final Cat-Boost Regressor\n\nparams = {'iterations': 6000,\n          'learning_rate': 0.005,\n          'depth': 4,\n          'l2_leaf_reg': 1,\n          'eval_metric':'RMSE',\n          'early_stopping_rounds': 200,\n          'verbose': 200,\n          'random_seed': 42}\n         \ncat_f = CatBoostRegressor(**params)\ncat_model_f = cat_f.fit(Train_SS,y_train,\n                     eval_set = (Test_SS,y_test),\n                     plot=True,\n                     verbose = False)\n\ncatf_pred = cat_model_f.predict(Test_SS)\ncatf_score = rmse(y_test, catf_pred)","d1f50b9d":"catf_score","f76aa254":"X_test = Test_HD_Final[Req_Columns]\nt_pred = cat_model_f.predict(X_test)\nt_pred = np.exp(t_pred)","eaf87547":"t_pred","9ff434bb":"HouseSaleID = Test_House_Data['Id']\nRFR_DF_Sub = pd.DataFrame({'Id': HouseSaleID, 'SalePrice':t_pred })\nRFR_DF_Sub.head()","b498eef1":"RFR_DF_Sub.to_csv(\"SR_Class_Submission8.csv\", index = False)","eed0a4a2":"##### Missing Value treatment for MSZoning","ccaf4d96":"##### Missing value treatment for MasVnrType column","cb5c9593":"All missing data treatment is completed on Training data set","f13b0371":"##### Mising Value treatment of MasVnrType","0ce80fef":"## 10. Cat Boost Regressor","417cb479":"## 3.KNeighbors Regressor","cd0b36e1":"Sale price (Dependent Variable) is now normally distributed. Doing same for, only columns with Skewness less than 0.5 for independent variable too. ","f06fbb5b":"## 11.Stacked Regressor","9eebc6e0":"## FEATURE ENGINEERING on test data\nCreating some new features by combining the ones that we already have. These could help us to increase the performance of the model!","cf12762e":"# For Test Data Preparation","9e754912":"Inference: With every year passing the price increases at an average","cb0c8f65":"## Creating function for cross validation & root sqr mean error","413568a3":"## 7.Bayesian Ridge Regression","30c01c9b":"##### Missing Value treatment for KitchenQual","151b19d5":"Checking variables which are most correlated","e2afc41c":"# Applying different Regressor Models on train data","02743cac":"## To check Duplicate Values","f5da9771":"In literature, acceptable values for skewness are between -0.5 and 0.5 while -2 and 2 for Kurtosis. Looking at the plot, we can clearly see how the distribution does not seem to be normal, but highly right-skewed. The non-normality of our distribution is also supported by the Shapiro test for normality below (p-value really small that allows us to reject the hypotesis of normality). Despite that, let's leave it like that for now, we'll deal with that later in the notebook.","030b4391":"Inference: With increase of 1st floor surface the price generally increases","a4f2d80d":"##### Missing Value treatment for Exterior2nd","0441455a":"As Cat Boost Regressor is minimum RSME, so checking again with CBR","b6f7907b":"##### Missing Value treatment for SaleType","4220933e":"Inference: Bath 0 and 1 have almost similar cost but as the bath increases to 2 and 3 the price increases significantly. ","80ffabca":"Correlation matrix is used to know the relation between numerical variables. Checking all numerical variable relation with Sale Price. We are using Pearson correlation","c3602206":"## FEATURE ENGINEERING in Training Data Set\nCreating some new features by combining the ones that we already have. These could help us to increase the performance of the model!","977f578f":"Inference: None of the parameter is very highly correlated to LotFrontage, so we will consider all the parameters to predict the missing value","b9b7985c":"## Checking for Missing Value In Training Data Set","cc8eb770":"# Getting Mutual Information","32bdf25c":"## 5.XGBoost Regressor","ba49de5a":"##### Missing value treatment for MasVnrArea column","d851d963":"Inference: room with 3 and 4 are similar price and then it increases with every floor at an average and become similar again after more than 10th floor ","aa9ffd74":"### Hperparameter Optimization","741df365":"##### Treating the Null Values for LotFrontage\n1. In our test dataset, We have 227 null values for lotfrontage, if we replace with mean\/median or some random values, It might effects on accuracy or model performance. To make model more efficiency, I'm predicting the value by consdering the relavent parameters of lotfrontage.\n2. Here I'm making LotFrontage as a dependent variable and rest all are independent variables. All the null values of LotFrontage, I'm considering as test dataset. So that we can predict the missing values.","ae65660d":"##### Missing value treatment for MasVnrArea column","6d1077e5":"# Getting Train and Test Data Set","ceb22232":"## Checking distribution of dependent variable (sale price)","35867af5":"# EDA & Visualization\ndoing data analysis using statistics and visualization that helps to understand the data better and explore the relation between them.","856887ab":"# Scaling the data","96995d57":"##### Creating the Training data set with selected columns","7d72a3d2":"## 4.RandomForest Regressor","5c1f1b4a":"##### Missing Value treatment for Utilities","306a9397":"### Treating the Null Values for LotFrontage\n1. In our dataset, We have 259 null values for lotfrontage, if we replace with mean\/median or some random values, It might effects on accuracy or model performance. To make model more efficiency, I'm predicting the value by consdering the relavent parameters of lotfrontage.\n2. Here I'm making LotFrontage as a dependent variable and rest all are independent variables. All the null values of LotFrontage, I'm considering as test dataset. So that we can predict the missing values.","17e007a2":"## 1. Decision Tree Regressor","05372eaa":"Inference: We can see that the median is almost same through out the years and also we can see it have the least correlation","6e7c4322":"##### Missing Value treatment for Exterior1st","704a9b0b":"##### Missing Value treatment for Functional","95ae492a":"Inference: - As the quality is increasing toward 10 grade from 1 the proce gereally increases","e2032d4b":"## To check any infinity value in the data set","748f58ba":"## 9.Gradient Boosting Regressor","7fee9305":"### As we can see that sale price is not normally distributed, applying log transformation","89485755":"## 2.Linear Regression","8581b9c5":"Inference:- as the number of car increased the price increases with an exception of garage with 5 car space which is same as 2 cars space. But still have a very good dependence on price ","a9cc14df":"##### Missing value treatment for Electrical column","f735f78e":"Inference:- As the Garage Area increased the price increases with few exception, these might me the same exceptions as of above graph with 4 cars, which we can ignore and consider Garage Area as 1 of the influencing parameter","be668c87":"## 8.Light Gradient Boost Regressor","c9d23ffc":"All missing value treatment is completed for test data too.","bdce05c3":"Inference: With increase of Total Basement surface the price generally increases","66ff54c2":"# Predicting data for test data set","b05ebb6f":"Inference:- As the GrLiveArea increases the Price also increases in most cases. ","d99674fe":"## 6. SVR (Support Vector Regressor Machine)"}}