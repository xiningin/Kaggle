{"cell_type":{"fc88889d":"code","e7f3c03f":"code","d8d16cb1":"code","909577a6":"code","2a676625":"code","c318335b":"code","72440f45":"code","d946dbf9":"code","22a99ba3":"code","19c89475":"code","4c2abc9f":"code","ddaded24":"code","eddf3b2a":"code","3122f9f2":"code","5a609c60":"code","de0b648f":"code","5ee14204":"code","327f6b3b":"code","beb85e2c":"code","eecfbaa7":"markdown","d7654ef4":"markdown","b647799c":"markdown","5ba1dc28":"markdown","40babdd1":"markdown","6b07a244":"markdown","8202044b":"markdown","c1385fa7":"markdown","2f3bb779":"markdown","14bf2314":"markdown","090d6459":"markdown","2ad39a61":"markdown","7422dc9b":"markdown","db7019ad":"markdown"},"source":{"fc88889d":"!nvidia-smi","e7f3c03f":"# Asthetics\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n# General\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nfrom collections import defaultdict\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nimport random\nimport gc\nfrom torch.optim import lr_scheduler\npd.set_option('display.max_columns', None)\nnp.seterr(divide='ignore', invalid='ignore')\ngc.enable()\n\n# Deep Learning\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import OneCycleLR\n# NLP\nfrom transformers import AutoTokenizer, AutoModel\n\n# Random Seed Initialize\nRANDOM_SEED = 42\n\ndef seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything()\n\n# Device Optimization\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n    \nprint(f'Using device: {device}')","d8d16cb1":"data_dir = '..\/input\/jrstc-train-folds'\ntrain_file_path = os.path.join(data_dir, 'validation_data_5_folds.csv')\nprint(f'Train file: {train_file_path}')","909577a6":"train_df = pd.read_csv(train_file_path)","2a676625":"train_df.shape","c318335b":"def text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?:\/\/\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text","72440f45":"# \u4e0d\u8fdb\u884cclean\n# tqdm.pandas()\n# train_df['less_toxic'] = train_df['less_toxic'].progress_apply(text_cleaning)\n# train_df['more_toxic'] = train_df['more_toxic'].progress_apply(text_cleaning)","d946dbf9":"train_df.sample(10)","22a99ba3":"train_df.groupby(['kfold']).size()","19c89475":"params = {\n    'device': device,\n    'debug': False,\n    'checkpoint': 'roberta-base',\n    'output_logits': 768,\n    'max_len': 128,\n    'num_folds': train_df['kfold'].nunique(),\n    'batch_size': 16,\n    'dropout': 0.2,\n    'num_workers': 2,\n    'epochs': 4,\n    'lr': 1e-4, # 2e-5\n    'min_lr': 1e-6,\n    'T_max': 500,\n    'margin': 0.5, # 0.7\n    'scheduler_name': 'CosineAnnealingLR',              \n    'no_decay': True,\n    'weight_decay': 1e-6\n}","4c2abc9f":"if params['debug']:\n    train_df = train_df.sample(frac=0.01)\n    print('Reduced training Data Size for Debugging purposes')","ddaded24":"class LabelSmoothingLoss(nn.Module):\n    '''LabelSmoothingLoss\n    '''\n    def __init__(self, classes, smoothing=0.05, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            # true_dist = pred.data.clone()\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing \/ (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n    \n    \nclass FGM():\n    def __init__(self, model):\n        self.model = model\n        self.backup = {}\n\n    def attack(self, epsilon=1., emb_name='bert.embeddings.'):\n        # emb_name\u8fd9\u4e2a\u53c2\u6570\u8981\u6362\u6210\u4f60\u6a21\u578b\u4e2dembedding\u7684\u53c2\u6570\u540d\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and emb_name in name:\n                self.backup[name] = param.data.clone()\n                norm = torch.norm(param.grad)\n                if norm != 0 and not torch.isnan(norm):\n                    r_at = epsilon * param.grad \/ norm\n                    param.data.add_(r_at)\n\n    def restore(self, emb_name='bert.embeddings.'):\n        # emb_name\u8fd9\u4e2a\u53c2\u6570\u8981\u6362\u6210\u4f60\u6a21\u578b\u4e2dembedding\u7684\u53c2\u6570\u540d\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and emb_name in name: \n                assert name in self.backup\n                param.data = self.backup[name]\n        self.backup = {}\n\n\nclass PGD():\n    def __init__(self, model):\n        self.model = model\n        self.emb_backup = {}\n        self.grad_backup = {}\n\n    def attack(self, epsilon=1., alpha=0.3, emb_name='bert.embeddings.', is_first_attack=False):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and emb_name in name:\n                if is_first_attack:\n                    self.emb_backup[name] = param.data.clone()\n                norm = torch.norm(param.grad)\n                if norm != 0 and not torch.isnan(norm):\n                    r_at = alpha * param.grad \/ norm\n                    param.data.add_(r_at)\n                    param.data = self.project(name, param.data, epsilon)\n\n    def restore(self, emb_name='bert.embeddings.'):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and emb_name in name: \n                assert name in self.emb_backup\n                param.data = self.emb_backup[name]\n        self.emb_backup = {}\n\n    def project(self, param_name, param_data, epsilon):\n        r = param_data - self.emb_backup[param_name]\n        if torch.norm(r) > epsilon:\n            r = epsilon * r \/ torch.norm(r)\n        return self.emb_backup[param_name] + r\n\n    def backup_grad(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                # \u4e0d\u5bf9\u6700\u540e\u7684 bert.pooler \u5c42\u548c linear1 \u5c42\u505a\u5bf9\u6297\u8bad\u7ec3\n                if 'encoder' in name or 'bert.embeddings.' in name:\n                    self.grad_backup[name] = param.grad.clone()\n\n    def restore_grad(self):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                if 'encoder' in name or 'bert.embeddings.' in name:\n                    param.grad = self.grad_backup[name]\n","eddf3b2a":"class BERTDataset:\n    def __init__(self, more_toxic, less_toxic, max_len=params['max_len'], checkpoint=params['checkpoint']):\n        self.more_toxic = more_toxic\n        self.less_toxic = less_toxic\n        self.max_len = max_len\n        self.checkpoint = checkpoint\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n        self.num_examples = len(self.more_toxic)\n\n    def __len__(self):\n        return self.num_examples\n\n    def __getitem__(self, idx):\n        more_toxic = str(self.more_toxic[idx])\n        less_toxic = str(self.less_toxic[idx])\n\n        tokenized_more_toxic = self.tokenizer(\n            more_toxic,\n            add_special_tokens=True,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n        )\n\n        tokenized_less_toxic = self.tokenizer(\n            less_toxic,\n            add_special_tokens=True,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n        )\n        \n        ids_more_toxic = tokenized_more_toxic['input_ids']\n        mask_more_toxic = tokenized_more_toxic['attention_mask']\n        token_type_ids_more_toxic = tokenized_more_toxic['token_type_ids']\n\n        ids_less_toxic = tokenized_less_toxic['input_ids']\n        mask_less_toxic = tokenized_less_toxic['attention_mask']\n        token_type_ids_less_toxic = tokenized_less_toxic['token_type_ids']\n\n        return {'ids_more_toxic': torch.tensor(ids_more_toxic, dtype=torch.long),\n                'mask_more_toxic': torch.tensor(mask_more_toxic, dtype=torch.long),\n                'token_type_ids_more_toxic': torch.tensor(token_type_ids_more_toxic, dtype=torch.long),\n                'ids_less_toxic': torch.tensor(ids_less_toxic, dtype=torch.long),\n                'mask_less_toxic': torch.tensor(mask_less_toxic, dtype=torch.long),\n                'token_type_ids_less_toxic': torch.tensor(token_type_ids_less_toxic, dtype=torch.long),\n                'target': torch.tensor(1, dtype=torch.float)}","3122f9f2":"def get_scheduler(optimizer, scheduler_params=params):\n#     if scheduler_params['scheduler_name'] == 'CosineAnnealingWarmRestarts':\n#         scheduler = CosineAnnealingWarmRestarts(\n#             optimizer,\n#             T_0=scheduler_params['T_0'],\n#             eta_min=scheduler_params['min_lr'],\n#             last_epoch=-1\n#         )\n#     elif scheduler_params['scheduler_name'] == 'OneCycleLR':\n#         scheduler = OneCycleLR(\n#             optimizer,\n#             max_lr=scheduler_params['max_lr'],\n#             steps_per_epoch=int(df_train.shape[0] \/ params['batch_size']) + 1,\n#             epochs=scheduler_params['epochs'],\n#             pct_start=scheduler_params['pct_start'],\n#             anneal_strategy=scheduler_params['anneal_strategy'],\n#             div_factor=scheduler_params['div_factor'],\n#             final_div_factor=scheduler_params['final_div_factor'],\n#         )\n    if scheduler_params['scheduler_name'] == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=scheduler_params['T_max'], \n                                                   eta_min=scheduler_params['min_lr'])\n    elif scheduler_params['scheduler_name'] == 'CosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=scheduler_params['T_0'], \n                                                             eta_min=scheduler_params['min_lr'])\n    elif scheduler_params['scheduler_name'] == None:\n        return None\n    return scheduler","5a609c60":"class MetricMonitor:\n    def __init__(self, float_precision=4):\n        self.float_precision = float_precision\n        self.reset()\n\n    def reset(self):\n        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n\n    def update(self, metric_name, val):\n        metric = self.metrics[metric_name]\n\n        metric[\"val\"] += val\n        metric[\"count\"] += 1\n        metric[\"avg\"] = metric[\"val\"] \/ metric[\"count\"]\n\n    def __str__(self):\n        return \" | \".join(\n            [\n                \"{metric_name}: {avg:.{float_precision}f}\".format(\n                    metric_name=metric_name, avg=metric[\"avg\"],\n                    float_precision=self.float_precision\n                )\n                for (metric_name, metric) in self.metrics.items()\n            ]\n        )","de0b648f":"class ToxicityModel(nn.Module):\n    def __init__(self, checkpoint=params['checkpoint'], params=params):\n        super(ToxicityModel, self).__init__()\n        self.checkpoint = checkpoint\n        self.bert = AutoModel.from_pretrained(checkpoint, return_dict=False)\n        self.layer_norm = nn.LayerNorm(params['output_logits'])\n        self.dropout = nn.Dropout(params['dropout'])\n        self.dense = nn.Sequential(\n            nn.Linear(params['output_logits'], 128),\n            nn.SiLU(),\n            nn.Dropout(params['dropout']),\n            nn.Linear(128, 1)\n        )\n\n    def forward(self, input_ids, token_type_ids, attention_mask):\n        _, pooled_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n        pooled_output = self.layer_norm(pooled_output)\n        pooled_output = self.dropout(pooled_output)\n        preds = self.dense(pooled_output)\n        return preds","5ee14204":"def train_fn(train_loader, model, criterion, optimizer, epoch, params, scheduler=None):\n    metric_monitor = MetricMonitor()\n    model.train()\n    stream = tqdm(train_loader)\n\n    for i, batch in enumerate(stream, start=1):\n        ids_more_toxic = batch['ids_more_toxic'].to(device)\n        mask_more_toxic = batch['mask_more_toxic'].to(device)\n        token_type_ids_more_toxic = batch['token_type_ids_more_toxic'].to(device)\n        ids_less_toxic = batch['ids_less_toxic'].to(device)\n        mask_less_toxic = batch['mask_less_toxic'].to(device)\n        token_type_ids_less_toxic = batch['token_type_ids_less_toxic'].to(device)\n        target = batch['target'].to(device)\n        \n        #\u6b63\u5e38\u8bad\u7ec3\n        logits_more_toxic = model(ids_more_toxic, token_type_ids_more_toxic, mask_more_toxic)\n        logits_less_toxic = model(ids_less_toxic, token_type_ids_less_toxic, mask_less_toxic)\n        loss = criterion(logits_more_toxic, logits_less_toxic, target)\n        metric_monitor.update('Loss', loss.item())\n        loss.backward()\n        \n        optimizer.step()\n            \n        if scheduler is not None:\n            scheduler.step()\n        optimizer.zero_grad()\n        stream.set_description(f\"Epoch: {epoch:02}. Train. {metric_monitor}\")","327f6b3b":"def validate_fn(val_loader, model, criterion, epoch, params):\n    metric_monitor = MetricMonitor()\n    model.eval()\n    stream = tqdm(val_loader)\n    all_loss = []\n    with torch.no_grad():\n        for i, batch in enumerate(stream, start=1):\n            ids_more_toxic = batch['ids_more_toxic'].to(device)\n            mask_more_toxic = batch['mask_more_toxic'].to(device)\n            token_type_ids_more_toxic = batch['token_type_ids_more_toxic'].to(device)\n            ids_less_toxic = batch['ids_less_toxic'].to(device)\n            mask_less_toxic = batch['mask_less_toxic'].to(device)\n            token_type_ids_less_toxic = batch['token_type_ids_less_toxic'].to(device)\n            target = batch['target'].to(device)\n\n            logits_more_toxic = model(ids_more_toxic, token_type_ids_more_toxic, mask_more_toxic)\n            logits_less_toxic = model(ids_less_toxic, token_type_ids_less_toxic, mask_less_toxic)\n            loss = criterion(logits_more_toxic, logits_less_toxic, target)\n            all_loss.append(loss.item())\n            metric_monitor.update('Loss', loss.item())\n            stream.set_description(f\"Epoch: {epoch:02}. Valid. {metric_monitor}\")\n            \n    return np.mean(all_loss)","beb85e2c":"gc.collect()\ndf_train = train_df[train_df['kfold'] != 0].copy()\ndf_valid = train_df[train_df['kfold'] == 0].copy()\n\ntrain_dataset = BERTDataset(\n    df_train.more_toxic.values,\n    df_train.less_toxic.values\n)\nvalid_dataset = BERTDataset(\n    df_valid.more_toxic.values,\n    df_valid.less_toxic.values\n)\n\ntrain_dataloader = DataLoader(\n    train_dataset, batch_size=params['batch_size'], shuffle=True,\n    num_workers=params['num_workers'], pin_memory=True\n)\nvalid_dataloader = DataLoader(\n    valid_dataset, batch_size=params['batch_size']*2, shuffle=False,\n    num_workers=params['num_workers'], pin_memory=True\n)\n\nmodel = ToxicityModel()\nmodel = model.to(params['device'])\ncriterion = nn.MarginRankingLoss(margin=params['margin'])\nif params['no_decay']:\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.weight', 'LayerNorm.bias']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n    optimizer = optim.AdamW(optimizer_grouped_parameters, lr=params['lr'])\nelse:\n    optimizer = optim.AdamW(model.parameters(), lr=params['lr'])\nscheduler = get_scheduler(optimizer)\n\n# Training and Validation Loop\nbest_loss = np.inf\nbest_epoch = 0\nbest_model_name = None\nfor epoch in range(1, params['epochs'] + 1):\n    train_fn(train_dataloader, model, criterion, optimizer, epoch, params, scheduler)\n    valid_loss = validate_fn(valid_dataloader, model, criterion, epoch, params)\n    if valid_loss <= best_loss:\n        best_loss = valid_loss\n        best_epoch = epoch\n        if best_model_name is not None:\n            os.remove(best_model_name)\n        torch.save(model.state_dict(), f\"{params['checkpoint']}_{epoch}.pth\")\n        best_model_name = f\"{params['checkpoint']}_{epoch}.pth\"\n\n# Print summary\nprint('')\nprint(f'The best LOSS: {best_loss} was achieved on epoch: {best_epoch}.')\nprint(f'The Best saved model is: {best_model_name}')\ndel df_train, df_valid, train_dataset, valid_dataset, train_dataloader, valid_dataloader, model\n_ = gc.collect()\ntorch.cuda.empty_cache()","eecfbaa7":"# Imports","d7654ef4":"## 1. Train Function","b647799c":"## LabelSmothingLoss & FGM  & PGD","5ba1dc28":"# Get GPU Info","40babdd1":"# NLP Model","6b07a244":"# Training And Validation Loops","8202044b":"# Metrics","c1385fa7":"# Text Cleaning","2f3bb779":"# Reading File","14bf2314":"# CFG","090d6459":"## 2. Validate Function","2ad39a61":"# Scheduler","7422dc9b":"# Dataset","db7019ad":"# Run"}}