{"cell_type":{"470ff10b":"code","3d05b947":"code","3fadd895":"code","812ac8f9":"code","cf6133e6":"code","8bdadf90":"code","32f83aa7":"code","9105f622":"code","2effdcbe":"code","5d94d7ab":"code","b761004d":"code","351e36e1":"code","81917ef2":"code","d0f799cc":"code","89dc47ec":"code","a5275b66":"code","ff88e291":"code","0c50919d":"code","0c30cdd6":"code","8993af13":"code","2f9546dd":"code","e4a311ca":"code","8905be02":"code","9a07446e":"code","30c0df76":"code","47035ed2":"code","a994c12a":"code","ab7fd4a5":"code","e9ced2eb":"code","b586b768":"code","2c084cc0":"code","32a0f800":"code","8b5a32b0":"code","326150e8":"code","13aca8ac":"code","ed59c2f8":"code","47b3b8f3":"code","6df0d4e8":"code","ef322858":"code","ac10a7d6":"code","ef5c2b8e":"code","7a406f0e":"code","8a600bd6":"code","7c412bd0":"code","e186d096":"code","d955aba0":"code","9be99047":"code","0f4bbe7b":"code","0b38f280":"code","831bbcb9":"code","987876f0":"code","827f6c09":"code","e19b10dd":"code","fbf2d472":"code","b5649727":"code","add20d5f":"code","56331adb":"code","863fbbad":"code","039c59be":"code","38178eba":"code","7342534a":"code","19a296b7":"code","524325c1":"code","1f84744f":"code","25dc81d7":"code","9c472aed":"code","e821b795":"code","bde285b7":"code","9425d412":"code","6fa6565d":"code","71771973":"code","9d1df4e2":"code","465ba582":"code","3213a636":"code","82889a8d":"code","35d0dd29":"code","938c90d1":"code","b03a97ca":"code","9b19b826":"code","eea1176c":"code","a53acd0b":"code","8ca5ad79":"code","5803ce1b":"code","2cc85484":"markdown","8a615b03":"markdown","433d1e00":"markdown","d747a499":"markdown","b435c88c":"markdown","3b4f30f4":"markdown","f6e8409e":"markdown","8fe0e2de":"markdown","5be1f251":"markdown","31f3ad8e":"markdown","88e31b07":"markdown","d0d2e263":"markdown","744b6734":"markdown","be36101a":"markdown","0d92745b":"markdown","94788b0f":"markdown","882c2685":"markdown","26de8103":"markdown","94836bca":"markdown","99d7d81a":"markdown","8a41968c":"markdown","12f7b87f":"markdown","fbefb7ca":"markdown","c42867a1":"markdown","99d6dd47":"markdown","6b4bf914":"markdown","3f32ec36":"markdown","fe1245d5":"markdown","b30f8b62":"markdown","2595d7a8":"markdown","b0774e4f":"markdown","66e8b869":"markdown","825599ef":"markdown","c11b3e5a":"markdown","95947274":"markdown","d37c3a1a":"markdown","9d4d3ce1":"markdown","f3290682":"markdown"},"source":{"470ff10b":"!pip install scikit-learn --upgrade --quiet","3d05b947":"!pip install numpy pandas matplotlib seaborn plotly opendatasets jovian --quiet","3fadd895":"dataset_url = 'https:\/\/github.com\/JovianML\/opendatasets\/raw\/master\/data\/house-prices-advanced-regression-techniques.zip'","812ac8f9":"from urllib.request import urlretrieve","cf6133e6":"urlretrieve(dataset_url, 'house-prices.zip')","8bdadf90":"from zipfile import ZipFile","32f83aa7":"with ZipFile('house-prices.zip') as f:\n    f.extractall(path='house-prices')","9105f622":"import os","2effdcbe":"data_dir = 'house-prices'","5d94d7ab":"os.listdir(data_dir)","b761004d":"import pandas as pd\npd.options.display.max_columns = 200\npd.options.display.max_rows = 200","351e36e1":"train_csv_path = data_dir + '\/train.csv'\ntrain_csv_path","81917ef2":"prices_df = pd.read_csv('house-prices\/train.csv')","d0f799cc":"prices_df","89dc47ec":"prices_df.info()","a5275b66":"shape=prices_df.shape","ff88e291":"print('The dataset shape is {}.'.format(shape))","0c50919d":"import matplotlib\nimport seaborn as sns\n%matplotlib inline","0c30cdd6":"sns.set_style('darkgrid')\nmatplotlib.rcParams['font.size'] = 14\nmatplotlib.rcParams['figure.figsize'] = (9, 7)\nmatplotlib.rcParams['figure.facecolor'] = '#00000000'\nsns.heatmap(prices_df.corr(), cmap= \"bone_r\");","8993af13":"prices_df","2f9546dd":"prices_df.columns[-1]","e4a311ca":"# Identifing the input columns (a list of column names)\ninput_cols = prices_df.columns[1:-1]","8905be02":"# Identifing the name of the target column (a single string, not a list)\ntarget_col = prices_df.columns[-1]","9a07446e":"print(list(input_cols))","30c0df76":"len(input_cols)","47035ed2":"print(target_col)","a994c12a":"inputs_df = prices_df[input_cols].copy()","ab7fd4a5":"targets = prices_df[target_col]","e9ced2eb":"inputs_df","b586b768":"targets","2c084cc0":"prices_df.info()","32a0f800":"import numpy as np","8b5a32b0":"numeric_cols = inputs_df.select_dtypes(include=['int64', 'float64']).columns.tolist()","326150e8":"categorical_cols = inputs_df.select_dtypes(include=['object']).columns.tolist()","13aca8ac":"print(list(numeric_cols))","ed59c2f8":"print(list(categorical_cols))","47b3b8f3":"missing_counts = inputs_df[numeric_cols].isna().sum().sort_values(ascending=False)\nmissing_counts[missing_counts > 0]","6df0d4e8":"from sklearn.impute import SimpleImputer","ef322858":"# 1. Creating the imputer\nimputer = SimpleImputer(strategy = 'mean')","ac10a7d6":"# 2. Fitting the imputer to the numeric colums\nimputer.fit(prices_df[numeric_cols])","ef5c2b8e":"# 3. Transform and replace the numeric columns\ninputs_df[numeric_cols] = imputer.transform(inputs_df[numeric_cols])","7a406f0e":"missing_counts = inputs_df[numeric_cols].isna().sum().sort_values(ascending=False)\nmissing_counts[missing_counts > 0] # should be an empty list","8a600bd6":"inputs_df[numeric_cols].describe().loc[['min', 'max']]","7c412bd0":"from sklearn.preprocessing import MinMaxScaler","e186d096":"# Creating the scaler\nscaler = MinMaxScaler()","d955aba0":"# Fitting the scaler to the numeric columns\nscaler.fit(prices_df[numeric_cols])","9be99047":"# Transforming and replacing the numeric columns\ninputs_df[numeric_cols] = scaler.transform(inputs_df[numeric_cols])","0f4bbe7b":"inputs_df[numeric_cols].describe().loc[['min', 'max']]","0b38f280":"inputs_df[categorical_cols].nunique().sort_values(ascending=False)","831bbcb9":"from sklearn.preprocessing import OneHotEncoder","987876f0":"# 1. Creating the encoder\nencoder = OneHotEncoder(sparse=False, handle_unknown='ignore')","827f6c09":"# 2. Fitting the encoder to the categorical colums\nencoder.fit(prices_df[categorical_cols])","e19b10dd":"# 3. Generating column names for each category\nencoded_cols = list(encoder.get_feature_names(categorical_cols))\nlen(encoded_cols)","fbf2d472":"# 4. Transforming and adding new one-hot category columns\ninputs_df[encoded_cols] = encoder.transform(inputs_df[categorical_cols])","b5649727":"inputs_df","add20d5f":"from sklearn.model_selection import train_test_split","56331adb":"train_inputs, val_inputs, train_targets, val_targets = train_test_split(inputs_df[numeric_cols + encoded_cols], \n                                                                        targets, \n                                                                        test_size=0.25, \n                                                                        random_state=42)","863fbbad":"train_inputs","039c59be":"train_targets","38178eba":"val_inputs","7342534a":"val_targets","19a296b7":"from sklearn.linear_model import Ridge","524325c1":"# Creating the model\nmodel = Ridge()","1f84744f":"# Fitting the model using inputs and targets\nmodel.fit(train_inputs[numeric_cols + encoded_cols], train_targets)","25dc81d7":"from sklearn.metrics import mean_squared_error","9c472aed":"train_preds = model.predict(train_inputs)","e821b795":"train_preds","bde285b7":"train_rmse = mean_squared_error(train_targets, train_preds, squared= False)","9425d412":"print('The RMSE loss for the training set is $ {}.'.format(train_rmse))","6fa6565d":"val_preds = model.predict(val_inputs)","71771973":"val_preds","9d1df4e2":"val_rmse = mean_squared_error(val_targets, val_preds, squared= False)","465ba582":"print('The RMSE loss for the validation set is $ {}.'.format(val_rmse))","3213a636":"weights = model.coef_","82889a8d":"weights_df = pd.DataFrame({\n    'columns': train_inputs.columns,\n    'weight': weights\n}).sort_values('weight', ascending=False)","35d0dd29":"weights_df","938c90d1":"def predict_input(single_input):\n    input_df = pd.DataFrame([single_input])\n    input_df[numeric_cols] = imputer.transform(input_df[numeric_cols])\n    input_df[numeric_cols] = scaler.transform(input_df[numeric_cols])\n    input_df[encoded_cols] = encoder.transform(input_df[categorical_cols].values)\n    X_input = input_df[numeric_cols + encoded_cols]\n    return model.predict(X_input)[0]","b03a97ca":"sample_input = { 'MSSubClass': 20, 'MSZoning': 'RL', 'LotFrontage': 77.0, 'LotArea': 9320,\n 'Street': 'Pave', 'Alley': None, 'LotShape': 'IR1', 'LandContour': 'Lvl', 'Utilities': 'AllPub',\n 'LotConfig': 'Inside', 'LandSlope': 'Gtl', 'Neighborhood': 'NAmes', 'Condition1': 'Norm', 'Condition2': 'Norm',\n 'BldgType': '1Fam', 'HouseStyle': '1Story', 'OverallQual': 4, 'OverallCond': 5, 'YearBuilt': 1959,\n 'YearRemodAdd': 1959, 'RoofStyle': 'Gable', 'RoofMatl': 'CompShg', 'Exterior1st': 'Plywood',\n 'Exterior2nd': 'Plywood', 'MasVnrType': 'None','MasVnrArea': 0.0,'ExterQual': 'TA','ExterCond': 'TA',\n 'Foundation': 'CBlock','BsmtQual': 'TA','BsmtCond': 'TA','BsmtExposure': 'No','BsmtFinType1': 'ALQ',\n 'BsmtFinSF1': 569,'BsmtFinType2': 'Unf','BsmtFinSF2': 0,'BsmtUnfSF': 381,\n 'TotalBsmtSF': 950,'Heating': 'GasA','HeatingQC': 'Fa','CentralAir': 'Y','Electrical': 'SBrkr', '1stFlrSF': 1225,\n '2ndFlrSF': 0, 'LowQualFinSF': 0, 'GrLivArea': 1225, 'BsmtFullBath': 1, 'BsmtHalfBath': 0, 'FullBath': 1,\n 'HalfBath': 1, 'BedroomAbvGr': 3, 'KitchenAbvGr': 1,'KitchenQual': 'TA','TotRmsAbvGrd': 6,'Functional': 'Typ',\n 'Fireplaces': 0,'FireplaceQu': np.nan,'GarageType': np.nan,'GarageYrBlt': np.nan,'GarageFinish': np.nan,'GarageCars': 0,\n 'GarageArea': 0,'GarageQual': np.nan,'GarageCond': np.nan,'PavedDrive': 'Y', 'WoodDeckSF': 352, 'OpenPorchSF': 0,\n 'EnclosedPorch': 0,'3SsnPorch': 0, 'ScreenPorch': 0, 'PoolArea': 0, 'PoolQC': np.nan, 'Fence': np.nan, 'MiscFeature': 'Shed',\n 'MiscVal': 400, 'MoSold': 1, 'YrSold': 2010, 'SaleType': 'WD', 'SaleCondition': 'Normal'}","9b19b826":"predicted_price = predict_input(sample_input)","eea1176c":"print('The predicted sale price of the house is ${}'.format(predicted_price))","a53acd0b":"import joblib","8ca5ad79":"house_price_predictor = {\n    'model': model,\n    'imputer': imputer,\n    'scaler': scaler,\n    'encoder': encoder,\n    'input_cols': input_cols,\n    'target_col': target_col,\n    'numeric_cols': numeric_cols,\n    'categorical_cols': categorical_cols,\n    'encoded_cols': encoded_cols\n}","5803ce1b":"joblib.dump(house_price_predictor, 'house_price_predictor.joblib')","2cc85484":"### Imputing Numerical Data\n\nSome of the numeric columns in our dataset contain missing values (`NaN`).","8a615b03":"Generating predictions and compute the `RMSE` loss for the training and validation sets. ","433d1e00":"After imputation, none of the numeric columns should contain any missing values.","d747a499":"Imputing missing values in the numeric columns of `inputs_df` using a `SimpleImputer`. ","b435c88c":"Let's create a dataframe to view the weight assigned to each column.","3b4f30f4":"The dataset is extracted to the folder `house-prices`. Let's view the contents of the folder using the [`os`](https:\/\/docs.python.org\/3\/library\/os.html) module.","f6e8409e":"Let's explore the columns and data types within the dataset.","8fe0e2de":"# House Price Prediction with Linear Regression\n\n![](https:\/\/i.imgur.com\/3sw1fY9.jpg)\n\nWe are going to predict the price of a house using information like its location, area, no. of rooms etc. We'll use the dataset from the [House Prices - Advanced Regression Techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) competition on [Kaggle](https:\/\/kaggle.com). We'll follow a step-by-step process to train our model:\n\n1. Download and explore the data\n2. Prepare the dataset for training\n3. Train a linear regression model\n4. Make predictions and evaluate the model\n\n\n\n","5be1f251":"Machine learning models can't work with missing data. The process of filling missing values is called [imputation](https:\/\/scikit-learn.org\/stable\/modules\/impute.html).\n\n<img src=\"https:\/\/i.imgur.com\/W7cfyOp.png\" width=\"480\">\n\nThere are several techniques for imputation, but we'll use the most basic one: replacing missing values with the average value in the column using the `SimpleImputer` class from `sklearn.impute`.\n","31f3ad8e":"A good practice is to [scale numeric features](https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html#scaling-features-to-a-range) to a small range of values e.g. $(0,1)$. Scaling numeric features ensures that no particular feature has a disproportionate impact on the model's loss. Optimization algorithms also work better in practice with smaller numbers.\n","88e31b07":"Encoding categorical columns in the dataset as one-hot vectors using `OneHotEncoder` from `sklearn.preprocessing`. Add a new binary (0\/1) column for each category","d0d2e263":"### Training and Validation Set\n\nFinally, let's split the dataset into a training and validation set. We'll use a randomly select 25% subset of the data for validation. Also, we'll use just the numeric and encoded columns, since the inputs to our model must be numbers. ","744b6734":"Creating two lists `numeric_cols` and `categorical_cols` containing names of numeric and categorical input columns within the dataframe respectively. \n\nNumeric columns have data types `int64` and `float64`, whereas categorical columns have the data type `object`.","be36101a":"Creating a list `input_cols` of column names containing data that can be used as input to train the model, and identify the target column as the variable `target_col`.","0d92745b":"### Identifing Numeric and Categorical Data\n\nThe next step in data preparation is to identify numeric and categorical columns. We can do this by looking at the data type of each column.","94788b0f":"### Feature Importance\n\nLet's look at the weights assigned to different columns, to figure out which columns in the dataset are the most important.","882c2685":"Now that we've identified the input and target columns, we can separate input & target data.","26de8103":"We'll use the data in the file `train.csv` for training our model. We can load the for processing using the [Pandas](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/index.html) library.","94836bca":"## Step 2 - Prepare the Dataset for Training\n\nBefore we can train the model, we need to prepare the dataset. Here are the steps we'll follow:\n\n1. Identify the input and target column(s) for training the model.\n2. Identify numeric and categorical input columns.\n3. [Impute](https:\/\/scikit-learn.org\/stable\/modules\/impute.html) (fill) missing values in numeric columns\n4. [Scale](https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html#scaling-features-to-a-range) values in numeric columns to a $(0,1)$ range.\n5. [Encode](https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html#encoding-categorical-features) categorical data into one-hot vectors.\n6. Split the dataset into training and validation sets.\n","99d7d81a":"### Saving the model\n\nLet's save the model (along with other useful objects) to disk, so that we use it for making predictions without retraining.","8a41968c":"\n\nSince machine learning models can only be trained with numeric data, we need to convert categorical data to numbers. A common technique is to use one-hot encoding for categorical columns.\n\n<img src=\"https:\/\/i.imgur.com\/n8GuiOO.png\" width=\"640\">\n\nOne hot encoding involves adding a new binary (0\/1) column for each unique category of a categorical column.","12f7b87f":"We'll use the `urlretrieve` function from the module [`urllib.request`](https:\/\/docs.python.org\/3\/library\/urllib.request.html) to dowload the dataset.","fbefb7ca":"`model.fit` uses the following strategy for training the model (source):\n\n1. We initialize a model with random parameters (weights & biases).\n2. We pass some inputs into the model to obtain predictions.\n3. We compare the model's predictions with the actual targets using the loss function.\n4. We use an optimization technique (like least squares, gradient descent etc.) to reduce the loss by adjusting the weights & biases of the model\n5. We repeat steps 1 to 4 till the predictions from the model are good enough.\n\n<img src=\"https:\/\/www.deepnetts.com\/blog\/wp-content\/uploads\/2019\/02\/SupervisedLearning.png\" width=\"480\">","c42867a1":"## Step 1 - Download and Explore the Data\n\nThe dataset is available as a ZIP file at the following url:","99d6dd47":"### Identify Inputs and Targets\n\nWhile the dataset contains 81 columns, not all of them are useful for modeling. Note the following:\n\n- The first column `Id` is a unique ID for each house and isn't useful for training the model.\n- The last column `SalePrice` contains the value we need to predict i.e. it's the target column.\n- Data from all the other columns (except the first and the last column) can be used as inputs to the model.\n ","6b4bf914":"Let's begin by installing the required libraries:","3f32ec36":"Identifying the `weights` (or coefficients) assigned to for different features by the model.","fe1245d5":"### Encode Categorical Columns\n\nOur dataset contains several categorical columns, each with a different number of categories.","b30f8b62":"Now, Scaling numeric values to the $(0, 1)$ range using `MinMaxScaler` from `sklearn.preprocessing`.","2595d7a8":"After scaling, the ranges of all numeric columns should be $(0, 1)$.","b0774e4f":"### Making Predictions\n\nThe model can be used to make predictions on new inputs using the following helper function:","66e8b869":"## Step 3 - Train a Linear Regression Model\n\nWe're now ready to train the model. Linear regression is a commonly used technique for solving regression problems. In a linear regression model, the target is modeled as a linear combination (or weighted sum) of input features. The predictions from the model are evaluated using a loss function like the Root Mean Squared Error (RMSE).\n\n\nHere's a visual summary of how a linear regression model is structured:\n\n<img src=\"https:\/\/i.imgur.com\/iTM2s5k.png\" width=\"480\">\n\nHowever, linear regression doesn't generalize very well when we have a large number of input columns with co-linearity i.e. when the values one column are highly correlated with values in other column(s). This is because it tries to fit the training data perfectly. \n\nInstead, we'll use Ridge Regression, a variant of linear regression that uses a technique called L2 regularization to introduce another loss term that forces the model to generalize better.","825599ef":"Can you tell which columns have the greatest impact on the price of the house?","c11b3e5a":"## Step 4 - Make Predictions and Evaluate Your Model\n\nThe model is now trained, and we can use it to generate predictions for the training and validation inputs. We can evaluate the model's performance using the RMSE (root mean squared error) loss function.","95947274":"Creating and training a linear regression model using the `Ridge` class from `sklearn.linear_model`.","d37c3a1a":"### Scale Numerical Values\n\nThe numeric columns in our dataset have varying ranges. ","9d4d3ce1":"The file `housing-prices.zip` has been downloaded. Let's unzip it using the [`zipfile`](https:\/\/docs.python.org\/3\/library\/zipfile.html) module.","f3290682":"The new one-hot category columns should now be added to `inputs_df`."}}