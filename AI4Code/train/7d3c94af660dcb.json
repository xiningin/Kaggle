{"cell_type":{"94663243":"code","e691cb0a":"code","125a423f":"code","8fde8f1e":"code","9bbf7c7a":"code","00854de5":"code","4855c150":"code","cb01e248":"code","6f684327":"code","d8b79913":"markdown","5e99996c":"markdown","0110fd36":"markdown","d60b525a":"markdown","4853bc62":"markdown","369bb2f2":"markdown","427ea8e8":"markdown","7920b71d":"markdown","70226cb4":"markdown","312eac81":"markdown"},"source":{"94663243":"import torch\nfrom torch import optim, nn\nfrom torch.utils.data import TensorDataset, DataLoader\nimport pandas as pd\nimport numpy as np\nimport time\nimport copy\nimport os\nimport matplotlib.pyplot as plt\n\nimport warnings\n\nwarnings.filterwarnings('ignore')","e691cb0a":"data = pd.read_csv('..\/input\/digit-recognizer\/train.csv').sample(frac=1)  # load\n\ntrain_size = 37800\nprint(\"train_size: {}\".format(train_size))\n\nlabel = data['label']\ndel data['label']\n\ndata = torch.tensor(data.values)\nlabel = torch.tensor(label)\n\n# Normalization\ndata = data \/ 255.0\n\nprint(label.size())","125a423f":"all_ds = TensorDataset(data, label)\n\n# Split\ntrain_ds, valid_ds = torch.utils.data.random_split(all_ds, [train_size, 42000-train_size])\n\n# Create dataloaders\nbatch_size = 1000\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nvalid_dl = DataLoader(valid_ds, batch_size=batch_size * 2)\n\nprint(train_ds[0])","8fde8f1e":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\nprint('Device: {}'.format(device))","9bbf7c7a":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        self.linear = nn.Sequential(\n            nn.Linear(256 * 3 * 3, 256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, 10)\n        )\n\n    def forward(self, x):\n        x = x.view(-1, 1, 28, 28).float()\n        x = self.conv(x)\n        x = x.view(x.size(0), -1)\n        x = self.linear(x)\n        return x\n\n\nnet = Net()\nnet.to(device)\n\n\n# Load your model\n# if os.path.exists(net_path):\n#     net.load_state_dict(torch.load(net_path))","00854de5":"lr = 0.0001\n\noptimizer = optim.RMSprop(net.parameters(), lr=lr, alpha=0.9)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\nloss_fn = nn.CrossEntropyLoss()","4855c150":"def accuracy(out, y):\n    preds = torch.argmax(out, dim=1)\n    return (preds == y).float().mean().item(), len(y)\n\n\ndef loss_batch(net, x, y):\n    loss = loss_fn(net(x), y)\n    return loss.item(), len(x)\n\n\ndef train_model():\n    print('Train model')\n    best_model_wts = copy.deepcopy(net.state_dict())\n    best_acc = 0.0\n    loss_show = []\n    acc_show = []\n\n    for epoch in range(epochs):\n        epoch_since = time.time()\n        print(\"Epoch {}:\".format(epoch), end=' ')\n        net.train()\n        for step, (b_x, b_y) in enumerate(train_dl):\n            out = net(b_x.to(device))\n            loss = loss_fn(out, b_y.to(device))\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        net.eval()\n        with torch.no_grad():\n            losses, nums_loss = zip(\n                *[loss_batch(net, x.to(device), y.to(device)) for x, y in valid_dl]\n            )\n            acc, nums_acc = zip(\n                *[accuracy(net(x.to(device)), y.to(device)) for x, y in valid_dl]\n            )\n        val_loss = np.sum(np.multiply(losses, nums_loss)) \/ np.sum(nums_loss)\n        val_acc = np.sum(np.multiply(acc, nums_acc)) \/ np.sum(nums_acc)\n        epoch_time_elapsed = time.time() - epoch_since\n        print(\"time:{:.0f}s\".format(epoch_time_elapsed), \"loss:{:.10f}\".format(val_loss), \"accuracy:{:.10f}\".format(val_acc))\n        if val_acc > best_acc:\n            best_acc = val_acc\n            best_model_wts = copy.deepcopy(net.state_dict())\n        scheduler.step(val_loss)\n        loss_show.append(val_loss)\n        acc_show.append(val_acc)\n\n        if epoch % 10 == 9:\n            net.load_state_dict(best_model_wts)\n#             torch.save(net.state_dict(), net_path)\n    net.load_state_dict(best_model_wts)\n#     Save your model if you want\n#     torch.save(net.state_dict(), net_path)\n    plt.figure()\n    plt.plot(range(epochs), loss_show)\n    plt.figure()\n    plt.plot(range(epochs), acc_show)\n    plt.show()\n    \n\n\n# Set epoch to 30 to get 99.4% accuracy\nepochs = 10\ntrain_model()","cb01e248":"def save():\n    test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\n    test = torch.tensor(test.values)\n    test = test \/ 255.0\n    test_ds = TensorDataset(test)\n    test_dl = DataLoader(test_ds, batch_size=2)\n    print(\"Save preds\")\n    preds = []\n    with torch.no_grad():\n        for x in test_dl:\n            x = x[0]\n            out = net(x.to(device))\n            a, b = out.split(1, 0)\n            pred = torch.argmax(a, dim=1).to('cpu')\n            preds.append(pred.numpy().tolist()[0])\n            pred = torch.argmax(b, dim=1).to('cpu')\n            preds.append(pred.numpy().tolist()[0])\n    dataframe = pd.DataFrame({\"ImageId\": range(1, 28001), \"Label\": preds})\n    dataframe.to_csv(csv_path, index=False)\n    print('Saved')\n    return preds\n\n\n\ncsv_path = 'post.csv'\n# save()","6f684327":"acc, nums_acc = zip(\n    *[accuracy(net(x.to(device)), y.to(device)) for x, y in valid_dl]\n)\nvalid_acc = np.sum(np.multiply(acc, nums_acc)) \/ np.sum(nums_acc)\nprint(\"valid :{:.10f}\".format(valid_acc))","d8b79913":"## Accelerating computing with GPU","5e99996c":"## Split train and valid set. Create dataloaders","0110fd36":"## Set optimizer and loss function","d60b525a":"## Make predictions","4853bc62":"# 2. Data preparation\n\n##     Load Data","369bb2f2":"# 1. Import","427ea8e8":"# 3. CNN\n\n##     Define your model","7920b71d":"Thanks","70226cb4":"## View training results","312eac81":"## Start training"}}