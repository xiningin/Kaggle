{"cell_type":{"6f734bb2":"code","9d2a3f3e":"code","4f4941f9":"code","d054dab8":"code","f8132275":"code","4d144154":"code","fcf5c99d":"code","7c818167":"code","19829949":"code","19ead5d7":"code","023fe948":"code","ffd65382":"code","f7d29f07":"code","3ffebb22":"code","3a140779":"code","a7d7a217":"code","999ab1ed":"code","48c8af0a":"code","42f087c3":"code","2ba402e5":"code","e0d7c17c":"code","cd4694cc":"code","19a8a8af":"code","aa770c16":"code","e606598d":"code","d57ece32":"code","03e130c0":"code","c3286da6":"code","ac0d9ed6":"markdown","089971f0":"markdown","8cd682f1":"markdown","0d7e305c":"markdown","8a4d9bc9":"markdown","e630639a":"markdown","598dd135":"markdown","3134c3d5":"markdown","3ee073f1":"markdown","186d32b9":"markdown","db49169a":"markdown","757243ad":"markdown","08fa1de3":"markdown","ceefd700":"markdown","0b46b6ce":"markdown","9f5fcb7a":"markdown","545e493e":"markdown","05007080":"markdown","1b0fbc4d":"markdown","ad5802ac":"markdown","aeeb0d17":"markdown","ee462b27":"markdown","92074ab7":"markdown","aec39cd7":"markdown","8de2c717":"markdown","fece0fe3":"markdown","e754797a":"markdown","1260a218":"markdown","75f0a793":"markdown","8b5f2b6e":"markdown"},"source":{"6f734bb2":"!pip install h2o","9d2a3f3e":"import sys, os, os.path\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport pickle\n\nimport h2o\nfrom h2o.automl import H2OAutoML\n\nh2o.init(\n    nthreads=-1,     # number of threads when launching a new H2O server\n    max_mem_size=12  # in gigabytes\n)","4f4941f9":"train_df = pd.read_csv('..\/input\/mlcourse\/flight_delays_train.csv')\ntest_df = pd.read_csv('..\/input\/mlcourse\/flight_delays_test.csv')","d054dab8":"print('train_df cols:', list(train_df.columns))\nprint('test_df cols: ', list(test_df.columns))\ntrain_df.head()","f8132275":"train_df.dtypes","4d144154":"for df in [train_df, test_df]:\n    df['Month'] = df['Month'].apply(lambda s: s.split('-')[1]).astype('int')\n    df['DayofMonth'] = df['DayofMonth'].apply(lambda s: s.split('-')[1]).astype('int')\n    df['DayOfWeek'] = df['DayOfWeek'].apply(lambda s: s.split('-')[1]).astype('int')\n    \n    df['HourFloat'] = df['DepTime'].apply(\n        lambda t: (t \/\/ 100) % 24 + ((t % 100) % 60) \/ 60\n    ).astype('float')","fcf5c99d":"for df in [train_df, test_df]:\n    df['Route'] = df[['Origin', 'Dest']].apply(\n        lambda pair: ''.join([str(a) for a in pair]),\n        axis='columns'\n    ).astype('str')","7c818167":"target = train_df['dep_delayed_15min'].map({'Y': 1, 'N': 0})\n\nfeature_cols = [\n    'Month', 'DayofMonth', 'DayOfWeek', 'HourFloat', \n    'UniqueCarrier', 'Origin', 'Dest', 'Route', 'Distance',]\ntrain_df_modif = train_df[feature_cols]\ntest_df_modif = test_df[feature_cols]","19829949":"N_train = train_df_modif.shape[0]\ntrain_test_X = pd.concat([train_df_modif, test_df_modif], axis='index')\n\nfor feat in ['UniqueCarrier', 'Origin', 'Dest', 'Route']:\n    train_test_X[feat] = train_test_X[feat].astype('category')","19ead5d7":"X_train = train_test_X[:N_train]\nX_test = train_test_X[N_train:]\ny_train = target","023fe948":"X_y_train_h = h2o.H2OFrame(pd.concat([X_train, y_train], axis='columns'))\nX_y_train_h['dep_delayed_15min'] = X_y_train_h['dep_delayed_15min'].asfactor()\n# ^ the target column should have categorical type for classification tasks\n#   (numerical type for regression tasks)\n\nX_test_h = h2o.H2OFrame(X_test)\n\nX_y_train_h.describe()","ffd65382":"aml = H2OAutoML(\n    max_runtime_secs=(3600 * 8),  # 8 hours\n    max_models=None,  # no limit\n    seed=17\n)","f7d29f07":"%%time\n\n# aml.train(\n#     x=feature_cols,\n#     y='dep_delayed_15min',\n#     training_frame=X_y_train_h\n# )\n\n# lb = aml.leaderboard\n# model_ids = list(lb['model_id'].as_data_frame().iloc[:,0])\n# out_path = \".\"\n\n# for m_id in model_ids:\n#     mdl = h2o.get_model(m_id)\n#     h2o.save_model(model=mdl, path=out_path, force=True)\n\n# h2o.export_file(lb, os.path.join(out_path, 'aml_leaderboard.h2o'), force=True)","3ffebb22":"models_path = \"..\/input\/h2o-automl-saved-models-classif\/\"\n\nlb = h2o.import_file(path=os.path.join(models_path, \"aml_leaderboard.h2o\"))\n\nlb.head(rows=10)\n#lb.head(rows=lb.nrows)\n# ^ to see the entire leaderboard","3a140779":"se_all = h2o.load_model(os.path.join(models_path, \"StackedEnsemble_AllModels_AutoML_20190414_112210\"))\n# Get the Stacked Ensemble metalearner model\nmetalearner = h2o.get_model(se_all.metalearner()['name'])","a7d7a217":"%matplotlib inline\nmetalearner.std_coef_plot(num_of_features=20)\n# ^ all importance values starting from the 16th are zero\n\n#metalearner.coef_norm()\n# ^ to see the table in the text form","999ab1ed":"se_best_of_family = h2o.load_model(os.path.join(models_path, \"StackedEnsemble_BestOfFamily_AutoML_20190414_112210\"))\n# Get the Stacked Ensemble metalearner model\nmetalearner = h2o.get_model(se_best_of_family.metalearner()['name'])\n\n%matplotlib inline\nmetalearner.std_coef_plot(num_of_features=10)\n#metalearner.coef_norm()","48c8af0a":"from h2o.estimators.xgboost import H2OXGBoostEstimator\n\nmodel_01 = h2o.load_model(os.path.join(models_path, \"XGBoost_grid_1_AutoML_20190414_112210_model_19\"))\n\nexcluded_params = ['model_id', 'response_column', 'ignored_columns']\nmodel_01_actual_params = {k: v['actual'] for k, v in model_01.params.items() if k not in excluded_params}\n\nreprod_model_01 = H2OXGBoostEstimator(**model_01_actual_params)\nreprod_model_01.train(\n    x=feature_cols,\n    y='dep_delayed_15min',\n    training_frame=X_y_train_h\n)\nreprod_model_01.auc(xval=True)\n# ^ 0.749453, slightly worse compared to the leaderboard value","42f087c3":"from h2o.estimators.gbm import H2OGradientBoostingEstimator\n\nmodel_12 = h2o.load_model(os.path.join(models_path, \"GBM_grid_1_AutoML_20190414_112210_model_85\"))\n\nexcluded_params = ['model_id', 'response_column', 'ignored_columns']\nmodel_12_actual_params = {k: v['actual'] for k, v in model_12.params.items() if k not in excluded_params}\n\nreprod_model_12 = H2OGradientBoostingEstimator(**model_12_actual_params)\nreprod_model_12.train(\n    x=feature_cols,\n    y='dep_delayed_15min',\n    training_frame=X_y_train_h\n)\nreprod_model_12.auc(xval=True)\n# ^ 0.741785, the same as at the leaderboard","2ba402e5":"from h2o.estimators.glm import H2OGeneralizedLinearEstimator\nfrom h2o.grid.grid_search import H2OGridSearch\n\nmodel_93 = h2o.load_model(os.path.join(models_path, \"GLM_grid_1_AutoML_20190414_112210_model_1\"))\n\nexcluded_params = ['model_id', 'response_column', 'ignored_columns', 'lambda']\nmodel_93_actual_params = {k: v['actual'] for k, v in model_93.params.items() if k not in excluded_params}\n\nreprod_model_93 = H2OGeneralizedLinearEstimator(**model_93_actual_params)\nreprod_model_93.train(\n    x=feature_cols,\n    y='dep_delayed_15min',\n    training_frame=X_y_train_h\n)\nreprod_model_93.auc(xval=True)\n# ^ 0.699418, the same as at the leaderboard","e0d7c17c":"from catboost import Pool, CatBoostClassifier, cv\n\ncb_model = CatBoostClassifier(\n    eval_metric='AUC',\n    use_best_model=True,\n    random_seed=17\n)\n\ncv_data = cv(\n    Pool(X_train, y_train, cat_features=[4,5,6,7]),\n    cb_model.get_params(),\n    fold_count=5,\n    verbose=False\n)\n\nprint(\"CatBoostClassifier: the best cv auc is\", np.max(cv_data['test-AUC-mean']))","cd4694cc":"df_train = pd.read_csv('..\/input\/nyc-taxi-trip-duration\/train.csv', index_col=0)\ndf_test  = pd.read_csv('..\/input\/nyc-taxi-trip-duration\/test.csv',  index_col=0)","19a8a8af":"df_train['pickup_datetime'] = pd.to_datetime(df_train.pickup_datetime)\ndf_train.loc[:, 'pickup_date'] = df_train['pickup_datetime'].dt.date\ndf_train['dropoff_datetime'] = pd.to_datetime(df_train.dropoff_datetime)\ndf_train['store_and_fwd_flag'] = 1 * (df_train.store_and_fwd_flag.values == 'Y')\ndf_train['check_trip_duration'] = (df_train['dropoff_datetime'] - df_train['pickup_datetime']).map(\n    lambda x: x.total_seconds()\n)\ndf_train['log_trip_duration'] = np.log1p(df_train['trip_duration'].values)\n\ncnd = np.abs(df_train['check_trip_duration'].values  - df_train['trip_duration'].values) > 1\nduration_difference = df_train[cnd]\n\nif len(duration_difference[['pickup_datetime', 'dropoff_datetime', 'trip_duration', 'check_trip_duration']]) == 0:\n    print('Trip_duration and datetimes are ok.')\nelse:\n    print('Ooops.')","aa770c16":"common_cols = [\n    'vendor_id', \n    'pickup_datetime', \n    'passenger_count', \n    'pickup_longitude', 'pickup_latitude', \n    'dropoff_longitude', 'dropoff_latitude',\n    'store_and_fwd_flag',\n]\n\nX_y_train_h = h2o.H2OFrame(\n    pd.concat(\n        [df_train[common_cols], df_train['log_trip_duration']],\n        axis='columns'\n    )\n)\n\nfor ft in ['vendor_id', 'store_and_fwd_flag']:\n    X_y_train_h[ft] = X_y_train_h[ft].asfactor()\n    \nX_y_train_h.describe()","e606598d":"# aml = H2OAutoML(\n#     max_runtime_secs=(3600 * 8),  # 8 hours\n#     max_models=None,  # no limit\n#     seed=SEED,\n# )\n\n# aml.train(\n#     x=common_cols,\n#     y='log_trip_duration',\n#     training_frame=X_y_train_h\n# )\n\n# lb = aml.leaderboard\n# model_ids = list(lb['model_id'].as_data_frame().iloc[:,0])\n# out_path = \".\"\n\n# for m_id in model_ids:\n#     mdl = h2o.get_model(m_id)\n#     h2o.save_model(model=mdl, path=out_path, force=True)\n\n# h2o.export_file(lb, os.path.join(out_path, 'aml_leaderboard.h2o'), force=True)\n","d57ece32":"models_path = \"..\/input\/h2o-automl-saved-models-regress\/\"\n\nlb = h2o.import_file(path=os.path.join(models_path, \"aml_leaderboard.h2o\"))\nlb.head(rows=10)","03e130c0":"from catboost import Pool, CatBoostRegressor, cv\n\ncb_model = CatBoostRegressor(\n    eval_metric='RMSE',\n    use_best_model=True,\n    random_seed=17\n)\n\ncv_data = cv(\n    Pool(df_train[common_cols], df_train['log_trip_duration'], cat_features=[0,7]),\n    cb_model.get_params(),\n    fold_count=5,\n    verbose=False\n)","c3286da6":"print(\"CatBoostRegressor: the best cv rmse is\", np.min(cv_data['test-RMSE-mean']))","ac0d9ed6":"The features `Month`, `DayofMonth`, `DayOfWeek`, `DepTime`, `Distance` can be represented as numbers. Let's convert those features to numerical type (a new feature `HourFloat` is added):","089971f0":"The features `UniqueCarrier`, `Origin`, `Dest`, `Route` should be categorical:","8cd682f1":"I think that H2O AutoML is worth a try. And I hope you have found this tutorial useful.\n\nThere are extremely useful \"H2O AutoML Pro Tips\" in the presentation \"Scalable Automatic Machine Learning in H2O\" mentioned in the References below.","0d7e305c":"The AutoML Stacked Ensembles use the GLM with non-negative weights as the default metalearner (combiner) algorithm. Let's examine the variable importance of the metalearner algorithm in the ensemble. This shows us how much each base learner is contributing to the ensemble. `Intercept` represents the constant term in a linear model.","8a4d9bc9":"**References**\n\n* [H2O.ai](https:\/\/www.h2o.ai\/)\n* [H2O AutoML documentation](http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html)\n* [AutoML Tutorial](https:\/\/github.com\/h2oai\/h2o-tutorials\/tree\/master\/h2o-world-2017\/automl): R and Python notebooks\n* Intro to AutoML + Hands-on Lab: [1 hour video](https:\/\/www.youtube.com\/watch?v=42Oo8TOl85I), [slides](https:\/\/www.slideshare.net\/0xdata\/intro-to-automl-handson-lab-erin-ledell-machine-learning-scientist-h2oai)\n* Scalable Automatic Machine Learning in H2O: [1 hour video](https:\/\/www.youtube.com\/watch?v=j6rqrEYQNdo), [slides](https:\/\/www.slideshare.net\/0xdata\/scalable-automatic-machine-learning-in-h2o-89130971)\n* [H2O for GPU](https:\/\/www.h2o.ai\/products\/h2o4gpu\/) (H2O4GPU)","e630639a":"![h2o.ai](https:\/\/avatars0.githubusercontent.com\/u\/1402695?s=200&v=4)","598dd135":"Default CatBoost's RMSE is slightly worse than that of the XGBoost model from the H2O AutoML run.","3134c3d5":"Interestingly, there is only one model at the leaderboard:","3ee073f1":"Among the individual models, XGBoost is the leader (auc = 0.749523) for this task. Best individual GBM has auc = 0.741785, best XRT has auc = 0.731317, best DRF has auc = 0.725166, best DNN has auc = 0.706676.\n\n`StackedEnsemble_AllModels` is usually the leader, `StackedEnsemble_BestOfFamily` is usually at the 2nd place. Let's look inside the `StackedEnsemble_AllModels`. It is an ensemble of all of the individual models in the AutoML run. ","186d32b9":"Some of the arguments for `H2OAutoML.train()` are the following:\n* `training_frame` -- the H2OFrame having the columns indicated by `x` and `y`\n* `x` -- list of feature column names in `training_frame`\n* `y` -- a column name indicating the target\n* `validation_frame` -- the H2OFrame with validation data (by default and when `nfolds` > 1, `validation_frame` will be ignored)\n* `leaderboard_frame` -- the H2OFrame with test data for scoring the leaderboard (optinal; by default (`leaderboard_frame=None`) the cross-validation metric on `training_frame` will be used to generate the leaderboard rankings)\n\nLet's take a look at the leaderboard:","db49169a":"Let's also introduce a new feature `Route` that is the concatenation of `Origin` and `Dest`:","757243ad":"Let's reproduce the result (auc) of a few best individual models.","08fa1de3":"Let's compare the result of the model `XGBoost_1_AutoML_20190417_212831` with that of the CatBoostRegressor with the default parameters.","ceefd700":"**Conclusion**","0b46b6ce":"`StackedEnsemble_BestOfFamily` shows the following:","9f5fcb7a":"We will use only `df_train` (perform 5-fold cross-validation on it). Convert the date- and time-related features to the `datetime` format; take the logarithm (`log(1 + x)`) of the target value (trip duration). After the logarithm transform, the distribution of the target variable is close to normal (see this [kernel](https:\/\/www.kaggle.com\/gaborfodor\/from-eda-to-the-top-lb-0-367)).","545e493e":"**Example 2: a regression task**","05007080":"**Automated machine learning (AutoML)** is the process of automating the end-to-end process of applying machine learning to real-world problems. In a typical machine learning application, the typical stages (and sub-stages) of work are the following:\n1. Data preparation\n  * data pre-processing\n  * feature engineering\n  * feature extraction\n  * feature selection\n2. Model selection\n3. Hyperparameter optimization (to maximize the performance of the final model)\n\nMany of these steps are often beyond the abilities of non-experts. **AutoML** was proposed as an artificial intelligence-based solution to the ever-growing challenge of applying machine learning. \n\nSome of the notable platforms tackling various stages of AutoML are the following:\n* [auto-sklearn](https:\/\/automl.github.io\/auto-sklearn\/stable\/) is a Bayesian hyperparameter optimization layer on top of [scikit-learn](https:\/\/scikit-learn.org\/).\n* [TPOT](https:\/\/github.com\/EpistasisLab\/tpot) (TeaPOT) is a Python library that automatically creates and optimizes full machine learning pipelines using genetic programming.\n* [TransmogrifAI](https:\/\/github.com\/salesforce\/TransmogrifAI) is a Scala\/SparkML library created by [Salesforce](http:\/\/salesforce.com\/) for automated data cleansing, feature engineering, model selection, and hyperparameter optimization.\n* [H2O AutoML](http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html) performs (simple) data preprocessing, automates the process of training a large selection of candidate models, tunes hyperparameters of the models and creates stacked ensembles.\n* [H2O Driverless AI](https:\/\/www.h2o.ai\/products\/h2o-driverless-ai\/) is a commercial software package that automates lots of aspects of machine learning applications. It has a strong focus on automatic feature engineering. \n\nAn overview of AutoML capabilities of H2O library is presented in this tutorial. The library can be installed simply by","1b0fbc4d":"In the cell below, I call `aml.train()`, save the leaderboard and all individual models. The running time is about 8 hours, so after running it once I saved the output files as a new dataset, connected the dataset to this kernel and commented out the code in the cell.","ad5802ac":"Let's consider a regression task from the [\"New York City Taxi Trip Duration\" competition](https:\/\/www.kaggle.com\/c\/nyc-taxi-trip-duration). The challenge is to build a model that predicts the total ride duration of taxi trips in New York City. The features include pickup time, geo-coordinates, number of passengers, and a few other variables.","aeeb0d17":"Pandas DataFrames should be converted to H2O dataframes before calling `H2OAutoML()`.\n\nNote: if you don't have to preprocess the data, you can get H2O dataframes directly from the data files by a call like `df = h2o.import_file(datafile_path)` (where `datafile_path` is a filesystem path or a URL).","ee462b27":"The CatBoostClassifier cross-validation auc result is 0.749009. This value falls between the 2nd (auc = 0.749523) and 3rd (auc = 0.749192) places among the individual models at the leaderboard.","92074ab7":"Let's train the CatBoostClassifier with the default parameters and compare its results with AutoML run results.","aec39cd7":"**Example 1: a classification task**","8de2c717":"We will not use the column `DepTime` anymore. Split the target column from the features columns in `train_df`:","fece0fe3":"Among the most important arguments (with their default values) of `H2OAutoML()` are the following:\n* `nfolds=5` -- number of folds for k-fold cross-validation (`nfolds=0` disables cross-validation)\n* `balance_classes=False` -- balance training data class counts via over\/under-sampling\n* `max_runtime_secs=3600` -- how long the AutoML run will execute (in seconds)\n* `max_models=None` -- the maximum number of models to build in an AutoML run (`None` means no limitation)\n* `include_algos=None` -- list of algorithms to restrict to during the model-building phase (cannot be used in combination with `exclude_algos` parameter; `None` means that all appropriate H2O algorithms will be used)\n* `exclude_algos=None` -- list of algorithms to skip during the model-building phase (`None` means that all appropriate H2O algorithms will be used)\n* `seed=None` -- a random seed for reproducibility (AutoML can only guarantee reproducibility if `max_models` or\n  early stopping is used because `max_runtime_secs` is resource limited, meaning that if the resources are\n  not the same between runs, AutoML may be able to train more models on one run vs another)\n\nH2O AutoML trains and cross-validates:\n* a default Random Forest (DRF), \n* an Extremely-Randomized Forest (XRT),\n* a random grid of Generalized Linear Models (GLM),\n* a random grid of XGBoost (XGBoost),\n* a random grid of Gradient Boosting Machines (GBM), \n* a random grid of Deep Neural Nets (DeepLearning), \n* and 2 Stacked Ensembles, one of all the models, and one of only the best models of each kind.\n","e754797a":"Let's import the required packages and call `h2o.init()`. The specified arguments (`nthreads` and `max_mem_size`) are optional.","1260a218":"I have run the cell below (~8 hours), saved all models and the leaderboard, then commented out the code:","75f0a793":"Let's apply the power of H2O AutoML to the [\"Flight delays\" competition](https:\/\/www.kaggle.com\/c\/flight-delays-fall-2018) (it's a binary classification task) from [mlcourse.ai](https:\/\/mlcourse.ai\/).","8b5f2b6e":"Select the columns common to the train set and test set; convert `pd.DataFrame` to `H2OFrame`:"}}