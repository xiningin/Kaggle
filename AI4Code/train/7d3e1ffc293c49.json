{"cell_type":{"62f42168":"code","e1541d57":"code","457adc55":"code","769ec33a":"code","2921516d":"code","d818f03d":"code","56145af7":"code","9aafd22d":"code","a56c081c":"code","70e8985f":"code","fb44091c":"code","817a70bc":"code","4a94ae21":"code","59e81127":"code","bfd82833":"code","4b564ee4":"code","392c1952":"code","e1c2c5c8":"code","9b3dd28d":"code","355d1070":"code","531a25c6":"code","992c0d24":"code","5fff7a64":"code","3eea8517":"code","9071a400":"code","9185c558":"code","52695afc":"code","b9f54082":"code","a2ba770e":"code","2a34be82":"code","25841532":"code","4ca26e29":"code","5a105d1b":"code","dbdd2900":"code","a97bd0d0":"code","39cd8f5c":"code","030adc81":"code","ba846f17":"code","8dec1109":"code","4b14e605":"code","98406d12":"code","8adb4dc9":"code","27ac483d":"code","6475f2cb":"code","78bba41e":"code","a5aa1e4a":"code","d70614fd":"code","e28941d6":"code","6c310590":"code","214318d7":"code","08e988ee":"code","9ae53e90":"code","45c3b810":"code","531b2cbf":"code","0379db4c":"code","504c40e7":"code","683af897":"code","eae1dbff":"code","84e3118e":"code","06290239":"code","3ba04322":"code","397ab4d1":"code","d8999fd7":"code","74d787f1":"code","e8570b64":"code","6efdb374":"code","5c4f9e3a":"code","a8853b3f":"code","0f488c27":"code","261ee3cc":"code","adde26b8":"code","44d15aca":"code","61615700":"code","717254ad":"code","0a519c71":"code","ab3ec3af":"code","41c0c2c1":"code","2645b32f":"code","c2450764":"code","704a3b3e":"code","d8f80b6e":"code","c3f64d5c":"code","54aacb7c":"code","212e5035":"code","f65dafd5":"code","4e3cdc53":"code","7496f2fd":"code","2a48354f":"code","51cf1047":"code","ed329836":"code","fa10a79c":"code","33043b38":"code","6de27b30":"code","0c1496e0":"code","67c124b0":"code","3ad9c8cf":"code","d02c0156":"code","d690fc20":"code","1621f3b3":"code","21b38ba1":"code","cd0bfd89":"code","5350dc3e":"code","bb08e497":"code","cdf43b47":"code","4118aba2":"markdown","d8eb474a":"markdown","9880a5b5":"markdown","d51a40f2":"markdown","041e0f6b":"markdown","ba402b61":"markdown","a4287ce7":"markdown","28184b43":"markdown","6a47b08c":"markdown","840a56f5":"markdown","7475979b":"markdown","15d27396":"markdown","301a68ba":"markdown","9aa95d02":"markdown","d87dc746":"markdown","7f719459":"markdown","6c01933d":"markdown","280df852":"markdown","b15e358e":"markdown","1095047b":"markdown","e1c1dfd6":"markdown","596f5dc0":"markdown","bbeec902":"markdown","441b8bca":"markdown","5d6c2ed7":"markdown","dcd15594":"markdown","33a060ea":"markdown","85772639":"markdown","0073363e":"markdown","9128059b":"markdown","3ceccbc7":"markdown","c28449e3":"markdown","a0fdecff":"markdown","268f8120":"markdown","41229ee7":"markdown","7c37df90":"markdown","8bfeca47":"markdown","54dc24fb":"markdown","1f4f3997":"markdown","2ce0ea48":"markdown","91f045d2":"markdown","237d1446":"markdown","1c7ce616":"markdown","7e203842":"markdown","e76ce5e4":"markdown","3c29c963":"markdown","f024b2a3":"markdown","597e6376":"markdown","5696e912":"markdown","a1e1a370":"markdown","ba2c3ede":"markdown","c04b43ce":"markdown","742ca738":"markdown","87603536":"markdown","f8f59fea":"markdown","efb0efc7":"markdown","c770eae8":"markdown","de1b4fac":"markdown"},"source":{"62f42168":"# importing libraries\n%matplotlib inline \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats as st\nimport seaborn as sns\nimport re\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nimport missingno as msno\nfrom math import* \nfrom reportlab.lib.styles import ParagraphStyle\nimport statsmodels.api as sm\nfrom statsmodels.sandbox.regression.predstd import wls_prediction_std\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error","e1541d57":"# importing the dataset\ndf=pd.read_csv(\"HousePrices_HalfMil.csv\", decimal = ',')","457adc55":"df.head()","769ec33a":"df.describe()","2921516d":"top_housing_prices = df.sort_values('Prices',ascending=False)\n# Look at top 20\ntop_housing_prices[['Prices','Area']].head(20)","d818f03d":"fig, ax = plt.subplots(figsize=(16,6))\nsns.barplot(x='Area', y='Prices', data=top_housing_prices.head(33), palette='Set1')\nax.set_xlabel(ax.get_xlabel(), labelpad=15)\nax.set_ylabel(ax.get_ylabel(), labelpad=30)\nax.xaxis.label.set_fontsize(16)\nax.yaxis.label.set_fontsize(16)\nplt.xticks(rotation=90)\nplt.show()","56145af7":"fig, ax = plt.subplots(figsize=(16,6))\nsns.barplot(x='Area', y='Prices', data=top_housing_prices.tail(33), palette='Set1')\nax.set_xlabel(ax.get_xlabel(), labelpad=15)\nax.set_ylabel(ax.get_ylabel(), labelpad=30)\nax.xaxis.label.set_fontsize(16)\nax.yaxis.label.set_fontsize(16)\nplt.xticks(rotation=90)\nplt.show()","9aafd22d":"total = df.isnull().sum()[df.isnull().sum() != 0].sort_values(ascending = False)\npercent = pd.Series(round(total\/len(df)*100,2))\npd.concat([total, percent], axis=1, keys=['total_missing', 'percent'])","a56c081c":"print(np.isnan(df['Prices']))","70e8985f":"msno.matrix(df)","fb44091c":"df.skew()","817a70bc":"y = df['Prices']\nplt.figure(2); plt.title('Normal')\nsns.distplot(y, kde=False, fit=st.norm)","4a94ae21":"df.kurt()","59e81127":"sns.distplot(df['Prices'])","bfd82833":"df.corr()","4b564ee4":"plt.figure(figsize=(16,12))\nsns.heatmap(data=df.iloc[:,:].corr(),annot=True,fmt='.2f',cmap='coolwarm')\nplt.show()","392c1952":"xs = df[['Area','Garage','FirePlace','Baths','White Marble','Black Marble','Indian Marble','Floors','City','Solar','Electric','Fiber','Glass Doors','Swiming Pool','Garden']]\nys = df['Prices']\nlen(xs), len(ys)","e1c2c5c8":"x_train, x_test, y_train, y_test = train_test_split(xs,ys,test_size = 0.2, random_state = 4)","9b3dd28d":"x_train.head()","355d1070":"regr = LinearRegression()\nregr.fit(x_train, y_train)","531a25c6":"regr.intercept_","992c0d24":"print('Coefficients: ', regr.coef_)\nprint(\"Mean Squared Error: %.2f\"\n     % np.mean((regr.predict(x_test) - y_test) **2))\nprint ('Variance Score: %.2f'% regr.score(x_test,y_test))\nprint('Score'%regr.score(x_train,y_train))","5fff7a64":"names = [i for i in list(xs)]\nprint(names)","3eea8517":"#style.use(\"bmh\")\nplt.scatter(regr.predict(x_test),y_test)\nplt.show()","9071a400":"print(xs.shape)\n","9185c558":"import statsmodels.formula.api as sm\nxs = np.append(arr = np.ones((500000,1)).astype(int), values = xs,axis =1)\ndef backwardElimination(x, sl):\n    numVars = len(x[0])\n    for i in range(0, numVars):\n        reg_OLS = sm.OLS(ys, x).fit()\n        maxVar = max(reg_OLS.pvalues)\n        if maxVar > sl:\n            for j in range(0, numVars - i):\n                if (reg_OLS.pvalues[j].astype(float) == maxVar):\n                    x = np.delete(x, j, 1)\n    reg_OLS.summary()\n    return x\n\nSL = 0.05\nX_opt = xs[:, [0, 1, 2, 3, 4, 5,6,7,8,9,10,11,12,13,14]]\nX_Mod = backwardElimination(X_opt, SL)\ne_df = pd.DataFrame(X_Mod)\ne_df","52695afc":"model1=sm.OLS(y_train,x_train)","b9f54082":"result = model1.fit()","a2ba770e":"print(result.summary())","2a34be82":"a= cross_val_score(regr,x_train,y_train,cv=10)\nb = (np.sqrt(a).mean())\nprint(b)","25841532":"xs = df[['Area','Garage','FirePlace','Baths','Black Marble','Indian Marble','Floors','City']]\nys = df['Prices']\nlen(xs), len(ys)","4ca26e29":"x_train, x_test, y_train, y_test = train_test_split(xs,ys,test_size = 0.2, random_state = 4)","5a105d1b":"regr = LinearRegression()\nregr.fit(x_train, y_train)","dbdd2900":"regr.intercept_","a97bd0d0":"print('Coefficients: ', regr.coef_)\nprint(\"Mean Squared Error: %.2f\"\n     % np.mean((regr.predict(x_test) - y_test) **2))\nprint ('Variance Score: %.2f'% regr.score(x_test,y_test))\nprint(regr.score(x_train,y_train))","39cd8f5c":"#style.use(\"bmh\")\nplt.scatter(regr.predict(x_test),y_test)\nplt.show()","030adc81":"import statsmodels.formula.api as sm\nxs = np.append(arr = np.ones((500000,1)).astype(int), values = xs,axis =1)\ndef backwardElimination(x, sl):\n    numVars = len(x[0])\n    for i in range(0, numVars):\n        reg_OLS = sm.OLS(ys, x).fit()\n        maxVar = max(reg_OLS.pvalues)\n        if maxVar > sl:\n            for j in range(0, numVars - i):\n                if (reg_OLS.pvalues[j].astype(float) == maxVar):\n                    x = np.delete(x, j, 1)\n    reg_OLS.summary()\n    return x\n\nSL = 0.05\nX_opt = xs[:, [0, 1, 2, 3, 4, 5,6,7]]\nX_Mod = backwardElimination(X_opt, SL)\ne_df = pd.DataFrame(X_Mod)\ne_df","ba846f17":"model1=sm.OLS(y_train,x_train)","8dec1109":"result = model1.fit()","4b14e605":"print(result.summary())","98406d12":"print(np.sqrt(cross_val_score(regr,x_train,y_train,cv=10)).mean())","8adb4dc9":"xs = df[['Area','Garage','FirePlace','Baths','Floors','City','Electric','Fiber','Glass Doors']]\nys = df['Prices']\nlen(xs), len(ys)","27ac483d":"x_train, x_test, y_train, y_test = train_test_split(xs,ys,test_size = 0.2, random_state = 4)","6475f2cb":"regr = LinearRegression()\nregr.fit(x_train, y_train)\ny_pred = regr.predict(x_test)","78bba41e":"regr.intercept_","a5aa1e4a":"print('Coefficients: ', regr.coef_)\nprint(\"Mean Squared Error: %.2f\"\n     % np.mean((regr.predict(x_test) - y_test) **2))\nprint ('Variance Score: %.2f'% regr.score(x_test,y_test))\nprint(regr.score(x_train,y_train))","d70614fd":"#style.use(\"bmh\")\nplt.scatter(regr.predict(x_test),y_test)\nplt.show()","e28941d6":"import statsmodels.formula.api as sm\nxs = np.append(arr = np.ones((500000,1)).astype(int), values = xs,axis =1)\ndef backwardElimination(x, sl):\n    numVars = len(x[0])\n    for i in range(0, numVars):\n        reg_OLS = sm.OLS(ys, x).fit()\n        maxVar = max(reg_OLS.pvalues)\n        if maxVar > sl:\n            for j in range(0, numVars - i):\n                if (reg_OLS.pvalues[j].astype(float) == maxVar):\n                    x = np.delete(x, j, 1)\n    reg_OLS.summary()\n    return x\n\nSL = 0.05\nX_opt = xs[:, [0, 1, 2, 3, 4, 5,6,7,8]]\nX_Mod = backwardElimination(X_opt, SL)\ne_df = pd.DataFrame(X_Mod)\ne_df","6c310590":"model1=sm.OLS(y_train,x_train)","214318d7":"result = model1.fit()","08e988ee":"print(result.summary())","9ae53e90":"print(np.sqrt(cross_val_score(regr,x_train,y_train,cv=10)).mean())","45c3b810":"xs = df[['Area','Garage','FirePlace','Baths','White Marble','Black Marble','Indian Marble','Floors','City','Solar','Electric','Fiber','Glass Doors','Swiming Pool','Garden']]\nys = df['Prices']\nlen(xs), len(ys)\nx_train, x_test, y_train, y_test = train_test_split(xs,ys,test_size = 0.2, random_state = 4)","531b2cbf":"from statsmodels.stats import outliers_influence\ndef variance_IF(x):\n    vif=vif = pd.DataFrame()\n    vif[\"VIF Factor\"] = [outliers_influence.variance_inflation_factor(x.values, i) for i in range(x.shape[1])]\n    vif[\"features\"] = x.columns\n    return vif\n\nprint(variance_IF(xs))","0379db4c":"xs = df[['Area','Garage','FirePlace','Baths','Floors','City','Solar','Electric','Fiber','Glass Doors','Swiming Pool','Garden']]\nys = df['Prices']\nlen(xs), len(ys)\nx_train, x_test, y_train, y_test = train_test_split(xs,ys,test_size = 0.2, random_state = 4)","504c40e7":"regr = LinearRegression()\nregr.fit(x_train, y_train)\ny_pred = regr.predict(x_test)","683af897":"regr.intercept_","eae1dbff":"print('Coefficients: ', regr.coef_)\nprint(\"Mean Squared Error: %.2f\"\n     % np.mean((regr.predict(x_test) - y_test) **2))\nprint ('Variance Score: %.2f'% regr.score(x_test,y_test))\nprint(regr.score(x_train,y_train))","84e3118e":"corr_df = x_train.corr(method = 'pearson')\nprint(\"------------Create a Corelation plot----------------\")\nmask = np.zeros_like(corr_df)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr_df, cmap='RdYlGn_r', vmax = 1.0, vmin=-1.0, mask = mask, linewidths = 2.5)\nplt.yticks(rotation = 0)\nplt.xticks(rotation = 90)\nplt.show()","06290239":"print(np.sqrt(cross_val_score(regr,x_train,y_train,cv=10)).mean())","3ba04322":"df['interaction1'] = df['Indian Marble']*df['Black Marble']*df['White Marble']\nxs = df[['Area','Garage','FirePlace','Baths','interaction1','Floors','City','Solar','Electric','Fiber','Glass Doors','Swiming Pool','Garden']]\nys = df['Prices']\nlen(xs), len(ys)","397ab4d1":"x_train, x_test, y_train, y_test = train_test_split(xs,ys,test_size = 0.2, random_state = 4)","d8999fd7":"regr = LinearRegression()\nregr.fit(x_train, y_train)","74d787f1":"regr.intercept_","e8570b64":"print('Coefficients: ', regr.coef_)\nprint(\"Mean Squared Error: %.2f\"\n     % np.mean((regr.predict(x_test) - y_test) **2))\nprint ('Variance Score: %.2f'% regr.score(x_test,y_test))\nprint(regr.score(x_train,y_train))","6efdb374":"model1=sm.OLS(y_train,x_train)","5c4f9e3a":"result_interaction = model1.fit()","a8853b3f":"print(result_interaction.summary())","0f488c27":"scaler = MinMaxScaler()\nscaler.fit(x_train)\nx_train_scaled = scaler.fit_transform(x_train)\nx_test_scaled = scaler.transform(x_test)\nlinridge = Ridge(alpha= this_alpha).fit(x_train_scaled,y_train)\nprint('ridge regression linear model intercept : {}'.format(linridge.intercept_))\nprint('ridge regression linear model coeff : {}'.format(linridge.coef_))\nprint('R-squared score (training) : {:.3f}'.format(linridge.score(x_train_scaled,y_train)))\nprint('R- squared score (test) : {:.3f}'.format(linridge.score(x_test_scaled,y_test)))\nprint('Number of non zero features:{}'.format(np.sum(linridge.coef_!=0)))","261ee3cc":"scaler = MinMaxScaler()\nscaler.fit(x_train)\nprint('Effect of Alpha regularisation parameter')\nfor this_alpha in [0,0.2,0.4,0.6,0.8,1.0] :\n    linridge = Ridge(alpha= this_alpha).fit(x_train_scaled,y_train)\n    r2_train = linridge.score(x_train_scaled,y_train)\n    r2_test =  linridge.score(x_test_scaled,y_test)\n    num_coeff_bigger = np.sum(abs(linridge.coef_)>1.0)\n    print('Alpha = {:2f}\\n\\num abs(coeff)>1.0:{},r-squared training: {:.2f}, r-squared test: {:.2f}\\n'.format(this_alpha, num_coeff_bigger, r2_train, r2_test))","adde26b8":"#Logistic Regression\nmedium = (df['Prices'].max()-df['Prices'].min())\/2\ndf['Price_Logistic'] = np.where(df['Prices']>=medium, 1, 0)\nprint(medium)","44d15aca":"df.tail()","61615700":"sns.countplot(x=\"Price_Logistic\", data = df)","717254ad":"df.tail()","0a519c71":"xs_logi = df[['Area','Garage','FirePlace','Baths','White Marble','Black Marble','Indian Marble','Floors','City','Solar','Electric','Fiber','Glass Doors','Swiming Pool','Garden','Prices']]\n\nys_logi = df['Price_Logistic']","ab3ec3af":"x_train1, x_test1, y_train1, y_test1 = train_test_split(xs_logi,ys_logi,test_size=0.33, random_state= 1)","41c0c2c1":"sc = StandardScaler()\nx_train1 = sc.fit_transform(x_train1)\nx_test1 = sc.transform(x_test1)","2645b32f":"logmodel = LogisticRegression()\nlogmodel.fit(x_train1,y_train1)","c2450764":"prediction1 = logmodel.predict(x_test1)","704a3b3e":"#how my mdel is performing\nclassification_report(y_test1,prediction1)","d8f80b6e":"confusion_matrix(y_test1,prediction1)","c3f64d5c":"accuracy_score(y_test1,prediction1)\n#quiet good","54aacb7c":"print(classification_report(y_test1,prediction1))","212e5035":"print(np.sqrt(cross_val_score(logmodel,xs_logi,ys_logi,cv=10)).mean())","f65dafd5":"xs_logi2 = df[['Area','Garage','FirePlace','Baths','Floors','City','Solar','Electric','Fiber','Glass Doors','Swiming Pool','Garden']]\n\nys_logi2 = df['Price_Logistic']\nx_train2, x_test2, y_train2, y_test2 = train_test_split(xs_logi2,ys_logi2,test_size=0.33, random_state= 1)","4e3cdc53":"sc = StandardScaler()\nx_train2 = sc.fit_transform(x_train2)\nx_test2 = sc.transform(x_test2)","7496f2fd":"logmodel2 = LogisticRegression(random_state = 0)\nlogmodel2.fit(x_train2,y_train2)","2a48354f":"y_pred2 = logmodel2.predict(x_test2)","51cf1047":"accuracy_score(y_test2,y_pred2)*100","ed329836":"logit_model2 = sm.Logit(y_train2, x_train2)\nresult2 = logit_model2.fit()\nprint(result2.summary())","fa10a79c":"print(np.sqrt(cross_val_score(logmodel2,xs_logi2,ys_logi2,cv=10)).mean())","33043b38":"xs_logi3 = df[['Area','Garage','FirePlace','Baths','White Marble','Floors','City','Solar','Glass Doors','Swiming Pool','Garden']]\n\nys_logi3 = df['Price_Logistic']\nx_train3, x_test3, y_train3, y_test3 = train_test_split(xs_logi3,ys_logi3,test_size=0.33, random_state= 1)","6de27b30":"sc = StandardScaler()\nx_train3 = sc.fit_transform(x_train3)\nx_test3 = sc.transform(x_test3)","0c1496e0":"logmodel3 = LogisticRegression(random_state = 0)\nlogmodel3.fit(x_train3,y_train3)","67c124b0":"y_pred3 = logmodel3.predict(x_test3)","3ad9c8cf":"accuracy_score(y_test3,y_pred3)*100","d02c0156":"logit_model3 = sm.Logit(y_train3, x_train3)\nresult3 = logit_model3.fit()\nprint(result3.summary())","d690fc20":"##Cross validation","1621f3b3":"print(np.sqrt(cross_val_score(logmodel3,xs_logi3,ys_logi3,cv=10)).mean())","21b38ba1":"df['interaction_logi'] = df['White Marble'] * df['Black Marble'] *df['Indian Marble']\nxs_logi4 = df[['Area','Garage','FirePlace','Baths','interaction_logi','Floors','City','Solar','Electric','Fiber','Glass Doors','Swiming Pool','Garden','Prices']]\nys_logi4 = df['Price_Logistic']","cd0bfd89":"x_train4, x_test4, y_train4, y_test4 = train_test_split(xs_logi4,ys_logi4,test_size=0.33, random_state= 1)\nsc = StandardScaler()\nx_train4 = sc.fit_transform(x_train4)\nx_test4 = sc.transform(x_test4)","5350dc3e":"logmodel4 = LogisticRegression(random_state = 0)\nlogmodel4.fit(x_train4,y_train4)","bb08e497":"y_pred4 = logmodel4.predict(x_test4)","cdf43b47":"accuracy_score(y_test4,y_pred4)*100","4118aba2":"**The above graph shows that there are no missing values**","d8eb474a":" - 1)yes it is significant as most of the predictor value matches the test variables, also the accuracy is good.**\n - 2)No assumption is being violated except multicolinearity**\n - 3)Yes the model makes sense, -Area, baths-it is continuos value, 'Garbage' and 'City' - multi class categorical variable, rest other columns are binary categorical variable.**\n - 4)Yes cross validated the model**\n - 5)the probability is 83.68**","9880a5b5":"## Cross Validation","d51a40f2":"**Thus the 1st model is our best model**","041e0f6b":"### Interaction Effect","ba402b61":"**Heat map represents the corealtion in a better way**\nHere we can see that White Marble,Indian Marble, Fiber and Floors are max corelated\nIndian Marble and White Marble are corelated with each other as well","a4287ce7":"**The p values in any model should be less than 0.05, we have perfect p values, we can remove garden(p value = 0.02) or Swiming Pool with negative t values, We can remove Swiming Pool**","28184b43":"### Model 3 of Logistic Regression","6a47b08c":"**Through above code we can see that no element here is null, thus we dont have to drop any row**","840a56f5":" - 1) Yes, The Relationship is significant as the p values is less than 0.05 and the regression line differs significantly from 0.\n - 2.a)linearity and additivity**- Not Violated\n - 2.b)multi collinearity** - it is there it will be solved later in the code \n - 2.c)homoscedasticity**- not violated\n - 2.d)Normality**-not violated(already shown in above graph)\n - 3)Yes the model makes sense-Area, baths  -it is continuos value, 'Garbage' and 'City' - multi class categorical variable, rest other columns are binary categorical variable.**\n - 4)yes cross validated it did well\n - 5)The AIC, BIC should decrease but it increased here as compared to previous model a, R square increased(Thus model 1 was better","7475979b":" - 1) Yes, The Relationship is significant as the p values is less than 0.05 and the regression line differs significantly from 0.\n - 2.a)linearity and additivity**- Not Violated\n - 2.b)multi collinearity** - it is there it will be solved later in the code \n - 2.c)homoscedasticity**- not violated\n - 2.d)Normality**-not violated(already shown in above graph)\n - 3)Yes the model makes sense-Area, baths and Prices -it is continuos value, 'Garbage' and 'City' - multi class categorical variable, rest other columns are binary categorical variable.**\n - 4)yes cross validated it did well\n - 5)The AIC, BIC decreased from 2nd model but increased from 1st. Thus 1st is the best model, R square increased from 2nd but still decreased from 1st \n - The best model is Model 1","15d27396":"**Yes there was synergy in the tested terms**","301a68ba":"**After Performing regularisation the performance didnt improve**","9aa95d02":"**White and Indian Marble are corelated with each other and are inter corelated as well**","d87dc746":"**The above chart shows the corelation of each coloumn with the other column**","7f719459":"### Interaction Effect","6c01933d":"**Making a barplot of top 20 Prices of houses and last 20 houses's prices according to its Area**","280df852":"### Multicolinearity","b15e358e":"## Cross Validation","1095047b":"**Taking the general idea about the dataset using head()**","e1c1dfd6":"**We do standard Scalar when the dataset is large enough(say more that 10k) to improve the accuracy**","596f5dc0":"# License\n**Copyright\nJyoti Goyal**\n**THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.**","bbeec902":"**Assigning values and xs = independent value and ys = dependent or target value**","441b8bca":" - 1)yes it is significant as most of the predictor value matches the test variables, also the accuracy is good.**\n - 2)No assumption is being violated except multicolinearity**\n - 3)Yes the model makes sense, -Area, baths and Prices -it is continuos value, 'Garbage' and 'City' - multi class categorical variable, rest other columns are binary categorical variable.**\n - 4)Yes cross validated the model**\n - 5)the probability is 99.99**","5d6c2ed7":"## Regularization for Linear Regression (Ridge)","dcd15594":"# Citation\n - https:\/\/github.com\/ResidentMario\/missingno\n - https:\/\/towardsdatascience.com\/visualizing-data-with-pair-plots-in-python-f228cf529166 \n - https:\/\/stackoverflow.com\/\n - https:\/\/www.kaggle.com\/ekami66\/detailed-exploratory-data-analysis-with-python \n - https:\/\/www.datacamp.com\/\n - https:\/\/www.datascience.com\/blog\/introduction-to-correlation-learn-data-science-tutorials\n - https:\/\/www.edureka.com\n - https:\/\/www.dummies.com\/programming\/big-data\/data-science\/data-science-how-to-create-interactions-between-variables-with-python\/","33a060ea":"**The p value of Garden is greater than 0.05, removing the column garden to make our model more accurate** ","85772639":"**Adding another column in our dataset as Price Logistic which the target column- 1 = High Price, 0 = Low Price**","0073363e":"# Contribution\n**My contribution is 60% while 40% is the materials i have taken from internet**","9128059b":" - 1) Yes, The Relationship is significant as the p values is less than 0.05 and the regression line differs significantly from 0.\n - 2.a)linearity and additivity**- Not Violated\n - 2.b)multi collinearity** - it is there it will be solved later in the code \n - 2.c)homoscedasticity**- not violated\n - 2.d)Normality**-not violated(already shown in above graph)\n - 3)Yes the model makes sense-Area, baths and Prices -it is continuos value, 'Garbage' and 'City' - multi class categorical variable, rest other columns are binary categorical variable.**\n - 4)yes cross validated it did well\n - 5)The AIC, BIC should decrease in a better model which we can check later, R square is perfect\n\n\n","3ceccbc7":"**The purpose of my regression and exploratory data analysis is to get an insight of the housing prices with respect to its other attributes. Here I am studying the dataset \u201c Housing Prices\u201d by Kaggle. The main focus is on  analyzing the factors which are affecting the prices of houses from the given 500000 houses with their prices and other columns which will be taken into consideration as factors which might affect the prices.**","c28449e3":"### Model 2 for Linear Regression","a0fdecff":"**Subplots which shows the distribution of high and low price**","268f8120":" - 1)yes it is significant as most of the predictor value matches the test variables, also the accuracy is good.**\n - 2)No assumption is being violated except multicolinearity**\n - 3)Yes the model makes sense, -Area, baths -it is continuos value, 'Garbage' and 'City' - multi class categorical variable, rest other columns are binary categorical variable.**\n - 4)Yes cross validated the model**\n - 5)the probability is 86.34(Model 1 was better)** ","41229ee7":"**Checking the columns and the number of missing entries in them\nExamining this is important as because of this the dataset can lose expressiveness, which can lead to weak and biased analyses**","7c37df90":"### Cross Validation","8bfeca47":"**Just taking basic summary statistics of the data, which gives counts, min, max, quantiles, std deviation, mean of the data**","54dc24fb":"**Checking the first 5 rows of training set before further analysis**","1f4f3997":"# Conclusion\n**The best linear regression model was model 1, and the best logistic regression model was model 1 as well.**","2ce0ea48":"**Clearly the accuracy has decreased**","91f045d2":"# Linear Regression","237d1446":"# Logistic Regression","1c7ce616":"**Checking the top 20 Houses with their areas and prices**","7e203842":"**Applying Linear Regression and fitting that in our training dataset**","e76ce5e4":"**Checking the intercept after applying Linear regression to ur model**","3c29c963":"**The higher the kurtosis the longer is the tail of the histogram which can be seen in the above graph of skewness, but here the kurtosis ins't higher for any column**","f024b2a3":"### Cross Validation","597e6376":"### Model 2 of Logistic Regression","5696e912":"# Abstract","a1e1a370":"**Skewness tells how assymmetric data is spread around the mean.. If the right tail of histogram then positive skew and negative tail is negative skew**","ba2c3ede":"**Dist plot shows hw symettrically the data is spread as we are doing regression we need to check for this, we can see the data is symmetrical**","c04b43ce":"### Cross Validation","742ca738":"**Importing the libraries and reading the dataset from CSV file**","87603536":"**Finding the coeffecient, Mean squared error and the varience score**","f8f59fea":"### Model 3 of Linear Regression","efb0efc7":" - 1) Yes, there was multi-colinearity  in the model, which has been removed now.**\n - 2)yes, the predictor variables are independent of all other predictor variables.**\n - 3)The most significant predictor variables are Floors, Fiber, White Marbles and City, The insignificant ones are already excluded**\n - 4)Performed Cross validation**","c770eae8":"**After appling interaction the performance is better than model 2 and 3 but not model 1**","de1b4fac":"**Using tarin_test_split to train and test the data, training with 80% of data and testing with remaining 20 %**"}}