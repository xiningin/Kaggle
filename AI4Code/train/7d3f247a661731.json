{"cell_type":{"7394a3f8":"code","d44b3880":"code","e77de0b3":"code","041973c8":"code","4c4ba04a":"code","a2b16c93":"code","24c84af8":"code","d3e8eccd":"code","2ecfa47d":"code","882cbb4a":"code","cfa7bf31":"code","de7df383":"code","4c5d81e6":"code","66d0203c":"code","4fc43740":"code","696802a8":"code","2ac45016":"code","ad8c8f33":"code","8943c004":"code","2b00344d":"code","6c1c5a64":"code","532af1e7":"code","82453fd6":"code","d69adea1":"code","2cada5ed":"code","429ecb30":"code","c5822577":"markdown","ef935fb1":"markdown","1e096ff0":"markdown","603e06ee":"markdown","8392509f":"markdown","ac09d6df":"markdown","874fd64c":"markdown"},"source":{"7394a3f8":"import os, time\nimport pandas\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom kaggle_datasets import KaggleDatasets\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\n\n# We'll use a tokenizer for the BERT model from the modelling demo notebook.\n!pip install bert-tensorflow\nfrom bert.tokenization import FullTokenizer\n\nprint(tf.version.VERSION)","d44b3880":"PATH = \"..\/input\/jigsaw-multilingual-toxic-comment-classification\"\n\n#herre is am using data from bert_multi dataset which has 'vocab.txt' and other required for bert.\n# https:\/\/www.kaggle.com\/kivlichangoogle\/bert-multi\nBERT_PATH = \"..\/input\/bert-multi\/bert_multi_from_tfhub\"\n\n#training data\ntrain1 = pd.read_csv(f\"{PATH}\/jigsaw-toxic-comment-train.csv\")\n\n#we will use only train1 data for now \n# train2 = pd.read_csv(f\"{PATH}\/jigsaw-unintended-bias-train.csv\")\n\ntest = pd.read_csv(f\"{PATH}\/test.csv\")\nvalidation = pd.read_csv(f\"{PATH}\/validation.csv\")  \n\n#sample submission\nsample = pd.read_csv(f\"{PATH}\/sample_submission.csv\")\n\nprint(\"train 1 : \",train1.shape)\nprint(\"test : \",test.shape)\nprint(\"validation: \",validation.shape)\nprint(\"samaple : \", sample.shape)","e77de0b3":"train1.head()","041973c8":"test.head()","4c4ba04a":"def get_tokenizer(bert_path = BERT_PATH):\n    \n    #load bert_layer from BERT_PATH\n    bert_layer = tf.saved_model.load(bert_path)\n    bert_layer = hub.KerasLayer(bert_layer,trainable=False)\n    \n    #get the vocab file required for tokenizer\n    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n    tf.gfile = tf.io.gfile\n    \n    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n\ntokenizer = get_tokenizer()","a2b16c93":"#demo of what tokenizer actually does\nprint(train1.comment_text[0])\n\ntokens = tokenizer.tokenize(train1.comment_text[0])\nprint(tokens)\n\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\nprint(token_ids)","24c84af8":"#maximum words allowed in single sentence including [CLS] AND [SEP]\nmax_length = 128\n\ndef comment_to_ids(comment,tokenizer=tokenizer,MAX_LEN = max_length):\n    \n    #tokenize the  sentence\n    tokens = tokenizer.tokenize(comment)\n    \n    #truncate the sentence\n    if len(tokens) > (MAX_LEN - 2): # -2 because [CLS] ans [SEP] are 2 default tokens to add\n        tokens = tokens[:(MAX_LEN -2)]\n    \n    token_ids = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens+[\"[SEP]\"])\n    \n    input_mask = [1] * len(token_ids)\n    \n    #padding sequence\n    padding_len = MAX_LEN - len(token_ids)\n    \n    token_ids.extend([0] * padding_len)\n    input_mask.extend([0] * padding_len)\n    \n    segment_ids = [0] * MAX_LEN\n    \n    return token_ids, input_mask, segment_ids","d3e8eccd":"%%time\ndef convert_comments_to_ids(data,MAX_LEN = max_length,text_label=\"comment_text\"):\n    \n    data[\"input_ids\"], data[\"input_mask\"],data[\"segment_ids\"]  = zip(*data[text_label].apply(comment_to_ids))\n    \n    return data\n\ntrain_ids_df = convert_comments_to_ids(train1)\n\n\n#need some help here \n#as comment_to_ids is returning three values\n#I have written it like this zip(*data[text_label].apply(comment_to_ids))\n# so Is this the best practice or there is any othr way to do it.","2ecfa47d":"%%time\n# converting various toxic label to one column toxicity\ndef get_toxic_label(comment):\n    toxic_labels = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']\n    \n    if comment[toxic_labels].any():\n        return 1 \n    else:\n        return 0\n\ntrain_ids_df[\"Toxicity\"] = train_ids_df.apply(get_toxic_label,axis=1)","882cbb4a":"X = train_ids_df[[\"input_ids\",\"input_mask\",\"segment_ids\"]]\ny = train_ids_df[\"Toxicity\"]","cfa7bf31":"X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.2,shuffle=True)","de7df383":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","4c5d81e6":"BERT_GCS_PATH = KaggleDatasets().get_gcs_path('bert-multi')\nBERT_GCS_PATH_SAVEDMODEL = BERT_GCS_PATH + \"\/bert_multi_from_tfhub\"","66d0203c":"def bert_model(bert_path=BERT_GCS_PATH_SAVEDMODEL,MAX_LEN=max_length,trainable_bert=True):\n    \n    input_word_ids = tf.keras.layers.Input(shape=(MAX_LEN,),dtype=tf.int32, name=\"input_word_ids\")\n    input_mask_ids = tf.keras.layers.Input(shape=(MAX_LEN,),dtype=tf.int32, name=\"input_mask_ids\")\n    segment_ids = tf.keras.layers.Input(shape=(MAX_LEN,),dtype=tf.int32, name=\"segment_ids\")\n    \n    bert_layer = tf.saved_model.load(bert_path)\n    bert_layer = hub.KerasLayer(bert_path,trainable=trainable_bert)\n    \n    pooled_output,_ = bert_layer([input_word_ids,input_mask_ids,segment_ids])\n    \n    output = tf.keras.layers.Dense(32, activation='relu')(pooled_output)\n    output = tf.keras.layers.Dense(1, activation='sigmoid', name='labels')(output)\n    \n    model = tf.keras.Model(inputs ={\"input_word_ids\":input_word_ids,\n                                    \"input_mask_ids\":input_mask_ids,\n                                    \"segment_ids\":segment_ids},outputs=output)\n    \n    return model","4fc43740":"AUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nEPOCHS = 3\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(((X_train[\"input_ids\"],X_train[\"input_mask\"],X_train[\"segment_ids\"]),y_train))\n    .repeat()\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n    )\nvalid_dataset = (\n     tf.data.Dataset\n    .from_tensor_slices(((X_val[\"input_ids\"],X_val[\"input_mask\"],X_val[\"segment_ids\"]),y_val))\n    .repeat()\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)","696802a8":"with strategy.scope():\n    bert_ = bert_model()\n\n    # Compile the model. Optimize using stochastic gradient descent.\n    bert_.compile(\n        loss=tf.keras.losses.BinaryCrossentropy(),\n        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n        metrics=[tf.keras.metrics.AUC()])\n\nbert_.summary()","2ac45016":"history = bert_.fit(\n    # Set steps such that the number of examples per epoch is fixed.\n    # This makes training on different accelerators more comparable.\n    train_dataset, steps_per_epoch=4000\/strategy.num_replicas_in_sync,\n    epochs=5, verbose=1, validation_data=valid_dataset,\n    validation_steps=100)","ad8c8f33":"import transformers\nfrom tokenizers import BertWordPieceTokenizer","8943c004":"tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n\n#this function creates three files given in output\ntokenizer.save_pretrained('.')\n\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt',lowercase=False)\nfast_tokenizer","2b00344d":"#this code does the tokenizing part and return ids\ndef fast_encode(texts, tokenizer, chunk_size=256, MAX_LEN=max_length):\n    \"\"\"\n    https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n    #truncating\n    tokenizer.enable_truncation(max_length=MAX_LEN)\n    \n    #padding\n    tokenizer.enable_padding(max_length=MAX_LEN)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","6c1c5a64":"%%time\nX = fast_encode(train1.comment_text.astype(str), fast_tokenizer, maxlen=max_length)\ny = train1[\"Toxicity\"]","532af1e7":"x_train,x_valid,y_test,y_valid = train_test_split(X,y,test_size=0.2,shuffle=0.2)","82453fd6":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)","d69adea1":"def bert_model_2(transformer, MAX_LEN=max_length):\n    \"\"\"\n    https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n    input_word_ids = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    \n    output = tf.keras.layers.Dense(32, activation='relu')(cls_token)\n\n    out = tf.keras.layers.Dense(1, activation='sigmoid')(output)\n    \n    model = tf.keras.models.Model(inputs=input_word_ids, outputs=out)\n    \n    return model","2cada5ed":"with strategy.scope():\n    \n    transformer_layer = (\n        transformers.TFDistilBertModel\n        .from_pretrained('distilbert-base-multilingual-cased')\n    )\n    \n    bert_ = bert_model_2(transformer_layer)\n\n    # Compile the model. Optimize using stochastic gradient descent.\n    bert_.compile(\n        loss=tf.keras.losses.BinaryCrossentropy(),\n        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n        metrics=[tf.keras.metrics.AUC()])\n\nbert_.summary()","429ecb30":"history2 = bert_.fit(\n    # Set steps such that the number of examples per epoch is fixed.\n    # This makes training on different accelerators more comparable.\n    train_dataset, steps_per_epoch=4000\/strategy.num_replicas_in_sync,\n    epochs=5, verbose=1, validation_data=valid_dataset,\n    validation_steps=100)","c5822577":"# Hugging Face","ef935fb1":"# LOok inTo tHe dAta","1e096ff0":"There are four basic steps performs while making BERT model.\n1. tokenizing the words and converting it to ids.\n2. adding bert layer to keras model\n3. training data\n4. making prediction","603e06ee":"It shows that tokenization in hugging face is superfast and <br\/>\nloading and training transformer is quit simple.","8392509f":"## Bert Model","ac09d6df":"## TPU Config","874fd64c":"# NLP- Challange (part 4)\n\n## Jigsaw Multilingual Challange.\n\n![image.png](attachment:image.png)\n\n[**Jigsaw**](https:\/\/jigsaw.google.com\/) is a unit of Google, which is aim towards \"Using technology to make internet safer\"<br\/>\n\n\"Disinformation campaigns are becoming more sophisticated, but so are the countermeasures.\"                                                                                    - Jigsaw \n                                                                                    \nJigsaw use forecasting technology to predicts threat in internet and solves it to maintaining sanity of internet.\n\nThis challange by **jigsaw** is aimed towards finding toxic comments which could destabalize internet.<br\/>\nWe are given training, testing, validation which contains sentences in different languages and they are classified in one of the<br\/>\nfollowing category \n\n1. toxic \n2. severe_toxic\n3. obscence\n4. threat\n5. insult\n6. identiy_threat\n7. none (all columns zero)\n\n\nwe will see the code using tensorflow and then using hugging face."}}