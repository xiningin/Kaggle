{"cell_type":{"5e1658db":"code","c0435d85":"code","7728c6df":"code","8079b155":"code","002478d5":"code","c7147f64":"code","640e98b0":"code","144db068":"code","45c5b0b9":"code","84d65d0c":"code","03167a03":"code","8b5620a1":"code","78ceb231":"code","86d7e589":"code","a6b67bfc":"code","838a7dde":"code","58e2d450":"code","2ae5ee6e":"code","90369d4b":"code","605a144c":"code","b01eafd3":"code","29c9dad6":"code","738c469c":"code","776e99bc":"code","5a4ab928":"code","ef94a505":"code","84cc1a3a":"code","c52f3dd8":"code","008d623c":"code","6e9688f1":"code","8cbc2848":"code","241268fe":"code","7df0fbb4":"code","a48f755d":"code","992e1dcf":"markdown","7a9ff85a":"markdown","9566ce88":"markdown","2e7f6dc6":"markdown","fd6d9e0b":"markdown","4bbda649":"markdown","f973d2e4":"markdown","f2e3957a":"markdown","b04b6339":"markdown","f41dcfbb":"markdown","50fe7df0":"markdown","df571ccb":"markdown","764e1d9b":"markdown","3793c798":"markdown","678983ba":"markdown"},"source":{"5e1658db":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport gc\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n# Any results you write to the current directory are saved as output.\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler\n\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.utils import class_weight\n\nfrom sklearn.metrics import accuracy_score, make_scorer\nfrom sklearn.metrics import roc_curve, auc, accuracy_score, cohen_kappa_score\nfrom sklearn.metrics import mean_squared_error, f1_score, confusion_matrix\n","c0435d85":"import tensorflow as tf\nimport tensorflow.keras as keras\n\n\nfrom tensorflow.keras.models import Sequential, Model\n\n#from tensorflow.keras.layers import InputLayer\nfrom tensorflow.keras.layers import LSTM, Bidirectional, add, concatenate, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Conv2DTranspose, AveragePooling1D, UpSampling1D\nfrom tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization, Activation, TimeDistributed\nfrom tensorflow.keras.layers import Multiply, Add, Concatenate, Flatten, Average, Lambda\n\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.callbacks import EarlyStopping, Callback, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.constraints import unit_norm, max_norm\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.utils import to_categorical\n#from tensorflow.keras.utils import np_utils\n\nfrom tensorflow.keras import backend as K","7728c6df":"## utils","8079b155":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        if col != 'time':\n            col_type = df[col].dtypes\n            if col_type in numerics:\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)  \n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ndef get_stats(df):\n    stats = pd.DataFrame(index=df.columns, columns=['na_count', 'n_unique', 'type', 'memory_usage'])\n    for col in df.columns:\n        stats.loc[col] = [df[col].isna().sum(), df[col].nunique(dropna=False), df[col].dtypes, df[col].memory_usage(deep=True, index=False) \/ 1024**2]\n    stats.loc['Overall'] = [stats['na_count'].sum(), stats['n_unique'].sum(), None, df.memory_usage(deep=True).sum() \/ 1024**2]\n    return stats\n\ndef print_header():\n    print('col         conversion        dtype    na    uniq  size')\n    print()\n    \ndef print_values(name, conversion, col):\n    template = '{:10}  {:16}  {:>7}  {:2}  {:6}  {:1.2f}MB'\n    print(template.format(name, conversion, str(col.dtypes), col.isna().sum(), col.nunique(dropna=False), col.memory_usage(deep=True, index=False) \/ 1024 ** 2))\n    \ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)    ","002478d5":"import tensorflow as tf\nimport keras.backend as K\n\ndef f1(y_true, y_pred):\n    y_pred = K.round(y_pred)\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K.epsilon())\n    r = tp \/ (tp + fn + K.epsilon())\n\n    f1 = 2*p*r \/ (p+r+K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)\n\ndef f1_loss(y_true, y_pred):\n    \n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K.epsilon())\n    r = tp \/ (tp + fn + K.epsilon())\n\n    f1 = 2*p*r \/ (p+r+K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return 1 - K.mean(f1)","c7147f64":"def display_set(df, column, n_sample, figsize ):\n    f, ax1 = plt.subplots(nrows = 1, ncols = 1, figsize = figsize )\n    sns.lineplot(x= df.index[::n_sample], y = df[column][::n_sample], ax=ax1)\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    #tf.random.set_seed(seed)  ","640e98b0":"# Showing Confusion Matrix\n# Thanks to https:\/\/www.kaggle.com\/marcovasquez\/basic-nlp-with-tensorflow-and-wordcloud\ndef plot_cm(y_true, y_pred, title):\n    figsize=(14,14)\n    y_pred = y_pred.astype(int)\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","144db068":"def get_class_weight(classes, exp=1):\n    '''\n    Weight of the class is inversely proportional to the population of the class.\n    There is an exponent for adding more weight.\n    '''\n    hist, _ = np.histogram(classes, bins=np.arange(12)-0.5)\n    class_weight = hist.sum()\/np.power(hist, exp)\n    \n    return class_weight\n\n# Thanks to https:\/\/www.kaggle.com\/siavrez\/simple-eda-model\ndef MacroF1Metric(preds, dtrain):\n    labels = dtrain.get_label()\n    preds = np.round(np.clip(preds, 0, 10)).astype(int)\n    score = f1_score(labels, preds, average = 'macro')\n    return ('MacroF1Metric', score, True)\n\ndef multiclass_F1_Metric(preds, dtrain):\n    labels = dtrain.get_label()\n    num_labels = 11\n    preds = preds.reshape(num_labels, len(preds)\/\/num_labels)\n    preds = np.argmax(preds, axis=0)\n    score = f1_score(labels, preds, average=\"macro\")\n    return ('MacroF1Metric', score, True)","45c5b0b9":"from functools import partial\nimport scipy as sp\nclass OptimizedRounder(object):\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize F1 (Macro) score\n    # https:\/\/www.kaggle.com\/naveenasaithambi\/optimizedrounder-improved\n    \"\"\"\n    def __init__(self):\n        self.coef_ = 0\n\n    def _f1_loss(self, coef, X, y):\n        \"\"\"\n        Get loss according to\n        using current coefficients\n        \n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n        return -f1_score(y, X_p, average = 'macro')\n\n    def fit(self, X, y):\n        \"\"\"\n        Optimize rounding thresholds\n        \n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        loss_partial = partial(self._f1_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        \"\"\"\n        Make predictions with specified thresholds\n        \n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n\n    def coefficients(self):\n        \"\"\"\n        Return the optimized coefficients\n        \"\"\"\n        return self.coef_['x']\n    \n    \ndef optimize_predictions(prediction, coefficients):\n    prediction[prediction <= coefficients[0]] = 0\n    prediction[np.where(np.logical_and(prediction > coefficients[0], prediction <= coefficients[1]))] = 1\n    prediction[np.where(np.logical_and(prediction > coefficients[1], prediction <= coefficients[2]))] = 2\n    prediction[np.where(np.logical_and(prediction > coefficients[2], prediction <= coefficients[3]))] = 3\n    prediction[np.where(np.logical_and(prediction > coefficients[3], prediction <= coefficients[4]))] = 4\n    prediction[np.where(np.logical_and(prediction > coefficients[4], prediction <= coefficients[5]))] = 5\n    prediction[np.where(np.logical_and(prediction > coefficients[5], prediction <= coefficients[6]))] = 6\n    prediction[np.where(np.logical_and(prediction > coefficients[6], prediction <= coefficients[7]))] = 7\n    prediction[np.where(np.logical_and(prediction > coefficients[7], prediction <= coefficients[8]))] = 8\n    prediction[np.where(np.logical_and(prediction > coefficients[8], prediction <= coefficients[9]))] = 9\n    prediction[prediction > coefficients[9]] = 10\n    \n    return prediction    ","84d65d0c":"PATH = '\/kaggle\/input\/data-without-drift\/'\n#PATH = '\/kaggle\/input\/liverpool-ion-switching\/'\n\ntrain = pd.read_csv(PATH + 'train_clean.csv')\ntest = pd.read_csv(PATH + 'test_clean.csv')\n\ntrain.head()","03167a03":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","8b5620a1":"DATA_BATCH_SIZE = 500000\n\nTRAIN_SAMPLE_RATE = 100\nTRAIN_BATCH_SIZE = int(len(train)\/TRAIN_SAMPLE_RATE)\n\nf, ax1 = plt.subplots(nrows = 1, ncols = 1, figsize = (20,4))\nsns.lineplot(data=train.signal[::TRAIN_SAMPLE_RATE], ax=ax1, hue=\"size\", size=\"size\")\nsns.lineplot(data=train.open_channels[::TRAIN_SAMPLE_RATE], ax=ax1, hue=\"size\", size=\"size\")\nax1.set_title(f'Full train signal')\n\nf, ax1 = plt.subplots(nrows = 1, ncols = 1, figsize = (10,4))\nsns.lineplot(data=test.signal[::TRAIN_SAMPLE_RATE], ax=ax1, hue=\"size\", size=\"size\")\nax1.set_title(f'Full test signal')\n","78ceb231":"f, axes = plt.subplots(nrows = 2, ncols = 5, figsize = (26,12))\nfor i in range(10):\n    XX = train.signal[i*DATA_BATCH_SIZE:(i+1)*DATA_BATCH_SIZE + 1]\n    yy = train.open_channels[i*DATA_BATCH_SIZE:(i+1)*DATA_BATCH_SIZE + 1]\n    sns.scatterplot(data=XX[::TRAIN_SAMPLE_RATE], ax=axes[int(i\/5), i%5], hue=\"size\", size=\"size\")\n    sns.scatterplot(data=yy[::TRAIN_SAMPLE_RATE], ax=axes[int(i\/5), i%5], hue=\"size\", size=\"size\")\n    axes[int(i\/5), i%5].set_title(f'Train Batch# {i+1}')\n    \nf, axes = plt.subplots(nrows = 1, ncols = 5, figsize = (26,6))\nfor i in range(4):\n    XX = test.signal[i*DATA_BATCH_SIZE:(i+1)*DATA_BATCH_SIZE + 1]\n    sns.scatterplot(data=XX[::TRAIN_SAMPLE_RATE], ax=axes[i], hue=\"size\", size=\"size\")\n    axes[i].set_title(f'Test Batch# {i+1}')\n","86d7e589":"f, ax = plt.subplots(figsize=(15, 6))\nsns.countplot(x=\"open_channels\", data=train, ax=ax)","a6b67bfc":"sns.set(style=\"whitegrid\")\n\nf, axes = plt.subplots(nrows = 2, ncols = 5, figsize = (26,12))\nfor i in range(10):\n    y = pd.DataFrame()\n    sns.countplot( x = train.open_channels[i*DATA_BATCH_SIZE:(i+1)*DATA_BATCH_SIZE + 1], ax=axes[int(i\/5), i%5])\n    axes[int(i\/5), i%5].set_title(f'Train Batch# {i+1}')","838a7dde":"## Most of the ideas have been taken from https:\/\/www.kaggle.com\/gpreda\/ion-switching-advanced-eda-and-prediction","58e2d450":"RANDOM_SEED = 42\nGROUP_BATCH_SIZE = 2000\nWINDOW_SIZES = [3, 5, 10, 50, 100, 500, 1000, 5000]\n\nseed_everything(RANDOM_SEED)","2ae5ee6e":"%%time\n\n\n\n# create batches of GROUP_BATCH_SIZE observations\ndef batching(df, batch_size, gr_name='group'):\n    df[gr_name] = df.groupby(df.index\/\/batch_size, sort=False)['signal'].agg(['ngroup']).values\n    df[gr_name] = df[gr_name].astype(np.uint16)\n    return df\n\n# normalize the data (standard scaler). We can also try other scalers for a better score!\ndef normalize(train, test):\n    train_input_mean = train.signal.values.mean()\n    train_input_sigma = train.signal.std()\n    train['signal'] = (train.signal - train_input_mean) \/ train_input_sigma\n    test['signal'] = (test.signal - train_input_mean) \/ train_input_sigma\n    return train, test\n\ndef run_feat_engineering(df, batch_size, gr_name='group'):\n    df = batching(df, batch_size = batch_size, gr_name=gr_name)\n    df['signal_2'] = df['signal'] ** 2\n    df['signal_2-7500-mean'] = df['signal_2'] - df['signal_2'].rolling(window=7500).mean()    \n    return df\n\n\ntrain = run_feat_engineering(train, batch_size = GROUP_BATCH_SIZE, gr_name='group')\ntest = run_feat_engineering(test, batch_size = GROUP_BATCH_SIZE, gr_name='group')","90369d4b":"%%time\n\n## add some noise to reduce overfitting\n\nSTD = 0.01\n\nold_data = train['signal']\nnew_data = old_data + np.random.normal(0,STD,size=len(train)) \ntrain['signal'] = new_data\n\nold_data = test['signal']\nnew_data = old_data + np.random.normal(0,STD,size=len(test)) \ntest['signal'] = new_data\n\ndel old_data, new_data","605a144c":"%%time\n\n## create rolling features\ndef gen_roll_features(full, win_sizes = WINDOW_SIZES):\n    for window in tqdm(win_sizes):\n        full[\"rolling_mean_\" + str(window)] = full['signal'].rolling(window=window).mean()\n        full[\"rolling_std_\" + str(window)] = full['signal'].rolling(window=window).std()\n        full[\"rolling_var_\" + str(window)] = full['signal'].rolling(window=window).var()\n        full[\"rolling_min_\" + str(window)] = full['signal'].rolling(window=window).min()\n        full[\"rolling_max_\" + str(window)] = full['signal'].rolling(window=window).max()\n\n        a = (full['signal'] - full['rolling_min_' + str(window)]) \/ (full['rolling_max_' + str(window)] - full['rolling_min_' + str(window)])\n        full[\"norm_\" + str(window)] = a * (np.floor(full['rolling_max_' + str(window)]) - np.ceil(full['rolling_min_' + str(window)]))\n    return full\n\ntrain = gen_roll_features(train)\ntest = gen_roll_features(test)","b01eafd3":"%%time\n\n## batch signal features \ndef gen_sig_features(df):\n    df = df.sort_values(by=['time']).reset_index(drop=True)\n    df.index = ((df.time * 10_000) - 1).values\n    df['batch'] = df.index \/\/ 25_000\n    df['batch_index'] = df.index  - (df.batch * 25_000)\n    df['batch_slices'] = df['batch_index']  \/\/ 2500\n    df['batch_slices2'] = df.apply(lambda r: '_'.join([str(r['batch']).zfill(3), str(r['batch_slices']).zfill(3)]), axis=1)\n    \n    for c in tqdm(['batch','batch_slices2']):\n        d = {}\n        d['mean'+c] = df.groupby([c])['signal'].mean()\n        d['median'+c] = df.groupby([c])['signal'].median()\n        d['max'+c] = df.groupby([c])['signal'].max()\n        d['min'+c] = df.groupby([c])['signal'].min()\n        d['std'+c] = df.groupby([c])['signal'].std()\n        d['mean_abs_chg'+c] = df.groupby([c])['signal'].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        d['abs_max'+c] = df.groupby([c])['signal'].apply(lambda x: np.max(np.abs(x)))\n        d['abs_min'+c] = df.groupby([c])['signal'].apply(lambda x: np.min(np.abs(x)))\n        d['range'+c] = d['max'+c] - d['min'+c]\n        d['maxtomin'+c] = d['max'+c] \/ d['min'+c]\n        d['abs_avg'+c] = (d['abs_min'+c] + d['abs_max'+c]) \/ 2\n        for v in d:\n            df[v] = df[c].map(d[v].to_dict())\n    df = reduce_mem_usage(df)\n    gc.collect()\n    return df\n\ntrain = gen_sig_features(train)\ntest = gen_sig_features(test)","29c9dad6":"%%time\n\n## create shift features\ndef gen_shift_features(df):\n    # add shifts\n    df['signal_shift_+1'] = [0,] + list(df['signal'].values[:-1])\n    df['signal_shift_-1'] = list(df['signal'].values[1:]) + [0]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+1'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-1'][i] = np.nan\n    \n    df['signal_shift_+2'] = [0,] + [1,] + list(df['signal'].values[:-2])\n    df['signal_shift_-2'] = list(df['signal'].values[2:]) + [0] + [1]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+2'][i] = np.nan\n    for i in df[df['batch_index']==1].index:\n        df['signal_shift_+2'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-2'][i] = np.nan\n    for i in df[df['batch_index']==49998].index:\n        df['signal_shift_-2'][i] = np.nan\n    \n    df.drop(columns=['batch', 'batch_index', 'batch_slices', 'batch_slices2'], inplace=True)\n    gc.collect()\n\n    for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal', 'open_channels', 'group', 'group1', 'group2', 'category', 'index']]:\n        df[c+'_msignal'] = df[c] - df['signal']\n        \n    df = df.replace([np.inf, -np.inf], np.nan)    \n    df.fillna(0, inplace=True)\n    df = reduce_mem_usage(df)\n    gc.collect()\n    return df\n\ntrain = gen_shift_features(train)\ntest = gen_shift_features(test)","738c469c":"ALL_FEATURES = [c for c in train.columns if c not in ['index', 'time', 'open_channels', 'group', 'group1', 'group2','category', 'index_msignal' ]]\nprint(len(ALL_FEATURES))","776e99bc":"## get all correlations \ncorr = train[ALL_FEATURES][::10].corr('spearman')\n\ncolumns = np.full((corr.shape[0],), True, dtype=bool)\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >= 0.9:\n            if columns[j]:\n                columns[j] = False\n                \nselected_columns = train[ALL_FEATURES].columns[columns]\nprint(len(selected_columns))            \n#print(selected_columns)        ","5a4ab928":"from sklearn.feature_selection import VarianceThreshold\n\ny = train['open_channels'].values\nX = train[selected_columns].values\n\nvt = VarianceThreshold(0.5)\nvt.fit(X, y)\n\n## let's take top 15\ntop_idx = np.argpartition(vt.variances_, -15)[-15:]\nSELECTED_FEATURES = [selected_columns[i] for i in top_idx]\nprint(SELECTED_FEATURES)","ef94a505":"X_all_train = train[SELECTED_FEATURES]\nX_all_test = test[SELECTED_FEATURES]\ny_all_train = train['open_channels'].values\n\n## reduce amount of data to speed things up\nX_train = X_all_train[::2]\ny_train = y_all_train[::2]\n\ngc.collect()\nprint(f'Original sizes: X_all_train: {X_all_train.shape}, y_all_train: {y_all_train.shape}, X_all_test: {X_all_test.shape}' )\nprint(f'Reduced train sizes: X_train: {X_train.shape}, y_train: {y_train.shape}' )","84cc1a3a":"def baseline_model(input_shape, units = 64, max_channels = 11, optimizer='adam'):\n    model = Sequential()\n    model.add(LSTM(units, input_shape=(input_shape[1], input_shape[2]), return_sequences=True))\n    model.add(LSTM(units, return_sequences=True))\n    model.add(LSTM(units, return_sequences=True))\n    model.add(LSTM(units, return_sequences=True))\n    model.add(LSTM(units))\n    model.add(Dense(units))\n    model.add(Dense(max_channels, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc', f1])\n    return model","c52f3dd8":"## it seems LSTM model will not converge with all generated features (at least for me :) \n## so, select some that work\n\n## reshape for LSTM\nX = X_train.values.reshape(-1,len(SELECTED_FEATURES),1)\n## using categorical_crossentropy\nyy = to_categorical(y_train, num_classes=11)\n\ntrain_idx, val_idx = train_test_split(np.arange(X.shape[0]), random_state = RANDOM_SEED, test_size = 0.2)\n\nX_t = X[train_idx] \ny_t = yy[train_idx] \nX_v = X[val_idx]\ny_v = yy[val_idx]","008d623c":"%%time\n\nBATCH_SIZE = 64\nEPOCHS = 6\n\nes = EarlyStopping(monitor='val_loss', mode='min', patience=10, verbose=1)\nlr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.000001, verbose=1)\n\nadam = Adam(0.003)\n\nmodel = baseline_model(X_t.shape, optimizer=adam)\nhistory = model.fit( X_t, y_t, validation_data=(X_v, y_v), callbacks=[es,lr],\n                    batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, \n                    shuffle=False, workers=8, use_multiprocessing=True )","6e9688f1":"_, ax1 = plt.subplots(nrows = 1, ncols = 1, figsize = (16,8))\nsns.lineplot(data=np.asarray(history.history['loss']), label='loss', ax=ax1)\nsns.lineplot(data=np.asarray(history.history['val_loss']), label='val_loss', ax=ax1)\nsns.lineplot(data=np.asarray(history.history['acc']), label='acc', ax=ax1)\nsns.lineplot(data=np.asarray(history.history['val_acc']), label='val_acc', ax=ax1)\nsns.lineplot(data=np.asarray(history.history['f1']), label='f1', ax=ax1)\nsns.lineplot(data=np.asarray(history.history['val_f1']), label='val_f1', ax=ax1)","8cbc2848":"y_pred = np.argmax(model.predict(X_v), axis=1).reshape(-1)\nyhat = y_train[val_idx]\n\nprint(\"F1 MACRO: \", f1_score(yhat, y_pred, average=\"macro\"))","241268fe":"y_pred = np.argmax(model.predict(X_all_test.values.reshape(-1,len(SELECTED_FEATURES), 1)), axis=1).reshape(-1)","7df0fbb4":"sub = pd.read_csv(\"..\/input\/liverpool-ion-switching\/sample_submission.csv\", dtype={'time':str})\nsub.open_channels = np.array(np.round(y_pred,0), np.int)\nsub.to_csv(\"submission.csv\",index=False)","a48f755d":"sub.head(25)","992e1dcf":"## By batch","7a9ff85a":"## Making predictions","9566ce88":"## Feature Selection","2e7f6dc6":"## Display train and test signals","fd6d9e0b":"### removing columns where correlation is high","4bbda649":"### using VarianceThreshold to remove all low-variance features","f973d2e4":"## LSTM model","f2e3957a":"**                              Identify the number of channels open at each time point**\n\nMany diseases, including cancer, are believed to have a contributing factor in common. Ion channels are pore-forming proteins present in animals and plants. They encode learning and memory, help fight infections, enable pain signals, and stimulate muscle contraction. If scientists could better study ion channels, which may be possible with the aid of machine learning, it could have a far-reaching impact.\n\nWhen ion channels open, they pass electric currents. Existing methods of detecting these state changes are slow and laborious. Humans must supervise the analysis, which imparts considerable bias, in addition to being tedious. These difficulties limit the volume of ion channel current analysis that can be used in research. Scientists hope that technology could enable rapid automatic detection of ion channel current events in raw data.\n\nThe University of Liverpool\u2019s Institute of Ageing and Chronic Disease is working to advance ion channel research. Their team of scientists have asked for your help. In this competition, you\u2019ll use ion channel data to better model automatic identification methods. If successful, you\u2019ll be able to detect individual ion channel events in noisy raw signals. The data is simulated and injected with real world noise to emulate what scientists observe in laboratory experiments.","b04b6339":"## Distributions for open channels for each batch","f41dcfbb":"## Introduction","50fe7df0":"## Evaluate model","df571ccb":"## Open channels value count","764e1d9b":"## Merge train and test and feature engineering","3793c798":"**IMPORTANT: While the time series appears continuous, the data is from discrete batches of 50 seconds long 10 kHz samples (500,000 rows per batch).\nIn other words, the data from 0.0001 - 50.0000 is a different batch than 50.0001 - 100.0000, and thus discontinuous between 50.0000 and 50.0001.**","678983ba":"## Load train and test datasets"}}