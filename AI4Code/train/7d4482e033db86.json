{"cell_type":{"a2729414":"code","8a7b6e73":"code","2dc8b752":"code","d965337f":"code","bf26d3ed":"code","5f17b5c8":"code","98092bc4":"markdown","5e954992":"markdown","9da45d29":"markdown","cdc426cd":"markdown"},"source":{"a2729414":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nimport itertools\nfrom sklearn.metrics import accuracy_score, confusion_matrix, pairwise_distances, pairwise_distances_argmin\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, train_test_split, GroupShuffleSplit, StratifiedShuffleSplit, GroupKFold\nfrom sklearn.neighbors import NearestNeighbors\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","8a7b6e73":"train = pd.read_csv('..\/input\/X_train.csv')\ny = pd.read_csv('..\/input\/y_train.csv')\ntest = pd.read_csv('..\/input\/X_test.csv')\nsub = pd.read_csv('..\/input\/sample_submission.csv')","2dc8b752":"def get_start_end_points(data):\n    start = data.query(\"measurement_number==0\").reset_index()\n    end = data.query(\"measurement_number==127\").reset_index()\n\n    columns = [\"orientation_X\", \"orientation_Y\", \"orientation_Z\", \"orientation_W\"]\n    \n    start, end = start[columns], end[columns]\n\n    points = start.join(end, lsuffix=\"_start\", rsuffix=\"_end\").join(y)\n    return points","d965337f":"train_points = get_start_end_points(train)\ntest_points = get_start_end_points(test)\n\ntrain_points.head()","bf26d3ed":"# https:\/\/www.kaggle.com\/artgor\/where-do-the-robots-drive\n\ndef plot_confusion_matrix(truth, pred, classes, normalize=False, title=''):\n    cm = confusion_matrix(truth, pred)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=(10, 10))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion matrix', size=15)\n    plt.colorbar(fraction=0.046, pad=0.04)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.tight_layout()","5f17b5c8":"le = LabelEncoder()\ny['surface'] = le.fit_transform(y['surface'])\nscores = []\n\nfolds = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(train_points.values, y['surface'].values)):\n    # Compare start to end, and end to start\n    x_train_se, x_valid_se = train_points.iloc[trn_idx].filter(regex='start'), train_points.iloc[val_idx].filter(regex='end')\n    x_train_es, x_valid_es = train_points.iloc[trn_idx].filter(regex='end'), train_points.iloc[val_idx].filter(regex='start')\n\n    y_train, y_valid = y[\"surface\"][trn_idx], y[\"surface\"][val_idx],\n        \n    neigh = NearestNeighbors(1)\n    \n    neigh.fit(x_train_se)\n    distances_se, indices_se = neigh.kneighbors(x_valid_se)\n    \n    neigh.fit(x_train_es)\n    distances_es, indices_es = neigh.kneighbors(x_valid_es)\n    \n    # Find the minimum distance to select the nearest match\n    distances = np.concatenate([distances_se, distances_es], -1)\n    indices = np.concatenate([indices_se, indices_es], -1)    \n    indices_best = np.array([indices[i, x] for i, x in enumerate(np.argmin(distances, axis=1))])\n    \n    indices = indices_best.flatten()\n    accuracy = accuracy_score(y_valid, y_train.iloc[indices])\n    scores.append(accuracy)\n    print(\"Fold %i, score: %0.5f, mean distance %0.5f\" % (fold, accuracy, np.mean(distances)))\n    \n    plot_confusion_matrix(y_valid, y_train.iloc[indices], le.classes_, normalize=True)\n    \nprint(\"Average accuracy %0.5f\" % np.mean(scores))","98092bc4":"# Discussion\nAs you can see it's possible to get a CV score of 0.992 using only orientation data due to the leakage in the train set. This is why it's very difficult to get a reliable CV score that matches the public LB.\n\nAs far as I can tell this doesn't help much with the test dataset (creating a submission using this method scores 0.32), but that doesn't mean you can't use the methods in this kernel to help with your predictions","5e954992":"# Get the start and end orientations\nWe'll create features that represent the start and end orientations (and Euler angles) of the robot","9da45d29":"As discussed in the discussion forums (https:\/\/www.kaggle.com\/c\/career-con-2019\/discussion\/87239#latest-508136) it looks as if each series is part of longer aquisition periods that have been cut up into chunks with 128 samples.\n\nThis means that each series is not truely independent and there is leakage between them via the orientation data. Therefore if you have any features that use orientation, you will get a very high CV score due to this leakage in the train set.\n\nThis kernel will show you how it is possible to get a CV score of 0.992 using only the orientation data.","cdc426cd":"# Use nearest neighbours to find similar series\nWe'll compare the start of each series to the end of every other. We'll also to the reverse and pick the match with the smallest distance"}}