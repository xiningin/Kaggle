{"cell_type":{"2039c3de":"code","f0c01194":"code","14469b3d":"code","5ed9e5e9":"code","86b3d7d9":"code","28f5d042":"code","9153b50e":"code","9eabb736":"code","5fcbb6d5":"code","cfcc22e1":"code","9bd241cb":"code","41095f6f":"markdown","947f98a8":"markdown","8feab7c0":"markdown","192b814d":"markdown","fc3f3587":"markdown"},"source":{"2039c3de":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f0c01194":"import pandas as pd\nelectricity = pd.read_excel(\"..\/input\/Folds5x2_pp.xlsx\")\nelectricity.info()","14469b3d":"electricity.head()","5ed9e5e9":"# There are no null values in the dataset.\n\nelectricity.describe()\n","86b3d7d9":"train_sizes = [1, 100, 500, 2000, 5000, 7654]","28f5d042":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import learning_curve\n\nfeatures = ['AT', 'V', 'AP', 'RH']\ntarget = 'PE'\n\ntrain_sizes, train_scores, validation_scores = learning_curve(\n                                                   estimator = LinearRegression(), X = electricity[features],\n                                                   y = electricity[target], train_sizes = train_sizes, cv = 5,\n                                                   scoring = 'neg_mean_squared_error')","9153b50e":"print('Training scores:\\n\\n', train_scores)\nprint('\\nValidation scores:\\n\\n', validation_scores)","9eabb736":"train_scores_mean = -train_scores.mean(axis = 1)\nvalidation_scores_mean = -validation_scores.mean(axis = 1)\n\nprint('Mean training scores\\n\\n', pd.Series(train_scores_mean, index = train_sizes))\nprint('\\nMean validation scores\\n\\n',pd.Series(validation_scores_mean, index = train_sizes))","5fcbb6d5":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.style.use('seaborn')\n\nplt.plot(train_sizes, train_scores_mean, label = 'Training error')\nplt.plot(train_sizes, validation_scores_mean, label = 'Validation error')\n\nplt.ylabel('MSE', fontsize = 14)\nplt.xlabel('Training set size', fontsize = 14)\nplt.title('Learning curves for a linear regression model', fontsize = 18, y = 1.03)\nplt.legend()\nplt.ylim(0,40)","cfcc22e1":"from  sklearn.ensemble import RandomForestRegressor\n","9bd241cb":"### Bundling our previous work into a function ###\n\ndef learning_curves(estimator, data, features, target, train_sizes, cv):\n    train_sizes, train_scores, validation_scores = learning_curve(\n                                                 estimator, data[features], data[target], train_sizes = train_sizes,\n                                                 cv = cv, scoring = 'neg_mean_squared_error')\n    train_scores_mean = -train_scores.mean(axis = 1)\n    validation_scores_mean = -validation_scores.mean(axis = 1)\n    \n    plt.plot(train_sizes, train_scores_mean, label = 'Training error')\n    plt.plot(train_sizes, validation_scores_mean, label = 'Validation error')\n\n    plt.ylabel('MSE', fontsize = 14)\n    plt.xlabel('Training set size', fontsize = 14)\n    title = 'Learning curves for a ' + str(estimator).split('(')[0] + ' model'\n    plt.title(title, fontsize = 18, y = 1.03)\n    plt.legend()\n    plt.ylim(0,40)\n\n\n### Plotting the two learning curves ###\nfrom sklearn.ensemble import RandomForestRegressor\n\nplt.figure(figsize = (16,5))\n\nfor model, i in [(RandomForestRegressor(), 1), (LinearRegression(),2)]:\n    plt.subplot(1,2,i)\n    learning_curves(model, electricity, features, target, train_sizes, 5)","41095f6f":"There's a lot of information we can extract from this plot. Let's proceed granularly.\n\nWhen the training set size is 1, we can see that the MSE for the training set is 0. This is normal behavior, since the model has no problem fitting perfectly a single data point. So when tested upon the same data point, the prediction is perfect.\n\nBut when tested on the validation set (which has 1914 instances), the MSE rockets up to roughly 423.4. This relatively high value is the reason we restrict the y-axis range between 0 and 40. This enables us to read most MSE values with precision. Such a high value is expected, since it's extremely unlikely that a model trained on a single data point can generalize accurately to 1914 new instances it hasn't seen in training.\n\nWhen the training set size increases to 100, the training MSE increases sharply, while the validation MSE decreases likewise. The linear regression model doesn't predict all 100 training points perfectly, so the training MSE is greater than 0. However, the model performs much better now on the validation set because it's estimated with more data.\n\nFrom 500 training data points onward, the validation MSE stays roughly the same. This tells us something extremely important: adding more training data points won't lead to significantly better models.\n\nIn our case, the validation MSE stagnates at a value of approximately 20 Megawatt squared units. But how good is that?\nThe values in our target column are in MW (according to the documentation). Taking the square root of 20 MW results in approximately 4.5 MW. Each target value represents net hourly electrical energy output. So for each hour our model is off by 4.5 MW on average.\n\n\nIn this case, the training MSE plateaus at around 20, and that's a high value. So besides the narrow gap, we now have another confirmation that we have a low variance problem.\n\nSo far, we can conclude that:\n\n* Our learning algorithm suffers from high bias and low variance, underfitting the training data.\n* Adding more instances (rows) to the training data is hugely unlikely to lead to better models under the current learning algorithm.\n\n**Conclusion :** One solution at this point is to change to a more complex learning algorithm by generating polynimial features. This should decrease the bias and increase the variance.\n\nLets see how Randonforest performs\n\n","947f98a8":"\nColumns description \n\nFeatures consist of hourly average ambient variables \n- Ambient Temperature (AT) in the range 1.81\u00b0C and 37.11\u00b0C,\n- Ambient Pressure (AP) in the range 992.89-1033.30 milibar,\n- Relative Humidity (RH) in the range 25.56% to 100.16%\n- Exhaust Vacuum (V) in teh range 25.36-81.56 cm Hg\n- Net hourly electrical energy output (PE) 420.26-495.76 MW\n\nA combined cycle power plant (CCPP) is composed of gas turbines (GT), steam turbines (ST) and heat recovery steam generators. In a CCPP, the electricity is generated by gas and steam turbines, which are combined in one cycle, and is transferred from one turbine to another. While the Vacuum is colected from and has effect on the Steam Turbine, he other three of the ambient variables effect the GT performance.\n\n\n\nTHe PE is our target variable.\n","8feab7c0":"Diagnose Bias and Variance to Reduce Error.\nIn any model, two major sources of error are bias and variance. If we managed to reduce these two, then we could build more accurate models.\n\nAim is to  earn how to answer both these questions using learning curves. We'll work with  UCI power plant data set and try to predict the electrical energy output of a power plant.\n\n\n","192b814d":"To plot the learning curves, we need only a single error score per training set size, not 5. For this reason, in the next code cell we take the mean value of each row and also flip the signs of the error scores ","fc3f3587":"Let's first decide what training set sizes we want to use for generating the learning curves.\n\nThe minimum value is 1. The maximum is given by the number of instances in the training set. Our training set has 9568 instances, so the maximum value is 9568.\n\nHowever, we haven't yet put aside a validation set. We'll do that using an 80:20 ratio, ending up with a training set of 7654 instances (80%), and a validation set of 1914 instances (20%). Given that our training set will have 7654 instances, the maximum value we can use to generate our learning curves is 7654.\n\nFor our case, here, we use these six sizes:"}}