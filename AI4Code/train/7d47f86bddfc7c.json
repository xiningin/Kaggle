{"cell_type":{"e3c86e69":"code","4a00cccc":"code","4d03c515":"code","f713bddc":"code","5db632e3":"code","9e273483":"code","4434a432":"code","e222864e":"code","f2ba927d":"code","4cf88ca2":"code","3d7ff398":"code","6e184653":"code","275204b8":"code","2d6214dc":"code","66d71e54":"code","221ee56a":"code","bfaa6073":"code","d617e326":"code","e355d2af":"code","0319ab94":"markdown","8bff2514":"markdown","e18e57c7":"markdown","fb904d18":"markdown","297843e9":"markdown","3ac47ae7":"markdown","07e4d992":"markdown","4f960a12":"markdown","d080621c":"markdown","4097158a":"markdown","9d357ff5":"markdown"},"source":{"e3c86e69":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4a00cccc":"import torch\nimport torch.nn as nn\n# import torch.nn.functional as F\n\nfrom collections import Counter\nimport warnings\n\nimport string\nimport nltk\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","4d03c515":"%matplotlib inline","f713bddc":"warnings.filterwarnings('ignore')","5db632e3":"torch.autograd.set_detect_anomaly(True)","9e273483":"batch_size = 16\nseq_size = 32\nembedding_size = 64\nlstm_size = 64\ngradients_norm = 5\n# set device parameter\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","4434a432":"# read document\nwith open ('..\/input\/shakespeare-plays\/alllines.txt', 'r') as f:\n    doc = f.read()","e222864e":"# get list of words from document\ndef doc2words(doc):\n    lines = doc.split('\\n')\n    lines = [line.strip(r'\\\"') for line in lines]\n    words = ' '.join(lines).split()\n    return words","f2ba927d":"def removepunct(words):\n    punct = set(string.punctuation)\n    words = [''.join([char for char in list(word) if char not in punct]) for word in words]\n    return words","4cf88ca2":"# get vocab from word list\ndef getvocab(words):\n    wordfreq = Counter(words)\n    sorted_wordfreq = sorted(wordfreq, key=wordfreq.get)\n    return sorted_wordfreq","3d7ff398":"# get dictionary of int to words and word to int\ndef vocab_map(vocab):\n    int_to_vocab = {k:w for k,w in enumerate(vocab)}\n    vocab_to_int = {w:k for k,w in int_to_vocab.items()}\n    return int_to_vocab, vocab_to_int","6e184653":"words = removepunct(doc2words(doc))\nvocab = getvocab(words)\nint_to_vocab, vocab_to_int = vocab_map(vocab)","275204b8":"def get_batches(words, vocab_to_int, batch_size, seq_size):\n    # generate a Xs and Ys of shape (batchsize * num_batches) * seq_size\n    word_ints = [vocab_to_int[word] for word in words]\n    num_batches = int(len(word_ints) \/ (batch_size * seq_size))\n    Xs = word_ints[:num_batches*batch_size*seq_size]\n    Ys = np.zeros_like(Xs)\n    Ys[:-1] = Xs[1:]\n    Ys[-1] = Xs[0]\n    Xs = np.reshape(Xs, (num_batches*batch_size, seq_size))\n    Ys= np.reshape(Ys, (num_batches*batch_size, seq_size))\n    \n    # iterate over rows of Xs and Ys to generate batches\n    for i in range(0, num_batches*batch_size, batch_size):\n        yield Xs[i:i+batch_size, :], Ys[i:i+batch_size, :]","2d6214dc":"class RNNModule(nn.Module):\n    # initialize RNN module\n    def __init__(self, n_vocab, seq_size=32, embedding_size=64, lstm_size=64):\n        super(RNNModule, self).__init__()\n        self.seq_size = seq_size\n        self.lstm_size = lstm_size\n        self.embedding = nn.Embedding(n_vocab, embedding_size)\n        self.lstm = nn.LSTM(embedding_size,\n                            lstm_size,\n                            batch_first=True)\n        self.dense = nn.Linear(lstm_size, n_vocab)\n        \n    def forward(self, x, prev_state):\n        embed = self.embedding(x)\n        output, state = self.lstm(embed, prev_state)\n        logits = self.dense(output)\n\n        return logits, state\n    \n    def zero_state(self, batch_size):\n        return (torch.zeros(1, batch_size, self.lstm_size),torch.zeros(1, batch_size, self.lstm_size))\n ","66d71e54":"def get_loss_and_train_op(net, lr=0.001):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n\n    return criterion, optimizer","221ee56a":"def generate_text(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n    net.eval()\n\n    state_h, state_c = net.zero_state(1)\n    state_h = state_h.to(device)\n    state_c = state_c.to(device)\n    for w in words:\n        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n        output, (state_h, state_c) = net(ix, (state_h, state_c))\n    \n    _, top_ix = torch.topk(output[0], k=top_k)\n    choices = top_ix.tolist()\n    choice = np.random.choice(choices[0])\n\n    words.append(int_to_vocab[choice])\n    \n    for _ in range(100):\n        ix = torch.tensor([[choice]]).to(device)\n        output, (state_h, state_c) = net(ix, (state_h, state_c))\n\n        _, top_ix = torch.topk(output[0], k=top_k)\n        choices = top_ix.tolist()\n        choice = np.random.choice(choices[0])\n        words.append(int_to_vocab[choice])\n\n    print(' '.join(words))","bfaa6073":"def train_rnn(words, vocab_to_int, int_to_vocab, n_vocab):\n    \n    # RNN instance\n    net = RNNModule(n_vocab, seq_size, embedding_size, lstm_size)\n    net = net.to(device)\n    criterion, optimizer = get_loss_and_train_op(net, 0.01)\n\n    iteration = 0\n    \n    for e in range(50):\n        batches = get_batches(words, vocab_to_int, batch_size, seq_size)\n        state_h, state_c = net.zero_state(batch_size)\n\n        # Transfer data to GPU\n        state_h = state_h.to(device)\n        state_c = state_c.to(device)\n        for x, y in batches:\n            iteration += 1\n\n            # Tell it we are in training mode\n            net.train()\n\n            # Reset all gradients\n            optimizer.zero_grad()\n\n            # Transfer data to GPU\n            x = torch.tensor(x).to(device)\n            y = torch.tensor(y).to(device)\n\n            logits, (state_h, state_c) = net(x, (state_h, state_c))\n            loss = criterion(logits.transpose(1, 2), y)\n\n            state_h = state_h.detach()\n            state_c = state_c.detach()\n\n            loss_value = loss.item()\n\n            # Perform back-propagation\n            loss.backward(retain_graph=True)\n\n            _ = torch.nn.utils.clip_grad_norm_(net.parameters(), gradients_norm)\n            \n            # Update the network's parameters\n            optimizer.step()\n\n            if iteration % 100 == 0:\n                print('Epoch: {}\/{}'.format(e, 200),'Iteration: {}'.format(iteration),'Loss: {}'.format(loss_value))\n\n            # if iteration % 1000 == 0:\n                # predict(device, net, flags.initial_words, n_vocab,vocab_to_int, int_to_vocab, top_k=5)\n                # torch.save(net.state_dict(),'checkpoint_pt\/model-{}.pth'.format(iteration))\n                \n    return net","d617e326":"rnn_net = train_rnn(words, vocab_to_int, int_to_vocab, len(vocab))","e355d2af":"generate_text(device, rnn_net, ['hey', 'you'], len(vocab), vocab_to_int, int_to_vocab)","0319ab94":"## Train","8bff2514":"## Import libraries","e18e57c7":"## References","fb904d18":"# Text Generation using Pytorch (for beginners)","297843e9":"## Model","3ac47ae7":"## Load data","07e4d992":"*  looks good\n*  text is missing punctuations, because I removed them\n*  Its a giant sentence, that doesn't make sense, but its english at least","4f960a12":"*  I took code from this blog (modified it a little bit): https:\/\/machinetalk.org\/2019\/02\/08\/text-generation-with-pytorch\/\n*  follow this blog for doing same exercise in tensorflow: https:\/\/machinetalk.org\/2019\/02\/1\/text-generation-with-tensorflow\/\n*  yet another notebook to make it solid: https:\/\/www.kaggle.com\/ab971631\/beginners-guide-to-text-generation-pytorch\n*  and last one is a little advanced: https:\/\/www.kaggle.com\/ankitjha\/anyone-can-learn-text-generation\n\nhappy learning!","d080621c":"## generate text","4097158a":"## Data prepration","9d357ff5":"## environment variables"}}