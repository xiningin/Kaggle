{"cell_type":{"da075a95":"code","e42acc13":"code","33b2121a":"code","05f57b4e":"code","27a8c4fe":"code","ca5a1b48":"code","af079d04":"code","8ce5639b":"code","855f484c":"code","38a66a2d":"code","5f73733b":"code","deeb0c32":"code","b799385f":"code","4dfd9b7b":"code","c1c1d7c0":"code","69284c3c":"code","2fc4d910":"code","3f1b8c77":"code","1eed3d88":"markdown","1f95d07f":"markdown","285b5f8c":"markdown","9a63275a":"markdown","1156ef58":"markdown","ab8e0972":"markdown","64f0f52c":"markdown","5941afc9":"markdown","2c48caeb":"markdown","27ad316f":"markdown","48227e28":"markdown","c23b499e":"markdown","c7403f8e":"markdown"},"source":{"da075a95":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e42acc13":"#Set up input list\nkeywords = ['stomach ache','ebola','fever','vaccin','epidemic','#ebola','#Virus','vomit','virus ebola','symptoms ebola','\"diarrhea due to ebola\"']\n#n = int(input(\"Enter the number of keywords : \")) \n#for i in range(0,n):\n#    keywords.append(input())\nprint(keywords)","33b2121a":"#Install and import module for this part\n!pip install googletrans\nfrom googletrans import Translator","05f57b4e":"#Translate\ntranslator = Translator()\nkeywords_trad = []\ntranslations = translator.translate(keywords, dest='fr')\nfor translation in translations:\n    keywords_trad.append(translation.text)\n    print(translation.origin, ' -> ', translation.text)","27a8c4fe":"#Install and import module for this part\n!pip install twython\nfrom twython import Twython\nimport json","ca5a1b48":"# Load credentials from json file\nwith open(\"\/kaggle\/input\/credentials\/twitter_credentials.json\", \"r\") as file:\n    creds = json.load(file)","af079d04":"# Instantiate an object\npython_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])\n\n# Create our query and search tweets\ndict_ = {'user': [], 'date': [], 'text': [], 'location': []}\nfor i in keywords_trad:\n    query = {'q': ' {}  -filter:retweets'.format(i),\n             #'geocode':'-4.437584, 15.252361,1000km',\n            'result_type': 'mixed',\n            'count': 1000,\n            'lang': 'fr',\n            }\n    for status in python_tweets.search(**query)['statuses']:\n        dict_['user'].append(status['user']['screen_name'])\n        dict_['location'].append(status['user']['location'])\n        dict_['date'].append(status['created_at'])\n        dict_['text'].append(status['text'])","8ce5639b":"#Modules\nimport pandas as pd\nfrom datetime import datetime \nfrom email.utils import mktime_tz, parsedate_tz\n\n#Function to convert date format\ndef parse_datetime(value):\n    time_tuple = parsedate_tz(value)\n    timestamp = mktime_tz(time_tuple)\n    return datetime.fromtimestamp(timestamp)\n\n# Structure data in a pandas DataFrame for easier manipulation\ndf = pd.DataFrame(dict_)\n#convert twitter date to another date time format\nfor i in range(len(df['date'])):\n    df['date'][i] = parse_datetime(df['date'][i])\ndf['date'] = df['date'].map(lambda x: str(x)[0:10])\ndf\n#If we want to save this dataframe\n#df.to_csv('tweets_saved.csv')","855f484c":"#Load an older dataframe with older tweets\nolder_tweets = pd.read_csv(\"\/kaggle\/input\/oldertweets\/tweets2.csv\")\nolder_tweets = older_tweets.drop(older_tweets.columns[[0]], axis=1)\nolder_tweets['date'] = older_tweets['date'].map(lambda x: str(x)[0:10])\nolder_tweets","38a66a2d":"#Merge dataframes into a big one and sort it by date\ndata = [df,older_tweets]\ntweets = pd.concat(data)\ntweets = tweets.drop_duplicates()\ntweets = tweets.sort_values(by='date')\ntweets","5f73733b":"#Module for regular expression matching operations\nimport re\n#Function that found our word in tweets\ndef word_in_text(word, text):\n    word = word.lower()\n    text = text.lower()\n    match = re.search(word, text)\n    if match:\n        return 1\n    return 0\n#Function to delete emojis\ndef deEmojify(inputString):\n    return inputString.encode('ascii', 'ignore').decode('ascii')\n\ndata_kw = pd.DataFrame()\nfor k in keywords_trad:\n    data_kw[k] = tweets['text'].apply(lambda tweet: word_in_text(k, tweet))\ndata_kw['date'] = tweets['date']\ndata_kw_date = data_kw.groupby('date').sum()\ndata_kw_date","deeb0c32":"#Modules\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker, cm\n\n#Count and time series\ncount = data_kw_date.cumsum()\ntime = ['day'+'{}'.format(i+1) for i in range(len(data_kw_date.index))]\n\n#Plot\nplt.figure(figsize = (7,5))\nplt.style.use('seaborn-paper')\ncol=iter(cm.rainbow(np.linspace(0,1,len(data_kw_date.columns))))\nfor k in list(data_kw_date.columns):\n    c = next(col)\n    plt.plot(time,count[k],'o-',color=c, label='{}'.format(k))\nplt.xlabel(\"Time\", fontsize=15)\nplt.ylabel(\"Count\", fontsize=15)\nplt.tick_params(axis = 'both', labelsize = 12)\nplt.legend(fontsize=12)\nplt.show()\n","b799385f":"#Find which keywords are more used and more relevent for our reasearch\ndata_count = pd.DataFrame({'Count': data_kw_date.sum(axis = 0, skipna = True)}).sort_values(by=['Count'])\ndata_count.reset_index(level=0, inplace=True)\ndata_count = data_count.rename(columns={\"index\": \"Keywords\"})\n\n#Bar chart\nplt.figure(figsize=(10, 10))\ndata_count.plot.barh(x='Keywords',y='Count',color='deepskyblue')\nplt.xlabel('Occurence', fontsize=15)\nplt.ylabel('keywords', fontsize=15)\nplt.legend(fontsize=12)\nplt.tick_params(axis = 'both', labelsize = 12)\nplt.title(\"Count of keywords founded over {} Tweets\".format(len(tweets['text'])), fontsize = 20)\nplt.show()","4dfd9b7b":"!pip install geopy\n!pip install gmplot\nfrom gmplot import gmplot\nfrom geopy.geocoders import Nominatim\nfrom geopy.extra.rate_limiter import RateLimiter","c1c1d7c0":"data_kw['location'] = tweets['location']\ndata_kw_location = data_kw.groupby('location').sum()\nfor i in range(len(data_kw_location.index.values)):\n    data_kw_location.index.values[i] = deEmojify(data_kw_location.index.values[i])\ndata_kw_location\n","69284c3c":"geolocator = Nominatim(user_agent=\"Mozilla\/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/76.0.3809.132 Safari\/537.36\")\n# Go through all tweets and add locations to 'coordinates' dictionary\ncoordinates = {'latitude': [], 'longitude': []}\n#Tracking Ebola keyword\nebola = data_kw_location['ebola']\nebola = ebola[ebola.values > 0]\nfor count,loc in enumerate(ebola.index):\n    try:\n        location = geolocator.geocode(loc)\n        # If coordinates are found for location\n        if location:\n            coordinates['latitude'].append(location.latitude)\n            coordinates['longitude'].append(location.longitude)\n            \n    # If too many connection requests\n    except:\n        pass","2fc4d910":"# Instantiate and center a GoogleMapPlotter object to show our map\ngmap = gmplot.GoogleMapPlotter(30, 0, 3)\ngmap.heatmap(coordinates['latitude'], coordinates['longitude'], radius=20)\ngmap.draw(\"map_ebola.html\")","3f1b8c77":"#Heatmap of regon where the keyword 'ebola' was used\nfrom IPython.core.display import Image\nImage(\"..\/input\/map-image-ebola\/map.jpg\")","1eed3d88":"## Assignment: \nThe goal is to build a tool that can track the usage of disease-related\nkeywords (e.g. stomache ache, headache, fever) in non-English tweets\n(for this case in French).\n","1f95d07f":"## Code:","285b5f8c":"- Most keywords used","9a63275a":"### Classification and cleaning data:\n\nWe classify our data into a dataframe that contain informations for each tweets related to keywords researched. We have user, location, date and text informations. We convert date format into a simple year\/month\/day format and perform other cleaning task. The free twitter API provide use data for only 7 days (including the day on which we perform the task). In order to get more data, we save an older dataframe for example the day before, add it to the new one and delete all duplicated data. ","1156ef58":"## Context:\nAn NGO wants to track the outbreak of diseases, and believes\nthis could be done by analyzing sentiments and keywords of (geotagged)\nnon-English twitter messages.","ab8e0972":"## Resources:\n- https:\/\/pypi.org\/project\/googletrans\/\n- https:\/\/stackabuse.com\/accessing-the-twitter-api-with-python\/ \n- https:\/\/developer.twitter.com\/en\/docs\/tweets\/search\/api-reference\/get-search-tweets\n- https:\/\/pypi.org\/project\/googletrans\/\n- https:\/\/journals.plos.org\/plosone\/article\/file?id=10.1371\/journal.pone.0019467&type=printable\n- https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC4763717\/ \n- https:\/\/geopy.readthedocs.io\/en\/1.16.0\/#usage-with-pandas\n","64f0f52c":"## Conclusions and Perspectives:\n\nIn conclusion, we build a tool that can track the usage of disease-related keywords, in this case with French tweets about Ebola. Despite the fact that we were limited in the amount of data due to API free version, we found most used keywords, track it over time and localize it. To get more data, we saved a previous version of our dataframe and add it to the new one. However, the professional version of the API can provide more data. \n\nMoreover, to improve this process, we can build the same thing in real time by using streaming tool provided by Twython. We notice that we need to apply more filter to target only relevant tweets about ebola. For example, we can searching only place where we talk more about Ebola. We also remark that not all tweets have a location, which reduces the information that can be drawn from the data.\n\n\nIn order to make it a MVP that can be marketed, we can make a website or mobile app with a dashboard including all charts related to a specific research like in this case. It can be selled as a web service with subscriptions and each keywords can have a price depending on its relevance.","5941afc9":"- Over time:","2c48caeb":"- Over location","27ad316f":"We can now apply a cumulative sum to see the evolution of occurence of each keywords over time.\n","48227e28":"### Tracking keywords:\nWe count the occurence of all keywords appearing in tweets and track it over time and position (if it is possible). \n","c23b499e":"### Initialisation:\n\n\nWe suppose that we want to track ebola disease, where poeple talk about it in french. We set up a list of keywords in english and translate it to french using google translate tool.","c7403f8e":"### Keywords research on Twitter:\n\nNow that we have our translate keywords, we have to search it on twitter by mean of twython. This module use twitter api thus we signed to a twitter developper account. We save all credentials parameters in a dictionary in json format and load it in our code for the next."}}