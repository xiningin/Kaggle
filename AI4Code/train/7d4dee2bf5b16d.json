{"cell_type":{"f48e072d":"code","c6357155":"code","d250809b":"code","d1993fdb":"code","8d0898af":"code","289a67cf":"code","a90a4b6a":"code","903e1fe9":"code","5120ce7d":"code","a909a321":"markdown","ad677918":"markdown","9f6083ac":"markdown","7fa1b12e":"markdown","27f944ed":"markdown","ab73b8dc":"markdown","70afac75":"markdown","308757e6":"markdown"},"source":{"f48e072d":"import gym\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport requests\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.initializers import he_normal\n\nfrom PIL import Image\nfrom io import BytesIO\n","c6357155":"r = requests.get('https:\/\/gym.openai.com\/videos\/2019-10-21--mqt8Qj1mwo\/CartPole-v1\/poster.jpg')\n\nplt.figure(figsize = (10, 6))\nplt.imshow(Image.open(BytesIO(r.content)))\nplt.axis('off')\nplt.show()","d250809b":"env = gym.make('CartPole-v0') # Create an environment\n\n# To reset the environment we can use reset() function which returns an array with 4 values\n# This 4 values is an observation, which tells us a position of pole. We don't need to know what these values mean\n# this is a job for our ANN.\nobservation = env.reset()\nprint(f'Observation, returned by reset() function: {observation}')\n\n# To see action space we can use action_space attribute\n# Discrete(2) means that actions can be 0 or 1 which can be left or right \nprint('Action space: ', env.action_space)\n\n# To make step we need to use step(action) function, it returns 4 values:\n# Obseravtion - current observation after action\n# Reward - recieved reward after action\n# Done - whether game over or not\n# Debug data which we don't need\nobservation, reward, done, debug = env.step(env.action_space.sample()) # Doing random action\nprint(f'Observation after action: {observation}')\nprint(f'Reward for the action: {reward}')\nprint(f'Game is over: {done}')\nprint(f'Debug data: {debug}')","d1993fdb":"def generate_data(env = gym.make('CartPole-v0'), n_games = 1000, model = None, percentile = 70):\n    '''\n       env - an environment to solve\n       n_games - number of games to play to collect raw data\n       model - if None, the random actions will be taken to collect data, to predict actions a model must be passed\n       percentile - (100% - percentile%) of the best games will be selected as training data\n    '''\n    observation = env.reset() # Resetting the environment to get our first observation\n\n    train_data = [] # List to store raw data\n    rewards = [] # List to store total rewards of each game\n    \n    print(f'Playing {n_games} games...')\n    \n    # Step 1 of the algorithm - Play N numbers of games using random actions or actions predicted by model to collect raw data.\n    for i in range(n_games):\n        temp_reward = 0 # Counts a current game total reward\n        temp_data = [] # Stores (observation, action) tuples for each step\n        \n        # Playing a current game until done\n        while True:\n            # Use model to predict actions if passed, otherwise take random actions\n            if model:\n                action = model.predict(observation.reshape((-1, 4)))\n                action = int(np.rint(action))\n            else:\n                action = env.action_space.sample()\n            \n            temp_data.append((list(observation), action)) # Appending (observation, action) tuple to temp_data list\n\n            observation, reward, done, _ = env.step(action) # Making action\n\n            temp_reward += reward # Counting reward\n            \n            # If game over - reset environment and break while loop\n            if done:\n                observation = env.reset()\n                break\n        \n        # Append data of last game to train_data list and total reward of last game to rewards list\n        train_data.append(temp_data)\n        \n        # Step 2 of the algorithm - Collect a total reward for each game and calculate threshold - 70 percentile of all total rewards.\n        rewards.append(temp_reward)\n        \n    print('Done playing games\\n')\n    \n    # Calculating threshold value using rewards list an np.percentile function\n    thresh = int(np.percentile(rewards, percentile))\n    print(f'Score threshold value: {thresh}')\n    \n    print(f'Selecting games according to threshold...')\n    # Step 3 of the algorithm - Select games from raw data which have total reward more than threshold.\n    train_data = [episode for (i, episode) in enumerate(train_data) if rewards[i] >= thresh]\n    \n    # Now train_data list contains lists of tuples: [[(observation, action), ...], [(observation, action), ...], ...]\n    # The next string flattens train_data list: [(observation, action), (observation, action), ...]\n    train_data = [observation for episode in train_data for observation in episode]\n    \n    # Creating labels array\n    labels = np.array([observation[1] for observation in train_data])\n    \n    # Storing only observations in train_data array\n    train_data = np.array([observation[0] for observation in train_data])\n    print(f'Total observations: {train_data.shape[0]}' )\n    \n    return train_data, labels\n","8d0898af":"# Generating first training data\ntrain_data, labels = generate_data(n_games = 2000)","289a67cf":"# Weights initializer\ninit = he_normal(seed = 666)\n\nmodel = Sequential()\n\n# We are using observations from environment as input data, so input shape of our ANN is (4, )\nmodel.add(Dense(64, input_shape = (4,), activation = 'relu', kernel_initializer = init))\nmodel.add(Dense(128, activation = 'relu', kernel_initializer = init))\n\n# Because our action can be only 0 or 1, I'll use Dense layer with one neuron and sigmoid activation function\nmodel.add(Dense(1, activation = 'sigmoid'))\n\n# Compile model using SGD and binary_crossentropy\nmodel.compile(optimizer = 'sgd', loss = 'binary_crossentropy')","a90a4b6a":"# Plotting learnig curves\n\ndef plot_loss():    \n    H = model.history.history\n    \n    plt.figure(figsize = (15, 5))\n    plt.plot(H['loss'], label = 'loss')\n    plt.plot(H['val_loss'], label = 'val_loss')\n    plt.grid()\n    plt.legend()\n    plt.show()","903e1fe9":"# Model training\nmodel.fit(train_data, labels, epochs = 100, batch_size = 32, validation_split = 0.2, verbose = 0)\nplot_loss()","5120ce7d":"print('Ahmed Affan')","a909a321":"#### Lets create an environment and look at it in details","ad677918":"## Algorithm implementation\n#### Let's start coding. The main piece of code - is a function that will generate and preprocess data.","9f6083ac":"##### The goal of the environment is to hold pole in vertical position moving cart left or right. The game is completed if the pole is balanced for 200 episodes. The environment looks like this:","7fa1b12e":"## Algorithm\n#### To solve this environment a cross-entropy algorithm have been used. The algorithm can be written in several steps:\n\n1. Play N numbers of games using random actions or actions predicted by model to collect raw data.\n2. Collect a total reward for each game and calculate threshold - 70 percentile of all total rewards.\n3. Select games from raw data which have total reward more than threshold.\n4. Train model on selected data where observations are an input to the model and actions are targets.\n5. Repeat from step 1 untill good results.\n\n#### In this kernel I want to train model with two steps - first, on data, generated using random actions, second, on data, generated using actions, predicted by the model.","27f944ed":"## Model creation\n* This is step 4 of our algorithm - train model on selected data where observations are an input to the model and actions are targets.\n\n* As a model I'll use simple ANN with two hidden layers (64, and 128 neurons).","ab73b8dc":"## Conclusion\n* And this is all here. The cross-entropy method is very simple, I'd rather say it's primitive, but it works very well for simple tasks like CartPole.\n\n* Of course the code here is just a baseline and can be easly improved or written as a convenient class, but my main goal was to show you the algorithm and implement it step by step, keeping things as simple as possible, so I hope it will help anybody who doing first steps in deep reinforcement learning area","70afac75":"## GOAL\n##### Reinforcement learning is a very interesting area which studies creation of self-educating agents, which can solve different tasks in   different environments. In this kernel I'm going to solve classic OpenAI Gym CartPole-v0 environment using cross-entropy method. So let's start.\n\n","308757e6":"## Model training - sample actions data\n* First - I'll train model using data, generated on random actions. To plot loss I'll also create a plot_loss function:"}}