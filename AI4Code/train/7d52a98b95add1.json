{"cell_type":{"c6594e1f":"code","f2fea6c8":"code","d4a7f1d2":"code","521f4592":"code","fcc7c3e3":"code","b6cf1ee4":"code","00ce27ac":"code","08f69d1e":"code","d8bb4c2d":"code","b344896c":"code","acefddbd":"code","0185dd8b":"code","4898d0a8":"code","78cbbf49":"code","2c5dcbb8":"code","30a307a8":"code","7d224226":"code","d8e5355a":"code","95b3d768":"code","7066f8b2":"code","94abe6ad":"code","87337833":"code","6865e980":"code","4ed0bf6e":"code","ac1b78f1":"code","a0124c60":"code","4e70959e":"code","ca3b5ead":"code","1ee7f4de":"code","b08be09d":"code","d42672d9":"code","2ac0e09a":"code","1a88f3e1":"code","01a4c195":"code","688cb9d5":"code","35b77194":"code","5c501779":"code","419c23bf":"code","6259736a":"markdown","6367a509":"markdown","0dd0f042":"markdown","2439d109":"markdown","4332bd94":"markdown","1fc5f78d":"markdown","2499f721":"markdown","009bcb64":"markdown","6fd5da62":"markdown"},"source":{"c6594e1f":"#Importing Libraries\n\nimport pandas as pd\nimport numpy as np\nimport re \nimport nltk \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\n\nfrom nltk.corpus import stopwords\nfrom tqdm import tqdm\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nimport keras\nfrom keras.layers import Dense,LSTM,Embedding,Input,GlobalMaxPool1D\nfrom keras.models import Sequential\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom keras.callbacks import EarlyStopping,ReduceLROnPlateau\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n\nfrom io import StringIO\nfrom sklearn.feature_selection import chi2\nfrom IPython.display import display\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\n\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.style.use('ggplot')\nimport matplotlib.patches as mpatches","f2fea6c8":"#Importing Dataset\n\ntrain = pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\")\ntest = pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv\")\ntrain.head()","d4a7f1d2":"train.info()","521f4592":"test.head()","fcc7c3e3":"test.isnull().any()","b6cf1ee4":"train.shape","00ce27ac":"test.shape","08f69d1e":"sns.set_style(\"whitegrid\")\nsns.set(rc={'figure.figsize':(11,4)})\nsns.countplot(train['Sentiment'])","d8bb4c2d":"def change_sen(sentiment):\n    if sentiment == \"Extremely Positive\":\n        return 'positive'\n    elif sentiment == \"Extremely Negative\":\n        return 'negative'\n    elif sentiment == \"Positive\":\n        return 'positive'\n    elif sentiment == \"Negative\":\n        return 'negative'\n    else:\n        return 'netural'","b344896c":"train['Sentiment']=train['Sentiment'].apply(lambda x:change_sen(x))\ntest['Sentiment']=test['Sentiment'].apply(lambda x:change_sen(x))","acefddbd":"train.head()","0185dd8b":"from nltk.corpus import stopwords\n# load stop words\nstop_word = stopwords.words('english')","4898d0a8":"#Cleaning the tweet\n\ndef clean(text):\n    #     remove urls\n    text = re.sub(r'http\\S+', \" \", text)\n    #     remove mentions\n    text = re.sub(r'@\\w+',' ',text)\n    #     remove hastags\n    text = re.sub(r'#\\w+', ' ', text)\n    #     remove digits\n    text = re.sub(r'\\d+', ' ', text)\n    #     remove html tags\n    text = re.sub('r<.*?>',' ', text) \n    #     remove stop words \n    text = text.split()\n    text = \" \".join([word for word in text if not word in stop_word])\n        \n    return text","78cbbf49":"train['OriginalTweet'] = train['OriginalTweet'].apply(lambda x: clean(x))\ntest['OriginalTweet'] = test['OriginalTweet'].apply(lambda x: clean(x))","2c5dcbb8":"train = train.iloc[:,4:]\ntest = test.iloc[:,4:]","30a307a8":"train.head()","7d224226":"test.head()","d8e5355a":"label_encoder = preprocessing.LabelEncoder()\n  \n# Encode labels in column 'Sentiment'.\ntrain['Sentiment']= label_encoder.fit_transform(train['Sentiment'])\n  \ntrain['Sentiment'].unique()","95b3d768":"train.head()","7066f8b2":"train_text,val_text,train_label,val_label=train_test_split(train.OriginalTweet,train.Sentiment,\n                                                             test_size=0.3,random_state=42)","94abe6ad":"class Lemmatizer(object):\n    def __init__(self):\n        self.lemmatizer = WordNetLemmatizer()\n    def __call__(self, sentence):\n        sentence=re.sub('(https?:\\\/\\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\\/\\w \\.-]*)',' ',sentence)\n        sentence=re.sub('[^0-9a-z]',' ',sentence)\n        return [self.lemmatizer.lemmatize(word) for word in sentence.split() if len(word)>1]","87337833":"tokenizer=CountVectorizer(max_features=5000,stop_words='english',lowercase=True,tokenizer=Lemmatizer())","6865e980":"train_x=tokenizer.fit_transform(train_text).toarray()","4ed0bf6e":"tokenizer.get_params()","ac1b78f1":"feature_names=tokenizer.get_feature_names()","a0124c60":"val_x=tokenizer.transform(val_text).toarray()","4e70959e":"test_x=test.OriginalTweet\ntest_label=label_encoder.transform(test['Sentiment'])","ca3b5ead":"test_x_1=tokenizer.transform(test_x).toarray()","1ee7f4de":"early_stop=EarlyStopping(monitor='val_accuracy',patience=3)\nreduceLR=ReduceLROnPlateau(monitor='val_accuarcy',patience=2)","b08be09d":"token=Tokenizer(num_words=5000,oov_token=Lemmatizer())\ntoken.fit_on_texts(train_text)\ntrain_x_2=token.texts_to_sequences(train_text)\ntrain_x_2=pad_sequences(train_x_2,maxlen=60,padding='post',truncating='post')","d42672d9":"val_x_2=token.texts_to_sequences(val_text)\nval_x_2=pad_sequences(val_x_2,maxlen=60,padding='post',truncating='post')","2ac0e09a":"embedding_dimension=32\nv=len(token.word_index)\nmodel=Sequential()\nmodel.add(Input(shape=(60,)))\nmodel.add(Embedding(v+1,embedding_dimension))\nmodel.add(LSTM(64,return_sequences=True))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(64))\nmodel.add(Dense(3,activation='softmax'))","1a88f3e1":"model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\nr=model.fit(train_x_2,train_label,validation_data=(val_x_2,val_label),\n            epochs=50,batch_size=64,callbacks=[reduceLR,early_stop])","01a4c195":"plt.plot(r.history['loss'])\nplt.plot(r.history['val_loss'])\nplt.title('LOSS',fontdict={'size':'22'})\nplt.plot()","688cb9d5":"test_x_2=token.texts_to_sequences(test['OriginalTweet'])\ntest_x_2=pad_sequences(test_x_2,maxlen=60,padding='post',truncating='post')","35b77194":"print(classification_report(test_label,model.predict_classes(test_x_2)))","5c501779":"from keras.layers import Bidirectional\n\nembedding_dimension=32\nv=len(token.word_index)\nmodelnew=Sequential()\nmodelnew.add(Input(shape=(60,)))\nmodelnew.add(Embedding(v+1,embedding_dimension))\nmodelnew.add(Bidirectional(LSTM(64,return_sequences=True)))\nmodelnew.add(GlobalMaxPool1D())\nmodelnew.add(Dense(64))\nmodelnew.add(Dense(3,activation='softmax'))","419c23bf":"modelnew.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\nr=modelnew.fit(train_x_2,train_label,validation_data=(val_x_2,val_label),\n            epochs=50,batch_size=64,callbacks=[reduceLR,early_stop])","6259736a":"### Accuracy on Test set","6367a509":"## LSTM","0dd0f042":"### Changing the sentiment to 3 classes only","2439d109":"## Lemmatization\n#### In lemmatization, we try to reduce a given word to its root word.","4332bd94":"## EDA","1fc5f78d":"### We only need Original Tweet and Sentiment column","2499f721":"### Only Location has some null values but that column is not important so we won't drop the whole row because of Location null","009bcb64":"## Negative 0\n## Neutral 1\n## Positive 2","6fd5da62":"## Bidirectional LSTM"}}