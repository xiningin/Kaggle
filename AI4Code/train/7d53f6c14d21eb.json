{"cell_type":{"45882fa5":"code","db154934":"code","b36f1f20":"code","12d6ff71":"code","c78cea74":"code","c431c445":"code","2f651170":"code","0112465b":"code","7bc4aebc":"code","f90c1b75":"code","12bd6ef2":"code","2a1ea280":"code","d5b8861e":"code","2be9f6dc":"code","caad02a4":"code","6f4d38f8":"code","b961749d":"code","10b3200c":"code","63c73202":"code","0bd12cb5":"code","c208410d":"code","3f87b915":"code","b57b9538":"code","d566ad4b":"code","27fa0b7f":"code","ef3c8e64":"code","c92d1865":"code","bc6332f1":"code","9349cd08":"markdown","324fe928":"markdown","ef34c433":"markdown","86b52128":"markdown","95b36d5c":"markdown","95a23b4e":"markdown","b4297b3e":"markdown","f7e3407a":"markdown","49cad0a8":"markdown","361f260c":"markdown","68f47c9f":"markdown","dd40dd42":"markdown","15659ccd":"markdown","313811c1":"markdown","c849e408":"markdown","117dedd3":"markdown","88fb634a":"markdown","52af0f66":"markdown","aaea8174":"markdown","2952ac8e":"markdown","ef08c0d8":"markdown","ed9d2634":"markdown"},"source":{"45882fa5":"# warning\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# for NLP\nimport spacy\nfrom spacy.lang.en import English\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport en_core_web_sm\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.corpora import Dictionary\n\n# for data processing\nimport pandas as pd\nimport numpy as np\n\n# for statistics\nimport statistics\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nimport collections\n\n# for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nimport umap\n\n# for network analysis\nimport networkx as nx\n\n# for modeling\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nfrom sklearn import metrics","db154934":"train = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\ntrain['flag'] = 'train'\ntest = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')\ntest['flag'] = 'test'\nalldata = pd.concat([train,test], axis=0, ignore_index=True)","b36f1f20":"alldata['url_YN'] = [0 if pd.isna(i) else 1 for i in alldata['url_legal']]\nalldata['license_YN'] = [0 if pd.isna(i) else 1 for i in alldata['license']]\nalldata['url_domain'] = [\"\" if pd.isna(s) else s.split('\/')[2] for s in alldata['url_legal']]","12d6ff71":"CC_3 = ['CC BY 3.0', 'CC BY-NC 3.0', 'CC BY-NC-SA 3.0 ', 'CC BY-SA 3.0', 'CC BY-SA 3.0 and GFD']\nCC_4 = ['CC BY 4.0', 'CC BY-NC-ND 4.0', 'CC BY-NC-SA 4.0']\n\nlicense_cat = []\n\nfor cat in alldata['license']:\n    if cat in CC_3:\n        license_cat.append('CC_3')\n    elif cat in CC_4:\n        license_cat.append('CC_4')\n    elif pd.isna(cat):\n        license_cat.append('nan')\n    else:\n        license_cat.append('others')\n\nalldata['license_cat'] = license_cat\n        \nprint(alldata.groupby('license').count()['id'])\nprint(alldata.groupby('license_cat').count()['id'])","c78cea74":"wiki = ['en.wikibooks.org', 'en.wikipedia.org']\nsimple = ['simple.wikipedia.org']\nafrican = ['www.africanstorybook.org']\nkids = ['kids.frontiersin.org']\nlit = ['www.commonlit.org']\n\nurl_cat = []\n\nfor cat in alldata['url_domain']:\n    if cat in wiki:\n        url_cat.append('wiki')\n    elif cat in simple:\n        url_cat.append('simple')\n    elif cat in african:\n        url_cat.append('african')\n    elif cat in kids:\n        url_cat.append('kids')\n    elif cat in lit:\n        url_cat.append('lit')\n    elif cat=='':\n        url_cat.append('nan')\n    else:\n        url_cat.append('others')\n        \nalldata['url_cat'] = url_cat\n\nprint(alldata.groupby('url_domain').count()['id'])\nprint(alldata.groupby('url_cat').count()['id'])","c431c445":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\nsns.boxplot(x=alldata['license_cat'],y=alldata['target'], ax=ax1)\nsns.boxplot(x=alldata['url_cat'],y=alldata['target'], ax=ax2)","2f651170":"# sentence count and words in sentence count\nnlp = English()\nsbd = nlp.create_pipe('sentencizer')\nnlp.add_pipe(sbd)\nnlp = spacy.load(\"en_core_web_sm\")","0112465b":"sent_list = []\nnum_sent_list = []\nnum_word_list = []\nnum_word_stats_list = []\n\nremove_list = [',', '.', ':', ';', '!', '?', '\\n', '-', '=', '\/', '#', '$', '(', ')']\n\nfor doc in alldata['excerpt']:\n    doc = nlp(doc)\n    sent_temp = [s for s in doc.sents]\n    sent_list.append(sent_temp)\n    num_sent_list.append(len(sent_temp))\n    \n    num_word_temp = []\n    word_list = []\n    for s in sent_temp:\n        text_temp = [token.text for token in s]\n        word_list = [w for w in text_temp if w not in remove_list]\n        num_word_temp.append(len(word_list))\n        \n    num_word_stats_list.append([min(num_word_temp), max(num_word_temp), statistics.mean(num_word_temp), statistics.median(num_word_temp), statistics.stdev(num_word_temp)])\n    num_word_list.append(num_word_temp)","7bc4aebc":"df_statistics = pd.DataFrame({'num_sentence':num_sent_list,\n                              'num_word_min':[x[0] for x in num_word_stats_list],\n                              'num_word_max':[x[1] for x in num_word_stats_list],\n                              'num_word_mean':[x[2] for x in num_word_stats_list],\n                              'num_word_median':[x[3] for x in num_word_stats_list],\n                              'num_word_stdev':[x[4] for x in num_word_stats_list]})","f90c1b75":"statistics_col = df_statistics.columns\nfig,axes = plt.subplots(nrows=3,ncols=2,figsize=(14,14))\n\nfor i in range(3):\n    for j in range(2):\n        axes[i,j].scatter(df_statistics[statistics_col[j+(i*2)]],alldata['target'])\n        axes[i,j].set_title(statistics_col[j+(i*2)])","12bd6ef2":"for i in range(len(alldata)):\n    doc_id = alldata['id'][i]\n    test_doc = nlp(alldata['excerpt'][i])\n    token_pos = [w.pos_ for w in test_doc]\n    c = collections.Counter(token_pos)\n    token_df_temp = pd.DataFrame.from_dict(c, orient='index', columns=[doc_id])\n    token_df_temp[doc_id] = token_df_temp[doc_id]\/sum(token_df_temp[doc_id])\n    \n    if i == 0:\n        token_df = token_df_temp\n    else:\n        token_df = token_df.merge(token_df_temp,left_index=True, right_index=True, how='outer')","2a1ea280":"token_df = token_df.fillna(0).T","d5b8861e":"fig,axes = plt.subplots(nrows=6,ncols=3,figsize=(16,24))\npos_name_list = token_df.columns\n\nfor i in range(6):\n    for j in range(3):\n        axes[i,j].scatter(token_df[pos_name_list[j+(i*3)]],alldata['target'])\n        axes[i,j].set_title(pos_name_list[j+(i*3)])","2be9f6dc":"for i in range(len(alldata)):\n    doc_id = alldata['id'][i]\n    test_doc = nlp(alldata['excerpt'][i])\n    token_pos = [w.dep_ for w in test_doc]\n    c = collections.Counter(token_pos)\n    dep_df_temp = pd.DataFrame.from_dict(c, orient='index', columns=[doc_id])\n    dep_df_temp[doc_id] = dep_df_temp[doc_id]\/sum(dep_df_temp[doc_id])\n    \n    if i == 0:\n        dep_df = dep_df_temp\n    else:\n        dep_df = dep_df.merge(dep_df_temp,left_index=True, right_index=True, how='outer')","caad02a4":"dep_df = dep_df.fillna(0).T","6f4d38f8":"fig,axes = plt.subplots(nrows=9,ncols=5,figsize=(16,24))\ndep_name_list = dep_df.columns\n\nfor i in range(9):\n    for j in range(5):\n        #print(complexity_col[j+(i*2)])\n        axes[i,j].scatter(dep_df[dep_name_list[j+(i*5)]],alldata['target'])\n        axes[i,j].set_title(dep_name_list[j+(i*5)])","b961749d":"test_doc = nlp(alldata['excerpt'][0])\ntest_sent = [s for s in test_doc.sents]\n\nsource_list = []\ntarget_list = []\n\nfor token in test_sent[0]:\n    if token.pos_=='PUNCT' or token.dep_=='ROOT':\n        continue\n    else:\n        source_list.append(token.head.i)\n        target_list.append(token.i)\n        \ndf_dep = pd.DataFrame({'source':source_list,\n                       'target':target_list})\nprint(df_dep)\ndep_g = nx.from_pandas_edgelist(df_dep)\nnx.draw_networkx(dep_g)","10b3200c":"def create_edge_df(doc):\n    doc = nlp(doc)\n    source_list = []\n    target_list = []\n\n    for token in doc:\n        if token.pos_=='PUNCT' or token.dep_=='ROOT':\n            continue\n        else:\n            source_list.append(token.head.i)\n            target_list.append(token.i)\n\n    df_dep = pd.DataFrame({'source':source_list,\n                           'target':target_list})\n    \n    return(df_dep)","63c73202":"def create_dep_network(doc):\n    edge_df = create_edge_df(doc)\n    g = nx.from_pandas_edgelist(edge_df)\n    return(g)","0bd12cb5":"def calculate_index(doc):\n    g = create_dep_network(doc)\n    index_dict = {}\n    # degree\n    degree_df_temp = pd.DataFrame.from_dict(dict(nx.degree(g)), orient='index')\n    # density\n    density_list = [nx.density(g.subgraph(c)) for c in nx.connected_components(g)]\n    # diameter\n    diameter_list = [nx.diameter(g.subgraph(c)) for c in nx.connected_components(g)]\n    \n    index_dict = {'degree_min':degree_df_temp[0].min(),\n                  'degree_max':degree_df_temp[0].max(),\n                  'degree_mean':degree_df_temp[0].mean(),\n                  'degree_median':degree_df_temp[0].median(),\n                  'degree_std':degree_df_temp[0].std(),\n                  'density_min':min(density_list),\n                  'density_max':max(density_list),\n                  'density_mean':statistics.mean(density_list),\n                  'density_median':statistics.median(density_list),\n                  'density_std':statistics.stdev(density_list),\n                  'diameter_min':min(diameter_list),\n                  'diameter_max':max(diameter_list),\n                  'diameter_mean':statistics.mean(diameter_list),\n                  'diameter_median':statistics.median(diameter_list),\n                  'diameter_std':statistics.stdev(diameter_list)}\n    return(index_dict)","c208410d":"for i in range(len(alldata)):\n    doc_id = alldata['id'][i]\n    doc = alldata['excerpt'][i]\n    index_dict = calculate_index(doc)\n\n    index_df_temp = pd.DataFrame.from_dict(index_dict, orient='index', columns=[doc_id])\n    \n    if i == 0:\n        index_df = index_df_temp\n    else:\n        index_df = index_df.merge(index_df_temp,left_index=True, right_index=True, how='outer')\n        \nindex_df = index_df.T","3f87b915":"fig,axes = plt.subplots(nrows=5,ncols=3,figsize=(16,24))\nindex_name_list = index_df.columns\n\nfor i in range(5):\n    for j in range(3):\n        #print(complexity_col[j+(i*2)])\n        axes[i,j].scatter(index_df[index_name_list[j+(i*3)]],alldata['target'])\n        axes[i,j].set_title(index_name_list[j+(i*3)])","b57b9538":"test_doc = nlp(alldata['excerpt'][0])\ntest_sent = [s for s in test_doc.sents]\nprint('word vector for sentence : ', test_sent[0].vector)\nprint('word vector length : ', len(test_sent[0].vector))","d566ad4b":"POS = ['PRON', 'PROPN', 'NOUN', 'VERB', 'ADJ', 'ADV','ADP']\nnew_docs = []\ndocs = alldata['excerpt']\nfor doc in docs:\n    doc = nlp(doc)\n    new_docs.append(\" \".join([token.lemma_ for token in doc if token.pos_ in POS or len(POS) ==0]))","27fa0b7f":"new_wv_list = []\n\nfor doc in new_docs:\n    doc_temp = nlp(doc)\n    new_wv_list.append(doc_temp.vector)","ef3c8e64":"df_wv_new = pd.DataFrame(new_wv_list)","c92d1865":"pca = PCA(n_components = 10)\nres_pca_10 = pca.fit_transform(df_wv_new)","bc6332f1":"fig,axes = plt.subplots(nrows=5,ncols=2,figsize=(16,32))\n\nfor i in range(5):\n    for j in range(2):\n        axes[i,j].scatter(res_pca_10[:,j+(i*2)],alldata['target'])\n        axes[i,j].set_title('PCA'+str(j+(i*2)))","9349cd08":"## PCA for word vector\n* Word vector in Spacy has 96 dimensions. Plotting them directly is difficult to understand.\n* So I plotted some principle components vs target score.","324fe928":"## URL","ef34c433":"# Load library","86b52128":"## dependency type","95b36d5c":"## License","95a23b4e":"## Extract features from network analysis","b4297b3e":"# Dependency network analysis","f7e3407a":"## extract word vector\n* We can extract word vector from SpaCy default.\n* If it is document, we can extract mean word vector.","49cad0a8":"# Word vector from SpaCy","361f260c":"**Wow, VERV ratio and NOUN ratio may affect on readability!**","68f47c9f":"# Load dataset","dd40dd42":"**The distributions of target score in different license\/URL differ. This may effect on target score, but observation which has URL and License is very few.**","15659ccd":"**Oh, some depencency types have effects on readability!**","313811c1":"# Word type and dependency type\n* Dose the ratio of word type (e.g. ADJ, NOUN, VERB etc) in the sentence affect on readability?\n* Dose the ratio of dependency type (e.g. prep, proj, det etc) in the sentence affect on readability?","c849e408":"# Basic statistics\n* Explore the effect of sentense length and its statistics.","117dedd3":"## sentense as a network\n* Using depencensy analysis result, we can analysis the sentense as a network. So we can extract some network indice (e.g. degree, density, diameter etc) as features.","88fb634a":"Number of words in sentense may affect on readability. Intuitively, it is reasonabole because students cannot understand the meaning clearly if the sentence is too long.","52af0f66":"## extract word vector from all document\n* But before extraction, we need to some word type because those cause noise.\n* Please imagine, the word 'it' has a vector, the punctuation ',' also has a vectore. Do these words\/punctuation contribute to total meaning?","aaea8174":"**Yes, 1st principal component related to target score!**","2952ac8e":"# Explore URL and License\n* This dataset has source URL and License in about 800 observations.\n* First of all, I explored effects of URL and License on target.","ef08c0d8":"## word type","ed9d2634":"**It's super interesting!**"}}