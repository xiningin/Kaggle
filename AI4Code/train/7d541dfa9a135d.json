{"cell_type":{"c073668c":"code","2a60050e":"code","da880be6":"code","4e6f30d0":"code","7d315f5c":"code","f74db8f9":"code","0d552b46":"code","91a765e4":"code","0450f8a0":"code","0241b66e":"code","f423aabe":"code","21e61a51":"code","e2b617dc":"code","e0674f85":"code","e990d4c9":"code","97587df8":"code","fc2963c9":"code","00c1b09f":"code","bc01fef2":"code","ad1aef31":"code","15a13765":"code","5754a175":"code","42282ac3":"code","4c338244":"code","840ad685":"code","839bb565":"code","7573d015":"code","0b28e090":"code","f8400a7a":"markdown","7bd4b2fb":"markdown","b6bc1b0d":"markdown","1b9e25e7":"markdown","d8c1d0bb":"markdown","1b9b33af":"markdown","d63540d3":"markdown","7aa90da1":"markdown","144e94cb":"markdown","29d0b819":"markdown","487e439c":"markdown","f0c9c601":"markdown","00978495":"markdown","9b0bca1c":"markdown","c79c3326":"markdown","84ded993":"markdown","eae9809d":"markdown","0ea5c8f8":"markdown","a06690cc":"markdown","f811d14a":"markdown","2218d658":"markdown","4bb2e1cf":"markdown","6b871650":"markdown","f05ab787":"markdown","894ccbb9":"markdown","b494460d":"markdown","6251192a":"markdown","7f669dea":"markdown","b0c30656":"markdown","6c8e916f":"markdown"},"source":{"c073668c":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Data Cleaning\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# PreProcessing\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\n# Splitting Data\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n\n# Modeling\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.svm import LinearSVC\nfrom xgboost.sklearn import XGBClassifier\n\n# Resampling\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.combine import SMOTETomek","2a60050e":"survey = pd.read_csv('..\/input\/starbucks-customer-retention-malaysia-survey\/Starbucks satisfactory survey.csv')\nsurvey.head()","da880be6":"survey.info()","4e6f30d0":"sb = survey.copy()","7d315f5c":"sb.drop(columns=['Timestamp'], inplace = True)","f74db8f9":"sb.rename({'1. Your Gender' : 'Gender', '2. Your Age' : 'Age', '3. Are you currently....?' : 'Working_Status', '4. What is your annual income?' : 'Annual_Income', '5. How often do you visit Starbucks?' : 'Visit_Duration', '6. How do you usually enjoy Starbucks?' : 'Visit_Plan', '7. How much time do you normally  spend during your visit?' : 'Spending_Time', \"8. The nearest Starbucks's outlet to you is...?\" : 'Outlet_Location', '9. Do you have Starbucks membership card?' : 'Member_Card', '10. What do you most frequently purchase at Starbucks?' : 'Frequent_Purchase', '11. On average, how much would you spend at Starbucks per visit?' : 'Average_Spending', '12. How would you rate the quality of Starbucks compared to other brands (Coffee Bean, Old Town White Coffee..) to be:' : 'Product_Rating', '13. How would you rate the price range at Starbucks?' : 'Price_Rating', '14. How important are sales and promotions in your purchase decision?' : 'Promotion_Rating', '15. How would you rate the ambiance at Starbucks? (lighting, music, etc...)' : 'Ambiance_Rating', '16. You rate the WiFi quality at Starbucks as..' : 'Wifi_Rating', '17. How would you rate the service at Starbucks? (Promptness, friendliness, etc..)' : 'Service_Rating', '18. How likely you will choose Starbucks for doing business meetings or hangout with friends?' : 'Hangout_Place_Rating', '19. How do you come to hear of promotions at Starbucks? Check all that apply.' : 'Promotion_Tools', '20. Will you continue buying at Starbucks?' : 'Loyal_Customer'} , inplace = True , axis = 1)","0d552b46":"sb.isna().sum()\/len(sb.index)*100","91a765e4":"sb['Visit_Plan'].unique()","0450f8a0":"sb['Visit_Plan'].replace(['never', 'Never buy', 'I dont like coffee', 'Never', 'Never ',], 'Never buy', inplace = True)\nsb['Visit_Plan'].value_counts()","0241b66e":"sb['Frequent_Purchase'].unique()","f423aabe":"cvr = CountVectorizer(tokenizer = lambda x:x.split(';'))\npurchase = cvr.fit_transform(sb['Frequent_Purchase'])\n\nprint(cvr.get_feature_names())","21e61a51":"purchase_value = pd.DataFrame(purchase.toarray(), columns = cvr.get_feature_names())\npurchase_value['Never_Buy']= purchase_value.iloc[:, -6:-3].sum(axis=1)\n\npurchase_value.drop(columns=['never', 'never buy any', 'nothing '], inplace = True)\n\npurchase_value.rename({'cake ' : 'Buy_Cake', 'coffee' : 'Buy_Coffee', 'cold drinks' : 'Buy_ColdDrinks', 'jaws chip ' : 'Buy_JawsChip', 'juices' : 'Buy_Juices', 'pastries' : 'Buy_Pastries', 'sandwiches' : 'Buy_Sandwiches'}, inplace = True , axis = 1)\n\nsb = pd.concat([sb, purchase_value], axis = 1)\nsb","e2b617dc":"sb['Promotion_Tools'].unique()","e0674f85":"sb['Promotion_Tools'].replace([np.nan,], 'Social Media', inplace = True)","e990d4c9":"cvr = CountVectorizer(tokenizer = lambda x:x.split(';'))\npromo = cvr.fit_transform(sb['Promotion_Tools'])\n\nprint(cvr.get_feature_names())","97587df8":"promo_value = pd.DataFrame(promo.toarray(), columns = cvr.get_feature_names())\n\npromo_value.rename({'application offer' : 'Promo_AppsOffer', 'billboards' : 'Promo_Billboards', 'deal sites (fave, iprice, etc...)' : 'Promo_Sites', 'emails' : 'Promo_Emails', 'in store displays' : 'Promo_StoreDisplay', 'never hear' : 'Never_Heard', 'social media' : 'Promo_SocMed', 'starbucks website\/apps' : 'Promo_SBucksApps', 'through friends and word of mouth' : 'Promo_WoM'} , inplace = True , axis = 1)\n\nsb = pd.concat([sb, promo_value], axis = 1)\nsb","fc2963c9":"sb.drop(columns=['Frequent_Purchase', 'Promotion_Tools'], inplace = True)","00c1b09f":"sbucks = sb.copy()\nsbucks.head()","bc01fef2":"mode_onehot_pipe = Pipeline([\n    ('encoder', SimpleImputer(strategy = 'most_frequent')),\n    ('one hot encoder', OneHotEncoder(handle_unknown = 'ignore'))])\n\ntransformer = ColumnTransformer([\n    ('one hot', OneHotEncoder(handle_unknown = 'ignore'), ['Gender', 'Age', 'Working_Status', 'Annual_Income', 'Visit_Duration', 'Spending_Time', 'Outlet_Location', 'Member_Card', 'Average_Spending']),\n    ('mode_onehot_pipe', mode_onehot_pipe, ['Visit_Plan']),\n], remainder = 'passthrough')","ad1aef31":"sbucks['Loyal_Customer'].value_counts()\/sbucks.shape[0]*100","15a13765":"sbucks['Loyal_Customer'] = np.where(sbucks['Loyal_Customer'] == 'Yes', 1, 0)","5754a175":"X = sbucks.drop('Loyal_Customer', axis = 1)\ny = sbucks['Loyal_Customer']\n\nX.shape","42282ac3":"X_train, X_test, y_train, y_test = train_test_split(X,y, stratify = y, test_size = 0.3, random_state = 3434)","4c338244":"logreg = LogisticRegression(random_state = 3434)\ntree = DecisionTreeClassifier(random_state = 3434)\nknn = KNeighborsClassifier()\nrf = RandomForestClassifier(random_state = 3434)\nsvc = LinearSVC(random_state = 3434)\nada = AdaBoostClassifier(random_state = 3434)\ngrad = GradientBoostingClassifier(random_state = 3434)\nxgb = XGBClassifier(verbosity = 0, random_state = 3434)","840ad685":"logreg_pipe = Pipeline([('transformer', transformer), ('logreg', logreg)])\ntree_pipe = Pipeline([('transformer', transformer), ('tree', tree)])\nknn_pipe = Pipeline([('transformer', transformer), ('knn', knn)])\nrf_pipe = Pipeline([('transformer', transformer), ('rf', rf)])\nsvc_pipe = Pipeline([('transformer', transformer), ('svc', svc)])\nada_pipe = Pipeline([('transformer', transformer), ('ada', ada)])\ngrad_pipe = Pipeline([('transformer', transformer), ('grad', grad)])\nxgb_pipe = Pipeline([('transformer', transformer), ('xgb', xgb)])\n\nfor model in [logreg_pipe, tree_pipe, knn_pipe, rf_pipe, svc_pipe, ada_pipe, grad_pipe, xgb_pipe]:\n    model.fit(X_train, y_train)\n\nscore_acc = [accuracy_score(y_test, logreg_pipe.predict(X_test)),\n             accuracy_score(y_test, tree_pipe.predict(X_test)),\n             accuracy_score(y_test, knn_pipe.predict(X_test)),\n             accuracy_score(y_test, rf_pipe.predict(X_test)),\n             accuracy_score(y_test, svc_pipe.predict(X_test)),\n             accuracy_score(y_test, ada_pipe.predict(X_test)),\n             accuracy_score(y_test, grad_pipe.predict(X_test)),\n             accuracy_score(y_test, xgb_pipe.predict(X_test))]\nmethod_name = ['Logistic Regression', 'Decision Tree Classifier', 'KNN Classifier', 'Random Forest Classifier', 'LinearSVC', 'AdaBoost Classifier', 'Gradient Boosting Classifier', 'XGB Classifier']\n\nacc_summary = pd.DataFrame({'method': method_name, 'accuracy score': score_acc})\nacc_summary","839bb565":"rus = RandomUnderSampler(random_state = 3434)\n\nlogreg_pipe_rus = Pipeline([('transformer', transformer), ('rus', rus), ('logreg', logreg)])\ntree_pipe_rus = Pipeline([('transformer', transformer), ('rus', rus), ('tree', tree)])\nknn_pipe_rus = Pipeline([('transformer', transformer), ('rus', rus), ('knn', knn)])\nrf_pipe_rus = Pipeline([('transformer', transformer), ('rus', rus), ('rf', rf)])\nsvc_pipe_rus = Pipeline([('transformer', transformer), ('rus', rus), ('svc', svc)])\nada_pipe_rus = Pipeline([('transformer', transformer), ('rus', rus), ('ada', ada)])\ngrad_pipe_rus = Pipeline([('transformer', transformer), ('rus', rus), ('grad', grad)])\nxgb_pipe_rus = Pipeline([('transformer', transformer), ('rus', rus), ('xgb', xgb)])\n\nfor model in [logreg_pipe_rus, tree_pipe_rus, knn_pipe_rus, rf_pipe_rus, svc_pipe_rus, ada_pipe_rus, grad_pipe_rus, xgb_pipe_rus]:\n    model.fit(X_train, y_train)\n\nscore_acc = [accuracy_score(y_test, logreg_pipe_rus.predict(X_test)),\n             accuracy_score(y_test, tree_pipe_rus.predict(X_test)),\n             accuracy_score(y_test, knn_pipe_rus.predict(X_test)),\n             accuracy_score(y_test, rf_pipe_rus.predict(X_test)),\n             accuracy_score(y_test, svc_pipe_rus.predict(X_test)),\n             accuracy_score(y_test, ada_pipe_rus.predict(X_test)),\n             accuracy_score(y_test, grad_pipe_rus.predict(X_test)),\n             accuracy_score(y_test, xgb_pipe_rus.predict(X_test))]\nmethod_name = ['Logistic Regression UnderSampling', 'Decision Tree Classifier UnderSampling', 'KNN Classifier UnderSampling', 'Random Forest Classifier UnderSampling', 'LinearSVC UnderSampling', 'AdaBoost Classifier UnderSampling', 'Gradient Boosting Classifier UnderSampling', 'XGB Classifier UnderSampling']\n\nacc_rus_summary = pd.DataFrame({'method': method_name, 'accuracy score': score_acc})\nacc_rus_summary","7573d015":"ros = RandomOverSampler(random_state = 3434)\n\nlogreg_pipe_ros = Pipeline([('transformer', transformer), ('ros', ros), ('logreg', logreg)])\ntree_pipe_ros = Pipeline([('transformer', transformer), ('ros', ros), ('tree', tree)])\nknn_pipe_ros = Pipeline([('transformer', transformer), ('ros', ros), ('knn', knn)])\nrf_pipe_ros = Pipeline([('transformer', transformer), ('ros', ros), ('rf', rf)])\nsvc_pipe_ros = Pipeline([('transformer', transformer), ('ros', ros), ('svc', svc)])\nada_pipe_ros = Pipeline([('transformer', transformer), ('ros', ros), ('ada', ada)])\ngrad_pipe_ros = Pipeline([('transformer', transformer), ('ros', ros), ('grad', grad)])\nxgb_pipe_ros = Pipeline([('transformer', transformer), ('ros', ros), ('xgb', xgb)])\n\nfor model in [logreg_pipe_ros, tree_pipe_ros, knn_pipe_ros, rf_pipe_ros, svc_pipe_ros, ada_pipe_ros, grad_pipe_ros, xgb_pipe_ros]:\n    model.fit(X_train, y_train)\n\nscore_acc = [accuracy_score(y_test, logreg_pipe_ros.predict(X_test)),\n             accuracy_score(y_test, tree_pipe_ros.predict(X_test)),\n             accuracy_score(y_test, knn_pipe_ros.predict(X_test)),\n             accuracy_score(y_test, rf_pipe_ros.predict(X_test)),\n             accuracy_score(y_test, svc_pipe_ros.predict(X_test)),\n             accuracy_score(y_test, ada_pipe_ros.predict(X_test)),\n             accuracy_score(y_test, grad_pipe_ros.predict(X_test)),\n             accuracy_score(y_test, xgb_pipe_ros.predict(X_test))]\nmethod_name = ['Logistic Regression OverSampling', 'Decision Tree Classifier OverSampling', 'KNN Classifier OverSampling', 'Random Forest Classifier OverSampling', 'LinearSVC OverSampling', 'AdaBoost Classifier OverSampling', 'Gradient Boosting Classifier OverSampling', 'XGB Classifier OverSampling']\n\nacc_ros_summary = pd.DataFrame({'method': method_name, 'accuracy score': score_acc})\nacc_ros_summary","0b28e090":"smotetom = SMOTETomek(random_state = 3434)\n\nlogreg_pipe_smotetom = Pipeline([('transformer', transformer), ('smotetomek', smotetom), ('logreg', logreg)])\ntree_pipe_smotetom = Pipeline([('transformer', transformer), ('smotetomek', smotetom), ('tree', tree)])\nknn_pipe_smotetom = Pipeline([('transformer', transformer), ('smotetomek', smotetom), ('knn', knn)])\nrf_pipe_smotetom = Pipeline([('transformer', transformer), ('smotetomek', smotetom), ('rf', rf)])\nsvc_pipe_smotetom = Pipeline([('transformer', transformer), ('smotetomek', smotetom), ('svc', svc)])\nada_pipe_smotetom = Pipeline([('transformer', transformer), ('smotetomek', smotetom), ('ada', ada)])\ngrad_pipe_smotetom = Pipeline([('transformer', transformer), ('smotetomek', smotetom), ('grad', grad)])\nxgb_pipe_smotetom = Pipeline([('transformer', transformer), ('smotetomek', smotetom), ('xgb', xgb)])\n\nfor model in [logreg_pipe_smotetom, tree_pipe_smotetom, knn_pipe_ros, rf_pipe_smotetom, svc_pipe_smotetom, ada_pipe_smotetom, grad_pipe_smotetom, xgb_pipe_smotetom]:\n    model.fit(X_train, y_train)\n\nscore_acc = [accuracy_score(y_test, logreg_pipe_smotetom.predict(X_test)),\n             accuracy_score(y_test, tree_pipe_smotetom.predict(X_test)),\n             accuracy_score(y_test, knn_pipe_smotetom.predict(X_test)),\n             accuracy_score(y_test, rf_pipe_smotetom.predict(X_test)),\n             accuracy_score(y_test, svc_pipe_smotetom.predict(X_test)),\n             accuracy_score(y_test, ada_pipe_smotetom.predict(X_test)),\n             accuracy_score(y_test, grad_pipe_smotetom.predict(X_test)),\n             accuracy_score(y_test, xgb_pipe_smotetom.predict(X_test))]\nmethod_name = ['Logistic Regression SMOTETomek', 'Decision Tree Classifier SMOTETomek', 'KNN Classifier SMOTETomek', 'Random Forest Classifier SMOTETomek', 'LinearSVC SMOTETomek', 'AdaBoost Classifier SMOTETomek', 'Gradient Boosting Classifier SMOTETomek', 'XGB Classifier SMOTETomek']\n\nacc_smotetom_summary = pd.DataFrame({'method': method_name, 'accuracy score': score_acc})\nacc_smotetom_summary","f8400a7a":"* From the SMOTETomek method, there are 3 models that have the highest accuracy score, it's Random Forest, LinearSVC, and Ada Boost Classifier. All of them have been in the same score.","7bd4b2fb":"# PreProcessing","b6bc1b0d":"* Dataset have 35 columns of features and 122 rows.","1b9e25e7":"*Drop Columns*","d8c1d0bb":"* Rename all features name in eda dataset to make it easier while seeing all columns.","1b9b33af":"***Missing Value***","d63540d3":"***Preprocessing Scheme***\n\n- OneHotEncoding: Gender, Age, Working_Status, Annual_Income, Visit_Duration, Spending_Time, Outlet_Location, Member_Card, Average_Spending\n    * Simple Imputer Most Frequent: Visit_Plan\n- PassThrough: Product_Rating, Price_Rating, Promotion_Rating, Ambiance_Rating, Wifi_Rating, Service_Rating, Hangout_Place_Rating, Buy_Cake, Buy_Coffee, Buy_ColdDrinks, Buy_JawsChip, Buy_Juices, Buy_Pastries, Buy_Sandwiches, Never_Buy, Promo_AppsOffer, Promo_Billboards, Promo_Sites, Promo_Emails, Promo_StoreDisplay, Never_Hear, Promo_SocMed, Promo_SBucksApps, Promo_WoM\n- Target: Loyal_Customer","7aa90da1":"* From the RandomOverSampler method, the model that has the highest accuracy score are Logistic Regression, Decision Tree and Ada Boost Classifier. Let's use another method to compare. ","144e94cb":"*Visit_Plan*","29d0b819":"* I have to fill the missing value first with mode so the tokenizer can work.","487e439c":"### Combine Over and Under\n\n*SMOTETomek*","f0c9c601":"***Define Target Data***","00978495":"*Frequent_Purchase*","9b0bca1c":"# Raw Datasets\n\n- Contains with 2 datasets, the questions of survey and a data that already encoded. I only process using survey data.","c79c3326":"*Promotion_Tools*","84ded993":"* From the RandomUnderSampler method, the model with the highest accuracy score is the Random Forest Classifier. Let's use another method. ","eae9809d":"* From the cross validation process, AdaBoost Classifier has the highest accuracy score. Let's continue to handle imbalanced data.","0ea5c8f8":"# Handling Imbalance\n\n### UnderSampling\n\n*RandomUnderSampler*","a06690cc":"***Drop Columns***","f811d14a":"# Modeling\n\n***Define Model***","2218d658":"***Feature's Value Checking***\n\n* In this section, I process the value of columns that not suitable to neatly arranged.","4bb2e1cf":"# Datasets","6b871650":"***Splitting Data***","f05ab787":"### OverSampling\n\n*RandomOverSampler*","894ccbb9":"- Drop Timestamp column in eda dataset because it's contains date, month, year and time the customer took this survey, which is irrelevant.","b494460d":"## Summary\n\n* All tree-based models seem to perform pretty well with imbalanced datasets. Since they work by coming up with conditions\/rules at each stage of splitting, they end up taking both classes into consideration.\n* **From all methods, the highest accuracy score is AdaBoost Classifier with 0.891892. It means 8 to 9 from 10 customers will buy again to Starbucks.**","6251192a":"# Data Cleaning","7f669dea":"* I use 0.3 as default score for test_size and X.shape for random_state so the data will be devided equally.","b0c30656":"- After all data cleaning process, I have 34 features column left.","6c8e916f":"***Rename Columns***"}}