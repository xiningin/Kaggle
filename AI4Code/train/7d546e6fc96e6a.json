{"cell_type":{"2164dddc":"code","e075f7b9":"code","24b8fe52":"code","21618dc4":"code","4f833d0b":"code","078869c6":"code","eef274d3":"code","f92db530":"code","94370f1a":"code","99bfc257":"code","fb968541":"code","7fadde69":"code","76a6cdbf":"code","d9b01d99":"code","909747c5":"code","591ff681":"code","499883ef":"code","a688ed7a":"code","7d0040e4":"code","74d7b5ff":"code","ffc1ad0b":"code","4e12ce54":"code","d0bd2f00":"code","43b3f85c":"code","c5bd975a":"code","f8937fc1":"code","54b2a877":"markdown","c5e06af7":"markdown","c3d80fb6":"markdown","c7c2a0bc":"markdown","d3932838":"markdown","27e54802":"markdown","6566a65b":"markdown","40e39003":"markdown","3762a8db":"markdown","4dcb0d78":"markdown","ab5dc69c":"markdown","e920256a":"markdown","87be0ca7":"markdown","4d3abb0e":"markdown","fd1fddfd":"markdown","adcbdc78":"markdown","1a2d19f1":"markdown","740866b6":"markdown","1d95fee7":"markdown","e41f4938":"markdown","6c4af5af":"markdown"},"source":{"2164dddc":"import pandas as pd\nimport numpy as np\n","e075f7b9":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndf = pd.read_csv('\/kaggle\/input\/breast-cancer-prediction-dataset\/Breast_cancer_data.csv')\ndf","24b8fe52":"X = df.drop('diagnosis', axis=1)\nY = df['diagnosis']","21618dc4":"\nfrom sklearn.model_selection import train_test_split","4f833d0b":"x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)","078869c6":"\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score","eef274d3":"def summarize_classification(y_test, y_pred):\n    \n    acc = accuracy_score(y_test, y_pred, normalize=True)\n    num_acc = accuracy_score(y_test, y_pred, normalize=False)\n\n    prec = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    \n    print(\"Test data count: \",len(y_test))\n    print(\"accuracy_count : \" , num_acc)\n    print(\"accuracy_score : \" , acc)\n    print(\"precision_score : \" , prec)\n    print(\"recall_score : \", recall)","f92db530":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier","94370f1a":"parameters = {'max_depth': [1,2,3,4,5,6,7,8,9,10,11,12]}\n\ngrid_search = GridSearchCV(DecisionTreeClassifier(), parameters, cv=3, return_train_score=True)\ngrid_search.fit(x_train, y_train)\n\ngrid_search.best_params_","99bfc257":"decision_tree_model = DecisionTreeClassifier(max_depth = grid_search.best_params_['max_depth']).fit(x_train, y_train)","fb968541":"y_pred = decision_tree_model.predict(x_test)\nsummarize_classification(y_test, y_pred)","7fadde69":"\n# example of bayesian optimization with scikit-optimize\nfrom numpy import mean\nfrom sklearn.model_selection import cross_val_score\nfrom skopt.space import Integer\nfrom skopt.utils import use_named_args\nfrom skopt import gp_minimize","76a6cdbf":"\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d9b01d99":"# define the model\nmodel_tree = DecisionTreeClassifier()","909747c5":"# define the search space of hyperparameters to search\nsearch_space = [Integer(1, 12, name='max_depth')]","591ff681":"# define the function used to evaluate a given configuration\n@use_named_args(search_space)\ndef evaluate_model(**params):\n    # something\n    model_tree.set_params(**params)\n    # calculate 10-fold cross validation\n    result = cross_val_score(model_tree, x_train, y_train, cv=10, n_jobs=-1, scoring='accuracy')\n    # calculate the mean of the scores\n    estimate = mean(result)\n    return 1.0 - estimate","499883ef":"# perform optimization\nresult = gp_minimize(evaluate_model, search_space)\n","a688ed7a":"print('Best Accuracy: %.f' % (1.0 - result.fun))\nprint('Best Parameters: max_depth=%d' % (result.x[0]))","7d0040e4":"model_tree= DecisionTreeClassifier( max_depth = result.x[0]).fit(x_train, y_train)","74d7b5ff":"y_pred_tree = model_tree.predict(x_test)\nsummarize_classification(y_test, y_pred_tree)","ffc1ad0b":"from skopt.plots import plot_convergence\nplot_convergence(result);","4e12ce54":"from sklearn.neighbors import KNeighborsClassifier","d0bd2f00":"# define the model\nmodel_kn =KNeighborsClassifier()\n\n\n# define the search space of hyperparameters to search\nsearch_space = [Integer(1, 12, name='n_neighbors'), Integer(1, 3, name='p')]\n\n# define the function used to evaluate a given configuration\n@use_named_args(search_space)\ndef evaluate_model(**params):\n    # something\n    model_kn.set_params(**params)\n    # calculate 10-fold cross validation\n    result = cross_val_score(model_kn, x_train, y_train, cv=10, n_jobs=-1, scoring='accuracy')\n    # calculate the mean of the scores\n    estimate = mean(result)\n    return 1.0 - estimate\n\n# perform optimization\nresult = gp_minimize(evaluate_model, search_space)\n# summarizing finding:\nprint('Best Accuracy: %.f' % (1.0 - result.fun))\nprint('Best Parameters: n_neighbors=%d, p=%d' % (result.x[0], result.x[1]))","43b3f85c":"model_kn = KNeighborsClassifier( n_neighbors = result.x[0],p=result.x[1]).fit(x_train, y_train)","c5bd975a":"y_pred = model_kn.predict(x_test)\nsummarize_classification(y_test, y_pred)","f8937fc1":"from skopt.plots import plot_convergence\nplot_convergence(result);","54b2a877":"#### pip install scikit-optimize is used to install it","c5e06af7":"#### Summary of the calculation metrices achieve after predicting the values for x_test and then checking the accuracy by comparing the y_test and y_pred","c3d80fb6":"<b>Bayesian Optimization<\/b> provides a technique based on Bayes Theorem to direct a search of a global optimization problem that is efficient and effective. It works by building a probabilistic model of the objective function, called the surrogate function, that is then searched efficiently with an acquisition function before candidate samples are chosen for evaluation on the real objective function.\nBayesian Optimization is often used in applied machine learning to tune the hyperparameters of a given well-performing model on a validation dataset.It is an approach that is most useful for objective functions that are complex, noisy, and\/or expensive to evaluate.\n\nBayes Theorem is an approach for calculating the conditional probability of an event:\n<li>P(A|B) = P(B|A) * P(A) \/ P(B)<\/li>\nWe can simplify this calculation by removing the normalizing value of P(B) and describe the conditional probability as a proportional quantity. This is useful as we are not interested in calculating a specific conditional probability, but instead in optimizing a quantity.\n<li>P(A|B) = P(B|A) * P(A)<\/li>\nThe conditional probability that we are calculating is referred to generally as the posterior probability, the reverse conditional probability is sometimes referred to as the likelihood, and the marginal probability is referred to as the prior probability, for example:\n<li>posterior = likelihood * prior<\/li>\n\nThis provides a framework that can be used to quantify the beliefs about an unknown objective function given samples from the domain and their evaluation via the objective function.\n\nWe can devise specific samples (x1, x2, \u2026, xn) and evaluate them using the objective function f(xi) that returns the cost or outcome for the sample xi. Samples and their outcome are collected sequentially and define our data D, e.g. D = {xi, f(xi), \u2026 xn, f(xn)} and is used to define the prior. The likelihood function is defined as the probability of observing the data given the function P(D | f). This likelihood function will change as more observations are collected.\n<li>P(f|D) = P(D|f) * P(f)<\/li>\nThe posterior represents everything we know about the objective function. It is an approximation of the objective function and can be used to estimate the cost of different candidate samples that we may want to evaluate.","c7c2a0bc":"## Hyparameter Tuning- Grid search vs Bayesian optimization On Breast Cancer Prediction Dataset","d3932838":"#### Search Space \nIt is used to set the parameter is going to tuned or the dimensionality on which wwe are apllying the hyperarameter tuning.<br>\nEach search dimension can be defined either as\n\n<li>a (lower_bound, upper_bound) tuple (for Real or Integer dimensions),<\/li>\n\n<li>a (lower_bound, upper_bound, \"prior\") tuple (for Real dimensions),<\/li>\n\n<li>as a list of categories (for Categorical dimensions), or<\/li>\n\n<li>an instance of a Dimension object (Real, Integer or Categorical).<\/li>\n\nAlso you can refer to : https:\/\/scikit-optimize.github.io\/stable\/modules\/generated\/skopt.space.space.check_dimension.html","27e54802":"### use_named_args & set_params\nWe can use the use_named_args() decorator from the scikit-optimize project on the function definition that allows the function to be called directly with a specific set of parameters from the search space.\n\nAs such, our custom function will take the hyperparameter values as arguments, which can be provided to the model directly in order to configure it. We can define these arguments generically in python using the  **params argument to the function, then pass them to the model via the set_params function.","6566a65b":"#### Two popular libraries for Bayesian Optimization include\n\n<li>Scikit-Optimize<\/li> \n<li>HyperOpt<\/li>\nIn machine learning, these libraries are often used to tune the hyperparameters of algorithms.\nHyperparameter tuning is a good fit for Bayesian Optimization because the evaluation function is computationally expensive (e.g. training models for each set of hyperparameters) and noisy (e.g. noise in training data and stochastic learning algorithms).","40e39003":" There are many warning messages while using the gp_minimize,\n such as: UserWarning: The objective has been evaluated at this point before.\n\nThis is to be expected and is caused by the same hyperparameter configuration being evaluated more than once.","3762a8db":"#### So, lets implement both hyperparameter tuning method for the dataset that is available on the kaggle, the Breast Canceer Prediction\nLink to the kaggle dataset https:\/\/www.kaggle.com\/merishnasuwal\/breast-cancer-prediction-datas","4dcb0d78":"### Loading the dataset","ab5dc69c":"In machine learning, <b>hyperparameter optimization or tuning<\/b> is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process.","e920256a":"<b>Surrogate Function:<\/b> Bayesian approximation of the objective function that can be sampled efficiently.\nThe surrogate function gives us an estimate of the objective function, which can be used to direct future sampling. Sampling involves careful use of the posterior in a function known as the \u201cacquisition\u201d function, e.g. for acquiring more samples. We want to use our belief about the objective function to sample the area of the search space that is most likely to pay off, therefore the acquisition will optimize the conditional probability of locations in the search to generate the next sample.","87be0ca7":"### Importing the Grid search from the sklearn model selection and forming a variable parameter contining the max depth for fiting the decision tree with the best parameter suggested by the grid search","4d3abb0e":"#### Now using the KNeighborsClassifier","fd1fddfd":"I have used Scikit-Optimize library to optimize the hyperparameters for this classification problem. The Scikit-Optimize project is designed to provide access to Bayesian Optimization for applications that use SciPy and NumPy, or applications that use scikit-learn machine learning algorithms.","adcbdc78":"### Importing some sklearn metrices for calculating the accuracy, precision and recall score and form a function for to calculate all the metrices","1a2d19f1":"### Dividing the value in X & Y to make prediction and spliting the dataset for training and testing","740866b6":"###  importing the important libraries","1d95fee7":"### gp_ minimize\nBayesian optimization using Gaussian Processes.<br>\nIf every function evaluation is expensive, for instance when the parameters are the hyperparameters of a neural network and the function evaluation is the mean cross-validation score across ten folds, optimizing the hyperparameters by standard optimization routines would take for ever!<br>\nThe idea is to approximate the function using a Gaussian process. In other words the function values are assumed to follow a multivariate gaussian. The covariance of the function values are given by a GP kernel between the parameters. Then a smart choice to choose the next parameter to evaluate can be made by the acquisition function over the Gaussian prior which is much quicker to evaluate.\nhttps:\/\/scikit-optimize.github.io\/stable\/modules\/generated\/skopt.gp_minimize.html#skopt.gp_minimize","e41f4938":"<b>Acquisition Function:<\/b> Technique by which the posterior is used to select the next sample from the search space.","6c4af5af":"\n<b>Grid Search<\/b> is the process of scanning the data to configure optimal parameters for a given model. Depending on the type of model utilized, certain parameters are necessary. Grid-searching does NOT only apply to one model type. Grid-searching can be applied across machine learning to calculate the best parameters to use for any given model. It is important to note that Grid-searching can be extremely computationally expensive and may take your machine quite a long time to run. Grid-Search will build a model on each parameter combination possible. It iterates through every parameter combination and stores a model for each combination."}}