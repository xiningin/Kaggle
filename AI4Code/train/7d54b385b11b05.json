{"cell_type":{"d343174f":"code","67f5c890":"code","f0ab0eee":"code","88d89a74":"code","4bd2451f":"code","a09d6331":"code","c60bc568":"code","8f90286f":"code","6694f2b4":"code","ddaf7643":"code","af7f877f":"markdown"},"source":{"d343174f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","67f5c890":"train = pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv')\ntrain = train.query('weight>0').reset_index(drop=True)\n\ntrain.fillna(train.median(),inplace=True)\n\ntrain['feature_stock_id_sum'] = train['feature_41'] + train['feature_42'] + train['feature_43']\ntrain['feature_1_2_cross'] = train['feature_1']\/(train['feature_2']+1e-5)\n\nNUM_TRAIN_EXAMPLES = len(train)","f0ab0eee":"features = [c for c in train.columns if 'feature' in c]\nf_mean = np.nanmedian(train[features[1:]].values,axis=0)","88d89a74":"TRAINING = False\nPATH = '..\/input\/highwayjs'","4bd2451f":"resp_cols = [c for c in train.columns if 'resp' in c]","a09d6331":"import tensorflow_addons as tfa\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\ndef mish(x):\n    return tf.keras.layers.Lambda(lambda x: x*K.tanh(K.softplus(x)))(x)\n\ntf.keras.utils.get_custom_objects().update({'mish': tf.keras.layers.Activation(mish)})\n\ndef create_model(input_shape):\n    \n    inp = tf.keras.layers.Input(input_shape)\n    tmp = tf.keras.layers.BatchNormalization()(inp)\n    xs = [tmp]\n    for _ in range(10):\n        if len(xs) > 1:\n            tmp = tf.keras.layers.Concatenate(axis=-1)(xs)\n        else:\n            tmp = xs[0]\n        tmp = tf.keras.layers.Dense(32,activation='mish')(tmp)\n        tmp = tf.keras.layers.BatchNormalization()(tmp)\n        tmp = tf.keras.layers.Dropout(0.2)(tmp)\n        xs.append(tmp)\n    \n    output = tf.keras.layers.Dense(len(resp_cols),activation='sigmoid')(tf.keras.layers.Concatenate()(xs))\n    model = tf.keras.models.Model(inp,output)\n    optimizer = tfa.optimizers.RectifiedAdam(1e-3)\n    model.compile(optimizer, loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001),\n                    metrics=[tf.keras.metrics.BinaryAccuracy(name='binary_accuracy')])\n    return model","c60bc568":"import random\nimport os\ndef set_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)","8f90286f":"X_tr = train.query('date<440')[features].values\ny_tr = (train.query('date<440')[resp_cols].values > 0).astype(int)\n    \nX_val = train.query('date>460')[features].values\ny_val = (train.query('date>460')[resp_cols].values > 0).astype(int)\n    \nif TRAINING:\n    metric = {}\n    \n    for seed in [6, 28, 496, 8128]:\n        set_all_seeds(seed)\n\n        model = create_model(X_tr.shape[-1])\n        hist = model.fit(X_tr,y_tr,\n                         validation_data=(X_val,y_val),\n                         epochs=200,\n                         batch_size=8192,\n                         callbacks=[tf.keras.callbacks.EarlyStopping('val_binary_accuracy',mode='max',patience=20)])\n        \n        model.save_weights(f'model_{seed}.tf')\n        metric[seed] = max(hist.history['binary_accuracy'])\n    print(metric)       \nelse:\n    models = []\n    for seed in [6, 28, 496, 8128]:\n        model = create_model(X_tr.shape[-1])\n        model.load_weights(f'{PATH}\/model_{seed}.tf')\n        model.call = tf.function(model.call, experimental_relax_shapes=True)\n        models.append(model)\n","6694f2b4":"blend = np.asarray([0.55722,0.55301,0.56278,0.55624])\nblend = blend\/sum(blend)","ddaf7643":"from tqdm import tqdm\nif not TRAINING:\n    f = np.median\n    import janestreet\n    janestreet.competition.make_env.__called__ = False\n    env = janestreet.make_env()\n    th = 0.503\n    for (test_df, pred_df) in tqdm(env.iter_test()):\n        if test_df['weight'].item() > 0:\n            test_df['feature_stock_id_sum'] = test_df['feature_41'] + test_df['feature_42'] + test_df['feature_43']\n            test_df['feature_1_2_cross'] = test_df['feature_1']\/(test_df['feature_2']+1e-5)\n            x_tt = test_df.loc[:, features].values\n            if np.isnan(x_tt[:, 1:].sum()):\n                x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n            \n            p = sum(blend*np.asarray([f(model(x_tt,training=False).numpy()) for model in models]))\n            pred_df.action = np.where(p > th, 1, 0).astype(int)\n        else:\n            pred_df.action = 0\n        env.predict(pred_df)","af7f877f":"One of the adavantages of DenseNet is the ability to have a deep narrow network without loss in performance."}}