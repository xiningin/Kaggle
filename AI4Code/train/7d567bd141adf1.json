{"cell_type":{"80ff0236":"code","cdc6c55d":"code","f1e86f34":"code","c9c49b1b":"code","cfae8195":"code","f5557dea":"code","d6a129c3":"code","6bebccb1":"code","93fed57d":"code","e66f04d3":"code","bb01e62b":"code","35daa63b":"code","6e935cdc":"code","ffd6cee4":"code","5aa08631":"code","11835282":"code","51988f08":"code","3e1ce031":"code","1aeaa022":"markdown","078416ff":"markdown","6b2878ef":"markdown","f5e19a3c":"markdown","124544f4":"markdown","3ba82bde":"markdown","3f2258a9":"markdown","76911f65":"markdown","4efdccc1":"markdown","6052dacb":"markdown","b883247f":"markdown","a2e260b4":"markdown","965a0c8f":"markdown","97e88dfa":"markdown","b5b839f5":"markdown","35f850e7":"markdown","f8fab5ef":"markdown","bdec849f":"markdown","a9887c1a":"markdown","7870902a":"markdown","cf0d6b1d":"markdown","ade3ad72":"markdown","b8a5e45d":"markdown","3cc5f7ab":"markdown","4a2a1165":"markdown","72f5b4c6":"markdown","1ab3d088":"markdown"},"source":{"80ff0236":"# \u0423\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043c \u0440\u0430\u0437\u043c\u0435\u0440 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u0446\u0438\u0440\u0443\u0435\u043c\u044b\u0445 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439\nPIC_SIZE = 50\n# \u041f\u0443\u0442\u044c \u043a \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u044b\u043c \u0434\u0430\u043d\u043d\u044b\u043c\ndata_path = '..\/\/input\/\/'\n# \u041f\u0443\u0442\u044c, \u043a\u0443\u0434\u0430 \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c\nmodel_save_path = 'signs_classifier.pth'","cdc6c55d":"import pandas as pd\nimport numpy as np\nimport torch\nimport os\nfrom PIL import Image\nimport torchvision\nimport matplotlib.pyplot as plt\n\nprint('\u0412\u0435\u0440\u0441\u0438\u044f torch', torch.__version__)\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nif torch.cuda.is_available():\n    print(torch.cuda.get_device_name(device))\nelse:\n    print('Using CPU')","f1e86f34":"import torchvision.transforms as transforms\nfrom torch.utils.data import Dataset\n\nclass SignsDataset(Dataset):\n    \"\"\"Road signs dataset.\"\"\"\n\n    def __init__(self, csv_file, root_dir, transform=None, no_labels=False):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.signs_frame = pd.read_csv(csv_file)\n        print(f'{len(self.signs_frame)} samples loaded')\n        \n        self.root_dir = root_dir\n        self.transform = transform\n        self.no_labels = no_labels\n        \n        # \u0412 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u043d\u0435\u0442 \u043e\u0442\u0432\u0435\u0442\u043e\u0432\n        if not self.no_labels:\n            # C\u043e\u0437\u0434\u0430\u0451\u043c \u043c\u0430\u0441\u0441\u0438\u0432 label->index \u0438 \u043c\u0430\u0441\u0441\u0438\u0432 index->label\n            self.labels = self.signs_frame['label'].unique()\n            self.label_indexes = {}\n            for i, label in enumerate(self.labels):\n                self.label_indexes[label] = i\n\n    def __len__(self):\n        return len(self.signs_frame)\n\n    def __getitem__(self, idx):\n        # \u0417\u0430\u0433\u0440\u0443\u0437\u0438\u043c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0438 \u043f\u0440\u0438\u0432\u0435\u0434\u0451\u043c \u043a \u0440\u0430\u0437\u043c\u0435\u0440\u0443 50\u044550\n        img_name = self.root_dir + self.signs_frame.iloc[idx, 0]\n        image = Image.open(img_name)\n        image = image.resize((PIC_SIZE, PIC_SIZE), Image.ANTIALIAS)\n        \n        # \u041f\u0440\u0438\u043c\u0435\u043d\u0438\u043c \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044e)\n        if self.transform:\n            image = self.transform(image)\n            \n        # \u0414\u043b\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u043d\u0435 \u043d\u0443\u0436\u043d\u044b \u043e\u0442\u0432\u0435\u0442\u044b\n        if not self.no_labels:\n            # \u0412 \u0440\u043e\u043b\u0438 \u043e\u0442\u0432\u0435\u0442\u0430 \u0431\u0443\u0434\u0435\u043c \u0434\u0430\u0432\u0430\u0442\u044c \u043d\u043e\u043c\u0435\u0440 label\n            label_string = self.signs_frame.iloc[idx, 1]\n            label = self.label_indexes[label_string]\n        \n            sample = {'image': image, 'label': label}\n        else:\n            sample = {'image': image}\n            \n        return sample","c9c49b1b":"from torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.utils.data import DataLoader\n\n# \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0437\u0430\u0433\u0440\u0443\u0437\u0447\u0438\u043a \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 (\u043a\u043b\u0430\u0441\u0441 \u0432\u044b\u0448\u0435)\ndataset = SignsDataset(data_path + 'train.csv', \n                       data_path + 'data\/\/data\/\/', \n                       torchvision.transforms.ToTensor())\n\nindicies = np.arange(len(dataset))\n\nnp.random.seed(0)\nnp.random.shuffle(indicies)\n\n# \u0420\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u043d\u0430 train \u0438 validation\ntrain_sampler = SubsetRandomSampler(indicies[:int(len(dataset)*0.5)])\nvalidation_sampler = SubsetRandomSampler(indicies[int(len(dataset)*0.5):])\n\n# DataLoader \u0434\u043e\u0441\u0442\u0430\u0451\u0442 \u0434\u0430\u043d\u043d\u044b\u0435 \u0438\u0437 dataset \u0431\u0430\u0442\u0447\u0430\u043c\u0438\nsignsTrainLoader = DataLoader(dataset, batch_size=16, sampler=train_sampler)\nsignsValidationLoader = DataLoader(dataset, batch_size=32, sampler=validation_sampler)","cfae8195":"# \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c, \u0447\u0442\u043e \u0432\u044b\u0434\u0430\u0451\u0442 \u043e\u0434\u043d\u0430 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u044f DataLoader\nbatch = next(iter(signsTrainLoader))\nimg = batch['image'][0]\nimg = np.transpose(img, (1, 2, 0))\n\nplt.imshow(img)","f5557dea":"df = dataset.signs_frame\nclasses_number = df['label'].nunique()\nprint('Classes number:', classes_number)\ndf.groupby('label')['file_name'].nunique()","d6a129c3":"import torch.nn as nn\nimport torch.nn.functional as F  # Functional","6bebccb1":"# \u041a\u043b\u0430\u0441\u0441 \u0441\u0432\u0451\u0440\u0442\u043e\u0447\u043d\u043e\u0439 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438\nclass SimpleConvNet(nn.Module):\n    def __init__(self, class_number):\n        # \u0432\u044b\u0437\u043e\u0432 \u043a\u043e\u043d\u0441\u0442\u0440\u0443\u043a\u0442\u043e\u0440\u0430 \u043f\u0440\u0435\u0434\u043a\u0430\n        super(SimpleConvNet, self).__init__()\n        # \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u043c\u043e \u0437\u0430\u0440\u0430\u043d\u0435\u0435 \u0437\u043d\u0430\u0442\u044c, \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043a\u0430\u043d\u0430\u043b\u043e\u0432 \u0443 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 (\u0441\u0435\u0439\u0447\u0430\u0441 = 3),\n        # \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u0431\u0443\u0434\u0435\u043c \u043f\u043e\u0434\u0430\u0432\u0430\u0442\u044c \u0432 \u0441\u0435\u0442\u044c, \u0431\u043e\u043b\u044c\u0448\u0435 \u043d\u0438\u0447\u0435\u0433\u043e\n        # \u043f\u0440\u043e \u0432\u0445\u043e\u0434\u044f\u0449\u0438\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u0437\u043d\u0430\u0442\u044c \u043d\u0435 \u043d\u0443\u0436\u043d\u043e\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n        \n        self.fc1 = nn.Linear(16 * 9 * 9, 120)  # !!! \n        \n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, class_number)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        \n        # \u041f\u043e\u0434\u0430\u0451\u043c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443 \u043d\u0430 \u0432\u0445\u043e\u0434 \u0438 \u0443\u0432\u0438\u0434\u0438\u043c, \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u043d\u0430 \u0432\u0445\u043e\u0434\u0435 \u043f\u0435\u0440\u0432\u043e\u0433\u043e fully connected!\n        # \u041f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 \u0437\u0430\u043c\u0435\u0447\u0430\u043d\u0438\u0435 \u0441\u043d\u0438\u0437\u0443 (\u043e\u043d\u043e \u0438\u0437 \u043f\u0440\u043e\u0448\u043b\u043e\u0433\u043e \u0441\u0435\u043c\u0438\u043d\u0430\u0440\u0430)\n        # print(x.shape)\n        ## \u0418\u041b\u0418 \u0416\u0415 \u041c\u041e\u0416\u041d\u041e \u042d\u0422\u041e \u0420\u0410\u0421\u0421\u0427\u0418\u0422\u0410\u0422\u042c, \u041d\u041e \u042d\u0422\u041e \u0414\u041e\u041b\u042c\u0428\u0415\n        \n        x = x.view(-1, 16 * 9 * 9)  # !!! \u0410\u043d\u0430\u043b\u043e\u0433 Flatten \u0432 keras\n        \n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        # \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0430\u043a\u043a\u0442\u0438\u0432\u0430\u0446\u0438\u0438 \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442\n        # torch.nn.CrossEntropyLoss \u0440\u0430\u0437\u0431\u0438\u0440\u0430\u0435\u0442\u0441\u044f \u0441 \u044d\u0442\u0438\u043c \u0441\u0430\u043c\n        return x","93fed57d":"# \u0421\u043e\u0437\u0434\u0430\u0451\u043c \u0441\u0435\u0442\u044c\ncnn = SimpleConvNet(classes_number).to(device)","e66f04d3":"# \u0412\u0437\u0433\u043b\u044f\u043d\u0435\u043c \u043d\u0430 \u0432\u044b\u0432\u043e\u0434\nbatch = next(iter(signsTrainLoader))\ncnn(batch['image'].to(device))[0]","bb01e62b":"from tqdm import tqdm_notebook\n\n# \u0421 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u044d\u0442\u043e\u0433\u043e \u0443\u0432\u0438\u0434\u0438\u043c, \u043a\u0430\u043a \u0441\u0435\u0442\u044c \u043e\u0431\u0443\u0447\u0430\u043b\u0430\u0441\u044c\nhistory = {'loss':[], 'val_loss':[]}\n\n# \u0412\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u043f\u043e\u0442\u0435\u0440\u044c\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# \u0412\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u0438 learning_rate\nlearning_rate = 1e-4\noptimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n\n# \u0426\u0438\u043a\u043b \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\ni = 0\nfor epoch in tqdm_notebook(range(100)):\n\n    running_loss = 0.0\n    for batch in signsTrainLoader:\n        \n        # \u0422\u0430\u043a \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0442\u0435\u043a\u0443\u0449\u0438\u0439 \u0431\u0430\u0442\u0447\n        X_batch, y_batch = batch['image'].to(device), batch['label'].to(device)\n        \n        # \u041e\u0431\u043d\u0443\u043b\u044f\u0435\u043c \u0432\u0435\u0441\u0430\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        y_pred = cnn(X_batch)\n        loss = loss_fn(y_pred, y_batch)\n        loss.backward()\n        optimizer.step()\n\n        # \u0412\u044b\u0432\u0435\u0434\u0435\u043c \u0442\u0435\u043a\u0443\u0449\u0438\u0439 loss\n        running_loss += loss.item()\n        \n        # \u041f\u0438\u0448\u0435\u043c \u0432 \u043b\u043e\u0433 \u043a\u0430\u0436\u0434\u044b\u0435 50 \u0431\u0430\u0442\u0447\u0435\u0439\n        if i % 50 == 49:\n            batch = next(iter(signsValidationLoader))\n            X_batch, y_batch = batch['image'].to(device), batch['label'].to(device)\n            y_pred = cnn(X_batch)\n            \n            history['loss'].append(loss.item())\n            history['val_loss'].append(loss_fn(y_pred, y_batch).item())\n        \n        # \u0412\u044b\u0432\u0435\u0434\u0435\u043c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u0430\u0436\u0434\u044b\u0435 1000 \u0431\u0430\u0442\u0447\u0435\u0439\n        if i % 1000 == 999:\n            print('[%d, %5d] loss: %.4f' % (epoch + 1, i + 1, running_loss \/ 1000))\n            running_loss = 0\n        i += 1\n\n# \u0421\u043e\u0445\u0440\u0430\u043d\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c\ntorch.save(cnn.state_dict(), model_save_path)\nprint('\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0437\u0430\u043a\u043e\u043d\u0447\u0435\u043d\u043e')","35daa63b":"# \u0421\u043a\u043e\u043b\u044c\u0437\u044f\u0449\u0435\u0435 \u0441\u0440\u0435\u0434\u043d\u0435\u0435\ndef smooth_curve(points, factor=0.9):\n    smoothed_points = []\n    for point in points:\n        if smoothed_points:\n            previous = smoothed_points[-1]\n            smoothed_points.append(previous * factor + point * (1 - factor))\n        else:\n            smoothed_points.append(point)\n    return smoothed_points\n\nplt.clf()\nloss_values = smooth_curve(history['loss'])\nval_loss_values = smooth_curve(history['val_loss'])\nepochs = np.arange(len(loss_values))\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","6e935cdc":"import itertools\n    \n# \u0412\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u0441\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u0435\u0439 \u0438\u0437 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 matplotlib, \u0432\u044b\u0432\u043e\u0434\u044f\u0449\u0435\u0439 confusion matrix \n# Source https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html    \ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    cm = cm.T\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    #print(cm)\n    plt.figure(figsize=(16,11))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n    plt.tight_layout()","ffd6cee4":"from sklearn.metrics import confusion_matrix\n\ny_test_all = torch.Tensor().long()\npredictions_all = torch.Tensor().long()\n\n# \u041f\u0440\u043e\u0439\u0434\u0451\u043c \u043f\u043e \u0432\u0441\u0435\u043c\u0443 validation \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0443 \u0438 \u0437\u0430\u043f\u0438\u0448\u0435\u043c \u043e\u0442\u0432\u0435\u0442\u044b \u0441\u0435\u0442\u0438\nwith torch.no_grad():\n    for batch in signsValidationLoader:\n        predictions = cnn(batch['image'].to(device))\n        y_test = batch['label']\n        _, predictions = torch.max(predictions.cpu(), 1)\n        \n        # \u0410\u043d\u0430\u043b\u043e\u0433 append \u0434\u043b\u044f list\n        y_test_all = torch.cat((y_test_all, y_test), 0)\n        predictions_all = torch.cat((predictions_all, predictions), 0)\n\nfeature_names = signsTrainLoader.dataset.labels\n\ny_test_all = y_test_all.numpy()\npredictions_all = predictions_all.numpy()\n\n# \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0438\u0437 sklearn, \u0441\u043e\u0437\u0434\u0430\u0451\u0442 confusion \u043c\u0430\u0442\u0440\u0438\u0446\u0443\ncm = confusion_matrix(y_test_all, predictions_all, np.arange(classes_number))\n# \u0412\u044b\u0432\u0435\u0434\u0435\u043c \u0435\u0451\nplot_confusion_matrix(cm, dataset.labels, normalize=True)","5aa08631":"class_correct = [0 for i in range(classes_number)]\nclass_total = [0 for i in range(classes_number)]\n\nc = (predictions_all == y_test_all).squeeze()\nfor i in range(len(predictions_all)):\n    label = predictions_all[i]            \n    class_correct[label] += c[i].item()\n    class_total[label] += 1\n\nprint(class_total)\n\nfor i in range(classes_number):\n    print('Accuracy of %5s : %2d %%' % (\n        (dataset.labels[i], (100 * class_correct[i] \/ class_total[i]) if class_total[i] != 0 else -1)))","11835282":"batch = next(iter(signsValidationLoader))\npredictions = cnn(batch['image'].to(device))\ny_test = batch['label']\n\n\n#print(predictions, y_test)\n_, predictions = torch.max(predictions, 1)\n\nimg = batch['image'][0]\nimg = np.transpose(img, (1, 2, 0))\nplt.imshow(img)\n\nprint('Gound-true:', dataset.labels[batch['label'][0]])\nprint('Prediction:', dataset.labels[predictions[0]])","51988f08":"from torch.utils.data import DataLoader\n\n# \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0437\u0430\u0433\u0440\u0443\u0437\u0447\u0438\u043a \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 (\u043a\u043b\u0430\u0441\u0441 \u0432\u044b\u0448\u0435)\ntest_dataset = SignsDataset(data_path + 'test.csv', \n                       data_path + 'data\/\/data\/\/', \n                       torchvision.transforms.ToTensor(),\n                       no_labels=True)\n\n\n# DataLoader \u0434\u043e\u0441\u0442\u0430\u0451\u0442 \u0434\u0430\u043d\u043d\u044b\u0435 \u0438\u0437 dataset \u0431\u0430\u0442\u0447\u0430\u043c\u0438\nsignsTestLoader = DataLoader(test_dataset, batch_size=1)","3e1ce031":"answer = []\n\nfor batch in signsTestLoader:\n    predictions = cnn(batch['image'].to(device))\n    _, predictions = torch.max(predictions, 1)\n    answer.append(dataset.labels[predictions[0]])    \n\nprediction_df = test_dataset.signs_frame\n    \nprediction_df['label'] = pd.Series(answer)\nprediction_df.to_csv('my_submission.csv', index=False)","1aeaa022":"# \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0435\u0439 (transfer learning)","078416ff":"### \u041d\u0430\u0447\u0435\u0440\u0442\u0438\u043c \u043a\u0440\u0438\u0432\u044b\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f","6b2878ef":"# \u041f\u043e\u043b\u0435\u0437\u043d\u044b\u0435 \u0441\u0441\u044b\u043b\u043a\u0438","f5e19a3c":"### \u0412\u044b\u0432\u0435\u0434\u0435\u043c confusion matrix","124544f4":"### \u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c DataLoader'\u044b, \u043e\u0431\u043b\u0435\u0433\u0447\u0430\u044e\u0449\u0438\u0435 \u0437\u0430\u043a\u0440\u0443\u0437\u043a\u0443 \u0438 \u0441\u044d\u043c\u043f\u043b\u0438\u043d\u0433 \u0434\u0430\u043d\u043d\u044b\u0445","3ba82bde":"<p style=\"align: center;\"><img src=\"https:\/\/static.tildacdn.com\/tild6636-3531-4239-b465-376364646465\/Deep_Learning_School.png\", width=300, height=300><\/p>\n\n<h3 style=\"text-align: center;\"><b>\u0424\u0438\u0437\u0442\u0435\u0445-\u0428\u043a\u043e\u043b\u0430 \u041f\u0440\u0438\u043a\u043b\u0430\u0434\u043d\u043e\u0439 \u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u043a\u0438 \u0438 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0442\u0438\u043a\u0438 (\u0424\u041f\u041c\u0418) \u041c\u0424\u0422\u0418<\/b><\/h3>\n\n--- \n# [kaggle] \u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u0434\u043e\u0440\u043e\u0436\u043d\u044b\u0445 \u0437\u043d\u0430\u043a\u043e\u0432","3f2258a9":"\u0412\u0430\u043c \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u0435\u0442\u0441\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043a\u043e\u0434 \u0438 \u043c\u0430\u0442\u0435\u0440\u0438\u0430\u043b\u044b \u0441 \u0441\u0435\u043c\u0438\u043d\u0430\u0440\u0430 \u0434\u043e\u0441\u0442\u0438\u0447\u044c \u043b\u0443\u0447\u0448\u0438\u0445 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u0438 \u0432\u044b\u0440\u0432\u0430\u0442\u044c\u0441\u044f \u0432 \u043b\u0438\u0434\u0435\u0440\u044b \u043d\u0430\u0448\u0435\u0433\u043e kaggle!","76911f65":"\u041b\u0443\u0447\u0448\u0435\u0435 \u0440\u0443\u043a\u043e\u0432\u043e\u0434\u0441\u0442\u0432\u043e \u043f\u043e matplotlib: https:\/\/matplotlib.org\/faq\/usage_faq.html","4efdccc1":"## \u0421\u043e\u0437\u0434\u0430\u0451\u043c \u0438 \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u0441\u0435\u0442\u044c","6052dacb":"### \u0414\u0430\u043d\u043d\u044b\u0435 \u0441\u0438\u043b\u044c\u043d\u043e \u043d\u0435\u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u044b (unbalanced dataset)\n### \u0417\u0430\u0434\u0430\u0447\u0430\n    \u0412\u0437\u0433\u043b\u044f\u043d\u0438\u0442\u0435 \u043d\u0430 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u0435\u043b\u0435\u0439 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430 \n    \u041a \u0447\u0435\u043c\u0443 \u044d\u0442\u043e \u043c\u043e\u0436\u0435\u0442 \u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438?\n    \n    \u041f\u043e\u0434\u0443\u043c\u0430\u0439\u0442\u0435 \u043e \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430\u0445 \u0438\u0441\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u044b\n#### upsampling, \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445","b883247f":"\u0421\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442 \u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0451\u043d\u043d\u044b\u0439 \u0438 \u043e\u0447\u0435\u043d\u044c \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u043f\u043e\u0434\u0445\u043e\u0434 \u043a \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044e \u0433\u043b\u0443\u0431\u043e\u043a\u0438\u0445 \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0435\u0439 \u043d\u0430 \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439. \u041c\u043e\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0443\u044e \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u044c. \u041f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0430\u044f \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u044c - \u044d\u0442\u043e \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u044c \u0441 \u0441\u043e\u0445\u0440\u0430\u043d\u0451\u043d\u043d\u044b\u043c\u0438 \u0432\u0435\u0441\u0430\u043c\u0438, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0431\u044b\u043b\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0430 \u043d\u0430 \u0431\u043e\u043b\u044c\u0448\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435, \u043e\u0431\u044b\u0447\u043d\u043e \u043d\u0430 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0431\u043e\u043b\u044c\u0448\u043e\u043c \u043e\u0431\u044a\u0451\u043c\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432\u0440\u043e\u0434\u0435 ImageNet. \u0415\u0441\u043b\u0438 \u044d\u0442\u043e\u0442 \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u0438 \u0438\u0441\u0447\u0435\u0440\u043f\u044b\u0432\u0430\u044e\u0449\u0438\u0439, \u0442\u043e \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0435\u043d\u043d\u0430\u044f \u0438\u0435\u0440\u0430\u0440\u0445\u0438\u044f \u0444\u0438\u0447\u0435\u0439, \u0432\u044b\u0443\u0447\u0435\u043d\u043d\u0430\u044f \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u044c\u044e \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0430 \u043a\u0430\u043a \u043e\u0431\u043e\u0431\u0449\u0451\u043d\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c \u0432\u0438\u0437\u0443\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u043c\u0438\u0440\u0430, \u0438, \u0442\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u044d\u0442\u0438 \u0444\u0438\u0447\u0438 \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u043f\u043e\u043b\u0435\u0437\u043d\u044b \u0432 \u0440\u0430\u0437\u043d\u044b\u0445 \u0437\u0430\u0434\u0430\u0447\u0430\u0445 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0433\u043e \u0437\u0440\u0435\u043d\u0438\u044f, \u0445\u043e\u0442\u044f \u043d\u043e\u0432\u044b\u0435 \u0437\u0430\u0434\u0430\u0447\u0438 \u043c\u043e\u0433\u0443\u0442 \u0438\u043c\u0435\u0442\u044c \u0441\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u043d\u043e \u0434\u0440\u0443\u0433\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u044b, \u0447\u0435\u043c \u0432 \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435.","a2e260b4":"### \u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u043a\u043b\u0430\u0441\u0441-\u043e\u0431\u0451\u0440\u0442\u043a\u0443 \u0434\u043b\u044f \u043d\u0430\u0448\u0435\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430","965a0c8f":"\u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u043e\u0431\u0443\u0447\u0438\u0432 \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u044c \u043d\u0430 ImageNet (\u0433\u0434\u0435 \u043a\u043b\u0430\u0441\u0441\u044b \u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c \u0436\u0438\u0432\u043e\u0442\u043d\u044b\u0435 \u0438 \u0431\u044b\u0442\u043e\u0432\u044b\u0435 \u043e\u0431\u044a\u0435\u043a\u0442\u044b), \u0435\u0451 \u043c\u043e\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0432 \u0437\u0430\u0434\u0430\u0447\u0435 \u0441 \u0441\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u043d\u043e \u0434\u0440\u0443\u0433\u0438\u043c\u0438 \u0434\u043e\u043c\u0435\u043d\u0430\u043c\u0438, \u043a \u043f\u0440\u0438\u043c\u0435\u0440\u0443, \u043d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u043c\u0435\u0431\u0435\u043b\u0438 \u043d\u0430 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u0445. \u0422\u0430\u043a\u0430\u044f \u043f\u0435\u0440\u0435\u043d\u043e\u0441\u0438\u043c\u043e\u0441\u0442\u044c \u0432\u044b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u0444\u0438\u0447\u0435\u0439 \u043d\u0430 \u0440\u0430\u0437\u043d\u044b\u0435 \u0437\u0430\u0434\u0430\u0447\u0438 \u0435\u0441\u0442\u044c \u043a\u043b\u044e\u0447\u0435\u0432\u043e\u0435 \u043f\u0440\u0435\u0438\u043c\u0443\u0449\u0435\u0441\u0442\u0432\u043e \u0433\u043b\u0443\u0431\u0438\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043d\u0430\u0434 \u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u0438\u043c\u0438 \u043f\u043e\u0434\u0445\u043e\u0434\u0430\u043c\u0438. \u0418 \u044d\u0442\u043e \u0434\u0435\u043b\u0430\u0435\u0442 \u0433\u043b\u0443\u0431\u0438\u043d\u043d\u043e\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043a\u0440\u0430\u0439\u043d\u0435 \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u044b\u043c \u043d\u0430 \u0437\u0430\u0434\u0430\u0447\u0430\u0445 \u0441 \"\u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0438\u043c\u0438\" \u0434\u0430\u043d\u043d\u044b\u043c\u0438.","97e88dfa":"### \u0417\u0430\u0434\u0430\u0447\u0430\n    - \u043a\u0430\u043a\u0438\u0435 \u0432\u044b\u0432\u043e\u0434\u044b \u043c\u043e\u0436\u043d\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0438\u0437 confusion matrix?","b5b839f5":"# \u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u0444\u0430\u0439\u043b\u0430 \u043e\u0442\u0432\u0435\u0442\u043e\u0432 \u043d\u0430 test","35f850e7":"\u0412\u0430\u0436\u043d\u043e\u0435 \u043f\u0440\u0438\u043c\u0435\u0447\u0430\u043d\u0438\u0435: \u0412\u044b \u043c\u043e\u0436\u0435\u0442\u0435 \u0437\u0430\u043c\u0435\u0442\u0438\u0442\u044c, \u0447\u0442\u043e \u0432 \u0441\u0442\u0440\u043e\u0447\u043a\u0430\u0445 \u0441 #!!! \u0435\u0441\u0442\u044c \u043d\u0435 \u043e\u0447\u0435\u043d\u044c \u043f\u043e\u043d\u044f\u0442\u043d\u044b\u0439 \u0441\u0445\u043e\u0434\u0443 $9 * 9 * 16$. \u042d\u0442\u043e -- \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u043f\u0435\u0440\u0435\u0434 FC-\u0441\u043b\u043e\u044f\u043c\u0438 (H x W x C), \u0442\u0443\u0442 \u0435\u0451 \u043f\u0440\u0438\u0445\u043e\u0434\u0438\u0442\u044c\u0441\u044f \u0432\u044b\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0442\u044c \u0432\u0440\u0443\u0447\u043d\u0443\u044e (\u0432 Keras, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, .Flatten() \u0432\u0441\u0451 \u0434\u0435\u043b\u0430\u0435\u0442 \u0437\u0430 \u0412\u0430\u0441). \u041e\u0434\u043d\u0430\u043a\u043e \u0435\u0441\u0442\u044c \u043e\u0434\u0438\u043d \u043b\u0430\u0439\u0444\u0445\u0430\u043a -- \u043f\u0440\u043e\u0441\u0442\u043e \u0441\u0434\u0435\u043b\u0430\u0439\u0442\u0435 \u0432 forward() print(x.shape) (\u0437\u0430\u043a\u043e\u043c\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u0430\u044f \u0441\u0442\u0440\u043e\u043a\u0430). \u0412\u044b \u0443\u0432\u0438\u0434\u0438\u0442\u0435 \u0440\u0430\u0437\u043c\u0435\u0440 (batch_size, C, H, W) -- \u043d\u0443\u0436\u043d\u043e \u043f\u0435\u0440\u0435\u043c\u043d\u043e\u0436\u0438\u0442\u044c \u0432\u0441\u0435, \u043a\u0440\u043e\u043c\u0435 \u043f\u0435\u0440\u0432\u043e\u0433\u043e (batch_size), \u044d\u0442\u043e \u0438 \u0431\u0443\u0434\u0435\u0442 \u043f\u0435\u0440\u0432\u0430\u044f \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c Linear(), \u0438 \u0438\u043c\u0435\u043d\u043d\u043e \u0432 C H W \u043d\u0443\u0436\u043d\u043e \"\u0440\u0430\u0437\u0432\u0435\u0440\u043d\u0443\u0442\u044c\" x \u043f\u0435\u0440\u0435\u0434 \u043f\u043e\u0434\u0430\u0447\u0435\u0439 \u0432 Linear().\n\n\u0422\u043e \u0435\u0441\u0442\u044c \u043d\u0443\u0436\u043d\u043e \u0431\u0443\u0434\u0435\u0442 \u0437\u0430\u043f\u0443\u0441\u0442\u0438\u0442\u044c \u0446\u0438\u043a\u043b \u0441 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\u043c \u043f\u0435\u0440\u0432\u044b\u0439 \u0440\u0430\u0437 \u0441 print() \u0438 \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043f\u043e\u0441\u043b\u0435 \u043d\u0435\u0433\u043e break, \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u0440\u0430\u0437\u043c\u0435\u0440, \u0432\u043f\u0438\u0441\u0430\u0442\u044c \u0435\u0433\u043e \u0432 \u043d\u0443\u0436\u043d\u044b\u0435 \u043c\u0435\u0441\u0442\u0430 \u0438 \u0441\u0442\u0435\u0440\u0435\u0442\u044c print() \u0438 break.","f8fab5ef":"### \u0412 \u044d\u0442\u043e\u043c \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0435 \u0432\u0430\u043c \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u0435\u0442\u0441\u044f \u043f\u043e\u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0432 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0434\u043e\u0440\u043e\u0436\u043d\u044b\u0445 \u0437\u043d\u0430\u043a\u043e\u0432 \u0428\u0432\u0435\u0446\u0438\u0438. \n\u0412\u0438\u043a\u0438: https:\/\/commons.wikimedia.org\/wiki\/Road_signs_in_Sweden \n\nKaggle: https:\/\/www.kaggle.com\/c\/sweden-traffic-signs-classification","bdec849f":"## \u041d\u0438\u0436\u0435 \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d base-line \u0434\u043b\u044f \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u044f","a9887c1a":"\u0421\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442 \u0434\u0432\u0430 \u043f\u043e\u0434\u0445\u043e\u0434\u0430 \u043a \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044e \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u0441\u0435\u0442\u0435\u0439: **feature extraction (\u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0442\u0435\u043b\u044c \u0444\u0438\u0447\u0435\u0439)** and **fine-tuning (\u0442\u043e\u043d\u043a\u0430\u044f \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430)**.","7870902a":"#### \u041f\u0440\u0438\u043c\u0435\u0447\u0430\u043d\u0438\u0435 \u0438\u0437 \u043f\u0440\u043e\u0448\u043b\u043e\u0433\u043e \u0441\u0435\u043c\u0438\u043d\u0430\u0440\u0430","cf0d6b1d":"### \u041e\u0446\u0435\u043d\u0438\u043c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0430 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0445 \u043a\u0430\u0434\u0440\u0430\u0445 \u0438\u0437 validation'\u0430","ade3ad72":"### \u0417\u0430\u0434\u0430\u0447\u0430\n    - \u043a\u0430\u043a\u0430\u044f \u0441\u0432\u044f\u0437\u044c \u043c\u0435\u0436\u0434\u0443 confusion matrix \u0438 accuracy \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430?\n    \n#### \u0427\u0438\u0441\u043b\u0430 \u043d\u0430 \u0434\u0438\u0430\u0433\u043e\u043d\u0430\u043b\u0438 confusion matrix \u0438 \u0435\u0441\u0442\u044c \u044d\u0442\u0438 accuracy","b8a5e45d":"### \u0412\u044b\u0432\u0435\u0434\u0435\u043c \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430","3cc5f7ab":"\u0410\u0432\u0442\u043e\u0440: \u041c\u0443\u0440\u0430\u0448\u043e\u0432 \u041b\u0435\u043e\u043d\u0438\u0434","4a2a1165":"\u041d\u043e\u0443\u0442\u0431\u0443\u043a \u0441\u043e\u0437\u0434\u0430\u043d \u043f\u043e\u0434 \u0432\u0435\u0440\u0441\u0438\u0435\u0439 torch '1.0.1'","72f5b4c6":"*\u0414\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e:*  \n* \u0432 \u044d\u0442\u043e\u0439 \u0441\u0442\u0430\u0442\u044c\u0435 \u0440\u0430\u0441\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u043e \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0437\u043d\u0430\u043a\u043e\u0432 \u0438 \u043a\u0430\u043a \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u0441\u043a\u043e\u0440, \u043d\u043e \u043a\u043e\u0434 \u043f\u043e\u0434 TensorFlow :P  \n https:\/\/navoshta.com\/traffic-signs-classification\/\n* \u0430 \u0432\u043e\u0442 \u0437\u0434\u0435\u0441\u044c \u043c\u043e\u0436\u043d\u043e \u0434\u043e\u0441\u0442\u0430\u0442\u044c **\u043c\u043d\u043e\u0433\u043e** (\u0432 15 \u0440\u0430\u0437 \u0431\u043e\u043b\u044c\u0448\u0435, \u0447\u0435\u043c \u0443 \u043d\u0430\u0441) \u0434\u0430\u043d\u043d\u044b\u0445 \u043a\u0430\u043a \u0440\u0430\u0437 \u043e \u0434\u043e\u0440\u043e\u0436\u043d\u044b\u0445 \u0437\u043d\u0430\u043a\u0430\u0445, \u043d\u043e \u0438\u0437 \u0413\u0435\u0440\u043c\u0430\u043d\u0438\u0438.  \nhttps:\/\/www.kaggle.com\/meowmeowmeowmeowmeow\/gtsrb-german-traffic-sign\/  \n\n\u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0439\u0442\u0435 \u0440\u0430\u0437\u043d\u044b\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044b, \u0438\u043c\u0435\u044f \u0432 \u0440\u0430\u0441\u043f\u043e\u0440\u044f\u0436\u0435\u043d\u0438\u0438 [\u044d\u0442\u043e\u0442 \u0434\u0430\u0442\u0430\u0441\u0435\u0442](https:\/\/www.kaggle.com\/meowmeowmeowmeowmeow\/gtsrb-german-traffic-sign\/):\n 1. \u043e\u0431\u0443\u0447\u0438\u0442\u0435 \u043d\u0430 \u043d\u0451\u043c \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u044c \u0438\u0437 \u0441\u0435\u043c\u0438\u043d\u0430\u0440\u0430 \u21169 \u0438\u043b\u0438 \u043b\u044e\u0431\u0443\u044e \u0438\u0437 [\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0445 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440](https:\/\/pytorch.org\/docs\/stable\/torchvision\/models.html)\n 2. \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0439\u0442\u0435 \u043f\u043e\u0445\u043e\u0436\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u044b \u043d\u0430\u043f\u0440\u044f\u043c\u0443\u044e (\u0437\u043d\u0430\u043a\u0438 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u044f \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u0438 90)  \n 3. fine-tuning \u0441\u0435\u0442\u0438, \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u043d\u0430 \u0434\u043e\u0440\u043e\u0433\u0430\u0445 \u0413\u0435\u0440\u043c\u0430\u043d\u0438\u0438 \u043f\u043e\u0434 \u0434\u043e\u0440\u043e\u0433\u0438 \u0428\u0432\u0435\u0446\u0438\u0438","1ab3d088":"\u042d\u0442\u043e \u043f\u0440\u043e\u0441\u0442\u043e\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440(CNN) \u0438\u0437 \u0441\u0435\u043c\u0438\u043d\u0430\u0440\u0430 \u21169, \u0442.\u0435. **\u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u0441\u0435\u0442\u0438 \u043d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u044b**."}}