{"cell_type":{"7df5e2c3":"code","cf1c6a4c":"code","33708854":"code","c3eaedbf":"code","005361f7":"code","9af8c5f9":"code","1ba9814e":"code","ab8dafe5":"code","5e17fcec":"code","c858912e":"code","085e027a":"code","d2bf4753":"code","007feb95":"code","cc2f9e8e":"markdown"},"source":{"7df5e2c3":"#importing required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n#reading the data\ndata = pd.read_csv('\/content\/train.csv')\nprint(data.head(5))","cf1c6a4c":"#file contains respective labels of each data points\n#seperating label column from data df\nlabel = data['label']\nprint(label.head(5))\nprint(type(label))\n\n# Drop the label feature and store the pixel data in feature.\nfeature = data.drop(\"label\",axis=1)\nprint(feature.head(5))\nprint(type(feature)) ","33708854":"print(label.shape)\nprint(feature.shape)","c3eaedbf":"#data standardization - column-wise\nfrom sklearn.preprocessing import StandardScaler\nstandardized_data = StandardScaler().fit_transform(feature)\nprint(standardized_data.shape)","005361f7":"#find the co-variance matrix which is : A^T * A\nsample_data = standardized_data\n\n# matrix multiplication using numpy\ncovar_matrix = np.matmul(sample_data.T , sample_data)\n\nprint ( \"The shape of variance matrix = \", covar_matrix.shape)","9af8c5f9":"# finding the top two eigen-values and corresponding eigen-vectors \n# for projecting onto a 2-Dim space.\n\nfrom scipy.linalg import eigh \n\n# the parameter 'eigvals' is defined (low value to heigh value) \n# eigh function will return the eigen values in asending order\n# this code generates only the top 2 (782 and 783) eigenvalues.\nvalues, vectors = eigh(covar_matrix, eigvals=(782,783))\n\nprint(\"Shape of eigen vectors = \",vectors.shape)\n# converting the eigen vectors into (2,d) shape for easyness of further computations\nvectors = vectors.T\n\nprint(\"Updated shape of eigen vectors = \",vectors.shape)\n# here the vectors[1] represent the eigen vector corresponding 1st principal eigen vector\n# here the vectors[0] represent the eigen vector corresponding 2nd principal eigen vector","1ba9814e":"# projecting the original data sample on the plane \n#formed by two principal eigen vectors by vector-vector multiplication.\n\nimport matplotlib.pyplot as plt\nnew_coordinates = np.matmul(vectors, sample_data.T)\n\nprint (\" resultant new data points' shape \", vectors.shape, \"X\", sample_data.T.shape,\" = \", new_coordinates.shape)","ab8dafe5":"import pandas as pd\n\n# appending label to the 2d projected data\nnew_coordinates = np.vstack((new_coordinates, label)).T\n\n# creating a new data frame for ploting the labeled points.\ndataframe = pd.DataFrame(data=new_coordinates, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\nprint(dataframe.head())","5e17fcec":"# ploting the 2d data points with seaborn\nimport seaborn as sn\nsn.FacetGrid(dataframe, hue=\"label\", size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()","c858912e":"# initializing the pca\nfrom sklearn import decomposition\npca = decomposition.PCA()","085e027a":"# configuring the parameteres\n# the number of components = 2\npca.n_components = 2\npca_data = pca.fit_transform(sample_data)\n\n# pca_reduced will contain the 2-d projects of simple data\nprint(\"shape of pca_reduced.shape = \", pca_data.shape)","d2bf4753":"# attaching the label for each 2-d data point \npca_data = np.vstack((pca_data.T, label)).T\n\n# creating a new data fram which help us in ploting the result data\npca_df = pd.DataFrame(data=pca_data, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\nsn.FacetGrid(pca_df, hue=\"label\", size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()","007feb95":"# PCA for dimensionality redcution (non-visualization)\n\npca.n_components = 784\npca_data = pca.fit_transform(sample_data)\n\npercentage_var_explained = pca.explained_variance_ \/ np.sum(pca.explained_variance_);\n\ncum_var_explained = np.cumsum(percentage_var_explained)\n\n# Plot the PCA spectrum\nplt.figure(1, figsize=(6, 4))\n\nplt.clf()\nplt.plot(cum_var_explained, linewidth=2)\nplt.axis('tight')\nplt.grid()\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()\n\n\n# If we take 200-dimensions, approx. 90% of variance is explained.","cc2f9e8e":"**<h1>Implemenation of PCA to MNIST dataset<\/h1>**\nhttps:\/\/www.kaggle.com\/c\/digit-recognizer\/data"}}