{"cell_type":{"e97920c9":"code","b42fad6b":"code","04a4aff1":"code","eb755377":"code","c9125584":"code","b7ecea1a":"code","92f3f79b":"code","a6c1ea24":"code","6b14d002":"code","6c41eed5":"code","999a6ab4":"code","5a5aea0b":"code","fb6bd9f7":"code","74e7ca02":"code","64345ae6":"code","c4d47366":"code","769c9976":"code","9af3440c":"code","ec5cd0af":"code","395e2250":"code","daf209e8":"code","d6a257e5":"code","8c056fdd":"code","5e76382e":"code","d749ac7b":"code","025482a9":"code","2faf7cdd":"code","b535c094":"code","4742c726":"code","5ee4dde2":"code","aace3525":"code","d63becc3":"code","fe3efd6b":"code","99b2dd1f":"code","77d57bc0":"code","e61ce7cb":"code","04f86467":"code","4b61e1ba":"code","c952d86c":"code","65b806bd":"code","adac1e24":"code","ef4c917d":"code","e8745a41":"code","baa40477":"code","f32c37b1":"code","9db559a9":"code","af4ea4ba":"code","33686dc4":"code","a87dca89":"code","a65f3e7e":"markdown","d1e8857d":"markdown","262247fb":"markdown","0c807277":"markdown","a3526232":"markdown","d287604b":"markdown","dc67dcfd":"markdown","0d215c08":"markdown","142e907a":"markdown","e365ced5":"markdown","f74e3135":"markdown","17239213":"markdown"},"source":{"e97920c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b42fad6b":"# First, read the csv files.\ndata_2015 = pd.read_csv(\"\/kaggle\/input\/world-happiness\/2015.csv\")\ndata_2016 = pd.read_csv(\"\/kaggle\/input\/world-happiness\/2016.csv\")\ndata_2017 = pd.read_csv(\"\/kaggle\/input\/world-happiness\/2017.csv\")\ndata_2018 = pd.read_csv(\"\/kaggle\/input\/world-happiness\/2018.csv\")\ndata_2019 = pd.read_csv(\"\/kaggle\/input\/world-happiness\/2019.csv\")","04a4aff1":"#data_2015","eb755377":"#data_2015.columns","c9125584":"data1_2015 = data_2015.copy()\ndata1_2015.drop(['Happiness Rank','Region','Standard Error','Economy (GDP per Capita)','Family','Dystopia Residual'], axis=1, inplace=True)\ndata1_2015.columns = ['Country','Happiness','Health','Freedom','Corruption','Generosity']\ndata1_2015['Year'] = '2015'\ndata1_2015 = data1_2015[['Year','Country','Happiness','Health','Freedom','Corruption','Generosity']]\ndata1_2015","b7ecea1a":"#data_2016","92f3f79b":"#data_2016.columns","a6c1ea24":"data1_2016 = data_2016.copy()\ndata1_2016.drop(['Region','Happiness Rank','Lower Confidence Interval', 'Upper Confidence Interval',\n                 'Economy (GDP per Capita)', 'Family','Dystopia Residual'], axis=1, inplace=True)\ndata1_2016.columns = ['Country','Happiness','Health','Freedom','Corruption','Generosity']\ndata1_2016['Year'] = '2016'\ndata1_2016 = data1_2016[['Year','Country','Happiness','Health','Freedom','Corruption','Generosity']]\ndata1_2016","6b14d002":"#data_2017","6c41eed5":"#data_2017.columns","999a6ab4":"data1_2017 = data_2017.copy()\ndata1_2017.drop(['Happiness.Rank','Whisker.high','Whisker.low', 'Economy..GDP.per.Capita.', \n                 'Family','Dystopia.Residual'], axis=1, inplace=True)\ndata1_2017.columns = ['Country','Happiness','Health','Freedom','Generosity','Corruption']\ndata1_2017['Year'] = '2017'\ndata1_2017 = data1_2017[['Year','Country','Happiness','Health','Freedom','Corruption','Generosity']]\ndata1_2017","5a5aea0b":"# This code checks how many rows in country column match in two DFs. \n#data1_2017['Country'].isin(data_reg['Country']).value_counts()","fb6bd9f7":"#data_2018","74e7ca02":"#data_2018.columns","64345ae6":"data1_2018 = data_2018.copy()\ndata1_2018.drop(['Overall rank','GDP per capita','Social support'], axis=1, inplace=True)\ndata1_2018.columns = ['Country','Happiness','Health','Freedom','Generosity','Corruption']\ndata1_2018['Year'] = '2018'\ndata1_2018 = data1_2018[['Year','Country','Happiness','Health','Freedom','Corruption','Generosity']]\ndata1_2018","c4d47366":"#data_2019","769c9976":"#data_2019.columns","9af3440c":"data1_2019 = data_2019.copy()\ndata1_2019.drop(['Overall rank','GDP per capita','Social support'], axis=1, inplace=True)\ndata1_2019.columns = ['Country','Happiness','Health','Freedom','Generosity','Corruption']\ndata1_2019['Year'] = '2019'\ndata1_2019 = data1_2019[['Year','Country','Happiness','Health','Freedom','Corruption','Generosity']]\ndata1_2019","ec5cd0af":"# Merge 5 DFs vertically with concat method after making sure that all variable names and their order match. \ndata = pd.concat([data1_2015,data1_2016,data1_2017,data1_2018,data1_2019], ignore_index=True)","395e2250":"# Check the country list for duplicate country names with different expressions.\nsorted(list(data.Country.unique()))","daf209e8":"# Replacing different names for the same countries for consistency.\nprint(data[data.Country=='Hong Kong S.A.R., China'])\nprint(data[data.Country=='Northern Cyprus'])\nprint(data[data.Country=='North Macedonia'])\nprint(data[data.Country=='Trinidad & Tobago'])\nprint(data[data.Country=='Taiwan Province of China'])\ndata.loc[385,'Country'] = 'Hong Kong'\ndata.loc[527,'Country'] = 'North Cyprus'\ndata.loc[689,'Country'] = 'North Cyprus'\ndata.loc[709,'Country'] = 'Macedonia'\ndata.loc[507,'Country'] = 'Trinidad and Tobago'\ndata.loc[664,'Country'] = 'Trinidad and Tobago'\ndata.loc[347,'Country'] = 'Taiwan'","d6a257e5":"# We merge the region info and main data frame with how = 'left' command for not losing any row. \ndata_reg = data_2015.iloc[:,0:2] # Take the country and region info from 2015 to complete region info.\ndata = pd.merge(data, data_reg, on = 'Country', how = 'left')\ndata = data[['Year','Country','Region','Happiness','Health','Freedom','Corruption','Generosity']]\ndata","8c056fdd":"data.isna().sum()","5e76382e":"# List of countries with NaN values in the Region column.\ndata[data.Region.isna()].Country","d749ac7b":"nan_region = data[data.Region.isna()].Country.unique() # Total list of countries that do not have region info.\nlist1 = []\nfor i in list(nan_region):\n    if i in data1_2016.Country.values:\n        list1.append(i)\nprint(list1)  # The list of countries whose region info can be taken from 2016 DF. \nprint(data[data.Country=='Belize'])\nprint(data[data.Country=='Somalia'])\nprint(data[data.Country=='Namibia'])\nprint(data[data.Country=='South Sudan'])\nprint(data[data.Country=='Gambia'])\nprint(data[data.Country=='Puerto Rico'])\ndata.loc[209,'Region'] = 'Latin America and Caribbean' #Replace NaN values with region info for these countries. \ndata.loc[364,'Region'] = 'Latin America and Caribbean'  \ndata.loc[518,'Region'] = 'Latin America and Caribbean'\ndata.loc[233,'Region'] = 'Sub-Saharan Africa'\ndata.loc[407,'Region'] = 'Sub-Saharan Africa'\ndata.loc[567,'Region'] = 'Sub-Saharan Africa'\ndata.loc[737,'Region'] = 'Sub-Saharan Africa'\ndata.loc[270,'Region'] = 'Sub-Saharan Africa'\ndata.loc[425,'Region'] = 'Sub-Saharan Africa'\ndata.loc[588,'Region'] = 'Sub-Saharan Africa'\ndata.loc[738,'Region'] = 'Sub-Saharan Africa'\ndata.loc[300,'Region'] = 'Sub-Saharan Africa'\ndata.loc[461,'Region'] = 'Sub-Saharan Africa'\ndata.loc[623,'Region'] = 'Sub-Saharan Africa'\ndata.loc[781,'Region'] = 'Sub-Saharan Africa'\ndata.loc[254,'Region'] = 'Sub-Saharan Africa'\ndata.loc[745,'Region'] = 'Sub-Saharan Africa'\ndata.loc[172,'Region'] = 'Latin America and Caribbean' ","025482a9":"# NaN value in Corruption column.\ndata[data.Corruption.isna()]","2faf7cdd":"# Replace the NaN value in Corruption column with the mean.\ndata['Corruption'].fillna(data.Corruption.mean(), inplace=True)","b535c094":"# After dealing with NaN values we have no NaN values. \ndata.isna().sum()","4742c726":"data.info()","5ee4dde2":"data.describe()","aace3525":"data.describe().T","d63becc3":"# Regions in the data frame.\ndata['Region'].unique()","fe3efd6b":"# Number of regions in the data frame. \nlen(data['Region'].unique())","99b2dd1f":"# General correlation values between all the variables\ncorrelation = data.corr()\ncorrelation","77d57bc0":"# Correlation values bigger than 0.5\ncorrelation.abs()[correlation.abs()>0.5]\n# We can make an initial inference that there is a positive relation between happiness and health and freedom. \n# The correlation between happiness and healt is highest.","e61ce7cb":"# Correlation values by years\ncorrelation_yr = data.groupby('Year').corr()\ncorrelation_yr","04f86467":"# Correlation values bigger than 0.5\ncorrelation_yr.abs()[correlation_yr.abs()>0.5]\n# We can make an initial inference that there is not any significant change in the positive relation between happiness and health \n# and freedom over the years. ","4b61e1ba":"# Use nunique() to count unique countries since there are repeating countries for each year. \ndata.groupby('Region')['Country'].nunique()","c952d86c":"# Calculate the mean of happiness score grouped by countries\nscore_mean = data.groupby('Country')['Happiness'].mean()","65b806bd":"# The top 3 happiest countries\nscore_mean.sort_values(ascending=False).head(3)","adac1e24":"# The top 3 unhappiest countries\nscore_mean.sort_values(ascending=True).head(3)","ef4c917d":"# Calculate the mean of corruption grouped by countries\ncorruption_mean = data.groupby('Country')['Corruption'].mean()","e8745a41":"# Top 3 countries with the best scores for corruption\ncorruption_mean.sort_values(ascending=False).head(3)","baa40477":"# Top 3 countries with the worst scores for corruption\ncorruption_mean.sort_values(ascending=True).head(3)","f32c37b1":"freedom_mean = data.groupby('Region')['Freedom'].mean()","9db559a9":"# The region with highest freedom score\nfreedom_mean.sort_values(ascending=False).head(1)","af4ea4ba":"# The region with lowest freedom score\nfreedom_mean.sort_values(ascending=False).tail(1)","33686dc4":"# The region with lowest health score\nhealth_mean = data.groupby('Region')['Health'].mean()\nhealth_mean.sort_values(ascending=False).tail(1)","a87dca89":"data.groupby('Region').aggregate({'Happiness':'mean','Freedom':'mean', 'Corruption':'mean'}) ","a65f3e7e":"# 5. General info and descriptive stats of the data","d1e8857d":"# 7. Relation between Happiness and other variables","262247fb":"# 6. Regions in the data frame","0c807277":"# 8. Number of countries by region","a3526232":"# 9. Top 3 happiest and unhappiest countries","d287604b":"# 3. Displaying number of NaN values\n- We have 18 NaN values in region since the country\/region info of 2015 data frame that we used as reference for other tables do not completely match with the other data frames. \n- We have 1 NaN value in corruption variable. ","dc67dcfd":"# 11. The highest and lowest freedom scores by region","0d215c08":"# 13. The means of happiness, freedom and corruption by regions","142e907a":"# 12. The region with lowest healt score","e365ced5":"# 1-2. Merging the tables and completing region info\n- Take a copy of the file, drop the unused variables, add year variable, rearranging the variable names and their order. ","f74e3135":"# 10. Top 3 countries with the best and worst corruption scores","17239213":"# 4. Filling NaN values\n- We have only 1 NaN value in Corruption, we will replace this with the mean of this column.\n- We cannot do a similar thing in Region, because it is not numeric. We can use region info of 2016 and internet research."}}