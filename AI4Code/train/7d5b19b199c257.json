{"cell_type":{"8f77e35c":"code","a634b776":"code","f451543e":"code","cc06ecef":"code","5f987340":"code","c4e712d6":"code","e1a9ddd9":"code","2f6f564f":"code","bbe22082":"code","d35307f0":"code","dce9664a":"code","f7ab4277":"code","5daa21a0":"code","5ea9f2a1":"code","3f0f8a69":"code","dfd78d88":"code","439c7b66":"code","fb048672":"code","e53149b2":"code","7995e9d0":"code","40c427aa":"code","5ea04acf":"code","e6b7d837":"code","586c6d61":"code","dec4b30c":"code","78448f8f":"code","8ad6abb9":"code","64297e83":"code","6bd6a384":"code","d83d324f":"code","2976088f":"code","70532566":"code","ba82b300":"markdown","d5d58461":"markdown","9ee102d7":"markdown","a50012d2":"markdown","65924493":"markdown","c7e4802f":"markdown","ac99d22d":"markdown"},"source":{"8f77e35c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a634b776":"os.chdir('..\/input')\n#checking basic features of train data\ndf_train = pd.read_csv('fashion-mnist_train.csv')\ndf_train.head()","f451543e":"df_train.shape","cc06ecef":"#checking basic features of test data\n\ndf_test = pd.read_csv('fashion-mnist_test.csv')\ndf_test.head()","5f987340":"df_test.shape","c4e712d6":"#check for duplicated observation\n\ndf_train.duplicated().sum()","e1a9ddd9":"#Removing the duplicated values\n\ndf_train = df_train.drop_duplicates()","2f6f564f":"#Cross checking duplicated value\nprint(df_train.duplicated().sum())\nprint(df_train.shape)","bbe22082":"#checking the values of label column\n df_train.label.value_counts()","d35307f0":"#Creating X_train and y_train\n\ny_train = df_train['label']\nX_train = df_train.drop('label', axis =1)","dce9664a":"#checking the statistics of y_train\nprint(y_train.value_counts().sort_index())\nprint(y_train.describe())","f7ab4277":"#import the seaborn to check the distribution of target variable\n\nimport seaborn as sns\n\nsns.countplot(df_train.label)","5daa21a0":"#Checking for null values in target variable\n\ny_train.isna().sum()\n","5ea9f2a1":"X_train = X_train\/255\ndf_test = df_test\/255","3f0f8a69":"from keras.utils import to_categorical\ny_train = to_categorical(y_train)","dfd78d88":"#Checking for unique vectors formation of labels\n\nunique_rows = np.unique(y_train, axis = 0)\nunique_rows","439c7b66":"#Reshaping and checking the shape of the output\nX_train = X_train.values.reshape(-1,28,28,1)\nX_train.shape","fb048672":"#Checking the shape of  a single image\nX_train[0].shape","e53149b2":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size = 0.1, random_state = 42) ","7995e9d0":"df_test = df_test.drop('label', axis = 1)\ndf_test = df_test.values.reshape(-1,28,28,1)\ndf_test[0].shape","40c427aa":"import matplotlib.pyplot as plt\ng = plt.imshow(X_train[1][:,:,0])","5ea04acf":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPool2D, Dense, Dropout, Flatten\n","e6b7d837":"model = Sequential()\nmodel.add(Conv2D(filters = 32, padding = 'Same', kernel_size = (5,5), activation = 'relu', input_shape = (28,28,1)))\nmodel.add(MaxPool2D(pool_size = (2,2)))\nmodel.add(Conv2D(filters = 64, kernel_size = (5,5), activation = 'relu'))\nmodel.add(MaxPool2D(pool_size = (2,2)))\nmodel.add(Dropout (0.25))\nmodel.add(Conv2D(filters = 64, kernel_size = (2,2), activation ='relu'))\nmodel.add(MaxPool2D(pool_size = (2,2)))\nmodel.add(Dropout (0.33))\nmodel.add(Conv2D(filters = 64, kernel_size = (2,2), activation = \"relu\", padding = \"Same\" ))\nmodel.add(MaxPool2D(pool_size= (2,2)))\nmodel.add(Flatten())\nmodel.add(Dense(64, activation = 'relu'))\nmodel.add(Dropout (0.25))\nmodel.add(Dense(10, activation = 'softmax'))","586c6d61":"model.summary()","dec4b30c":"#Defining optimizer\n\nfrom keras.optimizers import RMSprop\n\noptimizer = RMSprop(lr = 0.001, rho = 0.9, epsilon = 1e-08, decay =0.0)","78448f8f":"#Compiling the model\nmodel.compile(optimizer = optimizer, loss  = 'categorical_crossentropy', metrics =['accuracy'])","8ad6abb9":"from keras.callbacks import ReduceLROnPlateau\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor = 'val_acc',\n                                           patience = 3,\n                                           verbose = 1,\n                                           factor = 0.5,\n                                           min_lr = 0.00001)","64297e83":"#Agumenting the data \n\nfrom keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(featurewise_center = False,\n                            samplewise_center = False,\n                            featurewise_std_normalization= False,\n                            samplewise_std_normalization= False,\n                            zca_whitening= False,\n                            rotation_range= 20,\n                            zoom_range= 0.05,\n                            width_shift_range= 0.1,\n                            height_shift_range= 0.1,\n                            horizontal_flip= False,\n                            vertical_flip= False)\n\ndatagen.fit(X_train)","6bd6a384":"batch_size = 50\nhistory = model.fit_generator(datagen.flow(X_train, y_train, batch_size = batch_size),\n                             epochs  = 45, validation_data= (X_val, y_val), verbose = 1,\n                             steps_per_epoch= X_train.shape[0]\/\/batch_size,\n                             callbacks = [learning_rate_reduction])\n","d83d324f":"\n\nfig, ax  = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color = 'b', label = 'Training loss')\nax[0].plot(history.history['val_loss'], color = 'r', label = 'Validation loss')\nlegend = ax[0].legend(loc ='best', shadow = True)\n\nax[1].plot(history.history['acc'], color = 'b', label = 'Training Accuracy')\nax[1].plot(history.history['val_acc'], color = 'r', label = 'Validation Accuracy')\nlegend = ax[1].legend(loc = 'best', shadow =True)","2976088f":"\nfrom sklearn.metrics import confusion_matrix\ny_pred = model.predict(X_val)\ny_pred = np.argmax(y_pred, axis = 1)\ny_true = np.argmax(y_val, axis = 1)\ncm = confusion_matrix(y_true, y_pred)\n# plot the confusion matrix\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(cm, cmap= \"YlGnBu\", annot=True, fmt='', ax=ax)\n","70532566":"results = model.predict(df_test)\nresults = np.argmax(results, axis = 1)\nresults = pd.Series(results, name = 'Label')","ba82b300":"### Feature Scaling\n**We will be dividing all the observation by 255 as the range of pixel value lies between (0,255)**","d5d58461":"**Converting the target varaible into categorical form**","9ee102d7":"## Data Preprocessing","a50012d2":"### Spitting the Train data into training and validation set","65924493":"### Reshaping the Data\n**As we would be using CNN for our model fitting, we have to reshape our data as the CNN takes data in matrix form**","c7e4802f":"**The above plot shows a uniform distribution of the target variable**","ac99d22d":"**Now as the data is in required format we would be building are Network layers so that we can fit are model later**"}}