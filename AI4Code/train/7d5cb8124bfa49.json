{"cell_type":{"a10af6b8":"code","2bcfdfff":"code","2143cb18":"code","cca341fd":"code","fb9f855d":"code","fd6b1cdb":"code","43df3066":"code","924da8e6":"code","1f9e6970":"code","cfaba9b1":"code","b41921b9":"code","9bef38a8":"code","763cba50":"code","349d1607":"code","26766493":"code","968d4c1a":"code","0de13614":"code","103d587c":"code","6c85a4a5":"code","1adff24f":"code","e48d6a5b":"code","a2f3031a":"code","cf785984":"code","874208f3":"code","5bc2de1e":"code","b14a588f":"code","bcdc5797":"code","e8bc6389":"code","35a62c1a":"code","ef2460ce":"code","161a6a6d":"code","242e84e9":"code","0956ea87":"code","538cf4ab":"code","b8c489e3":"code","1e0439ec":"code","4e0b2002":"code","be9d7f1f":"code","92b20a47":"code","398aa9b5":"code","68c3680a":"code","320b2f5e":"code","b0d8350d":"code","8d56ef4f":"code","b9d739cc":"code","9066b2af":"code","20d39a81":"code","1898ea88":"code","8516b9a3":"code","2d12acbe":"code","60cac506":"code","af88b034":"code","37690136":"code","0fcfa622":"code","6e6e1126":"code","c68c1bbd":"code","4619a90e":"code","70582e2f":"code","55d8b4dd":"code","d7c28cdc":"code","4e48a272":"code","2b40a4dd":"code","de26b65a":"code","b97e37ea":"code","0fd59d51":"code","0bb1a8c0":"code","5d076a64":"code","46d2a773":"code","752bc0ff":"code","429d2361":"code","7016e034":"code","82fe8576":"code","3a14ca96":"code","716d47a3":"code","a9e7d8b9":"code","89d9d52c":"code","56d7bb2f":"code","98d123ff":"code","b0aa22c3":"code","3e609e71":"code","cc055d0e":"code","d7c8c5c3":"code","6def00a5":"code","930383ec":"code","42b479ce":"code","8f7dc06c":"code","a3e4ab2c":"code","7e27c963":"code","b33a4a58":"code","44f6ec39":"code","72295b5d":"code","4954aeb1":"code","a2d8a7df":"code","833b7a87":"code","e397f48d":"code","1a326faf":"code","96c627b8":"code","27c1a08d":"markdown","466874c1":"markdown","dcc7c740":"markdown","7199e652":"markdown","800103c7":"markdown","9185dea1":"markdown","b8618a83":"markdown","cb196718":"markdown","527465d6":"markdown","36c394fa":"markdown","9621e79c":"markdown","bd6331a4":"markdown","6125bb5e":"markdown","8365759d":"markdown","c58fffce":"markdown","2ae69645":"markdown","566d6cbf":"markdown","377c1e74":"markdown","d80005bc":"markdown","e8e7d85e":"markdown","ffba61d7":"markdown","a1aacd59":"markdown","98d62177":"markdown","ac32232c":"markdown","fd9213c8":"markdown","1a1d5813":"markdown","5666ae19":"markdown","8bf79967":"markdown","3c2c58a1":"markdown","976dfcf4":"markdown","e475f946":"markdown","571ac754":"markdown","146b0387":"markdown","5642ee85":"markdown","eb4acf68":"markdown","1d3b15ae":"markdown","f8c97dbf":"markdown","30459bf0":"markdown","d67fd321":"markdown","5f6f861d":"markdown","9edea450":"markdown","8918422f":"markdown","bb511c8e":"markdown","19854a54":"markdown","300679a2":"markdown","12de83f7":"markdown","1c934d46":"markdown","df47bd58":"markdown","b1c954e9":"markdown","b9a4c23c":"markdown","909d1dbd":"markdown"},"source":{"a10af6b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n# importing required libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn import metrics\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","2bcfdfff":"# importing train, test and gender_submission.csv file\ntitanic_train = pd.read_csv('\/kaggle\/input\/train.csv')\ntitanic_test = pd.read_csv('\/kaggle\/input\/test.csv')\ngender_submission = pd.read_csv('\/kaggle\/input\/gender_submission.csv')","2143cb18":"# number of rows and columns in train set  \ntitanic_train.shape","cca341fd":"# looking the spread of numerical variables of train set\ntitanic_train.describe()","fb9f855d":"# column names\ntitanic_train.columns","fd6b1cdb":"# train data information\ntitanic_train.info()","43df3066":"# plotting the number of people who survived\nsns.countplot('Survived', data = titanic_train)\nplt.show()","924da8e6":"# percentage of passengers who survived\nround((sum(titanic_train['Survived'])\/len(titanic_train.index))*100, 2)","1f9e6970":"# Survived passengers based on Pclass and Sex\nplt.figure(figsize = (14,13))\nplt.subplot(2,2,1)\nsns.countplot('Pclass', data = titanic_train)\nplt.title('Count of Pclass')\nplt.subplot(2,2,2)\ntitanic_train.groupby('Pclass')['Survived'].value_counts().sort_values(ascending = False).plot('bar')\nplt.ylabel('Count')\nplt.title('Pclass v\/s Survived')\nplt.subplot(2,2,3)\nsns.countplot('Sex', data = titanic_train)\nplt.title('Count of Male and Female')\nplt.subplot(2,2,4)\ntitanic_train.groupby('Sex')['Survived'].value_counts().sort_values(ascending = False).plot('bar')\nplt.ylabel('Count')\nplt.title('Sex v\/s Survived')\nplt.show()","cfaba9b1":"# SipSb and Parch\nplt.figure(figsize = (14,13))\nplt.subplot(2,2,1)\nsns.countplot('SibSp', data = titanic_train)\nplt.title('Count of SibSp')\nplt.subplot(2,2,2)\ntitanic_train.groupby('SibSp')['Survived'].value_counts().sort_values(ascending = False).plot('bar')\nplt.ylabel('Count')\nplt.title('SibSp v\/s Survived')\nplt.subplot(2,2,3)\nsns.countplot('Parch', data = titanic_train)\nplt.title('Count of Parch')\nplt.subplot(2,2,4)\ntitanic_train.groupby('Parch')['Survived'].value_counts().sort_values(ascending = False).plot('bar')\nplt.ylabel('Count')\nplt.title('Parch v\/s Survived')\nplt.show()","b41921b9":"# spread of Fare and how Fare affected the possibility of survival\nplt.figure(figsize = (12,5))\nplt.subplot(1,2,1)\nsns.distplot(titanic_train.Fare)\nplt.title('Histogram of Fare')\nplt.subplot(1,2,2)\nsns.scatterplot(y = 'Fare', x = 'Survived', data = titanic_train)\nplt.title('Fare v\/s Survived')\nplt.show()","9bef38a8":"sns.lmplot(x = 'Age', y = 'Fare', hue='Pclass', col='Survived', data = titanic_train)\nplt.ylim(0,300)","763cba50":"# Finding the number of missing values in the data set\ntitanic_train.isnull().sum()","349d1607":"# Finding percentage of missing values\nround((titanic_train.isnull().sum()\/len(titanic_train.index))*100, 2)","26766493":"# Imputing the missing value of Age with it's median\ntitanic_train['Age'] = titanic_train['Age'].fillna(titanic_train['Age'].median())","968d4c1a":"# counting the different values in Embarked column\ntitanic_train['Embarked'].value_counts()","0de13614":"# Inputing the missing values of Embarked coloumn with it's mode \ntitanic_train['Embarked'] = titanic_train['Embarked'].fillna('S')","103d587c":"# since Cabin has 77.10% of missing values therefore dropping the entire column\ntitanic_train = titanic_train.drop('Cabin', 1)\ntitanic_train.head()","6c85a4a5":"# Again looking at the column for any missing values\nround((titanic_train.isnull().sum()\/len(titanic_train.index))*100, 2)","1adff24f":"# Correlation among the columns\ntitanic_train.corr()","e48d6a5b":"# visualizing the correlation of train data set\nsns.heatmap(titanic_train.corr(), annot = True)\nplt.show()","a2f3031a":"# pairplot \nsns.pairplot(titanic_train)\nplt.show()","cf785984":"# Looking for outliers if any\ntitanic_train.describe()","874208f3":"sns.boxplot(y = 'Fare', data = titanic_train)\nplt.show()","5bc2de1e":"# Looking the train data set\ntitanic_train.head()","b14a588f":"# dropping Name and Ticket columns as they doesn't seem much intuitive\ntitanic_train = titanic_train.drop('Name', 1)\ntitanic_train = titanic_train.drop('Ticket', 1)\ntitanic_train.head()","bcdc5797":"# creating dummy variables\ndef dummies(x,df):\n    temp = pd.get_dummies(df[x], prefix = x, drop_first = True)\n    df = pd.concat([df, temp], axis = 1)\n    df.drop([x], axis = 1, inplace = True)\n    return df\n\ntitanic_train = dummies('Pclass', titanic_train)\ntitanic_train = dummies('Sex', titanic_train)\ntitanic_train = dummies('Embarked', titanic_train)","e8bc6389":"# dummy variable created\ntitanic_train.head()","35a62c1a":"# feature scaling\nscaler = MinMaxScaler()\n\nnum_vars = ['Age', 'Fare']\n\ntitanic_train[num_vars] = scaler.fit_transform(titanic_train[num_vars])\ntitanic_train.head()","ef2460ce":"# splitting independent and dependent variables\ny_train = titanic_train.pop('Survived')\nX_train = titanic_train ","161a6a6d":"# Looking test set\ntitanic_test.head()","242e84e9":"# Looking for null values if any\ntitanic_test.isnull().sum()","0956ea87":"# Finding percentage of missing values\nround((titanic_test.isnull().sum()\/len(titanic_test.index))*100, 2)","538cf4ab":"# imputing missing value of Age with its median\ntitanic_test['Age'] = titanic_test['Age'].fillna(titanic_test['Age'].median())","b8c489e3":"# drooping Cabin column\ntitanic_test = titanic_test.drop('Cabin', 1)","1e0439ec":"# imputing missing value of Fare with its mean\ntitanic_test['Fare'] = titanic_test['Fare'].fillna(titanic_test.Fare.mean())","4e0b2002":"# again looking the test set for null values if any\ntitanic_test.isnull().sum()","be9d7f1f":"# dropping Name and Ticket columns as done for the train data set\ntitanic_test = titanic_test.drop('Name', 1)\ntitanic_test = titanic_test.drop('Ticket', 1)\ntitanic_test.head()","92b20a47":"# creating dummies\ndef dummies(x,df):\n    temp = pd.get_dummies(df[x], prefix = x, drop_first = True)\n    df = pd.concat([df, temp], axis = 1)\n    df.drop([x], axis = 1, inplace = True)\n    return df\n\ntitanic_test = dummies('Pclass', titanic_test)\ntitanic_test = dummies('Sex', titanic_test)\ntitanic_test = dummies('Embarked', titanic_test)","398aa9b5":"# Final test set\ntitanic_test.head()","68c3680a":"logm1 = sm.GLM(y_train, sm.add_constant(X_train), family = sm.families.Binomial())\nlogm1.fit().summary()","320b2f5e":"logreg = LogisticRegression()\nrfe = RFE(logreg, 6) # selecting 6 variables\nrfe = rfe.fit(X_train, y_train)","b0d8350d":"# features selected after RFE are:\nX_train.columns[~rfe.support_]","8d56ef4f":"# rank wise features:\nlist(zip(X_train.columns, rfe.support_, rfe.ranking_))","b9d739cc":"X_train.columns[~rfe.support_]","9066b2af":"cols = X_train.columns[rfe.support_]","20d39a81":"X_train.columns","1898ea88":"# dropping the variables not selected after RFE\nX_train = X_train[cols]\nX_train.columns","8516b9a3":"X_train_sm = sm.add_constant(X_train)\nlogm2 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","2d12acbe":"X_train = X_train.drop('Fare', 1)\nX_train.columns","60cac506":"X_train_sm = sm.add_constant(X_train)\nlogm3 = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","af88b034":"# Check for the VIF values of the feature variables. \n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = 'VIF', ascending = True)\nvif","37690136":"# getting predited values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:5]","0fcfa622":"y_train_pred_final = pd.DataFrame({'Survived': y_train.values, 'Survived_Prob': y_train_pred.values})\ny_train_pred_final['PassengerID'] = y_train.index\ny_train_pred_final.head()","6e6e1126":"y_train_pred_final['Predicted'] = y_train_pred_final.Survived_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","c68c1bbd":"# confusion matrix\nconfusion = metrics.confusion_matrix(y_train_pred_final.Survived, y_train_pred_final.Predicted)\nprint(confusion)","4619a90e":"#Actual\/Predicted     not_survived    survived\n#not_survived              463          86\n#survived                  101          241 ","70582e2f":"# overall accuracy\nprint(metrics.accuracy_score(y_train_pred_final.Survived, y_train_pred_final.Predicted))","55d8b4dd":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","d7c28cdc":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","4e48a272":"# Let us calculate specificity\nTN \/ float(TN+FP)","2b40a4dd":"# Calculate false postive rate - predicting survived when passenger does not survived\nprint(FP\/ float(TN+FP))","de26b65a":"# positive predictive value \nprint (TP \/ float(TP+FP))","b97e37ea":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","0fd59d51":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","0bb1a8c0":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Survived, y_train_pred_final.Survived_Prob, drop_intermediate = False )","5d076a64":"draw_roc(y_train_pred_final.Survived, y_train_pred_final.Survived_Prob)","46d2a773":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Survived_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","752bc0ff":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Survived, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","429d2361":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","7016e034":"y_train_pred_final['Final_Predicted'] = y_train_pred_final.Survived_Prob.map( lambda x: 1 if x > 0.4 else 0)\ny_train_pred_final.head()","82fe8576":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Survived, y_train_pred_final.Final_Predicted))","3a14ca96":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Survived, y_train_pred_final.Final_Predicted )\nconfusion2","716d47a3":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","a9e7d8b9":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","89d9d52c":"# Let us calculate specificity\nTN \/ float(TN+FP)","56d7bb2f":"# Calculate false postive rate\nprint(FP\/ float(TN+FP))","98d123ff":"# Positive predictive value \nprint (TP \/ float(TP+FP))","b0aa22c3":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","3e609e71":"from sklearn.metrics import precision_score, recall_score\nprint(precision_score(y_train_pred_final.Survived, y_train_pred_final.Final_Predicted))\nprint(recall_score(y_train_pred_final.Survived, y_train_pred_final.Final_Predicted))","cc055d0e":"from sklearn.metrics import precision_recall_curve\np, r, thresholds = precision_recall_curve(y_train_pred_final.Survived, y_train_pred_final.Survived_Prob)","d7c8c5c3":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","6def00a5":"titanic_test.head()","930383ec":"num_vars = ['Age', 'Fare']\n\ntitanic_test[num_vars] = scaler.transform(titanic_test[num_vars])\ntitanic_test.head()","42b479ce":"X_test = titanic_test[['Pclass_2', 'Pclass_3', 'Sex_male', 'Age', 'Embarked_S']]\nX_test.head()","8f7dc06c":"X_test_sm = sm.add_constant(X_test)","a3e4ab2c":"y_test_pred = res.predict(X_test_sm)\ny_test_pred.head()","7e27c963":"# creating dataframe \ny_test_pred_final = pd.DataFrame()\ny_test_pred_final['Survived_Prob'] = y_test_pred.values\ny_test_pred_final.head()","b33a4a58":"# Putting PassengerId to index\ny_test_pred_final['PassengerId'] = titanic_test.PassengerId.values\ny_test_pred_final.head()","44f6ec39":"# Let's see the head of y_pred_final\ny_test_pred_final['Final_Predicted'] = y_test_pred_final.Survived_Prob.map( lambda x: 1 if x > 0.4 else 0)\ny_test_pred_final.head()","72295b5d":"y_test_pred_final = pd.merge(y_test_pred_final, gender_submission, on='PassengerId', how='inner')\ny_test_pred_final.head()","4954aeb1":"metrics.accuracy_score(y_test_pred_final.Survived, y_test_pred_final.Final_Predicted)","a2d8a7df":"confusion2 = metrics.confusion_matrix(y_test_pred_final.Survived, y_test_pred_final.Final_Predicted)\nconfusion2","833b7a87":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","e397f48d":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","1a326faf":"# Let us calculate specificity\nTN \/ float(TN+FP)","96c627b8":"submission = y_test_pred_final[['PassengerId','Final_Predicted']]\nsubmission = submission.rename(columns={'Final_Predicted': 'Survived'})\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()","27c1a08d":"* Fare and Pclass have highest negative correlation between them\n* Then Survived and Pclass have large negative correlation\n* Pclass and Age have large negative correlation which means as the age is increaing it's Pclass is decreasing \n* As the age is increaing the suvival chance is increasing","466874c1":"### Checking missing values","dcc7c740":"making prediction on the test set","7199e652":"* SibSp = 0 had more survived passenger count\n* Parch = 0 had also more survived passenger count","800103c7":"As there are not much outliers so we can keep it","9185dea1":"p-value of all the features are within range","b8618a83":"# Step6: Model building","cb196718":"# Step12: Making prediction on test data","527465d6":"### creating dataframe with actually survived and predicted probabilities","36c394fa":"# Step7: Feature selection using RFE","9621e79c":"### Creating dummies for categorical columns of test data set\ncategorical variables are:\n* Pclass\n* Sex\n* Embarked","bd6331a4":"### Univariate and Bivariate Analysis","6125bb5e":"**out of 891 passengers only 38.38% of people survived**","8365759d":"The data is spread nicely and values are increasing gradually except for Fare, which has some outliers","c58fffce":"# Step11: Precision and Recall curve","2ae69645":"# Step9: Plotting ROC curve","566d6cbf":"### merging gender_submission to y_test_pred_final to see the accuracy","377c1e74":"### Model 2","d80005bc":"### Checking for outliers","e8e7d85e":"VIFs are within range and model seems fine","ffba61d7":"From the curve above, **0.4** is the optimum point to take it as a cutoff probability","a1aacd59":"# Step10: Finding optimal cut-off point","98d62177":"The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.","ac32232c":"* Age has 19.87% of missing values\n* Cabin has 77.10% of missing values\n* Embarked has 0.22% of missing values","fd9213c8":"Optimal cutoff probability is that prob where we get balanced sensitivity and specificity","1a1d5813":"### Accessing the model with StatsModels","5666ae19":"* mostly the fare was between 0 to 300\n* the person who paid the highest fare of 500 survived","8bf79967":"* The age was between 0 to 80 years\n* Pclass 1 has high Fare ","3c2c58a1":"# Step4: Data cleaning for test set","976dfcf4":"# **Logistic Regression with RFE using python**","e475f946":"Steps involved are:\n1. Data importing and understanding\n2. Data cleaning for train set\n3. Data preparation for train set\n4. Data cleaning for test set\n5. Data preparation for test data\n6. Model building\n7. Feature selection using RFE\n8. Metrics beyond accuracy\n9. Plotting ROC curve\n10. Finding optimal cut-off point\n11. Precision and Recall curve\n12. Making prediction on test data\n","571ac754":"### creating new column 'Predicted' with value 1 if Survived_Prob > 0.5 else 0","146b0387":"# Step1: Data importing and understanding","5642ee85":"### Creating dummies for categorical columns of train data set\ncategorical variables are:\n* Pclass\n* Sex\n* Embarked","eb4acf68":"**Univariate analysis** is the simplest form of analyzing data. \u201cUni\u201d means \u201cone\u201d, so in other words your data has only one variable. <br>It doesn\u2019t deal with causes or relationships (unlike regression) and it\u2019s major purpose is to describe; it takes data, summarizes that data and finds patterns in the data.\n<br>Analysing the particular column is called Univariate Analysis. Which either have\n* Categorical variables\n* Quntitative or numerical variables\n\n**Bivariate anaylsis** means the relationship between two variables. We need to perform Bivariate Analysis on\n* Continous variables\n* Categorical variables","1d3b15ae":"### Model 3","f8c97dbf":"### Feature scaling (MinMaxScaler)","30459bf0":"* There were more people from Pclass = 3 \n* But more passengers survived from Pclass = 1\n* Passengers boarded had more Male ratio than Female ratio\n* But more Female survived the tragic accident","d67fd321":"### checking vifs","5f6f861d":"### Model 1","9edea450":"p-value of Fare is greater than 0.05, therefore dropping it.","8918422f":"# Step2: Data cleaning for train set","bb511c8e":"Missing value percentage are:\n* Age = 20.57%\n* Cabin = 78.23%\n* Fare = 0.24%","19854a54":"# Step8: Metrics beyond accuracy","300679a2":"# Step5: Data preparation for test data","12de83f7":"An ROC curve demonstrates several things:\n* It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n* The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n* The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","1c934d46":"# Step3: Data preparation for train set","df47bd58":"**Now since we have dealt with the missing values, let's look at the correlation among the numerical columns of the train data set**","b1c954e9":"### Splitting dependent and independent variables","b9a4c23c":"### Checking missing values","909d1dbd":"* p value should be less than 0.05\n* but here, PassengerId, Parch, Fare, Embarked_Q and Embarked_S has very high p-value"}}