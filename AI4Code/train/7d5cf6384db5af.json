{"cell_type":{"538a8ca9":"code","aaed108a":"code","8c51b864":"code","60794a49":"code","7fbe3fec":"code","8d36590f":"code","c705c0af":"code","e6480e6f":"code","29473294":"code","4e385a5a":"code","ff194774":"code","1e1f9347":"code","23c1e842":"code","2e731f0d":"code","d1b8e1c7":"code","97b3cc7b":"code","52750665":"code","bd5e0a69":"code","a4b7d4e1":"code","87bf275a":"code","059fd8d8":"code","87ddee92":"code","3d0dd34d":"code","fe0be344":"code","730218ce":"code","191fcb2e":"code","cda2b9bb":"code","b82173ee":"code","2b710647":"code","61295476":"code","f64a24d7":"code","a3090954":"code","268ea99f":"code","9f5c7d2a":"code","1f42e176":"code","210e0417":"code","76f186c8":"code","c0459615":"code","075db9f4":"markdown","febd309f":"markdown","80cf161b":"markdown","f4e276bb":"markdown","c0f9f415":"markdown","49322c6d":"markdown","e1e06f5e":"markdown","01a75a80":"markdown","d0a73b92":"markdown","661e7550":"markdown","6a5e0f11":"markdown","258b330d":"markdown","1337c59d":"markdown","d2aa8139":"markdown","96c931d7":"markdown","f1d4a5b9":"markdown","660617c7":"markdown","bb707e33":"markdown","f43396a2":"markdown","0f9a203f":"markdown","e1aa2770":"markdown","cd59cccd":"markdown","ea124704":"markdown","970e7296":"markdown"},"source":{"538a8ca9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import preprocessing\nfrom sklearn.base import BaseEstimator\nimport seaborn as sns","aaed108a":"adult_train = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/train_data.csv\")\nadult_test = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/test_data.csv\")","8c51b864":"adult_train.tail(5)","60794a49":"# Code reference:\n# https:\/\/stackoverflow.com\/questions\/47002168\/correlation-between-two-non-numeric-columns-in-a-pandas-dataframe\neducations = adult_train[[\"education\", \"education.num\"]]\neducations.apply(lambda x: x.factorize()[0])","7fbe3fec":"sns.heatmap(pd.crosstab(adult_train[\"education\"], adult_train[\"education.num\"]))","8d36590f":"adult_train.describe()","c705c0af":"# Correlation matrix\nadult_train_corr = adult_train.corr(method=\"pearson\")\nsns.heatmap(adult_train_corr)","e6480e6f":"sns.histplot(data=adult_train, y=\"workclass\")","29473294":"sns.histplot(data=adult_train, y=\"marital.status\")","4e385a5a":"sns.histplot(data=adult_train, y=\"occupation\")","ff194774":"sns.histplot(data=adult_train, y=\"relationship\")","1e1f9347":"sns.histplot(data=adult_train, y=\"race\")","23c1e842":"sns.histplot(data=adult_train, y=\"native.country\")","2e731f0d":"adult_train = adult_train.dropna()\nadult_test = adult_test.dropna()","d1b8e1c7":"adult_train = adult_train.drop(columns=[\"education\", \"Id\", \"fnlwgt\"])\nadult_test = adult_test.drop(columns=[\"education\", \"Id\", \"fnlwgt\"])","97b3cc7b":"adult_train[\"net.capital\"] = adult_train[\"capital.gain\"] - adult_train[\"capital.loss\"]\nadult_train = adult_train.drop(columns=[\"capital.gain\", \"capital.loss\"])\n\nadult_test[\"net.capital\"] = adult_test[\"capital.gain\"] - adult_test[\"capital.loss\"]\nadult_test = adult_test.drop(columns=[\"capital.gain\", \"capital.loss\"])","52750665":"adult_train[\"male\"] = adult_train[\"sex\"].apply(lambda x: 1 if (x == \"Male\") else 0)\nadult_test[\"male\"] = adult_test[\"sex\"].apply(lambda x: 1 if (x == \"Male\") else 0)\n\nadult_train = adult_train.drop(columns=[\"sex\"])\nadult_test = adult_test.drop(columns=[\"sex\"])","bd5e0a69":"adult_train[\"private\"] = adult_train[\"workclass\"].apply(lambda x: 1 if (x == \"Private\") else 0)\nadult_test[\"private\"] = adult_test[\"workclass\"].apply(lambda x: 1 if (x == \"Private\") else 0)\nadult_train = adult_train.drop(columns=[\"workclass\"])\nadult_test = adult_test.drop(columns=[\"workclass\"])\n\nadult_train[\"white\"] = adult_train[\"race\"].apply(lambda x: 1 if (x == \"White\") else 0)\nadult_test[\"white\"] = adult_test[\"race\"].apply(lambda x: 1 if (x == \"White\") else 0)\nadult_train = adult_train.drop(columns=[\"race\"])\nadult_test = adult_test.drop(columns=[\"race\"])\n\nadult_train[\"united.states\"] = adult_train[\"native.country\"].apply(lambda x: 1 if (x == \"United-States\") else 0)\nadult_test[\"united.states\"] = adult_test[\"native.country\"].apply(lambda x: 1 if (x == \"United-States\") else 0)\nadult_train = adult_train.drop(columns=[\"native.country\"])\nadult_test = adult_test.drop(columns=[\"native.country\"])","a4b7d4e1":"label_encoder = preprocessing.LabelEncoder()\n\nadult_train[\"marital.status\"] = label_encoder.fit_transform(adult_train[\"marital.status\"])\nadult_test[\"marital.status\"] = label_encoder.fit_transform(adult_test[\"marital.status\"])\n\nadult_train[\"occupation\"] = label_encoder.fit_transform(adult_train[\"occupation\"])\nadult_test[\"occupation\"] = label_encoder.fit_transform(adult_test[\"occupation\"])\n\nadult_train[\"relationship\"] = label_encoder.fit_transform(adult_train[\"relationship\"])\nadult_test[\"relationship\"] = label_encoder.fit_transform(adult_test[\"relationship\"])","87bf275a":"adult_train.head()","059fd8d8":"# Code ref: https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html\n\n# We temporarily need to remove the \"income\" feature for this, as it doesn't accept non-numerical values.\ntemp_income = adult_train[\"income\"]\nadult_train = adult_train.drop(columns=[\"income\"])","87ddee92":"scaler_train = preprocessing.RobustScaler().fit(adult_train)\nscaler_test = preprocessing.RobustScaler().fit(adult_test)\n\n# Code Reference: https:\/\/stackoverflow.com\/questions\/24645153\/pandas-dataframe-columns-scaling-with-sklearn\nadult_train[adult_train.columns] = scaler_train.transform(adult_train)\nadult_test[adult_test.columns] = scaler_test.transform(adult_test)\nadult_train[\"income\"] = temp_income # Check later if this might cause trouble\n\nadult_train.head()","3d0dd34d":"adult_train[\"income\"].value_counts()","fe0be344":"# In practice, we make this by giving it no actual information about the dataset,\n# only making a KNN classifier and giving it a feature full of zeros.\nyes_x_adult_train = adult_train[[\"age\"]].assign(age = 0)\nyes_y_adult_train = adult_train[\"income\"]\n\nyes_knn = KNeighborsClassifier(n_neighbors = 2)\nyes_scores = cross_val_score(yes_knn, yes_x_adult_train, yes_y_adult_train, cv=10)\nyes_scores.mean()","730218ce":"# Let's temporarily make it so that the \"income\" label is encoded into [0, 1]\n\nadult_train_income_encoded = adult_train.copy()\nadult_train_income_encoded[\"income\"] = adult_train_income_encoded[\"income\"].apply(lambda x: 1 if (x == \">50K\") else 0)\n\nadult_train_income_encoded_corr = adult_train_income_encoded.corr(method=\"pearson\")\nsns.heatmap(adult_train_income_encoded_corr)","191fcb2e":"adult_train_income_encoded_corr[\"income\"]","cda2b9bb":"min_x_adult_train = adult_train[[\"education.num\"]]\nmin_y_adult_train = adult_train[\"income\"]","b82173ee":"min_15nn = KNeighborsClassifier(n_neighbors = 15)\nmin_15nn_scores = cross_val_score(min_15nn, min_x_adult_train, min_y_adult_train, cv=10)\nmin_15nn_scores.mean()","2b710647":"min_25nn = KNeighborsClassifier(n_neighbors = 25)\nmin_25nn_scores = cross_val_score(min_25nn, min_x_adult_train, min_y_adult_train, cv=10)\nmin_25nn_scores.mean()","61295476":"min_35nn = KNeighborsClassifier(n_neighbors = 35)\nmin_35nn_scores = cross_val_score(min_35nn, min_x_adult_train, min_y_adult_train, cv=10)\nmin_35nn_scores.mean()","f64a24d7":"max_x_adult_train = adult_train.drop(columns=[\"income\"])\nmax_y_adult_train = adult_train[\"income\"]","a3090954":"max_15nn = KNeighborsClassifier(n_neighbors = 15)\nmax_15nn_scores = cross_val_score(max_15nn, max_x_adult_train, max_y_adult_train, cv=10)\nmax_15nn_scores.mean()","268ea99f":"max_25nn = KNeighborsClassifier(n_neighbors = 25)\nmax_25nn_scores = cross_val_score(max_25nn, max_x_adult_train, max_y_adult_train, cv=10)\nmax_25nn_scores.mean()","9f5c7d2a":"max_35nn = KNeighborsClassifier(n_neighbors = 35)\nmax_35nn_scores = cross_val_score(max_35nn, max_x_adult_train, max_y_adult_train, cv=10)\nmax_35nn_scores.mean()","1f42e176":"max_35nn.fit(max_x_adult_train, max_y_adult_train)","210e0417":"y_adult_test_pred = max_35nn.predict(adult_test)","76f186c8":"output = pd.DataFrame({\"income\": y_adult_test_pred})\noutput.to_csv(\"submission.csv\", index=True, index_label='Id')","c0459615":"# Checking the submission formatting:\npd.read_csv(\"submission.csv\").head()","075db9f4":"Highest scoring solution: max_35nn","febd309f":"## Solution comparison","80cf161b":"## 2. Data prep","f4e276bb":"Now we need to do **data standardization**","c0f9f415":"The two columns \"capital.gain\" and \"capital.loss\" could be condensed into one \"net.capital\" column","49322c6d":"We know intuitively that this is a bad classifier, but this score of 0.759 serves as a good baseline.\n\nNaturally, all real classifiers should have a better score.","e1e06f5e":"The most significant correlation here is \"education.num\" with \"hours.per.week\", with a correlation of 0.148.\n\nThe second most significant correlation is \"hours.per.week\" and \"capital.gain\", with a correlation of 0.123.","01a75a80":"Now let's look at the rest of the class features:","d0a73b92":"Let's examine the \"education\" and \"education.num\" columns:","661e7550":"I've decided to do label encoding on the other class columns: \"marital.status\", \"occupation\" and \"relationship\", as they have many possible labels and there is no dominating label.","6a5e0f11":"### 3.3. Maximal features solution\n\nNow we throw everything into the mix: let's build a classifier that takes in all features. Will it be better than the minimal classifier?\n\nHere, we will also build three classifiers, each for 15, 25 and 35 neighbors.","258b330d":"### 3.2. Minimal features solution\n\nLet's now ask the question: what is the best score we can get with only one feature? In other words, what is the feature that best predicts our class feature?\n\nLet's find the highest correlation between the features and the \"income\" class:","1337c59d":"# Adult dataset\n### Predicting income via census data\n\n* Produce a classifier for the variable Target (that is, whether <=50K or >50K). The classifier must be a kNN where k is selected through cross validation; you must use the holdout test data to produce an empirical error rate. You must use the Jupyter system to produce a notebook with your solution. \n\n## Index\n1. Exploratory analysis\n2. Data prep\n3. Solutions\n4. Solution comparison\n\n","d2aa8139":"Let's remind ourselves of the accuracy of the \"Yes\" benchmark: 75.9%.\n\nThe following are the accuracies of the KNN models:\n\n|Neighbors    |15    |25    |35    |\n|-------------|-----:|-----:|-----:|\n|Min Features |0.7739|0.7782|0.7766|\n|Max Features |0.8577|0.8597|0.8599|","96c931d7":"I've elected to do one-hot encoding on some other features: \"workclass\", \"race\" and \"native.country\".\nIn practice, each of these features are dominated by one class, so we will treat them as binary classes.\n","f1d4a5b9":"We can see that the feature with the highest correlation with the label \"income\" is \"education.num\". So we will build a classifier that uses only that data point.\n\nWe will build three KNN classifiers: for 15, 25 and 35 neighbors.","660617c7":"## 1. Exploratory analysis","bb707e33":"The column \"sex\" has only two values, so we could use one-hot encoding here.\nBut that would mean that we would have one column that is the opposite of another which is redundant.\nSo, we can instead encode the column.","f43396a2":"The \"education.num\" is a factorization of the \"education\" column, which means they are redundant, so we drop the \"education\" column.\n\nThe \"Id\" and \"fnlwgt\" columns are not necessary for the label prediction, so we drop them:","0f9a203f":"## 3. Solutions","e1aa2770":"First we drop the lines with missing features","cd59cccd":"The data now looks like this:","ea124704":"We can see that 75.9% of all entries have income <=50K.\n\nSo this means we can make a classifier with 75.9% accuracy just by labeling everything as \"<=50K\".","970e7296":"### 3.1. \"Yes\" solution\n\nFirst, it's interesting to know what is the *least* effort we have to do to get a solution.\n\nCan we have a \"good\" solution without even considering the data?"}}