{"cell_type":{"13a78bd5":"code","3cf343af":"code","dbb6adcc":"code","1f871d46":"code","e4910b16":"code","41bb08d4":"code","42e865ef":"code","440b204c":"code","6f71864a":"code","93e9895c":"code","7c3a6640":"code","818f0b3e":"code","a87ca02a":"code","ed6e8c6b":"code","136cebfc":"code","6aee18ee":"code","d271eaaf":"markdown","75eb039b":"markdown","c04ac347":"markdown","25552d6e":"markdown","5a2df003":"markdown","e4959b71":"markdown","6abad016":"markdown","327f3f4e":"markdown","1314bcc1":"markdown","1a6ccc37":"markdown","08951648":"markdown"},"source":{"13a78bd5":"library(tidyverse)\nlibrary(keras)\n\ntrain <- read_csv(\"..\/input\/tabular-playground-series-jun-2021\/train.csv\")\ntest <- read_csv(\"..\/input\/tabular-playground-series-jun-2021\/test.csv\")\nsub <- read_csv(\"..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")","3cf343af":"preprocess_data <- function(data, wide_cols, deep_cols, y){\n  \n  return(\n    list(\n      train=list(wide=data$train[,wide_cols],\n                deep=data$train[,deep_cols],\n                y=y),\n      test=list(wide=data$test[,wide_cols],\n               deep=data$test[,deep_cols],\n               y=NA)))\n}","dbb6adcc":"x_train <- train %>% select(-id, -target)\nx_test <- test %>% select(-id)\n\ny <- str_extract(train$target, \"\\\\d\") %>% as.numeric()\ny <- to_categorical(y)[, 2:10]\n\ndata <- list(train = x_train, test = x_test)","1f871d46":"wide_cols <- colnames(x_train)[which(map_lgl(x_train, ~length(unique(.x))>=100))]\ndeep_cols <- colnames(x_train)[which(map_lgl(x_train, ~length(unique(.x))<100))]\n\ndata <- preprocess_data(data, wide_cols, deep_cols, y)","e4910b16":"size_deep <- NCOL(data$train$deep)\nsize_wide <- NCOL(data$train$wide)\nepoch <- 100\nbatchsize <- 128\nfolds <- 8","41bb08d4":"early_stopping <- callback_early_stopping(\n  monitor = \"val_loss\",\n  min_delta = 0.0000001,\n  patience = 15,\n  restore_best_weights = TRUE\n)\n\nplateau <- callback_reduce_lr_on_plateau(\n  monitor = \"val_loss\",\n  factor = 0.05,\n  patience = 2,\n  verbose = 1,\n  min_delta = 0.0000001,\n  cooldown = 0,\n  min_lr = 0\n)","42e865ef":"instance_model <- function(){\n\n  wide <- layer_input(size_wide)\n  \n  encoded_wide <- wide %>% \n    layer_dense(units = 1, activation = 'relu')\n  \n  deep <- layer_input(size_deep)\n  \n  encoded_deep <- deep %>% \n    layer_embedding(input_dim = 360, output_dim =  8, input_length = size_deep) %>%\n    \n    layer_conv_1d(filters = 16, kernel_size = 1, activation = 'relu') %>%\n    layer_flatten() %>%\n    layer_dropout(0.3) %>%\n    \n    layer_dense(units = 128, activation = 'relu') %>% \n    layer_batch_normalization() %>% \n    layer_dropout(0.3) %>%\n    \n    layer_dense(units = 64, activation = 'relu') %>% \n    layer_batch_normalization() %>% \n    layer_dropout(0.3) %>%\n    \n    layer_dense(units = 32, activation = 'relu') %>% \n    layer_batch_normalization() %>% \n    layer_dropout(0.2) %>%\n    \n    layer_dense(units = 9)\n  \n  merged <- list(encoded_wide, encoded_deep) %>%\n    layer_concatenate(axis=1) %>%\n    layer_dropout(rate=0.3)\n  \n  preds <- merged %>%\n    layer_dense(units=9, activation = \"softmax\")\n  \n  wide_n_deep <- keras_model(inputs = list(wide,deep), outputs = preds)\n  \n  return(wide_n_deep)\n}","440b204c":"keras::k_clear_session()","6f71864a":"wide_n_deep <- instance_model()\n\nwide_n_deep %>%\n  compile(\n    loss = 'categorical_crossentropy',\n    optimizer = optimizer_adam(\n      lr = 0.0002) )","93e9895c":"history <- wide_n_deep %>% fit(\n  x = list(as.matrix(data$train$wide), \n           as.matrix(data$train$deep)),\n  y = data$train$y,\n  batch_size = batchsize,\n  epochs = epoch,\n  validation_split=0.2,\n  callbacks = c(early_stopping, plateau)\n)","7c3a6640":"history","818f0b3e":"plot(history)","a87ca02a":"set.seed(314)\nkf <- caret::createFolds(train$target, folds)\n\nsub_nn <- matrix(0, nrow(x_test), ncol(y))\noof_nn <- matrix(0, nrow(x_train), ncol(y))\noof_logloss <- c()\n\nfor(i in 1:folds){\n    \n  keras::k_clear_session()  \n  print(glue::glue(\"\\n ===== FOLD {i} ===== \\n\"))\n    \n  train_idx <- unlist(kf[-i])\n  val_idx   <- kf[[i]]\n  \n  wide_n_deep <- instance_model()\n  \n  wide_n_deep %>%\n    compile(\n      loss = 'categorical_crossentropy',\n      optimizer = optimizer_adam(\n        lr = 0.0002) )\n  \n  tictoc::tic()\n  wide_n_deep %>% fit(\n    x = list(as.matrix(data$train$wide[train_idx, ]), \n             as.matrix(data$train$deep[train_idx, ])),\n    y = data$train$y[train_idx, ],\n    batch_size = batchsize,\n    epochs = epoch,\n    validation_data=list(list(as.matrix(data$train$wide[val_idx, ]), \n                              as.matrix(data$train$deep[val_idx, ])),\n                        list(data$train$y[val_idx, ])),\n    callbacks = c(early_stopping, plateau)\n  )\n  tictoc::toc()\n    \n  oof_nn[val_idx, ] <- predict(wide_n_deep, \n                               list(as.matrix(data$train$wide[val_idx, ]),\n                                    as.matrix(data$train$deep[val_idx, ])))\n  \n  sub_nn <- sub_nn + predict(wide_n_deep, \n                             list(as.matrix(data$test$wide),\n                                  as.matrix(data$test$deep))) \/ folds\n  \n  oof_loss <- yardstick::mn_log_loss_vec(\n    truth = factor(train$target),\n    estimate = oof_nn)\n    \n  print(glue::glue(\"\\n OOF logloss: {oof_loss} \\n\"))\n  \n  oof_logloss <- c(oof_logloss, oof_loss)\n  \n}","ed6e8c6b":"print(glue::glue(\"CV Logloss: {mean(oof_logloss)}\"))","136cebfc":"for(i in 1:9) sub[,i+1]=sub_nn[,i]\nreadr::write_csv(sub, \"sub_widedeep.csv\")","6aee18ee":"oof <- bind_cols(select(train, id), \n                rename_all(data.frame(oof_nn), \n                           ~str_replace(.x, \"X\", \"Class_\")))\nreadr::write_csv(oof, \"oof_widedeep.csv\")","d271eaaf":"[Bonus lesson from the Kaggle\u2019s course \u2018Intro to Deep Learning\u2019](https:\/\/www.kaggle.com\/ryanholbrook\/detecting-the-higgs-boson-with-tpus)\n\n[TPS06\/21 - Wide and Deep NN w\/Keras](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/tps06-21-wide-and-deep-nn-w-keras) by @jonaspalucibarbosa\n\nhttps:\/\/tensorflow.rstudio.com\/guide\/tfestimators\/examples\/wide_and_deep\/\n\nhttps:\/\/github.com\/prodipta\/R-examples\/blob\/master\/wide_and_deep.R\n\n\n[Wide & Deep Learning for Recommender Systems](https:\/\/arxiv.org\/abs\/1606.07792)","75eb039b":"# K-Fold Prediction","c04ac347":"# Model\n\nSet up the network and training parameters","25552d6e":"# Sub","5a2df003":"# Prepare data","e4959b71":"Split wide and deep variables","6abad016":"Design the network","327f3f4e":"Results:","1314bcc1":"Save predictions in test and oof (out-of-fold) data","1a6ccc37":"# Load Dependencies","08951648":"Train model"}}