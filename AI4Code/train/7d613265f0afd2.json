{"cell_type":{"378a85a5":"code","98f6c24b":"code","acace48c":"code","69fd28b2":"code","b39b54b3":"code","2236aba6":"code","a9a1b26f":"code","53437546":"code","1b8a95c7":"code","27053b43":"code","0b4d842e":"code","39fec9ff":"code","2620df79":"code","da92305b":"code","7b099215":"code","58197927":"code","96aca38a":"code","cf6d3716":"code","6bc57ac5":"code","ebaee0d7":"code","22bf4cf6":"code","1a83d46d":"code","aa323013":"code","3f430ed4":"markdown","3f2498cc":"markdown","f5c5d599":"markdown","af4318d1":"markdown","dd344ad7":"markdown","69729e86":"markdown","004e6932":"markdown","2ce5fa65":"markdown","33834d8a":"markdown"},"source":{"378a85a5":"import cv2\nimport glob\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","98f6c24b":"training_data_path = \"..\/input\/horses-or-humans-dataset\/horse-or-human\/train\/\"","acace48c":"horse_image_name = glob.glob(training_data_path + \"horses\/*png\")\nhuman_image_name = glob.glob(training_data_path + \"humans\/*png\")","69fd28b2":"print(\"Number of horse images: \", len(horse_image_name))\nprint(\"Number of human images: \", len(human_image_name))","b39b54b3":"def show_image_label(images, label, is_path=True):\n    \n    idx = 0\n    fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(20, 20))\n    \n    for i in range(5):\n        for j in range(5):\n            \n            if is_path:\n                img = cv2.imread(images[idx])\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            else:\n                img = images[idx]\n                \n            idx += 1\n            \n            axes[i, j].set_title(label=label, color=\"green\", fontsize=15)\n            axes[i, j].set_xticks([])\n            axes[i, j].set_yticks([])\n            axes[i, j].imshow(img)","2236aba6":"show_image_label(horse_image_name[0:25], \"horse\")","a9a1b26f":"show_image_label(human_image_name[0:25], \"human\")","53437546":"train_datagen = ImageDataGenerator(rotation_range=270,\n                                   width_shift_range=0.1,\n                                   # height_shift_range=0.5,\n                                   zoom_range=0.15,\n                                   # horizontal_flip=True,\n                                   # vertical_flip=True,\n                                   rescale=1\/255)","1b8a95c7":"training_generator = train_datagen.flow_from_directory(directory=training_data_path,\n                                  target_size=(300, 300),\n                                  batch_size=25,\n                                  class_mode=\"categorical\")","27053b43":"batch_data = training_generator.next()","0b4d842e":"training_generator.batch_index","39fec9ff":"show_image_label(batch_data[0], \"Not Sure\", is_path=False)","2620df79":"training_generator.reset()","da92305b":"training_generator.batch_index","7b099215":"inputs = keras.layers.Input(shape=(300, 300, 3))\nx = keras.layers.Conv2D(filters=4, kernel_size=(3, 3), strides=1, padding=\"same\", activation=tf.nn.relu)(inputs)\nx = keras.layers.MaxPooling2D(pool_size=2)(x)\nx = keras.layers.Conv2D(filters=16, kernel_size=(3, 3), strides=1, padding=\"same\", activation=tf.nn.relu)(x)\nx = keras.layers.MaxPooling2D(pool_size=2)(x)\nx = keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=1, padding=\"same\", activation=tf.nn.relu)(x)\nx = keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=1, padding=\"same\", activation=tf.nn.relu)(x)\nx = keras.layers.MaxPooling2D(pool_size=2)(x)\nx = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding=\"same\", activation=tf.nn.relu)(x)\nx = keras.layers.MaxPooling2D(pool_size=2)(x)\nx = keras.layers.Conv2D(filters=128, kernel_size=(3, 3), strides=1, padding=\"same\", activation=tf.nn.relu)(x)\nx = keras.layers.Conv2D(filters=128, kernel_size=(3, 3), strides=1, padding=\"same\", activation=tf.nn.relu)(x)\nx = keras.layers.MaxPooling2D(pool_size=2)(x)\nx = keras.layers.Flatten()(x)\nx = keras.layers.Dense(units=128, activation=tf.nn.relu)(x)\nx = keras.layers.Dense(units=16, activation=tf.nn.relu)(x)\noutputs = keras.layers.Dense(units=2, activation=tf.nn.softmax)(x)\n\nmodel = keras.models.Model(inputs=inputs, outputs=outputs)","58197927":"model.summary()","96aca38a":"model.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=[\"acc\"])","cf6d3716":"class CustomeCallback(keras.callbacks.Callback):\n    \n    def on_epoch_end(self, epoch, logs):\n        \n        if logs[\"acc\"] >= 0.97:\n            print(\"Model's accuracy is enough !\")\n            self.model.stop_training = True","6bc57ac5":"custom_callback = CustomeCallback()","ebaee0d7":"model.fit(x=training_generator, epochs=50, verbose=1, callbacks=[custom_callback], steps_per_epoch=32)","22bf4cf6":"label = [\"horse\", \"human\"]","1a83d46d":"def load_image(path):\n    \n    img = cv2.imread(path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (300, 300))\n    \n    img = np.reshape(img, (1, 300, 300, 3))\n    \n    return img","aa323013":"img = load_image(\"..\/input\/horse-breeds\/01_005.png\")\nlabel[np.argmax(model.predict(img))]","3f430ed4":"### Compilation","3f2498cc":"## Train Model","f5c5d599":"## Import Package","af4318d1":"## Create ImageDataGenerator","dd344ad7":"## Show Training Image","69729e86":"### Architechture","004e6932":"### Callback","2ce5fa65":"## Prediction","33834d8a":"## Define Model"}}