{"cell_type":{"d8a313e1":"code","baaeb6a7":"code","9e6dde6e":"code","151287ce":"code","51105583":"code","c0157b33":"code","8146f14d":"code","c430e46b":"code","380b0aec":"code","3e921848":"code","01da4473":"code","3b17d278":"code","e2da3dd4":"code","e13da246":"code","848574de":"code","e4c6644c":"code","0d1dbade":"code","daa8f152":"code","cb95a5c7":"code","637b1b5d":"code","281c56dc":"code","ff838d3a":"code","ff0dc678":"code","fc5f665f":"code","a19ab7e8":"code","1c9c5fba":"code","407ae3e7":"code","4fbe3bc8":"code","e01af963":"code","a093e65c":"code","3baee4f4":"code","944c57f6":"code","d38cdeed":"code","75230dd7":"code","2a331c7b":"code","39a3b64a":"code","3a7e01c5":"code","6f7452f1":"code","aff1a016":"code","f988f907":"code","3dcd3cc1":"code","a3f4dbfd":"code","b10559bf":"code","1b2403c8":"code","8834a350":"code","703c956f":"code","ec4c0532":"code","5804194a":"code","5e6a08f0":"code","d364be3f":"code","928c6fe0":"code","631597d2":"code","7955cf6f":"code","bd0d6218":"code","0b92a938":"code","464a39a8":"code","bb0122aa":"code","4e05fed8":"code","fa87b041":"code","618b107d":"code","f184450c":"code","0642c769":"code","7c8ede88":"code","52e34dd1":"code","60d836c3":"code","5435af2d":"code","739f99b5":"code","a480e9c9":"code","ab66195e":"code","9cf7ea20":"code","c2229c53":"code","7d665ead":"code","f2e1b4e1":"code","53bcf8a3":"code","3b51561b":"code","7f609163":"code","eab6f702":"code","3c202c00":"code","9c9d432f":"code","d71eda3b":"code","93eea267":"code","87dd50a9":"code","74048547":"code","9a3b701e":"code","309a6ce4":"code","554a870e":"code","e5a4346f":"code","7d62ad2e":"code","e45110c8":"code","f1ae8ba9":"code","09b3399b":"code","ab154295":"code","dd6bceb1":"code","f25f6dd4":"code","76cbe419":"code","bb7de78c":"code","89a4227b":"code","275fb21d":"code","e3597b88":"code","2505137b":"code","d800ee96":"code","72390049":"code","7d44cd13":"code","3e8984ef":"code","f2daf655":"code","3223ef4f":"code","eef1d58f":"code","4aa7c2ed":"code","35e79c0d":"code","ec97d4e8":"code","726ac746":"code","f430ad5f":"code","e52d345d":"code","a29732d5":"code","2064f391":"code","c243d020":"code","c19cb862":"code","ebd9772f":"code","8589ceac":"code","71c2a280":"code","41d46116":"code","99604730":"code","5fc5d974":"code","c2c5e605":"code","d9c55b97":"code","4c2fcad1":"code","29d22296":"code","cd36c79d":"code","4f80b6bb":"code","7b58f332":"code","09f98eb6":"code","89ade17c":"code","d460bf2a":"code","30393896":"code","f6c7f87c":"code","8a499cb6":"code","46f5c647":"code","f533b975":"code","60fb4637":"code","2d51a76b":"code","750fcd51":"code","f27f3225":"code","9d7a5ac2":"code","420f5062":"code","94c56971":"code","89df1b2c":"code","c8d564b7":"code","bc9a6b1d":"code","c938c3f4":"code","b3dd360b":"code","f1a3e2f9":"code","a6eb01af":"code","1d9c4fc3":"code","df6714e9":"code","01869567":"code","b0240452":"code","7baae622":"code","f18b2897":"code","6a9f5581":"code","d76d287d":"markdown","8c83d9c1":"markdown","571fda39":"markdown","e2cddf5c":"markdown","c5315da3":"markdown","582d9299":"markdown","ab222d72":"markdown","2736a337":"markdown","6c67ac06":"markdown","2f6db59b":"markdown","714b52a2":"markdown","34d5c64a":"markdown","6d8452f1":"markdown","03cab67e":"markdown","a7212976":"markdown","88c654b2":"markdown","8a021e40":"markdown","402982bc":"markdown","b1e973e8":"markdown","38aa3610":"markdown","88453325":"markdown","b7836381":"markdown","f68d9b4c":"markdown","38392be1":"markdown","3ff5edcd":"markdown","edf9718a":"markdown","061aabfb":"markdown","ad18fd3a":"markdown","543f0dc2":"markdown","ad66eeb3":"markdown","0b652e0e":"markdown","19739aae":"markdown","9677c712":"markdown","5d4bea4c":"markdown","e8761955":"markdown","154b0e55":"markdown","9c6d5068":"markdown","7cde67a3":"markdown","2c3aa2e5":"markdown","9e01815f":"markdown","00b48d57":"markdown","2f018c2c":"markdown","ec0f0641":"markdown","62e1d77f":"markdown","31095cb9":"markdown","c4c2c17f":"markdown","9c717c5a":"markdown","71b726c3":"markdown","7ad9a030":"markdown","49eb5bc3":"markdown","64fa6329":"markdown","a742d8f7":"markdown","00ec0abb":"markdown","f9621b76":"markdown","b4b65d61":"markdown","2673534f":"markdown","926cf6e0":"markdown","c8bc434b":"markdown","6b0d63e5":"markdown","d5e878c4":"markdown","a27f1a34":"markdown","2769af8e":"markdown","374b181a":"markdown","8ca39d92":"markdown","27da582d":"markdown","7409aaa0":"markdown","67e31a06":"markdown","1c4005da":"markdown","0a56180f":"markdown","90fe058d":"markdown","aa17123d":"markdown","0fda26f3":"markdown","e7901496":"markdown","82a580a8":"markdown","54274c6d":"markdown","a6020f44":"markdown","fb23873f":"markdown","4d457fc8":"markdown","cdee2dda":"markdown","f300b6cd":"markdown","311e24ee":"markdown","9353eaa3":"markdown","5cf71f99":"markdown","5669c1b5":"markdown","be378acd":"markdown","96d0df08":"markdown"},"source":{"d8a313e1":"import pandas as pd      \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import scale, StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_absolute_error,mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.decomposition import PCA\nfrom sqlalchemy import create_engine\nfrom IPython.core.pylabtools import figsize\nfrom scipy.stats import zscore\nfrom scipy import stats\nfrom numpy import percentile\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfont_title = {'family': 'times new roman', \n              'color': 'darkred', \n              'weight': 'bold', \n              'size': 14}\n\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.dpi'] = 100","baaeb6a7":"tree=pd.read_csv(\"..\/input\/forest-cover-type-dataset\/covtype.csv\")\ntree.head()","9e6dde6e":"tree.info()","151287ce":"def reversed_dummies(df: pd.DataFrame, dummy_cols: list, new_header: str, prefix='_') ->'DataFrame':\n    serial=df[dummy_cols].idxmax(axis=1).str.split(prefix).str[-1]\n    return pd.concat([df.drop(dummy_cols,axis=1),pd.DataFrame(serial,columns=[new_header])],axis=1)","51105583":"tree.columns","c0157b33":"dummy_cols=['Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5',\n       'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10',\n       'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14',\n       'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18',\n       'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22',\n       'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26',\n       'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30',\n       'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34',\n       'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38',\n       'Soil_Type39', 'Soil_Type40']\ndf=reversed_dummies(tree,dummy_cols,'Soil')\ndummy_cols=['Wilderness_Area1','Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4']\ndf=reversed_dummies(df,dummy_cols,'Wilderness')\ndf.head()","8146f14d":"df.describe()","c430e46b":"# categorik verilerin istatistiksel analizi icin:\ndf.describe(include=['O'])","380b0aec":"# her columns in num of uniques\n\ndf.apply(lambda x: x.nunique())","3e921848":"# Null percentages of each features\n\ndf.isnull().sum()*100\/tree.shape[0]","01da4473":"# Target Label\n\ndf.Cover_Type.unique() ","3b17d278":"# df.Cover_Type.value_counts(dropna=False).sort_index()\nsns.countplot(x='Cover_Type',data=df);","e2da3dd4":"plt.figure(figsize=(11,11))\nsns.heatmap(df.corr(),annot=True, cmap=\"coolwarm\");","e13da246":"df.corr()[\"Cover_Type\"].sort_values().plot.barh();","848574de":"print('Unique Values of Each Features:\\n')\nfor i in df:\n    print(f'{i}:\\n{sorted(df[i].unique())}\\n')","e4c6644c":"# sns.pairplot(df);","0d1dbade":"df.groupby(['Wilderness']).Soil.describe(include=['O'])","daa8f152":"plt.figure(figsize=(25,25))\nsns.scatterplot(x='Horizontal_Distance_To_Hydrology',y='Vertical_Distance_To_Hydrology',data=df, hue='Wilderness');","cb95a5c7":"df.Soil.value_counts(dropna=False)","637b1b5d":"plt.figure(figsize=(15,11))\nsns.countplot(y='Soil',data=df.sort_values(by=['Soil'], key=lambda col: col.str.extract('(\\d+)')[0].astype('float')));","281c56dc":"df.Wilderness.value_counts(dropna=False)","ff838d3a":"# plt.figure(figsize=(20,10))\nsns.countplot(y='Wilderness',data=df);","ff0dc678":"df.groupby(['Cover_Type', 'Wilderness']).Elevation.describe()","fc5f665f":"df[[\"Elevation\", \"Cover_Type\"]].groupby([\"Cover_Type\"], as_index = False).mean()","a19ab7e8":"sns.factorplot(x=\"Cover_Type\", y =\"Elevation\", data=df, kind=\"bar\", palette='RdBu_r',size=5);","1c9c5fba":"plt.figure(figsize=(10,6))\nsns.boxplot(x='Cover_Type', y='Elevation', data=df, palette=\"coolwarm\");","407ae3e7":"plt.figure(figsize=(6,4))\nsns.boxplot(x='Wilderness', y='Elevation', data=df, palette=\"coolwarm\");","4fbe3bc8":"plt.figure(figsize=(20,25))\nsns.boxplot(y='Soil', x='Elevation', data=df.sort_values(by=['Soil'], key=lambda col: col.str.extract('(\\d+)')[0].astype('float')), orient=\"h\",palette=\"coolwarm\");","e01af963":"# plt.figure(figsize=(15,15))\nsns.displot(df['Elevation'].dropna(), kde=True, height= 6, color='darkred', bins=20);","a093e65c":"df[[\"Slope\", \"Cover_Type\"]].groupby([\"Cover_Type\"], as_index = False).mean()","3baee4f4":"df.groupby(['Cover_Type', 'Wilderness']).Slope.describe()","944c57f6":"sns.factorplot(x=\"Cover_Type\", y =\"Slope\", data=df, kind=\"bar\", palette='RdBu_r',size=5);","d38cdeed":"sns.displot(df['Slope'].dropna(), kde=True,height= 6,color='darkred',bins=20);","75230dd7":"plt.figure(figsize=(10,6))\nsns.boxplot(x='Cover_Type', y='Slope', data=df, palette=\"coolwarm\");","2a331c7b":"df[[\"Horizontal_Distance_To_Roadways\", \"Cover_Type\"]].groupby([\"Cover_Type\"], as_index = False).mean()","39a3b64a":"sns.factorplot(x=\"Cover_Type\", y =\"Horizontal_Distance_To_Roadways\", data=df, kind=\"bar\", palette='RdBu_r',size=5);","3a7e01c5":"sns.displot(df['Horizontal_Distance_To_Roadways'].dropna(), kde=True,height= 6,color='darkred',bins=20);","6f7452f1":"plt.figure(figsize=(10,6))\nsns.boxplot(x='Cover_Type', y='Horizontal_Distance_To_Roadways', data=df, palette=\"coolwarm\");","aff1a016":"df[[\"Horizontal_Distance_To_Fire_Points\", \"Cover_Type\"]].groupby([\"Cover_Type\"], as_index = False).mean()","f988f907":"sns.factorplot(x=\"Cover_Type\", y =\"Horizontal_Distance_To_Fire_Points\", data=df, kind=\"bar\", palette='RdBu_r',size=5);","3dcd3cc1":"sns.displot(df['Horizontal_Distance_To_Fire_Points'].dropna(), kde=True,height= 6,color='darkred',bins=20);","a3f4dbfd":"plt.figure(figsize=(10,6))\nsns.boxplot(x='Cover_Type', y='Horizontal_Distance_To_Fire_Points', data=df, palette=\"coolwarm\");","b10559bf":"df.isnull().sum().any()","1b2403c8":"# numeric = []\n# for col in df.columns:\n#     if df[col].nunique() > 7 : numeric.append(col)\n# print(numeric)","8834a350":"# features = df.loc[:,'Elevation':'Horizontal_Distance_To_Fire_Points']","703c956f":"df.shape","ec4c0532":"df.corr()[\"Cover_Type\"].sort_values(ascending=False)","5804194a":"df.columns","5e6a08f0":"def col_plot(df,col_name):\n    plt.figure(figsize=(15,6))\n    \n    plt.subplot(141) # 1 satir x 4 sutun dan olusan ax in 1. sutununda calis\n    plt.hist(df[col_name], bins = 20)\n    f=lambda x:(np.sqrt(x) if x>=0 else -np.sqrt(-x))\n    \n    # \u00fc\u00e7 sigma aralikta(verinin %99.7 sini icine almasi beklenen bolum) iki kirmizi cizgi arasinda\n    plt.axvline(x=df[col_name].mean() + 3*df[col_name].std(),color='red')\n    plt.axvline(x=df[col_name].mean() - 3*df[col_name].std(),color='red')\n    plt.xlabel(col_name)\n    plt.tight_layout\n    plt.xlabel(\"Histogram \u00b13z\")\n    plt.ylabel(col_name)\n\n    plt.subplot(142)\n    plt.boxplot(df[col_name]) # IQR katsayisi, defaultu 1.5\n    plt.xlabel(\"IQR=1.5\")\n\n    plt.subplot(143)\n    plt.boxplot(df[col_name].apply(f), whis = 2.5)\n    plt.xlabel(\"ROOT SQUARE - IQR=2.5\")\n\n    plt.subplot(144)\n    plt.boxplot(np.log(df[col_name]+0.1), whis = 2.5)\n    plt.xlabel(\"LOGARITMIC - IQR=2.5\")\n    plt.show()","d364be3f":"for i in df.columns[:-3]:\n    col_plot(df,i)","928c6fe0":"from scipy.stats.mstats import winsorize\n\ndef plot_winsorize(df,col_name,up=0.1,down=0):\n    plt.figure(figsize = (15, 6))\n\n    winsor=winsorize(df[col_name], (down,up))\n    logr=np.log(df[col_name]+0.1)\n\n    plt.subplot(141)\n    plt.hist(winsor, bins = 22)\n    plt.axvline(x=winsor.mean()+3*winsor.std(),color='red')\n    plt.axvline(x=winsor.mean()-3*winsor.std(),color='red')\n    plt.xlabel('Winsorize_Histogram')\n    plt.ylabel(col_name)\n    plt.tight_layout\n\n    plt.subplot(142)\n    plt.boxplot(winsor, whis = 1.5)\n    plt.xlabel('Winsorize - IQR:1.5')\n    \n    plt.subplot(143)\n    plt.hist(logr, bins=22)\n    plt.axvline(x=logr.mean()+3*logr.std(),color='red')\n    plt.axvline(x=logr.mean()-3*logr.std(),color='red')\n    plt.xlabel('Logr_col_name')\n\n    plt.subplot(144)\n    plt.boxplot(logr, whis = 1.5)\n    plt.xlabel(\"Logaritmic - IQR=1.5\")\n    plt.show()    \n","631597d2":"\nfor i in df.columns[:-3]:\n    plot_winsorize(df,i)","7955cf6f":"features=['Elevation', \n#            'Aspect', # angle\n           'Slope', \n           'Horizontal_Distance_To_Hydrology',\n           'Vertical_Distance_To_Hydrology',\n           'Horizontal_Distance_To_Roadways', # not expected normal distribution\n#            'Hillshade_9am', # bitwise\n#            'Hillshade_Noon',\n#            'Hillshade_3pm',\n           'Horizontal_Distance_To_Fire_Points']","bd0d6218":"df_winsorised=df.copy()\nfor i in features:\n    df_winsorised[i]=winsorize(df_winsorised[i], (0,0.1))","0b92a938":"df_log=df.copy()\nfor i in features:\n    df_log[i]=np.log(df_log[i])","464a39a8":"df_root=df.copy()\nf=lambda x:(np.sqrt(x) if x>=0 else -np.sqrt(-x))\nfor i in features:\n    df_root[i]=df_root[i].apply(f)","bb0122aa":"from numpy import percentile\nfrom scipy.stats import zscore\nfrom scipy import stats\n\ndef outlier_zscore(df, col, min_z=1, max_z = 5, step = 0.1, print_list = False):\n    z_scores = zscore(df[col].dropna())\n    threshold_list = []\n    for threshold in np.arange(min_z, max_z, step):\n        threshold_list.append((threshold, len(np.where(z_scores > threshold)[0])))\n        df_outlier = pd.DataFrame(threshold_list, columns = ['threshold', 'outlier_count'])\n        df_outlier['pct'] = (df_outlier.outlier_count - df_outlier.outlier_count.shift(-1))\/df_outlier.outlier_count*100\n    plt.plot(df_outlier.threshold, df_outlier.outlier_count)\n    best_treshold = round(df_outlier.iloc[df_outlier.pct.argmax(), 0],2)\n    outlier_limit = int(df[col].dropna().mean() + (df[col].dropna().std()) * df_outlier.iloc[df_outlier.pct.argmax(), 0])\n    percentile_threshold = stats.percentileofscore(df[col].dropna(), outlier_limit)\n    plt.vlines(best_treshold, 0, df_outlier.outlier_count.max(), \n               colors=\"r\", ls = \":\"\n              )\n    plt.annotate(\"Zscore : {}\\nValue : {}\\nPercentile : {}\".format(best_treshold, outlier_limit, \n                                                                   (np.round(percentile_threshold, 3), \n                                                                    np.round(100-percentile_threshold, 3))), \n                 (best_treshold, df_outlier.outlier_count.max()\/2))\n    #plt.show()\n    if print_list:\n        print(df_outlier)\n    return (plt, df_outlier, best_treshold, outlier_limit, percentile)","4e05fed8":"from scipy.stats import zscore\nfrom scipy import stats\n\ndef outlier_inspect(df, col, min_z=1, max_z = 5, step = 0.5, max_hist = None, bins = 50):\n    fig = plt.figure(figsize=(20, 6))\n    fig.suptitle(col, fontsize=16)\n    plt.subplot(1,3,1)\n    if max_hist == None:\n        sns.distplot(df[col], kde=False, bins = 50)\n    else :\n        sns.distplot(df[df[col]<=max_hist][col], kde=False, bins = 50)\n   \n    plt.subplot(1,3,2)\n    sns.boxplot(df[col])\n    plt.subplot(1,3,3)\n    z_score_inspect = outlier_zscore(df, col, min_z=min_z, max_z = max_z, step = step)\n    \n    plt.subplot(1,3,1)\n    plt.axvline(x=df[col].mean() + z_score_inspect[2]*df[col].std(),color='red',linewidth=1,linestyle =\"--\")\n    plt.axvline(x=df[col].mean() - z_score_inspect[2]*df[col].std(),color='red',linewidth=1,linestyle =\"--\")\n    plt.show()\n    \n    return z_score_inspect","fa87b041":"def detect_outliers(df:pd.DataFrame, col_name:str, p=1.5) ->int:\n    ''' \n    this function detects outliers based on 3 time IQR and\n    returns the number of lower and uper limit and number of outliers respectively\n    '''\n    first_quartile = np.percentile(np.array(df[col_name].tolist()), 25)\n    third_quartile = np.percentile(np.array(df[col_name].tolist()), 75)\n    IQR = third_quartile - first_quartile\n                      \n    upper_limit = third_quartile+(p*IQR)\n    lower_limit = first_quartile-(p*IQR)\n    outlier_count = 0\n                      \n    for value in df[col_name].tolist():\n        if (value < lower_limit) | (value > upper_limit):\n            outlier_count +=1\n    return lower_limit, upper_limit, outlier_count","618b107d":"print(\"Number of Outliers for k*IQR\\n\")\nk=3\n\ntotal=0\nfor col in features:\n    if detect_outliers(df, col)[2] > 0:\n        outliers=detect_outliers(df, col, k)[2]\n        total+=outliers\n        print(\"{} outliers in '{}'\".format(outliers,col))\nprint(\"\\n{} OUTLIERS TOTALLY\".format(total))","f184450c":"k=2\nprint(f\"Number of Outliers for {k}*IQR after Winsorised\\n\")\n\ntotal=0\nfor col in features:\n    if detect_outliers(df_winsorised, col)[2] > 0:\n        outliers=detect_outliers(df_winsorised, col, k)[2]\n        total+=outliers\n        print(\"{} outliers in '{}'\".format(outliers,col))\nprint(\"\\n{} OUTLIERS TOTALLY\".format(total))","0642c769":"k=3\nprint(f\"Number of Outliers for {k}*IQR after Logarithmed\\n\")\n\ntotal=0\nfor col in features:\n    if detect_outliers(df_log, col)[2] > 0:\n        outliers=detect_outliers(df_log, col, k)[2]\n        total+=outliers\n        print(\"{} outliers in '{}'\".format(outliers,col))\nprint(\"\\n{} OUTLIERS TOTALLY\".format(total))","7c8ede88":"k=2.2\nprint(f\"Number of Outliers for {k}*IQR after Root Square\\n\")\n\ntotal=0\nfor col in features:\n    if detect_outliers(df_root, col)[2] > 0:\n        outliers=detect_outliers(df_root, col, k)[2]\n        total+=outliers\n        print(\"{} outliers in '{}'\".format(outliers,col))\nprint(\"\\n{} OUTLIERS TOTALLY\".format(total))","52e34dd1":"z_scores=[]\nfor i in df.columns[:-3]:\n    z_scores.append(outlier_inspect(df,i)[2])","60d836c3":"z_scores","5435af2d":"df_z=df.copy()\nfor i in features:\n    down_limit= df_z[i].mean() - z_scores[df_z.columns.get_loc(i)]*df_z[i].std()\n    upper_limit= df_z[i].mean() + z_scores[df_z.columns.get_loc(i)]*df_z[i].std()\n    condition= (down_limit < df_z[i]) & (df_z[i] < upper_limit)\n    df_z=df_z[condition]","739f99b5":"print('Number of Outliers:',len(df)-len(df_z))","a480e9c9":"df_3z=df.copy()\nfor i in features:\n    down_limit= df_3z[i].mean() - (3*df_3z[i].std())\n    upper_limit= df_3z[i].mean() + (3*df_3z[i].std())\n    condition= (down_limit < df_3z[i]) & (df_3z[i] < upper_limit)\n    df_3z=df_3z[condition]","ab66195e":"print('Number of Outliers:',len(df)-len(df_3z))","9cf7ea20":"k=2\nfor i in features:\n    lower,upper,_=detect_outliers(df_winsorised,i,k)\n    df_winsorised=df_winsorised[(df_winsorised[i]>lower)&(df_winsorised[i]<upper)]","c2229c53":"df=df_winsorised\ndf.shape","7d665ead":"# After dropping rows, you can run reset_index()\ndf = df.reset_index(drop=True)","f2e1b4e1":"df.Cover_Type.value_counts(dropna=False).sort_index()","53bcf8a3":"sns.countplot(x='Cover_Type',data=df);","3b51561b":"def perc_col(df,col):\n    print(f'Percentage of df[\"{col}\"] Unique Values:\\n')\n    for i in sorted(df[col].unique()):\n        print('%s: %%%.2f' % (i, 100*df[col].value_counts()[i]\/len(df)))","7f609163":"perc_col(df,'Cover_Type')","eab6f702":"plt.figure(figsize=(25,13))\nsns.heatmap(df.corr(),square=True, annot=True, linewidths=.5, vmin=-1, vmax=1, cmap='viridis')\nplt.title(\"Correlation Matrix\", fontdict=font_title)\nplt.show()","3c202c00":"df=pd.get_dummies(df)\ndf.head(2)","9c9d432f":"df.to_csv(\"covtype_EDA.csv\", index = False)","d71eda3b":"import sqlite3","93eea267":"# Creation of the connection and importing dataframe into database.\n\n# with sqlite3.connect(\"tree_database.db\") as cnnct:\n#     tree1.to_sql(\"covtype2\", cnnct, if_exists = \"replace\")","87dd50a9":"# create a database named as tree_database.db\ncnnct = sqlite3.connect('tree_database.db')  \n\nwith cnnct: \n    # create a table named as covtype in tree_database database\n    df.to_sql(\"covtype\", cnnct, if_exists = \"replace\")","74048547":"query1 = \"\"\"SELECT *,\n(Horizontal_Distance_To_Hydrology*Horizontal_Distance_To_Hydrology)+(Vertical_Distance_To_Hydrology*Vertical_Distance_To_Hydrology) as Square_Hypo_Distance,\n(Horizontal_Distance_To_Hydrology + Horizontal_Distance_To_Roadways)\/2 as Average_Dist_Road_Hydro, \n(Elevation + Vertical_Distance_To_Hydrology) \/2 as Average_Elevation_Hydro\nFROM covtype;\"\"\"","9a3b701e":"query2 = \"\"\"SELECT Elevation, Aspect, Slope, Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, \nHorizontal_Distance_To_Fire_Points, Wilderness_Area1, Wilderness_Area2, Wilderness_Area3,\nWilderness_Area4, Soil_Type1, Soil_Type2, Soil_Type3, Soil_Type4, Soil_Type5, Soil_Type6,\nSoil_Type9, Soil_Type10, Soil_Type11, Soil_Type12, Soil_Type13, Soil_Type16, Soil_Type17, \nSoil_Type18, Soil_Type19, Soil_Type20, Soil_Type22, Soil_Type23, Soil_Type24, \nSoil_Type26, Soil_Type27, Soil_Type29, Soil_Type30, Soil_Type31, Soil_Type32, Soil_Type33, \nSoil_Type34, Soil_Type35, Soil_Type38, Soil_Type39, Soil_Type40, Cover_Type,\nSquare_Hypo_Distance, Average_Dist_Road_Hydro, Average_Elevation_Hydro\nFROM covtype;\"\"\"","309a6ce4":"# connect tree_database.db\ncnnct = sqlite3.connect('tree_database.db')  \n\nwith cnnct: \n    # create a table named as covtype in tree_database database\n    df1 = pd.read_sql_query(query1, cnnct)  # transforming and exporting to df1\n    df1.to_sql(\"covtype\", cnnct, if_exists = \"replace\")  # importing again to dropping columns\n    df2 = pd.read_sql_query(query2, cnnct)  # exporting table to df2","554a870e":"df2.to_csv(\"covtype_FE.csv\", index = False)","e5a4346f":"import pandas as pd\nimport numpy as np\nfrom numpy import percentile\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\n\nfrom sqlalchemy import create_engine\nfrom IPython.core.pylabtools import figsize\nfrom statsmodels.formula.api import ols\nfrom scipy.stats import zscore\nfrom scipy import stats\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, f1_score, recall_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, train_test_split\n\nfrom sklearn.preprocessing import LabelEncoder, scale, StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom yellowbrick.classifier import ClassificationReport\nfrom yellowbrick.datasets import load_occupancy\nfrom sklearn.metrics import f1_score\nimport warnings\nfont_title = {'family': 'times new roman', 'color': 'darkred', \n              'weight': 'bold', 'size': 14}\n\nwarnings.filterwarnings('ignore')\nsns.set_style(\"whitegrid\")\n\nplt.rcParams['figure.dpi'] = 100","7d62ad2e":"df = pd.read_csv(\"covtype_EDA.csv\")\ndf.head()","e45110c8":"df.shape","f1ae8ba9":"df.columns","09b3399b":"df.Cover_Type.value_counts()","ab154295":"row_num=df.Cover_Type.value_counts().min()","dd6bceb1":"df2=pd.DataFrame()\n\nfor i in df.Cover_Type.unique():\n    df2=pd.concat([df2,df[df.Cover_Type==i].sample(row_num)])\n    ","f25f6dd4":"df2.Cover_Type.value_counts()","76cbe419":"X = df2.drop(\"Cover_Type\", axis = 1)\ny = df2[\"Cover_Type\"]","bb7de78c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","89a4227b":"from sklearn.preprocessing import StandardScaler","275fb21d":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test= sc.transform(X_test)","e3597b88":"xgb= XGBClassifier()\nxgb.fit(X_train , y_train)","2505137b":"y_pred = xgb.predict(X_test)","d800ee96":"print(classification_report(y_test,y_pred))","72390049":"xgb_accuracy = accuracy_score(y_test, y_pred)\nxgb_f1_score = f1_score(y_test, y_pred, average='weighted')\nxgb_recall = recall_score(y_test, y_pred, average='weighted')\nprint('xgb_accuracy:',xgb_accuracy,\n      '\\nxgb_f1_score:',xgb_f1_score,\n      '\\nxgb_recall:',xgb_recall)","7d44cd13":"xgb_params = {\"n_estimators\": [50,300,500],\n             \"subsample\":[0.5,0.8,1],\n             \"max_depth\":[3,9,17],\n             \"learning_rate\":[0.1,0.01,0.3]}","3e8984ef":"# xgb_grid= GridSearchCV(xgb, xgb_params, cv = 5, \n#                             n_jobs = -1, verbose = 2).fit(X_train, y_train)","f2daf655":"# xgb_grid= RandomizedSearchCV(xgb, xgb_params, cv = 5, \n#                             n_iter=10, n_jobs = -1, verbose = 2).fit(X_train, y_train)","3223ef4f":"# xgb_grid.best_params_","eef1d58f":"xgb_tuned = XGBClassifier(learning_rate= 0.3, \n                                max_depth= 15, \n                                n_estimators= 200, \n                                subsample= 0.7).fit(X_train, y_train)","4aa7c2ed":"y_pred = xgb_tuned.predict(X_test)\nprint(classification_report(y_test, y_pred))","35e79c0d":"xgb_accuracy = accuracy_score(y_test, y_pred)\nxgb_f1_score = f1_score(y_test, y_pred, average='weighted')\nxgb_recall = recall_score(y_test, y_pred, average='weighted')\nprint('xgb_accuracy:',xgb_accuracy,\n      '\\nxgb_f1_score:',xgb_f1_score,\n      '\\nxgb_recall:',xgb_recall)","ec97d4e8":"# xgb_accuracy = cross_val_score(xgb_tuned, X_test, y_test,cv = 10).mean()\n# xgb_f1_score = cross_val_score(xgb_tuned, X_test, y_test,cv = 10,scoring='f1_weighted').mean()\n# xgb_recall = cross_val_score(xgb_tuned, X_test, y_test,cv = 10,scoring='recall_weighted').mean()\n# print('xgb_accuracy:',xgb_accuracy,\n#       '\\nxgb_f1_score:',xgb_f1_score,\n#       '\\nxgb_recall:',xgb_recall)","726ac746":"# from sklearn.datasets import load_digits\n# from sklearn.model_selection import train_test_split as tts\n# from sklearn.linear_model import LogisticRegression\nfrom yellowbrick.classifier import ConfusionMatrix\n\n# The ConfusionMatrix visualizer taxes a model\ncm = ConfusionMatrix(xgb)\n\n# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\ncm.fit(X_train, y_train)\n\n# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n# and then creates the confusion_matrix from scikit-learn.\ncm.score(X_test, y_test)\n\ncm.show();","f430ad5f":"# from sklearn.datasets import make_classification\n# from sklearn.model_selection import train_test_split\n# from sklearn.ensemble import RandomForestClassifier\nfrom yellowbrick.classifier import ClassPredictionError\n\nvisualizer = ClassPredictionError(xgb)\n\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n\n# Draw visualization\nvisualizer.show();","e52d345d":"rfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train, y_train)","a29732d5":"y_pred = rfc.predict(X_test)","2064f391":"print('Confusion Matrix:',*confusion_matrix(y_test,y_pred), sep=\"\\n\")\nprint(classification_report(y_test, y_pred))","c243d020":"rfc_accuracy = accuracy_score(y_test, y_pred)\nrfc_f1_score = f1_score(y_test, y_pred, average='weighted')\nrfc_recall = recall_score(y_test, y_pred, average='weighted')\nprint('rfc_accuracy:',rfc_accuracy,\n      '\\nrfc_f1_score:',rfc_f1_score,\n      '\\nrfc_recall:',rfc_recall)","c19cb862":"# rfc_accuracy = cross_val_score(rfc, X_test, y_test,cv = 10).mean()\n# rfc_f1_score = cross_val_score(rfc, X_test, y_test,cv = 10,scoring='f1_weighted').mean()\n# rfc_recall = cross_val_score(rfc, X_test, y_test,cv = 10,scoring='recall_weighted').mean()\n# print('rfc_accuracy:',rfc_accuracy,\n#       '\\nrfc_f1_score:',rfc_f1_score,\n#       '\\nrfc_recall:',rfc_recall)","ebd9772f":"# rfc_params = {\"n_estimators\":[50, 100, 300],\n#               \"max_depth\":[3,5,7],\n#               \"max_features\": [2,4,6,8],\n#               \"min_samples_split\": [2,4,6]}","8589ceac":"# rfc_grid = GridSearchCV(rfc, rfc_params, cv = 5, n_jobs = -1, verbose = 2).fit(X_train, y_train)","71c2a280":"# rfc_grid.best_params_","41d46116":"# rfc_tuned = RandomForestClassifier(max_depth = 7,             \n#                                   max_features = 8, \n#                                   min_samples_split = 2, \n#                                   n_estimators = 1000).fit(X_train, y_train)","99604730":"# y_pred = rfc_tuned.predict(X_test)\n# print(classification_report(y_test, y_pred))","5fc5d974":"# The ConfusionMatrix visualizer taxes a model\ncm = ConfusionMatrix(rfc)\n\n# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\ncm.fit(X_train, y_train)\n\n# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n# and then creates the confusion_matrix from scikit-learn.\ncm.score(X_test, y_test)\n\ncm.show();","c2c5e605":"visualizer = ClassPredictionError(rfc)\n\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n\n# Draw visualization\nvisualizer.show();","d9c55b97":"knn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train,y_train)","4c2fcad1":"y_pred = knn.predict(X_test)","29d22296":"neighbors = range(1,8,2) # k nin tek sayi olmasi beklenir.\ntrain_accuracy =np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\nfor i,k in enumerate(neighbors):\n    #Setup a knn classifier with k neighbors\n    knn = KNeighborsClassifier(n_neighbors = k)\n    \n    #Fit the model\n    knn.fit(X_train, y_train)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n    \n    #Compute accuracy on the test set\n    test_accuracy[i] = knn.score(X_test, y_test)","cd36c79d":"plt.figure(figsize=(8,5))\nplt.title('k-NN assesment of number of neighbors')\nplt.plot(neighbors, test_accuracy, label='Accuracy of Test Data')\nplt.plot(neighbors, train_accuracy, label='Accuracy of Training Data')\nplt.legend()\nplt.xlabel('Number of neighbors')\nplt.ylabel('Accuracy')\nplt.show()","4f80b6bb":"error_rate = []\n# Her bir error rate icin olusan k degeri bu listeye atilacak\n# k nin tek sayi olmasi beklenir.\n# Will take some time\nfor i in range(1,8,2):\n    \n    model = KNeighborsClassifier(n_neighbors=i) # k= i\n    model.fit(X_train,y_train)\n    y_pred_i = model.predict(X_test)\n    error_rate.append(np.mean(y_pred_i != y_test)) \n    \n\nprint('Optimum K_Value: ',error_rate.index(min(error_rate))+1)","7b58f332":"plt.figure(figsize=(10,6))\nplt.plot(range(1,8,2),\n         error_rate,\n         color='blue', \n         linestyle='dashed', \n         marker='o',\n         markerfacecolor='red', \n         markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate');","09f98eb6":"# knn = KNeighborsClassifier()\n# knn_params = {\"n_neighbors\": range(1,8,2)} # k tek sayi olmali\n\n# knn_cv_model = GridSearchCV(knn, knn_params, cv=10).fit(X_train, y_train)","89ade17c":"# knn_cv_model.best_params_","d460bf2a":"knn_tuned= KNeighborsClassifier(n_neighbors = 1).fit(X_train, y_train)\ny_pred = knn_tuned.predict(X_test)","30393896":"print('Confusion Matrix:',*confusion_matrix(y_test,y_pred), sep=\"\\n\")\nprint(classification_report(y_test, y_pred))","f6c7f87c":"knn_accuracy = accuracy_score(y_test, y_pred)\nknn_f1_score = f1_score(y_test, y_pred, average='weighted')\nknn_recall = recall_score(y_test, y_pred, average='weighted')\nprint('knn_accuracy:',knn_accuracy,\n      '\\nknn_f1_score:',knn_f1_score,\n      '\\nknn_recall:',knn_recall)","8a499cb6":"from yellowbrick.classifier import ConfusionMatrix\n# The ConfusionMatrix visualizer taxes a model\ncm = ConfusionMatrix(knn)\n\n# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\ncm.fit(X_train, y_train)\n\n# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n# and then creates the confusion_matrix from scikit-learn.\ncm.score(X_test, y_test)\n\ncm.show();","46f5c647":"# Alternative\n# from sklearn.metrics import classification_report,confusion_matrix\n# sns.heatmap(confusion_matrix(y_test,y_pred), annot=True, cmap=\"YlGnBu\",fmt='d')\n# plt.ylabel('Actual Label')\n# plt.xlabel('Predicted Label');","f533b975":"from yellowbrick.classifier import ClassPredictionError\n\nvisualizer = ClassPredictionError(knn)\n\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n\n# Draw visualization\nvisualizer.show();","60fb4637":"dt = DecisionTreeClassifier()\ndt.fit(X_train , y_train)\ny_pred = dt.predict(X_test)","2d51a76b":"print('Confusion Matrix:',*confusion_matrix(y_test,y_pred), sep=\"\\n\")\nprint(classification_report(y_test, y_pred))","750fcd51":"dt_accuracy = accuracy_score(y_test, y_pred)\ndt_f1_score = f1_score(y_test, y_pred, average='weighted')\ndt_recall = recall_score(y_test, y_pred, average='weighted')\nprint('dt_accuracy:',dt_accuracy,\n      '\\ndt_f1_score:',dt_f1_score,\n      '\\ndt_recall:',dt_recall)","f27f3225":"from yellowbrick.classifier import ConfusionMatrix\n# The ConfusionMatrix visualizer taxes a model\ncm = ConfusionMatrix(dt)\n\n# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\ncm.fit(X_train, y_train)\n\n# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n# and then creates the confusion_matrix from scikit-learn.\ncm.score(X_test, y_test)\n\ncm.show();","9d7a5ac2":"visualizer = ClassPredictionError(dt)\n\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n\n# Draw visualization\nvisualizer.show();","420f5062":"svm = SVC()\nsvm.fit(X_train,y_train)\n\ny_pred = svm.predict(X_test)","94c56971":"print('Confusion Matrix:',*confusion_matrix(y_test,y_pred), sep=\"\\n\")\nprint(classification_report(y_test, y_pred))","89df1b2c":"svm_accuracy = accuracy_score(y_test, y_pred)\nsvm_f1_score = f1_score(y_test, y_pred, average='weighted')\nsvm_recall = recall_score(y_test, y_pred, average='weighted')\nprint('svm_accuracy:',svm_accuracy,\n      '\\nsvm_f1_score:',svm_f1_score,\n      '\\nsvm_recall:',svm_recall)","c8d564b7":"from yellowbrick.classifier import ConfusionMatrix\n\n# The ConfusionMatrix visualizer taxes a model\ncm = ConfusionMatrix(svm)\n\n# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\ncm.fit(X_train, y_train)\n\n# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n# and then creates the confusion_matrix from scikit-learn.\ncm.score(X_test, y_test)\n\ncm.show();","bc9a6b1d":"visualizer = ClassPredictionError(svm)\n\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n\n# Draw visualization\nvisualizer.show();","c938c3f4":"from yellowbrick.classifier import ClassificationReport\n\nvisualizer = ClassificationReport(svm, support=True)\n\nvisualizer.fit(X_train, y_train)        # Fit the visualizer and the model\nvisualizer.score(X_test, y_test)        # Evaluate the model on the test data\nvisualizer.show();","b3dd360b":"# param_grid = {'C': [0.1, 1, 10, 100, 1000],\n#               'gamma': [1,0.1,0.01,0.001,0.0001], \n#               'kernel': ['rbf']} ","f1a3e2f9":"# grid_svc = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)\n# grid_svc.fit(X_train,y_train)\n# grid_svc.best_params_","a6eb01af":"# y_pred = grid_svc.predict(X_test)\n# print(classification_report(y_test, y_pred))","1d9c4fc3":"compare = pd.DataFrame({\"Model\": [\"Random Forest\", \n                                  \"XGBoost\",\n#                                   \"Logistic Regression\",\n#                                   'Naive Bayes',\n#                                   'Ada Boosting',\n                                  'Support Vector Machine', \n                                  \"Decision Tree\",\n                                  \"K-Nearest Neighbor\",\n                                 ],\n                        \"Accuracy\": [rfc_accuracy, \n                                     xgb_accuracy, \n#                                      log_accuracy, \n#                                      nb_accuracy,\n#                                      ada_accuracy, \n                                     svm_accuracy,\n                                     dt_accuracy,\n                                     knn_accuracy,\n                                     ],\n                        \"F1 Score\": [rfc_f1_score, \n                                     xgb_f1_score, \n#                                      log_f1_score, \n#                                      nb_f1_score, \n#                                      ada_f1_score, \n                                     svm_f1_score, \n                                     dt_f1_score,\n                                     knn_f1_score,\n                                    ],\n                        \"Recall\": [rfc_recall, \n                                   xgb_recall, \n#                                    log_recall, \n#                                    nb_recall,\n#                                    ada_recall, \n                                   svm_recall,\n                                   dt_recall,\n                                   knn_recall,\n                                  ]})\n\ndef labels(ax):\n    for p in ax.patches:\n        width = p.get_width()    # get bar length\n        ax.text(width,       # set the text at 1 unit right of the bar\n                p.get_y() + p.get_height() \/ 2, # get Y coordinate + X coordinate \/ 2\n                '{:1.2f}'.format(width), # set variable to display, 2 decimals\n                ha = 'left',   # horizontal alignment\n                va = 'center')  # vertical alignment\n    \nplt.subplot(311)\ncompare = compare.sort_values(by=\"Accuracy\", ascending=False)\nax=sns.barplot(x=\"Accuracy\", y=\"Model\", data=compare, palette=\"Blues_d\")\nlabels(ax)\nplt.show()\n\nplt.subplot(312)\ncompare = compare.sort_values(by=\"F1 Score\", ascending=False)\nax=sns.barplot(x=\"F1 Score\", y=\"Model\", data=compare, palette=\"Blues_d\")\nlabels(ax)\nplt.show()\n\nplt.subplot(313)\ncompare = compare.sort_values(by=\"Recall\", ascending=False)\nax=sns.barplot(x=\"Recall\", y=\"Model\", data=compare, palette=\"Blues_d\")\nlabels(ax)\nplt.show()","df6714e9":"X = df.drop(\"Cover_Type\", axis = 1)\ny = df[\"Cover_Type\"]","01869567":"y_pred = xgb_tuned.predict(X)\nprint(classification_report(y, y_pred))","b0240452":"xgb_accuracy = accuracy_score(y, y_pred)\nxgb_f1_score = f1_score(y, y_pred, average='weighted')\nxgb_recall = recall_score(y, y_pred, average='weighted')\nprint('xgb_accuracy:',xgb_accuracy,\n      '\\nxgb_f1_score:',xgb_f1_score,\n      '\\nxgb_recall:',xgb_recall)","7baae622":"import pickle\npickle.dump(xgb_tuned,open(\"model\",\"wb\"))\nxgb_model = pickle.load(open(\"model\",\"rb\"))","f18b2897":"input={  'Elevation': 3056,\n         'Aspect': 272,\n         'Slope': 6,\n         'Horizontal_Distance_To_Hydrology':319,\n         'Vertical_Distance_To_Hydrology': 60,\n         'Horizontal_Distance_To_Roadways': 1642,\n         'Hillshade_9am':204,\n         'Hillshade_Noon':242,\n         'Hillshade_3pm':176,\n         'Horizontal_Distance_To_Fire_Points':1591,\n         'Soil_Type1': 0,\n         'Soil_Type10':0,\n         'Soil_Type11':0,\n         'Soil_Type12':0,\n         'Soil_Type13':0,\n         'Soil_Type14':0,\n         'Soil_Type16':0,\n         'Soil_Type17':0,\n         'Soil_Type18':0,\n         'Soil_Type19':0,\n         'Soil_Type2':0,\n         'Soil_Type20':0,\n         'Soil_Type21':0,\n         'Soil_Type22':0,\n         'Soil_Type23':1,\n         'Soil_Type24':0,\n         'Soil_Type25':0,\n         'Soil_Type26':0,\n         'Soil_Type27':0,\n         'Soil_Type28':0,\n         'Soil_Type29':0,\n         'Soil_Type3':0,\n         'Soil_Type30':0,\n         'Soil_Type31':0,\n         'Soil_Type32':0,\n         'Soil_Type33':0,\n         'Soil_Type34':0,\n         'Soil_Type35':0,\n         'Soil_Type36':0,\n         'Soil_Type37':0,\n         'Soil_Type38':0,\n         'Soil_Type39':0,\n         'Soil_Type4':0,\n         'Soil_Type40':0,\n         'Soil_Type5':0,\n         'Soil_Type6':0,\n         'Soil_Type7':0,\n         'Soil_Type8':0,\n         'Soil_Type9':0,\n         'Wilderness_Area1':0,\n         'Wilderness_Area2':0,\n         'Wilderness_Area3':1,\n         'Wilderness_Area4':0}","6a9f5581":"xgb_model.predict(input)","d76d287d":"### Detect Missing Values and Outliers","8c83d9c1":"### Relationships and Correlations Between Features","571fda39":"# Modelling (Multi-class Classification)","e2cddf5c":"> **Horizontal_Distance_To_Roadways - Cover_Type**","c5315da3":"   **Features**\n   \n    Elevation - Elevation in meters\n    Aspect - Aspect in degrees azimuth\n    Slope - Slope in degrees\n    Horizontal_Distance_To_Hydrology - Horz Dist to nearest surface water features\n    Vertical_Distance_To_Hydrology - Vert Dist to nearest surface water features\n    Horizontal_Distance_To_Roadways - Horz Dist to nearest roadway\n    Hillshade_9am (0 to 255 index) - Hillshade index at 9am, summer solstice\n    Hillshade_Noon (0 to 255 index) - Hillshade index at noon, summer solstice\n    Hillshade_3pm (0 to 255 index) - Hillshade index at 3pm, summer solstice\n    Horizontal_Distance_To_Fire_Points - Horz Dist to nearest wildfire ignition points\n    Wilderness_Area (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation\n    Soil_Type (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation\n    Cover_Type (7 types, integers 1 to 7) - Forest Cover Type designation","582d9299":"**Yukaridaki grefige gore Datanin  %99.7 sini kapsayan aralik**","ab222d72":"**Training the Model with All Dataset**","2736a337":"**Visualization of Confusion Matrix with Countplot**","6c67ac06":"## Undersampling\n* Data is large and unbalanced. Oversampling has many challanges for this data. So we use `UNDERSAMPLING` technique. We choose a sample according to Cover Type 4 that has the least row counts(2369).","2f6db59b":"**Evaluate the Performance**","714b52a2":"#### *ii. Drop Outliers*","34d5c64a":"**`Cross Validation Scores`**","6d8452f1":"There are many different methods for outliers. You can use IQR values used as standard to deal with outliers, or you can define two functions to help you understand the outliers and how you can deal with them.\n- Two functions given as extra for outlier detection in *EDA Project (Auto Scout)* are given below.","03cab67e":"**Summary Results :**\n* \"Elevation\", \"Slope\", \"Horizontal_Distance_To_Hydrology\", \"Vertical_Distance_To_Hydrology\",\"Horizontal_Distance_To_Roadways\", \"Horizontal_Distance_To_Fire_Points\" are continuous variables and their values vary.\n* \"Aspect\" is also continuous and its values vary from 0 to 360. It has angular values.\n* \"Hillshade_3pm\", \"Hillshade_Noon\", \"Hillshade_3pm\" are also continuos and their values vary from 0 to 255. This means that the values represent bitwise value. I concluded that the values are RGB color representation of the shadow at a particular time.\n* Wilderness_Areas and Soil_Types are categorical (binary 1 or 0) data.","a7212976":"**Tunning SVM**\n\n* if C=1, soft margin\n* if C=1000, hard margin\n* C, margine yakinlik veya uzakligina gore ceza verir.\n* Gamma buyurse daha kompleks siniflandirici haline gelir ve overfitinge gidebilir.\n* rbf kullanilacaksa C ve Gamma optimize edilmelidir. Ozellikle Gamma !\n* Gamma buyukse, C onemsiz hale gelir.\n* 0.0001 < Gamma < 10  \n* 0.1 < C < 100","88c654b2":"### Import Libraries, Load Dataset, Exploring Data","8a021e40":"# WELCOME!","402982bc":"# #Determines","b1e973e8":"## 2.  Data Cleaning","38aa3610":"Creation a sqlite database (\"tree_database\") and transferring dataframe(tree1) from python into database table (covtype2)","88453325":"# ``1.XGBoost Classifer``\n- Import the modul \n- Fit the model \n- Predict the test set\n- Visualize and evaluate the result (use yellowbrick module)","b7836381":"**RF Tunning**","f68d9b4c":"**Visualization of Confusion Matrix with Countplot**","38392be1":"#### 1. Exploratory Data Analysis (EDA)\n- Import Libraries, Load Dataset, Exploring Data\n\n    *i. Import Libraries*\n    \n    *ii. Load Dataset*\n    \n    *iii. Explore Data*\n\n#### 2.  Data Cleaning\n- Detect Missing Values and Outliers \n\n    *i. Missing Value Detection*\n    \n    *ii. Outlier Detection*\n    \n- Deal with Outliers\n    \n    *i. Visualize Zscore Tresholds (how many times IQR) by Continuous Variables*\n    \n    *ii. Drop Outliers*\n\n\n#### 3. Feature Engineering with sqlite3 Module\n\n\n#### 4. Prediction (Multi-class Classification)\n- Import libraries\n- Data Preprocessing\n- Implement KNN Classifer\n- Implement Decision Tree Classifier\n- Implement Random Forest Classifer\n- Implement XGBoost Classifer\n- Implement SVM Classifer\n- Compare The Models\n\n","3ff5edcd":"#### *ii. Outlier Detection*\n\nThe columns which have continuous value should be examined in terms of [outliers](https:\/\/datascience.foundation\/sciencewhitepaper\/knowing-all-about-outliers-in-machine-learning) (Watch out for columns that look like continuous but not continuous!). Some algorithms are [sensitive to outliers](https:\/\/arsrinevetha.medium.com\/ml-algorithms-sensitivity-towards-outliers-f3862a13c94d), but some algorithms can tolerate them. You can decide to outlier detection according to the algorithm you will use.\n- You can check the outliers shape of continous features with respect to the target (Cover_Type) classes.\n- You can check how many outliers are there of each continuous variables.","edf9718a":"**Visualization of Confusion Matrix with Countplot**","061aabfb":"# ``2.Random Forest Classifier``\n- Import the modul \n- Fit the model \n- Predict the test set\n- Visualize and evaluate the result (use yellowbrick module)","ad18fd3a":"* default degerlerle model daha basarili cikti.","543f0dc2":"**Evaluate the Performance**","ad66eeb3":"## Feature Engineering with *sqlite3* Library\nFeature engineering is an optional process to increase the predictive success of the model. The effort you put in should be worth increasing success. So you can develop your own feature engineering approach.\n\nNote that, after seeing the result of the models, there may be a possibility of making minor changes to the features in the modeling phase.\n\nYou are requested to do feature engineering operations with SQL. There are two ways to do this:\n1. After moving the final version of your data set to ***SQLite Browser*** and performing feature engineering operations there, you can convert the resulting data set to dataframe format and use it again in python.\n2. In Python, you can create a database and table with your data set by using the functions of the sqlite3 library, after performing feature engineering with SQL, you can convert the resulting data set to a dataframe.\n\nIn this case, we will illustrate the second method.\n\nFollow the steps below to do feature engineering with the [sqlite3](https:\/\/docs.python.org\/3\/library\/sqlite3.html) library:\n 1. Import *sqlite3* library\n 2. Create a sqlite database (``\"tree_database\"``) and transferring dataframe(``tree1``) from python into database table (``covtype2``)\n  - You can use *connect(), to_sql() and read_sql_query()* functions.\n 3. Assign SQL codes for feature engineering to an object. (produce or transform new columns, get rid of unnecassary columns, make the dataset ready to model)\n 4. Transform final version of SQL table to dataframe.\n \nFinally, you can save the final version of your data as csv for use in your models and work on a different notebook. On the other hand, you can continue to work on this notebook with the last dataframe.","0b652e0e":"**Evaluate the performance**","19739aae":"> **Wilderness - Soil**","9677c712":"**Visualization of Confusion Matrix with Countplot**","5d4bea4c":"**Tunning XGBOOST**","e8761955":"#### *ii. Load Dataset*","154b0e55":"---\n---","9c6d5068":"---\n---","7cde67a3":"**Tunning KNN with GridSearchCV**","2c3aa2e5":"Welcome to \"***Tree Types Prediction Project***\".\n\nIn this project, you must perform EDA processes for implementing predictive models. Handling outliers, domain knowledge and feature engineering (using ***sqlite3*** library) will be challenges. \n\nAlso, this project aims to improve your ability to implement algorithms for ***Multi-Class Classification***. Thus, you will have the opportunity to implement many algorithms commonly used for Multi-Class Classification problems.\n\nBefore diving into the project, please take a look at the determines and tasks.\n\n- ***NOTE:*** *This project assumes that you already know the basics of coding in Python. You should also be familiar with the theory behind classification models and scikit-learn module as well as Machine Learning before you begin.*","9e01815f":"---\n---","00b48d57":"#### *i. Import Libraries*","2f018c2c":"### Deal with Outliers","ec0f0641":"**Visualization of Confusion Matrix with Table**","62e1d77f":"**Visualization of Confusion Matrix with Table**","31095cb9":"> **Elevation - Cover_Type**","c4c2c17f":"**Make a Prediction**","9c717c5a":"- Drop target variable\n- Train-Test Split\n\n*Note: You can use the train and test data generated here for all algorithms.*","71b726c3":"> **Horizontal_Distance_To_Hydrology - Vertical_Distance_To_Hydrology**","7ad9a030":"> **Soil - Cover_Type**","49eb5bc3":"> **Horizontal_Distance_To_Fire_Points - Cover_Type**","64fa6329":"**Visualization of Confusion Matrix with Table**","a742d8f7":"> **Wilderness - Cover_Type**","00ec0abb":"## 1. Exploratory Data Analysis","f9621b76":"**Visualization of Confusion Matrix with Table**","b4b65d61":"### `Wilderness_Area`\n![image-2.png](attachment:image-2.png)\n\n### `Cover_Type`\n![image.png](attachment:image.png)","2673534f":"**Evaluate the performance**","926cf6e0":"> **Slope - Cover_Type**","c8bc434b":"#### *iii. Explore Data*\n- Focus on numerical and categorical data\n- Detect Number of Unique values of each column\n- Focus on Target Variable (Cover_Type)\n - Detect relationships and correlations between independent variables and target variable.\n - It may be nice to visualize the class frequencies of the target variable.\n- Detect relationships and correlations between independent variables. (You can prefer to keep only one of the highly correlated continuous variables.)\n- Consider dropping features that contain little data or that you think will not contribute to the model.","6b0d63e5":"**Visualization of Confusion Matrix with Countplot**","d5e878c4":"#### *i. Missing Value Detection*","a27f1a34":"* Target Label is unbalanced. ","2769af8e":"**Visualization of Confusion Matrix with Table**","374b181a":"**Visualization of Classification Report**","8ca39d92":"### Import Libraries","27da582d":"# ``4.Decision Tree Classifier``\n- Import the modul \n- Fit the model \n- Predict the test set\n- Visualize and evaluate the result (use yellowbrick module)","7409aaa0":"Besides Numpy and Pandas, you need to import the necessary modules for data visualization, data preprocessing, Model building and tuning.\n\n*Note: Check out the course materials.*","67e31a06":"* We used different outlier detection methods. As a result, we decided to determine outliers with winsorize.","1c4005da":"***\n***","0a56180f":"**Evaluate the Performance**","90fe058d":"* Wilderness - Soil\n* Horizontal_Distance_To_Hydrology - Vertical_Distance_To_Hydrology\n* Soil - Cover_Type\n* Wilderness - Cover_Type\n* Elevation - Cover_Type\n* Slope - Cover_Type\n* Horizontal_Distance_To_Roadways - Cover_Type\n* Horizontal_Distance_To_Fire_Points - Cover_Type","aa17123d":"### Standarization","0fda26f3":"### Conclusion\n* Finally we trained 5 different Classification models and we got the best F1 Score,Accuracy and Recall of 0.88 for XGBoost Classifier. Now we will train the model with all dataset.","e7901496":"If you have done, use your data set resulting from Feature Engineering task. If you haven't done Feature Engineering, use the latest version of your data set.\nIn this section, you have two main tasks that apply to each algorithm:\n1. Model Building and Prediction\n\n - XGBoost (Use ``XGBClassifier`` model from``xgboost`` module)\n - SVM (Use ``LinearSVC`` model from``sklearn.svm`` module)\n - Decision Tree (Use ``DecisionTreeClassifier`` model from ``sklearn.tree`` module)\n - KNN (Use ``KNeighborsClassifier`` model from ``sklearn.neighbors`` module)\n - Random Forest (Use ``RandomForestClassifier`` model from ``sklearn.ensemble`` module) \n \n\n2. Visualizing the Result\n\n- Use [yellowbrick](https:\/\/www.scikit-yb.org\/en\/latest\/), [seaborn](https:\/\/seaborn.pydata.org\/tutorial\/regression.html) or [matplotlib](https:\/\/matplotlib.org\/) modules to visualize the model results.\n\n- Show three plots for the results:\n - Class Prediction Error Bar Plot\n - Confusion Matrix\n - Classification Report","82a580a8":"### Data Preprocessing","54274c6d":"#### *i. Visualize Zscore Tresholds (how many times IQR) by Continuous Variables*","a6020f44":"**Saving Model**","fb23873f":"# #Tasks","4d457fc8":"### My Plan of Feature Extraction\n- First, I decided to produce&transform a new column with ``Horizontal_Distance_To_Hydrology`` and ``Vertical_Distance_To_Hydrology`` columns. New column will contain the values of **Hypotenuse** of ``horizantal`` and ``vertical`` distances.\n\n- As second, we can produce&transform an additional column which contains **average** of Horizantal Distances to Hydrology and Roadways.\n\n- Third, I decided to transform a new column which contains **average** of `Elevation` and ``Vertical_Distance_To_Hydrology`` columns. So that, there is no need to have ``Horizontal_Distance_To_Hydrology`` and ``Vertical_Distance_To_Hydrology`` columns, because I have new columns which represent more value than them. I decide to drop these columns.\n\n- Lastly, I will drop unnecessary columns ``'Hillshade_3pm', 'Soil_Type7', 'Soil_Type8', 'Soil_Type14', 'Soil_Type15', 'Soil_Type21', 'Soil_Type25', 'Soil_Type28', 'Soil_Type36', 'Soil_Type37'``) that I conclued previously.\n\n- Note that, after seeing the result of the models, there may be a possibility of making minor changes to the features in the modeling phase.","cdee2dda":"xgb_tuned = XGBClassifier(learning_rate= 0.3, \n                                max_depth= 15, \n                                n_estimators= 200, \n                                subsample= 0.7).fit(X, y)","f300b6cd":"**`Cross Validation Scores`**","311e24ee":"### `Comparison of Accuracies & F1 Scores & Recall`\nSo far, you have created a multi-classifier model with 5 different algorithms and made predictions. You can observe the performance of the models together with a barplot of your choice.\n\n- Which algorithm did you achieve the highest prediction performance with? \n- What could be the factors that cause this? What are the principles of the most successful algorithm and its differences from other algorithms? \n\nIn contrast;\n\n- Which algorithm did you achieve the lowest prediction performance with? \n- What could be the factors that cause this? What are the principles of the most successful algorithm and its differences from other algorithms? \n\nThe answers you will look for to these questions will increase your gains from Machine Learning course.","9353eaa3":"# ``3.KNeighborsClassifer``\n\nThe first and most important step for the [KNN](https:\/\/www.analyticsvidhya.com\/blog\/2018\/03\/introduction-k-neighbours-algorithm-clustering\/) algorithm is to determine the optimal k (number of neighbors). \n\nBuild different models with k values in the range you specify. You can observe the change of train and test accuracy values according to different k values using a plot. The point at which train accuracy and test accuracy values begin to run parallel is the optimal k value. Then set up your final KNN model with the optimal k value you determined and calculate accuracy.\n\n- Import the modul\n- Fit the model \n- Predict the test set\n- Visualize the result\n- Evaluate the performance","5cf71f99":"**Find Optimum K Value with Elbow Method**","5669c1b5":"# ``5. Support Vector Machine``\n- Import the modul \n- Fit the model \n- Predict the test set\n- Visualize and evaluate the result (use yellowbrick module)\n\n*Note: You probably won't get a successful result. You may need to make some changes to the model or data. This may be a topic worth investigating, you decide.*","be378acd":"Dataset contains tree observations from four areas of one national forest district. This dataset includes information on tree type, shadow coverage, distance to nearby landmarks, soil type, and local topography. The goal of the project is to build a model that predicts what types of trees grow in an area.\n***The Forest Dataset*** contains approximately 600 thousand lines, also you can easily find many information about it on the web (especially Kaggle).\n\n---\n\nTo achieve high prediction success, you must understand the data well and develop different approaches that can affect the dependent variable.\n\nFirstly, try to understand the dataset column by column using pandas module. Do research within the scope of domain (forest, trees) knowledge on the internet to get to know the data set in the fastest way. \n\nYou should implement cleaning, handling with outliers and missing values using Pandas, NumPy and other required modules for the best result in modeling. You should do Feature Engineering using SQLite local database. \n\nAt this point, in order to improve your skills of using SQL with Python, you are asked to perform feature engineering operations using *sqlite3* library in Python.\n\nAfter that, your final dataset with the new variables you have created will be ready for model building. You will implement ***Support Vector Machine, XGBoost, Random Forest, Desicion Tree*** and ***K-Nearest Neighbors (KNN)*** algorithms. Also, evaluate the success of your models with appropriate performance metrics and visualize them using ***Yellowbrick, Seaborn*** or ***Matplotlib*** modules.\n\nAt the end of the project, create a chart comparing the performance of all models and choose the most successful model.\n\n- ***NOTE:*** *Evaluate your models knowing that this is [imbalanced data](https:\/\/towardsdatascience.com\/having-an-imbalanced-dataset-here-is-how-you-can-solve-it-1640568947eb). This is not the primary goal of the project, but you can study solve the class [imbalance problem](https:\/\/towardsdatascience.com\/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28) if you want.*\n\n","96d0df08":"**Visualize Accuracies of Train & Test Data by Different k`s**"}}