{"cell_type":{"e816b529":"code","4bae5fdb":"code","fc5263ff":"code","8df3e1ce":"code","d34ea9b0":"code","5c44e75c":"code","eb4cd71f":"code","d9c71dc0":"code","402b029f":"code","06e0a4f3":"code","a4a6adfc":"code","2ea9c7ab":"code","e99b3b03":"code","8d3013e2":"code","bab5e88c":"code","6dcf4644":"code","6f77672f":"code","d22ac285":"code","235e62c2":"code","b7b6b1f0":"code","27e5dbc0":"code","3d050f62":"code","597903f1":"code","2ab9cad0":"code","7c7a826c":"code","4fcee793":"code","f8f211d0":"code","aff84c5b":"code","d4bf40ea":"code","ec6b9116":"code","b91356f6":"code","5287fab2":"code","49d7b6bb":"code","19afec37":"code","2c180d9f":"code","7ec71cbe":"code","c0fb6b58":"code","48c80a27":"code","cf907a5d":"code","02ad2ee0":"code","940dba0f":"code","d89fd357":"code","278565cd":"code","8faecda1":"markdown","1c8fb76c":"markdown","e8033a0d":"markdown","0e840484":"markdown","398e69d0":"markdown","12dfea38":"markdown","40995861":"markdown","b702c5e2":"markdown","cdf13ffe":"markdown","8e702bbc":"markdown","ae0daa2a":"markdown","e87338de":"markdown","ccbeff3d":"markdown"},"source":{"e816b529":"# importing some useful libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns  \nimport time \n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom nltk.tokenize import RegexpTokenizer  \nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer  \nfrom sklearn.pipeline import make_pipeline\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport pickle ","4bae5fdb":"# Loading the dataset\ndf= pd.read_csv(\"..\/input\/phishing-site-urls\/phishing_site_urls.csv\")","fc5263ff":"df.head()","8df3e1ce":"df.info()","d34ea9b0":"df.shape","5c44e75c":"df.isnull().sum()","eb4cd71f":"sns.countplot(x=\"Label\",data=df)","d9c71dc0":"tokenizer = RegexpTokenizer(r'[A-Za-z]+')","402b029f":"tokenizer.tokenize(df.URL[0]) # this will fetch all the words from the first URL","06e0a4f3":"# Tokenizing all the rows \nprint('Getting words tokenized ...')\nt0= time.perf_counter()\ndf['text_tokenized'] = df.URL.map(lambda t: tokenizer.tokenize(t))\nt1 = time.perf_counter() - t0\nprint('Time taken',t1 ,'sec')","a4a6adfc":"df.sample(5)","2ea9c7ab":"stemmer = SnowballStemmer(\"english\") # choose a language","e99b3b03":"# Getting all the stemmed words\nprint('Getting words stemmed ...')\nt0= time.perf_counter()\ndf['text_stemmed'] = df['text_tokenized'].map(lambda l: [stemmer.stem(word) for word in l])\nt1= time.perf_counter() - t0\nprint('Time taken',t1 ,'sec')","8d3013e2":"df.sample(5)","bab5e88c":"# Joining all the stemmmed words.\nprint('Get joiningwords ...')\nt0= time.perf_counter()\ndf['text_sent'] = df['text_stemmed'].map(lambda l: ' '.join(l))\nt1= time.perf_counter() - t0\nprint('Time taken',t1 ,'sec')","6dcf4644":"bad_sites = df[df.Label == 'bad']\ngood_sites = df[df.Label == 'good']","6f77672f":"bad_sites.head()","d22ac285":"good_sites.head()","235e62c2":"df.head()","b7b6b1f0":"cv = CountVectorizer()","27e5dbc0":"feature = cv.fit_transform(df.text_sent) #transform all text which we tokenize and stemed","3d050f62":"feature[:5].toarray() # convert sparse matrix into array to print transformed features","597903f1":"from sklearn.model_selection import train_test_split\n","2ab9cad0":"trainX, testX, trainY, testY = train_test_split(feature, df.Label)","7c7a826c":"from sklearn.linear_model import LogisticRegression\n","4fcee793":"lr = LogisticRegression()\nlr.fit(trainX,trainY)","f8f211d0":"lr.score(testX,testY)","aff84c5b":"Scores_ml = {}\nScores_ml['Logistic Regression'] = np.round(lr.score(testX,testY),2)","d4bf40ea":"# creating confusing matrix\nprint('Training Accuracy :',lr.score(trainX,trainY))\nprint('Testing Accuracy :',lr.score(testX,testY))\ncon_mat = pd.DataFrame(confusion_matrix(lr.predict(testX), testY),\n            columns = ['Predicted:Bad', 'Predicted:Good'],\n            index = ['Actual:Bad', 'Actual:Good'])\n\n\nprint('\\nCLASSIFICATION REPORT\\n')\nprint(classification_report(lr.predict(testX), testY,\n                            target_names =['Bad','Good']))\n\nprint('\\nCONFUSION MATRIX')\nplt.figure(figsize= (6,4))\nsns.heatmap(con_mat, annot = True,fmt='d',cmap=\"YlGnBu\")","ec6b9116":"from sklearn.naive_bayes import MultinomialNB \n","b91356f6":"# create mnb object\nmnb = MultinomialNB()","5287fab2":"mnb.fit(trainX,trainY)","49d7b6bb":"mnb.score(testX,testY)","19afec37":"Scores_ml['MultinomialNB'] = np.round(mnb.score(testX,testY),2)","2c180d9f":"print('Training Accuracy :',mnb.score(trainX,trainY))\nprint('Testing Accuracy :',mnb.score(testX,testY))\ncon_mat = pd.DataFrame(confusion_matrix(mnb.predict(testX), testY),\n            columns = ['Predicted:Bad', 'Predicted:Good'],\n            index = ['Actual:Bad', 'Actual:Good'])\n\n\nprint('\\nCLASSIFICATION REPORT\\n')\nprint(classification_report(mnb.predict(testX), testY,\n                            target_names =['Bad','Good']))\n\nprint('\\nCONFUSION MATRIX')\nplt.figure(figsize= (6,4))\nsns.heatmap(con_mat, annot = True,fmt='d',cmap=\"YlGnBu\")","7ec71cbe":"# Lets compare the two models and find out which one is best.\nacc = pd.DataFrame.from_dict(Scores_ml,orient = 'index',columns=['Accuracy'])\nsns.set_style('darkgrid')\nsns.barplot(acc.index,acc.Accuracy)","c0fb6b58":"pipeline_ls = make_pipeline(CountVectorizer(tokenizer = RegexpTokenizer(r'[A-Za-z]+').tokenize,stop_words='english'), LogisticRegression())","48c80a27":"trainX, testX, trainY, testY = train_test_split(df.URL, df.Label)","cf907a5d":"pipeline_ls.fit(trainX,trainY)","02ad2ee0":"pipeline_ls.score(testX,testY)","940dba0f":"print('Training Accuracy :',pipeline_ls.score(trainX,trainY))\nprint('Testing Accuracy :',pipeline_ls.score(testX,testY))\ncon_mat = pd.DataFrame(confusion_matrix(pipeline_ls.predict(testX), testY),\n            columns = ['Predicted:Bad', 'Predicted:Good'],\n            index = ['Actual:Bad', 'Actual:Good'])\n\n\nprint('\\nCLASSIFICATION REPORT\\n')\nprint(classification_report(pipeline_ls.predict(testX), testY,\n                            target_names =['Bad','Good']))\n\nprint('\\nCONFUSION MATRIX')\nplt.figure(figsize= (6,4))\nsns.heatmap(con_mat, annot = True,fmt='d',cmap=\"YlGnBu\")","d89fd357":"pickle.dump(pipeline_ls,open('phishing.pkl','wb'))","278565cd":"loaded_model = pickle.load(open('phishing.pkl', 'rb'))\nresult = loaded_model.score(testX,testY)\nprint(result)","8faecda1":"### LogisticRegression\n#### Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). In other words, the logistic regression model predicts P(Y=1) as a function of X.","1c8fb76c":"### SnowballStemmer\n#### Snowball is a small string processing language that gives the root words","e8033a0d":"### About data\n#### It consist 549346 rows and 2 columns .The first column consist of links of website and the second column states whether the site is good or bad(phishing)","0e840484":"#### Now we can Vectoize the URLs.We can gather words from the URLs using Tokenizer\n### RegexpTokenizer\n#### we are able to extract the tokens from string by using regular expression with RegexpTokenizer() method.","398e69d0":"#### Thats it. Now the pkl file is deployed into Heroku and can be used to create an app.\n#### If you like the Notebook , do upvote.","12dfea38":"### Lets dump the model in pickle.","40995861":"#### MultinomialNB provide 95% accuracy,so we can store the score in the dictionary","b702c5e2":"#### Logistic Regression provide 96% accuracy,Now we will store the score in the dictionary so that we can find which model performs the best.\n","cdf13ffe":"## Creating Model\n### CountVectorizer- Convert a collection of text documents to a matrix of token counts","8e702bbc":"## MultinomialNB\n#### The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.","ae0daa2a":"\n#### Phishing is a method of trying to gather personal information like login credentials or credit card information using deceptive e-mails or  websites.\n\n#### Phishing websites are created to dupe unsuspecting users into thinking they are on a legitimate site. The criminals will spend a lot of time making the site seem as credible as possible and many sites will appear almost indistinguishable from the real thing","e87338de":"#### So, Logistic Regression is the best fit model, Now lets make sklearn pipeline using Logistic Regression","ccbeff3d":"### PREPROCESSING"}}