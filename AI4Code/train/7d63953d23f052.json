{"cell_type":{"07013e47":"code","0a9819cb":"code","7a96514a":"code","bdac306d":"code","bece5fdf":"code","631d7903":"code","e2747abe":"code","2a08a086":"code","0fcb216e":"code","881ef093":"code","88061a95":"code","65a1ae06":"code","6b511861":"code","32688987":"code","ca32f645":"code","7ea2a80d":"code","742af8f7":"code","22969a9a":"code","9a808a72":"code","464a3679":"code","86620e2c":"markdown","9546b2d7":"markdown","4359bb5d":"markdown","42b948b5":"markdown","05d18c70":"markdown","a3bce1e5":"markdown"},"source":{"07013e47":"import sys\n\nsys.path.insert(0, '..\/input\/computer-vision-training-fcis-21\/')\n\nimport utils\n\nworking_dir = '..\/input\/computer-vision-training-fcis-21\/224\/content\/flowers\/224\/'\n\ntrain_dir = f'{working_dir}\/train\/train\/'\ntest_dir = f'{working_dir}\/test\/test\/'","0a9819cb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7a96514a":"train_nbatch = utils.num_of_batches(train_dir + '\/origin')\n\nprint('No. Batches: %i'%train_nbatch)","bdac306d":"start = 0\nend =107\n\nxdata = utils.read_nbatch(train_dir + '\/origin', start, end)\nydata = utils.read_nbatch(train_dir + '\/masks', start, end)\n\nprint(xdata['x'].shape, ydata['x'].shape)","bece5fdf":"def plot(images: np.ndarray, nrows: int = 1, ncols: int = 4, figsize: tuple = (10, 6)):\n\n    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n    \n    axs = axs.flatten()\n\n    for i in range(len(axs)):\n\n        axs[i].imshow(images[i].squeeze())\n\n        axs[i].grid(None)\n        axs[i].axis('off')\n    \n    \n    fig.tight_layout()\n    \n    return fig, axs","631d7903":"def sample(images, sample_size=4, random_state=42):\n    \n    random = np.random.RandomState(random_state)\n\n    indices = np.arange(0, 320)\n\n    random.shuffle(indices)\n    \n    indices = indices[:sample_size]    \n    \n    return images[indices]","e2747abe":"x_sample = sample(xdata['x'])\ny_sample = sample(ydata['x'])\n\nprint(x_sample.shape, y_sample.shape)","2a08a086":"plt.style.use('grayscale')\n\nfig, ax = plot(x_sample)\nfig, ax = plot(y_sample)","0fcb216e":"import tensorflow as tf\n\nfrom tensorflow.keras.layers import Input, Dropout,AvgPool2D,concatenate,UpSampling2D,MaxPool2D,MaxPooling2D, Dense, Activation, BatchNormalization, Flatten, Reshape,Conv2D,Conv2DTranspose\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import optimizers, losses, metrics","881ef093":"inputs = Input(shape=(224, 224, 3))\n\n#high\n#d1=BatchNormalization()(inputs)\nd11=AvgPool2D((3,3),strides=(1,1),padding='same')(inputs)\nd11=Conv2D(32,(1,1),activation='relu', padding='same')(d11)\nd12=Conv2D(32,(1,1),activation='relu', padding='same')(inputs)\nd12=Conv2D(32,(5,5),activation='relu', padding='same')(d12)\nd13=Conv2D(32,(1,1),activation='relu', padding='same')(inputs)\nd13=Conv2D(32,(3,3),activation='relu', padding='same')(d13)\nd14=Conv2D(32,(1,1),activation='relu', padding='same')(inputs)\ndf1=concatenate([d11,d12,d13,d14, inputs], axis=3)\ndf1=BatchNormalization()(df1)\n\n\n#mid\nd2=MaxPool2D(pool_size=(2,2))(df1)\n\nd21=AvgPool2D((3,3),strides=(1,1),padding='same')(d2)\nd21=Conv2D(48,(1,1),activation='relu', padding='same')(d21)\n\nd22=Conv2D(48,(1,1),activation='relu', padding='same')(d2)\nd22=Conv2D(32,(5,5),activation='relu', padding='same')(d22)\n\nd23=Conv2D(48,(1,1),activation='relu', padding='same')(d2)\nd23=Conv2D(32,(3,3),activation='relu', padding='same')(d23)\n\nd24=Conv2D(48,(1,1),activation='relu', padding='same')(d2)\ndf2=concatenate([d21,d22,d23,d24], axis=3)\n#df2=BatchNormalization()(df2)\nd3=MaxPool2D(pool_size=(2,2))(df2)\n\n#low\n\nd31=AvgPool2D((3,3),strides=(1,1),padding='same')(d3)\nd31=Conv2D(64,(1,1),activation='relu', padding='same')(d31)\n\nd32=Conv2D(64,(1,1),activation='relu', padding='same')(d3)\nd32=Conv2D(64,(5,5),activation='relu', padding='same')(d32)\n\nd33=Conv2D(64,(1,1),activation='relu', padding='same')(d3)\nd33=Conv2D(64,(3,3),activation='relu', padding='same')(d33)\n\nd34=Conv2D(64,(1,1),activation='relu', padding='same')(d3)\n\ndf3=concatenate([d31,d32,d33,d34], axis=3)\n#df3=BatchNormalization()(df3)\nd4=MaxPool2D(pool_size=(2,2))(df3)\nd4=Conv2D(128,(3,3),activation='relu', padding='same')(d4)\nd4=Conv2D(128,(3,3),activation='relu', padding='same')(d4)\nd4=Conv2D(128,(3,3),activation='relu', padding='same')(d4)\n#d4=BatchNormalization()(d4)\n#center\nc=MaxPooling2D(pool_size=(2,2))(d4)\nc=Dropout(0.50)(c)\nc=Conv2D(256,(3,3),activation='relu', padding='same')(c)\nc=Conv2D(256,(3,3),activation='relu', padding='same')(c)\nc=Conv2D(256,(3,3),activation='relu', padding='same')(c)\nc=BatchNormalization()(c)\n#low\n#p4=Conv2DTranspose(128,(3,3),activation='relu',padding='same')(c)\np4=UpSampling2D()(c)\np4=concatenate([d4, p4], axis=3)\np4=Conv2D(128,(3,3),activation='relu', padding='same')(p4)\np4=Conv2D(128,(3,3),activation='relu', padding='same')(p4)\n\n\n#p3=Conv2DTranspose(64,(3,3),activation='relu',padding='same')(p4)\np3=UpSampling2D()(p4)\np3=concatenate([df3, p3], axis=3)\np3=Conv2D(64,(3,3),activation='relu', padding='same')(p3)\np3=Conv2D(64,(3,3),activation='relu', padding='same')(p3)\n\n#mid\n\n#p2=Conv2DTranspose(32,(3,3),activation='relu',padding='same')(p3)\np2=UpSampling2D()(p3)\np2=concatenate([df2, p2], axis=3)\np2=Conv2D(32,(3,3),activation='relu', padding='same')(p2)\np2=Conv2D(32,(3,3),activation='relu', padding='same')(p2)\n\n#high\n\n#p1=Conv2DTranspose(32,(3,3),activation='relu',padding='same')(p2)\np1=UpSampling2D()(p2)\np1=concatenate([df1, p1], axis=3)\np1=Conv2D(32,(3,3),activation='relu', padding='same')(p1)\np1=Conv2D(32,(3,3),activation='relu', padding='same')(p1)\n\n\nclassify = Conv2D(1, (1, 1), activation='sigmoid')(p1)\nmodel = Model(inputs=inputs, outputs=classify)\noptimizer = optimizers.Adam(learning_rate=1e-3)\nloss = losses.BinaryCrossentropy('loss')\nmetric = metrics.MeanIoU(2, name='iou')\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metric)","88061a95":"model.evaluate(xdata['x'], ydata['x'])","65a1ae06":"model.summary()","6b511861":"model.fit(xdata['x'], ydata['x'], epochs=25, batch_size=32)","32688987":"test_nbatches = utils.num_of_batches(test_dir + '\/origin')\n\nprint('No. Batches: %i'%test_nbatches)","ca32f645":"test_data = utils.read_nbatch(test_dir + 'origin\/', 108, 108 + test_nbatches)\n\ntest_size = len(test_data['id'])\n\nprint(test_size)","7ea2a80d":"test_ids = test_data['id']\n\ntest_masks = model.predict(test_data['x']) > 0.5","742af8f7":"x_sample = sample(test_data['x'])\ny_sample = sample(test_masks)","22969a9a":"plt.style.use('grayscale')\n\nfig, ax = plot(x_sample)\nfig, ax = plot(y_sample)","9a808a72":"submission = utils.Submission(shape=(224, 224))\n\nsubmission.init()\n\nfor i in tqdm(range(test_size)):\n    \n    submission.update(test_ids[i], test_masks[i])","464a3679":"submission.export('submission.csv')","86620e2c":"# Model","9546b2d7":"# Read N-Batches","4359bb5d":"# Explore","42b948b5":"# Import Modules","05d18c70":"# Predict","a3bce1e5":"# Submission"}}