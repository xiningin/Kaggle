{"cell_type":{"cdc399b6":"code","7ec93afb":"code","3b42f306":"code","8a53430d":"code","ddd83697":"code","b8146b85":"code","ab1f4de3":"code","03be8ccb":"code","41874590":"code","4bf91f18":"code","db6dc20c":"code","78edc95f":"code","b5579009":"code","eb2e8c92":"code","23f078d3":"code","53f5e3ea":"code","eccd3db4":"code","54567a8b":"markdown"},"source":{"cdc399b6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7ec93afb":"pip install transformers==4.0.0rc1","3b42f306":"from transformers import MT5ForConditionalGeneration, AutoTokenizer","8a53430d":"import time\nimport torch","ddd83697":"from transformers import MT5ForConditionalGeneration, AutoTokenizer\nmodel = MT5ForConditionalGeneration.from_pretrained(\"..\/input\/anti-romanization\")\n#\"Romanization: %s\"\ntokenizer = AutoTokenizer.from_pretrained(\"google\/mt5-base\")\n","b8146b85":"article = '''Romanization:\u092e\u0947\u0930\u093e \u0928\u093e\u092e \u092a\u093e\u0930\u094d\u0925 \u0939\u0948 langauge: Hindi'''","ab1f4de3":"def greedy_decoding (inp_ids,attn_mask):\n    greedy_output = model.generate(input_ids=inp_ids, attention_mask=attn_mask, max_length=256)\n    Question =  tokenizer.decode(greedy_output[0], skip_special_tokens=True,clean_up_tokenization_spaces=True)\n    return Question.strip().capitalize()","03be8ccb":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print (\"device \",device)\nmodel = model.to(device)","41874590":"start = time.time()\nencoding = tokenizer.encode_plus(article, return_tensors=\"pt\")\ninput_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\nprint(article)\noutput = greedy_decoding(input_ids,attention_masks)\nprint (\"Greedy decoding:: \\n \",output)\nend = time.time()\nprint (\"\\nTime elapsed \", end-start)\nprint (\"\\n\")","4bf91f18":"output","db6dc20c":"def top_kp(inp_ids, attn_mask):\n    ''' To generate multiple output for same input we are using top-k encoding\n    '''\n    topkp_output = model.generate(input_ids=inp_ids,\n                                       attention_mask=attn_mask,\n                                       do_sample=True,\n                                       max_length=30,\n                                       top_p=0.84,\n                                       top_k=80,\n                                       num_return_sequences=10,\n                                       min_length=3,\n                                       temperature=0.9,\n                                       repetition_penalty=1.2,\n                                       length_penalty=1.5,\n                                       no_repeat_ngram_size=2,\n                                       )\n    Questions = [tokenizer.decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True) for out in\n                 topkp_output]\n    return [Question.strip().capitalize() for Question in Questions]\n\ndef t5_topkp(input_text):\n    '''\n    \n    '''\n    con = \"Hindi context: %s\" % (input_text)\n    encoding = tokenizer.encode_plus(con, return_tensors=\"pt\")\n    input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n    output = top_kp(input_ids, attention_masks)\n    return output","78edc95f":"start = time.time()\nencoding = tokenizer.encode_plus(article, return_tensors=\"pt\")\ninput_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\nprint(article)\noutput = greedy_decoding(input_ids,attention_masks)\nprint (\"Greedy decoding:: \\n \",output)\nend = time.time()\nprint (\"\\nTime elapsed \", end-start)\nprint (\"\\n\")","b5579009":"start = time.time()\n\nprint(article)\noutput = t5_topkp(article)\nprint (\"Topkp decoding:: \\n \")\nfor i in range(len(output)):\n    print(output[i])\n    print (\"\\n\")\nend = time.time()\nprint (\"\\nTime elapsed \", end-start)\nprint (\"\\n\")","eb2e8c92":"def top_beam(inp_ids, attn_mask):\n    ''' To generate multiple output for same input we are using top-k encoding\n    '''\n    topkp_output = model.generate(input_ids=inp_ids,\n                                       attention_mask=attn_mask,\n                                       do_sample=True,\n                                       max_length=100,\n                                   num_beams=10, \n                                  temperature=0.9,\n                                       repetition_penalty=1.5,\n                                       length_penalty=1.5,\n    num_return_sequences=5, \n    early_stopping=True\n                                       \n                                       )\n    Questions = [tokenizer.decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True) for out in\n                 topkp_output]\n    return [Question.strip().capitalize() for Question in Questions]\n\ndef t5_topbeam(input_text):\n    '''\n    \n    '''\n    con = \"Hindi context: %s\" % (input_text)\n    encoding = tokenizer.encode_plus(con, return_tensors=\"pt\")\n    input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n    output = top_beam(input_ids, attention_masks)\n    return output","23f078d3":"start = time.time()\n\nprint(article)\noutput = t5_topbeam(article)\nprint (\"Topkp decoding:: \\n \")\nfor i in range(len(output)):\n    print(output[i])\n    print (\"\\n\")\nend = time.time()\nprint (\"\\nTime elapsed \", end-start)\nprint (\"\\n\")","53f5e3ea":"article = \"\u0915\u094d\u0930\u093f\u0936\u094d\u091a\u093f\u092f\u0928 \u0938\u092e\u0941\u0926\u093e\u092f \u0915\u0947 \u0932\u094b\u0917 \u0939\u0930 \u0938\u093e\u0932 25 \u0926\u093f\u0938\u0902\u092c\u0930 \u0915\u0947 \u0926\u093f\u0928 \u0915\u094d\u0930\u093f\u0938\u092e\u0938 \u0915\u093e \u0924\u094d\u092f\u094b\u0939\u093e\u0930 \u092e\u0928\u093e\u0924\u0947 \u0939\u0948\u0902\u0964 \u0915\u094d\u0930\u093f\u0938\u092e\u0938 \u0915\u093e \u0924\u094d\u092f\u094b\u0939\u093e\u0930 \u0908\u0938\u093e \u092e\u0938\u0940\u0939 \u0915\u0947 \u091c\u0928\u094d\u092e\u0926\u093f\u0928 \u0915\u0947 \u0930\u0942\u092a \u092e\u0947\u0902 \u092e\u0928\u093e\u092f\u093e \u091c\u093e\u0924\u093e \u0939\u0948\u0964 \u0915\u094d\u0930\u093f\u0938\u092e\u0938 \u0915\u094d\u0930\u093f\u0936\u094d\u091a\u093f\u092f\u0928 \u0938\u092e\u0941\u0926\u093e\u092f \u0915\u093e \u0938\u092c\u0938\u0947 \u092c\u0921\u093c\u093e \u0914\u0930 \u0916\u0941\u0936\u0940 \u0915\u093e \u0924\u094d\u092f\u094b\u0939\u093e\u0930 \u0939\u0948, \u0907\u0938 \u0915\u093e\u0930\u0923 \u0907\u0938\u0947 \u092c\u0921\u093c\u093e \u0926\u093f\u0928 \u092d\u0940 \u0915\u0939\u093e \u091c\u093e\u0924\u093e \u0939\u0948\u0964\"","eccd3db4":"start = time.time()\n\nprint(article)\noutput = t5_topbeam(article)\nprint (\"Topkp decoding:: \\n \")\nfor i in range(len(output)):\n    print(output[i])\n    print (\"\\n\")\n    \nend = time.time()\nprint (\"\\nTime elapsed \", end-start)\nprint (\"\\n\")","54567a8b":"## A bit harder"}}