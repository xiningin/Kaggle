{"cell_type":{"6456aea0":"code","7003c2ef":"code","fae4754c":"code","655078d2":"code","ebbd8899":"code","cc771eb4":"code","130258e8":"code","37666c44":"code","b1d272a3":"code","c0e263a3":"code","45803138":"code","8ea370f8":"code","33d970f3":"code","574f1ae8":"code","a0cd08f2":"code","8a632165":"code","37630485":"code","8fe4d9ee":"code","2972fc4d":"code","18d180fc":"code","06a25944":"code","9e313345":"code","e0d7dcd9":"code","d1ecfafe":"code","e4826160":"code","450cada3":"code","46fee01f":"code","05e6dbb8":"code","636f793c":"code","fb65677f":"code","c34cc248":"code","8008a941":"code","0b0f236d":"code","a60285f8":"code","37b947df":"code","6e591697":"code","3cd7252e":"code","69c050ec":"code","f5b7aaca":"code","46930e91":"code","ecf63784":"code","017813cf":"code","7a9145d1":"markdown","b2b366ae":"markdown","03d2adf5":"markdown","b275ae03":"markdown","3d5f4eb1":"markdown","215e416a":"markdown","56f0a3f7":"markdown","cd0fd792":"markdown","ff4a8028":"markdown","38f310e6":"markdown","1434d0c0":"markdown","bcd1ec40":"markdown","df56e1e8":"markdown","aed7a76e":"markdown","5a3c0221":"markdown"},"source":{"6456aea0":"%matplotlib inline\nimport cv2\nimport random\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nimport os","7003c2ef":"# types = []\n# for i in os.listdir(\"..\/input\/the-nature-conservancy-fisheries-monitoring\/train\")[:-1]:\n#     types.append(i.lower())\n# types.remove('other')\ntypes = ['alb', 'bet', 'dol', 'lag', 'nof', 'shark', 'yft']","fae4754c":"from sklearn.feature_extraction.text import CountVectorizer\ncount_v = CountVectorizer()","655078d2":"Types = count_v.fit_transform(types)","ebbd8899":"type_ = ['bet']\ncount_v.transform(type_).toarray()","cc771eb4":"import json\nfrom glob import glob\n\n# TODO: \u0441\u043a\u0430\u0447\u0430\u0439\u0442\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u0435 \u0432 \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u044e:\nTRAIN_PREFIX = '..\/input\/the-nature-conservancy-fisheries-monitoring\/train'\n\ndef load_boxes():\n    boxes = dict()\n    for path in glob('..\/input\/fish-json\/*.json'):\n        label = os.path.basename(path).split('_', 1)[0]\n        with open(path) as src:\n            boxes[label] = json.load(src)\n            for annotation in boxes[label]:\n                basename = os.path.basename(annotation['filename'])\n                annotation['filename'] = os.path.join(TRAIN_PREFIX, label.upper(), basename)\n            for annotation in boxes[label]:\n                for rect in annotation['annotations']:\n                    rect['x'] += rect['width'] \/ 2\n                    rect['y'] += rect['height'] \/ 2\n    return boxes\n\ndef draw_boxes(annotation, rectangles=None, image_size=None):\n    \n    def _draw(img, rectangles, scale_x, scale_y, color=(0, 255, 0)):\n        for rect in rectangles:\n            pt1 = (int((rect['x'] - rect['width'] \/ 2) * scale_x),\n                   int((rect['y'] - rect['height'] \/ 2) * scale_y))\n            pt2 = (int((rect['x'] + rect['width'] \/ 2) * scale_x),\n                   int((rect['y'] + rect['height'] \/ 2) * scale_y))\n            img = cv2.rectangle(img.copy(), pt1, pt2, \n                                color=color, thickness=4)\n        return img\n    \n    scale_x, scale_y = 1., 1.\n    \n    img = cv2.imread(annotation['filename'], cv2.IMREAD_COLOR)[...,::-1]\n    if image_size is not None:\n        scale_x = 1. * image_size[0] \/ img.shape[1]\n        scale_y = 1. * image_size[1] \/ img.shape[0]\n        img = cv2.resize(img, image_size)\n        \n    img = _draw(img, annotation['annotations'], scale_x, scale_y)\n    \n    if rectangles is not None:\n        img = _draw(img, rectangles, 1., 1., (255, 0, 0))\n\n    return img","130258e8":"boxes = load_boxes()  # \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0430 \u0434\u0435\u0442\u0435\u043a\u0446\u0438\u0439","37666c44":"boxes['bet'][4]","b1d272a3":"pd.DataFrame([(k, len(v)) for k, v in boxes.items()],\n             columns=['class', 'count'])","c0e263a3":"plt.figure(figsize=(6, 6), dpi=120)\nimg = draw_boxes(boxes['bet'][70])\nplt.imshow(img)\nplt.title('{}x{}'.format(*img.shape));","45803138":"annotations = sum([box['annotations']\n                  for box in sum(boxes.values(), [])], [])\n\nwidths = [rect['width'] for rect in annotations]\nheights = [rect['height'] for rect in annotations]\n\nplt.hist(widths)\nplt.hist(heights);","8ea370f8":"IMG_HEIGHT = 750\nIMG_WIDTH = 1200\n\nfeatures = keras.applications.vgg16.VGG16(include_top=False,\n                                          weights='imagenet',\n                                          input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n\nfeature_tensor = features.layers[-1].output\n\n# \u0434\u043e\u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0435 5 \u0441\u043b\u043e\u0435\u0432\nfor layer in features.layers[:-5]:\n    layer.trainable = False","33d970f3":"FEATURE_SHAPE = (feature_tensor.shape[1].value,\n                 feature_tensor.shape[2].value)\n\nGRID_STEP_H = IMG_HEIGHT \/ FEATURE_SHAPE[0]\nGRID_STEP_W = IMG_WIDTH \/ FEATURE_SHAPE[1]\n\nANCHOR_WIDTH = 150.\nANCHOR_HEIGHT = 150. \n\nANCHOR_CENTERS = np.mgrid[GRID_STEP_H\/2:IMG_HEIGHT:GRID_STEP_H,\n                          GRID_STEP_W\/2:IMG_WIDTH:GRID_STEP_W]","574f1ae8":"feature_tensor.shape","a0cd08f2":"def iou(rect, x_scale, y_scale, anchor_x, anchor_y,\n        anchor_w=ANCHOR_WIDTH, anchor_h=ANCHOR_HEIGHT):\n    \n    rect_x1 = (rect['x'] - rect['width'] \/ 2) * x_scale\n    rect_x2 = (rect['x'] + rect['width'] \/ 2) * x_scale\n    \n    rect_y1 = (rect['y'] - rect['height'] \/ 2) * y_scale\n    rect_y2 = (rect['y'] + rect['height'] \/ 2) * y_scale\n    \n    anch_x1, anch_x2 = anchor_x - anchor_w \/ 2, anchor_x + anchor_w \/ 2\n    anch_y1, anch_y2 = anchor_y - anchor_h \/ 2, anchor_y + anchor_h \/ 2\n    \n    dx = (min(rect_x2, anch_x2) - max(rect_x1, anch_x1))\n    dy = (min(rect_y2, anch_y2) - max(rect_y1, anch_y1))\n    \n    intersection = dx * dy if (dx > 0 and dy > 0) else 0.\n    \n    anch_square = (anch_x2 - anch_x1) * (anch_y2 - anch_y1)\n    rect_square = (rect_x2 - rect_x1) * (rect_y2 - rect_y1)\n    union = anch_square + rect_square - intersection\n    \n    return intersection \/ union\n\ndef encode_anchors(annotation, img_shape, iou_thr=0.5):\n    encoded = np.zeros(shape=(FEATURE_SHAPE[0],\n                              FEATURE_SHAPE[1], 5), dtype=np.float32)\n    x_scale = 1. * IMG_WIDTH \/ img_shape[1]\n    y_scale = 1. * IMG_HEIGHT \/ img_shape[0]\n    for rect in annotation['annotations']:\n        scores = []\n        for row in range(FEATURE_SHAPE[0]):\n            for col in range(FEATURE_SHAPE[1]):\n                anchor_x = ANCHOR_CENTERS[1, row, col]\n                anchor_y = ANCHOR_CENTERS[0, row, col]\n                score = iou(rect, x_scale, y_scale, anchor_x, anchor_y)\n                scores.append((score, anchor_x, anchor_y, row, col))\n        \n        scores = sorted(scores, reverse=True)\n        if scores[0][0] < iou_thr:\n            scores = [scores[0]]  # default anchor\n        else:\n            scores = [e for e in scores if e[0] > iou_thr]\n\n        for score, anchor_x, anchor_y, row, col in scores:\n            dx = (anchor_x - rect['x'] * x_scale) \/ ANCHOR_WIDTH\n            dy = (anchor_y - rect['y'] * y_scale) \/ ANCHOR_HEIGHT\n            dw = (ANCHOR_WIDTH - rect['width'] * x_scale) \/ ANCHOR_WIDTH\n            dh = (ANCHOR_HEIGHT - rect['height'] * y_scale) \/ ANCHOR_HEIGHT\n            encoded[row, col] = [1., dx, dy, dw, dh]\n            \n    return encoded\n\ndef _sigmoid(x):\n    return 1. \/ (1. + np.exp(-x))\n\ndef decode_prediction(prediction, conf_thr=0.1):\n    rectangles = []\n    for row in range(FEATURE_SHAPE[0]):\n        for col in range(FEATURE_SHAPE[1]):\n            logit, dx, dy, dw, dh = prediction[row, col]\n            conf = _sigmoid(logit)\n            if conf > conf_thr:\n                anchor_x = ANCHOR_CENTERS[1, row, col]\n                anchor_y = ANCHOR_CENTERS[0, row, col]\n                rectangles.append({'x': anchor_x - dx * ANCHOR_WIDTH,\n                                   'y': anchor_y - dy * ANCHOR_HEIGHT,\n                                   'width': ANCHOR_WIDTH - dw * ANCHOR_WIDTH,\n                                   'height': ANCHOR_HEIGHT - dh * ANCHOR_HEIGHT,\n                                   'conf': conf})\n    return rectangles","8a632165":"example = boxes['alb'][175]\n\nencoded = encode_anchors(example, (IMG_HEIGHT, IMG_WIDTH))\n\ndecoded = decode_prediction(encoded, conf_thr=0.5)\ndecoded = sorted(decoded, key = lambda e: -e['conf'])\n\nplt.figure(figsize=(6, 6), dpi=120)\nplt.imshow(draw_boxes(example, decoded[:10]))","37630485":"K = tf.keras.backend","8fe4d9ee":"def confidence_loss(y_true, y_pred):\n    conf_loss = K.binary_crossentropy(y_true[..., 0], \n                                      y_pred[..., 0],\n                                      from_logits=True)\n    return conf_loss\n\ndef smooth_l1(y_true, y_pred):\n    abs_loss = K.abs(y_true[..., 1:] - y_pred[..., 1:])\n    square_loss = 0.5 * K.square(y_true[..., 1:] - y_pred[..., 1:])\n    mask = K.cast(K.greater(abs_loss, 1.), 'float32')\n    total_loss = (abs_loss - 0.5) * mask + 0.5 * square_loss * (1. - mask)\n    return K.sum(total_loss, axis=-1)\n\ndef total_loss(y_true_1, y_pred_1, neg_pos_ratio=3):\n    batch_size = K.shape(y_true_1)[0]\n    \n    # TODO: \u0434\u043e\u0431\u0430\u0432\u044c\u0442\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u043f\u043e\u0442\u0435\u0440\u044c \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0434\u0435\u0442\u0435\u043a\u0446\u0438\u0438\n#     class_loss = K.categorical_crossentropy(y_true_2, y_pred_2)\n    \n    y_true_1 = K.reshape(y_true_1, (batch_size, -1, 5))\n    y_pred_1 = K.reshape(y_pred_1, (batch_size, -1, 5))\n\n    # confidence loss\n    conf_loss = confidence_loss(y_true_1, y_pred_1)\n    \n    # smooth l1 loss\n    loc_loss = smooth_l1(y_true_1, y_pred_1)\n    \n    # positive examples loss\n    pos_conf_loss = K.sum(conf_loss * y_true_1[..., 0], axis=-1)\n    pos_loc_loss = K.sum(loc_loss * y_true_1[..., 0], axis=-1)\n    \n    # negative examples loss\n    anchors = K.shape(y_true_1)[1]\n    num_pos = K.sum(y_true_1[..., 0], axis=-1)\n    num_pos_avg = K.mean(num_pos)\n    num_neg = K.min([neg_pos_ratio * (num_pos_avg) + 1., K.cast(anchors, 'float32')])\n    \n    # hard negative mining\n    neg_conf_loss, _ = tf.nn.top_k(conf_loss * (1. - y_true_1[..., 0]),\n                                   k=K.cast(num_neg, 'int32'))\n\n    neg_conf_loss = K.sum(neg_conf_loss, axis=-1)\n    \n    # total conf loss\n    total_conf_loss = (neg_conf_loss + pos_conf_loss) \/ (num_neg + num_pos + 1e-32)\n    loc_loss = pos_loc_loss \/ (num_pos + 1e-32)\n    \n    return total_conf_loss + 0.5 * loc_loss","2972fc4d":"def load_img(path, target_size=(IMG_WIDTH, IMG_HEIGHT)):\n    img = cv2.imread(path, cv2.IMREAD_COLOR)[...,::-1]\n    img_shape = img.shape\n    img_resized = cv2.resize(img, target_size)\n    return img_shape, keras.applications.vgg16.preprocess_input(img_resized.astype(np.float32))\n\ndef data_generator(boxes, batch_size=32):\n    boxes_ = sum(boxes.values(), [])\n    while True:\n        random.shuffle(boxes_)\n        for i in range(len(boxes_)\/\/batch_size):\n            X, y_1, y_2 = [], [], []\n            for j in range(i*batch_size,(i+1)*batch_size):\n                img_shape, img = load_img(boxes_[j]['filename'])\n                # TODO: \u0434\u043e\u0431\u0430\u0432\u044c\u0442\u0435 one-hot encoding \u0432 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0443 \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u043e\u0432\n                name = [boxes_[j]['filename'].split('\/')[-2].lower()]\n                y_2.append(count_v.transform(name).toarray().reshape(7))\n                y_1.append(encode_anchors(boxes_[j], img_shape))\n                X.append(img)\n            yield (np.array(X), {'box_output': np.array(y_1), 'category_output': np.array(y_2)})","18d180fc":"from tensorflow.keras.layers import Layer\n\nclass ROIPoolingLayer(Layer):\n    \"\"\" Implements Region Of Interest Max Pooling \n        for channel-first images and relative bounding box coordinates \"\"\"\n    def __init__(self, pooled_height, pooled_width, **kwargs):\n        self.pooled_height = pooled_height\n        self.pooled_width = pooled_width\n        \n        super(ROIPoolingLayer, self).__init__(**kwargs)\n        \n    def compute_output_shape(self, input_shape):\n        \"\"\" Returns the shape of the ROI Layer output\n        \"\"\"\n        feature_map_shape, rois_shape = input_shape # image and roi\n        assert feature_map_shape[0] == rois_shape[0]\n        batch_size = feature_map_shape[0]\n        n_rois = rois_shape[1]\n        n_channels = feature_map_shape[3]\n        return (batch_size, n_rois, self.pooled_height, \n                self.pooled_width, n_channels)\n\n    def call(self, x):\n        \"\"\" Maps the input tensor of the ROI layer to its output\"\"\"\n        \n        def curried_pool_rois(x): \n          return ROIPoolingLayer._pool_rois(x[0], x[1], \n                                            self.pooled_height, \n                                            self.pooled_width)\n        \n        pooled_areas = tf.map_fn(curried_pool_rois, x, dtype=tf.float32)\n\n        return pooled_areas\n    \n    @staticmethod\n    def _pool_rois(feature_map, rois, pooled_height, pooled_width):\n        \"\"\" Applies ROI pooling for a single image and varios ROIs\"\"\"\n        \n        def curried_pool_roi(roi): \n          return ROIPoolingLayer._pool_roi(feature_map, roi, \n                                           pooled_height, pooled_width)\n        \n        pooled_areas = tf.map_fn(curried_pool_roi, rois, dtype=tf.float32)\n        return pooled_areas\n    \n    @staticmethod\n    def _pool_roi(feature_map, roi, pooled_height, pooled_width):\n        \"\"\" Applies ROI pooling to a single image and a single region of interest\"\"\"\n\n        # Compute the region of interest        \n        feature_map_height = int(feature_map.shape[0])\n        feature_map_width  = int(feature_map.shape[1])\n        \n        h_start = tf.cast(feature_map_height * roi[0], 'int32')\n        w_start = tf.cast(feature_map_width  * roi[1], 'int32')\n        h_end   = tf.cast(feature_map_height * roi[2], 'int32')\n        w_end   = tf.cast(feature_map_width  * roi[3], 'int32')\n        \n        region = feature_map[h_start:h_end, w_start:w_end, :]\n        \n        # Divide the region into non overlapping areas\n        region_height = h_end - h_start\n        region_width  = w_end - w_start\n        h_step = tf.cast( region_height \/ pooled_height, 'int32')\n        w_step = tf.cast( region_width  \/ pooled_width , 'int32')\n        \n        areas = [[(\n                    i*h_step, \n                    j*w_step, \n                    (i+1)*h_step if i+1 < pooled_height else region_height, \n                    (j+1)*w_step if j+1 < pooled_width else region_width\n                   ) \n                   for j in range(pooled_width)] \n                  for i in range(pooled_height)]\n        \n        # take the maximum of each area and stack the result\n        def pool_area(x): \n          return tf.math.reduce_max(region[x[0]:x[2], x[1]:x[3], :], axis=[0,1])\n        \n        pooled_features = tf.stack([[pool_area(x) for x in row] for row in areas])\n        return pooled_features","06a25944":"def sigmoid_tf(x):\n    return 1. \/ (1. + tf.math.exp(-x))","9e313345":"act__ = 0.5\ndef preprocessing_for_roi(tensor): # \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u043a\u0440\u0438\u0432\u043e\u0439 \u043a\u043e\u0434, \u0435\u0441\u043b\u0438 \u0431\u0443\u0434\u0435\u0442 \u0432\u0440\u0435\u043c\u044f \u0434\u043e\u0432\u0435\u0434\u0443 \u0434\u043e \u0443\u043c\u0430\n    batch_ = []\n    x_min, y_min, x_max, y_max = 1, 1, tensor.shape[1], tensor.shape[2]\n    batch_n = 10\n    for batch in range(batch_n):\n        for row in range(tensor.shape[1]):\n            for col in range(tensor.shape[2]):\n                act = sigmoid_tf(tensor[batch][row][col][0])\n                if tf.math.less(act, act__) is not None:\n                    continue\n                else:\n                    x_min = row\n                    y_min = col\n                break\n            break\n        for row in range(x_min, tensor.shape[1]):\n            act = sigmoid_tf(tensor[batch][row][y_min][0])\n            if tf.math.less(act, act__) is not None:\n                x_max = row - 1\n        for col in range(y_min, tensor.shape[2]):\n            act = sigmoid_tf(tensor[batch][x_min][col][0])\n            if tf.math.less(act, act__) is not None:\n                y_max = col - 1\n        x_Min, y_Min, x_Max, y_Max = tf.truediv(x_min, tensor.shape[1]), tf.truediv(y_min, tensor.shape[2]), tf.truediv(x_max, tensor.shape[1]), tf.truediv(y_max, tensor.shape[2])\n        coor = np.array([x_Min, y_Min, x_Max, y_Max])\n        batch_.append(np.array([coor]))            \n    return np.array(batch_)","e0d7dcd9":"pooled_height = 7\npooled_width = 7\nbatch_size = 10\nfeature_maps_shape = (batch_size, feature_tensor.shape[1],\n                      feature_tensor.shape[2], feature_tensor.shape[3])\nfeature_maps_tf = tf.placeholder(tf.float32, shape=feature_maps_shape)\n\nroiss_tf = tf.placeholder(tf.float32, shape=(batch_size, 1, 4))\nroi_layer = ROIPoolingLayer(pooled_height, pooled_width)\npooled_features = roi_layer([feature_maps_tf, roiss_tf])","d1ecfafe":"output = keras.layers.BatchNormalization()(feature_tensor)\n\nseed = 29\nkernek_initializer = keras.initializers.glorot_normal(seed=seed)\n# roi_layer = ROIPoolingLayer(pooled_height, pooled_width)\n\n# TODO: \u0434\u043e\u0431\u0430\u0432\u044c\u0442\u0435 \u0432\u044b\u0445\u043e\u0434\u044b \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0434\u0435\u0442\u0435\u043a\u0446\u0438\u0438\n\noutput_1 = keras.layers.Conv2D(5, kernel_size=(1, 1), \n                             activation='linear',\n                             kernel_regularizer='l2', name=\"box_output\")(output)\n\n# output_2 = roi_layer([output, preprocessing_for_roi(output_1)])([output, output_1])\noutput_2 = keras.layers.Flatten()(output)\noutput_2 = keras.layers.BatchNormalization()(output_2)\noutput_2 = keras.layers.Dropout(0.3)(output_2)\noutput_2 = keras.layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l1(1e-4),\n                            kernel_initializer=kernek_initializer)(output_2)\noutput_2 = keras.layers.BatchNormalization()(output_2)\noutput_2 = keras.layers.Dropout(0.3)(output_2)\noutput_2 = keras.layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l1(1e-4),\n                            kernel_initializer=kernek_initializer)(output_2)\noutput_2 = keras.layers.BatchNormalization()(output_2)\noutput_2 = keras.layers.Dense(7, activation='softmax', kernel_regularizer=keras.regularizers.l1(1e-4),\n                            kernel_initializer=kernek_initializer, name=\"category_output\")(output_2)\n\n\nmodel = keras.models.Model(inputs=features.inputs, outputs=[output_1, output_2])\nmodel.summary()","e4826160":"losses = {\"box_output\": total_loss, \"category_output\": \"categorical_crossentropy\"}\n\nadam = keras.optimizers.Adam(lr=1e-4, decay=1e-6)\nmodel.compile(optimizer=adam, \n              loss={\"box_output\": total_loss, \"category_output\": \"categorical_crossentropy\"},\n              metrics={'box_output': confidence_loss, 'category_output': 'accuracy'})","450cada3":"def scheduler(epoch):\n  if epoch < 5:\n    return 1e-4\n  else:\n    return 1e-4 * tf.math.exp(0.1 * (5 - epoch))\ncallback = tf.keras.callbacks.LearningRateScheduler(scheduler)","46fee01f":"batch_size = 10\nsteps_per_epoch = sum(map(len, boxes.values()), 0) \/ batch_size\n\ngen = data_generator(boxes, batch_size=batch_size)\n\ncheckpoint = keras.callbacks.ModelCheckpoint(\n    'weights.{epoch:02d}-{loss:.3f}.hdf5',\n    monitor='loss',\n    verbose=1,  \n    save_best_only=True, \n    save_weights_only=False,\n    mode='auto', period=1)\n\nmodel.fit_generator(generator=gen, \n                    steps_per_epoch=steps_per_epoch,\n                    epochs=12, callbacks=[callback])","05e6dbb8":"example = boxes['lag'][10]\n\n_, sample_img = load_img(example['filename'])\npred = model.predict(np.array([sample_img,]))[0]","636f793c":"pred_2 = pred.reshape(23, 37, 5)\npred_2.shape","fb65677f":"decoded = decode_prediction(pred_2, conf_thr=0.1)\ndecoded = sorted(decoded, key=lambda e: -e['conf'])\n\nplt.figure(figsize=(6, 6), dpi=120)\nimg = draw_boxes(example, decoded[:3], (IMG_WIDTH, IMG_HEIGHT))\nplt.imshow(img)\nplt.title('{}x{}'.format(*img.shape));","c34cc248":"from glob import glob\ntest_files = sorted(glob('..\/input\/test-fish\/test_stg1\/test_stg1\/*.jpg'))\ntest_files_2 = sorted(glob('..\/input\/fullfish\/test\/test\/*.jpg'))\nlen(test_files) + len(test_files_2)","8008a941":"train_files = glob('..\/input\/the-nature-conservancy-fisheries-monitoring\/train\/YFT\/*.jpg')\nlen(train_files)","0b0f236d":"from keras.preprocessing.image import load_img, img_to_array\nfrom keras.applications.vgg16 import preprocess_input\n\ndef load_image(path, target_size=(IMG_HEIGHT, IMG_WIDTH)):\n    img = load_img(path, target_size=target_size)  # \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0438 \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\n    array = img_to_array(img)\n    return preprocess_input(array)\n# \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u0447\u0442\u0435\u043d\u0438\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u0434\u0438\u0441\u043a\u0430\ndef predict_generator(files):\n    while True:\n        for path in files:\n            yield np.array([load_image(path)])","a60285f8":"pred = model.predict_generator(predict_generator(test_files), len(test_files))","37b947df":"pred_full = model.predict_generator(predict_generator(test_files_2), len(test_files_2))","6e591697":"import pandas as pd\nmy_submit= pd.DataFrame(columns=['image', 'ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT'])\nmy_submit.image = np.arange(0,13153).astype(int)","3cd7252e":"for i in range(1000):\n    my_submit['image'].loc[my_submit.index==i] = os.path.basename(test_files[i])\n    my_submit['ALB'].loc[my_submit.index==i] = pred[1][i][0]\n    my_submit['BET'].loc[my_submit.index==i] = pred[1][i][1]\n    my_submit['DOL'].loc[my_submit.index==i] = pred[1][i][2]\n    my_submit['LAG'].loc[my_submit.index==i] = pred[1][i][3]\n    my_submit['NoF'].loc[my_submit.index==i] = pred[1][i][4]\n    my_submit['OTHER'].loc[my_submit.index==i] = 0. # other \u043d\u0435 \u0440\u0430\u0437\u043c\u0435\u0447\u0435\u043d\u0430, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0437\u0430\u0431\u0438\u0432\u0430\u044e \u043d\u0443\u043b\u044f\u043c\u0438\n    my_submit['SHARK'].loc[my_submit.index==i] = pred[1][i][5]\n    my_submit['YFT'].loc[my_submit.index==i] = pred[1][i][6]","69c050ec":"for i in range(1000, 13153):\n    my_submit['image'].loc[my_submit.index==i] = 'test_stg2\/' + os.path.basename(test_files_2[i-1000])\n    my_submit['ALB'].loc[my_submit.index==i] = pred_full[1][i-1000][0]\n    my_submit['BET'].loc[my_submit.index==i] = pred_full[1][i-1000][1]\n    my_submit['DOL'].loc[my_submit.index==i] = pred_full[1][i-1000][2]\n    my_submit['LAG'].loc[my_submit.index==i] = pred_full[1][i-1000][3]\n    my_submit['NoF'].loc[my_submit.index==i] = pred_full[1][i-1000][4]\n    my_submit['OTHER'].loc[my_submit.index==i] = 0. # other \u043d\u0435 \u0440\u0430\u0437\u043c\u0435\u0447\u0435\u043d\u0430, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0437\u0430\u0431\u0438\u0432\u0430\u044e \u043d\u0443\u043b\u044f\u043c\u0438\n    my_submit['SHARK'].loc[my_submit.index==i] = pred_full[1][i-1000][5]\n    my_submit['YFT'].loc[my_submit.index==i] = pred_full[1][i-1000][6]","f5b7aaca":"my_submit.shape","46930e91":"my_submit[980:1010]","ecf63784":"my_submit.to_csv('submit.csv', index=False)","017813cf":"# TODO: \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0436\u0438\u0442\u0435 \u043a\u043b\u0430\u0441\u0441 \u0440\u044b\u0431\u044b \u0434\u043b\u044f \u0444\u043e\u0442\u043e\u0433\u0440\u0430\u0444\u0438\u0438 \u0438\u0437 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\n#\n# \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u044c\u0442\u0435 \u0444\u0430\u0439\u043b \u0441 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f\u043c\u0438 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0439 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u0444\u043e\u0442\u043e\u0433\u0440\u0430\u0444\u0438\u0438:\n# image,ALB,BET,DOL,LAG,NoF,OTHER,SHARK,YFT\n# img_00001.jpg,1,0,0,0,0,...,0\n# img_00002.jpg,0.3,0.1,0.6,0,...,0","7a9145d1":"### \u0412\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f \u0435\u043d\u043a\u043e\u0434\u0438\u043d\u0433\u0430\/\u0434\u0435\u043a\u043e\u0434\u0438\u043d\u0433\u0430","b2b366ae":"## \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0440\u0430\u0431\u043e\u0442\u044b \u0434\u0435\u0442\u0435\u043a\u0442\u043e\u0440\u0430","03d2adf5":"## \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0432\u044b\u0445\u043e\u0434 \u0434\u0435\u0442\u0435\u043a\u0442\u043e\u0440\u0430","b275ae03":"### \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0443","3d5f4eb1":"# \u0421\u0435\u0442\u043a\u0430 \u044f\u043a\u043e\u0440\u0435\u0439 (anchor grid)","215e416a":"# \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0443","56f0a3f7":"## \u0421\u043e\u0437\u0434\u0430\u0451\u043c \u0441\u043b\u043e\u0439 ROI-pooling","cd0fd792":"## \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435","ff4a8028":"## \u0410\u0433\u0440\u0435\u0433\u0430\u0446\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432","38f310e6":"### \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u0432 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0438","1434d0c0":"https:\/\/www.kaggle.com\/c\/the-nature-conservancy-fisheries-monitoring","bcd1ec40":"## \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","df56e1e8":"# \u042d\u043a\u0441\u0442\u0440\u0430\u043a\u0442\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432","aed7a76e":"## \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c","5a3c0221":"# The Nature Conservancy Fisheries Monitoring"}}