{"cell_type":{"12345dc2":"code","4d109de9":"code","0921b62d":"code","2d6a946b":"code","66298ae6":"code","f8f82e14":"code","d850bbab":"code","febda2a2":"code","98904d0b":"code","97b2b500":"code","313c71aa":"code","2ab851f9":"code","397e6b00":"code","476a9876":"code","98c48d53":"code","d2d5e2fc":"code","df57572b":"code","1f514e01":"code","6ec65266":"code","eaf82ef4":"code","a4ec0373":"code","08935529":"code","efba0876":"code","453f5bff":"code","9120b176":"code","2669f9de":"code","64b37267":"code","80ce1623":"code","396e1b5a":"code","9731ea80":"code","d5e0e3f7":"code","687ec96c":"code","5fefd635":"code","2d728628":"code","d2503790":"code","dc2d2835":"code","4828a3b5":"code","a08455d1":"code","e352453f":"code","e3a70814":"code","bfcba163":"code","772b88f4":"code","4814c725":"code","de2652d4":"code","201252ea":"code","59bb906f":"code","d91c90c0":"code","63443cf7":"code","bc8980ce":"code","2a0efeb0":"code","c54a3ec9":"code","43276a4e":"code","a4abe328":"code","18fcd0a2":"code","d606d256":"code","38ec91a4":"code","ab8b154a":"code","5a5ad089":"code","24bf755d":"markdown","07fa2d86":"markdown","496a8ecd":"markdown","e9dee51a":"markdown","21b5f479":"markdown","e3419788":"markdown","da92b47c":"markdown","2cdf6e5b":"markdown","e359a0d7":"markdown","240b83a4":"markdown","2eb2bdc3":"markdown","5551b7a8":"markdown","fa613b0d":"markdown","cd6ae26d":"markdown","89f39c61":"markdown","6429eb89":"markdown","14affa68":"markdown","672b8197":"markdown","b7ffb189":"markdown","299e6aca":"markdown","267d8df2":"markdown","d27d6ea3":"markdown","e0599cf5":"markdown","4bc42263":"markdown","67249033":"markdown","bbc13b9b":"markdown","381f1ab1":"markdown","a0a5268f":"markdown","7ad20305":"markdown","ef189e73":"markdown","f80d00bd":"markdown","96e7e1e3":"markdown","65f3c421":"markdown","8704092f":"markdown","3b8fddf0":"markdown","6e6d2b26":"markdown","14dc97da":"markdown","0a11b05c":"markdown","dd08e9ee":"markdown","336fbd08":"markdown","cb1bda3d":"markdown","e40c5cff":"markdown","bb95c241":"markdown"},"source":{"12345dc2":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport statsmodels.api as sm\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt","4d109de9":"bikeSharing = pd.read_csv(\"\/kaggle\/input\/bikesharing\/day.csv\")\nprint(bikeSharing.shape)\nprint(bikeSharing.info())","0921b62d":"bikeSharing.head()","2d6a946b":"colsTobeDeleted = [\"instant\",\"dteday\",\"casual\",\"registered\"]\nbikeSharing.drop(colsTobeDeleted , axis =1 ,inplace = True)\nbikeSharing.head()","66298ae6":"bikeSharing['season'] = bikeSharing['season'].astype(str)\nbikeSharing['mnth'] = bikeSharing['mnth'].astype(str)\nbikeSharing['weekday'] = bikeSharing['weekday'].astype(str)\nbikeSharing['weathersit'] = bikeSharing['weathersit'].astype(str)\n# Binary columns - yr,holiday,workingday have 0,1 hence no requirement to create dummies\nbikeSharing['season'] = bikeSharing[['season']].apply(lambda x : x.map({\"1\" : \"spring\" , \"2\" : \"summer\", \"3\" : \"fall\" , \"4\" : \"winter\"}))\nbikeSharing['mnth'] = bikeSharing[['mnth']].apply(lambda x : x.map({\"1\" : \"Jan\" , \"2\" : \"Feb\", \"3\" : \"Mar\" , \"4\" : \"Apr\" , \"5\" : \"May\" , \n                                                                      \"6\" : \"Jun\" , \"7\" : \"Jul\" , \"8\" : \"Aug\" , \"9\" : \"Sep\",\n                                                                     \"10\" : \"Oct\" , \"11\" : \"Nov\" , \"12\" : \"Dec\"}))\nbikeSharing['weekday'] = bikeSharing[['weekday']].apply(lambda x : x.map({\"0\" : \"Sun\",\"1\" : \"Mon\" , \"2\" : \"Tue\", \"3\" : \"Wed\" , \"4\" : \"Thurs\" , \"5\" : \"Fri\" , \n                                                                      \"6\" : \"Sat\" }))\nbikeSharing['weathersit'] = bikeSharing[['weathersit']].apply(lambda x : x.map({\"1\" : \"clear\",\"2\" : \"mist-cloudy\" , \"3\" : \"Light snow-rain\", \"4\" : \"Heavy snow-rain\"}))\nbikeSharing.head()","f8f82e14":"bikeSharing.head()","d850bbab":"bikeSharing.info()","febda2a2":"bikeSharing.describe()","98904d0b":"#visualizing the numeric variables\nsns.pairplot(bikeSharing)\nplt.show()","97b2b500":"bikeSharing.corr()","313c71aa":"plt.figure(figsize = (16,10))\nsns.heatmap(bikeSharing.corr(),annot = True,cmap = \"YlGnBu\")\nplt.show()","2ab851f9":"bikeSharing.drop(\"atemp\",axis=1,inplace = True)\nbikeSharing.head()","397e6b00":"bikeSharing['season'].value_counts()\n# distribution is decent","476a9876":"bikeSharing['weathersit'].value_counts()\n# seems heavy snow-rain doesn't have any distributions\n# good for dummy variables splits","98c48d53":"bikeSharing['weekday'].value_counts()\n# distribution is decent","d2d5e2fc":"bikeSharing['mnth'].value_counts()\n# distribution is decent","df57572b":"#visualizing the categorical variables\nplt.figure(figsize=(30,20))\nplt.subplot(2,2,1)\nsns.boxplot(x='season',y='cnt',data=bikeSharing)\nplt.subplot(2,2,2)\nsns.boxplot(x='mnth',y='cnt',data=bikeSharing)\nplt.subplot(2,2,3)\nsns.boxplot(x='weekday',y='cnt',data=bikeSharing)\nplt.subplot(2,2,4)\nsns.boxplot(x='weathersit',y='cnt',data=bikeSharing)\nplt.show()","1f514e01":"plt.figure(figsize=(30,20))\nplt.subplot(2,2,1)\nsns.barplot(x= \"season\" , y = \"cnt\" ,hue = \"yr\",data = bikeSharing)\nplt.subplot(2,2,2)\nsns.barplot(x= \"mnth\" , y = \"cnt\" ,hue = \"yr\",data = bikeSharing)\nplt.subplot(2,2,3)\nsns.barplot(x= \"weekday\" , y = \"cnt\" ,hue = \"yr\",data = bikeSharing)\nplt.subplot(2,2,4)\nsns.barplot(x= \"weathersit\" , y = \"cnt\" ,hue = \"yr\",data = bikeSharing)\nplt.show()","6ec65266":"colToHaveDummies = [\"season\",\"mnth\",\"weekday\",\"weathersit\"]\nfor col in colToHaveDummies:\n    status = pd.get_dummies(bikeSharing[col],drop_first = True)\n    bikeSharing = pd.concat([bikeSharing,status],axis = 1)\nbikeSharing.head()","eaf82ef4":"\n# drop the original columns as dummies are created for them\nbikeSharing.drop(colToHaveDummies,axis=1,inplace = True)\nbikeSharing.columns","a4ec0373":"# Split the data to train test sets\ndf_train, df_test = train_test_split(bikeSharing, train_size = 0.7 ,random_state = 100)\nprint(df_train.shape)\nprint(df_test.shape)","08935529":"scaler = MinMaxScaler()","efba0876":"# Apply scaler to all columns except dummy variables\ncolToHaveScaling = [\"temp\",\"hum\",\"windspeed\"]\ndf_train[colToHaveScaling] = scaler.fit_transform(df_train[colToHaveScaling])\ndf_train.head()","453f5bff":"df_train.describe()","9120b176":"y_train = df_train.pop('cnt')\nX_train = df_train\nX_train.head()","2669f9de":"lm = LinearRegression()\nlm.fit(X_train,y_train)\nrfe = RFE(lm,20)\nrfe = rfe.fit(X_train,y_train)","64b37267":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","80ce1623":"col = X_train.columns[rfe.support_]\ncol","396e1b5a":"X_train.columns[~rfe.support_]","9731ea80":"# Generating X_train dataframe with RFE selected columns\nX_train_rfe = df_train[col]","d5e0e3f7":"# Calculate the VIFs for the new model\ndef VIF_Checker(X_train):\n    vif = pd.DataFrame()\n    X = X_train.drop('const', axis =1)\n    vif['Features'] = X.columns\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    return vif","687ec96c":"# Adding a constant\nX_train_rfe = sm.add_constant(X_train_rfe)\nlm = sm.OLS(y_train,X_train_rfe).fit()\nprint(lm.summary())\nprint(VIF_Checker(X_train_rfe))","5fefd635":"# Rebuilding the model\nX_train_2 = X_train_rfe.drop(\"workingday\",axis = 1)\nX_train_2 = sm.add_constant(X_train_2)\nlm = sm.OLS(y_train,X_train_2).fit()\nprint(lm.summary())\nprint(VIF_Checker(X_train_2))","2d728628":"X_train_3 = X_train_2.drop(['Sat'],axis = 1)\nX_train_3 = sm.add_constant(X_train_3)\nlm = sm.OLS(y_train,X_train_3).fit()\nprint(lm.summary())\nprint(VIF_Checker(X_train_3))","d2503790":"X_train_4 = X_train_3.drop(['Sun'],axis = 1)\nX_train_4 = sm.add_constant(X_train_4)\nlm = sm.OLS(y_train,X_train_4).fit()\nprint(lm.summary())\nprint(VIF_Checker(X_train_4))","dc2d2835":"X_train_5 = X_train_4.drop(['Feb'],axis = 1)\nX_train_5 = sm.add_constant(X_train_5)\nlm = sm.OLS(y_train,X_train_5).fit()\nprint(lm.summary())\nprint(VIF_Checker(X_train_5))","4828a3b5":"X_train_6 = X_train_5.drop(['Tue'],axis = 1)\nX_train_6 = sm.add_constant(X_train_6)\nlm = sm.OLS(y_train,X_train_6).fit()\nprint(lm.summary())\nprint(VIF_Checker(X_train_6))","a08455d1":"X_train_7 = X_train_6.drop(['Dec'],axis = 1)\nX_train_7 = sm.add_constant(X_train_7)\nlm = sm.OLS(y_train,X_train_7).fit()\nprint(lm.summary())\nprint(VIF_Checker(X_train_7))","e352453f":"X_train_8 = X_train_7.drop(['Nov'],axis = 1)\nX_train_8 = sm.add_constant(X_train_8)\nlm = sm.OLS(y_train,X_train_8).fit()\nprint(lm.summary())\nprint(VIF_Checker(X_train_8))","e3a70814":"X_train_9 = X_train_8.drop(['Jan'],axis = 1)\nX_train_9 = sm.add_constant(X_train_9)\nlm = sm.OLS(y_train,X_train_9).fit()\nprint(lm.summary())\nprint(VIF_Checker(X_train_9))","bfcba163":"X_train_10 = X_train_9.drop(['spring'],axis = 1)\nX_train_10 = sm.add_constant(X_train_10)\nlm = sm.OLS(y_train,X_train_10).fit()\nprint(lm.summary())\nprint(VIF_Checker(X_train_10))","772b88f4":"X_train_11 = X_train_10.drop(['Jul'],axis = 1)\nX_train_11 = sm.add_constant(X_train_11)\nlm = sm.OLS(y_train,X_train_11).fit()\nprint(lm.summary())\nprint(VIF_Checker(X_train_11))","4814c725":"X_train_12 = X_train_11.drop(['hum'],axis = 1)\nX_train_12 = sm.add_constant(X_train_12)\nlm = sm.OLS(y_train,X_train_12).fit()\nprint(lm.summary())\nprint(VIF_Checker(X_train_12))","de2652d4":"X_train_13 = X_train_12.drop(['mist-cloudy'],axis = 1)\nX_train_13 = sm.add_constant(X_train_13)\nlm = sm.OLS(y_train,X_train_13).fit()\nprint(lm.summary())\nprint(VIF_Checker(X_train_13))","201252ea":"X_train_14 = X_train_13.drop(['holiday'],axis = 1)\nX_train_14 = sm.add_constant(X_train_14)\nlm = sm.OLS(y_train,X_train_14).fit()\nprint(lm.summary())\nprint(VIF_Checker(X_train_14))","59bb906f":"print(lm.params)","d91c90c0":"y_train_pred = lm.predict(X_train_14)\nfig = plt.figure()\nsns.distplot((y_train - y_train_pred),bins = 20)\nfig.suptitle(\"Error Terms\",fontsize = 20)\nplt.xlabel('Errors' , fontsize = 16)\nplt.ylabel('Density' , fontsize = 16)\nplt.show()","63443cf7":"# Apply scaler to all columns except dummy variables\ncolToHaveScaling = [\"temp\",\"hum\",\"windspeed\"]\ndf_test[colToHaveScaling] = scaler.transform(df_test[colToHaveScaling])\ndf_test.head()","bc8980ce":"df_test.describe()","2a0efeb0":"y_test = df_test.pop('cnt')\nfinalCols = (X_train_14.drop('const',axis=1)).columns\nX_test = df_test[finalCols]\nX_test.columns","c54a3ec9":"X_test_sm = sm.add_constant(X_test)\nX_test_sm.head()","43276a4e":"y_pred = lm.predict(X_test_sm)\ny_pred","a4abe328":"fig = plt.figure()\nplt.scatter(y_test,y_pred,alpha = .5)\nfig.suptitle('y_test vs y_pred',fontsize = 20)\nplt.xlabel('y_test',fontsize = 18)\nplt.ylabel('y_pred',fontsize = 15)\nplt.show()","18fcd0a2":"r2_score(y_test, y_pred)","d606d256":"r2=0.7801240344317599\nX_test.shape\nn = X_test.shape[0]\n\n\n# Number of features (predictors, p) is the shape along axis 1\np = X_test.shape[1]\n\n# We find the Adjusted R-squared using the formula\n\nadjusted_r2 = 1-(1-r2)*(n-1)\/(n-p-1)\nadjusted_r2","38ec91a4":"rmse = sqrt(mean_squared_error(y_test,y_pred))\nnormalized_rmse = rmse\/(y_pred.max() - y_pred.min())\nprint(normalized_rmse)","ab8b154a":"#Actual vs Predicted\nc = [i for i in range(1,y_test.size + 1,1)]\nfig = plt.figure(figsize = (16,10))\nplt.plot(c,y_test, color=\"blue\", linewidth=2.5, linestyle=\"-\")\nplt.plot(c,y_pred, color=\"red\",  linewidth=2.5, linestyle=\"-\")\nfig.suptitle('Actual and Predicted', fontsize=20)              # Plot heading \nplt.xlabel('Index', fontsize=18)                               # X-label\nplt.ylabel('cnt', fontsize=16)                               # Y-label","5a5ad089":"# Error terms\nc = [i for i in range(1,y_pred.size + 1,1)]\nfig = plt.figure(figsize = (16,10))\nplt.plot(c,y_test - y_pred, color=\"blue\", linewidth=2.5, linestyle=\"-\")\nfig.suptitle('Error Terms', fontsize=20)              # Plot heading \nplt.xlabel('Index', fontsize=18)                      # X-label\nplt.ylabel('y_test - y_pred', fontsize=16)                # Y-label","24bf755d":"**Model 12 - Drop hum due to high VIF since all predictors are significant**","07fa2d86":"#### Inferences\n- temp and atemp shows me very steep positive relation. The data points scatter is very low.\n- there is a similiar pattern of scatter for cnt with temp and atemp.\n- when windspeed is low the cnt is high seems customers prefer decent windspeeds","496a8ecd":"## Replace the categories of the various categorical columns with respective category names\n- season = if there are values like 1,2,3,4 then model will assume 4 is of highest priority\n    - 1 = spring\n    - 2 = summer\n    - 3 = fall\n    - 4 = winter\n- month\n    - 1 = Jan\n    - 2 = Feb\n    - 3 = Mar\n    .\n    .\n    - 12 = Dec\n- weekday\n    - 0 = Sun\n    - 1 = Mon\n    .\n    .\n    .\n    - 6 = Sat\n- weathersit\n    - 1 = clear\n    - 2 = mist-cloudy\n    - 3 = Light snow-rain\n    - 4 = Heavy snow-rain","e9dee51a":"**Rules of removing the predictors**\n- Remove variables with high VIF(>5) and which are insignificant (p>0.02)\n- if model has high VIF and are significant , check and remove the insignificant variables\n- if some variables have a high VIF , remove the variable which is relatively less significant\n- if the no of variables is still high , remove them in order of insignificance","21b5f479":"##### Inference\n- there is a good positive correlation between cnt and temp,atemp. This can be a good predictor for business\n- Negative corr between cnt and humidity. Seems due the humidity consumers do not like to ride bikes but needs to analyzed more.\n- temp and atemp have very high correlation it shows they are interdependent on each other in a positive manner.\n- model will get affected due to same\n- need to drop either one of them - we will go for atemp","e3419788":"**Model 13 - Drop mist-cloudy due to high VIF though here 3rd highest was considered as clear and temp can be driving factor**","da92b47c":"**Model 5 - Dropping Feb for high p-value**","2cdf6e5b":"***Variance Inflation Factor - Common Function for finding multicollinearity***","e359a0d7":"**Model 9 - Dropping Jan due to high p-value**","240b83a4":"* **There is no multicollinearity among the predictors**\n* **the p-values shows all the predictor variables are significant**\n* **We can consider this as final model unless there is a huge difference in predicted set adjusted R2**","2eb2bdc3":"**Model 7 - Drop Dec for high p-value**","5551b7a8":"### the equation for the best fit line \n##### **cnt = *357.909484* + *2058.728737* * yr + *4756.400448* * temp - *1540.313416* * windspeed + *795.519937* * summer + *1038.170206* * winter + *822.595483* * Sep + *800.556156* * clear**","fa613b0d":"# Step 3 : Preprocessing\n- create dummies for \"season\",\"mnth\",\"weekday\",\"weathersit\"\n- split the data to train-test set\n- Perform scaling","cd6ae26d":"# Step-1 : Data Preparation","89f39c61":"**Model 4 - Dropping Sun due to high VIF**","6429eb89":"A lower value and closer to 0 represents better fitting model","14affa68":"- in fall season , the total count of bike users everyday is high in comparision to other seasons\n- To support I can see there is high amount of usage of bikes during September with peak period from June to Oct\n- When there is a clear weather there is high usage of bikes also and during light snow-rain there is low usage","672b8197":"# Step 4 - Modelling","b7ffb189":"Hum has high collinearity with other predictors","299e6aca":"**Error terms are normally distributed with mean at zero**","267d8df2":"# Step 2 : EDA\n- Perform EDA to understand various variables\n- Check the correlation between the variables - if independent columsn are high corr then drop one of the them","d27d6ea3":"# Bike Sharing - Multiple Linear Regression\n### Created By - Bhuban Mohan Nayak\n**Problem Statement**\nA bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.\n\n\nA US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. \n\n\nIn such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.\n\n\nThey have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:\n\nWhich variables are significant in predicting the demand for shared bikes.\nHow well those variables describe the bike demands\nBased on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors. \n\n\n**Business Goal:**\nYou are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market. \n\n\n**Data Preparation:**\n\nYou can observe in the dataset that some of the variables like 'weathersit' and 'season' have values as 1, 2, 3, 4 which have specific labels associated with them (as can be seen in the data dictionary). These numeric values associated with the labels may indicate that there is some order to them - which is actually not the case (Check the data dictionary and think why). So, it is advisable to convert such feature values into categorical string values before proceeding with model building. Please refer the data dictionary to get a better understanding of all the independent variables.\n \nYou might notice the column 'yr' with two values 0 and 1 indicating the years 2018 and 2019 respectively. At the first instinct, you might think it is a good idea to drop this column as it only has two values so it might not be a value-add to the model. But in reality, since these bike-sharing systems are slowly gaining popularity, the demand for these bikes is increasing every year proving that the column 'yr' might be a good variable for prediction. So think twice before dropping it. \n \n\n**Model Building**\n\nIn the dataset provided, you will notice that there are three columns named 'casual', 'registered', and 'cnt'. The variable 'casual' indicates the number casual users who have made a rental. The variable 'registered' on the other hand shows the total number of registered users who have made a booking on a given day. Finally, the 'cnt' variable indicates the total number of bike rentals, including both casual and registered. The model should be built taking this 'cnt' as the target variable.\n\n\n**Model Evaluation:**\nWhen you're done with model building and residual analysis and have made predictions on the test set, just make sure you use the following two lines of code to calculate the R-squared score on the test set.\n\n \n\nfrom sklearn.metrics import r2_score\nr2_score(y_test, y_pred)\n \n\nwhere y_test is the test data set for the target variable, and y_pred is the variable containing the predicted values of the target variable on the test set.\nPlease don't forget to perform this step as the R-squared score on the test set holds some marks. The variable names inside the 'r2_score' function can be different based on the variable names you have chosen.","e0599cf5":"# Step 6 : Prediction using test sets","4bc42263":"##### **F-Statistics**\nThe overall significance of the model . The lower the value of p(F-statistics) or higher the value of F-statistics the better the model\n- **F-statistic: 294.6**\n- **Prob (F-statistic): 3.08e-173**","67249033":"#### Dividing into X and Y sets for model building","bbc13b9b":"**Model 1 - Recursive Feature Elimination - Limit set to 20**","381f1ab1":"**Normalized RMSE**","a0a5268f":"## Drop the columns\n- instant = has unique values\n- dteday = date column cannnot used for model and we have already data associated to same in other columns\n- casual,registered\n    - They are interdependent with each other and directly correlated with count. 100% R2 value.\n    - These two numbers will not available until the day is complete. You cannot use as current for model building process","7ad20305":"* As the coef. are non zero we are rejecting H0 the null hyphothesis","ef189e73":"# Step 5 : Model Interpretation\n* **Hypothesis testing states that:-**\n  - H0 : B1,B2,.....Bn = 0\n  - H1 : atleast Bi != 0","f80d00bd":"#### **Building model using statsmodel for the detailed statistics**","96e7e1e3":"**Abs R2 score on test set**","65f3c421":"**Model 2 - Dropping workingday due to high VIF**","8704092f":"**Model 11 - We have high VIFs but dropping Jul due to relatively less significance**","3b8fddf0":"**Model 8 - Dropping 'Nov' due to high p-value**","6e6d2b26":"##### Inference\n- Though adj R2 shows 84% but workingday seems to have high collinearity with other variables. Drop workingday as it has highest VIF and p value","14dc97da":"##### from 2018 to 2019 the customer base has increased almost twice.","0a11b05c":"**Model 14 - Drop 'holiday' due to its less significance since all VIF are less than 5**","dd08e9ee":"**Model 10 - We have high VIFs but dropping spring due to its relatively less significance**","336fbd08":"**Model 3 - Dropping Sat due to its high VIF. As hum has highest VIF but it is significant**","cb1bda3d":"### Final Inference\n- Train = R2 - 80.4%, adj R2 -80.1%\n- Test = R2 - 78% , adj R2 - 77%\n- RMSE - 0.117\n- Demand dynamics of the market. Top predictors were temp, year and windspeed. A unit increase in temp or year have positive impact but unit increase with windspeed have negative impact.\n- In summer and winter also the market is high for bike customers the attention needs to be on spring season as fall had highest contributions in past.\n- The clear weather attracts more bikers.","e40c5cff":"**R2 score on test set**","bb95c241":"**Model 6 - Dropping Tue for high p-value**"}}