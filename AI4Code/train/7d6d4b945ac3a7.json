{"cell_type":{"5b392069":"code","178203b3":"code","75fb6a45":"code","676399b8":"code","77ebf9ad":"code","68483c64":"code","2273c1a6":"code","07276810":"code","50b06e9b":"code","31be0479":"code","a571049e":"code","f32b068e":"code","22736569":"code","32bf2157":"code","8d62a153":"code","91fcbc89":"code","c2969cfb":"code","dc682b62":"code","9f95f966":"code","5093f7d0":"code","6432945a":"code","969e5bfa":"code","4e3e96b1":"code","9d52197b":"code","1e095b35":"code","e4780dd1":"code","017dcf2f":"code","ec36d1ac":"code","0eb33544":"code","9303e455":"code","8ce43155":"code","c933aaa1":"code","755bb932":"code","c8d0c41b":"code","63b8543a":"code","1a62bfeb":"code","3788bf19":"code","87ecbc41":"code","e31e181d":"code","5afbd968":"code","ddb7c42e":"code","07b52725":"markdown","0420874e":"markdown","497146aa":"markdown","1d8fd9f6":"markdown","83003ceb":"markdown","569e588c":"markdown","6997e95b":"markdown","fd4c029d":"markdown","b71de23d":"markdown"},"source":{"5b392069":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense, Input, Flatten, Dropout, Add\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.layers import LSTM, Bidirectional\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping\nimport gensim\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nimport re\nimport codecs\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\nstop_words = set(stopwords.words('english'))\n# Any results you write to the current directory are saved as output.","178203b3":"EMBEDDING_DIM = 300 # how big is each word vector\nMAX_VOCAB_SIZE = 175303 # how many unique words to use (i.e num rows in embedding vector)\nMAX_SEQUENCE_LENGTH = 200 # max number of words in a comment to use\n\n#training params\nbatch_size = 256 \nnum_epochs = 2 ","75fb6a45":"train_comments = pd.read_csv(\"..\/input\/manifestos-en\/manifesots_en.csv\", sep=',', header=0)\ntrain_comments.columns=['text', 'cmp_code', 'eu_code', 'pos', 'manifesto_id', 'party', 'date', 'language', 'source', 'has_eu_code', 'is_primary_doc', 'may_contradict_core_dataset', 'md5sum_text', 'url_original', 'md5sum_original', 'annotations', 'handbook', 'is_copy_of', 'title', 'id']\n#'NA', '0', '101', '102', '103', '104', '105', '106', '107', '108', '109', '201', '202', '203', '204', '301', '302', '303', '304', '305','401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '501', '502', '503', '504', '505', '506', '507', '601', '602', '603', '604', '605', '606', '607', '608', '701', '702', '703', '704', '705', '706', '707', '708')\nprint(\"num train: \", train_comments.shape[0])\ntrain_comments.head()","676399b8":"# check that the values of cmp_code are of the right type\nprint(train_comments.cmp_code[1])\nprint(type(train_comments[\"cmp_code\"]))\n\n#turns values of cmp_code from object to a list, comma separated\nbuilder_list = [] #creates an empty list\n# loop - for every entry in the cmp_code column, add value to end of list, separated by commas\nfor data in train_comments[\"cmp_code\"]: \n    builder_list.append(str(data))\n\",\".join(builder_list)\n\nfrom numpy import array\nfrom numpy import argmax\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n# define dataset \ndata = builder_list\nvalues = array(data)\nprint(values[1])\n# encode codes to integer  \nlabel_encoder = LabelEncoder()\ninteger_encoded = label_encoder.fit_transform(values)\n# print the second entry as an integer code\nprint(integer_encoded[1])\n# encode integer codes to binary\nonehot_encoder = OneHotEncoder(sparse=False)\ninteger_encoded = integer_encoded.reshape(len(integer_encoded), 1)\nonehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n# print the second entry as a code expressed in binary\nprint(onehot_encoded[1])\n\n# invert the vector to output the code. This throws an error of Deprecation (DeprecationWarning: The truth value \n# of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` \n# to check that an array is not empty.)- will need to go back to it to check.\n\n# print(type(onehot_encoded))\n# inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n# print(inverted[1])","77ebf9ad":"#label_names = ['101', '102', '103', '104', '105', '106', '107', '108', '109', '201', '202', '203', '204', '301', '302', '303', '304', '305','401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '501', '502', '503', '504', '505', '506', '507', '601', '602', '603', '604', '605', '606', '607', '608', '701', '702', '703', '704', '705', '706', '707', '708']\n#y_train = train_comments[label_names].values\n\nY_train = onehot_encoded\nprint(Y_train[1])\n\n# trying to add the vectors in the main dataset\n#clean_train_comments[\"codes_encoded\"] = clean_train_comments[\"tokens\"].apply(lambda vec: [word for word in vec if word not in stop_words]) \n#clean_train_comments.head()","68483c64":"#test_comments = pd.read_csv(\"..\/input\/manifestos-aus\/test-aus.csv\", engine='python', sep=',', header=0)\n#print(\"ok\")\n#test_comments.columns=['text', 'cmp_code', 'eu_code']\n#print(\"num test: \", test_comments.shape[0])\n#test_comments.head()\n\n#This is no longer needed, as I will create the test on the fly from the dataset above (manifesots_en.csv)","2273c1a6":"def standardize_text(df, text_field):\n    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n    df[text_field] = df[text_field].str.lower()\n    return df","07276810":"train_comments.fillna('_NA_')\ntrain_comments.fillna('NaN')\ntrain_comments = standardize_text(train_comments, \"text\")\ntrain_comments.to_csv(\"train_clean_data.csv\")\ntrain_comments.head()","50b06e9b":"#test_comments.fillna('_NA_')\n#test_comments = standardize_text(test_comments, \"text\")\n#test_comments.to_csv(\"test_clean_data.csv\")\n#test_comments.head()\n\n#This is no longer needed, as I will create the test on the fly from the dataset above (manifesots_en.csv)","31be0479":"tokenizer = RegexpTokenizer(r'\\w+')\nclean_train_comments = pd.read_csv(\"train_clean_data.csv\")\nclean_train_comments['text'] = clean_train_comments['text'].astype('str') \nclean_train_comments.dtypes\nclean_train_comments[\"tokens\"] = clean_train_comments[\"text\"].apply(tokenizer.tokenize)\n# delete Stop Words\nclean_train_comments[\"tokens\"] = clean_train_comments[\"tokens\"].apply(lambda vec: [word for word in vec if word not in stop_words])\n   \nclean_train_comments.head()","a571049e":"#This is no longer needed, as I will create the test on the fly from the dataset above (manifesots_en.csv)\n#clean_test_comments = pd.read_csv(\"test_clean_data.csv\")\n#clean_test_comments['text'] = clean_test_comments['text'].astype('str') \n#clean_test_comments.dtypes\n#clean_test_comments[\"tokens\"] = clean_test_comments[\"text\"].apply(tokenizer.tokenize)\n#clean_test_comments[\"tokens\"] = clean_test_comments[\"tokens\"].apply(lambda vec: [word for word in vec if word not in stop_words])\n\n#clean_test_comments.head()","f32b068e":"all_training_words = [word for tokens in clean_train_comments[\"tokens\"] for word in tokens]\ntraining_sentence_lengths = [len(tokens) for tokens in clean_train_comments[\"tokens\"]]\nTRAINING_VOCAB = sorted(list(set(all_training_words)))\nprint(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\nprint(\"Max sentence length is %s\" % max(training_sentence_lengths))\n#print(clean_train_comments[\"tokens\"])","22736569":"#all_test_words = [word for tokens in clean_test_comments[\"tokens\"] for word in tokens]\n#test_sentence_lengths = [len(tokens) for tokens in clean_test_comments[\"tokens\"]]\n#TEST_VOCAB = sorted(list(set(all_test_words)))\n#print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n#print(\"Max sentence length is %s\" % max(test_sentence_lengths))\n","32bf2157":"word2vec_path = \"..\/input\/googles-trained-word2vec-model-in-python\/GoogleNews-vectors-negative300.bin.gz\"\nword2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n\ndef get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n    if len(tokens_list)<1:\n        return np.zeros(k)\n    if generate_missing:\n        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n    else:\n        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n    length = len(vectorized)\n    summed = np.sum(vectorized, axis=0)\n    averaged = np.divide(summed, length)\n    return averaged\n\ndef get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n                                                                                generate_missing=generate_missing))\n    return list(embeddings)","8d62a153":"training_embeddings = get_word2vec_embeddings(word2vec, clean_train_comments, generate_missing=True)\n# test_embeddings = get_word2vec_embeddings(word2vec, clean_test_comments, generate_missing=True)","91fcbc89":"tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\ntokenizer.fit_on_texts(clean_train_comments[\"text\"].tolist())\ntraining_sequences = tokenizer.texts_to_sequences(clean_train_comments[\"text\"].tolist())\n\ntrain_word_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(train_word_index))\n\ntrain_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n#print(train_cnn_data[:4])\n\ntrain_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n\nfor word,index in train_word_index.items():\n    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\nprint(train_embedding_weights[1])\nprint(\"-----------=====-----------\")\nprint(train_embedding_weights.shape)","c2969cfb":"#test_sequences = tokenizer.texts_to_sequences(clean_test_comments[\"text\"].tolist())\n#print(clean_test_comments[\"text\"][4])\n#print(test_sequences[4])\n#test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","dc682b62":"#I opted for splitting the train set in two parts : a small fraction (20%) became the validation set which the model is evaluated and the rest (80%) is used to train the model.\n\n#Since our dataset is not balanced (explain) , a random split of the train set causes some labels to be over represented in the validation set and we end up with an unbalanced dataset. A simple random split could cause inaccurate evaluation during the validation, hence to avoid that, we use stratify = True option in train_test_split function (**Only for >=0.17 sklearn versions**).\n# Split dataset into training and test\/validation\n\nfrom sklearn.model_selection import train_test_split\n# Set the random seed\nrandom_seed = 2\n\nY_train = onehot_encoded #(defined above)\n#X_train = clean_train_comments[\"tokens\"]\nX_train = train_embedding_weights\nprint(X_train[0])\nprint(Y_train[0])\nprint(clean_train_comments[\"cmp_code\"][0])\nprint(clean_train_comments[\"text\"][0])\nprint(clean_train_comments[\"tokens\"])\n\nprint(\"num X_train: \", X_train.shape[0])\nprint(\"num Y_train: \", Y_train.shape[0])\n# Split the train and the validation set for the fitting\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.2, random_state=random_seed)\n\n#print(X_val)\n#print(Y_val)\nprint(\"num start dataset: \", clean_train_comments.shape[0])\nprint(\"num X_val: \", X_val.shape[0])\nprint(\"num Y_val: \", Y_val.shape[0])\nprint(\"num X_train: \", X_train.shape[0])\nprint(\"num Y_train: \", Y_train.shape[0])\n\nprint(X_train)\n\n\n\n\n\n\n\n","9f95f966":"from keras.layers.merge import concatenate, add\n\ndef ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index, trainable=False, extra_conv=True):\n    #the filter\n    embedding_layer = Embedding(num_words,\n                            embedding_dim,\n                            weights=[embeddings],\n                            input_length=max_sequence_length,\n                            trainable=trainable)\n\n    #the unknown image\n    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n    #the merge function of the first convolution \n    embedded_sequences = embedding_layer(sequence_input)\n\n    # Yoon Kim model (https:\/\/arxiv.org\/abs\/1408.5882)\n    convs = []\n    filter_sizes = [3,4,5] # in the loop, first apply 3 as size, then 4 then 5\n\n    for filter_size in filter_sizes:\n        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n        #kernel is the filter\n        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n        convs.append(l_pool)\n\n    l_merge = concatenate(convs, axis=1)\n\n    \n    # activated if extra_convoluted is true at the def\n    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n    pool = MaxPooling1D(pool_size=3)(conv)\n\n    if extra_conv==True:\n        x = Dropout(0.5)(l_merge)  \n    else:\n        # Original Yoon Kim model\n        x = Dropout(0.5)(pool)\n    x = Flatten()(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    # Finally, we feed the output into a Sigmoid layer.\n    # The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0) \n    # for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1.\n    preds = Dense(labels_index, activation='sigmoid')(x)\n\n    model = Model(sequence_input, preds)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['acc'])\n    model.summary()\n    return model","5093f7d0":"x_train = train_cnn_data\ny_tr = y_train\nprint(len(list(label_names)))","6432945a":"model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, len(list(label_names)), False)","969e5bfa":"#define callbacks\nearly_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\ncallbacks_list = [early_stopping]","4e3e96b1":"# I opted for splitting the train set in two parts : a small fraction (20%) became the validation set which the model is \n# evaluated and the rest (80%) is used to train the model.\n# Since our dataset is not balanced (explain) , a random split of the train set causes some labels to be over represented \n# in the validation set and we end up with an unbalanced dataset. A simple random split could cause inaccurate evaluation \n# during the validation, hence to avoid that, we use stratify = True option in train_test_split function \n# (**Only for >=0.17 sklearn versions**).\n\n\n\n# Split dataset into training and test\/validation\n\nfrom sklearn.model_selection import train_test_split\n# Set the random seed\nrandom_seed = 2\n\nY_train = onehot_encoded #(defined above)\nX_train = clean_train_comments[\"tokens\"]\nprint(X_train[0])\nprint(Y_train[0])\nprint(clean_train_comments[\"cmp_code\"][0])\nprint(clean_train_comments[\"text\"][0])\n\nprint(\"num X_train: \", X_train.shape[0])\nprint(\"num Y_train: \", Y_train.shape[0])\n# Split the train and the validation set for the fitting\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.2, random_state=random_seed) # can also use random_state=random_seed as an option if dataset is balanced\n\n#print(X_val)\n#print(Y_val)\nprint(\"num start dataset: \", clean_train_comments.shape[0])\nprint(\"num X_val: \", X_val.shape[0])\nprint(\"num Y_val: \", Y_val.shape[0])\nprint(\"num X_train: \", X_train.shape[0])\nprint(\"num Y_train: \", Y_train.shape[0])","9d52197b":"hist = model.fit(x_train, y_tr, epochs=num_epochs, callbacks=callbacks_list, validation_split=0.1, shuffle=True, batch_size=batch_size)","1e095b35":"y_test = model.predict(test_cnn_data, batch_size=1024, verbose=1)\nprint(y_test)","e4780dd1":"#create a submission\nsubmission_df = pd.DataFrame(columns=['id'] + label_names)\nsubmission_df['id'] = test_comments['id'].values \nsubmission_df[label_names] = y_test \nsubmission_df.to_csv(\".\/cnn_submission.csv\", index=False)","017dcf2f":"#generate plots\nplt.figure()\nplt.plot(hist.history['loss'], lw=2.0, color='b', label='train')\nplt.plot(hist.history['val_loss'], lw=2.0, color='r', label='val')\nplt.title('CNN sentiment')\nplt.xlabel('Epochs')\nplt.ylabel('Cross-Entropy Loss')\nplt.legend(loc='upper right')\nplt.show()","ec36d1ac":"plt.figure()\nplt.plot(hist.history['acc'], lw=2.0, color='b', label='train')\nplt.plot(hist.history['val_acc'], lw=2.0, color='r', label='val')\nplt.title('CNN sentiment')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='upper left')\nplt.show()","0eb33544":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nfrom keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\nfrom keras.callbacks import Callback\nfrom keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\nfrom keras.preprocessing import text, sequence\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9303e455":"EMBEDDING_FILE = '..\/input\/googles-trained-word2vec-model-in-python\/GoogleNews-vectors-negative300.bin.gz'\ntrain = pd.read_csv('..\/input\/manifestos-aus\/train-aus.csv')\ntest = pd.read_csv('..\/input\/manifestos-aus\/test-aus.csv')","8ce43155":"train[\"text\"].fillna(\"fillna\")\ntest[\"text\"].fillna(\"fillna\")\nX_train = train[\"text\"].str.lower()\ny_train = train[['101', '102', '103', '104', '105', '106', '107', '108', '109', '201', '202', '203', '204', '301', '302', '303', '304', '305','401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '501', '502', '503', '504', '505', '506', '507', '601', '602', '603', '604', '605', '606', '607', '608', '701', '702', '703', '704', '705', '706', '707', '708']].values\n\nX_test = test[\"text\"].str.lower()","c933aaa1":"max_features=100000\nmaxlen=150\nembed_size=300","755bb932":"class RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))","c8d0c41b":"tok=text.Tokenizer(num_words=max_features,lower=True)\ntok.fit_on_texts(list(X_train)+list(X_test))\nX_train=tok.texts_to_sequences(X_train)\nX_test=tok.texts_to_sequences(X_test)\nx_train=sequence.pad_sequences(X_train,maxlen=maxlen)\nx_test=sequence.pad_sequences(X_test,maxlen=maxlen)","63b8543a":"embeddings_index = {}\nwith open(EMBEDDING_FILE,encoding='utf8') as f:\n    for line in f:\n        values = line.rstrip().rsplit(' ')\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs","1a62bfeb":"word_index = tok.word_index\n#prepare embedding matrix\nnum_words = min(max_features, len(word_index) + 1)\nembedding_matrix = np.zeros((num_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","3788bf19":"sequence_input = Input(shape=(maxlen, ))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(sequence_input)\nx = SpatialDropout1D(0.2)(x)\nx = Bidirectional(GRU(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\nx = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\navg_pool = GlobalAveragePooling1D()(x)\nmax_pool = GlobalMaxPooling1D()(x)\nx = concatenate([avg_pool, max_pool]) \n# x = Dense(128, activation='relu')(x)\n# x = Dropout(0.1)(x)\npreds = Dense(6, activation=\"sigmoid\")(x)\nmodel = Model(sequence_input, preds)\nmodel.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-3),metrics=['accuracy'])\nmodel.summary()","87ecbc41":"batch_size = 128\nepochs = 4\nX_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=233)","e31e181d":"# filepath=\"..\/input\/best-model\/best.hdf5\"\n#filepath=\"weights_base.best.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nearly = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\nra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval = 1)\ncallbacks_list = [ra_val,checkpoint, early]","5afbd968":"model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),callbacks = callbacks_list,verbose=1)\n#Loading model weights\n#model.load_weights(filepath) #try this with and without; with is bi-LSTM with convolution\nprint('Predicting....')\ny_pred = model.predict(x_test,batch_size=1024,verbose=1)","ddb7c42e":"# Write scores to file\nsubmission = pd.read_csv('..\/input\/manifestos-aus\/sample_submission.csv')\nsubmission[['101', '102', '103', '104', '105', '106', '107', '108', '109', '201', '202', '203', '204', '301', '302', '303', '304', '305','401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '501', '502', '503', '504', '505', '506', '507', '601', '602', '603', '604', '605', '606', '607', '608', '701', '702', '703', '704', '705', '706', '707', '708']] = y_pred\nsubmission.to_csv('submission.csv', index=False)","07b52725":"## LSTM (bidirectional RNN) & Word2Vec\nUsing the trained word to vector datasets, this section will classify the test sentences using a type of Recurrent Neural Network (Long Short Term Model) and Word2Vec, using Keras libraries.","0420874e":"**Cleaning Text**","497146aa":"Define a Convolutional Neural Network following Yoon Kim model [2]","1d8fd9f6":"In this Kernel uses Manifestos data [reference here], and presents a pipeline of using Deep Learning Algorithms to classify manifesto topics [info about the types of domains]. In the current version the following types of Neural Network Algorithms have been implemented:\n* **Convolutional Neural Networks (CNN)** (Kim, 2014) with **Word2Vec** (Google) \n* **Long Short Term Memory (LSTM)** Recurrent Neural Networks, with **Word2Vec** (Google)\n\n## CNN & Word2Vec Implementation\nThe general logic behind CNNs is presented in Kim (2014).  To use CNNs for sentence classification, imagine sentences and words as image pixels, where the input is sentences are represented as a matrix. \n\nEach row of the matrix is a vector that represents a sentence. \n\nThis vector is the average of  **word2vec** (Google\u2019s Word2Vec pre-trained model) scores of all words in our sentence.\n\nFor 10 sentences using a 300-dimensional embedding we would have a 10\u00d7300 matrix as our input. \nThat\u2019s our \u201cimage\u201d.\n\nFor computational reasons, the number of steps\/passes (epochs) has been set to 2 throughout. For improved accuracy set it to 20+ (I've seen it done up to 30), but you will need to run this on a separate server than kaggle (aws or locally with strong processors).\n","83003ceb":"Now let's train our Neural Network","569e588c":"**References**:   \n* [1] How to solve 90% of NLP problems: a step-by-step guide\n * https:\/\/blog.insightdatascience.com\/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e\n* [2] Yoon Kim model\n * https:\/\/arxiv.org\/abs\/1408.5882\n* [3] Understanding Convolutional Neural Networks for NLP:\n * http:\/\/www.wildml.com\/2015\/11\/understanding-convolutional-neural-networks-for-nlp\/","6997e95b":"Word2vec is a model that was pre-trained on a very large corpus, and provides embeddings that map words that are similar close to each other. A quick way to get a sentence embedding for our classifier, is to average word2vec scores of all words in our sentence. In this way we lose the syntax of our sentence, while keeping some semantic information.\n![](https:\/\/cdn-images-1.medium.com\/max\/1400\/1*THo9NKchWkCAOILvs1eHuQ.png)","fd4c029d":"Note that we set the _num_epochs_ is low on purpose, so as not to exceed the 14GB RAM of training of the Word2Vec operation later on. For comparison purposes, it might actually be best to do at least 10, so that the histogram gives some more data points.","b71de23d":"**Tokenizing Text**"}}