{"cell_type":{"36d18c65":"code","3da98ec7":"code","eed2897f":"code","3be85fee":"code","fa421cfe":"code","5f94241e":"code","4944b82e":"code","37cf3518":"code","0bd7ad29":"code","6e25349a":"markdown","7e2a28ed":"markdown","10a0f5dc":"markdown","7b7ee792":"markdown","61f8c852":"markdown","34007022":"markdown","1bbbfc24":"markdown"},"source":{"36d18c65":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3da98ec7":"train_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-spring-21-assignment-1\/train.csv\")\ntrain_data.head()","eed2897f":"test_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-spring-21-assignment-1\/test.csv\")\ntest_data.head()","3be85fee":"import matplotlib.pyplot as plt\n%matplotlib inline\n\n# Boxplots as a visual check for any outliers in the training data.\nfor column in train_data:\n    plt.figure()\n    train_data.boxplot([column])","fa421cfe":"# Removing binary values temporarily to remove outliers from applicable data in the dataset.\n#filtered_train = train_data.loc[:, train_data.columns != 'id']\n#filtered_train = filtered_train.loc[:, filtered_train.columns != 'Bankrupt']\n#filtered_train = filtered_train.loc[:, filtered_train.columns != 'one if net income was negative for the last two year zero otherwise']\n#filtered_train = filtered_train.loc[:, filtered_train.columns != 'one if total liabilities exceeds total assets zero otherwise']\n\n# Sets the lower and upper quartiles for each column.\n#low = .0025\n#high = .9975\n#quant_train = filtered_train.quantile([low, high])\n#print(quant_train )\n\n# Removes the values outside of our set quartiles.\n#filtered_train = filtered_train.apply(lambda x: x[(x>quant_train.loc[low,x.name]) & (x < quant_train.loc[high,x.name])], axis=0)\n\n# Restores the columns that were taken out prior to outlier removal to the dataset.\n#filtered_train = pd.concat([train_data.loc[:,'one if total liabilities exceeds total assets zero otherwise'], filtered_train], axis=1)\n#filtered_train = pd.concat([train_data.loc[:,'one if net income was negative for the last two year zero otherwise'], filtered_train], axis=1)\n#filtered_train = pd.concat([train_data.loc[:,'Bankrupt'], filtered_train], axis=1)\n#filtered_train = pd.concat([train_data.loc[:,'id'], filtered_train], axis=1)","5f94241e":"# Used the following to check for any NaN values in the given dataframe.\nnans = train_data.isnull().sum()\nif(np.all((nans == 0))) : print(\"No NaN values\")\nelse                    : print(\"Replacing NaN values with zeroes...\"); train_data = train_data.fillna(0)","4944b82e":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nfeatures = test_data.columns\ntrain_data = train_data.drop(train_data.index[3409])\ny_train = train_data['Bankrupt']\n\nx_train, x_test, y_train, y_test = train_test_split(train_data[features], y_train, test_size=.25, random_state=42, stratify=y_train)\n\ntree_model = DecisionTreeClassifier(criterion=\"entropy\", max_depth=7, splitter=\"best\")\ntree_model.fit(x_train, y_train)\n\nforest_model = RandomForestClassifier(random_state = 1,n_estimators = 75,min_samples_split = 2) \nforest_model.fit(x_train, y_train)\n","37cf3518":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\n# Generates predictions based off the training data on our model.\ntree_predictions = tree_model.predict_proba(x_train)[:, 1]\nforest_predictions = forest_model.predict_proba(x_train)[:, 1]\n\n# Computes all the accuracy \/ f1 \/ roc metrics.\nacc_tree = accuracy_score(y_train, tree_predictions.round())\nacc_forest = accuracy_score(y_train, tree_predictions.round())\nf1_tree = f1_score(y_train, tree_predictions.round())\nf1_forest = f1_score(y_train, forest_predictions.round())\nroc_tree = roc_auc_score(y_train, tree_predictions)\nroc_forest = roc_auc_score(y_train, forest_predictions)\n\n# Prints out the metrics we computed above.\nprint(\"Best Tree Model Accuracy: \" + str(acc_tree))\nprint(\"Best Forest Model Accuracy: \" + str(acc_forest))\n\nprint(\"Best Tree Model F1: \" + str(f1_tree))\nprint(\"Best Forest Model F1: \" + str(f1_forest))\n\nprint(\"Best Tree Model ROC: \" + str(roc_tree))\nprint(\"Best Forest Model ROC: \" + str(roc_forest))","0bd7ad29":"# Determines which model to use for our submission based on the ROC scores.\nif (roc_tree > roc_forest): predictions = tree_model.predict_proba(test_data)[:, 1]\nelse:                       predictions = forest_model.predict_proba(test_data)[:, 1]\n\n# Outputs the chosen predictions to a CSV to be submitted.\noutput = pd.DataFrame({'id': test_data.id, 'Bankrupt': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","6e25349a":"# This section gives us box plots of the training data to provide a visual for any outliers in the data.\n","7e2a28ed":"# The following section generates predictions and prints out our models respective metrics.","10a0f5dc":"# This final section outputs our best model's predictions of the test data to a csv file.","7b7ee792":"# The following sections read in the data into pandas dataFrames.","61f8c852":"# **The following section removes outliers from the training data using lower and upper quartiles.**\n\n**NOTE**: I've commented this section out because removing the outliers using lower and upper quartiles the way I did lowered my accuracy dramatically. Also because this section is commented out I have changed the NaN removal sections dataframe name to train_data since filtered_train was created in this now commented out code below.","34007022":"# The following section splits our data and sets up our models.\nI chose not to implement normalization, or rather, I implemented scaling in later versions and it brought my accuracy down so I removed it.\n\n","1bbbfc24":"# **The following section checks for NaN values in the training data, and if they are found they're replaced with 0's.**"}}