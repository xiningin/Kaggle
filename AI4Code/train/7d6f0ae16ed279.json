{"cell_type":{"e04fe677":"code","88cb9679":"code","92ec9285":"code","3e214505":"code","1b09e3ec":"code","4cf2c766":"code","332cb824":"code","2ea0bc66":"code","a5ede98e":"code","d92b0cb1":"code","5aca3039":"code","d30a8450":"code","82f3f79d":"code","fb5e3563":"code","7c057357":"code","52bf69ec":"code","01c720e6":"code","71e3330e":"code","0b1a816a":"code","78f01c3b":"code","d84001b1":"code","7b030c5c":"code","2f58ec52":"code","fd28bb57":"code","f8a09d41":"code","934df449":"code","27136d8c":"code","9d2d25a3":"code","c90a0966":"code","903bad44":"code","4d129fb9":"code","e1ae97d9":"code","3071ee5a":"code","d328b503":"code","7d6aac9a":"code","96bcbe33":"code","f18fc485":"markdown","d95f2971":"markdown","920378d3":"markdown","4f92b822":"markdown","331244d5":"markdown","5dfb26ce":"markdown","da90c8d8":"markdown","af6c0b4c":"markdown","b35dd896":"markdown","ad38147a":"markdown","38da2416":"markdown","b4b9a786":"markdown","bcd69173":"markdown","85fab663":"markdown","374e8bdd":"markdown","e7aad6a0":"markdown","07ceb893":"markdown","94a84cda":"markdown","ec450950":"markdown","deb3bf53":"markdown"},"source":{"e04fe677":"import joblib\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nfrom sklearn import metrics\nfrom sklearn import tree\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')","88cb9679":"TRAINING_PATH='..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv'\n\ndf=pd.read_csv(TRAINING_PATH)\ndf.head()","92ec9285":"TRAINING_FOLDS_PATH='.\/'\n\nimport pandas as pd\n\ndf_train=pd.read_csv(TRAINING_PATH)\ndf_train.head()\n\ndf_train['stroke'].value_counts()\n\ndf_train['kfolds']=-1\ndf_train=df_train.sample(frac=1).reset_index(drop=True)\ndf_train.head()\n\nfrom sklearn import model_selection\n\nstrat_kf=model_selection.StratifiedKFold(n_splits=5)\n\nfor fold,(trn_,val_) in enumerate(strat_kf.split(X=df_train,y=df_train['stroke'])):\n  df_train.loc[val_,'kfolds']=fold\ndf_train.head()\n\ndf_train.to_csv(TRAINING_FOLDS_PATH+'train_folds.csv')","3e214505":"TRAINING_PATH='.\/train_folds.csv'\nMODEL_PATH='.\/'\nSUBMISSION_FILES_PATH='.\/Submissions\/'","1b09e3ec":"df=pd.read_csv(TRAINING_PATH)\ndf.head()","4cf2c766":"df.describe()","332cb824":"df.isna().sum()","2ea0bc66":"len(df)","a5ede98e":"df['bmi'].fillna(int(df['bmi'].mean()),inplace=True)","d92b0cb1":"df.isna().sum()","5aca3039":"df = df.drop(['Unnamed: 0'],axis=1)\ndf.head()","d30a8450":"# Total number of unique values in each column\ndf.nunique()","82f3f79d":"# Checking for any numerical data. If present, it has to be scaled etc.\n\ncolumns = df.columns\nnumerical_columns = df._get_numeric_data().columns\nnumerical_columns","fb5e3563":"from sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\ndf_2 = df.loc[:,['age','avg_glucose_level','bmi']]\ndf.loc[:,df_2.columns] = pd.DataFrame(scaler.fit_transform(df_2),index=df.index,columns=df_2.columns)\ndf.head()","7c057357":"import seaborn as sns\nsns.set_theme(style=\"darkgrid\")\nax = sns.countplot(x=\"smoking_status\", data=df)","52bf69ec":"# One hot encode the categorical columns - smoking_status\n\ndf=pd.get_dummies(data=df,columns=['smoking_status','gender','work_type','Residence_type'])\ndf.head()","01c720e6":"# Move the target and kfolds column to the last\n\ndf=df[[column for column in df if column not in['stroke','kfolds']]+['stroke','kfolds']]\ndf.head()","71e3330e":"df['ever_married'] = df['ever_married'].replace({'No':0,'Yes':1})\ndf.head()","0b1a816a":"def run(fold,df,models,target_name, save_model, print_details=False):\n  \n  # print(df.head())\n  # Training and validation sets\n  df_train=df[df['kfolds']!=fold].reset_index(drop=True)\n  df_valid=df[df['kfolds']==fold].reset_index(drop=True)\n\n\n  # x and y of training dataset\n  x_train=df_train.drop(target_name,axis=1).values\n  y_train=df_train[target_name].values\n\n  # x and y of validation dataset\n  x_valid=df_valid.drop(target_name,axis=1).values\n  y_valid=df_valid[target_name].values\n\n  # accuracy => will store accuracies of the models  (same for confusion_matrices)\n  accuracy=[]\n  confusion_matrices=[]\n  classification_report=[]\n\n  for model_name,model_constructor in list(models.items()):\n    clf=model_constructor\n    clf.fit(x_train,y_train)\n\n    # preds_train, preds_valid => predictions when training and validation x are fed into the trained model\n    preds_train=clf.predict(x_train)\n    preds_valid=clf.predict(x_valid)\n\n    acc_train=metrics.accuracy_score(y_train,preds_train)\n    acc_valid=metrics.accuracy_score(y_valid,preds_valid)\n\n    f1_train = metrics.f1_score(y_train,preds_train)\n    f1_valid = metrics.f1_score(y_valid,preds_valid)\n\n    conf_matrix=metrics.confusion_matrix(y_valid,preds_valid)\n    class_report=metrics.classification_report(y_valid,preds_valid)\n\n    accuracy.append(acc_valid)\n    confusion_matrices.append(conf_matrix)\n    classification_report.append(class_report)\n\n    if(print_details==True):\n      print(f'Model => {model_name} => Fold = {fold} => Training Accuracy = {acc_train} => Validation Accuracy = {acc_valid}')\n\n    if(save_model==True):\n      joblib.dump(clf, f\"{MODEL_PATH}{model_name}_F1_{f1_valid}_ACC_{acc_valid}_FOLD_{fold}.bin\")\n\n  if(print_details==True):\n    print('\\n--------------------------------------------------------------------------------------------\\n')\n    \n  return accuracy,confusion_matrices,classification_report","78f01c3b":"def greedy_feature_selection(fold,df,models,target_name):\n\n  # target_index => stores the index of the target variable in the dataset\n  # kfolds_index => stores the index of kfolds column in the dataset\n\n  target_index=df.columns.get_loc(target_name)\n  kfolds_index=df.columns.get_loc('kfolds')\n\n  # good_features => stores the indices of all the optimal features\n  # best_scores => keeps track of the best scores \n  good_features=[]\n  best_scores=[]\n\n  # df has X and y and a kfolds column. \n  # no of features (no of columns in X) => total columns in df - 1 (there's 1 y) - 1 (there's 1 kfolds)\n  num_features=df.shape[1]-2\n\n  while True:\n\n    # this_feature => the feature added to the already selected features to measure the effect of the former on the model\n    # best_score => keeps track of the best score achieved while selecting features 1 at a time and checking its effect on the model\n    this_feature=None\n    best_score=0\n\n\n    for feature in range(num_features):\n\n      # if the feature is already in the good_features list, ignore and move ahead\n      if feature in good_features:\n        continue\n      \n      # add the currently selected feature to the already discovered good features\n      selected_features=good_features+[feature]\n\n      # all the selected features + target and kfolds column\n      df_train=df.iloc[:, selected_features + [target_index,kfolds_index]]\n\n      # fit the selected dataset to a model \n      accuracy,confusion_matrices,classification_report=run(fold,df_train,models,save_model= False, target_name=target_name)\n\n      # if any improvement is observed over the previous set of features\n      if(accuracy[0]>best_score):\n        this_feature=feature\n        best_score=accuracy[0]\n      \n    if(this_feature!=None):\n      good_features.append(this_feature)\n      best_scores.append(best_score)\n    \n    if(len(best_scores)>2):\n      if(best_scores[-1]<best_scores[-2]):\n        break\n    \n  return best_scores[:-1] , df.iloc[:, good_features[:-1] + [target_index,kfolds_index]]","d84001b1":"print('Greedy Feature Selection : ')\nprint('\\n')\nmodels={'XGB': XGBClassifier()}\nbest_scores,df_optimal_XGB=greedy_feature_selection(fold=4,df=df,models=models,target_name='stroke')\nprint(df_optimal_XGB.head())","7b030c5c":"models={'RFC' : RandomForestClassifier()}\nbest_scores,df_optimal_RFC=greedy_feature_selection(fold=4,df=df,models=models,target_name='stroke')\nprint(df_optimal_RFC.head())\n","2f58ec52":"models={'DT' : DecisionTreeClassifier()}\nbest_scores,df_optimal_DT=greedy_feature_selection(fold=4,df=df,models=models,target_name='stroke')\nprint(df_optimal_DT.head())","fd28bb57":"import optuna\nfrom functools import partial\n\ndef optimize_rfc(trial,df,total_folds,target_name):\n    criterion = trial.suggest_categorical(\"criterion\", ['gini','entropy'])\n    n_estimators = trial.suggest_int('n_estimators', 100, 1500)\n    max_depth = trial.suggest_int(\"max_depth\", 3, 30)\n    max_features = trial.suggest_uniform(\"max_features\", 0.01, 1.0)\n    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 100)\n    \n    model = RandomForestClassifier(\n        n_estimators = n_estimators, \n        max_depth = max_depth, \n        max_features = max_features, \n        min_samples_leaf = min_samples_leaf,\n        min_samples_split = min_samples_split,\n        criterion = criterion\n    )\n    \n    accuracies = []\n    \n    for fold in range(total_folds):\n        \n        df_train=df[df['kfolds']!=fold].reset_index(drop=True)\n        df_valid=df[df['kfolds']==fold].reset_index(drop=True)\n\n\n        # x and y of training dataset\n        x_train=df_train.drop(target_name,axis=1).values\n        y_train=df_train[target_name].values\n\n        # x and y of validation dataset\n        x_valid=df_valid.drop(target_name,axis=1).values\n        y_valid=df_valid[target_name].values\n        \n        model.fit(x_train, y_train)\n        preds= model.predict(x_valid)\n        \n        fold_acc = metrics.accuracy_score(y_valid, preds)\n        accuracies.append(fold_acc)\n        \n    return np.mean(accuracies)\n\noptimization_function_rfc = partial(optimize_rfc, df = df_optimal_RFC, total_folds = 5,target_name = 'stroke')\nstudy_rfc = optuna.create_study(direction = 'maximize')\nstudy_rfc.optimize(optimization_function_rfc, n_trials=15)","f8a09d41":"rfc_best_params = study_rfc.best_trial.params\nrfc_best_params","934df449":"def optimize_xgb(trial,df,total_folds,target_name):\n    \n    learning_rate = trial.suggest_uniform(\"learning_rate\", 0.01, 1.0)\n    gamma = trial.suggest_uniform(\"gamma\", 0.05, 1.0)\n    max_depth = trial.suggest_int(\"max_depth\", 3, 30)\n    min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 10)\n    subsample = trial.suggest_uniform(\"subsample\", 0.5, 1.0)\n    colsample_bytree = trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0)\n    reg_lambda = trial.suggest_uniform(\"reg_lambda\", 0.01, 1.0)\n    reg_alpha = trial.suggest_uniform(\"reg_alpha\", 0.01, 1.0)\n    \n    model = XGBClassifier(\n        learning_rate = learning_rate,\n        gamma = gamma,\n        max_depth = max_depth,\n        min_child_weight = min_child_weight,\n        subsample = subsample,\n        colsample_bytree = colsample_bytree,\n        reg_lambda = reg_lambda,\n        reg_alpha = reg_alpha\n    )\n    \n    accuracies = []\n    \n    for fold in range(total_folds):\n        \n        df_train=df[df['kfolds']!=fold].reset_index(drop=True)\n        df_valid=df[df['kfolds']==fold].reset_index(drop=True)\n\n\n        # x and y of training dataset\n        x_train=df_train.drop(target_name,axis=1).values\n        y_train=df_train[target_name].values\n\n        # x and y of validation dataset\n        x_valid=df_valid.drop(target_name,axis=1).values\n        y_valid=df_valid[target_name].values\n        \n        model.fit(x_train, y_train)\n        preds= model.predict(x_valid)\n        \n        fold_acc = metrics.accuracy_score(y_valid, preds)\n        accuracies.append(fold_acc)\n        \n    return np.mean(accuracies)\n\noptimization_function_xgb = partial(optimize_xgb, df = df_optimal_XGB, total_folds = 5,target_name = 'stroke')\nstudy_xgb = optuna.create_study(direction = 'maximize')\nstudy_xgb.optimize(optimization_function_xgb, n_trials=15)","27136d8c":"xgb_best_params = study_xgb.best_trial.params\nxgb_best_params","9d2d25a3":"def optimize_svc(trial,df,total_folds,target_name):\n    \n    C = trial.suggest_uniform(\"C\", 0.001, 1000)\n    gamma = trial.suggest_categorical(\"gamma\", ['auto'])\n    class_weight = trial.suggest_categorical(\"class_weight\", ['balanced'])\n    \n    model = SVC(\n        C = C,\n        gamma = gamma,\n        class_weight = class_weight\n    )\n    \n    accuracies = []\n    \n    for fold in range(total_folds):\n        \n        df_train=df[df['kfolds']!=fold].reset_index(drop=True)\n        df_valid=df[df['kfolds']==fold].reset_index(drop=True)\n\n\n        # x and y of training dataset\n        x_train=df_train.drop(target_name,axis=1).values\n        y_train=df_train[target_name].values\n\n        # x and y of validation dataset\n        x_valid=df_valid.drop(target_name,axis=1).values\n        y_valid=df_valid[target_name].values\n        \n        model.fit(x_train, y_train)\n        preds= model.predict(x_valid)\n        \n        fold_acc = metrics.accuracy_score(y_valid, preds)\n        accuracies.append(fold_acc)\n        \n    return np.mean(accuracies)\n\noptimization_function_svc = partial(optimize_svc, df = df, total_folds = 5,target_name = 'stroke')\nstudy_svc = optuna.create_study(direction = 'maximize')\nstudy_svc.optimize(optimization_function_svc, n_trials=15)","c90a0966":"svc_best_params = study_svc.best_trial.params\nsvc_best_params","903bad44":"def optimize_dt(trial,df,total_folds,target_name):\n    criterion = trial.suggest_categorical(\"criterion\", ['gini','entropy'])\n    max_depth = trial.suggest_int(\"max_depth\", 3, 30)\n    max_features = trial.suggest_uniform(\"max_features\", 0.01, 1.0)\n    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 100)\n    \n    model = DecisionTreeClassifier(\n        max_depth = max_depth, \n        max_features = max_features, \n        min_samples_leaf = min_samples_leaf,\n        min_samples_split = min_samples_split,\n        criterion = criterion\n    )\n    \n    accuracies = []\n    \n    for fold in range(total_folds):\n        \n        df_train=df[df['kfolds']!=fold].reset_index(drop=True)\n        df_valid=df[df['kfolds']==fold].reset_index(drop=True)\n\n\n        # x and y of training dataset\n        x_train=df_train.drop(target_name,axis=1).values\n        y_train=df_train[target_name].values\n\n        # x and y of validation dataset\n        x_valid=df_valid.drop(target_name,axis=1).values\n        y_valid=df_valid[target_name].values\n        \n        model.fit(x_train, y_train)\n        preds= model.predict(x_valid)\n        \n        fold_acc = metrics.accuracy_score(y_valid, preds)\n        accuracies.append(fold_acc)\n        \n    return np.mean(accuracies)\n\noptimization_function_dt = partial(optimize_dt, df = df_optimal_DT, total_folds = 5,target_name = 'stroke')\nstudy_dt = optuna.create_study(direction = 'maximize')\nstudy_dt.optimize(optimization_function_dt, n_trials=15)","4d129fb9":"dt_best_params = study_dt.best_trial.params\ndt_best_params","e1ae97d9":"XGB_model=XGBClassifier(**xgb_best_params)\nSVM_model=SVC(**svc_best_params)\nRFC_model=RandomForestClassifier(**rfc_best_params)\nDT_model=DecisionTreeClassifier(**dt_best_params)","3071ee5a":"models={\n    'Random Forest Classifier' : RFC_model\n    }\n\naccuracies,confusion_matrices,classification_reports=[],[],[]\nfor f in range(5):\n  accuracy,confusion_matrix,classification_report=run(f,df_optimal_RFC,models=models,target_name='stroke', save_model= True, print_details=True)\n  accuracies.append(accuracy)\n  confusion_matrices.append(confusion_matrix)\n  classification_reports.append(classification_report)","d328b503":"models={\n    'Decision Tree Classifier' : DT_model\n    }\n\naccuracies,confusion_matrices,classification_reports=[],[],[]\nfor f in range(5):\n  accuracy,confusion_matrix,classification_report=run(f,df_optimal_DT,models=models,target_name='stroke', save_model= True, print_details=True)\n  accuracies.append(accuracy)\n  confusion_matrices.append(confusion_matrix)\n  classification_reports.append(classification_report)","7d6aac9a":"models={\n    'SVM Classifier' : SVM_model\n    }\n\naccuracies,confusion_matrices,classification_reports=[],[],[]\nfor f in range(5):\n  accuracy,confusion_matrix,classification_report=run(f,df,models=models,target_name='stroke', save_model= True, print_details=True)\n  accuracies.append(accuracy)\n  confusion_matrices.append(confusion_matrix)\n  classification_reports.append(classification_report)","96bcbe33":"models={\n    'XGB Classifier' : XGB_model\n    }\n\naccuracies,confusion_matrices,classification_reports=[],[],[]\nfor f in range(5):\n  accuracy,confusion_matrix,classification_report=run(f,df_optimal_XGB,models=models,target_name='stroke', save_model= True, print_details=True)\n  accuracies.append(accuracy)\n  confusion_matrices.append(confusion_matrix)\n  classification_reports.append(classification_report)","f18fc485":"# \ud83d\udccb<b>Data Exploration<\/b>\n","d95f2971":"# \ud83d\udd0e Finding the best hyperparameters for the models using Optuna Library for <b>Hyperparameter Tuning<\/b>\n\n### <b>Models Considered:<\/b>\n### 1. XGBoost Classifier\n### 2. Random Forest Classifier\n### 3. Decision Tree Classifier\n### 4. SVM Classifier\n### -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","920378d3":"### 4. Decision Tree Classifier","4f92b822":"### 2. XGBoost Classifier","331244d5":"## If you like my work, an upvote would be great!","5dfb26ce":"## Handling the Categorical Features\n\n### 1. Features like smoking_status, gender, work_type and Residence_type have to be <b>One Hot Encoded<\/b> since they don't denote any Ordinal data (ie. the values don't denote any rank)\n### 2. The ever_married feature can be <b>Label Encoded<\/b> as the value is either Yes or No so we can encode them as 1 and 0 respectively.","da90c8d8":"### 3. Decision Tree Classifier","af6c0b4c":"# Creating 5 <b>Stratified K Fold cross validation<\/b> sets for better testing","b35dd896":"### As we can see that BMI has 201 null values out of a total of 5110 which need to be handled. Null values can be handled by either dropping those rows entirely or by replacing them with a constant value like mean, median or mode. I proceeded with replacing the NaNs with the mean value.","ad38147a":"# Importing Libraries","38da2416":"### 1. XGBoost Classifier","b4b9a786":"### 1. Random Forest Classifier","bcd69173":"## Scaling the Continuous Variables using <b>MinMaxScaler<\/b>","85fab663":"### <b>Greedy Feature Selection<\/b> - After choosing a model and scoring function (here, accuracy); we take a feature iteratively and if that feature improves the score then only it is kept in our optimal feature dataset. Hence, the optimal dataset can be different for different models.\n\n### <b>Models Considered:<\/b>\n### 1. XGBoost Classifier\n### 2. Random Forest Classifier\n### 3. Decision Tree Classifier\n#### Note: SVM Classifier was taking time for feature Selection so the entire dataset was considered the optimal dataset for SVM","374e8bdd":"### 3. SVM Classifier","e7aad6a0":"# Conclusion:\n\n## Random Forest, Decision Tree and XGBoost Classifier all performed relatively same with their accuracies being 95% for both train and validation set. Hence no overfitting was observed.\n## SVM had an accuracy of 94% on the validation set. This (slightly lower accuracy than the other 3 models) could be attributed to the fact that feature selection could not be applied because of time constraint. However, an important thing to note is that there was a bit of overfitting here as training accuracy was surprisingly 100% but validation accuracy was 94%. ","07ceb893":"## Handling Null Values","94a84cda":"# Now its time to Run the Models with their best hyperparameters!","ec450950":"# \u2705 Let's choose the optimal features from the dataset using some <b>Feature Selection<\/b> techniques","deb3bf53":"### 2. Random Forest Classifier"}}