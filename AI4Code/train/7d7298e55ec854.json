{"cell_type":{"03dba9da":"code","4c870827":"code","9923dedd":"code","1027ce86":"code","5ac75f50":"code","316f903d":"code","8837cfa3":"code","2362f755":"code","6aba7772":"code","3c5dd41b":"code","25792320":"code","04bcda82":"code","ce8b4226":"code","4110ab82":"code","e9b685f2":"code","376b8bfd":"code","a5ad8eae":"code","0f717db0":"code","f1038411":"code","786efe19":"code","71f73c7b":"code","087d3e0e":"code","231f9643":"code","3e201ae1":"code","838eb59d":"code","4a113641":"code","835db465":"code","0c5fc880":"code","ed9a9e0b":"code","89569a52":"markdown","7a0a79b9":"markdown","b0a78ff0":"markdown","12cc575c":"markdown","9346cd8d":"markdown","6a7f4b0d":"markdown","94c27612":"markdown","9657932c":"markdown","e89a8c02":"markdown","bb62cc14":"markdown","f3bc49c1":"markdown","d2af4982":"markdown","96951b44":"markdown","68dda7cb":"markdown","988bb67b":"markdown","3bbf432a":"markdown"},"source":{"03dba9da":"%config Completer.use_jedi = False\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nimport numpy as np\nimport matplotlib.pyplot as plt \nfrom sklearn.manifold import MDS\nfrom sklearn import preprocessing\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns","4c870827":"\nDataMatrix = np.load(\"..\/input\/unsupervisedgenesdata\/X.npy\")\n#Log Transformed Data\nDataMatrix = np.log2(DataMatrix+1)\nprint(DataMatrix.shape)\n","9923dedd":"df = pd.DataFrame(DataMatrix)\ndf[0].describe()","1027ce86":"# Apply PCA\n\npca = PCA()\npca.fit(DataMatrix)\n# Variance Explained by 1st Component\nVariance1stcomp = pca.explained_variance_ratio_[0]\nprint(\"Log Transformed Data : Variance Explained By 1st Principle  Component {} \".format(Variance1stcomp))\n\ncompo = [i+1 for i in range(DataMatrix.shape[0])]\ncdf = np.cumsum(pca.explained_variance_ratio_)\n\n\nplt.plot(compo,cdf,label=\"cdf\")\nplt.xlabel(\"Components\")\nplt.ylabel(\"Variance Explained\")\nplt.title(\"Variance Explained vs Number of Componenet  Data\")\nplt.legend()\nplt.show()","5ac75f50":"# keeping 1250 components that explained 85% variance\nTransformed_data = pca.transform(DataMatrix)[:,:1250]\nplt.scatter(Transformed_data[:,0],Transformed_data[:,1])","316f903d":"# TSNE for Data Visulization\nTransformed_embedded = TSNE()\ntsne_transformation = Transformed_embedded.fit_transform(Transformed_data)\n\nplt.scatter(tsne_transformation[:,0],tsne_transformation[:,1])","8837cfa3":"# TSNE with pcs initialization\nTransformed_embedded = TSNE(init='pca')\ntsne_transformation = Transformed_embedded.fit_transform(Transformed_data)\n\nplt.scatter(tsne_transformation[:,0],tsne_transformation[:,1])","2362f755":"# Kmeans clusteting using TSNE Transformation\n\nkmeans = KMeans(n_clusters=3, random_state=0).fit(tsne_transformation[:,:50])\nplt.scatter(tsne_transformation[:,0], tsne_transformation[:,1], c=kmeans.labels_)","6aba7772":"#Multi dimensional  scaling plotting\nembedding = MDS(n_components=2)\nmds_log_transformed = embedding.fit_transform(DataMatrix[:,:1250])\n# plotting first two component\nplt.scatter(mds_log_transformed[:,0],mds_log_transformed[:,1] )\nplt.title(\"MDS on Log Transformed Data\")","3c5dd41b":"# Kmeans clusteting using TSNE Transformation\n\nkmeans = KMeans(n_clusters=3, random_state=0).fit(mds_log_transformed[:,:50])\nplt.scatter(mds_log_transformed[:,0], mds_log_transformed[:,1], c=kmeans.labels_)\nplt.savefig('kmeans_mds.png')","25792320":"clusterCounter = [i for i in range(1,6)]\nWGSS=[]\nfor _c in clusterCounter:\n    \n    kmeans = KMeans(n_clusters=_c, random_state=0).fit(tsne_transformation)\n    \n    WGSS.append(kmeans.inertia_)\nplt.plot(clusterCounter, WGSS)","04bcda82":"kmeans = KMeans(n_clusters=3, random_state=0).fit(tsne_transformation)\nplt.scatter(tsne_transformation[:,0], tsne_transformation[:,1], c=kmeans.labels_)\n\n\nprediction = kmeans.predict(tsne_transformation)\n\n#print(tsne_transformation.shape)","ce8b4226":"X = DataMatrix\nY = prediction\npenalties = ['l1','l2','elasticnet']\nl1_ratio = [0.25,0.1, 0.2,0.4,0.3,0.5, 0.6]\n# l1_ratio=[.25]\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\nprint(\"Train Data shape {} \".format(X_train.shape))\nprint(\"Test Data shape {} \".format(X_test.shape))","4110ab82":"classfier_dict = {} \nfor i in penalties:\n    if i == 'elasticnet':\n        for r in l1_ratio:\n            clf = LogisticRegression(random_state=0, penalty=i,l1_ratio=r, multi_class='ovr', solver='saga', max_iter=200).fit(X_train, y_train)\n            score = clf.score(X_test,y_test)\n            if i not in classfier_dict:\n                classfier_dict[i]=[]\n            classfier_dict[i].append({'score':score, 'ratio':r, 'model':clf,'penalty':i })\n        \n    else:\n        clf = LogisticRegression(random_state=0, penalty=i, multi_class='ovr' , solver='liblinear', max_iter=200).fit(X_train, y_train)\n        score = clf.score(X_test,y_test)\n        if i not in classfier_dict:\n            classfier_dict[i]=[]\n        classfier_dict[i].append({'score':score, 'penalty':i, 'model':clf })\n        \n","e9b685f2":"classfier_dict","376b8bfd":"Finalclf = LogisticRegression(random_state=10, penalty='l1', multi_class='ovr', solver='liblinear').fit(X_train, y_train)\nscore = Finalclf.score(X_test,y_test)\nscore","a5ad8eae":"trainX = np.load(\"..\/input\/genecvdata\/X_train.npy\")\ntrainY = np.load(\"..\/input\/genecvdata\/y_train.npy\")\ntestX = np.load(\"..\/input\/genecvdata\/X_test.npy\")\ntestY = np.load(\"..\/input\/genecvdata\/y_test.npy\")\ntrainX = np.log2(trainX+1)\ntestX = np.log2(testX+1)\nprint(\"Train Data shape {}\".format(trainX.shape))\n\nprint(\"Test Data shape {}\".format(testX.shape))","0f717db0":"## Baseline Model Random 100 features\n# take 100 random feature \nindexes = [i for i in range(trainX.shape[1])]\nrandom_100_index = np.random.choice(indexes, size=100)\ntrainRandom_X = trainX[:,random_100_index]\ntestRandom_X = testX[:, random_100_index]\ntop_100feature_classfier =  LogisticRegression(random_state=10, penalty='l2', multi_class='ovr', solver='liblinear').fit(trainRandom_X, trainY)\nscore = top_100feature_classfier.score(testRandom_X,testY)\nprint(\"Validation Score for top 100 random features : {}\".format(score))","f1038411":"## Baseline Model  100 High Variance features\npca = PCA()\n","786efe19":"pca.fit(trainX)\n# Variance Explained by 1st Component\npca_transformed_trainX = pca.transform(trainX)[:,:100]\npca_transformed_testX = pca.transform(testX) [:,:100]\nprint(\"pca_transformed Train Data shape {}\".format(pca_transformed_trainX.shape))\n\nprint(\"pca_transformed Test Data shape {}\".format(pca_transformed_testX.shape))\n\ntop_100feature_pca_classfier =  LogisticRegression(random_state=10, penalty='l1', multi_class='ovr', solver='liblinear').fit(pca_transformed_trainX, trainY)\nscore = top_100feature_classfier.score(pca_transformed_testX,testY)\nprint(\"Validation Score for top 100 high variance features : {}\".format(score))","71f73c7b":"# get top 100 feature indexes for each classes and put all \n#unique indexes into indexes set these new index feature will be used for further analysis\nindexes = set()\nfor i in range(3):\n    sort100 = np.argsort(absCoeff[i])[::-1][:100]\n    for j in sort100:\n        indexes.add(j)\n    \n    \nprint(len(indexes))\nindex_ar = np.array(list(indexes))\n","087d3e0e":"\ntrainX = np.load(\"..\/input\/genecvdata\/X_train.npy\")\ntrainY = np.load(\"..\/input\/genecvdata\/y_train.npy\")\ntestX = np.load(\"..\/input\/genecvdata\/X_test.npy\")\ntestY = np.load(\"..\/input\/genecvdata\/y_test.npy\")\nprint(\"Take subset of features we get from top 100 features\")\n\ntrainX1 = trainX[:,index_ar]\ntestX1 = testX[:, index_ar]\ntrainX1 = np.log2(trainX1+1)\ntestX1 = np.log2(testX1+1)","231f9643":"top_100feature_classfier =  LogisticRegression(random_state=10, penalty='l2', multi_class='ovr', solver='liblinear').fit(trainX1, trainY)\nscore = top_100feature_classfier.score(testX1,testY)\nscore","3e201ae1":"top_100feature_classfier =  LogisticRegression(random_state=10, penalty='l1', multi_class='ovr', solver='liblinear').fit(trainX1, trainY)\nscore = top_100feature_classfier.score(testX1,testY)\nscore","838eb59d":"histog = np.var(trainX1[:,:100], axis=0)\nhistogRandom = np.var(trainRandom_X[:,:100], axis=0)\nhistogram_high_variance = np.var(pca_transformed_trainX[:,:100], axis=0)","4a113641":"sns.displot(histogRandom, kind='hist')\nprint(\"Histogram for top 100 features variance choosen randomly\")","835db465":"sns.displot(np.log(histogram_high_variance), kind='hist', height=6)\nprint(\"Histogram for top 100 features choosen from log(high variance) \")","0c5fc880":"sns.displot(histogram_high_variance, kind='hist', height=3)\nprint(\"Histogram for top 100 features choosen from high variance \")","ed9a9e0b":"index = [i for i in range(100)]\nsns.displot(histog, kind='hist')\nprint(\"Histogram for top 100 features choosen from clustering label\")\n","89569a52":"\n<a id=\"feature-selection\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#50aa50; border:0; color:white' role=\"tab\" aria-controls=\"home\">\n<center>Unsupervised Feature Selection<\/center><\/h1>","7a0a79b9":"## Baseline Model Random 100 features","b0a78ff0":"In this project, we will analyze a **single-cell RNA-seq dataset**, with the goal of unveiling hierarchical structure and discovering important genes. The datasets provided are all different subsets of a larger single-cell RNA-seq dataset, compiled by the **Allen Institute**. This data contains cells from the mouse neocortex, a region in the brain which governs higher-level functions such as perception and cognition.\n\nThe single-cell RNA-seq data comes in the form of a counts matrix, where\n\neach row corresponds to a cell\n\nDue to the presence of genes with extremely high magnitudes of expression in only a few cells, it is common to apply a log-transform to the data, that is, to apply the transform .each column corresponds to the **normalized transcript compatibility count (TCC) of an equivalence class of short RNA sequences, rescaled to units of counts per million.** You can think of the TCC entry at location (i, j)  of the data matrix as the level of expression of the i-th gene in the j-th cell.","12cc575c":"It looks like we have 3 cluster that gives best result. we will choose 3 for clustering. Now we will assign psudo label to every data point.","9346cd8d":"\n<a id=\"mds\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#50aa50; border:0; color:white' role=\"tab\" aria-controls=\"home\">\n<center>Multi Dimensional Scaling<\/center><\/h1>","6a7f4b0d":"### Log Transformation\nDue to the presence of genes with extremely high magnitudes of expression in only a few cells, it is common to apply a log-transform to the data.","94c27612":"<a id=\"top100\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#50aa50; border:0; color:white' role=\"tab\" aria-controls=\"home\">\n<center>Top 100 features<\/center><\/h1>","9657932c":"This is Extremely High Dimensional  data, let's use PCA technique to transfer the data into low dimensional space to do more meaningful visualization and classfication task. ","e89a8c02":"<a id=\"dataset-overview\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#50aa50; border:0; color:white' role=\"tab\" aria-controls=\"home\">\n<center>Dataset Overview<\/center><\/h1>","bb62cc14":"1. Now we attempt to find informative genes which can help us differentiate between cells, using only unlabeled data. \n2. A genomics researcher would use specialized, domain-specific tools to select these genes. We will instead take a general approach using logistic regression in conjunction with clustering.\n3. Briefly speaking, we will use one  dataset to cluster the data. Treating those cluster labels as ground truth, we will fit a logistic regression model and use its coefficients to select features. Finally, to evaluate the quality of these features, we will fit another logistic regression model on the training set in evalution dataset, and run it on the test set in the same folder.","f3bc49c1":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#50aa50; border:0; color:white' role=\"tab\" aria-controls=\"home\" ><center>Contents<\/center><\/h1>","d2af4982":"<a id=\"logit\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#50aa50; border:0; color:white' role=\"tab\" aria-controls=\"home\">\n<center>Logistic Regression<\/center><\/h1>\n<div> <b>cluster assignments as labels for supervised learning. We will Fit a logistic regression model to the original data (not principal components), with  clustering label as the target labels.<\/b><\/div>","96951b44":"1. We will get top 100 features from one vs rest classifier and then use those features to train new logistic regression on new evalution data","68dda7cb":"\n<a id=\"pca\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#50aa50; border:0; color:white' role=\"tab\" aria-controls=\"home\">\n<center>Principal Component Analysis<\/center><\/h1>\n","988bb67b":"\n<a id=\"tsne\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#50aa50; border:0; color:white' role=\"tab\" aria-controls=\"home\">\n<center>T-SNE<\/center><\/h1>","3bbf432a":"1. [Dataset Overview](#dataset-overview)  \n2. [Dataset Transformation](#transform)\n3. [Principal Component Analysis](#pca)\n4. [Multi Dimensional Scaling](#mds)\n5. [T-SNE](#tsne)\n6. [Unsupervised Feature Selection](#feature-selection)\n7. [Logistic Regression](#logit)\n8. [Top 100 features](#top100)\n"}}