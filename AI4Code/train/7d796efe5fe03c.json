{"cell_type":{"1d850c92":"code","389bae48":"code","91a00244":"code","3809a782":"code","a1a26022":"code","038350da":"code","db8f91e3":"code","d6ba1e24":"code","73e36f35":"code","f804490b":"code","292d1be2":"code","addbbd77":"code","6ddbe0a4":"code","db3533bf":"code","59e7d855":"code","c3a0b592":"code","470a3ad1":"code","002abaf8":"code","ddb0ec1f":"code","876871ea":"code","dd940b36":"code","f673d444":"code","37277c5e":"code","461cd294":"code","6ad000ad":"code","71e99df6":"code","29a32be4":"markdown","34be40c7":"markdown","13c3ea35":"markdown","9ffee150":"markdown","0284d6a7":"markdown","ae553686":"markdown","216d810f":"markdown","f52a6a96":"markdown","10b96f95":"markdown","9d5e42de":"markdown","12c85b24":"markdown","eb7f6670":"markdown"},"source":{"1d850c92":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","389bae48":"df = pd.read_csv('..\/input\/data.csv') #reading the dataset","91a00244":"df.head()","3809a782":"df.shape #Number of rows and columns","a1a26022":"df.drop(['id','Unnamed: 32'],axis=1,inplace=True)","038350da":"plt.figure(figsize=(11,6))\nsns.set()\nplt.title('Distribution of pokemon types')\nk = sns.countplot(x = 'diagnosis',data=df)\nk.set_xticklabels(k.get_xticklabels(), rotation=0)\nplt.show()","db8f91e3":"plt.figure(figsize=(11,6))\nplt.title('Distribution of radius mean in tumor')\nsns.distplot(df['radius_mean'],color='r')","d6ba1e24":"plt.figure(figsize=(11,6))\nplt.title('Distribution of area mean in tumor')\nsns.distplot(df['area_mean'],color='g')","73e36f35":"df.info()","f804490b":"def convert(x):\n    if x == 'M':\n        return 0\n    if x == 'B':\n        return 1","292d1be2":"df['diagnosis'] = df['diagnosis'].apply(lambda x: convert(x))","addbbd77":"#Independent variables\nX = df.iloc[:,1:].values","6ddbe0a4":"#Dependent variables\ny = df.iloc[:,0].values","db3533bf":"from sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import MinMaxScaler","59e7d855":"# Choosing attributes\npca = PCA(n_components = 3)\n# Normalizing data\nscaler = MinMaxScaler(feature_range = (0, 1))\nXi = scaler.fit_transform(X)\nfit = pca.fit(Xi)\n\n\nprint(\"Variance: %s\" % fit.explained_variance_ratio_)\nprint(np.sum(fit.explained_variance_ratio_))\np = []\nx = []\nfor i in range(1,25):\n    pca = PCA(n_components = i)\n    fit = pca.fit(Xi)\n    x.append(i)\n    p.append(np.sum(fit.explained_variance_ratio_))\nx_pca = pca.transform(Xi)","c3a0b592":"plt.grid(True)\nplt.scatter(x,p)\nplt.show()","470a3ad1":"from sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB","002abaf8":"# Folds\nnum_folds = 10\nseed = 7\n\n# Number of trees\nnum_trees = 100\n\n# Folds in data\nkfold = KFold(num_folds, True, random_state = seed)\n\n# model\nmodelo = GradientBoostingClassifier(n_estimators = num_trees, random_state = seed)# Cross Validation\nresultado = cross_val_score(modelo, X, y, cv = kfold)\nresultado1 = cross_val_score(modelo, Xi, y, cv = kfold)\nresultado2 = cross_val_score(modelo, x_pca, y, cv = kfold)\n\n# Printing result without modification\nprint(\"Accuracy without modification: %.3f\" % (resultado.mean() * 100))\n# Printing result with normalization\nprint(\"Accuracy with normalization: %.3f\" % (resultado1.mean() * 100))\n# Printing result with PCA\nprint(\"Accuracy with PCA: %.3f\" % (resultado2.mean() * 100))","ddb0ec1f":"# model\nmodelo = XGBClassifier(n_estimators = num_trees, random_state = seed)# Cross Validation\nresultado = cross_val_score(modelo, X, y, cv = kfold)\nresultado1 = cross_val_score(modelo, Xi, y, cv = kfold)\nresultado2 = cross_val_score(modelo, x_pca, y, cv = kfold)\n\n# Printing result without modification\nprint(\"Accuracy without modification: %.3f\" % (resultado.mean() * 100))\n# Printing result with normalization\nprint(\"Accuracy with normalization: %.3f\" % (resultado1.mean() * 100))\n# Printing result with PCA\nprint(\"Accuracy with PCA: %.3f\" % (resultado2.mean() * 100))","876871ea":"modelo = RandomForestClassifier(n_estimators = num_trees, random_state = seed)# Cross Validation\nresultado = cross_val_score(modelo, X, y, cv = kfold)\nresultado1 = cross_val_score(modelo, Xi, y, cv = kfold)\nresultado2 = cross_val_score(modelo, x_pca, y, cv = kfold)\n\n# Printing result without modification\nprint(\"Accuracy without modification: %.3f\" % (resultado.mean() * 100))\n# Printing result with normalization\nprint(\"Accuracy with normalization: %.3f\" % (resultado1.mean() * 100))\n# Printing result with PCA\nprint(\"Accuracy with PCA: %.3f\" % (resultado2.mean() * 100))","dd940b36":"modelo = MLPClassifier(hidden_layer_sizes=500,max_iter=1000,tol=1e-5,solver='adam')\nresultado = cross_val_score(modelo, X, y, cv = kfold)\nresultado1 = cross_val_score(modelo, Xi, y, cv = kfold)\nresultado2 = cross_val_score(modelo, x_pca, y, cv = kfold)\n\n# Printing result without modification\nprint(\"Accuracy without modification: %.3f\" % (resultado.mean() * 100))\n# Printing result with normalization\nprint(\"Accuracy with normalization: %.3f\" % (resultado1.mean() * 100))\n# Printing result with PCA\nprint(\"Accuracy with PCA: %.3f\" % (resultado2.mean() * 100))","f673d444":"# model\nmodelo = AdaBoostClassifier(n_estimators = num_trees, random_state = seed)# Cross Validation\nresultado = cross_val_score(modelo, X, y, cv = kfold)\nresultado1 = cross_val_score(modelo, Xi, y, cv = kfold)\nresultado2 = cross_val_score(modelo, x_pca, y, cv = kfold)\n\n# Printing result without modification\nprint(\"Accuracy without modification: %.3f\" % (resultado.mean() * 100))\n# Printing result with normalization\nprint(\"Accuracy with normalization: %.3f\" % (resultado1.mean() * 100))\n# Printing result with PCA\nprint(\"Accuracy with PCA: %.3f\" % (resultado2.mean() * 100))","37277c5e":"modelo = SVC()\nresultado = cross_val_score(modelo, X, y, cv = kfold)\nresultado1 = cross_val_score(modelo, Xi, y, cv = kfold)\nresultado2 = cross_val_score(modelo, x_pca, y, cv = kfold)\n\n# Printing result without modification\nprint(\"Accuracy without modification: %.3f\" % (resultado.mean() * 100))\n# Printing result with normalization\nprint(\"Accuracy with normalization: %.3f\" % (resultado1.mean() * 100))\n# Printing result with PCA\nprint(\"Accuracy with PCA: %.3f\" % (resultado2.mean() * 100))","461cd294":"# Preparing models\nmodelos = []\nmodelos.append(('LR', LogisticRegression()))\nmodelos.append(('XGBoost', XGBClassifier(n_estimators = num_trees, random_state = seed)))\nmodelos.append(('AdaBoost', AdaBoostClassifier(n_estimators = num_trees, random_state = seed)))\nmodelos.append(('KNN', KNeighborsClassifier()))\nmodelos.append(('GradientBoosting', GradientBoostingClassifier(n_estimators = num_trees, random_state = seed)))\nmodelos.append(('SVM', SVC()))\nmodelos.append(('R.Forest',RandomForestClassifier(n_estimators = num_trees, random_state = seed)))\nmodelos.append(('Neural',MLPClassifier(hidden_layer_sizes=500,max_iter=1000,tol=1e-5,solver='adam')))\n\n# Each model will be evaluated by loop\nresultados = []\nnomes = []\n\nfor nome, modelo in modelos:\n    kfold = KFold(n_splits = num_folds, random_state = seed)\n    cv_results = cross_val_score(modelo, X, y, cv = kfold, scoring = 'accuracy')\n    resultados.append(cv_results)\n    nomes.append(nome)\n    msg = \"%s: %f (%f)\" % (nome, cv_results.mean(), cv_results.std())\n    print(msg)\n\n# Boxplot to compare algorithms\nfig = plt.figure(figsize=(13,6))\nfig.suptitle('Comparing algorithms')\nax = fig.add_subplot(111)\nplt.boxplot(resultados)\nax.set_xticklabels(nomes)\nplt.show()","6ad000ad":"# Preparing models\nmodelos = []\nmodelos.append(('LR', LogisticRegression()))\nmodelos.append(('XGBoost', XGBClassifier(n_estimators = num_trees, random_state = seed)))\nmodelos.append(('AdaBoost', AdaBoostClassifier(n_estimators = num_trees, random_state = seed)))\nmodelos.append(('KNN', KNeighborsClassifier()))\nmodelos.append(('GradientBoosting', GradientBoostingClassifier(n_estimators = num_trees, random_state = seed)))\nmodelos.append(('SVM', SVC()))\nmodelos.append(('R.Forest',RandomForestClassifier(n_estimators = num_trees, random_state = seed)))\nmodelos.append(('Neural',MLPClassifier(hidden_layer_sizes=500,max_iter=1000,tol=1e-5,solver='adam')))\n\n# Each model will be evaluated by loop\nresultados = []\nnomes = []\n\nfor nome, modelo in modelos:\n    kfold = KFold(n_splits = num_folds, random_state = seed)\n    cv_results = cross_val_score(modelo, Xi, y, cv = kfold, scoring = 'accuracy')\n    resultados.append(cv_results)\n    nomes.append(nome)\n    msg = \"%s: %f (%f)\" % (nome, cv_results.mean(), cv_results.std())\n    print(msg)\n\n# Boxplot to compare algorithms\nfig = plt.figure(figsize=(13,6))\nfig.suptitle('Comparing algorithms')\nax = fig.add_subplot(111)\nplt.boxplot(resultados)\nax.set_xticklabels(nomes)\nplt.show()","71e99df6":"# Preparing models\nmodelos = []\nmodelos.append(('LR', LogisticRegression()))\nmodelos.append(('XGBoost', XGBClassifier(n_estimators = num_trees, random_state = seed)))\nmodelos.append(('AdaBoost', AdaBoostClassifier(n_estimators = num_trees, random_state = seed)))\nmodelos.append(('KNN', KNeighborsClassifier()))\nmodelos.append(('GradientBoosting', GradientBoostingClassifier(n_estimators = num_trees, random_state = seed)))\nmodelos.append(('SVM', SVC()))\nmodelos.append(('R.Forest',RandomForestClassifier(n_estimators = num_trees, random_state = seed)))\nmodelos.append(('Neural',MLPClassifier(hidden_layer_sizes=500,max_iter=1000,tol=1e-5,solver='adam')))\n\n# Each model will be evaluated by loop\nresultados = []\nnomes = []\n\nfor nome, modelo in modelos:\n    kfold = KFold(n_splits = num_folds, random_state = seed)\n    cv_results = cross_val_score(modelo, x_pca, y, cv = kfold, scoring = 'accuracy')\n    resultados.append(cv_results)\n    nomes.append(nome)\n    msg = \"%s: %f (%f)\" % (nome, cv_results.mean(), cv_results.std())\n    print(msg)\n\n# Boxplot to compare algorithms\nfig = plt.figure(figsize=(13,6))\nfig.suptitle('Comparing algorithms')\nax = fig.add_subplot(111)\nplt.boxplot(resultados)\nax.set_xticklabels(nomes)\nplt.show()","29a32be4":"AdaBoost","34be40c7":"XGBoost","13c3ea35":"Let's compare predictions with no modified data, normalized data and with PCA","9ffee150":"Random Forest","0284d6a7":"Checking counts of diagnosis of tumor","ae553686":"## Breast Cancer Analysis - EDA and Machine Learning - Predicting diagnosis","216d810f":"Checking accuracy of algorithms, with mean, std dev and a boxplot.\nHigh std dev may indicate overfitting.\nHigh median and mean indicates a good algorithm in this situation. We must considerate time to process the algorithm.\nThe first one evaluate the variables without modification. The second one evaluate the normalized variables.\nThe third with PCA","f52a6a96":"Gradient Boosting","10b96f95":"Let's convert the variables M in number 0 and B in number 1","9d5e42de":"Neural Networks","12c85b24":"SVM","eb7f6670":"The ID isn't useful to predict diagnosis - It'll be removed "}}