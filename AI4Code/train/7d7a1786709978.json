{"cell_type":{"51dd0379":"code","478815e2":"code","502c8347":"code","a7ef6f86":"code","b5184255":"code","ea04248c":"code","9cd65eb4":"code","f9f39741":"code","4f2c45c8":"code","a87a824c":"code","7ae95980":"code","291504f2":"code","c9a0061c":"code","c925d246":"code","12d6b11e":"code","ea67253a":"code","7f8aada9":"code","a377d9ca":"markdown","52772a34":"markdown","cc6a8949":"markdown","04f63bfe":"markdown","4a6ad3d5":"markdown","7a3544fe":"markdown","2e3e9f2d":"markdown","adf9ffd8":"markdown","a840825b":"markdown","974bbe62":"markdown","283eec14":"markdown","dd640a24":"markdown","b26b619b":"markdown"},"source":{"51dd0379":"# Import Libraries\n\nimport numpy as np\nimport pandas as pd \nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport os ","478815e2":"# load data\ndf = pd.read_csv(r\"..\/input\/digit-recognizer\/train.csv\",dtype = np.float32)\n\n# Split data into features X and labels y\nX = df.iloc[:,1:].values \/ 255 \ny = df.iloc[:,0].values\n\nfig,ax = plt.subplots(2,5)\nfor i in range(10):\n    nparray = X[i].reshape(28,28)\n    image = Image.fromarray(nparray * 255)\n    ax[i%2][i\/\/2].imshow(image)\nfig.show()","502c8347":"sns.histplot(y)","a7ef6f86":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\ntree_set = df.copy()\ntarget = tree_set.iloc[:,0]\ntree_set_X = tree_set.iloc[:,1:] \n\nclf = DecisionTreeClassifier(max_depth=4)\nclf.fit(tree_set_X, target)\nclf.score(tree_set_X, target)","b5184255":"lst = tree_set_X.columns.tolist()\ntext_representation = tree.export_text(clf, feature_names=lst)\nprint(text_representation)","ea04248c":"from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\ntrain_ratio = 0.90\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=1 - train_ratio, stratify = y, random_state = 0)\n\nscaler = preprocessing.StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_val = scaler.transform(X_val)","9cd65eb4":"from sklearn.ensemble import ExtraTreesClassifier\n\nclf = ExtraTreesClassifier(random_state=1)\n\nclf.fit(X_train, y_train)\nprint(clf.score(X_val, y_val))","f9f39741":"from sklearn.preprocessing import Binarizer\n\nX_train_bin = Binarizer().fit_transform(X_train)\nX_val_bin = Binarizer().fit_transform(X_val)\n\nfig,ax = plt.subplots(2,5)\nfor i in range(10):\n    nparray = X_train_bin[i].reshape(28,28)\n    image = Image.fromarray(nparray * 255)\n    ax[i%2][i\/\/2].imshow(image)\n    ax[i%2][i\/\/2].title.set_text(round(y_train[i]))\nfig.show()","4f2c45c8":"et_clf = ExtraTreesClassifier(random_state=1)\n\net_clf.fit(X_train_bin, y_train)\nprint(et_clf.score(X_val_bin, y_val))","a87a824c":"bad_image_list = []\nbad_image_list_y = []\nbad_predict = []\nfor _X, _y in zip(X_val_bin, y_val):\n    if et_clf.predict(_X.reshape(1, -1)) != _y:\n        bad_image_list.append(_X)\n        bad_image_list_y.append(_y)\n        bad_predict.append(et_clf.predict(_X.reshape(1, -1)).item())\n        if len(bad_image_list) > 9:\n            break\n    \nfig,ax = plt.subplots(2,5)\nfor i in range(10):\n    nparray = bad_image_list[i].reshape(28,28)\n    image = Image.fromarray(nparray * 255)\n    ax[i%2][i\/\/2].imshow(image)\n    ax[i%2][i\/\/2].title.set_text(\"Real: \" + str(round(bad_image_list_y[i])))\nfig.show()\n\nprint(\"Bad predictions:\")\nprint(bad_predict)","7ae95980":"from sklearn.metrics import confusion_matrix\n\nconf_mat = confusion_matrix(y_val, et_clf.predict(X_val_bin))\nsns.heatmap(conf_mat, vmax=10, cmap=\"Blues\")\nplt.show()","291504f2":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(random_state=1)\n\nrf_clf.fit(X_train_bin, y_train)\nprint(rf_clf.score(X_val_bin, y_val))","c9a0061c":"from sklearn.tree import DecisionTreeClassifier\n\ndt_clf = DecisionTreeClassifier(random_state=1)\n\ndt_clf.fit(X_train_bin, y_train)\nprint(dt_clf.score(X_val_bin, y_val))","c925d246":"from sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nestimators = [\n     ('rf', RandomForestClassifier(random_state=1)),\n     ('et', ExtraTreesClassifier(random_state=1))\n]\n\nstack_clf = StackingClassifier(\n    estimators=estimators, final_estimator=LogisticRegression()\n)\n\nstack_clf.fit(X_train_bin, y_train)\nprint(stack_clf.score(X_val_bin, y_val))","12d6b11e":"test_df = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\ntest_X = scaler.transform(test_df.values)\ntest_X = Binarizer().fit_transform(test_X)\npredicted = stack_clf.predict(test_X)","ea67253a":"fig,ax = plt.subplots(2,5)\nfor i in range(10):\n    nparray = test_X[i].reshape(28,28)\n    image = Image.fromarray(nparray * 255)\n    ax[i%2][i\/\/2].imshow(image)\n    ax[i%2][i\/\/2].title.set_text(round(predicted[i]))\nfig.show()","7f8aada9":"submission = pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")\nprint(submission.head())\nsubmission[\"Label\"] = np.rint(predicted)\nsubmission = submission.astype(int)\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(submission.head())","a377d9ca":"I'd say this is indicative that a few cental pixels can be definitive in this dataset in regards to which number they are.","52772a34":"Let's see a confusion matrix of this","cc6a8949":"Let's look at the images","04f63bfe":"# Create Decision Tree and Visualize Result","4a6ad3d5":"# Stacking Models\n\nWe can try stacking models for better results.","7a3544fe":"Let's look at the distribution of classes in the dataset","2e3e9f2d":"That's getting better in terms of accuracy. Let's try and see if we can find the bad images.","adf9ffd8":"Check output visually","a840825b":"Now let's try it in binary","974bbe62":"# Introduction\n\nFor a change of pace I'd like to try an unusual approach to this common challenge. Looking at how methods normally reserved for tabular data problems perform on the dataset.\n\nIt should be noted that all of these methods are likely to face a similar issue, that all columns are equally important. In image recognition it is often very helpful that pixels nearer to a selected pixel are assigned a higher importance in regard to that selected pixel than pixels far away from it. This is not desirable in tabular data and hence there is an issue, still it is interesting to see how far we might get.  ","283eec14":"With a max depth of 4 I find even this accuracy kind of crazy. Let's see it:","dd640a24":"Roughly equal which is good. Interestingly I always figured MNIST would be completely balanced.","b26b619b":"# Create Submission"}}