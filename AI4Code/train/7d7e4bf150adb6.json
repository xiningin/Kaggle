{"cell_type":{"2a1ea210":"code","ea86301f":"code","d024edab":"code","2e4ee979":"code","22b9fe2b":"code","5965754a":"code","0fd70da8":"code","9a9c91fb":"code","df2389db":"code","d42187dd":"code","27fe2304":"code","2d9e5dcc":"code","6007b5e2":"code","681e704d":"code","465f0d7b":"code","78282168":"markdown","fdb95d20":"markdown","ae80a4d7":"markdown","4870ae8d":"markdown","fe96ab8f":"markdown","70081b1e":"markdown","77d33719":"markdown","c1e99c4c":"markdown","ad1772e6":"markdown","656dec9e":"markdown","3f437ab2":"markdown","2b32923e":"markdown","4004e7a1":"markdown","59fc3aab":"markdown","c301c5b8":"markdown","cc4ab01b":"markdown","6ade8034":"markdown","6b146bb7":"markdown","d1c09a7d":"markdown","25e65ad1":"markdown","a33959d3":"markdown"},"source":{"2a1ea210":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom subprocess import check_output\nfrom datetime import time\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","ea86301f":"df = pd.read_csv(\"..\/input\/crowdedness-at-the-campus-gym\/data.csv\")\ndf_graphs = pd.read_csv(\"..\/input\/crowdedness-at-the-campus-gym\/data.csv\")\ndf.head()","d024edab":"df.describe()","2e4ee979":"del df['is_holiday']","22b9fe2b":"#show the correlation between features\ncorrelation = df.corr()\n#create and display a graph figure\nplt.figure(figsize=(10,10))\n#We use the heatmap from sns libray and we choose the options(color, shape)\nsns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='viridis')\n#Display the title of the graph\nplt.title('Correlation between different fearures')","5965754a":"# Drop columns\n# First we delete the date column because it doesn't fit our random forest regression study\ndf = df.drop(\"date\", axis=1)\n# Then we delete the timestamp as it is resulted from the heatmap\ndel df['timestamp']\n# one hot encode categorical columns\ncolumns = [\"day_of_week\", \"month\", \"hour\"]\n# get_dumies is a function from pandas that converts categorical variable into dummy\/indicator variables\ndf = pd.get_dummies(df, columns=columns)\ndf.head(10)","0fd70da8":"# Extract the training and test data\ndata = df.values\nX = data[:, 1:]  # all rows, no label\ny = data[:, 0]  # all rows, label only\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","9a9c91fb":"# Scale the data to be between -1 and 1\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","df2389db":"# Establish model of random forest algorithm\nmodel = RandomForestRegressor(n_jobs=-1)","d42187dd":"# Try different numbers of n_estimators - this will take a minute or so\nestimators = np.arange(10, 200, 10)\n# Create an array of scores\nscores = []\n\nfor n in estimators:\n    model.set_params(n_estimators=n) # Choose the parameters\n    model.fit(X_train, y_train) # Use the training datasets to build the forest of trees\n    scores.append(model.score(X_test, y_test)) # Fill the array with predictions\nplt.title(\"Effect of n_estimators\")\nplt.xlabel(\"n_estimator\")\nplt.ylabel(\"score\")\nplt.plot(estimators, scores)","27fe2304":"scores","2d9e5dcc":"round(max(scores), 4)","6007b5e2":"g = df_graphs[['hour','number_people','day_of_week']] #dataframe with hour, number of people and the day of the week columns\n\nF = g.groupby(['hour','day_of_week'], as_index = False).number_people.mean().pivot('day_of_week','hour','number_people').fillna(0) #resharp the dataframe with the mean of people number\n\ngrid_kws = {\"height_ratios\": (.9, .05), \"hspace\": .3}\n\ndow= 'Monday Tuesday Wednesday Thursday Friday Saturday Sunday'.split() #splitting the string\n\n\nax = sns.heatmap(F, cmap='RdBu_r',cbar_kws={\"orientation\": \"horizontal\"}) #cmap = Red\/Blue, colorbar horizontal\nax.set_yticklabels(dow, rotation = 0) # axis labels\nax.set_ylabel('')\nax.set_xlabel('Hour')\n\ncbar = ax.collections[0].colorbar\ncbar.set_label('Average Number of People')","681e704d":"def get_date(series):\n    return series.str.slice(8,11) #get characters between 8th and 11th place","465f0d7b":"df_graphs['day_of_month'] = df_graphs[['date']].apply(get_date) #dataframe with only the day of the month from the date\nmonth_date_count_df = pd.pivot_table(df_graphs, columns=['day_of_month'],index=['month'], values='number_people', aggfunc=np.mean) #reshape the dataframe\nmonth_date_count_df.fillna(0, inplace=True)\nfig, ax = plt.subplots(figsize=(18,7)) \nheatmap = sns.heatmap(month_date_count_df, annot=True, ax=ax, cmap=\"OrRd\") #heatmap with the reshaped dataframe, with colors from Orange to Red\nheatmap.set_ylabel('Month')\nheatmap.set_xlabel('Day of week')","78282168":"# VI. Conclusion: Discussion","fdb95d20":"The highest score is 0.9222, which proves that our predictions are really good.\nIf we use 130 estimators, we will get the highest score.","ae80a4d7":"Here we are using 2 functions provided by the StandardScaler library : \n* Fit is used to compute the mean and std to be used for later scaling.\n* Transform is used to perform standardization by centering and scaling.","4870ae8d":"As we want to check what times are the busiest at the gym, a heatmap has been created. On this heatmap, we can identify that most people are going during the afternoon probably after class, between 4PM and 10PM, except on Friday because of the weekend.\n\nWe also don't see that much traffic in the morning as people don't want to get up that early, even if the heatmap clearly shows that it is the best time to go if you do not want to wait for equipment.","fe96ab8f":"   As mentioned in the introduction, this project is an addition to the \"Hanyang personal gym trainer\"-project of Software engineering class. \nIt was a bit hard to find our way into the project because none of us had experience with data analysis and machine learning algorithms. Therefore we spent quite some time familiarizing our selves with the topic and looking into blogs for similar projects before we started coding on our own.\nOverall, the project was a great opportunity to practically apply what we have learned in the course of the lecture Artificial Intelligence & Application. We found this subject to be very interesting and are happy with the achieved results. \nIt was very satisfying, to play around with the different features of our dataset, resulting in better and better results for the prediction. In the end, we were able to archieve an accuracy of above 92%, using the random forest algorithm. \n","70081b1e":"# I. Introduction \/ Team\nThis Kernel has been created in terms of the asignment number 3 for the course \"Artificial Intelligence and Application\" at Hanyang University.\n\n### Motivation\nThis project is an addition to the \u201cHanyang University Personal Gym Trainer\u201d- Software Engineering project. \nOur goal is to provide a feature to predict the crowdedness of the Hanyang gym, in order to help the gym members to find the best time to train. \n\nWe want to find out which features are most important and which we can get rid of. \n\nFurthermore, we are going to provide some graphs and schemas referring to the general crowdedness at the gym based on the same dataset. Depending on this, the gym member can create a time-efficient schedule for his gym activity.\n\n### Constributors are\n* Mickael NGUYEN, 9277220193\n* Benjamin V\u00c4THJUNKER, 9297220193\n* William CHAN, 9278320194\n\n\n# II. Datasets\nThe Hanyang University Gym did not have any data about their crowdedness, so we decided to look for an existing dataset on the internet. After some research we found a suitable dataset on Kaggle. This is the Dataset we decided on: [Crowdedness at the gym](https:\/\/www.kaggle.com\/nsrose7224\/crowdedness-at-the-campus-gym)\n\nThe dataset consists of over 62.000 people counts of the UC Berkly University Gym. The number of people were counted about every 10 minutes over the time of 19 months (14\/08\/2015 - 28\/03\/2017). In addition the creator of the Dataset collected weather information and semester-specific informationen which might affect the crowdedness of the gym.\n","77d33719":"## Heatmap Year","c1e99c4c":"First we define the n_estimators range in which we will evaluate the predictions (the estimators are actually the number of trees in the forest). Then we set the parameters of the random forest model and we created a forest of trees with the training dataset. After that we filled the scores array out with prediction scores. Finally we just display the graph of prediction scores according to the estimator range we've previously chosen.","ad1772e6":"We can observe that timestamp and hour have 1 as correlation coefficient. Therefore we will delete the column of timestamp in order to keep only the most optimal features.","656dec9e":"Here we splitted the dataset into 2 datasets : Training and testing.\nAs it is generally used, we splitted 75% of the data for the training and 25% for the testing.","3f437ab2":"We can see that at the start of the semester, the number of people at the gym is considerably increasing (end of January and August).\n\nWe can also clearly identify that the traffic is getting slower before holidays (probably due to exams) and is even worse during holidays (June to August and December to January). ","2b32923e":"Let's see how features are correlated :","4004e7a1":"Youtube presentation : https:\/\/www.youtube.com\/watch?v=UsSno3uCnW4&feature=youtu.be","59fc3aab":"## Quality of the Data\nThe quality of the dataset seems to be quite high, because:\n1. Number of rows for all the data columns are the same, which means we have no missing values\n1. 62.184 values -> big data set to make more precise predictions\n1. Values look reasonable\n  * Number of people: min = 0, max = 138\n  * Hour: min = 0, max = 23\n  * day of the week: min = 0, max = 6\n1. No duplicates","c301c5b8":"The feature **is_holiday** only occurs on two days (and only in 2017). We are assuming that this feature was not maintained properly and remove it due to lack of significance ","cc4ab01b":"# III. Methodology\n\n## Choice of algorithm\nDue to the nature of our given data set (input-output pairs), we are dealing with a supervised learning problem. \nThis kind of problem can be further grouped into two kinds of problems:\n* Classification problem: when the output variable of the model is a category, such as \u201cred\u201d or \u201cblue\u201d or \u201cdisease\u201d and \u201cno disease\u201d.\n* Regression problem: when the output variable is a real value, such as \u201cdollars\u201d or \u201cweight\u201d.\n\n=> as our goal is to predict the number of people at the Hanyang University Gym, we are dealing with a classification problem.\n\nTo solve this problem we decided to use the Random Forest Algorithm, because:\n* results in a high prediction accuracy\n* works well with a big dataset\n* is fast and scalable\n\n## Random forest algorithm\nThe random forest algorithm can be used to solve regression and classification problems. It operates by constructing a multitude of decision trees and merging them together to obtain a more stable and accurate prediction. Each decision tree forecasts a response for an occurrence and the endmost response is decided through voting. In a regression problem, the final response is the average of all the responses.\n\n## Benefits of feature engineering\nIn order for the algorithm to accurately interpret and understand all features, we have to do feature engineering. Done correctly, this will result in the following benefits:\n* Reduces Overfitting: Less redundant data means fewer opportunities to make decisions based on noise.\n* Improves Accuracy: Less misleading data means modeling accuracy improves.\n* Reduces Training Time: fewer data means that algorithms train faster.\n\n## Further steps\nTaking into account what we just discussed, the next steps are as follows:\n1. do feature engineering\n2. split data into training & test sets\n3. create and train a model using random forest","6ade8034":"As you can see, the graph shows us some high scores for different numbers of estimators between 20 and 200. The highest is the score, the more accurate are the predictions.","6b146bb7":"## Heatmap Week","d1c09a7d":"Most of the features are self-explanatory, but some are unclear and will be explained in the following:\n\nFeature | Description\n:--- | :---\nis_start_of_semester | true for the first two weeks of the semester\ntimestamp | number of seconds that have elapsed in the day\nno_semester | semester free time in summer and winter","25e65ad1":"# IV. Evaluation & Analysis","a33959d3":"# V. Related Work\n\n**Libraries**\n\n\u27a4 numpy \u2192 NumPy (short for Numerical Python) provides an interface to store and perform data operations\n\u27a4 pandas: for data manipulation and analysis\n\n\u27a4 Sklearn\n\t \u2022 sklearn.ensemble: provides many unsupervised and supervised learning algorithms \n\t   (RandomForestRegressor in our case)\n\t \u2022 sklearn.model_selection \u2192 train_test_split: Split arrays or matrices into random train and test        subsets.\n\t \u2022 sklearn.preprocessing \u2192 StandardScaler\n\n\u27a4 matplotlib.pyplot: to display 2D figures\n\u27a4 seaborn: for statistical data vizualisation\n\n\u27a4 subprocess \u2192 check_output: Run command with arguments and return its output as a byte string\n\u27a4 datetime \u2192 time: Time access and conversions\n\n**Existing studies**\n\n\u27a4  Dataset extraction\nhttps:\/\/www.kaggle.com\/nsrose7224\/crowdedness-at-the-campus-gym\n\n\u27a4 PCA (Principal Component Analysis)\n https:\/\/www.kaggle.com\/nirajvermafcb\/principal-component-analysis-with-scikit-learn\n\n\u27a4 Heatmaps informations\nhttps:\/\/www.kaggle.com\/demetripananos\/a-heat-map-to-visualize-mean-crowdedness\n"}}