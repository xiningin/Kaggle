{"cell_type":{"76a1590a":"code","eba969cd":"code","90d219d3":"code","3dff796d":"code","06bc3bde":"code","2bdab4e7":"code","191e3637":"code","77589c2b":"code","5b667ca4":"code","4fa754e7":"code","8a391d96":"code","48f737d6":"code","8540e8cc":"code","5494c09e":"code","c514d414":"code","82528a6a":"code","afa98f44":"code","f83f6832":"code","05acd76a":"markdown","de8ab473":"markdown","3a640f1a":"markdown"},"source":{"76a1590a":"import numpy as np\nimport pandas as pd\nimport re\nfrom tqdm.auto import tqdm\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import AdamW, get_scheduler","eba969cd":"df = pd.read_csv('..\/input\/trip-advisor-hotel-reviews\/tripadvisor_hotel_reviews.csv')\n\ndf.head()","90d219d3":"df['Rating'] = df['Rating'].map(\n    {\n        1: 0,\n        2: 0,\n        3: 0,\n        4: 1,\n        5: 1\n    }\n)","3dff796d":"def text_preprocessing(text: str):\n    text = text.lower()\n    text = ' '.join(word for word in text.split() if word not in stopwords.words('english') + [\"n't\"])\n    text = re.sub('[^a-z]+', ' ', text)\n    text = text.strip()\n    return text","06bc3bde":"df['Review'] = [text_preprocessing(text) for text in tqdm(list(df.Review))]","2bdab4e7":"SEED = 0\nBATCH_SIZE = 8","191e3637":"x = df.Review\ny = df.Rating.values","77589c2b":"x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=.2, stratify=y, random_state=SEED)","5b667ca4":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","4fa754e7":"tokenizer = AutoTokenizer.from_pretrained(\"rohanrajpal\/bert-base-codemixed-uncased-sentiment\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"rohanrajpal\/bert-base-codemixed-uncased-sentiment\").to(device)","8a391d96":"train_text = tokenizer(list(x_train), padding=\"max_length\", truncation=True)\nval_text = tokenizer(list(x_val), padding=\"max_length\", truncation=True)","48f737d6":"class MyDataset(Dataset):\n    \n    def __init__(self, inputs, labels):\n        self.inputs = inputs\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        input_ids = self.inputs['input_ids'][idx]\n        att_mask = self.inputs['attention_mask'][idx]\n        labels = self.labels[idx]\n        output = {\n            'attention_mask': torch.tensor(att_mask),\n            'input_ids': torch.tensor(input_ids),\n            'labels': labels\n        }\n        return output","8540e8cc":"train_dataset = MyDataset(train_text, y_train)\nval_dataset = MyDataset(val_text, y_val)\n\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, pin_memory=True)\nval_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE, pin_memory=True)","5494c09e":"warmump_steps = 500\nepochs = 3\nnum_training_steps = epochs * len(train_dataloader)","c514d414":"optimizer = AdamW(model.parameters())\nlr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=warmump_steps, num_training_steps=num_training_steps)","82528a6a":"def train(dataloader):\n    train_loss = 0\n    model.train()\n    for batch in dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        optimizer.zero_grad()\n        outputs = model(**batch)\n        loss = outputs.loss\n        train_loss += loss.item() * len(batch)\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        progress_bar.update(1)\n    return train_loss","afa98f44":"def validate(dataloader):\n    model.eval()\n    with torch.no_grad():\n        val_loss = 0\n        for batch in dataloader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            val_loss += loss.item() * len(batch)\n    return val_loss","f83f6832":"progress_bar = tqdm(range(num_training_steps))\nfor epoch in range(1, epochs + 1):\n    train_loss = train(train_dataloader)\n    val_loss = validate(val_dataloader)\n    train_loss \/= len(train_dataset)\n    val_loss \/= len(val_dataset)\n    print(f'Epoch: {epoch}, Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}')","05acd76a":"Since I build binary classifier which based on sentiment analyse model I'm gonna do different mapping.","de8ab473":"As you can see we have reached pretty low loss score.\n\nIn addition, you also can try to train and test many many others transformers.","3a640f1a":"Some hyperparameters which you can and **should** tune if wanna reach high accuracy of your model. Also try to tune more parameters."}}