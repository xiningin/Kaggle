{"cell_type":{"f38a69d0":"code","08320ffe":"code","7ae28774":"code","b14cc731":"code","12bc60f3":"code","d5dd67d9":"code","a3a604af":"code","b8a42ded":"code","8b6ef1ea":"code","d4df6833":"code","d45ce88d":"code","2e5fb8a9":"code","26f54ef1":"code","743696b5":"code","65c7e98c":"code","13b01017":"code","1f426cb6":"code","cf161c22":"code","8d966d63":"code","37219982":"code","9875ee7f":"code","4c43a7dd":"code","1b1b1f9b":"code","a300fe62":"code","6c57936a":"code","89f1966f":"code","43a3cb5e":"code","352d7749":"code","2a964811":"code","6fc20fcb":"code","0df7ccdb":"code","e48eeba2":"code","641cb2a8":"code","a623797c":"code","100caa69":"code","44e6d927":"code","8688b600":"code","f32849a3":"code","62192fa3":"code","ae002563":"markdown","25404e7b":"markdown","ac5a3265":"markdown","9d84e154":"markdown","b44df110":"markdown","0a9ffe50":"markdown","ea370a61":"markdown","e41e6e21":"markdown","7d22322f":"markdown","b6d239ab":"markdown","4161f8b3":"markdown","c4caa740":"markdown","dd9864f3":"markdown","1efeff68":"markdown","88655ab1":"markdown","0948a156":"markdown","1bfd0153":"markdown","56b35377":"markdown","dde75ba6":"markdown","63d40d9f":"markdown","267c2677":"markdown","c93f9680":"markdown","a099864b":"markdown","e4a51421":"markdown","61974b6f":"markdown","1b384b9f":"markdown","a467b458":"markdown","6c07220b":"markdown","fce4cf8f":"markdown","0640f18b":"markdown","1e78efb8":"markdown","b580967e":"markdown","4eb400c0":"markdown","34f543e2":"markdown","50121390":"markdown","ff91f11b":"markdown","6ce79425":"markdown","a9abac1b":"markdown","395839ff":"markdown","c5a42398":"markdown","0af8b69c":"markdown","b23bb0f4":"markdown","77f222ea":"markdown","623d0746":"markdown","b6376505":"markdown"},"source":{"f38a69d0":"# Import the necessary modules\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as matplot\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","08320ffe":"# Read the analytics csv file and store our dataset into a dataframe called \"df\"\n#os.listdir(\"..\/input\")\ndf = pd.read_csv('..\/input\/WA_Fn-UseC_-HR-Employee-Attrition.csv', index_col=None)","7ae28774":"# Check to seeh if there are any missing values in our data set\ndf.isnull().any()","b14cc731":"# Get a quick overview of what we are dealing with in our dataset\ndf.head()","12bc60f3":"# Move the reponse variable \"Attrition\" to the front of the table\nfront = df['Attrition']\ndf.drop(labels=['Attrition'], axis=1,inplace = True)\ndf.insert(0, 'Attrition', front)\ndf.head()","d5dd67d9":"# The dataset contains 35 columns and 1470 observations\ndf.shape","a3a604af":"df.dtypes","b8a42ded":"# Looks like about 84% of employees stayed and 16% of employees left. \n# NOTE: When performing cross validation, its important to maintain this turnover ratio\nAttritionRate=df.Attrition.value_counts()\/len(df)\nAttritionRate","8b6ef1ea":"# Display the statistical overview of the employees\ndf.describe()","d4df6833":"# Overview of summary (Attrition V.S. Non-Attrition)\nAttrition_Summary = df.groupby('Attrition')\nAttrition_Summary.mean()","d45ce88d":"# Reassign target\ndf.Attrition.replace(to_replace = dict(Yes = 1, No = 0), inplace = True)\nattrition = df[(df['Attrition'] != 0)]\nno_attrition = df[(df['Attrition'] == 0)]","2e5fb8a9":"df.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8)","26f54ef1":"# Plotting the KDEplots\nf, axes = plt.subplots(3, 3, figsize=(12, 8), \n                       sharex=False, sharey=False)\n\n# Defining our colormap scheme\ns = np.linspace(0, 3, 10)\ncmap = sns.cubehelix_palette(start=0.0, light=1, as_cmap=True)\n\n# Generate and plot\nx = df['Age'].values\ny = df['TotalWorkingYears'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True, cut=5, ax=axes[0,0])\naxes[0,0].set( title = 'Age against Total working years')\n\ncmap = sns.cubehelix_palette(start=0.333333333333, light=1, as_cmap=True)\n# Generate and plot\nx = df['Age'].values\ny = df['DailyRate'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True, ax=axes[0,1])\naxes[0,1].set( title = 'Age against Daily Rate')\n\ncmap = sns.cubehelix_palette(start=0.666666666667, light=1, as_cmap=True)\n# Generate and plot\nx = df['YearsInCurrentRole'].values\ny = df['Age'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True, ax=axes[0,2])\naxes[0,2].set( title = 'Years in role against Age')\n\ncmap = sns.cubehelix_palette(start=1.0, light=1, as_cmap=True)\n# Generate and plot\nx = df['DailyRate'].values\ny = df['DistanceFromHome'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[1,0])\naxes[1,0].set( title = 'Daily Rate against DistancefromHome')\n\ncmap = sns.cubehelix_palette(start=1.333333333333, light=1, as_cmap=True)\n# Generate and plot\nx = df['DailyRate'].values\ny = df['JobSatisfaction'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[1,1])\naxes[1,1].set( title = 'Daily Rate against Job satisfaction')\n\ncmap = sns.cubehelix_palette(start=1.666666666667, light=1, as_cmap=True)\n# Generate and plot\nx = df['YearsAtCompany'].values\ny = df['JobSatisfaction'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[1,2])\naxes[1,2].set( title = 'Daily Rate against distance')\n\ncmap = sns.cubehelix_palette(start=2.0, light=1, as_cmap=True)\n# Generate and plot\nx = df['YearsAtCompany'].values\ny = df['DailyRate'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[2,0])\naxes[2,0].set( title = 'Years at company against Daily Rate')\n\ncmap = sns.cubehelix_palette(start=2.333333333333, light=1, as_cmap=True)\n# Generate and plot\nx = df['RelationshipSatisfaction'].values\ny = df['YearsWithCurrManager'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[2,1])\naxes[2,1].set( title = 'Relationship Satisfaction vs years with manager')\n\ncmap = sns.cubehelix_palette(start=2.666666666667, light=1, as_cmap=True)\n# Generate and plot\nx = df['WorkLifeBalance'].values\ny = df['JobSatisfaction'].values\nsns.kdeplot(x, y, cmap=cmap, shade=True,  ax=axes[2,2])\naxes[2,2].set( title = 'WorklifeBalance against Satisfaction')\n\nf.tight_layout()\n","743696b5":"def plot_distribution(var_select, bin_size) : \n# Calculate the correlation coefficient between the new variable and the target\n    corr = df['Attrition'].corr(df[var_select])\n    corr = np.round(corr,3)\n    tmp1 = attrition[var_select]\n    tmp2 = no_attrition[var_select]\n    hist_data = [tmp1, tmp2]\n    \n    group_labels = ['Yes_attrition', 'No_attrition']\n    colors = ['#FFD700', '#7EC0EE']\n\n    fig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, curve_type='kde', bin_size = bin_size)\n    \n    fig['layout'].update(title = var_select+' '+'(corr target ='+ str(corr)+')')\n\n    py.iplot(fig, filename = 'Density plot')","65c7e98c":"def barplot(var_select, x_no_numeric) :\n    tmp1 = df[(df['Attrition'] != 0)]\n    tmp2 = df[(df['Attrition'] == 0)]\n    tmp3 = pd.DataFrame(pd.crosstab(df[var_select],df['Attrition']), )\n    tmp3['Attr%'] = tmp3[1] \/ (tmp3[1] + tmp3[0]) * 100\n    if x_no_numeric == True  : \n        tmp3 = tmp3.sort_values(1, ascending = False)\n\n    color=['lightskyblue','gold' ]\n    trace1 = go.Bar(\n        x=tmp1[var_select].value_counts().keys().tolist(),\n        y=tmp1[var_select].value_counts().values.tolist(),\n        name='Yes_Attrition',opacity = 0.8, marker=dict(\n        color='gold',\n        line=dict(color='#000000',width=1)))\n\n    \n    trace2 = go.Bar(\n        x=tmp2[var_select].value_counts().keys().tolist(),\n        y=tmp2[var_select].value_counts().values.tolist(),\n        name='No_Attrition', opacity = 0.8, marker=dict(\n        color='lightskyblue',\n        line=dict(color='#000000',width=1)))\n    \n    trace3 =  go.Scatter(   \n        x=tmp3.index,\n        y=tmp3['Attr%'],\n        yaxis = 'y2',\n        name='% Attrition', opacity = 0.6, marker=dict(\n        color='black',\n        line=dict(color='#000000',width=0.5\n        )))\n\n    layout = dict(title =  str(var_select),\n              xaxis=dict(), \n              yaxis=dict(title= 'Count'), \n              yaxis2=dict(range= [-0, 75], \n                          overlaying= 'y', \n                          anchor= 'x', \n                          side= 'right',\n                          zeroline=False,\n                          showgrid= False, \n                          title= '% Attrition'\n                         ))\n\n    fig = go.Figure(data=[trace1, trace2, trace3], layout=layout)\n    py.iplot(fig)","13b01017":"plot_distribution('Age', False)\nbarplot('Age', False)\nplot_distribution('DailyRate', 100)\nplot_distribution('DistanceFromHome', False)\nbarplot('DistanceFromHome', False)\nplot_distribution('HourlyRate', False)\nplot_distribution('MonthlyIncome', 100)\nplot_distribution('MonthlyRate', 100)\nplot_distribution('NumCompaniesWorked', False)\nbarplot('NumCompaniesWorked',False)\nplot_distribution('PercentSalaryHike', False)\nbarplot('PercentSalaryHike', False) \nplot_distribution('TotalWorkingYears', False)\nbarplot('TotalWorkingYears', False)\nplot_distribution('TrainingTimesLastYear', False)\nbarplot('TrainingTimesLastYear',False)\nplot_distribution('YearsAtCompany', False)\nbarplot('YearsAtCompany', False)\nplot_distribution('YearsInCurrentRole', False)\nbarplot('YearsInCurrentRole', False)\nplot_distribution('YearsSinceLastPromotion', False)\nbarplot('YearsSinceLastPromotion', False)\nplot_distribution('YearsWithCurrManager', False)\nbarplot('YearsWithCurrManager', False)","1f426cb6":"def plot_pie(var_select) :\n    \n    colors = ['gold', 'lightgreen', 'lightcoral', 'lightskyblue', 'lightgrey', 'orange', 'white', 'lightpink']\n    trace1 = go.Pie(values  = attrition[var_select].value_counts().values.tolist(),\n                    labels  = attrition[var_select].value_counts().keys().tolist(),\n                    textfont=dict(size=15), opacity = 0.8,\n                    hoverinfo = \"label+percent+name\",\n                    domain  = dict(x = [0,.48]),\n                    name    = \"attrition employes\",\n                    marker  = dict(colors = colors, line = dict(width = 1.5)))\n    trace2 = go.Pie(values  = no_attrition[var_select].value_counts().values.tolist(),\n                    labels  = no_attrition[var_select].value_counts().keys().tolist(),\n                    textfont=dict(size=15), opacity = 0.8,\n                    hoverinfo = \"label+percent+name\",\n                    marker  = dict(colors = colors, line = dict(width = 1.5)),\n                    domain  = dict(x = [.52,1]),\n                    name    = \"Non attrition employes\" )\n\n    layout = go.Layout(dict(title = var_select + \" distribution in employes attrition \",\n                            annotations = [dict(text = \"Yes_attrition\",\n                                                font = dict(size = 13),\n                                                showarrow = False,\n                                                x = .22, y = -0.1),\n                                            dict(text = \"No_attrition\",\n                                                font = dict(size = 13),\n                                                showarrow = False,\n                                                x = .8,y = -.1)]))\n                                          \n\n    fig  = go.Figure(data = [trace1,trace2],layout = layout)\n    py.iplot(fig)","cf161c22":"plot_pie(\"Gender\")\nbarplot('Gender',True)\nplot_pie('OverTime')\nbarplot('OverTime',True)\nplot_pie('BusinessTravel')\nbarplot('BusinessTravel',True)\nplot_pie('JobRole')\nbarplot('JobRole',True)\nplot_pie('Department') \nbarplot('Department',True)\nplot_pie('MaritalStatus') \nbarplot('MaritalStatus',True)\nplot_pie('EducationField') \nbarplot('EducationField',True)\nplot_pie('Education') \nbarplot('Education',False)\nplot_pie('EnvironmentSatisfaction')\nbarplot('EnvironmentSatisfaction',False)\nplot_pie('JobInvolvement')\nbarplot('JobInvolvement', False)\nplot_pie('JobLevel')\nbarplot('JobLevel',False)\nplot_pie('JobSatisfaction')\nbarplot('JobSatisfaction',False)\nplot_pie('PerformanceRating')\nbarplot('PerformanceRating',False)\nplot_pie('RelationshipSatisfaction')\nbarplot('RelationshipSatisfaction', False)\nplot_pie('StockOptionLevel')\nbarplot('StockOptionLevel', False)\nplot_pie('WorkLifeBalance')\nbarplot('WorkLifeBalance', False)","8d966d63":"df_categorical = df[['Attrition', 'BusinessTravel','Department',\n                       'EducationField','Gender','JobRole',\n                       'MaritalStatus',\n                       'OverTime']].copy()\ndf_categorical.head()","37219982":"fig, axes = plt.subplots(round(len(df_categorical.columns) \/ 3), 3, figsize=(12, 20))\n\nfor i, ax in enumerate(fig.axes):\n    if i < len(df_categorical.columns):\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45)\n        sns.countplot(x=df_categorical.columns[i], alpha=0.7, data=df_categorical, ax=ax)\n\nfig.tight_layout()","9875ee7f":"# We create dummies for the remaining categorical variables\n\ndf_categorical = pd.get_dummies(df_categorical)\ndf_categorical.head()","4c43a7dd":"#Correlation Matrix\ncorr = df.corr()\ncorr = (corr)\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)\n\ncorr","1b1b1f9b":"#After removing the strongly correlated variables\ndf_numerical = df[['Age','DailyRate','DistanceFromHome','Education',\n                       'EnvironmentSatisfaction', 'HourlyRate',                     \n                       'JobInvolvement', 'JobLevel','MonthlyRate',\n                       'JobSatisfaction',\n                       'RelationshipSatisfaction', \n                       'StockOptionLevel',\n                        'TrainingTimesLastYear','WorkLifeBalance']].copy()\ndf_numerical.head()","a300fe62":"#Standardizing the numerical values\ndf_numerical = abs(df_numerical - df_numerical.mean())\/df_numerical.std()  \ndf_numerical.head()","6c57936a":"final_df = pd.concat([df_numerical,df_categorical], axis= 1)\nfinal_df.head()","89f1966f":"X = final_df.drop(['Attrition'],axis= 1)\ny = final_df[\"Attrition\"]","43a3cb5e":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.4,random_state = 4)","352d7749":"lr = LogisticRegression(solver = 'liblinear',random_state = 0) #Since this a small dataset, we use liblinear solver and Regularization strength as\n# default i.e C = 1.0\nlr.fit(X_train,y_train)","2a964811":"y_pred_lr = lr.predict(X_test)\naccuracy_score_lr = accuracy_score(y_pred_lr,y_test)\naccuracy_score_lr \n#Logistic Regression shows 85.7 percent accuracy","6fc20fcb":"dtree = DecisionTreeClassifier(criterion='entropy',max_depth = 4,random_state = 0)\ndtree.fit(X_train,y_train)","0df7ccdb":"y_pred_dtree = dtree.predict(X_test)\naccuracy_score_dtree = accuracy_score(y_pred_dtree,y_test)\naccuracy_score_dtree","e48eeba2":"rf = RandomForestClassifier(criterion = 'gini',random_state = 0)\nrf.fit(X_train,y_train)","641cb2a8":"y_pred_rf = rf.predict(X_test)\naccuracy_score_rf = accuracy_score(y_pred_rf,y_test)\naccuracy_score_rf","a623797c":"sv = svm.SVC(kernel= 'linear',gamma =2)\nsv.fit(X_train,y_train)","100caa69":"y_pred_svm = sv.predict(X_test)\naccuracy_score_svm = accuracy_score(y_pred_svm,y_test)\naccuracy_score_svm","44e6d927":"knn = KNeighborsClassifier(n_neighbors = 2)\nknn.fit(X_train,y_train)","8688b600":"y_pred_knn = knn.predict(X_test)\naccuracy_score_knn = accuracy_score(y_pred_knn,y_test)\naccuracy_score_knn","f32849a3":"scores = [accuracy_score_lr,accuracy_score_dtree,accuracy_score_rf,accuracy_score_svm,accuracy_score_knn]\nscores = [i*100 for i in scores]\nalgorithm  = ['Logistic Regression','Decision Tree','Random Forest','SVM', 'K-Means']\nindex = np.arange(len(algorithm))\nplt.bar(index, scores)\nplt.xlabel('Algorithm', fontsize=10)\nplt.ylabel('Accuracy Score', fontsize=5)\nplt.xticks(index, algorithm, fontsize=10, rotation=30)\nplt.title('Accuracy scores for each classification algorithm')\nplt.ylim(80,100)\nplt.show()    ","62192fa3":"feat_importances = pd.Series(rf.feature_importances_, index=X.columns)\nfeat_importances= feat_importances.nlargest(5)\nfeat_importances.plot(kind='barh')\nplt.show()","ae002563":"### Support Vector Machines ","25404e7b":"With these at the forefront, it highlights the general steps that can be taken to work towards any dataset. For the given attrition dataset, I wanted to focus more on the graphical and visual representation aspect while keeping the machine-learning concepts in the back-burner.","ac5a3265":"Since the given dataset is split into a numeric values such as Age and labeled values such as Department, it seems prudent to partition the data on quantitative and categorical values. This makes it easier to fine-tune the model for accurate results.","9d84e154":"A correlation matrix is the fastest way to see which metrics are related to each other. Although correlation does not mean causation, it is a great way to estimate the underlying behaviors within the dataset.","b44df110":"One thing is for sure, there is a LOT of data out there. Ever since we have built on the idea of assigning values to items, we have always been thinking about what-if scenarios on how we can make the most out of what we have. \n\nIt is no surprise that over the several years of exploratory advancements with numbers, we have at the cusp of unlocking the next big thing when it comes to Data Analysis. What we can do with the data we have and how we can optimize it to maximize our objectives, it almost seems magical.\n\nWith so many applications out there, I find it amazing to dive deeper into the realm of analytics and see what amazing insights can be taken. With that, I have chosen the concept of employee attrition and what steps we can take when looking at the signs to maximize employee retention.\n\nBefore I dive into the technical part, I would like to thank the following people from who I have gained valuable insights while implementing this project:\n","0a9ffe50":"### Categorical Data","ea370a61":"## STEP 4: Feature Engineering ","e41e6e21":"### Perfroming One-Hot Encoding on the categorical data ","7d22322f":"Comparing the various classifiers","b6d239ab":"## Step 2: Exploring the data","4161f8b3":"With credit given where credit is due, this Exploratory Data Analysis (EDA) encompasses with the amalgamation of concepts related to cleaning, defining and analyzing data to obtain meaningful insights. So let\u2019s dive in shall we?","c4caa740":"### Obtaining the quantitative data","dd9864f3":"With these implemented, we get an estimate on what would be the best approach to take.","1efeff68":"### DecisionTreeClassifier ","88655ab1":"Once you obtain the data, it is necessary to clean the dataset so that the valuable insights have a defined meaning. There are a number of ways for cleaning data, ranging from null-values to outlier detection; each with its own unique implementation.","0948a156":"1.\tLogistic Regression \u2013 Sine there are 1\/0 value datasets, it makes it ideal to estimate.\n2.\tDecision Tree \u2013 An easy to implement classification method.\n3.\tRandom Forrest \u2013 Decision trees with overfitting prevention\n4.\tSVM \u2013 Classify data based on its location on the hyperplane\n5.\tKNN - Classify data based on its location with similar clusters\n","1bfd0153":"### Quantative data","56b35377":"Another way of estimating relations between two values arise from the Kernel Density Estimate (KDE) plots.  This makes it easier to highlight the bivariate relationship between data distributions that occur within the framework. Some of the key insights are as follows:","dde75ba6":"Now that the data has been prepped for analysis, it is time to apply machine learning methodologies to the dataset.\n\nThe following technique were used to implement the dataset to estimate the best model that would be used","63d40d9f":"Now from the above heat-map plot, we can see that YearsAtCompany,YearsInCurrentRole,YearsSinceLastPromotion and YearsWtihCurrManager are closely related, Hence we drop those columns. Also, we'll drop the following rows based on their High Correlation coefficient from the plot:\n\n1. TotalWorkingYears\n2. PercentSalaryHike\n3. PerformanceRating\n4. NumCompaniesWorked\n5. MonthlyIncome\n\nWe will also delete EmployeeCount and StandardHours as they do not have any variance between their values.","267c2677":"### Logistic Regression ","c93f9680":"As we can see, SVM and Random forests take the lead. We can break down the metrics to see what the most driving factors are:","a099864b":"Data in its innate nature needs to be molded so that we can mine it for insights. The main drive behind this is to fine-tune the data so that we can build actionable metrics from the data as a whole. With this, both the categorical and quantitative data ca be shaped.","e4a51421":"### KNN Algorithm ","61974b6f":"Using multiple classfications alogorithms and selecting the best out of them:","1b384b9f":"# Gazing into the Data Abyss: Employee Attrition","a467b458":"## Step 3: Transforming the data to build insights","6c07220b":"[Randy Lao](https:\/\/www.kaggle.com\/randylaosat) : Predicting Employee Kernelover    \n\nhttps:\/\/www.kaggle.com\/randylaosat\/predicting-employee-kernelover\n\n<br>\n\n[Tuatini GODARD](https:\/\/www.kaggle.com\/ekami66) : Detailed exploratory data analysis with python\n\nhttps:\/\/www.kaggle.com\/ekami66\/detailed-exploratory-data-analysis-with-python\n\n<br>\n\n[Vincent Lugat](https:\/\/www.kaggle.com\/vincentlugat) : IBM Attrition Analysis and Prediction   \n\nhttps:\/\/www.kaggle.com\/vincentlugat\/ibm-attrition-analysis-and-prediction\n\n<br>\n\n[Janio Martinez Bachmann](https:\/\/www.kaggle.com\/janiobachmann) : Attrition in an Organization || Why Workers Quit?  \n\nhttps:\/\/www.kaggle.com\/janiobachmann\/attrition-in-an-organization-why-workers-quit\n\n<br>\n\n[Aniket](https:\/\/www.kaggle.com\/aniketkatyal) : IBM Employee Attrition Classifier  \n\nhttps:\/\/www.kaggle.com\/aniketkatyal\/ibm-employee-attrition-classifier\n\n<br>\n\n[Anisotropic](https:\/\/www.kaggle.com\/arthurtok) : Employee attrition via Ensemble tree-based methods  \n\nhttps:\/\/www.kaggle.com\/arthurtok\/employee-attrition-via-ensemble-tree-based-methods\n","fce4cf8f":"One way of visually communicating how data is spread is through histograms. This can give general insights in terms of data distribution. Some of the key takeaways from this dataset are shown below:","0640f18b":"Once cleaned, our dataset would look something like this:","1e78efb8":"### RandomForestClassifier ","b580967e":"If you found something that catches your eye or just have a chat about data science topics in general, I will be more than happy to connect with you on:\n\nLinkedIn: https:\/\/www.linkedin.com\/in\/shawn-dsouza\/\n\nMy Website: https:\/\/shawndsouza29.wixsite.com\/portfolio\n\nThis notebook will always be a work in progress. Please leave any comments about further improvements to the notebook! Any feedback or constructive criticism is greatly appreciated. Thank you guys!","4eb400c0":"## Outcomes and key takeaways","34f543e2":"# **Do Reach Out!**\n","50121390":"Employee retention is one of the biggest metrics that a company should have in mind when thinking of growth. It becomes all the more necessary when you consider the costs of finding, interviewing and hiring the right candidate for the various jobs within the organization. This is all the more detrimental when finding out why people leave the company at an unprecedented rate. Employee attrition is caused when the total strength of the company is greatly reduced as more employees leave the company than expected.\n\nThis data set comes from a fictitious scenario come up by the data scientists at IBM. Although it is not attuned to the real-word scenario, it still is a viable dataset to mine and build insights on.  The main crux of the data is to classify what type of people would leave a company based on 35 parameters. Our categorical target for this dataset is a Yes\/No classification from the attrition. \n\nWith that in mind, the various steps that we have gone into building insights from start to finish are highlighted in FOUR major blocks:\n","ff91f11b":"Based on the above results, how much people earn and how old they are greatly affect their reason for changing to a different company. In a world where we constantly strive to know our worth, it goes to say that we will find greener pastures if we do not find it at a single point. Ultimately, it falls on the company to maximize their employee retention with a focus on their pay. Although it might decrease net company revenue in the short run, the cost of rehiring and re-training each and every time an employee leaves will definitely leave a mark.","6ce79425":"## STEP 1: Collecting and Cleaning the Data","a9abac1b":"Another way of estimating relations between two values arise from the Kernel Density Estimate (KDE) plots.  This makes it easier to highlight the bivariate relationship between data distributions that occur within the framework. Some of the key insights are as followsv","395839ff":"When looking at the bare-bones of the given data, we can find a number of people with distinct categories ranging from age to number of years with their current manager. These parameters are what shape the given dataset and with a bit of machine leaning, we can delve deeper into the main causes of attrition.","c5a42398":"1.\tCollecting and Cleaning the data\n2.\tExploring the data\n3.\tTransforming the data to build insights\n4.\tApplying machine learning models for feature engineering\n","0af8b69c":"\nAll in all, there are so many ways to approach the same data problem from different angles. With practice, the end goal will be to form an effective story with the data to provide actionable insights. Thank you very much for coming along with me on this journey. This was a great learning experience for me and I hope that it would be for you as well. \n","b23bb0f4":"### Combining quantitative and categorical data   ","77f222ea":"###               General Statistical Insights ","623d0746":"When working with categorical values, there may or may not exist a quantifiable priority within the labels. Therefore, one-hot encoding is used to give a quantifiable value to the categorical labels. ","b6376505":"## Attrition: What makes them stay? What makes them go?"}}