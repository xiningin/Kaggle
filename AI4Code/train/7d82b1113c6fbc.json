{"cell_type":{"7f6acead":"code","84f036bd":"code","f9de1d98":"code","68022c5a":"code","fb6104d5":"code","5abe8b7e":"code","95d2cf6b":"code","7abe171e":"code","f8e26d05":"code","e709288b":"code","9a8fa0a9":"code","ff3d8937":"code","cddeadd5":"code","9149037e":"code","92aa6e30":"code","9c3c4588":"code","0dccf4ce":"code","1b3e92ca":"code","94ce1357":"code","18d388bb":"code","1a9c9449":"code","9b77adc5":"code","c9bb3b18":"code","c444a6e7":"code","6248c23e":"code","dd6ceae6":"code","3d1acab3":"code","fb6ddfdf":"code","c832a61d":"code","fa1a9fe8":"code","d920601f":"code","2df1ebe5":"code","283c3952":"code","f7de2acd":"code","d26326ec":"code","c474a531":"code","b0030ee3":"code","a3f65592":"code","c72c6ac5":"code","6cfeb2a7":"code","c8ed07e4":"code","748e8f51":"code","3b4834be":"code","bd00236b":"code","f531cafc":"code","e3500778":"code","f4d65c37":"code","1621cf64":"code","aab8593c":"code","8ef52c50":"code","6951671d":"code","6159f2f7":"markdown","cc803b84":"markdown","7a9f2ccf":"markdown","aa74fbef":"markdown","30450dcd":"markdown","125ce670":"markdown","64cab5fd":"markdown","5b8515cf":"markdown","3fc9cde0":"markdown","3e6c4173":"markdown","4a6f6834":"markdown","2eaa3900":"markdown","c6d8d2c7":"markdown","65c721e0":"markdown","df2dbb3c":"markdown","a8edf68f":"markdown","fd619c45":"markdown","a68bb123":"markdown","4760ea9c":"markdown","4075140e":"markdown","7c02960e":"markdown","0af92696":"markdown","05e799b3":"markdown","7d6e153a":"markdown","0a52b1ea":"markdown","ea89dcad":"markdown","e4588ee0":"markdown","237b6a85":"markdown","0670fd97":"markdown","2d0a355f":"markdown","0efda17b":"markdown","21dbe26d":"markdown","87ccfc96":"markdown","e9711449":"markdown","db488182":"markdown","4a6233d0":"markdown","ebcd9a26":"markdown","d6087d13":"markdown","a1c09989":"markdown","70db8385":"markdown","7ad2dc03":"markdown","c7e7922f":"markdown","8fa49b45":"markdown","9499d830":"markdown","5199d32a":"markdown"},"source":{"7f6acead":"BaselineTraining = True\nLogisticRegression = False\nRandomForest = True\n\nPolynomialFeaturesTesting = False\nDomainFeaturesTesting = True","84f036bd":"import pandas as pd\nimport os, sys\n\ndefault_dir = \"..\/input\/home-credit-default-risk\/\"\n\napp_train = pd.read_csv(os.path.join(default_dir,'application_train.csv'))\napp_test = pd.read_csv(os.path.join(default_dir,'application_test.csv'))\n\nprint(f'Training Data Shape: {app_train.shape}')\nprint(f'Testing Data Shape: {app_test.shape}')\n\napp_test.head()  \n","f9de1d98":"import numpy as np\n\ndef get_balance_data():\n    pos_dtype = {\n        'SK_ID_PREV':np.uint32, 'SK_ID_CURR':np.uint32, 'MONTHS_BALANCE':np.int32, 'SK_DPD':np.int32,\n        'SK_DPD_DEF':np.int32, 'CNT_INSTALMENT':np.float32,'CNT_INSTALMENT_FUTURE':np.float32\n    }\n\n    install_dtype = {\n        'SK_ID_PREV':np.uint32, 'SK_ID_CURR':np.uint32, 'NUM_INSTALMENT_NUMBER':np.int32, 'NUM_INSTALMENT_VERSION':np.float32,\n        'DAYS_INSTALMENT':np.float32, 'DAYS_ENTRY_PAYMENT':np.float32, 'AMT_INSTALMENT':np.float32, 'AMT_PAYMENT':np.float32\n    }\n\n    card_dtype = {\n        'SK_ID_PREV':np.uint32, 'SK_ID_CURR':np.uint32, 'MONTHS_BALANCE':np.int16,\n        'AMT_CREDIT_LIMIT_ACTUAL':np.int32, 'CNT_DRAWINGS_CURRENT':np.int32, 'SK_DPD':np.int32,'SK_DPD_DEF':np.int32,\n        'AMT_BALANCE':np.float32, 'AMT_DRAWINGS_ATM_CURRENT':np.float32, 'AMT_DRAWINGS_CURRENT':np.float32,\n        'AMT_DRAWINGS_OTHER_CURRENT':np.float32, 'AMT_DRAWINGS_POS_CURRENT':np.float32, 'AMT_INST_MIN_REGULARITY':np.float32,\n        'AMT_PAYMENT_CURRENT':np.float32, 'AMT_PAYMENT_TOTAL_CURRENT':np.float32, 'AMT_RECEIVABLE_PRINCIPAL':np.float32,\n        'AMT_RECIVABLE':np.float32, 'AMT_TOTAL_RECEIVABLE':np.float32, 'CNT_DRAWINGS_ATM_CURRENT':np.float32,\n        'CNT_DRAWINGS_OTHER_CURRENT':np.float32, 'CNT_DRAWINGS_POS_CURRENT':np.float32, 'CNT_INSTALMENT_MATURE_CUM':np.float32\n    }\n\n    pos_bal = pd.read_csv(os.path.join(default_dir,'POS_CASH_balance.csv'), dtype=pos_dtype)\n    install = pd.read_csv(os.path.join(default_dir,'installments_payments.csv'), dtype=install_dtype)\n    card_bal = pd.read_csv(os.path.join(default_dir, 'credit_card_balance.csv'), dtype=card_dtype)\n\n    return pos_bal, install, card_bal\n\npos_bal, install, card_bal = get_balance_data()","68022c5a":"# app_train['TARGET'].value_counts()\napp_train['TARGET'].astype(int).plot.hist()","fb6104d5":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","5abe8b7e":"missing_values = missing_values_table(app_train)\nmissing_values.head(20)","95d2cf6b":"app_train.dtypes.value_counts()","7abe171e":"app_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","f8e26d05":"from sklearn.preprocessing import LabelEncoder\n\n# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(app_train[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(app_train[col])\n            # Transform both training and testing data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","e709288b":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","9a8fa0a9":"train_labels = app_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n\n# Add the target back in\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","ff3d8937":"# (app_train['DAYS_BIRTH'] \/ -365).describe()\n(app_train['DAYS_EMPLOYED'] \/ -365).describe()","cddeadd5":"import matplotlib.pyplot as plt\n\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram')\napp_test['DAYS_EMPLOYED'].plot.hist()\nplt.xlabel('Days Employment')\n\napp_test['DAYS_EMPLOYED'][app_test['DAYS_EMPLOYED'] > 200000]","9149037e":"anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))","92aa6e30":"# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\napp_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\n\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram')\napp_test['DAYS_EMPLOYED'].plot.hist()\nplt.xlabel('Days Employment')\n\nprint('There are %d anomalies in the test data out of %d entries\\n' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))","9c3c4588":"# Find correlations with the target and sort\ncorrelations = app_train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","0dccf4ce":"# Find the correlation of the positive days since birth and target\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])","1b3e92ca":"# Set the style of plots\nplt.style.use('fivethirtyeight')\n\n# Plot the distribution of ages in years\nplt.hist(app_train['DAYS_BIRTH'] \/ 365, edgecolor = 'k', bins = 25)\nplt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count')","94ce1357":"import seaborn as sns\n\nplt.figure(figsize = (10, 8))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] \/ 365, label = 'target == 0')\n\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] \/ 365, label = 'target == 1')\n\n# Labeling of plot\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages')","18d388bb":"# Age information into a separate dataframe\nage_data = app_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] \/ 365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\nage_data.head(10)","1a9c9449":"# Group by the bin and calculate averages\nage_groups  = age_data.groupby('YEARS_BINNED').mean()\nage_groups","9b77adc5":"plt.figure(figsize = (8, 8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n\n# Plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group')","c9bb3b18":"ext_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","c444a6e7":"plt.figure(figsize = (8, 6))\n\n# Heatmap of correlations\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap')","6248c23e":"plt.figure(figsize = (10, 12))\n\n# iterate through the sources\nfor i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n    \n    # create a new subplot for each source\n    plt.subplot(3, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","dd6ceae6":"# Make a new dataframe for polynomial features\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n\n# imputer for handling missing values\n# from sklearn.preprocessing import Imputer\n# imputer = Imputer(strategy = 'median')\n# cannot import name 'Imputer' from 'sklearn.preprocessing'\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n\npoly_target = poly_features['TARGET']\n\npoly_features = poly_features.drop(columns = ['TARGET'])\n\n# Need to impute missing values\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.transform(poly_features_test)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n                                  \n# Create the polynomial object with specified degree\npoly_transformer = PolynomialFeatures(degree = 3)","3d1acab3":"# Train the polynomial features\npoly_transformer.fit(poly_features)\n\n# Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)","fb6ddfdf":"poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]","c832a61d":"# Create a dataframe of the features \npoly_features = pd.DataFrame(poly_features, \n                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                         'EXT_SOURCE_3', 'DAYS_BIRTH']))\n# Add in the target\npoly_features['TARGET'] = poly_target\n\n# Find the correlations with the target\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\n# Display most negative and most positive\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail(5))","fa1a9fe8":"# Put test features into dataframe\npoly_features_test = pd.DataFrame(poly_features_test, \n                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Merge polynomial features into training dataframe\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n\n# Merge polnomial features into testing dataframe\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n\n# Align the dataframes\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape: ', app_train_poly.shape)\nprint('Testing data with polynomial features shape:  ', app_test_poly.shape)","d920601f":"app_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] \/ app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] \/ app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] \/ app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] \/ app_train_domain['DAYS_BIRTH']\n\napp_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] \/ app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] \/ app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] \/ app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] \/ app_test_domain['DAYS_BIRTH']\n\nplt.figure(figsize = (12, 20))\n# iterate through the new features\nfor i, feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']):\n    \n    # create a new subplot for each source\n    plt.subplot(4, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 0, feature], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 1, feature], label = 'target == 1')\n    \n    # matplotlib.pyplot.legend\n    # https:\/\/matplotlib.org\/stable\/api\/_as_gen\/matplotlib.pyplot.legend.html\n    plt.legend()\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % feature)\n    plt.xlabel('%s' % feature); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","2df1ebe5":"def get_apps_processed(apps):\n    \"\"\"\n    feature engineering for apps\n    \"\"\"\n\n    # 1.EXT_SOURCE_X FEATURE \n    apps['APPS_EXT_SOURCE_MEAN'] = apps[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis = 1)\n    apps['APPS_EXT_SOURCE_STD'] = apps[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n    apps['APPS_EXT_SOURCE_STD'] = apps['APPS_EXT_SOURCE_STD'].fillna(apps['APPS_EXT_SOURCE_STD'].mean())\n    \n    # AMT_CREDIT \n    apps['APPS_ANNUITY_CREDIT_RATIO'] = apps['AMT_ANNUITY']\/apps['AMT_CREDIT']\n    apps['APPS_GOODS_CREDIT_RATIO'] = apps['AMT_GOODS_PRICE']\/apps['AMT_CREDIT']\n    \n    # AMT_INCOME_TOTAL \n    apps['APPS_ANNUITY_INCOME_RATIO'] = apps['AMT_ANNUITY']\/apps['AMT_INCOME_TOTAL']\n    apps['APPS_CREDIT_INCOME_RATIO'] = apps['AMT_CREDIT']\/apps['AMT_INCOME_TOTAL']\n    apps['APPS_GOODS_INCOME_RATIO'] = apps['AMT_GOODS_PRICE']\/apps['AMT_INCOME_TOTAL']\n    apps['APPS_CNT_FAM_INCOME_RATIO'] = apps['AMT_INCOME_TOTAL']\/apps['CNT_FAM_MEMBERS']\n    \n    # DAYS_BIRTH, DAYS_EMPLOYED \n    apps['APPS_EMPLOYED_BIRTH_RATIO'] = apps['DAYS_EMPLOYED']\/apps['DAYS_BIRTH']\n    apps['APPS_INCOME_EMPLOYED_RATIO'] = apps['AMT_INCOME_TOTAL']\/apps['DAYS_EMPLOYED']\n    apps['APPS_INCOME_BIRTH_RATIO'] = apps['AMT_INCOME_TOTAL']\/apps['DAYS_BIRTH']\n    apps['APPS_CAR_BIRTH_RATIO'] = apps['OWN_CAR_AGE'] \/ apps['DAYS_BIRTH']\n    apps['APPS_CAR_EMPLOYED_RATIO'] = apps['OWN_CAR_AGE'] \/ apps['DAYS_EMPLOYED']\n    \n    return apps","283c3952":"def get_prev_processed(prev):\n    \"\"\"\n    feature engineering \n    for previouse application credit history\n    \"\"\"\n    prev['PREV_CREDIT_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_CREDIT']\n    prev['PREV_GOODS_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_GOODS_PRICE']\n    prev['PREV_CREDIT_APPL_RATIO'] = prev['AMT_CREDIT']\/prev['AMT_APPLICATION']\n    # prev['PREV_ANNUITY_APPL_RATIO'] = prev['AMT_ANNUITY']\/prev['AMT_APPLICATION']\n    prev['PREV_GOODS_APPL_RATIO'] = prev['AMT_GOODS_PRICE']\/prev['AMT_APPLICATION']\n\n    # Data Cleansing\n    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n\n    # substraction between DAYS_LAST_DUE_1ST_VERSION and DAYS_LAST_DUE\n    prev['PREV_DAYS_LAST_DUE_DIFF'] = prev['DAYS_LAST_DUE_1ST_VERSION'] - prev['DAYS_LAST_DUE']\n\n    # 1.Calculate the interest rate\n    all_pay = prev['AMT_ANNUITY'] * prev['CNT_PAYMENT']\n    prev['PREV_INTERESTS_RATE'] = (all_pay\/prev['AMT_CREDIT'] - 1)\/prev['CNT_PAYMENT']\n\n    return prev\n\ndef get_prev_amt_agg(prev):\n    \"\"\"\n    feature engineering for the previous credit appliction\n    \"\"\"\n\n    agg_dict = {\n      'SK_ID_CURR':['count'],\n      'AMT_CREDIT':['mean', 'max', 'sum'],\n      'AMT_ANNUITY':['mean', 'max', 'sum'], \n      'AMT_APPLICATION':['mean', 'max', 'sum'],\n      'AMT_DOWN_PAYMENT':['mean', 'max', 'sum'],\n      'AMT_GOODS_PRICE':['mean', 'max', 'sum'],\n      'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n      'DAYS_DECISION': ['min', 'max', 'mean'],\n      'CNT_PAYMENT': ['mean', 'sum'],\n        \n      'PREV_CREDIT_DIFF':['mean', 'max', 'sum'], \n      'PREV_CREDIT_APPL_RATIO':['mean', 'max'],\n      'PREV_GOODS_DIFF':['mean', 'max', 'sum'],\n      'PREV_GOODS_APPL_RATIO':['mean', 'max'],\n      'PREV_DAYS_LAST_DUE_DIFF':['mean', 'max', 'sum'],\n      'PREV_INTERESTS_RATE':['mean', 'max']\n    }\n\n    prev_group = prev.groupby('SK_ID_CURR')\n    prev_amt_agg = prev_group.agg(agg_dict)\n\n    # multi index \n    prev_amt_agg.columns = [\"PREV_\"+ \"_\".join(x).upper() for x in prev_amt_agg.columns.ravel()]\n\n    return prev_amt_agg\n\ndef get_prev_refused_appr_agg(prev):\n    \"\"\"\n    PREV_APPROVED_COUNT : Credit application approved count\n    PREV_REFUSED_COUNT :  Credit application refused count\n    \"\"\"\n    prev_refused_appr_group = prev[prev['NAME_CONTRACT_STATUS'].isin(['Approved', 'Refused'])].groupby([ 'SK_ID_CURR', 'NAME_CONTRACT_STATUS'])\n    # unstack() \n    prev_refused_appr_agg = prev_refused_appr_group['SK_ID_CURR'].count().unstack()\n\n    # rename column \n    prev_refused_appr_agg.columns = ['PREV_APPROVED_COUNT', 'PREV_REFUSED_COUNT' ]\n\n    # NaN\n    prev_refused_appr_agg = prev_refused_appr_agg.fillna(0)\n\n    return prev_refused_appr_agg\n\n# DAYS_DECISION\ndef get_prev_days365_agg(prev):\n    \"\"\"\n    DAYS_DESCISION means How many days have been take since the previous credit application made.\n    Somehow this feature is important.\n    \"\"\"\n    cond_days365 = prev['DAYS_DECISION'] > -365\n    prev_days365_group = prev[cond_days365].groupby('SK_ID_CURR')\n    agg_dict = {\n      'SK_ID_CURR':['count'],\n      'AMT_CREDIT':['mean', 'max', 'sum'],\n      'AMT_ANNUITY':['mean', 'max', 'sum'], \n      'AMT_APPLICATION':['mean', 'max', 'sum'],\n      'AMT_DOWN_PAYMENT':['mean', 'max', 'sum'],\n      'AMT_GOODS_PRICE':['mean', 'max', 'sum'],\n      'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n      'DAYS_DECISION': ['min', 'max', 'mean'],\n      'CNT_PAYMENT': ['mean', 'sum'],\n      \n      'PREV_CREDIT_DIFF':['mean', 'max', 'sum'], \n      'PREV_CREDIT_APPL_RATIO':['mean', 'max'],\n      'PREV_GOODS_DIFF':['mean', 'max', 'sum'],\n      'PREV_GOODS_APPL_RATIO':['mean', 'max'],\n      'PREV_DAYS_LAST_DUE_DIFF':['mean', 'max', 'sum'],\n      'PREV_INTERESTS_RATE':['mean', 'max']\n    }\n\n    prev_days365_agg = prev_days365_group.agg(agg_dict)\n\n    # multi index \n    prev_days365_agg.columns = [\"PREV_D365_\"+ \"_\".join(x).upper() for x in prev_days365_agg.columns.ravel()]\n\n    return prev_days365_agg\n\ndef get_prev_agg(prev):\n    prev = get_prev_processed(prev)\n    prev_amt_agg = get_prev_amt_agg(prev)\n    prev_refused_appr_agg = get_prev_refused_appr_agg(prev)\n    prev_days365_agg = get_prev_days365_agg(prev)\n    \n    # prev_amt_agg\n    prev_agg = prev_amt_agg.merge(prev_refused_appr_agg, on='SK_ID_CURR', how='left')\n    prev_agg = prev_agg.merge(prev_days365_agg, on='SK_ID_CURR', how='left')\n    # SK_ID_CURR APPROVED_COUNT REFUSED_COUNT\n    prev_agg['PREV_REFUSED_RATIO'] = prev_agg['PREV_REFUSED_COUNT']\/prev_agg['PREV_SK_ID_CURR_COUNT']\n    prev_agg['PREV_APPROVED_RATIO'] = prev_agg['PREV_APPROVED_COUNT']\/prev_agg['PREV_SK_ID_CURR_COUNT']\n    # 'PREV_REFUSED_COUNT', 'PREV_APPROVED_COUNT' drop \n    prev_agg = prev_agg.drop(['PREV_REFUSED_COUNT', 'PREV_APPROVED_COUNT'], axis=1)\n    \n    return prev_agg","f7de2acd":"def get_bureau_processed(bureau):\n    bureau['BUREAU_ENDDATE_FACT_DIFF'] = bureau['DAYS_CREDIT_ENDDATE'] - bureau['DAYS_ENDDATE_FACT']\n    bureau['BUREAU_CREDIT_FACT_DIFF'] = bureau['DAYS_CREDIT'] - bureau['DAYS_ENDDATE_FACT']\n    bureau['BUREAU_CREDIT_ENDDATE_DIFF'] = bureau['DAYS_CREDIT'] - bureau['DAYS_CREDIT_ENDDATE']\n  \n    bureau['BUREAU_CREDIT_DEBT_RATIO']=bureau['AMT_CREDIT_SUM_DEBT']\/bureau['AMT_CREDIT_SUM']\n    #bureau['BUREAU_CREDIT_DEBT_DIFF'] = bureau['AMT_CREDIT_SUM'] - bureau['AMT_CREDIT_SUM_DEBT']\n    bureau['BUREAU_CREDIT_DEBT_DIFF'] = bureau['AMT_CREDIT_SUM_DEBT'] - bureau['AMT_CREDIT_SUM']\n    \n    bureau['BUREAU_IS_DPD'] = bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x > 0 else 0)\n    bureau['BUREAU_IS_DPD_OVER120'] = bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x >120 else 0)\n    \n    return bureau\n\ndef get_bureau_day_amt_agg(bureau):\n    bureau_agg_dict = {\n    'SK_ID_BUREAU':['count'],\n    'DAYS_CREDIT':['min', 'max', 'mean'],\n    'CREDIT_DAY_OVERDUE':['min', 'max', 'mean'],\n    'DAYS_CREDIT_ENDDATE':['min', 'max', 'mean'],\n    'DAYS_ENDDATE_FACT':['min', 'max', 'mean'],\n    'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n    'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n    'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n    'AMT_CREDIT_SUM_OVERDUE': ['max', 'mean', 'sum'],\n    'AMT_ANNUITY': ['max', 'mean', 'sum'],\n\n    'BUREAU_ENDDATE_FACT_DIFF':['min', 'max', 'mean'],\n    'BUREAU_CREDIT_FACT_DIFF':['min', 'max', 'mean'],\n    'BUREAU_CREDIT_ENDDATE_DIFF':['min', 'max', 'mean'],\n    'BUREAU_CREDIT_DEBT_RATIO':['min', 'max', 'mean'],\n    'BUREAU_CREDIT_DEBT_DIFF':['min', 'max', 'mean'],\n    'BUREAU_IS_DPD':['mean', 'sum'],\n    'BUREAU_IS_DPD_OVER120':['mean', 'sum']\n    }\n\n    bureau_grp = bureau.groupby('SK_ID_CURR')\n    bureau_day_amt_agg = bureau_grp.agg(bureau_agg_dict)\n    bureau_day_amt_agg.columns = ['BUREAU_'+('_').join(column).upper() for column in bureau_day_amt_agg.columns.ravel()]\n    # SK_ID_CURR reset_index()\n    bureau_day_amt_agg = bureau_day_amt_agg.reset_index()\n    #print('bureau_day_amt_agg shape:', bureau_day_amt_agg.shape)\n    return bureau_day_amt_agg\n\ndef get_bureau_active_agg(bureau):\n    '''\n    Bureau CREDIT_ACTIVE='Active' filtering\n    SK_ID_CURR aggregation\n    '''\n    # CREDIT_ACTIVE='Active' filtering\n    cond_active = bureau['CREDIT_ACTIVE'] == 'Active'\n    bureau_active_grp = bureau[cond_active].groupby(['SK_ID_CURR'])\n    bureau_agg_dict = {\n      'SK_ID_BUREAU':['count'],\n      'DAYS_CREDIT':['min', 'max', 'mean'],\n      'CREDIT_DAY_OVERDUE':['min', 'max', 'mean'],\n      'DAYS_CREDIT_ENDDATE':['min', 'max', 'mean'],\n      'DAYS_ENDDATE_FACT':['min', 'max', 'mean'],\n      'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n      'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n      'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n      'AMT_CREDIT_SUM_OVERDUE': ['max', 'mean', 'sum'],\n      'AMT_ANNUITY': ['max', 'mean', 'sum'],\n\n      'BUREAU_ENDDATE_FACT_DIFF':['min', 'max', 'mean'],\n      'BUREAU_CREDIT_FACT_DIFF':['min', 'max', 'mean'],\n      'BUREAU_CREDIT_ENDDATE_DIFF':['min', 'max', 'mean'],\n      'BUREAU_CREDIT_DEBT_RATIO':['min', 'max', 'mean'],\n      'BUREAU_CREDIT_DEBT_DIFF':['min', 'max', 'mean'],\n      'BUREAU_IS_DPD':['mean', 'sum'],\n      'BUREAU_IS_DPD_OVER120':['mean', 'sum']\n      }\n\n    bureau_active_agg = bureau_active_grp.agg(bureau_agg_dict)\n    bureau_active_agg.columns = ['BUREAU_ACT_'+('_').join(column).upper() for column in bureau_active_agg.columns.ravel()]\n    # SK_ID_CURR reset_index() \n    bureau_active_agg = bureau_active_agg.reset_index()\n    #print('bureau_active_agg shape:', bureau_active_agg.shape)\n    return bureau_active_agg\n\n# BUREAU DAYS_CREDIT -750\ndef get_bureau_days750_agg(bureau):\n    cond_days750 = bureau['DAYS_CREDIT'] > -750\n    bureau_days750_group = bureau[cond_days750].groupby('SK_ID_CURR')\n    bureau_agg_dict = {\n        'SK_ID_BUREAU':['count'],\n        'DAYS_CREDIT':['min', 'max', 'mean'],\n        'CREDIT_DAY_OVERDUE':['min', 'max', 'mean'],\n        'DAYS_CREDIT_ENDDATE':['min', 'max', 'mean'],\n        'DAYS_ENDDATE_FACT':['min', 'max', 'mean'],\n        'AMT_CREDIT_MAX_OVERDUE': ['max', 'mean'],\n        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n        'AMT_CREDIT_SUM_OVERDUE': ['max', 'mean', 'sum'],\n        'AMT_ANNUITY': ['max', 'mean', 'sum'],\n        \n        'BUREAU_ENDDATE_FACT_DIFF':['min', 'max', 'mean'],\n        'BUREAU_CREDIT_FACT_DIFF':['min', 'max', 'mean'],\n        'BUREAU_CREDIT_ENDDATE_DIFF':['min', 'max', 'mean'],\n        'BUREAU_CREDIT_DEBT_RATIO':['min', 'max', 'mean'],\n        'BUREAU_CREDIT_DEBT_DIFF':['min', 'max', 'mean'],\n        'BUREAU_IS_DPD':['mean', 'sum'],\n        'BUREAU_IS_DPD_OVER120':['mean', 'sum']\n        }\n\n    bureau_days750_agg = bureau_days750_group.agg(bureau_agg_dict)\n    bureau_days750_agg.columns = ['BUREAU_ACT_'+('_').join(column).upper() for column in bureau_days750_agg.columns.ravel()]\n    bureau_days750_agg = bureau_days750_agg.reset_index()\n    \n    return bureau_days750_agg\n\n# bureau_bal SK_ID_CURR MONTHS_BALANCE aggregation\ndef get_bureau_bal_agg(bureau, bureau_bal):\n    # SK_ID_CURR Group by bureau SK_ID_CURR \n    bureau_bal = bureau_bal.merge(bureau[['SK_ID_CURR', 'SK_ID_BUREAU']], on='SK_ID_BUREAU', how='left')\n    \n    # STATUS 120\n    bureau_bal['BUREAU_BAL_IS_DPD'] = bureau_bal['STATUS'].apply(lambda x: 1 if x in['1','2','3','4','5']  else 0)\n    bureau_bal['BUREAU_BAL_IS_DPD_OVER120'] = bureau_bal['STATUS'].apply(lambda x: 1 if x =='5'  else 0)\n    bureau_bal_grp = bureau_bal.groupby('SK_ID_CURR')\n    # SK_ID_CURR MONTHS_BALANCE aggregation\n    bureau_bal_agg_dict = {\n        'SK_ID_CURR':['count'],\n        'MONTHS_BALANCE':['min', 'max', 'mean'],\n        'BUREAU_BAL_IS_DPD':['mean', 'sum'],\n        'BUREAU_BAL_IS_DPD_OVER120':['mean', 'sum']\n    }\n\n    bureau_bal_agg = bureau_bal_grp.agg(bureau_bal_agg_dict)\n    bureau_bal_agg.columns = [ 'BUREAU_BAL_'+('_').join(column).upper() for column in bureau_bal_agg.columns.ravel() ]\n    # SK_ID_CURR reset_index()\n    bureau_bal_agg = bureau_bal_agg.reset_index()\n    #print('bureau_bal_agg shape:', bureau_bal_agg.shape)\n    return bureau_bal_agg\n\n# bureau aggregation\ndef get_bureau_agg(bureau, bureau_bal):\n    \n    bureau = get_bureau_processed(bureau)\n    bureau_day_amt_agg = get_bureau_day_amt_agg(bureau)\n    bureau_active_agg = get_bureau_active_agg(bureau)\n    bureau_days750_agg = get_bureau_days750_agg(bureau)\n    bureau_bal_agg = get_bureau_bal_agg(bureau, bureau_bal)\n    \n    # bureau_day_amt_agg bureau_active_agg\n    bureau_agg = bureau_day_amt_agg.merge(bureau_active_agg, on='SK_ID_CURR', how='left')\n    # STATUS ACTIVE IS_DPD RATIO\n    #bureau_agg['BUREAU_IS_DPD_RATIO'] = bureau_agg['BUREAU_BUREAU_IS_DPD_SUM']\/bureau_agg['BUREAU_SK_ID_BUREAU_COUNT']\n    #bureau_agg['BUREAU_IS_DPD_OVER120_RATIO'] = bureau_agg['BUREAU_BUREAU_IS_DPD_OVER120_SUM']\/bureau_agg['BUREAU_SK_ID_BUREAU_COUNT']\n    bureau_agg['BUREAU_ACT_IS_DPD_RATIO'] = bureau_agg['BUREAU_ACT_BUREAU_IS_DPD_SUM']\/bureau_agg['BUREAU_SK_ID_BUREAU_COUNT']\n    bureau_agg['BUREAU_ACT_IS_DPD_OVER120_RATIO'] = bureau_agg['BUREAU_ACT_BUREAU_IS_DPD_OVER120_SUM']\/bureau_agg['BUREAU_SK_ID_BUREAU_COUNT']\n    \n    # bureau_agg bureau_bal_agg\n    bureau_agg = bureau_agg.merge(bureau_bal_agg, on='SK_ID_CURR', how='left')\n    bureau_agg = bureau_agg.merge(bureau_days750_agg, on='SK_ID_CURR', how='left') \n    #bureau_bal_agg['BUREAU_BAL_IS_DPD_RATIO'] = bureau_bal_agg['BUREAU_BAL_BUREAU_BAL_IS_DPD_SUM']\/bureau_bal_agg['BUREAU_BAL_SK_ID_CURR_COUNT']\n    #bureau_bal_agg['BUREAU_BAL_IS_DPD_OVER120_RATIO'] = bureau_bal_agg['BUREAU_BAL_BUREAU_BAL_IS_DPD_OVER120_SUM']\/bureau_bal_agg['BUREAU_BAL_SK_ID_CURR_COUNT']\n\n    #print('bureau_agg shape:', bureau_agg.shape)\n    \n    return bureau_agg","d26326ec":"def get_pos_bal_agg(pos_bal):\n    # (SK_DPD) 0 , 0~ 100 , 100\n    cond_over_0 = pos_bal['SK_DPD'] > 0\n    cond_100 = (pos_bal['SK_DPD'] < 100) & (pos_bal['SK_DPD'] > 0)\n    cond_over_100 = (pos_bal['SK_DPD'] >= 100)\n\n    # 0~ 120 120\n    pos_bal['POS_IS_DPD'] = pos_bal['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n    pos_bal['POS_IS_DPD_UNDER_120'] = pos_bal['SK_DPD'].apply(lambda x:1 if (x > 0) & (x <120) else 0 )\n    pos_bal['POS_IS_DPD_OVER_120'] = pos_bal['SK_DPD'].apply(lambda x:1 if x >= 120 else 0)\n\n    # SK_ID_CURR aggregation\n    pos_bal_grp = pos_bal.groupby('SK_ID_CURR')\n    pos_bal_agg_dict = {\n        'SK_ID_CURR':['count'], \n        'MONTHS_BALANCE':['min', 'mean', 'max'], \n        'SK_DPD':['min', 'max', 'mean', 'sum'],\n        'CNT_INSTALMENT':['min', 'max', 'mean', 'sum'],\n        'CNT_INSTALMENT_FUTURE':['min', 'max', 'mean', 'sum'],\n        \n        'POS_IS_DPD':['mean', 'sum'],\n        'POS_IS_DPD_UNDER_120':['mean', 'sum'],\n        'POS_IS_DPD_OVER_120':['mean', 'sum']\n    }\n\n    pos_bal_agg = pos_bal_grp.agg(pos_bal_agg_dict)\n\n    pos_bal_agg.columns = [('POS_')+('_').join(column).upper() for column in pos_bal_agg.columns.ravel()]\n    \n    # MONTHS_BALANCE (20)\n    cond_months = pos_bal['MONTHS_BALANCE'] > -20\n    pos_bal_m20_grp = pos_bal[cond_months].groupby('SK_ID_CURR')\n    pos_bal_m20_agg_dict = {\n        'SK_ID_CURR':['count'], \n        'MONTHS_BALANCE':['min', 'mean', 'max'], \n        'SK_DPD':['min', 'max', 'mean', 'sum'],\n        'CNT_INSTALMENT':['min', 'max', 'mean', 'sum'],\n        'CNT_INSTALMENT_FUTURE':['min', 'max', 'mean', 'sum'],\n \n        'POS_IS_DPD':['mean', 'sum'],\n        'POS_IS_DPD_UNDER_120':['mean', 'sum'],\n        'POS_IS_DPD_OVER_120':['mean', 'sum']\n    }\n\n    pos_bal_m20_agg = pos_bal_m20_grp.agg(pos_bal_m20_agg_dict)\n\n    pos_bal_m20_agg.columns = [('POS_M20')+('_').join(column).upper() for column in pos_bal_m20_agg.columns.ravel()]\n    pos_bal_agg = pos_bal_agg.merge(pos_bal_m20_agg, on='SK_ID_CURR', how='left')\n    \n    # SK_ID_CURR reset_index()\n    pos_bal_agg = pos_bal_agg.reset_index()\n    \n    \n    return pos_bal_agg","c474a531":"def get_install_agg(install):\n    # DPD  \n    install['AMT_DIFF'] = install['AMT_INSTALMENT'] - install['AMT_PAYMENT']\n    install['AMT_RATIO'] =  (install['AMT_PAYMENT'] +1)\/ (install['AMT_INSTALMENT'] + 1)\n    install['SK_DPD'] = install['DAYS_ENTRY_PAYMENT'] - install['DAYS_INSTALMENT']\n\n    # 30~ 120 100\n    install['INS_IS_DPD'] = install['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n    install['INS_IS_DPD_UNDER_120'] = install['SK_DPD'].apply(lambda x:1 if (x > 0) & (x <120) else 0 )\n    install['INS_IS_DPD_OVER_120'] = install['SK_DPD'].apply(lambda x:1 if x >= 120 else 0)\n\n    # SK_ID_CURR aggregation\n    install_grp = install.groupby('SK_ID_CURR')\n\n    install_agg_dict = {\n        'SK_ID_CURR':['count'],\n        'NUM_INSTALMENT_VERSION':['nunique'], \n        'DAYS_ENTRY_PAYMENT':['mean', 'max', 'sum'],\n        'DAYS_INSTALMENT':['mean', 'max', 'sum'],\n        'AMT_INSTALMENT':['mean', 'max', 'sum'],\n        'AMT_PAYMENT':['mean', 'max','sum'],\n\n        'AMT_DIFF':['mean','min', 'max','sum'],\n        'AMT_RATIO':['mean', 'max'],\n        'SK_DPD':['mean', 'min', 'max'],\n        'INS_IS_DPD':['mean', 'sum'],\n        'INS_IS_DPD_UNDER_120':['mean', 'sum'],\n        'INS_IS_DPD_OVER_120':['mean', 'sum']    \n    }\n\n    install_agg = install_grp.agg(install_agg_dict)\n    install_agg.columns = ['INS_'+('_').join(column).upper() for column in install_agg.columns.ravel()]\n\n    \n    # (DAYS_ENTRY_PAYMENT) (1)\n    cond_day = install['DAYS_ENTRY_PAYMENT'] >= -365\n    install_d365_grp = install[cond_day].groupby('SK_ID_CURR')\n    install_d365_agg_dict = {\n        'SK_ID_CURR':['count'],\n        'NUM_INSTALMENT_VERSION':['nunique'], \n        'DAYS_ENTRY_PAYMENT':['mean', 'max', 'sum'],\n        'DAYS_INSTALMENT':['mean', 'max', 'sum'],\n        'AMT_INSTALMENT':['mean', 'max', 'sum'],\n        'AMT_PAYMENT':['mean', 'max','sum'],\n\n        'AMT_DIFF':['mean','min', 'max','sum'],\n        'AMT_RATIO':['mean', 'max'],\n        'SK_DPD':['mean', 'min', 'max'],\n        'INS_IS_DPD':['mean', 'sum'],\n        'INS_IS_DPD_UNDER_120':['mean', 'sum'],\n        'INS_IS_DPD_OVER_120':['mean', 'sum']    \n    }\n    \n    install_d365_agg = install_d365_grp.agg(install_d365_agg_dict)\n    install_d365_agg.columns = ['INS_D365'+('_').join(column).upper() for column in install_d365_agg.columns.ravel()]\n    \n    install_agg = install_agg.merge(install_d365_agg, on='SK_ID_CURR', how='left')\n    install_agg = install_agg.reset_index()\n    \n    return install_agg","b0030ee3":"def get_card_bal_agg(card_bal):\n    card_bal['BALANCE_LIMIT_RATIO'] = card_bal['AMT_BALANCE']\/card_bal['AMT_CREDIT_LIMIT_ACTUAL']\n    card_bal['DRAWING_LIMIT_RATIO'] = card_bal['AMT_DRAWINGS_CURRENT'] \/ card_bal['AMT_CREDIT_LIMIT_ACTUAL']\n\n    # DPD\n    card_bal['CARD_IS_DPD'] = card_bal['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n    card_bal['CARD_IS_DPD_UNDER_120'] = card_bal['SK_DPD'].apply(lambda x:1 if (x > 0) & (x <120) else 0 )\n    card_bal['CARD_IS_DPD_OVER_120'] = card_bal['SK_DPD'].apply(lambda x:1 if x >= 120 else 0)\n\n    # SK_ID_CURR aggregation\n    card_bal_grp = card_bal.groupby('SK_ID_CURR')\n    card_bal_agg_dict = {\n        'SK_ID_CURR':['count'],\n         #'MONTHS_BALANCE':['min', 'max', 'mean'],\n        'AMT_BALANCE':['max'],\n        'AMT_CREDIT_LIMIT_ACTUAL':['max'],\n        'AMT_DRAWINGS_ATM_CURRENT': ['max', 'sum'],\n        'AMT_DRAWINGS_CURRENT': ['max', 'sum'],\n        'AMT_DRAWINGS_POS_CURRENT': ['max', 'sum'],\n        'AMT_INST_MIN_REGULARITY': ['max', 'mean'],\n        'AMT_PAYMENT_TOTAL_CURRENT': ['max','sum'],\n        'AMT_TOTAL_RECEIVABLE': ['max', 'mean'],\n        'CNT_DRAWINGS_ATM_CURRENT': ['max','sum'],\n        'CNT_DRAWINGS_CURRENT': ['max', 'mean', 'sum'],\n        'CNT_DRAWINGS_POS_CURRENT': ['mean'],\n        'SK_DPD': ['mean', 'max', 'sum'],\n\n        'BALANCE_LIMIT_RATIO':['min','max'],\n        'DRAWING_LIMIT_RATIO':['min', 'max'],\n        'CARD_IS_DPD':['mean', 'sum'],\n        'CARD_IS_DPD_UNDER_120':['mean', 'sum'],\n        'CARD_IS_DPD_OVER_120':['mean', 'sum']    \n    }\n    card_bal_agg = card_bal_grp.agg(card_bal_agg_dict)\n    card_bal_agg.columns = ['CARD_'+('_').join(column).upper() for column in card_bal_agg.columns.ravel()]\n\n    card_bal_agg = card_bal_agg.reset_index()\n    \n    # MONTHS_BALANCE (3)\n    cond_month = card_bal.MONTHS_BALANCE >= -3\n    card_bal_m3_grp = card_bal[cond_month].groupby('SK_ID_CURR')\n    card_bal_m3_agg = card_bal_m3_grp.agg(card_bal_agg_dict)\n    card_bal_m3_agg.columns = ['CARD_M3'+('_').join(column).upper() for column in card_bal_m3_agg.columns.ravel()]\n    \n    card_bal_agg = card_bal_agg.merge(card_bal_m3_agg, on='SK_ID_CURR', how='left')\n    card_bal_agg = card_bal_agg.reset_index()\n    \n    return card_bal_agg","a3f65592":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import MinMaxScaler\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\nif BaselineTraining:\n    \n    if 'TARGET' in app_train:\n        train = app_train.drop(columns = ['TARGET'])\n    else:\n        train = app_train.copy()\n\n    test = app_test.copy()\n    features = list(train.columns)\n\n    imputer.fit(train)\n    # Median imputation of missing values\n    train = imputer.transform(train)\n    test = imputer.transform(app_test)\n\n    scaler.fit(train)\n    train = scaler.transform(train)\n    test = scaler.transform(test)\n\n    print('Training data shape:', train.shape)\n    print('Testing data shape:', test.shape)\n    \n    if LogisticRegression:\n\n        # Make the model with the specified regularization parameter\n        log_reg = LogisticRegression(C = 0.0001)\n\n        # Train on the training data\n        log_reg.fit(train, train_labels)\n\n        # Make predictions\n        # Make sure to select the second column only\n        log_reg_pred = log_reg.predict_proba(test)[:, 1]\n\n        # Submission dataframe\n        submit = app_test[['SK_ID_CURR']]\n        submit['TARGET'] = log_reg_pred\n\n        # submit.head()\n\n        # Save the submission to a csv file\n        submit.to_csv('log_reg_baseline.csv', index = False)\n    \n    if RandomForest:\n\n        # Make the random forest classifier\n        random_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n\n        # Train on the training data\n        random_forest.fit(train, train_labels)\n\n        # Extract feature importances\n        feature_importance_values = random_forest.feature_importances_\n        feature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\n        # Make predictions on the test data\n        predictions = random_forest.predict_proba(test)[:, 1]\n\n        # Make a submission dataframe\n        submit = app_test[['SK_ID_CURR']]\n        submit['TARGET'] = predictions\n\n        # submit.head()\n\n        # Save the submission dataframe\n        submit.to_csv('random_forest_baseline.csv', index = False)","c72c6ac5":"if PolynomialFeaturesTesting:\n    poly_features_names = list(app_train_poly.columns)\n\n    poly_features = imputer.fit_transform(app_train_poly)\n    poly_features_test = imputer.transform(app_test_poly)\n\n    poly_features = scaler.fit_transform(poly_features)\n    poly_features_test = scaler.transform(poly_features_test)\n\n    random_forest_poly = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n\n    # Train on the training data\n    random_forest_poly.fit(poly_features, train_labels)\n\n    # Make predictions on the test data\n    predictions = random_forest_poly.predict_proba(poly_features_test)[:, 1]\n\n    # Make a submission dataframe\n    submit = app_test[['SK_ID_CURR']]\n    submit['TARGET'] = predictions\n\n    submit.head()\n\n    # Save the submission dataframe\n    submit.to_csv('random_forest_baseline_engineered.csv', index = False)\n\nif DomainFeaturesTesting:\n    app_train_domain = app_train_domain.drop(columns = 'TARGET')\n\n    domain_features_names = list(app_train_domain.columns)\n\n    domain_features = imputer.fit_transform(app_train_domain)\n    domain_features_test = imputer.transform(app_test_domain)\n\n    domain_features = scaler.fit_transform(domain_features)\n    domain_features_test = scaler.transform(domain_features_test)\n\n    random_forest_domain = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n\n    # Train on the training data\n    random_forest_domain.fit(domain_features, train_labels)\n\n    # Extract feature importances\n    feature_importance_values_domain = random_forest_domain.feature_importances_\n    feature_importances_domain = pd.DataFrame({'feature': domain_features_names, 'importance': feature_importance_values_domain})\n\n    # Make predictions on the test data\n    predictions = random_forest_domain.predict_proba(domain_features_test)[:, 1]\n\n    # Make a submission dataframe\n    submit = app_test[['SK_ID_CURR']]\n    submit['TARGET'] = predictions\n\n    # Save the submission dataframe\n    submit.to_csv('random_forest_baseline_domain.csv', index = False)","6cfeb2a7":"def plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","c8ed07e4":"feature_importances_sorted = plot_feature_importances(feature_importances)","748e8f51":"feature_importances_domain_sorted = plot_feature_importances(feature_importances_domain)","3b4834be":"def get_dataset():\n    \"\"\"\n    load datasets\n    1.app_train - train dataset \n    2.app_test - test datasets\n    3.apps - concatenated app_train, app_test\n    4.pos_bal - POS_CACHE\n    5.install - installments_payments\n    6.card_bal - Credit Card\n    \"\"\"\n    app_train = pd.read_csv(os.path.join(default_dir,'application_train.csv'))\n    app_test = pd.read_csv(os.path.join(default_dir,'application_test.csv'))\n    apps = pd.concat([app_train, app_test])\n\n    prev = pd.read_csv(os.path.join(default_dir,'previous_application.csv'))\n    bureau = pd.read_csv(os.path.join(default_dir,'bureau.csv'))\n    bureau_bal = pd.read_csv(os.path.join(default_dir,'bureau_balance.csv'))\n\n    pos_bal, install, card_bal = get_balance_data()\n\n    return apps, prev, bureau, bureau_bal, pos_bal, install, card_bal\n\napps, prev, bureau, bureau_bal, pos_bal, install, card_bal = get_dataset()","bd00236b":"# apps prev_agg, bureau_agg, pos_bal_agg, install_agg, card_bal_agg\ndef get_apps_all_with_all_agg(apps, prev, bureau, bureau_bal, pos_bal, install, card_bal):\n    \"\"\"\n    Description :\n    1.Data preparation , aggregation \n    2.produce the finalized result\n    \"\"\"\n    apps_all =  get_apps_processed(apps)\n    prev_agg = get_prev_agg(prev)\n    bureau_agg = get_bureau_agg(bureau, bureau_bal)\n    pos_bal_agg = get_pos_bal_agg(pos_bal)\n    install_agg = get_install_agg(install)\n    card_bal_agg = get_card_bal_agg(card_bal)\n    print('prev_agg shape:', prev_agg.shape, 'bureau_agg shape:', bureau_agg.shape )\n    print('pos_bal_agg shape:', pos_bal_agg.shape, 'install_agg shape:', install_agg.shape, 'card_bal_agg shape:', card_bal_agg.shape)\n    print('apps_all before merge shape:', apps_all.shape)\n\n    # Join with apps_all\n    apps_all = apps_all.merge(prev_agg, on='SK_ID_CURR', how='left')\n    apps_all = apps_all.merge(bureau_agg, on='SK_ID_CURR', how='left')\n    apps_all = apps_all.merge(pos_bal_agg, on='SK_ID_CURR', how='left')\n    apps_all = apps_all.merge(install_agg, on='SK_ID_CURR', how='left')\n    apps_all = apps_all.merge(card_bal_agg, on='SK_ID_CURR', how='left')\n\n    print('apps_all after merge with all shape:', apps_all.shape)\n\n    return apps_all","f531cafc":"def get_apps_all_with_prev_agg(apps, prev):\n    apps_all =  get_apps_processed(apps)\n    prev_agg = get_prev_agg(prev)\n    print('prev_agg shape:', prev_agg.shape)\n    print('apps_all before merge shape:', apps_all.shape)\n    apps_all = apps_all.merge(prev_agg, on='SK_ID_CURR', how='left')\n    print('apps_all after merge with prev_agg shape:', apps_all.shape)\n    \n    return apps_all\n\ndef get_apps_all_encoded(apps_all):\n    object_columns = apps_all.dtypes[apps_all.dtypes == 'object'].index.tolist()\n    for column in object_columns:\n        apps_all[column] = pd.factorize(apps_all[column])[0]\n    \n    return apps_all","e3500778":"from sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMClassifier\n\ndef get_apps_all_train_test(apps_all):\n    apps_all_train = apps_all[~apps_all['TARGET'].isnull()]\n    apps_all_test = apps_all[apps_all['TARGET'].isnull()]\n\n    apps_all_test = apps_all_test.drop('TARGET', axis=1)\n    \n    return apps_all_train, apps_all_test\n\ndef train_apps_all(apps_all_train):\n    ftr_app = apps_all_train.drop(['SK_ID_CURR', 'TARGET'], axis=1)\n    target_app = apps_all_train['TARGET']\n\n    train_x, valid_x, train_y, valid_y = train_test_split(ftr_app, target_app, test_size=0.2, random_state=2020)\n    print('train shape:', train_x.shape, 'valid shape:', valid_x.shape)\n    clf = LGBMClassifier(\n                nthread=4,\n                n_estimators=2000,\n                learning_rate=0.02,\n                max_depth = 11,\n                num_leaves=58,\n                colsample_bytree=0.613,\n                subsample=0.708,\n                max_bin=407,\n                reg_alpha=3.564,\n                reg_lambda=4.930,\n                min_child_weight= 6,\n                min_child_samples=165,\n                silent=-1,\n                verbose=-1,\n                )\n   \n    clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= 'auc', verbose= 100, \n                early_stopping_rounds= 200)\n    \n    return clf","f4d65c37":"# application, previous, bureau, bureau_bal\napps_all = get_apps_all_with_all_agg(apps, prev, bureau, bureau_bal, pos_bal, install, card_bal)\n\n\n# Category Label\napps_all = get_apps_all_encoded(apps_all)\n\napps_all_train, apps_all_test = get_apps_all_train_test(apps_all)\n\nclf = train_apps_all(apps_all_train)","1621cf64":"output_dir = \"..\/output\/kaggle\/working\/\"\npreds = clf.predict_proba(apps_all_test.drop(['SK_ID_CURR'], axis=1))[:, 1]\napps_all_test['TARGET'] = preds\n#apps_all_test[['SK_ID_CURR', 'TARGET']].to_csv(os.path.join(output_dir,'pos_install_credit_02.csv'), index=False)\napps_all_test[['SK_ID_CURR', 'TARGET']]","aab8593c":"apps_all_test[['SK_ID_CURR', 'TARGET']].to_csv('submission.csv', index=False)","8ef52c50":"from lightgbm import plot_importance\n\nplot_importance(clf, figsize=(16, 32), max_num_features=100)","6951671d":"# https:\/\/stackoverflow.com\/questions\/42579908\/use-corr-to-get-the-correlation-between-two-columns\nDomain_Knowledge_Features_corrs = apps_all_train[['APPS_ANNUITY_CREDIT_RATIO', 'APPS_GOODS_CREDIT_RATIO', 'APPS_ANNUITY_INCOME_RATIO', 'APPS_INCOME_EMPLOYED_RATIO', 'TARGET']].corr()\n\nplt.figure(figsize = (12, 9))\n\n# Heatmap of correlations\nsns.heatmap(Domain_Knowledge_Features_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Domain Knowledge Features Correlation Heatmap')\n\n# plt.savefig('.\/Domain Knowledge Features Correlation Heatmap.png', bbox_inches = 'tight')\n# plt.imshow(plt.imread('.\/Domain Knowledge Features Correlation Heatmap.png'))","6159f2f7":"# Training (Engineered Features Implemented)","cc803b84":"# Prediction","7a9f2ccf":"# Reload The Original Datasets For Final Training","aa74fbef":"When it comes time to build our machine learning models, we will have to fill in these missing values (known as imputation). In later work, we will use models such as XGBoost that can [handle missing values with no need for imputation.](https:\/\/stats.stackexchange.com\/questions\/235489\/xgboost-can-handle-missing-data-in-the-forecasting-phase) Another option would be to drop columns with a high percentage of missing values.","30450dcd":"As the client gets older, there is a negative linear relationship with the target meaning that as clients get older, they tend to repay their loans on time more often.","125ce670":"#### Visualize The Effect of The Age on The Target","64cab5fd":"#### Distribution of Age","5b8515cf":"## Anomalies\n\nMis-typed numbers, errors in measuring equipment, or extreme measurements.","3fc9cde0":"# Feature Engineering (Example)\n\nKaggle competitions are won by feature engineering: those win are those who can create the most useful features out of the data. This represents one of the patterns in machine learning: **Feature engineering has a greater return on investment than model building and hyperparameter tuning.** As Andrew Ng is fond of saying: **Applied machine learning is basically feature engineering.**","3e6c4173":"[How to save a matplotlib figure and fix text cutting off || Matplotlib Tips](https:\/\/www.youtube.com\/watch?v=C8MT-A7Mvk4)","4a6f6834":"# Load In Data","2eaa3900":"### Aligning Training and Testing Data\n\nThere need to be the same features (columns) in both the training and testing data. One-hot encoding has created more columns in the training data because there were some categorical variables with categories not represented in the testing data.\n\n* Only column labels that are present in both df1 and df2 are retained [(join='inner').](https:\/\/stackoverflow.com\/questions\/51645195\/pandas-align-function-illustrative-example\/51645550)","c6d8d2c7":"All three `EXT_SOURCE` featureshave negative correlations with the target, indicating that as the value of the `EXT_SOURCE` increases, the client is more likely to repay the loan. We can also see that `DAYS_BIRTH` is positively correlated with `EXT_SOURCE_1` indicating that maybe one of the factors in this score is the client age.","65c721e0":"There is a clear trend: younger applicants are more likely to not repay the loan! The rate of failure to repay is above 10% for the youngest three age groups and beolow 5% for the oldest age group.","df2dbb3c":"# EDA","a8edf68f":"#### One-Hot Encoding","fd619c45":"Well that is extremely interesting! It turns out that the anomalies have a lower rate of default. We will fill in the anomalous values with not a number (np.nan) and then create a new boolean column indicating whether or not the value was anomalous.","a68bb123":"### Effect of Age on Repayment","4760ea9c":"Just out of curiousity, let's subset the anomalous clients and see if they tend to have higher or low rates of default than the rest of the clients.","4075140e":"## Data Imbalanced","7c02960e":"## Column Types","0af92696":"We will add these features to a copy of the training and testing data and then evaluate models with and without the features. The only way to know if an approach will work is to try it out!","05e799b3":"## Unique Entries\nof the `object`(categorical) columns.","7d6e153a":"#### Label Encoding","0a52b1ea":"This creates a considerable number of new features. To get the names we have to use the polynomial features `get_feature_names` method.","ea89dcad":"## Missing Values","e4588ee0":"## Polynomial Features\n\nDealing with [ImportError](https:\/\/stackoverflow.com\/questions\/59439096\/importerror-cannnot-import-name-imputer-from-sklearn-preprocessing): `cannot import name 'Imputer' from 'sklearn.preprocessing'`: <br>\nIt has been deprecated with **scikit-learn v0.20.4** and removed as of **v0.22.2**.","237b6a85":"Most of the categorical variables have a relatively small number of unique entries. We will need to find a way to deal with these categorical variables!","0670fd97":"# Feature Importances (Final)","2d0a355f":"## Datasets Concatenation and Join","0efda17b":"## Training and Testing","21dbe26d":"### Encoding Categorical Variables\n\nFor categorical variables with many classes, one-hot encoding is the safest approach because it does not impose arbitrary values to categories. The only downside to one-hot encoding is that the number of features (dimensions of the data) can explode with categorical variables with many categories. \n\nLet's implement the policy described above: for any categorical variable (`dtype == object`) with 2 unique categories, we will use label encoding, and for any categorical variable with more than 2 unique categories, we will use one-hot encoding.","87ccfc96":"# Feature Engineering (Final)","e9711449":"## Reduce Memory Allotted Size\n\nPOS_CASH_balance, installments_payments, credit_card_balance","db488182":"The target == 1 curve skews towards the younger end of the range. Although this is not a significant correlation (-0.07 correlation coefficient), this variable is likely going to be useful in a machine learning model because it does affect the target. Let's look at this relationship in another way: average failure to repay loans by age bracket.\n\nTo make this graph, first we cut the age category into bins of 5 years each. Then, for each bin, we calculate the average value of the target, which tells us the ratio of loans that were not repaid in each age category.","4a6233d0":"# Final Training (LGBM)","ebcd9a26":"# Training (Baseline)","d6087d13":"## Correlations","a1c09989":"### Exterior Sources\n\nThe 3 variables with the strongest negative correlations with the target are `EXT_SOURCE_1`, `EXT_SOURCE_2`, and `EXT_SOURCE_3`.","70db8385":"# Manual Setting","7ad2dc03":"# References\n\n* [Start Here: A Gentle Introduction](https:\/\/www.kaggle.com\/willkoehrsen\/start-here-a-gentle-introduction#Conclusions)\n* [Home Credit Default Risk Prediction](https:\/\/www.kaggle.com\/sangseoseo\/home-credit-default-risk-prediction#data-cleansing,-EDA,-model-creation)\n* [More domain knowledge from former Home Credit analyst](https:\/\/www.kaggle.com\/c\/home-credit-default-risk\/discussion\/63032)","c7e7922f":"# Feature Importances (Example)","8fa49b45":"# Introduction\nThe objective of this competition is to use historical loan application data to predict whether or not an applicant will be able to repay a loan. This is a standard supervised classification task.\n\n* **Supervised**: The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features\n* **Classification**: The label is a binary variable, 0 (will repay loan on time), 1 (will have difficulty repaying loan)\n\nSome practices to do for you to get familier with the **standard supervised classification task**:\n- [Titanic Top 11%| Starter I: Models Comparing](https:\/\/www.kaggle.com\/chienhsianghung\/titanic-top-11-starter-i-models-comparing)\n- [Titanic Top 11%| Starter II: Hyperparameter Tuning](https:\/\/www.kaggle.com\/chienhsianghung\/titanic-top-11-starter-ii-hyperparameter-tuning)\n- [TPS Apr.| Starter Pack: All Models](https:\/\/www.kaggle.com\/chienhsianghung\/tps-apr-starter-pack-all-models)\n","9499d830":"## Domain Knowledge Features\n\n* `CREDIT_INCOME_PERCENT`: the percentage of the credit amount relative to a client's income\n* `ANNUITY_INCOME_PERCENT`: the percentage of the loan annuity relative to a client's income\n* `CREDIT_TERM`: the length of the payment in months (since the annuity is the monthly amount due\n* `DAYS_EMPLOYED_PERCENT`: the percentage of the days employed relative to the client's age","5199d32a":"There are 35 features with individual features raised to powers up to degree 3 and interaction terms. Now, we can see whether any of these new features are correlated with the target."}}