{"cell_type":{"b7314d1d":"code","50dfaf00":"code","796a1fc6":"code","e8e4aa20":"code","afc5a151":"code","dd6d8d70":"code","ba0547d5":"code","b53776ce":"code","d4f4c9cd":"code","f125bad2":"code","c95c4861":"markdown","b219f628":"markdown","f7aac6c7":"markdown","d96e5547":"markdown","835da54d":"markdown","f19bdd9d":"markdown","1d048575":"markdown","b395d52e":"markdown","1223f9f3":"markdown","48da7e9a":"markdown","45a72243":"markdown"},"source":{"b7314d1d":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","50dfaf00":"dataset = pd.read_csv('..\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv')\nX = dataset.iloc[:, [3, 4]].values","796a1fc6":"import pandas_profiling as pp\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","e8e4aa20":"pp.ProfileReport(dataset, title = 'Pandas Profiling report of \"dataset\"', html = {'style':{'full_width': True}})","afc5a151":"from sklearn.cluster import KMeans\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1, 11), wcss, c = 'blue')\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","dd6d8d70":"import scipy.cluster.hierarchy as sch\ndendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\nplt.title('Dendrogram')\nplt.xlabel('Customers')\nplt.ylabel('Euclidean distances')\nplt.show()","ba0547d5":"kmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 42)\ny_kmeans = kmeans.fit_predict(X)","b53776ce":"from sklearn.cluster import AgglomerativeClustering\nhc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')\ny_hc = hc.fit_predict(X)","d4f4c9cd":"plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 50, c = 'red', label = 'Cluster 1')\nplt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 50, c = 'blue', label = 'Cluster 2')\nplt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 50, c = 'green', label = 'Cluster 3')\nplt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 50, c = 'cyan', label = 'Cluster 4')\nplt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = 50, c = 'magenta', label = 'Cluster 5')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, marker = '+', c = 'black', label = 'Centroids')\nplt.title('Clusters of customers')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend()\nplt.show()","f125bad2":"plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 50, c = 'red', label = 'Cluster 1')\nplt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 50, c = 'blue', label = 'Cluster 2')\nplt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 50, c = 'green', label = 'Cluster 3')\nplt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 50, c = 'cyan', label = 'Cluster 4')\nplt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], s = 50, c = 'magenta', label = 'Cluster 5')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, marker = '+', c = 'black', label = 'Centroids')\nplt.title('Clusters of customers')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend()\nplt.show()","c95c4861":"## <font color='red'>Using the dendrogram to find the optimal number of clusters<\/font>","b219f628":"## Dataset information (Pandas Profiling) ","f7aac6c7":"## <font color='blue'>Elbow method to find the optimal number of clusters<\/font>","d96e5547":"## <font color='blue'>Visualising the K-Means clusters<\/font>","835da54d":"\n## Importing the dataset","f19bdd9d":"# <font color='green'>conclusion<\/font>\n* **For large number of data K-Means Clustering is always good.**\n* **Reason: It's easy to find the optimal no of clusters in K-Means because of elbow method. (we can't find the optimal no of clusters accuratley by using dendogram for large number of data. hence, for large number of data Hierarchical clustering is not possible)**\n\n","1d048575":"\n## Importing the libraries","b395d52e":"## <font color='red'>Training the Hierarchical Clustering model on the dataset<\/font>","1223f9f3":"## <font color='red'>Visualising the Hierarchical clusters<\/font>","48da7e9a":"## <font color='blue'>Training the K-Means model on the dataset<\/font>","45a72243":"# Mall Customer Segmentation: <font color='blue'>K-Means Clustering<\/font> & <font color='red'>Hierarchical Clustering<\/font> \n* Importing the libraries\n* Importing the dataset\n* Dataset information (Pandas Profiling)\n* Elbow method to find the optimal number of clusters\n* Using the dendrogram to find the optimal number of clusters\n* Training the K-Means model on the dataset\n* Training the Hierarchical Clustering model on the dataset\n* Visualising the K-means clusters\n* Visualising the Hierarchical clusters\n* **Conclusion**"}}