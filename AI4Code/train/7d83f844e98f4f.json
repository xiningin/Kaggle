{"cell_type":{"9a6472d2":"code","def09a52":"code","98d59a9e":"code","4bd4e6e5":"code","e64b198e":"code","62e01552":"code","4b53a177":"code","31e5d19a":"code","107c9842":"code","efb6f3a2":"code","8395dadf":"code","e82d6f70":"code","1daff29b":"markdown","b3ffcdee":"markdown","5bafc03a":"markdown","5d854f28":"markdown","fb6fe2fc":"markdown","b4458c9c":"markdown","f5e51ea3":"markdown","8b0ae6e3":"markdown"},"source":{"9a6472d2":"from tensorflow.keras.datasets import imdb\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom tensorflow import keras\n\nimport matplotlib.pyplot as plt","def09a52":"DICTIONARY_WORD_SIZE = 500 \nTEST_SIZE = 0.2\nSEED = 2021\nONE_REVIEW_WORD_MAXLEN = 200","98d59a9e":"(train_input, train_target), (test_input, test_target) = imdb.load_data(num_words=DICTIONARY_WORD_SIZE)","4bd4e6e5":"train_input, val_input, train_target, val_target = train_test_split(\n    train_input, train_target, test_size=TEST_SIZE, random_state=SEED)","e64b198e":"train_seq = pad_sequences(train_input, maxlen=ONE_REVIEW_WORD_MAXLEN)\nval_seq = pad_sequences(val_input, maxlen=ONE_REVIEW_WORD_MAXLEN)","62e01552":"EMBEDDING_INPUT_DIM = DICTIONARY_WORD_SIZE\nEMBEDDING_OUTPUT_DIM = 16\nEMBEDDING_INPUT_LENGTH = ONE_REVIEW_WORD_MAXLEN\n\nGRU_UNITS= 8\nGRU_DROP_OUT = 0.3\n\nMODEL_TARGET_NUM = 1\nMODEL_ACTIVATION = 'sigmoid'","4b53a177":"model = keras.Sequential()\n\nmodel.add(keras.layers.Embedding(EMBEDDING_INPUT_DIM, EMBEDDING_OUTPUT_DIM,\n                                 input_length=EMBEDDING_INPUT_LENGTH))\nmodel.add(keras.layers.GRU(GRU_UNITS,dropout=GRU_DROP_OUT, return_sequences=True))\nmodel.add(keras.layers.GRU(GRU_UNITS, dropout=GRU_DROP_OUT))\nmodel.add(keras.layers.Dense(MODEL_TARGET_NUM, activation=MODEL_ACTIVATION))\n\nmodel.summary()","31e5d19a":"MODEL_LR = 1e-4\nMODEL_LOSS = 'binary_crossentropy'\nMODEL_METRICS = ['accuracy']\n\nBEST_MODEL_FILE_NAME = 'best-gru-model.h5'\nES_PATIENCE = 2\n\nEPOCHS = 100\nBATCH_SIZE = 64","107c9842":"rmsprop = keras.optimizers.RMSprop(learning_rate=MODEL_LR)\nmodel.compile(optimizer=rmsprop, loss=MODEL_LOSS, \n              metrics=MODEL_METRICS)\n\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(BEST_MODEL_FILE_NAME, save_best_only=True)\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=ES_PATIENCE,\n                                                  restore_best_weights=True)\n\nhistory = model.fit(train_seq, train_target, epochs=EPOCHS, batch_size=BATCH_SIZE,\n                    validation_data=(val_seq, val_target),\n                    callbacks=[checkpoint_cb, early_stopping_cb])","efb6f3a2":"TRAIN_LOSS = 'loss'\nTEST_LOSS = 'val_loss'\nX_LABEL = 'epoch'\nY_LABEL = 'loss'\nLEGEND = ['train', 'val']","8395dadf":"plt.plot(history.history[TRAIN_LOSS])\nplt.plot(history.history[TEST_LOSS])\nplt.xlabel(X_LABEL)\nplt.ylabel(Y_LABEL)\nplt.legend(LEGEND)\nplt.show()","e82d6f70":"test_seq = pad_sequences(test_input, maxlen=ONE_REVIEW_WORD_MAXLEN)\n\nrnn_model = keras.models.load_model(BEST_MODEL_FILE_NAME)\n\nrnn_model.evaluate(test_seq, test_target)","1daff29b":"# evaluate model","b3ffcdee":"# define model","5bafc03a":"# plot train history","5d854f28":"# preprocess data (sequence padding)","fb6fe2fc":"# split data","b4458c9c":"# GRU :Gated Recurrent Unit","f5e51ea3":"# build model","8b0ae6e3":"# load data "}}