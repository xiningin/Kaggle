{"cell_type":{"f02bfd41":"code","bc552779":"code","e0ccd628":"code","5a65b58b":"code","9bc40dde":"code","f990a40f":"code","571c7935":"code","0a1bdd53":"code","098a87bf":"code","a0aa1c6e":"code","32153c53":"code","b1b3ccc3":"code","ef3895eb":"code","7e3dc3b2":"code","caa47e50":"code","e5051ef1":"code","934d6427":"code","09482994":"code","ef3975ea":"code","0c48071a":"code","ac774912":"code","4e5e99e0":"code","6ff792fb":"code","6092d867":"code","2ab932f7":"code","ab9ddabc":"code","54b07dc0":"code","df9cbbb9":"code","153406ff":"code","ee573f76":"code","a6b7b460":"code","6606e050":"code","58540a34":"markdown","197aa095":"markdown"},"source":{"f02bfd41":"from fastai.tabular import *","bc552779":"PATH = Path('..\/input')","e0ccd628":"!ls {PATH}","5a65b58b":"df = pd.read_csv(PATH\/'train.csv')\ntest_df = pd.read_csv(PATH\/'test.csv')","9bc40dde":"features = [feature for feature in df.columns if 'var_' in feature]\nlen(features)","f990a40f":"def augment_df(df):\n    for feature in features:\n        df[f'sq_{feature}'] = df[feature]**2\n        df[f'repo_{feature}'] = df[feature].apply(lambda x: 0 if x==0 else 1\/x)\n    df['min'] = df[features].min(axis=1)\n    df['mean'] = df[features].mean(axis=1)\n    df['max'] = df[features].max(axis=1)\n    df['median'] = df[features].median(axis=1)\n    df['std'] = df[features].std(axis=1)\n    df['var'] = df[features].var(axis=1)\n    df['abs_mean'] = df[features].abs().mean(axis=1)\n    df['abs_median'] = df[features].abs().median(axis=1)\n    df['abs_std'] = df[features].abs().std(axis=1)\n    df['skew'] = df[features].skew(axis=1)\n    df['kurt'] = df[features].kurt(axis=1)\n    df['sq_kurt'] = df[[f'sq_{feature}' for feature in features]].kurt(axis=1)","571c7935":"augment_df(df)\ndf.head()","0a1bdd53":"augment_df(test_df)\ntest_df.head()","098a87bf":"features = features + [f'sq_{feature}' for feature in features] + [f'repo_{feature}' for feature in features]\nnum_features = len(features)","a0aa1c6e":"random.seed(2)\nvalid_idx = random.sample(list(df.index.values), int(len(df)*0.05))\ntrain_idx = df.drop(valid_idx).index","32153c53":"summary = df.iloc[train_idx].describe()","b1b3ccc3":"class roc(Callback):\n    def on_epoch_begin(self, **kwargs):\n        self.total = 0\n        self.batch_count = 0\n    \n    def on_batch_end(self, last_output, last_target, **kwargs):\n        preds = F.softmax(last_output, dim=1)\n        try: \n            roc_score = roc_auc_score(to_np(last_target), to_np(preds[:, -1]))\n            self.total+=roc_score\n            self.batch_count+=1\n        except:\n            pass\n    def on_epoch_end(self, num_batch, **kwargs):\n        self.metric = self.total\/self.batch_count","ef3895eb":"def train_and_eval_tabular_learner(train_df, train_features, valid_idx, add_noise=False, lr=0.02, epochs=1, layers=[200, 100], ps=[0.5, 0.2], name='learner'):\n    data = TabularDataBunch.from_df(path='.', df=train_df, dep_var='target', valid_idx=valid_idx,\n                                    cat_names=[], cont_names=train_features, bs=bs, procs=[FillMissing, Normalize], test_df=test_df)\n    learn = tabular_learner(data, layers=layers, ps=ps, metrics=[roc()])\n    if add_noise:\n        learn.data = None\n        data = None\n        noise = np.random.normal(summary[features].loc['mean'].values, summary[features].loc['std'].values, (len(train_df), num_features))\/100\n        train_df[features]+=noise\n        data = TabularDataBunch.from_df(path='.', df=train_df, dep_var='target', valid_idx=valid_idx,\n                                    cat_names=[], cont_names=train_features, bs=bs, procs=[FillMissing, Normalize], test_df=test_df)\n        learn.data = data\n        learn.fit_one_cycle(epochs, lr)\n        train_df[features]-=noise\n        noise = None\n    learn.fit_one_cycle(epochs, lr)\n    learn.save(name, with_opt=False)\n    valid_preds, _ = learn.get_preds(ds_type=DatasetType.Valid)\n    valid_probs = np.array(valid_preds[:, -1])\n    valid_targets = train_df.loc[valid_idx].target.values\n    valid_score = roc_auc_score(valid_targets, valid_probs)\n    test_preds, _ = learn.get_preds(ds_type=DatasetType.Test)\n    test_probs = to_np(test_preds[:, -1])\n    del(data)\n    del(learn)\n    gc.collect()\n    return valid_score, valid_probs, test_probs","7e3dc3b2":"sub_features = []\nvalid_scores = []\nvalid_preds = []\npreds = []\nnum_epochs = 10\ncv_counts = len(df)\/\/num_epochs\nsaved_model_prefix = 'learner'","caa47e50":"augmented_features = ['min', 'mean', 'max', 'median', 'std', 'abs_mean', 'abs_median', 'abs_std', 'skew', 'kurt', 'sq_kurt']","e5051ef1":"bs = 2048","934d6427":"import gc","09482994":"gc.collect()","ef3975ea":"from sklearn.metrics import roc_auc_score","0c48071a":"for i in range(num_epochs):\n    print('Training model: ', i)\n    sub_features.append(random.sample(list(features), int(num_features*0.75)) + augmented_features)\n    name = f'{saved_model_prefix}_{i}'\n    score, valid_probs, test_probs = train_and_eval_tabular_learner(df, sub_features[-1], valid_idx,\n                                                                    add_noise=True, epochs=3, lr=0.02, name=name)\n    valid_scores.append(score)\n    valid_preds.append(valid_probs)\n    preds.append(test_probs)","ac774912":"import lightgbm as lgb\nfrom sklearn.model_selection import KFold, StratifiedKFold","4e5e99e0":"target = df['target']","6ff792fb":"import time","6092d867":"param = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.4,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.01,\n    'max_depth': -1,  \n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': 1\n}","2ab932f7":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019)\noof = np.zeros(len(df))\npredictions = np.zeros(len(test_df))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(skf.split(df.values, target.values)):\n    print(\"fold n\u00b0{}\".format(fold_))\n    trn_data = lgb.Dataset(df.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(df.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 100)\n    oof[val_idx] = clf.predict(df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test_df[features], num_iteration=clf.best_iteration) \/ 5","ab9ddabc":"print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))","54b07dc0":"predictions","df9cbbb9":"preds1 = np.array(sum(preds)\/num_epochs)\npreds1","153406ff":"predictions.shape, preds1.shape","ee573f76":"all_ensemble_values = [0., 0.125, 0.25, 0.375, 0.5, 0.675, 0.75, 0.875, 1.0]","a6b7b460":"sample_ensemble_array = np.array(all_ensemble_values[0]*preds1 + (1-all_ensemble_values[0])*predictions)\nsample_ensemble_array","6606e050":"for i in range(len(all_ensemble_values)):\n    predict_array = np.array(all_ensemble_values[i]*preds1 + (1-all_ensemble_values[i])*predictions)\n    sub_df = pd.DataFrame({'ID_code': test_df['ID_code'].values})\n    sub_df['target'] = predict_array\n    sub_df.to_csv(f'submission_{i}.csv', index=False)","58540a34":"**Note** In a previous version, people were running out of memory in the kernel because for 10 epochs, each time we were creating a new data object and learn object which is quite memory intensive. So, I deleted them at last in the method and saved memory. You can use this technique at places which take quite a lot amount of memory. ","197aa095":"# References:\n\nhttps:\/\/www.kaggle.com\/chocozzz\/santander-lightgbm-baseline-lb-0-899\n\nhttps:\/\/www.kaggle.com\/quanghm\/fastai-1-0-tabular-learner-with-ensemble"}}