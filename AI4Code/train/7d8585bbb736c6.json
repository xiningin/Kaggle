{"cell_type":{"d3238820":"code","b47b9743":"code","c4f22ae3":"code","6d521619":"code","4bcde5bc":"code","30c372c7":"code","671a748d":"code","a5ec3623":"code","054e7a9b":"code","25e075e4":"code","0b57b57e":"code","b7e64d1c":"code","cc2e9314":"code","c327bf28":"code","be1c963d":"code","be38ae35":"code","13749ef2":"code","7f7b97ad":"code","cc2cbfda":"code","e9c368b7":"code","eae49578":"code","df71ac8d":"code","c39e6bfb":"code","185e1ed9":"code","af07e61d":"code","54e1af12":"code","d408a7de":"markdown","c1a15376":"markdown","e335f397":"markdown","c54ce551":"markdown","da79ebe5":"markdown","da8d292b":"markdown"},"source":{"d3238820":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\npd.options.display.max_columns = 100","b47b9743":"#Load data\ntrain = pd.read_csv(\"..\/input\/learn-together\/train.csv\")\ntest = pd.read_csv(\"..\/input\/learn-together\/test.csv\")\n\ntrain.describe()\n","c4f22ae3":"print(train.columns[train.isna().any()].tolist())\nprint(test.columns[test.isna().any()].tolist())\n","6d521619":"train.shape","4bcde5bc":"#Check column datatypes\ntrain.dtypes","30c372c7":"train.head()","671a748d":"train.tail()","a5ec3623":"train.hist(figsize=(40,40),xrot=45)\n","054e7a9b":"#Create single soil field (reverse the one hot encoding)\nsoil_fields = ['Soil_Type'+ str(i) for i in range(1,31)]\n\ntrain_soil = train[soil_fields]\ntrain['Soil_Type'] = train_soil.idxmax(1)\ntrain.head()","25e075e4":"#Create single wilderness area field (reverse the one hot encoding)\nWilderness_Area_Fields = ['Wilderness_Area'+ str(i) for i in range(1,5)]\n\ntrain_wilderness = train[Wilderness_Area_Fields]\ntrain['Wilderness_Area'] = train_wilderness.idxmax(1)\ntrain.head()","0b57b57e":"#plot categoricals\nfor feature in train.dtypes[train.dtypes == 'object'].index:\n    plt.figure(figsize=(16, 12))\n    sns.countplot(y=feature, data=train)\n    sns.set()\n    plt.show()","b7e64d1c":"sns.catplot(hue=\"Wilderness_Area\", y=\"Cover_Type\", kind=\"count\", data=train)\nplt.show()","cc2e9314":"#Build contingency tables for Wilderness Area to see numerical relationship with Cover type\nwild_cover_cont = pd.crosstab(index=train['Wilderness_Area'],columns=train['Cover_Type'])\nwild_cover_cont\n","c327bf28":"#Calculate chi-square for wilderness area and cover type\nimport scipy\nwild_cover_c, wild_cover_p, wild_cover_dof, wild_cover_expected = scipy.stats.chi2_contingency(wild_cover_cont)\nprint('c: ' + str(wild_cover_c))\nprint('p: ' + str(wild_cover_p))\nprint('dof: ' + str(wild_cover_dof))\nprint('expected: ' + str(wild_cover_expected))","be1c963d":"#Build contingency tables for Soil Type to see numerical relationship with Cover type\nsoil_cover_cont = pd.crosstab(index=train['Soil_Type'],columns=train['Cover_Type'])\nsoil_cover_cont","be38ae35":"#Calculate chi-square for soil type and cover type\nimport scipy\nsoil_cover_c, soil_cover_p, soil_cover_dof, soil_cover_expected = scipy.stats.chi2_contingency(soil_cover_cont)\nprint('c: ' + str(soil_cover_c))\nprint('p: ' + str(soil_cover_p))\nprint('dof: ' + str(soil_cover_dof))\nprint('expected: ' + str(soil_cover_expected))","13749ef2":"for i in range(1,10):\n    sns.catplot(x=\"Cover_Type\", y=train.columns[i], data=train)\n    plt.title(train.columns[i])\n    plt.show()","7f7b97ad":"#Need to one hot encode covertype\n\none_hot_cov_type = pd.get_dummies(train.Cover_Type, prefix='cov_type')\none_hot_cov_type.head()\n\n\n","cc2cbfda":"for i in range(1,10):\n    for j in range(1,8):\n        print(train.columns[i]+' cov_type_'+str(j) +' '+ str(scipy.stats.pointbiserialr(one_hot_cov_type['cov_type_'+str(j)], train[train.columns[i]]).correlation))","e9c368b7":"train.drop('Id', axis=1,inplace=True)\ntrain.head()","eae49578":"# Import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\n#from learntools.core import *\n\n# Create target object and call it y\ny = train.Cover_Type\n# Create X\nX = train.drop(['Cover_Type','Soil_Type','Wilderness_Area'], axis=1)\n\n# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1, stratify=train.Cover_Type, test_size=0.2)\nprint(len(train_X), len(val_X), len(train_y),len(val_y))\n","df71ac8d":"#Build Pipelines\n#pipeline = {'l1': make_pipeline(LogisticRegression(penalty='l1' , random_state=123)),\n#            'l2': make_pipeline(LogisticRegression(penalty='l2' , random_state=123)),\n#            'rf': make_pipeline(RandomForestClassifier(random_state=123)),\n#            'gb': make_pipeline(GradientBoostingClassifier(random_state=123))\n#}\n\npipeline = {'rf': make_pipeline(StandardScaler(), RandomForestClassifier(random_state=123))}\npipeline['rf'].get_params()","c39e6bfb":"#Define hyperparameters to test\n\n#Tested locally (crashes kaggle to run)\n#rf_hyperparameters = {\n#    'randomforestclassifier__n_estimators': [100, 500, 1000],\n#    'randomforestclassifier__max_features': ['auto', 'sqrt', 0.33],\n#    'randomforestclassifier__min_samples_split': [2, 5, 10, 15, 100],\n#    'randomforestclassifier__max_depth': [5, 8, 15, 25, 30],\n#    'randomforestclassifier__min_samples_leaf': [1, 2, 5, 10]\n#}\n\n#Identified best configuration\nrf_hyperparameters = {\n    'randomforestclassifier__n_estimators': [1000],\n    'randomforestclassifier__max_features': [0.33],\n    'randomforestclassifier__min_samples_split': [2],\n    'randomforestclassifier__max_depth': [30],\n    'randomforestclassifier__min_samples_leaf': [1]\n}\n\n\nhyperparameters = {'rf': rf_hyperparameters}","185e1ed9":"# Create empty dictionary called fitted_models\nfitted_models = {}\n\n# Loop through model pipelines, tuning each one and saving it to fitted_models\nfor name, pipeline in pipeline.items():\n\n\n    # Create cross-validation object from pipeline and hyperparameters\n    model = GridSearchCV(pipeline, hyperparameters[name], cv=10, n_jobs=1)\n    \n    # Fit model on X_train, y_train\n    model.fit(train_X, train_y)\n    \n    # Store model in fitted_models[name] \n    fitted_models[name] = model\n    \n    # Print '{name} has been fitted'\n    print(name, \" has been fitted\")","af07e61d":"# Display best_score and parameters for each fitted model\nfor name, model in fitted_models.items():\n    print(name, model.best_score_)\n    print(name, model.best_params_)","54e1af12":"# Retrain on full set of data\nrf_model = RandomForestClassifier(random_state=0, n_estimators=1000, max_depth=30,max_features=.33,min_samples_leaf=1,min_samples_split=2)\nrf_model.fit(train_X, train_y)\nrf_val_predictions = rf_model.predict(val_X)\nrf_val_f1 = f1_score(val_y, rf_val_predictions, average='weighted')\n\n#print(\"Validation f1 score for Random Forest Model: {:,.0f}\".format(rf_val_f1))\nprint(rf_val_f1)","d408a7de":"No NaN values in datasets.","c1a15376":"Looks to be dependent, p-value of 0.0","e335f397":"Looks to be dependent, p-value of 0.0","c54ce551":"Classify with random forest","da79ebe5":"Elevation looks to be very predictive, which makes sense. Let's formalize these relationships using biserial correlations.","da8d292b":"Explore relationships of numerical features to cover type"}}