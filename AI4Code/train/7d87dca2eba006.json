{"cell_type":{"802e6c70":"code","cc3ab97c":"code","fe9625af":"code","648e9b0d":"code","595b4800":"code","60f7591c":"code","d51b9de9":"code","91bee08c":"code","80d2a9ac":"code","0510df21":"code","d69267d7":"code","da0d954c":"code","6029eed6":"code","a551a8ee":"code","950a6693":"code","55e55a67":"code","21eb6d84":"code","117f38d1":"code","d4a83f9c":"code","21cc035d":"code","1b0900c9":"code","13965b4b":"code","7e9043fa":"code","0017e588":"code","e398f259":"code","388fe600":"code","91c15fcf":"code","27f76265":"code","58e4fdbe":"code","23537253":"code","479d7a8d":"code","8f6b9640":"code","c6107b06":"code","ae7fb2b5":"code","1dd5f7bf":"code","d5045073":"code","3c0e93ca":"code","3242bdd6":"code","de5c9079":"code","56623cc0":"code","948d7a7c":"code","c85e7a0d":"code","2724ea8e":"code","9a00c27a":"code","ea66d71a":"code","a155436d":"markdown","0dddf864":"markdown","1147ecdf":"markdown","bca26a0c":"markdown","5e9e3390":"markdown","f0d76ebf":"markdown","b2f84cc2":"markdown","a5223018":"markdown","17312099":"markdown","8fbee64a":"markdown","0ef7e72b":"markdown","bd4081c1":"markdown","35380c52":"markdown","74b58203":"markdown","c3283682":"markdown","dd5c227a":"markdown","7b21dad0":"markdown","6cb448d2":"markdown","ea817f0d":"markdown","a0120b4d":"markdown","40a9fd02":"markdown","6cae689a":"markdown","0ba3c376":"markdown","b96b317b":"markdown","642c24ac":"markdown","903805c4":"markdown","e88f7bc0":"markdown","b5277034":"markdown","2c87ce8e":"markdown","d11a3131":"markdown","56284605":"markdown","02cfe7d0":"markdown","b83ed728":"markdown","acd5ea39":"markdown","bc98b5f3":"markdown","7f9fecbf":"markdown","17b2bab3":"markdown","d50606a6":"markdown","e7d6ca6a":"markdown","c59c7a22":"markdown","59731954":"markdown","288538d5":"markdown","56651853":"markdown"},"source":{"802e6c70":"import os \nimport numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport plotly\nimport plotly.express as px\nimport matplotlib.pyplot as plt \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import TimeSeriesSplit ,train_test_split\nfrom sklearn.ensemble import RandomForestRegressor \nfrom sklearn.tree import plot_tree\nfrom datetime import datetime\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\n    \"figure\",\n    autolayout=True,\n    figsize=(11, 4),\n    titlesize=18,\n    titleweight='bold',\n)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"Packages Imported \")\n","cc3ab97c":"## import data \ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/train.csv\",index_col=\"row_id\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/test.csv\",index_col=\"row_id\")\nsample = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv\")","fe9625af":"train.head(4)","648e9b0d":"train.info()\ntrain.shape","595b4800":"train.isnull().sum()","60f7591c":"start_date =train.date.min()\nend_date = train.date.max()\nstart_date , end_date","d51b9de9":"plt.figure(figsize=(7, 4))\nsns.countplot(x='country', data=train, order=train[\"country\"].value_counts().index[:3],palette = \"flare_r\")\nplt.title(\"Total counts of country \", size=20)\nplt.show()","91bee08c":"plt.figure(figsize=(7, 4))\nsns.countplot(x='store', data=train, order=train[\"store\"].value_counts().index[:2],palette = \"flare\")\nplt.title(\"Total counts of stores \", size=20)\nplt.show()","80d2a9ac":"plt.figure(figsize=(7, 4))\nsns.countplot(x='product', data=train, order=train[\"product\"].value_counts().index[:3],palette = \"flag\")\nplt.title(\"Total count kaggle products \", size=20)\nplt.show()","0510df21":"train['date'] = pd.to_datetime(train['date'], format='%Y\/%m\/%d')\nplt.figure(figsize=(15,7))\nsld_time = train.groupby(['date']).sum().reset_index()\nsns.lineplot(x=sld_time.date, y=sld_time.num_sold,)\nplt.title('number sold over time ', fontsize=14)\nplt.show()\n","d69267d7":"sld_cont = train.groupby(['country']).sum().reset_index()\nplt.figure(figsize=(10,5))\nax = sns.barplot(x=\"country\", y='num_sold', data=sld_cont, palette=\"Blues\")\nax.bar_label(ax.containers[0])\nax.set_title('Number sold by country',\n             fontsize = 14)","da0d954c":"sld_store = train.groupby(['store']).sum().reset_index()\nplt.figure(figsize=(10,5))\nax = sns.barplot(x=\"store\", y='num_sold', data=sld_store, palette=\"Blues\")\nax.bar_label(ax.containers[0])\nax.set_title('Number sold in each store',\n             fontsize = 14)","6029eed6":"sld_item= train.groupby(['product']).sum().reset_index()\nplt.figure(figsize=(14,6))\nax = sns.barplot(x=\"num_sold\", y='product', data=sld_item,color=\"darkblue\")\nax.bar_label(ax.containers[0])\nax.set_title('Number sold of each product', fontsize = 18) ","a551a8ee":"##add time features \ntrain['year']=train['date'].dt.year \ntrain['month']=train['date'].dt.month_name()\ntrain['day']=train['date'].dt.day_name()\ntrain.head(3)","950a6693":"### plotdata preparation\nsld_year = train.groupby(['year']).mean().reset_index()\nsld_month = train.groupby(['month']).mean().reset_index().set_index('month')\nsld_day = train.groupby(['day']).mean().reset_index().set_index('day')\nnew_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\nsld_month= sld_month.reindex(new_order, axis=0)\nsorter = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday','Sunday']\nsorterIndex = dict(zip(sorter,range(len(sorter))))\nsld_day['Day_id'] = sld_day.index\nsld_day['Day_id'] = sld_day['Day_id'].map(sorterIndex)\nsld_day.sort_values('Day_id', inplace=True)","55e55a67":"fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(12, 16))\nsns.lineplot(x=sld_year.year, y=sld_year.num_sold ,ax=ax[0], color='dodgerblue')\nax[0].set_title('year trend for number sold', fontsize=14)\nax[0].set_ylabel(ylabel='mean of number sold', fontsize=14)\n\nsns.lineplot(x=\"month\", y=sld_month.num_sold,data= sld_month , ax=ax[1], color='dodgerblue')\nax[1].set_title('month trend for number sold', fontsize=14)\nax[1].set_ylabel(ylabel='mean of number sold', fontsize=14)\n\nsns.lineplot(x=\"day\", y=sld_day.num_sold, data = sld_day , ax=ax[2], color='dodgerblue')\nax[2].set_title('day trend for number sold', fontsize=14)\nax[2].set_ylabel(ylabel='mean of number sold', fontsize=14)\n\nplt.show()","21eb6d84":"plt1= train.groupby(['country','year'])['num_sold'].sum().reset_index()\nfig = px.line(plt1, x=\"year\", y=\"num_sold\", title= \" total number sold by year in each country\",color=\"country\", markers=True)\nfig.show()","117f38d1":"plt2= train.groupby(['country','month'])['num_sold'].sum().reset_index()\n#plt2.reset_index(inplace=True)   \nmonths = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \n          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\nplt2['time'] = pd.Categorical(plt2['month'], categories=months, ordered=True)\nplt2 = plt2.sort_values(['time','num_sold'], ascending= [True,False])\nfig = px.line(plt2, x=\"time\", y=\"num_sold\", title= \"Number sold products by month in each country\",color=\"country\", markers=True)\nfig.show()\n","d4a83f9c":"plt4= train.groupby(['country','day'])['num_sold'].sum().reset_index().set_index('day')\nplt4['Day_id'] = plt4.index\nplt4['Day_id'] = plt4['Day_id'].map(sorterIndex)\nplt4.sort_values('Day_id', inplace=True)\nfig = px.line(plt4, x=plt4.index, y=\"num_sold\", title= \"Total daily sold products in each country\",color=\"country\", markers=True)\nfig.show()","21cc035d":"plt3= train.groupby(['month','product'])['num_sold'].sum().reset_index()\nplt3['time'] = pd.Categorical(plt3['month'], categories=months, ordered=True)\nplt3 = plt3.sort_values(['time','num_sold'], ascending= [True,False])\nfig = px.line(plt3, x=\"month\", y=\"num_sold\",title= \"Total monthly solds for each products\",color=\"product\", markers=True)\nfig.show()","1b0900c9":"plt5= train.groupby(['date','country','product'])['num_sold'].sum().reset_index()\nplt5 = plt5.sort_values(['date','num_sold'], ascending= [True,False])\nsel_nor = plt5.loc[plt5['country'] == 'Norway']\nfig = px.line(sel_nor, x=\"date\", y=\"num_sold\",title= \"Evolution of number of sales for each product in Norway\",color=\"product\")\nsel_swed = plt5.loc[plt5['country'] == 'Sweden']\nfig.show()\nfig = px.line(sel_swed, x=\"date\", y=\"num_sold\",title= \"Evolution of number of sales for each product in Sweden\",color=\"product\")\nfig.show()\nsel_finla = plt5.loc[plt5['country'] == 'Sweden']\nfig = px.line(sel_finla, x=\"date\", y=\"num_sold\",title= \"Evolution of number of sales for each product in Finland\",color=\"product\")\nfig.show()","13965b4b":"plt6= train.groupby(['date','country','store'])['num_sold'].sum().reset_index()\nplt6 = plt6.sort_values(['date','num_sold'], ascending= [True,False])\nsel_nor = plt6.loc[plt5['country'] == 'Norway']\nfig = px.line(sel_nor, x=\"date\", y=\"num_sold\",title= \"Evolution of number of sales by each store in Norway\",color=\"store\")\nsel_swed = plt6.loc[plt5['country'] == 'Sweden']\nfig.show()\nfig = px.line(sel_swed, x=\"date\", y=\"num_sold\",title= \"Evolution of number of sales by each store in Sweden\",color=\"store\")\nfig.show()\nsel_finla = plt6.loc[plt5['country'] == 'Sweden']\nfig = px.line(sel_finla, x=\"date\", y=\"num_sold\",title= \"Evolution of number of sales by each store in Finland\",color=\"store\")\nfig.show()","7e9043fa":"test.head(4)","0017e588":"test.info()\ntest.shape","e398f259":"test.isnull().sum()","388fe600":"start_dt_ts =test.date.min()\nend_dt_ts = test.date.max()\nstart_dt_ts , end_dt_ts","91c15fcf":"plt.figure(figsize=(7, 4))\nsns.countplot(x='country', data=test, order=test[\"country\"].value_counts().index[:3],palette = \"Blues_d\")\nplt.title(\"Total counts of country for test data  \", size=20)\nplt.show()","27f76265":"plt.figure(figsize=(7, 4))\nsns.countplot(x='product', data=test, order=test[\"product\"].value_counts().index[:3],palette = \"Blues\")\nplt.title(\"Total counts of products \", size=20)\nplt.show()","58e4fdbe":"plt.figure(figsize=(6, 4))\nsns.countplot(x='store', data=test, order=test[\"store\"].value_counts().index[:2],palette =\"Blues_d\")\nplt.title(\"Total counts of stores with in the test data  \", size=20)\nplt.show()","23537253":"##add time features \ntest['date'] = pd.to_datetime(test['date'], format='%Y\/%m\/%d')\ntest['year']=test['date'].dt.year \ntest['month']=test['date'].dt.month_name()\ntest['day']=test['date'].dt.day_name()","479d7a8d":"train_df= train.copy()\ntest_df = test.copy()","8f6b9640":"#ddd extra features \ntrain_df[\"is_weekend\"]= train_df.day.apply(lambda x : 1 if x in ['Saturday','Sunday'] else 0)\ntrain_df['dayofweek'] = train_df['date'].dt.dayofweek\n#train_df['dayofyear'] = train_df['date'].dt.dayofyear\n#train_df['dayofmonth'] = train_df['date'].dt.days_in_month\ntrain_df['Time_step'] = np.arange(len(train_df.index)) # add a time step feature\n#train['weekday'] = train['date'].dt.weekday\n##test\ntest_df[\"is_weekend\"]= test_df.day.apply(lambda x : 1 if x in ['Saturday','Sunday'] else 0)\ntest_df['dayofweek'] = test_df['date'].dt.dayofweek\ntest_df['Time_step'] = np.arange(len(test_df.index))\n#test_df['dayofyear'] = test_df['date'].dt.dayofyear\n#test_df['dayofmonth'] = test_df['date'].dt.days_in_month\n\n#test['weekday'] = test['date'].dt.weekday","c6107b06":"#train_df.set_index(\"date\")\n#test_df.set_index(\"date\")","ae7fb2b5":"##label encoder for categorical features\nle = LabelEncoder()\ncols = [ \"country\",\"product\",\"store\",\"month\",\"day\" ,\"year\"]\nfor col in cols :\n    train_df[col] = le.fit_transform(train_df[col])\n    test_df[col] = le.fit_transform(test_df[col])","1dd5f7bf":"#train_valid_split \nX_train = train_df[train_df.date.between('2015-01-01', '2018-5-31')].copy()\nX_val = train_df[train_df.date.between('2018-06-01', '2018-12-31')].copy()","d5045073":"# remove date col \nX_train = X_train.drop(\"date\",axis=1)\nX_val = X_val.drop(\"date\",axis=1)\ntest_df= test_df.drop(\"date\",axis=1)","3c0e93ca":"y_train=X_train[\"num_sold\"]\nX_train = X_train.drop(\"num_sold\",axis=1)\ny_val= X_val[\"num_sold\"]\nX_val= X_val.drop(\"num_sold\",axis=1)","3242bdd6":"baseline_regressor = RandomForestRegressor(n_estimators=500,n_jobs=-1) # some rondom paramaters \nbaseline_regressor.fit(X_train, y_train)\nval_pred= baseline_regressor.predict(X_val)","de5c9079":"# thanks to https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/298201\ndef SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)","56623cc0":"print('The SMAPE value of the Random Forests model is :\\t',SMAPE(val_pred,y_val))","948d7a7c":"importance = baseline_regressor.feature_importances_ # get feature importance \nfeature_names = list(X_train.columns)# get feature names \nstd = np.std([tree.feature_importances_ for tree in baseline_regressor.estimators_], axis=0)\nbaseline_importances = pd.Series(importance, index=feature_names) # convert to pandas serie \n# plot \nfig, ax = plt.subplots() \nbaseline_importances.plot.bar(xerr=std, ax=ax)\nax.set_title(\"Feature importances\")\nax.set_ylabel(\"Mean decrease in impurity\")\nfig.tight_layout()","c85e7a0d":"test_pred = baseline_regressor.predict(test_df)","2724ea8e":"sample[\"num_sold\"] = test_pred\nsample.to_csv(\"baseliner.csv\",index=False)\nsample.head()","9a00c27a":"test[\"num_sold\"]= test_pred\nframes = [train, test]\ndata = pd.concat(frames)\n##########\"\"\ndata_plot = data.groupby(['date']).mean().reset_index()\nplt.figure(figsize=(15,7))\nsns.lineplot(x=data_plot.date, y=data_plot.num_sold,)\nplt.title('number sold by time ', fontsize=14)\nplt.show()","ea66d71a":"sld_store = test.groupby(['store']).sum().reset_index()\nplt.figure(figsize=(8,5))\nax = sns.barplot(x=\"store\", y='num_sold', data=sld_store, palette=\"Blues\")\nax.bar_label(ax.containers[0])\nax.set_title('Expected Number sold by each store for the year 2019 ',\n             fontsize = 14)","a155436d":"<center> <h1> Tabular Playground Series - Jan 2022  <\/h1> <\/center> \n\n![Capture.JPG](attachment:17548a61-0c3a-49bb-aa5c-a9de9c215161.JPG)","0dddf864":"#### Evolution of number of sales for each product in diffrent countries","1147ecdf":"<h2 style='background:#11489c; border:0; color:white'><center>Notebook Set up<\/center><\/h2>","bca26a0c":"#### Evolution of number of sales for each product in diffrent countries","5e9e3390":"### Values counts of country , stores and kaggle products.","f0d76ebf":"Looking at the three plots above we can see that number sold start increasing after 2016. we have two peaks ( around april and november )  where end of the year ( november and december ) is the period where number of solds reach it's maximun and while august and sepetemer saw the least number solds. on a daily basis we see that solds increases significantly on saturday and saunday ( weekend days ).","b2f84cc2":"It's clear that all three countries follow the same trend ( yearly , monthly , and daily ).","a5223018":"**Genral info about the data** ","17312099":"#### Submission ","8fbee64a":"### Total number sold over time ","0ef7e72b":"we need to look deeper before making conclusions but it's quite clear that the number sold increases on April an December  for each year( holydays shoppings ).","bd4081c1":"#### Train and evaluation of base model ","35380c52":"<h2 style='background:#11489c; border:0; color:white'><center>About the data<\/center><\/h2>\n\nFor this challenge, we will be predicting a full year worth of sales for three items at two stores located in three different countries. This dataset is completely fictional, but contains many effects you see in real-world data, e.g., weekend and holiday effect, seasonality, etc. The dataset is small enough to allow us to try numerous different modeling approaches.\n\n**Files**\n- **train.csv** - the training set, which includes the sales data for each date-country-store-item combination.\n- **test.csv**- the test set; your task is to predict the corresponding item sales for each date-country-store-item combination. Note the Public leaderboard is scored on the first quarter of the test year, and the Private on the remaining.\n- **sample_submission.csv** - a sample submission file in the correct format","74b58203":"We want to know how number sold evolved over the year , month and day , for that we need to creat extra features using the date column.","c3283682":"### Test data \n","dd5c227a":"Before going any further we want to look at our data frames and see what can that provide us ( insights , ideas ). ","7b21dad0":"Same for products,the number solds follow almost the same trend.","6cb448d2":" <h2 style='background:#11489c; border:0; color:white'><center> time series frocasting<\/center><\/h2>\n \n After we did some exploration for our data , now it's time to model it in order to predict future sales. For that we are going to use , in first verions i was using lightautoMl which is a general perpous tabular autoML ( didn't spend time tuning on it). for this time i'am exprimnting with a simple model ( may that change ) , Note the perpous of this notebook is to provide a general workflow for TS forcasting. ","ea817f0d":"Looking at the figure above we can say that for the year 2019 ( as it was in previous years ) KaggleRama are the strong condidate to become the official outlet for all things Kaggle.","a0120b4d":"### Year, month and day trends ","40a9fd02":"the test data comes with 6570 rows , 4 columns ( right ) and no missing values. the test data covers the next 12 months afeter train data ( from 1-1-2019 to 31-12-2019 ) ","6cae689a":"<h3 style='background:#11489c; border:0; color:white'><center>I Hope you find this usful , Good Luck<\/center><\/h3>","0ba3c376":"#### Number sold for each country ","b96b317b":"let's check Now how the number of solds for each products evolved over monthes.","642c24ac":"#### prediction analysis ","903805c4":"We can see that kaggleRama had almost twice the number of solds comparing to kaggleMart.","e88f7bc0":"Looking at the figure above we can see that norway has the most number sold , followed by sweden then finland.","b5277034":"We can see that kaggle Hat (how cute is it) are most popular product followed by kaggle Mugs then kaggle Strickers.","2c87ce8e":"### Train Data ","d11a3131":"#### data loiding ","56284605":"#### Total sold of each Product across all countries.","02cfe7d0":"#### Genral info about test data ","b83ed728":"The train data has 26298 rows and 5 columns with no missing values ( we are happy with that ) , all the columns are object type except number sold (our target)  is an int64 type.","acd5ea39":"<h2 style='background:#11489c; border:0; color:white'><center> Exploratory Data Analysis <\/center><\/h2>","bc98b5f3":"The train data cover a period of 4 years starting from 01-01-2015 and ending on 31-12-2018.","7f9fecbf":"#### Note \nAs mentioned before this is just a baseline for proof of concept , there is still a large room for improvement wether on data ( feature engineering) , or on exprimenting with other models , libraries.Which is  encouraged since the data is quite small. ","17b2bab3":"<h2 style='background:#11489c; border:0; color:white'><center>Overview<\/center><\/h2>\nThere are two (fictitious) independent store chains selling Kaggle merchandise that want to become the official outlet for all things Kaggle.  we want to figure out which of the store chains(KaggleMart or KaggleRama) would have the best sales going forward. \n","d50606a6":"Looking at returned plots we can tel that all variables are equally present with in the train data.","e7d6ca6a":"Same as in the train data , country , product and store are equally present with in the test data. before going further lets as add the same features we added to the train data.","c59c7a22":"Looking at the prdicted values ( year of 2019 ) we can see that the baseline could capture the trends in the second quarter and at the end of year","59731954":"Let's now check the number of solds in each country.","288538d5":"#### Data preparation ","56651853":"#### Predictions "}}