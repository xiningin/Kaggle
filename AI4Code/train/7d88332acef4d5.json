{"cell_type":{"bc8a17c9":"code","9e1d875d":"code","b952b610":"code","0bef8d24":"code","9243cf6b":"code","18a9be6d":"code","6889624a":"code","fadeb537":"code","6ce4c380":"code","288b0fd7":"code","45afe135":"code","d5d19264":"code","16cec3f6":"code","d4fcca0d":"code","333f1e90":"code","876e072b":"code","2da08070":"code","4c4c147a":"code","5f53c5d4":"code","9a510583":"code","73dc5d6f":"code","d5b37de4":"code","1866a8d8":"code","0a317083":"code","96c833c0":"code","732534a6":"code","682afda5":"code","ad5da880":"code","ee43b74e":"code","7d55828f":"code","4dc8f602":"markdown","27791940":"markdown","49e9d1c7":"markdown","4532bfda":"markdown"},"source":{"bc8a17c9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n!pip install pubchempy\nimport pubchempy as pcp","9e1d875d":"df=pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv', usecols=['title','abstract','authors','doi','publish_time'])\nprint (df.shape)\n#drop duplicates\n#df=df.drop_duplicates()\ndf = df.drop_duplicates(subset='abstract', keep=\"first\")\n#drop NANs \ndf=df.dropna()\n# convert abstracts to lowercase\ndf[\"abstract\"] = df[\"abstract\"].str.lower()\n#show 5 lines of the new dataframe\nprint (df.shape) #first 10 and first 5\ndf.head()","b952b610":"import functools\nfrom IPython.core.display import display, HTML\nfrom nltk import PorterStemmer","0bef8d24":"#tell the system how many sentences are needed\nmax_sentences=5","9243cf6b":"#\ndef stem_words(words):\n    stemmer = PorterStemmer()\n    singles=[]\n    for w in words:\n        singles.append(stemmer.stem(w))\n    return singles","18a9be6d":"# list of lists for topic words realting to tasks\ndisplay(HTML('<h1>COVID-19 summary page vaccines and therapeutics<\/h1>'))\ndisplay(HTML('<h3>Table of Contents (ctrl f and search the hash tag and words below to find table<\/h3>'))\n\n\ntasks = [['nsp12'],['RdRp']]\n\nz=0\nfor terms in tasks:\n    stra=' '\n    stra=' '.join(terms)\n    k=str(z)\n    #display(HTML('<a href=\"#'+k+'\">'+stra+'<\/a>'))\n    display(HTML('# '+stra))\n    z=z+1\n\n# loop through the list of lists\nz=0\nfor search_words in tasks:\n    df_table = pd.DataFrame(columns = [\"pub_date\",\"authors\",\"title\",\"excerpt\"])\n    str1=''\n    # a make a string of the search words to print readable search\n    str1=' '.join(search_words)\n    search_words=stem_words(search_words) #function to remove the ends of the words\n    \n    # add cov to focus the search the papers and avoid unrelated documents\n    search_words.append(\"cov\")\n    \n    # search the dataframe for all the keywords\n    dfa=df[functools.reduce(lambda a, b: a&b, (df['abstract'].str.contains(s) for s in search_words))]\n    search_words.pop()\n    search_words.append(\"-cov-\")\n    dfb=df[functools.reduce(lambda a, b: a&b, (df['abstract'].str.contains(s) for s in search_words))]\n    # remove the cov word for sentence level analysis\n    search_words.pop()\n    #combine frames with COVID and cov and drop dups\n    frames = [dfa, dfb]\n    df1 = pd.concat(frames)\n    df1=df1.drop_duplicates()\n    #I think this is the data frame we want to find\n    \n    display(HTML('<h3>Task Topic: '+str1+'<\/h3>'))\n    \n    display(HTML('# '+str1+' <a><\/a>'))\n    z=z+1\n    # record how many sentences have been saved for display\n    # loop through the result of the dataframe search\n    for index, row in df1.iterrows():\n        pub_sentence=''\n        sentences_used=0\n        #break apart the absracrt to sentence level\n        sentences = row['abstract'].split('. ')\n        #loop through the sentences of the abstract\n        for sentence in sentences:\n            # missing lets the system know if all the words are in the sentence\n            missing=0\n            #loop through the words of sentence\n            for word in search_words:\n                #if keyword missing change missing variable\n                if word not in sentence:\n                    missing=missing+1\n            # after all sentences processed show the sentences not missing keywords limit to max_sentences\n            if missing<len(search_words) and sentences_used < max_sentences and len(sentence)<1000 and sentence!='':\n                sentence=sentence.capitalize()\n                if sentence[len(sentence)-1]!='.':\n                    sentence=sentence+'.'\n                pub_sentence=pub_sentence+'<br><br>'+sentence\n        if pub_sentence!='':\n            sentence=pub_sentence\n            sentences_used=sentences_used+1\n            authors=row[\"authors\"].split(\" \")\n            link=row['doi']\n            title=row[\"title\"]\n            linka='https:\/\/doi.org\/'+link\n            linkb=title\n            sentence='<p align=\"left\">'+sentence+'<\/p>'\n            final_link='<p align=\"left\"><a href=\"{}\">{}<\/a><\/p>'.format(linka,linkb)\n            to_append = [row['publish_time'],authors[0]+' et al.',final_link,sentence]\n            df_length = len(df_table)\n            df_table.loc[df_length] = to_append\n    filename=str1+'.csv'\n    #df_table.to_csv(filename,index = False)\n        #display(HTML('<b>'+sentence+'<\/b> - <i>'+title+'<\/i>, '+'<a href=\"https:\/\/doi.org\/'+link+'\" target=blank>'+authors[0]+' et al.<\/a>'))\n    #df_table=HTML(df_table.to_html(escape=False,index=False))\n   # display(df_table)\n\n#Need to now add a column to the DataFrame for the protein search term\n\n\nprint (\"done\")","6889624a":"#Converting the printed table from the loop into a Data Frame\ndf = pd.DataFrame(data=df_table)\nprint(df)","fadeb537":"#!pip install -U textblob\n#!python -m textblob.download_corpora","6ce4c380":"excerpts = df['excerpt']","288b0fd7":"#test dataframe of two abstracts\ndata_df = pd.DataFrame(\n{\"abstract\": [\"Middle East Respiratory Syndrome, caused by the MERS coronavirus (MERS-CoV), continues to cause severe respiratory disease with a high case fatality rate. To date, potential antiviral treatments for MERS-CoV have shown limited efficacy in animal studies. Here, we tested the efficacy of the broad-acting antiviral remdesivir in the rhesus macaque model of MERS-CoV infection. Remdesivir reduced the severity of disease, virus replication, and damage to the lungs when administered either before or after animals were infected with MERS-CoV. Our data show that remdesivir is a promising antiviral treatment against MERS that could be considered for implementation in clinical trials. It may also have utility for related coronaviruses such as the novel coronavirus 2019-nCoV emerging from Wuhan, China.\", \n              \"The emergence of the 2019 novel coronavirus (COVID-19), for which there is no vaccine or any known effective treatment created a sense of urgency for novel drug discovery approaches. One of the most important COVID-19 protein targets is the 3C-like protease for which the crystal structure is known. Most of the immediate efforts are focused on drug repurposing of known clinically-approved drugs and virtual screening for the molecules available from chemical libraries that may not work well. For example, the IC50 of lopinavir, an HIV protease inhibitor, against the 3C-like protease is approximately 50 micromolar, which is far from ideal. In an attempt to address this challenge, on January 28th, 2020 Insilico Medicine decided to utilize a part of its generative chemistry pipeline to design novel drug-like inhibitors of COVID-19 and started generation on January 30th. It utilized three of its previously validated generative chemistry approaches: crystal-derived pocked-based generator, homology modelling-based generation, and ligand-based generation. Novel druglike compounds generated using these approaches were published at www.insilico.com\/ncov-sprint\/. Several molecules will be synthesized and tested using the internal resources; however, the team is seeking collaborations to synthesize, test, and, if needed, optimize the published molecules\"]},index=[1,2])\ndata_df","45afe135":"#how to index a specific row in \ndata_df.abstract[2]","d5d19264":"import re #Regular-Expressions\nimport string #Types of punctuation\nimport nltk\n\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \n\n#StopWords Sourced from: https:\/\/gist.github.com\/sebleier\/554280\n#all_stopwords = [\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"ain\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can\",\"couldn\",\"couldn't\",\"d\",\"did\",\"didn\",\"didn't\",\"do\",\"does\",\"doesn\",\"doesn't\",\"doing\",\"don\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn\",\"hadn't\",\"has\",\"hasn\",\"hasn't\",\"have\",\"haven\",\"haven't\",\"having\",\"he\",\"her\",\"here\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"i\",\"if\",\"in\",\"into\",\"is\",\"isn\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"just\",\"ll\",\"m\",\"ma\",\"me\",\"mightn\",\"mightn't\",\"more\",\"most\",\"mustn\",\"mustn't\",\"my\",\"myself\",\"needn\",\"needn't\",\"no\",\"nor\",\"not\",\"now\",\"o\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"re\",\"s\",\"same\",\"shan\",\"shan't\",\"she\",\"she's\",\"should\",\"should've\",\"shouldn\",\"shouldn't\",\"so\",\"some\",\"such\",\"t\",\"than\",\"that\",\"that'll\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"ve\",\"very\",\"was\",\"wasn\",\"wasn't\",\"we\",\"were\",\"weren\",\"weren't\",\"what\",\"when\",\"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\"y\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"could\",\"he'd\",\"he'll\",\"he's\",\"here's\",\"how's\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"let's\",\"ought\",\"she'd\",\"she'll\",\"that's\",\"there's\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"what's\",\"when's\",\"where's\",\"who's\",\"why's\",\"would\",\"able\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\"act\",\"actually\",\"added\",\"adj\",\"affected\",\"affecting\",\"affects\",\"afterwards\",\"ah\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"among\",\"amongst\",\"announce\",\"another\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apparently\",\"approximately\",\"arent\",\"arise\",\"around\",\"aside\",\"ask\",\"asking\",\"auth\",\"available\",\"away\",\"awfully\",\"b\",\"back\",\"became\",\"become\",\"becomes\",\"becoming\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\"begins\",\"behind\",\"believe\",\"beside\",\"besides\",\"beyond\",\"biol\",\"brief\",\"briefly\",\"c\",\"ca\",\"came\",\"cannot\",\"can't\",\"cause\",\"causes\",\"certain\",\"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\"contains\",\"couldnt\",\"date\",\"different\",\"done\",\"downwards\",\"due\",\"e\",\"ed\",\"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\"enough\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"except\",\"f\",\"far\",\"ff\",\"fifth\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"former\",\"formerly\",\"forth\",\"found\",\"four\",\"furthermore\",\"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"gone\",\"got\",\"gotten\",\"h\",\"happens\",\"hardly\",\"hed\",\"hence\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hes\",\"hi\",\"hid\",\"hither\",\"home\",\"howbeit\",\"however\",\"hundred\",\"id\",\"ie\",\"im\",\"immediate\",\"immediately\",\"importance\",\"important\",\"inc\",\"indeed\",\"index\",\"information\",\"instead\",\"invention\",\"inward\",\"itd\",\"it'll\",\"j\",\"k\",\"keep\",\"keeps\",\"kept\",\"kg\",\"km\",\"know\",\"known\",\"knows\",\"l\",\"largely\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"'ll\",\"look\",\"looking\",\"looks\",\"ltd\",\"made\",\"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"merely\",\"mg\",\"might\",\"million\",\"miss\",\"ml\",\"moreover\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"n\",\"na\",\"name\",\"namely\",\"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"ninety\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"normally\",\"nos\",\"noted\",\"nothing\",\"nowhere\",\"obtain\",\"obtained\",\"obviously\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"omitted\",\"one\",\"ones\",\"onto\",\"ord\",\"others\",\"otherwise\",\"outside\",\"overall\",\"owing\",\"p\",\"page\",\"pages\",\"part\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"poorly\",\"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\"primarily\",\"probably\",\"promptly\",\"proud\",\"provides\",\"put\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"readily\",\"really\",\"recent\",\"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\"run\",\"said\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sent\",\"seven\",\"several\",\"shall\",\"shed\",\"shes\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\"significantly\",\"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\"specifying\",\"still\",\"stop\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\"tell\",\"tends\",\"th\",\"thank\",\"thanks\",\"thanx\",\"thats\",\"that've\",\"thence\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\"therein\",\"there'll\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"there've\",\"theyd\",\"theyre\",\"think\",\"thou\",\"though\",\"thoughh\",\"thousand\",\"throug\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"together\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"unto\",\"upon\",\"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"'ve\",\"via\",\"viz\",\"vol\",\"vols\",\"vs\",\"w\",\"want\",\"wants\",\"wasnt\",\"way\",\"wed\",\"welcome\",\"went\",\"werent\",\"whatever\",\"what'll\",\"whats\",\"whence\",\"whenever\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\"whereupon\",\"wherever\",\"whether\",\"whim\",\"whither\",\"whod\",\"whoever\",\"whole\",\"who'll\",\"whomever\",\"whos\",\"whose\",\"widely\",\"willing\",\"wish\",\"within\",\"without\",\"wont\",\"words\",\"world\",\"wouldnt\",\"www\",\"x\",\"yes\",\"yet\",\"youd\",\"youre\",\"z\",\"zero\",\"a's\",\"ain't\",\"allow\",\"allows\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"associated\",\"best\",\"better\",\"c'mon\",\"c's\",\"cant\",\"changes\",\"clearly\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"corresponding\",\"course\",\"currently\",\"definitely\",\"described\",\"despite\",\"entirely\",\"exactly\",\"example\",\"going\",\"greetings\",\"hello\",\"help\",\"hopefully\",\"ignored\",\"inasmuch\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"it'd\",\"keep\",\"keeps\",\"novel\",\"presumably\",\"reasonably\",\"second\",\"secondly\",\"sensible\",\"serious\",\"seriously\",\"sure\",\"t's\",\"third\",\"thorough\",\"thoroughly\",\"three\",\"well\",\"wonder\"]\nall_stopwords = nltk.corpus.words.words()\nporter=PorterStemmer()\n\ndef clean_text_round1(text):\n    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text) #if there's a character in [] get rid of it\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    #text = re.sub('\\w*\\d\\w*', '', text) #remove all the digits\n    \n    word_tokens = word_tokenize(text) \n    \n    filtered_sentence = [w for w in word_tokens if not w in all_stopwords] \n    for w in word_tokens: \n        if w not in all_stopwords: \n            filtered_sentence.append(w)\n    \n    stem_sentence = []\n    for word in filtered_sentence:\n            stem_sentence.append(porter.stem(word))\n            #stem_sentence.append(word)\n    print(stem_sentence)\n    \n    filtered2_sentence = [z for z in stem_sentence if not z in all_stopwords] \n    for z in stem_sentence: \n        if z not in all_stopwords: \n            filtered2_sentence.append(z)\n\n    print(filtered2_sentence)\n    \n    text = filtered2_sentence\n    \n    return text\n\nround1 = lambda x: clean_text_round1(x)","16cec3f6":"if 'is' in all_stopwords :\n    print(\"Yes, 'the word' found in List : \" , all_stopwords)","d4fcca0d":"data_clean = pd.DataFrame(data_df.abstract.apply(round1))\ndata_df['tokenized_abstract'] = data_clean #appending tokenized_abstract to our original data frame, data_df\ntype(data_df.tokenized_abstract[2]) #spit out a list","333f1e90":"data_df.tokenized_abstract[2][3]","876e072b":"test = data_df.tokenized_abstract[2]\ntest = pd.DataFrame(test)\nprint(test)\ntest.loc[:,0]","2da08070":"[str(i) for i in test.loc[:,0]]","4c4c147a":"def pubchem(token):\n    '''Search the tokens for pubchem compound ID's token is the tokenized abstract, defined as df.tokenized_abstract'''\n    for y in token:\n        results = pcp.get_compounds(y, 'name')\n        #if results == []:\n            #continue\n        print(results)\n    return results\n    \ngetID = lambda x: pubchem(x)","5f53c5d4":"pubchem(data_df.tokenized_abstract[2])","9a510583":"results = pcp.get_compounds([str(i) for i in test.loc[:,0]], 'name')","73dc5d6f":"results = pcp.get_compounds(data_df.tokenized_abstract[2], 'name')\nprint(results)","d5b37de4":"token = data_df.tokenized_abstract[2]\ntoken[1]","1866a8d8":"token = data_df.tokenized_abstract[2]\ndata_ID = pd.DataFrame(token.apply(getID))","0a317083":"abstract = data_clean.abstract[1]\n#data_clean.abstract = data_clean.abstract.astype(str)\nprint(data_clean.abstract[1])\nprint(data_clean.abstract[2])","96c833c0":"stop_words = set(stopwords.words('english')) \nall_stopwords = [\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"ain\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren\",\"aren't\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can\",\"couldn\",\"couldn't\",\"d\",\"did\",\"didn\",\"didn't\",\"do\",\"does\",\"doesn\",\"doesn't\",\"doing\",\"don\",\"don't\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn\",\"hadn't\",\"has\",\"hasn\",\"hasn't\",\"have\",\"haven\",\"haven't\",\"having\",\"he\",\"her\",\"here\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"i\",\"if\",\"in\",\"into\",\"is\",\"isn\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"just\",\"ll\",\"m\",\"ma\",\"me\",\"mightn\",\"mightn't\",\"more\",\"most\",\"mustn\",\"mustn't\",\"my\",\"myself\",\"needn\",\"needn't\",\"no\",\"nor\",\"not\",\"now\",\"o\",\"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"re\",\"s\",\"same\",\"shan\",\"shan't\",\"she\",\"she's\",\"should\",\"should've\",\"shouldn\",\"shouldn't\",\"so\",\"some\",\"such\",\"t\",\"than\",\"that\",\"that'll\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"ve\",\"very\",\"was\",\"wasn\",\"wasn't\",\"we\",\"were\",\"weren\",\"weren't\",\"what\",\"when\",\"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\"y\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"could\",\"he'd\",\"he'll\",\"he's\",\"here's\",\"how's\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"let's\",\"ought\",\"she'd\",\"she'll\",\"that's\",\"there's\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"what's\",\"when's\",\"where's\",\"who's\",\"why's\",\"would\",\"able\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\"act\",\"actually\",\"added\",\"adj\",\"affected\",\"affecting\",\"affects\",\"afterwards\",\"ah\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"among\",\"amongst\",\"announce\",\"another\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apparently\",\"approximately\",\"arent\",\"arise\",\"around\",\"aside\",\"ask\",\"asking\",\"auth\",\"available\",\"away\",\"awfully\",\"b\",\"back\",\"became\",\"become\",\"becomes\",\"becoming\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\"begins\",\"behind\",\"believe\",\"beside\",\"besides\",\"beyond\",\"biol\",\"brief\",\"briefly\",\"c\",\"ca\",\"came\",\"cannot\",\"can't\",\"cause\",\"causes\",\"certain\",\"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\"contains\",\"couldnt\",\"date\",\"different\",\"done\",\"downwards\",\"due\",\"e\",\"ed\",\"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\"enough\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"except\",\"f\",\"far\",\"ff\",\"fifth\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"former\",\"formerly\",\"forth\",\"found\",\"four\",\"furthermore\",\"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"gone\",\"got\",\"gotten\",\"h\",\"happens\",\"hardly\",\"hed\",\"hence\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hes\",\"hi\",\"hid\",\"hither\",\"home\",\"howbeit\",\"however\",\"hundred\",\"id\",\"ie\",\"im\",\"immediate\",\"immediately\",\"importance\",\"important\",\"inc\",\"indeed\",\"index\",\"information\",\"instead\",\"invention\",\"inward\",\"itd\",\"it'll\",\"j\",\"k\",\"keep\",\"keeps\",\"kept\",\"kg\",\"km\",\"know\",\"known\",\"knows\",\"l\",\"largely\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"'ll\",\"look\",\"looking\",\"looks\",\"ltd\",\"made\",\"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"merely\",\"mg\",\"might\",\"million\",\"miss\",\"ml\",\"moreover\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"n\",\"na\",\"name\",\"namely\",\"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"ninety\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"normally\",\"nos\",\"noted\",\"nothing\",\"nowhere\",\"obtain\",\"obtained\",\"obviously\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"omitted\",\"one\",\"ones\",\"onto\",\"ord\",\"others\",\"otherwise\",\"outside\",\"overall\",\"owing\",\"p\",\"page\",\"pages\",\"part\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"poorly\",\"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\"primarily\",\"probably\",\"promptly\",\"proud\",\"provides\",\"put\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"readily\",\"really\",\"recent\",\"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\"run\",\"said\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sent\",\"seven\",\"several\",\"shall\",\"shed\",\"shes\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\"significantly\",\"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\"specifying\",\"still\",\"stop\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\"tell\",\"tends\",\"th\",\"thank\",\"thanks\",\"thanx\",\"thats\",\"that've\",\"thence\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\"therein\",\"there'll\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"there've\",\"theyd\",\"theyre\",\"think\",\"thou\",\"though\",\"thoughh\",\"thousand\",\"throug\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"together\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"unto\",\"upon\",\"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"'ve\",\"via\",\"viz\",\"vol\",\"vols\",\"vs\",\"w\",\"want\",\"wants\",\"wasnt\",\"way\",\"wed\",\"welcome\",\"went\",\"werent\",\"whatever\",\"what'll\",\"whats\",\"whence\",\"whenever\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\"whereupon\",\"wherever\",\"whether\",\"whim\",\"whither\",\"whod\",\"whoever\",\"whole\",\"who'll\",\"whomever\",\"whos\",\"whose\",\"widely\",\"willing\",\"wish\",\"within\",\"without\",\"wont\",\"words\",\"world\",\"wouldnt\",\"www\",\"x\",\"yes\",\"yet\",\"youd\",\"youre\",\"z\",\"zero\",\"a's\",\"ain't\",\"allow\",\"allows\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"associated\",\"best\",\"better\",\"c'mon\",\"c's\",\"cant\",\"changes\",\"clearly\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"corresponding\",\"course\",\"currently\",\"definitely\",\"described\",\"despite\",\"entirely\",\"exactly\",\"example\",\"going\",\"greetings\",\"hello\",\"help\",\"hopefully\",\"ignored\",\"inasmuch\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"it'd\",\"keep\",\"keeps\",\"novel\",\"presumably\",\"reasonably\",\"second\",\"secondly\",\"sensible\",\"serious\",\"seriously\",\"sure\",\"t's\",\"third\",\"thorough\",\"thoroughly\",\"three\",\"well\",\"wonder\"]","732534a6":"test = [\"Middle East Respiratory Syndrome, caused by the MERS coronavirus (MERS-CoV), continues to cause severe respiratory disease with a high case fatality rate. To date, potential antiviral treatments for MERS-CoV have shown limited efficacy in animal studies. Here, we tested the efficacy of the broad-acting antiviral remdesivir in the rhesus macaque model of MERS-CoV infection. Remdesivir reduced the severity of disease, virus replication, and damage to the lungs when administered either before or after animals were infected with MERS-CoV. Our data show that remdesivir is a promising antiviral treatment against MERS that could be considered for implementation in clinical trials. It may also have utility for related coronaviruses such as the novel coronavirus 2019-nCoV emerging from Wuhan, China.\"]\nwords = set(nltk.corpus.words.words())\nnew = \" \".join(w for w in nltk.wordpunct_tokenize(test) if w.lower() in words or not w.isalpha())","682afda5":"print(stopwords.words('english'))\nprint(all_stopwords)","ad5da880":"word_tokens = word_tokenize(abstract) \n  \nfiltered_sentence = [w for w in word_tokens if not w in all_stopwords] \n  \nfiltered_sentence = [] \n  \nfor w in word_tokens: \n    if w not in all_stopwords: \n        filtered_sentence.append(w) \n  \nprint(word_tokens) \nprint(filtered_sentence)","ee43b74e":"ID","7d55828f":"len(token)\nprint(abstract)","4dc8f602":"# Identifying previously studied inhibitors of COVID Proteins\nIn a recent paper by Gordon, et al., 2020, they used proteomics approaches to identify human host proteins that interact with proteins from SARS-CoV-2 (the virus that causes COVID-19). From these human proteins, the were able to \"identify 66 druggable human proteins or host factors targeted by 69 existing FDA-approved drugs, drugs in clinical trials and\/or preclinical\ncompounds, that we are currently evaluating for efficacy in live SARS-CoV-2 infection assays.\"\n\nThis approach looks specifically at SARS-CoV proteins, their interaction partners, and the compounds that have been shown to inhibit them. The massive amount of research material contained in Kaggle's CORD-19 research challenge contains published papers of both SARS-CoV-2 (COVID-19) as well as SARS-CoV (SARS), making it a major resource to discover new inhibitors or drugs. Because the current outbreak of the virus SARS-CoV-2 has high biochemical similarity to SARS-CoV, we believe this literature dataset likely contains a significant amount of information about protein-protein interactions as well as protein-inhibitor interactions that relate to these viruses. \n\nOur process will go as follows:\n1. Parse through abstract excerpts to collect a subset of papers that mention proteins in either SARS-CoV-2 or SARS-CoV.\n2. Use natural language processing to search the keywords from the abstact excerpts using PubChemPy   ","27791940":"**PubChemPy**\nAfter natural language processingPubChemPy will search through the key words and phrases ","49e9d1c7":"* NLTK - Natural Learning Toolkit: PorterStemmer\nhttp:\/\/www.nltk.org\/howto\/stem.html","4532bfda":"Here, we move into Natural Language Processing to analyze the words in the abstract data frame. We will remove punctuation, tokenize into individual words, then remove all words that are English. This will hopefully leave compounds names behind."}}