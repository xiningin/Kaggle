{"cell_type":{"3c3c3454":"code","ced1990e":"code","2b02359c":"code","199f7690":"code","cecc33b3":"code","982dcf78":"markdown"},"source":{"3c3c3454":"# imports\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport matplotlib.pyplot as plt\n\nimport os\nimport yaml\nimport h5py\nfrom pprint import pprint","ced1990e":"# Model code\ndef as_shape(shape):\n    if isinstance(shape, tuple):\n        return shape\n    elif isinstance(shape, list):\n        return tuple(shape)\n    elif isinstance(shape, int):\n        return (shape,)\n    else:\n        raise ValueError(\"Invalid shape argument: {0}\".format(str(shape)))\n\ndef conv_output_shape(input_shape, out_channels, kernel_size=1, stride=1, pad=0, dilation=1):\n    '''\n        Get the output shape of a convolution given the input_shape.\n    '''\n    input_shape = as_shape(input_shape)\n    h,w = input_shape[-2:]\n    if type(kernel_size) is not tuple:\n        kernel_size = (kernel_size, kernel_size)\n    h = math.floor(((h + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )\/ stride) + 1)\n    w = math.floor(((w + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )\/ stride) + 1)\n    return out_channels, h, w\n\nclass CNet(nn.Module):\n    '''\n        A convolutional network that takes as input an image of dimension input_shape = (C,H,W).\n    '''\n    \n    def __init__(self, input_shape, device='cpu'):\n        super(CNet, self).__init__() \n        self.input_shape = as_shape(input_shape)\n        s1 = conv_output_shape(input_shape, 16, kernel_size=4, stride=2)\n        s2 = conv_output_shape(s1, 32, kernel_size=4, stride=1)\n        s3 = conv_output_shape(s2, 64, kernel_size=4, stride=1)\n    \n        self.conv1 = nn.Conv2d(input_shape[0], 16, kernel_size=4, stride=2)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=1)\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=4, stride=1)\n\n        self.output_shape = as_shape(int(np.prod(s3)))\n\n        self.device = device\n        self.to(device)\n    \n    def to(self, device):\n        self.device = device\n        return super(CNet, self).to(device)\n\n    def forward(self, x_):\n        x_ = x_.to(self.device)\n        y_ = F.leaky_relu(self.conv1(x_))\n        y_ = F.leaky_relu(self.conv2(y_))\n        y_ = F.leaky_relu(self.conv3(y_)).view(x_.shape[0], -1)\n        return y_\n    \n# This simple model architecture is used in the original paper.\nclass CNet2(CNet):\n    '''\n        A convolutional network based on CNet with a fully connected output layer of given dimension.\n    '''\n    \n    def __init__(self, input_shape, output_shape, output_activation=nn.Identity()):\n        super(CNet2, self).__init__(input_shape)\n        output_shape = as_shape(output_shape)\n        self.out_layer = nn.Linear(self.output_shape[0], output_shape[0])\n        self.output_shape = output_shape\n        self.output_activation = output_activation\n\n    def forward(self, x_):\n        x_ = super(CNet2, self).forward(x_)\n        y_ = self.output_activation(self.out_layer(x_))\n        return y_","2b02359c":"# Load Model and Atari Anomaly Dataset trajectory\n\npath = \"\/kaggle\/input\/s3n-pretrained-models\/models\/dryrun-sssn-Breakout-2-20200209131829\"\n#path = \"\/kaggle\/input\/s3n-pretrained-models\/models\/dryrun-sssn-Pong-64-20200429154006\"\nconfig_path = os.path.join(path, \"config.yaml\")\nmodel_path = os.path.join(path, \"model.pt\")\n\n#load config\nwith open(config_path) as file: \n    config = {k:v['value'] for k,v in yaml.full_load(file).items() if \"wandb\" not in k}\n    # fix issue with Breakout-2 (old config version) to be compatible with new config\n    if 'state' not in config:\n        config['state'] = {'shape':config['state_shape'], 'dtype':'float32', 'format':['CHW', 'RGB']}\n        del config['state_shape']\n    \npprint(config)\n    \n#load model\nmodel = CNet2(config['state']['shape'], config['latent_shape'])\nmodel.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n\n#load example data\nepisode = 86 # CHANGE ME - to visualise different episodes (and different kinds of anomaly)\npath = \"\/kaggle\/input\/atari-anomaly-dataset-aad\/AAD\/anomaly\/BreakoutNoFrameskip-v4\/episode({0}).hdf5\".format(str(episode))\nfile = h5py.File(path, 'r')\nstate = file['state'][...] #for use in visualisation\nlabel = file['label'][...].astype(np.uint8)","199f7690":"# Encode the trajectory\ndef encode(model, state, batch_size=64):\n    model.eval()\n    Z = [] #prevent memory errors if not logged in\n    for i in range(0,state.shape[0],batch_size): \n        Z.append(model(state[i:i+batch_size]).detach().numpy())\n    return np.concatenate(Z, axis=0)\n\n_state = torch.from_numpy(state.transpose((0,3,1,2)).astype(np.float32) \/ 255.) #convert to torch tensor\nz = encode(model, _state)","cecc33b3":"# Interactive visualisation of 2D embedding\n\nimport cv2\nimport plotly.graph_objs as go\nfrom ipywidgets import Image, Layout, HBox\n\nx, y = z[:,0], z[:,1]\n\nscatter_colour = np.array(['blue','red'])[label]\nfig = go.FigureWidget(data=[dict(type='scattergl',x=x, y=y,\n            mode='lines+markers',\n            marker=dict(color=scatter_colour),\n            line=dict(color='#b9d1fa'))])\nfig.update_layout(autosize=False, width=500, height=500, margin=dict(l=5,b=5,r=5,t=5))\n\ndef to_bytes(image):\n    _, image = cv2.imencode(\".png\", image)\n    return image.tobytes()\n\n#convert images to png format\nscale = 2\nimage_width = '{0}px'.format(int(state.shape[2] * scale))\nimage_height = '{0}px'.format(int(state.shape[1] * scale))\nprint(image_width, image_height)\nimages = [to_bytes(image) for image in state]\n\nimage_widget = Image(value=images[0], layout=Layout(height=image_height, width=image_width))\n    \ndef hover_fn(trace, points, state):\n    ind = points.point_inds[0]\n    image_widget.value = images[ind]\n\nfig.data[0].on_hover(hover_fn)\n\ndisplay(HBox([fig, image_widget]))","982dcf78":"# S3N Starter Kernel\nThis kernel contains an interactive demo of the S3N model for a 2D embedding space on the game Breakout. \n\nInteractivity requires a python backend, simply: `copy & edit` the kernel, then `run all`. "}}