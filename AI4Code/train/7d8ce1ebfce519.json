{"cell_type":{"3d04edc6":"code","f391af2e":"code","351a1f9f":"code","73c29bb7":"code","6a22ea8c":"code","42b4608f":"code","40d7e2d8":"code","79240a3c":"code","22c668ef":"code","6a1f0866":"code","b9ba4862":"code","7acc9aa1":"code","ef050e11":"code","1a992eb7":"code","e9f3320e":"code","31478c4e":"code","24348721":"code","20f2f9ab":"code","67bc7923":"code","945a9349":"code","ee13c05f":"code","073c2a14":"code","7b87afd9":"code","3b59d476":"code","51e502d4":"code","187380d2":"code","75bc2fa2":"code","52e6954d":"code","ffa0f302":"code","1373aaaa":"code","d55a3f15":"code","026cce66":"code","b87090e4":"code","a66a23bb":"code","b45cf253":"code","b3aefeab":"code","3f470f84":"code","1ad1ebbe":"code","e3d7c086":"code","122ff1ba":"code","54f942d1":"code","6a5eba56":"code","a2db494a":"code","4acae2bf":"code","13aac3b2":"code","38c6652c":"code","0d0fbbe4":"code","0d6c0e86":"code","07c9a41d":"code","c5ea0f0a":"code","155cad5f":"code","ce5ce415":"code","4402f0cf":"code","b8052133":"code","57a2d9a4":"code","b7b2d182":"code","86cb8846":"code","6d3cc9d4":"code","831c14ad":"code","d9da8be0":"code","b485046d":"code","ef6d1f1e":"code","dfa3eb46":"code","24abad3d":"code","59019523":"code","cecbe9d2":"code","4e0f9f90":"code","35d83a79":"code","5a6546e4":"code","4d20d346":"code","fbd1243a":"code","44eb2c72":"code","9bf31d25":"code","2b548621":"code","d7e8319f":"code","8d645b93":"code","512b24bf":"code","ae18a210":"code","4e0f33d7":"code","db8e7fef":"code","94efa003":"code","14a81b1d":"code","e8df54d5":"code","47e9b6c6":"code","273dddab":"code","f878ef47":"code","6b8a137c":"code","f7df4b80":"code","8985da75":"code","d091c2f8":"code","961acefe":"code","4cab92c2":"code","06abfcee":"code","7fe8a346":"code","c92584ad":"code","d6e0da2b":"code","5b5a741e":"code","fce32561":"code","629ba145":"code","306b6755":"code","f725b6ad":"markdown","8d18f371":"markdown","9f2112d6":"markdown","6f6c32e5":"markdown","9b73117c":"markdown","cfeed910":"markdown","30254fa0":"markdown","eed39d9d":"markdown","012b262a":"markdown","c2f89861":"markdown","7ddac50f":"markdown","9e442c8a":"markdown","143f52fd":"markdown","2fad2232":"markdown","20898e9e":"markdown","ed5ce52c":"markdown","0c8b12a4":"markdown","cc4673ca":"markdown","e79b55a2":"markdown","1b1f5af9":"markdown","9e1e4f2e":"markdown"},"source":{"3d04edc6":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set()","f391af2e":"train_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")","351a1f9f":"train_data.head(10)","73c29bb7":"## PassengerId does not train, Cabin has lot's of missing values,\n## Ticket is a string with many different values and Sex is a better feature than name.","6a22ea8c":"train_data.drop(columns=['Name', 'Ticket', 'Cabin','PassengerId'], axis=1, inplace=True)","42b4608f":"train_data['Sex'] = train_data['Sex'].map({'male':0,'female':1})","40d7e2d8":"train_data['Age'].isnull().sum() ##Many missing values in the dataset","79240a3c":"train_data['Embarked'].value_counts() ##S is more common","22c668ef":"train_data['Embarked'].fillna(value='S',axis=0, inplace=True)","6a1f0866":"train_data['Embarked'] = train_data['Embarked'].map({'S':0,'C':1,'Q':2})","b9ba4862":"train_data.fillna(value=train_data['Age'].mean(), axis=0, inplace=True) ## let's fill with the mean","7acc9aa1":"train_data['Embarked'].unique()","ef050e11":"train_data.describe()","1a992eb7":"train_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean()","e9f3320e":"train_data[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean()","31478c4e":"train_data[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean()","24348721":"train_data[['SibSp', 'Survived']].groupby('SibSp', as_index=False).mean()","20f2f9ab":"train_data[['Parch', 'Survived']].groupby('Parch', as_index=False).mean()","67bc7923":"train_data['AgeBand'] = pd.cut(train_data['Age'], 9) ## Need to save in another column to discover the ageband, then we can replace in the age feature","945a9349":"train_data.head()","ee13c05f":"train_data[['AgeBand','Survived']].groupby('AgeBand', as_index=False).mean()","073c2a14":"train_data.loc[ train_data['Age'] <= 18, 'Age'] = 0\ntrain_data.loc[(train_data['Age'] > 18) & (train_data['Age'] <= 44), 'Age'] = 1\ntrain_data.loc[(train_data['Age'] > 44) & (train_data['Age'] <= 53), 'Age'] = 2\ntrain_data.loc[(train_data['Age'] > 53) & (train_data['Age'] <= 62), 'Age'] = 3\ntrain_data.loc[ train_data['Age'] > 62, 'Age'] = 4","7b87afd9":"train_data.drop(labels='AgeBand', axis=1, inplace=True)","3b59d476":"train_data['FareBand'] = pd.qcut(train_data['Fare'], 5)","51e502d4":"train_data[['FareBand', 'Survived']].groupby('FareBand', as_index=False).mean()","187380d2":"train_data.loc[ train_data['Fare'] <= 10, 'Fare'] = 0\ntrain_data.loc[(train_data['Fare'] > 10) & (train_data['Fare'] <= 40), 'Fare'] = 1\ntrain_data.loc[train_data['Fare'] > 40 , 'Fare'] = 2","75bc2fa2":"train_data.drop(labels='FareBand', axis=1, inplace=True)","52e6954d":"fig, corr = plt.subplots(figsize=(20,15))\nsns.heatmap(train_data.corr(), annot=True)","ffa0f302":"test_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")","1373aaaa":"test_data.head(10)","d55a3f15":"test_data.drop(columns=['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)","026cce66":"test_data['Sex'] = test_data['Sex'].map({'male':0,'female':1})","b87090e4":"test_data['Embarked'] = test_data['Embarked'].map({'S':0,'C':1,'Q':2})","a66a23bb":"test_data.describe() ##Missing one value on fare.","b45cf253":"test_data['Fare'].fillna(value=test_data['Fare'].median(),axis=0, inplace=True) ## We replace with the median due to it's high variance on the data","b3aefeab":"test_data.fillna(value=test_data['Age'].mean(), axis=0, inplace=True)","3f470f84":"test_data.loc[ test_data['Age'] <= 18, 'Age'] = 0\ntest_data.loc[(test_data['Age'] > 18) & (test_data['Age'] <= 44), 'Age'] = 1\ntest_data.loc[(test_data['Age'] > 44) & (test_data['Age'] <= 53), 'Age'] = 2\ntest_data.loc[(test_data['Age'] > 53) & (test_data['Age'] <= 62), 'Age'] = 3\ntest_data.loc[ test_data['Age'] > 62, 'Age'] = 4","1ad1ebbe":"test_data.loc[ test_data['Fare'] <= 10, 'Fare'] = 0\ntest_data.loc[(test_data['Fare'] > 10) & (test_data['Fare'] <= 40), 'Fare'] = 1\ntest_data.loc[test_data['Fare'] > 40 , 'Fare'] = 2","e3d7c086":"submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","122ff1ba":"features = [\"Pclass\", \"Sex\", \"Age\",\"Fare\",\"SibSp\",\"Parch\",\"Embarked\"]\ntargets = [\"Survived\"]\nX_train, X_test = train_data[features], test_data[features]\ny_train, y_test = train_data[targets],submission[targets]","54f942d1":"import statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics","6a5eba56":"lr = LogisticRegression()\nmodel = lr.fit(X_train,y_train)\npredictions_lr = model.predict(X_test)","a2db494a":"predictions_lr = model.predict(X_test)","4acae2bf":"print(pd.crosstab(predictions_lr,submission['Survived'],margins=True,rownames=['Previsto'],colnames=['         Real']))\nprint(metrics.classification_report(y_test,predictions_lr))","13aac3b2":"output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions_lr})","38c6652c":"output = output.to_csv(\"My_submission_Logistic.csv\", index=False)","0d0fbbe4":"X_train.head(),X_test.head()","0d6c0e86":"y_train.head(), y_test.head()","07c9a41d":"from sklearn.neighbors import KNeighborsClassifier","c5ea0f0a":"def KNN_1(neighbors):\n    neighbors = range(1,neighbors+1,1)\n    print(\"Para p = 1\")\n    for i in neighbors:\n        KNN = KNeighborsClassifier(n_neighbors=i,p=1)\n        model = KNN.fit(X_train,np.ravel(y_train))      \n        results_KNN = model.predict(test_data[features])\n        print(\"Para uma quantidade de vizinhos de:\", i,\"a precis\u00e3o do modelo foi de \",round((model.score(X_train,y_train))*100,2),\"%\")","155cad5f":"def KNN_2(neighbors):\n    neighbors = range(1,neighbors+1,1)\n    print(\"Para p = 2\")\n    for i in neighbors:\n        KNN = KNeighborsClassifier(n_neighbors=i,p=2)\n        model = KNN.fit(X_train,np.ravel(y_train))\n        results_KNN = model.predict(test_data[features])\n        print(\"Para uma quantidade de vizinhos de:\", i,\"a precis\u00e3o do modelo foi de \",round((model.score(X_train,y_train))*100,2),\"%\")","ce5ce415":"def choosen_KNN(neighbors,p):\n    KNN = KNeighborsClassifier(n_neighbors=neighbors,p=p)\n    model = KNN.fit(X_train,np.ravel(y_train))\n    results_KNN = model.predict(test_data[features])\n    print(\"Precis\u00e3o \",round((model.score(X_train,y_train))*100,2),\"% \\n\\n\")\n    output_KNN = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': results_KNN})\n    output_KNN.to_csv('my_submission_KNN.csv', index=False)\n    print(pd.crosstab(results_KNN,submission['Survived'],margins=True,rownames=['Previsto'],colnames=['         Real']))","4402f0cf":"KNN_1(20)","b8052133":"KNN_2(20)","57a2d9a4":"choosen_KNN(7,2)","b7b2d182":"from sklearn.tree import DecisionTreeClassifier","86cb8846":"def DTC(depth):\n    depth = range(1,depth+1,1)\n    for i in depth:\n        DTC = DecisionTreeClassifier(max_depth=i)\n        model = DTC.fit(X_train, y_train)\n        predictions_DTC = model.predict(X_test)\n        print(\"Para uma profundidade de:\", i,\"a precis\u00e3o do modelo foi de \",round((model.score(X_train,y_train))*100,2),\"%\")","6d3cc9d4":"def choosen_DTC(depth):\n    \n    DTC = DecisionTreeClassifier(max_depth=depth)\n    model = DTC.fit(X_train, y_train)\n    predictions_DTC = model.predict(X_test)\n    \n    print (pd.crosstab(predictions_DTC, submission['Survived'], margins=True,rownames=['Previsto'],colnames=['         Real']))\n    print (metrics.classification_report(submission['Survived'],predictions_DTC))\n\n    output_DTC = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions_DTC})\n    output_DTC.to_csv('my_submission_DTC.csv', index=False)","831c14ad":"DTC(15)","d9da8be0":"choosen_DTC(3)","b485046d":"from sklearn.model_selection import cross_val_predict,cross_val_score,cross_validate\nfrom sklearn import metrics","ef6d1f1e":"X_test.shape,X_train.shape","dfa3eb46":"y_test.shape,y_train.shape","24abad3d":"def DTC_CROSS(depth, folds):\n    model_DTC = DecisionTreeClassifier(max_depth=depth)\n    model_DTC.fit(X_train, y_train)\n    cross_validate(model_DTC,X_train,y_train,cv=folds)\n    predictions_DTC_cross = model_DTC.predict(X_test)\n    accuracy = metrics.accuracy_score(y_test,predictions_DTC_cross)\n    \n    print(\"\\n\")\n    print (pd.crosstab(predictions_DTC_cross, submission['Survived'], margins=True,rownames=['Previsto'],colnames=['         Real']))\n    print (metrics.classification_report(submission['Survived'],predictions_DTC_cross))\n\n    \n    output_DTC = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions_DTC_cross})\n    output_DTC.to_csv('my_submission_DTC_cross.csv', index=False)\n\n    return print(\"accuracy = \",accuracy*100,\"%\")","59019523":"DTC_CROSS(4,2) ## We got the same results","cecbe9d2":"from sklearn import svm","4e0f9f90":"features = ['Sex', 'Pclass','Fare','Age','Embarked','SibSp','Parch']\ntargets = ['Survived']","35d83a79":"X_train = train_data[features]\nX_test = test_data[features]\nX_train.head()","5a6546e4":"y_train = train_data[targets]\ny_test = submission['Survived']\ny_train.head()","4d20d346":"def choosen_SVM(C, gamma):\n    model_SVM = svm.SVC(C=C,gamma=gamma,random_state=0)\n    model_SVM.fit(X_train,y_train)\n    predictions_SVM = model_SVM.predict(X_test)\n    print(\"Para C = \",C,\"e gamma = \", gamma,\"   Precis\u00e3o = \",round((model_SVM.score(X_train,y_train))*100,2),\"% \")\n    print(pd.crosstab(predictions_SVM,submission['Survived'],margins=True,rownames=['Previsto'],colnames=['         Real']))\n    print (metrics.classification_report(submission['Survived'],predictions_SVM))\n\n    output_SVM = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions_SVM})\n    output_SVM.to_csv('my_submission_SVM.csv', index=False)","fbd1243a":"choosen_SVM(1000,0.01)","44eb2c72":"sns.distplot(train_data[targets],kde=False)","9bf31d25":"train_data['Survived'].value_counts()","2b548621":"from imblearn.under_sampling import NearMiss","d7e8319f":"X  = train_data[features]\nX_test = test_data[features]\n\ny = train_data[targets]\ny_test = submission[targets]","8d645b93":"nrm = NearMiss()","512b24bf":"X, y =  nrm.fit_sample(X,y)","ae18a210":"X.shape","4e0f33d7":"sns.distplot(y,kde=False) ##Now we see that the dataset is balanced, let's check the metrics now","db8e7fef":"lr_us = LogisticRegression()\nmodel = lr_us.fit(X,y)\npredictions_lr_us = model.predict(X_test)\nprint(pd.crosstab(predictions_lr_us,submission['Survived'],margins=True,rownames=['Previsto'],colnames=['         Real']))\nprint(metrics.classification_report(y_test,predictions_lr_us))","94efa003":"predictions_lr_us.shape","14a81b1d":"output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions_lr_us})\noutput.to_csv(\"My_submission_Logistic_US.csv\", index=False)","e8df54d5":"from imblearn.over_sampling import SMOTE","47e9b6c6":"features = ['Sex', 'Pclass','Fare','Age','Embarked','SibSp','Parch']\ntargets = ['Survived']","273dddab":"sns.distplot(train_data['Survived'],kde=False)\ntrain_data['Survived'].value_counts()","f878ef47":"print(\"Percentage of 1's = \",(342\/549)*100,\"%\")","6b8a137c":"sampling = np.linspace(0.65,1,8)\nsampling","f7df4b80":"def Over_samp(train, test, submisson):\n    \n    features = ['Sex', 'Pclass','Fare','Age','Embarked','SibSp','Parch']\n    targets = ['Survived']\n    \n    sampling = np.linspace(0.65,1,16)\n    \n    X  = train[features]\n    X_test = test[features]\n    \n    y = train[targets]\n    y_test = submission[targets]\n    \n    for i in sampling:\n        print(\"For sampling strategy = \",i*100,\"%\\n\")\n        smt = SMOTE(sampling_strategy = i)\n        X, y = smt.fit_sample(X,y)\n        lr_os = LogisticRegression()\n        model = lr_os.fit(X,y)\n        predictions_lr_os = model.predict(X_test)\n        print(pd.crosstab(predictions_lr_os,submission['Survived'],margins=True,rownames=['Previsto'],colnames=['         Real']))\n        print(metrics.classification_report(y_test,predictions_lr_os))\n        print(\"\\n\")","8985da75":"Over_samp(train_data,test_data, submission)","d091c2f8":"## Let's try 65%","961acefe":"X  = train_data[features]\nX_test = test_data[features]\n    \ny = train_data[targets]\ny_test = submission[targets]","4cab92c2":"smt = SMOTE(sampling_strategy = 0.65)\nX, y = smt.fit_sample(X,y)\nlr_os = LogisticRegression()\nmodel = lr_os.fit(X,y)\npredictions_lr_os = model.predict(X_test)\nprint(pd.crosstab(predictions_lr_os,submission['Survived'],margins=True,rownames=['Previsto'],colnames=['         Real']))\nprint(metrics.classification_report(y_test,predictions_lr_os))\noutput = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived': predictions_lr_os})\noutput.to_csv(\"My_submission_Logistic_OS.csv\", index=False)","06abfcee":"def DTC_US(depth):\n    features = ['Sex', 'Pclass','Fare','Age','Embarked','SibSp','Parch']\n    targets = ['Survived']  \n    \n    X_train  = train_data[features]\n    X_test = test_data[features]\n    \n    y_train = train_data[targets]\n    y_test = submission[targets]\n    \n    X,y = nrm.fit_sample(X_train, y_train)    \n    sns.distplot(y, kde=False)\n    \n    DTC = DecisionTreeClassifier(max_depth=depth)\n    model = DTC.fit(X, y)\n    predictions_DTC_US = model.predict(X_test)\n    \n    print (pd.crosstab(predictions_DTC_US, submission['Survived'], margins=True,rownames=['Previsto'],colnames=['         Real']))\n    print(metrics.classification_report(y_test,predictions_DTC_US))\n    output_DTC = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions_DTC_US})\n    output_DTC.to_csv('my_submission_DTC_US.csv', index=False)","7fe8a346":"DTC_US(5)","c92584ad":"def DTC_OS(depth): \n    features = ['Sex', 'Pclass','Fare','Age','Embarked','SibSp','Parch']\n    targets = ['Survived']\n\n    X_train  = train_data[features]\n    X_test = test_data[features]\n\n    y_train = train_data[targets]\n    y_test = submission[targets]\n\n    smt = SMOTE(sampling_strategy = 0.70)\n    X,y = smt.fit_sample(X_train, y_train)\n    sns.distplot(y, kde=False)\n\n    DTC = DecisionTreeClassifier(max_depth=depth, random_state=42)\n    model = DTC.fit(X, y)\n    predictions_DTC_OS = model.predict(X_test)\n\n    print (pd.crosstab(predictions_DTC_OS, submission['Survived'], margins=True,rownames=['Previsto'],colnames=['         Real']))\n    output_DTC = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions_DTC_OS})\n    output_DTC.to_csv('my_submission_DTC_OS.csv', index=False)","d6e0da2b":"DTC_OS(5)","5b5a741e":"def SVM_US(C, gamma): \n    features = ['Sex', 'Pclass','Fare','Age','Embarked','SibSp','Parch']\n    targets = ['Survived']  \n    \n    X_train  = train_data[features]\n    X_test = test_data[features]\n    \n    y_train = train_data[targets]\n    y_test = submission[targets]\n    \n    X,y = nrm.fit_sample(X_train, y_train)    \n    sns.distplot(y, kde=False)\n    \n    sv = svm.SVC(C=C, gamma=gamma)\n    model = sv.fit(X, y)\n    predictions_SVM_US = model.predict(X_test)\n    \n    print (pd.crosstab(predictions_SVM_US, submission['Survived'], margins=True,rownames=['Previsto'],colnames=['         Real']))\n    print(metrics.classification_report(y_test,predictions_SVM_US))\n    output_DTC = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions_SVM_US})\n    output_DTC.to_csv('my_submission_SVM_US.csv', index=False)","fce32561":"SVM_US(5,0.01)","629ba145":"def SVM_OS(C,gamma): \n    features = ['Sex', 'Pclass','Fare','Age','Embarked','SibSp','Parch']\n    targets = ['Survived']\n\n    X_train  = train_data[features]\n    X_test = test_data[features]\n\n    y_train = train_data[targets]\n    y_test = submission[targets]\n\n    smt = SMOTE(sampling_strategy = 0.75)\n    X,y = smt.fit_sample(X_train, y_train)\n    sns.distplot(y, kde=False)\n\n    sv = svm.SVC(C=C, gamma=gamma)\n    model = sv.fit(X, y)\n    predictions_SVM_OS = model.predict(X_test)\n    \n    print (pd.crosstab(predictions_SVM_OS, submission['Survived'], margins=True,rownames=['Previsto'],colnames=['         Real']))\n    print(metrics.classification_report(y_test,predictions_SVM_OS))\n    output_DTC = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions_SVM_OS})\n    output_DTC.to_csv('my_submission_SVM_OS.csv', index=False)","306b6755":"SVM_OS(100,0.01)","f725b6ad":"## Since we have many values for Age and Fare, we need to create ranges for each one of them and see the correlation between an age group\/fare group and it's survival rate.\n\n## Feature that may be added: Age, Fare","8d18f371":"## Now, we are going to use DecisionTreeClassifier","9f2112d6":"## We ca also clearly see the higher survival rate for those who paid > 10 and even higher for those who paid >40","6f6c32e5":"## Those who embaked at C had a higher survival rate, females had a greater survival rate and Pclass 1-2 had higher survival rate. These features need to be used, as they influenced the survival rate of a person.\n\n## Choose features so far: Pclass, Sex, Embarked","9b73117c":"## The KNN is predicting many 1's as 0's. We need to correct that, maybe a better model. After editing ","cfeed910":"## We can now see that the model is not learning well the 0 values, that's because we cut many of data 0 data points.","30254fa0":"## It seems the Undersampling and Oversampling are giving the same results. For Undersampling our machine learn models learn better to categorize 1's, but decreased accuracy of 0's.","eed39d9d":"## Now we're going to try SVM, as this can give us a better division of 0's and 1's","012b262a":"## Dropping columns that are not useful to the analysis.","c2f89861":"> ## We performed many classification machine learning models, my best score was almost 80%. I'll try to performa a undersampling and oversampling just to try\/learn it, but I believe that the dataset is not (too much unbalanced, after all we will check that 1's value are is 62% size of the 0's. I'll try to use 75% and check if it improves","7ddac50f":"## Loading the data","9e442c8a":"## Cleaning and adjusting the test data.","143f52fd":"## Now, we're going to use KNN to predict the survival rate of the test data.","2fad2232":"## Converting categorical features into numerical for better analysis and also filling missing values","20898e9e":"## My best model was with a Decision Tree Regressor, so I'll try to use cross-validation with 5 folds to check if the accuracy is improved.","ed5ce52c":"## Well, that clearly did not work for a logistic regression, after submiting, I got roughly 70% on Oversampling and Undersampling!\n\n## I'll try the same method for SVM and DTC","0c8b12a4":"## For oversampling our DTC algorithm had a very similar accuracy. We'll finally try for SVM's and stop there. The best score was using DTC and it was 79,83%","cc4673ca":"## Checking the data","e79b55a2":"## NOW WE CHECK CORRELATION BETWEEN SOME OF THE FEATURES AND SURVIVED","1b1f5af9":"## We can see a good correlation value between Survived and Pclass, Embarked, Age and Sex. These will be used as features! Fare was removed due to it's high P-value","9e1e4f2e":"## By dividing into Bands, we can see the higher survival rate of Children and Young People"}}