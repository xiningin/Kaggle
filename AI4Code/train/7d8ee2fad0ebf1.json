{"cell_type":{"4c6a94ee":"code","4cf6eafe":"code","647215bc":"code","11755b5c":"code","8e71be18":"code","8fdbabc6":"code","852cc4d6":"markdown"},"source":{"4c6a94ee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4cf6eafe":"\n#------------------------   Loading Modules  ---------------------------------\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import RobustScaler\n\nfrom sklearn.model_selection import cross_val_score \nfrom sklearn.feature_selection import SelectKBest,mutual_info_classif\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA","647215bc":"#-------------------  Loading Dataset  ---------------------------------------------\ntrain=pd.read_csv('\/kaggle\/input\/titanic\/train.csv',index_col='PassengerId')\ntest=pd.read_csv('\/kaggle\/input\/titanic\/test.csv',index_col='PassengerId')","11755b5c":"#-----------------------    One Hot Encoder  --------------------------------------\ntrain=pd.get_dummies(train,columns=['Sex','Embarked'],drop_first=True)\n\n#----------------------------  Dropping unnecessary columns    --------------------\ntrain.drop(['Name','Ticket','Cabin'],axis=1,inplace=True)","8e71be18":"#---------------------------  Pipelile basic ---------------------------------------\npipeline=Pipeline(steps=[('scale',RobustScaler()),('impute',KNNImputer(n_neighbors=7)),('scales',PCA(n_components=7)),('model',LogisticRegression(max_iter=1000))])\nscore=cross_val_score(pipeline,train.drop('Survived',axis=1),train['Survived'],scoring='accuracy',cv=5)\nprint(np.mean(score))\npipeline.fit(train.drop('Survived',axis=1),train['Survived'])","8fdbabc6":"#-----------------------  Model Prediction -------------------------------------------\ntest=pd.get_dummies(test,columns=['Sex','Embarked'],drop_first=True)\ntest.drop(['Name','Ticket','Cabin'],axis=1,inplace=True)\n\n#--------------------------------------------------------------------\nsubmit=pd.DataFrame(test.index)\nsubmit['Survived']=pipeline.predict(test)\nsubmit.to_csv('index.csv',index=False)","852cc4d6":"# 1. Loading Dataset and modules"}}