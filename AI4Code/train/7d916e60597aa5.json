{"cell_type":{"71870bb3":"code","a6e4d30a":"code","796a667d":"code","4e6332c5":"code","f67b42e5":"code","be815ada":"code","7902d63d":"code","d4cd9b95":"code","63fc115f":"code","869cb928":"code","61da478a":"code","fb0c56be":"code","4ea262ea":"code","00e6f0df":"code","05855f71":"code","b5d32378":"code","fe7c3693":"code","89dacb39":"code","c6704e8c":"code","4800b52c":"code","07b85610":"code","54e0c484":"code","b0f4684b":"code","34e01af4":"code","5e5a0ea8":"code","2d3f1576":"code","be4223bb":"code","dded46b0":"code","7099f188":"code","eb3eed3c":"code","398dc529":"code","030e274a":"code","974f7b57":"code","1867e3e4":"code","40dc6776":"code","0fdcf4cb":"code","063f0b13":"code","4eff396b":"code","319904f3":"code","1522789c":"code","559ed3bb":"code","003e2571":"code","718db518":"code","4e469735":"code","cdb36214":"code","b86bbda8":"code","6a42c24c":"code","8cc9acff":"markdown","36230ad9":"markdown","1cb51ca4":"markdown","4c9bd909":"markdown","d9fa6f04":"markdown","f87381b4":"markdown","536b91d5":"markdown","89797abe":"markdown","975c7084":"markdown","e5aadee8":"markdown","de1419a2":"markdown","ebf48e8f":"markdown","9eb7cd33":"markdown","f8aac342":"markdown","4925e65b":"markdown","3a53f00e":"markdown","bb3ebd67":"markdown","1599b304":"markdown","e5bb06a9":"markdown","812a610d":"markdown","3494c492":"markdown","d22b197c":"markdown","7fd8c687":"markdown","f16f34e5":"markdown","14e2e86f":"markdown"},"source":{"71870bb3":"# import os\n# os.listdir('..\/input\/')","a6e4d30a":"import time\nimport matplotlib.pyplot as plt\nimport cv2 as cv\nfrom math import sqrt \nimport pandas as pd\nimport numpy as np\nfrom torchvision import transforms as tfs\nimport torch\nfrom PIL import Image","796a667d":"time_start = time.time()","4e6332c5":"datas = pd.read_csv(\"..\/input\/fer2013\/fer2013.csv\")\ndatas","f67b42e5":"lab = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neural']\nlabels_num = datas.emotion.value_counts()\nla = [0,1,2,3,4,5,6]\nla_num = [labels_num[i] for i in range(len(labels_num))]\nprint(labels_num)\nplt.bar(range(len(la_num)), la_num,color='rgbc',tick_label=lab)  #plt.barh\u5219\u662f\u628a\u8be5\u56fe\u53d8\u6210\u6a2a\u5411\u7684  #3fa4ff\nfor a,b in zip(la,la_num):  \n    plt.text(a, b+0.05, '%.0f' % b, ha='center', va= 'bottom',fontsize=10)  \nplt.show() \n ","be815ada":"sets = datas.Usage.value_counts()\nda = [sets[i] for i in range(len(sets))]\nset_la = ['Training','PublicTest','PrivateTest']\nprint(sets)\nplt.axes(aspect=1)\nplt.title('Size of Training,PublicTest,PrivateTest sets in the image dataset')\nplt.pie(x = da,labels = set_la,autopct='%3.1f %%', shadow=True)\nplt.show()","7902d63d":"print('\u56fe\u7247\u957f\u5ea6:',len(datas.pixels[1].split()))","d4cd9b95":"time_1 = time.time()\nprint('\u6570\u636e\u8bfb\u53d6\u8017\u65f6\uff1a',round((time_1 - time_start),2),'s')","63fc115f":"train_set = datas[(datas.Usage == 'Training')] \nval_set = datas[(datas.Usage == 'PublicTest')]\ntest_set = datas[(datas.Usage == 'PrivateTest')] \nX_train = np.array(list(map(str.split, train_set.pixels)), np.float32) #, np.float32\nX_val = np.array(list(map(str.split, val_set.pixels)), np.float32) \nX_test = np.array(list(map(str.split, test_set.pixels)), np.float32) \nX_train = X_train.reshape(X_train.shape[0], 48, 48) \nX_val = X_val.reshape(X_val.shape[0],48,48) \nX_test = X_test.reshape(X_test.shape[0],48, 48) ","869cb928":"y_train = list(train_set.emotion) \ny_val = list(val_set.emotion)\ny_test = list(test_set.emotion )","61da478a":"fig = plt.figure(figsize = (10,8))\nfor i in range(len(X_train[:35])):\n    if(y_train[i] == 0 ):\n        str_la = 'Angry'\n        img = Image.fromarray(np.uint8(X_train[i]))\n    elif(y_train[i] == 1):\n        str_la = 'Disgust'\n        img = Image.fromarray(np.uint8(X_train[i]))\n    elif(y_train[i] == 2):\n        str_la = 'Fear'\n        img = Image.fromarray(np.uint8(X_train[i]))\n    elif(y_train[i] == 3):\n        str_la = 'Happy'\n        img = Image.fromarray(np.uint8(X_train[i]))\n    elif(y_train[i] == 4):\n        str_la = 'Sad'\n        img = Image.fromarray(np.uint8(X_train[i]))\n    elif(y_train[i] == 5):\n        str_la = 'Surprise'\n        img = Image.fromarray(np.uint8(X_train[i]))\n    elif(y_train[i] == 6):\n        str_la = 'Neural'\n        img = Image.fromarray(np.uint8(X_train[i]))\n    y = fig.add_subplot(5,7,i+1)\n    y.imshow(img,cmap='gray')\n    plt.title(str_la)\n    y.axes.get_xaxis().set_visible(False)\n    y.axes.get_yaxis().set_visible(False)\nplt.show()","fb0c56be":"train_preprocess = tfs.Compose([\n    tfs.ToPILImage(),#\u5c11\u4e86\u8fd9\u4e00\u884c\u5c31\u4f1a\u62a5'int' object is not iterable\n    tfs.RandomCrop(44),\n    tfs.RandomHorizontalFlip(),\n    tfs.ToTensor(),\n])\n\n\n\nval_preprocess = tfs.Compose([\n    tfs.ToPILImage(),\n    tfs.TenCrop(44),\n    tfs.Lambda(lambda crops: torch.stack([tfs.ToTensor()(crop) for crop in crops])),\n])\n\n","4ea262ea":"time_2 = time.time()\nprint('\u6570\u636e\u5904\u7406\u8017\u65f6\uff1a',round((time_2 - time_1),2),'s')","00e6f0df":"import torch\nfrom torch.utils import data\nimport numpy as np\nimport torch.optim as optim\nimport torch.nn as nn\nimport pandas as pd\nimport torch.nn.functional as F\nimport torchvision.models as models","05855f71":"class Train_Dataset(data.Dataset):#\u62ec\u53f7\u91cc\u4e00\u5b9a\u8981\u5199\u6210data.Dataset,\u5426\u5219\u4f1a\u62a5\u9519\n#       \u521d\u59cb\u5316 \n    def __init__(self,X_train,labels):\n        super(Train_Dataset,self).__init__()\n        img = []\n        label = []\n        label = labels\n        a = [train_preprocess(X_train[i])  for i in range(X_train.shape[0])]\n        img = a\n        self.img = img\n        self.label=labels\n      \n            \n    def __getitem__(self, index):\n        \n        imgs = self.img[index]\n        labels = self.label[index]\n        imgs_tensors =  imgs.type('torch.cuda.FloatTensor')\n        return imgs_tensors, labels\n        \n    \n    def __len__(self):\n        return len(self.img)","b5d32378":"class Val_Dataset(data.Dataset):#\u62ec\u53f7\u91cc\u4e00\u5b9a\u8981\u5199\u6210data.Dataset,\u5426\u5219\u4f1a\u62a5\u9519\n#       \u521d\u59cb\u5316 \n    def __init__(self,X_val,labels):\n        super(Val_Dataset,self).__init__()\n        img = []\n        label = []\n        label = labels\n        b = [val_preprocess(X_val[i])  for i in range(X_val.shape[0])]\n        img = b\n        self.img = img\n        self.label=labels\n      \n             \n    def __getitem__(self, index):\n        \n        imgs = self.img[index]\n        labels = self.label[index]\n        imgs_tensors =  imgs.type('torch.cuda.FloatTensor')\n        return imgs_tensors, labels\n        \n    \n    def __len__(self):\n        return len(self.img)","fe7c3693":"def validate_train(model,dataset,batch_size):\n    val_loader = data.DataLoader(dataset,batch_size,shuffle=True)\n    result,num = 0.0, 0\n    for images,labels in val_loader:#ctrl + \/\u591a\u884c\u6ce8\u91ca\n        images = images.cuda()\n        pre = model.forward(images)\n        pre = pre.cpu()#\u8981\u4ecegpu\u4e2d\u6362\u56decpu\u4e0a\n        pre = np.argmax(pre.data.numpy(),axis = 1)\n        labels = labels.data.numpy()\n        result += np.sum((pre == labels))\n        num += len(images)\n    acc = result \/ num\n    return acc","89dacb39":"def validate_val(model,dataset,batch_size):\n    val_loader = data.DataLoader(dataset,batch_size,shuffle=True)\n    result,num = 0.0, 0\n    for images,labels in val_loader:#ctrl + \/\u591a\u884c\u6ce8\u91ca\n        for i in range(len(images)):\n            images[i] = images[i].cuda()\n            pre = model.forward(images[i])\n            pre =pre.cpu()#\u8981\u4ecegpu\u4e2d\u6362\u56decpu\u4e0a\n            pre = np.argmax(pre.data.numpy().mean(0))\n            if pre == labels[i] :\n                result = result + 1\n        num += len(images)\n    acc = result \/ num\n    return acc","c6704e8c":"train_dataset = X_train\ntrain_labels = y_train\nVal_dataset = Val_Dataset( X_val,y_val)\nTest_dataset = Val_Dataset(X_test,y_test)\n\nbatch_size= 128\nlearning_rate = 0.001\nepochs= 20\n\n#resnet18\nresnet_historyloss = []\nresnet_historyacc = []\nresnet_historytrac = []\nresnet_historytestac = []\n\n#vgg19\nvgg19_historyloss = []\nvgg19_historyacc = []\nvgg19_historytrac = []\nvgg19_historytestac = []\n\n#multiple\nmultiple_historyloss = []\nmultiple_historyacc = []\nmultiple_historytrac = []\nmultiple_historytestac = []","4800b52c":"# def train_resnet18(train_dataset,train_labels,Val_dataset,Test_dataset,batch_size,epochs,learning_rate,momen_tum,wt_decay):\n\n    \n# #     \u6784\u5efa\u6a21\u578b\n#     resnet18 = models.resnet18()\n#     resnet18.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n#     resnet18.fc = torch.nn.Linear(in_features=512, out_features=7, bias=True)\n    \n#     model = resnet18.cuda()#\u653e\u5165gpu\n# #     \u635f\u5931\u51fd\u6570\n#     loss_function = nn.CrossEntropyLoss()\n#     loss_function =  loss_function.cuda()#\u653e\u5165gpu\n    \n# #     \u4f18\u5316\u5668\n#     optimizer = optim.SGD(model.parameters(),lr=learning_rate,momentum=momen_tum,weight_decay=wt_decay)\n\n# #     \u9010\u8f6e\u8bad\u7ec3\n#     print(\"Resnet18\u6a21\u578b\u5f00\u59cb\u8bad\u7ec3\uff01\")\n#     for epoch in range(epochs):\n# #         \u6bcf\u4e00\u8f6e\u8f93\u5165\u7684\u56fe\u7247\u90fd\u505a\u4e00\u4e9b\u53d8\u5316\uff0c\u51cf\u7f13\u8fc7\u62df\u5408\u7684\u901f\u5ea6\n#         Train_dataset = Train_Dataset(train_dataset,train_labels)\n#         train_loader = data.DataLoader(Train_dataset,batch_size,shuffle=True)\n#         loss_rate = 0\n#         model.train()#\u6a21\u578b\u8bad\u7ec3\n#         for images,labels in train_loader:#DataLoader\u7684\u5de5\u4f5c\u673a\u5236\uff1f\uff1f\uff1f\uff1f\uff1f\n# #             \u68af\u5ea6\u6e05\u96f6\n            \n#             images = images.cuda()#\u653e\u5165gpu\n#             labels = labels.cuda()#\u653e\u5165gpu\n\n#             optimizer.zero_grad()\n# #             \u5411\u524d\u4f20\u64ad\n#             output = model.forward(images)\n# #             \u8ba1\u7b97\u8bef\u5dee\n#             loss_rate = loss_function(output,labels)\n# #             \u8bef\u5dee\u7684\u53cd\u5411\u4f20\u64ad\n#             loss_rate.backward()\n# #             \u66f4\u65b0\u53c2\u6570\n#             optimizer.step()\n#         resnet_historyloss.append(loss_rate.item())\n\n# #         \u6253\u5370\u6bcf\u8f6e\u7684\u635f\u5931\n        \n       \n            \n#         model.eval()\n#         #\u8bc4\u4f30\n#         acc_train = validate_train(model, Train_dataset, batch_size)\n#         resnet_historytrac.append(acc_train)\n\n#         acc_val = validate_val(model,Val_dataset,batch_size)\n#         resnet_historyacc.append(acc_val)\n\n#         acc_test = validate_val(model,Test_dataset,batch_size)\n#         resnet_historytestac.append(acc_test)\n        \n#         if( (epoch+1) == epochs):\n#             print(\"Resnet18\u6a21\u578b\u6700\u7ec8\u6d4b\u8bd5\u7ed3\u679c\uff1a\")\n#             print('The acc_train is :',acc_train)\n#             print('The acc_val is :',acc_val)\n#             print('The acc_test is :',acc_test)\n#             print('\\n')\n\n     \n#     print(\"Resnet18\u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff01\")        \n#     return model","07b85610":"# def train_vgg19(train_dataset,train_labels,Val_dataset,Test_dataset,batch_size,epochs,learning_rate,momen_tum,wt_decay):\n\n    \n# #     \u6784\u5efa\u6a21\u578b\n#     vgg19 = models.vgg19()\n#     vgg19.features[0] = torch.nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n#     vgg19.classifier[6] = torch.nn.Linear(in_features=4096, out_features=7, bias=True)\n   \n#     model = vgg19.cuda()#\u653e\u5165gpu\n# #     \u635f\u5931\u51fd\u6570\n#     loss_function = nn.CrossEntropyLoss()\n#     loss_function =  loss_function.cuda()#\u653e\u5165gpu\n    \n# #     \u4f18\u5316\u5668\n#     optimizer = optim.SGD(model.parameters(),lr=learning_rate,momentum=momen_tum,weight_decay=wt_decay)\n\n# #     \u9010\u8f6e\u8bad\u7ec3\n#     print(\"VGG19\u6a21\u578b\u5f00\u59cb\u8bad\u7ec3\uff01\")\n#     for epoch in range(epochs):\n# #         \u6bcf\u4e00\u8f6e\u8f93\u5165\u7684\u56fe\u7247\u90fd\u505a\u4e00\u4e9b\u53d8\u5316\uff0c\u51cf\u7f13\u8fc7\u62df\u5408\u7684\u901f\u5ea6\n#         Train_dataset = Train_Dataset(train_dataset,train_labels)\n#         train_loader = data.DataLoader(Train_dataset,batch_size,shuffle=True)\n#         loss_rate = 0\n#         model.train()#\u6a21\u578b\u8bad\u7ec3\n#         for images,labels in train_loader:\n            \n#             images = images.cuda()#\u653e\u5165gpu\n#             labels = labels.cuda()#\u653e\u5165gpu\n            \n# #             \u68af\u5ea6\u6e05\u96f6\n#             optimizer.zero_grad()\n# #             \u5411\u524d\u4f20\u64ad\n#             output = model.forward(images)\n# #             \u8ba1\u7b97\u8bef\u5dee\n#             loss_rate = loss_function(output,labels)\n# #             \u8bef\u5dee\u7684\u53cd\u5411\u4f20\u64ad\n#             loss_rate.backward()\n# #             \u66f4\u65b0\u53c2\u6570\n#             optimizer.step()\n# #         \u6253\u5370\u6bcf\u8f6e\u7684\u635f\u5931    \n#         vgg19_historyloss.append(loss_rate.item())\n\n\n#         model.eval()\n#         #\u8bc4\u4f30\n#         acc_train = validate_train(model, Train_dataset, batch_size)\n#         vgg19_historytrac.append(acc_train)\n\n#         acc_val = validate_val(model,Val_dataset,batch_size)\n#         vgg19_historyacc.append(acc_val)\n\n#         acc_test = validate_val(model,Test_dataset,batch_size)\n#         vgg19_historytestac.append(acc_test)\n\n\n#         if((epoch+1) == epochs):\n            \n#             print(\"VGG19\u6a21\u578b\u6700\u7ec8\u6d4b\u8bd5\u7ed3\u679c\uff1a\")\n#             print('The acc_train is :',acc_train)\n#             print('The acc_val is :',acc_val)\n#             print('The acc_test is :',acc_test)\n#             print('\\n')\n\n#     print(\"VGG19\u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff01\")       \n#     return model","54e0c484":"# resnet18 = train_resnet18(train_dataset,train_labels,Val_dataset,Test_dataset,batch_size,epochs,learning_rate ,momen_tum=0.9,wt_decay = 5e-4)\n# torch.save(resnet18,'fer2013_resnet18_model.pkl')\n\n# vgg19 = train_vgg19(train_dataset,train_labels,Val_dataset,Test_dataset,batch_size,epochs,learning_rate ,momen_tum=0.9,wt_decay = 5e-4)\n# torch.save(vgg19,'fer2013_vgg19_model.pkl')","b0f4684b":"resnet = torch.load(\"..\/input\/fer2013-resnet18-model\/fer2013_resnet18_model.pkl\")","34e01af4":"\nvgg = torch.load(\"..\/input\/fer2013-vgg19-modelpkl\/fer2013_vgg19_model.pkl\")","5e5a0ea8":"class Multiple(nn.Module):\n    def __init__(self):\n        super(Multiple,self).__init__()        \n        \n        self.fc = nn.Sequential(\n             nn.Linear(in_features = 14,out_features = 7),\n        )\n        \n    def forward(self,x):\n        \n        #\u7ecf\u8fc7\u57fa\u6a21\u578b\u9884\u5904\u7406\n        result_1 = vgg(x)\n        result_2 = resnet(x)\n        \n        #\u62fc\u63a5\u57fa\u6a21\u578b\u5904\u7406\u540e\u7684\u7279\u5f81\n        result_1 = result_1.view(result_1.shape[0],-1)\n        result_2 = result_2.view(result_2.shape[0],-1)\n        result = torch.cat((result_1,result_2),1)\n        \n        #\u5c06\u57fa\u6a21\u578b\u5904\u7406\u540e\u7684\u7279\u5f81\u8f93\u5165\u878d\u5408\u6a21\u578b\u4e2d\n        y = self.fc(result)\n        \n        return y","2d3f1576":"def multiple_train(train_dataset,train_labels,Val_dataset,Test_dataset,batch_size,epochs,learning_rate,momen_tum,wt_decay):\n\n    \n#     \u6784\u5efa\u6a21\u578b\n    model = Multiple()\n    model = model.cuda()#\u653e\u5165gpu\n#     \u635f\u5931\u51fd\u6570\n    loss_function = nn.CrossEntropyLoss()\n    loss_function =  loss_function.cuda()#\u653e\u5165gpu\n    \n#     \u4f18\u5316\u5668\n    optimizer = optim.SGD(model.parameters(),lr=learning_rate,momentum=momen_tum,weight_decay=wt_decay)\n\n#     \u9010\u8f6e\u8bad\u7ec3\n    print(\"\u878d\u5408\u6a21\u578b\u5f00\u59cb\u8bad\u7ec3\uff01\")\n    for epoch in range(epochs):\n#         \u6bcf\u4e00\u8f6e\u8f93\u5165\u7684\u56fe\u7247\u90fd\u505a\u4e00\u4e9b\u53d8\u5316\uff0c\u51cf\u7f13\u8fc7\u62df\u5408\u7684\u901f\u5ea6\n        Train_dataset = Train_Dataset(train_dataset,train_labels)\n        train_loader = data.DataLoader(Train_dataset,batch_size,shuffle=True)\n        loss_rate = 0\n        model.train()#\u6a21\u578b\u8bad\u7ec3\n        for images,labels in train_loader:#DataLoader\u7684\u5de5\u4f5c\u673a\u5236\uff1f\uff1f\uff1f\uff1f\uff1f\n#             \u68af\u5ea6\u6e05\u96f6\n            \n            images = images.cuda()#\u653e\u5165gpu\n            labels = labels.cuda()#\u653e\u5165gpu\n\n            optimizer.zero_grad()\n#             \u5411\u524d\u4f20\u64ad\n            output = model(images)\n#             \u8ba1\u7b97\u8bef\u5dee\n            loss_rate = loss_function(output,labels)\n#             \u8bef\u5dee\u7684\u53cd\u5411\u4f20\u64ad\n            loss_rate.backward()\n#             \u66f4\u65b0\u53c2\u6570\n            optimizer.step()\n        multiple_historyloss.append(loss_rate.item())\n#         \u6253\u5370\u6bcf\u8f6e\u7684\u635f\u5931\n        \n        \n        model.eval()\n        #\u8bc4\u4f30\n        acc_train = validate_train(model, Train_dataset, batch_size)\n        multiple_historytrac.append(acc_train)\n        \n        acc_val = validate_val(model,Val_dataset,batch_size)\n        multiple_historyacc.append(acc_val)\n        \n        acc_test = validate_val(model,Test_dataset,batch_size)\n        multiple_historytestac.append(acc_test)\n\n        \n        print('After {} epochs : '.format(epoch+1))\n        print('The loss_rate is :',loss_rate.item())\n        print('The acc_train is :',acc_train)\n        print('The acc_val is :',acc_val)\n        print('The acc_test is :',acc_test)\n        print('\\n')\n    \n    print(\"\u878d\u5408\u6a21\u578b\u8bad\u7ec3\u7ed3\u675f\uff01\")   \n    \n    return model","be4223bb":"model = multiple_train(train_dataset,train_labels,Val_dataset,Test_dataset,batch_size,epochs,learning_rate ,momen_tum=0.9,wt_decay = 5e-4)\ntorch.save(model,'fer2013_multiple_model.pkl')","dded46b0":"# print(\"\u6240\u6709\u6a21\u578b\u8bad\u7ec3\u7ed3\u675f\uff01\")","7099f188":"time_3 = time.time()\nprint('\u6a21\u578b\u8bad\u7ec3\u8017\u65f6\uff1a',round((time_3 - time_2) \/ 3600,2),'h')","eb3eed3c":"mul = torch.load('fer2013_multiple_model.pkl')\n# vgg = torch.load('fer2013_vgg19_model.pkl')\n# resnet = torch.load('fer2013_resnet18_model.pkl')","398dc529":"def plots(historyloss,historyacc,historytrac,historytestac):\n    \n    epochs = range(len(historyacc))\n\n    plt.plot(epochs,historyloss,'r', label='train_loss')\n    plt.plot(epochs,historyacc,'b', label='acc_val')\n    plt.plot(epochs,historytrac,'g', label='acc_train')\n    plt.plot(epochs,historytestac,'y', label='acc_test')\n\n    plt.title('epoch and acc and loss_rate')\n    plt.xlabel('epoch')\n    plt.ylabel('acc and loss')\n    plt.legend()\n    plt.figure()\n    ","030e274a":"# print(\"resnet18\u6a21\u578b\")\n# plots(resnet_historyloss,resnet_historyacc,resnet_historytrac,resnet_historytestac)","974f7b57":"# print(\"vgg19\u6a21\u578b\")\n# plots(vgg19_historyloss,vgg19_historyacc,vgg19_historytrac,vgg19_historytestac)","1867e3e4":"print(\"\u878d\u5408\u6a21\u578b\")\nplots(multiple_historyloss,multiple_historyacc,multiple_historytrac,multiple_historytestac)","40dc6776":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport numpy as np","0fdcf4cb":"def plot_confusion_matrix(model,dataset,batch_size):\n    y_true = []\n    y_pred = []\n    label = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neural']#\u91cc\u9762\u662f\u4e2d\u6587\u4f1a\u663e\u793a\u4e0d\u51fa\u6765\uff0c\u4e0d\u77e5\u9053\u4e3a\u4ec0\u4e48\uff1f\n    tick_marks = np.array(range(len(label))) + 0.5\n    \n    val_loader = data.DataLoader(dataset,batch_size,shuffle=True)\n    for images,labels in val_loader:#ctrl + \/\u591a\u884c\u6ce8\u91ca\n        for i in range(len(images)):\n            images[i] = images[i].cuda()\n            pre = model.forward(images[i])\n            pre =pre.cpu()#\u8981\u4ecegpu\u4e2d\u6362\u56decpu\u4e0a\n            pre = np.argmax(pre.data.numpy().mean(0))\n            y_true.append(labels[i])\n            y_pred.append(pre)\n    \n    cm = confusion_matrix(y_true, y_pred)\n    np.set_printoptions(precision=2)\n    cm_normalized = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    plt.figure(figsize=(8, 6), dpi=80)\n\n    ind_array = np.arange(len(label))\n    x, y = np.meshgrid(ind_array, ind_array)\n    for x_val, y_val in zip(x.flatten(), y.flatten()):\n        c = cm_normalized[y_val][x_val]\n        if c > 0.01:\n            plt.text(x_val, y_val, \"%0.2f\" % (c,), color='red', fontsize=10, va='center', ha='center')\n    plt.gca().set_xticks(tick_marks, minor=True)\n    plt.gca().set_yticks(tick_marks, minor=True)\n    plt.gca().xaxis.set_ticks_position('none')\n    plt.gca().yaxis.set_ticks_position('none')\n    plt.grid(True, which='minor', linestyle='-')\n    plt.gcf().subplots_adjust(bottom=0.15)\n\n   \n    plt.imshow(cm_normalized, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n    xlocations = np.array(range(len(label)))\n    plt.xticks(xlocations, label, rotation=70)#\u8c03\u6574\u5e95\u4e0b\u6807\u7b7e\u7684\u65cb\u8f6c\u89d2\u5ea6\n    plt.yticks(xlocations, label)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n    plt.show()","063f0b13":"y_val = list(val_set.emotion)\ndataset = Val_Dataset( X_val,y_val)\nbatch_size = 128","4eff396b":"# print(\"vgg19\u6a21\u578b  Confusion Matrix\")\n# plot_confusion_matrix(vgg,dataset,batch_size)\n\n# print(\"resnet18\u6a21\u578b Confusion Matrix\")\n# plot_confusion_matrix(resnet,dataset,batch_size)\n\nprint(\"\u878d\u5408\u6a21\u578b Confusion Matrix\")\nplot_confusion_matrix(mul,dataset,batch_size)","319904f3":"model = torch.load('fer2013_multiple_model.pkl')","1522789c":"y_val = list(val_set.emotion)\ny_test = list(test_set.emotion )\n#y_val\u4f1a\u53d8\u7a7a\uff0c\u4e0d\u77e5\u9053\u4e3a\u4ec0\u4e48\uff0c\u6240\u4ee5\u8fd9\u91cc\u91cd\u65b0\u751f\u6210\u6807\u7b7e\u4fe1\u606f","559ed3bb":"Va_dataset = Val_Dataset( X_val,y_val)\nacc_val = validate_val(model,Va_dataset,128)\nprint('\u51c6\u786e\u7387\uff1a',acc_val)","003e2571":"def validate(model,dataset,batch_size):\n    val_loader = data.DataLoader(dataset,batch_size,shuffle=True)\n    result,num = 0.0, 0\n    y_pred = []\n    \n    for images,labels in val_loader:#ctrl + \/\u591a\u884c\u6ce8\u91ca\n        for i in range(len(images)):\n            images[i] = images[i].cuda()\n            pre = model.forward(images[i])\n            pre =pre.cpu()#\u8981\u4ecegpu\u4e2d\u6362\u56decpu\u4e0a\n            pre = np.argmax(pre.data.numpy().mean(0))\n            y_pred.append(pre)\n    return y_pred","718db518":"i = 1000\nj = 1100\nimg_val = X_val[i:j]\nlabel_val = y_val[i:j]\nim_0= []\nim_1= []\nim_2= []\nim_3= []\nim_4= []\nim_5= []\nim_6= []\nla_0 = []\nla_1 = []\nla_2 = []\nla_3 = []\nla_4 = []\nla_5 = []\nla_6 = []\nfor k in range(len(img_val)):\n    \n    if(label_val[k] == 0):\n        \n        im_0.append(img_val[k])\n        la_0.append(0)\n                \n    elif(label_val[k] == 1):\n        \n        im_1.append(img_val[k])\n        la_1.append(1)\n        \n    elif(label_val[k] ==2):\n        \n        im_2.append(img_val[k])\n        la_2.append(2)\n        \n    elif(label_val[k] ==3):\n        \n        im_3.append(img_val[k])\n        la_3.append(3)\n        \n    elif(label_val[k] ==4):\n        \n        im_4.append(img_val[k])\n        la_4.append(4)\n        \n    elif(label_val[k] ==5):\n        \n        im_5.append(img_val[k])\n        la_5.append(5)\n        \n    elif(label_val[k] ==6):\n        \n        im_6.append(img_val[k])\n        la_6.append(6)\n         ","4e469735":"x_lis = []\ny_lis = []\nx_lis.append(np.array(im_0))\nx_lis.append(np.array(im_1))\nx_lis.append(np.array(im_2))\nx_lis.append(np.array(im_3))\nx_lis.append(np.array(im_4))\nx_lis.append(np.array(im_5))\nx_lis.append(np.array(im_6))\ny_lis.append(la_0)\ny_lis.append(la_1)\ny_lis.append(la_2)\ny_lis.append(la_3)\ny_lis.append(la_4)\ny_lis.append(la_5)\ny_lis.append(la_6)","cdb36214":"labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neural']","b86bbda8":"for i in range(len(x_lis)):\n    Va_dataset = Val_Dataset( x_lis[i],y_lis[i])\n    pre = validate(model,Va_dataset,1)\n    print(labels[y_lis[i][0]])\n    print('\u5b9e\u9645\u7ed3\u679c\uff1a\\t',y_lis[i])\n    print('\u9884\u6d4b\u7ed3\u679c\u4e3a\uff1a\\t',pre)","6a42c24c":"time_end = time.time()\ntime_c= time_end - time_start \nprint('\u603b\u8017\u65f6:',round(time_c \/ 3600,2) , 'h')","8cc9acff":"\u7a0b\u5e8f\u7ed3\u675f\uff0c\u8fd0\u884c\u65f6\u95f4","36230ad9":"1.\u6570\u636e\u8bfb\u53d6\u5e76\u663e\u793a\u6570\u636e","1cb51ca4":"\u4e8c\uff1a\u6570\u636e\u5904\u7406","4c9bd909":"\u5f00\u59cb\u8ba1\u65f6","d9fa6f04":"batch_size = 128\u6548\u679c\u6700\u597d\uff0c\u5927\u4e86\uff0c\u51c6\u786e\u7387\u4e0a\u4e0d\u53bb","f87381b4":"2.\u6807\u7b7e\u6570\u636e\u5904\u7406","536b91d5":"\u516d\uff1a\u6a21\u578b\u52a0\u8f7d\u5e76\u9a8c\u8bc1","89797abe":"\u4e03\uff1a\u968f\u673a\u5355\u4e2a\u56fe\u7247\u8fdb\u884c\u6d4b\u8bd5","975c7084":"\u4e94\uff1a\u7ed8\u5236\u6df7\u6dc6\u77e9\u9635","e5aadee8":"\u4e00\uff1a\u6570\u636e\u5206\u6790","de1419a2":"4.\u6570\u636e\u589e\u5f3a","ebf48e8f":"3.\u5c55\u793a\u4e00\u4e9b\u8bad\u7ec3\u6837\u672c\uff0c\u786e\u4fdd\u6570\u636e\u6b63\u5e38\u3002","9eb7cd33":"\u56db\uff1a\u7ed8\u5236\u66f2\u7ebf\u56fe","f8aac342":"4.\u663e\u793a\u56fe\u7247\u50cf\u7d20\u4fe1\u606f","4925e65b":"\u67e5\u770b\u878d\u5408\u6a21\u578b\u7b2c\u4e00\u5c42\u7684\u6a21\u578b\u53c2\u6570\u662f\u5426\u4fdd\u6301\u4e0d\u53d8\uff08\u4fdd\u6301\u4e0d\u53d8\uff09\n\u8bad\u7ec3\u5b8c\u5c31\u4fdd\u5b58\u6a21\u578b\uff0c\u548c\u6240\u6709\u8bad\u7ec3\u5b8c\u624d\u4fdd\u5b58\u6a21\u578b\u6709\u533a\u522b\u7684\u3002\n\u8bad\u7ec3\u5b8c\u5c31\u4fdd\u5b58\u6a21\u578b\uff0c\u5982\u4f55\u4e0d\u56fa\u5b9a\u53c2\u6570\uff0c\u4f1a\u53d1\u73b0\u878d\u5408\u6a21\u578b\u4e2d\uff0c\u5148\u524d\u8bad\u7ec3\u7684\u6a21\u578b\u53c2\u6570\u53d1\u751f\u53d8\u5316\n\u6240\u6709\u6a21\u578b\u8bad\u7ec3\u5b8c\u4e00\u8d77\u4fdd\u5b58\u7684\u8bdd\uff0c\u6700\u540e\u7ed3\u679c\u4f1a\u51fa\u73b0\u6240\u6709\u6a21\u578b\u53c2\u6570\u53d8\u6210\u76f8\u540c\uff0c\u56e0\u4e3a\u5728\u878d\u5408\u6a21\u578b\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u5176\u4ed6\u6a21\u578b\u6ca1\u6709\u56fa\u5b9a\u53c2\u6570\uff0c\u7136\u540e\u5728\u878d\u5408\u6a21\u578b\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u53c8\u5c06\u53c2\u6570\u66f4\u65b0\u4e86","3a53f00e":"\u878d\u5408\u6a21\u578b\u7b2c\u4e00\u5c42\uff0c\u8bad\u7ec3\u5404\u4e2a\u57fa\u6a21\u578b\uff08VGG9\u548cresnet8\uff09","bb3ebd67":"\u591a\u6a21\u578b\u878d\u5408(vgg19 + resnet18)","1599b304":"\u52a0\u8f7d\u8bad\u7ec3\u597d\u7684\u57fa\u6a21\u578b","e5bb06a9":"2.\u663e\u793a\u8868\u60c5\u7c7b\u522b\u4fe1\u606f","812a610d":"\u6784\u5efa\u878d\u5408\u6a21\u578b\u7f51\u7edc","3494c492":"\u4e09\uff1a\u6a21\u578b\u642d\u5efa","d22b197c":"\u5206\u522b\u52a0\u8f7d\u57fa\u6a21\u578b\u548c\u878d\u5408\u6a21\u578b","7fd8c687":"\u5728\u4e00\u5b9a\u8303\u56f4\u5185\u5bf9\u6240\u6709\u7c7b\u522b\u56fe\u7247\u5206\u522b\u8fdb\u884c\u6d4b\u8bd5\u3002","f16f34e5":"3.\u663e\u793a\u6570\u636e\u96c6\u5206\u7c7b\u4fe1\u606f","14e2e86f":"1.\u56fe\u50cf\u6570\u636e\u5904\u7406"}}