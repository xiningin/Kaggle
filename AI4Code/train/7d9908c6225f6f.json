{"cell_type":{"f583cade":"code","e8f23855":"code","128448ce":"code","970c1e57":"code","2b1362e3":"code","34f8f589":"code","ff074848":"code","66863282":"code","751cffa6":"code","bae245a5":"code","e1c4c637":"code","6e3c567b":"markdown","6c2ea98d":"markdown","b925a665":"markdown"},"source":{"f583cade":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n\nimport numpy as np\nimport pandas as pd\nimport cudf\n\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\n%matplotlib inline\nimport plotly.figure_factory as ff\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nimport shap","e8f23855":"%%time\ntrain = cudf.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv', index_col=0)\ntest = cudf.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv', index_col=0)\n\nsample_submission = cudf.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv\").to_pandas()\n\nmemory_usage = train.memory_usage(deep=True) \/ 1024 ** 2\nstart_mem = memory_usage.sum()","128448ce":"feature_cols = [col for col in test.columns.tolist()]\n\ncnt_features =[]\ncat_features =[]\n\nfor col in feature_cols:\n    if train[col].dtype=='float64':\n        cnt_features.append(col)\n    else:\n        cat_features.append(col)\n        \n\ntrain[cnt_features] = train[cnt_features].astype('float32')\ntrain[cat_features] = train[cat_features].astype('uint8')\n\ntest[cnt_features] = test[cnt_features].astype('float32')\ntest[cat_features] = test[cat_features].astype('uint8')\n\nmemory_usage = train.memory_usage(deep=True) \/ 1024 ** 2\nend_mem = memory_usage.sum()\n\ntrain = train.to_pandas()\ntest = test.to_pandas()","970c1e57":"print(\"Mem. usage decreased from {:.2f} MB to {:.2f} MB ({:.2f}% reduction)\".format(start_mem, end_mem, 100 * (start_mem - end_mem) \/ start_mem))","2b1362e3":"folds = 10\ntrain[\"kfold\"] = -1\nkf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(train,train[\"target\"])):\n    train.loc[valid_indicies, \"kfold\"] = fold","34f8f589":"%%time\nfinal_test_predictions = []\nscores = []\n\nfor fold in range(folds):\n    x_train = train[train.kfold != fold].copy()\n    x_valid = train[train.kfold == fold].copy()\n    x_test  = test[feature_cols].copy()\n    \n    y_train = x_train['target']\n    y_valid = x_valid['target']\n    \n    x_train = x_train[feature_cols]\n    x_valid = x_valid[feature_cols]\n    xgb_params = {\n        'eval_metric': 'auc', \n        'objective': 'binary:logistic', \n        'tree_method': 'gpu_hist', \n        'gpu_id': 0, \n        'predictor': 'gpu_predictor', \n        'n_estimators': 10000, \n        'learning_rate': 0.01063045229441343, \n        'gamma': 0.24652519525750877, \n        'max_depth': 4, \n        'seed': 42,       \n        'min_child_weight': 366, \n        'subsample': 0.6423040816299684, \n        'colsample_bytree': 0.7751264493218339, \n        'colsample_bylevel': 0.8675692743597421, \n        'use_label_encoder': False,\n        'lambda': 0, \n        'alpha': 10\n    }\n    \n    xgb_model = XGBClassifier(**xgb_params)\n    xgb_model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], verbose=False)\n    \n    preds_train = xgb_model.predict_proba(x_train)[:,1]\n    preds_valid = xgb_model.predict_proba(x_valid)[:,1]\n    auc_train = roc_auc_score(y_train, preds_train)\n    auc = roc_auc_score(y_valid, preds_valid)\n    print(\"Fold\",fold,\", train:\", f\"{auc_train:.5f}\", \", valid:\", f\"{auc:.5f}\")\n    scores.append(auc)\n    \n    preds_test = xgb_model.predict_proba(x_test)[:,1]\n    final_test_predictions.append(preds_test)\n    \n    \nprint(\"AVG AUC:\",np.mean(scores))","ff074848":"shap_values = shap.TreeExplainer(xgb_model).shap_values(x_valid)\nshap.summary_plot(shap_values, x_valid)","66863282":"shap.dependence_plot(\"f22\", shap_values, x_valid)","751cffa6":"idx = 5\ndata_for_prediction = x_valid.iloc[idx]\ndata_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n\n\nprint(xgb_model.predict_proba(data_for_prediction_array))\n\nshap.initjs()\nexplainer = shap.TreeExplainer(xgb_model)\nshap_values = explainer.shap_values(data_for_prediction_array)\nshap.force_plot(explainer.expected_value, shap_values, data_for_prediction)","bae245a5":"labels = [f'fold {i}' for i in range(folds)]\n\nfig = ff.create_distplot(final_test_predictions, labels, bin_size=.3, show_hist=False, show_rug=False)\nfig.show()","e1c4c637":"sample_submission['target'] = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.to_csv(\"submission.csv\", index=False)","6e3c567b":"# KFold","6c2ea98d":"# SHAP Values","b925a665":"# Plot Prediction"}}