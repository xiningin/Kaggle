{"cell_type":{"cac14a36":"code","2b599c45":"code","e2c0d699":"code","81576205":"code","6432f632":"code","5ed6ac5e":"code","7fc6e86f":"code","e38a4645":"code","4561fad3":"code","0579eb11":"code","01a12f88":"code","23237068":"code","6e0cd02b":"code","def66885":"code","10de5388":"code","3b0a4303":"code","96cde39b":"code","cd21be70":"code","42000da3":"code","5005eae6":"code","9a06d43b":"code","bcee2a1c":"code","169aaa1e":"code","8c3672f5":"code","5657bf68":"code","211df808":"code","c097810b":"code","94f6190c":"code","e6252bad":"code","0750139b":"code","b0b9f42c":"code","9225db40":"code","5abc7699":"code","445ff147":"code","6456942d":"code","f0756a35":"code","9a47747e":"code","4e91d58f":"code","5c32b246":"code","0b9612ad":"code","41abe145":"code","99d7e57a":"code","4c275a89":"code","f0c90b87":"code","165e3475":"code","a91a6f67":"code","e957eb80":"code","01442bd1":"code","8634b19c":"code","81bc7ef9":"code","57f10295":"code","aae8702a":"code","421e7e02":"code","5d904fb6":"code","78afb915":"code","99b4548d":"code","56cc26a3":"code","2fc9c132":"code","6e81cd22":"code","bedb3499":"code","ebb2b159":"code","aa88ae15":"code","b36ee4d0":"code","3ac03341":"code","889307fb":"code","a979bfa4":"code","7f16cf5a":"markdown","238c1a1d":"markdown","434a342a":"markdown","195c740c":"markdown","972b2c7d":"markdown","ae3b0d73":"markdown","1562cee0":"markdown","cfe07e8b":"markdown","974c5919":"markdown","2e4addcb":"markdown","2ca1178b":"markdown","29034a5c":"markdown","2486ca84":"markdown","748834c2":"markdown","39191ed8":"markdown","b8865897":"markdown","8d6ba18c":"markdown","d58a85cd":"markdown","43c50d67":"markdown","2e85475f":"markdown","5eb5a7ec":"markdown","ed65a793":"markdown","8eed23bc":"markdown","d88c2f8c":"markdown","205a01ff":"markdown"},"source":{"cac14a36":"# loading necessary libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score, roc_auc_score, roc_curve\nfrom sklearn.linear_model import LogisticRegression  \nfrom sklearn.neighbors import KNeighborsClassifier  \nfrom sklearn.svm import SVC\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score,recall_score\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nwarnings.filterwarnings(\"ignore\", category=FutureWarning) \nwarnings.filterwarnings(\"ignore\", category=UserWarning) \n\n%config InlineBackend.figure_format = 'retina'\n\n# to display all columns and rows:\npd.set_option('display.max_columns', None); pd.set_option('display.max_rows', None);","2b599c45":"# reading the data\ndf = pd.read_csv(\"..\/input\/churn-for-bank-customers\/churn.csv\", index_col=0)","e2c0d699":"# The first 5 observation \ndf.head()","81576205":"# The size of the data set \ndf.shape","6432f632":"# Feature information\ndf.info()","5ed6ac5e":"# Descriptive statistics of the data set\ndf.describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99])","7fc6e86f":"# categorical Variables\ncategorical_variables = [col for col in df.columns if col in \"O\"\n                        or df[col].nunique() <=11\n                        and col not in \"Exited\"]\n\ncategorical_variables","e38a4645":"# Numeric Variables\nnumeric_variables = [col for col in df.columns if df[col].dtype != \"object\"\n                        and df[col].nunique() >11\n                        and col not in \"CustomerId\"]\nnumeric_variables","4561fad3":"# Frequency of classes of dependent variable\ndf[\"Exited\"].value_counts()","0579eb11":"# Customers leaving the bank\nchurn = df.loc[df[\"Exited\"]==1]","01a12f88":"# Customers who did not leave the bank\nnot_churn = df.loc[df[\"Exited\"]==0]","23237068":"# Frequency of not_churn group according to Tenure\nnot_churn[\"Tenure\"].value_counts().sort_values()","6e0cd02b":"# Frequency of churn group according to Tenure\nchurn[\"Tenure\"].value_counts().sort_values()","def66885":"# Frequency of not_churn group according to NumOfProducts\nnot_churn[\"NumOfProducts\"].value_counts().sort_values()","10de5388":"# Frequency of churn group according to NumOfProducts\nchurn[\"NumOfProducts\"].value_counts().sort_values()","3b0a4303":"# examining the HasCrCard of the not_churn group\nnot_churn[\"HasCrCard\"].value_counts()","96cde39b":"# examining the HasCrCard of the churn group\nchurn[\"HasCrCard\"].value_counts()","cd21be70":"# examining the IsActiveMember of the not_churn group\nnot_churn[\"IsActiveMember\"].value_counts()","42000da3":"# examining the IsActiveMember of the churn group\nchurn[\"IsActiveMember\"].value_counts()","5005eae6":"# Frequency of not_churn group according to Geography\nnot_churn.Geography.value_counts().sort_values()","9a06d43b":"# Frequency of churn group according to Geography\nchurn.Geography.value_counts().sort_values()","bcee2a1c":"# Frequency of not_churn group according to Gender\nnot_churn.Gender.value_counts()","169aaa1e":"# Frequency of churn group according to Gender\nchurn.Gender.value_counts()","8c3672f5":"# Let's examine the credit score of the not_churn group\nnot_churn[\"CreditScore\"].describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99])","5657bf68":"# distribution of the Credit Score for not_churn\npyplot.figure(figsize=(8,6))\npyplot.xlabel('CreditScore')\npyplot.hist(not_churn[\"CreditScore\"],bins=15, alpha=0.7, label='Not Churn')\npyplot.legend(loc='upper right')\npyplot.show()","211df808":"# Let's examine the credit score of the churn group\nchurn[\"CreditScore\"].describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99])","c097810b":"# distribution of the Credit Score for churn\npyplot.figure(figsize=(8,6))\npyplot.xlabel('CreditScore')\npyplot.hist(churn[\"CreditScore\"],bins=15, alpha=0.8, label='Churn')\npyplot.legend(loc='upper right')\npyplot.show()","94f6190c":"sns.catplot(\"Exited\", \"CreditScore\", data = df)","e6252bad":"# examining the age of the not_churn group\nnot_churn[\"Age\"].describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99])","0750139b":"# distribution of the Age for not_churn\npyplot.figure(figsize=(8,6))\npyplot.xlabel('Age')\npyplot.hist(not_churn[\"Age\"],bins=15, alpha=0.7, label='Not Churn')\npyplot.legend(loc='upper right')\npyplot.show()","b0b9f42c":"# examine the age of the churn group\nchurn[\"Age\"].describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99])","9225db40":"# distribution of the Age for not_churn\npyplot.figure(figsize=(8,6))\npyplot.xlabel('Age')\npyplot.hist(churn[\"Age\"],bins=15, alpha=0.7, label='Churn')\npyplot.legend(loc='upper right')\npyplot.show()","5abc7699":"sns.catplot(\"Exited\", \"Age\", data = df)","445ff147":"# examining the Balance of the not_churn group\nnot_churn[\"Balance\"].describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99])","6456942d":"# distribution of the Balance for not_churn\npyplot.figure(figsize=(8,6))\npyplot.xlabel('Balance')\npyplot.hist(not_churn[\"Balance\"],bins=15, alpha=0.7, label='Not Churn')\npyplot.legend(loc='upper right')\npyplot.show()","f0756a35":"# examining the Balance of the churn group\nchurn[\"Balance\"].describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99])","9a47747e":"# distribution of the Balance for churn\npyplot.figure(figsize=(8,6))\npyplot.xlabel('Balance')\npyplot.hist(churn[\"Balance\"],bins=15, alpha=0.7, label='Churn')\npyplot.legend(loc='upper right')\npyplot.show()","4e91d58f":"sns.catplot(\"Exited\", \"Balance\", data = df)","5c32b246":"# examining the EstimatedSalary of the not_churn group\nnot_churn[\"EstimatedSalary\"].describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99])","0b9612ad":"# distribution of the Balance for churn\npyplot.figure(figsize=(8,6))\npyplot.xlabel('EstimatedSalary')\npyplot.hist(not_churn[\"EstimatedSalary\"],bins=15, alpha=0.7, label='Not Churn')\npyplot.legend(loc='upper right')\npyplot.show()","41abe145":"# examining the EstimatedSalary of the churn group\nchurn[\"EstimatedSalary\"].describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99])","99d7e57a":"# distribution of the EstimatedSalary for churn\npyplot.figure(figsize=(8,6))\npyplot.xlabel('EstimatedSalary')\npyplot.hist(churn[\"EstimatedSalary\"],bins=15, alpha=0.7, label='Churn')\npyplot.legend(loc='upper right')\npyplot.show()","4c275a89":"sns.catplot(\"Exited\", \"EstimatedSalary\", data = df)","f0c90b87":"# Exited correlation matrix\nk = 10 #number of variables for heatmap\ncols = df.corr().nlargest(k, 'Exited')['Exited'].index\ncm = df[cols].corr()\nplt.figure(figsize=(10,6))\nsns.heatmap(cm, annot=True, cmap = 'viridis')","165e3475":"# # Missing Observation Analysis\ndf.isnull().sum()","a91a6f67":"# To determine the threshold value for outliers\ndef outlier_thresholds(dataframe, variable, low_quantile=0.05, up_quantile=0.95):\n    quantile_one = dataframe[variable].quantile(low_quantile)\n    quantile_three = dataframe[variable].quantile(up_quantile)\n    interquantile_range = quantile_three - quantile_one\n    up_limit = quantile_three + 1.5 * interquantile_range\n    low_limit = quantile_one - 1.5 * interquantile_range\n    return low_limit, up_limit","e957eb80":"# Are there any outliers in the variables\ndef has_outliers(dataframe, numeric_columns, plot=False):\n   # variable_names = []\n    for col in numeric_columns:\n        low_limit, up_limit = outlier_thresholds(dataframe, col)\n        if dataframe[(dataframe[col] > up_limit) | (dataframe[col] < low_limit)].any(axis=None):\n            number_of_outliers = dataframe[(dataframe[col] > up_limit) | (dataframe[col] < low_limit)].shape[0]\n            print(col, \" : \", number_of_outliers, \"outliers\")\n            #variable_names.append(col)\n            if plot:\n                sns.boxplot(x=dataframe[col])\n                plt.show()\n    #return variable_names","01442bd1":"# There is no outlier\nfor var in numeric_variables:\n    print(var, \"has \" , has_outliers(df, [var]),  \"Outliers\")","8634b19c":"# we standardize tenure with age\ndf[\"NewTenure\"] = df[\"Tenure\"]\/df[\"Age\"]\ndf[\"NewCreditsScore\"] = pd.qcut(df['CreditScore'], 6, labels = [1, 2, 3, 4, 5, 6])\ndf[\"NewAgeScore\"] = pd.qcut(df['Age'], 8, labels = [1, 2, 3, 4, 5, 6, 7, 8])\ndf[\"NewBalanceScore\"] = pd.qcut(df['Balance'].rank(method=\"first\"), 5, labels = [1, 2, 3, 4, 5])\ndf[\"NewEstSalaryScore\"] = pd.qcut(df['EstimatedSalary'], 10, labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])","81bc7ef9":"df.head()","57f10295":"# Variables to apply one hot encoding\nlist = [\"Gender\", \"Geography\"]\ndf = pd.get_dummies(df, columns =list, drop_first = True)","aae8702a":"df.head()","421e7e02":"# Removing variables that will not affect the dependent variable\ndf = df.drop([\"CustomerId\",\"Surname\"], axis = 1)","5d904fb6":"# Scale features using statistics that are robust to outliers.\ndef robust_scaler(variable):\n    var_median = variable.median()\n    quartile1 = variable.quantile(0.25)\n    quartile3 = variable.quantile(0.75)\n    interquantile_range = quartile3 - quartile1\n    if int(interquantile_range) == 0:\n        quartile1 = variable.quantile(0.05)\n        quartile3 = variable.quantile(0.95)\n        interquantile_range = quartile3 - quartile1\n        if int(interquantile_range) == 0:\n            quartile1 = variable.quantile(0.01)\n            quartile3 = variable.quantile(0.99)\n            interquantile_range = quartile3 - quartile1\n            z = (variable - var_median) \/ interquantile_range\n            return round(z, 3)\n\n        z = (variable - var_median) \/ interquantile_range\n        return round(z, 3)\n    else:\n        z = (variable - var_median) \/ interquantile_range\n    return round(z, 3)","78afb915":"new_cols_ohe = [\"Gender_Male\",\"Geography_Germany\",\"Geography_Spain\"]\nlike_num = [col for col in df.columns if df[col].dtypes != 'O' and len(df[col].value_counts()) <= 10]\ncols_need_scale = [col for col in df.columns if col not in new_cols_ohe\n                   and col not in \"Exited\"\n                   and col not in like_num]\n\nfor col in cols_need_scale:\n    df[col] = robust_scaler(df[col])","99b4548d":"df.head()","56cc26a3":"from xgboost import XGBClassifier\nX = df.drop(\"Exited\",axis=1)\ny = df[\"Exited\"]\n# Train-Test Separation\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=12345)\n# Models for Classification\nmodels = [('LR', LogisticRegression(random_state=123456)),\n          ('KNN', KNeighborsClassifier()),\n          ('CART', DecisionTreeClassifier(random_state=123456)),\n          ('RF', RandomForestClassifier(random_state=123456)),\n          ('SVR', SVC(gamma='auto',random_state=123456)),\n          ('XGB', GradientBoostingClassifier(random_state = 12345)),\n          (\"LightGBM\", LGBMClassifier(random_state=123456))]\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=123456)\n    cv_results = cross_val_score(model, X, y, cv=kfold)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","2fc9c132":"# GB Confusion Matrix\nmodel_XGB = GradientBoostingClassifier(random_state=12345)\nmodel_XGB.fit(X_train, y_train)\ny_pred = model_GB.predict(X_test)\nconf_mat = confusion_matrix(y_pred,y_test)\nconf_mat","6e81cd22":"print(\"True Positive : \", conf_mat[1, 1])\nprint(\"True Negative : \", conf_mat[0, 0])\nprint(\"False Positive: \", conf_mat[0, 1])\nprint(\"False Negative: \", conf_mat[1, 0])","bedb3499":"# Classification Report for XGB Model\nprint(classification_report(model_GB.predict(X_test),y_test))","ebb2b159":"# Auc Roc Curve\ndef generate_auc_roc_curve(clf, X_test):\n    y_pred_proba = clf.predict_proba(X_test)[:, 1]\n    fpr, tpr, thresholds = roc_curve(y_test,  y_pred_proba)\n    auc = roc_auc_score(y_test, y_pred_proba)\n    plt.plot(fpr,tpr,label=\"AUC ROC Curve with Area Under the curve =\"+str(auc))\n    plt.legend(loc=4)\n    plt.show()\n    pass","aa88ae15":"generate_auc_roc_curve(model_GB, X_test)","b36ee4d0":"# LightGBM: \nlgb_model = LGBMClassifier()\n# Model Tuning\nlgbm_params = {'colsample_bytree': 0.5,\n 'learning_rate': 0.01,\n 'max_depth': 6,\n 'n_estimators': 500}\n\nlgbm_tuned = LGBMClassifier(**lgbm_params).fit(X, y)","3ac03341":"#Let's choose the highest 4 models\n# GBM\ngbm_model = GradientBoostingClassifier()\n# Model Tuning\ngbm_params = {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1}\ngbm_tuned = GradientBoostingClassifier(**gbm_params).fit(X,y)","889307fb":"# evaluate each model in turn\nmodels = [(\"LightGBM\", lgbm_tuned),\n          (\"GB\",gbm_tuned)]\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=123456)\n    cv_results = cross_val_score(model, X, y, cv=10, scoring=\"accuracy\")\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","a979bfa4":"for name, model in models:\n        base = model.fit(X_train,y_train)\n        y_pred = base.predict(X_test)\n        acc_score = accuracy_score(y_test, y_pred)\n        feature_imp = pd.Series(base.feature_importances_,\n                        index=X.columns).sort_values(ascending=False)\n\n        sns.barplot(x=feature_imp, y=feature_imp.index)\n        plt.xlabel('De\u011fi\u015fken \u00d6nem Skorlar\u0131')\n        plt.ylabel('De\u011fi\u015fkenler')\n        plt.title(name)\n        plt.show()","7f16cf5a":"## Outliers","238c1a1d":"# Correlation Matrix","434a342a":"# 3- Modeling","195c740c":"## IsActiveMember","972b2c7d":"## Tenure","ae3b0d73":"## Missing Value","1562cee0":"## One Hot Encoding","cfe07e8b":"## HasCrCard","974c5919":"# Categorical Variables","2e4addcb":"#\u00a04- Model Tuning","2ca1178b":"## EstimatedSalary","29034a5c":"## Churn Prediction \n### A Machine Learning Model That Can Predict Customers Who Will Leave The Company\n\nThe aim is to predict whether a bank's customers leave the bank or not. If the Client has closed his\/her bank account, he\/she has left.\n\n## Dataset\n\n- **RowNumber:** corresponds to the record (row) number and has no effect on the output.\n- **CustomerId:** contains random values and has no effect on customer leaving the bank.\n- **Surname:** the surname of a customer has no impact on their decision to leave the bank.\n- **CreditScore:** can have an effect on customer churn, since a customer with a higher credit score is less likely to leave the bank.\n- **Geography:** a customer\u2019s location can affect their decision to leave the bank.\n- **Gender:** it\u2019s interesting to explore whether gender plays a role in a customer leaving the bank.\n- **Age:** this is certainly relevant, since older customers are less likely to leave their bank than younger ones.\n- **Tenure:** refers to the number of years that the customer has been a client of the bank. Normally, older clients are more loyal and less likely to leave a bank.\n- **Balance:** also a very good indicator of customer churn, as people with a higher balance in their accounts are less likely to leave the bank compared to those with lower balances.\n- **NumOfProducts:** refers to the number of products that a customer has purchased through the bank.\n- **HasCrCard:** denotes whether or not a customer has a credit card. This column is also relevant, since people with a credit card are less likely to leave the bank.\n- **IsActiveMember:** active customers are less likely to leave the bank.\n- **EstimatedSalary:** as with balance, people with lower salaries are more likely to leave the bank compared to those with higher salaries.\n- **Exited:** whether or not the customer left the bank.  (0=No,1=Yes)\n\n\n\n### The model created as a result of LightGBM hyperparameter optimization (0.867300)","2486ca84":"## Gender","748834c2":"## Feature Engineering","39191ed8":"## NumOfProducts","b8865897":"## Balance","8d6ba18c":"Report\n\n1) Churn Data Set read.\n\n2) With Exploratory Data Analysis\n\n4) During Model Buildingost\n\n5) The model created as a result of LightGBM hyperparameter optimization (AUC 0.87)","d58a85cd":"## Geography","43c50d67":"# 2- Data Preprocessing","2e85475f":"## Age","5eb5a7ec":"## Exited (Dependent Variable)","ed65a793":"# Scalling","8eed23bc":"# Numerical Variables","d88c2f8c":"## CreditScore","205a01ff":"# 1- EDA"}}