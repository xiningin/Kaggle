{"cell_type":{"a7a7ebd9":"code","978f12d1":"code","b399d675":"code","4388ed23":"code","a4c9ca51":"code","e7c231f6":"code","9645913d":"code","37668e35":"code","4f903b9f":"code","afe94bce":"code","84f53dba":"code","e6a91af0":"code","327d6232":"code","fd4063d9":"code","b9edfe34":"code","b5c8b0d0":"code","2c19c8f1":"code","407c75ae":"code","db33198f":"code","58d6bb20":"code","d7aebad1":"code","2d4a5e43":"code","a393a049":"code","7936b85b":"code","fa603d24":"code","064b7fd9":"code","8440f94e":"code","34f88257":"code","4ab66f18":"code","cd9d8b7a":"code","1aeba805":"code","06793a84":"code","8d30665f":"code","dd009c0c":"code","679329a4":"code","5d92a228":"code","601c48be":"code","91f09f65":"code","1f2ecdd6":"code","35692c06":"code","8d29d448":"code","bd23cb36":"code","a35b53fd":"code","3b548214":"code","1d4d7117":"code","abe5e40c":"code","065ac628":"code","3f6af8a8":"code","da61dd56":"code","1ce0716e":"code","7d8f51bf":"markdown","63fbf3de":"markdown","28ddf34a":"markdown","9ed871c8":"markdown","bb508405":"markdown","529c61a0":"markdown","b65975fc":"markdown","95881419":"markdown","d8282a75":"markdown","37dadb51":"markdown","e381dc1d":"markdown","f94e9134":"markdown","9343f5b0":"markdown","d71518b5":"markdown","48ba8b41":"markdown","bb4835bd":"markdown","016624f9":"markdown","534f1ced":"markdown","edd9b889":"markdown","80fe1250":"markdown","1e4e6f3e":"markdown","720ddaaf":"markdown","791322da":"markdown","e4ca5e23":"markdown"},"source":{"a7a7ebd9":"#Import Dependencies\n%matplotlib inline\n\n# Start Python Imports\nimport math, time, random, datetime\n\n# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualization \nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\n# Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize\n\n# Machine learning\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n# Let's be rebels and ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')","978f12d1":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","b399d675":"train.head()","4388ed23":"test.head()","a4c9ca51":"# To get a quick overview of train dataset\ntrain.info()","e7c231f6":"# To get a quick overview of test dataset\ntest.info()","9645913d":"train.describe()","37668e35":"# checking for null values in train dataset\ntrain.isna().sum()","4f903b9f":"# checking for null values in test dataset\ntest.isna().sum()","afe94bce":"# heatmap plot to visualize the amount of missing values in train dataset\nsns.heatmap(train.isnull(), cbar = False , \n            yticklabels = False , cmap = 'flare')","84f53dba":"# heatmap plot to visualize the amount of missing values in test dataset\nsns.heatmap(test.isnull(), cbar = False , \n            yticklabels = False , cmap = 'flare')","e6a91af0":"# Function to create count and distribution visualization\ndef plot_count_dist(data, bin_df, label_column, target_column, figsize=(20, 5), use_bin_df=False):\n    if use_bin_df: \n        fig = plt.figure(figsize=figsize)\n        plt.subplot(1, 2, 1)\n        sns.countplot(y=target_column, data=bin_df);\n        plt.subplot(1, 2, 2)\n        sns.distplot(data.loc[data[label_column] == 1][target_column], \n                     kde_kws={\"label\": \"Survived\"});\n        sns.distplot(data.loc[data[label_column] == 0][target_column], \n                     kde_kws={\"label\": \"Did not survive\"});\n    else:\n        fig = plt.figure(figsize=figsize)\n        plt.subplot(1, 2, 1)\n        sns.countplot(y=target_column, data=data);\n        plt.subplot(1, 2, 2)\n        sns.distplot(data.loc[data[label_column] == 1][target_column], \n                     kde_kws={\"label\": \"Survived\"});\n        sns.distplot(data.loc[data[label_column] == 0][target_column], \n                     kde_kws={\"label\": \"Did not survive\"});\n","327d6232":"# Lets have a look at the number of people who survived\nfig = plt.figure(figsize=(20,1))\nsns.countplot(y='Survived', data=train);\nprint(train.Survived.value_counts())","fd4063d9":"# visualize the distribution of Fare for people who survived and died\ngrid = sns.FacetGrid(train, hue='Survived', size=4, aspect=1.5)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=range(0,210,10))\ngrid.add_legend()\nplt.show()","b9edfe34":"#Replacing null values with the mean of the colummn\ntrain['Age'].fillna((train['Age'].mean()), inplace=True)\ntest['Age'].fillna((test['Age'].mean()), inplace=True)","b5c8b0d0":"# age feature dealt with; no missing age records\nsns.heatmap(train.isnull(), yticklabels = False, cbar = False, cmap = 'flare')","2c19c8f1":"#binning the age feature\nbins = [ 0, 4, 12, 18, 30, 50, 65, 100] # This is somewhat arbitrary\nage_index = (1,2,3,4,5,6,7) #('baby','child','teenager','young','mid-age','over-50','senior')\ntrain['Age'] = pd.cut(train.Age, bins, labels=age_index).astype(int)\ntest['Age'] = pd.cut(test.Age, bins, labels=age_index).astype(int)","407c75ae":"train.head()","db33198f":"# checking for null values\ntrain.SibSp.isnull().sum(axis=0), train.Parch.isnull().sum(axis=0)","58d6bb20":"# Visualise the counts of SibSp and the distribution of the values against Survived\nplot_count_dist(train,bin_df=train,label_column='Survived',target_column='SibSp',figsize=(20, 10))","d7aebad1":"#which gender has a better chane at survival\ntrain['Sex'] = np.where(train['Sex'] == 'female', 1, 0)\nfig = plt.figure(figsize=(10, 10))\nsns.distplot(train.loc[train['Survived'] == 1]['Sex'], kde_kws={'label': 'Survived'});\nsns.distplot(train.loc[train['Survived'] == 0]['Sex'], kde_kws={'label': 'Did not survive'});\n","2d4a5e43":"#missing values\ntrain.Parch.isnull().sum()","a393a049":"# Visualise the counts of Parch and the distribution of the values against Survived\nplot_count_dist(train,bin_df=train,label_column='Survived',target_column='Parch',figsize=(20, 10))","7936b85b":"# checking for null values\ntrain.Embarked.isnull().sum()","fa603d24":"sns.countplot(y='Embarked', data=train)","064b7fd9":"# dropping the null values\ntrain = train.dropna(subset=['Embarked'])\ntest = test.dropna(subset=['Embarked'])","8440f94e":"#checking for null values\nprint(train.Fare.isnull().sum())\nprint(test.Fare.isnull().sum())","34f88257":"#replacing the null value in test dataset with the median\ntest['Fare'].fillna((test['Fare'].mean()), inplace=True)","4ab66f18":"# visualizing the distribution of Fare for people who survived and died\ngrid = sns.FacetGrid(train, hue='Survived', size=4, aspect=1.5)\ngrid.map(plt.hist, 'Fare', alpha=.5, bins=range(0,210,10))\ngrid.add_legend()\nplt.show()","cd9d8b7a":"# visualizing the correlation between Fare and Survived using a scatter plot\ntrain[['Fare', 'Survived']].groupby(['Fare'],as_index=False).mean().plot.scatter('Fare','Survived')\nplt.show()\n","1aeba805":"# bin Fare into five intervals with equal amount of people\ntrain['Fare'] = pd.qcut(train.Fare,5,labels=[1,2,3,4,5]).astype(int)\ntest['Fare'] = pd.qcut(test.Fare,5,labels=[1,2,3,4,5]).astype(int)\n\n# inspect the correlation between Fare-bin and Survived\ntrain[['Fare', 'Survived']].groupby(['Fare'], as_index=False).mean()","06793a84":"# check if there is any NAN\ntrain.Cabin.isnull().sum(axis=0)","8d30665f":"# There is too many NAN values so we drop this column\ntrain = train.drop(labels=['Cabin'], axis=1)\ntest = test.drop(labels=['Cabin'],axis=1)","dd009c0c":"# heatmap plot to visualize the amount of missing values in train dataset\nsns.heatmap(train.isnull(), cbar = False , \n            yticklabels = False , cmap = 'flare')","679329a4":"# heatmap plot to visualize the amount of missing values in test dataset\nsns.heatmap(test.isnull(), cbar = False , \n            yticklabels = False , cmap = 'flare')","5d92a228":"# One hot encode the categorical columns\ndf_embarked_one_hot = pd.get_dummies(train['Embarked'], prefix='embarked')\ndf_sex_one_hot = pd.get_dummies(train['Sex'], prefix='sex')\ndf_plcass_one_hot = pd.get_dummies(train['Pclass'], prefix='pclass')\n                                   \ndf_embarked_one_hot1 = pd.get_dummies(test['Embarked'], prefix='embarked')\ndf_sex_one_hot1 = pd.get_dummies(test['Sex'], prefix='sex')\ndf_plcass_one_hot1 = pd.get_dummies(test['Pclass'], prefix='pclass')\n                                  \n\n#combining the one hot encoded columns\ntrain = pd.concat([train,df_embarked_one_hot, df_sex_one_hot,df_plcass_one_hot], axis=1)\ntest = pd.concat([test,df_embarked_one_hot1,df_sex_one_hot1,df_plcass_one_hot1], axis=1)\n","601c48be":"train=train.drop(['Pclass', 'Sex', 'Embarked'], axis=1)\ntest=test.drop(['Pclass', 'Sex', 'Embarked'], axis=1)","91f09f65":"#dropping name and ticket\ntrain=train.drop(['Name', 'Ticket'], axis=1)\ntest=test.drop(['Name', 'Ticket'], axis=1)","1f2ecdd6":"train.head()","35692c06":"X_train = train.drop([\"Survived\",'PassengerId'], axis=1)\nY_train = train[\"Survived\"]\nX_test  = test.drop(\"PassengerId\", axis=1).copy()","8d29d448":"X_train.head()","bd23cb36":"X_test.head()","a35b53fd":"#Apply RandomForestClassifier\nrandom_forest= RandomForestClassifier(n_estimators=100,\n                             max_features='auto',\n                             criterion='entropy',\n                             max_depth=10)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")","3b548214":"#Apply GradientBoostingClassifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nclf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n                                 max_depth=1, random_state=0).fit(X_train, Y_train)\ny_prediction= clf.predict(X_test)\nclf.score(X_train, Y_train)\nacc_clf = round(clf.score(X_train, Y_train) * 100, 2)\nprint(round(acc_clf,2,), \"%\")","1d4d7117":"#Apply Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nprint(round(acc_log,2,), \"%\")","abe5e40c":"# Apply Decision Tree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\n\nY_pred = decision_tree.predict(X_test)\n\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nprint(round(acc_decision_tree,2,), \"%\")","065ac628":"results = pd.DataFrame({\n    'Model': [ 'Logistic Regression', \n              'Random Forest', 'Boosting', \n              'Decision Tree'],\n    'Score': [ acc_log,\n              acc_random_forest, acc_clf,\n              acc_decision_tree]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(7)","3f6af8a8":"from sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","da61dd56":"y_preds = decision_tree.fit(X_train, Y_train).predict(X_test)\nprint(\"Score: \",decision_tree.score, 4*100, \"%\")\n","1ce0716e":"submission = pd.DataFrame({\n        \"PassengerId\": test['PassengerId'],\n        \"Survived\":  y_preds\n    })\n\nsubmission.to_csv('submission.csv', index=False)","7d8f51bf":"# Analysing the feature **Fare**","63fbf3de":"# Hence we can see all the null values are dealt with.\n\n---\n","28ddf34a":"# A peek into the dataset we loaded\n\n---\n\n","9ed871c8":"# Data Preprocessing","bb508405":"## We can see that correlation much more clear after binning the data","529c61a0":"# So we can see that Decision Tree model performed best on our dataset\n","b65975fc":"# Analysing the feature **Embarked**","95881419":"## We can see that people with younger age are more likely to survive To prevent over-fitting by using the original values, we can use intervals of age to feed the ML algorithms.","d8282a75":"## We can see **Age** and **cabin** have a lot of missing values in the testing set.\n\n---\n","37dadb51":"# Analysing the feature **SibSp**","e381dc1d":"\n\n# Importing necessary libraries\n\n\n---\n\n\n","f94e9134":"## We can see **Age** and **cabin** have a lot of missing values in the training set","9343f5b0":"# Analysing the feature **Cabin**","d71518b5":"# We can see that people with lower Fare are less likely to survive. ","48ba8b41":"## More female passengers seem to have survived.\n","bb4835bd":"# Modelling and Prediction","016624f9":"\n\n---\n\n","534f1ced":"# We can see that passengers having a lot of siblings\/spouses have less chance to survive. \n\n","edd9b889":"# Analysing the feature **Sex**","80fe1250":"# Feature Encoding","1e4e6f3e":"# Analysing the feature **Parch**","720ddaaf":"# Reading the Dataset\n\n---\n\n","791322da":"# Analysing the feature **Age**","e4ca5e23":"## We can see that small families have more chance to survive, than single."}}