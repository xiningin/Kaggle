{"cell_type":{"3c2b5789":"code","9dc223e8":"code","6e0a528d":"code","f67b879e":"code","b7bc52b4":"code","2b83ac40":"code","3f5f3652":"code","6727acdf":"code","ea738a83":"code","bfd278b4":"code","db861df3":"code","2094408b":"code","23c1e407":"code","6fa51bc1":"code","dec2faa7":"code","9264e611":"code","7932845a":"code","e4d41beb":"code","68800b45":"code","bb7309ea":"code","7321119c":"code","ccca9f70":"code","a098993e":"code","d2b24268":"code","0056f357":"code","d5fb23f0":"code","6958dddb":"markdown","fe595be4":"markdown","8df28fb6":"markdown","5faaaef0":"markdown","b9208664":"markdown","0575b884":"markdown","1ece0360":"markdown","b8c9ded4":"markdown","4f88f028":"markdown","ac75c571":"markdown","199a6b59":"markdown","14aa97a4":"markdown","514f458f":"markdown","88817107":"markdown","a661397b":"markdown","86791509":"markdown","15208f74":"markdown","b0536548":"markdown","7b19eef3":"markdown","3580bad4":"markdown","49a794d4":"markdown","02e2fbfc":"markdown","f41d8831":"markdown","b6a51690":"markdown","2803f027":"markdown","cc640505":"markdown","358b1cc6":"markdown","80a2efca":"markdown"},"source":{"3c2b5789":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport time\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score, mean_squared_error, f1_score\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9dc223e8":"train = pd.read_feather(\"..\/input\/ion-switching-expriment-lag-and-lead-features\/train_lag30.feather\")\ntest = pd.read_feather(\"..\/input\/ion-switching-expriment-lag-and-lead-features\/test_lag30.feather\")\nsample_submission = pd.read_csv(\"..\/input\/liverpool-ion-switching\/sample_submission.csv\")","6e0a528d":"print(train.shape)\nprint(test.shape)","f67b879e":"train.columns","b7bc52b4":"## Preparation\n\ndrop_cols = [\n    'time','open_channels'\n    ]\n\ntrain['open_channels'] = train['open_channels'].astype(int)\n\nX = train.drop(drop_cols, axis = 1)\nX_test = test.drop(drop_cols, axis = 1)\n\ny = train['open_channels']\n\nprint(f'Number of features = {len(X.columns)}')\n\ndel train\ndel test\ngc.collect()\n","2b83ac40":"#--------------------------------------------------------\n# Parameter Setting\n#--------------------------------------------------------\n\n# I also use 'tracking'thanks Rob!!\n#\nTOTAL_FOLDS = 5\nMODEL_TYPE = 'LGBM'\nSHUFFLE = True\nNUM_BOOST_ROUND = 2_500\nEARLY_STOPPING_ROUNDS = 50\nVERBOSE_EVAL = 500\nRANDOM_SEED = 31452\n\n##\ncalc_feature_imp = True\ncalc_perm_imp = True  ##!!\u3000Calculation takes time.\n\n# prams for validation\nparams = {\n    'learning_rate': 0.03, \n    'max_depth': -1,\n    'num_leaves': 2**8+1,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.82,\n    'bagging_freq': 0,\n    'n_jobs': 8,\n    'random_state': 31452,\n    'metric': 'rmse',\n    'objective' : 'regression'\n    }\n\nparams2 = {\n    #'eval_metric': 'rmse', #This did not work.4\/26\n    'n_estimators': 50000,\n    'early_stopping_rounds': 50\n    }\n\nparams.update(params2)\n","3f5f3652":"#--------------------------------------------------------\n# Validation\n#--------------------------------------------------------\n\nfold = StratifiedKFold(n_splits=TOTAL_FOLDS, shuffle=SHUFFLE, random_state=RANDOM_SEED)\n\ndf_feature_importance = pd.DataFrame()\noof_pred = np.ones(len(X))*-1\nmodels = []\n\n\nfor i, (train_index, valid_index) in enumerate(fold.split(X, y)):\n    print(f'Fold-{i+1} started at {time.ctime()}')\n    X_train, X_valid = X.iloc[train_index, :], X.iloc[valid_index, :]\n    y_train, y_valid = y[train_index], y[valid_index]\n    \n    model = lgb.LGBMRegressor()\n    model.set_params(**params)\n    model.fit(X_train, y_train,\n              eval_set = [(X_train, y_train),(X_valid, y_valid)],\n              verbose = 500\n              )\n    \n    y_pred_valid = model.predict(X_valid, num_iteration = model.best_iteration_)\n    y_pred_valid = np.round(np.clip(y_pred_valid, 0, 10)).astype(int)\n    f1_valid = f1_score(y_valid, y_pred_valid, average = 'macro')\n    \n    oof_pred[valid_index] = y_pred_valid\n    #len(oof_pred[oof_pred >= 0])\n    print(f'Fold-{i+1} f1 score = {f1_valid:0.5f}')\n    \n    models.append(model)\n    \n    #del X_train, y_train\n    gc.collect()\n    \n    ## Feateure Importance\n    \n    if i == 0: # for time reducing\n        if calc_feature_imp:\n            #model.feature_importances_\n            fold_importance = pd.DataFrame()\n            fold_importance['feature'] = X.columns.tolist()\n            fold_importance['fold'] = i + 1\n            fold_importance['split_importance'] = model.booster_.feature_importance(importance_type='split')\n            fold_importance['gain_importance'] = model.booster_.feature_importance(importance_type='gain')\n\n\n        if calc_feature_imp * calc_perm_imp:\n            # from corochann's notebooks\n            print(\"Calculating permutation importance... \")\n            perm = PermutationImportance(model, random_state = 1, cv = 'prefit')\n            perm.fit(X_valid, y_valid)\n            fold_importance['permutation_importance'] = perm.feature_importances_\n\n        if calc_feature_imp:\n            df_feature_importance =\\\n                pd.concat([df_feature_importance, fold_importance], axis= 0)\n\n    del X_valid, y_valid\n    gc.collect()\n    \n    break # <- !!! fold1 only, add 5\/2","6727acdf":"# Only fold1 calculation, so commented out here.\n\n#oof_f1 = f1_score(y, oof_pred, average = \"macro\")\n#print(f'f1_score_oof = {oof_f1:0.5f}')","ea738a83":"def plot_feature_importance_by3(df_feature_importance, null_imp = False):\n    \n    fig, ax = plt.subplots(1, 3, figsize = (18, 14))\n    plt.rcParams[\"font.size\"] = 15\n\n    if not null_imp:\n        col_name = [\"split_importance\", \"gain_importance\", \"permutation_importance\"]\n    else:\n        col_name = [\"p_by_split_distribution\", \"p_by_gain_distribution\", \"dummy\"]\n\n\n    for i, importance_name in enumerate(col_name):\n        try:\n            cols = (df_feature_importance[[\"feature\", importance_name]]\\\n                    .groupby(\"feature\").mean()\\\n                    .sort_values(by = importance_name, ascending=False)[:100].index)\n\n            best_features = df_feature_importance.loc[df_feature_importance.feature.isin(cols), ['feature', importance_name]]\n            sns.barplot(x = importance_name, y =\"feature\", ax=ax[i], \n                        data = best_features.sort_values(by = importance_name, ascending=False))\n            plt.tight_layout()\n            ax[i].set_title(f'{importance_name} (averaged over folds)')\n        except:\n            pass\n    plt.tight_layout()","bfd278b4":"#Name shortening\nst1 = df_feature_importance[df_feature_importance['feature'].str.contains(\"_10s\")]['feature'].str.replace(\"_10s\", \"\")\ndf_feature_importance.loc[df_feature_importance['feature'].str.contains(\"_10s\"), 'feature'] = st1\n\nplot_feature_importance_by3(df_feature_importance)","db861df3":"df_lags = df_feature_importance[df_feature_importance['feature']\\\n                      .str.startswith(\"lag\")]\nplot_feature_importance_by3(df_lags)","2094408b":"df_leads = df_feature_importance[df_feature_importance['feature']\\\n                      .str.startswith(\"lead\")]\nplot_feature_importance_by3(df_leads)","23c1e407":"#--------------------------------------------------------\n# Test Predoction and Submission\n#--------------------------------------------------------  \n\nflg_submit = False\n\nif flg_submit:\n    \n    test_preds = pd.DataFrame()\n    for i, model in enumerate(models):\n        print(f'Predictinig {i+1}th model...')\n        test_pred = model.predict(X_test, num_iteration = model.best_iteration_)\n        test_pred = np.round(np.clip(test_pred, 0, 10)).astype(int)\n        test_preds[f'Fold{i+1}'] = test_pred\n\n    sample_submission['open_channels'] = test_preds.median(axis=1).astype(int)\n    #sample_submission.open_channels.value_counts()\n\n    save_sub_name = 'submission.csv'\n\n    sample_submission.to_csv(save_sub_name,\n            index=False,\n            float_format='%0.4f')","6fa51bc1":"#dir()\ndel df_lags, df_leads, fold_importance, models, sample_submission, oof_pred\ngc.collect()","dec2faa7":"#--------------------------------------\n# Calculation null importnce\n#--------------------------------------\n\n## re-define params\n\nn_est = model.booster_.best_iteration\n\n# prams for validation\nparams_null = {\n    'learning_rate': 0.03, \n    'max_depth': -1,\n    'num_leaves': 2**8+1,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.82,\n    'bagging_freq': 0,\n    'n_jobs': 8,\n    'random_state': 31452,\n    'metric': 'rmse',\n    'objective' : 'regression'\n    }\n\nparams_null2 = {\n    #'eval_metric': 'rmse', #This did not work.4\/26\n    'n_estimators': n_est #,\n    #'early_stopping_rounds': 50\n    }\n\nparams_null.update(params_null2)\n\n\nn_round = 30 #!!\n\ndf_null_importance = pd.DataFrame()\n\nfor i in range(n_round):\n    if i % 10 == 0:\n        print(f'Calculating null importance round {i}')\n    y_null = y_train.copy().sample(frac = 1.0)\n    \n    model_null = lgb.LGBMRegressor()\n    model_null.set_params(**params_null)\n    \n    model_null.fit(X_train, y_null, \n                   eval_set = [(X_train, y_null)], #,(X_valid, y_valid)],\n                   verbose = 300)\n    \n    tmp_importance = pd.DataFrame()\n    tmp_importance['feature'] = X.columns.tolist()\n    tmp_importance['round'] = i + 1 \n    tmp_importance['split_importance'] =\\\n        model_null.booster_.feature_importance(importance_type = 'split')\n    tmp_importance['gain_importance'] =\\\n        model_null.booster_.feature_importance(importance_type = 'gain')\n    \n    df_null_importance =\\\n        pd.concat([df_null_importance, tmp_importance], axis = 0)\n\n#Name shotening\nst1 = df_null_importance[df_null_importance['feature'].str.contains(\"_10s\")]['feature'].str.replace(\"_10s\", \"\")\ndf_null_importance.loc[df_null_importance['feature'].str.contains(\"_10s\"), 'feature'] = st1","9264e611":"def plot_null_dist(df_feature_importance, df_null_importance, X_cols, imp):\n\n    fig, ax = plt.subplots(3, 4, figsize = (15, 10))\n\n    k = 0\n    for i in range(3):\n        for j in range(4):\n            try:\n                disp_col = X_cols[k]\n\n                act_imp =\\\n                    df_feature_importance[df_feature_importance['feature'] == disp_col][imp].values\n                null_imp =\\\n                    df_null_importance[df_null_importance['feature'] == disp_col][imp]\n\n                f = ax[i, j].hist(null_imp, alpha = 0.8, color = \"dodgerblue\", \n                             label = \"Null importance\")\n                y_max = np.max(f[0])\n\n                ax[i, j].plot([act_imp, act_imp], [0.0, y_max], linewidth = 7, color =\"magenta\", label = \"Real target\")\n                ax[i, j].set_title(disp_col.replace(\"_10s\", \"\"),  fontsize = 16)\n                k += 1\n            except:\n                pass\n    plt.tight_layout()\n    fig.suptitle(\"Distribution of \" + imp + \": actual(magenta), target_permutation(blue) \", fontsize=20)\n    plt.subplots_adjust(top=0.9)\n    plt.show()","7932845a":"X_cols = df_feature_importance[df_feature_importance['feature'].str.contains(\"mean|sd\")]['feature'].unique()\nimp = \"split_importance\"\nplot_null_dist(df_feature_importance, df_null_importance, X_cols, imp)","e4d41beb":"X_cols = df_feature_importance[df_feature_importance['feature'].str.contains(\"lag\")]['feature'].unique()\nimp = \"split_importance\"\nplot_null_dist(df_feature_importance, df_null_importance, X_cols, imp)","68800b45":"X_cols = df_feature_importance[df_feature_importance['feature'].str.contains(\"lead\")]['feature'].unique()\nimp = \"split_importance\"\nplot_null_dist(df_feature_importance, df_null_importance, X_cols, imp)","bb7309ea":"X_cols = df_feature_importance[df_feature_importance['feature'].str.contains(\"mean|sd\")]['feature'].unique()\nimp = \"gain_importance\"\nplot_null_dist(df_feature_importance, df_null_importance, X_cols, imp)","7321119c":"X_cols = df_feature_importance[df_feature_importance['feature'].str.contains(\"lag\")]['feature'].unique()\nimp = \"gain_importance\"\nplot_null_dist(df_feature_importance, df_null_importance, X_cols, imp)","ccca9f70":"X_cols = df_feature_importance[df_feature_importance['feature'].str.contains(\"lead\")]['feature'].unique()\nimp = \"gain_importance\"\nplot_null_dist(df_feature_importance, df_null_importance, X_cols, imp)","a098993e":"## evaluation of null_importance\n\n#df_null_p = df_null_importance.groupby('feature').agg([\"mean\", \"std\"], sort = False).reset_index()\ndf_null_mean = df_null_importance.groupby('feature', sort = False).mean().reset_index()\ndf_null_mean.drop('round', axis = 1, inplace = True)\ndf_null_mean.rename(columns = {'split_importance':'split_importance_mean', \n                               'gain_importance':'gain_importance_mean'}, inplace = True)\n\ndf_null_sd = df_null_importance.groupby('feature', sort = False).std().reset_index()\ndf_null_sd.drop('round', axis = 1, inplace = True)\ndf_null_sd.rename(columns = {'split_importance':'split_importance_sd', \n                               'gain_importance':'gain_importance_sd'}, inplace = True)\n\ndf_act = df_feature_importance.groupby('feature', sort =False).mean().reset_index()\ndf_act.drop('fold', axis = 1, inplace = True)\n\ndf_null_summary = df_act.merge(df_null_mean, how = \"left\", on = 'feature')\ndf_null_summary = df_null_summary.merge(df_null_sd, how = \"left\", on = 'feature')\ndf_null_summary['z_split'] = (df_null_summary['split_importance']-df_null_summary['split_importance_mean'])\/df_null_summary['split_importance_sd'] \ndf_null_summary['z_gain'] = (df_null_summary['gain_importance']-df_null_summary['gain_importance_mean'])\/df_null_summary['gain_importance_sd'] \n\nfrom scipy.stats import norm\ndf_null_summary['p_by_split_distribution'] = norm.cdf(x = df_null_summary['z_split'], loc = 0, scale = 1)\ndf_null_summary['p_by_gain_distribution'] = norm.cdf(x = df_null_summary['z_gain'], loc = 0, scale = 1)\n\n#df_null_summary.columns","d2b24268":"df_null_summary_tmp =\\\ndf_null_summary[~df_null_summary['feature'].str.contains(\"lag2[0-9]|lead2[0-9]|lag3[0-9]|lead3[0-9]\")]\nplot_feature_importance_by3(df_null_summary_tmp, null_imp = True)","0056f357":"df_null_summary_tmp = df_null_summary[df_null_summary['feature'].str.contains(\"lag\")]\nplot_feature_importance_by3(df_null_summary_tmp, null_imp = True)","d5fb23f0":"df_null_summary_tmp = df_null_summary[df_null_summary['feature'].str.contains(\"lead\")]\nplot_feature_importance_by3(df_null_summary_tmp, null_imp = True)","6958dddb":"### Feature importance: Lead features","fe595be4":"* Important variables in \"split\" are the almost same as permutation importance.  \n* In \"gain\", only Lag-1 \/ Lead-1,2 became important.\n\nI think the accuracy increases as the number of iterations increases. It takes more time to calculate, but I think it takes less time than permutation importance.","8df28fb6":"**Lag features**","5faaaef0":"### Quantification of null importance\nI examined how to quantify the above distribution graph and use it as feature importance.  \nHere is the method I tried.  \n* The target permutated distribution was assumed to be gaussian.  \n* The probability value was calculated by normalized actual value.  \n* p = norm.cdf(x=normalized_actual_importance_vlaue, loc=0, scale=1)","b9208664":"## Validation","0575b884":"**Without lag and lead features**","1ece0360":"### Distribution plot: Split Importance  \n\nThe red actual importance is greater than the blue null importance histogram value, and the farther away it is, the higher the feature importance is.","b8c9ded4":"### Update\nAdd null importance. In order to save the calculation time, only fold1 is calculate.  \nCorrection of typographical errors and expressions.","4f88f028":"**Lag features**","ac75c571":"### Feature importance: All features","199a6b59":"### Distribution plot: Gain Importance","14aa97a4":"**Lead features**","514f458f":"## Prediction\nNot done here.","88817107":"**Without lag and lead features**","a661397b":"## Acknowledgment\nThanks to Chris to share clean data: [Data Without Drift](https:\/\/www.kaggle.com\/cdeotte\/data-without-drift), and thanks to always amaizing works.  \nThanks to corochann for many suggestions: [Permutation importance for feature selection part1](https:\/\/www.kaggle.com\/corochann\/permutation-importance-for-feature-selection-part1)  \nThanks to Gabriel to share the study materials for modeling: [Ion Switching Advanced EDA and Prediction](https:\/\/www.kaggle.com\/gpreda\/ion-switching-advanced-eda-and-prediction)  \nThank to Rob to share really helpfull notobook: [Ion Switching - 5kfold LGBM & Tracking](https:\/\/www.kaggle.com\/robikscube\/ion-switching-5kfold-lgbm-tracking)  \nThanks to Andrew for your notobooks(I have learned a lot of things): [EDA and models](https:\/\/www.kaggle.com\/artgor\/eda-and-models) etc.  \nThanks to olivier to share notebook about null importrance:[ Feature Selection with Null Importances](https:\/\/www.kaggle.com\/ogrellier\/feature-selection-with-null-importances)","86791509":"### Feature importance: Lag features","15208f74":"## Introduction\nThis notebook is created for my experiment.  \nI want to find out how effective the lag and lead features are in predicting.  \nI would be happy if this could be useful for someone's modeling.  \n  \nThis is the my first notebook by python.  \nAfter joining kaggle, I can use python.   \nThanks to kagglers!","b0536548":"Permutation importance takes so long to calculate, so I calculate once(Fold-1) in this notebook.","7b19eef3":"## Features\nLag, lead, and minimal features have been created.  \n* Batch: 50s, 10s, 5s, 0.5s, 0.05s(does not use in model)\n* mean, sd: Applies to 5s, 0.5s, 0.05s batch.\n* signal lag: 30 features. NA fills using by first data of 10s batch.\n* signal lead: 30 features. NA fills using by last data of 10s batch.\n* rolling mean: Created at the center of the lead and lag range(61points ma). NA fills using by mean of signal. \n\n[I have created features by R.](https:\/\/www.kaggle.com\/kei96kag\/ion-switching-expriment-1-lag-and-lead-features)","3580bad4":"## CV and LB score\n\nI tried additional calculations and got the following results.  \nI did the calculation with and without lag1-6 and lead1-6 , from the result of Permutation importance.\n  \n### * 30 lag + 30 lead features + minimal features\n**CV score: 0.93589 ---> LB score: 0.937** \n### * 6 lag + 6 lead features + minimal features\n**CV score: 0.93584 ---> LB score: 0.937**\n### * 24 lag(with out lag1-6) + 24 lead(with out lead1-6) + minimal features\n**CV score: 0.93084 ---> LB score: 0.933**\n\nFor lag7 and lead7 and above, it doesn't seem to affect the accuracy with or without.  \nHowever, lag1-6 and lead1-6 seems to affect prediction accuracy.  \nIt is considered that the smaller the lag and lead value, the more it contributes to accuracy.  \nI think I will make a model by incorporating lag1-6(or less) and lead1-6(or less).","49a794d4":"### Feature importance: Lag features","02e2fbfc":"**Lead fetures**","f41d8831":"Let's display the calculated feature importances side by side.  \nI created the code to arrange graphs by referring to Andrew's notebook.","b6a51690":"It seems that all the lag and lead features are used in terms of split importance, but it seems that only lag1 and lead1 contributes to accuracy in gain importance.  \nLooking at permutation importance, it seems that there is a contribution up to about lag6 and lead6.","2803f027":"### Feature importance: Lead features","cc640505":"There was almost no difference in the score between StratifiedKFold and KFold.  \nI would also like to consider GroupKFold.","358b1cc6":"## Additional study: Null importance\nI'm going to conider[ null inmportance that olivier shared](https:\/\/www.kaggle.com\/ogrellier\/feature-selection-with-null-importances).  \nI use the only first fold data used above to reduce code creation time. \nAnd sorry for the unsophisticated code. \n  \nTarget permutation is done, and calculation is performed with lgb. The best_iteration value of the original calculation, was used as the value of n_estimator. Please refer the code.  \n\nThis notebook uses 30 rounds to save time.","80a2efca":"### Feature importance: All features"}}