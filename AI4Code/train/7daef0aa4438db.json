{"cell_type":{"6934650b":"code","0bea772d":"code","a00d93f2":"code","a4f151ec":"code","6f0827d3":"code","b5484c23":"code","04a95e5e":"code","abcef0a9":"code","c98bcef2":"code","f469231e":"code","8a163947":"code","ea538203":"code","21e21234":"code","7f4fde6f":"code","e4e664fa":"code","f90b98f9":"code","3e7dc4c9":"code","af224de8":"code","ad39a8f3":"code","6b955433":"code","e437cb5a":"code","5f0a4121":"code","9d6b3ef3":"code","59190c8f":"code","8f403fb8":"code","bd07152f":"code","be43d6b1":"code","177e6865":"code","af966fb4":"code","6f83a6b1":"code","171cb8e8":"code","49866bf9":"code","8cd576c7":"code","92b9d6f7":"code","5372dbee":"code","cf4809d2":"code","8647498c":"code","f5e95056":"code","1a5a6f68":"code","5ca0799b":"code","5347f8a9":"code","456e07ce":"code","868d219f":"code","fe2c0fce":"code","ea80ab0d":"code","f704989e":"code","426e7db5":"code","3637c505":"code","d6404c67":"code","0e40fee4":"code","98e754fe":"code","7825e3f6":"code","6eb1cd1f":"code","3cb1f534":"code","fb5b9877":"code","341420ba":"code","ca69f475":"code","5c9dc9db":"code","ec21a0d6":"code","5bfaf9a9":"code","0b436cc9":"code","60346eb8":"code","69add861":"code","474c9833":"code","05aa9708":"code","e6ceb8b8":"code","fa2ccc77":"code","47333442":"code","1c93c171":"code","c646e776":"code","e1ec4534":"code","29ab75a3":"markdown","bce54d54":"markdown","0780c292":"markdown","d0655e89":"markdown","703e4c11":"markdown","8dc522bf":"markdown","3c88affb":"markdown","1d2efe8e":"markdown","8ae90dbb":"markdown","9b5dfc08":"markdown","3fc540ca":"markdown","3e4d62c4":"markdown","3b777be1":"markdown","3efc366b":"markdown"},"source":{"6934650b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas_profiling\n%matplotlib inline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics as metrics","0bea772d":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\nsubmission_df = pd.read_csv('..\/input\/sample_submission.csv')","a00d93f2":"# Checking for outliers\nplt.scatter(train_df.GrLivArea,train_df.SalePrice)\nplt.xlabel('GrLiveArea')\nplt.ylabel('SalePrice')\nplt.title('Outliers')\nplt.show()","a4f151ec":"train_df = train_df.drop(train_df[(train_df.GrLivArea > 4000)&(train_df.SalePrice < 300000)].index,axis=0)\ntarget = train_df.SalePrice","6f0827d3":"# Checking for outliers\nplt.scatter(train_df.GrLivArea,target)\nplt.xlabel('GrLiveArea')\nplt.ylabel('SalePrice')\nplt.title('Outliers')\nplt.show()","b5484c23":"train_df.drop(['SalePrice'],axis=1,inplace=True)","04a95e5e":"# Lets concatenate Train and Test data to a dataframe df\ndf = pd.concat([train_df,test_df],axis=0)","abcef0a9":"df.head()","c98bcef2":"#Checking all the missing values in complete data\nnullValue = pd.DataFrame(df.isnull().sum())\ndf.columns.map(lambda x : print(nullValue.loc[x]))","f469231e":"train_size = train_df.shape[0]\ntest_size = test_df.shape[0]\nprint(train_size,test_size)","8a163947":"# lets fill all Na's with meaning no such feature with a common term None because we can define a function .\ndef filla(feature,typo):\n    '''typo define if the feature to be filled with none or 0'''\n    df[feature].fillna('NONE',inplace=True) if typo else df[feature].fillna(0,inplace=True)","ea538203":"# lets fill the actually missing data\ndf.LotFrontage.fillna(df.LotFrontage.median(),inplace=True)","21e21234":"# lets fill the missing data with frequent occurred value\ndf.Electrical.fillna(df.Electrical.mode()[0],inplace=True)\ndf.Exterior1st.fillna(df.Exterior1st.mode()[0],inplace=True)\ndf.Exterior2nd.fillna(df.Exterior2nd.mode()[0],inplace=True)\ndf.Functional.fillna(df.Functional.mode()[0],inplace=True)\ndf.KitchenQual.fillna(df.KitchenQual.mode()[0],inplace=True)\ndf.SaleType.fillna(df.SaleType.mode()[0],inplace=True)\ndf.Utilities.fillna(df.Utilities.mode()[0],inplace=True)","7f4fde6f":"# These are the features where missing values need to be filled with None and zeros\nNonefeatures = ['Alley','MasVnrType','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n               'FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PoolQC','Fence','MiscFeature']\nZerofeatures = ['MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtFullBath','BsmtHalfBath',\n               'BsmtUnfSF','GarageArea','GarageCars','GarageYrBlt','TotalBsmtSF']","e4e664fa":"for nonefeature in Nonefeatures:\n    filla(nonefeature,True)\nfor zerofeature in Zerofeatures:\n    filla(zerofeature,False)","f90b98f9":"df.info()","3e7dc4c9":"# Seggregating data types into two different lists\nCatefeatures = [x for x in df.columns if str(df[x].dtypes) == 'object']\nNumfeatures = [x for x in df.columns if str(df[x].dtypes) != 'object']","af224de8":"# lets check the numerical features if any variable is categorical with numeric values\n#these are the variables identified as categorical which has numerical representation\ncate_num =['MSSubClass','OverallQual','OverallCond','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath',\n'BedroomAbvGr','KitchenAbvGr','TotRmsAbvGrd','Fireplaces','GarageCars','MoSold','YrSold']\n","ad39a8f3":"# lets remove this categorical variables from Numerical features\nfor features in cate_num:\n    Numfeatures.remove(features)\n    Catefeatures.append(features)","6b955433":"# lets see the normality of the target element\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n","e437cb5a":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(target,fit=norm)\nplt.subplot(1,2,2)\nstats.probplot(target,plot=plt)\nplt.show()\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(np.sqrt(target),fit=norm)\nplt.subplot(1,2,2)\nstats.probplot(np.sqrt(target),plot=plt)\nplt.show()\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(np.cbrt(target),fit=norm)\nplt.subplot(1,2,2)\nstats.probplot(np.cbrt(target),plot=plt)\nplt.show()\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(np.log(target),fit=norm)\nplt.subplot(1,2,2)\nstats.probplot(np.log(target),plot=plt)\nplt.show()","5f0a4121":"# since the logarithm is following almost normality let take it as target variable\ntarget_ln = np.log(target)","9d6b3ef3":"# lets check the skewness in the data\nskewness = df[Numfeatures].apply(lambda x : skew(x)).sort_values(ascending=False)\nskewness_df = pd.DataFrame({'skew':skewness})\nskewness_df.head()","59190c8f":"# lets reduce the skewness by transforming using boxcox \nskewed =  skewness_df[abs(skewness_df['skew']) > 0.75]\nfrom scipy.special import boxcox1p\nskewed_features = skewed.index\nlam = 0.15\nfor fea in skewed_features:\n    df[fea] = boxcox1p(df[fea],lam)\n    ","8f403fb8":"df_cate = pd.get_dummies(df[Catefeatures],drop_first=True)","bd07152f":"#Concatenating numerical features and categorical features into a single dataframe\ndf_new = pd.concat([df[Numfeatures],df_cate],axis=1)\n","be43d6b1":"# lets divide the dataframe into test and train\ntrain_df = df_new[df_new.Id<1461]\ntest_df = df_new[df_new.Id>1460]","177e6865":"from sklearn.linear_model import RidgeCV,Ridge,ElasticNet\nfrom sklearn.model_selection import KFold, cross_val_score\nridge = Ridge(alpha=10)\nridge.fit(train_df, target_ln)\nnp.sqrt(-cross_val_score(ridge, train_df, target_ln, cv=5, scoring=\"neg_mean_squared_error\")).mean()","af966fb4":"y_pred = ridge.predict(train_df)\nresid = target_ln - y_pred\nmean_resid = resid.mean()\nstd_resid = resid.std()\nz = (resid - mean_resid) \/ std_resid\nz = np.array(z)\noutliers1 = np.where(abs(z) > abs(z).std() * 3)[0]\noutliers1","6f83a6b1":"plt.figure(figsize=(6, 6))\nplt.scatter(target_ln, y_pred)\nplt.scatter(target_ln.iloc[outliers1], y_pred[outliers1])\nplt.plot(range(10, 15), range(10, 15), color=\"red\")","171cb8e8":"er = ElasticNet(alpha=0.001, l1_ratio=0.58)\ner.fit(train_df, target_ln)\nnp.sqrt(-cross_val_score(ridge, train_df, target_ln, cv=5, scoring=\"neg_mean_squared_error\")).mean()","49866bf9":"y_pred = er.predict(train_df)\nresid = target_ln - y_pred\nmean_resid = resid.mean()\nstd_resid = resid.std()\nz = (resid - mean_resid) \/ std_resid\nz = np.array(z)\noutliers2 = np.where(abs(z) > abs(z).std() * 3)[0]\noutliers2","8cd576c7":"plt.figure(figsize=(6, 6))\nplt.scatter(target_ln, y_pred)\nplt.scatter(target_ln.iloc[outliers2], y_pred[outliers2])\nplt.plot(range(10, 15), range(10, 15), color=\"red\")","92b9d6f7":"outliers = []\nfor i in outliers1:\n    for j in outliers2:\n        if i == j:\n            outliers.append(i)","5372dbee":"train = pd.concat([train_df,target],axis=1)\ntrain = train.drop(outliers)\ntarget = train.SalePrice\ntarget_ln = np.log(target)","cf4809d2":"train_df = train.drop(['SalePrice','Id'],axis=1)\ntest_df.drop(['Id'],axis=1,inplace=True)","8647498c":"# Lets Build model to predict the value\n#lets start with ensemble randomforest\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(criterion='mse')","f5e95056":"#param = {\n#    'max_depth':[2,3,5,7,9],\n#    'n_estimators':[100,200,300,400,500,600,700,750,800]\n#}\n#rf_cv = GridSearchCV(estimator=rf,param_grid=param,verbose=True)","1a5a6f68":"trainx,testx,trainy,testy = train_test_split(train_df,target_ln,test_size=0.2)","5ca0799b":"rf_cv = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=9,\n           max_features='auto', max_leaf_nodes=None,\n           min_impurity_decrease=0.0, min_impurity_split=None,\n           min_samples_leaf=1, min_samples_split=2,\n           min_weight_fraction_leaf=0.0, n_estimators=750, n_jobs=None,\n           oob_score=False, random_state=None, verbose=0, warm_start=False)\nrf_cv.fit(trainx,trainy)","5347f8a9":"#print(rf_cv.best_estimator_)\n#print('_'*40)\n#print(rf_cv.best_params_)","456e07ce":"train_predict = rf_cv.predict(trainx)\ntest_predict = rf_cv.predict(testx)","868d219f":"print(np.sqrt(metrics.mean_squared_error(trainy,train_predict)))\nprint(np.sqrt(metrics.mean_squared_error(testy,test_predict)))","fe2c0fce":"trains_predict = rf_cv.predict(train_df)","ea80ab0d":"sns.distplot(target_ln,color='green')\nsns.distplot(trains_predict,color='red')\nplt.show()","f704989e":"submission_df.head()","426e7db5":"#now fitting the model on entire train data\n#rf_cv = GridSearchCV(estimator=rf,param_grid=param,verbose=True)\nrf_cv_1 = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=9,\n           max_features='auto', max_leaf_nodes=None,\n           min_impurity_decrease=0.0, min_impurity_split=None,\n           min_samples_leaf=1, min_samples_split=2,\n           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=None,\n           oob_score=False, random_state=10, verbose=0, warm_start=False)\nrf_cv_1.fit(train_df,target_ln)","3637c505":"#rf_cv.best_estimator_","d6404c67":"test_df_predict = rf_cv_1.predict(test_df)","0e40fee4":"submission_df['SalePrice'] = np.exp(test_df_predict)","98e754fe":"submission_df.to_csv('House_sale_submission.csv',index=False)","7825e3f6":"from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler,StandardScaler\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","6eb1cd1f":"#Validation function\nn_folds = 5\nkfolds = KFold(n_splits=10, shuffle=True, random_state=10)\ndef cv_rmse(model, X=train_df):\n    rmse = np.sqrt(-cross_val_score(model, X, target_ln, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)","3cb1f534":"alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]","fb5b9877":"lasso = make_pipeline(RobustScaler(),LassoCV(max_iter=1e7,alphas=alphas2,random_state=10,cv=kfolds))","341420ba":"ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))","ca69f475":"elasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))","5c9dc9db":"svr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,))","ec21a0d6":"#param_gradient = {\n#    'learning_rate' :[0.01,0.03,0.05,0.06],\n#    'n_estimators':[1000,2000,3000,3500],\n#    'max_depth':[3,4,5],\n#    'min_samples_leaf':[5,10,15,20],\n#    'min_samples_split':[5,10,15]\n#}\n#gbr_cv = GridSearchCV(estimator=GradientBoostingRegressor(max_features='sqrt',loss='huber'),\n#                      param_grid=param_gradient,\n#                     verbose=True)\n#gbr_cv.fit(train_df,target_ln)\n#gbr = GradientBoostingRegressor(n_estimators=3000, \n#                                learning_rate=0.05, \n#                                max_depth=4, \n#                                max_features='sqrt', \n#                                min_samples_leaf=15, \n#                                min_samples_split=10, \n#                                loss='huber', \n#                                random_state =42) ","5bfaf9a9":"gbr = GradientBoostingRegressor(n_estimators=3000, \n                                learning_rate=0.05, \n                                max_depth=4, \n                                max_features='sqrt', \n                                min_samples_leaf=15, \n                                min_samples_split=10, \n                                loss='huber')\n   ","0b436cc9":"#gbr_cv.best_params_","60346eb8":"lightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       )","69add861":"xgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)","474c9833":"stack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","05aa9708":"score = cv_rmse(ridge)\nprint(\"Ridge: \",score.mean(), score.std())\n\nscore = cv_rmse(lasso)\nprint(\"LASSO: \",score.mean(), score.std() )\n\nscore = cv_rmse(elasticnet)\nprint(\"elastic \",score.mean(), score.std() )\n\nscore = cv_rmse(svr)\nprint(\"SVR: \",score.mean(), score.std() )\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm:\",score.mean(), score.std() )\n\nscore = cv_rmse(gbr)\nprint(\"gbr_cv: \",score.mean(), score.std() )\n\nscore = cv_rmse(xgboost)\nprint(\"xgboost: \",score.mean(), score.std())","e6ceb8b8":"X=train_df\ny = target_ln\nprint('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(y))\n\nprint('elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y)\n\nprint('Lasso')\nlasso_model_full_data = lasso.fit(X, y)\n\nprint('Ridge')\nridge_model_full_data = ridge.fit(X, y)\n\nprint('Svr')\nsvr_model_full_data = svr.fit(X, y)\n\nprint('GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)\n\nprint('xgboost')\nxgb_model_full_data = xgboost.fit(X, y)\n\nprint('lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)","fa2ccc77":"# the coeffiecents are more or less are scores of the respective models.\ndef blend_models_predict(X):\n    return ((0.1 * elastic_model_full_data.predict(X)) + \\\n            (0.05 * lasso_model_full_data.predict(X)) + \\\n            (0.1 * ridge_model_full_data.predict(X)) + \\\n            (0.1 * svr_model_full_data.predict(X)) + \\\n            (0.1 * gbr_model_full_data.predict(X)) + \\\n            (0.15 * xgb_model_full_data.predict(X)) + \\\n            (0.1 * lgb_model_full_data.predict(X)) + \\\n            (0.3 * stack_gen_model.predict(np.array(X))))","47333442":"print('RMSLE score on train data:')\nprint(np.sqrt(metrics.mean_squared_error(y, blend_models_predict(X))))","1c93c171":"stacked_sales = np.floor(np.expm1(blend_models_predict(test_df)))","c646e776":"submission_df['SalePrice'] = stacked_sales","e1ec4534":"submission_df.to_csv('stacked_prediction.csv',index=False)","29ab75a3":"- seems like now the skewness got corrected little bit","bce54d54":"- Lasson Regression","0780c292":"- there is some skewness in the distribution","d0655e89":"##### above gives me a score of 0.146 RMSE and position of 2638","703e4c11":"- with large space and less price is something mischiveous \n- removing all the outliers is not recommended it may have impact on the output so lets make the model robust and regularize","8dc522bf":"### Imputing Missing values","3c88affb":"- Lets check if there is a mismatch in datatype and also seggregate them into numerical and categorical variables","1d2efe8e":"#### As per the explanation from the document provided \n- there are few columns in which NAN holds a meaning and  lets replace them with proper meaning\n- LotFrontage   :   this is a numeric field so lets fill it with the median\n- Alley         :   if NA then the meaning is no access lets fill it with No access\n- MasVnrType    :   if NA then its none type\n- MasVnrArea    :   Since this is an area numeric value if Na then we can fill it with 0\n- BsmtQual      :   fill with Nobase\n- BsmtCond      :   fill with Nobase\n- BsmtExposure  :   fill with Nobase\n- BsmtFinSF1    :   fill with zero\n- BsmtFinSF2    :   fill with zero\n- BsmtFinType1  :   fill with nobase\n- BsmtFinType2  :   fill with nobase\n- BsmtFullBath  :   fill with zero\n- BsmtHalfBath  :   fill with zero\n- BsmtUnfSF     :   fill with zero\n- Electrical    :   Since there is no meaning for NA lets fill \/it with most occurence ie mode\n- Exterior1st   :   fill with Mode(frequent) or predictive\n- Exterior2nd   :   fill with Mode(frequent) or predictive\n- Functional    :   fill with Mode(frequent) or predictive\n- GarageArea    :   Fill with zero\n- GarageCars    :   fill with zero\n- FireplaceQu   :   fill with no fireplace\n- GarageType    :   fill with no garage\n- GarageYrBlt   :   lets use predictive imputing here\n- GarageFinish  :   fill with no garage\n- GarageQual    :   fill with No garage\n- GarageCond    :   fill with no garage\n- PoolQC        :   fill with no pool\n- Fence         :   fill with no fence\n- MiscFeature   :   fill none feature\n- KitchenQual   :   fill with Mode(frequent) or predictive\n- SaleType      :   fill with Mode\n- TotalBsmtSF   :   fill with zero\n- Utilities     :   fill with Mode\n\n","8ae90dbb":"lets find the outliers using ridg and elastic algorithms\n","9b5dfc08":"#### Lets check the normality of the target variable","3fc540ca":"# lets try other models","3e4d62c4":"the points where the two algorithms predict poor results are  considered as outliers","3b777be1":"- to remove larger values of outlier RobustScaler is much usefull","3efc366b":"Lets use Elastic regression"}}