{"cell_type":{"fde60d5f":"code","576f5b42":"code","b3f7ed36":"code","882321ea":"code","e3784bb9":"code","5b91e6ae":"code","2a9c03a8":"code","912f83d7":"code","1094db73":"code","e7a4fbf6":"code","e9511e5f":"code","24bf950a":"code","6933119e":"code","e73705a6":"code","2ca5fe2d":"code","b456116f":"code","0638fe7f":"code","926a33a7":"code","28231ec5":"code","91ea774a":"code","3a07475d":"code","a9e69767":"code","45d1fc0b":"code","50bafe40":"code","8c7832cc":"code","89b90f3c":"code","ba5d8a6c":"code","8e2d37a8":"code","e09f281e":"code","4a288927":"code","0fa5b969":"code","7bf0dd85":"code","42718c1b":"markdown","a3fcc710":"markdown","3d275bc3":"markdown","e1d9b0eb":"markdown","93f9b815":"markdown","8320d7f6":"markdown","a20749d0":"markdown","67fe9b9a":"markdown","41f542c6":"markdown","2911eb09":"markdown"},"source":{"fde60d5f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom lightgbm import LGBMClassifier ## Add for XGBoost, CatBoost as well\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfrom sklearn.metrics import roc_auc_score\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\n\nimport optuna\nfrom optuna.samplers import TPESampler\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","576f5b42":"df_train = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')","b3f7ed36":"print(df_train.info())\nprint('*****')\nprint(df_test.info())","882321ea":"print(df_train.shape)\nprint(df_test.shape)","e3784bb9":"features = [col for col in df_train.columns if 'f' in col]","5b91e6ae":"print(features)","2a9c03a8":"import seaborn as sns\n\nsns.countplot(x = 'target', data = df_train)","912f83d7":"import matplotlib.pyplot as plt\n\nfor idx, feature in enumerate(features):\n    plt.hist(df_train[feature], bins=30, alpha=0.5, label='Train set')\n    plt.hist(df_test[feature], bins=30, alpha=0.5, label='Test set')\n    plt.title(feature + \" Train\/Test\")\n    plt.xlabel(feature)\n    plt.ylabel('Frequency')\n\n    plt.legend()\n    plt.show()","1094db73":"print(df_train[features].isna().sum().sum())\nprint(df_test[features].isna().sum().sum())","e7a4fbf6":"X = df_train[features].copy()\ny = df_train['target'].copy()\n\nx_test = df_test[features].copy()","e9511e5f":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX[features] = scaler.fit_transform(X[features])\nx_test[features] = scaler.transform(x_test[features])","24bf950a":"from lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\n\n### Hyperparameter Optimization Usinf optuna\n\ndef lgb_objective(trial):\n    params = {\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'n_estimators': trial.suggest_int(\"n_estimators\", 64, 8192),\n        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-3, 0.25, log=True),\n        'num_leaves': trial.suggest_int(\"num_leaves\", 20, 3000),\n        'max_depth': trial.suggest_int(\"max_depth\", 3, 12),\n        'feature_fraction': trial.suggest_float(\"feature_fraction\", 0.1, 1.0),\n        'min_gain_to_split' : trial.suggest_int('min_gain_to_split', 0, 15),\n        'min_data_in_leaf' : trial.suggest_int(\"min_data_in_leaf\", 100, 1000),\n        'lambda_l1': trial.suggest_loguniform(\"lambda_l1\", 1e-8, 100.0),\n        'lambda_l2': trial.suggest_loguniform(\"lambda_l2\", 1e-8, 100.0),\n        'bagging_fraction' : trial.suggest_float(\"bagging_fraction\", 0, 0.8),\n        'bagging_freq' : trial.suggest_int(\"bagging_freq\", 1, 100),\n        'seed': 42,\n        'deterministic': True,\n        'metric' : 'auc',\n        'verbose':-1\n    }\n    \n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 42)\n    \n    model = LGBMClassifier(**params)\n    model.fit(X_train, y_train,\n             eval_set = [(X_train, y_train), (X_val, y_val)],\n              early_stopping_rounds = 100,\n              eval_metric = 'auc',\n              verbose = 0\n            )\n    pred_val = model.predict(X_val)\n    \n    return roc_auc_score(y_val, pred_val)","6933119e":"sampler = TPESampler(seed = 42)\nstudy = optuna.create_study(study_name = 'lgbm_hpo',direction = 'maximize',sampler = sampler)\n\nstudy.optimize(lgb_objective, n_trials = 10)","e73705a6":"lgbmparams = study.best_params","2ca5fe2d":"RANDOM_SEED = 42\nn_splits = 5\nskf = StratifiedKFold(n_splits = n_splits, shuffle = True, random_state = RANDOM_SEED)\n\ntest_preds_lgb = []\nmean_auc = 0\n\nmodel = LGBMClassifier(**lgbmparams)\nbest_lgb_model = None\nbest_roc_score_lgb = 0\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    X_train, X_val = X.loc[train_idx], X.loc[val_idx]\n    y_train, y_val = y.loc[train_idx], y.loc[val_idx]\n    \n    model.fit(X_train, y_train,\n             verbose = 0,\n             eval_set = [(X_val, y_val)],\n             eval_metric = 'auc',\n             early_stopping_rounds = 100)\n   \n    y_pred = model.predict(X_val)\n    score = roc_auc_score(y_val, y_pred)\n    mean_auc += score\n    \n    \n    if score>best_roc_score_lgb:\n        best_roc_score_lgb = score\n        best_lgb_model = model\n    \n    print(f\"Fold {fold}'s score: {score}\")\n        \n    test_preds_lgb.append(model.predict(x_test))\n\nprint(\"==========================================\")\nprint(f\"Mean auc of all folds: {mean_auc \/ n_splits}\")","b456116f":"feature_impt=pd.DataFrame()\nfeature_impt['features']=best_lgb_model.feature_name_\nfeature_impt['importance']=best_lgb_model.feature_importances_\n\nfeature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (20,25))\nsns.barplot(x=feature_impt['importance'],y=feature_impt['features'],data=feature_impt);","0638fe7f":"#final_lgb = pd.DataFrame()\n#final_lgb['id'] = df_test['id']\n#final_lgb['target'] = np.mean(test_preds_lgb, axis = 0)","926a33a7":"#final_lgb.to_csv('final_lgb', index=False)","28231ec5":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nimport xgboost as xgb\n\ndef xgb_objective(trial):\n    params = {\n        'boosting_type': 'gbtree',\n        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-3, 0.25, log=True),\n        'verbose':0,\n        'gamma':trial.suggest_float(\"gamma\", 1.0, 10),\n        'max_depth': trial.suggest_int(\"max_depth\", 3, 12),\n        'lambda' : trial.suggest_loguniform(\"lambda\", 1e-8, 100.0),\n        'alpha' : trial.suggest_float(\"alpha\", 1.0, 10),\n        'eval_metric' : 'auc',\n        'seed': 42,\n        'objective': 'binary:hinge',\n        'deterministic': True\n    }\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 42)\n    \n    model = xgb.XGBClassifier(**params)\n    model.fit(X_train, y_train,\n              eval_set = [(X_train, y_train), (X_val, y_val)],\n              early_stopping_rounds = 100,\n              eval_metric = 'auc',\n              verbose = 0\n            )\n    pred_val = model.predict(X_val)\n    \n    return roc_auc_score(y_val, pred_val)","91ea774a":"sampler = TPESampler(seed = 42)\nstudy = optuna.create_study(study_name = 'xgb_hpo',direction = 'maximize',sampler = sampler)\n\nstudy.optimize(xgb_objective, n_trials = 5)","3a07475d":"xgbparams = study.best_params","a9e69767":"print(xgbparams)","45d1fc0b":"from sklearn.model_selection import StratifiedKFold\n\nRANDOM_SEED = 42\nn_splits = 5\nskf = StratifiedKFold(n_splits = n_splits, shuffle = True, random_state = RANDOM_SEED)\n\ntest_preds_xgb = []\nmean_auc = 0\n\nmodel = xgb.XGBClassifier(**xgbparams)\nbest_xgb_model = None\nbest_roc_score_xgb = 0\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    X_train, X_val = X.loc[train_idx], X.loc[val_idx]\n    y_train, y_val = y.loc[train_idx], y.loc[val_idx]\n    \n    model.fit(X_train, y_train,\n             verbose = 0,\n             eval_set = [(X_val, y_val)],\n             eval_metric = 'auc',\n             early_stopping_rounds = 100)\n    \n    y_pred = model.predict(X_val)\n    score = roc_auc_score(y_val, y_pred)\n    mean_auc += score\n    \n    \n    if score>best_roc_score_xgb:\n        best_roc_score_xgb = score\n        best_xgb_model = model\n    \n    print(f\"Fold {fold}'s score: {score}\")\n        \n    test_preds_xgb.append(model.predict(x_test))\n\nprint(\"==========================================\")\nprint(f\"Mean auc of all folds: {mean_auc \/ n_splits}\")","50bafe40":"feature_impt=pd.DataFrame(list(best_xgb_model.get_booster().get_fscore().items()),\ncolumns=['feature','importance']).sort_values('importance', ascending=False)\nfeature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (20,25))\nsns.barplot(x=feature_impt['importance'],y=feature_impt['feature'],data=feature_impt);","8c7832cc":"submission = pd.DataFrame()\nsubmission['id'] = df_test['id']\nsubmission['target'] = np.mean(test_preds_xgb, axis = 0)","89b90f3c":"submission.to_csv('submission', index=False)","ba5d8a6c":"#from sklearn.model_selection import StratifiedKFold\n#from sklearn.metrics import roc_auc_score\n\n#import catboost as catb\n\n#def catb_objective(trial):\n#    params = {\n#        'iterations' : trial.suggest_int(\"iterations\", 50,100),\n#        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-3, 0.25),\n#        'depth': trial.suggest_int(\"depth\", 3, 12),\n#        'boosting_type': 'Plain',\n#        'objective': 'CrossEntropy',\n#        'random_seed': 42,\n#        'eval_metric' : 'AUC',\n#        'bootstrap_type': 'Bernoulli',\n#        'logging_level': None,\n#        'verbose':-1\n#    }\n    \n#    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 42)\n    \n#    model = catb.CatBoostClassifier(**params)\n#    model.fit(X_train, y_train,\n#              eval_set = [(X_val, y_val)],\n#              early_stopping_rounds = 100,\n#              verbose = 0\n#            )\n#    pred_val = model.predict(X_val)\n    \n#    return roc_auc_score(y_val, pred_val)","8e2d37a8":"#sampler = TPESampler(seed = 42)\n#study = optuna.create_study(study_name = 'catb_hpo',direction = 'maximize',sampler = sampler)\n\n#study.optimize(catb_objective, n_trials = 10)","e09f281e":"#catbparams = study.best_params","4a288927":"#RANDOM_SEED = 42\n#n_splits = 5\n#skf = StratifiedKFold(n_splits = n_splits, shuffle = True, random_state = RANDOM_SEED)\n\n#test_preds_catb = []\n#mean_auc = 0\n\n#model = catb.CatBoostClassifier(**catbparams)\n#best_catb_model = None\n#best_roc_score_catb = 0\n\n#for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n#    X_train, X_val = X.loc[train_idx], X.loc[val_idx]\n#    y_train, y_val = y.loc[train_idx], y.loc[val_idx]\n    \n#    model.fit(X_train, y_train,\n#             verbose = 0,\n#             eval_set = [(X_val, y_val)],\n#             early_stopping_rounds = 100)\n    \n#    y_pred = model.predict(X_val)\n#    score = roc_auc_score(y_val, y_pred)\n#    mean_auc += score\n    \n    \n#    if score>best_roc_score_catb:\n#        best_roc_score_catb = score\n#        best_catb_model = model\n    \n#    print(f\"Fold {fold}'s score: {score}\")\n        \n#    test_preds_catb.append(model.predict(x_test))\n\n#print(\"==========================================\")\n#print(f\"Mean auc of all folds: {mean_auc \/ n_splits}\")","0fa5b969":"#feature_impt=pd.DataFrame()\n#feature_impt['features']=best_catb_model.feature_name_\n#feature_impt['importance']=best_catb_model.feature_importances_\n\n#feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\n#plt.figure(figsize = (20,25))\n#sns.barplot(y=feature_impt['features'],data=feature_impt);","7bf0dd85":"#final_catb = pd.DataFrame()\n#final_catb['id'] = df_test['id']\n#final_catb['target'] = np.mean(test_preds_catb, axis = 0)","42718c1b":"### Implementing LGBM Classifier","a3fcc710":"## Observing the test data","3d275bc3":"## Observing the distribution of data in the train and test set","e1d9b0eb":"No missing values in train and test data. That is one less thing to worry about","93f9b815":"### Implementing XGB Classifier","8320d7f6":"### Let's have an Overview of the Data","a20749d0":"## Conclusion\nThe distribution of data with target = 0 and target = 1 made are the same. This is one less thing to worry about :)","67fe9b9a":"Uncomment the following lines to implement CATBoost model.","41f542c6":"## Conclusion\nThe train and test data distribution are similar. This is great!!!","2911eb09":"### Implementing CATBoost Classifier"}}