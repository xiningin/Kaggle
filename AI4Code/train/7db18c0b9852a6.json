{"cell_type":{"85ba3751":"code","47ab602b":"code","4bd6181f":"code","afa6a20c":"code","63afa116":"code","3a02df8e":"code","250d25a8":"code","e82d2a5a":"code","8d454242":"code","6d9cb990":"code","31b2b8b3":"code","378c69d5":"code","b1eb67d4":"code","b48b77e2":"code","234fd7f9":"code","0971a1b1":"code","6e6262e0":"code","faa820c9":"code","78e147f2":"code","5c4ff41c":"code","6efc63f0":"code","78097459":"code","06fd716f":"code","dd4aaa88":"code","02861968":"code","0bd408da":"code","649e40fa":"code","96624daa":"code","00120218":"code","13f12739":"code","2e0ef1f5":"code","da5419e8":"code","5286c87d":"code","1f5dbca3":"code","d81be489":"code","6d84476f":"code","597ee588":"code","46adf098":"code","af3cc761":"code","b4a1a3bc":"code","b513dc50":"code","29b31015":"markdown","f6c642aa":"markdown","7c0fdb9c":"markdown","76c45b54":"markdown","0189b776":"markdown","0caa483b":"markdown","ee758924":"markdown","f8a063c9":"markdown","57c04c90":"markdown","9754658a":"markdown","4963ba53":"markdown","aac6c17e":"markdown","28fdf54a":"markdown","59fd9f6d":"markdown","d593ca31":"markdown","fae8a9a2":"markdown","82d6a606":"markdown","a4479c97":"markdown","50f8efa4":"markdown","f0b218e7":"markdown","9c534051":"markdown","327a3919":"markdown","57e601c3":"markdown","c9fdab25":"markdown","dc8dd46b":"markdown","e126a7b3":"markdown","832cdace":"markdown","3aafd1cb":"markdown","835a06b9":"markdown","44bf042b":"markdown","19651361":"markdown","df7fd199":"markdown","0474ef6d":"markdown","ae51cc59":"markdown","71c10a2e":"markdown","6815bc83":"markdown","77525a31":"markdown"},"source":{"85ba3751":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas_profiling as pp\nimport matplotlib.style as style \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os","47ab602b":"df_trans = pd.read_csv('..\/input\/train_transaction.csv', index_col='TransactionID')\ndf_test_trans = pd.read_csv('..\/input\/test_transaction.csv', index_col='TransactionID')\n\ndf_id = pd.read_csv('..\/input\/train_identity.csv', index_col='TransactionID')\ndf_test_id = pd.read_csv('..\/input\/test_identity.csv', index_col='TransactionID')\n","4bd6181f":"df_train = df_trans.merge(df_id, how='left', left_index=True, right_index=True)\ndf_test = df_test_trans.merge(df_test_id, how='left', left_index=True, right_index=True)\n\nprint(df_train.shape)\nprint(df_test.shape)\ndel df_trans,df_test_trans,df_id,df_test_id ","afa6a20c":"features = [c for c in df_train.columns if c not in ['isFraud']] #basic features","63afa116":"card_features=['card1', 'card2', 'card3', 'card4', 'card5', 'card6']\nc_features=['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14']\nd_features=[ 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15']\nm_features=[ 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']\nv1_features=[ 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50']\nv2_features=['V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100']\nv3_features=[  'V101', 'V102', 'V103', 'V104', 'V105', 'V106', 'V107', 'V108', 'V109', 'V110', 'V111', 'V112', 'V113', 'V114', 'V115', 'V116', 'V117', 'V118', 'V119', 'V120', 'V121', 'V122', 'V123', 'V124', 'V125', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V132', 'V133', 'V134', 'V135', 'V136', 'V137', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150']\nv4_features=[ 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170','V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200']\nv5_features=['V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250',]\nv6_features=[ 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298', 'V299', 'V300']\nv7_features=['V301', 'V302', 'V303', 'V304', 'V305', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339']\nid_features=[ 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38']\nother_features=['TransactionDT', 'TransactionAmt', 'ProductCD' ,'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain', 'DeviceType', 'DeviceInfo']","3a02df8e":"FF=[card_features,c_features,d_features,m_features,v1_features,v2_features,v3_features,v4_features,v5_features,v6_features,v7_features,id_features,other_features]\n","250d25a8":"df_train.fillna(-999,inplace=True)\ndf_test.fillna(-999,inplace=True)\nfrom sklearn import preprocessing\n\nfor f in df_train[features]:\n    if df_train[f].dtype=='object' or df_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df_train[f].values) + list(df_test[f].values))\n        df_train[f] = lbl.transform(list(df_train[f].values))\n        df_test[f] = lbl.transform(list(df_test[f].values)) ","e82d2a5a":"# From kernel https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\nX_train = reduce_mem_usage(df_train)\nX_test = reduce_mem_usage(df_test)","8d454242":"t0 = X_train.loc[X_train['isFraud'] == 0]\nt1 = X_train.loc[X_train['isFraud'] == 1]","6d9cb990":"def card(df1, df2, label1, label2, features):\n    i = 0\n    style.use('ggplot')\n    plt.figure()\n    fig, ax = plt.subplots(3,2,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(3,2,i)\n        sns.distplot(df1[feature], hist=False,label=label1)\n        sns.distplot(df2[feature], hist=False,label=label2)\n        plt.xlabel(feature, fontsize=12)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=12, pad=6)\n        plt.tick_params(axis='y', which='major', labelsize=12,pad=6)\n    plt.show();\n\ncard(t0, t1, '0', '1', FF[0])            ","31b2b8b3":"def c_ff(df1, df2, label1, label2, features):\n    i = 0\n    style.use('ggplot')\n    plt.figure()\n    fig, ax = plt.subplots(7,2,figsize=(18,50))\n\n    for feature in features:\n        i += 1\n        plt.subplot(7,2,i)\n        sns.distplot(df1[feature], hist=False,label=label1)\n        sns.distplot(df2[feature], hist=False,label=label2)\n        plt.xlabel(feature, fontsize=12)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=12, pad=6)\n        plt.tick_params(axis='y', which='major', labelsize=12,pad=6)\n    plt.show();\n\nc_ff(t0, t1, '0', '1', FF[1])            ","378c69d5":"def d_ff(df1, df2, label1, label2, features):\n    i = 0\n    style.use('ggplot') \n    plt.figure()\n    fig, ax = plt.subplots(5,3,figsize=(18,22))\n    \n    for feature in features:\n        i += 1\n        plt.subplot(5,3,i)\n        sns.distplot(df1[feature], hist=False,label=label1)\n        sns.distplot(df2[feature], hist=False,label=label2)\n        plt.xlabel(feature, fontsize=13)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=12, pad=6)\n        plt.tick_params(axis='y', which='major', labelsize=12,pad=6)\n    plt.show();\n\nd_ff(t0, t1, '0', '1', FF[2])            ","b1eb67d4":"def m_ff(df1, df2, label1, label2, features):\n    i = 0\n    style.use('ggplot')\n    plt.figure()\n    fig, ax = plt.subplots(9,1,figsize=(20,42))\n\n    for feature in features:\n        i += 1\n        plt.subplot(9,1,i)\n        sns.distplot(df1[feature], hist=False,label=label1)\n        sns.distplot(df2[feature], hist=False,label=label2)\n        plt.xlabel(feature, fontsize=12)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=12, pad=6)\n        plt.tick_params(axis='y', which='major', labelsize=12, pad=6)\n    plt.show();\n\nm_ff(t0, t1, '0', '1', FF[3])            ","b48b77e2":"def id_ff(df1, df2, label1, label2, features):\n    i = 0\n    style.use('ggplot')\n    plt.figure()\n    fig, ax = plt.subplots(19,2,figsize=(30,60))\n\n    for feature in features:\n        i += 1\n        plt.subplot(19,2,i)\n        sns.distplot(df1[feature], hist=False,label=label1)\n        sns.distplot(df2[feature], hist=False,label=label2)\n        plt.ylabel(feature, fontsize=12)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=12, pad=6)\n        plt.tick_params(axis='y', which='major', labelsize=12, pad=6)\n    plt.show();\n\nid_ff(t0, t1, '0', '1', FF[11])            ","234fd7f9":"def o_ff(df1, df2, label1, label2, features):\n    i = 0\n    style.use('ggplot')\n    plt.figure()\n    fig, ax = plt.subplots(11,1,figsize=(30,52))\n\n    for feature in features:\n        i += 1\n        plt.subplot(11,1,i)\n        sns.distplot(df1[feature], hist=False,label=label1)\n        sns.distplot(df2[feature], hist=False,label=label2)\n        plt.xlabel(feature, fontsize=12)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=12, pad=4)\n        plt.tick_params(axis='y', which='major', labelsize=12,pad=4)\n    plt.show();\n\no_ff(t0, t1, '0', '1', FF[12])            ","0971a1b1":"sns.set(rc={'figure.figsize':(10,10)})\ncorr=df_train[FF[0]+['isFraud']].corr()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True,cmap=\"YlGnBu\")\n","6e6262e0":"sns.set(rc={'figure.figsize':(18,18)})\ncorr=df_train[FF[1]+['isFraud']].corr()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True,cmap=\"YlGnBu\")\n","faa820c9":"sns.set(rc={'figure.figsize':(17,17)})\ncorr=df_train[FF[2]+['isFraud']].corr()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True,cmap=\"YlGnBu\")\n","78e147f2":"sns.set(rc={'figure.figsize':(12,12)})\ncorr=df_train[FF[3]+['isFraud']].corr()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True,cmap=\"YlGnBu\")\n","5c4ff41c":"sns.set(rc={'figure.figsize':(40,40)})\ncorr=df_train[FF[4]+['isFraud']].corr()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True,cmap=\"YlGnBu\")\n","6efc63f0":"sns.set(rc={'figure.figsize':(40,40)})\ncorr=df_train[FF[5]+['isFraud']].corr()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True,cmap=\"YlGnBu\")","78097459":"sns.set(rc={'figure.figsize':(40,40)})\ncorr=df_train[FF[6]+['isFraud']].corr()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True,cmap=\"YlGnBu\")","06fd716f":"sns.set(rc={'figure.figsize':(40,40)})\ncorr=df_train[FF[7]+['isFraud']].corr()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True,cmap=\"YlGnBu\")","dd4aaa88":"sns.set(rc={'figure.figsize':(40,40)})\ncorr=df_train[FF[8]+['isFraud']].corr()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True,cmap=\"YlGnBu\")","02861968":"sns.set(rc={'figure.figsize':(40,40)})\ncorr=df_train[FF[9]+['isFraud']].corr()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True,cmap=\"YlGnBu\")","0bd408da":"sns.set(rc={'figure.figsize':(40,40)})\ncorr=df_train[FF[10]+['isFraud']].corr()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True,cmap=\"YlGnBu\")","649e40fa":"sns.set(rc={'figure.figsize':(50,50)})\ncorr=df_train[FF[11]+['isFraud']].corr()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True,cmap=\"YlGnBu\")","96624daa":"sns.set(rc={'figure.figsize':(17,17)})\ncorr=df_train[FF[12]+['isFraud']].corr()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True,cmap=\"YlGnBu\")\n","00120218":"sns.set(rc={'figure.figsize':(10,10)})\ncorr=df_train[FF[0]+['isFraud']].cov()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True, annot_kws={\"size\": 10})\n","13f12739":"sns.set(rc={'figure.figsize':(18,18)})\ncorr=df_train[FF[1]+['isFraud']].cov()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True ,annot_kws={\"size\": 10})\n","2e0ef1f5":"sns.set(rc={'figure.figsize':(19,19)})\ncorr=df_train[FF[2]+['isFraud']].cov()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True, annot_kws={\"size\": 9})\n","da5419e8":"sns.set(rc={'figure.figsize':(12,12)})\ncorr=df_train[FF[3]+['isFraud']].cov()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True, annot_kws={\"size\": 10})\n","5286c87d":"sns.set(rc={'figure.figsize':(50,50)})\ncorr=df_train[FF[4]+['isFraud']].cov()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True, annot_kws={\"size\":7 })\n","1f5dbca3":"sns.set(rc={'figure.figsize':(50,50)})\ncorr=df_train[FF[5]+['isFraud']].cov()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True, annot_kws={\"size\": 7})","d81be489":"sns.set(rc={'figure.figsize':(40,40)})\ncorr=df_train[FF[6]+['isFraud']].cov()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True, annot_kws={\"size\": 8})","6d84476f":"sns.set(rc={'figure.figsize':(40,40)})\ncorr=df_train[FF[7]+['isFraud']].cov()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True, annot_kws={\"size\": 6})","597ee588":"sns.set(rc={'figure.figsize':(40,40)})\ncorr=df_train[FF[8]+['isFraud']].cov()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True, annot_kws={\"size\": 6})","46adf098":"sns.set(rc={'figure.figsize':(40,40)})\ncorr=df_train[FF[9]+['isFraud']].cov()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True, annot_kws={\"size\":6})","af3cc761":"sns.set(rc={'figure.figsize':(40,40)})\ncorr=df_train[FF[10]+['isFraud']].cov()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True, annot_kws={\"size\": 6})","b4a1a3bc":"sns.set(rc={'figure.figsize':(50,50)})\ncorr=df_train[FF[11]+['isFraud']].cov()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True,annot_kws={\"size\": 10})","b513dc50":"sns.set(rc={'figure.figsize':(17,17)})\ncorr=df_train[FF[12]+['isFraud']].cov()\nplt.figure() \nax = sns.heatmap(corr,linewidths=.5, annot=True,annot_kws={\"size\": 10})\n","29b31015":"## D 1-15","f6c642aa":"## V 201-250","7c0fdb9c":"## C 1-14","76c45b54":"## D 1-15","0189b776":"## Other","0caa483b":"## V 251-300","ee758924":"## M 1-9","f8a063c9":"## Other ","57c04c90":"## V 151-200","9754658a":" ## <div style=\"text-align: left\">Simple EDA IEEE : Vesta Fraud Data <\/div> \n\n![](https:\/\/cdn1.imggmi.com\/uploads\/2019\/8\/7\/884ef7fecc277f2c58396ed766d3d569-full.png)\n\n<br>\n<div style=\"text-align: left\">\nVesta Corporation provided the dataset for this competition. Vesta Corporation is the forerunner in guaranteed e-commerce payment solutions. Founded in 1995, Vesta pioneered the process of fully guaranteed card-not-present (CNP) payment transactions for the telecommunications industry. Since then, Vesta has firmly expanded data science and machine learning capabilities across the globe and solidified its position as the leader in guaranteed ecommerce payments. Today, Vesta guarantees more than $18B in transactions annually.\n\nIn this competition, you\u2019ll benchmark machine learning models on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. You also have the opportunity to create new features to improve your results.<\/div>","4963ba53":"## C 1-14","aac6c17e":"## Card 1-6","28fdf54a":"## D 1-15","59fd9f6d":"## V 301-339","d593ca31":"## iD 1-38","fae8a9a2":"## V 101-150","82d6a606":" # What is Correlation?\n\nVariables within a dataset can be related for lots of reasons.\n\nFor example:\n\n* One variable could cause or depend on the values of another variable.\n* One variable could be lightly associated with another variable.\n* Two variables could depend on a third unknown variable.\n\nIt can be useful in data analysis and modeling to better understand the relationships between variables. The statistical relationship between two variables is referred to as their correlation.\n\nA correlation could be positive, meaning both variables move in the same direction, or negative, meaning that when one variable\u2019s value increases, the other variables\u2019 values decrease. Correlation can also be neural or zero, meaning that the variables are unrelated.\n\n1. > **Positive Correlation:** both variables change in the same direction.\n2. > **Neutral Correlation:** No relationship in the change of the variables.\n3. > **Negative Correlation:** variables change in opposite directions.\n\nThe performance of some algorithms can deteriorate if two or more variables are tightly related, called multicollinearity. An example is linear regression, where one of the offending correlated variables should be removed in order to improve the skill of the model.\n\nWe may also be interested in the correlation between input variables with the output variable in order provide insight into which variables may or may not be relevant as input for developing a model.\n\nThe structure of the relationship may be known, e.g. it may be linear, or we may have no idea whether a relationship exists between two variables or what structure it may take. Depending what is known about the relationship and the distribution of the variables, different correlation scores can be calculated.\n\n","a4479c97":"## Card 1-6","50f8efa4":"## iD 1-38","f0b218e7":"## Card 1-6","9c534051":"## V 51-100","327a3919":"## M 1-9","57e601c3":"## V 201-250","c9fdab25":"## V 151-200","dc8dd46b":"## V 301-339","e126a7b3":"## V 1-50","832cdace":"## V 101-150","3aafd1cb":"## M 1-9","835a06b9":"## ID 1-38","44bf042b":"# Distribution plots of features\n\n-999 value is injected for Nan values","19651361":"## V 51-100","df7fd199":"## V 1-50","0474ef6d":"# Heatmaps of Covariance\n\nIn probability, covariance is the measure of the joint probability for two random variables. It describes how the two variables change together.","ae51cc59":"## Other","71c10a2e":"## V 251-300","6815bc83":"## C 1-14","77525a31":"\n<div style=\"text-align: center; color:gold\"> **To be continue & UPVOTE IF YOU LIKE** <\/div> \n"}}