{"cell_type":{"dcd86feb":"code","785193c2":"code","26f17a51":"code","b3728213":"code","af9da711":"code","f7613566":"code","9bc2a317":"code","17c35455":"code","18348cc8":"code","d7e8c4e4":"code","084c9199":"code","accf5592":"code","fd9d70b6":"code","e6cd7ee4":"code","eeac4773":"code","1448e1c3":"code","6d940b11":"code","e44fbd06":"code","6a2bc7bc":"code","3ff465aa":"code","44bd86b0":"code","d6c9eb6f":"code","31bc8da0":"code","edeb17db":"code","d843bb2e":"code","3d0b9890":"code","94fb3fdf":"code","1b04425e":"code","1129ab51":"code","46a06785":"code","03ae11d2":"code","a635d87b":"code","ab2facf0":"code","d75699c3":"code","abcf8152":"code","2c3f62cc":"code","8084fc6c":"code","4f550e82":"code","2cb04a6c":"code","31c8cdce":"code","10fcbf3b":"code","1110b20c":"code","2fdee470":"code","d86c4634":"code","d27b148a":"code","c7b81281":"code","acd53d03":"code","0c02b98f":"code","501e9dee":"code","10a15a07":"code","65ff006a":"code","20990c34":"code","86795dcf":"code","9044673d":"code","5139a189":"code","491c2ab9":"code","3e0c6859":"code","50e2d73e":"code","e2ddd500":"code","c09650e0":"code","625fe039":"code","71914e9c":"code","e8c6e5d9":"code","0b7f9ba9":"code","ad95eba0":"code","5b064959":"code","b149e689":"code","926d30d7":"code","3e0a1f52":"code","15edfb6c":"code","272b6ffd":"code","927bd32e":"code","d243a776":"code","877bc665":"code","e093963d":"code","519c2297":"code","ca4ad2f0":"code","6387f6f8":"code","c52bcc54":"code","44c37671":"code","12f49db0":"code","48330c3d":"code","2ddaedae":"code","2325abd1":"code","4fef96f4":"code","ba0c86fd":"code","63b6ea15":"code","2efd256d":"code","8aae1c24":"code","ebdcf0f5":"code","94fe0d21":"code","ddb4f0c2":"code","2f5bcbec":"code","72d5c81b":"code","17646097":"code","95821cc8":"code","1fc073fd":"code","5497cf0d":"code","433d3d91":"code","3c98980e":"code","adc0f3f9":"code","f57c82f5":"code","f2d96983":"markdown","72dfa210":"markdown","dce64fae":"markdown","cc4ddeff":"markdown","d94bf515":"markdown","bed7ec9f":"markdown","169e7d8b":"markdown","31c78d8d":"markdown","dcb01f54":"markdown","b97572f4":"markdown","ab6fbb18":"markdown","eaa6ba85":"markdown","d1688169":"markdown","35363a18":"markdown","74ec1d43":"markdown","fe11ba04":"markdown","6786fc56":"markdown","6654d496":"markdown","10e87661":"markdown","b504e0ad":"markdown","f33476f6":"markdown","a692162b":"markdown","e5e8cd6f":"markdown","fe594436":"markdown","ba934e81":"markdown","a1c460b9":"markdown","ce77ea6b":"markdown","6e34d4de":"markdown","2f3b36a1":"markdown","0f76d18a":"markdown","437bad9b":"markdown","035bbf2d":"markdown","33e9b974":"markdown","86d809c4":"markdown","a5760643":"markdown","9d81c542":"markdown","bcf766a5":"markdown","987e55ed":"markdown","d3aa4de8":"markdown","9824ceea":"markdown","f8f29fda":"markdown","ad117973":"markdown","68bc5693":"markdown","09c26847":"markdown","d91acada":"markdown","ad14efab":"markdown","7d2fa986":"markdown","020b5c9b":"markdown","bdd72177":"markdown","6b2451f3":"markdown","985e26ef":"markdown","1e6d8bcd":"markdown","f6567dde":"markdown","a42913f0":"markdown","ffff13de":"markdown","ef1665ff":"markdown","35248b42":"markdown","4b49ce22":"markdown","bd77a440":"markdown","93d762cf":"markdown","111006a5":"markdown","94a1e67c":"markdown","d9bf1fa6":"markdown","31378000":"markdown","2eb8cb44":"markdown","9701c764":"markdown","253c5757":"markdown","07d96259":"markdown","c856ef52":"markdown","21a91f4f":"markdown","07a10475":"markdown","58583d67":"markdown","20bf8309":"markdown","0463f8c8":"markdown","477f57dc":"markdown","e78cb0eb":"markdown","370ceaf5":"markdown","209fcee1":"markdown","7158934b":"markdown","72d4473e":"markdown","870ed95c":"markdown","2b254c96":"markdown"},"source":{"dcd86feb":"import numpy as np \nimport pandas as pd\n\npd.set_option('display.max_rows', 200)\npd.set_option('display.max_columns', 200)","785193c2":"# Data file name\nfilename = \"..\/input\/seattle-sdot-collisions-data\/Collisions.csv\"\n\n# Read the whole data file\ndf = pd.read_csv(filename, low_memory=False)\n\n## ---------------------------------------------------------------------------------------------------------------\n## Read first maxrows of data\n## ---------------------------------------------------------------------------------------------------------------\n\n## maxrows = 20000\n## df = pd.read_csv(filename, nrows=nrows, skiprows=0, low_memory=False)\n\n## ---------------------------------------------------------------------------------------------------------------\n## Read percent_rows % of the data rows\n## ---------------------------------------------------------------------------------------------------------------\n\n#percent_rows = 0.50  # 20% of the lines\n## keep the header, then take only 20% of lines\n## if random from [0,1] interval is greater than 0.01 the row will be skipped\n\n#df = pd.read_csv(filename,header=0, \n#         skiprows=lambda i: i>0 and np.random.random() > percent_rows, low_memory=False)\n\n# shuffle the DataFrame rows \n#df = df.sample(frac = 1) ","26f17a51":"#!pip install -q pandas_profiling\n\n#from pandas_profiling import ProfileReport\n\n# Generate profile report\n# profile = ProfileReport(df, title=\"Data Profile Report\", explorative=True)\n# profile.to_notebook_iframe()","b3728213":"# Convert INCDTTM to date type\n\ndf['INCDTTM'] = pd.to_datetime(df['INCDTTM'], errors='coerce')\n\n# Extract month, weekday, hour information\n\ndf['Month']=df['INCDTTM'].dt.month\ndf['Weekday']=df['INCDTTM'].dt.weekday\ndf['Hour']=df['INCDTTM'].dt.hour","af9da711":"df.drop(['INCDATE', 'INCDTTM'], axis=1, inplace=True)","f7613566":"# Unique ID columns are not predictors, hence drop:\n# OBJECTID, INCKEY, COLDETKEY, INTKEY, SEGLANEKEY, CROSSWALKKEY\n\ndf.drop(['OBJECTID', 'INCKEY', 'COLDETKEY', 'INTKEY', 'SEGLANEKEY', 'CROSSWALKKEY'], axis=1, inplace=True)","9bc2a317":"# Undefined Columns:\n# X, Y, EXCEPTRSNCODE, EXCEPTRSNDESC, REPORTNO, STATUS, SDOTCOLNUM\n\ndf.drop(['X', 'Y', 'EXCEPTRSNCODE', 'REPORTNO', 'STATUS', 'SDOTCOLNUM'], axis=1, inplace=True)","17c35455":"# Drop columns having descriptions corresponding to codes:\n# EXCEPTRSNDESC, SEVERITYDESC, SDOT_COLDESC, ST_COLDESC\n\ndf.drop(['EXCEPTRSNDESC', 'SEVERITYDESC', 'SDOT_COLDESC', 'ST_COLDESC', 'LOCATION'], axis=1, inplace=True)","18348cc8":"df.drop(['SDOT_COLCODE'], axis=1, inplace=True)","d7e8c4e4":"# Export the intermediate semi-processed data file\nfilename = \"Collisions_100_initial_processing_before_EDA.csv\"\ndf.to_csv(filename,index=False)","084c9199":"# Read the intermediate semi-processed data file\nfilename = \"Collisions_100_initial_processing_before_EDA.csv\"\ndf = pd.read_csv(filename)","accf5592":"import matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import cm","fd9d70b6":"df['SEVERITYCODE'].value_counts(normalize=True, dropna=False).round(5)","e6cd7ee4":"df['SEVERITYCODE'].replace('0', np.nan, inplace=True)","eeac4773":"df.dropna(axis=0, how='any',thresh=None, subset=['SEVERITYCODE'], inplace=True)","1448e1c3":"df['SEVERITYCODE'].replace('3', '4', inplace=True)\ndf['SEVERITYCODE'].replace('2b', '3', inplace=True)","6d940b11":"df['SEVERITYCODE'] =  df['SEVERITYCODE'].astype('int64')\ndf['SEVERITYCODE'].value_counts(normalize=True, ascending=False,dropna=False).round(5)","e44fbd06":"countseverity = df.SEVERITYCODE.unique()\ncount_by_severity=[]\nfor i in df.SEVERITYCODE.unique():\n    count_by_severity.append(df.loc[df.SEVERITYCODE == i, 'SEVERITYCODE'].count())\nfig, ax = plt.subplots(figsize=(5,5))\nplt.title('Count of Accidents by Severity', y=1.05)\nax.set(xlabel='Severity Code', ylabel='Count')\nsns.barplot(countseverity, count_by_severity)","6a2bc7bc":"plt.figure(figsize=(10,5))\nsns.countplot(x='Hour', hue='SEVERITYCODE', data=df, palette=\"Set1\")\nplt.legend(loc='best', prop={'size': 10})\nplt.title('Count of Accidents by Hour', y=1.05)\nplt.show()","3ff465aa":"plt.figure(figsize=(10,5))\nsns.countplot(x='Weekday', hue='SEVERITYCODE', data=df, palette=\"Set1\")\nplt.legend(loc='best', prop={'size': 10})\nplt.title('Count of Accidents by Weekday', y=1.05)\nplt.show()","44bd86b0":"plt.figure(figsize=(10,5))\nsns.countplot(x='Month', hue='SEVERITYCODE', data=df, palette=\"Set1\")\nplt.legend(loc='best', prop={'size': 10})\nplt.title('Count of Accidents by Month', y=1.05)\nplt.show()","d6c9eb6f":"df['WEATHER'].value_counts(dropna=False, ascending=False)","31bc8da0":"df.replace({'WEATHER' : {np.nan : 'Unknown'}}, inplace=True)","edeb17db":"plt.figure(figsize=(30,5))\nsns.countplot(x='WEATHER', hue='SEVERITYCODE', data=df, palette=\"Set1\")\nplt.title('Count of Accidents by Weather Condition', size=10, y=1.05)\nplt.legend(loc='best', prop={'size': 10})\nplt.show()","d843bb2e":"df['ADDRTYPE'].value_counts(normalize=True, ascending=False, dropna=False).round(5)","3d0b9890":"df.replace({'ADDRTYPE' : {np.nan : 'Unknown'}}, inplace=True)","94fb3fdf":"plt.figure(figsize=(10,5))\nsns.countplot(x='ADDRTYPE', hue='SEVERITYCODE', data=df, palette=\"Set1\")\nplt.title('Count of Accidents by Address Type', size=10, y=1.05)\nplt.legend(loc='best', prop={'size': 10})\nplt.show()","1b04425e":"df['COLLISIONTYPE'].value_counts(normalize=True, ascending=False,dropna=False).round(5)","1129ab51":"df.replace({'COLLISIONTYPE' : {np.nan : 'Unknown'}}, inplace=True)","46a06785":"plt.figure(figsize=(15,5))\nsns.countplot(x='COLLISIONTYPE', hue='SEVERITYCODE', data=df, palette=\"Set1\")\nplt.title('Count of Accidents by Collision Type', size=10, y=1.05)\nplt.legend(loc='best', prop={'size': 10})\nplt.show()","03ae11d2":"df['JUNCTIONTYPE'].value_counts(normalize=True, ascending=False,dropna=False).round(5)","a635d87b":"df.replace({'JUNCTIONTYPE' : {np.nan : 'Unknown'}}, inplace=True)","ab2facf0":"df.replace({'JUNCTIONTYPE' : {'Mid-Block (not related to intersection)': 'Mid-Block (Not Intersect)', \n                              'At Intersection (intersection related)': 'At Intersection (Intersect)',\n                              'Mid-Block (but intersection related)': 'Mid-Block (Intersect)',\n                              'At Intersection (but not related to intersection)': 'At Intersection (Intersect)'}}, inplace=True)","d75699c3":"plt.figure(figsize=(25,5))\nsns.countplot(x='JUNCTIONTYPE', hue='SEVERITYCODE', data=df, palette=\"Set1\")\nplt.title('Count of Accidents by Junction Type', size=10, y=1.05)\nplt.legend(loc='best', prop={'size': 10})\nplt.show()","abcf8152":"df['UNDERINFL'].value_counts(normalize=True, ascending=False, dropna=False)","2c3f62cc":"df.replace({'UNDERINFL' : {np.nan : 'Unknown', '0': 'N', '1': 'Y'}}, inplace=True)","8084fc6c":"df['UNDERINFL'].value_counts(normalize=True, ascending=False, dropna=False)","4f550e82":"underinfluence = df.UNDERINFL.unique()\ncount_by_underinfluence=[]\nfor i in df.UNDERINFL.unique():\n    count_by_underinfluence.append(df.loc[df.UNDERINFL == i, 'UNDERINFL'].count())\nfig, ax = plt.subplots(figsize=(5,3))\nax.set(xlabel='Under Influence Flag', ylabel='Count', title='Under Influence Flag')\nsns.barplot(underinfluence, count_by_underinfluence)","2cb04a6c":"df['ROADCOND'].value_counts(normalize=True, ascending=False,dropna=False).round(5)","31c8cdce":"df.replace({'ROADCOND' : {np.nan: 'Unknown'}}, inplace=True)","10fcbf3b":"plt.figure(figsize=(10,5))\nsns.countplot(x='ROADCOND', hue='SEVERITYCODE', data=df, palette=\"Set1\")\nplt.title('Count of Accidents by Road Condition Type', size=10, y=1.05)\nplt.legend(loc='upper right', prop={'size': 10})\nplt.show()","1110b20c":"df['LIGHTCOND'].value_counts(normalize=True, ascending=False,dropna=False).round(5)","2fdee470":"df.replace({'LIGHTCOND' : {np.nan: 'Unknown'}}, inplace=True)","d86c4634":"plt.figure(figsize=(20,5))\nsns.countplot(x='LIGHTCOND', hue='SEVERITYCODE', data=df, palette=\"Set1\")\nplt.title('Count of Accidents by Light Condition Type', size=10, y=1.05)\nplt.legend(loc='upper right', prop={'size': 10})\nplt.show()","d27b148a":"df['HITPARKEDCAR'].value_counts(normalize=True, ascending=False,dropna=False).round(5)","c7b81281":"plt.figure(figsize=(10,5))\nsns.countplot(x='HITPARKEDCAR', hue='SEVERITYCODE', data=df, palette=\"Set1\")\nplt.title('Count of Accidents where parked car was hit', size=10, y=1.05)\nplt.legend(loc='upper right', prop={'size': 10})\nplt.show()","acd53d03":"df.replace({'ST_COLCODE' : {np.nan: '31', ' ': '31'}}, inplace=True)","0c02b98f":"df['ST_COLCODE'].value_counts(normalize=True, ascending=False,dropna=False).round(5).head()","501e9dee":"plt.figure(figsize=(20,5))\nsns.countplot(x='ST_COLCODE', hue='SEVERITYCODE', data=df, palette=\"Set1\")\nplt.title('Count of Accidents by Collision Code', size=10, y=1.05)\nplt.legend(loc='upper left', prop={'size': 10})\nplt.show()","10a15a07":"df.groupby(\n     ['SEVERITYCODE']\n ).agg(\n     sum_INJURIES =            ('INJURIES','sum'),\n     sum_SERIOUSINJURIES =     ('SERIOUSINJURIES','sum'),\n     sum_FATALITIES =          ('FATALITIES','sum'),\n ).reset_index()","65ff006a":"x_cols = [col for col in df.columns if col not in ['SEVERITYCODE'] if ((col =='INJURIES') or (col == 'SERIOUSINJURIES')\n                                                                       or (col == 'FATALITIES'))]\n\nlabels = []\nvalues = []\nfor col in x_cols:\n    labels.append(col)\n    values.append(np.corrcoef(df[col].values, df['SEVERITYCODE'].values)[0,1])\ncorr_df = pd.DataFrame({'col_labels':labels, 'corr_values':values})\ncorr_df = corr_df.sort_values(by='corr_values')\n\nind = np.arange(len(labels))\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(5,2))\nrects = ax.barh(ind, np.array(corr_df.corr_values.values), color='y')\nax.set_yticks(ind)\nax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\", fontsize=10)\nax.set_title(\"Correlation coefficient of the injury variables\", fontsize=10)\nplt.show()","20990c34":"df['PEDROWNOTGRNT'].value_counts(normalize=True, ascending=False,dropna=False).round(5)","86795dcf":"df.replace({'PEDROWNOTGRNT' : {np.nan: 'N'}}, inplace=True)","9044673d":"df['SPEEDING'].value_counts(normalize=True, ascending=False,dropna=False).round(5)","5139a189":"df.replace({'SPEEDING' : {np.nan: 'N'}}, inplace=True)","491c2ab9":"df['INATTENTIONIND'].value_counts(normalize=True, ascending=False,dropna=False).round(5)","3e0c6859":"df.replace({'INATTENTIONIND' : {np.nan: 'N'}}, inplace=True)","50e2d73e":"df['PERSONCOUNT'].value_counts(normalize=True, ascending=False,dropna=False).round(5).head()","e2ddd500":"df['PEDCYLCOUNT'].value_counts(normalize=True, ascending=False,dropna=False).round(5)","c09650e0":"df['PEDCOUNT'].value_counts(normalize=True, ascending=False,dropna=False).round(5)","625fe039":"df['VEHCOUNT'].value_counts(normalize=True, ascending=False,dropna=False).round(5).head()","71914e9c":"# Target variable\ntarget='SEVERITYCODE'\n\ndf_sev_1 = df.loc[df[target] == 1]\ndf_sev_1 = df_sev_1.drop(target, axis=1)\ndf_sev_1 = df_sev_1.mode().T\nnew_header = ['Mode (S = 1)']\ndf_sev_1 = df_sev_1[1:]           \ndf_sev_1.columns = new_header\n\ndf_sev_2 = df.loc[df[target] == 2]\ndf_sev_2 = df_sev_2.drop(target, axis=1)\ndf_sev_2 = df_sev_2.mode().T\nnew_header = ['Mode (S = 2)']\ndf_sev_2 = df_sev_2[1:]           \ndf_sev_2.columns = new_header\n\ndf_sev_3 = df.loc[df[target] == 3]\ndf_sev_3 = df_sev_3.drop(target, axis=1)\ndf_sev_3 = df_sev_3.mode().T\nnew_header = ['Mode (S = 3)']\ndf_sev_3 = df_sev_3[1:]           \ndf_sev_3.columns = new_header\n\ndf_sev_4 = df.loc[df[target] == 3]\ndf_sev_4 = df_sev_4.drop(target, axis=1)\ndf_sev_4 = df_sev_4.mode().T\nnew_header = ['Mode (S = 4)']\ndf_sev_4 = df_sev_4[1:]           \ndf_sev_4.columns = new_header\n\ndf_res = pd.concat([df_sev_1, df_sev_2, df_sev_3, df_sev_4], axis=1)\ndf_res","e8c6e5d9":"filename = \"Collisions_100_after_EDA_unbalanced.csv\"\ndf.to_csv(filename, index=False)","0b7f9ba9":"filename = \"Collisions_100_after_EDA_unbalanced.csv\"\ndf = pd.read_csv(filename)","ad95eba0":"df['SEVERITYCODE'] = df['SEVERITYCODE'].astype('category')","5b064959":"#features = ['ADDRTYPE','COLLISIONTYPE','PERSONCOUNT','PEDCOUNT', 'PEDCYLCOUNT', 'VEHCOUNT', 'INJURIES', \n#            'SERIOUSINJURIES', 'FATALITIES', 'JUNCTIONTYPE', 'INATTENTIONIND', 'UNDERINFL', 'WEATHER', \n#            'ROADCOND', 'LIGHTCOND', 'PEDROWNOTGRNT', 'SPEEDING', \n#            'ST_COLCODE', 'HITPARKEDCAR', 'Month', 'Weekday', 'Hour']\n\nall_cols = ['SEVERITYCODE','ADDRTYPE','COLLISIONTYPE', \n            'JUNCTIONTYPE', 'INATTENTIONIND', 'UNDERINFL', 'WEATHER', \n            'ROADCOND', 'LIGHTCOND', 'PEDROWNOTGRNT', 'SPEEDING', \n            'ST_COLCODE', 'HITPARKEDCAR']\n\nall_features = ['ADDRTYPE','COLLISIONTYPE',\n            'JUNCTIONTYPE', 'INATTENTIONIND', 'UNDERINFL', 'WEATHER', \n            'ROADCOND', 'LIGHTCOND', 'PEDROWNOTGRNT', 'SPEEDING', \n            'ST_COLCODE', 'HITPARKEDCAR']\n\ndf_sel = df.loc[:, all_cols]    \ndf_sel[all_features] = df_sel[all_features].astype('category')\ndf_sel = pd.get_dummies(df_sel, columns=all_features, drop_first=True, dtype='int64')\n\ndf_sel.head()","b149e689":"df_sel.drop(df_sel.columns[df_sel.columns.str.contains('Unknown')], axis=1, inplace=True)","926d30d7":"# Check if all the data types are int64 or not\n\ndf.select_dtypes(exclude=['int64'])","3e0a1f52":"filename = \"Collisions_100_after_Feature_Selected_unbalanced.csv\"\ndf_sel.to_csv(filename, index=False)","15edfb6c":"filename = \"Collisions_100_after_Feature_Selected_unbalanced.csv\"\ndf = pd.read_csv(filename)","272b6ffd":"import itertools\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import balanced_accuracy_score\n\nfrom sklearn.ensemble import BaggingClassifier\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom imblearn.ensemble import BalancedRandomForestClassifier\nfrom imblearn.ensemble import EasyEnsembleClassifier\n\n# --------------------------------------------------------------------------------\n# Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>\n# License: MIT\n# --------------------------------------------------------------------------------\ndef plot_confusion_matrix(cm, classes, ax,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n\n    ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.set_title(title)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.sca(ax)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        ax.text(j, i, format(cm[i, j], fmt),\n                horizontalalignment=\"center\",\n                color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    ax.set_ylabel('True label')\n    ax.set_xlabel('Predicted label')\n# --------------------------------------------------------------------------------\n# --------------------------------------------------------------------------------","927bd32e":"# Target variable\ntarget='SEVERITYCODE'\n\n# set X and y\ny = df[target]\nX = df.drop(target, axis=1)\n\nX = StandardScaler().fit(X).transform(X)\n\n# Split the data set into training and testing data sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)","d243a776":"bagging = BaggingClassifier(n_estimators=50, random_state=0, n_jobs=-1)\nbalanced_bagging = BalancedBaggingClassifier(n_estimators=50, random_state=0, n_jobs=-1)\nbrf = BalancedRandomForestClassifier(n_estimators=50, random_state=0, n_jobs=-1)\neec = EasyEnsembleClassifier(n_estimators=10, n_jobs=-1)\n\nbagging.fit(X_train, y_train)\nbalanced_bagging.fit(X_train, y_train)\nbrf.fit(X_train, y_train)\neec.fit(X_train, y_train)\n\ny_pred_bc = bagging.predict(X_test)\ny_pred_bbc = balanced_bagging.predict(X_test)\ny_pred_brf = brf.predict(X_test)\ny_pred_eec = eec.predict(X_test)\n\nfig, ax = plt.subplots(ncols=4, figsize=(20,20))\n\ncm_bagging = confusion_matrix(y_test, y_pred_bc)\nplot_confusion_matrix(cm_bagging, classes=np.unique(df[target]), ax=ax[0],\n                      title='Bagging\\nBalanced accuracy: {:.2f}'.format(balanced_accuracy_score(y_test, y_pred_bc)))\n\ncm_balanced_bagging = confusion_matrix(y_test, y_pred_bbc)\nplot_confusion_matrix(cm_balanced_bagging, classes=np.unique(df[target]), ax=ax[1],\n                      title='Balanced bagging\\nBalanced accuracy: {:.2f}'.format(balanced_accuracy_score(y_test, y_pred_bbc)))\n\ncm_brf = confusion_matrix(y_test, y_pred_brf)\nplot_confusion_matrix(cm_brf, classes=np.unique(df[target]), ax=ax[2],\n                      title='Balanced Random Forest\\nBalanced accuracy: {:.2f}'.format(balanced_accuracy_score(y_test, y_pred_brf)))\n\ncm_eec = confusion_matrix(y_test, y_pred_eec)\nplot_confusion_matrix(cm_eec, classes=np.unique(df[target]), ax=ax[3],\n                      title='Balanced EasyEnsemble\\nBalanced accuracy: {:.2f}'.format(balanced_accuracy_score(y_test, y_pred_eec)))\n\n\nplt.show()","877bc665":"filename = \"Collisions_100_after_Feature_Selected_unbalanced.csv\"\ndf = pd.read_csv(filename)","e093963d":"df['SEVERITYCODE'].value_counts(normalize=True, ascending=False,dropna=False).round(5)","519c2297":"df['Severity 4'] = 0\ndf.loc[df['SEVERITYCODE'] == 4, 'Severity 4'] = 1\ndf['Severity 4'].value_counts()","ca4ad2f0":"df = pd.concat([df[df['Severity 4']==1].sample(10000, replace = True),\n                   df[df['Severity 4']==0].sample(10000)], axis=0)\nprint('Resampled data:', df['Severity 4'].value_counts())","6387f6f8":"df['Severity 4'].value_counts(normalize=True, ascending=False, dropna=False).round(5)","c52bcc54":"df.drop(['SEVERITYCODE'], axis=1, inplace=True)","44c37671":"filename = \"Collisions_100_after_Feature_Selected_balanced.csv\"\ndf.to_csv(filename,index=False)","12f49db0":"filename = \"Collisions_100_after_Feature_Selected_balanced.csv\"\ndf = pd.read_csv(filename)","48330c3d":"import math\nimport warnings\n\nwarnings.simplefilter(action='ignore', category=Warning)\n\ndf_sel = df.drop(df.columns[df.columns.str.contains('ST_COLCODE')], axis=1)\n\nx_cols = [col for col in df_sel.columns if col not in ['Severity 4'] if (df[col].dtype=='int64')]\n\nlabels = []\nvalues = []\nfor col in x_cols:\n    if not (math.isnan(np.corrcoef(df_sel[col].values, df_sel['Severity 4'].values)[0,1])):\n        labels.append(col)\n        values.append(np.corrcoef(df_sel[col].values, df_sel['Severity 4'].values)[0,1])\ncorr_df = pd.DataFrame({'col_labels':labels, 'corr_values':values})\ncorr_df = corr_df.sort_values(by='corr_values')\n\nind = np.arange(len(labels))\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(10,12))\nrects = ax.barh(ind, np.array(corr_df.corr_values.values), color='g')\nax.set_yticks(ind)\nax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\", fontsize=10)\nax.set_title(\"Correlation coefficient of the variables with respect to Severity Code\", fontsize=10)\nplt.show()","2ddaedae":"filename = \"Collisions_100_after_Feature_Selected_balanced.csv\"\ndf = pd.read_csv(filename)","2325abd1":"from sklearn import preprocessing\n\n# Import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\n\n# Import KNeighborsClassifier from sklearn.neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Import DecisionTreeClassifier from sklearn.tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, auc","4fef96f4":"# Target variable\ntarget='Severity 4'\n\n# set X and y\ny = df[target]\nX = df.drop(target, axis=1)\n\nX = preprocessing.StandardScaler().fit(X).transform(X)\n\n# Split the data set into training and testing data sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)\n\n# List of classification algorithms\nalgorithm_list=['Logistic Regression', 'k-Nearest Neighbors', 'Decision Trees', 'Random Forest']\n\n# Initialize an empty list for the accuracy for each algorithm\naccuracy_list=[]","ba0c86fd":"# Logistic regression with default setting.\nfrom sklearn.linear_model import LogisticRegression\n\n# Classifier Model = Logistic Regression\nlreg_clf = LogisticRegression(max_iter=10000, random_state=42)\n\nlreg_clf.fit(X_train, y_train)\n\nlreg_accuracy_train = lreg_clf.score(X_train, y_train)\nprint(\"Training Accuracy: %.1f%%\"% (lreg_accuracy_train*100))\n\nlreg_accuracy_test = lreg_clf.score(X_test, y_test)\nprint(\"Testing Accuracy: %.1f%%\"% (lreg_accuracy_test*100))","63b6ea15":"#Grid Search\nfrom sklearn.model_selection import GridSearchCV\n\nLR_grid = {\n           'C':        [0.001, 0.009, 0.01, 0.09, 1, 5, 10, 25],\n           'max_iter': [1000, 10000, 100000]\n          }\n\nlr_cv = GridSearchCV(estimator=LogisticRegression(random_state=42), param_grid = LR_grid, scoring = 'accuracy', cv = 5)\n\nlr_cv.fit(X_train, y_train)\nprint('Best Parameters: ', lr_cv.best_params_)","2efd256d":"%%time\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\n# Classifier Model = Logistic Regression\nlreg_clf = LogisticRegression(C=5, max_iter=1000, penalty='l2')\n\nlreg_clf.fit(X_train, y_train)\n\nlreg_accuracy_train = lreg_clf.score(X_train, y_train)\nprint(\"Training Accuracy: %.1f%%\"% (lreg_accuracy_train*100))\n\nlreg_accuracy_test = lreg_clf.score(X_test, y_test)\nprint(\"Testing Accuracy: %.1f%%\"% (lreg_accuracy_test*100))\n\n# Append to the accuracy list\naccuracy_list.append(lreg_accuracy_test)","8aae1c24":"from sklearn.metrics import confusion_matrix\n\ny_pred = lreg_clf.predict(X_test)\n\nlreg_cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\n\ndf_conf = pd.DataFrame(data=lreg_cm, columns=['Predicted: 0','Predicted: 1'], index=['Actual: 0','Actual: 1'])\n\nplt.figure(figsize = (5,3))\n\nsns.heatmap(df_conf, annot=True, fmt='d', cmap=\"YlGnBu\").set_title(\n            \"Logistic Regression\\nTrain Acc %: {:.2f} Test Acc %: {:.2f}\".format(\n                lreg_accuracy_train*100, lreg_accuracy_test*100), fontsize=12)\n\nplt.show()","ebdcf0f5":"# Import KNeighborsClassifier from sklearn.neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Create a k-NN classifier with 7 neighbors\nknn_clf = KNeighborsClassifier(n_neighbors=7)\n\nknn_clf.fit(X_train, y_train)\n\nknn_accuracy_train = knn_clf.score(X_train, y_train)\nprint(\"Train Accuracy: %.1f%%\"% (knn_accuracy_train*100))\n\nknn_accuracy_test = knn_clf.score(X_test,y_test)\nprint(\"Test Accuracy: %.1f%%\"% (knn_accuracy_test*100))\n\n# Append to the accuracy list\naccuracy_list.append(knn_accuracy_test)","94fe0d21":"from sklearn.metrics import confusion_matrix\n\ny_pred = knn_clf.predict(X_test)\n\nknn_cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\n\ndf_conf = pd.DataFrame(data=knn_cm, columns=['Predicted: 0','Predicted: 1'], index=['Actual: 0','Actual: 1'])\n\nplt.figure(figsize = (5,3))\nsns.heatmap(df_conf, annot=True,fmt='d',cmap=\"YlGnBu\").set_title(\n    \"k-NN\\nTrain Acc %: {:.2f} Test Acc %: {:.2f}\".format(\n                knn_accuracy_train*100, knn_accuracy_test*100), fontsize=12)\nplt.show()","ddb4f0c2":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nDT_grid = {'min_samples_split': [5, 10, 20, 30], \n           'max_features': [None, 'log2', 'sqrt']}\n\nCV_DT = GridSearchCV(DecisionTreeClassifier(random_state=42), DT_grid, verbose=1, cv=3)\nCV_DT.fit(X_train, y_train)\n\nprint('Best Parameters: ', CV_DT.best_params_)","2f5bcbec":"%%time\nfrom sklearn import tree\n\n# Training step, on X_train with y_train\ntree_clf = tree.DecisionTreeClassifier(min_samples_split = 5, max_features = 'log2', \n                                       class_weight='balanced', random_state=42)\ntree_clf = tree_clf.fit(X_train, y_train)\n\ntree_accuracy_train = tree_clf.score(X_train, y_train)\nprint(\"Train Accuracy: %.1f%%\"% (tree_accuracy_train*100))\n\ntree_accuracy_test = tree_clf.score(X_test,y_test)\nprint(\"Test Accuracy: %.1f%%\"% (tree_accuracy_test*100))\n\n# Append to the accuracy list\naccuracy_list.append(tree_accuracy_test)","72d5c81b":"y_pred = tree_clf.predict(X_test)\n\ntree_cm = confusion_matrix(y_true=y_test, y_pred=y_pred)\n\ndf_conf = pd.DataFrame(data=tree_cm, columns=['Predicted: 0','Predicted: 1'], index=['Actual: 0','Actual: 1'])\n\nplt.figure(figsize=(5, 3))\nsns.heatmap(df_conf, annot=True, fmt='d',cmap=\"YlGnBu\").set_title(\n    \"Decision Tree\\nTrain Acc %: {:.2f} Test Acc %: {:.2f}\".format(\n                tree_accuracy_train*100, tree_accuracy_test*100), fontsize=12)\nplt.show()","17646097":"fig, ax = plt.subplots(figsize=(20, 10))\ntree.plot_tree(tree_clf, max_depth=4, fontsize=10,\n               feature_names=df.drop('Severity 4', axis=1).columns.to_list(), class_names=True, filled=True)\nplt.show()","95821cc8":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid ={'bootstrap': [True, False],\n 'max_depth': [5, 10, 20, 30, 40],\n 'max_features': ['auto', 'sqrt'],\n 'n_estimators': [10, 20, 30]\n            }\n\nCV_RF = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid,cv=4)\nCV_RF.fit(X_train, y_train)\nprint('Best Parameters: ', CV_RF.best_params_)","1fc073fd":"%%time\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nrf_clf = RandomForestClassifier(bootstrap=False, max_depth=40, max_features='sqrt', n_estimators=20, random_state=42)\nrf_clf.fit(X_train,y_train)\n\nf = lambda x: 1 if x>=0.5 else 0\ntrain_pred = np.array(list(map(f, rf_clf.predict(X_train))))\ntest_pred = np.array(list(map(f, rf_clf.predict(X_test))))\n\nrf_train_accuracy = accuracy_score(y_train, train_pred)\nprint(\"Train Accuracy: %.1f%%\"% (rf_train_accuracy*100))\n\nrf_test_accuracy = accuracy_score(y_test, test_pred)\nprint(\"Test Accuracy: %.1f%%\"% (rf_test_accuracy*100))\n\n# Append to the accuracy list\naccuracy_list.append(rf_test_accuracy)","5497cf0d":"rf_cm = confusion_matrix(y_true=y_test, y_pred=test_pred)\n\ndf_conf = pd.DataFrame(data=rf_cm, columns=['Predicted: 0','Predicted: 1'], index=['Actual: 0','Actual: 1'])\n\nplt.figure(figsize = (5, 3))\nsns.heatmap(df_conf, annot=True,fmt='d',cmap=\"YlGnBu\").set_title(\n    \"Random Forest\\nTrain Acc %: {:.2f} Test Acc %: {:.2f}\".format(\n                rf_train_accuracy*100, rf_test_accuracy*100), fontsize=12)\nplt.show()","433d3d91":"# Generate a list of ticks for y-axis\ny_ticks = np.arange(len(algorithm_list))\n\n# Combine the list of algorithms and list of accuracy scores into a dataframe, sort the value based on accuracy score\ndf_accuracy = pd.DataFrame(list(zip(algorithm_list, accuracy_list)), \n                    columns=['Algorithm','Accuracy_Score']).sort_values(by=['Accuracy_Score'], ascending = True)\n\n# Make a plot\nax = df_accuracy.plot.barh('Algorithm', 'Accuracy_Score', align='center', legend=False, color='g')\n\n# Add the data label on to the plot\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+0.02, i.get_y()+0.2, str(round(i.get_width(),2)), fontsize=10)\n\n# Set the limit, lables, ticks and title\nplt.xlim(0, 1.1)\nplt.xlabel('Accuracy Score')\nplt.yticks(y_ticks, df_accuracy['Algorithm'], rotation=0)\nplt.title('Algorithm performance')\n\nplt.show()","3c98980e":"fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n\nplot_confusion_matrix(lreg_cm, classes=np.unique(df[target]), ax=ax[0],\n                      title=\"Logistic Regression\\nTrain Acc %: {:.2f} Test Acc %: {:.2f}\".format(\n                      lreg_accuracy_train*100, lreg_accuracy_test*100))\n\nplot_confusion_matrix(knn_cm, classes=np.unique(df[target]), ax=ax[1],\n                      title=\"k-NN\\nTrain Acc %: {:.2f} Test Acc %: {:.2f}\".format(\n                             knn_accuracy_train*100, knn_accuracy_test*100))\n\nplot_confusion_matrix(tree_cm, classes=np.unique(df[target]), ax=ax[2],\n                      title=\"Decision Tree\\nTrain Acc %: {:.2f} Test Acc %: {:.2f}\".format(\n                      tree_accuracy_train*100, tree_accuracy_test*100))\n\nplot_confusion_matrix(rf_cm, classes=np.unique(df[target]), ax=ax[3],\n                      title=\"Random Forest\\nTrain Acc %: {:.2f} Test Acc %: {:.2f}\".format(\n                      rf_train_accuracy*100, rf_test_accuracy*100))\n\n\nplt.show()","adc0f3f9":"importances = pd.DataFrame(np.zeros((X_train.shape[1], 1)), columns=['importance'], \n                           index=df.drop('Severity 4',axis=1).columns)\n\nimportances.iloc[:,0] = tree_clf.feature_importances_\n\nimportances.sort_values(by='importance', inplace=True, ascending=False)\nimportancestop = importances.head(30)\n\nplt.figure(figsize=(10, 10))\nsns.barplot(x='importance', y=importancestop.index, data=importancestop)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.title('Feature Importance (using Decision Tree Classifier)', size=15)\n\nplt.show()","f57c82f5":"importances = pd.DataFrame(np.zeros((X_train.shape[1], 1)), columns=['importance'], index=df.drop('Severity 4',axis=1).columns)\n\nimportances.iloc[:,0] = rf_clf.feature_importances_\n\nimportances.sort_values(by='importance', inplace=True, ascending=False)\nimportancestop = importances.head(30)\n\nplt.figure(figsize=(10, 10))\nsns.barplot(x='importance', y=importancestop.index, data=importancestop)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.title('Feature Importance (using Random Forest Classifier)', size=15)\n\nplt.show()","f2d96983":"### Drop ALL the columns having 'Unknown' in their titles.","72dfa210":"## Mode values of the features\n\nIt will be interesting to see the mode (highest frequency) values of each feature with respect to the severity codes.","dce64fae":"Random Forest and Decision Tree have similar accuracy.","cc4ddeff":"# Feature Selection","d94bf515":"We replace the missing values by 'Unknown' and shorten the labels somewhat so that they are readable in the diagrams we are going to plot.","bed7ec9f":"This denotes if the collision has taken place in a block, intersection or alley.","169e7d8b":"Save intermediate data file.","31c78d8d":"We subsitute missing values by 'Unknown'.","dcb01f54":"## Multi-class to Two-class","b97572f4":"### Inspect Collision Codes","ab6fbb18":"## Inspect under influence","eaa6ba85":"### Person Count, Pedestrian Count, Pedestrian Cycle Count and Vehicle Count (PERSONCOUNT, PEDCOUNT, PEDCYLCOUNT and VEHCOUNT) variables","d1688169":"Show confusion matrices side by side.","35363a18":"We generate the confusion matrices along with the blanced accuracy scores for the four chosen models.","74ec1d43":"# Introduction\nAccording to the statistics by WHO (7th Feb, 2020):\n\n- Every year the lives of approximately 1.35 million people are cut short as a result of a road traffic crash. Between 20 and 50 million more people suffer non-fatal injuries, with many incurring a disability as a result of their injury.\n- Road traffic injuries cause considerable economic losses to individuals, their families, and to nations as a whole. These losses arise from the cost of treatment as well as lost productivity for those killed or disabled by their injuries, and for family members who need to take time off work or school to care for the injured. Road traffic crashes cost most countries 3% of their gross domestic product.\n- Road traffic injuries are the leading cause of death for children and young adults aged 5-29 years.\n\nThis, therefore, needs serious attention, as it concerns human lives which is irreplacable. It is possible, thanks to machine learning, to predict severity of car accidents as a result of the complex interplay of multitudes of factors like weather, road condition, light condition, speeding etc. and also to identify which factors are more important. The information thus gathered can be used to take preventive measurements.\n\n## Objective\n\nIn 2017, WHO released Save LIVES (a road safety technical package focuses on Speed management, Leadership, Infrastructure design and improvement, Vehicle safety standards, Enforcement of traffic laws and post-crash Survival) which synthesizes evidence-based measures that can significantly reduce road traffic fatalities and injuries.\n\nWhile the pursuit of saving lives is obvious, the 2030 Agenda for Sustainable Development has set an ambitious and quantifiable goal of halving the global number of deaths and injuries from road traffic crashes by 2020.\n\nAccording to the National Safety Council, traffic collisions cause more than 40,000 deaths and injure thousands of people every year across the United States. These are not traffic accidents, but entirely preventable tragedies.\n\nIn order to reduce accidents, we need to predict it based on the external parameters. Since the accident occurs due to very many factors (unsafe road infrastructure, light condition, vulnerable road users, speeding, driving under the influence of alcohol and other psychoactive substances, distracted driving, weather) prediction of accidental severity is a challenge. Machine Learning is ideally suited here as this is a scientific approach for modelling and predicting the parameter of interest demanding only a low budget.\n\nThe current project attempts to apply a machine learning technique to predict the severity of the accident given the parameters as stated before using car collision data for the city of Seattle, USA. Additionally, the accident data is extremely skewed (class imbalance), which makes the models heavily biased towards mild injuries. A practical methodology is used here to tackle this situation.\n\n\n## Interests\n\nThe practical utilities of the prediction, besides saving lives:\n\n- Safe route planning\n- Emergency vehicle allocation\n- Roadway design\n- Reduce property damage\n- Where to place additional signage (e.g. to warn for curves)\n\nStudy of accidents in one US city can definitely help any other city having similar characteristics. As car accidents are correlated to socioeconomic condition (according to WHO), the model developed cannot be used across countries but the same methodology can, of course, be utilised. The stakeholders of the present problem are federal, state and local government agencies, non-governmental organizations, regional authorities, and possible individuals (if this model is deployed as an app for personal use).","fe11ba04":"# END","6786fc56":"## Inspect Hit Parked Car Flag\n\nThis flag is set to \u2018Y\u2019 if the collision involved hitting a parked car.","6654d496":"Although we know a priori, that this extremely skewed and multi-class data may not be amenable even to the specialized classification models that deal with unbalanced data, we go ahead and have a taste of their performance, nevertheless. Here we have chosen\u00a0\n- Bagging\n- Balanced Bagging\n- Balanced Random Forest\n- EasyEnsemble classifiers \n\nthat are capable to deal with unbalanced data inherently.","10e87661":"#### B. Unique Keys:","b504e0ad":"## Logistic Regression\nLogistic regression is employed as a baseline to perform binary classification task.","f33476f6":"## Inspect Weather features\n\nIn the data profile report we have seen this contains missing values. Keeping that in mind let's see the distinct values:","a692162b":"### Dr Debdarsan Niyogi\n\n#### 11th Sept, 2020","e5e8cd6f":"We subsitute missing values by 'Unknown'.","fe594436":"#### The new mapping is as follows:\n\n\n|SEVERITYCODE|SEVERITYDESC|\n|:-|:-|\n|1|Property Damage Only Collision|\n|2|Injury Collision|\n|3|Serious Injury Collision|\n|4|Fatality Collision|","ba934e81":"## Initial Data Processing\n\nNow we are ready to proceed towards data preparation methodically. Statistics show that Data Scientists spend almost 60% of their time in preparing data suitable for machine learning. This part is not very exciting but is still included here to point out what to look for in a huge and real data set.\n\n#### Date-Time variables\n\nTo explore the date-time feature, let us extract the Month, Weekday, Hour information from INCDTTIME and add those as separate columns.","a1c460b9":"## Inspect Collision Address Type","ce77ea6b":"## Inspect Light Conditions","6e34d4de":"#### A. Date Time information has been extracted to new columns:","2f3b36a1":"Now, drop 'SEVERITYCODE'.","0f76d18a":"## Inspect Date Time features\n\nThere are no missing values, as seen earlier.","437bad9b":"We see that in all the four cases, balanced accuracy did not even cross 50%.","035bbf2d":"Shift Severity 3 to 4 and 2b to 3","33e9b974":"We see that 50% of Severity Code is 1, and rest 50% has value 0","86d809c4":"As the dataset is huge, instead of loading the entire data set, one might load a percentage of total observations by random sampling, or shuffle the data and take first n rows. This will be useful to reduce the time required to train and test multiple ML models. Once we are satisfied with the results, we can load the entire data and determine the exact figures for the different performance evaluation matrices.","a5760643":"# Exploratory Data Analysis (EDA) \n\nNow our data is relatively cleaner and lighter. In this phase we concentrate on Data visualization, Cleaning up missing data, Value imputation, Value re-grouping.","9d81c542":"# Classification Models for Multi-class, Skewed Distribution","bcf766a5":"As seen above, Severity 4 is extremely rare, or in other words, the data is highly skewed. The main challenge of dealing with this type of data is that the machine learning algorithms train with almost 100% accuracy and fails to classify the minority class. This is intuitive since when the occurrence of the majority class is 99% per cent, even if the classifier is hard-coded to predict majority class always, the accuracy will still be 99%.\u00a0\nWe appreciate that false negative is very costly here, that is actual severity code 4 is not predicted. The situation is just like the detection of fraudulent transactions or diagnosing diseases.\nThere are many ways to deal with this situation by balancing the data synthetically by exploration method before training. We might \n\n(1) under-sample the majority class\n\n(2) over-sample the minority class or\n\n(3) have a combination of (1) and (2), i.e. over- and under-sample simultaneously.\n\nThe combination of over- and under-sampling will be used since the data is large enough. level 4 will be randomly over-sampled to 10000 and other levels will be randomly under-sampled to 10000.","987e55ed":"### Pedestrian ROW (Right Of Way) not granted, Speeding and Inattention indicator (PEDROWNOTGRNT, SPEEDING and INATTENTIONIND) Variables\n\nThese variables have missing values which are substituted by 'Unknown'.","d3aa4de8":"Code 31 is 'Not Stated'. So, we can replace the missing values by code '31'","9824ceea":"# Model Evaluation","f8f29fda":"We can conclude that the Random Forest is the best model in this scenario (Decision Tree is the second best, but very close to Random Forest). An interesting point to note here is that the top important features are somewhat different between Random Forest and the Decision Tree models. \n\n# Inference\n\nFollowing the Random Forest model, we see that special attention needs to be given to pedestrians (topmost important feature), speeding, collision with a parked car, rear-ended collision, under influence. The collision codes 50 (Struck Fixed Object), 32 (One Parked\u200a-\u200aOne Moving), 10 (Entering At Angle) are the most frequent ones.","ad117973":"The matrix is indicating a very strong correlation with severity.","68bc5693":"# Future Study\n\n- The relations between the key features and accident severity can be further studied in details\n- Different data balancing techniques can be applied and evaluated\n- Development of a much more complex real-time accident risk prediction model","09c26847":"#### Dropping Useless Variables\n\nA few variables are discarded because of the following factors:\n\nThe features having the same rationale behind removal are grouped and described below tabularly.","d91acada":"Often times, the skewed multi-class classification problem is converted to the two-class problem by taking the minority class versus the addition of the rest of the classes. In our situation, the accidents with severity level 4 are fatal and others are non-fatal. Therefore, we can focus on level 4 accidents and regroup the levels of severity into level 4 versus other levels. In this process, a new column 'Severity 4' is created.","ad14efab":"|Attribute|Data Type, Length|Description|Reason for Dropping|\n|:-|:-|:-|:-|\n|INCDATE|Date|The date of the incident.|INCDTTM has both date and time info|\n|INCDTTM|Text, 30|The date and time of the incident.|Extracted Month, Weekday, Hour info|","7d2fa986":"There are three variables: Injuries, Serious injuries and Fatalities. Let us see how the numbers are distributed amongst the four severity codes we have.","020b5c9b":"|Attribute|Data Type, Length|Description|Reason for Dropping|\n|:-|:-|:-|:-|\n|EXCEPTRSNDESC|Text, 300|Not specified|Description is not needed|\n|SEVERITYDESC|Text|A detailed description of the severity of the collision|Description is not needed, we have the key|\n|SDOT_COLDESC|Text, 300|A description of the collision corresponding to the collision code.|Description is not needed, we have the key|\n|ST_COLDESC|Text, 300|A description that corresponds to the state\u2019s coding designation.|Description is not needed, we have the key|\n|LOCATION|Text, 255|Description of the general location of the collision|Description is not needed (we do not have corresponding codes)|","bdd72177":"#### Data Profile","6b2451f3":"Plot the Accuracy vs Algorithm","985e26ef":"#### C. Not specified in the attribute list (unknown): ","1e6d8bcd":"## Data\n\n## Acquisition\n\nThe car collision data is obtained from Seattle Govt\u2019s website (Timeframe: 2004 to Present).\n\nData Source: http:\/\/data-seattlecitygis.opendata.arcgis.com\/datasets\/5b5c745e0f1f48e7a53acec63a0022ab_0\/data\n\nData Catalogue: https:\/\/www.seattle.gov\/Documents\/Departments\/SDOT\/GIS\/Collisions_OD.pdf\n\n### Loading","f6567dde":"#### D. Description columns:","a42913f0":"We substitute the missing values by 'Unknown' as above.","ffff13de":"#### E. Duplicate information:\n\nWe ignore SDOT_COLCODE, firstly because we already have got a collision code (ST_COLCODE) and secondly, because this code is not described in the attribute information published on the Seattle Govt. website.","ef1665ff":"|Attribute|Data Type, Length|Description|Reason for Dropping|\n|:-|:-|:-|:-|\n|EXCEPTRSNCODE|Text, 10|Not specified|Unknow key, where the only value is 'Not Enough Information (NEI)|\n|REPORTNO|Unknown (Not in the Attribute list|Unknow key|Unknow key|\n|X|Unknown (Not in the Attribute list|Unknown|Unknown key|\n|Y|Unknown (Not in the Attribute list|Unknown|Unknow key|\n|STATUS|Unknown (Not in the Attribute list|Unknow key|Unknow key|\n|SDOTCOLNUM|Text, 10|A number given to the collision by SDOT.|Unknow Number or Code|Unknow key|","35248b42":"Since, apart from Severity Code = 1 (\"Property Damage Only Collision\"), Severity code is assigned based on the injury level, the former is a direct reflection of the latter. If we use injury features as predictors, it is easly seen that those will overwhelm the other features, and the prediction will be based on the after-effects of a collision. Therefore, these three features will be ignored.","4b49ce22":"Select Features of interest and one-hot encode them.","bd77a440":"#### Data Attributes\n\nListed below for convenience:\n\n|Attribute|Data Type, Length|Description|\n|:-|:-|:-|\n|OBJECTID|ObjectID|ESRI unique identifier|\n|SHAPE|Geometry|ESRI geometry field|\n|INCKEY|Long|A unique key for the incident|\n|COLDETKEY|Long|Secondary key for the incident|\n|ADDRTYPE|Text, 12|Collision address type: Alley, Block, Intersection|\n|INTKEY|Double|Key that corresponds to the intersection associated with a collision|\n|LOCATION|Text, 255|Description of the general location of the collision|\n|EXCEPTRSNCODE|Text, 10|Not specified|\n|EXCEPTRSNDESC|Text, 300|Not specified|\n|SEVERITYCODE|Text, 100|A code that corresponds to the severity of the collision: 3\u2014fatality, 2b\u2014serious injury, 2\u2014injury, 1\u2014prop damage, 0\u2014unknown|\n|SEVERITYDESC|Text|A detailed description of the severity of the collision|\n|COLLISIONTYPE|Text, 300|Collision type|\n|PERSONCOUNT|Double|The total number of people involved in the collision|\n|PEDCOUNT|Double|The number of pedestrians involved in the collision. This is entered by the state.|\n|PEDCYLCOUNT|Double|The number of bicycles involved in the collision. This is entered by the state.|\n|VEHCOUNT|Double|The number of vehicles involved in the collision. This is entered by the state.|\n|INJURIES|Double|The number of total injuries in the collision. This is entered by the state.|\n|SERIOUSINJURIES|Double|The number of serious injuries in the collision. This is entered by the state.|\n|FATALITIES|Double|The number of fatalities in the collision. This is entered by the state.|\n|INCDATE|Date|The date of the incident.|\n|INCDTTM|Text, 30|The date and time of the incident.|\n|JUNCTIONTYPE|Text, 300|Category of junction at which collision took place|\n|SDOT_COLCODE|Text, 10|A code given to the collision by SDOT.|\n|SDOT_COLDESC|Text, 300|A description of the collision corresponding to the collision code.|\n|INATTENTIONIND|Text, 1|Whether or not collision was due to inattention. (Y\/N)|\n|UNDERINFL|Text, 10|Whether or not a driver involved was under the influence of drugs or alcohol.|\n|WEATHER|Text, 300|A description of the weather conditions during the time of the collision.|\n|ROADCOND|Text, 300|The condition of the road during the collision.|\n|LIGHTCOND|Text, 300|The light conditions during the collision.|\n|PEDROWNOTGRNT|Text, 1|Whether or not the pedestrian right of way was not granted. (Y\/N)|\n|SDOTCOLNUM|Text, 10|A number given to the collision by SDOT.|\n|SPEEDING|Text, 1|Whether or not speeding was a factor in the collision. (Y\/N)|\n|ST_COLCODE|Text, 10|A code provided by the state that describes the collision. For more information about these codes, please see the State Collision Code Dictionary.|\n|ST_COLDESC|Text, 300|A description that corresponds to the state\u2019s coding designation.|\n|SEGLANEKEY|Long|A key for the lane segment in which the collision occurred.|\n|CROSSWALKKEY|Long|A key for the crosswalk at which the collision occurred.|\n|HITPARKEDCAR|Text, 1|Whether or not the collision involved hitting a parked car. (Y\/N)|","93d762cf":"### Inspect Injuries, Serious Injuries and Fatalitites Features","111006a5":"## Inspect Road Condition","94a1e67c":"### Balancing Severity (Resampling)","d9bf1fa6":"Since the Person Count has a long list, we can consider the top few values only, and assign the rest of the codes to a category 'Other'.","31378000":"#### Train Test Split\nSplit the data into X_train, X_test, y_train, and y_test, after standardising the input features.","2eb8cb44":"# Balancing","9701c764":"## Random Forest","253c5757":"# Comparison of Different Classification Models","07d96259":"# Correlation\n\nLet us now get an idea of how the variables are correlated (except ST_COLCODE, as it contains a very long list of codes).","c856ef52":"## Inspect Junction Type","21a91f4f":"## K Nearest Neighbor (kNN)","07a10475":"## Inspect Collision Type","58583d67":"To see the skewness, let's take a percent counts for each Severity Code:","20bf8309":"The flag UNDERINFL indicates whether or not a driver involved was under the influence of drugs or alcohol, has ambigous data: ","0463f8c8":"We can safely map 0 to 'N', 1 to 'Y' and NaN as 'Unknown'.","477f57dc":"## Inspect Severity Code\n\nSeverity Code (SEVERITYCODE) is the target\/dependent variable. Let us scrutinize that first.","e78cb0eb":"## Decision Tree","370ceaf5":"|Attribute|Data Type, Length|Description|Reason for Dropping|\n|:-|:-|:-|:-|\n|SDOT_COLCODE|Text, 10|A code given to the collision by SDOT.|Description of this code is not provided. We already have ST_COLCODE|","209fcee1":"Upon loading the data, we see that there are 40 variables and 2,21,006 number of observations with zero duplicates but having a missing cell percent 15.8%.","7158934b":"#### Train Test Split\nSplit the data into X_train, X_test, y_train, and y_test, after standardising the input features.","72d4473e":"|Attribute|Data Type, Length|Description|Reason for Dropping|\n|:-|:-|:-|:-|\n|OBJECTID|ObjectID|ESRI unique identifier|Unique identifier is not a predictor|\n|INCKEY|Long|A unique key for the incident|Unique identifier is not a predictor|\n|COLDETKEY|Long|Secondary key for the incident|Unique identifier is not a predictor|\n|INTKEY|Double|Key that corresponds to the intersection associated with a collision|Unknow key|\n|SEGLANEKEY|Long|A key for the lane segment in which the collision occurred.|Unknow key|\n|CROSSWALKKEY|Long|A key for the crosswalk at which the collision occurred.|Unknow key|","870ed95c":"# Data Science: A Road to Safer\u00a0Roads","2b254c96":"Since we are not going to predict an 'Unknown' severity (SEVERITYCODE = 0), these observations, along with the rows with missing values can safely be deleted. The categorical values can also be remapped to a scale of 1 to 4, where 3 is assigned to 4 and 2b to 3."}}