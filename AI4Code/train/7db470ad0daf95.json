{"cell_type":{"4b549cb1":"code","d2c3c672":"code","aa30e37f":"code","fa43a2b0":"code","aaf2b8a3":"code","5bb0fd5f":"code","531d7be6":"code","dabd1249":"code","d6fe915d":"code","6522f792":"code","9e3d05d3":"markdown","a4443f1c":"markdown","9e83e8dc":"markdown","069684e0":"markdown","ae0c6ebd":"markdown","9ff2d80c":"markdown","d8419083":"markdown","fb3598ee":"markdown","484c63dc":"markdown","80b21796":"markdown","04f9fc05":"markdown","fcca5a77":"markdown","342ee793":"markdown","a1db2caa":"markdown","d69d76d4":"markdown","9d20918c":"markdown","3763dea1":"markdown","24db1b9b":"markdown","dc6ab755":"markdown","206dd60f":"markdown","5460a4eb":"markdown","a1ce60f6":"markdown","a26c7603":"markdown","e938fbb8":"markdown","1a77a102":"markdown","9e597cf5":"markdown","455eab8c":"markdown","98e56f61":"markdown"},"source":{"4b549cb1":"from typing import List, Tuple\nimport random\nimport html\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold, KFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport os\nfrom scipy.stats import spearmanr\nfrom scipy.optimize import minimize\nfrom math import floor, ceil\nfrom transformers import *\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout, GlobalAveragePooling1D\nfrom tensorflow.keras.models import Model","d2c3c672":"train=pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest=pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsub=pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\ndef get_dataset_information(df):\n    print(\"Number of Columns in dataset:\",df.shape[1])\n    print(\"Number of Rows in dataset:\",df.shape[0])\n    print(\"Name of Columns in dataset:\",df.columns)\n    return df.head(2)\nget_dataset_information(train) ","aa30e37f":"output_categories = train['target']\ninput_categories = train['excerpt']\nprint('\\noutput categories:\\n\\t', output_categories)\nprint('\\ninput categories:\\n\\t', input_categories)","fa43a2b0":"max_seq_length=512\n\ntokenizer = BertTokenizer.from_pretrained('..\/input\/huggingface-bert\/bert-large-uncased\/vocab.txt')","aaf2b8a3":"def fix_length(tokens, max_sequence_length=512):\n    length = len(tokens)\n    if length > max_sequence_length:\n        tokens = tokens[:max_sequence_length-1]\n    return tokens\n\n# function for tokenizing the input data for transformer.\ndef transformer_inputs(text,tokenizer,MAX_SEQUENCE_LENGTH = 512):\n\n    text_tokens = tokenizer.tokenize(str(text))\n    text_tokens = fix_length(text_tokens)\n    ids_q = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + text_tokens)\n    padded_ids = (ids_q + [tokenizer.pad_token_id] * (MAX_SEQUENCE_LENGTH - len(ids_q)))[:MAX_SEQUENCE_LENGTH]\n    token_type_ids = ([0] * MAX_SEQUENCE_LENGTH)[:MAX_SEQUENCE_LENGTH]\n    attention_mask = ([1] * len(ids_q) + [0] * (MAX_SEQUENCE_LENGTH - len(ids_q)))[:MAX_SEQUENCE_LENGTH]\n\n    return padded_ids, token_type_ids, attention_mask\n\n# function for creating the input_ids, masks and segments for the bert input\ndef input_for_model(df, tokenizer):\n    print(f'generating input for transformer...')\n    input_ids, input_token_type_ids, input_attention_masks = [], [], []\n    for text in tqdm(zip(df['excerpt'].values)):\n        ids, type_ids, mask = transformer_inputs(text,tokenizer)\n        input_ids.append(ids)\n        input_token_type_ids.append(type_ids)\n        input_attention_masks.append(mask)\n    \n    return (\n        np.asarray(input_ids, dtype=np.int32),\n        np.asarray(input_attention_masks, dtype=np.int32),\n        np.asarray(input_token_type_ids, dtype=np.int32))\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","5bb0fd5f":"inputs=input_for_model(train,tokenizer)\ntest_inputs=input_for_model(test,tokenizer)\noutputs=compute_output_arrays(train,'target')","531d7be6":"def create_model():\n    K.clear_session()\n    config=BertConfig()\n    config.output_hiffen_states=False\n\n    bert_model=TFBertModel.from_pretrained('..\/input\/huggingface-bert\/bert-base-uncased\/tf_model.h5',config=config)\n    max_len=512\n    input_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n    token_type_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"token_type_ids\")\n    attention_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n\n    sequence_output = bert_model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n    clf_output = sequence_output[:, 0, :]\n    clf_output = Dropout(.1)(clf_output)\n    out = Dense(1, activation='linear')(clf_output)\n\n    model = Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=out)\n    \n    return model","dabd1249":"from tensorflow.keras.metrics import RootMeanSquaredError\ndef root_mean_square_error(label,prediction):\n    return tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(label, prediction)))).numpy()","d6fe915d":"gkf = GroupKFold(n_splits=7).split(X=train.excerpt, groups=train.excerpt)\nvalid_preds = []\ntest_preds = []\nfor fold,(train_idx,valid_idx) in enumerate(gkf):\n\n    train_inputs=[inputs[i][train_idx] for i in range(3)]\n    train_outputs=outputs[train_idx].reshape((-1,1)).flatten()\n\n    valid_inputs = [inputs[i][valid_idx] for i in range(3)]\n    valid_outputs = outputs[valid_idx].reshape((-1,1)).flatten()\n    K.clear_session()\n    model=create_model()\n    model.compile(tf.keras.optimizers.Adam(lr=1e-4), loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    model.fit(train_inputs, train_outputs, epochs=5, batch_size=8,validation_data=(valid_inputs, valid_outputs))\n\n\n    valid_preds.append(model.predict(valid_inputs))\n    test_preds.append(model.predict(test_inputs))\n    ","6522f792":"sub['target']=model.predict(test_inputs)\nsub.to_csv('submission.csv', index=False)","9e3d05d3":"# Preparition for Bert Input","a4443f1c":"# Model Architecture:\nBERT uses a multi-layer bidirectional Transformer encoder.Google has released two variants of the model:\n\n\n1. BERT(base):\n    - Number of layers(L)=12,Hidden size(H)=768 and Total Paramters=110M.\n2. BERT(large)\n    - Number of layers(L)=24,Hidden size(H)=1024 and Total Paramters=340M.\n\nIn both variants number of layers means Transformer blocks.\n","9e83e8dc":"# Step-4:\n1. After getting the score for each word now multiply the value vector by softmax score. Purpose of this step to keep preserve only the word we want to focus on and rest of words can be removed  by multiplying with very small number like 0.001.\n\n# Step-5:\n1. Final step is to sum up the weight value vector. This step produces the output for the self-attention layer.\n\n![image.png](attachment:11120709-9378-4e9b-9607-05f6cb1c56a4.png)","069684e0":"# BERT:\nBert is a pre-trained model on a large set of book corpus and wikipedia unlabeled data and can be fine-tuned according to our task demand. Bert is launched in 2018 by Google.\nTo understand Bert in more depth we discuss two major steps in this section known as: \n1. Pre-training, and\n2. Fine-tuning\n\nPre-trained model is already trained on unlabeled data over different pre-training tasks so we don\u2019t need to trained model from scratch but we can fine-tuned it by reproduce the model if we initialized with the pre-trained parameters and all of the parameters are fine-tuned using labeled data from downstream task.","ae0c6ebd":"# Training Self-attention layer:\n# Step-1\nLike we have to train other deep neural networks in the same way we have to train the self-attention layer.\nTo understand the training process of the self-attention layer, I will go through the below steps:\n\n1. We have word embedding vectors which pass to the self- attention layer and to train self-attention layer we have to create below three new vectors:\n    1.     Query vector\n    2.     key vector\n    3.     Value vector\n2. These vectors are trained and updated during the training process by multiplying your input vector(X) with weight matrices that are learnt during training.\n\nLet\u2019s talk about query, key, value vector to avoid any confusion when understanding the process of training the self-attention layer.\n\nFor example, When we search anything in a search engine (our query) it will map to a set of keys in the database which return output(values).\n\nSo\n1. The query is set of vectors for which we want to calculate attention for,\n2. Key is set of vector for which we want to calculate attention against and\n3. After dot product multiplication we get weights which shows how attended each query against keys and then multiply by value to get resulting set of vectors.\n\n![image.png](attachment:58c45ab9-951b-4d49-a189-234c0b4eb4ae.png)\n","9ff2d80c":"# Model Output:\n\nBert gives two types of output\n1. Pooled_output: Pooled output of shape [batch size, 768] which represents the entire input sequence. It is the output of the \u2018[CLS]\u2019 token that we add at the start of each sentence.\n2. Sequence_output: Sequence_output of shape [batch size, max_seq_len, 768] which represents the entire input sequence.\n","d8419083":"# About This Notebook:\nIn this notebook, we will go to cover about Bert Model and its implementation with help of the Hugging Face library. I try to cover all theory which related to Bert\u2019s context and try to explain code in very simple language.\nAfter reading a lot about the Bert model, I try to implement but it takes a quite long time to understand its implementation. Bert code hardly contains 50 lines but in starting that 50 lines were very hard for me. So that I decided to write one detailed notebook which can help beginners like me and they can do more experiments regarding the model rather than spend time in implementation.\n","fb3598ee":"# How did Bert come into the picture?\nBert is the hottest model now, let\u2019s have a look into the history behind it. As we know Bert is built on top of the Transformer model. This model came into picture to improve the drawbacks which we had faced in old techniques. First we will look at the transformer.\n1. Transformer is a set of encoder and decoders which was introduced from the paper \u201cAttention is all you need\u201d. \n2. It resolves drawbacks of the earlier model which was used for machine translation and many other NLP applications by introducing the concept of self-attention layers in them.\n3. The transformer is based on an attention mechanism and stores all the information i.e. memorize for long time\n4. It does not require the sequential data to be processed in the order which allows more parallelization compared to RNN.\n5. The Transformer consists of 6 stacked encoders and 6 stacked decoders to form the main architecture of the model and All the encoder and decoder are identical and similar in nature ","484c63dc":"# Why Multi-head attention?\n1. Many times chances of missing words that are important in a sentence are high when we used single attention but if we use multi-head attention we have multiple query, key, value weight matrices which can help us to reduce the error or miscalculation by any single attention head and also able to focus on words increases in case of multi-head attention.\n2. By using multi-head attention we get eight different z matrices but the feed-forward neural network accepts only one single score, for that we perform concatenation of these eight matrices to convert them into a single matrix and which can be passed to the feed-forward layer. \n\n![image.png](attachment:50584250-080b-4dd5-a4c9-4e4b637b3156.png)","80b21796":"# End Note:\nNo doubt first Transformer and then Bert model makes a huge development and improvement in Natural language processing in data science, But only getting theoretical knowledge is not enough so in the next section let us implement this theory in application with help of Hugging face library.\n","04f9fc05":"# Refrences:\n1. Bert: https:\/\/arxiv.org\/pdf\/1810.04805.pdf\n2. The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning): http:\/\/jalammar.github.io\/illustrated-bert\/\n3. The Illustrated Transformer: https:\/\/jalammar.github.io\/illustrated-transformer\/\n4. Attention Is All You Need: https:\/\/arxiv.org\/pdf\/1706.03762.pdf\n5. Query,key,value vector: https:\/\/stats.stackexchange.com\/questions\/421935\/what-exactly-are-keys-queries-and-values-in-attention-mechanisms","fcca5a77":"# Define Input and output categories","342ee793":"# Load Dataset:","a1db2caa":"# Content:\n1. Introduction\n2. Why do we need Bert?\n3. How did Bert come into the picture?\n4. Transformer Introduction\n5. Encoder part of Transformer\n6. Why Bert?\n7. Detailed Discussion on Bert\n8. How does Bert work?\n9. Fine-tuning application and examples\n10. Endnote\n11. References\n12. Practical Implementation of the ongoing competition. ","d69d76d4":"# Introduction:\nBERT stands as **Bi-directional encoder representation from transformer**. Bert is a deep learning model based on transformer and nowadays mostly used to solve natural language processing tasks and give you the best results compared to traditional methods to solve NLP problems. Bert is pertained model on large set of book corpus data(800M words) and English Wikipedia(2,500M words) launched by Google in 2018.\nAs we know Bert is based on Transformer which came in picture in 2017 from the research paper \u201c**Attention is all you need**\u201d to resolve drawbacks of standard deep learning model of RNN. So up to now we can get feeling that to understand Bert in depth we should familiar with Transformer encoder part, as Transformer is a set of bunch of encoders and decoders but we need to know about only encoder part of Transformer to understand how actually Bert works. ","9d20918c":"# Encoder part of Transformer:\n\nEncoder part of the Transformer contains two layers:\n1. Self-attention layer which is defined as an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence i.e. a layer that helps an encoder to look at another word in the input sentence as it encoded a specific word.\n2. Feed-forward neural network layer.\n\nAfter getting embedding from text, we pass input to the encoder block. In the encoder block first, it goes to a self-attention layer of the first encoder then a feed-forward layer of it, that it will pass through the second encoder, and the process continues till sixth encoder processing. So the first encoder gets input embedding whereas the rest of the encoders get inputs from their previous encoder.\n\n\n![image.png](attachment:2e32cd53-4d39-4c2d-984d-b08d3c0d355d.png)","3763dea1":"# Welcome BERT:\n\nDo you think to solve every NLP task we need both the encoder and decoder parts of Transformer?\n\nThe answer is NO, task like classification problem, Question- Answers task (we need to pass two different sentences) in these kind of task we do not need to use Decoder part of Transformer and that\u2019s the reason, Model like Bert takes a entry in field of data science to solve many application of NLP tasks.\n\n![image.png](attachment:e4a88df9-aa88-4815-8d3d-1df8b4b54027.png)","24db1b9b":"# Create BERT Model","dc6ab755":"# Model Input:\nAs we know Bert handles many variants of downstream tasks such as Question-Answer, Classification etc. and with respect to tasks our input is able to represent both a pair of sentences and a single sentence in one token sequence.\n\n1. Input IDs:\n    - First we have to perform tokenization which splits a text into words or sub words. To perform this task, Bert uses WordPiece embeddings with a 30,000 token vocabulary.\n    - We add two special tokens:\n        - [CLS] token: represent classification toke which should be put before start of any sentence\n        - [SEP] token: used to distinguish pairs of sentences.\n\nLet\u2019s suppose we are fine tuning the Bert model for a question-answer task which has two sentences, one is a question and another is answer. Hence our input should be in form like ([CLS]+Question+[SEP]+Answer+[SEP]), whereas adding [SEP] token is not necessary when we are dealing with classification task as we will be going to pass only single sentence, in this case our input look like this ([CLS]+text).\n\n2. Segment Embedding:indicating whether tokens belongs to sentence A and or sentence B.\n3. Attention Mask:indicates to the model which tokens should be attended to, and which should not be by giving Boolean value 0 and 1 for padded token and real token.","206dd60f":"Before concluding the encoder part discussion of Transformer, I would like to mention about Residual Connection  which allow the output from the previous layer to bypass layers in between i.e. It skip some layers that are not so important or not learninh much.After the residual connections are added we get normalized resultant.\n\n![image.png](attachment:4a8235a7-a669-4b00-88fb-8a471da80931.png)","5460a4eb":"# Step-2:\n1. Having Query, key, value vectors, now we calculate the score which determines how much focus we need to place on other parts of the input sentence as we encode a word at a certain position.\n\n**Score = query*key**\n\n![image.png](attachment:2684d322-1ff3-44cf-bd35-ed07810c52dd.png)\n","a1ce60f6":"# Evaluation Matric:\nRoot mean square error","a26c7603":"# Pre-Training Bert:\n\nBi-directionality is a game changer approach in NLP which actually allows each word to see itself so the model could predict the target word in a multi-layered context. This was missing in standard conditional language models because they can be trained either left-to-right or right-to-left.\nSo to pre-trained Bert, Google used two unsupervised tasks:\n1. Masked Language Model and \n2. Next sentence prediction\n\n# Masked Language Model:\n1. To train a deep bidirectional representation, we mask some percentage of input tokens at random and then predict those masked tokens nd for this purpose they mask randomly 15% of wordpiece tokens in each sequence at random.\n\nBut remember we are masking tokens at pre-trained, this procedure does not appear in fine-tuning. Hence we do not always replace masked words with actual [MASK] tokens.\n\nTo avoid that researchers used the below techniques:\n\n1. 80% of the time the words were replaced with the masked token [MASK]\n2. 10% of the time the words were replaced with random word\n3. 10% of the time the words were left unchanged\nAdvantage of Masked Language Modeling is that the Transformer encoder does not know which words it will be asked to predict or which have been replaced by random words, so it is forced to keep a distributional contextual representation of every input token.\n\n# Next Sentence Prediction:\n\nNext sentence prediction task is a binary classification task in which, given a pair of sentences, it is predicted if the second sentence is the actual next sentence of the first sentence.\nIt is helpful because many downstream tasks such as Question and Answering and Natural Language Inference require an understanding of the relationship between two sentences.\n","e938fbb8":"# Why do we need Bert?\nIf we remember earlier, Word2vec and Gensim were famous for embedding before Transformer. When we had text data, we were building model by using techniques like TF-IDF(Term Frequency-Inverse Document Frequency) or w2v or both in combination but the embedding we get from w2v technique is context-independent i.e. if same word appears twice in a sentence for two different meanings, it will generate same embedding. These techniques failed to capture their essence and generate the same embedding for two different words whereas Bert is context dependent i.e. if two words come repeatedly in the same sentence, it tries to generate their embedding according to context.\n\nFor example: Let\u2019s take this quote by Roberto Bolano:\n\n**\u201cI felt happy because I saw the others were happy and because I knew I should feel happy, but I wasn\u2019t really happy.\u201d**\n\nHere the word happy has four different meanings but if we use old embedding techniques which are context-independent, for those techniques happy is just a word and embedding will be generated with respect to it. However Bert understands all happy words according to their context and generates embedding. Just to add one more point here that Bert uses Word Piece technique to generate embedding.\n","1a77a102":"# Load Tokeizer","9e597cf5":"# Fine Tuning \nWe can fine-tuned Bert model for the following task:\n\n**Question-Answering Tasks:**\nAs we already discussed, Bert accepts input sentences by adding two special tokens one is [CLS] and another is [SEP]. [CLS] stands for classification and we know that for question-answering tasks we do not need to consider output regarding [CLS] tokens. We just consider output of 768 dim from the answer and each output has two softmax layers, one for start span and another for end span. That softmax layer predicts the probability of that token i(for example) in the start of span and in the end of span. Here every token has two probability and we pick max span probability from those as the starting index of the paragraph.\n\n**Sequence Classification Tasks:**\nWhen we have one single sentence we do not need to use [SEP] token to design input for Bert model. All calculations happening regarding [CLS] token and label probabilities are computed with a standard softmax.\n","455eab8c":"Now pass this z vector to feed-forward neural network and then this becomes the input for the next encoder and the process continues.\nFor Faster calculation, the whole steps which we discussed above can be done in matrix form in the following manner.\n\n1. Create query, key, value vector by multiplying embedding with weight matrices.\n\n![image.png](attachment:99970893-c286-4f3b-8a88-3dadb4c7141c.png)\n\n2. Now calculate output for self-attention layer in one single step.\n\n![image.png](attachment:dc75f438-f74b-4301-bf7a-99023ab523c6.png)\n\nThe above-mentioned step for calculating output for the self-attention layer is for one time but in actual implementation, this output is calculated parallelly and independently many times, therefore, it is referred to as Multi-head attention.\n","98e56f61":"# Step-3:\n1. In this step, we divide the score by 8, as we know the dimension of the query, key, value vector is 64 so to achieve a more stable gradient we divide the score by the root of their dimension.\n2. After dividing, pass to softmax operation which normalizes the score. As we know after normalization all scores come in one scale so it becomes easy to determine the importance of each word expressed at their positions.\n\n![image.png](attachment:c411dc34-057f-457b-a931-0c9e7cde3b8a.png)"}}