{"cell_type":{"b5e0a50c":"code","603a0d86":"code","8929a3ce":"code","e2239b11":"code","62e30834":"code","7015d996":"code","47ad6552":"code","920543ee":"code","189db97f":"code","ccefea1d":"code","18a1921a":"code","c9b87c9c":"code","9ead780b":"code","46046bd0":"code","973f3cc0":"code","56275b6f":"code","6f825a2c":"code","346eb197":"code","36fa053a":"code","1553c974":"code","b2eb69f7":"code","3dbbc587":"code","1d57eb90":"code","b283eac1":"code","cef214f7":"code","69f3ce2d":"code","22ec9228":"code","9196329d":"code","4df83740":"code","66446541":"code","6a6d69d7":"code","139ba300":"code","e10aeca6":"code","2c4fa0f0":"code","c5d6de93":"code","80656692":"code","e5ab7393":"code","464ee83c":"code","ab5881f4":"code","32b79712":"code","f80245f6":"code","b8f4d1bf":"code","39e343b3":"code","f19c0084":"code","d6a3826c":"code","6183b76a":"code","f7e8b5f2":"code","833773bc":"code","f3c6a44c":"code","9402332d":"code","f98106b2":"code","9b19ce99":"code","3070145a":"code","a7033829":"code","bf1cfd97":"code","e78b91ce":"code","9f0f1ee4":"code","ee2356ff":"code","fb440389":"code","1d00af2f":"code","7c4e0b13":"code","61121f0b":"code","48b31d8d":"code","bcda0dce":"code","4323e4c9":"code","6fd1d345":"code","86d33e50":"code","ed6f5f67":"code","66dabec9":"code","06c57ab0":"code","221b0480":"code","5c9ebb7f":"code","f0c6749a":"code","373f75d4":"code","ed0b3cf4":"code","037653c6":"code","00175b2d":"code","a2525262":"code","81f965a7":"code","5b67af6f":"code","329132b4":"code","65f9fa41":"markdown","bdec34d0":"markdown","fbd50226":"markdown","b019f630":"markdown","73c608ea":"markdown","9f2f7839":"markdown","011e6f8a":"markdown","c00b644b":"markdown","d178c2c5":"markdown","ab793c2e":"markdown","dd64b5bb":"markdown","9611c1c4":"markdown","f8e3c0b9":"markdown","c3683b88":"markdown","e0e5b3b3":"markdown","1306481c":"markdown","67259cda":"markdown","ac92dee7":"markdown","7a5d4b2e":"markdown","2a1d725e":"markdown","5c8e00d9":"markdown","60c9c917":"markdown","b0432f75":"markdown","910af955":"markdown","159b5480":"markdown","893bdec8":"markdown","0fce3177":"markdown","e842a919":"markdown","202ea8f2":"markdown","d2aff2e3":"markdown","62adc285":"markdown","422a5881":"markdown","36f454e0":"markdown","7b0ad6e2":"markdown","7268ebb7":"markdown","0b8efdb1":"markdown","dfb7e658":"markdown","68218793":"markdown","cff5aeb6":"markdown","e27dce39":"markdown","97046559":"markdown","b51e5abb":"markdown","99700951":"markdown","28e4ebee":"markdown","fddf7c42":"markdown","bee78d3d":"markdown","90b14a7e":"markdown","59d45d25":"markdown","b5253fc4":"markdown","c434e926":"markdown","a402fd3d":"markdown","a645fa41":"markdown","53f26ad6":"markdown","21672630":"markdown","ae5137e2":"markdown","d7f26c5b":"markdown","716539c0":"markdown","c49291e7":"markdown","54321629":"markdown","a0899158":"markdown","34befe85":"markdown","54736330":"markdown","a1f146c5":"markdown","ec7b43d3":"markdown","68c0a167":"markdown","d51330c7":"markdown","52d2ba7c":"markdown","37904524":"markdown","0d124afe":"markdown","8bf1e4ba":"markdown","b1d95c34":"markdown","dbfadb4d":"markdown","5e5d76ac":"markdown","8fc9ad2b":"markdown","99c0e977":"markdown","8bc44b0a":"markdown","9a81b180":"markdown","607f257d":"markdown","17b62c09":"markdown","df6d6e08":"markdown","e3ab79d6":"markdown","04e52fb6":"markdown","81115b8f":"markdown","e571e5c5":"markdown","c9e5e22f":"markdown"},"source":{"b5e0a50c":"# Every library that will be used in this project is imported at the start.\n\n# Data handling and processing\nimport pandas as pd\nimport numpy as np\n\n# Data visualisation & images\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Pipeline and machine learning algorithms\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n# Model fine-tuning and evaluation\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn import model_selection\n\n# Hide system warnings\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","603a0d86":"# Data downloaded from Kaggle as a .csv file and read into this notebook from my local directory.\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","8929a3ce":"# Join all data into one file\nntrain = train.shape[0]\nntest = test.shape[0]\n\n# Creating y_train variable; we'll need this when modelling, but not before\ny_train = train['Survived'].values\n\n# Saving the passenger ID's ready for our submission file at the very end\npassId = test['PassengerId']\n\n# Create a new all-encompassing dataset\ndata = pd.concat((train, test))\n\n# Printing overall data shape\nprint(\"data size is: {}\".format(data.shape))","e2239b11":"# Let's see some basic info about the dataset\ndata.info()","62e30834":"# Inspecting the first five rows, or 'observations'\ndata.head()","7015d996":"# Returning descriptive statistics of the train dataset\ndata.describe()","47ad6552":"# Provide NaN count for each feature in the dataset\nprint(data.isnull().sum())","920543ee":"# Initiate correlation matrix\ncorr = train.corr()\n# Set-up mask\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set-up figure\nplt.figure(figsize=(14, 8))\n# Title\nplt.title('Overall Correlation of Titanic Features', fontsize=18)\n# Correlation matrix\nsns.heatmap(corr, mask=mask, annot=False,cmap='RdYlGn', linewidths=0.2, annot_kws={'size':20})\nplt.show()","189db97f":"# Plot for survived\nfig = plt.figure(figsize = (10,5))\nsns.countplot(x='Survived', data = train)\nprint(train['Survived'].value_counts())","ccefea1d":"# Bar chart of each Pclass type\nfig = plt.figure(figsize = (10,10))\nax1 = plt.subplot(2,1,1)\nax1 = sns.countplot(x = 'Pclass', hue = 'Survived', data = train)\nax1.set_title('Ticket Class Survival Rate')\nax1.set_xticklabels(['1 Upper','2 Middle','3 Lower'])\nax1.set_ylim(0,400)\nax1.set_xlabel('Ticket Class')\nax1.set_ylabel('Count')\nax1.legend(['No','Yes'])\n\n# Pointplot Pclass type\nax2 = plt.subplot(2,1,2)\nsns.pointplot(x='Pclass', y='Survived', data=train)\nax2.set_xlabel('Ticket Class')\nax2.set_ylabel('Percent Survived')\nax2.set_title('Percentage Survived by Ticket Class')","18a1921a":"# Bar chart of age mapped against sex. For now, missing values have been dropped and will be dealt with later\nsurvived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwomen = train[train['Sex']=='female']\nmen = train[train['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=20, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=20, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=20, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=20, label = not_survived, ax = axes[1], kde = False)\nax.legend()\n_ = ax.set_title('Male')","c9b87c9c":"# Plotting survival rate vs Siblings or Spouse on board\nfig = plt.figure(figsize = (10,12))\nax1 = plt.subplot(2,1,1)\nax1 = sns.countplot(x = 'SibSp', hue = 'Survived', data = train)\nax1.set_title('Survival Rate with Total of Siblings and Spouse on Board')\nax1.set_ylim(0,500)\nax1.set_xlabel('# of Sibling and Spouse')\nax1.set_ylabel('Count')\nax1.legend(['No','Yes'],loc = 1)\n\n# Plotting survival rate vs Parents or Children on board\nax2 = plt.subplot(2,1,2)\nax2 = sns.countplot(x = 'Parch', hue = 'Survived', data = train)\nax2.set_title('Survival Rate with Total Parents and Children on Board')\nax2.set_ylim(0,500)\nax2.set_xlabel('# of Parents and Children')\nax2.set_ylabel('Count')\nax2.legend(['No','Yes'],loc = 1)","9ead780b":"# Graph to display fare paid per the three ticket types\nfig = plt.figure(figsize = (10,5))\nsns.swarmplot(x=\"Pclass\", y=\"Fare\", data=train, hue='Survived')","46046bd0":"# Extract last name\ndata['Last_Name'] = data['Name'].apply(lambda x: str.split(x, \",\")[0])\n\n# Fill in missing Fare value by overall Fare mean\ndata['Fare'].fillna(data['Fare'].mean(), inplace=True)\n\n# Setting coin flip (e.g. random chance of surviving)\ndefault_survival_chance = 0.5\ndata['Family_Survival'] = default_survival_chance\n\n# Grouping data by last name and fare - looking for families\nfor grp, grp_df in data[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n    \n    # If not equal to 1, a family is found \n    # Then work out survival chance depending on whether or not that family member survived\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 1\n            elif (smin == 0.0):\n                data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 0\n\n# Print the headline\nprint(\"Number of passengers with family survival information:\", \n      data.loc[data['Family_Survival']!=0.5].shape[0])","973f3cc0":"# If not equal to 1, a group member is found\n# Then work out survival chance depending on whether or not that group member survived\nfor _, grp_df in data.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 0\n\n# Print the headline\nprint(\"Number of passenger with family\/group survival information: \" \n      +str(data[data['Family_Survival']!=0.5].shape[0]))","56275b6f":"# Reset index for remaining feature engineering steps\ndata = data.reset_index(drop=True)\ndata = data.drop('Survived', axis=1)\ndata.tail()","6f825a2c":"# Visualising fare data\nplt.hist(data['Fare'], bins=40)\nplt.xlabel('Fare')\nplt.ylabel('Count')\nplt.title('Distribution of fares')\nplt.show()","346eb197":"# Turning fare into 4 bins due to heavy skew in data\ndata['Fare'] = pd.qcut(data['Fare'], 4)\n\n# I will now use Label Encoder to convert the bin ranges into numbers\nlbl = LabelEncoder()\ndata['Fare'] = lbl.fit_transform(data['Fare'])","36fa053a":"# Visualise new look fare variable\nsns.countplot(data['Fare'])\nplt.xlabel('Fare Bin')\nplt.ylabel('Count')\nplt.title('Fare Bins')","1553c974":"# Inspecting the first five rows of Name\ntrain['Name'].head()","b2eb69f7":"#\u00a0New function to return name title only\ndef get_title(name):\n    if '.' in name:\n        return name.split(',')[1].split('.')[0].strip()\n    else:\n        return 'Unknown'","3dbbc587":"# Creating two lists of titles, one for each dataset\ntitles_data = sorted(set([x for x in data['Name'].map(lambda x: get_title(x))]))","1d57eb90":"#\u00a0Printing list length and items in each list\nprint(len(titles_data), ':', titles_data)","b283eac1":"#\u00a0New function to classify each title into 1 of 4 overarching titles\ndef set_title(x):\n    title = x['Title']\n    if title in ['Capt', 'Col', 'Don', 'Jonkheer', 'Major', 'Rev', 'Sir']:\n        return 'Mr'\n    elif title in ['the Countess', 'Mme', 'Lady','Dona']:\n        return 'Mrs'\n    elif title in ['Mlle', 'Ms']:\n        return 'Miss'\n    elif title =='Dr':\n        if x['Sex']=='male':\n            return 'Mr'\n        else:\n            return 'Mrs'\n    else:\n        return title","cef214f7":"# Applying the get_title function to create the new 'Title' feature\ndata['Title'] = data['Name'].map(lambda x: get_title(x))\ndata['Title'] = data.apply(set_title, axis=1)","69f3ce2d":"#\u00a0Printing values of the title column (checking function worked!)\nprint(data['Title'].value_counts())","22ec9228":"# Returning NaN within Age across Train & Test set\nprint('Total missing age data: ', pd.isnull(data['Age']).sum())","9196329d":"# Check which statistic to use in imputation\nprint(data['Age'].describe(exclude='NaN'))","4df83740":"# Imputing Age within the train & test set with the Median, grouped by Pclass and title\ndata['Age'] = data.groupby('Title')['Age'].apply(lambda x: x.fillna(x.median()))","66446541":"# Visualise new look age variable\nplt.hist(data['Age'], bins=40)\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.title('Distribution of ages')\nplt.show()","6a6d69d7":"# Turning data into 4 bins due to heavy skew in data\ndata['Age'] = pd.qcut(data['Age'], 4)\n\n# Transforming bins to numbers\nlbl = LabelEncoder()\ndata['Age'] = lbl.fit_transform(data['Age'])","139ba300":"# Visualise new look fare variable\nplt.xticks(rotation='90')\nsns.countplot(data['Age'])\nplt.xlabel('Age Bin')\nplt.ylabel('Count')\nplt.title('Age Bins')","e10aeca6":"data['Title'] = data['Title'].replace(['Mr', 'Miss', 'Mrs', 'Master'], [0, 1, 2, 3])","2c4fa0f0":"# Recoding sex to numeric values with use of a dictionary for machine learning model compatibility\ndata['Sex'] = data['Sex'].replace(['male', 'female'], [0, 1])","c5d6de93":"data['Embarked'].describe()","80656692":"# Filling in missing embarked values with the mode (S)\ndata['Embarked'] = data['Embarked'].fillna(data['Embarked'].mode()[0])\n\n# Converting to numeric values\ndata['Embarked'] = data['Embarked'].replace(['S', 'C', 'Q'], [0, 1, 2])","e5ab7393":"# Inspecting head of Cabin column\ntrain.head()","464ee83c":"#\u00a0Labelling all NaN values as 'Unknown'\ndata['Cabin'].fillna('Unknown',inplace=True)","ab5881f4":"# Extracting the first value in the each row of Cabin\ndata['Cabin'] = data['Cabin'].map(lambda x: x[0])","32b79712":"#\u00a0Return the counts of each unique value in the Cabin column\ndata['Cabin'].value_counts()","f80245f6":"# New function to classify known cabins as 'Known', otherwise 'Unknown'\ndef unknown_cabin(cabin):\n    if cabin != 'U':\n        return 1\n    else:\n        return 0\n    \n# Applying new function to Cabin feature\ndata['Cabin'] = data['Cabin'].apply(lambda x:unknown_cabin(x))","b8f4d1bf":"#\u00a0Creating two features of relatives and not alone\ndata['Family Size'] = data['SibSp'] + data['Parch']","39e343b3":"# Final look at the data\ndata.head()","f19c0084":"# Dropping what we know need for Machine Learning\ndata = data.drop(['Name', 'Parch', 'SibSp', 'Ticket', 'Last_Name', 'PassengerId'], axis = 1)","d6a3826c":"# Return to train\/test sets\ntrain = data[:ntrain]\ntest = data[ntrain:]","6183b76a":"# Set up feature and target variables in train set, and remove Passenger ID from test set\nX_test = test\nX_train = train\n\n# Scaling data to support modelling\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","f7e8b5f2":"# Initiate 11 classifier models\nran = RandomForestClassifier(random_state=1)\nknn = KNeighborsClassifier()\nlog = LogisticRegression()\nxgb = XGBClassifier()\ngbc = GradientBoostingClassifier()\nsvc = SVC(probability=True)\next = ExtraTreesClassifier()\nada = AdaBoostClassifier()\ngnb = GaussianNB()\ngpc = GaussianProcessClassifier()\nbag = BaggingClassifier()\n\n# Prepare lists\nmodels = [ran, knn, log, xgb, gbc, svc, ext, ada, gnb, gpc, bag]         \nscores = []\n\n# Sequentially fit and cross validate all models\nfor mod in models:\n    mod.fit(X_train, y_train)\n    acc = cross_val_score(mod, X_train, y_train, scoring = \"accuracy\", cv = 10)\n    scores.append(acc.mean())","833773bc":"# Creating a table of results, ranked highest to lowest\nresults = pd.DataFrame({\n    'Model': ['Random Forest', 'K Nearest Neighbour', 'Logistic Regression', 'XGBoost', 'Gradient Boosting', 'SVC', 'Extra Trees', 'AdaBoost', 'Gaussian Naive Bayes', 'Gaussian Process', 'Bagging Classifier'],\n    'Score': scores})\n\nresult_df = results.sort_values(by='Score', ascending=False).reset_index(drop=True)\nresult_df.head(11)","f3c6a44c":"# Plot results\nsns.barplot(x='Score', y = 'Model', data = result_df, color = 'c')\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')\nplt.xlim(0.80, 0.86)","9402332d":"# Function for new graph\ndef importance_plotting(data, x, y, palette, title):\n    sns.set(style=\"whitegrid\")\n    ft = sns.PairGrid(data, y_vars=y, x_vars=x, size=5, aspect=1.5)\n    ft.map(sns.stripplot, orient='h', palette=palette, edgecolor=\"black\", size=15)\n    \n    for ax, title in zip(ft.axes.flat, titles):\n    # Set a different title for each axes\n        ax.set(title=title)\n    # Make the grid horizontal instead of vertical\n        ax.xaxis.grid(False)\n        ax.yaxis.grid(True)\n    plt.show()","f98106b2":"# Building feature importance into a DataFrame\nfi = {'Features':train.columns.tolist(), 'Importance':xgb.feature_importances_}\nimportance = pd.DataFrame(fi, index=None).sort_values('Importance', ascending=False)","9b19ce99":"# Creating graph title\ntitles = ['The most important features in predicting survival on the Titanic: XGBoost']\n\n# Plotting graph\nimportance_plotting(importance, 'Importance', 'Features', 'Reds_r', titles)","3070145a":"# Building feature importance into a DataFrame\nfi = {'Features':train.columns.tolist(), 'Importance':np.transpose(log.coef_[0])}\nimportance = pd.DataFrame(fi, index=None).sort_values('Importance', ascending=False)","a7033829":"# Creating graph title\ntitles = ['The most important features in predicting survival on the Titanic: Logistic Regression']\n\n# Plotting graph\nimportance_plotting(importance, 'Importance', 'Features', 'Reds_r', titles)","bf1cfd97":"# Getting feature importances for the 5 models where we can\ngbc_imp = pd.DataFrame({'Feature':train.columns, 'gbc importance':gbc.feature_importances_})\nxgb_imp = pd.DataFrame({'Feature':train.columns, 'xgb importance':xgb.feature_importances_})\nran_imp = pd.DataFrame({'Feature':train.columns, 'ran importance':ran.feature_importances_})\next_imp = pd.DataFrame({'Feature':train.columns, 'ext importance':ext.feature_importances_})\nada_imp = pd.DataFrame({'Feature':train.columns, 'ada importance':ada.feature_importances_})\n\n# Merging results into a single dataframe\nimportances = gbc_imp.merge(xgb_imp, on='Feature').merge(ran_imp, on='Feature').merge(ext_imp, on='Feature').merge(ada_imp, on='Feature')\n\n# Calculating average importance per feature\nimportances['Average'] = importances.mean(axis=1)\n\n# Ranking top to bottom\nimportances = importances.sort_values(by='Average', ascending=False).reset_index(drop=True)\n\n# Display\nimportances","e78b91ce":"# Building feature importance into a DataFrame\nfi = {'Features':importances['Feature'], 'Importance':importances['Average']}\nimportance = pd.DataFrame(fi, index=None).sort_values('Importance', ascending=False)","9f0f1ee4":"# Creating graph title\ntitles = ['The most important features in predicting survival on the Titanic: 5 model average']\n\n# Plotting graph\nimportance_plotting(importance, 'Importance', 'Features', 'Reds_r', titles)","ee2356ff":"# Drop redundant features\ntrain = train.drop(['Embarked', 'Cabin'], axis=1)\ntest = test.drop(['Embarked', 'Cabin'], axis=1)\n\n# Re-build model variables\nX_train = train\nX_test = test\n\n# Transform\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","fb440389":"# Initiate models\nran = RandomForestClassifier(random_state=1)\nknn = KNeighborsClassifier()\nlog = LogisticRegression()\nxgb = XGBClassifier(random_state=1)\ngbc = GradientBoostingClassifier(random_state=1)\nsvc = SVC(probability=True)\next = ExtraTreesClassifier(random_state=1)\nada = AdaBoostClassifier(random_state=1)\ngnb = GaussianNB()\ngpc = GaussianProcessClassifier()\nbag = BaggingClassifier(random_state=1)\n\n# Lists\nmodels = [ran, knn, log, xgb, gbc, svc, ext, ada, gnb, gpc, bag]         \nscores_v2 = []\n\n# Fit & cross validate\nfor mod in models:\n    mod.fit(X_train, y_train)\n    acc = cross_val_score(mod, X_train, y_train, scoring = \"accuracy\", cv = 10)\n    scores_v2.append(acc.mean())","1d00af2f":"# Creating a table of results, ranked highest to lowest\nresults = pd.DataFrame({\n    'Model': ['Random Forest', 'K Nearest Neighbour', 'Logistic Regression', 'XGBoost', 'Gradient Boosting', 'SVC', 'Extra Trees', 'AdaBoost', 'Gaussian Naive Bayes', 'Gaussian Process', 'Bagging Classifier'],\n    'Original Score': scores,\n    'Score with feature selection': scores_v2})\n\nresult_df = results.sort_values(by='Score with feature selection', ascending=False).reset_index(drop=True)\nresult_df.head(11)","7c4e0b13":"# Plot results\nsns.barplot(x='Score with feature selection', y = 'Model', data = result_df, color = 'c')\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')\nplt.xlim(0.80, 0.86)","61121f0b":"# Parameter's to search\nCs = [0.001, 0.01, 0.1, 1, 5, 10, 15, 20, 50, 100]\ngammas = [0.001, 0.01, 0.1, 1]\n\n# Setting up parameter grid\nhyperparams = {'C': Cs, 'gamma' : gammas}\n\n# Run GridSearch CV\ngd=GridSearchCV(estimator = SVC(probability=True), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\n# Fitting model and return results\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","48b31d8d":"# Parameter's to search\nlearning_rate = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2]\nn_estimators = [100, 250, 500, 750, 1000, 1250, 1500]\n\n# Setting up parameter grid\nhyperparams = {'learning_rate': learning_rate, 'n_estimators': n_estimators}\n\n# Run GridSearch CV\ngd=GridSearchCV(estimator = GradientBoostingClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\n# Fitting model and return results\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","bcda0dce":"# Parameter's to search\npenalty = ['l1', 'l2']\nC = np.logspace(0, 4, 10)\n\n# Setting up parameter grid\nhyperparams = {'penalty': penalty, 'C': C}\n\n# Run GridSearch CV\ngd=GridSearchCV(estimator = LogisticRegression(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\n# Fitting model and return results\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","4323e4c9":"# Parameter's to search\nlearning_rate = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2]\nn_estimators = [10, 25, 50, 75, 100, 250, 500, 750, 1000]\n\n# Setting up parameter grid\nhyperparams = {'learning_rate': learning_rate, 'n_estimators': n_estimators}\n\n# Run GridSearch CV\ngd=GridSearchCV(estimator = XGBClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\n# Fitting model and return results\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","6fd1d345":"max_depth = [3, 4, 5, 6, 7, 8, 9, 10]\nmin_child_weight = [1, 2, 3, 4, 5, 6]\n\nhyperparams = {'max_depth': max_depth, 'min_child_weight': min_child_weight}\n\ngd=GridSearchCV(estimator = XGBClassifier(learning_rate=0.0001, n_estimators=10), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","86d33e50":"gamma = [i*0.1 for i in range(0,5)]\n\nhyperparams = {'gamma': gamma}\n\ngd=GridSearchCV(estimator = XGBClassifier(learning_rate=0.0001, n_estimators=10, max_depth=3, \n                                          min_child_weight=1), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","ed6f5f67":"subsample = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\ncolsample_bytree = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n    \nhyperparams = {'subsample': subsample, 'colsample_bytree': colsample_bytree}\n\ngd=GridSearchCV(estimator = XGBClassifier(learning_rate=0.0001, n_estimators=10, max_depth=3, \n                                          min_child_weight=1, gamma=0), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","66dabec9":"reg_alpha = [1e-5, 1e-2, 0.1, 1, 100]\n    \nhyperparams = {'reg_alpha': reg_alpha}\n\ngd=GridSearchCV(estimator = XGBClassifier(learning_rate=0.0001, n_estimators=10, max_depth=3, \n                                          min_child_weight=1, gamma=0, subsample=0.6, colsample_bytree=0.9),\n                                         param_grid = hyperparams, verbose=True, cv=5, scoring = \"accuracy\")\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","06c57ab0":"# Parameter's to search\nn_restarts_optimizer = [0, 1, 2, 3]\nmax_iter_predict = [1, 2, 5, 10, 20, 35, 50, 100]\nwarm_start = [True, False]\n\n# Setting up parameter grid\nhyperparams = {'n_restarts_optimizer': n_restarts_optimizer, 'max_iter_predict': max_iter_predict, 'warm_start': warm_start}\n\n# Run GridSearch CV\ngd=GridSearchCV(estimator = GaussianProcessClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\n# Fitting model and return results\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","221b0480":"# Parameter's to search\nn_estimators = [10, 25, 50, 75, 100, 125, 150, 200]\nlearning_rate = [0.001, 0.01, 0.1, 0.5, 1, 1.5, 2]\n\n# Setting up parameter grid\nhyperparams = {'n_estimators': n_estimators, 'learning_rate': learning_rate}\n\n# Run GridSearch CV\ngd=GridSearchCV(estimator = AdaBoostClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\n# Fitting model and return results\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","5c9ebb7f":"# Parameter's to search\nn_neighbors = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 16, 18, 20]\nalgorithm = ['auto']\nweights = ['uniform', 'distance']\nleaf_size = [1, 2, 3, 4, 5, 10, 15, 20, 25, 30]\n\n# Setting up parameter grid\nhyperparams = {'algorithm': algorithm, 'weights': weights, 'leaf_size': leaf_size, \n               'n_neighbors': n_neighbors}\n\n# Run GridSearch CV\ngd=GridSearchCV(estimator = KNeighborsClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\n# Fitting model and return results\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","f0c6749a":"# Parameter's to search\nn_estimators = [10, 25, 50, 75, 100]\nmax_depth = [3, None]\nmax_features = [1, 3, 5, 7]\nmin_samples_split = [2, 4, 6, 8, 10]\nmin_samples_leaf = [2, 4, 6, 8, 10]\n\n# Setting up parameter grid\nhyperparams = {'n_estimators': n_estimators, 'max_depth': max_depth, 'max_features': max_features,\n               'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n\n# Run GridSearch CV\ngd=GridSearchCV(estimator = RandomForestClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\n# Fitting model and return results\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","373f75d4":"# Parameter's to search\nn_estimators = [10, 25, 50, 75, 100]\nmax_depth = [3, None]\nmax_features = [1, 3, 5, 7]\nmin_samples_split = [2, 4, 6, 8, 10]\nmin_samples_leaf = [2, 4, 6, 8, 10]\n\n# Setting up parameter grid\nhyperparams = {'n_estimators': n_estimators, 'max_depth': max_depth, 'max_features': max_features,\n               'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n\n# Run GridSearch CV\ngd=GridSearchCV(estimator = ExtraTreesClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\n# Fitting model and return results\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","ed0b3cf4":"# Parameter's to search\nn_estimators = [10, 15, 20, 25, 50, 75, 100, 150]\nmax_samples = [1, 2, 3, 5, 7, 10, 15, 20, 25, 30, 50]\nmax_features = [1, 3, 5, 7]\n\n# Setting up parameter grid\nhyperparams = {'n_estimators': n_estimators, 'max_samples': max_samples, 'max_features': max_features}\n\n# Run GridSearch CV\ngd=GridSearchCV(estimator = BaggingClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\n# Fitting model and return results\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","037653c6":"# Initiate tuned models\nran = RandomForestClassifier(n_estimators=25,\n                             max_depth=3, \n                             max_features=3,\n                             min_samples_leaf=2, \n                             min_samples_split=8,  \n                             random_state=1)\n\nknn = KNeighborsClassifier(algorithm='auto', \n                           leaf_size=1, \n                           n_neighbors=5, \n                           weights='uniform')\n\nlog = LogisticRegression(C=2.7825594022071245,\n                         penalty='l2')\n\nxgb = XGBClassifier(learning_rate=0.0001, \n                    n_estimators=10,\n                    random_state=1)\n\ngbc = GradientBoostingClassifier(learning_rate=0.0005,\n                                 n_estimators=1250,\n                                 random_state=1)\n\nsvc = SVC(probability=True)\n\next = ExtraTreesClassifier(max_depth=None, \n                           max_features=3,\n                           min_samples_leaf=2, \n                           min_samples_split=8,\n                           n_estimators=10,\n                           random_state=1)\n\nada = AdaBoostClassifier(learning_rate=0.1, \n                         n_estimators=50,\n                         random_state=1)\n\ngpc = GaussianProcessClassifier()\n\nbag = BaggingClassifier(random_state=1)\n\n# Lists\nmodels = [ran, knn, log, xgb, gbc, svc, ext, ada, gnb, gpc, bag]         \nscores_v3 = []\n\n# Fit & cross-validate\nfor mod in models:\n    mod.fit(X_train, y_train)\n    acc = cross_val_score(mod, X_train, y_train, scoring = \"accuracy\", cv = 10)\n    scores_v3.append(acc.mean())","00175b2d":"# Creating a table of results, ranked highest to lowest\nresults = pd.DataFrame({\n    'Model': ['Random Forest', 'K Nearest Neighbour', 'Logistic Regression', 'XGBoost', 'Gradient Boosting', 'SVC', 'Extra Trees', 'AdaBoost', 'Gaussian Naive Bayes', 'Gaussian Process', 'Bagging Classifier'],\n    'Original Score': scores,\n    'Score with feature selection': scores_v2,\n    'Score with tuned parameters': scores_v3})\n\nresult_df = results.sort_values(by='Score with tuned parameters', ascending=False).reset_index(drop=True)\nresult_df.head(11)","a2525262":"# Plot results\nsns.barplot(x='Score with tuned parameters', y = 'Model', data = result_df, color = 'c')\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')\nplt.xlim(0.82, 0.86)","81f965a7":"#Hard Vote or majority rules w\/Tuned Hyperparameters\ngrid_hard = VotingClassifier(estimators = [('Random Forest', ran), \n                                           ('Logistic Regression', log),\n                                           ('XGBoost', xgb),\n                                           ('Gradient Boosting', gbc),\n                                           ('Extra Trees', ext),\n                                           ('AdaBoost', ada),\n                                           ('Gaussian Process', gpc),\n                                           ('SVC', svc),\n                                           ('K Nearest Neighbour', knn),\n                                           ('Bagging Classifier', bag)], voting = 'hard')\n\ngrid_hard_cv = model_selection.cross_validate(grid_hard, X_train, y_train, cv = 10)\ngrid_hard.fit(X_train, y_train)\n\nprint(\"Hard voting on train set score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100)) \nprint(\"Hard voting on test set score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))","5b67af6f":"grid_soft = VotingClassifier(estimators = [('Random Forest', ran), \n                                           ('Logistic Regression', log),\n                                           ('XGBoost', xgb),\n                                           ('Gradient Boosting', gbc),\n                                           ('Extra Trees', ext),\n                                           ('AdaBoost', ada),\n                                           ('Gaussian Process', gpc),\n                                           ('SVC', svc),\n                                           ('K Nearest Neighbour', knn),\n                                           ('Bagging Classifier', bag)], voting = 'soft')\n\ngrid_soft_cv = model_selection.cross_validate(grid_soft, X_train, y_train, cv = 10)\ngrid_soft.fit(X_train, y_train)\n\nprint(\"Soft voting on train set score mean: {:.2f}\". format(grid_soft_cv['train_score'].mean()*100)) \nprint(\"Soft voting on test set score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))","329132b4":"# Final predictions\npredictions = grid_soft.predict(X_test)\n\nsubmission = pd.concat([pd.DataFrame(passId), pd.DataFrame(predictions)], axis = 'columns')\n\nsubmission.columns = [\"PassengerId\", \"Survived\"]\nsubmission.to_csv('titanic_submission.csv', header = True, index = False)","65f9fa41":"There are two missing values for Embarked - let's replace it with the most frequently occurring value. I'll then convert the letters to numeric values.","bdec34d0":"## 4. Visual Data Exploration\nBefore we can know how much (or little) feature engineering is needed, we need to have a good sense of what we're working with. The simple explorations discussed above are useful in terms of getting a holistic view of the overall dataset. To understand more about specific features, it is considered best practice to visualise it first. Step 4 walks through some simple visualisations, beginning with a correlation matrix","fbd50226":"For this feature to work, the train\/test index had to be kept as is. For the remaining feature engineering steps this isn't essential so I will reset the index so we now have one continual index (1-1309), rather than 1-end of train set followed by 1-end of test set.","b019f630":"# Titanic: Machine Learning from Disaster\n\n## Table of contents\n\n- <b>Introduction<\/b>\n- <b>Step 1:<\/b> Import libraries & data\n- <b>Step 2:<\/b> Join data files\n- <b>Step 3:<\/b> Initial inspection\n- <b>Step 4:<\/b> Visual data exploration\n    - <i>Survived<\/i>\n    - <i>Pclass<\/i> \n    - <i>Age<\/i>\n    - <i>SibSp & Parch<\/i>\n    - <i>Fare<\/i>\n- <b>Step 5:<\/b> Feature Engineering\n    - <i>Family survival<\/i>\n    - <i>Fare<\/i>\n    - <i>Name<\/i>\n    - <i>Age<\/i>\n    - <i>Sex<\/i>\n    - <i>Embarked<\/i>\n    - <i>Cabin<\/i>\n    - <i>SibSp & ParCh<\/i>\n    - <i>Final look<\/i>\n- <b>Step 6:<\/b> Machine Learning\n    - <i>Initial models<\/i>\n    - <i>Feature selection<\/i>\n    - <i>Model tuning<\/i>\n    - <i>Voting Classifier<\/i>\n- <b>Step 7:<\/b> Final model predictions and submission","73c608ea":"#### Gradient Boosting Classifier\nNote: There are many more parameter's that could, and possibly should be tested here, but in the interest i've limited the tuning to establishing the appropriate learning_rate vs n_estimators trade off. The higher one value, the lower the other.","9f2f7839":"Round 3 results in and Gaussian Process remains our strongest model. I actually omitted parameters for a few models where I saw the score had dropped subsequent to specifying them - I presume this could be because I did not test the full set of parameters in those cases. So the Gaussian Process score remains as before but for the Extra Trees Classifier, it has shot right up into 2nd place after a 2% jump in accuracy. Aside from the Gaussian Naive Bayes, each model now predicts with over 84% accuracy which I am really pleased to see. A strong set of final models which I can now carry forward into the final step 4 - Voting Classifier.","011e6f8a":"Well it was pretty close, but for the test set the soft voting classifier came out just on top, so I will proceed to use this as my final model for prediction and submission. I'll complete that final step below before wrapping up:","c00b644b":"Every column that is either an int or a float can be 'described'. Taking the mean value of the 'Survived' column, we can see that <b>38%<\/b> of passengers (within the train set) survived when the Titanic sank.\n\nTo understand a little more around how much data is actually missing, the final step before some more visual EDA is conducted will be to inspect per feature the quantity of missing values:","d178c2c5":"## 7. Final model prediction & submission","ab793c2e":"## 1. Import libraries & data\n\nI will firstly begin by importing every library that will be used at somepoint throughout this project. ","dd64b5bb":"Group sizing looks good enough, i'm happy to continue with this!","9611c1c4":"# Introduction\n\nWelcome Kagglers! In this kernel, I will be working with the very famous Titanic dataset. It provides information on the fate of each passenger on board the vessel when it tragically sank on its maiden voyage in 1912. Inclusive in the file is further passenger information including age, sex, ticket type and cabin. The challenge set by Kaggle using this dataset is to build a predictive model that can determine whether or not an individual would have survived the 1912 tragedy.\n\nThis kernel is ideally pitched at relative beginners to Data Science & Machine Learning, who already have a grasp of the key concepts and a typical workflow and are now interested in learning some further (simple) tools and tricks in order to yield a strong competition score. At the time of submission, this kernel obtained a score (0.818) that pitched me within the top 5% of the leaderboard. It's now currently flitting between the top 5%\/6%. \n\nPlease ask any questions that you have on my code or approach, or suggestions if you think any part could be done better - I am always looking to improve! And if you found this kernel helpful, i'd very much like to hear it :). OK, the last thing to now say is: Enjoy the read!","f8e3c0b9":"Everything that I want to carry forward to the machine learning stage looks ready. There are some features remaining that, as previously discussed, I don't wish to use. The very last step will be to therefore remove these features.","c3683b88":"### Round 1: Initial models\nThe current problem is a classification problem, that is, the outcome can be classified into one class or the other (survived or not). Each of the below algorithms is applicable for such a problem and I'm interested to see how each of them perform on the current dataset, and which comes out on top. I would encourage to check the sci-kit learn documentation for a more detailed overview in regards to how each algorithm specifically works.\n\nWhat i'm going to do here is fit each of these 11 models in turn, before returning what's know as their cross-validated score. A model's performance is dependent on the way the data has been split between training and test data. This isn't really that representative of the model's ability to generalise, because there could be unique quirks within the train (or test) set which the model either learns (or does not learn) incorrectly. In order to gain a smooth out representation of the full dataset, K fold cross validation can be applied. It works in the following way: \n\nRandomly split your entire dataset into k 'folds'. \n- For each k folds in your dataset, build your model on k \u2013 1 folds of the data set. Then, test the model to check the effectiveness for kth fold. \n- Record the error you see on each of the predictions. \n- Repeat this until each of the k folds has served as the test set. \n\nThe average of your k recorded errors is called the cross-validation error and will serve as your performance metric for the model. I am going to use 10 folds in this example, which will be plenty.","e0e5b3b3":"##\u00a05. Feature engineering\nWith a clearer understanding about the current data shape, we can now start engineering features so they're ready for modelling. Some steps will be things we need to do, e.g. filling in blanks. Other steps are more choice in terms of making features more useful, thus allowing for stronger model performance. I'll begin with one such feature.","1306481c":"It looks clear now that Embarked & Cabin really aren't helping us out, and therefore I am going to get rid of them. If I was looking to prioritise 1 or 2 models rather than 11, I would also consider removal of Sex, Fare and potentially Pclass too (depending on which were the models I was considering). However we can see from the above table that for some algorithms, each feature has an important part to play, and I want to prioritise strong performance across the board in preparation for the Voting Classifier (more on that later). So for now it's just the two for the chop.","67259cda":"#### Gaussian Naive Bayes\nGaussian Naive Bayes doesn't have parameters to tune, so we're stuck with the current score. This algorithm is known to be designed to work best on text data (e.g. once passed into a matrix), so perhaps in comparison to the other algorithms, it's less of a surprise to see it performing less favourably on the Titanic dataset.","ac92dee7":"#### Adaboost","7a5d4b2e":"#### Survived","2a1d725e":"#### SVC","5c8e00d9":"### Round 3: Model (hyper-parameter) tuning\nWithin most machine learning algorithm exist a number of parameters that together can be fine tuned to produce the most accurate prediction on a given dataset. This is essentially what Hyperparamter tuning is - finding the best parameters for your model. The best way to achieve this, while computationally expensive, is to use a GridSearchCV. \n\nGridSearchCV is an exhaustive search over specified parameter values for an estimator, in order to find the values for optimum model performance. Each model has its own parameter's, so the Grid Search needs to be specific for each model. Below, I will complete a GridSearchCV for each model, specifying search ranges for each model's most important parameters, in order to find the one's that yield the highest accuracy score for the Titanic train set. \n\nFYI, if you're forking this kernel, it will take a while to run. I've kept my GridSearch relatively light touch in all, but expect to wait around 30 minutes from start to finish.\n\nMore information on each model's parameter's can be found online. If you're also wondering how I arrived at suitable ranges for each parameter, I used a combination of Google, past experience and trial and error. That's it :).","60c9c917":"#### Pclass","b0432f75":"Rather than feature importances, Logistic Regression uses coefficients which actually aid better real world interpretation. There are both positive & negative impact features on display, this time with Title being most useful. Sex is also a strong feature for this model, in stark contrast to XGB. Those in the middle (Embarked, Fare, Cabin) all had largely no part to play except for possibly creating unhelpful 'noise' in the dataset. Noise weakens the model's ability to find patterns clearly, and can in some cases lead to lower overall performance. Embarked & cabin are two repeat offenders, and as such go onto my watch list!","910af955":"Confirmation of what was seen in the correlation matrix that a trend is visible between ticket class and survival chance. The higher class ticket, the more likely one is to have survived. This will become a very handy predictor in the machine learning algorithm.","159b5480":"##\u00a06. Machine Learning\nBefore we can fit models, a few more steps are needed in order to get the data in the correct shape for modelling. This involves re-splitting the train & test datasets, followed by setting up our X_train & X_test variables. Note that we already have our y_train variable from before. We don't have a y_test variable, this would be the survival stat per users in the test set, and this is what we are looking to predict!\n\nI am also going to scale the data using the StandardScaler tool. This isn't a requirement for every algorithm, but for those that use what's known as the euclidean distance (or straight line distance) in order to make predictions, having features that all operate on different length scales will unfairly skew the model's interpretation of the data. So, StandardScaler aligns all features onto the same scale and thus avoids us running into this issue.","893bdec8":"## 3. Initial inspection","0fce3177":"Now onto the fun stuff, training our cleaned up data. I am going to tackle the modelling in a series of steps which are hopefully easy to follow:\n\n1. <b>Train initial models<\/b>\n2. <b>Remove features and re-train<\/b>\n3. <b>Tuned parameters and re-train<\/b>\n4. <b>Build into a Voting Classifier and predict.<\/b>\n\nI'll explain more about each step as I go along!","e842a919":"And there we go! In this kernel I have taken several steps to analyse, clean and engineer features before applying and fine-tuning a selection of classification models for a final test set score of ~86%. I have hopefully been clear with my code and sufficient in my explanation so that you've been able to read along and enjoy the ride with me. I do intend to return to this kernel to add in more explanation where I can do and, because I am a sucker for a competition, see if I can edge myself up the leaderboard any further. Some thoughts on how I could do this include:\n\n- Exploring new features in the data\n- Using a predictive model to impute age, rather than a simple groupby operation\n- Investing more time in parameter tuning and model optimisation\n- Prioritising fewer models and thus cutting more redundant features\n- Exploring deep learning application\n\nAs mentioned at the beginning, I would love to hear your feedback on my work, including what you found helpful or what perhaps needs clearer explanation. I'll do my best to help! Thanks for reading, and assuming that you are also participating in this competition - good luck!","202ea8f2":"#### Fare","d2aff2e3":"Great stuff, some big shifters on display. Gaussian Process now heads the pack over Gradient Boosting and the KNN model is now also mixing it up at the top, whereas prior to feature selection it was languishing down at the bottom. In all, 9\/11 models have improved in score, with XGBoost remaining static and Adaboost taking a slight knock. That's a very favourable from something as simple as removing two features. We can now progress to round 3 - model tuning.","62adc285":"Well, that's a fare from ideal view! There is quite a severe left-side skew which probably won't pair up all that well with Machine Learning algorithms. I think there's two possible approaches here.\n\n1. Turn Fare into categorical data, by breaking it down into bins\n2. Transform the data so it fits in with a normal distribution (e.g. log transformation could work here)\n\nI played around with both options, in the end deciding on using bins was the most effective means of proceeding. I also achieved a better final score using the bin approach. To create my bins, i'm going to use the clever Pandas qcut tool, which creates equal size bins based on the quantity chosen (in this case, 4).","422a5881":"#### Family information","36f454e0":"### Round 2: Feature selection\nWe have made good progress so far and have a few particularly strong performing models, however, if we did we'd be failing to harness the true potential of each model's ability to predict on this dataset. The following steps will walk through how we can optimise our 'baseline' predictions to yield a stronger overall score (and ultimately climb further up the leaderboard).\n\nThe first step is feature selection, and by that I mean carry forward only helpful features and remove those that are not contributing towards model performance. This step I anticipate should help to unshackle some currently weak performing models (i'm looking at the KNN model when I say this).\n\nTo get a better gist in terms of what needs the boost, i'm going to get a collective view of all models that will give us a 'feature importance' list. We can then compare these values and draw a cut-off for the features that, overall, provides the least amount of help.","7b0ad6e2":"#### Logistic Regression","7268ebb7":"### Embarked","0b8efdb1":"## 2. Join data files\nIt's helpful to bring the train and test files together for all steps pre-modelling. That way, we can inspect and edit features within one line of code rather than having duplicte every action over both the train and test set individually. The below code completes this combination.","dfb7e658":"There's a reason I chose to look at Age after Name, that's because I'm going to use the new Title feature to calculate the missing the Age values - a technique called 'imputation'. Imputing can either be completed by the Mean or Median. Let's use the below information to help decide on which might be most accurate.","68218793":"### Cabin","cff5aeb6":"#### XGBoost\n##### Step 1\nGradient boosting algorithms are best tuned sequentially - it's less expensive and can yield better results. I'll demonstrate the approach for XGBoost. I could have done the same for the Gradient Boosting classifier, but it's a slower algorithm to run and I am obviously too impatient! Each time I find the appropriate parameter value, i'll specify this within my model in the subsequent testing step.","e27dce39":"Not surprisingly, the structure of these two graphs appear similar, with a similar density of passengers featured within each count, with also a similar ratio of survived vs not survived. This adds further rationale for these two features to be combined, which will be performed at the Data Preprocessing stage.","97046559":"A quick inspection into the survived feature reveals that as before seen, 38% of passengers within the training set survived when the Titanic sank. This equates to 342 passengers out of 891 in total.","b51e5abb":"#### Bagging Classifier","99700951":"And that's the lot - onto machine learning!","28e4ebee":"### Round 4: Voting Classifier\nVoting is one of the simplest ways of combining the predictions from multiple machine learning algorithms.\n\nIt works by first creating two or more standalone models from your training dataset. A Voting Classifier can then be used to wrap your models and average the predictions of the sub-models when asked to make predictions for new data.\n\nThere are two types of Voting Classifier, hard or soft. I saw a very simple explanation on a Q&A forum which i'll share with you now to explain the difference in approach:\n\n- Suppose you have probabilities: 0.45 0.45 0.90\n- The hard voting would give you a score of 1\/3 (1 vote in favour and 2 against), so it would classify as a \"negative\".\n- Soft voting would give you the average of the probabilities, which is 0.6, and would be a \"positive\".\n\nThe code below completes both a hard and soft voting classifier on all 11 models, minus the Gaussian Naive Bayes which, due to it's poorer score, I will omit from this final step. I will then compare the scores of either classifier, before selecting the best to proceed with making final predictions.","fddf7c42":"Fare has been displayed per ticket type, revealing that those within Pclass 3 paid a similar fare to those in Pclass 2, but their chance of survival appears to be a lot lower. Pclass contains the highest fares, along with the highest rate of survivial, denoted by the higher ratio of orange points.","bee78d3d":"### Final look","90b14a7e":"##### Step 2","59d45d25":"The full names as they are will not be helpful to us, although, there's probably something useful within title e.g. categorising males and females, boys and girls. Therefore, i'm going to extract this data and create a new feature for Title, before binning Name.","b5253fc4":"#### Group information","c434e926":"#### Age","a402fd3d":"### Family survival\nI must reference this [kernel](http:\/\/www.kaggle.com\/shunjiangxu\/blood-is-thicker-than-water-friendship-forever), which is credited with introducing this feature:\n\nThe aim of this feature is to group together people (usually families) with similar ticket information, with the logic that groups that are together have similar survival chances. I encourage you to follow the link to learn more about this feature, as the code is quite complex. In my opinion it seemed an interesting and logical feature to create, so i'm adding it in my dataset.","a645fa41":"So Age and Cabin are the main culprits - we'll come back to these a little later on.","53f26ad6":"#### Random Forest","21672630":"The league table is out and we have a winner in Title. Maybe this isn't so surprising given within title information on both age and gender is available. I find it interesting to see that where some models lean very heavily on a certain feature, others gain barely any value from it. The variance in Sex and Age is particularly striking. Let's get this data int a final graph before making a decision on which to cull.","ae5137e2":"### Sex\nA simple step for Sex, just recoding to numbers for Machine Learning.","d7f26c5b":"### SibSp & Parch\n\nIt would make sense that these two features were combined into one, so that's exactly what I'll do. Nice and simple!","716539c0":"#### SibSp & ParCh","c49291e7":"Perhaps not the most insightful view at this stage given that some features are pending engineering, however a visible correlation does exist between Survived and Pclass and Fare. Age, SibSp & Parch would also seem like logical predictors and it would be expected that after these variables have been preprocessed their correlation to Survived will increase.\n\nLet's get an initial sense of these features then, beginning with the target: Survived.","54321629":"The mean and percentile breakdown indicates multiple features converging around the 30 mark, which perhaps isn't surprising. Based on this it may be better to proceed with imputing with the median (middle) value. What i'm now going to do is group the dataset by the four different titles, and then impute the missing age values with the average age of each title, be that Mr, Mrs, Master or Miss. The below code completes this:","a0899158":"You may already know this, however for those who may not, datasets on Kaggle are typically split into two separate files, a train set and a test set. We 'train' and optimise our predictive model on the train set, holding back the test set data for final prediction and submission. We want our model to generalise well to new, never before seen data, hence the test set provides this barometer to us. If our model could predict accurately on the train set, but then fails to generalise to new data (e.g. it predicts poorly), this is a sign that it has 'overfit' to the unique characteristics of the train data. We don't the know 'Survived' labels of the test set, and that's what we're here to find out. If our model can't generalise to new data, it won't be very good in predicting these new labels for us. Therefore, the model won't be very useful!","34befe85":"##### Step 3","54736330":"We can initially see a mixture of numbers (float & int) & words (objects) in this dataset, and some missing values that we'll need to take care of later. Machine Learning does not take kindly to missing values, that is, it will not work. So we'll need to fill them in somehow.\n\nFor reference, here's a description of what each feature contains:\n\n<b> - PassengerId:<\/b> The unique identifier in this Data file <br>\n<b> - Survived:<\/b>    The fate of each passenger (target) <br>\n<b> - Pclass:<\/b>      The ticket class <br>\n<b> - Name:<\/b>       The passenger name <br>\n<b> - Sex:<\/b>         The passenger sex <br>\n<b> - Age:<\/b>         The passenger age in years   \n<b> - Sibsp:<\/b>       The number of siblings\/spouses also travelling <br>\n<b> - Parch:<\/b>       The number of parents\/children also travelling <br>\n<b> - Ticket:<\/b>      The passenger ticket number <br>\n<b> - Fare:<\/b>        The passenger fare <br>\n<b> - Cabin:<\/b>       The passenger cabin number <br>\n<b> - Embarked:<\/b>    The passenger's port of Embarkation <br>","a1f146c5":"#### Gaussian Process","ec7b43d3":"##### Step 4","68c0a167":"More can be understood about age when plotted alongside sex. These graphs reveal that overall women were much more likely to survive than men, and this is largely regardless of age. For both sexes, it appears that chances of survival are more likely at a younger age, which is what might have been expected. From the age of 20, it was consistently more likely that men would not have survived, up until their age approached 80. For women, apart from a potentially anomalous finding around the 8-9 bracket, they were always more likely to survive.","d51330c7":"### Model re-training\nWe're into round three now, and hopefully optimising more of our models and really squeezing every last drop of predictive out of the dataset. I will run the same procedure as before and stack the latest results against the previous two. The only change now is that I can specify exact parameters as per the above GridSearchCV results to yield even better performance scores.","52d2ba7c":"To understand better what we are now working with, the list size and values will be printed below.","37904524":"The above for loop has packed my 11 model cross validated scores into the list 'score'. I'm now going to unpack this data into a table first, then graph to learn the results.\n\nNote at this time that our scoring metric is 'accuracy'. This is the competition requirement and is simply a percentage count of the correct classifications provided. For classification problems, however, it's not always best practice to use accuracy as your scoring metric. Consider a dataset where 80% of the target is class A, and the remaining 20% is class B. An accuracy score of 80% on this dataset might on face value sound positive, however, the classifier could have simply predicted every observation as class A. There are 80% in class A in total, thus it is 80% correct. This model clearly lacks any real use, but a mis-leading accuracy score could prevent that being so easily spotted. Metrics such as precision & recall or the roc-auc score can be used as alternative scoring metrics for classification problems. I'll spare the description here, but have a Google if you're interested in learning more.","0d124afe":"So now we have a completed view of Age, it makes sense to visualise it. The distribution isn't bad, but I'm not totally satisfied - a slight left skew can still be seen. I'm going to follow the approach used for the Fare feature by turning Age into bins.","8bf1e4ba":"I will stick with these bins for now. Now that I'm finished using the Title feature, I will round up by transferring the titles over to numbers ready for Machine Learning.","b1d95c34":"##### Step 5","dbfadb4d":"Great, the index runs from 1-1309. Let's progress with engineering features.","5e5d76ac":"Round 1 complete and it is the Gradient Boosting algorithms that come out top. This isn't a surprise - Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function. They are typically regarded as best in class predictors and they form the basis on many winning competition models.\n\nI want to now see how heavily each feature was leaned in the modelling process. Let's look at what feature XGBoost found most useful when achieved the top score in round one. To help present this data i'm going to construct my own gragh - code below.","8fc9ad2b":"We're now into Round 2, i will complete the same steps as above and then stack our new results against the previous results, hopefully showing the increments gained from feature selection.","99c0e977":"### Age\nAge has some missing values, as was seen earlier when initially inspecting the data. Let's recap on the current state-of-play:","8bc44b0a":"### Name","9a81b180":"As previously seen, there is an overwhelming majority of unknown Cabins in the train dataset. Based on this, the best option here might be to create two groups: known and unknown. This will avoid overfitting on the sparse data by cabin level, and, with the help of a new function, is what will be computed next.","607f257d":"18 unique title values is a lot, and I anticipate that for many only a few observations exist, which isn't helpful. I'm going to keep this simple and band titles in one of four categories: Mr, Mrs, Master & Miss. To help me complete this I will define my own handy function - see below:","17b62c09":"### Model re-training","df6d6e08":"### Fare\nThe single missing fare value was taken care of previously, so let's take a look at fare overall and see whether we could or should take any further action.","e3ab79d6":"Always the most interesting - in my opinion anyway! The imputed age feature went down a treat with XGBoost, followed closely by the new Family Survival feature. I'm pretty happy with that. Sex, Cabin & Embarked were a little more, well, useless shall we say. Scores so low indicates that these features more likely hindered rather than helped in the model prediction. Let's check out a different top performing algorithm.","04e52fb6":"#### Extra Trees","81115b8f":"Evidence of missing values is straight away evident in the 'cabin' column - NaN (not a number) indicates this. Ticket type also looks like a bit of a mish\/mash - initial thoughts are that this might prove a difficult feature to engineer anything meaningful out of. ","e571e5c5":"Upon closer inspection into cabin, we can see that it follows a Letter\/Number format. A bit of extra internet research reveals that the letter actually refers to the floor in the titanic where each passenger resided. This  information may be helpful in the prediction, e.g. did those in lower cabins have a smaller\/larger chance of survival? Therefore we will begin by extracting the letter only from the Cabin column, and then labelling all NaN's with an 'Unknown' cabin reference.","c9e5e22f":"#### K Nearest Neighbours"}}