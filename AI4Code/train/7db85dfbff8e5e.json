{"cell_type":{"87c0dc95":"code","28b054b3":"code","793192b1":"code","aa9f0430":"code","93f8b703":"code","8fad974e":"code","bcc76cd8":"code","59eea062":"code","51c2f637":"code","13fa2103":"code","9cb2de0f":"code","cb2f0840":"code","9a92f60e":"code","bc59ddf3":"code","b71f9b82":"code","56961e5c":"markdown","9a99a075":"markdown","980d0a03":"markdown","36d1ebcf":"markdown","664e3b43":"markdown","ec019a91":"markdown","5701a74a":"markdown","e1fa94bc":"markdown","ee00543b":"markdown","4ed554b0":"markdown","940202e1":"markdown","457b80cb":"markdown","1e745c68":"markdown","cd15f37b":"markdown"},"source":{"87c0dc95":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, accuracy_score, recall_score, roc_curve, precision_recall_curve, auc\nfrom sklearn.tree import DecisionTreeClassifier\nimport warnings","28b054b3":"np.random.seed(1)\n\nwarnings.filterwarnings(\"ignore\")\n\ndata = pd.read_csv(\"..\/input\/auto-insurance-claims-data\/insurance_claims.csv\")\n\ndrop_columns = [\"_c39\", \"auto_model\", \"policy_bind_date\", \"policy_state\", \"incident_date\",\n               \"incident_state\", \"incident_city\", \"incident_location\", \"policy_csl\"]\n\ndata = data.drop(drop_columns, axis=1)\n\nnew_response = []\nresponse = data.iloc[:, -1]\nfor i in range(len(response)):\n    new_response.append(1 if response[i]=='Y' else 0)\n\ndata[\"fraud_reported\"] = pd.Series(new_response)","793192b1":"plt.hist(data.age)\nplt.title(\"Histogram of age of the customers\")\nplt.xlabel(\"Age of the customers\")\nplt.ylabel(\"Number of customers\")","aa9f0430":"plt.hist(data.insured_sex)\nplt.title(\"Histogram of gender count of the customers\")\nplt.xlabel(\"Gender\")\nplt.ylabel(\"Number of customers\")","93f8b703":"plt.hist(data.fraud_reported)\nplt.title(\"Histogram of fraud reported\")\nplt.xlabel(\"response\")\nplt.ylabel(\"Number of responses\")","8fad974e":"sns.heatmap(data.corr())\nplt.show()","bcc76cd8":"predictors = data.iloc[:,:-1]\nresponse = data.iloc[:, -1]\n\n# new_response = []\n\n# for i in range(len(response)):\n#     new_response.append(1 if response[i]=='Y' else 0)\n    \n# response = pd.Series(new_response)\n\ncategorical_data = predictors.select_dtypes(exclude=\"number\")\ncategorical_predictors = categorical_data.columns\n\npredictors = predictors.drop(categorical_predictors, axis=1)","59eea062":"one_hot_data = pd.get_dummies(categorical_data)\npredictors = predictors.join(one_hot_data)\n\npredictor_columns = predictors.columns\nresponse_columns = response\n\npredictors_train, predictors_test, response_train, response_test = train_test_split(predictors,\n                                                                                    response,\n                                                                                    test_size=0.3)","51c2f637":"sm = SMOTE(random_state=24)\npredictors, response = sm.fit_resample(predictors_train, response_train)\n\npredictors_train = pd.DataFrame(predictors, columns=predictor_columns)\nresponse_train = pd.Series(response)\n\nmodel_preds = {}","13fa2103":"model = LogisticRegression()\nmodel.fit(predictors_train, response_train)\npredictions_test = model.predict(predictors_test)\npredictions_train = model.predict(predictors_train)\n\nconf_matrix = confusion_matrix(predictions_test, response_test)\nplot_confusion_matrix(conf_matrix)\n\nprecision = precision_score(predictions_test, response_test)\nrecall = recall_score(predictions_test, response_test)\n\nprint(\"*****************************************\")\nprint(\"Results on testing data:\")\nprint(\"*****************************************\")\nprint(\"Accuracy = \"+str(accuracy_score(predictions_test, response_test)))\nprint(\"Precision = \"+str(precision))\nprint(\"Recall = \"+str(recall))\n\ntpr, fpr, threshold = roc_curve(predictions_test, response_test, pos_label=1)\nmodel_preds[\"Logistic Regression\"] = [tpr, fpr]\nprint()\nprint(\"AUC value = \"+str(auc(tpr, fpr)))","9cb2de0f":"knn = KNeighborsClassifier()\nknn.fit(predictors_train, response_train)\n\npredictions_train = knn.predict(predictors_train)\npredictions_test = knn.predict(predictors_test)\n\nconf_matrix = confusion_matrix(predictions_test, response_test)\nplot_confusion_matrix(conf_matrix)\n\nprecision = precision_score(predictions_test, response_test)\nrecall = recall_score(predictions_test, response_test)\n\nprint(\"*****************************************\")\nprint(\"Results on testing data:\")\nprint(\"*****************************************\")\nprint(\"Accuracy = \"+str(accuracy_score(predictions_test, response_test)))\nprint(\"Precision = \"+str(precision))\nprint(\"Recall = \"+str(recall))\n\ntpr, fpr, threshold = roc_curve(predictions_test, response_test, pos_label=1)\nmodel_preds[\"K Nearest Neighbor\"] = [tpr, fpr]\nprint()\nprint(\"AUC value = \"+str(auc(tpr, fpr)))","cb2f0840":"## Since it has a lot of categorical variables and the dataset is also not huge we \n## will use decision trees to get more accuracy.\n\npredictors_train, predictors_test, response_train, response_test = train_test_split(predictors,response,test_size=0.3)\n\ntree = DecisionTreeClassifier()\ntree.fit(predictors_train, response_train)\npredictions_test = tree.predict(predictors_test)\npredictions_train = tree.predict(predictors_train)\n\nconf_matrix = confusion_matrix(predictions_test, response_test)\nplot_confusion_matrix(conf_matrix)\n\nprecision = precision_score(predictions_test, response_test)\nrecall = recall_score(predictions_test, response_test)\n\nprint(\"*****************************************\")\nprint(\"Results on testing data:\")\nprint(\"*****************************************\")\nprint(\"Accuracy = \"+str(accuracy_score(predictions_test, response_test)))\nprint(\"Precision = \"+str(precision))\nprint(\"Recall = \"+str(recall))\n\ntpr, fpr, threshold = roc_curve(predictions_test, response_test, pos_label=1)\nmodel_preds[\"Decision Tree\"] = [tpr, fpr]\nprint()\nprint(\"AUC value = \"+str(auc(tpr, fpr)))","9a92f60e":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier()\nrandom_forest.fit(predictors_train, response_train)\npredictions_test = random_forest.predict(predictors_test)\npredictions_train = random_forest.predict(predictors_train)\n\nconf_matrix = confusion_matrix(predictions_test, response_test)\nplot_confusion_matrix(conf_matrix)\n\nprecision = precision_score(predictions_test, response_test)\nrecall = recall_score(predictions_test, response_test)\n\nprint(\"*****************************************\")\nprint(\"Results on testing data:\")\nprint(\"*****************************************\")\nprint(\"Accuracy = \"+str(accuracy_score(predictions_test, response_test)))\nprint(\"Precision = \"+str(precision))\nprint(\"Recall = \"+str(recall))\n\ntpr, fpr, threshold = roc_curve(predictions_test, response_test, pos_label=1)\nmodel_preds[\"Random Forest\"] = [tpr, fpr]\nprint()\nprint(\"AUC value = \"+str(auc(tpr, fpr)))","bc59ddf3":"lda = LinearDiscriminantAnalysis()\nlda.fit(predictors_train, response_train)\npredictions_test = lda.predict(predictors_test)\npredictions_train = lda.predict(predictors_train)\n\nconf_matrix = confusion_matrix(predictions_test, response_test)\nplot_confusion_matrix(conf_matrix)\n\nprecision = precision_score(predictions_test, response_test)\nrecall = recall_score(predictions_test, response_test)\n\nprint(\"*****************************************\")\nprint(\"Results on testing data:\")\nprint(\"*****************************************\")\nprint(\"Accuracy = \"+str(accuracy_score(predictions_test, response_test)))\nprint(\"Precision = \"+str(precision_score(predictions_test, response_test)))\nprint(\"Recall = \"+str(recall_score(predictions_test, response_test)))\n\ntpr, fpr, threshold = roc_curve(predictions_test, response_test, pos_label=1)\nmodel_preds[\"Linear Discriminant Analysis\"] = [tpr, fpr]\nprint()\nprint(\"AUC value = \"+str(auc(tpr, fpr)))","b71f9b82":"plt.title(\"ROC curve for various classifiers:\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\n\nfor key, value in model_preds.items():\n    model_list = model_preds[key]\n    plt.plot(model_list[0], model_list[1], label=key)\n    plt.legend()\nplt.show()","56961e5c":"- We can see that male and female are almost in the same proportion.","9a99a075":"# Decision Trees","980d0a03":"- Since most of our data is categorical we have two options, assign a integer value to each level of the categorical variable or one-hot encode these categorical variables. \n\n- One major drawback of assigning integer value to each level is that it adds additional charecteristics to the data. For example let's say we have a variable with levels as BMW, Mazda, Mercedes and Subaru and we are assigning 0, 1, 2 and 3 integer values to them respectively. When we apply any model the model considers these values as continuous and assumes an unwanted hierarchy like BMQ < Mazda < Mercedes < Subaru, which might not be the case at all.\n\n- Hence we go with one hot encoding of categorical variables. ","36d1ebcf":"We will be dropping insignificant features like: \n\n- policy_bind_date\n- policy_state\n- incident_date\n- auto_model\n- _c39\n- policy_csl","664e3b43":"# Logistic Regression","ec019a91":"# Receiving Operator Charecteristic","5701a74a":"- As we can see from the above ROC curves and results LDA is performing well when compared to all the classifiers.\n- KNN is performing the worst out of all the classifiers.\n- I was hoping to get better results with Random Forests but with this size of the data I am not surprised with this result. ","e1fa94bc":"# Car Insurance Fraud Claim Detection","ee00543b":"# Linear Discriminant Analysis","4ed554b0":"# Performing initial EDA.","940202e1":"- Since we have class imbalance in the data, we perform minority class oversampling using SMOTE, Synthetic Minority Oversampling Technique, which uses K nearest neighbors to come up with new samples in the minority class.","457b80cb":"# Random Forest Classifier","1e745c68":"# K Nearest Neighbors","cd15f37b":"- We can also see that there is a significant class imbalance, we will be using SMOTE, Synthetic Minority Oversampling Technique to add additional minority class data points."}}