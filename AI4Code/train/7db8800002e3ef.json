{"cell_type":{"26801f63":"code","4b729e4d":"code","ebf8fca4":"code","95b49b91":"code","04b17ff1":"code","660e43a6":"code","0cc983bc":"code","62005fc3":"code","afd721e0":"code","e298a942":"markdown","108cd1d3":"markdown","3499e647":"markdown","091570f0":"markdown","4e7901f6":"markdown","b42ec28c":"markdown","fed757b9":"markdown","fe01e529":"markdown","1e4bf113":"markdown","af33cfc8":"markdown"},"source":{"26801f63":"# Clear previous memories\nfrom IPython import get_ipython\nget_ipython().magic('reset -sf')\n\n# Import necessary libraries\nimport pandas as pd\n\n# Import the dataset\ndata_calendar = pd.read_csv('..\/input\/m5-forecasting-uncertainty\/calendar.csv')\ndata_sales_train_evaluation = pd.read_csv('..\/input\/m5-forecasting-uncertainty\/sales_train_evaluation.csv')\ndata_sales_train_validation = pd.read_csv('..\/input\/m5-forecasting-uncertainty\/sales_train_validation.csv')\ndata_sample_submission = pd.read_csv('..\/input\/m5-forecasting-uncertainty\/sample_submission.csv')\ndata_sell_prices = pd.read_csv('..\/input\/m5-forecasting-uncertainty\/sell_prices.csv')","4b729e4d":"# Shape of the datasets\nprint('data_calendar \\nShape: ', data_calendar.shape,)\nprint('data_sales_train_evaluation \\nShape: ', data_sales_train_evaluation.shape)\nprint('data_sales_train_validation \\nShape: ', data_sales_train_validation.shape)\nprint('data_sample_submission \\nShape: ', data_sample_submission.shape)\nprint('data_sell_prices \\nShape: ', data_sell_prices.shape)","ebf8fca4":"#print('data_calendar \\nShape: ', data_calendar.shape, '\\n', data_calendar.head())\n#print('---------------\\ndata_sales_train_evaluation \\nShape: ', data_sales_train_evaluation.shape, '\\n', data_sales_train_evaluation.head())\nprint('---------------\\ndata_sales_train_validation \\nShape: ', data_sales_train_validation.shape, '\\n', data_sales_train_validation.head())\n#print('---------------\\ndata_sample_submission \\nShape: ', data_sample_submission.shape, '\\n', data_sample_submission.head())\n#print('---------------\\ndata_sell_prices \\nShape: ', data_sell_prices.shape, '\\n', data_sell_prices.head())","95b49b91":"# print('---------------\\ndata_sales_train_evaluation \\nShape: ', data_sales_train_evaluation.shape, '\\n', data_sales_train_evaluation.head())\n# I know it almost does not make sense to build the following dataset, but I am doing it just to implement the NN I learnt from the \"Intro to Deep Learning\" course.\n# I am just slicing last column as response & previous 10 columns as predictors\npredictors = data_sales_train_evaluation.iloc[:,1936:1945]\npredictors.head()\nresponse = data_sales_train_evaluation.iloc[:,1946]","04b17ff1":"# Import necessary modules\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\n# Save the number of columns in predictors: \nn_cols = predictors.shape[1]\n# Set up the model: model\nmodel = Sequential()\n# Add the first layer\nmodel.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n# Add the second layer\nmodel.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n# Add the output layer\nmodel.add(Dense(1, input_shape=(n_cols,)))","660e43a6":"# fitting a model \n# Applying backpropagation and gradient descent with your data to update the weights.\nmodel.compile(optimizer='sgd', loss='mean_squared_error')\nmodel.fit(predictors, response)\n# Scale the data before fitting can ease optimization\nmodel.summary()","0cc983bc":"model_1_training = model.fit(predictors, response, epochs=30, validation_split=0.2, verbose=False)\nmodel_1_training","62005fc3":"# But technically we may not need 10 epoch or may be more epoch. Running extra epoch is definitely computationally costly. \n# Lets perform Early Stopping\nfrom keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\n\n# Compile the Model\nmodel.compile(optimizer='sgd', loss='mean_squared_error')\n# Define early_stopping_monitor\nearly_stopping_monitor = EarlyStopping(patience=3)\n\n# Fit model_2\nmodel_2_training = model.fit(predictors, response, epochs=20, validation_split=0.3, callbacks=[early_stopping_monitor], verbose=False)\n\n# Create the plot\nplt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\nplt.xlabel('Epochs')\nplt.ylabel('Validation score')\nplt.show()","afd721e0":"from keras.models import load_model\nmodel.save('model_file_M5.h5')\nnn_model = load_model('model_file_M5.h5')\n# predictions = nn_model.predict(data_to_predict_with)","e298a942":"### Optimizing the Neural Network","108cd1d3":"### Model Building - Neural Network","3499e647":"Note: Recurrent Neural Network or Bayesian Neural Network would be more appropriate. But, I am going to implement a simple generic NN anyway.","091570f0":"### Import necessary dataset & libraries","4e7901f6":"### Using models\n* Save\n* Reload\n* Make prediction","b42ec28c":"![](https:\/\/imgur.com\/0aY4rqr.png)","fed757b9":"## Optimizing the Neural Network\nThis aim of this jupyter notebook is to build a simple Neural Network and optimize it, not to solve the uncertainty problem in the m5 forecasting dataset. Please don't try notebook for M5 forecasting competition. Learning source: **[Introduction to Deep Learning](https:\/\/learn.datacamp.com\/courses\/introduction-to-deep-learning-in-python) ** in Python from DataCamp\n","fe01e529":"So, our MSE is 31.50 at first epoch. We may run 10 epoch to get a better performance! Lets optimize","1e4bf113":"### Specifying a Model","af33cfc8":"### Compile & Fit the model"}}