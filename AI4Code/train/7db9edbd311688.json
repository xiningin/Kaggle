{"cell_type":{"8c4fecde":"code","7a07cdb4":"code","e0abf79b":"code","7c6f8123":"code","a9981741":"code","a40932d5":"code","8b051644":"code","97768262":"code","fcab93c7":"code","c1248551":"code","806588ca":"code","8b456984":"code","276cc261":"code","593192ee":"code","260c212d":"code","573bcfbd":"code","10968aa9":"code","909cf589":"code","fb195b17":"code","8456da11":"code","d8440a7d":"code","1d07ba17":"code","506d376d":"code","0eacc8c8":"code","0a9bdba8":"code","5d31e2c7":"code","9cfa3601":"code","fe846c5c":"code","ae48ae8d":"code","b351893a":"code","8e6c4cdf":"code","2e95ade2":"code","e988d89e":"code","80c4b520":"code","2226919c":"code","d8f97dda":"code","e8b13943":"code","830b2e37":"code","1a39579a":"code","598495a2":"code","17c69cd0":"code","b3714dd5":"code","222b14c3":"code","4da908ab":"code","acaea8aa":"code","4581506c":"code","d64e0a5f":"code","5770afbe":"code","2bfea1a8":"code","83587bdd":"code","b78a3b63":"code","4e4f3c6a":"code","e82e0bbc":"code","280626f4":"code","5c9bc061":"code","1ba41161":"code","d9fa3161":"code","c5c8c2b8":"code","089d985d":"code","9ca035c6":"code","ae094c6e":"code","c4182070":"code","a8e00813":"code","1c5898a7":"code","534f02f2":"code","90774b7e":"code","416d5cfc":"code","cdcf6fa7":"markdown","eec97df4":"markdown","275b88ff":"markdown","de776bee":"markdown","1ba52ed9":"markdown","671893a5":"markdown","44851346":"markdown","989b42ca":"markdown","5a5cf5d9":"markdown"},"source":{"8c4fecde":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nfrom numpy.random import randint,uniform\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom collections import OrderedDict\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\nimport warnings\n%matplotlib inline\nimport seaborn as sns\nsns.set(style = 'darkgrid') #\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom pylab import rcParams\nimport lightgbm as lgb\nfrom datetime import datetime\n#from lightgbm import LGBMClassifier,LGBMModel\nfrom lightgbm.sklearn import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold,cross_val_score,GridSearchCV,RandomizedSearchCV,train_test_split\nfrom sklearn.metrics import classification_report,f1_score,make_scorer, precision_score, recall_score, confusion_matrix\nrcParams['figure.figsize'] = 25, 12.5\nrcParams['font.size'] = 50\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\npd.set_option('display.max_columns',None)\nwarnings.filterwarnings('ignore')\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7a07cdb4":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntrain.head()","e0abf79b":"submission_sample = pd.read_csv('..\/input\/sample_submission.csv')","7c6f8123":"train.info()","a9981741":"test.info()","a40932d5":"int_uniq_values = train.select_dtypes('int').nunique().value_counts()\ndf = pd.DataFrame(int_uniq_values,columns = ['# columns']).reset_index().rename(columns = {'index':'Unique value'})\nsns.barplot(df['Unique value'],df['# columns'])\nplt.title('Frequency distribution of unique values in a integer column')\nplt.show()\n#int_uniq_values\n#sns.barplot(int_uniq_values)","8b051644":"continuos_var = train.select_dtypes('float').columns.tolist()\ncontinuos_var","97768262":"color_dict = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\npoverty_dict = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})","fcab93c7":"def plot_continuos(data,var):\n    for key,clr in color_dict.items():\n        sns.kdeplot(train[train.Target==key][var],color=clr,label = poverty_dict[key])\n    plt.xlabel(var)\n    plt.ylabel('Density')","c1248551":"plot_continuos(train,continuos_var[0])\n ","806588ca":"plot_continuos(train,continuos_var[1])","8b456984":"plot_continuos(train,continuos_var[2])","276cc261":"plot_continuos(train,continuos_var[3])","593192ee":"plot_continuos(train,continuos_var[4])","260c212d":"plot_continuos(train,continuos_var[6])","573bcfbd":"train.select_dtypes('object').head()","10968aa9":"test.select_dtypes('object').head()","909cf589":"train.loc[:,['dependency','edjefe','edjefa']] =train[['dependency','edjefe','edjefa']] .replace(to_replace={'yes':1,'no':0}).astype(np.float64) \ntest.loc[:,['dependency','edjefe','edjefa']] = test[['dependency','edjefe','edjefa']].replace(to_replace={'yes':1,'no':0}).astype(np.float64) ","fb195b17":"household_train = train[train.parentesco1==1]\nprint('Number of households in the train data:{}'.format(len(household_train)))\nsns.countplot(household_train['Target'])\nplt.ylabel('Number of households')\nplt.xlabel('Household type')\nplt.xticks([x - 1 for x in poverty_dict.keys()], list(poverty_dict.values()))\nplt.show()","8456da11":"household_checktarget =train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\nidhogar_mult_target = household_checktarget[household_checktarget!=True].index\nprint('Number of households where the poverty level is more than one:{}'.format(len(idhogar_mult_target)))","d8440a7d":"for idh in idhogar_mult_target:\n    actual_povertylevel = train.loc[((train.parentesco1==1) &(train.idhogar==idh)),'Target']\n    train.loc[(train.idhogar==idh),'Target'] = actual_povertylevel","1d07ba17":"household_num_head = train.groupby('idhogar')['parentesco1'].sum()\nidhogar_nohead = household_num_head[household_num_head!=1].index\nprint('Number of households where there are no head:{}'.format(len(idhogar_nohead)))","506d376d":"idhogar_nohead","0eacc8c8":"train[train.idhogar == 'f2bfa75c4']","0a9bdba8":"household_test= test[test['parentesco1'] == 1]\n\n# Create a gender mapping\nhousehold_train['gender'] = household_train['male'].replace({1: 'M', 0: 'F'})\n\n# Boxplot\nsns.boxplot(x = 'Target', y = 'meaneduc', hue = 'gender', data = household_train)\nplt.title('Mean Education vs poverty level by Gender')\nplt.xticks([x - 1 for x in poverty_dict.keys()], list(poverty_dict.values()))\nplt.show()","5d31e2c7":"sns.violinplot(x = 'Target',y='meaneduc',hue = 'gender', data = household_train)\nplt.title('Mean Education vs poverty level by Gender')\nplt.xticks([x - 1 for x in poverty_dict.keys()], list(poverty_dict.values()))\nplt.show()","9cfa3601":"id_columns =  ['Id', 'idhogar', 'Target']\n\nhousehold_boolean = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n           'pisonatur', 'pisonotiene', 'pisomadera',\n           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2']\n\nhousehold_continuos = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']\nhousehold_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin',\n              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\n\n###########################################################################################################\nind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n            'instlevel9', 'mobilephone']\nind_ordered = ['rez_esc', 'escolari', 'age']\n\nsquared_common = ['SQBescolari','SQBage', 'SQBhogar_total','SQBedjefe','SQBhogar_nin','SQBovercrowding', \n           'SQBdependency', 'SQBmeaned']","fe846c5c":"household_train.head(2)","ae48ae8d":"feature_train_household = household_train.drop(squared_common+['gender'],axis=1)[id_columns[0:3]+household_boolean+household_continuos+household_ordered]\nfeature_test_household = household_test.drop(squared_common,axis=1)[id_columns[0:2]+household_boolean+household_continuos+household_ordered]\n#target_train = household_train['Target']-1\n\nprint('Household training features data shape:{}\\nHousehold Test data shape:{}'.format(feature_train_household.shape,feature_test_household.shape))","b351893a":"##Target variable is present in train data but not in test data","8e6c4cdf":"feature_train_ind_bool = train.groupby('idhogar')[ind_bool].sum().reset_index()\nfeature_test_ind_bool = test.groupby('idhogar')[ind_bool].sum().reset_index()\n\nprint('Individual training boolean features data shape:{}\\nIndividual test boolean features data shape:{}'.format(feature_train_ind_bool.shape,feature_test_ind_bool.shape))\n","2e95ade2":"feature_train_ind_bool.head(2)","e988d89e":"feature_train_ind_cont = train.groupby('idhogar')[ind_ordered].agg(['mean', 'max', 'min', 'sum'])\nfeature_test_ind_cont = test.groupby('idhogar')[ind_ordered].agg(['mean', 'max', 'min', 'sum'])\n\nnew_cols = []\nfor col in feature_train_ind_cont.columns.levels[0]:\n    for stat in feature_train_ind_cont.columns.levels[1]:\n        new_cols.append(f'{col}-{stat}')\nfeature_train_ind_cont.columns = new_cols\nfeature_test_ind_cont.columns = new_cols\n\nfeature_train_ind_cont.reset_index(inplace=True)\nfeature_test_ind_cont.reset_index(inplace=True)","80c4b520":"print('Individual training continuous features data shape:{}\\nIndividual test continuous features data shape:{}'.format(feature_train_ind_cont.shape,feature_test_ind_cont.shape))\n","2226919c":"feature_train_ind_cont.head(2)","d8f97dda":"feature_train_ind_cont['rez_esc-mean'].isnull().sum()\/len(feature_train_ind_cont)","e8b13943":"feature_train_ind = pd.merge(feature_train_ind_bool,feature_train_ind_cont,on='idhogar',how='inner')\nfeature_test_ind = pd.merge(feature_test_ind_bool,feature_test_ind_cont,on='idhogar',how='inner')\nprint('Individual training  features data shape:{}\\nIndividual test  features data shape:{}'.format(feature_train_ind.shape,feature_test_ind.shape))","830b2e37":"feature_train_ind.head(2)","1a39579a":"feature_train = pd.merge(feature_train_household,feature_train_ind,on='idhogar',how='left')\n#feature_train.drop('Target',axis=1,inplace=True)\nfeature_test = pd.merge(feature_test_household,feature_test_ind,on='idhogar',how='left')\nprint('Combined features train data shape:{}\\nCombined features test data shape:{}'.format(feature_train.shape,feature_test.shape))","598495a2":"feature_train.head(2)","17c69cd0":"target_train = feature_train['Target']-1\ntest_idhogar = feature_test['idhogar'].values\ntry:\n    feature_train.drop('Target',axis=1,inplace=True)\n    feature_train.drop(['Id','idhogar'],axis=1,inplace=True)\n    feature_test.drop(['Id','idhogar'],axis=1,inplace=True)\nexcept:\n    print(\"Couldnot drop all columns\")\nfeature_train, feature_test = feature_train.align(feature_test, axis = 1, join = 'inner')\nfeature_X_train,feature_X_valid,target_y_train,target_y_valid = train_test_split(feature_train,target_train,test_size=0.2,stratify=target_train,random_state=123)\n\n\n","b3714dd5":"feature_train.head(2)","222b14c3":"feature_test.head(2)","4da908ab":"print('train features data shape:{}\\ntvalid features data shape:{}\\ntrain target shape:{}\\nvalid target shape:{}'.format(feature_X_train.shape,feature_X_valid.shape,target_y_train.shape,target_y_valid.shape))","acaea8aa":"def macro_f1_score(labels, predictions):\n    # Reshape the predictions as needed\n    predictions = predictions.reshape(len(np.unique(labels)), -1 ).argmax(axis = 0)\n    \n    metric_value = f1_score(labels, predictions, average = 'macro')\n    \n    # Return is name, value, is_higher_better\n    return 'macro_f1', metric_value, True","4581506c":"nfolds=5\nsfold = StratifiedKFold(n_splits= nfolds,shuffle=True)","d64e0a5f":"'''grid_params ={\n              'num_leaves': randint(12,30,5), \n              'learning_rate':[0.001,0.01,0.1,0.2,0.3],\n              'min_child_samples': randint(12,30,5), \n             #'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n              'subsample': uniform(0.5,1,4), \n              'colsample_bytree': uniform(0.5,1,4),\n              'random_state':[100,200,300]             \n            }\n\n#my_scorer = make_scorer(macro_f1_score, greater_is_better=True)\ndef get_best_params(train_X,train_y):\n    clf_lgb = lgb.LGBMClassifier(max_depth=-1,objective='multiclass',silent=True, metric='None',n_jobs=-1, class_weight='balanced')\n    grid = RandomizedSearchCV(clf_lgb, grid_params,verbose=20, cv=5,n_jobs=-1,scoring='f1_macro')\n    grid.fit(train_X,train_y)\n    return grid.best_params_,grid.best_score_\n    '''","5770afbe":"#best_params,best_score = get_best_params(feature_train,target_train)","2bfea1a8":"#best_params","83587bdd":"'''\ncolsample_bytree = best_params['colsample_bytree']\nlearning_rate = best_params['learning_rate']\nmin_child_samples = best_params['min_child_samples']\nnum_leaves = best_params['num_leaves']\nrandom_state = best_params['random_state']\nsubsample = best_params['subsample']\n'''","b78a3b63":"dict_optimum_params = {'colsample_bytree': 0.605756972263847,\n 'learning_rate': 0.01,\n 'min_child_samples': 19,\n 'num_leaves': 29,\n 'subsample': 0.5522796688057564}","4e4f3c6a":"def train_eval(feature_train,feature_test,target_train):\n    '''\n    clf_lgb = lgb.LGBMClassifier(objective = 'multiclass',\n                                 n_jobs = -1,\n                                 metric = 'None',\n                                 colsample_bytree=dict_optimum_params['colsample_bytree'],\n                                 learning_rate=learning_rate,\n                                 min_child_samples=min_child_samples,\n                                 num_leaves=num_leaves,\n                                 subsample=subsample,\n                                 random_state=random_state,\n                                 class_weight='balanced')\n                                 '''\n    valid_scores_list = []\n    test_predictions_df = pd.DataFrame()\n    feature_columns = feature_train.columns\n    feature_importance = np.zeros(len(feature_columns))\n    featuresNames = []\n    featureImps =[]\n\n    feature_train_arr = feature_train.values\n    feature_test_arr = feature_test.values\n    target_train_arr = target_train.values\n    \n    clfs =[]\n    for i in range(10):\n        clf=LGBMClassifier(objective = 'multiclass',\n                           colsample_bytree= 0.605,\n                           learning_rate= 0.01,\n                           min_child_samples= 19,\n                           num_leaves=29,\n                           subsample=0.552,\n                           n_jobs = -1,\n                           metric = 'None',\n                           random_state=100+i,\n                           class_weight='balanced')\n        clfs.append(('lgbm{}'.format(i), clf))\n        \n    \n    vc = VotingClassifier(clfs, voting='soft')\n    \n    for i, (train_index,valid_index) in enumerate(sfold.split(feature_train,target_train)):\n        fold_predictions_df = pd.DataFrame()        \n        # Training and validation data\n        X_train = feature_train_arr[train_index]\n        X_valid = feature_train_arr[valid_index]\n        y_train = target_train_arr[train_index]\n        y_valid = target_train_arr[valid_index]\n        \n        '''\n        fit_params={\"early_stopping_rounds\":100,\n            \"eval_metric\" : macro_f1_score, \n            \"eval_set\" : [(X_train,y_train), (X_valid,y_valid)],\n            'eval_names': ['train', 'valid'],\n            'verbose': 100,\n            'categorical_feature': 'auto'}\n        '''\n        vc.fit(X_train,y_train)\n        score = f1_score(y_valid,vc.predict(X_valid),average='macro')\n        print('Mean score on fold:{}:{}'.format(i+1,score))\n        #valid_scores_list.append(clf_lgb.best_score_['valid']['macro_f1'])\n        #display(f'Fold {i + 1}, Validation Score: {round(valid_scores_list[i], 5)}, Estimators Trained: {clf_lgb.best_iteration_}')\n        fold_probabilitites = vc.predict_proba(feature_test_arr)\n        for j in range(4):\n            fold_predictions_df[(j + 1)] = fold_probabilitites[:, j]\n            \n        fold_predictions_df['idhogar'] = test_idhogar\n        fold_predictions_df['fold'] = (i+1)\n        \n        test_predictions_df = test_predictions_df.append(fold_predictions_df)\n        #fold_feature_importance = vc.feature_importances_\n        #fold_feature_importance = 100.0 * (fold_feature_importance \/ fold_feature_importance.max())\n        #feature_importance = (feature_importance+fold_feature_importance)\/nfolds\n        #predictions.columns = ['Poverty_Extreme','Poverty_Mderate','Poverty_Vulnerable','Poverty_Unvulnerable']\n    test_predictions_df = test_predictions_df.groupby('idhogar', as_index = False).mean()\n    test_predictions_df['Target'] = test_predictions_df[[1, 2, 3, 4]].idxmax(axis = 1)\n    test_predictions_df['Score'] = test_predictions_df[[1, 2, 3, 4]].max(axis = 1)\n    #sorted_idx = np.argsort(feature_importance)\n    '''\n    for item in sorted_idx[::-1][:]:\n        featuresNames.append(np.asarray(feature_columns)[item])\n        featureImps.append(feature_importance[item])\n        featureImportance = pd.DataFrame([featuresNames, featureImps]).transpose()\n        featureImportance.columns = ['FeatureName', 'Importance']\n        \n    ''' \n    return test_predictions_df","e82e0bbc":"test_predictions_df=train_eval(feature_train,feature_test,target_train)","280626f4":"#sns.barplot(x =featureImportance['Importance'][0:10],y=featureImportance['FeatureName'][0:10] )","5c9bc061":"#featureImportance.tail(30)","1ba41161":"#mean_feat_importance = np.mean(featureImportance.Importance)\n#important_feat_filtered = featureImportance.loc[featureImportance.Importance>mean_feat_importance,'FeatureName'].tolist()","d9fa3161":"#important_feat_filtered","c5c8c2b8":"'''\nfeature_train_imp = feature_train[important_feat_filtered]\nfeature_test_imp= feature_test[important_feat_filtered]\nfeature_train_imp, feature_test_imp = feature_train_imp.align(feature_test_imp, axis = 1, join = 'inner')\nprint('train features data shape:{}\\ntest features data shape:{}\\ntrain target shape:{}'.format(feature_train_imp.shape,feature_test_imp.shape,target_train.shape))\n'''","089d985d":"#_,test_predictions_df=train_eval(feature_train_imp,feature_test_imp,target_train)","9ca035c6":"test_predictions_df.head(2)","ae094c6e":"test.head(2)","c4182070":"submission_df = test.loc[:,['Id','idhogar']]","a8e00813":"submission = pd.merge(submission_df,test_predictions_df[['idhogar','Target']],on='idhogar',how='left').drop('idhogar',axis=1)","1c5898a7":"submission.head(2)","534f02f2":"submission['Target'] = submission['Target'].fillna(4).astype(np.int8)","90774b7e":"submission.head(2)","416d5cfc":"#today = datetime.now()\n#sub_file = 'submission_LGB_{}.csv'.format(str(today.strftime('%Y-%m-%d-%H-%M')))\nsubmission.to_csv('sample_submission.csv',index=False)","cdcf6fa7":"**Initialize  cross validation folds**","eec97df4":"**Baseline model(with no tuning)**\n\n1.LightGBM","275b88ff":"**Drop all squared variables as they exhibit significant  correlation with the original variables**","de776bee":"**Custom Performance Metric**","1ba52ed9":"**Merging individual features**","671893a5":"**Merging individual features with household features**","44851346":"**Create aggregate features out of individual rows**","989b42ca":"** Target label distribution**\n1. How many households  are non-vulnerable to extremely poor?","5a5cf5d9":"**Drop id columns which are not required and align  the test columns  with train columns **"}}