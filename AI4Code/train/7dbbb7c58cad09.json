{"cell_type":{"616659da":"code","8da9d5cf":"code","c8d51262":"code","71af88f1":"code","a7074123":"code","c8595cdc":"code","2034150d":"code","1c8fe070":"code","aaae75bc":"markdown","7cc780a9":"markdown","be3611e8":"markdown","d887dae9":"markdown","6b680d8f":"markdown","fc0ab99d":"markdown","03a70c22":"markdown","5e5a29e8":"markdown"},"source":{"616659da":"import tensorflow as tf\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport os\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","8da9d5cf":"def build_generator(image_size=28, input_size=100):\n    \n    #Build an input layer\n    gen_input = keras.Input(shape=(input_size,))\n    \n    #Increase dimensions and resize to 3D to feed it to Conv2DTranspose layer\n    x = layers.Dense(7 * 7 * 128)(gen_input)\n    x = layers.Reshape((7, 7, 128))(x)\n    \n    #Use ConvTranspose\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.Conv2DTranspose(128, kernel_size=[5,5], strides=2, padding='same')(x)\n    \n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.Conv2DTranspose(64, kernel_size=[5,5], strides=2, padding='same')(x)\n    \n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.Conv2DTranspose(32, kernel_size=[5,5], strides=1, padding='same')(x)\n    \n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.Conv2DTranspose(1, kernel_size=[5,5], strides=1, padding='same')(x)\n    \n    #Output layer for Generator\n    x = layers.Activation('sigmoid')(x)\n    \n    #Build model using Model API\n    generator = keras.Model(gen_input, x, name='generator')\n    \n    return generator","c8d51262":"def build_discriminator(data_shape=[28,28,1,]):\n    \n    #Build the network\n    dis_input = keras.Input(data_shape)\n    x = layers.LeakyReLU(alpha=0.2)(dis_input)\n    x = layers.Conv2D(32, kernel_size=[5,5], strides=2, padding='same')(x)\n    \n    x = layers.LeakyReLU(alpha=0.2)(x)\n    x = layers.Conv2D(64, kernel_size=[5,5], strides=2, padding='same')(x)\n    \n    x = layers.LeakyReLU(alpha=0.2)(x)\n    x = tf.keras.layers.Conv2D(128, kernel_size=[5,5], strides=2, padding='same')(x)\n    \n    x = layers.LeakyReLU(alpha=0.2)(x)\n    x = tf.keras.layers.Conv2D(256, kernel_size=[5,5], strides=1, padding='same')(x)\n    \n    #Flatten the output and build an output layer\n    x = layers.Flatten()(x)\n    x = layers.Dense(1, activation='sigmoid')(x)\n    \n    #Build Model\n    discriminator = keras.Model(dis_input, x, name='discriminator')\n    \n    return discriminator","71af88f1":"def build_models():\n    \n    noise_size = 100\n    lr = 2e-4\n    decay = 6e-8\n    \n    #Build Base Discriminator model\n    base_discriminator = build_discriminator(data_shape=(28,28,1,))\n    \n    #Define optimizer and compile model\n    discriminator = keras.Model(inputs=base_discriminator.inputs, \n                                          outputs=base_discriminator.outputs)\n    optimizer = keras.optimizers.RMSprop(lr=lr, decay=decay)\n    discriminator.compile(loss='binary_crossentropy',\n                          optimizer=optimizer,\n                          metrics=['accuracy'])\n    \n    #Build Generator model\n    generator = build_generator(image_size=28, input_size=noise_size)\n    \n    #Build Frozen Discriminator\n    frozen_discriminator = keras.Model(inputs=base_discriminator.inputs, \n                                          outputs=base_discriminator.outputs)\n    #Freeze the weights of discriminator during adversarial training\n    frozen_discriminator.trainable = False\n\n    #Build Adversarial model\n    optimizer = keras.optimizers.RMSprop(lr=lr * 0.5, decay=decay * 0.5)\n    #Adversarial = generator + discriminator\n    adversarial = keras.Model(generator.input, \n                        frozen_discriminator(generator.output))\n    \n    adversarial.compile(loss='binary_crossentropy',\n                        optimizer=optimizer,\n                        metrics=['accuracy'])    \n    \n    return generator, discriminator, adversarial","a7074123":"def train_gan(generator, discriminator, adversarial, noise_size=100):\n    \n    #Training parameters\n    batch_size = 64\n    train_steps = 10000\n    image_size = 28\n    \n    # load MNIST dataset\n    (train_x, _), (_, _) = tf.keras.datasets.mnist.load_data()\n    #Make it 3D dataset\n    train_x = np.reshape(train_x, [-1, image_size, image_size, 1])\n    #Standardize data : 0 to 1\n    train_x = train_x.astype('float32') \/ 255\n    \n    #Input for testing generator at different intervals, we will generate 16 images\n    test_noise_input = np.random.uniform(-1.0,1.0, size=[16, noise_size])\n    \n    #Start training\n    for i in range(train_steps):\n        \n        #Train DISCRIMATOR\n        \n        #1. Get fake images from Generator\n        noise_input = np.random.uniform(-1.0,1.0, size=[batch_size, noise_size])\n        fake_images = generator.predict(noise_input)\n        \n        #2. Get real images from training set\n        img_indexes = np.random.randint(0, train_x.shape[0], size=batch_size)\n        real_images = train_x[img_indexes]\n        \n        #3. Prepare input for training Discriminator\n        X = np.concatenate((real_images, fake_images))\n        \n        #4. Labels for training\n        y_real = np.ones((batch_size, 1))\n        y_fake = np.zeros((batch_size, 1))\n        y = np.concatenate((y_real, y_fake))\n        \n        #5. Train Discriminator\n        d_loss, d_acc = discriminator.train_on_batch(X, y)\n        \n        \n        #Train ADVERSARIAL Network\n        \n        #1. Prepare input - create a new batch of noise\n        X = noise_input = np.random.uniform(-1.0,1.0, size=[batch_size, noise_size])\n        \n        #2. Prepare labels - training Adversarial network to lie :) - All 1s\n        y = np.ones((batch_size, 1))\n        \n        #3. Train - Pls note Discrimator is not getting trained here\n        a_loss, a_acc = adversarial.train_on_batch(X, y)\n        \n        if i % 100 == 0:\n            #Print loss and Accuracy for both networks\n            print(\"%s [Discriminator loss: %f, acc: %f, Adversarial loss: %f, acc: %f]\" % (i, d_loss, d_acc, a_loss, a_acc) )\n        \n        #Save generated images to see how well Generator is doing\n        if (i+1) % 500 == 0:\n            \n            #Generate 16 images\n            fake_images = generator.predict(test_noise_input)\n            \n            #Display images\n            plot_images(fake_images, i+1)\n            \n    #Save Generator model\n    generator.save('mnist_generator_dcgan.h5')    ","c8595cdc":"def plot_images(fake_images, step):\n    \n    plt.figure(figsize=(2.5,2.5))\n    num_images = fake_images.shape[0]\n    \n    image_size = fake_images.shape[1]\n    rows = int(math.sqrt(fake_images.shape[0]))\n    \n    for i in range(num_images):\n        plt.subplot(rows, rows, i + 1)\n        image = np.reshape(fake_images[i], [image_size, image_size])\n        plt.imshow(image, cmap='gray')\n        plt.axis('off')\n    plt.show()","2034150d":"G, D, A = build_models()\nG.summary()\nD.summary()\nA.summary()","1c8fe070":"train_gan(G, D, A)","aaae75bc":"Buiding a Generator Model","7cc780a9":"Import libraries","be3611e8":"Build Adversarial model","d887dae9":"model summary","6b680d8f":"TRAINING ","fc0ab99d":"utility function to draw images","03a70c22":"building a Discriminator","5e5a29e8":"Lets begin the training\n-first iteration- initially the generator have no idea, after 500 epochs the generated images doesnt seems like digits.\n\n-second iteration- Onward the generator have developed little bit understanding of data, geerated images have so similarity to minst dataset\n\nafter every cycle it can be visually confirmed that quality of generated images are increasing.\n\nTHANKS FOR PATIENTLY READING TILL END"}}