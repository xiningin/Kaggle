{"cell_type":{"12405c46":"code","5a34bb2e":"code","a997e077":"code","c2315a05":"code","e66006bf":"code","4390a25f":"code","d8caf2ca":"code","4cb4812c":"code","0e25237d":"code","2ed51be9":"code","d58f3093":"code","5a32ae69":"code","1f883789":"code","5e367c81":"code","7750e38c":"code","3a7e72b4":"code","5c10c14a":"code","497a5a63":"code","3cd957f6":"code","8265264d":"code","c13d2750":"code","c05edcd6":"code","3509e0ed":"code","41ad75fc":"code","606035ca":"code","6bd08292":"code","7cd81eeb":"code","20226701":"code","d12ddb53":"code","c3117c4d":"code","49ffb9c6":"code","2384d75a":"code","45881731":"code","86f42072":"code","8feeb4e9":"code","7004efac":"code","15e82cda":"markdown","56b76278":"markdown","0224056d":"markdown","2ed4f7d1":"markdown","ea611557":"markdown","e2390e4f":"markdown","790298b2":"markdown","7f05eb07":"markdown","468d6f0b":"markdown","2cdf80ee":"markdown","6271068f":"markdown","9a66ec80":"markdown","e0c9f202":"markdown","9954952a":"markdown","29e1c930":"markdown","9824f06d":"markdown","f5cf80f9":"markdown","8c1742ea":"markdown","961fc306":"markdown","4c011c46":"markdown","6e4ea1a5":"markdown","857ec90c":"markdown","9586feb9":"markdown","6ad452ed":"markdown","f87deb25":"markdown","45e5be95":"markdown","7cbc0621":"markdown","71208f04":"markdown","00059d80":"markdown","d7de800f":"markdown","51f21055":"markdown","601d7d9f":"markdown","a165ca22":"markdown","4d7a7fde":"markdown","ca4885be":"markdown","a61ccf21":"markdown","bbfd2361":"markdown","a533b80f":"markdown","f812c10e":"markdown"},"source":{"12405c46":"import pandas as pd\nimport numpy as np\nimport datetime as dt\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom mlxtend.regressor import StackingCVRegressor","5a34bb2e":"def df_characteristics(df):\n    \n    print('The shape of this dataframe is: {}'.format(df.shape), '\\n')\n    \n    df_num = df.select_dtypes(include=[np.number])\n    print('This dataframe has {} numeric features.'.format(df_num.shape[1]), '\\n')\n    print(df_num.columns, '\\n')\n    \n    df_cat = df.select_dtypes(exclude=[np.number])\n    print('This dataframe has {} categorical features.'.format(df_cat.shape[1]), '\\n')\n    print(df_cat.columns)","a997e077":"def check_null_values(df):\n    \n    nanframe = pd.DataFrame((df.isnull().sum() \/ len(df)) * 100)\n    nanframe.columns = ['NaN(%)']\n    nanframe['Blank_Record_Counts'] = pd.DataFrame(df.isnull().sum())\n    nanframe = nanframe[nanframe['Blank_Record_Counts'] != 0]\n    return nanframe.sort_values(by='NaN(%)', ascending=False).reset_index()","c2315a05":"def remove_collinear_features(x, threshold):\n    \n    # Create correlation matrix:\n    corr_matrix = x.corr()\n    iters = range(len(corr_matrix.columns) - 1)\n    drop_cols = []\n    \n    # Work through the iterations setup:\n    for i in iters:\n        for j in range(i+1):\n            items = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n            col = items.columns\n            row = items.index\n            val = abs(items.values)\n            \n            # Compare against threshold:\n            if val >= threshold:\n                print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n                drop_cols.append(col.values[0])\n                \n    cols_to_drop = set(drop_cols)\n    x = x.drop(columns = cols_to_drop, axis=1)\n    \n    return x","e66006bf":"data = pd.read_csv('..\/input\/kc-house-data\/kc_house_data.csv')","4390a25f":"df_characteristics(data)","d8caf2ca":"data.describe()","4cb4812c":"data['Yrs_since_refurb'] = np.where(data['yr_renovated'] == 0, (2020 - data['yr_built']), (2020 - data['yr_renovated']))","0e25237d":"check_null_values(data)","2ed51be9":"data['sqft_above'] = data['sqft_above'].fillna(1788.396095)","d58f3093":"data.sample(5)","5a32ae69":"dateObj = dt.datetime.strptime('20140623T000000', '%Y%m%dT%H%M%S')","1f883789":"print(dateObj)","5e367c81":"data['date'] = data['date'].apply(lambda x: dt.datetime.strptime(x, '%Y%m%dT%H%M%S'))","7750e38c":"data['date']=data['date'].map(dt.datetime.toordinal)","3a7e72b4":"sns.scatterplot(data['date'], data['price'])","5c10c14a":"data.drop(columns=['date'], inplace=True)","497a5a63":"plt.figure(figsize=(20,10))\nsns.heatmap(data.corr(), annot=True, vmin=-1, vmax=+1, cmap='RdYlGn')","3cd957f6":"features = data.drop('price', axis=1)","8265264d":"remove_collinear_features(features, 0.7)","c13d2750":"features = features.drop(columns = ['sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'sqft_lot15', 'yr_built'], axis=1)","c05edcd6":"df_characteristics(features)","3509e0ed":"X = features # This is the set of features identified at the end of the last section \ny = data['price'] # target variable","41ad75fc":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","606035ca":"print('X_train shape: {}'.format(X_train.shape))\nprint('X_test shape: {}'.format(X_test.shape))\nprint('y_train shape: {}'.format(y_train.shape))\nprint('y_test shape: {}'.format(y_test.shape))","6bd08292":"lin_reg = LinearRegression()\nmodel = lin_reg.fit(X_train, y_train)\n\ny_preds = model.predict(X_test)\n\nrmse = np.sqrt(mean_squared_error(y_test, y_preds))\nprint('RMSE: ', rmse)\nrsq = r2_score(y_test, y_preds)\nprint('R2 Score: ', rsq)\nmae = mean_absolute_error(y_test, y_preds)\nprint('MAE: ', mae)","7cd81eeb":"ridge = Ridge(random_state=42)\nridge_mod = ridge.fit(X_train, y_train)\n\nridge_preds = ridge_mod.predict(X_test)\n\nrmse = np.sqrt(mean_squared_error(y_test, ridge_preds))\nprint('RMSE: ', rmse)\nrsq = r2_score(y_test, ridge_preds)\nprint('R2 Score: ', rsq)\nmae = mean_absolute_error(y_test, ridge_preds)\nprint('MAE: ', mae)","20226701":"random_forest = RandomForestRegressor()\nforest = random_forest.fit(X_train, y_train)\n\nrf_preds = forest.predict(X_test)\n\nrmse = np.sqrt(mean_squared_error(y_test, rf_preds))\nprint('RMSE: ', rmse)\nrsq = r2_score(y_test, rf_preds)\nprint('R2 Score: ', rsq)\nmae = mean_absolute_error(y_test, rf_preds)\nprint('MAE: ', mae)","d12ddb53":"gbr = GradientBoostingRegressor(learning_rate=0.01, n_estimators=1000)\ngbm = gbr.fit(X_train, y_train)\n\ngbm_preds = gbm.predict(X_test)\n\nrmse = np.sqrt(mean_squared_error(y_test, gbm_preds))\nprint('RMSE: ', rmse)\nrsq = r2_score(y_test, gbm_preds)\nprint('R2 Score: ', rsq)\nmae = mean_absolute_error(y_test, gbm_preds)\nprint('MAE: ', mae)","c3117c4d":"DT = DecisionTreeRegressor()\ntree = DT.fit(X_train, y_train)\n\ntree_preds = tree.predict(X_test)\n\nrmse = np.sqrt(mean_squared_error(y_test, tree_preds))\nprint('RMSE: ', rmse)\nrsq = r2_score(y_test, tree_preds)\nprint('R2 Score: ', rsq)\nmae = mean_absolute_error(y_test, tree_preds)\nprint('MAE: ', mae)","49ffb9c6":"svr = SVR(gamma='auto')\nsvr_model = svr.fit(X_train, y_train)\n\nsvr_preds = svr_model.predict(X_test)\n\nrmse = np.sqrt(mean_squared_error(y_test, svr_preds))\nprint('RMSE: ', rmse)\nrsq = r2_score(y_test, svr_preds)\nprint('R2 Score: ', rsq)\nmae = mean_absolute_error(y_test, svr_preds)\nprint('MAE: ', mae)","2384d75a":"xgb = XGBRegressor(n_estimators=1000, learning_rate=0.09)\nxgbm = xgb.fit(X_train, y_train)\n\nxgb_preds = xgbm.predict(X_test)\n\nrmse = np.sqrt(mean_squared_error(y_test, xgb_preds))\nprint('RMSE: ', rmse)\nrsq = r2_score(y_test, xgb_preds)\nprint('R2 Score: ', rsq)\nmae = mean_absolute_error(y_test, xgb_preds)\nprint('MAE: ', mae)","45881731":"lgb = LGBMRegressor(n_estimators=1000, learning_rate=0.1)\nlgbm = lgb.fit(X_train, y_train)\n\nlgb_preds = lgbm.predict(X_test)\n\nrmse = np.sqrt(mean_squared_error(y_test, lgb_preds))\nprint('RMSE: ', rmse)\nrsq = r2_score(y_test, lgb_preds)\nprint('R2 Score: ', rsq)\nmae = mean_absolute_error(y_test, lgb_preds)\nprint('MAE: ', mae)","86f42072":"scaler = StandardScaler()\nX_train_MLP = scaler.fit_transform(X_train)\nX_test_MLP = scaler.fit_transform(X_test)\n\nnet = MLPRegressor(max_iter=1000, learning_rate_init=0.05, hidden_layer_sizes=(50,25,25), random_state=42)\nnetwork = net.fit(X_train_MLP, y_train)\n\nnet_preds = network.predict(X_test_MLP)\n\nrmse = np.sqrt(mean_squared_error(y_test, net_preds))\nprint('RMSE: ', rmse)\nrsq = r2_score(y_test, net_preds)\nprint('R2 Score: ', rsq)\nmae = mean_absolute_error(y_test, net_preds)\nprint('MAE: ', mae)","8feeb4e9":"stacked_gen = StackingCVRegressor(regressors=(lin_reg, ridge, random_forest, gbr, DT, svr, xgb, lgb),\n                                  meta_regressor=lgb, use_features_in_secondary=True)\n\nstacked_gen_mod = stacked_gen.fit(np.array(X_train), np.array(y_train))\nstacked_gen_preds = stacked_gen_mod.predict(np.array(X_test))\n\nrmse = np.sqrt(mean_squared_error(y_test, stacked_gen_preds))\nprint('RMSE: ', rmse)\nrsq = r2_score(y_test, stacked_gen_preds)\nprint('R2 Score: ', rsq)\nmae = mean_absolute_error(y_test, stacked_gen_preds)\nprint('MAE: ', mae)","7004efac":"scores = {\n         'Model': ['Lin_Reg', 'Ridge', 'Random_Forest', 'Gradient_Boost', 'Decision_Tree', \n                    'SVR', 'XGB', 'LGB', 'MLP','Stacked_Gen'],\n          \n         'RMSE': [(np.sqrt(mean_squared_error(y_test, y_preds))),\n                  (np.sqrt(mean_squared_error(y_test, ridge_preds))),\n                 (np.sqrt(mean_squared_error(y_test, rf_preds))),\n                 (np.sqrt(mean_squared_error(y_test, gbm_preds))),\n                 (np.sqrt(mean_squared_error(y_test, tree_preds))),\n                 (np.sqrt(mean_squared_error(y_test, svr_preds))),\n                 (np.sqrt(mean_squared_error(y_test, xgb_preds))),\n                 (np.sqrt(mean_squared_error(y_test, lgb_preds))),\n                  (np.sqrt(mean_squared_error(y_test, net_preds))),\n                 (np.sqrt(mean_squared_error(y_test, stacked_gen_preds)))],\n    \n         'R2 Score': [(r2_score(y_test, y_preds)),\n                     (r2_score(y_test, ridge_preds)),\n                     (r2_score(y_test, rf_preds)),\n                     (r2_score(y_test, gbm_preds)),\n                     (r2_score(y_test, tree_preds)),\n                     (r2_score(y_test, svr_preds)),\n                     (r2_score(y_test, xgb_preds)),\n                     (r2_score(y_test, lgb_preds)),\n                     (r2_score(y_test, net_preds)),\n                     (r2_score(y_test, stacked_gen_preds))],\n    \n        'MAE': [(mean_absolute_error(y_test, y_preds)),\n                  (mean_absolute_error(y_test, ridge_preds)),\n                 (mean_absolute_error(y_test, rf_preds)),\n                 (mean_absolute_error(y_test, gbm_preds)),\n                 (mean_absolute_error(y_test, tree_preds)),\n                 (mean_absolute_error(y_test, svr_preds)),\n                 (mean_absolute_error(y_test, xgb_preds)),\n                 (mean_absolute_error(y_test, lgb_preds)),\n                (mean_absolute_error(y_test, net_preds)),\n                 (mean_absolute_error(y_test, stacked_gen_preds))]\n            }\n\ncol = ['Model', 'RMSE', 'R2 Score', 'MAE']\n\nerror_matrix = pd.DataFrame(data=scores, columns=col).sort_values(by='MAE', ascending=True).reset_index()\nerror_matrix.drop(columns=['index'], inplace=True)\n\nerror_matrix","15e82cda":"The last one is quite specific to a regression exercise. I found this originally on stack overflow and have adapted it ever since and can't imagine solving a regression problem without it. The issue of multicollinearity can be addressed with this function, which identifies feature variables that are highly correlated with each other (for this exercise, 'highly correlated' means a threshold of 0.7, which is the generally accepted threshold).","56b76278":"... hardly any, which is always a good sign :)","0224056d":"In addition to the libraries imported above, I typically build the following functions everytime I attempt a regression problem. I have tried my hand at a few of these, and I find that these user defined functions are incredibly helpful with both exploratory data analytics as well as data cleaning.","2ed4f7d1":"There appears to be one additional item to fix - the date appears to be in epoch format. This is again easily done using a `datetime` class (`strptime`). I first tried this on an example to make sure my code works, and then used a `lambda` function to convert this for all entries in the `date` field.\n\nOnce converted, all that remained was to change the `date` to an ordinal datatype, to enable regression modelling. ","ea611557":"____\n\n<center><h1> Regression Modelling <\/h1><\/center>\n\n_____","e2390e4f":"**Model 7 - XGB Model**","790298b2":"This concludes the EDA phase of this exercise, and we are now ready to initiate modelling.","7f05eb07":"Having introduced a new feature and fixed the blank values, let's check out a random sample of 5 records in the dataframe. I always prefer `sample` to `head` as I like the randomness of the output, which although not powerful, has helped me spot anomalies in my approach in the past.","468d6f0b":"Reviewing the table above, there's a tie between the LGB and XGB models, which are both gradient boosters (neural networks) and have a MAE of $85,000. The LGB appears (very) marginally better than XGB, and therefore is my preferred choice of model. The accuracy rate is shown in the R2 score of 81%, which means that the model can explain 81% of calculated variances.","2cdf80ee":"**Model 10 - Stacked Gen Model**","6271068f":"____\n\n<center><h1> Library Imports & User Defined Functions <\/h1><\/center>\n\n_____","9a66ec80":"**Model 3 - Random Forest Regression**","e0c9f202":"Let's start by applying our first UDF to explore the dataframe's characteristics.","9954952a":"A good test at this point would be to verify that the features are in matrix format and the targets in vector format.","29e1c930":"**Model 2 - Ridge Regression**","9824f06d":"**End of Notebook**","f5cf80f9":"In addition to clearly calling out features that do have a strong positive (or negative) correlation with price, we can also see a lot of features that are correlated to each other. Multicollinearity is a problem as it can hinder the predictive power of any model we create. Therefore, now is the time to:\n\n* Segregate the features from the dataset into a separate dataframe.\n* Remove collinear features from the features collected. For this purpose, I have arbitrarily used a correlation threshold of 0.7.","8c1742ea":"Finally, to facilitate readability and comparability of scores, it is sensible to tabulate the error metrics for each model and save this down to one dataframe.","961fc306":"The first function is used to succinctly summarise the key attributes of a dataframe, split across numeric and categorical features. This isn't a 'scientific' split, categorical is defined here as anything non-numeric, so take it with a pinch of salt. Regardless, useful in helping us decide what we need to do with the data.","4c011c46":"**Model 9 - MLP Regressor**","6e4ea1a5":"That said, all this was a bit of a wasted effort as there's clearly no correlation whatsoever between date and price, so it's best to drop this field.","857ec90c":"The other function which I always use is a `check_null_values` function, which is designed to trawl through the data and return a dataframe (which I've called the `nanframe` for hopefully obvious reasons) which shows not only the fields in the dataframe carrying blank (`NaN`) values, but also the proportion of such values relative to the size of the data. This is a very useful statistic that helps us decide how to handle null values. As a rule of thumb, I drop anything where the proportion of `NaN` records is over 50% of the population of that field, as it doesn't seem intrinsically useful to estimate data for the majority of the feature's population.","9586feb9":"**Model 6 - SVR Model**","6ad452ed":"**Model 5 - Decision Tree Regression**","f87deb25":"I can now use a `scatterplot` to visualise the relationship, if any, between the dates and sale prices.","45e5be95":"The function has worked and has identified 6 features that are highly correlated. I have removed them from the final selection of features for regression modelling in the next step.","7cbc0621":"____\n\n<center><h1> Exploratory Data Analysis <\/h1><\/center>\n\n_____","71208f04":"To help me choose the best model for the prediction, I have considered 10 different models, which are explained below. The high-level approach in each case is:\n\n* Setting up the model, and fitting this on the train datasets.\n* Generating predictions\n* Calculating 3 error metrics - RMSE, R2 and Mean Absolute Error.\n\nThe MAE is my preferred error metric for this exercise given that it's easy to interpret. In our case this actually means the quantum by which any prediction is wrong, and therefore my choice of model will be influenced by whichever model generates the lowest MAE.","00059d80":"**Model 4 - Gradient Boosting Regression**","d7de800f":"So that's it :). There are clearly many more sophisticated methods that can be used to achieve a higher level of predictive power. Intuitively, I would have liked a model with a maximum MAE of $25,000 for a problem on house prices, as it would generate very reliable predictions that can be used by a real-estate firm or prospective customers thinking about buying or selling a house in the area. Whilst neural networks can achieve this to some degree, some finetuning is required to take this exercise to the next level. So please review the kernel and let me know if you have any suggestions for improvement - Any constructive comments will be very gratefully received!!\n\nCheers...","51f21055":"As shown above, the data seems to be very much numerical at first glance. However, upon a closer look - the year fields `yr_built` and `yr_renovated` - are not really numeric features, and are temporal variables. Arguably `zipcode` should also be classed a categorical feature, but am probably overthinking now :).\n\nThat said, the `yr_built` and `yr_renovated` features enable us to think about a new feature - `Yrs_since_refurb`. It stands to reason that a recently renovated house is almost always worth more than one which was renovated, say, 10 years ago. We can very easily create this feature using `np.where()`.","601d7d9f":"Let's now supplement this with the `pandas` `describe()` function.","a165ca22":"____\n\n<center><h1> Data Sourcing <\/h1><\/center>\n\n_____","4d7a7fde":"**Model 8 - LGB Model**","ca4885be":"**Model 1 - Multivariate Linear Regression**","a61ccf21":"We begin by importing a whole load of python libraries that will help us with predicting sale prices for homes in King County, Washington. These may appear random, but actually have been set out in a logical fashion.\n\n`pandas`, `numpy` and `datetime` are basic functions - regardless of what we do with the data, it's hard to imagine any analytic exercise without using these libraries in some measure. Likewise, the visualisation libraries - `seaborn` and `matplotlib` are used in many instances to visualise. I've seen experienced data scientists also use `plotly`, but as I'm not fluent in this library I've steered clear :).","bbfd2361":"Let's now check out if we have any null values to deal with..","a533b80f":"The next step is to assess linear relationships between price and the other variables, which we will do using a correlation heatmap.","f812c10e":"We've got just two records in the `sqft_above` field that are blank. These are inconsequential considering the size of the dataset. I'm going to take the lazy approach and just fill up the blanks with the mean `sqft_above` value that I've seen in the output of the `data.describe()` function above."}}