{"cell_type":{"4597e2fc":"code","f614eb47":"code","13b6997c":"code","354ce070":"code","876d0c87":"code","6d75570c":"code","932badfa":"code","4283eb86":"code","67ab8c26":"code","b8644c81":"code","6a0aff82":"code","5db04e3f":"code","67f2f5fb":"code","b64a653b":"code","33be571e":"code","521d3797":"code","36fa25e2":"code","5aa5f132":"code","fd34a07a":"code","f61c20e7":"code","858ebbcd":"code","5817f78f":"code","2c040336":"code","c8478723":"code","115b19fc":"code","51c100a5":"code","27dd744d":"code","0fc10253":"code","b4acf068":"code","30cbe535":"code","b08b1de9":"code","fef8c3f8":"code","49a49fb2":"code","6cdade8b":"code","301b5d46":"code","c47bb67e":"code","934ebbe3":"code","fe1ec38e":"code","5304992e":"code","bedaba26":"code","6d6ef722":"code","6bc3c07e":"code","5ccee0b9":"code","47e7a6d0":"code","74871ff8":"code","70253065":"markdown","cc1a9771":"markdown","3fe3f16b":"markdown","f68d915b":"markdown","29582f9d":"markdown","5a0b4f39":"markdown","72d5ddd7":"markdown","87e4c742":"markdown","54be3ebf":"markdown","d8a191b7":"markdown","bf9404df":"markdown","f962f3ec":"markdown","72eae30c":"markdown","464ec592":"markdown","f0dbd7d4":"markdown","9580678a":"markdown","16c34a77":"markdown","bd6b524f":"markdown","f2ae5fba":"markdown","b1902945":"markdown"},"source":{"4597e2fc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\nimport torch\nimport torch.nn.functional as F\nimport torchvision\nimport cv2\nimport albumentations\nimport albumentations.pytorch\nimport os\nimport PIL\nimport copy\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f614eb47":"!pip install --upgrade git+git:\/\/github.com\/carloalbertobarbano\/pytorch-train-utils","13b6997c":"from pytorchtrainutils import trainer\nfrom pytorchtrainutils import metrics\nfrom pytorchtrainutils import utils","354ce070":"def plot_cm(logs):\n    accs = logs['top1-acc']\n    cms = logs['cm']\n\n    classes = ['healthy', 'multiple_diseases', 'rust', 'scab']\n\n    plt.figure(figsize=(20, 3))\n    plt.suptitle(f'CM')\n    for class_idx, class_name in enumerate(classes):\n        plt.subplot(1, 4, class_idx+1)\n        plt.title(f'{class_name}')\n        ax = sns.heatmap(\n            cms.class_cm(class_idx, normalized=True), \n            annot=True, fmt=\".2f\", vmin=0., vmax=1.\n        )\n\n    plt.savefig('cm.png')\n    plt.show()\n\n    plt.figure(figsize=(20, 3))\n    plt.suptitle(f'CM (best threshold)')\n    for class_idx, class_name in enumerate(classes):\n        plt.subplot(1, 4, class_idx+1)\n        plt.title(f'{class_name}')\n\n        best_threshold = accs.get_best_threshold(class_idx)\n        ax = sns.heatmap(\n            cms.class_cm(class_idx, normalized=True, threshold=best_threshold), \n            annot=True, fmt=\".2f\", vmin=0., vmax=1.\n        )\n\n    plt.savefig('cm-t')\n    plt.show()\n    \n\ndef plot_roc_auc(logs):\n    aucs = logs['col-auc']\n    accs = logs['top1-acc']\n    classes = ['healthy', 'multiple_diseases', 'rust', 'scab']\n\n    plt.figure(figsize=(20, 5))\n    plt.suptitle(f'Classification report - average AUC: {aucs.get():.4f}')\n    for class_idx, class_name in enumerate(classes):\n        plt.subplot(1, 4, class_idx+1)\n        plt.title(f'{class_name} BA={accs.class_ba(class_idx):.4f}')\n        fpr, tpr, _ = aucs.class_curve(class_idx)\n        plt.plot(fpr, tpr, label=f'AUC: {aucs.class_auc(class_idx):.4f}')\n        plt.plot([0, 1], [0, 1], color='navy', lw=0.5, linestyle='--')\n        plt.legend(loc='lower right')\n    plt.savefig('auc.png')\n    plt.show()","876d0c87":"seed = 42\nutils.set_seed(seed)\ndevice = torch.device('cuda')\n\nlr = 1e-2\nbatch_size = 8\nn_epochs_224 = 30\nn_epochs_448 = 20\n\narch = 'resnet18'\n\nmean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225] #Imagenet","6d75570c":"!ls \/kaggle\/input","932badfa":"dataset_path = '\/kaggle\/input\/plantpathology2020fgvc7pickles\/plant-pathology-2020-fgvc7-pickles'\ntrain_df = pd.read_csv(os.path.join(dataset_path, 'train.csv'))\ntest_df = pd.read_csv(os.path.join(dataset_path, 'test.csv'))","4283eb86":"def preprocess_df(df):\n    df['label'] =  df.multiple_diseases * 2 + \\\n                df.rust * 3 + \\\n                df.scab * 4 + \\\n                df.healthy\n    df.label -= 1\n    return df","67ab8c26":"train_df = preprocess_df(train_df)\ntrain_df.head()","b8644c81":"train_df.iloc[:, 1:].sum()","6a0aff82":"class PlantDataset(torch.utils.data.dataset.Dataset):\n    def __init__(self, df, path, transform):\n        super().__init__()\n\n        self.df = df\n        self.path = os.path.join(path, 'images')\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        entry = self.df.iloc[index]\n        fname = entry.image_id + '.npy'\n        fname = os.path.join(self.path, fname)\n\n        img = np.load(fname)\n        img = self.transform(img)\n\n        return img, entry.values[1:5].astype('float64')","5db04e3f":"train_df, val_df = train_test_split(train_df, test_size=0.3, random_state=seed, stratify=train_df.label)","67f2f5fb":"train_weight = val_weight = torch.tensor([\n    [1., 1.],\n    [1.5, 2.],\n    [1., 1.],\n    [1., 1.]\n]).to(device)","b64a653b":"class_weights = torch.tensor([1., 1.3, 1.1, 1.])\nsampler_weights = class_weights[train_df.label.values]","33be571e":"def stack_image(image, **kwargs):\n    to_tensor = albumentations.pytorch.ToTensor()\n    vflip = albumentations.VerticalFlip(always_apply=True)\n    hflip = albumentations.HorizontalFlip(always_apply=True)\n    \n    return torch.stack([\n        to_tensor(image=vflip(image=image)['image'])['image'],\n        to_tensor(image=hflip(image=image)['image'])['image'],\n        to_tensor(image=vflip(image=hflip(image=image)['image'])['image'])['image'],\n        to_tensor(image=image)['image']\n    ])\n\ndef get_transform(img_size, crop_size):\n    train_transform = albumentations.Compose([\n        albumentations.Resize(img_size, img_size, always_apply=True),\n        albumentations.HorizontalFlip(p=0.5),\n        albumentations.VerticalFlip(p=0.5),\n        albumentations.ShiftScaleRotate(rotate_limit=30.0, scale_limit=0.2, shift_limit=0.15, p=0.7),\n        albumentations.CenterCrop(crop_size, crop_size, always_apply=True),\n        albumentations.Normalize(mean, std),\n        albumentations.pytorch.ToTensor()\n    ])\n\n    transform = albumentations.Compose([\n        albumentations.Resize(img_size, img_size, always_apply=True),\n        albumentations.CenterCrop(crop_size, crop_size, always_apply=True),\n        albumentations.Normalize(mean, std),\n        albumentations.pytorch.ToTensor()\n    ])\n\n    tta_transform = albumentations.Compose([\n        albumentations.Resize(img_size, img_size, always_apply=True),\n        albumentations.CenterCrop(crop_size, crop_size, always_apply=True),\n        albumentations.Normalize(mean, std),\n        albumentations.Lambda(stack_image, always_apply=True) \n    ])\n    \n    lambda_train = lambda image: train_transform(image=image)['image']\n    lambda_valid = lambda image: transform(image=image)['image']\n    lambda_tta = lambda image: tta_transform(image=image)['image']\n    \n    return lambda_train, lambda_valid, lambda_tta","521d3797":"model = torchvision.models.resnet18(pretrained=True)\nnum_ft = model.fc.in_features\nmodel.fc = torch.nn.Linear(in_features=num_ft, out_features=4, bias=True)\nmodel = model.to(device)","36fa25e2":"class Softmaxer(torch.nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n        \n    def forward(self, x):\n        x = self.model(x)\n        return F.softmax(x, dim=1)","5aa5f132":"def bce(preds, targets, weight=None):    \n    loss = F.binary_cross_entropy_with_logits(preds, targets.type(preds.dtype), reduction='none')\n    if weight is not None:\n        weight = weight[:, targets.T.long()]\n        idx = np.diag_indices(weight.shape[0])\n        weight = weight[idx[0], idx[1], :]\n        loss *= weight.T\n    return loss.mean()","fd34a07a":"img_size = 250\ncrop_size = 224\nname = f'{arch}-{crop_size}'\nprint(f'{name} image size: {img_size}, crop size: {crop_size}')","f61c20e7":"train_transform, valid_transform, tta_transform = get_transform(img_size=img_size, crop_size=crop_size)\n\ntrain_dataset = PlantDataset(train_df, dataset_path, train_transform)\ntrain_sampler = torch.utils.data.sampler.WeightedRandomSampler(sampler_weights, len(train_df))\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, num_workers=4, \n    sampler=train_sampler, shuffle=False\n)\n\nval_dataset = PlantDataset(val_df, dataset_path, valid_transform)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=10, num_workers=4, shuffle=False)","858ebbcd":"criterion = bce\noptimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=1e-5)\nlr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)","5817f78f":"tracked_metrics = [\n    metrics.MultilabelAccuracy(),\n    metrics.MultilabelRocAuc(),\n    metrics.MultilabelConfusionMatrix()\n]\n\nbest_model = trainer.fit(\n    model, train_dataloader=train_loader, val_dataloader=val_loader,\n    test_dataloader=None, test_every=0, criterion=criterion,\n    optimizer=optimizer, scheduler=lr_scheduler, metrics=tracked_metrics, n_epochs=n_epochs_224,\n    metric_choice='col-auc', mode='max',\n    name=name, device=device, weight={'train': train_weight, 'val': val_weight}\n)","2c040336":"tracked_metrics = [\n    metrics.MultilabelAccuracy(metric='top1-acc', apply_sigmoid=False),\n    metrics.MultilabelRocAuc(apply_sigmoid=False),\n    metrics.MultilabelConfusionMatrix()\n]\n\nsoftmaxer = Softmaxer(model)\nsoftmaxer_best = Softmaxer(best_model)\n\nval_dataset = PlantDataset(val_df, dataset_path, tta_transform)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=10, num_workers=4, shuffle=False)\n\nval_logs = trainer.test(softmaxer, criterion=criterion, test_dataloader=val_loader, metrics=tracked_metrics, weight=val_weight, device=device, tta=True)\nbest_val_logs = trainer.test(softmaxer_best, criterion=criterion, test_dataloader=val_loader, metrics=tracked_metrics, weight=val_weight, device=device, tta=True)","c8478723":"print(f'Final {name} val:', trainer.summarize_metrics(val_logs))\nprint(f'Best {name} val:', trainer.summarize_metrics(best_val_logs))","115b19fc":"ax = sns.heatmap(val_logs['cm'].get(normalized=True), annot=True, fmt=\".2f\", vmin=0., vmax=1.)","51c100a5":"ax = sns.heatmap(best_val_logs['cm'].get(normalized=True), annot=True, fmt=\".2f\", vmin=0., vmax=1.)","27dd744d":"plot_cm(best_val_logs)","0fc10253":"plot_roc_auc(best_val_logs)","b4acf068":"img_size *= 2\ncrop_size *= 2\nname = f'{arch}-{crop_size}'\nprint(f'{name} image size: {img_size}, crop size: {crop_size}')","30cbe535":"train_transform, valid_transform, tta_transform = get_transform(img_size=img_size, crop_size=crop_size)\n\ntrain_dataset = PlantDataset(train_df, dataset_path, train_transform)\ntrain_sampler = torch.utils.data.sampler.WeightedRandomSampler(sampler_weights, len(train_df))\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, num_workers=4, \n    sampler=train_sampler, shuffle=False\n)\n\nval_dataset = PlantDataset(val_df, dataset_path,  valid_transform)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=10, num_workers=4, shuffle=False)","b08b1de9":"criterion = bce\noptimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=1e-5)\nlr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)","fef8c3f8":"tracked_metrics = [\n    metrics.MultilabelAccuracy(apply_sigmoid=True),\n    metrics.MultilabelRocAuc(apply_sigmoid=True),\n    metrics.MultilabelConfusionMatrix()\n]\n\nbest_model = trainer.fit(\n    model, train_dataloader=train_loader, val_dataloader=val_loader,\n    test_dataloader=None, test_every=0, criterion=criterion,\n    optimizer=optimizer, scheduler=lr_scheduler, metrics=tracked_metrics, n_epochs=n_epochs_448,\n    metric_choice='col-auc', mode='max',\n    name=name, device=device, weight={'train': train_weight, 'val': val_weight}\n)","49a49fb2":"tracked_metrics = [\n    metrics.MultilabelAccuracy(metric='top1-acc', apply_sigmoid=False),\n    metrics.MultilabelRocAuc(apply_sigmoid=False),\n    metrics.MultilabelConfusionMatrix()\n]\n\nsoftmaxer = Softmaxer(model)\nsoftmaxer_best = Softmaxer(best_model)\n\nval_dataset = PlantDataset(val_df, dataset_path, tta_transform)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=10, num_workers=4, shuffle=False)\n\nval_logs = trainer.test(softmaxer, criterion=criterion, test_dataloader=val_loader, metrics=tracked_metrics, weight=val_weight, device=device, tta=True)\nbest_val_logs = trainer.test(softmaxer_best, criterion=criterion, test_dataloader=val_loader, metrics=tracked_metrics, weight=val_weight, device=device, tta=True)","6cdade8b":"print(f'Final {name} val:', trainer.summarize_metrics(val_logs))\nprint(f'Best {name} val:', trainer.summarize_metrics(best_val_logs))","301b5d46":"ax = sns.heatmap(val_logs['cm'].get(normalized=True), annot=True, fmt=\".2f\", vmin=0., vmax=1.)","c47bb67e":"ax = sns.heatmap(best_val_logs['cm'].get(normalized=True), annot=True, fmt=\".2f\", vmin=0., vmax=1.)","934ebbe3":"plot_cm(best_val_logs)","fe1ec38e":"plot_roc_auc(best_val_logs)","5304992e":"test_dataset = PlantDataset(test_df, dataset_path, tta_transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False)","bedaba26":"def get_test_preds(model, test_loader):\n    outputs = []\n    for batch_idx, (data, labels) in enumerate(tqdm(test_loader)):\n        batch_size, n_crops, c, h, w = data.size()\n        data = data.view(-1, c, h, w)\n        with torch.no_grad():\n            output = model(data.to(device))\n        output = output.view(batch_size, n_crops, -1).mean(1)\n        outputs.append(output.cpu())\n\n    return torch.cat(outputs, dim=0).numpy()","6d6ef722":"def make_submission_df(df, preds):\n    classes = ['healthy', 'multiple_diseases', 'rust', 'scab']\n    for idx, name in enumerate(classes):\n        df[name] = pd.Series(preds[:, idx])\n    return df","6bc3c07e":"softmaxer.eval()\ntest_preds = get_test_preds(softmaxer, test_loader)\ntest_submission = make_submission_df(test_df, test_preds)\ntest_submission.to_csv(f'submission-{name}.csv', index=False)","5ccee0b9":"softmaxer_best.eval()\ntest_preds = get_test_preds(softmaxer_best, test_loader)\ntest_submission = make_submission_df(test_df, test_preds)\ntest_submission.to_csv(f'submission-best-{name}.csv', index=False)","47e7a6d0":"pd.read_csv(f'submission-{name}.csv').head()","74871ff8":"pd.read_csv(f'submission-best-{name}.csv').head()","70253065":"We'll treat this task as a multilabel classification problem; even if the given data is actually a single-class classification. ","cc1a9771":"# Utilities functions\nUnhide code cell","3fe3f16b":"# Hyperparams & values","f68d915b":"We actually don't need a very large or sophisticated model to reach LB ~0.963. A resnet18 is enough, though you can probably do better with a slightly bigger model","29582f9d":"# Train on 448","5a0b4f39":"Let's see how the best model does on the validation set, with 4xTTA.","72d5ddd7":"# Model creation","87e4c742":"As you can see the *multiple_diseases* class is very unbalanced, so we will apply weights to the loss function. Undersampling of the other classes is not really an option here","54be3ebf":"# Inference & submission","d8a191b7":"I will also apply a slight oversampling on the minority class, by giving it a higher weight.","bf9404df":"### Install pytorchtrainutils and efficientnet-pytorch","f962f3ec":"Very simple yet useful set of util functions for training models in pytorch https:\/\/github.com\/carloalbertobarbano\/pytorch-train-utils","72eae30c":"# Train on 224","464ec592":"Let's increase the resolution up to 448 and apply transfer learning","f0dbd7d4":"# Loss function","9580678a":"# Data transforms & aug","16c34a77":"Applying TTA with albumentations and [pytorchtrainutils](https:\/\/github.com\/carloalbertobarbano\/pytorch-train-utils) is very easy","bd6b524f":"Pytorchtrainutils provides utilities functions and classes (i.e.) metrics","f2ae5fba":"# Data loading\n\nI converted the competition dataset to pickles (.npy). \nImages are already downscaled to 600x600. You can find the dataset (along with the generating script) [here](https:\/\/www.kaggle.com\/carloalbertobarbano\/plantpathology2020fgvc7pickles).\nThe speedup is noticeable.","b1902945":"The loss function is a normal binary cross entropy (with logits), we only need some indexing magic to correctly apply the weight"}}