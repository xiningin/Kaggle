{"cell_type":{"85a25d0b":"code","c10b737d":"code","91e605f4":"code","16aeb301":"code","fd4f6d2e":"code","d0c9e371":"code","38285e82":"code","b1dfbf4d":"code","9494c655":"code","d4a6fb33":"code","15ce1c12":"code","97ef9e55":"code","3e41ca84":"code","e6a67f30":"code","8902a77f":"code","a6a106f4":"code","ea4ecd66":"code","22f74e9d":"code","1a249325":"code","dcc6248a":"code","99cedd2a":"code","dd97d4b7":"code","ad025f87":"code","e4d5fab0":"code","56a03e12":"code","3a976c79":"code","65d76232":"code","e33af535":"code","6114dbb5":"code","eeb60d8c":"code","16b0f693":"code","07fd255f":"code","4e9e6417":"code","c91d9a82":"code","67aa8e9e":"code","9a042532":"code","408b2903":"code","e9b3e036":"code","a8903bc9":"code","59d40960":"code","1830101c":"code","e8532aa0":"markdown","396d87a7":"markdown","a6a8e4d8":"markdown","46f15397":"markdown","990ba1ec":"markdown","f2f17473":"markdown","8aecab44":"markdown","583bd4f8":"markdown","17011c75":"markdown","f557250e":"markdown","361a64d2":"markdown","90da2b8d":"markdown","dac21c40":"markdown","4a7f253d":"markdown","e7e64e4c":"markdown","7fec5c83":"markdown","407382d1":"markdown","3dad01b2":"markdown"},"source":{"85a25d0b":"import pandas as pd\n\n# To do linear algebra\nimport numpy as np\nfrom numpy import pi\n\n# To create plots\nfrom matplotlib.colors import rgb2hex\nfrom matplotlib.cm import get_cmap\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\n# To create nicer plots\nimport seaborn as sns\n\n# To create interactive plots\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\n\n# To get new datatypes and functions\nfrom collections import Counter\nfrom cycler import cycler\n\nfrom scipy.stats import norm,skew, probplot\nfrom scipy.optimize import curve_fit\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\nfrom lightgbm import LGBMClassifier\n\nfrom time import time\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import confusion_matrix\nimport scikitplot as skplt\nimport itertools\nfrom sklearn.model_selection import GridSearchCV\nimport warnings\nwarnings.filterwarnings('ignore')","c10b737d":"#load dataset\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\n\ntrain_df['Data'] = 'Train'\ntest_df['Data'] = 'Test'\n\ntt_df = pd.concat([train_df,test_df], axis = 0).reset_index(drop = True)\n","91e605f4":"#Check null values\ntrain_df.isnull().values.any()\ntest_df.isnull().values.any()\n# no null values in train and test data","16aeb301":"tt_df['subject'] = '#' + tt_df['subject'].astype(str)\n","fd4f6d2e":"tt_df.shape","d0c9e371":"tt_df.columns.values","38285e82":"tt_df['Activity'].value_counts()","b1dfbf4d":"label = tt_df.pop('Activity')","9494c655":"train_df.head()","d4a6fb33":"pd.DataFrame.from_dict(Counter([col.split('-')[0].split('(')[0] \n                                for col in tt_df.columns]), \n                       orient = 'index').rename(columns = {0:'count'}).sort_values('count',ascending=False)\n","15ce1c12":"print('Null values in DataFrme:{}\\n'.format(tt_df.isna().sum().sum()))\ntt_df.info()","97ef9e55":"label_counts = label.value_counts()\n\nn = label_counts.shape[0]\ncolormap = get_cmap('viridis')\ncolors = [rgb2hex(colormap(col)) for col in np.arange(0,1.01,1\/(n-1))]\n\ndata = go.Bar(x = label_counts.index,\n             y = label_counts,\n             marker = dict(color = colors))\n\nlayout = go.Layout(title = 'Smartphone Activity Label Distribution',\n                  xaxis = dict(title = 'Activity'),\n                  yaxis = dict(title = 'Count'))\n\nfig = go.Figure(data = [data], layout=layout)\niplot(fig)","3e41ca84":"tsne_data = tt_df.copy()\ndata_data = tsne_data.pop('Data')\nsubject_data = tsne_data.pop('subject')\n\n# Scale data\nscl = StandardScaler()\ntsne_data = scl.fit_transform(tsne_data)\n\n# Reduce dimensions (speed up)\npca = PCA(n_components=0.9, random_state=3)\ntsne_data = pca.fit_transform(tsne_data)\n\n# Transform data\ntsne = TSNE(random_state=3)\ntsne_transformed = tsne.fit_transform(tsne_data)\n\n\n# Create subplots\nfig, axarr = plt.subplots(2, 1, figsize=(15,10))\n\n### Plot Activities\n# Get colors\nn = label.unique().shape[0]\ncolormap = get_cmap('viridis')\ncolors = [rgb2hex(colormap(col)) for col in np.arange(0, 1.01, 1\/(n-1))]\n\n# Plot each activity\nfor i, group in enumerate(label_counts.index):\n    # Mask to separate sets\n    mask = (label==group).values\n    axarr[0].scatter(x=tsne_transformed[mask][:,0], y=tsne_transformed[mask][:,1], c=colors[i], alpha=0.5, label=group)\naxarr[0].set_title('TSNE: Activity Visualisation')\naxarr[0].legend()\n\n\n### Plot Subjects\n# Get colors\nn = subject_data.unique().shape[0]\ncolormap = get_cmap('gist_ncar')\ncolors = [rgb2hex(colormap(col)) for col in np.arange(0, 1.01, 1\/(n-1))]\n\n# Plot each participant\nfor i, group in enumerate(subject_data.unique()):\n    # Mask to separate sets\n    mask = (subject_data==group).values\n    axarr[1].scatter(x=tsne_transformed[mask][:,0], y=tsne_transformed[mask][:,1], c=colors[i], alpha=0.5, label=group)\n\naxarr[1].set_title('TSNE: Participant Visualisation')\nplt.show()","e6a67f30":"#Split training testing data\nenc = LabelEncoder()\nlabel_encoded = enc.fit_transform(label)\nX_train,X_test, y_train, y_test = train_test_split(tsne_data, label_encoded, random_state=3)\n\n#Create the model\nlgbm = LGBMClassifier(n_estimators = 500, random_state=3)\nlgbm = lgbm.fit(X_train, y_train)\n\n# Test the model\nscore = accuracy_score(y_true=y_test, y_pred=lgbm.predict(X_test))\nprint('Accuracy on testset:\\t{:.4f}\\n'.format(score))","8902a77f":"#Store the data\ndata = []\n#Iterate over each activity\nfor activity in label_counts.index:\n    #Create dataset\n    act_data = tt_df[label==activity].copy()\n    act_data_data = act_data.pop('Data')\n    act_subject_data = act_data.pop('subject')\n    \n    #Scale data\n    scl = StandardScaler()\n    act_data = scl.fit_transform(act_data)\n    \n    #Reduce dimensions\n    pca = PCA(n_components=0.9,random_state=3)\n    act_data = pca.fit_transform(act_data)\n    \n    #Split training testing data\n    enc = LabelEncoder()\n    label_encoded = enc.fit_transform(act_subject_data)\n    X_train,X_test,y_train,y_test = train_test_split(act_data, label_encoded,random_state=3)\n    \n    #Fit basic model\n    print('Activity: {}'.format(activity))\n    lgbm = LGBMClassifier(n_estimators=500,random_state=3)\n    lgbm = lgbm.fit(X_train,y_train)\n    \n    score = accuracy_score(y_true=y_test, y_pred=lgbm.predict(X_test))\n    print('Accuracy on testset: \\t{:.4f}\\n'.format(score))\n    data.append([activity, score])","a6a106f4":"#Create duration datafrae\nduration_df = (tt_df.groupby([label,subject_data])['Data'].count().reset_index().groupby('Activity').agg({'Data':'mean'})*1.28).rename(columns = {'Data':'Seconds'})\nactivity_df = pd.DataFrame(data, columns=['Activity','Accuracy']).set_index('Activity')\nactivity_df.join(duration_df)","ea4ecd66":"# Create dataset\ntsne_data = tt_df[label=='WALKING'].copy()\ndata_data = tsne_data.pop('Data')\nsubject_data = tsne_data.pop('subject')\n\n# Scale data\nscl = StandardScaler()\ntsne_data = scl.fit_transform(tsne_data)\n\n# Split training testing data\nenc = LabelEncoder()\nlabel_encoded = enc.fit_transform(subject_data)\nX_train, X_test, y_train, y_test = train_test_split(tsne_data, label_encoded, random_state=3)\n\n\n# Create model\nlgbm = LGBMClassifier(n_estimators=500, random_state=3)\nlgbm = lgbm.fit(X_train, y_train)\n\n# Get importances\nfeatures = tt_df.drop(['Data', 'subject'], axis=1).columns\nimportances = lgbm.feature_importances_\n\n# Sum importances\ndata = {'Gyroscope':0, 'Accelerometer':0}\nfor importance, feature in zip(importances, features):\n    if 'Gyro' in feature:\n        data['Gyroscope'] += importance\n    if 'Acc' in feature:\n        data['Accelerometer'] += importance\n        \n# Create dataframe and plot\nsensor_df = pd.DataFrame.from_dict(data, orient='index').rename(columns={0:'Importance'})\nsensor_df.plot(kind='barh', figsize=(14,4), title='Sensor Importance For Classifing Participants By Walking Style (Feature Importance Sum)')\nplt.show()","22f74e9d":"# Group the data by participant and compute total duration of staircase walking\nmask = label.isin(['WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS'])\nduration_df = (tt_df[mask].groupby([label[mask], 'subject'])['Data'].count() * 1.28)\n\n# Create plot\nplot_data = duration_df.reset_index().sort_values('Data', ascending=False)\nplot_data['Activity'] = plot_data['Activity'].map({'WALKING_UPSTAIRS':'Upstairs', 'WALKING_DOWNSTAIRS':'Downstairs'})\n\nplt.figure(figsize=(15,5))\nsns.barplot(data=plot_data, x='subject', y='Data', hue='Activity')\nplt.title('Participants Compared By Their Staircase Walking Duration')\nplt.xlabel('Participants')\nplt.ylabel('Total Duration [s]')\nplt.show()","1a249325":"# Create data and plot\nplt.figure(figsize=(15,5))\nplot_data = ((duration_df.loc['WALKING_UPSTAIRS'] \/ duration_df.loc['WALKING_DOWNSTAIRS']) -1).sort_values(ascending=False)\nsns.barplot(x=plot_data.index, y=plot_data)\nplt.title('By What Percentage Is The Participant Faster In Walking Downstairs Than Upstairs?')\nplt.xlabel('Participants')\nplt.ylabel('Percent')\nplt.show()","dcc6248a":"def plotSkew(x):\n    # Fit label to norm\n    (mu, sigma) = norm.fit(x)\n    alpha = skew(x)\n\n    fig, axarr = plt.subplots(1, 2, figsize=(15,4))\n\n    # Plot label and fit\n    sns.distplot(x , fit=norm, ax=axarr[0])\n    axarr[0].legend(['$\\mu=$ {:.2f}, $\\sigma=$ {:.2f}, $\\\\alpha=$ {:.2f}'.format(mu, sigma, alpha)], loc='best')\n    axarr[0].set_title('Staircase Walking Duration Distribution')\n    axarr[0].set_ylabel('Frequency')\n\n    # Plot probability plot\n    res = probplot(x, plot=axarr[1])\n    plt.show()\n    \n    \nplotSkew(duration_df)","99cedd2a":"fig, axarr = plt.subplots(5, 6, figsize=(15,6))\n\nfor person in range(0, 30):\n    # Get data\n    single_person = tt_df[(label=='WALKING') & (tt_df['subject']=='#{}'.format(person+1))].drop(['subject', 'Data'], axis=1)\n    # Scale data\n    scl = StandardScaler()\n    tsne_data = scl.fit_transform(single_person)\n    # Reduce dimensions\n    pca = PCA(n_components=0.9, random_state=3)\n    tsne_data = pca.fit_transform(tsne_data)\n    # Transform data\n    tsne = TSNE(random_state=3)\n    tsne_transformed = tsne.fit_transform(tsne_data)\n    \n    # Create plot\n    axarr[person\/\/6][person%6].plot(tsne_transformed[:,0], tsne_transformed[:,1], '.-')\n    axarr[person\/\/6][person%6].set_title('Participant #{}'.format(person+1))\n    axarr[person\/\/6][person%6].axis('off')\n    \nplt.tight_layout()\nplt.show()","dd97d4b7":"# Group the data by participant and compute total duration of walking\nmask = label=='WALKING'\nduration_df = (tt_df[mask].groupby('subject')['Data'].count() * 1.28)\n\n# Create plot\nplot_data = duration_df.reset_index().sort_values('Data', ascending=False)\n\nplt.figure(figsize=(15,5))\nsns.barplot(data=plot_data, x='subject', y='Data')\nplt.title('Participants Compared By Their Walking Duration')\nplt.xlabel('Participants')\nplt.ylabel('Total Duration [s]')\nplt.show()","ad025f87":"# Create subplots\nfig, axarr = plt.subplots(10, 6, figsize=(15,15))\n\n# Iterate over each participant\nfor person in range(0, 30):\n    # Get data\n    single_person_up = tt_df[(label=='WALKING_UPSTAIRS') & (tt_df['subject']=='#{}'.format(person+1))].drop(['subject', 'Data'], axis=1)\n    single_person_down = tt_df[(label=='WALKING_DOWNSTAIRS') & (tt_df['subject']=='#{}'.format(person+1))].drop(['subject', 'Data'], axis=1)\n    # Scale data\n    scl = StandardScaler()\n    tsne_data_up = scl.fit_transform(single_person_up)\n    tsne_data_down = scl.fit_transform(single_person_down)\n    # Reduce dimensions\n    pca = PCA(n_components=0.9, random_state=3)\n    tsne_data_up = pca.fit_transform(tsne_data_up)\n    tsne_data_down = pca.fit_transform(tsne_data_down)\n    # Transform data\n    tsne = TSNE(random_state=3)\n    tsne_transformed_up = tsne.fit_transform(tsne_data_up)\n    tsne_transformed_down = tsne.fit_transform(tsne_data_down)\n    \n    # Create plot\n    axarr[2*person\/\/6][2*person%6].plot(tsne_transformed_up[:,0], tsne_transformed_up[:,1], '.b-')\n    axarr[2*person\/\/6][2*person%6].set_title('Up: Participant #{}'.format(person+1))\n    axarr[2*person\/\/6][2*person%6].axis('off')\n    axarr[2*person\/\/6][(2*person%6)+1].plot(tsne_transformed_down[:,0], tsne_transformed_down[:,1], '.g-')\n    axarr[2*person\/\/6][(2*person%6)+1].set_title('Down: Participant #{}'.format(person+1))\n    axarr[2*person\/\/6][(2*person%6)+1].axis('off')\n    \nplt.tight_layout()\nplt.show()","e4d5fab0":"# Get data\ntsne_data = tt_df[label=='WALKING'].copy()\ndata_data = tsne_data.pop('Data')\nsubject_data = tsne_data.pop('subject')\n\n# Scale data\nscl = StandardScaler()\ntsne_data = scl.fit_transform(tsne_data)\n\n# Reduce dimensions\npca = PCA(n_components=0.9, random_state=3)\ntsne_data = pca.fit_transform(tsne_data)\n\n# Transform data\ntsne = TSNE(random_state=3)\ntsne_transformed = tsne.fit_transform(tsne_data)\n\n\n# Create subplots\nfig, axarr = plt.subplots(1, 1, figsize=(15,10))\n\n### Plot Subjects\n# Get colors\nn = subject_data.unique().shape[0]\ncolormap = get_cmap('gist_ncar')\ncolors = [rgb2hex(colormap(col)) for col in np.arange(0, 1.01, 1\/(n-1))]\n\nfor i, group in enumerate(subject_data.unique()):\n    # Mask to separate sets\n    mask = (subject_data==group).values\n    axarr.scatter(x=tsne_transformed[mask][:,0], y=tsne_transformed[mask][:,1], c=colors[i], alpha=0.5, label=group)\n\naxarr.set_title('TSNE Walking Style By Participant')\nplt.show()","56a03e12":"train_df=pd.read_csv(\"..\/input\/train.csv\")\ntest_df=pd.read_csv(\"..\/input\/test.csv\")","3a976c79":"#subject col is not useful hence dropped\nif('subject' in train_df.columns):\n    train_df.drop('subject', axis=1, inplace=True)\nif('subject' in test_df.columns):\n    test_df.drop('subject', axis=1, inplace=True)\n","65d76232":"# Encoding target - converting non-numto num variable\nle = preprocessing.LabelEncoder()\nfor x in [train_df,  test_df]:\n    x['Activity'] = le.fit_transform(x.Activity)\n\n# Split into features and class\ndf_traindata, df_trainlabel = train_df.iloc[:,0:len(train_df.columns)-1],train_df.iloc[:,-1]\ndf_testdata, df_testlabel = test_df.iloc[:,0:len(test_df.columns)-1],test_df.iloc[:,-1]","e33af535":"df_trainlabel.value_counts()","6114dbb5":"#Baseline Comparing model accuracy using all features accross classifiers\nclassifiers = [\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    KNeighborsClassifier(),\n    SVC(kernel=\"linear\"),\n    GaussianNB(),\n    LogisticRegression()\n    ]\n\n\n# Naive Train Accuracy\nalgo = []\nscores = []\nfor clf in classifiers:\n    algo.append(clf.__class__.__name__)\n    scores.append(cross_val_score(clf,df_traindata,df_trainlabel, cv=5).mean())\nwarnings.filterwarnings('ignore')\nNaivescore_df_Train = pd.DataFrame({'Algorithm': algo, 'Score': scores}).set_index('Algorithm')\n\n\n# Naive Test Accuracy\n\nalgo = []\nscores = []\n\nfor clf in classifiers:\n    clf = clf.fit(df_traindata, df_trainlabel)\n    y_pred = clf.predict(df_testdata)\n    algo.append(clf.__class__.__name__)\n    scores.append(accuracy_score(y_pred, df_testlabel))\nwarnings.filterwarnings('ignore')\nNaivescore_df_Test  = pd.DataFrame({'Algorithm': algo, 'Score': scores}).set_index('Algorithm')\n\n# Bar plot between Train and Test Accuracy\nfig = plt.figure(figsize=(5,5)) # Create matplotlib figure\n\nax = fig.add_subplot(111) # Create matplotlib axes\nax2 = ax.twinx() # Create another axes that shares the same x-axis as a\nwidth = .3\n\nNaivescore_df_Train.Score.plot(kind='bar',color='green',ax=ax,width=width, position=0)\nNaivescore_df_Test.Score.plot(kind='bar',color='red', ax=ax2,width = width,position=1)\n\nax.grid(None, axis=1)\nax2.grid(None)\n\nax.set_ylabel('Train')\nax2.set_ylabel('Test')\n\nax.set_xlim(-1,7)\nplt.show()","eeb60d8c":"# Feature selection using Random Forest Classifer\n# Bagged decision trees for feature importance - embedded method\nRtree_clf = RandomForestClassifier()\nRtree_clf = Rtree_clf.fit(df_traindata, df_trainlabel)\nmodel = SelectFromModel(Rtree_clf,prefit = True)\nRF_tree_featuresTrain = df_traindata.loc[:,model.get_support()]\nRF_tree_featuresTest = df_testdata.loc[:,model.get_support()]\n\n# Based on Feature Selection only 87 features were selected\n\n# Feature Importance\n# Important scores\n\nimportances = Rtree_clf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in Rtree_clf.estimators_],axis=0)\nindices = np.argsort(importances)[::-1]\nindices.shape\nindices = indices[:200]\n\n#Plot feature importances\n\nplt.figure(1, figsize=(14,13))\nplt.title('Feature importances')\nplt.xlabel('# of features')\nplt.ylabel('Importance score')\nplt.bar(range(200), importances[indices],color='r',yerr=std[indices],align='center')\nplt.xlim([0,200])\nplt.show()","16b0f693":"skplt.estimators.plot_learning_curve(Rtree_clf,RF_tree_featuresTrain,df_trainlabel)","07fd255f":"# Applying RFECV with svm classifier\nsvc=SVC(kernel=\"linear\")\nrfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2), # Stratified fold inorder to reduce bias\n              scoring='accuracy')\nrfetrain=rfecv.fit(RF_tree_featuresTrain, df_trainlabel)\nprint('Optimal number of features :', rfecv.n_features_)\n\n\n# Plot showing the Cross Validation score\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","4e9e6417":"#ApplyingRFE with optimal number of features\nrfe = RFE(estimator=svc, n_features_to_select=rfecv.n_features_, step=1)\nrfe = rfe.fit(RF_tree_featuresTrain, df_trainlabel)\n\nrfe_train=RF_tree_featuresTrain.loc[:,rfe.get_support()]\nrfe_test=RF_tree_featuresTest.loc[:,rfe.get_support()]\n\n#Checking the Accuracy after rfe\n# Train Accuracy\nprint(\"Train Accuracy:\", cross_val_score(svc,rfe_train,df_trainlabel, cv=5).mean())\n#Test Accuracy\nscv = svc.fit(rfe_train,df_trainlabel)\ny_pred = svc.predict(rfe_test)\nprint('Test Accuracy:', accuracy_score(y_pred,df_testlabel))","c91d9a82":"# Variance threshold\nselector = VarianceThreshold(0.95*(1-.95))\nvarsel=selector.fit(rfe_train)\nrfe_train.loc[:,varsel.get_support()].shape\n# 55\nvartrain = rfe_train.loc[:, varsel.get_support()]\nvartest = rfe_test.loc[:,varsel.get_support()]\n\n#Checking the Accuracy after Variance threshold\n# Train Accuracy\nprint('Train Accuracy:',cross_val_score(svc,vartrain,df_trainlabel,cv=5).mean())\n\n# Test Accuracy\nscv = svc.fit(vartrain, df_trainlabel)\ny_pred = scv.predict(vartest)\nprint(\"Test Accuracy:\", accuracy_score(y_pred, df_testlabel))","67aa8e9e":"# PCA\npca = PCA(n_components=len(vartrain.columns))\npca_traindata = pca.fit(vartrain)\n\npca_traindata.explained_variance_\npca_traindata.n_components_\npcatrain = pca_traindata.transform(vartrain)\npcatest = pca_traindata.transform(vartest)\ncum_ratio = (np.cumsum(pca_traindata.explained_variance_ratio_))\n\n# Visualize PCA result\nplt.plot(np.cumsum(pca_traindata.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')","9a042532":"# 21 features - constant after that\npca = PCA(n_components = 21)\npca_traindata = pca.fit(vartrain)\n\npca_traindata.explained_variance_\npca_traindata.n_components_\npcatrain = pca_traindata.transform(vartrain)\npcatest = pca_traindata.transform(vartest)\n(np.cumsum(pca_traindata.explained_variance_ratio_))","408b2903":"# PCA in 2D projection\nskplt.decomposition.plot_pca_2d_projection(pca,vartrain,df_trainlabel)","e9b3e036":"# Checking Accuracy after applying PCA\n\n# Train Accuracy\nprint('Train Accuracy:',cross_val_score(svc,pcatrain,df_trainlabel,cv=5).mean())\n\n# Test Accuracy\nscv = svc.fit(pcatrain, df_trainlabel)\ny_pred = scv.predict(pcatest)\nac_score = accuracy_score(y_pred, df_testlabel)\nprint('Test Accuracy:', accuracy_score(y_pred,df_testlabel))","a8903bc9":"# Confusion Matrixf\ncf_mat = confusion_matrix(df_testlabel,y_pred)\nprint('Accuracy: %f' %ac_score)\nactivities = le.classes_\n\n#Plotting Confusion matrix heatmap\ndef plot_confusion_matrix(cm,classes, normalize=False,\n                         title='Confusion matrix',\n                         cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float')\/cm.sum(axis=1)[:,np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n    \n    print(cm)\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks= np.arange(len(classes))\n    plt.xticks(tick_marks,classes, rotation=45)\n    plt.yticks(tick_marks,classes)\n    \n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max()\/2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n\nplot_confusion_matrix(cf_mat, classes=activities,title=\"Confusion Matrix for Test data\")","59d40960":"# Parameter Tuning \n\n# Perfromance tuning using GridScore\nparam_grid = [\n  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['linear']},\n  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n ]\nsvr = SVC()\nclf = GridSearchCV(svr, param_grid,cv=5)\nclf.fit(pcatrain,df_trainlabel)\nprint(clf.best_params_)","1830101c":"svr = SVC(kernel=\"rbf\",C=1000,gamma=0.001)\nprint(\"Train Accuracy:\",cross_val_score(svr,pcatrain,df_trainlabel, cv=5).mean())\n# Test Accuracy\nscv = svr.fit(pcatrain, df_trainlabel)\ny_pred = scv.predict(pcatest)\nprint(\"Test Accuracy:\",accuracy_score(y_pred, df_testlabel))","e8532aa0":"Applying Variance Threshold method to remove low variance variable","396d87a7":"Is There A Unique Walking Style For Each Participant?","a6a8e4d8":"Confusion Matrix after applying PCA","46f15397":"With test and train accuracy almost equal to 90%, we are getting a maximum accuracy at this level.","990ba1ec":"Train Test Accuracy check with best params and features","f2f17473":"Applying PCA with number of components=21","8aecab44":"How Long Does The Participant Use The Staircase?","583bd4f8":"Accuracy check on Test and Train to see if it has increased","17011c75":"Dimension Reduction using PCA (Principal Component Analysis)","f557250e":"Are There Conspicuities In The Staircase Walking Duration Distribution?","361a64d2":"**Modelling**","90da2b8d":"**Exploring Personal Information**\n","dac21c40":"Hyper Parameter Tuning- finding the best parameters and kernel","4a7f253d":"Is There A Unique Staircase Walking Style For Each Participant?","e7e64e4c":"Variable Encoding for classes","7fec5c83":"Visualizing top 2 principal components in scatter plot with data points segregated based on their activities","407382d1":"How Long Does The Participant Walk?","3dad01b2":"How Much Does The Up-\/Downstairs Ratio Vary?"}}