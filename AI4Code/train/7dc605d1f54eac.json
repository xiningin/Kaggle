{"cell_type":{"1d451e22":"code","580cc9db":"code","49e8ad52":"code","fac94ff7":"code","76d588f2":"code","803c3658":"code","d0a6e0d1":"code","552a949e":"code","7cc71652":"code","f9e060ac":"code","3dcd4a9b":"code","03a58508":"code","7d86f5a2":"code","34fc36f6":"code","dfb72ff9":"code","febb99e4":"code","8ffad19f":"code","7eb8acb2":"code","c180abd5":"code","689850a2":"code","383fef1b":"code","b5888034":"code","71452994":"markdown","c0860005":"markdown","c7dc4b33":"markdown","759b2fba":"markdown","ba80adc3":"markdown","959c5c69":"markdown","89be2638":"markdown","1b2f9030":"markdown","13de01d3":"markdown"},"source":{"1d451e22":"import numpy as np\nimport pandas as pd\nimport pandas_profiling\nfrom sklearn import preprocessing\nimport xgboost","580cc9db":"train_data = pd.read_csv(\"..\/input\/train.csv\")","49e8ad52":"train_data.head()","fac94ff7":"profile = pandas_profiling.ProfileReport(train_data)\nprofile","76d588f2":"#Identify columns type and extract type to array\ncol = [c for c in train_data.columns]\nnumclasses=[]\nfor c in col:\n    numclasses.append(train_data[c].dtypes)\nnumclasses","803c3658":"# from previous extraction find dtype = object and keep column name\ncategorical_variables = list(np.array(col)[np.array(numclasses)==np.object])\ncategorical_variables","d0a6e0d1":"# from extracted names of categorical variables, dummify them and drop the categorical variable from main dataframe\ncollectdf2=[]\nfor name2 in categorical_variables:\n    df2 = pd.get_dummies(train_data[name2],prefix=name2,dummy_na=True)\n    train_data.drop(name2,axis=1,inplace=True)\n    collectdf2=pd.concat([pd.DataFrame(collectdf2),df2],axis=1)\ncollectdf2","552a949e":"# we realise that centralAir is already dummy but with object dtype (Yes or No). Drop one of two created variables\ncollectdf2.drop(\"CentralAir_N\",axis=1,inplace=True)","7cc71652":"# Derive two new variables from YearBuilt and YearRemodAdd\ntrain_data[\"YrAdd\"]= train_data[\"YrSold\"]-train_data[\"YearRemodAdd\"]\ntrain_data[\"Yrbuilt\"]= train_data[\"YrSold\"]-train_data[\"YearBuilt\"]\ntrain_data.drop(\"YearRemodAdd\",axis=1,inplace=True)\ntrain_data.drop(\"YearBuilt\",axis=1,inplace=True)","f9e060ac":"# After fetching data find out 5 addtional potential categorical variables to dummify\ncollectdf1=[]\ncategorical_variables=['MSSubClass','OverallQual','OverallCond','YrSold','MoSold']\nfor name1 in categorical_variables:\n    df1 = pd.get_dummies(train_data[name1],prefix=name1,dummy_na=True)\n    train_data.drop(name1,axis=1,inplace=True)\n    collectdf1=pd.concat([pd.DataFrame(collectdf1),df1],axis=1)\ncollectdf1","3dcd4a9b":"# lets replace missing values with zero\ntrain_data['LotFrontage'] = train_data['LotFrontage'].fillna(0)\ntrain_data['MasVnrArea'] = train_data['MasVnrArea'].fillna(0)\ntrain_data['GarageYrBlt'] = train_data['GarageYrBlt'].fillna(0)","03a58508":"# drop id (useless)\ntrain_data.drop(\"Id\",axis=1,inplace=True)","7d86f5a2":"train_data.corr(method='pearson')","34fc36f6":"X_scaled = preprocessing.scale(train_data.drop(\"SalePrice\",axis=1))","dfb72ff9":"data_ready = pd.concat([pd.DataFrame(X_scaled),collectdf1,collectdf2],axis=1)","febb99e4":"X = data_ready\nY = train_data[\"SalePrice\"]\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nmodel = ExtraTreesClassifier()\nmodel.fit(X, Y)\nprint(model.feature_importances_)","8ffad19f":"#let's concatenate feature importance and columns names\nImport_feat = pd.DataFrame([X.columns,model.feature_importances_]).T\nImport_feat.sort_values(by=1, inplace=True, ascending=False)\nImport_feat","7eb8acb2":"#drop data that has zero in feature importance\ndropcolmn = Import_feat[0][Import_feat[1]==0].values\n\nfor d in dropcolmn:\n    data_ready.drop(d,axis=1,inplace=True)","c180abd5":"X = data_ready\n\nY = np.log(train_data[\"SalePrice\"])","689850a2":"from sklearn.model_selection import train_test_split\n\ntest_size = 0.33\nseed = 7\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)","383fef1b":"model = xgboost.XGBRegressor(learning_rate =0.01,\n n_estimators=5000,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n reg_alpha=0,\n subsample=0.8,\n colsample_bytree=0.8,\n nthread=4,\n objective='reg:linear',                           \n seed=27)\nresults = model.fit(X_train, Y_train,eval_metric=\"rmse\")","b5888034":"Y_HAT = model.predict(X_test)\n\nfrom sklearn.metrics import mean_squared_error\n\nmean_squared_error(Y_test, Y_HAT)","71452994":"**Let's scale the data ! (continuous variables)**","c0860005":"**Data transfomation & one hot encoding**","c7dc4b33":"**Read the data and perform some descriptive statistics**","759b2fba":"**Correlation analysis for continuous variables**","ba80adc3":"**Data is ready !! let's convert the target to logarithm**","959c5c69":"**Result performance**","89be2638":"**Now we can use xgboost and fit**","1b2f9030":"**Too many variables, let's use feature extraction based on feature importance using Trees**","13de01d3":"**Dealing with missing values**"}}