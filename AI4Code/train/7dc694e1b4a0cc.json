{"cell_type":{"1c78e350":"code","57413646":"code","c9ae93c0":"code","ec33d947":"code","b871a6fc":"code","4a2c75a4":"code","089a97f5":"code","315e87fb":"code","fcbaefac":"code","57f42409":"code","8566045b":"code","33f7b64a":"code","5b869162":"code","788309f2":"code","c2c36a7c":"code","15129647":"code","f584279a":"code","44955837":"code","17b4764b":"code","2a4f0021":"code","80bc011a":"code","7e01902b":"code","97fbfba6":"code","af5c6ca5":"code","fc6049ed":"code","cdeabd19":"code","544ab0bc":"code","5d21c5a4":"code","ce585d5b":"code","f42aa434":"code","2050e4c1":"code","35d02bd9":"code","6601b365":"code","93f24f7d":"code","ec445a25":"code","2bfa6879":"code","b9f8be33":"code","f6c28f13":"code","9739c68a":"code","00d66f6d":"code","6cdeb426":"code","b7846c62":"code","28c91ec3":"code","f70bfd08":"code","29a5941b":"code","107cc63c":"code","76d5062c":"code","3b2d56d6":"code","039145b9":"code","bb8e44a9":"code","a2e3595b":"code","51ef5b13":"code","6b5b0744":"code","05bef5b3":"code","5e28be99":"code","b3e8a35b":"code","a5532f74":"code","09c4627f":"code","e72623b1":"code","6f1a9ed4":"code","f550aeb5":"code","7dbb1dbb":"code","5ed7956a":"code","430fee56":"code","e39c66c1":"code","fcc7e0f0":"code","d4722d09":"code","e83b27e7":"code","85c38830":"code","a968f489":"markdown","ca1f2c57":"markdown","d91c6ce9":"markdown","aed153e4":"markdown","32fa6cdb":"markdown","d601500f":"markdown","e38cc447":"markdown","9463743e":"markdown","c4eac77e":"markdown","947ba4c8":"markdown","de8f7ea1":"markdown","e910c264":"markdown","7ade97b1":"markdown","1d633d10":"markdown","8c0e09f6":"markdown","bea56123":"markdown","81fb4ce1":"markdown","bd805b3d":"markdown","0b062dc7":"markdown","65a1385b":"markdown","7ad06eda":"markdown","36da7945":"markdown","547272b1":"markdown","567edf45":"markdown","7873b3c8":"markdown","2719e2e6":"markdown","4d4c6565":"markdown","c8b80ed8":"markdown","618ddcc5":"markdown","a7c8e661":"markdown","a4620638":"markdown","fdb18fb6":"markdown","f55c5627":"markdown","e7ffec21":"markdown","70039667":"markdown","656441c6":"markdown","4d5b8ff6":"markdown","59232a3d":"markdown","483764f0":"markdown","2ee878ab":"markdown","9847f483":"markdown","aa8bce68":"markdown","33bff4a4":"markdown","f62aafde":"markdown","53793bc7":"markdown","840203be":"markdown","57c8e350":"markdown","786ccbd9":"markdown","bcebba12":"markdown","d1141a93":"markdown","0499844d":"markdown","1804c1e8":"markdown","39a226ab":"markdown","4aa95ec4":"markdown","dde801b7":"markdown","af482347":"markdown","1a34a61c":"markdown","74d32b01":"markdown","b49c35a5":"markdown","0c1ad971":"markdown","6a702f6f":"markdown","f9ec53a3":"markdown","6abc15eb":"markdown","c05aea8b":"markdown","ca7a9f65":"markdown"},"source":{"1c78e350":"!git clone https:\/\/github.com\/ISorokos\/SafeML.git","57413646":"import sys\nsys.path.insert(1, '.\/SafeML\/Implementation_in_Python')\n\n# Importing local modules (statistical distance measures)\nfrom CVM_Distance import CVM_Dist as Cramer_Von_Mises_Dist\nfrom Anderson_Darling_Distance import Anderson_Darling_Dist\nfrom Kolmogorov_Smirnov_Distance import Kolmogorov_Smirnov_Dist\nfrom KuiperDistance import Kuiper_Dist\nfrom WassersteinDistance import Wasserstein_Dist\nfrom DTS_Distance import DTS_Dist # Combo of Anderson_Darling and CVM distance.","c9ae93c0":"%matplotlib inline\nimport os, sys # For accessing Python Modules in the System Path (for accessing the Statistical Measures modules)\n# See: https:\/\/stackoverflow.com\/a\/39311677\nnb_dir = os.path.split(os.getcwd())[0]\nif nb_dir not in sys.path:\n    sys.path.append(nb_dir)\n\nimport pandas as pd # For DataFrames, Series, and reading csv data in.\nimport seaborn as sns # Graphing, built ontop of MatPlot for ease-of-use and nicer diagrams.\nimport matplotlib.pyplot as plt # MatPlotLib for graphing data visually. Seaborn more likely to be used.\nimport numpy as np # For manipulating arrays and changing data into correct formats for certain libraries\nimport sklearn # For Machine Learning algorithms\nimport scikitplot # Confusion matrix plotting\nfrom sklearn.decomposition import PCA # For PCA dimensionality reduction technique\nfrom sklearn.preprocessing import StandardScaler # For scaling to unit scale, before PCA application\nfrom sklearn.preprocessing import LabelBinarizer # For converting categorical data into numeric, for modeling stage\nfrom sklearn.model_selection import StratifiedKFold # For optimal train_test splitting, for model input data\nfrom sklearn.model_selection import train_test_split # For basic dataset splitting\nfrom sklearn.neighbors import KNeighborsClassifier # K-Nearest Neighbors ML classifier (default n. of neighbors = 5)\nfrom scikitplot.metrics import plot_confusion_matrix # For plotting confusion matrices\nfrom sklearn.metrics import accuracy_score # For getting the accuracy of a model's predictions\nfrom sklearn.metrics import classification_report # Various metrics for model performance\nfrom sklearn.neural_network import MLPClassifier # For Neural Network classifier\nfrom sklearn.linear_model import LogisticRegression","ec33d947":"def clean_dataset(df):\n    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n    df.dropna(inplace=True)\n    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n    return df[indices_to_keep]","b871a6fc":"def get_PCA_feature_names(num_of_pca_components):\n    feature_names = []\n    for i in range(num_of_pca_components):    \n        feature_names.append(f\"Principal component {i+1}\")\n    return feature_names","4a2c75a4":"# See documentation above to understand what each step does, and why.\ndef train_model_predict(model, model_name, X, y, skf):\n    for train_index, test_index in skf.split(X, y): # 1)\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index] # 2)\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n        reshaped_y_train = np.asarray(y_train).reshape(-1, 1) # 3)\n        reshaped_y_test = np.asarray(y_test).reshape(-1, 1)\n        \n    model.fit(X_train, reshaped_y_train.ravel()) # 4)\n    pred_y = model.predict(X_test) # 5)\n    score = classification_report(reshaped_y_test, pred_y) # 6)\n    print('Classification report: \\n', score, '\\n')\n    plot_confusion_matrix(reshaped_y_test, pred_y, title='Confusion Matrix for {}'.format(model_name))\n        \n    return accuracy_score(reshaped_y_test, pred_y), X_train, X_test, y_train, pred_y","089a97f5":"def get_shuffled_stratifiedKFold_train_test_split(X, y):\n    # Shuffle to True, to get different shuffles each time. Permutations being varied is the goal here.\n    skf = StratifiedKFold(n_splits=3, shuffle=True)\n    \n    # For loop to get index for training and test data, using StratifiedKFold (3 splits)\n    for train_index, test_index in skf.split(X, y):\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        # NOTE: Sometimes 'reshape' needed for label, so that the model can use it (required in SciKit)\n        #reshaped_y_train = np.asarray(y_train).reshape(-1, 1)\n        #reshaped_y_test = np.asarray(y_test).reshape(-1, 1)\n        \n    # After for loop ends, all training and test data has been retrieved thus can return it.\n    return X_train, X_test, y_train, y_test","315e87fb":"def train_and_predict_Neural_Network_MLP_model(X_train, X_test, y_train, y_test):\n    # Instantiating new Neural Network classifier and setting Hyperparameters\n    Neural_Net_model = MLPClassifier(hidden_layer_sizes=(10,), activation='relu',\n                                    solver='adam', alpha=0.01, batch_size='auto',\n                                    learning_rate='adaptive', learning_rate_init=0.1,\n                                    max_iter=2)\n    \n    # Fitting the model is synonymous to training the model. Need to call .ravel() to get array in correct format.\n    Neural_Net_model.fit(X_train, y_train.ravel())\n    \n    # Using the model to predict the label\/ classes, based upon X_test data only. This is the model's answers.\n    pred_y = Neural_Net_model.predict(X_test) \n    \n    # Returning model answers and the accuracy of the model i.e. how well it predicts the answers.\n    return pred_y, accuracy_score(y_test, pred_y)","fcbaefac":"import lightgbm as lgb\n\ndef train_and_predict_LGB(X_train, X_test, y_train, y_test):\n    # Instantiating new Light GBM classifier and setting Hyperparameters\n    lgb_params = {'n_estimators':1000,\n                  'boosting_type': 'gbdt',\n                  'objective': 'binary',\n                  'metric': 'auc',\n                 }\n    #Defining the training dataset\n    lgb_data = lgb.Dataset(X_train, y_train)\n    \n    #Training the Light GBM classifier\n    bst = lgb.train(lgb_params, lgb_data)\n    \n    # Using the model to predict the label\/ classes, based upon X_test data only. This is the model's answers.\n    pred_y = bst.predict(X_test) \n    \n    # Evaluate AUC Score\n    from sklearn.metrics import roc_auc_score\n    \n    # Returning model answers and the accuracy of the model i.e. how well it predicts the answers.\n    return pred_y, roc_auc_score(y_test, pred_y)","57f42409":"# send to top and add documentation\ndef get_statistical_dist_measures_for_class_result(accuracy, X_train_L, X_test_L):\n    # Can use this to loop over all the features, since the ECDF Python methods are currently Univariate only\n    num_of_features = len(X_train_L.columns)\n    \n    # Instantiate empty arrays with large enough size, to hold statistical distance data\n    CVM_distances = np.zeros(num_of_features)\n    Anderson_Darling_distances = np.zeros(num_of_features)\n    Kolmogorov_Smirnov_distances = np.zeros(num_of_features)\n    Kuiper_distances = np.zeros(num_of_features)\n    Wasserstein_distances = np.zeros(num_of_features)\n    DTS_distances = np.zeros(num_of_features)\n\n    for i in range(0, num_of_features):\n        # iloc[:, i] allows selection of the ith feature in the Pandas dataframe\n        # Calling the methods from the imported Python modules (see import section at top of notebook)\n        CVM_distances[i] = Cramer_Von_Mises_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n        Anderson_Darling_distances[i] = Anderson_Darling_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n        Kolmogorov_Smirnov_distances[i] = Kolmogorov_Smirnov_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n        Kuiper_distances[i] = Kuiper_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n        Wasserstein_distances[i] = Wasserstein_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n        DTS_distances[i] = DTS_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n        \n    # Computing mean\/ average, to get ECDF distance of full dataset. Float64 to keep accuracy high.\n    # See: https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.mean.html\n    CVM_distance = np.mean(CVM_distances, dtype=np.float64)\n    Anderson_Darling_distance = np.mean(Anderson_Darling_distances, dtype=np.float64)\n    Kolmogorov_Smirnov_distance = np.mean(Kolmogorov_Smirnov_distances, dtype=np.float64)\n    Kuiper_distance = np.mean(Kuiper_distances, dtype=np.float64)\n    Wasserstein_distance = np.mean(Wasserstein_distances, dtype=np.float64)\n    DTS_distance = np.mean(DTS_distances, dtype=np.float64)\n    \n    # Returning dictionary, for efficient and fast DataFrame creation. Returns mean for each distance.\n    # See https:\/\/stackoverflow.com\/a\/17496530. Fast way to 'append' to dataframe for results table.\n    # PRESERVE THE ORDERING\n    return {'Accuracy': accuracy,\n            'Anderson_Darling_dist': Anderson_Darling_distance,\n            'CVM_dist': CVM_distance,\n            'DTS_dist':DTS_distance,\n            'Kolmogorov_Smirnov_dist':Kolmogorov_Smirnov_distance,\n            'Kuiper_dist': Kuiper_distance,\n            'Wasserstein_dist': Wasserstein_distance}","8566045b":"def get_X_train_and_test_data_for_given_label(labels, label_index, pred_y, X_train, X_test, y_train, y_test):\n    X_train_loc_for_label = X_train.loc[y_train == labels[label_index]]\n    X_test_loc_for_label = X_test.loc[pred_y == labels[label_index]]\n    \n    return X_train_loc_for_label, X_test_loc_for_label","33f7b64a":"# Using f-strings: https:\/\/www.journaldev.com\/23642\/python-concatenate-string-and-int\ndef get_concatenated_results_holding_all_classes_results(number_of_classes, result_dataframes):\n    # Empty list to dynamically hold the dataframes which have been assigned a new dataset column\n    dataframes = []\n    \n    # For each class\/ label\n    for i in range(number_of_classes):\n        class_name = f'class{i}'\n        dataframes.append(result_dataframes[i].assign(dataset = class_name))\n    \n    concatenated = pd.concat(dataframes)\n    \n    return concatenated","5b869162":"# 'Reduced dimensions' variable for altering the number of PCA principal components. Can be altered for needs.\n# Only 7 principal components needed when using non-normalised PCA dataset.\ndimensions_num_for_PCA = 7\n\n# Max number of permutations to run. Can be altered for needs.\nnumber_of_permutations = 100\n\n# 10 folds is usually the heuristic to follow for larger datasets of around this size.\nnum_of_splits_for_skf = 10\n\n# Seed value to pass into models so that repeated runs result in the same output\nseed_val = 1\n\n# Number of statistical distance measures to run (for the results, columns section)\nnum_of_statistical_dist_measures = 6","788309f2":"# The .csv dataset file should be in the same project directory as this Jupyter Notebook file\nFriday_Morning_Data = pd.read_csv('..\/input\/cicids2017\/MachineLearningCSV\/MachineLearningCVE\/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv')\ndf = Friday_Morning_Data.copy()\nprint(\"Datatype of Dataframe i.e. Pandas Dataframe: \", type(df))\nprint(\"Datatype of Column i.e. Pandas Series: \", type(df.iloc[:, 1]))\ndf.head()","c2c36a7c":"df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\ndf.head()","15129647":"df.dtypes","f584279a":"df_cleaned = df.copy()\ndf_cleaned = clean_dataset(df_cleaned) # see methods at top of notebook\ndf_cleaned","44955837":"df_cleaned = df_cleaned.reset_index()\n# Removing un-needed index column added by reset_index method\ndf_cleaned.drop('index', axis=1, inplace=True)\ndf_cleaned","17b4764b":"df.columns.tolist()","2a4f0021":"# Saving the label attribute before dropping it.\ndf_labels = df_cleaned['label']\n# Shows all the possible labels\/ classes a model can predict.\n# Need to alter these to numeric 0, 1, etc... for model comprehension (e.g. pd.get_dummies()).\ndf_labels.unique()","80bc011a":"# Axis=1 means columns. Axis=0 means rows. inplace=False means that the original 'df' isn't altered.\ndf_no_labels = df_cleaned.drop('label', axis=1, inplace=False)\n# Getting feature names for the StandardScaler process\ndf_features = df_no_labels.columns.tolist()\n# Printing out Dataframe with no label column, to show successful dropping\ndf_no_labels","7e01902b":"df_scaled = StandardScaler().fit_transform(df_no_labels)\n# Converting back to dataframe\ndf_scaled = pd.DataFrame(data = df_scaled, columns = df_features)\ndf_scaled","97fbfba6":"pca_test = PCA().fit(df_scaled)\nplt.plot(np.cumsum(pca_test.explained_variance_ratio_))\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.show()","af5c6ca5":"# The df_no_labels dataset holds the un-normalised dataset.\npca_test = PCA().fit(df_no_labels)\nplt.plot(np.cumsum(pca_test.explained_variance_ratio_))\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.show()","fc6049ed":"pca = PCA(n_components=dimensions_num_for_PCA)\n#principal_components = pca.fit(df_scaled).transform(df_scaled) => for normalised PCA\n\n# Non-normalised PCA\nprincipal_components = pca.fit(df_no_labels).transform(df_no_labels)\nprincipal_components","cdeabd19":"# See Methods at the top of the notebook\nprincipal_component_headings = get_PCA_feature_names(dimensions_num_for_PCA)","544ab0bc":"df_pc = pd.DataFrame(data = principal_components, columns = principal_component_headings)\ndf_pc","5d21c5a4":"df_final = pd.concat([df_pc, df_labels], axis = 1)\n# Scroll to the RHS end of dataframe to see attached label feature\ndf_final","ce585d5b":"lb = LabelBinarizer()\ndf_final['label'] = lb.fit_transform(df_final['label'])\ndf_final","f42aa434":"print(\"Before LabelBinarizer: \", df_labels.unique())\nprint(\"After LabelBinarizer: \", df_final['label'].unique())","2050e4c1":"# Separating the label so that the answers aren't provided to the model, in training.\nX = df_final.drop(['label'], axis = 1)\ny = df_final['label']\ny","35d02bd9":"skf = StratifiedKFold(n_splits=num_of_splits_for_skf, shuffle=False)\nskf","6601b365":"for train_index, test_index in skf.split(X, y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    reshaped_y_train = np.asarray(y_train).reshape(-1, 1)\n    reshaped_y_test = np.asarray(y_test).reshape(-1, 1)\n    \nprint( 'X_train length: ', len(X_train) ) # To check if splits worked\nprint( 'y_train length: ', len(y_train) )\nprint( 'X_test length: ', len(X_test) )\nprint( 'y_test length: ', len(y_test) )","93f24f7d":"knn_model = KNeighborsClassifier(n_neighbors=5, weights='uniform',\n                                    algorithm='auto', leaf_size=30,\n                                    p=2, metric='minkowski',\n                                    metric_params=None, n_jobs=None)","ec445a25":"# Unpacking the method return values. Last 4 are needed for statistical distance measure methods.\naccuracy, X_train, X_test, y_train, pred_y = train_model_predict(knn_model, \"K-Nearest Neighbor\", X, y, skf)\nprint(\"Model accuracy= \", accuracy*100, \"%\\n\")\nprint(\"Dataset labels: \", df_labels.unique())\nprint(\"Dataset numeric labels after encoding for model: \", df_final['label'].unique())","2bfa6879":"# Extracting the number of classes and labels from the label feature\nclass_num = len(df_final['label'].unique())\nlabels = df_final['label'].unique()\nprint(\"Number of classes: \", class_num)\nprint(\"Labels: \", labels)","b9f8be33":"X_train","f6c28f13":"# x1 = X_test[np.where(np.asarray(y_train).reshape(-1, 1).ravel() == labels[0])]\n# X_train_L = X_train.iloc[np.where(y_train[:,1] == 1)]\nX_train_L = X_train.loc[y_train == labels[0]]\nX_train_L","9739c68a":"X_test_L = X_test.loc[pred_y == labels[0]]\nX_test_L","00d66f6d":"num_of_features = len(X_train_L.columns)\nnum_of_features","6cdeb426":"CVM_distances = np.zeros(num_of_features)\nAnderson_Darling_distances = np.zeros(num_of_features)\nKolmogorov_Smirnov_distances = np.zeros(num_of_features)\nKuiper_distances = np.zeros(num_of_features)\nWasserstein_distances = np.zeros(num_of_features)\nDTS_distances = np.zeros(num_of_features)\n\nfor i in range(0, num_of_features):\n    # iloc[:, i] allows selection of the ith feature in the Pandas dataframe \n    CVM_distances[i] = Cramer_Von_Mises_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n    Anderson_Darling_distances[i] = Anderson_Darling_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n    Kolmogorov_Smirnov_distances[i] = Kolmogorov_Smirnov_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n    Kuiper_distances[i] = Kuiper_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n    Wasserstein_distances[i] = Wasserstein_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n    DTS_distances[i] = DTS_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n    \nprint(\" Cramer Von Mises distances: \", CVM_distances)\nprint(\"\\n Anderson Darling distances: \", Anderson_Darling_distances)\nprint(\"\\n Kolmogorov Smirnov distances: \", Kolmogorov_Smirnov_distances)\nprint(\"\\n Kuiper distances: \", Kuiper_distances)\nprint(\"\\n Wasserstein distances: \", Wasserstein_distances)\nprint(\"\\n DTS distances: \", DTS_distances)","b7846c62":"fig, axs = plt.subplots(2, 3)\n\naxs[0, 0].boxplot(CVM_distances)\naxs[0, 0].set_title('CVM distances')\naxs[0, 0].set_ylabel(\"Distance\")\n\naxs[0, 1].boxplot(Anderson_Darling_distances)\naxs[0, 1].set_title('Anderson Darling distances')\naxs[0, 1].set_ylabel(\"Distance\")\n\naxs[0, 2].boxplot(Kolmogorov_Smirnov_distances)\naxs[0, 2].set_title('Kolmogorov Smirnov distances')\naxs[0, 2].set_ylabel(\"Distance\")\n\naxs[1, 0].boxplot(Kuiper_distances)\naxs[1, 0].set_title(\"Kuiper distances\")\naxs[1, 0].set_ylabel(\"Distance\")\n\naxs[1, 1].boxplot(Wasserstein_distances)\naxs[1, 1].set_title(\"Wasserstein distances\")\naxs[1, 1].set_ylabel(\"Distance\")\n\naxs[1, 2].boxplot(DTS_distances)\naxs[1, 2].set_title(\"DTS distances\")\naxs[1, 2].set_ylabel(\"Distance\")\n\nfig.subplots_adjust(left=0.08, right=1.8, bottom=0.05, top=1.5, hspace=0.5, wspace=0.5)\n\nplt.show()","28c91ec3":"df_final","f70bfd08":"# Firstly, creating the final 2D-array (Pandas Dataframe) which will be used to store the Results\n# PRESERVE THE ORDERING\nresults_column_names = ['Accuracy', 'Anderson_Darling_dist', 'CVM_dist',\n                                     'DTS_dist', 'Kolmogorov_Smirnov_dist','Kuiper_dist', 'Wasserstein_dist']\n# Creating the empty Dataframe for Results\ndf_results = pd.DataFrame(columns = results_column_names)\n# Can copy this dataframe for future results tables e.g. for each class\/ label\ndf_results","29a5941b":"number_of_permutations = 100","107cc63c":"from tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\n# 1st Set all variables needed outside for loop scope, e.g. the multi-dimensional Results array.\nlabels = df_final['label'].unique()\nnumber_of_classes = len(df_final['label'].unique())\n\n# List of Lists to hold each separate label result, using list_index->label_number mapping (for accessing)\nlist_of_lists_results = [[] for i in range(number_of_classes)]\n\nfor current_permutation in tqdm(range(number_of_permutations)):\n    # 1.1. Cross validation, train test stratified splitting (extracted to a function, see top of file)\n    #koo #X_train, X_test, y_train, y_test = get_shuffled_stratifiedKFold_train_test_split(X, y)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=114)\n    \n    # 1.2. Train MLP (Neural net) model using the train test data, and model.predict to get pred_y\n    pred_y, accuracy = train_and_predict_Neural_Network_MLP_model(X_train, X_test, y_train, y_test)\n    #koo #pred_y, accuracy = train_and_predict_LGB(X_train, X_test, y_train, y_test)\n    \n    # 1.3. loop over each label\/ class\n    for current_label in range(number_of_classes):\n        \n        # 1.4. Gets the correct indices for the train and test data ('X' data holds no labels)\n        X_train_loc_for_label, X_test_loc_for_label = get_X_train_and_test_data_for_given_label(labels,\n                                                        current_label, pred_y, X_train, X_test, y_train, y_test)\n        \n        # 1.5. Gets all ECDF statistical distance measures for current_label\n        dict_result_row = get_statistical_dist_measures_for_class_result(accuracy,\n                                                X_train_loc_for_label, X_test_loc_for_label)\n        \n        # 1.6. Append new dict row to current_label index of list of lists\n        list_of_lists_results[current_label].append(dict_result_row)","76d5062c":"# Access specific dataframe by index e.g. class 1 dataframe->index 1 mapping\nresult_dataframes = []\n\nfor dict_result_list in list_of_lists_results:\n    result_dataframes.append(pd.DataFrame(dict_result_list, columns = results_column_names))","3b2d56d6":"# Print first dataframe result table, for class 0\nresult_dataframes[0].head()","039145b9":"!pip install openpyxl","bb8e44a9":"result_dataframes[0].to_excel(\"Class0.xlsx\")","a2e3595b":"# Print second dataframe result table, for class 1\nresult_dataframes[1].head(8)","51ef5b13":"result_dataframes[1].to_excel(\"Class1.xlsx\")","6b5b0744":"concatenated = get_concatenated_results_holding_all_classes_results(number_of_classes, result_dataframes)\nconcatenated.head(110)","05bef5b3":"sns.relplot(data=concatenated, x='Accuracy', y='Anderson_Darling_dist',\n                hue='dataset', height=8.27, aspect=11.7\/8.27)","5e28be99":"plot_title = \"100 Permutations, Friday Afternoon PortScan dataset\"\n\ng = sns.pairplot(\n    data=concatenated,\n    y_vars=['Accuracy'],\n    x_vars=['Anderson_Darling_dist', 'CVM_dist','DTS_dist'],\n    hue='dataset',\n    height=4,\n    aspect=1.4\n)\ng.fig.suptitle(plot_title, y=1.08, fontsize='x-large')\n\nsns.pairplot(\n    data=concatenated,\n    y_vars=['Accuracy'],\n    x_vars=['Kolmogorov_Smirnov_dist','Kuiper_dist', 'Wasserstein_dist'],\n    hue='dataset',\n    height=4,\n    aspect=1.4\n)","b3e8a35b":"plt.figure(figsize=(14,8))\nsns.lineplot(data=concatenated, x='Accuracy', y='Anderson_Darling_dist', hue=\"dataset\", style=\"dataset\",\n    markers=True, dashes=False, err_style=\"bars\", ci=40, estimator=\"mean\")","a5532f74":"plt.figure(figsize=(14,8))\nsns.lineplot(data=concatenated, x='Accuracy', y='CVM_dist', hue=\"dataset\", style=\"dataset\",\n    markers=True, dashes=False)","09c4627f":"plt.figure(figsize=(14,8))\nsns.lineplot(data=concatenated, x='Accuracy', y='DTS_dist', hue=\"dataset\", style=\"dataset\",\n    markers=True, dashes=False)","e72623b1":"plt.figure(figsize=(14,8))\nsns.lineplot(data=concatenated, x='Accuracy', y='Kolmogorov_Smirnov_dist', hue=\"dataset\", style=\"dataset\",\n    markers=True, dashes=False)","6f1a9ed4":"plt.figure(figsize=(14,8))\nsns.lineplot(data=concatenated, x='Accuracy', y='Kuiper_dist', hue=\"dataset\", style=\"dataset\",\n    markers=True, dashes=False)","f550aeb5":"plt.figure(figsize=(14,8))\nsns.lineplot(data=concatenated, x='Accuracy', y='Wasserstein_dist', hue=\"dataset\", style=\"dataset\",\n    markers=True, dashes=False)","7dbb1dbb":"sns.boxplot(data=concatenated, x='dataset', y='Anderson_Darling_dist')\n\naccuracies = concatenated['Accuracy']\nmean_accuracy = accuracies[concatenated['dataset'] == 'class0'].mean()\nprint('Mean Accuracy (Mu) =', mean_accuracy*100, '%')","5ed7956a":"concatenated","430fee56":"import seaborn as sns\nsns.pairplot(concatenated, hue=\"dataset\")","e39c66c1":"sns.pairplot(concatenated, kind=\"kde\", hue=\"dataset\")","fcc7e0f0":"#source1: https:\/\/github.com\/ietuday\/fullstack-python\/blob\/199061d392bcd531ec03ea04f52a20d9482427fb\/Bayesian%20Projects\/6.BayesianInferenceMean.ipynb\n\n#source2: https:\/\/subscription.packtpub.com\/book\/big_data_and_business_intelligence\/9781838823733\/1\/ch01lvl1sec15\/bayesian-analysis-for-means\n\nimport numpy as np\nfrom scipy.special import gamma\nfrom scipy.stats import t\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef dnig(x, s, mu, nu, alpha, beta):\n    \"\"\"Computes the PDF of the NIG(mu, nu, alpha, beta) distribution\n    \n    args:\n        x: float; The x coordinate of the PDF\n        s: float; The s coordinate of the PDF (corresponds to sigma^2)\n        mu: float; The mu parameter of NIG(mu, nu, alpha, beta)\n        nu: float; The nu parameter of NIG(mu, nu, alpha, beta)\n        alpha: float; The alpha parameter of NIG(mu, nu, alpha, beta)\n        beta: float; The beta parameter of NIG(mu, nu, alpha, beta)\n    \n    return:\n        float; The value of the PDF\n    \"\"\"\n    \n    if nu < 0 or alpha < 0 or beta < 0:\n        raise ValueError(\"Cannot have negative nu, alpha, beta\")\n    \n    return np.sqrt(nu \/ (np.abs(s) * 2 * np.pi)) * beta ** alpha \/ gamma(alpha) *\\\n           s**(-(alpha + 1)) * np.exp(-(2*beta + nu * (x - mu)**2)\/(2*s)) * np.maximum(s, 0)\n\ndef get_posterior_nig(x, mu, nu, alpha, beta):\n    \"\"\"Computes the parameters of the posterior NIG distribution\n    \n    args:\n        x: array-like; The data set\n        mu: float; prior parameter mu\n        nu: float; prior parameter nu\n        alpha: foat; prior parameter alpha\n        beta: float; prior parameter beta\n    \n    return:\n        tuple: Of the form (mu, nu, alpha, beta)\n    \"\"\"\n    \n    if nu < 0 or alpha < 0 or beta < 0:\n        raise ValueError(\"Cannot have negative nu, alpha, beta\")\n    \n    xbar = x.mean()\n    n = len(x)\n    \n    p_mu = (nu * mu + n * xbar)\/(nu + n)\n    p_nu = nu + n\n    p_alpha = alpha + n\/2\n    p_beta = beta + ((x - xbar)**2).sum()\/2 + n * nu \/ (n + nu) * (xbar - mu)**2\/2\n    \n    return (p_mu, p_nu, p_alpha, p_beta)","d4722d09":"# Demonstrating dnig\nx = np.linspace(-3, 3)\ny = np.linspace(-0.5, 11)\nX, Y = np.meshgrid(x, y)\n\nplt.contour(X, Y, dnig(X, Y, 0, 1, 1\/2, 1\/2))\nplt.show()","e83b27e7":"class0 = result_dataframes[0].copy()\npost_x = get_posterior_nig(class0['Kolmogorov_Smirnov_dist'], 1, 1, 1\/2, 0.0005)\npost_x","85c38830":"class0 = result_dataframes[0].copy()\n\n\nmu_x = np.mean(class0['Kuiper_dist'])\nvar_x = np.var(class0['Kuiper_dist'])\n\nv_x = (mu_x*(1-mu_x)\/var_x)-1\nalpha_x = mu_x*v_x\nbeta_x = (1-mu_x)*v_x\n\ngamma_y = np.mean(class0['Accuracy'])\nnu_y = var_x\/v_x\n\nx = np.linspace(-30, 30)\ny = np.linspace(-1, 500)\n\nX, Y = np.meshgrid(x, y)\n\nplt.contour(X, Y, dnig(X, Y, 0.65, 101, 50.5, 0.2))\nplt.show()","a968f489":"**train_model_predict()** method is used to train an input model, using StratifiedFKold for train_test splitting, and uses the trained model to predict the test data. It outputs a classification report which has various useful prediction metrics displayed. It also outputs a confusion matrix for the model's predictions. Finally, it returns the accuracy of the model's predictions.\n\n- 1) The for loop ('for train_index, test_index in skf.split(X, y):') is required as it uses the indexes that the  StratifiedKFold model (**skf**) produces to select the appropriate data rows\/ points required for each data split.\n\n\n- 2) The 'X_train, X_test = X.iloc[train_index], X.iloc[test_index]' uses the skf indexes to find the index location (iloc) of each index, so it can extract the correct rows for the train_test split.\n\n\n- 3) The 'reshaped_y_train = np.asarray(y_train).reshape(-1, 1)' is required to reshape the label (y_train and y_test) to a 1D array, rather than a 2D array that is output by the train_test split.\n\n\n- 4) The 'model.fit(X_train, reshaped_y_train.ravel())' uses the input model and fits it (trains the model) on the training data. The '.ravel()' method just reshapes the label array again (flattens it) to match the input structure required by the sklearn method.\n\n\n- 5) The 'pred_y = model.predict(X_test)' uses the, now trained, model to attempt to predict the test data (X_test is passed in, and it predicts the label, pred_y).\n\n\n- 6) The 'score = classification_report(reshaped_y_test, pred_y)' calculates prediction metrics based upon the model's predictions. More info in the docs: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.classification_report.html\n\nThe rest is self-explanatory. A confusion matrix is plotted and output after the method runs. The accuracy of the model is returned back to the caller, as well as other data required for the statistical distance measure methods.","ca1f2c57":"**train_and_predict_Neural_Network_MLP_model()** method trains a Neural Network (MLP) classifier and uses this to predict the labels of the X_test data. See more in-depth information about Neural Networks above^^^. For in-depth info on the hyperparameter tuning in SciKit (will be similar to TensorFlow & Keras, PyTorch, FastAI, and any other ML library), see below... (https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPClassifier.html)\n\n**Hyperparameter explanations:**\n- **activiation=** ReLu is an industry-standard activation function \"rectified linear unit function, f(x) = max(0, x)\" (SciKit docs).\n- **hidden_layer_sizes=** (x,) means 1 hidden layer with x nodes in. Each comma added defines a new hidden layer.\n- **solver=** Adam solver works efficiently on large datasets, and is the \"stochastic gradient-based optimizer\" (SciKit docs).\n- **alpha=** is a penalty (regularization term) parameter.\n- **batch_size=** refers to the size of mini-batches for stochastic optimizers. If the solver is \u2018lbfgs\u2019, the classifier will not use minibatch. When set to \u201cauto\u201d, batch_size=min(200, n_samples).\n- **learning_rate=** is the \"learning rate schedule for weight updates\". Setting as 'constant' would keep the learning rate the same as the learning_rate_init value, even if two epochs were to not decrease by at least 'tol', which is not ideal. Thus, an 'adaptive' learning rate is used and this \"keeps the learning rate constant to \u2018learning_rate_init\u2019 as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if \u2018early_stopping\u2019 is on, the current learning rate is divided by 5\" (SciKit docs).\n- **learning_rate_init=** is just the initial learning rate value. It \"controls the step-size in updating the weights\".\n- **max_iter=** refers to the maximum number of iterations the MLP classifier will run through (each iteration\/ epoch updates the node weights in accordance to other hyperparam info e.g. activation function, alpha, learning rate, etc... This is to try and increase the predictive capability of the MLP model). SciKit docs state \"For stochastic solvers (\u2018sgd\u2019, \u2018adam\u2019), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps\". Although, the more iterations, the longer the training takes. After a certain threshold, more iterations will not improve the predictive capability of the MLP model.\n\nNote: Could add if statement to print out confusion matrix x amount of times, during this method.","d91c6ce9":"# Data Preparation stage\n\nSee CRISP-DM for more information. Following industry-standard Data Science\/ ML practises.\n\nSee: https:\/\/www.datascience-pm.com\/crisp-dm-2\/","aed153e4":"### Transforming the label feature's categorical data into numeric data (via LabelBinarizer)\n\nAgain, a model can't understand, for example, 'yes' and 'no' strings but... these can be mapped to a 1 for yes and a 0 for no. Then a model can understand, as it requires numeric input to distinguish the feature's domain of values.\n\nThe **sklearn.preprocessing.LabelBinarizer** can be used to convert the column data into binary numbers, which will then be correctly interpreted.\n\n1. Fit the List- this tells the LabelBinarizer what values exist, and how to map them. \n\n2. Call transform, passing a List, and this will return the encoded List.\n\n**(Note: if label column has more than 2 unique labels, pandas.get_dummies is required instead)**\n\n(Code reference: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelBinarizer.html)","32fa6cdb":"Pre-finished code using just the first label in the labels array. Selects all the relevant rows\/ data points. \n\n- **.loc[]** method can be used to locate certain rows based upon the query passed into the square brackets[].","d601500f":"Running the statistical measure (univariate version) methods from the local Python modules defined in the project folder.\n\nBecause the Python methods are slightly different to the current MATLAB versions **(as of 8th March 2021)**, one has to pass in a feature 1 at a time (univariate). Then, after all the features have been passed in and sent back, this means the full dataset has been used to calculate the various ECDF-based statistical distance measures. This is what is displayed in the plotting of results section below.","e38cc447":"**get_PCA_feature_names()** method is used to generate feature names for the number of PCA components passed in as a param. Returns a list of feature names for principal component column headings, in a Pandas Dataframe.\n\n- No special code here, just pure Python code. Instantiating a list, and populating it with strings for the PCA header names. Then, appending each to the list and returning it.","9463743e":"Turning the Principal Components back into a Pandas Dataframe, ready for concatting back with the **label** feature.","c4eac77e":"### Seaborn pairplot\nA Seaborn **'pairplot'** can be used to plot 1 continuous feature against another, as a scatterplot in the example below.\n\nThe **data=x** parameter is where the Pandas Dataframe (results) is to be passed. Seaborn only allows 1 Pandas Dataframe to be passed in. This is where it can differ from Matplotlib. Seaborn can use this Dataframe **x**, and it can pull out column names, etc... automatically for you. Thus, instead of writing **'y_vars=concatenated['Accuracy']'**, which is what one would usually have to do, you can just write the column name, and Seaborn will find this in via the **data=x** provided.\n\nThe **y_vars=x[y]** parameter defines what column's data will be used on the y-axis. The column\/ feature name, **y**, provided via **x[y]** is the column that will be used in the plot.\n\nThe **x_vars=x[x1, x2, x3, ...]** parameter defines what column or columns' data will be used on the x-axis. Since a pairplot is built specific to plot 'pairs' of columns against each other, an array can be passed into this. Each new array index will add an extra plot (horizontally) to the pair plot.\n\nThe **hue** parameter describes which feature\/ column will be used to differentiate between data points plotted. It's like a **legend** or **c** parameter in Matplotlib. See here for in-depth explanation, comparing Matplotlib and Seaborn: https:\/\/stackoverflow.com\/a\/26139658\n\nThe **height** and **aspect** parameters are used to define the size of the plots. Height for the individual plot height, and aspect to control the width of each plot.\n\nSee docs: https:\/\/seaborn.pydata.org\/generated\/seaborn.pairplot.html","947ba4c8":"### Seaborn lineplot\n\nA Seaborn lineplot is just a normal lineplot graph, where the x and y axis can be defined, and a hue for colouring and differentiating each class can be defined on a column.\n\nThe **markers=True** parameter defines an \"Object determining how to draw the markers for different levels of the style variable. Setting to True will use default markers\" (Seaborn docs).\n\nThe **dashes=False** parameter just prevents dashing lines being used, which can make the plot less pleasing-to-the-eye.\n\nThe **err_style=\"bars\"** parameter defines that the std should be shown as bars, instead of the default which can be really distracting. The **ci=40** parameter is linked to this: \"Show error bars instead of error bands and plot the 68% confidence interval (standard error)\" (Seaborn docs).\n\nThe **estimator=\"mean\"** parameter just denotes that each point should plot the mean value, if multiple points lie on the exact same x-axis level i.e. 2 points with exactly 98.8687% accuracy for example. The distance of these 2 points will be used to calculate a mean value, and this mean value will be plotted.\n\nSee docs here: https:\/\/seaborn.pydata.org\/generated\/seaborn.lineplot.html#seaborn.lineplot\n\nThe **plt.figure(figsize=(14,8))** just defines the plot\/ graph size, in inches on the screen. (14, 8) is 14 inch width and 8 inch height. Used in Matplotlib, see: https:\/\/matplotlib.org\/stable\/api\/_as_gen\/matplotlib.pyplot.figure.html\n\n### What do the below graphs show?\n\n**All the lineplots below show the accuracy plotted against each ECDF-based statistical distance measure, where each data point is 1 permutation**","de8f7ea1":"### Getting Principal Component feature names, dynamically\n\nGetting the Principal Component feature names, dynamically, for the optimal number of components (passed in as a param). Allows dynamic changing of PCs used. \n\ni.e. **One can change the 'dimensions_num_for_PCA' environment variable, and all the code will still work.**","e910c264":"#### Finding the optimal Neural Network hyperparameters is hard\n\"So what about size of the hidden layer(s)--how many neurons? There are some empirically-derived rules-of-thumb, of these, the most commonly relied on is 'the optimal size of the hidden layer is usually between the size of the input and size of the output layers\" by Jeff Heaton (https:\/\/stats.stackexchange.com\/questions\/181\/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw). As per the reference, a low amount of nodes in the hidden layer can be ideal for a particular dataset. \n\nIf the Input size is around 20 PCA component features, with 1 output node (for the label), then between 10 and 15 nodes, with 1 hidden layer, would be following these heuristics. It can be incredibly hard to train larger Neural Network correctly, although it has been proved- theoretically- that a Neural Network can solve any task, as long as it's large enough and has the correct hyperparameter tuning. This is an NP-hard problems though.\n\nSee docs: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPClassifier.html","7ade97b1":"**get_X_train_and_test_data_for_given_label()** method is used to get the X_train and X_test data rows that match the desired class\/ label input, in the parameter. \n\nThe **labels** parameter holds a numeric array, representing the number of classes\/ labels in the y_train and y_test variables e.g. labels = [0, 1], which denotes that there are 2 classes\/ labels in the incoming y_train and test parameters.\n\nThe **label_index** parameter holds the specific index for the **labels** array, to denote which specific class\/ label is to be used e.g. label_index = 0, which denotes that label\/ class 0 is the one to be queried for in the X_train and X_test data.\n\nThe **pred_y** parameter holds the model's predictions for which class a row should be (from fitting and predicting, using a model). This is used to query the X_test data to figure out how different the X_test and pred_y is from each other- as this is essentially what is needed for calculating the ECDF-based statistical distance measures for a class.\n\nThe other params **X_train, X_test, y_train, y_test** are just the dataset split done, for a model to trained. These are just passed in so that the required info can be extracted into the returned values. See this for an example of how these might be made: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html\n\nThe results returned are used and sent to the ECDF-based statistical distance measure methods, which use them to calculate the statistical distance of the specific class\/ label.\n\n**DataFrame.loc[]** can be used to get the indexes\/ rows which match the query inside the square brackets. See: https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.loc.html","1d633d10":"#### This Scree plot shows the variance explained by each principal component within the un-normalised dataset.\n\nTherefore, using the un-normalised dataset for PCA, only around **5 to 7** principal components are needed to explain the entire (99.9...%) variation of the original dataset. This is a very large dimension reduction.\n\nThus, looking at the Environment Variables (at the top of the notebook), the 'dimensions_num_for_PCA' variable will be set to between **5 and 7** based upon this evidence, to maximise efficiency and also dataset accuracy (after PCA). If the non-normalised dataset is being used, of course.\n\n(Code reference: https:\/\/medium.com\/district-data-labs\/principal-component-analysis-with-python-4962cd026465)","8c0e09f6":"## Seaborn plotting\n\nSeaborn is \"a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\" via https:\/\/seaborn.pydata.org\/. \n\n**In the imports section, Seaborn is imported as 'sns'** which is the industry-norm.\n\nIt provides useful functions for plotting graphs that are more pleasing-to-the-eye than Matplotlib can be.\n\nBelow is a larger example **relplot** (lineplot) that is being used to visualise the results. See further below for full results.\n\n### What are the graphs\/ plots showing, and why?\n\nThe graphs are plotting a model's varying accuracy against an ECDF-based statistical distance measure. Each row from the 'concatenated' Pandas Dataframe results table represents 1 permutation. \n\nEach permutation uses a different dataset split to train, test, and predict classes\/ labels on. \n\nEach permutation can vary in accuracy, and also vary in its ECDF-based statistical distance, of a specific measure i.e. Anderson_Darling_dist.\n\nEssentially, these plots show that accuracy can be predicted based upon the ECDF-based statistical distance values calculated, based upon the data. This is a novel\/ new idea in the field of AI\/ ML safety, and is the basis of the SafeML idea.\n\nIn safety critical environments, predicting a model's accuracy based upon the incoming data- before the model can even use it to predict the next 'action' to take- is a great way to spot potential issues before they occur. Then, these potential errors- based upon a threshold- can be used to decide what steps to take next e.g.\n\n- In a human-in-the-loop system such as a self-driving car, if a large statistical distance value is detected (i.e. the input data is widely different to previous data fed to the model), then the AI system driving the car can pass control back to the driver before an accident occurs because the AI model driving the car could make a poor decision, since a large statistical distance value has been positively correlated to poorer model performance. This is the key idea of SafeML.","bea56123":"### Importing the dataset into Pandas.DataFrame and showing the top 5 entries via 'df.head()'\n\n- **pd.read_csv()** reads in the csv dataset, which is in the **.gitignore file** as this shouldn't be included in the repo. Thus, to run, please put the same dataset into the same folder as this jupyter notebook file (see docs: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.read_csv.html)\n\n\n- The csv data is converted into a **Pandas Dataframe** object (via the pd.read_csv method). Pandas is really useful for data manipulation, and acts as a standardised way to work with data- allowing easier use of in-built functions without worrying about datatype coversion and compatability (see docs: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.html).\n\n\n- The Pandas Dataframe consists of multiple **Pandas Series** objects, which are \"One-dimensional ndarray with axis labels (including time series)\". Thus, each Pandas Dataframe column\/ feature\/ attribute can be pulled out, and would then be a Pandas Series object (see docs: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.Series.html)\n\n\n- Finally, to note, **numpy** can also be used to manipulate the data where needed; if the functionality you're looking for is not in the Pandas library (e.g. specialised array-based manip functions). Numpy works nicely with Pandas (see docs: https:\/\/numpy.org\/)\n\n\n- **DataFrame.copy()** does a **deep copy** of the Dataframe, whereas **df_copy = df** would only do a **shallow copy**.\n\n\n- **df.head()** just shows the top (5 by default) entries in the imported dataset, which is now of type Pandas.Dataframe (see docs: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.head.html)","81fb4ce1":"### Fixing issues with ScikitLearn's PCA transform on this dataset\n\nWithout cleaning the dataset, the PCA transform will throw this error: \n- \"sklearn error ValueError: Input contains NaN, infinity or a value too large for dtype('float64')\". \n\nIt isn't obvious which attribute and\/ or data points are causing this, as the input dataset is supposed to be fully clean with no Nan or erroneous values. Also, there are too many attributes to manually search through to check this too. Thus, a quick solution via stackoverflow was found to work (see the 'clean_dataset(df)' method at the top of the notebook).","bd805b3d":"<a id=\"github_project\"><\/a>\n## Related GitHub Projects\n\n[SafeML Project](https:\/\/github.com\/ISorokos\/SafeML): The idea that has been briefly explained in this story.\n\n[NN-Dependability-KIT Project](https:\/\/github.com\/dependable-ai\/nn-dependability-kit): Toolbox for software dependability engineering of artificial neural networks.\n\n[Confident-NN Project](https:\/\/github.com\/cfinlay\/confident-nn): Toolbox for empirical confidence estimation in neural networks-based classification.\n\n[SafeAI Project](https:\/\/eth-sri.github.io\/research\/safeai): Different toolboxes like DiffAI, DL2 and ERAN from SRILab ETH Z\u00fcrich focusing on robust, safe and interpretable AI.","0b062dc7":"### Running the Statistical distance measure algorithms\nThis is just a preliminary run to show an example of how to use the methods.","65a1385b":"**get_concatenated_results_holding_all_classes_results()** method is used to get a Pandas Dataframe object, that has a new column added (called 'dataset') which can differentate rows\/ data points based upon the class. **It allows for dynamic returning of a results table, for the number of classes in the current dataset**.\n\nThe **result_dataframes** parameter holds a list of lists, where each list is the results for a specific class e.g. result_dataframes[0] would be the results table for class 0, which is a Pandas Dataframe. So essentially, this holds a list of Pandas Dataframes.\n\nA Python 3.6 concept called 'f-strings' is used to dynamically create the 'dataset' column's datapoint names, based upon the current class, using the for loop. See: https:\/\/www.journaldev.com\/23642\/python-concatenate-string-and-int\n\n**dataframe.append()** just appends something to the dataframes Python list.\n\n**result_dataframes[i].assign(dataset = class_name)** takes the Pandas Dataframe at the index i, and uses the .assign() method. See the .assign() method docs here: https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.assign.html. This essentially returns a new object with all the original columns, but in addition to new ones. The new column will have the name defined in the first index, i.e. **'dataset'**, and a row in this column is defined by the **class_name**.\n\n**pd.concat()** takes a list of Pandas Dataframes or Series objects, and concatinates\/ joins them together. See here: https:\/\/stackoverflow.com\/a\/51733133 for the reasoning behind why this is being done.","7ad06eda":"Printing the result dataframes. The .head(x) method just shows the top x rows in the dataframe.","36da7945":"**get_statistical_dist_measures_for_class_result()** method is for calculating the ECDF-based statistical distance measures for the entire dataset. Currently, the Python implementations of the statistical dist measures are **Univariate** meaning that they require each feature to be passed in, 1 at a time (**as of 8th April 2021**). Thus, an empty numpy array is set-up, with the size matching the number of features. Each index holds the specific statistical distance measure result, for the ith feature\/ column.\n\nThe accuracy parameter holds some model accuracy that is to be passed in, so the result dictionary can be correctly returned in the perfect format (for the results table).\n\nThe X_train_L and X_test_L parameters hold the X_train and X_test data (a Pandas Dataframe i.e. 2D array, like a results table) for the dataset used in this Jupyter Notebook. The '**_L**' stands for 'label' so essentially this holds the train and test data, for a specific label\/ class, based upon a query.\n\nThis method calculates all the Univariate results, and then calculates the mean\/ average of the entire numpy array. This essentially retrieves the correct distance\/ value for the ECDF-based statistical distance measure, as the mean is the correct value that should be returned. The mean represents the distance for the entire dataset, for that specific label\/ class.\n\nFinally, it returns a Python dictionary which is used to efficiently create a Pandas Dataframe results table, for plotting of the results. It's just a more efficient way of appending dynamic results to a Pandas Dataframe object. See the links and documentation for the code, where this method is called from, for more info.\n\nSee https:\/\/stackoverflow.com\/a\/17496530. Fast way to 'append' to dataframe for results table.","547272b1":"## Importing Libraries","567edf45":"### Data Preparation: PCA Dimension reduction and scaling (Hughes' Phenomenon)\n\nPCA acts to reduce the dimensions\/ search space of the dataset as much as possible, while trying to maintain the most information possible e.g. It can easily reduce the dimensionality by more than half, while still maintaining 99% of the original data's information- it does this by extracting out the most important information\/ trends\/ spread (variance) of each dimension\/ attribute- into n 'principal components'.\n\nMore formally: PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance.\n\n##### *Key note:* \n\"**PCA centers but does not scale the input data** for each feature before applying the SVD. The optional parameter whiten=True makes it possible to project the data onto the singular space while scaling each component to unit variance. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for **Support Vector Machines with the RBF kernel** and the **K-Means clustering algorithm**.\" (https:\/\/scikit-learn.org\/stable\/modules\/decomposition.html#pca)\n\nPCA still works without standardizing the features to unit scale **but tranforming to unit scale should still be done** to prevent large variance features from having an over-bearing affect on other lower variance features (via something like StandardScaler here https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html). \n\nThis is **particularly important with this dataset**, as some features have massively wide variances and others do not (e.g. the 'idle_std' values can range from e+06, all the way to zero).\n\n- **Dataframe['x']** syntax allows the selection of one of the Dataframe columns (where 'x' is the column heading\/ name).\n\n\n- **df_Series.unique()** returns an array, showing all the unique values inside the Pandas Series object.","7873b3c8":"**Clean_dataset()** method is used to remove infinite and Nan value errors (in the original dataset), which can cause issues in the PCA transform step.\n\n- **assert** keyword can be used for testing purposes. If the input param is not of type Pandas Dataframe, then the error message will be shown. Thus, you 'assert' that a pd.Dataframe is input.\n\n\n- **.dropna(inplace=True)** drops any rows which contain NaN\/ Null values. NaN values would cause issues later on, with model training and other functions. A lot of functions require NaN values to be removed to work (see docs: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.dropna.html).\n\nCode reference: https:\/\/stackoverflow.com\/a\/46581125 (with a minor change = removed the conversion to float64 type)","2719e2e6":"Boxplot to show the variance in Anderson_darling distance between each class in the dataset. Just an example of what could be plotted further.","4d4c6565":"Showing the transformation. **Again, to note, if label isn't binary then pd.get_dummies is required.**\n\nSee pandas get_dummies docs here for more information: https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.get_dummies.html","c8b80ed8":"## Methods\n\nDocumentation for each method lives a cell above the method code. Method code is described and explained like this, and **all methods live in this section**.","618ddcc5":"### Using StandardScaler to transform features into unit scale (optional for PCA)\n- **StandardScaler()** is an imported model from the sklearn.preprocessing library. It scales the specified Pandas Dataframe or Series object values to unit scale\/ variance. This is usually required for certain functions to perform correctly, e.g. the PCA transform later (see docs: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html)\n\n\n- StandardScaler().**fit_transform(df)** fits the StandardScaler model to the data, and transforms it into unit scale.\n\n\n- **pd.Dataframe(data=x, columns=y)** can convert the data 'x' into a Pandas Dataframe object, using the respective columns 'y'\n\nCode references: https:\/\/towardsdatascience.com\/pca-using-python-scikit-learn-e653f8989e60","a7c8e661":"After all permutations, convert each index (dict list) to a pandas dataframe. \n\nSee this resource behind the idea. It's highly efficient but was more complicated to implement: www.stackoverflow.com\/a\/17496530","a4620638":"Now, splitting the data into train and test data, using the optimal splitting techniques of K-Fold and Stratified Splitting.\n\nCode reference: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedKFold.html","fdb18fb6":"## Code for each permutation\nNow, running the code for each permutation requires Stratified splitting (KFold optional), model training and predicting classes via the chosen classifier, and finally the iteration over each class\/ label to calculate the statistical distance. Then, once complete, the results table can be used to plot the results from all the permutations.\n\n**Quick note on terminology:** The terminology 'class' and 'label' is used interchangeably throughout the industry, but both mean the same things. They represent the different possible answers the dataset holds and\/ or the model can output e.g. a dataset (for supervised learning i.e. given the answers to train with) with a person's personal data may have 2 classes\/ labels (answers). Either Male or Female. Male is 1 class\/ label, and Female is 1 class\/ label. The number of classes in the dataset would be 2. Also, a model would only be able to predict and output 2 different options\/ results.\n\nUsing Python 'List Comprehensions'. This way is Python sub-list ID safe, see link for more info: https:\/\/thispointer.com\/how-to-create-and-initialize-a-list-of-lists-in-python\/\n\nSee https:\/\/stackoverflow.com\/a\/17496530. Fast way to 'append' to dataframe for results table.\n\n**Most functionality has been extracted to functions\/ methods, which reside at the top of this Jupyter Notebook file. Documentation and explanations behind each step can be found in the Methods section at the top of this Jupyter Notebook. There, it describes what each method does, what the parameters are for, and why the returned values are to be returned** ","f55c5627":"## Starting the main loop and all other required loops (for permutations)\nThe permutation number is instantiated at the top of the notebook, in the environment variables section. This allows the number of permutations to be easily changed.\n\n**IMPORTANT NOTE:** A **for loop** in one Jupyter Notebook cell cannot be extended past a single cell. This is bad for documentation purposes because markdown cells cannot be inserted between cells as this would cause the **for loop** to become out of scope for other cells. Therefore, a highly modulated approach must be taken, where everything is put into a function\/ method. This allows markdown cells to exist above a section of code\/ method so it can be explained\/ documented. Code comments will be used when necessary too.","e7ffec21":"### Joining\/ concatinating the label feature back onto the pca transformed dataset. \n\nLabel still needs to be transformed into binary data (for model comprehension\/ understanding i.e. the model doesn't understand string data but string data can be transformed into numeric data, which is model can understand and use).\n\nSee the Pandas.concat docs here: https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.concat.html","70039667":"Initialising the StratifiedKFold model (https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedKFold.html)","656441c6":"### Plotting principle component variance\n\nA scree plot displays the variance explained by each principal component within the analysis, of the **Normalised** dataset.\n\n**The plot below shows that using the first 30 PCA components actually describes most\/ all (99.9%) of the variation (information) within the Normalised dataset. This is a huge dimension reduction from the initial 78 features, down to just 30.**\n\nThus, looking at the Environment Variables (at the top of the notebook), the 'dimensions_num_for_PCA' variable will be set to between **10 and 25** based upon this evidence, to maximise efficiency and also dataset accuracy (after PCA).\n\n(Code reference: https:\/\/medium.com\/district-data-labs\/principal-component-analysis-with-python-4962cd026465)","4d5b8ff6":"### Plotting the test\/ example results\nThis shows the varying distances of each statistical distance measure, between **each feature only**, in a single dataset split (1 permutation). Therefore, it is only an example and not representative of the real run (like a CRISP-DM cycle run through, where one can evaluate and re-run after).\n\nThe real run will plot the distances between all the features (entire dataset) of each dataset split (Permutation) against the average accuracy from all permutations. Thus, showing how statistical distances can be used to estimate the accuracy of models (the SafeML idea).","59232a3d":"Some rows have been removed by the cleaning, indicating that some rows did have issues\/ errors within them.","483764f0":"### Fixing column name issues\n\nBecause of Excel being used to create the csv, the column headings\/ names contain whitespace padding, incorrect capitalisation, etc... which makes it difficult to correctly select by column names. This piece of code below just removes these issues.\n\n- **df_columns** can be used to access all the column heading names (see docs: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.columns.html)\n\n\n- The code below acts on each of the header names (for ease of use): \n    - **str.strip()** just strips\/ removes any leading and trailing spaces\n    - **str.lower()** just converts all characters to lower case\n    - **str.replace('x', 'y')** just replaces all instances of x with y, i.e. changing spaces to '_'\n    - **str.replace('x', '')** can be used to remove the specified x characters (replacing with nothing\/ empty char)\n\nCode Reference: https:\/\/medium.com\/@chaimgluck1\/working-with-pandas-fixing-messy-column-names-42a54a6659cd","2ee878ab":"### Considerations before PCA can be used correctly (before Data Preparation feature selection via PCA)\nLooking at this resource and many others (https:\/\/towardsdatascience.com\/pca-is-not-feature-selection-3344fb764ae6), it can be seen that PCA can, quite easily, be used incorrectly without proper consideration and\/ or understanding.\n\nFrom the resource:\n- \"A common mistake new data scientists make is to apply PCA to non-continuous variables. While it is technically possible to use PCA on discrete variables, or categorical variables that have been one hot encoded variables, you should not. Simply put, if your variables don\u2019t belong on a coordinate plane, then do not apply PCA to them\"\n\nThus, PCA should **only** be applied to the numeric features- which should be scaled down to unit scale.\n\n### What features should be included from PCA, and why?\n\nLooking at the list of feature names in the dataset (shown below), one can see that all other features should be of numeric type (with domain knowledge). They're all currently numeric type (either float or int). Consequently, PCA **can be** fully applied.\n\n- **df.columns.tolist()** converts the Dataframe column names into a Python list","9847f483":"### Important note on these above scree plot results ^\n\n**The results of these graphs can change\/ vary between different datasets used. Thus, these need to be inspected each time a new dataset (.csv file) is to be run on this code. Then, using the information the plots show, one should alter the environment variables, and thus the Neural Network\/ MLP model's hyperparamters (tuning the Neural Network hyperparameters relies on knowing and using the number of features in a dataset).**","aa8bce68":"There's only 2 classes, so only 2 result dataframes are shown. Each row represents a permutation. Each dataset represent a single class.","33bff4a4":"# Modeling stage\nData is now fully transformed and ready for ML model training and predictions.\n\nSee CRISP-DM for more information. Following industry-standard Data Science\/ ML practises.\n\nSee: https:\/\/www.datascience-pm.com\/crisp-dm-2\/","f62aafde":"### Now fitting and transforming the data with PCA\n\nThus, the optimal number of principle components is set to the environment variable and this is now used to produce the appropriate multi-dimensional principle component array. This will be formatted back to a Pandas dataframe afterwards.\n\nReferences: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html and https:\/\/towardsdatascience.com\/pca-using-python-scikit-learn-e653f8989e60","53793bc7":"#### The data is now fully cleaned and transformed, ready for pre-modeling test_train data splitting\n\n## K-Fold Cross Validation and Stratified splitting\nK-Fold is a technique which splits data into K folds (splits). Train of a model K times, and for each training iteration, K-Fold selects a different fold to use for testing; the remaining K - 1 folds become the training data. Typically, the optimal K value can be derived using the size of your dataset (num of rows). Ideally, each fold should be statistically representative of the population. Too small and it won't be useful. Too large, and you lose the positives from doing K-Fold.\n\nYou can use Stratified splitting with K-Fold, which ensures balance between some criteria (balances out the classes) e.g. equal portion of label classes in each fold.\n\nClass Imbalance is a significant issue in the ML\/ Data Mining domain. It leads to incorrect results e.g. if one fold had all of 1 label (accidentally), then it would produce terrible predictive results as it wouldn't know what the other label class data point would look like. You can only work with the data you have, so this has to be dealt with.\n\nBenefits of K-Fold:\n- Use more of the data towards making a succesful model.\n- Obtain K models to evaluate, can improve the confidence that you have selected an appropriate model algorithm and cleaned\/ prepared the data correctly, e.g. normal split with 1 model, one doesn't know if it's good or not- it could be heavily biased. Multiple models ensures less bias and increased variance.\n- Looking at the accuracy results from each of the k-Folds, you can identify data issues e.g. a certain fold performs really badly. Could this suggest that more cleaning is required? Maybe the data preparation was performed incorrectly?\n- If all folds return similar accuracies, one can be more confident that a deployed model will perform similarly to how one expects.\n\nIssues with K-Fold:\n- Creating K separate models requires more computation.\n- If you haven't got much data, you might not get many folds. Less folds means K-Fold loses its benefits.\n- If K is very large, each fold is small, and harder to ensure statistical distribution of.\n- Choosing the best of K models introduces bias. Real world data could perform better under a more general, lower performing model.\n\nCode reference: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedKFold.html","840203be":"## Code set-up: Imports, Packages, Environment variables, and Methods\n\nThe software versions used are:\n- The Python3 version used for this work is: Python 3.8.x\n- The scikit-learn version used is: scikit-learn 0.24.0\n- The seaborn version used is: 0.11.1\n- The Pandas version used is: 1.1.5 (although 1.2.0 was released recently, this should also work)\n\nBefore running, please run these commands via pip, in the terminal:\n- pip install pandas\n- pip install scikit-learn\n- pip install scikit-plot\n- pip install seaborn","57c8e350":"<a id=\"Safeml_idea\"><\/a>\n## 1. SafeML Idea\n\nSafeML idea has been proposed by (Aslansefat et al., 2020-b), and the goal was to somehow monitor the decisions of the classifiers when there is no available label.\n\nThe following figure demonstrates the flowchart of the SafeML idea. In this flowchart, there are two main sections including training phase and application phase.\n\n![Flowchart of SafeML (Aslansefat et al. 2020-b)](https:\/\/miro.medium.com\/max\/1832\/1*iIpJOHOL1GWrp234jfYW0g.png)\n\nA) The training phase is an offline procedure in which a trusted or certified dataset will be used to train the intelligent algorithm that can be a machine learning or deep learning algorithm. Thus, using a trusted dataset the classifier will be trained and its performance will be measured with existing KPIs (e.g. ROC, Accuracy and Kappa). Meanwhile, the statistical parameters and distributions of each class will be estimated and stored to be used for comparison (e.g. Mean values, Variance Values, and the Empirical Cumulative Distribution Functions (ECDFs)).\n\nB) The application phase is an online procedure in which real-time and unlabelled data is going to be feed to the system. For example, consider a security attack detector that has been trained to detect different security attacks and it should filter the attacker IPs. Therefore, in the application phase, the trained classifier should distinguish between the normal network traffic and the security attacks (classification task). One important and critical issue in the application phase is that the data does not have any label. So, it cannot be assured that the classifier can operate as accurate as of the training phase.\n\nIn the application phase, the buffered data will be separated based on classifier decision (that is not certified) and the statistical parameters of each class will be stored to be compared with the one in trained phased. Using the statistical distance measures that will be explained in the next section, the distance between features for each class in the trained phase and application phase will be compared. If the calculated distance and expected confidence (defined by an expert) difference was very low, the classifier results and its accuracy can be trusted (the system is acting autonomously), if the difference was low, the system can ask for more data and re-evaluation to make sure about the distance. In case of larger difference, the classifier results and accuracy are no longer valid, and the system should use an alternative approach or notify a human agent (In this example, the system will ask the responsible security agent to check the network traffic manually and make a decision).\n\n![Application of SafeML in security intrusion detection (e.g. datasets like CICIDS2017)](https:\/\/miro.medium.com\/max\/3642\/1*qBLu6THDIyIJueq5BBKjng.png)\n","786ccbd9":"# Starting the real run, using the PCA-transformed dataset\nThe dataset shown below is the PCA-transformed data, with the label\/ class feature attached (which has been LabelBinarized, into numbers from class names- so the model can understand).\n\nTo simulate the SafeML idea, a number of permutations have to be run. Each permutation will have its data split differently, and this results in varying model accuracies (different data => different accuracies). The hope (for the simulation) is that as these accuracies vary between permutations, the statistical difference values vary along with it- in a highly (positive) correlated way. \n\nThis would show that ECDF-based statistical distance measures can actually be used to predict model performance. The final graphing\/ plotting will show these relationships, so the correlation can be easily identified. A case-study for this dataset has been run in MATLAB, but not Python. \n\nThe aim of this file is to have a Python code case-study for the same dataset, and verify the results to see if they match\/ are similar to the MATLAB implementation. \n\nIf it matches, then this case-study can be confidently merged into the master branch, of the SafeML repository.","bcebba12":"# Code starting point","d1141a93":"The label column has to be removed as you don't want this involved in the PCA process. It can be concatted back with the PCA tranformed dataframe.","0499844d":"**get_shuffled_stratifiedKFold_train_test_split()** method is for retrieving shuffled, StratifiedKFold train test split data. It's similar to the 'train_model_predict()' method above- see this for details behind each step\/ line of code. This is industry-standard for dealing with datasets which have a large class imbalance, such as this one.\n\nThe method takes in X (dataset without the labels\/ classes) and y (the label\/ class feature). It instantiates a new StratifiedKFold 'model' which comes from the 'sklearn.model_selection' library. The **n_splits** parameter denotes how many folds (K) are to be run. The **Shuffle** parameter denotes that each split will be shuffled and therefore randomized to get varying shuffles between runs. This is ideal, as each permutation to be run should be as varied as possible to give good variance in plotting- so the SafeML idea can be showcased & plotted optimally.\n\nThe for loop uses the StratifiedKFold 'model' to extract indices that have been found by the skf model. The label\/ class feature needs to be reshaped in SkLearn, as un-shaped label data causes exceptions\/ errors when attempting to train, predict, or use any SkLearn ML classifiers\/ models.\n\nFinally, all the extracted data is returned.\n\nSee docs here: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedKFold.html","1804c1e8":"## Useful environment variables","39a226ab":"### K-Nearest neighbor ML classifier\nThe KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are assumed to be near to each other.\n\nThe most important factor in training a KNN model is the **number of neighbors hyperparameter**. You want to choose the K  value that reduces the number of errors, while maintaining the algorithm\u2019s ability to accurately make predictions when it\u2019s given data it hasn\u2019t seen before. Here are some important considerations:\n\n- As you decrease the value of K to **1**, predictions become less stable e.g. imagine K=1 and you have a query point surrounded by several red 'dots' and one green 'dot', but the green dot is the single nearest neighbor. Reasonably, you would think the query point is most likely red, but because K=1, KNN incorrectly predicts that the query point is green.\n\n- Inversely, as you increase the value of K, predictions become more stable due to majority voting\/ averaging, and thus, more likely to make more accurate predictions (up to a certain critical point- an **'overfitting' threshold**). Eventually, you would begin to witness an increasing number of errors. It is at this point you'd know that you have pushed the value of K too far.\n\n- In cases where you are taking a majority vote (e.g. picking the mode in a classification problem) amongst labels\/ classes, you usually make K an odd number to have a tiebreaker.\n\nThe sklearn's default k value is 5 (also true for MATLAB's implementation).\n\nReferences: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html & https:\/\/towardsdatascience.com\/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761","4aa95ec4":"Neural Networks, and specifically the sklearn Multi-Layer Perceptron (MLPClassifier), aim to model the brain's neurons and its inner workings. \n\nA set of input values and weights are evaluated, and passed into the first layer of the neural network. The neural network can have multiple layers, with a varied amount of nodes (via hyperparameters) in each layer. \n\nHyperparameters are the parameters of the model that- if changed- affect the learning rate and prediction accuracy.\n\nIf an input weight, X, along with a learning rate, Y, evaluates to a value above a given **threshold** (like a neuron's Potassium & Sodium based Action Potential threshold), then just like a real neuron, the neuron will fire and send information to the next node in the layer, or possibly the output node. \n\nThe output node will take all of the processed information, from all the nodes in each interconnected (fully-connected MLP) layer, and a classification result will be given.\n\nThe inner workings of large Neural Nets are still un-explainable (as of March 2021), which is why they're referred to as a **Black-box classifier\/ model**.\n\nThis is how a Neural Network learns and predicts a label\/ class based on input data. The methods by which learning occurs (Backpropagation, Momentum, Learning rate input, Weight adjustments, etc...) is beyond the scope of this solution.\n\nFrom docs: \"Multi-layer Perceptron classifier. This model optimizes the log-loss function using LBFGS or stochastic gradient descent\".\n\nImportant note from the default MLP params, from the docs: \n- \"Note: The default solver \u2018adam\u2019 works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score. For small datasets, however, \u2018lbfgs\u2019 can converge faster and perform better\". \n\n###### So the 'lbfgs solver' hyperparameter may not be ideal for this larger dataset. Thus, the 'Adam solver' will be used. \n\n(Code references: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPClassifier.html)","dde801b7":"Pre-finished code using just the first label in the labels array. Selects all the relevant rows\/ data points based upon the query placed inside the square brackets.","af482347":"# Python Notebook for the Security Case-study run on the CICIDS Security Dataset\n\n#### by William Bridges (Part of BSc Dissertation Work)\n#### Edited by Koorosh Aslansefat\n#### Supervisor: Prof. Yiannis Papadopoulos\n\n\nIt is a story about the accuracy estimation of Machine Learning\/Deep Learning classifiers based on statistical distance measures (SafeML, and SafeDL).\n\n\n![SafeML logo from: https:\/\/github.com\/ISorokos\/SafeML](https:\/\/miro.medium.com\/max\/700\/1*H0lN2Q9lmSRgfaGj9VqqGA.png)","1a34a61c":"### Neural Networks (MLP), quick explanation\/ clarification (for below method's code explanation)","74d32b01":"### Looking at the original data types\nSelf-explanatory. Just shows the data types of each feature\/ column.","b49c35a5":"## Plotting the results\n\nFollowing: https:\/\/stackoverflow.com\/a\/51733133 to show both datasets in the same Seaborn graph. Seaborn doesn't allow multiple plots to be layered over each other, like in MatPlotLib, so you have to concatenate the results and use 'hue' on the new column, which represents each dataset.\n\nFirst, one has to get all the data into 1 Dataframe. The number of classes in the current dataset is passed to the method below, which **dynamically** returns the concatenated Dataframe, holding the row information for all the classes, separated by a newly added column called **'dataset'**, which one can see below. Each class can be separated in this way.\n\nThis is really important, as the **Seaborn graphing\/ plotting only works when 1 Dataframe is used**. It can't graph using multiple dataframes passed in, unlike MatPlotLib may be able to.\n\nNow, in Seaborn graphing\/ plotting, the **'hue'** parameter can be placed upon the **'dataset'**, which allows the plots to differentiate between classes visually, by using a different colour or pattern for each class. If a dataset holds more than 7 or 8 different classes, this may cause the plotting to look too cluttered and separate plotting may be required. **For this dataset, and others in the Network Intrusion CICIDS2017_Security_dataset, it works well.**","0c1ad971":"### Training the model and predicting test data results (confusion matrix)\nThe selected Machine Learning classifier\/ model\/ models can now be trained on training data (from the StratifiedKFold splitting). Once the model is trained, it can be used to predict the test data's labels (based upon what it has seen before).\n\nThe performance of the model can be seen below in the Classification Report, Confusion Matrix, and the model's predicitive accuracy result.\n\n(see **methods** section, at the top of the notebook, for the train_model_predict() method code. Note that it can take a few minutes to run due to the vast amount of data used, and the training time required for the model to learn).","6a702f6f":"Instead of a 3D array (like the MATLAB implementation), this uses just 1 classifier at a time and uses Pandas DataFrames (2D array essentially) for easy plotting later, as one can use the Seaborn plotting library (https:\/\/seaborn.pydata.org\/). Each Pandas Dataframe row is a permutation and each column has: the model's accuracy in 1st column, then each subsequent statistical distance measure to be used in the later columns.\n\nThen, one can graph a line graph\/ Seaborn 'relplot' (https:\/\/seaborn.pydata.org\/api.html) of the accuracy on the x-axis vs the specific statistical distance measure on the y-axis. Therefore resulting in 6 plots\/ graphs, 1 for each ECDF statistical distance measure.\n\nEach class will represent a line on the graph (differentiated via hue colouring). Classes\/ labels could also be separated by having separate plots for each class\/ label. The hue approach is what has been used.\n\nNOTE: Could also store F1-Score, Sensitivity score, Precision score, etc... to see how these relate to the ECDF statistical distance measures too.","f9ec53a3":"## The SafeML statistical distance measures: test\/ example run\nNow the ML models have been run, and the accuracy is found (along with other useful performance metrics), a test run of using the SafeML statistical distance measures can be run. This is a test run, and the real run will be done later. This test run is a guided 'tour' around how to set-up the code, in Python and SciKit-Learn, so that results can be re-produced easily for future case-studies or projects.","6abc15eb":"Resetting indexes since rows have been dropped. See the difference between above dataframe indexes and below.\n\n- **.reset_index()** method resets the Pandas Dataframe indexes, for the rows. Useful to do after removing rows, as this messes up the indexes. It creates a new 'index' column that needs to be dropped, as it's useless (see docs: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.reset_index.html)\n\n\n- **.drop([column_name], axis=x, inplace=y)** drops a specified column from the dataframe and returns a new copy. When **inplace=True**, it transforms the Dataframe itself (instead of needing to copy). The **axis** specifies the dimension of what to drop i.e. if **axis=0, it drops a row**. If **axis=1, it drops a column**.","c05aea8b":"<a id=\"references\"><\/a>\n## References\n\nAmodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., & Man\u00e9, D. (2016). Concrete problems in AI safety. arXiv preprint arXiv:1606.06565.\n\nAslansefat, K., Gogani, M. B., Kabir, S., Shoorehdeli, M. A., & Yari, M. (2020-a). Performance evaluation and design for variable threshold alarm systems through semi-Markov process. ISA transactions, 97, 282\u2013295. https:\/\/doi.org\/10.1016\/j.isatra.2019.08.015\n\nAslansefat, K., Sorokos, I., Whiting, D., Kolagari, R. T., & Papadopoulos, Y. (2020-b). SafeML: Safety Monitoring of Machine Learning Classifiers through Statistical Difference Measure. arXiv preprint arXiv:2005.13166.\n\nBalunovic, M., Baader, M., Singh, G., Gehr, T., & Vechev, M. (2019). Certifying Geometric Robustness of Neural Networks. In Advances in Neural Information Processing Systems (pp. 15287\u201315297).\n\nGehr, T., Mirman, M., Drachsler-Cohen, D., Tsankov, P., Chaudhuri, S., & Vechev, M. (2018, May). Ai2: Safety and robustness certification of neural networks with abstract interpretation. In IEEE Symposium on Security and Privacy (SP) (pp. 3\u201318). https:\/\/doi.org\/10.1109\/SP.2018.00058\n\nGulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., & Courville, A. C. (2017). Improved training of Wasserstein gans. In Advances in neural information processing systems (pp. 5767\u20135777).\n\nGheraibia, Y., Kabir, S., Aslansefat, K., Sorokos, I., & Papadopoulos, Y. (2019). Safety+ AI: A Novel Approach to Update Safety Models Using Artificial Intelligence. IEEE Access, 7, 135855\u2013135869. https:\/\/doi.org\/10.1109\/ACCESS.2019.2941566\n\nKabir, S., Sorokos, I., Aslansefat, K., Papadopoulos, Y., Gheraibia, Y., Reich, J., \u2026 & Wei, R. (2019, October). A Runtime Safety Analysis Concept for Open Adaptive Systems. In International Symposium on Model-Based Safety and Assessment (pp. 332\u2013346). Springer, Cham. https:\/\/doi.org\/10.1007\/978-3-030-32872-6_22\n\nScudellari, S. (2020) Hospitals Deploy AI Tools to Detect COVID-19 on Chest Scans, IEEE Spectrum.\n\nSchulam, P., & Saria, S. (2019). Can you trust this prediction? Auditing pointwise reliability after learning. arXiv preprint arXiv:1901.00403.\n\nShen, J., Qu, Y., Zhang, W., & Yu, Y. (2018, April). Wasserstein distance guided representation learning for domain adaptation. In Thirty-Second AAAI Conference on Artificial Intelligence.\n\n","ca7a9f65":"**May add feature-importance plotting here, for future SafeML Python package to be made...**"}}