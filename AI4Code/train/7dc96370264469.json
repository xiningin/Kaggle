{"cell_type":{"aa0079b9":"code","bc6b5176":"code","76ca9b47":"code","6245104f":"code","866534f3":"code","b56683e0":"code","e6b086e7":"code","48f16f8f":"code","65a3edfa":"code","d22119b8":"code","6e8d5494":"code","0e130777":"code","f213c127":"code","f00e1be2":"code","02525d92":"code","6a5ff866":"code","dd2cd95e":"code","7a0198aa":"code","8d1d43c7":"code","7c02f54c":"code","00febc07":"code","bfeda93e":"code","016a34d7":"code","f7951739":"code","614b818a":"code","2a773d25":"code","98e96214":"code","3fa2d922":"code","e4ca2cd8":"markdown","f8f18d04":"markdown","6da1f7cb":"markdown","bac3848b":"markdown","b479e961":"markdown","c0163b84":"markdown","23bad81d":"markdown","f22dc2b7":"markdown","6779012d":"markdown","27f2b168":"markdown","923c191b":"markdown","1e041317":"markdown","dd2debcd":"markdown","d34d4300":"markdown","449bc98a":"markdown","0a5f8649":"markdown","db3b9629":"markdown","0ba9c6c3":"markdown","f69c6ca4":"markdown","f5296ece":"markdown","ab2a9d19":"markdown","3b19f345":"markdown","1b3fc783":"markdown","889cad11":"markdown","07e8f3ba":"markdown","58d11d56":"markdown"},"source":{"aa0079b9":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression, Ridge,Lasso,LassoCV\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,cross_val_predict,TimeSeriesSplit\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import mean_squared_error","bc6b5176":"nomeArquivo = '..\/input\/beer-consumption-sao-paulo\/Consumo_cerveja.csv'\ndataset = pd.read_csv(nomeArquivo,parse_dates=True)","76ca9b47":"pd.set_option('display.max_columns', None)\nprint(dataset.head())\nprint(dataset.columns)","6245104f":"print(dataset.info())","866534f3":"print(dataset.tail())","b56683e0":"indicesNulos = dataset['Consumo de cerveja (litros)'].isna()\nlistaNaN = dataset[indicesNulos].index.tolist()\nlistaNum = dataset[~indicesNulos].index.tolist()\n\nplt.figure(1)\nplt.scatter(listaNaN,np.random.random(len(listaNaN))*2,marker='X',c='orange',alpha=0.2)\nplt.scatter(listaNum,np.random.random(len(listaNum))*2,marker='X',c='b',alpha=0.2)\nplt.xlabel('\u00cdndice no dataset')\nplt.legend(['NaN','Num'],loc='upper right')\nplt.tick_params(axis='y',which='both',left=False,bottom=False,labelleft=False)\nplt.title('Identifica\u00e7\u00e3o por cor de dados nulos e n\u00e3o nulos')\n\nplt.figure(2)\nfig,ax = plt.subplots(1,1)\nplt.scatter(dataset.iloc[listaNum,0],np.random.random(len(listaNum))*2,marker='X',c='b',alpha=0.3)\nprimeiroRegistroNum = dataset.iloc[listaNum[0],0]\nultimoRegistroNum = dataset.iloc[listaNum[-1],0]\nplt.plot([primeiroRegistroNum,primeiroRegistroNum],[0,2],c='black',linestyle='--')\nplt.plot([ultimoRegistroNum,ultimoRegistroNum],[0,2],c='black',linestyle='--')\nplt.xlabel('Data')\nplt.title('Identifica\u00e7\u00e3o por cor de dados nulos e n\u00e3o nulos')\nplt.tick_params(axis='y',which='both',left=False,bottom=False,labelleft=False)\nax.xaxis.set_major_locator(MaxNLocator(8)) \nplt.tick_params(labelrotation=60)\nax.annotate('primeiro: {}'.format(dataset.iloc[listaNum[0],0]),xy=(0.05, 0.9), xycoords='axes fraction',fontsize=15)\nax.annotate('\u00faltimo: {}'.format(dataset.iloc[listaNum[-1],0]),xy=(0.50, 0.1), xycoords='axes fraction',fontsize=15)\n","e6b086e7":"dsPreprocessado = dataset\ndsPreprocessado = dataset.dropna()","48f16f8f":"dsPreprocessador = dsPreprocessado.replace(',','.',inplace=True,regex=True)\n","65a3edfa":"dsPreprocessado = dsPreprocessado.drop(columns='Data')\n","d22119b8":"dsPreprocessado.iloc[:,0:4] = dsPreprocessado.iloc[:,0:4].apply(pd.to_numeric)\ndsPreprocessado.iloc[:,4] = dsPreprocessado.iloc[:,4].astype(int)# apply(pd.to_numeric(downcast='int'))\nprint(dsPreprocessado.info())","6e8d5494":"print(dsPreprocessado.describe())","0e130777":"plt.figure(1)\nsns.pairplot(dsPreprocessado,x_vars=['Temperatura Minima (C)','Temperatura Media (C)','Temperatura Maxima (C)','Precipitacao (mm)'],y_vars=['Consumo de cerveja (litros)'],hue='Final de Semana',diag_kind=None)\n","f213c127":"plt.figure(2)\nsns.swarmplot(x='Final de Semana',y='Consumo de cerveja (litros)',data=dsPreprocessado)\nplt.grid()\nplt.xlabel('Final de semana')\nplt.ylabel('Consumo de cerveja [L]')\n\n","f00e1be2":"correlation = dsPreprocessado.corr()\n\nf, ax = plt.subplots(figsize=(12, 10))\n\n# Generate a mask for upper traingle\n\nmask = np.triu(np.ones_like(correlation,dtype=bool))\n\n# Configure a custom diverging colormap\n\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n\nsns.heatmap(correlation,cmap=cmap,mask=mask,annot=True)\n","02525d92":"reg = LinearRegression()\ny = dsPreprocessado['Consumo de cerveja (litros)'].values #target\nx = dsPreprocessado.drop(columns='Consumo de cerveja (litros)').values #fetures\nxColunas = dsPreprocessado.drop(columns='Consumo de cerveja (litros)').columns","6a5ff866":"xTrain,xTest,yTrain,yTest = train_test_split(x,y,test_size=0.3,random_state=42)","dd2cd95e":"reg.fit(xTrain,yTrain)\nyPred = reg.predict(xTest)\nres = yPred - yTest\navgRes = round(np.mean(res),3)\nstdRes = round(np.std(res),3)\n\n\nplt.figure(1)\nplt.scatter(yPred,res)\nplt.plot([min(yPred),max(yPred)],[avgRes,avgRes],linestyle='--',c='red')\nplt.xlabel('Valor previsto')\nplt.ylabel('Erro residual')\nplt.text(17.5,4,'m\u00e9dia = {} \\nstd = {}'.format(avgRes,stdRes))","7a0198aa":"#fun\u00e7\u00e3o para calcular R2 ajustado\ndef calculaR2ajustado(r2Score,x):\n  n = np.shape(x)[0]\n  k = np.shape(x)[1]\n  return 1 - (1-r2Score)*(n-1)\/(n-k-1)","8d1d43c7":"print('Valor de R2: {}'.format(reg.score(xTest,yTest)))\nprint('Valor MSE: {}' .format(mean_squared_error(yTest,yPred)))\nprint('Valor de R2 ajustado: {}'.format(calculaR2ajustado(reg.score(xTest,yTest),xTest)))\nprint('Coeficientes da regress\u00e3o: {}'.format(reg.coef_))\nprint('Intercept da regress\u00e3o: {} \\n'.format(reg.intercept_))\n","7c02f54c":"vif = [variance_inflation_factor(xTrain, i) for i in range(xTrain.shape[1])]\nfor v, c in zip(vif,xColunas):\n  print('VIF de {} : {}'.format(c,v))","00febc07":"dsTMaxMin  = dsPreprocessado.drop(columns='Temperatura Media (C)')\ndsTMedia = dsPreprocessado.drop(columns=['Temperatura Maxima (C)','Temperatura Minima (C)'])\ndsTMax = dsPreprocessado.drop(columns=['Temperatura Media (C)','Temperatura Minima (C)'])\ndsTMin = dsPreprocessado.drop(columns=['Temperatura Media (C)','Temperatura Maxima (C)'])\ndsTMediaMax = dsPreprocessado.drop(columns=['Temperatura Minima (C)'])\ndsTMediaMin = dsPreprocessado.drop(columns=['Temperatura Maxima (C)'])\n\nyTMaxMin = dsTMaxMin['Consumo de cerveja (litros)'].values\nxTMaxMin = dsTMaxMin.drop(columns='Consumo de cerveja (litros)').values\n\nyTMedia = dsTMedia['Consumo de cerveja (litros)'].values\nxTMedia = dsTMedia.drop(columns='Consumo de cerveja (litros)').values\n\nyTMediaMax = dsTMediaMax['Consumo de cerveja (litros)'].values\nxTMediaMax = dsTMediaMax.drop(columns='Consumo de cerveja (litros)').values\n\nyTMediaMin = dsTMediaMin['Consumo de cerveja (litros)'].values\nxTMediaMin = dsTMediaMin.drop(columns='Consumo de cerveja (litros)').values\n\nyTMedia = dsTMedia['Consumo de cerveja (litros)'].values\nxTMedia = dsTMedia.drop(columns='Consumo de cerveja (litros)').values\n\nyTMax = dsTMax['Consumo de cerveja (litros)'].values\nxTMax = dsTMax.drop(columns='Consumo de cerveja (litros)').values\n\nyTMin = dsTMin['Consumo de cerveja (litros)'].values\nxTMin = dsTMin.drop(columns='Consumo de cerveja (litros)').values","bfeda93e":"xTMaxMinTrain,xTMaxMinTest,yTMaxMinTrain,yTMaxMinTest = train_test_split(xTMaxMin,yTMaxMin,test_size=0.4,random_state=42)\nregTMaxMin = LinearRegression()\nregTMaxMin.fit(xTMaxMinTrain,yTMaxMinTrain)\nyTMaxMinPred = regTMaxMin.predict(xTMaxMinTest)\nresTMaxMin = yTMaxMinPred - yTMaxMinTest\navgResTMaxMin = round(np.mean(resTMaxMin),3)\nstdResTMaxMin = round(np.std(resTMaxMin),3)\nvifTMaxMin = [variance_inflation_factor(xTMaxMinTrain, i) for i in range(xTMaxMinTrain.shape[1])]\n\nprint('--- TMaxMin ---')\nprint('Valor de R2: {}'.format(regTMaxMin.score(xTMaxMinTest,yTMaxMinTest)))\nprint('Valor de R2 ajustado: {}'.format(calculaR2ajustado(regTMaxMin.score(xTMaxMinTest,yTMaxMinTest),xTMaxMinTest)))\nprint('Valor MSE: {}' .format(mean_squared_error(yTMaxMinTest,yTMaxMinPred)))\nprint('Coeficientes da regress\u00e3o: {}'.format(regTMaxMin.coef_))\nprint('Intercept da regress\u00e3o: {}'.format(regTMaxMin.intercept_))\nprint('VIF : {}\\n'.format(vifTMaxMin))\nprint('* * * * * *\\n')\n\n\nxTMediaTrain,xTMediaTest,yTMediaTrain,yTMediaTest = train_test_split(xTMedia,yTMedia,test_size=0.4,random_state=42)\nregTMedia = LinearRegression()\nregTMedia.fit(xTMediaTrain,yTMediaTrain)\nyTMediaPred = regTMedia.predict(xTMediaTest)\nresTMedia = yTMediaPred - yTMediaTest\navgResTMedia = round(np.mean(resTMedia),3)\nstdResTMedia = round(np.std(resTMedia),3)\nvifTMedia = [variance_inflation_factor(xTMediaTrain, i) for i in range(xTMediaTrain.shape[1])]\n\n\n\nprint('--- TMedia ---')\nprint('Valor de R2: {}'.format(regTMedia.score(xTMediaTest,yTMediaTest)))\nprint('Valor de R2 ajustado: {}'.format(calculaR2ajustado(regTMedia.score(xTMediaTest,yTMediaTest),xTMediaTest)))\nprint('Valor MSE: {}' .format(mean_squared_error(yTMediaTest,yTMediaPred)))\nprint('Coeficientes da regress\u00e3o: {}'.format(regTMedia.coef_))\nprint('Intercept da regress\u00e3o: {}'.format(regTMedia.intercept_))\nprint('VIF : {}\\n'.format(vifTMedia))\nprint('* * * * * *\\n')\n\nxTMediaMaxTrain,xTMediaMaxTest,yTMediaMaxTrain,yTMediaMaxTest = train_test_split(xTMediaMax,yTMediaMax,test_size=0.4,random_state=42)\nregTMediaMax = LinearRegression()\nregTMediaMax.fit(xTMediaMaxTrain,yTMediaMaxTrain)\nyTMediaMaxPred = regTMediaMax.predict(xTMediaMaxTest)\nresTMediaMax = yTMediaMaxPred - yTMediaMaxTest\navgResTMediaMax = round(np.mean(resTMediaMax),3)\nstdResTMediaMax = round(np.std(resTMediaMax),3)\nvifTMediaMax = [variance_inflation_factor(xTMediaMaxTrain, i) for i in range(xTMediaMaxTrain.shape[1])]\n\n\n\nprint('--- TMediaMax ---')\nprint('Valor de R2: {}'.format(regTMediaMax.score(xTMediaMaxTest,yTMediaMaxTest)))\nprint('Valor de R2 ajustado: {}'.format(calculaR2ajustado(regTMediaMax.score(xTMediaMaxTest,yTMediaMaxTest),xTMediaMaxTest)))\nprint('Valor MSE: {}' .format(mean_squared_error(yTMediaMaxTest,yTMediaMaxPred)))\nprint('Coeficientes da regress\u00e3o: {}'.format(regTMediaMax.coef_))\nprint('Intercept da regress\u00e3o: {}'.format(regTMediaMax.intercept_))\nprint('VIF : {}\\n'.format(vifTMediaMax))\nprint('* * * * * *\\n')\n\nxTMediaMinTrain,xTMediaMinTest,yTMediaMinTrain,yTMediaMinTest = train_test_split(xTMediaMin,yTMediaMin,test_size=0.4,random_state=42)\nregTMediaMin = LinearRegression()\nregTMediaMin.fit(xTMediaMinTrain,yTMediaMinTrain)\nyTMediaMinPred = regTMediaMin.predict(xTMediaMinTest)\nresTMediaMin = yTMediaMinPred - yTMediaMinTest\navgResTMediaMin = round(np.mean(resTMediaMin),3)\nstdResTMediaMin = round(np.std(resTMediaMin),3)\nvifTMediaMin = [variance_inflation_factor(xTMediaMinTrain, i) for i in range(xTMediaMinTrain.shape[1])]\n\n\n\nprint('--- TMediaMin ---')\nprint('Valor de R2: {}'.format(regTMediaMin.score(xTMediaMinTest,yTMediaMinTest)))\nprint('Valor de R2 ajustado: {}'.format(calculaR2ajustado(regTMediaMin.score(xTMediaMinTest,yTMediaMinTest),xTMediaMinTest)))\nprint('Valor MSE: {}' .format(mean_squared_error(yTMediaMinTest,yTMediaMinPred)))\nprint('Coeficientes da regress\u00e3o: {}'.format(regTMediaMin.coef_))\nprint('Intercept da regress\u00e3o: {}'.format(regTMediaMin.intercept_))\nprint('VIF : {}\\n'.format(vifTMediaMin))\nprint('* * * * * *\\n')\n\n\nxTMaxTrain,xTMaxTest,yTMaxTrain,yTMaxTest = train_test_split(xTMax,yTMax,test_size=0.4,random_state=42)\nregTMax = LinearRegression()\nregTMax.fit(xTMaxTrain,yTMaxTrain)\nyTMaxPred = regTMax.predict(xTMaxTest)\nresTMax = yTMaxPred - yTMaxTest\navgResTMax = round(np.mean(resTMax),3)\nstdResTMax = round(np.std(resTMax),3)\nvifTMax = [variance_inflation_factor(xTMaxTrain, i) for i in range(xTMaxTrain.shape[1])]\n\nprint('--- TMax ---')\nprint('Valor de R2: {}'.format(regTMax.score(xTMaxTest,yTMaxTest)))\nprint('Valor de R2 ajustado: {}'.format(calculaR2ajustado(regTMax.score(xTMaxTest,yTMaxTest),xTMaxTest)))\nprint('Valor MSE: {}' .format(mean_squared_error(yTMaxTest,yTMaxPred)))\nprint('Coeficientes da regress\u00e3o: {}'.format(regTMax.coef_))\nprint('Intercept da regress\u00e3o: {}'.format(regTMax.intercept_))\nprint('VIF : {}\\n'.format(vifTMax))\nprint('* * * * * *\\n')\n\n\nxTMinTrain,xTMinTest,yTMinTrain,yTMinTest = train_test_split(xTMin,yTMin,test_size=0.4,random_state=42)\nregTMin = LinearRegression()\nregTMin.fit(xTMinTrain,yTMinTrain)\nyTMinPred = regTMin.predict(xTMinTest)\nresTMin = yTMinPred - yTMinTest\navgResTMin = round(np.mean(resTMin),3)\nstdResTMin = round(np.std(resTMin),3)\nvifTMin = [variance_inflation_factor(xTMinTrain, i) for i in range(xTMinTrain.shape[1])]\n\nprint('--- TMin ---')\nprint('Valor de R2: {}'.format(regTMin.score(xTMinTest,yTMinTest)))\nprint('Valor de R2 ajustado: {}'.format(calculaR2ajustado(regTMin.score(xTMinTest,yTMinTest),xTMinTest)))\nprint('Valor MSE: {}' .format(mean_squared_error(yTMinTest,yTMinPred)))\nprint('Coeficientes da regress\u00e3o: {}'.format(regTMin.coef_))\nprint('Intercept da regress\u00e3o: {}'.format(regTMin.intercept_))\nprint('VIF : {}\\n'.format(vifTMin))\nprint('* * * * * *\\n')\n\n","016a34d7":"folds = 6\nscores = cross_val_score(reg,xTrain,yTrain,cv=folds)\navgScores = np.mean(scores)\n\nplt.scatter(list(range(1,folds+1)),scores)\nplt.plot([1,folds],[avgScores,avgScores],linestyle='--',c='red')\nplt.xlabel('Fold')\nplt.ylabel('Score')\nplt.xticks(ticks=list(range(1,folds+1)))\nplt.legend(['M\u00e9dia','Score'],loc='lower right')\nplt.title('CV com todos os par\u00e2metros ')\nplt.grid()\nprint('Valores de R2 para valida\u00e7\u00e3o cruzada: {}'.format(scores))\nprint('M\u00e9dia de R2 para valida\u00e7\u00e3o cruzada: {}'.format(avgScores))\nprint('Std todos: {}'.format(np.std(scores)))","f7951739":"scoresTMax = cross_val_score(regTMax,xTMaxTrain,yTMaxTrain,cv=folds)\navgScoresTMax = np.mean(scoresTMax)\nplt.scatter(list(range(1,folds+1)),scoresTMax)\nplt.plot([1,folds],[avgScoresTMax,avgScoresTMax],linestyle='--',c='red')\nplt.xlabel('Fold')\nplt.ylabel('Score')\nplt.xticks(ticks=list(range(1,folds+1)))\nplt.legend(['M\u00e9dia','Scores'],loc='upper right')\nplt.title('CV com Temp. M\u00e1xima, precipita\u00e7\u00e3o e dia da semana ')\nplt.grid()\nprint('Valores de R2 para valida\u00e7\u00e3o cruzada: {}'.format(scoresTMax))\nprint('M\u00e9dia de R2 para valida\u00e7\u00e3o cruzada: {}'.format(avgScoresTMax))\nprint('Std TMax: {}'.format(np.std(scoresTMax)))","614b818a":"scoresLasso = []\nalphas = np.linspace(5e-4,2e-1,num=10000)\n\nfor a in alphas:\n  regLasso = Lasso(alpha=a)\n  regLasso.fit(xTrain,yTrain)\n  yPredLasso = regLasso.predict(xTest)\n  scoresLasso.append(regLasso.score(xTest,yTest))\n\nplt.plot(alphas,scoresLasso)\nplt.xlabel('Alpha')\nplt.ylabel('Score')\nplt.title(r'Varia\u00e7\u00e3o de $R^2$ em fun\u00e7\u00e3o de $\\alpha$')\nplt.grid()\n\n","2a773d25":"alphasCV = np.linspace(5e-4,2e-1,num=10000)\nparamGridLasso = {'alpha' : alphasCV,\n              'normalize': [True,False]}\nregLassoCV= GridSearchCV(Lasso(),cv=folds,param_grid= paramGridLasso)\nregLassoCV.fit(xTrain,yTrain)\nyPredRegLassoCV = regLassoCV.predict(xTest)\nresLassoCV = yPredRegLassoCV - yTest\nprint('Valor de R2 Lasso: {}'.format(regLassoCV.score(xTest,yTest)))\nprint('Valor de R2 ajustado Lasso: {}'.format(calculaR2ajustado(regLassoCV.score(xTest,yTest),xTest)))\nprint('Valor MSE Lasso: {}' .format(mean_squared_error(yTest,yPredRegLassoCV)))\nprint('Coeficientes : {}'.format(regLassoCV.best_estimator_.coef_))\nprint('Intercept : {}'.format(regLassoCV.best_estimator_.intercept_))\nprint('Melhores par\u00e2metros: {}'.format(regLassoCV.best_params_))","98e96214":"resLassoCV = yPredRegLassoCV - yTest\navgResLassoCV = round(np.mean(resLassoCV),3)\nstdResLassoCV = round(np.std(resLassoCV),3)\n\nplt.scatter(yPredRegLassoCV,resLassoCV)\nplt.plot([min(yPredRegLassoCV),max(yPredRegLassoCV)],[avgResLassoCV,avgResLassoCV],linestyle='--',c='red')\nplt.title('Erro residual: Regress\u00e3o Lasso')\nplt.xlabel('Valor previsto')\nplt.ylabel('Erro residual')\nplt.text(16.8,4,'$\\mu$ = {}\\n$\\sigma$ = {}'.format(avgResLassoCV,stdResLassoCV))\n","3fa2d922":"regTMaxCV= GridSearchCV(regTMax,cv=folds,param_grid={'normalize':[True,False]})\nregTMaxCV.fit(xTMaxTrain,yTMaxTrain)\nyPredTMaxCV = regTMaxCV.predict(xTMaxTest)\nresTMaxCV = yPredTMaxCV - yTMaxTest\navgResTMaxCV = round(np.mean(resTMaxCV),3)\nstdResTMaxCV = round(np.std(resTMaxCV),3)\n\n\nprint('Valor de R2 TMax: {}'.format(regTMaxCV.score(xTMaxTest,yTMaxTest)))\nprint('Valor de R2 ajustado TMax: {}'.format(calculaR2ajustado(regTMaxCV.score(xTMaxTest,yTMaxTest),xTMaxTest)))\nprint('Valor MSE: {}' .format(mean_squared_error(yTMaxTest,yPredTMaxCV)))\nprint('Coeficientes : {}'.format(regTMaxCV.best_estimator_.coef_))\nprint('Intercept : {}'.format(regTMaxCV.best_estimator_.intercept_))\nprint('Melhores par\u00e2metros: {}'.format(regTMaxCV.best_params_))\n\nplt.scatter(yPredTMaxCV,resTMaxCV)\nplt.plot([min(yPredTMaxCV),max(yPredTMaxCV)],[avgResTMaxCV,avgResTMaxCV],linestyle='--',c='red')\nplt.text(16.8,3.8,'$\\mu$ = {}\\n$\\sigma$ = {}'.format(avgResTMaxCV,stdResTMaxCV))\nplt.title('Erro residual:Temp. M\u00e1xima, precipita\u00e7\u00e3o e dia da semana')\nplt.xlabel('Valor previsto')\nplt.ylabel('Erro residual')\n","e4ca2cd8":"O consumo de cerveja tende a ser maior quando os term\u00f4metros registram maior temperatura.  O gr\u00e1fico de precipita\u00e7\u00e3o mostra alta amplitude nos valores de consumo de cerveja para dias sem chuva. Essa amplitude \u00e9 reduzida quanto maior o valor de precipita\u00e7\u00e3o, bem como o volume de cerveja consumido.","f8f18d04":"Os resultados acima mostram que o melhor desempenho \u00e9 obtido com o modelo que mant\u00e9m, entre os par\u00e2metros de temperatura, apenas a temperatura m\u00e1xima, pois apresenta o maior valor de $R^2$ ajustado e menores valores de $VIF$ e $MSE$.","6da1f7cb":"O gr\u00e1fico abaixo relaciona o consumo com o fato do dia ser fim de semana (1) ou n\u00e3o (0). Percebe-se que a m\u00e9dia de consumo de cerveja \u00e9 maior nos finais de semana:","bac3848b":"Os gr\u00e1ficos acima mostram o resultado do erro residual, calculado pela subtra\u00e7\u00e3o do valor observado e o valor estimado, ou ajustado:\n\n$$e = y - \\hat{y}$$\n\n[Os res\u00edduos devem estar centrados em 0 ao longo de todo a faixa de valores ajustados](https:\/\/blog.minitab.com\/blog\/adventures-in-statistics-2\/why-you-need-to-check-your-residual-plots-for-regression-analysis). Nos interessa ter um conjunto de valores de erro residual com m\u00e9dia pr\u00f3xima de 0 e com baixa dispers\u00e3o.","b479e961":"Agora podemos realizar an\u00e1lises sobre os dados relevantes utilizando recursos gr\u00e1ficos:","c0163b84":"Deve-se fazer uma investiga\u00e7\u00e3o sobre quais par\u00e2metros s\u00e3o relevantes na predi\u00e7\u00e3o e se existe alguma rela\u00e7\u00e3o intr\u00ednseca entre eles que possam afetar o modelo. Isso ocorre quando h\u00e1 colinearidade e multicolinearidade. Esse \u00faltimo \u00e9 [um fen\u00f4meno no qual uma vari\u00e1vel preditora em um modelo de regress\u00e3o m\u00faltipla pode ser linearmente predito por outras com um substancial grau de acur\u00e1ria.](https:\/\/en.wikipedia.org\/wiki\/Multicollinearity)\n\nPara medir a multicolinearidade existe o par\u00e2metro *Variance Inflation Fator* (VIF):\n\n$$ V.I.F. = 1 \/ (1 - R^2)$$\n\nDeve-se calcular o VIF para cada vari\u00e1vel e [se estiver entre 5 e 10 \u00e9 prov\u00e1vel que a multicolinearidade esteja presente e deve-se considerar descartar a vari\u00e1vel.](https:\/\/etav.github.io\/python\/vif_factor_python.html)\n","23bad81d":"Nos par\u00e2metros que envolvem temperatura \u00e9 poss\u00edvel identifcar altos valores de VIF e, por esssa raz\u00e3o, existe a necessidade extrair do modelo a redund\u00e2ncia provocada por eles. Existem seis possibilidades:  ","f22dc2b7":"#**Considera\u00e7\u00f5es finais**\n\nOs modelos Lasso e regress\u00e3o com temperatura m\u00e1xima foram os que apresentaram  melhor desempenho. Seus valores de $R^2$ ajustado foram de aproximadamente 0.7349 e 0.7533, respectivamente. O modelo com temperatura m\u00e1xima obteve valor de $MSE\\approx$ 5.3900 , menor que Lasso que obteve 5.5173. Outra informa\u00e7\u00e3o que vale ser ressaltada se refere \u00e0 sele\u00e7\u00e3o de *features*. Enquanto Lasso faz uso de quatro *features* (temperatura m\u00e9dia, temperatura m\u00e1xima, precipita\u00e7\u00e3o e dia da semana) o outro modelo faz uso de tr\u00eas (temperatura m\u00e1xima, precipita\u00e7\u00e3o e dia da semana), sendo o \u00faltimo menos complexo.\n","6779012d":"A partir das informa\u00e7\u00f5es coletadas pode-se criar um modelo de predi\u00e7\u00e3o baseado em regress\u00e3o linear que calcule a quantidade de cerveja consumida em um dia considerando os dados de temperatura , precipita\u00e7\u00e3o e dia da semana. Em outras palavras, assume-se que temperatura, precipita\u00e7\u00e3o e dia da semana respeitam o modelamento de equa\u00e7\u00e3o de reta:\n\n$$y = \\Theta^TX $$\n\n\nOnde $y$ \u00e9 o valor do consumo de cerveja, $\\Theta$ \u00e9 uma matriz que cont\u00e9m os coeficientes a serem calculados e $X$ \u00e9 uma matriz com as valores de temperatura, precipita\u00e7\u00e3o e dia da semana.\n\nSer\u00e1 utiliza a biblioteca [Scikit-learn](https:\/\/scikit-learn.org\/stable\/getting_started.html) para a cria\u00e7\u00e3o de modelos.\n","27f2b168":"Uma das m\u00e9tricas utilizadas para avaliar o desempenho do modelo \u00e9 $R^2$, coeficiente de determina\u00e7\u00e3o, definida a seguir:\n\n$$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$$\n\nOnde:\n\n$$SS_{res} = \\sum_{i=1}^n(y_i-\\hat{y_{i}})^2$$ e $$SS_{tot} = \\sum_{i=1}^n(y_i-\\bar{y})^2$$\n\nSendo $y_i$ o i-\u00e9simo valor observado,  $\\hat{y_i}$ a i-\u00e9sima predi\u00e7\u00e3o e $\\bar{y}$ a m\u00e9dia dos valores observados.\n\nA inte\u00e7\u00e3o \u00e9 que $R^2$ se aproxime de 1. Na pr\u00e1tica isso significa que $\\sum_{i=1}^n(y_i-\\hat{y_{i}})^2 = \\sum_{i=1}^ne_i^2$, chamado de \n[soma dos quadrados dos res\u00edduos](https:\/\/pt.wikipedia.org\/wiki\/Coeficiente_de_determina%C3%A7%C3%A3o), ser\u00e1 menor quanto melhor for o modelo adotado.\n","923c191b":"Outra maneira de resolver o problema de multicolinearidade \u00e9 utilizando a regress\u00e3o Lasso. [Ela \u00e9 \u00fatil em alguns contextos devido a sua tend\u00eancia de preferir solu\u00e7\u00f5es com menos coeficientes n\u00e3o nulos, reduzindo efetivamente o n\u00famero de *features* das quais a solu\u00e7\u00e3o dada depende.](https:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#lasso) \n\nLasso utiliza um algoritmo que tenta minimizar:\n\n$$\\sum_{i=1}^n(y_i-\\hat{y_{i}})^2 + \\alpha \\sum_{i=1}^n|\\beta_i|$$\n\nA diferen\u00e7a desse modelo para o anterior reside no termo $\\alpha \\sum_{i=1}^n|\\beta_i|$, onde $\\alpha$ \u00e9 uma constante que deve ser ajustada e $\\beta$ s\u00e3o os coeficientes da reta. Quanto maior o valor de $\\alpha$ menor deve ser o valor do somat\u00f3rio dos coenficientes, for\u00e7ando o algoritmo a encontrar solu\u00e7\u00f5es com baixos valores de coeficientes podendo inclusive zer\u00e1-los. ","1e041317":"Utilizando GridsearchCV \u00e9 poss\u00edvel realizar valida\u00e7\u00e3o cruzada e fazer o ajuste de hiperpar\u00e2metros que s\u00e3o [par\u00e2metros que n\u00e3o s\u00e3o diretamente aprendidos nos estimadores.](https:\/\/scikit-learn.org\/stable\/modules\/grid_search.html#grid-search) No nosso caso, os hipermar\u00e2metros s\u00e3o $\\alpha$ e normaliza\u00e7\u00e3o.\n  [GridsearchCV gera exaustivamente candidatos a partir de uma grade de valores de par\u00e2metros especificados com o par\u00e2metro param_grid.](https:\/\/scikit-learn.org\/stable\/modules\/grid_search.html#grid-search)\n  ","dd2debcd":"#**Estudo sobre regress\u00e3o linear usando dataset \"Beer Consumption - Sao Paulo\"**\n\nO objetivo deste trabalho \u00e9 estudar modelos de regress\u00e3o linear atrav\u00e9s do uso de um [dataset de consumo de cerveja.](https:\/\/www.kaggle.com\/dongeorge\/beer-consumption-sao-paulo) O dataset disponibiliza o consumo em litros de cerveja registrado por dia em uma regi\u00e3o de bares universit\u00e1rios na cidade de S\u00e3o Paulo. Al\u00e9m disso, ele possui os registros di\u00e1rios de temperatura (m\u00e1xima, m\u00ednima e m\u00e9dia), precipita\u00e7\u00e3o e fim de semana (anotado como 1) ou n\u00e3o (anotado com 0). ","d34d4300":"Como a temperatura e o n\u00edvel de chuva variam ao longo do ano, faz-se necess\u00e1rio investigar onde est\u00e3o os dados nulos para verificar se existe predomin\u00e2nica de registro em um per\u00eddo do ano.","449bc98a":"Exitem sete colunas, por\u00e9m apenas duas s\u00e3o interpretadas como n\u00famero:","0a5f8649":"Outra observa\u00e7\u00e3o relevante retirada da sa\u00edda acima \u00e9 que existem 941 linhas, entretanto apenas 365 n\u00e3o s\u00e3o nulas.","db3b9629":"Valores pr\u00f3ximos a 0 indicam baixa ou nenhuma correla\u00e7\u00e3o e valores pr\u00f3ximos a 1 ou -1 refletem alta correla\u00e7\u00e3o, sendo o primeiro diretamente proporcional e o segundo inversamente proporcional. Vale ressaltar que, apesar da correla\u00e7\u00e3o medir a [intensidade de rela\u00e7\u00e3o entre duas vari\u00e1veis](https:\/\/medium.com\/swlh\/covariance-correlation-r-sqaured-5cbefc5cbe1c), isso n\u00e3o significa causalidade entre elas.","0ba9c6c3":"Outra m\u00e9trica que ser\u00e1 utilizada \u00e9 MSE (*Mean Squared Error*), ou Erro M\u00e9dio Quadr\u00e1tico:\n$$ MSE = \\frac{1}{n}\\sum_{i=1}^n(y_i-\\bar{y})^2$$\n\n\n","f69c6ca4":"Os gr\u00e1ficos acima evidenciam que os registros v\u00e1lidos est\u00e3o apenas entre as primeiras 365 entradas e portanto compreendem um ano de registros consecutivos, entre 01 de janeiro de 2015 e 31 de dezembro de 2015. Por essa raz\u00e3o, \u00e9 razo\u00e1vel que todas linhas nulas sejam eliminadas: \n\n","f5296ece":"Tendo substitu\u00eddo as v\u00edrgulas por pontos, podemos transformar as colunas identificadas como 'object' em 'float64':","ab2a9d19":"O m\u00e9todo de separa\u00e7\u00e3o entre teste e treinamento utilizado pode n\u00e3o ser a melhor abordagem, pois, dependendo dos dados aleat\u00f3rios selecionados para cada grupo, \u00e9 poss\u00edvel que ainda assim o treinamento do modelo seja feito sobre amostras peculiares do dataset e, por isso, ele n\u00e3o apresentar\u00e1 o melhor desempenho poss\u00edvel quando submetido a dados que est\u00e3o fora do conjunto de treinamento. \n\nPor essa raz\u00e3o, deve-se realizar a valida\u00e7\u00e3o cruzada. Essa \u00e9 uma [t\u00e9cnica utilizada para avaliar os resultados de uma an\u00e1lise estat\u00edstica e garantir que s\u00e3o independentes da parti\u00e7\u00e3o entre os dados de treinamento e de teste.](https:\/\/es.wikipedia.org\/wiki\/Validaci%C3%B3n_cruzada) Existem in\u00fameras formas de realizar tal procedimento. [Em uma abordagem b\u00e1sica, chamada *k-fold*, o conjunto de treinamento \u00e9 dividido em *k*  subconjuntos.](https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html#cross-validation)\nO modelo \u00e9 treinado sobre os $k-1$ subconjuntos e o subconjunto remanescente \u00e9 usado para validar o modelo. Esse processo \u00e9 repetido $k$ vezes para que todos os subconjuntos sejam contemplados na constru\u00e7\u00e3o de um modelo de predi\u00e7\u00e3o.\n\n","3b19f345":"As colunas de 1 a 4 deveriam ser interpretadas como n\u00famero. Isso n\u00e3o ocorre porque os n\u00famero foram registrados com ',' para indicar as casas decimais em vez de usar '.':","1b3fc783":"Caso us\u00e1ssemos todos os registros dispon\u00edveis no dataset para treinar o modelo, perder\u00edamos a refer\u00eancia de como o modelo se comporta com dados desconhecidos. Por essa raz\u00e3o, faz-se necess\u00e1rio seperar os dados entre treinamento e teste  de forma pseudorand\u00f4mica:","889cad11":"A coluna 'Data' n\u00e3o traz informa\u00e7\u00e3o relevante para o resto desta an\u00e1lise, portanto pode ser descartada:","07e8f3ba":"Para realizar compara\u00e7\u00e3o entre modelos n\u00e3o \u00e9 indidcado o uso de $R^2$ pois ele [ aumenta com o aumento dos termos mesmo se o modelo n\u00e3o estiver realmente melhorando.](https:\/\/www.kdnuggets.com\/2018\/04\/right-metric-evaluating-machine-learning-models-1.html) Isso significa que adicionar mais *features* ao modelo ir\u00e1 aumentar o valor de $R^2$, por\u00e9m isso n\u00e3o quer dizer que a adi\u00e7\u00e3o dessas *features* cria um modelo melhor.\n\nO $R^2$ ajustado oferece melhor refer\u00eancia para compara\u00e7\u00e3o entre modelos:\n$$R^2_{adj} = 1 - [\\frac{(1-R^2)(n-1)}{n-k-1}]$$\n\nOnde $n$ representa o n\u00famero de *datapoints* e $k$ o n\u00famero de vari\u00e1veis independentes.","58d11d56":"O mapa de calor plotado abaixo oferece representa\u00e7\u00e3o visual e num\u00e9rica do grau de correla\u00e7\u00e3o entre os dados. "}}