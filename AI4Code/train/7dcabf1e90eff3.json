{"cell_type":{"a70f7b98":"code","f5f5f199":"code","3358e8a3":"code","9090882c":"code","a7487d8d":"code","8ff3aa8d":"code","e1becaa7":"code","f738c987":"code","724a36aa":"code","d71546b6":"code","82cd52ed":"code","cfcdca24":"code","91e402ca":"code","143b3917":"code","d35b0368":"code","dc0efd8d":"code","76abbca7":"code","c64160e5":"code","013e48b7":"code","8c819d28":"code","c10357cd":"code","72ef714b":"code","24346d6a":"code","1421e725":"code","67e94ea5":"code","54c76d25":"code","4483c8ae":"code","917fc231":"code","fb10ae50":"code","c8b11a45":"code","15b565e8":"code","906f232e":"code","5bba9d0b":"code","954e37b7":"code","004e3e48":"code","e4507a9d":"code","ec49732e":"code","cc67fe72":"markdown","7c66505f":"markdown","20c3672d":"markdown","a5e7f9ed":"markdown","31f2a76b":"markdown","69159a8b":"markdown","b981db81":"markdown","00ebcc49":"markdown","33070d3a":"markdown","8e212843":"markdown"},"source":{"a70f7b98":"import numpy as np \nimport pandas as pd\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import *\n\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\n\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import export_graphviz\n\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\n\ninit_notebook_mode(connected=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')","f5f5f199":"df=pd.read_csv(\"..\/input\/creditcard.csv\")","3358e8a3":"df.head()","9090882c":"df.describe().T","a7487d8d":"plt.figure(figsize=(15,10))\ncor_df=df.corr()\ncor_df\nsns.heatmap(cor_df, cmap=\"Blues\",\n        xticklabels=cor_df.columns,\n        yticklabels=cor_df.columns)\nplt.title(\"Correlation Plot\")\nplt.show()","8ff3aa8d":"print(df[\"Class\"].value_counts())\nsns.countplot(df[\"Class\"])\nplt.title(\"Countplot of Class: Fraud\/Not Fraud\")\nplt.show()","e1becaa7":"trace1 = go.Box(\n    y=df[\"Amount\"],\n    name = 'Log transformed Distribution of Amount',\n    marker = dict(\n        color = 'rgb(12, 128, 128)',\n    )\n)\n\ndata = [trace0,trace1]\niplot(data, filename = 'basic-line')","f738c987":"trace1 = go.Box(\n    y=np.log1p(df[\"Amount\"]),\n    name = 'Log transformed Distribution of Amount',\n    marker = dict(\n        color = 'rgb(12, 128, 128)',\n    )\n)\n\ndata = [trace1]\niplot(data, filename = 'basic-line')","724a36aa":"print(\"Number of values with class 0 and amount< 1\",len(df[(df[\"Class\"]==0 )& (df[\"Amount\"]<1)][\"Amount\"]))","d71546b6":"sns.distplot(df[df[\"Class\"]==1][\"Amount\"])\nplt.xlabel(\"Frauduelent case\")\nplt.ylabel(\"Amount variations\")\nplt.title(\"Distribution of frauduelet cases with amount\")\nplt.show()","82cd52ed":"#Analysing Outliers in data\nfor i in df.columns:\n    print(i)\n    q1, q3= np.percentile(df[i],[25,75])\n    iqr = q3 - q1\n    lower_bound = q1 -(1.5 * iqr) \n    upper_bound = q3 +(1.5 * iqr) \n    print(\"Lower bound \",df[df[i]<lower_bound][i].count())\n    print(\"Upper bound \",df[df[i]>upper_bound][i].count())","cfcdca24":"#Since we saw that the Amount values were highly skewed \n#we tried converting to a normalized version using log transformation\n\ndf[\"log_amount\"]=np.log1p(df[\"Amount\"])\n\ndf.head().T","91e402ca":"#Fraud : -1 , Not Fraud : 1\ndf[\"unsup_class\"]=np.where(df[\"Class\"]==1,-1,1)","143b3917":"X_train, X_test, y_train, y_test = train_test_split(df.drop([\"unsup_class\",\"Class\",\"Amount\"],axis=1), df.unsup_class, test_size=0.25, random_state=0)","d35b0368":"iso_fac=IsolationForest(n_estimators=100, max_samples='auto', \n                   contamination=float(.2),max_features=1.0,\n                   bootstrap=False, n_jobs=-1, random_state=42, verbose=0,behaviour=\"new\")\niso_fac.fit(X_train)\nprint(\"IsolationForest\\n \",iso_fac)\n","dc0efd8d":"train_pred=iso_fac.predict(X_train)\nlabels=y_train.unique()\nprint(\"Result of Training data\")\nprint(\"Confusion Matrix\")\ncm=confusion_matrix(y_train,train_pred)\nprint(pd.DataFrame(confusion_matrix(y_train,train_pred, labels=labels), index=labels, columns=labels))\n#Since through Isolation Forest we get -1 as outliers and 1 as correct values\n#Our main concern here is to predict error values lets calculate TNR :equivalent to recall\n\nprint(\"True Neagtive rate\",cm[1,1]\/(cm[1,1]+cm[1,0]))\nprint(\"Accuracy score\",accuracy_score(y_train,train_pred))\n\ntest_pred=iso_fac.predict(X_test)\nlabels=y_test.unique()\n\nprint(\"\\nResult of Testing data\")\nprint(\"Confusion Matrix\")\ncm=confusion_matrix(y_test,test_pred)\nprint(pd.DataFrame(confusion_matrix(y_test,test_pred, labels=labels), index=labels, columns=labels))\nprint(\"True Neagtive rate\",cm[1,1]\/(cm[1,1]+cm[1,0]))\nprint(\"Accuracy score\",accuracy_score(y_test,test_pred))","76abbca7":"lf= LocalOutlierFactor(n_neighbors=20, contamination=0.2)\npred=lf.fit_predict(X_train)\nprint(\"Local Outlier Factor \\n\",lf)\n","c64160e5":"train_pred=lf.fit_predict(X_train)\nlabels=y_train.unique()\nprint(\"Result of Training data\")\nprint(\"Confusion Matrix\")\ncm=confusion_matrix(y_train,train_pred)\nprint(pd.DataFrame(confusion_matrix(y_train,train_pred, labels=labels), index=labels, columns=labels))\n#Since through Isolation Forest we get -1 as outliers and 1 as correct values\n#Our main concern here is to predict error values lets calculate TNR :equivalent to recall\n\nprint(\"True Neagtive rate\",cm[1,1]\/(cm[1,1]+cm[1,0]))\nprint(\"Accuracy score\",accuracy_score(y_train,train_pred))\n\ntest_pred=lf.fit_predict(X_test)\nlabels=y_test.unique()\n\nprint(\"\\nResult of Testing data\")\nprint(\"Confusion Matrix\")\ncm=confusion_matrix(y_test,test_pred)\nprint(pd.DataFrame(confusion_matrix(y_test,test_pred, labels=labels), index=labels, columns=labels))\nprint(\"True Neagtive rate\",cm[1,1]\/(cm[1,1]+cm[1,0]))\nprint(\"Accuracy score\",accuracy_score(y_test,test_pred))","013e48b7":"X_train, X_test, y_train, y_test = train_test_split(df.drop([\"unsup_class\",\"Class\",\"Amount\"],axis=1), df.Class, test_size=0.25, random_state=0)","8c819d28":"def plot_roc():\n    plt.plot(fpr, tpr, label = 'ROC curve', linewidth = 2)\n    plt.plot([0,1],[0,1], 'k--', linewidth = 2)\n    plt.xlim([0.0,0.001])\n    plt.ylim([0.0,1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.show();","c10357cd":"# precision-recall curve\ndef plot_precision_recall():\n    plt.step(recall, precision, color = 'b', alpha = 0.2,\n             where = 'post')\n    plt.fill_between(recall, precision, step ='post', alpha = 0.2,\n                 color = 'b')\n\n    plt.plot(recall, precision, linewidth=2)\n    plt.xlim([0.0,1])\n    plt.ylim([0.0,1.05])\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision Recall Curve')\n    plt.show();","72ef714b":"lr= LogisticRegression(C=0.5,class_weight=\"balanced\")\nlr.fit(X_train,y_train)","24346d6a":"train_pred=lr.predict(X_train)\nlabels=y_train.unique()\nprint(\"Result of Training data\")\nprint(\"Confusion Matrix\")\nprint(pd.DataFrame(confusion_matrix(y_train,train_pred, labels=labels), index=labels, columns=labels))\nprint(\"roc-auc score on train data :\",roc_auc_score(y_train,train_pred))\nprint(\"precision\",precision_score(y_train, train_pred))\nprint(\"recall\",recall_score(y_train, train_pred))\nprint(\"F1 score\",f1_score(y_train, train_pred))\n\ntest_pred=lr.predict(X_test)\nlabels=y_test.unique()\n\nprint(\"\\n Result of Testing data\")\nprint(\"Confusion Matrix\")\nprint(pd.DataFrame(confusion_matrix(y_test,test_pred, labels=labels), index=labels, columns=labels))\nprint(\"roc-auc score on test data :\",roc_auc_score(y_test,test_pred))\nprint(\"precision\",precision_score(y_test,test_pred))\nprint(\"recall\",recall_score(y_test,test_pred))\nprint(\"F1 score\",f1_score(y_test,test_pred))\n","1421e725":"fpr, tpr, t = roc_curve(y_test, test_pred)\nplot_roc()\n\n# Precision-recall curve\nprecision, recall, thresholds = precision_recall_curve(y_test, test_pred)\nplot_precision_recall()","67e94ea5":"rf=RandomForestClassifier(n_estimators=100, max_depth=12,random_state=0,class_weight=\"balanced\")\nrf.fit(X_train, y_train)\nprint(\"Random Forest \\n\",rf)","54c76d25":"train_pred=rf.predict(X_train)\nlabels=y_train.unique()\nprint(\"Result of Training data\")\nprint(\"Confusion Matrix\")\nprint(pd.DataFrame(confusion_matrix(y_train,train_pred, labels=labels), index=labels, columns=labels))\nprint(\"roc-auc score on train data :\",roc_auc_score(y_train,train_pred))\nprint(\"precision\",precision_score(y_train, train_pred))\nprint(\"recall\",recall_score(y_train, train_pred))\nprint(\"F1 score\",f1_score(y_train, train_pred))\n\ntest_pred=rf.predict(X_test)\nlabels=y_test.unique()\n\nprint(\"\\n Result of Testing data\")\nprint(\"Confusion Matrix\")\nprint(pd.DataFrame(confusion_matrix(y_test,test_pred, labels=labels), index=labels, columns=labels))\nprint(\"roc-auc score on test data :\",roc_auc_score(y_test,test_pred))\nprint(\"precision\",precision_score(y_test,test_pred))\nprint(\"recall\",recall_score(y_test,test_pred))\nprint(\"F1 score\",f1_score(y_test,test_pred))\n","4483c8ae":"fpr, tpr, t = roc_curve(y_test, test_pred)\nplot_roc()\n\n# Precision-recall curve\nprecision, recall, thresholds = precision_recall_curve(y_test, test_pred)\nplot_precision_recall()","917fc231":"scale_pos_weight = [0.125,0.5,1,2,3]\nfor i in scale_pos_weight:\n    print('scale_pos_weight = {}: '.format(i))\n    xgb = XGBClassifier(scale_pos_weight=i)\n    print(xgb)\n    xgb.fit(X_train, y_train)\n    train_pred=xgb.predict(X_train)\n    labels=y_train.unique()\n    print(\"Result of Training data\")\n    print(\"Confusion Matrix\")\n    print(pd.DataFrame(confusion_matrix(y_train,train_pred, labels=labels), index=labels, columns=labels))\n    print(\"roc-auc score on train data :\",roc_auc_score(y_train,train_pred))\n    print(\"precision\",precision_score(y_train, train_pred))\n    print(\"recall\",recall_score(y_train, train_pred))\n    print(\"F1 score\",f1_score(y_train, train_pred))\n\n    test_pred=xgb.predict(X_test)\n    labels=y_test.unique()\n\n    print(\"\\nResult of Testing data\")\n    print(\"Confusion Matrix\")\n    print(pd.DataFrame(confusion_matrix(y_test,test_pred, labels=labels), index=labels, columns=labels))\n    print(\"roc-auc score on test data :\",roc_auc_score(y_test,test_pred))\n    print(\"precision\",precision_score(y_test,test_pred))\n    print(\"recall\",recall_score(y_test,test_pred))\n    print(\"F1 score\",f1_score(y_test,test_pred))\n","fb10ae50":"#We have select xgb classifier  with 0.125 scale_pos_weight as the better performing classifier \nxgb = XGBClassifier(scale_pos_weight=0.125)\nxgb.fit(X_train, y_train)\n\ntest_pred = xgb.predict(X_test)\n\nfpr, tpr, t = roc_curve(y_test, test_pred)\nplot_roc()\n\n# Precision-recall curve\nprecision, recall, thresholds = precision_recall_curve(y_test, test_pred)\nplot_precision_recall()","c8b11a45":"\nimport keras.backend as K\nfrom keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, BatchNormalization, Input, Conv1D\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import metrics\nfrom keras.optimizers import Adam \nfrom sklearn.preprocessing import StandardScaler\nimport keras\nfrom keras.models import Model\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import *\nfrom keras import regularizers\nimport tensorflow as tf\nfrom keras.losses import binary_crossentropy\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nBATCH_SIZE = 1024\nNUM_FEATURES = 1200\n","15b565e8":"#Creation of our own evaluation metric\ndef recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall","906f232e":"\n\ndef _Model1():\n    inp = Input(shape=(30, 1))\n    d1 = Dense(16, activation='sigmoid')(inp)\n    d2 = Dense(16, activation='relu')(d1)\n    d3 = Dense(8, activation='sigmoid')(d2)\n    f2 = Flatten()(d3)\n    preds = Dense(1, activation='sigmoid')(f2)\n    model = Model(inputs=inp, outputs=preds)\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',recall])\n    return model\n\n","5bba9d0b":"cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n","954e37b7":"X_train.columns","004e3e48":"\nscaler=StandardScaler()\ndf_train=X_train\ndf_test=X_test","e4507a9d":"\n# also add dropout\n# different activations as well\n\npreds = []\nc = 0\noof_preds = np.zeros((len(df_train), 1))\nfor train, valid in cv.split(df_train, y_train):\n    print(\"VAL %s\" % c)\n    X_train = np.reshape(df_train.iloc[train].values, (-1, 30, 1))\n    y_train_ = y_train.iloc[train].values\n    X_valid = np.reshape(df_train.iloc[valid].values, (-1, 30, 1))\n    y_valid = y_train.iloc[valid].values\n    model = _Model1()\n    history=model.fit(X_train, y_train_, validation_data=(X_valid, y_valid), epochs=20, verbose=2, batch_size=128)\n\n    \n    X_test = np.reshape(df_test.values, (-1, 30, 1))\n    curr_preds = model.predict(X_test, batch_size=256)\n    oof_preds[valid] = model.predict(X_valid)\n    preds.append(curr_preds)\n    c += 1\n\n\n","ec49732e":"print(\"Training data results:\")\nauc = roc_auc_score(y_train, oof_preds)\nprint(\"CV_AUC: {}\".format(auc))\n\n\nprint(\"\\nTest data results: \")\nprint(\"Precision score\",round(precision_score(y_test, np.around(preds[1])),2))\nprint(\"Confusion metrics \\n\",confusion_matrix(y_test, np.around(preds[1])))\n","cc67fe72":"### Logistic Regression","7c66505f":"## Supervised Learning \n>Logistic Regression\n\n>Random Forest Classifier\n\n>XGBoost Classifier","20c3672d":"### Local Outlier Factor","a5e7f9ed":"# Credit card Fraud Detection \n","31f2a76b":"### XGBoost Classifier ","69159a8b":"**Importing required libraries**","b981db81":"### Isolation Forest","00ebcc49":"## Basic Neural Network Implemenation ","33070d3a":"### Random Forest Classifier","8e212843":"## Unsupervised Learning Methods\nBased on anamoly detection \n>Isolation Forest\n\n>LocalOutlierFactor"}}