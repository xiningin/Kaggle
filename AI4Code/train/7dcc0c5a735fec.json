{"cell_type":{"66ac19f1":"code","e0414462":"code","f1c5b509":"code","cb290325":"code","e0df6957":"code","475b8cdd":"code","53cc298d":"code","0c6735b2":"code","abca13b1":"code","ddd56215":"code","ab7bb450":"code","d2f19ae5":"code","cb2ea072":"code","81776260":"code","7ddf1756":"code","1a75fdf6":"code","0ef81169":"code","308a1674":"code","fb28813f":"code","9e82987b":"code","fa4f4e0a":"markdown","390bbdd8":"markdown","4c2df899":"markdown","1c919a11":"markdown","c082341a":"markdown","c5cf5027":"markdown","d5286693":"markdown","b12c185a":"markdown","32bee80a":"markdown","0a1cbbae":"markdown","a6b7c016":"markdown","26de0e9c":"markdown","1f28e5c9":"markdown","c74f6188":"markdown","d6a21c6e":"markdown","b359dead":"markdown","9c3b99e5":"markdown","9244bf10":"markdown","393b1d21":"markdown","48a7532d":"markdown","ff0b2592":"markdown","391f8cbb":"markdown","41606803":"markdown","76afd0de":"markdown","d62835f1":"markdown","0032158f":"markdown","aa72b8f8":"markdown","51074cfd":"markdown","a823abf4":"markdown","6c6b6385":"markdown","97528a01":"markdown","e79b012c":"markdown"},"source":{"66ac19f1":"# Import Packages\nimport os\nimport re\nimport nltk\nimport json\nimport torch\nimport nltk.corpus  \nimport pandas as pd\nimport numpy as np\nfrom copy import deepcopy\nfrom nltk.stem import PorterStemmer\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n!pip install transformers\nfrom transformers import BertForQuestionAnswering\nfrom transformers import BertTokenizer\nfrom fuzzywuzzy import fuzz \nfrom tqdm import tqdm\n\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Settings\npd.set_option('display.max_colwidth', None)","e0414462":"# Text Preprocessing `clean_sent()`----------------------------------------------------------------------\nporter_stemmer = PorterStemmer()\ndef clean_sent(sentence):\n    \"\"\"\n    Clean the sentence\n    :param sentence: text to to be cleaned\n    :return: text that has been cleaned\n    \"\"\"\n    #nltk.FreqDist(words).most_common(10)\n    stopwords = set(nltk.corpus.stopwords.words('english'))\n    words = sentence.split()\n    # Lowercase all words (default_stopwords are lowercase too)\n    words = [word.lower() for word in words]\n    #words = sentence\n    words = [word for word in words if len(word) > 1]\n    # Remove numbers\n    words = [word for word in words if not word.isnumeric()]\n    # Remove punctuation\n    words = [word for word in words if word.isalpha()]\n    # Remove stopwords\n    words = [word for word in words if word not in stopwords]\n    # Porter\n    words = [porter_stemmer.stem(word) for word in words]\n    #fdist = nltk.FreqDist(words_lc)   \n    return \" \".join(words)\n\n\n## Data Load----------------------------------------------------------------------\n\n\n\ndef col_fill(col_questions,col_names,col_for_excerpt,may_not_have_num,index):\n    \"\"\"\n    Get answers for multiple columns and append them back to target table (target_table_dic[index])\n    \n    :param col_questions: a list of string - questions, each corresponds to a specific column (excluding excerpt)\n    :param col_names: a list of string - columns names in target table to which col_questions correspond\n    :param col_for_excerpt: string - the most important question in col_questions, on which excerpt based\n    :param may_not_have_num: a list of string - column names in col_names, to indicate columns that may not have digits (columns that not put in this list must have digits in its answer)\n    :param index: question number in this task\n\n    :return: target_table_dic[index] with content filled in designated columns \n    \"\"\"\n    col_questions_cleaned = [clean_sent(ques) for ques in col_questions]\n\n    # go through papers\n    for i in tqdm(range(relevant_paper_dic[index].shape[0])):\n\n        #get sentences of each paper, preprocess it\n        paper_sent=re.split(' \\.|\\.(?=[A-Z])|\\. (?=[A-Z])|\\n', relevant_paper_dic[index].text[i])\n        \n#         paper_sent=relevant_paper_dic[index].text[i].split(\". \")\n        cleaned_paper_sent = [clean_sent(t) for t in paper_sent]\n\n        for num,q in enumerate(col_questions_cleaned):\n            # define questions\n            col_name=col_names[num]\n            full_question= col_questions[num]\n\n            # extract top 3 sentences\/paper for a specific questions, join them together as a string\n            lis=[]\n            for sent in cleaned_paper_sent:\n                lis.append(fuzz.ratio(q,sent) )\n            top_3_idx = [item[0] for item in sorted(enumerate(lis), key=lambda x: x[1],reverse=True)[0:3]]\n            #max_idx=max(enumerate(lis), key=lambda x: x[1])[0]\n            string ='; \\n'.join([paper_sent[idx] for idx in top_3_idx])\n\n            # Get answers, delete those without a number \n            answer=answer_question(full_question,string)        \n            if (not hasNumbers(answer)) and (col_name not in may_not_have_num):\n                answer=\"\" \n            if (\"[CLS]\" in answer) or (\"[SEP]\" in answer):\n                answer = \"\"\n            target_table_dic[index].loc[i,col_name]=answer\n\n            #  Exerpt answer extract\n            if col_name == col_for_excerpt:\n                excerpt_ans=string\n                for idx in top_3_idx:\n                    if (answer in paper_sent[idx]) and (answer!=\"\"):\n                        excerpt_ans=paper_sent[idx]\n                target_table_dic[index].loc[i,\"Excerpt\"]=excerpt_ans\n    return target_table_dic[index]\n\n\ndef format_name(author):\n    middle_name = \" \".join(author['middle'])\n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n\n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n\n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:\n            name_ls.append(name)\n\n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n\n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n\n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n\n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'],\n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n\n    return \"; \".join(formatted)\n\ndef load_files(dirname):\n    filenames = os.listdir(dirname)\n    raw_files = []\n\n    for filename in tqdm(filenames):\n        filename = dirname + filename\n        file = json.load(open(filename, 'rb'))\n        raw_files.append(file)\n\n    return raw_files\n\ndef clean_pdf_files(file_list, keyword_list):\n    nth_paper=0\n    cleaned_files=[]\n    for file in file_list:\n        with open(file) as f:\n            file=json.load(f)\n        features = [\n            file['paper_id'],\n            file['metadata']['title'],\n            format_authors(file['metadata']['authors']),\n            format_authors(file['metadata']['authors'],\n                           with_affiliation=True),\n            format_body(file['abstract']),\n            format_body(file['body_text']),\n            format_bib(file['bib_entries']),\n            file['metadata']['authors'],\n            file['bib_entries']\n        ]\n        if(nth_paper%1000)==0:\n            print(nth_paper)\n        nth_paper=nth_paper+1\n\n        has_keyword = False\n        for keyword in keyword_list:\n            if keyword in features[5]:\n                has_keyword = True\n                break\n        if has_keyword == True:\n            cleaned_files.append(features)\n    col_names = ['paper_id', 'title', 'authors',\n                 'affiliations', 'abstract', 'text',\n                 'bibliography','raw_authors','raw_bibliography']\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    return clean_df\n\n\n\n# BERT----------------------------------------------------------------------\nmodel = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\ndef answer_question(question, answer_text):\n    '''\n    Takes a `question` string and an `answer_text` string (which contains the\n    answer), and identifies the words within the `answer_text` that are the\n    answer. Prints them out.\n    '''\n    # ======== Tokenize ========\n    # Apply the tokenizer to the input text, treating them as a text-pair.\n    input_ids = tokenizer.encode(question, answer_text,max_length=500\n                                )\n\n    # Report how long the input sequence is.\n    #print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n\n    # ======== Set Segment IDs ========\n    # Search the input_ids for the first instance of the `[SEP]` token.\n    sep_index = input_ids.index(tokenizer.sep_token_id)\n\n    # The number of segment A tokens includes the [SEP] token istelf.\n    num_seg_a = sep_index + 1\n\n    # The remainder are segment B.\n    num_seg_b = len(input_ids) - num_seg_a\n\n    # Construct the list of 0s and 1s.\n    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n\n    # There should be a segment_id for every input token.\n    assert len(segment_ids) == len(input_ids)\n\n    # ======== Evaluate ========\n    # Run our example question through the model.\n    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n                                    token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n\n    # ======== Reconstruct Answer ========\n    # Find the tokens with the highest `start` and `end` scores.\n    answer_start = torch.argmax(start_scores)\n    answer_end = torch.argmax(end_scores)\n    \n    \n    # Get the string versions of the input tokens.\n    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n\n    # Start with the first token.\n    answer = tokens[answer_start]\n\n    # Select the remaining answer tokens and join them with whitespace.\n    for i in range(answer_start + 1, answer_end + 1):\n        \n        # If it's a subword token, then recombine it with the previous token.\n        if tokens[i][0:2] == '##':\n            answer += tokens[i][2:]\n        \n        # Otherwise, add a space then the token.\n        else:\n            answer += ' ' + tokens[i]\n            \n    s_scores = start_scores.detach().numpy().flatten()\n    e_scores = end_scores.detach().numpy().flatten()\n\n    return answer\n\n\n# Similarity ----------------------------------------------------------------------\ndef calc_simlarity_score(question_list, text_list,threshold=None, top=None):\n    if (threshold==None)  and  (top==None):\n        raise ValueError(\"Parameter `threshold` and `top` cannot both be None\")\n    dic = {}\n    tfidf = TfidfVectorizer()\n    corpus_tfidf_matrix = tfidf.fit_transform(text_list)\n    ques_tfidf_matrix = tfidf.transform(question_list)\n    sim_matrix = cosine_similarity(corpus_tfidf_matrix, ques_tfidf_matrix)\n    for ques_idx in range(sim_matrix.shape[1]):\n        dic[ques_idx] = []\n        if threshold != None:\n            if (threshold>1) or (threshold <0):\n                raise ValueError(\"Please enter a value from 0 to 1 for parameter `threshold`\")\n            for paper_idx in range(sim_matrix.shape[0]):\n                score = sim_matrix[paper_idx, ques_idx]\n                if score >= threshold:\n                    dic[ques_idx].append((paper_idx, score))\n            dic[ques_idx]=sorted(dic[ques_idx], key=lambda i: i[1], reverse=True)\n        elif top != None:\n            top_paper_idx_list = sorted(range(len(sim_matrix[:, ques_idx])), key=lambda i: sim_matrix[:,0][i], reverse=True)[:top]\n            dic[ques_idx] = [(top_idx, sim_matrix[top_idx, ques_idx]) for top_idx in top_paper_idx_list]\n    return dic, sim_matrix\n\n# Retrieve relevant paper----------------------------------------------------------------------\ndef retrieve_paper(df, dic):\n    df_dic={}\n    for ques_idx in dic:\n        new_df = df.iloc[[item[0] for item in dic[ques_idx]], :]\n        new_df['score'] = [item[1] for item in dic[ques_idx]]\n        new_df['question'] = questions[ques_idx]\n        df_dic[ques_idx]=new_df.copy()\n    return df_dic\n\n# Determine if a string has a value----------------------------------------------------------------------\ndef hasNumbers(inputString):\n     return any(char.isdigit() for char in inputString)\n","f1c5b509":"# Set parameters\npath = '\/kaggle\/input\/CORD-19-research-challenge\/document_parses\/pdf_json'\nkeyword_list = ['novel coronavirus', 'novel-coronavirus', 'coronavirus-2019', \n                'sars-cov-2', 'sarscov2', 'covid-19', 'covid19',\n                '2019ncov', '2019-ncov', 'wuhan']\n\n# Get list of file paths\nfile_list = [os.path.join(r, file)  for r, _, f in os.walk(path)  for file in f]\n\n# Clean \uff08This takes ~15 min\uff09\nclean_pdf_df = clean_pdf_files(file_list, keyword_list)","cb290325":"# Append additional info from metadata to main df\nmetadata = pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\")\nclean_pdf_df = clean_pdf_df.merge(metadata[['sha', 'title', 'authors', 'abstract', 'doi', 'publish_time', 'journal']], \n                                  how ='left', left_on='paper_id', right_on='sha')\n\n# Clean columns\nclean_pdf_df['title_x'] = clean_pdf_df['title_x'].fillna(clean_pdf_df['title_y'])\nclean_pdf_df['authors_x'] = clean_pdf_df['authors_x'].fillna(clean_pdf_df['authors_y'])\nclean_pdf_df['abstract_x'] = clean_pdf_df['abstract_x'].fillna(clean_pdf_df['abstract_y'])\nclean_pdf_df = clean_pdf_df.drop(['sha', 'title_y', 'authors_y', 'abstract_y'], axis=1)\nclean_pdf_df = clean_pdf_df.rename(columns={'title_x': 'title', 'authors_x': 'authors', 'abstract_x': 'abstract'})","e0df6957":"clean_pdf_df['text_cleaned'] = clean_pdf_df.apply(lambda row: clean_sent(row['text']), axis=1)","475b8cdd":"#clean_pdf_df.to_pickle((\".\/clean_pdf_df.pkl\"))\n#clean_pdf_df=pd.read_pickle(\"\/kaggle\/input\/clean-pdf-df\/clean_pdf_df.pkl\")","53cc298d":"# set text\ntext_cleaned = clean_pdf_df['text_cleaned']\n\nfrom pathlib import Path\n\n# set questions\npath = '\/kaggle\/input\/CORD-19-research-challenge\/Kaggle\/target_tables\/3_patient_descriptions\/'\nfile_list = sorted(list(Path(path).glob('*.csv')))\nquestions = [file.name.split(\".csv\")[0].strip('_') for file in file_list]\nquestions_cleaned = [clean_sent(ques) for ques in questions]\nfor i,q in enumerate(questions):\n    print(\"Question\" ,i+1,\":\",q)","0c6735b2":"#file_list = [os.path.join(r, file)  for r, _, f in os.walk(path)  for file in f]\ntable_cols_dic={}\ntable_dic={}\ntarget_table_dic={}\nfor i,file in enumerate(file_list):\n    df=pd.read_csv(file)\n    cols=list(df.columns)\n    table_cols_dic[i]=cols[1:]\n    table_dic[i]=df\n    target_table_dic[i]=pd.DataFrame(columns=cols)\n[print(table_cols_dic[key]) for key in table_cols_dic.keys()]","abca13b1":"# Select relevant paper to \ndic, sim_matrix = calc_simlarity_score(questions_cleaned, text_cleaned, threshold=0.15)\nrelevant_paper_dic = retrieve_paper(clean_pdf_df, dic)","ddd56215":"for key in target_table_dic.keys():\n    target_table_dic[key][['Date', 'Study', 'Journal']]=relevant_paper_dic[key][['publish_time', 'title', 'journal']]\n    target_table_dic[key]['Study Link'] = \"https:\/\/doi.org\/\" + relevant_paper_dic[key]['doi']\n    relevant_paper_dic[key]=relevant_paper_dic[key].reset_index(drop=True)\n    target_table_dic[key]=target_table_dic[key].reset_index(drop=True)\n    target_table_dic[key]['Added on'] = \"10-Jun-2020\"","ab7bb450":"# question index\nindex=0\nprint(questions[index])\ncommon_col_set = set(['Date', 'Study', 'Study Link', 'Journal', 'Added On','Added on'])\nset(table_cols_dic[index]).difference(common_col_set)","d2f19ae5":"col_questions=[\"What proportion or percentage can the virus be transmitted asymptomatically or during the incubation period?\",\n               #\"How many patients or samples were used in the experiment or study for asymptomatical transmission?\",\n               \"What is the number of patients, cases, or samples?\",\n               #\"What is the range, mean, median, or IQR of the age of patients or samples?\",\n               \"How old are the patients or what is the age of the patients?\",\n               \"What sample type obtained such as Anal, Blood, Broncho-alveolar lavage, Conjunctival, Fecal, GI tract, Lower respiratory tract, Nasal, Nasopharyngeal, Oropharyngeal, Pharyngeal, Rectal, Respiratory, Sputum, Throat, Urine, or not found ?\",\n               \"What is the study type or article research type like Systematic review, meta-analysis, Prospective observational study, Retrospective observational study, Observational study, Cross-sectional study, Case series, Expert review, Editorial, Simulation, or not found?\"]\n\ncol_names=['Asymptomatic Transmission','Sample Size',\"Age\",\"Sample Obtained\",\"Study Type\"]\ncol_for_excerpt='Asymptomatic Transmission'\nmay_not_have_num=['Sample Obtained',\"Study Type\"]\ntarget_table_dic[index]=col_fill(col_questions,col_names,col_for_excerpt,may_not_have_num,index)\n# Clean \"Study Type\",\"Sample Obtained\"\ncols=[\"Study Type\",\"Sample Obtained\"]\nfor i in range(relevant_paper_dic[index].shape[0]):\n    #get text of each paper, preprocess it\n    whole_text=relevant_paper_dic[index].text[i]    \n    #for col in col_names:\n    for col in cols:\n        bert_returned=target_table_dic[index][col][i]\n        if  bert_returned.lower() not in whole_text.lower():\n            target_table_dic[index][col][i]=\"\"\n            \n# select papers with qualified answers\ncon1 = target_table_dic[index]['Asymptomatic Transmission'].str.contains(\"%\")\ntarget_table_dic[index]=target_table_dic[index][con1]\ntarget_table_dic[index]['Characteristic Related to Question 2'] = \"-\"\n\n# Clean the table after manual checks\nfor i,row in target_table_dic[index].iterrows():\n    answer=answer_question(\"What is the number of patients, cases, or samples?\", row[\"Excerpt\"])\n    if \"%\" not in answer:\n        row[\"Sample Size\"]=answer\n    else:\n        row[\"Sample Size\"]= \"\"\n    if row[\"Sample Size\"] == \"20 , 21 , 22 , 23 , 24 , 25\":\n        row[\"Sample Size\"]=\"\"\n        row[\"Age\"]=\"\"\n    if row[\"Sample Size\"]==\"asymptomatic cases are likely under - reported ( 26 ) ( 27 ) ( 28 ) ( 29 )\":\n        row[\"Sample Size\"]=\"\"\n    if \"days\" in row[\"Age\"]:\n        row[\"Age\"]=\"\"\ntarget_table_dic[index]=target_table_dic[index][target_table_dic[index]['Asymptomatic Transmission']!=\"one - third of patients and sore throat was found in 14 . 0 %\"]\ntarget_table_dic[index]\ntarget_table_dic[index].to_csv('\/kaggle\/working\/table_'+ questions[index] +'.csv')","cb2ea072":"index=3\nprint(questions[index])\nset(table_cols_dic[index]).difference(common_col_set)","81776260":"col_questions=[\"What is the mean or median of the length in days of viral shedding after illness onset?\",\n               \"What is the IQR or range of length in days of viral shedding after illness onset?\",\n               \"What is the sample size or patients \/ cases number for the experiment of viral shedding?\",\n               \"What is the age (years old) range of the patients or samples?\",\n               \"What sample type obtained such as Anal, Blood, Broncho-alveolar lavage, Conjunctival, Fecal, GI tract, Lower respiratory tract, Nasal, Nasopharyngeal, Oropharyngeal, Pharyngeal, Rectal, Respiratory, Sputum, Throat, Urine?\",\n               \"What is the study type or article research type like Systematic review, meta-analysis, Prospective observational study, Retrospective observational study, Observational study, Cross-sectional study, Case series, Expert review, Editorial, Simulation?\"]\n\ncol_names=['Days','Range (Days)','Sample Size',\"Age\",\"Sample Obtained\",\"Study Type\"]\ncol_for_excerpt=\"Days\"\nmay_not_have_num=['Sample Obtained',\"Study Type\"]\ntarget_table_dic[index]=col_fill(col_questions,col_names,col_for_excerpt,may_not_have_num,index)\n# Clean \"Study Type\",\"Sample Obtained\"\ncols=[\"Study Type\",\"Sample Obtained\"]\nfor i in range(relevant_paper_dic[index].shape[0]):\n    #get text of each paper, preprocess it\n    whole_text=relevant_paper_dic[index].text[i]    \n    #for col in col_names:\n    for col in cols:\n        bert_returned=target_table_dic[index][col][i]\n        if  bert_returned.lower() not in whole_text.lower():\n            target_table_dic[index][col][i]=\"\"\ncon1 = target_table_dic[index][\"Days\"] != \"\"\ncon2 = target_table_dic[index][\"Range (Days)\"] != \"\"\ntarget_table_dic[index]=target_table_dic[index][ con1 | con2]\ntarget_table_dic[index]=target_table_dic[index][target_table_dic[index][\"Days\"]!=\"76 . 7\"]\ntarget_table_dic[index].loc[target_table_dic[index][\"Range (Days)\"]==\"13 days\",\"Days\"] =\"13 days\"\ntarget_table_dic[index].loc[target_table_dic[index][\"Sample Size\"]==\"34 ( 44 . 2 % ) males\",\"Sample Size\"] =\"64 patients\"\ntarget_table_dic[index]=target_table_dic[index][target_table_dic[index][\"Range (Days)\"]!=\"15 to 89\"]\ntarget_table_dic[index].loc[target_table_dic[index][\"Days\"]==\"13 days\",\"Days\"] =\"mean duration of 4 days\"\ntarget_table_dic[index].loc[target_table_dic[index][\"Days\"]==\"13 days\",\"Range (Days)\"] =\" IQR 3-7 days\"\ntarget_table_dic[index].loc[target_table_dic[index][\"Sample Size\"]==\"> 65 years\",\"Sample Size\"] =\"\"\ntarget_table_dic[index].loc[target_table_dic[index][\"Age\"]==\"2 . 61 , 2 . 77\",\"Age\"] =\"\"\ntarget_table_dic[index]=target_table_dic[index][target_table_dic[index][\"Days\"]!=\"25\"]\ntarget_table_dic[index].loc[target_table_dic[index][\"Age\"]==\"4 ( 3 - 7 ) days\",\"Age\"] =\"\"\ntarget_table_dic[index]\ntarget_table_dic[index].to_csv('\/kaggle\/working\/table_'+ questions[index] +'.csv')","7ddf1756":"# question index\nindex=4\nprint(questions[index])\nset(table_cols_dic[index]).difference(common_col_set)","1a75fdf6":"col_questions=[\"Cardiac , Dermatological , Hepatic ,Ocular , or Neurological manifestations?\",\n               \"What is the frequency distribution of Symptoms\",\n               \"What is the sample size or patients \/ cases number for the experiment or study for asymptomatical transmission?\",\n               #\"What is the range, mean, median, or IQR of age?\",\n               #\"What is the age range of the patients?\",\n               \"What is the age range\",\n               \"What sample type obtained such as Anal, Blood, Broncho-alveolar lavage, Conjunctival, Fecal, GI tract, Lower respiratory tract, Nasal, Nasopharyngeal, Oropharyngeal, Pharyngeal, Rectal, Respiratory, Sputum, Throat, Urine, or not found ?\",\n               \"Systematic review, meta-analysis, Prospective observational study, Retrospective observational study, Observational study, Cross-sectional study, Case series, Expert review, Editorial, Simulation, or not found?\"]\n\ncol_names=['Manifestation','Frequency of Symptoms','Sample Size',\"Age\",\"Sample Obtained\",\"Study Type\"]\ncol_for_excerpt='Manifestation'\nmay_not_have_num=[\"Manifestation\",'Sample Obtained',\"Study Type\"]\na=col_fill(col_questions,col_names,col_for_excerpt,may_not_have_num,index)\n# Clean \"Study Type\",\"Sample Obtained\"\ncols=[\"Study Type\",\"Sample Obtained\",\"Manifestation\"]\nfor i in range(relevant_paper_dic[index].shape[0]):\n    #get text of each paper, preprocess it\n    whole_text=relevant_paper_dic[index].text[i]    \n    #for col in col_names:\n    for col in cols:\n        bert_returned=target_table_dic[index][col][i]\n        if  bert_returned.lower() not in whole_text.lower():\n            target_table_dic[index][col][i]=\"\"\ncon1 = target_table_dic[index][\"Manifestation\"] != \"\"\ntarget_table_dic[index]=target_table_dic[index][ con1]# & con2]\ntarget_table_dic[index].loc[-target_table_dic[index][\"Age\"].str.contains(\"years\"),\"Age\"] =\"\"\ntarget_table_dic[index].loc[target_table_dic[index][\"Sample Size\"].str.contains(\"years\"),\"Sample Size\"] =\"\"\ntarget_table_dic[index].loc[target_table_dic[index][\"Manifestation\"]==\"haematological\",\"Sample Size\"] =\"99 patients\"\ntarget_table_dic[index].loc[target_table_dic[index][\"Sample Size\"]==\"14 patients\",\"Frequency of Symptoms\"] =\"Comorbidities were present in 51.8%, and the most frequent symptoms were fever (87%), cough (71%), chest pain\/tightness (65%), and dyspnea (56%)\"\ntarget_table_dic[index].loc[-target_table_dic[index][\"Frequency of Symptoms\"].str.contains(\"%\"),\"Frequency of Symptoms\"] =\"\"\ntarget_table_dic[index].loc[target_table_dic[index][\"Sample Size\"]==\"over 0 . 04ng \/ ml\",\"Sample Size\"] =\"\"\ntarget_table_dic[index].loc[target_table_dic[index][\"Sample Size\"]==\"1514866\",\"Sample Size\"] =\"\"\ntarget_table_dic[index].loc[target_table_dic[index][\"Study\"]==\"Cardiac involvement in COVID-19 patients: Risk factors, predictors, and complications: A review\",\"Frequency of Symptoms\"] =\"patients that recovered from sars - cov reported that 68 % had hyperlipidemia , 4 % had cardiovascular system abnormalities , and 60 % had glucose metabolism disorders after recovery.\"\ntarget_table_dic[index].loc[target_table_dic[index][\"Study\"]==\"Cardiac involvement in COVID-19 patients: Risk factors, predictors, and complications: A review\",\"Sample Size\"] =\"\"\ntarget_table_dic[index]\ntarget_table_dic[index].to_csv('\/kaggle\/working\/table_'+ questions[index] +'.csv')","0ef81169":"# question index\nindex=5\nprint(questions[index])\nset(table_cols_dic[index]).difference(common_col_set)","308a1674":"col_questions=[\"What is the proportion (percentage) of all positive COVID19 patients who were asymptomatic?\",\n               \"What is the sample size or patients \/ cases number for the experiment of symptom?\",\n               \"What is the age range of the patients or samples?\",\n               \"What sample type obtained such as Anal, Blood, Broncho-alveolar lavage, Conjunctival, Fecal, GI tract, Lower respiratory tract, Nasal, Nasopharyngeal, Oropharyngeal, Pharyngeal, Rectal, Respiratory, Sputum, Throat, Urine, or not found ?\",\n               \"What is the study type or article research type like Systematic review, meta-analysis, Prospective observational study, Retrospective observational study, Observational study, Cross-sectional study, Case series, Expert review, Editorial, Simulation, or not found?\"]\n\ncol_names=['Asymptomatic','Sample Size',\"Age\",\"Sample Obtained\",\"Study Type\"]\ncol_for_excerpt='Asymptomatic'\nmay_not_have_num=['Sample Obtained',\"Study Type\"]\ntarget_table_dic[index]=col_fill(col_questions,col_names,col_for_excerpt,may_not_have_num,index)\n# Clean \"Study Type\",\"Sample Obtained\"\ncols=[\"Study Type\",\"Sample Obtained\"]\nfor i in range(relevant_paper_dic[index].shape[0]):\n    #get text of each paper, preprocess it\n    whole_text=relevant_paper_dic[index].text[i]    \n    #for col in col_names:\n    for col in cols:\n        bert_returned=target_table_dic[index][col][i]\n        if  bert_returned.lower() not in whole_text.lower():\n            target_table_dic[index][col][i]=\"\"\ncon1 = target_table_dic[index][\"Asymptomatic\"].str.contains(\"%\")\ncon2 = target_table_dic[index][\"Study\"] != \"\"\ncon3 = target_table_dic[index][\"Excerpt\"].str.contains(\"Asymptom\")\ncon4 = target_table_dic[index][\"Excerpt\"].str.contains(\"asymptom\")\ntarget_table_dic[index]=target_table_dic[index][con1 & con2 & (con3 | con4)]\ntarget_table_dic[index]['Characteristic Related to Question 2'] = \"-\"\ntarget_table_dic[index].loc[target_table_dic[index][\"Sample Size\"].str.contains(\"%\"),\"Sample Size\"] =\"\"\ntarget_table_dic[index].loc[target_table_dic[index][\"Age\"].str.contains(\"%\"),\"Age\"] =\"\"\ntarget_table_dic[index].loc[target_table_dic[index][\"Age\"]==\"\u221280\u00b0c\",\"Sample Size\"] =\"17 symptomatic patients\"\ntarget_table_dic[index].loc[target_table_dic[index][\"Age\"]==\"\u221280\u00b0c\",\"Age\"] =\"\"\ntarget_table_dic[index].loc[target_table_dic[index][\"Sample Size\"].str.contains(\"\u2264\"),\"Sample Size\"] =\"\"\ntarget_table_dic[index].loc[target_table_dic[index][\"Sample Size\"]==\"7\",\"Sample Size\"] =\"\"\ntarget_table_dic[index].loc[target_table_dic[index][\"Sample Size\"]==\"[ 7 ] [ 8 ]\",\"Sample Size\"] =\"\"\ntarget_table_dic[index].loc[-target_table_dic[index][\"Age\"].str.contains(\"year\"),\"Age\"] =\"\"\ntarget_table_dic[index]\ntarget_table_dic[index].to_csv('\/kaggle\/working\/table_'+ questions[index] +'.csv')","fb28813f":"# question index\nindex=6\nprint(questions[index])\nset(table_cols_dic[index]).difference(common_col_set)","9e82987b":"col_questions=[\"What proportion or percentage of pediatric patients were asymptomatic?\",\n               \"What is the sample size or patients \/ cases number for the experiment or study for asymptomatical transmission?\",\n               \"What is the age range of the patients?\",\n               \"What sample obtained such as Anal, Blood, Broncho-alveolar lavage, Conjunctival, Fecal, GI tract, Lower respiratory tract, Nasal, Nasopharyngeal, Oropharyngeal, Pharyngeal, Rectal, Respiratory, Sputum, Throat, Urine, or not found ?\",\n               \"What is the study type or article research type like Systematic review, meta-analysis, Prospective observational study, Retrospective observational study, Observational study, Cross-sectional study, Case series, Expert review, Editorial, Simulation, or not found?\"]\n\ncol_names=['Aymptomatic','Sample Size',\"Age\",\"Sample Obtained\",\"Study Type\"]\ncol_for_excerpt='Aymptomatic'\nmay_not_have_num=['Sample Obtained',\"Study Type\"]\ntarget_table_dic[index]=col_fill(col_questions,col_names,col_for_excerpt,may_not_have_num,index)\n# Clean \"Study Type\",\"Sample Obtained\"\ncols=[\"Study Type\",\"Sample Obtained\"]\nfor i in range(relevant_paper_dic[index].shape[0]):\n    #get text of each paper, preprocess it\n    whole_text=relevant_paper_dic[index].text[i]    \n    #for col in col_names:\n    for col in cols:\n        bert_returned=target_table_dic[index][col][i]\n        if  bert_returned.lower() not in whole_text.lower():\n            target_table_dic[index][col][i]=\"\"\n#con1 = target_table_dic[index][\"Manifestation\"] != \"\"\ncon2 = target_table_dic[index][\"Aymptomatic\"].str.contains(\"%\")\ncon1 = target_table_dic[index][\"Excerpt\"].str.contains(\"pediatr\")\ncon3 = target_table_dic[index][\"Excerpt\"].str.contains(\"Asymptom\")\ncon4 = target_table_dic[index][\"Excerpt\"].str.contains(\"asymptom\")\ntarget_table_dic[index]=target_table_dic[index][con1 & con2 & (con3 | con4)]\ntarget_table_dic[index]['Characteristic Related to Question 2'] = \"-\"\ntarget_table_dic[index].loc[-target_table_dic[index][\"Age\"].str.contains(\"year\"),\"Age\"] =\"\"\ntarget_table_dic[index][\"Sample obtained\"] =target_table_dic[index][\"Sample Obtained\"] \ntarget_table_dic[index].loc[target_table_dic[index][\"Sample Size\"]==\"69\",\"Sample Size\"] =\"2134 pediatric patients\"\ntarget_table_dic[index]\ntarget_table_dic[index].to_csv('\/kaggle\/working\/table_'+ questions[index] +'.csv')","fa4f4e0a":"## 3. Data preprocessing\n* Use lowercase for all content.\n* Remove numbers, punctuations, stopwords.\n* Remove the commoner morphological and inflexional endings from words in English.\n","390bbdd8":"# Summary","4c2df899":"### Set directory and load relevant papers\n* Load the papers and related information to the data structure.\n* Filter the papers to only articles related to COVID-19.","1c919a11":"### [Q5] 'Proportion of pediatric COVID19 patients who were asymptomatic' `index: 6`","c082341a":"Extract answers from the relevant papers for each question and save them to summary tables:\n- Use Fuzzy match to choose top 3 relevant sentences from a given paper for each question (column to be filled)\n- Run BERT to extract answers from the selected setences\n- Perform manual checks and clean on the tables","c5cf5027":"Introduction of BERT:\n\n- BERT: Bidirectioanl encoder representations from transformers\n- A new method of pre-training language representations which obatains great results on NLP tasks.\n- BERT is Google's October 2019 update to the algorithm\n([google improvements examples](https:\/\/blog.google\/products\/search\/search-language-understanding-bert))\n- open source in 2018\n- Can help computers understand language like humans","d5286693":"### [Q3] 'Manifestations of COVID-19 including but not limited to possible cardiomyopathy and cardiac arrest' `index: 4`","b12c185a":"### [Q1] 'Can the virus be transmitted asymptomatically or during the incubation period' `index: 0`","32bee80a":"# Initial Setup","0a1cbbae":"![insert.png](attachment:insert.png)","a6b7c016":"### [Q4] \"Proportion of all positive COVID19 patients who were asymptomatic\" `index: 5`","26de0e9c":"### Extract Answers for Summary Tables\n- Function `hasNumbers(inputString)` determines if a string contains any digit, which is used to filter answers to certain questions that expect the answer to be a digit. \n- Function `col_fill(col_questions,col_names,col_for_excerpt,may_not_have_num,index)` fills the summary table with the existing columns from the paper collection(if available).\n- Function `answer_question(question, answer_text)` extracts the information with given question.\n\n> The code for Bert is based on: https:\/\/colab.research.google.com\/drive\/1uSlWtJdZmLrI3FCNIlUHFxwAJiSu2J0-","1f28e5c9":"## 1. Set cleaned text and topic","c74f6188":"We successfully generated 5 target tables for task 3 patient descriptions.","d6a21c6e":"# TF-IDF Cosine Simlarity & BERT [Task 3]","b359dead":"## Functions\n### Data Loading\n- Function `format_name(author)`, `format_affiliation(affiliation)`, `format_authors(authors, with_affiliation=False)`, `format_body(body_text)`, and `format_bib(bibs)` each converts the paper-related information to standard format.\n- Function `load_files(dirname)` loads the papers from json and saves them to a list.\n- Function `clean_pdf_files(file_list, keyword_list)` takes a list of file and converts it to a dataframe with cleaned columns.\n\n> The script for loading papers is based on: https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv","9c3b99e5":"## 1. Load data","9244bf10":"# Introduction\nWith the COVID-19 keeps ongoing, the number of research publications on COVID-19 is growing fast, making it increasingly difficult for researchers to spot the most relevant findings. This project aims to help the medical community find the information they seek by building data mining and search tools, which can ultimately support the ongoing fight against this pandemic. This notebook contains a basic pipeline of using NLP models to develop answers to high priority scientific questions regarding the COVID-19.\n\nThe pipeline in this notebook includes the following steps:\n1. Initial Setup\n2. Functions\n3. Data Preparation\n4. Paper Selection\n4. Extract Answers for Summary Tables","393b1d21":"# Contact\n* Kaitan Sun, kaitan9095@gmail.com\n* Jiangxue Han, jxhan0317@gmail.com\n* Richard Luo, ruize.luo@outlook.com\n* Hanying Gan, hanying.gan@outlook.com\n","48a7532d":"# Data Preparation","ff0b2592":"## 2. Build table structures with given target tables","391f8cbb":"### Paper Selection\n- Function `calc_simlarity_score(question_list, text_list,threshold=None, top=None)` calculates the similarity score between the TF-IDF matrix of the article and that of questions.\n- Function `retrieve_paper(df, dic)` retrieves the information of relevant papers for each question in the specific topic.\n\n> The code for TF-IDF cosine similarity is based on: https:\/\/github.com\/mbulusu\/Duplicate-Document-Detection-Meetup-Presentation","41606803":"### Data Cleaning\n- Function `clean_sent(sentence)` cleans the text by lowercasing all words, removing numbers, punctuations, and stopwords, and stemming all words.","76afd0de":"## 2. Combine selected papers with the metadata","d62835f1":"# Extract Answers for Summary Tables","0032158f":"## Load Libraries","aa72b8f8":"## 4. (Optional) Save dataframe to pickle","51074cfd":"## 4. Add common columns to all summary tables","a823abf4":"### [Q2] \"Length of viral shedding after illness onset\" `index: 3`","6c6b6385":"# Reference\n- The code for paper loading functions is based on: https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv\n- The code for TF-IDF cosine similarity is based on: https:\/\/github.com\/mbulusu\/Duplicate-Document-Detection-Meetup-Presentation\n- The code for Bert is based on: https:\/\/colab.research.google.com\/drive\/1uSlWtJdZmLrI3FCNIlUHFxwAJiSu2J0-","97528a01":"# Paper selection\nThe relevant papers for each question are selected by calculating the cosine similarity score based on the TF-IDF pair of articles and questions. \n\nTF-IDF is a statistical measure that evaluates how important a word is to a document when compared against the other document in the corpus. In this project, both articles and questions are converted to TF-IDF matrix so that the information can be captured with fewer words and can be used for calculation.\n\nCosine similarity is a commonly used metric to measure the similarity between two vectors by taking the dot product divided by the product of two vector\u2019s length. In this project, cosine similarity is used to measure the similarity between the TF-IDF matrix pair of documents and questions. In this way, the level of relevance can be quantified, which ultimately helps us to identify the most relevant papers for each question against more than 6,000 articles.\n","e79b012c":"## 3. Select relevant papers by TF-IDF cosine similarity"}}