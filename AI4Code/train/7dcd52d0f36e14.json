{"cell_type":{"43817deb":"code","1a143140":"code","b4913dd6":"code","45d460c4":"code","78e3d69a":"code","5d8f54cf":"code","bfbf50d6":"code","0d8faa00":"code","8dda2699":"code","cb03846a":"code","7f44a01e":"code","59131003":"code","f6609ded":"code","8d1388c9":"code","bb14d0e6":"code","f0dfe39d":"code","3b8ffa08":"code","e78a9fcf":"code","362e6c18":"code","fa77ec34":"code","801ca0d5":"code","2fd80ac9":"code","4481ad9a":"code","85375563":"code","3bd9ecba":"code","4374d28f":"code","dc126088":"code","76bb308a":"code","60004f26":"code","33cb7bc9":"code","e546b6a9":"code","20341508":"code","67c1f9a4":"code","7e092ff7":"code","79c20f39":"code","dd88bd71":"code","2cd5ec9d":"code","4e643426":"code","e45953ab":"code","35f8f35a":"code","7204b510":"code","74a182c7":"code","07a41e84":"code","aae1ca10":"code","1f62a6b6":"code","182f486e":"code","17630b44":"code","0615f385":"code","b10b80ee":"markdown","32146677":"markdown","86670286":"markdown","daa5df65":"markdown","3c3b5e9e":"markdown","d93d3985":"markdown","05d96c0f":"markdown","de74bcd4":"markdown","92b7988a":"markdown","814598b5":"markdown"},"source":{"43817deb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1a143140":"# important librarys\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.metrics import accuracy_score, confusion_matrix,classification_report\nfrom sklearn.ensemble import AdaBoostClassifier,BaggingClassifier,GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nimport scipy as sp\nimport warnings\nimport os \nwarnings.filterwarnings(\"ignore\")\nimport datetime\nfrom sklearn.model_selection import train_test_split","b4913dd6":"data =pd.read_csv(\"\/kaggle\/input\/drinking-water-probability\/drinking_water_potability.csv\")","45d460c4":"data.head()","78e3d69a":"#shape of the data\ndata.shape","5d8f54cf":"# data.info() use to see null values,data types of columns, shape of data\ndata.info()","bfbf50d6":"# uniques values in every column\ndata.nunique()","0d8faa00":"# check dataset is balanced or imbalnced\nsns.countplot(data=data,x=data.Potability)\ndata.Potability.value_counts()","8dda2699":"# null values in columns\ndata.isnull().sum()","cb03846a":"import missingno as msng\nmsng.matrix(data)","7f44a01e":"#data Preprocissing","59131003":"sns.distplot(data.ph)","f6609ded":"# replace null values with mean\ndata['ph'] = data['ph'].replace(np.nan, data.ph.mean())","8d1388c9":"sns.histplot(data.Hardness) # we can also use histplot or displot","bb14d0e6":"sns.distplot(data.Sulfate)","f0dfe39d":"# replace null values with mean\ndata['Sulfate'] = data['Sulfate'].replace(np.nan,data.Sulfate.mean())","3b8ffa08":"sns.distplot(data.Trihalomethanes)","e78a9fcf":"# replace null values with mean\ndata['Trihalomethanes'] = data['Trihalomethanes'].replace(np.nan,data.Trihalomethanes.mean())","362e6c18":"data.isnull().sum()","fa77ec34":"import missingno as msng\nmsng.matrix(data)","801ca0d5":"#distributions of data after filling null values","2fd80ac9":"sns.displot(data.ph)","4481ad9a":"sns.pairplot(data=data,hue='Potability')","85375563":"#statistical analysis of dataset\ndata.describe()","3bd9ecba":"# A Box Plot is also known as Whisker plot is created to display the summary of the set\n# of data values having properties like minimum, first quartile, median, third quartile and maximum. \n# In the box plot, a box is created from the first quartile to the third quartile, a vertical line\n# is also there which goes through the box at the median. Here x-axis denotes the data to be plotted while \n# the y-axis shows the frequency distribution.\nfor column in data:\n    plt.figure()\n    data.boxplot([column])","4374d28f":"data","dc126088":"# statistical anaysis of ph with respect to the Potability\ndata.groupby(['Potability'])['ph'].describe()","76bb308a":"# statistical anaysis of Hardness with respect to the Potability\ndata.groupby(['Potability'])['Hardness'].describe()","60004f26":"# statistical anaysis of Solids with respect to the Potability\ndata.groupby(['Potability'])['Solids'].describe()","33cb7bc9":"sns.pairplot(data=data)","e546b6a9":"# statistical analysis of data\ndata.describe()","20341508":"# lets see the correlation of data","67c1f9a4":"data.corr()","7e092ff7":"plt.figure(figsize=(20,10))\nsns.heatmap(data.corr(),annot=True)","79c20f39":"#feature selection","dd88bd71":"# lets see feature importance\nfrom sklearn.ensemble import ExtraTreesClassifier\nx = data.drop(['Potability'],axis=1)\ny =data.Potability","2cd5ec9d":"Ext = ExtraTreesClassifier()\nExt.fit(x,y)","4e643426":"print(Ext.feature_importances_)","e45953ab":"feature = pd.Series(Ext.feature_importances_,index=x.columns)\nfeature.sort_values(ascending=True).nlargest(10).plot(kind='barh')","35f8f35a":"#we will take all the features for prediction","7204b510":"data","74a182c7":"X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=0)","07a41e84":"print('Train values shape:', X_train.shape)\nprint('Test values shape:', X_test.shape)\nprint('Train target shape:', y_train.shape)\nprint('Test target shape:', y_test.shape)","aae1ca10":"# Logistic Regression\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\ny_train_hat = lr.predict(X_train)\ny_test_hat = lr.predict(X_test)\n\nprint(lr)\nprint('Train performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_train, y_train_hat))\n\nprint('Test performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_test, y_test_hat))\n\nprint('Roc_auc score')\nprint('-------------------------------------------------------')\nprint(roc_auc_score(y_test, y_test_hat))\nprint('')\n\nprint('Confusion matrix')\nprint('-------------------------------------------------------')\nprint(confusion_matrix(y_test, y_test_hat))\nprint('')\n\nprint('accuracy score')\nprint('-------------------------------------------------------')\nprint(\"test data accuracy score:\",accuracy_score(y_test, y_test_hat)*100)\nprint(\"train data accuracy score:\",accuracy_score(y_train, y_train_hat)*100)","1f62a6b6":"# SVM\nsvm = SVC()\nsvm.fit(X_train, y_train)\ny_train_hat = svm.predict(X_train)\ny_test_hat = svm.predict(X_test)\n\nprint(svm)\nprint('Train performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_train, y_train_hat))\n\nprint('Test performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_test, y_test_hat))\n\nprint('Roc_auc score')\nprint('-------------------------------------------------------')\nprint(roc_auc_score(y_test, y_test_hat))\nprint('')\n\nprint('Confusion matrix')\nprint('-------------------------------------------------------')\nprint(confusion_matrix(y_test, y_test_hat))\nprint('')\n\nprint('accuracy score')\nprint('-------------------------------------------------------')\nprint(accuracy_score(y_test, y_test_hat)*100)\nprint(\"test data accuracy score:\",accuracy_score(y_test, y_test_hat)*100)\nprint(\"train data accuracy score:\",accuracy_score(y_train, y_train_hat)*100)","182f486e":"# Random Forest\nrf = RandomForestClassifier(n_jobs=-1,random_state=123)\nrf.fit(X_train, y_train)\ny_train_hat = rf.predict(X_train)\ny_test_hat = rf.predict(X_test)\n\nprint(rf)\nprint('Train performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_train, y_train_hat))\n\nprint('Test performance')\nprint('-------------------------------------------------------')\nprint(classification_report(y_test, y_test_hat))\n\nprint('Roc_auc score')\nprint('-------------------------------------------------------')\nprint(roc_auc_score(y_test, y_test_hat))\nprint('')\n\nprint('Confusion matrix')\nprint('-------------------------------------------------------')\nprint(confusion_matrix(y_test, y_test_hat))\nprint('')\n\nprint('accuracy score')\nprint('-------------------------------------------------------')\nprint(\"test data accuracy score:\",accuracy_score(y_test, y_test_hat)*100)\nprint(\"train data accuracy score:\",accuracy_score(y_train, y_train_hat)*100)","17630b44":"#here with RANDOM FOREST we got training accuracy 100% but 67% on test data its mean our model is overfitting","0615f385":"# How to handle overfitting\n# In contrast to underfitting, there are several techniques available for handing overfitting that one can try to use. Let us look at them one by one.\n# 1. Get more training data: Although getting more data may not always be feasible, getting more representative data is extremely helpful. Having a larger diverse dataset usually aids in the model performance. You can get a better model that may generalize better. This means that the performance of the model on unseen data (true test set) will be better.\n# 2. Augmentation: If you can\u2019t get more data, you can try augmentation to add variation in your data. Augmentation means artificially modifying your existing data by means of transforms that resemble the variation you might expect in the real data. For imagery data, https:\/\/imgaug.readthedocs.io\/en\/latest\/ is a very comprehensive library that gives you a ton of augmentation methods. It allows you to compose powerful sequences of augmentations quickly and efficiently. I would recommend the following two articles for further reading. \n# Olga Chernytska\n#  has a detailed article on image augmentation which you should consider reading. \n# Valentina Alto\n#  nicely explains how image augmentation is done in Keras in this article.\n# 3. Early stopping[2,3]: Early stopping is a form of regularization to avoid overfitting when training a learner with an iterative method, such as gradient descent [2]. While training neural networks, we iteratively use gradients from the training data and try to make the model fit better approximate the underlying real-world function. In a sense, this method allows you to stop at or near the optimal fitting point. Thereby preventing overfitting to the training set and reducing generalization error. To decide when to stop, we can monitor certain metrics such as the loss, test_accuracy, val_accuracy and depending on certain conditions being met stop the training.\n# 4. Regularization L1, L2: Regularization is an additional term that is added to the loss function to impose a penalty on large network parameter weights to reduce overfitting. L1 and L2 regularization the two widely used techniques. Although they penalize large weights, they both achieve the regularization differently.","b10b80ee":"#SVM","32146677":"so we found some null values in columns","86670286":"* #Logistic Regression","daa5df65":"lets see the distribution of data for handling null values","3c3b5e9e":"# Random Forest","d93d3985":"Commentdown your Thought ;) btw im new on kaggle !!!","05d96c0f":"#Lets create model without feature scalling","de74bcd4":"Yayyy\ud83d\udc9d\ud83e\udd17\ud83e\udd73...now we dont have null values","92b7988a":"found really good artical pn handling overfitting- https:\/\/towardsdatascience.com\/techniques-for-handling-underfitting-and-overfitting-in-machine-learning-348daa2380b9#:~:text=%20How%20to%20handle%20overfitting%20%201%20Get,function%20to%20impose%20a%20penalty%20on...%20More%20","814598b5":"our dataset is not fully balnced it might will give probleam"}}