{"cell_type":{"65daa6f9":"code","d0f26e79":"code","7de2fae3":"code","ea6228c6":"code","e7160a0d":"code","d185b2f9":"code","e37228bd":"code","f59f4f55":"code","5bf8ab52":"code","9a841d94":"code","8155e523":"code","40abb846":"code","83190045":"code","a41a2f7d":"code","3aed6afd":"code","63ae36fe":"code","e2b0ef29":"code","e98e783e":"code","c67a30bb":"code","f2d97a40":"code","7baf819b":"code","7503c0f0":"code","8d7633b9":"code","1ce1f0ae":"code","684eb692":"code","d26ebb30":"code","61b53134":"code","a8ed6f23":"code","79a94a16":"code","44786f6d":"code","611addaf":"code","95a7fc0d":"code","59a4dad1":"code","670134f4":"code","176ed0cf":"markdown","5c354ddc":"markdown","75bc304c":"markdown","87d7259b":"markdown","41d03775":"markdown","9613fcf0":"markdown","ecaef975":"markdown","1462db6f":"markdown"},"source":{"65daa6f9":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport math\nfrom statistics import *\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import BatchNormalization\nfrom keras.layers import LSTM\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom pandas.api.types import CategoricalDtype\n\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import BayesianRidge\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping","d0f26e79":"data=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nprint(data.shape)\nprint(test.shape)","7de2fae3":"sns.distplot(data['SalePrice'])","ea6228c6":"plt.figure(figsize=(10,8))\ndata.corr()['SalePrice'].sort_values(ascending=False).plot(kind='bar')","e7160a0d":"  sns.scatterplot(x='SalePrice', y='GrLivArea',data=data)","d185b2f9":"sns.countplot(data['LotShape'])","e37228bd":"sns.countplot(data['PoolArea'])","f59f4f55":"data.corr()['SalePrice'][abs(data.corr()['SalePrice'])>0.3].sort_values()","5bf8ab52":"data.isnull().sum()[data.isnull().sum()>0]","9a841d94":"data.drop(['Id','FireplaceQu','Alley','PoolQC','Fence','MiscFeature'],axis=1,inplace=True)\ntest.drop(['Id','FireplaceQu','Alley','PoolQC','Fence','MiscFeature'],axis=1,inplace=True)","8155e523":"num_cols = data._get_numeric_data().columns\ncols=data.columns\nfeatures_cat = list(set(cols) - set(num_cols))\nfeatures_num = list(num_cols)\nfeatures_num.remove('SalePrice')","40abb846":"for i in features_cat:\n    data[i] = data[i].fillna(data[i].mode()[0])\n    test[i] = test[i].fillna(test[i].mode()[0])\nfor i in features_num:\n    data[i] = data[i].fillna(data[i].mean())\n    test[i] = test[i].fillna(test[i].mode()[0])\ndata.isnull().sum()[data.isnull().sum()>0]","83190045":"import copy\nX=data.drop('SalePrice',axis=1)\ny=data['SalePrice']","a41a2f7d":"for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    \ndiscrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\nmi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\nmi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\nmi_scores = mi_scores.sort_values(ascending=False)\nmi_scores\n","3aed6afd":"print(X.shape)\ntrain_objs_num = len(X)\ndataset = pd.concat(objs=[X, test], axis=0)\nl=dataset.select_dtypes(['object']).columns\ndummies = pd.get_dummies(dataset[l],drop_first=True)\n#dataset = pd.get_dummies(dataset)\ndataset = dataset.drop(l,axis=1)\ndataset = pd.concat([dataset,dummies],axis=1)","63ae36fe":"dataset[\"LivLotRatio\"] = data.GrLivArea \/ data.LotArea\ndataset[\"MedNhbdArea\"] = data.groupby(\"Neighborhood\")[\"GrLivArea\"].transform(\"median\")\nfeatures = [\n    \"LotArea\",\n    \"TotalBsmtSF\",\n    \"1stFlrSF\",\n    \"2ndFlrSF\",\n    \"GrLivArea\",\n]","e2b0ef29":"print(dataset.shape)\ndataset.isnull().sum()[dataset.isnull().sum()>0]","e98e783e":"dataset[\"Feature1\"] = dataset.GrLivArea + dataset.TotalBsmtSF\ndataset[\"Feature2\"] = dataset.YearRemodAdd * dataset.TotalBsmtSF","c67a30bb":"X = copy.copy(dataset[:train_objs_num])\ntest = copy.copy(dataset[train_objs_num:])","f2d97a40":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8)","7baf819b":"s=StandardScaler()\nX_train1 = s.fit_transform(X_train)\nX_valid1 = s.transform(X_valid)\ntest1 = s.transform(test)","7503c0f0":"xgb_params = dict(\n    max_depth=7,           # maximum depth of each tree - try 2 to 10\n    learning_rate=0.05,    # effect of each tree - try 0.0001 to 0.1\n    n_estimators=1000,     # number of trees (that is, boosting rounds) - try 1000 to 8000\n    min_child_weight=4,    # minimum number of houses in a leaf - try 1 to 10\n    colsample_bytree=0.7,  # fraction of features (columns) per tree - try 0.2 to 1.0\n    subsample=0.7,         # fraction of instances (rows) per tree - try 0.2 to 1.0\n    reg_alpha=0.5,         # L1 regularization (like LASSO) - try 0.0 to 10.0\n    reg_lambda=1.0,        # L2 regularization (like Ridge) - try 0.0 to 10.0\n    num_parallel_tree=1,   #  > 1 for boosted rf\n)\n\nmodel1 = XGBRegressor(**xgb_params)\nmodel1.fit(X_train1,y_train)\npreds1=model1.predict(X_valid1)\nprint(model1.score(X_valid1,y_valid))\nprint('MAE',mean_absolute_error(y_valid, preds1))","8d7633b9":"plt.figure(figsize=(8,8))\nplt.scatter(y_valid,preds1)\nplt.plot(y_valid,y_valid)","1ce1f0ae":"model1.fit(X, np.log(y))\npredictions = np.exp(model1.predict(test1))","684eb692":"from sklearn.decomposition import PCA\npca=PCA(n_components=4)\npca.fit(X_train)\nx_train_pca = pca.transform(X_train)\nx_val_pca=pca.transform(X_valid)\ncomponent_names = [f\"PC{i+1}\" for i in range(x_train_pca.shape[1])]\nx_train_pca = pd.DataFrame(x_train_pca, columns=component_names)\nx_train_pca","d26ebb30":"component_names = [f\"PC{i+1}\" for i in range(x_val_pca.shape[1])]\nx_val_pca = pd.DataFrame(x_val_pca, columns=component_names)\nx_val_pca","61b53134":"from sklearn.feature_selection import mutual_info_regression\ndef score_pca(df,dt):\n    mi_scores = mutual_info_regression(df, dt, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=df.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","a8ed6f23":"mi_scores=score_pca(x_train_pca,y_train)\ng=pd.DataFrame(x_train_pca)[mi_scores.index[0]]\nX_train['fet1']=list(g)","79a94a16":"mi_scores1=score_pca(x_val_pca,y_valid)\ng=pd.DataFrame(x_val_pca)[mi_scores1.index[0]]\nX_valid['fet1']=list(g)","44786f6d":"X_valid.head()","611addaf":"s1=MinMaxScaler()\nX_train2 = s1.fit_transform(X_train)\nX_valid2 = s1.transform(X_valid)","95a7fc0d":"model2 = RandomForestRegressor()\nmodel2.fit(X_train1,y_train)\npreds2=model2.predict(X_valid1)\nprint(model2.score(X_valid1,y_valid))\nprint('MAE',mean_absolute_error(y_valid, preds2))","59a4dad1":"nn_model = Sequential()\n\nnn_model.add(Dense(512,input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\nnn_model.add(Dense(256,kernel_initializer='normal', activation='relu'))\nnn_model.add(Dense(128,kernel_initializer='normal', activation='relu'))\nnn_model.add(Dense(64,kernel_initializer='normal', activation='relu'))\nnn_model.add(Dense(32,kernel_initializer='normal', activation='relu'))\nnn_model.add(Dense(8,kernel_initializer='normal', activation='relu'))\nnn_model.add(Dense(8,kernel_initializer='normal', activation='relu'))\nnn_model.add(Dense(8,kernel_initializer='normal', activation='relu'))\nnn_model.add(Dense(1))\nnn_model.compile(loss='mse', optimizer=Adam(lr=0.001))\n\n\nearly_stopping = EarlyStopping(\n    min_delta=0.001, \n    patience=5, \n    restore_best_weights=True,\n)","670134f4":"nn_model.fit(X_train2,y_train, validation_data=(X_valid2,y_valid), epochs=100, callbacks=[early_stopping], \n             batch_size=64, verbose=1)\nloss=pd.DataFrame(nn_model.history.history)\nloss.plot()\npreds_nn=nn_model.predict(X_valid2)\nprint('MAE',mean_absolute_error(y_valid, preds_nn))\nprint('MSE',mean_squared_error(y_valid, preds_nn))","176ed0cf":"# PCA features","5c354ddc":"# Missing Values","75bc304c":"# XGB","87d7259b":"# Preprocessing","41d03775":"# RF","9613fcf0":"# Visualization","ecaef975":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1462db6f":"# NN"}}