{"cell_type":{"bac7b7e7":"code","da9de20e":"code","9d5cb6af":"code","aabc7cb3":"code","52122fa1":"code","fc07cd52":"code","bd17b52d":"code","b2dce3b6":"code","519c9bf2":"code","fb2e0dfc":"code","57760ab3":"code","b6057559":"code","d601274a":"code","4520e1f6":"code","d8b075a6":"code","dac259e6":"code","1f7cfc32":"code","5525cf1f":"code","dc28f6e2":"code","a2a7021c":"code","eca615b4":"code","3f373bf9":"code","94b769ba":"code","b4abbe3d":"code","5c992a57":"code","d9fe4acd":"code","4b3cf618":"code","533bcbc2":"code","b7cd1178":"code","c610070f":"code","a08f7d6c":"code","2c61bd3b":"code","e32cd07b":"code","855c3ff2":"code","b4bb8c05":"code","3f9ddc5d":"code","f4123bfd":"code","a4c1b4a7":"code","7aefd6d0":"code","68000714":"code","27ddbe72":"markdown","4562e753":"markdown","0708f4ac":"markdown","d5dc6bd2":"markdown","e9107199":"markdown","4794030c":"markdown","574af63b":"markdown","1bd42066":"markdown","9e2fbbc7":"markdown","d96fa451":"markdown","44fa04f6":"markdown","d9d4f766":"markdown","8440ffe4":"markdown","9525ff78":"markdown","a204dd27":"markdown","80aa71bc":"markdown","51ef9d2a":"markdown","7a31de31":"markdown","f0126e86":"markdown","a7f8bab1":"markdown","30ad751b":"markdown","bf3cd653":"markdown"},"source":{"bac7b7e7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","da9de20e":"from IPython.display import Image\nurl=\"https:\/\/i.stack.imgur.com\/FQhxk.jpg\"\nImage(url,width=800, height=800)","9d5cb6af":"df=pd.read_csv(\"https:\/\/cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud\/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork\/labs\/Module%204\/data\/Cust_Segmentation.csv\")\ndf.head()","aabc7cb3":"df.drop([\"Customer Id\",\"Address\"],axis=1, inplace=True)\ndf.head()","52122fa1":"df.isnull().sum()","fc07cd52":"df[\"Defaulted\"].fillna(df[\"Defaulted\"].mean(),inplace=True)\n","bd17b52d":"df.isnull().sum()#So we have no missing data","b2dce3b6":"df.select_dtypes(\"object\")\n","519c9bf2":"\nfrom sklearn.preprocessing import StandardScaler\nX = df.values\nX = np.nan_to_num(X)\nX = StandardScaler().fit_transform(X)\nX","fb2e0dfc":"from sklearn.cluster import KMeans","57760ab3":"kmeans=KMeans(n_clusters=3)","b6057559":"kmeans.fit(X)","d601274a":"kmeans.cluster_centers_\n# Here are the cluster center vectors","4520e1f6":"labels=kmeans.labels_\nlabels\n#These are the labels that the algorithm has determined for 3 clusters","d8b075a6":"df[\"Labels\"]=labels\ndf.head()","dac259e6":"df.groupby(\"Labels\").mean()","1f7cfc32":"df.groupby(\"Labels\")[\"Income\"].plot(figsize=(20,10))\n#Here we get the plot of cluster according to their incomes","5525cf1f":"import matplotlib.pyplot as plt\nimport seaborn as sns","dc28f6e2":"plt.figure(figsize=(20,10))\nsns.scatterplot(x=\"Card Debt\", y=\"Income\", hue=\"Labels\",data=df)","a2a7021c":"from mpl_toolkits.mplot3d import Axes3D \nfig = plt.figure(1, figsize=(8, 6))\nplt.clf()\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n\nplt.cla()\n# plt.ylabel('Age', fontsize=18)\n# plt.xlabel('Income', fontsize=16)\n# plt.zlabel('Education', fontsize=16)\nax.set_xlabel('Education')\nax.set_ylabel('Age')\nax.set_zlabel('Income')\n\nax.scatter(X[:, 1], X[:, 0], X[:, 3], c= labels.astype(np.float))","eca615b4":"plt.figure(figsize=(20,10))\nplt.imshow(plt.imread(\"..\/input\/dendog\/unnamed-chunk-13-1.png\"))","3f373bf9":"df=pd.read_csv(\"https:\/\/cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud\/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork\/labs\/Module%204\/data\/cars_clus.csv\")\ndf.info()","94b769ba":"df.head()","b4abbe3d":"df[[ 'sales', 'resale', 'type', 'price', 'engine_s',\n       'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap',\n       'mpg', 'lnsales']] = df[['sales', 'resale', 'type', 'price', 'engine_s',\n       'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap',\n       'mpg', 'lnsales']].apply(pd.to_numeric, errors='coerce')","5c992a57":"df.select_dtypes(\"object\")\n#Now we have only two non numeric features","d9fe4acd":"df.isnull().sum()\n","4b3cf618":"df.dropna(inplace=True)\ndf.isnull().sum()","533bcbc2":"df.head()\n#we also need to reset index because it is not in order","b7cd1178":"df.reset_index(drop=True,inplace=True)","c610070f":"df.head()","a08f7d6c":"X=df.drop([\"manufact\",\"model\"],axis=1)\nX","2c61bd3b":"from sklearn.preprocessing import StandardScaler","e32cd07b":"scaler=StandardScaler()","855c3ff2":"X=scaler.fit_transform(X)\nX","b4bb8c05":"from sklearn.cluster import AgglomerativeClustering\n#We will agglomerative clustering","3f9ddc5d":"model=AgglomerativeClustering()\n#Hierarchical clustering does not require a pre-specified number of clusters","f4123bfd":"model.fit(X)","a4c1b4a7":"df[\"Label\"]=model.labels_\n\n#As we can see our data labeled as 2 distinct clusters and we add them into our data frame","7aefd6d0":"df.head()","68000714":"import matplotlib.cm as cm\nn_clusters = max(model.labels_)+1\ncolors = cm.rainbow(np.linspace(0, 1, n_clusters))\ncluster_labels = list(range(0, n_clusters))\n\n# Create a figure of size 6 inches by 4 inches.\nplt.figure(figsize=(16,14))\n\nfor color, label in zip(colors, cluster_labels):\n    subset = df[df.Label == label]\n    for i in subset.index:\n            plt.text(subset.horsepow[i], subset.mpg[i],str(subset['model'][i]), rotation=25) \n    plt.scatter(subset.horsepow, subset.mpg, s= subset.price*10, c=color, label='cluster'+str(label),alpha=0.5)\n#    plt.scatter(subset.horsepow, subset.mpg)\nplt.legend()\nplt.title('Clusters')\nplt.xlabel('horsepow')\nplt.ylabel('mpg')","27ddbe72":"<font color=\"blue\">\n    We can easily check the centroid values by averaging the features in each cluster as follows:","4562e753":"<font color=\"blue\">\n    There are only 150 missing values in a single column, we will fill them with the mean of this column","0708f4ac":"We can the distribution of each cluster using the scatter plot, but it is not very clear where is the centroid of each cluster. Moreover, there are 2 types of vehicles in our dataset, Label 0 represents \"truck\"  and and label 1 represents \"car\".","d5dc6bd2":"<font color=\"blue\">\n    We will assign the labels to each row in dataframe and add our data frame.","e9107199":"### Feature Selection and Normalization","4794030c":"<font color=\"blue\">\nDespite its simplicity, the **K-means** is vastly used for clustering in many data science applications, especially useful if you need to quickly discover insights from **unlabeled data**. In this notebook, you will learn how to use k-Means for customer segmentation.\n\nSome real-world applications of k-means:\n\n-   Customer segmentation\n-   Understand what the visitors of a website are trying to accomplish\n-   Pattern recognition\n-   Machine learning\n-   Data compression","574af63b":"We have nothing to do with manufact and model columns in our algorithm, so we will ignore them.","1bd42066":"<font color=\"blue\">\n    #There is no any string data, all of them are numerical and ready for the algorithm\n    \n    # However, we need to standardize our data in order to get better clustering","9e2fbbc7":"We see that there are 13 column that are seen as string although they are numerical, we need to transform them into numerical values.","d96fa451":"# 2. Hierarchical Clustering","44fa04f6":"#Here we get a list of how many null values the columns has, there are not many null values, so we can just drop these rows with null values","d9d4f766":"<h1 id=\"customer_segmentation_K_means\"> 1.1. Customer Segmentation with K-Means:<\/h1>","8440ffe4":"Lets imagine that an automobile manufacturer has developed prototypes for a new vehicle. Before introducing the new model into its range, the manufacturer wants to determine which existing vehicles on the market are most like the prototypes--that is, how vehicles can be grouped, which group is the most similar with the model, and therefore which models they will be competing against.\n\nOur objective here, is to use clustering methods, to find the most distinctive clusters of vehicles. It will summarize the existing vehicles and help manufacturers to make decision about the supply of new models.","9525ff78":"* The algorithm\n\n        -Choose a number of Clusters K\n        -Randomly assign each point to a specific cluster\n        -Until clusters stop changing, repeat the following steps:\n        -For each cluster, compute the cluster's centroid by taking the mean vector points in the cluster\n        -Assign each data point to the cluster for which the centroid is the closest","a204dd27":"<font color=\"blue\">\n    We can visualzie oor data and see the differences according to their labels after the segmentation by the KMeans algorithm as follows:","80aa71bc":"<font color=\"blue\">\nCustomer Id and Address columns are not really meaningful for discrete variables for clustering, so we will just drop them.","51ef9d2a":"# 2.1. Hierarchical Clustering on Vehicle Dataset","7a31de31":"# 1. K Means:","f0126e86":"<font color=\"blue\">\n    \n\nHierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. \n    \nStrategies for hierarchical clustering generally fall into two types:[1]\n\n1.Agglomerative: This is a \"bottom-up\" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\n    \n2.Divisive: This is a \"top-down\" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.\n    \nIn general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering[2] are usually presented in a dendrogram.","a7f8bab1":"<font color=\"blue\">\n\n\nIn the step 1 in the algorithm, each observation is randomly assigned to a cluster.\n\nIn the step 2a in the algorithm,the cluster centroid for each cluster is computed, which are shown as large colored disk as shown top-right of the figure.\n\nInitially these centroids are almost overlapping as we can see from the figure because initial cluster assignments are chosen randomly.\n\nIn the step 2a in the algorithm(bottom-left of the figure above), each observation is assigned to the nearest centroid.\n\nIn bottom-center of the figure above, step 2a once again is performed which lead to new cluster centroids.\n\nWe basically keep repeating these steps until there is no new cluster which means data points are being reassigned to a new cluster centroid.\n\nAt the bottom-right, we have the results obtained after about 10 iterations","30ad751b":"<font color=\"blue\">\n\n    *This algorithm allows us to cluster and label data as an unsupervised machine learning algorithm\n\n    *We apply this algorithm for unlabeled data and it attempts to group similar clusters in unlabeled data.\n\n    *The overall goal is to divide data into distinct groups such that observations within each group are similar","bf3cd653":"<font color=\"blue\">\nk-means will partition your customers into mutually exclusive groups, for example, into 3 clusters. The customers in each cluster are similar to each other demographically.\nNow we can create a profile for each group, considering the common characteristics of each cluster. \nFor example, the 3 clusters can be:\n\n-   AFFLUENT, EDUCATED AND OLD AGED\n-   MIDDLE AGED AND MIDDLE INCOME\n-   YOUNG AND LOW INCOME"}}