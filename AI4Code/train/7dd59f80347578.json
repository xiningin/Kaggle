{"cell_type":{"c329636f":"code","0fce9f69":"code","be78626f":"code","138d99fc":"code","7584d10b":"code","9ee5496c":"code","53911542":"code","e10cb52f":"code","15c1ef1d":"code","e08a6edf":"code","ac53e503":"code","791052e2":"code","e21c04f4":"code","f67b3992":"code","ec34acc9":"code","8bed26e8":"code","d1afd625":"code","c2266697":"code","fc06e637":"code","269d9c0b":"code","0a045efd":"code","92b9f363":"code","59655ab7":"code","d59bf1e0":"code","d5f77d5d":"code","344a2578":"code","9adc71c2":"code","630d927c":"code","ed4ec3cf":"code","89e7a5f5":"code","d86c9eff":"code","d285c28a":"code","ed06ea1e":"code","e9636ab8":"code","abf2b698":"code","94f07547":"code","5f9bfc31":"code","8ec98936":"markdown","b7c84a59":"markdown","931e4546":"markdown","e0a6c32b":"markdown","66b9b425":"markdown","a059d069":"markdown","92bb5d14":"markdown","3a27d3fd":"markdown","23da4bf3":"markdown","abee49d0":"markdown","d3c8ded5":"markdown","f25299c5":"markdown","79aaa7b3":"markdown","a008e0d0":"markdown"},"source":{"c329636f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.offline as py\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nos.listdir(\"..\/input\")","0fce9f69":"data = pd.read_csv('..\/input\/GrammarandProductReviews.csv')","be78626f":"data.head()","138d99fc":"data.shape","7584d10b":"data.info()","9ee5496c":"data.isnull().sum()","53911542":"data = data.dropna(subset = ['reviews.text'])","e10cb52f":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title=None):\n    wordcloud = WordCloud(\n    background_color = 'black',\n    stopwords=stopwords,\n    max_words=200,\n    max_font_size=40,\n    scale=3,\n    random_state=1 #choose at random\n    ).generate(str(data))\n    \n    fig = plt.figure(1, figsize=(15,15))\n    plt.axis('off')\n    if title:\n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top = 2.3)\n        \n    plt.imshow(wordcloud)\n    plt.show()\n    \nshow_wordcloud(data['reviews.text'])\n        ","15c1ef1d":"data['reviews.rating'].value_counts()\n","e08a6edf":"data['reviews_length'] = data['reviews.text'].apply(len)","ac53e503":"sns.set(font_scale = 2.0)\n\ng = sns.FacetGrid(data, col='reviews.rating', size=5)\ng.map(plt.hist, 'reviews_length')","791052e2":"data['reviews.didPurchase'].fillna('Review N\/A', inplace=True)","e21c04f4":"plt.figure(figsize=(10,8))\nax = sns.countplot(data['reviews.didPurchase'])\nax.set_xlabel(xlabel='Peoples Reviews', fontsize=17)\nax.set_ylabel(ylabel='No. of Reviews', fontsize=17)\nax.axes.set_title('Genuine No. of Reviews', fontsize=17)\nax.tick_params(labelsize=13)","f67b3992":"sns.set(font_scale=1.4)\nplt.figure(figsize = (10,5))\nsns.heatmap(data.corr(), cmap='coolwarm', annot=True, linewidth=.5)","ec34acc9":"from sklearn.model_selection import cross_val_score\nfrom scipy.sparse import hstack\nfrom sklearn.feature_extraction.text import TfidfVectorizer","8bed26e8":"all_text = data['reviews.text']\ntrain_text = data['reviews.text']\ny=data['reviews.rating']","d1afd625":"word_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1,1),\n    max_features=10000)\nword_vectorizer.fit(all_text)\ntrain_word_features = word_vectorizer.transform(train_text)\n","c2266697":"char_vectorizer = TfidfVectorizer(\n    sublinear_tf= True,\n    strip_accents='unicode',\n    analyzer='char',\n    stop_words='english',\n    ngram_range=(2,6),\n    max_features=50000)\nchar_vectorizer.fit(all_text)\ntrain_char_features = char_vectorizer.transform(train_text)\n\ntrain_features = hstack([train_char_features, train_word_features])","fc06e637":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\ntrain_features, y, test_size=0.3, random_state=101)","269d9c0b":"from sklearn.preprocessing import LabelEncoder\n# if any categorical data has available, convert to numerical\nEncoder = LabelEncoder()\ny_train = Encoder.fit_transform(y_train)\ny_test = Encoder.fit_transform(y_test)\n","0a045efd":"from sklearn.ensemble import RandomForestClassifier\n\nclassifier = RandomForestClassifier()\nclassifier.fit(X_train, y_train)\nr_pred = classifier.predict(X_test)","92b9f363":"from sklearn.metrics import accuracy_score\nrf_accuracy = accuracy_score(r_pred, y_test)","59655ab7":"print('random forest model accuracy is', rf_accuracy)","d59bf1e0":"from sklearn.svm import LinearSVC","d5f77d5d":"model = LinearSVC()","344a2578":"model.fit(X_train, y_train)","9adc71c2":"s_pred = model.predict(X_test)","630d927c":"svm_accuracy = accuracy_score(s_pred, y_test)","ed4ec3cf":"print('svm accuracy is', svm_accuracy)","89e7a5f5":"data['sentiment'] = data['reviews.rating']<4","d86c9eff":"from sklearn.model_selection import train_test_split\ntrain_text, test_text, train_y, test_y = train_test_split(\n    data['reviews.text'], data['sentiment'], test_size=0.2)","d285c28a":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.utils.np_utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import load_model \nfrom keras.optimizers import Adam","ed06ea1e":"MAX_NB_WORDS = 20000\n\n#get the text\ntexts_train = train_text.astype(str)\ntexts_test = test_text.astype(str)\n\n#vectorize sample into 2d int\ntokenizer = Tokenizer(nb_words = MAX_NB_WORDS,\n                     char_level=False)\ntokenizer.fit_on_texts(texts_train)\nsequences = tokenizer.texts_to_sequences(texts_train)\nsequences_test = tokenizer.texts_to_sequences(texts_test)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","e9636ab8":"MAX_SEQUENCE_LENGTH = 200\n#pad sequences are used to bring all sentences to same size\n#pad sequence with o's\nx_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\nx_test = pad_sequences(sequences_test, maxlen=\n                      MAX_SEQUENCE_LENGTH)\nprint('shape of data tensor:', x_train.shape)\nprint('shape of data test tensor:', x_test.shape)\n","abf2b698":"model = Sequential()\nmodel.add(Embedding(MAX_NB_WORDS, 128))\nmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, input_shape =(1,)))\nmodel.add(Dense(1, activation='sigmoid'))","94f07547":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","5f9bfc31":"model.fit(x_train, train_y,batch_size=64, epochs = 20, validation_data=(x_test, test_y))","8ec98936":"# Model Creation","b7c84a59":"# SVM","931e4546":"# NOW, lets have a look what do the length of the reviews tell about the ratings","e0a6c32b":"# Random Forest","66b9b425":"We will do one thing here, we will classify ratings<4 as sentiments, i.e. we will replace ratings less than 4 as not happy and vice-versa.\n\n1 for happy\n\n2 for unhappy\n\n","a059d069":"# to get equal size of all sentense","92bb5d14":"Since our prediction model will be beased on the text reviews, we will drop the rows having null value.","3a27d3fd":"# feature training","23da4bf3":"# Correlation matrix","abee49d0":"# What are the words that people have used the most in their reviews?","d3c8ded5":"# Deep Learning","f25299c5":"# So, What is the maximum no. of ratings that people gave?","79aaa7b3":"# using the n-gram tfidf vectorizer","a008e0d0":"# who all are giving fake reviews?"}}