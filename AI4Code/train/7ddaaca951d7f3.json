{"cell_type":{"71547903":"code","476ef6ce":"code","fc0541f0":"code","ebc82d68":"code","94b276cb":"code","6305c315":"code","ed892d02":"code","7f00047d":"markdown","3e765a02":"markdown","55830495":"markdown","4e7d9dd2":"markdown","8a05b398":"markdown","6e70fda2":"markdown","614b305c":"markdown","d7a44597":"markdown"},"source":{"71547903":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D","476ef6ce":"# number of observations (i.e. records,rows) of our sample data\nobservations = 1000\n\n# random vectors for Xs and Zs\nxs = np.random.uniform(low=-10,high=10, size=(observations,1))\nzs = np.random.uniform(-10,10,size=(observations,1))\n\n# stacking 2 random data columns together\ninputs = np.column_stack((xs,zs))\n\n# checking resulting column shape\nprint (\"Inputs matrix: \",inputs.shape)\nprint (\"Inputs matrix sample: \",inputs)\n\n# targets = f(x,z) = 2x + 3z + 5 + noise\n\nnoise = np.random.uniform(-1,1,(observations,1))\n\ntargets = 2*xs + 3*zs + 5 + noise\n\nprint (\"Targets matrix: \",targets.shape)\nprint (\"Targets matrix sample: \",targets[:5])","fc0541f0":"targets = targets.reshape(observations,)\nfig = plt.figure(figsize=(15,15))\nax = fig.add_subplot(111, projection='3d')\nax.plot(xs,zs,targets)\nax.set_xlabel('X-s')\nax.set_ylabel('Y-s')\nax.set_zlabel('Targets')\nax.view_init(azim=70)\nplt.show()\ntargets = targets.reshape(observations,1)","ebc82d68":"# initial range for weights and biases\ninit_range = 0.01\n\nweights = np.random.uniform(-init_range,init_range,size=(2,1))\nbiases = np.random.uniform(-init_range,init_range,size=1)","94b276cb":"# etas for performing grad descent\n#learning_rates = [0.1,]\nfig=plt.figure(figsize=(10,60))\n\n\nw = np.random.uniform(-init_range,init_range,size=(2,1))\nb = np.random.uniform(-init_range,init_range,size=1)\n\nlearning_rates = [round(i\/20,3) for i in range(1,7)][::-1]\n\nfor n,z in enumerate(learning_rates):\n    weights = w\n    biases = b\n    for i in range(100):\n        outputs = np.dot(inputs,weights) + biases\n        deltas = outputs - targets\n        loss = np.sum(deltas ** 2) \/ 2 \/ observations\n        deltas_scaled = deltas\/observations\n        weights = weights - z * np.dot(inputs.T,deltas_scaled)\n        biases = biases - z * np.sum(deltas_scaled)\n    ax=fig.add_subplot(len(learning_rates),1,n+1)\n    ax.plot(outputs,targets)\n    ax.set_title(\"Learning rate:\"+str(z))\n\n#print (weights,biases)\n#print (outputs,targets)\n\n        \n#ax = fig.add_subplot()\n#plt.plot(outputs,targets)\n","6305c315":"print (weights,biases)","ed892d02":"fig = plt.figure(figsize=(15,15))\nplt.plot (outputs,targets)\nplt.xlabel('Outputs')\nplt.ylabel('Targets')\nplt.show()","7f00047d":"## Found weights and biases","3e765a02":"# Plotting the diff","55830495":"# Visualizing random data ","4e7d9dd2":"## Loss function","8a05b398":"# Machine learning","6e70fda2":"# Imports","614b305c":"# Variables for weights and biases","d7a44597":"# Generaring random data"}}