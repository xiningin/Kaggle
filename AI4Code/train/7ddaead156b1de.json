{"cell_type":{"ab8e8eb2":"code","e383afb3":"code","eeb20336":"code","457182cc":"code","cdba3a5f":"code","c0687894":"code","28f8bd25":"code","e6c5bafd":"code","856de0dc":"code","9eba7b4a":"code","aac5e326":"code","6e5275f2":"code","ed927823":"code","cfe156a6":"code","d48664ee":"code","0a0595c0":"code","f7c9f316":"code","e44dae46":"code","fba6f722":"code","b6dadb26":"code","2130d39e":"code","e4a5277c":"code","14d7e3b4":"code","80b045ff":"code","c5552f82":"markdown","b4420734":"markdown","ab9a669e":"markdown","855fbf20":"markdown","119d0507":"markdown","0674f914":"markdown","419cd841":"markdown","2ceda602":"markdown","9ab17166":"markdown","36b21738":"markdown","4cf9dc17":"markdown"},"source":{"ab8e8eb2":"# Import dependencies \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \n%matplotlib inline\n\nimport os\nimport pathlib\nimport gc\nimport sys\nimport re\nimport math \nimport random\nimport time \nfrom tqdm import tqdm \nfrom pprint import pprint\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.model_selection import KFold \nfrom sklearn.model_selection import StratifiedKFold \nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import GridSearchCV\n\nimport xgboost as xgb\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.layers.experimental import preprocessing\n\nimport transformers \nimport datasets \n\nprint('import done!')","e383afb3":"# global config\nconfig = {'window_size': 7,\n          'batch_size': 8,\n          'valid_num': 200,\n          #'rnn_hidden': 128,\n          #'learning_rate': 5e-4,\n          'num_epochs': 20,\n          'max_trials': 20,\n         }\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n# For reproducible results    \ndef seed_all(s):\n    random.seed(s)\n    np.random.seed(s)\n    tf.random.set_seed(s)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n    os.environ['PYTHONHASHSEED'] = str(s) \n    print('Seeds setted!')\nglobal_seed = 42\nseed_all(global_seed)","eeb20336":"data_config = {'train_csv_path': '..\/input\/tabular-playground-series-jan-2022\/train.csv',\n              'test_csv_path': '..\/input\/tabular-playground-series-jan-2022\/test.csv',\n              'sample_submission_path': '..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv',\n              }\n\ntrain_df = pd.read_csv(data_config['train_csv_path'])\ntest_df = pd.read_csv(data_config['test_csv_path'])\nsubmission_df = pd.read_csv(data_config['sample_submission_path'])\n\nprint(train_df.shape, test_df.shape, submission_df.shape)\ntrain_df.head()","457182cc":"train_df['num_sold'].describe()","cdba3a5f":"print(len(train_df))\nprint()\ntrain_df.dtypes","c0687894":"def unique_category(df, column):\n    print(f'unique_category_number: {df[column].nunique()}')\n    print(f'cagetories: {df[column].unique()}')\n    print()\n\nunique_category(train_df, 'country')\nunique_category(train_df, 'store')\nunique_category(train_df, 'product')","28f8bd25":"train_df.isnull().sum()","e6c5bafd":"def date_features(df):\n    df['date'] = pd.to_datetime(df['date'])\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['day'] = df['date'].dt.day\n    df['dayofweek'] = df['date'].dt.dayofweek\n    return df \n\ntrain_df = date_features(train_df)\ntrain_df = train_df.drop(['date', 'year'], axis=1)\n\ntest_df = date_features(test_df)\ntest_df = test_df.drop(['date', 'year'], axis=1)\n\ntrain_df.head()","856de0dc":"train_df.head(20)","9eba7b4a":"feature_num = len(train_df['country'].unique()) * len(train_df['store'].unique()) * len(train_df['product'].unique()) # 18\nseries_data_num = int(len(train_df) \/ feature_num) # 1461\n\nseries_features = []\nfor i in range(series_data_num):\n    feature = list(train_df['num_sold'][i* feature_num : (i+1) * feature_num])\n    series_features.append(feature)\n\nseries_columns = []\nfor country in train_df['country'].unique():\n    for store in train_df['store'].unique():\n        for product in train_df['product'].unique():\n            name = f'{country}_{store}_{product}'\n            series_columns.append(name)\n            \nseries_df = pd.DataFrame(series_features, columns=series_columns)\nprint(len(series_df))\n\nseries_df.head()","aac5e326":"series_df.describe()","6e5275f2":"train_series_df = series_df[:-200].copy()\nvalid_series_df = series_df[-200:].copy()\n\nprint(len(train_series_df), len(valid_series_df))","ed927823":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit(train_series_df)\nprint(sc.mean_, sc.scale_)\n\ntrain_series_df = pd.DataFrame(sc.transform(train_series_df), columns=series_columns)\nvalid_series_df = pd.DataFrame(sc.transform(valid_series_df), columns=series_columns)\nall_series_df = pd.DataFrame(sc.transform(series_df), columns=series_columns)\nprint(len(train_series_df), len(valid_series_df), len(all_series_df))\n\ntrain_series_df.head(10)","cfe156a6":"window_size = config['window_size']\n\ntrain_X = []\ntrain_y = []\n\nfor i in range(len(train_series_df) - window_size):\n    tmp_X = np.array(train_series_df.iloc[i:(i+window_size)])\n    tmp_y = np.array(train_series_df.iloc[(i+1):(i+window_size+1)])\n    train_X.append(tmp_X)\n    train_y.append(tmp_y)\n\ntrain_X = tf.constant(train_X, dtype=tf.float32)\ntrain_y = tf.constant(train_y, dtype=tf.float32)\nprint(train_X.shape, train_y.shape)","d48664ee":"valid_X = []\nvalid_y = []\nfor i in range(len(valid_series_df) - window_size):\n    tmp_X = np.array(valid_series_df.iloc[i:(i+window_size)])\n    tmp_y = np.array(valid_series_df.iloc[(i+1):(i+window_size+1)])\n    valid_X.append(tmp_X)\n    valid_y.append(tmp_y)\nvalid_X = tf.constant(valid_X, dtype=tf.float32)\nvalid_y = tf.constant(valid_y, dtype=tf.float32)\nprint(valid_X.shape, valid_y.shape)\n\nall_X = []\nall_y = []\nfor i in range(len(all_series_df) - window_size):\n    tmp_X = np.array(all_series_df.iloc[i:(i+window_size)])\n    tmp_y = np.array(all_series_df.iloc[(i+1):(i+window_size+1)])\n    all_X.append(tmp_X)\n    all_y.append(tmp_y)\nall_X = tf.constant(all_X, dtype=tf.float32)\nall_y = tf.constant(all_y, dtype=tf.float32)\nprint(all_X.shape, all_y.shape)","0a0595c0":"train_X_ds = tf.data.Dataset.from_tensor_slices(train_X)\ntrain_y_ds = tf.data.Dataset.from_tensor_slices(train_y)\ntrain_ds = tf.data.Dataset.zip((train_X_ds, train_y_ds))\nprint(train_ds, len(train_ds) )\n\nvalid_X_ds = tf.data.Dataset.from_tensor_slices(valid_X)\nvalid_y_ds = tf.data.Dataset.from_tensor_slices(valid_y)\nvalid_ds = tf.data.Dataset.zip((valid_X_ds, valid_y_ds))\nprint(valid_ds, len(valid_ds))\n\nall_X_ds = tf.data.Dataset.from_tensor_slices(all_X)\nall_y_ds = tf.data.Dataset.from_tensor_slices(all_y)\nall_ds = tf.data.Dataset.zip((all_X_ds, all_y_ds))\nprint(all_ds, len(all_ds))","f7c9f316":"BATCH_SIZE = config['batch_size']\n\ntrain_ds = train_ds.batch(BATCH_SIZE)\ntrain_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\nprint(train_ds)\n\nvalid_ds = valid_ds.batch(BATCH_SIZE)\nvalid_ds = valid_ds.prefetch(buffer_size=AUTOTUNE)\nprint(valid_ds)\n\nall_ds = all_ds.batch(BATCH_SIZE)\nall_ds = all_ds.prefetch(buffer_size=AUTOTUNE)\nprint(all_ds)","e44dae46":"import kerastuner as kt\n\nNUM_TRAIN_STEPS = len(train_ds) * config['batch_size'] * config['num_epochs']\n\ndef build_model(hp):\n    \n    feature_num = len(train_series_df.columns)\n\n    hp_rnn_hidden = hp.Int('rnn_hidden', min_value=16, max_value=256, step=16)\n    hp_dense_hidden = hp.Int('dense_hidden', min_value=16, max_value=256, step=16)\n    hp_activation = hp.Choice('activation', values=['selu', 'relu', 'tanh'])\n\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.GRU(hp_rnn_hidden, return_sequences=True, input_shape=[None, feature_num]),\n        tf.keras.layers.GRU(hp_rnn_hidden, return_sequences=True),\n        tf.keras.layers.Dense(hp_dense_hidden, activation=hp_activation),\n        tf.keras.layers.Dense(18)\n        ])\n    \n    hp_initial_learning_rate = hp.Float('initial_learning_rate', 1e-4, 1e-3, sampling='log')\n    lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n        initial_learning_rate=hp_initial_learning_rate,\n        end_learning_rate=1e-5,\n        decay_steps=NUM_TRAIN_STEPS)\n    \n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_scheduler),\n              loss=tf.keras.losses.MeanSquaredError(),\n              )\n  \n    return model\n\n\ntuner = kt.BayesianOptimization(\n    build_model,\n    objective='val_loss',\n    max_trials=config['max_trials'],\n    directory = 'hp_tuning',\n    project_name = 'ex_no_1',\n)","fba6f722":"tuner.search(train_ds, epochs=config['num_epochs'], validation_data=valid_ds)","b6dadb26":"tuner.results_summary(num_trials=3)","2130d39e":"best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\nmodel = tuner.hypermodel.build(best_hps)\nmodel.summary()","e4a5277c":"NUM_TRAIN_STEPS = len(all_ds) * config['batch_size'] * config['num_epochs']\nmodel.fit(all_ds, epochs=config['num_epochs'])","14d7e3b4":"def prediction(model, prediction_num):\n    input_for_predict = tf.constant(series_df[-window_size:], dtype=tf.float32)\n    input_for_predict = tf.expand_dims(input_for_predict, 0)\n    predictions = []\n\n    for i in range(prediction_num):\n        pred = model(input_for_predict) # TensorShape([1, window_size, 18])\n        pred = pred[:, -1, :] # TensorShape([1, 18])\n        predictions.append(pred)\n        \n        pred = tf.expand_dims(pred, 0)\n        input_for_predict = input_for_predict[:, 1:, :]\n        input_for_predict = tf.concat([input_for_predict, pred], axis=1)\n\n    return np.array(predictions)","80b045ff":"predictions = prediction(model, int(len(test_df)\/feature_num) )\npredictions = np.squeeze(predictions, axis=1)\n\npred_num_sold = (predictions * sc.scale_) + sc.mean_\npred_num_sold = pred_num_sold.ravel()\n\nsubmission_df['num_sold'] = pred_num_sold\nsubmission_df.to_csv(\"submission.csv\", index=False)\nsubmission_df.head()","c5552f82":"## 3.1 Hyperparameter Tuning with keras-tuner","b4420734":"---\n# [Tabular Playground Series - Jan 2022][1]\n---\n\n---\n[1]: https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022","ab9a669e":"## 1.3 Feature Engneering -2","855fbf20":"# 1. Data Preprocessing","119d0507":"## 3.2 Model Training with Best Parameters","0674f914":"# 3. Prediction and Submission","419cd841":"## 1.1 Data Check","2ceda602":"# 0. Settings","9ab17166":"# 3. Model Training","36b21738":"## 1.2 Feature Engneering -1","4cf9dc17":"# 2. Dataset"}}