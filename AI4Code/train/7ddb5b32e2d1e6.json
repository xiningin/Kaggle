{"cell_type":{"e6ad3571":"code","973c8780":"code","1fb58b71":"code","794322c6":"code","ac84159a":"code","ef5dbc5d":"code","b0a51d42":"code","3d741dc7":"code","01c6e50d":"code","ee80db90":"code","68af3cac":"code","53d0e074":"code","11e7a909":"code","16278eb2":"code","158332e6":"code","cb313804":"code","6c5f7fbb":"code","eac79419":"code","16930a48":"code","5e2126f0":"code","fa614248":"code","0b6d2fa2":"code","bac8e7a4":"code","e63a427b":"code","757fddea":"code","4bdc46a2":"code","0852d3d5":"code","9c45ab39":"code","7b9443ec":"code","6d02a846":"code","f288c98e":"code","eb1c8691":"code","c3f8c382":"markdown","7e73e7e9":"markdown","aa24359f":"markdown","45f9f890":"markdown","c7004e91":"markdown","f80c7b75":"markdown","ba8a6c23":"markdown","35c377b1":"markdown","d99068ec":"markdown","fd7dd2bc":"markdown","addf2991":"markdown","3008049e":"markdown","d6d19bce":"markdown","331782b0":"markdown","f7655207":"markdown","1019181a":"markdown","6c14da2c":"markdown","b41917c0":"markdown","54ca2ad9":"markdown"},"source":{"e6ad3571":"\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","973c8780":"#import libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.feature_selection import RFE\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import IncrementalPCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nimport random\nfrom sklearn import metrics\nfrom imblearn.over_sampling import SMOTE","1fb58b71":"train = pd.read_csv('\/kaggle\/input\/santander-customer-satisfaction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/santander-customer-satisfaction\/test.csv')","794322c6":"train.head()","ac84159a":"train.shape","ef5dbc5d":"#check null values\nnp.isnan(train).sum()","b0a51d42":"train['imp_ent_var16_ult1'].value_counts()","3d741dc7":"#-----------------------------------------Explore Data------------------------------------------\n#line number\nNb_clients=train.TARGET.count()\nprint (Nb_clients)","01c6e50d":"#In TARGET column: 0 means happy, 1 means unhappy\n#Distribution of Customer Satisfaction\ndf = train.TARGET.value_counts(1)\ndf","ee80db90":"#show distribution in Pie chart (just for fun)\nrate=[df[0],df[1]]\nlabels = ['happy', 'unhappy']\ncolors = ['blue','orange']\n\nplt.pie(rate, labels=labels, autopct='%1.2f%%', colors=colors)\nplt.show()\n#unbalanced positive and negative samples","68af3cac":"# Checking outliers at 25%,50%,75%,90%,95% and 99%\ntrain.describe(percentiles=[.25,.5,.75,.90,.95, .975,.99,.999])","53d0e074":"high = .99\nfirst_quartile = 0.25\nthird_quartile = 0.75\nquant_df = train.quantile([high, first_quartile, third_quartile])\nquant_df","11e7a909":"train_df = train.drop(['ID', 'TARGET'], axis = 1)\n","16278eb2":"train_df = train_df.apply(lambda x: x[(x <= quant_df.loc[high,x.name])], axis=0)\ntrain_df.describe(include='all')","158332e6":"train_df.shape","cb313804":"train_df.head()","6c5f7fbb":"train_df = pd.concat([train.loc[:,'ID'], train_df], axis=1)\n\ntrain_df = pd.concat([train.loc[:,'TARGET'], train_df], axis=1)\ntrain_df.describe()","eac79419":"train_df.isnull().sum().sort_values(ascending=False)\n","16930a48":"import random\nnew_train_df = train_df\nfor col in new_train_df.columns:\n    min_val = min(new_train_df[col])\n    max_val = max(new_train_df[col])\n    new_train_df[col].fillna(round(random.uniform(min_val, max_val), 2), inplace =True)","5e2126f0":"new_train_df.isna().sum().sort_values(ascending=False)\n","fa614248":"y = new_train_df['TARGET']\nX = new_train_df.drop(['TARGET','ID'], axis=1)","0b6d2fa2":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99)","bac8e7a4":"lr = LogisticRegression()\nlr.fit(X_train,y_train)","e63a427b":"preds = lr.predict(X_test)\nprint(\"Accuracy with Logistic = \", metrics.accuracy_score(y_test, preds))","757fddea":"new_train_df['TARGET'].value_counts()\n","4bdc46a2":"sm = SMOTE()\nX_tr,y_tr = sm.fit_sample(X_train,y_train)\nprint(X_tr.shape)\nprint(y_tr.shape)","0852d3d5":"lr.fit(X_tr,y_tr)\n\nlr_preds = lr.predict(X_test)\nprint(\"Accuracy with Logistic = \", metrics.accuracy_score(y_test, lr_preds))","9c45ab39":"dt1 = DecisionTreeClassifier(max_depth=5)\ndt1.fit(X_tr, y_tr)\n\ndt_preds = dt1.predict(X_test)\nprint(\"Accuracy with Decision Tree = \", metrics.accuracy_score(y_test, dt_preds))","7b9443ec":"rft = RandomForestClassifier(n_jobs=-1)\nrft.fit(X_tr, y_tr)\n\nrft_preds = rft.predict(X_test)\nprint(\"Accuracy with Random Forest = \", metrics.accuracy_score(y_test, rft_preds))","6d02a846":"x_test_final = test.drop(['ID'], axis=1)\nfinal_prediction = rft.predict(x_test_final)\nsubmission = pd.DataFrame({\n        \"ID\": test[\"ID\"],\n        \"TARGET\": final_prediction\n    })\nsubmission.to_csv('RandomForect.csv',header=True, index=False)","f288c98e":"final=pd.read_csv('RandomForect.csv')","eb1c8691":"final.head()","c3f8c382":"**------------------------------Data Preparation-----------------------------------------------------------**","7e73e7e9":"You are provided with an anonymized dataset containing a large number of numeric variables. The \"TARGET\" column is the variable to predict. It equals one for unsatisfied customers and 0 for satisfied customers.\n\nThe task is to predict the probability that each customer in the test set is an unsatisfied customer.\n\n**File descriptions**\n\n    - train.csv - the training set including the target\n    - test.csv - the test set without the target\n    - sample_submission.csv - a sample submission file in the correct format","aa24359f":"We have got multiple models with all their accuracies. We can choose anyone of them to make further predictions.\n\nAs RFT has maximum accuracy lets use that.","45f9f890":"Now we can see our dataset is free of null values :)\n\nNow lets go to model testing","c7004e91":"**Handling null values**\n\nTo handle null values we will take a random value between minimum and maximum value of each column and use that random value for imputation\n\n","f80c7b75":"**Final Prediction**","ba8a6c23":"By analysing this visually it seems that ranges 0.99, 0.25, 0.75 seems important. Lets save their value somewhere","35c377b1":"**Model Building**","d99068ec":"So we can see that almost 96% of dataset is having value 0. We need to balance this dataset to get better results|","fd7dd2bc":"**Dataset balancing\nWe will use SMOTE technique to balance the dataset**","addf2991":"Yes there it is . We can clearly see there is only 1 value of 17595.15 which definitely serve as an outlier. To clean this dataset outlier ommission is needed.\n","3008049e":"We can see that we didnt have that much data loss and also our data is now free of outliers.\n\nNow lets get back our ID and TARGET columns","d6d19bce":"Lets take 99% as threshold for outlier. Drop all values above 0.99 percentile\u00b6\n","331782b0":"**RandomForestClassifier**","f7655207":"There are no null values present either\n\nLets check one of the column to confirm our theory about outliers","1019181a":"**> Outlier handling**\n\nFirst we will see the values of different quartiles","6c14da2c":"**DecisionTreeClassifier**","b41917c0":"Now we have our quartile dataframe\n\nLets now prepare and clean our training dataset.\n\nFirst we need to remove TARGET and ID column from our training dataset.","54ca2ad9":"Lets now fit the data\n\n**Logistic Regression**"}}