{"cell_type":{"a940e723":"code","8adc0443":"code","a309b89d":"code","bcb7d86a":"code","8f21583b":"code","b25e3490":"code","c6db82cf":"code","90614be2":"code","d513f1a3":"code","1fe7cd1e":"code","1d5737a8":"code","985cd230":"code","852e80b9":"code","313d60be":"code","db21ff78":"code","5b29f257":"code","71daa04a":"code","03a5a2be":"code","8bd38be7":"code","9b800aa1":"code","154c1d03":"code","245bd7e0":"code","3b95f196":"code","48a78462":"code","c7dca194":"code","ec81db9d":"code","2135a0f5":"code","ccb2b6eb":"code","81959049":"code","01205b80":"code","37f2a187":"code","b375ba78":"code","f554df6f":"code","aa023854":"code","ffe42e7d":"code","cd2ba26f":"code","0c2dbfcf":"code","e67d9547":"code","2ecc67dd":"code","c53c7cc3":"markdown","f7202165":"markdown","9e6c3573":"markdown","da558770":"markdown","c8134b22":"markdown","f5bdb744":"markdown","01d2e553":"markdown","7334a055":"markdown","00a3cc26":"markdown","d6a73ddc":"markdown","4f70f0fd":"markdown","bca2e941":"markdown","802166db":"markdown"},"source":{"a940e723":"# Import libraryes\nimport numpy as np # For math operations\nimport pandas as pd # For the import data\nfrom pandas.plotting import scatter_matrix\nimport matplotlib as mp\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# For the scaling data\n\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, LogisticRegression","8adc0443":"data = pd.read_csv('..\/input\/dissolved-oxygen-prediction-in-river-water\/train.csv')","a309b89d":"data.head(10)","bcb7d86a":"data.info()","8f21583b":"data.isna().sum()","b25e3490":"null_columns = list(data.columns[data.isna().sum() > 100])\ndata.drop(null_columns, axis=1, inplace=True)","c6db82cf":"data","90614be2":"data.isna().sum()","d513f1a3":"print(\"Columns with missing values:\", (data.isna().sum(axis=0) != 0).sum())\nprint(\"Rows with missing values:\", (data.isna().sum(axis=1) != 0).sum())","1fe7cd1e":"data.dropna(axis=0, inplace=True)","1d5737a8":"data.isna().sum().sum()","985cd230":"data","852e80b9":"data.describe()","313d60be":"d1 = data[['target',\"O2_1\", \"O2_2\"]]\nd1","db21ff78":"d1.plot(kind='box')","5b29f257":"scatter_matrix(d1, figsize=(10,7))","71daa04a":"a1 = sns.distplot(d1)","03a5a2be":"d1[['O2_1', 'O2_2']].plot(kind=\"line\", figsize=(8, 4))","8bd38be7":"d1.describe()","9b800aa1":"d2 = d1.replace(d1[\"O2_1\"].max(), value=d1[\"O2_1\"].mean())\nd3 = d2.replace(d1[\"O2_2\"].max(), value=d1[\"O2_2\"].mean())\n\nd3[['target','O2_1' ,'O2_2']].plot(kind=\"line\", figsize=(7,4))","154c1d03":"a = sns.distplot(d3)","245bd7e0":"X = d1.drop('target', axis=1)\ny = d1[['target']]","3b95f196":"x_train, x_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=35)\nX_train = preprocessing.StandardScaler().fit(x_train).transform(x_train)\nX_test = preprocessing.StandardScaler().fit(x_test).transform(x_test)\n\n\n## change to 1d array\ny_train = np.array(y_train)\ny_train = y_train.ravel()\n\ny_test = np.array(y_test)\ny_test = y_test.ravel()","48a78462":"from sklearn.metrics import mean_squared_error\nfrom sklearn import linear_model","c7dca194":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)","ec81db9d":"def model_result(m_odel):\n    m = m_odel\n    m.fit(X_train, y_train)\n    print('R2 model score:  ', m.score(X_test, y_test))\n    print('RMSE    :  ', np.sqrt(mean_squared_error(m.predict(X_test), y_test)))","2135a0f5":"#testing ridge regression\nfrom sklearn.linear_model import Ridge\nmodel_result(m_odel=Ridge(alpha = 1, random_state = 42))","ccb2b6eb":"#testing Lasso regression\nfrom sklearn.linear_model import Lasso\nmodel_result(m_odel=Lasso(alpha = 1, random_state = 42))","81959049":"#polynomial reg\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\n\nX_poly = poly_features.fit_transform(X)\n\nXp_train, Xp_test, yp_train, yp_test = train_test_split(X_poly, y, test_size=0.2, random_state=35)\n\nyp_train = np.array(yp_train)\nyp_train = yp_train.ravel()\n\nyp_test = np.array(yp_test)\nyp_test = yp_test.ravel()\ndef model_resultp(m_odelp):\n    mp = m_odelp\n    mp.fit(Xp_train, yp_train)\n    print('R2 model score =',mp.score(Xp_test, yp_test))\n    print('RMSE =',np.sqrt(mean_squared_error(mp.predict(Xp_test), yp_test)))\nmodel_resultp(m_odelp=linear_model.LinearRegression(fit_intercept=True))","01205b80":"##support vector regressor\n\nfrom sklearn.svm import LinearSVR\n\nsvm_reg = LinearSVR(epsilon=2.5543, random_state=42)\nsvm_reg.fit(X_train, y_train)\nsvm_reg.score(X_test, y_test)","37f2a187":"\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly_features.fit_transform(X)\nXp_train, Xp_test, yp_train, yp_test = train_test_split(X_poly, y, test_size=0.2, random_state=35)\n\nmy_model = Lasso(alpha=0.25455422642263903, copy_X=True, fit_intercept=True, max_iter=1000,\n      normalize=False, positive=False, precompute=False, random_state=None,\n      selection='cyclic', tol=0.0001, warm_start=False)\n\nmy_model.fit(Xp_train, yp_train)\nmy_model.score(Xp_test, yp_test)","b375ba78":"#cross validation on polynomial features\n\nmodelsT = [Ridge, Lasso]\nmodel_names = ['ridge', 'lasso']\n\nfor x in range(len(modelsT)):\n    print(model_names[x])\n    \n    param_grid = {'alpha' : np.logspace(-1,0.009,2500),\n                  'max_iter' : [1000]}\n    lin_model  = modelsT[x]() \n    model_cv   = GridSearchCV(estimator  = lin_model, \n                        param_grid = [param_grid],\n                        cv = 5,\n                        scoring='neg_mean_squared_error', \n                        n_jobs = -1,\n                        verbose = 1)\n    model_cv.fit(Xp_train, yp_train)\n\n    best_model              = model_cv.best_estimator_\n    print(best_model)\n    bestmodelFitTime        = model_cv.cv_results_['mean_fit_time'][model_cv.best_index_]\n    bestmodelScoreTime      = model_cv.cv_results_['mean_score_time'][model_cv.best_index_]\n    best_model.fit(Xp_train, yp_train)\n    print('R2 score: ', best_model.score(Xp_test, yp_test))\n\n    \n    y_pred = best_model.predict(Xp_test)\n    rmse   = np.sqrt(mean_squared_error(y_pred, yp_test))\n    print('Test RMSE : ', rmse)\n    print(\"**********************************\")","f554df6f":"my_model = Lasso(alpha=0.25455422642263903, copy_X=True, fit_intercept=True, max_iter=1000,\n      normalize=False, positive=False, precompute=False, random_state=None,\n      selection='cyclic', tol=0.0001, warm_start=False)\n\nmy_model.fit(Xp_train, yp_train)\nmy_model.score(Xp_test, yp_test)","aa023854":"my_model.predict(Xp_test)","ffe42e7d":"Test = pd.read_csv(\"..\/input\/dissolved-oxygen-prediction-in-river-water\/test.csv\")\nTest","cd2ba26f":"Test.describe()","0c2dbfcf":"Test = Test.replace(Test[\"O2_2\"].max(), value=Test[\"O2_2\"].mean())\nTest_X = Test[[\"O2_1\", \"O2_2\"]]\nTest_X","e67d9547":"poly_features_X = PolynomialFeatures(degree=2, include_bias=False)\n\nTes_X_poly = poly_features_X.fit_transform(Test_X)\n","2ecc67dd":"my_model.predict(Tes_X_poly)","c53c7cc3":"**Conclusion**","f7202165":"# **Data Modeling**","9e6c3573":"# **Predicting the target station from test data**","da558770":"**To make the analysis more accurate, delete all columns with an empty data set**","c8134b22":"# **Data Visualization**","f5bdb744":"**To create a better picture of events, create a line graph**","01d2e553":"The provided data set contained a large number of gaps, as a result of which the model was built on two function sets. Stations 1 and 2. The best indicator that was made as a result of the analysis - 72%\n","7334a055":"**Vector Model**","00a3cc26":"**Regression**","d6a73ddc":"As a result of the received information, it is possible to draw a conclusion that oxygen concentration makes about 46,95 and 40,9. Because this statement is highly questionable, it may be erroneous. For accuracy, this data will be replaced by the average value of the corresponding columns\n","4f70f0fd":"The best performing model turned out to be the Lasso regression with alpha = 0.254. An accuracy of 72.69% was achieved!","bca2e941":"# **Data Exploration**","802166db":"**Due to the fact that we work only with oxygen concentration, we can create our own data sample, which will include columns \"target\", \"O2_1\" and \"O2_2\"**"}}