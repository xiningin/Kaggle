{"cell_type":{"09637444":"code","034bcac5":"code","1651f31f":"code","58231cdb":"code","380234de":"code","8630aa5d":"code","499a9dec":"code","ad10ea3e":"code","a9df637a":"code","27c1fb85":"code","022fbb45":"code","fff4fb70":"code","253bace8":"code","cfb3dd48":"code","64d5e3fc":"code","f5af41f8":"code","14f8daa6":"code","7a1820b8":"code","ec0daaa0":"code","96726532":"code","95fecc1a":"code","47fdbb0a":"code","4c8e8209":"code","a4411888":"code","8491faa2":"code","291bbfa3":"code","5a48572a":"code","fa2c1d3e":"code","0e86ef4f":"code","72a73706":"code","d2790497":"code","69568ff4":"code","e0f7afc4":"code","e4597ced":"code","b41e655d":"code","788e88a0":"code","13296957":"code","2576a8c9":"code","1be9d1fe":"code","4239f958":"code","37f4a890":"code","c3839a24":"code","b48e0221":"code","4513b9b1":"code","22c9e4d1":"code","b976720f":"code","b64c5c01":"code","c4548fe3":"code","8b87c2da":"code","88208a81":"code","bfbc62b4":"code","def24084":"code","ba3afdb3":"code","1b2fc17b":"code","b47e688c":"code","359caef5":"code","0bb701d6":"code","945610c8":"code","c71545ac":"code","513a168c":"code","e897433e":"code","8a3b3fa0":"code","22596529":"code","3c5297ec":"code","7801a0a6":"code","d544a5f4":"code","8934f439":"code","bd4cab47":"code","29973855":"code","5f88014f":"code","731fbc2e":"markdown","7caae78c":"markdown","4b7f3803":"markdown","40f07752":"markdown","8326d418":"markdown","8ade47c2":"markdown","886a962e":"markdown","84cfd689":"markdown","912c8311":"markdown","38e34b33":"markdown","4dfbe49d":"markdown","96d04e6d":"markdown","2c9609e6":"markdown","287ce207":"markdown","2fac2862":"markdown","9e0a019d":"markdown","7bc12e23":"markdown"},"source":{"09637444":"!pip install sidetable","034bcac5":"!pip install --upgrade seaborn ","1651f31f":"# import libraries..................\n# TOOLS \nimport warnings\nimport numpy as np\nimport pandas as pd\nimport sidetable \nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom IPython.core.interactiveshell import InteractiveShell\nfrom sklearn.impute import SimpleImputer \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score, RandomizedSearchCV, GridSearchCV\nfrom sklearn.model_selection import KFold \nfrom sklearn.preprocessing import Normalizer \nfrom sklearn.metrics import precision_score, recall_score, f1_score \nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# MODELS \nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier \nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.ensemble import BaggingClassifier,VotingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier","58231cdb":"print(\"seaborn\",sns.__version__)\nprint(\"pandas\", pd.__version__)\nprint(\"numpy\", np.__version__)\n\n","380234de":"# setting notebook...............\nwarnings.simplefilter('always')\n# This class makes notebook to produce many output\n# from different code in one notebook \nInteractiveShell.ast_node_interactivity = \"all\"\n# changing plotting style\nsns.set_style(\"whitegrid\")\n# configuring size of graphs \nplt.rcParams[\"figure.figsize\"]=[13, 6]","8630aa5d":"# loading data in the notebook...........\n# data input is stored in variable: train_fileinput, test_fileinput\ntest_fileinput = \"\/kaggle\/input\/titanic\/test.csv\"\ntrain_fileinput = \"\/kaggle\/input\/titanic\/train.csv\"\n\n# inserting in both train_fileinput and test_fileinput in dataframe: train_df, test_df\n# setting the PassengerId column as index column \ntrain_df = pd.read_csv(train_fileinput, index_col=\"PassengerId\")\ntest_val = pd.read_csv(test_fileinput, index_col=\"PassengerId\")\n\n# making copy from data to provide change data\ntrain_df = train_df.copy(deep=True)\n\n\n# combine data in the list for easy cleaning of \n# both train and test data \ndatasets = [train_df, test_val]","499a9dec":"print(\"First Five Row Of Train Data...........\")\ntrain_df.head()\nprint(\"First Five Row Of Test Data...........\")\ntest_val.head()","ad10ea3e":"print(\"Description Of Train Data.........\")\ntrain_df.describe()\nprint(\"Description Of Test Data..........\")\ntest_val.describe()","a9df637a":"print(\"Information Of Train Data..........\")\nprint()\ntrain_df.info()\nprint()\nprint(\"Information Of Teat Data...........\")\nprint()\ntest_val.info()","27c1fb85":"train_df.stb.missing ()","022fbb45":"sns.heatmap(train_df.isnull(), cbar=False).set_title(\"Virtual Missing data in Train data\");","fff4fb70":"test_val.stb.missing()","253bace8":"sns.heatmap(test_val.isnull(), cbar=False).set_title(\"Virtual Missing data in Test data\");","cfb3dd48":"# loop through datasets using for loop\nfor data in datasets:\n    # filling in missing values in age column with median\n    data[\"Age\"].fillna(data[\"Age\"].median(), inplace=True)\n    \n    # using mode to fill in missing value in embarked\n    data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0], inplace=True)\n    \n    # impute fare column using mean\n    data[\"Fare\"].fillna(data[\"Fare\"].mean(), inplace=True)","64d5e3fc":"print(\"train_df Nan..........\")\ndatasets[0].isnull().sum()\nprint(\"test_df Nan...........\")\ndatasets[1].isnull().sum()","f5af41f8":"# Since Cabin column contain too many Nan,\n# i will new column(Deck) then i will drop it \ndef deck(column):\n    my_list = []\n    for i in column:\n        if type(i) == str:\n            my_list.append(list(i)[0])\n        else:\n            my_list.append('N')\n    return my_list\n\nfor data in datasets:\n    # assigning Deck column to train, test........\n    data[\"Deck\"] = deck(data.Cabin)\n    data[\"Deck\"] = deck(data.Cabin)\n    \n# dropping useless column.................\nfor d in range(2):\n    # drop useless column.............\n    # like this column. Cabin, which is filled with\n    # alot of nan value had to be dropped \n    datasets[d] = datasets[d].drop(\"Cabin\", axis=1)\n    \n    datasets[d] = datasets[d].drop(\"Ticket\", axis=1)\n    # creating Eighteen_plus\n    datasets[d][\"Eighteen_plus\"] = datasets[d].Age >= 18\n\nprint(\"train data ________\")\ndatasets[0]\nprint(\"test data _________\")\ndatasets[1]","14f8daa6":"# loop through datasets list \nfor data in datasets:\n    #  The list contain title from names: title\n    title = [name.split()[1].rstrip(\".\") \n             for name in data[\"Name\"]]\n    # assigning Title to the data\n    data[\"Title\"] = title \n\ndef low_nunique(data):\n    # changing title column to python list for easy handling \n    title_col = list(datasets[data][\"Title\"])\n    lst = []\n    # checking if number of unique is less than stat_min variable\n    for title_name in title_col:\n        if title_col.count(title_name) < 10:\n            lst.append (\"Rare\")\n        else:\n            lst.append (title_name)\n    return np.array(lst)\n\n# loop in range for 0 to 1 and used in num pick\n# in data from datasets list\nfor num in range(2):\n    # assigning configured title from low_nunique function\n    datasets[num][\"Title\"] = low_nunique(num)\n    # dropping name column \n    datasets[num].drop([\"Name\"], axis=1, inplace=True)\n    \n    \nprint(\"train data _________\")\ndatasets[0][\"Title\"].value_counts()\ndatasets[0].info()\nprint(\"test data ___________\")\ndatasets[1][\"Title\"].value_counts()\ndatasets[1].info()\n","7a1820b8":"# new features in data: FamilySize, Agebin, Farebin, Alone\n# loop through datasets\nfor data in datasets:\n    data[\"FamilySize\"] = data[\"SibSp\"] + data[\"Parch\"]\n    data[\"Alone\"] = (data[\"SibSp\"] + data[\"Parch\"]) < 1 \n    data[\"Farebin\"] = pd.qcut(data[\"Fare\"], 5)\n    data[\"Agebin\"] = pd.cut(data[\"Age\"], 4) \nprint(\"Train data________\")  \ndatasets[0].sample(10)\nprint(\"Test data________\")\ndatasets[1].sample(10)\n","ec0daaa0":"# train columns from datasets list\ncolumn_names = datasets[0].drop('Survived', axis=1).columns\nfor col in column_names:\n    datasets[0].stb.freq([col], value=\"Survived\")","96726532":"# survival chances by sex\ndatasets[0].groupby('Sex')['Survived'].mean()","95fecc1a":"# survival chances by pclass and sex\ndatasets[0].groupby([\"Pclass\", \"Sex\"])[\"Survived\"].mean()","47fdbb0a":"# Survival chances with Pclass and FamilySize \ndatasets[0].groupby([\"Pclass\", \"FamilySize\"])[\"Survived\"].mean()","4c8e8209":"# survival chances to these who came alone \ndatasets[0].groupby(\"Alone\")[\"Survived\"].mean()","a4411888":"# survival chances by Fare\ndatasets[0].groupby(['Fare'])['Survived'].mean()","8491faa2":"# survival chances by place of embarked\ndatasets[0].groupby('Embarked')[\"Survived\"].mean()","291bbfa3":"datasets[0].groupby([\"Embarked\", \"Farebin\"])[\"Survived\"].mean()","5a48572a":"# survival chances by Title \ndatasets[0].groupby([\"Title\"])[\"Survived\"].mean()","fa2c1d3e":"# survival chances with Parch\ndatasets[0].groupby(\"Parch\")[\"Survived\"].mean()","0e86ef4f":"# what was Pclass where most old or young people lived in\ndatasets[0].groupby(['Pclass', \"Age\"])['Survived'].mean()","72a73706":"# survival chances with Sibsp\ndatasets[0].groupby(\"SibSp\")[\"Survived\"].mean()","d2790497":"sns.heatmap(datasets[0].corr(), annot=True).set_title(\"Correlation of Train data\");","69568ff4":"sns.displot(datasets[0].Age, bins=10);","e0f7afc4":"sns.histplot(datasets[0], x=\"Age\", hue=\"Survived\", element=\"poly\");","e4597ced":"sns.displot(datasets[0], x=\"Age\", col=\"Sex\", hue=\"Survived\", multiple=\"stack\", bins=10);","b41e655d":"sns.displot(datasets[0].Fare, bins=10);","788e88a0":"sns.displot(data=datasets[0], x=\"Fare\", hue=\"Survived\", bins=10);","13296957":"sns.jointplot(data=datasets[0], x=\"Age\", y=\"Fare\", hue=\"Survived\");","2576a8c9":"sns.jointplot(data=datasets[0], x=\"Fare\", y=\"FamilySize\", hue=\"Survived\");","1be9d1fe":"sns.displot(datasets[0], x=\"Fare\", hue=\"Pclass\", col=\"Sex\", bins=10, multiple=\"stack\");","4239f958":"sns.histplot(datasets[0], x=\"FamilySize\", hue=\"Survived\", element=\"step\",);","37f4a890":"sns.set_theme(style=\"ticks\")\nsns.catplot(data=datasets[0], x=\"Sex\", y=\"Fare\", hue=\"Survived\");","c3839a24":"sns.set_theme(style=\"ticks\")\nsns.catplot(data=datasets[0], x=\"Sex\", y=\"Age\", hue=\"Survived\");","b48e0221":"sns.catplot(data=datasets[0], kind=\"count\",\n            x=\"Alone\", hue=\"Survived\", col=\"Deck\",\n           col_wrap=4);","4513b9b1":"sns.catplot(x=\"Age\", y=\"Embarked\",\n                hue=\"Survived\", row=\"Pclass\",\n                data=datasets[0],\n                orient=\"h\", height=2, aspect=3,\n                kind=\"violin\", dodge=True, cut=0, bw=.2);","22c9e4d1":"sns.set_theme(style=\"darkgrid\")\nsns.stripplot(x=\"FamilySize\", y=\"Age\", hue=\"Survived\", data=datasets[0]);","b976720f":"sns.catplot(data=datasets[0], x=\"Alone\", y=\"Age\", hue=\"Survived\", kind=\"strip\");","b64c5c01":"sns.catplot(data=datasets[0], x=\"Eighteen_plus\", y=\"Fare\", hue=\"Survived\", kind=\"boxen\");","c4548fe3":"sns.catplot(data=datasets[0], x=\"Deck\", y=\"Fare\", hue=\"Survived\", kind=\"bar\", capsize=.2);","8b87c2da":"sns.catplot(data=datasets[0], y=\"Sex\", hue=\"Survived\",\n            kind=\"count\");","88208a81":"sns.catplot(data=datasets[0], x=\"Sex\", y=\"Fare\", hue=\"Survived\", kind=\"strip\");","bfbc62b4":"sns.catplot(data=datasets[0], x=\"SibSp\", hue=\"Survived\", kind=\"count\");","def24084":"sns.catplot(data=datasets[0], x=\"SibSp\", y=\"Fare\", hue=\"Survived\", kind=\"point\");","ba3afdb3":"sns.catplot(data=datasets[0], kind=\"count\", y=\"Parch\", hue=\"Survived\");","1b2fc17b":"sns.catplot(data=datasets[0], x=\"Parch\", y=\"Fare\", hue=\"Survived\", kind=\"point\");","b47e688c":"sns.catplot(data=datasets[0], x=\"Embarked\", hue=\"Survived\", kind=\"count\");","359caef5":"sns.catplot(data=datasets[0], x=\"Embarked\", y=\"Fare\", hue=\"Survived\", kind=\"point\");","0bb701d6":"sns.relplot(data=datasets[0], col=\"Title\", x=\"Embarked\", y=\"Fare\", hue=\"Survived\");","945610c8":"# access information of train data \ndatasets[0].info()\nprint(\"_\"*40)\n# test data \ndatasets[1].info()","c71545ac":"# Handling categorical column.........\ncol_category = datasets[0].select_dtypes(exclude=[\"int64\", \"float64\"]).columns\nnp.delete(col_category, 0)","513a168c":"# initializing LabelEncoder: label_encoder\nlabel_encoder = LabelEncoder()\n\n# encodering variables with label_encoder method..........\nfor col in col_category:\n    datasets[0][col] = label_encoder.fit_transform(datasets[0][col])\n    datasets[1][col] = label_encoder.fit_transform(datasets[1][col])\n    \n# making all data sample with float dtype: encoder\nclean_train_data = datasets[0].astype('float64')\n# and test data: encodered_train\nclean_test_data = datasets[1].astype('float64')\n\n# display five row in both data\nclean_train_data.head()\nclean_test_data.head()\n","e897433e":"# Sperate target variable from the features\nX = clean_train_data.drop(\"Survived\", axis=1).values\ny = clean_train_data.Survived.values\n\n\n# Sperate train data into train and valid data\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=.30)\n ","8a3b3fa0":"def evaluation(model, train_X=train_X, valid_X=valid_X, train_y=train_y, valid_y=valid_y ):\n    \"\"\" evaluation function take model and provides different score e.g:-\n    training accuracy score -> 0,\n    testing accuracy score -> 1,\n    precision score -> 3\n    recall score -> 4\n    confusion matrix -> 5\n    classification report -> 6\n    prediction list -> 7\n    \"\"\"\n    # Fit model\n    model.fit(train_X, train_y)\n    # Prediction from Validation data X: pred_valid_X \n    pred_valid_X = model.predict(valid_X)\n    \n    # Evaluate\n    # Train score from accuracy_score on predicted and valid_y: train_score\n    train_score = round(accuracy_score(pred_valid_X, valid_y)*100, 2)\n    \n    # precision score: precision \n    precision = round (precision_score(pred_valid_X,valid_y)*100, 2)\n    \n    # recall score: recall \n    recall = round (recall_score(pred_valid_X, valid_y)*100, 2)\n    \n    # f1 score:f1\n    f1 = round(f1_score(pred_valid_X, valid_y)*100, 2)\n\n    \n    # cofusion metric dataframe: df_metric\n    df_metric = pd.DataFrame(data=confusion_matrix(pred_valid_X, valid_y),\n                             columns=[\"Died\", \"survived\"])\n    \n    # classsification report in dataframe format\n    df_report = classification_report(pred_valid_X, valid_y).split(\"\\n\")\n    report = \"\\n\".join(df_report)\n    \n    \n    # Test score using cross_val_score: test_score \n    test_score = round (cross_val_score(model, valid_X,\n                                 valid_y, cv=3, scoring=\"accuracy\").mean()*100, 2)\n    \n    \n    return (train_score, test_score, precision, recall, f1, df_metric, report)","22596529":"# initialize algorithms: models \nmodels = [RandomForestClassifier(), KNeighborsClassifier(), GaussianNB(), LinearDiscriminantAnalysis(),\n          GaussianProcessClassifier(), MLPClassifier(), ExtraTreesClassifier(), SVC(), XGBClassifier(), \n          GradientBoostingClassifier(), AdaBoostClassifier(), CatBoostClassifier(), LGBMClassifier(), BaggingClassifier(),\n          SGDClassifier(), RidgeClassifier(), LogisticRegression(), DecisionTreeClassifier()]\n\n# initialize models names: model_names \nmodel_names = [\"RandomForest\" , \"KNeighbors\", \"GaussianNB\", \"LinearDiscriminant\",\n               \"GaussianProcess\", \"MLP\", \"ExtraTrees\", \"SVC\",\"XGB\",\n              \"GradientBoost\", \"AdaBoost\", \"CatBoost\", \"LGBM\", \"Bagging\",\n              \"SGD\", \"Ridge\", \"Logistic\", \"DecisionTree\"]\n\ntrain_score = []\ntest_score = []\nprecision_list = []\nrecall_list = []\nf1_list = []\nf_importances = []\nfor model in models:\n    # pass through the model in evaluation function\n    evaluate = evaluation(model)\n    # Add accuracy score to train_score\n    train_score.append(evaluate[0])\n    # Add cross_val_score to test_score\n    test_score.append(evaluate[1])\n    # Precision score  added in precision_list\n    precision_list.append(evaluate[2])\n    # Recall score added in recall_list\n    recall_list.append(evaluate[3])\n    # Adding f1 score to f1_list\n    f1_list.append(evaluate[4])\n    \n","3c5297ec":"# Create DataFrame for  model: model_df\nmodels_df = pd.DataFrame({\"Model_names\":model_names, \"Accuracy_score\":train_score,\n                          \"Cross_val_score\": test_score, \"Precision_score\":precision_list,\n                         \"Recall_Score\": recall_list, \"f1_score\": f1_list})\nmodels_df ","7801a0a6":"# In this case the model to be wi\u013al be CatBoost \ncat_boost = CatBoostClassifier()\ncat_boost.fit(train_X, train_y)","d544a5f4":"pred_test = cat_boost.predict(clean_test_data)","8934f439":"# initialize catboost parameters\ncat_params = {'iterations': [num for num in range(2, 10, 2)],\n            'depth': [ele for ele in np.arange(2, 14, 2)],\n            'learning_rate': [0.001, 0.010, 0.030, 0.060, 0.090, 0.1, 0.130, 0.160, 0.190, 0.2],\n            'loss_function': ['Logloss','RMSE'],\n            'verbose': [True, False]}\n\n# Create RandomizedSearchCV \nrandomized = RandomizedSearchCV(cat_boost, cat_params)\n\nrandomized.fit(train_X, train_y)\n","bd4cab47":"# making predictions: cat_pred \ncat_pred = randomized.predict(valid_X)\n\ncat_test = randomized.predict(clean_test_data)\n\n# model accuracy score\ncat_score = accuracy_score(cat_pred, valid_y)\nprint(cat_score*100)\n\n# cross validation score: cross_valid\ncross_valid = cross_val_score(randomized, valid_X, valid_y, cv=5)\nprint(cross_valid.mean()*100)","29973855":"print(cross_valid.mean())","5f88014f":"# make predictions from clean_test_data: pred_test\n\n# make submission \nsub_df = pd.DataFrame({\"PassengerId\": clean_test_data.index, \"Survived\": cat_test})\nsub_df.to_csv (\"Submission_2\" ,index=False)\n\nprint(\"saved..............\")","731fbc2e":"* Data cleaning","7caae78c":"* Data visualisation","4b7f3803":"1. **Gathering Data** ","40f07752":"* Determine the problem","8326d418":"3.Choosing a Model","8ade47c2":"Since data is  already provide from kaggle i don't need gather","886a962e":"2. **Preparing that Data**","84cfd689":"- Examine data","912c8311":"- Distribution Plots","38e34b33":"* Feature engineering ","4dfbe49d":"**GOAL**:The task is to predict if a passenger survived the sinking of the Titanic or not.For each in the test set, I must predict a 0 or 1 value for the variable.","96d04e6d":"![image.png](https:\/\/i.ytimg.com\/vi\/1PhMWUoPDsk\/maxresdefault.jpg)","2c9609e6":"Variable    Definition\t   Key\nsurvival\tSurvival\t   0 = No, 1 = Yes\npclass\t    Ticket class   1 = 1st, 2 = 2nd, 3 = 3rd\nsex\t        Sex\t\nAge\t        Age in years\t\nsibsp\t    # of siblings \/ spouses aboard the Titanic\t\nparch\t    # of parents \/ children aboard the Titanic\t\nticket\t    Ticket number\t\nfare\t    Passenger fare\t\ncabin\t    Cabin number\t\nembarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton","287ce207":"* Data transformation ","2fac2862":"Steps of Machine Learning\n* Step 1: Gathering Data. ...\n* Step 2: Preparing that Data. ...\n* Step 3: Choosing a Model. ...\n* Step 4: Training. ...\n* Step 5: Evaluation. ...\n* Step 6: Hyperparameter Tuning. ...\n* Step 7: Prediction.","9e0a019d":"* Exploratory Analysis","7bc12e23":"***THE TITANIC PROJECT***"}}