{"cell_type":{"a2cf0005":"code","983f36d2":"code","09742c5b":"code","7eaf43ad":"code","e064373b":"code","d764485f":"code","696fe366":"code","eaf280a8":"code","81463787":"code","3d75bf3c":"code","3eee865c":"code","abf2a5c5":"code","568c9fdc":"code","d1c62970":"code","c9de4fdb":"code","56408bb8":"code","3b2fe3c0":"code","b517b090":"code","8f1115ea":"code","6366d7d2":"code","021d829e":"code","def4572d":"code","d82f853b":"code","f5610f26":"code","fe516729":"code","1ed346d4":"markdown"},"source":{"a2cf0005":"import os\nimport numpy as np\nimport torch\nimport glob\nimport torch.nn as nn\nfrom torchvision.transforms import transforms\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\nfrom torch.autograd import Variable\nimport torchvision\nimport pathlib\n\nfrom torchvision.models import squeezenet1_1\nimport torch.nn.functional as F\nfrom io import open\nimport os\nimport cv2","983f36d2":"#checking for device\ndevice=torch.device('cuda' if torch.cuda.is_available() else 'cpu')","09742c5b":"print(device)","7eaf43ad":"#Transforms\ntransformer=transforms.Compose([\n    transforms.Resize((150,150)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)\/std\n                        [0.5,0.5,0.5])\n])","e064373b":"data_dir = '..\/input\/ai-test\/Dataset'\ntrain_path = \"..\/input\/ai-test\/Dataset\/Training\/\"\nval_path = \"..\/input\/ai-test\/Dataset\/Validation\/\"","d764485f":"#categories\nroot=pathlib.Path(train_path)\nclasses=sorted([j.name.split('\/')[-1] for j in root.iterdir()])","696fe366":"train_loader = DataLoader(\n    torchvision.datasets.ImageFolder(train_path, transform=transformer),\n    batch_size=64, shuffle=True\n)\nval_loader = DataLoader(\n    torchvision.datasets.ImageFolder(val_path, transform=transformer),\n    batch_size=64, shuffle=True\n)","eaf280a8":"num_epochs=15","81463787":"print(classes)","3d75bf3c":"#CNN Network\n\n\nclass ConvNet(nn.Module):\n    def __init__(self,num_classes=6):\n        super(ConvNet,self).__init__()\n        \n        #Output size after convolution filter\n        #((w-f+2P)\/s) +1\n        \n        #Input shape= (256,3,150,150)\n        \n        self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1)\n        #Shape= (256,12,150,150)\n        self.bn1=nn.BatchNorm2d(num_features=12)\n        #Shape= (256,12,150,150)\n        self.relu1=nn.ReLU()\n        #Shape= (256,12,150,150)\n        \n        self.pool=nn.MaxPool2d(kernel_size=2)\n        #Reduce the image size be factor 2\n        #Shape= (256,12,75,75)\n        \n        \n        self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\n        #Shape= (256,20,75,75)\n        self.relu2=nn.ReLU()\n        #Shape= (256,20,75,75)\n        \n        \n        \n        self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\n        #Shape= (256,32,75,75)\n        self.bn3=nn.BatchNorm2d(num_features=32)\n        #Shape= (256,32,75,75)\n        self.relu3=nn.ReLU()\n        #Shape= (256,32,75,75)\n        \n        \n        self.fc=nn.Linear(in_features=75 * 75 * 32,out_features=num_classes)\n        \n        \n        \n        #Feed forwad function\n        \n    def forward(self,input):\n        output=self.conv1(input)\n        output=self.bn1(output)\n        output=self.relu1(output)\n            \n        output=self.pool(output)\n            \n        output=self.conv2(output)\n        output=self.relu2(output)\n            \n        output=self.conv3(output)\n        output=self.bn3(output)\n        output=self.relu3(output)\n            \n            \n            #Above output will be in matrix form, with shape (256,32,75,75)\n            \n        output=output.view(-1,32*75*75)\n            \n            \n        output=self.fc(output)\n            \n        return output","3eee865c":"model=ConvNet(num_classes=6).to(device)","abf2a5c5":"optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\nloss_function=nn.CrossEntropyLoss()","568c9fdc":"train_count = len(glob.glob(train_path+'\/**\/*.jpg'))\nval_count = len(glob.glob(val_path+'\/**\/*.jpg'))","d1c62970":"#Model training and saving best model\n\nbest_accuracy=0.0\n\nfor epoch in range(num_epochs):\n    \n    #Evaluation and training on training dataset\n    model.train()\n    train_accuracy=0.0\n    train_loss=0.0\n    \n    for i, (images,labels) in enumerate(train_loader):\n        if torch.cuda.is_available():\n            images=Variable(images.cuda())\n            labels=Variable(labels.cuda())\n            \n        optimizer.zero_grad()\n        \n        outputs=model(images)\n        loss=loss_function(outputs,labels)\n        loss.backward()\n        optimizer.step()\n        \n        \n        train_loss+= loss.cpu().data*images.size(0)\n        _,prediction=torch.max(outputs.data,1)\n        \n        train_accuracy+=int(torch.sum(prediction==labels.data))\n        \n    train_accuracy=train_accuracy\/train_count\n    train_loss=train_loss\/train_count\n    \n    \n    # Evaluation on testing dataset\n    model.eval()\n    \n    test_accuracy=0.0\n    for i, (images,labels) in enumerate(val_loader):\n        if torch.cuda.is_available():\n            images=Variable(images.cuda())\n            labels=Variable(labels.cuda())\n            \n        outputs=model(images)\n        _,prediction=torch.max(outputs.data,1)\n        test_accuracy+=int(torch.sum(prediction==labels.data))\n    \n    test_accuracy=test_accuracy\/val_count\n    \n    \n    print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))\n    \n    #Save the best model\n    if test_accuracy>best_accuracy:\n        torch.save(model.state_dict(),'.\/best_checkpoint.model')\n        best_accuracy=test_accuracy\n","c9de4fdb":"train_path = \"..\/input\/ai-test\/Dataset\/Training\/\"\npred_path = \"..\/input\/ai-test\/Dataset\/Testing\/\"","56408bb8":"#categories\nroot=pathlib.Path(train_path)\nclasses=sorted([j.name.split('\/')[-1] for j in root.iterdir()])","3b2fe3c0":"#CNN Network\n\n\nclass ConvNet(nn.Module):\n    def __init__(self,num_classes=6):\n        super(ConvNet,self).__init__()\n        \n        #Output size after convolution filter\n        #((w-f+2P)\/s) +1\n        \n        #Input shape= (256,3,150,150)\n        \n        self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1)\n        #Shape= (256,12,150,150)\n        self.bn1=nn.BatchNorm2d(num_features=12)\n        #Shape= (256,12,150,150)\n        self.relu1=nn.ReLU()\n        #Shape= (256,12,150,150)\n        \n        self.pool=nn.MaxPool2d(kernel_size=2)\n        #Reduce the image size be factor 2\n        #Shape= (256,12,75,75)\n        \n        \n        self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\n        #Shape= (256,20,75,75)\n        self.relu2=nn.ReLU()\n        #Shape= (256,20,75,75)\n        \n        \n        \n        self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\n        #Shape= (256,32,75,75)\n        self.bn3=nn.BatchNorm2d(num_features=32)\n        #Shape= (256,32,75,75)\n        self.relu3=nn.ReLU()\n        #Shape= (256,32,75,75)\n        \n        \n        self.fc=nn.Linear(in_features=75 * 75 * 32,out_features=num_classes)\n        \n        \n        \n        #Feed forwad function\n        \n    def forward(self,input):\n        output=self.conv1(input)\n        output=self.bn1(output)\n        output=self.relu1(output)\n            \n        output=self.pool(output)\n            \n        output=self.conv2(output)\n        output=self.relu2(output)\n            \n        output=self.conv3(output)\n        output=self.bn3(output)\n        output=self.relu3(output)\n            \n            \n            #Above output will be in matrix form, with shape (256,32,75,75)\n            \n        output=output.view(-1,32*75*75)\n            \n            \n        output=self.fc(output)\n            \n        return output","b517b090":"checkpoint = torch.load('.\/best_checkpoint.model')\nmodel = ConvNet(num_classes=6)\nmodel.load_state_dict(checkpoint)\nmodel.eval()","8f1115ea":"\"\"\"\n#Transforms\ntransformer=transforms.Compose([\n    transforms.Resize((150,150)),\n    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)\/std\n                        [0.5,0.5,0.5])\n])\n\"\"\"","6366d7d2":"from PIL import Image\n\n# prediction function\ndef prediction(img_path, transformer):\n    image = Image.open(img_path)\n    image_tensor = transformer(image).float()\n    image_tensor = image_tensor.unsqueeze_(0)\n    \n    if torch.cuda.is_available():\n        image.tensor.cuda()\n    input = Variable(image_tensor)\n    \n    output = model(input)\n    index = output.data.numpy().argmax()\n    pred  = classes[index]\n    return pred","021d829e":"image_path = glob.glob(pred_path+'\/*.jpg')","def4572d":"image_path","d82f853b":"print(type(transformer))","f5610f26":"pred_dict={}\n\nfor i in image_path:\n    pred_dict[i[i.rfind('\/')+1:]]=prediction(i, transformer)","fe516729":"pred_dict","1ed346d4":"# Inferencing"}}