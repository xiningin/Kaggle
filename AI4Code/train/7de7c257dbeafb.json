{"cell_type":{"3207313a":"code","5f3fe0e8":"code","2b1177b7":"code","6ccee130":"code","a645d417":"code","6d20db34":"code","3335d3d6":"code","41266277":"code","a3bb632a":"markdown"},"source":{"3207313a":"import gc\nimport os\nimport logging\nfrom datetime import datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nwarnings.filterwarnings('ignore')\nfrom sklearn import decomposition\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler","5f3fe0e8":"# Code from: https:\/\/stackoverflow.com\/questions\/45028260\/gaussian-to-uniform-distribution-conversion-has-errors-at-the-edges-of-uniform-d\ndef gaussian_estimation(vector):\n    mu = np.mean(vector)\n    sig = np.std(vector)\n    return mu, sig\n\n# Adjusts the data so it forms a gaussian with mean of 0 and std of 1\ndef gaussian_normalization(vector, char = None):\n    if char is None:\n        mu , sig = gaussian_estimation(vector)\n    else:\n        mu = char[0]\n        sig = char[1]\n    normalized = (vector-mu)\/sig\n    return normalized\n\n# Taken from https:\/\/en.wikipedia.org\/wiki\/Normal_distribution#Cumulative_distribution_function\ndef CDF(x, max_i = 100):\n    sum = x\n    value = x\n    for i in np.arange(max_i)+1:\n        value = value*x*x\/(2.0*i+1)\n        sum = sum + value\n    return 0.5 + (sum\/np.sqrt(2*np.pi))*np.exp(-1*(x*x)\/2)\n\ndef gaussian_to_uniform(vector, if_normal = False):\n    if (if_normal == False):\n        vector = gaussian_normalization(vector)\n    uni = np.apply_along_axis(CDF, 0, vector)\n    return uni\n","2b1177b7":"# Original DB\ntrain_df_orig = pd.read_csv(\"..\/input\/train.csv\")\ntest_df_orig = pd.read_csv(\"..\/input\/test.csv\")\ntarget = train_df_orig['target']\norig_features = [c for c in train_df_orig.columns if c not in ['ID_code', 'target']]","6ccee130":"# Convert features to Uniform Distribution\ntrain_uniform = pd.DataFrame()\ntest_uniform = pd.DataFrame()\nbin_cnt = 10**3\nfor namecol in tqdm_notebook(orig_features):\n    train_uniform[namecol+'_unif'] = gaussian_to_uniform(train_df_orig[namecol])\n    test_uniform[namecol+'_unif'] = gaussian_to_uniform(test_df_orig[namecol]) ","a645d417":"# Code thanks to: https:\/\/www.kaggle.com\/youhanlee\/yh-eda-i-want-to-see-all\nfrom scipy.stats import ks_2samp\ntarget_mask = train_df_orig['target'] == 1\nnon_target_mask = train_df_orig['target'] == 0 \nstatistics_array = []\nfor col in tqdm_notebook(train_uniform.columns):\n    statistic, pvalue = ks_2samp(train_uniform.loc[non_target_mask, col], train_uniform.loc[target_mask, col])\n    statistics_array.append(statistic)\n    fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n    sns.kdeplot(train_uniform.loc[non_target_mask, col], ax=ax, label='Target == 0')\n    sns.kdeplot(train_uniform.loc[target_mask, col], ax=ax, label='Target == 1')\n    ax.set_title('name: {}, statistics: {:.5f}, pvalue: {:5f}'.format(col, statistic, pvalue))\n    plt.show()\n    \n    fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n    sns.distplot(train_uniform.loc[non_target_mask, col], bins=50, kde=False, rug=False);\n    sns.distplot(train_uniform.loc[target_mask, col], bins=50, kde=False, rug=False);\n    plt.show()","6d20db34":"train_df = pd.concat([train_df_orig[orig_features],train_uniform],axis=1)\ntest_df = pd.concat([test_df_orig[orig_features],test_uniform],axis=1)\n","3335d3d6":"features = [c for c in train_df.columns if c not in ['target', 'ID_code']]\n\nparam = {\n    'bagging_freq': 5,          \n    'bagging_fraction': 0.30,   \n    'boost_from_average':'false',   \n    'boost': 'gbdt',\n    'feature_fraction': 0.03368,   \n    'learning_rate': 0.01,      \n    'max_depth': -1,                \n    'metric':'auc',\n    'min_data_in_leaf': 80,     \n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 4,           \n    'tree_learner': 'serial',   \n    'objective': 'binary',      \n    'verbosity': 1\n}\n\nnfolds = 5\n\nfolds = StratifiedKFold(n_splits=nfolds, shuffle=True, random_state=31415)\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\nAUC_l = []\niter_l = []\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train_df.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train_df.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 500000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, \n                    early_stopping_rounds = 250)\n    oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n\n    AUC_l.append(roc_auc_score(target[val_idx], oof[val_idx]))\n    iter_l.append(clf.best_iteration)\n    predictions += clf.predict(test_df[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nprint(\"CV score:{:<8.5f} CV_stats:[{:<8.5f}, {:<8.5f} ({:<8.5f}), {:<8.5f}]\".format(roc_auc_score(target, oof),np.min(AUC_l),np.mean(AUC_l), np.std(AUC_l), np.max(AUC_l)))\nscore = roc_auc_score(target, oof)\nbest_iter = np.mean(iter_l)\nbest_score_final = score","41266277":"# Submission\nsub_df = pd.DataFrame({\"ID_code\":test_df_orig[\"ID_code\"].values})\nsub_df[\"target\"] = predictions\nsub_df.to_csv('submission.csv.gz', index=False, compression='gzip')\nprint(pd.read_csv('submission.csv.gz').head())","a3bb632a":"Transforming Features from Gaussian Distribution to Uniform Distribution...\n\nPlots density and histograms are interesting.\n\nCan they really useful to create new categorical features?"}}