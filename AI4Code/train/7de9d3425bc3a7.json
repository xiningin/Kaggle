{"cell_type":{"3b7f73de":"code","f2e454f5":"code","740d4f02":"code","643bd32f":"code","7f7a3246":"code","97f42256":"code","8b0f79fc":"code","e22348bb":"code","a316fee2":"code","f95fbf46":"code","9e86813f":"code","bf18e184":"code","eb6031fd":"code","d4ca8b97":"code","6f644d7e":"code","9493e729":"code","9173bbcf":"code","40bf1780":"code","f3d74299":"code","bf5da907":"code","bf9a6cf1":"markdown","3a2afb8b":"markdown","bfb04ed2":"markdown","d3ed1ad9":"markdown","21675932":"markdown","c0527456":"markdown","7a547858":"markdown","78cfd4ec":"markdown","ef6e0440":"markdown","68401b2a":"markdown","d95ec33b":"markdown","3ee89359":"markdown","3383094a":"markdown","26129eff":"markdown","929b3a6f":"markdown","e8471e58":"markdown","e199e70a":"markdown","84d219e0":"markdown","c8ea52e1":"markdown","c8c151d1":"markdown"},"source":{"3b7f73de":"import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport plotly.express as px","f2e454f5":"df = pd.read_csv('\/kaggle\/input\/minneapolis-police-stops-and-police-violence\/police_stop_data.csv', low_memory = False)\nforce_df = pd.read_csv('\/kaggle\/input\/minneapolis-police-stops-and-police-violence\/police_use_of_force.csv')","740d4f02":"df.head()","643bd32f":"df.info()","7f7a3246":"year_values = []\nfor i in range(len(df)):\n    date = df['responseDate'][i].split(\" \")[0]\n    year = date.split(\"\/\")[0]\n    year_values.append(year)\n    \nyear_counts = dict(Counter(year_values))\nyear_counts = {'year': list(year_counts.keys()), 'count': list(year_counts.values())}\nyears_df = pd.DataFrame(year_counts)\nyears_df","97f42256":"fig_yearly = px.pie(years_df, values = 'count', names = 'year', title = 'Yearly Cases Distribution', hole = .5, color_discrete_sequence = px.colors.diverging.Portland)\nfig_yearly.show()","8b0f79fc":"problem_counts_dict = dict(Counter(df['problem']))\nproblem_df_dict = {'problem': list(problem_counts_dict.keys()), 'count': list(problem_counts_dict.values())}\n\nproblem_df = pd.DataFrame(problem_df_dict)\nproblem_df","e22348bb":"fig_yearly = px.pie(problem_df, values = 'count', names = 'problem', title = 'Type of Cases', hole = .5, color_discrete_sequence = px.colors.sequential.Agsunset)\nfig_yearly.show()","a316fee2":"import folium\nfrom folium.plugins import FastMarkerCluster\nlocations = df[['lat', 'long']]\nlocationlist = locations.values.tolist()","f95fbf46":"map = folium.Map(location=[44.986656, -93.258133], zoom_start=12)\nFastMarkerCluster(data=list(zip(df['lat'].values, df['long'].values))).add_to(map)\nmap","9e86813f":"df['race'].fillna('No Data', inplace = True)\nrace_counts_dict = dict(Counter(df['race']))\n\nrace_counts_dict['Unknown'] += race_counts_dict['No Data']\ndel race_counts_dict['No Data']\n\nrace_df_dict = {'race': list(race_counts_dict.keys()), 'count': list(race_counts_dict.values())}\n\nrace_df = pd.DataFrame(race_df_dict)\nrace_df","bf18e184":"fig_race = px.pie(race_df, values = 'count', names = 'race', title = 'Distribution of Races', hole = .5, color_discrete_sequence = px.colors.diverging.Temps)\nfig_race.show()","eb6031fd":"force_new = force_df[['ForceType', 'EventAge', 'TypeOfResistance', 'Is911Call']]\nforce_new.head()","d4ca8b97":"force_counts_dict = dict(Counter(force_new['ForceType']))\n\nforce_counts_dict['Unknown'] = force_counts_dict[np.nan]\ndel force_counts_dict[np.nan]\n\nforce_df_dict = {'force': list(force_counts_dict.keys()), 'count': list(force_counts_dict.values())}\n\nforce_type_df = pd.DataFrame(force_df_dict)\nforce_type_df","6f644d7e":"fig_force = px.bar(force_type_df, x = 'force', y = 'count')\nfig_force.show()","9493e729":"fig_age_hist = px.histogram(force_new, x = 'EventAge', nbins=10, opacity = 0.7)\nfig_age_hist.show()","9173bbcf":"force_df['TypeOfResistance'].fillna('Unknown', inplace = True)\ncleaned_types = []\nfor item in force_df['TypeOfResistance']:\n    p1_item = item.strip()\n    p2_item = p1_item.title()\n    cleaned_types.append(p2_item)\n    \nforce_df['TypeNew'] = cleaned_types\n\nresistance_counts_dict = dict(Counter(force_df['TypeNew']))\n\nresistance_counts_dict['Unspecified'] += resistance_counts_dict['Unknown']\ndel resistance_counts_dict['Unknown']\n\nresistance_counts_dict['Commission Of Crime'] += resistance_counts_dict['Commission Of A Crime']\ndel resistance_counts_dict['Commission Of A Crime']\n\nresistance_counts_dict['Fled In Vehicle'] += resistance_counts_dict['Fled In A Vehicle']\ndel resistance_counts_dict['Fled In A Vehicle']\n\nresistance_counts_dict['Assaulting Police Horse'] += resistance_counts_dict['Assaulted Police Horse']\ndel resistance_counts_dict['Assaulted Police Horse']\n\nresistance_counts_df_dict = {'type': list(resistance_counts_dict.keys()), 'count': list(resistance_counts_dict.values())}\n\nresistance_df = pd.DataFrame(resistance_counts_df_dict)\nresistance_df","40bf1780":"fig_resistance = px.pie(resistance_df, values = 'count', names = 'type', title = 'Distribution of Resistance', hole = .5, color_discrete_sequence = px.colors.diverging.Picnic)\nfig_resistance.show()","f3d74299":"_911_counts_dict = dict(Counter(force_new['Is911Call']))\n\n_911_counts_dict['Unspecified'] = _911_counts_dict[np.nan]\ndel _911_counts_dict[np.nan]\n\n_911_df_dict = {'val': list(_911_counts_dict.keys()), 'count': list(_911_counts_dict.values())}\n\n_911_df = pd.DataFrame(_911_df_dict)\n_911_df","bf5da907":"fig_911 = px.pie(_911_df, values = 'count', names = 'val', title = 'Distribution of 911 Calls', hole = .5, color_discrete_sequence = ['#ff4757', '#10ac84', '#2f3542'])\nfig_911.show()","bf9a6cf1":"## Feel free to  give suggestions and upvote this kernel if you loved it!","3a2afb8b":"# Interactive Maps\n\nNow, we'll be using an interactive map to see at which locations were the cases recorded:","bfb04ed2":"# Distribution of types of Resistance\n\nThe DataFrame contains several values of the same value in different formats.\n\nFor example: There are several rows with 'Assualting Police Horse' as the value which is similar to 'Assualted Police Horse'. We need to merge these values together.\n\nThis is a serious issue as the same type of resistance is classified into different bins. For ideal plotting, we'll process the values and add into several bins.\n\nHere is the dataframe after pre-processing:","d3ed1ad9":"Let us now use the second dataset:","21675932":"# Minneapolis Police Interactions: A Detailed Analysis\n\nMinneapolis is the largest city in the U.S. state of Minnesota and the principal city of the 16th-largest metropolitan area in the United States.\n\nThe dataset contains interactions of the Minneapolis Police Department.\n\nLet us explore the dataset by importing the libraries. ","c0527456":"# Distribution of Ages of People Involved\n\nWe can see that people between the 20-40 were arrested. ","7a547858":"# Forces Used by the Police\n\nLet us now see the various types of forces used by the police in incidents:","78cfd4ec":"**This map is interactive. Click on the orange clusters to see more cases in the neighborhood.**\n\n**Each cluster indicates the collective amount of cases in the surrounding areas highlighted in blue (visible on hover).**","ef6e0440":"We'll do the same pre-processing for other variables too:","68401b2a":"# Distribution of Races","d95ec33b":"We can see that most people were arrested with the help of bodily force or some chemical irritant. Tasers were also used in forceful arrest.","3ee89359":"Let us inspect the DataFrame and check which columns have missing values.","3383094a":"# How many people called 911?","26129eff":"### This kernel is meant for educational purposes and isn't intented to hurt anyone's sentiments. The objective of the kernel is to explore the dataset and to not objectify\/degrade particular communities.","929b3a6f":"Most cases were recorded in 2017 and 2018.","e8471e58":"Now, we'll load the .csv file into a DataFrame.","e199e70a":"# Distribution of Cases Per Year\n\nNow we'll extract years from dates, which will help us in further plotting.\n\nWe'll append the counts of each year to a DataFrame.","84d219e0":"Here is our first donut chart, which contains the distribution of cases recorded in each year:","c8ea52e1":"# Distribution of Case Types\nLet us see the distribution of cases on the basis of the problem:","c8c151d1":"Hence, most people were caught violating traffic laws or were displaying suspicious activity."}}