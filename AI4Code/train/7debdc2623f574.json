{"cell_type":{"9fcb83ee":"code","86af1206":"code","17679fc1":"code","c6fd132f":"code","39d9a1fb":"code","8e2a924a":"code","59552bb9":"code","d9002913":"code","94b78a0e":"code","7455abcf":"code","e5a8118b":"code","0c0709ad":"code","a89b1a4c":"code","d1b44c72":"code","9dd517d2":"code","c7c9c51c":"code","c00f913e":"code","8232e883":"code","3e125427":"code","e818e990":"code","1e73440e":"code","23ebe25b":"code","d14822eb":"code","426f013d":"code","b8b4e0f3":"code","f1a797b3":"code","22301f28":"code","f8f76795":"code","4f9c0c60":"code","b1e098be":"code","7f3654b2":"markdown","9d3e9c80":"markdown","f1672590":"markdown","6045ae43":"markdown","a2b88a4b":"markdown"},"source":{"9fcb83ee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","86af1206":"import csv\nimport pandas as pd\nimport numpy as np\nfrom numpy.random import randint, random, normal\nimport matplotlib.pyplot as plt","17679fc1":"import os\nimport numpy as np\nimport pandas as pd\nimport scipy.io\nfrom matplotlib import pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nnp.random.seed(1)","c6fd132f":"#filenames\nlabel_table = pd.read_csv(dirname+ '\/' +'sat6annotations.csv', header=None)\nlabel_table","39d9a1fb":"EPOCHS = 10\nBATCH_SIZE = 64\nIMG_SHAPE = 28, 28, 4\n\nTRAIN_NUM, TEST_NUM = 324000, 81000  # from the dataset site\ntrain_steps, test_steps = TRAIN_NUM \/\/ BATCH_SIZE, TEST_NUM \/\/ BATCH_SIZE","8e2a924a":"def get_gen(x_csv_path, y_csv_path, batch_size=BATCH_SIZE,\n            img_shape=IMG_SHAPE, labels_num=6, augment=False):\n  \"\"\"\n  Return a generator.\n  \n  This function returns a data generator. Every time the generator is called\n  it yields one batch of pairs (images, labels).\n  \"\"\"\n  xf, yf = open(x_csv_path), open(y_csv_path)\n  x_reader, y_reader = csv.reader(xf, delimiter=\",\"), csv.reader(yf, delimiter=\",\")\n\n  while True:\n    imgs = np.zeros((batch_size, *img_shape))\n    labels = np.zeros((batch_size, labels_num))\n\n    for i in range(batch_size):\n\n      try:\n        x_line, y_line = next(x_reader), next(y_reader)\n      except:\n        # this except clause resets the line readers when they reach the end of the files\n        xf.close()\n        yf.close()\n        xf, yf = open(x_csv_path), open(y_csv_path)\n        x_reader, y_reader = csv.reader(xf, delimiter=\",\"), csv.reader(yf, delimiter=\",\")\n        x_line, y_line = next(x_reader), next(y_reader)\n\n      img = np.reshape(list(map(int, x_line)), img_shape)\n      img = preprocess_img(img, augment)\n      imgs[i] = img\n\n      label = np.array(list(map(int, y_line)))\n      labels[i] = label\n\n    yield imgs, labels\n\n\ndef preprocess_img(img, augment=False):\n  \"\"\"\n  Preprocess the images.\n  \n  Takes as input an image and a boolean value (augment).\n  The standard preprocess includes only division by 255.0 (which maps from [0, 255] to [0.0, 1.0]).\n  \"\"\"\n  img = img \/ 255.0\n  if augment:\n    img = augment_img(img)\n  \n  return img\n\n\ndef augment_img(img):\n  \"\"\"\n  Augment an image.\n  \n  Image augmentation is useful in order to avoid overfitting.\n  This function adds random (normal) noise and randomly flip and\/or rotate each image.\n  \"\"\"\n  # add noise\n  gauss = normal(0, 0.05, img.shape)\n  img = img + gauss\n  img = np.clip(img, 0, 1)\n\n  # rotate 0\/90\/180\/270\n  img = np.rot90(img, randint(0, 3), axes=(0, 1))\n\n  # flip\/no-flip orizontaly\/vertical\n  if random() < 0.5: img = img[:, ::-1, :]\n  if random() < 0.5: img = img[::-1, :, :]\n\n  return img\n\n\ndef plot_images(images, labels=None, preds=None):\n  \"\"\"\n  Plot a batch of images and labels.\n  \n  This function plots a sample of images from the dataset along with their labels as image header.\n  If preds is given the header is real label\/predicted label.\n  \"\"\"\n  c = 8\n  labels = np.argmax(labels, -1) if labels is not None else labels\n  r = int(len(images) \/ c)\n  if preds is not None: preds = np.argmax(preds, -1)\n  fig, axs = plt.subplots(r, c, figsize=(16, 16))\n  cnt = 0\n  for i in range(r):\n    for j in range(c):\n      axs[i, j].imshow(images[cnt, ..., :3], cmap='gray')\n      axs[i, j].axis('off')\n      if labels is not None:\n        title = label_names[labels[cnt]] if preds is None else '%s\/%s' % (label_names[labels[cnt]],\n                                                                        label_names[preds[cnt]])\n        axs[i, j].set_title(title, fontsize=12 if preds is None else 8)\n      cnt += 1\n  plt.show()","59552bb9":"train_gen = get_gen('X_train_sat6.csv', 'y_train_sat6.csv', augment=True)\ntest_gen = get_gen('X_test_sat6.csv', 'y_test_sat6.csv')\n\nlabel_names = list(pd.read_csv('\/kaggle\/input\/deepsat-sat6\/sat6annotations.csv', header=None)[0])\nprint('label names:\\n%s' % ', '.join(label_names))","d9002913":"classes = ['building', 'barren_land','trees', 'grassland', 'road', 'water']\ntraining_labels = pd.read_csv(dirname+ '\/' + 'y_train_sat6.csv', header=None)\ntraining_labels.columns = classes\ntraining_labels[:5]","94b78a0e":"perc_of_class_train = {x: round(training_labels[x].sum()\/324000, 3) * 100 for (i, x) in enumerate(classes)}\nplt.bar(range(len(perc_of_class_train)), list(perc_of_class_train.values()), align='center')\nplt.xticks(range(len(perc_of_class_train)), list(perc_of_class_train.keys()))\nplt.ylabel('Percent of Class')\nplt.xlabel('Classes')\nplt.title('Representation of Classes in Training Set')\nplt.show()","7455abcf":"perc_of_class_train","e5a8118b":"test_labels = pd.read_csv(dirname+ '\/' +'y_test_sat6.csv', header=None)\ntest_labels.columns = classes\nperc_of_class_test = {x: round(test_labels[x].sum()\/81000, 3) * 100 for (i, x) in enumerate(classes)}\nplt.bar(range(len(perc_of_class_test)), list(perc_of_class_test.values()), align='center')\nplt.xticks(range(len(perc_of_class_test)), list(perc_of_class_test.keys()))\nplt.ylabel('Percent of Class')\nplt.xlabel('Classes')\nplt.title('Representation of Classes in Test Set')\nplt.show()","0c0709ad":"perc_of_class_test","a89b1a4c":"X_train = pd.read_csv(dirname+ '\/''X_train_sat6.csv', header=None, nrows=300)","d1b44c72":"def row_to_img(row_values, ir=False):\n    if ir:\n        return row_values.reshape(-1, 28, 28, 4).clip(0, 255).astype(np.uint8).squeeze(axis=0)[:,:,-1]\n    else:\n        return row_values.reshape(-1, 28, 28, 4).clip(0, 255).astype(np.uint8).squeeze(axis=0)[:,:,:3]","9dd517d2":"def get_labels(row_values):\n    annotations = ['building', 'barren_land','trees', 'grassland', 'road', 'water']\n    labels = [annotations[i] for i, x in enumerate(row_values) if x == 1]\n    return labels[0]","c7c9c51c":"fig, axs = plt.subplots(5, 5, figsize = (20, 20))\nfor i, ax in enumerate(axs.flatten()):\n    ax.set_title(get_labels(training_labels.iloc[i].values))\n    ax.imshow(row_to_img(X_train.iloc[i].values))","c00f913e":"fig, axs = plt.subplots(5, 5, figsize = (20, 20))\nfor i, ax in enumerate(axs.flatten()):\n    ax.set_title(get_labels(training_labels.iloc[i].values))\n    ax.imshow(row_to_img(X_train.iloc[i].values, ir=True))","8232e883":"sample = X_train.copy()\nsample['labels'] = [get_labels(x.values) for i, x in training_labels[:300].iterrows()]","3e125427":"pca = PCA(n_components=2)\ncomponents = pd.DataFrame(pca.fit_transform(sample.drop('labels', axis=1)), columns=['component_1', 'component_2'])\ncomponents['labels'] = sample['labels']","e818e990":"components.head()","1e73440e":"subsets = [components.loc[components['labels'] == x] for x in classes]","23ebe25b":"fig = plt.figure(figsize = (10,10))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('PCA of SAT-6', fontsize = 20)\n\ncolor_map = {'building': '#011627', 'barren_land': '#F71735', 'trees': '#41EAD4', 'grassland': '#5AFF15', 'road': '#FF9F1C', 'water': '#3C6E71'}\n\nfor subset in subsets:\n    label = subset['labels'].values.tolist()[0]\n    ax.scatter(x=subset['component_1'], y=subset['component_2'], s=50, c=color_map[label])\n\nax.legend(color_map.keys())\nax.grid()\nplt.show()","d14822eb":"pca.explained_variance_ratio_.cumsum()","426f013d":"pca_95 = PCA(n_components=0.95, svd_solver='full')\ncomponents_95 = pca_95.fit_transform(sample.drop('labels', axis=1))\ncomponents_95.shape","b8b4e0f3":"tsne = TSNE(n_components=2, perplexity=32)\ncomponents = pd.DataFrame(tsne.fit_transform(sample.drop('labels', axis=1)), columns=['component_1', 'component_2'])\ncomponents['labels'] = sample['labels']\ncomponents.head()","f1a797b3":"subsets = [components.loc[components['labels'] == x] for x in classes]\nfig = plt.figure(figsize = (10,10))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('TSNE of SAT-6', fontsize = 20)\n\ncolor_map = {'building': '#011627', 'barren_land': '#F71735', 'trees': '#41EAD4', 'grassland': '#5AFF15', 'road': '#FF9F1C', 'water': '#3C6E71'}\n\nfor subset in subsets:\n    label = subset['labels'].values.tolist()[0]\n    ax.scatter(x=subset['component_1'], y=subset['component_2'], s=50, c=color_map[label])\n\nax.legend(color_map.keys())\nax.grid()\nplt.show()","22301f28":"clf = RandomForestClassifier(verbose=True)\nX = components_95\ny = sample['labels']\nX_train, X_test, y_train, y_test = train_test_split(X, y)","f8f76795":"clf.fit(X_train, y_train)","4f9c0c60":"clf.score(X_test, y_test)","b1e098be":"y_pred = clf.predict(X_test)\nprint(classification_report(y_pred, y_test))","7f3654b2":"Baseline\n---\nBefore training deep neural networks training on hundres of thousands of images, lets attempt to train at least a weak classifier on the subsample of data. Anything classifier that is 16% accurate or more is better than random.\n\nFor our baseline, we will use the a random forest classifier trained on 51 principal components.","9d3e9c80":"The TSNE plot shows a similar relationship to the PCA except that the two clusters of the 5 classes excluding `water` are located much more closely together.","f1672590":"We would need to include at least 51 components to achieve an explained variance ratio of 95%.","6045ae43":"Our classifier recieved an overall accuracy score of 90.6%~ 91%. We see from the classification report that the model failed to correctly identify any `road` chips. It also struggled with `grassland`, `building`, and `trees` chips. We will attempt to correct these errors with a more robust classifier.","a2b88a4b":"We note that the 2 principal components constitute together only achieve an explained variance ratio of 83%."}}