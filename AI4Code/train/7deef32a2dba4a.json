{"cell_type":{"7fa4d572":"code","1af18527":"code","60ba97fc":"code","ebfec2d5":"code","64c0ee64":"code","a5df80ab":"code","12359b2c":"code","8ed94078":"code","f9ced0a6":"code","d63c8916":"code","8068d4f6":"code","9aae56ab":"code","cf924f9c":"code","da136b72":"code","2dbdaebb":"code","6b5c6e29":"code","ce548ba1":"code","02a64607":"code","a1bc4c35":"code","38331508":"code","3e58eb3a":"code","8f85e67c":"code","aa71c1c3":"code","3de05b5c":"code","c85af9b5":"code","7d1b2b08":"code","af08bb6e":"code","5209eff1":"code","5d49e65a":"code","d3d2a486":"code","5fae1c3d":"markdown","bf9ed1cb":"markdown","68dfe17e":"markdown","c4af6794":"markdown","f9e49116":"markdown","583f79e9":"markdown","8705ebf7":"markdown","2e28afd3":"markdown","6dd26733":"markdown","206e2c1d":"markdown","4e517cd1":"markdown","3d6f7be1":"markdown","9e2353eb":"markdown","7e82eb7d":"markdown","3c06d3a7":"markdown","cf303588":"markdown","fc8388a6":"markdown","5682e62a":"markdown","f0a72c7a":"markdown","0d9fec59":"markdown","fac0e317":"markdown","34e3cea7":"markdown","0fcacf6e":"markdown","c847be52":"markdown","92faadea":"markdown","64578ceb":"markdown","ed2bbd3f":"markdown","9e7fb3b7":"markdown","b4eb402b":"markdown","406f63db":"markdown","c6bff7b6":"markdown","56604490":"markdown"},"source":{"7fa4d572":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1af18527":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 70)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import preprocessing","60ba97fc":"train = pd.read_csv(r'..\/input\/learn-together\/train.csv')\ntest = pd.read_csv(r'..\/input\/learn-together\/test.csv')\n\nprint(\"Info training dataset :\")\nprint(train.shape)\nprint(\"Null values for training dataset:\")\nprint(train.isnull().sum())\n\n\nprint(\"Info testing dataset :\")\nprint(test.shape)\nprint(\"Null values for testing dataset:\")\nprint(test.isnull().sum())\n","ebfec2d5":"# adding description to Cover_Type\ncover_type = {1:'Spruce\/Fir', 2:'Lodgepole Pine',3:'Ponderosa Pine',4:'Cottonwood\/Willow',5:'Aspen',6:'Douglas-fir',7:'Krummholz'}\ntrain['Cover_type_description'] = train['Cover_Type'].map(cover_type)","64c0ee64":"list_of_cover_type = ['Spruce\/Fir','Lodgepole Pine','Ponderosa Pine','Cottonwood\/Willow','Aspen','Douglas-fir','Krummholz']\n\n#distribution of Aspect by cover type\n\nfor cover_type in list_of_cover_type:\n    plt.title(\"Cover type by Aspect\")\n    sns.distplot(a = train[train['Cover_type_description'] == cover_type]['Aspect'], label = cover_type, rug = True)\n    plt.legend()\nplt.show()","a5df80ab":"#decoding of Aspect feature\n#TRAINING\ntrain['Azimut'] = 0\ntrain['Azimut'].loc[(train['Aspect'] == 0)] = 'north' \ntrain['Azimut'].loc[(train['Aspect'] > 0) & (train['Aspect'] < 90)] = 'north_east' \ntrain['Azimut'].loc[(train['Aspect'] == 90)] = 'east' \ntrain['Azimut'].loc[(train['Aspect'] > 90) &(train['Aspect'] < 180)] = 'south_east' \ntrain['Azimut'].loc[(train['Aspect'] == 180)] = 'south'\ntrain['Azimut'].loc[(train['Aspect'] > 180) & (train['Aspect'] < 270)] = 'south_west' \ntrain['Azimut'].loc[(train['Aspect'] == 270)] = 'west'\ntrain['Azimut'].loc[(train['Aspect'] > 270) &(train['Aspect'] < 360)] = 'north_west' \ntrain['Azimut'].loc[(train['Aspect'] == 360)] = 'north'\n\n#TEST\ntest['Azimut'] = 0\ntest['Azimut'].loc[(test['Aspect'] == 0)] = 'north' \ntest['Azimut'].loc[(test['Aspect'] > 0) & (test['Aspect'] < 90)] = 'north_east' \ntest['Azimut'].loc[(test['Aspect'] == 90)] = 'east' \ntest['Azimut'].loc[(test['Aspect'] > 90) &(test['Aspect'] < 180)] = 'south_east' \ntest['Azimut'].loc[(test['Aspect'] == 180)] = 'south'\ntest['Azimut'].loc[(test['Aspect'] > 180) & (test['Aspect'] < 270)] = 'south_west' \ntest['Azimut'].loc[(test['Aspect'] == 270)] = 'west'\ntest['Azimut'].loc[(test['Aspect'] > 270) &(test['Aspect'] < 360)] = 'north_west' \ntest['Azimut'].loc[(test['Aspect'] == 360)] = 'north'","12359b2c":"for cover_type in list_of_cover_type:\n    plt.title(\"Countplot by Azimut in training dataset\")\n    sns.countplot(x = train['Azimut'])\nplt.show()","8ed94078":"for cover_type in list_of_cover_type:\n    plt.title(\"Cover type by Elevation\")\n    sns.distplot(a = train[train['Cover_type_description'] == cover_type]['Elevation'], label = cover_type)\n    plt.legend()\nplt.show()\n","f9ced0a6":"#Elevation grouping  for testing and training\n\ntrain['Elevation_bins'] = 0\ntrain['Elevation_bins'].loc[train['Elevation'] <= 2000] = 'less than 2000'\ntrain['Elevation_bins'].loc[(train['Elevation'] > 2000) & (train['Elevation'] <= 2500)] = 'between 2000 and 2500'\ntrain['Elevation_bins'].loc[(train['Elevation'] > 2500) & (train['Elevation'] <= 3000)] = 'between 2000 and 3000'\ntrain['Elevation_bins'].loc[(train['Elevation'] > 3000) & (train['Elevation'] <= 3500)] = 'between 3000 and 3500'\ntrain['Elevation_bins'].loc[train['Elevation'] > 3500] = 'greater than 3500'\n\n\ntest['Elevation_bins'] = 0\ntest['Elevation_bins'].loc[test['Elevation'] <= 2000] = 'less than 2000'\ntest['Elevation_bins'].loc[(test['Elevation'] > 2000) & (test['Elevation'] <= 2500)] = 'between 2000 and 2500'\ntest['Elevation_bins'].loc[(test['Elevation'] > 2500) & (test['Elevation'] <= 3000)] = 'between 2000 and 3000'\ntest['Elevation_bins'].loc[(test['Elevation'] > 3000) & (test['Elevation'] <= 3500)] = 'between 3000 and 3500'\ntest['Elevation_bins'].loc[test['Elevation'] > 3500] = 'greater than 3500'","d63c8916":"for cover_type in list_of_cover_type:\n    plt.title(\"Cover Type by Slope\")\n    sns.distplot(a = train[train['Cover_type_description'] == cover_type]['Slope'], label = cover_type)\n    plt.legend()\nplt.show()","8068d4f6":"print(\"Max slope for training\")\nprint(np.max(train['Slope']))\n\nprint(\"Max slope for testing\")\nprint(np.max(test['Slope']))","9aae56ab":"# create category for slope for training and test dataset <=10, >10 and <= 20, > 20 and <=30 , > 30\n\ntest['Slope_category'] = 0\ntest['Slope_category'].loc[(test['Slope'] <= 10)] = 'slope less than 10'\ntest['Slope_category'].loc[(test['Slope'] > 10) & (test['Slope'] <= 20)] = 'slope between 10 and 20'\ntest['Slope_category'].loc[(test['Slope'] > 20) & (test['Slope'] <= 30)] = 'slope between 20 and 30'\ntest['Slope_category'].loc[(test['Slope'] > 30)] = 'slope greater than 30'\n\ntrain['Slope_category'] = 0\ntrain['Slope_category'].loc[(train['Slope'] <= 10)] = 'slope less than 10'\ntrain['Slope_category'].loc[(train['Slope'] > 10) & (train['Slope'] <= 20)] = 'slope between 10 and 20'\ntrain['Slope_category'].loc[(train['Slope'] > 20) & (train['Slope'] <= 30)] = 'slope between 20 and 30'\ntrain['Slope_category'].loc[(train['Slope'] > 30)] = 'slope greater than 30'","cf924f9c":"train['mean_Hillshade'] = (train['Hillshade_9am']+ train['Hillshade_Noon']+train['Hillshade_3pm'])\/3\ntest['mean_Hillshade'] = (test['Hillshade_9am']+ test['Hillshade_Noon']+test['Hillshade_3pm'])\/3","da136b72":"def distance(a,b):\n    return (np.sqrt(np.power(a,2)+np.power(b,2)))\n\ntrain['Distance_to_hidrology'] = distance(train['Horizontal_Distance_To_Hydrology'],train['Vertical_Distance_To_Hydrology'])\ntest['Distance_to_hidrology'] = distance(test['Horizontal_Distance_To_Hydrology'],test['Vertical_Distance_To_Hydrology'])","2dbdaebb":"for cover_type in list_of_cover_type:\n    plt.title(\"Cover Type by Distance to hidrology  in training\")\n    sns.distplot(a = train[train['Cover_type_description'] == cover_type]['Distance_to_hidrology'], label = cover_type)\n    plt.legend()\nplt.show()","6b5c6e29":"for cover_type in list_of_cover_type:\n    plt.title(\"Cover Type by Distance to fire points\")\n    sns.distplot(a = train[train['Cover_type_description'] == cover_type]['Horizontal_Distance_To_Fire_Points'], label = cover_type)\n    plt.legend()\nplt.show()","ce548ba1":"#relationship for training dataset\ncolumns = ['Elevation', 'Aspect','Slope', 'mean_Hillshade']\n#for cover_type in list_of_cover_type:\nsns.pairplot(train, hue = \"Cover_type_description\", vars = columns)\nplt.legend()\nplt.show()","02a64607":"columns_to_drop = ['Elevation','Aspect','Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology','Hillshade_9am','Hillshade_Noon','Hillshade_3pm']\n\ntrain.drop(columns= columns_to_drop, inplace=True)\ntest.drop(columns= columns_to_drop, inplace=True)","a1bc4c35":"columns_to_encode = ['Azimut','Elevation_bins','Slope_category']\n\ntrain  = pd.get_dummies(train, columns = columns_to_encode)\ntest = pd.get_dummies(test, columns = columns_to_encode)","38331508":"\ncolumns_to_drop_train = ['Cover_type_description']\n\ntrain.drop(columns= columns_to_drop_train,inplace=True)","3e58eb3a":"s_scaler = preprocessing.StandardScaler()\ncolumns_to_scaled = ['Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points','mean_Hillshade', 'Distance_to_hidrology']\ntrain_to_scaled = train[columns_to_scaled]\ntraining_scaled = s_scaler.fit_transform(train_to_scaled)\ntraining_scaled_df = pd.DataFrame(data = training_scaled, columns = columns_to_scaled)","8f85e67c":"df_training_concatenated = pd.concat([train[['Id','Cover_Type','Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n       'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n       'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n       'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40','Azimut_east',\n       'Azimut_north', 'Azimut_north_east', 'Azimut_north_west',\n       'Azimut_south', 'Azimut_south_east', 'Azimut_south_west', 'Azimut_west',\n       'Elevation_bins_between 2000 and 2500',\n       'Elevation_bins_between 2000 and 3000',\n       'Elevation_bins_between 3000 and 3500',\n       'Elevation_bins_greater than 3500', 'Elevation_bins_less than 2000',\n       'Slope_category_slope between 10 and 20',\n       'Slope_category_slope between 20 and 30',\n       'Slope_category_slope greater than 30',\n       'Slope_category_slope less than 10']],training_scaled_df],axis =1)","aa71c1c3":"test_to_scaled = test[columns_to_scaled]\ntesting_scaled = s_scaler.fit_transform(test_to_scaled)\ntesting_scaled_df = pd.DataFrame(data = testing_scaled, columns = columns_to_scaled)    \n\ndf_testing_concatenated = pd.concat([test[['Id','Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n       'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n       'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n       'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40','Azimut_east',\n       'Azimut_north', 'Azimut_north_east', 'Azimut_north_west',\n       'Azimut_south', 'Azimut_south_east', 'Azimut_south_west', 'Azimut_west',\n       'Elevation_bins_between 2000 and 2500',\n       'Elevation_bins_between 2000 and 3000',\n       'Elevation_bins_between 3000 and 3500',\n       'Elevation_bins_greater than 3500', 'Elevation_bins_less than 2000',\n       'Slope_category_slope between 10 and 20',\n       'Slope_category_slope between 20 and 30',\n       'Slope_category_slope greater than 30',\n       'Slope_category_slope less than 10']],testing_scaled_df],axis =1)  \n    ","3de05b5c":"target = 'Cover_Type'\nfeatures = ['Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points',\n   'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n   'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n   'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n   'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n   'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n   'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n   'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n   'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n   'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n   'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n   'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40',\n   'mean_Hillshade', 'Distance_to_hidrology', 'Azimut_east',\n   'Azimut_north', 'Azimut_north_east', 'Azimut_north_west',\n   'Azimut_south', 'Azimut_south_east', 'Azimut_south_west', 'Azimut_west',\n   'Elevation_bins_between 2000 and 2500',\n   'Elevation_bins_between 2000 and 3000',\n   'Elevation_bins_between 3000 and 3500',\n   'Elevation_bins_greater than 3500', 'Elevation_bins_less than 2000',\n   'Slope_category_slope between 10 and 20',\n   'Slope_category_slope between 20 and 30',\n   'Slope_category_slope greater than 30',\n   'Slope_category_slope less than 10']\n","c85af9b5":"y = df_training_concatenated[target]\nX = df_training_concatenated[features]\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,  test_size=0.3,random_state=0)","7d1b2b08":"print(\"XGBOOST with grid search\")\nxgbclassifier = XGBClassifier()\nparams_xgbclassifier = {\"n_estimators\": [ 50, 100,150],\"learning_rate\":[0.01, 0.03,0.05]}\ngrid_search_xgboost = GridSearchCV(xgbclassifier, param_grid= params_xgbclassifier, cv=5, n_jobs=-1)\ngrid_search_xgboost.fit(X_train,y_train)\nprint(\"best parameter for xgboost_classifier \", grid_search_xgboost.best_params_)","af08bb6e":"model = XGBClassifier(n_estimators=grid_search_xgboost.best_params_['n_estimators'], random_state=0,learning_rate=grid_search_xgboost.best_params_['learning_rate'])\nmodel.fit(X_train, y_train)\npreds = model.predict(X_valid)\n","5209eff1":"x_test = test[features]\ny_prediction = model.predict(x_test)","5d49e65a":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(model, random_state = 42).fit(X_valid,y_valid)\neli5.show_weights(perm, feature_names = X_valid.columns.tolist())\n","d3d2a486":"import shap\n\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_valid)\nshap.summary_plot(shap_values[1], X_valid)\n    ","5fae1c3d":"Permutation importance shift every variable and by calculating the accuracy of our model define what are the most important variables. Another approch is SHAP that can be used to see how each variable impact on the whole outcome. So elevation is the most important variable for our model followed by wilderness area and distance. Maybe we could improve our model by digging more into features engineering for the variable distance. ","bf9ed1cb":"I import the data for both testing and training dataset and I display some information about both types of data:","68dfe17e":"Countplot for training dataset:","c4af6794":"The goal of this competition is to predict what different types of tree there are in an area based on some geographical features.\nData are related to Roosevelt National Forest. Here a link with more information:\n\nhttps:\/\/en.wikipedia.org\/wiki\/Roosevelt_National_Forest","f9e49116":"The difference between cover type with this varaible is much more define; for example Cottonwood\/Willow is at a lower altitude compared to Krummholtz and Aspen is in between them. \n\nLike for Aspect we can group elevation both for training and testing dataset. ","583f79e9":"First I import modules required to perfomr subsequent analysis. I set pd.set_option('display.max_columns', 70) in order to see all of the columns in training and testing dataset.","8705ebf7":"Now I can apply the model to test data in order to predict the value for covert types in testing data.","2e28afd3":"Distribution of distance to hidrology for training ","6dd26733":"If we describe variables in our training and testing dataset we will see that range between variables 'Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points','mean_Hillshade', 'Distance_to_hidrology' and others variables is high so we need to scaler them before building our model. Otherwise these variables would be valued as too important respect to others variables. We can do so using StandardScaler from preprocessing module. The output of standard scaler is an array so we need to create a new dataframe (training_scaled_df and testing_scaled_df) to display our variables after scaling. ","206e2c1d":"\nI will put togheter Horizontal_Distance_To_Hydrology and Vertical_Distance_To_Hydrology defining a function and applying it to training and test dataset. Then I will plot the new variable for the different cover types.","4e517cd1":"PERMUTATION IMPORTANCE","3d6f7be1":"Slope is not so well defined as elevation for the different cover types.  Minimum level is 52 and maximum is 66 and most of the cover types are grouped between 10 to 30. We can create four different groups (< 10, from 10 to 20, from 20 to 30,  and > 30) both for training and testing dataset. ","9e2353eb":"As you can see some cover types for esample Krummholtz have a peak on a well define orientation (above 100 degress in this case) while other cover type are much more spread. \n\nWe can create a new viariable and decode Aspect  in the geographics coordinate (north, south, east etc.) both for training and testing dataset.","7e82eb7d":"From univarite plotting of features with target variable we can see that the most important variable seems to be elevation. At this point I want to further investigate the relation of this variable with Aspect, Slope and mean_Hillshade and I want to analyze what are the mutual relation for each cover type. \n\nI will do so using a pairplot.","3c06d3a7":"Much of the cover types are oriented toward north or north west whereas there east, south and west are far less represented.\n\nNow we can display elevation for training dataset:","cf303588":"Now we can create a new dataframe with the scaled variables. We will call it df_training_concatenated. This dataframe is the resulting of concatenation between train dataframe and training_scale_dataframe. We need to concatenate along columns so we use axis =1","fc8388a6":"As you can see there is a peak for Krummholtz that need to be near to water. Ather species are Douglas\/fir and Cottonwood\/willow whereas other types are more distribuited.\n\nNow I will plot distribution of types to fire points","5682e62a":"SHAP is very important because we can see not only what variables are important but also the impact of each variable on our model (showed by colors) and the importance of each variable. For example we can see that elevation, distance and wilderness area 4 are particularly important for our model (and this is a coherent with what we have seen with permutation importance) wheras variables like Soil_type32 or  Soil_type 17 have no importance. \nThese results can give us important information about feature engineering to improve our model.\n\nIf you have found this kernel useful please up vote.","f0a72c7a":"As you can see from pairplot we can gain a lot of information!\n\nFor example we can see that elevation and slope can divide differents cover types pretty well as well as elevation and aspect.\n\nNow I can start building the model. For the first version of the kernel I will apply xgboost with grid seaarch to find the best hyperparameters.\n\nFirst I will drop the columns for which I've created a category variable.","0d9fec59":"We can see that testing dataset has much more rows compared to training dataset; on both dataset there are 55 columns of features plus, of course, target columns in training dataset.\nOn both dataset there are no missing values  so we don't have to impute missing data and all data are int64.\n\nCover_Type is codified as an integer ranging from 1 for Spruce\/Fir to 7 for Krummholz.\nIn order to perform an explanatory analysis and have more readable graphs we can decode the variable creating a dictionary and mappint it in our training dataset.","fac0e317":"Now we can display slope:","34e3cea7":"The features of my model are the following:","0fcacf6e":"Before building the model I drop the target description from training model.","c847be52":"After having created a model is important to perform what is called as machine learning explainability, that is finding out what are the most important variables used in our model. This can give use useful insight for improving our model. In fact, using business acumen we can have a better understanding if our model is using the variables that we expect are important for out model or if a new, unexpectd variable is important for the model we are trying to develop. We will start with permutation importance (library is eli5)","92faadea":"SHAP","64578ceb":"For hillshade I will take the mean of the values at noon , 9pm and 3pm:","ed2bbd3f":"Once I've found the best parameters I can create a new model by applying them. ","9e7fb3b7":"Now we can make some plots to see how features are related to the target variable starting from Aspect variable.  \nAspect is the degree in Azimut, basically represent the orientation of the different type of forest. \n\nFor more information you can see the link below:\n\nhttps:\/\/en.wikipedia.org\/wiki\/Azimuth\n\nWe can make a graph by first creating a list of Cover type and then we can plot aspect for each cover type using a distplot","b4eb402b":"I will apply cross validation by splitting the training dataset in 30% for testing and the other part for training ","406f63db":"Now I set grid search for xgboost classifier in order to find the best parameters for the model:","c6bff7b6":"Then I have to hot encoded then in order to use them in xgboost algorithm.","56604490":"We perform the same tranformation for testing data"}}