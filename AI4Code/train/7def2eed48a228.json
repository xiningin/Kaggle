{"cell_type":{"de7f35ce":"code","9f2e74a2":"code","c9db3ec3":"code","e5b5f312":"code","4031be89":"code","06646ed9":"code","45dc93d9":"code","10975f13":"code","cf44672a":"code","ed4076c5":"code","4d57c565":"code","e69f94f0":"code","e691afd1":"code","b5ddb72e":"code","627b9214":"code","a66f32e1":"code","083c038e":"code","d9111098":"code","66ae2d8c":"code","899dcee7":"code","29b83c30":"code","80b177ee":"code","da0cf1f8":"code","7d310df5":"code","ee66f9a7":"code","910181e3":"code","c6d74e5a":"code","823b1541":"code","35c98bca":"code","1a1f86e6":"code","ae8b4341":"code","da826816":"code","90f5a548":"code","a291cd95":"code","a86d046c":"code","a76c7807":"code","5c3a770e":"code","061f7a98":"code","6fd3e5ec":"code","f57dac3a":"code","4f3accf6":"code","818d9acd":"code","2324c3e7":"code","20f2ca9a":"code","9cc502d8":"code","dc35d0a1":"code","98977631":"code","f5be0306":"code","ea216eba":"code","f0bccdbc":"code","54b14d2a":"code","1d7568c0":"code","8b7aa86f":"code","5b93dd39":"code","ea0f2e0b":"code","1aaf60b4":"code","e24549fa":"code","2203dd69":"code","ad3b1c88":"code","c0c16794":"code","92e6e9a7":"code","2aa9c2e3":"code","f9594995":"code","77c0f585":"code","068b7d7c":"code","6841d486":"code","c46f86dd":"code","7751a776":"code","0e5ce2ba":"code","bcac859a":"code","2445742b":"code","02d15736":"code","263f0725":"code","062e0d4e":"code","786bc8ff":"code","f1c0a275":"code","78980620":"code","83845f33":"code","7f6d3f21":"code","91312a65":"code","5bd31d0b":"code","b0a4a2e8":"code","3064aa8f":"code","505ea09f":"markdown","50a827cb":"markdown","79296148":"markdown","b4b583cc":"markdown","891daac6":"markdown","3f704d59":"markdown","7698a52d":"markdown","4e084936":"markdown","fbb7409c":"markdown","1c818d8c":"markdown","619b44ef":"markdown","59886fb7":"markdown","5c9391d9":"markdown","20a33daa":"markdown","0c9dbbf1":"markdown","b75d4103":"markdown","95f6aee0":"markdown","e70c06d7":"markdown","493dba5d":"markdown","59f670b7":"markdown","00297fb3":"markdown","e1e19e69":"markdown","ec175a07":"markdown","f8383c67":"markdown","3d5b7598":"markdown","dfabb7a3":"markdown","9f48f4c3":"markdown","bcb59fd2":"markdown","208ce10e":"markdown","a99d97dc":"markdown","dffc9e60":"markdown","95a9fd6e":"markdown","c1760742":"markdown","f9b5cf4b":"markdown","41c84f4b":"markdown","951b3ad8":"markdown","30bf0992":"markdown","7ccced85":"markdown","2a7b3e3e":"markdown","ec894516":"markdown","c0459b7d":"markdown","48c7fada":"markdown","a9afd05e":"markdown","b7417a5f":"markdown","77c4ef57":"markdown","680ae005":"markdown","a2326eab":"markdown","42691fe2":"markdown","da9970ff":"markdown","5636dcca":"markdown","39ade3e5":"markdown","4542ddb5":"markdown","cded53a5":"markdown","7a366841":"markdown","9a4e264f":"markdown","97e2221d":"markdown","62721ab6":"markdown","33345160":"markdown","653ca219":"markdown","482202e5":"markdown","51a6a0f0":"markdown","b8a88c9a":"markdown","94d34f57":"markdown"},"source":{"de7f35ce":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# # For example, here's several helpful packages to load in \n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# # Input data files are available in the \"..\/input\/\" directory.\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # Any results you write to the current directory are saved as output.","9f2e74a2":"# Import section\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport types\nimport pandas as pd\nfrom botocore.client import Config\n#import ibm_boto3\n\n# Stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n# Models\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\n# Misc\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\nfrom datetime import datetime\n\npd.set_option('display.max_columns', None)\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000","c9db3ec3":"trainDf = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')","e5b5f312":"trainDf.head(10)","4031be89":"testDf = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","06646ed9":"testDf.head()","45dc93d9":"# First let us look at the Target and plot it visually\nimport matplotlib.pyplot as plt\n\nsns.set_style (\"white\")\nsns.set_color_codes (palette = 'deep')\nf, ax = plt.subplots (figsize=(8, 7))\n\nsns.distplot (trainDf['SalePrice'], color='b')\nax.xaxis.grid =False\nax.set (ylabel='Frequency' )\nax.set (ylabel='SalePrice' )\nax.set (title = 'SalePrice Distribution')\nsns.despine(trim=True, left=True)\nplt.show()","10975f13":"# Skewness and Kurtosis\n\nprint (\"Skewness of Data : %.2f\" % trainDf['SalePrice'].skew())\nprint (\"Kurtosis of Data : %.2f\" % trainDf['SalePrice'].kurt())","cf44672a":"pd.set_option('display.float_format', lambda x: '%.2f' %x)\ntrainDf.describe()","ed4076c5":"trainDf.describe(include = ['object'], exclude = ['int', 'float'])","4d57c565":"fig, axes = plt.subplots(nrows=1,ncols=2, figsize=(10, 8))\nfig.tight_layout(pad=6.0)\nsns.boxplot(x='OverallQual', y='SalePrice', data=trainDf, orient='v', ax=axes[0])\nsns.boxplot(x='OverallCond', y='SalePrice', data=trainDf, orient='v', ax=axes[1])","e69f94f0":"NumericColumns = trainDf.select_dtypes([np.int16, np.int32, np.int64, np.float16, np.float32, np.float64]).columns\nNumericColumns\nFeaturePlot = NumericColumns.drop([ 'SalePrice'])","e691afd1":"ColumnDisplay = 2\nfig, axex = plt.subplots( ncols=ColumnDisplay, nrows=0, figsize=(12,120) )\n\nplt.subplots_adjust (top=2, right=2)\nsns.color_palette ('husl', 8)\nfor i, feature in enumerate (list(trainDf[FeaturePlot]), 1) :\n    plt.subplot (len(list(FeaturePlot)), ColumnDisplay, i )\n    sns.scatterplot (x=feature, y='SalePrice', data=trainDf, hue='SalePrice', palette='Blues')","b5ddb72e":"CategoricColumns = trainDf.select_dtypes([np.object]).columns\n#CategoricColumns","627b9214":"DisplayColumns=2\nfig, axes = plt.subplots (ncols=DisplayColumns, nrows=0, figsize=(12, 120))\nplt.subplots_adjust (top=2, right=2)\nsns.color_palette('RdGy', 10)\n\nfor i, feature in enumerate (list(trainDf[CategoricColumns]), 1) :\n    plt.subplot (len(list(CategoricColumns)), ColumnDisplay, i )\n    sns.boxplot (x=feature, y='SalePrice', data=trainDf, orient='v')","a66f32e1":"trainDf['TranSalePrice'] = np.log1p(trainDf['SalePrice'])\n","083c038e":"trainDf[['Id', 'SalePrice', 'TranSalePrice']].head()","d9111098":"fig, axes = plt.subplots(nrows=1,ncols=2, figsize=(8, 6))\n\nfig.tight_layout(pad=4.0)\n\n#Set the generic properties of Seaborn\nsns.set_style(\"white\")\nsns.set_color_codes (palette = 'deep')\nsns.despine(trim=True, left=True)\n\n# The first distribution plot is for the original SalePrice data\nsns.distplot(trainDf['SalePrice'], color=\"b\", ax=axes[0]);\n#ax.grid(False)\naxes[0].set(ylabel=\"Frequency\")\naxes[0].set(xlabel=\"SalePrice\")\n#axes[0].xticks(rotation=90)\naxes[0].set(title=\"SalePrice distribution-Original\")\n\n# The Second distribution plot is for the original SalePrice data\nsns.distplot(trainDf['TranSalePrice'], fit=norm, color=\"g\", ax=axes[1]);\n(mu, sigma) = norm.fit(trainDf['TranSalePrice'])\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\n#ax.xaxis.grid(False)\naxes[1].set(ylabel=\"Frequency\")\naxes[1].set(xlabel=\"SalePrice\")\naxes[1].set(title=\"SalePrice distribution-Transform\")\n\nplt.show()","66ae2d8c":"# Remove outliers\n\ntrainDf.drop(trainDf[(trainDf['GrLivArea']>4000) & (trainDf['SalePrice']<300000)].index, inplace=True)\ntrainDf.reset_index(drop=True, inplace=True)","899dcee7":"trainEnd = trainDf.shape[0] #retain the count for segregation in future\nCombineDf = pd.concat ([trainDf, testDf], sort=True).reset_index(drop=True)\n#trainEnd","29b83c30":"CombineDf.head()","80b177ee":"CombineDf.drop(['Id', 'SalePrice', 'TranSalePrice'], axis=1, inplace=True)","da0cf1f8":"# Let us check the extent of values that are NaN (missing Values)\n\n# Create a subroutine to list down the NaN % in a tabular form. This subroutine will be invoked multiple times.\ndef ListEmptiness (df) :\n    CombineNaN = (df.isnull().sum()\/df.shape[0]) * 100 # Get the % of the Attributes that have Null value\n    CombineNaN = CombineNaN[CombineNaN !=0].sort_values(ascending=False)\n    nanData = pd.DataFrame({'Nan Ratio': CombineNaN})\n    return nanData\n\n","7d310df5":"Emptyness = ListEmptiness (CombineDf)\n#Emptyness","ee66f9a7":"#Visualize the Missing Attributes\nf,ax = plt.subplots (figsize=(10,8))\nsns.barplot (y='Nan Ratio', x=Emptyness.index, data=Emptyness)\nplt.xticks(rotation=90);\nplt.ylabel('Percentage of Missing data in the Feature')\nplt.xlabel('Features')\nplt.title('Missng data by Feature');","910181e3":"CombineDf['PoolQC'].value_counts(dropna=False).to_frame()","c6d74e5a":"CombineDf['PoolArea'].describe()","823b1541":"CombineDf['PoolQC'] = CombineDf['PoolQC'].fillna(\"None\")","35c98bca":"CombineDf['MiscFeature'].value_counts(dropna=False).to_frame()","1a1f86e6":"CombineDf['MiscFeature'] = CombineDf['MiscFeature'].fillna(\"None\")","ae8b4341":"CombineDf['Alley'] = CombineDf['Alley'].fillna(\"None\")","da826816":"CombineDf['Fence'] = CombineDf['Fence'].fillna (\"None\")","90f5a548":"CombineDf['FireplaceQu'] = CombineDf['FireplaceQu'].fillna (\"None\")","a291cd95":"CombineDf['LotFrontage'] = CombineDf.groupby('Neighborhood')['LotFrontage'].transform (lambda x:x.fillna(x.median()))","a86d046c":"for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n        CombineDf[col] = CombineDf[col].fillna(\"None\")\n","a76c7807":"# Replacing the missing values with 0, since no garage = no cars in garage\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n        CombineDf[col] = CombineDf[col].fillna(0)","5c3a770e":"# NaN values for these categorical basement features, means there's no basement\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    CombineDf[col] = CombineDf[col].fillna('None')","061f7a98":"# Create a Multi-culumn display to visualize same category columns\nfrom IPython.display import display_html\ndef display_side_by_side(*args):\n    html_str=''\n    for df in args:\n        html_str+=df.to_html()\n        html_str+=\"<td>&nbsp&nbsp&nbsp<\/td>\"\n    #print(html_str)\n    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)","6fd3e5ec":"display_side_by_side (CombineDf['Electrical'].value_counts(dropna=False).to_frame(), \\\n                      CombineDf['Functional'].value_counts(dropna=False).to_frame(), \\\n                      CombineDf['Utilities'].value_counts(dropna=False).to_frame(), \\\n                      CombineDf['SaleType'].value_counts(dropna=False).to_frame(), \\\n                      CombineDf['KitchenQual'].value_counts(dropna=False).to_frame()\n                     )","f57dac3a":"ColumnList = {'Electrical', 'Functional', 'Utilities', 'SaleType', 'KitchenQual' }\nfor col in ColumnList :\n    #print (CombineDf[col].mode()[0])\n    CombineDf[col] = CombineDf[col].fillna (CombineDf[col].mode()[0] )","4f3accf6":"display_side_by_side (\n                      CombineDf['Exterior1st'].value_counts(dropna=False).to_frame(), \\\n                      CombineDf['Exterior2nd'].value_counts(dropna=False).to_frame() \\\n                     )","818d9acd":"ColumnList = { 'Exterior1st', 'Exterior2nd' }\nfor col in ColumnList :\n    #print (CombineDf[col].mode()[0])\n    CombineDf[col] = CombineDf[col].fillna (CombineDf[col].mode()[0] )","2324c3e7":"CombineDf['BsmtFinSF1' ] = CombineDf['BsmtFinSF1'].fillna(0)\n\nCombineDf['BsmtFinSF2' ] = CombineDf['BsmtFinSF2'].fillna(0)\n\nCombineDf['BsmtFullBath' ] = CombineDf['BsmtFullBath'].fillna(0)\nCombineDf['BsmtHalfBath' ] = CombineDf['BsmtHalfBath'].fillna(0)\nCombineDf['BsmtUnfSF' ] = CombineDf['BsmtUnfSF'].fillna(0)\nCombineDf['TotalBsmtSF' ] = CombineDf['TotalBsmtSF'].fillna(0)\n","20f2ca9a":"display_side_by_side (CombineDf['MasVnrType'].value_counts(dropna=False).to_frame(), \\\n                      #CombineDf['MasVnrArea'].value_counts(dropna=False).to_frame(), \\\n                      CombineDf['MSZoning'].value_counts(dropna=False).to_frame() )","9cc502d8":"CombineDf['MasVnrType'] = CombineDf['MasVnrType'].fillna(CombineDf['MasVnrType'].mode()[0])\n\nCombineDf['MasVnrArea'] = CombineDf['MasVnrArea'].fillna(0)","dc35d0a1":"CombineDf['MSZoning'] = CombineDf.groupby('Neighborhood')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))","98977631":"# Let's make sure we handled all the missing values\n\nEmptyness = ListEmptiness (CombineDf)\nEmptyness","f5be0306":"NumericColumns = CombineDf.select_dtypes([np.int16, np.int32, np.int64, np.float16, np.float32, np.float64]).columns\n#NumericColumns","ea216eba":"# Create box plots for all numeric features\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=CombineDf[NumericColumns] , orient=\"h\", palette=\"RdGy\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","f0bccdbc":"SkewFeatures = CombineDf[NumericColumns].apply(lambda x: skew(x)).sort_values (ascending=False)\n\nHighSkews = SkewFeatures[SkewFeatures > 0.5]\nSkewIndex = HighSkews.index\n\nprint('There are {} Numerical Features with High Skew Values'.format(SkewIndex.shape[0]))","54b14d2a":"for i in SkewIndex :\n    CombineDf[i] = boxcox1p( CombineDf[i], boxcox_normmax(CombineDf[i] +1) )","1d7568c0":"SkewFeatures = CombineDf[NumericColumns].apply(lambda x: skew(x)).sort_values (ascending=False)\n\nHighSkews = SkewFeatures[SkewFeatures > 0.5]\nSkewIndex = HighSkews.index\n\nprint('There are {} Numerical Features with High Skew Values'.format(SkewIndex.shape[0]))","8b7aa86f":"CombineDf['YearsSinceRemodel'] = CombineDf['YrSold'].astype(int) - CombineDf['YearRemodAdd'].astype(int)\nCombineDf['Total_Home_Quality'] = CombineDf['OverallQual'] + CombineDf['OverallCond']\nCombineDf['TotalSF'] = CombineDf['TotalBsmtSF'] + CombineDf['1stFlrSF'] + CombineDf['2ndFlrSF']\nCombineDf['Total_Bathrooms'] = (CombineDf['FullBath'] + (0.5 * CombineDf['HalfBath']) +\\\n                               CombineDf['BsmtFullBath'] + (0.5 * CombineDf['BsmtHalfBath']))","5b93dd39":"CombineDf = CombineDf.drop(['Utilities', 'Street', 'PoolQC',], axis=1)","ea0f2e0b":"train= pd.concat([CombineDf.iloc[:trainEnd, :], trainDf['TranSalePrice']], axis=1)\ntrain_corr = train.corr()","1aaf60b4":"mask = np.zeros_like(train_corr)\nmask[np.triu_indices_from(mask)] = True\n\ncmap = sns.diverging_palette (180, 30, as_cmap=True)\n\nwith sns.axes_style(\"white\"):\n     fig, ax = plt.subplots(figsize=(13,11))\n     sns.heatmap(train_corr, vmax=.8, mask=mask, cmap=cmap, cbar_kws={'shrink':.5}, linewidth=.05);","e24549fa":"DummyCombineDf = pd.get_dummies(CombineDf)","2203dd69":"print ('Shape of the Dataset (Train+ Test) Rows:{}, Columns:{}'.format(DummyCombineDf.shape[0], DummyCombineDf.shape[1]))","ad3b1c88":"X_Train = DummyCombineDf.iloc[ : trainEnd, :]\nX_Test  = DummyCombineDf.iloc[trainEnd :, :] \n\n#y_train = trainDf[['TranSalePrice']]\ny_train = trainDf['TranSalePrice'].reset_index(drop=True)","c0c16794":"y_train.head(5)","92e6e9a7":"#Common Params\nKf = KFold(n_splits=4, random_state=42, shuffle=True) # Number of K-Folds\n\n# Light Gradient Boosting Regressor\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=7000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)\n\n# XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=6000,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n#                       objective='reg:linear',\n                       objective='reg:squarederror',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n# Ridge Regressor\n#ridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nridge_alphas = [ 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=Kf))\n\n# Support Vector Regressor\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)  \n\n# Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=1200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=42)\n\n# Stack up all the models above, optimized using xgboost\nstack_gen = StackingCVRegressor(regressors=(xgboost, lightgbm, svr, ridge, gbr, rf),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","2aa9c2e3":"def cv_rmse(model, TrainFeature, TrainTarget):\n    rmse = np.sqrt(-cross_val_score(model, TrainFeature, TrainTarget, scoring=\"neg_mean_squared_error\", cv=Kf))\n    return (rmse)","f9594995":"\nScores = {}\n\n\nprint(\"About to start Scoring for First Set Algorithms Time:%s\" % datetime.now())\nfor clf, label in zip([ridge, svr, rf], [ 'Ridge', 'SVM', 'Random Forest']):\n    score = cv_rmse(clf, X_Train, y_train)\n    print(\"Neg. MSE Score: %0.4f (+\/- %0.4f) [%s] Time::%s\" % ( score.mean(), score.std(), label, datetime.now()))\n    Scores[label] = (score.mean(), score.std())\n    \n","77c0f585":"print(\"About to start Scoring for Second Set Algorithms Time:%s\" % datetime.now())\n\nfor clf, label in zip([ xgboost, gbr, lightgbm], [ 'xgBoost', 'GradientBooster', 'lightGBM']):\n    score = cv_rmse(clf, X_Train, y_train)\n    print(\"Neg. MSE Score: %0.4f (+\/- %0.4f) [%s] Time::%s\" % ( score.mean(), score.std(), label, datetime.now()))\n    Scores[label] = (score.mean(), score.std())","068b7d7c":"Scores","6841d486":"print (X_Train.shape, y_train.shape)","c46f86dd":"print('stack_gen Start Time:%s' %  datetime.now())\nstack_gen_model = stack_gen.fit(np.array(X_Train), np.array(y_train) )\nprint('stack_gen End   Time:%s' % datetime.now())","7751a776":"print('lightgbm Start Time:%s' %  datetime.now())\nlightgbm_gen_model = lightgbm.fit(X_Train, y_train )\nprint('lightgbm End   Time:%s' % datetime.now())","0e5ce2ba":"print('xgBoost Start Time:%s' %  datetime.now())\nxgb_gen_model = xgboost.fit(X_Train, y_train )\nprint('xgBoost End   Time:%s' % datetime.now())","bcac859a":"print('SVR Start Time:%s' %  datetime.now())\nsvr_gen_model = svr.fit(X_Train, y_train )\nprint('SVR End   Time:%s' % datetime.now())","2445742b":"print('Ridge Start Time:%s' %  datetime.now())\nridge_gen_model = ridge.fit(X_Train, y_train )\nprint('Ridge End   Time:%s' % datetime.now())","02d15736":"print('Random Forest Start Time:%s' %  datetime.now())\nrf_gen_model = rf.fit(X_Train, y_train )\nprint('Random Forest End   Time:%s' % datetime.now())","263f0725":"print('GradientBoosting  Start Time:%s' %  datetime.now())\ngbr_gen_model = gbr.fit(X_Train, y_train )\nprint('GradientBoosting  End   Time:%s' % datetime.now())","062e0d4e":"# Blend models in order to make the final predictions more robust to overfitting\ndef blended_predictions(XBlend):\n    #print (XBlend.shape)\n    #print(XBlend.columns)\n    return (   (0.1  * ridge_gen_model.predict(XBlend))  \\\n             + (0.2  * svr_gen_model.predict(XBlend))  \\\n             + (0.1  * gbr_gen_model.predict(XBlend)) \\\n             + (0.1  * xgb_gen_model.predict(XBlend))  \\\n             + (0.1  * lightgbm_gen_model.predict(XBlend))  \\\n             + (0.05 * rf_gen_model.predict(XBlend))  \\\n             + (0.35 * stack_gen_model.predict(np.array(XBlend)))\n            )\n","786bc8ff":"# Get final precitions from the blended model\nprint('Blended   Start Time:%s' %  datetime.now())\nBlended_Yhat = blended_predictions(X_Train)\nprint('Blended   End Time:%s' %  datetime.now())\n\n","f1c0a275":"Blended_Yhat","78980620":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","83845f33":"blended_score = rmsle(y_train, blended_predictions(X_Train))\n\nScores['blended'] = (blended_score, 0)\nprint('RMSLE score on train data:')\nprint(blended_score)","7f6d3f21":"# Plot the predictions for each model\nsns.set_style(\"white\")\nfig = plt.figure(figsize=(24, 12))\n\nax = sns.pointplot(x=list(Scores.keys()), y=[score for score, _ in Scores.values()], markers=['o'], linestyles=['-'])\nfor i, score in enumerate(Scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nplt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\nplt.xlabel('Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\n\nplt.title('Scores of Models', size=20)\n\nplt.show()","91312a65":"#Apply Prediction to the Test dataset using the best Regression Algorithm (in this case Ridge)\ntestDf['TranSalePrice'] = blended_predictions(X_Test)","5bd31d0b":"#Get the SalePrice via the inverse Normalization tranformation\ntestDf['SalePrice'] = np.floor(np.expm1(testDf['TranSalePrice']))","b0a4a2e8":"testDf[['Id', 'TranSalePrice', 'SalePrice']].head(10).style.format({'SalePrice': \"{:,.0f}\"})","3064aa8f":"my_submission = pd.DataFrame({'Id': testDf.Id, 'SalePrice': testDf.SalePrice})\n# Use any filename. I choose submission here\n\nmy_submission.to_csv('submission.csv', index=False)\n","505ea09f":"### **Blend Models to get a balanced prediction**","50a827cb":"Creating  a new Worksheet after learing few new techniques from other notebooks that have been shared within the community. I have also worked on few improvisation over the learning from other notebook. ","79296148":"# 6.0 Fit the Models","b4b583cc":"# 7.0 Evaluation of the Models Considered","891daac6":"**MSZoning**  \nThere is no easy way to default this. The best approximation is to take the Mode of each Neighborhood.<br>\nThis is not a perfect logic, but the this will ensure that for a particular Neighborhood, <br>\nif it is primarily Residential, then the default will be Residential, if it is primarily Commercial then the default will be Commercial  and so on.\n","3f704d59":"# 2. Feature Engineering","7698a52d":"### Let us now observe how the SalePrice is related\/influenced to some of the Features.<br>\nLet us start with two Features namely OverallQual and OverallCond","4e084936":"Once the Normalized Price is predicted, we use the **inverse transformation** to derive the Real price from teh Normalized price","fbb7409c":"5. FireplaceQu - Fill the NaNs with \"None\" as per the data description","1c818d8c":"# 1. Exploratory Data Analysis (EDA)","619b44ef":"### 2.4.1 Visualize the correlation","59886fb7":"11. GarageYrBlt<br>\n12. GarageArea<br>\n13. GarageCars<br>\n\nThe above three Numeric Features are filled with 0 for the missing rows. 0 means No Garage.","5c9391d9":"Based on the above two observation related to Pool (Swimming Pool), it is recommended that the PoolQC be made \"none\" for the missing values","20a33daa":"Read the Training dataset into a dataframe","0c9dbbf1":"Next read the Test dataset into another Dataframe","b75d4103":"A huge majority **(2907 \/ 2917)** of the housed do not have the value captured for PoolQC. We could have dropped this Feature altogether since we have another Feature called PoolCond\nHowever, we are going to make NaN = \"None\" for the feature PoolQC. This is as per the Data Description File.\nWhat matters is whether a Pool exist or not","95f6aee0":"# 8.0 Predict the Test Data ","e70c06d7":"2. MiscFeature","493dba5d":"**Now let us combine the two datasets into a single dataset**","59f670b7":"26. BsmtFinSF1<br>\n27. BsmtFinSF2<br>\n28. BsmtFullBath<br>\n29. BsmtHalfBath<br>\n30. BsmtUnfSF<br>\n31. TotalBsmtSF<br>\nAbove six numerical Continuous features are updated with zero(0)","00297fb3":"Let us visualize the Categorical Features especially that are of type Object\/String","e1e19e69":"# The Housing price of Ames, Iowa, USA\nThis is my very first problem solving in the Regression space. Before this, it was all theory; and the very first realization was that, even in a friendly competition like this one in Kaggle, one has a lot to struggle and learn.\n\nMy humble admission is that I have learn a lot from other people's work.<br> \nHowever, I have also added a few perspective of mine. I hope some new-comer might be benefitted from my work that I am publishing.\n\nPlease feel free to use my notebook as a baseline and make more improvements on the same. Push your suggestions, comments to me as it is also my starting journey in the world of Data Science.<br>\nAlso, in case, you like some part of my work, please do upvote. Thank you in advance !\n\n## The Goal\n* Each row in the dataset describes the characteristics of a house.\n* The characteristics is indicated\/captured by 80 features e.g Neighborhood, Utilities, Landslope, Bedrooms, Kitchens, heating etc. (More fully available in the Data Description text file ) \n* Our goal is to predict the SalePrice, given these features.\n* Our models are evaluated on the Root-Mean-Squared-Error (RMSE) between the log of the SalePrice predicted by our model, and the log of the actual SalePrice.\n\n## Salient features of this work:\n1. I used Visualization techniques in the Exploratory Data Analysis (EDA) stage to present the data in a concise manner so that a person can capture a good view of the data with very little scrolling. ***This give a good perspective or bird-eye view of the data***\n2. In the Feature Engineering stage, I used a lot of generic techniques to fill the missing values. **Very little 'Hard-coding' has been used**. This will ensure that the notebood can be used on a different set of data whose data elements might be the same but the characteristics of the data is very different.\n3. Lastly, in the model-fitting stage, I have used a wighted average of the models that are used. Since, the belended model provided a better RMSE score, I used that in teh final prediction of the price.","ec175a07":"## 1.2 Feature Observation","f8383c67":"2. PoolArea","3d5b7598":"### **First import the necessary libraries **","dfabb7a3":"**Let us re-look at the Target once more i.e the SalePrice. We noticed that it had a tail towards the right or in other words it had a positive skew of 1.88.**<br>\nMost models do not perform well if the data-distribution is not normal.\nSo we apply log(1+x) transformation on SalePrice","9f48f4c3":"## 2.3 Drop some of the features that are Categorical, String type and having extreme Bias towards one value","bcb59fd2":"## 2.2 Dealing with Skewed data","208ce10e":"**Observation** :The feature OverallQual has less of overlaps but OverallCond have more overlap between the categories\nAlso, in OverallCond, for the value 5, we have huge outliers","a99d97dc":"As part of the Feature Engineering, we need to carry out steps to make the data ready for feeding into the Algorithm.\nThis will involve the Observations made in the EDA step. Feature Engineering will involve :\n\nRemoving Features that do not seem to add value, rather might make the Algorithm pick up the noise\n<li>Removing Outliers if required<\/li>\n<li>Finding out NaN (Null) values and see how to update them without distorting the sets<\/li>\n<li>Fixing Skewness of the data<\/li>","dffc9e60":"**Key Observations**\nThe belowFeatures appear to be Categorical and not Continuous\n\n<li>MSSubClass<\/li>\n<li>OverallQual<\/li>\n<li>OverallCond<\/li>\n<li>LowQualFinSF<\/li>\n<li>BsmtFullBath<\/li>\n<li>BsmtHalfBath<\/li>\n<li>FullBath<\/li>\n<li>HalfBath<\/li>\n<li>BedroomAbvGr<\/li>\n<li>KitchenAbvGr<\/li>\n<li>TotalRmsAbvGnd<\/li>\n<li>Fireplaces<\/li>\n<li>GarageCars<\/li>\n<li>MoSold<\/li>\n<li>YrSold<\/li>","95a9fd6e":"**Observations**\n\n<li>LotShape - Has a lot of overlap across two categories<\/li>\n<li>LandContour - Has a lot of overlap across 4 categories<\/li>\n<li>LotConfig - Has a lot of overlap<\/li>\n<li>LandSlope - Has a lot of overlap<\/li>\n<li>BsmtFinType1 and BsmtFinType2 have lot of overlaps<\/li>\n<li>Functional has overlaps across 7 categories<\/li>","c1760742":"**As a Next step, let us merge the Test data with the Train data.**\nWe can later seperate them out.This will keep the two dataset in sync.\nAlso the operations can be done in one iteration.","f9b5cf4b":"Let us look at the Predicted \"Normalized Price\" and the \"Real Price\" side-by-side","41c84f4b":"**Now let us plot the two bar-charts side-by-side to visualize the transformation**","951b3ad8":"We shall use the Blended model to first predict the **Normalized** Price","30bf0992":"### We have to now take the features on a case-by-case basis and handle them.<br>\n","7ccced85":"## 2.5 And Finally the One-hot Encoding for the categorical values of Object\/String type","2a7b3e3e":"24. Exterior1st<br>\n25. Exterior2nd<br>\n\nThe above Features have one or two rows of missing data. The best solution will be to update those with the Mode of the dataset","ec894516":"## 2.1 Dealing with missing Values (NaN)","c0459b7d":"4. Fence - No data is construed to be \"None\"","48c7fada":"**With all Feature Engineering done, we shall split the Combined Dataset into Train and Test as per the original dataset**","a9afd05e":"## 2.1 Remove Outliers","b7417a5f":"Let us observe the Numeric features. Some Numeric can be continuous and some will be Categorical","77c4ef57":"## 1.1 Target Attribute (SalePrice) Observation","680ae005":"14. BsmtQual<br>\n15. BsmtCond<br>\n16. BsmtExposure<br>\n17. BsmtFinType1<br>\n18. BsmtFinType2<br>\n\nFor the above Categorical Features related to Basedment, NaN means No Basement","a2326eab":"## 2.4 Get Correlation between Features and also between Features and SalePrice","42691fe2":"7. GarageType<br>\n8. GarageFinish<br>\n9. GarageQual<br>\n10. GarageCond<br>\nFor the four categorical features above (all related to Garage), NaN mean No Garage. So, empty rows will be filled with \"None\"","da9970ff":"**Observation**:<br>\nThe attributes **Street**, **Utilities**, **Condition2**, **RoofMatl**, **Heating** have very ***little variation***. Most of the data is in one category. Suspicion is that they might cause undue influence on the model.","5636dcca":"3. Alley- No data is construed to be \"None\"","39ade3e5":"A huge majority **(2812 \/ 2917)** of the housed do not have the value captured for MiscFeature. We are going to make NaN = \"None\" for the feature MiscFeature. This is as per the Data Description File.\n**This is also a potential Feature that can be dropped**","4542ddb5":"Let us observe some of the features in the training dataset","cded53a5":"**Let us use the scipy function boxcox1p which does Box-Cox transformation**","7a366841":"**By Applying boxcox1p , we have reduced the Skewness of 10 features.**","9a4e264f":"As depicted in the Graph above, the blended model gives an output whose RMSE is far less compared to individual Models.<br>. Hence we should go with the blended model to predict the outcome","97e2221d":"## 2.2 Create Additional features (including conversion from String\/Object type to numeric","62721ab6":"19. Electrical<br>\n20. Functional<br>\n21. Utilities<br>\n22. SaleType<br>\n23. KitchenQual<br>\n\nThe above Features have one or two rows of missing data. The best solution will be to update those with the Mode of the dataset","33345160":"6.LotFrontage -<br>\nLotFrantage is a Continuous Feature. As per Data Description, it is the Linear feet of street connected to property\nIt will be tempting to assign the mean value of the dataset to this feature with the missing values.\n***However, a better way is to find the Median of each Neighborhood and assign the same to the missing rows based on Neighborhood.***\nThe assumption being that the LotFrontage will be similar within a Neighborhood.\n","653ca219":"1. YearSinceRemodel - Captures how recently the house was touched upon<br>\n2. Total Home Quality - It combines OverallQual and OverallCond through a simple addition logic<br>\n3. Total Square Ft of the house","482202e5":"32. MasVnrType<br>\n33. MasVnrArea<br>\n34. MSZoning<br>\n\nThe last remaining Features.\nMasVnrType - To be filled by the Mode of the dataset\nMasVnrArea - To be filled by zero (0)\n","51a6a0f0":"**Observation** : The data distribution is skewed towards Right. In other words, the tail is towards the right","b8a88c9a":"1. PoolQC","94d34f57":"# 3.0 Setting up the Models"}}