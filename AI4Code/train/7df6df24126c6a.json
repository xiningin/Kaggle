{"cell_type":{"1482e5a6":"code","ba0fc6f7":"code","9732700d":"code","577f164f":"code","6d0cf2b1":"code","48dd8c4d":"code","f449f99e":"code","f79f7e8d":"code","75bf9f86":"code","9a3adf38":"code","f7722bc9":"code","20381271":"code","d6cbb535":"code","daac4303":"code","d4dcd0b1":"code","1e746aea":"code","76e54b6a":"code","b0da8f24":"code","92cc9c10":"code","9605108b":"code","7113ae00":"code","418415f4":"code","85784db8":"code","90017322":"code","5e409891":"code","f2fc8d89":"code","3c2c3f96":"code","a703e8cb":"code","e61f7e60":"code","6dd123b5":"code","e5e729c0":"code","952e2665":"code","76ad00ba":"code","b6866235":"code","8a5a0ad4":"code","26fe35f0":"code","c621f96c":"code","b1513b40":"code","e9ee53ab":"code","d3753e0b":"code","f6decbf3":"code","2806a0d3":"code","3b87c9da":"code","5fab2a05":"code","47ce6fa0":"code","99bf3ade":"code","8506b3d6":"markdown","593d1ab4":"markdown","92eb0b44":"markdown","b4ef66d2":"markdown","38d95d21":"markdown","73ce1b4c":"markdown","29e1de6f":"markdown","aa207a3c":"markdown","794ffe9f":"markdown","afe828ca":"markdown","bb486fe1":"markdown","0d2aa6fe":"markdown","3b3fe6eb":"markdown","dce9d3da":"markdown","907f3213":"markdown","2a12b89e":"markdown","74983270":"markdown","c9119803":"markdown","efc3a97b":"markdown","ea3b57d3":"markdown","dc8d4a0e":"markdown"},"source":{"1482e5a6":"!pip install pycaret","ba0fc6f7":"import numpy as np\nimport pandas as pd\npd.set_option('max_columns', None)\npd.set_option('max_rows', 90)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nfrom sklearn.neighbors import KNeighborsRegressor\nimport scipy.stats\nfrom sklearn.preprocessing import StandardScaler\nfrom pycaret.regression import setup, compare_models\nfrom sklearn.model_selection import KFold, cross_val_score\n\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import BayesianRidge, HuberRegressor, Ridge, OrthogonalMatchingPursuit\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\n\nimport optuna","9732700d":"train0 = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest0 = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsample_submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","577f164f":"train0","6d0cf2b1":"test0","48dd8c4d":"sample_submission","f449f99e":"target = train0['SalePrice']\ntest_ids = test0['Id']\n\ntrain1 = train0.drop(['Id', 'SalePrice'], axis=1)\ntest1 = test0.drop('Id', axis=1)\n\ndata1 = pd.concat([train1, test1], axis=0).reset_index(drop=True)\ndata1","f79f7e8d":"target","75bf9f86":"data2 = data1.copy()","9a3adf38":"data2['MSSubClass'] = data2['MSSubClass'].astype(str)","f7722bc9":"# Impute using a constant value\nfor column in [\n    'Alley',\n    'BsmtQual',\n    'BsmtCond',\n    'BsmtExposure',\n    'BsmtFinType1',\n    'BsmtFinType2',\n    'FireplaceQu',\n    'GarageType',\n    'GarageFinish',\n    'GarageQual',\n    'GarageCond',\n    'PoolQC',\n    'Fence',\n    'MiscFeature'\n]:\n    data2[column] = data2[column].fillna(\"None\")\n\n# Impute using the column mode\nfor column in [\n    'MSZoning',\n    'Utilities',\n    'Exterior1st',\n    'Exterior2nd',\n    'MasVnrType',\n    'Electrical',\n    'KitchenQual',\n    'Functional',\n    'SaleType'\n]:\n    data2[column] = data2[column].fillna(data2[column].mode()[0])","20381271":"data3 = data2.copy()","d6cbb535":"def knn_impute(df, na_target):\n    df = df.copy()\n    \n    numeric_df = df.select_dtypes(np.number)\n    non_na_columns = numeric_df.loc[: ,numeric_df.isna().sum() == 0].columns\n    \n    y_train = numeric_df.loc[numeric_df[na_target].isna() == False, na_target]\n    X_train = numeric_df.loc[numeric_df[na_target].isna() == False, non_na_columns]\n    X_test = numeric_df.loc[numeric_df[na_target].isna() == True, non_na_columns]\n    \n    knn = KNeighborsRegressor()\n    knn.fit(X_train, y_train)\n    \n    y_pred = knn.predict(X_test)\n    \n    df.loc[df[na_target].isna() == True, na_target] = y_pred\n    \n    return df","daac4303":"for column in [\n    'LotFrontage',\n    'MasVnrArea',\n    'BsmtFinSF1',\n    'BsmtFinSF2',\n    'BsmtUnfSF',\n    'TotalBsmtSF',\n    'BsmtFullBath',\n    'BsmtHalfBath',\n    'GarageYrBlt',\n    'GarageCars',\n    'GarageArea'\n]:\n    data3 = knn_impute(data3, column)","d4dcd0b1":"data4 = data3.copy()","1e746aea":"data4[\"SqFtPerRoom\"] = data4[\"GrLivArea\"] \/ (data4[\"TotRmsAbvGrd\"] +\n                                                       data4[\"FullBath\"] +\n                                                       data4[\"HalfBath\"] +\n                                                       data4[\"KitchenAbvGr\"])\n\ndata4['Total_Home_Quality'] = data4['OverallQual'] + data4['OverallCond']\n\ndata4['Total_Bathrooms'] = (data4['FullBath'] + (0.5 * data4['HalfBath']) +\n                               data4['BsmtFullBath'] + (0.5 * data4['BsmtHalfBath']))\n\ndata4[\"HighQualSF\"] = data4[\"1stFlrSF\"] + data4[\"2ndFlrSF\"]","76e54b6a":"data5 = data4.copy()","b0da8f24":"skew_df = pd.DataFrame(data5.select_dtypes(np.number).columns, columns=['Feature'])\nskew_df['Skew'] = skew_df['Feature'].apply(lambda feature: scipy.stats.skew(data5[feature]))\nskew_df['Absolute Skew'] = skew_df['Skew'].apply(abs)\nskew_df['Skewed'] = skew_df['Absolute Skew'].apply(lambda x: True if x >= 0.5 else False)\nskew_df","92cc9c10":"for column in skew_df.query(\"Skewed == True\")['Feature'].values:\n    data5[column] = np.log1p(data5[column])","9605108b":"data4['MoSold'] = (-np.cos(0.5236 * data5['MoSold']))","7113ae00":"data6 = data5.copy()","418415f4":"data6 = pd.get_dummies(data6)","85784db8":"data7 = data6.copy()","90017322":"scaler = StandardScaler()\nscaler.fit(data7)\n\ndata7 = pd.DataFrame(scaler.transform(data7), index=data7.index, columns=data7.columns)","5e409891":"data7","f2fc8d89":"data8 = data7.copy()","3c2c3f96":"plt.figure(figsize=(20, 10))\n\nplt.subplot(1, 2, 1)\nsns.distplot(target, kde=True, fit=scipy.stats.norm)\nplt.title(\"Without Log Transform\")\n\nplt.subplot(1, 2, 2)\nsns.distplot(np.log(target), kde=True, fit=scipy.stats.norm)\nplt.xlabel(\"Log SalePrice\")\nplt.title(\"With Log Transform\")\n\nplt.show()","a703e8cb":"log_target = np.log(target)","e61f7e60":"train_final = data8.loc[:train0.index.max(), :].copy()\ntest_final = data8.loc[train0.index.max() + 1:, :].reset_index(drop=True).copy()","6dd123b5":"train_final","e5e729c0":"test_final","952e2665":"# _ = setup(data=pd.concat([train_final, log_target], axis=1), target='SalePrice')","76ad00ba":"# compare_models()","b6866235":"# def br_objective(trial):\n#     n_iter = trial.suggest_int('n_iter', 50, 600)\n#     tol = trial.suggest_loguniform('tol', 1e-8, 10.0)\n#     alpha_1 = trial.suggest_loguniform('alpha_1', 1e-8, 10.0)\n#     alpha_2 = trial.suggest_loguniform('alpha_2', 1e-8, 10.0)\n#     lambda_1 = trial.suggest_loguniform('lambda_1', 1e-8, 10.0)\n#     lambda_2 = trial.suggest_loguniform('lambda_2', 1e-8, 10.0)\n    \n#     model = BayesianRidge(\n#         n_iter=n_iter,\n#         tol=tol,\n#         alpha_1=alpha_1,\n#         alpha_2=alpha_2,\n#         lambda_1=lambda_1,\n#         lambda_2=lambda_2\n#     )\n    \n#     model.fit(train_final, log_target)\n    \n#     cv_scores = np.exp(np.sqrt(-cross_val_score(model, train_final, log_target, scoring='neg_mean_squared_error', cv=kf)))\n    \n#     return np.mean(cv_scores)","8a5a0ad4":"# study = optuna.create_study(direction='minimize')\n# study.optimize(br_objective, n_trials=100)","26fe35f0":"# study.best_params","c621f96c":"catboost_params = {\n    'iterations': 6000,\n    'learning_rate': 0.005,\n    'depth': 4,\n    'l2_leaf_reg': 1,\n    'eval_metric':'RMSE',\n    'early_stopping_rounds': 200,\n    'random_seed': 42\n}\n\nbr_params = {\n    'n_iter': 304,\n    'tol': 0.16864712769300896,\n    'alpha_1': 5.589616542154059e-07,\n    'alpha_2': 9.799343618469923,\n    'lambda_1': 1.7735725582463822,\n    'lambda_2': 3.616928181181732e-06\n}\n\nlightgbm_params = {\n    'num_leaves': 39,\n    'max_depth': 2,\n    'learning_rate': 0.13705339989856127,\n    'n_estimators': 273\n}\n\nridge_params = {\n    'alpha': 631.1412445239156\n}","b1513b40":"models = {\n    \"catboost\": CatBoostRegressor(**catboost_params, verbose=0),\n    \"br\": BayesianRidge(**br_params),\n    \"lightgbm\": LGBMRegressor(**lightgbm_params),\n    \"ridge\": Ridge(**ridge_params),\n    \"omp\": OrthogonalMatchingPursuit()\n}","e9ee53ab":"for name, model in models.items():\n    model.fit(train_final, log_target)\n    print(name + \" trained.\")","d3753e0b":"results = {}\n\nkf = KFold(n_splits=10)\n\nfor name, model in models.items():\n    result = np.exp(np.sqrt(-cross_val_score(model, train_final, log_target, scoring='neg_mean_squared_error', cv=kf)))\n    results[name] = result","f6decbf3":"results","2806a0d3":"for name, result in results.items():\n    print(\"----------\\n\" + name)\n    print(np.mean(result))\n    print(np.std(result))","3b87c9da":"final_predictions = (\n    0.4 * np.exp(models['catboost'].predict(test_final)) +\n    0.2 * np.exp(models['br'].predict(test_final)) +\n    0.2 * np.exp(models['lightgbm'].predict(test_final)) +\n    0.1 * np.exp(models['ridge'].predict(test_final)) +\n    0.1 * np.exp(models['omp'].predict(test_final))\n)","5fab2a05":"final_predictions","47ce6fa0":"submission = pd.concat([test_ids, pd.Series(final_predictions, name='SalePrice')], axis=1)\nsubmission","99bf3ade":"submission.to_csv('.\/submission.csv', index=False, header=True)","8506b3d6":"## Cosine Transform for Cyclical Features","593d1ab4":"# Combine Predictions","92eb0b44":"# Split Data","b4ef66d2":"# Livestream Included!\n\n***\n\nThis notebook was created during a YouTube live session.  \nFor an in-depth guide, check it out here!  \nhttps:\/\/youtu.be\/zwYHloLXH0c","38d95d21":"# Make Submission","73ce1b4c":"# Scaling","29e1de6f":"## Log Transform for Skewed Features","aa207a3c":"# Feature Transformations","794ffe9f":"# Bagging Ensemble","afe828ca":"# Target Transformation","bb486fe1":"# Feature Engineering","0d2aa6fe":"## Numeric Missing Values","3b3fe6eb":"# Cleaning","dce9d3da":"# Encode Categoricals","907f3213":"# Evaluate","2a12b89e":"## Fill Categorical Missing Values","74983270":"# Hyperparameter Optimization","c9119803":"# Getting Started","efc3a97b":"# Model Selection","ea3b57d3":"# Combine Train and Test Sets","dc8d4a0e":"## Ensure Proper Data Types"}}