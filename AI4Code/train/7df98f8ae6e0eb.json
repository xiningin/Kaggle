{"cell_type":{"2625a63b":"code","88da49b8":"code","5ec2ec67":"code","c4837294":"code","c5104b6a":"code","ea26dbb8":"code","31735dde":"code","3bfaf251":"code","535c1264":"code","04329b4d":"code","dcff293f":"code","3fcfa52d":"code","dea072af":"code","95b10fcc":"code","539d70f2":"code","b7e6a103":"code","b9ff894e":"code","40f2ffd7":"code","86fcea7e":"code","96f1fd4c":"code","92af32db":"code","38da873d":"code","3c598d83":"code","a834b106":"code","a66849c6":"code","fbe74b3c":"code","ae5313d2":"code","6fd27796":"code","cb10b3b1":"code","f9039d01":"code","59770cd2":"code","78b1b074":"code","0a4b5c9e":"code","4fe9ca50":"code","8ba44f68":"code","1e51df87":"code","f54a4685":"code","60c322c9":"code","5828e37a":"code","abfcc5ca":"code","cf7f7ed8":"code","b2cf9a76":"code","90e52a7f":"code","590d543e":"code","28f6efb6":"code","ffeadc9e":"code","e273c5cd":"code","69657607":"code","2587fdfc":"code","726f57d2":"code","b736a631":"code","645414da":"markdown","40181f01":"markdown","25e88ecb":"markdown","6b008736":"markdown","90cff0ae":"markdown","039f2e35":"markdown","6cefdec3":"markdown","259b4b02":"markdown","3cc90c5f":"markdown","20134c1f":"markdown","3bd48e43":"markdown","2910bbd6":"markdown","fb120595":"markdown","990f017b":"markdown","38a6fc5c":"markdown","b2c1ee49":"markdown","0c37c6d4":"markdown","2a70420a":"markdown","782fbffd":"markdown","4be69828":"markdown","eab54141":"markdown","bc901ef6":"markdown","c858bde6":"markdown"},"source":{"2625a63b":"# Import required libraries:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport os\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Set our random seed:\nSEED = 17\nPATH_TO_DIR = '..\/input\/amazoncom-employee-access-challenge\/'\n\nprint(os.listdir(PATH_TO_DIR))","88da49b8":"# Import data:\ntrain = pd.read_csv(PATH_TO_DIR + 'train.csv')","5ec2ec67":"y = train['ACTION']\ntrain = train[['RESOURCE', 'MGR_ID', 'ROLE_FAMILY_DESC', 'ROLE_FAMILY', 'ROLE_CODE']]","c4837294":"logit = LogisticRegression(random_state=SEED)\nrf = RandomForestClassifier(random_state=SEED)","c5104b6a":"# Split dataset into train and validation subsets:\nX_train, X_val, y_train, y_val = train_test_split(train, y, test_size=0.2, random_state=SEED)","ea26dbb8":"# We create a helper function to get the scores for each encoding method:\ndef get_score(model, X, y, X_val, y_val):\n    model.fit(X, y)\n    y_pred = model.predict_proba(X_val)[:,1]\n    score = roc_auc_score(y_val, y_pred)\n    return score","31735dde":"# Lets have a quick look at our data:\nX_train.head(5)","3bfaf251":"X_train.info()","535c1264":"# Discover the number of categories within each categorical feature:\nlen(X_train.RESOURCE.unique()), len(X_train.MGR_ID.unique()), len(X_train.ROLE_FAMILY_DESC.unique()), len(X_train.ROLE_FAMILY.unique()),len(X_train.ROLE_CODE.unique())","04329b4d":"# Create a list of each categorical column name:\ncolumns = [i for i in X_train.columns]","dcff293f":"%%time\nbaseline_logit_score = get_score(logit, X_train, y_train, X_val, y_val)\nprint('Logistic Regression score without feature engineering:', baseline_logit_score)","3fcfa52d":"%%time\nbaseline_rf_score = get_score(rf, X_train, y_train, X_val, y_val)\nprint('Random Forest score without feature engineering:', baseline_rf_score)","dea072af":"from sklearn.preprocessing import OneHotEncoder\n\none_hot_enc = OneHotEncoder(sparse=False)","95b10fcc":"print('Original number of features: \\n', X_train.shape[1], \"\\n\")\ndata_ohe_train = (one_hot_enc.fit_transform(X_train))\ndata_ohe_val = (one_hot_enc.transform(X_val))\nprint('Features after OHE: \\n', data_ohe_train.shape[1])","539d70f2":"%%time\nohe_logit_score = get_score(logit, data_ohe_train, y_train, data_ohe_val, y_val)\nprint('Logistic Regression score with one-hot encoding:', ohe_logit_score)","b7e6a103":"%%time\nohe_rf_score = get_score(rf, data_ohe_train, y_train, data_ohe_val, y_val)\nprint('Random Forest score with one-hot encoding:', ohe_rf_score)","b9ff894e":"# Install category_encoders:\n# pip install category_encoders","40f2ffd7":"from category_encoders import HashingEncoder","86fcea7e":"n_components_list = [100, 500, 1000, 5000, 10000]\nn_components_list_str = [str(i) for i in n_components_list]","96f1fd4c":"fh_logit_scores = []\n\n# Iterate over different n_components:\nfor n_components in n_components_list:\n    \n    hashing_enc = HashingEncoder(cols=columns, n_components=n_components).fit(X_train, y_train)\n    \n    X_train_hashing = hashing_enc.transform(X_train.reset_index(drop=True))\n    X_val_hashing = hashing_enc.transform(X_val.reset_index(drop=True))\n    \n    fe_logit_score = get_score(logit, X_train_hashing, y_train, X_val_hashing, y_val)\n    fh_logit_scores.append(fe_logit_score)","92af32db":"plt.figure(figsize=(8, 5))\nplt.plot(n_components_list_str, fh_logit_scores, linewidth=3)\nplt.title('n_compontents vs roc_auc for feature hashing with logistic regression')\nplt.xlabel('n_components')\nplt.ylabel('score')\nplt.show;","38da873d":"hashing_enc = HashingEncoder(cols=columns, n_components=10000).fit(X_train, y_train)\n\nX_train_hashing = hashing_enc.transform(X_train.reset_index(drop=True))\nX_val_hashing = hashing_enc.transform(X_val.reset_index(drop=True))","3c598d83":"X_train_hashing.head()","a834b106":"%%time\nhashing_logit_score = get_score(logit, X_train_hashing, y_train, X_val_hashing, y_val)\nprint('Logistic Regression score with feature hashing:', hashing_logit_score)","a66849c6":"%%time\nhashing_rf_score = get_score(rf, X_train_hashing, y_train, X_val_hashing, y_val)\nprint('Random Forest score with feature hashing:', hashing_rf_score)","fbe74b3c":"# Create example dataframe with numbers ranging from 1 to 5:\nexample_df = pd.DataFrame([1,2,3,4,5], columns=['example'])\n\nfrom category_encoders import BinaryEncoder\n\nexample_binary = BinaryEncoder(cols=['example']).fit_transform(example_df)\n\nexample_binary","ae5313d2":"binary_enc = BinaryEncoder(cols=columns).fit(X_train, y_train)","6fd27796":"X_train_binary = binary_enc.transform(X_train.reset_index(drop=True))\nX_val_binary = binary_enc.transform(X_val.reset_index(drop=True))\n# note: category_encoders implementations can't handle shuffled datasets. ","cb10b3b1":"print('Features after Binary Encoding: \\n', X_train_binary.shape[1])","f9039d01":"%%time\nbe_logit_score = get_score(logit, X_train_binary, y_train, X_val_binary, y_val)\nprint('Logistic Regression score with binary encoding:', be_logit_score)","59770cd2":"%%time\nbinary_rf_score = get_score(rf, X_train_binary, y_train, X_val_binary, y_val)\nprint('Random Forest score with binary encoding:', binary_rf_score)","78b1b074":"from category_encoders import TargetEncoder\n\ntarg_enc = TargetEncoder(cols=columns, smoothing=8, min_samples_leaf=5).fit(X_train, y_train)","0a4b5c9e":"X_train_te = targ_enc.transform(X_train.reset_index(drop=True))\nX_val_te = targ_enc.transform(X_val.reset_index(drop=True))","4fe9ca50":"X_train_te.head()","8ba44f68":"%%time\nte_logit_score = get_score(logit, X_train_te, y_train, X_val_te, y_val)\nprint('Logistic Regression score with target encoding:', te_logit_score)","1e51df87":"%%time\nte_rf_score = get_score(rf, X_train_te, y_train, X_val_te, y_val)\nprint('Random Forest score with target encoding:', te_rf_score)","f54a4685":"targ_enc = TargetEncoder(cols=columns, smoothing=8, min_samples_leaf=5).fit(X_train, y_train)\n\nX_train_te = targ_enc.transform(X_train.reset_index(drop=True))\nX_val_te = targ_enc.transform(X_val.reset_index(drop=True))","60c322c9":"%%time\nme_logit_score = get_score(logit, X_train_te, y_train, X_val_te, y_val)\nprint('Logistic Regression score with target encoding with regularization:', me_logit_score)","5828e37a":"%%time\nme_rf_score = get_score(rf, X_train_te, y_train, X_val_te, y_val)\nprint('Random Forest score with target encoding with regularization:', me_rf_score)","abfcc5ca":"from sklearn.model_selection import KFold\n\n# Create 5 kfold splits:\nkf = KFold(random_state=17, n_splits=5, shuffle=False)","cf7f7ed8":"# Create copy of data:\nX_train_te = X_train.copy()\nX_train_te['target'] = y_train","b2cf9a76":"all_set = []\n\nfor train_index, val_index in kf.split(X_train_te):\n    # Create splits:\n    train, val = X_train_te.iloc[train_index], X_train_te.iloc[val_index]\n    val=val.copy()\n    \n    # Calculate the mean of each column:\n    means_list = []\n    for col in columns:\n        means_list.append(train.groupby(str(col)).target.mean())\n    \n    # Calculate the mean of each category in each column:\n    col_means = []\n    for means_series in means_list:\n        col_means.append(means_series.mean())\n    \n    # Encode the data:\n    for column, means_series, means in zip(columns, means_list, col_means):\n        val[str(column) + '_target_enc'] = val[str(column)].map(means_series).fillna(means) \n    \n    list_of_mean_enc = [str(column) + '_target_enc' for column in columns]\n    list_of_mean_enc.extend(columns)\n    \n    all_set.append(val[list_of_mean_enc].copy())\n\nX_train_te=pd.concat(all_set, axis=0)","90e52a7f":"# Apply encodings to validation set:\nX_val_te = pd.DataFrame(index=X_val.index)\nfor column, means in zip(columns, col_means):\n    enc_dict = X_train_te.groupby(column).mean().to_dict()[str(column) + '_target_enc']\n    X_val_te[column] = X_val[column].map(enc_dict).fillna(means)","590d543e":"# Create list of target encoded columns:\nlist_of_target_enc = [str(column) + '_target_enc' for column in columns]","28f6efb6":"%%time\nkf_reg_logit_score = get_score(logit, X_train_te[list_of_target_enc], y_train, X_val_te, y_val)\nprint('Logistic Regression score with kfold-regularized target encoding:', kf_reg_logit_score)","ffeadc9e":"%%time\nkf_reg_rf_score = get_score(rf, X_train_te[list_of_target_enc], y_train, X_val_te, y_val)\nprint('Random Forest score with kfold-regularized target encoding:', kf_reg_rf_score)","e273c5cd":"from category_encoders import WOEEncoder\n\nwoe_enc = WOEEncoder(cols=columns, random_state=17).fit(X_train, y_train)","69657607":"X_train_woe = woe_enc.transform(X_train.reset_index(drop=True))\nX_val_woe = woe_enc.transform(X_val.reset_index(drop=True))","2587fdfc":"X_train_woe.head()","726f57d2":"%%time\nwoe_logit_score = get_score(logit, X_train_woe, y_train, X_val_woe, y_val)\nprint('Logistic Regression score with woe encoding:', woe_logit_score)","b736a631":"%%time\nwoe_rf_score = get_score(rf, X_train_woe, y_train, X_val_woe, y_val)\nprint('Random Forest score with woe encoding:', woe_rf_score)","645414da":"Binary encoding involves converting each category into a binary code, for example 2 becomes 11 and 3 becomes 100, and then splitting the resulting binary string into columns. \n\nThis may be easier to understand with an example:","40181f01":"As a result of using the target variable, data-leakage and overfitting is a huge concern. The category_encoders implementation has two out of the box ways of regularizing the encodings, 'smoothing' and 'min_samples_leaf'. These parameters may be treated as hyperparameters.\n\n'smoothing' determines the weighting of the individual category's mean with the mean of the entire categorical variable. This is to prevent the influence of unreliable means from categories with low sample sizes.\n\n'min_samples_leaf' is the minimum number of samples within a category to take it's mean into account.","25e88ecb":"Before getting started, lets have a look at the speed and performance of training these models without any feature encoding.","6b008736":"### Further Reading:\n\n* [category_encoder documentation](http:\/\/contrib.scikit-learn.org\/categorical-encoding\/)\n* [weight of evidence](https:\/\/www.listendata.com\/2015\/03\/weight-of-evidence-woe-and-information.html)\n* [smarter ways of encoding categorical data for machine learning](https:\/\/towardsdatascience.com\/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159)\n* [an exploration of categorical variables](http:\/\/www.willmcginnis.com\/2015\/11\/29\/beyond-one-hot-an-exploration-of-categorical-variables\/)","90cff0ae":"# Categorical Feature Encoding:\n\n## Introduction:\n\nIn most data science problems, our datasets will contain categorical features. Categorical features contain a finite number of discrete values. How we represent these features will have an impact on the performance of our model. Like in other aspects of machine learning, there are no silver bullets. Determining the correct approach, specific to our model and data is part of the challenge.\n\nThis tutorial aims to cover a few of these methods. We begin by covering a straight-forward technique before tackling more complicated lesser-known approaches.\n\n**List of methods covered**:\n1. One-Hot Encoding\n2. Feature Hashing\n3. Binary Encoding\n4. Target Encoding\n5. Weight of Evidence","039f2e35":"The first method we will be covering is one that no doubt will be familiar to you. One-hot encoding expands a categorical feature made up of m categories into m* distinct features with values of either 0 or 1.\n\nThere are two ways of implementing one-hot encoding, either with pandas or scikit-learn. In this tutorial we have chosen to use the latter.\n\n*Actually, it is seen as more correct to expand m categories into (m - 1) distinct features. The reason for this is twofold. Firstly, if the values of (m - 1) features are known, the m-th feature can be inferred and secondly because including the m-th feature can cause certain linear models to become unstable. More on that can be found [here](https:\/\/www.algosome.com\/articles\/dummy-variable-trap-regression.html). In practice I think this depends on your model. Some non-linear models actually do better with m features.","6cefdec3":"In summary, categorical features may be represented in more ways than the traditional one-hot encoding. These representations have different effects on our models and the choice of representation is task specific. Feature hashing and binary encoding offer us ways of encoding the data with lower dimensions which is cheaper computationally as well as being better suited for tree-based models. Target encoding and weight of evidence encoding seem to be much more task specific. \n\nFeedback would be appreciated, as well as upvotes! Thank you.","259b4b02":"## One-Hot Encoding:","3cc90c5f":"For this tutorial, we will be using the '[Amazon.com Employee Access Challenge](https:\/\/www.kaggle.com\/c\/amazon-employee-access-challenge)' dataset. This binary classification dataset is made up of strictly categorical features, which are already converted into numerals, making it a particularly suitable choice to explore various encoding techniques. To simplify things we will only be using a subset of the features for this demonstration.","20134c1f":"Target encoding is the first of our Bayesian encoders. These are a family of encoders which take information about the target variable into account. Target encoding may refer to an encoder which considers the statistical correlation between the individual categories of a categorical feature. In this tutorial we will only look at target encoders which focus on the relationship between each category and the mean of the target as this is the most commonly used variation of target encoding.","3bd48e43":"Weight of evidence (WOE) encoder calculates the natural log of the % of non-events divided by the % of events for each category within a categotical feature. For clarification, the events are referring to the target variable. ","2910bbd6":"## Feature Hashing:","fb120595":"It improves! As we may have guessed, reducing the number of features improves the performance of tree-based models. ","990f017b":"Binary encoding is clearly very similar to feature hashing however much more restricted. In practice using feature hashing is often advised over binary encoding due to the control you have over the output dimensions.","38a6fc5c":"## Weight Of Evidence (WOE):","b2c1ee49":"As we can see, while the performance of the model has improved, training took longer as well. This is due to the increase in the number of features. Computational costs are not the only problem associated with the increase in dimensions. A dataset with more features will require a model with more parameters which in turn will require more data to train these parameters. In many cases, such as kaggle competitions, the size of our data is fixed and as a result the dimensionality of our data should always be a concern.\n\nOne way of dealing with high dimensionality is by compressing the features. Feature hashing, which we will be covering next, is an example of this.","0c37c6d4":"As we can see, performance on the Logistic Regression model improves as the number of components increase. But let us have a look at the effect of reducing the dimensions has on a Random Forest model.","2a70420a":"## Binary Encoding:","782fbffd":"Feature hashing maps each category in a categorical feature to an integer within a pre-determined range. This output range is smaller than the input range so multiple categories may be mapped to the same integer. Feature hashing is very similar to one-hot encoding but with a control over the output dimensions.\n\nTo implement feature hashing in python we can use category_encoder, a library containing sklearn compabitable category encoders.","4be69828":"We will compare differences of these encoding methods on both a linear model and tree-based model. These represent two families of models which have contrasting behaviours when it comes to different feature representations.","eab54141":"## Target Encoding:","bc901ef6":"Another approach to regularizing the target encoder is to calculate the statistic relationship between each category and the target variable via a kfold split. This method is currently not available in category_encoders implementation and needs to be written from scratch.","c858bde6":"The size of the output dimensions is controlled by the variable n_components. This can be treated as a hyperparameter."}}