{"cell_type":{"5c22aebe":"code","37b5b74a":"code","6b8cba56":"code","c5f3327e":"code","0d9acd42":"code","5b792258":"code","3a8e7207":"code","8022941a":"code","273f0e0a":"code","c9c12854":"code","4071aa01":"code","712692cf":"code","d97e9186":"code","2a676985":"code","fec2145a":"code","e160513e":"code","e7fb6e2a":"code","ef0cbb14":"code","49c14f40":"code","e12dc9a3":"code","d65ce276":"code","cd1a8657":"code","5f8011b8":"code","2745d5c4":"code","f78698f5":"code","70301e75":"code","ead89da3":"code","33df4004":"code","e0a9ec98":"code","0863858c":"code","076b81de":"code","4d93898a":"code","9b721b76":"code","4b642249":"code","d7c73067":"code","8bc07ef4":"code","d6e85c71":"code","2bf661a2":"code","0b498afd":"code","33fab92b":"code","b1414279":"code","93f52edd":"code","0b9092b2":"code","d3ebc34e":"code","6898e1b9":"code","b1ee7334":"code","c2d1c73e":"code","ebe7d375":"code","96d5e6b0":"code","940bfe68":"code","87bc2ed5":"code","efe8ecca":"code","e195d1dc":"code","49235524":"code","a22a78e9":"code","1e4e9da6":"code","34e00e24":"code","35ae66bc":"code","a37d4983":"code","4d64a2ab":"code","283cc087":"code","5701aa6b":"code","b503a86a":"code","6c73b3e9":"code","eee0b74b":"code","c672118f":"code","1d14e8d5":"code","b3ab0c70":"code","35a36f6d":"code","f1b4af71":"code","5b0677c9":"code","ef764ffc":"code","956f9107":"code","466ccfac":"code","be12b76a":"code","fce9144f":"code","1bc87f9f":"code","02062f64":"markdown","93cada00":"markdown","3c420abd":"markdown","d565332d":"markdown","7b6cd7c2":"markdown","1e8d545b":"markdown","6d63716a":"markdown","696289f2":"markdown","eb21ac3c":"markdown","25d87124":"markdown","89840b26":"markdown","0f1c48d5":"markdown","592d8221":"markdown","2e41020b":"markdown","aa4800bc":"markdown","aa55ccca":"markdown","49a659ad":"markdown","6709cf65":"markdown","6465bd82":"markdown","00e12123":"markdown","fd0ab79c":"markdown","07f3bf35":"markdown","85fb5889":"markdown","d668e7bd":"markdown","9b7497cf":"markdown","e59c61a3":"markdown","a1e2c6d3":"markdown","a91654f9":"markdown","bcb1dafb":"markdown","f0512952":"markdown","0d6c4a12":"markdown","a488c358":"markdown","6402df44":"markdown","1b0ac808":"markdown","5ef43fcd":"markdown","eabc118e":"markdown","2e87a28e":"markdown","d27e554f":"markdown","0f03e90d":"markdown","3e88c9e5":"markdown","c3c57350":"markdown","6a01877d":"markdown","9812d12a":"markdown","b974e400":"markdown","dbd6ab97":"markdown","e0628317":"markdown","86b8209b":"markdown","4bac7445":"markdown","33501afd":"markdown","c2853fc3":"markdown","67a1a32d":"markdown","c7ff7176":"markdown","6a570593":"markdown","cd0315fe":"markdown","51fd8db7":"markdown","f8ea9b31":"markdown","fd198e37":"markdown","e99b5f07":"markdown","cf80c61c":"markdown","8b4bf2b8":"markdown","16305b43":"markdown","c76f5d26":"markdown","5b9810f2":"markdown","c9536f02":"markdown","4e95cbe4":"markdown","19ecbb29":"markdown","8a99ecc7":"markdown","b5c81e6b":"markdown","1e079a1e":"markdown","f9dc34ac":"markdown","17aa0ee7":"markdown","1e8b8ac7":"markdown","b8434265":"markdown","e83b3cd2":"markdown","ffce9f4d":"markdown","c0da70ae":"markdown","571cc97a":"markdown","a3237ffa":"markdown","12d869f8":"markdown","3c0f19a4":"markdown","e45aef33":"markdown","da0f81e5":"markdown","ef292ac3":"markdown","07b834c5":"markdown","38c5962d":"markdown","d2e8e06f":"markdown","7e38d365":"markdown","47c7ac52":"markdown","fdbad474":"markdown","2d8544ed":"markdown","33072384":"markdown"},"source":{"5c22aebe":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings \nwarnings.filterwarnings('ignore')  #Code to ignore errors as version changes\nfrom collections import Counter\n\n#Modeling\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import VotingClassifier\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import learning_curve\n\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\n\nsns.set(style='white', context='notebook', palette='deep')","37b5b74a":"#Load train and test data \ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\nIDtest = test['PassengerId']","6b8cba56":"# Outlier detection \n\ndef detect_outliers(df,n,features):\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   ","c5f3327e":"Outliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])","0d9acd42":"#Show the outliers rows \ntrain.loc[Outliers_to_drop]","5b792258":"#Drop outliers\ntrain = train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","3a8e7207":"train_len = len(train)\ndataset = pd.concat(objs=[train, test], axis = 0).reset_index(drop=True)","8022941a":"dataset = dataset.fillna(np.nan)\ndataset.isnull().sum()","273f0e0a":"train.info()\ntrain.isnull().sum()","c9c12854":"#Check the train data\ntrain.head()\ntrain.dtypes\ntrain.describe()","4071aa01":"g = sns.heatmap(train[['Survived', 'SibSp', 'Parch', 'Age', 'Fare']].corr(), annot=True,fmt ='.2f', cmap='YlGnBu')","712692cf":"g = sns.factorplot('SibSp','Survived', data=train, kind='bar', height=6, palette='rocket_r')\ng = g.set_ylabels('survival probability')","d97e9186":"g = sns.factorplot('Parch', 'Survived', data=train, kind='bar', height=6, palette='rocket_r')\ng = g.set_ylabels('survived probability')","2a676985":"g = sns.FacetGrid(train, col='Survived',height = 6)\ng = g.map(sns.distplot, 'Age')\n","fec2145a":"#Explore Age distribution \nplt.figure(figsize=(16, 6))\ng = sns.kdeplot(train['Age'][(train['Survived'] == 0) & (train['Age'].notnull())], color ='Red', shade = True)\ng = sns.kdeplot(train['Age'][(train['Survived'] == 1) & (train['Age'].notnull())], ax = g, color= 'Blue', shade =True)\ng.set_xlabel('Age')\ng.set_ylabel('Frequency')\ng = g.legend(['Not Survived', 'Survived'])","e160513e":"#Let's check out the 'Fare' data. \ndataset['Fare'].isnull().sum()","e7fb6e2a":"#Fill Fare missing values with the median value\ndataset[\"Fare\"] = dataset[\"Fare\"].fillna(dataset[\"Fare\"].median())","ef0cbb14":"#Explore Fare distribution\nplt.figure(figsize=(8, 6))\ng = sns.distplot(dataset[\"Fare\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")","49c14f40":"#Apply log to Fare to reduce skewness distribution\ndataset['Fare'] = dataset['Fare'].map(lambda i: np.log(i) if i > 0 else 0)","e12dc9a3":"plt.figure(figsize=(8, 6))\ng = sns.distplot(dataset['Fare'], color='b', label ='Skewness : %.2f'%(dataset['Fare'].skew()))\ng = g.legend(loc='best')","d65ce276":"#Let's check the Sex data \nplt.figure(figsize=(8, 6))\ng = sns.barplot('Sex', 'Survived', data=train,palette = 'rocket_r')\ng = g.set_ylabel('Survival Probability')","cd1a8657":"train[['Sex', 'Survived']].groupby('Sex').mean()","5f8011b8":"f, ax = plt.subplots(1,2, figsize=(18,8))\ntrain[['Sex', 'Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Sex')\n\nsns.countplot('Sex', hue='Survived', data=train, ax=ax[1])\nax[1].set_title('Sex:Survived vs Dead')\nplt.show()","2745d5c4":"g = sns.factorplot('Pclass', 'Survived', data=train, kind='bar', height=6, palette = 'rocket_r')\ng.despine(left=True)\ng = g.set_ylabels('survival probability')","f78698f5":"g = sns.factorplot('Pclass', 'Survived', hue='Sex', data=train, height=6, kind='bar', palette='rocket_r')\ng = g.set_ylabels('survival probability')","70301e75":"#Let's check the embarked null data \ndataset['Embarked'].isnull().sum()","ead89da3":"dataset['Embarked'] = dataset['Embarked'].fillna('S')","33df4004":"g = sns.factorplot('Embarked', 'Survived', data=train, height=6, kind='bar', palette='rocket_r')\ng = g.set_ylabels('Survival probability')","e0a9ec98":"g = sns.factorplot('Pclass', col = 'Embarked', data=train, height=6, kind='count', palette='rocket_r')\ng = g.set_ylabels('Count')","0863858c":"#Put y = 'Age', x = 'Sex' to see better. \ng = sns.factorplot(y=\"Age\",x=\"Sex\",data=dataset,kind=\"box\",palette='rocket_r')\ng = sns.factorplot(y=\"Age\",x=\"Sex\",hue=\"Pclass\", data=dataset,kind=\"box\",palette='rocket_r')\ng = sns.factorplot(y=\"Age\",x=\"Parch\", data=dataset,kind=\"box\",palette='rocket_r')\ng = sns.factorplot(y=\"Age\",x=\"SibSp\", data=dataset,kind=\"box\",palette='rocket_r')","076b81de":"dataset['Sex'] = dataset['Sex'].map({'male':0, 'female':1})","4d93898a":"g =sns.heatmap(dataset[['Age','Sex','SibSp','Parch','Pclass']].corr(), cmap='YlGnBu',annot=True)","9b721b76":"#Fill Age with the median age of similar rows according to Pclass, Parch, SibSp\n#Index of NaN age rows \nindex_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\n\nfor i in index_NaN_age:\n    age_med = dataset['Age'].median()\n    age_pred = dataset['Age'][((dataset['SibSp'] == dataset.iloc[i]['SibSp']) & \n                              (dataset['Parch'] == dataset.iloc[i]['Parch']) &\n                              (dataset['Pclass'] == dataset.iloc[i]['Pclass']))].median()\n    if not np.isnan(age_pred):\n        dataset['Age'].iloc[i] = age_pred\n    else:\n        dataset['Age'].iloc[i] = age_med","4b642249":"g = sns.factorplot('Survived', 'Age', data=train, kind='box', palette='rocket_r')\ng = sns.factorplot('Survived', 'Age', data=train, kind='violin', palette='rocket_r')","d7c73067":"dataset['Name'].head()","8bc07ef4":"#Get title from Name \ndataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]]\ndataset[\"Title\"] = pd.Series(dataset_title)\ndataset[\"Title\"].head()","d6e85c71":"f, ax = plt.subplots(1,1, figsize=(18,8))\n\ng = sns.countplot('Title', data=dataset, palette='rocket_r')\ng = plt.setp(g.get_xticklabels(), rotation=45)","2bf661a2":"# Convert to categorical values Title \ndataset[\"Title\"] = dataset[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndataset[\"Title\"] = dataset[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ndataset[\"Title\"] = dataset[\"Title\"].astype(int)","0b498afd":"f, ax = plt.subplots(1,1, figsize=(18,8))\n\ng = sns.countplot(dataset['Title'], palette = 'rocket_r')\ng = g.set_xticklabels(['Master', 'Miss\/Ms\/Mme\/Mlle\/Mrs','Mr','Rare'])","33fab92b":"g = sns.factorplot('Title','Survived',data=dataset, kind='bar', palette='rocket_r')\ng = g.set_xticklabels(['Master', 'Miss\/Ms\/Mme\/Mlle\/Mrs','Mr','Rare'])\ng = g.set_ylabels('survival probability')","b1414279":"# Drop Name variable\ndataset.drop(labels = [\"Name\"], axis = 1, inplace = True)","93f52edd":"#Create a family size\ndataset['Fsize'] = dataset['SibSp'] + dataset['Parch'] + 1","0b9092b2":"g = sns.factorplot('Fsize', 'Survived', data=dataset, palette='rocket_r')\ng = g.set_ylabels('Survival Probability')","d3ebc34e":"#Create new feature of family size \ndataset['Single'] = dataset['Fsize'].map(lambda s:1 if s ==1 else 0)\ndataset['SmallF'] = dataset['Fsize'].map(lambda s:1 if s ==2 else 0)\ndataset['MedF'] = dataset['Fsize'].map(lambda s:1 if 3 <= s <= 4 else 0)\ndataset['LargeF'] = dataset['Fsize'].map(lambda s:1 if s >= 5 else 0)","6898e1b9":"g = sns.factorplot(x=\"Single\",y=\"Survived\",data=dataset,kind=\"bar\", palette='rocket_r')\ng = g.set_ylabels(\"Survival Probability\")\n\ng = sns.factorplot(x=\"SmallF\",y=\"Survived\",data=dataset,kind=\"bar\", palette='rocket_r')\ng = g.set_ylabels(\"Survival Probability\")\n\ng = sns.factorplot(x=\"MedF\",y=\"Survived\",data=dataset,kind=\"bar\", palette='rocket_r')\ng = g.set_ylabels(\"Survival Probability\")\n\ng = sns.factorplot(x=\"LargeF\",y=\"Survived\",data=dataset,kind=\"bar\", palette='rocket_r')\ng = g.set_ylabels(\"Survival Probability\")","b1ee7334":"#Convert to indicate values Title and Embarked \ndataset = pd.get_dummies(dataset, columns=['Title'])\ndataset = pd.get_dummies(dataset, columns =['Embarked'])","c2d1c73e":"#Check the dataset. We can see Title, Embarked have been added \ndataset.head()","ebe7d375":"dataset['Cabin'].describe()","96d5e6b0":"dataset['Cabin'].isnull().sum()","940bfe68":"dataset['Cabin'][dataset['Cabin'].notnull()].head()","87bc2ed5":"#Replace the Cabin number to the type 'X' \ndataset['Cabin'] = pd.Series(i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin'])","efe8ecca":"g = sns.countplot(dataset['Cabin'],order=['A','B','C','D','E','F','G','T','X'], palette='rocket')","e195d1dc":"g = sns.factorplot(y ='Survived', x ='Cabin', data=dataset, kind='bar',order=['A','B','C','D','E','F','G','T','X'], palette='rocket_r')\ng = g.set_ylabels('Survival Probability')","49235524":"dataset = pd.get_dummies(dataset,columns = ['Cabin'], prefix ='Cabin')","a22a78e9":"dataset['Ticket'].head","1e4e9da6":"#Treat Ticket by extracting the ticket prefix. When there is no prefix it returns X. \nTicket = []\nfor i in list(dataset.Ticket):\n    if not i.isdigit() :\n        Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket.append(\"X\")\n        \ndataset[\"Ticket\"] = Ticket\ndataset[\"Ticket\"].head()","34e00e24":"dataset = pd.get_dummies(dataset, columns = [\"Ticket\"], prefix=\"T\")","35ae66bc":"dataset.describe()","a37d4983":"#Create categorical values for Pclass \ndataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"category\")\ndataset = pd.get_dummies(dataset, columns = [\"Pclass\"],prefix=\"Pc\")","4d64a2ab":"#Drop useless variables \ndataset.drop(labels = ['PassengerId'], axis = 1, inplace=True)","283cc087":"#Separate\ntrain = dataset[:train_len]\ntest = dataset[train_len:]\n\ntest.drop(labels=[\"Survived\"],axis = 1,inplace=True)\n","5701aa6b":"#Separate train features and label \ntrain['Survived'] = train['Survived'].astype(int)\n\ny_train = train['Survived']\nx_train = train.drop(labels = ['Survived'], axis = 1)","b503a86a":"kfold = StratifiedKFold(n_splits = 10)","6c73b3e9":"random_state = 42\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())","eee0b74b":"cv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, x_train, y = y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\n                       \"CrossValerrors\": cv_std,\n                       \"Algorithm\":[\"SVC\",\n                                    \"DecisionTree\",\n                                    \"AdaBoost\",\n                                    \"RandomForest\",\n                                    \"ExtraTrees\",\n                                    \"GradientBoosting\",\n                                    \"MultipleLayerPerceptron\",\n                                    \"KNeighboors\",\n                                    \"LogisticRegression\",\n                                    \"LinearDiscriminantAnalysis\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"rocket_r\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","c672118f":"#SVC classifier\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'],\n                  'gamma': [0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100, 200, 300, 1000]}\n\ngsSVMC = GridSearchCV(SVMC, param_grid = svc_param_grid, cv=kfold, scoring='accuracy', n_jobs=4,verbose=1)\ngsSVMC.fit(x_train,y_train)\n\nSVMC_best = gsSVMC.best_estimator_\n\n#Best score\ngsSVMC.best_score_","1d14e8d5":"#Adaboost \nDTC = DecisionTreeClassifier()\n\nadaDTC = AdaBoostClassifier(DTC, random_state=42)\n\nada_param_grid = {'base_estimator__criterion':['gini','entropy'],\n                  'base_estimator__splitter':['best','random'],\n                  'algorithm': ['SAMME','SAMME.R'],\n                  'n_estimators': [1,2],\n                  'learning_rate': [0.0001, 0.001,0.01,0.1,0.2,0.3,1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC, param_grid = ada_param_grid, cv=kfold, scoring='accuracy', n_jobs=4,verbose=1)\ngsadaDTC.fit(x_train,y_train)\n\nada_best = gsadaDTC.best_estimator_\n\ngsadaDTC.best_score_","b3ab0c70":"#RandomForest\nRFC = RandomForestClassifier()\n\nrf_param_grid = {'max_depth': [None],\n                 'max_features': [1,3,10],\n                 'min_samples_split': [2,3,10],\n                 'min_samples_leaf': [1,3,10],\n                 'bootstrap': [False],\n                 'n_estimators': [100,300],\n                 'criterion': ['gini']}\n\ngsRFC = GridSearchCV(RFC, param_grid = rf_param_grid, cv=kfold, scoring='accuracy', n_jobs=4,verbose=1)\ngsRFC.fit(x_train,y_train)\n\nRFC_best = gsRFC.best_estimator_\n\ngsRFC.best_score_","35a36f6d":"#ExtraTree\nExtC = ExtraTreesClassifier()\n\nex_param_grid = {'max_depth': [None],\n                 'max_features': [1,3,10],\n                 'min_samples_split': [2,3,10],\n                 'min_samples_leaf': [1,3,10],\n                 'bootstrap': [False], \n                 'n_estimators': [100,300],\n                 'criterion': ['gini']}\n\ngsExtC = GridSearchCV(ExtC, param_grid = ex_param_grid, cv=kfold, scoring='accuracy', n_jobs =4, verbose=1)\n\ngsExtC.fit(x_train, y_train)\n\nExtC_best = gsExtC.best_estimator_\n\ngsExtC.best_score_","f1b4af71":"GBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(x_train,y_train)\n\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_","5b0677c9":"def plot_learning_curve(estimator, title, x, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    \n    plt.xlabel('Training examples')\n    plt.ylabel('Score')\n    train_sizes, train_scores, test_scores = learning_curve(estimator, x, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","ef764ffc":"g = plot_learning_curve(gsSVMC.best_estimator_,\"SVC learning curves\",x_train,y_train,cv=kfold)\ng = plot_learning_curve(gsadaDTC.best_estimator_,\"AdaBoost learning curves\",x_train,y_train,cv=kfold)\ng = plot_learning_curve(gsRFC.best_estimator_,\"RF mearning curves\",x_train,y_train,cv=kfold)\ng = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",x_train,y_train,cv=kfold)\ng = plot_learning_curve(gsGBC.best_estimator_,\"GradientBoosting learning curves\",x_train,y_train,cv=kfold)","956f9107":"nrows = ncols = 2\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15))\n\nnames_classifiers = [(\"AdaBoosting\", ada_best),\n                     (\"ExtraTrees\",ExtC_best),\n                     (\"RandomForest\",RFC_best),\n                     (\"GradientBoosting\",GBC_best)]\n\nnclassifier = 0\nfor row in range(nrows):\n    for col in range(ncols):\n        name = names_classifiers[nclassifier][0]\n        classifier = names_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n        \n        g = sns.barplot(y=x_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col], palette='rocket_r')\n        g.set_xlabel(\"Relative importance\",fontsize=12)\n        g.set_ylabel(\"Features\",fontsize=12)\n        g.tick_params(labelsize=9)\n        g.set_title(name + \" feature importance\")\n        nclassifier += 1","466ccfac":"test_Survived_RFC = pd.Series(RFC_best.predict(test), name=\"RFC\")\ntest_Survived_ExtC = pd.Series(ExtC_best.predict(test), name=\"ExtC\")\ntest_Survived_SVMC = pd.Series(SVMC_best.predict(test), name=\"SVC\")\ntest_Survived_AdaC = pd.Series(ada_best.predict(test), name=\"Ada\")\ntest_Survived_GBC = pd.Series(GBC_best.predict(test), name=\"GBC\")\n\n\n# Concatenate all classifier results\nensemble_results = pd.concat([test_Survived_RFC,test_Survived_ExtC,test_Survived_AdaC,test_Survived_GBC, test_Survived_SVMC],axis=1)\n\n\ng= sns.heatmap(ensemble_results.corr(),annot=True)","be12b76a":"RFC_best","fce9144f":"votingC = VotingClassifier(estimators=[('rfc', RFC_best), \n                                       ('extc', ExtC_best),\n                                       ('svc', SVMC_best), \n                                       ('adac',ada_best),\n                                       ('gbc',GBC_best)], voting='soft', n_jobs=4)\n\nvotingC = votingC.fit(x_train, y_train)\n","1bc87f9f":"test_Survived = pd.Series(votingC.predict(test), name=\"Survived\")\n\nresults = pd.concat([IDtest,test_Survived],axis=1)\n\nresults.to_csv(\"my_first_submission.csv\",index=False)","02062f64":"* It seems that passenger coming from Cherbourg(C) have more chance to survive. \n* My hypothesis is that the proportion of first class passengers is higher for those who came from Cherbourg than Queenstown(Q), Southampton(S).","93cada00":"### Conclusion:\nSo we are going to use **Name\/Title, Family Size, Cabin, Ticket** for the modeling. Check the dataset.head()","3c420abd":"####  2)AdaBoost","d565332d":"### 2.3 Joining train and test set ","7b6cd7c2":"Factorplots of family size categories show that Small and Medium families have more chance to survive than single passenger and large families. ","1e8d545b":"### 2.2 Outlier Detection\n\nOutlier is a data that differs from other observations. An outlier can cause serious problems in statistical analysis, for example it can cause 'measurment error' or heavy-tailed distribution' problem.So we have to detect the outlier before we make our model. \n\n**IQR(interquartile range)** is often used to find outliers in data. IQR is a measure of variability, based on dividing a data set into quartiles. \n\nWe used the **Tukey's method**. This method is a single-step multiple comparison preocedure and statistical test. It can be used to find means that are significantly different from each other.\n\n","6d63716a":"I decided to choose the SVC, AdaBoost, RandomForest, ExtraTrees and the GradientBoosting classifiers for the ensemble modeling. ","696289f2":"We can say that: \n\n* Pc_1, Pc_2, Pc_3 and Fare refer to the general social standing of passengers. \n* Sex and Title_2(Mrs\/Mlle\/Mme\/Miss\/Ms) and Title_3(Mr) refer to the gender. \n* Age and Title_1(Master) refer to the age of passengers. \n* Fsize, LargeF, MedF, Single refer to the size of the passenger family. ","eb21ac3c":"### 2.1 Load Data","25d87124":"## Inner Working of Cross Validation <a id =\"2\"><\/a>\n\n* Shuffle the dataset in order to remove any kind of order\n* Split the data into K number of folds. K= 5 or 10 will work for most of the cases.\n* Now keep one fold for testing and remaining all the folds for training.\n* Train(fit) the model on train set and test(evaluate) it on test set and note down the results for that split\n* Now repeat this process for all the folds, every time choosing separate fold as test data\n* So for every iteration our model gets trained and tested on different sets of data\n* At the end sum up the scores from each split and get the mean score\n\n![Inner_Working_KFold](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/Inner_Working_KFold.png)\n\n## K Fold Cross Validation <a id =\"3\"><\/a>\n\nIn case of K Fold cross validation input data is divided into 'K' number of folds, hence the name K Fold. Suppose we have divided data into 5 folds i.e. K=5. Now we have 5 sets of data to train and test our model. So the model will get trained and tested 5 times, but for every iteration we will use one fold as test data and rest all as training data. Note that for every iteration, data in training and test fold changes which adds to the effectiveness of this method. \n\nThis significantly reduces underfitting as we are using most of the data for training(fitting), and also significantly reduces overfitting as most of the data is also being used in validation set. K Fold cross validation helps to generalize the machine learning model, which results in better predictions on unknown data. To know more about underfitting & overfitting please refer [this](https:\/\/satishgunjal.com\/underfitting_overfitting\/) article.\n\nFor most of the cases 5 or 10 folds are sufficient but depending on problem you can split the data into any number of folds.\n\n![KFold_Cross_Validation](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/KFold_Cross_Validation.png)","89840b26":"####  5)GradientBooster","0f1c48d5":"We note that the four classifiers have different top features according to the relative importance. It means that their predictions are not based on the same features. Nevvertheless, they share some common important features for the classification. \n\n**'Fare', 'Title_2'(Mrs\/Mlle\/Mme\/Miss\/Ms), 'Age', 'Sex' seems to be the most relative importance.** ","592d8221":"* Survival probability is worst in the largest familiy size. \n* We decided to group them by in 4 categories, according to their family size.  \n\n**Create new 4 feature**","2e41020b":"### 6.1 Simple modeling\n\n### 6.1.1 Cross validation models \nCompare 10 popular classifiers and evaluate the mean accuracy of each of them by a stratified k-fold cross validation procedure. \n\nCheck out the explanation about 'Cross Validation'. \n\n**@Satish Gunjal** Thanks for a good explation about cross validation. ","aa4800bc":"* We can check that age is not correlated with 'Sex', but is negatively correlated with 'Pclass, Parch, SibSp'.\n* 'Age' & 'Parch', we can check that age is growing with the number of parents\/children. But the general correlation is negative. \n* We will going to use 'SibSp, Parch, Pclass' since it is positive and going to input the missing ages data. \n\nAge will be filled with the mdeian age of similar rows according to Pclass, Parch and SibSp. ","aa55ccca":"#### Filling missing value of Age ","49a659ad":"\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">\ud83d\udcdaGuide to Feature Analysis & Ensemble modeling   <\/p>","6709cf65":"#### Age distribution","6465bd82":"### 6.1.2 Hyperparameter tunning for best models","00e12123":"*  It could mean that tickets sharing the same prefixes could be booked for cabins placed together. It could therfore lead to the actual placement of the cains within the ship. \n*  Tickets with same prefixes may have a similar class and survival. \n*  So I decided to replace the Ticket feature column by the ticket prefix. Which may be more informative. ","fd0ab79c":"### 2.4 Checking null & missing values ","07f3bf35":"## 4. Filling missing Values\n\n### 4.1 Age\nAs we see, age column contains 256 missing values in the whole dataset. \n\nSince there is subpopulations that have more chance to survive(children for example), it is preferable to keep the age feature and to impute the missing values. \n\nTo adress this problem, I looked at the most correlated features with Age(Sex, Parch, Pclass, ShibSp)\n\nLet's explore age vs (sex, parch, pclass, sibsp)","85fb5889":"Let's make other plot, since we have changed to the log function.","d668e7bd":"Skewness is clearly reduced after the log transformation ","9b7497cf":"Detect outliers from **Age, SibSp, Parch and Fare**","e59c61a3":"### Age vs ( Sex, Parch, Pclass, SibSP)","a1e2c6d3":"### Age: Age vs Survived","a91654f9":"* Beacuse of the low number of passenger that have cabin, survival probabilities have an important standard deviation and we can't distinguish between survival probability of passengers in the different desks.  \n* But we can see that passengers with a cabin have generally more chance to survive than passengers without X. ","bcb1dafb":"* Since outliers can have a dramatic effect on the prediction(especially for regression problems), we choosed to manage it. \n \n* Used te Tukey method to detect outliers which defines an interquartile range comprised between the 1st and 3rd quartile of the distribution values. An outlier is a row that have a feature value outside the(IQR + an outlier step). \n \n* Detected outliers from the numerical values features(Age, SibSp, Sarch and Fare). Then considered outliers as rows that have at least two outlied numerical values. \n ","f0512952":"* We detect 10 outliers. The 28, 89, 342 passenger have a high Ticket Fare\n* The 7 others have very high values of SibSp.","0d6c4a12":"We performed 'grid search optimization' for SVC clssifiers, AdaBoost,RandomForest,ExtraTrees, GradientBoosting.\n\n+I set the \"n_jobs\" parameter to 4 since i have 4 cpu . The computation time is clearly reduced. But be carefull, this step can take a long time, i took me 15 min in total on 4 cpu","a488c358":"Thanks for @Yassine Ghouzam for making a such good notebook for study. I added more explanations and details for Beginner. Let's study together by using this notebook together!!!\n\nPleas give me a upvote! It would be really motivated for me!! ","6402df44":"* It is clearly obvious that the Male have less chance to survive than Female. \n* So, we can think that 'Sex' might play an important role in the predcition of the survival. ","1b0ac808":"Convert Sex into categorical value 0 for male and 1 for female. ","5ef43fcd":"### Embarked ","eabc118e":"* We notice that age distributions are not the same in the survived and not survived subpopulations. Indeed, there is a peak corresponding to young passengers, that have survived. We also see that passengers between 60-80 hve less survived. \n* Even if 'Age' is not correlated with 'Survived', we can see that there is age categories of passengers that of have more or less chance to survive.  \n* It seems that very young passengers, under 10 had more chance to survive. ","2e87a28e":"### Pclass: Pclass vs Survived by Sex ","d27e554f":"### 5.4 Ticket","0f03e90d":"## 6. Modeling\n\nFirst separate train dataset and test dataset by the len of the train data. ","3e88c9e5":"* **SVC(Support Vector Classifier)**: Fit to the data you provide, returning a 'best fit' hyperplane that divides, or categorizes. \n* **AdaBoost**: It combines multiple classifiers to increase the accuracy of classifiers. It is an iterative ensemble method.\n* **RandomForest**: It is consists of a large number of individual decision trees that operate as an ensemble.\n* **ExtraTrees**:  Implements a meta estimator that fits a number of randomized decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n* **GradientBoosting**: Implementation of gradient boosted decision trees designed for speed and performance.\n","c3c57350":"### 6.1.3 Plot Learning Curves \n Learning curves are a good way to see the overfitting effect on the training set and the effect of the training size on the accuracy. ","6a01877d":"Fill Embarked nan values of dataset set with 'S', since it is most frequent value. ","9812d12a":"## 3. Feature analysis","b974e400":"* The passenger survival is not the same in the 3 classes. First class passengers have more chance to survive than second class and third class passengers. \n* This trend is conserved when we look at both male and female passengers.","dbd6ab97":"* Indeed, the third class is the most frequent for passenger coming from Southampton(S) and Queenstown(Q), whereas Cherbourg passengers are mostly in first class which have the highest survival rate. \n* At the point, I can't explain why first class has an higher survival rate. My hypothesis is that first class passengers were prioritised during the evacuation due to their influence.","e0628317":"## 3.2 Categorical Values\n\n### Sex ","86b8209b":"####  4)ExtraTree","4bac7445":"### Embarked vs Survived ","33501afd":"Let's look at the correlation between 'SibSp, Parch, Age, Fare, Values, Survived' using heatmap","c2853fc3":"### 6.2 Ensemble Modeling\n\n### 6.2.1. Combining Models\nI choosed a voting classifier to combine the predictions coming from the 5 classifiers. \n\nI preferred to pass the argument 'soft' to the voting parameter to take into account the probability ef each vote. ","67a1a32d":"There is lot's of title in the dataset, and we can group them into 4 groups. ","c7ff7176":"### 6.3 Prediction\n \n### 6.3.1. Predict and Submit results","6a570593":"### This notebook follows 3 main parts: \n \n1) Feature analysis \n\n2) Feature engineering\n\n3) Modeling \n\n","cd0315fe":"First create cross-validate model with K-fold stratified cross val ","51fd8db7":" **1 Import**\n \n **2 Load and check data**\n    * 2.1 load data\n    * 2.2 Outlier detection\n    * 2.3 joining train and test set\n    * 2.4 check for null and missing values\n **3 Feature analysis**\n    * 3.1 Numerical values\n    * 3.2 Categorical values\n **4 Filling missing Values**\n    * 4.1 Age\n **5 Feature engineering**\n    * 5.1 Name\/Title\n    * 5.2 Family Size\n    * 5.3 Cabin\n    * 5.4 Ticket\n **6 Modeling**\n    * 6.1 Simple modeling\n        * 6.1.1 Cross validate models\n        * 6.1.2 Hyperparamater tunning for best models\n        * 6.1.3 Plot learning curves\n        * 6.1.4 Feature importance of the tree based classifiers\n    * 6.2 Ensemble modeling\n        * 6.2.1 Combining models\n    * 6.3 Prediction\n        * 6.3.1 Predict and Submit results\n>     ","f8ea9b31":"### Fare","fd198e37":"Let's see the Pclass distribution vs Embarked \n\n### Pclass vs Embarked ","e99b5f07":"## 2. Load & Check Data","cf80c61c":"####  3)RandomForest","8b4bf2b8":"The first letter of the cabin indicates the Desk, so we choosed to keep this information only, since it indicates the probable location of the passenger in the Titanic. ","16305b43":"#### Meta Modeling\n\n#### 1)SVC classifier","c76f5d26":"### **< 10 popular classifiers >**\n \n* SVC\n* Decision Tree\n* AdaBoost\n* Random Forest\n* Extra Trees\n* Gradient Boosting\n* Multiple layer perceprton (neural network)\n* KNN\n* Logistic regression\n* Linear Discriminant Analysis","5b9810f2":"When we fold up the two densities, we clearly see a peak corresponsing to babies and very young children. ","c9536f02":"### 6.1.4. Feature importance of tree based classifiers \nIn order to see the most informative features for the prediction of passengers survival, we displayed the feature importance for the 4 tree based classifiers. ","4e95cbe4":"### Parch: Parch vs Survived ","19ecbb29":"* **Age** and **Cabin** features have an important part of missing values. \n* We don't have to care about the **'Survived'** null values. Because only the train set have the Survived data. It is recorded as 418 since we concatenate the train and test set ","8a99ecc7":"* GradientBoosting and Adaboost classifiers tend to overfit the training set. According to the growing cross-validation curves GradientBoosting and Adaboost could perform better with more training examples.\n* SVC and ExtraTrees classifiers seem to better generalize the prediction since the training and cross-validation curves are close together.","b5c81e6b":"Only fare feature seems to have a significative correlation with the survival probability. \n\nIt doesn't mean that the other features are not usefull. Subpopulations in these features can be correlated with the survival. To determine this, we need to explore in detail theses features. ","1e079a1e":"**Be careful of overfitting!**\nAs we can see, Fare distribution is very skewed. This can lead to overweight very high values in the model, even if it is scaled. \n\nIt is better to transform it with the **log function** to reduce this skew. ","f9dc34ac":"* We can also see that children and the women survived much more than a man,\n* Next, we drop the labels 'Name', since we are not going to use all of the name. We will just going to use the label 'Title' instead.","17aa0ee7":"* Small families have more chance to survive, more than single(Parch 0), medium(Parch4), large(Parch5,6).\n* There is an important standard deviation in the survival of passengers with 3 parents\/children","1e8b8ac7":"Test differents algorithms by using modeling step ","b8434265":" For **hyperparameter** tuning or to find the best model we have to run the model against multiple combination of parameters and features and record score for analysis. To do this we can use sklearns 'cross_val_score' function. This function evaluates a score by cross-validation, and depending on the scores we can finalize the hyperparameter which provides the best results. Similarly, we can try multiple model and choose the model which provides the best score.","e83b3cd2":"Since the name of the passengers are vary, we have to make them distinguse by adding a title to the name ","ffce9f4d":"* We can check that there is no difference between median value of age in survived and not survived subpopulation. \n* But we can check that very young passengers have more survival rate in the violin plot. ","c0da70ae":"Plot the feature importance for the 4 tree based classifiers. \n\n[ Adaboost, ExtraTrees, RandomForest, GradientBoosting ]","571cc97a":"* Age distribution seems to be the same in Male and Female subpopulations, so Sex is not informative to predict Age. \n* However, 1st class passengers are older than 2nd class passengers who are also older than 3rd class passengers. \n* Moreover, the more a passengers has parents\/children the older he is and the more a passenger has sibilings\/spouses the younger he is ","a3237ffa":"* We can find out that passengers who have a lot of siblings\/spouses have less chance to survive \n* 1, 2 SibSp have more chance to survive thatn 3,4. \n* We can consider a new feature describing these categories in feature engineering. ","12d869f8":"**According to the feature importance of this 4 classifiers, the prediction of the survival seems to be more associated with the Age, the Sex, the family size and the social standing of the passengers more than the location in the boat**","3c0f19a4":"### 5.3 Cabin","e45aef33":"**Grid search** is essentially an optimization algorithm which lets you select the best parameters for your optimization problem from a list of parameter options that you provide, hence automating the 'trial-and-error' method.","da0f81e5":"## 5.1 Feature Engineering \nIn the Feature Engineering, we will going to choose the feature that we are going to use in machine learning. \n### 5.1 Name & Title ","ef292ac3":"We can find there is a one missing values. So let's fill out the null data with the median value which might be not the important effect on the prediction. ","07b834c5":"Cabin feature have 292 values and 1007 Null values. \nWe can suppose that passengers without a cabin have a missing value displayed instead of the cabin number. ","38c5962d":"Age distribution seems to be a tailed distribution, maybe a gaussian distribution. \n+ tailed distribution: probability distributions whose tails are not exponentially bounded\n+ gaussian distribution: a normal distribution is a type of continuous probability distribution for a real-valued random variable. ","d2e8e06f":"### SibSP: SibSp vs Survived\n","7e38d365":"### Pclass: Pclass vs Survived","47c7ac52":"## 1. Import ","fdbad474":"* Join train and test datasets in order to obtain the same number of features during categorical conversion. \n* This process is to obtain the same number of geatures during categorical conversion in  **feature engineering**","2d8544ed":"### 5.2 Family Size \nWe found that larger families have less survival rate. \n\nSo we choosed to create a 'Family Size' feature which is sum of **SibSp, Parch and 1(inculding the passenger).\n\nSibSp + Parch + 1**","33072384":"### 3.1 Numerical values"}}