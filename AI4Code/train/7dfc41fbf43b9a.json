{"cell_type":{"3c51498e":"code","dd3a9a66":"code","bedd5e3c":"code","dd00c4c2":"code","a8b88768":"code","1ba52457":"code","3875d1ba":"code","8551baae":"code","46666ddf":"code","abf19dff":"code","e707f028":"code","a1535521":"code","f11d57e7":"code","b5541023":"code","082061e3":"code","f41fdc95":"code","42dc8bad":"code","c8c82d18":"code","798fb916":"code","9b55bdd0":"code","7fe7b773":"code","35977e46":"code","8015271e":"code","f374de6c":"code","91200bae":"code","9ba222b6":"code","38ffd5c0":"code","b36b80c7":"code","193ced87":"code","caef5c8f":"code","abd7cd0d":"code","6e8f10a9":"code","8b3f90cd":"code","b5ce4f24":"code","d25a75d0":"code","7de556c7":"code","d90ca748":"code","83c3a6cd":"code","0be89cb6":"code","52813112":"code","7903b61e":"code","d4d62725":"code","afb4f8c0":"code","d3364975":"code","b4db76a9":"code","98f02703":"code","6df9b11f":"code","82afbcc8":"code","6f809aef":"code","579bce00":"code","26ee23af":"code","59af3abe":"code","5a4efc4c":"code","e6639586":"code","727a9f97":"code","73227f7d":"code","29c4bf23":"code","ee47530e":"code","404d767f":"markdown","64f90950":"markdown","cb6bb762":"markdown","cef56731":"markdown","2dd86389":"markdown","afd25698":"markdown","5314bf11":"markdown","9f0062f7":"markdown","7a865ca3":"markdown","314392d3":"markdown","f35f4df5":"markdown","a440d952":"markdown","8eaf58ab":"markdown","c6609ef8":"markdown","24b71555":"markdown","e617ccdc":"markdown","0212a1b0":"markdown","a8a5e04a":"markdown","c1cd763e":"markdown","e28e5ccf":"markdown","da0f050a":"markdown","c10fc3f6":"markdown","2ad4454f":"markdown","14153a87":"markdown","579e5295":"markdown","8718e018":"markdown","0f3dd518":"markdown","2fd6d88c":"markdown","01081fed":"markdown","d53dd673":"markdown","84ecdea1":"markdown","372ad49e":"markdown","7f3635e5":"markdown"},"source":{"3c51498e":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","dd3a9a66":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","bedd5e3c":"df = pd.read_csv('..\/input\/graduate-admissions\/Admission_Predict.csv')","dd00c4c2":"df.head()","a8b88768":"df.columns","1ba52457":"df = df.rename(columns={'LOR ':'LOR','Chance of Admit ':'Chance of Admit'})","3875d1ba":"df.columns","8551baae":"for col in df.columns:\n    print(col+' has '+ str(df[col].isnull().sum()) + ' null values')","46666ddf":"df.info()","abf19dff":"df.describe()","e707f028":"for col in df.columns:\n    print(col + ' ' + str(len(df[col].value_counts())))","a1535521":"df.info()","f11d57e7":"catList = ['Research','SOP','LOR','University Rating']\nfor col in catList:\n    print(df[col].unique())","b5541023":"for col in df.columns:\n    plt.figure()\n    sns.distplot(df[col])","082061e3":"for col in df.columns:\n    if col != 'Chance of Admit':\n        plt.figure()\n        sns.relplot(x=col,y='Chance of Admit',data=df)\n        plt.ylabel('Chance of Admit')\n        plt.xlabel(col)","f41fdc95":"for col in catList:\n    plt.figure()\n    sns.catplot(x=col,y='Chance of Admit',kind='bar',data=df)","42dc8bad":"dfCorr = df.corr()\ndfCorr['Chance of Admit'].sort_values(ascending=False)","c8c82d18":"plt.figure(figsize=(10,10))\nsns.heatmap(dfCorr,annot=True,linewidths=.5,cmap=\"magma\",fmt='.2f')","798fb916":"def detect_outliers(df,features):\n    outlier_indices = []\n    \n    for col in features:\n        #1st quartile\n        Q1 = np.percentile(df[col],25)\n        #3rd quartile\n        Q3 = np.percentile(df[col],75)\n        #Interquartile range\n        IQR = Q3-Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        #list of indices of outliers\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | \n                             (df[col] > Q3 + outlier_step)].index\n        \n        outlier_indices.extend(outlier_list_col)\n        \n    return outlier_indices","9b55bdd0":"outliers_to_drop = detect_outliers(df,df.columns)\noutlierDf = df.loc[outliers_to_drop]","7fe7b773":"df = df.drop(outliers_to_drop,axis=0).reset_index(drop=True)","35977e46":"df.head()","8015271e":"df = df.drop(['Serial No.'], axis=1)","f374de6c":"# Feature Importance\nX = df.iloc[:,:7]\nY = df.iloc[:,-1]\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nmodel = ExtraTreesRegressor()\nmodel.fit(X,Y)\nprint(model.feature_importances_)\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","91200bae":"from sklearn.preprocessing import OneHotEncoder\n# saving a copy\ndfEnc = df.copy()","9ba222b6":"enc = OneHotEncoder(categorical_features=[6])\ndfEnc = enc.fit_transform(dfEnc).toarray()","38ffd5c0":"dfEnc.shape","b36b80c7":"dfEnc = pd.DataFrame(dfEnc)","193ced87":"dfEnc.head()","caef5c8f":"df.head()\ndf.rename(columns={})","abd7cd0d":"y = pd.DataFrame(dfEnc.iloc[:,-1])\nx = dfEnc.iloc[:,0:8]","6e8f10a9":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)","8b3f90cd":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","b5ce4f24":"from sklearn.linear_model import LinearRegression, Ridge,Lasso, ElasticNet \nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nmodelList = [LinearRegression, Ridge,Lasso, ElasticNet,BayesianRidge,\nSVR,SGDRegressor,KNeighborsRegressor,GaussianProcessRegressor,RandomForestRegressor,\nDecisionTreeRegressor]","d25a75d0":"RegModelScores = []\ndef predictFunc(model):\n    model=model()\n    model_name = model.__class__.__name__\n    model.fit(x_train,y_train)\n    model_score_test = model.score(x_test,y_test)\n    model_score_train = model.score(x_train,y_train)\n    model_pred = model.predict(x_test)\n    \n    plt.figure()\n    sns.distplot(y_test,hist=False,color='blue')\n    sns.distplot(model_pred,hist=False,color='red')\n    plt.xlabel(model_name)\n    RegModelScores.append([model_name,model_score_test,model_score_train])","7de556c7":"for model in modelList:\n    predictFunc(model)","d90ca748":"dfReg = pd.DataFrame(RegModelScores,columns=['model','train_score','test_score'])","83c3a6cd":"dfReg","0be89cb6":"fig,ax = plt.subplots(figsize=(20,20))\np2 = sns.catplot(ax=ax,y='model',x='train_score',data=dfReg,kind='bar')\n#p2.set_xticklabels(p2.get_xticklabels(),rotation=45)\nplt.setp(ax.get_yticklabels(),fontsize=24)\nplt.close(p2.fig)","52813112":"y_train[8] = y_train[8].apply(lambda x:1 if x>0.8 else 0)\ny_test[8] = y_test[8].apply(lambda x:1 if x>0.8 else 0)","7903b61e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import precision_score,recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score,StratifiedKFold, learning_curve","d4d62725":"conMatList = []\nprcList = []\nclRep= []\nrocDet = []\npreScore = []\nrecScore = []\nf1Score = []\nyPred = []\n\ndef getClassModel(model):\n    model = model()\n    model_name = model.__class__.__name__\n    model.fit(x_train,y_train)\n    \n    #getting prediction\n    y_pred = model.predict(x_test)\n    yPred.append([model_name,y_pred])\n    \n    # getting scores\n    \n    pre_score = precision_score(y_test,y_pred)\n    rec_score= recall_score(y_test,y_pred)\n    f1score = f1_score(y_test,y_pred)\n    \n    preScore.append([model_name,pre_score])\n    recScore.append([model_name,rec_score])\n    f1Score.append([model_name,f1score])\n    \n    ## getting confusion matrix\n    cm = confusion_matrix(y_test,y_pred)\n    matrix = pd.DataFrame(cm,columns=['predicted 0','predicted 1'],\n                         index=['Actual 0','Actual 1'])\n    conMatList.append([model_name,matrix])\n    \n     ## getting precision recall curve values\n    \n    precision, recall, thresholds = precision_recall_curve(y_test,y_pred)\n    prcList.append([model_name,precision,recall,thresholds])\n    \n    ## roc details\n    \n    fpr,tpr,thresholds = roc_curve(y_test,y_pred)\n    rocDet.append([model_name,fpr,tpr,thresholds])\n    \n    ## classification report\n    \n    classRep = classification_report(y_test,y_pred)\n    clRep.append([model_name,classRep])","afb4f8c0":"kfold = StratifiedKFold(n_splits=10)\nclassModelList = [LogisticRegression,SVC,GaussianNB,DecisionTreeClassifier\n                 ,RandomForestClassifier,KNeighborsClassifier]\n\nfor model in classModelList:\n    getClassModel(model)\n    \n","d3364975":"#getting cross validation scores for each model\ncv_results = []\nfor model in classModelList:\n    cv_results.append(cross_val_score(model(),x_train,y_train,scoring='accuracy',\n                                     cv=kfold,n_jobs=4))\ncv_means = []\ncv_std = []\n\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n    \nmodel_name = []\nfor model in classModelList:\n    modelIns = model()\n    model_name.append(modelIns.__class__.__name__)\n    \ncv_res = pd.DataFrame({\n    \"CrossValMeans\":cv_means,\n    \"CrossValErrors\":cv_std,\n    \"Model\":model_name\n})\n  \ncv_res","b4db76a9":"fig,ax = plt.subplots(figsize=(20,10))\np2 = sns.distplot(y_test,hist=False,label='test_set',ax=ax)\nfor pred in yPred:\n    sns.distplot(pred[1],hist=False,label=pred[0],ax=ax)\nplt.setp(ax.get_legend().get_texts(), fontsize='22') \n1==1\n#plt.close()","98f02703":"#conMatList,prcList ,clRep ,rocDet ,preScore ,recScore \nfor mat in conMatList:\n    print(mat[0])\n    print(' ')\n    print(mat[1])\n    print('-----------------------------------------------')","6df9b11f":"precisionDf = pd.DataFrame(preScore,columns=['model','precisionScore'])\nrecallDf = pd.DataFrame(recScore,columns=['model','recallScore'])\nf1Df = pd.DataFrame(f1Score,columns=['model','f1Score'])\nprecisionDf['f1Score'] = f1Df['f1Score']\nprecisionDf['recallScore'] = recallDf['recallScore']\nprecisionDf","82afbcc8":"for roc in rocDet:\n    print(roc[0])\n    fpr = roc[1]\n    tpr = roc[2]\n    plt.plot(fpr,tpr,label=roc[0])\n    plt.legend()\n","6f809aef":"for prc in prcList:\n    precision = prc[1]\n    recall = prc[2]\n    plt.plot(precision,recall,label=prc[0])\n    plt.legend()","579bce00":"lreg = LinearRegression()\nlreg.fit(x_train,y_train)","26ee23af":"#saving model\nimport pickle\npkl_Filename = \"regModel\"\n\nwith open(pkl_Filename, 'wb') as file:\n    pickle.dump(lreg,file)","59af3abe":"#cheking if model saved works\nwith open(pkl_Filename, 'rb') as file: \n    print(file)\n    Pickled_LR_Model = pickle.load(file)\n\ny_pred = Pickled_LR_Model.predict(x_test[0].reshape(1,-1))","5a4efc4c":"# creating link to download the model\nfrom IPython.display import FileLink\nFileLink(pkl_Filename)","e6639586":"#preparing data for classification\ny_train[8] = y_train[8].apply(lambda x:1 if x>0.8 else 0)\ny_test[8] = y_test[8].apply(lambda x:1 if x>0.8 else 0)","727a9f97":"rfc = RandomForestClassifier()\nrfc.fit(x_train,y_train)","73227f7d":"#saving model\nimport pickle\npkl_Filename = \"classModel\"\n\nwith open(pkl_Filename, 'wb') as file:\n    pickle.dump(rfc,file)","29c4bf23":"#cheking if model saved works\nwith open(pkl_Filename, 'rb') as file: \n    print(file)\n    Pickled_LR_Model = pickle.load(file)\n\ny_pred = Pickled_LR_Model.predict(x_test)","ee47530e":"# creating link to download the model\nfrom IPython.display import FileLink\nFileLink(pkl_Filename)","404d767f":"### 3.b Checking Distribution Of Each Column","64f90950":"### 6.b) Regression Algorithms Visual Representation","cb6bb762":"### 3.e Checking Correlation Between Features\n> Serial No. has no relation to Chance Of Admit <br>\n> CGPA, GRE Score, TOEFL Score and University Rating are top 5 parameters <br>\n> Serial No. is not showing much correlation with any feature <br>\n> GRE Score score has strong coorelation with TOEFL Score and CGPA <br>\n> Research correlations are not very strong with any feature <br>","cef56731":"## 7.c) Generating prediction distribution chart","2dd86389":"## 7. Classification Algorithms\n> Random Forest CLassifier is the most suitable model to use","afd25698":"## 8 Finalising algorithms and saving models\n","5314bf11":"## 5. Feature Scaling and Data Split","9f0062f7":"### 3.a Checking for columns with categorical data and contineous data\n* Columns SOP,LOR,Research And University Rating have categorical data\n* Refrence Links\n    https:\/\/www.datacamp.com\/community\/tutorials\/categorical-data\n    ","7a865ca3":"### 2.b Checking For Null Values In Columns\n* No null values were found in columns","314392d3":"### 4.c Feature Selection\n> from correlation analysis we can remove serial number column","f35f4df5":"### 8.a) Regression Althorithm\n> Linear Regression is selected as final prediction algorithm","a440d952":"### 5.a) Splitting data","8eaf58ab":"### 5.b) Scaling Data","c6609ef8":"## 7.d) Generating Confusion Matrix Chart","24b71555":"### 7.a) preparing target value (making it discrete from contineous)\n> We are assuming that if chance of admit is more that 0.8 student will get selected","e617ccdc":"# 1. Importing Libraries","0212a1b0":"## 3. Exploratory Data Analysis","a8a5e04a":"#### 3.e.1 correlational maps","c1cd763e":"## 7.b) Generating cross validation chart","e28e5ccf":"### 6.a) Regression Algorithms Chart Representation\n> Linear Regression has highest Score","da0f050a":"### 2.c understanding datatypes in dataframe","c10fc3f6":"### 3.c Checking relation between target feature and each feature\n> no conclusion can be drawn from serial number <br>\n> chance of admit increases with increase in gre score <br>\n> chance of admit increases with increase in toefl score <br>\n> chance of admit increases with increase in university rating <br>\n> chance of admit increases with increase in SOP <br>\n> chance of admit increases with increase in LOR <br>\n> chance of admit increases with increase in CGPA <br>\n> chance of admit increases with increase in Research <br>","2ad4454f":"### 3.d Plotting Categorical Distributions\n> Chance Of Admit is more with research <br>\n> Chance of Admit increases with SOP <br>\n> Chance Of Admit increases with LOR <br>\n> CHance of admit increases with University Rating <br>","14153a87":"checking distinct values in categorical features","579e5295":"### 4.a Outlier detection","8718e018":"## 4.Feature Engineering","0f3dd518":" ### 2.a Checking Columns And Removing Spaces","2fd6d88c":"## 7.f) Generating ROC Curve","01081fed":"### 4.b Encoding\n> since for research both 0 and 1 are at same level this needs one hot encoding","d53dd673":"### 8.b) Saving Classification Model\n> Random Forest Classifier was selected","84ecdea1":"## 7.e) Generating precision,f1 and recall score Chart","372ad49e":"# 2. Loading Dataset","7f3635e5":"## 6 Regression Algorithms"}}