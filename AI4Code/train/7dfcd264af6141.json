{"cell_type":{"331563da":"code","025ed24b":"code","953a2618":"code","03956921":"code","526ee3a3":"code","e8c9edfd":"code","41053431":"code","cc4fc4e8":"code","7ffedf8f":"code","f5cec666":"code","55f97a93":"code","037a0545":"code","0fd42891":"code","1361adf2":"code","6bb7a2ba":"code","468e74b8":"code","88b4a89a":"code","c7d8474d":"code","f27ff684":"code","88221bdb":"code","29e3ddc0":"code","7adad9da":"code","aff18d2d":"code","e270b522":"code","c49e8271":"code","020f5233":"code","742584e0":"code","e90e88ee":"code","a11c4b2b":"code","ceaa8364":"code","8ad74a27":"code","d1368719":"code","fadb3133":"code","3b648663":"code","6ab731a9":"code","391bae50":"code","8dc8826d":"code","51f99f47":"code","5513f8a6":"code","16652a4d":"code","ee7899d6":"code","cba955bf":"code","8e37d419":"code","c8e12410":"code","5f2a30a8":"code","839b6595":"code","8b70a9a5":"code","814f10f9":"code","bfde02c1":"code","2384fd25":"code","02ee7d49":"code","d1670eac":"code","53a4fbd5":"code","504ff453":"code","0d8402ec":"code","b8655799":"code","4dadea6f":"code","e8130966":"markdown","0d289c90":"markdown","e6b1bd70":"markdown","aef430fc":"markdown","472b6fa7":"markdown"},"source":{"331563da":"import os\n","025ed24b":"train_images_list = os.listdir('..\/input\/flickr30k_images\/flickr30k_images\/flickr30k_images\/')","953a2618":"sample_size = 30\ntrain_images_list = train_images_list[:sample_size]","03956921":"import tensorflow as tf\nimport cv2\nimport numpy as np\nimport os\nfrom matplotlib import pyplot as plt\nimport random","526ee3a3":"size = (256, 256)\nnum_channels = 3","e8c9edfd":"train = np.array([None] * sample_size)\nreal_images = np.array([None] * sample_size)","41053431":"j = 0\nfor i in train_images_list:\n    real_images[j] = np.array(plt.imread('..\/input\/flickr30k_images\/flickr30k_images\/flickr30k_images\/' + i))\n    train[j] = np.array(plt.imread('..\/input\/flickr30k_images\/flickr30k_images\/flickr30k_images\/' + i))\n    j += 1","cc4fc4e8":"real_images[0].shape","7ffedf8f":"train[0].shape","f5cec666":"j = 0\nfor i in train:\n    train[j] = cv2.resize(i, size)\n    train[j] = train[j].reshape(1, size[0], size[1], num_channels)\n    j += 1","55f97a93":"train[0].shape","037a0545":"train = np.vstack(train[:])","0fd42891":"train.shape","1361adf2":"plt.imshow(np.squeeze(train[0]))\nplt.show()","6bb7a2ba":"import pandas as pd","468e74b8":"train_captions = pd.read_csv('..\/input\/flickr30k_images\/flickr30k_images\/results.csv', delimiter='|')","88b4a89a":"def get_images_id(names):\n    names = [int(x.split('_')[-1].split('.')[0]) for x in names]\n    return names","c7d8474d":"# ids = get_images_id(train_images_list[:sample_size])","f27ff684":"train_captions.columns = ['image_name', 'comment_number', 'comment']","88221bdb":"def images_map_caption(train_images_list, train_captions):\n    caption = []\n    for i in train_images_list:\n        caption.append(train_captions[train_captions['image_name'] == i]['comment'].iat[0])\n    return caption","29e3ddc0":"captions = np.array(images_map_caption(train_images_list, train_captions))\nprint(captions.shape)","7adad9da":"import re","aff18d2d":"start_tag = '<s>'\nend_tag = '<e>'","e270b522":"def get_vocab(captions):\n    arr = []\n    m = captions.shape[0]\n    sentence = [None ] * m\n    j  = 0\n    for i in captions:\n        i = re.sub(' +',' ',i)\n        i = start_tag + ' ' + i + ' ' + end_tag\n        sentence[j] = i.split()\n        j += 1\n        arr = arr + i.split()\n    arr = list(set(arr))\n    vocab_size = len(arr)\n    j = 0\n    fwd_dict = {}\n    rev_dict = {}\n    j = 0\n    for i in arr:\n        fwd_dict[i] = j\n        rev_dict[j] = i\n        j += 1\n    return vocab_size, sentence, fwd_dict, rev_dict","c49e8271":"vocab_size, sentences, fwd_dict, rev_dict = get_vocab(captions)","020f5233":"from scipy.sparse import csr_matrix\nfrom scipy.sparse import vstack","742584e0":"m = len(sentences)\ntrain_caption = [None] * m\ni = 0\nfor sentence in sentences:\n    cap_array = None\n    for word in sentence:\n        row = [0]\n        col = [fwd_dict[word]]\n        data = [1]\n        if cap_array is None:\n            cap_array = csr_matrix((data, (row, col)), shape=(1, vocab_size))\n        else:\n            cap_array = vstack((cap_array, csr_matrix((data, (row, col)), shape=(1, vocab_size))))\n    print(cap_array.shape)\n    train_caption[i] = cap_array\n    i += 1","e90e88ee":"train_caption[0].shape","a11c4b2b":"def create_weights(shape, suffix):\n    return tf.Variable(tf.truncated_normal(shape, stddev=0.7), name='W_' + suffix)\n\ndef create_biases(size, suffix):\n    return tf.Variable(tf.zeros([size]), name='b_' + suffix)","ceaa8364":"def conv_layer(inp, kernel_shape, num_channels, num_kernels, suffix):\n    filter_shape = [kernel_shape[0], kernel_shape[1], num_channels, num_kernels]\n    weights = create_weights(shape=filter_shape, suffix=suffix)\n    biases = create_biases(num_kernels, suffix=suffix)\n    layer = tf.nn.conv2d(input=inp, filter=weights, padding='SAME', strides=[1, 1, 1, 1], name='conv_' + suffix)\n    layer += biases\n    layer = tf.nn.relu6(layer, name='relu_' + suffix)\n    #layer = tf.nn.max_pool(layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2,1], padding= 'SAME')\n    return layer\n\n","8ad74a27":"def flatten_layer(layer, suffix):\n    layer_shape = layer.get_shape()\n    num_features = layer_shape[1:4].num_elements()\n    layer = tf.reshape(layer, [-1, num_features], name='flat_' + suffix )\n    return layer\n","d1368719":"def dense_layer(inp, num_inputs, num_outputs, suffix, use_relu=True):\n    weights = create_weights([num_inputs, num_outputs], suffix)\n    biases = create_biases(num_outputs, suffix)\n    layer = tf.matmul(inp, weights) + biases\n    layer = tf.nn.relu(layer)\n    return layer","fadb3133":"def rnn_cell(Win ,Wout, Wfwd, b, hprev, inp):\n    h = tf.tanh(tf.add(tf.add(tf.matmul(inp, Win), tf.matmul(hprev, Wfwd)), b))\n    out = tf.matmul(h, Wo)\n    return h, out","3b648663":"import tensorflow as tf","6ab731a9":"tf.device(\"\/device:GPU:0\")","391bae50":"learning_rate = 0.0001\ntraining_iters = 5000\ndisplay_step = 1000\nmax_sent_limit = 50\nnum_tests = 12\nbridge_size = 1024\nkeep_prob = 0.3","8dc8826d":"x_caption = tf.placeholder(tf.float32, [None, vocab_size], name = 'x_caption')\nx_inp = tf.placeholder(tf.float32, shape=[1, size[0],size[1],num_channels], name='x_image')\ny = tf.placeholder(tf.float32, [None, vocab_size], name = 'x_caption')","51f99f47":"Wconv = tf.Variable(tf.truncated_normal([bridge_size, vocab_size], stddev=0.7))\nbconv = tf.Variable(tf.zeros([1, vocab_size]))\nWi= tf.Variable(tf.truncated_normal([vocab_size, vocab_size], stddev=0.7))\nWf= tf.Variable(tf.truncated_normal([vocab_size, vocab_size], stddev=0.7))\nWo= tf.Variable(tf.truncated_normal([vocab_size, vocab_size], stddev=0.7))\nb = tf.Variable(tf.zeros([1, vocab_size]))","5513f8a6":"layer_conv1 = conv_layer(inp=x_inp, kernel_shape=(3, 3), num_kernels=32, num_channels=3, suffix='1')\nlayer_conv2 = conv_layer(inp=layer_conv1, kernel_shape=(3, 3), num_kernels=32, num_channels=32, suffix='2')\nmaxpool1 = tf.nn.max_pool(layer_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2,1], padding= 'SAME')\nlayer_conv3 = conv_layer(inp=maxpool1, kernel_shape=(3, 3), num_kernels=64, num_channels=32, suffix='3')\nlayer_conv4 = conv_layer(inp=layer_conv3, kernel_shape=(3, 3), num_kernels=64, num_channels=64, suffix='4')\nmaxpool2 = tf.nn.max_pool(layer_conv4, ksize=[1, 2, 2, 1], strides=[1, 2, 2,1], padding= 'SAME')\nlayer_conv5 = conv_layer(inp=maxpool2, kernel_shape=(3, 3), num_kernels=128, num_channels=64, suffix='5')\nlayer_conv6 = conv_layer(inp=layer_conv5, kernel_shape=(3, 3), num_kernels=128, num_channels=128, suffix='6')\nmaxpool3 = tf.nn.max_pool(layer_conv6, ksize=[1, 2, 2, 1], strides=[1, 2, 2,1], padding= 'SAME')\nlayer_conv7 = conv_layer(inp=maxpool3, kernel_shape=(3, 3), num_kernels=256, num_channels=128, suffix='7')\nlayer_conv8 = conv_layer(inp=layer_conv7, kernel_shape=(3, 3), num_kernels=256, num_channels=256, suffix='8')","16652a4d":"flat_layer = flatten_layer(layer_conv8, suffix='9')\n#flat_layer = tf.layers.dropout(flat_layer, rate= keep_prob)\ndense_layer_1 = dense_layer(inp=flat_layer, num_inputs=262144 , num_outputs=bridge_size, suffix='10')","ee7899d6":"start_hook = tf.cast(csr_matrix(([1], ([0], [fwd_dict[start_tag]])), shape=(1, vocab_size)).A, tf.float32)\nend_hook = tf.cast(csr_matrix(([1], ([0], [fwd_dict[end_tag]])), shape=(1, vocab_size)).A, tf.float32)","cba955bf":"hook = tf.slice(x_caption, [0, 0], [1, vocab_size])\nh = dense_layer_1\nh, out = rnn_cell(Wi ,Wo, Wconv, bconv, h, hook)","8e37d419":"def fn(prev, curr):\n    h = prev[0]\n    curr = tf.reshape(curr, [1, vocab_size])\n    h, out = rnn_cell(Wi ,Wo, Wf, b, h, curr)\n    return h, out","c8e12410":"_, output = tf.scan(fn, x_caption[1:], initializer=(h, out))","5f2a30a8":"output = tf.squeeze(output, axis  = 1)","839b6595":"outputs = tf.concat([out, output], axis = 0)","8b70a9a5":"cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=outputs, labels=y))\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)","814f10f9":"pred = tf.nn.softmax(outputs)","bfde02c1":"# Model evaluation\ncorrect_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))","2384fd25":"out_tensor = tf.TensorArray(dtype=tf.float32, dynamic_size=True, size = 0)","02ee7d49":"htest = dense_layer_1\nhtest, out_first = rnn_cell(Wi ,Wo, Wconv, bconv, htest, start_hook)\nt = 0\nout_ = tf.one_hot(tf.argmax(tf.nn.softmax(out_first), 1), depth=vocab_size)\nout_tensor = out_tensor.write(t, out_)\nt += 1","d1670eac":"def condition(res, h, out_tensor, t):\n    return tf.logical_and(tf.logical_not(tf.equal(tf.argmax(res, 1)[0], fwd_dict[end_tag])), tf.less(t, max_sent_limit))","53a4fbd5":"def action(res, h, out_tensor, t):\n    h, out = rnn_cell(Wi ,Wo, Wf, b, h, res)\n    res = tf.one_hot(tf.argmax(tf.nn.softmax(out), 1), depth=vocab_size)\n    out_tensor = out_tensor.write(t, res)\n    return res, h, out_tensor, t + 1","504ff453":"_, __, final_outputs, T = tf.while_loop(condition, action, [out_, htest, out_tensor, t])","0d8402ec":"final_prediction = tf.squeeze(final_outputs.stack())","b8655799":"saver = tf.train.Saver()\ninit = tf.global_variables_initializer()","4dadea6f":"with tf.Session() as sess:\n    sess.run(init)\n    m = len(train_caption)\n    for epoch in range(training_iters):\n        total_cost = 0\n        total_acc = 0\n        for i in range(m):\n            _, cst, acc = sess.run([optimizer, cost, accuracy], feed_dict = {x_caption:train_caption[i][:-1].A, x_inp:train[i:i+1], y:train_caption[i][1:].A})\n            total_cost += cst\n            total_acc += acc\n        if (epoch + 1) % display_step == 0:\n            print('After ', (epoch + 1), 'iterations: Cost = ', total_cost \/ m, 'and Accuracy: ', total_acc * 100\/ m , '%' )\n    print('Optimization finished!')\n    print(\"Let's check\")\n    for tests in range(num_tests):\n        image_num = random.randint(0, sample_size - 1)\n        caption = sess.run(final_prediction, feed_dict = {x_inp:train[image_num:image_num + 1]})\n        print(caption.shape)\n        caption = np.argmax(caption[:-1], 1)\n        capt = ''\n        for i in caption:\n            capt += rev_dict[i] + ' '\n        print('Predicted Caption:->', capt)\n        orig_cap = np.argmax(train_caption[image_num:image_num + 1][0][1:-1].A, 1)\n        orignalcaption = ''\n        for i in orig_cap:\n            orignalcaption += rev_dict[i] + ' '\n        print('Orignal Caption:->', orignalcaption)\n        plt.imshow(real_images[image_num])\n        plt.title('Image')\n        plt.show()","e8130966":"## Training Model","0d289c90":"## Predictive Model","e6b1bd70":"# Model Implemetation","aef430fc":"real_images[0].shape","472b6fa7":"# Model Design"}}