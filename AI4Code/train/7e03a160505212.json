{"cell_type":{"82e46432":"code","f220bac0":"code","b94562cd":"code","5f1c2499":"code","f9d82b6e":"code","e32fe405":"code","4194f03b":"code","59a26581":"code","ed4f5128":"code","7a947f88":"code","90523a81":"code","4526a9e4":"code","d4e6e3b5":"code","6f8f2c8b":"code","7df35135":"code","917e6e24":"code","0b57254f":"code","e88dbf6d":"code","33d70d39":"code","4012f917":"code","a80f5209":"code","bfa56557":"code","e25b960a":"code","3498e3fb":"code","aeb23106":"code","0e302b6a":"code","2c4ffe92":"code","a0d34cea":"code","14e70331":"code","631392be":"code","1123031c":"code","18dc5bb8":"code","1a2e4497":"code","2946bcdd":"code","37faff48":"code","a83e3edb":"code","34afb0a3":"code","8404bfbf":"code","8ee18326":"code","2bc5adea":"code","0a2288a1":"code","80023d29":"code","0803600f":"code","fccbe7f7":"code","979b4be3":"code","da8b7262":"code","0e977042":"code","e4edd026":"markdown","ee258102":"markdown","f4e0e993":"markdown","bb2b9f72":"markdown","8dca497d":"markdown","905ae95f":"markdown","681b9627":"markdown","11d0c590":"markdown","e1abb54b":"markdown","a0a346e1":"markdown","3417ffab":"markdown","6bf6521a":"markdown","b3735dee":"markdown","5715078f":"markdown","d93a9ebf":"markdown","9b70fb2c":"markdown","46cfa401":"markdown","fedf22dc":"markdown","dc0a4bd8":"markdown","fa458607":"markdown","6bc3cdbc":"markdown","2cb05e09":"markdown","e3998d8f":"markdown","6dd9d899":"markdown","6d3f33a2":"markdown","348e985a":"markdown","56c93916":"markdown","52b049f5":"markdown","86fa01fe":"markdown","8d41801d":"markdown","4f2d86b9":"markdown","4d2e1cc2":"markdown","883f37b7":"markdown"},"source":{"82e46432":"#Load ML libraries\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\nimport numpy as np\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\n\n#Load DL libraries\nimport keras\nimport sys\nfrom keras.layers import LSTM, Dense, Dropout, Activation\nfrom keras.models import Sequential\n#from keras.layers.core import Dense, Dropout, Activation\nimport lightgbm as lgb\n\nfrom keras.preprocessing.sequence import TimeseriesGenerator\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f220bac0":"retail_club = pd.read_csv('\/kaggle\/input\/retailer-data\/0029977001001UA096_ST10.csv')\nretail_club.head()","b94562cd":"print(retail_club.iloc[0])","5f1c2499":"print(retail_club.iloc[114])","f9d82b6e":"retail_df = retail_club.iloc[0:115]\nprint('First date record of retail data: \\n', retail_df.head(1).StartDate)\nprint('\\n')\nprint('Last date record of retail data: \\n',retail_df.tail(1).StartDate)","e32fe405":"#Transform string to date\nretail_df['StartDate'] = pd.to_datetime(retail_df.StartDate, format=\"%d-%b-%y\")\n\n#Extracting Year\nretail_df['Year'] = retail_df['StartDate'].dt.year","4194f03b":"f, ax = plt.subplots(1,1, figsize=(20,8))\nplot = sns.lineplot(x='StartDate', y='Base', data=retail_df)\nplot.set(title='Quarterly Sales')","59a26581":"retail_df.corr()","ed4f5128":"retail_data = retail_df[[\"Type - 1\", \"Base\"]]\nretail_data.shape","7a947f88":"retail_data.isnull().sum()","90523a81":"from sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=2)\nretail_data_imputation = imputer.fit_transform(retail_data)\nretail_imputed_df = pd.DataFrame(data=retail_data_imputation, columns=[\"Type - 1\",\"Base\"])\nretail_imputed_df.head()","4526a9e4":"print('After Imputation: \\n')\nprint(retail_imputed_df.isnull().sum(), '\\n')","d4e6e3b5":"retail_imputed_df.describe()","6f8f2c8b":"sns.pairplot(retail_imputed_df, diag_kind='kde')","7df35135":"## Checking the presence of outliers\npos = 1\nplt.figure(figsize=(10,5))\nfor i in retail_imputed_df.columns:\n    plt.subplot(2, 2, pos)\n    sns.boxplot(retail_imputed_df[i],color=\"red\")\n    pos += 1","917e6e24":"## Detect outliers using IQR and Handling outliers\nQ1 = retail_imputed_df.quantile(0.25)\nQ3 = retail_imputed_df.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","0b57254f":"## Checking for outliers presence of data points with \"True\"\nbool_outs= (retail_imputed_df < (Q1 - 1.5 * IQR)) |(retail_imputed_df > (Q3 + 1.5 * IQR))\nprint(bool_outs)","e88dbf6d":"## Removing outliers from dataframe\nretail_final = retail_imputed_df[~bool_outs.any(axis=1)]\nprint('Shape of dataframe with outliers: {}'.format(retail_imputed_df.shape))\nprint('Shape of dataframe without outliers: {}'.format(retail_final.shape))","33d70d39":"pos = 1\nplt.figure(figsize=(10,5))\nfor i in retail_final.columns:\n    plt.subplot(2, 2, pos)\n    sns.boxplot(retail_final[i],color=\"green\")\n    pos += 1","4012f917":"# Checking correlation to remove unnecessary columns from dataset\nretail_final.corr()","a80f5209":"retail_model = retail_final[[\"Base\"]]\nretail_model.head()","bfa56557":"# Lag features and Shift index\nfor i in range(1,5):\n    lag_i = 'lag_' + str(i)\n    retail_model[lag_i] = retail_model.Base.shift(i)\n    \n# Rolling window\nretail_model['rolling_mean'] = retail_model.Base.rolling(window=2).mean()\nretail_model['rolling_max'] = retail_model.Base.rolling(window=2).max()\nretail_model['rolling_min'] = retail_model.Base.rolling(window=2).min()","e25b960a":"# Correlation matrix with heatmap\ncorr = retail_model.corr()\nfig = plt.figure(figsize=(6,4))\nsns.heatmap(corr, linewidths=.5)","3498e3fb":"retail_model.corr()","aeb23106":"retail_model_final = retail_model[[\"rolling_mean\", \"rolling_min\", \"Base\"]]\nprint('Shape:', retail_model_final.shape)\nretail_model_final.head()","0e302b6a":"retail_model_final.mean()","2c4ffe92":"retail_model_final.fillna(retail_model_final.mean(), inplace = True)\nretail_model_final.head()","a0d34cea":"sns.pairplot(retail_model_final, diag_kind = 'kde')","14e70331":"print('Shape of the dataset:', retail_model_final.shape)","631392be":"# Split the time series data (Train-86, Test-22)\nprint('Total records in dataset:', len(retail_model_final))\nretail_train = retail_model_final.iloc[0:86]               \nretail_test = retail_model_final.iloc[86:]\n\nretail_pred_train = retail_model_final.iloc[0:86]               \nretail_pred_test = retail_model_final.iloc[86:]\nprint('Total records in Training set:', len(retail_train))\nprint('Total records in Test set:', len(retail_test))","1123031c":"retail_train['Base'].plot(legend=True,label='TRAIN (80%)')\nretail_test['Base'].plot(legend=True,label='TEST(20%)',figsize=(12,8));","18dc5bb8":"X_train = retail_train.drop(['Base'], axis=1)\ny_train = retail_train['Base'].values\n\nX_test = retail_test.drop(['Base'], axis=1)\ny_test = retail_test['Base'].values","1a2e4497":"X_pred_train = retail_pred_train.drop(['Base'], axis=1)\ny_pred_train = retail_pred_train['Base'].values\n\nX_pred_test = retail_pred_test.drop(['Base'], axis=1)\ny_pred_test = retail_pred_test['Base'].values","2946bcdd":"import statsmodels.api as sm\n\n# Note the difference in argument order\nml_model = sm.OLS(retail_model_final.Base, retail_model_final.drop(columns= [\"Base\"])).fit() ## sm.OLS(output, input)\nml_preds = ml_model.predict(X_test)\n\n# Print out the statistics\nml_model.summary()","37faff48":"ml_model.params","a83e3edb":"ml_preds","34afb0a3":"ml_errors_df = retail_test[['Base']]\nml_errors_df['Predicted_Base'] = ml_preds\nml_errors_df['Error'] = ml_preds - y_test\nml_errors_df.insert(0, 'Modelname', 'Multi Linear Regression')\nml_errors_df.head()","8404bfbf":"def mae(err):\n    return np.mean(np.abs(err))\n\ndef rmse(err):\n    return np.sqrt(np.mean(err ** 2))\n\ndef mape(err, sales=ml_errors_df['Base']):\n    return np.sum(np.abs(err))\/np.sum(sales) * 100","8ee18326":"# Evaluate predictions for Linear Regression\nfig = plt.figure(figsize=(14,7))\nplt.plot(retail_train.index, retail_train['Base'], label='Train')\nplt.plot(retail_test.index, retail_test['Base'], label='Test')\nplt.plot(ml_errors_df.index, ml_errors_df['Predicted_Base'], label='Forecast - Multi Linear Regression')\nplt.legend(loc='best')\nplt.xlabel('StartDate')\nplt.ylabel('Base')\nplt.title('Forecast using Multi Linear Regression')\nplt.show()","2bc5adea":"fig = plt.figure(figsize=(14,7))\nplt.plot(ml_errors_df.index, ml_errors_df.Error, label='Error')\nplt.plot(ml_errors_df.index, ml_errors_df.Base, label='Actual Sales')\nplt.plot(ml_errors_df.index, ml_errors_df.Predicted_Base, label='Forecasted Sales')\nplt.legend(loc='best')\nplt.xlabel('Date')\nplt.ylabel('Sales')\nplt.title('Multi Linear Regression forecasting with Actual sales vs errors')\nplt.show()","0a2288a1":"result_df_mlr = ml_errors_df.groupby('Modelname').agg(Total_Sales=('Base', 'sum'),\n                                          Total_Pred_Sales=('Predicted_Base', 'sum'),\n                                          Model_Overall_Error=('Error', 'sum'),\n                                          MAE=('Error', mae),\n                                          RMSE=('Error', rmse), \n                                          MAPE=('Error', mape))\nresult_df_mlr","80023d29":"print('R-squared of model: {:.2%}'.format(ml_model.rsquared))","0803600f":"# Create submission dataframe\nsubmission_mlr_df = pd.DataFrame(data=ml_preds, columns=['Predicted_Base'])\nsubmission_mlr_df.head()\n\n","fccbe7f7":"retail_pred_test.head()","979b4be3":"retail_pred_test.drop(columns= ['rolling_mean','rolling_min'], inplace=True)\nretail_pred_test.head()","da8b7262":"#retail_pred_test = retail_pred_test.drop(columns=[\"index\"])\nsubmission_results = pd.merge(retail_pred_test, submission_mlr_df, left_index=True, right_index=True)\nsubmission_results.head()","0e977042":"## Saving file to csv\nsubmission_results.to_csv('\/kaggle\/working\/MLR_Sales_Predictions.csv', index= False)\nprint('CSV file loaded with results in the path: \/content\/drive\/My Drive\/retail_forecast\/')","e4edd026":"## **7. Descriptive Statistics**","ee258102":"**Elimination of Outliers using IQR Method:**","f4e0e993":"**Inference**: Base is less correlated with lag_1, lag_2, lag_3, lag_4 and rolling_max. Hence removing them from dataset.","bb2b9f72":"### **Multiple Linear Regression**","8dca497d":"**Inference**: Outliers are founded in the data.","905ae95f":"## **4. Feature Engineering (using todatetime)**","681b9627":"## **10. Correlation of features**","11d0c590":"## **2. Load Dataset using Pandas**","e1abb54b":"Formula for MLR: \n\n\u200byi=**\u03b20+\u03b21xi1+\u03b22xi2+...+\u03b2pxip+\u03f5**\n\n*   yi -> Target\/Predictor\n*   xi1, xi2,...xip -> Features\n*   \u03b21,\u03b22,...\u03b2p -> Coefficients\n*   \u03b20 -> Intercept\n*   \u03f5 -> Model Error","a0a346e1":"## **11. Split data (Train=80%, Test= 20%)**","3417ffab":"**Inference**: \n\nType - 1 variable has only good correlation with Base. Hence excluding rest of all variables from the dataset. ","6bf6521a":"# **Problem Statement**:\nFind a best Forecast model to predict sales for future with given retail data.\n\n##### **Features**:\nDmdUnit, DmdGroup, Loc, StartDate, Type -1, Type - 2, Type - 2 DNR, Week.\n\n##### **Label\/Target**:\n\nBase\n\n**Calculation**: Base= [Type -1] - [Type - 2]","b3735dee":"**Inference**:\nBase is even less correlated with Type - 1. Hence removing Type - 1 from the dataset and moving forward with feature engineering using rolling window calculations to introduce new features into dataset for predictions.","5715078f":"## **1. Import Python Libraries**","d93a9ebf":"## **14. Calculate Metrics (MAE\/RMSE\/MAPE)**","9b70fb2c":"## **9. Feature Engineering (Using rolling window calculations)**","46cfa401":"## **16. Submission file with Prediction values**","fedf22dc":"**Inference**:\n\nOnce again we can see NULL values from the above data. Moving on replacing null values with mean values to form normal distribution curve.","dc0a4bd8":"**R-squared** is a statistical measure of how close the data points are to the fitted regression line. It is also known as the \"Coefficient of Determination\", or the coefficient of multiple determination for multiple regression.\n\nR-squared = Explained variation \/ Total variation.\n\nR-squared is always in between **0** and **100%**\n*   0% indicates that the model explains none of the variability of the response data around its mean.\n*   100% indicates that the model explains all the variability of the response data around its mean.\n\n**Note**: Higher the R-squared, the better the model fits your data.\n\n\n\n","fa458607":"## **6. Find missing data and Handling Null\/NaN values**","6bc3cdbc":"**Inference**: \"**Multiple Linear Regression**\" is a best generalized model for this data in terms of R-Squared metric.","2cb05e09":"## **3. Data Slicing**","e3998d8f":"**Statistical Significance (Hypothesis Testing)**:\n\nFor the Level of Significance, \u03b1=0.05\n\n*   rolling mean (p-value): 0.000\n*   rolling min (p-value): 0.005\n\nwhich clearly shows that **p-value <= \u03b1** ie., for some null hypothesis were assumed to be true, the model is rejecting the null hypothesis and is capable to explain 95% statistical significance.","6dd9d899":"**Fill NaN with Mean values**","6d3f33a2":"# **Line Equation**:\n\nMLR line equation looks like:\nBase = (0.658067)* rolling_mean + (0.360764) * rolling_min + (-94.890582)","348e985a":"**Inference**: Type - 1 has Null\/NaN values. Hence moving on to impute the null values with nearest neighbor mean values to form a normal distribution curve.","56c93916":"## **13. Modelling**","52b049f5":"#### **Plot forecast sales using MLR model**","86fa01fe":"**Inference**: \nAll data points of the respective columns are shown in bell curve and forms normal distribution according to Central Limit Theorem.","8d41801d":"## **12. Plot Train and Test data**","4f2d86b9":"## **5. Plot Historical Data**","4d2e1cc2":"**Handling Missing Data (Using KNN Imputation Method)**","883f37b7":"## **8. Checking th presence of Outliers using Boxplot**"}}