{"cell_type":{"2c36c58a":"code","1753417c":"code","47cd6004":"code","e98afa1d":"code","01cb029d":"code","e045b060":"code","90e7010c":"code","59be4939":"code","fdb3d913":"code","4a6e2bd8":"code","11b7b550":"code","5b83a05f":"code","baed4a59":"code","8f103e06":"code","e3190869":"code","e6a4814b":"code","5a8663eb":"code","a6345b37":"code","bf7b73de":"code","cfba42de":"code","c72accb4":"code","ca490404":"code","7d73c244":"code","5eeee41a":"code","69e8ec6b":"code","8ad280ac":"code","c6cc97fe":"code","dc93f60c":"code","4dc4abf0":"code","90966f36":"code","ea2661f9":"code","bd53c12a":"code","0d7e5935":"code","8a0373a2":"code","db5e413a":"code","aed309fd":"code","0277cb8a":"code","e896a663":"code","d99db59f":"markdown","0d13be95":"markdown","def006e6":"markdown","dd065152":"markdown","16fe210c":"markdown","ddd9dd82":"markdown","d168c379":"markdown","010292ef":"markdown","0324588d":"markdown","d9437259":"markdown"},"source":{"2c36c58a":"DATA_PATH = '..\/input\/shopee-product-matching\/'\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2, matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nimport gc\n\n# f1 score metric\ndef getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n \/ (len(row.target)+len(row[col]))\n    return f1score\n\ntrain = pd.read_csv(DATA_PATH + 'train.csv')\ntrain['image'] = DATA_PATH + 'train_images\/' + train['image']\ntmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain['target'] = train.label_group.map(tmp)","1753417c":"train = train.sort_values(by='label_group')\ntrain['title'] = train['title'].str.lower()\ntrain.head()","47cd6004":"def title_share_distance(s1, s2):\n    s1_word = set(s1.split(' '))\n    s2_word = set(s2.split(' '))\n    return 1 - len((s1_word.intersection(s2_word))) \/ len(s1_word.union(s2_word))","e98afa1d":"# same group\ntitle_share_distance(train['title'].iloc[0], train['title'].iloc[1])","01cb029d":"# same group\ntitle_share_distance(train['title'].iloc[0], train['title'].iloc[2])","e045b060":"# different group\ntitle_share_distance(train['title'].iloc[0], train['title'].iloc[1000])","90e7010c":"# different group\ntitle_share_distance(train['title'].iloc[0], train['title'].iloc[2000])","59be4939":"from sklearn.feature_extraction.text import TfidfVectorizer\ncorpus = [\n    'This is the first document.',\n    'This document is the second document.',\n    'And this is the third one.',\n    'Is this the first document?',\n]\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\n\nprint(X.shape)","fdb3d913":"from sklearn.feature_extraction.text import TfidfVectorizer\ncorpus = [\n    'This is the first document.',\n    'This document is the second document.',\n    'And this is the third one.',\n    'Is this the first document?',\n]\nvectorizer = TfidfVectorizer(ngram_range=(1, 2))\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\n\nprint(X.shape)","4a6e2bd8":"model = TfidfVectorizer(stop_words=None, binary=True, max_features=55000)\ntext_embeddings = model.fit_transform(train.title).toarray()\nprint('text embeddings shape',text_embeddings.shape)","11b7b550":"import torch\ntext_embeddings = torch.from_numpy(text_embeddings)\ntext_embeddings = text_embeddings.cuda()","5b83a05f":"preds = []\nCHUNK = 1024*2\n\nprint('Finding similar titles...')\nCTS = len(train)\/\/CHUNK\nif len(train)%CHUNK!=0: CTS += 1\ntext_ids = None\n    \nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(train))\n    print('chunk',a,'to',b)\n    \n    cts = torch.matmul(text_embeddings, text_embeddings[a:b].T).T\n    cts = cts.data.cpu().numpy()\n    for k in range(b-a):\n        IDX = np.where(cts[k,]>0.6)[0]\n        o = train.iloc[IDX].posting_id.values\n        preds.append(o)\n        \n    del cts\n    torch.cuda.empty_cache()","baed4a59":"del text_embeddings\ntorch.cuda.empty_cache()\n\ntrain['oof_text'] = preds","8f103e06":"from collections import Counter\nword_counter = Counter(' '.join(train['title'].values).split(' '))\nmost_occur = pd.DataFrame({'word': word_counter.keys(), 'count': word_counter.values()})\nmost_occur = most_occur[most_occur['word'].apply(len) > 1]\n\nmost_occur = most_occur.sort_values(by='count', ascending=False)\nmost_occur.head(10)","e3190869":"!pip install rank_bm25","e6a4814b":"from rank_bm25 import BM25Okapi\nfrom nltk.tokenize import word_tokenize\ntrain_title_token = train['title'].apply(lambda x: word_tokenize(x))","5a8663eb":"bm25 = BM25Okapi(train_title_token)","a6345b37":"ids = bm25.get_scores(train_title_token.iloc[200])\nidx = np.where(ids>50)[0]\nprint(ids[idx])\ntrain.iloc[idx]","bf7b73de":"ids = bm25.get_scores(train_title_token.iloc[1000])\nidx = np.where(ids>50)[0]\nprint(ids[idx])\ntrain.iloc[idx]","cfba42de":"from gensim.test.utils import get_tmpfile\nfrom gensim.models import KeyedVectors\n\nvectors = KeyedVectors.load_word2vec_format(\"..\/input\/glove2word2vec\/glove_w2v.txt\") # import the data file","c72accb4":"text_embeddings = []\nfor title in tqdm_notebook(train_title_token[:]):\n    title_feat = []\n    for word in title:\n        if word in vectors:\n            title_feat.append(vectors[word])\n    \n    if len(title_feat) == 0:\n        title_feat = np.random.rand(200)\n    else:\n        # max-pooling\n        # mean-pooling\n        # IDF\n        # SIF\n        title_feat = np.vstack(title_feat).max(0)\n    text_embeddings.append(title_feat)\n    # break","ca490404":"from sklearn.preprocessing import normalize\n\n# l2 norm to kill all the sim in 0-1\ntext_embeddings = np.vstack(text_embeddings)\ntext_embeddings = normalize(text_embeddings)\n\nimport torch\ntext_embeddings = torch.from_numpy(text_embeddings)\ntext_embeddings = text_embeddings.cuda()\n","7d73c244":"preds = []\nCHUNK = 1024*4\n\n\nprint('Finding similar images...')\nCTS = len(text_embeddings)\/\/CHUNK\nif len(text_embeddings)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(train))\n    print('chunk',a,'to',b)\n    \n    cts = torch.matmul(text_embeddings, text_embeddings[a:b].T).T\n    cts = cts.data.cpu().numpy()\n    for k in range(b-a):\n        IDX = np.where(cts[k,]>0.93)[0]\n        o = train.iloc[IDX].posting_id.values\n        preds.append(o)\n        \n    del cts\n    torch.cuda.empty_cache()","5eeee41a":"train['oof_w2v'] = preds","69e8ec6b":"from gensim.test.utils import common_texts\nfrom gensim.models import Word2Vec\n\nmodel = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\nmodel.save(\"word2vec.model\")","8ad280ac":"common_texts","c6cc97fe":"from gensim.test.utils import common_texts\nfrom gensim.models import Word2Vec\n\nmodel = Word2Vec(sentences=train_title_token, vector_size=200, window=5, min_count=1, workers=4)\nmodel.save(\"word2vec.model\")","dc93f60c":"def combine_for_oof(row):\n    x = np.concatenate([row.oof_text,row.oof_w2v])\n    return np.unique(x)\n\n# merge product proposal by tfidf and word2vec, we have positive and negative example.\n# if two product in same group they are positive label.\ntrain['oof'] = train.apply(combine_for_oof,axis=1)","4dc4abf0":"train = train.set_index('posting_id')","90966f36":"title_pair = []\nfor row in tqdm_notebook(train.iterrows()):\n    for pair in row[1].oof:\n        # not match self\n        if pair == row[0]:\n            continue\n    \n        if pair in row[1].target:\n            lbl = 1\n        else:\n            lbl = 0\n\n        title_pair.append(\n            [row[1].title, train.loc[pair]['title'], lbl]\n        )","ea2661f9":"title_pair = pd.DataFrame(title_pair, columns=['s1', 's2', 'label'])\ntitle_pair = title_pair.sample(frac=1)\ntitle_pair.head(5)","bd53c12a":"train_pair = title_pair.iloc[:5000]\nval_pair = title_pair.iloc[5000:6000]","0d7e5935":"from transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\", model_max_length=128)\nsequence_a = \"HuggingFace is based in NYC\"\nsequence_b = \"Where is HuggingFace based?\"\nencoded_dict = tokenizer(sequence_a, sequence_a)\nencoded_dict","8a0373a2":"import torch\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nimport random\nimport re\n\nclass ShopeeTextDataset(Dataset):\n    def __init__(self, df):\n        self.df = df\n        \n    def __getitem__(self, idx):\n        item = tokenizer(self.df.iloc[idx]['s1'], self.df.iloc[idx]['s2'], \n                         truncation=True, padding='max_length',  max_length=128)\n        item = {key: torch.tensor(val) for key, val in item.items()}\n        item['labels'] = torch.tensor(self.df.iloc[idx]['label'])\n        return item\n    \n    def __len__(self):\n        return self.df.shape[0]","db5e413a":"from transformers import BertForSequenceClassification, AdamW\nmodel = BertForSequenceClassification.from_pretrained('bert-base-cased')\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","aed309fd":"optim = AdamW(model.parameters(), lr=2e-5)\ntrain_dataset = ShopeeTextDataset(train_pair)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\nval_dataset = ShopeeTextDataset(val_pair)\nval_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)","0277cb8a":"def flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) \/ len(labels_flat)\n\ndef train():\n    model.train()\n    total_train_loss = 0\n    iter_num = 0\n    total_iter = len(train_loader)\n    for batch in train_loader:\n        optim.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs[0]\n        total_train_loss += loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optim.step()\n\n        iter_num += 1\n        if(iter_num % 100==0):\n            print(\"epoth: %d, iter_num: %d, loss: %.4f, %.2f%%\" % (epoch, iter_num, loss.item(), iter_num\/total_iter*100))\n        \n    print(\"Epoch: %d, Average training loss: %.4f\"%(epoch, total_train_loss\/len(train_loader)))\n    \ndef validation():\n    model.eval()\n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    for batch in tqdm_notebook(val_loader):\n        with torch.no_grad():\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        \n        loss = outputs[0]\n        logits = outputs[1]\n\n        total_eval_loss += loss.item()\n        logits = logits.detach().cpu().numpy()\n        label_ids = labels.to('cpu').numpy()\n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n        \n    avg_val_accuracy = total_eval_accuracy \/ len(val_loader)\n    print(\"Accuracy: %.4f\" % (avg_val_accuracy))\n    print(\"Average testing loss: %.4f\"%(total_eval_loss\/len(val_loader)))\n    print(\"-------------------------------\")","e896a663":"for epoch in range(3):\n    print(\"------------Epoch: %d ----------------\" % epoch)\n    train()\n    validation()","d99db59f":"# Conclusion & Advice\n\n1. BM25 is more effective than TFIDF, but it's a litter slow.\n2. use title embedding in word2vec to find similar product maybe not a good idea.\n3. we can use bert to rank the product:\n    * first search product by cnn feature\n    * then match the product by bert title model.\n4. try sentence-bet.","0d13be95":"\nTFIDF can count single word or two-word\/three-word together, in shopee compte single word is good and memory effectient.\n\nTFIDF \u53ef\u4ee5\u8ba1\u7b971-gram\uff0c2-gram\uff0c\u4f46\u5728shopee\u9898\u76ee\u4e2d1-gram\u8db3\u591f\u3002\n\n![image.png](attachment:image.png)","def006e6":"# Word2Vec\n\nWord embedding capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\n\nWord2Vec is one of the most popular technique to learn word embeddings using shallow neural network. Word2vec represents each distinct word with a particular list of numbers called a vector. The vectors are chosen carefully such that a simple mathematical function (the cosine similarity between the vectors) indicates the level of semantic similarity between the words represented by those vectors.\n\n\u8bcd\u5411\u91cf\u53ef\u4ee5\u5c06\u5355\u8bcd\u6620\u5c04\u5230\u5411\u91cf\u7a7a\u95f4\uff0c\u76f8\u4f3c\u7684\u76f8\u4f3c\u6620\u5c04\u540e\u8ddd\u79bb\u76f8\u8fd1\uff0c\u6211\u4eec\u4e5f\u53ef\u4ee5\u501f\u52a9\u9884\u8bad\u7ec3\u597d\u8bcd\u5411\u91cf\u6765\u5b8c\u6210title \u76f8\u4f3c\u5ea6\u8ba1\u7b97\u3002\n\n![image.png](attachment:image.png)\n\n* https:\/\/en.wikipedia.org\/wiki\/Word2vec\n* https:\/\/towardsdatascience.com\/introduction-to-word-embedding-and-word2vec-652d0c2060fa","dd065152":"we can also omit the top fraq word.","16fe210c":"## Train Your Own Word2Vec\n\nwe can train word2vec on shopee dataset.\n\n* https:\/\/machinelearningmastery.com\/develop-word-embeddings-python-gensim\/\n* https:\/\/radimrehurek.com\/gensim\/models\/word2vec.html","ddd9dd82":"# BM25\n\nOkapi BM25 (BM is an abbreviation of best matching) is a ranking function used by search engines to estimate the relevance of documents to a given search query.\n\nBM25 improves upon TF*IDF. BM25 stands for \u201cBest Match 25\u201d. Released in 1994, it\u2019s the 25th iteration of tweaking the relevance computation. BM25 has its roots in probabilistic information retrieval. \n\nBM25\u662f\u5bf9TFIDF\u7684\u6539\u8fdb\uff0c\u5728\u7cbe\u5ea6\u4e0a\u6709\u4e00\u5b9a\u63d0\u9ad8\u3002\n\n![image.png](attachment:image.png)\n\nhttps:\/\/github.com\/dorianbrown\/rank_bm25\nhttps:\/\/www.kaggle.com\/ideanlabib\/bm25-search-query-similarity-ranking\/\nhttps:\/\/gist.github.com\/koreyou\/f3a8a0470d32aa56b32f198f49a9f2b8","d168c379":"# Bert\n\nBidirectional Encoder Representations from Transformers (BERT) is a Transformer-based machine learning technique for natural language processing (NLP) pre-training developed by Google.\n\nwe can finetune bert on Next Sentense Prediction (NSP) task, try to match two product by title.\n* first construct title pair (match or not match)\n* load bert model\n* define loss and train\n\nBert\u6a21\u578b\u975e\u5e38\u5f3a\u5927\uff0c\u4e14\u53ef\u4ee5\u7528\u4e8e\u6587\u672c\u5339\u914d\u4efb\u52a1\u3002\u5982\u679c\u4f7f\u7528\u6587\u672c\u5339\u914d\u7684\u601d\u8def\uff0c\u5219\u9700\u8981\u6784\u5efa\u5339\u914d\u8bad\u7ec3\u96c6\uff0c\u7136\u540e\u5b9a\u4e49\u6a21\u578b\u5e76\u8bad\u7ec3\u3002\n\n![image.png](attachment:image.png)\n\nhttps:\/\/en.wikipedia.org\/wiki\/BERT_(language_model)","010292ef":"# TFIDF\n\nThe tf\u2013idf is the product of two statistics, term frequency and inverse document frequency. There are various ways for determining the exact values of both statistics. \n\nTFIDF contains two parts: TF(term frequency) and IDF(inverse document frequency).\n* TF is the word count in each sentence.\n* IDF is the importance of each word.\n\nTFIDF\u662f\u975e\u5e38\u6709\u6548\u7684\uff0cTF\u8ba1\u7b97\u4e86\u8bcd\u9891\uff0cIDF\u8ba1\u7b97\u4e86\u8bcd\u7684\u91cd\u8981\u6027\u3002\n\n![image.png](attachment:image.png)\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html","0324588d":"# Title Edit Distance\n\nEdit distance is a way of quantifying how dissimilar two strings (e.g., words) are to one another by counting the minimum number of operations required to transform one string into the other.\n\n\u5b57\u7b26\u7f16\u8f91\u8ddd\u79bb\u53ef\u7528\u8861\u91cf\u6807\u9898\u7684\u8ddd\u79bb\u3002\u5728\u8fd9\u91cc\u53ef\u4ee5\u76f4\u63a5\u8ba1\u7b97\u4e24\u4e2a\u5546\u54c1\u7684\u6807\u9898\u5171\u540c\u5305\u542b\u4e86\u591a\u5c11\u4e2a\u5355\u8bcd\u3002\n\n![image.png](attachment:image.png)\n\nhttps:\/\/stackoverflow.com\/questions\/2460177\/edit-distance-in-python\n\nIn shopee, you can find product in same group may have common word, so we can count the intersect words count.","d9437259":"In this notebook, some text feature is introduced. And i will give some insight and product method in shopee compte. If you find usefule, please give upvote, thx.\n\n- Title edit distance\n- TFIDF\n- BM25\n- Word2vec\n- Bert\n\n\u5728\u672c\u4e2anotebook\u4e2d\uff0c\u4ecb\u7ecd\u4e86\u4e00\u4e9b\u6587\u672c\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u3002\u6211\u4e5f\u4f1a\u5728\u4ecb\u7ecd\u4f7f\u7528\u7684\u540c\u65f6\uff0c\u7ed3\u5408shopee\u6bd4\u8d5b\u7ed9\u51fa\u4e00\u4e9b\u5efa\u8bae\u3002\u5982\u679c\u4f60\u611f\u8c22\u5185\u5bb9\u5bf9\u4f60\u6709\u5e2e\u52a9\uff0c\u8bf7\u7ed9\u6211\u70b9\u8d5e\uff0c\u8c22\u8c22\u3002\n\n\nYou can check my other notebooks:\n\n- [Shopee Products Matching: Image Part [English+\u4e2d\u6587]](https:\/\/www.kaggle.com\/finlay\/shopee-products-matching-image-part-english)\n- [Shopee Products Matching: Text Part [English+\u4e2d\u6587]](https:\/\/www.kaggle.com\/finlay\/shopee-products-matching-text-part-english)\n- [Shopee Products Matching: BoF Part [English+\u4e2d\u6587]](https:\/\/www.kaggle.com\/finlay\/shopee-products-matching-bof-part-english)\n- [Shopee Products Matching: Augment Part [English\u4e2d\u6587]](https:\/\/www.kaggle.com\/finlay\/shopee-products-matching-augment-part-english)\n- [[Unsupervised] Image + Text Baseline in 20min](https:\/\/www.kaggle.com\/finlay\/unsupervised-image-text-baseline-in-20min)"}}