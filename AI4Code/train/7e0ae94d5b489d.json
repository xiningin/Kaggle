{"cell_type":{"b1af86d9":"code","5e1b1be3":"code","8445ae8d":"code","ad9adf20":"code","3e3ac827":"code","2748afad":"code","74694386":"code","f47cfd8f":"code","8cff3a2a":"code","15a8cb60":"code","e8a65b39":"code","1477a4f9":"code","b674f217":"code","28df16d4":"code","b7f59272":"code","170b9468":"code","f5930233":"code","2ac1e2c9":"code","5985191c":"code","83ad6b5f":"code","8d9914b9":"code","c097e4c0":"code","55de42ff":"code","ce2265f7":"code","1398653a":"code","ecffb2f2":"code","228a7c99":"code","8aad6bb2":"code","a6bfd0d4":"code","591c4e16":"code","82f35d97":"code","2430129a":"code","0efa2eca":"code","9c5f1737":"markdown","ff08e3c6":"markdown","f9888693":"markdown","cf2da3c0":"markdown","01f970a6":"markdown","70dec9ad":"markdown","a75d31c6":"markdown","9e38c161":"markdown","c41c6a09":"markdown","9120b3e6":"markdown","66391cef":"markdown","1179a2e9":"markdown","7bbc0537":"markdown","0ebf43b0":"markdown","39b40dc4":"markdown","61062b82":"markdown","e9430cb2":"markdown","a81b7c7b":"markdown","6633487d":"markdown"},"source":{"b1af86d9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# import warnings\nimport warnings\n# filter warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","5e1b1be3":"train = pd.read_csv('..\/input\/train.csv')\nprint('train data shape: ', train.shape)\ntrain.head()","8445ae8d":"train_data = train.iloc[:, 1:]\ntrain_data.head()","ad9adf20":"train_label = train.iloc[:,0]\ntrain_label.head()","3e3ac827":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(train_data,train_label, test_size=0.1, random_state=42)","2748afad":"print('the number on the left shows # of imgaes, the number on the right shows # of pixels')\nprint('x_tain has shape: ', x_train.shape)\nprint('x_test has shape: ', x_test.shape)\nprint()\nprint('number of labels')\nprint('y_tain has shape: ', y_train.shape)\nprint('y_test has shape: ', y_test.shape)","74694386":"# visualize number of digits classes (or labels)\nplt.figure(figsize=(15,7))\nsns.countplot(y_train, palette=\"icefire\")\nplt.title(\"Number of digit classes\")\ny_train.value_counts()","f47cfd8f":"# plot some samples\nimg1 = x_train.iloc[1,:].as_matrix()\nimg1 = img1.reshape((28,28))\nplt.imshow(img1,cmap='gray')\nplt.title(y_train.iloc[1])\nplt.axis(\"off\")\nplt.show()","8cff3a2a":"# plot some samples\nimg1 = x_train.iloc[3,:].as_matrix()\nimg1 = img1.reshape((28,28))\nplt.imshow(img1,cmap='gray')\nplt.title(y_train.iloc[3])\nplt.axis(\"off\")\nplt.show()","15a8cb60":"# Normalize the data\nx_train = x_train \/ 255.0\nx_test = x_test \/ 255.0\nprint(\"x_train shape: \",x_train.shape)\nprint(\"x_test shape: \",x_test.shape)","e8a65b39":"# plot some samples\nimg1 = x_train.iloc[1,:].as_matrix()\nimg1 = img1.reshape((28,28))\nplt.imshow(img1,cmap='gray')\nplt.title(y_train.iloc[1])\nplt.axis(\"off\")\nplt.show()","1477a4f9":"# plot some samples\nimg1 = x_train.iloc[3,:].as_matrix()\nimg1 = img1.reshape((28,28))\nplt.imshow(img1,cmap='gray')\nplt.title(y_train.iloc[3])\nplt.axis(\"off\")\nplt.show()","b674f217":"x_train.shape","28df16d4":"# Reshape\n# x_train = x_train.values.reshape(33600,28,28,1)\nx_train = x_train.values.reshape(-1,28,28,1)\nx_test = x_test.values.reshape(-1,28,28,1)\nprint(\"x_train shape: \",x_train.shape)\nprint(\"x_test shape: \",x_test.shape)\n# x_train: 80 images, 28x28 pixels in each image, 1 dimensional color (grayscale)\n# x_test: 20 images, 28x28 pixels in each image, 1 dimensional color (grayscale)","b7f59272":"# Label Encoding \nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\ny_train = to_categorical(y_train, num_classes = 10)\ny_test = to_categorical(y_test, num_classes = 10)","170b9468":"y_train[0] # it is 8","f5930233":"y_train[10] # it is 6","2ac1e2c9":"# \nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nmodel = Sequential()\n\n# 1. filter & 2. filter + maxpool + dropout\n\nmodel.add(Conv2D(filters = 16, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 16, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n# 3. filter & 4. filter + maxpool + dropout\n\nmodel.add(Conv2D(filters = 8, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(filters = 8, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\n# 5. filter & 6. filter + maxpool + dropout\n\n# model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n#                  activation ='relu'))\n# model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n#                  activation ='relu'))\n# model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n# model.add(Dropout(0.25))\n\n\n# fully connected (ANN)\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))\n\n# The activation is \u2018softmax\u2019. Softmax makes the output sum up to 1 so \n# the output can be interpreted as probabilities. \n# The model will then make its prediction based on which option has the highest probability.\n# https:\/\/towardsdatascience.com\/building-a-convolutional-neural-network-cnn-in-keras-329fbbadc5f5\n","5985191c":"# Define the optimizer\n# adam: adaptive momentum\n# lr: learning rate\n# optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999) # for epoch>20 almost no change","83ad6b5f":"# Define the optimizer\noptimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)","8d9914b9":"# Compile the model\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","c097e4c0":"# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n#                                             patience=3, \n#                                             verbose=1, \n#                                             factor=0.5, \n#                                             min_lr=0.0001)","55de42ff":"# data augmentation\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # dimesion reduction\n        rotation_range=10,  # randomly rotate images in the range 10 degrees\n        zoom_range = 0.1, # Randomly zoom image 10%\n        width_shift_range=0.1,  # randomly shift images horizontally 10%\n        height_shift_range=0.1,  # randomly shift images vertically 10%\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(x_train)","ce2265f7":"epochs = 20  # for better result increase the epochs\nbatch_size = 64\n\n# Fit the model\nhistory = model.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (x_test,y_test), steps_per_epoch=x_train.shape[0] \/\/ batch_size)\n\n# Without data augmentation\n#history = model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, \n#          validation_data = (X_val, Y_val), verbose = 2)","1398653a":"# Plot the loss curve for training and validation \nplt.plot(history.history['val_loss'], color='r', label=\"validation loss\")\nplt.title(\"Validation Loss\")\nplt.xlabel(\"Number of Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","ecffb2f2":"# Plot the accuracy curves for training and validation \nplt.plot(history.history['val_acc'], color='g', label=\"validation accuracy\")\nplt.title(\"Validation Accuracy\")\nplt.xlabel(\"Number of Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","228a7c99":"# confusion matrix\nimport seaborn as sns\n# Predict the values from the validation dataset\nY_pred = model.predict(x_test)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_test,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Blues\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","8aad6bb2":"print('-'*80)\nprint('train accuracy of the model: ', history.history['acc'][-1])\nprint('-'*80)","a6bfd0d4":"print('-'*80)\nprint('validation accuracy of the model: ', history.history['val_acc'][-1])\nprint('-'*80)","591c4e16":"test_data = pd.read_csv('..\/input\/test.csv')\ntest_data = test_data.values.reshape(-1,28,28,1)\ntest_data.shape","82f35d97":"# predict results\nresults = model.predict(test_data)","2430129a":"# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")","0efa2eca":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"Digit_Recognizer_Mechmet_Kasap.csv\",index=False)","9c5f1737":"**Convolution Layer and Pooling Layer**\n\n* To begin with, when a computer sees an image (takes an image as input), it will see an array of pixel values. Depending on the resolution and size of the image, it will see a 32 x 32 x 3 array of numbers (The 3 refers to RGB values). Just to drive home the point, let's say we have a color image in JPG form and its size is 480 x 480. The representative array will be 480 x 480 x 3. Each of these numbers is given a value from 0 to 255 which describes the pixel intensity at that point. These numbers, while meaningless to us when we perform image classification, are the only inputs available to the computer.  The idea is that you give the computer this array of numbers and it will output numbers that describe the probability of the image being a certain class (.80 for cat, .15 for dog, .05 for bird, etc).\n<a href=\"https:\/\/adeshpande3.github.io\/assets\/Corgi3.png\"><img src=\"https:\/\/adeshpande3.github.io\/assets\/Corgi3.png\" alt=\"gec2\" border=\"0\"><\/a>\n\n* Convolution layer\n\n The first layer in a CNN is always a Convolutional Layer. First thing to make sure you remember is what the input to this conv (I\u2019ll be using that abbreviation a lot) layer is. Like we mentioned before, the input is a 32 x 32 x 3 array of pixel values. Now, the best way to explain a conv layer is to imagine a flashlight that is shining over the top left of the image. Let\u2019s say that the light this flashlight shines covers a 5 x 5 area. And now, let\u2019s imagine this flashlight sliding across all the areas of the input image. In machine learning terms, this flashlight is called a filter(or sometimes referred to as a neuron or a kernel) and the region that it is shining over is called the receptive field. Now this filter is also an array of numbers (the numbers are called weights or parameters). A very important note is that the depth of this filter has to be the same as the depth of the input (this makes sure that the math works out), so the dimensions of this filter is 5 x 5 x 3. Now, let\u2019s take the first position the filter is in for example.  It would be the top left corner. As the filter is sliding, or convolving, around the input image, it is multiplying the values in the filter with the original pixel values of the image (aka computing element wise multiplications). These multiplications are all summed up (mathematically speaking, this would be 75 multiplications in total). So now you have a single number. Remember, this number is just representative of when the filter is at the top left of the image. Now, we repeat this process for every location on the input volume. (Next step would be moving the filter to the right by 1 unit, then right again by 1, and so on). Every unique location on the input volume produces a number. After sliding the filter over all the locations, you will find out that what you\u2019re left with is a 28 x 28 x 1 array of numbers, which we call an activation map or feature map. The reason you get a 28 x 28 array is that there are 784 different locations that a 5 x 5 filter can fit on a 32 x 32 input image. These 784 numbers are mapped to a 28 x 28 array.\n<a href=\"https:\/\/adeshpande3.github.io\/assets\/ActivationMap.png \n\"><img src=\"https:\/\/adeshpande3.github.io\/assets\/ActivationMap.png \n\" alt=\"gec2\" border=\"0\"><\/a>\n\n* How this filter works\n\n    If the filter (at this case it is a curve detector) resembles a lot to the original image at that matrix, the convolution will be high value. If they dont resemble then it will be close to zero. Hence,  the activation map (convolution of the original image with the filter) will show the areas in which there at mostly likely to be curves in the picture\n\n<a href=\"https:\/\/adeshpande3.github.io\/assets\/Filter.png \n\"><img src=\"https:\/\/adeshpande3.github.io\/assets\/Filter.png \n\" alt=\"gec2\" border=\"0\"><\/a>\n\nThis is just a filter that is going to detect lines that curve outward and to the right. We can have other filters for lines that curve to the left or for straight edges. The more filters, the greater the depth of the activation map, and the more information we have about the input volume.\n\n* Pooling Layer\n\nIt is common to periodically insert a pooling layer between successive convolutional layers in a CNN architecture. The pooling operation provides another form of translation invariance. The pooling layer operates independently on every depth slice of the input and resizes it spatially.\n\n<a href=\"http:\/\/cs231n.github.io\/assets\/cnn\/maxpool.jpeg\"><img src=\"http:\/\/cs231n.github.io\/assets\/cnn\/maxpool.jpeg\" alt=\"gec2\" border=\"0\"><\/a>\n\n* Overall Convolution Layer and Pooling Layer Process\n\n<a href=\"http:\/\/cs231n.github.io\/assets\/cnn\/convnet.jpeg\"><img src=\"http:\/\/cs231n.github.io\/assets\/cnn\/convnet.jpeg\" alt=\"gec2\" border=\"0\"><\/a>\n\n* Overall CNN \n\n<a href=\"https:\/\/adeshpande3.github.io\/assets\/Cover.png\"><img src=\"https:\/\/adeshpande3.github.io\/assets\/Cover.png\" alt=\"gec2\" border=\"0\"><\/a>\n","ff08e3c6":"## How Companies Use CNNs\n\nData, data, data. The companies that have lots of this magic 4 letter word are the ones that have an inherent advantage over the rest of the competition. The more training data that you can give to a network, the more training iterations you can make, the more weight updates you can make, and the better tuned to the network is when it goes to production. Facebook (and Instagram) can use all the photos of the billion users it currently has, Pinterest can use information of the 50 billion pins that are on its site, Google can use search data, and Amazon can use data from the millions of products that are bought every day. And now you know the magic behind how they use it.","f9888693":"Lets see how the images depicted above changed","cf2da3c0":"**Fit Model**","01f970a6":"**Normalization**\n1. We perform a grayscale normalization to reduce the effect of illumination's differences.\n1. If we perform normalization, CNN works faster.","70dec9ad":"**3. Metrics**\n\nTo make things even easier to interpret, we will use the \u2018accuracy\u2019 metric to see the accuracy score on the validation set when we train the model.","a75d31c6":"**Label Encoding**: \nEncode labels to one hot vectors\n\n2 => [0,0,1,0,0,0,0,0,0,0]\n\n4 => [0,0,0,0,1,0,0,0,0,0]\n\n(One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction)\n\nhttps:\/\/hackernoon.com\/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f","9e38c161":"## REFERENCES\n* https:\/\/towardsdatascience.com\/building-a-convolutional-neural-network-cnn-in-keras-329fbbadc5f5\n\n* https:\/\/medium.com\/nanonets\/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced\n\n* https:\/\/towardsdatascience.com\/build-your-own-convolution-neural-network-in-5-mins-4217c2cf964f\n\n* https:\/\/www.kaggle.com\/kanncaa1\/convolutional-neural-network-cnn-tutorial\n\n* https:\/\/adeshpande3.github.io\/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks\/\n\n* https:\/\/en.wikipedia.org\/wiki\/Convolutional_neural_network","c41c6a09":"<a id=\"14\"><\/a>\n### Epochs and Batch Size\n* Say you have a dataset of 10 examples (or samples). You have a **batch size** of 2, and you've specified you want the algorithm to run for 3 **epochs**. Therefore, in each epoch, you have 5 **batches** (10\/2 = 5). Each batch gets passed through the algorithm, therefore you have 5 iterations **per epoch**.\n* reference: https:\/\/stackoverflow.com\/questions\/4752626\/epoch-vs-iteration-when-training-neural-networks","9120b3e6":"**1. Optimizer**\n\nThe optimizer controls the learning rate. We will be using \u2018adam\u2019 as our optmizer. Adam is generally a good optimizer to use for many cases. The adam optimizer adjusts the learning rate throughout training.","66391cef":"<a id=\"15\"><\/a>\n### Data Augmentation\n* To avoid overfitting problem, we need to expand artificially our handwritten digit dataset\n* Alter the training data with small transformations to reproduce the variations of digit.\n* For example, the number is not centered The scale is not the same (some who write with big\/small numbers) The image is rotated.\n* <a href=\"https:\/\/ibb.co\/k24CUp\"><img src=\"https:\/\/preview.ibb.co\/nMxXUp\/augment.jpg\" alt=\"augment\" border=\"0\"><\/a>","1179a2e9":"**LETS PREDICT TEST DATA USING THIS MODEL**","7bbc0537":"<a id=\"4\"><\/a>\n## Convolutional Neural Network \n* CNN is used for image classification, object detection \n* <a href=\"https:\/\/ibb.co\/kV1j9p\"><img src=\"https:\/\/preview.ibb.co\/nRkBpp\/gec2.jpg\" alt=\"gec2\" border=\"0\"><\/a>","0ebf43b0":"Lets examine the appearace of some pictures among the train data","39b40dc4":"We will use \u2018categorical_crossentropy\u2019 for our loss function. This is the most common choice for classification. A lower score indicates that the model is performing better.","61062b82":"**Train Test Split of Train Data**","e9430cb2":"**Reshape**\n1. Train and test images (28 x 28)\n1. We reshape all data to 28x28x1 3D matrices.\n1. Keras needs an extra dimension in the end which correspond to channels. Our images are gray scaled so it use only one channel.","a81b7c7b":" **Compiling the model**\n \nNext, we need to compile our model. Compiling the model takes three parameters: optimizer, loss and metrics.","6633487d":"## COCLUSIONS\nHyperparameters that affect accuracy\n1. Epoch size\n1. The values chosen in data augmentation part\n1. Batch size\n1. Dense\n1. Filter size\n1. Kernel size"}}