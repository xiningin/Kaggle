{"cell_type":{"c5cdfe91":"code","9027377f":"code","dda1b7b2":"code","8babf214":"code","5f31c978":"code","c18bb660":"code","4fe21f9a":"code","4970f4a2":"code","b61461d8":"code","b327c605":"code","b49824eb":"code","e7a16acd":"markdown","1b9c5ae6":"markdown","e2f17bb8":"markdown","9c1af010":"markdown","0bd83442":"markdown","3ba8dadb":"markdown","e018e265":"markdown","ca96ef5f":"markdown","58717cd7":"markdown","e73a9169":"markdown","60197683":"markdown"},"source":{"c5cdfe91":"# ====================================================\n# Directory settings\n# ====================================================\nimport os\n\nOUTPUT_DIR = '.\/'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","9027377f":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    model_dir='..\/input\/jigsaw4-luke-base-starter-train\/'\n    num_workers=4\n    model=\"studio-ousia\/luke-base\"\n    batch_size=128\n    fc_dropout=0.\n    text=\"text\"\n    target=\"target\"\n    target_size=1\n    head=32\n    tail=32\n    seed=42\n    n_fold=5\n\n\nCFG.max_len = CFG.head + CFG.tail","dda1b7b2":"# ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport re\nimport sys\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\nos.system('pip uninstall -q transformers -y')\nos.system('pip uninstall -q tokenizers -y')\nos.system('pip uninstall -q huggingface_hub -y')\n\nos.system('mkdir -p \/tmp\/pip\/cache-tokenizers\/')\nos.system('cp ..\/input\/tokenizers-0103\/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl \/tmp\/pip\/cache-tokenizers\/')\nos.system('pip install -q --no-index --find-links \/tmp\/pip\/cache-tokenizers\/ tokenizers')\n\nos.system('mkdir -p \/tmp\/pip\/cache-huggingface-hub\/')\nos.system('cp ..\/input\/huggingface-hub-008\/huggingface_hub-0.0.8-py3-none-any.whl \/tmp\/pip\/cache-huggingface-hub\/')\nos.system('pip install -q --no-index --find-links \/tmp\/pip\/cache-huggingface-hub\/ huggingface_hub')\n\nos.system('mkdir -p \/tmp\/pip\/cache-transformers\/')\nos.system('cp ..\/input\/transformers-470\/transformers-4.7.0-py3-none-any.whl \/tmp\/pip\/cache-transformers\/')\nos.system('pip install -q --no-index --find-links \/tmp\/pip\/cache-transformers\/ transformers')\n\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import LukeTokenizer, LukeModel, LukeConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","8babf214":"# ====================================================\n# Utils\n# ====================================================\ndef get_score(df):\n    score = len(df[df['less_toxic_pred'] < df['more_toxic_pred']]) \/ len(df)\n    return score\n\n\ndef get_logger(filename=OUTPUT_DIR+'train'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","5f31c978":"# ====================================================\n# Data Loading\n# ====================================================\ntest = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')\nsubmission = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv')\nprint(test.shape, submission.shape)\ndisplay(test.head())\ndisplay(submission.head())","c18bb660":"CFG.tokenizer = LukeTokenizer.from_pretrained(CFG.model_dir+'tokenizer\/')","4fe21f9a":"# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input(text, cfg):\n    if cfg.tail == 0:\n        inputs = cfg.tokenizer.encode_plus(text, \n                                           return_tensors=None, \n                                           add_special_tokens=True, \n                                           max_length=cfg.max_len,\n                                           pad_to_max_length=True,\n                                           truncation=True)\n        for k, v in inputs.items():\n            inputs[k] = torch.tensor(v, dtype=torch.long)\n    else:\n        inputs = cfg.tokenizer.encode_plus(text,\n                                           return_tensors=None, \n                                           add_special_tokens=True, \n                                           truncation=True)\n        for k, v in inputs.items():\n            v_length = len(v)\n            if v_length > cfg.max_len:\n                v = np.hstack([v[:cfg.head], v[-cfg.tail:]])\n            if k == 'input_ids':\n                new_v = np.ones(cfg.max_len) * cfg.tokenizer.pad_token_id\n            else:\n                new_v = np.zeros(cfg.max_len)\n            new_v[:v_length] = v \n            inputs[k] = torch.tensor(new_v, dtype=torch.long)\n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.text = df[cfg.text].fillna(\"none\").values\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        text = str(self.text[item])\n        inputs = prepare_input(text, self.cfg)\n        return inputs","4970f4a2":"# ====================================================\n# Model\n# ====================================================\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = LukeConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = LukeModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = LukeModel(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, cfg.target_size)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        feature = torch.mean(last_hidden_states, 1)\n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output","b61461d8":"# ====================================================\n# inference\n# ====================================================\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions","b327c605":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, \n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\nconfig_path = CFG.model_dir+\"config.pth\"\npredictions = []\nfor fold in range(CFG.n_fold):\n    model = CustomModel(CFG, config_path=config_path, pretrained=False)\n    state = torch.load(CFG.model_dir+f\"{CFG.model.replace('\/', '-')}_fold{fold}_best.pth\", map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    predictions.append(prediction)\n    del model, state; gc.collect()\n    torch.cuda.empty_cache()","b49824eb":"submission['score'] = np.mean(predictions, axis=0)\nsubmission[['comment_id', 'score']].to_csv('submission.csv', index=False)","e7a16acd":"# Directory settings","1b9c5ae6":"# Library","e2f17bb8":"# Dataset","9c1af010":"# Utils","0bd83442":"# Model","3ba8dadb":"# Data Loading","e018e265":"# About this notebook\n- [Luke](https:\/\/arxiv.org\/pdf\/2010.01057v1.pdf)-base starter notebook\n- [Training notebook](https:\/\/www.kaggle.com\/yasufuminakama\/jigsaw4-luke-base-starter-train)\n- Approach References\n    - https:\/\/www.kaggle.com\/c\/jigsaw-toxic-severity-rating\/discussion\/286471\n    - https:\/\/www.kaggle.com\/debarshichanda\/pytorch-w-b-jigsaw-starter\n    - https:\/\/www.kaggle.com\/debarshichanda\/0-816-jigsaw-inference\n    - Thanks for sharing @debarshichanda","ca96ef5f":"# tokenizer","58717cd7":"# submission","e73a9169":"# inference","60197683":"# CFG"}}