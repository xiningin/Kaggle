{"cell_type":{"d8c876fb":"code","79b3e96b":"code","df953694":"code","fe9727eb":"code","a4d8b2aa":"code","d241539a":"code","064ddadb":"code","d150739d":"code","52003fd0":"code","0f1a2417":"code","efd66bcc":"code","0e7d7aa1":"code","359d6921":"code","a21dab16":"code","149a6397":"code","d7addf55":"code","93387b36":"code","10f6d693":"code","8de7ebff":"code","cf94f16e":"code","b74c0ded":"code","20161b87":"code","6e27bd01":"code","f44486a7":"code","e3aaa54c":"code","535921ba":"code","497986f6":"code","19e91a74":"code","62ec65c1":"code","71b54815":"code","077128a7":"code","3acd255d":"code","0ebc2f6b":"code","d9678a6a":"code","75f0dd5c":"code","5ee1e636":"code","e511fc58":"code","84ad2056":"code","fddde72d":"code","efd8a93e":"code","92e5192b":"code","cf8a3229":"code","8ef55163":"code","607afde9":"code","21d8296f":"code","fec2a9a8":"code","1a89c2b2":"code","74a41364":"code","3a4931bf":"code","61a26182":"code","c0ec4899":"code","b4735a0c":"code","9dd1bdc0":"code","652c63f7":"code","f5bd6913":"code","e3b35be8":"code","4be63646":"code","ba3587dd":"code","bfe96b07":"code","d9703fb3":"code","a6a1691d":"code","c1f4d82d":"code","c4dbf90c":"code","7da62eb6":"code","679123de":"code","f25e9dd0":"code","607dd422":"code","426f9be1":"code","441c15b0":"code","43cded66":"code","238e75f5":"code","0ad70449":"markdown","67587aa1":"markdown"},"source":{"d8c876fb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n#\nfrom sklearn import preprocessing\nfrom statistics import mean\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\n%matplotlib inline\n# Any results you write to the current directory are saved as output.","79b3e96b":"data = pd.read_csv('..\/input\/train.csv')\ntest  = pd.read_csv('..\/input\/test.csv')\ndesc = open('..\/input\/data_description.txt').read()\nle = preprocessing.LabelEncoder()","df953694":"sampl = pd.read_csv('..\/input\/sample_submission.csv')","fe9727eb":"data.columns","a4d8b2aa":"data.head()","d241539a":"#histogram\nsns.distplot(data['SalePrice']);","064ddadb":"#skewness and kurtosis\nprint(\"Skewness: %f\" % data['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % data['SalePrice'].kurt())","d150739d":"#Lets check the correlation between other factors and Sale Price\ncormat = data.corr()\nf, ax = plt.subplots(figsize=(15,10))\nsns.heatmap(cormat, vmax=0.8, square=True)","52003fd0":"#scatter plot grlivarea\/saleprice\nvar = 'GrLivArea'\ndata1 = pd.concat([data['SalePrice'], data[var]], axis=1)\ndata1.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","0f1a2417":"#scatter plot LotArea\/saleprice\nvar = 'LotArea'\ndata1 = pd.concat([data['SalePrice'], data[var]], axis=1)\ndata1.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","efd66bcc":"#scatter plot OverallQual\/saleprice\nsns.distplot(data['OverallQual'], fit=norm)\nfig = plt.figure()\nprb = stats.probplot(data['OverallQual'], plot=plt)\nvar = 'OverallQual'\ndata1 = pd.concat([data['SalePrice'], data[var]], axis=1)\nf, ax = plt.subplots(figsize=(8,6))\nfig = sns.boxplot(x=var, y='SalePrice', data=data1)\nfig.axis(ymin=0, ymax=800000)","0e7d7aa1":"#scatter plot totalbsmtsf\/saleprice\nvar = 'YearBuilt'\ndata1 = pd.concat([data['SalePrice'], data[var]], axis=1)\nf, ax = plt.subplots(figsize=(18,10))\nfig = sns.boxplot(x=var, y='SalePrice', data=data1)\nfig.axis(ymin=0, ymax=800000)\nplt.xticks(rotation=90)","359d6921":"#scatter plot totalbsmtsf\/saleprice\nvar = 'GarageCars'\ndata1 = pd.concat([data['SalePrice'], data[var]], axis=1)\ndata1.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","a21dab16":"#saleprice correlation matrix with higher Correlation\nk = 10 #number of variables for heatmap\ncols = cormat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(data[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","149a6397":"#Remove Outlier LotArea\ndata = data.drop(data[data['LotArea'] > 30000].index)","d7addf55":"mean_area = float(data['LotArea'].mean())","93387b36":"data['AreaMeanVar'] = data.LotArea - mean_area","10f6d693":"# Lets fill non existing data either by zeroes for continous data, and Mode for Caregorical data\nlst_d = {'LotFrontage'}\nfor i in lst_d:\n    df = pd.DataFrame(data[i])\n    if(data[i].dtypes=='float64'):\n        data[i] = df.fillna(0)","8de7ebff":"#get Id of Outliar LotFrontage \n#data.loc[data['LotFrontage'] > 250]","cf94f16e":"# Let's remove the outlier LotFrontage\ndata = data.drop(data[data['LotFrontage'] > 250].index)","b74c0ded":"#var = 'LotFrontage'\nsns.distplot(data['LotFrontage'] , fit=norm)\nfig = plt.figure()\nprb = stats.probplot(data['LotFrontage'], plot=plt)","20161b87":"# Lets fill non existing data either by zeroes for continous data, and Mode for Caregorical data\nl_fill = {'Fence'}\nfor i in l_fill:\n    df = pd.DataFrame(data[i])\n    data[i] = df.fillna('None')","6e27bd01":"total = data.isnull().sum().sort_values(ascending=False)\npercent = (data.isnull().sum()\/data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n#missing_data.head(20)","f44486a7":"# Lets remove the not needed columns (Outliers)\ndata = data.drop((missing_data[missing_data['Total'] > 1]).index,1)","e3aaa54c":"#in Electrical column only 1 data is missing, lets delete only this observation\ndata = data.drop(data.loc[data['Electrical'].isnull()].index)","535921ba":"#Lets analysis saleprice vs grlivarea\nvar = 'GrLivArea'\ndata1 = pd.concat([data['SalePrice'], data[var]], axis=1)\ndata1.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","497986f6":"#get Id of Outliar GrLivArea \n#data.loc[data['LotArea'] == 0]","19e91a74":"#Lets sort the data by GrLivArea and Delete the Outliar values\n#data.sort_values(by = 'GrLivArea', ascending=False)[:2]\ndata = data.drop(data[data['Id'] == 524].index)\ndata = data.drop(data[data['Id'] == 1299].index)","62ec65c1":"#Lets Analyze the Normality of SalePrice using Histogram and Normal Probability plot\nsns.distplot(data['SalePrice'], fit=norm)\nfig = plt.figure()\nprb = stats.probplot(data['SalePrice'], plot=plt)","71b54815":"# Since Price has Positive Skewness and Peakedness, it can be resolved using log transformation\ndata['SalePrice'] = np.log(data['SalePrice'])","077128a7":"#Lets Analyze again the Normality of SalePrice using Histogram and Normal Probability plot\nsns.distplot(data['SalePrice'], fit=norm)\nfig = plt.figure()\nprb = stats.probplot(data['SalePrice'], plot=plt)","3acd255d":"#Lets Analyze  the Normality of GrLivArea using Histogram and Normal Probability plot\nsns.distplot(data['GrLivArea'], fit=norm)\nfig = plt.figure()\nprb = stats.probplot(data['GrLivArea'], plot=plt)","0ebc2f6b":"# Since GrLivArea has Positive Skewness and Peakedness, it can be resolved using log transformation\ndata['GrLivArea'] = np.log(data['GrLivArea'])","d9678a6a":"#Lets Analyze  the Normality of GrLivArea using Histogram and Normal Probability plot\nsns.distplot(data['GrLivArea'], fit=norm)\nfig = plt.figure()\nprb = stats.probplot(data['GrLivArea'], plot=plt)","75f0dd5c":"#Lets Analyze  the Normality of TotalBsmtSF using Histogram and Normal Probability plot\nsns.distplot(data['TotalBsmtSF'], fit=norm)\nfig = plt.figure()\nprb = stats.probplot(data['TotalBsmtSF'], plot=plt)","5ee1e636":"data.head()","e511fc58":"# List of Features to remove from data\nlst_remv = {'EnclosedPorch','Exterior2nd','Exterior1st','BedroomAbvGr','HouseStyle','HeatingQC',\n            'Functional','SaleCondition','LandContour','YrSold','LandSlope','ExterQual','PavedDrive',\n            'LotConfig','Foundation','RoofStyle','KitchenAbvGr','BsmtFullBath','HalfBath','Electrical',\n            'Condition1','BsmtFinSF2','BldgType','ScreenPorch','SaleType','BsmtHalfBath','MiscVal',\n            'MiscVal','Heating','RoofMatl','LowQualFinSF','Utilities','Street','PoolArea','Condition2',\n            '3SsnPorch','GarageArea','MoSold','Id','CentralAir','ExterCond'}\n# List of Features to label Encode\nls = {'MSSubClass','MSZoning','LotShape','Neighborhood','KitchenQual'\n     ,'YearBuilt','YearRemodAdd','FullBath','TotRmsAbvGrd','Fireplaces',\n     'GarageCars','Fence'}","84ad2056":"# Feature Removal\nfor i in lst_remv:\n    data =  data.drop([i],axis=1)","fddde72d":"# Lets use Label Encoder to change String content to Integer content\nfor i in ls:\n    le.fit(data[i])\n    data[i] = le.transform(data[i])","efd8a93e":"train = data","92e5192b":"reslt1 = data.filter(['SalePrice'],axis=1)","cf8a3229":"train =  train.drop(['SalePrice'],axis=1)","8ef55163":"train.head()","607afde9":"#Lets split data into Train and Test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train, reslt1, test_size=0.2, random_state=10)\nsnum = 0\nenum = len(y_test)","21d8296f":"import xgboost as xgb\nmodel= xgb.XGBClassifier(max_depth=5,booster='gbtree',learning_rate=0.05,subsample=0.5,\n                         colsample_bytree=0.5,colsample_bylevel=0.5)","fec2a9a8":"model.fit(X_train,y_train)\npr = model.predict(X_test[snum:enum])","1a89c2b2":"names = X_train.columns.values\nprint(\"Features sorted by their score:\")\nprint(sorted(zip(map(lambda x: round(x, 4), model.feature_importances_), names), \n             reverse=True))","74a41364":"#prd1 = np.exp(pr)\n#t_data = np.exp(y_test[snum:enum])\n#print('Predicted result: ', prd1, '\\nActual result:   ', t_data.values.reshape(1,enum))","3a4931bf":"t_data2 = y_test[snum:enum].values\nt_data2 = t_data2.reshape(enum,)\ntotal_error=0\nfor i in range(len(t_data2)):\n    error = (t_data2[i] - pr[i])**2\n    total_error = total_error+ error\n\ntotal_error = total_error**(1\/2) \/ len(t_data2)\nprint(total_error)","61a26182":"from sklearn.ensemble import GradientBoostingRegressor\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","c0ec4899":"GBoost.fit(X_train, y_train)  \npr_gbr = GBoost.predict(X_test[snum:enum])","b4735a0c":"t_datal1 = y_test[snum:enum].values\nt_datal1 = t_datal1.reshape(enum,)\n\ntotal_error=0\nfor i in range(len(t_datal1)):\n    error = (t_datal1[i] - pr_gbr[i])**2\n    total_error = total_error+ error\n\ntotal_error = total_error**(1\/2) \/ len(t_datal1)\nprint(\"total_error: \", total_error)","9dd1bdc0":"names = X_train.columns.values\nprint(\"Features sorted by their score:\")\nprint(sorted(zip(map(lambda x: round(x, 4), GBoost.feature_importances_), names), \n             reverse=True))","652c63f7":"import lightgbm as lgb\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","f5bd6913":"model_lgb.fit(X_train, y_train)  \npr_lgb = model_lgb.predict(X_test[snum:enum])","e3b35be8":"pr1_lgb = np.exp(pr_lgb)\nt_lgb = np.exp(y_test[snum:enum])\n#print('Predicted result: ', prd3, '\\nActual result:   ', t_lgb.values.reshape(1,enum))","4be63646":"t_datal1 = y_test[snum:enum].values\nt_datal1 = t_datal1.reshape(enum,)\n\ntotal_error=0\nfor i in range(len(t_datal1)):\n    error = (t_datal1[i] - pr_lgb[i])**2\n    total_error = total_error+ error\n\ntotal_error = total_error**(1\/2) \/ len(t_datal1)\nprint(\"total_error: \", total_error)","ba3587dd":"names = X_train.columns.values\nprint(\"Features sorted by their score:\")\nprint(sorted(zip(map(lambda x: round(x, 4), model_lgb.feature_importances_), names), \n             reverse=True))","bfe96b07":"# Lets fill non existing data either by zeroes for continous data, and Mode for Caregorical data\ntest_dt = {'LotFrontage','Fence'}\nfor i in test_dt:\n    df = pd.DataFrame(test[i])\n    if(test[i].dtypes=='float64'):\n        test[i] = df.fillna(0)\n    elif(test[i].dtypes=='object'):\n        test[i] = df.fillna('None')","d9703fb3":"totalt = test.isnull().sum().sort_values(ascending=False)\npercent = (test.isnull().sum()\/test.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([totalt, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(22)","a6a1691d":"# Lets remove the not needed columns (Outliers)\ntest = test.drop((missing_data[missing_data['Total'] > 4]).index,1)","c1f4d82d":"total = test.isnull().sum().sort_values(ascending=False)\npercent = (test.isnull().sum()\/test.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(11)","c4dbf90c":"# Lets fill non existing data either by zeroes for continous data, and Mode for Caregorical data\nlst = {'MSZoning','BsmtHalfBath','BsmtFullBath','Functional','Utilities',\n       'Exterior2nd','KitchenQual','GarageCars','BsmtFinSF1',\n       'SaleType','TotalBsmtSF','BsmtUnfSF','BsmtFinSF2','Exterior1st'}\nfor i in lst:\n    df = pd.DataFrame(test[i])\n    if(test[i].dtypes=='float64'):\n        test[i] = df.fillna(0)\n    elif(test[i].dtypes=='object'):\n        test[i] = df.fillna(test[i].mode())","7da62eb6":"# Adding feature \nmean_area_t = float(test['LotArea'].mean())\n\ntest['AreaMeanVar'] = test.LotArea - mean_area_t","679123de":"# Log transform the test data\ntest['GrLivArea'] = np.log(test['GrLivArea'])","f25e9dd0":"# Feature Removal\nfor i in lst_remv:\n    test =  test.drop([i],axis=1)","607dd422":"# Lets use Label Encoder to change String content to Integer content\nfor i in ls:\n    if(test[i].dtypes=='float64'):\n        le.fit(test[i])\n        test[i] = le.transform(test[i])\n    elif(test[i].dtypes=='object'):\n        le.fit(test[i].astype(str))\n        test[i] = le.transform(test[i].astype(str))","426f9be1":"snum = 0\nenum = len(test)\n#prdt1 = model.predict(test[snum:enum])\n#prdt11 = np.exp(prdt1)\n#print('Predicted result: ', prdt11)","441c15b0":"prdt2 = model.predict(test[snum:enum])   #using Xtreme Gradient Boost Classifier\n#prdt2 = model_lgb.predict(test[snum:enum])   #using LGBM\n#prdt2 = GBoost.predict(test[snum:enum])      #using Gradient Boost Regressor\nprdt21 = np.exp(prdt2)\nprint('Predicted result: ', prdt21)","43cded66":"sampl['SalePrice'] = pd.DataFrame(prdt21)","238e75f5":"sampl.to_csv('submission.csv', index=False)","0ad70449":"That's great, Lets check GrLivArea","67587aa1":"Since at many places, TotalBsmtSF is zero and we cannot apply Log transformation, but we can do so by introducing new binary column HasBsmt, thus avoiding zeroes issue"}}