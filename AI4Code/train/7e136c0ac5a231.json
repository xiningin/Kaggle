{"cell_type":{"91f65e7f":"code","fad9ab27":"code","a43e7273":"code","9a5a05f4":"code","8427a6a6":"code","b966c51c":"code","499a35fb":"code","1325bb4c":"code","5356439a":"code","ee8d8814":"code","2e92603e":"code","757e5709":"code","44f7df6b":"code","b2f6d31d":"code","28dc99df":"code","8ca53d03":"code","b11012eb":"code","b63ea1b7":"code","79a2199d":"code","d605a7bf":"code","a0af9f2c":"code","302a947d":"code","c7ac5a41":"code","ef826cb0":"code","c2554d06":"code","d84c4164":"code","903bf576":"code","fa02c76f":"code","bc3c0417":"code","903e1d4a":"code","74e7f43c":"code","955d9d81":"code","bc588d15":"code","4d308d85":"code","c3a47755":"code","83f265bb":"code","1f594f6e":"code","51b13ac8":"code","f1d15620":"code","9ee93f8a":"code","9006ec8e":"code","08d537a6":"code","7d8b6b8b":"code","6e6ae798":"code","21b177cb":"code","35411ada":"code","8c863731":"code","4e7f02fd":"code","3655992a":"code","f868ab6d":"code","36f49e40":"code","5d9fd3f0":"code","087f2a27":"markdown","a85718fd":"markdown"},"source":{"91f65e7f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fad9ab27":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","a43e7273":"from sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pandas_datareader as web\nimport datetime as dt","9a5a05f4":"import keras\nimport tensorflow as tf\nfrom keras.models import Model\nfrom keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate\nfrom keras import optimizers\nimport time","8427a6a6":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM\nfrom statsmodels.tsa.seasonal import seasonal_decompose","b966c51c":"df1=pd.read_csv(\"\/kaggle\/input\/bitstamp-btc-stock-time-series\/bitstampUSD_1-min_data_2012-01-01_to_2020-04-22.csv\")","499a35fb":"df1.head()","1325bb4c":"df1.shape","5356439a":"sns.heatmap(df1.isnull(),yticklabels=False)","ee8d8814":"df1.dropna(how='any',inplace=True)#drops rows who have all of their inputs as NaN ","2e92603e":"df1.shape","757e5709":"df1.head()","44f7df6b":"plt.plot(df1['Close'], color=\"black\")\nplt.show()","b2f6d31d":"df1.dtypes","28dc99df":"#first =df1[\"Timestamp\"][0]","8ca53d03":"#from datetime import datetime\n#dt_obj = datetime.fromtimestamp(first)\n  \n#print(\"date_time:\",dt_obj)\n#print(\"type of dt:\",type(dt_obj))","b11012eb":"\n%matplotlib inline \nplt.gcf().set_size_inches(20, 10, forward=True)\nplt.plot(df1['Weighted_Price'], color=\"black\")\n\nplt.title(\" Share Price\")\nplt.xlabel(\"Days\")\nplt.ylabel(\"Share Price\")\n\nplt.show()\n","b63ea1b7":"#pd.to_datetime(df1['Timestamp']).apply(lambda x: x.date())","79a2199d":"#df2=df1\n#pd.to_datetime(df2['Timestamp']).dt.date","d605a7bf":"#df1.head()","a0af9f2c":"#df2.head()","302a947d":"df1[\"Date\"]=df1.Timestamp.apply(lambda x: pd.datetime.fromtimestamp(x).date())","c7ac5a41":"df1.head()","ef826cb0":"df1.drop(['Timestamp'], axis = 1,inplace=True)","c2554d06":"df1.head()","d84c4164":"df1.set_index('Date', inplace=True)\ndf1.info()","903bf576":"%matplotlib inline \nplt.gcf().set_size_inches(20, 10, forward=True)\nplt.plot(df1['Weighted_Price'], color=\"red\")\n\nplt.title(\" Share Price\")\nplt.xlabel(\"Days\")\nplt.ylabel(\"Share Price\")\n\nplt.show()\n","fa02c76f":"df2=df1.iloc[2458600:,:]","bc3c0417":"%matplotlib inline \nplt.gcf().set_size_inches(20, 10, forward=True)\nplt.plot(df2['Weighted_Price'], color=\"red\")\n\nplt.title(\" Share Price\")\nplt.xlabel(\"Days\")\nplt.ylabel(\"Share Price\")\n\nplt.show()\n","903e1d4a":"from sklearn import preprocessing\ndata_normaliser = preprocessing.StandardScaler()\ndf1_normalised = data_normaliser.fit_transform(df2)","74e7f43c":"df1_normalised[0,:]","955d9d81":"history_points=60\n ","bc588d15":"ohlcv_histories_normalised = np.array([df1_normalised[i  : i + history_points].copy() for i in range(len(df1_normalised) - history_points)])","4d308d85":"weighted_avg_values_normalised = np.array([df1_normalised[:,-1][i + history_points].copy() for i in range(len(df1_normalised) - history_points)])","c3a47755":"print(weighted_avg_values_normalised.shape)\nweighted_avg_values_normalised = np.expand_dims(weighted_avg_values_normalised, -1)\nweighted_avg_values_normalised.shape","83f265bb":"weighted_avg_values = np.array([df2[\"Weighted_Price\"][i + history_points].copy() for i in range(len(df2) - history_points)])","1f594f6e":"y_normaliser = preprocessing.StandardScaler()\ny_normaliser.fit(np.expand_dims( weighted_avg_values,-1 ))","51b13ac8":"print(weighted_avg_values.shape)","f1d15620":"test_split = 0.90 # the percent of data to be used for testing\nn = int(ohlcv_histories_normalised.shape[0] * test_split)\nn","9ee93f8a":"x_train = ohlcv_histories_normalised[:n]\nprint(x_train.shape)","9006ec8e":"y_train = weighted_avg_values_normalised[:n]\ny_train.reshape(n,1)\nprint(y_train.shape)","08d537a6":"x_test = ohlcv_histories_normalised[n:]\ny_test = weighted_avg_values_normalised[n:]","7d8b6b8b":"corrMatx =  df1.corr()\ncorrMatx","6e6ae798":"tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()","21b177cb":"tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","35411ada":"with tpu_strategy.scope():\n    start = time.process_time()\n    model21=Sequential()\n    model21.add(LSTM(units=100, return_sequences=True, input_shape=(history_points,7)))\n    model21.add(Dropout(0.20))\n    model21.add(LSTM(units=100, return_sequences=True))\n    model21.add(Dropout(0.20)) \n    model21.add(LSTM(units=100, return_sequences=True))\n    model21.add(Dropout(0.20)) \n    model21.add(LSTM(units=100))\n    model21.add(Dropout(0.20)) \n    model21.add(Dense(units=1))\n    model21.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n\n","8c863731":"model21.fit(x_train,y_train,batch_size=128, epochs=12) \nprint(time.process_time() - start)","4e7f02fd":"print(model21.summary())","3655992a":"y_test_predicted21 = model21.predict(x_test)","f868ab6d":"y_test= y_normaliser.inverse_transform(y_test)\ny_test","36f49e40":"y_test_predicted21 = y_normaliser.inverse_transform(y_test_predicted21)\ny_test_predicted21","5d9fd3f0":"plt.gcf().set_size_inches(22, 10, forward=True)\n\nstart = 0\nend = -1\n#actual=plt.plot(actual[start+60:end], label= \"actual\")\nreal = plt.plot(y_test[start:end], label='real', color=\"black\")\npred = plt.plot(y_test_predicted21[start:end], label='predicted', color=\"red\")\n\nplt.legend(['Real', 'Predicted'])\nplt.title(\" Share Price\")\nplt.xlabel(\"Days\")\nplt.ylabel(\"Share Price\")\nplt.show()","087f2a27":"time series analysis","a85718fd":"To find out the null values"}}