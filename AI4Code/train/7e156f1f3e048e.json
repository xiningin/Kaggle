{"cell_type":{"d53ccfcb":"code","137bb3b0":"code","74cc05b7":"code","a8652515":"code","a1ecbfed":"code","1b4aba29":"code","14a1cb2b":"code","95a84db9":"markdown","655af19e":"markdown","26bebd09":"markdown","92dff92c":"markdown","ba7e265c":"markdown","891ea011":"markdown"},"source":{"d53ccfcb":"import numpy as np\nimport torch\nfrom torch.autograd import Variable","137bb3b0":"# \u0417\u0430\u0433\u0440\u0443\u0437\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043f\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044e \u0440\u0430\u043a\u0430 \u0433\u0440\u0443\u0434\u0438\nfrom sklearn import datasets, model_selection\nimport matplotlib.pyplot as plt\n\nbreast_cancer_data = datasets.load_breast_cancer()\n\npoints = breast_cancer_data['data']\nlabels = breast_cancer_data['target'].reshape(-1, 1)\ntraining_points, test_points, training_labels, test_labels = model_selection.train_test_split(\n    points, labels, test_size=0.33, random_state=42)","74cc05b7":"# TODO \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u043a\u043b\u0430\u0441\u0441 \u0434\u043b\u044f \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438\n# \u0437\u0430\u0434\u0430\u0434\u0438\u043c \u043a\u043b\u0430\u0441\u0441 \u0434\u043b\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438. \u041c\u044b \u043d\u0430\u0441\u043b\u0435\u0434\u0443\u0435\u043c \u0435\u0433\u043e \u043e\u0442 torch.nn.Module\n# \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u044b \u0434\u0432\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438: \n# * __init__ \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442, \u043a\u0430\u043a \u043c\u044b \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u043e\u0431\u044a\u0435\u043a\u0442 \u044d\u0442\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430\n# * forward \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442, \u043a\u0430\u043a \u043c\u044b \u0434\u0435\u043b\u0430\u0435\u043c \u043f\u0440\u043e\u0433\u043d\u043e\u0437\n\nclass logisticRegression(torch.nn.Module):\n  # \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043e\u0431\u044a\u0435\u043a\u0442\u0430 \u043a\u043b\u0430\u0441\u0441\u0430\n  def __init__(self, input_dimension, output_dimension ):\n    # \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u0437\u0430\u043f\u0443\u0441\u0442\u0438\u043c init \u0430\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u044b\u0439 \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u0441\u043a\u043e\u043c\u0443 \u043a\u043b\u0430\u0441\u0441\u0443\n    super(logisticRegression, self).__init__() \n    # \u0437\u0430\u0442\u0435\u043c \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0441\u0432\u043e\u0439 \u043f\u0435\u0440\u0432\u044b\u0439 \u0441\u043b\u043e\u0439 \u043d\u0430 pytorch\n    # \u0442\u0430\u043a \u043a\u0430\u043a \u043c\u043e\u0434\u0435\u043b\u044c \u0443 \u043d\u0430\u0441 \u043b\u0438\u043d\u0435\u0439\u043d\u0430\u044f, \u0442\u043e \u0438 \u0441\u043b\u043e\u0439 \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0439\n    self.linear = torch.nn.Linear(input_dimension, output_dimension)\n    self.Sigmoid = torch.nn.Sigmoid()\n  # \u043f\u043e\u0434\u0441\u0447\u0435\u0442 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0430\n  def forward(self, x):\n    out = self.linear(x)\n    out = self.Sigmoid(out)\n    return out","a8652515":"# \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043e\u0431\u044a\u0435\u043a\u0442 \u043d\u0430\u0448\u0435\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430, \u043f\u043e\u043b\u0443\u0447\u0438\u0432 \u0441\u043f\u0435\u0440\u0432\u0430 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 \ninput_dimension = training_points.shape[1]\noutput_dimension = 1\n\n\n# \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c GPU, \u0447\u0442\u043e\u0431\u044b \u0432\u0441\u0435 \u0441\u0447\u0438\u0442\u0430\u043b\u043e\u0441\u044c \u0431\u044b\u0441\u0442\u0440\u0435\u0435\nif torch.cuda.is_available():\n    logistic_regression_model.cuda()\n    \n# \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c, \u0447\u0442\u043e \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0438 \u043a\u0430\u043a\nlearning_rate = 1e-3\n\n\n# \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043e\u0431\u044a\u0435\u043a\u0442 \u043d\u0430\u0448\u0435\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430, \u043f\u043e\u043b\u0443\u0447\u0438\u0432 \u0441\u043f\u0435\u0440\u0432\u0430 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 \ninput_dimension = training_points.shape[1] # training_points, test_points, training_labels, test_labels\noutput_dimension = 1\n\nmodel = logisticRegression(input_dimension, output_dimension)\n\n# \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c GPU, \u0447\u0442\u043e\u0431\u044b \u0432\u0441\u0435 \u0441\u0447\u0438\u0442\u0430\u043b\u043e\u0441\u044c \u0431\u044b\u0441\u0442\u0440\u0435\u0435\nif torch.cuda.is_available():\n    model.cuda()\n    \n    \n\n# \u0431\u0443\u0434\u0435\u043c \u043c\u0438\u043d\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043a\u0432\u0430\u0434\u0440\u0430\u0442\u0438\u0447\u043d\u0443\u044e \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u043f\u043e\u0442\u0435\u0440\u044c\ncriterion = torch.nn.BCEWithLogitsLoss()# MSELoss() \n# \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 ADAM (\u0434\u0430\u043b\u044c\u0448\u0435 \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0430\u0437\u043d\u044b\u0435 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u044b \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n","a1ecbfed":"epochs = 12000\n\n\n# \u0432\u0445\u043e\u0434\u043d\u044b\u0435 \u0438 \u0432\u044b\u0445\u043e\u0434\u043d\u044b\u0435 \u043c\u0430\u0442\u0440\u0438\u0446\u044b \u0441\u043a\u043e\u043d\u0435\u0432\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0432 \u0442\u0435\u043d\u0437\u043e\u0440\u044b\nif torch.cuda.is_available():\n  inputs = Variable(torch.from_numpy(training_points).float().type(torch.float).cuda())\n  outputs = Variable(torch.from_numpy(training_labels).float().type(torch.float).cuda())\nelse:\n  inputs = Variable(torch.from_numpy(training_points).float())\n  outputs = Variable(torch.from_numpy(training_labels).float())\n\nfor epoch in range(epochs):\n  # \u043d\u0443\u0436\u043d\u043e \u043e\u0431\u043d\u0443\u043b\u0438\u0442\u044c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u044b, \u0438\u043d\u0430\u0447\u0435 \u0431\u0443\u0434\u0435\u0442 \u0438\u0445 \u0430\u043a\u043a\u0443\u043c\u0443\u043b\u0438\u0440\u043e\u0432\u0430\u0442\u044c\n  optimizer.zero_grad()\n\n  # \u043f\u043e\u043b\u0443\u0447\u0438\u043c \u0432\u044b\u0445\u043e\u0434 \u043c\u043e\u0434\u0435\u043b\u0438 \u0441 \u0443\u0447\u0435\u0442\u043e\u043c \u043d\u0430\u043b\u0438\u0447\u0438\u044f \u0432\u0445\u043e\u0434\u043e\u0432\n  training_predictions = model(inputs)\n\n  # \u043f\u043e\u043b\u0443\u0447\u0438\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u043e\u0432\n  loss_value = criterion(training_predictions, outputs)\n  # \u043f\u043e\u0434\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u044b\n  loss_value.backward()\n\n  # \u0441\u0434\u0435\u043b\u0430\u0435\u043c \u0430\u043f\u0434\u0435\u0439\u0442 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432\n  optimizer.step()\n  \n  if epoch % 100 == 1:\n    print('epoch {}, loss {}'.format(epoch, loss_value.item()))","1b4aba29":"# For train sample \nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\ntraining_predictions = model(inputs)\nroc_auc_score(outputs, training_predictions.detach().numpy().ravel() )\n\n","14a1cb2b":"# For test sample:\n\n# \u0432\u0445\u043e\u0434\u043d\u044b\u0435 \u0438 \u0432\u044b\u0445\u043e\u0434\u043d\u044b\u0435 \u043c\u0430\u0442\u0440\u0438\u0446\u044b \u0441\u043a\u043e\u043d\u0435\u0432\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0432 \u0442\u0435\u043d\u0437\u043e\u0440\u044b\nif torch.cuda.is_available():\n  inputs2 = Variable(torch.from_numpy(test_points).float().type(torch.float).cuda())\n  outputs2 = Variable(torch.from_numpy(test_labels).float().type(torch.float).cuda())\nelse:\n  inputs2 = Variable(torch.from_numpy(test_points).float())\n  outputs2 = Variable(torch.from_numpy(test_labels).float())\n    \ntraining_predictions = model(inputs2)\n\nroc_auc_score(outputs2, training_predictions.detach().numpy().ravel() )\n","95a84db9":"# What is about ?\n\nImplement Logistic Regression with pytorch","655af19e":"# Train neural network","26bebd09":"# Class Initialization","92dff92c":"# Calculate roc_auc_score ","ba7e265c":"# Load data ","891ea011":"# Create class for LogReg with pytorch neural networkds setup "}}