{"cell_type":{"9184dd61":"code","59759a80":"code","541175ec":"code","d252193d":"code","7fda6436":"code","63949994":"code","932f1a2c":"code","45828bd8":"code","12959967":"code","ae2fa6d6":"code","76b38f9f":"code","0670b0d3":"code","5fdf703a":"code","7c692364":"code","d0f987f0":"code","b5e2f96a":"code","e0a808ec":"code","e851a5e1":"markdown","2b7a50c1":"markdown","83226591":"markdown","7dbb8da7":"markdown","122f3fac":"markdown","4a51390d":"markdown","4a632fee":"markdown","e3fcb411":"markdown"},"source":{"9184dd61":"# Run on GPU kernel\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, LSTM, concatenate, Activation, CuDNNLSTM, MaxPool1D, Flatten\nfrom keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, Permute, Dropout, Bidirectional\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, roc_auc_score","59759a80":"raw_data = np.loadtxt('..\/input\/exoTrain.csv', skiprows=1, delimiter=',')\nx_train = raw_data[:, 1:]\ny_train = raw_data[:, 0, np.newaxis] - 1.\nraw_data = np.loadtxt('..\/input\/exoTest.csv', skiprows=1, delimiter=',')\nx_test = raw_data[:, 1:]\ny_test = raw_data[:, 0, np.newaxis] - 1.\ndel raw_data","541175ec":"x_train = ((x_train - np.mean(x_train, axis=1).reshape(-1,1)) \/ \n           np.std(x_train, axis=1).reshape(-1,1))\nx_test = ((x_test - np.mean(x_test, axis=1).reshape(-1,1)) \/ \n          np.std(x_test, axis=1).reshape(-1,1))","d252193d":"X_train, X_val, y_train, y_val = train_test_split(x_train, y_train, stratify=y_train,\n                                                  test_size=0.3, random_state=123)","7fda6436":"np.set_printoptions(threshold=np.inf)\nX_train_r = np.expand_dims(X_train, axis=2)\nX_val_r = np.expand_dims(X_val, axis=2)\nx_test = np.expand_dims(x_test, axis=2)","63949994":"def batch_generator(x_train, y_train, batch_size=32):\n    \"\"\"\n    Gives equal number of positive and negative samples, and rotates them randomly in time\n    \"\"\"\n    half_batch = batch_size \/\/ 2\n    x_batch = np.empty((batch_size, x_train.shape[1], x_train.shape[2]), dtype='float32')\n    y_batch = np.empty((batch_size, y_train.shape[1]), dtype='float32')\n    \n    yes_idx = np.where(y_train[:,0] == 1.)[0]\n    non_idx = np.where(y_train[:,0] == 0.)[0]\n    \n    while True:\n        np.random.shuffle(yes_idx)\n        np.random.shuffle(non_idx)\n    \n        x_batch[:half_batch] = x_train[yes_idx[:half_batch]]\n        x_batch[half_batch:] = x_train[non_idx[half_batch:batch_size]]\n        y_batch[:half_batch] = y_train[yes_idx[:half_batch]]\n        y_batch[half_batch:] = y_train[non_idx[half_batch:batch_size]]\n    \n        for i in range(batch_size):\n            sz = np.random.randint(x_batch.shape[1])\n            x_batch[i] = np.roll(x_batch[i], sz, axis = 0)\n     \n        yield x_batch, y_batch","932f1a2c":"ip = Input(shape=(3197, 1))","45828bd8":"# LSTM\nx = Permute((2, 1))(ip)\nx = CuDNNLSTM(16, return_sequences=True)(x)\nx = CuDNNLSTM(32, return_sequences=True)(x)\nx = CuDNNLSTM(64, return_sequences=True)(x)\nx = CuDNNLSTM(128)(x)\nx = Dropout(0.25)(x)","12959967":"y = Conv1D(filters=16, kernel_size=11, activation='relu')(ip)\ny = MaxPool1D(strides=4)(y)\ny = BatchNormalization()(y)\ny = Conv1D(filters=32, kernel_size=11, activation='relu')(y)\ny = MaxPool1D(strides=4)(y)\ny = BatchNormalization()(y)\ny = Conv1D(filters=64, kernel_size=11, activation='relu')(y)\ny = MaxPool1D(strides=4)(y)\ny = BatchNormalization()(y)\ny = Conv1D(filters=128, kernel_size=11, activation='relu')(y)\ny = MaxPool1D(strides=4)(y)\ny = Flatten()(y)\ny = Dropout(0.25)(y)\ny = Dense(64, activation='relu')(y)","ae2fa6d6":"#Concatenate\nx = concatenate([x, y])\nx = Dense(32, activation='relu')(x)\nout = Dense(1, activation='sigmoid')(x)\nmodel = Model(ip, out)\nmodel.summary()","76b38f9f":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","0670b0d3":"model.fit_generator(batch_generator(X_train_r, y_train, 32), \n                           validation_data=(X_val_r, y_val), \n                           epochs=100,\n                           steps_per_epoch=X_train_r.shape[1]\/\/32, verbose=0)","5fdf703a":"y_pred = model.predict(x=x_test)","7c692364":"non_idx = np.where(y_test[:,0] == 0.)[0]\nyes_idx = np.where(y_test[:,0] == 1.)[0]\nplt.plot([y_pred[i] for i in yes_idx], 'bo')\nplt.show()\nplt.plot([y_pred[i] for i in non_idx], 'ro')\nplt.show()","d0f987f0":"#ROC Area under curve\ny_true = (y_test[:, 0] + 0.5).astype(\"int\")\nfpr, tpr, thresholds = roc_curve(y_true, y_pred)\nplt.plot(thresholds, 1.-fpr)\nplt.plot(thresholds, tpr)\nplt.show()\ncrossover_index = np.min(np.where(1.-fpr <= tpr))\ncrossover_cutoff = thresholds[crossover_index]\ncrossover_specificity = 1.-fpr[crossover_index]\nprint(\"Crossover at {0:.2f} with specificity {1:.2f}\".format(crossover_cutoff, crossover_specificity))\nplt.plot(fpr, tpr)\nplt.show()\nprint(\"ROC area under curve is {0:.2f}\".format(roc_auc_score(y_true, y_pred)))","b5e2f96a":"predthr = np.where(y_pred > 0.5, 1, 0)\nprint(classification_report(y_test, predthr))","e0a808ec":"print(confusion_matrix(y_test, predthr))","e851a5e1":"****Plot prediction results, as probabilities****","2b7a50c1":"****Reshape****","83226591":"****Scaling****","7dbb8da7":"***This model tries to improve upon Mr. Peter Grenholm's model (CNN), and therefore uses parts of his model and data visualization.*","122f3fac":"Model: \nTemporal convolutions have proven to be an effective learning model for time series classi\ufb01cation problems. Fully Convolutional Networks, comprised of temporal convolutions, are typically used as feature extractors. Global average pooling is used to reduce the number of parameters in the model prior to classi\ufb01cation. In the proposed models, the fully convolutional block is augmented by an LSTM block (multiple LSTM layers, each followed by dropout). The fully convolutional block consists of four stacked temporal convolutional blocks. Each block consists of a temporal convolutional layer, which is accompanied by batch normalization and followed by a ReLU activation function. Finally, global average pooling is applied after the \ufb01nal convolution block. Simultaneously, the time series input is conveyed into a dimension shu\ufb04e layer. The transformed time series from the dimension shuffle is then passed into the LSTM block. The LSTM comprising of a LSTM layer, is followed by a dropout. The output of the global pooling layer and the LSTM block is concatenated and passed onto a sigmoid classi\ufb01cation layer. The fully convolutional block and LSTM block perceive the same time series input in two di\ufb00erent views. The fully convolutional block views the time series as a univariate time series with multiple time steps. If there is a time series of length N, the fully convolutional block will receive the data in N time steps. In contrast, the LSTM block in the proposed architecture receives the input time series as a multivariate time series with a single time step. This is accomplished by the dimension shuffle layer, which transposes the temporal dimension of the time series. A univariate time series of length N, after transformation, will be viewed as a multivariate time series (having N variables) with a single time step.","4a51390d":"****Create model****","4a632fee":"Label encoder (1\/2 -> 0\/1) and separate label from train and test","e3fcb411":"****Add validation set (split train set)****"}}