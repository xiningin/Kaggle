{"cell_type":{"0c58fc33":"code","f7bdaaec":"code","3f9880dd":"code","c21bfc09":"code","f876157d":"code","9cf3a2a7":"code","4068a7e1":"code","82dcac49":"code","e4f20f96":"code","8f905ebd":"code","8d60295a":"code","bf967bd6":"code","b2f1fbf6":"code","c3fb3ab8":"code","77a10de4":"code","64bb3af3":"code","d602eae9":"code","b046e7e2":"markdown","3b4cfa72":"markdown","da9e47a5":"markdown","64f71fbb":"markdown","d0fffe1b":"markdown","8e6fa68e":"markdown","b355739e":"markdown","92f21ee8":"markdown","3ab4ab35":"markdown","a3a19ee4":"markdown","061e8db7":"markdown","e99bf48c":"markdown"},"source":{"0c58fc33":"#BASIC\nimport numpy as np \nimport pandas as pd \nimport os\nimport cv2\nimport re\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\nimport hashlib\nimport plotly.graph_objects as go\nimport matplotlib.patches as patches\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Image, display\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff","f7bdaaec":"DIR = \"..\/input\/global-wheat-detection\/\"\nTRAIN = \"train.csv\"\n\nTRAIN_IMG = \"train\"\nTEST_IMG= \"test\"\nWIDTH = 1024\nHEIGHT = 1024\n\nTRAIN_IMAGES = [os.path.join(DIR, \"train\", fname) for fname in os.listdir(os.path.join(DIR, \"train\"))]\nTEST_IMAGES = [os.path.join(DIR, \"test\", fname) for fname in os.listdir(os.path.join(DIR, \"test\"))]\n\ntrain_df = pd.read_csv(os.path.join(DIR, TRAIN))\n","3f9880dd":"train_df.info()","c21bfc09":"train_df.head()","f876157d":"print(\"unique ids : \", len(train_df.image_id.unique()))\nprint(\"unique width : \", len(train_df.width.unique()))\nprint(\"unique height : \", len(train_df.height.unique()))\nprint(\"unique source : \", len(train_df.source.unique()))","9cf3a2a7":"train_df.source.value_counts()\n#7 unique sources of wheat head images","4068a7e1":"print(f\"Total training images: {len(TRAIN_IMAGES)}\")\nprint(f\"Total test images: {len(TEST_IMAGES)}\")\n","82dcac49":"bbox_wrt_source = train_df.groupby([\"source\"]).apply(lambda x:x[\"image_id\"].value_counts().mean())","e4f20f96":"bbox_wrt_source","8f905ebd":"source=train_df['source'].value_counts()\nfig = go.Figure(data=[\n    go.Pie(labels=source.index, values=source.values)\n])\n\nfig.update_layout(title='Source distribution for data')\nfig.show()","8d60295a":"plt.figure(figsize=(12, 8))\nsns.distplot(train_df['image_id'].value_counts().values)\nplt.show()","bf967bd6":"plt.figure(figsize=(12, 8))\nbbox_wrt_source.plot(kind='bar')\nplt.show()","b2f1fbf6":"area_per_image = train_df.groupby(\"image_id\").apply(lambda x: (x[\"width\"]*x[\"height\"]).sum()\/(WIDTH*HEIGHT))\nplt.figure(figsize=(10, 6))\nplt.title(\"Area % for each image.\")\nprint(f\"Min area per image: {area_per_image.min()}%\")\nprint(f\"Max area per image: {area_per_image.max()}%\")\nprint(f\"Mean area per image: {area_per_image.mean()}%\")\nprint(f\"Std area per image: {area_per_image.std()}%\")\nsns.distplot(area_per_image)\nplt.show()","c3fb3ab8":"# Visualize few samples of current training dataset\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20, 10))\ncount=0\nfor row in ax:\n    for col in row:\n        img = plt.imread(f'{os.path.join(DIR, \"train\", train_df[\"image_id\"].unique()[count])}.jpg')\n        col.grid(False)\n        col.set_xticks([])\n        col.set_yticks([])\n        col.imshow(img)\n        count += 1\nplt.show()","77a10de4":"import ast\ntrain_df[['x_min','y_min', 'width', 'height']] = pd.DataFrame([ast.literal_eval(x) for x in train_df.bbox.tolist()], index= train_df.index)\ntrain_df = train_df[['image_id', 'bbox', 'source', 'x_min', 'y_min', 'width', 'height']]\ntrain_df","64bb3af3":"def get_bbox(image_id, df, col, color='white'):\n    bboxes = df[df['image_id'] == image_id]\n    \n    for i in range(len(bboxes)):\n        # Create a Rectangle patch\n        rect = patches.Rectangle(\n            (bboxes['x_min'].iloc[i], bboxes['y_min'].iloc[i]),\n            bboxes['width'].iloc[i], \n            bboxes['height'].iloc[i], \n            linewidth=2, \n            edgecolor=color, \n            facecolor='none')\n\n        # Add the patch to the Axes\n        col.add_patch(rect)\n    ","d602eae9":"# Visualize few samples of current training dataset\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20, 20))\ncount=0\nfor row in ax:\n    for col in row:\n        img_id = train_df[\"image_id\"].unique()[count]\n        img = plt.imread(f'{os.path.join(DIR, \"train\", img_id)}.jpg')\n        col.grid(False)\n        col.set_xticks([])\n        col.set_yticks([])\n        get_bbox(img_id, train_df, col, color='red')\n        col.imshow(img)\n        count += 1\nplt.show()\n","b046e7e2":"> Normally\/ Gaussian distribution of % area","3b4cfa72":"**What only ten training images**","da9e47a5":"> Area per image","64f71fbb":"> **EDA**","d0fffe1b":"> **Total Training & Testing  Images **","8e6fa68e":"![Wheat](http:\/\/cdn-a.william-reed.com\/var\/wrbm_gb_food_pharma\/storage\/images\/1\/9\/4\/3\/2903491-1-eng-GB\/Global-wheat-production-to-fall-in-2016-season-reports-FAO_wrbm_large.jpg)","b355739e":"**Urbanization, rising incomes and working women are driving a rapid rise in global wheat consumption. Models predict that by 2050 consumers will require 60 percent more wheat than today. Challenges are big: this demand must be met without opening new land and with better use of fertilizer, water, and labor.**","92f21ee8":"**Spread of bounding boxes per image is:**","3ab4ab35":"> Extracting Dimensions of bounding boxes in data frame","a3a19ee4":"*Let **Visulize** BBOX For Corresponding Images of Wheat Heads *","061e8db7":"![Wheat](https:\/\/wheat.org\/wp-content\/uploads\/sites\/4\/2014\/10\/LoadingOven-08-890x1024.jpg)","e99bf48c":"\"About this Competition\"\n\"Supporting the shit for sake of the breads to have for the dinner\"\n\n> In this competition, you\u2019ll detect wheat heads from outdoor images of wheat plants, including wheat datasets from around the globe. Using worldwide data, you will focus on a generalized solution to estimate the number and size of wheat heads. To better gauge the performance for unseen genotypes, environments, and observational conditions, the training dataset covers multiple regions. You will use more than 3,000 images from Europe (France, UK, Switzerland) and North America (Canada). The test data includes about 1,000 images from Australia, Japan, and China.\n\nWheat is a staple across the globe, which is why this competition must account for different growing conditions. Models developed for wheat phenotyping need to be able to generalize between environments. If successful, researchers can accurately estimate the density and size of wheat heads in different varieties. With improved detection farmers can better assess their crops, ultimately bringing cereal, toast, and other favorite dishes to your table.\n\n"}}