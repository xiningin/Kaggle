{"cell_type":{"3a705118":"code","b2398a48":"code","5466418d":"code","8b0a36b9":"code","6778b8fb":"code","442ed20f":"code","2ac83b80":"code","05683d5d":"code","b6da44c6":"code","8a69cd30":"code","1fdfd1ac":"code","5eb9f52c":"code","60359629":"code","e1a06a40":"code","d5096cab":"code","4724cf44":"code","7b70463c":"code","925eb480":"code","0e901ccd":"code","fe90974f":"code","773fd406":"code","dfe34997":"code","56b2908e":"code","7fd01f70":"code","ffcd0e77":"code","f22c3084":"code","f8d4c6ad":"code","287ba685":"code","a8c01dc6":"code","47a299d3":"code","053bb891":"code","80b7e2a0":"code","52dfd974":"code","5a2e0f41":"code","19e985ab":"code","bbe4238b":"code","578855b9":"code","60439cc9":"code","2a41e965":"code","b4b8510c":"code","ffa19f86":"code","03332caa":"code","0975cf5e":"code","4576e1cb":"code","eba205c4":"code","caa75b4e":"code","64eb007c":"code","8670ca51":"code","484006dd":"code","d184f5ee":"code","02b363da":"code","efefdf70":"code","22a5868f":"code","f511ccfb":"code","f3504dc0":"code","080fa3ce":"code","fcb88a51":"code","d1a57fbb":"code","367906ce":"code","82be19dc":"code","16f555d2":"code","10cf7e17":"code","7b25eaec":"code","a156a13c":"markdown","586a7af7":"markdown","cd26eea4":"markdown","cb3b9303":"markdown","4c505d99":"markdown","a1130185":"markdown","047f5665":"markdown","21cf8e3e":"markdown","8bfe9293":"markdown","197ac3d2":"markdown","e606a7bc":"markdown","ddb5d1c1":"markdown","9fa495b9":"markdown","aaa77c20":"markdown","f348cec2":"markdown","02cceee3":"markdown","45459882":"markdown","a3433478":"markdown","962a1ec8":"markdown","ab35ff8d":"markdown","4dc4f43e":"markdown"},"source":{"3a705118":"!pip install simple-colors","b2398a48":"import pandas as pd\nimport numpy as np\nimport optuna\nimport warnings\n\nfrom simple_colors import *\nfrom termcolor import colored\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import normaltest\nfrom scipy import stats\n\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, GradientBoostingRegressor, ExtraTreesRegressor, BaggingRegressor\nfrom sklearn import linear_model\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import SGDRegressor, RidgeCV, HuberRegressor, PoissonRegressor\nfrom catboost import CatBoostRegressor","5466418d":"# Load the data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")","8b0a36b9":"def data_desc(df):\n    \n    \"\"\"\n    This function helps us with simple data analysis.\n    We may explore the common information about the dataset, missing values, features distribution and duplicated rows\n    \"\"\"\n    \n    # applying info() method\n    print('*******************')\n    print(cyan('General information of this dataset', 'bold'))\n    print('*******************\\n')\n    print(df.info())\n    \n    # missing values\n    print('\\n*******************')\n    if df.isna().sum().sum() == 0:\n        print(colored('There are no missing values', 'green'))\n        print('*******************')\n    else:\n        print(colored('Missing value detected', 'green'))\n        display(df.isna().sum())\n\n    \n    # applying describe() method for categorical features\n    cat_feats = [col for col in df.columns if df[col].dtype == object]\n    print('\\n*******************')\n    print(cyan('Categorical columns', 'bold'))\n    print('*******************\\n')\n    print(\"Total categorical features:\", colored(len(cat_feats), 'green', attrs=['bold']))\n    display(df.describe(include = 'object').T)\n    \n    # same describe() but for numerical features\n    cont_feats = [col for col in df.columns if df[col].dtype != object and col not in ('id', 'target')]\n    print('\\n*******************')\n    print(cyan('Numerical columns', 'bold'))\n    print('*******************\\n')\n    print(\"Total numerical features:\", colored(len(cont_feats), 'green', attrs=['bold']))\n    df = df[df.columns.difference(['id', 'target'], sort = False)]\n    display(df.describe().T)\n    \n    # Checking for duplicated rows\n    if df.duplicated().sum() == 0:\n        print('\\n*******************')\n        print(colored('There are no duplicates', 'green'))\n        print('*******************')\n    else:\n        print('\\n*******************')\n        print(colored('Duplicates found', 'green'))\n        print('*******************')\n        display(df[df.duplicated()])\n\n    print('\\n*******************')\n    print(cyan('Number of rows and columns', 'bold'))\n    print('*******************\\n')\n    print('Rows and Columns in this dataset:', colored(train.shape, 'green', attrs=['bold']))\n\n\n    print('\\n*******************')\n    print(cyan('Preview the data - Top 10 rows', 'bold'))\n    print('*******************\\n')\n    display(df.head(10))\n    print('*******************\\n')\n    \n    print('\\n*******************')\n    print(cyan('End of the report', 'bold'))","6778b8fb":"data_desc(train)","442ed20f":"data_desc(test)","2ac83b80":"categorical_features =[]\nnumerical_features =[]\n\nfor col in train.columns:\n    if train[col].dtype == object:\n        categorical_features.append(col)\n    elif train[col].dtype != 'object' and col not in ('id', 'target'):\n        numerical_features.append(col)\nprint('Catagoric features: ', categorical_features)\nprint('Numerical features: ', numerical_features)","05683d5d":"# Cardinality check\n\nprint(colored(\"In Train Dataset\", 'cyan', attrs=['bold', 'underline']))\nfor col in categorical_features:\n    print('{} unique values in {}'.format(train[col].nunique(), col))\n\nprint()\nprint(colored(\"In Test Dataset\", 'cyan', attrs=['bold', 'underline']))\nfor col in categorical_features:\n    print('{} unique values in {}'.format(test[col].nunique(), col))","b6da44c6":"def cardinality(data):\n    for k in categorical_features:\n        print(f'{k}\\n{(np.round((data[k].value_counts() \/ len(data[k]))*100,3))}\\n')","8a69cd30":"cardinality(train)","1fdfd1ac":"cardinality(test)","5eb9f52c":"import plotly.graph_objects as go\n\nfig = go.Figure([go.Bar(x = train[categorical_features].nunique().index, y = train[categorical_features].nunique().values, marker_color='rgb(100, 14, 175)')])\n#fig.show()\n\nfig.update_traces(marker_line_color='rgb(120, 15, 155)', marker_line_width=1, opacity=0.7)\n\nfig.update_layout(\n    title=\"<b>Number of unique values of categorical features<b>\",\n    width=1200,\n    height=700,\n    \n    xaxis = dict(showline=True,\n    title = '<b>Categorical Variables<b>',\n    tickangle = -30,\n    tickfont = dict(family='Times New Roman', color='black', size=16),\n    titlefont_size = 16,\n    ),\n\n    yaxis = dict(showline=True,\n    ticks = \"outside\", tickwidth=2, tickcolor='red', ticklen=7.5,\n    title = '<b># of unique values<b>',\n    tickfont = dict(family = 'Times New Roman', color='black', size=16),\n    titlefont_size = 16,\n    title_standoff = 5,\n    ),\n    bargap = 0.25, # gap between bars of adjacent location coordinates.   \n)","60359629":"def count_plot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 4\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(35, 20))\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last -= 1\n\n    fig.subplots_adjust(top=0.95)\n\n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.countplot(x = feature, palette='rocket_r', data=data, hue=None)\n        plt.xlabel(feature, fontsize=14, fontweight = 'bold')\n        plt.ylabel('#', fontsize=14, fontweight = 'bold')\n        for p in ax.patches:\n            height = p.get_height()\n            ax.text(p.get_x()+p.get_width()\/2., height+1000, height, ha=\"center\", fontsize = 10, fontweight = 'bold')     \n        i += 1\n    \n    plt.suptitle(titleText, fontsize = 25, fontweight = 'bold', color = 'darkorange')\n    plt.show()    ","e1a06a40":"count_plot(train, categorical_features, 'Categorical features of train dataset', hue=None)","d5096cab":"count_plot(test, categorical_features, 'Categorical features of test dataset', hue=None)","4724cf44":"def count_plot_testTrain(data1, data2, features, titleText):\n  \n    L = len(features)\n    nrow= int(np.ceil(L\/4))\n    ncol= 4\n    remove_last= (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol,figsize=(35, 20))\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last = remove_last - 1\n\n    fig.subplots_adjust(top=0.95)\n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.countplot(x=feature, color='#61057c', data=data1, label='train')\n        ax = sns.countplot(x=feature, color='#b7f035', data=data2, label='test')\n        plt.xlabel(feature, fontsize=14, fontweight = 'bold')\n        plt.ylabel('#', fontsize=14, fontweight = 'bold')\n        ax = ax.legend(loc = \"best\")\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 25, fontweight = 'bold', color = 'indigo')\n    plt.show()","7b70463c":"count_plot_testTrain(train, test, categorical_features, titleText='Categorical features of train & test datasets')","925eb480":"def box_plot(data, features, titleText):\n\n    v0 = sns.color_palette(palette = \"crest\").as_hex()[0]\n    fig = plt.figure(figsize=(15, 8))\n    sns.boxplot(data = data[features], color=v0,saturation=.75);\n    plt.xticks(fontsize= 14)\n    plt.yticks(fontsize= 14)\n    plt.title(titleText, fontsize=16, fontweight = 'bold');","0e901ccd":"box_plot(train, numerical_features, 'Box Plot of Numerical Columns of Train Dataset')","fe90974f":"box_plot(train, [('target')], 'Box Plot of Target Value of Train Dataset')","773fd406":"box_plot(test, numerical_features, 'Box Plot of Numerical Columns of Test Dataset')","dfe34997":"def kde_plot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 4\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(35, 20))\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last -= 1\n\n    fig.subplots_adjust(top = 0.95)\n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.kdeplot(data[feature], color=\"m\", shade=True, label=\"%.3f\"%(data[feature].skew()))  \n        ax = ax.legend(loc = \"best\")    \n        plt.xlabel(feature, fontsize=14, fontweight = 'bold')\n        plt.ylabel('Density', fontsize=14, fontweight = 'bold')\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 25, fontweight = 'bold', color = 'navy')\n    plt.show()","56b2908e":"kde_plot(train, numerical_features, titleText = 'KDE Plot of Numerical Features of Train Dataset', hue = None)","7fd01f70":"kde_plot(test, numerical_features, titleText = 'KDE Plot of Numerical Features of Test Dataset', hue = None)","ffcd0e77":"def correlation_matrix(data):\n\n    fig, ax = plt.subplots(1, 1, figsize=(25, 10))\n    plt.title('Pearson Correlation Matrix', fontweight='bold', fontsize=25)\n    fig.set_facecolor('#d0d0d0') \n    corr = data.drop('id', axis=1).corr()\n\n    # Mask to hide upper-right part of plot as it is a duplicate\n    mask = np.triu(np.ones_like(corr, dtype = bool))\n    sns.heatmap(corr, annot = True, center = 0, cmap = 'jet', mask = mask, vmin = -1, vmax = 1, linewidths=.5)\n    ax.set_xticklabels(ax.get_xticklabels(), fontfamily='sans', rotation=90, fontsize=12)\n    ax.set_yticklabels(ax.get_yticklabels(), fontfamily='sans', rotation = 0, fontsize=12)\n    plt.tight_layout()\n    plt.show()","f22c3084":"correlation_matrix(train)","f8d4c6ad":"correlation_matrix(test)","287ba685":"train[numerical_features].hist(figsize=(32,25));","a8c01dc6":"def qqplot(data, features, titleText, hue=None):\n\n    L = len(features)\n    nrow = int(np.ceil(L\/4))\n    ncol = 4\n    remove_last = (nrow * ncol) - L\n\n    fig, axs = plt.subplots(nrow, ncol, figsize=(35, 20))\n\n    while remove_last > 0:\n      axs.flat[-remove_last].set_visible(False)\n      remove_last -= 1\n\n    fig.subplots_adjust(top = 0.95)\n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)   \n        stats.probplot(data[feature],plot=plt)\n        plt.title('\\nQ-Q Plot')\n        plt.xlabel(feature, fontsize=14, fontweight = 'bold')\n        plt.ylabel('Density', fontsize=14, fontweight = 'bold')\n        i += 1\n\n    plt.suptitle(titleText, fontsize = 25, fontweight = 'bold', color = 'navy')\n    plt.show()","47a299d3":"qqplot(train, numerical_features, 'Q-Q Plot of Numerical Features of Train Dataset', hue=None)","053bb891":"qqplot(test, numerical_features, 'Q-Q Plot of Numerical Features of Test Dataset', hue=None)","80b7e2a0":"# D'Agostino and Pearson's Test\n\ndef normality_check(data):\n  for i in numerical_features:\n    # normality test\n    stat, p = normaltest(data[[i]])\n    print('Statistics=%.3f, p=%.3f' % (stat, p))\n    # interpret results\n    alpha = 1e-2\n    if p > alpha:\n        print(f'{i} looks Gaussian (fail to reject H0)\\n')\n    else:\n        print(f'{i} does not look Gaussian (reject H0)\\n')","52dfd974":"normality_check(train)","5a2e0f41":"normality_check(test)","19e985ab":"def detect_outliers(x, c = 1.5):\n    \"\"\"\n    Function to detect outliers.\n    \"\"\"\n    q1, q3 = np.percentile(x, [25,75])\n    iqr = (q3 - q1)\n    lob = q1 - (iqr * c)\n    uob = q3 + (iqr * c)\n\n    # Generate outliers\n\n    indicies = np.where((x > uob) | (x < lob))\n\n    return indicies","bbe4238b":"# Detect all Outliers \noutliers = detect_outliers(train['target'])\nprint(\"Total Outliers count for target : \", len(outliers[0]))\n\nprint(\"\\nShape before removing outliers : \",train.shape)\n\n# Remove outliers\n#train.drop(outliers[0],inplace=True, errors = 'ignore')\nprint(\"Shape after removing outliers : \",train.shape)","578855b9":"def create_stratified_folds_for_regression(df, n_splits = 10):\n\n    \"\"\"\n    @param data_df: training data to split in Stratified K Folds for a continous target value\n    @param n_splits: number of splits\n    @return: the training data with a column with kfold id\n    \"\"\"\n\n    df['StratifiedKFold'] = -1\n\n    # randomize the data\n    df = df.sample(frac=1).reset_index(drop=True)\n\n    # calculate the optimal number of bins based on log2(df.shape[0])\n    df_test = []\n    k = 0\n    df_ = df.select_dtypes(include='number')\n    df_ = df_.drop(['id', 'target', 'StratifiedKFold'], axis=1)\n\n    while k <= len(df_.columns)-1:\n      q1 = df_.iloc[:,k].quantile(0.25)\n      q3 = df_.iloc[:,k].quantile(0.75)\n      iqr = q3 - q1\n      bin_width = (2 * iqr) \/ (len(df_) ** (1 \/ 3))\n      bin_count = int(np.ceil((df_.iloc[:,k].max() - df_.iloc[:,k].min()) \/ bin_width))\n      df_test.append(bin_count)\n      mean_bin = np.ceil(sum(df_test) \/ len(df_test))\n      k = k + 1\n    print(f\"Num bins: {mean_bin}\")\n\n    # bins value will be the equivalent of class value of target feature used by StratifiedKFold to distribute evenly the classed over each fold\n    df.loc[:, \"bins\"] = pd.cut(pd.to_numeric(df['target'], downcast=\"signed\"), bins=int(mean_bin), labels=False)\n    kf = model_selection.StratifiedKFold(n_splits=n_splits, shuffle = True, random_state = 606)\n    \n    # set the fold id as a new column in the df data\n    for fold, (df_indicies, valid_indicies) in enumerate(kf.split(X=df, y=df.bins.values)):\n        df.loc[valid_indicies, \"StratifiedKFold\"] = fold\n    \n    # drop the bins column (no longer needed)\n    df = df.drop(\"bins\", axis=1)\n    \n    return df","60439cc9":"n_splits = 10\ntrain = create_stratified_folds_for_regression(train, n_splits)","2a41e965":"train.to_csv(\"train_folds(10).csv\", index=False)","b4b8510c":"train.StratifiedKFold.value_counts()","ffa19f86":"plt.figure(figsize=(25,12))\nplt.title(\"Distribution of target values (StratifiedKFolds with bins)\")\nfor k in range(0,n_splits):\n    df = train.loc[train.StratifiedKFold==k]\n    sns.distplot(df['target'],kde=True,hist=False, bins=12, label=k)\nplt.legend(); plt.show()","03332caa":"# List categorical column\noec = ['cat6', 'cat7', 'cat8', 'cat9']\nohec = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5']","0975cf5e":"## LGBM parameter tuning\n\ntrain = pd.read_csv(\"train_folds(10).csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in train.columns if c not in (\"id\", \"target\", \"StratifiedKFold\")]\nnumerical_features = [c for c in train.columns if c not in (\"id\", \"target\", \"StratifiedKFold\") and not 'cat' in c]\nobject_cols = [col for col in useful_features if 'cat' in col]\ntest = test[useful_features]\n\npoly = preprocessing.PolynomialFeatures(degree = 2, interaction_only = True, include_bias = False)\ntrain_poly = poly.fit_transform(train[numerical_features])\ntest_poly = poly.fit_transform(test[numerical_features])\n\ndf_train_poly = pd.DataFrame(train_poly, columns=[f\"POLY_{i}\" for i in range(train_poly.shape[1])])\ndf_test_poly = pd.DataFrame(test_poly, columns=[f\"POLY_{i}\" for i in range(test_poly.shape[1])])\n\ntrain = pd.concat([train, df_train_poly], axis=1)\ntest = pd.concat([test, df_test_poly], axis=1)\n\nfor col in object_cols:\n    temp_df = []\n    temp_test_feat = None\n    for fold in range(10):\n        xtrain =  train[train.StratifiedKFold != fold].reset_index(drop=True)\n        xvalid = train[train.StratifiedKFold == fold].reset_index(drop=True)\n        feat = xtrain.groupby(col)[\"target\"].agg(\"mean\")\n        feat = feat.to_dict()\n        xvalid.loc[:, f\"tar_enc_{col}\"] = xvalid[col].map(feat)\n        temp_df.append(xvalid)\n        if temp_test_feat is None:\n            temp_test_feat = test[col].map(feat)\n        else:\n            temp_test_feat += test[col].map(feat)\n    \n    temp_test_feat \/= 10\n    test.loc[:, f\"tar_enc_{col}\"] = temp_test_feat\n    train = pd.concat(temp_df)\n\nuseful_features = [c for c in train.columns if c not in (\"id\", \"target\", \"StratifiedKFold\")]\nnumerical_features = [c for c in train.columns if c not in (\"id\", \"target\", \"StratifiedKFold\") and not c.startswith(\"cat\")]\nobject_cols = [col for col in useful_features if col.startswith(\"cat\")]\ntest = test[useful_features]","4576e1cb":"def opt_lgbm(trial):\n    fold = 3\n\n    params = {\n        'objective': 'regression',\n        'metric': 'rmse',\n        'verbosity': 0,\n        'boosting_type': 'gbdt',\n        'learning_rate' : trial.suggest_float('learning_rate', 1e-2, 0.30, log=True),\n        'reg_lambda' : trial.suggest_loguniform('reg_lambda', 1e-8, 100.0),\n        'num_leaves' : trial.suggest_int('num_leaves', 25, 250),\n        'reg_alpha' : trial.suggest_loguniform('reg_alpha', 1e-8, 100.0),\n        'subsample' : trial.suggest_float('subsample', 0.1, 1.0),\n        'colsample_bytree' : trial.suggest_float('colsample_bytree', 0.05, 1.0),\n        'min_child_samples' : trial.suggest_int('min_child_samples', 10, 250),\n        'cat_smooth' : trial.suggest_float('cat_smooth', 10, 150),\n        'max_bin' : trial.suggest_int('max_bin', 50, 750),\n        'min_data_per_group' : trial.suggest_int('min_data_per_group', 10, 250),\n        'cat_l2' : trial.suggest_float('cat_l2', 1e-2, 10),\n        'bagging_freq' : trial.suggest_int('bagging_freq', 1, 10),\n        'bagging_fraction' : trial.suggest_float('bagging_fraction', 0.1, 1),\n        'max_depth' : trial.suggest_int('max_depth', 1, 100)\n    }\n\n    xtrain = train[train.StratifiedKFold != fold].reset_index(drop=True)\n    xvalid = train[train.StratifiedKFold == fold].reset_index(drop=True)\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n\n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n\n    #Scaling\n    scaler = preprocessing.StandardScaler()\n    xtrain[numerical_features] = scaler.fit_transform(xtrain[numerical_features])\n    xvalid[numerical_features] = scaler.transform(xvalid[numerical_features])\n    \n    # Ordinal encoding\n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[oec] = ordinal_encoder.fit_transform(xtrain[oec])\n    xvalid[oec] = ordinal_encoder.transform(xvalid[oec])\n\n    # One-hot encoding\n    ohe = preprocessing.OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n    xtrain_ohe = ohe.fit_transform(xtrain[ohec])\n    xvalid_ohe = ohe.transform(xvalid[ohec])\n    \n    xtrain_ohe = pd.DataFrame(xtrain_ohe, columns=[f\"OHE_{i}\" for i in range(xtrain_ohe.shape[1])])\n    xvalid_ohe = pd.DataFrame(xvalid_ohe, columns=[f\"OHE_{i}\" for i in range(xvalid_ohe.shape[1])])\n    \n    xtrain = pd.concat([xtrain, xtrain_ohe], axis=1)\n    xvalid = pd.concat([xvalid, xvalid_ohe], axis=1)\n    \n    xtrain = xtrain.drop(ohec, axis=1)\n    xvalid = xvalid.drop(ohec, axis=1)\n\n    model = LGBMRegressor(**params)\n\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict(xvalid)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    return rmse","eba205c4":"study = optuna.create_study(direction=\"minimize\")\nstudy.optimize(opt_lgbm, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial of LGBM:', study.best_trial.params)","caa75b4e":"study.trials_dataframe()","64eb007c":"lgbm_params = study.best_params\nlgbm_params['num_iteration'] = 10000\nlgbm_params['n_jobs'] = -1\nlgbm_params['early_stopping_round'] = 200","8670ca51":"## LightGBM Regressor Model\n\n\ntrain = pd.read_csv(\"train_folds(10).csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in train.columns if c not in (\"id\", \"target\", \"StratifiedKFold\")]\nnumerical_features = [c for c in train.columns if c not in (\"id\", \"target\", \"StratifiedKFold\") and not 'cat' in c]\nobject_cols = [col for col in useful_features if 'cat' in col]\ntest = test[useful_features]\n\npoly = preprocessing.PolynomialFeatures(degree = 2, interaction_only = True, include_bias = False)\ntrain_poly = poly.fit_transform(train[numerical_features])\ntest_poly = poly.fit_transform(test[numerical_features])\n\ndf_train_poly = pd.DataFrame(train_poly, columns=[f\"POLY_{i}\" for i in range(train_poly.shape[1])])\ndf_test_poly = pd.DataFrame(test_poly, columns=[f\"POLY_{i}\" for i in range(test_poly.shape[1])])\n\ntrain = pd.concat([train, df_train_poly], axis=1)\ntest = pd.concat([test, df_test_poly], axis=1)\n\nfor col in object_cols:\n    temp_df = []\n    temp_test_feat = None\n    for fold in range(10):\n        xtrain =  train[train.StratifiedKFold != fold].reset_index(drop=True)\n        xvalid = train[train.StratifiedKFold == fold].reset_index(drop=True)\n        feat = xtrain.groupby(col)[\"target\"].agg(\"mean\")\n        feat = feat.to_dict()\n        xvalid.loc[:, f\"tar_enc_{col}\"] = xvalid[col].map(feat)\n        temp_df.append(xvalid)\n        if temp_test_feat is None:\n            temp_test_feat = test[col].map(feat)\n        else:\n            temp_test_feat += test[col].map(feat)\n    \n    temp_test_feat \/= 10\n    test.loc[:, f\"tar_enc_{col}\"] = temp_test_feat\n    train = pd.concat(temp_df)\n\nuseful_features = [c for c in train.columns if c not in (\"id\", \"target\", \"StratifiedKFold\")]\nnumerical_features = [c for c in train.columns if c not in (\"id\", \"target\", \"StratifiedKFold\") and not c.startswith(\"cat\")]\nobject_cols = [col for col in useful_features if col.startswith(\"cat\")]\ntest = test[useful_features]\n\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\n\nfor fold in range(len(train['StratifiedKFold'].unique().tolist())):\n    xtrain =  train[train.StratifiedKFold != fold].reset_index(drop=True)\n    xvalid = train[train.StratifiedKFold == fold].reset_index(drop=True)\n    xtest = test.copy()\n\n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n\n    #Scaling\n    scaler = preprocessing.StandardScaler()\n    xtrain[numerical_features] = scaler.fit_transform(xtrain[numerical_features])\n    xvalid[numerical_features] = scaler.transform(xvalid[numerical_features])\n    xtest[numerical_features] = scaler.transform(xtest[numerical_features])\n    \n    # Ordinal encoding\n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[oec] = ordinal_encoder.fit_transform(xtrain[oec])\n    xvalid[oec] = ordinal_encoder.transform(xvalid[oec])\n    xtest[oec] = ordinal_encoder.transform(xtest[oec])\n\n    ohe = preprocessing.OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n    xtrain_ohe = ohe.fit_transform(xtrain[ohec])\n    xvalid_ohe = ohe.transform(xvalid[ohec])\n    xtest_ohe = ohe.transform(xtest[ohec])\n    \n    xtrain_ohe = pd.DataFrame(xtrain_ohe, columns=[f\"ohe_{i}\" for i in range(xtrain_ohe.shape[1])])\n    xvalid_ohe = pd.DataFrame(xvalid_ohe, columns=[f\"ohe_{i}\" for i in range(xvalid_ohe.shape[1])])\n    xtest_ohe = pd.DataFrame(xtest_ohe, columns=[f\"ohe_{i}\" for i in range(xtest_ohe.shape[1])])\n    \n    xtrain = pd.concat([xtrain, xtrain_ohe], axis=1)\n    xvalid = pd.concat([xvalid, xvalid_ohe], axis=1)\n    xtest = pd.concat([xtest, xtest_ohe], axis=1)\n    \n    xtrain = xtrain.drop(ohec, axis=1)\n    xvalid = xvalid.drop(ohec, axis=1)\n    xtest = xtest.drop(ohec, axis=1)\n\n    #lgb_train = LGBMRegressor.Dataset(xtrain, ytrain)\n    #lgb_valid = LGBMRegressor.Dataset(xvalid, yvalid)\n    model = LGBMRegressor(**lgbm_params)\n    model.fit(xtrain, ytrain, early_stopping_rounds = 200, eval_set=[(xvalid, yvalid)])\n    #print(name + ' was trained for fold ' + str(fold) + '.')\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\n\n\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_1\"]\nfinal_valid_predictions.to_csv(\"train_pred_1.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_1\"]\nsample_submission.to_csv(\"test_pred_1.csv\", index=False)","484006dd":"## XGB Regressor parameter tuning\n\ntrain = pd.read_csv(\"train_folds(10).csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in train.columns if c not in (\"id\", \"target\", \"StratifiedKFold\")]\nnumerical_features = [c for c in train.columns if c not in (\"id\", \"target\", \"StratifiedKFold\") and not 'cat' in c]\nobject_cols = [col for col in useful_features if 'cat' in col]\ntest = test[useful_features]\n\npoly = preprocessing.PolynomialFeatures(degree = 2, interaction_only = True, include_bias = False)\ntrain_poly = poly.fit_transform(train[numerical_features])\ntest_poly = poly.fit_transform(test[numerical_features])\n\ndf_train_poly = pd.DataFrame(train_poly, columns=[f\"POLY_{i}\" for i in range(train_poly.shape[1])])\ndf_test_poly = pd.DataFrame(test_poly, columns=[f\"POLY_{i}\" for i in range(test_poly.shape[1])])\n\ntrain = pd.concat([train, df_train_poly], axis=1)\ntest = pd.concat([test, df_test_poly], axis=1)\n\nfor col in object_cols:\n    temp_df = []\n    temp_test_feat = None\n    for fold in range(10):\n        xtrain =  train[train.StratifiedKFold != fold].reset_index(drop=True)\n        xvalid = train[train.StratifiedKFold == fold].reset_index(drop=True)\n        feat = xtrain.groupby(col)[\"target\"].agg(\"mean\")\n        feat = feat.to_dict()\n        xvalid.loc[:, f\"tar_enc_{col}\"] = xvalid[col].map(feat)\n        temp_df.append(xvalid)\n        if temp_test_feat is None:\n            temp_test_feat = test[col].map(feat)\n        else:\n            temp_test_feat += test[col].map(feat)\n    \n    temp_test_feat \/= 10\n    test.loc[:, f\"tar_enc_{col}\"] = temp_test_feat\n    train = pd.concat(temp_df)\n\nuseful_features = [c for c in train.columns if c not in (\"id\", \"target\", \"StratifiedKFold\")]\nnumerical_features = [c for c in train.columns if c not in (\"id\", \"target\", \"StratifiedKFold\") and not c.startswith(\"cat\")]\nobject_cols = [col for col in useful_features if col.startswith(\"cat\")]\ntest = test[useful_features]","d184f5ee":"def opt_xgb(trial):\n    fold = 6\n\n    params = {\n        'objective': \"reg:squarederror\",\n        'random_state': 606,\n        #'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        #'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'max_depth': trial.suggest_int('max_depth', 1, 15),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-2, 0.30, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 100.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 100.0),\n        #'gamma': trial.suggest_float('gamma', 0, 10),\n        'subsample': trial.suggest_float(\"subsample\", 1e-1, 1.0),\n        #'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n        'colsample_bytree' : trial.suggest_float(\"colsample_bytree\", 1e-1, 1.0),\n        'tree_method' : \"gpu_hist\", \n        'gpu_id' : 1,\n        'predictor' : \"gpu_predictor\",\n        'n_estimators' : 7500,\n        \n    }\n\n    xtrain = train[train.StratifiedKFold != fold].reset_index(drop=True)\n    xvalid = train[train.StratifiedKFold == fold].reset_index(drop=True)\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n\n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n\n    #Scaling\n    scaler = preprocessing.StandardScaler()\n    xtrain[numerical_features] = scaler.fit_transform(xtrain[numerical_features])\n    xvalid[numerical_features] = scaler.transform(xvalid[numerical_features])\n    \n    # Ordinal encoding\n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[oec] = ordinal_encoder.fit_transform(xtrain[oec])\n    xvalid[oec] = ordinal_encoder.transform(xvalid[oec])\n\n    # One-hot encoding\n    ohe = preprocessing.OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n    xtrain_ohe = ohe.fit_transform(xtrain[ohec])\n    xvalid_ohe = ohe.transform(xvalid[ohec])\n    \n    xtrain_ohe = pd.DataFrame(xtrain_ohe, columns=[f\"OHE_{i}\" for i in range(xtrain_ohe.shape[1])])\n    xvalid_ohe = pd.DataFrame(xvalid_ohe, columns=[f\"OHE_{i}\" for i in range(xvalid_ohe.shape[1])])\n    \n    xtrain = pd.concat([xtrain, xtrain_ohe], axis=1)\n    xvalid = pd.concat([xvalid, xvalid_ohe], axis=1)\n    \n    xtrain = xtrain.drop(ohec, axis=1)\n    xvalid = xvalid.drop(ohec, axis=1)\n\n    model = XGBRegressor(**params)\n    model.fit(xtrain, ytrain, early_stopping_rounds = 200, eval_set=[(xvalid, yvalid)], verbose = 1000)\n    preds_valid = model.predict(xvalid)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    return rmse","02b363da":"study = optuna.create_study(direction=\"minimize\")\nstudy.optimize(opt_xgb, n_trials=10)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial of XGB Regressor:', study.best_trial.params)","efefdf70":"study.trials_dataframe()","22a5868f":"xgb_params = study.best_params\nxgb_params['tree_method'] = \"gpu_hist\"\nxgb_params['gpu_id'] = 1\nxgb_params['predictor'] = \"gpu_predictor\"\nxgb_params['n_estimators'] = 10000","f511ccfb":"## XGB Regressor Model\n\n\ntrain = pd.read_csv(\"train_folds(10).csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in train.columns if c not in (\"id\", \"target\", \"StratifiedKFold\")]\nnumerical_features = [c for c in train.columns if c not in (\"id\", \"target\", \"StratifiedKFold\") and not 'cat' in c]\nobject_cols = [col for col in useful_features if 'cat' in col]\ntest = test[useful_features]\n\npoly = preprocessing.PolynomialFeatures(degree = 2, interaction_only = True, include_bias = False)\ntrain_poly = poly.fit_transform(train[numerical_features])\ntest_poly = poly.fit_transform(test[numerical_features])\n\ndf_train_poly = pd.DataFrame(train_poly, columns=[f\"POLY_{i}\" for i in range(train_poly.shape[1])])\ndf_test_poly = pd.DataFrame(test_poly, columns=[f\"POLY_{i}\" for i in range(test_poly.shape[1])])\n\ntrain = pd.concat([train, df_train_poly], axis=1)\ntest = pd.concat([test, df_test_poly], axis=1)\n\nfor col in object_cols:\n    temp_df = []\n    temp_test_feat = None\n    for fold in range(10):\n        xtrain =  train[train.StratifiedKFold != fold].reset_index(drop=True)\n        xvalid = train[train.StratifiedKFold == fold].reset_index(drop=True)\n        feat = xtrain.groupby(col)[\"target\"].agg(\"mean\")\n        feat = feat.to_dict()\n        xvalid.loc[:, f\"tar_enc_{col}\"] = xvalid[col].map(feat)\n        temp_df.append(xvalid)\n        if temp_test_feat is None:\n            temp_test_feat = test[col].map(feat)\n        else:\n            temp_test_feat += test[col].map(feat)\n    \n    temp_test_feat \/= 10\n    test.loc[:, f\"tar_enc_{col}\"] = temp_test_feat\n    train = pd.concat(temp_df)\n\nuseful_features = [c for c in train.columns if c not in (\"id\", \"target\", \"StratifiedKFold\")]\nnumerical_features = [c for c in train.columns if c not in (\"id\", \"target\", \"StratifiedKFold\") and not c.startswith(\"cat\")]\nobject_cols = [col for col in useful_features if col.startswith(\"cat\")]\ntest = test[useful_features]\n\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\n\nfor fold in range(len(train['StratifiedKFold'].unique().tolist())):\n    xtrain =  train[train.StratifiedKFold != fold].reset_index(drop=True)\n    xvalid = train[train.StratifiedKFold == fold].reset_index(drop=True)\n    xtest = test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n\n    #Scaling\n    scaler = preprocessing.StandardScaler()\n    xtrain[numerical_features] = scaler.fit_transform(xtrain[numerical_features])\n    xvalid[numerical_features] = scaler.transform(xvalid[numerical_features])\n    xtest[numerical_features] = scaler.transform(xtest[numerical_features])\n    \n    # Ordinal encoding\n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[oec] = ordinal_encoder.fit_transform(xtrain[oec])\n    xvalid[oec] = ordinal_encoder.transform(xvalid[oec])\n    xtest[oec] = ordinal_encoder.transform(xtest[oec])\n\n    ohe = preprocessing.OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n    xtrain_ohe = ohe.fit_transform(xtrain[ohec])\n    xvalid_ohe = ohe.transform(xvalid[ohec])\n    xtest_ohe = ohe.transform(xtest[ohec])\n    \n    xtrain_ohe = pd.DataFrame(xtrain_ohe, columns=[f\"ohe_{i}\" for i in range(xtrain_ohe.shape[1])])\n    xvalid_ohe = pd.DataFrame(xvalid_ohe, columns=[f\"ohe_{i}\" for i in range(xvalid_ohe.shape[1])])\n    xtest_ohe = pd.DataFrame(xtest_ohe, columns=[f\"ohe_{i}\" for i in range(xtest_ohe.shape[1])])\n    \n    xtrain = pd.concat([xtrain, xtrain_ohe], axis=1)\n    xvalid = pd.concat([xvalid, xvalid_ohe], axis=1)\n    xtest = pd.concat([xtest, xtest_ohe], axis=1)\n    \n    xtrain = xtrain.drop(ohec, axis=1)\n    xvalid = xvalid.drop(ohec, axis=1)\n    xtest = xtest.drop(ohec, axis=1)\n\n    model = XGBRegressor(**xgb_params)\n    model.fit(xtrain, ytrain, early_stopping_rounds = 200, eval_set=[(xvalid, yvalid)], verbose = 100)\n    #print(name + ' was trained for fold ' + str(fold) + '.')\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\n\n\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_2\"]\nfinal_valid_predictions.to_csv(\"train_pred_2.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_2\"]\nsample_submission.to_csv(\"test_pred_2.csv\", index=False)","f3504dc0":"## CatBoost Regressor parameter tuning\n\ntrain = pd.read_csv(\"train_folds(10).csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in train.columns if c not in (\"id\", \"target\", \"StratifiedKFold\")]\nnumerical_features = [c for c in train.columns if c not in (\"id\", \"target\", \"StratifiedKFold\") and not 'cat' in c]\nobject_cols = [col for col in useful_features if 'cat' in col]\ntest = test[useful_features]\n\npoly = preprocessing.PolynomialFeatures(degree = 2, interaction_only = True, include_bias = False)\ntrain_poly = poly.fit_transform(train[numerical_features])\ntest_poly = poly.fit_transform(test[numerical_features])\n\ndf_train_poly = pd.DataFrame(train_poly, columns=[f\"POLY_{i}\" for i in range(train_poly.shape[1])])\ndf_test_poly = pd.DataFrame(test_poly, columns=[f\"POLY_{i}\" for i in range(test_poly.shape[1])])\n\ntrain = pd.concat([train, df_train_poly], axis=1)\ntest = pd.concat([test, df_test_poly], axis=1)\n\nfor col in object_cols:\n    temp_df = []\n    temp_test_feat = None\n    for fold in range(10):\n        xtrain =  train[train.StratifiedKFold != fold].reset_index(drop=True)\n        xvalid = train[train.StratifiedKFold == fold].reset_index(drop=True)\n        feat = xtrain.groupby(col)[\"target\"].agg(\"mean\")\n        feat = feat.to_dict()\n        xvalid.loc[:, f\"tar_enc_{col}\"] = xvalid[col].map(feat)\n        temp_df.append(xvalid)\n        if temp_test_feat is None:\n            temp_test_feat = test[col].map(feat)\n        else:\n            temp_test_feat += test[col].map(feat)\n    \n    temp_test_feat \/= 10\n    test.loc[:, f\"tar_enc_{col}\"] = temp_test_feat\n    train = pd.concat(temp_df)\n\nuseful_features = [c for c in train.columns if c not in (\"id\", \"target\", \"StratifiedKFold\")]\nnumerical_features = [c for c in train.columns if c not in (\"id\", \"target\", \"StratifiedKFold\") and not c.startswith(\"cat\")]\nobject_cols = [col for col in useful_features if col.startswith(\"cat\")]\ntest = test[useful_features]","080fa3ce":"def opt_ctb(trial):\n    fold = 8\n\n    params = {\n        'eval_metric': 'RMSE',\n        'objective': trial.suggest_categorical('objective', ['RMSE']),\n        'colsample_bylevel': trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n        'depth': trial.suggest_int(\"depth\", 1, 12),\n        'n_estimators' : 5000,\n        'learning_rate': trial.suggest_float('learning_rate', 1e-2, 0.30, log=True),\n        'max_bin' : trial.suggest_int(\"max_bin\", 50, 750),\n        'min_data_in_leaf' : trial.suggest_int(\"min_data_in_leaf\", 2, 50),\n        #'random_strength': trial.suggest_float('random_strength', 2, 15),\n        'boosting_type': trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        'bootstrap_type': trial.suggest_categorical(\n            'bootstrap_type', [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n        ),\n            }\n\n    if params[\"bootstrap_type\"] == \"Bayesian\":\n        params[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif params[\"bootstrap_type\"] == \"Bernoulli\":\n        params[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n\n    xtrain = train[train.StratifiedKFold != fold].reset_index(drop=True)\n    xvalid = train[train.StratifiedKFold == fold].reset_index(drop=True)\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n\n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n\n    #Scaling\n    scaler = preprocessing.StandardScaler()\n    xtrain[numerical_features] = scaler.fit_transform(xtrain[numerical_features])\n    xvalid[numerical_features] = scaler.transform(xvalid[numerical_features])\n    \n    # Ordinal encoding\n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[oec] = ordinal_encoder.fit_transform(xtrain[oec])\n    xvalid[oec] = ordinal_encoder.transform(xvalid[oec])\n\n    # One-hot encoding\n    ohe = preprocessing.OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n    xtrain_ohe = ohe.fit_transform(xtrain[ohec])\n    xvalid_ohe = ohe.transform(xvalid[ohec])\n    \n    xtrain_ohe = pd.DataFrame(xtrain_ohe, columns=[f\"OHE_{i}\" for i in range(xtrain_ohe.shape[1])])\n    xvalid_ohe = pd.DataFrame(xvalid_ohe, columns=[f\"OHE_{i}\" for i in range(xvalid_ohe.shape[1])])\n    \n    xtrain = pd.concat([xtrain, xtrain_ohe], axis=1)\n    xvalid = pd.concat([xvalid, xvalid_ohe], axis=1)\n    \n    xtrain = xtrain.drop(ohec, axis=1)\n    xvalid = xvalid.drop(ohec, axis=1)\n\n    model = CatBoostRegressor(**params)\n\n    model.fit(xtrain, ytrain, early_stopping_rounds = 200, eval_set=[(xvalid, yvalid)], verbose = 1000)\n    preds_valid = model.predict(xvalid)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    return rmse","fcb88a51":"study = optuna.create_study(direction=\"minimize\")\nstudy.optimize(opt_ctb, n_trials=10)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial of CatBoost Regressor:', study.best_trial.params)","d1a57fbb":"study.trials_dataframe()","367906ce":"ctb_params = study.best_params\nctb_params['n_estimators'] = 7500","82be19dc":"## CatBoost Regressor Model\n\n\ntrain = pd.read_csv(\"train_folds(10).csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in train.columns if c not in (\"id\", \"target\", \"StratifiedKFold\")]\nnumerical_features = [c for c in train.columns if c not in (\"id\", \"target\", \"StratifiedKFold\") and not 'cat' in c]\nobject_cols = [col for col in useful_features if 'cat' in col]\ntest = test[useful_features]\n\npoly = preprocessing.PolynomialFeatures(degree = 2, interaction_only = True, include_bias = False)\ntrain_poly = poly.fit_transform(train[numerical_features])\ntest_poly = poly.fit_transform(test[numerical_features])\n\ndf_train_poly = pd.DataFrame(train_poly, columns=[f\"POLY_{i}\" for i in range(train_poly.shape[1])])\ndf_test_poly = pd.DataFrame(test_poly, columns=[f\"POLY_{i}\" for i in range(test_poly.shape[1])])\n\ntrain = pd.concat([train, df_train_poly], axis=1)\ntest = pd.concat([test, df_test_poly], axis=1)\n\nfor col in object_cols:\n    temp_df = []\n    temp_test_feat = None\n    for fold in range(10):\n        xtrain =  train[train.StratifiedKFold != fold].reset_index(drop=True)\n        xvalid = train[train.StratifiedKFold == fold].reset_index(drop=True)\n        feat = xtrain.groupby(col)[\"target\"].agg(\"mean\")\n        feat = feat.to_dict()\n        xvalid.loc[:, f\"tar_enc_{col}\"] = xvalid[col].map(feat)\n        temp_df.append(xvalid)\n        if temp_test_feat is None:\n            temp_test_feat = test[col].map(feat)\n        else:\n            temp_test_feat += test[col].map(feat)\n    \n    temp_test_feat \/= 10\n    test.loc[:, f\"tar_enc_{col}\"] = temp_test_feat\n    train = pd.concat(temp_df)\n\nuseful_features = [c for c in train.columns if c not in (\"id\", \"target\", \"StratifiedKFold\")]\nnumerical_features = [c for c in train.columns if c not in (\"id\", \"target\", \"StratifiedKFold\") and not c.startswith(\"cat\")]\nobject_cols = [col for col in useful_features if col.startswith(\"cat\")]\ntest = test[useful_features]\n\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\n\nfor fold in range(len(train['StratifiedKFold'].unique().tolist())):\n    xtrain =  train[train.StratifiedKFold != fold].reset_index(drop=True)\n    xvalid = train[train.StratifiedKFold == fold].reset_index(drop=True)\n    xtest = test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n\n    #Scaling\n    scaler = preprocessing.StandardScaler()\n    xtrain[numerical_features] = scaler.fit_transform(xtrain[numerical_features])\n    xvalid[numerical_features] = scaler.transform(xvalid[numerical_features])\n    xtest[numerical_features] = scaler.transform(xtest[numerical_features])\n    \n    # Ordinal encoding\n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[oec] = ordinal_encoder.fit_transform(xtrain[oec])\n    xvalid[oec] = ordinal_encoder.transform(xvalid[oec])\n    xtest[oec] = ordinal_encoder.transform(xtest[oec])\n\n    ohe = preprocessing.OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n    xtrain_ohe = ohe.fit_transform(xtrain[ohec])\n    xvalid_ohe = ohe.transform(xvalid[ohec])\n    xtest_ohe = ohe.transform(xtest[ohec])\n    \n    xtrain_ohe = pd.DataFrame(xtrain_ohe, columns=[f\"ohe_{i}\" for i in range(xtrain_ohe.shape[1])])\n    xvalid_ohe = pd.DataFrame(xvalid_ohe, columns=[f\"ohe_{i}\" for i in range(xvalid_ohe.shape[1])])\n    xtest_ohe = pd.DataFrame(xtest_ohe, columns=[f\"ohe_{i}\" for i in range(xtest_ohe.shape[1])])\n    \n    xtrain = pd.concat([xtrain, xtrain_ohe], axis=1)\n    xvalid = pd.concat([xvalid, xvalid_ohe], axis=1)\n    xtest = pd.concat([xtest, xtest_ohe], axis=1)\n    \n    xtrain = xtrain.drop(ohec, axis=1)\n    xvalid = xvalid.drop(ohec, axis=1)\n    xtest = xtest.drop(ohec, axis=1)\n\n    model = CatBoostRegressor(**ctb_params)\n    model.fit(xtrain, ytrain, early_stopping_rounds = 200, eval_set=[(xvalid, yvalid)], verbose = 100)\n    model.fit(xtrain, ytrain)\n    #print(name + ' was trained for fold ' + str(fold) + '.')\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\n\n\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_3\"]\nfinal_valid_predictions.to_csv(\"train_pred_3.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_3\"]\nsample_submission.to_csv(\"test_pred_3.csv\", index=False)","16f555d2":"train = pd.read_csv(\"train_folds(10).csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\ntrain1 = pd.read_csv(\"train_pred_1.csv\")\ntrain2 = pd.read_csv(\"train_pred_2.csv\")\ntrain3 = pd.read_csv(\"train_pred_3.csv\")\n\ntest1 = pd.read_csv(\"test_pred_1.csv\")\ntest2 = pd.read_csv(\"test_pred_2.csv\")\ntest3 = pd.read_csv(\"test_pred_3.csv\")\n\ntrain = train.merge(train1, on=\"id\", how=\"left\")\ntrain = train.merge(train2, on=\"id\", how=\"left\")\ntrain = train.merge(train3, on=\"id\", how=\"left\")\n\ntest = test.merge(test1, on=\"id\", how=\"left\")\ntest = test.merge(test2, on=\"id\", how=\"left\")\ntest = test.merge(test3, on=\"id\", how=\"left\")","10cf7e17":"useful_features = [\"pred_1\", \"pred_2\", \"pred_3\"]\ntest = test[useful_features]\n\npoly = preprocessing.PolynomialFeatures(degree = 2, interaction_only = False, include_bias = False)\ntrain_poly = poly.fit_transform(train[useful_features])\ntest_poly = poly.fit_transform(test[useful_features])\n\ndf_train_poly = pd.DataFrame(train_poly, columns=[f\"POLY_{i}\" for i in range(train_poly.shape[1])])\ndf_test_poly = pd.DataFrame(test_poly, columns=[f\"POLY_{i}\" for i in range(test_poly.shape[1])])\n\ntrain = pd.concat([train, df_train_poly], axis=1)\ntest = pd.concat([test, df_test_poly], axis=1)\n\nuseful_features = [col for col in test.columns]\n\nfinal_predictions = []\nscores = []\n\nfor fold in range(10):\n    xtrain =  train[train.StratifiedKFold != fold].reset_index(drop=True)\n    xvalid = train[train.StratifiedKFold == fold].reset_index(drop=True)\n    xtest = test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    model = LGBMRegressor(**lgbm_params)\n    model.fit(xtrain, ytrain, early_stopping_rounds = 200, eval_set=[(xvalid, yvalid)])\n    \n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))","7b25eaec":"sample_submission.target = np.mean(np.column_stack(final_predictions), axis=1)\nsample_submission.to_csv(\"submission.csv\", index=False)","a156a13c":"<a id=\"lgbm_model\"><\/a>\n## <p style=\"background-color:MediumPurple; font-family:newtimeroman; font-size:120%; text-align:center\">3.1. LGBM Model<\/p>","586a7af7":"We will begin by using all three trained model to blend predictions, which we will save to a CSV file.","cd26eea4":"<a id=\"norm_check_outlier_detect\"><\/a>\n## <p style=\"background-color:MediumPurple; font-family:newtimeroman; font-size:120%; text-align:center\">2.3. Normality Check and Outlier Detection<\/p>","cb3b9303":"There is no significant correlation between variables in train dataset.","4c505d99":"<a id=\"catboost_model\"><\/a>\n## <p style=\"background-color:MediumPurple; font-family:newtimeroman; font-size:120%; text-align:center\">3.3. CatBoost Model<\/p>","a1130185":"[back to top](#table-of-contents)\n<a id=\"preperation\"><\/a>\n# <p style=\"background-color:BlueViolet; font-family:newtimeroman; font-size:150%; text-align:center\">1. Preperation<\/p>\n\n\n<a id=\"load_packages_import_libraries\"><\/a>\n## <p style=\"background-color:MediumPurple; font-family:newtimeroman; font-size:120%; text-align:center\">1.1. Loading Packages and Importing Libraries<\/p>\n\nLoading packages and importing some helpful libraries.","047f5665":"<a id=\"numerical_variables\"><\/a>\n## <p style=\"background-color:MediumPurple; font-family:newtimeroman; font-size:120%; text-align:center\">2.2. Numerical Variables<\/p>","21cf8e3e":"There is also no significant correlation between variables in test dataset.","8bfe9293":"## <p style=\"background-color:BlueViolet; font-family:newtimeroman; margin-bottom:2px; font-size:32px; color: white; text-align:center\">Table of Content<\/p>  \n\n<a id=\"table-of-contents\"><\/a>\n1. [Preperation](#preperation)\n    * 1.1. [Loading Packages and Importing Libraries](#load_packages_import_libraries)\n    * 1.2. [Data Description](#data_description)\n2. [Exploratory Data Analysis (EDA)](#eda)\n    * 2.1. [Categorical Variables](#categorical_variables)\n    * 2.2. [Numerical Variables](#numerical_variables)\n    * 2.3. [Normality Check and Outlier Detection](#norm_check_outlier_detect)\n3. [Feature Engineering and Modeling](#feat_eng_model)\n    * 3.1. [LightGBM Model](#lgbm_model)\n    * 3.2. [XGB Model](#xgb_model)\n    * 3.3. [CatBoost Model](#catboost_model)\n4. [Model Blending](#model_blending)","197ac3d2":"<a id=\"xgb_model\"><\/a>\n## <p style=\"background-color:MediumPurple; font-family:newtimeroman; font-size:120%; text-align:center\">3.2. XGB Model<\/p>","e606a7bc":"[back to top](#table-of-contents)\n<a id=\"eda\"><\/a>\n# <p style=\"background-color:BlueViolet; font-family:newtimeroman; font-size:150%; text-align:center\">2. Exploratory Data Analysis (EDA)<\/p>\n\nBoth categorical and numerical variables will be explored in this section.","ddb5d1c1":"After some preprocessing, we can tune CatBoost parameters and train model. ","9fa495b9":"<a id=\"data_description\"><\/a>\n## <p style=\"background-color:MediumPurple; font-family:newtimeroman; font-size:120%; text-align:center\">1.2. Data Description<\/p>\n\nNext, I will load the train, test and sample_submission datasets and display train and test datasets.","aaa77c20":"According to D'Agostino and Pearson's Test, numerical variables both in train and test datasets are not distributed normally. Therefore, we should not detect outliers using Z-score.","f348cec2":"#  30 Days ML Challenge | Competition\n\n![](https:\/\/github.com\/MhmdSyd\/needed_image\/blob\/main\/30_Days_ML.png?raw=true)","02cceee3":"Creating 10-fold using Stratified KFold method.","45459882":"After some preprocessing, we can tune LGBM parameters and train model.","a3433478":"[back to top](#table-of-contents)\n<a id=\"feat_eng_model\"><\/a>\n# <p style=\"background-color:BlueViolet; font-family:newtimeroman; font-size:150%; text-align:center\">3. Feature Engineering and Modeling<\/p>","962a1ec8":"<a id=\"categorical_variables\"><\/a>\n## <p style=\"background-color:MediumPurple; font-family:newtimeroman; font-size:120%; text-align:center\">2.1. Categorical Variables<\/p>","ab35ff8d":"[back to top](#table-of-contents)\n<a id=\"model_blending\"><\/a>\n# <p style=\"background-color:BlueViolet; font-family:newtimeroman; font-size:150%; text-align:center\">4. Model Blending<\/p>","4dc4f43e":"After some preprocessing, we can tune XGB parameters and train model."}}