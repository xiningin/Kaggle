{"cell_type":{"6b6337f0":"code","4823c23e":"code","7b091377":"code","44ef4195":"code","a251a2ec":"code","cd0f17b7":"code","a4f0c836":"code","a99946b6":"code","32d99072":"code","83fd8f5d":"code","8772dc6e":"code","899f7ecb":"code","a5276973":"code","3fa29ff3":"code","e0845b20":"code","050e4e16":"code","220b94e8":"code","929a89b3":"code","2fff5b44":"code","c70f7696":"code","a9c7b29f":"code","5907e233":"code","d5d14480":"code","87e9cbaf":"code","1141ae05":"code","e290b142":"code","391ac26e":"code","7d2bb20c":"code","eb0ed087":"code","79b56a14":"code","3481dc21":"code","32f67904":"code","b87e3b34":"code","d383f497":"code","cd4eed77":"code","bbea73a3":"code","a878b485":"code","0f32ebca":"code","8f142b82":"code","c0438379":"code","fcb338ed":"code","9a1523b5":"code","c4619509":"code","78ce082b":"code","b5d9da61":"code","666f1318":"code","ca5729e7":"code","e0fb6183":"code","85a68b58":"code","961c2763":"code","a7b9d65f":"code","e238379b":"markdown"},"source":{"6b6337f0":"# after a complete run program  has to get stopped -- to run -- more higher performance metrics -- to solve the problems -->\n\n#setting\n# This Python 3 environment\n#comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n# there are lots of deliverable projects -- \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname,\n                           filename))\n# local files that could be good for the  prediction sequences --> \n# if  the training  losses and  validation losses  are particularly high when running t\n# Any results you write to the current directory are saved as output.\n# load with the several data flow algorithms --> with all the loadable values |-->  \n# all_data ","4823c23e":"general_matic = pd.read_csv(\"..\/cryptocurrency\/data_matic_hourly (1).csv\")\ngeneral_matic.shape","7b091377":"general_matic.head()\n\"\"\"\nkeys attribute --> \nhigh + low + volumefrom-volumeto\n# remember that volume from is always more than volume 2 there are also several data points -- that was getting collected while system wasn't live --> when there no active trading ==> involved -->\n\n\n\"\"\"\ngeneral_matic[[\"volumefrom\", \"volumeto\"]]# there are  several tasks that can build the rest values --> \nmax(general_matic[\"volumefrom\"] - general_matic[\"volumeto\"])# the volume to is always less than --> porfolio management  --> #","44ef4195":"#plt.plot(general_matic[\"volumefrom\"]- general_matic[\"volumeto\"]) # difference scaling --> could be important --> at that particular time\n","a251a2ec":"general_matic[\"high\"]# price low high scaling -->  low prediction will low high + difference in the system high will also predict ","cd0f17b7":"# several task that can change all of above --> with particular load -- that can change or can't get changed --> \nprice_scaler = sklearn.S\ndef build_features(column, timestep):\n    \n    current = []# target preparation has to get done --> this is data that has pretty significant number of values --> that has been build to enhance the following tasks.\n    for i in range(len(column)-timestep):\n        current.append(column[i:i+timestep])\n    current = np.array(current)\n    return current\nvolume_scaler = \ndef volume_added(current_df):\n    \"\"\"\n    current_df has the two dataframe --> \n    \n    \"\"\"\n    return current_df[\"volumefrom\"] - current_df[\"volumeto\"]\n    \nvolume_diff = \n    \n# values needs to be build to run higher performing algorithms --> \n","a4f0c836":"\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport csv\nimport requests\nimport json\nimport datetime\nfrom datetime import datetime\nfrom tensorflow import keras # some  required vision for the running the algorithms -- \n# linux time stamp is applied in this  time series \n# json and requests libraries needs to be imported before hand\n# cryptocompare credentials for  this part of the functionis very optional to run -- so needs to load all the rest of the cases .. it \n\n#url = \"https:\/\/min-api.cryptocompare.com\/data\/v2\/histominute?fsym=BTC&tsym=USD&limit=10\"\n#apikey = \"424e0c65bd9be940591fe30fae4a23fdfce9e192eaa09e991f65aad43285277b\"\n# run all the rest of the methods .. \n#params = {\"key\":apikey}\n#response = requests.get(url, params)\n# run all of those values that run in the sins\n# form of the potential energy based on the fragility and dependency on the electronic systems .. \n# two different scaler making sure to get loaded on the data frame --. \ndef get_historical_hourly():\n    \"\"\"\n    goal is that one side stationary to predict the price values for the following  time stamp with not high precision\n    \n    \"\"\"\n    \n    print(\"Which coins table data do you want to have?\")\n    #fsym = input()\n    fsym = \"MATIC\"# \n    print(\"How many minutes of data in from the past do you want to extract?\")\n    #limit = input() # limit is a number of minutes that program will extract data  -->\n    limit = \"2000\"\n    url = \"https:\/\/min-api.cryptocompare.com\/data\/v2\/histohour?fsym=\"+fsym+\"&tsym=USDT&limit=\"+limit\n    \n    apikey = \"424e0c65bd9be940591fe30fae4a23fdfce9e192eaa09e991f65aad43285277b\"# can deploy the latest version on the data --> \n    params = {\"key\":apikey}\n    response = requests.get(url, params)\n    temp = response.text\n    values = json.loads(temp)\n    # read --> the functional and \n    \n    return values[\"Data\"][\"Data\"]\ndef get_historical_daily():\n    # it is interesting to download by hourly or daily that can be newer version will be applied that can't be changed -->\n    # access whatever happens to the world that is mattering --> \n    \"\"\"\n    \n    goal is that one side stationary to predict the price values for the following  time stamp with not high precision\n    \n    \"\"\"\n    \n    print(\"Which coins table data do you want to have?\")\n    #fsym = input()\n    fsym = \"MATIC\"\n    print(\"How many minutes of data in from the past do you want to extract?\")\n    #limit = input() # limit is a number of minutes that program will extract data  -->\n    limit = \"2000\"\n    url = \"https:\/\/min-api.cryptocompare.com\/data\/v2\/histominute?fsym=\"+fsym+\"&tsym=USDT&limit=\"+limit\n    \n    apikey = \"424e0c65bd9be940591fe30fae4a23fdfce9e192eaa09e991f65aad43285277b\"\n    params = {\"key\":apikey}\n    response = requests.get(url, params)\n    temp = response.text\n    values = json.loads(temp)\n    return values[\"Data\"][\"Data\"]\n\ndef tabledatabuilder(data):\n    \"\"\"\n    this method will get the collecion of the dictionary  and return the tabular data on that -->\n\n    \"\"\"\n    columns = [\"time\", \"high\", \"low\", \"volumefrom\", \"volumeto\", \"close\"]\n    price_minutes = pd.DataFrame(columns = [\"time\", \"high\", \"low\", \"volumefrom\", \"volumeto\", \"close\"])\n    time =  []\n    high_prices = []\n    low_prices =  []\n    volumefrom = []\n    volumeto = []\n    close_prices = []\n    standard_time =  []\n\n    for eachminutedata in data:\n        time.append(eachminutedata[\"time\"])\n        high_prices.append(eachminutedata[\"high\"])\n        low_prices.append(eachminutedata[\"low\"])\n        volumefrom.append(eachminutedata[\"volumefrom\"])\n        volumeto.append(eachminutedata['volumeto'])\n        close_prices.append(eachminutedata['close'])\n        standard_time.append(datetime.fromtimestamp(eachminutedata[\"time\"]))\n    price_minutes['time'] = time\n    price_minutes[\"high\"] = high_prices\n    price_minutes['low'] = low_prices\n    price_minutes[\"volumefrom\"] = volumefrom\n    price_minutes['volumeto'] = volumeto\n    price_minutes[\"close\"] = close_prices\n    price_minutes[\"standardtime\"] = standard_time\n    return price_minutes\n\n\n\n\ndef get_historical():\n    \"\"\"\n    goal is that one side stationary to predict the price values for the following  time stamp with not high precision\n    \n    \"\"\"\n    \n    print(\"Which coins table data do you want to have?\")\n    #fsym = input()\n    fsym = \"MATIC\"\n    print(\"How many minutes of data in from the past do you want to extract?\")\n    #limit = input() # limit is a number of minutes that program will extract data  -->\n    limit = \"2000\"\n    url = \"https:\/\/min-api.cryptocompare.com\/data\/v2\/histohour?fsym=\"+fsym+\"&tsym=USDT&limit=\"+limit\n    \n    apikey = \"92c068168aa199f05c3d785ae1687f0e65346a2dbd21a5f9d18176148a0e7337\"\n    params = {\"key\":apikey}\n    response = requests.get(url, params)\n    temp = response.text\n    values = json.loads(temp)\n    return values[\"Data\"][\"Data\"]\n\ndata = get_historical()\nprint(data[-1]['time'])\ndata = tabledatabuilder(data)\nsorteddata = data.sort_values(\"time\", ascending = False)\n# can make the data that is called by  the time and day of the year --> \ndata.to_csv(\"data_matic_hourly.csv\", index = False, encoding = \"utf-8\")\nprint(data.shape)\ndata.head()\n","a99946b6":"highs = data.high\ntype(highs)","32d99072":"wealth_index = 1000*(1+data[\"high\"]).cumprod()# --> there are several values --> ","83fd8f5d":"print(len(wealth_index))\nwealth_index.max() #--> ","8772dc6e":"wealth_index.min()","899f7ecb":"# values in 10 timestamp. --> ","a5276973":"#prev_peaks = .cummax()","3fa29ff3":"prev_high = data.high.cummax()","e0845b20":"plt.plot(prev_high) # ","050e4e16":"dd = (wealth_index-prev_high)\/prev_high\nlen(dd)","220b94e8":"def add_member(np_series, value):\n    \"\"\"\n    np_series is the 1 element -->\n    adds a values from the last element - for the np.series -->  that is possible to \n    all index is shifting by 1 to the the end --> \n    \"\"\"\n    np_series = np_series[1:]\n    np_series = list(np_series)\n    np_series.append(value)\n    np_series = np.array(np_series)\n    np_series = np_series.reshape(1, np_series.shape[0], 1)\n    return np_series","929a89b3":"import pandas as pd\nimport numpy as np\n# where can we see the values -- that are prevalent to the  training for the learning data_sets\n# how is it possible to laod --> rest or all the elements in the datasets.\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow import keras\n# load the data - \nscaler = MinMaxScaler(feature_range = (0, 1))\ndata = pd.read_csv(\"..\/working\/data_matic_hourly.csv\")\nprint(data.shape)\ndata[\"difference\"] = data[\"high\"] - data[\"low\"]\ndata_high = data[[\"high\"]].values\ndata_low = data[[\"low\"]].values # high and low will be correlated with each other to run the program --> \n# some normalization for the data --. \nscaled_high = scaler.fit_transform(data_high)\nall_data = []\ntarget = []\nall_data_low = []\ntarget_low = []\n\nfor i in range(168, scaled_high.shape[0]):\n    all_data.append(scaled_high[i-168:i, 0])\n    target.append(scaled_high[i, 0])\n    #all_data_low.append(scale)\nall_data = np.array(all_data)\ntarget = np.array(target)\n\n\ncols = []\n\nfor i in range(all_data.shape[1]):\n    temp =  \"col\"+ str(i)\n    cols.append(temp)\nprint(len(cols))\n#cols\n\n# in order to see the correlation graph --> building the data frame is very critital\ncurrent_df = pd.DataFrame(data = all_data, columns = cols)\n#current_df.corr()# there is significant correlation for the data --> \ncurrent_df.head()\ntarget = np.array(target.reshape(target.shape[0], 1))\nall_data = all_data.reshape(all_data.shape[0], all_data.shape[1], 1)\nall_data_train = all_data[:1500]\nall_data_test = all_data[1500:]\ntrain_target = target[:1500]\ntest_target = target[1500:]\n\n\nprint(all_data_train.shape, train_target.shape, all_data_test.shape, test_target.shape) # this is \n\n# values should be called with those --. \n\n","2fff5b44":"# what kind of engineering works are great fit for loading the datasets -- all around here to maintain higher performance -- metrics --> \n# model creation for this is --> simple but --> prevalent to build it .\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.LSTM(units = 100, return_sequences = True, input_shape = (all_data.shape[1], 1)))\nmodel.add(keras.layers.LSTM(units = 100, return_sequences = True))\nmodel.add(keras.layers.LSTM(units = 100, return_sequences = True))\nmodel.add(keras.layers.LSTM(units = 100))\nmodel.add(keras.layers.Dense(units = 1))\nmodel.compile(optimizer = \"adam\", loss = \"mean_squared_error\")\nmodel.summary()\n","c70f7696":"epochs = 80# this is optimized hyper parameter\n# for the test just train within in a single epochs --> \n# 7 days --> correlations -- trigger  the generality \n# batch_size -- smaller is  slower training through the details\nhistory  = model.fit(all_data_train, train_target, epochs= epochs, batch_size=16, validation_data= (all_data_test, test_target)) ","a9c7b29f":"# there should be some loads for the datasets -->  that is running on the current service --> \n# all the correlation -- within the data --> \n# can reset the model to run --> build on it. cdc -- ","5907e233":"print(\"Loss function representation \") # this is the scaled loss functions -- that is running -- \nimport matplotlib.pyplot as plt\nplt.plot(history.history[\"loss\"])# can give the color coding here to build \nplt.plot(history.history[\"val_loss\"])\nplt.legend([\"training_loss\", \"validation_loss\"])\nplt.show()","d5d14480":"\"\"\"\ntest and training data_split approximate in size  -- similar loss values for good model.\n\n\"\"\"\n","87e9cbaf":"# not required to plot with this->\nplt.plot(history.history[\"val_loss\"]) #the validation loss -- will be updated when values  will be prevalent.\n# there is signicant drop in the performance of the model --. when it runs on the server\n# by showing the values the proper epochs would be 40\nplt.show()\n# by particular point -- required to have -- the scatter values --> ","1141ae05":"#model.save(\"model_high_with_30_epochs.pb\")\n# theere is some tendency for the graph that is missing very high defferentiation --> \nprint(max(history.history[\"val_loss\"]), min(history.history[\"val_loss\"]))\nmi(history.history[\"val_loss\"])\/max(history.history[\"val_loss\"]) ","e290b142":"# when the program runs -- i will get the highest -- points --> \n# some components of the program will be suffled to run -\nda","391ac26e":"current = all_data_test[-1]\ncurrent = current.reshape(1, current.shape[0], current.shape[1])\nvalue = model.predict(current)\npredicted = scaler.inverse_transform(value)\n\n\nts = list(current.reshape(current.shape[1]))\n\nts.append(value[0][0]) # this is the non scaled value that needs to included in the this data --> \nts = np.array(ts)\nts = ts.reshape(ts.shape[0], 1)\nts = scaler.inverse_transform(ts)\nts_high = ts.reshape(ts.shape[0])\nprint(ts_high.shape)\nplt.plot(ts_high[-20:]) # some outliers -->  if it represents the \nplt.show()\npredicted","7d2bb20c":"plt.plot(ts_high[-24:])# the minimum prices --. \n","eb0ed087":"ts.","79b56a14":"current = all_data_test[-1]\ncurrent = current.reshape(1, current.shape[0], current.shape[1])\ndef buildgraph(current, model):\n    \"\"\"\n    This will get the particular time series then returns - the values -- useful for the next prediciton.\n    current is test set to get the one stop data precition value -- np.array 1d\n    \n    \"\"\"\n    value = model.predict(current)\n    predicted = scaler.inverse_transform(value)\n\n    ts = list(current.reshape(current.shape[1]))\n\n    ts.append(value[0][0]) # this is the non scaled value that needs to included in the this data --> \n    ts = np.array(ts)\n    #ts = ts.reshape(ts.shape[0], 1)\n    #ts = scaler.inverse_transform(ts)\n    ts_high = ts\n    ts_high = ts_high[1:]# comes into the the  same length of row features\n    \n    print(ts_high.shape)\n    \n    \n    plt.plot(ts_high[-20:]) # some outliers -->  if it represents the \n    plt.legend([\"prices_scaled\"])\n    plt.show()\n    ts_high  = ts_high.reshape(1, ts_high.shape[0], 1)\n    \n    \n    return ts_high, predicted\n\n\n    \n    \n    \n    #ts = ts[1:]\n    #ts = list(ts)\n    #ts.append(value)\n    #ts = np.array(ts)\n    #ts = ts.reshape(1, ts.shape[0], 1)\n    #return ts\npredicted_steps = []\n# 20 data points will be calculated --> \nfor i in range(20): # get 10 steps\n    current, predicted_s = buildgraph(current, model)\n    print(predicted_s)\n    predicted_steps.append(predicted_s)\n    # running for forecast of th 3 stmaage predictions --> \n    # can use the last real values.\n    ","3481dc21":"predicted_steps # exposing --> ","32f67904":"import pandas as pd\nimport numpy as np\n# where can we see the values -- that are prevalent to the  training for the learning data_sets\n# how is it possible to laod --> rest or all the elements in the datasets.\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow import keras\n# load the data - \nscaler = MinMaxScaler(feature_range = (0, 1))\ndata = pd.read_csv(\"..\/working\/data_matic_hourly.csv\")\nprint(data.shape)\ndata[\"difference\"] = data[\"high\"] - data[\"low\"]\ndata_high = data[[\"low\"]].values\ndata_low = data[[\"low\"]].values # high and low will be correlated with each other to run the program --> \n# some normalization for the data --. \nscaled_high = scaler.fit_transform(data_high)\nall_data = []\ntarget = []\nall_data_low = []\ntarget_low = []\n\nfor i in range(168, scaled_high.shape[0]):\n    all_data.append(scaled_high[i-168:i, 0])\n    target.append(scaled_high[i, 0])\n    #all_data_low.append(scale)\nall_data = np.array(all_data)\ntarget = np.array(target)\n\n\n\ncols = []\n\n#temporary data load and validation that can clearly be the higher performance loads -- withall the rest of the variables.\n\n\nfor i in range(all_data.shape[1]):\n    temp =  \"col\"+ str(i)\n    cols.append(temp)\nprint(len(cols))\n#cols\n\n# in order to see the correlation graph --> building the data frame is very critital\ncurrent_df = pd.DataFrame(data = all_data, columns = cols)\n#current_df.corr()# there is significant correlation for the data --> \ncurrent_df.head()\ntarget = np.array(target.reshape(target.shape[0], 1))\nall_data = all_data.reshape(all_data.shape[0], all_data.shape[1], 1)\nall_data_train = all_data[:1500]\nall_data_test = all_data[1500:]\ntrain_target = target[:1500]\ntest_target = target[1500:]\n\n\nprint(all_data_train.shape, train_target.shape, all_data_test.shape, test_target.shape) # this is ","b87e3b34":"\n# model creation for this is --> simple but --> prevalent to build it .\nmodel1 = keras.models.Sequential()\nmodel1.add(keras.layers.LSTM(units = 100, return_sequences = True, input_shape = (all_data.shape[1], 1)))\nmodel1.add(keras.layers.LSTM(units = 100, return_sequences = True))\nmodel1.add(keras.layers.LSTM(units = 100, return_sequences = True))\nmodel1.add(keras.layers.LSTM(units = 100))\nmodel1.add(keras.layers.Dense(units = 1))\nmodel1.compile(optimizer = \"adam\", loss = \"mean_squared_error\")\nmodel1.summary() # there is the summary of the model that can be loaded into a several values.\n","d383f497":"# it is really important to make the current graph to the default --  \n# significance correlation graph \n# same training is includedhere -- run --> \nepochs = 80 # setting this epochs into the  50 then -- loading  with the lower batch_size as with hyper parameter  optimization --> \nhistory = model1.fit(all_data_train, train_target, epochs= epochs, batch_size=16, validation_data= (all_data_test, test_target)) ","cd4eed77":"# with non loss function, it almost better to have smaller batch_size but not really best --> \nprint(\"Loss function representation \") # this is the scaled loss functions -- that is running -- \nimport matplotlib.pyplot as plt\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.legend([\"training_loss\", \"validation_loss\"])\n\nplt.show()\nplt.plot(history.history[\"val_loss\"])\nplt.legend([\"validation_loss\"])\nplt.show()","bbea73a3":"current = all_data_test[-1]\ncurrent = current.reshape(1, current.shape[0], current.shape[1])\ndef buildgraph(current, model):\n    \"\"\"\n    This will get the particular time series then returns - the values -- useful for the next prediciton.\n    current is test set to get the one stop data precition value -- np.array 1d\n    \n    \"\"\"\n    value = model.predict(current)\n    predicted = scaler.inverse_transform(value)# predicted is already scaled\n\n    ts = list(current.reshape(current.shape[1]))\n\n    ts.append(value[0][0]) # this is the non scaled value that needs to included in the this data --> \n    ts = np.array(ts)\n    #ts = ts.reshape(ts.shape[0], 1)\n    #ts = scaler.inverse_transform(ts)\n    ts_high = ts\n    ts_high = ts_high[1:]# comes into the the  same length of row features\n    \n    print(ts_high.shape)\n    \n    # this comes into the series that needs to run with following statements --> \n    plt.plot(ts_high[-20:]) # some outliers -->  if it represents the \n    plt.legend([\"prices_scaled\"])\n    plt.show()\n    ts_high  = ts_high.reshape(1, ts_high.shape[0], 1)\n    \n    \n    return ts_high, predicted \n    \n    #ts = ts[1:]\n    #ts = list(ts)\n    #ts.append(value)\n    #ts = np.array(ts)\n    #ts = ts.reshape(1, ts.shape[0], 1)\n    #return ts\npredicted_steps_low = []\n# I will get the predictions for the next 20 time periods --> \nfor i in range(20):\n    current, predicted_s = buildgraph(current, model1)\n    print(predicted_s)\n    predicted_steps_low.append(predicted_s)\n    # running for forecast of th 3 stage predictions --> \n    # can use the last real values.\n    ","a878b485":"highest_predicted = []\nfor i in predicted_steps:\n    highest_predicted.append(i[0][0])# there\n\nlowest_predicted = []\nfor i in predicted_steps_low:\n    lowest_predicted.append(i[0][0])\n    ","0f32ebca":"# re-scale ","8f142b82":"\"\"\"\ncurrent = all_data_test[-1]\ncurrent = current.reshape(1, current.shape[0], current.shape[1])\nvalue = model1.predict(current)\npredicted = scaler.inverse_transform(value)\n\n\nts = list(current.reshape(current.shape[1]))\n\nts.append(value[0][0]) # this is the non scaled value that needs to included in the this data --> \nts = np.array(ts)\nts = ts[1:]\nts = ts.reshape(1, ts.shape[0], 1)\nvalue = model1.predict(ts)\npredicted1 = scaler.inverse_transform(value)\n#ts = scaler.inverse_transform(value)\nts_high = ts.reshape(ts.shape[0])\n# there are several functions that can run with higher performance metrics -->  \nprint(predicted, predicted1)\nts = list(current.reshape(current.shape[1]))\nplt.plot(ts_high[-20:]) # some outliers -->  if it represents the \nplt.show()\n\n \n\"\"\"","c0438379":"plt.plot(highest_predicted)\nplt.plot(lowest_predicted)\nplt.legend([\"highest\", \"lowest\"])\nplt.title(\"hourly prices for next 10 hrs\")\nplt.xlabel(\"time points\")\nplt.ylabel(\"prices\")\n# model is not to comptitive --> but runs okay --> build the relevance chart  in the model system","fcb338ed":"highest_predicted\n# just let the model get the trained ","9a1523b5":"data[-20:][[\"high\", \"low\"]].values[0][1]","c4619509":"import time\ntime.time()","78ce082b":"model1.save(filepath = \"..\/working\/\")","b5d9da61":"model.save(filepath  = \"..\/working\/\")","666f1318":"model.summary()","ca5729e7":"model1.summary()","e0fb6183":"# there are crititcal points that runs -- with following elements -- that can be loaded --  with the rest of the elements .\nimport os","85a68b58":"os.listdir() #there\n","961c2763":"#current_model = keras.models.load_model(\"..\/working\/\")# there there are assets for the model loading --\n\"\"\"\nFor following actions to take careful consideration on the dataflow -- that can\"\"","a7b9d65f":"# encoding the original data with particular valued kernel --> that is --> \n# Dropout there developer can set values --> that can be built to affect the dropout\n# this is hyperparameter --. \n# build values using exponents that is related to values -->\n# accomplice --> \ndata.to_csv(\"matic_late.csv\", index = False, encoding = \"utf-8\")","e238379b":"history  = model.fit(all_data_train, train_target, epochs= 100, batch_size=100, validation_data= (all_data_test, test_target)) "}}