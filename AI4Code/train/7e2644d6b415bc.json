{"cell_type":{"0ccfc74b":"code","a4fcf759":"code","138e3b65":"code","1e027cfc":"code","7cce5268":"code","557f4050":"code","68abe2d3":"code","ed4db709":"code","6c1b2b33":"code","c6c0d4fe":"code","cdab1e1e":"code","b529245a":"code","dda5c719":"code","4889419f":"code","451a9f25":"code","3c1178b9":"code","dbd254c3":"code","8cf69234":"code","680c4cd4":"code","ab73ef11":"code","dddced29":"code","dfdebaa2":"code","4f87c87e":"code","b2cc118c":"code","b9fc2db6":"code","6a62d0ba":"code","bfa89d87":"code","3b821e04":"code","6c8048d3":"code","6e7a6370":"code","a63782c9":"code","7a77515c":"code","c5e47f31":"code","43ed9706":"code","32297700":"code","af72c6dc":"code","10d6025c":"code","49c04027":"code","7141dc0a":"code","ab05bc29":"code","2b6544e8":"markdown","2321998e":"markdown","522383ea":"markdown","d8138cb7":"markdown","b93f8391":"markdown","63bd3394":"markdown"},"source":{"0ccfc74b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a4fcf759":"# Load some extra packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom matplotlib import rcParams\nimport plotly.graph_objects as go\npd.set_option('display.max_columns', 80) \nimport random","138e3b65":"# Load the dataset\ndata = pd.read_csv(\"\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv\")","1e027cfc":"# First 5 row\ndata.head()","7cce5268":"# Shape of data\ndata.shape","557f4050":"# Info of data\ndata.info()","68abe2d3":"# Unique value for some columns\na = [\"ssc_b\", \"hsc_b\" ,\"hsc_s\",\"degree_t\", \"workex\", \"specialisation\", \"status\"]\ni = 0\nfor i in range(len(a)):\n    b = data[a[i]].unique()\n    c = a[i]\n    i += 1\n    print(f\"column name: {c} \\nUnique value: {b}\")","ed4db709":"# Fill the salary column - because in salary column where status is Not Placed the salary column value is np.nan, So replace it by 0\ndata[\"salary\"] = data[\"salary\"].fillna(0)","6c1b2b33":"# Create random palettes - try to make some fun\n\ncmap_list = ['Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', \n             'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2', 'Dark2_r', 'GnBu', 'GnBu_r', \n             'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', \n             'PRGn', 'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', \n             'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', \n             'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 'RdPu', 'RdPu_r', \n             'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', \n             'Set2_r', 'Set3', 'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', \n             'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r', 'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', \n             'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 'bwr', 'bwr_r', 'cividis', \n             'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix', \n             'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', \n             'gist_heat', 'gist_heat_r', 'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', \n             'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 'gnuplot', 'gnuplot2', 'gnuplot2_r', \n             'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', 'inferno', \n             'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', \n             'ocean', 'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', \n             'rainbow_r', 'rocket', 'rocket_r', 'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', \n             'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 'tab20b_r', 'tab20c', 'tab20c_r', \n             'terrain', 'terrain_r', 'twilight', 'twilight_r', 'twilight_shifted', 'twilight_shifted_r', 'viridis',\n             'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r']\n\ni = 0\nfor i in range(len(cmap_list)):\n    n = random.randint(0, i)\n    a = cmap_list[n]\nsns.set_palette(a)","c6c0d4fe":"# Count of gender from data\n\ngenders = data[\"gender\"].value_counts()\nsns.set_style(\"whitegrid\")\nfig, (ax, ax1) = plt.subplots(1, 2, figsize=(18, 6))\n\ndef func(pct, allvals):\n    absolute = int(pct\/100.*np.sum(allvals))\n\n    return \"{:.1f}%\\n({:d})\".format(pct, absolute)\nwedges, texts, autotexts = ax.pie(genders, autopct=lambda pct: func(pct, genders),\n                                  textprops=dict(color=\"w\"), colors=[\"#ff6f69\", \"#ffcc5c\"])\nsns.countplot(data[\"gender\"])\nax.legend(genders.index,\n          title=\"Gender\",\n          loc=\"center left\",\n          bbox_to_anchor=(1, 0, 0, 1))\n\nplt.setp(autotexts, size=8, weight=\"bold\")\n\nplt.suptitle(\"Count of gender\")","cdab1e1e":"plt.subplots(figsize=(10, 6))\nsns.countplot(data[\"ssc_b\"], hue=data[\"status\"])\nplt.title(\"SSC Board Vs Status\")","b529245a":"plt.subplots(figsize=(10, 6))\nsns.countplot(data[\"hsc_b\"], hue=data[\"status\"])\nplt.title(\"HSC Board Vs Status\")","dda5c719":"plt.subplots(figsize=(10, 6))\nsns.countplot(data[\"hsc_s\"])","4889419f":"a = data[data[\"ssc_b\"]==\"Others\"].groupby([\"ssc_b\", \"ssc_p\"]).mean().reset_index()\nb = data[data[\"ssc_b\"]==\"Central\"].groupby([\"ssc_b\", \"ssc_p\"]).mean().reset_index()\n\nfig = px.line(a, x=a.index, y='ssc_p', title=\"Central and Others Board SSC Percentage\")\n\n# Only thing I figured is - I could do this \nfig.add_scatter(x=b.index, y=b['ssc_p'], name=\"Central\", showlegend=False) # Not what is desired - need a line \n\n# Show plot \nfig.show()","451a9f25":"a = data[data[\"hsc_b\"]==\"Others\"].groupby([\"hsc_b\", \"hsc_p\"]).mean().reset_index()\nb = data[data[\"hsc_b\"]==\"Central\"].groupby([\"hsc_b\", \"hsc_p\"]).mean().reset_index()\n\nfig = px.line(a, x=a.index, y='hsc_p', title=\"Central and Others Board HSC Percentage\")\n\n# Only thing I figured is - I could do this \nfig.add_scatter(x=b.index, y=b['hsc_p'], name=\"Central\", showlegend=False) # Not what is desired - need a line \n\n# Show plot \nfig.show()","3c1178b9":"a = data[data[\"degree_t\"]==\"Sci&Tech\"].groupby(\"degree_p\").mean().reset_index()\nb = data[data[\"degree_t\"]==\"Comm&Mgmt\"].groupby(\"degree_p\").mean().reset_index()\nc = data[data[\"degree_t\"]==\"Others\"].groupby(\"degree_p\").mean().reset_index()\nsns.set_style(\"whitegrid\")\n\nplt.subplots(figsize=(16,7))\na[\"degree_p\"].plot(label=\"Sci&Tech\", linewidth=3)\nb[\"degree_p\"].plot(label=\"Comm&Mgmt\", linewidth=3)\nc[\"degree_p\"].plot(label=\"Others\", linewidth=3)\nplt.legend()\nplt.title(\"Under Graduation(Degree type) Vs Under Graduation(Degree percentage)\")","dbd254c3":"plt.subplots(figsize=(10, 6))\nsns.countplot(data[\"degree_t\"], hue=data[\"status\"])\nplt.title(\"Under Graduation(Degree type) Vs Status\")","8cf69234":"plt.subplots(figsize=(10, 6))\nsns.countplot(data[\"degree_t\"], hue=data[\"workex\"])\nplt.title(\"Under Graduation(Degree type) Vs Work Experience\")","680c4cd4":"plt.subplots(figsize=(10, 6))\nsns.countplot(data[\"degree_t\"], hue=data[\"status\"])\nplt.title(\"Under Graduation(Degree type) Vs Status\")","ab73ef11":"plt.subplots(figsize=(10, 6))\nsns.countplot(data[\"specialisation\"], hue=data[\"status\"])\nplt.title(\"Specialisation Vs Status\")","dddced29":"# Label Encoding\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\nlist_col = [\"gender\", \"ssc_b\", \"hsc_b\", \"hsc_s\", \"degree_t\", \"workex\", \"specialisation\", \"status\"]\ni = 0\nfor i in range(len(list_col)):\n    data[list_col[i]] = label_encoder.fit_transform(data[list_col[i]])\n    i+=1","dfdebaa2":"data.head()","4f87c87e":"# drop sl_no\ndata.drop('sl_no', axis=1, inplace=True)","b2cc118c":"# Correlation and visualization in heatmap\n\ncmap_list = ['Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', \n             'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2', 'Dark2_r', 'GnBu', 'GnBu_r', \n             'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', \n             'PRGn', 'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', \n             'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', \n             'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 'RdPu', 'RdPu_r', \n             'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', \n             'Set2_r', 'Set3', 'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', \n             'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r', 'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', \n             'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 'bwr', 'bwr_r', 'cividis', \n             'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix', \n             'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', \n             'gist_heat', 'gist_heat_r', 'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', \n             'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 'gnuplot', 'gnuplot2', 'gnuplot2_r', \n             'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', 'inferno', \n             'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', \n             'ocean', 'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', \n             'rainbow_r', 'rocket', 'rocket_r', 'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', \n             'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 'tab20b_r', 'tab20c', 'tab20c_r', \n             'terrain', 'terrain_r', 'twilight', 'twilight_r', 'twilight_shifted', 'twilight_shifted_r', 'viridis',\n             'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r']\n\ni=0\nfor i in range(len(cmap_list)):\n    n = random.randint(0, i)\n    cmap = cmap_list[n]\n    i+=1\nplt.subplots(figsize=(16,7))\ncorre = data.corr()\nsns.heatmap(corre, annot=True, linewidths=.5, cmap=cmap)","b9fc2db6":"# Relation between status and other columns\ncorre[\"status\"].sort_values(ascending=False)","6a62d0ba":"# Copy data, so that, it can't hamper real data\ndata1 = data.copy()\n# data1 = data.drop(\"salary\", axis=1)","bfa89d87":"# Split the data\n\nfrom sklearn.model_selection import train_test_split\nX = data1.drop([\"status\", \"salary\"], axis=1)\ny = data1[\"status\"]\n\nseed = 42\ntest = 0.3\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed, test_size=test)","3b821e04":"# Scale down features\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","6c8048d3":"# Load necessary package\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score","6e7a6370":"# different types of model and accuracy\nnames = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SVC\",\n         \"Naive Bayes\"]\n\nclassifiers = [\n    KNeighborsClassifier(n_neighbors=3),\n    DecisionTreeClassifier(random_state=0),\n    RandomForestClassifier(n_estimators=700),\n    LogisticRegression(),\n    BernoulliNB(),\n    SVC(kernel = 'linear')\n]\n\nmodels = zip(names, classifiers)\nfor name, model in models:\n    model_name = model\n    model_name.fit(X_train, y_train)\n    predict = model_name.predict(X_test)\n    accuracy1 = accuracy_score(predict, y_test)\n    accuracy3 = classification_report(predict, y_test)\n    scores = cross_val_score(model, X_train, y_train, cv = 5, scoring=\"accuracy\")\n\n    print(f\"{name} model Accuracy Score {accuracy1}\")\n    \n    print(\"******************************\")\n        \n    print(f\"classification report {accuracy3}\")\n    \n    print(\"******************************\")\n    \n    print(f\"{name} and prediction {predict}\")\n    \n    print(\"******************************\")\n    \n    print(f\"cross val score {scores}\")\n    \n    print(\"*******************************\")","a63782c9":"X = data.drop(\"salary\", axis=1)\ny = data[\"salary\"].copy()","7a77515c":"seed = 42\ntest = 0.3\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed, test_size=test)","c5e47f31":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","43ed9706":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error","32297700":"def regression(pipeline, X_train, y_train, X_test, y_test):\n    model_fit = pipeline.fit(X_train, y_train)\n    model_pred = model_fit.predict(X_test)\n    mse = mean_squared_error(y_test, model_pred)\n    print(\"-\" * 50)\n    rmse = np.sqrt(mse)\n    print(\"accuracy score: {}%\".format(rmse))\n    return rmse, model_pred\n\n    \nnames = [\"LinearRegression\", \"RandomForest\", \"DecisionTree\"]\nregressor = [\n    LinearRegression(),\n    RandomForestRegressor(),\n    DecisionTreeRegressor()\n]\n\nzipped_reg = zip(names, regressor)\n\ndef compare_clf(regressor=zipped_reg):\n    result = []\n    for n, r in zipped_reg:\n        checker_pipeline = Pipeline([\n            (\"regressor\", r)\n        ])\n        reg = regression(checker_pipeline, X_train, y_train, X_test, y_test)\n        result.append((n, reg))\n        \n    return result","af72c6dc":"results = compare_clf()\nresults","10d6025c":"df = pd.DataFrame({'Actual': y_test, 'Linear': results[0][1][1], 'Random': results[1][1][1], 'Decision': results[2][1][1]})\ndf.reset_index()","49c04027":"df1 = df.head(25)\ndf1.plot(kind='bar',figsize=(16,10))\nplt.show()","7141dc0a":"a = list(results[0][1][1])\nb = list(results[1][1][1])\nc = list(results[2][1][1])\nd = list(y_test)","ab05bc29":"plt.subplots(figsize=(13, 6))\nplt.plot(a,c='green', label= 'predictions Logistic')\nplt.plot(d,c='black', label= 'actual')\nplt.legend()\nplt.show()\n\nplt.subplots(figsize=(13, 6))\nplt.plot(b,c='red', label= 'predictions RandomForest')\nplt.plot(d,c='black', label= 'actual')\nplt.legend()\nplt.show()\n\nplt.subplots(figsize=(13, 6))\nplt.plot(c,c='blue', label= 'predictions DecisionTree')\nplt.plot(d,c='black', label= 'actual')\nplt.legend()\nplt.show()","2b6544e8":"# If you like, please upvote","2321998e":"![Difference-Between-Recruitment-and-Selection.png](attachment:Difference-Between-Recruitment-and-Selection.png)","522383ea":"# From my curiosity ------ I do regression analysis","d8138cb7":"# Questions And Answer\n\n## 1. Which factor influenced a candidate in getting placed?\n\n# Ans: Status is highly related with salary, then ssc percentage(ssc_p)\n\n## 2. Does percentage matters for one to get placed?\n\n# Ans: Yes, percentage matters for one get placed\n\n## 3. Which degree specialization is much demanded by corporate?\n\n# Ans: Marketing and Finance is much demanded by corporate\n\n## 4. Play with the data conducting all statistical tests.","b93f8391":"# If you like, Please upvote","63bd3394":"**data is too little. It is clear that salary column is fully depended on status column. If placed then have salary, either notplaced have salary 0. So, if we take salary column it will fool us giving 100% accuracy.**\n\n**You Can Try**"}}