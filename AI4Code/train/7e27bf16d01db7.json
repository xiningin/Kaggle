{"cell_type":{"d4cb1daf":"code","8c5544a7":"code","2441958b":"code","c4e3ed4c":"code","31be553d":"code","00974f3b":"code","efa1946e":"code","9a0243c7":"code","a0b8c71e":"code","88e38560":"code","8cef5e08":"code","ebf71acd":"code","51e250cf":"code","9f74a276":"code","0a4c413f":"code","abbac253":"code","145d0cbf":"code","ae903682":"code","d5903f93":"code","5cc3d684":"code","eacb19d6":"code","c7c6b59e":"code","7624dc5b":"code","b0d19892":"code","01c0f07d":"code","9f109c37":"code","98736046":"code","d08dca8b":"code","7864b6b1":"code","0c7978e3":"code","c1ecae24":"code","036defd2":"code","d611dd80":"code","928060ea":"code","d48f25fa":"code","eb944875":"code","151b1e38":"code","4f88c0bb":"code","39c43801":"code","755e8d06":"code","bf3fb06e":"code","bbfac997":"code","fb82972e":"code","c6edf133":"code","856e476d":"code","44d55cf8":"code","5814f20e":"code","be243c62":"code","05200c04":"code","e89694cc":"code","2ad65969":"code","2f95fe71":"code","640e8333":"code","8d2bd492":"code","0521aecd":"code","3e21b744":"code","2d63858f":"code","e2c6f5e7":"code","d6c5ce78":"code","0d208c7b":"code","cc16f257":"code","e985f0a9":"code","aea1636a":"code","ada09e36":"code","e9f0ead6":"code","6c394b22":"code","fdcbb224":"code","541cfc22":"code","8db001c1":"code","6d88d6a5":"code","bf0f3a1c":"code","3201835c":"code","18ba3ad1":"code","f7bf5fb9":"code","10450227":"code","f82be326":"code","47311e2e":"code","79d74cc1":"code","9f68a804":"markdown","6d38cfc5":"markdown","385c104f":"markdown","d2c0a219":"markdown","981b0ab4":"markdown","844f7a93":"markdown","294fb13c":"markdown","7892c6f9":"markdown","112e19c7":"markdown","cc3abc2d":"markdown","1f4a7c33":"markdown","879991cb":"markdown","f49e21a2":"markdown","2deeb751":"markdown","c51c839c":"markdown","136c3624":"markdown","93054ea8":"markdown","87d99b39":"markdown","80d57146":"markdown","c2deeed3":"markdown","a36a6a57":"markdown","051424dd":"markdown","e1258c2b":"markdown","4e99bda9":"markdown"},"source":{"d4cb1daf":"import os\nimport numpy as np\nimport pandas as pd\nimport datetime as dt","8c5544a7":"!pip install -U newspaper3k","2441958b":"import requests\nfrom newspaper import Article\nfrom newspaper.article import ArticleException","c4e3ed4c":"!pip install -U stanza","31be553d":"import stanza\nimport hashlib","00974f3b":"all_files = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if filename.startswith('Subreddit_Coronavirus_') and filename.endswith('.tsv'):\n            f = os.path.join(dirname, filename)\n            all_files.append(f)\nlen(all_files)","efa1946e":"frames = []\nfor fname in sorted(all_files)[-10:-9]: # ONLY ONE DAY in this Notebook due to runtime restrictions...\n    print(fname)\n    df = pd.read_csv(fname, sep='\\t')\n    frames.append(df)\nDATA = pd.concat(frames)\nlen(DATA)","9a0243c7":"DATA.head()","a0b8c71e":"URLS = list(set(DATA.url.values.tolist()))\nlen(URLS)","88e38560":"exclude_urls = ['reddit.','redd.','youtube','youtu.','twitter','facebook','fb','google.','.mp3','.pdf','\/r\/','imgur.']\nexclude_urls","8cef5e08":"def parse_article(url):\n    try:\n        #r = requests.get(url, verify=False, timeout=10)\n        r = requests.get(url, timeout=5)\n        article = Article(url)\n        article.download()\n        article.parse()\n    except:\n        article = None\n    return article","ebf71acd":"all_articles = []\nfor count_u, URL in enumerate(URLS[:]):\n    print(\"%4i\"%(count_u+1), '\/', len(URLS), '|', \"%4i\"%len(all_articles), '|', dt.datetime.now(), URL[:200])\n    S = set([x in URL for x in exclude_urls])\n    if len(S)==1 and not True in S:\n        article = parse_article(URL)\n        if article!=None:\n            all_articles.append(article)\nlen(all_articles)","51e250cf":"len(all_articles)","9f74a276":"stanza.download('en')","0a4c413f":"nlp = stanza.Pipeline(processors='tokenize', lang='en', use_gpu=True)","abbac253":"ALL_SENTENCES = []\nprint(dt.datetime.now())\nfor count_a, article in enumerate(all_articles):\n    file_id = hashlib.md5(article.url.encode()).hexdigest()\n    article_url = article.url\n    article_published_datetime = article.publish_date\n    # ---\n    article_title = article.title\n    doc = nlp(article_title)\n    for sent in doc.sentences:\n        S = ' '.join([w.text for w in sent.words])\n        sH = hashlib.md5(S.encode('utf-8')).hexdigest()\n        print(\"%6i\"%count_a, '|', file_id, sH, \"%10i\"%len(ALL_SENTENCES), (S[:150],))\n        ALL_SENTENCES.append([file_id, sH, S])\n    # ---\n    article_text = [a.strip() for a in article.text.splitlines() if a.strip()]\n    for paragraph in article_text:\n        doc = nlp(paragraph)\n        for sent in doc.sentences:\n            #S = ' '.join([w.text for w in sent.words])\n            S = str(sent.text)\n            sH = hashlib.md5(S.encode('utf-8')).hexdigest()\n            print(\"%6i\"%count_a, '|', file_id, sH, \"%10i\"%len(ALL_SENTENCES), (S[:150],))\n            ALL_SENTENCES.append([file_id, sH, S, article_published_datetime, article_url])\n    #break\nprint(dt.datetime.now())\nlen(ALL_SENTENCES)","145d0cbf":"len(ALL_SENTENCES)","ae903682":"ALL_SENTENCES_df = pd.DataFrame(ALL_SENTENCES, columns=[\"file_id\", \"sentence_hash\", \"sentence\", \"published_datetime\", \"article_url\"])\nlen(ALL_SENTENCES_df)","d5903f93":"ALL_SENTENCES_df.to_csv('ALL_SENTENCES_Subreddit_Coronavirus_2020-04-03T000000_2020-04-04T000000.tsv', sep='\\t', index=False, index_label=False)","5cc3d684":"print(len(ALL_SENTENCES_df))\nALL_SENTENCES_df = ALL_SENTENCES_df.drop_duplicates('sentence_hash')\nprint(len(ALL_SENTENCES_df))","eacb19d6":"kw = ['corona', 'covid', 'it', 'sars-cov-2', 'virus']\nprint(kw)\ntarget_sentence_hashes = []\nfor sH, sentence in ALL_SENTENCES_df[[\"sentence_hash\",\"sentence\"]].values.tolist():\n    sl = sentence.lower()\n    if kw[0] in sl or kw[1] in sl or kw[2] in sl or kw[3] in sl or kw[4] in sl:\n        target_sentence_hashes.append(sH)\n        #break\nlen(target_sentence_hashes)","c7c6b59e":"TARGET_SENTENCES_df = ALL_SENTENCES_df.loc[ALL_SENTENCES_df.sentence_hash.isin(target_sentence_hashes)]\nlen(TARGET_SENTENCES_df)","7624dc5b":"TARGET_SENTENCES_df.to_csv('TARGET_SENTENCES_Subreddit_Coronavirus_2020-04-03T000000_2020-04-04T000000.tsv', sep='\\t', index=False, index_label=False)","b0d19892":"research_findings = dict()","01c0f07d":"research_findings[\"Preventions and Cures\"] = \"\"\"gargling with bleach will prevent or cure\ndrinking corona beer will prevent or cure\ntaking acetic acid will prevent or cure\ntaking steroids will prevent orcure\ntaking colloidal silver will cure\ntaking MMS (which contains chlorine dioxide) can cure\nusing essential oils will prevent \ngargling with salt water will prevent or cure \ngargling with ethanol will prevent or cure \neating raw garlic will prevent\ngarlic will cure\nspraying normal drinking alcohol on body will prevent\nspraying chlorine on body prevents coronavirus \ndrinking or washing in sesame oil prevents coronavirus \nusing a hand dryer will kill corona virus\ndrinking water every 15 minutes will wash virus to the stomach where it will die\ndrinking hot water will kill the virus\navoiding eating ice cream will prevent\nrinsing your nose with saline will prevent\naspirating boiling water vapor for five minutes deactivates the coronavirus\nhaving a pneumonia shot will prevent you from getting the disease\nhaving a flu shot will prevent you from getting the disease\nusing cocaine prevents\/cures\nAsians are more likely to get Covid 19\nWearing more than one mask will help prevent the virus\nIt is dangerous to receive packages from china\nIt is dangerous to eat in a Chinese restaurant (more so than other restaurants)\nCold weather and snow can kill off the SARS-CoV-2 virus\nWarmer weather will kill off the SARS-CoV-2 virus\nTaking a hot bath prevents COVID-19\nHand dryers are effective in killing off the SARS-CoV-2 virus\nAntibiotics are effective in preventing and treating the SARS-CoV-2 virus\nVinegar is more effective than hand sanitizer\nGargling with Vinegar prevents covid-19\nHand sanitizer does not work\nShuanghuanglian (Chinese medical herb) can prevent\ndrinking cow urine and applying cow dung on the body can cure coronavirus\nthere is no risk to mass public gatherings as the virus is impossible to contract outdoors\ndrinking water every 15 minutes will prevent coronavirus\nMiracle Mineral Supplement cures\nsilver-infused toothpaste will kill the virus\nKenneth Copeland can cure the virus directly from his tv studio\nyou have to be with someone for 10 minutes to catch the virus from them\ntaking ibuprofen makes COVID-19 worse, especially in new patients\nreading the Quran will make you immune\nyou can kill the virus by holding a blow dryer up to your nose\nSheep head\u2019s soup is a preventative\nLemon and bicarbonate kill the virus\nLemon juice with salt is a cure\nLemon juice with Chinese mesona is a cure\nLemon juice with turmeric prevents\nConsuming alcoholic beverages may help reduce the risk of infection by the novel coronavirus\nCoronavirus will be cured after 14 hours of curfew\nBitter gourd juice can cure coronavirus in two hours\n\u201cGale of the wind\u201d and neem tree leaves can prevent corona virus\nSitting in the sunlight will cure COVID-19\nCoronavirus can be cured by sniffing clove and camphor\nGreen chiretta can cure\nEating bananas will prevent\n15 minutes in Sauna will kill the virus\nMorel mushrooms increase risk by 200%\nEating onions with salt will cure\nPropolis cures COVID-19\nArsenic album-30 homeopathic medicine can prevent coronavirus\nDrinking boiled garlic water cures\nA bovine vaccine can be used to inoculate people against coronavirus\nShaving your beard prevents\nTurkish raki prevents\nConstant sex kills coronavirus\nAn alkaline diet prevents\nVolcanic ash kills coronavirus\nKetamine can cure\nPutting an opinion in your room can prevent\nBlack tea can prevent\nPu\u2019er tea can prevent\nEnvironmental enzymes can prevent\nCocaine kills coronavirus\"\"\".splitlines()\nprint(len(research_findings[\"Preventions and Cures\"]))","9f109c37":"research_findings[\"Nature of Virus or Disease\"] = \"\"\"Corona virus is just a cold \nCovid 19 is a normal flu and is no more dangerous than that\nChildren cannot catch corona virus\nSARS-CoV-2 is mutating faster than normal viruses \nPregnant women who have Covid 19 can pass the virus through the placenta to the unborn child\nCovid 19 only affects the elderly\nThe SARS-CoV-2 virus can be transmitted through mosquito bite\nAfricans are immune\nVegetarians are immune\nPeople in India can resist the coronavirus\nPeople with type-A blood are more prone to get coronavirus\nEveryone with Covid 19 dies\nThere is no corona (virus)\nThis is \u201cfake news\u201d invented by Trump to strengthen the dollar\nChloroquine in any amount can cure Covid 19\nThe outbreak of covid 19 is not real\nCOVID-19 breeds rapidly in toilet paper\nTurks area immune to covid-19\nCoronavirus is caused by snakes\nCoronavirus activates all bacterias and viruses in your body\"\"\".splitlines()\nprint(len(research_findings[\"Nature of Virus or Disease\"]))","98736046":"research_findings[\"Conspiracy theories including bioweapons\"] = \"\"\"It was created in a lab\nIt is a US\/CIA created bioweapon\nIt is a Chinese bioweapon\nIt is a Russian bioweapon\nIt leaked from a bio-weapons lab in China\nIt leaked from the Wuhan Institute of Virology in China\nIt was caused by an infected rat biting a student in the Wuhan Institute of Virology in China\nMarch 12, Ayatollah Khamenei falsely claimed that there is evidence that COVID-19 might be a \u201cbiological attack.\u201d\nElectromagnetic fields and the introduction of 5G wireless technologies led to Covid 19 outbreaks\nWashing hands is propaganda by soap companies\nThis was a plan from Gate Foundation to increase the Gate\u2019s wealth\nQatar knew abut Covid-19 since 2015 and Doha paid billions to China to \u201cgrow the virus\u201d\nCorona virus is being spread by Coca-Cola\nUS military brought the virus to Wuhan on October 2019\nBill Gates and Vatican have a plan to depopulate world with coronavirus vaccine\nSaddam Hussein in 90s told his cabinet that US threatened to spread coronavirus if he didn\u2019t follow US commands\nCuba gave a vaccine to China\nCuba has a vaccine called Interferon\nIsrael found the cure\nChina finds a vaccine 3\/17\nInstitut Pasteur in France invented COVID-19\nCOVID-19 is no worse than other outbreaks, it is just being hyped to hurt Trump\nCOVID-19 is a scripted narrative to justify closed borders\nCOVID-19 is a scripted narrative to force people to stay at home\nEgypt developed cure (doctor developed a fluid)\nEgypt gave China the vaccine\nUS has 40 thousands troops immune to COVID-19 conducting an invasion of Europe\nCDC admits coronavirus originated in the US\nCow urine and feces can cure\nCOVID-19 was created in 2014 by an English Institute\nEcuadorean doctor develops vaccine 3\/3\nFilipino student develops vaccine and gets 5million US from Alibaba founder Jack Ma\nRa\u00fal Rodolfo Abhduz Khan, biochemical engineer from Karmalah Laboratories, is the creator of coronavirus.\"\"\".splitlines()\nprint(len(research_findings[\"Conspiracy theories including bioweapons\"]))","d08dca8b":"research_findings[\"False Diagnostic Procedures\"] = \"\"\"If you can hold your breath for 10 seconds you don\u2019t have Covid 19\"\"\".splitlines()\nprint(len(research_findings[\"False Diagnostic Procedures\"]))","7864b6b1":"research_findings[\"Emergency Measures\"] = \"\"\"NYC is under martial law 3\/20\nOnly people who have tested positive need to stay home and isolate themselves\nOnly large gatherings have to be stopped\nAll human interaction needs to be stopped\nCoronavirus is spread only by coughing and sneezing\nSocial distancing will lead to dramatic immediate results\n1 month of social distancing will stop the epidemic permanently\nFriend\/relative at Pentagon said the US is going on total lockdown\nFriend\/relative at White House said the US is going on total lockdown\nRussia has unleashed over 500 lions to ensure that people stay inside their homes amid the coronavirus outbreak\nIn Spain \u2013 free internet during 60 days due to quarantine\nPolish telecommunications company used quarantine as a cover up to put 100 5G antennas in Gdynia\nRiots in London due to unavailability of food leading to lockdown and martial law 3\/22\nIn USA Helicopters are spraying disinfectants to try to eradicate coronavirus\nBangalore Municipal body will be spraying medicine in the air to kill coronavirus\nHomeland Security is mobilizing the national guard to combat coronavirus 3\/20\nJanta curfew in India will break the chain of transmission\nElderly people in Brazil who are caught wandering outside with have their pensions cancelled by the government\nPeople in Spain are committing suicide due to quarantine 3\/20\nParks in Adra Spain are being fumigated with poison\nSpanish helicopters are spreading medicines against coronavirus\nOman helicopters are spraying pesticides to eliminate coronavirus\nVenezuela helicopters are going to spread chemicals against coronavirus\nTrump will impose a nationwide mandatory quarantine in 38 to 72 hours 3\/16\nMilitary in Philadelphia are preparing to invoke martial law and bring citizens to FEMA camps\nSupermarkets are recalling coronavirus-infected toilet paper\nPence urges people with coronavirus to go to police 3\/2\nPolice in China kill covid-19 patients 3\/2\nChinese coronavirus patients are being cremated alive 2\/25\"\"\".splitlines()\nprint(len(research_findings[\"Emergency Measures\"]))","0c7978e3":"research_findings[\"Good News Stories\"] = \"\"\"Pandemic caused Venice\u2019s water to be clear so the swans returned\nPandemic caused Venice\u2019s water to be clear so the dolphins returned\nElephants break into a village due to social distancing and get drunk on corn wine\"\"\".splitlines()\nprint(len(research_findings[\"Good News Stories\"]))","c1ecae24":"curated_claims = []\nfor story_type, claims in research_findings.items():\n    print(story_type)\n    print(len(claims))\n    for claim in claims:\n        curated_claims.append(claim)\n    print()\nlen(curated_claims)","036defd2":"pip install -U sentence-transformers","d611dd80":"from sentence_transformers import SentenceTransformer","928060ea":"model = SentenceTransformer('bert-base-nli-mean-tokens')","d48f25fa":"curated_claims_embeddings = model.encode(curated_claims)\nlen(curated_claims_embeddings)","eb944875":"sentences = [sentence[0] for sentence in TARGET_SENTENCES_df[['sentence']].values.tolist()]\nlen(sentences)","151b1e38":"sentence_embeddings = model.encode(sentences)\nlen(sentence_embeddings)","4f88c0bb":"from sklearn.metrics.pairwise import cosine_similarity","39c43801":"cos_sim_list = []\ncos_sim_array = np.zeros((len(sentence_embeddings), len(curated_claims_embeddings)))\nprint(cos_sim_array.shape)\nfor count_s, sentence_embedding in enumerate(sentence_embeddings):\n    if not count_s % 1000:\n        print(\"%6i\"%count_s, '|', \"%6i\"%len(cos_sim_list), '|', dt.datetime.now())\n    cos_sim_sent = []\n    for count_c, curated_claims_embedding in enumerate(curated_claims_embeddings):\n        cs = cosine_similarity(\n            curated_claims_embedding.reshape(1,len(curated_claims_embedding)),\n            sentence_embedding.reshape(1,len(sentence_embedding)))\n        cos_sim_sent.append(cs)\n        cos_sim_array[count_s, count_c] = cs\n        #print(\"%.4f\"%cs)\n    cos_sim_list.append(cos_sim_sent)\n    #break\nlen(cos_sim_list)","755e8d06":"similarity_threshold = 0.84\nmatches = np.where(cos_sim_array >= similarity_threshold)\nlen(matches), len(matches[0]), len(matches[1])","bf3fb06e":"positions = list(zip(matches[0], matches[1]))\nfor x,y in positions:\n    print(x,y)\n    c = curated_claims[y]\n    print(c)\n    s = sentences[x]\n    print(s)\n    print()\n    #break","bbfac997":"\n########################################\nsimilarity_threshold = 0.75\n########################################\n\nTOP_N_SIM = 100\nR = cos_sim_array.shape[1]\nprint(R, \"%.4f\"%similarity_threshold, TOP_N_SIM)\nprint()\n\nCLAIM_CANDIDATES = dict()\nfor i in range(R):\n    cos_sim_vec = cos_sim_array[:,i]\n    #val = np.amax(cos_sim_vec)\n    #pos = np.where(cos_sim_vec==val)[0][0]\n    #print(pos, \"%.4f\"%val)\n    c = curated_claims[i]\n    print(c)\n    VALS = sorted(cos_sim_vec.tolist())[::-1][:TOP_N_SIM]\n    VALS = [val for val in VALS if val>=similarity_threshold]\n    print(len(VALS))\n    for val in VALS:\n        pos = np.where(cos_sim_vec==val)[0][0]\n        s = sentences[pos]\n        print(\"%.4f\"%val, s)\n        try:\n            CLAIM_CANDIDATES[c].append([val, s])\n        except:\n            CLAIM_CANDIDATES[c] = [[val, s]]\n    print()\n    #break\nlen(CLAIM_CANDIDATES), len(CLAIM_CANDIDATES.values())","fb82972e":"CLAIM_CANDIDATES_SENTENCES = []\nfor c,X in CLAIM_CANDIDATES.items():\n    for val,s in X:\n        CLAIM_CANDIDATES_SENTENCES.append(s)\nprint(len(CLAIM_CANDIDATES_SENTENCES))\nCLAIM_CANDIDATES_SENTENCES = list(set(CLAIM_CANDIDATES_SENTENCES))\nprint(len(CLAIM_CANDIDATES_SENTENCES))","c6edf133":"CLAIM_CANDIDATES_SENTENCES_df = pd.DataFrame(CLAIM_CANDIDATES_SENTENCES, columns=['claims'])\nlen(CLAIM_CANDIDATES_SENTENCES_df)","856e476d":"CLAIM_CANDIDATES_SENTENCES_df.to_csv('CLAIM_CANDIDATES_SENTENCES_Subreddit_Coronavirus_2020-04-03T000000_2020-04-04T000000.tsv', sep='\\t', index=False, index_label=False)","44d55cf8":"NO_CLAIM_CANDIDATES_SENTENCES = []\nfor s in sentences:\n    if not s in CLAIM_CANDIDATES_SENTENCES:\n        NO_CLAIM_CANDIDATES_SENTENCES.append(s)\nprint(len(NO_CLAIM_CANDIDATES_SENTENCES))\nNO_CLAIM_CANDIDATES_SENTENCES = list(set(NO_CLAIM_CANDIDATES_SENTENCES))\nprint(len(NO_CLAIM_CANDIDATES_SENTENCES))\nlen(sentences), len(CLAIM_CANDIDATES_SENTENCES), len(NO_CLAIM_CANDIDATES_SENTENCES), len(CLAIM_CANDIDATES_SENTENCES)+len(NO_CLAIM_CANDIDATES_SENTENCES), len(sentences)==len(CLAIM_CANDIDATES_SENTENCES)+len(NO_CLAIM_CANDIDATES_SENTENCES)","5814f20e":"NO_CLAIM_CANDIDATES_SENTENCES_df = pd.DataFrame(NO_CLAIM_CANDIDATES_SENTENCES, columns=['sentences'])\nlen(NO_CLAIM_CANDIDATES_SENTENCES_df)","be243c62":"NO_CLAIM_CANDIDATES_SENTENCES_df.to_csv('NO_CLAIM_CANDIDATES_SENTENCES_Subreddit_Coronavirus_2020-04-03T000000_2020-04-04T000000.tsv', sep='\\t', index=False, index_label=False)","05200c04":"MY_CLAIMS = list(set([c for c in CLAIM_CANDIDATES_SENTENCES_df.claims.values.tolist()]))\nlen(MY_CLAIMS)","e89694cc":"MY_CLAIMS[:3]","2ad65969":"MY_NO_CLAIMS = list(set([c for c in NO_CLAIM_CANDIDATES_SENTENCES_df.sentences.values.tolist()]))\nlen(MY_NO_CLAIMS)","2f95fe71":"MY_NO_CLAIMS[:3]","640e8333":"X, Y = [], []\nfor c in MY_CLAIMS:\n    X.append(c)\n    Y.append('claim')\nfor s in MY_NO_CLAIMS:\n    X.append(s)\n    Y.append('no_claim')\nlen(X), len(Y)","8d2bd492":"from sentence_transformers import SentenceTransformer","0521aecd":"model = SentenceTransformer('bert-base-nli-mean-tokens')","3e21b744":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support, classification_report","2d63858f":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, shuffle=True)\nlen(X_train), len(X_test), len(Y_train), len(Y_test)","e2c6f5e7":"from collections import Counter\nprint(len(Y_train), len(Y_test))\nC_train = Counter(Y_train)\nprint(C_train)\nC_test = Counter(Y_test)\nprint(C_test)\nprint(\"%.2f\"%(100*C_train['claim'] \/ sum(list(C_train.values()))), \"%\", \"claims in train set!\")\nprint(\"%.2f\"%(100*C_test['claim'] \/ sum(list(C_test.values()))), \"%\", \"claims in test set!\")","d6c5ce78":"X_train_embedds = model.encode(X_train)\nlen(X_train_embedds)","0d208c7b":"X_test_embedds = model.encode(X_test)\nlen(X_test_embedds)","cc16f257":"from sklearn.svm import SVC","e985f0a9":"svc = SVC(kernel='rbf', gamma='auto')","aea1636a":"print(dt.datetime.now())\nY_pred_svm = svc.fit(X_train_embedds, Y_train).predict(X_test_embedds)\nprint(dt.datetime.now())\nlen(Y_pred_svm), Counter(Y_pred_svm.tolist())","ada09e36":"p,r,f1,s = precision_recall_fscore_support(y_true=Y_test, y_pred=Y_pred_svm, average='macro', warn_for=tuple())\nprint(\"%.3f Precision\\n%.3f Recall\\n%.3f F1\"%(p,r,f1))","e9f0ead6":"print(classification_report(y_true=Y_test, y_pred=Y_pred_svm))","6c394b22":"count_i = 0\nfor i, y_pred in enumerate(Y_pred_svm):\n    if y_pred=='claim':\n        print(X_test[i])\n        print()\n        if count_i>=10:\n            break\n        count_i+=1","fdcbb224":"from sklearn.naive_bayes import GaussianNB","541cfc22":"gnb = GaussianNB()","8db001c1":"print(dt.datetime.now())\nY_pred_gnb = gnb.fit(X_train_embedds, Y_train).predict(X_test_embedds)\nprint(dt.datetime.now())\nlen(Y_pred_gnb), Counter(Y_pred_gnb.tolist())","6d88d6a5":"p,r,f1,s = precision_recall_fscore_support(y_true=Y_test, y_pred=Y_pred_gnb, average='macro', warn_for=tuple())\nprint(\"%.3f Precision\\n%.3f Recall\\n%.3f F1\"%(p,r,f1))","bf0f3a1c":"print(classification_report(y_true=Y_test, y_pred=Y_pred_gnb))","3201835c":"count_i = 0\nfor i, y_pred in enumerate(Y_pred_gnb):\n    if y_pred=='claim':\n        print(X_test[i])\n        print()\n        if count_i>=10:\n            break\n        count_i+=1","18ba3ad1":"from sklearn.ensemble import RandomForestClassifier","f7bf5fb9":"rfc = RandomForestClassifier(n_estimators=10)","10450227":"print(dt.datetime.now())\nY_pred_rfc = rfc.fit(X_train_embedds, Y_train).predict(X_test_embedds)\nprint(dt.datetime.now())\nlen(Y_pred_rfc), Counter(Y_pred_rfc.tolist())","f82be326":"p,r,f1,s = precision_recall_fscore_support(y_true=Y_test, y_pred=Y_pred_rfc, average='macro', warn_for=tuple())\nprint(\"%.3f Precision\\n%.3f Recall\\n%.3f F1\"%(p,r,f1))","47311e2e":"print(classification_report(y_true=Y_test, y_pred=Y_pred_rfc))","79d74cc1":"count_i = 0\nfor i, y_pred in enumerate(Y_pred_rfc):\n    if y_pred=='claim':\n        print(X_test[i])\n        print()\n        if count_i>=10:\n            break\n        count_i+=1","9f68a804":"### 4. Claim Candidates (curated)","6d38cfc5":"    List of Known Misinformation and Disinformation Regarding Corona Virus in Social Media\n    IDeaS Center and CASOS Center \u2013 Under Direction of Dr. Kathleen M. Carley\n    Last updated 3\/27\/2020\n    Stories containing inaccurate information regarding COVID 19 fall into at least three categories:\n\n    Stories relating inaccurate information about cures or preventative measures or treatments that make it worse\n    Stories relating inaccurate information about the nature of the virus\n    Stories relating inaccurate information that are conspiracy stories\n    Stories relating false diagnosis procedures\n    Stories relating inaccurate information about emergency responses\n    Stories relating inaccurate but funny or feel good stories\n    We note that there are also stories containing inaccurate information with respect to the number of deaths, politicians\u2019 response, the original source, the first reported cases, and other government activities.  Such stories have not been tracked.\n\n    Following is a list of stories of each type.","385c104f":"---","d2c0a219":"### 2. Download News Articles from List of URLs","981b0ab4":"##### Random Forest","844f7a93":"### 6. Classify Claims with ML Models","294fb13c":"##### Support Vector Machines","7892c6f9":"---","112e19c7":"---","cc3abc2d":"    ~1h for 2150 articles, resulting in ~75k sentences","1f4a7c33":"---","879991cb":"# Project COVIEWED\n\nIn this Notebook, we go through the steps to create the coronavirus claims candidate corpus.\n\nDue to runtime contrainst, we run in this notebook the claim candidate selections from news articles submitted for one day.","f49e21a2":"---","2deeb751":"---","c51c839c":"List based on https:\/\/www.cmu.edu\/ideas-social-cybersecurity\/research\/coronavirus.html","136c3624":"##### Naive Bayes","93054ea8":"### 5. Compute Claim Similarity","87d99b39":"### 1. Load List of URLs","80d57146":"---","c2deeed3":"---","a36a6a57":"---","051424dd":"### 3. Segmentation of all Sentence in the Articles ","e1258c2b":"    filter by keywords: keywords = ['corona', 'covid', 'it', 'sars-cov-2', 'virus']","4e99bda9":"    ~1h for 3k urls"}}