{"cell_type":{"4f667ffa":"code","739deb8a":"code","55451593":"code","225debec":"code","bb96aa01":"code","c7a86085":"code","5023eb82":"code","2dd97d38":"code","bbea0a5c":"code","c55dac13":"code","f59262ca":"code","00c5b574":"code","b2bfaa67":"code","995f10b9":"code","dc06d0b3":"code","0fd97192":"code","332c2633":"code","88b35afb":"code","1280ba85":"code","b4444f08":"code","6d942b7c":"code","8cddee35":"code","b7c4414f":"code","55d30992":"code","93fe3e42":"code","58716195":"code","764cac1c":"code","7fa2944e":"code","109164ac":"code","fe36efe4":"code","ba33c8e0":"code","68f415b6":"code","61e14b1f":"code","2ad4d414":"code","e59573a0":"code","b77ea37d":"code","0b2b82c8":"code","3e7fed4a":"code","f84fc9ca":"code","2991810f":"code","8ae06f6c":"code","4948e563":"code","833e6c83":"code","d417ca64":"code","33db73e6":"code","42563b86":"code","12f666ae":"code","6a8e0248":"code","20eae143":"code","0410639c":"code","7a2273e2":"code","94fc018f":"code","2a405f60":"code","2b2dcb83":"code","4e2fe64a":"code","57b34731":"code","32dca083":"code","76898b45":"code","969b28c8":"code","4addaf2b":"code","8b6b7a42":"code","937a81a1":"code","d5ad1f9d":"code","d5d0222a":"code","f8f01304":"code","29adf664":"code","287d1936":"code","42ae4d26":"code","0e21d47a":"code","918b8699":"code","24ad002b":"code","01811b58":"code","993b1a95":"code","310260a8":"code","b4240b54":"code","48b979d8":"code","675f756d":"code","54c228ce":"markdown","55e6ca90":"markdown","622a99f2":"markdown","031b6239":"markdown","41f6656d":"markdown","498577c1":"markdown","f8c3db0b":"markdown","24362a79":"markdown","7166dcb2":"markdown","ad300c7c":"markdown","4bb42042":"markdown","416381c1":"markdown","537230dc":"markdown","6536d4e9":"markdown","da0cc5ab":"markdown","32a7b39c":"markdown","ca1b8147":"markdown","c01fedfa":"markdown","8c8c4376":"markdown","c9cf6624":"markdown","91d5351f":"markdown","f9724423":"markdown","884e8b31":"markdown","d6d2bfd3":"markdown","b4bfcc3b":"markdown","125c5d45":"markdown","8f728bce":"markdown","24ac9fed":"markdown","4dcc7d42":"markdown","05427302":"markdown","b1cf1326":"markdown","ba847c67":"markdown","da6aaa9c":"markdown","8b486d07":"markdown","7ff9cc85":"markdown","6f19b834":"markdown","c9f1196b":"markdown","0783f997":"markdown","5968a72c":"markdown","d7708463":"markdown","d1b02982":"markdown","48d4fdd8":"markdown","65222592":"markdown","4b155516":"markdown","7b277d24":"markdown","1c67bf6a":"markdown","c7ac1c49":"markdown","68a8de4c":"markdown","42293166":"markdown","6c189442":"markdown"},"source":{"4f667ffa":"##########################Load Libraries  ####################################\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn import preprocessing, metrics\nfrom ipywidgets import widgets, interactive\nimport gc\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom datetime import datetime, timedelta \nfrom typing import Union\nfrom tqdm.notebook import tqdm_notebook as tqdm\nfrom itertools import cycle\nimport datetime as dt\nfrom torch.autograd import Variable\nimport random \nimport os\nfrom matplotlib.pyplot import figure\nfrom fastprogress import master_bar, progress_bar\nimport torch\nimport torch.nn as nn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nimport time \nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom sklearn.metrics import mean_squared_error\nimport torch \nfrom sklearn.model_selection import TimeSeriesSplit\n\nfrom math import log\nfrom math import exp\nfrom scipy.stats import boxcox\n\n \nimport torch.optim as optim\n\n%matplotlib inline\n\n#from gensim.models import Word2Vec\n#import gensim.downloader as api\n\npd.set_option('max_columns', 50)\nplt.style.use('bmh')\ncolor_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\ncolor_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n\n ","739deb8a":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"device is:\",device)","55451593":"INPUT_DIR_PATH = '..\/input\/m5-forecasting-accuracy\/'","225debec":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics: \n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\n\ndef read_data():\n    sell_prices_df = pd.read_csv(INPUT_DIR_PATH + 'sell_prices.csv')\n    sell_prices_df = reduce_mem_usage(sell_prices_df)\n    print('Sell prices has {} rows and {} columns'.format(sell_prices_df.shape[0], sell_prices_df.shape[1]))\n\n    calendar_df = pd.read_csv(INPUT_DIR_PATH + 'calendar.csv')\n    calendar_df = reduce_mem_usage(calendar_df)\n    print('Calendar has {} rows and {} columns'.format(calendar_df.shape[0], calendar_df.shape[1]))\n\n    sales_train_validation_df = pd.read_csv(INPUT_DIR_PATH + 'sales_train_validation.csv')\n    print('Sales train validation has {} rows and {} columns'.format(sales_train_validation_df.shape[0], sales_train_validation_df.shape[1]))\n\n    submission_df = pd.read_csv(INPUT_DIR_PATH + 'sample_submission.csv')\n    return sell_prices_df, calendar_df, sales_train_validation_df, submission_df","bb96aa01":"_,  calendar_df, sales_train_validation_df, _ = read_data()","c7a86085":"#Create date index\ndate_index = calendar_df['date']\ndates = date_index[0:1913]\ndates_list = [dt.datetime.strptime(date, '%Y-%m-%d').date() for date in dates]","5023eb82":"# Create a data frame for items sales per day with item ids (with Store Id) as columns names  and dates as the index \nsales_train_validation_df['item_store_id'] = sales_train_validation_df.apply(lambda x: x['item_id']+'_'+x['store_id'],axis=1)\nDF_Sales = sales_train_validation_df.loc[:,'d_1':'d_1913'].T\nDF_Sales.columns = sales_train_validation_df['item_store_id'].values\n\n#Set Dates as index \nDF_Sales = pd.DataFrame(DF_Sales).set_index([dates_list])\nDF_Sales.index = pd.to_datetime(DF_Sales.index)\n\nDF_Sales.head()","2dd97d38":"#Select arbitrary index and plot the time series\nindex = 321\ny = pd.DataFrame(DF_Sales.iloc[:,index])\nTS_selected = y \ny = pd.DataFrame(y).set_index([dates_list])\ny.index = pd.to_datetime(y.index)\nax = y.plot(figsize=(30, 9),color='black')\nax.set_facecolor('lightgrey')\nplt.xticks(fontsize=21 )\nplt.yticks(fontsize=21 )\nplt.legend(fontsize=20)\nplt.title(label = 'Sales Demand Selected Time Series Over Time',fontsize = 23)\nplt.ylabel(ylabel = 'Sales Demand',fontsize = 21)\nplt.xlabel(xlabel = 'Date',fontsize = 21)\nplt.show()\n","bbea0a5c":"SEED = 42\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\nseed_everything(SEED)","c55dac13":"train_size = int((len(TS_selected)-28) * 0.67)\nvalid_size = (len(TS_selected)-28)- train_size\nprint(\"train size is:\",train_size)\nprint(\"validation size is:\",valid_size)","f59262ca":"train_data = TS_selected.iloc[0:train_size,:]\nvalid_data = TS_selected.iloc[train_size:train_size+valid_size,:]\nprint(\"train data shape is:\",train_data.shape)\nprint(\"validation data shape is:\",valid_data.shape)\n","00c5b574":"train_data.head()","b2bfaa67":"## Simple Difference transform  --> X(t)' = X(t) - X(t-7)\ndef difference(data, interval):\n    return [data[i] - data[i - interval] for i in range(interval, len(data))]\n\nnp_tarin_data = train_data.iloc[:,0]\ninterval = 7 \ntransformed_train = difference(np_tarin_data, interval)\n\nnp_valid_data = valid_data.iloc[:,0]\ninterval = 7 \ntransformed_valid = difference(np_valid_data, interval)\n\ntrain_data = np.array(transformed_train).reshape(len(transformed_train),1)\nvalid_data = np.array(transformed_valid).reshape(len(transformed_valid),1)","995f10b9":"train_data.shape","dc06d0b3":"valid_data.shape","0fd97192":"scaler = MinMaxScaler((-1,1)).fit(train_data)\n#scaler = StandardScaler().fit(train_data)\ntrain_data_std = scaler.transform(train_data)\nvalid_data_std = scaler.transform(valid_data)","332c2633":"fig, axs = plt.subplots(2)\n \nfig.suptitle('Data Distribution Before and After Data Transformation   ',fontsize = 19)\npd.DataFrame(train_data).plot(kind='hist',ax = axs[0] , alpha=.4 , figsize=[12,6], legend = False,title = ' Before Transformation',color ='red') \npd.DataFrame(train_data_std).plot(kind='hist', ax = axs[1] ,figsize=[12,6], alpha=.4 , legend = False,title = ' After Transformation'\\\n                                         ,color = 'blue')\n   ","88b35afb":"###  This function creates a sliding window or sequences of seq_length days and labels_length  days label ####\ndef sliding_windows(data, seq_length,labels_length):\n    x = []\n    y = []\n\n    for i in range(len(data)-(seq_length+labels_length)):\n        _x = data[i:(i+seq_length)]\n        _y = data[(i+seq_length):(i+seq_length+labels_length)]\n        x.append(_x)\n        y.append(_y)\n\n    return x,y","1280ba85":"seq_length = 90\nlabels_length =28\ntrain_X, train_y = sliding_windows(train_data_std, seq_length,labels_length)\nprint(\"train X  has:\", len(train_X) , \"series\")\nprint(\"train labels  has:\", len(train_y) , \"series\")\nvalid_X, valid_y = sliding_windows(valid_data_std, seq_length,labels_length)\nprint(\"validiation  X  has:\", len(valid_X) , \"series\")\nprint(\"Validiation  labels  has:\" ,len(valid_y) , \"series\")","b4444f08":"plt.figure(figsize=(15,17))\nfig, axs =plt.subplots(3,figsize=(12,9))\n\naxs[0].plot(train_data_std[0:seq_length+labels_length])\naxs[0].title.set_text('Original Time Series')\naxs[0].set_xlim(0,seq_length+labels_length)\naxs[0].set_ylim(-1,1)\naxs[1].plot(train_X[0].flatten(),color=\"red\")\naxs[1].title.set_text('Train Data')\naxs[1].set_xlim(0,seq_length+labels_length)\naxs[1].set_ylim(-1,1)\naxs[2].plot(np.pad(train_y[0].flatten(),seq_length),color='black')\naxs[2].title.set_text('Labels Data')\naxs[2].set_xlim(0,seq_length+labels_length)\naxs[2].set_ylim(-1,1)\n","6d942b7c":"\ntrainX = Variable(torch.Tensor(train_X))\ntrainy = Variable(torch.Tensor(train_y))\n\nvalidX = Variable(torch.Tensor(valid_X))\nvalidy= Variable(torch.Tensor(valid_y))\n\nprint (\"trainX shape is:\",trainX.size())\nprint (\"trainy shape is:\",trainy.size())\nprint (\"validX shape is:\",validX.size())\nprint (\"validy shape is:\",validy.size())\n","8cddee35":"class Encoder(nn.Module):\n    def __init__(self, seq_len, n_features, embedding_dim=64):\n        super(Encoder, self).__init__()\n\n        self.seq_len, self.n_features = seq_len, n_features\n        self.embedding_dim, self.hidden_dim = embedding_dim,  embedding_dim\n        self.num_layers = 3\n        self.rnn1 = nn.LSTM(\n          input_size=n_features,\n          hidden_size=self.hidden_dim,\n          num_layers=3,\n          batch_first=True,\n          dropout = 0.35\n        )\n   \n    def forward(self, x):\n       \n        x = x.reshape((1, self.seq_len, self.n_features))\n        \n        h_1 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_dim).to(device))\n         \n        \n        c_1 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_dim).to(device))\n              \n        x, (hidden, cell) = self.rnn1(x,(h_1, c_1))\n        \n        \n        #return hidden_n.reshape((self.n_features, self.embedding_dim))\n        return x, hidden , cell ","b7c4414f":"class Attention(nn.Module):\n    def __init__(self, enc_hid_dim, dec_hid_dim):\n        super().__init__()\n        \n        self.attn = nn.Linear((enc_hid_dim ) + dec_hid_dim, dec_hid_dim)\n        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n        \n    def forward(self, hidden, encoder_outputs):\n        \n        #hidden = [batch size, dec hid dim]\n        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n        \n        batch_size = encoder_outputs.shape[0]\n        src_len = encoder_outputs.shape[1]\n        \n       \n        hidden = hidden[2:3,:,:]\n        \n        #print(\"hidden size is\",hidden.size())\n        \n        \n        \n        #repeat decoder hidden state src_len times\n        #hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n        hidden = hidden.repeat(1, src_len, 1)\n     \n        \n        #encoder_outputs = encoder_outputs.permute(1, 0, 2)\n        \n        #print(\"encode_outputs size after permute is:\",encoder_outputs.size())\n        \n        \n        #hidden = [batch size, src len, dec hid dim]\n        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n        \n        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n        \n        #energy = [batch size, src len, dec hid dim]\n\n        attention = self.v(energy).squeeze(2)\n        \n        #attention= [batch size, src len]\n        \n        \n        return F.softmax(attention, dim=1)","55d30992":"class Decoder(nn.Module):\n    def __init__(self, seq_len, input_dim=64, n_features=1):\n        super(Decoder, self).__init__()\n\n        self.seq_len, self.input_dim = seq_len, input_dim\n        self.hidden_dim, self.n_features =  input_dim, n_features\n        \n        self.rnn1 = nn.LSTM(\n          input_size=1,\n          hidden_size=input_dim,\n          num_layers=3,\n          batch_first=True,\n          dropout = 0.35\n        )\n        \n        \n      \n        self.output_layer = nn.Linear(self.hidden_dim, n_features)\n\n    def forward(self, x,input_hidden,input_cell):\n       \n       \n        x = x.reshape((1,1,1))\n        \n        \n     \n\n        x, (hidden_n, cell_n) = self.rnn1(x,(input_hidden,input_cell))\n    \n        x = self.output_layer(x)\n        return x, hidden_n, cell_n","93fe3e42":"class AttentionDecoder(nn.Module):\n    def __init__(self, seq_len,attention, input_dim=64, n_features=1,encoder_hidden_state = 512):\n        super(AttentionDecoder, self).__init__()\n\n        self.seq_len, self.input_dim = seq_len, input_dim\n        self.hidden_dim, self.n_features =  input_dim, n_features\n        self.attention = attention \n        \n        self.rnn1 = nn.LSTM(\n          #input_size=1,\n          input_size= encoder_hidden_state + 1,  # Encoder Hidden State + One Previous input\n          hidden_size=input_dim,\n          num_layers=3,\n          batch_first=True,\n          dropout = 0.35\n        )\n        \n        \n      \n        self.output_layer = nn.Linear(self.hidden_dim * 2 , n_features)\n\n    def forward(self, x,input_hidden,input_cell,encoder_outputs):\n       \n        a = self.attention(input_hidden, encoder_outputs)\n        \n        a = a.unsqueeze(1)\n        \n        #a = [batch size, 1, src len]\n        \n        #encoder_outputs = encoder_outputs.permute(1, 0, 2)\n        \n        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n        \n      \n        \n        weighted = torch.bmm(a, encoder_outputs)\n        \n        \n     \n        x = x.reshape((1,1,1))\n       \n        \n        \n        rnn_input = torch.cat((x, weighted), dim = 2)\n       \n\n        #x, (hidden_n, cell_n) = self.rnn1(x,(input_hidden,input_cell))\n        x, (hidden_n, cell_n) = self.rnn1(rnn_input,(input_hidden,input_cell))\n        \n        output = x.squeeze(0)\n        weighted = weighted.squeeze(0)\n        \n        x = self.output_layer(torch.cat((output, weighted), dim = 1))\n        return x, hidden_n, cell_n\n    \n      ","58716195":"class Seq2Seq(nn.Module):\n\n    def __init__(self, seq_len, n_features, embedding_dim=64,output_length = 28):\n        super(Seq2Seq, self).__init__()\n\n        \n        self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device)\n        self.attention = Attention(512,512)\n        self.output_length = output_length\n        self.decoder = AttentionDecoder(seq_len, self.attention, embedding_dim, n_features).to(device)\n        \n\n    def forward(self,x, prev_y):\n        \n        encoder_output,hidden,cell = self.encoder(x)\n         \n        #Prepare place holder for decoder output\n        targets_ta = []\n        #prev_output become the next input to the LSTM cell\n        prev_output = prev_y\n        \n        #itearate over LSTM - according to the required output days\n        for out_days in range(self.output_length) :\n        \n            prev_x,prev_hidden,prev_cell = self.decoder(prev_output,hidden,cell,encoder_output)\n            hidden,cell = prev_hidden,prev_cell\n            prev_output = prev_x\n            \n            targets_ta.append(prev_x.reshape(1))\n           \n            \n        \n        \n        targets = torch.stack(targets_ta)\n\n        return targets","764cac1c":"n_features = 1\nmodel = Seq2Seq(seq_length, n_features, 512)\nmodel = model.to(device)","7fa2944e":"model","109164ac":"def init_weights(m):\n    for name, param in m.named_parameters():\n        nn.init.uniform_(param.data, -0.08, 0.08)\n        \nmodel.apply(init_weights)","fe36efe4":"optimizer = torch.optim.Adam(model.parameters(), lr=4e-3,weight_decay=1e-5)\ncriterion = torch.nn.MSELoss().to(device) \nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 5e-3, eta_min=1e-8, last_epoch=-1)\n#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  patience=10, factor =0.5 ,min_lr=1e-7, eps=1e-08)","ba33c8e0":"def train_model(model, TrainX,Trainy,ValidX,Validy,seq_length, n_epochs):\n  \n    history = dict(train=[], val=[])\n\n    #best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = 10000.0\n    mb = master_bar(range(1, n_epochs + 1))\n\n    for epoch in mb:\n        model = model.train()\n\n        train_losses = []\n        for i in progress_bar(range(TrainX.size()[0]),parent=mb):\n            seq_inp = TrainX[i,:,:].to(device)\n            seq_true = Trainy[i,:,:].to(device)\n           \n            optimizer.zero_grad()\n\n            \n            seq_pred = model(seq_inp,seq_inp[seq_length-1:seq_length,:])\n            \n            \n            loss = criterion(seq_pred, seq_true)\n\n            loss.backward()\n            \n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n            optimizer.step()\n\n            train_losses.append(loss.item())\n\n        val_losses = []\n        model = model.eval()\n        with torch.no_grad():\n            for i in progress_bar(range(validX.size()[0]),parent=mb):\n                seq_inp = ValidX[i,:,:].to(device)\n                seq_true = Validy[i,:,:].to(device)\n        \n                seq_pred = model(seq_inp,seq_inp[seq_length-1:seq_length,:])\n               \n\n                loss = criterion(seq_pred, seq_true)\n                val_losses.append(loss.item())\n\n        train_loss = np.mean(train_losses)\n        val_loss = np.mean(val_losses)\n\n        history['train'].append(train_loss)\n        history['val'].append(val_loss)\n\n        if val_loss < best_loss:\n            best_loss = val_loss\n            torch.save(model.state_dict(), 'best_model.pt')\n            print(\"saved best model epoch:\",epoch,\"val loss is:\",val_loss)\n        \n        print(f'Epoch {epoch}: train loss {train_loss} val loss {val_loss}')\n        scheduler.step()\n    #model.load_state_dict(best_model_wts)\n    return model.eval(), history","68f415b6":"model, history = train_model(\n  model,\n  trainX,trainy,\n  validX,validy,\n  seq_length,\n  n_epochs=30, ## Training only on 30 epochs to save GPU time \n    \n)","61e14b1f":"TestX = np.array(TS_selected.iloc[-118:-28:,:])\nTesty = np.array(TS_selected.iloc[-28:,:])\nTestX = Variable(torch.Tensor(TestX))\n ","2ad4d414":"TestX_Diff = difference(TestX, interval)\nTestX_Diff_Norm = scaler.transform(np.array(TestX_Diff).reshape(len(TestX_Diff),1))","e59573a0":"######Prediction###############\nmodel.load_state_dict(torch.load('best_model.pt'))\nmodel.eval()\n\nwith torch.no_grad():\n    seq_inp = TestX.to(device)\n    \n        \n    seq_pred = model(seq_inp,seq_inp[seq_length-1:seq_length,:])\n          \n","b77ea37d":"data_predict = scaler.inverse_transform(seq_pred.cpu().numpy())\nlabels = Testy","0b2b82c8":"orignal_data = np.array(TS_selected.iloc[-118:-28:,:]).flatten()\npred = data_predict.flatten()\norig_pred = np.concatenate([orignal_data,pred])\n","3e7fed4a":"time_diff = 7 \ninv_pred = np.zeros((28))\nfor  index in range(28):\n    inv_pred[index] = orig_pred[90+index]+ orig_pred[90+index-7]\n    orig_pred[90+index] = inv_pred[index]\n","f84fc9ca":"data_predict= np.array(inv_pred)\ndata_predict = np.where(data_predict<0,0,data_predict)\n","2991810f":"## Add dates\ndf_predict = pd.DataFrame(data_predict)\ndf_predict = df_predict.set_index([dates_list[-28:]])\ndf_labels = pd.DataFrame(labels)\ndf_labels = df_labels.set_index([dates_list[-28:]])\n\n# Plot \nfigure(num=None, figsize=(24, 6), dpi=80, facecolor='w', edgecolor='k')\n#plt.axvline(x=dates_list[train_size], c='r')\nplt.plot( df_labels[0])\nplt.plot(df_predict[0])\nplt.legend(['Time Series','Prediction'],fontsize = 21)\nplt.suptitle('Time-Series Prediction Last 28 Days',fontsize = 23)\nplt.xticks(fontsize=21 )\nplt.yticks(fontsize=21 )\nplt.ylabel(ylabel = 'Sales Demand',fontsize = 21)\nplt.xlabel(xlabel = 'Date',fontsize = 21)\nplt.show()\n","8ae06f6c":"np.sqrt(((data_predict - labels) ** 2).mean())","4948e563":"DF = TS_selected\ncolnames = DF.columns\nDF = DF.rename(columns={colnames[0]:'sales'})\nDF.tail()","833e6c83":"## Simple Difference transform  --> X(t)' = X(t) - X(t-7)\n\ndef difference(data, interval):\n    return [data[i] - data[i - interval] for i in range(interval, len(data))]\n\ntransformed_data = difference(DF['sales'], interval)\nDF = DF.iloc[7:,:]\nDF['orig_sales'] = DF['sales']\nDF['sales'] = transformed_data\nDF.shape","d417ca64":"start_time = time.time()\nfor i in (1,7,14,28,365):\n    print('Shifting:', i)\n    DF['lag_'+str(i)] = DF['sales'].transform(lambda x: x.shift(i))\nprint('%0.2f min: Time for bulk shift' % ((time.time() - start_time) \/ 60))\n","33db73e6":"for i in [7,14,28,60,180,365]:\n    print('Rolling period:', i)\n    DF['rolling_mean_'+str(i)] = DF['orig_sales'].transform(lambda x: x.shift(28).rolling(i).mean())\n    DF['rolling_std_'+str(i)]  = DF['orig_sales'].transform(lambda x: x.shift(28).rolling(i).std())\n\n\nprint('%0.2f min: Time for loop' % ((time.time() - start_time) \/ 60))\nDF.head()","42563b86":"DF = DF.replace('nan', np.nan).fillna(0)\nDF.head()","12f666ae":"DF = DF.drop('orig_sales',1)\nDF_normlized = DF.copy(deep=True)\nscaler = MinMaxScaler(feature_range=(-1, 1))\ny_scaler = MinMaxScaler(feature_range=(-1, 1))\nscaled_data = scaler.fit_transform(DF) \ny_scaler.fit_transform(DF['sales'].values.reshape(-1, 1))\nDF_normlized.iloc[:,:] =  scaled_data\n   \n\nDF_normlized.head()","6a8e0248":"train_size = int((DF_normlized.shape[0]-28) * 0.67)\nvalid_size = (DF_normlized.shape[0]-28)- train_size\nprint(\"train size is:\",train_size)\nprint(\"validation size is:\",valid_size)\ntrain_data = DF_normlized.iloc[0:train_size,:]\nvalid_data = DF_normlized.iloc[train_size:train_size+valid_size,:]\nprint(\"train data shape is:\",train_data.shape)\nprint(\"validation data shape is:\",valid_data.shape)\n","20eae143":"###  This function creates a sliding window or sequences of seq_length days and labels_length  days label ####\ndef sliding_windows(data, seq_length,labels_length):\n    x = []\n    y = []\n    z = []\n\n    for i in range(len(data)-(seq_length+labels_length)):\n        _x = data.iloc[i:(i+seq_length),:]\n        _y = data.iloc[(i+seq_length):(i+seq_length+labels_length),0:1]\n        _z  = data.iloc[(i+seq_length):(i+seq_length+labels_length),1:]\n        x.append(np.array(_x))\n        y.append(np.array(_y))\n        z.append(np.array(_z))\n\n    return x,y,z","0410639c":"seq_length = 90\nlabels_length =28\ntrain_X, train_y,train_features = sliding_windows(train_data, seq_length,labels_length)\nprint(\"train X  has:\", len(train_X) , \"series\")\nprint(\"train labels  has:\", len(train_y) , \"series\")\nvalid_X, valid_y,valid_features = sliding_windows(valid_data, seq_length,labels_length)\nprint(\"validiation  X  has:\", len(valid_X) , \"series\")\nprint(\"Validiation  labels  has:\" ,len(valid_y) , \"series\")","7a2273e2":"train_X[0].shape","94fc018f":"train_y[0].shape","2a405f60":"train_features[0].shape","2b2dcb83":"trainX = Variable(torch.Tensor(train_X))\ntrainy = Variable(torch.Tensor(train_y))\ntrain_features = Variable(torch.Tensor(train_features))\nvalidX = Variable(torch.Tensor(valid_X))\nvalidy= Variable(torch.Tensor(valid_y))\nvalid_features = Variable(torch.Tensor(valid_features))\n\n\nprint (\"trainX shape is:\",trainX.size())\nprint (\"trainy shape is:\",trainy.size())\nprint (\"train features  shape is:\",train_features.size())\nprint (\"validX shape is:\",validX.size())\nprint (\"validy shape is:\",validy.size())\nprint (\"valid features  shape is:\",valid_features.size())\n","4e2fe64a":"class Encoder(nn.Module):\n    def __init__(self, seq_len, n_features, embedding_dim=64):\n        super(Encoder, self).__init__()\n\n        self.seq_len, self.n_features = seq_len, n_features\n        self.embedding_dim, self.hidden_dim = embedding_dim,  embedding_dim\n        self.num_layers = 3\n        self.rnn1 = nn.LSTM(\n          input_size=n_features,\n          hidden_size=self.hidden_dim,\n          num_layers=3,\n          batch_first=True,\n          dropout = 0.35\n        )\n   \n    def forward(self, x):\n       \n        x = x.reshape((1, self.seq_len, self.n_features))\n        \n        h_1 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_dim).to(device))\n         \n        \n        c_1 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_dim).to(device))\n              \n        x, (hidden, cell) = self.rnn1(x,(h_1, c_1))\n        \n        \n        #return hidden_n.reshape((self.n_features, self.embedding_dim))\n        return hidden , cell ","57b34731":"class Decoder(nn.Module):\n    def __init__(self, seq_len, input_dim=64, n_features=1):\n        super(Decoder, self).__init__()\n\n        self.seq_len, self.input_dim = seq_len, input_dim\n        self.hidden_dim, self.n_features =  input_dim, n_features\n        \n        self.rnn1 = nn.LSTM(\n          input_size=n_features,\n          hidden_size=input_dim,\n          num_layers=3,\n          batch_first=True,\n          dropout = 0.35\n        )\n        \n        \n      \n        self.output_layer = nn.Linear(self.hidden_dim, n_features)\n\n    def forward(self, x,input_hidden,input_cell):\n       \n       \n        x = x.reshape((1,1,self.n_features ))\n        #print(\"decode input\",x.size())\n             \n\n        x, (hidden_n, cell_n) = self.rnn1(x,(input_hidden,input_cell))\n    \n        x = self.output_layer(x)\n        return x, hidden_n, cell_n","32dca083":"class Seq2Seq(nn.Module):\n\n    def __init__(self, seq_len, n_features, embedding_dim=64,output_length = 28):\n        super(Seq2Seq, self).__init__()\n\n        self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device)\n        self.n_features = n_features\n        self.output_length = output_length\n        self.decoder = Decoder(seq_len, embedding_dim, n_features).to(device)\n        \n\n    def forward(self,x, prev_y,features):\n        \n       \n        hidden,cell = self.encoder(x)\n         \n        #Prepare place holder for decoder output\n        targets_ta = []\n        #prev_output become the next input to the LSTM cell\n        dec_input = prev_y\n        \n        \n        \n       #dec_input = torch.cat([prev_output, curr_features], dim=1) \n        \n        #itearate over LSTM - according to the required output days\n        for out_days in range(self.output_length) :\n            \n          \n            prev_x,prev_hidden,prev_cell = self.decoder(dec_input,hidden,cell)\n            hidden,cell = prev_hidden,prev_cell\n            \n            prev_x = prev_x[:,:,0:1]\n            #print(\"preve x shape is:\",prev_x.size())\n            #print(\"features shape is:\",features[out_days+1].size())\n            \n            if out_days+1 < self.output_length :\n                dec_input = torch.cat([prev_x,features[out_days+1].reshape(1,1,17)], dim=2) \n            \n            targets_ta.append(prev_x.reshape(1))\n           \n            \n        \n        \n        targets = torch.stack(targets_ta)\n\n        return targets","76898b45":"n_features = trainX.shape[2]\nmodel = Seq2Seq(seq_length, n_features, 512)\nmodel = model.to(device)\nmodel","969b28c8":"def init_weights(m):\n    for name, param in m.named_parameters():\n        nn.init.uniform_(param.data, -0.08, 0.08)\n        \nmodel.apply(init_weights)","4addaf2b":"def train_model(model, TrainX,Trainy,ValidX,Validy,seq_length, n_epochs):\n  \n    history = dict(train=[], val=[])\n\n    #best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = 10000.0\n    mb = master_bar(range(1, n_epochs + 1))\n\n    for epoch in mb:\n        model = model.train()\n\n        train_losses = []\n        for i in progress_bar(range(TrainX.size()[0]),parent=mb):\n            seq_inp = TrainX[i,:,:].to(device)\n            seq_true = Trainy[i,:,:].to(device)\n            features = train_features[i,:,:].to(device)\n           \n            optimizer.zero_grad()\n\n            \n            seq_pred = model(seq_inp,seq_inp[seq_length-1:seq_length,:],features)\n            \n            \n            loss = criterion(seq_pred, seq_true)\n\n            loss.backward()\n            \n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n            optimizer.step()\n\n            train_losses.append(loss.item())\n\n        val_losses = []\n        model = model.eval()\n        with torch.no_grad():\n            for i in progress_bar(range(validX.size()[0]),parent=mb):\n                seq_inp = ValidX[i,:,:].to(device)\n                seq_true = Validy[i,:,:].to(device)\n                features = valid_features[i,:,:].to(device)\n        \n                seq_pred = model(seq_inp,seq_inp[seq_length-1:seq_length,:],features)\n               \n\n                loss = criterion(seq_pred, seq_true)\n                val_losses.append(loss.item())\n\n        train_loss = np.mean(train_losses)\n        val_loss = np.mean(val_losses)\n\n        history['train'].append(train_loss)\n        history['val'].append(val_loss)\n\n        if val_loss < best_loss:\n            best_loss = val_loss\n            torch.save(model.state_dict(), 'best_model_n_features.pt')\n            print(\"saved best model epoch:\",epoch,\"val loss is:\",val_loss)\n        \n        print(f'Epoch {epoch}: train loss {train_loss} val loss {val_loss}')\n        scheduler.step(val_loss)\n    #model.load_state_dict(best_model_wts)\n    return model.eval(), history","8b6b7a42":"optimizer = torch.optim.Adam(model.parameters(), lr=1e-2,weight_decay=1e-5)\ncriterion = torch.nn.MSELoss().to(device) \n#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 5e-3, eta_min=1e-8, last_epoch=-1)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  patience=10, factor =0.5 ,min_lr=1e-7, eps=1e-08)","937a81a1":"model, history = train_model(\n  model,\n  trainX,trainy,\n  validX,validy,\n  seq_length,\n  n_epochs=10, #Train for few epochs as illustration, \n    \n)","d5ad1f9d":"TestX = np.array(TS_selected.iloc[-700:-28:,:]).flatten()\nTesty = np.array(TS_selected.iloc[-28:,:])\n\n\nTestX.shape","d5d0222a":"\ndef difference(data, interval):\n    return [data[i] - data[i - interval] for i in range(interval, len(data))]\n\ntransformed_data = difference(TestX, interval)\nDF = pd.DataFrame()\nDF['sales'] = transformed_data\nDF['orig_sales'] = TestX[7:]\nDF.head(10)","f8f01304":"for i in (1,7,14,28,365):\n    print('Shifting:', i)\n    DF['lag_'+str(i)] = DF['sales'].transform(lambda x: x.shift(i))\nprint('%0.2f min: Time for bulk shift' % ((time.time() - start_time) \/ 60))\n","29adf664":"for i in [7,14,28,60,180,365]:\n    print('Rolling period:', i)\n    DF['rolling_mean_'+str(i)] = DF['orig_sales'].transform(lambda x: x.shift(28).rolling(i).mean())\n    DF['rolling_std_'+str(i)]  = DF['orig_sales'].transform(lambda x: x.shift(28).rolling(i).std())\n\n\nprint('%0.2f min: Time for loop' % ((time.time() - start_time) \/ 60))\nDF.head()","287d1936":"DF = DF.drop('orig_sales', 1)\n\n\nDF_normlized = DF.copy(deep=True)\nscaler = MinMaxScaler(feature_range=(-1, 1))\ny_scaler = MinMaxScaler(feature_range=(-1, 1))\nscaled_data = scaler.fit_transform(DF) \ny_scaler.fit_transform(DF['sales'].values.reshape(-1, 1))\nDF_normlized.iloc[:,:] =  scaled_data\nDF_normlized = DF_normlized.iloc[-90:,:]\nTestX = Variable(torch.Tensor(np.array(DF_normlized)))","42ae4d26":"TestX","0e21d47a":"######Prediction###############\nmodel.load_state_dict(torch.load('..\/input\/seq2seq-simple-model\/best_model_n_features.pt'))\nmodel.eval()\n\nwith torch.no_grad():\n    seq_inp = TestX.to(device)\n    \n        \n    seq_pred = model(seq_inp,seq_inp[seq_length-1:seq_length,:],seq_inp[:,1:])\n          \n","918b8699":"seq_pred.shape","24ad002b":"data_predict = y_scaler.inverse_transform(seq_pred.cpu().numpy())\nlabels = Testy","01811b58":"orignal_data = np.array(TS_selected.iloc[-118:-28:,:]).flatten()\npred = data_predict.flatten()\norig_pred = np.concatenate([orignal_data,pred])\n","993b1a95":"time_diff = 7 \ninv_pred = np.zeros((28))\nfor  index in range(28):\n    inv_pred[index] = orig_pred[90+index]+ orig_pred[90+index-7]\n    orig_pred[90+index] = inv_pred[index]\n","310260a8":"data_predict","b4240b54":"data_predict= np.array(inv_pred)\n#data_predict = np.where(data_predict<0,0,data_predict)\n","48b979d8":"## Add dates\ndf_predict = pd.DataFrame(data_predict)\ndf_predict = df_predict.set_index([dates_list[-28:]])\ndf_labels = pd.DataFrame(labels)\ndf_labels = df_labels.set_index([dates_list[-28:]])\n\n# Plot \nfigure(num=None, figsize=(23, 6), dpi=80, facecolor='w', edgecolor='k')\n#plt.axvline(x=dates_list[train_size], c='r')\nplt.plot( df_labels[0])\nplt.plot(df_predict[0])\nplt.legend(['Time Series','Prediction'],fontsize = 21)\nplt.suptitle('Time-Series Prediction 28 Days',fontsize = 23)\nplt.xticks(fontsize=21 )\nplt.yticks(fontsize=21 )\nplt.ylabel(ylabel = 'Sales Demand',fontsize = 21)\nplt.xlabel(xlabel = 'Date',fontsize = 21)\nplt.show()\n","675f756d":"np.sqrt(((data_predict - labels) ** 2).mean())","54c228ce":"# Difference Transform <a id=\"3\"><\/a>\nA difference transform is a simple way for removing a systematic structure from the time series.\n\nFor example, a trend can be removed by subtracting the previous value from each value in the series. This is called first order differencing. The process can be repeated (e.g. difference the differenced series) to remove second order trends, and so on.\n\nA seasonal structure can be removed in a similar way by subtracting the observation from the prior season, e.g. 12 time steps ago for monthly data with a yearly seasonal structure.\n\nSee this article \nhttps:\/\/machinelearningmastery.com\/machine-learning-data-transforms-for-time-series-forecasting\/\n\nUsually, it is good to clean Sesonabilty and trends from time series before we feed the data to the model \nAs an illustration, I chose a simple time difference to transform for seven days \nBut you can try a different days number  or use more complex statistical models to remove seasonable or trends \nThe transform is done by \nSimple Difference transform  --> X(t)' = X(t) - X(t-7)\n","55e6ca90":"# Split Data \nWe will split the Time Series into a training set and a validation set    \nWe will use all data except from the last 28 days as train and validation \nThe last 28 days will be used for tests data\n","622a99f2":"# One Time Series Model <a id=\"1\"><\/a> ","031b6239":"# Transformiation ","41f6656d":"## Seq2Seq","498577c1":"Features can be lags or rolling windows \nIt is easier to implement using data-frame\nLag is just shifting the sales demand.\n\n![Lag1.JPG](attachment:Lag1.JPG)","f8c3db0b":"## Seq2Seq","24362a79":"# Building the Seq2Seq Model <a id=\"5\"><\/a>","7166dcb2":"###### Resources \n* Pytorch toturial about various seq2seq models \n\n    https:\/\/github.com\/bentrevett\/pytorch-seq2seq\n* Kaggle 1st place - eb-traffic-time-series-forecasting\n\n  https:\/\/www.kaggle.com\/c\/web-traffic-time-series-forecasting\/discussion\/43795\n  \n* Some of the ideas were taken from my previous kernel \n\n  https:\/\/www.kaggle.com\/omershect\/learning-pytorch-lstm-deep-learning-with-m5-data\n  \n* Notebook with Encoder-Decoder and Attention for Text translating - but a good example that I have used and converted to time series   \n  https:\/\/github.com\/bentrevett\/pytorch-seq2seq\/blob\/master\/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb\n  \n  \n  \n\n\n\n#### Version 1  - Two Models \n#### Version 2-3 - Add TOC \n#### Version 4-6  Add Attention to the first example \n#### Version 7 Add illustrations  ","ad300c7c":"# Read Date ","4bb42042":"![Features.JPG](attachment:Features.JPG)","416381c1":"## Seed All","537230dc":" ## Inverse Normalise ","6536d4e9":"## Convert to Pytorch Tensors ","da0cc5ab":"## GPU ","32a7b39c":"To do the manipulation and add the features it is easier to work with a pandas Data frame \nSo we will return our Selected time series into a data frame. ","ca1b8147":"#  Normlize\/Scale Data <a id=\"4\"><\/a>\nScale or Normalization is a technique often applied as part of data preparation for machine learning.\nThe goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values.\nFor machine learning, every dataset does not require normalization.\nIt is required only when features have different rangesor scales.\n\n\n\nNormalizes our data using the min\/max scaler with minimum and maximum values of -1 and 1, respectively","c01fedfa":"## Plot ","8c8c4376":"## Encoder","c9cf6624":"## Decoder \nThe Decoder is a bit more tricky part \n* We feed the Hidden state and the cell state of the last cell of the Encoder\n* We use RNN (in this case LSTM)\n* We predict using the first cell, and then we iterate (using our prediction)\n        to create N-1 more predictions (in this case overall of 28 TS samples)\n* It also use the Attention layer \n\n\n![Decoder_Attention.JPG](attachment:Decoder_Attention.JPG)","91d5351f":" ##  If you like my kernel - Please Vote","f9724423":"# Predict <a id=\"6\"><\/a>\nTo illustrate the  model let's take, the last 28 days available as our target for prediction \nand the previous 90 days as the input to the model \nWe take it from the original series, and we need to issue the transformation again ","884e8b31":"We wil use a sliding window of 90 days \nOur target(labels) is 28 days","d6d2bfd3":"## Normlize ","b4bfcc3b":"## Normlize \nAnoter approach is normlize according to a rolling window ","125c5d45":"# Seq2Seq Model \nThe model needs to be adjusted \nThe main difference is that the decoder input is also fed with the N features (except the previous prediction)\n\n![seq2seq.JPG](attachment:seq2seq.JPG)","8f728bce":"## Plot Example \nTo verify the concept \nLet's plot \n* The original TS (take the length of the sliding window + the label size - 90 days + 28 days)\n* The first array of trainX - which is the first sequence\n* The first array of trainy , which is the first label ","24ac9fed":"## Table of Content \n* [<font size=4>One Time Series Model<\/font>](#1)\n* [<font size=4>Select One Time Series as an Example<\/font>](#2)\n* [<font size=4>Difference Transform<\/font>](#3)\n* [<font size=4>Normlize\/Scale Data<\/font>](#4)\n* [<font size=4>Building the Seq2Seq Model<\/font>](#5)\n* [<font size=4>Predict<\/font>](#6)\n* [<font size=4>Add Features<\/font>](#7)\n","4dcc7d42":"Rolling window is some calculation over a window (example mean , std  )\n\n![rollwindow.JPG](attachment:rollwindow.JPG)","05427302":"# Select One Time Series as an Example <a id=\"2\"><\/a>\nSelecting one arbitrary Time Series \nAt the first phase, it is easier to try,\nlearn and understand the concepts on \none Time Series ","b1cf1326":"# Split Data \nAgain We will split the data into a training set and a validation set    \nWe will use all data except from the last 28 days as train and validation \nThe last 28 days will be used for tests data\n","ba847c67":"There are various Seq2Seq models.\nThe Basic one is based on RNN (can be GRU or LSTM) - with simple connection between \nEncoder and Decoder\n","da6aaa9c":"# Add Features <a id=\"7\"><\/a>\nSo far we add only one feature - The Sales demand \nNow  let's  more features \nWe will add more dimension to the input \nand we will feed it both to the Encoder and the decoder \n","8b486d07":"The Encoder structure is very similar to an LSTM\/RNN network \nfor sequence modeling\/training.\n\nThe difference is that we output the Hidden and Cell states (cell in case of LSTM) \nof the last cell, as an output to  feed it to the decoder.\n\n\n\n\n![Encoder.JPG](attachment:Encoder.JPG)","7ff9cc85":"## Date List\nHere we create dates list, that will help later on to display the Time Series, with the right dates ","6f19b834":"## Pytorch Tensors \nPytorch use tensors as the input to the model \nVariable is a wrapper to the tensor \nThis kernel is only a preliminary starter, \n\nSo I use the Variable wrapper\nA more common way is to train with batches and use the dataset class\nBut this is for later.\n\nIf you want to learn more about Tensors \nRead this tutorial \n\nhttps:\/\/pytorch.org\/tutorials\/beginner\/former_torchies\/tensor_tutorial.html","c9f1196b":"## RMSE ","0783f997":"## RMSE ","5968a72c":"![seq2seqmodel1.JPG](attachment:seq2seqmodel1.JPG)","d7708463":"# Load Libraries ","d1b02982":"## Encoder","48d4fdd8":"## Attention\nThis will take in the previous hidden state of the decoder, $s_{t-1}$, and all of the stacked forward and backward hidden states from the encoder, $H$. The layer will output an attention vector, $a_t$, that is the length of the source sentence, each element is between 0 and 1 and the entire vector sums to 1.\n\n\n![Attention.JPG](attachment:Attention.JPG)\n\nImage was taken from : \nhttps:\/\/github.com\/bentrevett\/pytorch-seq2seq\/blob\/master\/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb","65222592":"## Decoder ","4b155516":"We wil use a sliding window of 90 days \nOur target(labels) is 28 days","7b277d24":"## Pytorch Seq2Seq for M5 Data-Set\nLearning Pytorch Seq2Seq for Time Series with M5 Data-Set \n\nThis notebook is a Pytorch implementation of Seq2Seq model for the M5 competitions. \n\nIt is a basic kernel that illustrates some ideas like \n\n* Seq2Seq with Encoder and Decoder \n* Data Transformation \n* Some Basic Features \n* Prediction and visualization \n* New Version with Attention  example \nThe illustration is done on one Time Series as an example ","1c67bf6a":"## Inverse Difference transform \nThis one is more tricky, and we need to do the following. \n\n* X(t)  = X(t)' + X(t-7)  \n* x(t)'  is the sample after the iffernce transformaition at time \n* X(t) is regular sample at time t \n* x(t-7) is the regular sample at current time minus 7 days \n* The Diff formula is \n* X(t)'  = X(t) - X(t-7)\n* We want X(t) now so \n\n* X(t) = X(t)' + X(t-7) \n* conctnate the original 90 days (input) to the prediction (which is in diff transform domain) , \nuse the formula to update all predicted 28 days \n","c7ac1c49":"## Multi-Dimensional Sliding Window","68a8de4c":"## Attention Decoder ","42293166":"## Create Sequance \nIt is common not to feed the entire data to the network (to the encoder)\nBut to break it into sequences and windows as illustrated below \n\n![Seq2seq_Window.JPG](attachment:Seq2seq_Window.JPG)","6c189442":"# Predict "}}