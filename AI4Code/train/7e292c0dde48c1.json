{"cell_type":{"d770c175":"code","fc7cc4d0":"code","1fdf39d2":"code","d6d84bc6":"code","4e94fc05":"code","d560ad90":"code","794b2bf3":"code","6efc8c18":"code","332df429":"code","b3eae197":"code","69f4164d":"code","589c8eb7":"code","dd4ad857":"code","eebbfe17":"code","351a3f12":"code","3f4046a1":"code","7b685221":"code","5840b18e":"code","c2a7ad4d":"code","3433e138":"code","455f4f4f":"code","20881bfd":"code","7ca6cffe":"code","fc9e6e28":"code","253a800a":"code","c9f5c9cf":"code","8e6a07c5":"code","a83feaf9":"code","3a279056":"code","f0b95556":"code","5f060667":"code","5ee09498":"code","109b3a7e":"code","8466ecd8":"code","c69963b8":"code","8fbc7797":"code","00e7620a":"code","27281361":"code","2bb831e7":"markdown","292af24e":"markdown","28899338":"markdown","5c752cf7":"markdown","9dbf4f17":"markdown","f0ed1237":"markdown","d09f7e24":"markdown","1c644a2f":"markdown","aaf3b937":"markdown"},"source":{"d770c175":"print(\"Setup Complete\")\nimport numpy as np\nimport pandas as pd\nimport os\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport scipy.stats as st\nfrom sklearn import ensemble, tree, linear_model\nimport missingno as msno\nimport seaborn as sns","fc7cc4d0":"#Read Data\nX_full = pd.read_csv('..\/input\/train.csv', index_col='Id')\nX_test_full = pd.read_csv('..\/input\/test.csv', index_col='Id')\nprint(X_full.shape)\nX_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\nprint(X_full.describe())\n#print(X_full.columns)","1fdf39d2":"fig, ax = plt.subplots()\nax.scatter(x = X_full['GrLivArea'], y = X_full.SalePrice)\nplt.show()","d6d84bc6":"print(X_full['GrLivArea'].dtype)\nX_full = X_full.drop(X_full[(X_full['GrLivArea'] > 4000) & (X_full['SalePrice'] < 300000)].index)\nfig, ax = plt.subplots()\nax.scatter(x = X_full['GrLivArea'], y = X_full.SalePrice)\nplt.show()","4e94fc05":"y = X_full.SalePrice\nX_full.drop(['SalePrice'], axis = 1, inplace = True)","d560ad90":"X_full[\"LotFrontage\"] = X_full.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\ninput_full = pd.concat([X_full, X_test_full], axis = 0)\n\nnumeric_features = input_full._get_numeric_data()\nprint(numeric_features)\n\ncategoric_cols = list(set(X_full.columns) - set(numeric_features))\ncategoric_features = input_full[categoric_cols]\nprint(categoric_features)","794b2bf3":"from scipy import stats\nfrom scipy.stats import norm, skew\nsns.distplot(y, fit = norm);\nfig = plt.figure\nplt.show()","6efc8c18":"stats.probplot(y, plot=plt)","332df429":"y = np.log(y)\nsns.distplot(y, fit = norm);","b3eae197":"stats.probplot(y, plot=plt)","69f4164d":"numeric_features['MSSubClass']","589c8eb7":"for col in numeric_features:\n    numeric_features[col] = numeric_features.fillna(0)\n\nfor col in ('Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'):\n    categoric_features[col] = categoric_features.fillna(\"None\")\n\nfor col in ('MasVnrType', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    categoric_features[col] = categoric_features[col].fillna('None')\n    \nfor col in ('MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'GarageYrBlt', 'GarageArea', 'GarageCars', 'MSSubClass'):\n    numeric_features[col] = numeric_features.fillna(0.0)\n    \nfor col in ('MSZoning','Electrical', 'Exterior1st', 'Exterior2nd', 'KitchenQual', 'SaleType'):\n    categoric_features[col] = categoric_features[col].fillna(categoric_features[col].mode()[0])\n    \ncategoric_features[\"Functional\"] = categoric_features[\"Functional\"].fillna(\"Typ\")\n\n#Make sure every variable is cleaned up\nfeatures_w_na = pd.concat([numeric_features.isnull().sum(), categoric_features.isnull().sum()], axis = 0)\nif(features_w_na[features_w_na > 0].empty):\n    print(\"yay data is cleaned up!!!! Nice Job XD!\")\nelse:\n    print(features_w_na[features_w_na > 0])","dd4ad857":"categoric_features.drop(['Utilities'], axis=1)","eebbfe17":"skewed_cols = numeric_features.apply(lambda x: skew(x)).sort_values(ascending = False)\nprint(skewed_cols)","351a3f12":"skewed_cols = skewed_cols[abs(skewed_cols) > 0.75]\n\nfrom scipy.special import boxcox1p\nskewed_features = skewed_cols.index\nlam = 0.15\nfor feat in skewed_features:\n    numeric_features[feat] = boxcox1p(X_full[feat], lam)","3f4046a1":"#checking skewness\nskewed_cols = numeric_features.apply(lambda x: skew(x)).sort_values(ascending = False)\nskewed_cols\n#yay it looks slightly better!","7b685221":"print('Shape of input: {}'.format(X_full.shape))\nprint('Shape of test input: {}'.format(X_test_full.shape))\nprint(list(set(X_test_full.columns) - set(X_full.columns)))","5840b18e":"from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\nohe_input_full = pd.DataFrame(ohe.fit_transform(categoric_features))\nohe_input_full.index = categoric_features.index","c2a7ad4d":"#seperate test and train from data\ninput_full = pd.concat([ohe_input_full, numeric_features], axis=1)\n\nX_full = input_full.head(X_full.shape[0])\nX_test_full = input_full.tail(X_test_full.shape[0])\n\nprint('Shape of input: {}'.format(X_full.shape))\nprint('Shape of test input: {}'.format(X_test_full.shape))\nprint(X_full.head)\nprint(X_test_full.head)","3433e138":"X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n                                                                train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\ncategorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype == \"object\"]\n\nnumerical_cols = [cname for cname in X_train_full.columns if \n                X_train_full[cname].dtype in ['int64', 'float64']]\n\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()","455f4f4f":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nnumerical_transformer = SimpleImputer(strategy='constant')\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n\nevi = [80]\nscore = []\nfor i in evi:\n    model = RandomForestRegressor(n_estimators=i, random_state=0)\n    clf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ])\n    clf.fit(X_train, y_train)\n    preds = clf.predict(X_valid)\n    score.append(mean_absolute_error(y_valid, preds))\nprint('MAE:', score)","20881bfd":"my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(X_valid)\n\n# Evaluate the model\nscore = mean_absolute_error(y_valid, preds)\nprint(score)","7ca6cffe":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","fc9e6e28":"n_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=1).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","253a800a":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","c9f5c9cf":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","8e6a07c5":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","a83feaf9":"GBoost = GradientBoostingRegressor(n_estimators=2999, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","3a279056":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","f0b95556":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","5f060667":"lassoscore = rmsle_cv(lasso)","5ee09498":"enetscore = rmsle_cv(ENet)","109b3a7e":"krrscore = rmsle_cv(KRR)","8466ecd8":"gboostscore = rmsle_cv(GBoost)","c69963b8":"xgbscore = rmsle_cv(model_xgb)","8fbc7797":"lgbscore = rmsle_cv(model_lgb)","00e7620a":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n    \n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   \n        \n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self","27281361":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)","2bb831e7":"# skearln model attempt 2","292af24e":"### Deleted Outlier Points","28899338":"## Clean Up Data","5c752cf7":"# SKLearn model attempt 1","9dbf4f17":"## Setup","f0ed1237":"### Fill in missing values","d09f7e24":"### Skewed Features\nReferenced from https:\/\/www.kaggle.com\/shaygu\/house-prices-begginer-top-7, not sure if this is the right way of giving credit, sorry if it's not!","1c644a2f":"### Seperate Numerical and Categorical Data","aaf3b937":"### OneHot encoding categorical columns"}}