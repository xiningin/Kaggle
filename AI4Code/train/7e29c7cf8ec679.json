{"cell_type":{"a870cc8a":"code","ef52fa58":"code","d63360af":"code","c6f7a014":"code","4ab62ae8":"code","5b3cd5ca":"code","fcecd080":"code","90a3b8e8":"code","997efcca":"code","e1579246":"code","5eab3ad7":"code","6ca921f6":"code","444a0b2b":"code","040aea29":"code","3fd1aa80":"code","96716e87":"code","fd638cfa":"code","651ff1dd":"code","f79d5b42":"code","5829097e":"code","4c645d06":"code","36bc9f7d":"code","39274c43":"code","3b5756a4":"code","ff0db12c":"markdown","6e5da8cc":"markdown","f4c81b5d":"markdown","38b59147":"markdown","292103d7":"markdown","81b557be":"markdown","84d05c0e":"markdown","addd715e":"markdown","127fd734":"markdown","3b1290ea":"markdown","e951fc1e":"markdown","7493c2e5":"markdown","11db17df":"markdown"},"source":{"a870cc8a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import PercentFormatter\nfrom scipy.interpolate import make_interp_spline, BSpline\nimport warnings\nplt.style.use('fivethirtyeight')\nwarnings.filterwarnings('ignore')","ef52fa58":"df = (pd.read_csv('\/kaggle\/input\/usa-cers-dataset\/USA_cars_datasets.csv')\n      .drop('Unnamed: 0', axis=1))\n# Dropping 'Unnamed: 0' feature. (It's an artifact of not saving the csv file properly. One should save any csv with index=False argument.) ","d63360af":"df.sample(10)","c6f7a014":"df.sample(10).T","4ab62ae8":"df.drop(['vin', 'lot'], axis=1, inplace=True)","5b3cd5ca":"df.describe()","fcecd080":"df.info()","90a3b8e8":"for col in df.columns:\n    print('{:15} : {:5} : {:}'.format(col, df[col].nunique(), df[col].dtype))","997efcca":"num_feat = df.select_dtypes(include=np.number).columns\ncat_feat = df.select_dtypes(include=['object']).columns","e1579246":"fig, ax = plt.subplots(ncols=3, nrows=1, figsize=(20, 5))\nfor i in range(3):\n    ax[i].hist(df[num_feat[i]])\n    ax[i].set_title(num_feat[i])\nplt.tight_layout()\nplt.show()","5eab3ad7":"round(df.groupby('title_status').agg({'price':'mean'}))","6ca921f6":"df['state'] = df['state'].str.capitalize()\nplt.figure(figsize=(20, 20))\nsns.heatmap(pd.pivot_table(df, values='price', index=['state'], columns=['year'], aggfunc='mean').iloc[:, 20:], annot=True, fmt='g', cmap='Greens')\nplt.show()","444a0b2b":"sns.pairplot(df)\nplt.show()","040aea29":"fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(20, 5))\ndf[df.year > 2010].plot.scatter(x='price', y='mileage', c='year', cmap='coolwarm', ax=ax[0], ylim=(0, 2*10**5))\ndf[df.year > 1990].plot.hexbin(x='price', y='mileage', gridsize=15, cmap='coolwarm', ax=ax[1], ylim=(0, 2*10**5))\nplt.tight_layout()\nplt.show()","3fd1aa80":"year = df[df['year'] >= 1995].groupby('year').agg({'price':'mean'}).index.to_numpy()\nprice = df[df['year'] >= 1995].groupby('year').agg({'price':'mean'}).to_numpy()\nmileage = df[df['year'] >= 1995].groupby('year').agg({'mileage':'mean'}).to_numpy()\n\nxnew = np.linspace(year.min(), year.max(), 4) \nspl1 = make_interp_spline(year, price, k=3)\npower_smooth1 = spl1(xnew)\nspl2 = make_interp_spline(year, mileage, k=3)\npower_smooth2 = spl2(xnew)\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\ndf[df['year'] >= 1995].groupby('year').agg({'price':'mean'}).plot(kind='line', marker='o', linewidth=1, ax=ax[0])\ndf[df['year'] >= 1995].groupby('year').agg({'mileage':'mean'}).plot(kind='line', marker='o', linewidth=1, ax=ax[1])\nax[0].plot(xnew, power_smooth1, linewidth=2)\nax[1].plot(xnew, power_smooth2, linewidth=2)\nax[0].set_title('Average Car Price by Model year')\nax[1].set_title('Average Mileage by Model year')\nax[0].set_ylabel('Price ($)')\nax[1].set_ylabel('Mileage')\nax[0].legend(['Actual Price', 'Spline Fit'])\nax[1].legend(['Actual Mileage', 'Spline Fit'])\nplt.show()","96716e87":"from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import classification_report, confusion_matrix","fd638cfa":"num_feat = df.drop('price', axis=1).select_dtypes(include=np.number).columns\ncat_feat = df.drop('price', axis=1).select_dtypes(include=['object']).columns\nX = df.drop('price', axis=1)\ny = df['price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","651ff1dd":"numeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler()) #('scaler', MinMaxScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('one_hot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, num_feat),\n        ('cat', categorical_transformer, cat_feat)\n    ])","f79d5b42":"from sklearn.linear_model import LinearRegression\npipe = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier',  LinearRegression())\n])\n\nmodel = pipe.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nmodel.score(X_test, y_test)","5829097e":"from time import time\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score","4c645d06":"results = pd.DataFrame(columns=['Name', 'Scores', 'StdDev', 'Time(s)'])\n\nfor model in [\n    DummyRegressor,\n    LinearRegression, \n    KNeighborsRegressor,\n    DecisionTreeRegressor,\n    RandomForestRegressor, \n    GradientBoostingRegressor,\n    XGBRegressor,\n    LGBMRegressor,\n#     MLPRegressor\n]:\n    pipe = make_pipeline(preprocessor, model())\n    start_time = time()\n    kfold = StratifiedKFold(n_splits=10, random_state=1)\n    scores = cross_val_score(pipe, X_train, \n                             y_train, scoring='r2', cv=kfold)\n    time_mod = time() - start_time\n    results = results.append({\n        'Name' : model.__name__, \n        'Scores' : round(scores.mean(), 2), \n        'StdDev' : round(scores.std(), 2), \n        'Time(s)': round(time_mod, 2)\n    }, ignore_index=True)\n    del pipe\n    print('Analyzed {}.'.format(model.__name__))\nprint('Done!')\nresults = results.sort_values('Scores', ascending=False)","36bc9f7d":"results","39274c43":"results.set_index('Name')['Scores'].plot(kind='barh')\nplt.ylabel('Model Name')\nplt.xlabel('Model Score ($r^2$)')\nplt.xlim(0, 1)\nplt.show()","3b5756a4":"plt.errorbar(results['Scores'], results['Name'], xerr=results['StdDev'], linestyle='None', marker='o')\nplt.xticks(rotation=90)\nplt.xlabel('Model Score ($r^2$) and $\\sigma$')\nplt.ylabel('Model Name')\nplt.xlim(0, 1.1)\nplt.show()","ff0db12c":"This is as expected. Price probably follows a lognormal distribution, the data probably is biased towards newer cars, and the mileage also probably follows a lognormal distribution.","6e5da8cc":"## Let's test the pipeline with Linear Regression Model","f4c81b5d":"Is there a difference in the price due to the 'Title Status'?","38b59147":"Two trends emerge:\n1. Average price of a car model decreases exponentially with the age of the car.\n2. Average mileage of a car seems to decrease almost linearly with the age of the car.\n\nNote, these two variables are not independent.","292103d7":"# Spline Interpolation\n\nSince, the size of the dataset is really small, to see the relationships between different variables, it might be interesting to interpolate with splines, before working on rigorous linear\/non-linear models to fit the data.","81b557be":"<a id=\"pipeline\"><\/a>\n## Model Pipelines\n#### Let's build a general modeling pipeline using sklearn and test 7 different models","84d05c0e":"### To summarize, we've built robust model pipelines using sklearn, after performing exploratory data analysis. These pipelines can be iterated simply by changing a few parameters only at a single place (which avoids repetition).","addd715e":"# Cars Dataset: (EDA + Models pipelines)\n\nWhenever you start working on any data related projects, the work is mostly divided into two parts.\n\n### 1. [Exploratory Data Analysis](#eda)\nIn the initial stages of any project, it is crucial to grasp the data intuitively. This is a research phase, where you perform calculate various statistics related to the data, perform statistical tests and visualize the data. This phase should quickly help one understand the strengths and issues with data. This phase also contains a lot of data cleaning and data wrangling steps to make data amenable for modeling. Sometimes, this phase also involves formulating hypotheses and formulating questions that can be solved analytically or via building statistical models on data.\n\n### 2. [Model Pipelines](#pipeline)\nOnce it's clear that the data needs to be modeled for further applications, the second phase involved building modeling pipelines that can quickly produce a few simple models that can become baseline models against which all the other complicated models need to be evaluated and compared. Scikit-learn is a wonderfully designed package that quickly lets one build a robust machine learning pipelines. It also lets one iterate over the established pipelines very easily. The present notebook demonstrated such a pipeline for cars dataset to show how pipelines can be built within scikit-learn package.","127fd734":"## Most of the cars are priced between \\$13k and \\$20k and have mileages between 50k to 100k.","3b1290ea":"A simple distinct count for the numerical columns and plot the graphs for numerical ones:","e951fc1e":"<a id=\"eda\"><\/a>\n## Exploratory Data Analysis\n\n\nThe dataset is perfect for starting for a simple exploration demonstration. Let's see what the attributes are:","7493c2e5":"So, only 3 attributes are numerical. But, perhaps, 'condition' can be converted to a numerical column, as well. Even the color could be converted to a wavelength. So, color can be converted to numeric as well. State can probably be converted to numerical columns as well, by providing the longitude and latitude. ","11db17df":"Let's get another (equivalent) perspective on the data."}}