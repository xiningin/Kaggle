{"cell_type":{"43bcc5fb":"code","18636e29":"code","2aafb499":"code","d77639b6":"code","0f43a45f":"code","da6d97d9":"code","7ac7e8d2":"code","30572e49":"code","e999bd46":"code","2fabb9ed":"code","460aff65":"code","7ab639ae":"code","718089f8":"code","48eee267":"code","ad1f89d0":"code","769c5308":"code","d374bb57":"code","f1eefca7":"code","a6b79a8a":"code","e69f93f8":"code","a66bf2a8":"code","e90f0bc5":"code","c3e2a7b0":"code","a8d05969":"code","83c8ce6d":"code","45662796":"code","026fc225":"code","1a1b4d13":"code","a2c7b825":"code","9b8d9287":"code","fe779f42":"code","156eb597":"code","181fd3b2":"code","e74e0236":"code","5d4e741d":"code","62dc001e":"code","1eaf1991":"code","90157687":"code","cb5c93bc":"code","7c386115":"code","a4d4a407":"code","8cf11ecb":"code","80e4db52":"code","663a6e50":"code","c64d9f5a":"code","37512625":"code","0f945129":"code","f6916dca":"code","ee6c7f59":"code","f88966b6":"code","d0bc95ff":"code","533ff54f":"code","3cbbd3bb":"code","138be53c":"code","60b27451":"code","a1edab9b":"code","3504efb4":"code","d9e729a7":"code","ae8cac48":"code","205ab583":"code","16a66c3c":"code","e854c12f":"code","cc3889b1":"code","0c63d71f":"code","a5d1ac42":"code","9a9e6ca6":"code","0dbefbdf":"code","7abaa814":"code","48614e28":"code","041b63a3":"code","82621847":"code","1cdf6ec1":"code","ca9a5165":"code","e8b1cf03":"code","281b0b7f":"code","3417ab40":"code","4b9dfcbe":"code","0ac516e6":"code","acb55e8c":"code","b2c797bb":"code","c612a601":"code","0c523610":"code","15f29ebf":"code","4f422f13":"code","f494f47d":"code","53d98611":"code","4e70da84":"code","184f5ac6":"code","e134c9de":"code","9b98a1e1":"code","585214d2":"code","93a56375":"code","48538e56":"code","f1971073":"code","83c7f429":"code","86e15eea":"code","1aa76c8b":"code","4c16816f":"code","9d9cf9a8":"markdown","a1a9f285":"markdown","859907a9":"markdown","31c543c4":"markdown","87ac9e41":"markdown","ade4f833":"markdown","3fe42b69":"markdown","bdb1f172":"markdown","e6f69bd2":"markdown","90295231":"markdown","b9e63349":"markdown","7b4ad7cb":"markdown","469702b6":"markdown","2929c08b":"markdown","a92b93d6":"markdown","bc81ecda":"markdown","b712ed30":"markdown","7cae3cab":"markdown","bd0482dd":"markdown","44564fa5":"markdown","4a2288e9":"markdown","7289bfbe":"markdown","b96e9f09":"markdown","807ffc5e":"markdown","51947fd7":"markdown","f1dd479f":"markdown","34a110de":"markdown","7fdf55b0":"markdown","aa45fbae":"markdown","7aee55dd":"markdown","0f9ca766":"markdown","cc88f758":"markdown","2335c3b4":"markdown","1102a45d":"markdown","ebe01333":"markdown","1cd9e927":"markdown","41c985f1":"markdown","53a41cc6":"markdown"},"source":{"43bcc5fb":"#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler,Normalizer,RobustScaler,MaxAbsScaler,MinMaxScaler,QuantileTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neighbors import KNeighborsClassifier\n# Import StandardScaler from scikit-learn\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import KNNImputer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\n\nfrom sklearn.manifold import TSNE\n# Import train_test_split()\n# Metrics\nfrom sklearn.metrics import roc_auc_score, average_precision_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_curve\nfrom datetime import datetime, date\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.linear_model import LinearRegression, RidgeCV\n\nimport lightgbm as lgbm\nfrom catboost import CatBoostRegressor\nimport tensorflow as tf \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n#import smogn\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n# For training random forest model\nimport lightgbm as lgb\nfrom scipy import sparse\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans \n# Model selection\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif\nfrom sklearn.feature_selection import mutual_info_regression\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nfrom itertools import combinations\n#import smong \n\nimport category_encoders as ce\nimport warnings\nimport optuna \nwarnings.filterwarnings('ignore')","18636e29":"!pip install klib \nimport klib ","2aafb499":"%%time\npath = '\/kaggle\/input\/store-sales-time-series-forecasting\/'\noil_data = pd.read_csv(path+'oil.csv')\ntrain = pd.read_csv(path+'train.csv', parse_dates = True, low_memory = False)\ntest = pd.read_csv(path+'test.csv')\nsubmission_sample = pd.read_csv(path+'sample_submission.csv')\nholidays_data = pd.read_csv(path+'holidays_events.csv',parse_dates = True, low_memory = False)\nstore_data =  pd.read_csv(path+'stores.csv')\ntransaction_data = pd.read_csv(path+'transactions.csv', parse_dates = True, low_memory = False)\n\n\n# time series as indexes\ntrain.index","d77639b6":"# Comparing the datasets length\nfig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"salmon\", \"teal\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","0f43a45f":"print('Number of train samples: ', train.shape)\nprint('Number of test samples: ', test.shape)\nprint('Number of store data: ', store_data.shape)\nprint('Number of Holiday data: ', holidays_data.shape)\nprint('Number of Oil Price data: ', oil_data.shape)\nprint('Number of features: ', len(train.columns))","da6d97d9":"test.tail()","7ac7e8d2":"oil_data.tail()","30572e49":"holidays_data.head()","e999bd46":"store_data.head()","2fabb9ed":"transaction_data.tail()","460aff65":"train.tail()","7ab639ae":"train.columns","718089f8":"train.info()","48eee267":"\ndef reduce_mem_usage(df):\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n    if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.uint8).min and c_max < np.iinfo(np.uint8).max:\n                    df[col] = df[col].astype(np.uint8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.uint16).min and c_max < np.iinfo(np.uint16).max:\n                    df[col] = df[col].astype(np.uint16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.uint32).min and c_max < np.iinfo(np.uint32).max:\n                    df[col] = df[col].astype(np.uint32)                    \n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n                elif c_min > np.iinfo(np.uint64).min and c_max < np.iinfo(np.uint64).max:\n                    df[col] = df[col].astype(np.uint64)\n            elif str(col_type)[:5] == 'float':\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    return df","ad1f89d0":"train['year'] = pd.DatetimeIndex(train['date']).year\ntrain['month'] = pd.DatetimeIndex(train['date']).month\ntrain['day'] = pd.DatetimeIndex(train['date']).day\ntrain['day_of_week'] = pd.DatetimeIndex(train['date']).weekday\ntrain['week_of_year'] = pd.DatetimeIndex(train['date']).weekofyear\ntrain['quarter'] = pd.DatetimeIndex(train['date']).quarter\ntrain['season'] = train.month%12 \/\/ 3 + 1","769c5308":"train1=reduce_mem_usage(train)","d374bb57":"del train ","f1eefca7":"holidays_data['month'] = pd.DatetimeIndex(holidays_data['date']).month\nholidays_data['week_of_year'] = pd.DatetimeIndex(holidays_data['date']).weekofyear\nholidays_data['quarter'] = pd.DatetimeIndex(holidays_data['date']).quarter\nholidays_data['season'] = holidays_data.month%12 \/\/ 3 + 1","a6b79a8a":"holidays_data=holidays_data.drop(['date'], axis=1).head()","e69f93f8":"holidays_data.nunique()","a66bf2a8":"holidays_data=reduce_mem_usage(holidays_data)","e90f0bc5":"holidays_data.columns","c3e2a7b0":"train1 = pd.merge(train1, holidays_data,  how='left', left_on=['month','week_of_year','quarter','season'], right_on = ['month','week_of_year','quarter','season'])","a8d05969":"del holidays_data","83c8ce6d":"#oil_data['year'] = pd.DatetimeIndex(oil_data['date']).year\n#oil_data['month'] = pd.DatetimeIndex(oil_data['date']).month\n\n# oil_data['avgoil_price'] = oil_dat.groupby(['']).transform(np.mean)","45662796":"oil_data=reduce_mem_usage(oil_data)","026fc225":"train1 = pd.merge(train1, oil_data,  how='left', left_on=['date'], right_on = ['date'])","1a1b4d13":"del oil_data","a2c7b825":"store_data=reduce_mem_usage(store_data)","9b8d9287":"train1 = pd.merge(train1, store_data,  how='left', left_on=['store_nbr'], right_on = ['store_nbr'])","fe779f42":"del store_data ","156eb597":"#oil_data['year'] = pd.DatetimeIndex(oil_data['date']).year\n#oil_data['month'] = pd.DatetimeIndex(oil_data['date']).month\n\n# oil_data['avgoil_price'] = oil_dat.groupby(['']).transform(np.mean)","181fd3b2":"transaction_data.head()","e74e0236":"transaction_data=reduce_mem_usage(transaction_data)","5d4e741d":"train1 = pd.merge(train1, transaction_data,  how='left', left_on=['store_nbr', 'date'], right_on = ['store_nbr','date'])","62dc001e":" del transaction_data","1eaf1991":"train1.tail()","90157687":"  train1=reduce_mem_usage(train1)","cb5c93bc":"train1.isnull().sum().values","7c386115":"test.isnull().sum().values","a4d4a407":"train1[train1.isnull().sum(axis=1) >=1].shape","8cf11ecb":"# summarize the number of rows with missing values for each column\nfor i in range(train1.shape[1]):\n    # count number of rows with missing values\n    n_miss = train1.iloc[:,i].isnull().sum()\n    perc = n_miss \/ train1.shape[0] * 100\n    print('> %d, Missing: %d (%.1f%%)' % (i, n_miss, perc))","80e4db52":"df = pd.DataFrame()\ndf[\"n_missing\"] = train1.drop([\"id\", \"sales\"], axis=1).isna().sum(axis=1)\ndf[\"sales\"] = train1[\"sales\"].copy()\n\nfig, ax = plt.subplots(figsize=(12,5))\nax.hist(df[df[\"sales\"]==0][\"n_missing\"],\n        bins=40, edgecolor=\"black\",\n        color=\"darkseagreen\", alpha=0.7, label=\"sales\")\nax.hist(df[df[\"sales\"]==1][\"n_missing\"],\n        bins=40, edgecolor=\"black\",\n        color=\"darkorange\", alpha=0.7, label=\"sales\")\nax.set_title(\"Missing values distributionin  \", fontsize=20, pad=15)\nax.set_xlabel(\"Missing values per row\", fontsize=14, labelpad=10)\nax.set_ylabel(\"Amount of rows\", fontsize=14, labelpad=10)\nax.legend()\nplt.show();","663a6e50":"# summarize the number of rows with missing values for each column\nfor i in range(test.shape[1]):\n    # count number of rows with missing values\n    n_miss = test.iloc[:,i].isnull().sum()\n    perc = n_miss \/ test.shape[0] * 100\n    print('> %d, Missing test data: %d (%.1f%%)' % (i, n_miss, perc))","c64d9f5a":"klib.missingval_plot(train1)","37512625":"train1.duplicated(subset='id', keep='first').sum()","0f945129":"len(train1)-len(train1.drop_duplicates())","f6916dca":"skew =train1.skew().sort_values(ascending =False )\nskew_df= pd.DataFrame({'skew':skew})\nskew_df.head()","ee6c7f59":"skew_df[(skew_df['skew']>=1) |(skew_df['skew']<=-1) ].index","f88966b6":"ax = sns.distplot(train1['onpromotion'])","d0bc95ff":"sns.boxplot(data=train1['onpromotion'], saturation=.3)","533ff54f":"\ntrain1['onpromotion_log'] = np.log(train1['onpromotion' ]+1)\n\nprint(train1['onpromotion'].skew())\n\nprint(train1['onpromotion_log'].skew())","3cbbd3bb":"ax = sns.distplot(train1['onpromotion_log'])","138be53c":"kurtosis= pd.DataFrame(train1.kurtosis(),columns=['Kurtosis'])\nkurtosis.head(8)","60b27451":"kurtosis[(kurtosis['Kurtosis']>=3) |(kurtosis['Kurtosis']<=-3) ].index","a1edab9b":"sns.boxplot(data=train1['sales'], saturation=.5)","3504efb4":"var= train1.var().sort_values(ascending =True )\nvar_df= pd.DataFrame({'var':var})\nvar_df.tail(5)","d9e729a7":"Q1 = train1.quantile(0.25)\nQ3 = train1.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","ae8cac48":"df_out = train1[~((train1 < (Q1 - 1.5 * IQR)) |(train1 > (Q3 + 1.5 * IQR))).any(axis=1)]\n\nprint(df_out.shape)","205ab583":"train1.describe().T","16a66c3c":"plot = klib.corr_plot(train1, annot=False, figsize=(12,10))","e854c12f":"plot.figure.savefig('figure1.pdf')","cc3889b1":"klib.corr_plot(train1, split='pos') # displaying only positive correlations, other settings include threshold, cmap...\n#klib.corr_plot(train, split='neg') # displaying only negative correlations","0c63d71f":"# Comparing the datasets length\nfig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([len(train1), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"salmon\", \"teal\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=18)\nfig.set_facecolor('white')\nplt.show","a5d1ac42":"### klib.cat_plot(train1, figsize=(50,50))","9a9e6ca6":"klib.dist_plot(train1)","0dbefbdf":"train1.info()","7abaa814":"train1[train1.select_dtypes(['float64','float16']).columns] = train1[train1.select_dtypes(['float64','float16']).columns].apply(pd.to_numeric)\ntrain1[train1.select_dtypes(['object','int64','int8']).columns] = train1.select_dtypes(['object','int64','int8']).apply(lambda x: x.astype('category'))\n#test[test.select_dtypes(['float64']).columns] = test[test.select_dtypes(['float64']).columns].apply(pd.to_numeric)\n#test[test.select_dtypes(['object','int64']).columns] = test.select_dtypes(['object','int64']).apply(lambda x: x.astype('category'))","48614e28":"cat_columns = train1.drop(['id','date','store_nbr'], axis=1).select_dtypes(exclude=['float64','float16']).columns\nnum_columns = train1.drop(['id','date','store_nbr'], axis=1).select_dtypes(exclude=['int64','category','int8']).columns","041b63a3":"cat_columns","82621847":"num_columns","1cdf6ec1":"# Numerical features distribution \ni = 1\nplt.figure()\nfig, ax = plt.subplots(3, 2,figsize=(20, 24))\nfor feature in num_columns:\n    plt.subplot(3, 2,i)\n    sns.histplot(train1[feature],color=\"red\", kde=True,bins=100, label='train')\n    #sns.histplot(test[feature],color=\"olive\", kde=True,bins=100, label='test')\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","ca9a5165":"train1.corr()['sales'][:-1].plot.barh(figsize=(8,6),alpha=.6,color='darkblue')\nplt.xlim(-.075,.075);\nplt.xticks([-0.065, -0.05 , -0.025,  0.   ,  0.025,  0.05 ,  0.065],\n           [str(100*i)+'%' for i in [-0.065, -0.05 , -0.025,  0.   ,  0.025,  0.05 ,  0.065]],fontsize=12)\nplt.title('Correlation between target and numerical variables',fontsize=14);","e8b1cf03":"train1.corr().style.background_gradient(cmap='viridis')","281b0b7f":"v0 = sns.color_palette(palette='viridis').as_hex()[0]\nfig = plt.figure(figsize=(18,6))\nsns.boxplot(data=train1[num_columns], color=v0,saturation=.5);\nplt.xticks(fontsize= 14)\nplt.title('Box plot of train numerical columns', fontsize=16);","3417ab40":"fig = plt.figure(figsize=(10,10))\nsns.barplot(y=train1[cat_columns].nunique().values, x=train1[cat_columns].nunique().index, color='blue', alpha=.5)\nplt.xticks(rotation=60)\nplt.title('Number of categorical unique values',fontsize=16);","4b9dfcbe":"labels = train1['family'].astype('category').cat.categories.tolist()\ncounts = train1['family'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nfig1, ax1 = plt.subplots()\nfig = plt.figure(figsize=(10,10))\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()","0ac516e6":"labels = train1['city'].astype('category').cat.categories.tolist()\ncounts = train1['city'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()","acb55e8c":"labels = train1['type_y'].astype('category').cat.categories.tolist()\ncounts = train1['type_y'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()","b2c797bb":"plt.plot(train1['sales'].iloc[0:8000])\n","c612a601":"cat_mini= [ 'year', 'month', 'day_of_week', 'week_of_year',\n       'quarter','type_x', 'locale_name', 'city', 'cluster']\nfig = plt.figure(figsize=(80,80))\ngrid =  gridspec.GridSpec(9,1,figure=fig,hspace=.2,wspace=.2)\nn =1\nfor i in range(1):\n    for j in range(len(cat_mini)):\n        ax = fig.add_subplot(grid[j, i])\n        sns.violinplot(data =  train1, y = 'sales' , x =cat_mini[j] ,ax=ax, alpha =.7, fill=True,palette='viridis')\n        ax.set_title(cat_mini[j],fontsize=14)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        n += 1\nfig.suptitle('Violin plot of target with categorical features', fontsize=16,y=.93);","0c523610":"fig = plt.figure(figsize=(30,30))\ngrid =  gridspec.GridSpec(9,1,figure=fig,hspace=.6,wspace=.6)\nn =1\nfor i in range(1):\n    for j in range(len(cat_mini)):\n        ax = fig.add_subplot(grid[j, i])\n        sns.kdeplot(data =  train1,hue  = cat_mini[j] , x ='sales' ,ax=ax, alpha =.7, fill=True,palette='viridis')\n        ax.set_title(cat_columns[j],fontsize=14)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        n += 1\nfig.suptitle('kdeplot plot of target with categorical features', fontsize=16,y=.93);","15f29ebf":"# Categorical features distribution \ni = 1\nplt.figure()\nfig, ax = plt.subplots(6, 2,figsize=(10,10))\nfor feature in cat_mini:\n    plt.subplot(6, 2,i)\n    sns.histplot(train1[feature],color=\"blue\", label='train')\n    #sns.histplot(test[feature],color=\"olive\", label='test')\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","4f422f13":"train1.sales.nunique()","f494f47d":"train1['sales'].describe()","53d98611":"train1['sales'].describe().iloc[1:].plot.barh(color=v0,alpha=.5,figsize=(12,5))\nplt.title('Target data statistics',fontsize=16)\nplt.yticks(fontsize=14)\nplt.xticks(np.arange(0,10.8,.5));","4e70da84":"import plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom scipy import stats\nf, axes = plt.subplots(nrows=3, ncols=1, figsize=(12, 12))\nf.suptitle('sales', fontsize=16)\ng = sns.kdeplot(train1['sales'], shade=True, label=\"%.2f\"%(train1['sales'].skew()), ax=axes[0])\ng = g.legend(loc=\"best\")\nstats.probplot(train1['sales'], plot=axes[1])\nsns.boxplot(x='sales', data=train1, orient='h', ax=axes[2]);\nplt.tight_layout()\nplt.show()","184f5ac6":"y=train1['sales']\nplt.figure(figsize=(12,6))\nsns.boxplot(x=y, width=.4);\nplt.axvline(np.percentile(y,.1), label='.1%', c='blue', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,2), label='2%', c='gold', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,10), label='10%', c='red', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,90), label='90%', c='red', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,99), label='99%', c='blue', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,98), label='98%', c='gold', linestyle=':', linewidth=3)\nplt.legend()\nplt.title('Box plot of target data', fontsize=16)\nplt.xticks(np.arange(0,10.8,.5));","e134c9de":"%%time \nm = TSNE(learning_rate=50)\ndf_numeric =train1.drop(['id','date','store_nbr'], axis=1).iloc[0:80000]._get_numeric_data()\ndf_numeric=df_numeric.dropna()\nX_train =RobustScaler().fit_transform(df_numeric)\ndel df_numeric \n# Fit and transform the t-SNE model on the numeric dataset\ntsne_features = m.fit_transform(X_train)\nprint(tsne_features.shape)","9b98a1e1":"trainessai=train1.iloc[0:80000].dropna()\ntrainessai['x']=tsne_features[:, 0]\ntrainessai['y']=tsne_features[:, 1]\n# Color the points according to Army Component\nsns.scatterplot(x='x', y='y', hue='sales', data=trainessai)\n# Show the plot\nplt.show()","585214d2":"# Import MiniBatchKmeans \nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.preprocessing import  RobustScaler \nfrom yellowbrick.cluster import KElbowVisualizer","93a56375":"# Define K-means model \nkmeans = MiniBatchKMeans( random_state=42)\nvisualizer = KElbowVisualizer(kmeans, k=(1,40))\nvisualizer.fit(X_train)        # Fit the data to the visualizer\nvisualizer.show()        # Finalize and render the figure","48538e56":"kmeans1 = MiniBatchKMeans(n_clusters=4 ,random_state=42)\nkmean_label1= kmeans1.fit_predict(X_train)\nprint(kmean_label1)","f1971073":"train1.info()","83c7f429":"# select non-numeric columns\ncat_columns = train1.drop(['id','date','store_nbr','sales'], axis=1).select_dtypes(exclude=['int64','float64','float16']).columns\ncat_columns","86e15eea":"# select the float columns\nnum_columns = train1.drop(['id','date','store_nbr','sales'], axis=1).select_dtypes(include=['int64','float64','float16']).columns\nnum_columns","1aa76c8b":"all_columns = num_columns.tolist()+cat_columns.tolist()\nprint(cat_columns)\nprint(num_columns)\nprint(all_columns)","4c16816f":"if set(all_columns) == set(train1.drop(['id','date','store_nbr', 'sales'], axis=1).columns):\n    print('Ok')\nelse:\n    # Let's see the difference \n    print('dans all_columns mais pas dans train  :', set(all_columns) - set(train1.drop(['id','date','store_nbr','sales'], axis=1).columns))\n    print('dans X.columns   mais pas dans all_columns :', set(train.drop(['id','date','store_nbr','sales'], axis=1).columns) - set(all_columns))","9d9cf9a8":"Numerical Data seems to be with few outliers appearing in the box plot Also test numerical data seems to looks like the train ones.\n","a1a9f285":"###  KDE plot of target with  features ","859907a9":"**IQR Score**\n\nThis technique uses the IQR scores calculated earlier to remove outliers. \n\nThe rule of thumb is that anything not in the range of (Q1 - 1.5 IQR) and (Q3 + 1.5 IQR) is an outlier, and can be removed. \n\n","31c543c4":"# Data collection +Data Curation ","87ac9e41":"## Box plot of target data with percentile of .1% and 99.9%","ade4f833":"### Correlation ","3fe42b69":"### Visual Exploratory ","bdb1f172":"\n## Step 2: Load the data\n\nNext, we'll load the training and test data.\n\nWe set index_col=0 in the code cell below to use the id column to index the DataFrame. (If you're not sure how this works, try temporarily removing index_col=0 and see how it changes the result.)\n","e6f69bd2":"### Numerical features distribution\n#### Histograms of numerical features","90295231":"oil_data +train =complete date","b9e63349":"## Outlier Identification\n### Skewness : \n\nSkewness is computed for each row or each column of the data present in the DataFrame object.\n\n\nSkewness is a measure of symmetry, or more precisely, the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point. \n\n**Important Notes:**\n\n\u00b7 If the skewness is between **-0.5 and 0.5**, the data are **fairly symmetrical**\n\n\u00b7 If the skewness is between **-1 and \u2014 0.5** or between **0.5 and 1**, the **data are moderately skewed**\n\n\u00b7 If the skewness is **less than -1 or greater than 1**, the data are **highly skewed**","7b4ad7cb":"## EDA \n\n### Explore the data\n\n    Null Data\n    Categorical data\n    Itrain.isnull().sum().valuess there Text data\n    wich columns will we use\n    IS there outliers that can destory our algo\n    IS there diffrent range of data\n    Curse of dimm...\n    \n\n####  Null Data \n**How sparse is my data?**\nMost data sets contain missing values, often represented as NaN (Not a Number). If you are working with Pandas you can easily check how many missing values exist in each column.","469702b6":"holidays + train =date= day + month ","2929c08b":"<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Data Preparation<\/center><\/h3>\n\n## Data preprocessing\n\nData preprocessing comes after you've cleaned up your data and after you've done some exploratory analysis to understand your dataset. Once you understand your dataset, you'll probably have some idea about how you want to model your data. Machine learning models in Python require numerical input, so if your dataset has categorical variables, you'll need to transform them. Think of data preprocessing as a prerequisite for modeling:\n\nOutlier Handling\n\nScaling\n\nFeature Engineering\n\nFeature Selection \n\n\n","a92b93d6":"### Quantile data :","bc81ecda":"This plot kinda agrees with previous one but it looks like the KDE of some categorical values are pretty much flat compared to other value.","b712ed30":"### Num\/Cat Features ","7cae3cab":"## Categorical features distribution","bd0482dd":"###  exploring target data main statistics","44564fa5":"## check that we have all column","4a2288e9":"## KDE plot of target with features","7289bfbe":"## Convert Dtypes ","b96e9f09":"### Variance : \nFeatures with low variance should be eliminated","807ffc5e":"##  What should we do for each colmun\n### Separate features by dtype\n\nNext we\u2019ll separate the features in the dataframe by their datatype. There are a few different ways to achieve this. I\u2019ve used the select_dtypes() function to obtain specific data types by passing in np.number to obtain the numeric data and exclude=['np.number'] to return the categorical data. Appending .columns to the end returns an Index list containing the column names. For the categorical features, we don\u2019t want to include the target income column, so I\u2019ve dropped that.\n### Cat Features ","51947fd7":"\n## Number of categorical unique values","f1dd479f":"## Target \n### Sales","34a110de":"moth+store_nrb +trasnctions==> mean","7fdf55b0":"## t-SNE visualization of high-dimensional data\n\nt-SNE intuition t-SNE is super powerful, but do you know exactly when to use it? When you want to visually explore the patterns in a high dimensional dataset. \n","aa45fbae":"### Num Features ","7aee55dd":"store + tain = store_nrb ","0f9ca766":"### Box plot of numerical columns","cc88f758":"<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Data Modeling<\/center><\/h3>\n\nModeling is the part of the Cross-Industry Standard Process for Data Mining (CRISP-DM) process model that i like best. Our data is already in good shape, and now we can search for useful patterns in our data.\n\n<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Data Evaluation  <\/center><\/h3>\n\n\n\n\n**MAE**\n\nRegression predictive modeling is the task of approximating a mapping function (f) from input variables (X) to a continuous output variable (y).\n\nRegression is different from classification, which involves predicting a category or class label.\n\nEvaluating Regression Models\n\nA common question by beginners to regression predictive modeling projects is:\n\n    How do I calculate accuracy for my regression model?\n\nAccuracy (e.g. classification accuracy) is a measure for classification, not regression.\n\nWe cannot calculate accuracy for a regression model.\n\nThe skill or performance of a regression model must be reported as an error in those predictions.\n\nThis makes sense if you think about it. If you are predicting a numeric value like a height or a dollar amount, you don\u2019t want to know if the model predicted the value exactly (this might be intractably difficult in practice); instead, we want to know how close the predictions were to the expected values.\n\nError addresses exactly this and summarizes on average how close predictions were to their expected values.\n\nThere are three error metrics that are commonly used for evaluating and reporting the performance of a regression model; they are:\n\n    Mean Squared Error (MSE).\n    Root Mean Squared Error (RMSE).\n    Mean Absolute Error (MAE)\n\nThe evaluation metric for this competition is Root Mean Squared Logarithmic Error.\n\nThe RMSLE is calculated as:\n\n![image.png](attachment:08b6c603-0863-4e58-9e42-ffc37a03094b.png)\n\nwe have done all EDA needed to chose the best preprocessing steps and begin modeling .\nWork is in progress .. \n\nUpvote if you find it useful .","2335c3b4":"**Histograms : numerical data seems to be similar to train numerical data.**\n### Zooming on the correlation between numerical variables and target.","1102a45d":"### Kurtosis \n\n**Describe:**\n\n\u00b7 Kurtosis is one of the two measures that quantify shape of a distribution. **kutosis determine the volume of the outlier**\n\nKurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or outliers. \n\n\u00b7 Kurtosis describes the peakedness of the distribution.\n\n\u00b7 If the distribution is tall and thin it is called a leptokurtic distribution(Kurtosis > 3). Values in a leptokurtic distribution are near the mean or at the extremes.\n\n\u00b7 A flat distribution where the values are moderately spread out (i.e., unlike leptokurtic) is called platykurtic(Kurtosis <3) distribution.\n\n\u00b7 A distribution whose shape is in between a leptokurtic distribution and a platykurtic distribution is called a mesokurtic(Kurtosis=3) distribution. A mesokurtic distribution looks more close to a normal distribution.\n\n\u00b7 Kurtosis is sometimes reported as \u201cexcess kurtosis.\u201d Excess kurtosis is determined by subtracting 3 from the kurtosis. This makes the normal distribution kurtosis equal 0.\n\n**Important Notes:**\n\n\u00b7 Along with skewness, kurtosis is an important descriptive statistic of data distribution. However, the two concepts must not be confused with each other. Skewness essentially measures the symmetry of the distribution, while kurtosis determines the heaviness of the distribution tails.\n\n\u00b7 It is the sharpness of the peak of a frequency-distribution curve .It is actually the measure of outliers present in the distribution.\n\n\u00b7 **High kurtosis** in a data set is an indicator that **data has heavy outliers**.\n\n\u00b7 **Low kurtosis** in a data set is an indicator that **data has lack of outliers.**\n\n    The kurtosis of a normal distribution is 3.\n    If a given distribution has a kurtosis less than 3, it is said to be playkurtic, which means it tends to produce fewer and less extreme outliers than the normal distribution.\n    If a given distribution has a kurtosis greater than 3, it is said to be leptokurtic, which means it tends to produce more outliers than the normal distribution.","ebe01333":"# Kmeans : \n","1cd9e927":"fig = plt.figure(figsize=(18,6))\nsns.boxplot(data=test[num_columns], color=v0,saturation=.5);\nplt.xticks(fontsize= 14)\nplt.title('Box plot of test numerical columns', fontsize=16);","41c985f1":"### Test data ","53a41cc6":"\n<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>CRISP-DM Methodology<\/center><\/h3>\n\n* [Buissness Understanding](#1)\n* [Data Understanding](#2)\n* [Data Preparation](#3)\n* [Data Modeling](#4)   \n* [Data Evaluation](#5)\n\nIn this section we overview our selected method for engineering our solution. CRISP-DM stands for Cross-Industry Standard Process for Data Mining. It is an open standard guide that describes common approaches that are used by data mining experts. CRISP-DM includes descriptions of the typical phases of a project, including tasks details and provides an overview of the data mining lifecycle. The lifecycle model consists of six phases with arrows indicating the most important and frequent dependencies between phases. The sequence of the phases is not strict. In fact, most projects move back and forth between phases as necessary. It starts with business understanding, and then moves to data understanding, data preparation, modelling, evaluation, and deployment. The CRISP-DM model is flexible and can be customized easily.\n## Buissness Understanding\n\n    Tasks:\n\n    1.Determine business objectives\n\n    2.Assess situation\n\n    3.Determine data mining goals\n\n    4.Produce project plan\n\n## Data Understanding\n     Tasks:\n\n    1.Collect data\n\n    2.Describe data\n\n    3.Explore data    \n\n## Data Preparation\n    Tasks\n    1.Data selection\n\n    2.Data preprocessing\n\n    3.Feature engineering\n\n    4.Dimensionality reduction\n\n            Steps:\n\n            Data cleaning\n\n            Data integration\n\n            Data sampling\n\n            Data dimensionality reduction\n\n            Data formatting\n\n            Data transformation\n\n            Scaling\n\n            Aggregation\n\n            Decomposition\n\n## Data Modeling :\n\nModeling is the part of the Cross-Industry Standard Process for Data Mining (CRISP-DM) process model that i like best. Our data is already in good shape, and now we can search for useful patterns in our data.\n\n    Tasks\n    1. Select modeling technique Select technique\n\n    2. Generate test design\n\n    3. Build model\n\n    4. Assess model\n\n## Data Evaluation :\n    Tasks\n\n    1.Evaluate Result\n\n    2.Review Process\n\n    3.Determine next steps\n\n<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Buissness Understanding<\/center><\/h3>\n\n**Goal of the Competition**\nIn this \u201cgetting started\u201d competition, you\u2019ll use time-series forecasting to forecast store sales on data from Corporaci\u00f3n Favorita, a large Ecuadorian-based grocery retailer.\n\nSpecifically, you'll build a model that more accurately predicts the unit sales for thousands of items sold at different Favorita stores. You'll practice your machine learning skills with an approachable training dataset of dates, store, and item information, promotions, and unit sales.\n\n<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Data Understanding<\/center><\/h3>\n\n    \n## Step 1: Import helpful libraries"}}