{"cell_type":{"be9574d5":"code","4e02331d":"code","d5355ac4":"code","7612bff5":"code","b79fbc95":"code","067f5ec8":"code","5d880bf4":"code","7986ad49":"code","353b9bc7":"code","c6f27735":"code","8b52f368":"code","ba95ecd6":"code","08858f4a":"code","7ea75e49":"code","a384a818":"code","ff0d541a":"code","9a69c106":"code","c0d55f55":"markdown","6a2c4c10":"markdown","785589d6":"markdown","94c6bd69":"markdown","bcffa22c":"markdown","98df6074":"markdown","c4fae1c5":"markdown","d543975a":"markdown","960b0934":"markdown","d7198f4e":"markdown","8e3903fd":"markdown","88c05937":"markdown","2fc591cd":"markdown","2177fb76":"markdown"},"source":{"be9574d5":"import pandas as pd\nfrom gensim.models import Word2Vec\nfrom gensim.utils import simple_preprocess\nfrom gensim.test.utils import datapath, get_tmpfile\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nfrom gensim.models import KeyedVectors\nfrom gensim.models import Phrases\nfrom gensim.models.phrases import Phraser\n\n\n# a function for testing w2v pairs\ndef test_w2v(model, pairs):\n  for (pos, neg) in pairs:\n    math_result = model.most_similar(positive=pos, negative=neg)\n    print(f'Positive - {pos}\\tNegative - {neg}')\n    [print(f\"- {result[0]} ({round(result[1],5)})\") for result in math_result[:5]]\n    print()\n    \n\n","4e02331d":"df = pd.read_csv('..\/input\/history-of-philosophy\/phil_nlp.csv')\n\ndf.sample(5)","d5355ac4":"# using gensim's built-in tokenizer \ndf['gensim_tokenized'] = df['sentence'].map(lambda x: simple_preprocess(x.lower(),deacc=True,\n                                                        max_len=100))\n\n# check how it worked\nprint(df.iloc[290646]['sentence'])\ndf['gensim_tokenized'][290646]","7612bff5":"# load the vectors. other vector sizes were used but yielded generally less sensible models\nglove_file = datapath('\/kaggle\/input\/glove-global-vectors-for-word-representation\/glove.6B.50d.txt')\ntmp_file = get_tmpfile(\"test_word2vec.txt\")\n\n_ = glove2word2vec(glove_file, tmp_file)\n\nglove_vectors = KeyedVectors.load_word2vec_format(tmp_file)","b79fbc95":"#defining some pairs to test out\npairs_to_try = [(['law', 'moral'], []),\n                (['self', 'consciousness'], []),\n                (['dialectic'], []),\n                (['logic'], []),\n]","067f5ec8":"# check out how GloVe works on our test pairs\ntest_w2v(glove_vectors, pairs_to_try)","5d880bf4":"# isolate the relevant school\ndocuments = df[df['school'] == 'german_idealism']['gensim_tokenized']\n\n# format the series to be used\nstopwords = []\n\nsentences = [sentence for sentence in documents]\ncleaned = []\nfor sentence in sentences:\n  cleaned_sentence = [word.lower() for word in sentence]\n  cleaned_sentence = [word for word in sentence if word not in stopwords]\n  cleaned.append(cleaned_sentence)\n\n# get bigrams\nbigram = Phrases(cleaned, min_count=20, threshold=10, delimiter=b' ')\nbigram_phraser = Phraser(bigram)\n\nbigramed_tokens = []\nfor sent in cleaned:\n    tokens = bigram_phraser[sent]\n    bigramed_tokens.append(tokens)\n\n# run again to get trigrams\ntrigram = Phrases(bigramed_tokens, min_count=20, threshold=10, delimiter=b' ')\ntrigram_phraser = Phraser(trigram)\n\ntrigramed_tokens = []\nfor sent in bigramed_tokens:\n    tokens = trigram_phraser[sent]\n    trigramed_tokens.append(tokens)\n\n# build a toy model to update with\nbase_model = Word2Vec(size=300, min_count=5)\nbase_model.build_vocab(trigramed_tokens)\ntotal_examples = base_model.corpus_count\n\n# add GloVe's vocabulary & weights\nbase_model.build_vocab([list(glove_vectors.vocab.keys())], update=True)\n\n# train on our data\nbase_model.train(trigramed_tokens, total_examples=total_examples, epochs=base_model.epochs)\nbase_model_wv = base_model.wv\ndel base_model","7986ad49":"test_w2v(base_model_wv, pairs_to_try)","353b9bc7":"# turn the process into a function for easy use in the future\ndef train_glove(source_type, source, glove_vectors, threshold=10, stopwords=[],\n                min_count=20):\n  # isolate the relevant school\n  documents = df[df[source_type] == source]['gensim_tokenized']\n\n  # format the series to be used\n  stopwords = []\n\n  sentences = [sentence for sentence in documents]\n  cleaned = []\n  for sentence in sentences:\n    cleaned_sentence = [word.lower() for word in sentence]\n    cleaned_sentence = [word for word in sentence if word not in stopwords]\n    cleaned.append(cleaned_sentence)\n\n  # get bigrams\n  bigram = Phrases(cleaned, min_count=min_count, threshold=threshold, \n                   delimiter=b' ')\n  bigram_phraser = Phraser(bigram)\n\n  bigramed_tokens = []\n  for sent in cleaned:\n      tokens = bigram_phraser[sent]\n      bigramed_tokens.append(tokens)\n\n  # run again to get trigrams\n  trigram = Phrases(bigramed_tokens, min_count=min_count, threshold=threshold, \n                    delimiter=b' ')\n  trigram_phraser = Phraser(trigram)\n\n  trigramed_tokens = []\n  for sent in bigramed_tokens:\n      tokens = trigram_phraser[sent]\n      trigramed_tokens.append(tokens)\n\n  # build a toy model to update with\n  model = Word2Vec(size=300, min_count=5)\n  model.build_vocab(trigramed_tokens)\n  total_examples = model.corpus_count\n\n  # add GloVe's vocabulary & weights\n  model.build_vocab([list(glove_vectors.vocab.keys())], update=True)\n\n  # train on our data\n  model.train(trigramed_tokens, total_examples=total_examples, epochs=model.epochs)\n  model_wv = model.wv\n  del model\n  return model_wv","c6f27735":"ph_model = train_glove(source_type='school', source='phenomenology', glove_vectors=glove_vectors)\ntest_w2v(ph_model, pairs_to_try)","8b52f368":"pairs_to_try = [(['perception'], []),\n                (['dasein'], []),\n                (['consciousness'], []),\n                (['method'], []),]\n\ntest_w2v(ph_model, pairs_to_try)","ba95ecd6":"w2v_dict = {}\n\nfor school in df['school'].unique():\n  w2v_dict[school] = train_glove(source_type='school', source=school, glove_vectors=glove_vectors)\n  print(f'{school} completed')","08858f4a":"for school in df['school'].unique():\n  print(f'\\t{school.upper()}')\n  print('----------------------')\n  test_w2v(w2v_dict[school], [(['philosophy'], [])])","7ea75e49":"for author in df['author'].unique():\n  w2v_dict[author] = train_glove('author', author, glove_vectors=glove_vectors)\n  print(f'{author} completed')","a384a818":"documents = df['gensim_tokenized']\n\n# format the series to be used\nstopwords = []\n\nsentences = [sentence for sentence in documents]\ncleaned = []\nfor sentence in sentences:\n  cleaned_sentence = [word.lower() for word in sentence]\n  cleaned_sentence = [word for word in sentence if word not in stopwords]\n  cleaned.append(cleaned_sentence)\n\n# get bigrams\nbigram = Phrases(cleaned, min_count=30, threshold=10, \n                  delimiter=b' ')\nbigram_phraser = Phraser(bigram)\n\nbigramed_tokens = []\nfor sent in cleaned:\n    tokens = bigram_phraser[sent]\n    bigramed_tokens.append(tokens)\n\n# run again to get trigrams\ntrigram = Phrases(bigramed_tokens, min_count=30, threshold=10, \n                  delimiter=b' ')\ntrigram_phraser = Phraser(trigram)\n\ntrigramed_tokens = []\nfor sent in bigramed_tokens:\n    tokens = trigram_phraser[sent]\n    trigramed_tokens.append(tokens)\n\n# build a toy model to update with\nall_text_model = Word2Vec(size=300, min_count=5)\nall_text_model.build_vocab(trigramed_tokens)\ntotal_examples = all_text_model.corpus_count\n\n# add GloVe's vocabulary & weights\nall_text_model.build_vocab([list(glove_vectors.vocab.keys())], update=True)\n\n# train on our data\nall_text_model.train(trigramed_tokens, total_examples=total_examples, \n                     epochs=all_text_model.epochs)\nall_text_wv = all_text_model.wv","ff0d541a":"for model in [1, 2]:\n  if model == 1:\n    print(f'\\tPHILOSOPHY CORPUS')\n    print('------------------------------------')\n    test_w2v(all_text_wv, [(['philosophy'], [])])\n  if model == 2:\n    print(f'\\tBASE GLOVE')\n    print('------------------------------------')\n    test_w2v(glove_vectors, [(['philosophy'], [])])","9a69c106":"all_text_wv.save_word2vec_format('\/w2v_models\/w2v_for_nn.bin')\nall_text_wv.save('\/w2v_models\/w2v_for_nn.wordvectors')\n\nfor source in w2v_dict.keys():\n  w2v_dict[source].save(f'\/w2v_models\/{source}_w2v.wordvectors')","c0d55f55":"### Training on every School & Author","6a2c4c10":"These seem to reflect the true uses of words in German Idealism. 'Self' + 'consciousness' is rightly associated with 'self consciousness' and 'moral' + 'law' with 'moral law'. It even identifies the German Idealist tendency to unify logic and method. \n\nThese vectors can be fairly said to reflect how german idealists use these terms. Moreover, they are significantly different than the original GloVe model, which indicates that there was real learning going on here.\n\nFor comparison, let's check these same terms, but as used by Phenomenologists.","785589d6":"### Exporting","94c6bd69":"We'll import GloVe vectors as w2v, then use those as a base from which to train new vectors that are tuned to our corpus.","bcffa22c":"Interestingly, many of these top words align quite strongly with the school's general attitude towards philosophy. Continental thinkers mentioning unreason, analytic philosophers focusing on semantics, and phenomenologists associating philosophy with a method all track well. The ones that don't make sense are those that don't problematize the nature of philosophy to any great degree - capitalist thinkers aren't out there trying to discuss the nature of philosophy.\n\nWe'd also like vectors trained for each individual author.","98df6074":"### W2V Modeling with GloVe Vectors\n\nThis notebook does w2v modeling on the dataset using the pre-trained GloVe vectors as a base. This method is the one used to generate the w2v models found on the [Philosophy Data Project website](http:\/\/www.philosophydata.com).\n\nFirst we import some libraries and get the data.","c4fae1c5":"This sort of stands to reason - 'metaphysics' often has a different meaning outside of philosophical discussion, so it's not surprising to see it as the most changed term here. ","d543975a":"As a test case, let's see how the philosophy thinks of itself as compared to how glove thinks of philosophy.","960b0934":"Using the phenomenology vectors on some central terms of German idealism once again yields some pretty compelling results, except for where the words are rarely used by the phenomenologists. This is to be expected. Let's try the word vectors on some central terms of phenomenology.","d7198f4e":"These look pretty strong. Overall, the GloVe-trained vectors seem to be an effective tool for revealing how a word is used by a school. ","8e3903fd":"Now we want these to be trained on our actual philosophical texts - that way we can see how different thinkers use different words and potentially use the vectors for classification.\n\nSo in the cells below we train the existing GloVe model on on the German Idealist texts as a test.","88c05937":"To further explore this, we'll train w2v models in this way for each school and examine how each of them looks at the same word - 'philosophy.' We can use these in our future dashboard work.","2fc591cd":"### Training on the Full Corpus","2177fb76":"And there you have it, word vectors for each school and author and for the whole corpus. These can be used in classification or just to help understand how the different sources use their terms."}}